machine	B
learning	I
machine	O
learning	B
a	O
probabilistic	O
perspective	O
kevin	O
p.	O
murphy	O
today	O
’	O
s	O
web-enabled	O
deluge	O
of	O
electronic	O
data	O
calls	O
for	O
automated	O
methods	O
of	O
data	O
analysis	O
.	O
machine	B
learning	I
provides	O
these	O
,	O
developing	O
methods	O
that	O
can	O
automatically	O
detect	O
patterns	O
in	O
data	O
and	O
use	O
the	O
uncovered	O
patterns	O
to	O
predict	O
future	O
data	O
.	O
this	O
textbook	O
offers	O
a	O
comprehensive	O
and	O
self-contained	O
introduction	O
to	O
the	O
field	O
of	O
machine	B
learning	I
,	O
a	O
unified	O
,	O
probabilistic	O
approach	O
.	O
the	O
coverage	O
combines	O
breadth	O
and	O
depth	O
,	O
offering	O
necessary	O
background	O
material	O
on	O
such	O
topics	O
as	O
probabili-	O
ty	O
,	O
optimization	B
,	O
and	O
linear	O
algebra	O
as	O
well	O
as	O
discussion	O
of	O
recent	O
developments	O
in	O
the	O
field	O
,	O
including	O
conditional	O
ran-	O
dom	O
fields	O
,	O
l1	O
regularization	B
,	O
and	O
deep	B
learning	I
.	O
the	O
book	O
is	O
written	O
in	O
an	O
informal	O
,	O
accessible	O
style	O
,	O
complete	B
with	O
pseudo-code	O
for	O
the	O
most	O
important	O
algorithms	O
.	O
all	O
topics	O
are	O
copiously	O
illustrated	O
with	O
color	O
images	O
and	O
worked	O
examples	O
drawn	O
from	O
such	O
application	O
domains	O
as	O
biology	O
,	O
text	O
processing	O
,	O
computer	O
vision	O
,	O
and	O
robotics	O
.	O
rather	O
than	O
providing	O
a	O
cookbook	O
of	O
different	O
heuristic	O
methods	O
,	O
the	O
book	O
stresses	O
a	O
principled	O
model-based	O
approach	O
,	O
often	O
using	O
the	O
language	O
of	O
graphical	O
models	O
to	O
specify	O
mod-	O
els	O
in	O
a	O
concise	O
and	O
intuitive	O
way	O
.	O
almost	O
all	O
the	O
models	O
described	O
have	O
been	O
implemented	O
in	O
a	O
matlab	O
software	O
package—pmtk	O
(	O
probabilistic	O
modeling	O
toolkit	O
)	O
—that	O
is	O
freely	O
available	O
online	O
.	O
the	O
book	O
is	O
suitable	O
for	O
upper-level	O
undergraduates	O
with	O
an	O
introductory-level	O
college	O
math	O
background	O
and	O
beginning	O
graduate	O
students	O
.	O
kevin	O
p.	O
murphy	O
is	O
a	O
research	O
scientist	O
at	O
google	O
.	O
previ-	O
ously	O
,	O
he	O
was	O
associate	O
professor	O
of	O
computer	O
science	O
and	O
statistics	O
at	O
the	O
university	O
of	O
british	O
columbia	O
.	O
adaptive	O
computation	O
and	O
machine	B
learning	I
series	O
“	O
an	O
astonishing	O
machine	B
learning	I
book	O
:	O
intuitive	O
,	O
full	B
of	O
examples	O
,	O
fun	O
to	O
read	O
but	O
still	O
comprehensive	O
,	O
strong	O
,	O
and	O
deep	B
!	O
a	O
great	O
starting	O
point	O
for	O
any	O
univer-	O
sity	O
student—and	O
a	O
must-have	O
for	O
anybody	O
in	O
the	O
field.	O
”	O
jan	O
peters	O
,	O
darmstadt	O
university	O
of	O
technology	O
;	O
max-planck	O
institute	O
for	O
intelligent	O
systems	O
“	O
kevin	O
murphy	O
excels	O
at	O
unraveling	O
the	O
complexities	O
of	O
machine	B
learning	I
methods	O
while	O
motivating	O
the	O
reader	O
with	O
a	O
stream	O
of	O
illustrated	O
examples	O
and	O
real-world	O
case	O
studies	O
.	O
the	O
accompanying	O
software	O
package	O
includes	O
source	O
code	O
for	O
many	O
of	O
the	O
figures	O
,	O
making	O
it	O
both	O
easy	O
and	O
very	O
tempting	O
to	O
dive	O
in	O
and	O
explore	O
these	O
methods	O
for	O
yourself	O
.	O
a	O
must-buy	O
for	O
anyone	O
interested	O
in	O
machine	B
learning	I
or	O
curious	O
about	O
how	O
to	O
extract	O
useful	O
knowledge	O
from	O
big	O
data.	O
”	O
john	O
winn	O
,	O
microsoft	O
research	O
“	O
this	O
is	O
a	O
wonderful	O
book	O
that	O
starts	O
with	O
basic	O
topics	O
in	O
statistical	O
modeling	O
,	O
culminating	O
in	O
the	O
most	O
ad-	O
vanced	O
topics	O
.	O
it	O
provides	O
both	O
the	O
theoretical	O
foun-	O
dations	O
of	O
probabilistic	O
machine	O
learning	B
as	O
well	O
as	O
practical	O
tools	O
,	O
in	O
the	O
form	O
of	O
matlab	O
code	O
.	O
the	O
book	O
should	O
be	O
on	O
the	O
shelf	O
of	O
any	O
student	O
interested	O
in	O
the	O
topic	B
,	O
and	O
any	O
practitioner	O
working	O
in	O
the	O
field.	O
”	O
yoram	O
singer	O
,	O
google	O
research	O
“	O
this	O
book	O
will	O
be	O
an	O
essential	O
reference	O
for	O
practitio-	O
ners	O
of	O
modern	O
machine	B
learning	I
.	O
it	O
covers	O
the	O
basic	O
concepts	O
needed	O
to	O
understand	O
the	O
field	O
as	O
a	O
whole	O
,	O
and	O
the	O
powerful	O
modern	O
methods	O
that	O
build	O
on	O
those	O
concepts	O
.	O
in	O
machine	B
learning	I
,	O
the	O
language	O
of	O
prob-	O
ability	O
and	O
statistics	O
reveals	O
important	O
connections	O
be-	O
tween	O
seemingly	O
disparate	O
algorithms	O
and	O
strategies	O
.	O
thus	O
,	O
its	O
readers	O
will	O
become	O
articulate	O
in	O
a	O
holistic	O
view	O
of	O
the	O
state-of-the-art	O
and	O
poised	O
to	O
build	O
the	O
next	O
generation	O
of	O
machine	B
learning	I
algorithms.	O
”	O
david	O
blei	O
,	O
princeton	O
university	O
978-0-262-01802-9	O
the	O
mit	O
press	O
massachusetts	O
institute	O
of	O
technology	O
cambridge	O
,	O
massachusetts	O
02142	O
http	O
:	O
//mitpress.mit.edu	O
the	O
cover	O
image	O
is	O
based	O
on	O
sequential	B
bayesian	O
updating	O
of	O
a	O
2d	O
gaussian	O
distribution	O
.	O
see	O
figure	O
7.11	O
for	O
details	O
.	O
machine	B
learning	I
a	O
probabilistic	O
perspective	O
kevin	O
p.	O
murphy	O
machine	B
learning	I
:	O
a	O
probabilistic	O
perspective	O
machine	B
learning	I
a	O
probabilistic	O
perspective	O
kevin	O
p.	O
murphy	O
the	O
mit	O
press	O
cambridge	O
,	O
massachusetts	O
london	O
,	O
england	O
©	O
2012	O
massachusetts	O
institute	O
of	O
technology	O
all	O
rights	O
reserved	O
.	O
no	O
part	O
of	O
this	O
book	O
may	O
be	O
reproduced	O
in	O
any	O
form	O
by	O
any	O
electronic	O
or	O
mechanical	O
means	O
(	O
including	O
photocopying	O
,	O
recording	O
,	O
or	O
information	B
storage	O
and	O
retrieval	O
)	O
without	O
permission	O
in	O
writing	O
from	O
the	O
publisher	O
.	O
for	O
information	B
about	O
special	O
quantity	O
discounts	O
,	O
please	O
email	O
special_sales	O
@	O
mitpress.mit.edu	O
this	O
book	O
was	O
set	O
in	O
the	O
latex	O
programming	O
language	O
by	O
the	O
author	O
.	O
printed	O
and	O
bound	O
in	O
the	O
united	O
states	O
of	O
america	O
.	O
library	O
of	O
congress	O
cataloging-in-publication	O
information	B
murphy	O
,	O
kevin	O
p.	O
machine	B
learning	I
:	O
a	O
probabilistic	O
perspective	O
/	O
kevin	O
p.	O
murphy	O
.	O
p.	O
cm	O
.	O
—	O
(	O
adaptive	O
computation	O
and	O
machine	B
learning	I
series	O
)	O
includes	O
bibliographical	O
references	O
and	O
index	O
.	O
isbn	O
978-0-262-01802-9	O
(	O
hardcover	O
:	O
alk	O
.	O
paper	O
)	O
1.	O
machine	B
learning	I
.	O
2.	O
probabilities	O
.	O
i.	O
title	O
.	O
q325.5.m87	O
2012	O
006.3	O
’	O
1—dc23	O
2012004558	O
10	O
9	O
8	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
this	O
book	O
is	O
dedicated	O
to	O
alessandro	O
,	O
michael	O
and	O
stefano	O
,	O
and	O
to	O
the	O
memory	O
of	O
gerard	O
joseph	O
murphy	O
.	O
contents	O
preface	O
xxvii	O
1	O
introduction	O
1.1	O
1	O
1.2	O
1.3	O
1.4	O
3	O
11	O
10	O
14	O
1	O
2	O
3	O
8	O
types	O
of	O
machine	B
learning	I
classiﬁcation	O
regression	B
9	O
discovering	O
clusters	B
discovering	O
latent	B
factors	I
discovering	O
graph	B
structure	O
matrix	B
completion	I
machine	O
learning	B
:	O
what	O
and	O
why	O
?	O
1.1.1	O
supervised	B
learning	I
1.2.1	O
1.2.2	O
unsupervised	B
learning	I
1.3.1	O
1.3.2	O
1.3.3	O
1.3.4	O
some	O
basic	O
concepts	O
in	O
machine	B
learning	I
1.4.1	O
1.4.2	O
1.4.3	O
1.4.4	O
1.4.5	O
1.4.6	O
1.4.7	O
1.4.8	O
1.4.9	O
parametric	O
vs	O
non-parametric	O
models	O
a	O
simple	O
non-parametric	O
classiﬁer	O
:	O
k-nearest	O
neighbors	B
the	O
curse	B
of	I
dimensionality	I
parametric	O
models	O
for	O
classiﬁcation	B
and	O
regression	B
linear	O
regression	B
logistic	O
regression	B
overﬁtting	O
22	O
model	B
selection	I
22	O
no	B
free	I
lunch	I
theorem	I
19	O
21	O
13	O
16	O
24	O
16	O
18	O
16	O
19	O
2	O
probability	O
2.1	O
2.2	O
27	O
27	O
introduction	O
a	O
brief	O
review	O
of	O
probability	O
theory	O
discrete	O
random	O
variables	O
2.2.1	O
fundamental	O
rules	B
2.2.2	O
28	O
bayes	O
rule	O
2.2.3	O
29	O
2.2.4	O
independence	O
and	O
conditional	B
independence	I
continuous	O
random	O
variables	O
2.2.5	O
28	O
28	O
32	O
30	O
viii	O
contents	O
the	O
binomial	B
and	O
bernoulli	O
distributions	O
the	O
multinomial	B
and	O
multinoulli	O
distributions	O
the	O
poisson	O
distribution	O
the	O
empirical	B
distribution	I
37	O
37	O
34	O
2.3	O
2.4	O
2.5	O
2.6	O
2.7	O
2.8	O
34	O
42	O
33	O
39	O
41	O
41	O
quantiles	O
mean	B
and	O
variance	B
2.2.6	O
2.2.7	O
33	O
some	O
common	O
discrete	B
distributions	O
2.3.1	O
2.3.2	O
2.3.3	O
2.3.4	O
some	O
common	O
continuous	O
distributions	O
gaussian	O
(	O
normal	B
)	O
distribution	O
2.4.1	O
degenerate	B
pdf	O
2.4.2	O
2.4.3	O
the	O
laplace	O
distribution	O
the	O
gamma	B
distribution	I
2.4.4	O
the	O
beta	B
distribution	I
2.4.5	O
2.4.6	O
pareto	O
distribution	O
joint	O
probability	O
distributions	O
2.5.1	O
2.5.2	O
2.5.3	O
47	O
2.5.4	O
transformations	O
of	O
random	O
variables	O
2.6.1	O
2.6.2	O
2.6.3	O
monte	O
carlo	O
approximation	O
2.7.1	O
2.7.2	O
2.7.3	O
information	B
theory	I
2.8.1	O
2.8.2	O
2.8.3	O
43	O
44	O
44	O
covariance	B
and	O
correlation	O
the	O
multivariate	O
gaussian	O
46	O
multivariate	O
student	O
t	O
distribution	O
dirichlet	O
distribution	O
linear	O
transformations	O
general	O
transformations	O
central	B
limit	I
theorem	I
entropy	O
56	O
kl	O
divergence	O
mutual	B
information	I
50	O
56	O
59	O
49	O
52	O
57	O
38	O
38	O
46	O
49	O
51	O
35	O
54	O
example	O
:	O
change	B
of	I
variables	I
,	O
the	O
mc	O
way	O
53	O
example	O
:	O
estimating	O
π	O
by	O
monte	O
carlo	O
integration	O
accuracy	O
of	O
monte	O
carlo	O
approximation	O
54	O
3	O
generative	O
models	O
for	O
discrete	B
data	O
65	O
3.1	O
3.2	O
3.3	O
67	O
65	O
65	O
68	O
likelihood	B
prior	O
posterior	O
posterior	O
predictive	B
distribution	O
a	O
more	O
complex	O
prior	O
72	O
introduction	O
bayesian	O
concept	B
learning	I
3.2.1	O
67	O
3.2.2	O
3.2.3	O
3.2.4	O
3.2.5	O
the	O
beta-binomial	B
model	O
3.3.1	O
3.3.2	O
3.3.3	O
3.3.4	O
likelihood	B
prior	O
posterior	O
posterior	O
predictive	B
distribution	O
73	O
75	O
72	O
74	O
71	O
77	O
contents	O
ix	O
3.4	O
3.5	O
79	O
79	O
79	O
78	O
likelihood	B
prior	O
posterior	O
posterior	O
predictive	B
82	O
83	O
the	O
dirichlet-multinomial	O
model	O
3.4.1	O
3.4.2	O
3.4.3	O
3.4.4	O
naive	O
bayes	O
classiﬁers	O
3.5.1	O
model	O
ﬁtting	O
using	O
the	O
model	O
for	O
prediction	O
3.5.2	O
the	O
log-sum-exp	B
trick	O
3.5.3	O
feature	B
selection	I
using	O
mutual	B
information	I
3.5.4	O
3.5.5	O
classifying	O
documents	O
using	O
bag	B
of	I
words	I
86	O
85	O
81	O
97	O
notation	O
basics	O
mle	O
for	O
an	O
mvn	O
maximum	B
entropy	I
derivation	O
of	O
the	O
gaussian	O
*	O
99	O
97	O
4.1	O
4.2	O
4.3	O
101	O
107	O
104	O
108	O
106	O
97	O
97	O
quadratic	B
discriminant	I
analysis	I
(	O
qda	O
)	O
linear	B
discriminant	I
analysis	I
(	O
lda	O
)	O
two-class	O
lda	O
mle	O
for	O
discriminant	B
analysis	I
strategies	O
for	O
preventing	O
overﬁtting	B
regularized	O
lda	O
*	O
diagonal	B
lda	O
nearest	B
shrunken	I
centroids	I
classiﬁer	O
*	O
4	O
gaussian	O
models	O
introduction	O
4.1.1	O
4.1.2	O
4.1.3	O
4.1.4	O
gaussian	O
discriminant	B
analysis	I
4.2.1	O
4.2.2	O
4.2.3	O
4.2.4	O
4.2.5	O
4.2.6	O
4.2.7	O
4.2.8	O
inference	B
in	O
jointly	O
gaussian	O
distributions	O
4.3.1	O
4.3.2	O
4.3.3	O
4.3.4	O
linear	O
gaussian	O
systems	O
4.4.1	O
4.4.2	O
124	O
4.4.3	O
125	O
digression	O
:	O
the	O
wishart	O
distribution	O
*	O
inverse	O
wishart	O
distribution	O
4.5.1	O
126	O
4.5.2	O
visualizing	B
the	O
wishart	O
distribution	O
*	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	O
posterior	O
distribution	O
of	O
μ	O
4.6.1	O
posterior	O
distribution	O
of	O
σ	O
*	O
4.6.2	O
128	O
posterior	O
distribution	O
of	O
μ	O
and	O
σ	O
*	O
4.6.3	O
4.6.4	O
sensor	B
fusion	I
with	O
unknown	B
precisions	O
*	O
statement	O
of	O
the	O
result	O
examples	O
111	O
information	B
form	I
proof	O
of	O
the	O
result	O
*	O
127	O
128	O
119	O
statement	O
of	O
the	O
result	O
examples	O
proof	O
of	O
the	O
result	O
*	O
120	O
4.6	O
110	O
115	O
116	O
119	O
4.4	O
4.5	O
111	O
86	O
87	O
101	O
102	O
103	O
106	O
109	O
127	O
132	O
138	O
x	O
contents	O
149	O
map	O
estimation	O
credible	O
intervals	O
inference	B
for	O
a	O
difference	O
in	O
proportions	O
149	O
152	O
156	O
163	O
155	O
bayesian	O
occam	O
’	O
s	O
razor	O
computing	O
the	O
marginal	B
likelihood	I
(	O
evidence	B
)	O
bayes	O
factors	B
jeffreys-lindley	O
paradox	O
*	O
165	O
uninformative	B
priors	O
jeffreys	O
priors	O
*	O
robust	B
priors	I
mixtures	O
of	O
conjugate	B
priors	I
166	O
168	O
165	O
164	O
5.3	O
5.1	O
5.2	O
149	O
149	O
5	O
bayesian	O
statistics	O
introduction	O
summarizing	O
posterior	O
distributions	O
5.2.1	O
5.2.2	O
5.2.3	O
bayesian	O
model	B
selection	I
5.3.1	O
5.3.2	O
5.3.3	O
5.3.4	O
priors	O
5.4.1	O
5.4.2	O
5.4.3	O
5.4.4	O
hierarchical	O
bayes	O
5.5.1	O
empirical	O
bayes	O
5.6.1	O
5.6.2	O
bayesian	O
decision	B
theory	O
5.7.1	O
5.7.2	O
5.7.3	O
168	O
176	O
172	O
5.6	O
5.7	O
5.4	O
5.5	O
171	O
bayes	O
estimators	O
for	O
common	O
loss	B
functions	O
the	O
false	B
positive	I
vs	O
false	B
negative	I
tradeoff	O
other	O
topics	O
*	O
184	O
example	O
:	O
beta-binomial	B
model	O
example	O
:	O
gaussian-gaussian	O
model	O
173	O
173	O
example	O
:	O
modeling	O
related	O
cancer	O
rates	O
154	O
158	O
171	O
177	O
180	O
6	O
frequentist	B
statistics	I
191	O
bootstrap	B
large	O
sample	O
theory	O
for	O
the	O
mle	O
*	O
192	O
193	O
191	O
6.1	O
6.2	O
6.3	O
6.4	O
6.5	O
191	O
194	O
bayes	O
risk	B
minimax	O
risk	B
196	O
admissible	B
estimators	O
introduction	O
sampling	B
distribution	I
of	O
an	O
estimator	B
6.2.1	O
6.2.2	O
frequentist	B
decision	O
theory	O
6.3.1	O
195	O
6.3.2	O
6.3.3	O
desirable	O
properties	O
of	O
estimators	O
6.4.1	O
6.4.2	O
6.4.3	O
6.4.4	O
empirical	B
risk	I
minimization	I
6.5.1	O
6.5.2	O
6.5.3	O
6.5.4	O
200	O
consistent	B
estimators	I
unbiased	O
estimators	O
200	O
minimum	O
variance	O
estimators	O
the	O
bias-variance	B
tradeoff	I
204	O
197	O
200	O
201	O
202	O
regularized	B
risk	I
minimization	I
structural	O
risk	B
minimization	O
estimating	O
the	O
risk	B
using	O
cross	B
validation	I
upper	O
bounding	O
the	O
risk	B
using	O
statistical	B
learning	I
theory	I
*	O
206	O
206	O
205	O
209	O
contents	O
xi	O
6.6	O
surrogate	B
loss	I
functions	O
6.5.5	O
pathologies	B
of	O
frequentist	B
statistics	I
*	O
6.6.1	O
6.6.2	O
6.6.3	O
6.6.4	O
counter-intuitive	O
behavior	O
of	O
conﬁdence	B
intervals	I
p-values	O
considered	O
harmful	O
the	O
likelihood	B
principle	I
214	O
why	O
isn	O
’	O
t	O
everyone	O
a	O
bayesian	O
?	O
210	O
211	O
213	O
215	O
212	O
7	O
217	O
221	O
219	O
220	O
7.4	O
7.5	O
derivation	O
of	O
the	O
mle	O
geometric	O
interpretation	O
convexity	O
linear	B
regression	I
217	O
introduction	O
7.1	O
217	O
model	O
speciﬁcation	O
7.2	O
7.3	O
maximum	O
likelihood	O
estimation	O
(	O
least	B
squares	I
)	O
7.3.1	O
7.3.2	O
7.3.3	O
robust	B
linear	O
regression	B
*	O
ridge	B
regression	I
225	O
7.5.1	O
7.5.2	O
7.5.3	O
7.5.4	O
bayesian	O
linear	B
regression	I
7.6.1	O
7.6.2	O
7.6.3	O
7.6.4	O
basic	O
idea	O
numerically	O
stable	B
computation	O
*	O
connection	O
with	O
pca	O
*	O
228	O
regularization	B
effects	O
of	O
big	B
data	I
231	O
computing	O
the	O
posterior	O
computing	O
the	O
posterior	O
predictive	O
233	O
bayesian	O
inference	B
when	O
σ2	O
is	O
unknown	B
*	O
eb	O
for	O
linear	B
regression	I
(	O
evidence	B
procedure	I
)	O
217	O
227	O
230	O
232	O
223	O
225	O
7.6	O
8	O
logistic	B
regression	I
introduction	O
245	O
247	O
249	O
245	O
246	O
245	O
245	O
8.1	O
8.2	O
model	O
speciﬁcation	O
8.3	O
model	O
ﬁtting	O
mle	O
steepest	B
descent	I
newton	O
’	O
s	O
method	O
iteratively	B
reweighted	I
least	I
squares	I
(	O
irls	O
)	O
quasi-newton	O
(	O
variable	O
metric	O
)	O
methods	O
(	O
cid:4	O
)	O
2	O
regularization	B
multi-class	O
logistic	B
regression	I
8.3.1	O
8.3.2	O
8.3.3	O
8.3.4	O
8.3.5	O
8.3.6	O
8.3.7	O
bayesian	O
logistic	B
regression	I
8.4.1	O
8.4.2	O
8.4.3	O
8.4.4	O
8.4.5	O
online	B
learning	I
and	O
stochastic	B
optimization	I
8.5.1	O
laplace	O
approximation	O
derivation	O
of	O
the	O
bic	O
gaussian	O
approximation	O
for	O
logistic	B
regression	I
approximating	O
the	O
posterior	O
predictive	O
256	O
residual	B
analysis	I
(	O
outlier	O
detection	O
)	O
*	O
online	B
learning	I
and	O
regret	B
minimization	O
8.4	O
8.5	O
260	O
255	O
255	O
252	O
252	O
254	O
261	O
234	O
238	O
250	O
251	O
256	O
262	O
xii	O
8.6	O
264	O
stochastic	B
optimization	I
and	O
risk	B
minimization	O
the	O
lms	O
algorithm	O
the	O
perceptron	B
algorithm	I
266	O
a	O
bayesian	O
view	O
8.5.2	O
8.5.3	O
8.5.4	O
8.5.5	O
generative	O
vs	O
discriminative	B
classiﬁers	O
8.6.1	O
8.6.2	O
8.6.3	O
pros	O
and	O
cons	O
of	O
each	O
approach	O
dealing	O
with	O
missing	B
data	I
fisher	O
’	O
s	O
linear	B
discriminant	I
analysis	I
(	O
flda	O
)	O
*	O
268	O
269	O
265	O
267	O
contents	O
262	O
271	O
9	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	I
281	O
282	O
282	O
deﬁnition	O
examples	O
284	O
log	B
partition	I
function	I
mle	O
for	O
the	O
exponential	B
family	I
bayes	O
for	O
the	O
exponential	B
family	I
*	O
maximum	B
entropy	I
derivation	O
of	O
the	O
exponential	B
family	I
*	O
286	O
287	O
289	O
294	O
9.1	O
9.2	O
9.3	O
9.4	O
9.6	O
9.7	O
281	O
281	O
introduction	O
the	O
exponential	B
family	I
9.2.1	O
9.2.2	O
9.2.3	O
9.2.4	O
9.2.5	O
9.2.6	O
generalized	B
linear	I
models	I
(	O
glms	O
)	O
9.3.1	O
9.3.2	O
9.3.3	O
probit	B
regression	I
9.4.1	O
9.4.2	O
9.4.3	O
9.4.4	O
basics	O
ml	O
and	O
map	O
estimation	O
bayesian	O
inference	B
290	O
293	O
9.5.1	O
9.5.2	O
9.5.3	O
9.5.4	O
generalized	O
linear	O
mixed	O
models	O
*	O
9.6.1	O
9.6.2	O
learning	B
to	I
rank	I
*	O
9.7.1	O
9.7.2	O
9.7.3	O
9.7.4	O
the	O
pointwise	B
approach	I
the	O
pairwise	O
approach	O
the	O
listwise	O
approach	O
loss	B
functions	O
for	O
ranking	B
300	O
10	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
10.1	O
introduction	O
10.1.1	O
10.1.2	O
307	O
chain	B
rule	I
conditional	O
independence	O
307	O
ml/map	O
estimation	O
using	O
gradient-based	O
optimization	B
latent	O
variable	O
interpretation	O
ordinal	B
probit	O
regression	B
*	O
multinomial	B
probit	I
models	O
*	O
295	O
295	O
294	O
9.5	O
multi-task	B
learning	I
296	O
hierarchical	O
bayes	O
for	O
multi-task	B
learning	I
application	O
to	O
personalized	O
email	O
spam	B
ﬁltering	O
application	O
to	O
domain	B
adaptation	I
other	O
kinds	O
of	O
prior	O
297	O
297	O
296	O
example	O
:	O
semi-parametric	O
glmms	O
for	O
medical	O
data	O
computational	O
issues	O
300	O
296	O
298	O
290	O
292	O
293	O
298	O
301	O
301	O
302	O
303	O
307	O
308	O
contents	O
xiii	O
311	O
311	O
310	O
308	O
graphical	O
models	O
graph	O
terminology	O
309	O
directed	O
graphical	O
models	O
naive	O
bayes	O
classiﬁers	O
markov	O
and	O
hidden	B
markov	O
models	O
medical	O
diagnosis	O
313	O
genetic	B
linkage	I
analysis	I
*	O
directed	B
gaussian	O
graphical	O
models	O
*	O
10.1.3	O
10.1.4	O
10.1.5	O
examples	O
10.2.1	O
10.2.2	O
10.2.3	O
10.2.4	O
10.2.5	O
inference	B
learning	O
10.4.1	O
10.4.2	O
10.4.3	O
conditional	B
independence	I
properties	O
of	O
dgms	O
10.5.1	O
plate	O
notation	O
learning	B
from	O
complete	B
data	I
learning	O
with	O
missing	B
and/or	O
latent	B
variables	O
324	O
312	O
318	O
319	O
320	O
320	O
322	O
315	O
323	O
d-separation	O
and	O
the	O
bayes	O
ball	O
algorithm	O
(	O
global	O
markov	O
properties	O
)	O
other	O
markov	O
properties	O
of	O
dgms	O
markov	O
blanket	O
and	O
full	B
conditionals	O
324	O
327	O
327	O
10.5.2	O
10.5.3	O
inﬂuence	O
(	O
decision	B
)	O
diagrams	O
*	O
328	O
11	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
337	O
11.1	O
11.2	O
mixture	B
models	O
latent	B
variable	I
models	I
337	O
337	O
10.2	O
10.3	O
10.4	O
10.5	O
10.6	O
11.3	O
11.4	O
unidentiﬁability	O
computing	O
a	O
map	O
estimate	O
is	O
non-convex	O
346	O
347	O
342	O
339	O
340	O
mixtures	O
of	O
gaussians	O
mixture	O
of	O
multinoullis	O
using	O
mixture	B
models	O
for	O
clustering	B
mixtures	O
of	O
experts	O
11.2.1	O
11.2.2	O
11.2.3	O
11.2.4	O
parameter	B
estimation	O
for	O
mixture	B
models	O
11.3.1	O
11.3.2	O
the	O
em	O
algorithm	O
11.4.1	O
11.4.2	O
11.4.3	O
11.4.4	O
11.4.5	O
11.4.6	O
11.4.7	O
11.4.8	O
11.4.9	O
basic	O
idea	O
em	O
for	O
gmms	O
em	O
for	O
mixture	B
of	I
experts	I
em	O
for	O
dgms	O
with	O
hidden	B
variables	I
em	O
for	O
the	O
student	O
distribution	O
*	O
362	O
em	O
for	O
probit	B
regression	I
*	O
theoretical	O
basis	O
for	O
em	O
*	O
363	O
online	O
em	O
365	O
other	O
em	O
variants	O
*	O
350	O
348	O
349	O
367	O
357	O
340	O
345	O
358	O
359	O
11.5	O
model	B
selection	I
for	O
latent	B
variable	I
models	I
370	O
model	B
selection	I
for	O
probabilistic	O
models	O
model	B
selection	I
for	O
non-probabilistic	O
methods	O
11.5.1	O
11.5.2	O
fitting	O
models	O
with	O
missing	B
data	I
372	O
11.6	O
370	O
370	O
xiv	O
contents	O
11.6.1	O
em	O
for	O
the	O
mle	O
of	O
an	O
mvn	O
with	O
missing	B
data	I
373	O
381	O
387	O
387	O
392	O
12.1	O
382	O
389	O
12.2	O
fa	O
is	O
a	O
low	O
rank	O
parameterization	O
of	O
an	O
mvn	O
inference	B
of	O
the	O
latent	B
factors	I
unidentiﬁability	O
mixtures	O
of	O
factor	B
analysers	O
em	O
for	O
factor	B
analysis	I
models	O
fitting	O
fa	O
models	O
with	O
missing	B
data	I
387	O
386	O
383	O
385	O
381	O
381	O
classical	B
pca	O
:	O
statement	O
of	O
the	O
theorem	O
proof	O
*	O
singular	B
value	I
decomposition	I
(	O
svd	O
)	O
probabilistic	O
pca	O
em	O
algorithm	O
for	O
pca	O
12	O
latent	B
linear	O
models	O
factor	B
analysis	I
12.1.1	O
12.1.2	O
12.1.3	O
12.1.4	O
12.1.5	O
12.1.6	O
principal	B
components	I
analysis	I
(	O
pca	O
)	O
12.2.1	O
12.2.2	O
12.2.3	O
12.2.4	O
12.2.5	O
choosing	O
the	O
number	O
of	O
latent	B
dimensions	O
12.3.1	O
12.3.2	O
pca	O
for	O
categorical	B
data	O
pca	O
for	O
paired	O
and	O
multi-view	O
data	O
12.5.1	O
supervised	O
pca	O
(	O
latent	B
factor	O
regression	B
)	O
12.5.2	O
partial	B
least	I
squares	I
12.5.3	O
canonical	O
correlation	O
analysis	O
independent	O
component	O
analysis	O
(	O
ica	O
)	O
12.6.1	O
12.6.2	O
12.6.3	O
12.6.4	O
maximum	O
likelihood	O
estimation	O
the	O
fastica	O
algorithm	O
using	O
em	O
other	O
estimation	O
principles	O
*	O
model	B
selection	I
for	O
fa/ppca	O
model	B
selection	I
for	O
pca	O
402	O
12.4	O
12.5	O
407	O
407	O
12.6	O
12.3	O
406	O
396	O
398	O
398	O
399	O
404	O
395	O
411	O
410	O
415	O
414	O
405	O
13	O
sparse	B
linear	O
models	O
421	O
13.1	O
13.2	O
13.3	O
13.4	O
421	O
introduction	O
bayesian	O
variable	O
selection	O
13.2.1	O
13.2.2	O
13.2.3	O
(	O
cid:4	O
)	O
1	O
regularization	B
:	O
basics	O
13.3.1	O
13.3.2	O
13.3.3	O
13.3.4	O
13.3.5	O
13.3.6	O
(	O
cid:4	O
)	O
1	O
regularization	B
:	O
algorithms	O
13.4.1	O
coordinate	O
descent	O
429	O
441	O
441	O
422	O
the	O
spike	B
and	I
slab	I
model	O
from	O
the	O
bernoulli-gaussian	O
model	O
to	O
(	O
cid:4	O
)	O
0	O
regularization	B
algorithms	O
426	O
424	O
425	O
why	O
does	O
(	O
cid:4	O
)	O
1	O
regularization	B
yield	O
sparse	B
solutions	O
?	O
optimality	O
conditions	O
for	O
lasso	B
comparison	O
of	O
least	B
squares	I
,	O
lasso	B
,	O
ridge	O
and	O
subset	O
selection	O
regularization	B
path	I
model	O
selection	O
bayesian	O
inference	B
for	O
linear	O
models	O
with	O
laplace	O
priors	O
430	O
436	O
439	O
431	O
440	O
435	O
contents	O
xv	O
13.5	O
449	O
lars	O
and	O
other	O
homotopy	B
methods	O
proximal	O
and	O
gradient	O
projection	O
methods	O
em	O
for	O
lasso	B
447	O
441	O
13.4.2	O
13.4.3	O
13.4.4	O
(	O
cid:4	O
)	O
1	O
regularization	B
:	O
extensions	O
13.5.1	O
13.5.2	O
13.5.3	O
group	B
lasso	I
fused	O
lasso	B
elastic	O
net	O
(	O
ridge	O
and	O
lasso	B
combined	O
)	O
449	O
454	O
442	O
455	O
13.6	O
non-convex	O
regularizers	O
457	O
13.7	O
13.8	O
458	O
462	O
463	O
bridge	B
regression	I
458	O
hierarchical	B
adaptive	I
lasso	I
other	O
hierarchical	O
priors	O
13.6.1	O
13.6.2	O
13.6.3	O
automatic	B
relevance	I
determination	I
(	O
ard	O
)	O
/sparse	O
bayesian	O
learning	B
(	O
sbl	O
)	O
13.7.1	O
13.7.2	O
13.7.3	O
13.7.4	O
13.7.5	O
sparse	B
coding	I
*	O
13.8.1	O
13.8.2	O
13.8.3	O
13.8.4	O
learning	B
a	O
sparse	B
coding	I
dictionary	O
results	O
of	O
dictionary	B
learning	O
from	O
image	O
patches	O
compressed	B
sensing	I
image	O
inpainting	O
and	O
denoising	O
ard	O
for	O
linear	B
regression	I
whence	O
sparsity	B
?	O
465	O
connection	O
to	O
map	O
estimation	O
algorithms	O
for	O
ard	O
*	O
466	O
ard	O
for	O
logistic	B
regression	I
468	O
468	O
469	O
465	O
470	O
472	O
472	O
463	O
14	O
kernels	O
479	O
14.1	O
14.2	O
14.3	O
14.4	O
14.5	O
481	O
479	O
479	O
480	O
480	O
rbf	O
kernels	O
kernels	O
for	O
comparing	O
documents	O
mercer	O
(	O
positive	B
deﬁnite	I
)	O
kernels	O
482	O
linear	O
kernels	O
482	O
matern	O
kernels	O
483	O
string	O
kernels	O
pyramid	O
match	O
kernels	O
kernels	O
derived	O
from	O
probabilistic	O
generative	O
models	O
introduction	O
kernel	B
functions	O
14.2.1	O
14.2.2	O
14.2.3	O
14.2.4	O
14.2.5	O
14.2.6	O
14.2.7	O
14.2.8	O
using	O
kernels	O
inside	O
glms	O
14.3.1	O
kernel	B
machines	O
14.3.2	O
l1vms	O
,	O
rvms	O
,	O
and	O
other	O
sparse	O
vector	O
machines	O
the	O
kernel	B
trick	I
14.4.1	O
14.4.2	O
14.4.3	O
14.4.4	O
support	B
vector	I
machines	I
(	O
svms	O
)	O
14.5.1	O
14.5.2	O
kernelized	O
nearest	B
neighbor	I
classiﬁcation	O
kernelized	O
k-medoids	O
clustering	B
489	O
kernelized	O
ridge	B
regression	I
kernel	O
pca	O
svms	O
for	O
regression	B
svms	O
for	O
classiﬁcation	B
486	O
486	O
496	O
488	O
489	O
498	O
493	O
492	O
484	O
497	O
485	O
487	O
xvi	O
14.6	O
14.7	O
504	O
504	O
choosing	O
c	O
summary	O
of	O
key	O
points	O
a	O
probabilistic	O
interpretation	O
of	O
svms	O
14.5.3	O
14.5.4	O
14.5.5	O
comparison	O
of	O
discriminative	B
kernel	O
methods	O
kernels	O
for	O
building	O
generative	O
models	O
507	O
14.7.1	O
14.7.2	O
14.7.3	O
14.7.4	O
14.7.5	O
smoothing	O
kernels	O
kernel	B
density	I
estimation	I
(	O
kde	O
)	O
from	O
kde	O
to	O
knn	O
kernel	B
regression	I
locally	O
weighted	O
regression	O
509	O
510	O
507	O
512	O
508	O
515	O
515	O
516	O
524	O
519	O
521	O
525	O
15.3	O
15.1	O
15.2	O
predictions	O
using	O
noise-free	O
observations	O
predictions	O
using	O
noisy	O
observations	O
effect	O
of	O
the	O
kernel	B
parameters	O
estimating	O
the	O
kernel	B
parameters	O
computational	O
and	O
numerical	O
issues	O
*	O
semi-parametric	O
gps	O
*	O
15	O
gaussian	O
processes	O
introduction	O
gps	O
for	O
regression	B
15.2.1	O
15.2.2	O
15.2.3	O
15.2.4	O
15.2.5	O
15.2.6	O
gps	O
meet	O
glms	O
15.3.1	O
15.3.2	O
15.3.3	O
connection	O
with	O
other	O
methods	O
15.4.1	O
15.4.2	O
15.4.3	O
15.4.4	O
15.4.5	O
15.4.6	O
15.4.7	O
gp	O
latent	O
variable	O
model	O
approximation	O
methods	O
for	O
large	O
datasets	O
532	O
linear	O
models	O
compared	O
to	O
gps	O
linear	O
smoothers	O
compared	O
to	O
gps	O
svms	O
compared	O
to	O
gps	O
l1vm	O
and	O
rvms	O
compared	O
to	O
gps	O
neural	B
networks	I
compared	O
to	O
gps	O
smoothing	O
splines	O
compared	O
to	O
gps	O
*	O
rkhs	O
methods	O
compared	O
to	O
gps	O
*	O
binary	B
classiﬁcation	I
multi-class	O
classiﬁcation	B
gps	O
for	O
poisson	B
regression	I
15.5	O
15.6	O
15.4	O
540	O
528	O
532	O
525	O
534	O
531	O
533	O
534	O
535	O
536	O
538	O
542	O
544	O
contents	O
505	O
505	O
517	O
518	O
524	O
16	O
adaptive	O
basis	O
function	O
models	O
543	O
16.1	O
16.2	O
16.3	O
543	O
introduction	O
classiﬁcation	B
and	O
regression	B
trees	O
(	O
cart	O
)	O
16.2.1	O
16.2.2	O
16.2.3	O
16.2.4	O
16.2.5	O
16.2.6	O
generalized	O
additive	O
models	O
basics	O
544	O
growing	O
a	O
tree	B
545	O
pruning	B
a	O
tree	B
549	O
pros	O
and	O
cons	O
of	O
trees	O
random	B
forests	I
cart	O
compared	O
to	O
hierarchical	B
mixture	I
of	I
experts	I
*	O
550	O
550	O
552	O
551	O
contents	O
xvii	O
554	O
553	O
555	O
553	O
552	O
557	O
562	O
561	O
563	O
560	O
558	O
559	O
a	O
bayesian	O
view	O
backﬁtting	B
computational	O
efficiency	O
multivariate	B
adaptive	I
regression	I
splines	I
(	O
mars	O
)	O
forward	B
stagewise	I
additive	I
modeling	I
l2boosting	O
adaboost	O
logitboost	O
boosting	B
as	O
functional	B
gradient	I
descent	I
sparse	O
boosting	B
multivariate	O
adaptive	O
regression	O
trees	O
(	O
mart	O
)	O
16.3.1	O
16.3.2	O
16.3.3	O
boosting	B
16.4.1	O
16.4.2	O
16.4.3	O
16.4.4	O
16.4.5	O
16.4.6	O
16.4.7	O
16.4.8	O
why	O
does	O
boosting	B
work	O
so	O
well	O
?	O
16.4.9	O
feedforward	O
neural	O
networks	O
(	O
multilayer	O
perceptrons	O
)	O
16.5.1	O
16.5.2	O
16.5.3	O
16.5.4	O
16.5.5	O
16.5.6	O
16.5.7	O
ensemble	B
learning	I
stacking	O
16.6.1	O
16.6.2	O
error-correcting	B
output	I
codes	I
16.6.3	O
ensemble	B
learning	I
is	O
not	O
equivalent	O
to	O
bayes	O
model	O
averaging	O
experimental	O
comparison	O
16.7.1	O
16.7.2	O
interpreting	O
black-box	B
models	O
convolutional	O
neural	O
networks	O
other	O
kinds	O
of	O
neural	B
networks	I
a	O
brief	O
history	O
of	O
the	O
ﬁeld	O
the	O
backpropagation	B
algorithm	I
identiﬁability	O
regularization	B
bayesian	O
inference	B
*	O
572	O
572	O
582	O
low-dimensional	O
features	B
high-dimensional	O
features	B
564	O
568	O
580	O
580	O
582	O
583	O
568	O
569	O
562	O
563	O
585	O
576	O
581	O
16.4	O
16.5	O
16.6	O
16.7	O
16.8	O
17	O
markov	O
and	O
hidden	B
markov	O
models	O
589	O
introduction	O
17.1	O
17.2	O
markov	O
models	O
589	O
589	O
17.2.1	O
17.2.2	O
17.2.3	O
17.2.4	O
589	O
transition	B
matrix	I
application	O
:	O
language	B
modeling	I
stationary	O
distribution	O
of	O
a	O
markov	O
chain	O
*	O
application	O
:	O
google	O
’	O
s	O
pagerank	O
algorithm	O
for	O
web	O
page	O
ranking	B
*	O
596	O
591	O
17.3	O
hidden	B
markov	O
models	O
603	O
applications	O
of	O
hmms	O
604	O
17.4	O
606	O
17.3.1	O
inference	B
in	O
hmms	O
17.4.1	O
17.4.2	O
17.4.3	O
17.4.4	O
17.4.5	O
types	O
of	O
inference	B
problems	O
for	O
temporal	O
models	O
the	O
forwards	O
algorithm	O
the	O
forwards-backwards	B
algorithm	I
the	O
viterbi	O
algorithm	O
forwards	O
ﬁltering	O
,	O
backwards	O
sampling	O
609	O
610	O
616	O
612	O
606	O
581	O
600	O
xviii	O
17.5	O
17.6	O
617	O
617	O
620	O
learning	B
for	O
hmms	O
training	O
with	O
fully	O
observed	O
data	O
17.5.1	O
em	O
for	O
hmms	O
(	O
the	O
baum-welch	O
algorithm	O
)	O
17.5.2	O
bayesian	O
methods	O
for	O
“	O
ﬁtting	O
”	O
hmms	O
*	O
17.5.3	O
discriminative	B
training	O
17.5.4	O
621	O
17.5.5	O
model	B
selection	I
generalizations	O
of	O
hmms	O
621	O
17.6.1	O
17.6.2	O
17.6.3	O
17.6.4	O
17.6.5	O
17.6.6	O
17.6.7	O
variable	O
duration	O
(	O
semi-markov	O
)	O
hmms	O
hierarchical	O
hmms	O
input-output	O
hmms	O
auto-regressive	O
and	O
buried	O
hmms	O
factorial	O
hmm	O
coupled	O
hmm	O
and	O
the	O
inﬂuence	B
model	I
dynamic	O
bayesian	O
networks	O
(	O
dbns	O
)	O
624	O
625	O
626	O
627	O
628	O
contents	O
618	O
620	O
622	O
628	O
632	O
637	O
632	O
633	O
640	O
18.3	O
18.1	O
18.2	O
ssms	O
for	O
object	O
tracking	O
robotic	O
slam	O
online	O
parameter	O
learning	B
using	O
recursive	B
least	I
squares	I
ssm	O
for	O
time	O
series	O
forecasting	O
*	O
18	O
state	B
space	I
models	O
631	O
introduction	O
631	O
applications	O
of	O
ssms	O
18.2.1	O
18.2.2	O
18.2.3	O
18.2.4	O
inference	B
in	O
lg-ssm	O
18.3.1	O
18.3.2	O
learning	B
for	O
lg-ssm	O
18.4.1	O
18.4.2	O
18.4.3	O
18.4.4	O
18.4.5	O
approximate	O
online	O
inference	B
for	O
non-linear	O
,	O
non-gaussian	O
ssms	O
18.5.1	O
18.5.2	O
18.5.3	O
identiﬁability	O
and	O
numerical	O
stability	O
training	O
with	O
fully	O
observed	O
data	O
em	O
for	O
lg-ssm	O
subspace	O
methods	O
bayesian	O
methods	O
for	O
“	O
ﬁtting	O
”	O
lg-ssms	O
the	O
kalman	O
ﬁltering	B
algorithm	O
the	O
kalman	O
smoothing	O
algorithm	O
extended	O
kalman	O
ﬁlter	O
(	O
ekf	O
)	O
unscented	O
kalman	O
ﬁlter	O
(	O
ukf	O
)	O
assumed	B
density	I
ﬁltering	I
(	O
adf	O
)	O
655	O
18.6	O
hybrid	O
discrete/continuous	O
ssms	O
650	O
652	O
18.5	O
18.4	O
640	O
643	O
647	O
647	O
646	O
647	O
646	O
647	O
648	O
18.6.1	O
18.6.2	O
18.6.3	O
18.6.4	O
656	O
inference	B
application	O
:	O
data	B
association	I
and	O
multi-target	B
tracking	I
application	O
:	O
fault	B
diagnosis	I
659	O
application	O
:	O
econometric	B
forecasting	I
660	O
19	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
661	O
19.1	O
19.2	O
661	O
introduction	O
conditional	B
independence	I
properties	O
of	O
ugms	O
19.2.1	O
key	O
properties	O
661	O
661	O
636	O
647	O
658	O
contents	O
xix	O
an	O
undirected	B
alternative	O
to	O
d-separation	O
comparing	O
directed	B
and	O
undirected	O
graphical	O
models	O
663	O
664	O
the	O
hammersley-clifford	O
theorem	O
representing	O
potential	O
functions	O
665	O
667	O
669	O
674	O
19.3	O
19.4	O
19.5	O
19.6	O
19.7	O
665	O
671	O
676	O
668	O
668	O
ising	O
model	O
hopﬁeld	O
networks	O
potts	O
model	O
gaussian	O
mrfs	O
672	O
markov	O
logic	O
networks	O
*	O
19.2.2	O
19.2.3	O
parameterization	O
of	O
mrfs	O
19.3.1	O
19.3.2	O
examples	O
of	O
mrfs	O
19.4.1	O
19.4.2	O
19.4.3	O
19.4.4	O
19.4.5	O
learning	B
19.5.1	O
19.5.2	O
19.5.3	O
19.5.4	O
19.5.5	O
19.5.6	O
19.5.7	O
conditional	B
random	I
ﬁelds	I
(	O
crfs	O
)	O
19.6.1	O
19.6.2	O
19.6.3	O
structural	O
svms	O
19.7.1	O
19.7.2	O
19.7.3	O
19.7.4	O
19.7.5	O
693	O
676	O
training	O
maxent	O
models	O
using	O
gradient	O
methods	O
training	O
partially	O
observed	O
maxent	O
models	O
approximate	O
methods	O
for	O
computing	O
the	O
mles	O
of	O
mrfs	O
pseudo	B
likelihood	I
stochastic	O
maximum	O
likelihood	O
679	O
feature	B
induction	I
for	O
maxent	B
models	O
*	O
iterative	B
proportional	I
ﬁtting	I
(	O
ipf	O
)	O
*	O
680	O
678	O
677	O
681	O
684	O
chain-structured	O
crfs	O
,	O
memms	O
and	O
the	O
label-bias	O
problem	O
applications	O
of	O
crfs	O
crf	O
training	O
692	O
686	O
678	O
684	O
693	O
ssvms	O
:	O
a	O
probabilistic	O
view	O
ssvms	O
:	O
a	O
non-probabilistic	O
view	O
695	O
cutting	B
plane	I
methods	O
for	O
ﬁtting	O
ssvms	O
online	O
algorithms	O
for	O
ﬁtting	O
ssvms	O
latent	B
structural	O
svms	O
701	O
698	O
700	O
20	O
exact	O
inference	B
for	O
graphical	O
models	O
707	O
20.1	O
20.2	O
20.3	O
20.4	O
707	O
710	O
707	O
707	O
709	O
serial	O
protocol	O
parallel	O
protocol	O
gaussian	O
bp	O
*	O
other	O
bp	O
variants	O
*	O
introduction	O
belief	B
propagation	I
for	O
trees	O
20.2.1	O
20.2.2	O
20.2.3	O
20.2.4	O
the	O
variable	B
elimination	I
algorithm	O
20.3.1	O
20.3.2	O
20.3.3	O
the	O
junction	B
tree	I
algorithm	I
*	O
20.4.1	O
20.4.2	O
message	B
passing	I
on	O
a	O
junction	B
tree	I
20.4.3	O
714	O
the	O
generalized	O
distributive	O
law	O
*	O
computational	O
complexity	O
of	O
ve	O
a	O
weakness	O
of	O
ve	O
computational	O
complexity	O
of	O
jta	O
creating	O
a	O
junction	B
tree	I
720	O
720	O
720	O
712	O
717	O
717	O
722	O
725	O
xx	O
contents	O
20.4.4	O
jta	O
generalizations	O
*	O
726	O
20.5	O
computational	O
intractability	O
of	O
exact	O
inference	B
in	O
the	O
worst	O
case	O
726	O
20.5.1	O
approximate	B
inference	I
727	O
21	O
variational	B
inference	I
731	O
alternative	O
interpretations	O
of	O
the	O
variational	O
objective	O
forward	O
or	O
reverse	O
kl	O
?	O
*	O
733	O
733	O
derivation	O
of	O
the	O
mean	B
ﬁeld	I
update	O
equations	O
example	O
:	O
mean	B
ﬁeld	I
for	O
the	O
ising	O
model	O
737	O
732	O
735	O
739	O
example	O
:	O
factorial	O
hmm	O
introduction	O
731	O
variational	B
inference	I
21.2.1	O
21.2.2	O
the	O
mean	B
ﬁeld	I
method	O
21.3.1	O
21.3.2	O
structured	B
mean	I
ﬁeld	I
*	O
21.4.1	O
variational	O
bayes	O
21.5.1	O
21.5.2	O
variational	O
bayes	O
em	O
21.6.1	O
variational	B
message	I
passing	I
and	O
vibes	O
local	O
variational	O
bounds	O
*	O
21.8.1	O
21.8.2	O
21.8.3	O
21.8.4	O
21.8.5	O
756	O
749	O
742	O
740	O
example	O
:	O
vb	O
for	O
a	O
univariate	O
gaussian	O
example	O
:	O
vb	O
for	O
linear	B
regression	I
742	O
746	O
example	O
:	O
vbem	O
for	O
mixtures	O
of	O
gaussians	O
*	O
756	O
756	O
motivating	O
applications	O
bohning	O
’	O
s	O
quadratic	O
bound	O
to	O
the	O
log-sum-exp	B
function	O
bounds	O
for	O
the	O
sigmoid	B
function	O
other	O
bounds	O
and	O
approximations	O
to	O
the	O
log-sum-exp	B
function	O
*	O
variational	B
inference	I
based	O
on	O
upper	O
bounds	O
760	O
763	O
758	O
762	O
736	O
750	O
21.1	O
21.2	O
21.3	O
21.4	O
21.5	O
21.6	O
21.7	O
21.8	O
22.1	O
22.2	O
22.3	O
22.4	O
22	O
more	O
variational	B
inference	I
767	O
767	O
771	O
774	O
767	O
767	O
769	O
768	O
a	O
brief	O
history	O
lbp	O
on	O
pairwise	O
models	O
lbp	O
on	O
a	O
factor	B
graph	I
convergence	O
accuracy	O
of	O
lbp	O
other	O
speedup	O
tricks	O
for	O
lbp	O
*	O
introduction	O
loopy	B
belief	I
propagation	I
:	O
algorithmic	O
issues	O
22.2.1	O
22.2.2	O
22.2.3	O
22.2.4	O
22.2.5	O
22.2.6	O
loopy	B
belief	I
propagation	I
:	O
theoretical	O
issues	O
*	O
ugms	O
represented	O
in	O
exponential	B
family	I
form	O
22.3.1	O
the	O
marginal	B
polytope	I
22.3.2	O
exact	O
inference	B
as	O
a	O
variational	O
optimization	O
problem	O
22.3.3	O
mean	B
ﬁeld	I
as	O
a	O
variational	O
optimization	O
problem	O
22.3.4	O
lbp	O
as	O
a	O
variational	O
optimization	O
problem	O
22.3.5	O
22.3.6	O
loopy	O
bp	O
vs	O
mean	B
ﬁeld	I
extensions	O
of	O
belief	B
propagation	I
*	O
22.4.1	O
generalized	B
belief	I
propagation	I
783	O
783	O
783	O
776	O
779	O
775	O
777	O
776	O
778	O
779	O
contents	O
xxi	O
22.5	O
787	O
785	O
convex	B
belief	I
propagation	I
22.4.2	O
expectation	B
propagation	I
22.5.1	O
22.5.2	O
22.5.3	O
22.5.4	O
22.5.5	O
22.5.6	O
ep	O
as	O
a	O
variational	B
inference	I
problem	O
optimizing	O
the	O
ep	O
objective	O
using	O
moment	B
matching	I
ep	O
for	O
the	O
clutter	B
problem	I
lbp	O
is	O
a	O
special	O
case	O
of	O
ep	O
ranking	B
players	O
using	O
trueskill	O
other	O
applications	O
of	O
ep	O
791	O
792	O
788	O
799	O
793	O
22.6	O
map	O
state	B
estimation	I
799	O
22.6.1	O
linear	O
programming	O
relaxation	O
22.6.2	O
max-product	B
belief	I
propagation	I
22.6.3	O
22.6.4	O
22.6.5	O
graphcuts	B
experimental	O
comparison	O
of	O
graphcuts	B
and	O
bp	O
dual	B
decomposition	I
799	O
800	O
806	O
801	O
23	O
monte	O
carlo	O
inference	B
815	O
804	O
817	O
789	O
822	O
23.1	O
23.2	O
23.3	O
23.4	O
23.5	O
23.6	O
815	O
using	O
the	O
cdf	B
sampling	O
from	O
a	O
gaussian	O
(	O
box-muller	O
method	O
)	O
815	O
815	O
819	O
819	O
818	O
820	O
817	O
817	O
introduction	O
sampling	O
from	O
standard	O
distributions	O
23.2.1	O
23.2.2	O
rejection	B
sampling	I
basic	O
idea	O
23.3.1	O
example	O
23.3.2	O
23.3.3	O
application	O
to	O
bayesian	O
statistics	O
adaptive	B
rejection	I
sampling	I
23.3.4	O
23.3.5	O
rejection	B
sampling	I
in	O
high	O
dimensions	O
importance	B
sampling	I
23.4.1	O
23.4.2	O
23.4.3	O
23.4.4	O
particle	B
ﬁltering	I
23.5.1	O
23.5.2	O
23.5.3	O
23.5.4	O
23.5.5	O
23.5.6	O
23.5.7	O
rao-blackwellised	O
particle	B
ﬁltering	I
(	O
rbpf	O
)	O
23.6.1	O
23.6.2	O
23.6.3	O
sequential	B
importance	O
sampling	O
the	O
degeneracy	B
problem	I
825	O
the	O
resampling	O
step	O
the	O
proposal	B
distribution	I
application	O
:	O
robot	O
localization	O
application	O
:	O
visual	O
object	O
tracking	B
application	O
:	O
time	O
series	O
forecasting	O
rbpf	O
for	O
switching	O
lg-ssms	O
application	O
:	O
tracking	B
a	O
maneuvering	O
target	O
application	O
:	O
fast	O
slam	O
828	O
823	O
825	O
834	O
824	O
827	O
828	O
831	O
831	O
831	O
820	O
basic	O
idea	O
handling	O
unnormalized	O
distributions	O
importance	B
sampling	I
for	O
a	O
dgm	O
:	O
likelihood	B
weighting	I
sampling	O
importance	O
resampling	O
(	O
sir	O
)	O
822	O
821	O
820	O
832	O
24	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
837	O
xxii	O
introduction	O
24.1	O
24.2	O
gibbs	O
sampling	O
837	O
838	O
24.2.1	O
24.2.2	O
24.2.3	O
24.2.4	O
24.2.5	O
24.2.6	O
24.2.7	O
24.2.8	O
838	O
basic	O
idea	O
example	O
:	O
gibbs	O
sampling	O
for	O
the	O
ising	O
model	O
example	O
:	O
gibbs	O
sampling	O
for	O
inferring	O
the	O
parameters	O
of	O
a	O
gmm	O
collapsed	O
gibbs	O
sampling	O
*	O
gibbs	O
sampling	O
for	O
hierarchical	O
glms	O
bugs	O
and	O
jags	O
the	O
imputation	B
posterior	O
(	O
ip	O
)	O
algorithm	O
blocking	O
gibbs	O
sampling	O
838	O
846	O
844	O
847	O
847	O
841	O
24.3	O
metropolis	O
hastings	O
algorithm	O
848	O
contents	O
840	O
basic	O
idea	O
gibbs	O
sampling	O
is	O
a	O
special	O
case	O
of	O
mh	O
proposal	O
distributions	O
adaptive	O
mcmc	O
initialization	O
and	O
mode	B
hopping	O
850	O
853	O
854	O
849	O
reversible	O
jump	O
(	O
trans-dimensional	O
)	O
mcmc	O
*	O
857	O
858	O
854	O
856	O
848	O
24.3.1	O
24.3.2	O
24.3.3	O
24.3.4	O
24.3.5	O
24.3.6	O
why	O
mh	O
works	O
*	O
24.3.7	O
speed	O
and	O
accuracy	O
of	O
mcmc	O
24.4.1	O
24.4.2	O
24.4.3	O
24.4.4	O
24.4.5	O
auxiliary	O
variable	O
mcmc	O
*	O
24.5.1	O
24.5.2	O
24.5.3	O
24.5.4	O
856	O
the	O
burn-in	B
phase	I
mixing	O
rates	O
of	O
markov	O
chains	O
*	O
practical	O
convergence	O
diagnostics	O
accuracy	O
of	O
mcmc	O
how	O
many	O
chains	O
?	O
860	O
862	O
863	O
auxiliary	O
variable	O
sampling	O
for	O
logistic	B
regression	I
slice	O
sampling	O
swendsen	O
wang	O
hybrid/hamiltonian	O
mcmc	O
*	O
866	O
868	O
864	O
24.4	O
24.5	O
24.6	O
annealing	B
methods	O
868	O
24.7	O
869	O
simulated	B
annealing	I
annealed	O
importance	B
sampling	I
parallel	O
tempering	O
24.6.1	O
24.6.2	O
24.6.3	O
approximating	O
the	O
marginal	B
likelihood	I
24.7.1	O
24.7.2	O
24.7.3	O
the	O
candidate	O
method	O
harmonic	B
mean	I
estimate	O
annealed	B
importance	I
sampling	I
872	O
871	O
872	O
871	O
872	O
873	O
25	O
clustering	B
25.1	O
875	O
introduction	O
25.1.1	O
25.1.2	O
875	O
measuring	O
(	O
dis	O
)	O
similarity	O
evaluating	O
the	O
output	O
of	O
clustering	B
methods	O
*	O
875	O
25.2	O
dirichlet	O
process	O
mixture	B
models	O
879	O
25.2.1	O
25.2.2	O
from	O
ﬁnite	O
to	O
inﬁnite	B
mixture	I
models	I
the	O
dirichlet	O
process	O
882	O
879	O
855	O
863	O
876	O
contents	O
xxiii	O
25.2.3	O
25.2.4	O
applying	O
dirichlet	O
processes	O
to	O
mixture	B
modeling	O
fitting	O
a	O
dp	O
mixture	B
model	I
886	O
885	O
25.3	O
affinity	B
propagation	I
25.4	O
spectral	B
clustering	I
25.4.1	O
25.4.2	O
25.4.3	O
887	O
890	O
graph	B
laplacian	O
normalized	O
graph	O
laplacian	O
example	O
893	O
891	O
25.5	O
hierarchical	B
clustering	I
893	O
25.5.1	O
25.5.2	O
25.5.3	O
25.5.4	O
agglomerative	B
clustering	I
divisive	O
clustering	B
choosing	O
the	O
number	O
of	O
clusters	B
bayesian	O
hierarchical	B
clustering	I
898	O
895	O
25.6	O
clustering	B
datapoints	O
and	O
features	B
901	O
25.6.1	O
903	O
25.6.2	O
multi-view	O
clustering	B
biclustering	O
903	O
26	O
graphical	B
model	I
structure	O
learning	B
907	O
892	O
899	O
899	O
directed	B
or	O
undirected	B
tree	O
?	O
chow-liu	O
algorithm	O
for	O
ﬁnding	O
the	O
ml	O
tree	B
structure	O
finding	O
the	O
map	O
forest	B
912	O
911	O
912	O
approximating	O
the	O
marginal	B
likelihood	I
when	O
we	O
have	O
missing	B
data	I
structural	O
em	O
discovering	O
hidden	B
variables	I
case	O
study	O
:	O
google	O
’	O
s	O
rephil	O
structural	B
equation	I
models	I
*	O
926	O
928	O
929	O
925	O
922	O
26.1	O
26.2	O
26.3	O
26.4	O
26.5	O
26.6	O
26.7	O
914	O
914	O
914	O
907	O
909	O
908	O
relevance	O
networks	O
dependency	B
networks	I
910	O
markov	O
equivalence	O
exact	O
structural	O
inference	O
scaling	O
up	O
to	O
larger	O
graphs	O
introduction	O
structure	B
learning	I
for	O
knowledge	B
discovery	I
26.2.1	O
26.2.2	O
learning	B
tree	O
structures	O
26.3.1	O
26.3.2	O
26.3.3	O
26.3.4	O
mixtures	O
of	O
trees	O
learning	B
dag	O
structures	O
26.4.1	O
916	O
26.4.2	O
26.4.3	O
920	O
learning	B
dag	O
structure	O
with	O
latent	B
variables	O
26.5.1	O
26.5.2	O
26.5.3	O
26.5.4	O
26.5.5	O
learning	B
causal	O
dags	O
26.6.1	O
26.6.2	O
26.6.3	O
learning	B
undirected	O
gaussian	O
graphical	O
models	O
26.7.1	O
26.7.2	O
26.7.3	O
26.7.4	O
931	O
causal	O
interpretation	O
of	O
dags	O
using	O
causal	O
dags	O
to	O
resolve	O
simpson	O
’	O
s	O
paradox	O
learning	B
causal	O
dag	O
structures	O
935	O
931	O
mle	O
for	O
a	O
ggm	O
graphical	B
lasso	I
bayesian	O
inference	B
for	O
ggm	O
structure	O
*	O
handling	O
non-gaussian	O
data	O
using	O
copulas	O
*	O
938	O
939	O
941	O
908	O
922	O
938	O
933	O
942	O
xxiv	O
26.8	O
learning	B
undirected	O
discrete	B
graphical	O
models	O
26.8.1	O
26.8.2	O
graphical	B
lasso	I
for	O
mrfs/crfs	O
thin	B
junction	I
trees	I
944	O
942	O
27	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
945	O
introduction	O
27.1	O
27.2	O
distributed	O
state	O
lvms	O
for	O
discrete	B
data	O
945	O
946	O
contents	O
942	O
947	O
953	O
955	O
956	O
961	O
950	O
950	O
957	O
959	O
mixture	B
models	O
946	O
exponential	B
family	I
pca	O
lda	O
and	O
mpca	O
948	O
gap	O
model	O
and	O
non-negative	B
matrix	I
factorization	I
basics	O
unsupervised	O
discovery	O
of	O
topics	O
quantitatively	O
evaluating	O
lda	O
as	O
a	O
language	B
model	I
fitting	O
using	O
(	O
collapsed	O
)	O
gibbs	O
sampling	O
example	O
fitting	O
using	O
batch	B
variational	O
inference	B
fitting	O
using	O
online	O
variational	O
inference	B
determining	O
the	O
number	O
of	O
topics	O
27.2.1	O
27.2.2	O
27.2.3	O
27.2.4	O
latent	B
dirichlet	O
allocation	O
(	O
lda	O
)	O
27.3.1	O
27.3.2	O
27.3.3	O
27.3.4	O
27.3.5	O
27.3.6	O
27.3.7	O
27.3.8	O
extensions	O
of	O
lda	O
27.4.1	O
27.4.2	O
27.4.3	O
27.4.4	O
lvms	O
for	O
graph-structured	O
data	O
stochastic	O
block	O
model	O
27.5.1	O
27.5.2	O
mixed	B
membership	I
stochastic	I
block	I
model	I
relational	O
topic	B
model	I
27.5.3	O
lvms	O
for	O
relational	O
data	O
975	O
inﬁnite	B
relational	I
model	I
27.6.1	O
27.6.2	O
probabilistic	B
matrix	I
factorization	I
for	O
collaborative	B
ﬁltering	I
restricted	O
boltzmann	O
machines	O
(	O
rbms	O
)	O
27.7.1	O
27.7.2	O
27.7.3	O
correlated	B
topic	I
model	I
dynamic	O
topic	B
model	I
lda-hmm	O
supervised	O
lda	O
varieties	O
of	O
rbms	O
learning	B
rbms	O
applications	O
of	O
rbms	O
961	O
962	O
970	O
971	O
960	O
985	O
987	O
974	O
976	O
963	O
967	O
983	O
973	O
27.3	O
27.4	O
27.5	O
27.6	O
27.7	O
949	O
953	O
979	O
991	O
28	O
deep	B
learning	I
995	O
introduction	O
28.1	O
28.2	O
deep	B
generative	O
models	O
995	O
28.2.1	O
28.2.2	O
28.2.3	O
28.2.4	O
995	O
deep	B
directed	I
networks	I
deep	O
boltzmann	O
machines	O
deep	O
belief	O
networks	O
greedy	O
layer-wise	O
learning	B
of	O
dbns	O
996	O
997	O
996	O
28.3	O
deep	B
neural	O
networks	O
999	O
998	O
contents	O
28.3.1	O
28.3.2	O
28.3.3	O
deep	B
multi-layer	O
perceptrons	O
deep	B
auto-encoders	I
1000	O
stacked	O
denoising	O
auto-encoders	O
999	O
1001	O
28.4	O
applications	O
of	O
deep	B
networks	I
1001	O
28.4.1	O
28.4.2	O
28.4.3	O
28.4.4	O
28.4.5	O
handwritten	O
digit	O
classiﬁcation	B
using	O
dbns	O
data	O
visualization	O
and	O
feature	O
discovery	O
using	O
deep	B
auto-encoders	I
information	O
retrieval	O
using	O
deep	B
auto-encoders	I
(	O
semantic	B
hashing	I
)	O
learning	B
audio	O
features	B
using	O
1d	O
convolutional	O
dbns	O
learning	B
image	O
features	B
using	O
2d	O
convolutional	O
dbns	O
1004	O
1005	O
1001	O
xxv	O
1002	O
1003	O
28.5	O
discussion	O
1005	O
notation	O
1009	O
bibliography	O
1015	O
indexes	O
1047	O
index	O
to	O
code	O
index	O
to	O
keywords	O
1047	O
1050	O
preface	O
introduction	O
with	O
the	O
ever	O
increasing	O
amounts	O
of	O
data	O
in	O
electronic	O
form	O
,	O
the	O
need	O
for	O
automated	O
methods	O
for	O
data	O
analysis	O
continues	O
to	O
grow	O
.	O
the	O
goal	O
of	O
machine	B
learning	I
is	O
to	O
develop	O
methods	O
that	O
can	O
automatically	O
detect	O
patterns	O
in	O
data	O
,	O
and	O
then	O
to	O
use	O
the	O
uncovered	O
patterns	O
to	O
predict	O
future	O
data	O
or	O
other	O
outcomes	O
of	O
interest	O
.	O
machine	B
learning	I
is	O
thus	O
closely	O
related	O
to	O
the	O
ﬁelds	O
of	O
statistics	O
and	O
data	O
mining	O
,	O
but	O
differs	O
slightly	O
in	O
terms	O
of	O
its	O
emphasis	O
and	O
terminology	O
.	O
this	O
book	O
provides	O
a	O
detailed	O
introduction	O
to	O
the	O
ﬁeld	O
,	O
and	O
includes	O
worked	O
examples	O
drawn	O
from	O
application	O
domains	O
such	O
as	O
molecular	O
biology	O
,	O
text	O
processing	O
,	O
computer	O
vision	O
,	O
and	O
robotics	O
.	O
target	O
audience	O
this	O
book	O
is	O
suitable	O
for	O
upper-level	O
undergraduate	O
students	O
and	O
beginning	O
graduate	O
students	O
in	O
computer	O
science	O
,	O
statistics	O
,	O
electrical	O
engineering	O
,	O
econometrics	O
,	O
or	O
any	O
one	O
else	O
who	O
has	O
the	O
appropriate	O
mathematical	O
background	O
.	O
speciﬁcally	O
,	O
the	O
reader	O
is	O
assumed	O
to	O
already	O
be	O
familiar	O
with	O
basic	O
multivariate	O
calculus	O
,	O
probability	O
,	O
linear	O
algebra	O
,	O
and	O
computer	O
programming	O
.	O
prior	O
exposure	O
to	O
statistics	O
is	O
helpful	O
but	O
not	O
necessary	O
.	O
a	O
probabilistic	O
approach	O
this	O
books	O
adopts	O
the	O
view	O
that	O
the	O
best	O
way	O
to	O
make	O
machines	O
that	O
can	O
learn	O
from	O
data	O
is	O
to	O
use	O
the	O
tools	O
of	O
probability	O
theory	O
,	O
which	O
has	O
been	O
the	O
mainstay	O
of	O
statistics	O
and	O
engineering	O
for	O
centuries	O
.	O
probability	O
theory	O
can	O
be	O
applied	O
to	O
any	O
problem	O
involving	O
uncertainty	B
.	O
in	O
machine	B
learning	I
,	O
uncertainty	B
comes	O
in	O
many	O
forms	O
:	O
what	O
is	O
the	O
best	O
prediction	O
(	O
or	O
decision	B
)	O
given	O
some	O
data	O
?	O
what	O
is	O
the	O
best	O
model	O
given	O
some	O
data	O
?	O
what	O
measurement	O
should	O
i	O
perform	O
next	O
?	O
etc	O
.	O
including	O
inferring	O
parameters	O
of	O
statistical	O
models	O
,	O
is	O
sometimes	O
called	O
a	O
bayesian	O
approach	O
.	O
however	O
,	O
this	O
term	O
tends	O
to	O
elicit	O
very	O
strong	O
reactions	O
(	O
either	O
positive	O
or	O
negative	O
,	O
depending	O
on	O
who	O
you	O
ask	O
)	O
,	O
so	O
we	O
prefer	O
the	O
more	O
neutral	O
term	O
“	O
probabilistic	O
approach	O
”	O
.	O
besides	O
,	O
we	O
will	O
often	O
use	O
techniques	O
such	O
as	O
maximum	O
likelihood	O
estimation	O
,	O
which	O
are	O
not	O
bayesian	O
methods	O
,	O
but	O
certainly	O
fall	O
within	O
the	O
probabilistic	O
paradigm	O
.	O
the	O
systematic	O
application	O
of	O
probabilistic	O
reasoning	O
to	O
all	O
inferential	O
problems	O
,	O
rather	O
than	O
describing	O
a	O
cookbook	O
of	O
different	O
heuristic	O
methods	O
,	O
this	O
book	O
stresses	O
a	O
princi-	O
pled	O
model-based	O
approach	O
to	O
machine	B
learning	I
.	O
for	O
any	O
given	O
model	O
,	O
a	O
variety	O
of	O
algorithms	O
xxviii	O
preface	O
can	O
often	O
be	O
applied	O
.	O
conversely	O
,	O
any	O
given	O
algorithm	O
can	O
often	O
be	O
applied	O
to	O
a	O
variety	O
of	O
models	O
.	O
this	O
kind	O
of	O
modularity	O
,	O
where	O
we	O
distinguish	O
model	O
from	O
algorithm	O
,	O
is	O
good	O
pedagogy	O
and	O
good	O
engineering	O
.	O
we	O
will	O
often	O
use	O
the	O
language	O
of	O
graphical	O
models	O
to	O
specify	O
our	O
models	O
in	O
a	O
concise	O
and	O
intuitive	O
way	O
.	O
in	O
addition	O
to	O
aiding	O
comprehension	O
,	O
the	O
graph	B
structure	O
aids	O
in	O
developing	O
efficient	O
algorithms	O
,	O
as	O
we	O
will	O
see	O
.	O
however	O
,	O
this	O
book	O
is	O
not	O
primarily	O
about	O
graphical	O
models	O
;	O
it	O
is	O
about	O
probabilistic	O
modeling	O
in	O
general	O
.	O
a	O
practical	O
approach	O
nearly	O
all	O
of	O
the	O
methods	O
described	O
in	O
this	O
book	O
have	O
been	O
implemented	O
in	O
a	O
matlab	O
software	O
package	O
called	O
pmtk	O
,	O
which	O
stands	O
for	O
probabilistic	O
modeling	O
toolkit	O
.	O
this	O
is	O
freely	O
available	O
from	O
pmtk3.googlecode.com	O
(	O
the	O
digit	O
3	O
refers	O
to	O
the	O
third	O
edition	O
of	O
the	O
toolkit	O
,	O
which	O
is	O
the	O
one	O
used	O
in	O
this	O
version	O
of	O
the	O
book	O
)	O
.	O
there	O
are	O
also	O
a	O
variety	O
of	O
supporting	O
ﬁles	O
,	O
written	O
by	O
other	O
people	O
,	O
available	O
at	O
pmtksupport.googlecode.com	O
.	O
these	O
will	O
be	O
downloaded	O
automatically	O
,	O
if	O
you	O
follow	O
the	O
setup	O
instructions	O
described	O
on	O
the	O
pmtk	O
website	O
.	O
matlab	O
is	O
a	O
high-level	O
,	O
interactive	O
scripting	O
language	O
ideally	O
suited	O
to	O
numerical	O
computation	O
and	O
data	O
visualization	O
,	O
and	O
can	O
be	O
purchased	O
from	O
www.mathworks.com	O
.	O
some	O
of	O
the	O
code	O
requires	O
the	O
statistics	O
toolbox	O
,	O
which	O
needs	O
to	O
be	O
purchased	O
separately	O
.	O
there	O
is	O
also	O
a	O
free	O
version	O
of	O
matlab	O
called	O
octave	O
,	O
available	O
at	O
http	O
:	O
//www.gnu.org/software/octave/	O
,	O
which	O
supports	O
most	O
of	O
the	O
functionality	O
of	O
matlab	O
.	O
some	O
(	O
but	O
not	O
all	O
)	O
of	O
the	O
code	O
in	O
this	O
book	O
also	O
works	O
in	O
octave	O
.	O
see	O
the	O
pmtk	O
website	O
for	O
details	O
.	O
pmtk	O
was	O
used	O
to	O
generate	O
many	O
of	O
the	O
ﬁgures	O
in	O
this	O
book	O
;	O
the	O
source	O
code	O
for	O
these	O
ﬁgures	O
is	O
included	O
on	O
the	O
pmtk	O
website	O
,	O
allowing	O
the	O
reader	O
to	O
easily	O
see	O
the	O
effects	O
of	O
changing	O
the	O
data	O
or	O
algorithm	O
or	O
parameter	B
settings	O
.	O
the	O
book	O
refers	O
to	O
ﬁles	O
by	O
name	O
,	O
e.g.	O
,	O
naivebayesfit	O
.	O
in	O
order	O
to	O
ﬁnd	O
the	O
corresponding	O
ﬁle	O
,	O
you	O
can	O
use	O
two	O
methods	O
:	O
within	O
matlab	O
you	O
can	O
type	O
which	O
naivebayesfit	O
and	O
it	O
will	O
return	O
the	O
full	B
path	O
to	O
the	O
ﬁle	O
;	O
or	O
,	O
if	O
you	O
do	O
not	O
have	O
matlab	O
but	O
want	O
to	O
read	O
the	O
source	O
code	O
anyway	O
,	O
you	O
can	O
use	O
your	O
favorite	O
search	O
engine	O
,	O
which	O
should	O
return	O
the	O
corresponding	O
ﬁle	O
from	O
the	O
pmtk3.googlecode.com	O
website	O
.	O
details	O
on	O
how	O
to	O
use	O
pmtk	O
can	O
be	O
found	O
on	O
the	O
website	O
,	O
which	O
will	O
be	O
udpated	O
over	O
time	O
.	O
details	O
on	O
the	O
underlying	O
theory	O
behind	O
these	O
methods	O
can	O
be	O
found	O
in	O
this	O
book	O
.	O
acknowledgments	O
a	O
book	O
this	O
large	O
is	O
obviously	O
a	O
team	O
effort	O
.	O
i	O
would	O
especially	O
like	O
to	O
thank	O
the	O
following	O
people	O
:	O
my	O
wife	O
margaret	O
,	O
for	O
keeping	O
the	O
home	O
ﬁres	O
burning	O
as	O
i	O
toiled	O
away	O
in	O
my	O
office	O
for	O
the	O
last	O
six	O
years	O
;	O
matt	O
dunham	O
,	O
who	O
created	O
many	O
of	O
the	O
ﬁgures	O
in	O
this	O
book	O
,	O
and	O
who	O
wrote	O
much	O
of	O
the	O
code	O
in	O
pmtk	O
;	O
baback	O
moghaddam	O
,	O
who	O
gave	O
extremely	O
detailed	O
feedback	O
on	O
every	O
page	O
of	O
an	O
earlier	O
draft	O
of	O
the	O
book	O
;	O
chris	O
williams	O
,	O
who	O
also	O
gave	O
very	O
detailed	O
feedback	O
;	O
cody	O
severinski	O
and	O
wei-lwun	O
lu	O
,	O
who	O
assisted	O
with	O
ﬁgures	O
;	O
generations	O
of	O
ubc	O
students	O
,	O
who	O
gave	O
helpful	O
comments	O
on	O
earlier	O
drafts	O
;	O
daphne	O
koller	O
,	O
nir	O
friedman	O
,	O
and	O
chris	O
manning	O
,	O
for	O
letting	O
me	O
use	O
their	O
latex	O
style	O
ﬁles	O
;	O
stanford	O
university	O
,	O
google	O
research	O
and	O
skyline	O
college	O
for	O
hosting	O
me	O
during	O
part	O
of	O
my	O
sabbatical	O
;	O
and	O
various	O
canadian	O
funding	O
agencies	O
(	O
nserc	O
,	O
crc	O
and	O
cifar	O
)	O
who	O
have	O
supported	O
me	O
ﬁnancially	O
over	O
the	O
years	O
.	O
in	O
addition	O
,	O
i	O
would	O
like	O
to	O
thank	O
the	O
following	O
people	O
for	O
giving	O
me	O
helpful	O
feedback	O
on	O
preface	O
xxix	O
parts	O
of	O
the	O
book	O
,	O
and/or	O
for	O
sharing	O
ﬁgures	O
,	O
code	O
,	O
exercises	O
or	O
even	O
(	O
in	O
some	O
cases	O
)	O
text	O
:	O
david	O
blei	O
,	O
hannes	O
bretschneider	O
,	O
greg	O
corrado	O
,	O
arnaud	O
doucet	O
,	O
mario	O
figueiredo	O
,	O
nando	O
de	O
freitas	O
,	O
mark	O
girolami	O
,	O
gabriel	O
goh	O
,	O
tom	O
griffiths	O
,	O
katherine	O
heller	O
,	O
geoff	O
hinton	O
,	O
aapo	O
hyvarinen	O
,	O
tommi	O
jaakkola	O
,	O
mike	O
jordan	O
,	O
charles	O
kemp	O
,	O
emtiyaz	O
khan	O
,	O
bonnie	O
kirkpatrick	O
,	O
daphne	O
koller	O
,	O
zico	O
kolter	O
,	O
honglak	O
lee	O
,	O
julien	O
mairal	O
,	O
andrew	O
mcpherson	O
,	O
tom	O
minka	O
,	O
ian	O
nabney	O
,	O
arthur	O
pope	O
,	O
carl	O
rassmussen	O
,	O
ryan	O
rifkin	O
,	O
ruslan	O
salakhutdinov	O
,	O
mark	O
schmidt	O
,	O
daniel	O
selsam	O
,	O
david	O
sontag	O
,	O
erik	O
sudderth	O
,	O
josh	O
tenenbaum	O
,	O
kai	O
yu	O
,	O
martin	O
wainwright	O
,	O
yair	O
weiss	O
.	O
kevin	O
patrick	O
murphy	O
palo	O
alto	O
,	O
california	O
june	O
2012	O
1	O
introduction	O
1.1	O
machine	B
learning	I
:	O
what	O
and	O
why	O
?	O
we	O
are	O
drowning	O
in	O
information	B
and	O
starving	O
for	O
knowledge	O
.	O
—	O
john	O
naisbitt	O
.	O
we	O
are	O
entering	O
the	O
era	O
of	O
big	B
data	I
.	O
for	O
example	O
,	O
there	O
are	O
about	O
1	O
trillion	O
web	O
pages1	O
;	O
one	O
hour	O
of	O
video	O
is	O
uploaded	O
to	O
youtube	O
every	O
second	O
,	O
amounting	O
to	O
10	O
years	O
of	O
content	O
every	O
day2	O
;	O
the	O
genomes	O
of	O
1000s	O
of	O
people	O
,	O
each	O
of	O
which	O
has	O
a	O
length	O
of	O
3.8	O
×	O
109	O
base	O
pairs	O
,	O
have	O
been	O
sequenced	O
by	O
various	O
labs	O
;	O
walmart	O
handles	O
more	O
than	O
1m	O
transactions	O
per	O
hour	O
and	O
has	O
databases	O
containing	O
more	O
than	O
2.5	O
petabytes	O
(	O
2.5	O
×	O
1015	O
)	O
of	O
information	B
(	O
cukier	O
2010	O
)	O
;	O
and	O
so	O
on	O
.	O
this	O
deluge	O
of	O
data	O
calls	O
for	O
automated	O
methods	O
of	O
data	O
analysis	O
,	O
which	O
is	O
what	O
machine	B
learning	I
provides	O
.	O
in	O
particular	O
,	O
we	O
deﬁne	O
machine	B
learning	I
as	O
a	O
set	O
of	O
methods	O
that	O
can	O
automatically	O
detect	O
patterns	O
in	O
data	O
,	O
and	O
then	O
use	O
the	O
uncovered	O
patterns	O
to	O
predict	O
future	O
data	O
,	O
or	O
to	O
perform	O
other	O
kinds	O
of	O
decision	B
making	O
under	O
uncertainty	B
(	O
such	O
as	O
planning	O
how	O
to	O
collect	O
more	O
data	O
!	O
)	O
.	O
this	O
books	O
adopts	O
the	O
view	O
that	O
the	O
best	O
way	O
to	O
solve	O
such	O
problems	O
is	O
to	O
use	O
the	O
tools	O
of	O
probability	O
theory	O
.	O
probability	O
theory	O
can	O
be	O
applied	O
to	O
any	O
problem	O
involving	O
uncertainty	B
.	O
in	O
machine	B
learning	I
,	O
uncertainty	B
comes	O
in	O
many	O
forms	O
:	O
what	O
is	O
the	O
best	O
prediction	O
about	O
the	O
future	O
given	O
some	O
past	O
data	O
?	O
what	O
is	O
the	O
best	O
model	O
to	O
explain	O
some	O
data	O
?	O
what	O
measurement	O
should	O
i	O
perform	O
next	O
?	O
etc	O
.	O
the	O
probabilistic	O
approach	O
to	O
machine	B
learning	I
is	O
closely	O
related	O
to	O
the	O
ﬁeld	O
of	O
statistics	O
,	O
but	O
differs	O
slightly	O
in	O
terms	O
of	O
its	O
emphasis	O
and	O
terminology3	O
.	O
we	O
will	O
describe	O
a	O
wide	O
variety	O
of	O
probabilistic	O
models	O
,	O
suitable	O
for	O
a	O
wide	O
variety	O
of	O
data	O
and	O
tasks	O
.	O
we	O
will	O
also	O
describe	O
a	O
wide	O
variety	O
of	O
algorithms	O
for	O
learning	B
and	O
using	O
such	O
models	O
.	O
the	O
goal	O
is	O
not	O
to	O
develop	O
a	O
cook	O
book	O
of	O
ad	O
hoc	O
techiques	O
,	O
but	O
instead	O
to	O
present	O
a	O
uniﬁed	O
view	O
of	O
the	O
ﬁeld	O
through	O
the	O
lens	O
of	O
probabilistic	O
modeling	O
and	O
inference	B
.	O
although	O
we	O
will	O
pay	O
attention	O
to	O
computational	O
efficiency	O
,	O
details	O
on	O
how	O
to	O
scale	O
these	O
methods	O
to	O
truly	O
massive	O
datasets	O
are	O
better	O
described	O
in	O
other	O
books	O
,	O
such	O
as	O
(	O
rajaraman	O
and	O
ullman	O
2011	O
;	O
bekkerman	O
et	O
al	O
.	O
2011	O
)	O
.	O
1.	O
http	O
:	O
//googleblog.blogspot.com/2008/07/we-knew-web-was-big.html	O
2.	O
source	O
:	O
http	O
:	O
//www.youtube.com/t/press_statistics	O
.	O
3.	O
rob	O
tibshirani	O
,	O
a	O
statistician	O
at	O
stanford	O
university	O
,	O
has	O
created	O
an	O
amusing	O
comparison	O
between	O
machine	B
learning	I
and	O
statistics	O
,	O
available	O
at	O
http	O
:	O
//www-stat.stanford.edu/~tibs/stat315a/glossary.pdf	O
.	O
2	O
chapter	O
1.	O
introduction	O
it	O
should	O
be	O
noted	O
,	O
however	O
,	O
that	O
even	O
when	O
one	O
has	O
an	O
apparently	O
massive	O
data	O
set	O
,	O
the	O
effective	O
number	O
of	O
data	O
points	O
for	O
certain	O
cases	O
of	O
interest	O
might	O
be	O
quite	O
small	O
.	O
in	O
fact	O
,	O
data	O
across	O
a	O
variety	O
of	O
domains	O
exhibits	O
a	O
property	O
known	O
as	O
the	O
long	B
tail	I
,	O
which	O
means	O
that	O
a	O
few	O
things	O
(	O
e.g.	O
,	O
words	O
)	O
are	O
very	O
common	O
,	O
but	O
most	O
things	O
are	O
quite	O
rare	O
(	O
see	O
section	O
2.4.6	O
for	O
details	O
)	O
.	O
for	O
example	O
,	O
20	O
%	O
of	O
google	O
searches	O
each	O
day	O
have	O
never	O
been	O
seen	O
before4	O
.	O
this	O
means	O
that	O
the	O
core	O
statistical	O
issues	O
that	O
we	O
discuss	O
in	O
this	O
book	O
,	O
concerning	O
generalizing	O
from	O
relatively	O
small	O
samples	O
sizes	O
,	O
are	O
still	O
very	O
relevant	O
even	O
in	O
the	O
big	B
data	I
era	O
.	O
1.1.1	O
types	O
of	O
machine	B
learning	I
in	O
the	O
predictive	B
or	O
supervised	O
machine	O
learning	B
is	O
usually	O
divided	O
into	O
two	O
main	O
types	O
.	O
learning	B
approach	O
,	O
the	O
goal	O
is	O
to	O
learn	O
a	O
mapping	O
from	O
inputs	O
x	O
to	O
outputs	O
y	O
,	O
given	O
a	O
labeled	O
set	O
of	O
input-output	O
pairs	O
d	O
=	O
{	O
(	O
xi	O
,	O
yi	O
)	O
}	O
n	O
i=1	O
.	O
here	O
d	O
is	O
called	O
the	O
training	B
set	I
,	O
and	O
n	O
is	O
the	O
number	O
of	O
training	O
examples	O
.	O
in	O
the	O
simplest	O
setting	O
,	O
each	O
training	O
input	O
xi	O
is	O
a	O
d-dimensional	O
vector	O
of	O
numbers	O
,	O
rep-	O
resenting	O
,	O
say	O
,	O
the	O
height	O
and	O
weight	O
of	O
a	O
person	O
.	O
these	O
are	O
called	O
features	B
,	O
attributes	B
or	O
covariates	B
.	O
in	O
general	O
,	O
however	O
,	O
xi	O
could	O
be	O
a	O
complex	O
structured	O
object	O
,	O
such	O
as	O
an	O
image	O
,	O
a	O
sentence	O
,	O
an	O
email	O
message	O
,	O
a	O
time	O
series	O
,	O
a	O
molecular	O
shape	O
,	O
a	O
graph	B
,	O
etc	O
.	O
similarly	O
the	O
form	O
of	O
the	O
output	O
or	O
response	B
variable	I
can	O
in	O
principle	O
be	O
anything	O
,	O
but	O
most	O
methods	O
assume	O
that	O
yi	O
is	O
a	O
categorical	B
or	O
nominal	B
variable	O
from	O
some	O
ﬁnite	O
set	O
,	O
yi	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
}	O
(	O
such	O
as	O
male	O
or	O
female	O
)	O
,	O
or	O
that	O
yi	O
is	O
a	O
real-valued	O
scalar	O
(	O
such	O
as	O
income	O
level	O
)	O
.	O
when	O
yi	O
is	O
categorical	B
,	O
the	O
problem	O
is	O
known	O
as	O
classiﬁcation	B
or	O
pattern	B
recognition	I
,	O
and	O
when	O
yi	O
is	O
real-valued	O
,	O
the	O
problem	O
is	O
known	O
as	O
regression	B
.	O
another	O
variant	O
,	O
known	O
as	O
ordinal	B
regression	I
,	O
occurs	O
where	O
label	B
space	O
y	O
has	O
some	O
natural	O
ordering	O
,	O
such	O
as	O
grades	O
a–f	O
.	O
the	O
second	O
main	O
type	O
of	O
machine	B
learning	I
is	O
the	O
descriptive	B
or	O
unsupervised	B
learning	I
approach	O
.	O
here	O
we	O
are	O
only	O
given	O
inputs	O
,	O
d	O
=	O
{	O
xi	O
}	O
n	O
i=1	O
,	O
and	O
the	O
goal	O
is	O
to	O
ﬁnd	O
“	O
interesting	O
patterns	O
”	O
in	O
the	O
data	O
.	O
this	O
is	O
sometimes	O
called	O
knowledge	B
discovery	I
.	O
this	O
is	O
a	O
much	O
less	O
well-deﬁned	O
problem	O
,	O
since	O
we	O
are	O
not	O
told	O
what	O
kinds	O
of	O
patterns	O
to	O
look	O
for	O
,	O
and	O
there	O
is	O
no	O
obvious	O
error	O
metric	O
to	O
use	O
(	O
unlike	O
supervised	B
learning	I
,	O
where	O
we	O
can	O
compare	O
our	O
prediction	O
of	O
y	O
for	O
a	O
given	O
x	O
to	O
the	O
observed	O
value	O
)	O
.	O
there	O
is	O
a	O
third	O
type	O
of	O
machine	B
learning	I
,	O
known	O
as	O
reinforcement	B
learning	I
,	O
which	O
is	O
somewhat	O
less	O
commonly	O
used	O
.	O
this	O
is	O
useful	O
for	O
learning	B
how	O
to	O
act	O
or	O
behave	O
when	O
given	O
occasional	O
reward	B
or	O
punishment	O
signals	O
.	O
(	O
for	O
example	O
,	O
consider	O
how	O
a	O
baby	O
learns	O
to	O
walk	O
.	O
)	O
unfortunately	O
,	O
rl	O
is	O
beyond	O
the	O
scope	B
of	O
this	O
book	O
,	O
although	O
we	O
do	O
discuss	O
decision	B
theory	O
in	O
section	O
5.7	O
,	O
which	O
is	O
the	O
basis	O
of	O
rl	O
.	O
see	O
e.g.	O
,	O
(	O
kaelbling	O
et	O
al	O
.	O
1996	O
;	O
sutton	O
and	O
barto	O
1998	O
;	O
russell	O
and	O
norvig	O
2010	O
;	O
szepesvari	O
2010	O
;	O
wiering	O
and	O
van	O
otterlo	O
2012	O
)	O
for	O
more	O
information	B
on	O
rl	O
.	O
4.	O
http	O
:	O
//certifiedknowledge.org/blog/are-search-queries-becoming-even-more-unique-statistic	O
s-from-google	O
.	O
1.2.	O
supervised	B
learning	I
3	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
1.1	O
left	O
:	O
some	O
labeled	O
training	O
examples	O
of	O
colored	O
shapes	O
,	O
along	O
with	O
3	O
unlabeled	O
test	O
cases	O
.	O
right	O
:	O
representing	O
the	O
training	O
data	O
as	O
an	O
n	O
×	O
d	O
design	B
matrix	I
.	O
row	O
i	O
represents	O
the	O
feature	O
vector	O
xi	O
.	O
the	O
last	O
column	O
is	O
the	O
label	B
,	O
yi	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
based	O
on	O
a	O
ﬁgure	O
by	O
leslie	O
kaelbling	O
.	O
1.2	O
supervised	B
learning	I
we	O
begin	O
our	O
investigation	O
of	O
machine	B
learning	I
by	O
discussing	O
supervised	B
learning	I
,	O
which	O
is	O
the	O
form	O
of	O
ml	O
most	O
widely	O
used	O
in	O
practice	O
.	O
1.2.1	O
classiﬁcation	B
in	O
this	O
section	O
,	O
we	O
discuss	O
classiﬁcation	B
.	O
here	O
the	O
goal	O
is	O
to	O
learn	O
a	O
mapping	O
from	O
inputs	O
x	O
to	O
outputs	O
y	O
,	O
where	O
y	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
}	O
,	O
with	O
c	O
being	O
the	O
number	O
of	O
classes	O
.	O
if	O
c	O
=	O
2	O
,	O
this	O
is	O
called	O
binary	B
classiﬁcation	I
(	O
in	O
which	O
case	O
we	O
often	O
assume	O
y	O
∈	O
{	O
0	O
,	O
1	O
}	O
)	O
;	O
if	O
c	O
>	O
2	O
,	O
this	O
is	O
called	O
multiclass	B
classiﬁcation	I
.	O
if	O
the	O
class	O
labels	O
are	O
not	O
mutually	O
exclusive	O
(	O
e.g.	O
,	O
somebody	O
may	O
be	O
classiﬁed	O
as	O
tall	O
and	O
strong	O
)	O
,	O
we	O
call	O
it	O
multi-label	B
classiﬁcation	I
,	O
but	O
this	O
is	O
best	O
viewed	O
as	O
predicting	O
multiple	O
related	O
binary	O
class	O
labels	O
(	O
a	O
so-called	O
multiple	B
output	I
model	I
)	O
.	O
when	O
we	O
use	O
the	O
term	O
“	O
classiﬁcation	B
”	O
,	O
we	O
will	O
mean	B
multiclass	O
classiﬁcation	B
with	O
a	O
single	O
output	O
,	O
unless	O
we	O
state	B
otherwise	O
.	O
one	O
way	O
to	O
formalize	O
the	O
problem	O
is	O
as	O
function	B
approximation	I
.	O
we	O
assume	O
y	O
=	O
f	O
(	O
x	O
)	O
for	O
some	O
unknown	B
function	O
f	O
,	O
and	O
the	O
goal	O
of	O
learning	B
is	O
to	O
estimate	O
the	O
function	O
f	O
given	O
a	O
labeled	O
training	O
set	O
,	O
and	O
then	O
to	O
make	O
predictions	O
using	O
ˆy	O
=	O
ˆf	O
(	O
x	O
)	O
.	O
(	O
we	O
use	O
the	O
hat	O
symbol	O
to	O
denote	O
an	O
estimate	O
.	O
)	O
our	O
main	O
goal	O
is	O
to	O
make	O
predictions	O
on	O
novel	O
inputs	O
,	O
meaning	O
ones	O
that	O
we	O
have	O
not	O
seen	O
before	O
(	O
this	O
is	O
called	O
generalization	B
)	O
,	O
since	O
predicting	O
the	O
response	O
on	O
the	O
training	B
set	I
is	O
easy	O
(	O
we	O
can	O
just	O
look	O
up	O
the	O
answer	O
)	O
.	O
1.2.1.1	O
example	O
as	O
a	O
simple	O
toy	O
example	O
of	O
classiﬁcation	B
,	O
consider	O
the	O
problem	O
illustrated	O
in	O
figure	O
1.1	O
(	O
a	O
)	O
.	O
we	O
have	O
two	O
classes	O
of	O
object	O
which	O
correspond	O
to	O
labels	O
0	O
and	O
1.	O
the	O
inputs	O
are	O
colored	O
shapes	O
.	O
these	O
have	O
been	O
described	O
by	O
a	O
set	O
of	O
d	O
features	B
or	O
attributes	B
,	O
which	O
are	O
stored	O
in	O
an	O
n	O
×	O
d	O
design	B
matrix	I
x	O
,	O
shown	O
in	O
figure	O
1.1	O
(	O
b	O
)	O
.	O
the	O
input	O
features	B
x	O
can	O
be	O
discrete	B
,	O
continuous	O
or	O
a	O
combination	O
of	O
the	O
two	O
.	O
in	O
addition	O
to	O
the	O
inputs	O
,	O
we	O
have	O
a	O
vector	O
of	O
training	O
labels	O
y.	O
in	O
figure	O
1.1	O
,	O
the	O
test	O
cases	O
are	O
a	O
blue	O
crescent	O
,	O
a	O
yellow	O
circle	O
and	O
a	O
blue	O
arrow	O
.	O
none	O
of	O
these	O
have	O
been	O
seen	O
before	O
.	O
thus	O
we	O
are	O
required	O
to	O
generalize	B
beyond	O
the	O
training	B
set	I
.	O
a	O
4	O
chapter	O
1.	O
introduction	O
reasonable	O
guess	O
is	O
that	O
blue	O
crescent	O
should	O
be	O
y	O
=	O
1	O
,	O
since	O
all	O
blue	O
shapes	O
are	O
labeled	O
1	O
in	O
the	O
training	B
set	I
.	O
the	O
yellow	O
circle	O
is	O
harder	O
to	O
classify	O
,	O
since	O
some	O
yellow	O
things	O
are	O
labeled	O
y	O
=	O
1	O
and	O
some	O
are	O
labeled	O
y	O
=	O
0	O
,	O
and	O
some	O
circles	O
are	O
labeled	O
y	O
=	O
1	O
and	O
some	O
y	O
=	O
0.	O
consequently	O
it	O
is	O
not	O
clear	O
what	O
the	O
right	O
label	O
should	O
be	O
in	O
the	O
case	O
of	O
the	O
yellow	O
circle	O
.	O
similarly	O
,	O
the	O
correct	O
label	B
for	O
the	O
blue	O
arrow	O
is	O
unclear	O
.	O
1.2.1.2	O
the	O
need	O
for	O
probabilistic	O
predictions	O
to	O
handle	O
ambiguous	O
cases	O
,	O
such	O
as	O
the	O
yellow	O
circle	O
above	O
,	O
it	O
is	O
desirable	O
to	O
return	O
a	O
probability	O
.	O
the	O
reader	O
is	O
assumed	O
to	O
already	O
have	O
some	O
familiarity	O
with	O
basic	O
concepts	O
in	O
probability	O
.	O
if	O
not	O
,	O
please	O
consult	O
chapter	O
2	O
for	O
a	O
refresher	O
,	O
if	O
necessary	O
.	O
we	O
will	O
denote	O
the	O
probability	O
distribution	O
over	O
possible	O
labels	O
,	O
given	O
the	O
input	O
vector	O
x	O
and	O
training	B
set	I
d	O
by	O
p	O
(	O
y|x	O
,	O
d	O
)	O
.	O
in	O
general	O
,	O
this	O
represents	O
a	O
vector	O
of	O
length	O
c.	O
(	O
if	O
there	O
are	O
just	O
two	O
classes	O
,	O
it	O
is	O
sufficient	O
to	O
return	O
the	O
single	O
number	O
p	O
(	O
y	O
=	O
1|x	O
,	O
d	O
)	O
,	O
since	O
p	O
(	O
y	O
=	O
1|x	O
,	O
d	O
)	O
+	O
p	O
(	O
y	O
=	O
0|x	O
,	O
d	O
)	O
=	O
1	O
.	O
)	O
in	O
our	O
notation	O
,	O
we	O
make	O
explicit	O
that	O
the	O
probability	O
is	O
conditional	O
on	O
the	O
test	O
input	O
x	O
,	O
as	O
well	O
as	O
the	O
training	B
set	I
d	O
,	O
by	O
putting	O
these	O
terms	O
on	O
the	O
right	O
hand	O
side	O
of	O
the	O
conditioning	B
bar	O
|	O
.	O
we	O
are	O
also	O
implicitly	O
conditioning	B
on	O
the	O
form	O
of	O
model	O
that	O
we	O
use	O
to	O
make	O
predictions	O
.	O
when	O
choosing	O
between	O
different	O
models	O
,	O
we	O
will	O
make	O
this	O
assumption	O
explicit	O
by	O
writing	O
p	O
(	O
y|x	O
,	O
d	O
,	O
m	O
)	O
,	O
where	O
m	O
denotes	O
the	O
model	O
.	O
however	O
,	O
if	O
the	O
model	O
is	O
clear	O
from	O
context	O
,	O
we	O
will	O
drop	O
m	O
from	O
our	O
notation	O
for	O
brevity	O
.	O
given	O
a	O
probabilistic	O
output	O
,	O
we	O
can	O
always	O
compute	O
our	O
“	O
best	O
guess	O
”	O
as	O
to	O
the	O
“	O
true	O
label	O
”	O
using	O
ˆy	O
=	O
ˆf	O
(	O
x	O
)	O
=	O
c	O
argmax	O
c=1	O
p	O
(	O
y	O
=	O
c|x	O
,	O
d	O
)	O
(	O
1.1	O
)	O
this	O
corresponds	O
to	O
the	O
most	O
probable	O
class	O
label	O
,	O
and	O
is	O
called	O
the	O
mode	B
of	O
the	O
distribution	O
p	O
(	O
y|x	O
,	O
d	O
)	O
;	O
it	O
is	O
also	O
known	O
as	O
a	O
map	O
estimate	O
(	O
map	O
stands	O
for	O
maximum	B
a	I
posteriori	I
)	O
.	O
using	O
the	O
most	O
probable	O
label	B
makes	O
intuitive	O
sense	O
,	O
but	O
we	O
will	O
give	O
a	O
more	O
formal	O
justiﬁcation	O
for	O
this	O
procedure	O
in	O
section	O
5.7.	O
now	O
consider	O
a	O
case	O
such	O
as	O
the	O
yellow	O
circle	O
,	O
where	O
p	O
(	O
ˆy|x	O
,	O
d	O
)	O
is	O
far	O
from	O
1.0.	O
in	O
such	O
a	O
case	O
we	O
are	O
not	O
very	O
conﬁdent	O
of	O
our	O
answer	O
,	O
so	O
it	O
might	O
be	O
better	O
to	O
say	O
“	O
i	O
don	O
’	O
t	O
know	O
”	O
instead	O
of	O
returning	O
an	O
answer	O
that	O
we	O
don	O
’	O
t	O
really	O
trust	O
.	O
this	O
is	O
particularly	O
important	O
in	O
domains	O
such	O
as	O
medicine	O
and	O
ﬁnance	O
where	O
we	O
may	O
be	O
risk	B
averse	I
,	O
as	O
we	O
explain	O
in	O
section	O
5.7.	O
another	O
application	O
where	O
it	O
is	O
important	O
to	O
assess	O
risk	B
is	O
when	O
playing	O
tv	O
game	O
shows	O
,	O
such	O
as	O
jeopardy	O
.	O
in	O
this	O
game	O
,	O
contestants	O
have	O
to	O
solve	O
various	O
word	O
puzzles	O
and	O
answer	O
a	O
variety	O
of	O
trivia	O
questions	O
,	O
but	O
if	O
they	O
answer	O
incorrectly	O
,	O
they	O
lose	O
money	O
.	O
in	O
2011	O
,	O
ibm	O
unveiled	O
a	O
computer	O
system	O
called	O
watson	O
which	O
beat	O
the	O
top	O
human	O
jeopardy	O
champion	O
.	O
watson	O
uses	O
a	O
variety	O
of	O
interesting	O
techniques	O
(	O
ferrucci	O
et	O
al	O
.	O
2010	O
)	O
,	O
but	O
the	O
most	O
pertinent	O
one	O
for	O
our	O
present	O
purposes	O
is	O
that	O
it	O
contains	O
a	O
module	O
that	O
estimates	O
how	O
conﬁdent	O
it	O
is	O
of	O
its	O
answer	O
.	O
the	O
system	O
only	O
chooses	O
to	O
“	O
buzz	O
in	O
”	O
its	O
answer	O
if	O
sufficiently	O
conﬁdent	O
it	O
is	O
correct	O
.	O
similarly	O
,	O
google	O
has	O
a	O
system	O
known	O
as	O
smartass	O
(	O
ad	O
selection	O
system	O
)	O
that	O
predicts	O
the	O
probability	O
you	O
will	O
click	O
on	O
an	O
ad	O
based	O
on	O
your	O
search	O
history	O
and	O
other	O
user	O
and	O
ad-speciﬁc	O
features	B
(	O
metz	O
2010	O
)	O
.	O
this	O
probability	O
is	O
known	O
as	O
the	O
click-through	B
rate	I
or	O
ctr	O
,	O
and	O
can	O
be	O
used	O
to	O
maximize	O
expected	B
proﬁt	I
.	O
we	O
will	O
discuss	O
some	O
of	O
the	O
basic	O
principles	O
behind	O
systems	O
such	O
as	O
smartass	O
later	O
in	O
this	O
book	O
.	O
1.2.	O
supervised	B
learning	I
5	O
s	O
t	O
n	O
e	O
m	O
u	O
c	O
o	O
d	O
100	O
200	O
300	O
400	O
500	O
600	O
700	O
800	O
900	O
1000	O
10	O
20	O
30	O
40	O
50	O
words	O
60	O
70	O
80	O
90	O
100	O
figure	O
1.2	O
subset	O
of	O
size	O
16242	O
x	O
100	O
of	O
the	O
20-newsgroups	O
data	O
.	O
we	O
only	O
show	O
1000	O
rows	O
,	O
for	O
clarity	O
.	O
each	O
row	O
is	O
a	O
document	O
(	O
represented	O
as	O
a	O
bag-of-words	B
bit	O
vector	O
)	O
,	O
each	O
column	O
is	O
a	O
word	O
.	O
the	O
red	O
lines	O
separate	O
the	O
4	O
classes	O
,	O
which	O
are	O
(	O
in	O
descending	O
order	O
)	O
comp	O
,	O
rec	O
,	O
sci	O
,	O
talk	O
(	O
these	O
are	O
the	O
titles	O
of	O
usenet	O
groups	O
)	O
.	O
we	O
can	O
see	O
that	O
there	O
are	O
subsets	O
of	O
words	O
whose	O
presence	O
or	O
absence	O
is	O
indicative	O
of	O
the	O
class	O
.	O
the	O
data	O
is	O
available	O
from	O
http	O
:	O
//cs.nyu.edu/~roweis/data.html	O
.	O
figure	O
generated	O
by	O
newsgroupsvisualize	O
.	O
1.2.1.3	O
real-world	O
applications	O
classiﬁcation	B
is	O
probably	O
the	O
most	O
widely	O
used	O
form	O
of	O
machine	B
learning	I
,	O
and	O
has	O
been	O
used	O
to	O
solve	O
many	O
interesting	O
and	O
often	O
difficult	O
real-world	O
problems	O
.	O
we	O
have	O
already	O
mentioned	O
some	O
important	O
applciations	O
.	O
we	O
give	O
a	O
few	O
more	O
examples	O
below	O
.	O
document	B
classiﬁcation	I
and	O
email	B
spam	I
ﬁltering	I
in	O
document	B
classiﬁcation	I
,	O
the	O
goal	O
is	O
to	O
classify	O
a	O
document	O
,	O
such	O
as	O
a	O
web	O
page	O
or	O
email	O
message	O
,	O
into	O
one	O
of	O
c	O
classes	O
,	O
that	O
is	O
,	O
to	O
compute	O
p	O
(	O
y	O
=	O
c|x	O
,	O
d	O
)	O
,	O
where	O
x	O
is	O
some	O
represen-	O
tation	O
of	O
the	O
text	O
.	O
a	O
special	O
case	O
of	O
this	O
is	O
email	B
spam	I
ﬁltering	I
,	O
where	O
the	O
classes	O
are	O
spam	B
y	O
=	O
1	O
or	O
ham	B
y	O
=	O
0.	O
most	O
classiﬁers	O
assume	O
that	O
the	O
input	O
vector	O
x	O
has	O
a	O
ﬁxed	O
size	O
.	O
a	O
common	O
way	O
to	O
represent	O
variable-length	O
documents	O
in	O
feature-vector	O
format	O
is	O
to	O
use	O
a	O
bag	B
of	I
words	I
representation	O
.	O
this	O
is	O
explained	O
in	O
detail	O
in	O
section	O
3.4.4.1	O
,	O
but	O
the	O
basic	O
idea	O
is	O
to	O
deﬁne	O
xij	O
=	O
1	O
iff	B
word	O
j	O
occurs	O
in	O
document	O
i.	O
if	O
we	O
apply	O
this	O
transformation	O
to	O
every	O
document	O
in	O
our	O
data	O
set	O
,	O
we	O
get	O
a	O
binary	O
document	O
×	O
word	O
co-occurrence	B
matrix	I
:	O
see	O
figure	O
1.2	O
for	O
an	O
example	O
.	O
essentially	O
the	O
document	B
classiﬁcation	I
problem	O
has	O
been	O
reduced	O
to	O
one	O
that	O
looks	O
for	O
subtle	O
changes	O
in	O
the	O
pattern	B
of	O
bits	B
.	O
for	O
example	O
,	O
we	O
may	O
notice	O
that	O
most	O
spam	B
messages	O
have	O
a	O
high	O
probability	O
of	O
containing	O
the	O
words	O
“	O
buy	O
”	O
,	O
“	O
cheap	O
”	O
,	O
“	O
viagra	O
”	O
,	O
etc	O
.	O
in	O
exercise	O
8.1	O
and	O
exercise	O
8.2	O
,	O
you	O
will	O
get	O
hands-on	O
experience	O
applying	O
various	O
classiﬁcation	B
techniques	O
to	O
the	O
spam	B
ﬁltering	O
problem	O
.	O
6	O
chapter	O
1.	O
introduction	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
1.3	O
three	O
types	O
of	O
iris	B
ﬂowers	O
:	O
setosa	O
,	O
versicolor	O
and	O
virginica	O
.	O
source	O
:	O
http	O
:	O
//www.statlab.u	O
ni-heidelberg.de/data/iris/	O
.	O
used	O
with	O
kind	O
permission	O
of	O
dennis	O
kramb	O
and	O
signa	O
.	O
sepal	O
length	O
sepal	O
width	O
petal	O
length	O
petal	O
width	O
h	O
t	O
g	O
n	O
e	O
l	O
l	O
a	O
p	O
e	O
s	O
h	O
t	O
i	O
d	O
w	O
l	O
a	O
p	O
e	O
s	O
h	O
t	O
g	O
n	O
e	O
l	O
l	O
a	O
t	O
e	O
p	O
t	O
h	O
d	O
w	O
i	O
l	O
a	O
t	O
e	O
p	O
figure	O
1.4	O
visualization	O
of	O
the	O
iris	B
data	O
as	O
a	O
pairwise	O
scatter	O
plot	O
.	O
the	O
diagonal	B
plots	O
the	O
marginal	O
histograms	O
of	O
the	O
4	O
features	B
.	O
the	O
off	O
diagonals	O
contain	O
scatterplots	O
of	O
all	O
possible	O
pairs	O
of	O
features	B
.	O
red	O
circle	O
=	O
setosa	O
,	O
green	O
diamond	O
=	O
versicolor	O
,	O
blue	O
star	O
=	O
virginica	O
.	O
figure	O
generated	O
by	O
fisheririsdemo	O
.	O
classifying	O
ﬂowers	O
figure	O
1.3	O
gives	O
another	O
example	O
of	O
classiﬁcation	B
,	O
due	O
to	O
the	O
statistician	O
ronald	O
fisher	O
.	O
the	O
goal	O
is	O
to	O
learn	O
to	O
distinguish	O
three	O
different	O
kinds	O
of	O
iris	B
ﬂower	O
,	O
called	O
setosa	O
,	O
versicolor	O
and	O
virginica	O
.	O
fortunately	O
,	O
rather	O
than	O
working	O
directly	O
with	O
images	O
,	O
a	O
botanist	O
has	O
already	O
extracted	O
4	O
useful	O
(	O
such	O
feature	O
features	O
or	O
characteristics	O
:	O
sepal	O
length	O
and	O
width	O
,	O
and	O
petal	O
length	O
and	O
width	O
.	O
extraction	O
is	O
an	O
important	O
,	O
but	O
difficult	O
,	O
task	O
.	O
most	O
machine	B
learning	I
methods	O
use	O
features	B
chosen	O
by	O
some	O
human	O
.	O
later	O
we	O
will	O
discuss	O
some	O
methods	O
that	O
can	O
learn	O
good	O
features	B
from	O
the	O
data	O
.	O
)	O
if	O
we	O
make	O
a	O
scatter	B
plot	I
of	O
the	O
iris	B
data	O
,	O
as	O
in	O
figure	O
1.4	O
,	O
we	O
see	O
that	O
it	O
is	O
easy	O
to	O
distinguish	O
setosas	O
(	O
red	O
circles	O
)	O
from	O
the	O
other	O
two	O
classes	O
by	O
just	O
checking	O
if	O
their	O
petal	O
length	O
1.2.	O
supervised	B
learning	I
7	O
true	O
class	O
=	O
7	O
true	O
class	O
=	O
2	O
true	O
class	O
=	O
1	O
true	O
class	O
=	O
7	O
true	O
class	O
=	O
2	O
true	O
class	O
=	O
1	O
true	O
class	O
=	O
0	O
true	O
class	O
=	O
4	O
true	O
class	O
=	O
1	O
true	O
class	O
=	O
0	O
true	O
class	O
=	O
4	O
true	O
class	O
=	O
1	O
true	O
class	O
=	O
4	O
true	O
class	O
=	O
9	O
true	O
class	O
=	O
5	O
true	O
class	O
=	O
4	O
true	O
class	O
=	O
9	O
true	O
class	O
=	O
5	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
a	O
)	O
first	O
9	O
test	O
mnist	O
gray-scale	O
images	O
.	O
figure	O
1.5	O
(	O
b	O
)	O
same	O
as	O
(	O
a	O
)	O
,	O
but	O
with	O
the	O
features	B
permuted	O
randomly	O
.	O
classiﬁcation	B
performance	O
is	O
identical	O
on	O
both	O
versions	O
of	O
the	O
data	O
(	O
assuming	O
the	O
training	O
data	O
is	O
permuted	O
in	O
an	O
identical	O
way	O
)	O
.	O
figure	O
generated	O
by	O
shuffleddigitsdemo	O
.	O
or	O
width	O
is	O
below	O
some	O
threshold	O
.	O
however	O
,	O
distinguishing	O
versicolor	O
from	O
virginica	O
is	O
slightly	O
harder	O
;	O
any	O
decision	B
will	O
need	O
to	O
be	O
based	O
on	O
at	O
least	O
two	O
features	B
.	O
(	O
it	O
is	O
always	O
a	O
good	O
idea	O
to	O
perform	O
exploratory	B
data	I
analysis	I
,	O
such	O
as	O
plotting	O
the	O
data	O
,	O
before	O
applying	O
a	O
machine	B
learning	I
method	O
.	O
)	O
image	B
classiﬁcation	I
and	O
handwriting	B
recognition	I
now	O
consider	O
the	O
harder	O
problem	O
of	O
classifying	O
images	O
directly	O
,	O
where	O
a	O
human	O
has	O
not	O
pre-	O
processed	O
the	O
data	O
.	O
we	O
might	O
want	O
to	O
classify	O
the	O
image	O
as	O
a	O
whole	O
,	O
e.g.	O
,	O
is	O
it	O
an	O
indoors	O
or	O
outdoors	O
scene	O
?	O
is	O
it	O
a	O
horizontal	O
or	O
vertical	O
photo	O
?	O
does	O
it	O
contain	O
a	O
dog	O
or	O
not	O
?	O
this	O
is	O
called	O
image	B
classiﬁcation	I
.	O
in	O
the	O
special	O
case	O
that	O
the	O
images	O
consist	O
of	O
isolated	O
handwritten	O
letters	O
and	O
digits	O
,	O
for	O
example	O
,	O
in	O
a	O
postal	O
or	O
zip	O
code	O
on	O
a	O
letter	O
,	O
we	O
can	O
use	O
classiﬁcation	B
to	O
perform	O
handwriting	B
recognition	I
.	O
a	O
standard	O
dataset	O
used	O
in	O
this	O
area	O
is	O
known	O
as	O
mnist	O
,	O
which	O
stands	O
for	O
“	O
modiﬁed	O
national	O
institute	O
of	O
standards	O
”	O
5	O
.	O
(	O
the	O
term	O
“	O
modiﬁed	O
”	O
is	O
used	O
because	O
the	O
images	O
have	O
been	O
preprocessed	O
to	O
ensure	O
the	O
digits	O
are	O
mostly	O
in	O
the	O
center	O
of	O
the	O
image	O
.	O
)	O
this	O
dataset	O
contains	O
60,000	O
training	O
images	O
and	O
10,000	O
test	O
images	O
of	O
the	O
digits	O
0	O
to	O
9	O
,	O
as	O
written	O
by	O
various	O
people	O
.	O
the	O
images	O
are	O
size	O
28×	O
28	O
and	O
have	O
grayscale	O
values	O
in	O
the	O
range	O
0	O
:	O
255.	O
see	O
figure	O
1.5	O
(	O
a	O
)	O
for	O
some	O
example	O
images	O
.	O
many	O
generic	O
classiﬁcation	B
methods	O
ignore	O
any	O
structure	O
in	O
the	O
input	O
features	B
,	O
such	O
as	O
spatial	O
layout	O
.	O
consequently	O
,	O
they	O
can	O
also	O
just	O
as	O
easily	O
handle	O
data	O
that	O
looks	O
like	O
figure	O
1.5	O
(	O
b	O
)	O
,	O
which	O
is	O
the	O
same	O
data	O
except	O
we	O
have	O
randomly	O
permuted	O
the	O
order	O
of	O
all	O
the	O
features	B
.	O
(	O
you	O
will	O
verify	O
this	O
in	O
exercise	O
1.1	O
.	O
)	O
this	O
ﬂexibility	O
is	O
both	O
a	O
blessing	O
(	O
since	O
the	O
methods	O
are	O
general	O
purpose	O
)	O
and	O
a	O
curse	O
(	O
since	O
the	O
methods	O
ignore	O
an	O
obviously	O
useful	O
source	O
of	O
information	B
)	O
.	O
we	O
will	O
discuss	O
methods	O
for	O
exploiting	O
structure	O
in	O
the	O
input	O
features	B
later	O
in	O
the	O
book	O
.	O
5.	O
available	O
from	O
http	O
:	O
//yann.lecun.com/exdb/mnist/	O
.	O
8	O
chapter	O
1.	O
introduction	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
1.6	O
example	O
of	O
face	B
detection	I
.	O
(	O
a	O
)	O
input	O
image	O
(	O
murphy	O
family	B
,	O
photo	O
taken	O
5	O
august	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
bernard	O
diedrich	O
of	O
sherwood	O
studios	O
.	O
(	O
b	O
)	O
output	O
of	O
classiﬁer	O
,	O
which	O
detected	O
5	O
faces	O
at	O
different	O
poses	O
.	O
this	O
was	O
produced	O
using	O
the	O
online	O
demo	O
at	O
http	O
:	O
//demo.pittpatt.com/	O
.	O
the	O
classiﬁer	O
was	O
trained	O
on	O
1000s	O
of	O
manually	O
labeled	O
images	O
of	O
faces	O
and	O
non-faces	O
,	O
and	O
then	O
was	O
applied	O
to	O
a	O
dense	O
set	O
of	O
overlapping	O
patches	O
in	O
the	O
test	O
image	O
.	O
only	O
the	O
patches	O
whose	O
probability	O
of	O
containing	O
a	O
face	O
was	O
sufficiently	O
high	O
were	O
returned	O
.	O
used	O
with	O
kind	O
permission	O
of	O
pittpatt.com	O
face	B
detection	I
and	O
recognition	O
a	O
harder	O
problem	O
is	O
to	O
ﬁnd	O
objects	O
within	O
an	O
image	O
;	O
this	O
is	O
called	O
object	B
detection	I
or	O
object	B
localization	I
.	O
an	O
important	O
special	O
case	O
of	O
this	O
is	O
face	B
detection	I
.	O
one	O
approach	O
to	O
this	O
problem	O
is	O
to	O
divide	O
the	O
image	O
into	O
many	O
small	O
overlapping	O
patches	O
at	O
different	O
locations	O
,	O
scales	O
and	O
orientations	O
,	O
and	O
to	O
classify	O
each	O
such	O
patch	O
based	O
on	O
whether	O
it	O
contains	O
face-like	O
texture	O
or	O
not	O
.	O
this	O
is	O
called	O
a	O
sliding	B
window	I
detector	I
.	O
the	O
system	O
then	O
returns	O
those	O
locations	O
where	O
the	O
probability	O
of	O
face	O
is	O
sufficiently	O
high	O
.	O
see	O
figure	O
1.6	O
for	O
an	O
example	O
.	O
such	O
face	B
detection	I
systems	O
are	O
built-in	O
to	O
most	O
modern	O
digital	B
cameras	I
;	O
the	O
locations	O
of	O
the	O
detected	O
faces	O
are	O
used	O
to	O
determine	O
the	O
center	O
of	O
the	O
auto-focus	O
.	O
another	O
application	O
is	O
automatically	O
blurring	O
out	O
faces	O
in	O
google	O
’	O
s	O
streetview	O
system	O
.	O
having	O
found	O
the	O
faces	O
,	O
one	O
can	O
then	O
proceed	O
to	O
perform	O
face	B
recognition	I
,	O
which	O
means	O
estimating	O
the	O
identity	O
of	O
the	O
person	O
(	O
see	O
figure	O
1.10	O
(	O
a	O
)	O
)	O
.	O
in	O
this	O
case	O
,	O
the	O
number	O
of	O
class	O
labels	O
might	O
be	O
very	O
large	O
.	O
also	O
,	O
the	O
features	B
one	O
should	O
use	O
are	O
likely	O
to	O
be	O
different	O
than	O
in	O
the	O
face	B
detection	I
problem	O
:	O
for	O
recognition	O
,	O
subtle	O
differences	O
between	O
faces	O
such	O
as	O
hairstyle	O
may	O
be	O
important	O
for	O
determining	O
identity	O
,	O
but	O
for	O
detection	O
,	O
it	O
is	O
important	O
to	O
be	O
invariant	B
to	O
such	O
details	O
,	O
and	O
to	O
just	O
focus	O
on	O
the	O
differences	O
between	O
faces	O
and	O
non-faces	O
.	O
for	O
more	O
information	B
about	O
visual	O
object	O
detection	O
,	O
see	O
e.g.	O
,	O
(	O
szeliski	O
2010	O
)	O
.	O
1.2.2	O
regression	B
regression	O
is	O
just	O
like	O
classiﬁcation	B
except	O
the	O
response	B
variable	I
is	O
continuous	O
.	O
figure	O
1.7	O
shows	O
a	O
simple	O
example	O
:	O
we	O
have	O
a	O
single	O
real-valued	O
input	O
xi	O
∈	O
r	O
,	O
and	O
a	O
single	O
real-valued	O
response	O
yi	O
∈	O
r.	O
we	O
consider	O
ﬁtting	O
two	O
models	O
to	O
the	O
data	O
:	O
a	O
straight	O
line	O
and	O
a	O
quadratic	O
function	O
.	O
(	O
we	O
explain	O
how	O
to	O
ﬁt	O
such	O
models	O
below	O
.	O
)	O
various	O
extensions	O
of	O
this	O
basic	O
problem	O
can	O
arise	O
,	O
such	O
as	O
having	O
high-dimensional	O
inputs	O
,	O
outliers	B
,	O
non-smooth	B
responses	O
,	O
etc	O
.	O
we	O
will	O
discuss	O
ways	O
to	O
handle	O
such	O
problems	O
later	O
in	O
the	O
book	O
.	O
1.3.	O
unsupervised	B
learning	I
9	O
degree	B
1	O
degree	B
2	O
15	O
10	O
5	O
0	O
−5	O
15	O
10	O
5	O
0	O
−5	O
−10	O
0	O
5	O
10	O
15	O
20	O
−10	O
0	O
5	O
10	O
15	O
20	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
1.7	O
figure	O
generated	O
by	O
linregpolyvsdegree	O
.	O
(	O
a	O
)	O
linear	B
regression	I
on	O
some	O
1d	O
data	O
.	O
(	O
b	O
)	O
same	O
data	O
with	O
polynomial	B
regression	I
(	O
degree	B
2	O
)	O
.	O
here	O
are	O
some	O
examples	O
of	O
real-world	O
regression	B
problems	O
.	O
•	O
predict	O
tomorrow	O
’	O
s	O
stock	O
market	O
price	O
given	O
current	O
market	O
conditions	O
and	O
other	O
possible	O
side	B
information	I
.	O
•	O
predict	O
the	O
age	O
of	O
a	O
viewer	O
watching	O
a	O
given	O
video	O
on	O
youtube	O
.	O
•	O
predict	O
the	O
location	O
in	O
3d	O
space	O
of	O
a	O
robot	O
arm	O
end	B
effector	I
,	O
given	O
control	O
signals	O
(	O
torques	O
)	O
sent	O
to	O
its	O
various	O
motors	O
.	O
•	O
predict	O
the	O
amount	O
of	O
prostate	O
speciﬁc	O
antigen	O
(	O
psa	O
)	O
in	O
the	O
body	O
as	O
a	O
function	O
of	O
a	O
number	O
of	O
different	O
clinical	O
measurements	O
.	O
•	O
predict	O
the	O
temperature	B
at	O
any	O
location	O
inside	O
a	O
building	O
using	O
weather	O
data	O
,	O
time	O
,	O
door	O
sensors	O
,	O
etc	O
.	O
1.3	O
unsupervised	B
learning	I
we	O
now	O
consider	O
unsupervised	B
learning	I
,	O
where	O
we	O
are	O
just	O
given	O
output	O
data	O
,	O
without	O
any	O
inputs	O
.	O
the	O
goal	O
is	O
to	O
discover	O
“	O
interesting	O
structure	O
”	O
in	O
the	O
data	O
;	O
this	O
is	O
sometimes	O
called	O
knowledge	B
discovery	I
.	O
unlike	O
supervised	B
learning	I
,	O
we	O
are	O
not	O
told	O
what	O
the	O
desired	O
output	O
is	O
instead	O
,	O
we	O
will	O
formalize	O
our	O
task	O
as	O
one	O
of	O
density	B
estimation	I
,	O
that	O
is	O
,	O
we	O
for	O
each	O
input	O
.	O
want	O
to	O
build	O
models	O
of	O
the	O
form	O
p	O
(	O
xi|θ	O
)	O
.	O
there	O
are	O
two	O
differences	O
from	O
the	O
supervised	O
case	O
.	O
first	O
,	O
we	O
have	O
written	O
p	O
(	O
xi|θ	O
)	O
instead	O
of	O
p	O
(	O
yi|xi	O
,	O
θ	O
)	O
;	O
that	O
is	O
,	O
supervised	B
learning	I
is	O
conditional	O
density	O
estimation	O
,	O
whereas	O
unsupervised	B
learning	I
is	O
unconditional	O
density	B
estimation	I
.	O
second	O
,	O
xi	O
is	O
a	O
vector	O
of	O
features	B
,	O
so	O
we	O
need	O
to	O
create	O
multivariate	O
probability	O
models	O
.	O
by	O
contrast	O
,	O
in	O
supervised	B
learning	I
,	O
yi	O
is	O
usually	O
just	O
a	O
single	O
variable	O
that	O
we	O
are	O
trying	O
to	O
predict	O
.	O
this	O
means	O
that	O
for	O
most	O
supervised	B
learning	I
problems	O
,	O
we	O
can	O
use	O
univariate	O
probability	O
models	O
(	O
with	O
input-dependent	O
parameters	O
)	O
,	O
which	O
signiﬁcantly	O
simpliﬁes	O
the	O
problem	O
.	O
(	O
we	O
will	O
discuss	O
multi-output	O
classiﬁcation	B
in	O
chapter	O
19	O
,	O
where	O
we	O
will	O
see	O
that	O
it	O
also	O
involves	O
multivariate	O
probability	O
models	O
.	O
)	O
unsupervised	B
learning	I
is	O
arguably	O
more	O
typical	O
of	O
human	O
and	O
animal	O
learning	B
.	O
it	O
is	O
also	O
more	O
widely	O
applicable	O
than	O
supervised	B
learning	I
,	O
since	O
it	O
does	O
not	O
require	O
a	O
human	O
expert	O
to	O
10	O
chapter	O
1.	O
introduction	O
t	O
i	O
h	O
g	O
e	O
w	O
280	O
260	O
240	O
220	O
200	O
180	O
160	O
140	O
120	O
100	O
80	O
55	O
60	O
65	O
70	O
75	O
80	O
height	O
(	O
a	O
)	O
t	O
i	O
h	O
g	O
e	O
w	O
280	O
260	O
240	O
220	O
200	O
180	O
160	O
140	O
120	O
100	O
80	O
55	O
k=2	O
60	O
65	O
70	O
75	O
80	O
height	O
(	O
b	O
)	O
figure	O
1.8	O
(	O
a	O
)	O
the	O
height	O
and	O
weight	O
of	O
some	O
people	O
.	O
figure	O
generated	O
by	O
kmeansheightweight	O
.	O
(	O
b	O
)	O
a	O
possible	O
clustering	B
using	O
k	O
=	O
2	O
clusters	B
.	O
manually	O
label	B
the	O
data	O
.	O
labeled	O
data	O
is	O
not	O
only	O
expensive	O
to	O
acquire6	O
,	O
but	O
it	O
also	O
contains	O
relatively	O
little	O
information	B
,	O
certainly	O
not	O
enough	O
to	O
reliably	O
estimate	O
the	O
parameters	O
of	O
complex	O
models	O
.	O
geoff	O
hinton	O
,	O
who	O
is	O
a	O
famous	O
professor	O
of	O
ml	O
at	O
the	O
university	O
of	O
toronto	O
,	O
has	O
said	O
:	O
when	O
we	O
’	O
re	O
learning	O
to	O
see	O
,	O
nobody	O
’	O
s	O
telling	O
us	O
what	O
the	O
right	O
answers	O
are	O
—	O
we	O
just	O
look	O
.	O
every	O
so	O
often	O
,	O
your	O
mother	O
says	O
“	O
that	O
’	O
s	O
a	O
dog	O
”	O
,	O
but	O
that	O
’	O
s	O
very	O
little	O
information	B
.	O
you	O
’	O
d	O
be	O
lucky	O
if	O
you	O
got	O
a	O
few	O
bits	B
of	O
information	B
—	O
even	O
one	O
bit	O
per	O
second	O
—	O
that	O
way	O
.	O
the	O
brain	O
’	O
s	O
visual	O
system	O
has	O
1014	O
neural	O
connections	O
.	O
and	O
you	O
only	O
live	O
for	O
109	O
seconds	O
.	O
so	O
it	O
’	O
s	O
no	O
use	O
learning	B
one	O
bit	O
per	O
second	O
.	O
you	O
need	O
more	O
like	O
105	O
bits	B
per	O
second	O
.	O
and	O
there	O
’	O
s	O
only	O
one	O
place	O
you	O
can	O
get	O
that	O
much	O
information	B
:	O
from	O
the	O
input	O
itself	O
.	O
—	O
geoffrey	O
hinton	O
,	O
1996	O
(	O
quoted	O
in	O
(	O
gorder	O
2006	O
)	O
)	O
.	O
below	O
we	O
describe	O
some	O
canonical	O
examples	O
of	O
unsupervised	B
learning	I
.	O
1.3.1	O
discovering	O
clusters	B
as	O
a	O
canonical	O
example	O
of	O
unsupervised	B
learning	I
,	O
consider	O
the	O
problem	O
of	O
clustering	B
data	O
into	O
groups	O
.	O
for	O
example	O
,	O
figure	O
1.8	O
(	O
a	O
)	O
plots	O
some	O
2d	O
data	O
,	O
representing	O
the	O
height	O
and	O
weight	O
of	O
a	O
group	O
of	O
210	O
people	O
.	O
it	O
seems	O
that	O
there	O
might	O
be	O
various	O
clusters	B
,	O
or	O
subgroups	O
,	O
although	O
it	O
is	O
not	O
clear	O
how	O
many	O
.	O
let	O
k	O
denote	O
the	O
number	O
of	O
clusters	B
.	O
our	O
ﬁrst	O
goal	O
is	O
to	O
estimate	O
the	O
distribution	O
over	O
the	O
number	O
of	O
clusters	B
,	O
p	O
(	O
k|d	O
)	O
;	O
this	O
tells	O
us	O
if	O
there	O
are	O
subpopulations	O
within	O
the	O
data	O
.	O
for	O
simplicity	O
,	O
we	O
often	O
approximate	O
the	O
distribution	O
p	O
(	O
k|d	O
)	O
by	O
its	O
mode	B
,	O
=	O
arg	O
maxk	O
p	O
(	O
k|d	O
)	O
.	O
in	O
the	O
supervised	O
case	O
,	O
we	O
were	O
told	O
that	O
there	O
are	O
two	O
classes	O
(	O
male	O
k∗	O
and	O
female	O
)	O
,	O
but	O
in	O
the	O
unsupervised	O
case	O
,	O
we	O
are	O
free	O
to	O
choose	O
as	O
many	O
or	O
few	O
clusters	B
as	O
we	O
like	O
.	O
picking	O
a	O
model	O
of	O
the	O
“	O
right	O
”	O
complexity	O
is	O
called	O
model	B
selection	I
,	O
and	O
will	O
be	O
discussed	O
in	O
detail	O
below	O
.	O
our	O
second	O
goal	O
is	O
to	O
estimate	O
which	O
cluster	O
each	O
point	O
belongs	O
to	O
.	O
let	O
zi	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
(	O
zi	O
is	O
an	O
example	O
of	O
a	O
hidden	B
or	O
represent	O
the	O
cluster	O
to	O
which	O
data	O
point	O
i	O
is	O
assigned	O
.	O
6.	O
the	O
advent	O
of	O
crowd	B
sourcing	I
web	O
sites	O
such	O
as	O
mechanical	O
turk	O
,	O
(	O
https	O
:	O
//www.mturk.com/mturk/welcome	O
)	O
,	O
which	O
outsource	O
data	O
processing	O
tasks	O
to	O
humans	O
all	O
over	O
the	O
world	O
,	O
has	O
reduced	O
the	O
cost	O
of	O
labeling	O
data	O
.	O
nevertheless	O
,	O
the	O
amount	O
of	O
unlabeled	O
data	O
is	O
still	O
orders	O
of	O
magnitude	O
larger	O
than	O
the	O
amount	O
of	O
labeled	O
data	O
.	O
1.3.	O
unsupervised	B
learning	I
11	O
2	O
0	O
−2	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−4	O
−8	O
−2	O
0	O
2	O
4	O
6	O
8	O
(	O
a	O
)	O
4	O
2	O
0	O
−2	O
−4	O
0	O
5	O
−5	O
(	O
b	O
)	O
figure	O
1.9	O
(	O
a	O
)	O
a	O
set	O
of	O
points	O
that	O
live	O
on	O
a	O
2d	O
linear	O
subspace	O
embedded	O
in	O
3d	O
.	O
the	O
solid	O
red	O
line	O
is	O
the	O
ﬁrst	O
principal	B
component	I
direction	O
.	O
the	O
dotted	O
black	O
line	O
is	O
the	O
second	O
pc	O
direction	O
.	O
(	O
b	O
)	O
2d	O
representation	O
of	O
the	O
data	O
.	O
figure	O
generated	O
by	O
pcademo3d	O
.	O
latent	O
variable	O
,	O
since	O
it	O
is	O
never	O
observed	O
in	O
the	O
training	B
set	I
.	O
)	O
we	O
can	O
infer	O
which	O
cluster	O
each	O
i	O
=	O
argmaxk	O
p	O
(	O
zi	O
=	O
k|xi	O
,	O
d	O
)	O
.	O
this	O
is	O
illustrated	O
in	O
data	O
point	O
belongs	O
to	O
by	O
computing	O
z∗	O
figure	O
1.8	O
(	O
b	O
)	O
,	O
where	O
we	O
use	O
different	O
colors	O
to	O
indicate	O
the	O
assignments	O
,	O
assuming	O
k	O
=	O
2.	O
in	O
this	O
book	O
,	O
we	O
focus	O
on	O
model	B
based	I
clustering	I
,	O
which	O
means	O
we	O
ﬁt	O
a	O
probabilistic	O
model	O
to	O
the	O
data	O
,	O
rather	O
than	O
running	O
some	O
ad	O
hoc	O
algorithm	O
.	O
the	O
advantages	O
of	O
the	O
model-based	O
approach	O
are	O
that	O
one	O
can	O
compare	O
different	O
kinds	O
of	O
models	O
in	O
an	O
objective	O
way	O
(	O
in	O
terms	O
of	O
the	O
likelihood	B
they	O
assign	O
to	O
the	O
data	O
)	O
,	O
we	O
can	O
combine	O
them	O
together	O
into	O
larger	O
systems	O
,	O
etc	O
.	O
here	O
are	O
some	O
real	O
world	O
applications	O
of	O
clustering	B
.	O
•	O
•	O
•	O
in	O
astronomy	O
,	O
the	O
autoclass	B
system	O
(	O
cheeseman	O
et	O
al	O
.	O
1988	O
)	O
discovered	O
a	O
new	O
type	O
of	O
star	O
,	O
based	O
on	O
clustering	B
astrophysical	O
measurements	O
.	O
in	O
e-commerce	B
,	O
it	O
is	O
common	O
to	O
cluster	O
users	O
into	O
groups	O
,	O
based	O
on	O
their	O
purchasing	O
or	O
web-surﬁng	O
behavior	O
,	O
and	O
then	O
to	O
send	O
customized	O
targeted	O
advertising	O
to	O
each	O
group	O
(	O
see	O
e.g.	O
,	O
(	O
berkhin	O
2006	O
)	O
)	O
.	O
in	O
biology	O
,	O
it	O
is	O
common	O
to	O
cluster	O
ﬂow-cytometry	O
data	O
into	O
groups	O
,	O
to	O
discover	O
different	O
sub-populations	O
of	O
cells	O
(	O
see	O
e.g.	O
,	O
(	O
lo	O
et	O
al	O
.	O
2009	O
)	O
)	O
.	O
1.3.2	O
discovering	O
latent	B
factors	I
when	O
dealing	O
with	O
high	O
dimensional	O
data	O
,	O
it	O
is	O
often	O
useful	O
to	O
reduce	O
the	O
dimensionality	O
by	O
projecting	O
the	O
data	O
to	O
a	O
lower	O
dimensional	O
subspace	O
which	O
captures	O
the	O
“	O
essence	O
”	O
of	O
the	O
data	O
.	O
this	O
is	O
called	O
dimensionality	B
reduction	I
.	O
a	O
simple	O
example	O
is	O
shown	O
in	O
figure	O
1.9	O
,	O
where	O
we	O
project	O
some	O
3d	O
data	O
down	O
to	O
a	O
2d	O
plane	O
.	O
the	O
2d	O
approximation	O
is	O
quite	O
good	O
,	O
since	O
most	O
points	O
lie	O
close	O
to	O
this	O
subspace	O
.	O
reducing	O
to	O
1d	O
would	O
involve	O
projecting	O
points	O
onto	O
the	O
red	O
line	O
in	O
figure	O
1.9	O
(	O
a	O
)	O
;	O
this	O
would	O
be	O
a	O
rather	O
poor	O
approximation	O
.	O
(	O
we	O
will	O
make	O
this	O
notion	O
precise	O
in	O
chapter	O
12	O
.	O
)	O
the	O
motivation	O
behind	O
this	O
technique	O
is	O
that	O
although	O
the	O
data	O
may	O
appear	O
high	O
dimensional	O
,	O
there	O
may	O
only	O
be	O
a	O
small	O
number	O
of	O
degrees	O
of	O
variability	O
,	O
corresponding	O
to	O
latent	B
factors	I
.	O
for	O
example	O
,	O
when	O
modeling	O
the	O
appearance	O
of	O
face	O
images	O
,	O
there	O
may	O
only	O
be	O
a	O
few	O
underlying	O
latent	B
factors	I
which	O
describe	O
most	O
of	O
the	O
variability	O
,	O
such	O
as	O
lighting	O
,	O
pose	O
,	O
identity	O
,	O
etc	O
,	O
as	O
illustrated	O
in	O
figure	O
1.10	O
.	O
12	O
chapter	O
1.	O
introduction	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
1.10	O
a	O
)	O
25	O
randomly	O
chosen	O
64	O
×	O
64	O
pixel	O
images	O
from	O
the	O
olivetti	O
face	O
database	O
.	O
and	O
the	O
ﬁrst	O
three	O
principal	B
component	I
basis	O
vectors	O
(	O
eigenfaces	B
)	O
.	O
figure	O
generated	O
by	O
pcaimagedemo	O
.	O
(	O
b	O
)	O
the	O
mean	B
when	O
used	O
as	O
input	O
to	O
other	O
statistical	O
models	O
,	O
such	O
low	O
dimensional	O
representations	O
often	O
result	O
in	O
better	O
predictive	B
accuracy	O
,	O
because	O
they	O
focus	O
on	O
the	O
“	O
essence	O
”	O
of	O
the	O
object	O
,	O
ﬁltering	B
out	O
inessential	O
features	B
.	O
also	O
,	O
low	O
dimensional	O
representations	O
are	O
useful	O
for	O
enabling	O
fast	O
nearest	O
neighbor	O
searches	O
and	O
two	O
dimensional	O
projections	O
are	O
very	O
useful	O
for	O
visualizing	B
high	O
dimensional	O
data	O
.	O
the	O
most	O
common	O
approach	O
to	O
dimensionality	B
reduction	I
is	O
called	O
principal	B
components	I
analysis	I
or	O
pca	O
.	O
this	O
can	O
be	O
thought	O
of	O
as	O
an	O
unsupervised	O
version	O
of	O
(	O
multi-output	O
)	O
linear	B
regression	I
,	O
where	O
we	O
observe	O
the	O
high-dimensional	O
response	O
y	O
,	O
but	O
not	O
the	O
low-dimensional	O
“	O
cause	O
”	O
z.	O
thus	O
the	O
model	O
has	O
the	O
form	O
z	O
→	O
y	O
;	O
we	O
have	O
to	O
“	O
invert	O
the	O
arrow	O
”	O
,	O
and	O
infer	O
the	O
latent	B
low-dimensional	O
z	O
from	O
the	O
observed	O
high-dimensional	O
y.	O
see	O
section	O
12.1	O
for	O
details	O
.	O
dimensionality	B
reduction	I
,	O
and	O
pca	O
in	O
particular	O
,	O
has	O
been	O
applied	O
in	O
many	O
different	O
areas	O
.	O
some	O
examples	O
include	O
the	O
following	O
:	O
•	O
•	O
•	O
•	O
in	O
biology	O
,	O
it	O
is	O
common	O
to	O
use	O
pca	O
to	O
interpret	O
gene	O
microarray	O
data	O
,	O
to	O
account	O
for	O
the	O
fact	O
that	O
each	O
measurement	O
is	O
usually	O
the	O
result	O
of	O
many	O
genes	O
which	O
are	O
correlated	O
in	O
their	O
behavior	O
by	O
the	O
fact	O
that	O
they	O
belong	O
to	O
different	O
biological	O
pathways	O
.	O
in	O
natural	O
language	O
processing	O
,	O
it	O
is	O
common	O
to	O
use	O
a	O
variant	O
of	O
pca	O
called	O
latent	B
semantic	I
analysis	I
for	O
document	O
retrieval	O
(	O
see	O
section	O
27.2.2	O
)	O
.	O
in	O
signal	B
processing	I
(	O
e.g.	O
,	O
of	O
acoustic	O
or	O
neural	O
signals	O
)	O
,	O
it	O
is	O
common	O
to	O
use	O
ica	O
(	O
which	O
is	O
a	O
variant	O
of	O
pca	O
)	O
to	O
separate	O
signals	O
into	O
their	O
different	O
sources	O
(	O
see	O
section	O
12.6	O
)	O
.	O
in	O
computer	O
graphics	O
,	O
it	O
is	O
common	O
to	O
project	O
motion	O
capture	O
data	O
to	O
a	O
low	O
dimensional	O
space	O
,	O
and	O
use	O
it	O
to	O
create	O
animations	O
.	O
see	O
section	O
15.5	O
for	O
one	O
way	O
to	O
tackle	O
such	O
problems	O
.	O
1.3.	O
unsupervised	B
learning	I
13	O
figure	O
1.11	O
a	O
sparse	B
undirected	O
gaussian	O
graphical	B
model	I
learned	O
using	O
graphical	B
lasso	I
(	O
section	O
26.7.2	O
)	O
applied	O
to	O
some	O
ﬂow	B
cytometry	I
data	O
(	O
from	O
(	O
sachs	O
et	O
al	O
.	O
2005	O
)	O
)	O
,	O
which	O
measures	O
the	O
phosphorylation	O
status	O
of	O
11	O
proteins	O
.	O
figure	O
generated	O
by	O
ggmlassodemo	O
.	O
1.3.3	O
discovering	O
graph	B
structure	O
sometimes	O
we	O
measure	O
a	O
set	O
of	O
correlated	O
variables	O
,	O
and	O
we	O
would	O
like	O
to	O
discover	O
which	O
ones	O
are	O
most	O
correlated	O
with	O
which	O
others	O
.	O
this	O
can	O
be	O
represented	O
by	O
a	O
graph	B
g	O
,	O
in	O
which	O
nodes	B
represent	O
variables	O
,	O
and	O
edges	B
represent	O
direct	O
dependence	O
between	O
variables	O
(	O
we	O
will	O
make	O
this	O
precise	O
in	O
chapter	O
10	O
,	O
when	O
we	O
discuss	O
graphical	O
models	O
)	O
.	O
we	O
can	O
then	O
learn	O
this	O
graph	B
structure	O
from	O
data	O
,	O
i.e.	O
,	O
we	O
compute	O
ˆg	O
=	O
argmax	O
p	O
(	O
g|d	O
)	O
.	O
as	O
with	O
unsupervised	B
learning	I
in	O
general	O
,	O
there	O
are	O
two	O
main	O
applications	O
for	O
learning	B
sparse	O
graphs	O
:	O
to	O
discover	O
new	O
knowledge	O
,	O
and	O
to	O
get	O
better	O
joint	O
probability	O
density	O
estimators	O
.	O
we	O
now	O
give	O
somes	O
example	O
of	O
each	O
.	O
•	O
much	O
of	O
the	O
motivation	O
for	O
learning	B
sparse	O
graphical	O
models	O
comes	O
from	O
the	O
systems	B
biology	I
community	O
.	O
for	O
example	O
,	O
suppose	O
we	O
measure	O
the	O
phosphorylation	O
status	O
of	O
some	O
proteins	O
in	O
a	O
cell	O
(	O
sachs	O
et	O
al	O
.	O
2005	O
)	O
.	O
figure	O
1.11	O
gives	O
an	O
example	O
of	O
a	O
graph	B
structure	O
that	O
was	O
learned	O
from	O
this	O
data	O
(	O
using	O
methods	O
discussed	O
in	O
section	O
26.7.2	O
)	O
.	O
as	O
another	O
example	O
,	O
smith	O
et	O
al	O
.	O
(	O
2006	O
)	O
showed	O
that	O
one	O
can	O
recover	O
the	O
neural	O
“	O
wiring	O
diagram	O
”	O
of	O
a	O
certain	O
kind	O
of	O
bird	O
from	O
time-series	O
eeg	O
data	O
.	O
the	O
recovered	O
structure	O
closely	O
matched	O
the	O
known	O
functional	O
connectivity	O
of	O
this	O
part	O
of	O
the	O
bird	O
brain	O
.	O
•	O
in	O
some	O
cases	O
,	O
we	O
are	O
not	O
interested	O
in	O
interpreting	O
the	O
graph	B
structure	O
,	O
we	O
just	O
want	O
to	O
use	O
it	O
to	O
model	O
correlations	O
and	O
to	O
make	O
predictions	O
.	O
one	O
example	O
of	O
this	O
is	O
in	O
ﬁnancial	O
portfolio	O
management	O
,	O
where	O
accurate	O
models	O
of	O
the	O
covariance	B
between	O
large	O
numbers	O
of	O
different	O
stocks	O
is	O
important	O
.	O
carvalho	O
and	O
west	O
(	O
2007	O
)	O
show	O
that	O
by	O
learning	B
a	O
sparse	B
graph	O
,	O
and	O
then	O
using	O
this	O
as	O
the	O
basis	O
of	O
a	O
trading	O
strategy	O
,	O
it	O
is	O
possible	O
to	O
outperform	O
(	O
i.e.	O
,	O
make	O
more	O
money	O
than	O
)	O
methods	O
that	O
do	O
not	O
exploit	O
sparse	B
graphs	O
.	O
another	O
example	O
is	O
predicting	O
traffic	O
jams	O
on	O
the	O
freeway	O
.	O
horvitz	O
et	O
al	O
.	O
(	O
2005	O
)	O
describe	O
a	O
deployed	O
system	O
called	O
jambayes	O
for	O
predicting	O
traffic	O
ﬂow	O
in	O
the	O
seattle	O
area	O
;	O
predictions	O
are	O
made	O
using	O
a	O
graphical	B
model	I
whose	O
structure	O
was	O
learned	O
from	O
data	O
.	O
14	O
chapter	O
1.	O
introduction	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
1.12	O
(	O
a	O
)	O
a	O
noisy	O
image	O
with	O
an	O
occluder	O
.	O
(	O
b	O
)	O
an	O
estimate	O
of	O
the	O
underlying	O
pixel	O
intensities	O
,	O
based	O
on	O
a	O
pairwise	O
mrf	O
model	O
.	O
source	O
:	O
figure	O
8	O
of	O
(	O
felzenszwalb	O
and	O
huttenlocher	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
pedro	O
felzenszwalb	O
.	O
1.3.4	O
matrix	B
completion	I
sometimes	O
we	O
have	O
missing	B
data	I
,	O
that	O
is	O
,	O
variables	O
whose	O
values	O
are	O
unknown	B
.	O
for	O
example	O
,	O
we	O
might	O
have	O
conducted	O
a	O
survey	O
,	O
and	O
some	O
people	O
might	O
not	O
have	O
answered	O
certain	O
questions	O
.	O
or	O
we	O
might	O
have	O
various	O
sensors	O
,	O
some	O
of	O
which	O
fail	O
.	O
the	O
corresponding	O
design	B
matrix	I
will	O
then	O
have	O
“	O
holes	O
”	O
in	O
it	O
;	O
these	O
missing	B
entries	O
are	O
often	O
represented	O
by	O
nan	O
,	O
which	O
stands	O
for	O
“	O
not	O
a	O
number	O
”	O
.	O
the	O
goal	O
of	O
imputation	B
is	O
to	O
infer	O
plausible	O
values	O
for	O
the	O
missing	B
entries	O
.	O
this	O
is	O
sometimes	O
called	O
matrix	B
completion	I
.	O
below	O
we	O
give	O
some	O
example	O
applications	O
.	O
1.3.4.1	O
image	B
inpainting	I
an	O
interesting	O
example	O
of	O
an	O
imputation-like	O
task	O
is	O
known	O
as	O
image	B
inpainting	I
.	O
the	O
goal	O
is	O
to	O
“	O
ﬁll	O
in	O
”	O
holes	O
(	O
e.g.	O
,	O
due	O
to	O
scratches	O
or	O
occlusions	O
)	O
in	O
an	O
image	O
with	O
realistic	O
texture	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
1.12	O
,	O
where	O
we	O
denoise	O
the	O
image	O
,	O
as	O
well	O
as	O
impute	O
the	O
pixels	O
hidden	B
behind	O
the	O
occlusion	O
.	O
this	O
can	O
be	O
tackled	O
by	O
building	O
a	O
joint	O
probability	O
model	O
of	O
the	O
pixels	O
,	O
given	O
a	O
set	O
of	O
clean	O
images	O
,	O
and	O
then	O
inferring	O
the	O
unknown	B
variables	O
(	O
pixels	O
)	O
given	O
the	O
known	O
variables	O
(	O
pixels	O
)	O
.	O
this	O
is	O
somewhat	O
like	O
masket	O
basket	O
analysis	O
,	O
except	O
the	O
data	O
is	O
real-valued	O
and	O
spatially	O
structured	O
,	O
so	O
the	O
kinds	O
of	O
probability	O
models	O
we	O
use	O
are	O
quite	O
different	O
.	O
see	O
sections	O
19.6.2.7	O
and	O
13.8.4	O
for	O
some	O
possible	O
choices	O
.	O
1.3.4.2	O
collaborative	B
ﬁltering	I
another	O
interesting	O
example	O
of	O
an	O
imputation-like	O
task	O
is	O
known	O
as	O
collaborative	B
ﬁltering	I
.	O
a	O
common	O
example	O
of	O
this	O
concerns	O
predicting	O
which	O
movies	O
people	O
will	O
want	O
to	O
watch	O
based	O
on	O
how	O
they	O
,	O
and	O
other	O
people	O
,	O
have	O
rated	O
movies	O
which	O
they	O
have	O
already	O
seen	O
.	O
the	O
key	O
idea	O
is	O
that	O
the	O
prediction	O
is	O
not	O
based	O
on	O
features	B
of	O
the	O
movie	O
or	O
user	O
(	O
although	O
it	O
could	O
be	O
)	O
,	O
but	O
merely	O
on	O
a	O
ratings	O
matrix	O
.	O
more	O
precisely	O
,	O
we	O
have	O
a	O
matrix	O
x	O
where	O
x	O
(	O
m	O
,	O
u	O
)	O
is	O
the	O
rating	O
1.3.	O
unsupervised	B
learning	I
15	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:88	O
)	O
(	O
cid:86	O
)	O
(	O
cid:72	O
)	O
(	O
cid:85	O
)	O
(	O
cid:86	O
)	O
(	O
cid:34	O
)	O
(	O
cid:22	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:34	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:24	O
)	O
(	O
cid:34	O
)	O
(	O
cid:3	O
)	O
(	O
cid:24	O
)	O
(	O
cid:34	O
)	O
(	O
cid:21	O
)	O
(	O
cid:80	O
)	O
(	O
cid:82	O
)	O
(	O
cid:89	O
)	O
(	O
cid:76	O
)	O
(	O
cid:72	O
)	O
(	O
cid:86	O
)	O
figure	O
1.13	O
example	O
of	O
movie-rating	O
data	O
.	O
training	O
data	O
is	O
in	O
red	O
,	O
test	O
data	O
is	O
denoted	O
by	O
?	O
,	O
empty	O
cells	O
are	O
unknown	B
.	O
(	O
say	O
an	O
integer	O
between	O
1	O
and	O
5	O
,	O
where	O
1	O
is	O
dislike	O
and	O
5	O
is	O
like	O
)	O
by	O
user	O
u	O
of	O
movie	O
m.	O
note	O
that	O
most	O
of	O
the	O
entries	O
in	O
x	O
will	O
be	O
missing	B
or	O
unknown	B
,	O
since	O
most	O
users	O
will	O
not	O
have	O
rated	O
most	O
movies	O
.	O
hence	O
we	O
only	O
observe	O
a	O
tiny	O
subset	O
of	O
the	O
x	O
matrix	O
,	O
and	O
we	O
want	O
to	O
predict	O
in	O
particular	O
,	O
for	O
any	O
given	O
user	O
u	O
,	O
we	O
might	O
want	O
to	O
predict	O
which	O
of	O
the	O
a	O
different	O
subset	O
.	O
unrated	O
movies	O
he/she	O
is	O
most	O
likely	O
to	O
want	O
to	O
watch	O
.	O
launched	O
in	O
2006	O
,	O
with	O
a	O
$	O
1m	O
usd	O
prize	O
(	O
see	O
http	O
:	O
//netflixprize.com/	O
)	O
.	O
in	O
order	O
to	O
encourage	O
research	O
in	O
this	O
area	O
,	O
the	O
dvd	O
rental	O
company	O
netﬂix	O
created	O
a	O
com-	O
petition	O
,	O
in	O
particular	O
,	O
they	O
provided	O
a	O
large	O
matrix	O
of	O
ratings	O
,	O
on	O
a	O
scale	O
of	O
1	O
to	O
5	O
,	O
for	O
∼	O
18k	O
movies	O
created	O
by	O
∼	O
500k	O
users	O
.	O
the	O
full	B
matrix	O
would	O
have	O
∼	O
9	O
×	O
109	O
entries	O
,	O
but	O
only	O
about	O
1	O
%	O
of	O
the	O
entries	O
are	O
observed	O
,	O
so	O
the	O
matrix	O
is	O
extremely	O
sparse	B
.	O
a	O
subset	O
of	O
these	O
are	O
used	O
for	O
training	O
,	O
and	O
the	O
rest	O
for	O
testing	O
,	O
as	O
shown	O
in	O
figure	O
1.13.	O
the	O
goal	O
of	O
the	O
competition	O
was	O
to	O
predict	O
more	O
accurately	O
than	O
netﬂix	O
’	O
s	O
existing	O
system	O
.	O
on	O
21	O
september	O
2009	O
,	O
the	O
prize	O
was	O
awarded	O
to	O
a	O
team	O
of	O
researchers	O
known	O
as	O
“	O
bellkor	O
’	O
s	O
pragmatic	O
chaos	O
”	O
.	O
section	O
27.6.2	O
discusses	O
some	O
of	O
their	O
methodology	O
.	O
further	O
details	O
on	O
the	O
teams	O
and	O
their	O
methods	O
can	O
be	O
found	O
at	O
http	O
:	O
//www.netflixprize.com/community/viewtopic.php	O
?	O
id=1537	O
.	O
1.3.4.3	O
market	B
basket	I
analysis	I
in	O
commercial	O
data	O
mining	O
,	O
there	O
is	O
much	O
interest	O
in	O
a	O
task	O
called	O
market	B
basket	I
analysis	I
.	O
the	O
data	O
consists	O
of	O
a	O
(	O
typically	O
very	O
large	O
but	O
sparse	B
)	O
binary	O
matrix	O
,	O
where	O
each	O
column	O
represents	O
an	O
item	O
or	O
product	O
,	O
and	O
each	O
row	O
represents	O
a	O
transaction	O
.	O
we	O
set	O
xij	O
=	O
1	O
if	O
item	O
j	O
was	O
purchased	O
on	O
the	O
i	O
’	O
th	O
transaction	O
.	O
many	O
items	O
are	O
purchased	O
together	O
(	O
e.g.	O
,	O
bread	O
and	O
butter	O
)	O
,	O
so	O
there	O
will	O
be	O
correlations	O
amongst	O
the	O
bits	B
.	O
given	O
a	O
new	O
partially	O
observed	O
bit	O
vector	O
,	O
representing	O
a	O
subset	O
of	O
items	O
that	O
the	O
consumer	O
has	O
bought	O
,	O
the	O
goal	O
is	O
to	O
predict	O
which	O
other	O
bits	B
are	O
likely	O
to	O
turn	O
on	O
,	O
representing	O
other	O
items	O
the	O
consumer	O
might	O
be	O
likely	O
to	O
buy	O
.	O
(	O
unlike	O
collaborative	B
ﬁltering	I
,	O
we	O
often	O
assume	O
there	O
is	O
no	O
missing	O
data	O
in	O
the	O
training	O
data	O
,	O
since	O
we	O
know	O
the	O
past	O
shopping	O
behavior	O
of	O
each	O
customer	O
.	O
)	O
this	O
task	O
arises	O
in	O
other	O
domains	O
besides	O
modeling	O
purchasing	O
patterns	O
.	O
for	O
example	O
,	O
similar	B
techniques	O
can	O
be	O
used	O
to	O
model	O
dependencies	O
between	O
ﬁles	O
in	O
complex	O
software	O
systems	O
.	O
in	O
this	O
case	O
,	O
the	O
task	O
is	O
to	O
predict	O
,	O
given	O
a	O
subset	O
of	O
ﬁles	O
that	O
have	O
been	O
changed	O
,	O
which	O
other	O
ones	O
need	O
to	O
be	O
updated	O
to	O
ensure	O
consistency	O
(	O
see	O
e.g.	O
,	O
(	O
hu	O
et	O
al	O
.	O
2010	O
)	O
)	O
.	O
it	O
is	O
common	O
to	O
solve	O
such	O
tasks	O
using	O
frequent	B
itemset	I
mining	I
,	O
which	O
create	O
association	B
rules	I
(	O
see	O
e.g.	O
,	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
sec	O
14.2	O
)	O
for	O
details	O
)	O
.	O
alternatively	O
,	O
we	O
can	O
adopt	O
a	O
probabilistic	O
approach	O
,	O
and	O
ﬁt	O
a	O
joint	O
density	O
model	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
to	O
the	O
bit	O
vectors	O
,	O
see	O
e.g.	O
,	O
(	O
hu	O
et	O
al	O
.	O
16	O
chapter	O
1.	O
introduction	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
a	O
)	O
illustration	O
of	O
a	O
k-nearest	O
neighbors	B
classiﬁer	O
in	O
2d	O
for	O
k	O
=	O
3.	O
the	O
3	O
nearest	O
neighbors	O
figure	O
1.14	O
of	O
test	O
point	O
x1	O
have	O
labels	O
1	O
,	O
1	O
and	O
0	O
,	O
so	O
we	O
predict	O
p	O
(	O
y	O
=	O
1|x1	O
,	O
d	O
,	O
k	O
=	O
3	O
)	O
=	O
2/3	O
.	O
the	O
3	O
nearest	O
neighbors	O
of	O
test	O
point	O
x2	O
have	O
labels	O
0	O
,	O
0	O
,	O
and	O
0	O
,	O
so	O
we	O
predict	O
p	O
(	O
y	O
=	O
1|x2	O
,	O
d	O
,	O
k	O
=	O
3	O
)	O
=	O
0/3	O
.	O
(	O
b	O
)	O
illustration	O
of	O
the	O
voronoi	O
tesselation	O
induced	O
by	O
1-nn	O
.	O
based	O
on	O
figure	O
4.13	O
of	O
(	O
duda	O
et	O
al	O
.	O
2001	O
)	O
.	O
figure	O
generated	O
by	O
knnvoronoi	O
.	O
2010	O
)	O
.	O
such	O
models	O
often	O
have	O
better	O
predictive	B
acccuracy	O
than	O
association	B
rules	I
,	O
although	O
they	O
may	O
be	O
less	O
interpretible	O
.	O
this	O
is	O
typical	O
of	O
the	O
difference	O
between	O
data	O
mining	O
and	O
machine	B
learning	I
:	O
in	O
data	O
mining	O
,	O
there	O
is	O
more	O
emphasis	O
on	O
interpretable	O
models	O
,	O
whereas	O
in	O
machine	B
learning	I
,	O
there	O
is	O
more	O
emphasis	O
on	O
accurate	O
models	O
.	O
1.4	O
some	O
basic	O
concepts	O
in	O
machine	B
learning	I
in	O
this	O
section	O
,	O
we	O
provide	O
an	O
introduction	O
to	O
some	O
key	O
ideas	O
in	O
machine	B
learning	I
.	O
we	O
will	O
expand	O
on	O
these	O
concepts	O
later	O
in	O
the	O
book	O
,	O
but	O
we	O
introduce	O
them	O
brieﬂy	O
here	O
,	O
to	O
give	O
a	O
ﬂavor	O
of	O
things	O
to	O
come	O
.	O
1.4.1	O
parametric	O
vs	O
non-parametric	O
models	O
in	O
this	O
book	O
,	O
we	O
will	O
be	O
focussing	O
on	O
probabilistic	O
models	O
of	O
the	O
form	O
p	O
(	O
y|x	O
)	O
or	O
p	O
(	O
x	O
)	O
,	O
depending	O
on	O
whether	O
we	O
are	O
interested	O
in	O
supervised	O
or	O
unsupervised	B
learning	I
respectively	O
.	O
there	O
are	O
many	O
ways	O
to	O
deﬁne	O
such	O
models	O
,	O
but	O
the	O
most	O
important	O
distinction	O
is	O
this	O
:	O
does	O
the	O
model	O
have	O
a	O
ﬁxed	O
number	O
of	O
parameters	O
,	O
or	O
does	O
the	O
number	O
of	O
parameters	O
grow	O
with	O
the	O
amount	O
of	O
training	O
data	O
?	O
the	O
former	O
is	O
called	O
a	O
parametric	B
model	I
,	O
and	O
the	O
latter	O
is	O
called	O
a	O
non-	O
parametric	B
model	I
.	O
parametric	O
models	O
have	O
the	O
advantage	O
of	O
often	O
being	O
faster	O
to	O
use	O
,	O
but	O
the	O
disadvantage	O
of	O
making	O
stronger	O
assumptions	O
about	O
the	O
nature	O
of	O
the	O
data	O
distributions	O
.	O
non-	O
parametric	O
models	O
are	O
more	O
ﬂexible	O
,	O
but	O
often	O
computationally	O
intractable	O
for	O
large	O
datasets	O
.	O
we	O
will	O
give	O
examples	O
of	O
both	O
kinds	O
of	O
models	O
in	O
the	O
sections	O
below	O
.	O
we	O
focus	O
on	O
supervised	B
learning	I
for	O
simplicity	O
,	O
although	O
much	O
of	O
our	O
discussion	O
also	O
applies	O
to	O
unsupervised	B
learning	I
.	O
1.4.2	O
a	O
simple	O
non-parametric	O
classiﬁer	O
:	O
k-nearest	O
neighbors	B
a	O
simple	O
example	O
of	O
a	O
non-parametric	O
classiﬁer	O
is	O
the	O
k	O
nearest	B
neighbor	I
(	O
knn	O
)	O
classiﬁer	O
.	O
this	O
simply	O
“	O
looks	O
at	O
”	O
the	O
k	O
points	O
in	O
the	O
training	B
set	I
that	O
are	O
nearest	O
to	O
the	O
test	O
input	O
x	O
,	O
1.4.	O
some	O
basic	O
concepts	O
in	O
machine	B
learning	I
17	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
120	O
100	O
80	O
60	O
40	O
20	O
train	O
−2	O
−1	O
0	O
1	O
2	O
3	O
(	O
a	O
)	O
p	O
(	O
y=2|data	O
,	O
k=10	O
)	O
20	O
40	O
80	O
100	O
60	O
(	O
c	O
)	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
p	O
(	O
y=1|data	O
,	O
k=10	O
)	O
20	O
40	O
80	O
100	O
60	O
(	O
b	O
)	O
predicted	O
label	B
,	O
k=10	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
120	O
100	O
80	O
60	O
40	O
20	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
c1	O
c2	O
c3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
(	O
d	O
)	O
figure	O
1.15	O
(	O
c	O
)	O
probability	O
of	O
class	O
2	O
.	O
(	O
d	O
)	O
map	O
estimate	O
of	O
class	O
label	O
.	O
figure	O
generated	O
by	O
knnclassifydemo	O
.	O
(	O
a	O
)	O
some	O
synthetic	O
3-class	O
training	O
data	O
in	O
2d	O
.	O
(	O
b	O
)	O
probability	O
of	O
class	O
1	O
for	O
knn	O
with	O
k	O
=	O
10.	O
counts	O
how	O
many	O
members	O
of	O
each	O
class	O
are	O
in	O
this	O
set	O
,	O
and	O
returns	O
that	O
empirical	O
fraction	O
as	O
the	O
estimate	O
,	O
as	O
illustrated	O
in	O
figure	O
1.14.	O
more	O
formally	O
,	O
(	O
cid:2	O
)	O
i∈nk	O
(	O
x	O
,	O
d	O
)	O
p	O
(	O
y	O
=	O
c|x	O
,	O
d	O
,	O
k	O
)	O
=	O
1	O
k	O
i	O
(	O
yi	O
=	O
c	O
)	O
(	O
1.2	O
)	O
where	O
nk	O
(	O
x	O
,	O
d	O
)	O
are	O
the	O
(	O
indices	O
of	O
the	O
)	O
k	O
nearest	O
points	O
to	O
x	O
in	O
d	O
and	O
i	O
(	O
e	O
)	O
is	O
the	O
indicator	B
function	I
deﬁned	O
as	O
follows	O
:	O
if	O
e	O
is	O
true	O
if	O
e	O
is	O
false	O
i	O
(	O
e	O
)	O
=	O
(	O
1.3	O
)	O
(	O
cid:3	O
)	O
1	O
0	O
this	O
method	O
is	O
an	O
example	O
of	O
memory-based	B
learning	I
or	O
instance-based	B
learning	I
.	O
it	O
can	O
be	O
derived	O
from	O
a	O
probabilistic	O
framework	O
as	O
explained	O
in	O
section	O
14.7.3.	O
the	O
most	O
common	O
18	O
chapter	O
1.	O
introduction	O
1	O
0	O
s	O
1	O
(	O
a	O
)	O
e	O
b	O
u	O
c	O
f	O
o	O
h	O
t	O
g	O
n	O
e	O
l	O
e	O
g	O
d	O
e	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
d=10	O
d=7	O
d=5	O
d=3	O
d=1	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
fraction	O
of	O
data	O
in	O
neighborhood	O
(	O
b	O
)	O
figure	O
1.16	O
illustration	O
of	O
the	O
curse	B
of	I
dimensionality	I
.	O
(	O
a	O
)	O
we	O
embed	O
a	O
small	O
cube	O
of	O
side	O
s	O
inside	O
a	O
larger	O
unit	O
cube	O
.	O
(	O
b	O
)	O
we	O
plot	O
the	O
edge	O
length	O
of	O
a	O
cube	O
needed	O
to	O
cover	O
a	O
given	O
volume	O
of	O
the	O
unit	O
cube	O
as	O
a	O
function	O
of	O
the	O
number	O
of	O
dimensions	O
.	O
based	O
on	O
figure	O
2.6	O
from	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
cursedimensionality	O
.	O
distance	O
metric	O
to	O
use	O
is	O
euclidean	O
distance	O
(	O
which	O
limits	O
the	O
applicability	O
of	O
the	O
technique	O
to	O
data	O
which	O
is	O
real-valued	O
)	O
,	O
although	O
other	O
metrics	O
can	O
be	O
used	O
.	O
figure	O
1.15	O
gives	O
an	O
example	O
of	O
the	O
method	O
in	O
action	B
,	O
where	O
the	O
input	O
is	O
two	O
dimensional	O
,	O
we	O
have	O
three	O
classes	O
,	O
and	O
k	O
=	O
10	O
.	O
(	O
we	O
discuss	O
the	O
effect	O
of	O
k	O
below	O
.	O
)	O
panel	O
(	O
a	O
)	O
plots	O
the	O
training	O
data	O
.	O
panel	O
(	O
b	O
)	O
plots	O
p	O
(	O
y	O
=	O
1|x	O
,	O
d	O
)	O
where	O
x	O
is	O
evaluated	O
on	O
a	O
grid	O
of	O
points	O
.	O
panel	O
(	O
c	O
)	O
plots	O
p	O
(	O
y	O
=	O
2|x	O
,	O
d	O
)	O
.	O
we	O
do	O
not	O
need	O
to	O
plot	O
p	O
(	O
y	O
=	O
3|x	O
,	O
d	O
)	O
,	O
since	O
probabilities	O
sum	O
to	O
one	O
.	O
panel	O
(	O
d	O
)	O
plots	O
the	O
map	O
estimate	O
ˆy	O
(	O
x	O
)	O
=	O
argmaxc	O
(	O
y	O
=	O
c|x	O
,	O
d	O
)	O
.	O
a	O
knn	O
classiﬁer	O
with	O
k	O
=	O
1	O
induces	O
a	O
voronoi	O
tessellation	O
of	O
the	O
points	O
(	O
see	O
figure	O
1.14	O
(	O
b	O
)	O
)	O
.	O
this	O
is	O
a	O
partition	O
of	O
space	O
which	O
associates	O
a	O
region	O
v	O
(	O
xi	O
)	O
with	O
each	O
point	O
xi	O
in	O
such	O
a	O
way	O
that	O
all	O
points	O
in	O
v	O
(	O
xi	O
)	O
are	O
closer	O
to	O
xi	O
than	O
to	O
any	O
other	O
point	O
.	O
within	O
each	O
cell	O
,	O
the	O
predicted	O
label	B
is	O
the	O
label	B
of	O
the	O
corresponding	O
training	O
point	O
.	O
1.4.3	O
the	O
curse	B
of	I
dimensionality	I
the	O
knn	O
classiﬁer	O
is	O
simple	O
and	O
can	O
work	O
quite	O
well	O
,	O
provided	O
it	O
is	O
given	O
a	O
good	O
distance	O
metric	O
and	O
has	O
enough	O
labeled	O
training	O
data	O
.	O
in	O
fact	O
,	O
it	O
can	O
be	O
shown	O
that	O
the	O
knn	O
classiﬁer	O
can	O
come	O
within	O
a	O
factor	B
of	O
2	O
of	O
the	O
best	O
possible	O
performance	O
if	O
n	O
→	O
∞	O
(	O
cover	O
and	O
hart	O
1967	O
)	O
.	O
however	O
,	O
the	O
main	O
problem	O
with	O
knn	O
classiﬁers	O
is	O
that	O
they	O
do	O
not	O
work	O
well	O
with	O
high	O
dimensional	O
inputs	O
.	O
the	O
poor	O
performance	O
in	O
high	O
dimensional	O
settings	O
is	O
due	O
to	O
the	O
curse	B
of	I
dimensionality	I
.	O
to	O
explain	O
the	O
curse	O
,	O
we	O
give	O
some	O
examples	O
from	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p22	O
)	O
.	O
consider	O
applying	O
a	O
knn	O
classiﬁer	O
to	O
data	O
where	O
the	O
inputs	O
are	O
uniformly	O
distributed	O
in	O
the	O
d-dimensional	O
unit	O
cube	O
.	O
suppose	O
we	O
estimate	O
the	O
density	O
of	O
class	O
labels	O
around	O
a	O
test	O
point	O
x	O
by	O
“	O
growing	O
”	O
a	O
hyper-cube	O
around	O
x	O
until	O
it	O
contains	O
a	O
desired	O
fraction	O
f	O
of	O
the	O
data	O
points	O
.	O
the	O
expected	O
edge	O
length	O
of	O
this	O
cube	O
will	O
be	O
ed	O
(	O
f	O
)	O
=	O
f	O
1/d	O
.	O
if	O
d	O
=	O
10	O
,	O
and	O
we	O
want	O
to	O
base	O
our	O
estimate	O
on	O
10	O
%	O
1.4.	O
some	O
basic	O
concepts	O
in	O
machine	B
learning	I
19	O
pdf	B
0.4	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
a	O
)	O
a	O
gaussian	O
pdf	B
with	O
mean	B
0	O
and	O
variance	B
1.	O
figure	O
generated	O
by	O
gaussplotdemo	O
.	O
figure	O
1.17	O
(	O
b	O
)	O
visualization	O
of	O
the	O
conditional	O
density	O
model	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
n	O
(	O
y|w0	O
+	O
w1x	O
,	O
σ2	O
)	O
.	O
the	O
density	O
falls	O
off	O
exponentially	O
fast	O
as	O
we	O
move	O
away	O
from	O
the	O
regression	B
line	O
.	O
figure	O
generated	O
by	O
linregwedgedemo2	O
.	O
of	O
the	O
data	O
,	O
we	O
have	O
e10	O
(	O
0.1	O
)	O
=	O
0.8	O
,	O
so	O
we	O
need	O
to	O
extend	O
the	O
cube	O
80	O
%	O
along	O
each	O
dimension	O
around	O
x.	O
even	O
if	O
we	O
only	O
use	O
1	O
%	O
of	O
the	O
data	O
,	O
we	O
ﬁnd	O
e10	O
(	O
0.01	O
)	O
=	O
0.63	O
:	O
see	O
figure	O
1.16.	O
since	O
the	O
entire	O
range	O
of	O
the	O
data	O
is	O
only	O
1	O
along	O
each	O
dimension	O
,	O
we	O
see	O
that	O
the	O
method	O
is	O
no	O
longer	O
very	O
local	O
,	O
despite	O
the	O
name	O
“	O
nearest	B
neighbor	I
”	O
.	O
the	O
trouble	O
with	O
looking	O
at	O
neighbors	B
that	O
are	O
so	O
far	O
away	O
is	O
that	O
they	O
may	O
not	O
be	O
good	O
predictors	O
about	O
the	O
behavior	O
of	O
the	O
input-output	O
function	O
at	O
a	O
given	O
point	O
.	O
1.4.4	O
parametric	O
models	O
for	O
classiﬁcation	B
and	O
regression	B
the	O
main	O
way	O
to	O
combat	O
the	O
curse	B
of	I
dimensionality	I
is	O
to	O
make	O
some	O
assumptions	O
about	O
the	O
nature	O
of	O
the	O
data	O
distribution	O
(	O
either	O
p	O
(	O
y|x	O
)	O
for	O
a	O
supervised	O
problem	O
or	O
p	O
(	O
x	O
)	O
for	O
an	O
unsupervised	O
problem	O
)	O
.	O
these	O
assumptions	O
,	O
known	O
as	O
inductive	B
bias	I
,	O
are	O
often	O
embodied	O
in	O
the	O
form	O
of	O
a	O
parametric	B
model	I
,	O
which	O
is	O
a	O
statistical	O
model	O
with	O
a	O
ﬁxed	O
number	O
of	O
parameters	O
.	O
below	O
we	O
brieﬂy	O
describe	O
two	O
widely	O
used	O
examples	O
;	O
we	O
will	O
revisit	O
these	O
and	O
other	O
models	O
in	O
much	O
greater	O
depth	O
later	O
in	O
the	O
book	O
.	O
1.4.5	O
linear	B
regression	I
one	O
of	O
the	O
most	O
widely	O
used	O
models	O
for	O
regression	B
is	O
known	O
as	O
linear	B
regression	I
.	O
this	O
asserts	O
that	O
the	O
response	O
is	O
a	O
linear	O
function	O
of	O
the	O
inputs	O
.	O
this	O
can	O
be	O
written	O
as	O
follows	O
:	O
d	O
(	O
cid:2	O
)	O
y	O
(	O
x	O
)	O
=	O
wt	O
x	O
+	O
	O
=	O
wjxj	O
+	O
	O
(	O
1.4	O
)	O
j=1	O
where	O
wt	O
x	O
represents	O
the	O
inner	O
or	O
scalar	B
product	I
between	O
the	O
input	O
vector	O
x	O
and	O
the	O
model	O
’	O
s	O
weight	B
vector	I
w7	O
,	O
and	O
	O
is	O
the	O
residual	B
error	I
between	O
our	O
linear	O
predictions	O
and	O
the	O
true	O
response	O
.	O
7.	O
in	O
statistics	O
,	O
it	O
is	O
more	O
common	O
to	O
denote	O
the	O
regression	B
weights	O
by	O
β	O
.	O
20	O
chapter	O
1.	O
introduction	O
degree	B
14	O
degree	B
20	O
15	O
10	O
5	O
0	O
−5	O
15	O
10	O
5	O
0	O
−5	O
−10	O
0	O
5	O
10	O
15	O
20	O
−10	O
0	O
5	O
10	O
15	O
20	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
1.18	O
polynomial	O
of	O
degrees	O
14	O
and	O
20	O
ﬁt	O
by	O
least	B
squares	I
to	O
21	O
data	O
points	O
.	O
figure	O
generated	O
by	O
linregpolyvsdegree	O
.	O
we	O
often	O
assume	O
that	O
	O
has	O
a	O
gaussian8	O
or	O
normal	B
distribution	O
.	O
we	O
denote	O
this	O
by	O
	O
∼	O
n	O
(	O
μ	O
,	O
σ2	O
)	O
,	O
where	O
μ	O
is	O
the	O
mean	B
and	O
σ2	O
is	O
the	O
variance	B
(	O
see	O
chapter	O
2	O
for	O
details	O
)	O
.	O
when	O
we	O
plot	O
this	O
distribution	O
,	O
we	O
get	O
the	O
well-known	O
bell	B
curve	I
shown	O
in	O
figure	O
1.17	O
(	O
a	O
)	O
.	O
to	O
make	O
the	O
connection	O
between	O
linear	B
regression	I
and	O
gaussians	O
more	O
explicit	O
,	O
we	O
can	O
rewrite	O
the	O
model	O
in	O
the	O
following	O
form	O
:	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
n	O
(	O
y|μ	O
(	O
x	O
)	O
,	O
σ2	O
(	O
x	O
)	O
)	O
(	O
1.5	O
)	O
this	O
makes	O
it	O
clear	O
that	O
the	O
model	O
is	O
a	O
conditional	B
probability	I
density	O
.	O
in	O
the	O
simplest	O
case	O
,	O
we	O
assume	O
μ	O
is	O
a	O
linear	O
function	O
of	O
x	O
,	O
so	O
μ	O
=	O
wt	O
x	O
,	O
and	O
that	O
the	O
noise	O
is	O
ﬁxed	O
,	O
σ2	O
(	O
x	O
)	O
=σ	O
2.	O
in	O
this	O
case	O
,	O
θ	O
=	O
(	O
w	O
,	O
σ2	O
)	O
are	O
the	O
parameters	O
of	O
the	O
model	O
.	O
for	O
example	O
,	O
suppose	O
the	O
input	O
is	O
1	O
dimensional	O
.	O
we	O
can	O
represent	O
the	O
expected	O
response	O
as	O
follows	O
:	O
μ	O
(	O
x	O
)	O
=	O
w0	O
+	O
w1x	O
=	O
wt	O
x	O
(	O
1.6	O
)	O
where	O
w0	O
is	O
the	O
intercept	O
or	O
bias	B
term	I
,	O
w1	O
is	O
the	O
slope	O
,	O
and	O
where	O
we	O
have	O
deﬁned	O
the	O
vector	O
x	O
=	O
(	O
1	O
,	O
x	O
)	O
.	O
(	O
prepending	O
a	O
constant	O
1	O
term	O
to	O
an	O
input	O
vector	O
is	O
a	O
common	O
notational	O
trick	O
which	O
if	O
w1	O
is	O
positive	O
,	O
allows	O
us	O
to	O
combine	O
the	O
intercept	O
term	O
with	O
the	O
other	O
terms	O
in	O
the	O
model	O
.	O
)	O
it	O
means	O
we	O
expect	O
the	O
output	O
to	O
increase	O
as	O
the	O
input	O
increases	O
.	O
this	O
is	O
illustrated	O
in	O
1d	O
in	O
figure	O
1.17	O
(	O
b	O
)	O
;	O
a	O
more	O
conventional	O
plot	O
,	O
of	O
the	O
mean	B
response	O
vs	O
x	O
,	O
is	O
shown	O
in	O
figure	O
1.7	O
(	O
a	O
)	O
.	O
linear	B
regression	I
can	O
be	O
made	O
to	O
model	O
non-linear	O
relationships	O
by	O
replacing	O
x	O
with	O
some	O
non-linear	O
function	O
of	O
the	O
inputs	O
,	O
φ	O
(	O
x	O
)	O
.	O
that	O
is	O
,	O
we	O
use	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
n	O
(	O
y|wt	O
φ	O
(	O
x	O
)	O
,	O
σ2	O
)	O
(	O
1.7	O
)	O
this	O
is	O
known	O
as	O
basis	B
function	I
expansion	I
.	O
for	O
example	O
,	O
figure	O
1.18	O
illustrates	O
the	O
case	O
where	O
φ	O
(	O
x	O
)	O
=	O
[	O
1	O
,	O
x	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xd	O
]	O
,	O
for	O
d	O
=	O
14	O
and	O
d	O
=	O
20	O
;	O
this	O
is	O
known	O
as	O
polynomial	B
regression	I
.	O
we	O
will	O
consider	O
other	O
kinds	O
of	O
basis	B
functions	I
later	O
in	O
the	O
book	O
.	O
in	O
fact	O
,	O
many	O
popular	O
machine	B
learning	I
methods	O
—	O
such	O
as	O
support	B
vector	I
machines	I
,	O
neural	B
networks	I
,	O
classiﬁcation	B
and	O
regression	B
trees	O
,	O
etc	O
.	O
—	O
can	O
be	O
seen	O
as	O
just	O
different	O
ways	O
of	O
estimating	O
basis	B
functions	I
from	O
data	O
,	O
as	O
we	O
discuss	O
in	O
chapters	O
14	O
and	O
16	O
.	O
8.	O
carl	O
friedrich	O
gauss	O
(	O
1777–1855	O
)	O
was	O
a	O
german	O
mathematician	O
and	O
physicist	O
.	O
1.4.	O
some	O
basic	O
concepts	O
in	O
machine	B
learning	I
21	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
−10	O
−5	O
0	O
(	O
a	O
)	O
5	O
10	O
460	O
480	O
500	O
520	O
540	O
560	O
580	O
600	O
620	O
640	O
(	O
b	O
)	O
figure	O
1.19	O
(	O
a	O
)	O
the	O
sigmoid	B
or	O
logistic	B
function	O
.	O
we	O
have	O
sigm	O
(	O
−∞	O
)	O
=	O
0	O
,	O
sigm	O
(	O
0	O
)	O
=	O
0.5	O
,	O
and	O
sigm	O
(	O
∞	O
)	O
=	O
1.	O
figure	O
generated	O
by	O
sigmoidplot	O
.	O
(	O
b	O
)	O
logistic	B
regression	I
for	O
sat	O
scores	B
.	O
solid	O
black	O
dots	O
are	O
the	O
data	O
.	O
the	O
open	O
red	O
circles	O
are	O
the	O
predicted	O
probabilities	O
.	O
the	O
green	O
crosses	O
denote	O
two	O
students	O
with	O
the	O
same	O
sat	O
score	O
of	O
525	O
(	O
and	O
hence	O
same	O
input	O
representation	O
x	O
)	O
but	O
with	O
different	O
training	O
labels	O
(	O
one	O
student	O
passed	O
,	O
y	O
=	O
1	O
,	O
the	O
other	O
failed	O
,	O
y	O
=	O
0	O
)	O
.	O
hence	O
this	O
data	O
is	O
not	O
perfectly	O
separable	O
using	O
just	O
the	O
sat	O
feature	O
.	O
figure	O
generated	O
by	O
logregsatdemo	O
.	O
1.4.6	O
logistic	B
regression	I
we	O
can	O
generalize	B
linear	O
regression	B
to	O
the	O
(	O
binary	O
)	O
classiﬁcation	B
setting	O
by	O
making	O
two	O
changes	O
.	O
first	O
we	O
replace	O
the	O
gaussian	O
distribution	O
for	O
y	O
with	O
a	O
bernoulli	O
distribution9	O
,	O
which	O
is	O
more	O
appropriate	O
for	O
the	O
case	O
when	O
the	O
response	O
is	O
binary	O
,	O
y	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
that	O
is	O
,	O
we	O
use	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
ber	O
(	O
y|μ	O
(	O
x	O
)	O
)	O
(	O
1.8	O
)	O
where	O
μ	O
(	O
x	O
)	O
=	O
e	O
[	O
y|x	O
]	O
=p	O
(	O
y	O
=	O
1|x	O
)	O
.	O
second	O
,	O
we	O
compute	O
a	O
linear	O
combination	O
of	O
the	O
inputs	O
,	O
as	O
before	O
,	O
but	O
then	O
we	O
pass	O
this	O
through	O
a	O
function	O
that	O
ensures	O
0	O
≤	O
μ	O
(	O
x	O
)	O
≤	O
1	O
by	O
deﬁning	O
μ	O
(	O
x	O
)	O
=	O
sigm	O
(	O
wt	O
x	O
)	O
(	O
1.9	O
)	O
where	O
sigm	O
(	O
η	O
)	O
refers	O
to	O
the	O
sigmoid	B
function	O
,	O
also	O
known	O
as	O
the	O
logistic	B
or	O
logit	B
function	O
.	O
this	O
is	O
deﬁned	O
as	O
sigm	O
(	O
η	O
)	O
(	O
cid:2	O
)	O
1	O
1	O
+	O
exp	O
(	O
−η	O
)	O
=	O
eη	O
eη	O
+	O
1	O
(	O
1.10	O
)	O
the	O
term	O
“	O
sigmoid	B
”	O
means	O
s-shaped	O
:	O
see	O
figure	O
1.19	O
(	O
a	O
)	O
for	O
a	O
plot	O
.	O
it	O
is	O
also	O
known	O
as	O
a	O
squashing	B
function	I
,	O
since	O
it	O
maps	O
the	O
whole	O
real	O
line	O
to	O
[	O
0	O
,	O
1	O
]	O
,	O
which	O
is	O
necessary	O
for	O
the	O
output	O
to	O
be	O
interpreted	O
as	O
a	O
probability	O
.	O
putting	O
these	O
two	O
steps	O
together	O
we	O
get	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
ber	O
(	O
y|sigm	O
(	O
wt	O
x	O
)	O
)	O
(	O
1.11	O
)	O
this	O
is	O
called	O
logistic	B
regression	I
due	O
to	O
its	O
similarity	O
to	O
linear	B
regression	I
(	O
although	O
it	O
is	O
a	O
form	O
of	O
classiﬁcation	B
,	O
not	O
regression	O
!	O
)	O
.	O
9.	O
daniel	O
bernoulli	O
(	O
1700–1782	O
)	O
was	O
a	O
dutch-swiss	O
mathematician	O
and	O
physicist	O
.	O
22	O
chapter	O
1.	O
introduction	O
a	O
simple	O
example	O
of	O
logistic	B
regression	I
is	O
shown	O
in	O
figure	O
1.19	O
(	O
b	O
)	O
,	O
where	O
we	O
plot	O
p	O
(	O
yi	O
=	O
1|xi	O
,	O
w	O
)	O
=	O
sigm	O
(	O
w0	O
+	O
w1xi	O
)	O
(	O
1.12	O
)	O
where	O
xi	O
is	O
the	O
sat10	O
score	O
of	O
student	O
i	O
and	O
yi	O
is	O
whether	O
they	O
passed	O
or	O
failed	O
a	O
class	O
.	O
the	O
solid	O
black	O
dots	O
show	O
the	O
training	O
data	O
,	O
and	O
the	O
red	O
circles	O
plot	O
p	O
(	O
y	O
=	O
1|xi	O
,	O
ˆw	O
)	O
,	O
where	O
ˆw	O
are	O
the	O
parameters	O
estimated	O
from	O
the	O
training	O
data	O
(	O
we	O
discuss	O
how	O
to	O
compute	O
these	O
estimates	O
in	O
section	O
8.3.4	O
)	O
.	O
if	O
we	O
threshold	O
the	O
output	O
probability	O
at	O
0.5	O
,	O
we	O
can	O
induce	O
a	O
decision	B
rule	I
of	O
the	O
form	O
ˆy	O
(	O
x	O
)	O
=	O
1	O
⇐⇒	O
p	O
(	O
y	O
=	O
1|x	O
)	O
>	O
0.5	O
(	O
1.13	O
)	O
by	O
looking	O
at	O
figure	O
1.19	O
(	O
b	O
)	O
,	O
we	O
see	O
that	O
sigm	O
(	O
w0	O
+	O
w1x	O
)	O
=	O
0.5	O
for	O
x	O
≈	O
545	O
=	O
x∗	O
.	O
we	O
can	O
imagine	O
drawing	O
a	O
vertical	O
line	O
at	O
x	O
=	O
x∗	O
;	O
this	O
is	O
known	O
as	O
a	O
decision	B
boundary	I
.	O
everything	O
to	O
the	O
left	O
of	O
this	O
line	O
is	O
classiﬁed	O
as	O
a	O
0	O
,	O
and	O
everything	O
to	O
the	O
right	O
of	O
the	O
line	O
is	O
classiﬁed	O
as	O
a	O
1.	O
we	O
notice	O
that	O
this	O
decision	B
rule	I
has	O
a	O
non-zero	O
error	O
rate	O
even	O
on	O
the	O
training	B
set	I
.	O
this	O
is	O
because	O
the	O
data	O
is	O
not	O
linearly	O
separable	O
,	O
i.e.	O
,	O
there	O
is	O
no	O
straight	O
line	O
we	O
can	O
draw	O
to	O
separate	O
the	O
0s	O
from	O
the	O
1s	O
.	O
we	O
can	O
create	O
models	O
with	O
non-linear	O
decision	B
boundaries	O
using	O
basis	B
function	I
expansion	I
,	O
just	O
as	O
we	O
did	O
with	O
non-linear	O
regression	B
.	O
we	O
will	O
see	O
many	O
examples	O
of	O
this	O
later	O
in	O
the	O
book	O
.	O
1.4.7	O
overﬁtting	B
when	O
we	O
ﬁt	O
highly	O
ﬂexible	O
models	O
,	O
we	O
need	O
to	O
be	O
careful	O
that	O
we	O
do	O
not	O
overﬁt	B
the	O
data	O
,	O
that	O
is	O
,	O
we	O
should	O
avoid	O
trying	O
to	O
model	O
every	O
minor	O
variation	O
in	O
the	O
input	O
,	O
since	O
this	O
is	O
more	O
likely	O
to	O
be	O
noise	O
than	O
true	O
signal	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
1.18	O
(	O
b	O
)	O
,	O
where	O
we	O
see	O
that	O
using	O
a	O
high	O
degree	O
polynomial	O
results	O
in	O
a	O
curve	O
that	O
is	O
very	O
“	O
wiggly	O
”	O
.	O
it	O
is	O
unlikely	O
that	O
the	O
true	O
function	O
has	O
such	O
extreme	O
oscillations	O
.	O
thus	O
using	O
such	O
a	O
model	O
might	O
result	O
in	O
accurate	O
predictions	O
of	O
future	O
outputs	O
.	O
as	O
another	O
example	O
,	O
consider	O
the	O
knn	O
classiﬁer	O
.	O
the	O
value	O
of	O
k	O
can	O
have	O
a	O
large	O
effect	O
on	O
the	O
behavior	O
of	O
this	O
model	O
.	O
when	O
k	O
=	O
1	O
,	O
the	O
method	O
makes	O
no	O
errors	O
on	O
the	O
training	B
set	I
(	O
since	O
we	O
just	O
return	O
the	O
labels	O
of	O
the	O
original	O
training	O
points	O
)	O
,	O
but	O
the	O
resulting	O
prediction	O
surface	O
is	O
very	O
“	O
wiggly	O
”	O
(	O
see	O
figure	O
1.20	O
(	O
a	O
)	O
)	O
.	O
therefore	O
the	O
method	O
may	O
not	O
work	O
well	O
at	O
predicting	O
future	O
in	O
figure	O
1.20	O
(	O
b	O
)	O
,	O
we	O
see	O
that	O
using	O
k	O
=	O
5	O
results	O
in	O
a	O
smoother	O
prediction	O
surface	O
,	O
data	O
.	O
because	O
we	O
are	O
averaging	O
over	O
a	O
larger	O
neighborhood	O
.	O
as	O
k	O
increases	O
,	O
the	O
predictions	O
becomes	O
smoother	O
until	O
,	O
in	O
the	O
limit	O
of	O
k	O
=	O
n	O
,	O
we	O
end	O
up	O
predicting	O
the	O
majority	O
label	B
of	O
the	O
whole	O
data	O
set	O
.	O
below	O
we	O
discuss	O
how	O
to	O
pick	O
the	O
“	O
right	O
”	O
value	O
of	O
k.	O
1.4.8	O
model	B
selection	I
when	O
we	O
have	O
a	O
variety	O
of	O
models	O
of	O
different	O
complexity	O
(	O
e.g.	O
,	O
linear	O
or	O
logistic	B
regression	I
models	O
with	O
different	O
degree	B
polynomials	O
,	O
or	O
knn	O
classiﬁers	O
with	O
different	O
values	O
of	O
k	O
)	O
,	O
how	O
should	O
we	O
pick	O
the	O
right	O
one	O
?	O
a	O
natural	O
approach	O
is	O
to	O
compute	O
the	O
misclassiﬁcation	B
rate	I
on	O
10.	O
sat	O
stands	O
for	O
“	O
scholastic	O
aptitude	O
test	O
”	O
.	O
this	O
is	O
a	O
standardized	B
test	O
for	O
college	O
admissions	O
used	O
in	O
the	O
united	O
states	O
(	O
the	O
data	O
in	O
this	O
example	O
is	O
from	O
(	O
johnson	O
and	O
albert	O
1999	O
,	O
p87	O
)	O
)	O
.	O
1.4.	O
some	O
basic	O
concepts	O
in	O
machine	B
learning	I
23	O
predicted	O
label	B
,	O
k=1	O
predicted	O
label	B
,	O
k=5	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
c1	O
c2	O
c3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
(	O
a	O
)	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
c1	O
c2	O
c3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
(	O
b	O
)	O
figure	O
1.20	O
prediction	O
surface	O
for	O
knn	O
on	O
the	O
data	O
in	O
figure	O
1.15	O
(	O
a	O
)	O
.	O
(	O
a	O
)	O
k=1	O
.	O
(	O
b	O
)	O
k=5	O
.	O
figure	O
generated	O
by	O
knnclassifydemo	O
.	O
the	O
training	B
set	I
for	O
each	O
method	O
.	O
this	O
is	O
deﬁned	O
as	O
follows	O
:	O
n	O
(	O
cid:2	O
)	O
i=1	O
err	O
(	O
f	O
,	O
d	O
)	O
=	O
1	O
n	O
i	O
(	O
f	O
(	O
xi	O
)	O
(	O
cid:10	O
)	O
=	O
yi	O
)	O
(	O
1.14	O
)	O
where	O
f	O
(	O
x	O
)	O
is	O
our	O
classiﬁer	O
.	O
in	O
figure	O
1.21	O
(	O
a	O
)	O
,	O
we	O
plot	O
this	O
error	O
rate	O
vs	O
k	O
for	O
a	O
knn	O
classiﬁer	O
(	O
dotted	O
blue	O
line	O
)	O
.	O
we	O
see	O
that	O
increasing	O
k	O
increases	O
our	O
error	O
rate	O
on	O
the	O
training	B
set	I
,	O
because	O
we	O
are	O
over-smoothing	O
.	O
as	O
we	O
said	O
above	O
,	O
we	O
can	O
get	O
minimal	B
error	O
on	O
the	O
training	B
set	I
by	O
using	O
k	O
=	O
1	O
,	O
since	O
this	O
model	O
is	O
just	O
memorizing	O
the	O
data	O
.	O
however	O
,	O
what	O
we	O
care	O
about	O
is	O
generalization	B
error	I
,	O
which	O
is	O
the	O
expected	B
value	I
of	O
the	O
misclassiﬁcation	B
rate	I
when	O
averaged	O
over	O
future	O
data	O
(	O
see	O
section	O
6.3	O
for	O
details	O
)	O
.	O
this	O
can	O
be	O
approximated	O
by	O
computing	O
the	O
misclassiﬁcation	B
rate	I
on	O
a	O
large	O
independent	O
test	O
set	O
,	O
not	O
used	O
during	O
model	O
training	O
.	O
we	O
plot	O
the	O
test	O
error	O
vs	O
k	O
in	O
figure	O
1.21	O
(	O
a	O
)	O
in	O
solid	O
red	O
(	O
upper	O
curve	O
)	O
.	O
now	O
we	O
see	O
a	O
u-shaped	O
curve	O
:	O
for	O
complex	O
models	O
(	O
small	O
k	O
)	O
,	O
the	O
method	O
overﬁts	O
,	O
and	O
for	O
simple	O
models	O
(	O
big	O
k	O
)	O
,	O
the	O
method	O
underﬁts	B
.	O
therefore	O
,	O
an	O
obvious	O
way	O
to	O
pick	O
k	O
is	O
to	O
pick	O
the	O
value	O
with	O
the	O
minimum	O
error	O
on	O
the	O
test	O
set	O
(	O
in	O
this	O
example	O
,	O
any	O
value	O
between	O
10	O
and	O
100	O
should	O
be	O
ﬁne	O
)	O
.	O
unfortunately	O
,	O
when	O
training	O
the	O
model	O
,	O
we	O
don	O
’	O
t	O
have	O
access	O
to	O
the	O
test	O
set	O
(	O
by	O
assumption	O
)	O
,	O
so	O
we	O
can	O
not	O
use	O
the	O
test	O
set	O
to	O
pick	O
the	O
model	O
of	O
the	O
right	O
complexity.11	O
however	O
,	O
we	O
can	O
create	O
a	O
test	O
set	O
by	O
partitioning	B
the	O
training	B
set	I
into	O
two	O
:	O
the	O
part	O
used	O
for	O
training	O
the	O
model	O
,	O
and	O
a	O
second	O
part	O
,	O
called	O
the	O
validation	B
set	I
,	O
used	O
for	O
selecting	O
the	O
model	O
complexity	O
.	O
we	O
then	O
ﬁt	O
all	O
the	O
models	O
on	O
the	O
training	B
set	I
,	O
and	O
evaluate	O
their	O
performance	O
on	O
the	O
validation	B
set	I
,	O
and	O
pick	O
the	O
best	O
.	O
once	O
we	O
have	O
picked	O
the	O
best	O
,	O
we	O
can	O
reﬁt	O
it	O
to	O
all	O
the	O
available	O
data	O
.	O
if	O
we	O
have	O
a	O
separate	O
test	O
set	O
,	O
we	O
can	O
evaluate	O
performance	O
on	O
this	O
,	O
in	O
order	O
to	O
estimate	O
the	O
accuracy	O
of	O
our	O
method	O
.	O
(	O
we	O
discuss	O
this	O
in	O
more	O
detail	O
in	O
section	O
6.5.3	O
.	O
)	O
often	O
we	O
use	O
about	O
80	O
%	O
of	O
the	O
data	O
for	O
the	O
training	B
set	I
,	O
and	O
20	O
%	O
for	O
the	O
validation	B
set	I
.	O
but	O
if	O
the	O
number	O
of	O
training	O
cases	O
is	O
small	O
,	O
this	O
technique	O
runs	O
into	O
problems	O
,	O
because	O
the	O
model	O
11.	O
in	O
academic	O
settings	O
,	O
we	O
usually	O
do	O
have	O
access	O
to	O
the	O
test	O
set	O
,	O
but	O
we	O
should	O
not	O
use	O
it	O
for	O
model	O
ﬁtting	O
or	O
model	B
selection	I
,	O
otherwise	O
we	O
will	O
get	O
an	O
unrealistically	O
optimistic	O
estimate	O
of	O
performance	O
of	O
our	O
method	O
.	O
this	O
is	O
one	O
of	O
the	O
“	O
golden	O
rules	B
”	O
of	O
machine	B
learning	I
research	O
.	O
24	O
e	O
t	O
a	O
r	O
n	O
o	O
i	O
t	O
a	O
c	O
i	O
f	O
i	O
s	O
s	O
a	O
c	O
s	O
m	O
i	O
l	O
chapter	O
1.	O
introduction	O
train	O
test	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
0	O
20	O
40	O
60	O
k	O
(	O
a	O
)	O
80	O
100	O
120	O
(	O
b	O
)	O
(	O
a	O
)	O
misclassiﬁcation	B
rate	I
vs	O
k	O
in	O
a	O
k-nearest	O
neighbor	O
classiﬁer	O
.	O
on	O
the	O
left	O
,	O
where	O
k	O
is	O
figure	O
1.21	O
small	O
,	O
the	O
model	O
is	O
complex	O
and	O
hence	O
we	O
overﬁt	B
.	O
on	O
the	O
right	O
,	O
where	O
k	O
is	O
large	O
,	O
the	O
model	O
is	O
simple	O
and	O
we	O
underﬁt	O
.	O
dotted	O
blue	O
line	O
:	O
training	B
set	I
(	O
size	O
200	O
)	O
.	O
solid	O
red	O
line	O
:	O
test	O
set	O
(	O
size	O
500	O
)	O
.	O
(	O
b	O
)	O
schematic	O
of	O
5-fold	O
cross	B
validation	I
.	O
figure	O
generated	O
by	O
knnclassifydemo	O
.	O
won	O
’	O
t	O
have	O
enough	O
data	O
to	O
train	O
on	O
,	O
and	O
we	O
won	O
’	O
t	O
have	O
enough	O
data	O
to	O
make	O
a	O
reliable	O
estimate	O
of	O
the	O
future	O
performance	O
.	O
a	O
simple	O
but	O
popular	O
solution	O
to	O
this	O
is	O
to	O
use	O
cross	B
validation	I
(	O
cv	O
)	O
.	O
the	O
idea	O
is	O
simple	O
:	O
we	O
split	O
the	O
training	O
data	O
into	O
k	O
folds	B
;	O
then	O
,	O
for	O
each	O
fold	O
k	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
we	O
train	O
on	O
all	O
the	O
folds	B
but	O
the	O
k	O
’	O
th	O
,	O
and	O
test	O
on	O
the	O
k	O
’	O
th	O
,	O
in	O
a	O
round-robin	O
fashion	O
,	O
as	O
sketched	O
in	O
figure	O
1.21	O
(	O
b	O
)	O
.	O
we	O
then	O
compute	O
the	O
error	O
averaged	O
over	O
all	O
the	O
folds	B
,	O
and	O
use	O
this	O
as	O
a	O
proxy	O
for	O
the	O
test	O
error	O
.	O
(	O
note	O
that	O
each	O
point	O
gets	O
predicted	O
only	O
once	O
,	O
although	O
it	O
will	O
be	O
used	O
for	O
training	O
k−1	O
times	O
.	O
)	O
it	O
is	O
common	O
to	O
use	O
k	O
=	O
5	O
;	O
this	O
is	O
called	O
5-fold	O
cv	O
.	O
if	O
we	O
set	O
k	O
=	O
n	O
,	O
then	O
we	O
get	O
a	O
method	O
called	O
leave-one	B
out	I
cross	I
validation	I
,	O
or	O
loocv	O
,	O
since	O
in	O
fold	O
i	O
,	O
we	O
train	O
on	O
all	O
the	O
data	O
cases	O
except	O
for	O
i	O
,	O
and	O
then	O
test	O
on	O
i.	O
exercise	O
1.3	O
asks	O
you	O
to	O
compute	O
the	O
5-fold	O
cv	O
estimate	O
of	O
the	O
test	O
error	O
vs	O
k	O
,	O
and	O
to	O
compare	O
it	O
to	O
the	O
empirical	O
test	O
error	O
in	O
figure	O
1.21	O
(	O
a	O
)	O
.	O
choosing	O
k	O
for	O
a	O
knn	O
classiﬁer	O
is	O
a	O
special	O
case	O
of	O
a	O
more	O
general	O
problem	O
known	O
as	O
model	B
selection	I
,	O
where	O
we	O
have	O
to	O
choose	O
between	O
models	O
with	O
different	O
degrees	O
of	O
ﬂexibility	O
.	O
cross-	O
validation	O
is	O
widely	O
used	O
for	O
solving	O
such	O
problems	O
,	O
although	O
we	O
will	O
discuss	O
other	O
approaches	O
later	O
in	O
the	O
book	O
.	O
1.4.9	O
no	B
free	I
lunch	I
theorem	I
all	O
models	O
are	O
wrong	O
,	O
but	O
some	O
models	O
are	O
useful	O
.	O
—	O
george	O
box	O
(	O
box	O
and	O
draper	O
1987	O
,	O
p424	O
)	O
.12	O
much	O
of	O
machine	B
learning	I
is	O
concerned	O
with	O
devising	O
different	O
models	O
,	O
and	O
different	O
algorithms	O
to	O
ﬁt	O
them	O
.	O
we	O
can	O
use	O
methods	O
such	O
as	O
cross	B
validation	I
to	O
empirically	O
choose	O
the	O
best	O
method	O
for	O
our	O
particular	O
problem	O
.	O
however	O
,	O
there	O
is	O
no	O
universally	O
best	O
model	O
—	O
this	O
is	O
sometimes	O
called	O
the	O
no	B
free	I
lunch	I
theorem	I
(	O
wolpert	O
1996	O
)	O
.	O
the	O
reason	O
for	O
this	O
is	O
that	O
a	O
set	O
of	O
assumptions	O
that	O
works	O
well	O
in	O
one	O
domain	O
may	O
work	O
poorly	O
in	O
another	O
.	O
12.	O
george	O
box	O
is	O
a	O
retired	O
statistics	O
professor	O
at	O
the	O
university	O
of	O
wisconsin	O
.	O
1.4.	O
some	O
basic	O
concepts	O
in	O
machine	B
learning	I
25	O
as	O
a	O
consequence	O
of	O
the	O
no	B
free	I
lunch	I
theorem	I
,	O
we	O
need	O
to	O
develop	O
many	O
different	O
types	O
of	O
models	O
,	O
to	O
cover	O
the	O
wide	O
variety	O
of	O
data	O
that	O
occurs	O
in	O
the	O
real	O
world	O
.	O
and	O
for	O
each	O
model	O
,	O
there	O
may	O
be	O
many	O
different	O
algorithms	O
we	O
can	O
use	O
to	O
train	O
the	O
model	O
,	O
which	O
make	O
different	O
speed-accuracy-complexity	O
tradeoffs	O
.	O
it	O
is	O
this	O
combination	O
of	O
data	O
,	O
models	O
and	O
algorithms	O
that	O
we	O
will	O
be	O
studying	O
in	O
the	O
subsequent	O
chapters	O
.	O
exercises	O
exercise	O
1.1	O
knn	O
classiﬁer	O
on	O
shuffled	O
mnist	O
data	O
run	O
mnist1nndemo	O
and	O
verify	O
that	O
the	O
misclassiﬁcation	B
rate	I
(	O
on	O
the	O
ﬁrst	O
1000	O
test	O
cases	O
)	O
of	O
mnist	O
of	O
a	O
1-nn	O
classiﬁer	O
is	O
3.8	O
%	O
.	O
(	O
if	O
you	O
run	O
it	O
all	O
on	O
all	O
10,000	O
test	O
cases	O
,	O
the	O
error	O
rate	O
is	O
3.09	O
%	O
.	O
)	O
modify	O
the	O
code	O
so	O
that	O
you	O
ﬁrst	O
randomly	O
permute	O
the	O
features	B
(	O
columns	O
of	O
the	O
training	O
and	O
test	O
design	O
matrices	O
)	O
,	O
as	O
in	O
shuffleddigitsdemo	O
,	O
and	O
then	O
apply	O
the	O
classiﬁer	O
.	O
verify	O
that	O
the	O
error	O
rate	O
is	O
not	O
changed	O
.	O
exercise	O
1.2	O
approximate	O
knn	O
classiﬁers	O
use	O
the	O
matlab/c++	O
code	O
at	O
http	O
:	O
//people.cs.ubc.ca/~mariusm/index.php/flann/flann	O
to	O
per-	O
form	O
approximate	O
nearest	O
neighbor	O
search	O
,	O
and	O
combine	O
it	O
with	O
mnist1nndemo	O
to	O
classify	O
the	O
mnist	O
data	O
set	O
.	O
how	O
much	O
speedup	O
do	O
you	O
get	O
,	O
and	O
what	O
is	O
the	O
drop	O
(	O
if	O
any	O
)	O
in	O
accuracy	O
?	O
exercise	O
1.3	O
cv	O
for	O
knn	O
use	O
knnclassifydemo	O
to	O
plot	O
the	O
cv	O
estimate	O
of	O
the	O
misclassiﬁcation	B
rate	I
on	O
the	O
test	O
set	O
.	O
compare	O
this	O
to	O
figure	O
1.21	O
(	O
a	O
)	O
.	O
discuss	O
the	O
similarities	O
and	O
differences	O
to	O
the	O
test	O
error	O
rate	B
.	O
2	O
probability	O
2.1	O
introduction	O
probability	O
theory	O
is	O
nothing	O
but	O
common	O
sense	O
reduced	O
to	O
calculation	O
.	O
—	O
pierre	O
laplace	O
,	O
1812	O
in	O
the	O
previous	O
chapter	O
,	O
we	O
saw	O
how	O
probability	O
can	O
play	O
a	O
useful	O
role	O
in	O
machine	B
learning	I
.	O
in	O
this	O
chapter	O
,	O
we	O
discuss	O
probability	O
theory	O
in	O
more	O
detail	O
.	O
we	O
do	O
not	O
have	O
to	O
space	O
to	O
go	O
into	O
great	O
detail	O
—	O
for	O
that	O
,	O
you	O
are	O
better	O
off	O
consulting	O
some	O
of	O
the	O
excellent	O
textbooks	O
available	O
on	O
this	O
topic	B
,	O
such	O
as	O
(	O
jaynes	O
2003	O
;	O
bertsekas	O
and	O
tsitsiklis	O
2008	O
;	O
wasserman	O
2004	O
)	O
.	O
but	O
we	O
will	O
brieﬂy	O
review	O
many	O
of	O
the	O
key	O
ideas	O
you	O
will	O
need	O
in	O
later	O
chapters	O
.	O
before	O
we	O
start	O
with	O
the	O
more	O
technical	O
material	O
,	O
let	O
us	O
pause	O
and	O
ask	O
:	O
what	O
is	O
probability	O
?	O
we	O
are	O
all	O
familiar	O
with	O
the	O
phrase	O
“	O
the	O
probability	O
that	O
a	O
coin	O
will	O
land	O
heads	O
is	O
0.5	O
”	O
.	O
but	O
what	O
does	O
this	O
mean	B
?	O
there	O
are	O
actually	O
at	O
least	O
two	O
different	O
interpretations	O
of	O
probability	O
.	O
one	O
is	O
called	O
the	O
frequentist	B
interpretation	O
.	O
in	O
this	O
view	O
,	O
probabilities	O
represent	O
long	O
run	O
frequencies	O
of	O
events	O
.	O
for	O
example	O
,	O
the	O
above	O
statement	O
means	O
that	O
,	O
if	O
we	O
ﬂip	O
the	O
coin	O
many	O
times	O
,	O
we	O
expect	O
it	O
to	O
land	O
heads	O
about	O
half	O
the	O
time.1	O
the	O
other	O
interpretation	O
is	O
called	O
the	O
bayesian	O
interpretation	O
of	O
probability	O
.	O
in	O
this	O
view	O
,	O
probability	O
is	O
used	O
to	O
quantify	O
our	O
uncertainty	B
about	O
something	O
;	O
hence	O
it	O
is	O
fundamentally	O
related	O
to	O
information	B
rather	O
than	O
repeated	O
trials	O
(	O
jaynes	O
2003	O
)	O
.	O
in	O
the	O
bayesian	O
view	O
,	O
the	O
above	O
statement	O
means	O
we	O
believe	O
the	O
coin	O
is	O
equally	O
likely	O
to	O
land	O
heads	O
or	O
tails	O
on	O
the	O
next	O
toss	O
.	O
one	O
big	O
advantage	O
of	O
the	O
bayesian	O
interpretation	O
is	O
that	O
it	O
can	O
be	O
used	O
to	O
model	O
our	O
uncer-	O
tainty	O
about	O
events	O
that	O
do	O
not	O
have	O
long	O
term	O
frequencies	O
.	O
for	O
example	O
,	O
we	O
might	O
want	O
to	O
compute	O
the	O
probability	O
that	O
the	O
polar	B
ice	O
cap	O
will	O
melt	O
by	O
2020	O
ce	O
.	O
this	O
event	O
will	O
happen	O
zero	O
or	O
one	O
times	O
,	O
but	O
can	O
not	O
happen	O
repeatedly	O
.	O
nevertheless	O
,	O
we	O
ought	O
to	O
be	O
able	O
to	O
quantify	O
our	O
uncertainty	B
about	O
this	O
event	O
;	O
based	O
on	O
how	O
probable	O
we	O
think	O
this	O
event	O
is	O
,	O
we	O
will	O
(	O
hopefully	O
!	O
)	O
take	O
appropriate	O
actions	B
(	O
see	O
section	O
5.7	O
for	O
a	O
discussion	O
of	O
optimal	O
decision	O
making	O
under	O
uncertainty	B
)	O
.	O
to	O
give	O
some	O
more	O
machine	B
learning	I
oriented	O
examples	O
,	O
we	O
might	O
have	O
received	O
a	O
speciﬁc	O
email	O
message	O
,	O
and	O
want	O
to	O
compute	O
the	O
probability	O
it	O
is	O
spam	B
.	O
or	O
we	O
might	O
have	O
observed	O
a	O
“	O
blip	O
”	O
on	O
our	O
radar	B
screen	O
,	O
and	O
want	O
to	O
compute	O
the	O
probability	O
distribution	O
over	O
the	O
location	O
of	O
the	O
corresponding	O
target	O
(	O
be	O
it	O
a	O
bird	O
,	O
plane	O
,	O
or	O
missile	O
)	O
.	O
in	O
all	O
these	O
cases	O
,	O
the	O
idea	O
of	O
repeated	O
trials	O
does	O
not	O
make	O
sense	O
,	O
but	O
the	O
bayesian	O
interpretation	O
is	O
valid	O
and	O
indeed	O
1.	O
actually	O
,	O
the	O
stanford	O
statistician	O
(	O
and	O
former	O
professional	O
magician	O
)	O
persi	O
diaconis	O
has	O
shown	O
that	O
a	O
coin	O
is	O
about	O
51	O
%	O
likely	O
to	O
land	O
facing	O
the	O
same	O
way	O
up	O
as	O
it	O
started	O
,	O
due	O
to	O
the	O
physics	O
of	O
the	O
problem	O
(	O
diaconis	O
et	O
al	O
.	O
2007	O
)	O
.	O
28	O
1	O
0.75	O
0.5	O
0.25	O
0	O
0	O
1	O
2	O
3	O
4	O
5	O
chapter	O
2.	O
probability	O
1	O
0.75	O
0.5	O
0.25	O
0	O
0	O
1	O
2	O
3	O
4	O
5	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
a	O
)	O
a	O
uniform	B
distribution	I
on	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
}	O
,	O
with	O
p	O
(	O
x	O
=	O
k	O
)	O
=	O
1/4	O
.	O
(	O
b	O
)	O
a	O
degenerate	B
distribution	O
figure	O
2.1	O
p	O
(	O
x	O
)	O
=	O
1	O
if	O
x	O
=	O
1	O
and	O
p	O
(	O
x	O
)	O
=	O
0	O
if	O
x	O
∈	O
{	O
2	O
,	O
3	O
,	O
4	O
}	O
.	O
figure	O
generated	O
by	O
discreteprobdistfig	O
.	O
quite	O
natural	O
.	O
we	O
shall	O
therefore	O
adopt	O
the	O
bayesian	O
interpretation	O
in	O
this	O
book	O
.	O
fortunately	O
,	O
the	O
basic	O
rules	O
of	O
probability	O
theory	O
are	O
the	O
same	O
,	O
no	O
matter	O
which	O
interpretation	O
is	O
adopted	O
.	O
2.2	O
a	O
brief	O
review	O
of	O
probability	O
theory	O
this	O
section	O
is	O
a	O
very	O
brief	O
review	O
of	O
the	O
basics	O
of	O
probability	O
theory	O
,	O
and	O
is	O
merely	O
meant	O
as	O
a	O
refresher	O
for	O
readers	O
who	O
may	O
be	O
“	O
rusty	O
”	O
.	O
readers	O
who	O
are	O
already	O
familiar	O
with	O
these	O
basics	O
may	O
safely	O
skip	O
this	O
section	O
.	O
2.2.1	O
discrete	O
random	O
variables	O
the	O
expression	O
p	O
(	O
a	O
)	O
denotes	O
the	O
probability	O
that	O
the	O
event	O
a	O
is	O
true	O
.	O
for	O
example	O
,	O
a	O
might	O
be	O
the	O
logical	O
expression	O
“	O
it	O
will	O
rain	O
tomorrow	O
”	O
.	O
we	O
require	O
that	O
0	O
≤	O
p	O
(	O
a	O
)	O
≤	O
1	O
,	O
where	O
p	O
(	O
a	O
)	O
=	O
0	O
means	O
the	O
event	O
deﬁnitely	O
will	O
not	O
happen	O
,	O
and	O
p	O
(	O
a	O
)	O
=	O
1	O
means	O
the	O
event	O
deﬁnitely	O
will	O
happen	O
.	O
we	O
write	O
p	O
(	O
a	O
)	O
to	O
denote	O
the	O
probability	O
of	O
the	O
event	O
not	O
a	O
;	O
this	O
is	O
deﬁned	O
to	O
p	O
(	O
a	O
)	O
=	O
1	O
−	O
p	O
(	O
a	O
)	O
.	O
we	O
will	O
often	O
write	O
a	O
=	O
1	O
to	O
mean	B
the	O
event	O
a	O
is	O
true	O
,	O
and	O
a	O
=	O
0	O
to	O
mean	B
the	O
event	O
a	O
is	O
false	O
.	O
we	O
can	O
extend	O
the	O
notion	O
of	O
binary	O
events	O
by	O
deﬁning	O
a	O
discrete	B
random	I
variable	I
x	O
,	O
which	O
can	O
take	O
on	O
any	O
value	O
from	O
a	O
ﬁnite	O
or	O
countably	O
inﬁnite	O
set	O
x	O
.	O
we	O
denote	O
the	O
probability	O
of	O
the	O
event	O
that	O
x	O
=	O
x	O
by	O
p	O
(	O
x	O
=	O
x	O
)	O
,	O
or	O
just	O
p	O
(	O
x	O
)	O
for	O
short	O
.	O
here	O
p	O
(	O
)	O
is	O
called	O
a	O
probability	B
mass	I
function	I
or	O
pmf	B
.	O
this	O
satisﬁes	O
the	O
properties	O
0	O
≤	O
p	O
(	O
x	O
)	O
≤	O
1	O
and	O
x∈x	O
p	O
(	O
x	O
)	O
=	O
1.	O
figure	O
2.1	O
shows	O
two	O
pmf	B
’	O
s	O
deﬁned	O
on	O
the	O
ﬁnite	O
state	O
space	O
x	O
=	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
}	O
.	O
on	O
the	O
left	O
we	O
have	O
a	O
uniform	B
distribution	I
,	O
p	O
(	O
x	O
)	O
=	O
1/5	O
,	O
and	O
on	O
the	O
right	O
,	O
we	O
have	O
a	O
degenerate	B
distribution	O
,	O
p	O
(	O
x	O
)	O
=	O
i	O
(	O
x	O
=	O
1	O
)	O
,	O
where	O
i	O
(	O
)	O
is	O
the	O
binary	O
indicator	O
function	O
.	O
this	O
distribution	O
represents	O
the	O
fact	O
that	O
x	O
is	O
always	O
equal	O
to	O
the	O
value	O
1	O
,	O
in	O
other	O
words	O
,	O
it	O
is	O
a	O
constant	O
.	O
(	O
cid:2	O
)	O
2.2.2	O
fundamental	O
rules	B
in	O
this	O
section	O
,	O
we	O
review	O
the	O
basic	O
rules	O
of	O
probability	O
.	O
2.2.	O
a	O
brief	O
review	O
of	O
probability	O
theory	O
2.2.2.1	O
probability	O
of	O
a	O
union	O
of	O
two	O
events	O
given	O
two	O
events	O
,	O
a	O
and	O
b	O
,	O
we	O
deﬁne	O
the	O
probability	O
of	O
a	O
or	O
b	O
as	O
follows	O
:	O
p	O
(	O
a	O
∨	O
b	O
)	O
=p	O
(	O
a	O
)	O
+p	O
(	O
b	O
)	O
−	O
p	O
(	O
a	O
∧	O
b	O
)	O
=	O
p	O
(	O
a	O
)	O
+p	O
(	O
b	O
)	O
if	O
a	O
and	O
b	O
are	O
mutually	O
exclusive	O
2.2.2.2	O
joint	O
probabilities	O
we	O
deﬁne	O
the	O
probability	O
of	O
the	O
joint	O
event	O
a	O
and	O
b	O
as	O
follows	O
:	O
p	O
(	O
a	O
,	O
b	O
)	O
=	O
p	O
(	O
a	O
∧	O
b	O
)	O
=	O
p	O
(	O
a|b	O
)	O
p	O
(	O
b	O
)	O
29	O
(	O
2.1	O
)	O
(	O
2.2	O
)	O
(	O
2.3	O
)	O
this	O
is	O
sometimes	O
called	O
the	O
product	B
rule	I
.	O
given	O
a	O
joint	B
distribution	I
on	O
two	O
events	O
p	O
(	O
a	O
,	O
b	O
)	O
,	O
we	O
deﬁne	O
the	O
marginal	B
distribution	I
as	O
follows	O
:	O
p	O
(	O
a|b	O
=	O
b	O
)	O
p	O
(	O
b	O
=	O
b	O
)	O
(	O
2.4	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
p	O
(	O
a	O
)	O
=	O
p	O
(	O
a	O
,	O
b	O
)	O
=	O
b	O
b	O
where	O
we	O
are	O
summing	O
over	O
all	O
possible	O
states	O
of	O
b.	O
we	O
can	O
deﬁne	O
p	O
(	O
b	O
)	O
similarly	O
.	O
this	O
is	O
sometimes	O
called	O
the	O
sum	B
rule	I
or	O
the	O
rule	B
of	I
total	I
probability	I
.	O
the	O
product	B
rule	I
can	O
be	O
applied	O
multiple	O
times	O
to	O
yield	O
the	O
chain	B
rule	I
of	O
probability	O
:	O
p	O
(	O
x1	O
:	O
d	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
x3|x2	O
,	O
x1	O
)	O
p	O
(	O
x4|x1	O
,	O
x2	O
,	O
x3	O
)	O
.	O
.	O
.	O
p	O
(	O
xd|x1	O
:	O
d−1	O
)	O
where	O
we	O
introduce	O
the	O
matlab-like	O
notation	O
1	O
:	O
d	O
to	O
denote	O
the	O
set	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
.	O
(	O
2.5	O
)	O
2.2.2.3	O
conditional	B
probability	I
we	O
deﬁne	O
the	O
conditional	B
probability	I
of	O
event	O
a	O
,	O
given	O
that	O
event	O
b	O
is	O
true	O
,	O
as	O
follows	O
:	O
p	O
(	O
a|b	O
)	O
=	O
p	O
(	O
a	O
,	O
b	O
)	O
p	O
(	O
b	O
)	O
if	O
p	O
(	O
b	O
)	O
>	O
0	O
2.2.3	O
bayes	O
rule	O
(	O
2.6	O
)	O
combining	O
the	O
deﬁnition	O
of	O
conditional	B
probability	I
with	O
the	O
product	O
and	O
sum	O
rules	O
yields	O
bayes	O
rule	O
,	O
also	O
called	O
bayes	O
theorem2	O
:	O
(	O
cid:2	O
)	O
p	O
(	O
x	O
=	O
x	O
)	O
p	O
(	O
y	O
=	O
y|x	O
=	O
x	O
)	O
x	O
(	O
cid:2	O
)	O
p	O
(	O
x	O
=	O
x	O
(	O
cid:3	O
)	O
)	O
p	O
(	O
y	O
=	O
y|x	O
=	O
x	O
(	O
cid:3	O
)	O
)	O
(	O
2.7	O
)	O
p	O
(	O
x	O
=	O
x|y	O
=	O
y	O
)	O
=	O
p	O
(	O
x	O
=	O
x	O
,	O
y	O
=	O
y	O
)	O
p	O
(	O
y	O
=	O
y	O
)	O
=	O
2.2.3.1	O
example	O
:	O
medical	O
diagnosis	O
as	O
an	O
example	O
of	O
how	O
to	O
use	O
this	O
rule	O
,	O
consider	O
the	O
following	O
medical	O
diagonsis	O
problem	O
.	O
suppose	O
you	O
are	O
a	O
woman	O
in	O
your	O
40s	O
,	O
and	O
you	O
decide	O
to	O
have	O
a	O
medical	O
test	O
for	O
breast	O
cancer	O
called	O
a	O
mammogram	B
.	O
if	O
the	O
test	O
is	O
positive	O
,	O
what	O
is	O
the	O
probability	O
you	O
have	O
cancer	O
?	O
that	O
obviously	O
depends	O
on	O
how	O
reliable	O
the	O
test	O
is	O
.	O
suppose	O
you	O
are	O
told	O
the	O
test	O
has	O
a	O
sensitivity	B
2.	O
thomas	O
bayes	O
(	O
1702–1761	O
)	O
was	O
an	O
english	O
mathematician	O
and	O
presbyterian	O
minister	O
.	O
30	O
chapter	O
2.	O
probability	O
of	O
80	O
%	O
,	O
which	O
means	O
,	O
if	O
you	O
have	O
cancer	O
,	O
the	O
test	O
will	O
be	O
positive	O
with	O
probability	O
0.8.	O
in	O
other	O
words	O
,	O
p	O
(	O
x	O
=	O
1|y	O
=	O
1	O
)	O
=	O
0.8	O
(	O
2.8	O
)	O
where	O
x	O
=	O
1	O
is	O
the	O
event	O
the	O
mammogram	B
is	O
positive	O
,	O
and	O
y	O
=	O
1	O
is	O
the	O
event	O
you	O
have	O
breast	O
cancer	O
.	O
many	O
people	O
conclude	O
they	O
are	O
therefore	O
80	O
%	O
likely	O
to	O
have	O
cancer	O
.	O
but	O
this	O
is	O
false	O
!	O
it	O
ignores	O
the	O
prior	O
probability	O
of	O
having	O
breast	O
cancer	O
,	O
which	O
fortunately	O
is	O
quite	O
low	O
:	O
p	O
(	O
y	O
=	O
1	O
)	O
=	O
0.004	O
(	O
2.9	O
)	O
ignoring	O
this	O
prior	O
is	O
called	O
the	O
base	B
rate	I
fallacy	I
.	O
we	O
also	O
need	O
to	O
take	O
into	O
account	O
the	O
fact	O
that	O
the	O
test	O
may	O
be	O
a	O
false	B
positive	I
or	O
false	B
alarm	I
.	O
unfortunately	O
,	O
such	O
false	O
positives	O
are	O
quite	O
likely	O
(	O
with	O
current	O
screening	B
technology	O
)	O
:	O
p	O
(	O
x	O
=	O
1|y	O
=	O
0	O
)	O
=	O
0.1	O
(	O
2.10	O
)	O
combining	O
these	O
three	O
terms	O
using	O
bayes	O
rule	O
,	O
we	O
can	O
compute	O
the	O
correct	O
answer	O
as	O
follows	O
:	O
p	O
(	O
y	O
=	O
1|x	O
=	O
1	O
)	O
=	O
=	O
p	O
(	O
x	O
=	O
1|y	O
=	O
1	O
)	O
p	O
(	O
y	O
=	O
1	O
)	O
p	O
(	O
x	O
=	O
1|y	O
=	O
1	O
)	O
p	O
(	O
y	O
=	O
1	O
)	O
+	O
p	O
(	O
x	O
=	O
1|y	O
=	O
0	O
)	O
p	O
(	O
y	O
=	O
0	O
)	O
0.8	O
×	O
0.004	O
+	O
0.1	O
×	O
0.996	O
0.8	O
×	O
0.004	O
=	O
0.031	O
where	O
p	O
(	O
y	O
=	O
0	O
)	O
=	O
1	O
−	O
p	O
(	O
y	O
=	O
1	O
)	O
=	O
0.996.	O
about	O
a	O
3	O
%	O
chance	O
of	O
actually	O
having	O
breast	O
cancer	O
!	O
3	O
in	O
other	O
words	O
,	O
if	O
you	O
test	O
positive	O
,	O
you	O
only	O
have	O
2.2.3.2	O
example	O
:	O
generative	O
classiﬁers	O
(	O
2.11	O
)	O
(	O
2.12	O
)	O
(	O
2.13	O
)	O
we	O
can	O
generalize	B
the	O
medical	O
diagonosis	O
example	O
to	O
classify	O
feature	O
vectors	O
x	O
of	O
arbitrary	O
type	O
as	O
follows	O
:	O
p	O
(	O
y	O
=	O
c|x	O
,	O
θ	O
)	O
=	O
(	O
cid:2	O
)	O
p	O
(	O
y	O
=	O
c|θ	O
)	O
p	O
(	O
x|y	O
=	O
c	O
,	O
θ	O
)	O
c	O
(	O
cid:2	O
)	O
p	O
(	O
y	O
=	O
c	O
(	O
cid:3	O
)	O
|θ	O
)	O
p	O
(	O
x|y	O
=	O
c	O
(	O
cid:3	O
)	O
,	O
θ	O
)	O
this	O
is	O
called	O
a	O
generative	B
classiﬁer	I
,	O
since	O
it	O
speciﬁes	O
how	O
to	O
generate	O
the	O
data	O
using	O
the	O
class-	O
conditional	O
density	O
p	O
(	O
x|y	O
=	O
c	O
)	O
and	O
the	O
class	O
prior	O
p	O
(	O
y	O
=	O
c	O
)	O
.	O
we	O
discuss	O
such	O
models	O
in	O
detail	O
in	O
chapters	O
3	O
and	O
4.	O
an	O
alternative	O
approach	O
is	O
to	O
directly	O
ﬁt	O
the	O
class	O
posterior	O
,	O
p	O
(	O
y	O
=	O
c|x	O
)	O
;	O
this	O
is	O
known	O
as	O
a	O
discriminative	B
classiﬁer	I
.	O
we	O
discuss	O
the	O
pros	O
and	O
cons	O
of	O
the	O
two	O
approaches	O
in	O
section	O
8.6	O
.	O
2.2.4	O
independence	O
and	O
conditional	B
independence	I
we	O
say	O
x	O
and	O
y	O
are	O
unconditionally	B
independent	I
or	O
marginally	B
independent	I
,	O
denoted	O
x	O
⊥	O
y	O
,	O
if	O
we	O
can	O
represent	O
the	O
joint	O
as	O
the	O
product	O
of	O
the	O
two	O
marginals	O
(	O
see	O
figure	O
2.2	O
)	O
,	O
i.e.	O
,	O
(	O
2.14	O
)	O
x	O
⊥	O
y	O
⇐⇒	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
2.2.	O
a	O
brief	O
review	O
of	O
probability	O
theory	O
31	O
(	O
cid:6	O
)	O
(	O
cid:2	O
)	O
(	O
cid:7	O
)	O
(	O
cid:4	O
)	O
(	O
cid:1	O
)	O
(	O
cid:8	O
)	O
(	O
cid:3	O
)	O
(	O
cid:6	O
)	O
(	O
cid:2	O
)	O
(	O
cid:8	O
)	O
(	O
cid:3	O
)	O
(	O
cid:5	O
)	O
(	O
cid:6	O
)	O
(	O
cid:2	O
)	O
(	O
cid:7	O
)	O
(	O
cid:3	O
)	O
figure	O
2.2	O
computing	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
,	O
where	O
x	O
⊥	O
y	O
.	O
here	O
x	O
and	O
y	O
are	O
discrete	O
random	O
variables	O
;	O
x	O
has	O
6	O
possible	O
states	O
(	O
values	O
)	O
and	O
y	O
has	O
5	O
possible	O
states	O
.	O
a	O
general	O
joint	B
distribution	I
on	O
two	O
such	O
variables	O
would	O
require	O
(	O
6×	O
5	O
)	O
−	O
1	O
=	O
29	O
parameters	O
to	O
deﬁne	O
it	O
(	O
we	O
subtract	O
1	O
because	O
of	O
the	O
sum-to-one	O
constraint	O
)	O
.	O
by	O
assuming	O
(	O
unconditional	O
)	O
independence	O
,	O
we	O
only	O
need	O
(	O
6	O
−	O
1	O
)	O
+	O
(	O
5	O
−	O
1	O
)	O
=	O
9	O
parameters	O
to	O
deﬁne	O
p	O
(	O
x	O
,	O
y	O
)	O
.	O
in	O
general	O
,	O
we	O
say	O
a	O
set	O
of	O
variables	O
is	O
mutually	B
independent	I
if	O
the	O
joint	O
can	O
be	O
written	O
as	O
a	O
product	O
of	O
marginals	O
.	O
unfortunately	O
,	O
unconditional	O
independence	O
is	O
rare	O
,	O
because	O
most	O
variables	O
can	O
inﬂuence	O
most	O
other	O
variables	O
.	O
however	O
,	O
usually	O
this	O
inﬂuence	O
is	O
mediated	O
via	O
other	O
variables	O
rather	O
than	O
being	O
direct	O
.	O
we	O
therefore	O
say	O
x	O
and	O
y	O
are	O
conditionally	B
independent	I
(	O
ci	O
)	O
given	O
z	O
iff	B
the	O
conditional	O
joint	O
can	O
be	O
written	O
as	O
a	O
product	O
of	O
conditional	O
marginals	O
:	O
x	O
⊥	O
y	O
|z	O
⇐⇒	O
p	O
(	O
x	O
,	O
y	O
|z	O
)	O
=	O
p	O
(	O
x|z	O
)	O
p	O
(	O
y	O
|z	O
)	O
(	O
2.15	O
)	O
when	O
we	O
discuss	O
graphical	O
models	O
in	O
chapter	O
10	O
,	O
we	O
will	O
see	O
that	O
we	O
can	O
write	O
this	O
assumption	O
as	O
a	O
graph	B
x	O
−z−y	O
,	O
which	O
captures	O
the	O
intuition	O
that	O
all	O
the	O
dependencies	O
between	O
x	O
and	O
y	O
are	O
mediated	O
via	O
z.	O
for	O
example	O
,	O
the	O
probability	O
it	O
will	O
rain	O
tomorrow	O
(	O
event	O
x	O
)	O
is	O
independent	O
of	O
whether	O
the	O
ground	O
is	O
wet	O
today	O
(	O
event	O
y	O
)	O
,	O
given	O
knowledge	O
of	O
whether	O
it	O
is	O
raining	O
today	O
(	O
event	O
z	O
)	O
.	O
intuitively	O
,	O
this	O
is	O
because	O
z	O
“	O
causes	O
”	O
both	O
x	O
and	O
y	O
,	O
so	O
if	O
we	O
know	O
z	O
,	O
we	O
do	O
not	O
need	O
to	O
know	O
about	O
y	O
in	O
order	O
to	O
predict	O
x	O
or	O
vice	O
versa	O
.	O
we	O
shall	O
expand	O
on	O
this	O
concept	B
in	O
chapter	O
10.	O
another	O
characterization	O
of	O
ci	O
is	O
this	O
:	O
theorem	O
2.2.1.	O
x	O
⊥	O
y	O
|z	O
iff	B
there	O
exist	O
function	O
g	O
and	O
h	O
such	O
that	O
p	O
(	O
x	O
,	O
y|z	O
)	O
=	O
g	O
(	O
x	O
,	O
z	O
)	O
h	O
(	O
y	O
,	O
z	O
)	O
for	O
all	O
x	O
,	O
y	O
,	O
z	O
such	O
that	O
p	O
(	O
z	O
)	O
>	O
0	O
.	O
(	O
2.16	O
)	O
3.	O
these	O
numbers	O
are	O
from	O
(	O
mcgrayne	O
2011	O
,	O
p257	O
)	O
.	O
based	O
on	O
this	O
analysis	O
,	O
the	O
us	O
government	O
decided	O
not	O
to	O
recommend	O
annual	O
mammogram	B
screening	O
to	O
women	O
in	O
their	O
40s	O
:	O
the	O
number	O
of	O
false	O
alarms	O
would	O
cause	O
needless	O
worry	O
and	O
stress	O
amongst	O
women	O
,	O
and	O
result	O
in	O
unnecesssary	O
,	O
expensive	O
,	O
and	O
potentially	O
harmful	O
followup	O
tests	O
.	O
see	O
section	O
5.7	O
for	O
the	O
optimal	O
way	O
to	O
trade	O
off	O
risk	B
reverse	O
reward	B
in	O
the	O
face	O
of	O
uncertainty	B
.	O
32	O
chapter	O
2.	O
probability	O
see	O
exercise	O
2.8	O
for	O
the	O
proof	O
.	O
ci	O
assumptions	O
allow	O
us	O
to	O
build	O
large	O
probabilistic	O
models	O
from	O
small	O
pieces	O
.	O
we	O
will	O
see	O
many	O
examples	O
of	O
this	O
throughout	O
the	O
book	O
.	O
in	O
particular	O
,	O
in	O
section	O
3.5	O
,	O
we	O
discuss	O
naive	O
bayes	O
classiﬁers	O
,	O
in	O
section	O
17.2	O
,	O
we	O
discuss	O
markov	O
models	O
,	O
and	O
in	O
chapter	O
10	O
we	O
discuss	O
graphical	O
models	O
;	O
all	O
of	O
these	O
models	O
heavily	O
exploit	O
ci	O
properties	O
.	O
2.2.5	O
continuous	O
random	O
variables	O
so	O
far	O
,	O
we	O
have	O
only	O
considered	O
reasoning	O
about	O
uncertain	O
discrete	B
quantities	O
.	O
we	O
will	O
now	O
show	O
(	O
following	O
(	O
jaynes	O
2003	O
,	O
p107	O
)	O
)	O
how	O
to	O
extend	O
probability	O
to	O
reason	O
about	O
uncertain	O
continuous	O
quantities	O
.	O
suppose	O
x	O
is	O
some	O
uncertain	O
continuous	O
quantity	O
.	O
the	O
probability	O
that	O
x	O
lies	O
in	O
any	O
interval	O
a	O
≤	O
x	O
≤	O
b	O
can	O
be	O
computed	O
as	O
follows	O
.	O
deﬁne	O
the	O
events	O
a	O
=	O
(	O
x	O
≤	O
a	O
)	O
,	O
b	O
=	O
(	O
x	O
≤	O
b	O
)	O
and	O
w	O
=	O
(	O
a	O
<	O
x	O
≤	O
b	O
)	O
.	O
we	O
have	O
that	O
b	O
=	O
a	O
∨	O
w	O
,	O
and	O
since	O
a	O
and	O
w	O
are	O
mutually	O
exclusive	O
,	O
the	O
sum	O
rules	O
gives	O
p	O
(	O
b	O
)	O
=	O
p	O
(	O
a	O
)	O
+p	O
(	O
w	O
)	O
and	O
hence	O
(	O
2.17	O
)	O
p	O
(	O
w	O
)	O
=	O
p	O
(	O
b	O
)	O
−	O
p	O
(	O
a	O
)	O
(	O
2.18	O
)	O
deﬁne	O
the	O
function	O
f	O
(	O
q	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
x	O
≤	O
q	O
)	O
.	O
this	O
is	O
called	O
the	O
cumulative	B
distribution	I
function	I
or	O
cdf	B
of	O
x.	O
this	O
is	O
obviously	O
a	O
monotonically	O
increasing	O
function	O
.	O
see	O
figure	O
2.3	O
(	O
a	O
)	O
for	O
an	O
example	O
.	O
using	O
this	O
notation	O
we	O
have	O
p	O
(	O
a	O
<	O
x	O
≤	O
b	O
)	O
=	O
f	O
(	O
b	O
)	O
−	O
f	O
(	O
a	O
)	O
(	O
2.19	O
)	O
now	O
deﬁne	O
f	O
(	O
x	O
)	O
=	O
d	O
dx	O
f	O
(	O
x	O
)	O
(	O
we	O
assume	O
this	O
derivative	O
exists	O
)	O
;	O
this	O
is	O
called	O
the	O
probability	B
density	I
function	I
or	O
pdf	B
.	O
see	O
figure	O
2.3	O
(	O
b	O
)	O
for	O
an	O
example	O
.	O
given	O
a	O
pdf	B
,	O
we	O
can	O
compute	O
the	O
probability	O
of	O
a	O
continuous	O
variable	O
being	O
in	O
a	O
ﬁnite	O
interval	O
as	O
follows	O
:	O
(	O
cid:4	O
)	O
b	O
p	O
(	O
a	O
<	O
x	O
≤	O
b	O
)	O
=	O
f	O
(	O
x	O
)	O
dx	O
a	O
(	O
2.20	O
)	O
as	O
the	O
size	O
of	O
the	O
interval	O
gets	O
smaller	O
,	O
we	O
can	O
write	O
p	O
(	O
x	O
≤	O
x	O
≤	O
x	O
+	O
dx	O
)	O
≈	O
p	O
(	O
x	O
)	O
dx	O
(	O
2.21	O
)	O
we	O
require	O
p	O
(	O
x	O
)	O
≥	O
0	O
,	O
but	O
it	O
is	O
possible	O
for	O
p	O
(	O
x	O
)	O
>	O
1	O
for	O
any	O
given	O
x	O
,	O
so	O
long	O
as	O
the	O
density	O
integrates	O
to	O
1.	O
as	O
an	O
example	O
,	O
consider	O
the	O
uniform	B
distribution	I
unif	O
(	O
a	O
,	O
b	O
)	O
:	O
unif	O
(	O
x|a	O
,	O
b	O
)	O
=	O
1	O
b	O
−	O
a	O
if	O
we	O
set	O
a	O
=	O
0	O
and	O
b	O
=	O
1	O
i	O
(	O
a	O
≤	O
x	O
≤	O
b	O
)	O
2	O
,	O
we	O
havep	O
(	O
x	O
)	O
=	O
2	O
for	O
any	O
x	O
∈	O
[	O
0	O
,	O
1	O
2	O
]	O
.	O
(	O
2.22	O
)	O
2.2.	O
a	O
brief	O
review	O
of	O
probability	O
theory	O
33	O
cdf	B
100	O
80	O
60	O
40	O
20	O
0	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
(	O
a	O
)	O
α/2	O
φ−1	O
(	O
α/2	O
)	O
0	O
(	O
b	O
)	O
α/2	O
φ−1	O
(	O
1−α/2	O
)	O
figure	O
2.3	O
(	O
a	O
)	O
plot	O
of	O
the	O
cdf	B
for	O
the	O
standard	B
normal	I
,	O
n	O
(	O
0	O
,	O
1	O
)	O
.	O
(	O
b	O
)	O
corresponding	O
pdf	B
.	O
the	O
shaded	O
regions	O
each	O
contain	O
α/2	O
of	O
the	O
probability	O
mass	O
.	O
therefore	O
the	O
nonshaded	O
region	O
contains	O
1	O
−	O
α	O
of	O
the	O
probability	O
mass	O
.	O
if	O
the	O
distribution	O
is	O
gaussian	O
n	O
(	O
0	O
,	O
1	O
)	O
,	O
then	O
the	O
leftmost	O
cutoff	O
point	O
is	O
φ	O
−1	O
(	O
α/2	O
)	O
,	O
where	O
−1	O
(	O
α/2	O
)	O
.	O
if	O
φ	O
is	O
the	O
cdf	B
of	O
the	O
gaussian	O
.	O
by	O
symmetry	O
,	O
the	O
rightost	O
cutoff	O
point	O
is	O
φ	O
α	O
=	O
0.05	O
,	O
the	O
central	B
interval	I
is	O
95	O
%	O
,	O
and	O
the	O
left	O
cutoff	O
is	O
-1.96	O
and	O
the	O
right	O
is	O
1.96.	O
figure	O
generated	O
by	O
quantiledemo	O
.	O
−1	O
(	O
1	O
−	O
α/2	O
)	O
=	O
−φ	O
2.2.6	O
quantiles	O
since	O
the	O
cdf	B
f	O
is	O
a	O
monotonically	O
increasing	O
function	O
,	O
it	O
has	O
an	O
inverse	O
;	O
let	O
us	O
denote	O
this	O
by	O
−1	O
(	O
α	O
)	O
is	O
the	O
value	O
of	O
xα	O
such	O
that	O
p	O
(	O
x	O
≤	O
xα	O
)	O
=	O
α	O
;	O
this	O
is	O
−1	O
.	O
if	O
f	O
is	O
the	O
cdf	B
of	O
x	O
,	O
then	O
f	O
f	O
−1	O
(	O
0.5	O
)	O
is	O
the	O
median	B
of	O
the	O
distribution	O
,	O
with	O
half	O
of	O
called	O
the	O
α	O
quantile	B
of	O
f	O
.	O
the	O
value	O
f	O
−1	O
(	O
0.75	O
)	O
the	O
probability	O
mass	O
on	O
the	O
left	O
,	O
and	O
half	O
on	O
the	O
right	O
.	O
the	O
values	O
f	O
are	O
the	O
lower	O
and	O
upper	O
quartiles	B
.	O
the	O
cdf	B
of	O
the	O
gaussian	O
distribution	O
n	O
(	O
0	O
,	O
1	O
)	O
,	O
then	O
points	O
to	O
the	O
left	O
of	O
φ	O
probability	O
mass	O
,	O
as	O
illustrated	O
in	O
figure	O
2.3	O
(	O
b	O
)	O
.	O
by	O
symmetry	O
,	O
points	O
to	O
the	O
right	O
of	O
φ	O
−1	O
(	O
α/2	O
)	O
,	O
φ	O
also	O
contain	O
α/2	O
of	O
the	O
mass	O
.	O
hence	O
the	O
central	B
interval	I
(	O
φ	O
1	O
−	O
α	O
of	O
the	O
mass	O
.	O
if	O
we	O
set	O
α	O
=	O
0.05	O
,	O
the	O
central	O
95	O
%	O
interval	O
is	O
covered	O
by	O
the	O
range	O
we	O
can	O
also	O
use	O
the	O
inverse	O
cdf	O
to	O
compute	O
tail	B
area	I
probabilities	I
.	O
for	O
example	O
,	O
if	O
φ	O
is	O
−1	O
(	O
α	O
)	O
/2	O
)	O
contain	O
α/2	O
−1	O
(	O
1−α/2	O
)	O
−1	O
(	O
1	O
−	O
α/2	O
)	O
)	O
contains	O
−1	O
(	O
0.25	O
)	O
and	O
f	O
−1	O
(	O
0.025	O
)	O
,	O
φ	O
−1	O
(	O
0.975	O
)	O
)	O
=	O
(	O
−1.96	O
,	O
1.96	O
)	O
(	O
φ	O
(	O
2.23	O
)	O
if	O
the	O
distribution	O
is	O
n	O
(	O
μ	O
,	O
σ2	O
)	O
,	O
then	O
the	O
95	O
%	O
interval	O
becomes	O
(	O
μ	O
−	O
1.96σ	O
,	O
μ	O
+	O
1.96σ	O
)	O
.	O
this	O
is	O
sometimes	O
approximated	O
by	O
writing	O
μ	O
±	O
2σ	O
.	O
2.2.7	O
mean	B
and	O
variance	B
(	O
cid:2	O
)	O
(	O
cid:5	O
)	O
the	O
most	O
familiar	O
property	O
of	O
a	O
distribution	O
is	O
its	O
mean	B
,	O
orexpected	O
value	O
,	O
denoted	O
by	O
μ.	O
for	O
discrete	B
rv	O
’	O
s	O
,	O
it	O
is	O
deﬁned	O
as	O
e	O
[	O
x	O
]	O
(	O
cid:2	O
)	O
x∈x	O
x	O
p	O
(	O
x	O
)	O
,	O
and	O
for	O
continuous	O
rv	O
’	O
s	O
,	O
it	O
is	O
deﬁned	O
as	O
e	O
[	O
x	O
]	O
(	O
cid:2	O
)	O
x	O
x	O
p	O
(	O
x	O
)	O
dx	O
.	O
if	O
this	O
integral	O
is	O
not	O
ﬁnite	O
,	O
the	O
mean	B
is	O
not	O
deﬁned	O
(	O
we	O
will	O
see	O
some	O
examples	O
of	O
this	O
later	O
)	O
.	O
the	O
variance	B
is	O
a	O
measure	O
of	O
the	O
“	O
spread	O
”	O
of	O
a	O
distribution	O
,	O
denoted	O
by	O
σ2	O
.	O
this	O
is	O
deﬁned	O
chapter	O
2.	O
probability	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
x	O
−	O
μ	O
)	O
2p	O
(	O
x	O
)	O
dx	O
p	O
(	O
x	O
)	O
dx	O
−	O
2μ	O
(	O
cid:6	O
)	O
(	O
cid:7	O
)	O
−	O
μ2	O
x	O
2	O
xp	O
(	O
x	O
)	O
dx	O
=	O
e	O
34	O
as	O
follows	O
:	O
(	O
cid:4	O
)	O
var	O
[	O
x	O
]	O
(	O
cid:2	O
)	O
e	O
(	O
cid:6	O
)	O
(	O
x	O
−	O
μ	O
)	O
2	O
(	O
cid:4	O
)	O
(	O
cid:7	O
)	O
=	O
=	O
x2p	O
(	O
x	O
)	O
dx	O
+	O
μ2	O
from	O
which	O
we	O
derive	O
the	O
useful	O
result	O
(	O
cid:6	O
)	O
(	O
cid:7	O
)	O
e	O
x	O
2	O
=	O
μ2	O
+	O
σ2	O
(	O
cid:8	O
)	O
the	O
standard	B
deviation	I
is	O
deﬁned	O
as	O
std	O
[	O
x	O
]	O
(	O
cid:2	O
)	O
var	O
[	O
x	O
]	O
(	O
2.24	O
)	O
(	O
2.25	O
)	O
(	O
2.26	O
)	O
(	O
2.27	O
)	O
(	O
2.31	O
)	O
(	O
2.32	O
)	O
this	O
is	O
useful	O
since	O
it	O
has	O
the	O
same	O
units	O
as	O
x	O
itself	O
.	O
2.3	O
some	O
common	O
discrete	B
distributions	O
in	O
this	O
section	O
,	O
we	O
review	O
some	O
commonly	O
used	O
parametric	O
distributions	O
deﬁned	O
on	O
discrete	B
state	O
spaces	O
,	O
both	O
ﬁnite	O
and	O
countably	O
inﬁnite	O
.	O
2.3.1	O
the	O
binomial	B
and	O
bernoulli	O
distributions	O
suppose	O
we	O
toss	O
a	O
coin	O
n	O
times	O
.	O
let	O
x	O
∈	O
{	O
0	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
be	O
the	O
number	O
of	O
heads	O
.	O
if	O
the	O
probability	O
of	O
heads	O
is	O
θ	O
,	O
then	O
we	O
say	O
x	O
has	O
a	O
binomial	B
distribution	I
,	O
written	O
as	O
x	O
∼	O
bin	O
(	O
n	O
,	O
θ	O
)	O
.	O
the	O
pmf	B
is	O
given	O
by	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
n	O
k	O
bin	O
(	O
k|n	O
,	O
θ	O
)	O
(	O
cid:2	O
)	O
where	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
n	O
k	O
(	O
cid:2	O
)	O
n	O
!	O
(	O
n	O
−	O
k	O
)	O
!	O
k	O
!	O
θk	O
(	O
1	O
−	O
θ	O
)	O
n−k	O
(	O
2.28	O
)	O
(	O
2.29	O
)	O
is	O
the	O
number	O
of	O
ways	O
to	O
choose	O
k	O
items	O
from	O
n	O
(	O
this	O
is	O
known	O
as	O
the	O
binomial	B
coefficient	I
,	O
and	O
is	O
pronounced	O
“	O
n	O
choose	O
k	O
”	O
)	O
.	O
see	O
figure	O
2.4	O
for	O
some	O
examples	O
of	O
the	O
binomial	B
distribution	I
.	O
this	O
distribution	O
has	O
the	O
following	O
mean	B
and	O
variance	B
:	O
mean	B
=	O
θ	O
,	O
var	O
=	O
nθ	O
(	O
1	O
−	O
θ	O
)	O
(	O
2.30	O
)	O
now	O
suppose	O
we	O
toss	O
a	O
coin	O
only	O
once	O
.	O
let	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
be	O
a	O
binary	O
random	O
variable	O
,	O
with	O
probability	O
of	O
“	O
success	O
”	O
or	O
“	O
heads	O
”	O
of	O
θ.	O
we	O
say	O
that	O
x	O
has	O
a	O
bernoulli	O
distribution	O
.	O
this	O
is	O
written	O
as	O
x	O
∼	O
ber	O
(	O
θ	O
)	O
,	O
where	O
the	O
pmf	B
is	O
deﬁned	O
as	O
ber	O
(	O
x|θ	O
)	O
=	O
θi	O
(	O
x=1	O
)	O
(	O
1	O
−	O
θ	O
)	O
i	O
(	O
x=0	O
)	O
(	O
cid:11	O
)	O
in	O
other	O
words	O
,	O
ber	O
(	O
x|θ	O
)	O
=	O
θ	O
1	O
−	O
θ	O
if	O
x	O
=	O
1	O
if	O
x	O
=	O
0	O
this	O
is	O
obviously	O
just	O
a	O
special	O
case	O
of	O
a	O
binomial	B
distribution	I
with	O
n	O
=	O
1	O
.	O
2.3.	O
some	O
common	O
discrete	B
distributions	O
35	O
θ=0.250	O
θ=0.900	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
0.4	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
(	O
a	O
)	O
(	O
b	O
)	O
illustration	O
of	O
the	O
binomial	B
distribution	I
with	O
n	O
=	O
10	O
and	O
θ	O
∈	O
{	O
0.25	O
,	O
0.9	O
}	O
.	O
figure	O
generated	O
figure	O
2.4	O
by	O
binomdistplot	O
.	O
2.3.2	O
the	O
multinomial	B
and	O
multinoulli	O
distributions	O
the	O
binomial	B
distribution	I
can	O
be	O
used	O
to	O
model	O
the	O
outcomes	O
of	O
coin	O
tosses	O
.	O
to	O
model	O
the	O
outcomes	O
of	O
tossing	O
a	O
k-sided	O
die	O
,	O
we	O
can	O
use	O
the	O
multinomial	B
distribution	O
.	O
this	O
is	O
deﬁned	O
as	O
let	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xk	O
)	O
be	O
a	O
random	O
vector	O
,	O
where	O
xj	O
is	O
the	O
number	O
of	O
times	O
side	O
j	O
of	O
follows	O
:	O
the	O
die	O
occurs	O
.	O
then	O
x	O
has	O
the	O
following	O
pmf	B
:	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
k	O
(	O
cid:12	O
)	O
xj	O
j	O
θ	O
n	O
x1	O
.	O
.	O
.	O
xk	O
j=1	O
mu	O
(	O
x|n	O
,	O
θ	O
)	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
(	O
cid:9	O
)	O
n	O
x1	O
.	O
.	O
.	O
xk	O
(	O
cid:2	O
)	O
n	O
!	O
x1	O
!	O
x2	O
!	O
···x	O
k	O
!	O
where	O
θj	O
is	O
the	O
probability	O
that	O
side	O
j	O
shows	O
up	O
,	O
and	O
(	O
2.33	O
)	O
(	O
2.34	O
)	O
(	O
cid:2	O
)	O
k	O
is	O
the	O
multinomial	B
coefficient	I
(	O
the	O
number	O
of	O
ways	O
to	O
divide	O
a	O
set	O
of	O
size	O
n	O
=	O
subsets	O
with	O
sizes	O
x1	O
up	O
to	O
xk	O
)	O
.	O
k=1	O
xk	O
into	O
now	O
suppose	O
n	O
=	O
1.	O
this	O
is	O
like	O
rolling	O
a	O
k-sided	O
dice	O
once	O
,	O
so	O
x	O
will	O
be	O
a	O
vector	O
of	O
0s	O
and	O
1s	O
(	O
a	O
bit	O
vector	O
)	O
,	O
in	O
which	O
only	O
one	O
bit	O
can	O
be	O
turned	O
on	O
.	O
speciﬁcally	O
,	O
if	O
the	O
dice	O
shows	O
up	O
as	O
face	O
k	O
,	O
then	O
the	O
k	O
’	O
th	O
bit	O
will	O
be	O
on	O
.	O
in	O
this	O
case	O
,	O
we	O
can	O
think	O
of	O
x	O
as	O
being	O
a	O
scalar	O
categorical	O
random	O
variable	O
with	O
k	O
states	O
(	O
values	O
)	O
,	O
and	O
x	O
is	O
its	O
dummy	B
encoding	I
,	O
that	O
is	O
,	O
x	O
=	O
[	O
i	O
(	O
x	O
=	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
i	O
(	O
x	O
=	O
k	O
)	O
]	O
.	O
for	O
example	O
,	O
if	O
k	O
=	O
3	O
,	O
we	O
encode	O
the	O
states	O
1	O
,	O
2	O
and	O
3	O
as	O
(	O
1	O
,	O
0	O
,	O
0	O
)	O
,	O
(	O
0	O
,	O
1	O
,	O
0	O
)	O
,	O
and	O
(	O
0	O
,	O
0	O
,	O
1	O
)	O
.	O
this	O
is	O
also	O
called	O
a	O
one-hot	B
encoding	I
,	O
since	O
we	O
imagine	O
that	O
only	O
one	O
of	O
the	O
k	O
“	O
wires	O
”	O
is	O
“	O
hot	O
”	O
or	O
on	O
.	O
in	O
this	O
case	O
,	O
the	O
pmf	B
becomes	O
θi	O
(	O
xj	O
=1	O
)	O
j	O
(	O
2.35	O
)	O
j=1	O
see	O
figure	O
2.1	O
(	O
b-c	O
)	O
for	O
an	O
example	O
.	O
this	O
very	O
common	O
special	O
case	O
is	O
known	O
as	O
a	O
categorical	B
or	O
discrete	B
distribution	O
.	O
(	O
gustavo	O
lacerda	O
suggested	O
we	O
call	O
it	O
the	O
multinoulli	B
distribution	I
,	O
by	O
analogy	O
with	O
the	O
binomial/	O
bernoulli	O
distinction	O
,	O
a	O
term	O
which	O
we	O
shall	O
adopt	O
in	O
this	O
book	O
.	O
)	O
we	O
k	O
(	O
cid:12	O
)	O
mu	O
(	O
x|1	O
,	O
θ	O
)	O
=	O
36	O
chapter	O
2.	O
probability	O
name	O
multinomial	B
multinoulli	O
binomial	B
bernoulli	O
n	O
k	O
x	O
-	O
1	O
-	O
1	O
(	O
cid:2	O
)	O
k	O
x	O
∈	O
{	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
k	O
,	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
k	O
,	O
x	O
∈	O
{	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
-	O
-	O
1	O
1	O
(	O
cid:2	O
)	O
k	O
k=1	O
xk	O
=	O
n	O
k=1	O
xk	O
=	O
1	O
(	O
1-of-k	O
encoding	O
)	O
table	O
2.1	O
summary	O
of	O
the	O
multinomial	B
and	O
related	O
distributions	O
.	O
a	O
t	O
a	O
g	O
c	O
c	O
g	O
g	O
t	O
a	O
c	O
g	O
g	O
c	O
a	O
t	O
t	O
a	O
g	O
c	O
t	O
g	O
c	O
a	O
a	O
c	O
c	O
g	O
c	O
a	O
t	O
c	O
a	O
g	O
c	O
c	O
a	O
c	O
t	O
a	O
g	O
a	O
g	O
c	O
a	O
a	O
t	O
a	O
a	O
c	O
c	O
g	O
c	O
g	O
a	O
c	O
c	O
g	O
c	O
a	O
t	O
t	O
a	O
g	O
c	O
c	O
g	O
c	O
t	O
a	O
a	O
g	O
g	O
t	O
a	O
t	O
a	O
a	O
g	O
c	O
c	O
t	O
c	O
g	O
t	O
a	O
c	O
g	O
t	O
a	O
t	O
t	O
a	O
g	O
c	O
c	O
g	O
t	O
t	O
a	O
c	O
g	O
g	O
c	O
c	O
a	O
t	O
a	O
t	O
c	O
c	O
g	O
g	O
t	O
a	O
c	O
a	O
g	O
t	O
a	O
a	O
t	O
a	O
g	O
c	O
a	O
g	O
g	O
t	O
a	O
c	O
c	O
g	O
a	O
a	O
a	O
c	O
a	O
t	O
c	O
c	O
g	O
t	O
g	O
a	O
c	O
g	O
g	O
a	O
a	O
2	O
s	O
t	O
i	O
b	O
1	O
0	O
1	O
2	O
3	O
4	O
5	O
7	O
6	O
sequence	O
position	O
8	O
9	O
10	O
11	O
12	O
13	O
14	O
15	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
2.5	O
seqlogodemo	O
.	O
(	O
a	O
)	O
some	O
aligned	O
dna	O
sequences	O
.	O
(	O
b	O
)	O
the	O
corresponding	O
sequence	B
logo	I
.	O
figure	O
generated	O
by	O
will	O
use	O
the	O
following	O
notation	O
for	O
this	O
case	O
:	O
cat	O
(	O
x|θ	O
)	O
(	O
cid:2	O
)	O
mu	O
(	O
x|1	O
,	O
θ	O
)	O
in	O
otherwords	O
,	O
if	O
x	O
∼	O
cat	O
(	O
θ	O
)	O
,	O
then	O
p	O
(	O
x	O
=	O
j|θ	O
)	O
=	O
θj	O
.	O
see	O
table	O
2.1	O
for	O
a	O
summary	O
.	O
(	O
2.36	O
)	O
2.3.2.1	O
application	O
:	O
dna	O
sequence	O
motifs	O
an	O
interesting	O
application	O
of	O
multinomial	B
models	O
arises	O
in	O
biosequence	B
analysis	I
.	O
suppose	O
we	O
have	O
a	O
set	O
of	O
(	O
aligned	O
)	O
dna	O
sequences	O
,	O
such	O
as	O
in	O
figure	O
2.5	O
(	O
a	O
)	O
,	O
where	O
there	O
are	O
10	O
rows	O
(	O
sequences	O
)	O
and	O
15	O
columns	O
(	O
locations	O
along	O
the	O
genome	B
)	O
.	O
we	O
see	O
that	O
several	O
locations	O
are	O
con-	O
served	O
by	O
evolution	O
(	O
e.g.	O
,	O
because	O
they	O
are	O
part	O
of	O
a	O
gene	O
coding	O
region	O
)	O
,	O
since	O
the	O
corresponding	O
columns	O
tend	O
to	O
be	O
“	O
pure	B
”	O
.	O
for	O
example	O
,	O
column	O
7	O
is	O
all	O
g	O
’	O
s	O
.	O
one	O
way	O
to	O
visually	O
summarize	O
the	O
data	O
is	O
by	O
using	O
a	O
sequence	B
logo	I
:	O
see	O
figure	O
2.5	O
(	O
b	O
)	O
.	O
we	O
plot	O
the	O
letters	O
a	O
,	O
c	O
,	O
g	O
and	O
t	O
with	O
a	O
fontsize	O
proportional	O
to	O
their	O
empirical	O
probability	O
,	O
and	O
with	O
the	O
most	O
probable	O
letter	O
on	O
the	O
top	O
.	O
the	O
empirical	O
probability	O
distribution	O
at	O
location	O
t	O
,	O
ˆθt	O
,	O
is	O
gotten	O
by	O
normalizing	O
the	O
vector	O
of	O
counts	O
(	O
see	O
equation	O
3.48	O
)	O
:	O
n	O
(	O
cid:3	O
)	O
n	O
(	O
cid:3	O
)	O
n	O
(	O
cid:3	O
)	O
(	O
cid:14	O
)	O
(	O
cid:13	O
)	O
n	O
(	O
cid:3	O
)	O
nt	O
=	O
i	O
(	O
xit	O
=	O
1	O
)	O
,	O
i	O
(	O
xit	O
=	O
2	O
)	O
,	O
i	O
(	O
xit	O
=	O
3	O
)	O
,	O
i	O
(	O
xit	O
=	O
4	O
)	O
i=1	O
ˆθt	O
=	O
nt/n	O
i=1	O
i=1	O
i=1	O
(	O
2.37	O
)	O
(	O
2.38	O
)	O
this	O
distribution	O
is	O
known	O
as	O
a	O
motif	B
.	O
we	O
can	O
also	O
compute	O
the	O
most	O
probable	O
letter	O
in	O
each	O
location	O
;	O
this	O
is	O
called	O
the	O
consensus	B
sequence	I
.	O
2.3.	O
some	O
common	O
discrete	B
distributions	O
37	O
poi	O
(	O
λ=1.000	O
)	O
poi	O
(	O
λ=10.000	O
)	O
0.4	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0.14	O
0.12	O
0.1	O
0.08	O
0.06	O
0.04	O
0.02	O
0	O
0	O
5	O
10	O
15	O
(	O
a	O
)	O
20	O
25	O
30	O
0	O
0	O
5	O
10	O
20	O
25	O
30	O
15	O
(	O
b	O
)	O
figure	O
2.6	O
illustration	O
of	O
some	O
poisson	O
distributions	O
for	O
λ	O
∈	O
{	O
1	O
,	O
10	O
}	O
.	O
we	O
have	O
truncated	O
the	O
x-axis	O
to	O
25	O
for	O
clarity	O
,	O
but	O
the	O
support	B
of	O
the	O
distribution	O
is	O
over	O
all	O
the	O
non-negative	O
integers	O
.	O
figure	O
generated	O
by	O
poissonplotdemo	O
.	O
the	O
poisson	O
distribution	O
we	O
say	O
that	O
x	O
∈	O
{	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
}	O
has	O
a	O
poisson	O
distribution	O
with	O
parameter	B
λ	O
>	O
0	O
,	O
written	O
x	O
∼	O
poi	O
(	O
λ	O
)	O
,	O
if	O
its	O
pmf	B
is	O
−λ	O
λx	O
x	O
!	O
poi	O
(	O
x|λ	O
)	O
=e	O
(	O
2.39	O
)	O
the	O
ﬁrst	O
term	O
is	O
just	O
the	O
normalization	O
constant	O
,	O
required	O
to	O
ensure	O
the	O
distribution	O
sums	O
to	O
1.	O
the	O
poisson	O
distribution	O
is	O
often	O
used	O
as	O
a	O
model	O
for	O
counts	O
of	O
rare	O
events	O
like	O
radioactive	O
decay	O
and	O
traffic	O
accidents	O
.	O
see	O
figure	O
2.6	O
for	O
some	O
plots	O
.	O
the	O
empirical	B
distribution	I
given	O
a	O
set	O
of	O
data	O
,	O
d	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
,	O
we	O
deﬁne	O
the	O
empirical	B
distribution	I
,	O
also	O
called	O
the	O
empirical	B
measure	I
,	O
as	O
follows	O
:	O
2.3.3	O
2.3.4	O
(	O
2.40	O
)	O
(	O
2.41	O
)	O
(	O
2.42	O
)	O
n	O
(	O
cid:3	O
)	O
δxi	O
(	O
a	O
)	O
i=1	O
pemp	O
(	O
a	O
)	O
(	O
cid:2	O
)	O
1	O
n	O
(	O
cid:11	O
)	O
δx	O
(	O
a	O
)	O
=	O
if	O
x	O
(	O
cid:12	O
)	O
∈	O
a	O
if	O
x	O
∈	O
a	O
0	O
1	O
where	O
δx	O
(	O
a	O
)	O
is	O
the	O
dirac	O
measure	O
,	O
deﬁned	O
by	O
in	O
general	O
,	O
we	O
can	O
associate	O
“	O
weights	O
”	O
with	O
each	O
sample	O
:	O
n	O
(	O
cid:3	O
)	O
p	O
(	O
x	O
)	O
=	O
wiδxi	O
(	O
x	O
)	O
i=1	O
(	O
cid:2	O
)	O
n	O
where	O
we	O
require	O
0	O
≤	O
wi	O
≤	O
1	O
and	O
i=1	O
wi	O
=	O
1.	O
we	O
can	O
think	O
of	O
this	O
as	O
a	O
histogram	B
,	O
with	O
“	O
spikes	O
”	O
at	O
the	O
data	O
points	O
xi	O
,	O
where	O
wi	O
determines	O
the	O
height	O
of	O
spike	O
i.	O
this	O
distribution	O
assigns	O
0	O
probability	O
to	O
any	O
point	O
not	O
in	O
the	O
data	O
set	O
.	O
38	O
chapter	O
2.	O
probability	O
2.4	O
some	O
common	O
continuous	O
distributions	O
in	O
this	O
section	O
we	O
present	O
some	O
commonly	O
used	O
univariate	O
(	O
one-dimensional	O
)	O
continuous	O
prob-	O
ability	O
distributions	O
.	O
2.4.1	O
gaussian	O
(	O
normal	B
)	O
distribution	O
the	O
most	O
widely	O
used	O
distribution	O
in	O
statistics	O
and	O
machine	B
learning	I
is	O
the	O
gaussian	O
or	O
normal	B
distribution	O
.	O
its	O
pdf	B
is	O
given	O
by	O
(	O
2.43	O
)	O
n	O
(	O
x|μ	O
,	O
σ2	O
)	O
(	O
cid:2	O
)	O
1√	O
2πσ2	O
−	O
1	O
2σ2	O
(	O
x−μ	O
)	O
2	O
e	O
here	O
μ	O
=	O
e	O
[	O
x	O
]	O
is	O
the	O
mean	B
(	O
and	O
mode	B
)	O
,	O
and	O
σ2	O
=	O
var	O
[	O
x	O
]	O
is	O
the	O
variance	B
.	O
normalization	O
constant	O
needed	O
to	O
ensure	O
the	O
density	O
integrates	O
to	O
1	O
(	O
see	O
exercise	O
2.11	O
)	O
.	O
√	O
2πσ2	O
is	O
the	O
if	O
x	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
,	O
we	O
say	O
x	O
follows	O
a	O
standard	B
normal	I
distribution	O
.	O
see	O
figure	O
2.3	O
(	O
b	O
)	O
for	O
a	O
plot	O
of	O
this	O
pdf	B
;	O
this	O
is	O
sometimes	O
called	O
the	O
bell	B
curve	I
.	O
we	O
write	O
x	O
∼	O
n	O
(	O
μ	O
,	O
σ2	O
)	O
to	O
denote	O
that	O
p	O
(	O
x	O
=	O
x	O
)	O
=n	O
(	O
x|μ	O
,	O
σ2	O
)	O
.	O
we	O
will	O
often	O
talk	O
about	O
the	O
precision	B
of	O
a	O
gaussian	O
,	O
by	O
which	O
we	O
mean	B
the	O
inverse	O
variance	O
:	O
λ	O
=	O
1/σ2	O
.	O
a	O
high	O
precision	O
means	O
a	O
narrow	O
distribution	O
(	O
low	O
variance	O
)	O
centered	O
on	O
μ.4	O
density	O
at	O
its	O
center	O
,	O
x	O
=	O
μ.	O
we	O
have	O
n	O
(	O
μ|μ	O
,	O
σ2	O
)	O
=	O
(	O
σ	O
p	O
(	O
x	O
)	O
>	O
1.	O
note	O
that	O
,	O
since	O
this	O
is	O
a	O
pdf	B
,	O
we	O
can	O
have	O
p	O
(	O
x	O
)	O
>	O
1.	O
to	O
see	O
this	O
,	O
consider	O
evaluating	O
the	O
2π	O
,	O
we	O
have	O
−1e0	O
,	O
so	O
if	O
σ	O
<	O
1/	O
2π	O
)	O
√	O
√	O
(	O
cid:4	O
)	O
x	O
−∞	O
the	O
cumulative	B
distribution	I
function	I
or	O
cdf	B
of	O
the	O
gaussian	O
is	O
deﬁned	O
as	O
φ	O
(	O
x	O
;	O
μ	O
,	O
σ2	O
)	O
(	O
cid:2	O
)	O
n	O
(	O
z|μ	O
,	O
σ2	O
)	O
dz	O
(	O
2.44	O
)	O
see	O
figure	O
2.3	O
(	O
a	O
)	O
for	O
a	O
plot	O
of	O
this	O
cdf	B
when	O
μ	O
=	O
0	O
,	O
σ2	O
=	O
1.	O
this	O
integral	O
has	O
no	O
closed	O
form	O
expression	O
,	O
but	O
is	O
built	O
in	O
to	O
most	O
software	O
packages	O
.	O
in	O
particular	O
,	O
we	O
can	O
compute	O
it	O
in	O
terms	O
of	O
the	O
error	B
function	I
(	O
erf	B
)	O
:	O
√	O
[	O
1	O
+	O
erf	B
(	O
z/	O
1	O
2	O
φ	O
(	O
x	O
;	O
μ	O
,	O
σ	O
)	O
=	O
where	O
z	O
=	O
(	O
x	O
−	O
μ	O
)	O
/σ	O
and	O
−t2	O
(	O
cid:4	O
)	O
x	O
erf	B
(	O
x	O
)	O
(	O
cid:2	O
)	O
2√	O
π	O
e	O
0	O
dt	O
2	O
)	O
]	O
(	O
2.45	O
)	O
(	O
2.46	O
)	O
the	O
gaussian	O
distribution	O
is	O
the	O
most	O
widely	O
used	O
distribution	O
in	O
statistics	O
.	O
there	O
are	O
several	O
reasons	O
for	O
this	O
.	O
first	O
,	O
it	O
has	O
two	O
parameters	O
which	O
are	O
easy	O
to	O
interpret	O
,	O
and	O
which	O
capture	O
some	O
of	O
the	O
most	O
basic	O
properties	O
of	O
a	O
distribution	O
,	O
namely	O
its	O
mean	B
and	O
variance	B
.	O
second	O
,	O
the	O
central	B
limit	I
theorem	I
(	O
section	O
2.6.3	O
)	O
tells	O
us	O
that	O
sums	O
of	O
independent	O
random	O
variables	O
have	O
an	O
approximately	O
gaussian	O
distribution	O
,	O
making	O
it	O
a	O
good	O
choice	O
for	O
modeling	O
residual	B
errors	O
or	O
“	O
noise	O
”	O
.	O
third	O
,	O
the	O
gaussian	O
distribution	O
makes	O
the	O
least	O
number	O
of	O
assumptions	O
(	O
has	O
4.	O
the	O
symbol	O
λ	O
will	O
have	O
many	O
different	O
meanings	O
in	O
this	O
book	O
,	O
in	O
order	O
to	O
be	O
consistent	B
with	O
the	O
rest	O
of	O
the	O
literature	O
.	O
the	O
intended	O
meaning	O
should	O
be	O
clear	O
from	O
context	O
.	O
2.4.	O
some	O
common	O
continuous	O
distributions	O
39	O
maximum	B
entropy	I
)	O
,	O
subject	O
to	O
the	O
constraint	O
of	O
having	O
a	O
speciﬁed	O
mean	B
and	O
variance	B
,	O
as	O
we	O
show	O
in	O
section	O
9.2.6	O
;	O
this	O
makes	O
it	O
a	O
good	O
default	O
choice	O
in	O
many	O
cases	O
.	O
finally	O
,	O
it	O
has	O
a	O
simple	O
mathematical	O
form	O
,	O
which	O
results	O
in	O
easy	O
to	O
implement	O
,	O
but	O
often	O
highly	O
effective	O
,	O
methods	O
,	O
as	O
we	O
will	O
see	O
.	O
see	O
(	O
jaynes	O
2003	O
,	O
ch	O
7	O
)	O
for	O
a	O
more	O
extensive	O
discussion	O
of	O
why	O
gaussians	O
are	O
so	O
widely	O
used	O
.	O
2.4.2	O
degenerate	B
pdf	O
in	O
the	O
limit	O
that	O
σ2	O
→	O
0	O
,	O
the	O
gaussian	O
becomes	O
an	O
inﬁnitely	O
tall	O
and	O
inﬁnitely	O
thin	O
“	O
spike	O
”	O
centered	O
at	O
μ	O
:	O
where	O
δ	O
is	O
called	O
a	O
dirac	O
delta	O
function	O
,	O
and	O
is	O
deﬁned	O
as	O
n	O
(	O
x|μ	O
,	O
σ2	O
)	O
=	O
δ	O
(	O
x	O
−	O
μ	O
)	O
lim	O
σ2→0	O
(	O
cid:11	O
)	O
∞	O
if	O
x	O
=	O
0	O
if	O
x	O
(	O
cid:12	O
)	O
=	O
0	O
0	O
δ	O
(	O
x	O
)	O
=	O
such	O
that	O
(	O
cid:4	O
)	O
∞	O
(	O
cid:4	O
)	O
∞	O
−∞	O
δ	O
(	O
x	O
)	O
dx	O
=	O
1	O
−∞	O
from	O
a	O
sum	O
or	O
integral	O
:	O
(	O
2.47	O
)	O
(	O
2.48	O
)	O
(	O
2.49	O
)	O
(	O
2.51	O
)	O
a	O
useful	O
property	O
of	O
delta	O
functions	O
is	O
the	O
sifting	B
property	I
,	O
which	O
selects	O
out	O
a	O
single	O
term	O
f	O
(	O
x	O
)	O
δ	O
(	O
x	O
−	O
μ	O
)	O
dx	O
=	O
f	O
(	O
μ	O
)	O
(	O
2.50	O
)	O
since	O
the	O
integrand	O
is	O
only	O
non-zero	O
if	O
x	O
−	O
μ	O
=	O
0.	O
one	O
problem	O
with	O
the	O
gaussian	O
distribution	O
is	O
that	O
it	O
is	O
sensitive	O
to	O
outliers	B
,	O
since	O
the	O
log-	O
probability	O
only	O
decays	O
quadratically	O
with	O
distance	O
from	O
the	O
center	O
.	O
a	O
more	O
robust	B
distribution	O
is	O
the	O
student	O
t	O
distribution5	O
its	O
pdf	B
is	O
as	O
follows	O
:	O
(	O
cid:15	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
2	O
(	O
cid:16	O
)	O
−	O
(	O
ν+1	O
2	O
)	O
t	O
(	O
x|μ	O
,	O
σ2	O
,	O
ν	O
)	O
∝	O
1	O
+	O
1	O
ν	O
x	O
−	O
μ	O
σ	O
where	O
μ	O
is	O
the	O
mean	B
,	O
σ2	O
>	O
0	O
is	O
the	O
scale	O
parameter	O
,	O
and	O
ν	O
>	O
0	O
is	O
called	O
the	O
degrees	B
of	I
freedom	I
.	O
see	O
figure	O
2.7	O
for	O
some	O
plots	O
.	O
for	O
later	O
reference	O
,	O
we	O
note	O
that	O
the	O
distribution	O
has	O
the	O
following	O
properties	O
:	O
mean	B
=	O
μ	O
,	O
mode	B
=	O
μ	O
,	O
var	O
=	O
νσ2	O
(	O
ν	O
−	O
2	O
)	O
(	O
2.52	O
)	O
5.	O
this	O
distribution	O
has	O
a	O
colourful	O
etymology	O
.	O
it	O
was	O
ﬁrst	O
published	O
in	O
1908	O
by	O
william	O
sealy	O
gosset	O
,	O
who	O
worked	O
at	O
the	O
guinness	O
brewery	O
in	O
dublin	O
.	O
since	O
his	O
employer	O
would	O
not	O
allow	O
him	O
to	O
use	O
his	O
own	O
name	O
,	O
he	O
called	O
it	O
the	O
“	O
student	O
”	O
distribution	O
.	O
the	O
origin	O
of	O
the	O
term	O
t	O
seems	O
to	O
have	O
arisen	O
in	O
the	O
context	O
of	O
tables	O
of	O
the	O
student	O
distribution	O
,	O
used	O
by	O
fisher	O
when	O
developing	O
the	O
basis	O
of	O
classical	B
statistical	O
inference	B
.	O
see	O
http	O
:	O
//jeff560.tripod.com/s.html	O
for	O
more	O
historical	O
details	O
.	O
40	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−4	O
gauss	O
student	O
laplace	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
−6	O
−7	O
−8	O
−9	O
−4	O
chapter	O
2.	O
probability	O
gauss	O
student	O
laplace	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
a	O
)	O
the	O
pdf	B
’	O
s	O
for	O
a	O
n	O
(	O
0	O
,	O
1	O
)	O
,	O
t	O
(	O
0	O
,	O
1	O
,	O
1	O
)	O
and	O
lap	O
(	O
0	O
,	O
1/	O
√	O
figure	O
2.7	O
2	O
)	O
.	O
the	O
mean	B
is	O
0	O
and	O
the	O
variance	B
is	O
1	O
for	O
both	O
the	O
gaussian	O
and	O
laplace	O
.	O
the	O
mean	B
and	O
variance	B
of	O
the	O
student	O
is	O
undeﬁned	O
when	O
ν	O
=	O
1	O
.	O
(	O
b	O
)	O
log	O
of	O
these	O
pdf	B
’	O
s	O
.	O
note	O
that	O
the	O
student	O
distribution	O
is	O
not	O
log-concave	O
for	O
any	O
parameter	B
value	O
,	O
unlike	O
the	O
laplace	O
distribution	O
,	O
which	O
is	O
always	O
log-concave	O
(	O
and	O
log-convex	O
...	O
)	O
nevertheless	O
,	O
both	O
are	O
unimodal	O
.	O
figure	O
generated	O
by	O
studentlaplacepdfplot	O
.	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−5	O
gaussian	O
student	O
t	O
laplace	O
0	O
5	O
10	O
(	O
a	O
)	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−5	O
gaussian	O
student	O
t	O
laplace	O
0	O
5	O
10	O
(	O
b	O
)	O
figure	O
2.8	O
illustration	O
of	O
the	O
effect	O
of	O
outliers	B
on	O
ﬁtting	O
gaussian	O
,	O
student	O
and	O
laplace	O
distributions	O
.	O
(	O
a	O
)	O
no	O
outliers	O
(	O
the	O
gaussian	O
and	O
student	O
curves	O
are	O
on	O
top	O
of	O
each	O
other	O
)	O
.	O
(	O
b	O
)	O
with	O
outliers	B
.	O
we	O
see	O
that	O
the	O
gaussian	O
is	O
more	O
affected	O
by	O
outliers	B
than	O
the	O
student	O
and	O
laplace	O
distributions	O
.	O
based	O
on	O
figure	O
2.16	O
of	O
(	O
bishop	O
2006a	O
)	O
.	O
figure	O
generated	O
by	O
robustdemo	O
.	O
the	O
variance	B
is	O
only	O
deﬁned	O
if	O
ν	O
>	O
2.	O
the	O
mean	B
is	O
only	O
deﬁned	O
if	O
ν	O
>	O
1.	O
as	O
an	O
illustration	O
of	O
the	O
robustness	B
of	O
the	O
student	O
distribution	O
,	O
consider	O
figure	O
2.8.	O
on	O
the	O
left	O
,	O
we	O
show	O
a	O
gaussian	O
and	O
a	O
student	O
ﬁt	O
to	O
some	O
data	O
with	O
no	O
outliers	O
.	O
on	O
the	O
right	O
,	O
we	O
add	O
some	O
outliers	B
.	O
we	O
see	O
that	O
the	O
gaussian	O
is	O
affected	O
a	O
lot	O
,	O
whereas	O
the	O
student	O
distribution	O
hardly	O
changes	O
.	O
this	O
is	O
because	O
the	O
student	O
has	O
heavier	O
tails	O
,	O
at	O
least	O
for	O
small	O
ν	O
(	O
see	O
figure	O
2.7	O
)	O
.	O
if	O
ν	O
=	O
1	O
,	O
this	O
distribution	O
is	O
known	O
as	O
the	O
cauchy	O
or	O
lorentz	O
distribution	O
.	O
this	O
is	O
notable	O
to	O
ensure	O
ﬁnite	O
variance	O
,	O
we	O
require	O
ν	O
>	O
2.	O
for	O
having	O
such	O
heavy	B
tails	I
that	O
the	O
integral	O
that	O
deﬁnes	O
the	O
mean	B
does	O
not	O
converge	O
.	O
it	O
is	O
common	O
to	O
use	O
ν	O
=	O
4	O
,	O
which	O
gives	O
good	O
performance	O
in	O
a	O
range	O
of	O
problems	O
(	O
lange	O
et	O
al	O
.	O
1989	O
)	O
.	O
for	O
ν	O
(	O
cid:17	O
)	O
5	O
,	O
the	O
student	O
distribution	O
rapidly	O
approaches	O
a	O
gaussian	O
distribution	O
and	O
loses	O
its	O
robustness	B
properties	O
.	O
2.4.	O
some	O
common	O
continuous	O
distributions	O
41	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
gamma	O
distributions	O
a=1.0	O
,	O
b=1.0	O
a=1.5	O
,	O
b=1.0	O
a=2.0	O
,	O
b=1.0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
0	O
0.5	O
1	O
1.5	O
2	O
2.5	O
(	O
a	O
)	O
(	O
b	O
)	O
if	O
a	O
≤	O
1	O
,	O
the	O
mode	B
is	O
at	O
0	O
,	O
otherwise	O
it	O
is	O
>	O
0.	O
as	O
figure	O
2.9	O
(	O
a	O
)	O
some	O
ga	O
(	O
a	O
,	O
b	O
=	O
1	O
)	O
distributions	O
.	O
we	O
increase	O
the	O
rate	B
b	O
,	O
we	O
reduce	O
the	O
horizontal	O
scale	O
,	O
thus	O
squeezing	O
everything	O
leftwards	O
and	O
upwards	O
.	O
figure	O
generated	O
by	O
gammaplotdemo	O
.	O
(	O
b	O
)	O
an	O
empirical	O
pdf	O
of	O
some	O
rainfall	O
data	O
,	O
with	O
a	O
ﬁtted	O
gamma	B
distribution	I
superimposed	O
.	O
figure	O
generated	O
by	O
gammarainfalldemo	O
.	O
2.4.3	O
the	O
laplace	O
distribution	O
another	O
distribution	O
with	O
heavy	B
tails	I
is	O
the	O
laplace	O
distribution6	O
,	O
also	O
known	O
as	O
the	O
double	B
sided	I
exponential	I
distribution	O
.	O
this	O
has	O
the	O
following	O
pdf	B
:	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
lap	O
(	O
x|μ	O
,	O
b	O
)	O
(	O
cid:2	O
)	O
1	O
2b	O
exp	O
−|x	O
−	O
μ|	O
b	O
(	O
2.53	O
)	O
here	O
μ	O
is	O
a	O
location	O
parameter	B
and	O
b	O
>	O
0	O
is	O
a	O
scale	O
parameter	O
.	O
see	O
figure	O
2.7	O
for	O
a	O
plot	O
.	O
this	O
distribution	O
has	O
the	O
following	O
properties	O
:	O
mean	B
=	O
μ	O
,	O
mode	B
=	O
μ	O
,	O
var	O
=	O
2b2	O
(	O
2.54	O
)	O
its	O
robustness	B
to	O
outliers	B
is	O
illustrated	O
in	O
figure	O
2.8.	O
it	O
also	O
put	O
mores	O
probability	O
density	O
at	O
0	O
than	O
the	O
gaussian	O
.	O
this	O
property	O
is	O
a	O
useful	O
way	O
to	O
encourage	O
sparsity	B
in	O
a	O
model	O
,	O
as	O
we	O
will	O
see	O
in	O
section	O
13.3	O
.	O
2.4.4	O
the	O
gamma	B
distribution	I
the	O
gamma	B
distribution	I
is	O
a	O
ﬂexible	O
distribution	O
for	O
positive	O
real	O
valued	O
rv	O
’	O
s	O
,	O
x	O
>	O
0.	O
deﬁned	O
in	O
terms	O
of	O
two	O
parameters	O
,	O
called	O
the	O
shape	O
a	O
>	O
0	O
and	O
the	O
rate	B
b	O
>	O
0	O
:	O
7	O
it	O
is	O
ga	O
(	O
t|shape	O
=	O
a	O
,	O
rate	B
=	O
b	O
)	O
(	O
cid:2	O
)	O
ba	O
γ	O
(	O
a	O
)	O
t	O
a−1e	O
−t	O
b	O
(	O
2.55	O
)	O
6.	O
pierre-simon	O
laplace	O
(	O
1749–1827	O
)	O
was	O
a	O
french	O
mathematician	O
,	O
who	O
played	O
a	O
key	O
role	O
in	O
creating	O
the	O
ﬁeld	O
of	O
bayesian	O
statistics	O
.	O
7.	O
there	O
is	O
an	O
alternative	O
parameterization	O
,	O
where	O
we	O
use	O
the	O
scale	O
parameter	O
instead	O
of	O
the	O
rate	B
:	O
gas	O
(	O
t|a	O
,	O
b	O
)	O
(	O
cid:2	O
)	O
ga	O
(	O
t|a	O
,	O
1/b	O
)	O
.	O
this	O
version	O
is	O
the	O
one	O
used	O
by	O
matlab	O
’	O
s	O
gampdf	O
,	O
although	O
in	O
this	O
book	O
will	O
use	O
the	O
rate	B
parameterization	O
unless	O
otherwise	O
speciﬁed	O
.	O
42	O
chapter	O
2.	O
probability	O
where	O
γ	O
(	O
a	O
)	O
is	O
the	O
gamma	B
function	I
:	O
(	O
cid:4	O
)	O
∞	O
0	O
γ	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
ux−1e	O
−udu	O
(	O
2.56	O
)	O
a	O
−	O
1	O
b	O
see	O
figure	O
2.9	O
for	O
some	O
plots	O
.	O
for	O
later	O
reference	O
,	O
we	O
note	O
that	O
the	O
distribution	O
has	O
the	O
following	O
properties	O
:	O
mean	B
=	O
a	O
b	O
,	O
mode	B
=	O
,	O
var	O
=	O
a	O
b2	O
(	O
2.57	O
)	O
there	O
are	O
several	O
distributions	O
which	O
are	O
just	O
special	O
cases	O
of	O
the	O
gamma	O
,	O
which	O
we	O
discuss	O
•	O
erlang	O
distribution	O
this	O
is	O
the	O
same	O
as	O
the	O
gamma	B
distribution	I
where	O
a	O
is	O
an	O
integer	O
.	O
below	O
.	O
•	O
exponential	O
distribution	O
this	O
is	O
deﬁned	O
by	O
expon	O
(	O
x|λ	O
)	O
(	O
cid:2	O
)	O
ga	O
(	O
x|1	O
,	O
λ	O
)	O
,	O
where	O
λ	O
is	O
the	O
rate	B
parameter	O
.	O
this	O
distribution	O
describes	O
the	O
times	O
between	O
events	O
in	O
a	O
poisson	O
process	O
,	O
i.e	O
.	O
a	O
process	O
in	O
which	O
events	O
occur	O
continuously	O
and	O
independently	O
at	O
a	O
constant	O
average	O
rate	B
λ.	O
it	O
is	O
common	O
to	O
ﬁx	O
a	O
=	O
2	O
,	O
yielding	O
the	O
one-parameter	O
erlang	O
distribution	O
,	O
erlang	O
(	O
x|λ	O
)	O
=	O
ga	O
(	O
x|2	O
,	O
λ	O
)	O
,	O
whereλ	O
is	O
the	O
rate	B
parameter	O
.	O
2	O
)	O
.	O
this	O
is	O
the	O
distribution	O
of	O
the	O
sum	O
of	O
squared	O
gaussian	O
random	O
variables	O
.	O
more	O
precisely	O
,	O
if	O
zi	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
,	O
and	O
s	O
=	O
•	O
chi-squared	O
distribution	O
this	O
is	O
deﬁned	O
by	O
χ2	O
(	O
x|ν	O
)	O
(	O
cid:2	O
)	O
ga	O
(	O
x|	O
ν	O
i	O
,	O
then	O
s	O
∼	O
χ2	O
ν	O
.	O
(	O
cid:2	O
)	O
ν	O
2	O
,	O
1	O
i=1	O
z	O
2	O
that	O
1	O
another	O
useful	O
result	O
is	O
the	O
following	O
:	O
x	O
∼	O
ig	O
(	O
a	O
,	O
b	O
)	O
,	O
whereig	O
is	O
the	O
inverse	B
gamma	I
distribution	O
deﬁned	O
by	O
ig	O
(	O
x|shape	O
=	O
a	O
,	O
scale	O
=	O
b	O
)	O
(	O
cid:2	O
)	O
−	O
(	O
a+1	O
)	O
e	O
x	O
−b/x	O
ba	O
γ	O
(	O
a	O
)	O
if	O
x	O
∼	O
ga	O
(	O
a	O
,	O
b	O
)	O
,	O
then	O
one	O
can	O
show	O
(	O
exercise	O
2.10	O
)	O
the	O
distribution	O
has	O
these	O
properties	O
mean	B
=	O
b	O
a	O
−	O
1	O
,	O
mode	B
=	O
b	O
a	O
+	O
1	O
,	O
var	O
=	O
b2	O
(	O
a	O
−	O
1	O
)	O
2	O
(	O
a	O
−	O
2	O
)	O
,	O
the	O
mean	B
only	O
exists	O
if	O
a	O
>	O
1	O
.	O
the	O
variance	B
only	O
exists	O
if	O
a	O
>	O
2	O
.	O
we	O
will	O
see	O
applications	O
of	O
these	O
distributions	O
later	O
on	O
.	O
2.4.5	O
the	O
beta	B
distribution	I
the	O
beta	B
distribution	I
has	O
support	B
over	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
and	O
is	O
deﬁned	O
as	O
follows	O
:	O
beta	O
(	O
x|a	O
,	O
b	O
)	O
=	O
1	O
b	O
(	O
a	O
,	O
b	O
)	O
xa−1	O
(	O
1	O
−	O
x	O
)	O
b−1	O
here	O
b	O
(	O
p	O
,	O
q	O
)	O
is	O
the	O
beta	B
function	I
,	O
b	O
(	O
a	O
,	O
b	O
)	O
(	O
cid:2	O
)	O
γ	O
(	O
a	O
)	O
γ	O
(	O
b	O
)	O
γ	O
(	O
a	O
+	O
b	O
)	O
see	O
figure	O
2.10	O
for	O
plots	O
of	O
some	O
beta	O
distributions	O
.	O
we	O
require	O
a	O
,	O
b	O
>	O
0	O
to	O
ensure	O
the	O
distribution	O
if	O
is	O
integrable	O
(	O
i.e.	O
,	O
to	O
ensure	O
b	O
(	O
a	O
,	O
b	O
)	O
exists	O
)	O
.	O
if	O
a	O
=	O
b	O
=	O
1	O
,	O
we	O
get	O
the	O
uniform	O
distirbution	O
.	O
(	O
2.58	O
)	O
(	O
2.59	O
)	O
(	O
2.60	O
)	O
(	O
2.61	O
)	O
2.4.	O
some	O
common	O
continuous	O
distributions	O
43	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
0	O
beta	O
distributions	O
a=0.1	O
,	O
b=0.1	O
a=1.0	O
,	O
b=1.0	O
a=2.0	O
,	O
b=3.0	O
a=8.0	O
,	O
b=4.0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
figure	O
2.10	O
some	O
beta	O
distributions	O
.	O
figure	O
generated	O
by	O
betaplotdemo	O
.	O
a	O
and	O
b	O
are	O
both	O
less	O
than	O
1	O
,	O
we	O
get	O
a	O
bimodal	O
distribution	O
with	O
“	O
spikes	O
”	O
at	O
0	O
and	O
1	O
;	O
if	O
a	O
and	O
b	O
are	O
both	O
greater	O
than	O
1	O
,	O
the	O
distribution	O
is	O
unimodal	O
.	O
for	O
later	O
reference	O
,	O
we	O
note	O
that	O
the	O
distribution	O
has	O
the	O
following	O
properties	O
(	O
exercise	O
2.16	O
)	O
:	O
mean	B
=	O
a	O
a	O
+	O
b	O
,	O
mode	B
=	O
2.4.6	O
pareto	O
distribution	O
a	O
−	O
1	O
a	O
+	O
b	O
−	O
2	O
,	O
var	O
=	O
ab	O
(	O
a	O
+	O
b	O
)	O
2	O
(	O
a	O
+	O
b	O
+	O
1	O
)	O
(	O
2.62	O
)	O
the	O
pareto	O
distribution	O
is	O
used	O
to	O
model	O
the	O
distribution	O
of	O
quantities	O
that	O
exhibit	O
long	B
tails	I
,	O
also	O
called	O
heavy	B
tails	I
.	O
for	O
example	O
,	O
it	O
has	O
been	O
observed	O
that	O
the	O
most	O
frequent	O
word	O
in	O
english	O
(	O
“	O
the	O
”	O
)	O
occurs	O
approximately	O
twice	O
as	O
often	O
as	O
the	O
second	O
most	O
frequent	O
word	O
(	O
“	O
of	O
”	O
)	O
,	O
which	O
occurs	O
twice	O
as	O
often	O
as	O
the	O
fourth	O
most	O
frequent	O
word	O
,	O
etc	O
.	O
if	O
we	O
plot	O
the	O
frequency	O
of	O
words	O
vs	O
their	O
rank	O
,	O
we	O
will	O
get	O
a	O
power	B
law	I
;	O
this	O
is	O
known	O
as	O
zipf	O
’	O
s	O
law	O
.	O
wealth	O
has	O
a	O
similarly	O
skewed	O
distribution	O
,	O
especially	O
in	O
plutocracies	B
such	O
as	O
the	O
usa.8	O
the	O
pareto	O
pdf	B
is	O
deﬁned	O
as	O
follow	O
:	O
pareto	O
(	O
x|k	O
,	O
m	O
)	O
=	O
kmkx	O
−	O
(	O
k+1	O
)	O
i	O
(	O
x	O
≥	O
m	O
)	O
(	O
2.63	O
)	O
this	O
density	O
asserts	O
that	O
x	O
must	O
be	O
greater	O
than	O
some	O
constant	O
m	O
,	O
but	O
not	O
too	O
much	O
greater	O
,	O
where	O
k	O
controls	O
what	O
is	O
“	O
too	O
much	O
”	O
.	O
as	O
k	O
→	O
∞	O
,	O
the	O
distribution	O
approaches	O
δ	O
(	O
x	O
−	O
m	O
)	O
.	O
see	O
if	O
we	O
plot	O
the	O
distibution	O
on	O
a	O
log-log	O
scale	O
,	O
it	O
forms	O
a	O
straight	O
figure	O
2.11	O
(	O
a	O
)	O
for	O
some	O
plots	O
.	O
line	O
,	O
of	O
the	O
form	O
log	O
p	O
(	O
x	O
)	O
=	O
a	O
log	O
x	O
+	O
c	O
for	O
some	O
constants	O
a	O
and	O
c.	O
see	O
figure	O
2.11	O
(	O
b	O
)	O
for	O
an	O
illustration	O
(	O
this	O
is	O
known	O
as	O
a	O
power	B
law	I
)	O
.	O
this	O
distribution	O
has	O
the	O
following	O
properties	O
mean	B
=	O
km	O
k	O
−	O
1	O
if	O
k	O
>	O
1	O
,	O
mode	B
=	O
m	O
,	O
var	O
=	O
m2k	O
(	O
k	O
−	O
1	O
)	O
2	O
(	O
k	O
−	O
2	O
)	O
if	O
k	O
>	O
2	O
(	O
2.64	O
)	O
400	O
americans	O
have	O
more	O
wealth	O
than	O
half	O
of	O
8.	O
in	O
the	O
usa	O
,	O
http	O
:	O
//www.politifact.com/wisconsin/statements/2011/mar/10/michael-moore/michael-moore-s	O
ays-400-americans-have-more-wealth-	O
.	O
)	O
see	O
(	O
hacker	O
and	O
pierson	O
2010	O
)	O
for	O
a	O
political	O
analysis	O
of	O
how	O
such	O
an	O
extreme	O
distribution	O
of	O
income	O
has	O
arisen	O
in	O
a	O
democratic	O
country	O
.	O
all	O
americans	O
combined	O
.	O
(	O
source	O
:	O
44	O
2	O
1.8	O
1.6	O
1.4	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0	O
chapter	O
2.	O
probability	O
pareto	O
distribution	O
101	O
m=0.01	O
,	O
k=0.10	O
m=0.00	O
,	O
k=0.50	O
m=1.00	O
,	O
k=1.00	O
pareto	O
(	O
m=1	O
,	O
k	O
)	O
on	O
log	O
scale	O
k=1.0	O
k=2.0	O
k=3.0	O
100	O
10−1	O
10−2	O
10−3	O
100	O
0.5	O
1	O
1.5	O
2	O
2.5	O
3	O
3.5	O
4	O
4.5	O
5	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
a	O
)	O
the	O
pareto	O
distribution	O
pareto	O
(	O
x|m	O
,	O
k	O
)	O
for	O
m	O
=	O
1	O
.	O
(	O
b	O
)	O
the	O
pdf	B
on	O
a	O
log-log	O
scale	O
.	O
figure	O
figure	O
2.11	O
generated	O
by	O
paretoplot	O
.	O
2.5	O
joint	O
probability	O
distributions	O
so	O
far	O
,	O
we	O
have	O
been	O
mostly	O
focusing	O
on	O
modeling	O
univariate	O
probability	O
distributions	O
.	O
in	O
this	O
section	O
,	O
we	O
start	O
our	O
discussion	O
of	O
the	O
more	O
challenging	O
problem	O
of	O
building	O
joint	O
probability	O
distributions	O
on	O
multiple	O
related	O
random	O
variables	O
;	O
this	O
will	O
be	O
a	O
central	O
topic	O
in	O
this	O
book	O
.	O
a	O
joint	B
probability	I
distribution	I
has	O
the	O
form	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
for	O
a	O
set	O
ofd	O
>	O
1	O
variables	O
,	O
and	O
models	O
the	O
(	O
stochastic	O
)	O
relationships	O
between	O
the	O
variables	O
.	O
if	O
all	O
the	O
variables	O
are	O
discrete	B
,	O
we	O
can	O
represent	O
the	O
joint	B
distribution	I
as	O
a	O
big	O
multi-dimensional	O
array	O
,	O
with	O
one	O
variable	O
per	O
dimension	O
.	O
however	O
,	O
the	O
number	O
of	O
parameters	O
needed	O
to	O
deﬁne	O
such	O
a	O
model	O
is	O
o	O
(	O
k	O
d	O
)	O
,	O
where	O
k	O
is	O
the	O
number	O
of	O
states	O
for	O
each	O
variable	O
.	O
we	O
can	O
deﬁne	O
high	O
dimensional	O
joint	O
distributions	O
using	O
fewer	O
parameters	O
by	O
making	O
con-	O
ditional	O
independence	O
assumptions	O
,	O
as	O
we	O
explain	O
in	O
chapter	O
10.	O
in	O
the	O
case	O
of	O
continuous	O
distributions	O
,	O
an	O
alternative	O
approach	O
is	O
to	O
restrict	O
the	O
form	O
of	O
the	O
pdf	B
to	O
certain	O
functional	O
forms	O
,	O
some	O
of	O
which	O
we	O
will	O
examine	O
below	O
.	O
2.5.1	O
covariance	B
and	O
correlation	O
the	O
covariance	B
between	O
two	O
rv	O
’	O
s	O
x	O
and	O
y	O
measures	O
the	O
degree	B
to	O
which	O
x	O
and	O
y	O
are	O
(	O
linearly	O
)	O
related	O
.	O
covariance	B
is	O
deﬁned	O
as	O
cov	O
[	O
x	O
,	O
y	O
]	O
(	O
cid:2	O
)	O
e	O
[	O
(	O
x	O
−	O
e	O
[	O
x	O
]	O
)	O
(	O
y	O
−	O
e	O
[	O
y	O
]	O
)	O
]	O
=	O
e	O
[	O
xy	O
]	O
−	O
e	O
[	O
x	O
]	O
e	O
[	O
y	O
]	O
(	O
2.65	O
)	O
2.5.	O
joint	O
probability	O
distributions	O
45	O
figure	O
2.12	O
several	O
sets	O
of	O
(	O
x	O
,	O
y	O
)	O
points	O
,	O
with	O
the	O
correlation	B
coefficient	I
of	O
x	O
and	O
y	O
for	O
each	O
set	O
.	O
note	O
that	O
the	O
correlation	O
reﬂects	O
the	O
noisiness	O
and	O
direction	O
of	O
a	O
linear	O
relationship	O
(	O
top	O
row	O
)	O
,	O
but	O
not	O
the	O
slope	O
of	O
that	O
relationship	O
(	O
middle	O
)	O
,	O
nor	O
many	O
aspects	O
of	O
nonlinear	O
relationships	O
(	O
bottom	O
)	O
.	O
n.b	O
.	O
:	O
the	O
ﬁgure	O
in	O
the	O
center	O
has	O
a	O
slope	O
of	O
0	O
but	O
in	O
that	O
case	O
the	O
correlation	B
coefficient	I
is	O
undeﬁned	O
because	O
the	O
variance	B
of	O
y	O
is	O
zero	O
.	O
source	O
:	O
http	O
:	O
//en.wikipedia.org/wiki/file	O
:	O
correlation_examples.png	O
if	O
x	O
is	O
a	O
d-dimensional	O
random	O
vector	O
,	O
its	O
covariance	B
matrix	I
is	O
deﬁned	O
to	O
be	O
the	O
following	O
symmetric	B
,	O
positive	B
deﬁnite	I
matrix	O
:	O
(	O
cid:18	O
)	O
(	O
cid:17	O
)	O
⎛	O
cov	O
[	O
x	O
]	O
(	O
cid:2	O
)	O
e	O
⎜⎜⎜⎝	O
=	O
(	O
x	O
−	O
e	O
[	O
x	O
]	O
)	O
(	O
x	O
−	O
e	O
[	O
x	O
]	O
)	O
var	O
[	O
x1	O
]	O
t	O
cov	O
[	O
x1	O
,	O
x2	O
]	O
cov	O
[	O
x2	O
,	O
x1	O
]	O
...	O
var	O
[	O
x2	O
]	O
...	O
cov	O
[	O
xd	O
,	O
x1	O
]	O
cov	O
[	O
xd	O
,	O
x2	O
]	O
⎞	O
⎟⎟⎟⎠	O
···	O
···	O
.	O
.	O
.	O
···	O
cov	O
[	O
x1	O
,	O
xd	O
]	O
cov	O
[	O
x2	O
,	O
xd	O
]	O
...	O
var	O
[	O
xd	O
]	O
(	O
2.66	O
)	O
(	O
2.67	O
)	O
covariances	O
can	O
be	O
between	O
0	O
and	O
inﬁnity	O
.	O
sometimes	O
it	O
is	O
more	O
convenient	O
to	O
work	O
with	O
a	O
normalized	O
measure	O
,	O
with	O
a	O
ﬁnite	O
upper	O
bound	O
.	O
the	O
(	O
pearson	O
)	O
correlation	B
coefficient	I
between	O
x	O
and	O
y	O
is	O
deﬁned	O
as	O
(	O
cid:8	O
)	O
corr	O
[	O
x	O
,	O
y	O
]	O
(	O
cid:2	O
)	O
cov	O
[	O
x	O
,	O
y	O
]	O
var	O
[	O
x	O
]	O
var	O
[	O
y	O
]	O
a	O
correlation	B
matrix	I
has	O
the	O
form	O
⎛	O
⎜⎝corr	O
[	O
x1	O
,	O
x1	O
]	O
...	O
r	O
=	O
corr	O
[	O
x1	O
,	O
x2	O
]	O
...	O
⎞	O
⎟⎠	O
···	O
.	O
.	O
.	O
···	O
corr	O
[	O
x1	O
,	O
xd	O
]	O
...	O
corr	O
[	O
xd	O
,	O
xd	O
]	O
(	O
2.68	O
)	O
(	O
2.69	O
)	O
corr	O
[	O
xd	O
,	O
x1	O
]	O
corr	O
[	O
xd	O
,	O
x2	O
]	O
one	O
can	O
show	O
(	O
exercise	O
4.3	O
)	O
that	O
−1	O
≤	O
corr	O
[	O
x	O
,	O
y	O
]	O
≤	O
1.	O
hence	O
in	O
a	O
correlation	B
matrix	I
,	O
each	O
entry	O
on	O
the	O
diagonal	B
is	O
1	O
,	O
and	O
the	O
other	O
entries	O
are	O
between	O
-1	O
and	O
1.	O
one	O
can	O
also	O
show	O
that	O
corr	O
[	O
x	O
,	O
y	O
]	O
=	O
1	O
if	O
and	O
only	O
if	O
y	O
=	O
ax	O
+	O
b	O
for	O
some	O
parameters	O
a	O
and	O
b	O
,	O
i.e.	O
,	O
if	O
there	O
is	O
a	O
linear	O
relationship	O
between	O
x	O
and	O
y	O
(	O
see	O
exercise	O
4.4	O
)	O
.	O
intuitively	O
one	O
46	O
chapter	O
2.	O
probability	O
might	O
expect	O
the	O
correlation	B
coefficient	I
to	O
be	O
related	O
to	O
the	O
slope	O
of	O
the	O
regression	B
line	O
,	O
i.e.	O
,	O
the	O
coefficient	O
a	O
in	O
the	O
expression	O
y	O
=	O
ax	O
+	O
b.	O
however	O
,	O
as	O
we	O
show	O
in	O
equation	O
7.99	O
later	O
,	O
the	O
regression	B
coefficient	O
is	O
in	O
fact	O
given	O
by	O
a	O
=	O
cov	O
[	O
x	O
,	O
y	O
]	O
/var	O
[	O
x	O
]	O
.	O
a	O
better	O
way	O
to	O
think	O
of	O
the	O
correlation	B
coefficient	I
is	O
as	O
a	O
degree	B
of	O
linearity	O
:	O
see	O
figure	O
2.12.	O
if	O
x	O
and	O
y	O
are	O
independent	O
,	O
meaning	O
p	O
(	O
x	O
,	O
y	O
)	O
=p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
(	O
see	O
section	O
2.2.4	O
)	O
,	O
then	O
cov	O
[	O
x	O
,	O
y	O
]	O
=	O
0	O
,	O
and	O
hence	O
corr	O
[	O
x	O
,	O
y	O
]	O
=	O
0	O
so	O
they	O
are	O
uncorrelated	O
.	O
however	O
,	O
the	O
con-	O
verse	O
is	O
not	O
true	O
:	O
uncorrelated	O
does	O
not	O
imply	O
independent	O
.	O
for	O
example	O
,	O
let	O
x	O
∼	O
u	O
(	O
−1	O
,	O
1	O
)	O
and	O
y	O
=	O
x	O
2.	O
clearly	O
y	O
is	O
dependent	O
on	O
x	O
(	O
in	O
fact	O
,	O
y	O
is	O
uniquely	O
determined	O
by	O
x	O
)	O
,	O
yet	O
one	O
can	O
show	O
(	O
exercise	O
4.1	O
)	O
that	O
corr	O
[	O
x	O
,	O
y	O
]	O
=	O
0.	O
some	O
striking	O
examples	O
of	O
this	O
fact	O
are	O
shown	O
in	O
figure	O
2.12.	O
this	O
shows	O
several	O
data	O
sets	O
where	O
there	O
is	O
clear	O
dependendence	O
between	O
x	O
and	O
y	O
,	O
and	O
yet	O
the	O
correlation	B
coefficient	I
is	O
0.	O
a	O
more	O
general	O
measure	O
of	O
dependence	O
between	O
random	O
variables	O
is	O
mutual	B
information	I
,	O
discussed	O
in	O
section	O
2.8.3.	O
this	O
is	O
only	O
zero	O
if	O
the	O
variables	O
truly	O
are	O
independent	O
.	O
2.5.2	O
the	O
multivariate	O
gaussian	O
(	O
cid:26	O
)	O
(	O
cid:25	O
)	O
the	O
multivariate	O
gaussian	O
or	O
multivariate	B
normal	I
(	O
mvn	O
)	O
is	O
the	O
most	O
widely	O
used	O
joint	O
prob-	O
ability	O
density	O
function	O
for	O
continuous	O
variables	O
.	O
we	O
discuss	O
mvns	O
in	O
detail	O
in	O
chapter	O
4	O
;	O
here	O
we	O
just	O
give	O
some	O
deﬁnitions	O
and	O
plots	O
.	O
1	O
−	O
1	O
2	O
the	O
pdf	B
of	O
the	O
mvn	O
in	O
d	O
dimensions	O
is	O
deﬁned	O
by	O
the	O
following	O
:	O
(	O
x	O
−	O
μ	O
)	O
t	O
σ−1	O
(	O
x	O
−	O
μ	O
)	O
n	O
(	O
x|μ	O
,	O
σ	O
)	O
(	O
cid:2	O
)	O
(	O
2π	O
)	O
d/2|σ|1/2	O
exp	O
(	O
2.70	O
)	O
where	O
μ	O
=	O
e	O
[	O
x	O
]	O
∈	O
r	O
d	O
is	O
the	O
mean	B
vector	O
,	O
and	O
σ	O
=	O
cov	O
[	O
x	O
]	O
is	O
the	O
d	O
×	O
d	O
covariance	B
matrix	I
.	O
sometimes	O
we	O
will	O
work	O
in	O
terms	O
of	O
the	O
precision	B
matrix	I
or	O
concentration	B
matrix	I
instead	O
.	O
this	O
is	O
just	O
the	O
inverse	O
covariance	O
matrix	O
,	O
λ	O
=	O
σ−1	O
.	O
the	O
normalization	O
constant	O
(	O
2π	O
)	O
−d/2|λ|1/2	O
just	O
ensures	O
that	O
the	O
pdf	B
integrates	O
to	O
1	O
(	O
see	O
exercise	O
4.5	O
)	O
.	O
figure	O
2.13	O
plots	O
some	O
mvn	O
densities	O
in	O
2d	O
for	O
three	O
different	O
kinds	O
of	O
covariance	B
matrices	O
.	O
a	O
full	B
covariance	O
matrix	O
has	O
d	O
(	O
d	O
+	O
1	O
)	O
/2	O
parameters	O
(	O
we	O
divide	O
by	O
2	O
since	O
σ	O
is	O
symmetric	B
)	O
.	O
a	O
diagonal	O
covariance	O
matrix	O
has	O
d	O
parameters	O
,	O
and	O
has	O
0s	O
in	O
the	O
off-diagonal	O
terms	O
.	O
a	O
spherical	B
or	O
isotropic	B
covariance	O
,	O
σ	O
=	O
σ2id	O
,	O
has	O
one	O
free	O
parameter	O
.	O
2.5.3	O
multivariate	O
student	O
t	O
distribution	O
a	O
more	O
robust	B
alternative	O
to	O
the	O
mvn	O
is	O
the	O
multivariate	O
student	O
t	O
distribution	O
,	O
whose	O
pdf	B
is	O
given	O
by	O
t	O
(	O
x|μ	O
,	O
σ	O
,	O
ν	O
)	O
=	O
=	O
γ	O
(	O
ν/2	O
+	O
d/2	O
)	O
γ	O
(	O
ν/2	O
)	O
γ	O
(	O
ν/2	O
+	O
d/2	O
)	O
γ	O
(	O
ν/2	O
)	O
×	O
|σ|−1/2	O
νd/2πd/2	O
|πv|−1/2	O
×	O
(	O
cid:6	O
)	O
(	O
cid:25	O
)	O
(	O
cid:26	O
)	O
−	O
(	O
ν+d	O
(	O
x	O
−	O
μ	O
)	O
t	O
σ−1	O
(	O
x	O
−	O
μ	O
)	O
(	O
cid:7	O
)	O
−	O
(	O
ν+d	O
1	O
+	O
1	O
+	O
(	O
x	O
−	O
μ	O
)	O
t	O
v−1	O
(	O
x	O
−	O
μ	O
)	O
1	O
ν	O
)	O
2	O
2	O
)	O
(	O
2.71	O
)	O
(	O
2.72	O
)	O
where	O
σ	O
is	O
called	O
the	O
scale	O
matrix	O
(	O
since	O
it	O
is	O
not	O
exactly	O
the	O
covariance	B
matrix	I
)	O
and	O
v	O
=	O
νς	O
.	O
this	O
has	O
fatter	O
tails	O
than	O
a	O
gaussian	O
.	O
the	O
smaller	O
ν	O
is	O
,	O
the	O
fatter	O
the	O
tails	O
.	O
as	O
ν	O
→	O
∞	O
,	O
the	O
2.5.	O
joint	O
probability	O
distributions	O
47	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−6	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
full	B
−4	O
−2	O
0	O
2	O
4	O
6	O
(	O
a	O
)	O
spherical	B
diagonal	O
−4	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
5	O
(	O
b	O
)	O
spherical	B
10	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
−10	O
−5	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
5	O
−4	O
−2	O
0	O
2	O
4	O
6	O
(	O
c	O
)	O
0	O
5	O
0	O
−5	O
−5	O
(	O
d	O
)	O
figure	O
2.13	O
we	O
show	O
the	O
level	B
sets	I
for	O
2d	O
gaussians	O
.	O
(	O
a	O
)	O
a	O
full	B
covariance	O
matrix	O
has	O
elliptical	O
contours	O
.	O
(	O
b	O
)	O
a	O
diagonal	O
covariance	O
matrix	O
is	O
an	O
axis	B
aligned	I
ellipse	O
.	O
(	O
c	O
)	O
a	O
spherical	B
covariance	O
matrix	O
has	O
a	O
circular	O
shape	O
.	O
(	O
d	O
)	O
surface	O
plot	O
for	O
the	O
spherical	B
gaussian	O
in	O
(	O
c	O
)	O
.	O
figure	O
generated	O
by	O
gaussplot2ddemo	O
.	O
distribution	O
tends	O
towards	O
a	O
gaussian	O
.	O
the	O
distribution	O
has	O
the	O
following	O
properties	O
mean	B
=	O
μ	O
,	O
mode	B
=	O
μ	O
,	O
cov	O
=	O
ν	O
ν	O
−	O
2	O
σ	O
(	O
2.73	O
)	O
2.5.4	O
dirichlet	O
distribution	O
a	O
multivariate	O
generalization	O
of	O
the	O
beta	B
distribution	I
is	O
the	O
dirichlet	O
distribution9	O
,	O
which	O
has	O
support	B
over	O
the	O
probability	B
simplex	I
,	O
deﬁned	O
by	O
k	O
(	O
cid:3	O
)	O
k=1	O
k	O
(	O
cid:12	O
)	O
xk	O
=	O
1	O
}	O
xαk−1	O
k	O
i	O
(	O
x	O
∈	O
sk	O
)	O
(	O
2.74	O
)	O
(	O
2.75	O
)	O
sk	O
=	O
{	O
x	O
:	O
0	O
≤	O
xk	O
≤	O
1	O
,	O
the	O
pdf	B
is	O
deﬁned	O
as	O
follows	O
:	O
dir	O
(	O
x|α	O
)	O
(	O
cid:2	O
)	O
1	O
b	O
(	O
α	O
)	O
k=1	O
9.	O
johann	O
dirichlet	O
was	O
a	O
german	O
mathematician	O
,	O
1805–1859	O
.	O
48	O
chapter	O
2.	O
probability	O
(	O
a	O
)	O
(	O
c	O
)	O
(	O
b	O
)	O
α=0.10	O
p	O
15	O
10	O
5	O
0	O
1	O
0.5	O
1	O
0.5	O
0	O
0	O
(	O
d	O
)	O
figure	O
2.14	O
(	O
a	O
)	O
the	O
dirichlet	O
distribution	O
when	O
k	O
=	O
3	O
deﬁnes	O
a	O
distribution	O
over	O
the	O
simplex	O
,	O
which	O
can	O
be	O
represented	O
by	O
the	O
triangular	O
surface	O
.	O
points	O
on	O
this	O
surface	O
satisfy	O
0	O
≤	O
θk	O
≤	O
1	O
and	O
k=1	O
θk	O
=	O
1.	O
figure	O
generated	O
by	O
visdirichletgui	O
,	O
by	O
jonathan	O
huang	O
.	O
(	O
d	O
)	O
α	O
=	O
(	O
0.1	O
,	O
0.1	O
,	O
0.1	O
)	O
.	O
(	O
the	O
comb-like	O
structure	O
on	O
the	O
edges	B
is	O
a	O
plotting	O
artifact	O
.	O
)	O
figure	O
generated	O
by	O
dirichlet3dplot	O
.	O
(	O
b	O
)	O
plot	O
of	O
the	O
dirichlet	O
density	O
when	O
α	O
=	O
(	O
2	O
,	O
2	O
,	O
2	O
)	O
.	O
(	O
cid:2	O
)	O
3	O
(	O
c	O
)	O
α	O
=	O
(	O
20	O
,	O
2	O
,	O
2	O
)	O
.	O
samples	B
from	O
dir	O
(	O
alpha=0.1	O
)	O
samples	B
from	O
dir	O
(	O
alpha=1	O
)	O
1	O
0.5	O
0	O
1	O
0.5	O
0	O
1	O
0.5	O
0	O
1	O
0.5	O
0	O
1	O
0.5	O
0	O
1	O
1	O
1	O
1	O
1	O
2	O
2	O
2	O
2	O
2	O
3	O
3	O
3	O
3	O
3	O
(	O
a	O
)	O
4	O
4	O
4	O
4	O
4	O
5	O
5	O
5	O
5	O
5	O
1	O
0.5	O
0	O
1	O
0.5	O
0	O
1	O
0.5	O
0	O
1	O
0.5	O
0	O
1	O
0.5	O
0	O
1	O
1	O
1	O
1	O
1	O
2	O
2	O
2	O
2	O
2	O
3	O
3	O
3	O
3	O
3	O
(	O
b	O
)	O
4	O
4	O
4	O
4	O
4	O
5	O
5	O
5	O
5	O
5	O
figure	O
2.15	O
(	O
a	O
)	O
α	O
=	O
(	O
0.1	O
,	O
.	O
.	O
.	O
,	O
0.1	O
)	O
.	O
this	O
results	O
in	O
very	O
sparse	B
distributions	O
,	O
with	O
many	O
0s	O
.	O
results	O
in	O
more	O
uniform	O
(	O
and	O
dense	O
)	O
distributions	O
.	O
figure	O
generated	O
by	O
dirichlethistogramdemo	O
.	O
samples	B
from	O
a	O
5-dimensional	O
symmetric	B
dirichlet	O
distribution	O
for	O
different	O
parameter	B
values	O
.	O
(	O
b	O
)	O
α	O
=	O
(	O
1	O
,	O
.	O
.	O
.	O
,	O
1	O
)	O
.	O
this	O
where	O
b	O
(	O
α1	O
,	O
.	O
.	O
.	O
,	O
αk	O
)	O
is	O
the	O
natural	O
generalization	O
of	O
the	O
beta	B
function	I
to	O
k	O
variables	O
:	O
2.6.	O
transformations	O
of	O
random	O
variables	O
(	O
cid:27	O
)	O
k	O
(	O
cid:2	O
)	O
k	O
b	O
(	O
α	O
)	O
(	O
cid:2	O
)	O
where	O
α0	O
(	O
cid:2	O
)	O
k=1	O
γ	O
(	O
αk	O
)	O
γ	O
(	O
α0	O
)	O
k=1	O
αk	O
.	O
(	O
cid:2	O
)	O
k	O
49	O
(	O
2.76	O
)	O
(	O
2.80	O
)	O
(	O
2.81	O
)	O
figure	O
2.14	O
shows	O
some	O
plots	O
of	O
the	O
dirichlet	O
when	O
k	O
=	O
3	O
,	O
and	O
figure	O
2.15	O
for	O
some	O
sampled	O
probability	O
vectors	O
.	O
we	O
see	O
that	O
α0	O
=	O
k=1	O
αk	O
controls	O
the	O
strength	O
of	O
the	O
distribution	O
(	O
how	O
peaked	O
it	O
is	O
)	O
,	O
and	O
the	O
αk	O
control	O
where	O
the	O
peak	O
occurs	O
.	O
for	O
example	O
,	O
dir	O
(	O
1	O
,	O
1	O
,	O
1	O
)	O
is	O
a	O
uniform	B
distribution	I
,	O
dir	O
(	O
2	O
,	O
2	O
,	O
2	O
)	O
is	O
a	O
broad	O
distribution	O
centered	O
at	O
(	O
1/3	O
,	O
1/3	O
,	O
1/3	O
)	O
,	O
and	O
dir	O
(	O
20	O
,	O
20	O
,	O
20	O
)	O
is	O
a	O
narrow	O
distribution	O
centered	O
at	O
(	O
1/3	O
,	O
1/3	O
,	O
1/3	O
)	O
.	O
if	O
αk	O
<	O
1	O
for	O
all	O
k	O
,	O
we	O
get	O
“	O
spikes	O
”	O
at	O
the	O
corner	O
of	O
the	O
simplex	O
.	O
for	O
future	O
reference	O
,	O
the	O
distribution	O
has	O
these	O
properties	O
e	O
[	O
xk	O
]	O
=	O
,	O
mode	B
[	O
xk	O
]	O
=	O
,	O
var	O
[	O
xk	O
]	O
=	O
(	O
2.77	O
)	O
αk	O
−	O
1	O
α0	O
−	O
k	O
αk	O
(	O
α0	O
−	O
αk	O
)	O
α2	O
0	O
(	O
α0	O
+	O
1	O
)	O
where	O
α0	O
=	O
case	O
,	O
the	O
mean	B
becomes	O
1/k	O
,	O
and	O
the	O
variance	B
becomes	O
var	O
[	O
xk	O
]	O
=	O
k−1	O
increases	O
the	O
precision	B
(	O
decreases	O
the	O
variance	B
)	O
of	O
the	O
distribution	O
.	O
k	O
αk	O
.	O
often	O
we	O
use	O
a	O
symmetric	B
dirichlet	O
prior	O
of	O
the	O
form	O
αk	O
=	O
α/k	O
.	O
in	O
this	O
k2	O
(	O
α+1	O
)	O
.	O
so	O
increasing	O
α	O
αk	O
α0	O
(	O
cid:2	O
)	O
2.6	O
transformations	O
of	O
random	O
variables	O
if	O
x	O
∼	O
p	O
(	O
)	O
is	O
some	O
random	O
variable	O
,	O
and	O
y	O
=	O
f	O
(	O
x	O
)	O
,	O
what	O
is	O
the	O
distribution	O
of	O
y	O
?	O
this	O
is	O
the	O
question	O
we	O
address	O
in	O
this	O
section	O
.	O
2.6.1	O
linear	O
transformations	O
suppose	O
f	O
(	O
)	O
is	O
a	O
linear	O
function	O
:	O
y	O
=	O
f	O
(	O
x	O
)	O
=	O
ax	O
+	O
b	O
(	O
2.78	O
)	O
in	O
this	O
case	O
,	O
we	O
can	O
easily	O
derive	O
the	O
mean	B
and	O
covariance	B
of	O
y	O
as	O
follows	O
.	O
first	O
,	O
for	O
the	O
mean	B
,	O
we	O
have	O
e	O
[	O
y	O
]	O
=	O
e	O
[	O
ax	O
+	O
b	O
]	O
=	O
aμ	O
+	O
b	O
(	O
2.79	O
)	O
where	O
μ	O
=	O
e	O
[	O
x	O
]	O
.	O
this	O
is	O
called	O
the	O
linearity	B
of	I
expectation	I
.	O
if	O
f	O
(	O
)	O
is	O
a	O
scalar-valued	O
function	O
,	O
f	O
(	O
x	O
)	O
=	O
at	O
x	O
+	O
b	O
,	O
the	O
corresponding	O
result	O
is	O
(	O
cid:6	O
)	O
(	O
cid:7	O
)	O
e	O
at	O
x	O
+	O
b	O
=	O
at	O
μ	O
+	O
b	O
for	O
the	O
covariance	B
,	O
we	O
have	O
cov	O
[	O
y	O
]	O
=	O
cov	O
[	O
ax	O
+	O
b	O
]	O
=	O
aσat	O
where	O
σ	O
=	O
cov	O
[	O
x	O
]	O
.	O
we	O
leave	O
the	O
proof	O
of	O
this	O
as	O
an	O
exercise	O
.	O
if	O
f	O
(	O
)	O
is	O
scalar	O
valued	O
,	O
the	O
result	O
becomes	O
(	O
cid:6	O
)	O
(	O
cid:7	O
)	O
var	O
[	O
y	O
]	O
=	O
var	O
at	O
x	O
+	O
b	O
=	O
at	O
σa	O
(	O
2.82	O
)	O
50	O
chapter	O
2.	O
probability	O
we	O
will	O
use	O
both	O
of	O
these	O
results	O
extensively	O
in	O
later	O
chapters	O
.	O
note	O
,	O
however	O
,	O
that	O
the	O
mean	B
and	O
covariance	B
only	O
completely	O
deﬁne	O
the	O
distribution	O
of	O
y	O
if	O
x	O
is	O
gaussian	O
.	O
in	O
general	O
we	O
must	O
use	O
the	O
techniques	O
described	O
below	O
to	O
derive	O
the	O
full	B
distribution	O
of	O
y	O
,	O
as	O
opposed	O
to	O
just	O
its	O
ﬁrst	O
two	O
moments	O
.	O
2.6.2	O
general	O
transformations	O
if	O
x	O
is	O
a	O
discrete	B
rv	O
,	O
we	O
can	O
derive	O
the	O
pmf	B
for	O
y	O
by	O
simply	O
summing	O
up	O
the	O
probability	O
mass	O
for	O
all	O
the	O
x	O
’	O
s	O
such	O
that	O
f	O
(	O
x	O
)	O
=y	O
:	O
(	O
cid:3	O
)	O
(	O
2.83	O
)	O
py	O
(	O
y	O
)	O
=	O
px	O
(	O
x	O
)	O
x	O
:	O
f	O
(	O
x	O
)	O
=y	O
(	O
cid:2	O
)	O
for	O
example	O
,	O
if	O
f	O
(	O
x	O
)	O
=	O
1	O
if	O
x	O
is	O
even	O
and	O
f	O
(	O
x	O
)	O
=	O
0	O
otherwise	O
,	O
and	O
px	O
(	O
x	O
)	O
is	O
uniform	O
on	O
the	O
set	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
10	O
}	O
,	O
then	O
py	O
(	O
1	O
)	O
=	O
x∈	O
{	O
2,4,6,8,10	O
}	O
px	O
(	O
x	O
)	O
=	O
0.5	O
,	O
and	O
py	O
(	O
0	O
)	O
=	O
0.5	O
similarly	O
.	O
note	O
that	O
in	O
this	O
example	O
,	O
f	O
is	O
a	O
many-to-one	O
function	O
.	O
if	O
x	O
is	O
continuous	O
,	O
we	O
can	O
not	O
use	O
equation	O
2.83	O
since	O
px	O
(	O
x	O
)	O
is	O
a	O
density	O
,	O
not	O
a	O
pmf	B
,	O
and	O
we	O
can	O
not	O
sum	O
up	O
densities	O
.	O
instead	O
,	O
we	O
work	O
with	O
cdf	B
’	O
s	O
,	O
and	O
write	O
py	O
(	O
y	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
y	O
≤	O
y	O
)	O
=	O
p	O
(	O
f	O
(	O
x	O
)	O
≤	O
y	O
)	O
=	O
p	O
(	O
x	O
∈	O
{	O
x|f	O
(	O
x	O
)	O
≤	O
y	O
}	O
)	O
we	O
can	O
derive	O
the	O
pdf	B
of	O
y	O
by	O
differentiating	O
the	O
cdf	B
.	O
in	O
the	O
case	O
of	O
monotonic	O
and	O
hence	O
invertible	O
functions	O
,	O
we	O
can	O
write	O
py	O
(	O
y	O
)	O
=	O
p	O
(	O
f	O
(	O
x	O
)	O
≤	O
y	O
)	O
=	O
p	O
(	O
x	O
≤	O
f	O
−1	O
(	O
y	O
)	O
)	O
=	O
px	O
(	O
f	O
−1	O
(	O
y	O
)	O
)	O
taking	O
derivatives	O
we	O
get	O
(	O
2.84	O
)	O
(	O
2.85	O
)	O
−1	O
(	O
y	O
)	O
)	O
=	O
px	O
(	O
f	O
d	O
dy	O
py	O
(	O
y	O
)	O
(	O
cid:2	O
)	O
d	O
(	O
2.86	O
)	O
py	O
(	O
y	O
)	O
=	O
dy	O
−1	O
(	O
y	O
)	O
.	O
we	O
can	O
think	O
of	O
dx	O
as	O
a	O
measure	O
of	O
volume	O
in	O
the	O
x-space	O
;	O
similarly	O
dy	O
dy	O
measures	O
the	O
change	O
in	O
volume	O
.	O
since	O
the	O
sign	O
of	O
this	O
where	O
x	O
=	O
f	O
measures	O
volume	O
in	O
y	O
space	O
.	O
thus	O
dx	O
change	O
is	O
not	O
important	O
,	O
we	O
take	O
the	O
absolute	O
value	O
to	O
get	O
the	O
general	O
expression	O
:	O
px	O
(	O
x	O
)	O
=	O
dx	O
dy	O
dx	O
dy	O
d	O
dx	O
px	O
(	O
x	O
)	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
dx	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
dy	O
py	O
(	O
y	O
)	O
=	O
px	O
(	O
x	O
)	O
this	O
is	O
called	O
change	B
of	I
variables	I
formula	O
.	O
we	O
can	O
understand	O
this	O
result	O
more	O
intuitively	O
as	O
follows	O
.	O
observations	O
falling	O
in	O
the	O
range	O
(	O
x	O
,	O
x	O
+	O
δx	O
)	O
will	O
get	O
transformed	O
into	O
(	O
y	O
,	O
y	O
+	O
δy	O
)	O
,	O
where	O
px	O
(	O
x	O
)	O
δx	O
≈	O
py	O
(	O
y	O
)	O
δy	O
.	O
hence	O
py	O
(	O
y	O
)	O
≈	O
px	O
(	O
x	O
)	O
|	O
δx	O
δy|	O
.	O
for	O
example	O
,	O
suppose	O
x	O
∼	O
u	O
(	O
−1	O
,	O
1	O
)	O
,	O
and	O
y	O
=	O
x	O
2.	O
then	O
py	O
(	O
y	O
)	O
=	O
1	O
2	O
y	O
2	O
.	O
see	O
also	O
exercise	O
2.10	O
.	O
−	O
1	O
2.6.2.1	O
multivariate	O
change	O
of	O
variables	O
*	O
we	O
can	O
extend	O
the	O
previous	O
results	O
to	O
multivariate	O
distributions	O
as	O
follows	O
.	O
let	O
f	O
be	O
a	O
function	O
that	O
maps	O
r	O
n	O
,	O
and	O
let	O
y	O
=	O
f	O
(	O
x	O
)	O
.	O
then	O
its	O
jacobian	O
matrix	O
j	O
is	O
given	O
by	O
n	O
to	O
r	O
jx→y	O
(	O
cid:2	O
)	O
∂	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
)	O
∂	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
(	O
cid:2	O
)	O
⎛	O
⎜⎝	O
∂y1	O
...	O
∂x1	O
∂yn	O
∂x1	O
···	O
.	O
.	O
.	O
···	O
⎞	O
⎟⎠	O
∂y1	O
∂xn	O
...	O
∂yn	O
∂xn	O
(	O
2.87	O
)	O
(	O
2.88	O
)	O
2.6.	O
transformations	O
of	O
random	O
variables	O
51	O
|	O
det	O
j|	O
measures	O
how	O
much	O
a	O
unit	O
cube	O
changes	O
in	O
volume	O
when	O
we	O
apply	O
f.	O
jacobian	O
of	O
the	O
inverse	O
mapping	O
y	O
→	O
x	O
:	O
if	O
f	O
is	O
an	O
invertible	O
mapping	O
,	O
we	O
can	O
deﬁne	O
the	O
pdf	B
of	O
the	O
transformed	O
variables	O
using	O
the	O
(	O
cid:9	O
)	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
det	O
∂x	O
∂y	O
(	O
cid:10	O
)	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
=	O
px	O
(	O
x	O
)	O
|	O
det	O
jy→x|	O
py	O
(	O
y	O
)	O
=	O
px	O
(	O
x	O
)	O
in	O
exercise	O
4.5	O
you	O
will	O
use	O
this	O
formula	O
to	O
derive	O
the	O
normalization	O
constant	O
for	O
a	O
multivariate	O
gaussian	O
.	O
as	O
a	O
simple	O
example	O
,	O
consider	O
transforming	O
a	O
density	O
from	O
cartesian	O
coordinates	O
x	O
=	O
(	O
x1	O
,	O
x2	O
)	O
to	O
polar	B
coordinates	O
y	O
=	O
(	O
r	O
,	O
θ	O
)	O
,	O
wherex	O
1	O
=	O
r	O
cos	O
θ	O
and	O
x2	O
=	O
r	O
sin	O
θ.	O
then	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
(	O
cid:9	O
)	O
=	O
cos	O
θ	O
−r	O
sin	O
θ	O
r	O
cos	O
θ	O
sin	O
θ	O
(	O
cid:10	O
)	O
jy→x	O
=	O
∂x1	O
∂r	O
∂x2	O
∂r	O
∂x1	O
∂θ	O
∂x2	O
∂θ	O
and	O
|	O
det	O
j|	O
=	O
|r	O
cos2	O
θ	O
+	O
r	O
sin2	O
θ|	O
=	O
|r|	O
hence	O
py	O
(	O
y	O
)	O
=p	O
x	O
(	O
x	O
)	O
|	O
det	O
j|	O
pr	O
,	O
θ	O
(	O
r	O
,	O
θ	O
)	O
=p	O
x1	O
,	O
x2	O
(	O
x1	O
,	O
x2	O
)	O
r	O
=	O
px1	O
,	O
x2	O
(	O
r	O
cos	O
θ	O
,	O
r	O
sin	O
θ	O
)	O
r	O
(	O
2.89	O
)	O
(	O
2.90	O
)	O
(	O
2.91	O
)	O
(	O
2.92	O
)	O
(	O
2.93	O
)	O
(	O
2.94	O
)	O
(	O
2.96	O
)	O
(	O
2.97	O
)	O
to	O
see	O
this	O
geometrically	O
,	O
notice	O
that	O
the	O
area	O
of	O
the	O
shaded	O
patch	O
in	O
figure	O
2.16	O
is	O
given	O
by	O
p	O
(	O
r	O
≤	O
r	O
≤	O
r	O
+	O
dr	O
,	O
θ	O
≤	O
θ	O
≤	O
θ	O
+	O
dθ	O
)	O
=p	O
r	O
,	O
θ	O
(	O
r	O
,	O
θ	O
)	O
drdθ	O
in	O
the	O
limit	O
,	O
this	O
is	O
equal	O
to	O
the	O
density	O
at	O
the	O
center	O
of	O
the	O
patch	O
,	O
p	O
(	O
r	O
,	O
θ	O
)	O
,	O
times	O
the	O
size	O
of	O
the	O
patch	O
,	O
r	O
dr	O
dθ	O
.	O
hence	O
pr	O
,	O
θ	O
(	O
r	O
,	O
θ	O
)	O
drdθ	O
=	O
px1	O
,	O
x2	O
(	O
r	O
cos	O
θ	O
,	O
r	O
sin	O
θ	O
)	O
r	O
dr	O
dθ	O
(	O
2.95	O
)	O
2.6.3	O
central	B
limit	I
theorem	I
now	O
consider	O
n	O
random	O
variables	O
with	O
pdf	B
’	O
s	O
(	O
not	O
necessarily	O
gaussian	O
)	O
p	O
(	O
xi	O
)	O
,	O
each	O
with	O
mean	B
μ	O
and	O
variance	B
σ2	O
.	O
we	O
assume	O
each	O
variable	O
is	O
independent	B
and	I
identically	I
distributed	I
or	O
iid	B
for	O
short	O
.	O
let	O
sn	O
=	O
i=1	O
xi	O
be	O
the	O
sum	O
of	O
the	O
rv	O
’	O
s	O
.	O
this	O
is	O
a	O
simple	O
but	O
widely	O
used	O
transformation	O
of	O
rv	O
’	O
s	O
.	O
one	O
can	O
show	O
that	O
,	O
as	O
n	O
increases	O
,	O
the	O
distribution	O
of	O
this	O
sum	O
approaches	O
(	O
cid:10	O
)	O
(	O
cid:2	O
)	O
n	O
(	O
cid:9	O
)	O
p	O
(	O
sn	O
=	O
s	O
)	O
=	O
1√	O
2πn	O
σ2	O
exp	O
−	O
(	O
s	O
−	O
n	O
μ	O
)	O
2	O
2n	O
σ2	O
hence	O
the	O
distribution	O
of	O
the	O
quantity	O
zn	O
(	O
cid:2	O
)	O
sn	O
−	O
n	O
μ	O
√	O
=	O
σ	O
n	O
x	O
−	O
μ	O
√	O
n	O
σ/	O
(	O
cid:2	O
)	O
n	O
converges	O
to	O
the	O
standard	B
normal	I
,	O
where	O
x	O
=	O
1	O
n	O
the	O
central	B
limit	I
theorem	I
.	O
see	O
e.g.	O
,	O
(	O
jaynes	O
2003	O
,	O
p222	O
)	O
or	O
(	O
rice	O
1995	O
,	O
p169	O
)	O
for	O
a	O
proof	O
.	O
i=1	O
xi	O
is	O
the	O
sample	O
mean	O
.	O
this	O
is	O
called	O
in	O
figure	O
2.17	O
we	O
give	O
an	O
example	O
in	O
which	O
we	O
compute	O
the	O
mean	B
of	O
rv	O
’	O
s	O
drawn	O
from	O
a	O
beta	B
distribution	I
.	O
we	O
see	O
that	O
the	O
sampling	B
distribution	I
of	O
the	O
mean	B
value	O
rapidly	O
converges	O
to	O
a	O
gaussian	O
distribution	O
.	O
52	O
chapter	O
2.	O
probability	O
figure	O
2.16	O
change	B
of	I
variables	I
from	O
polar	B
to	O
cartesian	O
.	O
the	O
area	O
of	O
the	O
shaded	O
patch	O
is	O
r	O
dr	O
dθ	O
.	O
based	O
on	O
(	O
rice	O
1995	O
)	O
figure	O
3.16.	O
n	O
=	O
1	O
n	O
=	O
5	O
3	O
2	O
1	O
0	O
0	O
3	O
2	O
1	O
0.5	O
(	O
a	O
)	O
1	O
0	O
0	O
0.5	O
(	O
b	O
)	O
1	O
figure	O
2.17	O
the	O
central	B
limit	I
theorem	I
in	O
pictures	O
.	O
we	O
plot	O
a	O
histogram	B
of	O
1	O
beta	O
(	O
1	O
,	O
5	O
)	O
,	O
for	O
j	O
=	O
1	O
:	O
10000.	O
as	O
n	O
→	O
∞	O
,	O
the	O
distribution	O
tends	O
towards	O
a	O
gaussian	O
.	O
n	O
n	O
=	O
5.	O
based	O
on	O
figure	O
2.6	O
of	O
(	O
bishop	O
2006a	O
)	O
.	O
figure	O
generated	O
by	O
centrallimitdemo	O
.	O
(	O
cid:2	O
)	O
n	O
i=1	O
xij	O
,	O
where	O
xij	O
∼	O
(	O
b	O
)	O
(	O
a	O
)	O
n	O
=	O
1	O
.	O
2.7	O
monte	O
carlo	O
approximation	O
in	O
general	O
,	O
computing	O
the	O
distribution	O
of	O
a	O
function	O
of	O
an	O
rv	O
using	O
the	O
change	B
of	I
variables	I
formula	O
can	O
be	O
difficult	O
.	O
one	O
simple	O
but	O
powerful	O
alternative	O
is	O
as	O
follows	O
.	O
first	O
we	O
generate	O
s	O
samples	B
from	O
the	O
distribution	O
,	O
call	O
them	O
x1	O
,	O
.	O
.	O
.	O
,	O
xs	O
.	O
(	O
there	O
are	O
many	O
ways	O
to	O
generate	O
such	O
samples	B
;	O
one	O
popular	O
method	O
,	O
for	O
high	O
dimensional	O
distributions	O
,	O
is	O
called	O
markov	O
chain	O
monte	O
carlo	O
or	O
mcmc	O
;	O
this	O
will	O
be	O
explained	O
in	O
chapter	O
24	O
.	O
)	O
given	O
the	O
samples	B
,	O
we	O
can	O
approximate	O
the	O
distribution	O
of	O
f	O
(	O
x	O
)	O
by	O
using	O
the	O
empirical	B
distribution	I
of	O
{	O
f	O
(	O
xs	O
)	O
}	O
s	O
s=1	O
.	O
this	O
is	O
called	O
a	O
monte	O
carlo	O
approximation	O
,	O
named	O
after	O
a	O
city	O
in	O
europe	O
known	O
for	O
its	O
plush	O
gambling	O
casinos	O
.	O
monte	O
carlo	O
techniques	O
were	O
ﬁrst	O
developed	O
in	O
the	O
area	O
of	O
statistical	O
physics	O
—	O
in	O
particular	O
,	O
during	O
development	O
of	O
the	O
atomic	B
bomb	I
—	O
but	O
are	O
now	O
widely	O
used	O
in	O
statistics	O
and	O
machine	B
learning	I
as	O
well	O
.	O
we	O
can	O
use	O
monte	O
carlo	O
to	O
approximate	O
the	O
expected	B
value	I
of	O
any	O
function	O
of	O
a	O
random	O
2.7.	O
monte	O
carlo	O
approximation	O
53	O
1.5	O
1	O
0.5	O
0	O
6	O
4	O
2	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
−0.5	O
−1	O
0	O
0	O
0	O
1	O
0.5	O
0	O
0	O
1	O
0.5	O
1	O
figure	O
2.18	O
computing	O
the	O
distribution	O
of	O
y	O
=	O
x2	O
,	O
where	O
p	O
(	O
x	O
)	O
is	O
uniform	O
(	O
left	O
)	O
.	O
the	O
analytic	O
result	O
is	O
shown	O
in	O
the	O
middle	O
,	O
and	O
the	O
monte	O
carlo	O
approximation	O
is	O
shown	O
on	O
the	O
right	O
.	O
figure	O
generated	O
by	O
changeofvarsdemo1d	O
.	O
variable	O
.	O
we	O
simply	O
draw	O
samples	B
,	O
and	O
then	O
compute	O
the	O
arithmetic	O
mean	B
of	O
the	O
function	O
applied	O
to	O
the	O
samples	B
.	O
this	O
can	O
be	O
written	O
as	O
follows	O
:	O
f	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
dx	O
≈	O
1	O
s	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
=	O
(	O
2.98	O
)	O
where	O
xs	O
∼	O
p	O
(	O
x	O
)	O
.	O
this	O
is	O
called	O
monte	O
carlo	O
integration	O
,	O
and	O
has	O
the	O
advantage	O
over	O
numerical	O
integration	O
(	O
which	O
is	O
based	O
on	O
evaluating	O
the	O
function	O
at	O
a	O
ﬁxed	O
grid	O
of	O
points	O
)	O
that	O
the	O
function	O
is	O
only	O
evaluated	O
in	O
places	O
where	O
there	O
is	O
non-negligible	O
probability	O
.	O
f	O
(	O
xs	O
)	O
s=1	O
by	O
varying	O
the	O
function	O
f	O
(	O
)	O
,	O
we	O
can	O
approximate	O
many	O
quantities	O
of	O
interest	O
,	O
such	O
as	O
(	O
cid:4	O
)	O
s	O
(	O
cid:3	O
)	O
•	O
x	O
=	O
1	O
s	O
(	O
cid:2	O
)	O
s	O
s=1	O
xs	O
→	O
e	O
[	O
x	O
]	O
(	O
cid:2	O
)	O
s	O
s=1	O
(	O
xs	O
−	O
x	O
)	O
2	O
→	O
var	O
[	O
x	O
]	O
s	O
#	O
{	O
xs	O
≤	O
c	O
}	O
→p	O
(	O
x	O
≤	O
c	O
)	O
1	O
s	O
1	O
•	O
•	O
•	O
median	B
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xs	O
}	O
→median	O
(	O
x	O
)	O
we	O
give	O
some	O
examples	O
below	O
,	O
and	O
will	O
see	O
many	O
more	O
in	O
later	O
chapters	O
.	O
2.7.1	O
example	O
:	O
change	B
of	I
variables	I
,	O
the	O
mc	O
way	O
in	O
section	O
2.6.2	O
,	O
we	O
discussed	O
how	O
to	O
analytically	O
compute	O
the	O
distribution	O
of	O
a	O
function	O
of	O
a	O
random	O
variable	O
,	O
y	O
=	O
f	O
(	O
x	O
)	O
.	O
a	O
much	O
simpler	O
approach	O
is	O
to	O
use	O
a	O
monte	O
carlo	O
approximation	O
.	O
for	O
example	O
,	O
suppose	O
x	O
∼	O
unif	O
(	O
−1	O
,	O
1	O
)	O
and	O
y	O
=	O
x2	O
.	O
we	O
can	O
approximate	O
p	O
(	O
y	O
)	O
by	O
drawing	O
many	O
samples	B
from	O
p	O
(	O
x	O
)	O
,	O
squaring	O
them	O
,	O
and	O
computing	O
the	O
resulting	O
empirical	B
distribution	I
.	O
see	O
figure	O
2.18	O
for	O
an	O
illustration	O
.	O
we	O
will	O
use	O
this	O
technique	O
extensively	O
in	O
later	O
chapters	O
.	O
see	O
also	O
figure	O
5.2	O
.	O
54	O
chapter	O
2.	O
probability	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−2	O
−2	O
−1	O
0	O
1	O
2	O
figure	O
2.19	O
estimating	O
π	O
by	O
monte	O
carlo	O
integration	O
.	O
blue	O
points	O
are	O
inside	O
the	O
circle	O
,	O
red	O
crosses	O
are	O
outside	O
.	O
figure	O
generated	O
by	O
mcestimatepi	O
.	O
2.7.2	O
example	O
:	O
estimating	O
π	O
by	O
monte	O
carlo	O
integration	O
mc	O
approximation	O
can	O
be	O
used	O
for	O
many	O
applications	O
,	O
not	O
just	O
statistical	O
ones	O
.	O
suppose	O
we	O
want	O
to	O
estimate	O
π.	O
we	O
know	O
that	O
the	O
area	O
of	O
a	O
circle	O
with	O
radius	O
r	O
is	O
πr2	O
,	O
but	O
it	O
is	O
also	O
equal	O
to	O
the	O
following	O
deﬁnite	O
integral	O
:	O
i	O
(	O
x2	O
+	O
y2	O
≤	O
r2	O
)	O
dxdy	O
(	O
2.99	O
)	O
(	O
cid:4	O
)	O
r	O
(	O
cid:4	O
)	O
r	O
i	O
=	O
−r	O
−r	O
hence	O
π	O
=	O
i/	O
(	O
r2	O
)	O
.	O
let	O
f	O
(	O
x	O
,	O
y	O
)	O
=	O
i	O
(	O
x2	O
+	O
y2	O
≤	O
r2	O
)	O
be	O
an	O
indicator	B
function	I
that	O
is	O
1	O
for	O
points	O
inside	O
the	O
circle	O
,	O
and	O
0	O
outside	O
,	O
and	O
let	O
p	O
(	O
x	O
)	O
and	O
p	O
(	O
y	O
)	O
be	O
uniform	O
distributions	O
on	O
[	O
−r	O
,	O
r	O
]	O
,	O
sop	O
(	O
x	O
)	O
=p	O
(	O
y	O
)	O
=	O
1/	O
(	O
2r	O
)	O
.	O
then	O
let	O
us	O
approximate	O
this	O
by	O
monte	O
carlo	O
integration	O
.	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
i	O
=	O
(	O
2r	O
)	O
(	O
2r	O
)	O
f	O
(	O
x	O
,	O
y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
dxdy	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
s	O
(	O
cid:3	O
)	O
=	O
4r2	O
≈	O
4r2	O
1	O
s	O
f	O
(	O
xs	O
,	O
ys	O
)	O
s=1	O
f	O
(	O
x	O
,	O
y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
dxdy	O
(	O
2.100	O
)	O
(	O
2.101	O
)	O
(	O
2.102	O
)	O
we	O
ﬁnd	O
ˆπ	O
=	O
3.1416	O
with	O
standard	B
error	I
0.09	O
(	O
see	O
section	O
2.7.3	O
for	O
a	O
discussion	O
of	O
standard	B
errors	I
)	O
.	O
we	O
can	O
plot	O
the	O
points	O
that	O
are	O
accepted/	O
rejected	O
as	O
in	O
figure	O
2.19	O
.	O
2.7.3	O
accuracy	O
of	O
monte	O
carlo	O
approximation	O
the	O
accuracy	O
of	O
an	O
mc	O
approximation	O
increases	O
with	O
sample	O
size	O
.	O
this	O
is	O
illustrated	O
in	O
fig-	O
ure	O
2.20	O
,	O
on	O
the	O
top	O
line	O
,	O
we	O
plot	O
a	O
histogram	B
of	O
samples	B
from	O
a	O
gaussian	O
distribution	O
.	O
on	O
the	O
bottom	O
line	O
,	O
we	O
plot	O
a	O
smoothed	O
version	O
of	O
these	O
samples	B
,	O
created	O
using	O
a	O
kernel	O
density	O
estimate	O
(	O
section	O
14.7.2	O
)	O
.	O
this	O
smoothed	O
distribution	O
is	O
then	O
evaluated	O
on	O
a	O
dense	O
grid	O
of	O
points	O
2.7.	O
monte	O
carlo	O
approximation	O
55	O
10	O
samples	B
100	O
samples	B
6	O
5	O
4	O
3	O
2	O
1	O
0	O
0.5	O
1	O
1.5	O
(	O
a	O
)	O
10	O
samples	B
2.5	O
2	O
1.5	O
1	O
0.5	O
2	O
1.8	O
1.6	O
1.4	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
2	O
2.5	O
0	O
0.5	O
1	O
1.8	O
1.6	O
1.4	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
2	O
2.5	O
1.5	O
(	O
b	O
)	O
100	O
samples	B
0	O
0.5	O
1	O
1.5	O
(	O
c	O
)	O
2	O
2.5	O
0	O
0.5	O
1	O
2	O
2.5	O
1.5	O
(	O
d	O
)	O
figure	O
2.20	O
10	O
and	O
100	O
samples	B
from	O
a	O
gaussian	O
distribution	O
,	O
n	O
(	O
μ	O
=	O
1.5	O
,	O
σ2	O
=	O
0.25	O
)	O
.	O
solid	O
red	O
line	O
is	O
true	O
pdf	O
.	O
top	O
line	O
:	O
histogram	B
of	O
samples	B
.	O
bottom	O
line	O
:	O
kernel	O
density	O
estimate	O
derived	O
from	O
samples	B
in	O
dotted	O
blue	O
,	O
solid	O
red	O
line	O
is	O
true	O
pdf	O
.	O
based	O
on	O
figure	O
4.1	O
of	O
(	O
hoff	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
mcaccuracydemo	O
.	O
and	O
plotted	O
.	O
note	O
that	O
this	O
smoothing	O
is	O
just	O
for	O
the	O
purposes	O
of	O
plotting	O
,	O
it	O
is	O
not	O
used	O
for	O
the	O
monte	O
carlo	O
estimate	O
itself	O
.	O
if	O
we	O
denote	O
the	O
exact	O
mean	B
by	O
μ	O
=	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
,	O
and	O
the	O
mc	O
approximation	O
by	O
ˆμ	O
,	O
one	O
can	O
show	O
that	O
,	O
with	O
independent	O
samples	O
,	O
(	O
ˆμ	O
−	O
μ	O
)	O
→	O
n	O
(	O
0	O
,	O
σ2	O
s	O
)	O
where	O
σ2	O
=	O
var	O
[	O
f	O
(	O
x	O
)	O
]	O
=	O
e	O
(	O
cid:6	O
)	O
(	O
cid:7	O
)	O
−	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
2	O
f	O
(	O
x	O
)	O
2	O
(	O
2.103	O
)	O
(	O
2.104	O
)	O
this	O
is	O
a	O
consequence	O
of	O
the	O
central-limit	B
theorem	I
.	O
of	O
course	O
,	O
σ2	O
is	O
unknown	B
in	O
the	O
above	O
expression	O
,	O
but	O
it	O
can	O
also	O
be	O
estimated	O
by	O
mc	O
:	O
ˆσ2	O
=	O
(	O
f	O
(	O
xs	O
)	O
−	O
ˆμ	O
)	O
2	O
s	O
(	O
cid:3	O
)	O
s=1	O
1	O
s	O
(	O
cid:11	O
)	O
then	O
we	O
have	O
μ	O
−	O
1.96	O
p	O
(	O
cid:29	O
)	O
≈	O
0.95	O
(	O
2.105	O
)	O
(	O
2.106	O
)	O
ˆσ√	O
s	O
≤	O
ˆμ	O
≤	O
μ	O
+	O
1.96	O
ˆσ√	O
s	O
56	O
(	O
cid:30	O
)	O
chapter	O
2.	O
probability	O
ˆσ2	O
s	O
is	O
called	O
the	O
(	O
numerical	O
or	O
empirical	O
)	O
standard	B
error	I
,	O
and	O
is	O
an	O
estimate	O
of	O
our	O
the	O
term	O
(	O
cid:8	O
)	O
uncertainty	B
about	O
our	O
estimate	O
of	O
μ	O
.	O
(	O
see	O
section	O
6.2	O
for	O
more	O
discussion	O
on	O
standard	B
errors	I
.	O
)	O
if	O
we	O
want	O
to	O
report	O
an	O
answer	O
which	O
is	O
accurate	O
to	O
within	O
±	O
with	O
probability	O
at	O
least	O
95	O
%	O
,	O
ˆσ2/s	O
≤	O
	O
.	O
we	O
can	O
approximate	O
we	O
need	O
to	O
use	O
a	O
number	O
of	O
samples	B
s	O
which	O
satisﬁes	O
1.96	O
the	O
1.96	O
factor	B
by	O
2	O
,	O
yielding	O
s	O
≥	O
4ˆσ2	O
2	O
.	O
2.8	O
information	B
theory	I
information	O
theory	O
is	O
concerned	O
with	O
representing	O
data	O
in	O
a	O
compact	O
fashion	O
(	O
a	O
task	O
known	O
as	O
data	B
compression	I
or	O
source	B
coding	I
)	O
,	O
as	O
well	O
as	O
with	O
transmitting	O
and	O
storing	O
it	O
in	O
a	O
way	O
that	O
is	O
robust	B
to	O
errors	O
(	O
a	O
task	O
known	O
as	O
error	B
correction	I
or	O
channel	B
coding	I
)	O
.	O
at	O
ﬁrst	O
,	O
this	O
seems	O
far	O
removed	O
from	O
the	O
concerns	O
of	O
probability	O
theory	O
and	O
machine	B
learning	I
,	O
but	O
in	O
fact	O
there	O
is	O
an	O
intimate	O
connection	O
.	O
to	O
see	O
this	O
,	O
note	O
that	O
compactly	O
representing	O
data	O
requires	O
allocating	O
short	O
codewords	O
to	O
highly	O
probable	O
bit	O
strings	O
,	O
and	O
reserving	O
longer	O
codewords	O
to	O
less	O
probable	O
bit	O
strings	O
.	O
this	O
is	O
similar	B
to	O
the	O
situation	O
in	O
natural	O
language	O
,	O
where	O
common	O
words	O
(	O
such	O
as	O
“	O
a	O
”	O
,	O
“	O
the	O
”	O
,	O
“	O
and	O
”	O
)	O
are	O
generally	O
much	O
shorter	O
than	O
rare	O
words	O
.	O
also	O
,	O
decoding	B
messages	O
sent	O
over	O
noisy	O
channels	O
requires	O
having	O
a	O
good	O
probability	O
model	O
of	O
the	O
kinds	O
of	O
messages	O
that	O
people	O
tend	O
to	O
send	O
.	O
in	O
both	O
cases	O
,	O
we	O
need	O
a	O
model	O
that	O
can	O
predict	O
which	O
kinds	O
of	O
data	O
are	O
likely	O
and	O
which	O
unlikely	O
,	O
which	O
is	O
also	O
a	O
central	O
problem	O
in	O
machine	B
learning	I
(	O
see	O
(	O
mackay	O
2003	O
)	O
for	O
more	O
details	O
on	O
the	O
connection	O
between	O
information	B
theory	I
and	O
machine	B
learning	I
)	O
.	O
obviously	O
we	O
can	O
not	O
go	O
into	O
the	O
details	O
of	O
information	B
theory	I
here	O
(	O
see	O
e.g.	O
,	O
(	O
cover	O
and	O
thomas	O
2006	O
)	O
if	O
you	O
are	O
interested	O
to	O
learn	O
more	O
)	O
.	O
however	O
,	O
we	O
will	O
introduce	O
a	O
few	O
basic	O
concepts	O
that	O
we	O
will	O
need	O
later	O
in	O
the	O
book	O
.	O
2.8.1	O
entropy	B
the	O
entropy	B
of	O
a	O
random	O
variable	O
x	O
with	O
distribution	O
p	O
,	O
denoted	O
by	O
h	O
(	O
x	O
)	O
or	O
sometimes	O
h	O
(	O
p	O
)	O
,	O
is	O
a	O
measure	O
of	O
its	O
uncertainty	B
.	O
in	O
particular	O
,	O
for	O
a	O
discrete	B
variable	O
with	O
k	O
states	O
,	O
it	O
is	O
deﬁned	O
by	O
h	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
−	O
k	O
(	O
cid:3	O
)	O
p	O
(	O
x	O
=	O
k	O
)	O
log2	O
p	O
(	O
x	O
=	O
k	O
)	O
(	O
2.107	O
)	O
k=1	O
usually	O
we	O
use	O
log	O
base	O
2	O
,	O
in	O
which	O
case	O
the	O
units	O
are	O
called	O
bits	B
(	O
short	O
for	O
binary	O
digits	O
)	O
.	O
if	O
we	O
use	O
log	O
base	O
e	O
,	O
the	O
units	O
are	O
called	O
nats	B
.	O
for	O
example	O
,	O
if	O
x	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
5	O
}	O
with	O
histogram	B
distribution	O
p	O
=	O
[	O
0.25	O
,	O
0.25	O
,	O
0.2	O
,	O
0.15	O
,	O
0.15	O
]	O
,	O
we	O
ﬁnd	O
h	O
=	O
2.2855.	O
the	O
discrete	B
distribution	O
with	O
maximum	B
entropy	I
is	O
the	O
uniform	B
distribution	I
(	O
see	O
section	O
9.2.6	O
for	O
a	O
proof	O
)	O
.	O
hence	O
for	O
a	O
k-ary	O
random	O
variable	O
,	O
the	O
entropy	B
is	O
maximized	O
if	O
p	O
(	O
x	O
=	O
k	O
)	O
=	O
1/k	O
;	O
in	O
this	O
case	O
,	O
h	O
(	O
x	O
)	O
=	O
log2	O
k.	O
conversely	O
,	O
the	O
distribution	O
with	O
minimum	O
entropy	O
(	O
which	O
is	O
zero	O
)	O
is	O
any	O
delta-function	O
that	O
puts	O
all	O
its	O
mass	O
on	O
one	O
state	B
.	O
such	O
a	O
distribution	O
has	O
no	O
uncertainty	O
.	O
in	O
figure	O
2.5	O
(	O
b	O
)	O
,	O
where	O
we	O
plotted	O
a	O
dna	O
sequence	B
logo	I
,	O
the	O
height	O
of	O
each	O
bar	O
is	O
deﬁned	O
to	O
be	O
2	O
−	O
h	O
,	O
where	O
h	O
is	O
the	O
entropy	B
of	O
that	O
distribution	O
,	O
and	O
2	O
is	O
the	O
maximum	O
possible	O
entropy	B
.	O
thus	O
a	O
bar	O
of	O
height	O
0	O
corresponds	O
to	O
a	O
uniform	B
distribution	I
,	O
whereas	O
a	O
bar	O
of	O
height	O
2	O
corresponds	O
to	O
a	O
deterministic	O
distribution	O
.	O
2.8.	O
information	B
theory	I
57	O
1	O
)	O
(	O
x	O
h	O
0.5	O
0	O
0	O
0.5	O
p	O
(	O
x	O
=	O
1	O
)	O
1	O
figure	O
2.21	O
entropy	B
of	O
a	O
bernoulli	O
random	O
variable	O
as	O
a	O
function	O
of	O
θ.	O
the	O
maximum	B
entropy	I
is	O
log2	O
2	O
=	O
1.	O
figure	O
generated	O
by	O
bernoullientropyfig	O
.	O
for	O
the	O
special	O
case	O
of	O
binary	O
random	O
variables	O
,	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
we	O
can	O
write	O
p	O
(	O
x	O
=	O
1	O
)	O
=	O
θ	O
and	O
p	O
(	O
x	O
=	O
0	O
)	O
=	O
1	O
−	O
θ.	O
hence	O
the	O
entropy	B
becomes	O
h	O
(	O
x	O
)	O
=−	O
[	O
p	O
(	O
x	O
=	O
1	O
)	O
log2	O
p	O
(	O
x	O
=	O
1	O
)	O
+	O
p	O
(	O
x	O
=	O
0	O
)	O
log2	O
p	O
(	O
x	O
=	O
0	O
)	O
]	O
=	O
−	O
[	O
θ	O
log2	O
θ	O
+	O
(	O
1−	O
θ	O
)	O
log2	O
(	O
1	O
−	O
θ	O
)	O
]	O
(	O
2.108	O
)	O
(	O
2.109	O
)	O
this	O
is	O
called	O
the	O
binary	B
entropy	I
function	I
,	O
and	O
is	O
also	O
written	O
h	O
(	O
θ	O
)	O
.	O
we	O
plot	O
this	O
in	O
figure	O
2.21.	O
we	O
see	O
that	O
the	O
maximum	O
value	O
of	O
1	O
occurs	O
when	O
the	O
distribution	O
is	O
uniform	O
,	O
θ	O
=	O
0.5	O
.	O
2.8.2	O
kl	O
divergence	O
one	O
way	O
to	O
measure	O
the	O
dissimilarity	O
of	O
two	O
probability	O
distributions	O
,	O
p	O
and	O
q	O
,	O
is	O
known	O
as	O
the	O
kullback-leibler	O
divergence	O
(	O
kl	O
divergence	O
)	O
orrelative	O
entropy	B
.	O
this	O
is	O
deﬁned	O
as	O
follows	O
:	O
kl	O
(	O
p||q	O
)	O
(	O
cid:2	O
)	O
pk	O
log	O
pk	O
qk	O
k	O
(	O
cid:3	O
)	O
k=1	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
k	O
k	O
pk	O
log	O
pk	O
−	O
(	O
cid:3	O
)	O
pk	O
log	O
qk	O
k	O
(	O
2.110	O
)	O
(	O
2.111	O
)	O
(	O
2.112	O
)	O
where	O
the	O
sum	O
gets	O
replaced	O
by	O
an	O
integral	O
for	O
pdfs.10	O
we	O
can	O
rewrite	O
this	O
as	O
kl	O
(	O
p||q	O
)	O
=	O
pk	O
log	O
qk	O
=	O
−h	O
(	O
p	O
)	O
+h	O
(	O
p	O
,	O
q	O
)	O
where	O
h	O
(	O
p	O
,	O
q	O
)	O
is	O
called	O
the	O
cross	B
entropy	I
,	O
h	O
(	O
p	O
,	O
q	O
)	O
(	O
cid:2	O
)	O
−	O
one	O
can	O
show	O
(	O
cover	O
and	O
thomas	O
2006	O
)	O
that	O
the	O
cross	B
entropy	I
is	O
the	O
average	O
number	O
of	O
bits	B
needed	O
to	O
encode	O
data	O
coming	O
from	O
a	O
source	O
with	O
distribution	O
p	O
when	O
we	O
use	O
model	O
q	O
to	O
10.	O
the	O
kl	O
divergence	O
is	O
not	O
a	O
distance	O
,	O
since	O
it	O
is	O
asymmetric	O
.	O
one	O
symmetric	B
version	O
of	O
the	O
kl	O
divergence	O
is	O
the	O
jensen-shannon	O
divergence	O
,	O
deﬁned	O
as	O
js	O
(	O
p1	O
,	O
p2	O
)	O
=	O
0.5kl	O
(	O
p1||q	O
)	O
+	O
0.5kl	O
(	O
p2||q	O
)	O
,	O
whereq	O
=	O
0.5p1	O
+	O
0.5p2	O
.	O
58	O
chapter	O
2.	O
probability	O
deﬁne	O
our	O
codebook	B
.	O
hence	O
the	O
“	O
regular	B
”	O
entropy	B
h	O
(	O
p	O
)	O
=	O
h	O
(	O
p	O
,	O
p	O
)	O
,	O
deﬁned	O
in	O
section	O
2.8.1	O
,	O
is	O
the	O
expected	O
number	O
of	O
bits	B
if	O
we	O
use	O
the	O
true	O
model	O
,	O
so	O
the	O
kl	O
divergence	O
is	O
the	O
difference	O
between	O
these	O
.	O
in	O
other	O
words	O
,	O
the	O
kl	O
divergence	O
is	O
the	O
average	O
number	O
of	O
extra	O
bits	B
needed	O
to	O
encode	O
the	O
data	O
,	O
due	O
to	O
the	O
fact	O
that	O
we	O
used	O
distribution	O
q	O
to	O
encode	O
the	O
data	O
instead	O
of	O
the	O
true	O
distribution	O
p.	O
the	O
“	O
extra	O
number	O
of	O
bits	B
”	O
interpretation	O
should	O
make	O
it	O
clear	O
that	O
kl	O
(	O
p||q	O
)	O
≥	O
0	O
,	O
and	O
that	O
the	O
kl	O
is	O
only	O
equal	O
to	O
zero	O
iff	O
q	O
=	O
p.	O
we	O
now	O
give	O
a	O
proof	O
of	O
this	O
important	O
result	O
.	O
theorem	O
2.8.1	O
.	O
(	O
information	B
inequality	O
)	O
kl	O
(	O
p||q	O
)	O
≥	O
0	O
with	O
equality	O
iff	B
p	O
=	O
q.	O
proof	O
.	O
to	O
prove	O
the	O
theorem	O
,	O
we	O
need	O
to	O
use	O
jensen	O
’	O
s	O
inequality	O
.	O
this	O
states	O
that	O
,	O
for	O
any	O
convex	B
function	O
f	O
,	O
we	O
have	O
that	O
f	O
λixi	O
λif	O
(	O
xi	O
)	O
(	O
2.113	O
)	O
i=1	O
λi	O
=	O
1.	O
this	O
is	O
clearly	O
true	O
for	O
n	O
=	O
2	O
(	O
by	O
deﬁnition	O
of	O
convexity	O
)	O
,	O
and	O
let	O
us	O
now	O
prove	O
the	O
main	O
theorem	O
,	O
following	O
(	O
cover	O
and	O
thomas	O
2006	O
,	O
p28	O
)	O
.	O
let	O
a	O
=	O
{	O
x	O
:	O
(	O
cid:13	O
)	O
n	O
(	O
cid:3	O
)	O
i=1	O
(	O
cid:14	O
)	O
≤	O
n	O
(	O
cid:3	O
)	O
(	O
cid:2	O
)	O
n	O
i=1	O
where	O
λi	O
≥	O
0	O
and	O
can	O
be	O
proved	O
by	O
induction	B
for	O
n	O
>	O
2	O
.	O
p	O
(	O
x	O
)	O
>	O
0	O
}	O
be	O
the	O
support	B
of	O
p	O
(	O
x	O
)	O
.	O
then	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
−kl	O
(	O
p||q	O
)	O
=−	O
p	O
(	O
x	O
)	O
log	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
x∈a	O
x∈a	O
≤	O
log	O
≤	O
log	O
(	O
cid:2	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
x∈a	O
=	O
p	O
(	O
x	O
)	O
log	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
=	O
log	O
q	O
(	O
x	O
)	O
x∈a	O
q	O
(	O
x	O
)	O
=	O
log	O
1	O
=	O
0	O
x∈x	O
q	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
(	O
2.114	O
)	O
(	O
2.115	O
)	O
(	O
2.116	O
)	O
(	O
cid:2	O
)	O
where	O
the	O
ﬁrst	O
inequality	O
follows	O
from	O
jensen	O
’	O
s	O
.	O
since	O
log	O
(	O
x	O
)	O
is	O
a	O
strictly	O
concave	O
function	O
,	O
we	O
have	O
equality	O
in	O
equation	O
2.115	O
iff	B
p	O
(	O
x	O
)	O
=cq	O
(	O
x	O
)	O
for	O
some	O
c.	O
we	O
have	O
equality	O
in	O
equation	O
2.116	O
x∈x	O
q	O
(	O
x	O
)	O
=	O
1	O
,	O
which	O
implies	O
c	O
=	O
1.	O
hence	O
kl	O
(	O
p||q	O
)	O
=	O
0	O
iff	B
p	O
(	O
x	O
)	O
=	O
q	O
(	O
x	O
)	O
iff	B
for	O
all	O
x.	O
x∈a	O
q	O
(	O
x	O
)	O
=	O
one	O
important	O
consequence	O
of	O
this	O
result	O
is	O
that	O
the	O
discrete	B
distribution	O
with	O
the	O
maximum	B
entropy	I
is	O
the	O
uniform	B
distribution	I
.	O
more	O
precisely	O
,	O
h	O
(	O
x	O
)	O
≤	O
log	O
|x|	O
,	O
where	O
|x|	O
is	O
the	O
number	O
of	O
states	O
for	O
x	O
,	O
with	O
equality	O
iff	B
p	O
(	O
x	O
)	O
is	O
uniform	O
.	O
to	O
see	O
this	O
,	O
let	O
u	O
(	O
x	O
)	O
=	O
1/|x|	O
.	O
then	O
0	O
≤	O
kl	O
(	O
p||u	O
)	O
=	O
(	O
cid:3	O
)	O
=	O
x	O
p	O
(	O
x	O
)	O
log	O
(	O
cid:3	O
)	O
p	O
(	O
x	O
)	O
u	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
log	O
u	O
(	O
x	O
)	O
=−	O
h	O
(	O
x	O
)	O
+	O
log	O
|x|	O
x	O
p	O
(	O
x	O
)	O
log	O
p	O
(	O
x	O
)	O
−	O
x	O
(	O
2.117	O
)	O
(	O
2.118	O
)	O
this	O
is	O
a	O
formulation	O
of	O
laplace	O
’	O
s	O
principle	B
of	I
insufficient	I
reason	I
,	O
which	O
argues	O
in	O
favor	O
of	O
using	O
uniform	O
distributions	O
when	O
there	O
are	O
no	O
other	O
reasons	O
to	O
favor	O
one	O
distribution	O
over	O
another	O
.	O
see	O
section	O
9.2.6	O
for	O
a	O
discussion	O
of	O
how	O
to	O
create	O
distributions	O
that	O
satisfy	O
certain	O
constraints	O
,	O
but	O
otherwise	O
are	O
as	O
least-commital	O
as	O
possible	O
.	O
(	O
for	O
example	O
,	O
the	O
gaussian	O
satisﬁes	O
ﬁrst	O
and	O
second	O
moment	O
constraints	O
,	O
but	O
otherwise	O
has	O
maximum	B
entropy	I
.	O
)	O
2.8.	O
information	B
theory	I
2.8.3	O
mutual	B
information	I
59	O
consider	O
two	O
random	O
variables	O
,	O
x	O
and	O
y	O
.	O
suppose	O
we	O
want	O
to	O
know	O
how	O
much	O
knowing	O
one	O
variable	O
tells	O
us	O
about	O
the	O
other	O
.	O
we	O
could	O
compute	O
the	O
correlation	B
coefficient	I
,	O
but	O
this	O
is	O
only	O
deﬁned	O
for	O
real-valued	O
random	O
variables	O
,	O
and	O
furthermore	O
,	O
this	O
is	O
a	O
very	O
limited	O
measure	O
of	O
dependence	O
,	O
as	O
we	O
saw	O
in	O
figure	O
2.12.	O
a	O
more	O
general	O
approach	O
is	O
to	O
determine	O
how	O
similar	B
the	O
joint	B
distribution	I
p	O
(	O
x	O
,	O
y	O
)	O
is	O
to	O
the	O
factored	O
distribution	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
.	O
this	O
is	O
called	O
the	O
mutual	B
information	I
or	O
mi	O
,	O
and	O
is	O
deﬁned	O
as	O
follows	O
:	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:2	O
)	O
kl	O
(	O
p	O
(	O
x	O
,	O
y	O
)	O
||p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
)	O
=	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
log	O
(	O
2.119	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
x	O
y	O
we	O
have	O
i	O
(	O
x	O
;	O
y	O
)	O
≥	O
0	O
with	O
equality	O
iff	B
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
.	O
that	O
is	O
,	O
the	O
mi	O
is	O
zero	O
iff	O
the	O
variables	O
are	O
independent	O
.	O
to	O
gain	O
insight	O
into	O
the	O
meaning	O
of	O
mi	O
,	O
it	O
helps	O
to	O
re-express	O
it	O
in	O
terms	O
of	O
joint	O
and	O
conditional	O
entropies	O
.	O
one	O
can	O
show	O
(	O
exercise	O
2.12	O
)	O
that	O
the	O
above	O
expression	O
is	O
equivalent	O
to	O
the	O
following	O
:	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
−	O
h	O
(	O
x|y	O
)	O
=	O
h	O
(	O
y	O
)	O
−	O
h	O
(	O
y	O
|x	O
)	O
(	O
cid:2	O
)	O
(	O
2.120	O
)	O
where	O
h	O
(	O
y	O
|x	O
)	O
is	O
the	O
conditional	B
entropy	I
,	O
deﬁned	O
as	O
h	O
(	O
y	O
|x	O
)	O
=	O
x	O
p	O
(	O
x	O
)	O
h	O
(	O
y	O
|x	O
=	O
x	O
)	O
.	O
thus	O
we	O
can	O
interpret	O
the	O
mi	O
between	O
x	O
and	O
y	O
as	O
the	O
reduction	O
in	O
uncertainty	B
about	O
x	O
after	O
observing	O
y	O
,	O
or	O
,	O
by	O
symmetry	O
,	O
the	O
reduction	O
in	O
uncertainty	B
about	O
y	O
after	O
observing	O
x.	O
we	O
will	O
encounter	O
several	O
applications	O
of	O
mi	O
later	O
in	O
the	O
book	O
.	O
see	O
also	O
exercises	O
2.13	O
and	O
2.14	O
for	O
the	O
connection	O
between	O
mi	O
and	O
correlation	O
coefficients	O
.	O
a	O
quantity	O
which	O
is	O
closely	O
related	O
to	O
mi	O
is	O
the	O
pointwise	B
mutual	I
information	I
or	O
pmi	O
.	O
for	O
two	O
events	O
(	O
not	O
random	O
variables	O
)	O
x	O
and	O
y	O
,	O
this	O
is	O
deﬁned	O
as	O
pmi	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:2	O
)	O
log	O
p	O
(	O
x	O
,	O
y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
=	O
log	O
p	O
(	O
x|y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y|x	O
)	O
p	O
(	O
y	O
)	O
=	O
log	O
(	O
2.121	O
)	O
this	O
measures	O
the	O
discrepancy	O
between	O
these	O
events	O
occuring	O
together	O
compared	O
to	O
what	O
would	O
be	O
expected	O
by	O
chance	O
.	O
clearly	O
the	O
mi	O
of	O
x	O
and	O
y	O
is	O
just	O
the	O
expected	B
value	I
of	O
the	O
pmi	O
.	O
interestingly	O
,	O
we	O
can	O
rewrite	O
the	O
pmi	O
as	O
follows	O
:	O
p	O
(	O
x|y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y|x	O
)	O
p	O
(	O
y	O
)	O
pmi	O
(	O
x	O
,	O
y	O
)	O
=	O
log	O
(	O
2.122	O
)	O
this	O
is	O
the	O
amount	O
we	O
learn	O
from	O
updating	O
the	O
prior	O
p	O
(	O
x	O
)	O
into	O
the	O
posterior	O
p	O
(	O
x|y	O
)	O
,	O
or	O
equiva-	O
lently	O
,	O
updating	O
the	O
prior	O
p	O
(	O
y	O
)	O
into	O
the	O
posterior	O
p	O
(	O
y|x	O
)	O
.	O
=	O
log	O
2.8.3.1	O
mutual	B
information	I
for	O
continuous	O
random	O
variables	O
*	O
the	O
above	O
formula	O
for	O
mi	O
is	O
deﬁned	O
for	O
discrete	O
random	O
variables	O
.	O
for	O
continuous	O
random	O
variables	O
,	O
it	O
is	O
common	O
to	O
ﬁrst	O
discretize	B
or	O
quantize	B
them	O
,	O
by	O
dividing	O
the	O
ranges	O
of	O
each	O
variable	O
into	O
bins	O
,	O
and	O
computing	O
how	O
many	O
values	O
fall	O
in	O
each	O
histogram	B
bin	O
(	O
scott	O
1979	O
)	O
.	O
we	O
can	O
then	O
easily	O
compute	O
the	O
mi	O
using	O
the	O
formula	O
above	O
(	O
see	O
mutualinfoallpairsmixed	O
for	O
some	O
code	O
,	O
and	O
mimixeddemo	O
for	O
a	O
demo	O
)	O
.	O
unfortunately	O
,	O
the	O
number	O
of	O
bins	O
used	O
,	O
and	O
the	O
location	O
of	O
the	O
bin	O
boundaries	O
,	O
can	O
have	O
a	O
signiﬁcant	O
effect	O
on	O
the	O
results	O
.	O
one	O
way	O
around	O
this	O
is	O
to	O
try	O
to	O
estimate	O
the	O
mi	O
directly	O
,	O
60	O
	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
(	O
cid:2	O
)	O
(	O
cid:1	O
)	O
(	O
cid:16	O
)	O
(	O
cid:10	O
)	O
(	O
cid:12	O
)	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
(	O
cid:10	O
)	O
(	O
cid:9	O
)	O
(	O
cid:9	O
)	O
	O
	O
	O
	O
(	O
cid:11	O
)	O
(	O
cid:1	O
)	O
(	O
cid:8	O
)	O
(	O
cid:13	O
)	O
(	O
cid:4	O
)	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
(	O
cid:10	O
)	O
(	O
cid:16	O
)	O
(	O
cid:6	O
)	O
(	O
cid:8	O
)	O
(	O
cid:14	O
)	O
(	O
cid:14	O
)	O
(	O
cid:13	O
)	O
(	O
cid:4	O
)	O
(	O
cid:1	O
)	O
(	O
cid:12	O
)	O
(	O
cid:13	O
)	O
(	O
cid:15	O
)	O
(	O
cid:14	O
)	O
(	O
cid:6	O
)	O
(	O
cid:8	O
)	O
(	O
cid:5	O
)	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
#	O
*57	O
,	O
chapter	O
2.	O
probability	O
	O
	O
8	O
7	O
=	O
	O
	O
	O
	O
8	O
,	O
/	O
7	O
:	O
4	O
0	O
	O
	O
5	O
9	O
	O
9	O
8	O
5	O
	O
,	O
-	O
/	O
	O
	O
	O
	O
	O
	O
=	O
9	O
/	O
8	O
,	O
)	O
2	O
	O
	O
,	O
(	O
3	O
,	O
	O
	O
	O
9	O
2	O
:	O
+	O
	O
	O
	O
	O
	O
	O
,49/89,48/9=6,7	O
	O
	O
	O
	O
	O
	O
4*53	O
,	O
	O
	O
!	O
,785449	O
	O
	O
	O
8	O
7	O
(	O
,	O
'	O
	O
	O
=	O
*	O
4	O
(	O
9	O
*	O
,	O
6	O
<	O
	O
	O
,	O
-	O
/	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
=	O
9	O
/	O
7	O
(	O
6	O
8	O
	O
/	O
	O
	O
3	O
3	O
2	O
	O
	O
8	O
,	O
8	O
(	O
,	O
	O
	O
	O
	O
#	O
	O
	O
%	O
	O
5	O
	O
	O
	O
	O
9	O
	O
,	O
:	O
+	O
	O
8	O
.	O
9	O
(	O
,	O
	O
	O
	O
<	O
	O
<	O
	O
<	O
	O
:3	O
)	O
,75-	O
!	O
.=8/*/	O
(	O
48	O
	O
	O
	O
	O
9	O
4	O
	O
	O
	O
4	O
5	O
8	O
7	O
,	O
!	O
	O
	O
	O
6	O
<	O
	O
	O
.	O
9	O
2	O
(	O
,	O
	O
	O
	O
	O
7588	O
(	O
9	O
>	O
24*	O
	O
!	O
,785449	O
	O
	O
	O
./2+7,4	O
!	O
,7	O
&	O
53	O
(	O
4	O
	O
	O
	O
,	O
(	O
29.	O
<	O
6	O
	O
!	O
,7854	O
$	O
#	O
	O
figure	O
2.22	O
left	O
:	O
correlation	B
coefficient	I
vs	O
maximal	O
information	O
criterion	O
(	O
mic	O
)	O
for	O
all	O
pairwise	O
relation-	O
ships	O
in	O
the	O
who	O
data	O
.	O
right	O
:	O
scatter	O
plots	O
of	O
certain	O
pairs	O
of	O
variables	O
.	O
the	O
red	O
lines	O
are	O
non-parametric	O
smoothing	O
regressions	O
(	O
section	O
15.4.6	O
)	O
ﬁt	O
separately	O
to	O
each	O
trend	O
.	O
source	O
:	O
figure	O
4	O
of	O
(	O
reshed	O
et	O
al	O
.	O
2011	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
david	O
reshef	O
and	O
the	O
american	O
association	O
for	O
the	O
advancement	O
of	O
science	O
.	O
without	O
ﬁrst	O
performing	O
density	B
estimation	I
(	O
learned-miller	O
2004	O
)	O
.	O
another	O
approach	O
is	O
to	O
try	O
many	O
different	O
bin	O
sizes	O
and	O
locations	O
,	O
and	O
to	O
compute	O
the	O
maximum	O
mi	O
achieved	O
.	O
this	O
statistic	O
,	O
appropriately	O
normalized	O
,	O
is	O
known	O
as	O
the	O
maximal	B
information	I
coefficient	I
(	O
mic	O
)	O
(	O
reshed	O
et	O
al	O
.	O
2011	O
)	O
.	O
more	O
precisely	O
,	O
deﬁne	O
maxg∈g	O
(	O
x	O
,	O
y	O
)	O
i	O
(	O
x	O
(	O
g	O
)	O
;	O
y	O
(	O
g	O
)	O
)	O
m	O
(	O
x	O
,	O
y	O
)	O
=	O
log	O
min	O
(	O
x	O
,	O
y	O
)	O
(	O
2.123	O
)	O
where	O
g	O
(	O
x	O
,	O
y	O
)	O
is	O
the	O
set	O
of	O
2d	O
grids	O
of	O
size	O
x×y	O
,	O
and	O
x	O
(	O
g	O
)	O
,	O
y	O
(	O
g	O
)	O
represents	O
a	O
discretization	O
of	O
the	O
variables	O
onto	O
this	O
grid	O
.	O
(	O
the	O
maximization	O
over	O
bin	O
locations	O
can	O
be	O
performed	O
efficiently	O
using	O
dynamic	B
programming	I
(	O
reshed	O
et	O
al	O
.	O
2011	O
)	O
.	O
)	O
now	O
deﬁne	O
the	O
mic	O
as	O
mic	O
(	O
cid:2	O
)	O
max	O
x	O
,	O
y	O
:	O
xy	O
<	O
b	O
m	O
(	O
x	O
,	O
y	O
)	O
(	O
2.124	O
)	O
where	O
b	O
is	O
some	O
sample-size	O
dependent	O
bound	O
on	O
the	O
number	O
of	O
bins	O
we	O
can	O
use	O
and	O
still	O
reliably	O
estimate	O
the	O
distribution	O
(	O
(	O
reshed	O
et	O
al	O
.	O
2011	O
)	O
suggest	O
b	O
=	O
n	O
0.6	O
)	O
.	O
it	O
can	O
be	O
shown	O
that	O
the	O
mic	O
lies	O
in	O
the	O
range	O
[	O
0	O
,	O
1	O
]	O
,	O
where	O
0	O
represents	O
no	O
relationship	O
between	O
the	O
variables	O
,	O
and	O
1	O
represents	O
a	O
noise-free	O
relationship	O
of	O
any	O
form	O
,	O
not	O
just	O
linear	O
.	O
figure	O
2.22	O
gives	O
an	O
example	O
of	O
this	O
statistic	O
in	O
action	B
.	O
the	O
data	O
consists	O
of	O
357	O
variables	O
measuring	O
a	O
variety	O
of	O
social	O
,	O
economic	O
,	O
health	O
and	O
political	O
indicators	O
,	O
collected	O
by	O
the	O
world	O
health	O
organization	O
(	O
who	O
)	O
.	O
on	O
the	O
left	O
of	O
the	O
ﬁgure	O
,	O
we	O
see	O
the	O
correlation	B
coefficient	I
(	O
cc	O
)	O
plotted	O
against	O
the	O
mic	O
for	O
all	O
63,566	O
variable	O
pairs	O
.	O
on	O
the	O
right	O
of	O
the	O
ﬁgure	O
,	O
we	O
see	O
scatter	O
plots	O
for	O
particular	O
pairs	O
of	O
variables	O
,	O
which	O
we	O
now	O
discuss	O
:	O
•	O
the	O
point	O
marked	O
c	O
has	O
a	O
low	O
cc	O
and	O
a	O
low	O
mic	O
.	O
the	O
corresponding	O
scatter	B
plot	I
makes	O
it	O
2.8.	O
information	B
theory	I
61	O
clear	O
that	O
there	O
is	O
no	O
relationship	O
between	O
these	O
two	O
variables	O
(	O
percentage	O
of	O
lives	O
lost	O
to	O
injury	O
and	O
density	O
of	O
dentists	O
in	O
the	O
population	O
)	O
.	O
•	O
the	O
points	O
marked	O
d	O
and	O
h	O
have	O
high	O
cc	O
(	O
in	O
absolute	O
value	O
)	O
and	O
high	O
mic	O
,	O
because	O
they	O
represent	O
nearly	O
linear	O
relationships	O
.	O
•	O
the	O
points	O
marked	O
e	O
,	O
f	O
,	O
and	O
g	O
have	O
low	O
cc	O
but	O
high	O
mic	O
.	O
this	O
is	O
because	O
they	O
correspond	O
to	O
non-linear	O
(	O
and	O
sometimes	O
,	O
as	O
in	O
the	O
case	O
of	O
e	O
and	O
f	O
,	O
non-functional	O
,	O
i.e.	O
,	O
one-to-many	O
)	O
relationships	O
between	O
the	O
variables	O
.	O
in	O
summary	O
,	O
we	O
see	O
that	O
statistics	O
(	O
such	O
as	O
mic	O
)	O
based	O
on	O
mutual	B
information	I
can	O
be	O
used	O
to	O
discover	O
interesting	O
relationships	O
between	O
variables	O
in	O
a	O
way	O
that	O
simpler	O
measures	O
,	O
such	O
as	O
correlation	O
coefficients	O
,	O
can	O
not	O
.	O
for	O
this	O
reason	O
,	O
the	O
mic	O
has	O
been	O
called	O
“	O
a	O
correlation	O
for	O
the	O
21st	O
century	O
”	O
(	O
speed	O
2011	O
)	O
.	O
exercises	O
exercise	O
2.1	O
probabilities	O
are	O
sensitive	O
to	O
the	O
form	O
of	O
the	O
question	O
that	O
was	O
used	O
to	O
generate	O
the	O
answer	O
(	O
source	O
:	O
minka	O
.	O
)	O
my	O
neighbor	O
has	O
two	O
children	B
.	O
assuming	O
that	O
the	O
gender	O
of	O
a	O
child	O
is	O
like	O
a	O
coin	O
ﬂip	O
,	O
it	O
is	O
most	O
likely	O
,	O
a	O
priori	O
,	O
that	O
my	O
neighbor	O
has	O
one	O
boy	O
and	O
one	O
girl	O
,	O
with	O
probability	O
1/2	O
.	O
the	O
other	O
possibilities—two	O
boys	O
or	O
two	O
girls—have	O
probabilities	O
1/4	O
and	O
1/4	O
.	O
a.	O
suppose	O
i	O
ask	O
him	O
whether	O
he	O
has	O
any	O
boys	O
,	O
and	O
he	O
says	O
yes	O
.	O
what	O
is	O
the	O
probability	O
that	O
one	O
child	O
is	O
a	O
girl	O
?	O
b.	O
suppose	O
instead	O
that	O
i	O
happen	O
to	O
see	O
one	O
of	O
his	O
children	B
run	O
by	O
,	O
and	O
it	O
is	O
a	O
boy	O
.	O
what	O
is	O
the	O
probability	O
that	O
the	O
other	O
child	O
is	O
a	O
girl	O
?	O
exercise	O
2.2	O
legal	O
reasoning	O
(	O
source	O
:	O
peter	O
lee	O
.	O
)	O
suppose	O
a	O
crime	O
has	O
been	O
committed	O
.	O
blood	O
is	O
found	O
at	O
the	O
scene	O
for	O
which	O
there	O
is	O
no	O
innocent	O
explanation	O
.	O
it	O
is	O
of	O
a	O
type	O
which	O
is	O
present	O
in	O
1	O
%	O
of	O
the	O
population	O
.	O
a.	O
the	O
prosecutor	O
claims	O
:	O
“	O
there	O
is	O
a	O
1	O
%	O
chance	O
that	O
the	O
defendant	O
would	O
have	O
the	O
crime	O
blood	O
type	O
if	O
he	O
were	O
innocent	O
.	O
thus	O
there	O
is	O
a	O
99	O
%	O
chance	O
that	O
he	O
guilty	O
”	O
.	O
this	O
is	O
known	O
as	O
the	O
prosecutor	O
’	O
s	O
fallacy	O
.	O
what	O
is	O
wrong	O
with	O
this	O
argument	O
?	O
b.	O
the	O
defender	O
claims	O
:	O
“	O
the	O
crime	O
occurred	O
in	O
a	O
city	O
of	O
800,000	O
people	O
.	O
the	O
blood	O
type	O
would	O
be	O
found	O
in	O
approximately	O
8000	O
people	O
.	O
the	O
evidence	B
has	O
provided	O
a	O
probability	O
of	O
just	O
1	O
in	O
8000	O
that	O
the	O
defendant	O
is	O
guilty	O
,	O
and	O
thus	O
has	O
no	O
relevance.	O
”	O
this	O
is	O
known	O
as	O
the	O
defender	O
’	O
s	O
fallacy	O
.	O
what	O
is	O
wrong	O
with	O
this	O
argument	O
?	O
exercise	O
2.3	O
variance	B
of	O
a	O
sum	O
show	O
that	O
the	O
variance	B
of	O
a	O
sum	O
is	O
var	O
[	O
x	O
+	O
y	O
]	O
=	O
var	O
[	O
x	O
]	O
+	O
var	O
[	O
y	O
]	O
+	O
2cov	O
[	O
x	O
,	O
y	O
]	O
,	O
where	O
cov	O
[	O
x	O
,	O
y	O
]	O
is	O
the	O
covariance	B
between	O
x	O
and	O
y	O
exercise	O
2.4	O
bayes	O
rule	O
for	O
medical	O
diagnosis	O
(	O
source	O
:	O
koller	O
.	O
)	O
after	O
your	O
yearly	O
checkup	O
,	O
the	O
doctor	O
has	O
bad	O
news	O
and	O
good	O
news	O
.	O
the	O
bad	O
news	O
is	O
that	O
you	O
tested	O
positive	O
for	O
a	O
serious	O
disease	O
,	O
and	O
that	O
the	O
test	O
is	O
99	O
%	O
accurate	O
(	O
i.e.	O
,	O
the	O
probability	O
of	O
testing	O
positive	O
given	O
that	O
you	O
have	O
the	O
disease	O
is	O
0.99	O
,	O
as	O
is	O
the	O
probability	O
of	O
tetsing	O
negative	O
given	O
that	O
you	O
don	O
’	O
t	O
have	O
the	O
disease	O
)	O
.	O
the	O
good	O
news	O
is	O
that	O
this	O
is	O
a	O
rare	O
disease	O
,	O
striking	O
only	O
one	O
in	O
10,000	O
people	O
.	O
what	O
are	O
the	O
chances	O
that	O
you	O
actually	O
have	O
the	O
disease	O
?	O
(	O
show	O
your	O
calculations	O
as	O
well	O
as	O
giving	O
the	O
ﬁnal	O
result	O
.	O
)	O
62	O
chapter	O
2.	O
probability	O
exercise	O
2.5	O
the	O
monty	O
hall	O
problem	O
(	O
source	O
:	O
mackay	O
.	O
)	O
on	O
a	O
game	O
show	O
,	O
a	O
contestant	O
is	O
told	O
the	O
rules	B
as	O
follows	O
:	O
there	O
are	O
three	O
doors	O
,	O
labelled	O
1	O
,	O
2	O
,	O
3.	O
a	O
single	O
prize	O
has	O
been	O
hidden	B
behind	O
one	O
of	O
them	O
.	O
you	O
get	O
to	O
select	O
one	O
door	O
.	O
initially	O
your	O
chosen	O
door	O
will	O
not	O
be	O
opened	O
.	O
instead	O
,	O
the	O
gameshow	O
host	O
will	O
open	O
one	O
of	O
the	O
other	O
two	O
doors	O
,	O
and	O
he	O
will	O
do	O
so	O
in	O
such	O
a	O
way	O
as	O
not	O
to	O
reveal	O
the	O
prize	O
.	O
for	O
example	O
,	O
if	O
you	O
ﬁrst	O
choose	O
door	O
1	O
,	O
he	O
will	O
then	O
open	O
one	O
of	O
doors	O
2	O
and	O
3	O
,	O
and	O
it	O
is	O
guaranteed	O
that	O
he	O
will	O
choose	O
which	O
one	O
to	O
open	O
so	O
that	O
the	O
prize	O
will	O
not	O
be	O
revealed	O
.	O
at	O
this	O
point	O
,	O
you	O
will	O
be	O
given	O
a	O
fresh	O
choice	O
of	O
door	O
:	O
you	O
can	O
either	O
stick	O
with	O
your	O
ﬁrst	O
choice	O
,	O
or	O
you	O
can	O
switch	O
to	O
the	O
other	O
closed	O
door	O
.	O
all	O
the	O
doors	O
will	O
then	O
be	O
opened	O
and	O
you	O
will	O
receive	O
whatever	O
is	O
behind	O
your	O
ﬁnal	O
choice	O
of	O
door	O
.	O
imagine	O
that	O
the	O
contestant	O
chooses	O
door	O
1	O
ﬁrst	O
;	O
then	O
the	O
gameshow	O
host	O
opens	O
door	O
3	O
,	O
revealing	O
nothing	O
behind	O
the	O
door	O
,	O
as	O
promised	O
.	O
should	O
the	O
contestant	O
(	O
a	O
)	O
stick	O
with	O
door	O
1	O
,	O
or	O
(	O
b	O
)	O
switch	O
to	O
door	O
2	O
,	O
or	O
(	O
c	O
)	O
does	O
it	O
make	O
no	O
difference	O
?	O
you	O
may	O
assume	O
that	O
initially	O
,	O
the	O
prize	O
is	O
equally	O
likely	O
to	O
be	O
behind	O
any	O
of	O
the	O
3	O
doors	O
.	O
hint	O
:	O
use	O
bayes	O
rule	O
.	O
exercise	O
2.6	O
conditional	B
independence	I
(	O
source	O
:	O
koller	O
.	O
)	O
a.	O
let	O
h	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
be	O
a	O
discrete	B
random	I
variable	I
,	O
and	O
let	O
e1	O
and	O
e2	O
be	O
the	O
observed	O
values	O
of	O
two	O
other	O
random	O
variables	O
e1	O
and	O
e2	O
.	O
suppose	O
we	O
wish	O
to	O
calculate	O
the	O
vector	O
(	O
cid:8	O
)	O
p	O
(	O
h|e1	O
,	O
e2	O
)	O
=	O
(	O
p	O
(	O
h	O
=	O
1|e1	O
,	O
e2	O
)	O
,	O
.	O
.	O
.	O
,	O
p	O
(	O
h	O
=	O
k|e1	O
,	O
e2	O
)	O
)	O
which	O
of	O
the	O
following	O
sets	O
of	O
numbers	O
are	O
sufficient	O
for	O
the	O
calculation	O
?	O
i.	O
p	O
(	O
e1	O
,	O
e2	O
)	O
,	O
p	O
(	O
h	O
)	O
,	O
p	O
(	O
e1|h	O
)	O
,	O
p	O
(	O
e2|h	O
)	O
ii	O
.	O
p	O
(	O
e1	O
,	O
e2	O
)	O
,	O
p	O
(	O
h	O
)	O
,	O
p	O
(	O
e1	O
,	O
e2|h	O
)	O
iii	O
.	O
p	O
(	O
e1|h	O
)	O
,	O
p	O
(	O
e2|h	O
)	O
,	O
p	O
(	O
h	O
)	O
b.	O
now	O
suppose	O
we	O
now	O
assume	O
e1	O
⊥	O
e2|h	O
(	O
i.e.	O
,	O
e1	O
and	O
e2	O
are	O
conditionally	B
independent	I
given	O
h	O
)	O
.	O
which	O
of	O
the	O
above	O
3	O
sets	O
are	O
sufficent	O
now	O
?	O
show	O
your	O
calculations	O
as	O
well	O
as	O
giving	O
the	O
ﬁnal	O
result	O
.	O
hint	O
:	O
use	O
bayes	O
rule	O
.	O
exercise	O
2.7	O
pairwise	O
independence	O
does	O
not	O
imply	O
mutual	O
independence	O
we	O
say	O
that	O
two	O
random	O
variables	O
are	O
pairwise	B
independent	I
if	O
p	O
(	O
x2|x1	O
)	O
=	O
p	O
(	O
x2	O
)	O
and	O
hence	O
p	O
(	O
x2	O
,	O
x1	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2|x1	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
we	O
say	O
that	O
n	O
random	O
variables	O
are	O
mutually	B
independent	I
if	O
p	O
(	O
xi|xs	O
)	O
=	O
p	O
(	O
xi	O
)	O
∀s	O
⊆	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
\	O
{	O
i	O
}	O
and	O
hence	O
n	O
(	O
cid:3	O
)	O
p	O
(	O
x1	O
:	O
n	O
)	O
=	O
p	O
(	O
xi	O
)	O
i=1	O
(	O
2.125	O
)	O
(	O
2.126	O
)	O
(	O
2.127	O
)	O
(	O
2.128	O
)	O
show	O
that	O
pairwise	O
independence	O
between	O
all	B
pairs	I
of	O
variables	O
does	O
not	O
necessarily	O
imply	O
mutual	O
inde-	O
pendence	O
.	O
it	O
suffices	O
to	O
give	O
a	O
counter	O
example	O
.	O
2.8.	O
information	B
theory	I
63	O
exercise	O
2.8	O
conditional	B
independence	I
iff	O
joint	O
factorizes	O
in	O
the	O
text	O
we	O
said	O
x	O
⊥	O
y	O
|z	O
iff	B
p	O
(	O
x	O
,	O
y|z	O
)	O
=	O
p	O
(	O
x|z	O
)	O
p	O
(	O
y|z	O
)	O
(	O
2.129	O
)	O
for	O
all	O
x	O
,	O
y	O
,	O
z	O
such	O
that	O
p	O
(	O
z	O
)	O
>	O
0.	O
now	O
prove	O
the	O
following	O
alternative	O
deﬁnition	O
:	O
x	O
⊥	O
y	O
|z	O
iff	B
there	O
exist	O
function	O
g	O
and	O
h	O
such	O
that	O
p	O
(	O
x	O
,	O
y|z	O
)	O
=	O
g	O
(	O
x	O
,	O
z	O
)	O
h	O
(	O
y	O
,	O
z	O
)	O
for	O
all	O
x	O
,	O
y	O
,	O
z	O
such	O
that	O
p	O
(	O
z	O
)	O
>	O
0	O
.	O
(	O
2.130	O
)	O
exercise	O
2.9	O
conditional	B
independence	I
(	O
source	O
:	O
koller	O
.	O
)	O
are	O
the	O
following	O
properties	O
true	O
?	O
prove	O
or	O
disprove	O
.	O
note	O
that	O
we	O
are	O
not	O
restricting	O
attention	O
to	O
distributions	O
that	O
can	O
be	O
represented	O
by	O
a	O
graphical	B
model	I
.	O
a.	O
true	O
or	O
false	O
?	O
(	O
x	O
⊥	O
w|z	O
,	O
y	O
)	O
∧	O
(	O
x	O
⊥	O
y	O
|z	O
)	O
⇒	O
(	O
x	O
⊥	O
y	O
,	O
w|z	O
)	O
b.	O
true	O
or	O
false	O
?	O
(	O
x	O
⊥	O
y	O
|z	O
)	O
∧	O
(	O
x	O
⊥	O
y	O
|w	O
)	O
⇒	O
(	O
x	O
⊥	O
y	O
|z	O
,	O
w	O
)	O
exercise	O
2.10	O
deriving	O
the	O
inverse	B
gamma	I
density	O
let	O
x	O
∼	O
ga	O
(	O
a	O
,	O
b	O
)	O
,	O
i.e	O
.	O
ga	O
(	O
x|a	O
,	O
b	O
)	O
=	O
ba	O
γ	O
(	O
a	O
)	O
xa−1e	O
−xb	O
let	O
y	O
=	O
1/x	O
.	O
show	O
that	O
y	O
∼	O
ig	O
(	O
a	O
,	O
b	O
)	O
,	O
i.e.	O
,	O
ig	O
(	O
x|shape	O
=	O
a	O
,	O
scale	O
=	O
b	O
)	O
=	O
ba	O
γ	O
(	O
a	O
)	O
−	O
(	O
a+1	O
)	O
e	O
x	O
−b/x	O
hint	O
:	O
use	O
the	O
change	B
of	I
variables	I
formula	O
.	O
exercise	O
2.11	O
normalization	O
constant	O
for	O
a	O
1d	O
gaussian	O
the	O
normalization	O
constant	O
for	O
a	O
zero-mean	O
gaussian	O
is	O
given	O
by	O
(	O
cid:4	O
)	O
b	O
(	O
cid:5	O
)	O
(	O
cid:6	O
)	O
z	O
=	O
exp	O
a	O
−	O
x2	O
2σ2	O
dx	O
where	O
a	O
=	O
−∞	O
and	O
b	O
=	O
∞	O
.	O
to	O
compute	O
this	O
,	O
consider	O
its	O
square	O
(	O
cid:4	O
)	O
b	O
(	O
cid:4	O
)	O
b	O
(	O
cid:5	O
)	O
z	O
2	O
=	O
exp	O
a	O
a	O
−	O
x2	O
+	O
y2	O
2σ2	O
dxdy	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
2.131	O
)	O
(	O
2.132	O
)	O
(	O
2.133	O
)	O
(	O
2.134	O
)	O
(	O
2.135	O
)	O
let	O
us	O
change	O
variables	O
from	O
cartesian	O
(	O
x	O
,	O
y	O
)	O
to	O
polar	B
(	O
r	O
,	O
θ	O
)	O
using	O
x	O
=	O
r	O
cos	O
θ	O
and	O
y	O
=	O
r	O
sin	O
θ.	O
since	O
dxdy	O
=	O
rdrdθ	O
,	O
and	O
cos2θ	O
+	O
sin2	O
θ	O
=	O
1	O
,	O
we	O
have	O
(	O
cid:4	O
)	O
2π	O
(	O
cid:4	O
)	O
∞	O
(	O
cid:5	O
)	O
z	O
2	O
=	O
r	O
exp	O
0	O
0	O
−	O
r2	O
2σ2	O
drdθ	O
(	O
cid:7	O
)	O
evaluate	O
this	O
integral	O
and	O
hence	O
show	O
z	O
=	O
σ	O
two	O
terms	O
,	O
the	O
ﬁrst	O
of	O
which	O
(	O
involving	O
dθ	O
)	O
du/dr	O
=	O
−	O
1	O
−r2/2σ2	O
σ2	O
re	O
(	O
2π	O
)	O
.	O
hint	O
1	O
:	O
separate	O
the	O
integral	O
into	O
a	O
product	O
of	O
then	O
is	O
constant	O
,	O
so	O
is	O
easy	O
.	O
hint	O
2	O
:	O
if	O
u	O
=	O
e	O
−r2/2σ2	O
(	O
cid:8	O
)	O
(	O
cid:3	O
)	O
,	O
so	O
the	O
second	O
integral	O
is	O
also	O
easy	O
(	O
since	O
u	O
(	O
r	O
)	O
dr	O
=	O
u	O
(	O
r	O
)	O
)	O
.	O
64	O
chapter	O
2.	O
probability	O
exercise	O
2.12	O
expressing	O
mutual	B
information	I
in	O
terms	O
of	O
entropies	O
show	O
that	O
i	O
(	O
x	O
,	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
−	O
h	O
(	O
x|y	O
)	O
=	O
h	O
(	O
y	O
)	O
−	O
h	O
(	O
y	O
|x	O
)	O
(	O
2.136	O
)	O
exercise	O
2.13	O
mutual	B
information	I
for	O
correlated	O
normals	O
(	O
source	O
:	O
(	O
cover	O
and	O
thomas	O
1991	O
,	O
q9.3	O
)	O
.	O
)	O
find	O
the	O
mutual	B
information	I
i	O
(	O
x1	O
,	O
x2	O
)	O
where	O
x	O
has	O
a	O
bivariate	O
normal	B
distribution	O
:	O
(	O
cid:5	O
)	O
(	O
cid:6	O
)	O
x1	O
x2	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
0	O
,	O
∼	O
n	O
σ2	O
ρσ2	O
ρσ2	O
σ2	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:10	O
)	O
(	O
2.137	O
)	O
evaluate	O
i	O
(	O
x1	O
,	O
x2	O
)	O
for	O
ρ	O
=	O
1	O
,	O
ρ	O
=	O
0	O
and	O
ρ	O
=	O
−1	O
and	O
comment	O
.	O
hint	O
:	O
the	O
(	O
differential	O
)	O
entropy	B
of	O
a	O
d-dimensional	O
gaussian	O
is	O
(	O
2.138	O
)	O
h	O
(	O
x	O
)	O
=	O
1	O
2	O
log2	O
(	O
2πe	O
)	O
d	O
det	O
σ	O
in	O
the	O
1d	O
case	O
,	O
this	O
becomes	O
(	O
cid:9	O
)	O
(	O
cid:11	O
)	O
2πeσ2	O
(	O
cid:12	O
)	O
1	O
2	O
h	O
(	O
x	O
)	O
=	O
log2	O
hint	O
:	O
log	O
(	O
0	O
)	O
=	O
∞	O
.	O
exercise	O
2.14	O
a	O
measure	O
of	O
correlation	O
(	O
normalized	B
mutual	I
information	I
)	O
(	O
source	O
:	O
(	O
cover	O
and	O
thomas	O
1991	O
,	O
q2.20	O
)	O
.	O
)	O
let	O
x	O
and	O
y	O
be	O
discrete	O
random	O
variables	O
which	O
are	O
identically	O
distributed	O
(	O
so	O
h	O
(	O
x	O
)	O
=	O
h	O
(	O
y	O
)	O
)	O
but	O
not	O
necessarily	O
independent	O
.	O
deﬁne	O
(	O
2.139	O
)	O
r	O
=	O
1	O
−	O
h	O
(	O
y	O
|x	O
)	O
h	O
(	O
x	O
)	O
h	O
(	O
x	O
)	O
a.	O
show	O
r	O
=	O
i	O
(	O
x	O
,	O
y	O
)	O
b.	O
show	O
0	O
≤	O
r	O
≤	O
1	O
c.	O
when	O
is	O
r	O
=	O
0	O
?	O
d.	O
when	O
is	O
r	O
=	O
1	O
?	O
(	O
2.140	O
)	O
exercise	O
2.15	O
mle	O
minimizes	O
kl	O
divergence	O
to	O
the	O
empirical	B
distribution	I
let	O
pemp	O
(	O
x	O
)	O
be	O
the	O
empirical	B
distribution	I
,	O
and	O
let	O
q	O
(	O
x|θ	O
)	O
be	O
some	O
model	O
.	O
show	O
that	O
argminq	O
kl	O
(	O
pemp||q	O
)	O
is	O
obtained	O
by	O
q	O
(	O
x	O
)	O
=q	O
(	O
x	O
;	O
ˆθ	O
)	O
,	O
where	O
ˆθ	O
is	O
the	O
mle	O
.	O
hint	O
:	O
use	O
non-negativity	O
of	O
the	O
kl	O
divergence	O
.	O
exercise	O
2.16	O
mean	B
,	O
mode	B
,	O
variance	B
for	O
the	O
beta	B
distribution	I
suppose	O
θ	O
∼	O
beta	O
(	O
a	O
,	O
b	O
)	O
.	O
derive	O
the	O
mean	B
,	O
mode	B
and	O
variance	B
.	O
exercise	O
2.17	O
expected	B
value	I
of	O
the	O
minimum	O
suppose	O
x	O
,	O
y	O
are	O
two	O
points	O
sampled	O
independently	O
and	O
uniformly	O
at	O
random	O
from	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
.	O
what	O
is	O
the	O
expected	O
location	O
of	O
the	O
left	O
most	O
point	O
?	O
3	O
generative	O
models	O
for	O
discrete	B
data	O
3.1	O
introduction	O
in	O
section	O
2.2.3.2	O
,	O
we	O
discussed	O
how	O
to	O
classify	O
a	O
feature	O
vector	O
x	O
by	O
applying	O
bayes	O
rule	O
to	O
a	O
generative	B
classiﬁer	I
of	O
the	O
form	O
p	O
(	O
y	O
=	O
c|x	O
,	O
θ	O
)	O
∝	O
p	O
(	O
x|y	O
=	O
c	O
,	O
θ	O
)	O
p	O
(	O
y	O
=	O
c|θ	O
)	O
(	O
3.1	O
)	O
the	O
key	O
to	O
using	O
such	O
models	O
is	O
specifying	O
a	O
suitable	O
form	O
for	O
the	O
class-conditional	B
density	I
p	O
(	O
x|y	O
=	O
c	O
,	O
θ	O
)	O
,	O
which	O
deﬁnes	O
what	O
kind	O
of	O
data	O
we	O
expect	O
to	O
see	O
in	O
each	O
class	O
.	O
in	O
this	O
chapter	O
,	O
we	O
focus	O
on	O
the	O
case	O
where	O
the	O
observed	O
data	O
are	O
discrete	B
symbols	O
.	O
we	O
also	O
discuss	O
how	O
to	O
infer	O
the	O
unknown	B
parameters	O
θ	O
of	O
such	O
models	O
.	O
3.2	O
bayesian	O
concept	B
learning	I
consider	O
how	O
a	O
child	O
learns	O
to	O
understand	O
the	O
meaning	O
of	O
a	O
word	O
,	O
such	O
as	O
“	O
dog	O
”	O
.	O
presumably	O
the	O
child	O
’	O
s	O
parents	B
point	O
out	O
positive	B
examples	I
of	O
this	O
concept	B
,	O
saying	O
such	O
things	O
as	O
,	O
“	O
look	O
at	O
the	O
cute	O
dog	O
!	O
”	O
,	O
or	O
“	O
mind	O
the	O
doggy	O
”	O
,	O
etc	O
.	O
however	O
,	O
it	O
is	O
very	O
unlikely	O
that	O
they	O
provide	O
negative	B
examples	I
,	O
by	O
saying	O
“	O
look	O
at	O
that	O
non-dog	O
”	O
.	O
certainly	O
,	O
negative	B
examples	I
may	O
be	O
obtained	O
during	O
an	O
active	B
learning	I
process	O
—	O
the	O
child	O
says	O
“	O
look	O
at	O
the	O
dog	O
”	O
and	O
the	O
parent	O
says	O
“	O
that	O
’	O
s	O
a	O
cat	O
,	O
dear	O
,	O
not	O
a	O
dog	O
”	O
—	O
but	O
psychological	O
research	O
has	O
shown	O
that	O
people	O
can	O
learn	O
concepts	O
from	O
positive	B
examples	I
alone	O
(	O
xu	O
and	O
tenenbaum	O
2007	O
)	O
.	O
we	O
can	O
think	O
of	O
learning	B
the	O
meaning	O
of	O
a	O
word	O
as	O
equivalent	O
to	O
concept	B
learning	I
,	O
which	O
in	O
turn	O
is	O
equivalent	O
to	O
binary	B
classiﬁcation	I
.	O
to	O
see	O
this	O
,	O
deﬁne	O
f	O
(	O
x	O
)	O
=	O
1	O
if	O
x	O
is	O
an	O
example	O
of	O
the	O
concept	B
c	O
,	O
and	O
f	O
(	O
x	O
)	O
=	O
0	O
otherwise	O
.	O
then	O
the	O
goal	O
is	O
to	O
learn	O
the	O
indicator	B
function	I
f	O
,	O
which	O
just	O
deﬁnes	O
which	O
elements	O
are	O
in	O
the	O
set	O
c.	O
by	O
allowing	O
for	O
uncertainty	B
about	O
the	O
deﬁnition	O
of	O
f	O
,	O
or	O
equivalently	O
the	O
elements	O
of	O
c	O
,	O
we	O
can	O
emulate	O
fuzzy	B
set	I
theory	I
,	O
but	O
using	O
standard	O
probability	O
calculus	O
.	O
note	O
that	O
standard	O
binary	O
classiﬁcation	B
techniques	O
require	O
positive	O
and	O
negative	B
examples	I
.	O
by	O
contrast	O
,	O
we	O
will	O
devise	O
a	O
way	O
to	O
learn	O
from	O
positive	B
examples	I
alone	O
.	O
for	O
pedagogical	O
purposes	O
,	O
we	O
will	O
consider	O
a	O
very	O
simple	O
example	O
of	O
concept	B
learning	I
called	O
the	O
number	B
game	I
,	O
based	O
on	O
part	O
of	O
josh	O
tenenbaum	O
’	O
s	O
phd	O
thesis	O
(	O
tenenbaum	O
1999	O
)	O
.	O
the	O
game	O
proceeds	O
as	O
follows	O
.	O
i	O
choose	O
some	O
simple	O
arithmetical	O
concept	B
c	O
,	O
such	O
as	O
“	O
prime	O
number	O
”	O
or	O
“	O
a	O
number	O
between	O
1	O
and	O
10	O
”	O
.	O
i	O
then	O
give	O
you	O
a	O
series	O
of	O
randomly	O
chosen	O
positive	B
examples	I
d	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
drawn	O
from	O
c	O
,	O
and	O
ask	O
you	O
whether	O
some	O
new	O
test	O
case	O
˜x	O
belongs	O
to	O
c	O
,	O
i.e.	O
,	O
i	O
ask	O
you	O
to	O
classify	O
˜x	O
.	O
66	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
examples	O
16	O
60	O
16	O
8	O
2	O
64	O
1	O
0.5	O
0	O
1	O
0.5	O
0	O
1	O
0.5	O
0	O
1	O
16	O
23	O
19	O
20	O
0.5	O
0	O
4	O
8	O
12	O
16	O
20	O
24	O
28	O
32	O
36	O
40	O
44	O
48	O
52	O
56	O
60	O
64	O
68	O
72	O
76	O
80	O
84	O
88	O
92	O
96	O
100	O
4	O
8	O
12	O
16	O
20	O
24	O
28	O
32	O
36	O
40	O
44	O
48	O
52	O
56	O
60	O
64	O
68	O
72	O
76	O
80	O
84	O
88	O
92	O
96	O
100	O
4	O
8	O
12	O
16	O
20	O
24	O
28	O
32	O
36	O
40	O
44	O
48	O
52	O
56	O
60	O
64	O
68	O
72	O
76	O
80	O
84	O
88	O
92	O
96	O
100	O
4	O
8	O
12	O
16	O
20	O
24	O
28	O
32	O
36	O
40	O
44	O
48	O
52	O
56	O
60	O
64	O
68	O
72	O
76	O
80	O
84	O
88	O
92	O
96	O
100	O
figure	O
3.1	O
empirical	O
predictive	O
distribution	O
averaged	O
over	O
8	O
humans	O
in	O
the	O
number	B
game	I
.	O
first	O
two	O
rows	O
:	O
after	O
seeing	O
d	O
=	O
{	O
16	O
}	O
and	O
d	O
=	O
{	O
60	O
}	O
.	O
this	O
illustrates	O
diffuse	O
similarity	O
.	O
third	O
row	O
:	O
after	O
seeing	O
d	O
=	O
{	O
16	O
,	O
8	O
,	O
2	O
,	O
64	O
}	O
.	O
this	O
illustrates	O
rule-like	O
behavior	O
(	O
powers	O
of	O
2	O
)	O
.	O
bottom	O
row	O
:	O
after	O
seeing	O
d	O
=	O
{	O
16	O
,	O
23	O
,	O
19	O
,	O
20	O
}	O
.	O
this	O
illustrates	O
focussed	O
similarity	O
(	O
numbers	O
near	O
20	O
)	O
.	O
source	O
:	O
figure	O
5.5	O
of	O
(	O
tenenbaum	O
1999	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
josh	O
tenenbaum	O
.	O
suppose	O
,	O
for	O
simplicity	O
,	O
that	O
all	O
numbers	O
are	O
integers	O
between	O
1	O
and	O
100.	O
now	O
suppose	O
i	O
tell	O
you	O
“	O
16	O
”	O
is	O
a	O
positive	O
example	O
of	O
the	O
concept	B
.	O
what	O
other	O
numbers	O
do	O
you	O
think	O
are	O
positive	O
?	O
17	O
?	O
6	O
?	O
32	O
?	O
99	O
?	O
it	O
’	O
s	O
hard	O
to	O
tell	O
with	O
only	O
one	O
example	O
,	O
so	O
your	O
predictions	O
will	O
be	O
quite	O
vague	O
.	O
presumably	O
numbers	O
that	O
are	O
similar	B
in	O
some	O
sense	O
to	O
16	O
are	O
more	O
likely	O
.	O
but	O
similar	B
in	O
what	O
way	O
?	O
17	O
is	O
similar	B
,	O
because	O
it	O
is	O
“	O
close	O
by	O
”	O
,	O
6	O
is	O
similar	B
because	O
it	O
has	O
a	O
digit	O
in	O
common	O
,	O
32	O
is	O
similar	B
because	O
it	O
is	O
also	O
even	O
and	O
a	O
power	O
of	O
2	O
,	O
but	O
99	O
does	O
not	O
seem	O
similar	B
.	O
thus	O
some	O
numbers	O
are	O
more	O
likely	O
than	O
others	O
.	O
we	O
can	O
represent	O
this	O
as	O
a	O
probability	O
distribution	O
,	O
p	O
(	O
˜x|d	O
)	O
,	O
which	O
is	O
the	O
probability	O
that	O
˜x	O
∈	O
c	O
given	O
the	O
data	O
d	O
for	O
any	O
˜x	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
100	O
}	O
.	O
this	O
is	O
called	O
the	O
posterior	B
predictive	I
distribution	I
.	O
figure	O
3.1	O
(	O
top	O
)	O
shows	O
the	O
predictive	B
distribution	O
of	O
people	O
derived	O
from	O
a	O
lab	O
experiment	O
.	O
we	O
see	O
that	O
people	O
predict	O
numbers	O
that	O
are	O
similar	B
to	O
16	O
,	O
under	O
a	O
variety	O
of	O
kinds	O
of	O
similarity	O
.	O
now	O
suppose	O
i	O
tell	O
you	O
that	O
8	O
,	O
2	O
and	O
64	O
are	O
also	O
positive	B
examples	I
.	O
now	O
you	O
may	O
guess	O
that	O
the	O
hidden	B
concept	O
is	O
“	O
powers	O
of	O
two	O
”	O
.	O
this	O
is	O
an	O
example	O
of	O
induction	B
.	O
given	O
this	O
hypothesis	O
,	O
the	O
predictive	B
distribution	O
is	O
quite	O
speciﬁc	O
,	O
and	O
puts	O
most	O
of	O
its	O
mass	O
on	O
powers	O
of	O
2	O
,	O
as	O
shown	O
if	O
instead	O
i	O
tell	O
you	O
the	O
data	O
is	O
d	O
=	O
{	O
16	O
,	O
23	O
,	O
19	O
,	O
20	O
}	O
,	O
you	O
will	O
get	O
a	O
in	O
figure	O
3.1	O
(	O
third	O
row	O
)	O
.	O
different	O
kind	O
of	O
generalization	B
gradient	I
,	O
as	O
shown	O
in	O
figure	O
3.1	O
(	O
bottom	O
)	O
.	O
how	O
can	O
we	O
explain	O
this	O
behavior	O
and	O
emulate	O
it	O
in	O
a	O
machine	O
?	O
the	O
classic	O
approach	O
to	O
induction	B
is	O
to	O
suppose	O
we	O
have	O
a	O
hypothesis	B
space	I
of	O
concepts	O
,	O
h	O
,	O
such	O
as	O
:	O
odd	O
numbers	O
,	O
even	O
numbers	O
,	O
all	O
numbers	O
between	O
1	O
and	O
100	O
,	O
powers	O
of	O
two	O
,	O
all	O
numbers	O
ending	O
in	O
j	O
(	O
for	O
3.2.	O
bayesian	O
concept	B
learning	I
67	O
0	O
≤	O
j	O
≤	O
9	O
)	O
,	O
etc	O
.	O
the	O
subset	O
of	O
h	O
that	O
is	O
consistent	B
with	O
the	O
data	O
d	O
is	O
called	O
the	O
version	B
space	I
.	O
as	O
we	O
see	O
more	O
examples	O
,	O
the	O
version	B
space	I
shrinks	O
and	O
we	O
become	O
increasingly	O
certain	O
about	O
the	O
concept	B
(	O
mitchell	O
1997	O
)	O
.	O
however	O
,	O
the	O
version	B
space	I
is	O
not	O
the	O
whole	O
story	O
.	O
after	O
seeing	O
d	O
=	O
{	O
16	O
}	O
,	O
there	O
are	O
many	O
consistent	B
rules	O
;	O
how	O
do	O
you	O
combine	O
them	O
to	O
predict	O
if	O
˜x	O
∈	O
c	O
?	O
also	O
,	O
after	O
seeing	O
d	O
=	O
{	O
16	O
,	O
8	O
,	O
2	O
,	O
64	O
}	O
,	O
why	O
did	O
you	O
choose	O
the	O
rule	O
“	O
powers	O
of	O
two	O
”	O
and	O
not	O
,	O
say	O
,	O
“	O
all	O
even	O
numbers	O
”	O
,	O
or	O
“	O
powers	O
of	O
two	O
except	O
for	O
32	O
”	O
,	O
both	O
of	O
which	O
are	O
equally	O
consistent	B
with	O
the	O
evidence	B
?	O
we	O
will	O
now	O
provide	O
a	O
bayesian	O
explanation	O
for	O
this	O
.	O
3.2.1	O
likelihood	B
we	O
must	O
explain	O
why	O
we	O
chose	O
htwo	O
(	O
cid:2	O
)	O
“	O
powers	O
of	O
two	O
”	O
,	O
and	O
not	O
,	O
say	O
,	O
heven	O
(	O
cid:2	O
)	O
“	O
even	O
numbers	O
”	O
after	O
seeing	O
d	O
=	O
{	O
16	O
,	O
8	O
,	O
2	O
,	O
64	O
}	O
,	O
given	O
that	O
both	O
hypotheses	O
are	O
consistent	B
with	O
the	O
evidence	B
.	O
the	O
key	O
intuition	O
is	O
that	O
we	O
want	O
to	O
avoid	O
suspicious	B
coincidences	I
.	O
if	O
the	O
true	O
concept	O
was	O
even	O
numbers	O
,	O
how	O
come	O
we	O
only	O
saw	O
numbers	O
that	O
happened	O
to	O
be	O
powers	O
of	O
two	O
?	O
to	O
formalize	O
this	O
,	O
let	O
us	O
assume	O
that	O
examples	O
are	O
sampled	O
uniformly	O
at	O
random	O
from	O
the	O
extension	B
of	O
a	O
concept	B
.	O
(	O
the	O
extension	B
of	O
a	O
concept	B
is	O
just	O
the	O
set	O
of	O
numbers	O
that	O
belong	O
to	O
it	O
,	O
e.g.	O
,	O
the	O
extension	B
of	O
heven	O
is	O
{	O
2	O
,	O
4	O
,	O
6	O
,	O
.	O
.	O
.	O
,	O
98	O
,	O
100	O
}	O
;	O
the	O
extension	B
of	O
“	O
numbers	O
ending	O
in	O
9	O
”	O
is	O
{	O
9	O
,	O
19	O
,	O
.	O
.	O
.	O
,	O
99	O
}	O
.	O
)	O
tenenbaum	O
calls	O
this	O
the	O
strong	B
sampling	I
assumption	I
.	O
given	O
this	O
assumption	O
,	O
the	O
probability	O
of	O
independently	O
sampling	O
n	O
items	O
(	O
with	O
replacement	O
)	O
from	O
h	O
is	O
given	O
by	O
(	O
cid:2	O
)	O
(	O
cid:3	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:3	O
)	O
n	O
p	O
(	O
d|h	O
)	O
=	O
1	O
size	O
(	O
h	O
)	O
=	O
1|h|	O
(	O
3.2	O
)	O
this	O
crucial	O
equation	O
embodies	O
what	O
tenenbaum	O
calls	O
the	O
size	B
principle	I
,	O
which	O
means	O
the	O
model	O
favors	O
the	O
simplest	O
(	O
smallest	O
)	O
hypothesis	O
consistent	O
with	O
the	O
data	O
.	O
this	O
is	O
more	O
commonly	O
known	O
as	O
occam	O
’	O
s	O
razor.1	O
to	O
see	O
how	O
it	O
works	O
,	O
let	O
d	O
=	O
{	O
16	O
}	O
.	O
then	O
p	O
(	O
d|htwo	O
)	O
=	O
1/6	O
,	O
since	O
there	O
are	O
only	O
6	O
powers	O
of	O
two	O
less	O
than	O
100	O
,	O
but	O
p	O
(	O
d|heven	O
)	O
=	O
1/50	O
,	O
since	O
there	O
are	O
50	O
even	O
numbers	O
.	O
so	O
the	O
likelihood	B
that	O
h	O
=	O
htwo	O
is	O
higher	O
than	O
if	O
h	O
=	O
heven	O
.	O
after	O
4	O
examples	O
,	O
the	O
likelihood	B
of	O
htwo	O
−4	O
,	O
whereas	O
the	O
likelihood	B
of	O
heven	O
is	O
(	O
1/50	O
)	O
4	O
=	O
1.6	O
×	O
10	O
is	O
(	O
1/6	O
)	O
4	O
=	O
7.7	O
×	O
10	O
−7	O
.	O
this	O
is	O
a	O
likelihood	B
ratio	I
of	O
almost	O
5000:1	O
in	O
favor	O
of	O
htwo	O
.	O
this	O
quantiﬁes	O
our	O
earlier	O
intuition	O
that	O
d	O
=	O
{	O
16	O
,	O
8	O
,	O
2	O
,	O
64	O
}	O
would	O
be	O
a	O
very	O
suspicious	B
coincidence	I
if	O
generated	O
by	O
heven	O
.	O
3.2.2	O
prior	O
suppose	O
d	O
=	O
{	O
16	O
,	O
8	O
,	O
2	O
,	O
64	O
}	O
.	O
given	O
this	O
data	O
,	O
the	O
concept	B
h	O
(	O
cid:2	O
)	O
more	O
likely	O
than	O
h	O
=	O
“	O
powers	O
of	O
two	O
”	O
,	O
since	O
h	O
(	O
cid:2	O
)	O
is	O
missing	B
from	O
the	O
set	O
of	O
examples	O
.	O
=	O
“	O
powers	O
of	O
two	O
except	O
32	O
”	O
is	O
does	O
not	O
need	O
to	O
explain	O
the	O
coincidence	O
that	O
32	O
however	O
,	O
the	O
hypothesis	O
h	O
(	O
cid:2	O
)	O
=	O
“	O
powers	O
of	O
two	O
except	O
32	O
”	O
seems	O
“	O
conceptually	O
unnatural	O
”	O
.	O
we	O
can	O
capture	O
such	O
intution	O
by	O
assigning	O
low	O
prior	O
probability	O
to	O
unnatural	O
concepts	O
.	O
of	O
course	O
,	O
your	O
prior	O
might	O
be	O
different	O
than	O
mine	O
.	O
this	O
subjective	B
aspect	O
of	O
bayesian	O
reasoning	O
is	O
a	O
source	O
of	O
much	O
controversy	O
,	O
since	O
it	O
means	O
,	O
for	O
example	O
,	O
that	O
a	O
child	O
and	O
a	O
math	O
professor	O
1.	O
william	O
of	O
occam	O
(	O
also	O
spelt	O
ockham	O
)	O
was	O
an	O
english	O
monk	O
and	O
philosopher	O
,	O
1288–1348	O
.	O
68	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
will	O
reach	O
different	O
answers	O
.	O
in	O
fact	O
,	O
they	O
presumably	O
not	O
only	O
have	O
different	O
priors	O
,	O
but	O
also	O
different	O
hypothesis	O
spaces	O
.	O
however	O
,	O
we	O
can	O
ﬁnesse	O
that	O
by	O
deﬁning	O
the	O
hypothesis	B
space	I
of	O
the	O
child	O
and	O
the	O
math	O
professor	O
to	O
be	O
the	O
same	O
,	O
and	O
then	O
setting	O
the	O
child	O
’	O
s	O
prior	O
weight	O
to	O
be	O
zero	O
on	O
certain	O
“	O
advanced	O
”	O
concepts	O
.	O
thus	O
there	O
is	O
no	O
sharp	O
distinction	O
between	O
the	O
prior	O
and	O
the	O
hypothesis	B
space	I
.	O
although	O
the	O
subjectivity	O
of	O
the	O
prior	O
is	O
controversial	O
,	O
it	O
is	O
actually	O
quite	O
useful	O
.	O
if	O
you	O
are	O
told	O
the	O
numbers	O
are	O
from	O
some	O
arithmetic	O
rule	O
,	O
then	O
given	O
1200	O
,	O
1500	O
,	O
900	O
and	O
1400	O
,	O
you	O
may	O
think	O
400	O
is	O
likely	O
but	O
1183	O
is	O
unlikely	O
.	O
but	O
if	O
you	O
are	O
told	O
that	O
the	O
numbers	O
are	O
examples	O
of	O
healthy	O
cholesterol	O
levels	O
,	O
you	O
would	O
probably	O
think	O
400	O
is	O
unlikely	O
and	O
1183	O
is	O
likely	O
.	O
thus	O
we	O
see	O
that	O
the	O
prior	O
is	O
the	O
mechanism	O
by	O
which	O
background	B
knowledge	I
can	O
be	O
brought	O
to	O
bear	O
on	O
a	O
problem	O
.	O
without	O
this	O
,	O
rapid	O
learning	B
(	O
i.e.	O
,	O
from	O
small	O
samples	O
sizes	O
)	O
is	O
impossible	O
.	O
so	O
,	O
what	O
prior	O
should	O
we	O
use	O
?	O
for	O
illustration	O
purposes	O
,	O
let	O
us	O
use	O
a	O
simple	O
prior	O
which	O
puts	O
uniform	O
probability	O
on	O
30	O
simple	O
arithmetical	O
concepts	O
,	O
such	O
as	O
“	O
even	O
numbers	O
”	O
,	O
“	O
odd	O
numbers	O
”	O
,	O
“	O
prime	O
numbers	O
”	O
,	O
“	O
numbers	O
ending	O
in	O
9	O
”	O
,	O
etc	O
.	O
to	O
make	O
things	O
more	O
interesting	O
,	O
we	O
make	O
the	O
concepts	O
even	O
and	O
odd	O
more	O
likely	O
apriori	O
.	O
we	O
also	O
include	O
two	O
“	O
unnatural	O
”	O
concepts	O
,	O
namely	O
“	O
powers	O
of	O
2	O
,	O
plus	O
37	O
”	O
and	O
“	O
powers	O
of	O
2	O
,	O
except	O
32	O
”	O
,	O
but	O
give	O
them	O
low	O
prior	O
weight	O
.	O
see	O
figure	O
3.2	O
(	O
a	O
)	O
for	O
a	O
plot	O
of	O
this	O
prior	O
.	O
we	O
will	O
consider	O
a	O
slightly	O
more	O
sophisticated	O
prior	O
later	O
on	O
.	O
3.2.3	O
posterior	O
the	O
posterior	O
is	O
simply	O
the	O
likelihood	B
times	O
the	O
prior	O
,	O
normalized	O
.	O
in	O
this	O
context	O
we	O
have	O
p	O
(	O
h|d	O
)	O
=	O
(	O
cid:4	O
)	O
p	O
(	O
d|h	O
)	O
p	O
(	O
h	O
)	O
h	O
(	O
cid:2	O
)	O
∈h	O
p	O
(	O
d	O
,	O
h	O
(	O
cid:2	O
)	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
h	O
)	O
i	O
(	O
d	O
∈	O
h	O
)	O
/|h|n	O
h	O
(	O
cid:2	O
)	O
∈h	O
p	O
(	O
h	O
(	O
cid:2	O
)	O
)	O
i	O
(	O
d	O
∈	O
h	O
(	O
cid:2	O
)	O
)	O
/|h	O
(	O
cid:2	O
)	O
|n	O
=	O
(	O
3.3	O
)	O
where	O
i	O
(	O
d	O
∈	O
h	O
)	O
is	O
1	O
iff	B
(	O
iff	B
and	O
only	O
if	O
)	O
all	O
the	O
data	O
are	O
in	O
the	O
extension	B
of	O
the	O
hypothesis	O
h.	O
figure	O
3.2	O
plots	O
the	O
prior	O
,	O
likelihood	B
and	O
posterior	O
after	O
seeing	O
d	O
=	O
{	O
16	O
}	O
.	O
we	O
see	O
that	O
the	O
posterior	O
is	O
a	O
combination	O
of	O
prior	O
and	O
likelihood	B
.	O
in	O
the	O
case	O
of	O
most	O
of	O
the	O
concepts	O
,	O
the	O
prior	O
is	O
uniform	O
,	O
so	O
the	O
posterior	O
is	O
proportional	O
to	O
the	O
likelihood	B
.	O
however	O
,	O
the	O
“	O
unnatural	O
”	O
concepts	O
of	O
“	O
powers	O
of	O
2	O
,	O
plus	O
37	O
”	O
and	O
“	O
powers	O
of	O
2	O
,	O
except	O
32	O
”	O
have	O
low	O
posterior	O
support	B
,	O
despite	O
having	O
high	O
likelihood	O
,	O
due	O
to	O
the	O
low	O
prior	O
.	O
conversely	O
,	O
the	O
concept	B
of	O
odd	O
numbers	O
has	O
low	O
posterior	O
support	B
,	O
despite	O
having	O
a	O
high	O
prior	O
,	O
due	O
to	O
the	O
low	O
likelihood	O
.	O
figure	O
3.3	O
plots	O
the	O
prior	O
,	O
likelihood	B
and	O
posterior	O
after	O
seeing	O
d	O
=	O
{	O
16	O
,	O
8	O
,	O
2	O
,	O
64	O
}	O
.	O
now	O
the	O
likelihood	B
is	O
much	O
more	O
peaked	O
on	O
the	O
powers	O
of	O
two	O
concept	B
,	O
so	O
this	O
dominates	B
the	O
posterior	O
.	O
essentially	O
the	O
learner	O
has	O
an	O
aha	B
moment	O
,	O
and	O
ﬁgures	O
out	O
the	O
true	O
concept	O
.	O
(	O
here	O
we	O
see	O
the	O
need	O
for	O
the	O
low	O
prior	O
on	O
the	O
unnatural	O
concepts	O
,	O
otherwise	O
we	O
would	O
have	O
overﬁt	B
the	O
data	O
and	O
picked	O
“	O
powers	O
of	O
2	O
,	O
except	O
for	O
32	O
”	O
.	O
)	O
in	O
general	O
,	O
when	O
we	O
have	O
enough	O
data	O
,	O
the	O
posterior	O
p	O
(	O
h|d	O
)	O
becomes	O
peaked	O
on	O
a	O
single	O
concept	O
,	O
namely	O
the	O
map	O
estimate	O
,	O
i.e.	O
,	O
p	O
(	O
h|d	O
)	O
→	O
δˆhm	O
ap	O
(	O
h	O
)	O
(	O
3.4	O
)	O
where	O
ˆhm	O
ap	O
=	O
argmaxh	O
p	O
(	O
h|d	O
)	O
is	O
the	O
posterior	B
mode	I
,	O
and	O
where	O
δ	O
is	O
the	O
dirac	O
measure	O
deﬁned	O
by	O
δx	O
(	O
a	O
)	O
=	O
if	O
x	O
∈	O
a	O
if	O
x	O
(	O
cid:6	O
)	O
∈	O
a	O
1	O
0	O
(	O
3.5	O
)	O
(	O
cid:5	O
)	O
3.2.	O
bayesian	O
concept	B
learning	I
69	O
data	O
=	O
16	O
35	O
30	O
25	O
20	O
15	O
10	O
5	O
35	O
30	O
25	O
20	O
15	O
10	O
5	O
even	O
odd	O
squares	O
mult	O
of	O
3	O
mult	O
of	O
4	O
mult	O
of	O
5	O
mult	O
of	O
6	O
mult	O
of	O
7	O
mult	O
of	O
8	O
mult	O
of	O
9	O
mult	O
of	O
10	O
ends	O
in	O
1	O
ends	O
in	O
2	O
ends	O
in	O
3	O
ends	O
in	O
4	O
ends	O
in	O
5	O
ends	O
in	O
6	O
ends	O
in	O
7	O
ends	O
in	O
8	O
ends	O
in	O
9	O
powers	O
of	O
2	O
powers	O
of	O
3	O
powers	O
of	O
4	O
powers	O
of	O
5	O
powers	O
of	O
6	O
powers	O
of	O
7	O
powers	O
of	O
8	O
powers	O
of	O
9	O
powers	O
of	O
10	O
all	O
powers	O
of	O
2	O
+	O
{	O
37	O
}	O
powers	O
of	O
2	O
−	O
{	O
32	O
}	O
0	O
0.1	O
prior	O
0	O
0.2	O
0	O
0	O
0.4	O
0	O
0.2	O
lik	O
0.2	O
post	O
0.4	O
figure	O
3.2	O
prior	O
,	O
likelihood	B
and	O
posterior	O
for	O
d	O
=	O
{	O
16	O
}	O
.	O
based	O
on	O
(	O
tenenbaum	O
1999	O
)	O
.	O
figure	O
generated	O
by	O
numbersgame	O
.	O
note	O
that	O
the	O
map	O
estimate	O
can	O
be	O
written	O
as	O
p	O
(	O
d|h	O
)	O
p	O
(	O
h	O
)	O
=	O
argmax	O
ˆhm	O
ap	O
=	O
argmax	O
h	O
h	O
[	O
log	O
p	O
(	O
d|h	O
)	O
+	O
log	O
p	O
(	O
h	O
)	O
]	O
(	O
3.6	O
)	O
since	O
the	O
likelihood	B
term	O
depends	O
exponentially	O
on	O
n	O
,	O
and	O
the	O
prior	O
stays	O
constant	O
,	O
as	O
we	O
get	O
more	O
and	O
more	O
data	O
,	O
the	O
map	O
estimate	O
converges	O
towards	O
the	O
maximum	B
likelihood	I
estimate	I
or	O
mle	O
:	O
ˆhmle	O
(	O
cid:2	O
)	O
argmax	O
h	O
p	O
(	O
d|h	O
)	O
=	O
argmax	O
log	O
p	O
(	O
d|h	O
)	O
h	O
(	O
3.7	O
)	O
in	O
other	O
words	O
,	O
if	O
we	O
have	O
enough	O
data	O
,	O
we	O
see	O
that	O
the	O
data	B
overwhelms	I
the	I
prior	I
.	O
in	O
this	O
70	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
data	O
=	O
16	O
8	O
2	O
64	O
35	O
30	O
25	O
20	O
15	O
10	O
5	O
35	O
30	O
25	O
20	O
15	O
10	O
5	O
even	O
odd	O
squares	O
mult	O
of	O
3	O
mult	O
of	O
4	O
mult	O
of	O
5	O
mult	O
of	O
6	O
mult	O
of	O
7	O
mult	O
of	O
8	O
mult	O
of	O
9	O
mult	O
of	O
10	O
ends	O
in	O
1	O
ends	O
in	O
2	O
ends	O
in	O
3	O
ends	O
in	O
4	O
ends	O
in	O
5	O
ends	O
in	O
6	O
ends	O
in	O
7	O
ends	O
in	O
8	O
ends	O
in	O
9	O
powers	O
of	O
2	O
powers	O
of	O
3	O
powers	O
of	O
4	O
powers	O
of	O
5	O
powers	O
of	O
6	O
powers	O
of	O
7	O
powers	O
of	O
8	O
powers	O
of	O
9	O
powers	O
of	O
10	O
all	O
powers	O
of	O
2	O
+	O
{	O
37	O
}	O
powers	O
of	O
2	O
−	O
{	O
32	O
}	O
0	O
0.1	O
prior	O
0	O
0.2	O
0	O
1	O
lik	O
0	O
0	O
2	O
x	O
10−3	O
0.5	O
post	O
1	O
figure	O
3.3	O
prior	O
,	O
likelihood	B
and	O
posterior	O
for	O
d	O
=	O
{	O
16	O
,	O
8	O
,	O
2	O
,	O
64	O
}	O
.	O
based	O
on	O
(	O
tenenbaum	O
1999	O
)	O
.	O
figure	O
generated	O
by	O
numbersgame	O
.	O
case	O
,	O
the	O
map	O
estimate	O
converges	O
towards	O
the	O
mle	O
.	O
if	O
the	O
true	O
hypothesis	O
is	O
in	O
the	O
hypothesis	B
space	I
,	O
then	O
the	O
map/	O
ml	O
estimate	O
will	O
converge	B
upon	O
this	O
hypothesis	O
.	O
thus	O
we	O
say	O
that	O
bayesian	O
inference	B
(	O
and	O
ml	O
estimation	O
)	O
are	O
consistent	B
estimators	I
(	O
see	O
section	O
6.4.1	O
for	O
details	O
)	O
.	O
we	O
also	O
say	O
that	O
the	O
hypothesis	B
space	I
is	O
identiﬁable	B
in	I
the	I
limit	I
,	O
meaning	O
we	O
can	O
recover	O
the	O
truth	O
in	O
the	O
limit	O
of	O
inﬁnite	O
data	O
.	O
if	O
our	O
hypothesis	O
class	O
is	O
not	O
rich	O
enough	O
to	O
represent	O
the	O
“	O
truth	O
”	O
(	O
which	O
will	O
usually	O
be	O
the	O
case	O
)	O
,	O
we	O
will	O
converge	B
on	O
the	O
hypothesis	O
that	O
is	O
as	O
close	O
as	O
possible	O
to	O
the	O
truth	O
.	O
however	O
,	O
formalizing	O
this	O
notion	O
of	O
“	O
closeness	O
”	O
is	O
beyond	O
the	O
scope	B
of	O
this	O
chapter	O
.	O
3.2.	O
bayesian	O
concept	B
learning	I
71	O
1	O
0.5	O
0	O
powers	O
of	O
4	O
powers	O
of	O
2	O
ends	O
in	O
6	O
squares	O
even	O
mult	O
of	O
8	O
mult	O
of	O
4	O
all	O
powers	O
of	O
2	O
−	O
{	O
32	O
}	O
powers	O
of	O
2	O
+	O
{	O
37	O
}	O
4	O
8	O
12	O
16	O
20	O
24	O
28	O
32	O
36	O
40	O
44	O
48	O
52	O
56	O
60	O
64	O
68	O
72	O
76	O
80	O
84	O
88	O
92	O
96	O
100	O
0	O
0.5	O
p	O
(	O
h	O
|	O
16	O
)	O
1	O
figure	O
3.4	O
posterior	O
over	O
hypotheses	O
and	O
the	O
corresponding	O
predictive	B
distribution	O
after	O
seeing	O
one	O
example	O
,	O
d	O
=	O
{	O
16	O
}	O
.	O
a	O
dot	O
means	O
this	O
number	O
is	O
consistent	B
with	O
this	O
hypothesis	O
.	O
the	O
graph	B
p	O
(	O
h|d	O
)	O
on	O
the	O
right	O
is	O
the	O
weight	O
given	O
to	O
hypothesis	O
h.	O
by	O
taking	O
a	O
weighed	O
sum	O
of	O
dots	O
,	O
we	O
get	O
p	O
(	O
˜x	O
∈	O
c|d	O
)	O
(	O
top	O
)	O
.	O
based	O
on	O
figure	O
2.9	O
of	O
(	O
tenenbaum	O
1999	O
)	O
.	O
figure	O
generated	O
by	O
numbersgame	O
.	O
3.2.4	O
posterior	B
predictive	I
distribution	I
the	O
posterior	O
is	O
our	O
internal	O
belief	B
state	I
about	O
the	O
world	O
.	O
the	O
way	O
to	O
test	O
if	O
our	O
beliefs	O
are	O
justiﬁed	O
is	O
to	O
use	O
them	O
to	O
predict	O
objectively	O
observable	O
quantities	O
(	O
this	O
is	O
the	O
basis	O
of	O
the	O
scientiﬁc	B
method	I
)	O
.	O
speciﬁcally	O
,	O
the	O
posterior	B
predictive	I
distribution	I
in	O
this	O
context	O
is	O
given	O
by	O
(	O
cid:6	O
)	O
p	O
(	O
˜x	O
∈	O
c|d	O
)	O
=	O
p	O
(	O
y	O
=	O
1|˜x	O
,	O
h	O
)	O
p	O
(	O
h|d	O
)	O
(	O
3.8	O
)	O
h	O
this	O
is	O
just	O
a	O
weighted	B
average	I
of	O
the	O
predictions	O
of	O
each	O
individual	O
hypothesis	O
and	O
is	O
called	O
bayes	O
model	O
averaging	O
(	O
hoeting	O
et	O
al	O
.	O
1999	O
)	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
3.4.	O
the	O
dots	O
at	O
the	O
bottom	O
show	O
the	O
predictions	O
from	O
each	O
hypothesis	O
;	O
the	O
vertical	O
curve	O
on	O
the	O
right	O
shows	O
the	O
weight	O
associated	O
with	O
each	O
hypothesis	O
.	O
if	O
we	O
multiply	O
each	O
row	O
by	O
its	O
weight	O
and	O
add	O
up	O
,	O
we	O
get	O
the	O
distribution	O
at	O
the	O
top	O
.	O
when	O
we	O
have	O
a	O
small	O
and/or	O
ambiguous	O
dataset	O
,	O
the	O
posterior	O
p	O
(	O
h|d	O
)	O
is	O
vague	O
,	O
which	O
induces	O
a	O
broad	O
predictive	B
distribution	O
.	O
however	O
,	O
once	O
we	O
have	O
“	O
ﬁgured	O
things	O
out	O
”	O
,	O
the	O
posterior	O
becomes	O
a	O
delta	O
function	O
centered	O
at	O
the	O
map	O
estimate	O
.	O
in	O
this	O
case	O
,	O
the	O
predictive	B
distribution	O
72	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
becomes	O
p	O
(	O
˜x	O
∈	O
c|d	O
)	O
=	O
(	O
cid:6	O
)	O
h	O
p	O
(	O
˜x|h	O
)	O
δˆh	O
(	O
h	O
)	O
=	O
p	O
(	O
˜x|ˆh	O
)	O
(	O
3.9	O
)	O
this	O
is	O
called	O
a	O
plug-in	B
approximation	I
to	O
the	O
predictive	B
density	O
and	O
is	O
very	O
widely	O
used	O
,	O
due	O
to	O
its	O
simplicity	O
.	O
however	O
,	O
in	O
general	O
,	O
this	O
under-represents	O
our	O
uncertainty	B
,	O
and	O
our	O
predictions	O
will	O
not	O
be	O
as	O
“	O
smooth	O
”	O
as	O
when	O
using	O
bma	O
.	O
we	O
will	O
see	O
more	O
examples	O
of	O
this	O
later	O
in	O
the	O
book	O
.	O
although	O
map	O
learning	B
is	O
simple	O
,	O
it	O
can	O
not	O
explain	O
the	O
gradual	O
shift	O
from	O
similarity-based	O
for	O
reasoning	O
(	O
with	O
uncertain	O
posteriors	O
)	O
to	O
rule-based	O
reasoning	O
(	O
with	O
certain	O
posteriors	O
)	O
.	O
example	O
,	O
suppose	O
we	O
observe	O
d	O
=	O
{	O
16	O
}	O
.	O
if	O
we	O
use	O
the	O
simple	O
prior	O
above	O
,	O
the	O
minimal	B
consistent	O
hypothesis	O
is	O
“	O
all	O
powers	O
of	O
4	O
”	O
,	O
so	O
only	O
4	O
and	O
16	O
get	O
a	O
non-zero	O
probability	O
of	O
being	O
predicted	O
.	O
this	O
is	O
of	O
course	O
an	O
example	O
of	O
overﬁtting	B
.	O
given	O
d	O
=	O
{	O
16	O
,	O
8	O
,	O
2	O
,	O
64	O
}	O
,	O
the	O
map	O
hypothesis	O
is	O
“	O
all	O
powers	O
of	O
two	O
”	O
.	O
thus	O
the	O
plug-in	B
predictive	O
distribution	O
gets	O
broader	O
(	O
or	O
stays	O
the	O
same	O
)	O
as	O
we	O
see	O
more	O
data	O
:	O
it	O
starts	O
narrow	O
,	O
but	O
is	O
forced	O
to	O
broaden	O
as	O
it	O
seems	O
more	O
data	O
.	O
in	O
contrast	O
,	O
in	O
the	O
bayesian	O
approach	O
,	O
we	O
start	O
broad	O
and	O
then	O
narrow	O
down	O
as	O
we	O
learn	O
more	O
,	O
in	O
particular	O
,	O
given	O
d	O
=	O
{	O
16	O
}	O
,	O
there	O
are	O
many	O
hypotheses	O
which	O
makes	O
more	O
intuitive	O
sense	O
.	O
with	O
non-negligible	O
posterior	O
support	O
,	O
so	O
the	O
predictive	B
distribution	O
is	O
broad	O
.	O
however	O
,	O
when	O
we	O
see	O
d	O
=	O
{	O
16	O
,	O
8	O
,	O
2	O
,	O
64	O
}	O
,	O
the	O
posterior	O
concentrates	O
its	O
mass	O
on	O
one	O
hypothesis	O
,	O
so	O
the	O
predictive	B
distribution	O
becomes	O
narrower	O
.	O
so	O
the	O
predictions	O
made	O
by	O
a	O
plug-in	B
approach	O
and	O
a	O
bayesian	O
approach	O
are	O
quite	O
different	O
in	O
the	O
small	O
sample	O
regime	O
,	O
although	O
they	O
converge	B
to	O
the	O
same	O
answer	O
as	O
we	O
see	O
more	O
data	O
.	O
3.2.5	O
a	O
more	O
complex	O
prior	O
to	O
model	O
human	O
behavior	O
,	O
tenenbaum	O
used	O
a	O
slightly	O
more	O
sophisticated	O
prior	O
which	O
was	O
de-	O
rived	O
by	O
analysing	O
some	O
experimental	O
data	O
of	O
how	O
people	O
measure	O
similarity	O
between	O
numbers	O
;	O
see	O
(	O
tenenbaum	O
1999	O
,	O
p208	O
)	O
for	O
details	O
.	O
the	O
result	O
is	O
a	O
set	O
of	O
arithmetical	O
concepts	O
similar	B
to	O
those	O
mentioned	O
above	O
,	O
plus	O
all	O
intervals	O
between	O
n	O
and	O
m	O
for	O
1	O
≤	O
n	O
,	O
m	O
≤	O
100	O
.	O
(	O
note	O
that	O
these	O
hypotheses	O
are	O
not	O
mutually	O
exclusive	O
.	O
)	O
thus	O
the	O
prior	O
is	O
a	O
mixture	O
of	O
two	O
priors	O
,	O
one	O
over	O
arithmetical	O
rules	B
,	O
and	O
one	O
over	O
intervals	O
:	O
interval	O
(	O
h	O
)	O
rules	B
(	O
h	O
)	O
+	O
(	O
1−	O
π0	O
)	O
p	O
p	O
(	O
h	O
)	O
=	O
π0p	O
(	O
3.10	O
)	O
the	O
only	O
free	O
parameter	O
in	O
the	O
model	O
is	O
the	O
relative	O
weight	O
,	O
π0	O
,	O
given	O
to	O
these	O
two	O
parts	O
of	O
the	O
prior	O
.	O
the	O
results	O
are	O
not	O
very	O
sensitive	O
to	O
this	O
value	O
,	O
so	O
long	O
as	O
π0	O
>	O
0.5	O
,	O
reﬂecting	O
the	O
fact	O
that	O
people	O
are	O
more	O
likely	O
to	O
think	O
of	O
concepts	O
deﬁned	O
by	O
rules	B
.	O
the	O
predictive	B
distribution	O
of	O
the	O
model	O
,	O
using	O
this	O
larger	O
hypothesis	B
space	I
,	O
is	O
shown	O
in	O
figure	O
3.5.	O
it	O
is	O
strikingly	O
similar	B
to	O
the	O
human	O
predictive	B
distribution	O
,	O
shown	O
in	O
figure	O
3.1	O
,	O
even	O
though	O
it	O
was	O
not	O
ﬁt	O
to	O
human	O
data	O
(	O
modulo	O
the	O
choice	O
of	O
hypothesis	B
space	I
)	O
.	O
3.3	O
the	O
beta-binomial	B
model	O
the	O
number	B
game	I
involved	O
inferring	O
a	O
distribution	O
over	O
a	O
discrete	B
variable	O
drawn	O
from	O
a	O
ﬁnite	O
hypothesis	O
space	O
,	O
h	O
∈	O
h	O
,	O
given	O
a	O
series	O
of	O
discrete	B
observations	O
.	O
this	O
made	O
the	O
computations	O
particularly	O
simple	O
:	O
we	O
just	O
needed	O
to	O
sum	O
,	O
multiply	O
and	O
divide	O
.	O
however	O
,	O
in	O
many	O
applications	O
,	O
k	O
,	O
where	O
the	O
unknown	B
parameters	O
are	O
continuous	O
,	O
so	O
the	O
hypothesis	B
space	I
is	O
(	O
some	O
subset	O
)	O
of	O
r	O
3.3.	O
the	O
beta-binomial	B
model	O
73	O
examples	O
16	O
60	O
16	O
8	O
2	O
64	O
1	O
0.5	O
0	O
1	O
0.5	O
0	O
1	O
0.5	O
0	O
1	O
16	O
23	O
19	O
20	O
0.5	O
0	O
4	O
8	O
12	O
16	O
20	O
24	O
28	O
32	O
36	O
40	O
44	O
48	O
52	O
56	O
60	O
64	O
68	O
72	O
76	O
80	O
84	O
88	O
92	O
96	O
100	O
4	O
8	O
12	O
16	O
20	O
24	O
28	O
32	O
36	O
40	O
44	O
48	O
52	O
56	O
60	O
64	O
68	O
72	O
76	O
80	O
84	O
88	O
92	O
96	O
100	O
4	O
8	O
12	O
16	O
20	O
24	O
28	O
32	O
36	O
40	O
44	O
48	O
52	O
56	O
60	O
64	O
68	O
72	O
76	O
80	O
84	O
88	O
92	O
96	O
100	O
4	O
8	O
12	O
16	O
20	O
24	O
28	O
32	O
36	O
40	O
44	O
48	O
52	O
56	O
60	O
64	O
68	O
72	O
76	O
80	O
84	O
88	O
92	O
96	O
100	O
figure	O
3.5	O
predictive	B
distributions	O
for	O
the	O
model	O
using	O
the	O
full	B
hypothesis	O
space	O
.	O
compare	O
to	O
figure	O
3.1.	O
the	O
predictions	O
of	O
the	O
bayesian	O
model	O
are	O
only	O
plotted	O
for	O
those	O
values	O
of	O
˜x	O
for	O
which	O
human	O
data	O
is	O
available	O
;	O
this	O
is	O
why	O
the	O
top	O
line	O
looks	O
sparser	O
than	O
figure	O
3.4.	O
source	O
:	O
figure	O
5.6	O
of	O
(	O
tenenbaum	O
1999	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
josh	O
tenenbaum	O
.	O
k	O
is	O
the	O
number	O
of	O
parameters	O
.	O
this	O
complicates	O
the	O
mathematics	O
,	O
since	O
we	O
have	O
to	O
replace	O
sums	O
with	O
integrals	O
.	O
however	O
,	O
the	O
basic	O
ideas	O
are	O
the	O
same	O
.	O
we	O
will	O
illustrate	O
this	O
by	O
considering	O
the	O
problem	O
of	O
inferring	O
the	O
probability	O
that	O
a	O
coin	O
shows	O
up	O
heads	O
,	O
given	O
a	O
series	O
of	O
observed	O
coin	O
tosses	O
.	O
although	O
this	O
might	O
seem	O
trivial	O
,	O
it	O
turns	O
out	O
that	O
this	O
model	O
forms	O
the	O
basis	O
of	O
many	O
of	O
the	O
methods	O
we	O
will	O
consider	O
later	O
in	O
this	O
book	O
,	O
including	O
naive	O
bayes	O
classiﬁers	O
,	O
markov	O
models	O
,	O
etc	O
.	O
it	O
is	O
historically	O
important	O
,	O
since	O
it	O
was	O
the	O
example	O
which	O
was	O
analyzed	O
in	O
bayes	O
’	O
original	O
paper	O
of	O
1763	O
.	O
(	O
bayes	O
’	O
analysis	O
was	O
subsequently	O
generalized	O
by	O
pierre-simon	O
laplace	O
,	O
creating	O
what	O
we	O
now	O
call	O
“	O
bayes	O
rule	O
”	O
—	O
see	O
(	O
stigler	O
1986	O
)	O
for	O
further	O
historical	O
details	O
.	O
)	O
we	O
will	O
follow	O
our	O
now-familiar	O
recipe	O
of	O
specifying	O
the	O
likelihood	B
and	O
prior	O
,	O
and	O
deriving	O
the	O
posterior	O
and	O
posterior	O
predictive	O
.	O
3.3.1	O
likelihood	B
suppose	O
xi	O
∼	O
ber	O
(	O
θ	O
)	O
,	O
where	O
xi	O
=	O
1	O
represents	O
“	O
heads	O
”	O
,	O
xi	O
=	O
0	O
represents	O
“	O
tails	O
”	O
,	O
and	O
θ	O
∈	O
[	O
0	O
,	O
1	O
]	O
is	O
the	O
rate	B
parameter	O
(	O
probability	O
of	O
heads	O
)	O
.	O
if	O
the	O
data	O
are	O
iid	B
,	O
the	O
likelihood	B
has	O
the	O
form	O
p	O
(	O
d|θ	O
)	O
=	O
θn1	O
(	O
1	O
−	O
θ	O
)	O
n0	O
(	O
3.11	O
)	O
74	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
n	O
i=1	O
i	O
(	O
xi	O
=	O
1	O
)	O
heads	O
and	O
n0	O
=	O
where	O
we	O
haven	O
1	O
=	O
i=1	O
i	O
(	O
xi	O
=	O
0	O
)	O
tails	O
.	O
these	O
two	O
counts	O
are	O
called	O
the	O
sufficient	B
statistics	I
of	O
the	O
data	O
,	O
since	O
this	O
is	O
all	O
we	O
need	O
to	O
know	O
about	O
d	O
to	O
infer	O
θ	O
.	O
(	O
an	O
alternative	O
set	O
of	O
sufficient	B
statistics	I
are	O
n1	O
and	O
n	O
=	O
n0	O
+	O
n1	O
.	O
)	O
more	O
formally	O
,	O
we	O
say	O
s	O
(	O
d	O
)	O
is	O
a	O
sufficient	O
statistic	O
for	O
data	O
d	O
if	O
p	O
(	O
θ|d	O
)	O
=p	O
(	O
θ|s	O
(	O
data	O
)	O
)	O
.	O
if	O
we	O
use	O
a	O
uniform	O
prior	O
,	O
this	O
is	O
equivalent	O
to	O
saying	O
p	O
(	O
d|θ	O
∝	O
p	O
(	O
s	O
(	O
d	O
)	O
|θ	O
)	O
.	O
consequently	O
,	O
if	O
we	O
have	O
two	O
datasets	O
with	O
the	O
same	O
sufficient	B
statistics	I
,	O
we	O
will	O
infer	O
the	O
same	O
value	O
for	O
θ.	O
now	O
suppose	O
the	O
data	O
consists	O
of	O
the	O
count	O
of	O
the	O
number	O
of	O
heads	O
n1	O
observed	O
in	O
a	O
ﬁxed	O
number	O
n	O
=	O
n1	O
+	O
n0	O
of	O
trials	O
.	O
in	O
this	O
case	O
,	O
we	O
have	O
n1	O
∼	O
bin	O
(	O
n	O
,	O
θ	O
)	O
,	O
where	O
bin	O
represents	O
the	O
binomial	B
distribution	I
,	O
which	O
has	O
the	O
following	O
pmf	B
:	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
bin	O
(	O
k|n	O
,	O
θ	O
)	O
(	O
cid:2	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
n	O
k	O
n	O
k	O
θk	O
(	O
1	O
−	O
θ	O
)	O
n−k	O
(	O
3.12	O
)	O
is	O
a	O
constant	O
independent	O
of	O
θ	O
,	O
the	O
likelihood	B
for	O
the	O
binomial	B
sampling	O
model	O
is	O
the	O
since	O
same	O
as	O
the	O
likelihood	B
for	O
the	O
bernoulli	O
model	O
.	O
so	O
any	O
inferences	O
we	O
make	O
about	O
θ	O
will	O
be	O
the	O
same	O
whether	O
we	O
observe	O
the	O
counts	O
,	O
d	O
=	O
(	O
n1	O
,	O
n	O
)	O
,	O
or	O
a	O
sequence	O
of	O
trials	O
,	O
d	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
.	O
3.3.2	O
prior	O
we	O
need	O
a	O
prior	O
which	O
has	O
support	B
over	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
.	O
to	O
make	O
the	O
math	O
easier	O
,	O
it	O
would	O
convenient	O
if	O
the	O
prior	O
had	O
the	O
same	O
form	O
as	O
the	O
likelihood	B
,	O
i.e.	O
,	O
if	O
the	O
prior	O
looked	O
like	O
p	O
(	O
θ	O
)	O
∝	O
θγ1	O
(	O
1	O
−	O
θ	O
)	O
γ2	O
(	O
3.13	O
)	O
for	O
some	O
prior	O
parameters	O
γ1	O
and	O
γ2	O
.	O
posterior	O
by	O
simply	O
adding	O
up	O
the	O
exponents	O
:	O
if	O
this	O
were	O
the	O
case	O
,	O
then	O
we	O
could	O
easily	O
evaluate	O
the	O
p	O
(	O
θ	O
)	O
∝	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
=	O
θn1	O
(	O
1	O
−	O
θ	O
)	O
n0	O
θγ1	O
(	O
1	O
−	O
θ	O
)	O
γ2	O
=	O
θn1+γ1	O
(	O
1	O
−	O
θ	O
)	O
n0+γ2	O
(	O
3.14	O
)	O
when	O
the	O
prior	O
and	O
the	O
posterior	O
have	O
the	O
same	O
form	O
,	O
we	O
say	O
that	O
the	O
prior	O
is	O
a	O
conjugate	B
prior	I
for	O
the	O
corresponding	O
likelihood	B
.	O
conjugate	B
priors	I
are	O
widely	O
used	O
because	O
they	O
simplify	O
computation	O
,	O
and	O
are	O
easy	O
to	O
interpret	O
,	O
as	O
we	O
see	O
below	O
.	O
in	O
the	O
case	O
of	O
the	O
bernoulli	O
,	O
the	O
conjugate	B
prior	I
is	O
the	O
beta	B
distribution	I
,	O
which	O
we	O
encountered	O
in	O
section	O
2.4.5	O
:	O
beta	O
(	O
θ|a	O
,	O
b	O
)	O
∝	O
θa−1	O
(	O
1	O
−	O
θ	O
)	O
b−1	O
(	O
3.15	O
)	O
the	O
parameters	O
of	O
the	O
prior	O
are	O
called	O
hyper-parameters	B
.	O
we	O
can	O
set	O
them	O
in	O
order	O
to	O
encode	O
our	O
prior	O
beliefs	O
.	O
for	O
example	O
,	O
to	O
encode	O
our	O
beliefs	O
that	O
θ	O
has	O
mean	B
0.7	O
and	O
standard	B
deviation	I
0.2	O
,	O
we	O
set	O
a	O
=	O
2.975	O
and	O
b	O
=	O
1.275	O
(	O
exercise	O
3.15	O
)	O
.	O
or	O
to	O
encode	O
our	O
beliefs	O
that	O
θ	O
has	O
mean	B
0.15	O
and	O
that	O
we	O
think	O
it	O
lives	O
in	O
the	O
interval	O
(	O
0.05	O
,	O
0.30	O
)	O
with	O
probability	O
,	O
then	O
we	O
ﬁnd	O
a	O
=	O
4.5	O
and	O
b	O
=	O
25.5	O
(	O
exercise	O
3.16	O
)	O
.	O
if	O
we	O
know	O
“	O
nothing	O
”	O
about	O
θ	O
,	O
except	O
that	O
it	O
lies	O
in	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
,	O
we	O
can	O
use	O
a	O
uni-	O
form	O
prior	O
,	O
which	O
is	O
a	O
kind	O
of	O
uninformative	B
prior	O
(	O
see	O
section	O
5.4.2	O
for	O
details	O
)	O
.	O
the	O
uniform	B
distribution	I
can	O
be	O
represented	O
by	O
a	O
beta	B
distribution	I
with	O
a	O
=	O
b	O
=	O
1	O
.	O
3.3.	O
the	O
beta-binomial	B
model	O
75	O
6	O
5	O
4	O
3	O
2	O
1	O
0	O
0	O
prior	O
be	O
(	O
2.0	O
,	O
2.0	O
)	O
lik	O
be	O
(	O
4.0	O
,	O
18.0	O
)	O
post	O
be	O
(	O
5.0	O
,	O
19.0	O
)	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1	O
4.5	O
4	O
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
0	O
prior	O
be	O
(	O
5.0	O
,	O
2.0	O
)	O
lik	O
be	O
(	O
12.0	O
,	O
14.0	O
)	O
post	O
be	O
(	O
16.0	O
,	O
15.0	O
)	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
3.6	O
(	O
a	O
)	O
updating	O
a	O
beta	O
(	O
2	O
,	O
2	O
)	O
prior	O
with	O
a	O
binomial	B
likelihood	O
with	O
sufficient	B
statistics	I
n1	O
=	O
3	O
,	O
n0	O
=	O
17	O
to	O
yield	O
a	O
beta	O
(	O
5,19	O
)	O
posterior	O
.	O
likeli-	O
hood	O
with	O
sufficient	B
statistics	I
n1	O
=	O
11	O
,	O
n0	O
=	O
13	O
to	O
yield	O
a	O
beta	O
(	O
16	O
,	O
15	O
)	O
posterior	O
.	O
figure	O
generated	O
by	O
binomialbetaposteriordemo	O
.	O
(	O
b	O
)	O
updating	O
a	O
beta	O
(	O
5	O
,	O
2	O
)	O
prior	O
with	O
a	O
binomial	B
3.3.3	O
posterior	O
if	O
we	O
multiply	O
the	O
likelihood	B
by	O
the	O
beta	O
prior	O
we	O
get	O
the	O
following	O
posterior	O
(	O
following	O
equa-	O
tion	O
3.14	O
)	O
:	O
p	O
(	O
θ|d	O
)	O
∝	O
bin	O
(	O
n1|θ	O
,	O
n0	O
+	O
n1	O
)	O
beta	O
(	O
θ|a	O
,	O
b	O
)	O
beta	O
(	O
θ|n1	O
+	O
a	O
,	O
n0	O
+	O
b	O
)	O
(	O
3.16	O
)	O
in	O
particular	O
,	O
the	O
posterior	O
is	O
obtained	O
by	O
adding	O
the	O
prior	O
hyper-parameters	B
to	O
the	O
empirical	O
counts	O
.	O
for	O
this	O
reason	O
,	O
the	O
hyper-parameters	B
are	O
known	O
as	O
pseudo	B
counts	I
.	O
the	O
strength	O
of	O
the	O
prior	O
,	O
also	O
known	O
as	O
the	O
effective	B
sample	I
size	I
of	O
the	O
prior	O
,	O
is	O
the	O
sum	O
of	O
the	O
pseudo	B
counts	I
,	O
a	O
+	O
b	O
;	O
this	O
plays	O
a	O
role	O
analogous	O
to	O
the	O
data	O
set	O
size	O
,	O
n1	O
+	O
n0	O
=	O
n	O
.	O
figure	O
3.6	O
(	O
a	O
)	O
gives	O
an	O
example	O
where	O
we	O
update	O
a	O
weak	O
beta	O
(	O
2,2	O
)	O
prior	O
with	O
a	O
peaked	O
likelihood	B
function	O
,	O
corresponding	O
to	O
a	O
large	O
sample	O
size	O
;	O
we	O
see	O
that	O
the	O
posterior	O
is	O
essentially	O
identical	O
to	O
the	O
likelihood	B
:	O
since	O
the	O
data	O
has	O
overwhelmed	O
the	O
prior	O
.	O
figure	O
3.6	O
(	O
b	O
)	O
gives	O
an	O
example	O
where	O
we	O
update	O
a	O
strong	O
beta	O
(	O
5,2	O
)	O
prior	O
with	O
a	O
peaked	O
likelihood	B
function	O
;	O
now	O
we	O
see	O
that	O
the	O
posterior	O
is	O
a	O
“	O
compromise	O
”	O
between	O
the	O
prior	O
and	O
likelihood	B
.	O
to	O
see	O
this	O
,	O
suppose	O
we	O
have	O
two	O
data	O
sets	O
da	O
and	O
db	O
with	O
sufficient	B
statistics	I
n	O
a	O
n	O
b	O
1	O
,	O
n	O
b	O
datasets	O
.	O
in	O
batch	B
mode	O
we	O
have	O
note	O
that	O
updating	O
the	O
posterior	O
sequentially	O
is	O
equivalent	O
to	O
updating	O
in	O
a	O
single	O
batch	O
.	O
0	O
and	O
0	O
be	O
the	O
sufficient	B
statistics	I
of	O
the	O
combined	O
1	O
and	O
n0	O
=	O
n	O
a	O
0	O
.	O
let	O
n1	O
=	O
n	O
a	O
1	O
+	O
n	O
b	O
0	O
+	O
n	O
b	O
1	O
,	O
n	O
a	O
p	O
(	O
θ|da	O
,	O
db	O
)	O
∝	O
bin	O
(	O
n1|θ	O
,	O
n1	O
+	O
n0	O
)	O
beta	O
(	O
θ|a	O
,	O
b	O
)	O
∝	O
beta	O
(	O
θ|n1	O
+	O
a	O
,	O
n0	O
+	O
b	O
)	O
in	O
sequential	B
mode	O
,	O
we	O
have	O
p	O
(	O
θ|da	O
,	O
db	O
)	O
∝	O
p	O
(	O
db|θ	O
)	O
p	O
(	O
θ|da	O
)	O
∝	O
bin	O
(	O
n	O
b	O
1|θ	O
,	O
n	O
b	O
∝	O
beta	O
(	O
θ|	O
n	O
a	O
1	O
+	O
n	O
b	O
1	O
+	O
n	O
b	O
0	O
)	O
beta	O
(	O
θ|n	O
a	O
1	O
+	O
a	O
,	O
n	O
a	O
0	O
+	O
n	O
b	O
1	O
+	O
a	O
,	O
n	O
a	O
0	O
+	O
b	O
)	O
0	O
+	O
b	O
)	O
(	O
3.17	O
)	O
(	O
3.18	O
)	O
(	O
3.19	O
)	O
(	O
3.20	O
)	O
this	O
makes	O
bayesian	O
inference	B
particularly	O
well-suited	O
to	O
online	B
learning	I
,	O
as	O
we	O
will	O
see	O
later	O
.	O
76	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
3.3.3.1	O
posterior	B
mean	I
and	O
mode	B
from	O
equation	O
2.62	O
,	O
the	O
map	O
estimate	O
is	O
given	O
by	O
ˆθm	O
ap	O
=	O
a	O
+	O
n1	O
−	O
1	O
a	O
+	O
b	O
+	O
n	O
−	O
2	O
(	O
3.21	O
)	O
if	O
we	O
use	O
a	O
uniform	O
prior	O
,	O
then	O
the	O
map	O
estimate	O
reduces	O
to	O
the	O
mle	O
,	O
which	O
is	O
just	O
the	O
empirical	O
fraction	O
of	O
heads	O
:	O
n1	O
n	O
ˆθm	O
le	O
=	O
(	O
3.22	O
)	O
this	O
makes	O
intuitive	O
sense	O
,	O
but	O
it	O
can	O
also	O
be	O
derived	O
by	O
applying	O
elementary	O
calculus	O
to	O
maximize	O
the	O
likelihood	B
function	O
in	O
equation	O
3.11	O
.	O
(	O
exercise	O
3.1	O
)	O
.	O
by	O
contrast	O
,	O
the	O
posterior	B
mean	I
is	O
given	O
by	O
,	O
θ	O
=	O
a	O
+	O
n1	O
a	O
+	O
b	O
+	O
n	O
this	O
difference	O
between	O
the	O
mode	B
and	O
the	O
mean	B
will	O
prove	O
important	O
later	O
.	O
(	O
3.23	O
)	O
we	O
will	O
now	O
show	O
that	O
the	O
posterior	B
mean	I
is	O
convex	B
combination	I
of	O
the	O
prior	O
mean	B
and	O
the	O
mle	O
,	O
which	O
captures	O
the	O
notion	O
that	O
the	O
posterior	O
is	O
a	O
compromise	O
between	O
what	O
we	O
previously	O
believed	O
and	O
what	O
the	O
data	O
is	O
telling	O
us	O
.	O
let	O
α0	O
=	O
a	O
+	O
b	O
be	O
the	O
equivalent	B
sample	I
size	I
of	O
the	O
prior	O
,	O
which	O
controls	O
its	O
strength	O
,	O
and	O
let	O
the	O
prior	O
mean	B
be	O
m1	O
=	O
a/α0	O
.	O
then	O
the	O
posterior	B
mean	I
is	O
given	O
by	O
α0	O
=	O
n	O
+	O
α0	O
n	O
+	O
α0	O
α0m1	O
+	O
n1	O
e	O
[	O
θ|d	O
]	O
=	O
where	O
λ	O
=	O
α0	O
n	O
+α0	O
is	O
the	O
ratio	O
of	O
the	O
prior	O
to	O
posterior	O
equivalent	O
sample	O
size	O
.	O
so	O
the	O
weaker	O
the	O
prior	O
,	O
the	O
smaller	O
is	O
λ	O
,	O
and	O
hence	O
the	O
closer	O
the	O
posterior	B
mean	I
is	O
to	O
the	O
mle	O
.	O
one	O
can	O
show	O
similarly	O
that	O
the	O
posterior	B
mode	I
is	O
a	O
convex	B
combination	I
of	O
the	O
prior	O
mode	B
and	O
the	O
mle	O
,	O
and	O
that	O
it	O
too	O
converges	O
to	O
the	O
mle	O
.	O
=	O
λm1	O
+	O
(	O
1−	O
λ	O
)	O
ˆθm	O
le	O
n	O
+	O
α0	O
n1	O
n	O
m1	O
+	O
(	O
3.24	O
)	O
n	O
3.3.3.2	O
posterior	O
variance	O
the	O
mean	B
and	O
mode	B
are	O
point	O
estimates	O
,	O
but	O
it	O
is	O
useful	O
to	O
know	O
how	O
much	O
we	O
can	O
trust	O
them	O
.	O
the	O
variance	B
of	O
the	O
posterior	O
is	O
one	O
way	O
to	O
measure	O
this	O
.	O
the	O
variance	B
of	O
the	O
beta	O
posterior	O
is	O
given	O
by	O
var	O
[	O
θ|d	O
]	O
=	O
(	O
a	O
+	O
n1	O
)	O
(	O
b	O
+	O
n0	O
)	O
(	O
a	O
+	O
n1	O
+	O
b	O
+	O
n0	O
)	O
2	O
(	O
a	O
+	O
n1	O
+	O
b	O
+	O
n0	O
+	O
1	O
)	O
we	O
can	O
simplify	O
this	O
formidable	O
expression	O
in	O
the	O
case	O
that	O
n	O
(	O
cid:8	O
)	O
a	O
,	O
b	O
,	O
to	O
get	O
(	O
3.25	O
)	O
(	O
3.26	O
)	O
var	O
[	O
θ|d	O
]	O
≈	O
n1n0	O
n	O
n	O
n	O
=	O
ˆθ	O
(	O
1	O
−	O
ˆθ	O
)	O
n	O
(	O
cid:10	O
)	O
(	O
cid:9	O
)	O
σ	O
=	O
var	O
[	O
θ|d	O
]	O
≈	O
where	O
ˆθ	O
is	O
the	O
mle	O
.	O
hence	O
the	O
“	O
error	B
bar	I
”	O
in	O
our	O
estimate	O
(	O
i.e.	O
,	O
the	O
posterior	O
standard	O
deviation	O
)	O
,	O
is	O
given	O
by	O
ˆθ	O
(	O
1	O
−	O
ˆθ	O
)	O
n	O
(	O
3.27	O
)	O
3.3.	O
the	O
beta-binomial	B
model	O
77	O
we	O
see	O
that	O
the	O
uncertainty	B
goes	O
down	O
at	O
a	O
rate	B
of	O
1/	O
n	O
.	O
note	O
,	O
however	O
,	O
that	O
the	O
uncertainty	B
(	O
variance	B
)	O
is	O
maximized	O
when	O
ˆθ	O
=	O
0.5	O
,	O
and	O
is	O
minimized	O
when	O
ˆθ	O
is	O
close	O
to	O
0	O
or	O
1.	O
this	O
means	O
it	O
is	O
easier	O
to	O
be	O
sure	O
that	O
a	O
coin	O
is	O
biased	O
than	O
to	O
be	O
sure	O
that	O
it	O
is	O
fair	O
.	O
√	O
3.3.4	O
posterior	B
predictive	I
distribution	I
so	O
far	O
,	O
we	O
have	O
been	O
focusing	O
on	O
inference	B
of	O
the	O
unknown	B
parameter	O
(	O
s	O
)	O
.	O
let	O
us	O
now	O
turn	O
our	O
attention	O
to	O
prediction	O
of	O
future	O
observable	O
data	O
.	O
consider	O
predicting	O
the	O
probability	O
of	O
heads	O
in	O
a	O
single	O
future	O
trial	O
under	O
a	O
beta	O
(	O
a	O
,	O
b	O
)	O
poste-	O
rior	O
.	O
we	O
have	O
p	O
(	O
˜x	O
=	O
1|d	O
)	O
=	O
(	O
cid:11	O
)	O
1	O
(	O
cid:11	O
)	O
1	O
0	O
=	O
0	O
p	O
(	O
x	O
=	O
1|θ	O
)	O
p	O
(	O
θ|d	O
)	O
dθ	O
θ	O
beta	O
(	O
θ|a	O
,	O
b	O
)	O
dθ	O
=	O
e	O
[	O
θ|d	O
]	O
=	O
a	O
a	O
+	O
b	O
(	O
3.28	O
)	O
(	O
3.29	O
)	O
3.3.4.1	O
thus	O
we	O
see	O
that	O
the	O
mean	B
of	O
the	O
posterior	B
predictive	I
distribution	I
is	O
equivalent	O
(	O
in	O
this	O
case	O
)	O
to	O
plugging	O
in	O
the	O
posterior	B
mean	I
parameters	O
:	O
p	O
(	O
˜x|d	O
)	O
=	O
ber	O
(	O
˜x|e	O
[	O
θ|d	O
]	O
)	O
.	O
overﬁtting	B
and	O
the	O
black	B
swan	I
paradox	I
suppose	O
instead	O
that	O
we	O
plug-in	B
the	O
mle	O
,	O
i.e.	O
,	O
we	O
use	O
p	O
(	O
˜x|d	O
)	O
≈	O
ber	O
(	O
˜x|ˆθm	O
le	O
)	O
.	O
unfortunately	O
,	O
this	O
approximation	O
can	O
perform	O
quite	O
poorly	O
when	O
the	O
sample	O
size	O
is	O
small	O
.	O
for	O
example	O
,	O
suppose	O
we	O
have	O
seen	O
n	O
=	O
3	O
tails	O
in	O
a	O
row	O
.	O
the	O
mle	O
is	O
ˆθ	O
=	O
0/3	O
=	O
0	O
,	O
since	O
this	O
makes	O
the	O
observed	O
data	O
as	O
probable	O
as	O
possible	O
.	O
however	O
,	O
using	O
this	O
estimate	O
,	O
we	O
predict	O
that	O
heads	O
are	O
impossible	O
.	O
this	O
is	O
called	O
the	O
zero	B
count	I
problem	I
or	O
the	O
sparse	B
data	I
problem	I
,	O
and	O
frequently	O
occurs	O
when	O
estimating	O
counts	O
from	O
small	O
amounts	O
of	O
data	O
.	O
one	O
might	O
think	O
that	O
in	O
the	O
era	O
of	O
“	O
big	B
data	I
”	O
,	O
such	O
concerns	O
are	O
irrelevant	O
,	O
but	O
note	O
that	O
once	O
we	O
partition	O
the	O
data	O
based	O
on	O
certain	O
criteria	O
—	O
such	O
as	O
the	O
number	O
of	O
times	O
a	O
speciﬁc	O
person	O
has	O
engaged	O
in	O
a	O
speciﬁc	O
activity	O
—	O
the	O
sample	O
sizes	O
can	O
become	O
much	O
smaller	O
.	O
this	O
problem	O
arises	O
,	O
for	O
example	O
,	O
when	O
trying	O
to	O
perform	O
personalized	B
recommendation	I
of	O
web	O
pages	O
.	O
thus	O
bayesian	O
methods	O
are	O
still	O
useful	O
,	O
even	O
in	O
the	O
big	B
data	I
regime	O
(	O
jordan	O
2011	O
)	O
.	O
the	O
zero-count	O
problem	O
is	O
analogous	O
to	O
a	O
problem	O
in	O
philosophy	O
called	O
the	O
black	B
swan	I
paradox	I
.	O
this	O
is	O
based	O
on	O
the	O
ancient	O
western	O
conception	O
that	O
all	O
swans	O
were	O
white	O
.	O
in	O
that	O
context	O
,	O
a	O
black	O
swan	O
was	O
a	O
metaphor	O
for	O
something	O
that	O
could	O
not	O
exist	O
.	O
(	O
black	O
swans	O
were	O
discovered	O
in	O
australia	O
by	O
european	O
explorers	O
in	O
the	O
17th	O
century	O
.	O
)	O
the	O
term	O
“	O
black	B
swan	I
paradox	I
”	O
was	O
ﬁrst	O
coined	O
by	O
the	O
famous	O
philosopher	O
of	O
science	O
karl	O
popper	O
;	O
the	O
term	O
has	O
also	O
been	O
used	O
as	O
the	O
title	O
of	O
a	O
recent	O
popular	O
book	O
(	O
taleb	O
2007	O
)	O
.	O
this	O
paradox	O
was	O
used	O
to	O
illustrate	O
the	O
problem	O
of	O
induction	B
,	O
which	O
is	O
the	O
problem	O
of	O
how	O
to	O
draw	O
general	O
conclusions	O
about	O
the	O
future	O
from	O
speciﬁc	O
observations	O
from	O
the	O
past	O
.	O
let	O
us	O
now	O
derive	O
a	O
simple	O
bayesian	O
solution	O
to	O
the	O
problem	O
.	O
we	O
will	O
use	O
a	O
uniform	O
prior	O
,	O
so	O
a	O
=	O
b	O
=	O
1.	O
in	O
this	O
case	O
,	O
plugging	O
in	O
the	O
posterior	B
mean	I
gives	O
laplace	O
’	O
s	O
rule	O
of	O
succession	O
p	O
(	O
˜x	O
=	O
1|d	O
)	O
=	O
n1	O
+	O
1	O
n1	O
+	O
n0	O
+	O
2	O
(	O
3.30	O
)	O
this	O
justiﬁes	O
the	O
common	O
practice	O
of	O
adding	O
1	O
to	O
the	O
empirical	O
counts	O
,	O
normalizing	O
and	O
then	O
plugging	O
them	O
in	O
,	O
a	O
technique	O
known	O
as	O
add-one	B
smoothing	I
.	O
(	O
note	O
that	O
plugging	O
in	O
the	O
map	O
78	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
parameters	O
would	O
not	O
have	O
this	O
smoothing	O
effect	O
,	O
since	O
the	O
mode	B
has	O
the	O
form	O
ˆθ	O
=	O
n1+a−1	O
n	O
+a+b−2	O
,	O
which	O
becomes	O
the	O
mle	O
if	O
a	O
=	O
b	O
=	O
1	O
.	O
)	O
3.3.4.2	O
predicting	O
the	O
outcome	O
of	O
multiple	O
future	O
trials	O
suppose	O
now	O
we	O
were	O
interested	O
in	O
predicting	O
the	O
number	O
of	O
heads	O
,	O
x	O
,	O
in	O
m	O
future	O
trials	O
.	O
this	O
is	O
given	O
by	O
(	O
cid:11	O
)	O
1	O
(	O
cid:7	O
)	O
0	O
m	O
x	O
p	O
(	O
x|d	O
,	O
m	O
)	O
=	O
bin	O
(	O
x|θ	O
,	O
m	O
)	O
beta	O
(	O
θ|a	O
,	O
b	O
)	O
dθ	O
(	O
cid:8	O
)	O
(	O
cid:11	O
)	O
1	O
1	O
θx	O
(	O
1	O
−	O
θ	O
)	O
m−xθa−1	O
(	O
1	O
−	O
θ	O
)	O
b−1dθ	O
=	O
(	O
3.32	O
)	O
we	O
recognize	O
the	O
integral	O
as	O
the	O
normalization	O
constant	O
for	O
a	O
beta	O
(	O
a+x	O
,	O
m−x+b	O
)	O
distribution	O
.	O
hence	O
(	O
cid:11	O
)	O
1	O
b	O
(	O
a	O
,	O
b	O
)	O
0	O
θx	O
(	O
1	O
−	O
θ	O
)	O
m−xθa−1	O
(	O
1	O
−	O
θ	O
)	O
b−1dθ	O
=	O
b	O
(	O
x	O
+	O
a	O
,	O
m	O
−	O
x	O
+	O
b	O
)	O
(	O
3.33	O
)	O
0	O
(	O
3.31	O
)	O
thus	O
we	O
ﬁnd	O
that	O
the	O
posterior	O
predictive	O
is	O
given	O
by	O
the	O
following	O
,	O
known	O
as	O
the	O
(	O
compound	O
)	O
beta-binomial	B
distribution	O
:	O
bb	O
(	O
x|a	O
,	O
b	O
,	O
m	O
)	O
(	O
cid:2	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
m	O
x	O
b	O
(	O
x	O
+	O
a	O
,	O
m	O
−	O
x	O
+	O
b	O
)	O
b	O
(	O
a	O
,	O
b	O
)	O
(	O
3.34	O
)	O
this	O
distribution	O
has	O
the	O
following	O
mean	B
and	O
variance	B
a	O
m	O
ab	O
(	O
a	O
+	O
b	O
+	O
m	O
)	O
e	O
[	O
x	O
]	O
=m	O
,	O
var	O
[	O
x	O
]	O
=	O
a	O
+	O
b	O
(	O
3.35	O
)	O
if	O
m	O
=	O
1	O
,	O
and	O
hence	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
we	O
see	O
that	O
the	O
mean	B
becomes	O
e	O
[	O
x|d	O
]	O
=	O
p	O
(	O
x	O
=	O
1|d	O
)	O
=	O
a	O
a+b	O
,	O
which	O
is	O
consistent	B
with	O
equation	O
3.29.	O
a	O
+	O
b	O
+	O
1	O
(	O
a	O
+	O
b	O
)	O
2	O
this	O
process	O
is	O
illustrated	O
in	O
figure	O
3.7	O
(	O
a	O
)	O
.	O
we	O
start	O
with	O
a	O
beta	O
(	O
2,2	O
)	O
prior	O
,	O
and	O
plot	O
the	O
posterior	B
predictive	I
density	I
after	O
seeing	O
n1	O
=	O
3	O
heads	O
and	O
n0	O
=	O
17	O
tails	O
.	O
figure	O
3.7	O
(	O
b	O
)	O
plots	O
a	O
plug-in	B
approximation	I
using	O
a	O
map	O
estimate	O
.	O
we	O
see	O
that	O
the	O
bayesian	O
prediction	O
has	O
longer	O
tails	O
,	O
spreading	O
its	O
probablity	O
mass	O
more	O
widely	O
,	O
and	O
is	O
therefore	O
less	O
prone	O
to	O
overﬁtting	B
and	O
blackswan	O
type	O
paradoxes	O
.	O
3.4	O
the	O
dirichlet-multinomial	O
model	O
in	O
the	O
previous	O
section	O
,	O
we	O
discussed	O
how	O
to	O
infer	O
the	O
probability	O
that	O
a	O
coin	O
comes	O
up	O
heads	O
.	O
in	O
this	O
section	O
,	O
we	O
generalize	B
these	O
results	O
to	O
infer	O
the	O
probability	O
that	O
a	O
dice	O
with	O
k	O
sides	O
comes	O
up	O
as	O
face	O
k.	O
this	O
might	O
seem	O
like	O
another	O
toy	O
exercise	O
,	O
but	O
the	O
methods	O
we	O
will	O
study	O
are	O
widely	O
used	O
to	O
analyse	O
text	O
data	O
,	O
biosequence	O
data	O
,	O
etc.	O
,	O
as	O
we	O
will	O
see	O
later	O
.	O
3.4.	O
the	O
dirichlet-multinomial	O
model	O
79	O
posterior	O
predictive	O
plugin	O
predictive	B
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
3.7	O
figure	O
generated	O
by	O
betabinompostpreddemo	O
.	O
(	O
a	O
)	O
posterior	O
predictive	O
distributions	O
after	O
seeing	O
n1	O
=	O
3	O
,	O
n0	O
=	O
17	O
.	O
(	O
b	O
)	O
plugin	O
approximation	O
.	O
3.4.1	O
likelihood	B
suppose	O
we	O
observe	O
n	O
dice	O
rolls	O
,	O
d	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
,	O
where	O
xi	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
.	O
the	O
data	O
is	O
iid	B
,	O
the	O
likelihood	B
has	O
the	O
form	O
if	O
we	O
assume	O
(	O
3.36	O
)	O
k	O
(	O
cid:12	O
)	O
p	O
(	O
d|θ	O
)	O
=	O
(	O
cid:4	O
)	O
n	O
k=1	O
θnk	O
k	O
i=1	O
i	O
(	O
yi	O
=	O
k	O
)	O
is	O
the	O
number	O
of	O
times	O
event	O
k	O
occured	O
(	O
these	O
are	O
the	O
sufficient	O
where	O
nk	O
=	O
statistics	O
for	O
this	O
model	O
)	O
.	O
the	O
likelihood	B
for	O
the	O
multinomial	B
model	O
has	O
the	O
same	O
form	O
,	O
up	O
to	O
an	O
irrelevant	O
constant	O
factor	O
.	O
3.4.2	O
prior	O
since	O
the	O
parameter	B
vector	O
lives	O
in	O
the	O
k-dimensional	O
probability	B
simplex	I
,	O
we	O
need	O
a	O
prior	O
that	O
has	O
support	B
over	O
this	O
simplex	O
.	O
ideally	O
it	O
would	O
also	O
be	O
conjugate	O
.	O
fortunately	O
,	O
the	O
dirichlet	O
distribution	O
(	O
section	O
2.5.4	O
)	O
satisﬁes	O
both	O
criteria	O
.	O
so	O
we	O
will	O
use	O
the	O
following	O
prior	O
:	O
dir	O
(	O
θ|α	O
)	O
=	O
1	O
b	O
(	O
α	O
)	O
3.4.3	O
posterior	O
k	O
(	O
cid:12	O
)	O
k=1	O
θαk−1	O
k	O
i	O
(	O
x	O
∈	O
sk	O
)	O
multiplying	O
the	O
likelihood	B
by	O
the	O
prior	O
,	O
we	O
ﬁnd	O
that	O
the	O
posterior	O
is	O
also	O
dirichlet	O
:	O
p	O
(	O
θ|d	O
)	O
∝	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
k	O
θαk−1	O
θnk	O
∝	O
k	O
(	O
cid:12	O
)	O
θαk+nk−1	O
=	O
dir	O
(	O
θ|α1	O
+	O
n1	O
,	O
.	O
.	O
.	O
,	O
αk	O
+	O
nk	O
)	O
k	O
(	O
cid:12	O
)	O
k=1	O
k	O
=	O
k	O
k=1	O
(	O
3.37	O
)	O
(	O
3.38	O
)	O
(	O
3.39	O
)	O
(	O
3.40	O
)	O
80	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
we	O
see	O
that	O
the	O
posterior	O
is	O
obtained	O
by	O
adding	O
the	O
prior	O
hyper-parameters	B
(	O
pseudo-counts	O
)	O
αk	O
to	O
the	O
empirical	O
counts	O
nk	O
.	O
we	O
can	O
derive	O
the	O
mode	B
of	O
this	O
posterior	O
(	O
i.e.	O
,	O
the	O
map	O
estimate	O
)	O
by	O
using	O
calculus	O
.	O
however	O
,	O
k	O
θk	O
=	O
1.2.	O
we	O
can	O
do	O
this	O
by	O
using	O
a	O
lagrange	O
we	O
must	O
enforce	O
the	O
constraint	O
that	O
multiplier	O
.	O
the	O
constrained	O
objective	O
function	O
,	O
or	O
lagrangian	O
,	O
is	O
given	O
by	O
the	O
log	O
likelihood	O
plus	O
log	O
prior	O
plus	O
the	O
constraint	O
:	O
(	O
cid:4	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:13	O
)	O
(	O
cid:14	O
)	O
(	O
cid:6	O
)	O
(	O
cid:9	O
)	O
(	O
θ	O
,	O
λ	O
)	O
=	O
nk	O
log	O
θk	O
+	O
(	O
αk	O
−	O
1	O
)	O
log	O
θk	O
+	O
λ	O
1	O
−	O
θk	O
(	O
3.41	O
)	O
k	O
k	O
k	O
k	O
(	O
cid:2	O
)	O
nk	O
+	O
αk	O
−	O
1.	O
taking	O
derivatives	O
with	O
respect	O
to	O
λ	O
yields	O
to	O
simplify	O
notation	O
,	O
we	O
deﬁne	O
n	O
(	O
cid:2	O
)	O
the	O
original	O
constraint	O
:	O
(	O
cid:14	O
)	O
(	O
cid:13	O
)	O
(	O
cid:6	O
)	O
k	O
1	O
−	O
θk	O
=	O
0	O
∂	O
(	O
cid:9	O
)	O
∂λ	O
=	O
taking	O
derivatives	O
with	O
respect	O
to	O
θk	O
yields	O
−	O
λ	O
=	O
0	O
=	O
n	O
(	O
cid:2	O
)	O
∂	O
(	O
cid:9	O
)	O
k	O
∂θk	O
θk	O
n	O
(	O
cid:2	O
)	O
k	O
=	O
λθk	O
(	O
cid:6	O
)	O
n	O
(	O
cid:2	O
)	O
k	O
=	O
λ	O
n	O
+	O
α0	O
−	O
k	O
=	O
λ	O
(	O
cid:4	O
)	O
k	O
k	O
where	O
α0	O
(	O
cid:2	O
)	O
given	O
by	O
ˆθk	O
=	O
nk	O
+	O
αk	O
−	O
1	O
n	O
+	O
α0	O
−	O
k	O
(	O
cid:6	O
)	O
k	O
θk	O
we	O
can	O
solve	O
for	O
λ	O
using	O
the	O
sum-to-one	O
constraint	O
:	O
(	O
3.42	O
)	O
(	O
3.43	O
)	O
(	O
3.44	O
)	O
(	O
3.45	O
)	O
(	O
3.46	O
)	O
(	O
3.47	O
)	O
k=1	O
αk	O
is	O
the	O
equivalent	B
sample	I
size	I
of	O
the	O
prior	O
.	O
thus	O
the	O
map	O
estimate	O
is	O
which	O
is	O
consistent	B
with	O
equation	O
2.77.	O
if	O
we	O
use	O
a	O
uniform	O
prior	O
,	O
αk	O
=	O
1	O
,	O
we	O
recover	O
the	O
mle	O
:	O
ˆθk	O
=	O
nk/n	O
(	O
3.48	O
)	O
this	O
is	O
just	O
the	O
empirical	O
fraction	O
of	O
times	O
face	O
k	O
shows	O
up	O
.	O
2.	O
we	O
do	O
not	O
need	O
to	O
explicitly	O
enforce	O
the	O
constraint	O
that	O
θk	O
≥	O
0	O
since	O
the	O
gradient	O
of	O
the	O
objective	O
has	O
the	O
form	O
nk/θk	O
−	O
λ	O
;	O
so	O
negative	O
values	O
would	O
reduce	O
the	O
objective	O
,	O
rather	O
than	O
maximize	O
it	O
.	O
(	O
of	O
course	O
,	O
this	O
does	O
not	O
preclude	O
setting	O
θk	O
=	O
0	O
,	O
and	O
indeed	O
this	O
is	O
the	O
optimal	O
solution	O
if	O
nk	O
=	O
0	O
and	O
αk	O
=	O
1	O
.	O
)	O
3.4.	O
the	O
dirichlet-multinomial	O
model	O
81	O
3.4.4	O
posterior	O
predictive	O
the	O
posterior	B
predictive	I
distribution	I
for	O
a	O
single	O
multinoulli	O
trial	O
expression	O
:	O
is	O
given	O
by	O
the	O
following	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
p	O
(	O
x	O
=	O
j|d	O
)	O
=	O
=	O
=	O
(	O
cid:2	O
)	O
(	O
cid:11	O
)	O
p	O
(	O
x	O
=	O
j|θ	O
)	O
p	O
(	O
θ|d	O
)	O
dθ	O
p	O
(	O
x	O
=	O
j|θj	O
)	O
θjp	O
(	O
θj|d	O
)	O
dθj	O
=	O
e	O
[	O
θj|d	O
]	O
=	O
p	O
(	O
θ−j	O
,	O
θj|d	O
)	O
dθ−j	O
(	O
cid:3	O
)	O
dθj	O
(	O
cid:4	O
)	O
αj	O
+	O
nj	O
k	O
(	O
αk	O
+	O
nk	O
)	O
=	O
αj	O
+	O
nj	O
α0	O
+	O
n	O
(	O
3.49	O
)	O
(	O
3.50	O
)	O
(	O
3.51	O
)	O
where	O
θ−j	O
are	O
all	O
the	O
components	O
of	O
θ	O
except	O
θj	O
.	O
see	O
also	O
exercise	O
3.13.	O
the	O
above	O
expression	O
avoids	O
the	O
zero-count	O
problem	O
,	O
just	O
as	O
we	O
saw	O
in	O
section	O
3.3.4.1.	O
in	O
fact	O
,	O
this	O
form	O
of	O
bayesian	O
smoothing	O
is	O
even	O
more	O
important	O
in	O
the	O
multinomial	B
case	O
than	O
the	O
binary	O
case	O
,	O
since	O
the	O
likelihood	B
of	O
data	O
sparsity	O
increases	O
once	O
we	O
start	O
partitioning	B
the	O
data	O
into	O
many	O
categories	O
.	O
3.4.4.1	O
worked	O
example	O
:	O
language	B
models	I
using	O
bag	B
of	I
words	I
one	O
application	O
of	O
bayesian	O
smoothing	O
using	O
the	O
dirichlet-multinomial	O
model	O
is	O
to	O
language	B
modeling	I
,	O
which	O
means	O
predicting	O
which	O
words	O
might	O
occur	O
next	O
in	O
a	O
sequence	O
.	O
here	O
we	O
will	O
take	O
a	O
very	O
simple-minded	O
approach	O
,	O
and	O
assume	O
that	O
the	O
i	O
’	O
th	O
word	O
,	O
xi	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
is	O
sampled	O
independently	O
from	O
all	O
the	O
other	O
words	O
using	O
a	O
cat	O
(	O
θ	O
)	O
distribution	O
.	O
this	O
is	O
called	O
the	O
bag	B
of	I
words	I
model	O
.	O
given	O
a	O
past	O
sequence	O
of	O
words	O
,	O
how	O
can	O
we	O
predict	O
which	O
one	O
is	O
likely	O
to	O
come	O
next	O
?	O
for	O
example	O
,	O
suppose	O
we	O
observe	O
the	O
following	O
sequence	O
(	O
part	O
of	O
a	O
children	B
’	O
s	O
nursery	O
rhyme	O
)	O
:	O
mary	O
had	O
a	O
little	O
lamb	O
,	O
little	O
lamb	O
,	O
little	O
lamb	O
,	O
mary	O
had	O
a	O
little	O
lamb	O
,	O
its	O
fleece	O
as	O
white	O
as	O
snow	O
furthermore	O
,	O
suppose	O
our	O
vocabulary	O
consists	O
of	O
the	O
following	O
words	O
:	O
mary	O
lamb	O
little	O
big	O
fleece	O
white	O
black	O
snow	O
rain	O
unk	B
10	O
1	O
2	O
3	O
4	O
5	O
8	O
9	O
6	O
7	O
here	O
unk	B
stands	O
for	O
unknown	B
,	O
and	O
represents	O
all	O
other	O
words	O
that	O
do	O
not	O
appear	O
elsewhere	O
on	O
the	O
list	O
.	O
to	O
encode	O
each	O
line	O
of	O
the	O
nursery	O
rhyme	O
,	O
we	O
ﬁrst	O
strip	O
off	O
punctuation	O
,	O
and	O
remove	O
any	O
stop	B
words	I
such	O
as	O
“	O
a	O
”	O
,	O
“	O
as	O
”	O
,	O
“	O
the	O
”	O
,	O
etc	O
.	O
we	O
can	O
also	O
perform	O
stemming	B
,	O
which	O
means	O
reducing	O
words	O
to	O
their	O
base	O
form	O
,	O
such	O
as	O
stripping	O
off	O
the	O
ﬁnal	O
s	O
in	O
plural	O
words	O
,	O
or	O
the	O
ing	O
from	O
verbs	O
(	O
e.g.	O
,	O
running	O
becomes	O
run	O
)	O
.	O
in	O
this	O
example	O
,	O
no	O
words	O
need	O
stemming	B
.	O
finally	O
,	O
we	O
replace	O
each	O
word	O
by	O
its	O
index	O
into	O
the	O
vocabulary	O
to	O
get	O
:	O
1	O
10	O
3	O
2	O
3	O
2	O
3	O
2	O
1	O
10	O
3	O
2	O
10	O
5	O
10	O
6	O
8	O
we	O
now	O
ignore	O
the	O
word	O
order	O
,	O
and	O
count	O
how	O
often	O
each	O
word	O
occurred	O
,	O
resulting	O
in	O
a	O
histogram	B
of	O
word	O
counts	O
:	O
82	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
1	O
token	O
word	O
mary	O
count	O
2	O
2	O
lamb	O
4	O
3	O
little	O
4	O
4	O
big	O
0	O
6	O
5	O
ﬂeece	O
white	O
1	O
1	O
7	O
black	O
0	O
9	O
8	O
snow	O
rain	O
1	O
0	O
10	O
unk	B
4	O
just	O
denote	O
the	O
above	O
counts	O
by	O
nj	O
.	O
(	O
cid:4	O
)	O
p	O
(	O
˜x	O
=	O
j|d	O
)	O
=	O
e	O
[	O
θj|d	O
]	O
=	O
if	O
we	O
use	O
a	O
dir	O
(	O
α	O
)	O
prior	O
for	O
θ	O
,	O
the	O
posterior	O
predictive	O
is	O
αj	O
+	O
nj	O
j	O
(	O
cid:2	O
)	O
αj	O
(	O
cid:2	O
)	O
+	O
nj	O
(	O
cid:2	O
)	O
=	O
1	O
+	O
nj	O
10	O
+	O
17	O
(	O
3.52	O
)	O
if	O
we	O
set	O
αj	O
=	O
1	O
,	O
we	O
get	O
p	O
(	O
˜x	O
=	O
j|d	O
)	O
=	O
(	O
3/27	O
,	O
5/27	O
,	O
5/27	O
,	O
1/27	O
,	O
2/27	O
,	O
2/27	O
,	O
1/27	O
,	O
2/27	O
,	O
1/27	O
,	O
5/27	O
)	O
(	O
3.53	O
)	O
the	O
modes	O
of	O
the	O
predictive	B
distribution	O
are	O
x	O
=	O
2	O
(	O
“	O
lamb	O
”	O
)	O
and	O
x	O
=	O
10	O
(	O
“	O
unk	B
”	O
)	O
.	O
note	O
that	O
the	O
words	O
“	O
big	O
”	O
,	O
“	O
black	O
”	O
and	O
“	O
rain	O
”	O
are	O
predicted	O
to	O
occur	O
with	O
non-zero	O
probability	O
in	O
the	O
future	O
,	O
even	O
though	O
they	O
have	O
never	O
been	O
seen	O
before	O
.	O
later	O
on	O
we	O
will	O
see	O
more	O
sophisticated	O
language	B
models	I
.	O
3.5	O
naive	O
bayes	O
classiﬁers	O
in	O
this	O
section	O
,	O
we	O
discuss	O
how	O
to	O
classify	O
vectors	O
of	O
discrete-valued	O
features	B
,	O
x	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
d	O
,	O
where	O
k	O
is	O
the	O
number	O
of	O
values	O
for	O
each	O
feature	O
,	O
and	O
d	O
is	O
the	O
number	O
of	O
features	B
.	O
we	O
will	O
use	O
a	O
generative	B
approach	I
.	O
this	O
requires	O
us	O
to	O
specify	O
the	O
class	O
conditional	O
distribution	O
,	O
p	O
(	O
x|y	O
=	O
c	O
)	O
.	O
the	O
simplest	O
approach	O
is	O
to	O
assume	O
the	O
features	B
are	O
conditionally	B
independent	I
given	O
the	O
class	O
label	O
.	O
this	O
allows	O
us	O
to	O
write	O
the	O
class	O
conditional	O
density	O
as	O
a	O
product	O
of	O
one	O
dimensional	O
densities	O
:	O
p	O
(	O
x|y	O
=	O
c	O
,	O
θ	O
)	O
=	O
p	O
(	O
xj|y	O
=	O
c	O
,	O
θjc	O
)	O
(	O
3.54	O
)	O
d	O
(	O
cid:12	O
)	O
j=1	O
the	O
resulting	O
model	O
is	O
called	O
a	O
naive	O
bayes	O
classiﬁer	O
(	O
nbc	O
)	O
.	O
the	O
model	O
is	O
called	O
“	O
naive	O
”	O
since	O
we	O
do	O
not	O
expect	O
the	O
features	B
to	O
be	O
independent	O
,	O
even	O
conditional	O
on	O
the	O
class	O
label	O
.	O
however	O
,	O
even	O
if	O
the	O
naive	O
bayes	O
assumption	O
is	O
not	O
true	O
,	O
it	O
often	O
results	O
in	O
classiﬁers	O
that	O
work	O
well	O
(	O
domingos	O
and	O
pazzani	O
1997	O
)	O
.	O
one	O
reason	O
for	O
this	O
is	O
that	O
the	O
model	O
is	O
quite	O
simple	O
(	O
it	O
only	O
has	O
o	O
(	O
cd	O
)	O
parameters	O
,	O
for	O
c	O
classes	O
and	O
d	O
features	B
)	O
,	O
and	O
hence	O
it	O
is	O
relatively	O
immune	O
to	O
overﬁtting	B
.	O
the	O
form	O
of	O
the	O
class-conditional	B
density	I
depends	O
on	O
the	O
type	O
of	O
each	O
feature	O
.	O
we	O
give	O
some	O
possibilities	O
below	O
:	O
•	O
•	O
jc	O
)	O
,	O
where	O
μjc	O
is	O
the	O
mean	B
of	O
feature	O
j	O
in	O
objects	O
of	O
class	O
c	O
,	O
and	O
σ2	O
(	O
cid:15	O
)	O
d	O
in	O
the	O
case	O
of	O
real-valued	O
features	B
,	O
we	O
can	O
use	O
the	O
gaussian	O
distribution	O
:	O
p	O
(	O
x|y	O
=	O
c	O
,	O
θ	O
)	O
=	O
j=1	O
n	O
(	O
xj|μjc	O
,	O
σ2	O
jc	O
is	O
its	O
variance	B
.	O
(	O
cid:15	O
)	O
d	O
in	O
the	O
case	O
of	O
binary	O
features	O
,	O
xj	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
we	O
can	O
use	O
the	O
bernoulli	O
distribution	O
:	O
p	O
(	O
x|y	O
=	O
j=1	O
ber	O
(	O
xj|μjc	O
)	O
,	O
where	O
μjc	O
is	O
the	O
probability	O
that	O
feature	O
j	O
occurs	O
in	O
class	O
c.	O
c	O
,	O
θ	O
)	O
=	O
this	O
is	O
sometimes	O
called	O
the	O
multivariate	O
bernoulli	O
naive	O
bayes	O
model	O
.	O
we	O
will	O
see	O
an	O
application	O
of	O
this	O
below	O
.	O
3.5.	O
naive	O
bayes	O
classiﬁers	O
83	O
•	O
(	O
cid:15	O
)	O
d	O
in	O
the	O
case	O
of	O
categorical	B
features	O
,	O
xj	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
we	O
can	O
model	O
use	O
the	O
multinoulli	B
distribution	I
:	O
p	O
(	O
x|y	O
=	O
c	O
,	O
θ	O
)	O
=	O
j=1	O
cat	O
(	O
xj|μjc	O
)	O
,	O
where	O
μjc	O
is	O
a	O
histogram	B
over	O
the	O
k	O
possible	O
values	O
for	O
xj	O
in	O
class	O
c.	O
obviously	O
we	O
can	O
handle	O
other	O
kinds	O
of	O
features	B
,	O
or	O
use	O
different	O
distributional	O
assumptions	O
.	O
also	O
,	O
it	O
is	O
easy	O
to	O
mix	O
and	O
match	O
features	B
of	O
different	O
types	O
.	O
3.5.1	O
model	O
ﬁtting	O
we	O
now	O
discuss	O
how	O
to	O
“	O
train	O
”	O
a	O
naive	O
bayes	O
classiﬁer	O
.	O
this	O
usually	O
means	O
computing	O
the	O
mle	O
or	O
the	O
map	O
estimate	O
for	O
the	O
parameters	O
.	O
however	O
,	O
we	O
will	O
also	O
discuss	O
how	O
to	O
compute	O
the	O
full	B
posterior	O
,	O
p	O
(	O
θ|d	O
)	O
.	O
3.5.1.1	O
mle	O
for	O
nbc	O
the	O
probability	O
for	O
a	O
single	O
data	O
case	O
is	O
given	O
by	O
p	O
(	O
xi	O
,	O
yi|θ	O
)	O
=	O
p	O
(	O
yi|π	O
)	O
p	O
(	O
xij|θj	O
)	O
=	O
πi	O
(	O
yi=c	O
)	O
c	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
p	O
(	O
xij|θjc	O
)	O
i	O
(	O
yi=c	O
)	O
j	O
c	O
j	O
c	O
hence	O
the	O
log-likelihood	O
is	O
given	O
by	O
log	O
p	O
(	O
d|θ	O
)	O
=	O
nc	O
log	O
πc	O
+	O
c	O
(	O
cid:6	O
)	O
d	O
(	O
cid:6	O
)	O
c	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
log	O
p	O
(	O
xij|θjc	O
)	O
c=1	O
j=1	O
c=1	O
i	O
:	O
yi=c	O
we	O
see	O
that	O
this	O
expression	O
decomposes	B
into	O
a	O
series	O
of	O
terms	O
,	O
one	O
concerning	O
π	O
,	O
and	O
dc	O
terms	O
containing	O
the	O
θjc	O
’	O
s	O
.	O
hence	O
we	O
can	O
optimize	O
all	O
these	O
parameters	O
separately	O
.	O
from	O
equation	O
3.48	O
,	O
the	O
mle	O
for	O
the	O
class	O
prior	O
is	O
given	O
by	O
(	O
3.55	O
)	O
(	O
3.56	O
)	O
(	O
3.57	O
)	O
ˆπc	O
=	O
nc	O
n	O
(	O
cid:4	O
)	O
i	O
i	O
(	O
yi	O
=	O
c	O
)	O
is	O
the	O
number	O
of	O
examples	O
in	O
class	O
c.	O
where	O
nc	O
(	O
cid:2	O
)	O
the	O
mle	O
for	O
the	O
likelihood	B
depends	O
on	O
the	O
type	O
of	O
distribution	O
we	O
choose	O
to	O
use	O
for	O
each	O
feature	O
.	O
for	O
simplicity	O
,	O
let	O
us	O
suppose	O
all	O
features	O
are	O
binary	O
,	O
so	O
xj|y	O
=	O
c	O
∼	O
ber	O
(	O
θjc	O
)	O
.	O
in	O
this	O
case	O
,	O
the	O
mle	O
becomes	O
ˆθjc	O
=	O
njc	O
nc	O
(	O
3.58	O
)	O
it	O
is	O
extremely	O
simple	O
to	O
implement	O
this	O
model	O
ﬁtting	O
procedure	O
:	O
see	O
algorithm	O
8	O
for	O
some	O
pseudo-code	O
(	O
and	O
naivebayesfit	O
for	O
some	O
matlab	O
code	O
)	O
.	O
this	O
algorithm	O
obviously	O
takes	O
o	O
(	O
n	O
d	O
)	O
time	O
.	O
the	O
method	O
is	O
easily	O
generalized	O
to	O
handle	O
features	B
of	O
mixed	O
type	O
.	O
this	O
simplicity	O
is	O
one	O
reason	O
the	O
method	O
is	O
so	O
widely	O
used	O
.	O
figure	O
3.8	O
gives	O
an	O
example	O
where	O
we	O
have	O
2	O
classes	O
and	O
600	O
binary	O
features	O
,	O
representing	O
the	O
presence	O
or	O
absence	O
of	O
words	O
in	O
a	O
bag-of-words	B
model	O
.	O
the	O
plot	O
visualizes	O
the	O
θc	O
vectors	O
for	O
the	O
two	O
classes	O
.	O
the	O
big	O
spike	O
at	O
index	O
107	O
corresponds	O
to	O
the	O
word	O
“	O
subject	O
”	O
,	O
which	O
occurs	O
in	O
both	O
classes	O
with	O
probability	O
1	O
.	O
(	O
in	O
section	O
3.5.4	O
,	O
we	O
discuss	O
how	O
to	O
“	O
ﬁlter	O
out	O
”	O
such	O
uninformative	B
features	O
.	O
)	O
84	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
algorithm	O
3.1	O
:	O
fitting	O
a	O
naive	O
bayes	O
classiﬁer	O
to	O
binary	O
features	O
1	O
nc	O
=	O
0	O
,	O
njc	O
=	O
0	O
;	O
2	O
for	O
i	O
=	O
1	O
:	O
n	O
do	O
3	O
c	O
=	O
yi	O
//	O
class	O
label	O
of	O
i	O
’	O
th	O
example	O
;	O
nc	O
:	O
=	O
nc	O
+	O
1	O
;	O
for	O
j	O
=	O
1	O
:	O
d	O
do	O
4	O
5	O
6	O
7	O
if	O
xij	O
=	O
1	O
then	O
njc	O
:	O
=	O
njc	O
+	O
1	O
8	O
ˆπc	O
=	O
nc	O
n	O
,	O
ˆθjc	O
=	O
njc	O
n	O
p	O
(	O
xj=1|y=1	O
)	O
p	O
(	O
xj=1|y=2	O
)	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
100	O
200	O
300	O
400	O
500	O
600	O
700	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
100	O
200	O
300	O
400	O
500	O
600	O
700	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
3.8	O
class	O
conditional	O
densities	O
p	O
(	O
xj	O
=	O
1|y	O
=	O
c	O
)	O
for	O
two	O
document	O
classes	O
,	O
corresponding	O
to	O
“	O
x	O
windows	O
”	O
and	O
“	O
ms	O
windows	O
”	O
.	O
figure	O
generated	O
by	O
naivebayesbowdemo	O
.	O
3.5.1.2	O
bayesian	O
naive	O
bayes	O
the	O
trouble	O
with	O
maximum	O
likelihood	O
is	O
that	O
it	O
can	O
overﬁt	B
.	O
for	O
example	O
,	O
consider	O
the	O
example	O
in	O
figure	O
3.8	O
:	O
the	O
feature	O
corresponding	O
to	O
the	O
word	O
“	O
subject	O
”	O
(	O
call	O
it	O
feature	O
j	O
)	O
always	O
occurs	O
in	O
both	O
classes	O
,	O
so	O
we	O
estimate	O
ˆθjc	O
=	O
1.	O
what	O
will	O
happen	O
if	O
we	O
encounter	O
a	O
new	O
email	O
which	O
does	O
not	O
have	O
this	O
word	O
in	O
it	O
?	O
our	O
algorithm	O
will	O
crash	O
and	O
burn	O
,	O
since	O
we	O
will	O
ﬁnd	O
that	O
p	O
(	O
y	O
=	O
c|x	O
,	O
ˆθ	O
)	O
=	O
0	O
for	O
both	O
classes	O
!	O
this	O
is	O
another	O
manifestation	O
of	O
the	O
black	B
swan	I
paradox	I
discussed	O
in	O
section	O
3.3.4.1.	O
a	O
simple	O
solution	O
to	O
overﬁtting	B
is	O
to	O
be	O
bayesian	O
.	O
for	O
simplicity	O
,	O
we	O
will	O
use	O
a	O
factored	O
prior	O
:	O
d	O
(	O
cid:12	O
)	O
c	O
(	O
cid:12	O
)	O
p	O
(	O
θ	O
)	O
=p	O
(	O
π	O
)	O
p	O
(	O
θjc	O
)	O
(	O
3.59	O
)	O
j=1	O
c=1	O
we	O
will	O
use	O
a	O
dir	O
(	O
α	O
)	O
prior	O
for	O
π	O
and	O
a	O
beta	O
(	O
β0	O
,	O
β1	O
)	O
prior	O
for	O
each	O
θjc	O
.	O
often	O
we	O
just	O
take	O
α	O
=	O
1	O
and	O
β	O
=	O
1	O
,	O
corresponding	O
to	O
add-one	O
or	O
laplace	O
smoothing	O
.	O
3.5.	O
naive	O
bayes	O
classiﬁers	O
85	O
combining	O
the	O
factored	O
likelihood	B
in	O
equation	O
3.56	O
with	O
the	O
factored	O
prior	O
above	O
gives	O
the	O
following	O
factored	O
posterior	O
:	O
d	O
(	O
cid:12	O
)	O
c	O
(	O
cid:12	O
)	O
p	O
(	O
θ|d	O
)	O
=p	O
(	O
π|d	O
)	O
p	O
(	O
π|d	O
)	O
=	O
dir	O
(	O
n1	O
+	O
α1	O
.	O
.	O
.	O
,	O
nc	O
+	O
αc	O
)	O
p	O
(	O
θjc|d	O
)	O
=	O
beta	O
(	O
(	O
nc	O
−	O
njc	O
)	O
+β	O
0	O
,	O
njc	O
+	O
β1	O
)	O
p	O
(	O
θjc|d	O
)	O
j=1	O
c=	O
(	O
3.60	O
)	O
(	O
3.61	O
)	O
(	O
3.62	O
)	O
in	O
other	O
words	O
,	O
to	O
compute	O
the	O
posterior	O
,	O
we	O
just	O
update	O
the	O
prior	O
counts	O
with	O
the	O
empirical	O
counts	O
from	O
the	O
likelihood	B
.	O
it	O
is	O
straightforward	O
to	O
modify	O
algorithm	O
8	O
to	O
handle	O
this	O
version	O
of	O
model	O
“	O
ﬁtting	O
”	O
.	O
3.5.2	O
using	O
the	O
model	O
for	O
prediction	O
at	O
test	O
time	O
,	O
the	O
goal	O
is	O
to	O
compute	O
p	O
(	O
y	O
=	O
c|x	O
,	O
d	O
)	O
∝	O
p	O
(	O
y	O
=	O
c|d	O
)	O
d	O
(	O
cid:12	O
)	O
j=1	O
p	O
(	O
xj|y	O
=	O
c	O
,	O
d	O
)	O
the	O
correct	O
bayesian	O
procedure	O
is	O
to	O
integrate	B
out	I
the	O
unknown	B
parameters	O
:	O
p	O
(	O
y	O
=	O
c|x	O
,	O
d	O
)	O
∝	O
(	O
cid:3	O
)	O
cat	O
(	O
y	O
=	O
c|π	O
)	O
p	O
(	O
π|d	O
)	O
dπ	O
(	O
cid:2	O
)	O
(	O
cid:11	O
)	O
ber	O
(	O
xj|y	O
=	O
c	O
,	O
θjc	O
)	O
p	O
(	O
θjc|d	O
)	O
(	O
cid:3	O
)	O
(	O
cid:2	O
)	O
(	O
cid:11	O
)	O
d	O
(	O
cid:12	O
)	O
j=1	O
in	O
particular	O
,	O
from	O
equa-	O
fortunately	O
,	O
this	O
is	O
easy	O
to	O
do	O
,	O
at	O
least	O
if	O
the	O
posterior	O
is	O
dirichlet	O
.	O
tion	O
3.51	O
,	O
we	O
know	O
the	O
posterior	B
predictive	I
density	I
can	O
be	O
obtained	O
by	O
simply	O
plugging	O
in	O
the	O
posterior	B
mean	I
parameters	O
θ.	O
hence	O
p	O
(	O
y	O
=	O
c|x	O
,	O
d	O
)	O
∝	O
πc	O
(	O
θjc	O
)	O
i	O
(	O
xj	O
=1	O
)	O
(	O
1	O
−	O
θjc	O
)	O
i	O
(	O
xj	O
=0	O
)	O
(	O
3.63	O
)	O
(	O
3.64	O
)	O
(	O
3.65	O
)	O
(	O
3.66	O
)	O
(	O
3.67	O
)	O
(	O
3.68	O
)	O
j=1	O
njc	O
+	O
β1	O
nc	O
+	O
β0	O
+	O
β1	O
nc	O
+	O
αc	O
n	O
+	O
α0	O
θjk	O
=	O
πc	O
=	O
(	O
cid:4	O
)	O
d	O
(	O
cid:12	O
)	O
d	O
(	O
cid:12	O
)	O
j=1	O
c	O
αc	O
.	O
where	O
α0	O
=	O
if	O
we	O
have	O
approximated	O
the	O
posterior	O
by	O
a	O
single	O
point	O
,	O
p	O
(	O
θ|d	O
)	O
≈	O
δˆθ	O
(	O
θ	O
)	O
,	O
where	O
ˆθ	O
may	O
be	O
the	O
ml	O
or	O
map	O
estimate	O
,	O
then	O
the	O
posterior	B
predictive	I
density	I
is	O
obtained	O
by	O
simply	O
plugging	O
in	O
the	O
parameters	O
,	O
to	O
yield	O
a	O
virtually	O
identical	O
rule	O
:	O
p	O
(	O
y	O
=	O
c|x	O
,	O
d	O
)	O
∝	O
ˆπc	O
(	O
ˆθjc	O
)	O
i	O
(	O
xj	O
=1	O
)	O
(	O
1	O
−	O
ˆθjc	O
)	O
i	O
(	O
xj	O
=0	O
)	O
(	O
3.69	O
)	O
86	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
the	O
only	O
difference	O
is	O
we	O
replaced	O
the	O
posterior	B
mean	I
θ	O
with	O
the	O
posterior	B
mode	I
or	O
mle	O
ˆθ	O
.	O
however	O
,	O
this	O
small	O
difference	O
can	O
be	O
important	O
in	O
practice	O
,	O
since	O
the	O
posterior	B
mean	I
will	O
result	O
in	O
less	O
overﬁtting	B
(	O
see	O
section	O
3.4.4.1	O
)	O
.	O
3.5.3	O
the	O
log-sum-exp	B
trick	O
we	O
now	O
discuss	O
one	O
important	O
practical	O
detail	O
that	O
arises	O
when	O
using	O
generative	O
classiﬁers	O
of	O
any	O
kind	O
.	O
we	O
can	O
compute	O
the	O
posterior	O
over	O
class	O
labels	O
using	O
equation	O
2.13	O
,	O
using	O
the	O
appropriate	O
class-conditional	B
density	I
(	O
and	O
a	O
plug-in	B
approximation	I
)	O
.	O
unfortunately	O
a	O
naive	O
implementation	O
of	O
equation	O
2.13	O
can	O
fail	O
due	O
to	O
numerical	B
underﬂow	I
.	O
the	O
problem	O
is	O
that	O
p	O
(	O
x|y	O
=	O
c	O
)	O
is	O
often	O
a	O
very	O
small	O
number	O
,	O
especially	O
if	O
x	O
is	O
a	O
high-dimensional	O
vector	O
.	O
this	O
is	O
because	O
we	O
require	O
x	O
p	O
(	O
x|y	O
)	O
=	O
1	O
,	O
so	O
the	O
probability	O
of	O
observing	O
any	O
particular	O
high-dimensional	O
vector	O
is	O
that	O
small	O
.	O
the	O
obvious	O
solution	O
is	O
to	O
take	O
logs	O
when	O
applying	O
bayes	O
rule	O
,	O
as	O
follows	O
:	O
(	O
cid:4	O
)	O
(	O
cid:17	O
)	O
ebc	O
(	O
cid:2	O
)	O
(	O
cid:16	O
)	O
c	O
(	O
cid:6	O
)	O
c	O
(	O
cid:2	O
)	O
=1	O
log	O
p	O
(	O
y	O
=	O
c|x	O
)	O
=b	O
c	O
−	O
log	O
bc	O
(	O
cid:2	O
)	O
log	O
p	O
(	O
x|y	O
=	O
c	O
)	O
+	O
log	O
p	O
(	O
y	O
=	O
c	O
)	O
however	O
,	O
this	O
requires	O
evaluating	O
the	O
following	O
expression	O
ebc	O
(	O
cid:2	O
)	O
]	O
=	O
log	O
p	O
(	O
y	O
=	O
c	O
(	O
cid:2	O
)	O
,	O
x	O
)	O
=	O
log	O
p	O
(	O
x	O
)	O
(	O
cid:6	O
)	O
log	O
[	O
c	O
(	O
cid:2	O
)	O
(	O
cid:6	O
)	O
c	O
(	O
cid:2	O
)	O
(	O
3.70	O
)	O
(	O
3.71	O
)	O
(	O
3.72	O
)	O
(	O
3.73	O
)	O
(	O
3.74	O
)	O
and	O
we	O
can	O
’	O
t	O
add	O
up	O
in	O
the	O
log	O
domain	O
.	O
fortunately	O
,	O
we	O
can	O
factor	B
out	O
the	O
largest	O
term	O
,	O
and	O
just	O
represent	O
the	O
remaining	O
numbers	O
relative	O
to	O
that	O
.	O
for	O
example	O
,	O
log	O
(	O
e−120	O
+	O
e−121	O
)	O
=	O
log	O
=	O
log	O
(	O
e0	O
+	O
e−1	O
)	O
−	O
120	O
(	O
cid:19	O
)	O
e−120	O
(	O
e0	O
+	O
e−1	O
)	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
(	O
cid:16	O
)	O
(	O
cid:6	O
)	O
c	O
ebc	O
=	O
log	O
(	O
ebc−b	O
)	O
eb	O
=	O
log	O
(	O
in	O
general	O
,	O
we	O
have	O
(	O
cid:6	O
)	O
log	O
c	O
(	O
cid:17	O
)	O
ebc−b	O
)	O
+	O
b	O
(	O
cid:6	O
)	O
c	O
where	O
b	O
=	O
maxc	O
bc	O
.	O
this	O
is	O
called	O
the	O
log-sum-exp	B
trick	O
,	O
and	O
is	O
widely	O
used	O
.	O
(	O
see	O
the	O
function	O
logsumexp	O
for	O
an	O
implementation	O
.	O
)	O
this	O
trick	O
is	O
used	O
in	O
algorithm	O
1	O
which	O
gives	O
pseudo-code	O
for	O
using	O
an	O
nbc	O
to	O
compute	O
p	O
(	O
yi|xi	O
,	O
ˆθ	O
)	O
.	O
see	O
naivebayespredict	O
for	O
the	O
matlab	O
code	O
.	O
note	O
that	O
we	O
do	O
not	O
need	O
the	O
log-sum-exp	B
trick	O
if	O
we	O
only	O
want	O
to	O
compute	O
ˆyi	O
,	O
since	O
we	O
can	O
just	O
maximize	O
the	O
unnormalized	O
quantity	O
log	O
p	O
(	O
yi	O
=	O
c	O
)	O
+	O
log	O
p	O
(	O
xi|y	O
=	O
c	O
)	O
.	O
3.5.4	O
feature	B
selection	I
using	O
mutual	B
information	I
since	O
an	O
nbc	O
is	O
ﬁtting	O
a	O
joint	B
distribution	I
over	O
potentially	O
many	O
features	B
,	O
it	O
can	O
suffer	O
from	O
overﬁtting	B
.	O
in	O
addition	O
,	O
the	O
run-time	O
cost	O
is	O
o	O
(	O
d	O
)	O
,	O
which	O
may	O
be	O
too	O
high	O
for	O
some	O
applications	O
.	O
one	O
common	O
approach	O
to	O
tackling	O
both	O
of	O
these	O
problems	O
is	O
to	O
perform	O
feature	B
selection	I
,	O
to	O
remove	O
“	O
irrelevant	O
”	O
features	B
that	O
do	O
not	O
help	O
much	O
with	O
the	O
classiﬁcation	B
problem	O
.	O
the	O
simplest	O
approach	O
to	O
feature	B
selection	I
is	O
to	O
evaluate	O
the	O
relevance	O
of	O
each	O
feature	O
separately	O
,	O
and	O
then	O
3.5.	O
naive	O
bayes	O
classiﬁers	O
87	O
algorithm	O
3.2	O
:	O
predicting	O
with	O
a	O
naive	O
bayes	O
classiﬁer	O
for	O
binary	O
features	O
1	O
for	O
i	O
=	O
1	O
:	O
n	O
do	O
2	O
for	O
c	O
=	O
1	O
:	O
c	O
do	O
lic	O
=	O
log	O
ˆπc	O
;	O
for	O
j	O
=	O
1	O
:	O
d	O
do	O
pic	O
=	O
exp	O
(	O
lic	O
−	O
logsumexp	O
(	O
li	O
,	O
:	O
)	O
)	O
;	O
ˆyi	O
=	O
argmaxc	O
pic	O
;	O
3	O
4	O
5	O
6	O
7	O
if	O
xij	O
=	O
1	O
then	O
lic	O
:	O
=	O
lic	O
+	O
log	O
ˆθjc	O
else	O
lic	O
:	O
=	O
lic	O
+	O
log	O
(	O
1−	O
ˆθjc	O
)	O
take	O
the	O
top	O
k	O
,	O
where	O
k	O
is	O
chosen	O
based	O
on	O
some	O
tradeoff	O
between	O
accuracy	O
and	O
complexity	O
.	O
this	O
approach	O
is	O
known	O
as	O
variable	O
ranking	O
,	O
ﬁltering	B
,	O
orscreening	O
.	O
one	O
way	O
to	O
measure	O
relevance	O
is	O
to	O
use	O
mutual	B
information	I
(	O
section	O
2.8.3	O
)	O
between	O
feature	O
xj	O
and	O
the	O
class	O
label	O
y	O
:	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
xj	O
y	O
i	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
xj	O
,	O
y	O
)	O
log	O
p	O
(	O
xj	O
,	O
y	O
)	O
p	O
(	O
xj	O
)	O
p	O
(	O
y	O
)	O
(	O
3.75	O
)	O
the	O
mutual	B
information	I
can	O
be	O
thought	O
of	O
as	O
the	O
reduction	O
in	O
entropy	B
on	O
the	O
label	B
distribution	O
once	O
we	O
observe	O
the	O
value	O
of	O
feature	O
j.	O
if	O
the	O
features	B
are	O
binary	O
,	O
it	O
is	O
easy	O
to	O
show	O
(	O
exercise	O
3.21	O
)	O
that	O
the	O
mi	O
can	O
be	O
computed	O
as	O
follows	O
(	O
cid:2	O
)	O
(	O
cid:6	O
)	O
c	O
ij	O
=	O
θjcπc	O
log	O
θjc	O
θj	O
+	O
(	O
1−	O
θjc	O
)	O
πc	O
log	O
(	O
cid:3	O
)	O
1	O
−	O
θjc	O
1	O
−	O
θj	O
where	O
πc	O
=	O
p	O
(	O
y	O
=	O
c	O
)	O
,	O
θjc	O
=	O
p	O
(	O
xj	O
=	O
1|y	O
=	O
c	O
)	O
,	O
and	O
θj	O
=	O
p	O
(	O
xj	O
=	O
1	O
)	O
=	O
quantities	O
can	O
be	O
computed	O
as	O
a	O
by-product	O
of	O
ﬁtting	O
a	O
naive	O
bayes	O
classiﬁer	O
.	O
)	O
c	O
πcθjc	O
.	O
figure	O
3.1	O
illustrates	O
what	O
happens	O
if	O
we	O
apply	O
this	O
to	O
the	O
binary	O
bag	O
of	O
words	O
dataset	O
used	O
in	O
figure	O
3.8.	O
we	O
see	O
that	O
the	O
words	O
with	O
highest	O
mutual	O
information	B
are	O
much	O
more	O
discriminative	B
than	O
the	O
words	O
which	O
are	O
most	O
probable	O
.	O
for	O
example	O
,	O
the	O
most	O
probable	O
word	O
in	O
both	O
classes	O
is	O
“	O
subject	O
”	O
,	O
which	O
always	O
occurs	O
because	O
this	O
is	O
newsgroup	O
data	O
,	O
which	O
always	O
has	O
a	O
subject	O
line	O
.	O
but	O
obviously	O
this	O
is	O
not	O
very	O
discriminative	B
.	O
the	O
words	O
with	O
highest	O
mi	O
with	O
the	O
class	O
label	O
are	O
(	O
in	O
decreasing	O
order	O
)	O
“	O
windows	O
”	O
,	O
“	O
microsoft	O
”	O
,	O
“	O
dos	O
”	O
and	O
“	O
motif	B
”	O
,	O
which	O
makes	O
sense	O
,	O
since	O
the	O
classes	O
correspond	O
to	O
microsoft	O
windows	O
and	O
x	O
windows	O
.	O
3.5.5	O
classifying	O
documents	O
using	O
bag	B
of	I
words	I
(	O
cid:4	O
)	O
(	O
3.76	O
)	O
(	O
all	O
of	O
these	O
document	B
classiﬁcation	I
is	O
the	O
problem	O
of	O
classifying	O
text	O
documents	O
into	O
different	O
categories	O
.	O
one	O
simple	O
approach	O
is	O
to	O
represent	O
each	O
document	O
as	O
a	O
binary	O
vector	O
,	O
which	O
records	O
whether	O
each	O
word	O
is	O
present	O
or	O
not	O
,	O
so	O
xij	O
=	O
1	O
iff	B
word	O
j	O
occurs	O
in	O
document	O
i	O
,	O
otherwise	O
xij	O
=	O
0.	O
we	O
can	O
then	O
use	O
the	O
following	O
class	O
conditional	O
density	O
:	O
p	O
(	O
xi|yi	O
=	O
c	O
,	O
θ	O
)	O
=	O
ber	O
(	O
xij|θjc	O
)	O
=	O
θi	O
(	O
xij	O
)	O
jc	O
(	O
1	O
−	O
θjc	O
)	O
i	O
(	O
1−xij	O
)	O
(	O
3.77	O
)	O
d	O
(	O
cid:12	O
)	O
d	O
(	O
cid:12	O
)	O
j=1	O
j=1	O
88	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
class	O
1	O
subject	O
this	O
with	O
but	O
you	O
prob	O
class	O
2	O
0.998	O
subject	O
0.628	O
windows	O
0.535	O
0.471	O
0.431	O
this	O
with	O
but	O
prob	O
0.998	O
0.639	O
0.540	O
0.538	O
0.518	O
highest	O
mi	O
windows	O
microsoft	O
dos	O
motif	B
window	O
mi	O
0.215	O
0.095	O
0.092	O
0.078	O
0.067	O
table	O
3.1	O
we	O
list	O
the	O
5	O
most	O
likely	O
words	O
for	O
class	O
1	O
(	O
x	O
windows	O
)	O
and	O
class	O
2	O
(	O
ms	O
windows	O
)	O
.	O
we	O
also	O
show	O
the	O
5	O
words	O
with	O
highest	O
mutual	O
information	B
with	O
class	O
label	O
.	O
produced	O
by	O
naivebayesbowdemo	O
this	O
is	O
called	O
the	O
bernoulli	O
product	O
model	O
,	O
or	O
thebinary	O
independence	O
model	O
.	O
however	O
,	O
ignoring	O
the	O
number	O
of	O
times	O
each	O
word	O
occurs	O
in	O
a	O
document	O
loses	O
some	O
in-	O
formation	O
(	O
mccallum	O
and	O
nigam	O
1998	O
)	O
.	O
a	O
more	O
accurate	O
representation	O
counts	O
the	O
number	O
let	O
xi	O
be	O
a	O
vector	O
of	O
counts	O
for	O
document	O
i	O
,	O
so	O
of	O
occurrences	O
of	O
each	O
word	O
.	O
speciﬁcally	O
,	O
xij	O
∈	O
{	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
ni	O
}	O
,	O
where	O
ni	O
is	O
the	O
number	O
of	O
terms	O
in	O
document	O
i	O
(	O
so	O
j=1	O
xij	O
=	O
ni	O
)	O
.	O
for	O
the	O
class	O
conditional	O
densities	O
,	O
we	O
can	O
use	O
a	O
multinomial	B
distribution	O
:	O
(	O
cid:4	O
)	O
d	O
θxij	O
jc	O
(	O
3.78	O
)	O
p	O
(	O
xi|yi	O
=	O
c	O
,	O
θ	O
)	O
=	O
mu	O
(	O
xi|ni	O
,	O
θc	O
)	O
=	O
ni	O
!	O
(	O
cid:15	O
)	O
d	O
j=1	O
xij	O
!	O
d	O
(	O
cid:12	O
)	O
j=1	O
where	O
we	O
have	O
implicitly	O
assumed	O
that	O
the	O
document	O
length	O
ni	O
is	O
independent	O
of	O
the	O
class	O
.	O
here	O
θjc	O
is	O
the	O
probability	O
of	O
generating	O
word	O
j	O
in	O
documents	O
of	O
class	O
c	O
;	O
these	O
parameters	O
satisfy	O
the	O
constraint	O
that	O
j=1	O
θjc	O
=	O
1	O
for	O
each	O
class	O
c.3	O
(	O
cid:4	O
)	O
d	O
although	O
the	O
multinomial	B
classiﬁer	O
is	O
easy	O
to	O
train	O
and	O
easy	O
to	O
use	O
at	O
test	O
time	O
,	O
it	O
does	O
not	O
work	O
particularly	O
well	O
for	O
document	B
classiﬁcation	I
.	O
one	O
reason	O
for	O
this	O
is	O
that	O
it	O
does	O
not	O
take	O
into	O
account	O
the	O
burstiness	B
of	O
word	O
usage	O
.	O
this	O
refers	O
to	O
the	O
phenomenon	O
that	O
most	O
words	O
never	O
appear	O
in	O
any	O
given	O
document	O
,	O
but	O
if	O
they	O
do	O
appear	O
once	O
,	O
they	O
are	O
likely	O
to	O
appear	O
more	O
than	O
once	O
,	O
i.e.	O
,	O
words	O
occur	O
in	O
bursts	O
.	O
the	O
multinomial	B
model	O
can	O
not	O
capture	O
the	O
burstiness	B
phenomenon	O
.	O
to	O
see	O
why	O
,	O
note	O
that	O
jc	O
,	O
and	O
since	O
θjc	O
(	O
cid:11	O
)	O
1	O
for	O
rare	O
words	O
,	O
it	O
becomes	O
increasingly	O
equation	O
3.78	O
has	O
the	O
form	O
θnij	O
unlikely	O
to	O
generate	O
many	O
of	O
them	O
.	O
for	O
more	O
frequent	O
words	O
,	O
the	O
decay	O
rate	B
is	O
not	O
as	O
fast	O
.	O
to	O
see	O
why	O
intuitively	O
,	O
note	O
that	O
the	O
most	O
frequent	O
words	O
are	O
function	O
words	O
which	O
are	O
not	O
speciﬁc	O
to	O
the	O
class	O
,	O
such	O
as	O
“	O
and	O
”	O
,	O
“	O
the	O
”	O
,	O
and	O
“	O
but	O
”	O
;	O
the	O
chance	O
of	O
the	O
word	O
“	O
and	O
”	O
occuring	O
is	O
pretty	O
much	O
the	O
same	O
no	O
matter	O
how	O
many	O
time	O
it	O
has	O
previously	O
occurred	O
(	O
modulo	O
document	O
length	O
)	O
,	O
so	O
the	O
independence	O
assumption	O
is	O
more	O
reasonable	O
for	O
common	O
words	O
.	O
however	O
,	O
since	O
rare	O
words	O
are	O
the	O
ones	O
that	O
matter	O
most	O
for	O
classiﬁcation	B
purposes	O
,	O
these	O
are	O
the	O
ones	O
we	O
want	O
to	O
model	O
the	O
most	O
carefully	O
.	O
various	O
ad	O
hoc	O
heuristics	B
have	O
been	O
proposed	O
to	O
improve	O
the	O
performance	O
of	O
the	O
multinomial	B
document	O
classiﬁer	O
(	O
rennie	O
et	O
al	O
.	O
2003	O
)	O
.	O
we	O
now	O
present	O
an	O
alternative	O
class	O
conditional	O
density	O
that	O
performs	O
as	O
well	O
as	O
these	O
ad	O
hoc	O
methods	O
,	O
yet	O
is	O
probabilistically	O
sound	O
(	O
madsen	O
et	O
al	O
.	O
2005	O
)	O
.	O
3.	O
since	O
equation	O
3.78	O
models	O
each	O
word	O
independently	O
,	O
this	O
model	O
is	O
often	O
called	O
a	O
naive	O
bayes	O
classiﬁer	O
,	O
although	O
technically	O
the	O
features	B
xij	O
are	O
not	O
independent	O
,	O
because	O
of	O
the	O
constraint	O
j	O
xij	O
=	O
ni	O
.	O
(	O
cid:2	O
)	O
3.5.	O
naive	O
bayes	O
classiﬁers	O
89	O
suppose	O
we	O
simply	O
replace	O
the	O
multinomial	B
class	O
conditional	O
density	O
with	O
the	O
dirichlet	O
compound	O
multinomial	B
or	O
dcm	O
density	O
,	O
deﬁned	O
as	O
follows	O
:	O
mu	O
(	O
xi|ni	O
,	O
θc	O
)	O
dir	O
(	O
θc|αc	O
)	O
dθc	O
=	O
p	O
(	O
xi|yi	O
=	O
c	O
,	O
α	O
)	O
=	O
(	O
cid:11	O
)	O
ni	O
!	O
(	O
cid:15	O
)	O
d	O
j=1	O
xij	O
!	O
b	O
(	O
xi	O
+	O
αc	O
)	O
b	O
(	O
αc	O
)	O
(	O
3.79	O
)	O
(	O
this	O
equation	O
is	O
derived	O
in	O
equation	O
5.24	O
.	O
)	O
surprisingly	O
this	O
simple	O
change	O
is	O
all	O
that	O
is	O
needed	O
to	O
capture	O
the	O
burstiness	B
phenomenon	O
.	O
the	O
intuitive	O
reason	O
for	O
this	O
is	O
as	O
follows	O
:	O
after	O
seeing	O
one	O
occurence	O
of	O
a	O
word	O
,	O
say	O
word	O
j	O
,	O
the	O
posterior	O
counts	O
on	O
θj	O
gets	O
updated	O
,	O
making	O
another	O
occurence	O
of	O
word	O
j	O
more	O
likely	O
.	O
by	O
contrast	O
,	O
if	O
θj	O
is	O
ﬁxed	O
,	O
then	O
the	O
occurences	O
of	O
each	O
word	O
are	O
independent	O
.	O
the	O
multinomial	B
model	O
corresponds	O
to	O
drawing	O
a	O
ball	O
from	O
an	O
urn	O
with	O
k	O
colors	O
of	O
ball	O
,	O
recording	O
its	O
color	O
,	O
and	O
then	O
replacing	O
it	O
.	O
by	O
contrast	O
,	O
the	O
dcm	O
model	O
corresponds	O
to	O
drawing	O
a	O
ball	O
,	O
recording	O
its	O
color	O
,	O
and	O
then	O
replacing	O
it	O
with	O
one	O
additional	O
copy	O
;	O
this	O
is	O
called	O
the	O
polya	O
urn	O
.	O
using	O
the	O
dcm	O
as	O
the	O
class	O
conditional	O
density	O
gives	O
much	O
better	O
results	O
than	O
using	O
the	O
multinomial	B
,	O
and	O
has	O
performance	O
comparable	O
to	O
state	B
of	O
the	O
art	O
methods	O
,	O
as	O
described	O
in	O
(	O
madsen	O
et	O
al	O
.	O
2005	O
)	O
.	O
the	O
only	O
disadvantage	O
is	O
that	O
ﬁtting	O
the	O
dcm	O
model	O
is	O
more	O
complex	O
;	O
see	O
(	O
minka	O
2000e	O
;	O
elkan	O
2006	O
)	O
for	O
the	O
details	O
.	O
exercises	O
exercise	O
3.1	O
mle	O
for	O
the	O
bernoulli/	O
binomial	B
model	O
derive	O
equation	O
3.22	O
by	O
optimizing	O
the	O
log	O
of	O
the	O
likelihood	B
in	O
equation	O
3.11.	O
exercise	O
3.2	O
marginal	B
likelihood	I
for	O
the	O
beta-bernoulli	O
model	O
in	O
equation	O
5.23	O
,	O
we	O
showed	O
that	O
the	O
marginal	B
likelihood	I
is	O
the	O
ratio	O
of	O
the	O
normalizing	O
constants	O
:	O
p	O
(	O
d	O
)	O
=	O
z	O
(	O
α1	O
+	O
n1	O
,	O
α0	O
+	O
n0	O
)	O
z	O
(	O
α1	O
,	O
α0	O
)	O
=	O
γ	O
(	O
α1	O
+	O
n1	O
)	O
γ	O
(	O
α0	O
+	O
n0	O
)	O
γ	O
(	O
α1	O
+	O
α0	O
+	O
n	O
)	O
γ	O
(	O
α1	O
+	O
α0	O
)	O
γ	O
(	O
α1	O
)	O
γ	O
(	O
α0	O
)	O
we	O
will	O
now	O
derive	O
an	O
alternative	O
derivation	O
of	O
this	O
fact	O
.	O
by	O
the	O
chain	B
rule	I
of	O
probability	O
,	O
p	O
(	O
x1	O
:	O
n	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
x3|x1:2	O
)	O
.	O
.	O
.	O
(	O
3.80	O
)	O
(	O
3.81	O
)	O
in	O
section	O
3.3.4	O
,	O
we	O
showed	O
that	O
the	O
posterior	B
predictive	I
distribution	I
is	O
p	O
(	O
x	O
=	O
k|d1	O
:	O
n	O
)	O
=	O
nk	O
+	O
αk	O
i	O
ni	O
+	O
αi	O
(	O
cid:2	O
)	O
nk	O
+	O
αk	O
n	O
+	O
α	O
(	O
3.82	O
)	O
where	O
k	O
∈	O
{	O
0	O
,	O
1	O
}	O
and	O
d1	O
:	O
n	O
is	O
the	O
data	O
seen	O
so	O
far	O
.	O
now	O
suppose	O
d	O
=	O
h	O
,	O
t	O
,	O
t	O
,	O
h	O
,	O
h	O
or	O
d	O
=	O
1	O
,	O
0	O
,	O
0	O
,	O
1	O
,	O
1.	O
then	O
p	O
(	O
d	O
)	O
=	O
=	O
=	O
·	O
α0	O
+	O
1	O
α	O
+	O
2	O
·	O
α0	O
·	O
α1	O
+	O
1	O
α	O
+	O
1	O
α	O
+	O
3	O
α	O
(	O
α	O
+	O
1	O
)	O
···	O
(	O
α	O
+	O
4	O
)	O
α1	O
α	O
[	O
α1	O
(	O
α1	O
+	O
1	O
)	O
(	O
α1	O
+	O
2	O
)	O
]	O
[	O
α0	O
(	O
α0	O
+	O
1	O
)	O
]	O
[	O
(	O
α1	O
)	O
···	O
(	O
α1	O
+	O
n1	O
−	O
1	O
)	O
]	O
[	O
(	O
α0	O
)	O
···	O
(	O
α0	O
+	O
n0	O
−	O
1	O
)	O
]	O
·	O
α1	O
+	O
2	O
α	O
+	O
4	O
(	O
α	O
)	O
···	O
(	O
α	O
+	O
n	O
−	O
1	O
)	O
show	O
how	O
this	O
reduces	O
to	O
equation	O
3.80	O
by	O
using	O
the	O
fact	O
that	O
,	O
for	O
integers	O
,	O
(	O
α	O
−	O
1	O
)	O
!	O
=	O
γ	O
(	O
α	O
)	O
.	O
(	O
3.83	O
)	O
(	O
3.84	O
)	O
(	O
3.85	O
)	O
(	O
cid:2	O
)	O
90	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
exercise	O
3.3	O
posterior	O
predictive	O
for	O
beta-binomial	B
model	O
recall	B
from	O
equation	O
3.32	O
that	O
the	O
posterior	O
predictive	O
for	O
the	O
beta-binomial	B
is	O
given	O
by	O
p	O
(	O
x|n	O
,	O
d	O
)	O
=bb	O
=	O
(	O
x|α	O
(	O
cid:2	O
)	O
b	O
(	O
x	O
+	O
α	O
(	O
cid:2	O
)	O
0	O
,	O
α	O
(	O
cid:2	O
)	O
1	O
,	O
n	O
)	O
1	O
,	O
n	O
−	O
x	O
+	O
α	O
(	O
cid:2	O
)	O
0	O
)	O
1	O
,	O
α	O
(	O
cid:2	O
)	O
b	O
(	O
α	O
(	O
cid:2	O
)	O
0	O
)	O
n	O
x	O
(	O
cid:3	O
)	O
(	O
cid:4	O
)	O
prove	O
that	O
this	O
reduces	O
to	O
α	O
(	O
cid:2	O
)	O
α	O
(	O
cid:2	O
)	O
0	O
+	O
α	O
(	O
cid:2	O
)	O
p	O
(	O
˜x	O
=	O
1|d	O
)	O
=	O
1	O
1	O
when	O
n	O
=	O
1	O
(	O
and	O
hence	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
)	O
.	O
i.e.	O
,	O
show	O
that	O
bb	O
(	O
1|α	O
(	O
cid:2	O
)	O
1	O
,	O
α	O
(	O
cid:2	O
)	O
0	O
,	O
1	O
)	O
=	O
α	O
(	O
cid:2	O
)	O
α	O
(	O
cid:2	O
)	O
1	O
+	O
α	O
(	O
cid:2	O
)	O
1	O
0	O
hint	O
:	O
use	O
the	O
fact	O
that	O
γ	O
(	O
α0	O
+	O
α1	O
+	O
1	O
)	O
=	O
(	O
α0	O
+	O
α1	O
+	O
1	O
)	O
γ	O
(	O
α0	O
+	O
α1	O
)	O
(	O
3.86	O
)	O
(	O
3.87	O
)	O
(	O
3.88	O
)	O
(	O
3.89	O
)	O
(	O
3.90	O
)	O
(	O
3.91	O
)	O
exercise	O
3.4	O
beta	O
updating	O
from	O
censored	O
likelihood	O
(	O
source	O
:	O
gelman	O
.	O
)	O
suppose	O
we	O
toss	O
a	O
coin	O
n	O
=	O
5	O
times	O
.	O
let	O
x	O
be	O
the	O
number	O
of	O
heads	O
.	O
we	O
observe	O
that	O
there	O
are	O
fewer	O
than	O
3	O
heads	O
,	O
but	O
we	O
don	O
’	O
t	O
know	O
exactly	O
how	O
many	O
.	O
let	O
the	O
prior	O
probability	O
of	O
heads	O
be	O
p	O
(	O
θ	O
)	O
=	O
beta	O
(	O
θ|1	O
,	O
1	O
)	O
.	O
compute	O
the	O
posterior	O
p	O
(	O
θ|x	O
<	O
3	O
)	O
up	O
to	O
normalization	O
constants	O
,	O
i.e.	O
,	O
derive	O
an	O
expression	O
proportional	O
to	O
p	O
(	O
θ	O
,	O
x	O
<	O
3	O
)	O
.	O
hint	O
:	O
the	O
answer	O
is	O
a	O
mixture	B
distribution	O
.	O
exercise	O
3.5	O
uninformative	B
prior	O
for	O
log-odds	B
ratio	I
let	O
φ	O
=	O
logit	B
(	O
θ	O
)	O
=	O
log	O
θ	O
1	O
−	O
θ	O
show	O
that	O
if	O
p	O
(	O
φ	O
)	O
∝	O
1	O
,	O
then	O
p	O
(	O
θ	O
)	O
∝	O
beta	O
(	O
θ|0	O
,	O
0	O
)	O
.	O
hint	O
:	O
use	O
the	O
change	B
of	I
variables	I
formula	O
.	O
exercise	O
3.6	O
mle	O
for	O
the	O
poisson	O
distribution	O
the	O
poisson	O
pmf	O
is	O
deﬁned	O
as	O
poi	O
(	O
x|λ	O
)	O
=e	O
−λ	O
λx	O
parameter	B
.	O
derive	O
the	O
mle	O
.	O
x	O
!	O
,	O
for	O
x	O
∈	O
{	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
}	O
where	O
λ	O
>	O
0	O
is	O
the	O
rate	B
exercise	O
3.7	O
bayesian	O
analysis	O
of	O
the	O
poisson	O
distribution	O
in	O
exercise	O
3.6	O
,	O
we	O
deﬁned	O
the	O
poisson	O
distribution	O
with	O
rate	B
λ	O
and	O
derived	O
its	O
mle	O
.	O
here	O
we	O
perform	O
a	O
conjugate	O
bayesian	O
analysis	O
.	O
a.	O
derive	O
the	O
posterior	O
p	O
(	O
λ|d	O
)	O
assuming	O
a	O
conjugate	B
prior	I
p	O
(	O
λ	O
)	O
=	O
ga	O
(	O
λ|a	O
,	O
b	O
)	O
∝	O
λa−1e−λb	O
.	O
hint	O
:	O
the	O
b.	O
what	O
does	O
the	O
posterior	B
mean	I
tend	O
to	O
as	O
a	O
→	O
0	O
and	O
b	O
→	O
0	O
?	O
(	O
recall	B
that	O
the	O
mean	B
of	O
a	O
ga	O
(	O
a	O
,	O
b	O
)	O
posterior	O
is	O
also	O
a	O
gamma	B
distribution	I
.	O
distribution	O
is	O
a/b	O
.	O
)	O
exercise	O
3.8	O
mle	O
for	O
the	O
uniform	B
distribution	I
(	O
source	O
:	O
kaelbling	O
.	O
)	O
consider	O
a	O
uniform	B
distribution	I
centered	O
on	O
0	O
with	O
width	O
2a	O
.	O
the	O
density	O
function	O
is	O
given	O
by	O
p	O
(	O
x	O
)	O
=	O
1	O
2a	O
i	O
(	O
x	O
∈	O
[	O
−a	O
,	O
a	O
]	O
)	O
(	O
3.92	O
)	O
3.5.	O
naive	O
bayes	O
classiﬁers	O
91	O
a.	O
given	O
a	O
data	O
set	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
what	O
is	O
the	O
maximum	B
likelihood	I
estimate	I
of	O
a	O
(	O
call	O
it	O
ˆa	O
)	O
?	O
b.	O
what	O
probability	O
would	O
the	O
model	O
assign	O
to	O
a	O
new	O
data	O
point	O
xn+1	O
using	O
ˆa	O
?	O
c.	O
do	O
you	O
see	O
any	O
problem	O
with	O
the	O
above	O
approach	O
?	O
brieﬂy	O
suggest	O
(	O
in	O
words	O
)	O
a	O
better	O
approach	O
.	O
exercise	O
3.9	O
bayesian	O
analysis	O
of	O
the	O
uniform	B
distribution	I
consider	O
the	O
uniform	B
distribution	I
unif	O
(	O
0	O
,	O
θ	O
)	O
.	O
the	O
maximum	B
likelihood	I
estimate	I
is	O
ˆθ	O
=	O
max	O
(	O
d	O
)	O
,	O
as	O
we	O
saw	O
in	O
exercise	O
3.8	O
,	O
but	O
this	O
is	O
unsuitable	O
for	O
predicting	O
future	O
data	O
since	O
it	O
puts	O
zero	O
probability	O
mass	O
outside	O
the	O
training	O
data	O
.	O
in	O
this	O
exercise	O
,	O
we	O
will	O
perform	O
a	O
bayesian	O
analysis	O
of	O
the	O
uniform	B
distribution	I
(	O
following	O
(	O
minka	O
2001a	O
)	O
)	O
.	O
the	O
conjugate	B
prior	I
is	O
the	O
pareto	O
distribution	O
,	O
p	O
(	O
θ	O
)	O
=	O
pareto	O
(	O
θ|b	O
,	O
k	O
)	O
,	O
deﬁned	O
in	O
section	O
2.4.6.	O
given	O
a	O
pareto	O
prior	O
,	O
the	O
joint	B
distribution	I
of	O
θ	O
and	O
d	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
is	O
p	O
(	O
d	O
,	O
θ	O
)	O
=	O
kbk	O
i	O
(	O
θ	O
≥	O
max	O
(	O
d	O
)	O
)	O
θn	O
+k+1	O
(	O
3.93	O
)	O
let	O
m	O
=	O
max	O
(	O
d	O
)	O
.	O
the	O
evidence	B
(	O
the	O
probability	O
that	O
all	O
n	O
samples	B
came	O
from	O
the	O
same	O
uniform	B
distribution	I
)	O
is	O
p	O
(	O
d	O
)	O
=	O
kbk	O
dθ	O
(	O
3.94	O
)	O
if	O
m	O
≤	O
b	O
if	O
m	O
>	O
b	O
(	O
3.95	O
)	O
(	O
cid:5	O
)	O
∞	O
(	O
cid:6	O
)	O
m	O
θn	O
+k+1	O
k	O
=	O
(	O
n	O
+k	O
)	O
bn	O
kbk	O
(	O
n	O
+k	O
)	O
mn	O
+k	O
derive	O
the	O
posterior	O
p	O
(	O
θ|d	O
)	O
,	O
and	O
show	O
that	O
if	O
can	O
be	O
expressed	O
as	O
a	O
pareto	O
distribution	O
.	O
exercise	O
3.10	O
taxicab	O
(	O
tramcar	O
)	O
problem	O
suppose	O
you	O
arrive	O
in	O
a	O
new	O
city	O
and	O
see	O
a	O
taxi	O
numbered	O
100.	O
how	O
many	O
taxis	O
are	O
there	O
in	O
this	O
city	O
?	O
let	O
us	O
assume	O
taxis	O
are	O
numbered	O
sequentially	O
as	O
integers	O
starting	O
from	O
0	O
,	O
up	O
to	O
some	O
unknown	B
upper	O
bound	O
θ	O
.	O
(	O
we	O
number	O
taxis	O
from	O
0	O
for	O
simplicity	O
;	O
we	O
can	O
also	O
count	O
from	O
1	O
without	O
changing	O
the	O
analysis	O
.	O
)	O
hence	O
the	O
likelihood	B
function	O
is	O
p	O
(	O
x	O
)	O
=u	O
(	O
0	O
,	O
θ	O
)	O
,	O
the	O
uniform	B
distribution	I
.	O
the	O
goal	O
is	O
to	O
estimate	O
θ.	O
we	O
will	O
use	O
the	O
bayesian	O
analysis	O
from	O
exercise	O
3.9.	O
a.	O
suppose	O
we	O
see	O
one	O
taxi	O
numbered	O
100	O
,	O
so	O
d	O
=	O
{	O
100	O
}	O
,	O
m	O
=	O
100	O
,	O
n	O
=	O
1.	O
using	O
an	O
(	O
improper	O
)	O
non-informative	B
prior	O
on	O
θ	O
of	O
the	O
form	O
p	O
(	O
θ	O
)	O
=	O
p	O
a	O
(	O
θ|0	O
,	O
0	O
)	O
∝	O
1/θ	O
,	O
what	O
is	O
the	O
posterior	O
p	O
(	O
θ|d	O
)	O
?	O
b.	O
compute	O
the	O
posterior	B
mean	I
,	O
mode	B
and	O
median	B
number	O
of	O
taxis	O
in	O
the	O
city	O
,	O
if	O
such	O
quantities	O
exist	O
.	O
c.	O
rather	O
than	O
trying	O
to	O
compute	O
a	O
point	B
estimate	I
of	O
the	O
number	O
of	O
taxis	O
,	O
we	O
can	O
compute	O
the	O
predictive	B
density	O
over	O
the	O
next	O
taxicab	O
number	O
using	O
p	O
(	O
d	O
(	O
cid:2	O
)	O
|d	O
,	O
α	O
)	O
=	O
p	O
(	O
d	O
(	O
cid:2	O
)	O
|θ	O
)	O
p	O
(	O
θ|d	O
,	O
α	O
)	O
dθ	O
=	O
p	O
(	O
d	O
(	O
cid:2	O
)	O
|β	O
)	O
(	O
3.96	O
)	O
(	O
cid:5	O
)	O
where	O
α	O
=	O
(	O
b	O
,	O
k	O
)	O
are	O
the	O
hyper-parameters	B
,	O
β	O
=	O
(	O
c	O
,	O
n	O
+	O
k	O
)	O
are	O
the	O
updated	O
hyper-parameters	B
.	O
now	O
consider	O
the	O
case	O
d	O
=	O
{	O
m	O
}	O
,	O
and	O
d	O
(	O
cid:2	O
)	O
=	O
{	O
x	O
}	O
.	O
using	O
equation	O
3.95	O
,	O
write	O
down	O
an	O
expression	O
for	O
p	O
(	O
x|d	O
,	O
α	O
)	O
(	O
3.97	O
)	O
as	O
above	O
,	O
use	O
a	O
non-informative	B
prior	O
b	O
=	O
k	O
=	O
0.	O
d.	O
use	O
the	O
predictive	B
density	O
formula	O
to	O
compute	O
the	O
probability	O
that	O
the	O
next	O
taxi	O
you	O
will	O
see	O
(	O
say	O
,	O
the	O
next	O
day	O
)	O
has	O
number	O
100	O
,	O
50	O
or	O
150	O
,	O
i.e.	O
,	O
compute	O
p	O
(	O
x	O
=	O
100|d	O
,	O
α	O
)	O
,	O
p	O
(	O
x	O
=	O
50|d	O
,	O
α	O
)	O
,	O
p	O
(	O
x	O
=	O
150|d	O
,	O
α	O
)	O
.	O
e.	O
brieﬂy	O
describe	O
(	O
1-2	O
sentences	O
)	O
some	O
ways	O
we	O
might	O
make	O
the	O
model	O
more	O
accurate	O
at	O
prediction	O
.	O
92	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
exercise	O
3.11	O
bayesian	O
analysis	O
of	O
the	O
exponential	O
distribution	O
a	O
lifetime	O
x	O
of	O
a	O
machine	O
is	O
modeled	O
by	O
an	O
exponential	O
distribution	O
with	O
unknown	B
parameter	O
θ.	O
the	O
likelihood	B
is	O
p	O
(	O
x|θ	O
)	O
=	O
θe−θx	O
for	O
x	O
≥	O
0	O
,	O
θ	O
>	O
0.	O
a.	O
show	O
that	O
the	O
mle	O
is	O
ˆθ	O
=	O
1/x	O
,	O
where	O
x	O
=	O
1	O
n	O
b.	O
suppose	O
we	O
observe	O
x1	O
=	O
5	O
,	O
x2	O
=	O
6	O
,	O
x3	O
=	O
4	O
(	O
the	O
lifetimes	O
(	O
in	O
years	O
)	O
of	O
3	O
different	O
iid	B
machines	O
)	O
.	O
(	O
cid:2	O
)	O
n	O
i=1	O
xi	O
.	O
what	O
is	O
the	O
mle	O
given	O
this	O
data	O
?	O
c.	O
assume	O
that	O
an	O
expert	O
believes	O
θ	O
should	O
have	O
a	O
prior	O
distribution	O
that	O
is	O
also	O
exponential	O
p	O
(	O
θ	O
)	O
=	O
expon	O
(	O
θ|λ	O
)	O
(	O
3.98	O
)	O
choose	O
the	O
prior	O
parameter	B
,	O
call	O
it	O
ˆλ	O
,	O
such	O
that	O
e	O
[	O
θ	O
]	O
=	O
1/3	O
.	O
hint	O
:	O
recall	B
that	O
the	O
gamma	B
distribution	I
has	O
the	O
form	O
ga	O
(	O
θ|a	O
,	O
b	O
)	O
∝	O
θa−1e−θb	O
and	O
its	O
mean	B
is	O
a/b	O
.	O
d.	O
what	O
is	O
the	O
posterior	O
,	O
p	O
(	O
θ|d	O
,	O
ˆλ	O
)	O
?	O
(	O
cid:7	O
)	O
e.	O
f.	O
what	O
is	O
the	O
posterior	B
mean	I
,	O
e	O
(	O
cid:8	O
)	O
?	O
θ|d	O
,	O
ˆλ	O
is	O
the	O
exponential	O
prior	O
conjugate	O
to	O
the	O
exponential	O
likelihood	O
?	O
g.	O
explain	O
why	O
the	O
mle	O
and	O
posterior	B
mean	I
differ	O
.	O
which	O
is	O
more	O
reasonable	O
in	O
this	O
example	O
?	O
exercise	O
3.12	O
map	O
estimation	O
for	O
the	O
bernoulli	O
with	O
non-conjugate	O
priors	O
(	O
source	O
:	O
prior	O
p	O
(	O
θ	O
)	O
=	O
beta	O
(	O
θ|α	O
,	O
β	O
)	O
.	O
we	O
know	O
that	O
,	O
with	O
this	O
prior	O
,	O
the	O
map	O
estimate	O
is	O
given	O
by	O
in	O
the	O
book	O
,	O
we	O
discussed	O
bayesian	O
inference	B
of	O
a	O
bernoulli	O
rate	B
parameter	O
with	O
the	O
jaakkola	O
.	O
)	O
(	O
3.99	O
)	O
(	O
3.100	O
)	O
ˆθ	O
=	O
n1	O
+	O
α	O
−	O
1	O
n	O
+	O
α	O
+	O
β	O
−	O
2	O
⎧⎨	O
⎩	O
0.5	O
0.5	O
0	O
p	O
(	O
θ	O
)	O
=	O
where	O
n1	O
is	O
the	O
number	O
of	O
heads	O
,	O
n0	O
is	O
the	O
number	O
of	O
tails	O
,	O
and	O
n	O
=	O
n0	O
+	O
n1	O
is	O
the	O
total	O
number	O
of	O
trials	O
.	O
a.	O
now	O
consider	O
the	O
following	O
prior	O
,	O
that	O
believes	O
the	O
coin	O
is	O
fair	O
,	O
or	O
is	O
slightly	O
biased	O
towards	O
tails	O
:	O
if	O
θ	O
=	O
0.5	O
if	O
θ	O
=	O
0.4	O
otherwise	O
(	O
3.101	O
)	O
derive	O
the	O
map	O
estimate	O
under	O
this	O
prior	O
as	O
a	O
function	O
of	O
n1	O
and	O
n	O
.	O
b.	O
suppose	O
the	O
true	O
parameter	O
is	O
θ	O
=	O
0.41.	O
which	O
prior	O
leads	O
to	O
a	O
better	O
estimate	O
when	O
n	O
is	O
small	O
?	O
which	O
prior	O
leads	O
to	O
a	O
better	O
estimate	O
when	O
n	O
is	O
large	O
?	O
exercise	O
3.13	O
posterior	B
predictive	I
distribution	I
for	O
a	O
batch	B
of	O
data	O
with	O
the	O
dirichlet-multinomial	O
model	O
in	O
equation	O
3.51	O
,	O
we	O
gave	O
the	O
the	O
posterior	B
predictive	I
distribution	I
for	O
a	O
single	O
multinomial	O
trial	O
using	O
a	O
dirichlet	O
prior	O
.	O
now	O
consider	O
predicting	O
a	O
batch	B
of	O
new	O
data	O
,	O
˜d	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
,	O
consisting	O
of	O
m	O
single	O
multinomial	O
trials	O
(	O
think	O
of	O
predicting	O
the	O
next	O
m	O
words	O
in	O
a	O
sentence	O
,	O
assuming	O
they	O
are	O
drawn	O
iid	B
)	O
.	O
derive	O
an	O
expression	O
for	O
p	O
(	O
˜d|d	O
,	O
α	O
)	O
(	O
3.102	O
)	O
3.5.	O
naive	O
bayes	O
classiﬁers	O
93	O
your	O
answer	O
should	O
be	O
a	O
function	O
of	O
α	O
,	O
and	O
the	O
old	O
and	O
new	O
counts	O
(	O
sufficient	B
statistics	I
)	O
,	O
deﬁned	O
as	O
n	O
old	O
k	O
n	O
new	O
k	O
=	O
=	O
i	O
(	O
xi	O
=	O
k	O
)	O
i	O
(	O
xi	O
=	O
k	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
i∈d	O
i∈	O
˜d	O
(	O
3.103	O
)	O
(	O
3.104	O
)	O
(	O
3.105	O
)	O
hint	O
:	O
recall	B
that	O
,	O
for	O
a	O
vector	O
of	O
counts	O
,	O
n1	O
:	O
k	O
,	O
the	O
marginal	B
likelihood	I
(	O
evidence	B
)	O
is	O
given	O
by	O
p	O
(	O
d|α	O
)	O
=	O
(	O
cid:2	O
)	O
γ	O
(	O
α	O
)	O
γ	O
(	O
n	O
+	O
α	O
)	O
k	O
where	O
α	O
=	O
k	O
αk	O
and	O
n	O
=	O
(	O
cid:13	O
)	O
γ	O
(	O
nk	O
+	O
αk	O
)	O
(	O
cid:2	O
)	O
γ	O
(	O
αk	O
)	O
k	O
nk	O
.	O
exercise	O
3.14	O
posterior	O
predictive	O
for	O
dirichlet-multinomial	O
(	O
source	O
:	O
koller.	O
)	O
.	O
a.	O
suppose	O
we	O
compute	O
the	O
empirical	B
distribution	I
over	O
letters	O
of	O
the	O
roman	O
alphabet	O
plus	O
the	O
space	O
character	O
(	O
a	O
distribution	O
over	O
27	O
values	O
)	O
from	O
2000	O
samples	B
.	O
suppose	O
we	O
see	O
the	O
letter	O
“	O
e	O
”	O
260	O
times	O
.	O
what	O
is	O
p	O
(	O
x2001	O
=	O
e|d	O
)	O
,	O
if	O
we	O
assume	O
θ	O
∼	O
dir	O
(	O
α1	O
,	O
.	O
.	O
.	O
,	O
α27	O
)	O
,	O
whereα	O
k	O
=	O
10	O
for	O
all	O
k	O
?	O
in	O
the	O
2000	O
samples	B
,	O
we	O
saw	O
“	O
e	O
”	O
260	O
times	O
,	O
“	O
a	O
”	O
100	O
times	O
,	O
and	O
“	O
p	O
”	O
87	O
times	O
.	O
what	O
is	O
p	O
(	O
x2001	O
=	O
p	O
,	O
x2002	O
=	O
a|d	O
)	O
,	O
if	O
we	O
assume	O
θ	O
∼	O
dir	O
(	O
α1	O
,	O
.	O
.	O
.	O
,	O
α27	O
)	O
,	O
where	O
αk	O
=	O
10	O
for	O
all	O
k	O
?	O
show	O
your	O
work	O
.	O
b.	O
suppose	O
,	O
exercise	O
3.15	O
setting	O
the	O
beta	O
hyper-parameters	O
suppose	O
θ	O
∼	O
β	O
(	O
α1	O
,	O
α2	O
)	O
and	O
we	O
believe	O
that	O
e	O
[	O
θ	O
]	O
=	O
m	O
and	O
var	O
[	O
θ	O
]	O
=	O
v.	O
using	O
equation	O
2.62	O
,	O
solve	O
for	O
α1	O
and	O
α2	O
in	O
terms	O
of	O
m	O
and	O
v.	O
what	O
values	O
do	O
you	O
get	O
if	O
m	O
=	O
0.7	O
and	O
v	O
=	O
0.22	O
?	O
exercise	O
3.16	O
setting	O
the	O
beta	O
hyper-parameters	O
ii	O
(	O
source	O
:	O
draper	O
.	O
)	O
suppose	O
θ	O
∼	O
β	O
(	O
α1	O
,	O
α2	O
)	O
and	O
we	O
believe	O
that	O
e	O
[	O
θ	O
]	O
=m	O
and	O
p	O
(	O
(	O
cid:7	O
)	O
<	O
θ	O
<	O
u	O
)	O
=	O
0.95.	O
write	O
a	O
program	O
that	O
can	O
solve	O
for	O
α1	O
and	O
α2	O
in	O
terms	O
of	O
m	O
,	O
(	O
cid:7	O
)	O
and	O
u.	O
hint	O
:	O
write	O
α2	O
as	O
a	O
function	O
of	O
α1	O
and	O
m	O
,	O
so	O
the	O
pdf	B
only	O
has	O
one	O
unknown	B
;	O
then	O
write	O
down	O
the	O
probability	O
mass	O
contained	O
in	O
the	O
interval	O
as	O
an	O
integral	O
,	O
and	O
minimize	O
its	O
squared	O
discrepancy	O
from	O
0.95.	O
what	O
values	O
do	O
you	O
get	O
if	O
m	O
=	O
0.15	O
,	O
(	O
cid:7	O
)	O
=	O
0.05	O
and	O
u	O
=	O
0.3	O
?	O
what	O
is	O
the	O
equivalent	B
sample	I
size	I
of	O
this	O
prior	O
?	O
exercise	O
3.17	O
marginal	B
likelihood	I
for	O
beta-binomial	B
under	O
uniform	O
prior	O
suppose	O
we	O
toss	O
a	O
coin	O
n	O
times	O
and	O
observe	O
n1	O
heads	O
.	O
let	O
n1	O
∼	O
bin	O
(	O
n	O
,	O
θ	O
)	O
and	O
θ	O
∼	O
beta	O
(	O
1	O
,	O
1	O
)	O
.	O
show	O
that	O
the	O
marginal	B
likelihood	I
is	O
p	O
(	O
n1|n	O
)	O
=	O
1/	O
(	O
n	O
+	O
1	O
)	O
.	O
hint	O
:	O
γ	O
(	O
x	O
+	O
1	O
)	O
=	O
x	O
!	O
if	O
x	O
is	O
an	O
integer	O
.	O
exercise	O
3.18	O
bayes	O
factor	B
for	O
coin	O
tossing	O
suppose	O
we	O
toss	O
a	O
coin	O
n	O
=	O
10	O
times	O
and	O
observe	O
n1	O
=	O
9	O
heads	O
.	O
let	O
the	O
null	B
hypothesis	I
be	O
that	O
the	O
coin	O
is	O
fair	O
,	O
and	O
the	O
alternative	O
be	O
that	O
the	O
coin	O
can	O
have	O
any	O
bias	B
,	O
so	O
p	O
(	O
θ	O
)	O
=	O
unif	O
(	O
0	O
,	O
1	O
)	O
.	O
derive	O
the	O
bayes	O
factor	B
bf1,0	O
in	O
favor	O
of	O
the	O
biased	O
coin	O
hypothesis	O
.	O
what	O
if	O
n	O
=	O
100	O
and	O
n1	O
=	O
90	O
?	O
hint	O
:	O
see	O
exercise	O
3.17.	O
exercise	O
3.19	O
irrelevant	O
features	B
with	O
naive	O
bayes	O
(	O
source	O
:	O
jaakkola	O
.	O
)	O
let	O
xiw	O
=	O
1	O
if	O
word	O
w	O
occurs	O
in	O
document	O
i	O
and	O
xiw	O
=	O
0	O
otherwise	O
.	O
let	O
θcw	O
be	O
the	O
estimated	O
probability	O
that	O
word	O
w	O
occurs	O
in	O
documents	O
of	O
class	O
c.	O
then	O
the	O
log-likelihood	O
that	O
document	O
94	O
chapter	O
3.	O
generative	O
models	O
for	O
discrete	B
data	O
x	O
belongs	O
to	O
class	O
c	O
is	O
log	O
p	O
(	O
xi|c	O
,	O
θ	O
)	O
=	O
log	O
w	O
(	O
cid:12	O
)	O
w	O
(	O
cid:12	O
)	O
=	O
w=1	O
w	O
(	O
cid:13	O
)	O
cw	O
(	O
1	O
−	O
θcw	O
)	O
1−xiw	O
θxiw	O
w=1	O
xiw	O
log	O
θcw	O
+	O
(	O
1	O
−	O
xiw	O
)	O
log	O
(	O
1	O
−	O
θcw	O
)	O
=	O
w=1	O
xiw	O
log	O
θcw	O
1	O
−	O
θcw	O
+	O
log	O
(	O
1	O
−	O
θcw	O
)	O
(	O
cid:12	O
)	O
w	O
where	O
w	O
is	O
the	O
number	O
of	O
words	O
in	O
the	O
vocabulary	O
.	O
we	O
can	O
write	O
this	O
more	O
succintly	O
as	O
log	O
p	O
(	O
xi|c	O
,	O
θ	O
)	O
=φ	O
(	O
xi	O
)	O
t	O
βc	O
where	O
xi	O
=	O
(	O
xi1	O
,	O
.	O
.	O
.	O
,	O
xiw	O
)	O
is	O
a	O
bit	O
vector	O
,	O
φ	O
(	O
xi	O
)	O
=	O
(	O
xi	O
,	O
1	O
)	O
,	O
and	O
βc	O
=	O
(	O
log	O
θc1	O
1	O
−	O
θc1	O
,	O
.	O
.	O
.	O
,	O
log	O
θcw	O
1	O
−	O
θcw	O
,	O
log	O
(	O
1	O
−	O
θcw	O
)	O
)	O
t	O
(	O
cid:12	O
)	O
w	O
(	O
3.106	O
)	O
(	O
3.107	O
)	O
(	O
3.108	O
)	O
(	O
3.109	O
)	O
(	O
3.110	O
)	O
(	O
3.111	O
)	O
we	O
see	O
that	O
this	O
is	O
a	O
linear	O
classiﬁer	O
,	O
since	O
the	O
class-conditional	B
density	I
is	O
a	O
linear	O
function	O
(	O
an	O
inner	O
product	O
)	O
of	O
the	O
parameters	O
βc	O
.	O
a.	O
assuming	O
p	O
(	O
c	O
=	O
1	O
)	O
=p	O
(	O
c	O
=	O
2	O
)	O
=	O
0.5	O
,	O
write	O
down	O
an	O
expression	O
for	O
the	O
log	O
posterior	O
odds	O
ratio	O
,	O
log2	O
p	O
(	O
c=1|xi	O
)	O
p	O
(	O
c=2|xi	O
)	O
,	O
in	O
terms	O
of	O
the	O
features	B
φ	O
(	O
xi	O
)	O
and	O
the	O
parameters	O
β1	O
and	O
β2	O
.	O
b.	O
intuitively	O
,	O
words	O
that	O
occur	O
in	O
both	O
classes	O
are	O
not	O
very	O
“	O
discriminative	B
”	O
,	O
and	O
therefore	O
should	O
not	O
affect	O
our	O
beliefs	O
about	O
the	O
class	O
label	O
.	O
consider	O
a	O
particular	O
word	O
w.	O
state	B
the	O
conditions	O
on	O
θ1	O
,	O
w	O
and	O
θ2	O
,	O
w	O
(	O
or	O
equivalently	O
the	O
conditions	O
on	O
β1	O
,	O
w	O
,	O
β2	O
,	O
w	O
)	O
under	O
which	O
the	O
presence	O
or	O
absence	O
of	O
w	O
in	O
a	O
test	O
document	O
will	O
have	O
no	O
effect	O
on	O
the	O
class	O
posterior	O
(	O
such	O
a	O
word	O
will	O
be	O
ignored	O
by	O
the	O
classiﬁer	O
)	O
.	O
hint	O
:	O
using	O
your	O
previous	O
result	O
,	O
ﬁgure	O
out	O
when	O
the	O
posterior	O
odds	O
ratio	O
is	O
0.5/0.5	O
.	O
c.	O
the	O
posterior	B
mean	I
estimate	O
of	O
θ	O
,	O
using	O
a	O
beta	O
(	O
1,1	O
)	O
prior	O
,	O
is	O
given	O
by	O
ˆθcw	O
=	O
1	O
+	O
i∈c	O
xiw	O
2	O
+n	O
c	O
where	O
the	O
sum	O
is	O
over	O
the	O
nc	O
documents	O
in	O
class	O
c.	O
consider	O
a	O
particular	O
word	O
w	O
,	O
and	O
suppose	O
it	O
always	O
occurs	O
in	O
every	O
document	O
(	O
regardless	O
of	O
class	O
)	O
.	O
let	O
there	O
be	O
n1	O
documents	O
of	O
class	O
1	O
and	O
n2	O
be	O
the	O
number	O
of	O
documents	O
in	O
class	O
2	O
,	O
where	O
n1	O
(	O
cid:8	O
)	O
=	O
n2	O
(	O
since	O
e.g.	O
,	O
we	O
get	O
much	O
more	O
non-spam	O
than	O
if	O
we	O
use	O
the	O
above	O
estimate	O
for	O
θcw	O
,	O
will	O
word	O
w	O
be	O
spam	B
;	O
this	O
is	O
an	O
example	O
of	O
class	B
imbalance	I
)	O
.	O
ignored	O
by	O
our	O
classiﬁer	O
?	O
explain	O
why	O
or	O
why	O
not	O
.	O
d.	O
what	O
other	O
ways	O
can	O
you	O
think	O
of	O
which	O
encourage	O
“	O
irrelevant	O
”	O
words	O
to	O
be	O
ignored	O
?	O
exercise	O
3.20	O
class	O
conditional	O
densities	O
for	O
binary	O
data	O
consider	O
a	O
generative	B
classiﬁer	I
for	O
c	O
classes	O
with	O
class	O
conditional	O
density	O
p	O
(	O
x|y	O
)	O
and	O
uniform	O
class	O
prior	O
p	O
(	O
y	O
)	O
.	O
suppose	O
all	O
the	O
d	O
features	B
are	O
binary	O
,	O
xj	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
if	O
we	O
assume	O
all	O
the	O
features	B
are	O
conditionally	B
independent	I
(	O
the	O
naive	O
bayes	O
assumption	O
)	O
,	O
we	O
can	O
write	O
p	O
(	O
x|y	O
=	O
c	O
)	O
=	O
ber	O
(	O
xj|θjc	O
)	O
(	O
3.112	O
)	O
j=1	O
this	O
requires	O
dc	O
parameters	O
.	O
(	O
cid:2	O
)	O
d	O
(	O
cid:13	O
)	O
3.5.	O
naive	O
bayes	O
classiﬁers	O
95	O
a.	O
now	O
consider	O
a	O
different	O
model	O
,	O
which	O
we	O
will	O
call	O
the	O
“	O
full	B
”	O
model	O
,	O
in	O
which	O
all	O
the	O
features	B
are	O
fully	O
dependent	O
(	O
i.e.	O
,	O
we	O
make	O
no	O
factorization	O
assumptions	O
)	O
.	O
how	O
might	O
we	O
represent	O
p	O
(	O
x|y	O
=	O
c	O
)	O
in	O
this	O
case	O
?	O
how	O
many	O
parameters	O
are	O
needed	O
to	O
represent	O
p	O
(	O
x|y	O
=	O
c	O
)	O
?	O
b.	O
assume	O
the	O
number	O
of	O
features	B
d	O
is	O
ﬁxed	O
.	O
let	O
there	O
be	O
n	O
training	O
cases	O
.	O
if	O
the	O
sample	O
size	O
n	O
is	O
very	O
small	O
,	O
which	O
model	O
(	O
naive	O
bayes	O
or	O
full	B
)	O
is	O
likely	O
to	O
give	O
lower	O
test	O
set	O
error	O
,	O
and	O
why	O
?	O
if	O
the	O
sample	O
size	O
n	O
is	O
very	O
large	O
,	O
which	O
model	O
(	O
naive	O
bayes	O
or	O
full	B
)	O
is	O
likely	O
to	O
give	O
lower	O
test	O
set	O
error	O
,	O
and	O
why	O
?	O
c.	O
d.	O
what	O
is	O
the	O
computational	O
complexity	O
of	O
ﬁtting	O
the	O
full	B
and	O
naive	O
bayes	O
models	O
as	O
a	O
function	O
of	O
n	O
(	O
fitting	O
the	O
model	O
here	O
means	O
computing	O
the	O
mle	O
or	O
map	O
parameter	B
and	O
d	O
?	O
use	O
big-oh	O
notation	O
.	O
estimates	O
.	O
you	O
may	O
assume	O
you	O
can	O
convert	O
a	O
d-bit	O
vector	O
to	O
an	O
array	O
index	O
in	O
o	O
(	O
d	O
)	O
time	O
.	O
)	O
e.	O
what	O
is	O
the	O
computational	O
complexity	O
of	O
applying	O
the	O
full	B
and	O
naive	O
bayes	O
models	O
at	O
test	O
time	O
to	O
a	O
single	O
test	O
case	O
?	O
f.	O
suppose	O
the	O
test	O
case	O
has	O
missing	B
data	I
.	O
let	O
xv	O
be	O
the	O
visible	B
features	O
of	O
size	O
v	O
,	O
and	O
xh	O
be	O
the	O
hidden	B
(	O
missing	B
)	O
features	B
of	O
size	O
h	O
,	O
where	O
v	O
+	O
h	O
=	O
d.	O
what	O
is	O
the	O
computational	O
complexity	O
of	O
computing	O
p	O
(	O
y|xv	O
,	O
ˆθ	O
)	O
for	O
the	O
full	B
and	O
naive	O
bayes	O
models	O
,	O
as	O
a	O
function	O
of	O
v	O
and	O
h	O
?	O
exercise	O
3.21	O
mutual	B
information	I
for	O
naive	O
bayes	O
classiﬁers	O
with	O
binary	O
features	O
derive	O
equation	O
3.76.	O
exercise	O
3.22	O
fitting	O
a	O
naive	O
bayes	O
spam	B
ﬁlter	O
by	O
hand	O
(	O
source	O
:	O
daphne	O
koller.	O
)	O
.	O
consider	O
a	O
naive	O
bayes	O
model	O
(	O
multivariate	O
bernoulli	O
version	O
)	O
for	O
spam	B
classiﬁca-	O
tion	O
with	O
the	O
vocabulary	O
v=	O
''	O
secret	O
''	O
,	O
``	O
offer	O
''	O
,	O
``	O
low	O
''	O
,	O
``	O
price	O
''	O
,	O
``	O
valued	O
''	O
,	O
``	O
customer	O
''	O
,	O
``	O
today	O
''	O
,	O
``	O
dollar	O
''	O
,	O
``	O
million	O
''	O
,	O
''	O
sports	O
''	O
,	O
``	O
is	O
''	O
,	O
``	O
for	O
''	O
,	O
``	O
play	O
''	O
,	O
``	O
healthy	O
''	O
,	O
``	O
pizza	O
''	O
.	O
we	O
have	O
the	O
following	O
example	O
spam	B
messages	O
``	O
million	O
dollar	O
offer	O
''	O
,	O
``	O
secret	O
offer	O
today	O
''	O
,	O
``	O
secret	O
is	O
secret	O
''	O
and	O
normal	B
messages	O
,	O
``	O
low	O
price	O
for	O
valued	O
customer	O
''	O
,	O
``	O
play	O
secret	O
sports	O
today	O
''	O
,	O
``	O
sports	O
is	O
healthy	O
''	O
,	O
``	O
low	O
price	O
pizza	O
''	O
.	O
give	O
the	O
mles	O
for	O
the	O
following	O
parameters	O
:	O
θspam	O
,	O
θ	O
secret|spam	O
,	O
θ	O
secret|non-spam	O
,	O
θ	O
sports|non-spam	O
,	O
θ	O
dollar|spam	O
.	O
4	O
gaussian	O
models	O
4.1	O
introduction	O
in	O
this	O
chapter	O
,	O
we	O
discuss	O
the	O
multivariate	O
gaussian	O
or	O
multivariate	B
normal	I
(	O
mvn	O
)	O
,	O
which	O
is	O
the	O
most	O
widely	O
used	O
joint	O
probability	O
density	O
function	O
for	O
continuous	O
variables	O
.	O
it	O
will	O
form	O
the	O
basis	O
for	O
many	O
of	O
the	O
models	O
we	O
will	O
encounter	O
in	O
later	O
chapters	O
.	O
unfortunately	O
,	O
the	O
level	O
of	O
mathematics	O
in	O
this	O
chapter	O
is	O
higher	O
than	O
in	O
many	O
other	O
chapters	O
.	O
in	O
particular	O
,	O
we	O
rely	O
heavily	O
on	O
linear	O
algebra	O
and	O
matrix	O
calculus	O
.	O
this	O
is	O
the	O
price	O
one	O
must	O
pay	O
in	O
order	O
to	O
deal	O
with	O
high-dimensional	O
data	O
.	O
beginners	O
may	O
choose	O
to	O
skip	O
sections	O
marked	O
with	O
a	O
*	O
.	O
in	O
addition	O
,	O
since	O
there	O
are	O
so	O
many	O
equations	O
in	O
this	O
chapter	O
,	O
we	O
have	O
put	O
a	O
box	O
around	O
those	O
that	O
are	O
particularly	O
important	O
.	O
4.1.1	O
notation	O
let	O
us	O
brieﬂy	O
say	O
a	O
few	O
words	O
about	O
notation	O
.	O
we	O
denote	O
vectors	O
by	O
boldface	O
lower	O
case	O
letters	O
,	O
such	O
as	O
x.	O
we	O
denote	O
matrices	O
by	O
boldface	O
upper	O
case	O
letters	O
,	O
such	O
as	O
x.	O
we	O
denote	O
entries	O
in	O
a	O
matrix	O
by	O
non-bold	O
upper	O
case	O
letters	O
,	O
such	O
as	O
xij	O
.	O
all	O
vectors	O
are	O
assumed	O
to	O
be	O
column	O
vectors	O
unless	O
noted	O
otherwise	O
.	O
we	O
use	O
[	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
]	O
to	O
denote	O
a	O
column	O
vector	O
created	O
by	O
stacking	B
d	O
scalars	O
.	O
similarly	O
,	O
if	O
we	O
write	O
x	O
=	O
[	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
]	O
,	O
where	O
the	O
left	O
hand	O
side	O
is	O
a	O
tall	O
column	O
vector	O
,	O
we	O
mean	B
to	O
stack	O
the	O
xi	O
along	O
the	O
rows	O
;	O
this	O
is	O
usually	O
written	O
as	O
x	O
=	O
(	O
xt	O
d	O
)	O
t	O
,	O
but	O
that	O
is	O
rather	O
ugly	O
.	O
if	O
we	O
write	O
x	O
=	O
[	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
]	O
,	O
where	O
the	O
left	O
hand	O
side	O
is	O
a	O
matrix	O
,	O
we	O
mean	B
to	O
stack	O
the	O
xi	O
along	O
the	O
columns	O
,	O
creating	O
a	O
matrix	O
.	O
1	O
,	O
.	O
.	O
.	O
,	O
xt	O
4.1.2	O
basics	O
recall	B
from	O
section	O
2.5.2	O
that	O
the	O
pdf	B
for	O
an	O
mvn	O
in	O
d	O
dimensions	O
is	O
deﬁned	O
by	O
the	O
following	O
:	O
n	O
(	O
x|μ	O
,	O
σ	O
)	O
(	O
cid:2	O
)	O
1	O
(	O
2π	O
)	O
d/2|σ|1/2	O
exp	O
(	O
cid:3	O
)	O
(	O
x	O
−	O
μ	O
)	O
t	O
σ−1	O
(	O
x	O
−	O
μ	O
)	O
(	O
cid:2	O
)	O
−	O
1	O
2	O
(	O
4.1	O
)	O
98	O
chapter	O
4.	O
gaussian	O
models	O
2	O
x	O
1/2	O
λ	O
2	O
u2	O
u1	O
1/2	O
λ	O
1	O
μ	O
x1	O
figure	O
4.1	O
visualization	O
of	O
a	O
2	O
dimensional	O
gaussian	O
density	O
.	O
the	O
major	O
and	O
minor	O
axes	O
of	O
the	O
ellipse	O
are	O
deﬁned	O
by	O
the	O
ﬁrst	O
two	O
eigenvectors	O
of	O
the	O
covariance	B
matrix	I
,	O
namely	O
u1	O
and	O
u2	O
.	O
based	O
on	O
figure	O
2.7	O
of	O
(	O
bishop	O
2006a	O
)	O
.	O
the	O
expression	O
inside	O
the	O
exponent	O
is	O
the	O
mahalanobis	O
distance	O
between	O
a	O
data	O
vector	O
x	O
and	O
the	O
mean	B
vector	O
μ	O
,	O
we	O
can	O
gain	O
a	O
better	O
understanding	O
of	O
this	O
quantity	O
by	O
performing	O
an	O
eigendecomposition	B
of	O
σ.	O
that	O
is	O
,	O
we	O
write	O
σ	O
=	O
uλut	O
,	O
whereu	O
is	O
an	O
orthonormal	O
matrix	O
of	O
eigenvectors	O
satsifying	O
ut	O
u	O
=	O
i	O
,	O
and	O
λ	O
is	O
a	O
diagonal	B
matrix	O
of	O
eigenvalues	O
.	O
using	O
the	O
eigendecomposition	B
,	O
we	O
have	O
that	O
σ−1	O
=	O
u−t	O
λ−1u−1	O
=	O
uλ−1ut	O
=	O
1	O
λi	O
uiut	O
i	O
(	O
4.2	O
)	O
where	O
ui	O
is	O
the	O
i	O
’	O
th	O
column	O
of	O
u	O
,	O
containing	O
the	O
i	O
’	O
th	O
eigenvector	O
.	O
hence	O
we	O
can	O
rewrite	O
the	O
mahalanobis	O
distance	O
as	O
follows	O
:	O
(	O
x	O
−	O
μ	O
)	O
t	O
σ−1	O
(	O
x	O
−	O
μ	O
)	O
=	O
(	O
x	O
−	O
μ	O
)	O
t	O
(	O
cid:14	O
)	O
1	O
λi	O
uiut	O
i	O
(	O
x	O
−	O
μ	O
)	O
d	O
(	O
cid:6	O
)	O
i=1	O
d	O
(	O
cid:6	O
)	O
=	O
i=1	O
1	O
λi	O
(	O
x	O
−	O
μ	O
)	O
t	O
uiut	O
i	O
(	O
x	O
−	O
μ	O
)	O
=	O
d	O
(	O
cid:6	O
)	O
i=1	O
(	O
cid:13	O
)	O
d	O
(	O
cid:6	O
)	O
i=1	O
y2	O
i	O
λi	O
(	O
4.3	O
)	O
(	O
4.4	O
)	O
(	O
4.5	O
)	O
where	O
yi	O
(	O
cid:2	O
)	O
ut	O
i	O
(	O
x	O
−	O
μ	O
)	O
.	O
recall	B
that	O
the	O
equation	O
for	O
an	O
ellipse	O
in	O
2d	O
is	O
y2	O
1	O
λ1	O
+	O
y2	O
2	O
λ2	O
=	O
1	O
hence	O
we	O
see	O
that	O
the	O
contours	O
of	O
equal	O
probability	O
density	O
of	O
a	O
gaussian	O
lie	O
along	O
ellipses	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
4.1.	O
the	O
eigenvectors	O
determine	O
the	O
orientation	O
of	O
the	O
ellipse	O
,	O
and	O
the	O
eigenvalues	O
determine	O
how	O
elogonated	O
it	O
is	O
.	O
in	O
general	O
,	O
we	O
see	O
that	O
the	O
mahalanobis	O
distance	O
corresponds	O
to	O
euclidean	O
distance	O
in	O
a	O
transformed	O
coordinate	O
system	O
,	O
where	O
we	O
shift	O
by	O
μ	O
and	O
rotate	O
by	O
u	O
.	O
4.1.	O
introduction	O
4.1.3	O
mle	O
for	O
an	O
mvn	O
99	O
we	O
now	O
describe	O
one	O
way	O
to	O
estimate	O
the	O
parameters	O
of	O
an	O
mvn	O
,	O
using	O
mle	O
.	O
in	O
later	O
sections	O
,	O
we	O
will	O
discuss	O
bayesian	O
inference	B
for	O
the	O
parameters	O
,	O
which	O
can	O
mitigate	O
overﬁtting	B
,	O
and	O
can	O
provide	O
a	O
measure	O
of	O
conﬁdence	O
in	O
our	O
estimates	O
.	O
theorem	O
4.1.1	O
(	O
mle	O
for	O
a	O
gaussian	O
)	O
.	O
if	O
we	O
have	O
n	O
iid	B
samples	O
xi	O
∼	O
n	O
(	O
μ	O
,	O
σ	O
)	O
,	O
then	O
the	O
mle	O
for	O
the	O
parameters	O
is	O
given	O
by	O
ˆμmle	O
=	O
ˆσmle	O
=	O
1	O
n	O
1	O
n	O
xi	O
(	O
cid:2	O
)	O
x	O
(	O
xi	O
−	O
x	O
)	O
(	O
xi	O
−	O
x	O
)	O
t	O
=	O
n	O
(	O
cid:6	O
)	O
i=1	O
1	O
n	O
(	O
i	O
)	O
−	O
x	O
xt	O
xixt	O
(	O
4.6	O
)	O
(	O
4.7	O
)	O
that	O
is	O
,	O
the	O
mle	O
is	O
just	O
the	O
empirical	O
mean	O
and	O
empirical	O
covariance	O
.	O
in	O
the	O
univariate	O
case	O
,	O
we	O
get	O
the	O
following	O
familiar	O
results	O
:	O
xi	O
=	O
x	O
(	O
xi	O
−	O
x	O
)	O
2	O
=	O
(	O
cid:13	O
)	O
(	O
cid:14	O
)	O
(	O
cid:6	O
)	O
i	O
1	O
n	O
x2	O
i	O
−	O
(	O
x	O
)	O
2	O
(	O
4.8	O
)	O
(	O
4.9	O
)	O
n	O
(	O
cid:6	O
)	O
n	O
(	O
cid:6	O
)	O
i=1	O
i=1	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
i	O
i	O
ˆμ	O
=	O
ˆσ2	O
=	O
1	O
n	O
1	O
n	O
4.1.3.1	O
proof	O
*	O
to	O
prove	O
this	O
result	O
,	O
we	O
will	O
need	O
several	O
results	O
from	O
matrix	O
algebra	O
,	O
which	O
we	O
summarize	O
in	O
the	O
equations	O
,	O
a	O
and	O
b	O
are	O
vectors	O
,	O
and	O
a	O
and	O
b	O
are	O
matrices	O
.	O
also	O
,	O
the	O
notation	O
below	O
.	O
tr	O
(	O
a	O
)	O
refers	O
to	O
the	O
trace	B
of	O
a	O
matrix	O
,	O
which	O
is	O
the	O
sum	O
of	O
its	O
diagonals	O
:	O
tr	O
(	O
a	O
)	O
=	O
(	O
cid:4	O
)	O
i	O
aii	O
.	O
∂	O
(	O
bt	O
a	O
)	O
∂a	O
∂	O
(	O
at	O
aa	O
)	O
∂a	O
=	O
b	O
=	O
(	O
a	O
+	O
at	O
)	O
a	O
∂	O
∂a	O
∂	O
∂a	O
tr	O
(	O
ba	O
)	O
=b	O
t	O
log	O
|a|	O
=	O
a−t	O
(	O
cid:2	O
)	O
(	O
a−1	O
)	O
t	O
tr	O
(	O
abc	O
)	O
=	O
tr	O
(	O
cab	O
)	O
=	O
tr	O
(	O
bca	O
)	O
(	O
4.10	O
)	O
the	O
last	O
equation	O
is	O
called	O
the	O
cyclic	B
permutation	I
property	I
of	O
the	O
trace	B
operator	O
.	O
using	O
this	O
,	O
we	O
can	O
derive	O
the	O
widely	O
used	O
trace	B
trick	I
,	O
which	O
reorders	O
the	O
scalar	O
inner	O
product	O
xt	O
ax	O
as	O
follows	O
xt	O
ax	O
=	O
tr	O
(	O
xt	O
ax	O
)	O
=	O
tr	O
(	O
xxt	O
a	O
)	O
=	O
tr	O
(	O
axxt	O
)	O
(	O
4.11	O
)	O
100	O
chapter	O
4.	O
gaussian	O
models	O
proof	O
.	O
we	O
can	O
now	O
begin	O
with	O
the	O
proof	O
.	O
the	O
log-likelihood	O
is	O
(	O
cid:9	O
)	O
(	O
μ	O
,	O
σ	O
)	O
=	O
log	O
p	O
(	O
d|μ	O
,	O
σ	O
)	O
=	O
n	O
2	O
log	O
|λ|	O
−	O
1	O
2	O
(	O
xi	O
−	O
μ	O
)	O
t	O
λ	O
(	O
xi	O
−	O
μ	O
)	O
where	O
λ	O
=	O
σ−1	O
is	O
the	O
precision	B
matrix	I
.	O
using	O
the	O
substitution	O
yi	O
=	O
xi	O
−	O
μ	O
and	O
the	O
chain	B
rule	I
of	O
calculus	O
,	O
we	O
have	O
∂	O
∂μ	O
(	O
xi	O
−	O
μ	O
)	O
t	O
σ−1	O
(	O
xi	O
−	O
μ	O
)	O
=	O
n	O
(	O
cid:6	O
)	O
i=1	O
∂	O
∂yi	O
i	O
σ−1yi	O
yt	O
∂yi	O
∂μ	O
=	O
−1	O
(	O
σ−1	O
+	O
σ−t	O
)	O
yi	O
n	O
(	O
cid:6	O
)	O
−2σ−1	O
(	O
xi	O
−	O
μ	O
)	O
=	O
σ−1	O
(	O
xi	O
−	O
μ	O
)	O
=	O
0	O
i=1	O
so	O
the	O
mle	O
of	O
μ	O
is	O
just	O
the	O
empirical	O
mean	O
.	O
now	O
we	O
can	O
use	O
the	O
trace-trick	O
to	O
rewrite	O
the	O
log-likelihood	O
for	O
λ	O
as	O
follows	O
:	O
tr	O
[	O
(	O
xi	O
−	O
μ	O
)	O
(	O
xi	O
−	O
μ	O
)	O
t	O
λ	O
]	O
hence	O
∂	O
∂μ	O
(	O
cid:9	O
)	O
(	O
μ	O
,	O
σ	O
)	O
=−	O
1	O
2	O
n	O
(	O
cid:6	O
)	O
n	O
(	O
cid:6	O
)	O
i=1	O
i=1	O
ˆμ	O
=	O
1	O
n	O
xi	O
=	O
x	O
(	O
cid:6	O
)	O
i	O
tr	O
[	O
sμλ	O
]	O
n	O
2	O
n	O
2	O
log	O
|λ|	O
−	O
1	O
2	O
log	O
|λ|	O
−	O
1	O
2	O
(	O
xi	O
−	O
μ	O
)	O
(	O
xi	O
−	O
μ	O
)	O
t	O
(	O
cid:9	O
)	O
(	O
λ	O
)	O
=	O
=	O
n	O
(	O
cid:6	O
)	O
i=1	O
where	O
sμ	O
(	O
cid:2	O
)	O
(	O
4.12	O
)	O
(	O
4.13	O
)	O
(	O
4.14	O
)	O
(	O
4.15	O
)	O
(	O
4.16	O
)	O
(	O
4.17	O
)	O
(	O
4.18	O
)	O
(	O
4.19	O
)	O
(	O
4.20	O
)	O
is	O
the	O
scatter	O
matrix	O
centered	O
on	O
μ.	O
taking	O
derivatives	O
of	O
this	O
expression	O
with	O
respect	O
to	O
λ	O
yields	O
∂	O
(	O
cid:9	O
)	O
(	O
λ	O
)	O
=	O
n	O
λ−t	O
−	O
1	O
∂λ	O
2	O
λ−t	O
=	O
λ−1	O
=	O
σ	O
=	O
2	O
st	O
μ	O
=	O
0	O
1	O
n	O
sμ	O
so	O
n	O
(	O
cid:6	O
)	O
i=1	O
ˆσ	O
=	O
1	O
n	O
(	O
xi	O
−	O
μ	O
)	O
(	O
xi	O
−	O
μ	O
)	O
t	O
(	O
4.21	O
)	O
(	O
4.22	O
)	O
(	O
4.23	O
)	O
if	O
we	O
plug-in	B
the	O
mle	O
μ	O
=	O
x	O
which	O
is	O
just	O
the	O
empirical	O
covariance	O
matrix	O
centered	O
on	O
μ	O
.	O
(	O
since	O
both	O
parameters	O
must	O
be	O
simultaneously	O
optimized	O
)	O
,	O
we	O
get	O
the	O
standard	O
equation	O
for	O
the	O
mle	O
of	O
a	O
covariance	B
matrix	I
.	O
4.2.	O
gaussian	O
discriminant	B
analysis	I
101	O
4.1.4	O
maximum	B
entropy	I
derivation	O
of	O
the	O
gaussian	O
*	O
in	O
this	O
section	O
,	O
we	O
show	O
that	O
the	O
multivariate	O
gaussian	O
is	O
the	O
distribution	O
with	O
maximum	B
entropy	I
subject	O
to	O
having	O
a	O
speciﬁed	O
mean	B
and	O
covariance	B
(	O
see	O
also	O
section	O
9.2.6	O
)	O
.	O
this	O
is	O
one	O
reason	O
the	O
gaussian	O
is	O
so	O
widely	O
used	O
:	O
the	O
ﬁrst	O
two	O
moments	O
are	O
usually	O
all	O
that	O
we	O
can	O
reliably	O
estimate	O
from	O
data	O
,	O
so	O
we	O
want	O
a	O
distribution	O
that	O
captures	O
these	O
properties	O
,	O
but	O
otherwise	O
makes	O
as	O
few	O
addtional	O
assumptions	O
as	O
possible	O
.	O
to	O
simplify	O
notation	O
,	O
we	O
will	O
assume	O
the	O
mean	B
is	O
zero	O
.	O
the	O
pdf	B
has	O
the	O
form	O
1	O
z	O
exp	O
(	O
−	O
1	O
2	O
xt	O
σ−1x	O
)	O
p	O
(	O
x	O
)	O
=	O
(	O
4.24	O
)	O
−1	O
)	O
ij	O
,	O
for	O
i	O
,	O
j	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
,	O
we	O
see	O
that	O
this	O
is	O
in	O
if	O
we	O
deﬁne	O
fij	O
(	O
x	O
)	O
=x	O
ixj	O
and	O
λij	O
=	O
1	O
the	O
same	O
form	O
as	O
equation	O
9.74.	O
the	O
(	O
differential	O
)	O
entropy	B
of	O
this	O
distribution	O
(	O
using	O
log	O
base	O
e	O
)	O
is	O
given	O
by	O
2	O
(	O
σ	O
h	O
(	O
n	O
(	O
μ	O
,	O
σ	O
)	O
)	O
=	O
1	O
2	O
ln	O
(	O
cid:20	O
)	O
(	O
2πe	O
)	O
d|σ|	O
(	O
cid:21	O
)	O
(	O
4.25	O
)	O
(	O
4.26	O
)	O
(	O
4.27	O
)	O
(	O
4.28	O
)	O
we	O
now	O
show	O
the	O
mvn	O
has	O
maximum	B
entropy	I
amongst	O
all	O
distributions	O
with	O
a	O
speciﬁed	O
co-	O
variance	B
σ	O
.	O
(	O
cid:22	O
)	O
q	O
(	O
x	O
)	O
xixj	O
=	O
σij	O
.	O
let	O
p	O
=	O
n	O
(	O
0	O
,	O
σ	O
)	O
.	O
then	O
theorem	O
4.1.2.	O
let	O
q	O
(	O
x	O
)	O
be	O
any	O
density	O
satisfying	O
h	O
(	O
q	O
)	O
≤	O
h	O
(	O
p	O
)	O
.	O
proof	O
.	O
(	O
from	O
(	O
cover	O
and	O
thomas	O
1991	O
,	O
p234	O
)	O
.	O
)	O
we	O
have	O
(	O
cid:11	O
)	O
0	O
≤	O
kl	O
(	O
q||p	O
)	O
=	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
=	O
−h	O
(	O
q	O
)	O
−	O
∗	O
−h	O
(	O
q	O
)	O
−	O
=	O
=	O
−h	O
(	O
q	O
)	O
+h	O
(	O
p	O
)	O
q	O
(	O
x	O
)	O
log	O
q	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
dx	O
q	O
(	O
x	O
)	O
log	O
p	O
(	O
x	O
)	O
dx	O
p	O
(	O
x	O
)	O
log	O
p	O
(	O
x	O
)	O
dx	O
(	O
4.29	O
)	O
where	O
the	O
key	O
step	O
in	O
equation	O
4.28	O
(	O
marked	O
with	O
a	O
*	O
)	O
follows	O
since	O
q	O
and	O
p	O
yield	O
the	O
same	O
moments	O
for	O
the	O
quadratic	O
form	O
encoded	O
by	O
log	O
p	O
(	O
x	O
)	O
.	O
4.2	O
gaussian	O
discriminant	B
analysis	I
one	O
important	O
application	O
of	O
mvns	O
is	O
to	O
deﬁne	O
the	O
the	O
class	O
conditional	O
densities	O
in	O
a	O
generative	B
classiﬁer	I
,	O
i.e.	O
,	O
p	O
(	O
x|y	O
=	O
c	O
,	O
θ	O
)	O
=	O
n	O
(	O
x|μc	O
,	O
σc	O
)	O
(	O
4.30	O
)	O
the	O
resulting	O
technique	O
is	O
called	O
(	O
gaussian	O
)	O
discriminant	B
analysis	I
or	O
gda	O
(	O
even	O
though	O
it	O
is	O
a	O
generative	O
,	O
not	O
discriminative	O
,	O
classiﬁer	O
—	O
see	O
section	O
8.6	O
for	O
more	O
on	O
this	O
distinction	O
)	O
.	O
if	O
σc	O
is	O
diagonal	B
,	O
this	O
is	O
equivalent	O
to	O
naive	O
bayes	O
.	O
102	O
chapter	O
4.	O
gaussian	O
models	O
t	O
i	O
h	O
g	O
e	O
w	O
280	O
260	O
240	O
220	O
200	O
180	O
160	O
140	O
120	O
100	O
80	O
55	O
red	O
=	O
female	O
,	O
blue=male	O
60	O
65	O
70	O
75	O
80	O
height	O
(	O
a	O
)	O
t	O
i	O
h	O
g	O
e	O
w	O
280	O
260	O
240	O
220	O
200	O
180	O
160	O
140	O
120	O
100	O
80	O
55	O
red	O
=	O
female	O
,	O
blue=male	O
60	O
65	O
70	O
75	O
80	O
height	O
(	O
b	O
)	O
figure	O
4.2	O
(	O
a	O
)	O
height/weight	O
data	O
.	O
(	O
b	O
)	O
visualization	O
of	O
2d	O
gaussians	O
ﬁt	O
to	O
each	O
class	O
.	O
95	O
%	O
of	O
the	O
probability	O
mass	O
is	O
inside	O
the	O
ellipse	O
.	O
figure	O
generated	O
by	O
gaussheightweight	O
.	O
we	O
can	O
classify	O
a	O
feature	O
vector	O
using	O
the	O
following	O
decision	B
rule	I
,	O
derived	O
from	O
equation	O
2.13	O
:	O
ˆy	O
(	O
x	O
)	O
=	O
argmax	O
c	O
[	O
log	O
p	O
(	O
y	O
=	O
c|π	O
)	O
+	O
log	O
p	O
(	O
x|θc	O
)	O
]	O
(	O
4.31	O
)	O
when	O
we	O
compute	O
the	O
probability	O
of	O
x	O
under	O
each	O
class	O
conditional	O
density	O
,	O
we	O
are	O
measuring	O
the	O
distance	O
from	O
x	O
to	O
the	O
center	O
of	O
each	O
class	O
,	O
μc	O
,	O
using	O
mahalanobis	O
distance	O
.	O
this	O
can	O
be	O
thought	O
of	O
as	O
a	O
nearest	B
centroids	I
classiﬁer	I
.	O
as	O
an	O
example	O
,	O
figure	O
4.2	O
shows	O
two	O
gaussian	O
class-conditional	O
densities	O
in	O
2d	O
,	O
representing	O
the	O
height	O
and	O
weight	O
of	O
men	O
and	O
women	O
.	O
we	O
can	O
see	O
that	O
the	O
features	B
are	O
correlated	O
,	O
as	O
is	O
to	O
be	O
expected	O
(	O
tall	O
people	O
tend	O
to	O
weigh	O
more	O
)	O
.	O
the	O
ellipses	O
for	O
each	O
class	O
contain	O
95	O
%	O
of	O
the	O
probability	O
mass	O
.	O
if	O
we	O
have	O
a	O
uniform	O
prior	O
over	O
classes	O
,	O
we	O
can	O
classify	O
a	O
new	O
test	O
vector	O
as	O
follows	O
:	O
ˆy	O
(	O
x	O
)	O
=	O
argmin	O
c	O
(	O
x	O
−	O
μc	O
)	O
t	O
σ−1	O
c	O
(	O
x	O
−	O
μc	O
)	O
4.2.1	O
quadratic	B
discriminant	I
analysis	I
(	O
qda	O
)	O
(	O
4.32	O
)	O
the	O
posterior	O
over	O
class	O
labels	O
is	O
given	O
by	O
equation	O
2.13.	O
we	O
can	O
gain	O
further	O
insight	O
into	O
this	O
model	O
by	O
plugging	O
in	O
the	O
deﬁnition	O
of	O
the	O
gaussian	O
density	O
,	O
as	O
follows	O
:	O
p	O
(	O
y	O
=	O
c|x	O
,	O
θ	O
)	O
=	O
(	O
cid:4	O
)	O
πc|2πσc|−	O
1	O
c	O
(	O
cid:2	O
)	O
πc	O
(	O
cid:2	O
)	O
|2πσc	O
(	O
cid:2	O
)	O
|−	O
1	O
2	O
exp	O
2	O
exp	O
(	O
cid:20	O
)	O
−	O
1	O
2	O
(	O
x	O
−	O
μc	O
)	O
t	O
σ−1	O
(	O
cid:20	O
)	O
−	O
1	O
2	O
(	O
x	O
−	O
μc	O
(	O
cid:2	O
)	O
)	O
t	O
σ−1	O
(	O
cid:21	O
)	O
c	O
(	O
cid:2	O
)	O
(	O
x	O
−	O
μc	O
(	O
cid:2	O
)	O
)	O
c	O
(	O
x	O
−	O
μc	O
)	O
(	O
cid:21	O
)	O
(	O
4.33	O
)	O
thresholding	O
this	O
results	O
in	O
a	O
quadratic	O
function	O
of	O
x.	O
the	O
result	O
is	O
known	O
as	O
quadratic	B
discriminant	I
analysis	I
(	O
qda	O
)	O
.	O
figure	O
4.3	O
gives	O
some	O
examples	O
of	O
what	O
the	O
decision	B
boundaries	O
look	O
like	O
in	O
2d	O
.	O
4.2.	O
gaussian	O
discriminant	B
analysis	I
103	O
parabolic	O
boundary	O
some	O
linear	O
,	O
some	O
quadratic	O
2	O
0	O
−2	O
8	O
6	O
4	O
2	O
0	O
−2	O
−2	O
0	O
(	O
a	O
)	O
2	O
−2	O
0	O
4	O
6	O
2	O
(	O
b	O
)	O
figure	O
4.3	O
quadratic	O
decision	O
boundaries	O
in	O
2d	O
for	O
the	O
2	O
and	O
3	O
class	O
case	O
.	O
discrimanalysisdboundariesdemo	O
.	O
figure	O
generated	O
by	O
t=100	O
t=1	O
t=0.1	O
t=0.01	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
1	O
2	O
3	O
1	O
0.5	O
0	O
1	O
2	O
3	O
1	O
0.5	O
0	O
1	O
2	O
3	O
1	O
0.5	O
0	O
1	O
2	O
3	O
softmax	B
distribution	O
s	O
(	O
η/t	O
)	O
,	O
where	O
η	O
=	O
(	O
3	O
,	O
0	O
,	O
1	O
)	O
,	O
at	O
different	O
temperatures	O
t	O
.	O
when	O
the	O
figure	O
4.4	O
temperature	B
is	O
high	O
(	O
left	O
)	O
,	O
the	O
distribution	O
is	O
uniform	O
,	O
whereas	O
when	O
the	O
temperature	B
is	O
low	O
(	O
right	O
)	O
,	O
the	O
distribution	O
is	O
“	O
spiky	O
”	O
,	O
with	O
all	O
its	O
mass	O
on	O
the	O
largest	O
element	O
.	O
figure	O
generated	O
by	O
softmaxdemo2	O
.	O
4.2.2	O
linear	B
discriminant	I
analysis	I
(	O
lda	O
)	O
p	O
(	O
y	O
=	O
c|x	O
,	O
θ	O
)	O
∝	O
πc	O
exp	O
(	O
cid:2	O
)	O
we	O
now	O
consider	O
a	O
special	O
case	O
in	O
which	O
the	O
covariance	B
matrices	O
are	O
tied	B
or	O
shared	B
across	O
(	O
cid:3	O
)	O
classes	O
,	O
σc	O
=	O
σ.	O
in	O
this	O
case	O
,	O
we	O
can	O
simplify	O
equation	O
4.33	O
as	O
follows	O
:	O
c	O
σ−1μc	O
μt	O
(	O
cid:3	O
)	O
exp	O
[	O
−	O
1	O
2	O
(	O
4.35	O
)	O
since	O
the	O
quadratic	O
term	O
xt	O
σ−1x	O
is	O
independent	O
of	O
c	O
,	O
it	O
will	O
cancel	O
out	O
in	O
the	O
numerator	O
and	O
denominator	O
.	O
if	O
we	O
deﬁne	O
xt	O
σ−1x	O
]	O
(	O
4.34	O
)	O
(	O
cid:2	O
)	O
xt	O
σ−1x	O
−	O
1	O
2	O
c	O
σ−1x	O
−	O
1	O
μt	O
2	O
c	O
σ−1μc	O
+	O
log	O
πc	O
μt	O
c	O
σ−1x	O
−	O
1	O
μt	O
2	O
=	O
exp	O
γc	O
=	O
−	O
1	O
c	O
σ−1μc	O
+	O
log	O
πc	O
μt	O
2	O
βc	O
=	O
σ−1μc	O
(	O
4.36	O
)	O
(	O
4.37	O
)	O
104	O
chapter	O
4.	O
gaussian	O
models	O
c	O
x+γc	O
eβt	O
c	O
(	O
cid:2	O
)	O
eβt	O
c	O
(	O
cid:2	O
)	O
x+γc	O
(	O
cid:2	O
)	O
=	O
s	O
(	O
η	O
)	O
c	O
cx	O
+	O
γc	O
]	O
,	O
and	O
s	O
is	O
the	O
softmax	B
function	O
,	O
deﬁned	O
as	O
follows	O
:	O
(	O
4.38	O
)	O
then	O
we	O
can	O
write	O
p	O
(	O
y	O
=	O
c|x	O
,	O
θ	O
)	O
=	O
(	O
cid:4	O
)	O
where	O
η	O
=	O
[	O
βt	O
s	O
(	O
η	O
)	O
c	O
=	O
1	O
x	O
+	O
γ1	O
,	O
.	O
.	O
.	O
,	O
βt	O
eηc	O
(	O
cid:4	O
)	O
c	O
c	O
(	O
cid:2	O
)	O
=1	O
eηc	O
(	O
cid:2	O
)	O
(	O
cid:5	O
)	O
(	O
4.39	O
)	O
(	O
4.40	O
)	O
the	O
softmax	B
function	O
is	O
so-called	O
since	O
it	O
acts	O
a	O
bit	O
like	O
the	O
max	O
function	O
.	O
to	O
see	O
this	O
,	O
let	O
us	O
divide	O
each	O
ηc	O
by	O
a	O
constant	O
t	O
called	O
the	O
temperature	B
.	O
then	O
as	O
t	O
→	O
0	O
,	O
we	O
ﬁnd	O
s	O
(	O
η/t	O
)	O
c	O
=	O
if	O
c	O
=	O
argmaxc	O
(	O
cid:2	O
)	O
ηc	O
(	O
cid:2	O
)	O
1.0	O
0.0	O
otherwise	O
in	O
other	O
words	O
,	O
at	O
low	O
temperatures	O
,	O
the	O
distribution	O
spends	O
essentially	O
all	O
of	O
its	O
time	O
in	O
the	O
most	O
probable	O
state	B
,	O
whereas	O
at	O
high	O
temperatures	O
,	O
it	O
visits	O
all	O
states	O
uniformly	O
.	O
see	O
figure	O
4.4	O
for	O
an	O
illustration	O
.	O
note	O
that	O
this	O
terminology	O
comes	O
from	O
the	O
area	O
of	O
statistical	O
physics	O
,	O
where	O
it	O
is	O
common	O
to	O
use	O
the	O
boltzmann	O
distribution	O
,	O
which	O
has	O
the	O
same	O
form	O
as	O
the	O
softmax	B
function	O
.	O
p	O
(	O
y	O
=	O
c|x	O
,	O
θ	O
)	O
=p	O
(	O
an	O
interesting	O
property	O
of	O
equation	O
4.38	O
is	O
that	O
,	O
if	O
we	O
take	O
logs	O
,	O
we	O
end	O
up	O
with	O
a	O
linear	O
(	O
the	O
reason	O
it	O
is	O
linear	O
is	O
because	O
the	O
xt	O
σ−1x	O
cancels	O
from	O
the	O
numerator	O
function	O
of	O
x.	O
and	O
denominator	O
.	O
)	O
thus	O
the	O
decision	B
boundary	I
between	O
any	O
two	O
classes	O
,	O
say	O
c	O
and	O
c	O
(	O
cid:2	O
)	O
,	O
will	O
be	O
a	O
straight	O
line	O
.	O
hence	O
this	O
technique	O
is	O
called	O
linear	B
discriminant	I
analysis	I
or	O
lda	O
.	O
1	O
we	O
can	O
derive	O
the	O
form	O
of	O
this	O
line	O
as	O
follows	O
:	O
y	O
=	O
c	O
(	O
cid:2	O
)	O
|x	O
,	O
θ	O
)	O
c	O
(	O
cid:2	O
)	O
x	O
+	O
γc	O
(	O
cid:2	O
)	O
xt	O
(	O
βc	O
(	O
cid:2	O
)	O
−	O
β	O
)	O
=γ	O
c	O
(	O
cid:2	O
)	O
−	O
γc	O
see	O
figure	O
4.5	O
for	O
some	O
examples	O
.	O
an	O
alternative	O
to	O
ﬁtting	O
an	O
lda	O
model	O
and	O
then	O
deriving	O
the	O
class	O
posterior	O
is	O
to	O
directly	O
ﬁt	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
cat	O
(	O
y|wx	O
)	O
for	O
some	O
c	O
×	O
d	O
weight	O
matrix	O
w.	O
this	O
is	O
called	O
multi-class	B
logistic	I
regression	I
,	O
or	O
multinomial	O
logistic	O
regression.2	O
we	O
will	O
discuss	O
this	O
model	O
in	O
detail	O
in	O
section	O
8.2.	O
the	O
difference	O
between	O
the	O
two	O
approaches	O
is	O
explained	O
in	O
section	O
8.6.	O
βt	O
c	O
x	O
+	O
γc	O
=	O
βt	O
(	O
4.41	O
)	O
(	O
4.42	O
)	O
(	O
4.43	O
)	O
4.2.3	O
two-class	O
lda	O
to	O
gain	O
further	O
insight	O
into	O
the	O
meaning	O
of	O
these	O
equations	O
,	O
let	O
us	O
consider	O
the	O
binary	O
case	O
.	O
in	O
this	O
case	O
,	O
the	O
posterior	O
is	O
given	O
by	O
eβt	O
p	O
(	O
y	O
=	O
1|x	O
,	O
θ	O
)	O
=	O
1	O
x+γ1	O
1	O
x+γ1	O
+	O
eβt	O
eβt	O
0	O
x+γ0	O
(	O
cid:18	O
)	O
(	O
β1	O
−	O
β0	O
)	O
t	O
x	O
+	O
(	O
γ1	O
−	O
γ0	O
)	O
(	O
cid:19	O
)	O
(	O
4.44	O
)	O
(	O
4.45	O
)	O
=	O
1	O
1	O
+	O
e	O
(	O
β0−β1	O
)	O
t	O
x+	O
(	O
γ0−γ1	O
)	O
=	O
sigm	O
1.	O
the	O
abbreviation	O
“	O
lda	O
”	O
,	O
could	O
either	O
stand	O
for	O
“	O
linear	B
discriminant	I
analysis	I
”	O
or	O
“	O
latent	B
dirichlet	O
allocation	O
”	O
(	O
sec-	O
tion	O
27.3	O
)	O
.	O
we	O
hope	O
the	O
meaning	O
is	O
clear	O
from	O
text	O
.	O
2.	O
in	O
the	O
language	B
modeling	I
community	O
,	O
this	O
model	O
is	O
called	O
a	O
maximum	B
entropy	I
model	O
,	O
for	O
reasons	O
explained	O
in	O
section	O
9.2.6	O
.	O
4.2.	O
gaussian	O
discriminant	B
analysis	I
105	O
linear	O
boundary	O
all	O
linear	O
boundaries	O
2	O
0	O
−2	O
6	O
4	O
2	O
0	O
−2	O
0	O
(	O
a	O
)	O
2	O
−2	O
−2	O
0	O
4	O
6	O
2	O
(	O
b	O
)	O
figure	O
4.5	O
discrimanalysisdboundariesdemo	O
.	O
linear	O
decision	O
boundaries	O
in	O
2d	O
for	O
the	O
2	O
and	O
3	O
class	O
case	O
.	O
figure	O
generated	O
by	O
figure	O
4.6	O
geometry	O
of	O
lda	O
in	O
the	O
2	O
class	O
case	O
where	O
σ1	O
=	O
σ2	O
=	O
i.	O
where	O
sigm	O
(	O
η	O
)	O
refers	O
to	O
the	O
sigmoid	B
function	O
(	O
equation	O
1.10	O
)	O
.	O
now	O
γ1	O
−	O
γ0	O
=	O
−	O
1	O
2	O
=	O
−	O
1	O
2	O
so	O
if	O
we	O
deﬁne	O
0	O
σ−1μ0	O
+	O
log	O
(	O
π1/π0	O
)	O
1	O
σ−1μ1	O
+	O
μt	O
μt	O
(	O
μ1	O
−	O
μ0	O
)	O
t	O
σ−1	O
(	O
μ1	O
+	O
μ0	O
)	O
+	O
log	O
(	O
π1/π0	O
)	O
1	O
2	O
w	O
=	O
β1	O
−	O
β0	O
=	O
σ−1	O
(	O
μ1	O
−	O
μ0	O
)	O
(	O
μ1	O
+	O
μ0	O
)	O
−	O
(	O
μ1	O
−	O
μ0	O
)	O
x0	O
=	O
1	O
2	O
log	O
(	O
π1/π0	O
)	O
(	O
μ1	O
−	O
μ0	O
)	O
t	O
σ−1	O
(	O
μ1	O
−	O
μ0	O
)	O
(	O
4.46	O
)	O
(	O
4.47	O
)	O
(	O
4.48	O
)	O
(	O
4.49	O
)	O
106	O
chapter	O
4.	O
gaussian	O
models	O
then	O
we	O
have	O
wt	O
x0	O
=	O
−	O
(	O
γ1	O
−	O
γ0	O
)	O
,	O
and	O
hence	O
p	O
(	O
y	O
=	O
1|x	O
,	O
θ	O
)	O
=	O
sigm	O
(	O
wt	O
(	O
x	O
−	O
x0	O
)	O
)	O
(	O
4.50	O
)	O
2	O
(	O
μ1	O
+	O
μ0	O
)	O
,	O
which	O
is	O
half	O
way	O
between	O
the	O
means	O
.	O
(	O
this	O
is	O
closely	O
related	O
to	O
logistic	B
regression	I
,	O
which	O
we	O
will	O
discuss	O
in	O
section	O
8.2	O
.	O
)	O
so	O
the	O
ﬁnal	O
decision	B
rule	I
is	O
as	O
follows	O
:	O
shift	O
x	O
by	O
x0	O
,	O
project	O
onto	O
the	O
line	O
w	O
,	O
and	O
see	O
if	O
the	O
result	O
is	O
positive	O
or	O
negative	O
.	O
if	O
σ	O
=	O
σ2i	O
,	O
then	O
w	O
is	O
in	O
the	O
direction	O
of	O
μ1	O
−	O
μ0	O
.	O
so	O
we	O
classify	O
the	O
point	O
based	O
on	O
whether	O
its	O
projection	B
is	O
closer	O
to	O
μ0	O
or	O
μ1	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
4.6.	O
furthemore	O
,	O
if	O
π1	O
=	O
π0	O
,	O
then	O
x0	O
=	O
1	O
if	O
we	O
make	O
π1	O
>	O
π0	O
,	O
then	O
x0	O
gets	O
closer	O
to	O
μ0	O
,	O
so	O
more	O
of	O
the	O
line	O
belongs	O
to	O
class	O
1	O
a	O
priori	O
.	O
conversely	O
if	O
π1	O
<	O
π0	O
,	O
the	O
boundary	O
shifts	O
right	O
.	O
thus	O
we	O
see	O
that	O
the	O
class	O
prior	O
,	O
πc	O
,	O
just	O
changes	O
the	O
decision	B
threshold	O
,	O
and	O
not	O
the	O
overall	O
geometry	O
,	O
as	O
we	O
claimed	O
above	O
.	O
(	O
a	O
similar	B
argument	O
applies	O
in	O
the	O
multi-class	O
case	O
.	O
)	O
the	O
magnitude	O
of	O
w	O
determines	O
the	O
steepness	O
of	O
the	O
logistic	B
function	O
,	O
and	O
depends	O
on	O
how	O
well-separated	O
the	O
means	O
are	O
,	O
relative	O
to	O
the	O
variance	B
.	O
in	O
psychology	O
and	O
signal	B
detection	I
theory	I
,	O
it	O
is	O
common	O
to	O
deﬁne	O
the	O
discriminability	B
of	O
a	O
signal	O
from	O
the	O
background	O
noise	O
using	O
a	O
quantity	O
called	O
d-prime	B
:	O
(	O
4.51	O
)	O
d	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
μ1	O
−	O
μ0	O
σ	O
where	O
μ1	O
is	O
the	O
mean	B
of	O
the	O
signal	O
and	O
μ0	O
is	O
the	O
mean	B
of	O
the	O
noise	O
,	O
and	O
σ	O
is	O
the	O
standard	B
deviation	I
of	O
the	O
noise	O
.	O
if	O
d	O
(	O
cid:2	O
)	O
is	O
large	O
,	O
the	O
signal	O
will	O
be	O
easier	O
to	O
discriminate	O
from	O
the	O
noise	O
.	O
4.2.4	O
mle	O
for	O
discriminant	B
analysis	I
we	O
now	O
discuss	O
how	O
to	O
ﬁt	O
a	O
discriminant	B
analysis	I
model	O
.	O
the	O
simplest	O
way	O
is	O
to	O
use	O
maximum	O
likelihood	O
.	O
the	O
log-likelihood	O
function	O
is	O
as	O
follows	O
:	O
(	O
cid:16	O
)	O
n	O
(	O
cid:6	O
)	O
c	O
(	O
cid:6	O
)	O
log	O
p	O
(	O
d|θ	O
)	O
=	O
(	O
cid:17	O
)	O
c	O
(	O
cid:6	O
)	O
⎡	O
⎣	O
(	O
cid:6	O
)	O
⎤	O
⎦	O
i	O
(	O
yi	O
=	O
c	O
)	O
log	O
πc	O
+	O
i=1	O
c=1	O
c=1	O
i	O
:	O
yi=c	O
log	O
n	O
(	O
x|μc	O
,	O
σc	O
)	O
(	O
4.52	O
)	O
we	O
see	O
that	O
this	O
factorizes	O
into	O
a	O
term	O
for	O
π	O
,	O
and	O
c	O
terms	O
for	O
each	O
μc	O
and	O
σc	O
.	O
hence	O
we	O
can	O
estimate	O
these	O
parameters	O
separately	O
.	O
for	O
the	O
class	O
prior	O
,	O
we	O
have	O
ˆπc	O
=	O
nc	O
n	O
,	O
as	O
with	O
naive	O
bayes	O
.	O
for	O
the	O
class-conditional	O
densities	O
,	O
we	O
just	O
partition	O
the	O
data	O
based	O
on	O
its	O
class	O
label	O
,	O
and	O
compute	O
the	O
mle	O
for	O
each	O
gaussian	O
:	O
(	O
xi	O
−	O
ˆμc	O
)	O
(	O
xi	O
−	O
ˆμc	O
)	O
t	O
(	O
4.53	O
)	O
(	O
cid:6	O
)	O
i	O
:	O
yi=c	O
ˆμc	O
=	O
1	O
nc	O
xi	O
,	O
ˆσc	O
=	O
1	O
nc	O
(	O
cid:6	O
)	O
i	O
:	O
yi=c	O
see	O
discrimanalysisfit	O
for	O
a	O
matlab	O
implementation	O
.	O
once	O
the	O
model	O
has	O
been	O
ﬁt	O
,	O
you	O
can	O
make	O
predictions	O
using	O
discrimanalysispredict	O
,	O
which	O
uses	O
a	O
plug-in	B
approximation	I
.	O
4.2.5	O
strategies	O
for	O
preventing	O
overﬁtting	B
the	O
speed	O
and	O
simplicity	O
of	O
the	O
mle	O
method	O
is	O
one	O
of	O
its	O
greatest	O
appeals	O
.	O
however	O
,	O
the	O
mle	O
can	O
badly	O
overﬁt	B
in	O
high	O
dimensions	O
.	O
in	O
particular	O
,	O
the	O
mle	O
for	O
a	O
full	B
covariance	O
matrix	O
is	O
singular	O
if	O
nc	O
<	O
d.	O
and	O
even	O
when	O
nc	O
>	O
d	O
,	O
the	O
mle	O
can	O
be	O
ill-conditioned	B
,	O
meaning	O
it	O
is	O
close	O
to	O
singular	O
.	O
there	O
are	O
several	O
possible	O
solutions	O
to	O
this	O
problem	O
:	O
4.2.	O
gaussian	O
discriminant	B
analysis	I
107	O
•	O
use	O
a	O
diagonal	O
covariance	O
matrix	O
for	O
each	O
class	O
,	O
which	O
assumes	O
the	O
features	B
are	O
conditionally	B
independent	I
;	O
this	O
is	O
equivalent	O
to	O
using	O
a	O
naive	O
bayes	O
classiﬁer	O
(	O
section	O
3.5	O
)	O
.	O
•	O
use	O
a	O
full	B
covariance	O
matrix	O
,	O
but	O
force	O
it	O
to	O
be	O
the	O
same	O
for	O
all	O
classes	O
,	O
σc	O
=	O
σ.	O
this	O
is	O
an	O
example	O
of	O
parameter	B
tying	I
or	O
parameter	B
sharing	I
,	O
and	O
is	O
equivalent	O
to	O
lda	O
(	O
section	O
4.2.2	O
)	O
.	O
•	O
use	O
a	O
diagonal	O
covariance	O
matrix	O
and	O
forced	O
it	O
to	O
be	O
shared	B
.	O
this	O
is	O
called	O
diagonal	O
covariance	O
lda	O
,	O
and	O
is	O
discussed	O
in	O
section	O
4.2.7	O
.	O
•	O
use	O
a	O
full	B
covariance	O
matrix	O
,	O
but	O
impose	O
a	O
prior	O
and	O
then	O
integrate	O
it	O
out	O
.	O
if	O
we	O
use	O
a	O
conjugate	B
prior	I
,	O
this	O
can	O
be	O
done	O
in	O
closed	O
form	O
,	O
using	O
the	O
results	O
from	O
section	O
4.6.3	O
;	O
this	O
is	O
analogous	O
to	O
the	O
“	O
bayesian	O
naive	O
bayes	O
”	O
method	O
in	O
section	O
3.5.1.2.	O
see	O
(	O
minka	O
2000f	O
)	O
for	O
details	O
.	O
•	O
fit	O
a	O
full	B
or	O
diagonal	O
covariance	O
matrix	O
by	O
map	O
estimation	O
.	O
we	O
discuss	O
two	O
different	O
kinds	O
of	O
prior	O
below	O
.	O
•	O
project	O
the	O
data	O
into	O
a	O
low	O
dimensional	O
subspace	O
and	O
ﬁt	O
the	O
gaussians	O
there	O
.	O
see	O
sec-	O
tion	O
8.6.3.3	O
for	O
a	O
way	O
to	O
ﬁnd	O
the	O
best	O
(	O
most	O
discriminative	B
)	O
linear	O
projection	O
.	O
we	O
discuss	O
some	O
of	O
these	O
options	O
below	O
.	O
4.2.6	O
regularized	O
lda	O
*	O
suppose	O
we	O
tie	O
the	O
covariance	B
matrices	O
,	O
so	O
σc	O
=	O
σ	O
,	O
as	O
in	O
lda	O
,	O
and	O
furthermore	O
we	O
perform	O
map	O
estimation	O
of	O
σ	O
using	O
an	O
inverse	O
wishart	O
prior	O
of	O
the	O
form	O
iw	O
(	O
diag	O
(	O
ˆσmle	O
)	O
,	O
ν0	O
)	O
(	O
see	O
section	O
4.5.1	O
)	O
.	O
then	O
we	O
have	O
ˆσ	O
=	O
λdiag	O
(	O
ˆσmle	O
)	O
+	O
(	O
1−	O
λ	O
)	O
ˆσmle	O
(	O
4.54	O
)	O
when	O
we	O
evaluate	O
the	O
class	O
conditional	O
densities	O
,	O
we	O
need	O
to	O
compute	O
ˆσ	O
where	O
λ	O
controls	O
the	O
amount	O
of	O
regularization	B
,	O
which	O
is	O
related	O
to	O
the	O
strength	O
of	O
the	O
prior	O
,	O
ν0	O
(	O
see	O
section	O
4.6.2.1	O
for	O
details	O
)	O
.	O
this	O
technique	O
is	O
known	O
as	O
regularized	B
discriminant	I
analysis	I
or	O
rda	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p656	O
)	O
.	O
−1	O
mle	O
,	O
which	O
is	O
impossible	O
to	O
compute	O
if	O
d	O
>	O
n	O
.	O
however	O
,	O
we	O
can	O
use	O
the	O
svd	O
of	O
x	O
(	O
section	O
12.2.3	O
)	O
to	O
get	O
around	O
this	O
,	O
as	O
we	O
show	O
below	O
.	O
(	O
note	O
that	O
this	O
trick	O
can	O
not	O
be	O
applied	O
to	O
qda	O
,	O
which	O
is	O
a	O
nonlinear	O
function	O
of	O
x	O
.	O
)	O
let	O
x	O
=	O
udvt	O
be	O
the	O
svd	O
of	O
the	O
design	B
matrix	I
,	O
where	O
v	O
is	O
d	O
×	O
n	O
,	O
u	O
is	O
an	O
n	O
×	O
n	O
orthogonal	O
matrix	O
,	O
and	O
d	O
is	O
a	O
diagonal	B
matrix	O
of	O
size	O
n	O
.	O
furthermore	O
,	O
deﬁne	O
the	O
n	O
×	O
n	O
matrix	O
z	O
=	O
ud	O
;	O
this	O
is	O
like	O
a	O
design	B
matrix	I
in	O
a	O
lower	O
dimensional	O
space	O
(	O
since	O
we	O
assume	O
n	O
<	O
d	O
)	O
.	O
also	O
,	O
deﬁne	O
μz	O
=	O
vt	O
μ	O
as	O
the	O
mean	B
of	O
the	O
data	O
in	O
this	O
reduced	O
space	O
;	O
we	O
can	O
recover	O
the	O
original	O
mean	B
using	O
μ	O
=	O
vμz	O
,	O
since	O
vt	O
v	O
=	O
vvt	O
=	O
i.	O
with	O
these	O
deﬁnitions	O
,	O
we	O
can	O
−1	O
,	O
and	O
hence	O
ˆσ	O
108	O
chapter	O
4.	O
gaussian	O
models	O
rewrite	O
the	O
mle	O
as	O
follows	O
:	O
ˆσmle	O
=	O
=	O
=	O
1	O
n	O
1	O
n	O
1	O
n	O
=	O
v	O
(	O
xt	O
x	O
−	O
μμt	O
(	O
zvt	O
)	O
t	O
(	O
zvt	O
)	O
−	O
(	O
vμz	O
)	O
(	O
vμz	O
)	O
t	O
vzt	O
zvt	O
−	O
vμzμt	O
1	O
n	O
zt	O
z	O
−	O
μzμt	O
z	O
)	O
vt	O
z	O
vt	O
=	O
v	O
ˆσzvt	O
where	O
ˆσz	O
is	O
the	O
empirical	O
covariance	O
of	O
z.	O
hence	O
we	O
can	O
rewrite	O
the	O
map	O
estimate	O
as	O
(	O
4.55	O
)	O
(	O
4.56	O
)	O
(	O
4.57	O
)	O
(	O
4.58	O
)	O
(	O
4.59	O
)	O
(	O
4.60	O
)	O
ˆσmap	O
=	O
v	O
˜σzvt	O
˜σz	O
=	O
λdiag	O
(	O
ˆσz	O
)	O
+	O
(	O
1−	O
λ	O
)	O
ˆσz	O
(	O
4.61	O
)	O
note	O
,	O
however	O
,	O
that	O
we	O
never	O
need	O
to	O
actually	O
compute	O
the	O
d×d	O
matrix	O
ˆσmap	O
.	O
this	O
is	O
because	O
equation	O
4.38	O
tells	O
us	O
that	O
to	O
classify	O
using	O
lda	O
,	O
all	O
we	O
need	O
to	O
compute	O
is	O
p	O
(	O
y	O
=	O
c|x	O
,	O
θ	O
)	O
∝	O
exp	O
(	O
δc	O
)	O
,	O
where	O
δc	O
=	O
−xt	O
βc	O
+	O
γc	O
,	O
βc	O
=	O
ˆσ	O
(	O
4.62	O
)	O
we	O
can	O
compute	O
the	O
crucial	O
βc	O
term	O
for	O
rda	O
without	O
inverting	O
the	O
d	O
×	O
d	O
matrix	O
as	O
follows	O
:	O
c	O
βc	O
+	O
log	O
πc	O
−1	O
μc	O
,	O
γc	O
−	O
1	O
2	O
μt	O
βc	O
=	O
ˆσ	O
−1	O
mapμc	O
=	O
(	O
v	O
˜σzvt	O
)	O
−1μc	O
=	O
v	O
˜σ	O
−1	O
z	O
vt	O
μc	O
=	O
v	O
˜σ	O
−1	O
z	O
μz	O
,	O
c	O
(	O
4.63	O
)	O
where	O
μz	O
,	O
c	O
=	O
vt	O
μc	O
is	O
the	O
mean	B
of	O
the	O
z	O
matrix	O
for	O
data	O
belonging	O
to	O
class	O
c.	O
see	O
rdafit	O
for	O
the	O
code	O
.	O
4.2.7	O
diagonal	B
lda	O
a	O
simple	O
alternative	O
to	O
rda	O
is	O
to	O
tie	O
the	O
covariance	B
matrices	O
,	O
so	O
σc	O
=	O
σ	O
as	O
in	O
lda	O
,	O
and	O
then	O
to	O
use	O
a	O
diagonal	O
covariance	O
matrix	O
for	O
each	O
class	O
.	O
this	O
is	O
called	O
the	O
diagonal	B
lda	O
model	O
,	O
and	O
is	O
equivalent	O
to	O
rda	O
with	O
λ	O
=	O
1.	O
the	O
corresponding	O
discriminant	B
function	I
is	O
as	O
follows	O
(	O
compare	O
to	O
equation	O
4.33	O
)	O
:	O
δc	O
(	O
x	O
)	O
=	O
log	O
p	O
(	O
x	O
,	O
y	O
=	O
c|θ	O
)	O
=	O
−	O
d	O
(	O
cid:6	O
)	O
typically	O
we	O
set	O
ˆμcj	O
=	O
xcj	O
and	O
ˆσ2	O
(	O
pooled	B
across	O
classes	O
)	O
deﬁned	O
by	O
(	O
cid:4	O
)	O
c	O
(	O
cid:4	O
)	O
(	O
xj	O
−	O
μcj	O
)	O
2	O
2σ2	O
j	O
+	O
log	O
πc	O
(	O
4.64	O
)	O
j=1	O
j	O
=	O
s2	O
j	O
,	O
which	O
is	O
the	O
pooled	B
empirical	I
variance	I
of	O
feature	O
j	O
s2	O
j	O
=	O
c=1	O
i	O
:	O
yi=c	O
(	O
xij	O
−	O
xcj	O
)	O
2	O
n	O
−	O
c	O
(	O
4.65	O
)	O
in	O
high	O
dimensional	O
settings	O
,	O
this	O
model	O
can	O
work	O
much	O
better	O
than	O
lda	O
and	O
rda	O
(	O
bickel	O
and	O
levina	O
2004	O
)	O
.	O
4.2.	O
gaussian	O
discriminant	B
analysis	I
109	O
2308	O
1	O
1355	O
352	O
106	O
36	O
12	O
5	O
0	O
number	O
of	O
genes	O
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
c	O
i	O
f	O
i	O
s	O
s	O
a	O
c	O
s	O
m	O
i	O
l	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
test	O
train	O
cv	O
1	O
2	O
3	O
4	O
λ	O
5	O
6	O
7	O
8	O
figure	O
4.7	O
error	O
versus	O
amount	O
of	O
shrinkage	B
for	O
nearest	O
shrunken	O
centroid	O
classiﬁer	O
applied	O
to	O
the	O
srbct	O
gene	O
expression	O
data	O
.	O
figure	O
generated	O
by	O
shrunkencentroidssrbctdemo	O
.	O
based	O
on	O
figure	O
18.4	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
4.2.8	O
nearest	B
shrunken	I
centroids	I
classiﬁer	O
*	O
one	O
drawback	O
of	O
diagonal	B
lda	O
is	O
that	O
it	O
depends	O
on	O
all	O
of	O
the	O
features	B
.	O
in	O
high	O
dimensional	O
problems	O
,	O
we	O
might	O
prefer	O
a	O
method	O
that	O
only	O
depends	O
on	O
a	O
subset	O
of	O
the	O
features	B
,	O
for	O
reasons	O
of	O
accuracy	O
and	O
interpretability	O
.	O
one	O
approach	O
is	O
to	O
use	O
a	O
screening	B
method	O
,	O
perhaps	O
based	O
on	O
mutual	B
information	I
,	O
as	O
in	O
section	O
3.5.4.	O
we	O
now	O
discuss	O
another	O
approach	O
to	O
this	O
problem	O
known	O
as	O
the	O
nearest	B
shrunken	I
centroids	I
classiﬁer	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p652	O
)	O
.	O
the	O
basic	O
idea	O
is	O
to	O
perform	O
map	O
estimation	O
for	O
diagonal	B
lda	O
with	O
a	O
sparsity-promoting	O
(	O
laplace	O
)	O
prior	O
(	O
see	O
section	O
13.3	O
)	O
.	O
more	O
precisely	O
,	O
deﬁne	O
the	O
class-speciﬁc	O
feature	O
mean	O
,	O
μcj	O
,	O
in	O
terms	O
of	O
the	O
class-independent	O
feature	O
mean	O
,	O
mj	O
,	O
and	O
a	O
class-speciﬁc	O
offset	O
,	O
δcj	O
.	O
thus	O
we	O
have	O
μcj	O
=	O
mj	O
+	O
δcj	O
(	O
4.66	O
)	O
we	O
will	O
then	O
put	O
a	O
prior	O
on	O
the	O
δcj	O
terms	O
to	O
encourage	O
them	O
to	O
be	O
strictly	O
zero	O
and	O
compute	O
a	O
map	O
estimate	O
.	O
if	O
,	O
for	O
feature	O
j	O
,	O
we	O
ﬁnd	O
that	O
δcj	O
=	O
0	O
for	O
all	O
c	O
,	O
then	O
feature	O
j	O
will	O
play	O
no	O
role	O
in	O
the	O
classiﬁcation	B
decision	O
(	O
since	O
μcj	O
will	O
be	O
independent	O
of	O
c	O
)	O
.	O
thus	O
features	B
that	O
are	O
not	O
discriminative	O
are	O
automatically	O
ignored	O
.	O
the	O
details	O
can	O
be	O
found	O
in	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p652	O
)	O
and	O
(	O
greenshtein	O
and	O
park	O
2009	O
)	O
.	O
see	O
shrunkencentroidsfit	O
for	O
some	O
code	O
.	O
let	O
us	O
give	O
an	O
example	O
of	O
the	O
method	O
in	O
action	B
,	O
based	O
on	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p652	O
)	O
.	O
consider	O
the	O
problem	O
of	O
classifying	O
a	O
gene	O
expression	O
dataset	O
,	O
which	O
2308	O
genes	O
,	O
4	O
classes	O
,	O
63	O
training	O
samples	O
and	O
20	O
test	O
samples	O
.	O
using	O
a	O
diagonal	B
lda	O
classiﬁer	O
produces	O
5	O
errors	O
on	O
the	O
test	O
set	O
.	O
using	O
the	O
nearest	B
shrunken	I
centroids	I
classiﬁer	O
produced	O
0	O
errors	O
on	O
the	O
test	O
set	O
,	O
for	O
a	O
range	O
of	O
λ	O
values	O
:	O
see	O
figure	O
4.7.	O
more	O
importantly	O
,	O
the	O
model	O
is	O
sparse	B
and	O
hence	O
more	O
interpretable	O
:	O
figure	O
4.8	O
plots	O
an	O
unpenalized	O
estimate	O
of	O
the	O
difference	O
,	O
dcj	O
,	O
in	O
gray	O
,	O
as	O
well	O
as	O
the	O
shrunken	O
(	O
these	O
estimates	O
are	O
computed	O
using	O
the	O
value	O
of	O
λ	O
estimated	O
by	O
cv	O
.	O
)	O
estimates	O
δcj	O
in	O
blue	O
.	O
we	O
see	O
that	O
only	O
39	O
genes	O
are	O
used	O
,	O
out	O
of	O
the	O
original	O
2308.	O
now	O
consider	O
an	O
even	O
harder	O
problem	O
,	O
with	O
16,603	O
genes	O
,	O
a	O
training	B
set	I
of	O
144	O
patients	O
,	O
a	O
test	O
set	O
of	O
54	O
patients	O
,	O
and	O
14	O
different	O
types	O
of	O
cancer	O
(	O
ramaswamy	O
et	O
al	O
.	O
2001	O
)	O
.	O
hastie	O
et	O
al	O
.	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p656	O
)	O
report	O
that	O
nearest	B
shrunken	I
centroids	I
produced	O
17	O
errors	O
on	O
the	O
test	O
110	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
0	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
0	O
class	O
1	O
class	O
2	O
chapter	O
4.	O
gaussian	O
models	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
0	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
0	O
500	O
1000	O
1500	O
2000	O
2500	O
(	O
a	O
)	O
class	O
3	O
500	O
1000	O
1500	O
2000	O
2500	O
500	O
1000	O
1500	O
2000	O
2500	O
(	O
b	O
)	O
class	O
4	O
500	O
1000	O
1500	O
2000	O
2500	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
4.8	O
proﬁle	O
of	O
ure	O
4.7	O
)	O
.	O
this	O
selects	O
39	O
genes	O
.	O
based	O
on	O
figure	O
18.4	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
shrunkencentroidssrbctdemo	O
.	O
the	O
shrunken	O
centroids	B
corresponding	O
to	O
λ	O
=	O
4.4	O
(	O
cv	O
optimal	O
in	O
fig-	O
figure	O
generated	O
by	O
set	O
,	O
using	O
6,520	O
genes	O
,	O
and	O
that	O
rda	O
(	O
section	O
4.2.6	O
)	O
produced	O
12	O
errors	O
on	O
the	O
test	O
set	O
,	O
using	O
all	O
16,603	O
genes	O
.	O
the	O
pmtk	O
function	O
cancerhighdimclassifdemo	O
can	O
be	O
used	O
to	O
reproduce	O
these	O
numbers	O
.	O
4.3	O
inference	B
in	O
jointly	O
gaussian	O
distributions	O
given	O
a	O
joint	B
distribution	I
,	O
p	O
(	O
x1	O
,	O
x2	O
)	O
,	O
it	O
is	O
useful	O
to	O
be	O
able	O
to	O
compute	O
marginals	O
p	O
(	O
x1	O
)	O
and	O
conditionals	O
p	O
(	O
x1|x2	O
)	O
.	O
we	O
discuss	O
how	O
to	O
do	O
this	O
below	O
,	O
and	O
then	O
give	O
some	O
applications	O
.	O
these	O
operations	O
take	O
o	O
(	O
d3	O
)	O
time	O
in	O
the	O
worst	O
case	O
.	O
see	O
section	O
20.4.3	O
for	O
faster	O
methods	O
.	O
4.3.	O
inference	B
in	O
jointly	O
gaussian	O
distributions	O
111	O
4.3.1	O
statement	O
of	O
the	O
result	O
theorem	O
4.3.1	O
(	O
marginals	O
and	O
conditionals	O
of	O
an	O
mvn	O
)	O
.	O
suppose	O
x	O
=	O
(	O
x1	O
,	O
x2	O
)	O
is	O
jointly	O
gaussian	O
with	O
parameters	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
μ	O
=	O
,	O
σ	O
=	O
σ11	O
σ12	O
σ21	O
σ22	O
,	O
λ	O
=	O
σ−1	O
=	O
λ11	O
λ12	O
λ21	O
λ22	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
μ1	O
μ2	O
then	O
the	O
marginals	O
are	O
given	O
by	O
p	O
(	O
x1	O
)	O
=n	O
(	O
x1|μ1	O
,	O
σ11	O
)	O
p	O
(	O
x2	O
)	O
=n	O
(	O
x2|μ2	O
,	O
σ22	O
)	O
and	O
the	O
posterior	O
conditional	O
is	O
given	O
by	O
p	O
(	O
x1|x2	O
)	O
=	O
n	O
(	O
x1|μ1|2	O
,	O
σ1|2	O
)	O
μ1|2	O
=	O
μ1	O
+	O
σ12σ−1	O
22	O
(	O
x2	O
−	O
μ2	O
)	O
11	O
λ12	O
(	O
x2	O
−	O
μ2	O
)	O
=	O
μ1	O
−	O
λ−1	O
=	O
σ1|2	O
(	O
λ11μ1	O
−	O
λ12	O
(	O
x2	O
−	O
μ2	O
)	O
)	O
σ1|2	O
=	O
σ11	O
−	O
σ12σ−1	O
22	O
σ21	O
=	O
λ−1	O
11	O
(	O
4.67	O
)	O
(	O
4.68	O
)	O
(	O
4.69	O
)	O
(	O
4.70	O
)	O
(	O
4.71	O
)	O
equation	O
4.69	O
is	O
of	O
such	O
crucial	O
importance	O
in	O
this	O
book	O
that	O
we	O
have	O
put	O
a	O
box	O
around	O
it	O
,	O
so	O
you	O
can	O
easily	O
ﬁnd	O
it	O
.	O
for	O
the	O
proof	O
,	O
see	O
section	O
4.3.4.	O
we	O
see	O
that	O
both	O
the	O
marginal	O
and	O
conditional	O
distributions	O
are	O
themselves	O
gaussian	O
.	O
for	O
the	O
marginals	O
,	O
we	O
just	O
extract	O
the	O
rows	O
and	O
columns	O
corresponding	O
to	O
x1	O
or	O
x2	O
.	O
for	O
the	O
conditional	O
,	O
we	O
have	O
to	O
do	O
a	O
bit	O
more	O
work	O
.	O
however	O
,	O
it	O
is	O
not	O
that	O
complicated	O
:	O
the	O
conditional	O
mean	O
is	O
just	O
a	O
linear	O
function	O
of	O
x2	O
,	O
and	O
the	O
conditional	O
covariance	O
is	O
just	O
a	O
constant	O
matrix	O
that	O
is	O
independent	O
of	O
x2	O
.	O
we	O
give	O
three	O
different	O
(	O
but	O
equivalent	O
)	O
expressions	O
for	O
the	O
posterior	B
mean	I
,	O
and	O
two	O
different	O
(	O
but	O
equivalent	O
)	O
expressions	O
for	O
the	O
posterior	O
covariance	O
;	O
each	O
one	O
is	O
useful	O
in	O
different	O
circumstances	O
.	O
4.3.2	O
examples	O
below	O
we	O
give	O
some	O
examples	O
of	O
these	O
equations	O
in	O
action	B
,	O
which	O
will	O
make	O
them	O
seem	O
more	O
intuitive	O
.	O
4.3.2.1	O
marginals	O
and	O
conditionals	O
of	O
a	O
2d	O
gaussian	O
let	O
us	O
consider	O
a	O
2d	O
example	O
.	O
the	O
covariance	B
matrix	I
is	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
σ	O
=	O
σ2	O
1	O
ρσ1σ2	O
ρσ1σ2	O
σ2	O
2	O
the	O
marginal	O
p	O
(	O
x1	O
)	O
is	O
a	O
1d	O
gaussian	O
,	O
obtained	O
by	O
projecting	O
the	O
joint	B
distribution	I
onto	O
the	O
x1	O
line	O
:	O
p	O
(	O
x1	O
)	O
=	O
n	O
(	O
x1|μ1	O
,	O
σ2	O
1	O
)	O
112	O
chapter	O
4.	O
gaussian	O
models	O
p	O
(	O
x1	O
,	O
x2	O
)	O
p	O
(	O
x1	O
)	O
p	O
(	O
x1|x2=1	O
)	O
3	O
2	O
1	O
2	O
x	O
0	O
−1	O
−2	O
−3	O
1	O
2	O
3	O
4	O
5	O
−5	O
−4	O
−3	O
−2	O
−1	O
0	O
x1	O
(	O
a	O
)	O
0.08	O
0.07	O
0.06	O
0.05	O
2	O
x	O
0.04	O
0.03	O
0.02	O
0.01	O
0	O
−5	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
2	O
x	O
−4	O
−3	O
−2	O
−1	O
0	O
x1	O
(	O
b	O
)	O
1	O
2	O
3	O
4	O
5	O
0	O
−5	O
−4	O
−3	O
−2	O
−1	O
1	O
2	O
3	O
4	O
5	O
0	O
x1	O
(	O
c	O
)	O
figure	O
4.9	O
(	O
a	O
)	O
a	O
joint	O
gaussian	O
distribution	O
p	O
(	O
x1	O
,	O
x2	O
)	O
with	O
a	O
correlation	B
coefficient	I
of	O
0.8.	O
we	O
plot	O
the	O
95	O
%	O
contour	O
and	O
the	O
principal	O
axes	O
.	O
(	O
b	O
)	O
the	O
unconditional	O
marginal	O
p	O
(	O
x1	O
)	O
.	O
(	O
c	O
)	O
the	O
conditional	O
p	O
(	O
x1|x2	O
)	O
=	O
n	O
(	O
x1|0.8	O
,	O
0.36	O
)	O
,	O
obtained	O
by	O
slicing	O
(	O
a	O
)	O
at	O
height	O
x2	O
=	O
1.	O
figure	O
generated	O
by	O
gausscondition2ddemo2	O
.	O
(	O
cid:7	O
)	O
x1|μ1	O
+	O
ρσ1σ2	O
σ2	O
2	O
1	O
−	O
(	O
ρσ1σ2	O
)	O
2	O
σ2	O
2	O
if	O
σ1	O
=	O
σ2	O
=	O
σ	O
,	O
we	O
get	O
suppose	O
we	O
observe	O
x2	O
=	O
x2	O
;	O
the	O
conditional	O
p	O
(	O
x1|x2	O
)	O
is	O
obtained	O
by	O
“	O
slicing	O
”	O
the	O
joint	B
distribution	I
through	O
the	O
x2	O
=	O
x2	O
line	O
(	O
see	O
figure	O
4.9	O
)	O
:	O
(	O
x2	O
−	O
μ2	O
)	O
,	O
σ2	O
p	O
(	O
x1|x2	O
)	O
=n	O
p	O
(	O
x1|x2	O
)	O
=n	O
(	O
cid:18	O
)	O
in	O
figure	O
4.9	O
we	O
show	O
an	O
example	O
where	O
ρ	O
=	O
0.8	O
,	O
σ1	O
=	O
σ2	O
=	O
1	O
,	O
μ	O
=	O
0	O
and	O
x2	O
=	O
1.	O
we	O
see	O
that	O
e	O
[	O
x1|x2	O
=	O
1	O
]	O
=	O
0.8	O
,	O
which	O
makes	O
sense	O
,	O
since	O
ρ	O
=	O
0.8	O
means	O
that	O
we	O
believe	O
that	O
if	O
x2	O
increases	O
by	O
1	O
(	O
beyond	O
its	O
mean	B
)	O
,	O
then	O
x1	O
increases	O
by	O
0.8.	O
we	O
also	O
see	O
var	O
[	O
x1|x2	O
=	O
1	O
]	O
=	O
1	O
−	O
0.82	O
=	O
0.36.	O
this	O
also	O
makes	O
sense	O
:	O
our	O
uncertainty	B
about	O
x1	O
has	O
gone	O
down	O
,	O
since	O
we	O
n	O
(	O
cid:18	O
)	O
if	O
ρ	O
=	O
0	O
,	O
we	O
get	O
p	O
(	O
x1|x2	O
)	O
=	O
have	O
learned	O
something	O
about	O
x1	O
(	O
indirectly	O
)	O
by	O
observing	O
x2	O
.	O
,	O
since	O
x2	O
conveys	O
no	O
information	O
about	O
x1	O
if	O
they	O
are	O
uncorrelated	O
(	O
and	O
hence	O
x1|μ1	O
+	O
ρ	O
(	O
x2	O
−	O
μ2	O
)	O
,	O
σ2	O
(	O
1	O
−	O
ρ2	O
)	O
(	O
4.72	O
)	O
(	O
4.73	O
)	O
(	O
cid:8	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
x1|μ1	O
,	O
σ2	O
1	O
independent	O
)	O
.	O
4.3.2.2	O
interpolating	O
noise-free	O
data	O
suppose	O
we	O
want	O
to	O
estimate	O
a	O
1d	O
function	O
,	O
deﬁned	O
on	O
the	O
interval	O
[	O
0	O
,	O
t	O
]	O
,	O
such	O
that	O
yi	O
=	O
f	O
(	O
ti	O
)	O
for	O
n	O
observed	O
points	O
ti	O
.	O
we	O
assume	O
for	O
now	O
that	O
the	O
data	O
is	O
noise-free	O
,	O
so	O
we	O
want	O
to	O
interpolate	B
it	O
,	O
that	O
is	O
,	O
ﬁt	O
a	O
function	O
that	O
goes	O
exactly	O
through	O
the	O
data	O
.	O
(	O
see	O
section	O
4.4.2.3	O
for	O
the	O
noisy	O
data	O
case	O
.	O
)	O
the	O
question	O
is	O
:	O
how	O
does	O
the	O
function	O
behave	O
in	O
between	O
the	O
observed	O
data	O
points	O
?	O
it	O
is	O
often	O
reasonable	O
to	O
assume	O
that	O
the	O
unknown	B
function	O
is	O
smooth	O
.	O
in	O
chapter	O
15	O
,	O
we	O
shall	O
see	O
how	O
to	O
encode	O
priors	O
over	O
functions	O
,	O
and	O
how	O
to	O
update	O
such	O
a	O
prior	O
with	O
observed	O
values	O
to	O
get	O
a	O
posterior	O
over	O
functions	O
.	O
but	O
in	O
this	O
section	O
,	O
we	O
take	O
a	O
simpler	O
approach	O
,	O
which	O
is	O
adequate	O
for	O
map	O
estimation	O
of	O
functions	O
deﬁned	O
on	O
1d	O
inputs	O
.	O
we	O
follow	O
the	O
presentation	O
of	O
(	O
calvetti	O
and	O
somersalo	O
2007	O
,	O
p135	O
)	O
.	O
we	O
start	O
by	O
discretizing	O
the	O
problem	O
.	O
first	O
we	O
divide	O
the	O
support	B
of	O
the	O
function	O
into	O
d	O
equal	O
subintervals	O
.	O
we	O
then	O
deﬁne	O
xj	O
=	O
f	O
(	O
sj	O
)	O
,	O
sj	O
=	O
jh	O
,	O
h	O
=	O
,	O
1	O
≤	O
j	O
≤	O
d	O
t	O
d	O
(	O
4.74	O
)	O
4.3.	O
inference	B
in	O
jointly	O
gaussian	O
distributions	O
113	O
λ=30	O
λ=0p1	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
0	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
(	O
a	O
)	O
0.6	O
0.7	O
0.8	O
0.9	O
1	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
0	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
(	O
b	O
)	O
0.6	O
0.7	O
0.8	O
0.9	O
1	O
figure	O
4.10	O
interpolating	O
noise-free	O
data	O
using	O
a	O
gaussian	O
with	O
prior	O
precision	B
λ	O
.	O
(	O
b	O
)	O
λ	O
=	O
0.01.	O
see	O
also	O
figure	O
4.15.	O
based	O
on	O
figure	O
7.1	O
of	O
(	O
calvetti	O
and	O
somersalo	O
2007	O
)	O
.	O
figure	O
generated	O
by	O
gaussinterpdemo	O
.	O
(	O
a	O
)	O
λ	O
=	O
30.	O
we	O
can	O
encode	O
our	O
smoothness	O
prior	O
by	O
assuming	O
that	O
xj	O
is	O
an	O
average	O
of	O
its	O
neighbors	B
,	O
xj−1	O
and	O
xj+1	O
,	O
plus	O
some	O
gaussian	O
noise	O
:	O
1	O
2	O
(	O
xj−1	O
+	O
xj+1	O
)	O
+	O
j	O
,	O
2	O
≤	O
j	O
≤	O
d	O
−	O
2	O
xj	O
=	O
(	O
4.75	O
)	O
where	O
	O
∼	O
n	O
(	O
0	O
,	O
(	O
1/λ	O
)	O
i	O
)	O
.	O
the	O
precision	B
term	O
λ	O
controls	O
how	O
much	O
we	O
think	O
the	O
function	O
will	O
vary	O
:	O
a	O
large	O
λ	O
corresponds	O
to	O
a	O
belief	O
that	O
the	O
function	O
is	O
very	O
smooth	O
,	O
a	O
small	O
λ	O
corresponds	O
to	O
a	O
belief	O
that	O
the	O
function	O
is	O
quite	O
“	O
wiggly	O
”	O
.	O
in	O
vector	O
form	O
,	O
the	O
above	O
equation	O
can	O
be	O
written	O
as	O
follows	O
:	O
where	O
l	O
is	O
the	O
(	O
d	O
−	O
2	O
)	O
×	O
d	O
second	B
order	I
ﬁnite	O
difference	O
matrix	O
(	O
4.76	O
)	O
(	O
4.77	O
)	O
(	O
4.78	O
)	O
lx	O
=	O
	O
⎛	O
⎜⎜⎜⎝	O
l	O
=	O
1	O
2	O
−1	O
2−	O
1	O
−1	O
2−	O
1	O
.	O
.	O
.	O
−1	O
the	O
corresponding	O
prior	O
has	O
the	O
form	O
−1	O
)	O
∝	O
exp	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x|0	O
,	O
(	O
λ2lt	O
l	O
)	O
⎞	O
⎟⎟⎟⎠	O
2−	O
1	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
||lx||2	O
2	O
−	O
λ2	O
2	O
we	O
will	O
henceforth	O
assume	O
we	O
have	O
scaled	O
l	O
by	O
λ	O
so	O
we	O
can	O
ignore	O
the	O
λ	O
term	O
,	O
and	O
just	O
write	O
λ	O
=	O
lt	O
l	O
for	O
the	O
precision	B
matrix	I
.	O
note	O
that	O
although	O
x	O
is	O
d-dimensional	O
,	O
the	O
precision	B
matrix	I
λ	O
only	O
has	O
rank	O
d	O
−	O
2.	O
thus	O
this	O
is	O
an	O
improper	B
prior	I
,	O
known	O
as	O
an	O
intrinsic	O
gaussian	O
random	O
ﬁeld	O
(	O
see	O
section	O
19.4.4	O
for	O
114	O
chapter	O
4.	O
gaussian	O
models	O
more	O
information	B
)	O
.	O
however	O
,	O
providing	O
we	O
observe	O
n	O
≥	O
2	O
data	O
points	O
,	O
the	O
posterior	O
will	O
be	O
proper	O
.	O
now	O
let	O
x2	O
be	O
the	O
n	O
noise-free	O
observations	O
of	O
the	O
function	O
,	O
and	O
x1	O
be	O
the	O
d−	O
n	O
unknown	B
function	O
values	O
.	O
without	O
loss	B
of	O
generality	O
,	O
assume	O
that	O
the	O
unknown	B
variables	O
are	O
ordered	O
ﬁrst	O
,	O
then	O
the	O
known	O
variables	O
.	O
then	O
we	O
can	O
partition	O
the	O
l	O
matrix	O
as	O
follows	O
:	O
l	O
=	O
[	O
l1	O
,	O
l2	O
]	O
,	O
l1	O
∈	O
r	O
(	O
d−2	O
)	O
×	O
(	O
d−n	O
)	O
,	O
l2	O
∈	O
r	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
(	O
d−2	O
)	O
×	O
(	O
n	O
)	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
we	O
can	O
also	O
partition	O
the	O
precision	B
matrix	I
of	O
the	O
joint	B
distribution	I
:	O
λ	O
=	O
lt	O
l	O
=	O
λ11	O
λ12	O
λ21	O
λ22	O
=	O
lt	O
lt	O
1	O
l1	O
lt	O
2	O
l1	O
lt	O
1	O
l2	O
2	O
l2	O
(	O
4.79	O
)	O
(	O
4.80	O
)	O
(	O
4.81	O
)	O
(	O
4.82	O
)	O
(	O
4.83	O
)	O
(	O
4.84	O
)	O
using	O
equation	O
4.69	O
,	O
we	O
can	O
write	O
the	O
conditional	O
distribution	O
as	O
follows	O
:	O
p	O
(	O
x1|x2	O
)	O
=n	O
(	O
μ1|2	O
,	O
σ1|2	O
)	O
μ1|2	O
=	O
−λ−1	O
σ1|2	O
=	O
λ−1	O
11	O
11	O
λ12x2	O
=	O
−lt	O
1	O
l2x2	O
note	O
that	O
we	O
can	O
compute	O
the	O
mean	B
by	O
solving	O
the	O
following	O
system	O
of	O
linear	O
equations	O
:	O
l1μ1|2	O
=	O
−l2x2	O
this	O
is	O
efficient	O
since	O
l1	O
is	O
tridiagonal	B
.	O
figure	O
4.10	O
gives	O
an	O
illustration	O
of	O
these	O
equations	O
.	O
we	O
see	O
that	O
the	O
posterior	B
mean	I
μ1|2	O
equals	O
the	O
observed	O
data	O
at	O
the	O
speciﬁed	O
points	O
,	O
and	O
smoothly	O
interpolates	O
in	O
between	O
,	O
as	O
desired	O
.	O
(	O
cid:9	O
)	O
it	O
is	O
also	O
interesting	O
to	O
plot	O
the	O
95	O
%	O
pointwise	B
marginal	I
credibility	I
intervals	I
,	O
μj	O
±	O
σ1|2	O
,	O
jj	O
,	O
shown	O
in	O
grey	O
.	O
we	O
see	O
that	O
the	O
variance	B
goes	O
up	O
as	O
we	O
move	O
away	O
from	O
the	O
2	O
data	O
.	O
we	O
also	O
see	O
that	O
the	O
variance	B
goes	O
up	O
as	O
we	O
decrease	O
the	O
precision	B
of	O
the	O
prior	O
,	O
λ.	O
in-	O
terestingly	O
,	O
λ	O
has	O
no	O
effect	O
on	O
the	O
posterior	B
mean	I
,	O
since	O
it	O
cancels	O
out	O
when	O
multiplying	O
λ11	O
and	O
λ12	O
.	O
by	O
contrast	O
,	O
when	O
we	O
consider	O
noisy	O
data	O
in	O
section	O
4.4.2.3	O
,	O
we	O
will	O
see	O
that	O
the	O
prior	O
precision	B
affects	O
the	O
smoothness	O
of	O
posterior	B
mean	I
estimate	O
.	O
the	O
marginal	O
credibility	O
intervals	O
do	O
not	O
capture	O
the	O
fact	O
that	O
neighboring	O
locations	O
are	O
correlated	O
.	O
we	O
can	O
represent	O
that	O
by	O
drawing	O
complete	B
functions	O
(	O
i.e.	O
,	O
vectors	O
x	O
)	O
from	O
the	O
posterior	O
,	O
and	O
plotting	O
them	O
.	O
these	O
are	O
shown	O
by	O
the	O
thin	O
lines	O
in	O
figure	O
4.10.	O
these	O
are	O
not	O
quite	O
as	O
smooth	O
as	O
the	O
posterior	B
mean	I
itself	O
.	O
this	O
is	O
because	O
the	O
prior	O
only	O
penalizes	O
ﬁrst-order	O
differences	O
.	O
see	O
section	O
4.4.2.3	O
for	O
further	O
discussion	O
of	O
this	O
point	O
.	O
4.3.2.3	O
data	O
imputation	O
suppose	O
we	O
are	O
missing	B
some	O
entries	O
in	O
a	O
design	B
matrix	I
.	O
if	O
the	O
columns	O
are	O
correlated	O
,	O
we	O
can	O
use	O
the	O
observed	O
entries	O
to	O
predict	O
the	O
missing	B
entries	O
.	O
figure	O
4.11	O
shows	O
a	O
simple	O
example	O
.	O
we	O
sampled	O
some	O
data	O
from	O
a	O
20	O
dimensional	O
gaussian	O
,	O
and	O
then	O
deliberately	O
“	O
hid	O
”	O
50	O
%	O
of	O
the	O
data	O
in	O
each	O
row	O
.	O
we	O
then	O
inferred	O
the	O
missing	B
entries	O
given	O
the	O
observed	O
entries	O
,	O
using	O
the	O
true	O
(	O
generating	O
)	O
model	O
.	O
more	O
precisely	O
,	O
for	O
each	O
row	O
i	O
,	O
we	O
compute	O
p	O
(	O
xhi	O
,	O
θ	O
)	O
,	O
where	O
hi	O
and	O
vi	O
are	O
the	O
indices	O
of	O
the	O
hidden	B
and	O
visible	B
entries	O
in	O
case	O
i.	O
from	O
this	O
,	O
we	O
compute	O
the	O
marginal	O
,	O
θ	O
)	O
.	O
we	O
then	O
plot	O
the	O
mean	B
of	O
this	O
distribution	O
,	O
distribution	O
of	O
each	O
missing	B
variable	O
,	O
p	O
(	O
xhij	O
ˆxij	O
=	O
e	O
[	O
xj|xvi	O
,	O
θ	O
]	O
;	O
this	O
represents	O
our	O
“	O
best	O
guess	O
”	O
about	O
the	O
true	O
value	O
of	O
that	O
entry	O
,	O
in	O
the	O
|xvi	O
|xvi	O
4.3.	O
inference	B
in	O
jointly	O
gaussian	O
distributions	O
115	O
observed	O
10	O
5	O
0	O
−5	O
imputed	O
10	O
5	O
0	O
−5	O
truth	O
10	O
5	O
0	O
−5	O
−10	O
0	O
5	O
10	O
15	O
20	O
−10	O
0	O
5	O
10	O
15	O
20	O
−10	O
0	O
5	O
10	O
15	O
20	O
figure	O
4.11	O
illustration	O
of	O
data	O
imputation	O
.	O
left	O
column	O
:	O
visualization	O
of	O
three	O
rows	O
of	O
the	O
data	O
matrix	O
with	O
missing	B
entries	O
.	O
middle	O
column	O
:	O
mean	B
of	O
the	O
posterior	O
predictive	O
,	O
based	O
on	O
partially	O
observed	O
data	O
in	O
that	O
row	O
,	O
but	O
the	O
true	O
model	O
parameters	O
.	O
right	O
column	O
:	O
figure	O
generated	O
by	O
gaussimputationdemo	O
.	O
true	O
values	O
.	O
(	O
cid:20	O
)	O
xhij|xvi	O
,	O
θ	O
sense	O
that	O
it	O
minimizes	O
our	O
expected	O
squared	O
error	O
(	O
see	O
section	O
5.7	O
for	O
details	O
)	O
.	O
figure	O
4.11	O
shows	O
that	O
the	O
estimates	O
are	O
quite	O
close	O
to	O
the	O
truth	O
.	O
(	O
of	O
course	O
,	O
if	O
j	O
∈	O
vi	O
,	O
the	O
expected	B
value	I
is	O
equal	O
(	O
cid:21	O
)	O
to	O
the	O
observed	O
value	O
,	O
ˆxij	O
=	O
xij	O
.	O
)	O
we	O
can	O
use	O
var	O
as	O
a	O
measure	O
of	O
conﬁdence	O
in	O
this	O
guess	O
,	O
although	O
this	O
is	O
not	O
,	O
θ	O
)	O
;	O
this	O
is	O
called	O
multiple	O
shown	O
.	O
alternatively	O
,	O
we	O
could	O
draw	O
multiple	O
samples	O
from	O
p	O
(	O
xhi	O
imputation	B
.	O
in	O
addition	O
to	O
imputing	O
the	O
missing	B
entries	O
,	O
we	O
may	O
be	O
interested	O
in	O
computing	O
the	O
like-	O
lihood	O
of	O
each	O
partially	O
observed	O
row	O
in	O
the	O
table	O
,	O
p	O
(	O
xvi|θ	O
)	O
,	O
which	O
can	O
be	O
computed	O
using	O
equation	O
4.68.	O
this	O
is	O
useful	O
for	O
detecting	O
outliers	B
(	O
atypical	O
observations	O
)	O
.	O
|xvi	O
4.3.3	O
information	B
form	I
suppose	O
x	O
∼	O
n	O
(	O
μ	O
,	O
σ	O
)	O
.	O
one	O
can	O
show	O
that	O
e	O
[	O
x	O
]	O
=μ	O
is	O
the	O
mean	B
vector	O
,	O
and	O
cov	O
[	O
x	O
]	O
=σ	O
is	O
the	O
covariance	B
matrix	I
.	O
these	O
are	O
called	O
the	O
moment	B
parameters	I
of	O
the	O
distribution	O
.	O
however	O
,	O
it	O
is	O
sometimes	O
useful	O
to	O
use	O
the	O
canonical	B
parameters	I
or	O
natural	B
parameters	I
,	O
deﬁned	O
as	O
λ	O
(	O
cid:2	O
)	O
σ−1	O
,	O
ξ	O
(	O
cid:2	O
)	O
σ−1μ	O
we	O
can	O
convert	O
back	O
to	O
the	O
moment	B
parameters	I
using	O
μ	O
=	O
λ−1ξ	O
,	O
σ	O
=	O
λ−1	O
(	O
4.85	O
)	O
(	O
4.86	O
)	O
using	O
the	O
canonical	B
parameters	I
,	O
we	O
can	O
write	O
the	O
mvn	O
in	O
information	B
form	I
(	O
i.e.	O
,	O
in	O
exponential	B
family	I
form	O
,	O
deﬁned	O
in	O
section	O
9.2	O
)	O
:	O
−d/2|λ|	O
1	O
(	O
xt	O
λx	O
+	O
ξt	O
λ−1ξ	O
−	O
2xt	O
ξ	O
)	O
nc	O
(	O
x|ξ	O
,	O
λ	O
)	O
=	O
(	O
2π	O
)	O
2	O
exp	O
(	O
cid:2	O
)	O
(	O
cid:3	O
)	O
(	O
4.87	O
)	O
where	O
we	O
use	O
the	O
notation	O
nc	O
(	O
)	O
to	O
distinguish	O
from	O
the	O
moment	O
parameterization	O
n	O
(	O
)	O
.	O
it	O
is	O
also	O
possible	O
to	O
derive	O
the	O
marginalization	O
and	O
conditioning	B
formulas	O
in	O
information	B
form	I
.	O
we	O
ﬁnd	O
p	O
(	O
x2	O
)	O
=n	O
c	O
(	O
x2|ξ2	O
−	O
λ21λ−1	O
p	O
(	O
x1|x2	O
)	O
=n	O
c	O
(	O
x1|ξ1	O
−	O
λ12x2	O
,	O
λ11	O
)	O
11	O
ξ1	O
,	O
λ22	O
−	O
λ21λ−1	O
11	O
λ12	O
)	O
(	O
4.88	O
)	O
(	O
4.89	O
)	O
−	O
1	O
2	O
116	O
chapter	O
4.	O
gaussian	O
models	O
thus	O
we	O
see	O
that	O
marginalization	O
is	O
easier	O
in	O
moment	O
form	O
,	O
and	O
conditioning	B
is	O
easier	O
in	O
information	B
form	I
.	O
another	O
operation	O
that	O
is	O
signiﬁcantly	O
easier	O
in	O
information	B
form	I
is	O
multiplying	O
two	O
gaussians	O
.	O
one	O
can	O
show	O
that	O
nc	O
(	O
ξf	O
,	O
λf	O
)	O
nc	O
(	O
ξg	O
,	O
λg	O
)	O
=n	O
c	O
(	O
ξf	O
+	O
ξg	O
,	O
λf	O
+	O
λg	O
)	O
however	O
,	O
in	O
moment	O
form	O
,	O
things	O
are	O
much	O
messier	O
:	O
(	O
cid:13	O
)	O
n	O
(	O
μf	O
,	O
σ2	O
f	O
)	O
n	O
(	O
μg	O
,	O
σ2	O
g	O
)	O
=	O
n	O
g	O
+	O
μgσ2	O
μf	O
σ2	O
f	O
σ2	O
g	O
+	O
σ2	O
g	O
,	O
σ2	O
f	O
σ2	O
g	O
g	O
+	O
σ2	O
σ2	O
g	O
(	O
cid:14	O
)	O
(	O
4.90	O
)	O
(	O
4.91	O
)	O
4.3.4	O
proof	O
of	O
the	O
result	O
*	O
we	O
now	O
prove	O
theorem	O
4.3.1.	O
readers	O
who	O
are	O
intimidated	O
by	O
heavy	O
matrix	O
algebra	O
can	O
safely	O
skip	O
this	O
section	O
.	O
we	O
ﬁrst	O
derive	O
some	O
results	O
that	O
we	O
will	O
need	O
here	O
and	O
elsewhere	O
in	O
the	O
book	O
.	O
we	O
will	O
return	O
to	O
the	O
proof	O
at	O
the	O
end	O
.	O
4.3.4.1	O
inverse	O
of	O
a	O
partitioned	O
matrix	O
using	O
schur	O
complements	O
the	O
key	O
tool	O
we	O
need	O
is	O
a	O
way	O
to	O
invert	O
a	O
partitioned	O
matrix	O
.	O
this	O
can	O
be	O
done	O
using	O
the	O
following	O
result	O
.	O
theorem	O
4.3.2	O
(	O
inverse	O
of	O
a	O
partitioned	O
matrix	O
)	O
.	O
consider	O
a	O
general	O
partitioned	O
matrix	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
m	O
=	O
e	O
f	O
g	O
h	O
where	O
we	O
assume	O
e	O
and	O
h	O
are	O
invertible	O
.	O
we	O
have	O
m−1	O
=	O
=	O
(	O
m/h	O
)	O
(	O
cid:7	O
)	O
−1	O
−h−1g	O
(	O
m/h	O
)	O
(	O
cid:7	O
)	O
e−1	O
+	O
e−1f	O
(	O
m/e	O
)	O
−	O
(	O
m/e	O
)	O
−	O
(	O
m/h	O
)	O
−1fh−1	O
−1	O
h−1	O
+	O
h−1g	O
(	O
m/h	O
)	O
−1ge−1	O
−e−1f	O
(	O
m/e	O
)	O
−1	O
(	O
m/e	O
)	O
−1ge−1	O
(	O
cid:8	O
)	O
−1fh−1	O
−1	O
where	O
m/h	O
(	O
cid:2	O
)	O
e	O
−	O
fh−1g	O
m/e	O
(	O
cid:2	O
)	O
h	O
−	O
ge−1f	O
(	O
cid:8	O
)	O
(	O
4.92	O
)	O
(	O
4.93	O
)	O
(	O
4.94	O
)	O
(	O
4.95	O
)	O
(	O
4.96	O
)	O
we	O
say	O
that	O
m/h	O
is	O
the	O
schur	O
complement	O
of	O
m	O
wrt	O
h.	O
equation	O
4.93	O
is	O
called	O
the	O
partitioned	B
inverse	I
formula	I
.	O
proof	O
.	O
if	O
we	O
could	O
block	O
diagonalize	O
m	O
,	O
it	O
would	O
be	O
easier	O
to	O
invert	O
.	O
to	O
zero	O
out	O
the	O
top	O
right	O
block	O
of	O
m	O
we	O
can	O
pre-multiply	O
as	O
follows	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
i	O
−fh−1	O
0	O
i	O
e	O
f	O
g	O
h	O
=	O
e	O
−	O
fh−1g	O
0	O
h	O
g	O
(	O
cid:8	O
)	O
(	O
4.97	O
)	O
similarly	O
,	O
to	O
zero	O
out	O
the	O
bottom	O
left	O
we	O
can	O
post-multiply	O
as	O
follows	O
4.3.	O
inference	B
in	O
jointly	O
gaussian	O
distributions	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
!	O
e	O
−	O
fh−1g	O
0	O
h	O
g	O
putting	O
it	O
all	O
together	O
we	O
get	O
i	O
−fh−1	O
0	O
i	O
''	O
#	O
(	O
cid:8	O
)	O
$	O
(	O
cid:7	O
)	O
!	O
e	O
f	O
g	O
h	O
''	O
#	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
=	O
i	O
0	O
−h−1g	O
i	O
(	O
cid:8	O
)	O
$	O
(	O
cid:7	O
)	O
!	O
``	O
#	O
i	O
0	O
−h−1g	O
i	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
$	O
(	O
cid:8	O
)	O
e	O
−	O
fh−1g	O
0	O
h	O
0	O
(	O
cid:7	O
)	O
!	O
=	O
e	O
−	O
fh−1g	O
0	O
h	O
0	O
''	O
#	O
(	O
cid:8	O
)	O
$	O
x	O
m	O
z	O
w	O
taking	O
the	O
inverse	O
of	O
both	O
sides	O
yields	O
z−1m−1x−1	O
=	O
w−1	O
and	O
hence	O
substituting	O
in	O
the	O
deﬁnitions	O
we	O
get	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
−1	O
e	O
f	O
g	O
h	O
m−1	O
=	O
zw−1x	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
−1	O
−h−1g	O
(	O
m/h	O
)	O
(	O
cid:7	O
)	O
−1	O
−h−1g	O
(	O
m/h	O
)	O
0	O
−h−1g	O
i	O
(	O
m/h	O
)	O
(	O
m/h	O
)	O
=	O
=	O
i	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
(	O
m/h	O
)	O
0	O
0	O
−1	O
h−1	O
−1	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
i	O
−fh−1	O
0	O
i	O
0	O
h−1	O
i	O
−fh−1	O
0	O
(	O
cid:8	O
)	O
−	O
(	O
m/h	O
)	O
i	O
−1fh−1	O
=	O
(	O
4.104	O
)	O
alternatively	O
,	O
we	O
could	O
have	O
decomposed	O
the	O
matrix	O
m	O
in	O
terms	O
of	O
e	O
and	O
m/e	O
=	O
(	O
h	O
−	O
ge−1f	O
)	O
,	O
yielding	O
(	O
cid:8	O
)	O
−1	O
−1	O
h−1	O
+	O
h−1g	O
(	O
m/h	O
)	O
−1fh−1	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
e−1	O
+	O
e−1f	O
(	O
m/e	O
)	O
−	O
(	O
m/e	O
)	O
−1ge−1	O
−1ge−1	O
−e−1f	O
(	O
m/e	O
)	O
−1	O
−1	O
(	O
m/e	O
)	O
=	O
e	O
f	O
g	O
h	O
117	O
(	O
4.98	O
)	O
(	O
4.99	O
)	O
(	O
4.100	O
)	O
(	O
4.101	O
)	O
(	O
4.102	O
)	O
(	O
4.103	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
4.105	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
e	O
f	O
g	O
h	O
,	O
(	O
4.106	O
)	O
(	O
4.107	O
)	O
(	O
4.108	O
)	O
4.3.4.2	O
the	O
matrix	B
inversion	I
lemma	I
we	O
now	O
derive	O
some	O
useful	O
corollaries	O
of	O
the	O
above	O
result	O
.	O
corollary	O
4.3.1	O
(	O
matrix	B
inversion	I
lemma	I
)	O
.	O
consider	O
a	O
general	O
partitioned	O
matrix	O
m	O
=	O
where	O
we	O
assume	O
e	O
and	O
h	O
are	O
invertible	O
.	O
we	O
have	O
(	O
e	O
−	O
fh−1g	O
)	O
−1	O
=	O
e−1	O
+	O
e−1f	O
(	O
h	O
−	O
ge−1f	O
)	O
−1ge−1	O
(	O
e	O
−	O
fh−1g	O
)	O
−1fh−1	O
=	O
e−1f	O
(	O
h	O
−	O
ge−1f	O
)	O
−1	O
|e	O
−	O
fh−1g|	O
=	O
|h	O
−	O
ge−1f||h−1||e|	O
118	O
chapter	O
4.	O
gaussian	O
models	O
the	O
ﬁrst	O
two	O
equations	O
are	O
s	O
known	O
as	O
the	O
matrix	B
inversion	I
lemma	I
or	O
the	O
sherman-	O
morrison-woodbury	O
formula	O
.	O
the	O
third	O
equation	O
is	O
known	O
as	O
the	O
matrix	B
determinant	I
lemma	I
.	O
a	O
typical	O
application	O
in	O
machine	O
learning/	O
statistics	O
is	O
the	O
following	O
.	O
let	O
e	O
=	O
σ	O
be	O
a	O
n	O
×	O
n	O
diagonal	B
matrix	O
,	O
let	O
f	O
=	O
gt	O
=	O
x	O
of	O
size	O
n	O
×	O
d	O
,	O
where	O
n	O
(	O
cid:8	O
)	O
d	O
,	O
and	O
let	O
h−1	O
=	O
−i	O
.	O
then	O
we	O
have	O
(	O
σ	O
+	O
xxt	O
)	O
−1	O
=	O
σ−1	O
−	O
σ−1x	O
(	O
i	O
+	O
xt	O
σ−1x	O
)	O
−1xt	O
σ−1	O
(	O
4.109	O
)	O
the	O
lhs	O
takes	O
o	O
(	O
n	O
3	O
)	O
time	O
to	O
compute	O
,	O
the	O
rhs	O
takes	O
time	O
o	O
(	O
d3	O
)	O
to	O
compute	O
.	O
another	O
application	O
concerns	O
computing	O
a	O
rank	B
one	I
update	I
of	O
an	O
inverse	O
matrix	O
.	O
h	O
=	O
−1	O
(	O
a	O
scalar	O
)	O
,	O
f	O
=	O
u	O
(	O
a	O
column	O
vector	O
)	O
,	O
and	O
g	O
=	O
vt	O
(	O
a	O
row	O
vector	O
)	O
.	O
then	O
we	O
have	O
let	O
(	O
e	O
+	O
uvt	O
)	O
−1	O
=	O
e−1	O
+	O
e−1u	O
(	O
−1	O
−	O
vt	O
e−1u	O
)	O
=	O
e−1	O
−	O
e−1uvt	O
e−1	O
1	O
+	O
vt	O
e−1u	O
−1vt	O
e−1	O
(	O
4.110	O
)	O
(	O
4.111	O
)	O
this	O
is	O
useful	O
when	O
we	O
incrementally	O
add	O
a	O
data	O
vector	O
to	O
a	O
design	B
matrix	I
,	O
and	O
want	O
to	O
update	O
our	O
sufficient	B
statistics	I
.	O
(	O
one	O
can	O
derive	O
an	O
analogous	O
formula	O
for	O
removing	O
a	O
data	O
vector	O
.	O
)	O
proof	O
.	O
to	O
prove	O
equation	O
4.106	O
,	O
we	O
simply	O
equate	O
the	O
top	O
left	O
block	O
of	O
equation	O
4.93	O
and	O
equa-	O
tion	O
4.94.	O
to	O
prove	O
equation	O
4.107	O
,	O
we	O
simple	O
equate	O
the	O
top	O
right	O
blocks	O
of	O
equations	O
4.93	O
and	O
4.94.	O
the	O
proof	O
of	O
equation	O
4.108	O
is	O
left	O
as	O
an	O
exercise	O
.	O
4.3.4.3	O
proof	O
of	O
gaussian	O
conditioning	B
formulas	O
we	O
can	O
now	O
return	O
to	O
our	O
original	O
goal	O
,	O
which	O
is	O
to	O
derive	O
equation	O
4.69.	O
let	O
us	O
factor	B
the	O
joint	O
p	O
(	O
x1	O
,	O
x2	O
)	O
as	O
p	O
(	O
x2	O
)	O
p	O
(	O
x1|x2	O
)	O
as	O
follows	O
:	O
(	O
cid:8	O
)	O
t	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
−1	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
&	O
%	O
e	O
=	O
exp	O
e	O
=	O
exp	O
using	O
equation	O
4.102	O
the	O
above	O
exponent	O
becomes	O
σ11	O
σ12	O
σ21	O
σ22	O
−	O
1	O
2	O
(	O
cid:7	O
)	O
x1	O
−	O
μ1	O
x2	O
−	O
μ2	O
%	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
t	O
(	O
cid:7	O
)	O
x1	O
−	O
μ1	O
−	O
1	O
x2	O
−	O
μ2	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
2	O
i	O
−σ12σ−1	O
(	O
cid:5	O
)	O
0	O
i	O
22	O
i	O
−σ−1	O
(	O
cid:8	O
)	O
’	O
22	O
σ21	O
x1	O
−	O
μ1	O
x2	O
−	O
μ2	O
(	O
x1	O
−	O
μ1	O
−	O
σ12σ−1	O
22	O
(	O
x2	O
−	O
μ2	O
)	O
)	O
(	O
x1	O
−	O
μ1	O
−	O
σ12σ−1	O
×	O
=	O
exp	O
−	O
1	O
2	O
this	O
is	O
of	O
the	O
form	O
exp	O
(	O
quadratic	O
form	O
in	O
x1	O
,	O
x2	O
)	O
×	O
exp	O
(	O
quadratic	O
form	O
in	O
x2	O
)	O
x1	O
−	O
μ1	O
x2	O
−	O
μ2	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
0	O
i	O
(	O
σ/σ22	O
)	O
0	O
−1	O
(	O
cid:8	O
)	O
0	O
σ−1	O
22	O
22	O
(	O
x2	O
−	O
μ2	O
)	O
)	O
t	O
(	O
σ/σ22	O
)	O
−1	O
(	O
cid:5	O
)	O
(	O
×	O
exp	O
(	O
x2	O
−	O
μ2	O
)	O
t	O
σ−1	O
22	O
(	O
x2	O
−	O
μ2	O
)	O
−	O
1	O
2	O
(	O
4.112	O
)	O
(	O
4.113	O
)	O
(	O
4.114	O
)	O
(	O
4.115	O
)	O
(	O
4.116	O
)	O
(	O
4.117	O
)	O
’	O
4.4.	O
linear	O
gaussian	O
systems	O
hence	O
we	O
have	O
successfully	O
factorized	O
the	O
joint	O
as	O
p	O
(	O
x1	O
,	O
x2	O
)	O
=p	O
(	O
x1|x2	O
)	O
p	O
(	O
x2	O
)	O
=	O
n	O
(	O
x1|μ1|2	O
,	O
σ1|2	O
)	O
n	O
(	O
x2|μ2	O
,	O
σ22	O
)	O
119	O
(	O
4.118	O
)	O
(	O
4.119	O
)	O
where	O
the	O
parameters	O
of	O
the	O
conditional	O
distribution	O
can	O
be	O
read	O
off	O
from	O
the	O
above	O
equations	O
using	O
22	O
(	O
x2	O
−	O
μ2	O
)	O
μ1|2	O
=	O
μ1	O
+	O
σ12σ−1	O
σ1|2	O
=	O
σ/σ22	O
=	O
σ11	O
−	O
σ12σ−1	O
(	O
4.121	O
)	O
we	O
can	O
also	O
use	O
the	O
fact	O
that	O
|m|	O
=	O
|m/h||h|	O
to	O
check	O
the	O
normalization	O
constants	O
are	O
22	O
σ21	O
(	O
4.120	O
)	O
correct	O
:	O
(	O
2π	O
)	O
(	O
d1+d2	O
)	O
/2|σ|	O
1	O
2	O
=	O
(	O
2π	O
)	O
(	O
d1+d2	O
)	O
/2	O
(	O
|σ/σ22|	O
|σ22|	O
)	O
1	O
2	O
=	O
(	O
2π	O
)	O
d1/2|σ/σ22|	O
1	O
2	O
(	O
2π	O
)	O
d2/2|σ22|	O
1	O
2	O
(	O
4.122	O
)	O
(	O
4.123	O
)	O
where	O
d1	O
=	O
dim	O
(	O
x1	O
)	O
and	O
d2	O
=	O
dim	O
(	O
x2	O
)	O
.	O
we	O
leave	O
the	O
proof	O
of	O
the	O
other	O
forms	O
of	O
the	O
result	O
in	O
equation	O
4.69	O
as	O
an	O
exercise	O
.	O
4.4	O
linear	O
gaussian	O
systems	O
suppose	O
we	O
have	O
two	O
variables	O
,	O
x	O
and	O
y.	O
let	O
x	O
∈	O
r	O
a	O
noisy	O
observation	B
of	O
x.	O
let	O
us	O
assume	O
we	O
have	O
the	O
following	O
prior	O
and	O
likelihood	B
:	O
dx	O
be	O
a	O
hidden	B
variable	I
,	O
and	O
y	O
∈	O
r	O
dy	O
be	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x|μx	O
,	O
σx	O
)	O
p	O
(	O
y|x	O
)	O
=	O
n	O
(	O
y|ax	O
+	O
b	O
,	O
σy	O
)	O
(	O
4.124	O
)	O
where	O
a	O
is	O
a	O
matrix	O
of	O
size	O
dy	O
×	O
dx	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
linear	O
gaussian	O
system	O
.	O
we	O
can	O
represent	O
this	O
schematically	O
as	O
x	O
→	O
y	O
,	O
meaning	O
x	O
generates	O
y.	O
in	O
this	O
section	O
,	O
we	O
show	O
how	O
to	O
“	O
invert	O
the	O
arrow	O
”	O
,	O
that	O
is	O
,	O
how	O
to	O
infer	O
x	O
from	O
y.	O
we	O
state	B
the	O
result	O
below	O
,	O
then	O
give	O
several	O
examples	O
,	O
and	O
ﬁnally	O
we	O
derive	O
the	O
result	O
.	O
we	O
will	O
see	O
many	O
more	O
applications	O
of	O
these	O
results	O
in	O
later	O
chapters	O
.	O
4.4.1	O
statement	O
of	O
the	O
result	O
theorem	O
4.4.1	O
(	O
bayes	O
rule	O
for	O
linear	O
gaussian	O
systems	O
)	O
.	O
given	O
a	O
linear	O
gaussian	O
system	O
,	O
as	O
in	O
equation	O
4.124	O
,	O
the	O
posterior	O
p	O
(	O
x|y	O
)	O
is	O
given	O
by	O
the	O
following	O
:	O
p	O
(	O
x|y	O
)	O
=	O
n	O
(	O
x|μx|y	O
,	O
σx|y	O
)	O
σ−1	O
x	O
+	O
at	O
σ−1	O
x|y	O
=	O
σ−1	O
y	O
a	O
(	O
y	O
−	O
b	O
)	O
+σ	O
−1	O
μx|y	O
=	O
σx|y	O
[	O
at	O
σ−1	O
y	O
x	O
μx	O
]	O
(	O
4.125	O
)	O
120	O
chapter	O
4.	O
gaussian	O
models	O
in	O
addition	O
,	O
the	O
normalization	O
constant	O
p	O
(	O
y	O
)	O
is	O
given	O
by	O
p	O
(	O
y	O
)	O
=	O
n	O
(	O
y|aμx	O
+	O
b	O
,	O
σy	O
+	O
aσxat	O
)	O
(	O
4.126	O
)	O
for	O
the	O
proof	O
,	O
see	O
section	O
4.4.3	O
.	O
4.4.2	O
examples	O
in	O
this	O
section	O
,	O
we	O
give	O
some	O
example	O
applications	O
of	O
the	O
above	O
result	O
.	O
4.4.2.1	O
inferring	O
an	O
unknown	B
scalar	O
from	O
noisy	O
measurements	O
suppose	O
we	O
make	O
n	O
noisy	O
measurements	O
yi	O
of	O
some	O
underlying	O
quantity	O
x	O
;	O
let	O
us	O
assume	O
the	O
measurement	O
noise	O
has	O
ﬁxed	O
precision	O
λy	O
=	O
1/σ2	O
,	O
so	O
the	O
likelihood	B
is	O
p	O
(	O
yi|x	O
)	O
=n	O
(	O
yi|x	O
,	O
λ−1	O
y	O
)	O
(	O
4.127	O
)	O
now	O
let	O
us	O
use	O
a	O
gaussian	O
prior	O
for	O
the	O
value	O
of	O
the	O
unknown	B
source	O
:	O
p	O
(	O
x	O
)	O
=n	O
(	O
x|μ0	O
,	O
λ−1	O
0	O
)	O
(	O
4.128	O
)	O
we	O
want	O
to	O
compute	O
p	O
(	O
x|y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
,	O
σ2	O
)	O
.	O
we	O
can	O
convert	O
this	O
to	O
a	O
form	O
that	O
lets	O
us	O
apply	O
n	O
(	O
an	O
1	O
×	O
n	O
row	O
vector	O
of	O
1	O
’	O
s	O
)	O
,	O
bayes	O
rule	O
for	O
gaussians	O
by	O
deﬁning	O
y	O
=	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
)	O
,	O
a	O
=	O
1t	O
and	O
σ−1	O
y	O
=	O
diag	O
(	O
λyi	O
)	O
.	O
then	O
we	O
get	O
p	O
(	O
x|y	O
)	O
=n	O
(	O
x|μn	O
,	O
λ−1	O
n	O
)	O
λn	O
=	O
λ0	O
+	O
n	O
λy	O
μn	O
=	O
n	O
λyy	O
+	O
λ0μ0	O
λn	O
=	O
n	O
λy	O
n	O
λy	O
+	O
λ0	O
y	O
+	O
λ0	O
n	O
λy	O
+	O
λ0	O
μ0	O
(	O
4.129	O
)	O
(	O
4.130	O
)	O
(	O
4.131	O
)	O
these	O
equations	O
are	O
quite	O
intuitive	O
:	O
the	O
posterior	O
precision	O
λn	O
is	O
the	O
prior	O
precision	B
λ0	O
plus	O
n	O
units	O
of	O
measurement	O
precision	B
λy	O
.	O
also	O
,	O
the	O
posterior	B
mean	I
μn	O
is	O
a	O
convex	B
combination	I
of	O
the	O
mle	O
y	O
and	O
the	O
prior	O
mean	B
μ0	O
.	O
this	O
makes	O
it	O
clear	O
that	O
the	O
posterior	B
mean	I
is	O
a	O
compromise	O
if	O
the	O
prior	O
is	O
weak	O
relative	O
to	O
the	O
signal	O
strength	O
(	O
λ0	O
is	O
between	O
the	O
mle	O
and	O
the	O
prior	O
.	O
small	O
relative	O
to	O
λy	O
)	O
,	O
we	O
put	O
more	O
weight	O
on	O
the	O
mle	O
.	O
if	O
the	O
prior	O
is	O
strong	O
relative	O
to	O
the	O
signal	O
strength	O
(	O
λ0	O
is	O
large	O
relative	O
to	O
λy	O
)	O
,	O
we	O
put	O
more	O
weight	O
on	O
the	O
prior	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
4.12	O
,	O
which	O
is	O
very	O
similar	B
to	O
the	O
analogous	O
results	O
for	O
the	O
beta-binomial	B
model	O
in	O
figure	O
3.6.	O
note	O
that	O
the	O
posterior	B
mean	I
is	O
written	O
in	O
terms	O
of	O
n	O
λyy	O
,	O
so	O
having	O
n	O
measurements	O
each	O
of	O
precision	B
λy	O
is	O
like	O
having	O
one	O
measurement	O
with	O
value	O
y	O
and	O
precision	B
n	O
λy	O
.	O
we	O
can	O
rewrite	O
the	O
results	O
in	O
terms	O
of	O
the	O
posterior	O
variance	O
,	O
rather	O
than	O
posterior	O
precision	O
,	O
4.4.	O
linear	O
gaussian	O
systems	O
prior	O
variance	B
=	O
1.00	O
prior	O
lik	O
post	O
0	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−5	O
5	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−5	O
prior	O
variance	B
=	O
5.00	O
prior	O
lik	O
post	O
0	O
121	O
5	O
figure	O
4.12	O
inference	B
about	O
x	O
given	O
a	O
noisy	O
observation	B
y	O
=	O
3	O
.	O
(	O
a	O
)	O
strong	O
prior	O
n	O
(	O
0	O
,	O
1	O
)	O
.	O
the	O
posterior	O
(	O
a	O
)	O
weak	O
prior	O
n	O
(	O
0	O
,	O
5	O
)	O
.	O
the	O
posterior	B
mean	I
is	O
mean	B
is	O
“	O
shrunk	O
”	O
towards	O
the	O
prior	O
mean	B
,	O
which	O
is	O
0.	O
similar	B
to	O
the	O
mle	O
.	O
figure	O
generated	O
by	O
gaussinferparamsmean1d	O
.	O
as	O
follows	O
:	O
p	O
(	O
x|d	O
,	O
σ2	O
)	O
=n	O
(	O
x|μn	O
,	O
τ	O
2	O
n	O
)	O
τ	O
2	O
n	O
=	O
μn	O
=	O
τ	O
2	O
n	O
1	O
n	O
(	O
cid:7	O
)	O
σ2	O
+	O
1	O
τ	O
2	O
0	O
μ0	O
τ	O
2	O
0	O
=	O
n	O
τ	O
2	O
σ2τ	O
2	O
0	O
(	O
cid:8	O
)	O
0	O
+	O
σ2	O
+	O
n	O
y	O
σ2	O
=	O
n	O
τ	O
2	O
μ0	O
+	O
σ2	O
0	O
+	O
σ2	O
n	O
=	O
1/λn	O
is	O
the	O
posterior	O
variance	O
.	O
n	O
τ	O
2	O
0	O
0	O
+	O
σ2	O
n	O
τ	O
2	O
y	O
where	O
τ	O
2	O
0	O
=	O
1/λ0	O
is	O
the	O
prior	O
variance	B
and	O
τ	O
2	O
we	O
can	O
also	O
compute	O
the	O
posterior	O
sequentially	O
,	O
by	O
updating	O
after	O
each	O
observation	B
.	O
if	O
n	O
=	O
1	O
,	O
we	O
can	O
rewrite	O
the	O
posterior	O
after	O
seeing	O
a	O
single	O
observation	O
as	O
follows	O
(	O
where	O
we	O
deﬁne	O
σy	O
=	O
σ2	O
,	O
σ0	O
=	O
τ	O
2	O
1	O
to	O
be	O
the	O
variances	O
of	O
the	O
likelihood	B
,	O
prior	O
and	O
posterior	O
)	O
:	O
0	O
and	O
σ1	O
=	O
τ	O
2	O
(	O
cid:8	O
)	O
−1	O
(	O
cid:8	O
)	O
σyς0	O
=	O
σ0	O
+	O
σy	O
(	O
cid:7	O
)	O
p	O
(	O
x|y	O
)	O
=n	O
(	O
x|μ1	O
,	O
σ1	O
)	O
1	O
σy	O
σ1	O
=	O
1	O
σ0	O
+	O
(	O
cid:7	O
)	O
μ1	O
=	O
σ1	O
μ0	O
σ0	O
+	O
y	O
σy	O
we	O
can	O
rewrite	O
the	O
posterior	B
mean	I
in	O
3	O
different	O
ways	O
:	O
μ1	O
=	O
σy	O
σy	O
+	O
σ0	O
μ0	O
+	O
=	O
μ0	O
+	O
(	O
y	O
−	O
μ0	O
)	O
=	O
y	O
−	O
(	O
y	O
−	O
μ0	O
)	O
y	O
σ0	O
σy	O
+	O
σ0	O
σ0	O
σy	O
+	O
σ0	O
σy	O
σy	O
+	O
σ0	O
(	O
4.132	O
)	O
(	O
4.133	O
)	O
(	O
4.134	O
)	O
(	O
4.135	O
)	O
(	O
4.136	O
)	O
(	O
4.137	O
)	O
(	O
4.138	O
)	O
(	O
4.139	O
)	O
(	O
4.140	O
)	O
122	O
chapter	O
4.	O
gaussian	O
models	O
the	O
ﬁrst	O
equation	O
is	O
a	O
convex	B
combination	I
of	O
the	O
prior	O
and	O
the	O
data	O
.	O
the	O
second	O
equation	O
is	O
the	O
prior	O
mean	B
adjusted	O
towards	O
the	O
data	O
.	O
the	O
third	O
equation	O
is	O
the	O
data	O
adjusted	O
towards	O
the	O
prior	O
mean	B
;	O
this	O
is	O
called	O
shrinkage	B
.	O
these	O
are	O
all	O
equivalent	O
ways	O
of	O
expressing	O
the	O
tradeoff	O
between	O
likelihood	B
and	O
prior	O
.	O
if	O
σ0	O
is	O
small	O
relative	O
to	O
σy	O
,	O
corresponding	O
to	O
a	O
strong	O
prior	O
,	O
the	O
amount	O
of	O
shrinkage	B
is	O
large	O
(	O
see	O
figure	O
4.12	O
(	O
a	O
)	O
)	O
,	O
whereas	O
if	O
σ0	O
is	O
large	O
relative	O
to	O
σy	O
,	O
corresponding	O
to	O
a	O
weak	O
prior	O
,	O
the	O
amount	O
of	O
shrinkage	B
is	O
small	O
(	O
see	O
figure	O
4.12	O
(	O
b	O
)	O
)	O
.	O
another	O
way	O
to	O
quantify	O
the	O
amount	O
of	O
shrinkage	B
is	O
in	O
terms	O
of	O
the	O
signal-to-noise	B
ratio	I
,	O
which	O
is	O
deﬁned	O
as	O
follows	O
:	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
snr	O
(	O
cid:2	O
)	O
e	O
x	O
2	O
e	O
[	O
2	O
]	O
σ0	O
+	O
μ2	O
0	O
=	O
(	O
4.141	O
)	O
where	O
x	O
∼	O
n	O
(	O
μ0	O
,	O
σ0	O
)	O
is	O
the	O
true	O
signal	O
,	O
y	O
=	O
x	O
+	O
	O
is	O
the	O
observed	O
signal	O
,	O
and	O
	O
∼	O
n	O
(	O
0	O
,	O
σy	O
)	O
is	O
the	O
noise	O
term	O
.	O
σy	O
4.4.2.2	O
inferring	O
an	O
unknown	B
vector	O
from	O
noisy	O
measurements	O
now	O
consider	O
n	O
vector-valued	O
observations	O
,	O
yi	O
∼	O
n	O
(	O
x	O
,	O
σy	O
)	O
,	O
and	O
a	O
gaussian	O
prior	O
,	O
x	O
∼	O
n	O
(	O
μ0	O
,	O
σ0	O
)	O
.	O
setting	O
a	O
=	O
i	O
,	O
b	O
=	O
0	O
,	O
and	O
using	O
y	O
for	O
the	O
effective	O
observation	O
with	O
precision	B
n	O
σ−1	O
y	O
,	O
we	O
have	O
p	O
(	O
x|y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
)	O
=n	O
(	O
x|μn	O
,	O
σn	O
)	O
0	O
+	O
n	O
σ−1	O
σ−1	O
n	O
=	O
σ−1	O
μn	O
=	O
σn	O
(	O
σ−1	O
y	O
y	O
(	O
n	O
y	O
)	O
+σ	O
−1	O
0	O
μ0	O
)	O
(	O
4.142	O
)	O
(	O
4.143	O
)	O
(	O
4.144	O
)	O
see	O
figure	O
4.13	O
for	O
a	O
2d	O
example	O
.	O
we	O
can	O
think	O
of	O
x	O
as	O
representing	O
the	O
true	O
,	O
but	O
unknown	B
,	O
location	O
of	O
an	O
object	O
in	O
2d	O
space	O
,	O
such	O
as	O
a	O
missile	O
or	O
airplane	O
,	O
and	O
the	O
yi	O
as	O
being	O
noisy	O
observations	O
,	O
such	O
as	O
radar	B
“	O
blips	O
”	O
.	O
as	O
we	O
receive	O
more	O
blips	O
,	O
we	O
are	O
better	O
able	O
to	O
localize	O
the	O
source	O
.	O
in	O
section	O
18.3.1	O
,	O
we	O
will	O
see	O
how	O
to	O
extend	O
this	O
example	O
to	O
track	O
moving	O
objects	O
using	O
the	O
famous	O
kalman	O
ﬁlter	O
algorithm	O
.	O
now	O
suppose	O
we	O
have	O
multiple	O
measuring	O
devices	O
,	O
and	O
we	O
want	O
to	O
combine	O
them	O
together	O
;	O
this	O
is	O
known	O
as	O
sensor	B
fusion	I
.	O
if	O
we	O
have	O
multiple	O
observations	O
with	O
different	O
covariances	O
(	O
cor-	O
responding	O
to	O
sensors	O
with	O
different	O
reliabilities	O
)	O
,	O
the	O
posterior	O
will	O
be	O
an	O
appropriate	O
weighted	B
average	I
of	O
the	O
data	O
.	O
consider	O
the	O
example	O
in	O
figure	O
4.14.	O
we	O
use	O
an	O
uninformative	B
prior	O
on	O
x	O
,	O
namely	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
μ0	O
,	O
σ0	O
)	O
=	O
n	O
(	O
0	O
,	O
1010i2	O
)	O
.	O
we	O
get	O
2	O
noisy	O
observations	O
,	O
y1	O
∼	O
n	O
(	O
x	O
,	O
σy,1	O
)	O
and	O
y2	O
∼	O
n	O
(	O
x	O
,	O
σy,2	O
)	O
.	O
we	O
then	O
compute	O
p	O
(	O
x|y1	O
,	O
y2	O
)	O
.	O
in	O
figure	O
4.14	O
(	O
a	O
)	O
,	O
we	O
set	O
σy,1	O
=	O
σy,2	O
=	O
0.01i2	O
,	O
so	O
both	O
sensors	O
are	O
equally	O
reliable	O
.	O
in	O
this	O
case	O
,	O
the	O
posterior	B
mean	I
is	O
half	O
way	O
between	O
the	O
two	O
observations	O
,	O
y1	O
and	O
y2	O
.	O
in	O
figure	O
4.14	O
(	O
b	O
)	O
,	O
we	O
set	O
σy,1	O
=	O
0.05i2	O
and	O
σy,2	O
=	O
0.01i2	O
,	O
so	O
sensor	O
2	O
is	O
more	O
reliable	O
than	O
sensor	O
1.	O
in	O
this	O
case	O
,	O
the	O
posterior	B
mean	I
is	O
closer	O
to	O
y2	O
.	O
in	O
figure	O
4.14	O
(	O
c	O
)	O
,	O
we	O
set	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
10	O
1	O
1	O
1	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
1	O
1	O
1	O
10	O
σy,1	O
=	O
0.01	O
,	O
σy,2	O
=	O
0.01	O
(	O
4.145	O
)	O
so	O
sensor	O
1	O
is	O
more	O
reliable	O
in	O
the	O
y2	O
component	O
(	O
vertical	O
direction	O
)	O
,	O
and	O
sensor	O
2	O
is	O
more	O
in	O
this	O
case	O
,	O
the	O
posterior	B
mean	I
uses	O
y1	O
’	O
s	O
reliable	O
in	O
the	O
y1	O
component	O
(	O
horizontal	O
direction	O
)	O
.	O
vertical	O
component	O
and	O
y2	O
’	O
s	O
horizontal	O
component	O
.	O
4.4.	O
linear	O
gaussian	O
systems	O
123	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1	O
post	O
after	O
10	O
obs	O
data	O
prior	O
1	O
0.5	O
0	O
−0.5	O
1	O
0.5	O
0	O
−0.5	O
0	O
1	O
−1	O
−1	O
0	O
1	O
−1	O
−1	O
0	O
1	O
figure	O
4.13	O
illustration	O
of	O
bayesian	O
inference	B
for	O
the	O
mean	B
of	O
a	O
2d	O
gaussian	O
.	O
(	O
a	O
)	O
the	O
data	O
is	O
generated	O
from	O
yi	O
∼	O
n	O
(	O
x	O
,	O
σy	O
)	O
,	O
where	O
x	O
=	O
[	O
0.5	O
,	O
0.5	O
]	O
t	O
and	O
σy	O
=	O
0.1	O
[	O
2	O
,	O
1	O
;	O
1	O
,	O
1	O
]	O
)	O
.	O
we	O
assume	O
the	O
sensor	O
noise	O
covariance	B
σy	O
is	O
known	O
but	O
x	O
is	O
unknown	B
.	O
the	O
black	O
cross	O
represents	O
x	O
.	O
(	O
b	O
)	O
the	O
prior	O
is	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x|0	O
,	O
0.1i2	O
)	O
.	O
(	O
c	O
)	O
we	O
show	O
the	O
posterior	O
after	O
10	O
data	O
points	O
have	O
been	O
observed	O
.	O
figure	O
generated	O
by	O
gaussinferparamsmean2d	O
.	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−0.6	O
−0.8	O
−1	O
−1.2	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−0.6	O
−0.8	O
−1	O
−1.2	O
−1.4	O
−1.4	O
−0.4	O
−0.2	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
1.2	O
1.4	O
−1.6	O
−0.6	O
−0.4	O
−0.2	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
1.2	O
1.4	O
(	O
a	O
)	O
(	O
b	O
)	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−1	O
−0.5	O
0	O
0.5	O
1	O
1.5	O
(	O
c	O
)	O
figure	O
4.14	O
we	O
observe	O
y1	O
=	O
(	O
0	O
,	O
−1	O
)	O
(	O
red	O
cross	O
)	O
and	O
y2	O
=	O
(	O
1	O
,	O
0	O
)	O
(	O
green	O
cross	O
)	O
and	O
infer	O
e	O
(	O
μ|y1	O
,	O
y2	O
,	O
θ	O
)	O
(	O
a	O
)	O
equally	O
reliable	O
sensors	O
,	O
so	O
the	O
posterior	B
mean	I
estimate	O
is	O
in	O
between	O
the	O
two	O
circles	O
.	O
(	O
black	O
cross	O
)	O
.	O
(	O
b	O
)	O
sensor	O
2	O
is	O
more	O
reliable	O
,	O
so	O
the	O
estimate	O
shifts	O
more	O
towards	O
the	O
green	O
circle	O
.	O
(	O
c	O
)	O
sensor	O
1	O
is	O
more	O
reliable	O
in	O
the	O
vertical	O
direction	O
,	O
sensor	O
2	O
is	O
more	O
reliable	O
in	O
the	O
horizontal	O
direction	O
.	O
the	O
estimate	O
is	O
an	O
appropriate	O
combination	O
of	O
the	O
two	O
measurements	O
.	O
figure	O
generated	O
by	O
sensorfusion2d	O
.	O
note	O
that	O
this	O
technique	O
crucially	O
relies	O
on	O
modeling	O
our	O
uncertainty	B
of	O
each	O
sensor	O
;	O
comput-	O
ing	O
an	O
unweighted	O
average	O
would	O
give	O
the	O
wrong	O
result	O
.	O
however	O
,	O
we	O
have	O
assumed	O
the	O
sensor	O
precisions	O
are	O
known	O
.	O
when	O
they	O
are	O
not	O
,	O
we	O
should	O
model	O
out	O
uncertainty	B
about	O
σ1	O
and	O
σ2	O
as	O
well	O
.	O
see	O
section	O
4.6.4	O
for	O
details	O
.	O
4.4.2.3	O
interpolating	O
noisy	O
data	O
we	O
now	O
revisit	O
the	O
example	O
of	O
section	O
4.3.2.2.	O
this	O
time	O
we	O
no	O
longer	O
assume	O
noise-free	O
let	O
us	O
assume	O
that	O
we	O
obtain	O
n	O
noisy	O
observations	O
yi	O
;	O
without	O
loss	B
observations	O
.	O
of	O
generality	O
,	O
assume	O
these	O
correspond	O
to	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
we	O
can	O
model	O
this	O
setup	O
as	O
a	O
linear	O
instead	O
,	O
124	O
chapter	O
4.	O
gaussian	O
models	O
gaussian	O
system	O
:	O
y	O
=	O
ax	O
+	O
	O
(	O
4.146	O
)	O
where	O
	O
∼	O
n	O
(	O
0	O
,	O
σy	O
)	O
,	O
σy	O
=	O
σ2i	O
,	O
σ2	O
is	O
the	O
observation	B
noise	O
,	O
and	O
a	O
is	O
a	O
n	O
×	O
d	O
projection	B
matrix	O
that	O
selects	O
out	O
the	O
observed	O
elements	O
.	O
for	O
example	O
,	O
if	O
n	O
=	O
2	O
and	O
d	O
=	O
4	O
we	O
have	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
a	O
=	O
(	O
4.147	O
)	O
−1	O
,	O
we	O
can	O
easily	O
compute	O
the	O
posterior	O
using	O
the	O
same	O
improper	B
prior	I
as	O
before	O
,	O
σx	O
=	O
(	O
lt	O
l	O
)	O
mean	B
and	O
variance	B
.	O
in	O
figure	O
4.15	O
,	O
we	O
plot	O
the	O
posterior	B
mean	I
,	O
posterior	O
variance	O
,	O
and	O
some	O
posterior	O
samples	O
.	O
now	O
we	O
see	O
that	O
the	O
prior	O
precision	B
λ	O
effects	O
the	O
posterior	B
mean	I
as	O
well	O
as	O
the	O
posterior	O
variance	O
.	O
in	O
particular	O
,	O
for	O
a	O
strong	O
prior	O
(	O
large	O
λ	O
)	O
,	O
the	O
estimate	O
is	O
very	O
smooth	O
,	O
and	O
the	O
uncertainty	B
is	O
low	O
.	O
but	O
for	O
a	O
weak	O
prior	O
(	O
small	O
λ	O
)	O
,	O
the	O
estimate	O
is	O
wiggly	O
,	O
and	O
the	O
uncertainty	B
(	O
away	O
from	O
the	O
data	O
)	O
is	O
high	O
.	O
the	O
posterior	B
mean	I
can	O
also	O
be	O
computed	O
by	O
solving	O
the	O
following	O
optimization	B
problem	O
:	O
*	O
d	O
(	O
cid:6	O
)	O
)	O
j=1	O
n	O
(	O
cid:6	O
)	O
i=1	O
(	O
cid:11	O
)	O
min	O
x	O
1	O
2σ2	O
(	O
xi	O
−	O
yi	O
)	O
2	O
+	O
λ	O
2	O
(	O
xj	O
−	O
xj−1	O
)	O
2	O
+	O
(	O
xj	O
−	O
xj+1	O
)	O
2	O
(	O
4.148	O
)	O
(	O
cid:11	O
)	O
where	O
we	O
have	O
deﬁned	O
x0	O
=	O
x1	O
and	O
xd+1	O
=	O
xd	O
for	O
notational	O
simplicity	O
.	O
we	O
recognize	O
this	O
as	O
a	O
discrete	B
approximation	O
to	O
the	O
following	O
problem	O
:	O
f	O
λ	O
[	O
f	O
(	O
cid:2	O
)	O
(	O
f	O
(	O
t	O
)	O
−	O
y	O
(	O
t	O
)	O
)	O
2dt	O
+	O
1	O
min	O
2σ2	O
where	O
f	O
(	O
cid:2	O
)	O
(	O
t	O
)	O
is	O
the	O
ﬁrst	O
derivative	O
of	O
f.	O
the	O
ﬁrst	O
term	O
measures	O
ﬁt	O
to	O
the	O
data	O
,	O
and	O
the	O
second	O
term	O
penalizes	O
functions	O
that	O
are	O
“	O
too	O
wiggly	O
”	O
.	O
this	O
is	O
an	O
example	O
of	O
tikhonov	O
regularization	B
,	O
which	O
is	O
a	O
popular	O
approach	O
to	O
functional	B
data	I
analysis	I
.	O
see	O
chapter	O
15	O
for	O
more	O
sophisticated	O
approaches	O
,	O
which	O
enforce	O
higher	O
order	O
smoothness	O
(	O
so	O
the	O
resulting	O
samples	B
look	O
less	O
“	O
jagged	O
”	O
)	O
.	O
(	O
t	O
)	O
]	O
2dt	O
(	O
4.149	O
)	O
2	O
4.4.3	O
proof	O
of	O
the	O
result	O
*	O
we	O
now	O
derive	O
equation	O
4.125.	O
the	O
basic	O
idea	O
is	O
to	O
derive	O
the	O
joint	B
distribution	I
,	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y|x	O
)	O
,	O
and	O
then	O
to	O
use	O
the	O
results	O
from	O
section	O
4.3.1	O
for	O
computing	O
p	O
(	O
x|y	O
)	O
.	O
in	O
more	O
detail	O
,	O
we	O
proceed	O
as	O
follows	O
.	O
the	O
log	O
of	O
the	O
joint	B
distribution	I
is	O
as	O
follows	O
(	O
dropping	O
irrelevant	O
constants	O
)	O
:	O
log	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
−	O
1	O
2	O
(	O
x	O
−	O
μx	O
)	O
t	O
σ−1	O
x	O
(	O
x	O
−	O
μx	O
)	O
−	O
1	O
2	O
(	O
y	O
−	O
ax	O
−	O
b	O
)	O
t	O
σ−1	O
y	O
(	O
y	O
−	O
ax	O
−	O
b	O
)	O
(	O
4.150	O
)	O
this	O
is	O
clearly	O
a	O
joint	O
gaussian	O
distribution	O
,	O
since	O
it	O
is	O
the	O
exponential	O
of	O
a	O
quadratic	O
form	O
.	O
expanding	O
out	O
the	O
quadratic	O
terms	O
involving	O
x	O
and	O
y	O
,	O
and	O
ignoring	O
linear	O
and	O
constant	O
terms	O
,	O
we	O
have	O
q	O
=	O
−	O
1	O
2	O
=	O
−	O
1	O
2	O
=	O
−	O
1	O
2	O
y	O
y	O
−	O
1	O
2	O
(	O
ax	O
)	O
t	O
σ−1	O
y	O
a	O
−at	O
σ−1	O
y	O
y	O
(	O
ax	O
)	O
+y	O
t	O
σ−1	O
y	O
ax	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
σ−1	O
y	O
x	O
y	O
yt	O
σ−1	O
x	O
x	O
−	O
1	O
xt	O
σ−1	O
(	O
cid:8	O
)	O
t	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
2	O
σ−1	O
x	O
+	O
at	O
σ−1	O
−σ−1	O
(	O
cid:8	O
)	O
t	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
x	O
y	O
σ−1	O
y	O
a	O
x	O
y	O
x	O
y	O
(	O
4.151	O
)	O
(	O
4.152	O
)	O
(	O
4.153	O
)	O
4.5.	O
digression	O
:	O
the	O
wishart	O
distribution	O
*	O
125	O
λ=30	O
λ=0p1	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
0	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
(	O
a	O
)	O
0.6	O
0.7	O
0.8	O
0.9	O
1	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
0	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
(	O
b	O
)	O
0.6	O
0.7	O
0.8	O
0.9	O
1	O
interpolating	O
noisy	O
data	O
(	O
noise	O
variance	O
σ2	O
=	O
1	O
)	O
using	O
a	O
gaussian	O
with	O
prior	O
precision	B
λ	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
λ	O
=	O
0.01.	O
see	O
also	O
figure	O
4.10.	O
based	O
on	O
figure	O
7.1	O
of	O
(	O
calvetti	O
and	O
somersalo	O
2007	O
)	O
.	O
figure	O
figure	O
4.15	O
λ	O
=	O
30.	O
generated	O
by	O
gaussinterpnoisydemo	O
.	O
see	O
also	O
splinebasisdemo	O
.	O
(	O
cid:7	O
)	O
σ−1	O
=	O
where	O
the	O
precision	B
matrix	I
of	O
the	O
joint	O
is	O
deﬁned	O
as	O
σ−1	O
x	O
+	O
at	O
σ−1	O
y	O
a	O
−at	O
σ−1	O
y	O
−σ−1	O
y	O
a	O
σ−1	O
y	O
(	O
cid:2	O
)	O
λ	O
=	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
λxx	O
λxy	O
λyx	O
λyy	O
from	O
equation	O
4.69	O
,	O
and	O
using	O
the	O
fact	O
that	O
μy	O
=	O
aμx	O
+	O
b	O
,	O
we	O
have	O
p	O
(	O
x|y	O
)	O
=n	O
(	O
μx|y	O
,	O
σx|y	O
)	O
σx|y	O
=	O
λ−1	O
xx	O
=	O
(	O
σ−1	O
μx|y	O
=	O
σx|y	O
=	O
σx|y	O
(	O
cid:18	O
)	O
(	O
cid:18	O
)	O
x	O
+	O
at	O
σ−1	O
−1	O
(	O
cid:19	O
)	O
y	O
a	O
)	O
λxxμx	O
−	O
λxy	O
(	O
y	O
−	O
μy	O
)	O
y	O
(	O
y	O
−	O
b	O
)	O
σ−1	O
x	O
μ	O
+	O
at	O
σ−1	O
(	O
cid:19	O
)	O
4.5	O
digression	O
:	O
the	O
wishart	O
distribution	O
*	O
(	O
4.154	O
)	O
(	O
4.155	O
)	O
(	O
4.156	O
)	O
(	O
4.157	O
)	O
(	O
4.158	O
)	O
(	O
4.159	O
)	O
the	O
wishart	O
distribution	O
is	O
the	O
generalization	B
of	O
the	O
gamma	B
distribution	I
to	O
positive	B
deﬁnite	I
matrices	O
.	O
press	O
(	O
press	O
2005	O
,	O
p107	O
)	O
has	O
said	O
“	O
the	O
wishart	O
distribution	O
ranks	O
next	O
to	O
the	O
(	O
multi-	O
variate	O
)	O
normal	B
distribution	O
in	O
order	O
of	O
importance	O
and	O
usefuleness	O
in	O
multivariate	O
statistics	O
”	O
.	O
we	O
will	O
mostly	O
use	O
it	O
to	O
model	O
our	O
uncertainty	B
in	O
covariance	B
matrices	O
,	O
σ	O
,	O
or	O
their	O
inverses	O
,	O
λ	O
=	O
σ−1	O
.	O
the	O
pdf	B
of	O
the	O
wishart	O
is	O
deﬁned	O
as	O
follows	O
:	O
wi	O
(	O
λ|s	O
,	O
ν	O
)	O
=	O
|λ|	O
(	O
ν−d−1	O
)	O
/2	O
exp	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
tr	O
(	O
λs−1	O
)	O
−	O
1	O
2	O
1	O
zwi	O
here	O
ν	O
is	O
called	O
the	O
“	O
degrees	B
of	I
freedom	I
”	O
and	O
s	O
is	O
the	O
“	O
scale	O
matrix	O
”	O
.	O
(	O
we	O
shall	O
get	O
more	O
intuition	O
for	O
these	O
parameters	O
shortly	O
.	O
)	O
the	O
normalization	O
constant	O
for	O
this	O
distribution	O
(	O
which	O
126	O
chapter	O
4.	O
gaussian	O
models	O
requires	O
integrating	O
over	O
all	O
symmetric	O
pd	O
matrices	O
)	O
is	O
the	O
following	O
formidable	O
expression	O
zwi	O
=	O
2νd/2γd	O
(	O
ν/2	O
)	O
|s|ν/2	O
where	O
γd	O
(	O
a	O
)	O
is	O
the	O
multivariate	B
gamma	I
function	I
:	O
γd	O
(	O
x	O
)	O
=π	O
d	O
(	O
d−1	O
)	O
/4	O
γ	O
(	O
x	O
+	O
(	O
1−	O
i	O
)	O
/2	O
)	O
d	O
(	O
cid:12	O
)	O
i=1	O
(	O
4.160	O
)	O
(	O
4.161	O
)	O
hence	O
γ1	O
(	O
a	O
)	O
=	O
γ	O
(	O
a	O
)	O
and	O
d	O
(	O
cid:12	O
)	O
ν0	O
+	O
1−	O
i	O
)	O
2	O
i=1	O
γ	O
(	O
γd	O
(	O
ν0/2	O
)	O
=	O
(	O
4.162	O
)	O
the	O
normalization	O
constant	O
only	O
exists	O
(	O
and	O
hence	O
the	O
pdf	B
is	O
only	O
well	O
deﬁned	O
)	O
if	O
ν	O
>	O
d	O
−	O
1.	O
in	O
particular	O
,	O
let	O
xi	O
∼	O
n	O
(	O
0	O
,	O
σ	O
)	O
.	O
then	O
the	O
scatter	O
matrix	O
s	O
=	O
i	O
has	O
a	O
wishart	O
distribution	O
:	O
s	O
∼	O
wi	O
(	O
σ	O
,	O
1	O
)	O
.	O
hence	O
e	O
[	O
s	O
]	O
=	O
n	O
σ.	O
more	O
generally	O
,	O
one	O
can	O
show	O
that	O
the	O
mean	B
and	O
mode	B
of	O
wi	O
(	O
s	O
,	O
ν	O
)	O
are	O
given	O
by	O
there	O
is	O
a	O
connection	O
between	O
the	O
wishart	O
distribution	O
and	O
the	O
gaussian	O
.	O
(	O
cid:4	O
)	O
n	O
i=1	O
xixt	O
mean	B
=	O
νs	O
,	O
mode	B
=	O
(	O
ν	O
−	O
d	O
−	O
1	O
)	O
s	O
where	O
the	O
mode	B
only	O
exists	O
if	O
ν	O
>	O
d	O
+	O
1.	O
if	O
d	O
=	O
1	O
,	O
the	O
wishart	O
reduces	O
to	O
the	O
gamma	B
distribution	I
:	O
wi	O
(	O
λ|s−1	O
,	O
ν	O
)	O
=	O
ga	O
(	O
λ|	O
ν	O
2	O
2	O
s	O
)	O
,	O
(	O
4.163	O
)	O
(	O
4.164	O
)	O
4.5.1	O
inverse	O
wishart	O
distribution	O
recall	B
that	O
we	O
showed	O
(	O
exercise	O
2.10	O
)	O
that	O
if	O
λ	O
∼	O
ga	O
(	O
a	O
,	O
b	O
)	O
,	O
then	O
that	O
1	O
λ	O
∼	O
ig	O
(	O
a	O
,	O
b	O
)	O
.	O
similarly	O
,	O
if	O
σ−1	O
∼	O
wi	O
(	O
s	O
,	O
ν	O
)	O
then	O
σ	O
∼	O
iw	O
(	O
s−1	O
,	O
ν	O
+	O
d	O
+	O
1	O
)	O
,	O
where	O
iw	O
is	O
the	O
inverse	O
wishart	O
,	O
the	O
multidimensional	O
generalization	O
of	O
the	O
inverse	B
gamma	I
.	O
it	O
is	O
deﬁned	O
as	O
follows	O
,	O
for	O
ν	O
>	O
d	O
−	O
1	O
and	O
s	O
(	O
cid:13	O
)	O
0	O
:	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
iw	O
(	O
σ|s	O
,	O
ν	O
)	O
=	O
|σ|−	O
(	O
ν+d+1	O
)	O
/2	O
exp	O
ziw	O
=	O
|s|−ν/22νd/2γd	O
(	O
ν/2	O
)	O
ziw	O
1	O
tr	O
(	O
s−1σ−1	O
)	O
−	O
1	O
2	O
one	O
can	O
show	O
that	O
the	O
distribution	O
has	O
these	O
properties	O
mean	B
=	O
s−1	O
ν	O
−	O
d	O
−	O
1	O
,	O
mode	B
=	O
s−1	O
ν	O
+	O
d	O
+	O
1	O
if	O
d	O
=	O
1	O
,	O
this	O
reduces	O
to	O
the	O
inverse	B
gamma	I
:	O
iw	O
(	O
σ2|s−1	O
,	O
ν	O
)	O
=	O
ig	O
(	O
σ2|ν/2	O
,	O
s/2	O
)	O
(	O
4.165	O
)	O
(	O
4.166	O
)	O
(	O
4.167	O
)	O
(	O
4.168	O
)	O
4.6.	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	O
127	O
0.08	O
0.06	O
0.04	O
0.02	O
0	O
0	O
wi	O
(	O
dof=3.0	O
,	O
s	O
)	O
,	O
e=	O
[	O
9.5	O
,	O
−0.1	O
;	O
−0.1	O
,	O
1.9	O
]	O
,	O
ρ=−0.0	O
−4	O
−2	O
0	O
2	O
4	O
−5	O
0	O
5	O
2	O
0	O
−2	O
4	O
2	O
0	O
−2	O
−4	O
2	O
0	O
−2	O
5	O
0	O
−5	O
5	O
0	O
−5	O
2	O
0	O
−2	O
5	O
0	O
−5	O
−5	O
0	O
5	O
−10	O
0	O
10	O
2	O
0	O
−2	O
−5	O
0	O
5	O
−4	O
−2	O
0	O
2	O
4	O
5	O
0	O
−5	O
−2	O
0	O
2	O
−4	O
−2	O
0	O
2	O
4	O
−10	O
0	O
10	O
(	O
a	O
)	O
σ2	O
1	O
ρ	O
(	O
1,2	O
)	O
0.8	O
0.6	O
0.4	O
0.2	O
5	O
10	O
15	O
20	O
0	O
−2	O
−1	O
1	O
2	O
0	O
σ2	O
2	O
1	O
2	O
3	O
4	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
(	O
b	O
)	O
figure	O
4.16	O
visualization	O
of	O
the	O
wishart	O
distribution	O
.	O
left	O
:	O
some	O
samples	B
from	O
the	O
wishart	O
distribution	O
,	O
σ	O
∼	O
wi	O
(	O
s	O
,	O
ν	O
)	O
,	O
where	O
s	O
=	O
[	O
3.1653	O
,	O
−0.0262	O
;	O
−0.0262	O
,	O
0.6477	O
]	O
and	O
ν	O
=	O
3.	O
right	O
:	O
plots	O
of	O
the	O
marginals	O
(	O
which	O
are	O
gamma	O
)	O
,	O
and	O
the	O
approximate	O
(	O
sample-based	O
)	O
marginal	O
on	O
the	O
correlation	B
coefficient	I
.	O
if	O
ν	O
=	O
3	O
there	O
is	O
a	O
lot	O
of	O
uncertainty	B
about	O
the	O
value	O
of	O
the	O
correlation	B
coefficient	I
ρ	O
(	O
see	O
the	O
almost	O
uniform	B
distribution	I
on	O
[	O
−1	O
,	O
1	O
]	O
)	O
.	O
the	O
sampled	O
matrices	O
are	O
highly	O
variable	O
,	O
and	O
some	O
are	O
nearly	O
singular	O
.	O
as	O
ν	O
increases	O
,	O
the	O
sampled	O
matrices	O
are	O
more	O
concentrated	O
on	O
the	O
prior	O
s.	O
figure	O
generated	O
by	O
wiplotdemo	O
.	O
4.5.2	O
visualizing	B
the	O
wishart	O
distribution	O
*	O
since	O
the	O
wishart	O
is	O
a	O
distribution	O
over	O
matrices	O
,	O
it	O
is	O
hard	O
to	O
plot	O
as	O
a	O
density	O
function	O
.	O
however	O
,	O
we	O
can	O
easily	O
sample	O
from	O
it	O
,	O
and	O
in	O
the	O
2d	O
case	O
,	O
we	O
can	O
use	O
the	O
eigenvectors	O
of	O
the	O
resulting	O
matrix	O
to	O
deﬁne	O
an	O
ellipse	O
,	O
as	O
explained	O
in	O
section	O
4.1.2.	O
see	O
figure	O
4.16	O
for	O
some	O
examples	O
.	O
for	O
higher	O
dimensional	O
matrices	O
,	O
we	O
can	O
plot	O
marginals	O
of	O
the	O
distribution	O
.	O
the	O
diagonals	O
of	O
a	O
wishart	O
distributed	O
matrix	O
have	O
gamma	O
distributions	O
,	O
so	O
are	O
easy	O
to	O
plot	O
.	O
it	O
is	O
hard	O
in	O
general	O
to	O
work	O
out	O
the	O
distribution	O
of	O
the	O
off-diagonal	O
elements	O
,	O
but	O
we	O
can	O
sample	O
matrices	O
from	O
the	O
distribution	O
,	O
and	O
then	O
compute	O
the	O
distribution	O
empirically	O
.	O
in	O
particular	O
,	O
we	O
can	O
convert	O
each	O
sampled	O
matrix	O
to	O
a	O
correlation	B
matrix	I
,	O
and	O
thus	O
compute	O
a	O
monte	O
carlo	O
approximation	O
(	O
section	O
2.7	O
)	O
to	O
the	O
expected	O
correlation	O
coefficients	O
:	O
e	O
[	O
rij	O
]	O
≈	O
1	O
s	O
r	O
(	O
σ	O
(	O
s	O
)	O
)	O
ij	O
s	O
(	O
cid:6	O
)	O
s=1	O
where	O
σ	O
(	O
s	O
)	O
∼	O
wi	O
(	O
σ	O
,	O
ν	O
)	O
and	O
r	O
(	O
σ	O
)	O
converts	O
matrix	O
σ	O
into	O
a	O
correlation	B
matrix	I
:	O
rij	O
=	O
σij	O
(	O
cid:9	O
)	O
σiiσjj	O
we	O
can	O
then	O
use	O
kernel	B
density	I
estimation	I
(	O
section	O
14.7.2	O
)	O
to	O
produce	O
a	O
smooth	O
approximation	O
to	O
the	O
univariate	O
density	O
e	O
[	O
rij	O
]	O
for	O
plotting	O
purposes	O
.	O
see	O
figure	O
4.16	O
for	O
some	O
examples	O
.	O
4.6	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	O
so	O
far	O
,	O
we	O
have	O
discussed	O
inference	B
in	O
a	O
gaussian	O
assuming	O
the	O
parameters	O
θ	O
=	O
(	O
μ	O
,	O
σ	O
)	O
are	O
known	O
.	O
we	O
now	O
discuss	O
how	O
to	O
infer	O
the	O
parameters	O
themselves	O
.	O
we	O
will	O
assume	O
the	O
data	O
has	O
(	O
4.169	O
)	O
(	O
4.170	O
)	O
128	O
chapter	O
4.	O
gaussian	O
models	O
the	O
form	O
xi	O
∼	O
n	O
(	O
μ	O
,	O
σ	O
)	O
for	O
i	O
=	O
1	O
:	O
n	O
and	O
is	O
fully	O
observed	O
,	O
so	O
we	O
have	O
no	O
missing	O
data	O
(	O
see	O
section	O
11.6.1	O
for	O
how	O
to	O
estimate	O
parameters	O
of	O
an	O
mvn	O
in	O
the	O
presence	O
of	O
missing	B
values	O
)	O
.	O
to	O
simplify	O
the	O
presentation	O
,	O
we	O
derive	O
the	O
posterior	O
in	O
three	O
parts	O
:	O
ﬁrst	O
we	O
compute	O
p	O
(	O
μ|d	O
,	O
σ	O
)	O
;	O
then	O
we	O
compute	O
p	O
(	O
σ|d	O
,	O
μ	O
)	O
;	O
ﬁnally	O
we	O
compute	O
the	O
joint	O
p	O
(	O
μ	O
,	O
σ|d	O
)	O
.	O
4.6.1	O
posterior	O
distribution	O
of	O
μ	O
we	O
have	O
discussed	O
how	O
to	O
compute	O
the	O
mle	O
for	O
μ	O
;	O
we	O
now	O
discuss	O
how	O
to	O
compute	O
its	O
posterior	O
,	O
which	O
is	O
useful	O
for	O
modeling	O
our	O
uncertainty	B
about	O
its	O
value	O
.	O
the	O
likelihood	B
has	O
the	O
form	O
p	O
(	O
d|μ	O
)	O
=	O
n	O
(	O
x|μ	O
,	O
1	O
n	O
σ	O
)	O
(	O
4.171	O
)	O
for	O
simplicity	O
,	O
we	O
will	O
use	O
a	O
conjugate	B
prior	I
,	O
which	O
in	O
this	O
case	O
is	O
a	O
gaussian	O
.	O
in	O
particular	O
,	O
if	O
p	O
(	O
μ	O
)	O
=	O
n	O
(	O
μ|m0	O
,	O
v0	O
)	O
then	O
we	O
can	O
derive	O
a	O
gaussian	O
posterior	O
for	O
μ	O
based	O
on	O
the	O
results	O
in	O
section	O
4.4.2.2.	O
we	O
get	O
p	O
(	O
μ|d	O
,	O
σ	O
)	O
=n	O
(	O
μ|mn	O
,	O
vn	O
)	O
0	O
+	O
n	O
σ−1	O
v−1	O
n	O
=	O
v−1	O
mn	O
=	O
vn	O
(	O
σ−1	O
(	O
n	O
x	O
)	O
+v	O
−1	O
0	O
m0	O
)	O
(	O
4.172	O
)	O
(	O
4.173	O
)	O
(	O
4.174	O
)	O
this	O
is	O
exactly	O
the	O
same	O
process	O
as	O
inferring	O
the	O
location	O
of	O
an	O
object	O
based	O
on	O
noisy	O
radar	B
“	O
blips	O
”	O
,	O
except	O
now	O
we	O
are	O
inferring	O
the	O
mean	B
of	O
a	O
distribution	O
based	O
on	O
noisy	O
samples	B
.	O
(	O
to	O
a	O
bayesian	O
,	O
there	O
is	O
no	O
difference	O
between	O
uncertainty	B
about	O
parameters	O
and	O
uncertainty	B
about	O
anything	O
else	O
.	O
)	O
we	O
can	O
model	O
an	O
uninformative	B
prior	O
by	O
setting	O
v0	O
=	O
∞i	O
.	O
in	O
this	O
case	O
we	O
have	O
p	O
(	O
μ|d	O
,	O
σ	O
)	O
=	O
n	O
(	O
x	O
,	O
1	O
n	O
σ	O
)	O
,	O
so	O
the	O
posterior	B
mean	I
is	O
equal	O
to	O
the	O
mle	O
.	O
we	O
also	O
see	O
that	O
the	O
posterior	O
variance	O
goes	O
down	O
as	O
1/n	O
,	O
which	O
is	O
a	O
standard	O
result	O
from	O
frequentist	B
statistics	I
.	O
4.6.2	O
posterior	O
distribution	O
of	O
σ	O
*	O
we	O
now	O
discuss	O
how	O
to	O
compute	O
p	O
(	O
σ|d	O
,	O
μ	O
)	O
.	O
the	O
likelihood	B
has	O
the	O
form	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
p	O
(	O
d|μ	O
,	O
σ	O
)	O
∝	O
|σ|−	O
n	O
2	O
exp	O
tr	O
(	O
sμς−1	O
)	O
−	O
1	O
2	O
(	O
4.175	O
)	O
the	O
corresponding	O
conjugate	B
prior	I
is	O
known	O
as	O
the	O
inverse	O
wishart	O
distribution	O
(	O
section	O
4.5.1	O
)	O
.	O
recall	B
that	O
this	O
has	O
the	O
following	O
pdf	B
:	O
iw	O
(	O
σ|s−1	O
0	O
,	O
ν0	O
)	O
∝	O
|σ|−	O
(	O
ν0+d+1	O
)	O
/2	O
exp	O
(	O
4.176	O
)	O
here	O
ν0	O
>	O
d	O
−	O
1	O
is	O
the	O
degrees	B
of	I
freedom	I
(	O
dof	O
)	O
,	O
and	O
s0	O
is	O
a	O
symmetric	B
pd	O
matrix	O
.	O
we	O
see	O
that	O
s−1	O
0	O
plays	O
the	O
role	O
of	O
the	O
prior	O
scatter	O
matrix	O
,	O
and	O
n0	O
(	O
cid:2	O
)	O
ν0	O
+	O
d	O
+	O
1	O
controls	O
the	O
strength	O
of	O
the	O
prior	O
,	O
and	O
hence	O
plays	O
a	O
role	O
analogous	O
to	O
the	O
sample	O
size	O
n	O
.	O
tr	O
(	O
s0σ−1	O
)	O
−	O
1	O
2	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
4.6.	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	O
129	O
1.5	O
1	O
0.5	O
l	O
e	O
u	O
a	O
v	O
n	O
e	O
g	O
e	O
i	O
n=100	O
,	O
d=50	O
1.5	O
n=50	O
,	O
d=50	O
1.5	O
n=25	O
,	O
d=50	O
true	O
,	O
k=10.00	O
mle	O
,	O
k=	O
71	O
map	O
,	O
k=8.62	O
true	O
,	O
k=10.00	O
mle	O
,	O
k=1.7e+17	O
map	O
,	O
k=8.85	O
true	O
,	O
k=10.00	O
mle	O
,	O
k=2.2e+18	O
map	O
,	O
k=21.09	O
l	O
e	O
u	O
a	O
v	O
n	O
e	O
g	O
e	O
i	O
1	O
0.5	O
0	O
0	O
5	O
10	O
15	O
20	O
25	O
l	O
e	O
u	O
a	O
v	O
n	O
e	O
g	O
e	O
i	O
1	O
0.5	O
0	O
0	O
5	O
10	O
15	O
20	O
25	O
0	O
0	O
5	O
10	O
15	O
20	O
25	O
figure	O
4.17	O
estimating	O
a	O
covariance	B
matrix	I
in	O
d	O
=	O
50	O
dimensions	O
using	O
n	O
∈	O
{	O
100	O
,	O
50	O
,	O
25	O
}	O
samples	B
.	O
we	O
plot	O
the	O
eigenvalues	O
in	O
descending	O
order	O
for	O
the	O
true	O
covariance	O
matrix	O
(	O
solid	O
black	O
)	O
,	O
the	O
mle	O
(	O
dotted	O
blue	O
)	O
and	O
the	O
map	O
estimate	O
(	O
dashed	O
red	O
)	O
,	O
using	O
equation	O
4.184	O
with	O
λ	O
=	O
0.9.	O
we	O
also	O
list	O
the	O
condition	O
number	O
of	O
each	O
matrix	O
in	O
the	O
legend	O
.	O
based	O
on	O
figure	O
1	O
of	O
(	O
schaefer	O
and	O
strimmer	O
2005	O
)	O
.	O
figure	O
generated	O
by	O
shrinkcovdemo	O
.	O
multiplying	O
the	O
likelihood	B
and	O
prior	O
we	O
ﬁnd	O
that	O
the	O
posterior	O
is	O
also	O
inverse	O
wishart	O
:	O
p	O
(	O
σ|d	O
,	O
μ	O
)	O
∝	O
|σ|−	O
n	O
(	O
cid:7	O
)	O
|σ|−	O
(	O
ν0+d+1	O
)	O
/2	O
tr	O
(	O
σ−1sμ	O
)	O
−	O
1	O
2	O
2	O
exp	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
tr	O
(	O
σ−1s0	O
)	O
(	O
cid:7	O
)	O
−	O
1	O
2	O
exp	O
=	O
|σ|−	O
n	O
+	O
(	O
ν0+d+1	O
)	O
=	O
iw	O
(	O
σ|sn	O
,	O
νn	O
)	O
2	O
exp	O
−	O
1	O
2	O
(	O
cid:20	O
)	O
tr	O
σ−1	O
(	O
sμ	O
+	O
s0	O
)	O
(	O
cid:21	O
)	O
(	O
cid:8	O
)	O
(	O
4.177	O
)	O
(	O
4.178	O
)	O
νn	O
=	O
ν0	O
+	O
n	O
s−1	O
n	O
=	O
s0	O
+	O
sμ	O
(	O
4.179	O
)	O
(	O
4.180	O
)	O
(	O
4.181	O
)	O
in	O
words	O
,	O
this	O
says	O
that	O
the	O
posterior	O
strength	O
νn	O
is	O
the	O
prior	O
strength	O
ν0	O
plus	O
the	O
number	O
of	O
observations	O
n	O
,	O
and	O
the	O
posterior	O
scatter	O
matrix	O
sn	O
is	O
the	O
prior	O
scatter	O
matrix	O
s0	O
plus	O
the	O
data	O
scatter	O
matrix	O
sμ	O
.	O
4.6.2.1	O
map	O
estimation	O
we	O
see	O
from	O
equation	O
4.7	O
that	O
ˆσmle	O
is	O
a	O
rank	O
min	O
(	O
n	O
,	O
d	O
)	O
matrix	O
.	O
if	O
n	O
<	O
d	O
,	O
this	O
is	O
not	O
full	O
rank	O
,	O
and	O
hence	O
will	O
be	O
uninvertible	O
.	O
and	O
even	O
if	O
n	O
>	O
d	O
,	O
it	O
may	O
be	O
the	O
case	O
that	O
ˆσ	O
is	O
ill-conditioned	B
(	O
meaning	O
it	O
is	O
nearly	O
singular	O
)	O
.	O
to	O
solve	O
these	O
problems	O
,	O
we	O
can	O
use	O
the	O
posterior	B
mode	I
(	O
or	O
mean	B
)	O
.	O
one	O
can	O
show	O
(	O
using	O
techniques	O
analogous	O
to	O
the	O
derivation	O
of	O
the	O
mle	O
)	O
that	O
the	O
map	O
estimate	O
is	O
given	O
by	O
ˆσmap	O
=	O
sn	O
νn	O
+	O
d	O
+	O
1	O
=	O
s0	O
+	O
sμ	O
n0	O
+	O
n	O
(	O
4.182	O
)	O
if	O
we	O
use	O
an	O
improper	O
uniform	O
prior	O
,	O
corresponding	O
to	O
n0	O
=	O
0	O
and	O
s0	O
=	O
0	O
,	O
we	O
recover	O
the	O
mle	O
.	O
130	O
chapter	O
4.	O
gaussian	O
models	O
let	O
us	O
now	O
consider	O
the	O
use	O
of	O
a	O
proper	O
informative	O
prior	O
,	O
which	O
is	O
necessary	O
whenever	O
d/n	O
is	O
large	O
(	O
say	O
bigger	O
than	O
0.1	O
)	O
.	O
let	O
μ	O
=	O
x	O
,	O
so	O
sμ	O
=	O
sx	O
.	O
then	O
we	O
can	O
rewrite	O
the	O
map	O
estimate	O
as	O
a	O
convex	B
combination	I
of	O
the	O
prior	O
mode	B
and	O
the	O
mle	O
.	O
to	O
see	O
this	O
,	O
let	O
σ0	O
(	O
cid:2	O
)	O
s0	O
n0	O
be	O
the	O
prior	O
mode	B
.	O
then	O
the	O
posterior	B
mode	I
can	O
be	O
rewritten	O
as	O
ˆσmap	O
=	O
s0	O
+	O
sx	O
n0	O
+	O
n	O
=	O
n0	O
n0	O
+	O
n	O
s0	O
n0	O
+	O
n	O
n0	O
+	O
n	O
s	O
n	O
=	O
λς0	O
+	O
(	O
1−	O
λ	O
)	O
ˆσmle	O
(	O
4.183	O
)	O
where	O
λ	O
=	O
n0	O
n0+n	O
,	O
controls	O
the	O
amount	O
of	O
shrinkage	B
towards	O
the	O
prior	O
.	O
this	O
begs	O
the	O
question	O
:	O
where	O
do	O
the	O
parameters	O
of	O
the	O
prior	O
come	O
from	O
?	O
it	O
is	O
common	O
to	O
set	O
λ	O
by	O
cross	B
validation	I
.	O
alternatively	O
,	O
we	O
can	O
use	O
the	O
closed-form	O
formula	O
provided	O
in	O
(	O
ledoit	O
and	O
wolf	O
2004b	O
,	O
a	O
;	O
schaefer	O
and	O
strimmer	O
2005	O
)	O
,	O
which	O
is	O
the	O
optimal	O
frequentist	O
estimate	O
if	O
we	O
use	O
squared	B
loss	I
.	O
this	O
is	O
arguably	O
not	O
the	O
most	O
natural	O
loss	O
function	O
for	O
covariance	B
matrices	O
(	O
because	O
it	O
ignores	O
the	O
postive	O
deﬁnite	O
constraint	O
)	O
,	O
but	O
it	O
results	O
in	O
a	O
simple	O
estimator	O
,	O
which	O
is	O
implemented	O
in	O
the	O
pmtk	O
function	O
shrinkcov	O
.	O
we	O
discuss	O
bayesian	O
ways	O
of	O
estimating	O
λ	O
later	O
.	O
as	O
for	O
the	O
prior	O
covariance	B
matrix	I
,	O
s0	O
,	O
it	O
is	O
common	O
to	O
use	O
the	O
following	O
(	O
data	O
dependent	O
)	O
prior	O
:	O
s0	O
=	O
diag	O
(	O
ˆσmle	O
)	O
.	O
in	O
this	O
case	O
,	O
the	O
map	O
estimate	O
is	O
given	O
by	O
ˆσmap	O
(	O
i	O
,	O
j	O
)	O
=	O
ˆσmle	O
(	O
i	O
,	O
j	O
)	O
(	O
1	O
−	O
λ	O
)	O
ˆσmle	O
(	O
i	O
,	O
j	O
)	O
if	O
i	O
=	O
j	O
otherwise	O
(	O
4.184	O
)	O
(	O
cid:5	O
)	O
thus	O
we	O
see	O
that	O
the	O
diagonal	B
entries	O
are	O
equal	O
to	O
their	O
ml	O
estimates	O
,	O
and	O
the	O
off	O
diago-	O
nal	O
elements	O
are	O
“	O
shrunk	O
”	O
somewhat	O
towards	O
0.	O
this	O
technique	O
is	O
therefore	O
called	O
shrinkage	B
estimation	I
,	O
orregularized	O
estimation	O
.	O
the	O
beneﬁts	O
of	O
map	O
estimation	O
are	O
illustrated	O
in	O
figure	O
4.17.	O
we	O
consider	O
ﬁtting	O
a	O
50	O
dimen-	O
sional	O
gaussian	O
to	O
n	O
=	O
100	O
,	O
n	O
=	O
50	O
and	O
n	O
=	O
25	O
data	O
points	O
.	O
we	O
see	O
that	O
the	O
map	O
estimate	O
is	O
always	O
well-conditioned	O
,	O
unlike	O
the	O
mle	O
.	O
in	O
particular	O
,	O
we	O
see	O
that	O
the	O
eigenvalue	B
spectrum	I
of	O
the	O
map	O
estimate	O
is	O
much	O
closer	O
to	O
that	O
of	O
the	O
true	O
matrix	O
than	O
the	O
mle	O
’	O
s	O
.	O
the	O
eigenvectors	O
,	O
however	O
,	O
are	O
unaffected	O
.	O
the	O
importance	O
of	O
regularizing	O
the	O
estimate	O
of	O
σ	O
will	O
become	O
apparent	O
in	O
later	O
chapters	O
,	O
when	O
we	O
consider	O
ﬁtting	O
covariance	B
matrices	O
to	O
high	O
dimensional	O
data	O
.	O
4.6.2.2	O
univariate	O
posterior	O
in	O
the	O
1d	O
case	O
,	O
the	O
likelihood	B
has	O
the	O
form	O
p	O
(	O
d|σ2	O
)	O
∝	O
(	O
σ2	O
)	O
−n/2	O
exp	O
(	O
cid:13	O
)	O
n	O
(	O
cid:6	O
)	O
i=1	O
−	O
1	O
2σ2	O
(	O
cid:14	O
)	O
(	O
xi	O
−	O
μ	O
)	O
2	O
(	O
4.185	O
)	O
the	O
standard	O
conjugate	O
prior	O
is	O
the	O
inverse	B
gamma	I
distribution	O
,	O
which	O
is	O
just	O
the	O
scalar	O
version	O
of	O
the	O
inverse	O
wishart	O
:	O
ig	O
(	O
σ2|a0	O
,	O
b0	O
)	O
∝	O
(	O
σ2	O
)	O
−	O
(	O
a0+1	O
)	O
exp	O
(	O
−	O
b0	O
σ2	O
)	O
(	O
4.186	O
)	O
4.6.	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	O
131	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
0	O
prior	O
=	O
iw	O
(	O
ν=0.001	O
,	O
s=0.001	O
)	O
,	O
true	O
σ2=10.000	O
n=2	O
n=5	O
n=50	O
n=100	O
5	O
10	O
15	O
σ2	O
figure	O
4.18	O
sequential	B
updating	O
of	O
the	O
posterior	O
for	O
σ2	O
starting	O
from	O
an	O
uninformative	B
prior	O
.	O
the	O
data	O
was	O
generated	O
from	O
a	O
gaussian	O
with	O
known	O
mean	B
μ	O
=	O
5	O
and	O
unknown	B
variance	O
σ2	O
=	O
10.	O
figure	O
generated	O
by	O
gausssequpdatesigma1d	O
.	O
multiplying	O
the	O
likelihood	B
and	O
the	O
prior	O
,	O
we	O
see	O
that	O
the	O
posterior	O
is	O
also	O
ig	O
:	O
p	O
(	O
σ2|d	O
)	O
=	O
ig	O
(	O
σ2|an	O
,	O
bn	O
)	O
an	O
=	O
a0	O
+	O
n/2	O
bn	O
=	O
b0	O
+	O
1	O
2	O
(	O
xi	O
−	O
μ	O
)	O
2	O
n	O
(	O
cid:6	O
)	O
i=1	O
(	O
4.187	O
)	O
(	O
4.188	O
)	O
(	O
4.189	O
)	O
see	O
figure	O
4.18	O
for	O
an	O
illustration	O
.	O
2	O
.	O
this	O
arises	O
because	O
iw	O
(	O
σ2|s0	O
,	O
ν0	O
)	O
=	O
ig	O
(	O
σ2|	O
s0	O
the	O
form	O
of	O
the	O
posterior	O
is	O
not	O
quite	O
as	O
pretty	O
as	O
the	O
multivariate	O
case	O
,	O
because	O
of	O
the	O
factors	B
of	O
1	O
2	O
)	O
.	O
another	O
problem	O
with	O
using	O
the	O
ig	O
(	O
a0	O
,	O
b0	O
)	O
distribution	O
is	O
that	O
the	O
strength	O
of	O
the	O
prior	O
is	O
encoded	O
in	O
both	O
a0	O
and	O
b0	O
.	O
to	O
avoid	O
both	O
of	O
these	O
problems	O
,	O
it	O
is	O
common	O
(	O
in	O
the	O
statistics	O
literature	O
)	O
to	O
use	O
an	O
alternative	O
parameterization	O
of	O
the	O
ig	O
distribution	O
,	O
known	O
as	O
the	O
(	O
scaled	O
)	O
inverse	B
chi-squared	I
distribution	I
.	O
this	O
is	O
deﬁned	O
as	O
follows	O
:	O
2	O
,	O
ν0	O
χ−2	O
(	O
σ2|ν0	O
,	O
σ2	O
0	O
)	O
=	O
ig	O
(	O
σ2|	O
ν0	O
2	O
,	O
ν0σ2	O
0	O
2	O
)	O
∝	O
(	O
σ2	O
)	O
−ν0/2−1	O
exp	O
(	O
−	O
ν0σ2	O
2σ2	O
)	O
0	O
(	O
4.190	O
)	O
here	O
ν0	O
controls	O
the	O
strength	O
of	O
the	O
prior	O
,	O
and	O
σ2	O
prior	O
,	O
the	O
posterior	O
becomes	O
p	O
(	O
σ2|d	O
,	O
μ	O
)	O
=χ	O
−2	O
(	O
σ2|νn	O
,	O
σ2	O
n	O
)	O
(	O
cid:4	O
)	O
n	O
i=1	O
(	O
xi	O
−	O
μ	O
)	O
2	O
νn	O
νn	O
=	O
ν0	O
+	O
n	O
0	O
+	O
ν0σ2	O
σ2	O
n	O
=	O
0	O
encodes	O
the	O
value	O
of	O
the	O
prior	O
.	O
with	O
this	O
(	O
4.191	O
)	O
(	O
4.192	O
)	O
(	O
4.193	O
)	O
we	O
see	O
that	O
the	O
posterior	O
dof	O
νn	O
is	O
the	O
prior	O
dof	O
ν0	O
plus	O
n	O
,	O
and	O
the	O
posterior	O
sum	O
of	O
squares	O
νn	O
σ2	O
n	O
is	O
the	O
prior	O
sum	B
of	I
squares	I
ν0σ2	O
we	O
can	O
emulate	O
an	O
uninformative	B
prior	O
,	O
p	O
(	O
σ2	O
)	O
∝	O
σ−2	O
,	O
by	O
setting	O
ν0	O
=	O
0	O
,	O
which	O
makes	O
0	O
plus	O
the	O
data	O
sum	O
of	O
squares	O
.	O
intuitive	O
sense	O
(	O
since	O
it	O
corresponds	O
to	O
a	O
zero	O
virtual	O
sample	O
size	O
)	O
.	O
132	O
chapter	O
4.	O
gaussian	O
models	O
4.6.3	O
posterior	O
distribution	O
of	O
μ	O
and	O
σ	O
*	O
we	O
now	O
discuss	O
how	O
to	O
compute	O
p	O
(	O
μ	O
,	O
σ|d	O
)	O
.	O
these	O
results	O
are	O
a	O
bit	O
complex	O
,	O
but	O
will	O
prove	O
useful	O
later	O
on	O
in	O
this	O
book	O
.	O
feel	O
free	O
to	O
skip	O
this	O
section	O
on	O
a	O
ﬁrst	O
reading	O
.	O
4.6.3.1	O
likelihood	B
the	O
likelihood	B
is	O
given	O
by	O
p	O
(	O
d|μ	O
,	O
σ	O
)	O
=	O
(	O
2π	O
)	O
−n	O
d/2|σ|−	O
n	O
2	O
exp	O
(	O
cid:13	O
)	O
n	O
(	O
cid:6	O
)	O
i=1	O
−	O
1	O
2	O
(	O
cid:14	O
)	O
(	O
xi	O
−	O
μ	O
)	O
t	O
σ−1	O
(	O
xi	O
−	O
μ	O
)	O
now	O
one	O
can	O
show	O
that	O
n	O
(	O
cid:6	O
)	O
i=1	O
(	O
xi	O
−	O
μ	O
)	O
t	O
σ−1	O
(	O
xi	O
−	O
μ	O
)	O
=	O
tr	O
(	O
σ−1sx	O
)	O
+n	O
(	O
x	O
−	O
μ	O
)	O
t	O
σ−1	O
(	O
x	O
−	O
μ	O
)	O
hence	O
we	O
can	O
rewrite	O
the	O
likelihood	B
as	O
follows	O
:	O
p	O
(	O
d|μ	O
,	O
σ	O
)	O
=	O
(	O
2π	O
)	O
−n	O
d/2|σ|−	O
n	O
(	O
cid:7	O
)	O
−	O
n	O
2	O
exp	O
tr	O
(	O
σ−1sx	O
)	O
2	O
exp	O
(	O
cid:8	O
)	O
(	O
μ	O
−	O
x	O
)	O
t	O
σ−1	O
(	O
μ	O
−	O
x	O
)	O
−	O
n	O
2	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
we	O
will	O
use	O
this	O
form	O
below	O
.	O
4.6.3.2	O
prior	O
the	O
obvious	O
prior	O
to	O
use	O
is	O
the	O
following	O
p	O
(	O
μ	O
,	O
σ	O
)	O
=n	O
(	O
μ|m0	O
,	O
v0	O
)	O
iw	O
(	O
σ|s0	O
,	O
ν0	O
)	O
(	O
4.194	O
)	O
(	O
4.195	O
)	O
(	O
4.196	O
)	O
(	O
4.197	O
)	O
(	O
4.198	O
)	O
unfortunately	O
,	O
this	O
is	O
not	O
conjugate	O
to	O
the	O
likelihood	B
.	O
to	O
see	O
why	O
,	O
note	O
that	O
μ	O
and	O
σ	O
appear	O
together	O
in	O
a	O
non-factorized	O
way	O
in	O
the	O
likelihood	B
;	O
hence	O
they	O
will	O
also	O
be	O
coupled	O
together	O
in	O
the	O
posterior	O
.	O
the	O
above	O
prior	O
is	O
sometimes	O
called	O
semi-conjugate	B
or	O
conditionally	B
conjugate	I
,	O
since	O
both	O
conditionals	O
,	O
p	O
(	O
μ|σ	O
)	O
and	O
p	O
(	O
σ|μ	O
)	O
,	O
are	O
individually	O
conjugate	O
.	O
to	O
create	O
a	O
full	B
conjugate	O
prior	O
,	O
we	O
need	O
to	O
use	O
a	O
prior	O
where	O
μ	O
and	O
σ	O
are	O
dependent	O
on	O
each	O
other	O
.	O
we	O
will	O
use	O
a	O
joint	B
distribution	I
of	O
the	O
form	O
p	O
(	O
μ	O
,	O
σ	O
)	O
=p	O
(	O
σ	O
)	O
p	O
(	O
μ|σ	O
)	O
(	O
4.199	O
)	O
looking	O
at	O
the	O
form	O
of	O
the	O
likelihood	B
equation	O
,	O
equation	O
4.197	O
,	O
we	O
see	O
that	O
a	O
natural	O
conjugate	O
4.6.	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	O
prior	O
has	O
the	O
form	O
of	O
a	O
normal-inverse-wishart	O
or	O
niw	O
distribution	O
,	O
deﬁned	O
as	O
follows	O
:	O
niw	O
(	O
μ	O
,	O
σ|m0	O
,	O
κ0	O
,	O
ν0	O
,	O
s0	O
)	O
(	O
cid:2	O
)	O
σ	O
)	O
×	O
iw	O
(	O
σ|s0	O
,	O
ν0	O
)	O
,	O
(	O
μ	O
−	O
m0	O
)	O
t	O
σ−1	O
(	O
μ	O
−	O
m0	O
)	O
tr	O
(	O
σ−1s0	O
)	O
(	O
cid:8	O
)	O
n	O
(	O
μ|m0	O
,	O
1	O
κ0	O
|σ|−	O
1	O
zn	O
iw	O
×|σ|−	O
ν0+d+1	O
1	O
2	O
exp	O
+	O
−	O
κ0	O
(	O
cid:7	O
)	O
2	O
−	O
1	O
2	O
2	O
exp	O
|σ|−	O
ν0+d+2	O
(	O
cid:7	O
)	O
2	O
1	O
zn	O
iw	O
×	O
exp	O
=	O
=	O
−	O
κ0	O
2	O
(	O
μ	O
−	O
m0	O
)	O
t	O
σ−1	O
(	O
μ	O
−	O
m0	O
)	O
−	O
1	O
2	O
zn	O
iw	O
=	O
2v0d/2γd	O
(	O
ν0/2	O
)	O
(	O
2π/κ0	O
)	O
d/2|s0|−ν0/2	O
(	O
cid:8	O
)	O
tr	O
(	O
σ−1s0	O
)	O
133	O
(	O
4.200	O
)	O
(	O
4.201	O
)	O
(	O
4.202	O
)	O
(	O
4.203	O
)	O
(	O
4.204	O
)	O
(	O
4.205	O
)	O
(	O
4.206	O
)	O
where	O
γd	O
(	O
a	O
)	O
is	O
the	O
multivariate	B
gamma	I
function	I
.	O
the	O
parameters	O
of	O
the	O
niw	O
can	O
be	O
interpreted	O
as	O
follows	O
:	O
m0	O
is	O
our	O
prior	O
mean	B
for	O
μ	O
,	O
and	O
κ0	O
is	O
how	O
strongly	O
we	O
believe	O
this	O
prior	O
;	O
and	O
s0	O
is	O
(	O
proportional	O
to	O
)	O
our	O
prior	O
mean	B
for	O
σ	O
,	O
and	O
ν0	O
is	O
how	O
strongly	O
we	O
believe	O
this	O
prior.3	O
one	O
can	O
show	O
(	O
minka	O
2000f	O
)	O
that	O
the	O
(	O
improper	O
)	O
uninformative	B
prior	O
has	O
the	O
form	O
lim	O
k→0	O
n	O
(	O
μ|m0	O
,	O
σ/k	O
)	O
iw	O
(	O
σ|s0	O
,	O
k	O
)	O
∝	O
|2πς|−	O
1	O
∝	O
|σ|−	O
(	O
d	O
2|σ|−	O
(	O
d+1	O
)	O
/2	O
2	O
+1	O
)	O
∝	O
niw	O
(	O
μ	O
,	O
σ|0	O
,	O
0	O
,	O
0	O
,	O
0i	O
)	O
(	O
4.207	O
)	O
(	O
4.208	O
)	O
in	O
practice	O
,	O
it	O
is	O
often	O
better	O
to	O
use	O
a	O
weakly	O
informative	O
data-dependent	O
prior	O
.	O
a	O
common	O
(	O
fraley	O
and	O
raftery	O
2007	O
,	O
p6	O
)	O
)	O
is	O
to	O
use	O
s0	O
=	O
choice	O
(	O
see	O
e.g.	O
,	O
diag	O
(	O
sx	O
)	O
/n	O
,	O
and	O
ν0	O
=	O
d	O
+	O
2	O
,	O
to	O
ensure	O
e	O
[	O
σ	O
]	O
=	O
s0	O
,	O
and	O
to	O
set	O
μ0	O
=	O
x	O
and	O
κ0	O
to	O
some	O
small	O
number	O
,	O
such	O
as	O
0.01	O
.	O
(	O
chipman	O
et	O
al	O
.	O
2001	O
,	O
p81	O
)	O
,	O
3.	O
although	O
this	O
prior	O
has	O
four	O
parameters	O
,	O
there	O
are	O
really	O
only	O
three	O
free	O
parameters	O
,	O
since	O
our	O
uncertainty	B
in	O
the	O
mean	B
is	O
proportional	O
to	O
the	O
variance	B
.	O
in	O
particular	O
,	O
if	O
we	O
believe	O
that	O
the	O
variance	B
is	O
large	O
,	O
then	O
our	O
uncertainty	B
in	O
μ	O
must	O
be	O
large	O
too	O
.	O
this	O
makes	O
sense	O
intuitively	O
,	O
since	O
if	O
the	O
data	O
has	O
large	O
spread	O
,	O
it	O
may	O
be	O
hard	O
to	O
pin	O
down	O
its	O
mean	B
.	O
see	O
also	O
exercise	O
9.1	O
,	O
where	O
we	O
will	O
see	O
the	O
three	O
free	O
parameters	O
more	O
explicitly	O
.	O
if	O
we	O
want	O
separate	O
“	O
control	O
”	O
over	O
our	O
conﬁdence	O
in	O
μ	O
and	O
σ	O
,	O
we	O
must	O
use	O
a	O
semi-conjugate	B
prior	O
.	O
134	O
4.6.3.3	O
posterior	O
chapter	O
4.	O
gaussian	O
models	O
the	O
posterior	O
can	O
be	O
shown	O
(	O
exercise	O
4.11	O
)	O
to	O
be	O
niw	O
with	O
updated	O
parameters	O
:	O
p	O
(	O
μ	O
,	O
σ|d	O
)	O
=	O
niw	O
(	O
μ	O
,	O
σ|mn	O
,	O
κn	O
,	O
νn	O
,	O
sn	O
)	O
m0	O
+	O
κ0m0	O
+	O
n	O
x	O
mn	O
=	O
κ0	O
=	O
κ0	O
+	O
n	O
n	O
κ0	O
+	O
n	O
x	O
(	O
4.209	O
)	O
(	O
4.210	O
)	O
κn	O
κn	O
=	O
κ0	O
+	O
n	O
νn	O
=	O
ν0	O
+	O
n	O
sn	O
=	O
s0	O
+	O
sx	O
+	O
κ0	O
+	O
n	O
(	O
cid:4	O
)	O
n	O
=	O
s0	O
+	O
s	O
+	O
κ0m0mt	O
i=1	O
xixt	O
κ0n	O
(	O
x	O
−	O
m0	O
)	O
(	O
x	O
−	O
m0	O
)	O
t	O
0	O
−	O
κn	O
mn	O
mt	O
i	O
as	O
the	O
uncentered	O
sum-of-squares	O
matrix	O
(	O
this	O
is	O
easier	O
(	O
4.213	O
)	O
(	O
4.214	O
)	O
n	O
where	O
we	O
have	O
deﬁned	O
s	O
(	O
cid:2	O
)	O
to	O
update	O
incrementally	O
than	O
the	O
centered	O
version	O
)	O
.	O
this	O
result	O
is	O
actually	O
quite	O
intuitive	O
:	O
the	O
posterior	B
mean	I
is	O
a	O
convex	B
combination	I
of	O
the	O
prior	O
mean	B
and	O
the	O
mle	O
,	O
with	O
“	O
strength	O
”	O
κ0	O
+	O
n	O
;	O
and	O
the	O
posterior	O
scatter	O
matrix	O
sn	O
is	O
the	O
prior	O
scatter	O
matrix	O
s0	O
plus	O
the	O
empirical	O
scatter	O
matrix	O
sx	O
plus	O
an	O
extra	O
term	O
due	O
to	O
the	O
uncertainty	B
in	O
the	O
mean	B
(	O
which	O
creates	O
its	O
own	O
virtual	O
scatter	O
matrix	O
)	O
.	O
4.6.3.4	O
posterior	B
mode	I
the	O
mode	B
of	O
the	O
joint	B
distribution	I
has	O
the	O
following	O
form	O
:	O
argmax	O
p	O
(	O
μ	O
,	O
σ|d	O
)	O
=	O
(	O
mn	O
,	O
sn	O
νn	O
+	O
d	O
+	O
2	O
)	O
if	O
we	O
set	O
κ0	O
=	O
0	O
,	O
this	O
reduces	O
to	O
argmax	O
p	O
(	O
μ	O
,	O
σ|d	O
)	O
=	O
(	O
x	O
,	O
s0	O
+	O
sx	O
ν0	O
+	O
n	O
+	O
d	O
+	O
2	O
)	O
(	O
4.215	O
)	O
(	O
4.216	O
)	O
the	O
corresponding	O
estimate	O
ˆσ	O
is	O
almost	O
the	O
same	O
as	O
equation	O
4.183	O
,	O
but	O
differs	O
by	O
1	O
in	O
the	O
denominator	O
,	O
because	O
this	O
is	O
the	O
mode	B
of	O
the	O
joint	O
,	O
not	O
the	O
mode	B
of	O
the	O
marginal	O
.	O
(	O
4.211	O
)	O
(	O
4.212	O
)	O
(	O
4.217	O
)	O
(	O
4.218	O
)	O
4.6.3.5	O
posterior	O
marginals	O
the	O
posterior	O
marginal	O
for	O
σ	O
is	O
simply	O
p	O
(	O
σ|d	O
)	O
=	O
p	O
(	O
μ	O
,	O
σ|d	O
)	O
dμ	O
=	O
iw	O
(	O
σ|sn	O
,	O
νn	O
)	O
the	O
mode	B
and	O
mean	B
of	O
this	O
marginal	O
are	O
given	O
by	O
νn	O
−	O
d	O
−	O
1	O
νn	O
+	O
d	O
+	O
1	O
,	O
e	O
[	O
σ	O
]	O
=	O
ˆσmap	O
=	O
sn	O
sn	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
one	O
can	O
show	O
that	O
the	O
posterior	O
marginal	O
for	O
μ	O
has	O
a	O
multivariate	O
student	O
t	O
distribution	O
:	O
p	O
(	O
μ|d	O
)	O
=	O
p	O
(	O
μ	O
,	O
σ|d	O
)	O
dς	O
=	O
t	O
(	O
μ|mn	O
,	O
sn	O
,	O
νn	O
−	O
d	O
+	O
1	O
)	O
1	O
(	O
4.219	O
)	O
κn	O
(	O
νn	O
−	O
d	O
+	O
1	O
)	O
this	O
follows	O
from	O
the	O
fact	O
that	O
the	O
student	O
distribution	O
can	O
be	O
represented	O
as	O
a	O
scaled	O
mixture	O
of	O
gaussians	O
(	O
see	O
equation	O
11.61	O
)	O
.	O
4.6.	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	O
135	O
0	O
is	O
the	O
prior	O
variance	B
and	O
ν0	O
is	O
how	O
strongly	O
we	O
believe	O
this	O
.	O
figure	O
4.19	O
the	O
n	O
iχ2	O
(	O
m0	O
,	O
κ0	O
,	O
ν0	O
,	O
σ2	O
believe	O
this	O
;	O
σ2	O
1	O
,	O
σ2	O
0	O
=	O
1.	O
notice	O
that	O
the	O
contour	O
plot	O
(	O
underneath	O
the	O
surface	O
)	O
is	O
shaped	O
like	O
a	O
“	O
squashed	O
egg	O
”	O
.	O
increase	O
the	O
strength	O
of	O
our	O
belief	O
in	O
the	O
mean	B
,	O
so	O
it	O
gets	O
narrower	O
:	O
m0	O
=	O
0	O
,	O
κ0	O
=	O
5	O
,	O
ν0	O
=	O
1	O
,	O
σ2	O
we	O
increase	O
the	O
strength	O
of	O
our	O
belief	O
in	O
the	O
variance	B
,	O
so	O
it	O
gets	O
narrower	O
:	O
m0	O
=	O
0	O
,	O
κ0	O
=	O
1	O
,	O
ν0	O
=	O
5	O
,	O
σ2	O
1.	O
figure	O
generated	O
by	O
nixdemo2	O
.	O
0	O
)	O
distribution	O
.	O
m0	O
is	O
the	O
prior	O
mean	B
and	O
κ0	O
is	O
how	O
strongly	O
we	O
(	O
a	O
)	O
m0	O
=	O
0	O
,	O
κ0	O
=	O
1	O
,	O
ν0	O
=	O
(	O
b	O
)	O
we	O
0	O
=	O
1	O
.	O
(	O
c	O
)	O
0	O
=	O
4.6.3.6	O
posterior	O
predictive	O
the	O
posterior	O
predictive	O
is	O
given	O
by	O
p	O
(	O
x|d	O
)	O
=	O
p	O
(	O
x	O
,	O
d	O
)	O
p	O
(	O
d	O
)	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
so	O
it	O
can	O
be	O
easily	O
evaluated	O
in	O
terms	O
of	O
a	O
ratio	O
of	O
marginal	O
likelihoods	O
.	O
it	O
turns	O
out	O
that	O
this	O
ratio	O
has	O
the	O
form	O
of	O
a	O
multivariate	O
student-t	O
distribution	O
:	O
p	O
(	O
x|d	O
)	O
=	O
n	O
(	O
x|μ	O
,	O
σ	O
)	O
niw	O
(	O
μ	O
,	O
σ|mn	O
,	O
κn	O
,	O
νn	O
,	O
sn	O
)	O
dμdς	O
=	O
t	O
(	O
x|mn	O
,	O
κn	O
+	O
1	O
κn	O
(	O
νn	O
−	O
d	O
+	O
1	O
)	O
sn	O
,	O
νn	O
−	O
d	O
+	O
1	O
)	O
(	O
4.220	O
)	O
(	O
4.221	O
)	O
(	O
4.222	O
)	O
the	O
student-t	O
has	O
wider	O
tails	O
than	O
a	O
gaussian	O
,	O
which	O
takes	O
into	O
account	O
the	O
fact	O
that	O
σ	O
is	O
unknown	B
.	O
however	O
,	O
this	O
rapidly	O
becomes	O
gaussian-like	O
.	O
4.6.3.7	O
posterior	O
for	O
scalar	O
data	O
we	O
now	O
specialise	O
the	O
above	O
results	O
to	O
the	O
case	O
where	O
xi	O
is	O
1d	O
.	O
these	O
results	O
are	O
widely	O
used	O
in	O
the	O
statistics	O
literature	O
.	O
as	O
in	O
section	O
4.6.2.2	O
,	O
it	O
is	O
conventional	O
not	O
to	O
use	O
the	O
normal	O
inverse	O
136	O
chapter	O
4.	O
gaussian	O
models	O
wishart	O
,	O
but	O
to	O
use	O
the	O
normal	B
inverse	I
chi-squared	I
or	O
nix	O
distribution	O
,	O
deﬁned	O
by	O
n	O
iχ2	O
(	O
μ	O
,	O
σ2|m0	O
,	O
κ0	O
,	O
ν0	O
,	O
σ2	O
0	O
)	O
(	O
cid:2	O
)	O
n	O
(	O
μ|m0	O
,	O
σ2/κ0	O
)	O
χ−2	O
(	O
σ2|ν0	O
,	O
σ2	O
0	O
)	O
(	O
cid:7	O
)	O
∝	O
(	O
1	O
σ2	O
)	O
(	O
ν0+3	O
)	O
/2	O
exp	O
0	O
+	O
κ0	O
(	O
μ	O
−	O
m0	O
)	O
2	O
−	O
ν0σ2	O
2σ2	O
(	O
cid:8	O
)	O
(	O
4.223	O
)	O
(	O
4.224	O
)	O
see	O
figure	O
4.19	O
for	O
some	O
plots	O
.	O
along	O
the	O
μ	O
axis	O
,	O
the	O
distribution	O
is	O
shaped	O
like	O
a	O
gaussian	O
,	O
and	O
along	O
the	O
σ2	O
axis	O
,	O
the	O
distribution	O
is	O
shaped	O
like	O
a	O
χ−2	O
;	O
the	O
contours	O
of	O
the	O
joint	O
density	O
have	O
interestingly	O
,	O
we	O
see	O
that	O
the	O
contours	O
for	O
μ	O
are	O
more	O
peaked	O
a	O
“	O
squashed	O
egg	O
”	O
appearance	O
.	O
for	O
small	O
values	O
of	O
σ2	O
,	O
which	O
makes	O
sense	O
,	O
since	O
if	O
the	O
data	O
is	O
low	O
variance	O
,	O
we	O
will	O
be	O
able	O
to	O
estimate	O
its	O
mean	B
more	O
reliably	O
.	O
one	O
can	O
show	O
that	O
the	O
posterior	O
is	O
given	O
by	O
p	O
(	O
μ	O
,	O
σ2|d	O
)	O
=n	O
iχ	O
2	O
(	O
μ	O
,	O
σ2|mn	O
,	O
κn	O
,	O
νn	O
,	O
σ2	O
n	O
)	O
κ0m0	O
+	O
n	O
x	O
mn	O
=	O
κn	O
κn	O
=	O
κ0	O
+	O
n	O
νn	O
=	O
ν0	O
+	O
n	O
νn	O
σ2	O
n	O
=	O
ν0σ2	O
0	O
+	O
n	O
(	O
cid:6	O
)	O
i=1	O
(	O
xi	O
−	O
x	O
)	O
2	O
+	O
n	O
κ0	O
κ0	O
+	O
n	O
(	O
m0	O
−	O
x	O
)	O
2	O
let	O
us	O
see	O
how	O
these	O
results	O
look	O
if	O
we	O
use	O
the	O
following	O
uninformative	B
prior	O
:	O
p	O
(	O
μ	O
,	O
σ2	O
)	O
∝	O
p	O
(	O
μ	O
)	O
p	O
(	O
σ2	O
)	O
∝	O
σ−2	O
∝	O
n	O
iχ2	O
(	O
μ	O
,	O
σ2|μ0	O
=	O
0	O
,	O
κ0	O
=	O
0	O
,	O
ν0	O
=	O
−1	O
,	O
σ2	O
0	O
=	O
0	O
)	O
with	O
this	O
prior	O
,	O
the	O
posterior	O
has	O
the	O
form	O
p	O
(	O
μ	O
,	O
σ2|d	O
)	O
=n	O
iχ	O
2	O
(	O
μ	O
,	O
σ2|mn	O
=	O
x	O
,	O
κn	O
=	O
n	O
,	O
νn	O
=	O
n	O
−	O
1	O
,	O
σ2	O
n	O
=	O
s2	O
)	O
where	O
s2	O
(	O
cid:2	O
)	O
1	O
n	O
−	O
1	O
n	O
(	O
cid:6	O
)	O
i=1	O
(	O
xi	O
−	O
x	O
)	O
2	O
=	O
n	O
n	O
−	O
1	O
ˆσ2	O
mle	O
is	O
the	O
the	O
sample	B
standard	I
deviation	I
.	O
estimate	O
of	O
the	O
variance	B
.	O
)	O
hence	O
the	O
marginal	O
posterior	O
for	O
the	O
mean	B
is	O
given	O
by	O
(	O
in	O
section	O
6.4.2	O
,	O
we	O
show	O
that	O
this	O
is	O
an	O
unbiased	B
p	O
(	O
μ|d	O
)	O
=	O
t	O
(	O
μ|x	O
,	O
,	O
n	O
−	O
1	O
)	O
s2	O
n	O
(	O
4.235	O
)	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
the	O
posterior	O
marginal	O
for	O
σ2	O
is	O
just	O
p	O
(	O
σ2|d	O
)	O
=	O
(	O
cid:20	O
)	O
with	O
the	O
posterior	B
mean	I
given	O
by	O
e	O
σ2|d	O
(	O
cid:21	O
)	O
p	O
(	O
μ	O
,	O
σ2|d	O
)	O
dμ	O
=	O
χ−2	O
(	O
σ2|νn	O
,	O
σ2	O
n	O
)	O
=	O
νn	O
νn−2	O
σ2	O
n	O
.	O
mixture	B
representation	O
of	O
the	O
student	O
:	O
p	O
(	O
μ|d	O
)	O
=	O
p	O
(	O
μ	O
,	O
σ2|d	O
)	O
dσ2	O
=	O
t	O
(	O
μ|mn	O
,	O
σ2	O
with	O
the	O
posterior	B
mean	I
given	O
by	O
e	O
[	O
μ|d	O
]	O
=	O
mn	O
.	O
n	O
/κn	O
,	O
νn	O
)	O
the	O
posterior	O
marginal	O
for	O
μ	O
has	O
a	O
student	O
t	O
distribution	O
,	O
which	O
follows	O
from	O
the	O
scale	O
(	O
4.225	O
)	O
(	O
4.226	O
)	O
(	O
4.227	O
)	O
(	O
4.228	O
)	O
(	O
4.229	O
)	O
(	O
4.230	O
)	O
(	O
4.231	O
)	O
(	O
4.232	O
)	O
(	O
4.233	O
)	O
(	O
4.234	O
)	O
4.6.	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	O
and	O
the	O
posterior	O
variance	O
of	O
μ	O
is	O
νn	O
νn	O
−	O
2	O
σ2	O
n	O
=	O
n	O
−	O
1	O
n	O
−	O
3	O
s2	O
n	O
→	O
s2	O
n	O
var	O
[	O
μ|d	O
]	O
=	O
(	O
cid:9	O
)	O
var	O
[	O
μ|d	O
]	O
≈	O
s√	O
n	O
the	O
square	O
root	O
of	O
this	O
is	O
called	O
the	O
standard	B
error	I
of	I
the	I
mean	I
:	O
137	O
(	O
4.236	O
)	O
(	O
4.237	O
)	O
(	O
4.238	O
)	O
thus	O
an	O
approximate	O
95	O
%	O
posterior	O
credible	O
interval	O
for	O
the	O
mean	B
is	O
i.95	O
(	O
μ|d	O
)	O
=	O
x	O
±	O
2	O
s√	O
n	O
4.6.3.8	O
(	O
bayesian	O
credible	O
intervals	O
are	O
discussed	O
in	O
more	O
detail	O
in	O
section	O
5.2.2	O
;	O
they	O
are	O
contrasted	O
with	O
frequentist	B
conﬁdence	O
intervals	O
in	O
section	O
6.6.1	O
.	O
)	O
(	O
cid:11	O
)	O
∞	O
bayesian	O
t-test	B
suppose	O
we	O
want	O
to	O
test	O
the	O
hypothesis	O
that	O
μ	O
(	O
cid:6	O
)	O
=	O
μ0	O
for	O
some	O
known	O
value	O
μ0	O
(	O
often	O
0	O
)	O
,	O
given	O
values	O
xi	O
∼	O
n	O
(	O
μ	O
,	O
σ2	O
)	O
.	O
this	O
is	O
called	O
a	O
two-sided	O
,	O
one-sample	O
t-test	B
.	O
a	O
simple	O
way	O
to	O
perform	O
such	O
a	O
test	O
is	O
just	O
to	O
check	O
if	O
μ0	O
∈	O
i0.95	O
(	O
μ|d	O
)	O
.	O
if	O
it	O
is	O
not	O
,	O
then	O
we	O
can	O
be	O
95	O
%	O
sure	O
that	O
μ	O
(	O
cid:6	O
)	O
=	O
μ0.4	O
a	O
more	O
common	O
scenario	O
is	O
when	O
we	O
want	O
to	O
test	O
if	O
two	O
paired	O
samples	O
have	O
the	O
same	O
mean	B
.	O
more	O
precisely	O
,	O
suppose	O
yi	O
∼	O
n	O
(	O
μ1	O
,	O
σ2	O
)	O
and	O
zi	O
∼	O
n	O
(	O
μ2	O
,	O
σ2	O
)	O
.	O
we	O
want	O
to	O
determine	O
if	O
μ	O
=	O
μ1	O
−	O
μ2	O
>	O
0	O
,	O
using	O
xi	O
=	O
yi	O
−	O
zi	O
as	O
our	O
data	O
.	O
we	O
can	O
evaluate	O
this	O
quantity	O
as	O
follows	O
:	O
p	O
(	O
μ	O
>	O
μ0|d	O
)	O
=	O
p	O
(	O
μ|d	O
)	O
dμ	O
(	O
4.239	O
)	O
μ0	O
this	O
is	O
called	O
a	O
one-sided	O
,	O
paired	B
t-test	I
.	O
the	O
difference	O
in	O
binomial	B
proportions	O
,	O
see	O
section	O
5.2.3	O
.	O
)	O
(	O
for	O
a	O
similar	B
approach	O
to	O
unpaired	O
tests	O
,	O
comparing	O
to	O
calculate	O
the	O
posterior	O
,	O
we	O
must	O
specify	O
a	O
prior	O
.	O
suppose	O
we	O
use	O
an	O
uninformative	B
prior	O
.	O
as	O
we	O
showed	O
above	O
,	O
we	O
ﬁnd	O
that	O
the	O
posterior	O
marginal	O
on	O
μ	O
has	O
the	O
form	O
p	O
(	O
μ|d	O
)	O
=	O
t	O
(	O
μ|x	O
,	O
,	O
n	O
−	O
1	O
)	O
s2	O
n	O
now	O
let	O
us	O
deﬁne	O
the	O
following	O
t	B
statistic	I
:	O
t	O
(	O
cid:2	O
)	O
x	O
−	O
μ0	O
√	O
s/	O
n	O
where	O
the	O
denominator	O
is	O
the	O
standard	B
error	I
of	I
the	I
mean	I
.	O
we	O
see	O
that	O
p	O
(	O
μ|d	O
)	O
=	O
1	O
−	O
fn−1	O
(	O
t	O
)	O
where	O
fν	O
(	O
t	O
)	O
is	O
the	O
cdf	B
of	O
the	O
standard	O
student	O
t	O
distribution	O
t	O
(	O
0	O
,	O
1	O
,	O
ν	O
)	O
.	O
(	O
4.240	O
)	O
(	O
4.241	O
)	O
(	O
4.242	O
)	O
4.	O
a	O
more	O
complex	O
approach	O
is	O
to	O
perform	O
bayesian	O
model	O
comparison	O
.	O
that	O
is	O
,	O
we	O
compute	O
the	O
bayes	O
factor	B
(	O
described	O
in	O
section	O
5.3.3	O
)	O
p	O
(	O
d|h0	O
)	O
/p	O
(	O
d|h1	O
)	O
,	O
where	O
h0	O
is	O
the	O
point	O
null	O
hypothesis	O
that	O
μ	O
=	O
μ0	O
,	O
and	O
h1	O
is	O
the	O
alternative	B
hypothesis	I
that	O
μ	O
(	O
cid:3	O
)	O
=	O
μ0	O
.	O
see	O
(	O
gonen	O
et	O
al	O
.	O
2005	O
;	O
rouder	O
et	O
al	O
.	O
2009	O
)	O
for	O
details	O
.	O
138	O
chapter	O
4.	O
gaussian	O
models	O
4.6.3.9	O
connection	O
with	O
frequentist	B
statistics	I
*	O
if	O
we	O
use	O
an	O
uninformative	B
prior	O
,	O
it	O
turns	O
out	O
that	O
the	O
above	O
bayesian	O
analysis	O
gives	O
the	O
same	O
result	O
as	O
derived	O
using	O
frequentist	B
methods	O
.	O
(	O
we	O
discuss	O
frequentist	B
statistics	I
in	O
chapter	O
6	O
.	O
)	O
speciﬁcally	O
,	O
from	O
the	O
above	O
results	O
,	O
we	O
see	O
that	O
μ	O
−	O
x	O
(	O
cid:9	O
)	O
s/n	O
|d	O
∼	O
tn−1	O
(	O
4.243	O
)	O
(	O
4.244	O
)	O
this	O
has	O
the	O
same	O
form	O
as	O
the	O
sampling	B
distribution	I
of	O
the	O
mle	O
:	O
μ	O
−	O
x	O
(	O
cid:9	O
)	O
s/n	O
|μ	O
∼	O
tn−1	O
the	O
reason	O
is	O
that	O
the	O
student	O
distribution	O
is	O
symmetric	B
in	O
its	O
ﬁrst	O
two	O
arguments	O
,	O
so	O
t	O
(	O
x|μ	O
,	O
σ2	O
,	O
ν	O
)	O
=	O
t	O
(	O
μ|x	O
,	O
σ2	O
,	O
ν	O
)	O
;	O
hence	O
statements	O
about	O
the	O
posterior	O
for	O
μ	O
have	O
the	O
same	O
form	O
as	O
statements	O
about	O
the	O
sampling	B
distribution	I
of	O
x.	O
consequently	O
,	O
the	O
(	O
one-sided	O
)	O
p-value	B
(	O
deﬁned	O
in	O
sec-	O
tion	O
6.6.2	O
)	O
returned	O
by	O
a	O
frequentist	B
test	O
is	O
the	O
same	O
as	O
p	O
(	O
μ	O
>	O
μ0|d	O
)	O
returned	O
by	O
the	O
bayesian	O
method	O
.	O
see	O
bayesttestdemo	O
for	O
an	O
example	O
.	O
despite	O
the	O
superﬁcial	O
similarity	O
,	O
these	O
two	O
results	O
have	O
a	O
different	O
interpretation	O
:	O
in	O
the	O
bayesian	O
approach	O
,	O
μ	O
is	O
unknown	B
and	O
x	O
is	O
ﬁxed	O
,	O
whereas	O
in	O
the	O
frequentist	B
approach	O
,	O
x	O
is	O
unknown	B
and	O
μ	O
is	O
ﬁxed	O
.	O
more	O
equivalences	O
between	O
frequentist	B
and	O
bayesian	O
inference	B
in	O
simple	O
models	O
using	O
uninformative	B
priors	O
can	O
be	O
found	O
in	O
(	O
box	O
and	O
tiao	O
1973	O
)	O
.	O
see	O
also	O
section	O
7.6.3.3	O
.	O
4.6.4	O
sensor	B
fusion	I
with	O
unknown	B
precisions	O
*	O
in	O
this	O
section	O
,	O
we	O
apply	O
the	O
results	O
in	O
section	O
4.6.3	O
to	O
the	O
problem	O
of	O
sensor	B
fusion	I
in	O
the	O
case	O
where	O
the	O
precision	B
of	O
each	O
measurement	O
device	O
is	O
unknown	B
.	O
this	O
generalizes	O
the	O
results	O
of	O
section	O
4.4.2.2	O
,	O
where	O
the	O
measurement	O
model	O
was	O
assumed	O
to	O
be	O
gaussian	O
with	O
known	O
precision	B
.	O
the	O
unknown	B
precision	O
case	O
turns	O
out	O
to	O
give	O
qualitatively	O
different	O
results	O
,	O
yielding	O
a	O
potentially	O
multi-modal	O
posterior	O
as	O
we	O
will	O
see	O
.	O
our	O
presentation	O
is	O
based	O
on	O
(	O
minka	O
2001e	O
)	O
.	O
suppose	O
we	O
want	O
to	O
pool	O
data	O
from	O
multiple	O
sources	O
to	O
estimate	O
some	O
quantity	O
μ	O
∈	O
r	O
,	O
but	O
the	O
reliability	O
of	O
the	O
sources	O
is	O
unknown	B
.	O
speciﬁcally	O
,	O
suppose	O
we	O
have	O
two	O
different	O
measurement	O
devices	O
,	O
x	O
and	O
y	O
,	O
with	O
different	O
precisions	O
:	O
xi|μ	O
∼	O
n	O
(	O
μ	O
,	O
λ−1	O
y	O
)	O
.	O
we	O
make	O
two	O
independent	O
measurements	O
with	O
each	O
device	O
,	O
which	O
turn	O
out	O
to	O
be	O
x	O
)	O
and	O
yi|μ	O
∼	O
n	O
(	O
μ	O
,	O
λ−1	O
x1	O
=	O
1.1	O
,	O
x2	O
=	O
1.9	O
,	O
y1	O
=	O
2.9	O
,	O
y2	O
=	O
4.1	O
(	O
4.245	O
)	O
we	O
will	O
use	O
a	O
non-informative	B
prior	O
for	O
μ	O
,	O
p	O
(	O
μ	O
)	O
∝	O
1	O
,	O
which	O
we	O
can	O
emulate	O
using	O
an	O
inﬁnitely	O
broad	O
gaussian	O
,	O
p	O
(	O
μ	O
)	O
=n	O
(	O
μ|m0	O
=	O
0	O
,	O
λ−1	O
0	O
=	O
∞	O
)	O
.	O
if	O
the	O
λx	O
and	O
λy	O
terms	O
were	O
known	O
,	O
then	O
the	O
posterior	O
would	O
be	O
gaussian	O
:	O
p	O
(	O
μ|d	O
,	O
λx	O
,	O
λy	O
)	O
=n	O
(	O
μ|mn	O
,	O
λ−1	O
n	O
)	O
λn	O
=	O
λ0	O
+	O
nxλx	O
+	O
nyλy	O
mn	O
=	O
λxnxx	O
+	O
λynyy	O
nxλx	O
+	O
nyλy	O
(	O
4.246	O
)	O
(	O
4.247	O
)	O
(	O
4.248	O
)	O
4.6.	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	O
139	O
(	O
cid:4	O
)	O
nx	O
i=1	O
xi	O
=	O
1.5	O
and	O
y	O
=	O
1	O
ny	O
where	O
nx	O
=	O
2	O
is	O
the	O
number	O
of	O
x	O
measurements	O
,	O
ny	O
=	O
2	O
is	O
the	O
number	O
of	O
y	O
measurements	O
,	O
x	O
=	O
1	O
i=1	O
yi	O
=	O
3.5.	O
this	O
result	O
follows	O
because	O
the	O
posterior	O
nx	O
precision	B
is	O
the	O
sum	O
of	O
the	O
measurement	O
precisions	O
,	O
and	O
the	O
posterior	B
mean	I
is	O
a	O
weighted	O
sum	O
of	O
the	O
prior	O
mean	B
(	O
which	O
is	O
0	O
)	O
and	O
the	O
data	O
means	O
.	O
(	O
cid:4	O
)	O
ny	O
however	O
,	O
the	O
measurement	O
precisions	O
are	O
not	O
known	O
.	O
initially	O
we	O
will	O
estimate	O
them	O
by	O
maximum	O
likelihood	O
.	O
the	O
log-likelihood	O
is	O
given	O
by	O
(	O
cid:9	O
)	O
(	O
μ	O
,	O
λx	O
,	O
λy	O
)	O
=	O
log	O
λx	O
−	O
λx	O
2	O
(	O
xi	O
−	O
μ	O
)	O
2	O
+	O
log	O
λy	O
−	O
λy	O
2	O
(	O
cid:6	O
)	O
i	O
(	O
cid:6	O
)	O
i	O
(	O
yi	O
−	O
μ	O
)	O
2	O
the	O
mle	O
is	O
obtained	O
by	O
solving	O
the	O
following	O
simultaneous	O
equations	O
:	O
∂	O
(	O
cid:9	O
)	O
∂μ	O
∂	O
(	O
cid:9	O
)	O
∂λx	O
∂	O
(	O
cid:9	O
)	O
∂λy	O
=	O
λxnx	O
(	O
x	O
−	O
μ	O
)	O
+λ	O
yny	O
(	O
y	O
−	O
μ	O
)	O
=	O
0	O
nx	O
(	O
cid:6	O
)	O
=	O
=	O
1	O
λx	O
−	O
1	O
nx	O
1	O
λy	O
−	O
1	O
ny	O
(	O
xi	O
−	O
μ	O
)	O
2	O
=	O
0	O
i=1	O
ny	O
(	O
cid:6	O
)	O
i=1	O
(	O
yi	O
−	O
μ	O
)	O
2	O
=	O
0	O
this	O
gives	O
ˆμ	O
=	O
1/ˆλx	O
=	O
1/ˆλy	O
=	O
nxˆλxx	O
+	O
ny	O
ˆλyy	O
(	O
cid:6	O
)	O
nxˆλx	O
+	O
ny	O
ˆλy	O
(	O
xi	O
−	O
ˆμ	O
)	O
2	O
1	O
(	O
cid:6	O
)	O
nx	O
i	O
(	O
yi	O
−	O
ˆμ	O
)	O
2	O
1	O
ny	O
i	O
(	O
4.249	O
)	O
(	O
4.250	O
)	O
(	O
4.251	O
)	O
(	O
4.252	O
)	O
(	O
4.253	O
)	O
(	O
4.254	O
)	O
(	O
4.255	O
)	O
we	O
notice	O
that	O
the	O
mle	O
for	O
μ	O
has	O
the	O
same	O
form	O
as	O
the	O
posterior	B
mean	I
,	O
mn	O
.	O
(	O
cid:4	O
)	O
ny	O
we	O
can	O
solve	O
these	O
equations	O
by	O
ﬁxed	B
point	I
iteration	O
.	O
let	O
us	O
initialize	O
by	O
estimating	O
λx	O
=	O
1/s2	O
x	O
i=1	O
(	O
yi	O
−	O
y	O
)	O
2	O
=	O
0.36.	O
and	O
λy	O
=	O
1/s2	O
using	O
this	O
,	O
we	O
get	O
ˆμ	O
=	O
2.1154	O
,	O
so	O
p	O
(	O
μ|d	O
,	O
ˆλx	O
,	O
ˆλy	O
)	O
=n	O
(	O
μ|2.1154	O
,	O
0.0554	O
)	O
.	O
if	O
we	O
now	O
iterate	O
,	O
we	O
converge	B
to	O
ˆλx	O
=	O
1/0.1662	O
,	O
ˆλy	O
=	O
1/4.0509	O
,	O
p	O
(	O
μ|d	O
,	O
ˆλx	O
,	O
ˆλy	O
)	O
=	O
n	O
(	O
μ|1.5788	O
,	O
0.0798	O
)	O
.	O
(	O
cid:4	O
)	O
nx	O
i=1	O
(	O
xi	O
−	O
x	O
)	O
2	O
=	O
0.16	O
and	O
s2	O
y	O
,	O
where	O
s2	O
x	O
=	O
1	O
nx	O
y	O
=	O
1	O
ny	O
the	O
plug-in	B
approximation	I
to	O
the	O
posterior	O
is	O
plotted	O
in	O
figure	O
4.20	O
(	O
a	O
)	O
.	O
this	O
weights	O
each	O
sensor	O
according	O
to	O
its	O
estimated	O
precision	O
.	O
since	O
sensor	O
y	O
was	O
estimated	O
to	O
be	O
much	O
less	O
reliable	O
than	O
sensor	O
x	O
,	O
we	O
havee	O
≈	O
x	O
,	O
so	O
we	O
effectively	O
ignore	O
the	O
y	O
sensor	O
.	O
μ|d	O
,	O
ˆλx	O
,	O
ˆλy	O
)	O
*	O
now	O
we	O
will	O
adopt	O
a	O
bayesian	O
approach	O
and	O
integrate	B
out	I
the	O
unknown	B
precisions	O
,	O
rather	O
than	O
trying	O
to	O
estimate	O
them	O
.	O
that	O
is	O
,	O
we	O
compute	O
p	O
(	O
μ|d	O
)	O
∝	O
p	O
(	O
μ	O
)	O
p	O
(	O
dx|μ	O
,	O
λx	O
)	O
p	O
(	O
λx|μ	O
)	O
dλx	O
(	O
4.256	O
)	O
we	O
will	O
use	O
uninformative	B
jeffrey	O
’	O
s	O
priors	O
,	O
p	O
(	O
μ	O
)	O
∝	O
1	O
,	O
p	O
(	O
λx|μ	O
)	O
∝	O
1/λx	O
and	O
p	O
(	O
λy|μ	O
)	O
∝	O
1/λy	O
.	O
p	O
(	O
dy|μ	O
,	O
λy	O
)	O
p	O
(	O
λy|μ	O
)	O
dλy	O
(	O
cid:2	O
)	O
(	O
cid:11	O
)	O
(	O
cid:3	O
)	O
(	O
cid:2	O
)	O
(	O
cid:11	O
)	O
(	O
cid:3	O
)	O
140	O
chapter	O
4.	O
gaussian	O
models	O
since	O
the	O
x	O
and	O
y	O
terms	O
are	O
symmetric	B
,	O
we	O
will	O
just	O
focus	O
on	O
one	O
of	O
them	O
.	O
the	O
key	O
integral	O
is	O
i	O
=	O
p	O
(	O
dx|μ	O
,	O
λx	O
)	O
p	O
(	O
λx|μ	O
)	O
dλx	O
∝	O
(	O
cid:11	O
)	O
λ−1	O
x	O
(	O
nxλx	O
)	O
nx/2	O
(	O
cid:7	O
)	O
−	O
nx	O
2	O
λx	O
(	O
x	O
−	O
μ	O
)	O
2	O
−	O
nx	O
2	O
exp	O
(	O
cid:8	O
)	O
(	O
4.257	O
)	O
s2	O
xλx	O
dλx	O
(	O
4.258	O
)	O
exploiting	O
the	O
fact	O
that	O
nx	O
=	O
2	O
this	O
simpliﬁes	O
to	O
i	O
=	O
λ−1	O
x	O
λ1	O
x	O
exp	O
(	O
−λx	O
[	O
(	O
x	O
−	O
μ	O
)	O
2	O
+	O
s2	O
x	O
]	O
)	O
dλx	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
(	O
4.259	O
)	O
(	O
4.260	O
)	O
we	O
recognize	O
this	O
as	O
proportional	O
to	O
the	O
integral	O
of	O
an	O
unnormalized	O
gamma	O
density	O
ga	O
(	O
λ|a	O
,	O
b	O
)	O
∝	O
λa−1e−λb	O
x.	O
hence	O
the	O
integral	O
is	O
proportional	O
to	O
the	O
normalizing	O
where	O
a	O
=	O
1	O
and	O
b	O
=	O
(	O
x	O
−	O
μ	O
)	O
2	O
+	O
s2	O
constant	O
of	O
the	O
gamma	B
distribution	I
,	O
γ	O
(	O
a	O
)	O
b−a	O
,	O
so	O
we	O
get	O
x	O
−	O
μ	O
)	O
2	O
+	O
s2	O
p	O
(	O
dx|μ	O
,	O
λx	O
)	O
p	O
(	O
λx|μ	O
)	O
dλx	O
∝	O
(	O
cid:18	O
)	O
i	O
∝	O
(	O
cid:11	O
)	O
x	O
and	O
the	O
posterior	O
becomes	O
p	O
(	O
μ|d	O
)	O
∝	O
1	O
(	O
x	O
−	O
μ	O
)	O
2	O
+	O
s2	O
x	O
1	O
(	O
y	O
−	O
μ	O
)	O
2	O
+	O
s2	O
y	O
(	O
cid:19	O
)	O
−1	O
(	O
4.261	O
)	O
(	O
4.262	O
)	O
the	O
exact	O
posterior	O
is	O
plotted	O
in	O
figure	O
4.20	O
(	O
b	O
)	O
.	O
we	O
see	O
that	O
it	O
has	O
two	O
modes	O
,	O
one	O
near	O
x	O
=	O
1.5	O
and	O
one	O
near	O
y	O
=	O
3.5.	O
these	O
correspond	O
to	O
the	O
beliefs	O
that	O
the	O
x	O
sensor	O
is	O
more	O
reliable	O
than	O
the	O
y	O
one	O
,	O
and	O
vice	O
versa	O
.	O
the	O
weight	O
of	O
the	O
ﬁrst	O
mode	B
is	O
larger	O
,	O
since	O
the	O
data	O
from	O
the	O
x	O
sensor	O
agree	O
more	O
with	O
each	O
other	O
,	O
so	O
it	O
seems	O
slightly	O
more	O
likely	O
that	O
the	O
x	O
sensor	O
is	O
the	O
reliable	O
one	O
.	O
(	O
they	O
obviously	O
can	O
not	O
both	O
be	O
reliable	O
,	O
since	O
they	O
disagree	O
on	O
the	O
values	O
that	O
they	O
are	O
reporting	O
.	O
)	O
however	O
,	O
the	O
bayesian	O
solution	O
keeps	O
open	O
the	O
possibility	O
that	O
the	O
y	O
sensor	O
is	O
the	O
more	O
reliable	O
one	O
;	O
from	O
two	O
measurements	O
,	O
we	O
can	O
not	O
tell	O
,	O
and	O
choosing	O
just	O
the	O
x	O
sensor	O
,	O
as	O
the	O
plug-in	B
approximation	I
does	O
,	O
results	O
in	O
over	O
conﬁdence	O
(	O
a	O
posterior	O
that	O
is	O
too	O
narrow	O
)	O
.	O
exercises	O
exercise	O
4.1	O
uncorrelated	O
does	O
not	O
imply	O
independent	O
let	O
x	O
∼	O
u	O
(	O
−1	O
,	O
1	O
)	O
and	O
y	O
=	O
x	O
2.	O
clearly	O
y	O
is	O
dependent	O
on	O
x	O
(	O
in	O
fact	O
,	O
y	O
is	O
uniquely	O
determined	O
if	O
x	O
∼	O
u	O
(	O
a	O
,	O
b	O
)	O
then	O
e	O
[	O
x	O
]	O
=	O
(	O
a	O
+	O
b	O
)	O
/2	O
and	O
by	O
x	O
)	O
.	O
however	O
,	O
show	O
that	O
ρ	O
(	O
x	O
,	O
y	O
)	O
=	O
0.	O
hint	O
:	O
var	O
[	O
x	O
]	O
=	O
(	O
b	O
−	O
a	O
)	O
2/12	O
.	O
exercise	O
4.2	O
uncorrelated	O
and	O
gaussian	O
does	O
not	O
imply	O
independent	O
unless	O
jointly	O
gaussian	O
let	O
x	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
and	O
y	O
=	O
w	O
x	O
,	O
where	O
p	O
(	O
w	O
=	O
−1	O
)	O
=	O
p	O
(	O
w	O
=	O
1	O
)	O
=	O
0.5.	O
it	O
is	O
clear	O
that	O
x	O
and	O
y	O
are	O
not	O
independent	O
,	O
since	O
y	O
is	O
a	O
function	O
of	O
x.	O
a.	O
show	O
y	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
.	O
4.6.	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	O
141	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−2	O
−1	O
0	O
1	O
2	O
(	O
a	O
)	O
3	O
4	O
5	O
6	O
1.5	O
1	O
0.5	O
0	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
(	O
b	O
)	O
figure	O
4.20	O
posterior	O
for	O
μ.	O
sensorfusionunknownprec	O
.	O
(	O
a	O
)	O
plug-in	B
approximation	I
.	O
(	O
b	O
)	O
exact	O
posterior	O
.	O
figure	O
generated	O
by	O
b.	O
show	O
cov	O
[	O
x	O
,	O
y	O
]	O
=	O
0.	O
thus	O
x	O
and	O
y	O
are	O
uncorrelated	O
but	O
dependent	O
,	O
even	O
though	O
they	O
are	O
gaussian	O
.	O
hint	O
:	O
use	O
the	O
deﬁnition	O
of	O
covariance	B
cov	O
[	O
x	O
,	O
y	O
]	O
=	O
e	O
[	O
xy	O
]	O
−	O
e	O
[	O
x	O
]	O
e	O
[	O
y	O
]	O
and	O
the	O
rule	B
of	I
iterated	I
expectation	I
e	O
[	O
xy	O
]	O
=	O
e	O
[	O
e	O
[	O
xy	O
|w	O
]	O
]	O
exercise	O
4.3	O
correlation	B
coefficient	I
is	O
between	O
-1	O
and	O
+1	O
prove	O
that	O
−1	O
≤	O
ρ	O
(	O
x	O
,	O
y	O
)	O
≤	O
1	O
exercise	O
4.4	O
correlation	B
coefficient	I
for	O
linearly	O
related	O
variables	O
is	O
±1	O
show	O
that	O
,	O
if	O
y	O
=	O
ax	O
+	O
b	O
for	O
some	O
parameters	O
a	O
>	O
0	O
and	O
b	O
,	O
then	O
ρ	O
(	O
x	O
,	O
y	O
)	O
=	O
1.	O
similarly	O
show	O
that	O
if	O
a	O
<	O
0	O
,	O
then	O
ρ	O
(	O
x	O
,	O
y	O
)	O
=	O
−1	O
.	O
exercise	O
4.5	O
normalization	O
constant	O
for	O
a	O
multidimensional	O
gaussian	O
prove	O
that	O
the	O
normalization	O
constant	O
for	O
a	O
d-dimensional	O
gaussian	O
is	O
given	O
by	O
(	O
cid:5	O
)	O
(	O
2π	O
)	O
d/2|σ|	O
1	O
2	O
=	O
exp	O
(	O
−	O
1	O
2	O
(	O
x	O
−	O
μ	O
)	O
t	O
σ−1	O
(	O
x	O
−	O
μ	O
)	O
)	O
dx	O
(	O
cid:14	O
)	O
hint	O
:	O
diagonalize	O
σ	O
and	O
use	O
the	O
fact	O
that	O
|σ|	O
=	O
i	O
λi	O
to	O
write	O
the	O
joint	O
pdf	O
as	O
a	O
product	O
of	O
d	O
one-	O
dimensional	O
gaussians	O
in	O
a	O
transformed	O
coordinate	O
system	O
.	O
(	O
you	O
will	O
need	O
the	O
change	B
of	I
variables	I
formula	O
.	O
)	O
finally	O
,	O
use	O
the	O
normalization	O
constant	O
for	O
univariate	O
gaussians	O
.	O
exercise	O
4.6	O
bivariate	O
gaussian	O
(	O
cid:4	O
)	O
let	O
x	O
∼	O
n	O
(	O
μ	O
,	O
σ	O
)	O
where	O
x	O
∈	O
r	O
(	O
cid:3	O
)	O
σ	O
=	O
σ2	O
1	O
ρσ1σ2	O
ρσ1σ2	O
σ2	O
2	O
2	O
and	O
where	O
ρ	O
is	O
the	O
correlation	B
coefficient	I
.	O
show	O
that	O
the	O
pdf	B
is	O
given	O
by	O
(	O
cid:15	O
)	O
1	O
p	O
(	O
x1	O
,	O
x2	O
)	O
=	O
(	O
cid:3	O
)	O
2πσ1σ2	O
−	O
exp	O
1	O
−	O
ρ2	O
1	O
2	O
(	O
1	O
−	O
ρ2	O
)	O
(	O
cid:3	O
)	O
(	O
x1	O
−	O
μ1	O
)	O
2	O
σ2	O
1	O
+	O
(	O
x2	O
−	O
μ2	O
)	O
2	O
σ2	O
2	O
−	O
2ρ	O
(	O
x1	O
−	O
μ1	O
)	O
σ1	O
(	O
x2	O
−	O
μ2	O
)	O
σ2	O
(	O
4.263	O
)	O
(	O
4.264	O
)	O
(	O
4.265	O
)	O
(	O
4.266	O
)	O
(	O
4.267	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
4.268	O
)	O
142	O
chapter	O
4.	O
gaussian	O
models	O
raw	O
22	O
35	O
57	O
34	O
44	O
25	O
19	O
65	O
13	O
20	O
11	O
2	O
21	O
36	O
70	O
67	O
40	O
52	O
51	O
17	O
48	O
45	O
71	O
23	O
31	O
16	O
53	O
66	O
43	O
24	O
42	O
61	O
49	O
27	O
9	O
26	O
73	O
68	O
4	O
37	O
55	O
72	O
33	O
64	O
8	O
54	O
60	O
39	O
10	O
6	O
3	O
58	O
32	O
5	O
7	O
15	O
18	O
47	O
69	O
50	O
63	O
62	O
14	O
29	O
59	O
1	O
38	O
12	O
46	O
56	O
28	O
30	O
41	O
280	O
260	O
240	O
220	O
200	O
180	O
160	O
140	O
120	O
100	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−3	O
standarized	O
22	O
35	O
34	O
44	O
57	O
13	O
20	O
11	O
67	O
40	O
52	O
51	O
17	O
48	O
45	O
71	O
21	O
2	O
36	O
70	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−3	O
whitened	O
35	O
22	O
34	O
44	O
25	O
19	O
57	O
13	O
20	O
65	O
11	O
51	O
67	O
71	O
40	O
9	O
49	O
52	O
17	O
48	O
27	O
15	O
68	O
4	O
37	O
55	O
45	O
72	O
33	O
18	O
47	O
69	O
64	O
50	O
6263	O
8	O
54	O
60	O
39	O
10	O
42	O
61	O
6	O
3	O
58	O
32	O
5	O
7	O
59	O
38	O
14	O
29	O
1	O
16	O
31	O
23	O
66	O
53	O
26	O
73	O
43	O
24	O
56	O
28	O
12	O
46	O
30	O
41	O
2	O
21	O
36	O
70	O
−2	O
−1	O
0	O
1	O
2	O
3	O
25	O
19	O
65	O
49	O
27	O
15	O
8	O
54	O
60	O
18	O
47	O
69	O
50	O
63	O
62	O
5	O
38	O
31	O
66	O
43	O
24	O
42	O
61	O
23	O
26	O
73	O
33	O
39	O
58	O
7	O
9	O
68	O
4	O
37	O
55	O
72	O
64	O
10	O
6	O
3	O
32	O
59	O
16	O
53	O
12	O
46	O
56	O
28	O
30	O
41	O
1	O
14	O
29	O
80	O
60	O
65	O
70	O
75	O
80	O
−2	O
−1	O
0	O
1	O
2	O
3	O
figure	O
4.21	O
(	O
a	O
)	O
height/weight	O
data	O
for	O
the	O
men	O
.	O
(	O
b	O
)	O
standardized	B
.	O
(	O
c	O
)	O
whitened	O
.	O
exercise	O
4.7	O
conditioning	B
a	O
bivariate	O
gaussian	O
consider	O
a	O
bivariate	O
gaussian	O
distribution	O
p	O
(	O
x1	O
,	O
x2	O
)	O
=n	O
(	O
x|μ	O
,	O
σ	O
)	O
where	O
(	O
cid:3	O
)	O
(	O
cid:4	O
)	O
σ	O
=	O
σ2	O
1	O
σ21	O
σ12	O
σ2	O
2	O
=	O
σ1σ2	O
(	O
cid:3	O
)	O
σ1	O
σ2	O
ρ	O
(	O
cid:4	O
)	O
ρ	O
σ2	O
σ1	O
where	O
the	O
correlation	B
coefficient	I
is	O
given	O
by	O
(	O
4.269	O
)	O
(	O
4.270	O
)	O
ρ	O
(	O
cid:2	O
)	O
σ12	O
σ1σ2	O
a.	O
what	O
is	O
p	O
(	O
x2|x1	O
)	O
?	O
simplify	O
your	O
answer	O
by	O
expressing	O
it	O
in	O
terms	O
of	O
ρ	O
,	O
σ2	O
,	O
σ1	O
,	O
μ1	O
,	O
μ2	O
and	O
x1	O
.	O
b.	O
assume	O
σ1	O
=	O
σ2	O
=	O
1.	O
what	O
is	O
p	O
(	O
x2|x1	O
)	O
now	O
?	O
exercise	O
4.8	O
whitening	B
vs	O
standardizing	O
a.	O
load	O
the	O
height/weight	O
data	O
using	O
rawdata	O
=	O
dlmread	O
(	O
’	O
heightweightdata.txt	O
’	O
)	O
.	O
the	O
ﬁrst	O
col-	O
umn	O
is	O
the	O
class	O
label	O
(	O
1=male	O
,	O
2=female	O
)	O
,	O
the	O
second	O
column	O
is	O
height	O
,	O
the	O
third	O
weight	O
.	O
extract	O
the	O
height/weight	O
data	O
corresponding	O
to	O
the	O
males	O
.	O
fit	O
a	O
2d	O
gaussian	O
to	O
the	O
male	O
data	O
,	O
using	O
the	O
empirical	O
mean	O
and	O
covariance	B
.	O
plot	O
your	O
gaussian	O
as	O
an	O
ellipse	O
(	O
use	O
gaussplot2d	O
)	O
,	O
superimposing	O
on	O
your	O
scatter	B
plot	I
.	O
it	O
should	O
look	O
like	O
figure	O
4.21	O
(	O
a	O
)	O
,	O
where	O
have	O
labeled	O
each	O
datapoint	O
by	O
its	O
index	O
.	O
turn	O
in	O
your	O
ﬁgure	O
and	O
code	O
.	O
b.	O
standardizing	O
the	O
data	O
means	O
ensuring	O
the	O
empirical	O
variance	O
along	O
each	O
dimension	O
is	O
1.	O
this	O
can	O
be	O
,	O
where	O
σj	O
is	O
the	O
empirical	O
std	O
of	O
dimension	O
j.	O
standardize	O
the	O
data	O
and	O
done	O
by	O
computing	O
replot	O
.	O
it	O
should	O
look	O
like	O
figure	O
4.21	O
(	O
b	O
)	O
.	O
(	O
use	O
axis	O
(	O
’	O
equal	O
’	O
)	O
.	O
)	O
turn	O
in	O
your	O
ﬁgure	O
and	O
code	O
.	O
xij−xj	O
σj	O
c.	O
whitening	B
or	O
sphereing	B
the	O
data	O
means	O
ensuring	O
its	O
empirical	O
covariance	O
matrix	O
is	O
proportional	O
to	O
i	O
,	O
so	O
the	O
data	O
is	O
uncorrelated	O
and	O
of	O
equal	O
variance	O
along	O
each	O
dimension	O
.	O
this	O
can	O
be	O
done	O
by	O
computing	O
λ−	O
1	O
2	O
ut	O
x	O
for	O
each	O
data	O
vector	O
x	O
,	O
where	O
u	O
are	O
the	O
eigenvectors	O
and	O
λ	O
the	O
eigenvalues	O
of	O
x.	O
whiten	O
the	O
data	O
and	O
replot	O
.	O
it	O
should	O
look	O
like	O
figure	O
4.21	O
(	O
c	O
)	O
.	O
note	O
that	O
whitening	B
rotates	O
the	O
data	O
,	O
so	O
people	O
move	O
to	O
counter-intuitive	O
locations	O
in	O
the	O
new	O
coordinate	O
system	O
(	O
see	O
e.g.	O
,	O
person	O
2	O
,	O
who	O
moves	O
from	O
the	O
right	O
hand	O
side	O
to	O
the	O
left	O
)	O
.	O
exercise	O
4.9	O
sensor	B
fusion	I
with	O
known	O
variances	O
in	O
1d	O
suppose	O
we	O
have	O
two	O
sensors	O
with	O
known	O
(	O
and	O
different	O
)	O
variances	O
v1	O
and	O
v2	O
,	O
but	O
unknown	B
(	O
and	O
the	O
same	O
)	O
i	O
∼	O
n	O
(	O
μ	O
,	O
v1	O
)	O
from	O
the	O
ﬁrst	O
sensor	O
and	O
n2	O
observations	O
mean	B
μ.	O
suppose	O
we	O
observe	O
n1	O
observations	O
y	O
(	O
1	O
)	O
4.6.	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	O
143	O
i	O
∼	O
n	O
(	O
μ	O
,	O
v2	O
)	O
from	O
the	O
second	O
sensor	O
.	O
y	O
(	O
2	O
)	O
(	O
for	O
example	O
,	O
suppose	O
μ	O
is	O
the	O
true	O
temperature	O
outside	O
,	O
and	O
sensor	O
1	O
is	O
a	O
precise	O
(	O
low	O
variance	O
)	O
digital	O
thermosensing	O
device	O
,	O
and	O
sensor	O
2	O
is	O
an	O
imprecise	O
(	O
high	O
variance	O
)	O
mercury	O
thermometer	O
.	O
)	O
let	O
d	O
represent	O
all	O
the	O
data	O
from	O
both	O
sensors	O
.	O
what	O
is	O
the	O
posterior	O
p	O
(	O
μ|d	O
)	O
,	O
assuming	O
a	O
non-informative	B
prior	O
for	O
μ	O
(	O
which	O
we	O
can	O
simulate	O
using	O
a	O
gaussian	O
with	O
a	O
precision	B
of	O
0	O
)	O
?	O
give	O
an	O
explicit	O
expression	O
for	O
the	O
posterior	B
mean	I
and	O
variance	B
.	O
exercise	O
4.10	O
derivation	O
of	O
information	B
form	I
formulae	O
for	O
marginalizing	O
and	O
conditioning	B
derive	O
the	O
information	B
form	I
results	O
of	O
section	O
4.3.1.	O
exercise	O
4.11	O
derivation	O
of	O
the	O
niw	O
posterior	O
derive	O
equation	O
4.209.	O
hint	O
:	O
one	O
can	O
show	O
that	O
n	O
(	O
x	O
−	O
μ	O
)	O
(	O
x	O
−	O
μ	O
)	O
t	O
+	O
κ0	O
(	O
μ	O
−	O
m0	O
)	O
(	O
μ	O
−	O
m0	O
)	O
t	O
=	O
κn	O
(	O
μ	O
−	O
mn	O
)	O
(	O
μ	O
−	O
mn	O
)	O
t	O
+	O
κ0n	O
κn	O
(	O
x	O
−	O
m0	O
)	O
(	O
x	O
−	O
m0	O
)	O
t	O
this	O
is	O
a	O
matrix	O
generalization	O
of	O
an	O
operation	O
called	O
completing	O
the	O
square.5	O
derive	O
the	O
corresponding	O
result	O
for	O
the	O
normal-wishart	O
model	O
.	O
(	O
4.271	O
)	O
(	O
4.272	O
)	O
exercise	O
4.12	O
bic	O
for	O
gaussians	O
(	O
source	O
:	O
jaakkola	O
.	O
)	O
the	O
bayesian	O
information	B
criterion	O
(	O
bic	O
)	O
is	O
a	O
penalized	O
log-likelihood	O
function	O
that	O
can	O
be	O
used	O
for	O
model	B
selection	I
(	O
see	O
section	O
5.3.2.4	O
)	O
.	O
it	O
is	O
deﬁned	O
as	O
bic	O
=	O
log	O
p	O
(	O
d|ˆθm	O
l	O
)	O
−	O
d	O
2	O
log	O
(	O
n	O
)	O
(	O
4.273	O
)	O
where	O
d	O
is	O
the	O
number	O
of	O
free	O
parameters	O
in	O
the	O
model	O
and	O
n	O
is	O
the	O
number	O
of	O
samples	B
.	O
in	O
this	O
question	O
,	O
we	O
will	O
see	O
how	O
to	O
use	O
this	O
to	O
choose	O
between	O
a	O
full	B
covariance	O
gaussian	O
and	O
a	O
gaussian	O
with	O
a	O
diagonal	O
covariance	O
.	O
obviously	O
a	O
full	B
covariance	O
gaussian	O
has	O
higher	O
likelihood	B
,	O
but	O
it	O
may	O
not	O
be	O
“	O
worth	O
”	O
the	O
extra	O
parameters	O
if	O
the	O
improvement	O
over	O
a	O
diagonal	O
covariance	O
matrix	O
is	O
too	O
small	O
.	O
so	O
we	O
use	O
the	O
bic	O
score	O
to	O
choose	O
the	O
model	O
.	O
following	O
section	O
4.1.3	O
,	O
we	O
can	O
write	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
log	O
p	O
(	O
d|	O
ˆσ	O
,	O
ˆμ	O
)	O
=−	O
n	O
n	O
(	O
cid:12	O
)	O
2	O
tr	O
ˆs	O
=	O
1	O
n	O
i=1	O
−1	O
ˆs	O
ˆσ	O
−	O
n	O
2	O
log	O
(	O
ˆ|σ|	O
)	O
(	O
xi	O
−	O
x	O
)	O
(	O
xi	O
−	O
x	O
)	O
t	O
(	O
4.274	O
)	O
(	O
4.275	O
)	O
where	O
ˆs	O
is	O
the	O
scatter	O
matrix	O
(	O
empirical	O
covariance	O
)	O
,	O
the	O
trace	B
of	O
a	O
matrix	O
is	O
the	O
sum	O
of	O
its	O
diagonals	O
,	O
and	O
we	O
have	O
used	O
the	O
trace	B
trick	I
.	O
a.	O
derive	O
the	O
bic	O
score	O
for	O
a	O
gaussian	O
in	O
d	O
dimensions	O
with	O
full	B
covariance	O
matrix	O
.	O
simplify	O
your	O
answer	O
as	O
much	O
as	O
possible	O
,	O
exploiting	O
the	O
form	O
of	O
the	O
mle	O
.	O
be	O
sure	O
to	O
specify	O
the	O
number	O
of	O
free	O
parameters	O
d.	O
b.	O
derive	O
the	O
bic	O
score	O
for	O
a	O
gaussian	O
in	O
d	O
dimensions	O
with	O
a	O
diagonal	O
covariance	O
matrix	O
.	O
be	O
sure	O
to	O
specify	O
the	O
number	O
of	O
free	O
parameters	O
d.	O
hint	O
:	O
for	O
the	O
digaonal	O
case	O
,	O
the	O
ml	O
estimate	O
of	O
σ	O
is	O
the	O
same	O
as	O
ˆσm	O
l	O
except	O
the	O
off-diagonal	O
terms	O
are	O
zero	O
:	O
ˆσdiag	O
=	O
diag	O
(	O
ˆσm	O
l	O
(	O
1	O
,	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
ˆσm	O
l	O
(	O
d	O
,	O
d	O
)	O
)	O
(	O
4.276	O
)	O
5.	O
in	O
the	O
scalar	O
case	O
,	O
completing	B
the	I
square	I
means	O
rewriting	O
c2x2	O
+	O
c1x	O
+	O
c0	O
as	O
−a	O
(	O
x	O
−	O
b	O
)	O
2	O
+	O
w	O
where	O
a	O
=	O
−c2	O
,	O
b	O
=	O
and	O
w	O
=	O
c2	O
1	O
4c2	O
+	O
c0	O
.	O
c1	O
2c2	O
144	O
chapter	O
4.	O
gaussian	O
models	O
exercise	O
4.13	O
gaussian	O
posterior	O
credible	O
interval	O
(	O
source	O
:	O
degroot	O
.	O
)	O
let	O
x	O
∼	O
n	O
(	O
μ	O
,	O
σ2	O
=	O
4	O
)	O
where	O
μ	O
is	O
unknown	B
but	O
has	O
prior	O
μ	O
∼	O
n	O
(	O
μ0	O
,	O
σ2	O
seeing	O
n	O
samples	B
is	O
μ	O
∼	O
n	O
(	O
μn	O
,	O
σ2	O
n	O
)	O
.	O
conﬁdence	B
interval	I
.	O
)	O
how	O
big	O
does	O
n	O
have	O
to	O
be	O
to	O
ensure	O
0	O
=	O
9	O
)	O
.	O
the	O
posterior	O
after	O
(	O
this	O
is	O
called	O
a	O
credible	B
interval	I
,	O
and	O
is	O
the	O
bayesian	O
analog	O
of	O
a	O
p	O
(	O
(	O
cid:7	O
)	O
≤	O
μn	O
≤	O
u|d	O
)	O
≥	O
0.95	O
(	O
4.277	O
)	O
where	O
(	O
(	O
cid:7	O
)	O
,	O
u	O
)	O
is	O
an	O
interval	O
(	O
centered	O
on	O
μn	O
)	O
of	O
width	O
1	O
and	O
d	O
is	O
the	O
data	O
.	O
hint	O
:	O
recall	B
that	O
95	O
%	O
of	O
the	O
probability	O
mass	O
of	O
a	O
gaussian	O
is	O
within	O
±1.96σ	O
of	O
the	O
mean	B
.	O
exercise	O
4.14	O
map	O
estimation	O
for	O
1d	O
gaussians	O
(	O
source	O
:	O
jaakkola	O
.	O
)	O
consider	O
samples	B
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
from	O
a	O
gaussian	O
random	O
variable	O
with	O
known	O
variance	B
σ2	O
and	O
unknown	B
mean	O
μ.	O
we	O
further	O
assume	O
a	O
prior	O
distribution	O
(	O
also	O
gaussian	O
)	O
over	O
the	O
mean	B
,	O
μ	O
∼	O
n	O
(	O
m	O
,	O
s2	O
)	O
,	O
with	O
ﬁxed	O
mean	O
m	O
and	O
ﬁxed	O
variance	O
s2	O
.	O
thus	O
the	O
only	O
unknown	B
is	O
μ.	O
a.	O
calculate	O
the	O
map	O
estimate	O
ˆμm	O
ap	O
.	O
you	O
can	O
state	B
the	O
result	O
without	O
proof	O
.	O
alternatively	O
,	O
with	O
a	O
lot	O
more	O
work	O
,	O
you	O
can	O
compute	O
derivatives	O
of	O
the	O
log	O
posterior	O
,	O
set	O
to	O
zero	O
and	O
solve	O
.	O
b.	O
show	O
that	O
as	O
the	O
number	O
of	O
samples	B
n	O
increase	O
,	O
the	O
map	O
estimate	O
converges	O
to	O
the	O
maximum	B
likelihood	I
estimate	I
.	O
c.	O
suppose	O
n	O
is	O
small	O
and	O
ﬁxed	O
.	O
what	O
does	O
the	O
map	O
estimator	B
converge	O
to	O
if	O
we	O
increase	O
the	O
prior	O
variance	B
s2	O
?	O
d.	O
suppose	O
n	O
is	O
small	O
and	O
ﬁxed	O
.	O
what	O
does	O
the	O
map	O
estimator	B
converge	O
to	O
if	O
we	O
decrease	O
the	O
prior	O
variance	B
s2	O
?	O
exercise	O
4.15	O
sequential	B
(	O
recursive	B
)	O
updating	O
of	O
ˆσ	O
(	O
source	O
:	O
(	O
duda	O
et	O
al	O
.	O
2001	O
,	O
q3.35,3.36	O
)	O
.	O
)	O
the	O
unbiased	B
estimates	O
for	O
the	O
covariance	B
of	O
a	O
d-dimensional	O
gaussian	O
based	O
on	O
n	O
samples	B
is	O
given	O
by	O
ˆσ	O
=	O
cn	O
=	O
1	O
n	O
−	O
1	O
(	O
xi	O
−	O
mn	O
)	O
(	O
xi	O
−	O
mn	O
)	O
t	O
(	O
4.278	O
)	O
n	O
(	O
cid:12	O
)	O
i=1	O
it	O
is	O
clear	O
that	O
it	O
takes	O
o	O
(	O
nd2	O
)	O
time	O
to	O
compute	O
cn	O
.	O
efficient	O
to	O
incrementally	O
update	O
these	O
estimates	O
than	O
to	O
recompute	O
from	O
scratch	O
.	O
if	O
the	O
data	O
points	O
arrive	O
one	O
at	O
a	O
time	O
,	O
it	O
is	O
more	O
a.	O
show	O
that	O
the	O
covariance	B
can	O
be	O
sequentially	O
udpated	O
as	O
follows	O
(	O
xn+1	O
−	O
mn	O
)	O
(	O
xn+1	O
−	O
mn	O
)	O
t	O
cn+1	O
=	O
cn	O
+	O
n	O
−	O
1	O
n	O
1	O
n	O
+	O
1	O
b.	O
how	O
much	O
time	O
does	O
it	O
take	O
per	O
sequential	B
update	O
?	O
(	O
use	O
big-o	O
notation	O
.	O
)	O
c.	O
show	O
that	O
we	O
can	O
sequentially	O
update	O
the	O
precision	B
matrix	I
using	O
c−1	O
n+1	O
=	O
n	O
n	O
−	O
1	O
n	O
−	O
c−1	O
c−1	O
n	O
(	O
xn+1	O
−	O
mn	O
)	O
(	O
xn+1	O
−	O
mn	O
)	O
t	O
c−1	O
n	O
+	O
(	O
xn+1	O
−	O
mn	O
)	O
t	O
c−1	O
n	O
(	O
xn+1	O
−	O
mn	O
)	O
n2−1	O
n	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
(	O
4.279	O
)	O
(	O
4.280	O
)	O
hint	O
:	O
notice	O
that	O
the	O
update	O
to	O
cn+1	O
consists	O
of	O
adding	O
a	O
rank-one	O
matrix	O
,	O
namely	O
uut	O
,	O
where	O
u	O
=	O
xn+1	O
−	O
mn	O
.	O
use	O
the	O
matrix	B
inversion	I
lemma	I
for	O
rank-one	O
updates	O
(	O
equation	O
4.111	O
)	O
,	O
which	O
we	O
repeat	O
here	O
for	O
convenience	O
:	O
(	O
e	O
+	O
uvt	O
)	O
−1	O
=	O
e−1	O
−	O
e−1uvt	O
e−1	O
1	O
+v	O
t	O
e−1u	O
(	O
4.281	O
)	O
4.6.	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	O
145	O
d.	O
what	O
is	O
the	O
time	O
complexity	O
per	O
update	O
?	O
exercise	O
4.16	O
likelihood	B
ratio	I
for	O
gaussians	O
source	O
:	O
source	O
:	O
alpaydin	O
p103	O
ex	O
4.	O
consider	O
a	O
binary	O
classiﬁer	O
where	O
the	O
k	O
class	O
conditional	O
densities	O
are	O
mvn	O
p	O
(	O
x|y	O
=	O
j	O
)	O
=	O
n	O
(	O
x|μj	O
,	O
σj	O
)	O
.	O
by	O
bayes	O
rule	O
,	O
we	O
have	O
p	O
(	O
y	O
=	O
1|x	O
)	O
p	O
(	O
y	O
=	O
0|x	O
)	O
log	O
=	O
log	O
p	O
(	O
x|y	O
=	O
1	O
)	O
p	O
(	O
x|y	O
=	O
0	O
)	O
+	O
log	O
p	O
(	O
y	O
=	O
1	O
)	O
p	O
(	O
y	O
=	O
0	O
)	O
(	O
4.282	O
)	O
in	O
other	O
words	O
,	O
the	O
log	O
posterior	O
ratio	O
is	O
the	O
log	O
likelihood	O
ratio	O
plus	O
the	O
log	O
prior	O
ratio	O
.	O
for	O
each	O
of	O
the	O
4	O
cases	O
in	O
the	O
table	O
below	O
,	O
derive	O
an	O
expression	O
for	O
the	O
log	O
likelihood	O
ratio	O
log	O
p	O
(	O
x|y=1	O
)	O
p	O
(	O
x|y=0	O
)	O
,	O
simplifying	O
as	O
much	O
as	O
possible	O
.	O
form	O
of	O
σj	O
arbitrary	O
shared	B
shared	O
,	O
axis-aligned	O
σj	O
=	O
σ	O
with	O
σij	O
=	O
0	O
for	O
i	O
(	O
cid:8	O
)	O
=	O
j	O
shared	B
,	O
spherical	B
cov	O
σj	O
σj	O
=	O
σ	O
σj	O
=	O
σ2i	O
num	O
parameters	O
kd	O
(	O
d	O
+	O
1	O
)	O
/2	O
d	O
(	O
d	O
+	O
1	O
)	O
/2	O
d	O
1	O
exercise	O
4.17	O
lda/qda	O
on	O
height/weight	O
data	O
the	O
function	O
discrimanalysisheightweightdemo	O
ﬁts	O
an	O
lda	O
and	O
qda	O
model	O
to	O
the	O
height/weight	O
data	O
.	O
compute	O
the	O
misclassiﬁcation	B
rate	I
of	O
both	O
of	O
these	O
models	O
on	O
the	O
training	B
set	I
.	O
turn	O
in	O
your	O
numbers	O
and	O
code	O
.	O
exercise	O
4.18	O
naive	O
bayes	O
with	O
mixed	O
features	O
consider	O
a	O
3	O
class	O
naive	O
bayes	O
classiﬁer	O
with	O
one	O
binary	O
feature	O
and	O
one	O
gaussian	O
feature	O
:	O
y	O
∼	O
mu	O
(	O
y|π	O
,	O
1	O
)	O
,	O
x1|y	O
=	O
c	O
∼	O
ber	O
(	O
x1|θc	O
)	O
,	O
x2|y	O
=	O
c	O
∼	O
n	O
(	O
x2|μc	O
,	O
σ2	O
c	O
)	O
let	O
the	O
parameter	B
vectors	O
be	O
as	O
follows	O
:	O
π	O
=	O
(	O
0.5	O
,	O
0.25	O
,	O
0.25	O
)	O
,	O
θ	O
=	O
(	O
0.5	O
,	O
0.5	O
,	O
0.5	O
)	O
,	O
μ	O
=	O
(	O
−1	O
,	O
0	O
,	O
1	O
)	O
,	O
σ2	O
=	O
(	O
1	O
,	O
1	O
,	O
1	O
)	O
(	O
4.283	O
)	O
(	O
4.284	O
)	O
a.	O
compute	O
p	O
(	O
y|x1	O
=	O
0	O
,	O
x2	O
=	O
0	O
)	O
(	O
the	O
result	O
should	O
be	O
a	O
vector	O
of	O
3	O
numbers	O
that	O
sums	O
to	O
1	O
)	O
.	O
b.	O
compute	O
p	O
(	O
y|x1	O
=	O
0	O
)	O
.	O
c.	O
compute	O
p	O
(	O
y|x2	O
=	O
0	O
)	O
.	O
d.	O
explain	O
any	O
interesting	O
patterns	O
you	O
see	O
in	O
your	O
results	O
.	O
hint	O
:	O
look	O
at	O
the	O
parameter	B
vector	O
θ.	O
exercise	O
4.19	O
decision	B
boundary	I
for	O
lda	O
with	O
semi	O
tied	B
covariances	O
consider	O
a	O
generative	B
classiﬁer	I
with	O
class	O
conditional	O
densities	O
of	O
the	O
form	O
n	O
(	O
x|μc	O
,	O
σc	O
)	O
.	O
in	O
lda	O
,	O
we	O
assume	O
σc	O
=	O
σ	O
,	O
and	O
in	O
qda	O
,	O
each	O
σc	O
is	O
arbitrary	O
.	O
here	O
we	O
consider	O
the	O
2	O
class	O
case	O
in	O
which	O
σ1	O
=	O
kς0	O
,	O
for	O
k	O
>	O
1.	O
that	O
is	O
,	O
the	O
gaussian	O
ellipsoids	O
have	O
the	O
same	O
“	O
shape	O
”	O
,	O
but	O
the	O
one	O
for	O
class	O
1	O
is	O
“	O
wider	O
”	O
.	O
derive	O
an	O
expression	O
for	O
p	O
(	O
y	O
=	O
1|x	O
,	O
θ	O
)	O
,	O
simplifying	O
as	O
much	O
as	O
possible	O
.	O
give	O
a	O
geometric	O
interpretation	O
of	O
your	O
result	O
,	O
if	O
possible	O
.	O
exercise	O
4.20	O
logistic	B
regression	I
vs	O
lda/qda	O
(	O
source	O
:	O
jaakkola	O
.	O
)	O
suppose	O
we	O
train	O
the	O
following	O
binary	O
classiﬁers	O
via	O
maximum	O
likelihood	O
.	O
matrices	O
set	O
to	O
i	O
(	O
identity	O
matrix	O
)	O
,	O
i.e.	O
,	O
p	O
(	O
x|y	O
=	O
c	O
)	O
=	O
n	O
(	O
x|μc	O
,	O
i	O
)	O
.	O
we	O
assume	O
p	O
(	O
y	O
)	O
is	O
uniform	O
.	O
a.	O
gaussi	O
:	O
a	O
generative	B
classiﬁer	I
,	O
where	O
the	O
class	O
conditional	O
densities	O
are	O
gaussian	O
,	O
with	O
both	O
covariance	B
b.	O
gaussx	O
:	O
as	O
for	O
gaussi	O
,	O
but	O
the	O
covariance	B
matrices	O
are	O
unconstrained	O
,	O
i.e.	O
,	O
p	O
(	O
x|y	O
=	O
c	O
)	O
=	O
n	O
(	O
x|μc	O
,	O
σc	O
)	O
.	O
146	O
chapter	O
4.	O
gaussian	O
models	O
c.	O
linlog	O
:	O
a	O
logistic	B
regression	I
model	O
with	O
linear	O
features	O
.	O
d.	O
quadlog	O
:	O
a	O
logistic	B
regression	I
model	O
,	O
using	O
linear	O
and	O
quadratic	O
features	O
(	O
i.e.	O
,	O
polynomial	O
basis	O
function	O
expansion	O
of	O
degree	B
2	O
)	O
.	O
after	O
training	O
we	O
compute	O
the	O
performance	O
of	O
each	O
model	O
m	O
on	O
the	O
training	B
set	I
as	O
follows	O
:	O
log	O
p	O
(	O
yi|xi	O
,	O
ˆθ	O
,	O
m	O
)	O
(	O
4.285	O
)	O
n	O
(	O
cid:12	O
)	O
i=1	O
l	O
(	O
m	O
)	O
=	O
1	O
n	O
(	O
note	O
that	O
this	O
is	O
the	O
conditional	O
log-likelihood	O
p	O
(	O
y|x	O
,	O
ˆθ	O
)	O
and	O
not	O
the	O
joint	O
log-likelihood	O
p	O
(	O
y	O
,	O
x|ˆθ	O
)	O
.	O
)	O
we	O
now	O
want	O
to	O
compare	O
the	O
performance	O
of	O
each	O
model	O
.	O
we	O
will	O
write	O
l	O
(	O
m	O
)	O
≤	O
l	O
(	O
m	O
(	O
cid:2	O
)	O
)	O
if	O
model	O
m	O
must	O
have	O
lower	O
(	O
or	O
equal	O
)	O
log	O
likelihood	O
(	O
on	O
the	O
training	B
set	I
)	O
than	O
m	O
(	O
cid:2	O
)	O
,	O
for	O
any	O
training	B
set	I
(	O
in	O
other	O
words	O
,	O
m	O
is	O
worse	O
than	O
m	O
(	O
cid:2	O
)	O
,	O
at	O
least	O
as	O
far	O
as	O
training	B
set	I
logprob	O
is	O
concerned	O
)	O
.	O
for	O
each	O
of	O
the	O
following	O
model	O
pairs	O
,	O
state	B
whether	O
l	O
(	O
m	O
)	O
≤	O
l	O
(	O
m	O
(	O
cid:2	O
)	O
)	O
,	O
or	O
whether	O
no	O
such	O
statement	O
can	O
be	O
made	O
(	O
i.e.	O
,	O
m	O
might	O
sometimes	O
be	O
better	O
than	O
m	O
(	O
cid:2	O
)	O
and	O
sometimes	O
worse	O
)	O
;	O
also	O
,	O
for	O
each	O
question	O
,	O
brieﬂy	O
(	O
1-2	O
sentences	O
)	O
explain	O
why	O
.	O
)	O
,	O
l	O
(	O
m	O
)	O
≥	O
l	O
(	O
m	O
(	O
cid:2	O
)	O
a.	O
gaussi	O
,	O
linlog	O
.	O
b.	O
gaussx	O
,	O
quadlog	O
.	O
c.	O
linlog	O
,	O
quadlog	O
.	O
d.	O
gaussi	O
,	O
quadlog	O
.	O
e.	O
now	O
suppose	O
we	O
measure	O
performance	O
in	O
terms	O
of	O
the	O
average	O
misclassiﬁcation	O
rate	B
on	O
the	O
training	B
set	I
:	O
r	O
(	O
m	O
)	O
=	O
1	O
n	O
n	O
(	O
cid:12	O
)	O
i=1	O
i	O
(	O
yi	O
(	O
cid:8	O
)	O
=	O
ˆy	O
(	O
xi	O
)	O
)	O
(	O
4.286	O
)	O
is	O
it	O
true	O
in	O
general	O
that	O
l	O
(	O
m	O
)	O
>	O
l	O
(	O
m	O
(	O
cid:2	O
)	O
)	O
implies	O
that	O
r	O
(	O
m	O
)	O
<	O
r	O
(	O
m	O
(	O
cid:2	O
)	O
)	O
?	O
explain	O
why	O
or	O
why	O
not	O
.	O
exercise	O
4.21	O
gaussian	O
decision	B
boundaries	O
(	O
source	O
:	O
1	O
,	O
μ2	O
=	O
1	O
,	O
σ2	O
(	O
duda	O
et	O
al	O
.	O
2001	O
,	O
q3.7	O
)	O
.	O
)	O
let	O
p	O
(	O
x|y	O
=	O
j	O
)	O
=n	O
(	O
x|μj	O
,	O
σj	O
)	O
where	O
j	O
=	O
1	O
,	O
2	O
and	O
μ1	O
=	O
0	O
,	O
σ2	O
1	O
=	O
2	O
=	O
106.	O
let	O
the	O
class	O
priors	O
be	O
equal	O
,	O
p	O
(	O
y	O
=	O
1	O
)	O
=p	O
(	O
y	O
=	O
2	O
)	O
=	O
0.5.	O
a.	O
find	O
the	O
decision	B
region	O
r1	O
=	O
{	O
x	O
:	O
p	O
(	O
x|μ1	O
,	O
σ1	O
)	O
≥	O
p	O
(	O
x|μ2	O
,	O
σ2	O
)	O
}	O
(	O
4.287	O
)	O
sketch	O
the	O
result	O
.	O
hint	O
:	O
draw	O
the	O
curves	O
and	O
ﬁnd	O
where	O
they	O
intersect	O
.	O
find	O
both	O
solutions	O
of	O
the	O
equation	O
p	O
(	O
x|μ1	O
,	O
σ1	O
)	O
=	O
p	O
(	O
x|μ2	O
,	O
σ2	O
)	O
hint	O
:	O
recall	B
that	O
to	O
solve	O
a	O
quadratic	O
equation	O
ax2	O
+	O
bx	O
+	O
c	O
=	O
0	O
,	O
we	O
use	O
−b	O
±	O
√	O
b2	O
−	O
4ac	O
2a	O
x	O
=	O
b.	O
now	O
suppose	O
σ2	O
=	O
1	O
(	O
and	O
all	O
other	O
parameters	O
remain	O
the	O
same	O
)	O
.	O
what	O
is	O
r1	O
in	O
this	O
case	O
?	O
(	O
4.288	O
)	O
(	O
4.289	O
)	O
4.6.	O
inferring	O
the	O
parameters	O
of	O
an	O
mvn	O
exercise	O
4.22	O
qda	O
with	O
3	O
classes	O
consider	O
a	O
three	O
category	O
classiﬁcation	B
problem	O
.	O
let	O
the	O
prior	O
probabilites	O
:	O
p	O
(	O
y	O
=	O
1	O
)	O
=p	O
(	O
y	O
=	O
2	O
)	O
=p	O
(	O
y	O
=	O
3	O
)	O
=	O
1/3	O
the	O
class-conditional	O
densities	O
are	O
multivariate	B
normal	I
densities	O
with	O
parameters	O
:	O
μ1	O
=	O
[	O
0	O
,	O
0	O
]	O
t	O
,	O
μ2	O
=	O
[	O
1	O
,	O
1	O
]	O
t	O
,	O
μ3	O
=	O
[	O
−1	O
,	O
1	O
]	O
t	O
(	O
cid:20	O
)	O
σ1	O
=	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
,	O
σ2	O
=	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
,	O
σ3	O
=	O
(	O
cid:21	O
)	O
0.8	O
0.2	O
0.2	O
0.8	O
0.8	O
0.2	O
0.2	O
0.8	O
0.7	O
0	O
0	O
0.7	O
147	O
(	O
4.290	O
)	O
(	O
4.291	O
)	O
(	O
4.292	O
)	O
classify	O
the	O
following	O
points	O
:	O
a.	O
x	O
=	O
[	O
−0.5	O
,	O
0.5	O
]	O
b.	O
x	O
=	O
[	O
0.5	O
,	O
0.5	O
]	O
exercise	O
4.23	O
scalar	O
qda	O
[	O
note	O
:	O
you	O
can	O
solve	O
this	O
exercise	O
by	O
hand	O
or	O
using	O
a	O
computer	O
(	O
matlab	O
,	O
r	O
,	O
whatever	O
)	O
.	O
in	O
either	O
case	O
,	O
show	O
your	O
work	O
.	O
]	O
consider	O
the	O
following	O
training	B
set	I
of	O
heights	O
x	O
(	O
in	O
inches	O
)	O
and	O
gender	O
y	O
(	O
male/female	O
)	O
of	O
some	O
us	O
college	O
students	O
:	O
x	O
=	O
(	O
67	O
,	O
79	O
,	O
71	O
,	O
68	O
,	O
67	O
,	O
60	O
)	O
,	O
y	O
=	O
(	O
m	O
,	O
m	O
,	O
m	O
,	O
f	O
,	O
f	O
,	O
f	O
)	O
.	O
a.	O
fit	O
a	O
bayes	O
classiﬁer	O
to	O
this	O
data	O
,	O
using	O
maximum	O
likelihood	O
estimation	O
,	O
i.e.	O
,	O
estimate	O
the	O
parameters	O
of	O
(	O
4.293	O
)	O
the	O
class	O
conditional	O
likelihoods	O
p	O
(	O
x|y	O
=	O
c	O
)	O
=	O
n	O
(	O
x	O
;	O
μc	O
,	O
σc	O
)	O
and	O
the	O
class	O
prior	O
p	O
(	O
y	O
=	O
c	O
)	O
=	O
πc	O
(	O
4.294	O
)	O
what	O
are	O
your	O
values	O
of	O
μc	O
,	O
σc	O
,	O
πc	O
for	O
c	O
=	O
m	O
,	O
f	O
?	O
show	O
your	O
work	O
(	O
so	O
you	O
can	O
get	O
partial	O
credit	O
if	O
you	O
make	O
an	O
arithmetic	O
error	O
)	O
.	O
b.	O
compute	O
p	O
(	O
y	O
=	O
m|x	O
,	O
ˆθ	O
)	O
,	O
where	O
x	O
=	O
72	O
,	O
and	O
ˆθ	O
are	O
the	O
mle	O
parameters	O
.	O
(	O
this	O
is	O
called	O
a	O
plug-in	B
prediction	O
.	O
)	O
c.	O
what	O
would	O
be	O
a	O
simple	O
way	O
to	O
extend	O
this	O
technique	O
if	O
you	O
had	O
multiple	O
attributes	O
per	O
person	O
,	O
such	O
as	O
height	O
and	O
weight	O
?	O
write	O
down	O
your	O
proposed	O
model	O
as	O
an	O
equation	O
.	O
5	O
bayesian	O
statistics	O
5.1	O
introduction	O
we	O
have	O
now	O
seen	O
a	O
variety	O
of	O
different	O
probability	O
models	O
,	O
and	O
we	O
have	O
discussed	O
how	O
to	O
i.e.	O
,	O
we	O
have	O
discussed	O
how	O
to	O
compute	O
map	O
parameter	B
estimates	O
ˆθ	O
=	O
ﬁt	O
them	O
to	O
data	O
,	O
argmax	O
p	O
(	O
θ|d	O
)	O
,	O
using	O
a	O
variety	O
of	O
different	O
priors	O
.	O
we	O
have	O
also	O
discussed	O
how	O
to	O
compute	O
the	O
full	B
posterior	O
p	O
(	O
θ|d	O
)	O
,	O
as	O
well	O
as	O
the	O
posterior	B
predictive	I
density	I
,	O
p	O
(	O
x|d	O
)	O
,	O
for	O
certain	O
special	O
cases	O
(	O
and	O
in	O
later	O
chapters	O
,	O
we	O
will	O
discuss	O
algorithms	O
for	O
the	O
general	O
case	O
)	O
.	O
using	O
the	O
posterior	O
distribution	O
to	O
summarize	O
everything	O
we	O
know	O
about	O
a	O
set	O
of	O
unknown	B
variables	O
is	O
at	O
the	O
core	O
of	O
bayesian	O
statistics	O
.	O
in	O
this	O
chapter	O
,	O
we	O
discuss	O
this	O
approach	O
to	O
statistics	O
in	O
more	O
detail	O
.	O
in	O
chapter	O
6	O
,	O
we	O
discuss	O
an	O
alternative	O
approach	O
to	O
statistics	O
known	O
as	O
frequentist	B
or	O
classical	B
statistics	I
.	O
5.2	O
summarizing	O
posterior	O
distributions	O
the	O
posterior	O
p	O
(	O
θ|d	O
)	O
summarizes	O
everything	O
we	O
know	O
about	O
the	O
unknown	B
quantities	O
θ.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
some	O
simple	O
quantities	O
that	O
can	O
be	O
derived	O
from	O
a	O
probability	O
distribution	O
,	O
such	O
as	O
a	O
posterior	O
.	O
these	O
summary	O
statistics	O
are	O
often	O
easier	O
to	O
understand	O
and	O
visualize	O
than	O
the	O
full	B
joint	O
.	O
5.2.1	O
map	O
estimation	O
we	O
can	O
easily	O
compute	O
a	O
point	B
estimate	I
of	O
an	O
unknown	B
quantity	O
by	O
computing	O
the	O
posterior	B
mean	I
,	O
median	B
or	O
mode	B
.	O
in	O
section	O
5.7	O
,	O
we	O
discuss	O
how	O
to	O
use	O
decision	B
theory	O
to	O
choose	O
between	O
these	O
methods	O
.	O
typically	O
the	O
posterior	B
mean	I
or	O
median	B
is	O
the	O
most	O
appropriate	O
choice	O
for	O
a	O
real-	O
valued	O
quantity	O
,	O
and	O
the	O
vector	O
of	O
posterior	O
marginals	O
is	O
the	O
best	O
choice	O
for	O
a	O
discrete	B
quantity	O
.	O
however	O
,	O
the	O
posterior	B
mode	I
,	O
aka	O
the	O
map	O
estimate	O
,	O
is	O
the	O
most	O
popular	O
choice	O
because	O
it	O
reduces	O
to	O
an	O
optimization	B
problem	O
,	O
for	O
which	O
efficient	O
algorithms	O
often	O
exist	O
.	O
futhermore	O
,	O
map	O
estimation	O
can	O
be	O
interpreted	O
in	O
non-bayesian	O
terms	O
,	O
by	O
thinking	O
of	O
the	O
log	O
prior	O
as	O
a	O
regularizer	O
(	O
see	O
section	O
6.5	O
for	O
more	O
details	O
)	O
.	O
although	O
this	O
approach	O
is	O
computationally	O
appealing	O
,	O
it	O
is	O
important	O
to	O
point	O
out	O
that	O
there	O
are	O
various	O
drawbacks	O
to	O
map	O
estimation	O
,	O
which	O
we	O
brieﬂy	O
discuss	O
below	O
.	O
this	O
will	O
provide	O
motivation	O
for	O
the	O
more	O
thoroughly	O
bayesian	O
approach	O
which	O
we	O
will	O
study	O
later	O
in	O
this	O
chapter	O
(	O
and	O
elsewhere	O
in	O
this	O
book	O
)	O
.	O
150	O
chapter	O
5.	O
bayesian	O
statistics	O
4.5	O
4	O
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−2	O
−1	O
0	O
1	O
(	O
a	O
)	O
2	O
3	O
4	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
(	O
b	O
)	O
figure	O
5.1	O
(	O
a	O
)	O
a	O
bimodal	O
distribution	O
in	O
which	O
the	O
mode	B
is	O
very	O
untypical	O
of	O
the	O
distribution	O
.	O
the	O
thin	O
blue	O
vertical	O
line	O
is	O
the	O
mean	B
,	O
which	O
is	O
arguably	O
a	O
better	O
summary	O
of	O
the	O
distribution	O
,	O
since	O
it	O
is	O
near	O
the	O
majority	O
of	O
the	O
probability	O
mass	O
.	O
figure	O
generated	O
by	O
bimodaldemo	O
.	O
(	O
b	O
)	O
a	O
skewed	O
distribution	O
in	O
which	O
the	O
mode	B
is	O
quite	O
different	O
from	O
the	O
mean	B
.	O
figure	O
generated	O
by	O
gammaplotdemo	O
.	O
5.2.1.1	O
no	O
measure	O
of	O
uncertainty	B
the	O
most	O
obvious	O
drawback	O
of	O
map	O
estimation	O
,	O
and	O
indeed	O
of	O
any	O
other	O
point	B
estimate	I
such	O
as	O
the	O
posterior	B
mean	I
or	O
median	B
,	O
is	O
that	O
it	O
does	O
not	O
provide	O
any	O
measure	O
of	O
uncertainty	B
.	O
in	O
many	O
applications	O
,	O
it	O
is	O
important	O
to	O
know	O
how	O
much	O
one	O
can	O
trust	O
a	O
given	O
estimate	O
.	O
we	O
can	O
derive	O
such	O
conﬁdence	O
measures	O
from	O
the	O
posterior	O
,	O
as	O
we	O
discuss	O
in	O
section	O
5.2.2	O
.	O
5.2.1.2	O
plugging	O
in	O
the	O
map	O
estimate	O
can	O
result	O
in	O
overﬁtting	B
in	O
machine	B
learning	I
,	O
we	O
often	O
care	O
more	O
about	O
predictive	B
accuracy	O
than	O
in	O
interpreting	O
the	O
parameters	O
of	O
our	O
models	O
.	O
however	O
,	O
if	O
we	O
don	O
’	O
t	O
model	O
the	O
uncertainty	B
in	O
our	O
parameters	O
,	O
then	O
our	O
predictive	B
distribution	O
will	O
be	O
overconﬁdent	O
.	O
we	O
saw	O
several	O
examples	O
of	O
this	O
in	O
chapter	O
3	O
,	O
and	O
we	O
will	O
see	O
more	O
examples	O
later	O
.	O
overconﬁdence	O
in	O
predictions	O
is	O
particularly	O
problematic	O
in	O
situations	O
where	O
we	O
may	O
be	O
risk	B
averse	I
;	O
see	O
section	O
5.7	O
for	O
details	O
.	O
5.2.1.3	O
the	O
mode	B
is	O
an	O
untypical	O
point	O
choosing	O
the	O
mode	B
as	O
a	O
summary	O
of	O
a	O
posterior	O
distribution	O
is	O
often	O
a	O
very	O
poor	O
choice	O
,	O
since	O
the	O
mode	B
is	O
usually	O
quite	O
untypical	O
of	O
the	O
distribution	O
,	O
unlike	O
the	O
mean	B
or	O
median	B
.	O
this	O
is	O
illustrated	O
in	O
figure	O
5.1	O
(	O
a	O
)	O
for	O
a	O
1d	O
continuous	O
space	O
.	O
the	O
basic	O
problem	O
is	O
that	O
the	O
mode	B
is	O
a	O
point	O
of	O
measure	O
zero	O
,	O
whereas	O
the	O
mean	B
and	O
median	B
take	O
the	O
volume	O
of	O
the	O
space	O
into	O
account	O
.	O
another	O
example	O
is	O
shown	O
in	O
figure	O
5.1	O
(	O
b	O
)	O
:	O
here	O
the	O
mode	B
is	O
0	O
,	O
but	O
the	O
mean	B
is	O
non-zero	O
.	O
such	O
skewed	O
distributions	O
often	O
arise	O
when	O
inferring	O
variance	B
parameters	O
,	O
especially	O
in	O
hierarchical	O
models	O
.	O
in	O
such	O
cases	O
the	O
map	O
estimate	O
(	O
and	O
hence	O
the	O
mle	O
)	O
is	O
obviously	O
a	O
very	O
bad	O
estimate	O
.	O
how	O
should	O
we	O
summarize	O
a	O
posterior	O
if	O
the	O
mode	B
is	O
not	O
a	O
good	O
choice	O
?	O
the	O
answer	O
is	O
to	O
use	O
decision	B
theory	O
,	O
which	O
we	O
discuss	O
in	O
section	O
5.7.	O
the	O
basic	O
idea	O
is	O
to	O
specify	O
a	O
loss	B
function	I
,	O
where	O
l	O
(	O
θ	O
,	O
ˆθ	O
)	O
is	O
the	O
loss	B
you	O
incur	O
if	O
the	O
truth	O
is	O
θ	O
and	O
your	O
estimate	O
is	O
ˆθ	O
.	O
if	O
we	O
use	O
0-1	O
loss	B
,	O
l	O
(	O
θ	O
,	O
ˆθ	O
)	O
=	O
i	O
(	O
θ	O
(	O
cid:6	O
)	O
=	O
ˆθ	O
)	O
,	O
then	O
the	O
optimal	O
estimate	O
is	O
the	O
posterior	B
mode	I
.	O
0-1	O
loss	B
means	O
you	O
only	O
get	O
“	O
points	O
”	O
if	O
you	O
make	O
no	O
errors	O
,	O
otherwise	O
you	O
get	O
nothing	O
:	O
there	O
is	O
no	O
“	O
partial	O
credit	O
”	O
under	O
5.2.	O
summarizing	O
posterior	O
distributions	O
151	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
py	O
g	O
px	O
2	O
4	O
6	O
8	O
10	O
12	O
figure	O
5.2	O
example	O
of	O
the	O
transformation	O
of	O
a	O
density	O
under	O
a	O
nonlinear	O
transform	O
.	O
note	O
how	O
the	O
mode	B
of	O
the	O
transformed	O
distribution	O
is	O
not	O
the	O
transform	O
of	O
the	O
original	O
mode	B
.	O
based	O
on	O
exercise	O
1.4	O
of	O
(	O
bishop	O
2006b	O
)	O
.	O
figure	O
generated	O
by	O
bayeschangeofvar	O
.	O
this	O
loss	B
function	I
!	O
for	O
continuous-valued	O
quantities	O
,	O
we	O
often	O
prefer	O
to	O
use	O
squared	B
error	I
loss	O
,	O
l	O
(	O
θ	O
,	O
ˆθ	O
)	O
=	O
(	O
θ−	O
ˆθ	O
)	O
2	O
;	O
the	O
corresponding	O
optimal	O
estimator	O
is	O
then	O
the	O
posterior	B
mean	I
,	O
as	O
we	O
show	O
in	O
section	O
5.7.	O
or	O
we	O
can	O
use	O
a	O
more	O
robust	B
loss	O
function	O
,	O
l	O
(	O
θ	O
,	O
ˆθ	O
)	O
=	O
|θ	O
−	O
ˆθ|	O
,	O
which	O
gives	O
rise	O
to	O
the	O
posterior	B
median	I
.	O
5.2.1.4	O
map	O
estimation	O
is	O
not	O
invariant	O
to	O
reparameterization	O
*	O
a	O
more	O
subtle	O
problem	O
with	O
map	O
estimation	O
is	O
that	O
the	O
result	O
we	O
get	O
depends	O
on	O
how	O
we	O
pa-	O
rameterize	O
the	O
probability	O
distribution	O
.	O
changing	O
from	O
one	O
representation	O
to	O
another	O
equivalent	O
representation	O
changes	O
the	O
result	O
,	O
which	O
is	O
not	O
very	O
desirable	O
,	O
since	O
the	O
units	O
of	O
measurement	O
are	O
arbitrary	O
(	O
e.g.	O
,	O
when	O
measuring	O
distance	O
,	O
we	O
can	O
use	O
centimetres	O
or	O
inches	O
)	O
.	O
to	O
understand	O
the	O
problem	O
,	O
suppose	O
we	O
compute	O
the	O
posterior	O
for	O
x.	O
if	O
we	O
deﬁne	O
y	O
=	O
f	O
(	O
x	O
)	O
,	O
the	O
distribution	O
for	O
y	O
is	O
given	O
by	O
equation	O
2.87	O
,	O
which	O
we	O
repeat	O
here	O
for	O
convenience	O
:	O
--	O
dx	O
--	O
dy	O
py	O
(	O
y	O
)	O
=	O
px	O
(	O
x	O
)	O
(	O
5.1	O
)	O
the	O
|	O
dx	O
dy|	O
term	O
is	O
called	O
the	O
jacobian	O
,	O
and	O
it	O
measures	O
the	O
change	O
in	O
size	O
of	O
a	O
unit	O
volume	O
passed	O
through	O
f.	O
let	O
ˆx	O
=	O
argmaxx	O
px	O
(	O
x	O
)	O
be	O
the	O
map	O
estimate	O
for	O
x.	O
in	O
general	O
it	O
is	O
not	O
the	O
case	O
that	O
ˆy	O
=	O
argmaxy	O
py	O
(	O
y	O
)	O
is	O
given	O
by	O
f	O
(	O
ˆx	O
)	O
.	O
for	O
example	O
,	O
let	O
x	O
∼	O
n	O
(	O
6	O
,	O
1	O
)	O
and	O
y	O
=	O
f	O
(	O
x	O
)	O
,	O
where	O
f	O
(	O
x	O
)	O
=	O
1	O
1	O
+	O
exp	O
(	O
−x	O
+	O
5	O
)	O
(	O
5.2	O
)	O
we	O
can	O
derive	O
the	O
distribution	O
of	O
y	O
using	O
monte	O
carlo	O
simulation	O
(	O
see	O
section	O
2.7.1	O
)	O
.	O
the	O
result	O
is	O
shown	O
in	O
figure	O
5.2.	O
we	O
see	O
that	O
the	O
original	O
gaussian	O
has	O
become	O
“	O
squashed	O
”	O
by	O
the	O
sigmoid	B
nonlinearity	O
.	O
in	O
particular	O
,	O
we	O
see	O
that	O
the	O
mode	B
of	O
the	O
transformed	O
distribution	O
is	O
not	O
equal	O
to	O
the	O
transform	O
of	O
the	O
original	O
mode	B
.	O
152	O
chapter	O
5.	O
bayesian	O
statistics	O
to	O
see	O
how	O
this	O
problem	O
arises	O
in	O
the	O
context	O
of	O
map	O
estimation	O
,	O
consider	O
the	O
following	O
example	O
,	O
due	O
to	O
michael	O
jordan	O
.	O
the	O
bernoulli	O
distribution	O
is	O
typically	O
parameterized	O
by	O
its	O
mean	B
μ	O
,	O
so	O
p	O
(	O
y	O
=	O
1|μ	O
)	O
=μ	O
,	O
where	O
y	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
suppose	O
we	O
have	O
a	O
uniform	O
prior	O
on	O
the	O
unit	O
interval	O
:	O
pμ	O
(	O
μ	O
)	O
=	O
1i	O
(	O
0	O
≤	O
μ	O
≤	O
1	O
)	O
.	O
if	O
there	O
is	O
no	O
data	O
,	O
the	O
map	O
estimate	O
is	O
just	O
the	O
mode	B
of	O
the	O
prior	O
,	O
which	O
can	O
be	O
anywhere	O
between	O
0	O
and	O
1.	O
we	O
will	O
now	O
show	O
that	O
different	O
parameterizations	O
can	O
pick	O
different	O
points	O
in	O
this	O
interval	O
arbitrarily	O
.	O
first	O
let	O
θ	O
=	O
√	O
μ	O
so	O
μ	O
=	O
θ2	O
.	O
the	O
new	O
prior	O
is	O
(	O
5.3	O
)	O
(	O
5.4	O
)	O
(	O
5.5	O
)	O
(	O
5.6	O
)	O
--	O
dμ	O
dθ	O
--	O
=	O
2θ	O
pθ	O
(	O
θ	O
)	O
=	O
pμ	O
(	O
μ	O
)	O
for	O
θ	O
∈	O
[	O
0	O
,	O
1	O
]	O
so	O
the	O
new	O
mode	B
is	O
ˆθm	O
ap	O
=	O
arg	O
max	O
θ∈	O
[	O
0,1	O
]	O
2θ	O
=	O
1	O
now	O
let	O
φ	O
=	O
1	O
−	O
√	O
1	O
−	O
μ.	O
the	O
new	O
prior	O
is	O
--	O
=	O
2	O
(	O
1	O
−	O
φ	O
)	O
--	O
dμ	O
for	O
φ	O
∈	O
[	O
0	O
,	O
1	O
]	O
,	O
so	O
the	O
new	O
mode	B
is	O
2	O
−	O
2φ	O
=	O
0	O
pφ	O
(	O
φ	O
)	O
=	O
pμ	O
(	O
μ	O
)	O
ˆφm	O
ap	O
=	O
arg	O
max	O
φ∈	O
[	O
0,1	O
]	O
dφ	O
thus	O
the	O
map	O
estimate	O
depends	O
on	O
the	O
parameterization	O
.	O
the	O
mle	O
does	O
not	O
suffer	O
from	O
this	O
since	O
the	O
likelihood	B
is	O
a	O
function	O
,	O
not	O
a	O
probability	O
density	O
.	O
bayesian	O
inference	B
does	O
not	O
suffer	O
from	O
this	O
problem	O
either	O
,	O
since	O
the	O
change	O
of	O
measure	O
is	O
taken	O
into	O
account	O
when	O
integrating	O
over	O
the	O
parameter	B
space	O
.	O
one	O
solution	O
to	O
the	O
problem	O
is	O
to	O
optimize	O
the	O
following	O
objective	O
function	O
:	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
|i	O
(	O
θ	O
)	O
|−	O
1	O
2	O
ˆθ	O
=	O
argmax	O
θ	O
(	O
5.7	O
)	O
here	O
i	O
(	O
θ	O
)	O
is	O
the	O
fisher	O
information	B
matrix	O
associated	O
with	O
p	O
(	O
x|θ	O
)	O
(	O
see	O
section	O
6.2.2	O
)	O
.	O
this	O
estimate	O
is	O
parameterization	O
independent	O
,	O
for	O
reasons	O
explained	O
in	O
(	O
jermyn	O
2005	O
;	O
druilhet	O
and	O
marin	O
2007	O
)	O
.	O
unfortunately	O
,	O
optimizing	O
equation	O
5.7	O
is	O
often	O
difficult	O
,	O
which	O
minimizes	O
the	O
appeal	O
of	O
the	O
whole	O
approach	O
.	O
5.2.2	O
credible	O
intervals	O
in	O
addition	O
to	O
point	O
estimates	O
,	O
we	O
often	O
want	O
a	O
measure	O
of	O
conﬁdence	O
.	O
a	O
standard	O
measure	O
of	O
conﬁdence	O
in	O
some	O
(	O
scalar	O
)	O
quantity	O
θ	O
is	O
the	O
“	O
width	O
”	O
of	O
its	O
posterior	O
distribution	O
.	O
this	O
can	O
be	O
measured	O
using	O
a	O
100	O
(	O
1	O
−	O
α	O
)	O
%	O
credible	B
interval	I
,	O
which	O
is	O
a	O
(	O
contiguous	O
)	O
region	O
c	O
=	O
(	O
(	O
cid:9	O
)	O
,	O
u	O
)	O
(	O
standing	O
for	O
lower	O
and	O
upper	O
)	O
which	O
contains	O
1	O
−	O
α	O
of	O
the	O
posterior	O
probability	O
mass	O
,	O
i.e.	O
,	O
cα	O
(	O
d	O
)	O
=	O
(	O
(	O
cid:9	O
)	O
,	O
u	O
)	O
:	O
p	O
(	O
(	O
cid:9	O
)	O
≤	O
θ	O
≤	O
u|d	O
)	O
=	O
1	O
−	O
α	O
(	O
5.8	O
)	O
there	O
may	O
be	O
many	O
such	O
intervals	O
,	O
so	O
we	O
choose	O
one	O
such	O
that	O
there	O
is	O
(	O
1−	O
α	O
)	O
/2	O
mass	O
in	O
each	O
tail	O
;	O
this	O
is	O
called	O
a	O
central	B
interval	I
.	O
5.2.	O
summarizing	O
posterior	O
distributions	O
153	O
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
(	O
a	O
)	O
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
(	O
b	O
)	O
figure	O
5.3	O
(	O
a	O
)	O
central	B
interval	I
and	O
(	O
b	O
)	O
hpd	O
region	O
for	O
a	O
beta	O
(	O
3,9	O
)	O
posterior	O
.	O
the	O
ci	O
is	O
(	O
0.06	O
,	O
0.52	O
)	O
and	O
the	O
hpd	O
is	O
(	O
0.04	O
,	O
0.48	O
)	O
.	O
based	O
on	O
figure	O
3.6	O
of	O
(	O
hoff	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
betahpd	O
.	O
if	O
the	O
posterior	O
has	O
a	O
known	O
functional	O
form	O
,	O
we	O
can	O
compute	O
the	O
posterior	O
central	O
interval	O
using	O
(	O
cid:9	O
)	O
=	O
f	O
−1	O
(	O
α/2	O
)	O
and	O
u	O
=	O
f	O
−1	O
(	O
1−α/2	O
)	O
,	O
where	O
f	O
is	O
the	O
cdf	B
of	O
the	O
posterior	O
.	O
for	O
example	O
,	O
if	O
the	O
posterior	O
is	O
gaussian	O
,	O
p	O
(	O
θ|d	O
)	O
=	O
n	O
(	O
0	O
,	O
1	O
)	O
,	O
and	O
α	O
=	O
0.05	O
,	O
then	O
we	O
have	O
(	O
cid:9	O
)	O
=	O
φ	O
(	O
α/2	O
)	O
=	O
−1.96	O
,	O
and	O
u	O
=	O
φ	O
(	O
1	O
−	O
α/2	O
)	O
=	O
1.96	O
,	O
where	O
φ	O
denotes	O
the	O
cdf	B
of	O
the	O
gaussian	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
2.3	O
(	O
c	O
)	O
.	O
this	O
justiﬁes	O
the	O
common	O
practice	O
of	O
quoting	O
a	O
credible	B
interval	I
in	O
the	O
form	O
of	O
μ	O
±	O
2σ	O
,	O
where	O
μ	O
represents	O
the	O
posterior	B
mean	I
,	O
σ	O
represents	O
the	O
posterior	O
standard	O
deviation	O
,	O
and	O
2	O
is	O
a	O
good	O
approximation	O
to	O
1.96.	O
of	O
course	O
,	O
the	O
posterior	O
is	O
not	O
always	O
gaussian	O
.	O
for	O
example	O
,	O
in	O
our	O
coin	O
example	O
,	O
if	O
we	O
use	O
a	O
uniform	O
prior	O
and	O
we	O
observe	O
n1	O
=	O
47	O
heads	O
out	O
of	O
n	O
=	O
100	O
trials	O
,	O
then	O
the	O
posterior	O
is	O
a	O
beta	B
distribution	I
,	O
p	O
(	O
θ|d	O
)	O
=	O
beta	O
(	O
48	O
,	O
54	O
)	O
.	O
we	O
ﬁnd	O
the	O
95	O
%	O
posterior	O
credible	O
interval	O
is	O
(	O
0.3749	O
,	O
0.5673	O
)	O
(	O
see	O
betacredibleint	O
for	O
the	O
one	O
line	O
of	O
matlab	O
code	O
we	O
used	O
to	O
compute	O
this	O
)	O
.	O
if	O
we	O
don	O
’	O
t	O
know	O
the	O
functional	O
form	O
,	O
but	O
we	O
can	O
draw	O
samples	B
from	O
the	O
posterior	O
,	O
then	O
we	O
can	O
use	O
a	O
monte	O
carlo	O
approximation	O
to	O
the	O
posterior	O
quantiles	O
:	O
we	O
simply	O
sort	O
the	O
s	O
samples	B
,	O
and	O
ﬁnd	O
the	O
one	O
that	O
occurs	O
at	O
location	O
α/s	O
along	O
the	O
sorted	O
list	O
.	O
as	O
s	O
→	O
∞	O
,	O
this	O
converges	O
to	O
the	O
true	O
quantile	O
.	O
see	O
mcquantiledemo	O
for	O
a	O
demo	O
.	O
people	O
often	O
confuse	O
bayesian	O
credible	O
intervals	O
with	O
frequentist	B
conﬁdence	O
intervals	O
.	O
how-	O
ever	O
,	O
they	O
are	O
not	O
the	O
same	O
thing	O
,	O
as	O
we	O
discuss	O
in	O
section	O
6.6.1.	O
in	O
general	O
,	O
credible	O
intervals	O
are	O
usually	O
what	O
people	O
want	O
to	O
compute	O
,	O
but	O
conﬁdence	B
intervals	I
are	O
usually	O
what	O
they	O
actually	O
compute	O
,	O
because	O
most	O
people	O
are	O
taught	O
frequentist	B
statistics	I
but	O
not	O
bayesian	O
statistics	O
.	O
fortu-	O
nately	O
,	O
the	O
mechanics	O
of	O
computing	O
a	O
credible	B
interval	I
is	O
just	O
as	O
easy	O
as	O
computing	O
a	O
conﬁdence	B
interval	I
(	O
see	O
e.g.	O
,	O
betacredibleint	O
for	O
how	O
to	O
do	O
it	O
in	O
matlab	O
)	O
.	O
5.2.2.1	O
highest	B
posterior	I
density	I
regions	O
*	O
a	O
problem	O
with	O
central	O
intervals	O
is	O
that	O
there	O
might	O
be	O
points	O
outside	O
the	O
ci	O
which	O
have	O
higher	O
probability	O
density	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
5.3	O
(	O
a	O
)	O
,	O
where	O
we	O
see	O
that	O
points	O
outside	O
the	O
left-most	O
ci	O
boundary	O
have	O
higher	O
density	O
than	O
those	O
just	O
inside	O
the	O
right-most	O
ci	O
boundary	O
.	O
this	O
motivates	O
an	O
alternative	O
quantity	O
known	O
as	O
the	O
highest	B
posterior	I
density	I
or	O
hpd	O
region	O
.	O
this	O
is	O
deﬁned	O
as	O
the	O
(	O
set	O
of	O
)	O
most	O
probable	O
points	O
that	O
in	O
total	O
constitute	O
100	O
(	O
1	O
−	O
α	O
)	O
%	O
of	O
the	O
154	O
chapter	O
5.	O
bayesian	O
statistics	O
α/2	O
α/2	O
pmin	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
5.4	O
figure	O
2.2	O
of	O
(	O
gelman	O
et	O
al	O
.	O
2004	O
)	O
.	O
figure	O
generated	O
by	O
postdensityintervals	O
.	O
(	O
a	O
)	O
central	B
interval	I
and	O
(	O
b	O
)	O
hpd	O
region	O
for	O
a	O
hypothetical	O
multimodal	O
posterior	O
.	O
based	O
on	O
probability	O
mass	O
.	O
more	O
formally	O
,	O
we	O
ﬁnd	O
the	O
threshold	O
p∗	O
(	O
cid:11	O
)	O
1	O
−	O
α	O
=	O
p	O
(	O
θ|d	O
)	O
dθ	O
θ	O
:	O
p	O
(	O
θ|d	O
)	O
>	O
p∗	O
and	O
then	O
deﬁne	O
the	O
hpd	O
as	O
cα	O
(	O
d	O
)	O
=	O
{	O
θ	O
:	O
p	O
(	O
θ|d	O
)	O
≥	O
p∗	O
}	O
on	O
the	O
pdf	B
such	O
that	O
(	O
5.9	O
)	O
(	O
5.10	O
)	O
in	O
1d	O
,	O
the	O
hpd	O
region	O
is	O
sometimes	O
called	O
a	O
highest	B
density	I
interval	I
or	O
hdi	O
.	O
for	O
example	O
,	O
figure	O
5.3	O
(	O
b	O
)	O
shows	O
the	O
95	O
%	O
hdi	O
of	O
a	O
beta	O
(	O
3	O
,	O
9	O
)	O
distribution	O
,	O
which	O
is	O
(	O
0.04	O
,	O
0.48	O
)	O
.	O
we	O
see	O
that	O
this	O
is	O
narrower	O
than	O
the	O
ci	O
,	O
even	O
though	O
it	O
still	O
contains	O
95	O
%	O
of	O
the	O
mass	O
;	O
furthermore	O
,	O
every	O
point	O
inside	O
of	O
it	O
has	O
higher	O
density	O
than	O
every	O
point	O
outside	O
of	O
it	O
.	O
for	O
a	O
unimodal	O
distribution	O
,	O
the	O
hdi	O
will	O
be	O
the	O
narrowest	O
interval	O
around	O
the	O
mode	B
contain-	O
ing	O
95	O
%	O
of	O
the	O
mass	O
.	O
to	O
see	O
this	O
,	O
imagine	O
“	O
water	O
ﬁlling	O
”	O
in	O
reverse	O
,	O
where	O
we	O
lower	O
the	O
level	O
until	O
95	O
%	O
of	O
the	O
mass	O
is	O
revealed	O
,	O
and	O
only	O
5	O
%	O
is	O
submerged	O
.	O
this	O
gives	O
a	O
simple	O
algorithm	O
for	O
computing	O
hdis	O
in	O
the	O
1d	O
case	O
:	O
simply	O
search	O
over	O
points	O
such	O
that	O
the	O
interval	O
contains	O
95	O
%	O
of	O
the	O
mass	O
and	O
has	O
minimal	B
width	O
.	O
this	O
can	O
be	O
done	O
by	O
1d	O
numerical	O
optimization	O
if	O
we	O
know	O
the	O
inverse	O
cdf	O
of	O
the	O
distribution	O
,	O
or	O
by	O
search	O
over	O
the	O
sorted	O
data	O
points	O
if	O
we	O
have	O
a	O
bag	O
of	O
samples	O
(	O
see	O
betahpd	O
for	O
a	O
demo	O
)	O
.	O
if	O
the	O
posterior	O
is	O
multimodal	O
,	O
the	O
hdi	O
may	O
not	O
even	O
be	O
a	O
connected	O
region	O
:	O
see	O
figure	O
5.4	O
(	O
b	O
)	O
for	O
an	O
example	O
.	O
however	O
,	O
summarizing	O
multimodal	O
posteriors	O
is	O
always	O
difficult	O
.	O
5.2.3	O
inference	B
for	O
a	O
difference	O
in	O
proportions	O
sometimes	O
we	O
have	O
multiple	O
parameters	O
,	O
and	O
we	O
are	O
interested	O
in	O
computing	O
the	O
posterior	O
distribution	O
of	O
some	O
function	O
of	O
these	O
parameters	O
.	O
for	O
example	O
,	O
suppose	O
you	O
are	O
about	O
to	O
buy	O
something	O
from	O
amazon.com	O
,	O
and	O
there	O
are	O
two	O
sellers	O
offering	O
it	O
for	O
the	O
same	O
price	O
.	O
seller	O
1	O
has	O
90	O
positive	O
reviews	O
and	O
10	O
negative	O
reviews	O
.	O
seller	O
2	O
has	O
2	O
positive	O
reviews	O
and	O
0	O
negative	O
reviews	O
.	O
who	O
should	O
you	O
buy	O
from	O
?	O
1	O
1.	O
this	O
example	O
is	O
from	O
www.johndcook.com/blog/2011/09/27/bayesian-amazon	O
.	O
see	O
also	O
lingpipe-blog.c	O
om/2009/10/13/bayesian-counterpart-to-fisher-exact-test-on-contingency-tables	O
.	O
5.3.	O
bayesian	O
model	B
selection	I
155	O
14	O
12	O
10	O
8	O
6	O
4	O
2	O
0	O
0	O
p	O
(	O
θ	O
p	O
(	O
θ	O
1|data	O
)	O
2|data	O
)	O
2.5	O
2	O
1.5	O
1	O
0.5	O
f	O
d	O
p	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1	O
(	O
a	O
)	O
δ	O
(	O
b	O
)	O
0	O
−0.4	O
−0.2	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
(	O
a	O
)	O
exact	O
posteriors	O
p	O
(	O
θi|di	O
)	O
.	O
(	O
b	O
)	O
monte	O
carlo	O
approximation	O
to	O
p	O
(	O
δ|d	O
)	O
.	O
we	O
use	O
kernel	O
density	O
figure	O
5.5	O
estimation	O
to	O
get	O
a	O
smooth	O
plot	O
.	O
the	O
vertical	O
lines	O
enclose	O
the	O
95	O
%	O
central	B
interval	I
.	O
figure	O
generated	O
by	O
amazonsellerdemo	O
,	O
on	O
the	O
face	O
of	O
it	O
,	O
you	O
should	O
pick	O
seller	O
2	O
,	O
but	O
we	O
can	O
not	O
be	O
very	O
conﬁdent	O
that	O
seller	O
2	O
is	O
better	O
since	O
it	O
has	O
had	O
so	O
few	O
reviews	O
.	O
in	O
this	O
section	O
,	O
we	O
sketch	O
a	O
bayesian	O
analysis	O
of	O
this	O
problem	O
.	O
similar	B
methodology	O
can	O
be	O
used	O
to	O
compare	O
rates	O
or	O
proportions	O
across	O
groups	O
for	O
a	O
variety	O
of	O
other	O
settings	O
.	O
let	O
θ1	O
and	O
θ2	O
be	O
the	O
unknown	B
reliabilities	O
of	O
the	O
two	O
sellers	O
.	O
since	O
we	O
don	O
’	O
t	O
know	O
much	O
about	O
them	O
,	O
we	O
’	O
ll	O
endow	O
them	O
both	O
with	O
uniform	O
priors	O
,	O
θi	O
∼	O
beta	O
(	O
1	O
,	O
1	O
)	O
.	O
the	O
posteriors	O
are	O
p	O
(	O
θ1|d1	O
)	O
=	O
beta	O
(	O
91	O
,	O
11	O
)	O
and	O
p	O
(	O
θ2|d2	O
)	O
=	O
beta	O
(	O
3	O
,	O
1	O
)	O
.	O
let	O
us	O
deﬁne	O
δ	O
=	O
θ1	O
−	O
θ2	O
as	O
the	O
difference	O
in	O
the	O
rates	O
.	O
(	O
alternatively	O
we	O
might	O
want	O
to	O
work	O
in	O
terms	O
of	O
the	O
log-odds	B
ratio	I
.	O
)	O
we	O
can	O
compute	O
the	O
desired	O
quantity	O
using	O
numerical	O
integration	O
:	O
we	O
want	O
to	O
compute	O
p	O
(	O
θ1	O
>	O
θ2|d	O
)	O
.	O
for	O
convenience	O
,	O
(	O
cid:11	O
)	O
1	O
(	O
cid:11	O
)	O
1	O
p	O
(	O
δ	O
>	O
0|d	O
)	O
=	O
i	O
(	O
θ1	O
>	O
θ2	O
)	O
beta	O
(	O
θ1|y1	O
+	O
1	O
,	O
n1	O
−	O
y1	O
+	O
1	O
)	O
0	O
0	O
beta	O
(	O
θ2|y2	O
+	O
1	O
,	O
n2	O
−	O
y2	O
+	O
1	O
)	O
dθ1dθ2	O
(	O
5.11	O
)	O
we	O
ﬁnd	O
p	O
(	O
δ	O
>	O
0|d	O
)	O
=	O
0.710	O
,	O
which	O
means	O
you	O
are	O
better	O
off	O
buying	O
from	O
seller	O
1	O
!	O
see	O
amazonsellerdemo	O
for	O
the	O
code	O
.	O
(	O
it	O
is	O
also	O
possible	O
to	O
solve	O
the	O
integral	O
analytically	O
(	O
cook	O
2005	O
)	O
.	O
)	O
a	O
simpler	O
way	O
to	O
solve	O
the	O
problem	O
is	O
to	O
approximate	O
the	O
posterior	O
p	O
(	O
δ|d	O
)	O
by	O
monte	O
carlo	O
sampling	O
.	O
this	O
is	O
easy	O
,	O
since	O
θ1	O
and	O
θ2	O
are	O
independent	O
in	O
the	O
posterior	O
,	O
and	O
both	O
have	O
beta	O
distributions	O
,	O
which	O
can	O
be	O
sampled	O
from	O
using	O
standard	O
methods	O
.	O
the	O
distributions	O
p	O
(	O
θi|di	O
)	O
are	O
shown	O
in	O
figure	O
5.5	O
(	O
a	O
)	O
,	O
and	O
a	O
mc	O
approximation	O
to	O
p	O
(	O
δ|d	O
)	O
,	O
together	O
with	O
a	O
95	O
%	O
hpd	O
,	O
is	O
shown	O
figure	O
5.5	O
(	O
b	O
)	O
.	O
an	O
mc	O
approximation	O
to	O
p	O
(	O
δ	O
>	O
0|d	O
)	O
is	O
obtained	O
by	O
counting	O
the	O
fraction	O
of	O
samples	O
where	O
θ1	O
>	O
θ2	O
;	O
this	O
turns	O
out	O
to	O
be	O
0.718	O
,	O
which	O
is	O
very	O
close	O
to	O
the	O
exact	O
value	O
.	O
(	O
see	O
amazonsellerdemo	O
for	O
the	O
code	O
.	O
)	O
5.3	O
bayesian	O
model	B
selection	I
in	O
figure	O
1.18	O
,	O
we	O
saw	O
that	O
using	O
too	O
high	O
a	O
degree	B
polynomial	O
results	O
in	O
overﬁtting	B
,	O
and	O
using	O
too	O
low	O
a	O
degree	B
results	O
in	O
underﬁtting	O
.	O
similarly	O
,	O
in	O
figure	O
7.8	O
(	O
a	O
)	O
,	O
we	O
saw	O
that	O
using	O
too	O
small	O
156	O
chapter	O
5.	O
bayesian	O
statistics	O
a	O
regularization	B
parameter	O
results	O
in	O
overﬁtting	B
,	O
and	O
too	O
large	O
a	O
value	O
results	O
in	O
underﬁtting	O
.	O
in	O
general	O
,	O
when	O
faced	O
with	O
a	O
set	O
of	O
models	O
(	O
i.e.	O
,	O
families	O
of	O
parametric	O
distributions	O
)	O
of	O
different	O
complexity	O
,	O
how	O
should	O
we	O
choose	O
the	O
best	O
one	O
?	O
this	O
is	O
called	O
the	O
model	B
selection	I
problem	O
.	O
one	O
approach	O
is	O
to	O
use	O
cross-validation	O
to	O
estimate	O
the	O
generalization	B
error	I
of	O
all	O
the	O
candiate	O
models	O
,	O
and	O
then	O
to	O
pick	O
the	O
model	O
that	O
seems	O
the	O
best	O
.	O
however	O
,	O
this	O
requires	O
ﬁtting	O
each	O
model	O
k	O
times	O
,	O
where	O
k	O
is	O
the	O
number	O
of	O
cv	O
folds	B
.	O
a	O
more	O
efficient	O
approach	O
is	O
to	O
compute	O
the	O
posterior	O
over	O
models	O
,	O
(	O
5.12	O
)	O
from	O
this	O
,	O
we	O
can	O
easily	O
compute	O
the	O
map	O
model	O
,	O
ˆm	O
=	O
argmax	O
p	O
(	O
m|d	O
)	O
.	O
this	O
is	O
called	O
bayesian	O
model	B
selection	I
.	O
if	O
we	O
use	O
a	O
uniform	O
prior	O
over	O
models	O
,	O
p	O
(	O
m	O
)	O
∝	O
1	O
,	O
this	O
amounts	O
to	O
picking	O
the	O
model	O
which	O
p	O
(	O
m|d	O
)	O
=	O
(	O
cid:4	O
)	O
p	O
(	O
d|m	O
)	O
p	O
(	O
m	O
)	O
m∈m	O
p	O
(	O
m	O
,	O
d	O
)	O
(	O
cid:11	O
)	O
maximizes	O
p	O
(	O
d|m	O
)	O
=	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ|m	O
)	O
dθ	O
(	O
5.13	O
)	O
this	O
quantity	O
is	O
called	O
the	O
marginal	B
likelihood	I
,	O
the	O
integrated	B
likelihood	I
,	O
or	O
the	O
evidence	B
for	O
model	O
m.	O
the	O
details	O
on	O
how	O
to	O
perform	O
this	O
integral	O
will	O
be	O
discussed	O
in	O
section	O
5.3.2.	O
but	O
ﬁrst	O
we	O
give	O
an	O
intuitive	O
interpretation	O
of	O
what	O
this	O
quantity	O
means	O
.	O
5.3.1	O
bayesian	O
occam	O
’	O
s	O
razor	O
one	O
might	O
think	O
that	O
using	O
p	O
(	O
d|m	O
)	O
to	O
select	O
models	O
would	O
always	O
favor	O
the	O
model	O
with	O
the	O
most	O
parameters	O
.	O
this	O
is	O
true	O
if	O
we	O
use	O
p	O
(	O
d|ˆθm	O
)	O
to	O
select	O
models	O
,	O
where	O
ˆθm	O
is	O
the	O
mle	O
or	O
map	O
estimate	O
of	O
the	O
parameters	O
for	O
model	O
m	O
,	O
because	O
models	O
with	O
more	O
parameters	O
will	O
ﬁt	O
the	O
data	O
better	O
,	O
and	O
hence	O
achieve	O
higher	O
likelihood	B
.	O
however	O
,	O
if	O
we	O
integrate	B
out	I
the	O
parameters	O
,	O
rather	O
than	O
maximizing	O
them	O
,	O
we	O
are	O
automatically	O
protected	O
from	O
overﬁtting	B
:	O
models	O
with	O
more	O
parameters	O
do	O
not	O
necessarily	O
have	O
higher	O
marginal	B
likelihood	I
.	O
this	O
is	O
called	O
the	O
bayesian	O
occam	O
’	O
s	O
razor	O
effect	O
(	O
mackay	O
1995b	O
;	O
murray	O
and	O
ghahramani	O
2005	O
)	O
,	O
named	O
after	O
the	O
principle	O
known	O
as	O
occam	O
’	O
s	O
razor	O
,	O
which	O
says	O
one	O
should	O
pick	O
the	O
simplest	O
model	O
that	O
adequately	O
explains	O
the	O
data	O
.	O
one	O
way	O
to	O
understand	O
the	O
bayesian	O
occam	O
’	O
s	O
razor	O
is	O
to	O
notice	O
that	O
the	O
marginal	B
likelihood	I
can	O
be	O
rewritten	O
as	O
follows	O
,	O
based	O
on	O
the	O
chain	B
rule	I
of	O
probability	O
(	O
equation	O
2.5	O
)	O
:	O
p	O
(	O
d	O
)	O
=	O
p	O
(	O
y1	O
)	O
p	O
(	O
y2|y1	O
)	O
p	O
(	O
y3|y1:2	O
)	O
.	O
.	O
.	O
p	O
(	O
yn|y1	O
:	O
n−1	O
)	O
(	O
5.14	O
)	O
where	O
we	O
have	O
dropped	O
the	O
conditioning	B
on	O
x	O
for	O
brevity	O
.	O
this	O
is	O
similar	B
to	O
a	O
leave-one-out	O
cross-validation	O
estimate	O
(	O
section	O
1.4.8	O
)	O
of	O
the	O
likelihood	B
,	O
since	O
we	O
predict	O
each	O
future	O
point	O
given	O
all	O
the	O
previous	O
ones	O
.	O
(	O
of	O
course	O
,	O
the	O
order	O
of	O
the	O
data	O
does	O
not	O
matter	O
in	O
the	O
above	O
expression	O
.	O
)	O
if	O
a	O
model	O
is	O
too	O
complex	O
,	O
it	O
will	O
overﬁt	B
the	O
“	O
early	O
”	O
examples	O
and	O
will	O
then	O
predict	O
the	O
remaining	O
ones	O
poorly	O
.	O
another	O
way	O
to	O
understand	O
the	O
bayesian	O
occam	O
’	O
s	O
razor	O
effect	O
is	O
to	O
note	O
that	O
probabilities	O
must	O
d	O
(	O
cid:2	O
)	O
p	O
(	O
d	O
(	O
cid:2	O
)	O
|m	O
)	O
=	O
1	O
,	O
where	O
the	O
sum	O
is	O
over	O
all	O
possible	O
data	O
sets	O
.	O
complex	O
sum	O
to	O
one	O
.	O
hence	O
models	O
,	O
which	O
can	O
predict	O
many	O
things	O
,	O
must	O
spread	O
their	O
probability	O
mass	O
thinly	O
,	O
and	O
hence	O
will	O
not	O
obtain	O
as	O
large	O
a	O
probability	O
for	O
any	O
given	O
data	O
set	O
as	O
simpler	O
models	O
.	O
this	O
is	O
sometimes	O
(	O
cid:4	O
)	O
5.3.	O
bayesian	O
model	B
selection	I
157	O
figure	O
5.6	O
a	O
schematic	O
illustration	O
of	O
the	O
bayesian	O
occam	O
’	O
s	O
razor	O
.	O
the	O
broad	O
(	O
green	O
)	O
curve	O
corresponds	O
to	O
a	O
complex	O
model	O
,	O
the	O
narrow	O
(	O
blue	O
)	O
curve	O
to	O
a	O
simple	O
model	O
,	O
and	O
the	O
middle	O
(	O
red	O
)	O
curve	O
is	O
just	O
right	O
.	O
based	O
on	O
figure	O
3.13	O
of	O
(	O
bishop	O
2006a	O
)	O
.	O
see	O
also	O
(	O
murray	O
and	O
ghahramani	O
2005	O
,	O
figure	O
2	O
)	O
for	O
a	O
similar	B
plot	O
produced	O
on	O
real	O
data	O
.	O
called	O
the	O
conservation	B
of	I
probability	I
mass	I
principle	O
,	O
and	O
is	O
illustrated	O
in	O
figure	O
5.6.	O
on	O
the	O
horizontal	O
axis	O
we	O
plot	O
all	O
possible	O
data	O
sets	O
in	O
order	O
of	O
increasing	O
complexity	O
(	O
measured	O
in	O
some	O
abstract	O
sense	O
)	O
.	O
on	O
the	O
vertical	O
axis	O
we	O
plot	O
the	O
predictions	O
of	O
3	O
possible	O
models	O
:	O
a	O
simple	O
one	O
,	O
m1	O
;	O
a	O
medium	O
one	O
,	O
m2	O
;	O
and	O
a	O
complex	O
one	O
,	O
m3	O
.	O
we	O
also	O
indicate	O
the	O
actually	O
observed	O
data	O
d0	O
by	O
a	O
vertical	O
line	O
.	O
model	O
1	O
is	O
too	O
simple	O
and	O
assigns	O
low	O
probability	O
to	O
d0	O
.	O
model	O
3	O
also	O
assigns	O
d0	O
relatively	O
low	O
probability	O
,	O
because	O
it	O
can	O
predict	O
many	O
data	O
sets	O
,	O
and	O
hence	O
it	O
spreads	O
its	O
probability	O
quite	O
widely	O
and	O
thinly	O
.	O
model	O
2	O
is	O
“	O
just	O
right	O
”	O
:	O
it	O
predicts	O
the	O
observed	O
data	O
with	O
a	O
reasonable	O
degree	B
of	O
conﬁdence	O
,	O
but	O
does	O
not	O
predict	O
too	O
many	O
other	O
things	O
.	O
hence	O
model	O
2	O
is	O
the	O
most	O
probable	O
model	O
.	O
as	O
a	O
concrete	O
example	O
of	O
the	O
bayesian	O
occam	O
’	O
s	O
razor	O
,	O
consider	O
the	O
data	O
in	O
figure	O
5.7.	O
we	O
plot	O
polynomials	O
of	O
degrees	O
1	O
,	O
2	O
and	O
3	O
ﬁt	O
to	O
n	O
=	O
5	O
data	O
points	O
.	O
it	O
also	O
shows	O
the	O
posterior	O
over	O
models	O
,	O
where	O
we	O
use	O
a	O
gaussian	O
prior	O
(	O
see	O
section	O
7.6	O
for	O
details	O
)	O
.	O
there	O
is	O
not	O
enough	O
data	O
to	O
justify	O
a	O
complex	O
model	O
,	O
so	O
the	O
map	O
model	O
is	O
d	O
=	O
1.	O
figure	O
5.8	O
shows	O
what	O
happens	O
when	O
n	O
=	O
30.	O
now	O
it	O
is	O
clear	O
that	O
d	O
=	O
2	O
is	O
the	O
right	O
model	O
(	O
the	O
data	O
was	O
in	O
fact	O
generated	O
from	O
a	O
quadratic	O
)	O
.	O
as	O
another	O
example	O
,	O
figure	O
7.8	O
(	O
c	O
)	O
plots	O
log	O
p	O
(	O
d|λ	O
)	O
vs	O
log	O
(	O
λ	O
)	O
,	O
for	O
the	O
polynomial	O
ridge	O
regres-	O
sion	O
model	O
,	O
where	O
λ	O
ranges	O
over	O
the	O
same	O
set	O
of	O
values	O
used	O
in	O
the	O
cv	O
experiment	O
.	O
we	O
see	O
that	O
the	O
maximum	O
evidence	O
occurs	O
at	O
roughly	O
the	O
same	O
point	O
as	O
the	O
minimum	O
of	O
the	O
test	O
mse	O
,	O
which	O
also	O
corresponds	O
to	O
the	O
point	O
chosen	O
by	O
cv	O
.	O
when	O
using	O
the	O
bayesian	O
approach	O
,	O
we	O
are	O
not	O
restricted	O
to	O
evaluating	O
the	O
evidence	B
at	O
a	O
=	O
argmaxλ	O
p	O
(	O
d|λ	O
)	O
.	O
ﬁnite	O
grid	O
of	O
values	O
.	O
instead	O
,	O
we	O
can	O
use	O
numerical	O
optimization	O
to	O
ﬁnd	O
λ∗	O
this	O
technique	O
is	O
called	O
empirical	O
bayes	O
or	O
type	O
ii	O
maximum	O
likelihood	O
(	O
see	O
section	O
5.6	O
for	O
details	O
)	O
.	O
an	O
example	O
is	O
shown	O
in	O
figure	O
7.8	O
(	O
b	O
)	O
:	O
we	O
see	O
that	O
the	O
curve	O
has	O
a	O
similar	B
shape	O
to	O
the	O
cv	O
estimate	O
,	O
but	O
it	O
can	O
be	O
computed	O
more	O
efficiently	O
.	O
158	O
70	O
60	O
50	O
40	O
30	O
20	O
10	O
0	O
−10	O
−20	O
−2	O
300	O
250	O
200	O
150	O
100	O
50	O
0	O
−50	O
−100	O
−150	O
−200	O
−2	O
d=1	O
,	O
logev=−18.593	O
,	O
eb	O
d=2	O
,	O
logev=−20.218	O
,	O
eb	O
chapter	O
5.	O
bayesian	O
statistics	O
80	O
60	O
40	O
20	O
0	O
−20	O
−40	O
−60	O
−80	O
−2	O
)	O
|	O
d	O
m	O
p	O
(	O
0	O
2	O
4	O
6	O
8	O
10	O
12	O
(	O
b	O
)	O
n=5	O
,	O
method=eb	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
2	O
3	O
m	O
1	O
(	O
d	O
)	O
0	O
2	O
4	O
6	O
8	O
10	O
12	O
(	O
a	O
)	O
d=3	O
,	O
logev=−21.718	O
,	O
eb	O
0	O
2	O
4	O
6	O
8	O
10	O
12	O
(	O
c	O
)	O
(	O
a-c	O
)	O
we	O
plot	O
polynomials	O
of	O
degrees	O
1	O
,	O
2	O
and	O
3	O
ﬁt	O
to	O
n	O
=	O
5	O
data	O
points	O
using	O
empirical	O
figure	O
5.7	O
bayes	O
.	O
the	O
solid	O
green	O
curve	O
is	O
the	O
true	O
function	O
,	O
the	O
dashed	O
red	O
curve	O
is	O
the	O
prediction	O
(	O
dotted	O
blue	O
lines	O
represent	O
±σ	O
around	O
the	O
mean	B
)	O
.	O
(	O
d	O
)	O
we	O
plot	O
the	O
posterior	O
over	O
models	O
,	O
p	O
(	O
d|d	O
)	O
,	O
assuming	O
a	O
uniform	O
prior	O
p	O
(	O
d	O
)	O
∝	O
1.	O
based	O
on	O
a	O
ﬁgure	O
by	O
zoubin	O
ghahramani	O
.	O
figure	O
generated	O
by	O
linregebmodelselvsn	O
.	O
5.3.2	O
computing	O
the	O
marginal	B
likelihood	I
(	O
evidence	B
)	O
when	O
discussing	O
parameter	B
inference	O
for	O
a	O
ﬁxed	O
model	O
,	O
we	O
often	O
wrote	O
p	O
(	O
θ|d	O
,	O
m	O
)	O
∝	O
p	O
(	O
θ|m	O
)	O
p	O
(	O
d|θ	O
,	O
m	O
)	O
(	O
5.15	O
)	O
thus	O
ignoring	O
the	O
normalization	O
constant	O
p	O
(	O
d|m	O
)	O
.	O
this	O
is	O
valid	O
since	O
p	O
(	O
d|m	O
)	O
is	O
constant	O
wrt	O
θ.	O
however	O
,	O
when	O
comparing	O
models	O
,	O
we	O
need	O
to	O
know	O
how	O
to	O
compute	O
the	O
marginal	B
likelihood	I
,	O
p	O
(	O
d|m	O
)	O
.	O
in	O
general	O
,	O
this	O
can	O
be	O
quite	O
hard	O
,	O
since	O
we	O
have	O
to	O
integrate	O
over	O
all	O
possible	O
parameter	B
values	O
,	O
but	O
when	O
we	O
have	O
a	O
conjugate	B
prior	I
,	O
it	O
is	O
easy	O
to	O
compute	O
,	O
as	O
we	O
now	O
show	O
.	O
let	O
p	O
(	O
θ	O
)	O
=q	O
(	O
θ	O
)	O
/z0	O
be	O
our	O
prior	O
,	O
where	O
q	O
(	O
θ	O
)	O
is	O
an	O
unnormalized	O
distribution	O
,	O
and	O
z0	O
is	O
the	O
normalization	O
constant	O
of	O
the	O
prior	O
.	O
let	O
p	O
(	O
d|θ	O
)	O
=q	O
(	O
d|θ	O
)	O
/z	O
(	O
cid:11	O
)	O
be	O
the	O
likelihood	B
,	O
where	O
z	O
(	O
cid:11	O
)	O
contains	O
any	O
constant	O
factors	O
in	O
the	O
likelihood	B
.	O
finally	O
let	O
p	O
(	O
θ|d	O
)	O
=	O
q	O
(	O
θ|d	O
)	O
/zn	O
be	O
our	O
poste-	O
5.3.	O
bayesian	O
model	B
selection	I
159	O
d=1	O
,	O
logev=−106.110	O
,	O
eb	O
d=2	O
,	O
logev=−103.025	O
,	O
eb	O
80	O
70	O
60	O
50	O
40	O
30	O
20	O
10	O
0	O
−10	O
−2	O
)	O
|	O
d	O
m	O
p	O
(	O
70	O
60	O
50	O
40	O
30	O
20	O
10	O
0	O
−10	O
−2	O
100	O
80	O
60	O
40	O
20	O
0	O
−20	O
−2	O
0	O
2	O
4	O
6	O
8	O
10	O
12	O
(	O
a	O
)	O
d=3	O
,	O
logev=−107.410	O
,	O
eb	O
0	O
2	O
4	O
6	O
8	O
10	O
12	O
(	O
c	O
)	O
0	O
2	O
4	O
6	O
8	O
10	O
12	O
(	O
b	O
)	O
n=30	O
,	O
method=eb	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
2	O
3	O
m	O
1	O
(	O
d	O
)	O
figure	O
5.8	O
same	O
as	O
figure	O
5.7	O
except	O
now	O
n	O
=	O
30.	O
figure	O
generated	O
by	O
linregebmodelselvsn	O
.	O
rior	O
,	O
where	O
q	O
(	O
θ|d	O
)	O
=	O
q	O
(	O
d|θ	O
)	O
q	O
(	O
θ	O
)	O
is	O
the	O
unnormalized	O
posterior	O
,	O
and	O
zn	O
is	O
the	O
normalization	O
constant	O
of	O
the	O
posterior	O
.	O
we	O
have	O
p	O
(	O
θ|d	O
)	O
=	O
q	O
(	O
θ|d	O
)	O
=	O
zn	O
p	O
(	O
d	O
)	O
=	O
p	O
(	O
d	O
)	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
q	O
(	O
d|θ	O
)	O
q	O
(	O
θ	O
)	O
z	O
(	O
cid:11	O
)	O
z0p	O
(	O
d	O
)	O
zn	O
z0z	O
(	O
cid:11	O
)	O
(	O
5.16	O
)	O
(	O
5.17	O
)	O
(	O
5.18	O
)	O
so	O
assuming	O
the	O
relevant	O
normalization	O
constants	O
are	O
tractable	O
,	O
we	O
have	O
an	O
easy	O
way	O
to	O
compute	O
the	O
marginal	B
likelihood	I
.	O
we	O
give	O
some	O
examples	O
below	O
.	O
p	O
(	O
d	O
)	O
(	O
cid:2	O
)	O
1	O
p	O
(	O
d	O
)	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
n	O
n1	O
b	O
(	O
a	O
,	O
b	O
)	O
1	O
p	O
(	O
d	O
)	O
1	O
b	O
(	O
a	O
,	O
b	O
)	O
(	O
cid:20	O
)	O
1	O
θa−1	O
(	O
1	O
−	O
θ	O
)	O
b−1	O
θn1	O
(	O
1	O
−	O
θ	O
)	O
n0	O
θa+n1−1	O
(	O
1	O
−	O
θ	O
)	O
b+n0−1	O
(	O
cid:21	O
)	O
(	O
cid:8	O
)	O
(	O
cid:3	O
)	O
(	O
cid:2	O
)	O
(	O
cid:7	O
)	O
n	O
n1	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
n	O
n1	O
n	O
n1	O
1	O
1	O
p	O
(	O
d	O
)	O
b	O
(	O
a	O
+	O
n1	O
,	O
b	O
+	O
n0	O
)	O
b	O
(	O
a	O
,	O
b	O
)	O
b	O
(	O
a	O
,	O
b	O
)	O
(	O
cid:3	O
)	O
(	O
5.19	O
)	O
(	O
5.20	O
)	O
(	O
5.21	O
)	O
(	O
5.22	O
)	O
(	O
5.23	O
)	O
=	O
=	O
1	O
so	O
b	O
(	O
a	O
+	O
n1	O
,	O
b	O
+	O
n0	O
)	O
=	O
p	O
(	O
d	O
)	O
=	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
n	O
n1	O
160	O
chapter	O
5.	O
bayesian	O
statistics	O
5.3.2.1	O
beta-binomial	B
model	O
let	O
us	O
apply	O
the	O
above	O
result	O
to	O
the	O
beta-binomial	B
model	O
.	O
since	O
we	O
know	O
p	O
(	O
θ|d	O
)	O
=	O
beta	O
(	O
θ|a	O
(	O
cid:2	O
)	O
,	O
b	O
(	O
cid:2	O
)	O
)	O
,	O
where	O
a	O
(	O
cid:2	O
)	O
=	O
b	O
+	O
n0	O
,	O
we	O
know	O
the	O
normalization	O
constant	O
of	O
the	O
posterior	O
is	O
b	O
(	O
a	O
(	O
cid:2	O
)	O
,	O
b	O
(	O
cid:2	O
)	O
=	O
a	O
+	O
n1	O
and	O
b	O
(	O
cid:2	O
)	O
)	O
.	O
hence	O
p	O
(	O
θ|d	O
)	O
=	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
the	O
marginal	B
likelihood	I
for	O
the	O
beta-bernoulli	O
model	O
is	O
the	O
same	O
as	O
above	O
,	O
except	O
it	O
is	O
missing	B
the	O
term	O
.	O
5.3.2.2	O
dirichlet-multinoulli	O
model	O
by	O
the	O
same	O
reasoning	O
as	O
the	O
beta-bernoulli	O
case	O
,	O
one	O
can	O
show	O
that	O
the	O
marginal	B
likelihood	I
for	O
the	O
dirichlet-multinoulli	O
model	O
is	O
given	O
by	O
p	O
(	O
d	O
)	O
=	O
b	O
(	O
n	O
+	O
α	O
)	O
b	O
(	O
α	O
)	O
where	O
b	O
(	O
α	O
)	O
=	O
(	O
cid:15	O
)	O
k	O
(	O
cid:4	O
)	O
k=1	O
γ	O
(	O
αk	O
)	O
k	O
αk	O
)	O
γ	O
(	O
(	O
5.24	O
)	O
(	O
5.25	O
)	O
hence	O
we	O
can	O
rewrite	O
the	O
above	O
result	O
in	O
the	O
following	O
form	O
,	O
which	O
is	O
what	O
is	O
usually	O
presented	O
in	O
the	O
literature	O
:	O
γ	O
(	O
nk	O
+	O
αk	O
)	O
γ	O
(	O
αk	O
)	O
(	O
5.26	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
k	O
αk	O
)	O
(	O
cid:12	O
)	O
p	O
(	O
d	O
)	O
=	O
γ	O
(	O
γ	O
(	O
n	O
+	O
k	O
αk	O
)	O
k	O
we	O
will	O
see	O
many	O
applications	O
of	O
this	O
equation	O
later	O
.	O
5.3.2.3	O
gaussian-gaussian-wishart	O
model	O
consider	O
the	O
case	O
of	O
an	O
mvn	O
with	O
a	O
conjugate	O
niw	O
prior	O
.	O
let	O
z0	O
be	O
the	O
normalizer	O
for	O
the	O
prior	O
,	O
zn	O
be	O
normalizer	O
for	O
the	O
posterior	O
,	O
and	O
let	O
zl	O
=	O
(	O
2π	O
)	O
n	O
d/2	O
be	O
the	O
normalizer	O
for	O
the	O
5.3.	O
bayesian	O
model	B
selection	I
likelihood	O
.	O
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
p	O
(	O
d	O
)	O
=	O
zn	O
z0zl	O
+	O
2π	O
κn	O
,	O
d/2	O
|sn|−νn	O
/22	O
(	O
ν0+n	O
)	O
d/2γd	O
(	O
νn	O
/2	O
)	O
+	O
,	O
d/2	O
|s0|−ν0/22ν0d/2γd	O
(	O
ν0/2	O
)	O
(	O
cid:8	O
)	O
d/2	O
|s0|ν0/2	O
|sn|νn	O
/2	O
γd	O
(	O
νn	O
/2	O
)	O
γd	O
(	O
ν0/2	O
)	O
2π	O
κ0	O
2n	O
d/2	O
(	O
cid:7	O
)	O
κ0	O
κn	O
1	O
1	O
=	O
=	O
πn	O
d/2	O
1	O
πn	O
d/2	O
161	O
(	O
5.27	O
)	O
(	O
5.28	O
)	O
(	O
5.29	O
)	O
this	O
equation	O
will	O
prove	O
useful	O
later	O
.	O
5.3.2.4	O
bic	O
approximation	O
to	O
log	O
marginal	O
likelihood	B
in	O
general	O
,	O
computing	O
the	O
integral	O
in	O
equation	O
5.13	O
can	O
be	O
quite	O
difficult	O
.	O
one	O
simple	O
but	O
popular	O
approximation	O
is	O
known	O
as	O
the	O
bayesian	O
information	B
criterion	O
or	O
bic	O
,	O
which	O
has	O
the	O
following	O
form	O
(	O
schwarz	O
1978	O
)	O
:	O
bic	O
(	O
cid:2	O
)	O
log	O
p	O
(	O
d|ˆθ	O
)	O
−	O
dof	O
(	O
ˆθ	O
)	O
2	O
log	O
n	O
≈	O
log	O
p	O
(	O
d	O
)	O
(	O
5.30	O
)	O
where	O
dof	O
(	O
ˆθ	O
)	O
is	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
in	O
the	O
model	O
,	O
and	O
ˆθ	O
is	O
the	O
mle	O
for	O
the	O
model.2	O
we	O
see	O
that	O
this	O
has	O
the	O
form	O
of	O
a	O
penalized	B
log	I
likelihood	I
,	O
where	O
the	O
penalty	O
term	O
depends	O
on	O
the	O
model	O
’	O
s	O
complexity	O
.	O
see	O
section	O
8.4.2	O
for	O
the	O
derivation	O
of	O
the	O
bic	O
score	O
.	O
as	O
an	O
example	O
,	O
consider	O
linear	B
regression	I
.	O
as	O
we	O
show	O
in	O
section	O
7.3	O
,	O
the	O
mle	O
is	O
given	O
by	O
ˆw	O
=	O
mlexi	O
)	O
2.	O
the	O
corresponding	O
−1xt	O
y	O
and	O
ˆσ2	O
=	O
rss/n	O
,	O
where	O
rss	O
=	O
(	O
cid:4	O
)	O
n	O
i=1	O
(	O
yi	O
−	O
ˆwt	O
(	O
xt	O
x	O
)	O
log	O
likelihood	O
is	O
given	O
by	O
log	O
p	O
(	O
d|ˆθ	O
)	O
=	O
−	O
n	O
2	O
log	O
(	O
2πˆσ2	O
)	O
−	O
n	O
2	O
hence	O
the	O
bic	O
score	O
is	O
as	O
follows	O
(	O
dropping	O
constant	O
terms	O
)	O
bic	O
=	O
−	O
n	O
2	O
log	O
(	O
ˆσ2	O
)	O
−	O
d	O
2	O
log	O
(	O
n	O
)	O
(	O
5.31	O
)	O
(	O
5.32	O
)	O
where	O
d	O
is	O
the	O
number	O
of	O
variables	O
in	O
the	O
model	O
.	O
use	O
an	O
alternative	O
deﬁnition	O
of	O
bic	O
,	O
which	O
we	O
call	O
the	O
bic	O
cost	O
(	O
since	O
we	O
want	O
to	O
minimize	O
it	O
)	O
:	O
in	O
the	O
statistics	O
literature	O
,	O
it	O
is	O
common	O
to	O
bic-cost	O
(	O
cid:2	O
)	O
−2	O
log	O
p	O
(	O
d|ˆθ	O
)	O
+	O
dof	O
(	O
ˆθ	O
)	O
log	O
n	O
≈	O
−2	O
log	O
p	O
(	O
d	O
)	O
in	O
the	O
context	O
of	O
linear	B
regression	I
,	O
this	O
becomes	O
bic-cost	O
=	O
n	O
log	O
(	O
ˆσ2	O
)	O
+d	O
log	O
(	O
n	O
)	O
(	O
5.33	O
)	O
(	O
5.34	O
)	O
2.	O
traditionally	O
the	O
bic	O
score	O
is	O
deﬁned	O
using	O
the	O
ml	O
estimate	O
ˆθ	O
,	O
so	O
it	O
is	O
independent	O
of	O
the	O
prior	O
.	O
however	O
,	O
for	O
models	O
such	O
as	O
mixtures	O
of	O
gaussians	O
,	O
the	O
ml	O
estimate	O
can	O
be	O
poorly	O
behaved	O
,	O
so	O
it	O
is	O
better	O
to	O
evaluate	O
the	O
bic	O
score	O
using	O
the	O
map	O
estimate	O
,	O
as	O
in	O
(	O
fraley	O
and	O
raftery	O
2007	O
)	O
.	O
162	O
chapter	O
5.	O
bayesian	O
statistics	O
the	O
bic	O
method	O
is	O
very	O
closely	O
related	O
to	O
the	O
minimum	B
description	I
length	I
or	O
mdl	O
principle	O
,	O
which	O
characterizes	O
the	O
score	O
for	O
a	O
model	O
in	O
terms	O
of	O
how	O
well	O
it	O
ﬁts	O
the	O
data	O
,	O
minus	O
how	O
complex	O
the	O
model	O
is	O
to	O
deﬁne	O
.	O
see	O
(	O
hansen	O
and	O
yu	O
2001	O
)	O
for	O
details	O
.	O
there	O
is	O
a	O
very	O
similar	B
expression	O
to	O
bic/	O
mdl	O
called	O
the	O
akaike	O
information	B
criterion	O
or	O
aic	O
,	O
deﬁned	O
as	O
aic	O
(	O
m	O
,	O
d	O
)	O
(	O
cid:2	O
)	O
log	O
p	O
(	O
d|ˆθm	O
le	O
)	O
−	O
dof	O
(	O
m	O
)	O
(	O
5.35	O
)	O
this	O
is	O
derived	O
from	O
a	O
frequentist	B
framework	O
,	O
and	O
can	O
not	O
be	O
interpreted	O
as	O
an	O
approximation	O
to	O
the	O
marginal	B
likelihood	I
.	O
nevertheless	O
,	O
the	O
form	O
of	O
this	O
expression	O
is	O
very	O
similar	B
to	O
bic	O
.	O
we	O
see	O
that	O
the	O
penalty	O
for	O
aic	O
is	O
less	O
than	O
for	O
bic	O
.	O
this	O
causes	O
aic	O
to	O
pick	O
more	O
complex	O
models	O
.	O
however	O
,	O
this	O
can	O
result	O
in	O
better	O
predictive	B
accuracy	O
.	O
see	O
e.g.	O
,	O
(	O
clarke	O
et	O
al	O
.	O
2009	O
,	O
sec	O
10.2	O
)	O
for	O
further	O
discussion	O
on	O
such	O
information	B
criteria	O
.	O
5.3.2.5	O
effect	O
of	O
the	O
prior	O
sometimes	O
it	O
is	O
not	O
clear	O
how	O
to	O
set	O
the	O
prior	O
.	O
when	O
we	O
are	O
performing	O
posterior	O
inference	O
,	O
the	O
details	O
of	O
the	O
prior	O
may	O
not	O
matter	O
too	O
much	O
,	O
since	O
the	O
likelihood	B
often	O
overwhelms	O
the	O
prior	O
anyway	O
.	O
but	O
when	O
computing	O
the	O
marginal	B
likelihood	I
,	O
the	O
prior	O
plays	O
a	O
much	O
more	O
important	O
role	O
,	O
since	O
we	O
are	O
averaging	O
the	O
likelihood	B
over	O
all	O
possible	O
parameter	B
settings	O
,	O
as	O
weighted	O
by	O
the	O
prior	O
.	O
in	O
figures	O
5.7	O
and	O
5.8	O
,	O
where	O
we	O
demonstrated	O
model	B
selection	I
for	O
linear	B
regression	I
,	O
we	O
used	O
a	O
prior	O
of	O
the	O
form	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
0	O
,	O
α−1i	O
)	O
.	O
here	O
α	O
is	O
a	O
tuning	O
parameter	B
that	O
controls	O
how	O
strong	O
the	O
prior	O
is	O
.	O
this	O
parameter	B
can	O
have	O
a	O
large	O
effect	O
,	O
as	O
we	O
discuss	O
in	O
section	O
7.5.	O
intuitively	O
,	O
if	O
α	O
is	O
large	O
,	O
the	O
weights	O
are	O
“	O
forced	O
”	O
to	O
be	O
small	O
,	O
so	O
we	O
need	O
to	O
use	O
a	O
complex	O
model	O
with	O
many	O
small	O
parameters	O
(	O
e.g.	O
,	O
a	O
high	O
degree	O
polynomial	O
)	O
to	O
ﬁt	O
the	O
data	O
.	O
conversely	O
,	O
if	O
α	O
is	O
small	O
,	O
we	O
will	O
favor	O
simpler	O
models	O
,	O
since	O
each	O
parameter	B
is	O
“	O
allowed	O
”	O
to	O
vary	O
in	O
magnitude	O
by	O
a	O
lot	O
.	O
if	O
the	O
prior	O
is	O
unknown	B
,	O
the	O
correct	O
bayesian	O
procedure	O
is	O
to	O
put	O
a	O
prior	O
on	O
the	O
prior	O
.	O
that	O
is	O
,	O
we	O
should	O
put	O
a	O
prior	O
on	O
the	O
hyper-parameter	O
α	O
as	O
well	O
as	O
the	O
parametrs	O
w.	O
to	O
compute	O
the	O
marginal	B
likelihood	I
,	O
we	O
should	O
integrate	B
out	I
all	O
unknowns	O
,	O
i.e.	O
,	O
we	O
should	O
compute	O
p	O
(	O
d|m	O
)	O
=	O
p	O
(	O
d|w	O
)	O
p	O
(	O
w|α	O
,	O
m	O
)	O
p	O
(	O
α|m	O
)	O
dwdα	O
(	O
5.36	O
)	O
of	O
course	O
,	O
this	O
requires	O
specifying	O
the	O
hyper-prior	O
.	O
fortunately	O
,	O
the	O
higher	O
up	O
we	O
go	O
in	O
the	O
bayesian	O
hierarchy	O
,	O
the	O
less	O
sensitive	O
are	O
the	O
results	O
to	O
the	O
prior	O
settings	O
.	O
so	O
we	O
can	O
usually	O
make	O
the	O
hyper-prior	O
uninformative	B
.	O
a	O
computational	O
shortcut	O
is	O
to	O
optimize	O
α	O
rather	O
than	O
integrating	O
it	O
out	O
.	O
that	O
is	O
,	O
we	O
use	O
p	O
(	O
d|m	O
)	O
≈	O
(	O
5.37	O
)	O
p	O
(	O
d|w	O
)	O
p	O
(	O
w|ˆα	O
,	O
m	O
)	O
dw	O
(	O
cid:11	O
)	O
p	O
(	O
d|α	O
,	O
m	O
)	O
=	O
argmax	O
α	O
where	O
ˆα	O
=	O
argmax	O
α	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
p	O
(	O
d|w	O
)	O
p	O
(	O
w|α	O
,	O
m	O
)	O
dw	O
(	O
5.38	O
)	O
this	O
approach	O
is	O
called	O
empirical	O
bayes	O
(	O
eb	O
)	O
,	O
and	O
is	O
discussed	O
in	O
more	O
detail	O
in	O
section	O
5.6.	O
this	O
is	O
the	O
method	O
used	O
in	O
figures	O
5.7	O
and	O
5.8	O
.	O
5.3.	O
bayesian	O
model	B
selection	I
163	O
bayes	O
factor	B
bf	O
(	O
1	O
,	O
0	O
)	O
bf	O
<	O
1	O
100	O
bf	O
<	O
1	O
10	O
3	O
1	O
10	O
<	O
bf	O
<	O
1	O
1	O
3	O
<	O
bf	O
<	O
1	O
1	O
<	O
bf	O
<	O
3	O
3	O
<	O
bf	O
<	O
10	O
bf	O
>	O
10	O
bf	O
>	O
100	O
interpretation	O
decisive	O
evidence	B
for	O
m0	O
strong	O
evidence	O
for	O
m0	O
moderate	O
evidence	B
for	O
m0	O
weak	O
evidence	O
for	O
m0	O
weak	O
evidence	O
for	O
m1	O
moderate	O
evidence	B
for	O
m1	O
strong	O
evidence	O
for	O
m1	O
decisive	O
evidence	B
for	O
m1	O
table	O
5.1	O
jeffreys	O
’	O
scale	B
of	I
evidence	I
for	O
interpreting	O
bayes	O
factors	B
.	O
5.3.3	O
bayes	O
factors	B
suppose	O
our	O
prior	O
on	O
models	O
is	O
uniform	O
,	O
p	O
(	O
m	O
)	O
∝	O
1.	O
then	O
model	B
selection	I
is	O
equivalent	O
to	O
picking	O
the	O
model	O
with	O
the	O
highest	O
marginal	O
likelihood	B
.	O
now	O
suppose	O
we	O
just	O
have	O
two	O
models	O
we	O
are	O
considering	O
,	O
call	O
them	O
the	O
null	B
hypothesis	I
,	O
m0	O
,	O
and	O
the	O
alternative	B
hypothesis	I
,	O
m1	O
.	O
deﬁne	O
the	O
bayes	O
factor	B
as	O
the	O
ratio	O
of	O
marginal	O
likelihoods	O
:	O
bf1,0	O
(	O
cid:2	O
)	O
p	O
(	O
d|m1	O
)	O
p	O
(	O
d|m0	O
)	O
p	O
(	O
m1|d	O
)	O
p	O
(	O
m0|d	O
)	O
=	O
/	O
p	O
(	O
m1	O
)	O
p	O
(	O
m0	O
)	O
(	O
5.39	O
)	O
(	O
this	O
is	O
like	O
a	O
likelihood	B
ratio	I
,	O
except	O
we	O
integrate	B
out	I
the	O
parameters	O
,	O
which	O
allows	O
us	O
to	O
if	O
bf1,0	O
>	O
1	O
then	O
we	O
prefer	O
model	O
1	O
,	O
otherwise	O
we	O
compare	O
models	O
of	O
different	O
complexity	O
.	O
)	O
prefer	O
model	O
0.	O
of	O
course	O
,	O
it	O
might	O
be	O
that	O
bf1,0	O
is	O
only	O
slightly	O
greater	O
than	O
1.	O
in	O
that	O
case	O
,	O
we	O
are	O
not	O
very	O
conﬁdent	O
that	O
model	O
1	O
is	O
better	O
.	O
jeffreys	O
(	O
1961	O
)	O
proposed	O
a	O
scale	B
of	I
evidence	I
for	O
interpreting	O
the	O
magnitude	O
of	O
a	O
bayes	O
factor	B
,	O
which	O
is	O
shown	O
in	O
table	O
5.1.	O
this	O
is	O
a	O
bayesian	O
alternative	O
to	O
the	O
frequentist	B
concept	O
of	O
a	O
p-value.3	O
alternatively	O
,	O
we	O
can	O
just	O
convert	O
the	O
bayes	O
factor	B
to	O
a	O
posterior	O
over	O
models	O
.	O
if	O
p	O
(	O
m1	O
)	O
=	O
p	O
(	O
m0	O
)	O
=	O
0.5	O
,	O
we	O
have	O
p	O
(	O
m0|d	O
)	O
=	O
bf0,1	O
1	O
+	O
bf0,1	O
=	O
1	O
bf1,0	O
+	O
1	O
(	O
5.40	O
)	O
5.3.3.1	O
example	O
:	O
testing	O
if	O
a	O
coin	O
is	O
fair	O
suppose	O
we	O
observe	O
some	O
coin	O
tosses	O
,	O
and	O
want	O
to	O
decide	O
if	O
the	O
data	O
was	O
generated	O
by	O
a	O
fair	O
coin	O
,	O
θ	O
=	O
0.5	O
,	O
or	O
a	O
potentially	O
biased	O
coin	O
,	O
where	O
θ	O
could	O
be	O
any	O
value	O
in	O
[	O
0	O
,	O
1	O
]	O
.	O
let	O
us	O
denote	O
the	O
ﬁrst	O
model	O
by	O
m0	O
and	O
the	O
second	O
model	O
by	O
m1	O
.	O
the	O
marginal	B
likelihood	I
under	O
m0	O
is	O
simply	O
p	O
(	O
d|m0	O
)	O
=	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
n	O
1	O
2	O
3.	O
a	O
p-value	B
,	O
is	O
deﬁned	O
as	O
the	O
probability	O
(	O
under	O
the	O
null	B
hypothesis	I
)	O
of	O
observing	O
some	O
test	B
statistic	I
f	O
(	O
d	O
)	O
(	O
such	O
as	O
the	O
chi-squared	B
statistic	I
)	O
that	O
is	O
as	O
large	O
or	O
larger	O
than	O
that	O
actually	O
observed	O
,	O
i.e.	O
,	O
pvalue	O
(	O
d	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
f	O
(	O
˜d	O
)	O
≥	O
f	O
(	O
d	O
)	O
|	O
˜d	O
∼	O
h0	O
)	O
.	O
note	O
that	O
has	O
almost	O
nothing	O
to	O
do	O
with	O
what	O
we	O
really	O
want	O
to	O
know	O
,	O
which	O
is	O
p	O
(	O
h0|d	O
)	O
.	O
(	O
5.41	O
)	O
164	O
−0.4	O
−0.6	O
−0.8	O
−1	O
−1.2	O
−1.4	O
−1.6	O
−1.8	O
log10	O
p	O
(	O
d|m1	O
)	O
0	O
1	O
1	O
1	O
1	O
1	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
4	O
4	O
4	O
4	O
4	O
5	O
−2	O
−2.05	O
−2.1	O
−2.15	O
−2.2	O
−2.25	O
−2.3	O
−2.35	O
−2.4	O
−2.45	O
−2.5	O
chapter	O
5.	O
bayesian	O
statistics	O
bic	O
approximation	O
to	O
log10	O
p	O
(	O
d|m1	O
)	O
0	O
1	O
1	O
1	O
1	O
1	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
4	O
4	O
4	O
4	O
4	O
5	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
5.9	O
(	O
a	O
)	O
log	O
marginal	O
likelihood	B
for	O
the	O
coins	O
example	O
.	O
(	O
b	O
)	O
bic	O
approximation	O
.	O
figure	O
generated	O
by	O
coinsmodelseldemo	O
.	O
where	O
n	O
is	O
the	O
number	O
of	O
coin	O
tosses	O
.	O
the	O
marginal	B
likelihood	I
under	O
m1	O
,	O
using	O
a	O
beta	O
prior	O
,	O
is	O
(	O
cid:11	O
)	O
b	O
(	O
α1	O
,	O
α0	O
)	O
b	O
(	O
α1	O
+	O
n1	O
,	O
α0	O
+	O
n0	O
)	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
dθ	O
=	O
p	O
(	O
d|m1	O
)	O
=	O
(	O
5.42	O
)	O
we	O
plot	O
log	O
p	O
(	O
d|m1	O
)	O
vs	O
the	O
number	O
of	O
heads	O
n1	O
in	O
figure	O
5.9	O
(	O
a	O
)	O
,	O
assuming	O
n	O
=	O
5	O
and	O
α1	O
=	O
α0	O
=	O
1	O
.	O
(	O
the	O
shape	O
of	O
the	O
curve	O
is	O
not	O
very	O
sensitive	O
to	O
α1	O
and	O
α0	O
,	O
as	O
long	O
as	O
α0	O
=	O
α1	O
.	O
)	O
if	O
we	O
observe	O
2	O
or	O
3	O
heads	O
,	O
the	O
unbiased	B
coin	O
hypothesis	O
m0	O
is	O
more	O
likely	O
than	O
m1	O
,	O
since	O
m0	O
is	O
a	O
simpler	O
model	O
(	O
it	O
has	O
no	O
free	O
parameters	O
)	O
—	O
it	O
would	O
be	O
a	O
suspicious	B
coincidence	I
if	O
the	O
coin	O
were	O
biased	O
but	O
happened	O
to	O
produce	O
almost	O
exactly	O
50/50	O
heads/tails	O
.	O
however	O
,	O
as	O
the	O
counts	O
become	O
more	O
extreme	O
,	O
we	O
favor	O
the	O
biased	O
coin	O
hypothesis	O
.	O
note	O
that	O
,	O
if	O
we	O
plot	O
the	O
log	O
bayes	O
factor	B
,	O
log	O
bf1,0	O
,	O
it	O
will	O
have	O
exactly	O
the	O
same	O
shape	O
,	O
since	O
log	O
p	O
(	O
d|m0	O
)	O
is	O
a	O
constant	O
.	O
see	O
also	O
exercise	O
3.18.	O
in	O
figure	O
5.9	O
(	O
b	O
)	O
shows	O
the	O
bic	O
approximation	O
to	O
log	O
p	O
(	O
d|m1	O
)	O
for	O
our	O
biased	O
coin	O
example	O
from	O
section	O
5.3.3.1.	O
we	O
see	O
that	O
the	O
curve	O
has	O
approximately	O
the	O
same	O
shape	O
as	O
the	O
exact	O
log	O
marginal	O
likelihood	B
,	O
which	O
is	O
all	O
that	O
matters	O
for	O
model	B
selection	I
purposes	O
,	O
since	O
the	O
absolute	O
scale	O
is	O
irrelevant	O
.	O
in	O
particular	O
,	O
it	O
favors	O
the	O
simpler	O
model	O
unless	O
the	O
data	O
is	O
overwhelmingly	O
in	O
support	B
of	O
the	O
more	O
complex	O
model	O
.	O
5.3.4	O
jeffreys-lindley	O
paradox	O
*	O
problems	O
can	O
arise	O
when	O
we	O
use	O
improper	O
priors	O
(	O
i.e.	O
,	O
priors	O
that	O
do	O
not	O
integrate	O
to	O
1	O
)	O
for	O
model	O
selection/	O
hypothesis	O
testing	O
,	O
even	O
though	O
such	O
priors	O
may	O
be	O
acceptable	O
for	O
other	O
purposes	O
.	O
for	O
example	O
,	O
consider	O
testing	O
the	O
hypotheses	O
m0	O
:	O
θ	O
∈	O
θ0	O
vs	O
m1	O
:	O
θ	O
∈	O
θ1	O
.	O
to	O
deﬁne	O
the	O
marginal	O
density	O
on	O
θ	O
,	O
we	O
use	O
the	O
following	O
mixture	B
model	I
θ|m0	O
)	O
p	O
(	O
m0	O
)	O
+p	O
(	O
θ|m1	O
)	O
p	O
(	O
m1	O
)	O
p	O
(	O
θ	O
)	O
=p	O
(	O
(	O
5.43	O
)	O
5.4.	O
priors	O
165	O
this	O
is	O
only	O
meaningful	O
if	O
p	O
(	O
θ|m0	O
)	O
and	O
p	O
(	O
θ|m1	O
)	O
are	O
proper	O
(	O
normalized	O
)	O
density	O
functions	O
.	O
in	O
this	O
case	O
,	O
the	O
posterior	O
is	O
given	O
by	O
p	O
(	O
m0|d	O
)	O
=	O
p	O
(	O
m0	O
)	O
p	O
(	O
d|m0	O
)	O
+p	O
(	O
m1	O
)	O
p	O
(	O
d|m1	O
)	O
p	O
(	O
m0	O
)	O
p	O
(	O
d|m0	O
)	O
(	O
cid:22	O
)	O
=	O
p	O
(	O
m0	O
)	O
(	O
cid:22	O
)	O
θ0	O
p	O
(	O
m0	O
)	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ|m0	O
)	O
dθ	O
+	O
p	O
(	O
m1	O
)	O
θ0	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ|m0	O
)	O
dθ	O
(	O
cid:22	O
)	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ|m1	O
)	O
dθ	O
θ1	O
now	O
suppose	O
we	O
use	O
improper	O
priors	O
,	O
p	O
(	O
θ|m0	O
)	O
∝	O
c0	O
and	O
p	O
(	O
θ|m1	O
)	O
∝	O
c1	O
.	O
then	O
p	O
(	O
m0|d	O
)	O
=	O
(	O
cid:22	O
)	O
θ0	O
p	O
(	O
m0	O
)	O
c0	O
p	O
(	O
d|θ	O
)	O
dθ	O
(	O
cid:22	O
)	O
p	O
(	O
m0	O
)	O
c0	O
p	O
(	O
d|θ	O
)	O
dθ	O
+	O
p	O
(	O
m1	O
)	O
c1	O
θ0	O
p	O
(	O
d|θ	O
)	O
dθ	O
θ1	O
(	O
cid:22	O
)	O
(	O
5.44	O
)	O
(	O
5.45	O
)	O
(	O
5.46	O
)	O
p	O
(	O
m0	O
)	O
c0	O
(	O
cid:9	O
)	O
0	O
p	O
(	O
m0	O
)	O
c0	O
(	O
cid:9	O
)	O
0	O
+	O
p	O
(	O
m1	O
)	O
c1	O
(	O
cid:9	O
)	O
1	O
(	O
5.47	O
)	O
=	O
p	O
(	O
d|θ	O
)	O
dθ	O
is	O
the	O
integrated	O
or	O
marginal	B
likelihood	I
for	O
model	O
i.	O
now	O
let	O
p	O
(	O
m0	O
)	O
=	O
(	O
cid:22	O
)	O
θi	O
where	O
(	O
cid:9	O
)	O
i	O
=	O
p	O
(	O
m1	O
)	O
=	O
1	O
2	O
.	O
hence	O
p	O
(	O
m0|d	O
)	O
=	O
c0	O
(	O
cid:9	O
)	O
0	O
c0	O
(	O
cid:9	O
)	O
0	O
+	O
c1	O
(	O
cid:9	O
)	O
1	O
=	O
(	O
cid:9	O
)	O
0	O
(	O
cid:9	O
)	O
0	O
+	O
(	O
c1/c0	O
)	O
(	O
cid:9	O
)	O
1	O
(	O
5.48	O
)	O
thus	O
we	O
can	O
change	O
the	O
posterior	O
arbitrarily	O
by	O
choosing	O
c1	O
and	O
c0	O
as	O
we	O
please	O
.	O
note	O
that	O
using	O
proper	O
,	O
but	O
very	O
vague	O
,	O
priors	O
can	O
cause	O
similar	B
problems	O
.	O
in	O
particular	O
,	O
the	O
bayes	O
factor	B
will	O
always	O
favor	O
the	O
simpler	O
model	O
,	O
since	O
the	O
probability	O
of	O
the	O
observed	O
data	O
under	O
a	O
complex	O
model	O
with	O
a	O
very	O
diffuse	O
prior	O
will	O
be	O
very	O
small	O
.	O
this	O
is	O
called	O
the	O
jeffreys-lindley	O
paradox	O
.	O
thus	O
it	O
is	O
important	O
to	O
use	O
proper	O
priors	O
when	O
performing	O
model	B
selection	I
.	O
note	O
,	O
however	O
,	O
that	O
,	O
if	O
m0	O
and	O
m1	O
share	O
the	O
same	O
prior	O
over	O
a	O
subset	O
of	O
the	O
parameters	O
,	O
this	O
part	O
of	O
the	O
prior	O
can	O
be	O
improper	O
,	O
since	O
the	O
corresponding	O
normalization	O
constant	O
will	O
cancel	O
out	O
.	O
5.4	O
priors	O
the	O
most	O
controversial	O
aspect	O
of	O
bayesian	O
statistics	O
is	O
its	O
reliance	O
on	O
priors	O
.	O
bayesians	O
argue	O
this	O
is	O
unavoidable	O
,	O
since	O
nobody	O
is	O
a	O
tabula	B
rasa	I
or	O
blank	B
slate	I
:	O
all	O
inference	O
must	O
be	O
done	O
conditional	O
on	O
certain	O
assumptions	O
about	O
the	O
world	O
.	O
nevertheless	O
,	O
one	O
might	O
be	O
interested	O
in	O
minimizing	O
the	O
impact	O
of	O
one	O
’	O
s	O
prior	O
assumptions	O
.	O
we	O
brieﬂy	O
discuss	O
some	O
ways	O
to	O
do	O
this	O
below	O
.	O
5.4.1	O
uninformative	B
priors	O
if	O
we	O
don	O
’	O
t	O
have	O
strong	O
beliefs	O
about	O
what	O
θ	O
should	O
be	O
,	O
it	O
is	O
common	O
to	O
use	O
an	O
uninformative	B
or	O
non-informative	B
prior	O
,	O
and	O
to	O
“	O
let	O
the	O
data	O
speak	O
for	O
itself	O
”	O
.	O
the	O
issue	O
of	O
designing	O
uninformative	B
priors	O
is	O
actually	O
somewhat	O
tricky	O
.	O
as	O
an	O
example	O
of	O
the	O
difficulty	O
,	O
consider	O
a	O
bernoulli	O
parameter	B
,	O
θ	O
∈	O
[	O
0	O
,	O
1	O
]	O
.	O
one	O
might	O
think	O
that	O
the	O
most	O
uninformative	B
prior	O
would	O
be	O
the	O
uniform	B
distribution	I
,	O
beta	O
(	O
1	O
,	O
1	O
)	O
.	O
but	O
the	O
posterior	B
mean	I
in	O
this	O
case	O
is	O
e	O
[	O
θ|d	O
]	O
=	O
n1+1	O
n1+n0	O
.	O
hence	O
one	O
could	O
argue	O
that	O
the	O
prior	O
wasn	O
’	O
t	O
completely	O
uninformative	B
after	O
all	O
.	O
n1+n0+2	O
,	O
whereas	O
the	O
mle	O
is	O
n1	O
166	O
chapter	O
5.	O
bayesian	O
statistics	O
clearly	O
by	O
decreasing	O
the	O
magnitude	O
of	O
the	O
pseudo	B
counts	I
,	O
we	O
can	O
lessen	O
the	O
impact	O
of	O
the	O
prior	O
.	O
by	O
the	O
above	O
argument	O
,	O
the	O
most	O
non-informative	B
prior	O
is	O
beta	O
(	O
c	O
,	O
c	O
)	O
=	O
beta	O
(	O
0	O
,	O
0	O
)	O
lim	O
c→0	O
(	O
5.49	O
)	O
which	O
is	O
a	O
mixture	O
of	O
two	O
equal	O
point	O
masses	O
at	O
0	O
and	O
1	O
(	O
see	O
(	O
zhu	O
and	O
lu	O
2004	O
)	O
)	O
.	O
this	O
is	O
also	O
called	O
the	O
haldane	O
prior	O
.	O
note	O
that	O
the	O
haldane	O
prior	O
is	O
an	O
improper	B
prior	I
,	O
meaning	O
it	O
does	O
not	O
integrate	O
to	O
1.	O
however	O
,	O
as	O
long	O
as	O
we	O
see	O
at	O
least	O
one	O
head	O
and	O
at	O
least	O
one	O
tail	O
,	O
the	O
posterior	O
will	O
be	O
proper	O
.	O
in	O
section	O
5.4.2.1	O
we	O
will	O
argue	O
that	O
the	O
“	O
right	O
”	O
uninformative	B
prior	O
is	O
in	O
fact	O
beta	O
(	O
1	O
2	O
,	O
1	O
2	O
)	O
.	O
clearly	O
the	O
difference	O
in	O
practice	O
between	O
these	O
three	O
priors	O
is	O
very	O
likely	O
negligible	O
.	O
in	O
general	O
,	O
it	O
is	O
advisable	O
to	O
perform	O
some	O
kind	O
of	O
sensitivity	B
analysis	I
,	O
in	O
which	O
one	O
checks	O
how	O
much	O
one	O
’	O
s	O
conclusions	O
or	O
predictions	O
change	O
in	O
response	O
to	O
change	O
in	O
the	O
modeling	O
assumptions	O
,	O
which	O
includes	O
the	O
choice	O
of	O
prior	O
,	O
but	O
also	O
the	O
choice	O
of	O
likelihood	B
and	O
any	O
kind	O
of	O
data	O
pre-	O
processing	O
.	O
if	O
the	O
conclusions	O
are	O
relatively	O
insensitive	O
to	O
the	O
modeling	O
assumptions	O
,	O
one	O
can	O
have	O
more	O
conﬁdence	O
in	O
the	O
results	O
.	O
5.4.2	O
jeffreys	O
priors	O
*	O
harold	O
jeffreys4	O
designed	O
a	O
general	O
purpose	O
technique	O
for	O
creating	O
non-informative	B
priors	O
.	O
the	O
result	O
is	O
known	O
as	O
the	O
jeffreys	O
prior	O
.	O
the	O
key	O
observation	B
is	O
that	O
if	O
p	O
(	O
φ	O
)	O
is	O
non-informative	B
,	O
then	O
any	O
re-parameterization	O
of	O
the	O
prior	O
,	O
such	O
as	O
θ	O
=	O
h	O
(	O
φ	O
)	O
for	O
some	O
function	O
h	O
,	O
should	O
also	O
be	O
non-informative	B
.	O
now	O
,	O
by	O
the	O
change	B
of	I
variables	I
formula	O
,	O
pθ	O
(	O
θ	O
)	O
=	O
pφ	O
(	O
φ	O
)	O
--	O
dφ	O
--	O
dθ	O
so	O
the	O
prior	O
will	O
in	O
general	O
change	O
.	O
however	O
,	O
let	O
us	O
pick	O
1	O
2	O
pφ	O
(	O
φ	O
)	O
∝	O
(	O
i	O
(	O
φ	O
)	O
)	O
(	O
cid:2	O
)	O
(	O
cid:7	O
)	O
i	O
(	O
φ	O
)	O
(	O
cid:2	O
)	O
−e	O
(	O
cid:3	O
)	O
where	O
i	O
(	O
φ	O
)	O
is	O
the	O
fisher	O
information	B
:	O
(	O
cid:8	O
)	O
d	O
log	O
p	O
(	O
x|φ	O
)	O
2	O
dφ	O
(	O
5.50	O
)	O
(	O
5.51	O
)	O
(	O
5.52	O
)	O
this	O
is	O
a	O
measure	O
of	O
curvature	O
of	O
the	O
expected	O
negative	O
log	O
likelihood	O
and	O
hence	O
a	O
measure	O
of	O
stability	O
of	O
the	O
mle	O
(	O
see	O
section	O
6.2.2	O
)	O
.	O
now	O
d	O
log	O
p	O
(	O
x|θ	O
)	O
dθ	O
d	O
log	O
p	O
(	O
x|φ	O
)	O
dφ	O
dφ	O
dθ	O
=	O
(	O
cid:7	O
)	O
squaring	O
and	O
taking	O
expectations	O
over	O
x	O
,	O
we	O
have	O
(	O
cid:17	O
)	O
(	O
cid:8	O
)	O
2	O
=	O
i	O
(	O
φ	O
)	O
i	O
(	O
θ	O
)	O
=−e	O
1	O
i	O
(	O
θ	O
)	O
2	O
=	O
i	O
(	O
φ	O
)	O
(	O
cid:16	O
)	O
(	O
cid:7	O
)	O
d	O
log	O
p	O
(	O
x|θ	O
)	O
--	O
--	O
dφ	O
dθ	O
1	O
2	O
dθ	O
(	O
cid:8	O
)	O
2	O
dφ	O
dθ	O
(	O
5.53	O
)	O
(	O
5.54	O
)	O
(	O
5.55	O
)	O
4.	O
harold	O
jeffreys	O
,	O
1891	O
–	O
1989	O
,	O
was	O
an	O
english	O
mathematician	O
,	O
statistician	O
,	O
geophysicist	O
,	O
and	O
astronomer	O
.	O
5.4.	O
priors	O
so	O
we	O
ﬁnd	O
the	O
transformed	O
prior	O
is	O
pθ	O
(	O
θ	O
)	O
=p	O
φ	O
(	O
φ	O
)	O
--	O
dφ	O
dθ	O
--	O
∝	O
(	O
i	O
(	O
φ	O
)	O
)	O
--	O
dφ	O
dθ	O
1	O
2	O
--	O
=	O
i	O
(	O
θ	O
)	O
1	O
2	O
so	O
pθ	O
(	O
θ	O
)	O
and	O
pφ	O
(	O
φ	O
)	O
are	O
the	O
same	O
.	O
some	O
examples	O
will	O
make	O
this	O
clearer	O
.	O
5.4.2.1	O
example	O
:	O
jeffreys	O
prior	O
for	O
the	O
bernoulli	O
and	O
multinoulli	O
suppose	O
x	O
∼	O
ber	O
(	O
θ	O
)	O
.	O
the	O
log	O
likelihood	O
for	O
a	O
single	O
sample	O
is	O
log	O
p	O
(	O
x|θ	O
)	O
=	O
x	O
log	O
θ	O
+	O
(	O
1−	O
x	O
)	O
log	O
(	O
1	O
−	O
θ	O
)	O
the	O
score	B
function	I
is	O
just	O
the	O
gradient	O
of	O
the	O
log-likelihood	O
:	O
s	O
(	O
θ	O
)	O
(	O
cid:2	O
)	O
d	O
dθ	O
log	O
p	O
(	O
x|θ	O
)	O
=	O
x	O
θ	O
−	O
1	O
−	O
x	O
1	O
−	O
θ	O
the	O
observed	B
information	I
is	O
the	O
second	O
derivative	O
of	O
the	O
log-likelihood	O
:	O
j	O
(	O
θ	O
)	O
=−	O
d2	O
dθ2	O
log	O
p	O
(	O
x|θ	O
)	O
=	O
−s	O
(	O
cid:2	O
)	O
(	O
θ|x	O
)	O
=	O
x	O
θ2	O
+	O
1	O
−	O
x	O
(	O
1	O
−	O
θ	O
)	O
2	O
the	O
fisher	O
information	B
is	O
the	O
expected	O
information	O
:	O
1	O
−	O
θ	O
(	O
1	O
−	O
θ	O
)	O
2	O
=	O
i	O
(	O
θ	O
)	O
=e	O
[	O
j	O
(	O
θ|x	O
)	O
|x	O
∼	O
θ	O
]	O
=	O
θ	O
θ2	O
+	O
1	O
θ	O
(	O
1	O
−	O
θ	O
)	O
hence	O
jeffreys	O
’	O
prior	O
is	O
2	O
(	O
1	O
−	O
θ	O
)	O
p	O
(	O
θ	O
)	O
∝	O
θ−	O
1	O
−	O
1	O
2	O
=	O
1	O
(	O
cid:9	O
)	O
θ	O
(	O
1	O
−	O
θ	O
)	O
∝	O
beta	O
(	O
1	O
2	O
,	O
1	O
2	O
)	O
167	O
(	O
5.56	O
)	O
(	O
5.57	O
)	O
(	O
5.58	O
)	O
(	O
5.59	O
)	O
(	O
5.60	O
)	O
(	O
5.61	O
)	O
now	O
consider	O
a	O
multinoulli	O
random	O
variable	O
with	O
k	O
states	O
.	O
one	O
can	O
show	O
that	O
the	O
jeffreys	O
’	O
prior	O
is	O
given	O
by	O
p	O
(	O
θ	O
)	O
∝	O
dir	O
(	O
1	O
2	O
,	O
.	O
.	O
.	O
,	O
1	O
2	O
)	O
(	O
5.62	O
)	O
note	O
that	O
this	O
is	O
different	O
from	O
the	O
more	O
obvious	O
choices	O
of	O
dir	O
(	O
1	O
k	O
,	O
.	O
.	O
.	O
,	O
1	O
k	O
)	O
or	O
dir	O
(	O
1	O
,	O
.	O
.	O
.	O
,	O
1	O
)	O
.	O
5.4.2.2	O
example	O
:	O
jeffreys	O
prior	O
for	O
location	O
and	O
scale	O
parameters	O
one	O
can	O
show	O
that	O
the	O
jeffreys	O
prior	O
for	O
a	O
location	O
parameter	B
,	O
such	O
as	O
the	O
gaussian	O
mean	B
,	O
is	O
p	O
(	O
μ	O
)	O
∝	O
1.	O
thus	O
is	O
an	O
example	O
of	O
a	O
translation	B
invariant	I
prior	I
,	O
which	O
satisﬁes	O
the	O
property	O
that	O
the	O
probability	O
mass	O
assigned	O
to	O
any	O
interval	O
,	O
[	O
a	O
,	O
b	O
]	O
is	O
the	O
same	O
as	O
that	O
assigned	O
to	O
any	O
other	O
shifted	O
interval	O
of	O
the	O
same	O
width	O
,	O
such	O
as	O
[	O
a	O
−	O
c	O
,	O
b	O
−	O
c	O
]	O
.	O
that	O
is	O
,	O
(	O
cid:11	O
)	O
b−c	O
(	O
cid:11	O
)	O
b	O
p	O
(	O
μ	O
)	O
dμ	O
=	O
(	O
a	O
−	O
c	O
)	O
−	O
(	O
b	O
−	O
c	O
)	O
=	O
(	O
a	O
−	O
b	O
)	O
=	O
p	O
(	O
μ	O
)	O
dμ	O
a	O
(	O
5.63	O
)	O
a−c	O
168	O
chapter	O
5.	O
bayesian	O
statistics	O
this	O
can	O
be	O
achieved	O
using	O
p	O
(	O
μ	O
)	O
∝	O
1	O
,	O
which	O
we	O
can	O
approximate	O
by	O
using	O
a	O
gaussian	O
with	O
inﬁnite	O
variance	O
,	O
p	O
(	O
μ	O
)	O
=n	O
(	O
μ|0	O
,	O
∞	O
)	O
.	O
note	O
that	O
this	O
is	O
an	O
improper	B
prior	I
,	O
since	O
it	O
does	O
not	O
integrate	O
to	O
1.	O
using	O
improper	O
priors	O
is	O
ﬁne	O
as	O
long	O
as	O
the	O
posterior	O
is	O
proper	O
,	O
which	O
will	O
be	O
the	O
case	O
provided	O
we	O
have	O
seen	O
n	O
≥	O
1	O
data	O
points	O
,	O
since	O
we	O
can	O
“	O
nail	O
down	O
”	O
the	O
location	O
as	O
soon	O
as	O
we	O
have	O
seen	O
a	O
single	O
data	O
point	O
.	O
similarly	O
,	O
one	O
can	O
show	O
that	O
the	O
jeffreys	O
prior	O
for	O
a	O
scale	O
parameter	O
,	O
such	O
as	O
the	O
gaussian	O
variance	B
,	O
is	O
p	O
(	O
σ2	O
)	O
∝	O
1/σ2	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
scale	B
invariant	I
prior	I
,	O
which	O
satisﬁes	O
the	O
property	O
that	O
the	O
probability	O
mass	O
assigned	O
to	O
any	O
interval	O
[	O
a	O
,	O
b	O
]	O
is	O
the	O
same	O
as	O
that	O
assigned	O
to	O
any	O
other	O
interval	O
[	O
a/c	O
,	O
b/c	O
]	O
which	O
is	O
scaled	O
in	O
size	O
by	O
some	O
constant	O
factor	O
c	O
>	O
0	O
.	O
(	O
for	O
example	O
,	O
if	O
we	O
change	O
units	O
from	O
meters	O
to	O
feet	O
we	O
do	O
not	O
want	O
that	O
to	O
affect	O
our	O
inferences	O
.	O
)	O
this	O
can	O
be	O
achieved	O
by	O
using	O
(	O
5.64	O
)	O
(	O
5.65	O
)	O
p	O
(	O
s	O
)	O
∝	O
1/s	O
(	O
cid:11	O
)	O
b/c	O
to	O
see	O
this	O
,	O
note	O
that	O
p	O
(	O
s	O
)	O
ds	O
=	O
[	O
log	O
s	O
]	O
a/c	O
b/c	O
a/c	O
=	O
log	O
(	O
b/c	O
)	O
−	O
log	O
(	O
a/c	O
)	O
(	O
cid:11	O
)	O
b	O
=	O
log	O
(	O
b	O
)	O
−	O
log	O
(	O
a	O
)	O
=	O
p	O
(	O
s	O
)	O
ds	O
(	O
5.66	O
)	O
we	O
can	O
approximate	O
this	O
using	O
a	O
degenerate	B
gamma	O
distribution	O
(	O
section	O
2.4.4	O
)	O
,	O
p	O
(	O
s	O
)	O
=	O
ga	O
(	O
s|0	O
,	O
0	O
)	O
.	O
the	O
prior	O
p	O
(	O
s	O
)	O
∝	O
1/s	O
is	O
also	O
improper	O
,	O
but	O
the	O
posterior	O
is	O
proper	O
as	O
soon	O
as	O
we	O
have	O
seen	O
n	O
≥	O
2	O
data	O
points	O
(	O
since	O
we	O
need	O
at	O
least	O
two	O
data	O
points	O
to	O
estimate	O
a	O
variance	B
)	O
.	O
a	O
5.4.3	O
robust	B
priors	I
in	O
many	O
cases	O
,	O
we	O
are	O
not	O
very	O
conﬁdent	O
in	O
our	O
prior	O
,	O
so	O
we	O
want	O
to	O
make	O
sure	O
it	O
does	O
not	O
have	O
an	O
undue	O
inﬂuence	O
on	O
the	O
result	O
.	O
this	O
can	O
be	O
done	O
by	O
using	O
robust	B
priors	I
(	O
insua	O
and	O
ruggeri	O
2000	O
)	O
,	O
which	O
typically	O
have	O
heavy	B
tails	I
,	O
which	O
avoids	O
forcing	O
things	O
to	O
be	O
too	O
close	O
to	O
the	O
prior	O
mean	B
.	O
let	O
us	O
consider	O
an	O
example	O
from	O
(	O
berger	O
1985	O
,	O
p7	O
)	O
.	O
suppose	O
x	O
∼	O
n	O
(	O
θ	O
,	O
1	O
)	O
.	O
we	O
observe	O
that	O
x	O
=	O
5	O
and	O
we	O
want	O
to	O
estimate	O
θ.	O
the	O
mle	O
is	O
of	O
course	O
ˆθ	O
=	O
5	O
,	O
which	O
seems	O
reasonable	O
.	O
the	O
posterior	B
mean	I
under	O
a	O
uniform	O
prior	O
is	O
also	O
θ	O
=	O
5.	O
but	O
now	O
suppose	O
we	O
know	O
that	O
the	O
prior	O
median	B
is	O
0	O
,	O
and	O
the	O
prior	O
quantiles	O
are	O
at	O
-1	O
and	O
1	O
,	O
so	O
p	O
(	O
θ	O
≤	O
−1	O
)	O
=	O
p	O
(	O
−1	O
<	O
θ	O
≤	O
0	O
)	O
=	O
p	O
(	O
0	O
<	O
θ	O
≤	O
1	O
)	O
=	O
p	O
(	O
1	O
<	O
θ	O
)	O
=	O
0.25.	O
let	O
us	O
also	O
assume	O
the	O
prior	O
is	O
smooth	O
and	O
unimodal	O
.	O
it	O
is	O
easy	O
to	O
show	O
that	O
a	O
gaussian	O
prior	O
of	O
the	O
form	O
n	O
(	O
θ|0	O
,	O
2.192	O
)	O
satisﬁes	O
these	O
prior	O
constraints	O
.	O
but	O
in	O
this	O
case	O
the	O
posterior	B
mean	I
is	O
given	O
by	O
3.43	O
,	O
which	O
doesn	O
’	O
t	O
seem	O
very	O
satisfactory	O
.	O
now	O
suppose	O
we	O
use	O
as	O
a	O
cauchy	O
prior	O
t	O
(	O
θ|0	O
,	O
1	O
,	O
1	O
)	O
.	O
this	O
also	O
satisﬁes	O
the	O
prior	O
constraints	O
of	O
our	O
example	O
.	O
but	O
this	O
time	O
we	O
ﬁnd	O
(	O
using	O
numerical	O
method	O
integration	O
:	O
see	O
robustpriordemo	O
for	O
the	O
code	O
)	O
that	O
the	O
posterior	B
mean	I
is	O
about	O
4.6	O
,	O
which	O
seems	O
much	O
more	O
reasonable	O
.	O
5.4.4	O
mixtures	O
of	O
conjugate	B
priors	I
robust	O
priors	O
are	O
useful	O
,	O
but	O
can	O
be	O
computationally	O
expensive	O
to	O
use	O
.	O
conjugate	B
priors	I
simplify	O
the	O
computation	O
,	O
but	O
are	O
often	O
not	O
robust	O
,	O
and	O
not	O
ﬂexible	O
enough	O
to	O
encode	O
our	O
prior	O
knowl-	O
5.4.	O
priors	O
169	O
edge	O
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
a	O
mixture	B
of	I
conjugate	I
priors	I
is	O
also	O
conjugate	O
(	O
exercise	O
5.1	O
)	O
,	O
and	O
can	O
approximate	O
any	O
kind	O
of	O
prior	O
(	O
dallal	O
and	O
hall	O
1983	O
;	O
diaconis	O
and	O
ylvisaker	O
1985	O
)	O
.	O
thus	O
such	O
priors	O
provide	O
a	O
good	O
compromise	O
between	O
computational	O
convenience	O
and	O
ﬂexibility	O
.	O
for	O
example	O
,	O
suppose	O
we	O
are	O
modeling	O
coin	O
tosses	O
,	O
and	O
we	O
think	O
the	O
coin	O
is	O
either	O
fair	O
,	O
or	O
is	O
biased	O
towards	O
heads	O
.	O
this	O
can	O
not	O
be	O
represented	O
by	O
a	O
beta	B
distribution	I
.	O
however	O
,	O
we	O
can	O
model	O
it	O
using	O
a	O
mixture	O
of	O
two	O
beta	O
distributions	O
.	O
for	O
example	O
,	O
we	O
might	O
use	O
p	O
(	O
θ	O
)	O
=	O
0.5	O
beta	O
(	O
θ|20	O
,	O
20	O
)	O
+	O
0.5	O
beta	O
(	O
θ|30	O
,	O
10	O
)	O
(	O
5.67	O
)	O
if	O
θ	O
comes	O
from	O
the	O
ﬁrst	O
distribution	O
,	O
the	O
coin	O
is	O
fair	O
,	O
but	O
if	O
it	O
comes	O
from	O
the	O
second	O
,	O
it	O
is	O
biased	O
towards	O
heads	O
.	O
we	O
can	O
represent	O
a	O
mixture	B
by	O
introducing	O
a	O
latent	B
indicator	O
variable	O
z	O
,	O
where	O
z	O
=	O
k	O
means	O
that	O
θ	O
comes	O
from	O
mixture	B
component	O
k.	O
the	O
prior	O
has	O
the	O
form	O
p	O
(	O
θ	O
)	O
=	O
p	O
(	O
z	O
=	O
k	O
)	O
p	O
(	O
θ|z	O
=	O
k	O
)	O
(	O
5.68	O
)	O
(	O
cid:6	O
)	O
k	O
where	O
each	O
p	O
(	O
θ|z	O
=	O
k	O
)	O
is	O
conjugate	O
,	O
and	O
p	O
(	O
z	O
=	O
k	O
)	O
are	O
called	O
the	O
(	O
prior	O
)	O
mixing	B
weights	I
.	O
one	O
can	O
show	O
(	O
exercise	O
5.1	O
)	O
that	O
the	O
posterior	O
can	O
also	O
be	O
written	O
as	O
a	O
mixture	O
of	O
conjugate	O
distributions	O
as	O
follows	O
:	O
p	O
(	O
θ|d	O
)	O
=	O
p	O
(	O
z	O
=	O
k|d	O
)	O
p	O
(	O
θ|d	O
,	O
z	O
=	O
k	O
)	O
(	O
5.69	O
)	O
(	O
cid:6	O
)	O
k	O
where	O
p	O
(	O
z	O
=	O
k|d	O
)	O
are	O
the	O
posterior	O
mixing	O
weights	O
given	O
by	O
p	O
(	O
z	O
=	O
k|d	O
)	O
=	O
(	O
cid:4	O
)	O
p	O
(	O
z	O
=	O
k	O
)	O
p	O
(	O
d|z	O
=	O
k	O
)	O
k	O
(	O
cid:2	O
)	O
p	O
(	O
z	O
=	O
k	O
(	O
cid:2	O
)	O
)	O
p	O
(	O
d|z	O
=	O
k	O
(	O
cid:2	O
)	O
)	O
(	O
5.70	O
)	O
here	O
the	O
quantity	O
p	O
(	O
d|z	O
=	O
k	O
)	O
is	O
the	O
marginal	B
likelihood	I
for	O
mixture	B
component	O
k	O
(	O
see	O
sec-	O
tion	O
5.3.2.1	O
)	O
.	O
5.4.4.1	O
example	O
suppose	O
we	O
use	O
the	O
mixture	B
prior	O
p	O
(	O
θ	O
)	O
=	O
0.5beta	O
(	O
θ|a1	O
,	O
b1	O
)	O
+	O
0.5beta	O
(	O
θ|a2	O
,	O
b2	O
)	O
(	O
5.71	O
)	O
where	O
a1	O
=	O
b1	O
=	O
20	O
and	O
a2	O
=	O
b2	O
=	O
10.	O
and	O
we	O
observe	O
n1	O
heads	O
and	O
n0	O
tails	O
.	O
the	O
posterior	O
becomes	O
p	O
(	O
θ|d	O
)	O
=	O
p	O
(	O
z	O
=	O
1|d	O
)	O
beta	O
(	O
θ|a1	O
+	O
n1	O
,	O
b1	O
+	O
n0	O
)	O
+p	O
(	O
z	O
=	O
2|d	O
)	O
beta	O
(	O
θ|a2	O
+	O
n1	O
,	O
b2	O
+	O
n0	O
)	O
(	O
5.72	O
)	O
if	O
n1	O
=	O
20	O
heads	O
and	O
n0	O
=	O
10	O
tails	O
,	O
then	O
,	O
using	O
equation	O
5.23	O
,	O
the	O
posterior	O
becomes	O
p	O
(	O
θ|d	O
)	O
=	O
0.346	O
beta	O
(	O
θ|40	O
,	O
30	O
)	O
+	O
0.654	O
beta	O
(	O
θ|50	O
,	O
20	O
)	O
(	O
5.73	O
)	O
see	O
figure	O
5.10	O
for	O
an	O
illustration	O
.	O
170	O
chapter	O
5.	O
bayesian	O
statistics	O
5	O
4.5	O
4	O
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
0	O
mixture	O
of	O
beta	O
distributions	O
prior	O
posterior	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1	O
figure	O
5.10	O
a	O
mixture	O
of	O
two	O
beta	O
distributions	O
.	O
figure	O
generated	O
by	O
mixbetademo	O
.	O
5.4.4.2	O
application	O
:	O
finding	O
conserved	O
regions	O
in	O
dna	O
and	O
protein	O
sequences	O
we	O
mentioned	O
that	O
dirichlet-multinomial	O
models	O
are	O
widely	O
used	O
in	O
biosequence	B
analysis	I
.	O
let	O
us	O
give	O
a	O
simple	O
example	O
to	O
illustrate	O
some	O
of	O
the	O
machinery	O
that	O
has	O
developed	O
.	O
speciﬁcally	O
,	O
consider	O
the	O
sequence	B
logo	I
discussed	O
in	O
section	O
2.3.2.1.	O
now	O
suppose	O
we	O
want	O
to	O
ﬁnd	O
locations	O
which	O
represent	O
coding	O
regions	O
of	O
the	O
genome	B
.	O
such	O
locations	O
often	O
have	O
the	O
same	O
letter	O
across	O
all	O
sequences	O
,	O
because	O
of	O
evolutionary	O
pressure	O
.	O
so	O
we	O
need	O
to	O
ﬁnd	O
columns	O
which	O
are	O
“	O
pure	B
”	O
,	O
or	O
nearly	O
so	O
,	O
in	O
the	O
sense	O
that	O
they	O
are	O
mostly	O
all	O
as	O
,	O
mostly	O
all	O
ts	O
,	O
mostly	O
all	O
cs	O
,	O
or	O
mostly	O
all	O
gs	O
.	O
one	O
approach	O
is	O
to	O
look	O
for	O
low-entropy	O
columns	O
;	O
these	O
will	O
be	O
ones	O
whose	O
distribution	O
is	O
nearly	O
deterministic	O
(	O
pure	B
)	O
.	O
but	O
suppose	O
we	O
want	O
to	O
associate	O
a	O
conﬁdence	O
measure	O
with	O
our	O
estimates	O
of	O
purity	B
.	O
this	O
in	O
this	O
case	O
,	O
we	O
can	O
let	O
can	O
be	O
useful	O
if	O
we	O
believe	O
adjacent	O
locations	O
are	O
conserved	O
together	O
.	O
z1	O
=	O
1	O
if	O
location	O
t	O
is	O
conserved	O
,	O
and	O
let	O
zt	O
=	O
0	O
otherwise	O
.	O
we	O
can	O
then	O
add	O
a	O
dependence	O
between	O
adjacent	O
zt	O
variables	O
using	O
a	O
markov	O
chain	O
;	O
see	O
chapter	O
17	O
for	O
details	O
.	O
in	O
any	O
case	O
,	O
we	O
need	O
to	O
deﬁne	O
a	O
likelihood	B
model	O
,	O
p	O
(	O
nt|zt	O
)	O
,	O
where	O
nt	O
is	O
the	O
vector	O
of	O
(	O
a	O
,	O
c	O
,	O
g	O
,	O
t	O
)	O
counts	O
for	O
column	O
t.	O
it	O
is	O
natural	O
to	O
make	O
this	O
be	O
a	O
multinomial	B
distribution	O
with	O
parameter	B
θt	O
.	O
since	O
each	O
column	O
has	O
a	O
different	O
distribution	O
,	O
we	O
will	O
want	O
to	O
integrate	B
out	I
θt	O
and	O
thus	O
compute	O
the	O
marginal	B
likelihood	I
p	O
(	O
nt|θt	O
)	O
p	O
(	O
θt|zt	O
)	O
dθt	O
(	O
5.74	O
)	O
but	O
what	O
prior	O
should	O
we	O
use	O
for	O
θt	O
?	O
when	O
zt	O
=	O
0	O
we	O
can	O
use	O
a	O
uniform	O
prior	O
,	O
p	O
(	O
θ|zt	O
=	O
0	O
)	O
=	O
dir	O
(	O
1	O
,	O
1	O
,	O
1	O
,	O
1	O
)	O
,	O
but	O
what	O
should	O
we	O
use	O
if	O
zt	O
=	O
1	O
?	O
after	O
all	O
,	O
if	O
the	O
column	O
is	O
conserved	O
,	O
it	O
could	O
be	O
a	O
(	O
nearly	O
)	O
pure	B
column	O
of	O
as	O
,	O
cs	O
,	O
gs	O
,	O
or	O
ts	O
.	O
a	O
natural	O
approach	O
is	O
to	O
use	O
a	O
mixture	O
of	O
dirichlet	O
priors	O
,	O
each	O
one	O
of	O
which	O
is	O
“	O
tilted	O
”	O
towards	O
the	O
appropriate	O
corner	O
of	O
the	O
4-dimensional	O
simplex	O
,	O
e.g.	O
,	O
p	O
(	O
nt|zt	O
)	O
=	O
(	O
cid:11	O
)	O
p	O
(	O
θ|zt	O
=	O
1	O
)	O
=	O
dir	O
(	O
θ|	O
(	O
10	O
,	O
1	O
,	O
1	O
,	O
1	O
)	O
)	O
+	O
···	O
+	O
dir	O
(	O
θ|	O
(	O
1	O
,	O
1	O
,	O
1	O
,	O
10	O
)	O
)	O
(	O
5.75	O
)	O
1	O
4	O
since	O
this	O
is	O
conjugate	O
,	O
we	O
can	O
easily	O
compute	O
p	O
(	O
nt|zt	O
)	O
.	O
see	O
(	O
brown	O
et	O
al	O
.	O
1993	O
)	O
for	O
an	O
1	O
4	O
5.5.	O
hierarchical	O
bayes	O
171	O
application	O
of	O
these	O
ideas	O
to	O
a	O
real	O
bio-sequence	O
problem	O
.	O
5.5	O
hierarchical	O
bayes	O
a	O
key	O
requirement	O
for	O
computing	O
the	O
posterior	O
p	O
(	O
θ|d	O
)	O
is	O
the	O
speciﬁcation	O
of	O
a	O
prior	O
p	O
(	O
θ|η	O
)	O
,	O
where	O
η	O
are	O
the	O
hyper-parameters	B
.	O
what	O
if	O
we	O
don	O
’	O
t	O
know	O
how	O
to	O
set	O
η	O
?	O
in	O
some	O
cases	O
,	O
we	O
can	O
use	O
uninformative	B
priors	O
,	O
we	O
we	O
discussed	O
above	O
.	O
a	O
more	O
bayesian	O
approach	O
is	O
to	O
put	O
a	O
prior	O
on	O
our	O
priors	O
!	O
in	O
terms	O
of	O
graphical	O
models	O
(	O
chapter	O
10	O
)	O
,	O
we	O
can	O
represent	O
the	O
situation	O
as	O
follows	O
:	O
η	O
→	O
θ	O
→	O
d	O
(	O
5.76	O
)	O
this	O
is	O
an	O
example	O
of	O
a	O
hierarchical	O
bayesian	O
model	O
,	O
also	O
called	O
a	O
multi-level	B
model	I
,	O
since	O
there	O
are	O
multiple	O
levels	O
of	O
unknown	B
quantities	O
.	O
we	O
give	O
a	O
simple	O
example	O
below	O
,	O
and	O
we	O
will	O
see	O
many	O
others	O
later	O
in	O
the	O
book	O
.	O
5.5.1	O
example	O
:	O
modeling	O
related	O
cancer	O
rates	O
consider	O
the	O
problem	O
of	O
predicting	O
cancer	O
rates	O
in	O
various	O
cities	O
(	O
this	O
example	O
is	O
from	O
(	O
johnson	O
and	O
albert	O
1999	O
,	O
p24	O
)	O
)	O
.	O
in	O
particular	O
,	O
suppose	O
we	O
measure	O
the	O
number	O
of	O
people	O
in	O
various	O
cities	O
,	O
ni	O
,	O
and	O
the	O
number	O
of	O
people	O
who	O
died	O
of	O
cancer	O
in	O
these	O
cities	O
,	O
xi	O
.	O
we	O
assume	O
xi	O
∼	O
bin	O
(	O
ni	O
,	O
θi	O
)	O
,	O
and	O
we	O
want	O
to	O
estimate	O
the	O
cancer	O
rates	O
θi	O
.	O
one	O
approach	O
is	O
to	O
estimate	O
them	O
all	O
separately	O
,	O
but	O
this	O
will	O
suffer	O
from	O
the	O
sparse	B
data	I
problem	I
(	O
underestimation	O
of	O
the	O
rate	B
of	O
cancer	O
due	O
to	O
small	O
ni	O
)	O
.	O
another	O
approach	O
is	O
to	O
assume	O
all	O
the	O
θi	O
are	O
the	O
same	O
;	O
this	O
is	O
called	O
parameter	B
tying	I
.	O
the	O
resulting	O
pooled	B
mle	O
is	O
just	O
ˆθ	O
=	O
i	O
ni	O
.	O
but	O
the	O
assumption	O
that	O
all	O
the	O
cities	O
have	O
the	O
same	O
rate	B
is	O
a	O
rather	O
strong	O
one	O
.	O
a	O
compromise	O
approach	O
is	O
to	O
assume	O
that	O
the	O
θi	O
are	O
similar	B
,	O
but	O
that	O
there	O
may	O
be	O
city-speciﬁc	O
variations	O
.	O
this	O
can	O
be	O
modeled	O
by	O
assuming	O
the	O
θi	O
are	O
drawn	O
from	O
some	O
common	O
distribution	O
,	O
say	O
θi	O
∼	O
beta	O
(	O
a	O
,	O
b	O
)	O
.	O
the	O
full	B
joint	O
distribution	O
can	O
be	O
written	O
as	O
p	O
(	O
d	O
,	O
θ	O
,	O
η|n	O
)	O
=p	O
(	O
η	O
)	O
bin	O
(	O
xi|ni	O
,	O
θi	O
)	O
beta	O
(	O
θi|η	O
)	O
(	O
cid:2	O
)	O
i	O
xi	O
(	O
cid:2	O
)	O
n	O
(	O
cid:12	O
)	O
(	O
5.77	O
)	O
i=1	O
where	O
η	O
=	O
(	O
a	O
,	O
b	O
)	O
.	O
note	O
that	O
it	O
is	O
crucial	O
that	O
we	O
infer	O
η	O
=	O
(	O
a	O
,	O
b	O
)	O
from	O
the	O
data	O
;	O
if	O
we	O
just	O
clamp	O
it	O
to	O
a	O
constant	O
,	O
the	O
θi	O
will	O
be	O
conditionally	B
independent	I
,	O
and	O
there	O
will	O
be	O
no	O
information	O
ﬂow	O
between	O
them	O
.	O
by	O
contrast	O
,	O
by	O
treating	O
η	O
as	O
an	O
unknown	B
(	O
hidden	B
variable	I
)	O
,	O
we	O
allow	O
the	O
data-poor	O
cities	O
to	O
borrow	B
statistical	I
strength	I
from	O
data-rich	O
ones	O
.	O
suppose	O
we	O
compute	O
the	O
joint	O
posterior	O
p	O
(	O
η	O
,	O
θ|d	O
)	O
.	O
from	O
this	O
we	O
can	O
get	O
the	O
posterior	O
marginals	O
p	O
(	O
θi|d	O
)	O
.	O
in	O
figure	O
5.11	O
(	O
a	O
)	O
,	O
we	O
plot	O
the	O
posterior	O
means	O
,	O
e	O
[	O
θi|d	O
]	O
,	O
as	O
blue	O
bars	O
,	O
as	O
well	O
as	O
the	O
population	O
level	O
mean	B
,	O
e	O
[	O
a/	O
(	O
a	O
+	O
b	O
)	O
|d	O
]	O
,	O
shown	O
as	O
a	O
red	O
line	O
(	O
this	O
represents	O
the	O
average	O
of	O
the	O
θi	O
’	O
s	O
)	O
.	O
we	O
see	O
that	O
the	O
posterior	B
mean	I
is	O
shrunk	O
towards	O
the	O
pooled	B
estimate	O
more	O
strongly	O
for	O
cities	O
with	O
small	O
sample	O
sizes	O
ni	O
.	O
for	O
example	O
,	O
city	O
1	O
and	O
city	O
20	O
both	O
have	O
a	O
0	O
observed	O
cancer	O
incidence	O
rate	B
,	O
but	O
city	O
20	O
has	O
a	O
smaller	O
population	O
,	O
so	O
its	O
rate	B
is	O
shrunk	O
more	O
towards	O
the	O
population-level	O
estimate	O
(	O
i.e.	O
,	O
it	O
is	O
closer	O
to	O
the	O
horizontal	O
red	O
line	O
)	O
than	O
city	O
1.	O
figure	O
5.11	O
(	O
b	O
)	O
shows	O
the	O
95	O
%	O
posterior	O
credible	O
intervals	O
for	O
θi	O
.	O
we	O
see	O
that	O
city	O
15	O
,	O
which	O
has	O
a	O
very	O
large	O
population	O
(	O
53,637	O
people	O
)	O
,	O
has	O
small	O
posterior	O
uncertainty	B
.	O
consequently	O
this	O
city	O
172	O
chapter	O
5.	O
bayesian	O
statistics	O
number	O
of	O
people	O
with	O
cancer	O
(	O
truncated	O
at	O
5	O
)	O
5	O
0	O
0	O
2000	O
1000	O
0	O
0	O
10	O
5	O
0	O
4	O
2	O
0	O
0	O
0	O
5	O
5	O
5	O
5	O
10	O
15	O
pop	O
of	O
city	O
(	O
truncated	O
at	O
2000	O
)	O
10	O
15	O
mle*1000	O
(	O
red	O
line=pooled	O
mle	O
)	O
20	O
25	O
20	O
25	O
10	O
15	O
posterior	O
mean*1000	O
(	O
red	O
line=pop	O
mean	B
)	O
20	O
25	O
10	O
15	O
20	O
25	O
95	O
%	O
credible	B
interval	I
on	O
theta	O
,	O
*=median	O
20	O
18	O
16	O
14	O
12	O
10	O
8	O
6	O
4	O
2	O
0	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
x	O
10−3	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
5.11	O
(	O
a	O
)	O
results	O
of	O
ﬁtting	O
the	O
model	O
using	O
the	O
data	O
from	O
(	O
johnson	O
and	O
albert	O
1999	O
,	O
p24	O
)	O
.	O
first	O
row	O
:	O
number	O
of	O
cancer	O
incidents	O
xi	O
in	O
20	O
cities	O
in	O
missouri	O
.	O
second	O
row	O
:	O
population	O
size	O
ni	O
.	O
the	O
largest	O
city	O
(	O
number	O
15	O
)	O
has	O
a	O
population	O
of	O
n15	O
=	O
53637	O
and	O
x15	O
=	O
54	O
incidents	O
,	O
but	O
we	O
truncate	O
the	O
vertical	O
axes	O
of	O
the	O
ﬁrst	O
two	O
rows	O
so	O
that	O
the	O
differences	O
between	O
the	O
other	O
cities	O
are	O
visible	B
.	O
third	O
row	O
:	O
mle	O
ˆθi	O
.	O
the	O
red	O
line	O
is	O
the	O
pooled	B
mle	O
.	O
fourth	O
row	O
:	O
posterior	B
mean	I
e	O
[	O
θi|d	O
]	O
.	O
the	O
red	O
line	O
is	O
e	O
[	O
a/	O
(	O
a	O
+	O
b	O
)	O
|d	O
]	O
,	O
the	O
population-level	O
mean	B
.	O
(	O
b	O
)	O
posterior	O
95	O
%	O
credible	O
intervals	O
on	O
the	O
cancer	O
rates	O
.	O
figure	O
generated	O
by	O
cancerrateseb	O
has	O
the	O
largest	O
impact	O
on	O
the	O
posterior	O
estimate	O
of	O
η	O
,	O
which	O
in	O
turn	O
will	O
impact	O
the	O
estimate	O
of	O
the	O
cancer	O
rates	O
for	O
other	O
cities	O
.	O
cities	O
10	O
and	O
19	O
,	O
which	O
have	O
the	O
highest	O
mle	O
,	O
also	O
have	O
the	O
highest	O
posterior	O
uncertainty	O
,	O
reﬂecting	O
the	O
fact	O
that	O
such	O
a	O
high	O
estimate	O
is	O
in	O
conﬂict	O
with	O
the	O
prior	O
(	O
which	O
is	O
estimated	O
from	O
all	O
the	O
other	O
cities	O
)	O
.	O
in	O
the	O
above	O
example	O
,	O
we	O
have	O
one	O
parameter	B
per	O
city	O
,	O
modeling	O
the	O
probability	O
the	O
response	O
is	O
on	O
.	O
by	O
making	O
the	O
bernoulli	O
rate	B
parameter	O
be	O
a	O
function	O
of	O
covariates	B
,	O
θi	O
=	O
sigm	O
(	O
wt	O
i	O
x	O
)	O
,	O
we	O
can	O
model	O
multiple	O
correlated	O
logistic	O
regression	B
tasks	O
.	O
this	O
is	O
called	O
multi-task	B
learning	I
,	O
and	O
will	O
be	O
discussed	O
in	O
more	O
detail	O
in	O
section	O
9.5	O
.	O
5.6	O
empirical	O
bayes	O
in	O
hierarchical	O
bayesian	O
models	O
,	O
we	O
need	O
to	O
compute	O
the	O
posterior	O
on	O
multiple	O
levels	O
of	O
latent	B
variables	O
.	O
for	O
example	O
,	O
in	O
a	O
two-level	O
model	O
,	O
we	O
need	O
to	O
compute	O
p	O
(	O
η	O
,	O
θ|d	O
)	O
∝	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ|η	O
)	O
p	O
(	O
η	O
)	O
in	O
some	O
cases	O
,	O
we	O
can	O
analytically	O
marginalize	O
out	O
θ	O
;	O
this	O
leaves	B
is	O
with	O
the	O
simpler	O
problem	O
of	O
just	O
computing	O
p	O
(	O
η|d	O
)	O
.	O
as	O
a	O
computational	O
shortcut	O
,	O
we	O
can	O
approximate	O
the	O
posterior	O
on	O
the	O
hyper-parameters	B
with	O
a	O
point-estimate	O
,	O
p	O
(	O
η|d	O
)	O
≈	O
δˆη	O
(	O
η	O
)	O
,	O
where	O
ˆη	O
=	O
argmax	O
p	O
(	O
η|d	O
)	O
.	O
since	O
η	O
is	O
typically	O
much	O
smaller	O
than	O
θ	O
in	O
dimensionality	O
,	O
it	O
is	O
less	O
prone	O
to	O
overﬁtting	B
,	O
so	O
we	O
can	O
safely	O
use	O
a	O
uniform	O
prior	O
on	O
η.	O
then	O
the	O
estimate	O
becomes	O
(	O
cid:2	O
)	O
(	O
cid:11	O
)	O
(	O
cid:3	O
)	O
ˆη	O
=	O
argmax	O
p	O
(	O
d|η	O
)	O
=	O
argmax	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ|η	O
)	O
dθ	O
(	O
5.78	O
)	O
(	O
5.79	O
)	O
5.6.	O
empirical	O
bayes	O
173	O
where	O
the	O
quantity	O
inside	O
the	O
brackets	O
is	O
the	O
marginal	O
or	O
integrated	B
likelihood	I
,	O
sometimes	O
called	O
the	O
evidence	B
.	O
this	O
overall	O
approach	O
is	O
called	O
empirical	O
bayes	O
(	O
eb	O
)	O
ortype-ii	O
maximum	O
likelihood	O
.	O
in	O
machine	B
learning	I
,	O
it	O
is	O
sometimes	O
called	O
the	O
evidence	B
procedure	I
.	O
empirical	O
bayes	O
violates	O
the	O
principle	O
that	O
the	O
prior	O
should	O
be	O
chosen	O
independently	O
of	O
the	O
data	O
.	O
however	O
,	O
we	O
can	O
just	O
view	O
it	O
as	O
a	O
computationally	O
cheap	O
approximation	O
to	O
inference	B
in	O
a	O
hierarchical	O
bayesian	O
model	O
,	O
just	O
as	O
we	O
viewed	O
map	O
estimation	O
as	O
an	O
approximation	O
to	O
inference	B
in	O
the	O
one	O
level	O
model	O
θ	O
→	O
d.	O
in	O
fact	O
,	O
we	O
can	O
construct	O
a	O
hierarchy	O
in	O
which	O
the	O
more	O
integrals	O
one	O
performs	O
,	O
the	O
“	O
more	O
bayesian	O
”	O
one	O
becomes	O
:	O
method	O
maximum	O
likelihood	O
map	O
estimation	O
ml-ii	O
(	O
empirical	O
bayes	O
)	O
map-ii	O
full	B
bayes	O
deﬁnition	O
ˆθ	O
=	O
argmaxθ	O
p	O
(	O
d|θ	O
)	O
ˆθ	O
=	O
argmaxθ	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ|η	O
)	O
ˆη	O
=	O
argmaxη	O
ˆη	O
=	O
argmaxη	O
p	O
(	O
θ	O
,	O
η|d	O
)	O
∝	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ|η	O
)	O
p	O
(	O
η	O
)	O
(	O
cid:22	O
)	O
(	O
cid:22	O
)	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ|η	O
)	O
dθ	O
=	O
argmaxη	O
p	O
(	O
d|η	O
)	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ|η	O
)	O
p	O
(	O
η	O
)	O
dθ	O
=	O
argmaxη	O
p	O
(	O
d|η	O
)	O
p	O
(	O
η	O
)	O
note	O
that	O
eb	O
can	O
be	O
shown	O
to	O
have	O
good	O
frequentist	B
properties	O
(	O
see	O
e.g.	O
,	O
(	O
carlin	O
and	O
louis	O
1996	O
;	O
efron	O
2010	O
)	O
)	O
,	O
so	O
it	O
is	O
widely	O
used	O
by	O
non-bayesians	O
.	O
for	O
example	O
,	O
the	O
popular	O
james-stein	O
estimator	B
,	O
discussed	O
in	O
section	O
6.3.3.2	O
,	O
can	O
be	O
derived	O
using	O
eb	O
.	O
5.6.1	O
example	O
:	O
beta-binomial	B
model	O
let	O
us	O
return	O
to	O
the	O
cancer	O
rates	O
model	O
.	O
we	O
can	O
analytically	O
integrate	B
out	I
the	O
θi	O
’	O
s	O
,	O
and	O
write	O
down	O
the	O
marginal	B
likelihood	I
directly	O
,	O
as	O
follows	O
:	O
(	O
cid:11	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
i	O
p	O
(	O
d|a	O
,	O
b	O
)	O
=	O
bin	O
(	O
xi|ni	O
,	O
θi	O
)	O
beta	O
(	O
θi|a	O
,	O
b	O
)	O
dθi	O
b	O
(	O
a	O
+	O
xi	O
,	O
b	O
+	O
ni	O
−	O
xi	O
)	O
b	O
(	O
a	O
,	O
b	O
)	O
=	O
i	O
(	O
5.80	O
)	O
(	O
5.81	O
)	O
various	O
ways	O
of	O
maximizing	O
this	O
wrt	O
a	O
and	O
b	O
are	O
discussed	O
in	O
(	O
minka	O
2000e	O
)	O
.	O
having	O
estimated	O
a	O
and	O
b	O
,	O
we	O
can	O
plug	O
in	O
the	O
hyper-parameters	B
to	O
compute	O
the	O
posterior	O
p	O
(	O
θi|ˆa	O
,	O
ˆb	O
,	O
d	O
)	O
in	O
the	O
usual	O
way	O
,	O
using	O
conjugate	O
analysis	O
.	O
the	O
net	O
result	O
is	O
that	O
the	O
posterior	B
mean	I
of	O
each	O
θi	O
is	O
a	O
weighted	B
average	I
of	O
its	O
local	O
mle	O
and	O
the	O
prior	O
means	O
,	O
which	O
depends	O
on	O
η	O
=	O
(	O
a	O
,	O
b	O
)	O
;	O
but	O
since	O
η	O
is	O
estimated	O
based	O
on	O
all	O
the	O
data	O
,	O
each	O
θi	O
is	O
inﬂuenced	O
by	O
all	O
the	O
data	O
.	O
5.6.2	O
example	O
:	O
gaussian-gaussian	O
model	O
we	O
now	O
study	O
another	O
example	O
that	O
is	O
analogous	O
to	O
the	O
cancer	O
rates	O
example	O
,	O
except	O
the	O
data	O
is	O
real-valued	O
.	O
we	O
will	O
use	O
a	O
gaussian	O
likelihood	B
and	O
a	O
gaussian	O
prior	O
.	O
this	O
will	O
allow	O
us	O
to	O
write	O
down	O
the	O
solution	O
analytically	O
.	O
in	O
particular	O
,	O
suppose	O
we	O
have	O
data	O
from	O
multiple	O
related	O
groups	O
.	O
for	O
example	O
,	O
xij	O
could	O
be	O
the	O
test	O
score	O
for	O
student	O
i	O
in	O
school	O
j	O
,	O
for	O
j	O
=	O
1	O
:	O
d	O
and	O
i	O
=	O
1	O
:	O
n	O
j.	O
we	O
want	O
to	O
estimate	O
the	O
mean	B
score	O
for	O
each	O
school	O
,	O
θj	O
.	O
however	O
,	O
since	O
the	O
sample	O
size	O
,	O
nj	O
,	O
may	O
be	O
small	O
for	O
174	O
chapter	O
5.	O
bayesian	O
statistics	O
some	O
schools	O
,	O
we	O
can	O
regularize	O
the	O
problem	O
by	O
using	O
a	O
hierarchical	O
bayesian	O
model	O
,	O
where	O
we	O
assume	O
θj	O
come	O
from	O
a	O
common	O
prior	O
,	O
n	O
(	O
μ	O
,	O
τ	O
2	O
)	O
.	O
the	O
joint	B
distribution	I
has	O
the	O
following	O
form	O
:	O
p	O
(	O
θ	O
,	O
d|η	O
,	O
σ2	O
)	O
=	O
n	O
(	O
θj|μ	O
,	O
τ	O
2	O
)	O
n	O
(	O
xij|θj	O
,	O
σ2	O
)	O
(	O
5.82	O
)	O
nj	O
(	O
cid:12	O
)	O
j=1	O
i=1	O
where	O
we	O
assume	O
σ2	O
is	O
known	O
for	O
simplicity	O
.	O
(	O
we	O
relax	O
this	O
assumption	O
in	O
exercise	O
24.4	O
.	O
)	O
we	O
explain	O
how	O
to	O
estimate	O
η	O
below	O
.	O
once	O
we	O
have	O
estimated	O
η	O
=	O
(	O
μ	O
,	O
τ	O
)	O
,	O
we	O
can	O
compute	O
the	O
posteriors	O
over	O
the	O
θj	O
’	O
s	O
.	O
to	O
do	O
that	O
,	O
it	O
simpliﬁes	O
matters	O
to	O
rewrite	O
the	O
joint	B
distribution	I
in	O
the	O
following	O
form	O
,	O
exploiting	O
the	O
fact	O
that	O
nj	O
gaussian	O
measurements	O
with	O
values	O
xij	O
and	O
variance	B
j	O
(	O
cid:2	O
)	O
σ2/nj	O
.	O
σ2	O
are	O
equivalent	O
to	O
one	O
measurement	O
of	O
value	O
xj	O
(	O
cid:2	O
)	O
1	O
this	O
yields	O
i=1	O
xij	O
with	O
variance	B
σ2	O
(	O
cid:4	O
)	O
nj	O
nj	O
d	O
(	O
cid:12	O
)	O
d	O
(	O
cid:12	O
)	O
p	O
(	O
θ	O
,	O
d|ˆη	O
,	O
σ2	O
)	O
=	O
n	O
(	O
θj|ˆμ	O
,	O
ˆτ	O
2	O
)	O
n	O
(	O
xj|θj	O
,	O
σ2	O
j	O
)	O
j=1	O
from	O
this	O
,	O
it	O
follows	O
from	O
the	O
results	O
of	O
section	O
4.4.1	O
that	O
the	O
posteriors	O
are	O
given	O
by	O
p	O
(	O
θj|d	O
,	O
ˆμ	O
,	O
ˆτ	O
2	O
)	O
=n	O
(	O
θj|	O
ˆbj	O
ˆμ	O
+	O
(	O
1−	O
ˆbj	O
)	O
xj	O
,	O
(	O
1	O
−	O
ˆbj	O
)	O
σ2	O
j	O
)	O
ˆbj	O
(	O
cid:2	O
)	O
σ2	O
j	O
σ2	O
j	O
+	O
ˆτ	O
2	O
where	O
ˆμ	O
=	O
x	O
and	O
ˆτ	O
2	O
will	O
be	O
deﬁned	O
below	O
.	O
the	O
quantity	O
0	O
≤	O
ˆbj	O
≤	O
1	O
controls	O
the	O
degree	B
of	O
shrinkage	B
towards	O
the	O
overall	O
mean	B
,	O
μ.	O
if	O
the	O
data	O
is	O
reliable	O
for	O
group	O
j	O
(	O
e.g.	O
,	O
because	O
the	O
sample	O
size	O
nj	O
is	O
large	O
)	O
,	O
then	O
σ2	O
j	O
will	O
be	O
small	O
relative	O
to	O
τ	O
2	O
;	O
hence	O
ˆbj	O
will	O
be	O
small	O
,	O
and	O
we	O
will	O
put	O
more	O
weight	O
on	O
xj	O
when	O
we	O
estimate	O
θj	O
.	O
however	O
,	O
groups	O
with	O
small	O
sample	O
sizes	O
will	O
get	O
regularized	O
(	O
shrunk	O
towards	O
the	O
overall	O
mean	B
μ	O
)	O
more	O
heavily	O
.	O
we	O
will	O
see	O
an	O
example	O
of	O
this	O
below	O
.	O
if	O
σj	O
=	O
σ	O
for	O
all	O
groups	O
j	O
,	O
the	O
posterior	B
mean	I
becomes	O
ˆθj	O
=	O
ˆbx	O
+	O
(	O
1−	O
ˆb	O
)	O
xj	O
=	O
x	O
+	O
(	O
1−	O
ˆb	O
)	O
(	O
xj	O
−	O
x	O
)	O
(	O
5.83	O
)	O
(	O
5.84	O
)	O
(	O
5.85	O
)	O
(	O
5.86	O
)	O
this	O
has	O
exactly	O
the	O
same	O
form	O
as	O
the	O
james	O
stein	O
estimator	B
discussed	O
in	O
section	O
6.3.3.2	O
.	O
5.6.2.1	O
example	O
:	O
predicting	O
baseball	O
scores	B
we	O
now	O
give	O
an	O
example	O
of	O
shrinkage	B
applied	O
to	O
baseball	O
batting	O
averages	O
,	O
from	O
(	O
efron	O
and	O
morris	O
1975	O
)	O
.	O
we	O
observe	O
the	O
number	O
of	O
hits	O
for	O
d	O
=	O
18	O
players	O
during	O
the	O
ﬁrst	O
t	O
=	O
45	O
games	O
.	O
call	O
the	O
number	O
of	O
hits	O
bi	O
.	O
we	O
assume	O
bj	O
∼	O
bin	O
(	O
t	O
,	O
θj	O
)	O
,	O
where	O
θj	O
is	O
the	O
“	O
true	O
”	O
batting	O
average	O
for	O
player	O
j.	O
the	O
goal	O
is	O
to	O
estimate	O
the	O
θj	O
.	O
the	O
mle	O
is	O
of	O
course	O
ˆθj	O
=	O
xj	O
,	O
where	O
xj	O
=	O
bj/t	O
is	O
the	O
empirical	O
batting	O
average	O
.	O
however	O
,	O
we	O
can	O
use	O
an	O
eb	O
approach	O
to	O
do	O
better	O
.	O
gaussian	O
,	O
xj	O
∼	O
n	O
(	O
θj	O
,	O
σ2	O
)	O
for	O
known	O
σ2	O
.	O
to	O
apply	O
the	O
gaussian	O
shrinkage	B
approach	O
described	O
above	O
,	O
we	O
require	O
that	O
the	O
likelihood	B
be	O
(	O
we	O
drop	O
the	O
i	O
subscript	O
since	O
we	O
assume	O
nj	O
=	O
1	O
,	O
5.6.	O
empirical	O
bayes	O
mle	O
(	O
top	O
)	O
and	O
shrinkage	B
estimates	O
(	O
bottom	O
)	O
mse	O
mle	O
=	O
0.0042	O
,	O
mse	O
shrunk	O
=	O
0.0013	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0.2	O
0.25	O
0.3	O
0.35	O
0.4	O
(	O
a	O
)	O
0.4	O
0.35	O
0.3	O
0.25	O
e	O
s	O
m	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
1	O
2	O
3	O
player	O
number	O
4	O
5	O
(	O
b	O
)	O
175	O
true	O
shrunk	O
mle	O
figure	O
5.12	O
(	O
a	O
)	O
mle	O
parameters	O
(	O
top	O
)	O
and	O
corresponding	O
shrunken	O
estimates	O
(	O
bottom	O
)	O
.	O
(	O
b	O
)	O
we	O
plot	O
the	O
true	O
parameters	O
(	O
blue	O
)	O
,	O
the	O
posterior	B
mean	I
estimate	O
(	O
green	O
)	O
,	O
and	O
the	O
mles	O
(	O
red	O
)	O
for	O
5	O
of	O
the	O
players	O
.	O
figure	O
generated	O
by	O
shrinkagedemobaseball	O
.	O
since	O
xj	O
already	O
represents	O
the	O
average	O
for	O
player	O
j	O
.	O
)	O
however	O
,	O
binomial	B
likelihood	O
.	O
while	O
this	O
has	O
the	O
right	O
mean	O
,	O
e	O
[	O
xj	O
]	O
=	O
θj	O
,	O
the	O
variance	B
is	O
not	O
constant	O
:	O
in	O
this	O
example	O
we	O
have	O
a	O
var	O
[	O
xj	O
]	O
=	O
1	O
t	O
2	O
var	O
[	O
bj	O
]	O
=	O
t	O
θj	O
(	O
1	O
−	O
θj	O
)	O
t	O
2	O
(	O
5.87	O
)	O
√	O
t	O
arcsin	O
(	O
2yj	O
−	O
1	O
)	O
√	O
so	O
let	O
us	O
apply	O
a	O
variance	O
stabilizing	O
transform5	O
to	O
xj	O
to	O
better	O
match	O
the	O
gaussian	O
assump-	O
tion	O
:	O
yj	O
=	O
f	O
(	O
yj	O
)	O
=	O
(	O
5.88	O
)	O
now	O
we	O
have	O
approximately	O
yj	O
∼	O
n	O
(	O
f	O
(	O
θj	O
)	O
,	O
1	O
)	O
=	O
n	O
(	O
μj	O
,	O
1	O
)	O
.	O
we	O
use	O
gaussian	O
shrinkage	B
to	O
estimate	O
the	O
μj	O
using	O
equation	O
5.86	O
with	O
σ2	O
=	O
1	O
,	O
and	O
we	O
then	O
transform	O
back	O
to	O
get	O
t	O
)	O
+	O
1	O
)	O
ˆθj	O
=	O
0.5	O
(	O
sin	O
(	O
ˆμj/	O
(	O
5.89	O
)	O
the	O
results	O
are	O
shown	O
in	O
figure	O
5.12	O
(	O
a-b	O
)	O
.	O
in	O
(	O
a	O
)	O
,	O
we	O
plot	O
the	O
mle	O
ˆθj	O
and	O
the	O
posterior	B
mean	I
θj	O
.	O
we	O
see	O
that	O
all	O
the	O
estimates	O
have	O
shrunk	O
towards	O
the	O
global	O
mean	O
,	O
0.265.	O
in	O
(	O
b	O
)	O
,	O
we	O
plot	O
the	O
true	O
value	O
θj	O
,	O
the	O
mle	O
ˆθj	O
and	O
the	O
posterior	B
mean	I
θj	O
.	O
(	O
the	O
“	O
true	O
”	O
values	O
of	O
θj	O
are	O
estimated	O
from	O
a	O
large	O
number	O
of	O
independent	O
games	O
.	O
)	O
we	O
see	O
that	O
,	O
on	O
average	O
,	O
the	O
shrunken	O
estimate	O
(	O
cid:4	O
)	O
d	O
is	O
much	O
closer	O
to	O
the	O
true	O
parameters	O
than	O
the	O
mle	O
is	O
.	O
speciﬁcally	O
,	O
the	O
mean	B
squared	I
error	I
,	O
j=1	O
(	O
θj	O
−	O
θj	O
)	O
2	O
,	O
is	O
over	O
three	O
times	O
smaller	O
using	O
the	O
shrinkage	B
estimates	O
deﬁned	O
by	O
mse	O
=	O
1	O
n	O
θj	O
than	O
using	O
the	O
mles	O
ˆθj	O
.	O
5.6.2.2	O
estimating	O
the	O
hyper-parameters	B
in	O
this	O
section	O
,	O
we	O
give	O
an	O
algorithm	O
for	O
estimating	O
η.	O
suppose	O
initially	O
that	O
σ2	O
j	O
=	O
σ2	O
is	O
the	O
same	O
for	O
all	O
groups	O
.	O
in	O
this	O
case	O
,	O
we	O
can	O
derive	O
the	O
eb	O
estimate	O
in	O
closed	O
form	O
,	O
as	O
we	O
now	O
show	O
.	O
from	O
equation	O
4.126	O
,	O
we	O
have	O
p	O
(	O
xj|μ	O
,	O
τ	O
2	O
,	O
σ2	O
)	O
=	O
n	O
(	O
xj|θj	O
,	O
σ2	O
)	O
n	O
(	O
θj|μ	O
,	O
τ	O
2	O
)	O
dθj	O
=	O
n	O
(	O
xj|μ	O
,	O
τ	O
2	O
+	O
σ2	O
)	O
(	O
5.90	O
)	O
(	O
cid:11	O
)	O
5.	O
suppose	O
e	O
[	O
x	O
]	O
=μ	O
and	O
var	O
[	O
x	O
]	O
=σ	O
2	O
(	O
μ	O
)	O
.	O
let	O
y	O
=	O
f	O
(	O
x	O
)	O
.	O
then	O
a	O
taylor	O
series	O
expansions	O
gives	O
y	O
≈	O
f	O
(	O
μ	O
)	O
+	O
(	O
x	O
−	O
μ	O
)	O
f	O
(	O
cid:2	O
)	O
(	O
μ	O
)	O
2σ2	O
(	O
μ	O
)	O
.	O
a	O
variance	O
stabilizing	O
transformation	O
is	O
a	O
function	O
f	O
such	O
that	O
f	O
(	O
cid:2	O
)	O
(	O
μ	O
)	O
.	O
hence	O
var	O
[	O
y	O
]	O
≈	O
f	O
(	O
cid:2	O
)	O
(	O
μ	O
)	O
2var	O
[	O
x	O
−	O
μ	O
]	O
=f	O
(	O
cid:2	O
)	O
(	O
μ	O
)	O
2σ2	O
(	O
μ	O
)	O
is	O
independent	O
of	O
μ	O
.	O
176	O
chapter	O
5.	O
bayesian	O
statistics	O
hence	O
the	O
marginal	B
likelihood	I
is	O
d	O
(	O
cid:12	O
)	O
j=1	O
p	O
(	O
d|μ	O
,	O
τ	O
2	O
,	O
σ2	O
)	O
=	O
n	O
(	O
xj|μ	O
,	O
τ	O
2	O
+	O
σ2	O
)	O
(	O
5.91	O
)	O
thus	O
we	O
can	O
estimate	O
the	O
hyper-parameters	B
using	O
the	O
usual	O
mles	O
for	O
a	O
gaussian	O
.	O
for	O
μ	O
,	O
we	O
have	O
xj	O
=	O
x	O
(	O
5.92	O
)	O
d	O
(	O
cid:6	O
)	O
j=1	O
ˆμ	O
=	O
1	O
d	O
ˆτ	O
2	O
+	O
σ2	O
=	O
1	O
d	O
d	O
(	O
cid:6	O
)	O
j=1	O
which	O
is	O
the	O
overall	O
mean	B
.	O
for	O
the	O
variance	B
,	O
we	O
can	O
use	O
moment	B
matching	I
(	O
which	O
is	O
equivalent	O
to	O
the	O
mle	O
for	O
a	O
gaussian	O
)	O
:	O
we	O
simply	O
equate	O
the	O
model	O
variance	O
to	O
the	O
empirical	O
variance	O
:	O
(	O
xj	O
−	O
x	O
)	O
2	O
(	O
cid:2	O
)	O
s2	O
(	O
5.93	O
)	O
so	O
ˆτ	O
2	O
=	O
s2	O
−	O
σ2	O
.	O
since	O
we	O
know	O
τ	O
2	O
must	O
be	O
positive	O
,	O
it	O
is	O
common	O
to	O
use	O
the	O
following	O
revised	O
estimate	O
:	O
ˆτ	O
2	O
=	O
max	O
{	O
0	O
,	O
s2	O
−	O
σ2	O
}	O
=	O
(	O
s2	O
−	O
σ2	O
)	O
+	O
hence	O
the	O
shrinkage	B
factor	I
is	O
ˆb	O
=	O
σ2	O
σ2	O
+	O
ˆτ	O
2	O
=	O
σ2	O
σ2	O
+	O
(	O
s2	O
−	O
σ2	O
)	O
+	O
(	O
5.94	O
)	O
(	O
5.95	O
)	O
in	O
the	O
case	O
where	O
the	O
σ2	O
j	O
’	O
s	O
are	O
different	O
,	O
we	O
can	O
no	O
longer	O
derive	O
a	O
solution	O
in	O
closed	O
form	O
.	O
exercise	O
11.13	O
discusses	O
how	O
to	O
use	O
the	O
em	O
algorithm	O
to	O
derive	O
an	O
eb	O
estimate	O
,	O
and	O
exercise	O
24.4	O
discusses	O
how	O
to	O
perform	O
full	B
bayesian	O
inference	B
in	O
this	O
hierarchical	O
model	O
.	O
5.7	O
bayesian	O
decision	B
theory	O
we	O
have	O
seen	O
how	O
probability	O
theory	O
can	O
be	O
used	O
to	O
represent	O
and	O
updates	O
our	O
beliefs	O
about	O
the	O
state	B
of	O
the	O
world	O
.	O
however	O
,	O
ultimately	O
our	O
goal	O
is	O
to	O
convert	O
our	O
beliefs	O
into	O
actions	B
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
the	O
optimal	O
way	O
to	O
do	O
this	O
.	O
we	O
can	O
formalize	O
any	O
given	O
statistical	O
decision	O
problem	O
as	O
a	O
game	B
against	I
nature	I
(	O
as	O
opposed	O
to	O
a	O
game	O
against	O
other	O
strategic	O
players	O
,	O
which	O
is	O
the	O
topic	B
of	O
game	B
theory	I
,	O
see	O
e.g.	O
,	O
(	O
shoham	O
and	O
leyton-brown	O
2009	O
)	O
for	O
details	O
)	O
.	O
in	O
this	O
game	O
,	O
nature	O
picks	O
a	O
state	B
or	O
parameter	B
or	O
label	B
,	O
y	O
∈	O
y	O
,	O
unknown	B
to	O
us	O
,	O
and	O
then	O
generates	O
an	O
observation	B
,	O
x	O
∈	O
x	O
,	O
which	O
we	O
get	O
to	O
see	O
.	O
we	O
then	O
have	O
to	O
make	O
a	O
decision	B
,	O
that	O
is	O
,	O
we	O
have	O
to	O
choose	O
an	O
action	B
a	O
from	O
some	O
action	B
space	I
a.	O
finally	O
we	O
incur	O
some	O
loss	B
,	O
l	O
(	O
y	O
,	O
a	O
)	O
,	O
which	O
measures	O
how	O
compatible	O
our	O
action	B
a	O
is	O
with	O
nature	O
’	O
s	O
hidden	B
state	O
y.	O
for	O
example	O
,	O
we	O
might	O
use	O
misclassiﬁcation	B
loss	I
,	O
l	O
(	O
y	O
,	O
a	O
)	O
=i	O
(	O
y	O
(	O
cid:6	O
)	O
=	O
a	O
)	O
,	O
or	O
squared	B
loss	I
,	O
l	O
(	O
y	O
,	O
a	O
)	O
=	O
(	O
y	O
−	O
a	O
)	O
2.	O
we	O
will	O
see	O
some	O
other	O
examples	O
below	O
.	O
5.7.	O
bayesian	O
decision	B
theory	O
177	O
our	O
goal	O
is	O
to	O
devise	O
a	O
decision	B
procedure	I
or	O
policy	B
,	O
δ	O
:	O
x	O
→	O
a	O
,	O
which	O
speciﬁes	O
the	O
optimal	B
action	I
for	O
each	O
possible	O
input	O
.	O
by	O
optimal	O
,	O
we	O
mean	B
the	O
action	B
that	O
minimizes	O
the	O
expected	O
loss	O
:	O
δ	O
(	O
x	O
)	O
=	O
argmin	O
a∈a	O
e	O
[	O
l	O
(	O
y	O
,	O
a	O
)	O
]	O
(	O
5.96	O
)	O
in	O
economics	O
,	O
u	O
(	O
y	O
,	O
a	O
)	O
=	O
−l	O
(	O
y	O
,	O
a	O
)	O
.	O
thus	O
the	O
above	O
rule	O
becomes	O
it	O
is	O
more	O
common	O
to	O
talk	O
of	O
a	O
utility	B
function	I
;	O
this	O
is	O
just	O
negative	O
loss	O
,	O
δ	O
(	O
x	O
)	O
=	O
argmax	O
a∈a	O
e	O
[	O
u	O
(	O
y	O
,	O
a	O
)	O
]	O
(	O
5.97	O
)	O
this	O
is	O
called	O
the	O
maximum	B
expected	I
utility	I
principle	I
,	O
and	O
is	O
the	O
essence	O
of	O
what	O
we	O
mean	B
by	O
rational	B
behavior	I
.	O
note	O
that	O
there	O
are	O
two	O
different	O
interpretations	O
of	O
what	O
we	O
mean	B
by	O
“	O
expected	O
”	O
.	O
in	O
the	O
bayesian	O
version	O
,	O
which	O
we	O
discuss	O
below	O
,	O
we	O
mean	B
the	O
expected	B
value	I
of	O
y	O
given	O
the	O
data	O
we	O
have	O
seen	O
so	O
far	O
.	O
in	O
the	O
frequentist	B
version	O
,	O
which	O
we	O
discuss	O
in	O
section	O
6.3	O
,	O
we	O
mean	B
the	O
expected	B
value	I
of	O
y	O
and	O
x	O
that	O
we	O
expect	O
to	O
see	O
in	O
the	O
future	O
.	O
in	O
the	O
bayesian	O
approach	O
to	O
decision	B
theory	O
,	O
the	O
optimal	B
action	I
,	O
having	O
observed	O
x	O
,	O
is	O
deﬁned	O
as	O
the	O
action	B
a	O
that	O
minimizes	O
the	O
posterior	B
expected	I
loss	I
:	O
(	O
cid:6	O
)	O
ρ	O
(	O
a|x	O
)	O
(	O
cid:2	O
)	O
ep	O
(	O
y|x	O
)	O
[	O
l	O
(	O
y	O
,	O
a	O
)	O
]	O
=	O
l	O
(	O
y	O
,	O
a	O
)	O
p	O
(	O
y|x	O
)	O
(	O
5.98	O
)	O
y	O
(	O
if	O
y	O
is	O
continuous	O
(	O
e.g.	O
,	O
when	O
we	O
want	O
to	O
estimate	O
a	O
parameter	B
vector	O
)	O
,	O
we	O
should	O
replace	O
the	O
sum	O
with	O
an	O
integral	O
.	O
)	O
hence	O
the	O
bayes	O
estimator	B
,	O
also	O
called	O
the	O
bayes	O
decision	B
rule	I
,	O
is	O
given	O
by	O
δ	O
(	O
x	O
)	O
=	O
arg	O
min	O
a∈a	O
ρ	O
(	O
a|x	O
)	O
(	O
5.99	O
)	O
5.7.1	O
bayes	O
estimators	O
for	O
common	O
loss	B
functions	O
in	O
this	O
section	O
we	O
show	O
how	O
to	O
construct	O
bayes	O
estimators	O
for	O
the	O
loss	B
functions	O
most	O
commonly	O
arising	O
in	O
machine	B
learning	I
.	O
5.7.1.1	O
map	O
estimate	O
minimizes	O
0-1	O
loss	B
the	O
0-1	O
loss	B
is	O
deﬁned	O
by	O
l	O
(	O
y	O
,	O
a	O
)	O
=	O
i	O
(	O
y	O
(	O
cid:6	O
)	O
=	O
a	O
)	O
=	O
(	O
cid:5	O
)	O
0	O
1	O
if	O
a	O
=	O
y	O
if	O
a	O
(	O
cid:6	O
)	O
=	O
y	O
(	O
5.100	O
)	O
this	O
is	O
commonly	O
used	O
in	O
classiﬁcation	B
problems	O
where	O
y	O
is	O
the	O
true	O
class	O
label	B
and	O
a	O
=	O
ˆy	O
is	O
the	O
estimate	O
.	O
for	O
example	O
,	O
in	O
the	O
two	O
class	O
case	O
,	O
we	O
can	O
write	O
the	O
loss	B
matrix	I
as	O
follows	O
:	O
ˆy	O
=	O
1	O
ˆy	O
=	O
0	O
y	O
=	O
1	O
y	O
=	O
0	O
0	O
1	O
1	O
0	O
178	O
chapter	O
5.	O
bayesian	O
statistics	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
figure	O
5.13	O
for	O
some	O
regions	O
of	O
input	O
space	O
,	O
where	O
the	O
class	O
posteriors	O
are	O
uncertain	O
,	O
we	O
may	O
prefer	O
not	O
to	O
choose	O
class	O
1	O
or	O
2	O
;	O
instead	O
we	O
may	O
prefer	O
the	O
reject	O
option	O
.	O
based	O
on	O
figure	O
1.26	O
of	O
(	O
bishop	O
2006a	O
)	O
.	O
(	O
in	O
section	O
5.7.2	O
,	O
we	O
generalize	B
this	O
loss	B
function	I
so	O
it	O
penalizes	O
the	O
two	O
kinds	O
of	O
errors	O
on	O
the	O
off-diagonal	O
differently	O
.	O
)	O
the	O
posterior	B
expected	I
loss	I
is	O
ρ	O
(	O
a|x	O
)	O
=	O
p	O
(	O
a	O
(	O
cid:6	O
)	O
=	O
y|x	O
)	O
=	O
1	O
−	O
p	O
(	O
y|x	O
)	O
(	O
5.101	O
)	O
hence	O
the	O
action	B
that	O
minimizes	O
the	O
expected	O
loss	O
is	O
the	O
posterior	B
mode	I
or	O
map	O
estimate	O
y∗	O
(	O
x	O
)	O
=	O
arg	O
max	O
y∈y	O
p	O
(	O
y|x	O
)	O
(	O
5.102	O
)	O
5.7.1.2	O
reject	O
option	O
in	O
classiﬁcation	B
problems	O
where	O
p	O
(	O
y|x	O
)	O
is	O
very	O
uncertain	O
,	O
we	O
may	O
prefer	O
to	O
choose	O
a	O
reject	B
action	I
,	O
in	O
which	O
we	O
refuse	O
to	O
classify	O
the	O
example	O
as	O
any	O
of	O
the	O
speciﬁed	O
classes	O
,	O
and	O
instead	O
say	O
“	O
don	O
’	O
t	O
know	O
”	O
.	O
such	O
ambiguous	O
cases	O
can	O
be	O
handled	O
by	O
e.g.	O
,	O
a	O
human	O
expert	O
.	O
see	O
figure	O
5.13	O
for	O
an	O
illustration	O
.	O
this	O
is	O
useful	O
in	O
risk	B
averse	I
domains	O
such	O
as	O
medicine	O
and	O
ﬁnance	O
.	O
we	O
can	O
formalize	O
the	O
reject	O
option	O
as	O
follows	O
.	O
let	O
choosing	O
a	O
=	O
c	O
+	O
1	O
correspond	O
to	O
picking	O
the	O
reject	B
action	I
,	O
and	O
choosing	O
a	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
}	O
correspond	O
to	O
picking	O
one	O
of	O
the	O
classes	O
.	O
suppose	O
we	O
deﬁne	O
the	O
loss	B
function	I
as	O
if	O
i	O
=	O
j	O
and	O
i	O
,	O
j	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
}	O
if	O
i	O
=	O
c	O
+	O
1	O
otherwise	O
(	O
5.103	O
)	O
l	O
(	O
y	O
=	O
j	O
,	O
a	O
=	O
i	O
)	O
=	O
⎧⎨	O
⎩	O
0	O
λr	O
λs	O
where	O
λr	O
is	O
the	O
cost	O
of	O
the	O
reject	B
action	I
,	O
and	O
λs	O
is	O
the	O
cost	O
of	O
a	O
substitution	O
error	O
.	O
in	O
exercise	O
5.3	O
,	O
you	O
will	O
show	O
that	O
the	O
optimal	B
action	I
is	O
to	O
pick	O
the	O
reject	B
action	I
if	O
the	O
most	O
probable	O
class	O
has	O
a	O
probability	O
below	O
1	O
−	O
λr	O
λs	O
;	O
otherwise	O
you	O
should	O
just	O
pick	O
the	O
most	O
probable	O
class	O
.	O
5.7.	O
bayesian	O
decision	B
theory	O
179	O
|x|0.2	O
|x|1.0	O
|x|2.0	O
2	O
1	O
0	O
−2	O
−1	O
0	O
1	O
2	O
(	O
a	O
)	O
2	O
1	O
0	O
−2	O
−1	O
0	O
1	O
2	O
(	O
b	O
)	O
2	O
1	O
0	O
−2	O
−1	O
0	O
1	O
2	O
(	O
c	O
)	O
(	O
a-c	O
)	O
.	O
plots	O
of	O
the	O
l	O
(	O
y	O
,	O
a	O
)	O
=|y	O
−	O
a|q	O
vs	O
|y	O
−	O
a|	O
for	O
q	O
=	O
0.2	O
,	O
q	O
=	O
1	O
and	O
q	O
=	O
2.	O
figure	O
figure	O
5.14	O
generated	O
by	O
lossfunctionfig	O
.	O
5.7.1.3	O
posterior	B
mean	I
minimizes	O
(	O
cid:9	O
)	O
2	O
(	O
quadratic	O
)	O
loss	B
for	O
continuous	O
parameters	O
,	O
a	O
more	O
appropriate	O
loss	B
function	I
is	O
squared	B
error	I
,	O
(	O
cid:9	O
)	O
2	O
loss	B
,	O
or	O
quadratic	B
loss	I
,	O
deﬁned	O
as	O
l	O
(	O
y	O
,	O
a	O
)	O
=	O
(	O
y	O
−	O
a	O
)	O
2	O
(	O
cid:20	O
)	O
ρ	O
(	O
a|x	O
)	O
=e	O
(	O
cid:21	O
)	O
(	O
y	O
−	O
a	O
)	O
2|x	O
=	O
e	O
the	O
posterior	B
expected	I
loss	I
is	O
given	O
by	O
(	O
cid:20	O
)	O
y2|x	O
(	O
cid:21	O
)	O
−	O
2ae	O
[	O
y|x	O
]	O
+a	O
2	O
(	O
cid:11	O
)	O
(	O
5.104	O
)	O
(	O
5.105	O
)	O
(	O
5.106	O
)	O
(	O
5.107	O
)	O
(	O
5.108	O
)	O
hence	O
the	O
optimal	O
estimate	O
is	O
the	O
posterior	B
mean	I
:	O
ρ	O
(	O
a|x	O
)	O
=	O
−2e	O
[	O
y|x	O
]	O
+	O
2a	O
=	O
0	O
⇒	O
ˆy	O
=	O
e	O
[	O
y|x	O
]	O
=	O
yp	O
(	O
y|x	O
)	O
dy	O
∂	O
∂a	O
this	O
is	O
often	O
called	O
the	O
minimum	B
mean	I
squared	I
error	I
estimate	O
or	O
mmse	O
estimate	O
.	O
in	O
a	O
linear	B
regression	I
problem	O
,	O
we	O
have	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
n	O
(	O
y|xt	O
w	O
,	O
σ2	O
)	O
in	O
this	O
case	O
,	O
the	O
optimal	O
estimate	O
given	O
some	O
training	O
data	O
d	O
is	O
given	O
by	O
e	O
[	O
y|x	O
,	O
d	O
]	O
=	O
xt	O
e	O
[	O
w|d	O
]	O
that	O
is	O
,	O
we	O
just	O
plug-in	B
the	O
posterior	B
mean	I
parameter	O
estimate	O
.	O
note	O
that	O
this	O
is	O
the	O
optimal	O
thing	O
to	O
do	O
no	O
matter	O
what	O
prior	O
we	O
use	O
for	O
w.	O
5.7.1.4	O
posterior	B
median	I
minimizes	O
(	O
cid:9	O
)	O
1	O
(	O
absolute	O
)	O
loss	B
the	O
(	O
cid:9	O
)	O
2	O
loss	B
penalizes	O
deviations	O
from	O
the	O
truth	O
quadratically	O
,	O
and	O
thus	O
is	O
sensitive	O
to	O
outliers	B
.	O
a	O
more	O
robust	B
alternative	O
is	O
the	O
absolute	O
or	O
(	O
cid:9	O
)	O
1	O
loss	B
,	O
l	O
(	O
y	O
,	O
a	O
)	O
=	O
|y−a|	O
(	O
see	O
figure	O
5.14	O
)	O
.	O
the	O
optimal	O
estimate	O
is	O
the	O
posterior	B
median	I
,	O
i.e.	O
,	O
a	O
value	O
a	O
such	O
that	O
p	O
(	O
y	O
<	O
a|x	O
)	O
=	O
p	O
(	O
y	O
≥	O
a|x	O
)	O
=	O
0.5.	O
see	O
exercise	O
5.9	O
for	O
a	O
proof	O
.	O
5.7.1.5	O
supervised	B
learning	I
consider	O
a	O
prediction	O
function	O
δ	O
:	O
x	O
→	O
y	O
,	O
and	O
suppose	O
we	O
have	O
some	O
cost	O
function	O
(	O
cid:9	O
)	O
(	O
y	O
,	O
y	O
(	O
cid:2	O
)	O
)	O
which	O
gives	O
the	O
cost	O
of	O
predicting	O
y	O
(	O
cid:2	O
)	O
when	O
the	O
truth	O
is	O
y.	O
we	O
can	O
deﬁne	O
the	O
loss	B
incurred	O
by	O
180	O
chapter	O
5.	O
bayesian	O
statistics	O
taking	O
action	B
δ	O
(	O
i.e.	O
,	O
using	O
this	O
predictor	O
)	O
when	O
the	O
unknown	B
state	O
of	O
nature	O
is	O
θ	O
(	O
the	O
parameters	O
of	O
the	O
data	O
generating	O
mechanism	O
)	O
as	O
follows	O
:	O
l	O
(	O
θ	O
,	O
δ	O
)	O
(	O
cid:2	O
)	O
e	O
(	O
x	O
,	O
y	O
)	O
∼p	O
(	O
x	O
,	O
y|θ	O
)	O
[	O
(	O
cid:9	O
)	O
(	O
y	O
,	O
δ	O
(	O
x	O
)	O
]	O
=	O
l	O
(	O
y	O
,	O
δ	O
(	O
x	O
)	O
)	O
p	O
(	O
x	O
,	O
y|θ	O
)	O
(	O
5.109	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
x	O
y	O
this	O
is	O
known	O
as	O
the	O
generalization	B
error	I
.	O
our	O
goal	O
is	O
to	O
minimize	O
the	O
posterior	B
expected	I
loss	I
,	O
given	O
by	O
ρ	O
(	O
δ|d	O
)	O
=	O
p	O
(	O
θ|d	O
)	O
l	O
(	O
θ	O
,	O
δ	O
)	O
dθ	O
(	O
5.110	O
)	O
(	O
cid:11	O
)	O
this	O
should	O
be	O
contrasted	O
with	O
the	O
frequentist	B
risk	O
which	O
is	O
deﬁned	O
in	O
equation	O
6.47	O
.	O
5.7.2	O
the	O
false	B
positive	I
vs	O
false	B
negative	I
tradeoff	O
in	O
this	O
section	O
,	O
we	O
focus	O
on	O
binary	O
decision	O
problems	O
,	O
such	O
as	O
hypothesis	O
testing	O
,	O
two-class	O
classiﬁcation	B
,	O
object/	O
event	O
detection	O
,	O
etc	O
.	O
there	O
are	O
two	O
types	O
of	O
error	O
we	O
can	O
make	O
:	O
a	O
false	B
positive	I
(	O
aka	O
false	B
alarm	I
)	O
,	O
which	O
arises	O
when	O
we	O
estimate	O
ˆy	O
=	O
1	O
but	O
the	O
truth	O
is	O
y	O
=	O
0	O
;	O
or	O
a	O
false	B
negative	I
(	O
aka	O
missed	B
detection	I
)	O
,	O
which	O
arises	O
when	O
we	O
estimate	O
ˆy	O
=	O
0	O
but	O
the	O
truth	O
is	O
y	O
=	O
1.	O
the	O
0-1	O
loss	B
treats	O
these	O
two	O
kinds	O
of	O
errors	O
equivalently	O
.	O
however	O
,	O
we	O
can	O
consider	O
the	O
following	O
more	O
general	O
loss	B
matrix	I
:	O
ˆy	O
=	O
1	O
y	O
=	O
1	O
y	O
=	O
0	O
0	O
lf	O
p	O
ˆy	O
=	O
0	O
lf	O
n	O
0	O
where	O
lf	O
n	O
is	O
the	O
cost	O
of	O
a	O
false	B
negative	I
,	O
and	O
lf	O
p	O
is	O
the	O
cost	O
of	O
a	O
false	B
positive	I
.	O
the	O
posterior	B
expected	I
loss	I
for	O
the	O
two	O
possible	O
actions	B
is	O
given	O
by	O
ρ	O
(	O
ˆy	O
=	O
0|x	O
)	O
=l	O
f	O
n	O
p	O
(	O
y	O
=	O
1|x	O
)	O
ρ	O
(	O
ˆy	O
=	O
1|x	O
)	O
=l	O
f	O
p	O
p	O
(	O
y	O
=	O
0|x	O
)	O
hence	O
we	O
should	O
pick	O
class	O
ˆy	O
=	O
1	O
iff	B
ρ	O
(	O
ˆy	O
=	O
0|x	O
)	O
>	O
ρ	O
(	O
ˆy	O
=	O
1|x	O
)	O
p	O
(	O
y	O
=	O
1|x	O
)	O
p	O
(	O
y	O
=	O
0|x	O
)	O
lf	O
p	O
lf	O
n	O
>	O
(	O
5.111	O
)	O
(	O
5.112	O
)	O
(	O
5.113	O
)	O
(	O
5.114	O
)	O
if	O
lf	O
n	O
=	O
clf	O
p	O
,	O
it	O
is	O
easy	O
to	O
show	O
(	O
exercise	O
5.10	O
)	O
that	O
we	O
should	O
pick	O
ˆy	O
=	O
1	O
iff	B
p	O
(	O
y	O
=	O
1|x	O
)	O
/p	O
(	O
y	O
=	O
0|x	O
)	O
>	O
τ	O
,	O
where	O
τ	O
=	O
c/	O
(	O
1	O
+	O
c	O
)	O
(	O
see	O
also	O
(	O
muller	O
et	O
al	O
.	O
2004	O
)	O
)	O
.	O
for	O
example	O
,	O
if	O
a	O
false	B
negative	I
costs	O
twice	O
as	O
much	O
as	O
false	B
positive	I
,	O
so	O
c	O
=	O
2	O
,	O
then	O
we	O
use	O
a	O
decision	B
threshold	O
of	O
2/3	O
before	O
declaring	O
a	O
positive	O
.	O
below	O
we	O
discuss	O
roc	O
curves	O
,	O
which	O
provide	O
a	O
way	O
to	O
study	O
the	O
fp-fn	O
tradeoff	O
without	O
having	O
to	O
choose	O
a	O
speciﬁc	O
threshold	O
.	O
5.7.2.1	O
roc	O
curves	O
and	O
all	O
that	O
suppose	O
we	O
are	O
solving	O
a	O
binary	O
decision	O
problem	O
,	O
such	O
as	O
classiﬁcation	B
,	O
hypothesis	O
testing	O
,	O
object	B
detection	I
,	O
etc	O
.	O
also	O
,	O
assume	O
we	O
have	O
a	O
labeled	O
data	O
set	O
,	O
d	O
=	O
{	O
(	O
xi	O
,	O
yi	O
)	O
}	O
.	O
let	O
δ	O
(	O
x	O
)	O
=	O
5.7.	O
bayesian	O
decision	B
theory	O
181	O
truth	O
estimate	O
1	O
0	O
σ	O
n+	O
=	O
t	O
p	O
+	O
f	O
n	O
n−	O
=	O
f	O
p	O
+	O
t	O
n	O
n	O
=	O
t	O
p	O
+	O
f	O
p	O
+	O
f	O
n	O
+	O
t	O
n	O
ˆn+	O
=	O
t	O
p	O
+	O
f	O
p	O
ˆn−	O
=	O
f	O
n	O
+	O
t	O
n	O
1	O
tp	O
fn	O
0	O
fp	O
tn	O
σ	O
table	O
5.2	O
quantities	O
derivable	O
from	O
a	O
confusion	B
matrix	I
.	O
n+	O
is	O
the	O
true	O
number	O
of	O
positives	O
,	O
ˆn+	O
is	O
the	O
“	O
called	O
”	O
number	O
of	O
positives	O
,	O
n−	O
is	O
the	O
true	O
number	O
of	O
negatives	O
,	O
ˆn−	O
is	O
the	O
“	O
called	O
”	O
number	O
of	O
negatives	O
.	O
y	O
=	O
1	O
t	O
p/n+=tpr=sensitivity=recall	O
ˆy	O
=	O
1	O
ˆy	O
=	O
0	O
f	O
n/n+=fnr=miss	O
rate=type	O
ii	O
y	O
=	O
0	O
f	O
p/n−=fpr=type	O
i	O
t	O
n/n−=tnr=speciﬁty	O
table	O
5.3	O
estimating	O
p	O
(	O
ˆy|y	O
)	O
from	O
a	O
confusion	B
matrix	I
.	O
abbreviations	O
:	O
fnr	O
=	O
false	B
negative	I
rate	O
,	O
fpr	O
=	O
false	B
positive	I
rate	I
,	O
tnr	O
=	O
true	O
negative	O
rate	B
,	O
tpr	O
=	O
true	B
positive	I
rate	I
.	O
i	O
(	O
f	O
(	O
x	O
)	O
>	O
τ	O
)	O
be	O
our	O
decision	B
rule	I
,	O
where	O
f	O
(	O
x	O
)	O
is	O
a	O
measure	O
of	O
conﬁdence	O
that	O
y	O
=	O
1	O
(	O
this	O
should	O
be	O
monotonically	O
related	O
to	O
p	O
(	O
y	O
=	O
1|x	O
)	O
,	O
but	O
does	O
not	O
need	O
to	O
be	O
a	O
probability	O
)	O
,	O
and	O
τ	O
is	O
some	O
threshold	O
parameter	B
.	O
for	O
each	O
given	O
value	O
of	O
τ	O
,	O
we	O
can	O
apply	O
our	O
decision	B
rule	I
and	O
count	O
the	O
number	O
of	O
true	O
positives	O
,	O
false	O
positives	O
,	O
true	O
negatives	O
,	O
and	O
false	O
negatives	O
that	O
occur	O
,	O
as	O
shown	O
in	O
table	O
5.2.	O
this	O
table	O
of	O
errors	O
is	O
called	O
a	O
confusion	B
matrix	I
.	O
from	O
this	O
table	O
,	O
we	O
can	O
compute	O
the	O
true	B
positive	I
rate	I
(	O
tpr	O
)	O
,	O
also	O
known	O
as	O
the	O
sensitivity	B
,	O
recall	B
or	O
hit	B
rate	I
,	O
by	O
using	O
t	O
p	O
r	O
=	O
t	O
p/n+	O
≈	O
p	O
(	O
ˆy	O
=	O
1|y	O
=	O
1	O
)	O
.	O
we	O
can	O
also	O
compute	O
the	O
false	B
positive	I
rate	I
(	O
fpr	O
)	O
,	O
also	O
called	O
the	O
false	B
alarm	I
rate	I
,	O
or	O
the	O
type	O
i	O
error	O
rate	O
,	O
by	O
using	O
f	O
p	O
r	O
=	O
f	O
p/n−	O
≈	O
p	O
(	O
ˆy	O
=	O
1|y	O
=	O
0	O
)	O
.	O
these	O
and	O
other	O
deﬁnitions	O
are	O
summarized	O
in	O
tables	O
5.3	O
and	O
5.4.	O
we	O
can	O
combine	O
these	O
errors	O
in	O
any	O
way	O
we	O
choose	O
to	O
compute	O
a	O
loss	B
function	I
.	O
however	O
,	O
rather	O
than	O
than	O
computing	O
the	O
tpr	O
and	O
fpr	O
for	O
a	O
ﬁxed	O
threshold	O
τ	O
,	O
we	O
can	O
run	O
our	O
detector	O
for	O
a	O
set	O
of	O
thresholds	O
,	O
and	O
then	O
plot	O
the	O
tpr	O
vs	O
fpr	O
as	O
an	O
implicit	O
function	O
of	O
τ	O
.	O
this	O
is	O
called	O
a	O
receiver	B
operating	I
characteristic	I
or	O
roc	O
curve	O
.	O
see	O
figure	O
5.15	O
(	O
a	O
)	O
for	O
an	O
example	O
.	O
any	O
system	O
can	O
achieve	O
the	O
point	O
on	O
the	O
bottom	O
left	O
,	O
(	O
f	O
p	O
r	O
=	O
0	O
,	O
t	O
p	O
r	O
=	O
0	O
)	O
,	O
by	O
setting	O
τ	O
=	O
1	O
and	O
thus	O
classifying	O
everything	O
as	O
negative	O
;	O
similarly	O
any	O
system	O
can	O
achieve	O
the	O
point	O
on	O
the	O
top	O
right	O
,	O
(	O
f	O
p	O
r	O
=	O
1	O
,	O
t	O
p	O
r	O
=	O
1	O
)	O
,	O
by	O
setting	O
τ	O
=	O
0	O
and	O
thus	O
classifying	O
everything	O
as	O
positive	O
.	O
if	O
a	O
system	O
is	O
performing	O
at	O
chance	O
level	O
,	O
then	O
we	O
can	O
achieve	O
any	O
point	O
on	O
the	O
diagonal	B
line	O
t	O
p	O
r	O
=	O
f	O
p	O
r	O
by	O
choosing	O
an	O
appropriate	O
threshold	O
.	O
a	O
system	O
that	O
perfectly	O
separates	O
the	O
positives	O
from	O
negatives	O
has	O
a	O
threshold	O
that	O
can	O
achieve	O
the	O
top	O
left	O
corner	O
,	O
(	O
f	O
p	O
r	O
=	O
0	O
,	O
t	O
p	O
r	O
=	O
1	O
)	O
;	O
by	O
varying	O
the	O
threshold	O
such	O
a	O
system	O
will	O
“	O
hug	O
”	O
the	O
left	O
axis	O
and	O
then	O
the	O
top	O
axis	O
,	O
as	O
shown	O
in	O
figure	O
5.15	O
(	O
a	O
)	O
.	O
the	O
quality	O
of	O
a	O
roc	O
curve	O
is	O
often	O
summarized	O
as	O
a	O
single	O
number	O
using	O
the	O
area	B
under	I
the	I
curve	I
or	O
auc	O
.	O
higher	O
auc	O
scores	B
are	O
better	O
;	O
the	O
maximum	O
is	O
obviously	O
1.	O
another	O
summary	O
statistic	O
that	O
is	O
used	O
is	O
the	O
equal	B
error	I
rate	I
or	O
eer	O
,	O
also	O
called	O
the	O
cross	B
over	I
rate	I
,	O
deﬁned	O
as	O
the	O
value	O
which	O
satisﬁes	O
f	O
p	O
r	O
=	O
f	O
n	O
r.	O
since	O
f	O
n	O
r	O
=	O
1	O
−	O
t	O
p	O
r	O
,	O
we	O
can	O
compute	O
the	O
eer	O
by	O
drawing	O
a	O
line	O
from	O
the	O
top	O
left	O
to	O
the	O
bottom	O
right	O
and	O
seeing	O
where	O
it	O
intersects	O
the	O
roc	O
curve	O
(	O
see	O
points	O
a	O
and	O
b	O
in	O
figure	O
5.15	O
(	O
a	O
)	O
)	O
.	O
lower	O
eer	O
scores	B
are	O
better	O
;	O
the	O
minimum	O
is	O
obviously	O
0	O
.	O
182	O
chapter	O
5.	O
bayesian	O
statistics	O
r	O
p	O
t	O
1	O
0	O
0	O
a	O
b	O
fpr	O
(	O
a	O
)	O
1	O
b	O
a	O
i	O
i	O
n	O
o	O
s	O
c	O
e	O
r	O
p	O
1	O
0	O
0	O
1	O
recall	B
(	O
b	O
)	O
figure	O
5.15	O
(	O
a	O
)	O
roc	O
curves	O
for	O
two	O
hypothetical	O
classiﬁcation	B
systems	O
.	O
a	O
is	O
better	O
than	O
b.	O
we	O
plot	O
the	O
true	B
positive	I
rate	I
(	O
tpr	O
)	O
vs	O
the	O
false	B
positive	I
rate	I
(	O
fpr	O
)	O
as	O
we	O
vary	O
the	O
threshold	O
τ	O
.	O
we	O
also	O
indicate	O
the	O
equal	B
error	I
rate	I
(	O
eer	O
)	O
with	O
the	O
red	O
and	O
blue	O
dots	O
,	O
and	O
the	O
area	B
under	I
the	I
curve	I
(	O
auc	O
)	O
for	O
classiﬁer	O
b	O
.	O
(	O
b	O
)	O
a	O
precision-recall	O
curve	O
for	O
two	O
hypothetical	O
classiﬁcation	B
systems	O
.	O
a	O
is	O
better	O
than	O
b.	O
figure	O
generated	O
by	O
prhand	O
.	O
y	O
=	O
1	O
y	O
=	O
0	O
ˆy	O
=	O
1	O
ˆy	O
=	O
0	O
t	O
p/	O
ˆn+=precision=ppv	O
f	O
p/	O
ˆn+=fdp	O
t	O
n/	O
ˆn−=npv	O
f	O
n/	O
ˆn−	O
table	O
5.4	O
estimating	O
p	O
(	O
y|ˆy	O
)	O
from	O
a	O
confusion	B
matrix	I
.	O
abbreviations	O
:	O
fdp	O
=	O
false	O
discovery	O
probability	O
,	O
npv	O
=	O
negative	O
predictive	O
value	O
,	O
ppv	O
=	O
positive	O
predictive	O
value	O
,	O
5.7.2.2	O
precision	O
recall	O
curves	O
when	O
trying	O
to	O
detect	O
a	O
rare	B
event	I
(	O
such	O
as	O
retrieving	O
a	O
relevant	O
document	O
or	O
ﬁnding	O
a	O
face	O
in	O
an	O
image	O
)	O
,	O
the	O
number	O
of	O
negatives	O
is	O
very	O
large	O
.	O
hence	O
comparing	O
t	O
p	O
r	O
=	O
t	O
p/n+	O
to	O
f	O
p	O
r	O
=	O
f	O
p/n−	O
is	O
not	O
very	O
informative	O
,	O
since	O
the	O
fpr	O
will	O
be	O
very	O
small	O
.	O
hence	O
all	O
the	O
“	O
action	B
”	O
in	O
the	O
roc	O
curve	O
will	O
occur	O
on	O
the	O
extreme	O
left	O
.	O
in	O
such	O
cases	O
,	O
it	O
is	O
common	O
to	O
plot	O
the	O
tpr	O
versus	O
the	O
number	O
of	O
false	O
positives	O
,	O
rather	O
than	O
vs	O
the	O
false	B
positive	I
rate	I
.	O
however	O
,	O
in	O
some	O
cases	O
,	O
the	O
very	O
notion	O
of	O
“	O
negative	O
”	O
is	O
not	O
well-deﬁned	O
.	O
for	O
example	O
,	O
when	O
detecting	O
objects	O
in	O
images	O
(	O
see	O
section	O
1.2.1.3	O
)	O
,	O
if	O
the	O
detector	O
works	O
by	O
classifying	O
patches	O
,	O
then	O
the	O
number	O
of	O
patches	O
examined	O
—	O
and	O
hence	O
the	O
number	O
of	O
true	O
negatives	O
—	O
is	O
a	O
parameter	B
of	O
the	O
algorithm	O
,	O
not	O
part	O
of	O
the	O
problem	O
deﬁnition	O
.	O
so	O
we	O
would	O
like	O
to	O
use	O
a	O
measure	O
that	O
only	O
talks	O
about	O
positives	O
.	O
the	O
precision	B
is	O
deﬁned	O
as	O
t	O
p/	O
ˆn+	O
=	O
p	O
(	O
y	O
=	O
1|ˆy	O
=	O
1	O
)	O
and	O
the	O
recall	B
is	O
deﬁned	O
as	O
t	O
p/n+	O
=	O
p	O
(	O
ˆy	O
=	O
1|y	O
=	O
1	O
)	O
.	O
precision	B
measures	O
what	O
fraction	O
of	O
our	O
detections	O
are	O
actually	O
positive	O
,	O
and	O
recall	B
measures	O
what	O
fraction	O
of	O
the	O
positives	O
we	O
actually	O
detected	O
.	O
if	O
ˆyi	O
∈	O
{	O
0	O
,	O
1	O
}	O
is	O
the	O
predicted	O
label	B
,	O
and	O
yi	O
∈	O
{	O
0	O
,	O
1	O
}	O
is	O
the	O
true	O
label	O
,	O
we	O
can	O
estimate	O
precision	B
and	O
recall	B
using	O
(	O
cid:4	O
)	O
i	O
yi	O
ˆyi	O
(	O
cid:4	O
)	O
i	O
ˆyi	O
(	O
cid:4	O
)	O
i	O
yi	O
ˆyi	O
(	O
cid:4	O
)	O
i	O
yi	O
p	O
=	O
,	O
r	O
=	O
(	O
5.115	O
)	O
a	O
precision	B
recall	I
curve	I
is	O
a	O
plot	O
of	O
precision	B
vs	O
recall	B
as	O
we	O
vary	O
the	O
threshold	O
τ	O
.	O
see	O
figure	O
5.15	O
(	O
b	O
)	O
.	O
hugging	O
the	O
top	O
right	O
is	O
the	O
best	O
one	O
can	O
do	O
.	O
this	O
curve	O
can	O
be	O
summarized	O
as	O
a	O
single	O
number	O
using	O
the	O
mean	B
precision	I
(	O
averaging	O
over	O
5.7.	O
bayesian	O
decision	B
theory	O
183	O
class	O
1	O
y	O
=	O
1	O
10	O
10	O
y	O
=	O
0	O
10	O
970	O
ˆy	O
=	O
1	O
ˆy	O
=	O
0	O
class	O
2	O
y	O
=	O
1	O
90	O
10	O
y	O
=	O
0	O
10	O
890	O
ˆy	O
=	O
1	O
ˆy	O
=	O
0	O
pooled	B
y	O
=	O
1	O
100	O
20	O
y	O
=	O
0	O
20	O
1860	O
ˆy	O
=	O
1	O
ˆy	O
=	O
0	O
illustration	O
of	O
the	O
difference	O
between	O
macro-	O
and	O
micro-averaging	O
.	O
y	O
is	O
the	O
true	O
label	O
,	O
and	O
ˆy	O
table	O
5.5	O
is	O
the	O
called	O
label	B
.	O
in	O
this	O
example	O
,	O
the	O
macro-averaged	O
precision	O
is	O
[	O
10/	O
(	O
10	O
+	O
10	O
)	O
+	O
90/	O
(	O
10	O
+	O
90	O
)	O
]	O
/2	O
=	O
(	O
0.5	O
+	O
0.9	O
)	O
/2	O
=	O
0.7.	O
the	O
micro-averaged	O
precision	O
is	O
100/	O
(	O
100	O
+	O
20	O
)	O
≈	O
0.83.	O
based	O
on	O
table	O
13.7	O
of	O
(	O
manning	O
et	O
al	O
.	O
2008	O
)	O
.	O
recall	B
values	O
)	O
,	O
which	O
approximates	O
the	O
area	B
under	I
the	I
curve	I
.	O
alternatively	O
,	O
one	O
can	O
quote	O
the	O
precision	B
for	O
a	O
ﬁxed	O
recall	O
level	O
,	O
such	O
as	O
the	O
precision	B
of	O
the	O
ﬁrst	O
k	O
=	O
10	O
entities	O
recalled	O
.	O
this	O
is	O
called	O
the	O
average	O
precision	O
at	O
k	O
score	O
.	O
this	O
measure	O
is	O
widely	O
used	O
when	O
evaluating	O
information	B
retrieval	I
systems	O
.	O
5.7.2.3	O
f-scores	O
*	O
for	O
a	O
ﬁxed	O
threshold	O
,	O
one	O
can	O
compute	O
a	O
single	O
precision	O
and	O
recall	B
value	O
.	O
these	O
are	O
often	O
combined	O
into	O
a	O
single	O
statistic	O
called	O
the	O
f	O
score	O
,	O
or	O
f1	O
score	O
,	O
which	O
is	O
the	O
harmonic	B
mean	I
of	O
precision	B
and	O
recall	B
:	O
f1	O
(	O
cid:2	O
)	O
2	O
1/p	O
+	O
1/r	O
=	O
2p	O
r	O
r	O
+	O
p	O
using	O
equation	O
5.115	O
,	O
we	O
can	O
write	O
this	O
as	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
n	O
f1	O
=	O
i=1	O
yi	O
ˆyi	O
(	O
cid:4	O
)	O
n	O
2	O
i=1	O
yi	O
+	O
i=1	O
ˆyi	O
(	O
5.116	O
)	O
(	O
5.117	O
)	O
this	O
is	O
a	O
widely	O
used	O
measure	O
in	O
information	B
retrieval	I
systems	O
.	O
to	O
understand	O
why	O
we	O
use	O
the	O
harmonic	B
mean	I
instead	O
of	O
the	O
arithmetic	O
mean	B
,	O
(	O
p	O
+	O
r	O
)	O
/2	O
,	O
consider	O
the	O
following	O
scenario	O
.	O
suppose	O
we	O
recall	B
all	O
entries	O
,	O
so	O
r	O
=	O
1.	O
the	O
precision	B
will	O
be	O
−4	O
.	O
the	O
given	O
by	O
the	O
prevalence	B
,	O
p	O
(	O
y	O
=	O
1	O
)	O
.	O
suppose	O
the	O
prevalence	B
is	O
low	O
,	O
say	O
p	O
(	O
y	O
=	O
1	O
)	O
=	O
10	O
−4	O
+	O
1	O
)	O
/2	O
≈	O
50	O
%	O
.	O
by	O
contrast	O
,	O
the	O
arithmetic	O
mean	B
of	O
p	O
and	O
r	O
is	O
given	O
by	O
(	O
p	O
+	O
r	O
)	O
/2	O
=	O
(	O
10	O
1+10−4	O
≈	O
0.2	O
%	O
.	O
harmonic	B
mean	I
of	O
this	O
strategy	O
is	O
only	O
2×10−4×1	O
in	O
the	O
multi-class	O
case	O
(	O
e.g.	O
,	O
for	O
document	B
classiﬁcation	I
problems	O
)	O
,	O
there	O
are	O
two	O
ways	O
to	O
generalize	B
f1	O
scores	B
.	O
the	O
ﬁrst	O
is	O
called	O
macro-averaged	O
f1	O
,	O
and	O
is	O
deﬁned	O
as	O
c=1	O
f1	O
(	O
c	O
)	O
/c	O
,	O
where	O
f1	O
(	O
c	O
)	O
is	O
the	O
f1	O
score	O
obtained	O
on	O
the	O
task	O
of	O
distinguishing	O
class	O
c	O
from	O
all	O
the	O
others	O
.	O
the	O
other	O
is	O
called	O
micro-averaged	O
f1	O
,	O
and	O
is	O
deﬁned	O
as	O
the	O
f1	O
score	O
where	O
we	O
pool	O
all	O
the	O
counts	O
from	O
each	O
class	O
’	O
s	O
contingency	B
table	I
.	O
(	O
cid:4	O
)	O
c	O
table	O
5.5	O
gives	O
a	O
worked	O
example	O
that	O
illustrates	O
the	O
difference	O
.	O
we	O
see	O
that	O
the	O
precision	B
of	O
class	O
1	O
is	O
0.5	O
,	O
and	O
of	O
class	O
2	O
is	O
0.9.	O
the	O
macro-averaged	O
precision	O
is	O
therefore	O
0.7	O
,	O
whereas	O
the	O
micro-averaged	O
precision	O
is	O
0.83.	O
the	O
latter	O
is	O
much	O
closer	O
to	O
the	O
precision	B
of	O
class	O
2	O
than	O
to	O
the	O
precision	B
of	O
class	O
1	O
,	O
since	O
class	O
2	O
is	O
ﬁve	O
times	O
larger	O
than	O
class	O
1.	O
to	O
give	O
equal	O
weight	O
to	O
each	O
class	O
,	O
use	O
macro-averaging	O
.	O
184	O
chapter	O
5.	O
bayesian	O
statistics	O
5.7.2.4	O
false	O
discovery	O
rates	O
*	O
suppose	O
we	O
are	O
trying	O
to	O
discover	O
a	O
rare	O
phenomenon	O
using	O
some	O
kind	O
of	O
high	B
throughput	I
measurement	O
device	O
,	O
such	O
as	O
a	O
gene	O
expression	O
micro	O
array	O
,	O
or	O
a	O
radio	O
telescope	O
.	O
we	O
will	O
need	O
to	O
make	O
many	O
binary	O
decisions	O
of	O
the	O
form	O
p	O
(	O
yi	O
=	O
1|d	O
)	O
>	O
τ	O
,	O
whered	O
=	O
{	O
xi	O
}	O
n	O
i=1	O
and	O
n	O
may	O
be	O
large	O
.	O
this	O
is	O
called	O
multiple	B
hypothesis	I
testing	I
.	O
note	O
that	O
the	O
difference	O
from	O
standard	O
binary	O
classiﬁcation	B
is	O
that	O
we	O
are	O
classifying	O
yi	O
based	O
on	O
all	O
the	O
data	O
,	O
not	O
just	O
based	O
on	O
xi	O
.	O
so	O
this	O
is	O
a	O
simultaneous	O
classiﬁcation	O
problem	O
,	O
where	O
we	O
might	O
hope	O
to	O
do	O
better	O
than	O
a	O
series	O
of	O
individual	O
classiﬁcation	B
problems	O
.	O
how	O
should	O
we	O
set	O
the	O
threshold	O
τ	O
?	O
a	O
natural	O
approach	O
is	O
to	O
try	O
to	O
minimize	O
the	O
expected	O
number	O
of	O
false	O
positives	O
.	O
in	O
the	O
bayesian	O
approach	O
,	O
this	O
can	O
be	O
computed	O
as	O
follows	O
:	O
f	O
d	O
(	O
τ	O
,	O
d	O
)	O
(	O
cid:2	O
)	O
(	O
cid:6	O
)	O
i	O
(	O
1	O
−	O
pi	O
)	O
!	O
``	O
#	O
$	O
pr	O
.	O
error	O
!	O
i	O
(	O
pi	O
>	O
τ	O
)	O
''	O
#	O
$	O
discovery	O
where	O
pi	O
(	O
cid:2	O
)	O
p	O
(	O
yi	O
=	O
1|d	O
)	O
is	O
your	O
belief	O
that	O
this	O
object	O
exhibits	O
the	O
phenomenon	O
in	O
question	O
.	O
we	O
then	O
deﬁne	O
the	O
posterior	O
expected	O
false	O
discovery	O
rate	B
as	O
follows	O
:	O
f	O
dr	O
(	O
τ	O
,	O
d	O
)	O
(	O
cid:2	O
)	O
f	O
d	O
(	O
τ	O
,	O
d	O
)	O
/n	O
(	O
τ	O
,	O
d	O
)	O
(	O
cid:4	O
)	O
where	O
n	O
(	O
τ	O
,	O
d	O
)	O
=	O
i	O
i	O
(	O
pi	O
>	O
τ	O
)	O
is	O
the	O
number	O
of	O
discovered	O
items	O
.	O
given	O
a	O
desired	O
fdr	O
tolerance	O
,	O
say	O
α	O
=	O
0.05	O
,	O
one	O
can	O
then	O
adapt	O
τ	O
to	O
achieve	O
this	O
;	O
this	O
is	O
called	O
the	O
direct	B
posterior	I
probability	I
approach	I
to	O
controlling	O
the	O
fdr	O
(	O
newton	O
et	O
al	O
.	O
2004	O
;	O
muller	O
et	O
al	O
.	O
2004	O
)	O
.	O
in	O
order	O
to	O
control	O
the	O
fdr	O
it	O
is	O
very	O
helpful	O
to	O
estimate	O
the	O
pi	O
’	O
s	O
jointly	O
(	O
e.g.	O
,	O
using	O
a	O
hierar-	O
chical	O
bayesian	O
model	O
,	O
as	O
in	O
section	O
5.5	O
)	O
,	O
rather	O
than	O
independently	O
.	O
this	O
allows	O
the	O
pooling	O
of	O
statistical	O
strength	O
,	O
and	O
thus	O
lower	O
fdr	O
.	O
see	O
e.g.	O
,	O
(	O
berry	O
and	O
hochberg	O
1999	O
)	O
for	O
more	O
information	B
.	O
(	O
5.118	O
)	O
(	O
5.119	O
)	O
5.7.3	O
other	O
topics	O
*	O
in	O
this	O
section	O
,	O
we	O
brieﬂy	O
mention	O
a	O
few	O
other	O
topics	O
related	O
to	O
bayesian	O
decision	B
theory	O
.	O
we	O
do	O
not	O
have	O
space	O
to	O
go	O
into	O
detail	O
,	O
but	O
we	O
include	O
pointers	O
to	O
the	O
relevant	O
literature	O
.	O
5.7.3.1	O
contextual	O
bandits	O
a	O
one-armed	B
bandit	I
is	O
a	O
colloquial	O
term	O
for	O
a	O
slot	B
machine	I
,	O
found	O
in	O
casinos	O
around	O
the	O
world	O
.	O
the	O
game	O
is	O
this	O
:	O
you	O
insert	O
some	O
money	O
,	O
pull	O
an	O
arm	O
,	O
and	O
wait	O
for	O
the	O
machine	O
to	O
stop	O
;	O
if	O
you	O
’	O
re	O
lucky	O
,	O
you	O
win	O
some	O
money	O
.	O
now	O
imagine	O
there	O
is	O
a	O
bank	O
of	O
k	O
such	O
machines	O
to	O
choose	O
from	O
.	O
which	O
one	O
should	O
you	O
use	O
?	O
this	O
is	O
called	O
a	O
multi-armed	B
bandit	I
,	O
and	O
can	O
be	O
modeled	O
(	O
cid:15	O
)	O
using	O
bayesian	O
decision	B
theory	O
:	O
there	O
are	O
k	O
possible	O
actions	B
,	O
and	O
each	O
action	B
has	O
an	O
unknown	B
reward	O
(	O
payoff	O
function	O
)	O
rk	O
.	O
by	O
maintaining	O
a	O
belief	B
state	I
,	O
p	O
(	O
r1	O
:	O
k|d	O
)	O
=	O
k	O
p	O
(	O
rk|d	O
)	O
,	O
one	O
can	O
devise	O
an	O
optimal	O
policy	O
;	O
this	O
can	O
be	O
compiled	O
into	O
a	O
series	O
of	O
gittins	O
indices	O
(	O
gittins	O
1989	O
)	O
.	O
this	O
optimally	O
solves	O
the	O
exploration-exploitation	B
tradeoff	O
,	O
which	O
speciﬁes	O
how	O
many	O
times	O
one	O
should	O
try	O
each	O
action	B
before	O
deciding	O
to	O
go	O
with	O
the	O
winner	O
.	O
now	O
consider	O
an	O
extension	B
where	O
each	O
arm	O
,	O
and	O
the	O
player	O
,	O
has	O
an	O
associated	O
feature	O
vector	O
;	O
call	O
all	O
these	O
features	B
x.	O
this	O
is	O
called	O
a	O
contextual	B
bandit	I
(	O
see	O
e.g.	O
,	O
(	O
sarkar	O
1991	O
;	O
scott	O
2010	O
;	O
li	O
et	O
al	O
.	O
2011	O
)	O
)	O
.	O
for	O
example	O
,	O
the	O
“	O
arms	O
”	O
could	O
represent	O
ads	O
or	O
news	O
articles	O
which	O
we	O
want	O
to	O
show	O
to	O
the	O
user	O
,	O
and	O
the	O
features	B
could	O
represent	O
properties	O
of	O
these	O
ads	O
or	O
articles	O
,	O
such	O
5.7.	O
bayesian	O
decision	B
theory	O
185	O
as	O
a	O
bag	B
of	I
words	I
,	O
as	O
well	O
as	O
properties	O
of	O
the	O
user	O
,	O
such	O
as	O
demographics	O
.	O
if	O
we	O
assume	O
a	O
linear	O
model	O
for	O
reward	B
,	O
rk	O
=	O
θt	O
k	O
x	O
,	O
we	O
can	O
maintain	O
a	O
distribution	O
over	O
the	O
parameters	O
of	O
each	O
arm	O
,	O
p	O
(	O
θk|d	O
)	O
,	O
where	O
d	O
is	O
a	O
series	O
of	O
tuples	B
of	O
the	O
form	O
(	O
a	O
,	O
x	O
,	O
r	O
)	O
,	O
which	O
speciﬁes	O
which	O
arm	O
was	O
pulled	O
,	O
what	O
its	O
features	B
were	O
,	O
and	O
what	O
the	O
resulting	O
outcome	O
was	O
(	O
e.g.	O
,	O
r	O
=	O
1	O
if	O
the	O
user	O
clicked	O
on	O
the	O
ad	O
,	O
and	O
r	O
=	O
0	O
otherwise	O
)	O
.	O
we	O
discuss	O
ways	O
to	O
compute	O
p	O
(	O
θk|d	O
)	O
from	O
linear	O
and	O
logistic	B
regression	I
models	O
in	O
later	O
chapters	O
.	O
given	O
the	O
posterior	O
,	O
we	O
must	O
decide	O
what	O
action	B
to	O
take	O
.	O
one	O
common	O
heuristic	O
,	O
known	O
as	O
ucb	O
(	O
which	O
stands	O
for	O
“	O
upper	O
conﬁdence	O
bound	O
”	O
)	O
is	O
to	O
take	O
the	O
action	B
which	O
maximizes	O
k∗	O
k	O
=	O
k=1	O
argmax	O
μk	O
+	O
λσk	O
(	O
5.120	O
)	O
k	O
=	O
var	O
[	O
rk|d	O
]	O
and	O
λ	O
is	O
a	O
tuning	O
parameter	B
that	O
trades	O
off	O
exploration	O
where	O
μk	O
=	O
e	O
[	O
rk|d	O
]	O
,	O
σ2	O
and	O
exploitation	O
.	O
the	O
intuition	O
is	O
that	O
we	O
should	O
pick	O
actions	B
about	O
which	O
we	O
believe	O
are	O
good	O
(	O
μk	O
is	O
large	O
)	O
,	O
and/	O
or	O
actions	B
about	O
which	O
we	O
are	O
uncertain	O
(	O
σk	O
is	O
large	O
)	O
.	O
an	O
even	O
simpler	O
method	O
,	O
known	O
as	O
thompson	O
sampling	O
,	O
is	O
as	O
follows	O
.	O
at	O
each	O
step	O
,	O
we	O
pick	O
(	O
cid:11	O
)	O
action	B
k	O
with	O
a	O
probability	O
that	O
is	O
equal	O
to	O
its	O
probability	O
of	O
being	O
the	O
optimal	B
action	I
:	O
i	O
(	O
e	O
[	O
r|a	O
,	O
x	O
,	O
θ	O
]	O
=	O
max	O
a	O
(	O
cid:2	O
)	O
e	O
[	O
r|a	O
(	O
cid:2	O
)	O
,	O
x	O
,	O
θ	O
]	O
)	O
p	O
(	O
θ|d	O
)	O
dθ	O
pk	O
=	O
(	O
5.121	O
)	O
we	O
can	O
approximate	O
this	O
by	O
drawing	O
a	O
single	O
sample	O
from	O
the	O
posterior	O
,	O
θt	O
∼	O
p	O
(	O
θ|d	O
)	O
,	O
and	O
then	O
choosing	O
k∗	O
.	O
despite	O
its	O
simplicity	O
,	O
this	O
has	O
been	O
shown	O
to	O
work	O
quite	O
=	O
argmaxk	O
e	O
well	O
(	O
chapelle	O
and	O
li	O
2011	O
)	O
.	O
r|x	O
,	O
k	O
,	O
θ	O
t	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
5.7.3.2	O
utility	O
theory	O
suppose	O
we	O
are	O
a	O
doctor	O
trying	O
to	O
decide	O
whether	O
to	O
operate	O
on	O
a	O
patient	O
or	O
not	O
.	O
we	O
imagine	O
there	O
are	O
3	O
states	O
of	O
nature	O
:	O
the	O
patient	O
has	O
no	O
cancer	O
,	O
the	O
patient	O
has	O
lung	O
cancer	O
,	O
or	O
the	O
patient	O
has	O
breast	O
cancer	O
.	O
since	O
the	O
action	B
and	O
state	B
space	I
is	O
discrete	B
,	O
we	O
can	O
represent	O
the	O
loss	B
function	I
l	O
(	O
θ	O
,	O
a	O
)	O
as	O
a	O
loss	B
matrix	I
,	O
such	O
as	O
the	O
following	O
:	O
surgery	O
no	O
surgery	O
no	O
cancer	O
lung	O
cancer	O
breast	O
cancer	O
20	O
10	O
10	O
0	O
50	O
60	O
these	O
numbers	O
reﬂects	O
the	O
fact	O
that	O
not	O
performing	O
surgery	O
when	O
the	O
patient	O
has	O
cancer	O
is	O
very	O
bad	O
(	O
loss	B
of	O
50	O
or	O
60	O
,	O
depending	O
on	O
the	O
type	O
of	O
cancer	O
)	O
,	O
since	O
the	O
patient	O
might	O
die	O
;	O
not	O
performing	O
surgery	O
when	O
the	O
patient	O
does	O
not	O
have	O
cancer	O
incurs	O
no	O
loss	O
(	O
0	O
)	O
;	O
performing	O
surgery	O
when	O
the	O
patient	O
does	O
not	O
have	O
cancer	O
is	O
wasteful	O
(	O
loss	B
of	O
20	O
)	O
;	O
and	O
performing	O
surgery	O
when	O
the	O
patient	O
does	O
have	O
cancer	O
is	O
painful	O
but	O
necessary	O
(	O
10	O
)	O
.	O
it	O
is	O
natural	O
to	O
ask	O
where	O
these	O
numbers	O
come	O
from	O
.	O
ultimately	O
they	O
represent	O
the	O
personal	O
preferences	B
or	O
values	O
of	O
a	O
ﬁctitious	O
doctor	O
,	O
and	O
are	O
somewhat	O
arbitrary	O
:	O
just	O
as	O
some	O
people	O
prefer	O
chocolate	O
ice	O
cream	O
and	O
others	O
prefer	O
vanilla	O
,	O
there	O
is	O
no	O
such	O
thing	O
as	O
the	O
“	O
right	O
”	O
loss/	O
utility	B
function	I
.	O
however	O
,	O
it	O
can	O
be	O
shown	O
(	O
see	O
e.g.	O
,	O
(	O
degroot	O
1970	O
)	O
)	O
that	O
any	O
set	O
of	O
consistent	B
preferences	O
can	O
be	O
converted	O
to	O
a	O
scalar	O
loss/	O
utility	B
function	I
.	O
note	O
that	O
utility	O
can	O
be	O
measured	O
on	O
an	O
arbitrary	O
scale	O
,	O
such	O
as	O
dollars	O
,	O
since	O
it	O
is	O
only	O
relative	O
values	O
that	O
matter.6	O
6.	O
people	O
are	O
often	O
squeamish	O
about	O
talking	O
about	O
human	O
lives	O
in	O
monetary	O
terms	O
,	O
but	O
all	O
decision	O
making	O
requires	O
186	O
chapter	O
5.	O
bayesian	O
statistics	O
5.7.3.3	O
sequential	B
decision	O
theory	O
so	O
far	O
,	O
we	O
have	O
concentrated	O
on	O
one-shot	O
decision	O
problems	O
,	O
where	O
we	O
only	O
have	O
to	O
make	O
one	O
decision	B
and	O
then	O
the	O
game	O
ends	O
.	O
in	O
setion	O
10.6	O
,	O
we	O
will	O
generalize	B
this	O
to	O
multi-stage	B
or	O
sequential	B
decision	O
problems	O
.	O
such	O
problems	O
frequently	O
arise	O
in	O
many	O
business	O
and	O
engineering	O
settings	O
.	O
this	O
is	O
closely	O
related	O
to	O
the	O
problem	O
of	O
reinforcement	B
learning	I
.	O
however	O
,	O
further	O
discussion	O
of	O
this	O
point	O
is	O
beyond	O
the	O
scope	B
of	O
this	O
book	O
.	O
exercises	O
exercise	O
5.1	O
proof	O
that	O
a	O
mixture	B
of	I
conjugate	I
priors	I
is	O
indeed	O
conjugate	O
derive	O
equation	O
5.69.	O
exercise	O
5.2	O
optimal	O
threshold	O
on	O
classiﬁcation	B
probability	O
consider	O
a	O
case	O
where	O
we	O
have	O
learned	O
a	O
conditional	B
probability	I
distribution	I
p	O
(	O
y|x	O
)	O
.	O
suppose	O
there	O
are	O
only	O
two	O
classes	O
,	O
and	O
let	O
p0	O
=	O
p	O
(	O
y	O
=	O
0|x	O
)	O
and	O
p1	O
=	O
p	O
(	O
y	O
=	O
1|x	O
)	O
.	O
consider	O
the	O
loss	B
matrix	I
below	O
:	O
predicted	O
label	B
ˆy	O
0	O
1	O
true	O
label	O
y	O
1	O
λ01	O
0	O
0	O
0	O
λ10	O
a.	O
show	O
that	O
the	O
decision	B
ˆy	O
that	O
minimizes	O
the	O
expected	O
loss	O
is	O
equivalent	O
to	O
setting	O
a	O
probability	O
threshold	O
θ	O
and	O
predicting	O
ˆy	O
=	O
0	O
if	O
p1	O
<	O
θ	O
and	O
ˆy	O
=	O
1	O
if	O
p1	O
≥	O
θ.	O
what	O
is	O
θ	O
as	O
a	O
function	O
of	O
λ01	O
and	O
λ10	O
?	O
(	O
show	O
your	O
work	O
.	O
)	O
b.	O
show	O
a	O
loss	B
matrix	I
where	O
the	O
threshold	O
is	O
0.1	O
.	O
(	O
show	O
your	O
work	O
.	O
)	O
exercise	O
5.3	O
reject	O
option	O
in	O
classiﬁers	O
(	O
source	O
:	O
(	O
duda	O
et	O
al	O
.	O
2001	O
,	O
q2.13	O
)	O
.	O
)	O
in	O
many	O
classiﬁcation	B
problems	O
one	O
has	O
the	O
option	O
either	O
of	O
assigning	O
x	O
to	O
class	O
j	O
or	O
,	O
if	O
you	O
are	O
too	O
uncertain	O
,	O
of	O
choosing	O
the	O
reject	O
option	O
.	O
if	O
the	O
cost	O
for	O
rejects	O
is	O
less	O
than	O
the	O
cost	O
of	O
falsely	O
classifying	O
the	O
object	O
,	O
it	O
may	O
be	O
the	O
optimal	B
action	I
.	O
let	O
αi	O
mean	B
you	O
choose	O
action	B
i	O
,	O
for	O
i	O
=	O
1	O
:	O
c	O
+	O
1	O
,	O
where	O
c	O
is	O
the	O
number	O
of	O
classes	O
and	O
c	O
+	O
1	O
is	O
the	O
reject	B
action	I
.	O
let	O
y	O
=	O
j	O
be	O
the	O
true	O
(	O
but	O
unknown	B
)	O
state	B
of	O
nature	O
.	O
deﬁne	O
the	O
loss	B
function	I
as	O
follows	O
λ	O
(	O
αi|y	O
=	O
j	O
)	O
=	O
if	O
i	O
=	O
j	O
and	O
i	O
,	O
j	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
}	O
⎧⎨	O
⎩	O
0	O
λr	O
λs	O
if	O
i	O
=	O
c	O
+	O
1	O
otherwise	O
(	O
5.122	O
)	O
in	O
otherwords	O
,	O
you	O
incur	O
0	O
loss	B
if	O
you	O
correctly	O
classify	O
,	O
you	O
incur	O
λr	O
loss	B
(	O
cost	O
)	O
if	O
you	O
choose	O
the	O
reject	O
option	O
,	O
and	O
you	O
incur	O
λs	O
loss	B
(	O
cost	O
)	O
if	O
you	O
make	O
a	O
substitution	O
error	O
(	O
misclassiﬁcation	O
)	O
.	O
tradeoffs	O
,	O
and	O
one	O
needs	O
to	O
use	O
some	O
kind	O
of	O
“	O
currency	O
”	O
to	O
compare	O
different	O
courses	O
of	O
action	B
.	O
insurance	O
companies	O
do	O
this	O
all	O
the	O
time	O
.	O
ross	O
schachter	O
,	O
a	O
decision	B
theorist	O
at	O
stanford	O
university	O
,	O
likes	O
to	O
tell	O
a	O
story	O
of	O
a	O
school	O
board	O
who	O
rejected	O
a	O
study	O
on	O
absestos	O
removal	O
from	O
schools	O
because	O
it	O
performed	O
a	O
cost-beneﬁt	B
analysis	I
,	O
which	O
was	O
considered	O
“	O
inhumane	O
”	O
because	O
they	O
put	O
a	O
dollar	O
value	O
on	O
children	B
’	O
s	O
health	O
;	O
the	O
result	O
of	O
rejecting	O
the	O
report	O
was	O
that	O
the	O
absestos	O
was	O
not	O
removed	O
,	O
which	O
is	O
surely	O
more	O
“	O
inhumane	O
”	O
.	O
in	O
medical	O
domains	O
,	O
one	O
often	O
measures	O
utility	O
in	O
terms	O
of	O
qaly	O
,	O
or	O
quality-adjusted	O
life-years	O
,	O
instead	O
of	O
dollars	O
,	O
but	O
it	O
’	O
s	O
the	O
same	O
idea	O
.	O
of	O
course	O
,	O
even	O
if	O
you	O
do	O
not	O
explicitly	O
specify	O
how	O
much	O
you	O
value	O
different	O
people	O
’	O
s	O
lives	O
,	O
your	O
behavior	O
will	O
reveal	O
your	O
implicit	O
values/	O
preferences	B
,	O
and	O
these	O
preferences	B
can	O
then	O
be	O
converted	O
to	O
a	O
real-valued	O
scale	O
,	O
such	O
as	O
dollars	O
or	O
qaly	O
.	O
inferring	O
a	O
utility	B
function	I
from	O
behavior	O
is	O
called	O
inverse	B
reinforcement	I
learning	I
.	O
5.7.	O
bayesian	O
decision	B
theory	O
187	O
decision	B
ˆy	O
predict	O
0	O
predict	O
1	O
reject	O
true	O
label	B
y	O
1	O
10	O
0	O
3	O
0	O
0	O
10	O
3	O
a.	O
show	O
that	O
the	O
minimum	O
risk	O
is	O
obtained	O
if	O
we	O
decide	O
y	O
=	O
j	O
if	O
p	O
(	O
y	O
=	O
j|x	O
)	O
≥	O
p	O
(	O
y	O
=	O
k|x	O
)	O
for	O
all	O
k	O
(	O
i.e.	O
,	O
j	O
is	O
the	O
most	O
probable	O
class	O
)	O
and	O
if	O
p	O
(	O
y	O
=	O
j|x	O
)	O
≥	O
1	O
−	O
λr	O
λs	O
;	O
otherwise	O
we	O
decide	O
to	O
reject	O
.	O
b.	O
describe	O
qualitatively	O
what	O
happens	O
as	O
λr/λs	O
is	O
increased	O
from	O
0	O
to	O
1	O
(	O
i.e.	O
,	O
the	O
relative	O
cost	O
of	O
rejection	O
increases	O
)	O
.	O
exercise	O
5.4	O
more	O
reject	O
options	O
in	O
many	O
applications	O
,	O
the	O
classiﬁer	O
is	O
allowed	O
to	O
“	O
reject	O
”	O
a	O
test	O
example	O
rather	O
than	O
classifying	O
it	O
into	O
one	O
of	O
the	O
classes	O
.	O
consider	O
,	O
for	O
example	O
,	O
a	O
case	O
in	O
which	O
the	O
cost	O
of	O
a	O
misclassiﬁcation	O
is	O
$	O
10	O
but	O
the	O
cost	O
of	O
having	O
a	O
human	O
manually	O
make	O
the	O
decison	O
is	O
only	O
$	O
3	O
.	O
we	O
can	O
formulate	O
this	O
as	O
the	O
following	O
loss	B
matrix	I
:	O
a.	O
suppose	O
p	O
(	O
y	O
=	O
1|x	O
)	O
is	O
predicted	O
to	O
be	O
0.2.	O
which	O
decision	B
minimizes	O
the	O
expected	O
loss	O
?	O
b.	O
now	O
suppose	O
p	O
(	O
y	O
=	O
1|x	O
)	O
=0.4	O
.	O
now	O
which	O
decision	B
minimizes	O
the	O
expected	O
loss	O
?	O
c.	O
show	O
that	O
in	O
general	O
,	O
for	O
this	O
loss	B
matrix	I
,	O
but	O
for	O
any	O
posterior	O
distribution	O
,	O
there	O
will	O
be	O
two	O
thresholds	O
θ0	O
and	O
θ1	O
such	O
that	O
the	O
optimal	O
decisionn	O
is	O
to	O
predict	O
0	O
if	O
p1	O
<	O
θ0	O
,	O
reject	O
if	O
θ0	O
≤	O
p1	O
≤	O
θ1	O
,	O
and	O
predict	O
1	O
if	O
p1	O
>	O
θ1	O
(	O
where	O
p1	O
=	O
p	O
(	O
y	O
=	O
1|x	O
)	O
)	O
.	O
what	O
are	O
these	O
thresholds	O
?	O
exercise	O
5.5	O
newsvendor	O
problem	O
consider	O
the	O
following	O
classic	O
problem	O
in	O
decision	B
theory/	O
economics	O
.	O
suppose	O
you	O
are	O
trying	O
to	O
decide	O
how	O
much	O
quantity	O
q	O
of	O
some	O
product	O
(	O
e.g.	O
,	O
newspapers	O
)	O
to	O
buy	O
to	O
maximize	O
your	O
proﬁts	O
.	O
the	O
optimal	O
amount	O
will	O
depend	O
on	O
how	O
much	O
demand	O
d	O
you	O
think	O
there	O
is	O
for	O
your	O
product	O
,	O
as	O
well	O
as	O
its	O
cost	O
to	O
you	O
c	O
and	O
its	O
selling	O
price	O
p	O
.	O
suppose	O
d	O
is	O
unknown	B
but	O
has	O
pdf	B
f	O
(	O
d	O
)	O
and	O
cdf	B
f	O
(	O
d	O
)	O
.	O
we	O
can	O
evaluate	O
the	O
expected	B
proﬁt	I
by	O
considering	O
two	O
cases	O
:	O
if	O
d	O
>	O
q	O
,	O
then	O
we	O
sell	O
all	O
q	O
items	O
,	O
and	O
make	O
proﬁt	O
π	O
=	O
(	O
p	O
−	O
c	O
)	O
q	O
;	O
but	O
if	O
d	O
<	O
q	O
,	O
we	O
only	O
sell	O
d	O
items	O
,	O
at	O
proﬁt	O
(	O
p	O
−	O
c	O
)	O
d	O
,	O
but	O
have	O
wasted	O
c	O
(	O
q	O
−	O
d	O
)	O
on	O
the	O
unsold	O
items	O
.	O
so	O
the	O
expected	B
proﬁt	I
if	O
we	O
buy	O
quantity	O
q	O
is	O
(	O
p	O
−	O
c	O
)	O
df	O
(	O
d	O
)	O
−	O
(	O
p	O
−	O
c	O
)	O
qf	O
(	O
d	O
)	O
dd	O
+	O
c	O
(	O
q	O
−	O
d	O
)	O
f	O
(	O
d	O
)	O
dd	O
(	O
cid:5	O
)	O
∞	O
(	O
cid:5	O
)	O
q	O
(	O
cid:5	O
)	O
q	O
eπ	O
(	O
q	O
)	O
=	O
q	O
0	O
0	O
simplify	O
this	O
expression	O
,	O
and	O
then	O
take	O
derivatives	O
wrt	O
q	O
to	O
show	O
that	O
the	O
optimal	O
quantity	O
q∗	O
maximizes	O
the	O
expected	B
proﬁt	I
)	O
satisﬁes	O
f	O
(	O
q∗	O
)	O
=	O
p	O
−	O
c	O
p	O
(	O
5.123	O
)	O
(	O
which	O
(	O
5.124	O
)	O
exercise	O
5.6	O
bayes	O
factors	B
and	O
roc	O
curves	O
let	O
b	O
=	O
p	O
(	O
d|h1	O
)	O
/p	O
(	O
d|h0	O
)	O
be	O
the	O
bayes	O
factor	B
in	O
favor	O
of	O
model	O
1.	O
suppose	O
we	O
plot	O
two	O
roc	O
curves	O
,	O
one	O
computed	O
by	O
thresholding	O
b	O
,	O
and	O
the	O
other	O
computed	O
by	O
thresholding	O
p	O
(	O
h1|d	O
)	O
.	O
will	O
they	O
be	O
the	O
same	O
or	O
different	O
?	O
explain	O
why	O
.	O
exercise	O
5.7	O
bayes	O
model	O
averaging	O
helps	O
predictive	B
accuracy	O
let	O
δ	O
be	O
a	O
quantity	O
that	O
we	O
want	O
to	O
predict	O
,	O
let	O
d	O
be	O
the	O
observed	O
data	O
and	O
m	O
be	O
a	O
ﬁnite	O
set	O
of	O
models	O
.	O
suppose	O
our	O
action	B
is	O
to	O
provide	O
a	O
probabilistic	O
prediction	O
p	O
(	O
)	O
,	O
and	O
the	O
loss	B
function	I
is	O
l	O
(	O
δ	O
,	O
p	O
(	O
)	O
)	O
=	O
188	O
chapter	O
5.	O
bayesian	O
statistics	O
−	O
log	O
p	O
(	O
δ	O
)	O
.	O
we	O
can	O
either	O
perform	O
bayes	O
model	O
averaging	O
and	O
predict	O
using	O
pbm	O
a	O
(	O
δ	O
)	O
=	O
p	O
(	O
δ|m	O
,	O
d	O
)	O
p	O
(	O
m|d	O
)	O
(	O
cid:12	O
)	O
m∈m	O
or	O
we	O
could	O
predict	O
using	O
any	O
single	O
model	O
(	O
a	O
plugin	O
approximation	O
)	O
show	O
that	O
,	O
for	O
all	O
models	O
m	O
∈	O
m	O
,	O
the	O
posterior	B
expected	I
loss	I
using	O
bma	O
is	O
lower	O
,	O
i.e.	O
,	O
pm	O
(	O
δ	O
)	O
=	O
p	O
(	O
δ|m	O
,	O
d	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
e	O
l	O
(	O
δ	O
,	O
pbm	O
a	O
)	O
≤	O
e	O
[	O
l	O
(	O
δ	O
,	O
pm	O
)	O
]	O
where	O
the	O
expectation	O
over	O
δ	O
is	O
with	O
respect	O
to	O
p	O
(	O
δ|d	O
)	O
=	O
p	O
(	O
δ|m	O
,	O
d	O
)	O
p	O
(	O
m|d	O
)	O
(	O
cid:12	O
)	O
m∈m	O
(	O
5.125	O
)	O
(	O
5.126	O
)	O
(	O
5.127	O
)	O
(	O
5.128	O
)	O
hint	O
:	O
use	O
the	O
non-negativity	O
of	O
the	O
kl	O
divergence	O
.	O
exercise	O
5.8	O
mle	O
and	O
model	B
selection	I
for	O
a	O
2d	O
discrete	B
distribution	O
(	O
source	O
:	O
jaakkola	O
.	O
)	O
let	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
denote	O
the	O
result	O
of	O
a	O
coin	O
toss	O
(	O
x	O
=	O
0	O
for	O
tails	O
,	O
x	O
=	O
1	O
for	O
heads	O
)	O
.	O
the	O
coin	O
is	O
potentially	O
biased	O
,	O
so	O
that	O
heads	O
occurs	O
with	O
probability	O
θ1	O
.	O
suppose	O
that	O
someone	O
else	O
observes	O
the	O
coin	O
ﬂip	O
and	O
reports	O
to	O
you	O
the	O
outcome	O
,	O
y.	O
but	O
this	O
person	O
is	O
unreliable	O
and	O
only	O
reports	O
the	O
result	O
correctly	O
with	O
probability	O
θ2	O
;	O
i.e.	O
,	O
p	O
(	O
y|x	O
,	O
θ2	O
)	O
is	O
given	O
by	O
x	O
=	O
0	O
x	O
=	O
1	O
y	O
=	O
0	O
θ2	O
1	O
−	O
θ2	O
y	O
=	O
1	O
1	O
−	O
θ2	O
θ2	O
assume	O
that	O
θ2	O
is	O
independent	O
of	O
x	O
and	O
θ1	O
.	O
a.	O
write	O
down	O
the	O
joint	B
probability	I
distribution	I
p	O
(	O
x	O
,	O
y|θ	O
)	O
as	O
a	O
2	O
×	O
2	O
table	O
,	O
in	O
terms	O
of	O
θ	O
=	O
(	O
θ1	O
,	O
θ2	O
)	O
.	O
b.	O
suppose	O
have	O
the	O
following	O
dataset	O
:	O
x	O
=	O
(	O
1	O
,	O
1	O
,	O
0	O
,	O
1	O
,	O
1	O
,	O
0	O
,	O
0	O
)	O
,	O
y	O
=	O
(	O
1	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
1	O
,	O
0	O
,	O
1	O
)	O
.	O
what	O
are	O
the	O
mles	O
for	O
θ1	O
and	O
θ2	O
?	O
justify	O
your	O
answer	O
.	O
hint	O
:	O
note	O
that	O
the	O
likelihood	B
function	O
factorizes	O
,	O
p	O
(	O
x	O
,	O
y|θ	O
)	O
=p	O
(	O
y|x	O
,	O
θ2	O
)	O
p	O
(	O
x|θ1	O
)	O
(	O
5.129	O
)	O
what	O
is	O
p	O
(	O
d|ˆθ	O
,	O
m2	O
)	O
where	O
m2	O
denotes	O
this	O
2-parameter	O
model	O
?	O
fractional	O
form	O
if	O
you	O
wish	O
.	O
)	O
(	O
you	O
may	O
leave	O
your	O
answer	O
in	O
c.	O
now	O
consider	O
a	O
model	O
with	O
4	O
parameters	O
,	O
θ	O
=	O
(	O
θ0,0	O
,	O
θ0,1	O
,	O
θ1,0	O
,	O
θ1,1	O
)	O
,	O
representing	O
p	O
(	O
x	O
,	O
y|θ	O
)	O
=θ	O
x	O
,	O
y	O
.	O
(	O
only	O
3	O
of	O
these	O
parameters	O
are	O
free	O
to	O
vary	O
,	O
since	O
they	O
must	O
sum	O
to	O
one	O
.	O
)	O
what	O
is	O
the	O
mle	O
of	O
θ	O
?	O
what	O
is	O
p	O
(	O
d|ˆθ	O
,	O
m4	O
)	O
where	O
m4	O
denotes	O
this	O
4-parameter	O
model	O
?	O
d.	O
suppose	O
we	O
are	O
not	O
sure	O
which	O
model	O
is	O
correct	O
.	O
we	O
compute	O
the	O
leave-one-out	O
cross	O
validated	O
log	O
likelihood	O
of	O
the	O
2-parameter	O
model	O
and	O
the	O
4-parameter	O
model	O
as	O
follows	O
:	O
l	O
(	O
m	O
)	O
=	O
log	O
p	O
(	O
xi	O
,	O
yi|m	O
,	O
ˆθ	O
(	O
d−i	O
)	O
)	O
(	O
5.130	O
)	O
i=1	O
and	O
ˆθ	O
(	O
d−i	O
)	O
)	O
denotes	O
the	O
mle	O
computed	O
on	O
d	O
excluding	O
row	O
i.	O
which	O
model	O
will	O
cv	O
pick	O
and	O
why	O
?	O
hint	O
:	O
notice	O
how	O
the	O
table	O
of	O
counts	O
changes	O
when	O
you	O
omit	O
each	O
training	O
case	O
one	O
at	O
a	O
time	O
.	O
n	O
(	O
cid:12	O
)	O
5.7.	O
bayesian	O
decision	B
theory	O
e.	O
recall	B
that	O
an	O
alternative	O
to	O
cv	O
is	O
to	O
use	O
the	O
bic	O
score	O
,	O
deﬁned	O
as	O
bic	O
(	O
m	O
,	O
d	O
)	O
(	O
cid:2	O
)	O
log	O
p	O
(	O
d|ˆθm	O
le	O
)	O
−	O
dof	O
(	O
m	O
)	O
2	O
log	O
n	O
189	O
(	O
5.131	O
)	O
where	O
dof	O
(	O
m	O
)	O
is	O
the	O
number	O
of	O
free	O
parameters	O
in	O
the	O
model	O
,	O
compute	O
the	O
bic	O
scores	B
for	O
both	O
models	O
(	O
use	O
log	O
base	O
e	O
)	O
.	O
which	O
model	O
does	O
bic	O
prefer	O
?	O
exercise	O
5.9	O
posterior	B
median	I
is	O
optimal	O
estimate	O
under	O
l1	O
loss	B
prove	O
that	O
the	O
posterior	B
median	I
is	O
optimal	O
estimate	O
under	O
l1	O
loss	B
.	O
exercise	O
5.10	O
decision	B
rule	I
for	O
trading	O
off	O
fps	O
and	O
fns	O
if	O
lf	O
n	O
=	O
clf	O
p	O
,	O
show	O
that	O
we	O
should	O
pick	O
ˆy	O
=	O
1	O
iff	B
p	O
(	O
y	O
=	O
1|x	O
)	O
/p	O
(	O
y	O
=	O
0|x	O
)	O
>	O
τ	O
,	O
where	O
τ	O
=	O
c/	O
(	O
1	O
+	O
c	O
)	O
6	O
frequentist	B
statistics	I
6.1	O
introduction	O
the	O
approach	O
to	O
statistical	O
inference	O
that	O
we	O
described	O
in	O
chapter	O
5	O
is	O
known	O
as	O
bayesian	O
statistics	O
.	O
perhaps	O
surprisingly	O
,	O
this	O
is	O
considered	O
controversial	O
by	O
some	O
people	O
,	O
whereas	O
the	O
ap-	O
plication	O
of	O
bayes	O
rule	O
to	O
non-statistical	O
problems	O
—	O
such	O
as	O
medical	O
diagnosis	O
(	O
section	O
2.2.3.1	O
)	O
,	O
spam	B
ﬁltering	O
(	O
section	O
3.4.4.1	O
)	O
,	O
or	O
airplane	O
tracking	B
(	O
section	O
18.2.1	O
)	O
—	O
is	O
not	O
controversial	O
.	O
the	O
reason	O
for	O
the	O
objection	O
has	O
to	O
do	O
with	O
a	O
misguided	O
distinction	O
between	O
parameters	O
of	O
a	O
statis-	O
tical	O
model	O
and	O
other	O
kinds	O
of	O
unknown	B
quantities.1	O
attempts	O
have	O
been	O
made	O
to	O
devise	O
approaches	O
to	O
statistical	O
inference	O
that	O
avoid	O
treating	O
parameters	O
like	O
random	O
variables	O
,	O
and	O
which	O
thus	O
avoid	O
the	O
use	O
of	O
priors	O
and	O
bayes	O
rule	O
.	O
such	O
approaches	O
are	O
known	O
as	O
frequentist	B
statistics	I
,	O
classical	B
statistics	I
or	O
orthodox	B
statistics	I
.	O
instead	O
of	O
being	O
based	O
on	O
the	O
posterior	O
distribution	O
,	O
they	O
are	O
based	O
on	O
the	O
concept	B
of	O
a	O
sampling	B
distribution	I
.	O
this	O
is	O
the	O
distribution	O
that	O
an	O
estimator	B
has	O
when	O
applied	O
to	O
multiple	O
data	O
sets	O
sampled	O
from	O
the	O
true	O
but	O
unknown	B
distribution	O
;	O
see	O
section	O
6.2	O
for	O
details	O
.	O
it	O
is	O
this	O
notion	O
of	O
variation	O
across	O
repeated	O
trials	O
that	O
forms	O
the	O
basis	O
for	O
modeling	O
uncertainty	B
used	O
by	O
the	O
frequentist	B
approach	O
.	O
by	O
contrast	O
,	O
in	O
the	O
bayesian	O
approach	O
,	O
we	O
only	O
ever	O
condition	O
on	O
the	O
actually	O
observed	O
data	O
;	O
there	O
is	O
no	O
notion	O
of	O
repeated	O
trials	O
.	O
this	O
allows	O
the	O
bayesian	O
to	O
compute	O
the	O
probability	O
of	O
one-off	O
events	O
,	O
as	O
we	O
discussed	O
in	O
section	O
2.1.	O
perhaps	O
more	O
importantly	O
,	O
the	O
bayesian	O
approach	O
avoids	O
certain	O
paradoxes	O
that	O
plague	O
the	O
frequentist	B
approach	O
(	O
see	O
section	O
6.6	O
)	O
.	O
nevertheless	O
,	O
it	O
is	O
important	O
to	O
be	O
familiar	O
with	O
frequentist	B
statistics	I
(	O
especially	O
section	O
6.5	O
)	O
,	O
since	O
it	O
is	O
widely	O
used	O
in	O
machine	B
learning	I
.	O
6.2	O
sampling	B
distribution	I
of	O
an	O
estimator	B
in	O
frequentist	B
statistics	I
,	O
a	O
parameter	B
estimate	O
ˆθ	O
is	O
computed	O
by	O
applying	O
an	O
estimator	B
δ	O
to	O
some	O
data	O
d	O
,	O
so	O
ˆθ	O
=	O
δ	O
(	O
d	O
)	O
.	O
the	O
parameter	B
is	O
viewed	O
as	O
ﬁxed	O
and	O
the	O
data	O
as	O
random	O
,	O
which	O
is	O
the	O
exact	O
opposite	O
of	O
the	O
bayesian	O
approach	O
.	O
the	O
uncertainty	B
in	O
the	O
parameter	B
estimate	O
can	O
be	O
measured	O
by	O
computing	O
the	O
sampling	B
distribution	I
of	O
the	O
estimator	B
.	O
to	O
understand	O
this	O
1.	O
parameters	O
are	O
sometimes	O
considered	O
to	O
represent	O
true	O
(	O
but	O
unknown	B
)	O
physical	O
quantities	O
,	O
which	O
are	O
therefore	O
not	O
random	O
.	O
however	O
,	O
we	O
have	O
seen	O
that	O
it	O
is	O
perfectly	O
reasonable	O
to	O
use	O
a	O
probability	O
distribution	O
to	O
represent	O
one	O
’	O
s	O
uncertainty	B
about	O
an	O
unknown	B
constant	O
.	O
192	O
chapter	O
6.	O
frequentist	B
statistics	I
boot	O
:	O
true	O
=	O
0.70	O
,	O
n=10	O
,	O
mle	O
=	O
0.90	O
,	O
se	O
=	O
0.001	O
boot	O
:	O
true	O
=	O
0.70	O
,	O
n=100	O
,	O
mle	O
=	O
0.70	O
,	O
se	O
=	O
0.000	O
4000	O
3500	O
3000	O
2500	O
2000	O
1500	O
1000	O
500	O
0	O
0	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1	O
3500	O
3000	O
2500	O
2000	O
1500	O
1000	O
500	O
0	O
0	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
6.1	O
a	O
bootstrap	B
approximation	O
to	O
the	O
sampling	B
distribution	I
of	O
ˆθ	O
for	O
a	O
bernoulli	O
distribution	O
.	O
we	O
use	O
b	O
=	O
10	O
,	O
000	O
bootstrap	B
samples	O
.	O
the	O
n	O
datacases	O
were	O
generated	O
from	O
ber	O
(	O
θ	O
=	O
0.7	O
)	O
.	O
(	O
a	O
)	O
mle	O
with	O
n	O
=	O
10	O
.	O
(	O
b	O
)	O
mle	O
with	O
n	O
=	O
100.	O
figure	O
generated	O
by	O
bootstrapdemober	O
.	O
i	O
}	O
n	O
i=1	O
,	O
where	O
xs	O
i	O
∼	O
p	O
(	O
·|θ∗	O
concept	B
,	O
imagine	O
sampling	O
many	O
different	O
data	O
sets	O
d	O
(	O
s	O
)	O
from	O
some	O
true	O
model	O
,	O
p	O
(	O
·|θ∗	O
)	O
,	O
i.e.	O
,	O
let	O
d	O
(	O
s	O
)	O
=	O
{	O
x	O
(	O
s	O
)	O
is	O
the	O
true	O
parameter	O
.	O
here	O
s	O
=	O
1	O
:	O
s	O
indexes	O
the	O
sampled	O
data	O
set	O
,	O
and	O
n	O
is	O
the	O
size	O
of	O
each	O
such	O
dataset	O
.	O
now	O
apply	O
the	O
estimator	B
ˆθ	O
(	O
·	O
)	O
to	O
each	O
d	O
(	O
s	O
)	O
to	O
get	O
a	O
set	O
of	O
estimates	O
,	O
{	O
ˆθ	O
(	O
d	O
(	O
s	O
)	O
)	O
}	O
.	O
as	O
we	O
let	O
s	O
→	O
∞	O
,	O
the	O
distribution	O
induced	O
on	O
ˆθ	O
(	O
·	O
)	O
is	O
the	O
sampling	B
distribution	I
of	O
the	O
estimator	B
.	O
we	O
will	O
discuss	O
various	O
ways	O
to	O
use	O
the	O
sampling	B
distribution	I
in	O
later	O
sections	O
.	O
but	O
ﬁrst	O
we	O
sketch	O
two	O
approaches	O
for	O
computing	O
the	O
sampling	B
distribution	I
itself	O
.	O
)	O
,	O
and	O
θ∗	O
6.2.1	O
bootstrap	B
the	O
bootstrap	B
is	O
a	O
simple	O
monte	O
carlo	O
technique	O
to	O
approximate	O
the	O
sampling	B
distribution	I
.	O
this	O
is	O
particularly	O
useful	O
in	O
cases	O
where	O
the	O
estimator	B
is	O
a	O
complex	O
function	O
of	O
the	O
true	O
parameters	O
.	O
the	O
idea	O
is	O
simple	O
.	O
if	O
we	O
knew	O
the	O
true	O
parameters	O
θ∗	O
,	O
we	O
could	O
generate	O
many	O
(	O
say	O
s	O
)	O
fake	O
i	O
∼	O
p	O
(	O
·|θ∗	O
datasets	O
,	O
each	O
of	O
size	O
n	O
,	O
from	O
the	O
true	O
distribution	O
,	O
xs	O
)	O
,	O
for	O
s	O
=	O
1	O
:	O
s	O
,	O
i	O
=	O
1	O
:	O
n	O
.	O
we	O
could	O
then	O
compute	O
our	O
estimator	B
from	O
each	O
sample	O
,	O
ˆθs	O
=	O
f	O
(	O
xs	O
1	O
:	O
n	O
)	O
and	O
use	O
the	O
empirical	B
distribution	I
of	O
the	O
resulting	O
samples	B
as	O
our	O
estimate	O
of	O
the	O
sampling	B
distribution	I
.	O
since	O
θ	O
is	O
unknown	B
,	O
the	O
idea	O
of	O
the	O
parametric	B
bootstrap	I
is	O
to	O
generate	O
the	O
samples	B
using	O
ˆθ	O
(	O
d	O
)	O
instead	O
.	O
an	O
alternative	O
,	O
called	O
the	O
non-parametric	B
bootstrap	I
,	O
is	O
to	O
sample	O
the	O
xs	O
i	O
(	O
with	O
replacement	O
)	O
from	O
the	O
original	O
data	O
d	O
,	O
and	O
then	O
compute	O
the	O
induced	O
distribution	O
as	O
before	O
.	O
some	O
methods	O
for	O
speeding	O
up	O
the	O
bootstrap	B
when	O
applied	O
to	O
massive	O
data	O
sets	O
are	O
discussed	O
in	O
(	O
kleiner	O
et	O
al	O
.	O
2011	O
)	O
.	O
figure	O
6.1	O
shows	O
an	O
example	O
where	O
we	O
compute	O
the	O
sampling	B
distribution	I
of	O
the	O
mle	O
for	O
a	O
bernoulli	O
using	O
the	O
parametric	B
bootstrap	I
.	O
(	O
results	O
using	O
the	O
non-parametric	B
bootstrap	I
are	O
essentially	O
the	O
same	O
.	O
)	O
we	O
see	O
that	O
the	O
sampling	B
distribution	I
is	O
asymmetric	O
,	O
and	O
therefore	O
quite	O
far	O
from	O
gaussian	O
,	O
when	O
n	O
=	O
10	O
;	O
when	O
n	O
=	O
100	O
,	O
the	O
distribution	O
looks	O
more	O
gaussian	O
,	O
as	O
theory	O
suggests	O
(	O
see	O
below	O
)	O
.	O
1	O
:	O
n	O
)	O
computed	O
by	O
the	O
bootstrap	B
and	O
parameter	B
values	O
sampled	O
from	O
the	O
posterior	O
,	O
θs	O
∼	O
p	O
(	O
·|d	O
)	O
?	O
a	O
natural	O
question	O
is	O
:	O
what	O
is	O
the	O
connection	O
between	O
the	O
parameter	B
estimates	O
ˆθs	O
=	O
ˆθ	O
(	O
xs	O
6.2.	O
sampling	B
distribution	I
of	O
an	O
estimator	B
193	O
conceptually	O
they	O
are	O
quite	O
different	O
.	O
but	O
in	O
the	O
common	O
case	O
that	O
that	O
the	O
prior	O
is	O
not	O
very	O
strong	O
,	O
they	O
can	O
be	O
quite	O
similar	B
.	O
for	O
example	O
,	O
figure	O
6.1	O
(	O
c-d	O
)	O
shows	O
an	O
example	O
where	O
we	O
compute	O
the	O
posterior	O
using	O
a	O
uniform	O
beta	O
(	O
1,1	O
)	O
prior	O
,	O
and	O
then	O
sample	O
from	O
it	O
.	O
we	O
see	O
that	O
the	O
posterior	O
and	O
the	O
sampling	B
distribution	I
are	O
quite	O
similar	B
.	O
so	O
one	O
can	O
think	O
of	O
the	O
bootstrap	B
distribution	O
as	O
a	O
“	O
poor	O
man	O
’	O
s	O
”	O
posterior	O
;	O
see	O
(	O
hastie	O
et	O
al	O
.	O
2001	O
,	O
p235	O
)	O
for	O
details	O
.	O
however	O
,	O
perhaps	O
surprisingly	O
,	O
bootstrap	B
can	O
be	O
slower	O
than	O
posterior	O
sampling	O
.	O
the	O
reason	O
is	O
that	O
the	O
bootstrap	B
has	O
to	O
ﬁt	O
the	O
model	O
s	O
times	O
,	O
whereas	O
in	O
posterior	O
sampling	O
,	O
we	O
usually	O
only	O
ﬁt	O
the	O
model	O
once	O
(	O
to	O
ﬁnd	O
a	O
local	O
mode	O
)	O
,	O
and	O
then	O
perform	O
local	O
exploration	O
around	O
the	O
mode	B
.	O
such	O
local	O
exploration	O
is	O
usually	O
much	O
faster	O
than	O
ﬁtting	O
a	O
model	O
from	O
scratch	O
.	O
6.2.2	O
large	O
sample	O
theory	O
for	O
the	O
mle	O
*	O
in	O
some	O
cases	O
,	O
the	O
sampling	B
distribution	I
for	O
some	O
estimators	O
can	O
be	O
computed	O
analytically	O
.	O
in	O
particular	O
,	O
it	O
can	O
be	O
shown	O
that	O
,	O
under	O
certain	O
conditions	O
,	O
as	O
the	O
sample	O
size	O
tends	O
to	O
inﬁnity	O
,	O
the	O
sampling	B
distribution	I
of	O
the	O
mle	O
becomes	O
gaussian	O
.	O
informally	O
,	O
the	O
requirement	O
for	O
this	O
result	O
to	O
hold	O
is	O
that	O
each	O
parameter	B
in	O
the	O
model	O
gets	O
to	O
“	O
see	O
”	O
an	O
inﬁnite	O
amount	O
of	O
data	O
,	O
and	O
that	O
the	O
model	O
be	O
identiﬁable	B
.	O
unfortunately	O
this	O
excludes	O
many	O
of	O
the	O
models	O
of	O
interest	O
to	O
machine	B
learning	I
.	O
nevertheless	O
,	O
let	O
us	O
assume	O
we	O
are	O
in	O
a	O
simple	O
setting	O
where	O
the	O
theorem	O
holds	O
.	O
the	O
center	O
of	O
the	O
gaussian	O
will	O
be	O
the	O
mle	O
ˆθ	O
.	O
but	O
what	O
about	O
the	O
variance	B
of	O
this	O
gaussian	O
?	O
intuitively	O
the	O
variance	B
of	O
the	O
estimator	B
will	O
be	O
(	O
inversely	O
)	O
related	O
to	O
the	O
amount	O
of	O
curvature	O
of	O
the	O
likelihood	B
surface	O
at	O
its	O
peak	O
.	O
if	O
the	O
curvature	O
is	O
large	O
,	O
the	O
peak	O
will	O
be	O
“	O
sharp	O
”	O
,	O
and	O
the	O
variance	B
low	O
;	O
in	O
this	O
case	O
,	O
the	O
estimate	O
is	O
“	O
well	O
determined	O
”	O
.	O
by	O
contrast	O
,	O
if	O
the	O
curvature	O
is	O
small	O
,	O
the	O
peak	O
will	O
be	O
nearly	O
“	O
ﬂat	O
”	O
,	O
so	O
the	O
variance	B
is	O
high	O
.	O
likelihood	B
evaluated	O
at	O
some	O
point	O
ˆθ	O
:	O
let	O
us	O
now	O
formalize	O
this	O
intuition	O
.	O
deﬁne	O
the	O
score	B
function	I
as	O
the	O
gradient	O
of	O
the	O
log	O
s	O
(	O
ˆθ	O
)	O
(	O
cid:2	O
)	O
∇	O
log	O
p	O
(	O
d|θ	O
)	O
|ˆθ	O
(	O
6.1	O
)	O
deﬁne	O
the	O
observed	B
information	I
matrix	I
as	O
the	O
gradient	O
of	O
the	O
negative	O
score	O
function	O
,	O
or	O
equivalently	O
,	O
the	O
hessian	O
of	O
the	O
nll	O
:	O
j	O
(	O
ˆθ	O
(	O
d	O
)	O
)	O
(	O
cid:2	O
)	O
−∇s	O
(	O
ˆθ	O
)	O
=	O
−∇2	O
θ	O
log	O
p	O
(	O
d|θ	O
)	O
|ˆθ	O
in	O
1d	O
,	O
this	O
becomes	O
j	O
(	O
ˆθ	O
(	O
d	O
)	O
)	O
=	O
−	O
d	O
dθ2	O
log	O
p	O
(	O
d|θ	O
)	O
|ˆθ	O
(	O
6.2	O
)	O
(	O
6.3	O
)	O
this	O
is	O
just	O
a	O
measure	O
of	O
curvature	O
of	O
the	O
log-likelihood	O
function	O
at	O
ˆθ	O
.	O
since	O
we	O
are	O
studying	O
the	O
sampling	B
distribution	I
,	O
d	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
is	O
a	O
set	O
of	O
random	O
variables	O
.	O
the	O
fisher	O
information	B
matrix	O
is	O
deﬁned	O
to	O
be	O
the	O
expected	B
value	I
of	O
the	O
observed	B
information	I
matrix:2	O
)	O
(	O
cid:2	O
)	O
eθ∗	O
in	O
(	O
ˆθ|θ∗	O
j	O
(	O
ˆθ|d	O
)	O
)	O
*	O
(	O
6.4	O
)	O
2.	O
this	O
is	O
not	O
the	O
usual	O
deﬁnition	O
,	O
but	O
is	O
equivalent	O
to	O
it	O
under	O
standard	O
assumptions	O
.	O
more	O
precisely	O
,	O
the	O
standard	O
deﬁnition	O
is	O
as	O
follows	O
(	O
we	O
just	O
give	O
the	O
scalar	O
case	O
to	O
simplify	O
notation	O
)	O
:	O
i	O
(	O
ˆθ|θ∗	O
,	O
that	O
is	O
,	O
the	O
variance	B
of	O
the	O
score	B
function	I
.	O
if	O
ˆθ	O
is	O
the	O
mle	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
eθ∗	O
(	O
cid:4	O
)	O
dθ	O
log	O
p	O
(	O
x|θ	O
)	O
|	O
ˆθ	O
)	O
(	O
cid:2	O
)	O
varθ∗	O
=	O
0	O
(	O
since	O
(	O
cid:3	O
)	O
d	O
(	O
cid:3	O
)	O
(	O
cid:4	O
)	O
dθ	O
log	O
p	O
(	O
x|θ	O
)	O
|	O
ˆθ	O
d	O
194	O
chapter	O
6.	O
frequentist	B
statistics	I
(	O
cid:4	O
)	O
n	O
i=1	O
f	O
(	O
xi	O
)	O
p	O
(	O
xi|θ∗	O
.	O
often	O
θ∗	O
n	O
where	O
eθ∗	O
[	O
f	O
(	O
d	O
)	O
]	O
(	O
cid:2	O
)	O
1	O
)	O
is	O
the	O
expected	B
value	I
of	O
the	O
function	O
f	O
when	O
applied	O
to	O
data	O
sampled	O
from	O
θ∗	O
,	O
representing	O
the	O
“	O
true	O
parameter	O
”	O
that	O
generated	O
the	O
data	O
,	O
is	O
assumed	O
known	O
,	O
so	O
we	O
just	O
write	O
in	O
(	O
ˆθ	O
)	O
(	O
cid:2	O
)	O
in	O
(	O
ˆθ|θ∗	O
)	O
for	O
short	O
.	O
furthermore	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
in	O
(	O
ˆθ	O
)	O
=n	O
i1	O
(	O
ˆθ	O
)	O
,	O
because	O
the	O
log-likelihood	O
for	O
a	O
sample	O
of	O
size	O
n	O
is	O
just	O
n	O
times	O
“	O
steeper	O
”	O
than	O
the	O
log-likelihood	O
for	O
a	O
sample	O
of	O
size	O
1.	O
so	O
we	O
can	O
drop	O
the	O
1	O
subscript	O
and	O
just	O
write	O
i	O
(	O
ˆθ	O
)	O
(	O
cid:2	O
)	O
i1	O
(	O
ˆθ	O
)	O
.	O
this	O
is	O
the	O
notation	O
that	O
is	O
usually	O
used	O
.	O
now	O
let	O
ˆθ	O
(	O
cid:2	O
)	O
ˆθmle	O
(	O
d	O
)	O
be	O
the	O
mle	O
,	O
where	O
d	O
∼	O
θ∗	O
ˆθ	O
→	O
n	O
(	O
θ∗	O
,	O
in	O
(	O
θ∗	O
−1	O
)	O
.	O
it	O
can	O
be	O
shown	O
that	O
)	O
(	O
6.5	O
)	O
as	O
n	O
→	O
∞	O
(	O
see	O
e.g.	O
,	O
(	O
rice	O
1995	O
,	O
p265	O
)	O
for	O
a	O
proof	O
)	O
.	O
we	O
say	O
that	O
the	O
sampling	B
distribution	I
of	O
the	O
mle	O
is	O
asymptotically	B
normal	I
.	O
in	O
the	O
mle	O
?	O
unfortunately	O
,	O
θ∗	O
distribution	O
.	O
however	O
,	O
we	O
can	O
approximate	O
the	O
sampling	B
distribution	I
by	O
replacing	O
θ∗	O
consequently	O
,	O
the	O
approximate	O
standard	O
errors	O
of	O
ˆθk	O
are	O
given	O
by	O
what	O
about	O
the	O
variance	B
of	O
the	O
mle	O
,	O
which	O
can	O
be	O
used	O
as	O
some	O
measure	O
of	O
conﬁdence	O
is	O
unknown	B
,	O
so	O
we	O
can	O
’	O
t	O
evaluate	O
the	O
variance	B
of	O
the	O
sampling	O
with	O
ˆθ	O
.	O
ˆsek	O
(	O
cid:2	O
)	O
in	O
(	O
ˆθ	O
)	O
−	O
1	O
2	O
kk	O
(	O
6.6	O
)	O
for	O
example	O
,	O
from	O
equation	O
5.60	O
we	O
know	O
that	O
the	O
fisher	O
information	B
for	O
a	O
binomial	B
sampling	O
model	O
is	O
i	O
(	O
θ	O
)	O
=	O
1	O
θ	O
(	O
1	O
−	O
θ	O
)	O
so	O
the	O
approximate	O
standard	O
error	O
of	O
the	O
mle	O
is	O
ˆθ	O
(	O
1	O
−	O
ˆθ	O
)	O
(	O
cid:13	O
)	O
ˆse	O
=	O
=	O
11	O
n	O
i	O
(	O
ˆθ	O
)	O
=	O
n	O
11	O
in	O
(	O
ˆθ	O
)	O
(	O
cid:4	O
)	O
where	O
ˆθ	O
=	O
1	O
n	O
under	O
a	O
uniform	O
prior	O
.	O
(	O
cid:14	O
)	O
1	O
2	O
(	O
6.7	O
)	O
(	O
6.8	O
)	O
i	O
xi	O
.	O
compare	O
this	O
to	O
equation	O
3.27	O
,	O
which	O
is	O
the	O
posterior	O
standard	O
deviation	O
6.3	O
frequentist	B
decision	O
theory	O
in	O
frequentist	B
or	O
classical	B
decision	O
theory	O
,	O
there	O
is	O
a	O
loss	B
function	I
and	O
a	O
likelihood	B
,	O
but	O
there	O
is	O
no	O
prior	O
and	O
hence	O
no	O
posterior	O
or	O
posterior	B
expected	I
loss	I
.	O
thus	O
there	O
is	O
no	O
automatic	O
way	O
of	O
deriving	O
an	O
optimal	O
estimator	O
,	O
unlike	O
the	O
bayesian	O
case	O
.	O
instead	O
,	O
in	O
the	O
frequentist	B
approach	O
,	O
we	O
are	O
free	O
to	O
choose	O
any	O
estimator	B
or	O
decision	B
procedure	I
δ	O
:	O
x	O
→	O
a	O
we	O
want.3	O
(	O
cid:3	O
)	O
the	O
gradient	O
must	O
be	O
zero	O
at	O
a	O
maximum	O
)	O
,	O
so	O
the	O
variance	B
reduces	O
to	O
the	O
expected	O
square	O
of	O
the	O
score	B
function	I
:	O
dθ	O
log	O
p	O
(	O
x|θ	O
)	O
)	O
2	O
i	O
(	O
ˆθ|θ∗	O
(	O
cid:3	O
)	O
(	O
d	O
)	O
=e	O
θ∗	O
=	O
dθ2	O
log	O
p	O
(	O
x|θ	O
)	O
−eθ∗	O
is	O
a	O
much	O
more	O
intuitive	O
quantity	O
than	O
the	O
variance	B
of	O
the	O
score	O
.	O
3.	O
in	O
practice	O
,	O
the	O
frequentist	B
approach	O
is	O
usually	O
only	O
applied	O
to	O
one-shot	O
statistical	O
decision	B
problems	O
—	O
such	O
as	O
classiﬁcation	B
,	O
regression	B
and	O
parameter	B
estimation	O
—	O
since	O
its	O
non-constructive	O
nature	O
makes	O
it	O
difficult	O
to	O
apply	O
to	O
sequential	B
decision	O
problems	O
,	O
which	O
adapt	O
to	O
data	O
online	O
.	O
,	O
so	O
now	O
the	O
fisher	O
information	B
reduces	O
to	O
the	O
expected	O
second	O
derivative	O
of	O
the	O
nll	O
,	O
which	O
dθ	O
log	O
p	O
(	O
x|θ	O
)	O
)	O
2	O
(	O
d	O
(	O
rice	O
1995	O
,	O
p263	O
)	O
)	O
that	O
eθ∗	O
it	O
can	O
be	O
shown	O
(	O
e.g.	O
,	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
.	O
(	O
cid:3	O
)	O
(	O
cid:4	O
)	O
d2	O
6.3.	O
frequentist	B
decision	O
theory	O
having	O
chosen	O
an	O
estimator	B
,	O
we	O
deﬁne	O
its	O
expected	O
loss	O
or	O
risk	B
as	O
follows	O
:	O
r	O
(	O
θ∗	O
,	O
δ	O
)	O
(	O
cid:2	O
)	O
ep	O
(	O
˜d|θ∗	O
)	O
l	O
(	O
θ∗	O
,	O
δ	O
(	O
˜d	O
)	O
)	O
p	O
(	O
˜d|θ∗	O
l	O
(	O
θ∗	O
,	O
δ	O
(	O
˜d	O
)	O
)	O
)	O
d	O
˜d	O
=	O
)	O
*	O
(	O
cid:11	O
)	O
195	O
(	O
6.9	O
)	O
(	O
cid:11	O
)	O
where	O
˜d	O
is	O
data	O
sampled	O
from	O
“	O
nature	O
’	O
s	O
distribution	O
”	O
,	O
which	O
is	O
represented	O
by	O
parameter	B
θ∗	O
.	O
in	O
other	O
words	O
,	O
the	O
expectation	O
is	O
wrt	O
the	O
sampling	B
distribution	I
of	O
the	O
estimator	B
.	O
compare	O
this	O
to	O
the	O
bayesian	O
posterior	B
expected	I
loss	I
:	O
ρ	O
(	O
a|d	O
,	O
π	O
)	O
(	O
cid:2	O
)	O
ep	O
(	O
θ|d	O
,	O
π	O
)	O
[	O
l	O
(	O
θ	O
,	O
a	O
)	O
]	O
=	O
(	O
6.10	O
)	O
we	O
see	O
that	O
the	O
bayesian	O
approach	O
averages	O
over	O
θ	O
(	O
which	O
is	O
unknown	B
)	O
and	O
conditions	O
on	O
d	O
(	O
which	O
is	O
known	O
)	O
,	O
whereas	O
the	O
frequentist	B
approach	O
averages	O
over	O
˜d	O
(	O
thus	O
ignoring	O
the	O
observed	O
data	O
)	O
,	O
and	O
conditions	O
on	O
θ∗	O
not	O
only	O
is	O
the	O
frequentist	B
deﬁnition	O
unnatural	O
,	O
it	O
can	O
not	O
even	O
be	O
computed	O
,	O
because	O
θ∗	O
is	O
unknown	B
.	O
consequently	O
,	O
we	O
can	O
not	O
compare	O
different	O
estimators	O
in	O
terms	O
of	O
their	O
frequentist	B
risk	O
.	O
we	O
discuss	O
various	O
solutions	O
to	O
this	O
below	O
.	O
l	O
(	O
θ	O
,	O
a	O
)	O
p	O
(	O
θ|d	O
,	O
π	O
)	O
dθ	O
(	O
which	O
is	O
unknown	B
)	O
.	O
θ	O
6.3.1	O
bayes	O
risk	B
how	O
do	O
we	O
choose	O
amongst	O
estimators	O
?	O
we	O
need	O
some	O
way	O
to	O
convert	O
r	O
(	O
θ∗	O
,	O
δ	O
)	O
into	O
a	O
single	O
measure	O
of	O
quality	O
,	O
r	O
(	O
δ	O
)	O
,	O
which	O
does	O
not	O
depend	O
on	O
knowing	O
θ∗	O
.	O
one	O
approach	O
is	O
to	O
put	O
a	O
prior	O
on	O
θ∗	O
,	O
and	O
then	O
to	O
deﬁne	O
bayes	O
risk	B
or	O
integrated	B
risk	I
of	O
an	O
estimator	B
as	O
follows	O
:	O
(	O
cid:11	O
)	O
rb	O
(	O
δ	O
)	O
(	O
cid:2	O
)	O
ep	O
(	O
θ∗	O
)	O
[	O
r	O
(	O
θ∗	O
,	O
δ	O
)	O
]	O
=	O
r	O
(	O
θ∗	O
,	O
δ	O
)	O
p	O
(	O
θ∗	O
)	O
dθ∗	O
a	O
bayes	O
estimator	B
or	O
bayes	O
decision	B
rule	I
is	O
one	O
which	O
minimizes	O
the	O
expected	O
risk	O
:	O
δb	O
(	O
cid:2	O
)	O
argmin	O
rb	O
(	O
δ	O
)	O
δ	O
note	O
that	O
the	O
integrated	B
risk	I
is	O
also	O
called	O
the	O
preposterior	B
risk	I
,	O
since	O
it	O
is	O
before	O
we	O
have	O
seen	O
the	O
data	O
.	O
minimizing	O
this	O
can	O
be	O
useful	O
for	O
experiment	O
design	O
.	O
we	O
will	O
now	O
prove	O
a	O
very	O
important	O
theorem	O
,	O
that	O
connects	O
the	O
bayesian	O
and	O
frequentist	B
approaches	O
to	O
decision	B
theory	O
.	O
theorem	O
6.3.1.	O
a	O
bayes	O
estimator	B
can	O
be	O
obtained	O
by	O
minimizing	O
the	O
posterior	B
expected	I
loss	I
for	O
each	O
x.	O
proof	O
.	O
by	O
switching	O
the	O
order	O
of	O
integration	O
,	O
we	O
have	O
(	O
cid:17	O
)	O
(	O
6.11	O
)	O
(	O
6.12	O
)	O
(	O
6.13	O
)	O
(	O
6.14	O
)	O
(	O
6.15	O
)	O
(	O
6.16	O
)	O
y	O
x	O
(	O
cid:11	O
)	O
(	O
cid:16	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:11	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:16	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
(	O
cid:6	O
)	O
θ	O
x	O
y	O
y	O
x	O
x	O
rb	O
(	O
δ	O
)	O
=	O
=	O
=	O
=	O
ρ	O
(	O
δ	O
(	O
x	O
)	O
|x	O
)	O
p	O
(	O
x	O
)	O
l	O
(	O
y	O
,	O
δ	O
(	O
x	O
)	O
)	O
p	O
(	O
x	O
,	O
y|θ∗	O
)	O
p	O
(	O
θ∗	O
)	O
dθ∗	O
l	O
(	O
y	O
,	O
δ	O
(	O
x	O
)	O
)	O
p	O
(	O
x	O
,	O
y	O
,	O
θ	O
∗	O
(	O
cid:17	O
)	O
)	O
dθ∗	O
l	O
(	O
y	O
,	O
δ	O
(	O
x	O
)	O
)	O
p	O
(	O
y|x	O
)	O
dy	O
p	O
(	O
x	O
)	O
196	O
chapter	O
6.	O
frequentist	B
statistics	I
r	O
r	O
(	O
θ	O
,	O
δ1	O
)	O
r	O
(	O
θ	O
,	O
δ2	O
)	O
θ	O
figure	O
6.2	O
risk	B
functions	O
for	O
two	O
decision	B
procedures	O
,	O
δ1	O
and	O
δ2	O
.	O
since	O
δ1	O
has	O
lower	O
worst	O
case	O
risk	O
,	O
it	O
is	O
the	O
minimax	O
estimator	O
,	O
even	O
though	O
δ2	O
has	O
lower	O
risk	B
for	O
most	O
values	O
of	O
θ.	O
thus	O
minimax	O
estimators	O
are	O
overly	O
conservative	O
.	O
to	O
minimize	O
the	O
overall	O
expectation	O
,	O
we	O
just	O
minimize	O
the	O
term	O
inside	O
for	O
each	O
x	O
,	O
so	O
our	O
decision	B
rule	I
is	O
to	O
pick	O
(	O
6.17	O
)	O
δb	O
(	O
x	O
)	O
=	O
argmin	O
a∈a	O
ρ	O
(	O
a|x	O
)	O
hence	O
we	O
see	O
that	O
the	O
picking	O
the	O
optimal	B
action	I
on	O
a	O
case-by-case	O
basis	O
(	O
as	O
in	O
the	O
bayesian	O
approach	O
)	O
is	O
optimal	O
on	O
average	O
(	O
as	O
in	O
the	O
frequentist	B
approach	O
)	O
.	O
in	O
other	O
words	O
,	O
the	O
bayesian	O
approach	O
provides	O
a	O
good	O
way	O
of	O
achieving	O
frequentist	B
goals	O
.	O
in	O
fact	O
,	O
one	O
can	O
go	O
further	O
and	O
prove	O
the	O
following	O
.	O
theorem	O
6.3.2	O
(	O
wald	O
,	O
1950	O
)	O
.	O
every	O
admissable	O
decision	B
rule	I
is	O
a	O
bayes	O
decision	B
rule	I
with	O
respect	O
to	O
some	O
,	O
possibly	O
improper	O
,	O
prior	O
distribution	O
.	O
this	O
theorem	O
shows	O
that	O
the	O
best	O
way	O
to	O
minimize	O
frequentist	B
risk	O
is	O
to	O
be	O
bayesian	O
!	O
see	O
(	O
bernardo	O
and	O
smith	O
1994	O
,	O
p448	O
)	O
for	O
further	O
discussion	O
of	O
this	O
point	O
.	O
6.3.2	O
minimax	O
risk	O
obviously	O
some	O
frequentists	O
dislike	O
using	O
bayes	O
risk	B
since	O
it	O
requires	O
the	O
choice	O
of	O
a	O
prior	O
(	O
al-	O
though	O
this	O
is	O
only	O
in	O
the	O
evaluation	O
of	O
the	O
estimator	B
,	O
not	O
necessarily	O
as	O
part	O
of	O
its	O
construction	O
)	O
.	O
an	O
alternative	O
approach	O
is	O
as	O
follows	O
.	O
deﬁne	O
the	O
maximum	B
risk	I
of	O
an	O
estimator	B
as	O
rmax	O
(	O
δ	O
)	O
(	O
cid:2	O
)	O
max	O
θ∗	O
r	O
(	O
θ∗	O
,	O
δ	O
)	O
a	O
minimax	B
rule	I
is	O
one	O
which	O
minimizes	O
the	O
maximum	B
risk	I
:	O
δmm	O
(	O
cid:2	O
)	O
argmin	O
δ	O
rmax	O
(	O
δ	O
)	O
(	O
6.18	O
)	O
(	O
6.19	O
)	O
6.3.	O
frequentist	B
decision	O
theory	O
197	O
for	O
example	O
,	O
in	O
figure	O
6.2	O
,	O
we	O
see	O
that	O
δ1	O
has	O
lower	O
worst-case	O
risk	B
than	O
δ2	O
,	O
ranging	O
over	O
all	O
possible	O
values	O
of	O
θ∗	O
,	O
so	O
it	O
is	O
the	O
minimax	O
estimator	O
(	O
see	O
section	O
6.3.3.1	O
for	O
an	O
explanation	O
of	O
how	O
to	O
compute	O
a	O
risk	B
function	O
for	O
an	O
actual	O
model	O
)	O
.	O
minimax	O
estimators	O
have	O
a	O
certain	O
appeal	O
.	O
however	O
,	O
computing	O
them	O
can	O
be	O
hard	O
.	O
and	O
furthermore	O
,	O
they	O
are	O
very	O
pessimistic	O
.	O
in	O
fact	O
,	O
one	O
can	O
show	O
that	O
all	O
minimax	O
estimators	O
are	O
equivalent	O
to	O
bayes	O
estimators	O
under	O
a	O
least	B
favorable	I
prior	I
.	O
in	O
most	O
statistical	O
situations	O
(	O
excluding	O
game	O
theoretic	O
ones	O
)	O
,	O
assuming	O
nature	O
is	O
an	O
adversary	O
is	O
not	O
a	O
reasonable	O
assumption	O
.	O
6.3.3	O
admissible	B
estimators	O
the	O
basic	O
problem	O
with	O
frequentist	B
decision	O
theory	O
is	O
that	O
it	O
relies	O
on	O
knowing	O
the	O
true	O
distri-	O
bution	O
p	O
(	O
·|θ∗	O
)	O
in	O
order	O
to	O
evaluate	O
the	O
risk	B
.	O
however	O
,	O
it	O
might	O
be	O
the	O
case	O
that	O
some	O
estimators	O
.	O
in	O
particular	O
,	O
if	O
r	O
(	O
θ	O
,	O
δ1	O
)	O
≤	O
r	O
(	O
θ	O
,	O
δ2	O
)	O
for	O
all	O
are	O
worse	O
than	O
others	O
regardless	O
of	O
the	O
value	O
of	O
θ∗	O
θ	O
∈	O
θ	O
,	O
then	O
we	O
say	O
that	O
δ1	O
dominates	B
δ2	O
.	O
the	O
domination	O
is	O
said	O
to	O
be	O
strict	B
if	O
the	O
inequality	O
is	O
strict	B
for	O
some	O
θ.	O
an	O
estimator	B
is	O
said	O
to	O
be	O
admissible	B
if	O
it	O
is	O
not	O
strictly	O
dominated	O
by	O
any	O
other	O
estimator	B
.	O
6.3.3.1	O
example	O
let	O
us	O
give	O
an	O
example	O
,	O
based	O
on	O
(	O
bernardo	O
and	O
smith	O
1994	O
)	O
.	O
consider	O
the	O
problem	O
of	O
estimating	O
the	O
mean	B
of	O
a	O
gaussian	O
.	O
we	O
assume	O
the	O
data	O
is	O
sampled	O
from	O
xi	O
∼	O
n	O
(	O
θ∗	O
,	O
σ2	O
=	O
1	O
)	O
and	O
use	O
quadratic	B
loss	I
,	O
l	O
(	O
θ	O
,	O
ˆθ	O
)	O
=	O
(	O
θ	O
−	O
ˆθ	O
)	O
2.	O
the	O
corresponding	O
risk	B
function	O
is	O
the	O
mse	O
.	O
some	O
possible	O
decision	B
rules	O
or	O
estimators	O
ˆθ	O
(	O
x	O
)	O
=	O
δ	O
(	O
x	O
)	O
are	O
as	O
follows	O
:	O
•	O
•	O
•	O
•	O
δ1	O
(	O
x	O
)	O
=	O
x	O
,	O
the	O
sample	O
mean	O
δ2	O
(	O
x	O
)	O
=	O
˜x	O
,	O
the	O
sample	O
median	O
δ3	O
(	O
x	O
)	O
=	O
θ0	O
,	O
a	O
ﬁxed	O
value	O
δκ	O
(	O
x	O
)	O
,	O
the	O
posterior	B
mean	I
under	O
a	O
n	O
(	O
θ|θ0	O
,	O
σ2/κ	O
)	O
prior	O
:	O
δκ	O
(	O
x	O
)	O
=	O
n	O
n	O
+	O
κ	O
x	O
+	O
κ	O
n	O
+	O
κ	O
θ0	O
=	O
wx	O
+	O
(	O
1−	O
w	O
)	O
θ0	O
(	O
6.20	O
)	O
for	O
δκ	O
,	O
we	O
consider	O
a	O
weak	O
prior	O
,	O
κ	O
=	O
1	O
,	O
and	O
a	O
stronger	O
prior	O
,	O
κ	O
=	O
5.	O
the	O
prior	O
mean	B
is	O
θ0	O
,	O
some	O
ﬁxed	O
value	O
.	O
we	O
assume	O
σ2	O
is	O
known	O
.	O
(	O
thus	O
δ3	O
(	O
x	O
)	O
is	O
the	O
same	O
as	O
δκ	O
(	O
x	O
)	O
with	O
an	O
inﬁnitely	O
strong	O
prior	O
,	O
κ	O
=	O
∞	O
.	O
)	O
we	O
know	O
the	O
true	O
parameter	O
θ∗	O
*	O
)	O
into	O
squared	O
bias	O
plus	O
variance	B
:	O
(	O
we	O
can	O
do	O
this	O
since	O
in	O
this	O
toy	O
example	O
,	O
in	O
section	O
6.4.4	O
,	O
we	O
show	O
that	O
the	O
mse	O
can	O
be	O
decomposed	O
let	O
us	O
now	O
derive	O
the	O
risk	B
functions	O
analytically	O
.	O
.	O
)	O
m	O
se	O
(	O
ˆθ	O
(	O
·	O
)	O
|θ∗	O
)	O
=	O
var	O
ˆθ	O
+	O
bias2	O
(	O
ˆθ	O
)	O
the	O
sample	O
mean	O
is	O
unbiased	B
,	O
so	O
its	O
risk	B
is	O
m	O
se	O
(	O
δ1|θ∗	O
)	O
=	O
var	O
[	O
x	O
]	O
=	O
σ2	O
n	O
(	O
6.21	O
)	O
(	O
6.22	O
)	O
198	O
chapter	O
6.	O
frequentist	B
statistics	I
risk	O
functions	O
for	O
n=5	O
mle	O
median	B
fixed	O
postmean1	O
postmean5	O
0.5	O
0.45	O
0.4	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
risk	B
functions	O
for	O
n=20	O
mle	O
median	B
fixed	O
postmean1	O
postmean5	O
0.18	O
0.16	O
0.14	O
0.12	O
0.1	O
0.08	O
0.06	O
0.04	O
0.02	O
*	O
)	O
δ	O
,	O
θ	O
(	O
r	O
*	O
)	O
δ	O
,	O
θ	O
(	O
r	O
0	O
−2	O
−1.5	O
−1	O
−0.5	O
0	O
θ	O
*	O
(	O
a	O
)	O
0.5	O
1	O
1.5	O
2	O
0	O
−2	O
−1.5	O
−1	O
−0.5	O
0.5	O
1	O
1.5	O
2	O
0	O
θ	O
*	O
(	O
b	O
)	O
figure	O
6.3	O
risk	B
functions	O
for	O
estimating	O
the	O
mean	B
of	O
a	O
gaussian	O
using	O
data	O
sampled	O
n	O
(	O
θ∗	O
,	O
σ2	O
=	O
1	O
)	O
.	O
the	O
solid	O
dark	O
blue	O
horizontal	O
line	O
is	O
the	O
mle	O
,	O
the	O
solid	O
light	O
blue	O
curved	O
line	O
is	O
the	O
posterior	B
mean	I
when	O
κ	O
=	O
5.	O
left	O
:	O
n	O
=	O
5	O
samples	B
.	O
right	O
:	O
n	O
=	O
20	O
samples	B
.	O
based	O
on	O
figure	O
b.1	O
of	O
(	O
bernardo	O
and	O
smith	O
1994	O
)	O
.	O
figure	O
generated	O
by	O
riskfngauss	O
.	O
the	O
sample	O
median	O
is	O
also	O
unbiased	B
.	O
one	O
can	O
show	O
that	O
the	O
variance	B
is	O
approximately	O
π/	O
(	O
2n	O
)	O
,	O
so	O
(	O
6.23	O
)	O
(	O
6.24	O
)	O
(	O
6.25	O
)	O
(	O
6.26	O
)	O
(	O
6.27	O
)	O
*	O
2	O
)	O
)	O
m	O
se	O
(	O
δ2|θ∗	O
)	O
=	O
π	O
2n	O
for	O
δ3	O
(	O
x	O
)	O
=	O
θ0	O
,	O
the	O
variance	B
is	O
zero	O
,	O
so	O
m	O
se	O
(	O
δ3|θ∗	O
)	O
=	O
(	O
θ∗	O
−	O
θ0	O
)	O
2	O
finally	O
,	O
for	O
the	O
posterior	B
mean	I
,	O
we	O
have	O
m	O
se	O
(	O
δκ|θ∗	O
)	O
=e	O
)	O
(	O
wx	O
+	O
(	O
1−	O
w	O
)	O
θ0	O
−	O
θ∗	O
)	O
(	O
w	O
(	O
x	O
−	O
θ∗	O
*	O
2	O
)	O
)	O
+	O
(	O
1−	O
w	O
)	O
(	O
θ0	O
−	O
θ∗	O
+	O
(	O
1−	O
w	O
)	O
2	O
(	O
θ0	O
−	O
θ∗	O
)	O
2	O
n	O
σ2	O
+	O
κ2	O
(	O
θ0	O
−	O
θ∗	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
)	O
2	O
=	O
e	O
=	O
w2	O
σ2	O
n	O
1	O
=	O
(	O
n	O
+	O
κ	O
)	O
2	O
(	O
6.28	O
)	O
these	O
functions	O
are	O
plotted	O
in	O
figure	O
6.3	O
for	O
n	O
∈	O
{	O
5	O
,	O
20	O
}	O
.	O
we	O
see	O
that	O
in	O
general	O
,	O
the	O
best	O
estimator	B
depends	O
on	O
the	O
value	O
of	O
θ∗	O
is	O
very	O
close	O
to	O
θ0	O
,	O
then	O
δ3	O
,	O
which	O
is	O
unknown	B
.	O
if	O
θ∗	O
is	O
within	O
some	O
reasonable	O
range	O
around	O
θ0	O
,	O
then	O
the	O
(	O
which	O
just	O
predicts	O
θ0	O
)	O
is	O
best	O
.	O
posterior	B
mean	I
,	O
which	O
combines	O
the	O
prior	O
guess	O
of	O
θ0	O
with	O
the	O
actual	O
data	O
,	O
is	O
best	O
.	O
if	O
θ∗	O
is	O
far	O
from	O
θ0	O
,	O
the	O
mle	O
is	O
best	O
.	O
none	O
of	O
this	O
should	O
be	O
suprising	O
:	O
a	O
small	O
amount	O
of	O
shrinkage	B
(	O
using	O
the	O
posterior	B
mean	I
with	O
a	O
weak	O
prior	O
)	O
is	O
usually	O
desirable	O
,	O
assuming	O
our	O
prior	O
mean	B
is	O
sensible	O
.	O
what	O
is	O
more	O
surprising	O
is	O
that	O
the	O
risk	B
of	O
decision	B
rule	I
δ2	O
(	O
sample	O
median	O
)	O
is	O
always	O
higher	O
than	O
that	O
of	O
δ1	O
(	O
sample	O
mean	O
)	O
for	O
every	O
value	O
of	O
θ∗	O
.	O
consequently	O
the	O
sample	O
median	O
is	O
an	O
if	O
θ∗	O
6.3.	O
frequentist	B
decision	O
theory	O
199	O
inadmissible	O
estimator	B
for	O
this	O
particular	O
problem	O
(	O
where	O
the	O
data	O
is	O
assumed	O
to	O
come	O
from	O
a	O
gaussian	O
)	O
.	O
in	O
practice	O
,	O
the	O
sample	O
median	O
is	O
often	O
better	O
than	O
the	O
sample	O
mean	O
,	O
because	O
it	O
is	O
more	O
robust	B
to	O
outliers	B
.	O
one	O
can	O
show	O
(	O
minka	O
2000d	O
)	O
that	O
the	O
median	B
is	O
the	O
bayes	O
estimator	B
(	O
under	O
squared	B
loss	I
)	O
if	O
we	O
assume	O
the	O
data	O
comes	O
from	O
a	O
laplace	O
distribution	O
,	O
which	O
has	O
heavier	O
tails	O
than	O
a	O
gaussian	O
.	O
more	O
generally	O
,	O
we	O
can	O
construct	O
robust	B
estimators	O
by	O
using	O
ﬂexible	O
models	O
of	O
our	O
data	O
,	O
such	O
as	O
mixture	B
models	O
or	O
non-parametric	O
density	O
estimators	O
(	O
section	O
14.7.2	O
)	O
,	O
and	O
then	O
computing	O
the	O
posterior	B
mean	I
or	O
median	B
.	O
6.3.3.2	O
stein	O
’	O
s	O
paradox	O
*	O
suppose	O
we	O
have	O
n	O
iid	B
random	O
variables	O
xi	O
∼	O
n	O
(	O
θi	O
,	O
1	O
)	O
,	O
and	O
we	O
want	O
to	O
estimate	O
the	O
θi	O
.	O
the	O
obvious	O
estimator	B
is	O
the	O
mle	O
,	O
which	O
in	O
this	O
case	O
sets	O
ˆθi	O
=	O
xi	O
.	O
it	O
turns	O
out	O
that	O
this	O
is	O
an	O
inadmissible	O
estimator	B
under	O
quadratic	B
loss	I
,	O
when	O
n	O
≥	O
4.	O
to	O
show	O
this	O
,	O
it	O
suffices	O
to	O
construct	O
an	O
estimator	B
that	O
is	O
better	O
.	O
the	O
james-stein	O
estimator	B
is	O
one	O
such	O
estimator	B
,	O
and	O
is	O
deﬁned	O
as	O
follows	O
:	O
ˆθi	O
=	O
ˆbx	O
+	O
(	O
1−	O
ˆb	O
)	O
xi	O
=	O
x	O
+	O
(	O
1−	O
ˆb	O
)	O
(	O
xi	O
−	O
x	O
)	O
(	O
cid:4	O
)	O
n	O
(	O
6.29	O
)	O
i=1	O
xi	O
and	O
0	O
<	O
b	O
<	O
1	O
is	O
some	O
tuning	O
constant	O
.	O
this	O
estimate	O
“	O
shrinks	O
”	O
the	O
(	O
we	O
derive	O
this	O
estimator	B
using	O
an	O
empirical	O
bayes	O
approach	O
in	O
where	O
x	O
=	O
1	O
n	O
θi	O
towards	O
the	O
overall	O
mean	B
.	O
section	O
5.6.2	O
.	O
)	O
it	O
can	O
be	O
shown	O
that	O
this	O
shrinkage	B
estimator	O
has	O
lower	O
frequentist	B
risk	O
(	O
mse	O
)	O
than	O
the	O
mle	O
(	O
sample	O
mean	O
)	O
for	O
n	O
≥	O
4.	O
this	O
is	O
known	O
as	O
stein	O
’	O
s	O
paradox	O
.	O
the	O
reason	O
it	O
is	O
called	O
a	O
paradox	O
is	O
illustrated	O
by	O
the	O
following	O
example	O
.	O
suppose	O
θi	O
is	O
the	O
“	O
true	O
”	O
iq	O
of	O
student	O
i	O
and	O
xi	O
is	O
his	O
test	O
score	O
.	O
why	O
should	O
my	O
estimate	O
of	O
θi	O
depend	O
on	O
the	O
global	O
mean	O
x	O
,	O
and	O
hence	O
on	O
some	O
other	O
student	O
’	O
s	O
scores	B
?	O
one	O
can	O
create	O
even	O
more	O
paradoxical	O
examples	O
by	O
making	O
the	O
different	O
dimensions	O
be	O
qualitatively	O
different	O
,	O
e.g.	O
,	O
θ1	O
is	O
my	O
iq	O
,	O
θ2	O
is	O
the	O
average	O
rainfall	O
in	O
vancouver	O
,	O
etc	O
.	O
the	O
solution	O
to	O
the	O
paradox	O
is	O
the	O
following	O
.	O
if	O
your	O
goal	O
is	O
to	O
estimate	O
just	O
θi	O
,	O
you	O
can	O
not	O
do	O
better	O
than	O
using	O
xi	O
,	O
but	O
if	O
the	O
goal	O
is	O
to	O
estimate	O
the	O
whole	O
vector	O
θ	O
,	O
and	O
you	O
use	O
squared	B
error	I
as	O
your	O
loss	B
function	I
,	O
then	O
shrinkage	B
helps	O
.	O
to	O
see	O
this	O
,	O
suppose	O
we	O
want	O
to	O
estimate	O
||θ||2	O
2	O
from	O
a	O
single	O
sample	O
x	O
∼	O
n	O
(	O
θ	O
,	O
i	O
)	O
.	O
a	O
simple	O
estimate	O
is	O
||x||2	O
2	O
,	O
but	O
this	O
will	O
overestimate	O
the	O
result	O
,	O
since	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
(	O
cid:17	O
)	O
x	O
(	O
cid:17	O
)	O
2	O
2	O
e	O
(	O
cid:16	O
)	O
(	O
cid:6	O
)	O
(	O
cid:17	O
)	O
n	O
(	O
cid:6	O
)	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
=	O
e	O
x2	O
i	O
=	O
1	O
+	O
θ2	O
i	O
=	O
n	O
+	O
||θ||2	O
2	O
(	O
6.30	O
)	O
i	O
i=1	O
consequently	O
we	O
can	O
reduce	O
our	O
risk	B
by	O
pooling	O
information	B
,	O
even	O
from	O
unrelated	O
sources	O
,	O
and	O
shrinking	O
towards	O
the	O
overall	O
mean	B
.	O
in	O
section	O
5.6.2	O
,	O
we	O
give	O
a	O
bayesian	O
explanation	O
for	O
this	O
.	O
see	O
also	O
(	O
efron	O
and	O
morris	O
1975	O
)	O
.	O
6.3.3.3	O
admissibility	O
is	O
not	O
enough	O
it	O
seems	O
clear	O
that	O
we	O
can	O
restrict	O
our	O
search	O
for	O
good	O
estimators	O
to	O
the	O
class	O
of	O
admissible	B
estimators	O
.	O
but	O
in	O
fact	O
it	O
is	O
easy	O
to	O
construct	O
admissible	B
estimators	O
,	O
as	O
we	O
show	O
in	O
the	O
following	O
example	O
.	O
200	O
chapter	O
6.	O
frequentist	B
statistics	I
=	O
θ0	O
.	O
then	O
r	O
(	O
θ∗	O
,	O
δ1	O
)	O
=	O
0	O
,	O
and	O
r	O
(	O
θ∗	O
,	O
δ2	O
)	O
=	O
(	O
cid:11	O
)	O
theorem	O
6.3.3.	O
let	O
x	O
∼	O
n	O
(	O
θ	O
,	O
1	O
)	O
,	O
and	O
consider	O
estimating	O
θ	O
under	O
squared	B
loss	I
.	O
let	O
δ1	O
(	O
x	O
)	O
=θ	O
0	O
,	O
a	O
constant	O
independent	O
of	O
the	O
data	O
.	O
this	O
is	O
an	O
admissible	B
estimator	O
.	O
proof	O
.	O
suppose	O
not	O
.	O
then	O
there	O
is	O
some	O
other	O
estimator	B
δ2	O
with	O
smaller	O
risk	B
,	O
so	O
r	O
(	O
θ∗	O
,	O
δ2	O
)	O
≤	O
r	O
(	O
θ∗	O
,	O
δ1	O
)	O
,	O
where	O
the	O
inequality	O
must	O
be	O
strict	B
for	O
some	O
θ∗	O
.	O
suppose	O
the	O
true	O
parameter	O
is	O
θ∗	O
(	O
δ2	O
(	O
x	O
)	O
−	O
θ0	O
)	O
2p	O
(	O
x|θ0	O
)	O
dx	O
since	O
0	O
≤	O
r	O
(	O
θ∗	O
,	O
δ2	O
)	O
≤	O
r	O
(	O
θ∗	O
,	O
δ1	O
)	O
for	O
all	O
θ∗	O
,	O
and	O
r	O
(	O
θ0	O
,	O
δ1	O
)	O
=	O
0	O
,	O
we	O
have	O
r	O
(	O
θ0	O
,	O
δ2	O
)	O
=	O
0	O
and	O
hence	O
δ2	O
(	O
x	O
)	O
=θ	O
0	O
=	O
δ1	O
(	O
x	O
)	O
.	O
thus	O
the	O
only	O
way	O
δ2	O
can	O
avoid	O
having	O
higher	O
risk	B
than	O
δ1	O
at	O
some	O
speciﬁc	O
point	O
θ0	O
is	O
by	O
being	O
equal	O
to	O
δ1	O
.	O
hence	O
there	O
is	O
no	O
other	O
estimator	B
δ2	O
with	O
strictly	O
lower	O
risk	B
,	O
so	O
δ2	O
is	O
admissible	B
.	O
(	O
6.31	O
)	O
6.4	O
desirable	O
properties	O
of	O
estimators	O
since	O
frequentist	B
decision	O
theory	O
does	O
not	O
provide	O
an	O
automatic	O
way	O
to	O
choose	O
the	O
best	O
estimator	B
,	O
we	O
need	O
to	O
come	O
up	O
with	O
other	O
heuristics	B
for	O
choosing	O
amongst	O
them	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
some	O
properties	O
we	O
would	O
like	O
estimators	O
to	O
have	O
.	O
unfortunately	O
,	O
we	O
will	O
see	O
that	O
we	O
can	O
not	O
achieve	O
all	O
of	O
these	O
properties	O
at	O
the	O
same	O
time	O
.	O
6.4.1	O
consistent	B
estimators	I
an	O
estimator	B
is	O
said	O
to	O
be	O
consistent	B
if	O
it	O
eventually	O
recovers	O
the	O
true	O
parameters	O
that	O
generated	O
as	O
|d|	O
→	O
∞	O
(	O
where	O
the	O
arrow	O
the	O
data	O
as	O
the	O
sample	O
size	O
goes	O
to	O
inﬁnity	O
,	O
i.e.	O
,	O
ˆθ	O
(	O
d	O
)	O
→	O
θ∗	O
denotes	O
convergence	O
in	O
probability	O
)	O
.	O
of	O
course	O
,	O
this	O
concept	B
only	O
makes	O
sense	O
if	O
the	O
data	O
actually	O
comes	O
from	O
the	O
speciﬁed	O
model	O
with	O
parameters	O
θ∗	O
,	O
which	O
is	O
not	O
usually	O
the	O
case	O
with	O
real	O
+	O
data	O
.	O
nevertheless	O
,	O
it	O
can	O
be	O
a	O
useful	O
theoretical	O
property	O
.	O
p	O
(	O
·|θ∗	O
it	O
can	O
be	O
shown	O
that	O
the	O
mle	O
is	O
a	O
consistent	B
estimator	I
.	O
the	O
intuitive	O
reason	O
is	O
that	O
maxi-	O
)	O
is	O
the	O
true	O
.4	O
,	O
where	O
p	O
(	O
·|θ∗	O
mizing	O
likelihood	B
is	O
equivalent	O
to	O
minimizing	O
kl	O
distribution	O
and	O
p	O
(	O
·|ˆθ	O
)	O
is	O
our	O
estimate	O
.	O
we	O
can	O
achieve	O
0	O
kl	O
divergence	O
iff	B
ˆθ	O
=	O
θ∗	O
)	O
||p	O
(	O
·|ˆθ	O
)	O
,	O
6.4.2	O
unbiased	B
estimators	O
the	O
bias	B
of	O
an	O
estimator	B
is	O
deﬁned	O
as	O
ˆθ	O
(	O
d	O
)	O
−	O
θ∗	O
bias	B
(	O
ˆθ	O
(	O
·	O
)	O
)	O
=	O
ep	O
(	O
d|θ∗	O
)	O
)	O
*	O
(	O
6.32	O
)	O
where	O
θ∗	O
is	O
the	O
true	O
parameter	O
value	O
.	O
if	O
the	O
bias	B
is	O
zero	O
,	O
the	O
estimator	B
is	O
called	O
unbiased	B
.	O
this	O
means	O
the	O
sampling	B
distribution	I
is	O
centered	O
on	O
the	O
true	O
parameter	O
.	O
for	O
example	O
,	O
the	O
mle	O
for	O
a	O
gaussian	O
mean	B
is	O
unbiased	B
:	O
bias	B
(	O
ˆμ	O
)	O
=	O
e	O
[	O
x	O
]	O
−	O
μ	O
=	O
e	O
−	O
μ	O
=	O
0	O
−	O
μ	O
=	O
n	O
(	O
cid:6	O
)	O
(	O
6.33	O
)	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
1	O
n	O
xi	O
i=1	O
n	O
μ	O
n	O
4.	O
if	O
the	O
model	O
is	O
unidentiﬁable	B
,	O
the	O
mle	O
may	O
select	O
a	O
set	O
of	O
parameters	O
that	O
is	O
different	O
from	O
the	O
true	O
parameters	O
but	O
for	O
which	O
the	O
induced	O
distribution	O
,	O
p	O
(	O
·|ˆθ	O
)	O
,	O
is	O
the	O
same	O
as	O
the	O
exact	O
distribution	O
.	O
such	O
parameters	O
are	O
said	O
to	O
be	O
likelihood	B
equivalent	I
.	O
6.4.	O
desirable	O
properties	O
of	O
estimators	O
201	O
however	O
,	O
the	O
mle	O
for	O
a	O
gaussian	O
variance	B
,	O
ˆσ2	O
,	O
is	O
not	O
an	O
unbiased	B
estimator	O
of	O
σ2	O
.	O
in	O
fact	O
,	O
one	O
can	O
show	O
(	O
exercise	O
6.3	O
)	O
that	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
e	O
ˆσ2	O
=	O
n	O
−	O
1	O
n	O
σ2	O
however	O
,	O
the	O
following	O
estimator	B
ˆσ2	O
n−1	O
=	O
ˆσ2	O
=	O
n	O
n	O
−	O
1	O
(	O
cid:2	O
)	O
(	O
cid:21	O
)	O
1	O
n	O
−	O
1	O
(	O
cid:3	O
)	O
n	O
(	O
cid:6	O
)	O
i=1	O
(	O
xi	O
−	O
x	O
)	O
2	O
is	O
an	O
unbiased	B
estimator	O
,	O
which	O
we	O
can	O
easily	O
prove	O
as	O
follows	O
:	O
(	O
cid:20	O
)	O
e	O
ˆσ2	O
n−1	O
=	O
e	O
n	O
n	O
−	O
1	O
ˆσ2	O
=	O
n	O
n	O
−	O
1	O
n	O
−	O
1	O
n	O
σ2	O
=	O
σ2	O
(	O
6.34	O
)	O
(	O
6.35	O
)	O
(	O
6.36	O
)	O
in	O
matlab	O
,	O
var	O
(	O
x	O
)	O
returns	O
ˆσ2	O
n	O
,	O
the	O
difference	O
will	O
be	O
negligible	O
.	O
n−1	O
,	O
whereas	O
var	O
(	O
x,1	O
)	O
returns	O
ˆσ2	O
(	O
the	O
mle	O
)	O
.	O
for	O
large	O
enough	O
although	O
the	O
mle	O
may	O
sometimes	O
be	O
a	O
biased	O
estimator	B
,	O
one	O
can	O
show	O
that	O
asymptotically	O
,	O
it	O
is	O
always	O
unbiased	B
.	O
(	O
this	O
is	O
necessary	O
for	O
the	O
mle	O
to	O
be	O
a	O
consistent	B
estimator	I
.	O
)	O
although	O
being	O
unbiased	B
sounds	O
like	O
a	O
desirable	O
property	O
,	O
this	O
is	O
not	O
always	O
true	O
.	O
see	O
sec-	O
tion	O
6.4.4	O
and	O
(	O
lindley	O
1972	O
)	O
for	O
discussion	O
of	O
this	O
point	O
.	O
6.4.3	O
minimum	O
variance	O
estimators	O
it	O
seems	O
intuitively	O
reasonable	O
that	O
we	O
want	O
our	O
estimator	B
to	O
be	O
unbiased	B
(	O
although	O
we	O
shall	O
give	O
some	O
arguments	O
against	O
this	O
claim	O
below	O
)	O
.	O
however	O
,	O
being	O
unbiased	B
is	O
not	O
enough	O
.	O
for	O
example	O
,	O
suppose	O
we	O
want	O
to	O
estimate	O
the	O
mean	B
of	O
a	O
gaussian	O
from	O
d	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
.	O
the	O
estimator	B
that	O
just	O
looks	O
at	O
the	O
ﬁrst	O
data	O
point	O
,	O
ˆθ	O
(	O
d	O
)	O
=x	O
1	O
,	O
is	O
an	O
unbiased	B
estimator	O
,	O
but	O
will	O
generally	O
be	O
further	O
from	O
θ∗	O
than	O
the	O
empirical	O
mean	O
x	O
(	O
which	O
is	O
also	O
unbiased	B
)	O
.	O
so	O
the	O
variance	B
of	O
an	O
estimator	B
is	O
also	O
important	O
.	O
a	O
natural	O
question	O
is	O
:	O
how	O
long	O
can	O
the	O
variance	B
go	O
?	O
a	O
famous	O
result	O
,	O
called	O
the	O
cramer-	O
rao	O
lower	O
bound	O
,	O
provides	O
a	O
lower	O
bound	O
on	O
the	O
variance	B
of	O
any	O
unbiased	B
estimator	O
.	O
more	O
precisely	O
,	O
theorem	O
6.4.1	O
(	O
cramer-rao	O
inequality	O
)	O
.	O
let	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
∼	O
p	O
(	O
x|θ0	O
)	O
and	O
ˆθ	O
=	O
ˆθ	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
be	O
an	O
unbiased	B
estimator	O
of	O
θ0	O
.	O
then	O
,	O
under	O
various	O
smoothness	O
assumptions	O
on	O
p	O
(	O
x|θ0	O
)	O
,	O
we	O
have	O
)	O
*	O
var	O
ˆθ	O
≥	O
1	O
ni	O
(	O
θ0	O
)	O
(	O
6.37	O
)	O
where	O
i	O
(	O
θ0	O
)	O
is	O
the	O
fisher	O
information	B
matrix	O
(	O
see	O
section	O
6.2.2	O
)	O
.	O
a	O
proof	O
can	O
be	O
found	O
e.g.	O
,	O
in	O
(	O
rice	O
1995	O
,	O
p275	O
)	O
.	O
it	O
can	O
be	O
shown	O
that	O
the	O
mle	O
achieves	O
the	O
cramer	O
rao	O
lower	O
bound	O
,	O
and	O
hence	O
has	O
the	O
smallest	O
asymptotic	O
variance	B
of	O
any	O
unbiased	B
estimator	O
.	O
thus	O
mle	O
is	O
said	O
to	O
be	O
asymptotically	B
optimal	I
.	O
202	O
chapter	O
6.	O
frequentist	B
statistics	I
6.4.4	O
the	O
bias-variance	B
tradeoff	I
∗	O
(	O
cid:2	O
)	O
∗	O
(	O
cid:3	O
)	O
∗	O
)	O
2	O
(	O
cid:2	O
)	O
e	O
(	O
cid:3	O
)	O
ˆθ	O
although	O
using	O
an	O
unbiased	B
estimator	O
seems	O
like	O
a	O
good	O
idea	O
,	O
this	O
is	O
not	O
always	O
the	O
case	O
.	O
to	O
see	O
why	O
,	O
suppose	O
we	O
use	O
quadratic	B
loss	I
.	O
as	O
we	O
showed	O
above	O
,	O
the	O
corresponding	O
risk	B
is	O
the	O
mse	O
.	O
we	O
now	O
derive	O
a	O
very	O
useful	O
decomposition	O
of	O
the	O
mse	O
.	O
(	O
all	O
expectations	O
and	O
variances	O
are	O
wrt	O
the	O
true	O
distribution	O
p	O
(	O
d|θ	O
)	O
,	O
but	O
we	O
drop	O
the	O
explicit	O
conditioning	O
for	O
notational	O
brevity	O
.	O
)	O
let	O
ˆθ	O
=	O
ˆθ	O
(	O
d	O
)	O
denote	O
the	O
estimate	O
,	O
and	O
θ	O
=	O
e	O
denote	O
the	O
expected	B
value	I
of	O
the	O
estimate	O
(	O
as	O
we	O
vary	O
d	O
)	O
.	O
then	O
we	O
have	O
(	O
cid:4	O
)	O
(	O
cid:2	O
)	O
(	O
ˆθ	O
−	O
θ	O
)	O
+	O
(	O
θ	O
−	O
θ	O
(	O
cid:4	O
)	O
(	O
cid:6	O
)	O
ˆθ	O
−	O
θ	O
(	O
cid:4	O
)	O
(	O
cid:6	O
)	O
ˆθ	O
−	O
θ	O
(	O
cid:2	O
)	O
(	O
cid:3	O
)	O
ˆθ	O
)	O
+	O
2	O
(	O
θ	O
−	O
θ	O
+	O
(	O
θ	O
−	O
θ	O
+	O
(	O
θ	O
−	O
θ	O
(	O
cid:7	O
)	O
2	O
(	O
cid:7	O
)	O
2	O
(	O
ˆθ	O
−	O
θ	O
+	O
bias2	O
(	O
ˆθ	O
)	O
ˆθ	O
−	O
θ	O
=	O
var	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
=	O
e	O
=	O
e	O
=	O
e	O
(	O
cid:5	O
)	O
(	O
cid:3	O
)	O
2	O
(	O
6.38	O
)	O
(	O
6.40	O
)	O
(	O
6.39	O
)	O
∗	O
)	O
e	O
∗	O
)	O
2	O
(	O
cid:2	O
)	O
(	O
cid:3	O
)	O
∗	O
)	O
2	O
(	O
6.41	O
)	O
in	O
words	O
,	O
mse	O
=	O
variance	B
+	O
bias2	O
(	O
6.42	O
)	O
this	O
is	O
called	O
the	O
bias-variance	B
tradeoff	I
(	O
see	O
e.g.	O
,	O
(	O
geman	O
et	O
al	O
.	O
1992	O
)	O
)	O
.	O
what	O
it	O
means	O
is	O
that	O
it	O
might	O
be	O
wise	O
to	O
use	O
a	O
biased	O
estimator	B
,	O
so	O
long	O
as	O
it	O
reduces	O
our	O
variance	B
,	O
assuming	O
our	O
goal	O
is	O
to	O
minimize	O
squared	B
error	I
.	O
6.4.4.1	O
example	O
:	O
estimating	O
a	O
gaussian	O
mean	B
let	O
us	O
give	O
an	O
example	O
,	O
based	O
on	O
(	O
hoff	O
2009	O
,	O
p79	O
)	O
.	O
suppose	O
we	O
want	O
to	O
estimate	O
the	O
mean	B
of	O
a	O
gaussian	O
from	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
.	O
we	O
assume	O
the	O
data	O
is	O
sampled	O
from	O
xi	O
∼	O
n	O
(	O
θ	O
=	O
1	O
,	O
σ2	O
)	O
.	O
an	O
obvious	O
estimate	O
is	O
the	O
mle	O
.	O
this	O
has	O
a	O
bias	B
of	O
0	O
and	O
a	O
variance	B
of	O
∗	O
var	O
[	O
x|θ	O
∗	O
]	O
=	O
σ2	O
n	O
(	O
6.43	O
)	O
but	O
we	O
could	O
also	O
use	O
a	O
map	O
estimate	O
.	O
in	O
section	O
4.6.1	O
,	O
we	O
show	O
that	O
the	O
map	O
estimate	O
under	O
a	O
gaussian	O
prior	O
of	O
the	O
form	O
n	O
(	O
θ0	O
,	O
σ2/κ0	O
)	O
is	O
given	O
by	O
˜x	O
(	O
cid:2	O
)	O
n	O
κ0	O
θ0	O
=	O
wx	O
+	O
(	O
1−	O
w	O
)	O
θ0	O
x	O
+	O
n	O
+	O
κ0	O
n	O
+	O
κ0	O
(	O
6.44	O
)	O
where	O
0	O
≤	O
w	O
≤	O
1	O
controls	O
how	O
much	O
we	O
trust	O
the	O
mle	O
compared	O
to	O
our	O
prior	O
.	O
(	O
this	O
is	O
also	O
the	O
posterior	B
mean	I
,	O
since	O
the	O
mean	B
and	O
mode	B
of	O
a	O
gaussian	O
are	O
the	O
same	O
.	O
)	O
the	O
bias	B
and	O
variance	B
are	O
given	O
by	O
e	O
[	O
˜x	O
]	O
−	O
θ	O
=	O
wθ0	O
+	O
(	O
1−	O
w	O
)	O
θ0	O
−	O
θ	O
=	O
(	O
1	O
−	O
w	O
)	O
(	O
θ0	O
−	O
θ	O
(	O
6.45	O
)	O
∗	O
∗	O
∗	O
)	O
var	O
[	O
˜x	O
]	O
=w	O
2	O
σ2	O
n	O
(	O
6.46	O
)	O
6.4.	O
desirable	O
properties	O
of	O
estimators	O
1.5	O
1	O
0.5	O
0	O
−1	O
sampling	B
distribution	I
,	O
truth	O
=	O
1.0	O
,	O
prior	O
=	O
0.0	O
,	O
n	O
=	O
5	O
postmean0	O
postmean1	O
postmean2	O
postmean3	O
−0.5	O
0	O
0.5	O
1	O
1.5	O
2	O
2.5	O
(	O
a	O
)	O
mse	O
of	O
postmean	O
/	O
mse	O
of	O
mle	O
203	O
postmean0	O
postmean1	O
postmean2	O
postmean3	O
10	O
20	O
30	O
40	O
50	O
sample	O
size	O
(	O
b	O
)	O
e	O
s	O
m	O
e	O
v	O
i	O
t	O
l	O
a	O
e	O
r	O
1.3	O
1.2	O
1.1	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0	O
left	O
:	O
sampling	B
distribution	I
of	O
the	O
map	O
estimate	O
with	O
different	O
prior	O
strengths	O
κ0	O
.	O
figure	O
6.4	O
(	O
the	O
mle	O
corresponds	O
to	O
κ0	O
=	O
0	O
.	O
)	O
right	O
:	O
mse	O
relative	O
to	O
that	O
of	O
the	O
mle	O
versus	O
sample	O
size	O
.	O
based	O
on	O
figure	O
5.6	O
of	O
(	O
hoff	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
samplingdistgaussshrinkage	O
.	O
let	O
us	O
assume	O
that	O
our	O
prior	O
is	O
slightly	O
misspeciﬁed	O
,	O
so	O
we	O
use	O
θ0	O
=	O
0	O
,	O
whereas	O
the	O
truth	O
is	O
=	O
1.	O
in	O
figure	O
6.4	O
(	O
a	O
)	O
,	O
we	O
see	O
that	O
the	O
sampling	B
distribution	I
of	O
the	O
map	O
estimate	O
for	O
κ0	O
>	O
0	O
so	O
although	O
the	O
map	O
estimate	O
is	O
biased	O
(	O
assuming	O
w	O
<	O
1	O
)	O
,	O
it	O
has	O
lower	O
variance	B
.	O
θ∗	O
is	O
biased	O
away	O
from	O
the	O
truth	O
,	O
but	O
has	O
lower	O
variance	B
(	O
is	O
narrower	O
)	O
than	O
that	O
of	O
the	O
mle	O
.	O
in	O
figure	O
6.4	O
(	O
b	O
)	O
,	O
we	O
plot	O
mse	O
(	O
˜x	O
)	O
/mse	O
(	O
x	O
)	O
vs	O
n	O
.	O
we	O
see	O
that	O
the	O
map	O
estimate	O
has	O
lower	O
mse	O
than	O
the	O
mle	O
,	O
especially	O
for	O
small	O
sample	O
size	O
,	O
for	O
κ0	O
∈	O
{	O
1	O
,	O
2	O
}	O
.	O
the	O
case	O
κ0	O
=	O
0	O
corresponds	O
to	O
the	O
mle	O
,	O
and	O
the	O
case	O
κ0	O
=	O
3	O
corresponds	O
to	O
a	O
strong	O
prior	O
,	O
which	O
hurts	O
performance	O
because	O
the	O
prior	O
mean	B
is	O
wrong	O
.	O
it	O
is	O
clearly	O
important	O
to	O
“	O
tune	O
”	O
the	O
strength	O
of	O
the	O
prior	O
,	O
a	O
topic	B
we	O
discuss	O
later	O
.	O
6.4.4.2	O
example	O
:	O
ridge	B
regression	I
another	O
important	O
example	O
of	O
the	O
bias	B
variance	O
tradeoff	O
arises	O
in	O
ridge	B
regression	I
,	O
which	O
we	O
discuss	O
in	O
section	O
7.5.	O
in	O
brief	O
,	O
this	O
corresponds	O
to	O
map	O
estimation	O
for	O
linear	B
regression	I
under	O
a	O
gaussian	O
prior	O
,	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
w|0	O
,	O
λ−1i	O
)	O
the	O
zero-mean	O
prior	O
encourages	O
the	O
weights	O
to	O
be	O
small	O
,	O
which	O
reduces	O
overﬁtting	B
;	O
the	O
precision	B
term	O
,	O
λ	O
,	O
controls	O
the	O
strength	O
of	O
this	O
prior	O
.	O
setting	O
λ	O
=	O
0	O
results	O
in	O
the	O
mle	O
;	O
using	O
λ	O
>	O
0	O
results	O
in	O
a	O
biased	O
estimate	O
.	O
to	O
illustrate	O
the	O
effect	O
on	O
the	O
variance	B
,	O
consider	O
a	O
simple	O
example	O
.	O
figure	O
6.5	O
on	O
the	O
left	O
plots	O
each	O
individual	O
ﬁtted	O
curve	O
,	O
and	O
on	O
the	O
right	O
plots	O
the	O
average	O
ﬁtted	O
curve	O
.	O
we	O
see	O
that	O
as	O
we	O
increase	O
the	O
strength	O
of	O
the	O
regularizer	O
,	O
the	O
variance	B
decreases	O
,	O
but	O
the	O
bias	B
increases	O
.	O
6.4.4.3	O
bias-variance	B
tradeoff	I
for	O
classiﬁcation	B
if	O
we	O
use	O
0-1	O
loss	B
instead	O
of	O
squared	B
error	I
,	O
the	O
above	O
analysis	O
breaks	O
down	O
,	O
since	O
the	O
frequentist	B
risk	O
is	O
no	O
longer	O
expressible	O
as	O
squared	O
bias	O
plus	O
variance	B
.	O
in	O
fact	O
,	O
one	O
can	O
show	O
(	O
exercise	O
7.2	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
)	O
that	O
the	O
bias	B
and	O
variance	B
combine	O
multiplicatively	O
.	O
if	O
the	O
estimate	O
is	O
on	O
204	O
chapter	O
6.	O
frequentist	B
statistics	I
ln	O
(	O
λ	O
)	O
=	O
5	O
ln	O
(	O
λ	O
)	O
=	O
5	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
ln	O
(	O
λ	O
)	O
=	O
−5	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
1	O
0.5	O
0	O
−0.5	O
−1	O
0	O
1	O
0.5	O
0	O
−0.5	O
−1	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
ln	O
(	O
λ	O
)	O
=	O
−5	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
figure	O
6.5	O
illustration	O
of	O
bias-variance	B
tradeoff	I
for	O
ridge	B
regression	I
.	O
we	O
generate	O
100	O
data	O
sets	O
from	O
the	O
true	O
function	O
,	O
shown	O
in	O
solid	O
green	O
.	O
left	O
:	O
we	O
plot	O
the	O
regularized	O
ﬁt	O
for	O
20	O
different	O
data	O
sets	O
.	O
we	O
use	O
linear	B
regression	I
with	O
a	O
gaussian	O
rbf	O
expansion	O
,	O
with	O
25	O
centers	O
evenly	O
spread	O
over	O
the	O
[	O
0	O
,	O
1	O
]	O
interval	O
.	O
right	O
:	O
we	O
plot	O
the	O
average	O
of	O
the	O
ﬁts	O
,	O
averaged	O
over	O
all	O
100	O
datasets	O
.	O
top	O
row	O
:	O
strongly	O
regularized	O
:	O
we	O
see	O
that	O
the	O
individual	O
ﬁts	O
are	O
similar	B
to	O
each	O
other	O
(	O
low	O
variance	O
)	O
,	O
but	O
the	O
average	O
is	O
far	O
from	O
the	O
truth	O
(	O
high	O
bias	O
)	O
.	O
bottom	O
row	O
:	O
lightly	O
regularized	O
:	O
we	O
see	O
that	O
the	O
individual	O
ﬁts	O
are	O
quite	O
different	O
from	O
each	O
other	O
(	O
high	O
variance	O
)	O
,	O
but	O
the	O
average	O
is	O
close	O
to	O
the	O
truth	O
(	O
low	O
bias	O
)	O
.	O
based	O
on	O
(	O
bishop	O
2006a	O
)	O
figure	O
3.5.	O
figure	O
generated	O
by	O
biasvarmodelcomplexity3	O
.	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	I
,	O
then	O
the	O
bias	B
is	O
negative	O
,	O
and	O
decreasing	O
the	O
variance	B
will	O
decrease	O
the	O
misclassiﬁcation	B
rate	I
.	O
but	O
if	O
the	O
estimate	O
is	O
on	O
the	O
wrong	O
side	O
of	O
the	O
decision	B
boundary	I
,	O
then	O
the	O
bias	B
is	O
positive	O
,	O
so	O
it	O
pays	O
to	O
increase	O
the	O
variance	B
(	O
friedman	O
1997a	O
)	O
.	O
this	O
little	O
known	O
fact	O
illustrates	O
that	O
the	O
bias-variance	B
tradeoff	I
is	O
not	O
very	O
useful	O
for	O
classiﬁcation	B
.	O
it	O
is	O
better	O
to	O
focus	O
on	O
expected	O
loss	O
(	O
see	O
below	O
)	O
,	O
not	O
directly	O
on	O
bias	B
and	O
variance	B
.	O
we	O
can	O
approximate	O
the	O
expected	O
loss	O
using	O
cross	O
validatinon	O
,	O
as	O
we	O
discuss	O
in	O
section	O
6.5.3	O
.	O
6.5	O
empirical	B
risk	I
minimization	I
frequentist	O
decision	B
theory	O
suffers	O
from	O
the	O
fundamental	O
problem	O
that	O
one	O
can	O
not	O
actually	O
compute	O
the	O
risk	B
function	O
,	O
since	O
it	O
relies	O
on	O
knowing	O
the	O
true	O
data	O
distribution	O
.	O
(	O
by	O
contrast	O
,	O
the	O
bayesian	O
posterior	B
expected	I
loss	I
can	O
always	O
be	O
computed	O
,	O
since	O
it	O
conditions	O
on	O
the	O
the	O
data	O
rather	O
than	O
conditioning	B
on	O
θ∗	O
.	O
)	O
however	O
,	O
there	O
is	O
one	O
setting	O
which	O
avoids	O
this	O
problem	O
,	O
and	O
that	O
is	O
where	O
the	O
task	O
is	O
to	O
predict	O
observable	O
quantities	O
,	O
as	O
opposed	O
to	O
estimating	O
hidden	B
variables	I
or	O
parameters	O
.	O
that	O
is	O
,	O
instead	O
of	O
looking	O
at	O
loss	B
functions	O
of	O
the	O
form	O
l	O
(	O
θ	O
,	O
δ	O
(	O
d	O
)	O
)	O
,	O
where	O
θ	O
is	O
the	O
true	O
but	O
unknown	B
parameter	O
,	O
and	O
δ	O
(	O
d	O
)	O
is	O
our	O
estimator	B
,	O
let	O
us	O
look	O
at	O
loss	B
6.5.	O
empirical	B
risk	I
minimization	I
205	O
functions	O
of	O
the	O
form	O
l	O
(	O
y	O
,	O
δ	O
(	O
x	O
)	O
)	O
,	O
where	O
y	O
is	O
the	O
true	O
but	O
unknown	B
response	O
,	O
and	O
δ	O
(	O
x	O
)	O
is	O
our	O
prediction	O
given	O
the	O
input	O
x.	O
in	O
this	O
case	O
,	O
the	O
frequentist	B
risk	O
becomes	O
r	O
(	O
p∗	O
,	O
δ	O
)	O
(	O
cid:2	O
)	O
e	O
(	O
x	O
,	O
y	O
)	O
∼p∗	O
[	O
l	O
(	O
y	O
,	O
δ	O
(	O
x	O
)	O
]	O
=	O
l	O
(	O
y	O
,	O
δ	O
(	O
x	O
)	O
)	O
p∗	O
(	O
x	O
,	O
y	O
)	O
(	O
6.47	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
x	O
y	O
where	O
p∗	O
represents	O
“	O
nature	O
’	O
s	O
distribution	O
”	O
.	O
of	O
course	O
,	O
this	O
distribution	O
is	O
unknown	B
,	O
but	O
a	O
simple	O
approach	O
is	O
to	O
use	O
the	O
empirical	B
distribution	I
,	O
derived	O
from	O
some	O
training	O
data	O
,	O
to	O
approximate	O
p∗	O
,	O
i.e.	O
,	O
p∗	O
(	O
x	O
,	O
y	O
)	O
≈	O
pemp	O
(	O
x	O
,	O
y	O
)	O
(	O
cid:2	O
)	O
1	O
n	O
δxi	O
(	O
x	O
)	O
δyi	O
(	O
y	O
)	O
we	O
then	O
deﬁne	O
the	O
empirical	B
risk	I
as	O
follows	O
:	O
remp	O
(	O
d	O
,	O
d	O
)	O
(	O
cid:2	O
)	O
r	O
(	O
pemp	O
,	O
δ	O
)	O
=	O
1	O
n	O
l	O
(	O
yi	O
,	O
δ	O
(	O
xi	O
)	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
6.48	O
)	O
(	O
6.49	O
)	O
in	O
the	O
case	O
of	O
0-1	O
loss	B
,	O
l	O
(	O
y	O
,	O
δ	O
(	O
x	O
)	O
)	O
=	O
i	O
(	O
y	O
(	O
cid:4	O
)	O
=	O
δ	O
(	O
x	O
)	O
)	O
,	O
this	O
becomes	O
the	O
misclassiﬁcation	B
rate	I
.	O
in	O
the	O
case	O
of	O
squared	B
error	I
loss	O
,	O
l	O
(	O
y	O
,	O
δ	O
(	O
x	O
)	O
)	O
=	O
(	O
y−δ	O
(	O
x	O
)	O
)	O
2	O
,	O
this	O
becomes	O
the	O
mean	B
squared	I
error	I
.	O
we	O
deﬁne	O
the	O
task	O
of	O
empirical	B
risk	I
minimization	I
or	O
erm	O
as	O
ﬁnding	O
a	O
decision	B
procedure	I
(	O
typically	O
a	O
classiﬁcation	B
rule	O
)	O
to	O
minimize	O
the	O
empirical	B
risk	I
:	O
δerm	O
(	O
d	O
)	O
=	O
argmin	O
δ	O
remp	O
(	O
d	O
,	O
δ	O
)	O
(	O
6.50	O
)	O
in	O
the	O
unsupervised	O
case	O
,	O
we	O
eliminate	O
all	O
references	O
to	O
y	O
,	O
and	O
replace	O
l	O
(	O
y	O
,	O
δ	O
(	O
x	O
)	O
)	O
with	O
l	O
(	O
x	O
,	O
δ	O
(	O
x	O
)	O
)	O
,	O
where	O
,	O
for	O
example	O
,	O
l	O
(	O
x	O
,	O
δ	O
(	O
x	O
)	O
)	O
=	O
||x	O
−	O
δ	O
(	O
x	O
)	O
||2	O
2	O
,	O
which	O
measures	O
the	O
reconstruc-	O
tion	O
error	O
.	O
we	O
can	O
deﬁne	O
the	O
decision	B
rule	I
using	O
δ	O
(	O
x	O
)	O
=decode	O
(	O
encode	O
(	O
x	O
)	O
)	O
,	O
as	O
in	O
vector	B
quantization	I
(	O
section	O
11.4.2.6	O
)	O
or	O
pca	O
(	O
section	O
12.2	O
)	O
.	O
finally	O
,	O
we	O
deﬁne	O
the	O
empirical	B
risk	I
as	O
l	O
(	O
xi	O
,	O
δ	O
(	O
xi	O
)	O
)	O
(	O
6.51	O
)	O
remp	O
(	O
d	O
,	O
δ	O
)	O
=	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
i=1	O
of	O
course	O
,	O
we	O
can	O
always	O
trivially	O
minimize	O
this	O
risk	B
by	O
setting	O
δ	O
(	O
x	O
)	O
=	O
x	O
,	O
so	O
it	O
is	O
critical	O
that	O
the	O
encoder-decoder	O
go	O
via	O
some	O
kind	O
of	O
bottleneck	B
.	O
6.5.1	O
regularized	B
risk	I
minimization	I
note	O
that	O
the	O
empirical	B
risk	I
is	O
equal	O
to	O
the	O
bayes	O
risk	B
if	O
our	O
prior	O
about	O
“	O
nature	O
’	O
s	O
distribution	O
”	O
is	O
that	O
it	O
is	O
exactly	O
equal	O
to	O
the	O
empirical	B
distribution	I
(	O
minka	O
2001b	O
)	O
:	O
e	O
[	O
r	O
(	O
p∗	O
,	O
δ	O
)	O
|p∗	O
=	O
pemp	O
]	O
=	O
remp	O
(	O
d	O
,	O
δ	O
)	O
(	O
6.52	O
)	O
therefore	O
minimizing	O
the	O
empirical	B
risk	I
will	O
typically	O
result	O
in	O
overﬁtting	B
.	O
necessary	O
to	O
add	O
a	O
complexity	O
penalty	O
to	O
the	O
objective	O
function	O
:	O
it	O
is	O
therefore	O
often	O
r	O
(	O
cid:4	O
)	O
(	O
d	O
,	O
δ	O
)	O
=	O
remp	O
(	O
d	O
,	O
δ	O
)	O
+λc	O
(	O
δ	O
)	O
(	O
6.53	O
)	O
206	O
chapter	O
6.	O
frequentist	B
statistics	I
where	O
c	O
(	O
δ	O
)	O
measures	O
the	O
complexity	O
of	O
the	O
prediction	O
function	O
δ	O
(	O
x	O
)	O
and	O
λ	O
controls	O
the	O
strength	O
of	O
the	O
complexity	O
penalty	O
.	O
this	O
approach	O
is	O
known	O
as	O
regularized	B
risk	I
minimization	I
(	O
rrm	O
)	O
.	O
note	O
that	O
if	O
the	O
loss	B
function	I
is	O
negative	B
log	I
likelihood	I
,	O
and	O
the	O
regularizer	O
is	O
a	O
negative	O
log	O
prior	O
,	O
this	O
is	O
equivalent	O
to	O
map	O
estimation	O
.	O
the	O
two	O
key	O
issues	O
in	O
rrm	O
are	O
:	O
how	O
do	O
we	O
measure	O
complexity	O
,	O
and	O
how	O
do	O
we	O
pick	O
λ.	O
for	O
a	O
linear	O
model	O
,	O
we	O
can	O
deﬁne	O
the	O
complexity	O
of	O
in	O
terms	O
of	O
its	O
degrees	B
of	I
freedom	I
,	O
discussed	O
in	O
section	O
7.5.3.	O
for	O
more	O
general	O
models	O
,	O
we	O
can	O
use	O
the	O
vc	O
dimension	O
,	O
discussed	O
in	O
section	O
6.5.4.	O
to	O
pick	O
λ	O
,	O
we	O
can	O
use	O
the	O
methods	O
discussed	O
in	O
section	O
6.5.2	O
.	O
6.5.2	O
structural	B
risk	I
minimization	I
the	O
regularized	B
risk	I
minimization	I
principle	O
says	O
that	O
we	O
should	O
ﬁt	O
the	O
model	O
,	O
for	O
a	O
given	O
complexity	O
penalty	O
,	O
by	O
using	O
ˆδλ	O
=	O
argmin	O
δ	O
[	O
remp	O
(	O
d	O
,	O
δ	O
)	O
+λc	O
(	O
δ	O
)	O
]	O
(	O
6.54	O
)	O
but	O
how	O
should	O
we	O
pick	O
λ	O
?	O
we	O
can	O
not	O
using	O
the	O
training	B
set	I
,	O
since	O
this	O
will	O
underestimate	O
the	O
true	O
risk	O
,	O
a	O
problem	O
known	O
as	O
optimism	B
of	I
the	I
training	I
error	I
.	O
as	O
an	O
alternative	O
,	O
we	O
can	O
use	O
the	O
following	O
rule	O
,	O
known	O
as	O
the	O
structural	B
risk	I
minimization	I
principle	O
:	O
(	O
vapnik	O
1998	O
)	O
:	O
ˆλ	O
=	O
argmin	O
λ	O
ˆr	O
(	O
ˆδλ	O
)	O
(	O
6.55	O
)	O
where	O
ˆr	O
(	O
δ	O
)	O
is	O
an	O
estimate	O
of	O
the	O
risk	B
.	O
there	O
are	O
two	O
widely	O
used	O
estimates	O
:	O
cross	B
validation	I
and	O
theoretical	O
upper	O
bounds	O
on	O
the	O
risk	B
.	O
we	O
discuss	O
both	O
of	O
these	O
below	O
.	O
6.5.3	O
estimating	O
the	O
risk	B
using	O
cross	B
validation	I
we	O
can	O
estimate	O
the	O
risk	B
of	O
some	O
estimator	B
using	O
a	O
validation	B
set	I
.	O
if	O
we	O
don	O
’	O
t	O
have	O
a	O
separate	O
validation	B
set	I
,	O
we	O
can	O
use	O
cross	B
validation	I
(	O
cv	O
)	O
,	O
as	O
we	O
brieﬂy	O
discussed	O
in	O
section	O
1.4.8.	O
more	O
precisely	O
,	O
cv	O
is	O
deﬁned	O
as	O
follows	O
.	O
let	O
there	O
be	O
n	O
=	O
|d|	O
data	O
cases	O
in	O
the	O
training	B
set	I
.	O
denote	O
the	O
data	O
in	O
the	O
k	O
’	O
th	O
test	O
fold	O
by	O
dk	O
and	O
all	O
the	O
other	O
data	O
by	O
d−k	O
.	O
(	O
in	O
stratiﬁed	O
cv	O
,	O
these	O
folds	B
are	O
chosen	O
so	O
the	O
class	O
proportions	O
(	O
if	O
discrete	B
labels	O
are	O
present	O
)	O
are	O
roughly	O
equal	O
in	O
each	O
fold	O
.	O
)	O
let	O
f	O
be	O
a	O
learning	B
algorithm	O
or	O
ﬁtting	O
function	O
that	O
takes	O
a	O
dataset	O
and	O
a	O
model	O
index	O
m	O
(	O
this	O
could	O
a	O
discrete	B
index	O
,	O
such	O
as	O
the	O
degree	B
of	O
a	O
polynomial	O
,	O
or	O
a	O
continuous	O
index	O
,	O
such	O
as	O
the	O
strength	O
of	O
a	O
regularizer	O
)	O
and	O
returns	O
a	O
parameter	B
vector	O
:	O
ˆθm	O
=	O
f	O
(	O
d	O
,	O
m	O
)	O
(	O
6.56	O
)	O
finally	O
,	O
let	O
p	O
be	O
a	O
prediction	O
function	O
that	O
takes	O
an	O
input	O
and	O
a	O
parameter	B
vector	O
and	O
returns	O
a	O
prediction	O
:	O
ˆy	O
=	O
p	O
(	O
x	O
,	O
ˆθ	O
)	O
=	O
f	O
(	O
x	O
,	O
ˆθ	O
)	O
thus	O
the	O
combined	O
ﬁt-predict	B
cycle	I
is	O
denoted	O
as	O
fm	O
(	O
x	O
,	O
d	O
)	O
=	O
p	O
(	O
x	O
,	O
f	O
(	O
d	O
,	O
m	O
)	O
)	O
(	O
6.57	O
)	O
(	O
6.58	O
)	O
6.5.	O
empirical	B
risk	I
minimization	I
207	O
k	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i∈dk	O
the	O
k-fold	O
cv	O
estimate	O
of	O
the	O
risk	B
of	O
fm	O
is	O
deﬁned	O
by	O
r	O
(	O
m	O
,	O
d	O
,	O
k	O
)	O
(	O
cid:2	O
)	O
1	O
n	O
l	O
(	O
yi	O
,	O
p	O
(	O
xi	O
,	O
f	O
(	O
d−k	O
,	O
m	O
)	O
)	O
)	O
(	O
6.59	O
)	O
m	O
(	O
x	O
)	O
=p	O
(	O
x	O
,	O
f	O
(	O
d−k	O
,	O
m	O
)	O
)	O
be	O
note	O
that	O
we	O
can	O
call	O
the	O
ﬁtting	O
algorithm	O
once	O
per	O
fold	O
.	O
let	O
f	O
k	O
the	O
function	O
that	O
was	O
trained	O
on	O
all	O
the	O
data	O
except	O
for	O
the	O
test	O
data	O
in	O
fold	O
k.	O
then	O
we	O
can	O
rewrite	O
the	O
cv	O
estimate	O
as	O
k=1	O
n	O
(	O
cid:2	O
)	O
(	O
cid:5	O
)	O
(	O
cid:4	O
)	O
1	O
n	O
(	O
cid:6	O
)	O
l	O
yi	O
,	O
f	O
k	O
m	O
(	O
xi	O
)	O
=	O
l	O
yi	O
,	O
f	O
k	O
(	O
i	O
)	O
m	O
(	O
xi	O
)	O
(	O
6.60	O
)	O
r	O
(	O
m	O
,	O
d	O
,	O
k	O
)	O
=	O
(	O
cid:3	O
)	O
k	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i∈dk	O
1	O
n	O
i=1	O
where	O
k	O
(	O
i	O
)	O
is	O
the	O
fold	O
in	O
which	O
i	O
is	O
used	O
as	O
test	O
data	O
.	O
model	O
that	O
was	O
trained	O
on	O
data	O
that	O
does	O
not	O
contain	O
xi	O
.	O
k=1	O
in	O
other	O
words	O
,	O
we	O
predict	O
yi	O
using	O
a	O
of	O
k	O
=	O
n	O
,	O
the	O
method	O
is	O
known	O
as	O
leave	B
one	I
out	I
cross	I
validation	I
or	O
loocv	O
.	O
in	O
this	O
case	O
,	O
n	O
the	O
estimated	O
risk	O
becomes	O
r	O
(	O
m	O
,	O
d	O
,	O
n	O
)	O
=	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
cid:3	O
)	O
(	O
cid:4	O
)	O
yi	O
,	O
f−i	O
m	O
(	O
xi	O
)	O
(	O
6.61	O
)	O
l	O
m	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
,	O
f	O
(	O
d−i	O
,	O
m	O
)	O
)	O
.	O
this	O
requires	O
ﬁtting	O
the	O
model	O
n	O
times	O
,	O
where	O
for	O
f−i	O
where	O
f	O
i	O
m	O
we	O
omit	O
the	O
i	O
’	O
th	O
training	O
case	O
.	O
fortunately	O
,	O
for	O
some	O
model	O
classes	O
and	O
loss	B
functions	O
(	O
namely	O
linear	O
models	O
and	O
quadratic	B
loss	I
)	O
,	O
we	O
can	O
ﬁt	O
the	O
model	O
once	O
,	O
and	O
analytically	O
“	O
remove	O
”	O
the	O
effect	O
of	O
the	O
i	O
’	O
th	O
training	O
case	O
.	O
this	O
is	O
known	O
as	O
generalized	B
cross	I
validation	I
or	O
gcv	O
.	O
6.5.3.1	O
example	O
:	O
using	O
cv	O
to	O
pick	O
λ	O
for	O
ridge	B
regression	I
k	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i∈dk	O
as	O
a	O
concrete	O
example	O
,	O
consider	O
picking	O
the	O
strength	O
of	O
the	O
(	O
cid:6	O
)	O
2	O
regularizer	O
in	O
penalized	O
linear	O
regression	B
.	O
we	O
use	O
the	O
following	O
rule	O
:	O
r	O
(	O
λ	O
,	O
dtrain	O
,	O
k	O
)	O
ˆλ	O
=	O
arg	O
min	O
λ∈	O
[	O
λmin	O
,	O
λmax	O
]	O
(	O
6.62	O
)	O
where	O
[	O
λmin	O
,	O
λmax	O
]	O
is	O
a	O
ﬁnite	O
range	O
of	O
λ	O
values	O
that	O
we	O
search	O
over	O
,	O
and	O
r	O
(	O
λ	O
,	O
dtrain	O
,	O
k	O
)	O
is	O
the	O
k-fold	O
cv	O
estimate	O
of	O
the	O
risk	B
of	O
using	O
λ	O
,	O
given	O
by	O
r	O
(	O
λ	O
,	O
dtrain	O
,	O
k	O
)	O
=	O
1	O
|dtrain|	O
(	O
6.63	O
)	O
λ	O
(	O
x	O
)	O
=	O
xt	O
ˆwλ	O
(	O
d−k	O
)	O
is	O
the	O
prediction	O
function	O
trained	O
on	O
data	O
excluding	O
fold	O
k	O
,	O
and	O
2	O
is	O
the	O
map	O
estimate	O
.	O
figure	O
6.6	O
(	O
b	O
)	O
gives	O
an	O
example	O
where	O
f	O
k	O
ˆwλ	O
(	O
d	O
)	O
=	O
arg	O
minw	O
n	O
ll	O
(	O
w	O
,	O
d	O
)	O
+λ||w||2	O
of	O
a	O
cv	O
estimate	O
of	O
the	O
risk	B
vs	O
log	O
(	O
λ	O
)	O
,	O
where	O
the	O
loss	B
function	I
is	O
squared	B
error	I
.	O
k=1	O
l	O
(	O
yi	O
,	O
f	O
k	O
λ	O
(	O
xi	O
)	O
)	O
when	O
performing	O
classiﬁcation	B
,	O
we	O
usually	O
use	O
0-1	O
loss	B
.	O
in	O
this	O
case	O
,	O
we	O
optimize	O
a	O
convex	B
upper	O
bound	O
on	O
the	O
empirical	B
risk	I
to	O
estimate	O
wλm	O
but	O
we	O
optimize	O
(	O
the	O
cv	O
estimate	O
of	O
)	O
the	O
risk	B
itself	O
to	O
estimate	O
λ.	O
we	O
can	O
handle	O
the	O
non-smooth	B
0-1	O
loss	B
function	I
when	O
estimating	O
λ	O
because	O
we	O
are	O
using	O
brute-force	O
search	O
over	O
the	O
entire	O
(	O
one-dimensional	O
)	O
space	O
.	O
when	O
we	O
have	O
more	O
than	O
one	O
or	O
two	O
tuning	O
parameters	O
,	O
this	O
approach	O
becomes	O
infeasible	O
.	O
in	O
such	O
cases	O
,	O
one	O
can	O
use	O
empirical	O
bayes	O
,	O
which	O
allows	O
one	O
to	O
optimize	O
large	O
numbers	O
of	O
hyper-parameters	B
using	O
gradient-based	O
optimizers	O
instead	O
of	O
brute-force	O
search	O
.	O
see	O
section	O
5.6	O
for	O
details	O
.	O
chapter	O
6.	O
frequentist	B
statistics	I
5−fold	O
cross	B
validation	I
,	O
ntrain	O
=	O
50	O
208	O
14	O
12	O
10	O
8	O
6	O
4	O
2	O
mean	B
squared	I
error	I
train	O
mse	O
test	O
mse	O
20	O
18	O
16	O
14	O
12	O
10	O
8	O
6	O
4	O
2	O
e	O
s	O
m	O
0	O
−25	O
−20	O
−15	O
−10	O
log	O
lambda	O
(	O
a	O
)	O
−5	O
0	O
5	O
0	O
−15	O
−10	O
0	O
5	O
−5	O
log	O
lambda	O
(	O
b	O
)	O
figure	O
6.6	O
(	O
a	O
)	O
mean	B
squared	I
error	I
for	O
(	O
cid:7	O
)	O
2	O
penalized	O
degree	O
14	O
polynomial	B
regression	I
vs	O
log	O
regularizer	O
.	O
same	O
as	O
in	O
figures	O
7.8	O
,	O
except	O
now	O
we	O
have	O
n	O
=	O
50	O
training	O
points	O
instead	O
of	O
21.	O
the	O
stars	O
correspond	O
to	O
the	O
values	O
used	O
to	O
plot	O
the	O
functions	O
in	O
figure	O
7.7	O
.	O
(	O
b	O
)	O
cv	O
estimate	O
.	O
the	O
vertical	O
scale	O
is	O
truncated	O
for	O
clarity	O
.	O
the	O
blue	O
line	O
corresponds	O
to	O
the	O
value	O
chosen	O
by	O
the	O
one	O
standard	B
error	I
rule	O
.	O
figure	O
generated	O
by	O
linregpolyvsregdemo	O
.	O
6.5.3.2	O
the	O
one	O
standard	B
error	I
rule	O
the	O
above	O
procedure	O
estimates	O
the	O
risk	B
,	O
but	O
does	O
not	O
give	O
any	O
measure	O
of	O
uncertainty	B
.	O
a	O
standard	O
frequentist	O
measure	O
of	O
uncertainty	B
of	O
an	O
estimate	O
is	O
the	O
standard	B
error	I
of	I
the	I
mean	I
,	O
deﬁned	O
by	O
ˆσ√	O
n	O
(	O
cid:7	O
)	O
ˆσ2	O
n	O
(	O
6.64	O
)	O
se	O
=	O
=	O
where	O
ˆσ2	O
is	O
an	O
estimate	O
of	O
the	O
variance	B
of	O
the	O
loss	B
:	O
(	O
li	O
−	O
l	O
)	O
2	O
,	O
li	O
=	O
l	O
(	O
yi	O
,	O
f	O
k	O
(	O
i	O
)	O
m	O
(	O
xi	O
)	O
)	O
l	O
=	O
n	O
(	O
cid:2	O
)	O
i=1	O
ˆσ2	O
=	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
i=1	O
1	O
n	O
li	O
(	O
6.65	O
)	O
note	O
that	O
σ	O
measures	O
the	O
intrinsic	O
variability	O
of	O
li	O
across	O
samples	B
,	O
whereas	O
se	O
measures	O
our	O
uncertainty	B
about	O
the	O
mean	B
l.	O
suppose	O
we	O
apply	O
cv	O
to	O
a	O
set	O
of	O
models	O
and	O
compute	O
the	O
mean	B
and	O
se	O
of	O
their	O
estimated	O
risks	O
.	O
a	O
common	O
heuristic	O
for	O
picking	O
a	O
model	O
from	O
these	O
noisy	O
estimates	O
is	O
to	O
pick	O
the	O
value	O
which	O
corresponds	O
to	O
the	O
simplest	O
model	O
whose	O
risk	B
is	O
no	O
more	O
than	O
one	O
standard	B
error	I
above	O
the	O
risk	B
of	O
the	O
best	O
model	O
;	O
this	O
is	O
called	O
the	O
one-standard	B
error	I
rule	I
(	O
hastie	O
et	O
al	O
.	O
2001	O
,	O
p216	O
)	O
.	O
for	O
example	O
,	O
in	O
figure	O
6.6	O
,	O
we	O
see	O
that	O
this	O
heuristic	O
does	O
not	O
choose	O
the	O
lowest	O
point	O
on	O
the	O
curve	O
,	O
but	O
one	O
that	O
is	O
slightly	O
to	O
its	O
right	O
,	O
since	O
that	O
corresponds	O
to	O
a	O
more	O
heavily	O
regularized	O
model	O
with	O
essentially	O
the	O
same	O
empirical	O
performance	O
.	O
6.5.	O
empirical	B
risk	I
minimization	I
209	O
6.5.3.3	O
cv	O
for	O
model	B
selection	I
in	O
non-probabilistic	O
unsupervised	B
learning	I
if	O
we	O
are	O
performing	O
unsupervised	B
learning	I
,	O
we	O
must	O
use	O
a	O
loss	B
function	I
such	O
as	O
l	O
(	O
x	O
,	O
δ	O
(	O
x	O
)	O
)	O
=	O
||x	O
−	O
δ	O
(	O
x	O
)	O
||2	O
,	O
which	O
measures	O
reconstruction	B
error	I
.	O
here	O
δ	O
(	O
x	O
)	O
is	O
some	O
encode-decode	O
scheme	O
.	O
however	O
,	O
as	O
we	O
discussed	O
in	O
section	O
11.5.2	O
,	O
we	O
can	O
not	O
use	O
cv	O
to	O
determine	O
the	O
complexity	O
of	O
δ	O
,	O
since	O
we	O
will	O
always	O
get	O
lower	O
loss	B
with	O
a	O
more	O
complex	O
model	O
,	O
even	O
if	O
evaluated	O
on	O
the	O
test	O
set	O
.	O
this	O
is	O
because	O
more	O
complex	O
models	O
will	O
compress	O
the	O
data	O
less	O
,	O
and	O
induce	O
less	O
distortion	B
.	O
consequently	O
,	O
we	O
must	O
either	O
use	O
probabilistic	O
models	O
,	O
or	O
invent	O
other	O
heuristics	B
.	O
6.5.4	O
upper	O
bounding	O
the	O
risk	B
using	O
statistical	B
learning	I
theory	I
*	O
the	O
principle	O
problem	O
with	O
cross	B
validation	I
is	O
that	O
it	O
is	O
slow	O
,	O
since	O
we	O
have	O
to	O
ﬁt	O
the	O
model	O
multiple	O
times	O
.	O
this	O
motivates	O
the	O
desire	O
to	O
compute	O
analytic	O
approximations	O
or	O
bounds	O
to	O
the	O
generalization	B
error	I
.	O
this	O
is	O
the	O
studied	O
in	O
the	O
ﬁeld	O
of	O
statistical	B
learning	I
theory	I
(	O
slt	O
)	O
.	O
more	O
precisely	O
,	O
slt	O
tries	O
to	O
bound	O
the	O
risk	B
r	O
(	O
p∗	O
,	O
h	O
)	O
for	O
any	O
data	O
distribution	O
p∗	O
and	O
hypothesis	O
h	O
∈	O
h	O
in	O
terms	O
of	O
the	O
empirical	B
risk	I
remp	O
(	O
d	O
,	O
h	O
)	O
,	O
the	O
sample	O
size	O
n	O
=	O
|d|	O
,	O
and	O
the	O
size	O
of	O
the	O
hypothesis	B
space	I
h.	O
let	O
us	O
initially	O
consider	O
the	O
case	O
where	O
the	O
hypothesis	B
space	I
is	O
ﬁnite	O
,	O
with	O
size	O
dim	O
(	O
h	O
)	O
=	O
|h|	O
.	O
in	O
other	O
words	O
,	O
we	O
are	O
selecting	O
a	O
model/	O
hypothesis	O
from	O
a	O
ﬁnite	O
list	O
,	O
rather	O
than	O
optimizing	O
real-valued	O
parameters	O
,	O
then	O
we	O
can	O
prove	O
the	O
following	O
.	O
theorem	O
6.5.1.	O
for	O
any	O
data	O
distribution	O
p∗	O
,	O
and	O
any	O
dataset	O
d	O
of	O
size	O
n	O
drawn	O
from	O
p∗	O
,	O
the	O
probability	O
that	O
our	O
estimate	O
of	O
the	O
error	O
rate	O
will	O
be	O
more	O
than	O
	O
wrong	O
,	O
in	O
the	O
worst	O
case	O
,	O
is	O
upper	O
bounded	O
as	O
follows	O
:	O
|remp	O
(	O
d	O
,	O
h	O
)	O
−	O
r	O
(	O
p∗	O
,	O
h	O
)	O
|	O
>	O
	O
≤	O
2	O
dim	O
(	O
h	O
)	O
e−2n	O
2	O
(	O
6.66	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
p	O
max	O
h∈h	O
proof	O
.	O
to	O
prove	O
this	O
,	O
we	O
need	O
two	O
useful	O
results	O
.	O
first	O
,	O
hoeffding	O
’	O
s	O
inequality	O
,	O
which	O
states	O
that	O
if	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
∼	O
ber	O
(	O
θ	O
)	O
,	O
then	O
,	O
for	O
any	O
	O
>	O
0	O
,	O
p	O
(	O
|x	O
−	O
θ|	O
>	O
	O
)	O
≤	O
2e−2n	O
2	O
(	O
cid:10	O
)	O
n	O
i=1ai	O
)	O
≤	O
(	O
cid:10	O
)	O
d	O
where	O
x	O
=	O
1	O
n	O
events	O
,	O
then	O
p	O
(	O
∪d	O
i=1	O
xi	O
.	O
second	O
,	O
the	O
union	B
bound	I
,	O
which	O
says	O
that	O
if	O
a1	O
,	O
.	O
.	O
.	O
,	O
ad	O
are	O
a	O
set	O
of	O
finally	O
,	O
for	O
notational	O
brevity	O
,	O
let	O
r	O
(	O
h	O
)	O
=	O
r	O
(	O
h	O
,	O
p∗	O
)	O
be	O
the	O
true	O
risk	O
,	O
and	O
ˆrn	O
(	O
h	O
)	O
=	O
remp	O
(	O
d	O
,	O
h	O
)	O
i=1	O
p	O
(	O
ai	O
)	O
.	O
be	O
the	O
empirical	B
risk	I
.	O
using	O
these	O
results	O
we	O
have	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
p	O
max	O
h∈h	O
|	O
ˆrn	O
(	O
h	O
)	O
−	O
r	O
(	O
h	O
)	O
|	O
>	O
	O
=	O
p	O
(	O
cid:11	O
)	O
(	O
cid:12	O
)	O
(	O
cid:5	O
)	O
(	O
cid:2	O
)	O
h∈h	O
(	O
cid:2	O
)	O
h∈h	O
h∈h	O
p	O
≤	O
≤	O
(	O
cid:13	O
)	O
|	O
ˆrn	O
(	O
h	O
)	O
−	O
r	O
(	O
h	O
)	O
|	O
>	O
	O
(	O
cid:6	O
)	O
|	O
ˆrn	O
(	O
h	O
)	O
−	O
r	O
(	O
h	O
)	O
|	O
>	O
	O
2e−2n	O
2	O
=	O
2	O
dim	O
(	O
h	O
)	O
e−2n	O
2	O
(	O
6.67	O
)	O
(	O
6.68	O
)	O
(	O
6.69	O
)	O
(	O
6.70	O
)	O
210	O
chapter	O
6.	O
frequentist	B
statistics	I
ths	O
bound	O
tells	O
us	O
that	O
the	O
optimism	B
of	I
the	I
training	I
error	I
increases	O
with	O
dim	O
(	O
h	O
)	O
but	O
de-	O
creases	O
with	O
n	O
=	O
|d|	O
,	O
as	O
is	O
to	O
be	O
expected	O
.	O
if	O
the	O
hypothesis	B
space	I
h	O
is	O
inﬁnite	O
(	O
e.g.	O
,	O
we	O
have	O
real-valued	O
parameters	O
)	O
,	O
we	O
can	O
not	O
use	O
dim	O
(	O
h	O
)	O
=|h|	O
.	O
instead	O
,	O
we	O
can	O
use	O
a	O
quantity	O
called	O
the	O
vapnik-chervonenkis	O
or	O
vc	O
dimen-	O
sion	O
of	O
the	O
hypothesis	O
class	O
.	O
see	O
(	O
vapnik	O
1998	O
)	O
for	O
details	O
.	O
stepping	O
back	O
from	O
all	O
the	O
theory	O
,	O
the	O
key	O
intuition	O
behind	O
statistical	B
learning	I
theory	I
is	O
quite	O
if	O
the	O
hypothesis	B
space	I
h	O
is	O
very	O
simple	O
.	O
suppose	O
we	O
ﬁnd	O
a	O
model	O
with	O
low	O
empirical	O
risk	B
.	O
big	O
,	O
relative	O
to	O
the	O
data	O
size	O
,	O
then	O
it	O
is	O
quite	O
likely	O
that	O
we	O
just	O
got	O
“	O
lucky	O
”	O
and	O
were	O
given	O
a	O
data	O
set	O
that	O
is	O
well-modeled	O
by	O
our	O
chosen	O
function	O
by	O
chance	O
.	O
however	O
,	O
this	O
does	O
not	O
mean	O
that	O
such	O
a	O
function	O
will	O
have	O
low	O
generalization	O
error	O
.	O
but	O
if	O
the	O
hypothesis	O
class	O
is	O
sufficiently	O
constrained	O
in	O
size	O
,	O
and/or	O
the	O
training	B
set	I
is	O
sufficiently	O
large	O
,	O
then	O
we	O
are	O
unlikely	O
to	O
get	O
lucky	O
in	O
this	O
way	O
,	O
so	O
a	O
low	O
empirical	O
risk	B
is	O
evidence	B
of	O
a	O
low	O
true	O
risk	B
.	O
note	O
that	O
optimism	B
of	I
the	I
training	I
error	I
does	O
not	O
necessarily	O
increase	O
with	O
model	O
complexity	O
,	O
but	O
it	O
does	O
increase	O
with	O
the	O
number	O
of	O
different	O
models	O
that	O
are	O
being	O
searched	O
over	O
.	O
the	O
advantage	O
of	O
statistical	B
learning	I
theory	I
compared	O
to	O
cv	O
is	O
that	O
the	O
bounds	O
on	O
the	O
risk	B
are	O
quicker	O
to	O
compute	O
than	O
using	O
cv	O
.	O
the	O
disadvantage	O
is	O
that	O
it	O
is	O
hard	O
to	O
compute	O
the	O
vc	O
dimension	O
for	O
many	O
interesting	O
models	O
,	O
and	O
the	O
upper	O
bounds	O
are	O
usually	O
very	O
loose	O
(	O
although	O
see	O
(	O
kaariainen	O
and	O
langford	O
2005	O
)	O
)	O
.	O
one	O
can	O
extend	O
statistical	B
learning	I
theory	I
by	O
taking	O
computational	O
complexity	O
of	O
the	O
learner	O
into	O
account	O
.	O
this	O
ﬁeld	O
is	O
called	O
computational	B
learning	I
theory	I
or	O
colt	O
.	O
most	O
of	O
this	O
work	O
focuses	O
on	O
the	O
case	O
where	O
h	O
is	O
a	O
binary	O
classiﬁer	O
,	O
and	O
the	O
loss	B
function	I
is	O
0-1	O
loss	B
.	O
if	O
we	O
observe	O
a	O
low	O
empirical	O
risk	B
,	O
and	O
the	O
hypothesis	B
space	I
is	O
suitably	O
“	O
small	O
”	O
,	O
then	O
we	O
can	O
say	O
that	O
our	O
estimated	O
function	O
is	O
probably	B
approximately	I
correct	I
or	O
pac	O
.	O
a	O
hypothesis	B
space	I
is	O
said	O
to	O
be	O
efficiently	O
pac-learnable	O
if	O
there	O
is	O
a	O
polynomial	O
time	O
algorithm	O
that	O
can	O
identify	O
a	O
function	O
that	O
is	O
pac	O
.	O
see	O
(	O
kearns	O
and	O
vazirani	O
1994	O
)	O
for	O
details	O
.	O
6.5.5	O
surrogate	B
loss	I
functions	O
minimizing	O
the	O
loss	B
in	O
the	O
erm/	O
rrm	O
framework	O
is	O
not	O
always	O
easy	O
.	O
for	O
example	O
,	O
we	O
might	O
want	O
to	O
optimize	O
the	O
auc	O
or	O
f1	O
scores	B
.	O
or	O
more	O
simply	O
,	O
we	O
might	O
just	O
want	O
to	O
minimize	O
the	O
0-1	O
loss	B
,	O
as	O
is	O
common	O
in	O
classiﬁcation	B
.	O
unfortunately	O
,	O
the	O
0-1	O
risk	B
is	O
a	O
very	O
non-smooth	B
objective	O
and	O
hence	O
is	O
hard	O
to	O
optimize	O
.	O
one	O
alternative	O
is	O
to	O
use	O
maximum	O
likelihood	O
estimation	O
instead	O
,	O
since	O
log-likelihood	O
is	O
a	O
smooth	O
convex	B
upper	O
bound	O
on	O
the	O
0-1	O
risk	B
,	O
as	O
we	O
show	O
below	O
.	O
to	O
see	O
this	O
,	O
consider	O
binary	O
logistic	O
regression	B
,	O
and	O
let	O
yi	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
.	O
suppose	O
our	O
decision	B
function	O
computes	O
the	O
log-odds	B
ratio	I
,	O
p	O
(	O
y	O
=	O
1|xi	O
,	O
w	O
)	O
p	O
(	O
y	O
=	O
−1|xi	O
,	O
w	O
)	O
f	O
(	O
xi	O
)	O
=	O
log	O
=	O
wt	O
xi	O
=	O
ηi	O
then	O
the	O
corresponding	O
probability	O
distribution	O
on	O
the	O
output	O
label	B
is	O
p	O
(	O
yi|xi	O
,	O
w	O
)	O
=	O
sigm	O
(	O
yiηi	O
)	O
let	O
us	O
deﬁne	O
the	O
log-loss	B
as	O
as	O
l	O
nll	O
(	O
y	O
,	O
η	O
)	O
=	O
−	O
log	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
log	O
(	O
1	O
+	O
e−yη	O
)	O
(	O
6.71	O
)	O
(	O
6.72	O
)	O
(	O
6.73	O
)	O
6.6.	O
pathologies	B
of	O
frequentist	B
statistics	I
*	O
211	O
3	O
2.5	O
2	O
s	O
s	O
o	O
l	O
1.5	O
1	O
0.5	O
0	O
0−1	O
hinge	O
logloss	O
−2	O
−1.5	O
−1	O
−0.5	O
0	O
η	O
0.5	O
1	O
1.5	O
2	O
figure	O
6.7	O
yη	O
,	O
the	O
vertical	O
axis	O
is	O
the	O
loss	B
.	O
the	O
log	O
loss	O
uses	O
log	O
base	O
2.	O
figure	O
generated	O
by	O
hingelossplot	O
.	O
illustration	O
of	O
various	O
loss	B
functions	O
for	O
binary	B
classiﬁcation	I
.	O
the	O
horizontal	O
axis	O
is	O
the	O
margin	B
now	O
consider	O
computing	O
the	O
most	O
probable	O
label	B
,	O
which	O
is	O
equivalent	O
to	O
using	O
ˆy	O
=	O
−1	O
if	O
it	O
is	O
clear	O
that	O
minimizing	O
the	O
average	O
log-loss	O
is	O
equivalent	O
to	O
maximizing	O
the	O
likelihood	B
.	O
ηi	O
<	O
0	O
and	O
ˆy	O
=	O
+1	O
if	O
ηi	O
≥	O
0.	O
the	O
0-1	O
loss	B
of	O
our	O
function	O
becomes	O
l01	O
(	O
y	O
,	O
η	O
)	O
=	O
i	O
(	O
y	O
(	O
cid:4	O
)	O
=	O
ˆy	O
)	O
=	O
i	O
(	O
yη	O
<	O
0	O
)	O
(	O
6.74	O
)	O
figure	O
6.7	O
plots	O
these	O
two	O
loss	B
functions	O
.	O
we	O
see	O
that	O
the	O
nll	O
is	O
indeed	O
an	O
upper	O
bound	O
on	O
the	O
0-1	O
loss	B
.	O
log-loss	B
is	O
an	O
example	O
of	O
a	O
surrogate	B
loss	I
function	I
.	O
another	O
example	O
is	O
the	O
hinge	B
loss	I
:	O
l	O
hinge	O
(	O
y	O
,	O
η	O
)	O
=	O
max	O
(	O
0	O
,	O
1	O
−	O
yη	O
)	O
(	O
6.75	O
)	O
see	O
figure	O
6.7	O
for	O
a	O
plot	O
.	O
we	O
see	O
that	O
the	O
function	O
looks	O
like	O
a	O
door	O
hinge	O
,	O
hence	O
its	O
name	O
.	O
this	O
loss	B
function	I
forms	O
the	O
basis	O
of	O
a	O
popular	O
classiﬁcation	B
method	O
known	O
as	O
support	B
vector	I
machines	I
(	O
svm	O
)	O
,	O
which	O
we	O
will	O
discuss	O
in	O
section	O
14.5.	O
the	O
surrogate	O
is	O
usually	O
chosen	O
to	O
be	O
a	O
convex	B
upper	O
bound	O
,	O
since	O
convex	B
functions	O
are	O
easy	O
to	O
minimize	O
.	O
see	O
e.g.	O
,	O
(	O
bartlett	O
et	O
al	O
.	O
2006	O
)	O
for	O
more	O
information	B
.	O
6.6	O
pathologies	B
of	O
frequentist	B
statistics	I
*	O
i	O
believe	O
that	O
it	O
would	O
be	O
very	O
difficult	O
to	O
persuade	O
an	O
intelligent	O
person	O
that	O
current	O
[	O
frequentist	B
]	O
statistical	O
practice	O
was	O
sensible	O
,	O
but	O
that	O
there	O
would	O
be	O
much	O
less	O
difficulty	O
with	O
an	O
approach	O
via	O
likelihood	B
and	O
bayes	O
’	O
theorem	O
.	O
—	O
george	O
box	O
,	O
1962.	O
frequentist	B
statistics	I
exhibits	O
various	O
forms	O
of	O
weird	O
and	O
undesirable	O
behaviors	O
,	O
known	O
as	O
pathologies	B
.	O
we	O
give	O
a	O
few	O
examples	O
below	O
,	O
in	O
order	O
to	O
caution	O
the	O
reader	O
;	O
these	O
and	O
other	O
examples	O
are	O
explained	O
in	O
more	O
detail	O
in	O
(	O
lindley	O
1972	O
;	O
lindley	O
and	O
phillips	O
1976	O
;	O
lindley	O
1982	O
;	O
berger	O
1985	O
;	O
jaynes	O
2003	O
;	O
minka	O
1999	O
)	O
.	O
212	O
chapter	O
6.	O
frequentist	B
statistics	I
6.6.1	O
counter-intuitive	O
behavior	O
of	O
conﬁdence	B
intervals	I
a	O
conﬁdence	B
interval	I
is	O
an	O
interval	O
derived	O
from	O
the	O
sampling	B
distribution	I
of	O
an	O
estimator	B
(	O
whereas	O
a	O
bayesian	O
credible	B
interval	I
is	O
derived	O
from	O
the	O
posterior	O
of	O
a	O
parameter	B
,	O
as	O
we	O
dis-	O
cussed	O
in	O
section	O
5.2.2	O
)	O
.	O
more	O
precisely	O
,	O
a	O
frequentist	B
conﬁdence	O
interval	O
for	O
some	O
parameter	B
θ	O
is	O
deﬁned	O
by	O
the	O
following	O
(	O
rather	O
un-natural	O
)	O
expression	O
:	O
α	O
(	O
θ	O
)	O
=	O
(	O
(	O
cid:6	O
)	O
,	O
u	O
)	O
:	O
p	O
(	O
(	O
cid:6	O
)	O
(	O
˜d	O
)	O
≤	O
θ	O
≤	O
u	O
(	O
˜d	O
)	O
|	O
˜d	O
∼	O
θ	O
)	O
=	O
1	O
−	O
α	O
c	O
(	O
cid:4	O
)	O
(	O
6.76	O
)	O
that	O
is	O
,	O
if	O
we	O
sample	O
hypothetical	O
future	O
data	O
˜d	O
from	O
θ	O
,	O
then	O
(	O
(	O
cid:6	O
)	O
(	O
˜d	O
)	O
,	O
u	O
(	O
˜d	O
)	O
)	O
,	O
is	O
a	O
conﬁdence	B
interval	I
if	O
the	O
parameter	B
θ	O
lies	O
inside	O
this	O
interval	O
1	O
−	O
α	O
percent	O
of	O
the	O
time	O
.	O
in	O
bayesian	O
statistics	O
,	O
we	O
condition	O
on	O
what	O
is	O
known	O
—	O
namely	O
the	O
observed	O
data	O
,	O
d	O
—	O
and	O
average	O
over	O
what	O
is	O
not	O
known	O
,	O
namely	O
the	O
parameter	B
θ.	O
in	O
frequentist	B
statistics	I
,	O
we	O
do	O
exactly	O
the	O
opposite	O
:	O
we	O
condition	O
on	O
what	O
is	O
unknown	B
—	O
namely	O
the	O
true	O
parameter	O
value	O
θ	O
—	O
and	O
average	O
over	O
hypothetical	O
future	O
data	O
sets	O
˜d	O
.	O
this	O
counter-intuitive	O
deﬁnition	O
of	O
conﬁdence	B
intervals	I
can	O
lead	O
to	O
bizarre	O
results	O
.	O
consider	O
the	O
following	O
example	O
from	O
(	O
berger	O
1985	O
,	O
p11	O
)	O
.	O
suppose	O
we	O
draw	O
two	O
integers	O
d	O
=	O
(	O
x1	O
,	O
x2	O
)	O
from	O
let	O
us	O
step	O
back	O
for	O
a	O
moment	O
and	O
think	O
about	O
what	O
is	O
going	O
on	O
.	O
⎧⎨	O
⎩	O
0.5	O
p	O
(	O
x|θ	O
)	O
=	O
if	O
x	O
=	O
θ	O
0.5	O
if	O
x	O
=	O
θ	O
+	O
1	O
0	O
otherwise	O
(	O
6.77	O
)	O
(	O
6.78	O
)	O
(	O
6.79	O
)	O
(	O
6.80	O
)	O
if	O
θ	O
=	O
39	O
,	O
we	O
would	O
expect	O
the	O
following	O
outcomes	O
each	O
with	O
probability	O
0.25	O
:	O
(	O
39	O
,	O
39	O
)	O
,	O
(	O
39	O
,	O
40	O
)	O
,	O
(	O
40	O
,	O
39	O
)	O
,	O
(	O
40	O
,	O
40	O
)	O
let	O
m	O
=	O
min	O
(	O
x1	O
,	O
x2	O
)	O
and	O
deﬁne	O
the	O
following	O
conﬁdence	B
interval	I
:	O
[	O
(	O
cid:6	O
)	O
(	O
d	O
)	O
,	O
u	O
(	O
d	O
)	O
]	O
=	O
[	O
m	O
,	O
m	O
]	O
for	O
the	O
above	O
samples	B
this	O
yields	O
[	O
39	O
,	O
39	O
]	O
,	O
[	O
39	O
,	O
39	O
]	O
,	O
[	O
39	O
,	O
39	O
]	O
,	O
[	O
40	O
,	O
40	O
]	O
hence	O
equation	O
6.79	O
is	O
clearly	O
a	O
75	O
%	O
ci	O
,	O
since	O
39	O
is	O
contained	O
in	O
3/4	O
of	O
these	O
intervals	O
.	O
however	O
,	O
if	O
d	O
=	O
(	O
39	O
,	O
40	O
)	O
then	O
p	O
(	O
θ	O
=	O
39|d	O
)	O
=	O
1.0	O
,	O
so	O
we	O
know	O
that	O
θ	O
must	O
be	O
39	O
,	O
yet	O
we	O
only	O
have	O
75	O
%	O
“	O
conﬁdence	O
”	O
in	O
this	O
fact	O
.	O
(	O
cid:10	O
)	O
n	O
another	O
,	O
less	O
contrived	O
example	O
,	O
is	O
as	O
follows	O
.	O
suppose	O
we	O
want	O
to	O
estimate	O
the	O
parameter	B
θ	O
(	O
cid:17	O
)	O
of	O
a	O
bernoulli	O
distribution	O
.	O
let	O
x	O
=	O
1	O
i=1	O
xi	O
be	O
the	O
sample	O
mean	O
.	O
the	O
mle	O
is	O
ˆθ	O
=	O
x.	O
an	O
n	O
x	O
(	O
1	O
−	O
x	O
)	O
/n	O
(	O
this	O
is	O
approximate	O
95	O
%	O
conﬁdence	B
interval	I
for	O
a	O
bernoulli	O
parameter	B
is	O
x±	O
1.96	O
called	O
a	O
wald	O
interval	O
and	O
is	O
based	O
on	O
a	O
gaussian	O
approximation	O
to	O
the	O
binomial	B
distribution	I
;	O
compare	O
to	O
equation	O
3.27	O
)	O
.	O
now	O
consider	O
a	O
single	O
trial	O
,	O
where	O
n	O
=	O
1	O
and	O
x1	O
=	O
0.	O
the	O
mle	O
is	O
0	O
,	O
which	O
overﬁts	O
,	O
as	O
we	O
saw	O
in	O
section	O
3.3.4.1.	O
but	O
our	O
95	O
%	O
conﬁdence	B
interval	I
is	O
also	O
(	O
0	O
,	O
0	O
)	O
,	O
which	O
seems	O
even	O
worse	O
.	O
it	O
can	O
be	O
argued	O
that	O
the	O
above	O
ﬂaw	O
is	O
because	O
we	O
approximated	O
the	O
true	O
sampling	O
distribution	O
with	O
a	O
gaussian	O
,	O
or	O
because	O
the	O
sample	O
size	O
was	O
to	O
small	O
,	O
or	O
the	O
parameter	B
“	O
too	O
extreme	O
”	O
.	O
however	O
,	O
the	O
wald	O
interval	O
can	O
behave	O
badly	O
even	O
for	O
large	O
n	O
,	O
and	O
non-extreme	O
parameters	O
(	O
brown	O
et	O
al	O
.	O
2001	O
)	O
.	O
6.6.	O
pathologies	B
of	O
frequentist	B
statistics	I
*	O
213	O
6.6.2	O
p-values	O
considered	O
harmful	O
suppose	O
we	O
want	O
to	O
decide	O
whether	O
to	O
accept	B
or	O
reject	O
some	O
baseline	O
model	O
,	O
which	O
we	O
will	O
call	O
the	O
null	B
hypothesis	I
.	O
we	O
need	O
to	O
deﬁne	O
some	O
decision	B
rule	I
.	O
in	O
frequentist	B
statistics	I
,	O
it	O
is	O
standard	O
to	O
ﬁrst	O
compute	O
a	O
quantity	O
called	O
the	O
p-value	B
,	O
which	O
is	O
deﬁned	O
as	O
the	O
probability	O
(	O
under	O
the	O
null	O
)	O
of	O
observing	O
some	O
test	B
statistic	I
f	O
(	O
d	O
)	O
(	O
such	O
as	O
the	O
chi-squared	B
statistic	I
)	O
that	O
is	O
as	O
large	O
or	O
larger	O
than	O
that	O
actually	O
observed:5	O
pvalue	O
(	O
d	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
f	O
(	O
˜d	O
)	O
≥	O
f	O
(	O
d	O
)	O
|	O
˜d	O
∼	O
h0	O
)	O
(	O
6.81	O
)	O
this	O
quantity	O
relies	O
on	O
computing	O
a	O
tail	B
area	I
probability	I
of	O
the	O
sampling	B
distribution	I
;	O
we	O
give	O
an	O
example	O
of	O
how	O
to	O
do	O
this	O
below	O
.	O
given	O
the	O
p-value	B
,	O
we	O
deﬁne	O
our	O
decision	B
rule	I
as	O
follows	O
:	O
we	O
reject	O
the	O
null	B
hypothesis	I
iff	O
the	O
p-value	B
is	O
less	O
than	O
some	O
threshold	O
,	O
such	O
as	O
α	O
=	O
0.05.	O
if	O
we	O
do	O
reject	O
it	O
,	O
we	O
say	O
the	O
difference	O
between	O
the	O
observed	O
test	O
statistic	O
and	O
the	O
expected	O
test	O
statistic	O
is	O
statistically	B
signiﬁcant	I
at	O
level	O
α.	O
this	O
approach	O
is	O
known	O
as	O
null	B
hypothesis	I
signiﬁcance	I
testing	I
,	O
ornhst	O
.	O
this	O
procedure	O
guarantees	O
that	O
our	O
expected	O
type	O
i	O
(	O
false	B
positive	I
)	O
error	O
rate	O
is	O
at	O
most	O
α.	O
this	O
is	O
sometimes	O
interpreted	O
as	O
saying	O
that	O
frequentist	B
hypothesis	O
testing	O
is	O
very	O
conservative	O
,	O
since	O
it	O
is	O
unlikely	O
to	O
accidently	O
reject	O
the	O
null	B
hypothesis	I
.	O
but	O
in	O
fact	O
the	O
opposite	O
is	O
the	O
case	O
:	O
because	O
this	O
method	O
only	O
worries	O
about	O
trying	O
to	O
reject	O
the	O
null	O
,	O
it	O
can	O
never	O
gather	O
evidence	B
in	O
favor	O
of	O
the	O
null	O
,	O
no	O
matter	O
how	O
large	O
the	O
sample	O
size	O
.	O
because	O
of	O
this	O
,	O
p-values	O
tend	O
to	O
overstate	O
the	O
evidence	B
against	O
the	O
null	O
,	O
and	O
are	O
thus	O
very	O
“	O
trigger	O
happy	O
”	O
.	O
in	O
general	O
there	O
can	O
be	O
huge	O
differences	O
between	O
p-values	O
and	O
the	O
quantity	O
that	O
we	O
really	O
care	O
about	O
,	O
which	O
is	O
the	O
posterior	O
probability	O
of	O
the	O
null	B
hypothesis	I
given	O
the	O
data	O
,	O
p	O
(	O
h0|d	O
)	O
.	O
in	O
particular	O
,	O
sellke	O
et	O
al	O
.	O
(	O
2001	O
)	O
show	O
that	O
even	O
if	O
the	O
p-value	B
is	O
as	O
slow	O
as	O
0.05	O
,	O
the	O
posterior	O
probability	O
of	O
h0	O
is	O
at	O
least	O
30	O
%	O
,	O
and	O
often	O
much	O
higher	O
.	O
so	O
frequentists	O
often	O
claim	O
to	O
have	O
“	O
signiﬁcant	O
”	O
evidence	B
of	O
an	O
effect	O
that	O
can	O
not	O
be	O
explained	O
by	O
the	O
null	B
hypothesis	I
,	O
whereas	O
bayesians	O
are	O
usually	O
more	O
conservative	O
in	O
their	O
claims	O
.	O
for	O
example	O
,	O
p-values	O
have	O
been	O
used	O
to	O
“	O
prove	O
”	O
that	O
esp	O
(	O
extra-sensory	O
perception	O
)	O
is	O
real	O
(	O
wagenmakers	O
et	O
al	O
.	O
2011	O
)	O
,	O
even	O
though	O
esp	O
is	O
clearly	O
very	O
improbable	O
.	O
for	O
this	O
reason	O
,	O
p-values	O
have	O
been	O
banned	O
from	O
certain	O
medical	O
journals	O
(	O
matthews	O
1998	O
)	O
.	O
another	O
problem	O
with	O
p-values	O
is	O
that	O
their	O
computation	O
depends	O
on	O
decisions	O
you	O
make	O
about	O
when	O
to	O
stop	O
collecting	O
data	O
,	O
even	O
if	O
these	O
decisions	O
don	O
’	O
t	O
change	O
the	O
data	O
you	O
actually	O
observed	O
.	O
for	O
example	O
,	O
suppose	O
i	O
toss	O
a	O
coin	O
n	O
=	O
12	O
times	O
and	O
observe	O
s	O
=	O
9	O
successes	O
(	O
heads	O
)	O
and	O
f	O
=	O
3	O
failures	O
(	O
tails	O
)	O
,	O
so	O
n	O
=	O
s	O
+	O
f.	O
in	O
this	O
case	O
,	O
n	O
is	O
ﬁxed	O
and	O
s	O
(	O
and	O
hence	O
f	O
)	O
is	O
random	O
.	O
the	O
relevant	O
sampling	O
model	O
is	O
the	O
binomial	B
bin	O
(	O
s|n	O
,	O
θ	O
)	O
=	O
θs	O
(	O
1	O
−	O
θ	O
)	O
n−s	O
(	O
6.82	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
n	O
s	O
let	O
the	O
null	B
hypothesis	I
be	O
that	O
the	O
coin	O
is	O
fair	O
,	O
θ	O
=	O
0.5	O
,	O
where	O
θ	O
is	O
the	O
probability	O
of	O
success	O
(	O
heads	O
)	O
.	O
the	O
one-sided	O
p-value	B
,	O
using	O
test	B
statistic	I
t	O
(	O
s	O
)	O
=	O
s	O
,	O
is	O
0.512	O
=	O
0.073	O
(	O
6.83	O
)	O
12	O
(	O
cid:2	O
)	O
p1	O
=	O
p	O
(	O
s	O
≥	O
9|h0	O
)	O
=	O
bin	O
(	O
s|12	O
,	O
0.5	O
)	O
=	O
s=9	O
s=9	O
(	O
cid:8	O
)	O
12	O
(	O
cid:2	O
)	O
(	O
cid:9	O
)	O
12	O
s	O
5.	O
the	O
reason	O
we	O
can	O
not	O
just	O
compute	O
the	O
probability	O
of	O
the	O
observed	O
value	O
of	O
the	O
test	B
statistic	I
is	O
that	O
this	O
will	O
have	O
probability	O
zero	O
under	O
a	O
pdf	B
.	O
the	O
p-value	B
is	O
deﬁned	O
in	O
terms	O
of	O
the	O
cdf	B
,	O
so	O
is	O
always	O
a	O
number	O
between	O
0	O
and	O
1	O
.	O
214	O
chapter	O
6.	O
frequentist	B
statistics	I
the	O
two-sided	O
p-value	B
is	O
p2	O
=	O
bin	O
(	O
s|12	O
,	O
0.5	O
)	O
+	O
12	O
(	O
cid:2	O
)	O
s=9	O
3	O
(	O
cid:2	O
)	O
s=0	O
bin	O
(	O
s|12	O
,	O
0.5	O
)	O
=	O
0.073	O
+	O
0.073	O
=	O
0.146	O
(	O
6.84	O
)	O
in	O
either	O
case	O
,	O
the	O
p-value	B
is	O
larger	O
than	O
the	O
magical	O
5	O
%	O
threshold	O
,	O
so	O
a	O
frequentist	B
would	O
not	O
reject	O
the	O
null	B
hypothesis	I
.	O
now	O
suppose	O
i	O
told	O
you	O
that	O
i	O
actually	O
kept	O
tossing	O
the	O
coin	O
until	O
i	O
observed	O
f	O
=	O
3	O
tails	O
.	O
in	O
this	O
case	O
,	O
f	O
is	O
ﬁxed	O
and	O
n	O
(	O
and	O
hence	O
s	O
=	O
n	O
−	O
f	O
)	O
is	O
random	O
.	O
the	O
probability	O
model	O
becomes	O
the	O
negative	B
binomial	I
distribution	I
,	O
given	O
by	O
negbinom	O
(	O
s|f	O
,	O
θ	O
)	O
=	O
θs	O
(	O
1	O
−	O
θ	O
)	O
f	O
(	O
6.85	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
s	O
+	O
f	O
−	O
1	O
f	O
−	O
1	O
where	O
f	O
=	O
n	O
−	O
s.	O
note	O
that	O
the	O
term	O
which	O
depends	O
on	O
θ	O
is	O
the	O
same	O
in	O
equations	O
6.82	O
and	O
6.85	O
,	O
so	O
the	O
posterior	O
over	O
θ	O
would	O
be	O
the	O
same	O
in	O
both	O
cases	O
.	O
however	O
,	O
these	O
two	O
interpretations	O
of	O
the	O
same	O
data	O
give	O
different	O
p-values	O
.	O
in	O
particular	O
,	O
under	O
the	O
negative	B
binomial	I
model	O
we	O
get	O
p3	O
=	O
p	O
(	O
s	O
≥	O
9|h0	O
)	O
=	O
(	O
1/2	O
)	O
s	O
(	O
1/2	O
)	O
3	O
=	O
0.0327	O
(	O
6.86	O
)	O
(	O
cid:9	O
)	O
3	O
+	O
s	O
−	O
1	O
(	O
cid:8	O
)	O
∞	O
(	O
cid:2	O
)	O
s=9	O
2	O
so	O
the	O
p-value	B
is	O
3	O
%	O
,	O
and	O
suddenly	O
there	O
seems	O
to	O
be	O
signiﬁcant	O
evidence	B
of	O
bias	B
in	O
the	O
coin	O
!	O
obviously	O
this	O
is	O
ridiculous	O
:	O
the	O
data	O
is	O
the	O
same	O
,	O
so	O
our	O
inferences	O
about	O
the	O
coin	O
should	O
be	O
the	O
same	O
.	O
after	O
all	O
,	O
i	O
could	O
have	O
chosen	O
the	O
experimental	O
protocol	O
at	O
random	O
.	O
it	O
is	O
the	O
outcome	O
of	O
the	O
experiment	O
that	O
matters	O
,	O
not	O
the	O
details	O
of	O
how	O
i	O
decided	O
which	O
one	O
to	O
run	O
.	O
although	O
this	O
might	O
seem	O
like	O
just	O
a	O
mathematical	O
curiosity	O
,	O
this	O
also	O
has	O
signiﬁcant	O
practical	O
in	O
particular	O
,	O
the	O
fact	O
that	O
the	O
stopping	B
rule	I
affects	O
the	O
computation	O
of	O
the	O
p-	O
implications	O
.	O
value	O
means	O
that	O
frequentists	O
often	O
do	O
not	O
terminate	O
experiments	O
early	O
,	O
even	O
when	O
it	O
is	O
obvious	O
what	O
the	O
conclusions	O
are	O
,	O
lest	O
it	O
adversely	O
affect	O
their	O
statistical	O
analysis	O
.	O
if	O
the	O
experiments	O
are	O
costly	O
or	O
harmful	O
to	O
people	O
,	O
this	O
is	O
obviously	O
a	O
bad	O
idea	O
.	O
perhaps	O
it	O
is	O
not	O
surprising	O
,	O
then	O
,	O
that	O
the	O
us	O
food	O
and	O
drug	O
administration	O
(	O
fda	O
)	O
,	O
which	O
regulates	O
clinical	O
trials	O
of	O
new	O
drugs	O
,	O
has	O
recently	O
become	O
supportive	O
of	O
bayesian	O
methods6	O
,	O
since	O
bayesian	O
methods	O
are	O
not	O
affected	O
by	O
the	O
stopping	B
rule	I
.	O
6.6.3	O
the	O
likelihood	B
principle	I
the	O
fundamental	O
reason	O
for	O
many	O
of	O
these	O
pathologies	B
is	O
that	O
frequentist	B
inference	O
violates	O
the	O
likelihood	B
principle	I
,	O
which	O
says	O
that	O
inference	B
should	O
be	O
based	O
on	O
the	O
likelihood	B
of	O
the	O
observed	O
data	O
,	O
not	O
based	O
on	O
hypothetical	O
future	O
data	O
that	O
you	O
have	O
not	O
observed	O
.	O
bayes	O
obviously	O
satisﬁes	O
the	O
likelihood	B
principle	I
,	O
and	O
consequently	O
does	O
not	O
suffer	O
from	O
these	O
pathologies	B
.	O
a	O
compelling	O
argument	O
in	O
favor	O
of	O
the	O
likelihood	B
principle	I
was	O
presented	O
in	O
(	O
birnbaum	O
1962	O
)	O
,	O
who	O
showed	O
that	O
it	O
followed	O
automatically	O
from	O
two	O
simpler	O
principles	O
.	O
the	O
ﬁrst	O
of	O
these	O
is	O
the	O
sufficiency	B
principle	I
,	O
which	O
says	O
that	O
a	O
sufficient	O
statistic	O
contains	O
all	O
the	O
relevant	O
information	B
6.	O
see	O
http	O
:	O
//yamlb.wordpress.com/2006/06/19/the-us-fda-is-becoming-progressively-more-bayes	O
ian/	O
.	O
6.6.	O
pathologies	B
of	O
frequentist	B
statistics	I
*	O
215	O
about	O
an	O
unknown	B
parameter	O
(	O
arguably	O
this	O
is	O
true	O
by	O
deﬁnition	O
)	O
.	O
the	O
second	O
principle	O
is	O
known	O
as	O
weak	B
conditionality	I
,	O
which	O
says	O
that	O
inferences	O
should	O
be	O
based	O
on	O
the	O
events	O
that	O
happened	O
,	O
not	O
which	O
might	O
have	O
happened	O
.	O
to	O
motivate	O
this	O
,	O
consider	O
an	O
example	O
from	O
(	O
berger	O
1985	O
)	O
.	O
suppose	O
we	O
need	O
to	O
analyse	O
a	O
substance	O
,	O
and	O
can	O
send	O
it	O
either	O
to	O
a	O
laboratory	O
in	O
new	O
york	O
or	O
in	O
california	O
.	O
the	O
two	O
labs	O
seem	O
equally	O
good	O
,	O
so	O
a	O
fair	O
coin	O
is	O
used	O
to	O
decide	O
between	O
them	O
.	O
the	O
coin	O
comes	O
up	O
heads	O
,	O
so	O
the	O
california	O
lab	O
is	O
chosen	O
.	O
when	O
the	O
results	O
come	O
back	O
,	O
should	O
it	O
be	O
taken	O
into	O
account	O
that	O
the	O
coin	O
could	O
have	O
come	O
up	O
tails	O
and	O
thus	O
the	O
new	O
york	O
lab	O
could	O
have	O
been	O
used	O
?	O
most	O
people	O
would	O
argue	O
that	O
the	O
new	O
york	O
lab	O
is	O
irrelevant	O
,	O
since	O
the	O
tails	O
event	O
didn	O
’	O
t	O
happen	O
.	O
this	O
is	O
an	O
example	O
of	O
weak	B
conditionality	I
.	O
given	O
this	O
principle	O
,	O
one	O
can	O
show	O
that	O
all	O
inferences	O
should	O
only	O
be	O
based	O
on	O
what	O
was	O
observed	O
,	O
which	O
is	O
in	O
contrast	O
to	O
standard	O
frequentist	O
procedures	O
.	O
see	O
(	O
berger	O
and	O
wolpert	O
1988	O
)	O
for	O
further	O
details	O
on	O
the	O
likelihood	B
principle	I
.	O
6.6.4	O
why	O
isn	O
’	O
t	O
everyone	O
a	O
bayesian	O
?	O
given	O
these	O
fundamental	O
ﬂaws	O
of	O
frequentist	B
statistics	I
,	O
and	O
the	O
fact	O
that	O
bayesian	O
methods	O
do	O
not	O
have	O
such	O
ﬂaws	O
,	O
an	O
obvious	O
question	O
to	O
ask	O
is	O
:	O
“	O
why	O
isn	O
’	O
t	O
everyone	O
a	O
bayesian	O
?	O
”	O
the	O
(	O
frequentist	B
)	O
statistician	O
bradley	O
efron	O
wrote	O
a	O
paper	O
with	O
exactly	O
this	O
title	O
(	O
efron	O
1986	O
)	O
.	O
his	O
short	O
paper	O
is	O
well	O
worth	O
reading	O
for	O
anyone	O
interested	O
in	O
this	O
topic	B
.	O
below	O
we	O
quote	O
his	O
opening	O
section	O
:	O
the	O
title	O
is	O
a	O
reasonable	O
question	O
to	O
ask	O
on	O
at	O
least	O
two	O
counts	O
.	O
first	O
of	O
all	O
,	O
everone	O
used	O
to	O
be	O
a	O
bayesian	O
.	O
laplace	O
wholeheatedly	O
endorsed	O
bayes	O
’	O
s	O
formulation	O
of	O
the	O
inference	B
problem	O
,	O
and	O
most	O
19th-century	O
scientists	O
followed	O
suit	O
.	O
this	O
included	O
gauss	O
,	O
whose	O
statistical	O
work	O
is	O
usually	O
presented	O
in	O
frequentist	B
terms	O
.	O
a	O
second	O
and	O
more	O
important	O
point	O
is	O
the	O
cogency	O
of	O
the	O
bayesian	O
argument	O
.	O
modern	O
statisticians	O
,	O
following	O
the	O
lead	O
of	O
savage	O
and	O
de	O
finetti	O
,	O
have	O
advanced	O
powerful	O
theoret-	O
ical	O
arguments	O
for	O
preferring	O
bayesian	O
inference	B
.	O
a	O
byproduct	O
of	O
this	O
work	O
is	O
a	O
disturbing	O
catalogue	O
of	O
inconsistencies	O
in	O
the	O
frequentist	B
point	O
of	O
view	O
.	O
nevertheless	O
,	O
everyone	O
is	O
not	O
a	O
bayesian	O
.	O
the	O
current	O
era	O
(	O
1986	O
)	O
is	O
the	O
ﬁrst	O
century	O
in	O
which	O
statistics	O
has	O
been	O
widely	O
used	O
for	O
scientiﬁc	O
reporting	O
,	O
and	O
in	O
fact	O
,	O
20th-century	O
statistics	O
is	O
mainly	O
non-bayesian	O
.	O
however	O
,	O
lindley	O
(	O
1975	O
)	O
predicts	O
a	O
change	O
for	O
the	O
21st	O
century	O
.	O
time	O
will	O
tell	O
whether	O
lindley	O
was	O
right	O
...	O
.	O
exercises	O
exercise	O
6.1	O
pessimism	O
of	O
loocv	O
(	O
source	O
:	O
witten05	O
,	O
p152.	O
)	O
.	O
suppose	O
we	O
have	O
a	O
completely	O
random	O
labeled	O
dataset	O
(	O
i.e.	O
,	O
the	O
features	B
x	O
tell	O
us	O
nothing	O
about	O
the	O
class	O
labels	O
y	O
)	O
with	O
n1	O
examples	O
of	O
class	O
1	O
,	O
and	O
n2	O
examples	O
of	O
class	O
2	O
,	O
where	O
n1	O
=	O
n2	O
.	O
what	O
is	O
the	O
best	O
misclassiﬁcation	B
rate	I
any	O
method	O
can	O
achieve	O
?	O
what	O
is	O
the	O
estimated	O
misclassiﬁcation	O
rate	B
of	O
the	O
same	O
method	O
using	O
loocv	O
?	O
exercise	O
6.2	O
james	O
stein	O
estimator	B
for	O
gaussian	O
means	O
consider	O
the	O
2	O
stage	O
model	O
yi|θi	O
∼	O
n	O
(	O
θi	O
,	O
σ2	O
)	O
and	O
θi|μ	O
∼	O
n	O
(	O
m0	O
,	O
τ	O
2	O
we	O
observe	O
the	O
following	O
6	O
data	O
points	O
,	O
i	O
=	O
1	O
:	O
6	O
:	O
0	O
)	O
.	O
suppose	O
σ2	O
=	O
500	O
is	O
known	O
and	O
216	O
chapter	O
6.	O
frequentist	B
statistics	I
1505	O
,	O
1528	O
,	O
1564	O
,	O
1498	O
,	O
1600	O
,	O
1470	O
a.	O
find	O
the	O
ml-ii	O
estimates	O
of	O
m0	O
and	O
τ	O
2	O
0	O
.	O
b.	O
find	O
the	O
posterior	O
estimates	O
e	O
[	O
θi|yi	O
,	O
m0	O
,	O
τ0	O
]	O
and	O
var	O
[	O
θi|yi	O
,	O
m0	O
,	O
τ0	O
]	O
for	O
i	O
=	O
1	O
.	O
(	O
the	O
other	O
terms	O
,	O
c.	O
give	O
a	O
95	O
%	O
credible	B
interval	I
for	O
p	O
(	O
θi|yi	O
,	O
m0	O
,	O
τ0	O
)	O
for	O
i	O
=	O
1.	O
do	O
you	O
trust	O
this	O
interval	O
(	O
assuming	O
the	O
i	O
=	O
2	O
:	O
6	O
,	O
are	O
computed	O
similarly	O
.	O
)	O
gaussian	O
assumption	O
is	O
reasonable	O
)	O
?	O
i.e	O
.	O
is	O
it	O
likely	O
to	O
be	O
too	O
large	O
or	O
too	O
small	O
,	O
or	O
just	O
right	O
?	O
d.	O
what	O
do	O
you	O
expect	O
would	O
happen	O
to	O
your	O
estimates	O
if	O
σ2	O
were	O
much	O
smaller	O
(	O
say	O
σ2	O
=	O
1	O
)	O
?	O
you	O
do	O
not	O
need	O
to	O
compute	O
the	O
numerical	O
answer	O
;	O
just	O
brieﬂy	O
explain	O
what	O
would	O
happen	O
qualitatively	O
,	O
and	O
why	O
.	O
exercise	O
6.3	O
ˆσ2	O
show	O
that	O
ˆσ2	O
m	O
le	O
=	O
1	O
n	O
m	O
le	O
is	O
biased	O
(	O
cid:2	O
)	O
n	O
n=1	O
(	O
xn	O
−	O
ˆμ	O
)	O
2	O
is	O
a	O
biased	O
estimator	B
of	O
σ2	O
,	O
i.e.	O
,	O
show	O
ex1	O
,	O
...	O
,	O
xn∼n	O
(	O
μ	O
,	O
σ	O
)	O
[	O
ˆσ2	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
)	O
(	O
cid:8	O
)	O
=	O
σ2	O
hint	O
:	O
note	O
that	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
are	O
independent	O
,	O
and	O
use	O
the	O
fact	O
that	O
the	O
expectation	O
of	O
a	O
product	O
of	O
independent	O
random	O
variables	O
is	O
the	O
product	O
of	O
the	O
expectations	O
.	O
exercise	O
6.4	O
estimation	O
of	O
σ2	O
when	O
μ	O
is	O
known	O
suppose	O
we	O
sample	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
∼	O
n	O
(	O
μ	O
,	O
σ2	O
)	O
where	O
μ	O
is	O
a	O
known	O
constant	O
.	O
derive	O
an	O
expression	O
for	O
the	O
mle	O
for	O
σ2	O
in	O
this	O
case	O
.	O
is	O
it	O
unbiased	B
?	O
7	O
linear	B
regression	I
7.1	O
introduction	O
linear	B
regression	I
is	O
the	O
“	O
work	O
horse	O
”	O
of	O
statistics	O
and	O
(	O
supervised	O
)	O
machine	B
learning	I
.	O
when	O
augmented	O
with	O
kernels	O
or	O
other	O
forms	O
of	O
basis	B
function	I
expansion	I
,	O
it	O
can	O
model	O
also	O
non-	O
linear	O
relationships	O
.	O
and	O
when	O
the	O
gaussian	O
output	O
is	O
replaced	O
with	O
a	O
bernoulli	O
or	O
multinoulli	B
distribution	I
,	O
it	O
can	O
be	O
used	O
for	O
classiﬁcation	B
,	O
as	O
we	O
will	O
see	O
below	O
.	O
so	O
it	O
pays	O
to	O
study	O
this	O
model	O
in	O
detail	O
.	O
7.2	O
model	O
speciﬁcation	O
as	O
we	O
discussed	O
in	O
section	O
1.4.5	O
,	O
linear	B
regression	I
is	O
a	O
model	O
of	O
the	O
form	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
n	O
(	O
y|wt	O
x	O
,	O
σ2	O
)	O
(	O
7.1	O
)	O
linear	B
regression	I
can	O
be	O
made	O
to	O
model	O
non-linear	O
relationships	O
by	O
replacing	O
x	O
with	O
some	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
n	O
(	O
y|wt	O
φ	O
(	O
x	O
)	O
,	O
σ2	O
)	O
non-linear	O
function	O
of	O
the	O
inputs	O
,	O
φ	O
(	O
x	O
)	O
.	O
that	O
is	O
,	O
we	O
use	O
(	O
7.2	O
)	O
this	O
is	O
known	O
as	O
basis	B
function	I
expansion	I
.	O
(	O
note	O
that	O
the	O
model	O
is	O
still	O
linear	O
in	O
the	O
parameters	O
w	O
,	O
so	O
it	O
is	O
still	O
called	O
linear	B
regression	I
;	O
the	O
importance	O
of	O
this	O
will	O
become	O
clear	O
below	O
.	O
)	O
a	O
simple	O
example	O
are	O
polynomial	O
basis	O
functions	O
,	O
where	O
the	O
model	O
has	O
the	O
form	O
φ	O
(	O
x	O
)	O
=	O
[	O
1	O
,	O
x	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
xd	O
]	O
figure	O
1.18	O
illustrates	O
the	O
effect	O
of	O
changing	O
d	O
:	O
increasingly	O
complex	O
functions	O
.	O
we	O
can	O
also	O
apply	O
linear	B
regression	I
to	O
more	O
than	O
1	O
input	O
.	O
for	O
example	O
,	O
consider	O
modeling	O
temperature	B
as	O
a	O
function	O
of	O
location	O
.	O
figure	O
7.1	O
(	O
a	O
)	O
plots	O
e	O
[	O
y|x	O
]	O
=w	O
0	O
+	O
w1x1	O
+	O
w2x2	O
,	O
and	O
figure	O
7.1	O
(	O
b	O
)	O
plots	O
e	O
[	O
y|x	O
]	O
=	O
w0	O
+	O
w1x1	O
+	O
w2x2	O
+	O
w3x2	O
1	O
+	O
w4x2	O
2	O
.	O
(	O
7.3	O
)	O
increasing	O
the	O
degree	B
d	O
allows	O
us	O
to	O
create	O
7.3	O
maximum	O
likelihood	O
estimation	O
(	O
least	B
squares	I
)	O
a	O
common	O
way	O
to	O
esitmate	O
the	O
parameters	O
of	O
a	O
statistical	O
model	O
is	O
to	O
compute	O
the	O
mle	O
,	O
which	O
is	O
deﬁned	O
as	O
ˆθ	O
(	O
cid:2	O
)	O
arg	O
max	O
log	O
p	O
(	O
d|θ	O
)	O
θ	O
(	O
7.4	O
)	O
218	O
chapter	O
7.	O
linear	B
regression	I
18	O
17.5	O
17	O
16.5	O
16	O
15.5	O
18	O
17.5	O
17	O
16.5	O
16	O
15.5	O
15	O
30	O
25	O
20	O
15	O
10	O
5	O
0	O
0	O
(	O
a	O
)	O
40	O
30	O
20	O
10	O
30	O
20	O
10	O
0	O
0	O
(	O
b	O
)	O
40	O
30	O
20	O
10	O
figure	O
7.1	O
linear	B
regression	I
applied	O
to	O
2d	O
data	O
.	O
vertical	O
axis	O
is	O
temperature	B
,	O
horizontal	O
axes	O
are	O
location	O
within	O
a	O
room	O
.	O
data	O
was	O
collected	O
by	O
some	O
remote	O
sensing	O
motes	B
at	O
intel	O
’	O
s	O
lab	O
in	O
berkeley	O
,	O
ca	O
(	O
data	O
courtesy	O
of	O
romain	O
thibaux	O
)	O
.	O
(	O
b	O
)	O
1	O
+	O
w4x2	O
temperature	B
data	O
is	O
ﬁtted	O
with	O
a	O
quadratic	O
of	O
the	O
form	O
ˆf	O
(	O
x	O
)	O
=w	O
0	O
+	O
w1x1	O
+	O
w2x2	O
+	O
w3x2	O
2.	O
produced	O
by	O
surfacefitdemo	O
.	O
(	O
a	O
)	O
the	O
ﬁtted	O
plane	O
has	O
the	O
form	O
ˆf	O
(	O
x	O
)	O
=w	O
0	O
+	O
w1x1	O
+	O
w2x2	O
.	O
it	O
is	O
common	O
to	O
assume	O
the	O
training	O
examples	O
are	O
independent	B
and	I
identically	I
distributed	I
,	O
commonly	O
abbreviated	O
to	O
iid	B
.	O
this	O
means	O
we	O
can	O
write	O
the	O
log-likelihood	O
as	O
follows	O
:	O
(	O
cid:6	O
)	O
(	O
θ	O
)	O
(	O
cid:2	O
)	O
log	O
p	O
(	O
d|θ	O
)	O
=	O
log	O
p	O
(	O
yi|xi	O
,	O
θ	O
)	O
(	O
7.5	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
nll	O
(	O
θ	O
)	O
(	O
cid:2	O
)	O
−	O
n	O
(	O
cid:2	O
)	O
instead	O
of	O
maximizing	O
the	O
log-likelihood	O
,	O
we	O
can	O
equivalently	O
minimize	O
the	O
negative	O
log	O
likeli-	O
hood	O
or	O
nll	O
:	O
log	O
p	O
(	O
yi|xi	O
,	O
θ	O
)	O
(	O
7.6	O
)	O
i=1	O
the	O
nll	O
formulation	O
is	O
sometimes	O
more	O
convenient	O
,	O
since	O
many	O
optimization	B
software	O
packages	O
are	O
designed	O
to	O
ﬁnd	O
the	O
minima	O
of	O
functions	O
,	O
rather	O
than	O
maxima	O
.	O
now	O
let	O
us	O
apply	O
the	O
method	O
of	O
mle	O
to	O
the	O
linear	B
regression	I
setting	O
.	O
inserting	O
the	O
deﬁnition	O
of	O
the	O
gaussian	O
into	O
the	O
above	O
,	O
we	O
ﬁnd	O
that	O
the	O
log	O
likelihood	O
is	O
given	O
by	O
(	O
cid:18	O
)	O
(	O
cid:8	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:9	O
)	O
1	O
2	O
(	O
cid:8	O
)	O
(	O
cid:6	O
)	O
(	O
θ	O
)	O
=	O
log	O
1	O
2πσ2	O
exp	O
−	O
1	O
2σ2	O
(	O
yi	O
−	O
wt	O
xi	O
)	O
2	O
(	O
cid:9	O
)	O
(	O
cid:19	O
)	O
i=1	O
−1	O
2σ2	O
=	O
rss	O
(	O
w	O
)	O
−	O
n	O
2	O
log	O
(	O
2πσ2	O
)	O
rss	O
stands	O
for	O
residual	B
sum	I
of	I
squares	I
and	O
is	O
deﬁned	O
by	O
n	O
(	O
cid:2	O
)	O
rss	O
(	O
w	O
)	O
(	O
cid:2	O
)	O
(	O
yi	O
−	O
wt	O
xi	O
)	O
2	O
(	O
7.7	O
)	O
(	O
7.8	O
)	O
(	O
7.9	O
)	O
i=1	O
the	O
rss	O
is	O
also	O
called	O
the	O
sum	B
of	I
squared	I
errors	I
,	O
or	O
sse	O
,	O
and	O
sse/n	O
is	O
called	O
the	O
mean	B
squared	I
error	I
or	O
mse	O
.	O
it	O
can	O
also	O
be	O
written	O
as	O
the	O
square	O
of	O
the	O
(	O
cid:6	O
)	O
2	O
norm	O
of	O
the	O
vector	O
of	O
7.3.	O
maximum	O
likelihood	O
estimation	O
(	O
least	B
squares	I
)	O
219	O
sum	B
of	I
squares	I
error	O
contours	O
for	O
linear	B
regression	I
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
prediction	O
truth	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
(	O
a	O
)	O
3	O
2.5	O
2	O
1.5	O
1	O
w	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1	O
2	O
3	O
0	O
1	O
w0	O
(	O
b	O
)	O
figure	O
7.2	O
(	O
a	O
)	O
in	O
linear	O
least	O
squares	O
,	O
we	O
try	O
to	O
minimize	O
the	O
sum	O
of	O
squared	O
distances	O
from	O
each	O
training	O
point	O
(	O
denoted	O
by	O
a	O
red	O
circle	O
)	O
to	O
its	O
approximation	O
(	O
denoted	O
by	O
a	O
blue	O
cross	O
)	O
,	O
that	O
is	O
,	O
we	O
minimize	O
the	O
sum	O
of	O
the	O
lengths	O
of	O
the	O
little	O
vertical	O
blue	O
lines	O
.	O
the	O
red	O
diagonal	B
line	O
represents	O
ˆy	O
(	O
x	O
)	O
=w	O
0	O
+	O
w1x	O
,	O
which	O
is	O
the	O
least	B
squares	I
regression	O
line	O
.	O
note	O
that	O
these	O
residual	B
lines	O
are	O
not	O
perpendicular	O
to	O
the	O
least	B
squares	I
line	O
,	O
in	O
contrast	O
to	O
figure	O
12.5.	O
figure	O
generated	O
by	O
residualsdemo	O
.	O
(	O
b	O
)	O
contours	O
of	O
the	O
rss	O
error	O
surface	O
for	O
the	O
same	O
example	O
.	O
the	O
red	O
cross	O
represents	O
the	O
mle	O
,	O
w	O
=	O
(	O
1.45	O
,	O
0.93	O
)	O
.	O
figure	O
generated	O
by	O
contoursssedemo	O
.	O
residual	B
errors	O
:	O
rss	O
(	O
w	O
)	O
=	O
||||2	O
2	O
=	O
n	O
(	O
cid:2	O
)	O
i=1	O
2	O
i	O
(	O
7.10	O
)	O
where	O
i	O
=	O
(	O
yi	O
−	O
wt	O
xi	O
)	O
.	O
we	O
see	O
that	O
the	O
mle	O
for	O
w	O
is	O
the	O
one	O
that	O
minimizes	O
the	O
rss	O
,	O
so	O
this	O
method	O
is	O
known	O
as	O
least	B
squares	I
.	O
this	O
method	O
is	O
illustrated	O
in	O
figure	O
7.2	O
(	O
a	O
)	O
.	O
the	O
training	O
data	O
(	O
xi	O
,	O
yi	O
)	O
are	O
shown	O
as	O
red	O
circles	O
,	O
the	O
estimated	O
values	O
(	O
xi	O
,	O
ˆyi	O
)	O
are	O
shown	O
as	O
blue	O
crosses	O
,	O
and	O
the	O
residuals	O
i	O
=	O
yi	O
−	O
ˆyi	O
are	O
shown	O
as	O
vertical	O
blue	O
lines	O
.	O
the	O
goal	O
is	O
to	O
ﬁnd	O
the	O
setting	O
of	O
the	O
parameters	O
(	O
the	O
slope	O
w1	O
and	O
intercept	O
w0	O
)	O
such	O
that	O
the	O
resulting	O
red	O
line	O
minimizes	O
the	O
sum	O
of	O
squared	O
residuals	O
(	O
the	O
lengths	O
of	O
the	O
vertical	O
blue	O
lines	O
)	O
.	O
in	O
figure	O
7.2	O
(	O
b	O
)	O
,	O
we	O
plot	O
the	O
nll	O
surface	O
for	O
our	O
linear	B
regression	I
example	O
.	O
we	O
see	O
that	O
it	O
is	O
a	O
quadratic	O
“	O
bowl	O
”	O
with	O
a	O
unique	O
minimum	O
,	O
which	O
we	O
now	O
derive	O
.	O
(	O
importantly	O
,	O
this	O
is	O
true	O
even	O
if	O
we	O
use	O
basis	B
function	I
expansion	I
,	O
such	O
as	O
polynomials	O
,	O
because	O
the	O
nll	O
is	O
still	O
linear	O
in	O
the	O
parameters	O
w	O
,	O
even	O
if	O
it	O
is	O
not	O
linear	O
in	O
the	O
inputs	O
x	O
.	O
)	O
7.3.1	O
derivation	O
of	O
the	O
mle	O
first	O
,	O
we	O
rewrite	O
the	O
objective	O
in	O
a	O
form	O
that	O
is	O
more	O
amenable	O
to	O
differentiation	O
:	O
nll	O
(	O
w	O
)	O
=	O
(	O
y	O
−	O
xw	O
)	O
t	O
(	O
y	O
−	O
xw	O
)	O
=	O
1	O
2	O
1	O
2	O
wt	O
(	O
xt	O
x	O
)	O
w	O
−	O
wt	O
(	O
xt	O
y	O
)	O
(	O
7.11	O
)	O
220	O
where	O
xt	O
x	O
=	O
n	O
(	O
cid:2	O
)	O
i=1	O
xixt	O
i	O
=	O
n	O
(	O
cid:2	O
)	O
⎛	O
⎜⎝	O
x2	O
i,1	O
i=1	O
xi	O
,	O
dxi,1	O
···	O
.	O
.	O
.	O
···	O
⎞	O
⎟⎠	O
xi,1xi	O
,	O
d	O
x2	O
i	O
,	O
d	O
is	O
the	O
sum	B
of	I
squares	I
matrix	O
and	O
n	O
(	O
cid:2	O
)	O
xt	O
y	O
=	O
xiyi	O
.	O
i=1	O
using	O
results	O
from	O
equation	O
4.10	O
,	O
we	O
see	O
that	O
the	O
gradient	O
of	O
this	O
is	O
given	O
by	O
g	O
(	O
w	O
)	O
=	O
[	O
xt	O
xw	O
−	O
xt	O
y	O
]	O
=	O
xi	O
(	O
wt	O
xi	O
−	O
yi	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
equating	O
to	O
zero	O
we	O
get	O
xt	O
xw	O
=	O
xt	O
y	O
chapter	O
7.	O
linear	B
regression	I
(	O
7.12	O
)	O
(	O
7.13	O
)	O
(	O
7.14	O
)	O
(	O
7.15	O
)	O
this	O
is	O
known	O
as	O
the	O
normal	B
equation	I
.	O
the	O
corresponding	O
solution	O
ˆw	O
to	O
this	O
linear	O
system	O
of	O
equations	O
is	O
called	O
the	O
ordinary	B
least	I
squares	I
or	O
ols	O
solution	O
,	O
which	O
is	O
given	O
by	O
ˆwols	O
=	O
(	O
xt	O
x	O
)	O
−1xt	O
y	O
(	O
7.16	O
)	O
7.3.2	O
geometric	O
interpretation	O
this	O
equation	O
has	O
an	O
elegant	O
geometrical	O
intrepretation	O
,	O
as	O
we	O
now	O
explain	O
.	O
we	O
assume	O
n	O
>	O
d	O
,	O
so	O
we	O
have	O
more	O
examples	O
than	O
features	B
.	O
the	O
columns	O
of	O
x	O
deﬁne	O
a	O
linear	O
subspace	O
of	O
dimensionality	O
d	O
which	O
is	O
embedded	O
in	O
n	O
dimensions	O
.	O
let	O
the	O
j	O
’	O
th	O
column	O
be	O
˜xj	O
,	O
which	O
is	O
d	O
,	O
which	O
represents	O
the	O
i	O
’	O
th	O
data	O
a	O
vector	O
in	O
r	O
n	O
.	O
for	O
example	O
,	O
suppose	O
we	O
have	O
n	O
=	O
3	O
examples	O
in	O
d	O
=	O
2	O
case	O
.	O
)	O
similarly	O
,	O
y	O
is	O
a	O
vector	O
in	O
r	O
⎞	O
dimensions	O
:	O
⎠	O
(	O
this	O
should	O
not	O
be	O
confused	O
with	O
xi	O
∈	O
r	O
⎞	O
⎠	O
,	O
y	O
=	O
⎛	O
⎝8.8957	O
x	O
=	O
n	O
.	O
(	O
7.17	O
)	O
⎛	O
⎝1	O
2	O
1	O
−2	O
2	O
1	O
0.6130	O
1.7761	O
these	O
vectors	O
are	O
illustrated	O
in	O
figure	O
7.3.	O
we	O
seek	O
a	O
vector	O
ˆy	O
∈	O
r	O
n	O
that	O
lies	O
in	O
this	O
linear	O
subspace	O
and	O
is	O
as	O
close	O
as	O
possible	O
to	O
y	O
,	O
i.e.	O
,	O
we	O
want	O
to	O
ﬁnd	O
argmin	O
ˆy∈span	O
(	O
{	O
˜x1	O
,	O
...	O
,	O
˜xd	O
}	O
)	O
(	O
cid:10	O
)	O
y	O
−	O
ˆy	O
(	O
cid:10	O
)	O
2.	O
since	O
ˆy	O
∈	O
span	O
(	O
x	O
)	O
,	O
there	O
exists	O
some	O
weight	B
vector	I
w	O
such	O
that	O
ˆy	O
=	O
w1˜x1	O
+	O
···	O
+	O
wd˜xd	O
=	O
xw	O
(	O
7.18	O
)	O
(	O
7.19	O
)	O
7.3.	O
maximum	O
likelihood	O
estimation	O
(	O
least	B
squares	I
)	O
221	O
3	O
x	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
1	O
ˆy	O
x1	O
x2	O
y	O
0.5	O
(	O
0,0,0	O
)	O
−0.5	O
−1	O
0	O
x2	O
0.2	O
0	O
0.4	O
x1	O
1	O
0.8	O
0.6	O
figure	O
7.3	O
graphical	O
interpretation	O
of	O
least	B
squares	I
for	O
n	O
=	O
3	O
examples	O
and	O
d	O
=	O
2	O
features	B
.	O
˜x1	O
and	O
3	O
but	O
does	O
not	O
lie	O
on	O
this	O
˜x2	O
are	O
vectors	O
in	O
r	O
2d	O
plane	O
.	O
the	O
orthogonal	B
projection	I
of	O
y	O
onto	O
this	O
plane	O
is	O
denoted	O
ˆy	O
.	O
the	O
red	O
line	O
from	O
y	O
to	O
ˆy	O
is	O
the	O
residual	B
,	O
whose	O
norm	O
we	O
want	O
to	O
minimize	O
.	O
for	O
visual	O
clarity	O
,	O
all	O
vectors	O
have	O
been	O
converted	O
to	O
unit	O
norm	O
.	O
figure	O
generated	O
by	O
leastsquaresprojection	O
.	O
3	O
;	O
together	O
they	O
deﬁne	O
a	O
2d	O
plane	O
.	O
y	O
is	O
also	O
a	O
vector	O
in	O
r	O
to	O
minimize	O
the	O
norm	O
of	O
the	O
residual	B
,	O
y	O
−	O
ˆy	O
,	O
we	O
want	O
the	O
residual	B
vector	O
to	O
be	O
orthogonal	O
to	O
every	O
column	O
of	O
x	O
,	O
so	O
˜xt	O
j	O
(	O
y	O
−	O
ˆy	O
)	O
=	O
0	O
for	O
j	O
=	O
1	O
:	O
d	O
.	O
hence	O
j	O
(	O
y	O
−	O
ˆy	O
)	O
=	O
0	O
⇒	O
xt	O
(	O
y	O
−	O
xw	O
)	O
=	O
0	O
⇒	O
w	O
=	O
(	O
xt	O
x	O
)	O
˜xt	O
hence	O
our	O
projected	O
value	O
of	O
y	O
is	O
given	O
by	O
ˆy	O
=	O
x	O
ˆw	O
=	O
x	O
(	O
xt	O
x	O
)	O
−1xt	O
y	O
−1xt	O
y	O
(	O
7.20	O
)	O
(	O
7.21	O
)	O
this	O
corresponds	O
to	O
an	O
orthogonal	B
projection	I
of	O
y	O
onto	O
the	O
column	O
space	O
of	O
x.	O
the	O
projection	B
matrix	O
p	O
(	O
cid:2	O
)	O
x	O
(	O
xt	O
x	O
)	O
−1xt	O
is	O
called	O
the	O
hat	B
matrix	I
,	O
since	O
it	O
“	O
puts	O
the	O
hat	O
on	O
y	O
”	O
.	O
7.3.3	O
convexity	O
when	O
discussing	O
least	B
squares	I
,	O
we	O
noted	O
that	O
the	O
nll	O
had	O
a	O
bowl	O
shape	O
with	O
a	O
unique	O
minimum	O
.	O
the	O
technical	O
term	O
for	O
functions	O
like	O
this	O
is	O
convex	B
.	O
convex	B
functions	O
play	O
a	O
very	O
important	O
role	O
in	O
machine	B
learning	I
.	O
let	O
us	O
deﬁne	O
this	O
concept	B
more	O
precisely	O
.	O
we	O
say	O
a	O
set	O
s	O
is	O
convex	B
if	O
for	O
any	O
θ	O
,	O
θ	O
(	O
cid:4	O
)	O
∈	O
s	O
,	O
we	O
have	O
λθ	O
+	O
(	O
1−	O
λ	O
)	O
θ	O
(	O
cid:4	O
)	O
∈	O
s	O
,	O
∀	O
λ	O
∈	O
[	O
0	O
,	O
1	O
]	O
(	O
7.22	O
)	O
222	O
chapter	O
7.	O
linear	B
regression	I
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
7.4	O
(	O
a	O
)	O
illustration	O
of	O
a	O
convex	B
set	O
.	O
(	O
b	O
)	O
illustration	O
of	O
a	O
nonconvex	O
set	O
.	O
1	O
−	O
λ	O
λ	O
x	O
y	O
a	O
b	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
a	O
)	O
illustration	O
of	O
a	O
convex	B
function	O
.	O
we	O
see	O
that	O
the	O
chord	O
joining	O
(	O
x	O
,	O
f	O
(	O
x	O
)	O
)	O
to	O
(	O
y	O
,	O
f	O
(	O
y	O
)	O
)	O
lies	O
figure	O
7.5	O
above	O
the	O
function	O
.	O
(	O
b	O
)	O
a	O
function	O
that	O
is	O
neither	O
convex	B
nor	O
concave	B
.	O
a	O
is	O
a	O
local	O
minimum	O
,	O
b	O
is	O
a	O
global	B
minimum	I
.	O
figure	O
generated	O
by	O
convexfnhand	O
.	O
that	O
is	O
,	O
if	O
we	O
draw	O
a	O
line	O
from	O
θ	O
to	O
θ	O
(	O
cid:4	O
)	O
for	O
an	O
illustration	O
of	O
a	O
convex	B
set	O
,	O
and	O
figure	O
7.4	O
(	O
b	O
)	O
for	O
an	O
illustration	O
of	O
a	O
non-convex	O
set	O
.	O
,	O
all	O
points	O
on	O
the	O
line	O
lie	O
inside	O
the	O
set	O
.	O
see	O
figure	O
7.4	O
(	O
a	O
)	O
a	O
function	O
f	O
(	O
θ	O
)	O
is	O
called	O
convex	B
if	O
its	O
epigraph	B
(	O
the	O
set	O
of	O
points	O
above	O
the	O
function	O
)	O
deﬁnes	O
a	O
convex	B
set	O
.	O
equivalently	O
,	O
a	O
function	O
f	O
(	O
θ	O
)	O
is	O
called	O
convex	B
if	O
it	O
is	O
deﬁned	O
on	O
a	O
convex	B
set	O
and	O
if	O
,	O
for	O
any	O
θ	O
,	O
θ	O
(	O
cid:4	O
)	O
∈	O
s	O
,	O
and	O
for	O
any	O
0	O
≤	O
λ	O
≤	O
1	O
,	O
we	O
have	O
f	O
(	O
λθ	O
+	O
(	O
1	O
−	O
λ	O
)	O
θ	O
(	O
cid:4	O
)	O
)	O
≤	O
λf	O
(	O
θ	O
)	O
+	O
(	O
1	O
−	O
λ	O
)	O
f	O
(	O
θ	O
(	O
cid:4	O
)	O
)	O
(	O
7.23	O
)	O
see	O
figure	O
7.5	O
for	O
a	O
1d	O
example	O
.	O
a	O
function	O
is	O
called	O
strictly	B
convex	I
if	O
the	O
inequality	O
is	O
strict	B
.	O
a	O
function	O
f	O
(	O
θ	O
)	O
is	O
concave	B
if	O
−f	O
(	O
θ	O
)	O
is	O
convex	B
.	O
examples	O
of	O
scalar	O
convex	O
functions	O
include	O
θ2	O
,	O
eθ	O
,	O
and	O
θ	O
log	O
θ	O
(	O
for	O
θ	O
>	O
0	O
)	O
.	O
examples	O
of	O
scalar	O
concave	O
functions	O
include	O
log	O
(	O
θ	O
)	O
and	O
intuitively	O
,	O
a	O
(	O
strictly	O
)	O
convex	B
function	O
has	O
a	O
“	O
bowl	O
shape	O
”	O
,	O
and	O
hence	O
has	O
a	O
unique	O
global	B
minimum	I
θ∗	O
corresponding	O
to	O
the	O
bottom	O
of	O
the	O
bowl	O
.	O
hence	O
its	O
second	O
derivative	O
must	O
be	O
positive	O
everywhere	O
,	O
d	O
dθ	O
f	O
(	O
θ	O
)	O
>	O
0.	O
a	O
twice-continuously	O
differentiable	O
,	O
multivariate	O
function	O
f	O
is	O
convex	B
iff	O
its	O
hessian	O
is	O
positive	B
deﬁnite	I
for	O
all	O
θ.1	O
in	O
the	O
machine	B
learning	I
context	O
,	O
the	O
function	O
f	O
often	O
corresponds	O
to	O
the	O
nll	O
.	O
√	O
θ	O
.	O
1.	O
recall	B
that	O
the	O
hessian	O
is	O
the	O
matrix	O
of	O
second	O
partial	O
derivatives	O
,	O
deﬁned	O
by	O
hjk	O
=	O
matrix	O
h	O
is	O
positive	B
deﬁnite	I
iff	O
vt	O
hv	O
>	O
0	O
for	O
any	O
non-zero	O
vector	O
v.	O
∂f	O
2	O
(	O
θ	O
)	O
∂θj	O
∂θk	O
.	O
also	O
,	O
recall	B
that	O
a	O
223	O
l2	O
l1	O
huber	O
7.4.	O
robust	B
linear	O
regression	B
*	O
linear	O
data	O
with	O
noise	O
and	O
outliers	B
least	O
squares	O
laplace	O
5	O
4.5	O
4	O
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
−6	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
−0.5	O
−3	O
−2	O
−1	O
(	O
a	O
)	O
1	O
2	O
3	O
0	O
(	O
b	O
)	O
figure	O
7.6	O
(	O
a	O
)	O
illustration	O
of	O
robust	B
linear	O
regression	B
.	O
figure	O
generated	O
by	O
linregrobustdemocombined	O
.	O
(	O
b	O
)	O
illustration	O
of	O
(	O
cid:7	O
)	O
2	O
,	O
(	O
cid:7	O
)	O
1	O
,	O
and	O
huber	O
loss	B
functions	O
.	O
figure	O
generated	O
by	O
huberlossdemo	O
.	O
models	O
where	O
the	O
nll	O
is	O
convex	B
are	O
desirable	O
,	O
since	O
this	O
means	O
we	O
can	O
always	O
ﬁnd	O
the	O
globally	O
optimal	O
mle	O
.	O
we	O
will	O
see	O
many	O
examples	O
of	O
this	O
later	O
in	O
the	O
book	O
.	O
however	O
,	O
many	O
models	O
of	O
interest	O
will	O
not	O
have	O
concave	B
likelihoods	O
.	O
in	O
such	O
cases	O
,	O
we	O
will	O
discuss	O
ways	O
to	O
derive	O
locally	O
optimal	O
parameter	B
estimates	O
.	O
7.4	O
robust	B
linear	O
regression	B
*	O
it	O
is	O
very	O
common	O
to	O
model	O
the	O
noise	O
in	O
regression	B
models	O
using	O
a	O
gaussian	O
distribution	O
with	O
zero	O
mean	O
and	O
constant	O
variance	O
,	O
i	O
∼	O
n	O
(	O
0	O
,	O
σ2	O
)	O
,	O
where	O
i	O
=	O
yi	O
−	O
wt	O
xi	O
.	O
in	O
this	O
case	O
,	O
maximizing	O
likelihood	B
is	O
equivalent	O
to	O
minimizing	O
the	O
sum	O
of	O
squared	O
residuals	O
,	O
as	O
we	O
have	O
seen	O
.	O
however	O
,	O
if	O
we	O
have	O
outliers	B
in	O
our	O
data	O
,	O
this	O
can	O
result	O
in	O
a	O
poor	O
ﬁt	O
,	O
as	O
illustrated	O
in	O
figure	O
7.6	O
(	O
a	O
)	O
.	O
(	O
the	O
outliers	B
are	O
the	O
points	O
on	O
the	O
bottom	O
of	O
the	O
ﬁgure	O
.	O
)	O
this	O
is	O
because	O
squared	B
error	I
penalizes	O
deviations	O
quadratically	O
,	O
so	O
points	O
far	O
from	O
the	O
line	O
have	O
more	O
affect	O
on	O
the	O
ﬁt	O
than	O
points	O
near	O
to	O
the	O
line	O
.	O
one	O
way	O
to	O
achieve	O
robustness	B
to	O
outliers	B
is	O
to	O
replace	O
the	O
gaussian	O
distribution	O
for	O
the	O
response	B
variable	I
with	O
a	O
distribution	O
that	O
has	O
heavy	B
tails	I
.	O
such	O
a	O
distribution	O
will	O
assign	O
higher	O
likelihood	B
to	O
outliers	B
,	O
without	O
having	O
to	O
perturb	O
the	O
straight	O
line	O
to	O
“	O
explain	O
”	O
them	O
.	O
one	O
possibility	O
is	O
to	O
use	O
the	O
laplace	O
distribution	O
,	O
introduced	O
in	O
section	O
2.4.3.	O
if	O
we	O
use	O
this	O
as	O
our	O
observation	B
model	I
for	O
regression	B
,	O
we	O
get	O
the	O
following	O
likelihood	B
:	O
p	O
(	O
y|x	O
,	O
w	O
,	O
b	O
)	O
=	O
lap	O
(	O
y|wt	O
x	O
,	O
b	O
)	O
∝	O
exp	O
(	O
−	O
1	O
b	O
(	O
7.24	O
)	O
the	O
robustness	B
arises	O
from	O
the	O
use	O
of	O
|y	O
−	O
wt	O
x|	O
instead	O
of	O
(	O
y	O
−	O
wt	O
x	O
)	O
2.	O
for	O
simplicity	O
,	O
we	O
will	O
assume	O
b	O
is	O
ﬁxed	O
.	O
let	O
ri	O
(	O
cid:2	O
)	O
yi	O
−	O
wt	O
xi	O
be	O
the	O
i	O
’	O
th	O
residual	B
.	O
the	O
nll	O
has	O
the	O
form	O
|y	O
−	O
wt	O
x|	O
)	O
(	O
cid:6	O
)	O
(	O
w	O
)	O
=	O
|ri	O
(	O
w	O
)	O
|	O
(	O
7.25	O
)	O
(	O
cid:2	O
)	O
i	O
224	O
chapter	O
7.	O
linear	B
regression	I
likelihood	O
gaussian	O
gaussian	O
gaussian	O
laplace	O
student	O
name	O
prior	O
uniform	O
least	O
squares	O
gaussian	O
laplace	O
uniform	O
robust	O
regression	B
uniform	O
robust	B
regression	O
ridge	O
lasso	O
section	O
7.3	O
7.5	O
13.3	O
7.4	O
exercise	O
11.12	O
table	O
7.1	O
summary	O
of	O
various	O
likelihoods	O
and	O
priors	O
used	O
for	O
linear	B
regression	I
.	O
the	O
likelihood	B
refers	O
to	O
the	O
distributional	O
form	O
of	O
p	O
(	O
y|x	O
,	O
w	O
,	O
σ2	O
)	O
,	O
and	O
the	O
prior	O
refers	O
to	O
the	O
distributional	O
form	O
of	O
p	O
(	O
w	O
)	O
.	O
map	O
estimation	O
with	O
a	O
uniform	B
distribution	I
corresponds	O
to	O
mle	O
.	O
unfortunately	O
,	O
this	O
is	O
a	O
non-linear	O
objective	O
function	O
,	O
which	O
is	O
hard	O
to	O
optimize	O
.	O
fortunately	O
,	O
we	O
can	O
convert	O
the	O
nll	O
to	O
a	O
linear	O
objective	O
,	O
subject	O
to	O
linear	O
constraints	O
,	O
using	O
the	O
following	O
split	B
variable	I
trick	O
.	O
first	O
we	O
deﬁne	O
and	O
then	O
we	O
impose	O
the	O
linear	O
inequality	O
constraints	O
that	O
r+	O
constrained	O
objective	O
becomes	O
i	O
−	O
r−	O
(	O
r+	O
i	O
)	O
s.t	O
.	O
i	O
≥	O
0	O
,	O
r−	O
r+	O
i	O
≥	O
0	O
,	O
wt	O
xi	O
+	O
r+	O
i	O
+	O
r−	O
i	O
=	O
yi	O
(	O
7.27	O
)	O
i	O
≥	O
0	O
and	O
r−	O
(	O
7.26	O
)	O
i	O
≥	O
0.	O
now	O
the	O
ri	O
(	O
cid:2	O
)	O
r+	O
i	O
−	O
r−	O
i	O
(	O
cid:2	O
)	O
min	O
w	O
,	O
r+	O
,	O
r−	O
i	O
this	O
is	O
an	O
example	O
of	O
a	O
linear	B
program	I
with	O
d	O
+	O
2n	O
unknowns	O
and	O
3n	O
constraints	O
.	O
since	O
this	O
is	O
a	O
convex	B
optimization	O
problem	O
,	O
it	O
has	O
a	O
unique	O
solution	O
.	O
to	O
solve	O
an	O
lp	O
,	O
we	O
must	O
ﬁrst	O
write	O
it	O
in	O
standard	O
form	O
,	O
which	O
as	O
follows	O
:	O
θ	O
min	O
f	O
t	O
θ	O
s.t	O
.	O
aθ	O
≤	O
b	O
,	O
aeqθ	O
=	O
beq	O
,	O
l	O
≤	O
θ	O
≤	O
u	O
(	O
7.28	O
)	O
)	O
,	O
f	O
=	O
[	O
0	O
,	O
1	O
,	O
1	O
]	O
,	O
a	O
=	O
[	O
]	O
,	O
b	O
=	O
[	O
]	O
,	O
aeq	O
=	O
[	O
x	O
,	O
i	O
,	O
−i	O
]	O
,	O
in	O
our	O
current	O
example	O
,	O
θ	O
=	O
(	O
w	O
,	O
r+	O
,	O
r−	O
beq	O
=	O
y	O
,	O
l	O
=	O
[	O
−∞1	O
,	O
0	O
,	O
0	O
]	O
,	O
u	O
=	O
[	O
]	O
.	O
this	O
can	O
be	O
solved	O
by	O
any	O
lp	O
solver	O
(	O
see	O
e.g.	O
,	O
(	O
boyd	O
and	O
vandenberghe	O
2004	O
)	O
)	O
.	O
see	O
figure	O
7.6	O
(	O
a	O
)	O
for	O
an	O
example	O
of	O
the	O
method	O
in	O
action	B
.	O
an	O
alternative	O
to	O
using	O
nll	O
under	O
a	O
laplace	O
likelihood	B
is	O
to	O
minimize	O
the	O
huber	O
loss	B
function	I
(	O
huber	O
1964	O
)	O
,	O
deﬁned	O
as	O
follows	O
:	O
(	O
cid:26	O
)	O
if	O
|r|	O
≤	O
δ	O
δ|r|	O
−δ	O
2/2	O
if	O
|r|	O
>	O
δ	O
r2/2	O
lh	O
(	O
r	O
,	O
δ	O
)	O
=	O
(	O
7.29	O
)	O
this	O
is	O
equivalent	O
to	O
(	O
cid:6	O
)	O
2	O
for	O
errors	O
that	O
are	O
smaller	O
than	O
δ	O
,	O
and	O
is	O
equivalent	O
to	O
(	O
cid:6	O
)	O
1	O
for	O
larger	O
errors	O
.	O
see	O
figure	O
7.6	O
(	O
b	O
)	O
.	O
the	O
advantage	O
of	O
this	O
loss	B
function	I
is	O
that	O
it	O
is	O
everywhere	O
differentiable	O
,	O
dr|r|	O
=	O
sign	O
(	O
r	O
)	O
if	O
r	O
(	O
cid:4	O
)	O
=	O
0.	O
we	O
can	O
also	O
check	O
that	O
the	O
function	O
is	O
c1	O
using	O
the	O
fact	O
that	O
d	O
continuous	O
,	O
since	O
the	O
gradients	O
of	O
the	O
two	O
parts	O
of	O
the	O
function	O
match	O
at	O
r	O
=	O
±δ	O
,	O
namely	O
dr	O
lh	O
(	O
r	O
,	O
δ	O
)	O
|r=δ	O
=	O
δ.	O
consequently	O
optimizing	O
the	O
huber	O
loss	B
is	O
much	O
faster	O
than	O
using	O
the	O
laplace	O
likelihood	B
,	O
since	O
we	O
can	O
use	O
standard	O
smooth	O
optimization	B
methods	O
(	O
such	O
as	O
quasi-	O
newton	O
)	O
instead	O
of	O
linear	O
programming	O
.	O
d	O
figure	O
7.6	O
(	O
a	O
)	O
gives	O
an	O
illustration	O
of	O
the	O
huber	O
loss	B
function	I
.	O
the	O
results	O
are	O
qualitatively	O
(	O
in	O
fact	O
,	O
it	O
turns	O
out	O
that	O
the	O
huber	O
method	O
also	O
has	O
a	O
similiar	O
to	O
the	O
probabilistic	O
methods	O
.	O
probabilistic	O
interpretation	O
,	O
although	O
it	O
is	O
rather	O
unnatural	O
(	O
pontil	O
et	O
al	O
.	O
1998	O
)	O
.	O
)	O
7.5.	O
ridge	B
regression	I
225	O
ln	O
lambda	O
−20.135	O
ln	O
lambda	O
−8.571	O
20	O
15	O
10	O
5	O
0	O
−5	O
−10	O
0	O
5	O
15	O
20	O
10	O
(	O
a	O
)	O
20	O
15	O
10	O
5	O
0	O
−5	O
−10	O
−15	O
0	O
5	O
10	O
(	O
b	O
)	O
15	O
20	O
figure	O
7.7	O
degree	B
14	O
polynomial	O
ﬁt	O
to	O
n	O
=	O
21	O
data	O
points	O
with	O
increasing	O
amounts	O
of	O
(	O
cid:7	O
)	O
2	O
regularization	B
.	O
data	O
was	O
generated	O
from	O
noise	O
with	O
variance	B
σ2	O
=	O
4.	O
the	O
error	O
bars	O
,	O
representing	O
the	O
noise	O
variance	O
σ2	O
,	O
get	O
wider	O
as	O
the	O
ﬁt	O
gets	O
smoother	O
,	O
since	O
we	O
are	O
ascribing	O
more	O
of	O
the	O
data	O
variation	O
to	O
the	O
noise	O
.	O
figure	O
generated	O
by	O
linregpolyvsregdemo	O
.	O
7.5	O
ridge	B
regression	I
one	O
problem	O
with	O
ml	O
estimation	O
is	O
that	O
it	O
can	O
result	O
in	O
overﬁtting	B
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
a	O
way	O
to	O
ameliorate	O
this	O
problem	O
by	O
using	O
map	O
estimation	O
with	O
a	O
gaussian	O
prior	O
.	O
for	O
simplicity	O
,	O
we	O
assume	O
a	O
gaussian	O
likelihood	B
,	O
rather	O
than	O
a	O
robust	B
likelihood	O
.	O
7.5.1	O
basic	O
idea	O
the	O
reason	O
that	O
the	O
mle	O
can	O
overﬁt	B
is	O
that	O
it	O
is	O
picking	O
the	O
parameter	B
values	O
that	O
are	O
the	O
best	O
for	O
modeling	O
the	O
training	O
data	O
;	O
but	O
if	O
the	O
data	O
is	O
noisy	O
,	O
such	O
parameters	O
often	O
result	O
in	O
complex	O
functions	O
.	O
as	O
a	O
simple	O
example	O
,	O
suppose	O
we	O
ﬁt	O
a	O
degree	B
14	O
polynomial	O
to	O
n	O
=	O
21	O
data	O
points	O
using	O
least	B
squares	I
.	O
the	O
resulting	O
curve	O
is	O
very	O
“	O
wiggly	O
”	O
,	O
as	O
shown	O
in	O
figure	O
7.7	O
(	O
a	O
)	O
.	O
the	O
corresponding	O
least	B
squares	I
coefficients	O
(	O
excluding	O
w0	O
)	O
are	O
as	O
follows	O
:	O
6.560	O
,	O
-36.934	O
,	O
-109.255	O
,	O
543.452	O
,	O
1022.561	O
,	O
-3046.224	O
,	O
-3768.013	O
,	O
8524.540	O
,	O
6607.897	O
,	O
-12640.058	O
,	O
-5530.188	O
,	O
9479.730	O
,	O
1774.639	O
,	O
-2821.526	O
we	O
see	O
that	O
there	O
are	O
many	O
large	O
positive	O
and	O
negative	O
numbers	O
.	O
these	O
balance	O
out	O
exactly	O
to	O
make	O
the	O
curve	O
“	O
wiggle	O
”	O
in	O
just	O
the	O
right	O
way	O
so	O
that	O
it	O
almost	O
perfectly	O
interpolates	O
the	O
data	O
.	O
but	O
this	O
situation	O
is	O
unstable	B
:	O
if	O
we	O
changed	O
the	O
data	O
a	O
little	O
,	O
the	O
coefficients	O
would	O
change	O
a	O
lot	O
.	O
we	O
can	O
encourage	O
the	O
parameters	O
to	O
be	O
small	O
,	O
thus	O
resulting	O
in	O
a	O
smoother	O
curve	O
,	O
by	O
using	O
a	O
(	O
7.30	O
)	O
zero-mean	O
gaussian	O
prior	O
:	O
n	O
(	O
wj|0	O
,	O
τ	O
2	O
)	O
p	O
(	O
w	O
)	O
=	O
(	O
cid:27	O
)	O
j	O
n	O
(	O
cid:2	O
)	O
argmax	O
w	O
i=1	O
where	O
1/τ	O
2	O
controls	O
the	O
strength	O
of	O
the	O
prior	O
.	O
the	O
corresponding	O
map	O
estimation	O
problem	O
becomes	O
log	O
n	O
(	O
yi|w0	O
+	O
wt	O
xi	O
,	O
σ2	O
)	O
+	O
log	O
n	O
(	O
wj|0	O
,	O
τ	O
2	O
)	O
(	O
7.31	O
)	O
d	O
(	O
cid:2	O
)	O
j=1	O
chapter	O
7.	O
linear	B
regression	I
negative	O
log	O
marg	O
.	O
likelihood	B
cv	O
estimate	O
of	O
mse	O
226	O
25	O
20	O
15	O
10	O
5	O
mean	B
squared	I
error	I
train	O
mse	O
test	O
mse	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0	O
−25	O
−20	O
−15	O
−10	O
log	O
lambda	O
(	O
a	O
)	O
−5	O
0	O
5	O
0.1	O
−20	O
−15	O
−10	O
−5	O
0	O
5	O
log	O
lambda	O
(	O
b	O
)	O
figure	O
7.8	O
(	O
a	O
)	O
training	O
error	O
(	O
dotted	O
blue	O
)	O
and	O
test	O
error	O
(	O
solid	O
red	O
)	O
for	O
a	O
degree	B
14	O
polynomial	O
ﬁt	O
by	O
ridge	B
regression	I
,	O
plotted	O
vs	O
log	O
(	O
λ	O
)	O
.	O
data	O
was	O
generated	O
from	O
noise	O
with	O
variance	B
σ2	O
=	O
4	O
(	O
training	B
set	I
has	O
size	O
n	O
=	O
21	O
)	O
.	O
note	O
:	O
models	O
are	O
ordered	O
from	O
complex	O
(	O
small	O
regularizer	O
)	O
on	O
the	O
left	O
to	O
simple	O
(	O
large	O
regularizer	O
)	O
on	O
the	O
right	O
.	O
the	O
stars	O
correspond	O
to	O
the	O
values	O
used	O
to	O
plot	O
the	O
functions	O
in	O
figure	O
7.7	O
.	O
(	O
b	O
)	O
estimate	O
of	O
performance	O
using	O
training	B
set	I
.	O
dotted	O
blue	O
:	O
5-fold	O
cross-validation	O
estimate	O
of	O
future	O
mse	O
.	O
solid	O
black	O
:	O
negative	O
log	O
marginal	O
likelihood	B
,	O
−	O
log	O
p	O
(	O
d|λ	O
)	O
.	O
both	O
curves	O
have	O
been	O
vertically	O
rescaled	O
to	O
[	O
0,1	O
]	O
to	O
make	O
them	O
comparable	O
.	O
figure	O
generated	O
by	O
linregpolyvsregdemo	O
.	O
it	O
is	O
a	O
simple	O
exercise	O
to	O
show	O
that	O
this	O
is	O
equivalent	O
to	O
minimizing	O
the	O
following	O
:	O
j	O
(	O
w	O
)	O
=	O
1	O
n	O
(	O
yi	O
−	O
(	O
w0	O
+	O
wt	O
xi	O
)	O
)	O
2	O
+	O
λ||w||2	O
2	O
(	O
7.32	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
cid:10	O
)	O
where	O
λ	O
(	O
cid:2	O
)	O
σ2/τ	O
2	O
and	O
||w||2	O
j	O
=	O
wt	O
w	O
is	O
the	O
squared	O
two-norm	O
.	O
here	O
the	O
ﬁrst	O
term	O
is	O
the	O
mse/	O
nll	O
as	O
usual	O
,	O
and	O
the	O
second	O
term	O
,	O
λ	O
≥	O
0	O
,	O
is	O
a	O
complexity	O
penalty	O
.	O
the	O
corresponding	O
solution	O
is	O
given	O
by	O
j	O
w2	O
2	O
=	O
ˆwridge	O
=	O
(	O
λid	O
+	O
xt	O
x	O
)	O
−1xt	O
y	O
(	O
7.33	O
)	O
this	O
technique	O
is	O
known	O
as	O
ridge	B
regression	I
,	O
or	O
penalized	B
least	I
squares	I
.	O
in	O
general	O
,	O
adding	O
a	O
gaussian	O
prior	O
to	O
the	O
parameters	O
of	O
a	O
model	O
to	O
encourage	O
them	O
to	O
be	O
small	O
is	O
called	O
(	O
cid:6	O
)	O
2	O
regularization	B
or	O
weight	B
decay	I
.	O
note	O
that	O
the	O
offset	O
term	O
w0	O
is	O
not	O
regularized	O
,	O
since	O
this	O
just	O
affects	O
the	O
height	O
of	O
the	O
function	O
,	O
not	O
its	O
complexity	O
.	O
by	O
penalizing	O
the	O
sum	O
of	O
the	O
magnitudes	O
of	O
the	O
weights	O
,	O
we	O
ensure	O
the	O
function	O
is	O
simple	O
(	O
since	O
w	O
=	O
0	O
corresponds	O
to	O
a	O
straight	O
line	O
,	O
which	O
is	O
the	O
simplest	O
possible	O
function	O
,	O
corresponding	O
to	O
a	O
constant	O
.	O
)	O
we	O
illustrate	O
this	O
idea	O
in	O
figure	O
7.7	O
,	O
where	O
we	O
see	O
that	O
increasing	O
λ	O
results	O
in	O
smoother	O
−3	O
,	O
we	O
functions	O
.	O
the	O
resulting	O
coefficients	O
also	O
become	O
smaller	O
.	O
for	O
example	O
,	O
using	O
λ	O
=	O
10	O
have	O
7.5.	O
ridge	B
regression	I
227	O
2.128	O
,	O
0.807	O
,	O
16.457	O
,	O
3.704	O
,	O
-24.948	O
,	O
-10.472	O
,	O
-2.625	O
,	O
4.360	O
,	O
13.711	O
,	O
10.063	O
,	O
8.716	O
,	O
3.966	O
,	O
-9.349	O
,	O
-9.232	O
in	O
figure	O
7.8	O
(	O
a	O
)	O
,	O
we	O
plot	O
the	O
mse	O
on	O
the	O
training	O
and	O
test	O
sets	O
vs	O
log	O
(	O
λ	O
)	O
.	O
we	O
see	O
that	O
,	O
as	O
we	O
increase	O
λ	O
(	O
so	O
the	O
model	O
becomes	O
more	O
constrained	O
)	O
,	O
the	O
error	O
on	O
the	O
training	B
set	I
increases	O
.	O
for	O
the	O
test	O
set	O
,	O
we	O
see	O
the	O
characteristic	O
u-shaped	O
curve	O
,	O
where	O
the	O
model	O
overﬁts	O
and	O
then	O
underﬁts	B
.	O
in	O
section	O
1.4.8	O
,	O
we	O
will	O
discuss	O
a	O
more	O
probabilistic	O
approach	O
.	O
it	O
is	O
common	O
to	O
use	O
cross	B
validation	I
to	O
pick	O
λ	O
,	O
as	O
shown	O
in	O
figure	O
7.8	O
(	O
b	O
)	O
.	O
we	O
will	O
consider	O
a	O
variety	O
of	O
different	O
priors	O
in	O
this	O
book	O
.	O
each	O
of	O
these	O
corresponds	O
to	O
a	O
different	O
form	O
of	O
regularization	B
.	O
this	O
technique	O
is	O
very	O
widely	O
used	O
to	O
prevent	O
overﬁtting	B
.	O
7.5.2	O
numerically	O
stable	B
computation	O
*	O
interestingly	O
,	O
ridge	B
regression	I
,	O
which	O
works	O
better	O
statistically	O
,	O
is	O
also	O
easier	O
to	O
ﬁt	O
numerically	O
,	O
since	O
(	O
λid	O
+	O
xt	O
x	O
)	O
is	O
much	O
better	O
conditioned	O
(	O
and	O
hence	O
more	O
likely	O
to	O
be	O
invertible	O
)	O
than	O
xt	O
x	O
,	O
at	O
least	O
for	O
suitable	O
largy	O
λ.	O
nevertheless	O
,	O
inverting	O
matrices	O
is	O
still	O
best	O
avoided	O
,	O
for	O
reasons	O
of	O
numerical	O
stability	O
.	O
(	O
indeed	O
,	O
if	O
you	O
write	O
w=inv	O
(	O
x	O
’	O
*	O
x	O
)	O
*x	O
’	O
*y	O
in	O
matlab	O
,	O
it	O
will	O
give	O
you	O
a	O
warning	O
.	O
)	O
we	O
now	O
describe	O
a	O
useful	O
trick	O
for	O
ﬁtting	O
ridge	B
regression	I
models	O
(	O
and	O
hence	O
by	O
extension	B
,	O
computing	O
vanilla	O
ols	O
estimates	O
)	O
that	O
is	O
more	O
numerically	O
robust	B
.	O
we	O
assume	O
the	O
prior	O
has	O
the	O
form	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
0	O
,	O
λ−1	O
)	O
,	O
where	O
λ	O
is	O
the	O
precision	B
matrix	I
.	O
in	O
the	O
case	O
of	O
ridge	B
regression	I
,	O
λ	O
=	O
(	O
1/τ	O
2	O
)	O
i.	O
to	O
avoid	O
penalizing	O
the	O
w0	O
term	O
,	O
we	O
should	O
center	O
the	O
data	O
ﬁrst	O
,	O
as	O
explained	O
in	O
exercise	O
7.5.	O
first	O
let	O
us	O
augment	O
the	O
original	O
data	O
with	O
some	O
“	O
virtual	O
data	O
”	O
coming	O
from	O
the	O
prior	O
:	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
x/σ√	O
λ	O
√	O
,	O
√	O
˜x	O
=	O
˜y	O
=	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
y/σ	O
0d×1	O
(	O
7.34	O
)	O
(	O
7.35	O
)	O
(	O
7.36	O
)	O
(	O
7.37	O
)	O
(	O
7.38	O
)	O
(	O
7.39	O
)	O
(	O
7.40	O
)	O
where	O
λ	O
=	O
where	O
the	O
extra	O
rows	O
represent	O
pseudo-data	O
from	O
the	O
prior	O
.	O
λ	O
λ	O
t	O
is	O
a	O
cholesky	O
decomposition	O
of	O
λ.	O
we	O
see	O
that	O
˜x	O
is	O
(	O
n	O
+	O
d	O
)	O
×	O
d	O
,	O
we	O
now	O
show	O
that	O
the	O
nll	O
on	O
this	O
expanded	O
data	O
is	O
equivalent	O
to	O
penalized	O
nll	O
on	O
the	O
original	O
data	O
:	O
(	O
cid:9	O
)	O
f	O
(	O
w	O
)	O
=	O
(	O
˜y	O
−	O
˜xw	O
)	O
t	O
(	O
˜y	O
−	O
˜xw	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
x/σ√	O
(	O
cid:9	O
)	O
t	O
(	O
cid:8	O
)	O
λ	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
=	O
0	O
y/σ	O
−	O
σ	O
(	O
y	O
−	O
xw	O
)	O
−√	O
λw	O
1	O
w	O
(	O
cid:9	O
)	O
t	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
1	O
σ	O
(	O
y	O
−	O
xw	O
)	O
−√	O
λw	O
√	O
=	O
=	O
=	O
1	O
σ2	O
(	O
y	O
−	O
xw	O
)	O
t	O
(	O
y	O
−	O
xw	O
)	O
+	O
(	O
λw	O
)	O
t	O
(	O
σ2	O
(	O
y	O
−	O
xw	O
)	O
t	O
(	O
y	O
−	O
xw	O
)	O
+w	O
t	O
λw	O
1	O
√	O
λw	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
−	O
x/σ√	O
λ	O
(	O
cid:9	O
)	O
(	O
cid:9	O
)	O
w	O
y/σ	O
0	O
(	O
cid:9	O
)	O
hence	O
the	O
map	O
estimate	O
is	O
given	O
by	O
ˆwridge	O
=	O
(	O
˜xt	O
˜x	O
)	O
−1	O
˜xt	O
˜y	O
as	O
we	O
claimed	O
.	O
228	O
now	O
let	O
˜x	O
=	O
qr	O
chapter	O
7.	O
linear	B
regression	I
(	O
7.41	O
)	O
be	O
the	O
qr	O
decomposition	O
of	O
x	O
,	O
where	O
q	O
is	O
orthonormal	O
(	O
meaning	O
qt	O
q	O
=	O
qqt	O
=	O
i	O
)	O
,	O
and	O
r	O
is	O
upper	O
triangular	O
.	O
then	O
(	O
˜xt	O
˜x	O
)	O
−1	O
=	O
(	O
rt	O
qt	O
qr	O
)	O
−1	O
=	O
(	O
rt	O
r	O
)	O
−1	O
=	O
r−1r−t	O
hence	O
ˆwridge	O
=	O
r−1r−t	O
rt	O
qt	O
˜y	O
=	O
r−1q˜y	O
(	O
7.42	O
)	O
(	O
7.43	O
)	O
note	O
that	O
r	O
is	O
easy	O
to	O
invert	O
since	O
it	O
is	O
upper	O
triangular	O
.	O
this	O
gives	O
us	O
a	O
way	O
to	O
compute	O
the	O
ridge	O
estimate	O
while	O
avoiding	O
having	O
to	O
invert	O
(	O
λ	O
+	O
xt	O
x	O
)	O
.	O
we	O
can	O
use	O
this	O
technique	O
to	O
ﬁnd	O
the	O
mle	O
,	O
by	O
simply	O
computing	O
the	O
qr	O
decomposition	O
of	O
the	O
unaugmented	O
matrix	O
x	O
,	O
and	O
using	O
the	O
original	O
y.	O
this	O
is	O
the	O
method	O
of	O
choice	O
for	O
solving	O
least	B
squares	I
problems	O
.	O
(	O
in	O
fact	O
,	O
it	O
is	O
so	O
sommon	O
that	O
it	O
can	O
be	O
implemented	O
in	O
one	O
line	O
of	O
matlab	O
,	O
using	O
the	O
backslash	B
operator	I
:	O
w=x\y	O
.	O
)	O
note	O
that	O
computing	O
the	O
qr	O
decomposition	O
of	O
an	O
n	O
×	O
d	O
matrix	O
takes	O
o	O
(	O
n	O
d2	O
)	O
time	O
,	O
and	O
is	O
numerically	O
very	O
stable	B
.	O
if	O
d	O
(	O
cid:15	O
)	O
n	O
,	O
we	O
should	O
ﬁrst	O
perform	O
an	O
svd	O
decomposition	O
.	O
in	O
particular	O
,	O
let	O
x	O
=	O
usvt	O
be	O
the	O
svd	O
of	O
x	O
,	O
where	O
vt	O
v	O
=	O
in	O
,	O
uut	O
=	O
ut	O
u	O
=	O
in	O
,	O
and	O
s	O
is	O
a	O
diagonal	B
n	O
×	O
n	O
matrix	O
.	O
now	O
let	O
z	O
=	O
ud	O
be	O
an	O
n	O
×	O
n	O
matrix	O
.	O
then	O
we	O
can	O
rewrite	O
the	O
ridge	O
estimate	O
thus	O
:	O
ˆwridge	O
=	O
v	O
(	O
zt	O
z	O
+	O
λin	O
)	O
−1zt	O
y	O
(	O
7.44	O
)	O
in	O
other	O
words	O
,	O
we	O
can	O
replace	O
the	O
d-dimensional	O
vectors	O
xi	O
with	O
the	O
n	O
-dimensional	O
vectors	O
zi	O
and	O
perform	O
our	O
penalized	O
ﬁt	O
as	O
before	O
.	O
we	O
then	O
transform	O
the	O
n	O
-dimensional	O
solution	O
to	O
the	O
d-dimensional	O
solution	O
by	O
multiplying	O
by	O
v.	O
geometrically	O
,	O
we	O
are	O
rotating	O
to	O
a	O
new	O
coordinate	O
system	O
in	O
which	O
all	O
but	O
the	O
ﬁrst	O
n	O
coordinates	O
are	O
zero	O
.	O
this	O
does	O
not	O
affect	O
the	O
solution	O
since	O
the	O
spherical	B
gaussian	O
prior	O
is	O
rotationally	O
invariant	B
.	O
the	O
overall	O
time	O
is	O
now	O
o	O
(	O
dn	O
2	O
)	O
operations	O
.	O
7.5.3	O
connection	O
with	O
pca	O
*	O
in	O
this	O
section	O
,	O
we	O
discuss	O
an	O
interesting	O
connection	O
between	O
ridge	B
regression	I
and	O
pca	O
(	O
sec-	O
tion	O
12.2	O
)	O
,	O
which	O
gives	O
further	O
insight	O
into	O
why	O
ridge	B
regression	I
works	O
well	O
.	O
our	O
discussion	O
is	O
based	O
on	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p66	O
)	O
.	O
let	O
x	O
=	O
usvt	O
be	O
the	O
svd	O
of	O
x.	O
from	O
equation	O
7.44	O
,	O
we	O
have	O
ˆwridge	O
=	O
v	O
(	O
s2	O
+	O
λi	O
)	O
−1sut	O
y	O
hence	O
the	O
ridge	O
predictions	O
on	O
the	O
training	B
set	I
are	O
given	O
by	O
ˆy	O
=	O
x	O
ˆwridge	O
=	O
usvt	O
v	O
(	O
s2	O
+	O
λi	O
)	O
−1sut	O
y	O
=	O
u˜sut	O
y	O
=	O
uj	O
˜sjjut	O
j	O
y	O
d	O
(	O
cid:2	O
)	O
j=1	O
(	O
7.45	O
)	O
(	O
7.46	O
)	O
(	O
7.47	O
)	O
7.5.	O
ridge	B
regression	I
229	O
u2	O
ml	O
estimate	O
u1	O
map	O
estimate	O
prior	O
mean	B
figure	O
7.9	O
geometry	O
of	O
ridge	B
regression	I
.	O
the	O
likelihood	B
is	O
shown	O
as	O
an	O
ellipse	O
,	O
and	O
the	O
prior	O
is	O
shown	O
as	O
a	O
circle	O
centered	O
on	O
the	O
origin	O
.	O
based	O
on	O
figure	O
3.15	O
of	O
(	O
bishop	O
2006b	O
)	O
.	O
figure	O
generated	O
by	O
geomridge	O
where	O
˜sjj	O
(	O
cid:2	O
)	O
[	O
s	O
(	O
s2	O
+	O
λi	O
)	O
−1s	O
]	O
jj	O
=	O
σ2	O
j	O
σ2	O
j	O
+	O
λ	O
and	O
σj	O
are	O
the	O
singular	B
values	I
of	O
x.	O
hence	O
ˆy	O
=	O
x	O
ˆwridge	O
=	O
d	O
(	O
cid:2	O
)	O
j=1	O
uj	O
σ2	O
j	O
σ2	O
j	O
+	O
λ	O
ut	O
j	O
y	O
in	O
contrast	O
,	O
the	O
least	B
squares	I
prediction	O
is	O
ˆy	O
=	O
x	O
ˆwls	O
=	O
(	O
usvt	O
)	O
(	O
vs−1ut	O
y	O
)	O
=	O
uut	O
y	O
=	O
d	O
(	O
cid:2	O
)	O
j=1	O
ujut	O
j	O
y	O
(	O
7.48	O
)	O
(	O
7.49	O
)	O
(	O
7.50	O
)	O
d	O
(	O
cid:2	O
)	O
j=1	O
j	O
is	O
small	O
compared	O
to	O
λ	O
,	O
then	O
direction	O
uj	O
will	O
not	O
have	O
much	O
effect	O
on	O
the	O
prediction	O
.	O
in	O
if	O
σ2	O
view	O
of	O
this	O
,	O
we	O
deﬁne	O
the	O
effective	O
number	O
of	O
degrees	B
of	I
freedom	I
of	O
the	O
model	O
as	O
follows	O
:	O
dof	O
(	O
λ	O
)	O
=	O
σ2	O
j	O
σ2	O
j	O
+	O
λ	O
(	O
7.51	O
)	O
let	O
us	O
try	O
to	O
understand	O
why	O
this	O
behavior	O
is	O
desirable	O
.	O
when	O
λ	O
=	O
0	O
,	O
dof	O
(	O
λ	O
)	O
=d	O
,	O
and	O
as	O
λ	O
→	O
∞	O
,	O
dof	O
(	O
λ	O
)	O
→	O
0.	O
in	O
section	O
7.6	O
,	O
we	O
show	O
that	O
cov	O
[	O
w|d	O
]	O
=σ	O
2	O
(	O
xt	O
x	O
)	O
−1	O
,	O
if	O
we	O
use	O
a	O
uniform	O
prior	O
for	O
w.	O
thus	O
the	O
directions	O
in	O
which	O
we	O
are	O
most	O
uncertain	O
about	O
w	O
are	O
determined	O
by	O
the	O
eigenvectors	O
of	O
this	O
matrix	O
with	O
the	O
smallest	O
eigenvalues	O
,	O
as	O
shown	O
in	O
figure	O
4.1.	O
furthermore	O
,	O
in	O
section	O
12.2.3	O
,	O
we	O
show	O
that	O
the	O
squared	O
singular	O
values	O
σ2	O
j	O
are	O
equal	O
to	O
the	O
eigenvalues	O
of	O
xt	O
x.	O
hence	O
small	O
singular	O
values	O
σj	O
correspond	O
to	O
directions	O
with	O
high	O
posterior	O
variance	B
.	O
it	O
is	O
these	O
directions	O
which	O
ridge	O
shrinks	O
the	O
most	O
.	O
230	O
chapter	O
7.	O
linear	B
regression	I
this	O
process	O
is	O
illustrated	O
in	O
figure	O
7.9.	O
the	O
horizontal	O
w1	O
parameter	B
is	O
not-well	O
determined	O
by	O
the	O
data	O
(	O
has	O
high	O
posterior	O
variance	B
)	O
,	O
but	O
the	O
vertical	O
w2	O
parameter	B
is	O
well-determined	O
.	O
hence	O
wmap	O
is	O
shifted	O
strongly	O
towards	O
the	O
prior	O
mean	B
,	O
which	O
is	O
0	O
.	O
(	O
compare	O
to	O
figure	O
4.14	O
(	O
c	O
)	O
,	O
which	O
illustrated	O
sensor	B
fusion	I
with	O
sensors	O
of	O
different	O
reliabilities	O
.	O
)	O
in	O
this	O
way	O
,	O
ill-determined	O
parameters	O
are	O
reduced	O
in	O
size	O
towards	O
0.	O
this	O
is	O
called	O
shrinkage	B
.	O
is	O
close	O
to	O
ˆwmle	O
2	O
,	O
but	O
wmap	O
1	O
2	O
there	O
is	O
a	O
related	O
,	O
but	O
different	O
,	O
technique	O
called	O
principal	B
components	I
regression	I
.	O
the	O
idea	O
is	O
this	O
:	O
ﬁrst	O
use	O
pca	O
to	O
reduce	O
the	O
dimensionality	O
to	O
k	O
dimensions	O
,	O
and	O
then	O
use	O
these	O
low	O
dimensional	O
features	B
as	O
input	O
to	O
regression	B
.	O
however	O
,	O
this	O
technique	O
does	O
not	O
work	O
as	O
well	O
as	O
ridge	O
in	O
terms	O
of	O
predictive	B
accuracy	O
(	O
hastie	O
et	O
al	O
.	O
2001	O
,	O
p70	O
)	O
.	O
the	O
reason	O
is	O
that	O
in	O
pc	O
regression	B
,	O
only	O
the	O
ﬁrst	O
k	O
(	O
derived	O
)	O
dimensions	O
are	O
retained	O
,	O
and	O
the	O
remaining	O
d	O
−	O
k	O
dimensions	O
are	O
entirely	O
ignored	O
.	O
by	O
contrast	O
,	O
ridge	B
regression	I
uses	O
a	O
“	O
soft	O
”	O
weighting	O
of	O
all	O
the	O
dimensions	O
.	O
7.5.4	O
regularization	B
effects	O
of	O
big	B
data	I
regularization	O
is	O
the	O
most	O
common	O
way	O
to	O
avoid	O
overﬁtting	B
.	O
however	O
,	O
another	O
effective	O
approach	O
—	O
which	O
is	O
not	O
always	O
available	O
—	O
is	O
to	O
use	O
lots	O
of	O
data	O
.	O
it	O
should	O
be	O
intuitively	O
obvious	O
that	O
the	O
more	O
training	O
data	O
we	O
have	O
,	O
the	O
better	O
we	O
will	O
be	O
able	O
to	O
learn.2	O
so	O
we	O
expect	O
the	O
test	O
set	O
error	O
to	O
decrease	O
to	O
some	O
plateau	O
as	O
n	O
increases	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
7.10	O
,	O
where	O
we	O
plot	O
the	O
mean	B
squared	I
error	I
incurred	O
on	O
the	O
test	O
set	O
achieved	O
by	O
polynomial	B
regression	I
models	O
of	O
different	O
degrees	O
vs	O
n	O
(	O
a	O
plot	O
of	O
error	O
vs	O
training	B
set	I
size	O
is	O
known	O
as	O
a	O
learning	B
curve	I
)	O
.	O
the	O
level	O
of	O
the	O
plateau	O
for	O
the	O
test	O
error	O
consists	O
of	O
two	O
terms	O
:	O
an	O
irreducible	B
component	O
that	O
all	O
models	O
incur	O
,	O
due	O
to	O
the	O
intrinsic	O
variability	O
of	O
the	O
generating	O
process	O
(	O
this	O
is	O
called	O
the	O
noise	B
ﬂoor	I
)	O
;	O
and	O
a	O
component	O
that	O
depends	O
on	O
the	O
discrepancy	O
between	O
the	O
generating	O
process	O
(	O
the	O
“	O
truth	O
”	O
)	O
and	O
the	O
model	O
:	O
this	O
is	O
called	O
structural	B
error	I
.	O
in	O
figure	O
7.10	O
,	O
the	O
truth	O
is	O
a	O
degree	B
2	O
polynomial	O
,	O
and	O
we	O
try	O
ﬁtting	O
polynomials	O
of	O
degrees	O
1	O
,	O
2	O
and	O
25	O
to	O
this	O
data	O
.	O
call	O
the	O
3	O
models	O
m1	O
,	O
m2	O
and	O
m25	O
.	O
we	O
see	O
that	O
the	O
structural	B
error	I
for	O
models	O
m2	O
and	O
m25	O
is	O
zero	O
,	O
since	O
both	O
are	O
able	O
to	O
capture	O
the	O
true	O
generating	O
process	O
.	O
however	O
,	O
the	O
structural	B
error	I
for	O
m1	O
is	O
substantial	O
,	O
which	O
is	O
evident	O
from	O
the	O
fact	O
that	O
the	O
plateau	O
occurs	O
high	O
above	O
the	O
noise	B
ﬂoor	I
.	O
for	O
any	O
model	O
that	O
is	O
expressive	O
enough	O
to	O
capture	O
the	O
truth	O
(	O
i.e.	O
,	O
one	O
with	O
small	O
structural	O
error	O
)	O
,	O
the	O
test	O
error	O
will	O
go	O
to	O
the	O
noise	B
ﬂoor	I
as	O
n	O
→	O
∞	O
.	O
however	O
,	O
it	O
will	O
typically	O
go	O
to	O
zero	O
faster	O
for	O
simpler	O
models	O
,	O
since	O
there	O
are	O
fewer	O
parameters	O
to	O
estimate	O
.	O
in	O
particular	O
,	O
for	O
ﬁnite	O
training	O
sets	O
,	O
there	O
will	O
be	O
some	O
discrepancy	O
between	O
the	O
parameters	O
that	O
we	O
estimate	O
and	O
the	O
best	O
parameters	O
that	O
we	O
could	O
estimate	O
given	O
the	O
particular	O
model	O
class	O
.	O
this	O
is	O
called	O
approximation	B
error	I
,	O
and	O
goes	O
to	O
zero	O
as	O
n	O
→	O
∞	O
,	O
but	O
it	O
goes	O
to	O
zero	O
faster	O
for	O
simpler	O
models	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
7.10.	O
see	O
also	O
exercise	O
7.1.	O
in	O
domains	O
with	O
lots	O
of	O
data	O
,	O
simple	O
methods	O
can	O
work	O
surprisingly	O
well	O
(	O
halevy	O
et	O
al	O
.	O
2009	O
)	O
.	O
however	O
,	O
there	O
are	O
still	O
reasons	O
to	O
study	O
more	O
sophisticated	O
learning	B
methods	O
,	O
because	O
there	O
will	O
always	O
be	O
problems	O
for	O
which	O
we	O
have	O
little	O
data	O
.	O
for	O
example	O
,	O
even	O
in	O
such	O
a	O
data-rich	O
domain	O
as	O
web	O
search	O
,	O
as	O
soon	O
as	O
we	O
want	O
to	O
start	O
personalizing	O
the	O
results	O
,	O
the	O
amount	O
of	O
data	O
available	O
for	O
any	O
given	O
user	O
starts	O
to	O
look	O
small	O
again	O
(	O
relative	O
to	O
the	O
complexity	O
of	O
the	O
problem	O
)	O
.	O
2.	O
this	O
assumes	O
the	O
training	O
data	O
is	O
randomly	O
sampled	O
,	O
and	O
we	O
don	O
’	O
t	O
just	O
get	O
repetitions	O
of	O
the	O
same	O
examples	O
.	O
having	O
informatively	O
sampled	O
data	O
can	O
help	O
even	O
more	O
;	O
this	O
is	O
the	O
motivation	O
for	O
an	O
approach	O
known	O
as	O
active	B
learning	I
,	O
where	O
you	O
get	O
to	O
choose	O
your	O
training	O
data	O
.	O
truth=degree	O
2	O
,	O
model	O
=	O
degree	B
2	O
231	O
train	O
test	O
7.6.	O
bayesian	O
linear	B
regression	I
22	O
20	O
18	O
16	O
14	O
12	O
10	O
8	O
6	O
4	O
2	O
e	O
s	O
m	O
truth=degree	O
2	O
,	O
model	O
=	O
degree	B
1	O
train	O
test	O
22	O
20	O
18	O
16	O
14	O
12	O
10	O
8	O
6	O
4	O
2	O
e	O
s	O
m	O
0	O
0	O
20	O
40	O
60	O
100	O
80	O
120	O
size	O
of	O
training	B
set	I
140	O
160	O
180	O
200	O
0	O
0	O
20	O
40	O
60	O
100	O
80	O
120	O
size	O
of	O
training	B
set	I
140	O
160	O
180	O
200	O
(	O
b	O
)	O
truth=degree	O
2	O
,	O
model	O
=	O
degree	B
25	O
train	O
test	O
(	O
a	O
)	O
truth=degree	O
2	O
,	O
model	O
=	O
degree	B
10	O
train	O
test	O
22	O
20	O
18	O
16	O
14	O
12	O
10	O
8	O
6	O
4	O
2	O
e	O
s	O
m	O
22	O
20	O
18	O
16	O
14	O
12	O
10	O
8	O
6	O
4	O
2	O
e	O
s	O
m	O
0	O
0	O
20	O
40	O
60	O
100	O
80	O
120	O
size	O
of	O
training	B
set	I
140	O
160	O
180	O
200	O
0	O
0	O
20	O
40	O
60	O
100	O
80	O
120	O
size	O
of	O
training	B
set	I
140	O
160	O
180	O
200	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
7.10	O
mse	O
on	O
training	O
and	O
test	O
sets	O
vs	O
size	O
of	O
training	B
set	I
,	O
for	O
data	O
generated	O
from	O
a	O
degree	B
2	O
polynomial	O
with	O
gaussian	O
noise	O
of	O
variance	B
σ2	O
=	O
4.	O
we	O
ﬁt	O
polynomial	O
models	O
of	O
varying	O
degree	B
to	O
this	O
data	O
.	O
(	O
a	O
)	O
degree	B
1	O
.	O
(	O
b	O
)	O
degree	B
2	O
.	O
(	O
c	O
)	O
degree	B
10	O
.	O
(	O
d	O
)	O
degree	B
25.	O
note	O
that	O
for	O
small	O
training	O
set	O
sizes	O
,	O
the	O
test	O
error	O
of	O
the	O
degree	B
25	O
polynomial	O
is	O
higher	O
than	O
that	O
of	O
the	O
degree	B
2	O
polynomial	O
,	O
due	O
to	O
overﬁtting	B
,	O
but	O
this	O
difference	O
vanishes	O
once	O
we	O
have	O
enough	O
data	O
.	O
note	O
also	O
that	O
the	O
degree	B
1	O
polynomial	O
is	O
too	O
simple	O
and	O
has	O
high	O
test	O
error	O
even	O
given	O
large	O
amounts	O
of	O
training	O
data	O
.	O
figure	O
generated	O
by	O
linregpolyvsn	O
.	O
in	O
such	O
cases	O
,	O
we	O
may	O
want	O
to	O
learn	O
multiple	O
related	O
models	O
at	O
the	O
same	O
time	O
,	O
which	O
is	O
known	O
as	O
multi-task	B
learning	I
.	O
this	O
will	O
allow	O
us	O
to	O
“	O
borrow	B
statistical	I
strength	I
”	O
from	O
tasks	O
with	O
lots	O
of	O
data	O
and	O
to	O
share	O
it	O
with	O
tasks	O
with	O
little	O
data	O
.	O
we	O
will	O
discuss	O
ways	O
to	O
do	O
later	O
in	O
the	O
book	O
.	O
7.6	O
bayesian	O
linear	B
regression	I
although	O
ridge	B
regression	I
is	O
a	O
useful	O
way	O
to	O
compute	O
a	O
point	B
estimate	I
,	O
sometimes	O
we	O
want	O
to	O
compute	O
the	O
full	B
posterior	O
over	O
w	O
and	O
σ2	O
.	O
for	O
simplicity	O
,	O
we	O
will	O
initially	O
assume	O
the	O
noise	O
variance	O
σ2	O
is	O
known	O
,	O
so	O
we	O
focus	O
on	O
computing	O
p	O
(	O
w|d	O
,	O
σ2	O
)	O
.	O
then	O
in	O
section	O
7.6.3	O
we	O
consider	O
232	O
chapter	O
7.	O
linear	B
regression	I
the	O
general	O
case	O
,	O
where	O
we	O
compute	O
p	O
(	O
w	O
,	O
σ2|d	O
)	O
.	O
we	O
assume	O
throughout	O
a	O
gaussian	O
likelihood	B
model	O
.	O
performing	O
bayesian	O
inference	B
with	O
a	O
robust	B
likelihood	O
is	O
also	O
possible	O
,	O
but	O
requires	O
more	O
advanced	O
techniques	O
(	O
see	O
exercise	O
24.5	O
)	O
.	O
7.6.1	O
computing	O
the	O
posterior	O
in	O
linear	B
regression	I
,	O
the	O
likelihood	B
is	O
given	O
by	O
p	O
(	O
y|x	O
,	O
w	O
,	O
μ	O
,	O
σ2	O
)	O
=n	O
(	O
y|μ	O
+	O
xw	O
,	O
σ2in	O
)	O
(	O
cid:8	O
)	O
∝	O
exp	O
2σ2	O
(	O
y	O
−	O
μ1n	O
−	O
xw	O
)	O
t	O
(	O
y	O
−	O
μ1n	O
−	O
xw	O
)	O
−	O
1	O
(	O
cid:10	O
)	O
where	O
μ	O
is	O
an	O
offset	O
term	O
.	O
if	O
the	O
inputs	O
are	O
centered	O
,	O
so	O
i	O
xij	O
=	O
0	O
for	O
each	O
j	O
,	O
the	O
mean	B
of	O
the	O
output	O
is	O
equally	O
likely	O
to	O
be	O
positive	O
or	O
negative	O
.	O
so	O
let	O
us	O
put	O
an	O
improper	B
prior	I
on	O
μ	O
of	O
the	O
form	O
p	O
(	O
μ	O
)	O
∝	O
1	O
,	O
and	O
then	O
integrate	O
it	O
out	O
to	O
get	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
7.52	O
)	O
(	O
7.53	O
)	O
(	O
7.54	O
)	O
p	O
(	O
y|x	O
,	O
w	O
,	O
σ2	O
)	O
∝	O
exp	O
−	O
1	O
2σ2	O
||y	O
−	O
y1n	O
−	O
xw||2	O
2	O
(	O
cid:10	O
)	O
n	O
i=1	O
yi	O
is	O
the	O
empirical	O
mean	O
of	O
the	O
output	O
.	O
for	O
notational	O
simplicity	O
,	O
we	O
shall	O
where	O
y	O
=	O
1	O
assume	O
the	O
output	O
has	O
been	O
centered	O
,	O
and	O
write	O
y	O
for	O
y	O
−	O
y1n	O
.	O
n	O
the	O
conjugate	B
prior	I
to	O
the	O
above	O
gaussian	O
likelihood	B
is	O
also	O
a	O
gaussian	O
,	O
which	O
we	O
will	O
denote	O
by	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
w|w0	O
,	O
v0	O
)	O
.	O
using	O
bayes	O
rule	O
for	O
gaussians	O
,	O
equation	O
4.125	O
,	O
the	O
posterior	O
is	O
given	O
by	O
p	O
(	O
w|x	O
,	O
y	O
,	O
σ2	O
)	O
∝	O
n	O
(	O
w|w0	O
,	O
v0	O
)	O
n	O
(	O
y|xw	O
,	O
σ2in	O
)	O
=	O
n	O
(	O
w|wn	O
,	O
vn	O
)	O
1	O
σ2	O
vn	O
xt	O
y	O
0	O
w0	O
+	O
wn	O
=	O
vn	O
v−1	O
n	O
=	O
v−1	O
v−1	O
1	O
σ2	O
vn	O
=	O
σ2	O
(	O
σ2v−1	O
0	O
+	O
xt	O
x	O
)	O
xt	O
x	O
0	O
+	O
−1	O
(	O
7.55	O
)	O
(	O
7.56	O
)	O
(	O
7.57	O
)	O
(	O
7.58	O
)	O
if	O
w0	O
=	O
0	O
and	O
v0	O
=	O
τ	O
2i	O
,	O
then	O
the	O
posterior	B
mean	I
reduces	O
to	O
the	O
ridge	O
estimate	O
,	O
if	O
we	O
deﬁne	O
λ	O
=	O
σ2	O
τ	O
2	O
.	O
this	O
is	O
because	O
the	O
mean	B
and	O
mode	B
of	O
a	O
gaussian	O
are	O
the	O
same	O
.	O
to	O
gain	O
insight	O
into	O
the	O
posterior	O
distribution	O
(	O
and	O
not	O
just	O
its	O
mode	B
)	O
,	O
let	O
us	O
consider	O
a	O
1d	O
example	O
:	O
y	O
(	O
x	O
,	O
w	O
)	O
=	O
w0	O
+	O
w1x	O
+	O
	O
(	O
7.59	O
)	O
where	O
the	O
“	O
true	O
”	O
parameters	O
are	O
w0	O
=	O
−0.3	O
and	O
w1	O
=	O
0.5.	O
in	O
figure	O
7.11	O
we	O
plot	O
the	O
prior	O
,	O
the	O
likelihood	B
,	O
the	O
posterior	O
,	O
and	O
some	O
samples	B
from	O
the	O
posterior	O
predictive	O
.	O
in	O
particular	O
,	O
the	O
right	O
hand	O
column	O
plots	O
the	O
function	O
y	O
(	O
x	O
,	O
w	O
(	O
s	O
)	O
)	O
where	O
x	O
ranges	O
over	O
[	O
−1	O
,	O
1	O
]	O
,	O
and	O
w	O
(	O
s	O
)	O
∼	O
n	O
(	O
w|wn	O
,	O
vn	O
)	O
is	O
a	O
sample	O
from	O
the	O
parameter	B
posterior	O
.	O
initially	O
,	O
when	O
we	O
sample	O
from	O
the	O
prior	O
(	O
ﬁrst	O
row	O
)	O
,	O
our	O
predictions	O
are	O
“	O
all	O
over	O
the	O
place	O
”	O
,	O
since	O
our	O
prior	O
is	O
uniform	O
.	O
after	O
we	O
see	O
one	O
data	O
point	O
(	O
second	O
row	O
)	O
,	O
our	O
posterior	O
becomes	O
constrained	O
by	O
the	O
corresponding	O
likelihood	B
,	O
and	O
our	O
predictions	O
pass	O
close	O
to	O
the	O
observed	O
data	O
.	O
however	O
,	O
we	O
see	O
that	O
the	O
posterior	O
has	O
a	O
ridge-like	O
shape	O
,	O
reﬂecting	O
the	O
fact	O
that	O
there	O
are	O
many	O
possible	O
solutions	O
,	O
with	O
different	O
7.6.	O
bayesian	O
linear	B
regression	I
233	O
likelihood	B
prior/posterior	O
1	O
w1	O
0	O
−1	O
−1	O
1	O
w1	O
0	O
1	O
0	O
w0	O
1	O
w1	O
0	O
−1	O
−1	O
1	O
0	O
w0	O
−1	O
−1	O
1	O
0	O
w0	O
1	O
w1	O
0	O
1	O
w1	O
0	O
−1	O
−1	O
1	O
0	O
w0	O
−1	O
−1	O
1	O
0	O
w0	O
1	O
w1	O
0	O
1	O
w1	O
0	O
−1	O
−1	O
1	O
0	O
w0	O
−1	O
−1	O
1	O
0	O
w0	O
data	O
space	O
1	O
0	O
y	O
−1	O
−1	O
1	O
0	O
y	O
−1	O
−1	O
1	O
0	O
y	O
−1	O
−1	O
1	O
0	O
y	O
−1	O
−1	O
1	O
0	O
x	O
1	O
0	O
x	O
1	O
0	O
x	O
1	O
0	O
x	O
sequential	B
bayesian	O
updating	O
of	O
a	O
linear	B
regression	I
model	O
p	O
(	O
y|x	O
)	O
=	O
n	O
(	O
y|w0x0	O
+	O
w1x1	O
,	O
σ2	O
)	O
.	O
figure	O
7.11	O
row	O
0	O
represents	O
the	O
prior	O
,	O
row	O
1	O
represents	O
the	O
ﬁrst	O
data	O
point	O
(	O
x1	O
,	O
y1	O
)	O
,	O
row	O
2	O
represents	O
the	O
second	O
data	O
point	O
(	O
x2	O
,	O
y2	O
)	O
,	O
row	O
3	O
represents	O
the	O
20th	O
data	O
point	O
(	O
x20	O
,	O
y20	O
)	O
.	O
left	O
column	O
:	O
likelihood	B
function	O
for	O
current	O
data	O
point	O
.	O
middle	O
column	O
:	O
posterior	O
given	O
data	O
so	O
far	O
,	O
p	O
(	O
w|x1	O
:	O
n	O
,	O
y1	O
:	O
n	O
)	O
(	O
so	O
the	O
ﬁrst	O
line	O
is	O
the	O
prior	O
)	O
.	O
right	O
column	O
:	O
samples	B
from	O
the	O
current	O
prior/posterior	O
predictive	B
distribution	O
.	O
the	O
white	O
cross	O
in	O
columns	O
1	O
and	O
2	O
represents	O
the	O
true	O
parameter	O
value	O
;	O
we	O
see	O
that	O
the	O
mode	B
of	O
the	O
posterior	O
rapidly	O
(	O
after	O
20	O
samples	B
)	O
converges	O
to	O
this	O
point	O
.	O
the	O
blue	O
circles	O
in	O
column	O
3	O
are	O
the	O
observed	O
data	O
points	O
.	O
based	O
on	O
figure	O
3.7	O
of	O
(	O
bishop	O
2006a	O
)	O
.	O
figure	O
generated	O
by	O
bayeslinregdemo2d	O
.	O
slopes/intercepts	O
.	O
this	O
makes	O
sense	O
since	O
we	O
can	O
not	O
uniquely	O
infer	O
two	O
parameters	O
from	O
one	O
observation	B
.	O
after	O
we	O
see	O
two	O
data	O
points	O
(	O
third	O
row	O
)	O
,	O
the	O
posterior	O
becomes	O
much	O
narrower	O
,	O
and	O
our	O
predictions	O
all	O
have	O
similar	B
slopes	O
and	O
intercepts	O
.	O
after	O
we	O
observe	O
20	O
data	O
points	O
(	O
last	O
row	O
)	O
,	O
the	O
posterior	O
is	O
essentially	O
a	O
delta	O
function	O
centered	O
on	O
the	O
true	O
value	O
,	O
indicated	O
by	O
a	O
white	O
cross	O
.	O
(	O
the	O
estimate	O
converges	O
to	O
the	O
truth	O
since	O
the	O
data	O
was	O
generated	O
from	O
this	O
model	O
,	O
and	O
because	O
bayes	O
is	O
a	O
consistent	B
estimator	I
;	O
see	O
section	O
6.4.1	O
for	O
discussion	O
of	O
this	O
point	O
.	O
)	O
7.6.2	O
computing	O
the	O
posterior	O
predictive	O
it	O
’	O
s	O
tough	O
to	O
make	O
predictions	O
,	O
especially	O
about	O
the	O
future	O
.	O
—	O
yogi	O
berra	O
234	O
chapter	O
7.	O
linear	B
regression	I
in	O
machine	B
learning	I
,	O
we	O
often	O
care	O
more	O
about	O
predictions	O
than	O
about	O
interpreting	O
the	O
parame-	O
ters	O
.	O
using	O
equation	O
4.126	O
,	O
we	O
can	O
easily	O
show	O
that	O
the	O
posterior	B
predictive	I
distribution	I
at	O
a	O
test	O
(	O
cid:28	O
)	O
point	O
x	O
is	O
also	O
gaussian	O
:	O
p	O
(	O
y|x	O
,	O
d	O
,	O
σ2	O
)	O
=	O
n	O
(	O
y|xt	O
w	O
,	O
σ2	O
)	O
n	O
(	O
w|wn	O
,	O
vn	O
)	O
dw	O
=	O
n	O
(	O
y|wt	O
n	O
x	O
,	O
σ2	O
σ2	O
n	O
(	O
x	O
)	O
=σ	O
2	O
+	O
xt	O
vn	O
x	O
n	O
(	O
x	O
)	O
)	O
(	O
7.60	O
)	O
(	O
7.61	O
)	O
(	O
7.62	O
)	O
the	O
variance	B
in	O
this	O
prediction	O
,	O
σ2	O
n	O
(	O
x	O
)	O
,	O
depends	O
on	O
two	O
terms	O
:	O
the	O
variance	B
of	O
the	O
observation	B
noise	O
,	O
σ2	O
,	O
and	O
the	O
variance	B
in	O
the	O
parameters	O
,	O
vn	O
.	O
the	O
latter	O
translates	O
into	O
variance	B
about	O
observations	O
in	O
a	O
way	O
which	O
depends	O
on	O
how	O
close	O
x	O
is	O
to	O
the	O
training	O
data	O
d.	O
this	O
is	O
illustrated	O
in	O
figure	O
7.12	O
,	O
where	O
we	O
see	O
that	O
the	O
error	O
bars	O
get	O
larger	O
as	O
we	O
move	O
away	O
from	O
the	O
training	O
points	O
,	O
representing	O
increased	O
uncertainty	B
.	O
this	O
is	O
important	O
for	O
applications	O
such	O
as	O
active	B
learning	I
,	O
where	O
we	O
want	O
to	O
model	O
what	O
we	O
don	O
’	O
t	O
know	O
as	O
well	O
as	O
what	O
we	O
do	O
.	O
by	O
contrast	O
,	O
the	O
plugin	O
approximation	O
has	O
constant	O
sized	O
error	O
bars	O
,	O
since	O
n	O
(	O
y|xt	O
w	O
,	O
σ2	O
)	O
δ	O
ˆw	O
(	O
w	O
)	O
dw	O
=	O
p	O
(	O
y|x	O
,	O
ˆw	O
,	O
σ2	O
)	O
(	O
7.63	O
)	O
(	O
cid:28	O
)	O
p	O
(	O
y|x	O
,	O
d	O
,	O
σ2	O
)	O
≈	O
see	O
figure	O
7.12	O
(	O
a	O
)	O
.	O
7.6.3	O
bayesian	O
inference	B
when	O
σ2	O
is	O
unknown	B
*	O
in	O
this	O
section	O
,	O
we	O
apply	O
the	O
results	O
in	O
section	O
4.6.3	O
to	O
the	O
problem	O
of	O
computing	O
p	O
(	O
w	O
,	O
σ2|d	O
)	O
for	O
a	O
linear	B
regression	I
model	O
.	O
this	O
generalizes	O
the	O
results	O
from	O
section	O
7.6.1	O
where	O
we	O
assumed	O
σ2	O
was	O
known	O
.	O
in	O
the	O
case	O
where	O
we	O
use	O
an	O
uninformative	B
prior	O
,	O
we	O
will	O
see	O
some	O
interesting	O
connections	O
to	O
frequentist	B
statistics	I
.	O
7.6.3.1	O
conjugate	B
prior	I
as	O
usual	O
,	O
the	O
likelihood	B
has	O
the	O
form	O
p	O
(	O
y|x	O
,	O
w	O
,	O
σ2	O
)	O
=	O
n	O
(	O
y|xw	O
,	O
σ2in	O
)	O
(	O
7.64	O
)	O
by	O
analogy	O
to	O
section	O
4.6.3	O
,	O
one	O
can	O
show	O
that	O
the	O
natural	O
conjugate	O
prior	O
has	O
the	O
following	O
form	O
:	O
p	O
(	O
w	O
,	O
σ2	O
)	O
=	O
nig	O
(	O
w	O
,	O
σ2|w0	O
,	O
v0	O
,	O
a0	O
,	O
b0	O
)	O
(	O
cid:2	O
)	O
n	O
(	O
w|w0	O
,	O
σ2v0	O
)	O
ig	O
(	O
σ2|a0	O
,	O
b0	O
)	O
=	O
ba0	O
0	O
(	O
cid:29	O
)	O
(	O
2π	O
)	O
d/2|v0|	O
1	O
×	O
exp	O
−	O
(	O
a0+	O
(	O
d/2	O
)	O
+1	O
)	O
(	O
σ2	O
)	O
0	O
(	O
w	O
−	O
w0	O
)	O
+	O
2b0	O
−	O
(	O
w	O
−	O
w0	O
)	O
t	O
v−1	O
2σ2	O
2	O
γ	O
(	O
a0	O
)	O
(	O
cid:30	O
)	O
(	O
7.65	O
)	O
(	O
7.66	O
)	O
(	O
7.67	O
)	O
(	O
7.68	O
)	O
posterior	O
predictive	O
(	O
known	O
variance	B
)	O
prediction	O
training	O
data	O
235	O
−6	O
−4	O
−2	O
0	O
(	O
b	O
)	O
2	O
4	O
6	O
8	O
functions	O
sampled	O
from	O
posterior	O
7.6.	O
bayesian	O
linear	B
regression	I
plugin	O
approximation	O
(	O
mle	O
)	O
prediction	O
training	O
data	O
60	O
50	O
40	O
30	O
20	O
10	O
0	O
−8	O
−6	O
−4	O
−2	O
2	O
4	O
6	O
8	O
0	O
(	O
a	O
)	O
functions	O
sampled	O
from	O
plugin	O
approximation	O
to	O
posterior	O
50	O
45	O
40	O
35	O
30	O
25	O
20	O
15	O
10	O
5	O
80	O
70	O
60	O
50	O
40	O
30	O
20	O
10	O
0	O
−10	O
−8	O
100	O
80	O
60	O
40	O
20	O
0	O
0	O
−8	O
−6	O
−4	O
−2	O
0	O
(	O
c	O
)	O
2	O
4	O
6	O
8	O
−20	O
−8	O
−6	O
−4	O
−2	O
2	O
4	O
6	O
8	O
0	O
(	O
d	O
)	O
figure	O
7.12	O
(	O
a	O
)	O
plug-in	B
approximation	I
to	O
predictive	B
density	O
(	O
we	O
plug	O
in	O
the	O
mle	O
of	O
the	O
parameters	O
)	O
.	O
(	O
b	O
)	O
posterior	B
predictive	I
density	I
,	O
obtained	O
by	O
integrating	O
out	O
the	O
parameters	O
.	O
black	O
curve	O
is	O
posterior	B
mean	I
,	O
error	O
bars	O
are	O
2	O
standard	O
deviations	O
of	O
the	O
posterior	B
predictive	I
density	I
.	O
(	O
c	O
)	O
10	O
samples	B
from	O
the	O
plugin	O
approximation	O
to	O
posterior	O
predictive	O
.	O
(	O
d	O
)	O
10	O
samples	B
from	O
the	O
posterior	O
predictive	O
.	O
figure	O
generated	O
by	O
linregpostpreddemo	O
.	O
with	O
this	O
prior	O
and	O
likelihood	B
,	O
one	O
can	O
show	O
that	O
the	O
posterior	O
has	O
the	O
following	O
form	O
:	O
p	O
(	O
w	O
,	O
σ2|d	O
)	O
=	O
nig	O
(	O
w	O
,	O
σ2|wn	O
,	O
vn	O
,	O
an	O
,	O
bn	O
)	O
(	O
7.69	O
)	O
(	O
7.70	O
)	O
(	O
7.71	O
)	O
(	O
7.72	O
)	O
(	O
7.73	O
)	O
0	O
w0	O
+	O
xt	O
y	O
)	O
0	O
+	O
xt	O
x	O
)	O
−1	O
wn	O
=	O
vn	O
(	O
v−1	O
vn	O
=	O
(	O
v−1	O
an	O
=	O
a0	O
+	O
n/2	O
bn	O
=	O
b0	O
+	O
1	O
2	O
(	O
cid:3	O
)	O
0	O
v−1	O
wt	O
0	O
w0	O
+	O
yt	O
y	O
−	O
wt	O
n	O
v−1	O
n	O
wn	O
(	O
cid:4	O
)	O
the	O
expressions	O
for	O
wn	O
and	O
vn	O
are	O
similar	B
to	O
the	O
case	O
where	O
σ2	O
is	O
known	O
.	O
the	O
expression	O
for	O
an	O
is	O
also	O
intuitive	O
,	O
since	O
it	O
just	O
updates	O
the	O
counts	O
.	O
the	O
expression	O
for	O
bn	O
can	O
be	O
interpreted	O
236	O
chapter	O
7.	O
linear	B
regression	I
it	O
is	O
the	O
prior	O
sum	B
of	I
squares	I
,	O
b0	O
,	O
plus	O
the	O
empirical	O
sum	O
of	O
squares	O
,	O
yt	O
y	O
,	O
plus	O
a	O
as	O
follows	O
:	O
term	O
due	O
to	O
the	O
error	O
in	O
the	O
prior	O
on	O
w.	O
the	O
posterior	O
marginals	O
are	O
as	O
follows	O
:	O
p	O
(	O
σ2|d	O
)	O
=	O
ig	O
(	O
an	O
,	O
bn	O
)	O
p	O
(	O
w|d	O
)	O
=t	O
bn	O
an	O
vn	O
,	O
2an	O
)	O
(	O
wn	O
,	O
we	O
give	O
a	O
worked	O
example	O
of	O
using	O
these	O
equations	O
in	O
section	O
7.6.3.3.	O
by	O
analogy	O
to	O
section	O
4.6.3.6	O
,	O
the	O
posterior	B
predictive	I
distribution	I
is	O
a	O
student	O
t	O
distribution	O
.	O
in	O
particular	O
,	O
given	O
m	O
new	O
test	O
inputs	O
˜x	O
,	O
we	O
have	O
p	O
(	O
˜y|	O
˜x	O
,	O
d	O
)	O
=t	O
(	O
˜y|	O
˜xwn	O
,	O
bn	O
an	O
(	O
im	O
+	O
˜xvn	O
˜xt	O
)	O
,	O
2an	O
)	O
(	O
7.76	O
)	O
the	O
predictive	B
variance	O
has	O
two	O
components	O
:	O
(	O
bn	O
/an	O
)	O
im	O
due	O
to	O
the	O
measurement	O
noise	O
,	O
and	O
(	O
bn	O
/an	O
)	O
˜xvn	O
˜xt	O
due	O
to	O
the	O
uncertainty	B
in	O
w.	O
this	O
latter	O
terms	O
varies	O
depending	O
on	O
how	O
close	O
the	O
test	O
inputs	O
are	O
to	O
the	O
training	O
data	O
.	O
it	O
is	O
common	O
to	O
set	O
a0	O
=	O
b0	O
=	O
0	O
,	O
corresponding	O
to	O
an	O
uninformative	B
prior	O
for	O
σ2	O
,	O
and	O
to	O
set	O
−1	O
for	O
any	O
positive	O
value	O
g.	O
this	O
is	O
called	O
zellner	O
’	O
s	O
g-prior	B
(	O
zellner	O
w0	O
=	O
0	O
and	O
v0	O
=	O
g	O
(	O
xt	O
x	O
)	O
1986	O
)	O
.	O
here	O
g	O
plays	O
a	O
role	O
analogous	O
to	O
1/λ	O
in	O
ridge	B
regression	I
.	O
however	O
,	O
the	O
prior	O
covariance	B
is	O
−1	O
rather	O
than	O
i.	O
this	O
ensures	O
that	O
the	O
posterior	O
is	O
invariant	B
to	O
scaling	O
proportional	O
to	O
(	O
xt	O
x	O
)	O
of	O
the	O
inputs	O
(	O
minka	O
2000b	O
)	O
.	O
see	O
also	O
exercise	O
7.10.	O
we	O
will	O
see	O
below	O
that	O
if	O
we	O
use	O
an	O
uninformative	B
prior	O
,	O
the	O
posterior	O
precision	O
given	O
n	O
measurements	O
is	O
v−1	O
n	O
=	O
xt	O
x.	O
the	O
unit	B
information	I
prior	I
is	O
deﬁned	O
to	O
contain	O
as	O
much	O
information	B
as	O
one	O
sample	O
(	O
kass	O
and	O
wasserman	O
1995	O
)	O
.	O
to	O
create	O
a	O
unit	B
information	I
prior	I
for	O
linear	B
regression	I
,	O
we	O
need	O
to	O
use	O
v−1	O
n	O
xt	O
x	O
,	O
which	O
is	O
equivalent	O
to	O
the	O
g-prior	B
with	O
g	O
=	O
n	O
.	O
0	O
=	O
1	O
7.6.3.2	O
uninformative	B
prior	O
an	O
uninformative	B
prior	O
can	O
be	O
obtained	O
by	O
considering	O
the	O
uninformative	B
limit	O
of	O
the	O
conjugate	O
g-prior	O
,	O
which	O
corresponds	O
to	O
setting	O
g	O
=	O
∞	O
.	O
this	O
is	O
equivalent	O
to	O
an	O
improper	O
nig	O
prior	O
with	O
w0	O
=	O
0	O
,	O
v0	O
=	O
∞i	O
,	O
a0	O
=	O
0	O
and	O
b0	O
=	O
0	O
,	O
which	O
gives	O
p	O
(	O
w	O
,	O
σ2	O
)	O
∝	O
σ−	O
(	O
d+2	O
)	O
.	O
alternatively	O
,	O
we	O
can	O
start	O
with	O
the	O
semi-conjugate	B
prior	O
p	O
(	O
w	O
,	O
σ2	O
)	O
=p	O
(	O
w	O
)	O
p	O
(	O
σ2	O
)	O
,	O
and	O
take	O
each	O
term	O
to	O
its	O
uninformative	B
limit	O
individually	O
,	O
which	O
gives	O
p	O
(	O
w	O
,	O
σ2	O
)	O
∝	O
σ−2	O
.	O
this	O
is	O
equivalent	O
to	O
an	O
improper	O
nig	O
prior	O
with	O
w0	O
=	O
0	O
,	O
v	O
=	O
∞i	O
,	O
a0	O
=	O
−d/2	O
and	O
b0	O
=	O
0.	O
the	O
corresponding	O
posterior	O
is	O
given	O
by	O
p	O
(	O
w	O
,	O
σ2|d	O
)	O
=	O
nig	O
(	O
w	O
,	O
σ2|wn	O
,	O
vn	O
,	O
an	O
,	O
bn	O
)	O
−1xt	O
y	O
wn	O
=	O
ˆwmle	O
=	O
(	O
xt	O
x	O
)	O
vn	O
=	O
(	O
xt	O
x	O
)	O
n	O
−	O
d	O
−1	O
an	O
=	O
2	O
s2	O
bn	O
=	O
s2	O
(	O
cid:2	O
)	O
(	O
y	O
−	O
x	O
ˆwmle	O
)	O
t	O
(	O
y	O
−	O
x	O
ˆwmle	O
2	O
(	O
7.74	O
)	O
(	O
7.75	O
)	O
(	O
7.77	O
)	O
(	O
7.78	O
)	O
(	O
7.79	O
)	O
(	O
7.80	O
)	O
(	O
7.81	O
)	O
(	O
7.82	O
)	O
237	O
7.6.	O
bayesian	O
linear	B
regression	I
wj	O
w0	O
w1	O
w2	O
w3	O
w4	O
w5	O
w6	O
w7	O
w8	O
w9	O
w10	O
e	O
[	O
wj|d	O
]	O
10.998	O
-0.004	O
-0.054	O
0.068	O
-1.294	O
0.232	O
-0.357	O
-0.237	O
0.181	O
-1.285	O
-0.433	O
(	O
cid:17	O
)	O
var	O
[	O
wj|d	O
]	O
3.06027	O
0.00156	O
0.02190	O
0.09947	O
0.56381	O
0.10438	O
1.56646	O
1.00601	O
0.23672	O
0.86485	O
0.73487	O
sig	O
*	O
*	O
*	O
*	O
*	O
95	O
%	O
ci	O
[	O
4.652	O
,	O
17.345	O
]	O
[	O
-0.008	O
,	O
-0.001	O
]	O
[	O
-0.099	O
,	O
-0.008	O
]	O
[	O
-0.138	O
,	O
0.274	O
]	O
[	O
-2.463	O
,	O
-0.124	O
]	O
[	O
0.015	O
,	O
0.448	O
]	O
[	O
-3.605	O
,	O
2.892	O
]	O
[	O
-2.324	O
,	O
1.849	O
]	O
[	O
-0.310	O
,	O
0.672	O
]	O
[	O
-3.079	O
,	O
0.508	O
]	O
[	O
-1.957	O
,	O
1.091	O
]	O
table	O
7.2	O
posterior	B
mean	I
,	O
standard	B
deviation	I
and	O
credible	O
intervals	O
for	O
a	O
linear	B
regression	I
model	O
with	O
an	O
uninformative	B
prior	O
ﬁt	O
to	O
the	O
caterpillar	O
data	O
.	O
produced	O
by	O
linregbayescaterpillar	O
.	O
the	O
marginal	B
distribution	I
of	O
the	O
weights	O
is	O
given	O
by	O
p	O
(	O
w|d	O
)	O
=	O
t	O
(	O
w|	O
ˆw	O
,	O
s2	O
c	O
,	O
n	O
−	O
d	O
)	O
where	O
c	O
=	O
(	O
xt	O
x	O
)	O
n	O
−	O
d	O
(	O
7.83	O
)	O
−1	O
and	O
ˆw	O
is	O
the	O
mle	O
.	O
we	O
discuss	O
the	O
implications	O
of	O
these	O
equations	O
below	O
.	O
7.6.3.3	O
an	O
example	O
where	O
bayesian	O
and	O
frequentist	B
inference	O
coincide	O
*	O
the	O
use	O
of	O
a	O
(	O
semi-conjugate	B
)	O
uninformative	B
prior	O
is	O
interesting	O
because	O
the	O
resulting	O
posterior	O
turns	O
out	O
to	O
be	O
equivalent	O
to	O
the	O
results	O
from	O
frequentist	B
statistics	I
(	O
see	O
also	O
section	O
4.6.3.9	O
)	O
.	O
in	O
particular	O
,	O
from	O
equation	O
7.83	O
we	O
have	O
p	O
(	O
wj|d	O
)	O
=	O
t	O
(	O
wj|	O
ˆwj	O
,	O
cjjs2	O
n	O
−	O
d	O
,	O
n	O
−	O
d	O
)	O
(	O
7.84	O
)	O
this	O
is	O
equivalent	O
to	O
the	O
sampling	B
distribution	I
of	O
the	O
mle	O
which	O
is	O
given	O
by	O
the	O
following	O
(	O
see	O
e.g.	O
,	O
(	O
rice	O
1995	O
,	O
p542	O
)	O
,	O
(	O
casella	O
and	O
berger	O
2002	O
,	O
p554	O
)	O
)	O
:	O
wj	O
−	O
ˆwj	O
sj	O
(	O
cid:7	O
)	O
where	O
sj	O
=	O
∼	O
tn−d	O
s2cjj	O
n	O
−	O
d	O
(	O
7.85	O
)	O
(	O
7.86	O
)	O
is	O
the	O
standard	O
error	O
of	O
the	O
estimated	O
parameter	B
.	O
(	O
see	O
section	O
6.2	O
for	O
a	O
discussion	O
of	O
sampling	O
distributions	O
.	O
)	O
consequently	O
,	O
the	O
frequentist	B
conﬁdence	O
interval	O
and	O
the	O
bayesian	O
marginal	O
credible	O
interval	O
for	O
the	O
parameters	O
are	O
the	O
same	O
in	O
this	O
case	O
.	O
as	O
a	O
worked	O
example	O
of	O
this	O
,	O
consider	O
the	O
caterpillar	O
dataset	O
from	O
(	O
marin	O
and	O
robert	O
2007	O
)	O
.	O
(	O
the	O
details	O
of	O
what	O
the	O
data	O
mean	O
don	O
’	O
t	O
matter	O
for	O
our	O
present	O
purposes	O
.	O
)	O
we	O
can	O
compute	O
238	O
chapter	O
7.	O
linear	B
regression	I
the	O
posterior	B
mean	I
and	O
standard	B
deviation	I
,	O
and	O
the	O
95	O
%	O
credible	O
intervals	O
(	O
ci	O
)	O
for	O
the	O
regression	B
coefficients	O
using	O
equation	O
7.84.	O
the	O
results	O
are	O
shown	O
in	O
table	O
7.2.	O
it	O
is	O
easy	O
to	O
check	O
that	O
these	O
95	O
%	O
credible	O
intervals	O
are	O
identical	O
to	O
the	O
95	O
%	O
conﬁdence	B
intervals	I
computed	O
using	O
standard	O
frequentist	O
methods	O
(	O
see	O
linregbayescaterpillar	O
for	O
the	O
code	O
)	O
.	O
we	O
can	O
also	O
use	O
these	O
marginal	O
posteriors	O
to	O
compute	O
if	O
the	O
coefficients	O
are	O
“	O
signiﬁcantly	O
”	O
different	O
from	O
0.	O
an	O
informal	O
way	O
to	O
do	O
this	O
(	O
without	O
using	O
decision	B
theory	O
)	O
is	O
to	O
check	O
if	O
its	O
95	O
%	O
ci	O
excludes	O
0.	O
from	O
table	O
7.2	O
,	O
we	O
see	O
that	O
the	O
cis	O
for	O
coefficients	O
0	O
,	O
1	O
,	O
2	O
,	O
4	O
,	O
5	O
are	O
all	O
signiﬁcant	O
by	O
this	O
measure	O
,	O
so	O
we	O
put	O
a	O
little	O
star	O
by	O
them	O
.	O
it	O
is	O
easy	O
to	O
check	O
that	O
these	O
results	O
are	O
the	O
same	O
as	O
those	O
produced	O
by	O
standard	O
frequentist	O
software	O
packages	O
which	O
compute	O
p-values	O
at	O
the	O
5	O
%	O
level	O
.	O
although	O
the	O
correspondence	B
between	O
the	O
bayesian	O
and	O
frequentist	B
results	O
might	O
seem	O
ap-	O
pealing	O
to	O
some	O
readers	O
,	O
recall	B
from	O
section	O
6.6	O
that	O
frequentist	B
inference	O
is	O
riddled	O
with	O
patholo-	O
gies	O
.	O
also	O
,	O
note	O
that	O
the	O
mle	O
does	O
not	O
even	O
exist	O
when	O
n	O
<	O
d	O
,	O
so	O
standard	O
frequentist	O
inference	B
theory	O
breaks	O
down	O
in	O
this	O
setting	O
.	O
bayesian	O
inference	B
theory	O
still	O
works	O
,	O
although	O
it	O
requires	O
the	O
use	O
of	O
proper	O
priors	O
.	O
(	O
see	O
(	O
maruyama	O
and	O
george	O
2008	O
)	O
for	O
one	O
extension	B
of	O
the	O
g-prior	B
to	O
the	O
case	O
where	O
d	O
>	O
n	O
.	O
)	O
7.6.4	O
eb	O
for	O
linear	B
regression	I
(	O
evidence	B
procedure	I
)	O
so	O
far	O
,	O
we	O
have	O
assumed	O
the	O
prior	O
is	O
known	O
.	O
in	O
this	O
section	O
,	O
we	O
describe	O
an	O
empirical	O
bayes	O
procedure	O
for	O
picking	O
the	O
hyper-parameters	B
.	O
more	O
precisely	O
,	O
we	O
choose	O
η	O
=	O
(	O
α	O
,	O
λ	O
)	O
to	O
maximize	O
the	O
marignal	O
likelihood	B
,	O
where	O
λ	O
=	O
1/σ2	O
be	O
the	O
precision	B
of	O
the	O
observation	B
noise	O
and	O
α	O
is	O
the	O
precision	B
of	O
the	O
prior	O
,	O
p	O
(	O
w	O
)	O
=n	O
(	O
w|0	O
,	O
α−1i	O
)	O
.	O
this	O
is	O
known	O
as	O
the	O
evidence	B
procedure	I
(	O
mackay	O
1995b	O
)	O
.3	O
see	O
section	O
13.7.4	O
for	O
the	O
algorithmic	O
details	O
.	O
the	O
evidence	B
procedure	I
provides	O
an	O
alternative	O
to	O
using	O
cross	B
validation	I
.	O
for	O
example	O
,	O
in	O
likelihood	B
for	O
different	O
values	O
of	O
α	O
,	O
as	O
well	O
as	O
the	O
figure	O
7.13	O
(	O
b	O
)	O
,	O
we	O
plot	O
the	O
log	O
marginal	O
maximum	O
value	O
found	O
by	O
the	O
optimizer	O
.	O
we	O
see	O
that	O
,	O
in	O
this	O
example	O
,	O
we	O
get	O
the	O
same	O
result	O
(	O
we	O
kept	O
λ	O
=	O
1/σ2	O
ﬁxed	O
in	O
both	O
methods	O
,	O
to	O
make	O
them	O
as	O
5-cv	O
,	O
shown	O
in	O
figure	O
7.13	O
(	O
a	O
)	O
.	O
comparable	O
.	O
)	O
the	O
principle	O
practical	O
advantage	O
of	O
the	O
evidence	B
procedure	I
over	O
cv	O
will	O
become	O
apparent	O
in	O
section	O
13.7	O
,	O
where	O
we	O
generalize	B
the	O
prior	O
by	O
allowing	O
a	O
different	O
αj	O
for	O
every	O
feature	O
.	O
this	O
can	O
be	O
used	O
to	O
perform	O
feature	B
selection	I
,	O
using	O
a	O
technique	O
known	O
as	O
automatic	B
relevancy	I
determination	I
or	O
ard	O
.	O
by	O
contrast	O
,	O
it	O
would	O
not	O
be	O
possible	O
to	O
use	O
cv	O
to	O
tune	O
d	O
different	O
hyper-parameters	B
.	O
the	O
evidence	B
procedure	I
is	O
also	O
useful	O
when	O
comparing	O
different	O
kinds	O
of	O
models	O
,	O
since	O
it	O
provides	O
a	O
good	O
approximation	O
to	O
the	O
evidence	B
:	O
p	O
(	O
d|m	O
)	O
=	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
≈	O
max	O
η	O
p	O
(	O
d|w	O
,	O
m	O
)	O
p	O
(	O
w|m	O
,	O
η	O
)	O
p	O
(	O
η|m	O
)	O
dwdη	O
(	O
cid:28	O
)	O
p	O
(	O
d|w	O
,	O
m	O
)	O
p	O
(	O
w|m	O
,	O
η	O
)	O
p	O
(	O
η|m	O
)	O
dw	O
(	O
7.87	O
)	O
(	O
7.88	O
)	O
it	O
is	O
important	O
to	O
(	O
at	O
least	O
approximately	O
)	O
integrate	O
over	O
η	O
rather	O
than	O
setting	O
it	O
arbitrarily	O
,	O
for	O
reasons	O
discussed	O
in	O
section	O
5.3.2.5.	O
indeed	O
,	O
this	O
is	O
the	O
method	O
we	O
used	O
to	O
evaluate	O
the	O
marginal	O
3.	O
alternatively	O
,	O
we	O
could	O
integrate	B
out	I
λ	O
analytically	O
,	O
as	O
shown	O
in	O
section	O
7.6.3	O
,	O
and	O
just	O
optimize	O
α	O
(	O
buntine	O
and	O
weigend	O
1991	O
)	O
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
this	O
is	O
less	O
accurate	O
than	O
optimizing	O
both	O
α	O
and	O
λ	O
(	O
mackay	O
1999	O
)	O
.	O
7.6.	O
bayesian	O
linear	B
regression	I
239	O
107	O
106	O
105	O
104	O
103	O
102	O
101	O
e	O
s	O
m	O
5−fold	O
cross	B
validation	I
,	O
ntrain	O
=	O
21	O
log	O
evidence	O
−50	O
−60	O
−70	O
−80	O
−90	O
−100	O
−110	O
−120	O
−130	O
−140	O
100	O
−25	O
−20	O
−15	O
−10	O
log	O
lambda	O
(	O
a	O
)	O
−5	O
0	O
5	O
−150	O
−25	O
−20	O
−15	O
−5	O
0	O
5	O
−10	O
log	O
alpha	O
(	O
b	O
)	O
figure	O
7.13	O
(	O
a	O
)	O
estimate	O
of	O
test	O
mse	O
produced	O
by	O
5-fold	O
cross-validation	O
vs	O
log	O
(	O
λ	O
)	O
.	O
the	O
smallest	O
value	O
is	O
indicated	O
by	O
the	O
vertical	O
line	O
.	O
note	O
the	O
vertical	O
scale	O
is	O
in	O
log	O
units	O
.	O
(	O
c	O
)	O
log	O
marginal	O
likelihood	B
vs	O
log	O
(	O
α	O
)	O
.	O
the	O
largest	O
value	O
is	O
indicated	O
by	O
the	O
vertical	O
line	O
.	O
figure	O
generated	O
by	O
linregpolyvsregdemo	O
.	O
likelihood	B
for	O
the	O
polynomial	B
regression	I
models	O
in	O
figures	O
5.7	O
and	O
5.8.	O
for	O
a	O
“	O
more	O
bayesian	O
”	O
approach	O
,	O
in	O
which	O
we	O
model	O
our	O
uncertainty	B
about	O
η	O
rather	O
than	O
computing	O
point	O
estimates	O
,	O
see	O
section	O
21.5.2.	O
exercises	O
exercise	O
7.1	O
behavior	O
of	O
training	B
set	I
error	O
with	O
increasing	O
sample	O
size	O
the	O
error	O
on	O
the	O
test	O
will	O
always	O
decrease	O
as	O
we	O
get	O
more	O
training	O
data	O
,	O
since	O
the	O
model	O
will	O
be	O
better	O
estimated	O
.	O
however	O
,	O
as	O
shown	O
in	O
figure	O
7.10	O
,	O
for	O
sufficiently	O
complex	O
models	O
,	O
the	O
error	O
on	O
the	O
training	B
set	I
can	O
increase	O
we	O
we	O
get	O
more	O
training	O
data	O
,	O
until	O
we	O
reach	O
some	O
plateau	O
.	O
explain	O
why	O
.	O
exercise	O
7.2	O
multi-output	O
linear	B
regression	I
(	O
source	O
:	O
jaakkola	O
.	O
)	O
when	O
we	O
have	O
multiple	O
independent	O
outputs	O
in	O
linear	B
regression	I
,	O
the	O
model	O
becomes	O
m	O
(	O
cid:13	O
)	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
n	O
(	O
yj|wt	O
j	O
xi	O
,	O
σ2	O
j	O
)	O
j=1	O
since	O
the	O
likelihood	B
factorizes	O
across	O
dimensions	O
,	O
so	O
does	O
the	O
mle	O
.	O
thus	O
ˆw	O
=	O
[	O
ˆw1	O
,	O
.	O
.	O
.	O
,	O
ˆwm	O
]	O
(	O
7.89	O
)	O
(	O
7.90	O
)	O
−1y	O
:	O
,j.	O
where	O
ˆwj	O
=	O
(	O
xt	O
x	O
)	O
in	O
this	O
exercise	O
we	O
apply	O
this	O
result	O
to	O
a	O
model	O
with	O
2	O
dimensional	O
response	O
vector	O
yi	O
∈	O
r	O
have	O
some	O
binary	O
input	O
data	O
,	O
xi	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
the	O
training	O
data	O
is	O
as	O
follows	O
:	O
2.	O
suppose	O
we	O
240	O
chapter	O
7.	O
linear	B
regression	I
x	O
0	O
0	O
0	O
1	O
1	O
1	O
y	O
(	O
−1	O
,	O
−1	O
)	O
t	O
(	O
−1	O
,	O
−2	O
)	O
t	O
(	O
−2	O
,	O
−1	O
)	O
t	O
(	O
1	O
,	O
1	O
)	O
t	O
(	O
1	O
,	O
2	O
)	O
t	O
(	O
2	O
,	O
1	O
)	O
t	O
let	O
us	O
embed	O
each	O
xi	O
into	O
2d	O
using	O
the	O
following	O
basis	O
function	O
:	O
φ	O
(	O
0	O
)	O
=	O
(	O
1	O
,	O
0	O
)	O
t	O
,	O
φ	O
(	O
1	O
)	O
=	O
(	O
0	O
,	O
1	O
)	O
t	O
the	O
model	O
becomes	O
ˆy	O
=	O
wt	O
φ	O
(	O
x	O
)	O
where	O
w	O
is	O
a	O
2	O
×	O
2	O
matrix	O
.	O
compute	O
the	O
mle	O
for	O
w	O
from	O
the	O
above	O
data	O
.	O
exercise	O
7.3	O
centering	O
and	O
ridge	B
regression	I
assume	O
that	O
x	O
=	O
0	O
,	O
so	O
the	O
input	O
data	O
has	O
been	O
centered	O
.	O
show	O
that	O
the	O
optimizer	O
of	O
j	O
(	O
w	O
,	O
w0	O
)	O
=	O
(	O
y	O
−	O
xw	O
−	O
w01	O
)	O
t	O
(	O
y	O
−	O
xw	O
−	O
w01	O
)	O
+λ	O
wt	O
w	O
is	O
ˆw0	O
=	O
y	O
w	O
=	O
(	O
xt	O
x	O
+	O
λi	O
)	O
−1xt	O
y	O
exercise	O
7.4	O
mle	O
for	O
σ2	O
for	O
linear	B
regression	I
show	O
that	O
the	O
mle	O
for	O
the	O
error	O
variance	O
in	O
linear	B
regression	I
is	O
given	O
by	O
n	O
(	O
cid:12	O
)	O
i=1	O
ˆσ2	O
=	O
1	O
n	O
(	O
yi	O
−	O
xt	O
i	O
ˆw	O
)	O
2	O
this	O
is	O
just	O
the	O
empirical	O
variance	O
of	O
the	O
residual	B
errors	O
when	O
we	O
plug	O
in	O
our	O
estimate	O
of	O
ˆw	O
.	O
(	O
7.91	O
)	O
(	O
7.92	O
)	O
(	O
7.93	O
)	O
(	O
7.94	O
)	O
(	O
7.95	O
)	O
(	O
7.96	O
)	O
exercise	O
7.5	O
mle	O
for	O
the	O
offset	O
term	O
in	O
linear	B
regression	I
linear	O
regression	B
has	O
the	O
form	O
e	O
[	O
y|x	O
]	O
=w	O
0	O
+	O
wt	O
x.	O
it	O
is	O
common	O
to	O
include	O
a	O
column	O
of	O
1	O
’	O
s	O
in	O
the	O
design	B
matrix	I
,	O
so	O
we	O
can	O
solve	O
for	O
the	O
offset	O
term	O
w0	O
term	O
and	O
the	O
other	O
parameters	O
w	O
at	O
the	O
same	O
time	O
using	O
the	O
normal	B
equations	O
.	O
however	O
,	O
it	O
is	O
also	O
possible	O
to	O
solve	O
for	O
w	O
and	O
w0	O
separately	O
.	O
show	O
that	O
ˆw0	O
=	O
1	O
n	O
yi	O
−	O
1	O
n	O
i	O
w	O
=	O
y	O
−	O
xt	O
w	O
xt	O
(	O
7.97	O
)	O
(	O
cid:12	O
)	O
i	O
(	O
cid:12	O
)	O
i	O
(	O
cid:18	O
)	O
n	O
(	O
cid:12	O
)	O
so	O
ˆw0	O
models	O
the	O
difference	O
in	O
the	O
average	O
output	O
from	O
the	O
average	O
predicted	O
output	O
.	O
also	O
,	O
show	O
that	O
ˆw	O
=	O
(	O
xt	O
c	O
xc	O
)	O
−1xt	O
c	O
yc	O
=	O
(	O
xi	O
−	O
x	O
)	O
(	O
xi	O
−	O
x	O
)	O
t	O
(	O
yi	O
−	O
y	O
)	O
(	O
xi	O
−	O
x	O
)	O
(	O
7.98	O
)	O
i=1	O
i=1	O
i	O
=	O
xi	O
−	O
x	O
along	O
its	O
rows	O
,	O
and	O
yc	O
=	O
y	O
−	O
y	O
is	O
where	O
xc	O
is	O
the	O
centered	O
input	O
matrix	O
containing	O
xc	O
the	O
centered	O
output	O
vector	O
.	O
thus	O
we	O
can	O
ﬁrst	O
compute	O
ˆw	O
on	O
centered	O
data	O
,	O
and	O
then	O
estimate	O
w0	O
using	O
y	O
−	O
xt	O
ˆw	O
.	O
(	O
cid:19	O
)	O
−1	O
(	O
cid:18	O
)	O
n	O
(	O
cid:12	O
)	O
(	O
cid:19	O
)	O
7.6.	O
bayesian	O
linear	B
regression	I
241	O
exercise	O
7.6	O
mle	O
for	O
simple	O
linear	O
regression	B
simple	O
linear	B
regression	I
refers	O
to	O
the	O
case	O
where	O
the	O
input	O
is	O
scalar	O
,	O
so	O
d	O
=	O
1.	O
show	O
that	O
the	O
mle	O
in	O
this	O
case	O
is	O
given	O
by	O
the	O
following	O
equations	O
,	O
which	O
may	O
be	O
familiar	O
from	O
basic	O
statistics	O
classes	O
:	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
xiyi	O
−	O
n	O
x	O
y	O
w1	O
=	O
i	O
x2	O
w0	O
=	O
¯y	O
−	O
w1	O
¯x	O
≈	O
e	O
[	O
y	O
]	O
−	O
w1e	O
[	O
x	O
]	O
(	O
cid:2	O
)	O
i	O
(	O
xi	O
−	O
x	O
)	O
(	O
yi	O
−	O
¯y	O
)	O
i	O
(	O
xi	O
−	O
¯x	O
)	O
2	O
=	O
i	O
−	O
n	O
x2	O
≈	O
cov	O
[	O
x	O
,	O
y	O
]	O
var	O
[	O
x	O
]	O
(	O
7.99	O
)	O
(	O
7.100	O
)	O
see	O
linregdemo1	O
for	O
a	O
demo	O
.	O
exercise	O
7.7	O
sufficient	B
statistics	I
for	O
online	O
linear	O
regression	B
(	O
source	O
:	O
not	O
keep	O
the	O
original	O
data	O
,	O
xi	O
,	O
yi	O
,	O
but	O
we	O
do	O
have	O
the	O
following	O
functions	O
(	O
statistics	O
)	O
of	O
the	O
data	O
:	O
jaakkola	O
.	O
)	O
consider	O
ﬁtting	O
the	O
model	O
ˆy	O
=	O
w0	O
+	O
w1x	O
using	O
least	B
squares	I
.	O
unfortunately	O
we	O
did	O
x	O
(	O
n	O
)	O
=	O
c	O
(	O
n	O
)	O
xx	O
=	O
1	O
n	O
1	O
n	O
xi	O
,	O
y	O
(	O
n	O
)	O
=	O
1	O
n	O
(	O
xi	O
−	O
x	O
)	O
2	O
,	O
c	O
(	O
n	O
)	O
xy	O
=	O
1	O
n	O
n	O
(	O
cid:12	O
)	O
i=1	O
(	O
xi	O
−	O
x	O
)	O
(	O
yi	O
−	O
y	O
)	O
,	O
c	O
(	O
n	O
)	O
yy	O
=	O
n	O
(	O
cid:12	O
)	O
i=1	O
1	O
n	O
(	O
yi	O
−	O
y	O
)	O
2	O
(	O
7.101	O
)	O
(	O
7.102	O
)	O
n	O
(	O
cid:12	O
)	O
i=1	O
yi	O
n	O
(	O
cid:12	O
)	O
n	O
(	O
cid:12	O
)	O
i=1	O
i=1	O
a.	O
what	O
are	O
the	O
minimal	B
set	O
of	O
statistics	O
that	O
we	O
need	O
to	O
estimate	O
w1	O
?	O
(	O
hint	O
:	O
see	O
equation	O
7.99	O
.	O
)	O
b.	O
what	O
are	O
the	O
minimal	B
set	O
of	O
statistics	O
that	O
we	O
need	O
to	O
estimate	O
w0	O
?	O
(	O
hint	O
:	O
see	O
equation	O
7.97	O
.	O
)	O
c.	O
suppose	O
a	O
new	O
data	O
point	O
,	O
xn+1	O
,	O
yn+1	O
arrives	O
,	O
and	O
we	O
want	O
to	O
update	O
our	O
sufficient	B
statistics	I
without	O
(	O
this	O
is	O
useful	O
for	O
online	B
learning	I
.	O
)	O
show	O
that	O
we	O
looking	O
at	O
the	O
old	O
data	O
,	O
which	O
we	O
have	O
not	O
stored	O
.	O
can	O
this	O
for	O
x	O
as	O
follows	O
.	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
(	O
cid:8	O
)	O
this	O
has	O
the	O
form	O
:	O
new	O
estimate	O
is	O
old	O
estimate	O
plus	O
correction	O
.	O
we	O
see	O
that	O
the	O
size	O
of	O
the	O
correction	O
diminishes	O
over	O
time	O
(	O
i.e.	O
,	O
as	O
we	O
get	O
more	O
samples	B
)	O
.	O
derive	O
a	O
similar	B
expression	O
to	O
update	O
y	O
d.	O
show	O
that	O
one	O
can	O
update	O
c	O
(	O
n+1	O
)	O
xy	O
recursively	O
using	O
c	O
(	O
n+1	O
)	O
xy	O
=	O
1	O
n	O
+	O
1	O
xn+1yn+1	O
+	O
nc	O
(	O
n	O
)	O
xy	O
+	O
nx	O
(	O
n	O
)	O
y	O
(	O
n	O
)	O
−	O
(	O
n	O
+	O
1	O
)	O
x	O
(	O
n+1	O
)	O
y	O
(	O
n+1	O
)	O
(	O
7.105	O
)	O
derive	O
a	O
similar	B
expression	O
to	O
update	O
cxx	O
.	O
implement	O
the	O
online	B
learning	I
algorithm	O
,	O
i.e.	O
,	O
write	O
a	O
function	O
of	O
the	O
form	O
[	O
w	O
,	O
ss	O
]	O
=	O
linregupdatess	O
(	O
ss	O
,	O
x	O
,	O
y	O
)	O
,	O
where	O
x	O
and	O
y	O
are	O
scalars	O
and	O
ss	O
is	O
a	O
structure	O
containing	O
the	O
sufficient	B
statistics	I
.	O
e.	O
f.	O
plot	O
the	O
coefficients	O
over	O
“	O
time	O
”	O
,	O
using	O
the	O
dataset	O
in	O
linregdemo1	O
.	O
(	O
speciﬁcally	O
,	O
use	O
[	O
x	O
,	O
y	O
]	O
=	O
polydatamake	O
(	O
’	O
sampling	O
’	O
,	O
’	O
thibaux	O
’	O
)	O
.	O
)	O
check	O
that	O
they	O
converge	B
to	O
the	O
solution	O
given	O
by	O
the	O
batch	B
(	O
offline	B
)	O
learner	O
(	O
i.e	O
,	O
ordinary	B
least	I
squares	I
)	O
.	O
your	O
result	O
should	O
look	O
like	O
figure	O
7.14.	O
turn	O
in	O
your	O
derivation	O
,	O
code	O
and	O
plot	O
.	O
exercise	O
7.8	O
bayesian	O
linear	B
regression	I
in	O
1d	O
with	O
known	O
σ2	O
(	O
source	O
:	O
bolstad	O
.	O
)	O
consider	O
ﬁtting	O
a	O
model	O
of	O
the	O
form	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
n	O
(	O
y|w0	O
+	O
w1x	O
,	O
σ2	O
)	O
to	O
the	O
data	O
shown	O
below	O
:	O
(	O
7.106	O
)	O
x	O
(	O
n+1	O
)	O
(	O
cid:2	O
)	O
1	O
n	O
+	O
1	O
=	O
x	O
(	O
n	O
)	O
+	O
xi	O
=	O
1	O
n	O
+	O
1	O
nx	O
(	O
n	O
)	O
+	O
xn+1	O
(	O
xn+1	O
−	O
x	O
(	O
n	O
)	O
)	O
(	O
7.103	O
)	O
(	O
7.104	O
)	O
n+1	O
(	O
cid:12	O
)	O
i=1	O
1	O
n	O
+	O
1	O
(	O
cid:7	O
)	O
242	O
chapter	O
7.	O
linear	B
regression	I
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
−6	O
s	O
t	O
h	O
g	O
e	O
w	O
i	O
online	O
linear	O
regression	B
w0	O
w1	O
w0	O
batch	B
w1	O
batch	B
0	O
5	O
10	O
time	O
15	O
20	O
figure	O
7.14	O
regression	B
coefficients	O
over	O
time	O
.	O
produced	O
by	O
exercise	O
7.7.	O
x	O
=	O
[	O
94,96,94,95,104,106,108,113,115,121,131	O
]	O
;	O
y	O
=	O
[	O
0.47	O
,	O
0.75	O
,	O
0.83	O
,	O
0.98	O
,	O
1.18	O
,	O
1.29	O
,	O
1.40	O
,	O
1.60	O
,	O
1.75	O
,	O
1.90	O
,	O
2.23	O
]	O
;	O
a.	O
compute	O
an	O
unbiased	B
estimate	O
of	O
σ2	O
using	O
n	O
(	O
cid:12	O
)	O
ˆσ2	O
=	O
1	O
n	O
−	O
2	O
(	O
yi	O
−	O
ˆyi	O
)	O
2	O
(	O
7.107	O
)	O
(	O
the	O
denominator	O
is	O
n−2	O
since	O
we	O
have	O
2	O
inputs	O
,	O
namely	O
the	O
offset	O
term	O
and	O
x	O
.	O
)	O
here	O
ˆyi	O
=	O
ˆw0+	O
ˆw1xi	O
,	O
and	O
ˆw	O
=	O
(	O
ˆw0	O
,	O
ˆw1	O
)	O
is	O
the	O
mle	O
.	O
i=1	O
b.	O
now	O
assume	O
the	O
following	O
prior	O
on	O
w	O
:	O
p	O
(	O
w	O
)	O
=p	O
(	O
w0	O
)	O
p	O
(	O
w1	O
)	O
(	O
7.108	O
)	O
use	O
an	O
(	O
improper	O
)	O
uniform	O
prior	O
on	O
w0	O
and	O
a	O
n	O
(	O
0	O
,	O
1	O
)	O
prior	O
on	O
w1	O
.	O
show	O
that	O
this	O
can	O
be	O
written	O
as	O
a	O
gaussian	O
prior	O
of	O
the	O
form	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
w|w0	O
,	O
v0	O
)	O
.	O
what	O
are	O
w0	O
and	O
v0	O
?	O
c.	O
compute	O
the	O
marginal	O
posterior	O
of	O
the	O
slope	O
,	O
p	O
(	O
w1|d	O
,	O
σ2	O
)	O
,	O
where	O
d	O
is	O
the	O
data	O
above	O
,	O
and	O
σ2	O
is	O
the	O
w1|d	O
,	O
σ2	O
unbiased	B
estimate	O
computed	O
above	O
.	O
what	O
is	O
e	O
show	O
your	O
work	O
.	O
(	O
you	O
can	O
use	O
matlab	O
if	O
you	O
like	O
.	O
)	O
hint	O
:	O
the	O
posterior	O
variance	O
is	O
a	O
very	O
small	O
number	O
!	O
w1|d	O
,	O
σ2	O
and	O
var	O
(	O
cid:22	O
)	O
(	O
cid:23	O
)	O
(	O
cid:22	O
)	O
(	O
cid:23	O
)	O
d.	O
what	O
is	O
a	O
95	O
%	O
credible	B
interval	I
for	O
w1	O
?	O
exercise	O
7.9	O
generative	O
model	O
for	O
linear	B
regression	I
linear	O
regression	B
is	O
the	O
problem	O
of	O
estimating	O
e	O
[	O
y	O
|x	O
]	O
using	O
a	O
linear	O
function	O
of	O
the	O
form	O
w0	O
+	O
wt	O
x.	O
typically	O
we	O
assume	O
that	O
the	O
conditional	O
distribution	O
of	O
y	O
given	O
x	O
is	O
gaussian	O
.	O
we	O
can	O
either	O
estimate	O
this	O
conditional	O
gaussian	O
directly	O
(	O
a	O
discriminative	B
approach	O
)	O
,	O
or	O
we	O
can	O
ﬁt	O
a	O
gaussian	O
to	O
the	O
joint	B
distribution	I
of	O
x	O
,	O
y	O
and	O
then	O
derive	O
e	O
[	O
y	O
|x	O
=	O
x	O
]	O
.	O
in	O
exercise	O
7.5	O
we	O
showed	O
that	O
the	O
discriminative	B
approach	O
leads	O
to	O
these	O
equations	O
e	O
[	O
y	O
|x	O
]	O
=w	O
0	O
+	O
wt	O
x	O
w0	O
=	O
y	O
−	O
xt	O
w	O
w	O
=	O
(	O
xt	O
c	O
xc	O
)	O
−1xt	O
c	O
yc	O
(	O
7.109	O
)	O
(	O
7.110	O
)	O
(	O
7.111	O
)	O
7.6.	O
bayesian	O
linear	B
regression	I
243	O
where	O
xc	O
=	O
x	O
−	O
¯x	O
is	O
the	O
centered	O
input	O
matrix	O
,	O
and	O
¯x	O
=	O
1nxt	O
replicates	O
x	O
across	O
the	O
rows	O
.	O
similarly	O
,	O
yc	O
=	O
y	O
−	O
y	O
is	O
the	O
centered	O
output	O
vector	O
,	O
and	O
y	O
=	O
1ny	O
replicates	O
y	O
across	O
the	O
rows	O
.	O
a.	O
by	O
ﬁnding	O
the	O
maximum	O
likelihood	O
estimates	O
of	O
σxx	O
,	O
σxy	O
,	O
μx	O
and	O
μy	O
,	O
derive	O
the	O
above	O
equations	O
by	O
ﬁtting	O
a	O
joint	O
gaussian	O
to	O
x	O
,	O
y	O
and	O
using	O
the	O
formula	O
for	O
conditioning	B
a	O
gaussian	O
(	O
see	O
section	O
4.3.1	O
)	O
.	O
show	O
your	O
work	O
.	O
b.	O
what	O
are	O
the	O
advantages	O
and	O
disadvantages	O
of	O
this	O
approach	O
compared	O
to	O
the	O
standard	O
discriminative	O
approach	O
?	O
exercise	O
7.10	O
bayesian	O
linear	B
regression	I
using	O
the	O
g-prior	B
show	O
that	O
when	O
we	O
use	O
the	O
g-prior	B
,	O
p	O
(	O
w	O
,	O
σ2	O
)	O
=	O
nig	O
(	O
w	O
,	O
σ2|0	O
,	O
g	O
(	O
xt	O
x	O
)	O
following	O
form	O
:	O
−1	O
,	O
0	O
,	O
0	O
)	O
,	O
the	O
posterior	O
has	O
the	O
p	O
(	O
w	O
,	O
σ2|d	O
)	O
=	O
nig	O
(	O
w	O
,	O
σ2|wn	O
,	O
vn	O
,	O
an	O
,	O
bn	O
)	O
vn	O
=	O
g	O
g	O
+	O
1	O
g	O
−1	O
(	O
xt	O
x	O
)	O
ˆwmle	O
wn	O
=	O
g	O
+	O
1	O
an	O
=	O
n/2	O
bn	O
=	O
s2	O
2	O
+	O
1	O
2	O
(	O
g	O
+	O
1	O
)	O
ˆwt	O
mlext	O
x	O
ˆwmle	O
(	O
7.112	O
)	O
(	O
7.113	O
)	O
(	O
7.114	O
)	O
(	O
7.115	O
)	O
(	O
7.116	O
)	O
(	O
7.117	O
)	O
8	O
logistic	B
regression	I
8.1	O
introduction	O
one	O
way	O
to	O
build	O
a	O
probabilistic	O
classiﬁer	O
is	O
to	O
create	O
a	O
joint	O
model	O
of	O
the	O
form	O
p	O
(	O
y	O
,	O
x	O
)	O
and	O
then	O
to	O
condition	O
on	O
x	O
,	O
thereby	O
deriving	O
p	O
(	O
y|x	O
)	O
.	O
this	O
is	O
called	O
the	O
generative	B
approach	I
.	O
an	O
alternative	O
approach	O
is	O
to	O
ﬁt	O
a	O
model	O
of	O
the	O
form	O
p	O
(	O
y|x	O
)	O
directly	O
.	O
this	O
is	O
called	O
the	O
discrimi-	O
native	O
approach	O
,	O
and	O
is	O
the	O
approach	O
we	O
adopt	O
in	O
this	O
chapter	O
.	O
in	O
particular	O
,	O
we	O
will	O
assume	O
discriminative	B
models	O
which	O
are	O
linear	O
in	O
the	O
parameters	O
.	O
this	O
will	O
turn	O
out	O
to	O
signiﬁcantly	O
sim-	O
plify	O
model	O
ﬁtting	O
,	O
as	O
we	O
will	O
see	O
.	O
in	O
section	O
8.6	O
,	O
we	O
compare	O
the	O
generative	O
and	O
discriminative	B
approaches	O
,	O
and	O
in	O
later	O
chapters	O
,	O
we	O
will	O
consider	O
non-linear	O
and	O
non-parametric	O
discriminative	O
models	O
.	O
8.2	O
model	O
speciﬁcation	O
as	O
we	O
discussed	O
in	O
section	O
1.4.6	O
,	O
logistic	B
regression	I
corresponds	O
to	O
the	O
following	O
binary	O
classiﬁ-	O
cation	O
model	O
:	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
ber	O
(	O
y|sigm	O
(	O
wt	O
x	O
)	O
)	O
a	O
1d	O
example	O
is	O
shown	O
in	O
figure	O
1.19	O
(	O
b	O
)	O
.	O
logistic	B
regression	I
can	O
easily	O
be	O
extended	O
to	O
higher-	O
dimensional	O
inputs	O
.	O
for	O
example	O
,	O
figure	O
8.1	O
shows	O
plots	O
of	O
p	O
(	O
y	O
=	O
1|x	O
,	O
w	O
)	O
=	O
sigm	O
(	O
wt	O
x	O
)	O
for	O
2d	O
input	O
and	O
different	O
weight	O
vectors	O
w.	O
if	O
we	O
threshold	O
these	O
probabilities	O
at	O
0.5	O
,	O
we	O
induce	O
a	O
linear	O
decision	O
boundary	O
,	O
whose	O
normal	B
(	O
perpendicular	O
)	O
is	O
given	O
by	O
w.	O
(	O
8.1	O
)	O
8.3	O
model	O
ﬁtting	O
in	O
this	O
section	O
,	O
we	O
discuss	O
algorithms	O
for	O
estimating	O
the	O
parameters	O
of	O
a	O
logistic	B
regression	I
model	O
.	O
246	O
chapter	O
8.	O
logistic	B
regression	I
w	O
=	O
(	O
1	O
,	O
4	O
)	O
w	O
=	O
(	O
5	O
,	O
4	O
)	O
1	O
0.5	O
0	O
−10	O
w	O
=	O
(	O
0	O
,	O
2	O
)	O
0	O
x	O
1	O
10	O
−10	O
0	O
x	O
2	O
1	O
0.5	O
0	O
−10	O
w	O
=	O
(	O
−2	O
,	O
3	O
)	O
0	O
x	O
1	O
10	O
−10	O
0	O
x	O
2	O
w2	O
10	O
1	O
0.5	O
0	O
−10	O
w	O
=	O
(	O
−2	O
,	O
−1	O
)	O
10	O
0	O
x	O
1	O
10	O
−10	O
0	O
x	O
2	O
1	O
0.5	O
0	O
−10	O
1	O
0.5	O
0	O
−10	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
0	O
x	O
1	O
10	O
−10	O
0	O
x	O
2	O
10	O
w	O
=	O
(	O
2	O
,	O
2	O
)	O
1	O
0.5	O
0	O
−10	O
10	O
w	O
=	O
(	O
1	O
,	O
0	O
)	O
0	O
x	O
1	O
10	O
−10	O
0	O
x	O
2	O
10	O
w	O
=	O
(	O
3	O
,	O
0	O
)	O
1	O
0.5	O
0	O
x	O
1	O
10	O
−10	O
1	O
0.5	O
0	O
−10	O
10	O
0	O
−10	O
0	O
x	O
2	O
w	O
=	O
(	O
2	O
,	O
−2	O
)	O
10	O
0	O
x	O
1	O
10	O
−10	O
0	O
x	O
2	O
10	O
0	O
x	O
1	O
10	O
−10	O
0	O
x	O
2	O
1	O
0.5	O
0	O
−10	O
1	O
0.5	O
0	O
−10	O
10	O
10	O
0	O
x	O
1	O
10	O
−10	O
0	O
x	O
2	O
w	O
=	O
(	O
5	O
,	O
1	O
)	O
0	O
x	O
1	O
10	O
−10	O
0	O
x	O
2	O
w1	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
plots	O
of	O
sigm	O
(	O
w1x1	O
+	O
w2x2	O
)	O
.	O
here	O
w	O
=	O
(	O
w1	O
,	O
w2	O
)	O
deﬁnes	O
the	O
normal	B
to	O
the	O
decision	B
figure	O
8.1	O
boundary	O
.	O
points	O
to	O
the	O
right	O
of	O
this	O
have	O
sigm	O
(	O
wt	O
x	O
)	O
>	O
0.5	O
,	O
and	O
points	O
to	O
the	O
left	O
have	O
sigm	O
(	O
wt	O
x	O
)	O
<	O
0.5.	O
based	O
on	O
figure	O
39.3	O
of	O
(	O
mackay	O
2003	O
)	O
.	O
figure	O
generated	O
by	O
sigmoidplot2d	O
.	O
8.3.1	O
mle	O
the	O
negative	O
log-likelihood	O
for	O
logistic	B
regression	I
is	O
given	O
by	O
nll	O
(	O
w	O
)	O
=	O
−	O
n	O
(	O
cid:2	O
)	O
=	O
−	O
n	O
(	O
cid:2	O
)	O
i=1	O
i=1	O
log	O
[	O
μi	O
(	O
yi=1	O
)	O
i	O
×	O
(	O
1	O
−	O
μi	O
)	O
i	O
(	O
yi=0	O
)	O
]	O
[	O
yi	O
log	O
μi	O
+	O
(	O
1	O
−	O
yi	O
)	O
log	O
(	O
1	O
−	O
μi	O
)	O
]	O
(	O
8.2	O
)	O
(	O
8.3	O
)	O
this	O
is	O
also	O
called	O
the	O
cross-entropy	B
error	O
function	O
(	O
see	O
section	O
2.8.2	O
)	O
.	O
another	O
way	O
of	O
writing	O
this	O
is	O
as	O
follows	O
.	O
suppose	O
˜yi	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
instead	O
of	O
yi	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
we	O
1	O
1+exp	O
(	O
+wt	O
x	O
)	O
.	O
hence	O
have	O
p	O
(	O
y	O
=	O
1	O
)	O
=	O
1+exp	O
(	O
−wt	O
x	O
)	O
and	O
p	O
(	O
y	O
=	O
1	O
)	O
=	O
n	O
(	O
cid:2	O
)	O
1	O
log	O
(	O
1	O
+	O
exp	O
(	O
−˜yiwt	O
xi	O
)	O
)	O
n	O
ll	O
(	O
w	O
)	O
=	O
i=1	O
unlike	O
linear	B
regression	I
,	O
we	O
can	O
no	O
longer	O
write	O
down	O
the	O
mle	O
in	O
closed	O
form	O
.	O
instead	O
,	O
we	O
need	O
to	O
use	O
an	O
optimization	B
algorithm	O
to	O
compute	O
it	O
.	O
for	O
this	O
,	O
we	O
need	O
to	O
derive	O
the	O
gradient	O
and	O
hessian	O
.	O
in	O
the	O
case	O
of	O
logistic	B
regression	I
,	O
one	O
can	O
show	O
(	O
exercise	O
8.3	O
)	O
that	O
the	O
gradient	O
and	O
hessian	O
(	O
8.4	O
)	O
8.3.	O
model	O
ﬁtting	O
247	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
0	O
0.5	O
1	O
1.5	O
2	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
0	O
0.5	O
1	O
1.5	O
2	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
8.2	O
gradient	B
descent	I
on	O
a	O
simple	O
function	O
,	O
starting	O
from	O
(	O
0	O
,	O
0	O
)	O
,	O
for	O
20	O
steps	O
,	O
using	O
a	O
ﬁxed	O
learning	O
rate	B
(	O
step	B
size	I
)	O
η.	O
the	O
global	B
minimum	I
is	O
at	O
(	O
1	O
,	O
1	O
)	O
.	O
(	O
a	O
)	O
η	O
=	O
0.1	O
.	O
(	O
b	O
)	O
η	O
=	O
0.6.	O
figure	O
generated	O
by	O
steepestdescentdemo	O
.	O
of	O
this	O
are	O
given	O
by	O
the	O
following	O
g	O
=	O
h	O
=	O
d	O
dw	O
d	O
dw	O
f	O
(	O
w	O
)	O
=	O
g	O
(	O
w	O
)	O
t	O
=	O
(	O
cid:2	O
)	O
(	O
μi	O
−	O
yi	O
)	O
xi	O
=	O
xt	O
(	O
μ	O
−	O
y	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
(	O
∇wμi	O
)	O
xt	O
i	O
=	O
i	O
i	O
μi	O
(	O
1	O
−	O
μi	O
)	O
xixt	O
i	O
(	O
8.5	O
)	O
(	O
8.6	O
)	O
=	O
xt	O
sx	O
(	O
8.7	O
)	O
where	O
s	O
(	O
cid:2	O
)	O
diag	O
(	O
μi	O
(	O
1	O
−	O
μi	O
)	O
)	O
.	O
one	O
can	O
also	O
show	O
(	O
exercise	O
8.3	O
)	O
that	O
h	O
is	O
positive	B
deﬁnite	I
.	O
hence	O
the	O
nll	O
is	O
convex	B
and	O
has	O
a	O
unique	O
global	B
minimum	I
.	O
below	O
we	O
discuss	O
some	O
methods	O
for	O
ﬁnding	O
this	O
minimum	O
.	O
8.3.2	O
steepest	B
descent	I
perhaps	O
the	O
simplest	O
algorithm	O
for	O
unconstrained	O
optimization	B
is	O
gradient	B
descent	I
,	O
also	O
known	O
as	O
steepest	B
descent	I
.	O
this	O
can	O
be	O
written	O
as	O
follows	O
:	O
θk+1	O
=	O
θk	O
−	O
ηkgk	O
(	O
8.8	O
)	O
where	O
ηk	O
is	O
the	O
step	B
size	I
or	O
learning	B
rate	I
.	O
the	O
main	O
issue	O
in	O
gradient	B
descent	I
is	O
:	O
how	O
should	O
we	O
set	O
the	O
step	B
size	I
?	O
this	O
turns	O
out	O
to	O
be	O
quite	O
tricky	O
.	O
if	O
we	O
use	O
a	O
constant	O
learning	O
rate	B
,	O
but	O
make	O
it	O
too	O
small	O
,	O
convergence	O
will	O
be	O
very	O
slow	O
,	O
but	O
if	O
we	O
make	O
it	O
too	O
large	O
,	O
the	O
method	O
can	O
fail	O
to	O
converge	B
at	O
all	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
8.2.	O
where	O
we	O
plot	O
the	O
following	O
(	O
convex	B
)	O
function	O
f	O
(	O
θ	O
)	O
=	O
0.5	O
(	O
θ2	O
1	O
−	O
θ2	O
)	O
2	O
+	O
0.5	O
(	O
θ1	O
−	O
1	O
)	O
2	O
,	O
(	O
8.9	O
)	O
we	O
arbitrarily	O
decide	O
to	O
start	O
from	O
(	O
0	O
,	O
0	O
)	O
.	O
in	O
figure	O
8.2	O
(	O
a	O
)	O
,	O
we	O
use	O
a	O
ﬁxed	O
step	O
size	O
of	O
η	O
=	O
0.1	O
;	O
we	O
see	O
that	O
it	O
moves	O
slowly	O
along	O
the	O
valley	O
.	O
in	O
figure	O
8.2	O
(	O
b	O
)	O
,	O
we	O
use	O
a	O
ﬁxed	O
step	O
size	O
of	O
η	O
=	O
0.6	O
;	O
we	O
see	O
that	O
the	O
algorithm	O
starts	O
oscillating	O
up	O
and	O
down	O
the	O
sides	O
of	O
the	O
valley	O
and	O
never	O
converges	O
to	O
the	O
optimum	O
.	O
248	O
chapter	O
8.	O
logistic	B
regression	I
exact	O
line	O
searching	O
1	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
0	O
0.5	O
1.5	O
2	O
1	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
8.3	O
(	O
a	O
)	O
steepest	B
descent	I
on	O
the	O
same	O
function	O
as	O
figure	O
8.2	O
,	O
starting	O
from	O
(	O
0	O
,	O
0	O
)	O
,	O
using	O
line	B
search	I
.	O
figure	O
generated	O
by	O
steepestdescentdemo	O
.	O
(	O
b	O
)	O
illustration	O
of	O
the	O
fact	O
that	O
at	O
the	O
end	O
of	O
a	O
line	B
search	I
(	O
top	O
of	O
picture	O
)	O
,	O
the	O
local	O
gradient	O
of	O
the	O
function	O
will	O
be	O
perpendicular	O
to	O
the	O
search	O
direction	O
.	O
based	O
on	O
figure	O
10.6.1	O
of	O
(	O
press	O
et	O
al	O
.	O
1988	O
)	O
.	O
let	O
us	O
develop	O
a	O
more	O
stable	B
method	O
for	O
picking	O
the	O
step	B
size	I
,	O
so	O
that	O
the	O
method	O
is	O
guaran-	O
(	O
this	O
property	O
is	O
called	O
global	O
teed	O
to	O
converge	B
to	O
a	O
local	O
optimum	O
no	O
matter	O
where	O
we	O
start	O
.	O
convergence	O
,	O
which	O
should	O
not	O
be	O
confused	O
with	O
convergence	O
to	O
the	O
global	O
optimum	O
!	O
)	O
by	O
taylor	O
’	O
s	O
theorem	O
,	O
we	O
have	O
f	O
(	O
θ	O
+	O
ηd	O
)	O
≈	O
f	O
(	O
θ	O
)	O
+η	O
gt	O
d	O
(	O
8.10	O
)	O
where	O
d	O
is	O
our	O
descent	O
direction	O
.	O
so	O
if	O
η	O
is	O
chosen	O
small	O
enough	O
,	O
then	O
f	O
(	O
θ	O
+	O
ηd	O
)	O
<	O
f	O
(	O
θ	O
)	O
,	O
since	O
the	O
gradient	O
will	O
be	O
negative	O
.	O
but	O
we	O
don	O
’	O
t	O
want	O
to	O
choose	O
the	O
step	B
size	I
η	O
too	O
small	O
,	O
or	O
we	O
will	O
move	O
very	O
slowly	O
and	O
may	O
not	O
reach	O
the	O
minimum	O
.	O
so	O
let	O
us	O
pick	O
η	O
to	O
minimize	O
φ	O
(	O
η	O
)	O
=	O
f	O
(	O
θk	O
+	O
ηdk	O
)	O
(	O
8.11	O
)	O
this	O
is	O
called	O
line	B
minimization	I
or	O
line	B
search	I
.	O
there	O
are	O
various	O
methods	O
for	O
solving	O
this	O
1d	O
optimization	B
problem	O
;	O
see	O
(	O
nocedal	O
and	O
wright	O
2006	O
)	O
for	O
details	O
.	O
figure	O
8.3	O
(	O
a	O
)	O
demonstrates	O
that	O
line	B
search	I
does	O
indeed	O
work	O
for	O
our	O
simple	O
problem	O
.	O
however	O
,	O
we	O
see	O
that	O
the	O
steepest	B
descent	I
path	O
with	O
exact	O
line	O
searches	O
exhibits	O
a	O
characteristic	O
zig-zag	O
behavior	O
.	O
to	O
see	O
why	O
,	O
note	O
that	O
an	O
exact	O
line	B
search	I
satisﬁes	O
ηk	O
=	O
arg	O
minη	O
>	O
0	O
φ	O
(	O
η	O
)	O
.	O
a	O
necessary	O
condition	O
for	O
the	O
optimum	O
is	O
φ	O
(	O
cid:4	O
)	O
(	O
η	O
)	O
=d	O
t	O
g	O
,	O
where	O
g	O
=	O
f	O
(	O
cid:4	O
)	O
(	O
θ	O
+	O
ηd	O
)	O
is	O
the	O
gradient	O
at	O
the	O
end	O
of	O
the	O
step	O
.	O
so	O
we	O
either	O
have	O
g	O
=	O
0	O
,	O
which	O
means	O
we	O
have	O
found	O
a	O
stationary	B
point	O
,	O
or	O
g	O
⊥	O
d	O
,	O
which	O
means	O
that	O
exact	O
search	O
stops	O
at	O
a	O
point	O
where	O
the	O
local	O
gradient	O
is	O
perpendicular	O
to	O
the	O
search	O
direction	O
.	O
hence	O
consecutive	O
directions	O
will	O
be	O
orthogonal	O
(	O
see	O
figure	O
8.3	O
(	O
b	O
)	O
)	O
.	O
this	O
explains	O
the	O
zig-zag	B
behavior	O
.	O
one	O
simple	O
heuristic	O
to	O
reduce	O
the	O
effect	O
of	O
zig-zagging	O
is	O
to	O
add	O
a	O
momentum	B
term	O
,	O
(	O
θk	O
−	O
(	O
η	O
)	O
=	O
0.	O
by	O
the	O
chain	B
rule	I
,	O
φ	O
(	O
cid:4	O
)	O
θk−1	O
)	O
,	O
as	O
follows	O
:	O
θk+1	O
=	O
θk	O
−	O
ηkgk	O
+	O
μk	O
(	O
θk	O
−	O
θk−1	O
)	O
(	O
8.12	O
)	O
8.3.	O
model	O
ﬁtting	O
249	O
where	O
0	O
≤	O
μk	O
≤	O
1	O
controls	O
the	O
importance	O
of	O
the	O
momentum	B
term	O
.	O
community	O
,	O
this	O
is	O
known	O
as	O
the	O
heavy	B
ball	I
method	I
(	O
see	O
e.g.	O
,	O
(	O
bertsekas	O
1999	O
)	O
)	O
.	O
in	O
the	O
optimization	B
an	O
alternative	O
way	O
to	O
minimize	O
“	O
zig-zagging	O
”	O
is	O
to	O
use	O
the	O
method	O
of	O
conjugate	B
gradients	I
(	O
see	O
e.g.	O
,	O
(	O
nocedal	O
and	O
wright	O
2006	O
,	O
ch	O
5	O
)	O
or	O
(	O
golub	O
and	O
van	O
loan	O
1996	O
,	O
sec	O
10.2	O
)	O
)	O
.	O
this	O
is	O
the	O
method	O
of	O
choice	O
for	O
quadratic	O
objectives	O
of	O
the	O
form	O
f	O
(	O
θ	O
)	O
=	O
θt	O
aθ	O
,	O
which	O
arise	O
when	O
solving	O
linear	O
systems	O
.	O
however	O
,	O
non-linear	O
cg	O
is	O
less	O
popular	O
.	O
8.3.3	O
newton	O
’	O
s	O
method	O
algorithm	O
8.1	O
:	O
newton	O
’	O
s	O
method	O
for	O
minimizing	O
a	O
strictly	B
convex	I
function	O
1	O
initialize	O
θ0	O
;	O
2	O
for	O
k	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
until	O
convergence	O
do	O
3	O
evaluate	O
gk	O
=	O
∇f	O
(	O
θk	O
)	O
;	O
evaluate	O
hk	O
=	O
∇2f	O
(	O
θk	O
)	O
;	O
solve	O
hkdk	O
=	O
−gk	O
for	O
dk	O
;	O
use	O
line	B
search	I
to	O
ﬁnd	O
stepsize	O
ηk	O
along	O
dk	O
;	O
θk+1	O
=	O
θk	O
+	O
ηkdk	O
;	O
4	O
5	O
6	O
7	O
one	O
can	O
derive	O
faster	O
optimization	B
methods	O
by	O
taking	O
the	O
curvature	O
of	O
the	O
space	O
(	O
i.e.	O
,	O
the	O
into	O
account	O
.	O
these	O
are	O
called	O
second	B
order	I
optimization	O
metods	O
.	O
the	O
primary	O
hessian	O
)	O
example	O
is	O
newton	O
’	O
s	O
algorithm	O
.	O
this	O
is	O
an	O
iterative	O
algorithm	O
which	O
consists	O
of	O
updates	O
of	O
the	O
form	O
θk+1	O
=	O
θk	O
−	O
ηkh−1	O
k	O
gk	O
(	O
8.13	O
)	O
(	O
8.14	O
)	O
(	O
8.15	O
)	O
(	O
8.16	O
)	O
(	O
8.17	O
)	O
the	O
full	B
pseudo-code	O
is	O
given	O
in	O
algorithm	O
2.	O
this	O
algorithm	O
can	O
be	O
derived	O
as	O
follows	O
.	O
consider	O
making	O
a	O
second-order	O
taylor	O
series	O
approximation	O
of	O
f	O
(	O
θ	O
)	O
around	O
θk	O
:	O
k	O
(	O
θ	O
−	O
θk	O
)	O
+	O
fquad	O
(	O
θ	O
)	O
=	O
fk	O
+	O
gt	O
(	O
θ	O
−	O
θk	O
)	O
t	O
hk	O
(	O
θ	O
−	O
θk	O
)	O
1	O
2	O
let	O
us	O
rewrite	O
this	O
as	O
fquad	O
(	O
θ	O
)	O
=	O
θt	O
aθ	O
+	O
bt	O
θ	O
+	O
c	O
where	O
a	O
=	O
1	O
2	O
hk	O
,	O
b	O
=	O
gk	O
−	O
hkθk	O
,	O
c	O
=	O
fk	O
−	O
gt	O
k	O
θk	O
+	O
θt	O
k	O
hkθk	O
1	O
2	O
the	O
minimum	O
of	O
fquad	O
is	O
at	O
θ	O
=	O
−	O
1	O
2	O
a−1b	O
=	O
θk	O
−	O
h−1	O
k	O
gk	O
thus	O
the	O
newton	O
step	O
dk	O
=	O
−h−1	O
order	O
approximation	O
of	O
f	O
around	O
θk	O
.	O
see	O
figure	O
8.4	O
(	O
a	O
)	O
for	O
an	O
illustration	O
.	O
k	O
gk	O
is	O
what	O
should	O
be	O
added	O
to	O
θk	O
to	O
minimize	O
the	O
second	O
250	O
chapter	O
8.	O
logistic	B
regression	I
f	O
(	O
x	O
)	O
f	O
quad	O
(	O
x	O
)	O
f	O
(	O
x	O
)	O
f	O
quad	O
(	O
x	O
)	O
x	O
k	O
x	O
+d	O
k	O
k	O
(	O
a	O
)	O
x	O
k	O
x	O
+d	O
k	O
k	O
(	O
b	O
)	O
illustration	O
of	O
newton	O
’	O
s	O
method	O
for	O
minimizing	O
a	O
1d	O
function	O
.	O
figure	O
8.4	O
(	O
a	O
)	O
the	O
solid	O
curve	O
is	O
the	O
function	O
f	O
(	O
x	O
)	O
.	O
the	O
dotted	O
line	O
fquad	O
(	O
x	O
)	O
is	O
its	O
second	B
order	I
approximation	O
at	O
xk	O
.	O
the	O
newton	O
step	O
dk	O
is	O
what	O
must	O
be	O
added	O
to	O
xk	O
to	O
get	O
to	O
the	O
minimum	O
of	O
fquad	O
(	O
x	O
)	O
.	O
based	O
on	O
figure	O
13.4	O
of	O
(	O
vandenberghe	O
2006	O
)	O
.	O
figure	O
generated	O
by	O
newtonsmethodminquad	O
.	O
(	O
b	O
)	O
illustration	O
of	O
newton	O
’	O
s	O
method	O
applied	O
to	O
a	O
nonconvex	O
function	O
.	O
we	O
ﬁt	O
a	O
quadratic	O
around	O
the	O
current	O
point	O
xk	O
and	O
move	O
to	O
its	O
stationary	B
point	O
,	O
xk+1	O
=	O
xk	O
+	O
dk	O
.	O
unfortunately	O
,	O
this	O
is	O
a	O
local	O
maximum	O
,	O
not	O
minimum	O
.	O
this	O
means	O
we	O
need	O
to	O
be	O
careful	O
about	O
the	O
extent	O
of	O
our	O
quadratic	O
approximation	O
.	O
based	O
on	O
figure	O
13.11	O
of	O
(	O
vandenberghe	O
2006	O
)	O
.	O
figure	O
generated	O
by	O
newtonsmethodnonconvex	O
.	O
in	O
its	O
simplest	O
form	O
(	O
as	O
listed	O
)	O
,	O
newton	O
’	O
s	O
method	O
requires	O
that	O
hk	O
be	O
positive	B
deﬁnite	I
,	O
which	O
will	O
hold	O
if	O
the	O
function	O
is	O
strictly	B
convex	I
.	O
if	O
not	O
,	O
the	O
objective	O
function	O
is	O
not	O
convex	O
,	O
then	O
hk	O
may	O
not	O
be	O
positive	B
deﬁnite	I
,	O
so	O
dk	O
=	O
−h−1	O
k	O
gk	O
may	O
not	O
be	O
a	O
descent	O
direction	O
(	O
see	O
figure	O
8.4	O
(	O
b	O
)	O
for	O
an	O
example	O
)	O
.	O
in	O
this	O
case	O
,	O
one	O
simple	O
strategy	O
is	O
to	O
revert	O
to	O
steepest	B
descent	I
,	O
dk	O
=	O
−gk	O
.	O
the	O
levenberg	O
marquardt	O
algorithm	O
is	O
an	O
adaptive	O
way	O
to	O
blend	O
between	O
newton	O
steps	O
and	O
steepest	B
descent	I
steps	O
.	O
this	O
method	O
is	O
widely	O
used	O
when	O
solving	O
nonlinear	O
least	B
squares	I
problems	O
.	O
an	O
alternative	O
approach	O
is	O
this	O
:	O
rather	O
than	O
computing	O
dk	O
=	O
−h−1	O
k	O
gk	O
directly	O
,	O
we	O
can	O
solve	O
the	O
linear	O
system	O
of	O
equations	O
hkdk	O
=	O
−gk	O
for	O
dk	O
using	O
conjugate	O
gradient	O
(	O
cg	O
)	O
.	O
if	O
hk	O
is	O
not	O
positive	O
deﬁnite	O
,	O
we	O
can	O
simply	O
truncate	O
the	O
cg	O
iterations	O
as	O
soon	O
as	O
negative	O
curvature	O
is	O
detected	O
;	O
this	O
is	O
called	O
truncated	O
newton	O
.	O
8.3.4	O
iteratively	B
reweighted	I
least	I
squares	I
(	O
irls	O
)	O
let	O
us	O
now	O
apply	O
newton	O
’	O
s	O
algorithm	O
to	O
ﬁnd	O
the	O
mle	O
for	O
binary	O
logistic	O
regression	B
.	O
the	O
newton	O
update	O
at	O
iteration	O
k	O
+	O
1	O
for	O
this	O
model	O
is	O
as	O
follows	O
(	O
using	O
ηk	O
=	O
1	O
,	O
since	O
the	O
hessian	O
is	O
exact	O
)	O
:	O
(	O
8.18	O
)	O
(	O
8.19	O
)	O
(	O
8.20	O
)	O
(	O
8.21	O
)	O
(	O
8.22	O
)	O
(	O
8.23	O
)	O
wk+1	O
=	O
wk	O
−	O
h−1gk	O
(	O
cid:31	O
)	O
=	O
wk	O
+	O
(	O
xt	O
skx	O
)	O
=	O
(	O
xt	O
skx	O
)	O
=	O
(	O
xt	O
skx	O
)	O
=	O
(	O
xt	O
skx	O
)	O
−1xt	O
(	O
y	O
−	O
μk	O
)	O
(	O
xt	O
skx	O
)	O
wk	O
+	O
xt	O
(	O
y	O
−	O
μk	O
)	O
−1	O
−1xt	O
[	O
skxwk	O
+	O
y	O
−	O
μk	O
]	O
−1xt	O
skzk	O
where	O
we	O
have	O
deﬁned	O
the	O
working	B
response	I
as	O
zk	O
(	O
cid:2	O
)	O
xwk	O
+	O
s−1	O
k	O
(	O
y	O
−	O
μk	O
)	O
8.3.	O
model	O
ﬁtting	O
251	O
equation	O
8.22	O
is	O
an	O
example	O
of	O
a	O
weighted	B
least	I
squares	I
problem	I
,	O
which	O
is	O
a	O
minimizer	O
of	O
ski	O
(	O
zki	O
−	O
wt	O
xi	O
)	O
2	O
(	O
8.24	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
since	O
sk	O
is	O
a	O
diagonal	B
matrix	O
,	O
we	O
can	O
rewrite	O
the	O
targets	O
in	O
component	O
form	O
(	O
for	O
each	O
case	O
i	O
=	O
1	O
:	O
n	O
)	O
as	O
zki	O
=	O
wt	O
k	O
xi	O
+	O
yi	O
−	O
μki	O
μki	O
(	O
1	O
−	O
μki	O
)	O
(	O
8.25	O
)	O
this	O
algorithm	O
is	O
known	O
as	O
iteratively	B
reweighted	I
least	I
squares	I
or	O
irls	O
for	O
short	O
,	O
since	O
at	O
each	O
iteration	O
,	O
we	O
solve	O
a	O
weighted	B
least	I
squares	I
problem	I
,	O
where	O
the	O
weight	O
matrix	O
sk	O
changes	O
at	O
each	O
iteration	O
.	O
see	O
algorithm	O
10	O
for	O
some	O
pseudocode	O
.	O
algorithm	O
8.2	O
:	O
iteratively	B
reweighted	I
least	I
squares	I
(	O
irls	O
)	O
1	O
w	O
=	O
0d	O
;	O
2	O
w0	O
=	O
log	O
(	O
y/	O
(	O
1	O
−	O
y	O
)	O
)	O
;	O
3	O
repeat	O
4	O
ηi	O
=	O
w0	O
+	O
wt	O
xi	O
;	O
μi	O
=	O
sigm	O
(	O
ηi	O
)	O
;	O
si	O
=	O
μi	O
(	O
1	O
−	O
μi	O
)	O
;	O
zi	O
=	O
ηi	O
+	O
yi−μi	O
;	O
s	O
=	O
diag	O
(	O
s1	O
:	O
n	O
)	O
;	O
w	O
=	O
(	O
xt	O
sx	O
)	O
si	O
−1xt	O
sz	O
;	O
5	O
6	O
7	O
8	O
9	O
10	O
until	O
converged	O
;	O
8.3.5	O
quasi-newton	O
(	O
variable	O
metric	O
)	O
methods	O
the	O
mother	O
of	O
all	O
second-order	O
optimization	B
algorithm	O
is	O
newton	O
’	O
s	O
algorithm	O
,	O
which	O
we	O
dis-	O
cussed	O
in	O
section	O
8.3.3.	O
unfortunately	O
,	O
it	O
may	O
be	O
too	O
expensive	O
to	O
compute	O
h	O
explicitly	O
.	O
quasi-	O
newton	O
methods	O
iteratively	O
build	O
up	O
an	O
approximation	O
to	O
the	O
hessian	O
using	O
information	B
gleaned	O
from	O
the	O
gradient	O
vector	O
at	O
each	O
step	O
.	O
the	O
most	O
common	O
method	O
is	O
called	O
bfgs	O
(	O
named	O
after	O
its	O
inventors	O
,	O
broyden	O
,	O
fletcher	O
,	O
goldfarb	O
and	O
shanno	O
)	O
,	O
which	O
updates	O
the	O
approximation	O
to	O
the	O
hessian	O
bk	O
≈	O
hk	O
as	O
follows	O
:	O
ykyt	O
k	O
yt	O
k	O
sk	O
sk	O
=	O
θk	O
−	O
θk−1	O
yk	O
=	O
gk	O
−	O
gk−1	O
−	O
(	O
bksk	O
)	O
(	O
bksk	O
)	O
t	O
bk+1	O
=	O
bk	O
+	O
st	O
k	O
bksk	O
(	O
8.26	O
)	O
(	O
8.27	O
)	O
(	O
8.28	O
)	O
this	O
is	O
a	O
rank-two	O
update	O
to	O
the	O
matrix	O
,	O
and	O
ensures	O
that	O
the	O
matrix	O
remains	O
positive	B
deﬁnite	I
(	O
under	O
certain	O
restrictions	O
on	O
the	O
step	B
size	I
)	O
.	O
we	O
typically	O
start	O
with	O
a	O
diagonal	B
approximation	O
,	O
b0	O
=	O
i.	O
thus	O
bfgs	O
can	O
be	O
thought	O
of	O
as	O
a	O
“	O
diagonal	B
plus	O
low-rank	O
”	O
approximation	O
to	O
the	O
hessian	O
.	O
252	O
chapter	O
8.	O
logistic	B
regression	I
alternatively	O
,	O
bfgs	O
can	O
iteratively	O
update	O
an	O
approximation	O
to	O
the	O
inverse	O
hessian	O
,	O
ck	O
≈	O
h−1	O
k	O
,	O
as	O
follows	O
:	O
ck+1	O
=	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
i	O
−	O
skyt	O
k	O
yt	O
k	O
sk	O
ck	O
i	O
−	O
ykst	O
k	O
yt	O
k	O
sk	O
+	O
skst	O
k	O
yt	O
k	O
sk	O
(	O
8.29	O
)	O
since	O
storing	O
the	O
hessian	O
takes	O
o	O
(	O
d2	O
)	O
space	O
,	O
for	O
very	O
large	O
problems	O
,	O
one	O
can	O
use	O
limited	O
memory	O
bfgs	O
,	O
orl-bfgs	O
,	O
where	O
hk	O
or	O
h−1	O
is	O
approximated	O
by	O
a	O
diagonal	B
plus	O
low	O
rank	O
matrix	O
.	O
in	O
particular	O
,	O
the	O
product	O
h−1	O
k	O
gk	O
can	O
be	O
obtained	O
by	O
performing	O
a	O
sequence	O
of	O
inner	O
products	O
with	O
sk	O
and	O
yk	O
,	O
using	O
only	O
the	O
m	O
most	O
recent	O
(	O
sk	O
,	O
yk	O
)	O
pairs	O
,	O
and	O
ignoring	O
older	O
information	B
.	O
the	O
storage	O
requirements	O
are	O
therefore	O
o	O
(	O
md	O
)	O
.	O
typically	O
m	O
∼	O
20	O
suffices	O
for	O
good	O
performance	O
.	O
see	O
(	O
nocedal	O
and	O
wright	O
2006	O
,	O
p177	O
)	O
for	O
more	O
information	B
.	O
l-bfgs	O
is	O
often	O
the	O
method	O
of	O
choice	O
for	O
most	O
unconstrained	O
smooth	O
optimization	B
problems	O
that	O
arise	O
in	O
machine	B
learning	I
(	O
although	O
see	O
section	O
8.5	O
)	O
.	O
k	O
8.3.6	O
(	O
cid:9	O
)	O
2	O
regularization	B
just	O
as	O
we	O
prefer	O
ridge	B
regression	I
to	O
linear	B
regression	I
,	O
so	O
we	O
should	O
prefer	O
map	O
estimation	O
for	O
logistic	B
regression	I
to	O
computing	O
the	O
mle	O
.	O
in	O
fact	O
,	O
regularization	B
is	O
important	O
in	O
the	O
classiﬁcation	B
setting	O
even	O
if	O
we	O
have	O
lots	O
of	O
data	O
.	O
to	O
see	O
why	O
,	O
suppose	O
the	O
data	O
is	O
linearly	B
separable	I
.	O
in	O
this	O
case	O
,	O
the	O
mle	O
is	O
obtained	O
when	O
||w||	O
→	O
∞	O
,	O
corresponding	O
to	O
an	O
inﬁnitely	O
steep	O
sigmoid	B
function	O
,	O
i	O
(	O
wt	O
x	O
>	O
w0	O
)	O
,	O
also	O
known	O
as	O
a	O
linear	B
threshold	I
unit	I
.	O
this	O
assigns	O
the	O
maximal	O
amount	O
of	O
probability	O
mass	O
to	O
the	O
training	O
data	O
.	O
however	O
,	O
such	O
a	O
solution	O
is	O
very	O
brittle	O
and	O
will	O
not	O
generalize	O
well	O
.	O
to	O
prevent	O
this	O
,	O
we	O
can	O
use	O
(	O
cid:6	O
)	O
2	O
regularization	B
,	O
just	O
as	O
we	O
did	O
with	O
ridge	B
regression	I
.	O
we	O
note	O
that	O
the	O
new	O
objective	O
,	O
gradient	O
and	O
hessian	O
have	O
the	O
following	O
forms	O
:	O
f	O
(	O
cid:4	O
)	O
g	O
(	O
cid:4	O
)	O
h	O
(	O
cid:4	O
)	O
(	O
w	O
)	O
=	O
nll	O
(	O
w	O
)	O
+λw	O
t	O
w	O
(	O
w	O
)	O
=g	O
(	O
w	O
)	O
+λw	O
(	O
w	O
)	O
=h	O
(	O
w	O
)	O
+λi	O
(	O
8.30	O
)	O
(	O
8.31	O
)	O
(	O
8.32	O
)	O
it	O
is	O
a	O
simple	O
matter	O
to	O
pass	O
these	O
modiﬁed	O
equations	O
into	O
any	O
gradient-based	O
optimizer	O
.	O
8.3.7	O
multi-class	B
logistic	I
regression	I
now	O
we	O
consider	O
multinomial	B
logistic	I
regression	I
,	O
sometimes	O
called	O
a	O
maximum	B
entropy	I
classiﬁer	I
.	O
this	O
is	O
a	O
model	O
of	O
the	O
form	O
p	O
(	O
y	O
=	O
c|x	O
,	O
w	O
)	O
=	O
(	O
cid:10	O
)	O
c	O
exp	O
(	O
wt	O
c	O
x	O
)	O
c	O
(	O
cid:2	O
)	O
=1	O
exp	O
(	O
wt	O
c	O
(	O
cid:2	O
)	O
x	O
)	O
(	O
8.33	O
)	O
a	O
slight	O
variant	O
,	O
known	O
as	O
a	O
conditional	B
logit	I
model	I
,	O
normalizes	O
over	O
a	O
different	O
set	O
of	O
classes	O
for	O
each	O
data	O
case	O
;	O
this	O
can	O
be	O
useful	O
for	O
modeling	O
choices	O
that	O
users	O
make	O
between	O
different	O
sets	O
of	O
items	O
that	O
are	O
offered	O
to	O
them	O
.	O
let	O
us	O
now	O
introduce	O
some	O
notation	O
.	O
let	O
μic	O
=	O
p	O
(	O
yi	O
=	O
c|xi	O
,	O
w	O
)	O
=	O
s	O
(	O
ηi	O
)	O
c	O
,	O
where	O
ηi	O
=	O
wt	O
xi	O
is	O
a	O
c	O
×	O
1	O
vector	O
.	O
also	O
,	O
let	O
yic	O
=	O
i	O
(	O
yi	O
=	O
c	O
)	O
be	O
the	O
one-of-c	O
encoding	O
of	O
yi	O
;	O
thus	O
yi	O
is	O
a	O
bit	O
vector	O
,	O
in	O
which	O
the	O
c	O
’	O
th	O
bit	O
turns	O
on	O
iff	B
yi	O
=	O
c.	O
following	O
(	O
krishnapuram	O
et	O
al	O
.	O
2005	O
)	O
,	O
let	O
us	O
8.3.	O
model	O
ﬁtting	O
253	O
set	O
wc	O
=	O
0	O
,	O
to	O
ensure	O
identiﬁability	O
,	O
and	O
deﬁne	O
w	O
=	O
vec	O
(	O
w	O
(	O
:	O
,	O
1	O
:	O
c	O
−	O
1	O
)	O
)	O
to	O
be	O
a	O
d×	O
(	O
c	O
−	O
1	O
)	O
column	O
vector	O
.	O
with	O
this	O
,	O
the	O
log-likelihood	O
can	O
be	O
written	O
as	O
n	O
(	O
cid:27	O
)	O
(	O
cid:18	O
)	O
(	O
cid:11	O
)	O
i=1	O
c	O
(	O
cid:27	O
)	O
c	O
(	O
cid:2	O
)	O
c=1	O
μyic	O
ic	O
=	O
c	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:13	O
)	O
i=1	O
yic	O
log	O
μic	O
(	O
cid:11	O
)	O
c	O
(	O
cid:2	O
)	O
c=1	O
−	O
log	O
c	O
(	O
cid:2	O
)	O
=1	O
yicwt	O
c	O
xi	O
exp	O
(	O
wt	O
c	O
(	O
cid:2	O
)	O
xi	O
)	O
(	O
cid:6	O
)	O
(	O
w	O
)	O
=	O
log	O
n	O
(	O
cid:2	O
)	O
=	O
i=1	O
c=1	O
deﬁne	O
the	O
nll	O
as	O
f	O
(	O
w	O
)	O
=	O
−	O
(	O
cid:6	O
)	O
(	O
w	O
)	O
(	O
cid:13	O
)	O
(	O
cid:19	O
)	O
(	O
8.34	O
)	O
(	O
8.35	O
)	O
(	O
8.36	O
)	O
we	O
now	O
proceed	O
to	O
compute	O
the	O
gradient	O
and	O
hessian	O
of	O
this	O
expression	O
.	O
since	O
w	O
is	O
block-	O
it	O
helps	O
to	O
deﬁne	O
a	O
⊗	O
b	O
structured	O
,	O
the	O
notation	O
gets	O
a	O
bit	O
heavy	O
,	O
but	O
the	O
ideas	O
are	O
simple	O
.	O
be	O
the	O
kronecker	B
product	I
of	O
matrices	O
a	O
and	O
b.	O
if	O
a	O
is	O
an	O
m	O
×	O
n	O
matrix	O
and	O
b	O
is	O
a	O
p	O
×	O
q	O
matrix	O
,	O
then	O
a	O
×	O
b	O
is	O
the	O
mp	O
×	O
nq	O
block	O
matrix	O
⎡	O
⎢⎣	O
a11b	O
···	O
.	O
.	O
.	O
am1b	O
···	O
...	O
⎤	O
⎥⎦	O
a1nb	O
...	O
amnb	O
a	O
⊗	O
b	O
=	O
(	O
8.37	O
)	O
(	O
8.39	O
)	O
(	O
8.40	O
)	O
n	O
(	O
cid:2	O
)	O
returning	O
to	O
the	O
task	O
at	O
hand	O
,	O
one	O
can	O
show	O
(	O
exercise	O
8.4	O
)	O
that	O
the	O
gradient	O
is	O
given	O
by	O
g	O
(	O
w	O
)	O
=	O
∇f	O
(	O
w	O
)	O
=	O
(	O
μi	O
−	O
yi	O
)	O
⊗	O
xi	O
(	O
8.38	O
)	O
where	O
yi	O
=	O
(	O
i	O
(	O
yi	O
=	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
i	O
(	O
yi	O
=	O
c	O
−	O
1	O
)	O
)	O
and	O
μi	O
(	O
w	O
)	O
=	O
[	O
p	O
(	O
yi	O
=	O
1|xi	O
,	O
w	O
)	O
,	O
.	O
.	O
.	O
,	O
p	O
(	O
yi	O
=	O
c	O
−	O
1|xi	O
,	O
w	O
)	O
]	O
are	O
column	O
vectors	O
of	O
length	O
c	O
−	O
1	O
,	O
for	O
example	O
,	O
if	O
we	O
have	O
d	O
=	O
3	O
feature	O
dimensions	O
and	O
c	O
=	O
3	O
classes	O
,	O
this	O
becomes	O
i=1	O
(	O
cid:2	O
)	O
i	O
g	O
(	O
w	O
)	O
=	O
⎞	O
⎟⎟⎟⎟⎟⎟⎠	O
⎛	O
⎜⎜⎜⎜⎜⎜⎝	O
(	O
μi1	O
−	O
yi1	O
)	O
xi1	O
(	O
μi1	O
−	O
yi1	O
)	O
xi2	O
(	O
μi1	O
−	O
yi1	O
)	O
xi3	O
(	O
μi2	O
−	O
yi2	O
)	O
xi1	O
(	O
μi2	O
−	O
yi2	O
)	O
xi2	O
(	O
μi2	O
−	O
yi2	O
)	O
xi3	O
(	O
cid:2	O
)	O
∇wc	O
f	O
(	O
w	O
)	O
=	O
(	O
μic	O
−	O
yic	O
)	O
xi	O
in	O
other	O
words	O
,	O
for	O
each	O
class	O
c	O
,	O
the	O
derivative	O
for	O
the	O
weights	O
in	O
the	O
c	O
’	O
th	O
column	O
is	O
i	O
this	O
has	O
the	O
same	O
form	O
as	O
in	O
the	O
binary	O
logistic	O
regression	B
case	O
,	O
namely	O
an	O
error	O
term	O
times	O
xi	O
.	O
(	O
this	O
turns	O
out	O
to	O
be	O
a	O
general	O
property	O
of	O
distributions	O
in	O
the	O
exponential	B
family	I
,	O
as	O
we	O
will	O
see	O
in	O
section	O
9.3.2	O
.	O
)	O
254	O
chapter	O
8.	O
logistic	B
regression	I
one	O
can	O
also	O
show	O
(	O
exercise	O
8.4	O
)	O
that	O
the	O
hessian	O
is	O
the	O
following	O
block	O
structured	O
d	O
(	O
c	O
−	O
1	O
)	O
×	O
d	O
(	O
c	O
−	O
1	O
)	O
matrix	O
:	O
h	O
(	O
w	O
)	O
=∇	O
2f	O
(	O
w	O
)	O
=	O
n	O
(	O
cid:2	O
)	O
i=1	O
for	O
example	O
,	O
if	O
we	O
have	O
3	O
features	B
and	O
3	O
classes	O
,	O
this	O
becomes	O
(	O
diag	O
(	O
μi	O
)	O
−	O
μiμt	O
i	O
)	O
⊗	O
(	O
xixt	O
i	O
)	O
⎛	O
⎝xi1xi1	O
xi1xi2	O
xi1xi3	O
(	O
cid:9	O
)	O
xi2xi1	O
xi2xi2	O
xi2xi3	O
xi3xi1	O
xi3xi2	O
xi3xi3	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
⊗	O
i1	O
−μi1μi2	O
μi2	O
−	O
μ2	O
i2	O
μi1	O
−	O
μ2	O
−μi1μi2	O
(	O
μi1	O
−	O
μ2	O
i1	O
)	O
xi	O
−μi1μi2xi	O
−μi1μi2xi	O
(	O
μi2	O
−	O
μ2	O
i2	O
)	O
xi	O
i	O
.	O
in	O
other	O
words	O
,	O
the	O
block	O
c	O
,	O
c	O
(	O
cid:4	O
)	O
i	O
(	O
cid:9	O
)	O
μic	O
(	O
δc	O
,	O
c	O
(	O
cid:2	O
)	O
−	O
μi	O
,	O
c	O
(	O
cid:2	O
)	O
)	O
xixt	O
i	O
submatrix	O
is	O
given	O
by	O
⎞	O
⎠	O
(	O
8.41	O
)	O
(	O
8.42	O
)	O
(	O
8.43	O
)	O
(	O
8.44	O
)	O
h	O
(	O
w	O
)	O
=	O
=	O
where	O
xi	O
=	O
xixt	O
hc	O
,	O
c	O
(	O
cid:2	O
)	O
(	O
w	O
)	O
=	O
(	O
cid:2	O
)	O
i	O
this	O
is	O
also	O
a	O
positive	B
deﬁnite	I
matrix	O
,	O
so	O
there	O
is	O
a	O
unique	O
mle	O
.	O
now	O
consider	O
minimizing	O
f	O
(	O
cid:4	O
)	O
(	O
w	O
)	O
(	O
cid:2	O
)	O
−	O
log	O
p	O
(	O
d|w	O
)	O
−	O
log	O
p	O
(	O
w	O
)	O
where	O
p	O
(	O
w	O
)	O
=	O
’	O
c	O
n	O
(	O
wc|0	O
,	O
v0	O
)	O
.	O
the	O
new	O
objective	O
,	O
its	O
gradient	O
and	O
hessian	O
are	O
given	O
by	O
(	O
8.45	O
)	O
(	O
cid:2	O
)	O
1	O
2	O
f	O
(	O
cid:4	O
)	O
g	O
(	O
cid:4	O
)	O
h	O
(	O
cid:4	O
)	O
(	O
w	O
)	O
+	O
(	O
w	O
)	O
=f	O
(	O
w	O
)	O
=g	O
(	O
w	O
)	O
+v	O
−1	O
0	O
(	O
(	O
w	O
)	O
=h	O
(	O
w	O
)	O
+i	O
c	O
⊗	O
v−1	O
c	O
c	O
0	O
0	O
wc	O
wcv−1	O
(	O
cid:2	O
)	O
wc	O
)	O
(	O
8.46	O
)	O
(	O
8.47	O
)	O
(	O
8.48	O
)	O
this	O
can	O
be	O
passed	O
to	O
any	O
gradient-based	O
optimizer	O
to	O
ﬁnd	O
the	O
map	O
estimate	O
.	O
note	O
,	O
however	O
,	O
that	O
the	O
hessian	O
has	O
size	O
o	O
(	O
(	O
cd	O
)	O
×	O
(	O
cd	O
)	O
)	O
,	O
which	O
is	O
c	O
times	O
more	O
row	O
and	O
columns	O
than	O
in	O
the	O
binary	O
case	O
,	O
so	O
limited	O
memory	O
bfgs	O
is	O
more	O
appropriate	O
than	O
newton	O
’	O
s	O
method	O
.	O
see	O
logregfit	O
for	O
some	O
matlab	O
code	O
.	O
8.4	O
bayesian	O
logistic	B
regression	I
it	O
is	O
natural	O
to	O
want	O
to	O
compute	O
the	O
full	B
posterior	O
over	O
the	O
parameters	O
,	O
p	O
(	O
w|d	O
)	O
,	O
for	O
logistic	B
regression	I
models	O
.	O
this	O
can	O
be	O
useful	O
for	O
any	O
situation	O
where	O
we	O
want	O
to	O
associate	O
conﬁdence	B
intervals	I
with	O
our	O
predictions	O
(	O
e.g.	O
,	O
this	O
is	O
necessary	O
when	O
solving	O
contextual	B
bandit	I
problems	O
,	O
discussed	O
in	O
section	O
5.7.3.1	O
)	O
.	O
unfortunately	O
,	O
unlike	O
the	O
linear	B
regression	I
case	O
,	O
this	O
can	O
not	O
be	O
done	O
exactly	O
,	O
since	O
there	O
is	O
no	O
convenient	O
conjugate	B
prior	I
for	O
logistic	B
regression	I
.	O
we	O
discuss	O
one	O
simple	O
approximation	O
below	O
;	O
some	O
other	O
approaches	O
include	O
mcmc	O
(	O
section	O
24.3.3.1	O
)	O
,	O
variational	B
inference	I
(	O
section	O
21.8.1.1	O
)	O
,	O
expectation	B
propagation	I
(	O
kuss	O
and	O
rasmussen	O
2005	O
)	O
,	O
etc	O
.	O
for	O
notational	O
simplicity	O
,	O
we	O
stick	O
to	O
binary	O
logistic	O
regression	B
.	O
8.4.	O
bayesian	O
logistic	B
regression	I
255	O
8.4.1	O
laplace	O
approximation	O
in	O
this	O
section	O
,	O
we	O
discuss	O
how	O
to	O
make	O
a	O
gaussian	O
approximation	O
to	O
a	O
posterior	O
distribution	O
.	O
the	O
approximation	O
works	O
as	O
follows	O
.	O
suppose	O
θ	O
∈	O
r	O
d.	O
let	O
p	O
(	O
θ|d	O
)	O
=	O
e−e	O
(	O
θ	O
)	O
1	O
z	O
(	O
8.49	O
)	O
where	O
e	O
(	O
θ	O
)	O
is	O
called	O
an	O
energy	B
function	I
,	O
and	O
is	O
equal	O
to	O
the	O
negative	O
log	O
of	O
the	O
unnormal-	O
ized	O
log	O
posterior	O
,	O
e	O
(	O
θ	O
)	O
=	O
−	O
log	O
p	O
(	O
θ	O
,	O
d	O
)	O
,	O
with	O
z	O
=	O
p	O
(	O
d	O
)	O
being	O
the	O
normalization	O
constant	O
.	O
performing	O
a	O
taylor	O
series	O
expansion	O
around	O
the	O
mode	B
θ∗	O
(	O
i.e.	O
,	O
the	O
lowest	O
energy	O
state	O
)	O
we	O
get	O
)	O
t	O
h	O
(	O
θ	O
−	O
θ∗	O
)	O
+	O
(	O
θ	O
−	O
θ∗	O
(	O
θ	O
−	O
θ∗	O
(	O
8.50	O
)	O
)	O
where	O
g	O
is	O
the	O
gradient	O
and	O
h	O
is	O
the	O
hessian	O
of	O
the	O
energy	B
function	I
evaluated	O
at	O
the	O
mode	B
:	O
)	O
t	O
g	O
+	O
e	O
(	O
θ	O
)	O
≈	O
e	O
(	O
θ∗	O
(	O
(	O
θ∗	O
,	O
h	O
(	O
cid:2	O
)	O
∂2e	O
(	O
θ	O
)	O
∂θ∂θt	O
g	O
(	O
cid:2	O
)	O
∇e	O
(	O
θ	O
)	O
1	O
2	O
|θ∗	O
(	O
cid:29	O
)	O
since	O
θ∗	O
is	O
the	O
mode	B
,	O
the	O
gradient	O
term	O
is	O
zero	O
.	O
hence	O
ˆp	O
(	O
θ|d	O
)	O
≈	O
1	O
z	O
(	O
cid:28	O
)	O
e−e	O
(	O
θ∗	O
)	O
exp	O
=	O
n	O
(	O
θ|θ∗	O
,	O
h−1	O
)	O
−	O
1	O
2	O
(	O
θ	O
−	O
θ∗	O
)	O
t	O
h	O
(	O
θ	O
−	O
θ∗	O
)	O
z	O
=	O
p	O
(	O
d	O
)	O
≈	O
ˆp	O
(	O
θ|d	O
)	O
dθ	O
=	O
e−e	O
(	O
θ∗	O
)	O
(	O
2π	O
)	O
d/2|h|−	O
1	O
2	O
(	O
cid:30	O
)	O
(	O
8.51	O
)	O
(	O
8.52	O
)	O
(	O
8.53	O
)	O
(	O
8.54	O
)	O
the	O
last	O
line	O
follows	O
from	O
normalization	O
constant	O
of	O
the	O
multivariate	O
gaussian	O
.	O
equation	O
8.54	O
is	O
known	O
as	O
the	O
laplace	O
approximation	O
to	O
the	O
marginal	B
likelihood	I
.	O
therefore	O
equation	O
8.52	O
is	O
sometimes	O
called	O
the	O
the	O
laplace	O
approximation	O
to	O
the	O
posterior	O
.	O
however	O
,	O
in	O
the	O
statistics	O
community	O
,	O
the	O
term	O
“	O
laplace	O
approximation	O
”	O
refers	O
to	O
a	O
more	O
sophisticated	O
method	O
(	O
see	O
e.g	O
.	O
it	O
may	O
therefore	O
be	O
better	O
to	O
use	O
the	O
term	O
“	O
gaussian	O
approximation	O
”	O
to	O
refer	O
to	O
equation	O
8.52.	O
a	O
gaussian	O
approximation	O
is	O
often	O
a	O
reasonable	O
approximation	O
,	O
since	O
posteriors	O
often	O
become	O
more	O
“	O
gaussian-like	O
”	O
as	O
the	O
sample	O
size	O
increases	O
,	O
for	O
reasons	O
analogous	O
to	O
the	O
central	O
(	O
in	O
physics	O
,	O
there	O
is	O
an	O
analogous	O
technique	O
known	O
as	O
a	O
saddle	B
point	I
approximation	I
.	O
)	O
(	O
rue	O
et	O
al	O
.	O
2009	O
)	O
for	O
details	O
)	O
.	O
limit	O
theorem	O
.	O
8.4.2	O
derivation	O
of	O
the	O
bic	O
we	O
can	O
use	O
the	O
gaussian	O
approximation	O
to	O
write	O
the	O
log	O
marginal	O
likelihood	B
as	O
follows	O
,	O
dropping	O
irrelevant	O
constants	O
:	O
log	O
p	O
(	O
d	O
)	O
≈	O
log	O
p	O
(	O
d|θ∗	O
)	O
+	O
log	O
p	O
(	O
θ∗	O
log	O
|h|	O
(	O
8.55	O
)	O
the	O
penalization	O
terms	O
which	O
are	O
added	O
to	O
the	O
log	O
p	O
(	O
d|θ∗	O
)	O
are	O
sometimes	O
called	O
the	O
occam	O
factor	B
,	O
and	O
are	O
a	O
measure	O
of	O
model	O
complexity	O
.	O
if	O
we	O
have	O
a	O
uniform	O
prior	O
,	O
p	O
(	O
θ	O
)	O
∝	O
1	O
,	O
we	O
can	O
drop	O
the	O
second	O
term	O
,	O
and	O
replace	O
θ∗	O
with	O
the	O
mle	O
,	O
ˆθ.	O
)	O
−	O
1	O
2	O
chapter	O
8.	O
logistic	B
regression	I
(	O
cid:10	O
)	O
n	O
we	O
now	O
focus	O
on	O
approximating	O
the	O
third	O
term	O
.	O
we	O
have	O
h	O
=	O
∇∇	O
log	O
p	O
(	O
di|θ	O
)	O
.	O
let	O
us	O
approximate	O
each	O
hi	O
by	O
a	O
ﬁxed	O
matrix	O
ˆh	O
.	O
then	O
we	O
have	O
i=1	O
hi	O
,	O
where	O
hi	O
=	O
log	O
|h|	O
=	O
log	O
|n	O
ˆh|	O
=	O
log	O
(	O
n	O
d|	O
ˆh|	O
)	O
=	O
d	O
log	O
n	O
+	O
log	O
|	O
ˆh|	O
(	O
8.56	O
)	O
where	O
d	O
=	O
dim	O
(	O
θ	O
)	O
and	O
we	O
have	O
assumed	O
h	O
is	O
full	B
rank	O
.	O
we	O
can	O
drop	O
the	O
log	O
|	O
ˆh|	O
term	O
,	O
since	O
it	O
is	O
independent	O
of	O
n	O
,	O
and	O
thus	O
will	O
get	O
overwhelmed	O
by	O
the	O
likelihood	B
.	O
putting	O
all	O
the	O
pieces	O
together	O
,	O
we	O
recover	O
the	O
bic	O
score	O
(	O
section	O
5.3.2.4	O
)	O
:	O
log	O
p	O
(	O
d	O
)	O
≈	O
log	O
p	O
(	O
d|ˆθ	O
)	O
−	O
d	O
2	O
log	O
n	O
(	O
8.57	O
)	O
8.4.3	O
gaussian	O
approximation	O
for	O
logistic	B
regression	I
now	O
let	O
us	O
apply	O
the	O
gaussian	O
approximation	O
to	O
logistic	B
regression	I
.	O
we	O
will	O
use	O
a	O
a	O
gaussian	O
prior	O
of	O
the	O
form	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
w|0	O
,	O
v0	O
)	O
,	O
just	O
as	O
we	O
did	O
in	O
map	O
estimation	O
.	O
the	O
approximate	O
posterior	O
is	O
given	O
by	O
256	O
p	O
(	O
w|d	O
)	O
≈	O
n	O
(	O
w|	O
ˆw	O
,	O
h−1	O
)	O
(	O
8.58	O
)	O
where	O
ˆw	O
=	O
arg	O
minw	O
e	O
(	O
w	O
)	O
,	O
e	O
(	O
w	O
)	O
=	O
−	O
(	O
log	O
p	O
(	O
d|w	O
)	O
+	O
log	O
p	O
(	O
w	O
)	O
)	O
,	O
and	O
h	O
=	O
∇2e	O
(	O
w	O
)	O
|	O
ˆw	O
.	O
as	O
an	O
example	O
,	O
consider	O
the	O
linearly	B
separable	I
2d	O
data	O
in	O
figure	O
8.5	O
(	O
a	O
)	O
.	O
there	O
are	O
many	O
parameter	B
settings	O
that	O
correspond	O
to	O
lines	O
that	O
perfectly	O
separate	O
the	O
training	O
data	O
;	O
we	O
show	O
4	O
examples	O
.	O
the	O
likelihood	B
surface	O
is	O
shown	O
in	O
figure	O
8.5	O
(	O
b	O
)	O
,	O
where	O
we	O
see	O
that	O
the	O
likelihood	B
is	O
unbounded	O
as	O
we	O
move	O
up	O
and	O
to	O
the	O
right	O
in	O
parameter	B
space	O
,	O
along	O
a	O
ridge	O
where	O
w2/w1	O
=	O
2.35	O
(	O
this	O
is	O
indicated	O
by	O
the	O
diagonal	B
line	O
)	O
.	O
the	O
reasons	O
for	O
this	O
is	O
that	O
we	O
can	O
maximize	O
the	O
likelihood	B
by	O
driving	O
||w||	O
to	O
inﬁnity	O
(	O
subject	O
to	O
being	O
on	O
this	O
line	O
)	O
,	O
since	O
large	O
regression	O
weights	O
make	O
the	O
sigmoid	B
function	O
very	O
steep	O
,	O
turning	O
it	O
into	O
a	O
step	O
function	O
.	O
consequently	O
the	O
mle	O
is	O
not	O
well	O
deﬁned	O
when	O
the	O
data	O
is	O
linearly	B
separable	I
.	O
to	O
regularize	O
the	O
problem	O
,	O
let	O
us	O
use	O
a	O
vague	O
spherical	B
prior	O
centered	O
at	O
the	O
origin	O
,	O
n	O
(	O
w|0	O
,	O
100i	O
)	O
.	O
multiplying	O
this	O
spherical	B
prior	O
by	O
the	O
likelihood	B
surface	O
results	O
in	O
a	O
highly	O
skewed	O
posterior	O
,	O
shown	O
in	O
figure	O
8.5	O
(	O
c	O
)	O
.	O
(	O
the	O
posterior	O
is	O
skewed	O
because	O
the	O
likelihood	B
function	O
“	O
chops	O
off	O
”	O
regions	O
of	O
parameter	B
space	O
(	O
in	O
a	O
“	O
soft	O
”	O
fashion	O
)	O
which	O
disagree	O
with	O
the	O
data	O
.	O
)	O
the	O
map	O
estimate	O
is	O
shown	O
by	O
the	O
blue	O
dot	O
.	O
unlike	O
the	O
mle	O
,	O
this	O
is	O
not	O
at	O
inﬁnity	O
.	O
the	O
gaussian	O
approximation	O
to	O
this	O
posterior	O
is	O
shown	O
in	O
figure	O
8.5	O
(	O
d	O
)	O
.	O
we	O
see	O
that	O
this	O
is	O
a	O
symmetric	B
distribution	O
,	O
and	O
therefore	O
not	O
a	O
great	O
approximation	O
.	O
of	O
course	O
,	O
it	O
gets	O
the	O
mode	B
correct	O
(	O
by	O
construction	O
)	O
,	O
and	O
it	O
at	O
least	O
represents	O
the	O
fact	O
that	O
there	O
is	O
more	O
uncertainty	B
along	O
the	O
southwest-northeast	O
direction	O
(	O
which	O
corresponds	O
to	O
uncertainty	B
about	O
the	O
orientation	O
of	O
separating	O
lines	O
)	O
than	O
perpendicular	O
to	O
this	O
.	O
although	O
a	O
crude	O
approximation	O
,	O
this	O
is	O
surely	O
better	O
than	O
approximating	O
the	O
posterior	O
by	O
a	O
delta	O
function	O
,	O
which	O
is	O
what	O
map	O
estimation	O
does	O
.	O
8.4.4	O
approximating	O
the	O
posterior	O
predictive	O
given	O
the	O
posterior	O
,	O
we	O
can	O
compute	O
credible	O
intervals	O
,	O
perform	O
hypothesis	O
tests	O
,	O
etc.	O
,	O
just	O
as	O
we	O
did	O
in	O
section	O
7.6.3.3	O
in	O
the	O
case	O
of	O
linear	B
regression	I
.	O
but	O
in	O
machine	B
learning	I
,	O
interest	O
usually	O
focusses	O
on	O
prediction	O
.	O
the	O
posterior	B
predictive	I
distribution	I
has	O
the	O
form	O
p	O
(	O
y|x	O
,	O
d	O
)	O
=	O
p	O
(	O
y|x	O
,	O
w	O
)	O
p	O
(	O
w|d	O
)	O
dw	O
(	O
8.59	O
)	O
(	O
cid:28	O
)	O
8.4.	O
bayesian	O
logistic	B
regression	I
257	O
data	O
log−likelihood	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
−10	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
−8	O
−5	O
0	O
5	O
(	O
a	O
)	O
log−unnormalised	O
posterior	O
−6	O
−4	O
−2	O
0	O
(	O
c	O
)	O
2	O
4	O
6	O
8	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
−8	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
−8	O
3	O
4	O
2	O
1	O
−6	O
−4	O
−2	O
0	O
(	O
b	O
)	O
2	O
4	O
6	O
8	O
laplace	O
approximation	O
to	O
posterior	O
−6	O
−4	O
−2	O
0	O
(	O
d	O
)	O
2	O
4	O
6	O
8	O
figure	O
8.5	O
(	O
a	O
)	O
two-class	O
data	O
in	O
2d	O
.	O
(	O
b	O
)	O
log-likelihood	O
for	O
a	O
logistic	B
regression	I
model	O
.	O
the	O
line	O
is	O
drawn	O
from	O
the	O
origin	O
in	O
the	O
direction	O
of	O
the	O
mle	O
(	O
which	O
is	O
at	O
inﬁnity	O
)	O
.	O
the	O
numbers	O
correspond	O
to	O
4	O
points	O
in	O
parameter	B
space	O
,	O
corresponding	O
to	O
the	O
lines	O
in	O
(	O
a	O
)	O
.	O
(	O
c	O
)	O
unnormalized	O
log	O
posterior	O
(	O
assuming	O
vague	O
spherical	B
prior	O
)	O
.	O
(	O
d	O
)	O
laplace	O
approximation	O
to	O
posterior	O
.	O
based	O
on	O
a	O
ﬁgure	O
by	O
mark	O
girolami	O
.	O
figure	O
generated	O
by	O
logreglaplacegirolamidemo	O
.	O
unfortunately	O
this	O
integral	O
is	O
intractable	O
.	O
the	O
simplest	O
approximation	O
is	O
the	O
plug-in	B
approximation	I
,	O
which	O
,	O
in	O
the	O
binary	O
case	O
,	O
takes	O
the	O
form	O
p	O
(	O
y	O
=	O
1|x	O
,	O
d	O
)	O
≈	O
p	O
(	O
y	O
=	O
1|x	O
,	O
e	O
[	O
w	O
]	O
)	O
(	O
8.60	O
)	O
where	O
e	O
[	O
w	O
]	O
is	O
the	O
posterior	B
mean	I
.	O
in	O
this	O
context	O
,	O
e	O
[	O
w	O
]	O
is	O
called	O
the	O
bayes	O
point	O
.	O
of	O
course	O
,	O
such	O
a	O
plug-in	B
estimate	O
underestimates	O
the	O
uncertainty	B
.	O
we	O
discuss	O
some	O
better	O
approximations	O
below	O
.	O
258	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
−8	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
−8	O
p	O
(	O
y=1|x	O
,	O
wmap	O
)	O
decision	B
boundary	I
for	O
sampled	O
w	O
chapter	O
8.	O
logistic	B
regression	I
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
−10	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
−8	O
−8	O
−6	O
−4	O
−2	O
0	O
2	O
4	O
6	O
8	O
(	O
b	O
)	O
numerical	O
approx	O
of	O
p	O
(	O
y=1|x	O
)	O
−6	O
−4	O
−2	O
0	O
(	O
d	O
)	O
2	O
4	O
6	O
8	O
−6	O
−4	O
−2	O
0	O
(	O
a	O
)	O
2	O
4	O
6	O
8	O
mc	O
approx	O
of	O
p	O
(	O
y=1|x	O
)	O
−6	O
−4	O
−2	O
0	O
(	O
c	O
)	O
2	O
4	O
6	O
8	O
figure	O
8.6	O
posterior	B
predictive	I
distribution	I
for	O
a	O
logistic	B
regression	I
model	O
in	O
2d	O
.	O
top	O
left	O
:	O
contours	O
of	O
p	O
(	O
y	O
=	O
1|x	O
,	O
ˆwmap	O
)	O
.	O
top	O
right	O
:	O
samples	B
from	O
the	O
posterior	B
predictive	I
distribution	I
.	O
bottom	O
left	O
:	O
averaging	O
over	O
these	O
samples	B
.	O
bottom	O
right	O
:	O
moderated	B
output	I
(	O
probit	B
approximation	O
)	O
.	O
based	O
on	O
a	O
ﬁgure	O
by	O
mark	O
girolami	O
.	O
figure	O
generated	O
by	O
logreglaplacegirolamidemo	O
.	O
8.4.4.1	O
monte	O
carlo	O
approximation	O
a	O
better	O
approach	O
is	O
to	O
use	O
a	O
monte	O
carlo	O
approximation	O
,	O
as	O
follows	O
:	O
s	O
(	O
cid:2	O
)	O
s=1	O
p	O
(	O
y	O
=	O
1|x	O
,	O
d	O
)	O
≈	O
1	O
s	O
sigm	O
(	O
(	O
ws	O
)	O
t	O
x	O
)	O
(	O
8.61	O
)	O
where	O
ws	O
∼	O
p	O
(	O
w|d	O
)	O
are	O
samples	B
from	O
the	O
posterior	O
.	O
(	O
this	O
technique	O
can	O
be	O
trivially	O
extended	O
to	O
the	O
multi-class	O
case	O
.	O
)	O
if	O
we	O
have	O
approximated	O
the	O
posterior	O
using	O
monte	O
carlo	O
,	O
we	O
can	O
reuse	O
these	O
samples	B
for	O
prediction	O
.	O
if	O
we	O
made	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
,	O
we	O
can	O
draw	O
independent	O
samples	O
from	O
the	O
gaussian	O
using	O
standard	O
methods	O
.	O
figure	O
8.6	O
(	O
b	O
)	O
shows	O
samples	B
from	O
the	O
posteiror	O
predictive	B
for	O
our	O
2d	O
example	O
.	O
figure	O
8.6	O
(	O
c	O
)	O
8.4.	O
bayesian	O
logistic	B
regression	I
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
259	O
sigmoid	B
probit	O
460	O
480	O
500	O
520	O
540	O
560	O
580	O
600	O
620	O
640	O
0	O
−6	O
−4	O
−2	O
(	O
a	O
)	O
2	O
4	O
6	O
0	O
(	O
b	O
)	O
figure	O
8.7	O
(	O
a	O
)	O
posterior	B
predictive	I
density	I
for	O
sat	O
data	O
.	O
the	O
red	O
circle	O
denotes	O
the	O
posterior	B
mean	I
,	O
the	O
blue	O
cross	O
the	O
posterior	B
median	I
,	O
and	O
the	O
blue	O
lines	O
denote	O
the	O
5th	O
and	O
95th	O
percentiles	O
of	O
the	O
predictive	B
(	O
b	O
)	O
the	O
logistic	B
(	O
sigmoid	B
)	O
function	O
sigm	O
(	O
x	O
)	O
in	O
distribution	O
.	O
figure	O
generated	O
by	O
logregsatdemobayes	O
.	O
solid	O
red	O
,	O
with	O
the	O
rescaled	O
probit	B
function	O
φ	O
(	O
λx	O
)	O
in	O
dotted	O
blue	O
superimposed	O
.	O
here	O
λ	O
=	O
π/8	O
,	O
which	O
was	O
chosen	O
so	O
that	O
the	O
derivatives	O
of	O
the	O
two	O
curves	O
match	O
at	O
x	O
=	O
0.	O
based	O
on	O
figure	O
4.9	O
of	O
(	O
bishop	O
2006b	O
)	O
.	O
figure	O
generated	O
by	O
probitplot	O
.	O
figure	O
generated	O
by	O
probitregdemo	O
.	O
(	O
cid:15	O
)	O
shows	O
the	O
average	O
of	O
these	O
samples	B
.	O
by	O
averaging	O
over	O
multiple	O
predictions	O
,	O
we	O
see	O
that	O
the	O
uncertainty	B
in	O
the	O
decision	B
boundary	I
“	O
splays	O
out	O
”	O
as	O
we	O
move	O
further	O
from	O
the	O
training	O
data	O
.	O
so	O
although	O
the	O
decision	B
boundary	I
is	O
linear	O
,	O
the	O
posterior	B
predictive	I
density	I
is	O
not	O
linear	O
.	O
note	O
also	O
that	O
the	O
posterior	B
mean	I
decision	O
boundary	O
is	O
roughly	O
equally	O
far	O
from	O
both	O
classes	O
;	O
this	O
is	O
the	O
bayesian	O
analog	O
of	O
the	O
large	B
margin	I
principle	I
discussed	O
in	O
section	O
14.5.2.2.	O
figure	O
8.7	O
(	O
a	O
)	O
shows	O
an	O
example	O
in	O
1d	O
.	O
the	O
red	O
dots	O
denote	O
the	O
mean	B
of	O
the	O
posterior	O
predictive	O
evaluated	O
at	O
the	O
training	O
data	O
.	O
the	O
vertical	O
blue	O
lines	O
denote	O
95	O
%	O
credible	O
intervals	O
for	O
the	O
posterior	O
predictive	O
;	O
the	O
small	O
blue	O
star	O
is	O
the	O
median	B
.	O
we	O
see	O
that	O
,	O
with	O
the	O
bayesian	O
approach	O
,	O
we	O
are	O
able	O
to	O
model	O
our	O
uncertainty	B
about	O
the	O
probability	O
a	O
student	O
will	O
pass	O
the	O
exam	O
based	O
on	O
his	O
sat	O
score	O
,	O
rather	O
than	O
just	O
getting	O
a	O
point	B
estimate	I
.	O
8.4.4.2	O
probit	B
approximation	O
(	O
moderated	B
output	I
)	O
*	O
if	O
we	O
have	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
p	O
(	O
w|d	O
)	O
≈	O
n	O
(	O
w|mn	O
,	O
vn	O
)	O
,	O
we	O
can	O
also	O
compute	O
a	O
deterministic	O
approximation	O
to	O
the	O
posterior	B
predictive	I
distribution	I
,	O
at	O
least	O
in	O
the	O
binary	O
case	O
.	O
we	O
proceed	O
as	O
follows	O
:	O
p	O
(	O
y	O
=	O
1|x	O
,	O
d	O
)	O
≈	O
sigm	O
(	O
wt	O
x	O
)	O
p	O
(	O
w|d	O
)	O
dw	O
=	O
sigm	O
(	O
a	O
)	O
n	O
(	O
a|μa	O
,	O
σ2	O
a	O
)	O
da	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
a	O
(	O
cid:2	O
)	O
wt	O
x	O
(	O
cid:28	O
)	O
μa	O
(	O
cid:2	O
)	O
e	O
[	O
a	O
]	O
=	O
mt	O
n	O
x	O
p	O
(	O
a|d	O
)	O
[	O
a2	O
−	O
e	O
a	O
(	O
cid:2	O
)	O
var	O
[	O
a	O
]	O
=	O
σ2	O
p	O
(	O
w|d	O
)	O
[	O
(	O
wt	O
x	O
)	O
2	O
−	O
(	O
mt	O
(	O
cid:28	O
)	O
=	O
(	O
cid:31	O
)	O
a2	O
]	O
da	O
n	O
x	O
)	O
2	O
]	O
dw	O
=	O
xt	O
vn	O
x	O
(	O
8.62	O
)	O
(	O
8.63	O
)	O
(	O
8.64	O
)	O
(	O
8.65	O
)	O
(	O
8.66	O
)	O
260	O
chapter	O
8.	O
logistic	B
regression	I
thus	O
we	O
see	O
that	O
we	O
need	O
to	O
evaluate	O
the	O
expectation	O
of	O
a	O
sigmoid	B
with	O
respect	O
to	O
a	O
gaussian	O
.	O
this	O
can	O
be	O
approximated	O
by	O
exploiting	O
the	O
fact	O
that	O
the	O
sigmoid	B
function	O
is	O
similar	B
to	O
the	O
probit	B
function	O
,	O
which	O
is	O
given	O
by	O
the	O
cdf	B
of	O
the	O
standard	B
normal	I
:	O
φ	O
(	O
a	O
)	O
(	O
cid:2	O
)	O
n	O
(	O
x|0	O
,	O
1	O
)	O
dx	O
(	O
8.67	O
)	O
(	O
cid:28	O
)	O
a	O
−∞	O
figure	O
8.7	O
(	O
b	O
)	O
plots	O
the	O
sigmoid	B
and	O
probit	B
functions	O
.	O
we	O
have	O
rescaled	O
the	O
axes	O
so	O
that	O
sigm	O
(	O
a	O
)	O
has	O
the	O
same	O
slope	O
as	O
φ	O
(	O
λa	O
)	O
at	O
the	O
origin	O
,	O
where	O
λ2	O
=	O
π/8	O
.	O
the	O
advantage	O
of	O
using	O
the	O
probit	B
is	O
that	O
one	O
can	O
convolve	O
it	O
with	O
a	O
gaussian	O
analytically	O
:	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
φ	O
(	O
λa	O
)	O
n	O
(	O
a|μ	O
,	O
σ2	O
)	O
da	O
=	O
φ	O
a	O
(	O
λ−2	O
+	O
σ2	O
)	O
1	O
2	O
(	O
8.68	O
)	O
we	O
now	O
plug	O
in	O
the	O
approximation	O
sigm	O
(	O
a	O
)	O
≈	O
φ	O
(	O
λa	O
)	O
to	O
both	O
sides	O
of	O
this	O
equation	O
to	O
get	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
sigm	O
(	O
a	O
)	O
n	O
(	O
a|μ	O
,	O
σ2	O
)	O
da	O
≈	O
sigm	O
(	O
κ	O
(	O
σ2	O
)	O
μ	O
)	O
−	O
1	O
κ	O
(	O
σ2	O
)	O
(	O
cid:2	O
)	O
(	O
1	O
+	O
πσ2/8	O
)	O
2	O
(	O
8.69	O
)	O
(	O
8.70	O
)	O
applying	O
this	O
to	O
the	O
logistic	B
regression	I
model	O
we	O
get	O
the	O
following	O
expression	O
(	O
ﬁrst	O
suggested	O
in	O
(	O
spiegelhalter	O
and	O
lauritzen	O
1990	O
)	O
)	O
:	O
p	O
(	O
y	O
=	O
1|x	O
,	O
d	O
)	O
≈	O
sigm	O
(	O
κ	O
(	O
σ2	O
(	O
8.71	O
)	O
a	O
)	O
μa	O
)	O
figure	O
8.6	O
(	O
d	O
)	O
indicates	O
that	O
this	O
gives	O
very	O
similar	B
results	O
to	O
the	O
monte	O
carlo	O
approximation	O
.	O
the	O
plug-in	B
estimate	O
.	O
to	O
see	O
this	O
,	O
note	O
that	O
0	O
≤	O
κ	O
(	O
σ2	O
)	O
≤	O
1	O
and	O
hence	O
using	O
equation	O
8.71	O
is	O
sometimes	O
called	O
a	O
moderated	B
output	I
,	O
since	O
it	O
is	O
less	O
extreme	O
than	O
sigm	O
(	O
κ	O
(	O
σ2	O
)	O
μ	O
)	O
≤	O
sigm	O
(	O
μ	O
)	O
=	O
p	O
(	O
y	O
=	O
1|x	O
,	O
ˆw	O
)	O
(	O
8.72	O
)	O
where	O
the	O
inequality	O
is	O
strict	B
if	O
μ	O
(	O
cid:4	O
)	O
=	O
0.	O
if	O
μ	O
>	O
0	O
,	O
we	O
have	O
p	O
(	O
y	O
=	O
1|x	O
,	O
ˆw	O
)	O
>	O
0.5	O
,	O
but	O
the	O
moderated	O
prediction	O
is	O
always	O
closer	O
to	O
0.5	O
,	O
so	O
it	O
is	O
less	O
conﬁdent	O
.	O
however	O
,	O
the	O
decision	B
boundary	I
occurs	O
whenever	O
p	O
(	O
y	O
=	O
1|x	O
,	O
d	O
)	O
=	O
sigm	O
(	O
κ	O
(	O
σ2	O
)	O
μ	O
)	O
=	O
0.5	O
,	O
which	O
implies	O
μ	O
=	O
ˆwt	O
x	O
=	O
0.	O
hence	O
the	O
decision	B
boundary	I
for	O
the	O
moderated	O
approximation	O
is	O
the	O
same	O
as	O
for	O
the	O
plug-in	B
approximation	I
.	O
so	O
the	O
number	O
of	O
misclassiﬁcations	O
will	O
be	O
the	O
same	O
for	O
the	O
two	O
methods	O
,	O
but	O
the	O
log-likelihood	O
will	O
not	O
.	O
(	O
note	O
that	O
in	O
the	O
multiclass	O
case	O
,	O
taking	O
into	O
account	O
posterior	O
covariance	O
gives	O
different	O
answers	O
than	O
the	O
plug-in	B
approach	O
:	O
see	O
exercise	O
3.10.3	O
of	O
(	O
rasmussen	O
and	O
williams	O
2006	O
)	O
.	O
)	O
8.4.5	O
residual	B
analysis	I
(	O
outlier	O
detection	O
)	O
*	O
it	O
is	O
sometimes	O
useful	O
to	O
detect	O
data	O
cases	O
which	O
are	O
“	O
outliers	B
”	O
.	O
this	O
is	O
called	O
residual	B
analysis	I
or	O
case	B
analysis	I
.	O
in	O
a	O
regression	B
setting	O
,	O
this	O
can	O
be	O
performed	O
by	O
computing	O
ri	O
=	O
yi−ˆyi	O
,	O
where	O
ˆyi	O
=	O
ˆwt	O
xi	O
.	O
these	O
values	O
should	O
follow	O
a	O
n	O
(	O
0	O
,	O
σ2	O
)	O
distribution	O
,	O
if	O
the	O
modelling	O
assumptions	O
are	O
correct	O
.	O
this	O
can	O
be	O
assessed	O
by	O
creating	O
a	O
qq-plot	B
,	O
where	O
we	O
plot	O
the	O
n	O
theoretical	O
quantiles	O
of	O
a	O
gaussian	O
distribution	O
against	O
the	O
n	O
empirical	O
quantiles	O
of	O
the	O
ri	O
.	O
points	O
that	O
deviate	O
from	O
the	O
straightline	O
are	O
potential	O
outliers	O
.	O
8.5.	O
online	B
learning	I
and	O
stochastic	B
optimization	I
261	O
classical	B
methods	O
,	O
based	O
on	O
residuals	O
,	O
do	O
not	O
work	O
well	O
for	O
binary	O
data	O
,	O
because	O
they	O
rely	O
on	O
asymptotic	O
normality	O
of	O
the	O
test	O
statistics	O
.	O
however	O
,	O
adopting	O
a	O
bayesian	O
approach	O
,	O
we	O
can	O
just	O
deﬁne	O
outliers	B
to	O
be	O
points	O
which	O
which	O
p	O
(	O
yi|ˆyi	O
)	O
is	O
small	O
,	O
where	O
we	O
typically	O
use	O
ˆyi	O
=	O
sigm	O
(	O
ˆwt	O
xi	O
)	O
.	O
note	O
that	O
ˆw	O
was	O
estimated	O
from	O
all	O
the	O
data	O
.	O
a	O
better	O
method	O
is	O
to	O
exclude	O
(	O
xi	O
,	O
yi	O
)	O
from	O
the	O
estimate	O
of	O
w	O
when	O
predicting	O
yi	O
.	O
that	O
is	O
,	O
we	O
deﬁne	O
outliers	B
to	O
be	O
points	O
which	O
have	O
low	O
probability	O
under	O
the	O
cross-validated	O
posterior	B
predictive	I
distribution	I
,	O
deﬁned	O
by	O
p	O
(	O
yi|xi	O
,	O
x−i	O
,	O
y−i	O
)	O
=	O
p	O
(	O
yi|xi	O
,	O
w	O
)	O
p	O
(	O
yi	O
(	O
cid:2	O
)	O
|xi	O
(	O
cid:2	O
)	O
,	O
w	O
)	O
p	O
(	O
w	O
)	O
dw	O
(	O
8.73	O
)	O
(	O
cid:28	O
)	O
(	O
cid:27	O
)	O
i	O
(	O
cid:2	O
)	O
(	O
cid:7	O
)	O
=i	O
this	O
can	O
be	O
efficiently	O
approximated	O
by	O
sampling	O
methods	O
(	O
gelfand	O
1996	O
)	O
.	O
for	O
further	O
discussion	O
of	O
residual	B
analysis	I
in	O
logistic	B
regression	I
models	O
,	O
see	O
e.g.	O
,	O
(	O
johnson	O
and	O
albert	O
1999	O
,	O
sec	O
3.4	O
)	O
.	O
8.5	O
online	B
learning	I
and	O
stochastic	B
optimization	I
traditionally	O
machine	B
learning	I
is	O
performed	O
offline	B
,	O
which	O
means	O
we	O
have	O
a	O
batch	B
of	O
data	O
,	O
and	O
we	O
optimize	O
an	O
equation	O
of	O
the	O
following	O
form	O
f	O
(	O
θ	O
)	O
=	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
i=1	O
f	O
(	O
θ	O
,	O
zi	O
)	O
(	O
8.74	O
)	O
where	O
zi	O
=	O
(	O
xi	O
,	O
yi	O
)	O
in	O
the	O
supervised	O
case	O
,	O
or	O
just	O
xi	O
in	O
the	O
unsupervised	O
case	O
,	O
and	O
f	O
(	O
θ	O
,	O
zi	O
)	O
is	O
some	O
kind	O
of	O
loss	B
function	I
.	O
for	O
example	O
,	O
we	O
might	O
use	O
f	O
(	O
θ	O
,	O
zi	O
)	O
=	O
−	O
log	O
p	O
(	O
yi|xi	O
,	O
θ	O
)	O
in	O
which	O
case	O
we	O
are	O
trying	O
to	O
maximize	O
the	O
likelihood	B
.	O
alternatively	O
,	O
we	O
might	O
use	O
f	O
(	O
θ	O
,	O
zi	O
)	O
=	O
l	O
(	O
yi	O
,	O
h	O
(	O
xi	O
,	O
θ	O
)	O
)	O
(	O
8.75	O
)	O
(	O
8.76	O
)	O
where	O
h	O
(	O
xi	O
,	O
θ	O
)	O
is	O
a	O
prediction	O
function	O
,	O
and	O
l	O
(	O
y	O
,	O
ˆy	O
)	O
is	O
some	O
other	O
loss	B
function	I
such	O
as	O
squared	B
error	I
or	O
the	O
huber	O
loss	B
.	O
in	O
frequentist	B
decision	O
theory	O
,	O
the	O
average	O
loss	O
is	O
called	O
the	O
risk	B
(	O
see	O
section	O
6.3	O
)	O
,	O
so	O
this	O
overall	O
approach	O
is	O
called	O
empirical	B
risk	I
minimization	I
or	O
erm	O
(	O
see	O
section	O
6.5	O
for	O
details	O
)	O
.	O
however	O
,	O
if	O
we	O
have	O
streaming	B
data	I
,	O
we	O
need	O
to	O
perform	O
online	B
learning	I
,	O
so	O
we	O
can	O
update	O
our	O
estimates	O
as	O
each	O
new	O
data	O
point	O
arrives	O
rather	O
than	O
waiting	O
until	O
“	O
the	O
end	O
”	O
(	O
which	O
may	O
never	O
occur	O
)	O
.	O
and	O
even	O
if	O
we	O
have	O
a	O
batch	B
of	O
data	O
,	O
we	O
might	O
want	O
to	O
treat	O
it	O
like	O
a	O
stream	O
if	O
it	O
is	O
too	O
large	O
to	O
hold	O
in	O
main	O
memory	O
.	O
below	O
we	O
discuss	O
learning	B
methods	O
for	O
this	O
kind	O
of	O
scenario.1	O
1.	O
a	O
simple	O
implementation	O
trick	O
can	O
be	O
used	O
to	O
speed	O
up	O
batch	B
learning	O
algorithms	O
when	O
applied	O
to	O
data	O
sets	O
that	O
are	O
too	O
large	O
to	O
hold	O
in	O
memory	O
.	O
first	O
note	O
that	O
the	O
naive	O
implementation	O
makes	O
a	O
pass	O
over	O
the	O
data	O
ﬁle	O
,	O
from	O
the	O
beginning	O
to	O
end	O
,	O
accumulating	O
the	O
sufficient	B
statistics	I
and	O
gradients	O
as	O
it	O
goes	O
;	O
then	O
an	O
update	O
is	O
performed	O
and	O
the	O
process	O
repeats	O
.	O
unfortunately	O
,	O
at	O
the	O
end	O
of	O
each	O
pass	O
,	O
the	O
data	O
from	O
the	O
beginning	O
of	O
the	O
ﬁle	O
will	O
have	O
been	O
evicted	O
from	O
the	O
cache	O
(	O
since	O
are	O
are	O
assuming	O
it	O
can	O
not	O
all	O
ﬁt	O
into	O
memory	O
)	O
.	O
rather	O
than	O
going	O
back	O
to	O
the	O
beginning	O
of	O
the	O
ﬁle	O
and	O
reloading	O
it	O
,	O
we	O
can	O
simply	O
work	O
backwards	O
from	O
the	O
end	O
of	O
the	O
ﬁle	O
,	O
which	O
is	O
already	O
in	O
memory	O
.	O
we	O
then	O
repeat	O
this	O
forwards-backwards	B
pattern	O
over	O
the	O
data	O
.	O
this	O
simple	O
trick	O
is	O
known	O
as	O
rocking	B
.	O
262	O
chapter	O
8.	O
logistic	B
regression	I
8.5.1	O
online	B
learning	I
and	O
regret	B
minimization	O
suppose	O
that	O
at	O
each	O
step	O
,	O
“	O
nature	O
”	O
presents	O
a	O
sample	O
zk	O
and	O
the	O
“	O
learner	O
”	O
must	O
respond	O
with	O
a	O
parameter	B
estimate	O
θk	O
.	O
in	O
the	O
theoretical	O
machine	B
learning	I
community	O
,	O
the	O
objective	O
used	O
in	O
online	B
learning	I
is	O
the	O
regret	B
,	O
which	O
is	O
the	O
averaged	O
loss	B
incurred	O
relative	O
to	O
the	O
best	O
we	O
could	O
have	O
gotten	O
in	O
hindsight	B
using	O
a	O
single	O
ﬁxed	O
parameter	B
value	O
:	O
regretk	O
(	O
cid:2	O
)	O
1	O
k	O
f	O
(	O
θt	O
,	O
zt	O
)	O
−	O
min	O
θ∗∈θ	O
1	O
k	O
k	O
(	O
cid:2	O
)	O
t=1	O
k	O
(	O
cid:2	O
)	O
t=1	O
f	O
(	O
θ∗	O
,	O
zt	O
)	O
(	O
8.77	O
)	O
for	O
example	O
,	O
imagine	O
we	O
are	O
investing	O
in	O
the	O
stock-market	O
.	O
let	O
θj	O
be	O
the	O
amount	O
we	O
invest	O
in	O
stock	O
j	O
,	O
and	O
let	O
zj	O
be	O
the	O
return	O
on	O
this	O
stock	O
.	O
our	O
loss	B
function	I
is	O
f	O
(	O
θ	O
,	O
z	O
)	O
=	O
−θt	O
z.	O
the	O
regret	B
is	O
how	O
much	O
better	O
(	O
or	O
worse	O
)	O
we	O
did	O
by	O
trading	O
at	O
each	O
step	O
,	O
rather	O
than	O
adopting	O
a	O
“	O
buy	O
and	O
hold	O
”	O
strategy	O
using	O
an	O
oracle	O
to	O
choose	O
which	O
stocks	O
to	O
buy	O
.	O
is	O
as	O
follows	O
:	O
at	O
each	O
step	O
k	O
,	O
update	O
the	O
parameters	O
using	O
one	O
simple	O
algorithm	O
for	O
online	B
learning	I
is	O
online	B
gradient	I
descent	I
(	O
zinkevich	O
2003	O
)	O
,	O
which	O
θk+1	O
=	O
projθ	O
(	O
θk	O
−	O
ηkgk	O
)	O
(	O
8.78	O
)	O
where	O
projv	O
(	O
v	O
)	O
=	O
argminw∈v	O
||w	O
−	O
v||2	O
is	O
the	O
projection	B
of	O
vector	O
v	O
onto	O
space	O
v	O
,	O
gk	O
=	O
∇f	O
(	O
θk	O
,	O
zk	O
)	O
is	O
the	O
gradient	O
,	O
and	O
ηk	O
is	O
the	O
step	B
size	I
.	O
(	O
the	O
projection	B
step	O
is	O
only	O
needed	O
if	O
d.	O
see	O
section	O
13.4.3	O
for	O
the	O
parameter	B
must	O
be	O
constrained	O
to	O
live	O
in	O
a	O
certain	O
subset	O
of	O
r	O
details	O
.	O
)	O
below	O
we	O
will	O
see	O
how	O
this	O
approach	O
to	O
regret	B
minimization	O
relates	O
to	O
more	O
traditional	O
objectives	O
,	O
such	O
as	O
mle	O
.	O
there	O
are	O
a	O
variety	O
of	O
other	O
approaches	O
to	O
regret	B
minimization	O
which	O
are	O
beyond	O
the	O
scope	B
of	O
this	O
book	O
(	O
see	O
e.g.	O
,	O
cesa-bianchi	O
and	O
lugosi	O
(	O
2006	O
)	O
for	O
details	O
)	O
.	O
8.5.2	O
stochastic	B
optimization	I
and	O
risk	B
minimization	O
now	O
suppose	O
that	O
instead	O
of	O
minimizing	O
regret	B
with	O
respect	O
to	O
the	O
past	O
,	O
we	O
want	O
to	O
minimize	O
expected	O
loss	O
in	O
the	O
future	O
,	O
as	O
is	O
more	O
common	O
in	O
(	O
frequentist	B
)	O
statistical	B
learning	I
theory	I
.	O
that	O
is	O
,	O
we	O
want	O
to	O
minimize	O
f	O
(	O
θ	O
)	O
=	O
e	O
[	O
f	O
(	O
θ	O
,	O
z	O
)	O
]	O
(	O
8.79	O
)	O
where	O
the	O
expectation	O
is	O
taken	O
over	O
future	O
data	O
.	O
optimizing	O
functions	O
where	O
some	O
of	O
the	O
variables	O
in	O
the	O
objective	O
are	O
random	O
is	O
called	O
stochastic	O
optimization.2	O
suppose	O
we	O
receive	O
an	O
inﬁnite	O
stream	O
of	O
samples	B
from	O
the	O
distribution	O
.	O
one	O
way	O
to	O
optimize	O
stochastic	O
objectives	O
such	O
as	O
equation	O
8.79	O
is	O
to	O
perform	O
the	O
update	O
in	O
equation	O
8.78	O
at	O
each	O
step	O
.	O
this	O
is	O
called	O
stochastic	B
gradient	I
descent	I
or	O
sgd	O
(	O
nemirovski	O
and	O
yudin	O
1978	O
)	O
.	O
since	O
we	O
typically	O
want	O
a	O
single	O
parameter	O
estimate	O
,	O
we	O
can	O
use	O
a	O
running	O
average	O
:	O
θt	O
(	O
8.80	O
)	O
k	O
(	O
cid:2	O
)	O
t=1	O
θk	O
=	O
1	O
k	O
2.	O
note	O
that	O
in	O
stochastic	B
optimization	I
,	O
the	O
objective	O
is	O
stochastic	O
,	O
and	O
therefore	O
the	O
algorithms	O
will	O
be	O
,	O
too	O
.	O
however	O
,	O
it	O
is	O
also	O
possible	O
to	O
apply	O
stochastic	B
optimization	I
algorithms	O
to	O
deterministic	O
objectives	O
.	O
examples	O
include	O
simulated	B
annealing	I
(	O
section	O
24.6.1	O
)	O
and	O
stochastic	B
gradient	I
descent	I
applied	O
to	O
the	O
empirical	B
risk	I
minimization	I
problem	O
.	O
there	O
are	O
some	O
interesting	O
theoretical	O
connections	O
between	O
online	B
learning	I
and	O
stochastic	B
optimization	I
(	O
cesa-bianchi	O
and	O
lugosi	O
2006	O
)	O
,	O
but	O
this	O
is	O
beyond	O
the	O
scope	B
of	O
this	O
book	O
.	O
8.5.	O
online	B
learning	I
and	O
stochastic	B
optimization	I
this	O
is	O
called	O
polyak-ruppert	O
averaging	O
,	O
and	O
can	O
be	O
implemented	O
recursively	O
as	O
follows	O
:	O
θk	O
=	O
θk−1	O
−	O
1	O
k	O
(	O
θk−1	O
−	O
θk	O
)	O
see	O
e.g.	O
,	O
(	O
spall	O
2003	O
;	O
kushner	O
and	O
yin	O
2003	O
)	O
for	O
details	O
.	O
8.5.2.1	O
setting	O
the	O
step	B
size	I
263	O
(	O
8.81	O
)	O
we	O
now	O
discuss	O
some	O
sufficient	O
conditions	O
on	O
the	O
learning	B
rate	I
to	O
guarantee	O
convergence	O
of	O
sgd	O
.	O
these	O
are	O
known	O
as	O
the	O
robbins-monro	O
conditions	O
:	O
∞	O
(	O
cid:2	O
)	O
∞	O
(	O
cid:2	O
)	O
ηk	O
=	O
∞	O
,	O
k	O
<	O
∞	O
.	O
η2	O
(	O
8.82	O
)	O
k=1	O
k=1	O
the	O
set	O
of	O
values	O
of	O
ηk	O
over	O
time	O
is	O
called	O
the	O
learning	B
rate	I
schedule	O
.	O
various	O
formulas	O
are	O
used	O
,	O
such	O
as	O
ηk	O
=	O
1/k	O
,	O
or	O
the	O
following	O
(	O
bottou	O
1998	O
;	O
bach	O
and	O
moulines	O
2011	O
)	O
:	O
−κ	O
ηk	O
=	O
(	O
τ0	O
+	O
k	O
)	O
(	O
8.83	O
)	O
where	O
τ0	O
≥	O
0	O
slows	O
down	O
early	O
iterations	O
of	O
the	O
algorithm	O
,	O
and	O
κ	O
∈	O
(	O
0.5	O
,	O
1	O
]	O
controls	O
the	O
rate	B
at	O
which	O
old	O
values	O
of	O
are	O
forgotten	O
.	O
the	O
need	O
to	O
adjust	O
these	O
tuning	O
parameters	O
is	O
one	O
of	O
the	O
main	O
drawback	O
of	O
stochastic	B
optimization	I
.	O
one	O
simple	O
heuristic	O
(	O
bottou	O
2007	O
)	O
is	O
as	O
follows	O
:	O
store	O
an	O
initial	O
subset	O
of	O
the	O
data	O
,	O
and	O
try	O
a	O
range	O
of	O
η	O
values	O
on	O
this	O
subset	O
;	O
then	O
choose	O
the	O
one	O
that	O
results	O
in	O
the	O
fastest	O
decrease	O
in	O
the	O
objective	O
and	O
apply	O
it	O
to	O
all	O
the	O
rest	O
of	O
the	O
data	O
.	O
note	O
that	O
this	O
may	O
not	O
result	O
in	O
convergence	O
,	O
but	O
the	O
algorithm	O
can	O
be	O
terminated	O
when	O
the	O
performance	O
improvement	O
on	O
a	O
hold-out	O
set	O
plateaus	O
(	O
this	O
is	O
called	O
early	B
stopping	I
)	O
.	O
8.5.2.2	O
per-parameter	O
step	O
sizes	O
one	O
drawback	O
of	O
sgd	O
is	O
that	O
it	O
uses	O
the	O
same	O
step	B
size	I
for	O
all	O
parameters	O
.	O
we	O
now	O
brieﬂy	O
present	O
a	O
method	O
known	O
as	O
adagrad	B
(	O
short	O
for	O
adaptive	O
gradient	O
)	O
(	O
duchi	O
et	O
al	O
.	O
2010	O
)	O
,	O
which	O
is	O
similar	B
in	O
spirit	O
to	O
a	O
diagonal	B
hessian	O
approximation	O
.	O
(	O
see	O
also	O
(	O
schaul	O
et	O
al	O
.	O
2012	O
)	O
for	O
a	O
similar	B
in	O
particular	O
,	O
if	O
θi	O
(	O
k	O
)	O
is	O
parameter	B
i	O
at	O
time	O
k	O
,	O
and	O
gi	O
(	O
k	O
)	O
is	O
its	O
gradient	O
,	O
then	O
we	O
approach	O
.	O
)	O
make	O
an	O
update	O
as	O
follows	O
:	O
θi	O
(	O
k	O
+	O
1	O
)	O
=	O
θi	O
(	O
k	O
)	O
−	O
η	O
gi	O
(	O
k	O
)	O
(	O
cid:17	O
)	O
(	O
8.84	O
)	O
τ0	O
+	O
si	O
(	O
k	O
)	O
where	O
the	O
diagonal	B
step	O
size	O
vector	O
is	O
the	O
gradient	O
vector	O
squared	O
,	O
summed	O
over	O
all	O
time	O
steps	O
.	O
this	O
can	O
be	O
recursively	O
updated	O
as	O
follows	O
:	O
si	O
(	O
k	O
)	O
=	O
si	O
(	O
k	O
−	O
1	O
)	O
+	O
gi	O
(	O
k	O
)	O
2	O
(	O
8.85	O
)	O
the	O
result	O
is	O
a	O
per-parameter	O
step	B
size	I
that	O
adapts	O
to	O
the	O
curvature	O
of	O
the	O
loss	B
function	I
.	O
this	O
method	O
was	O
original	O
derived	O
for	O
the	O
regret	B
minimization	O
case	O
,	O
but	O
it	O
can	O
be	O
applied	O
more	O
generally	O
.	O
264	O
chapter	O
8.	O
logistic	B
regression	I
8.5.2.3	O
sgd	O
compared	O
to	O
batch	B
learning	O
if	O
we	O
don	O
’	O
t	O
have	O
an	O
inﬁnite	O
data	O
stream	O
,	O
we	O
can	O
“	O
simulate	O
”	O
one	O
by	O
sampling	O
data	O
points	O
at	O
random	O
from	O
our	O
training	B
set	I
.	O
essentially	O
we	O
are	O
optimizing	O
equation	O
8.74	O
by	O
treating	O
it	O
as	O
an	O
expectation	O
with	O
respect	O
to	O
the	O
empirical	B
distribution	I
.	O
algorithm	O
8.3	O
:	O
stochastic	B
gradient	I
descent	I
1	O
initialize	O
θ	O
,	O
η	O
;	O
2	O
repeat	O
3	O
4	O
5	O
randomly	O
permute	O
data	O
;	O
for	O
i	O
=	O
1	O
:	O
n	O
do	O
g	O
=	O
∇f	O
(	O
θ	O
,	O
zi	O
)	O
;	O
θ	O
←	O
projθ	O
(	O
θ	O
−	O
ηg	O
)	O
;	O
update	O
η	O
;	O
8	O
until	O
converged	O
;	O
6	O
7	O
in	O
theory	O
,	O
we	O
should	O
sample	O
with	O
replacement	O
,	O
although	O
in	O
practice	O
it	O
is	O
usually	O
better	O
to	O
randomly	O
permute	O
the	O
data	O
and	O
sample	O
without	O
replacement	O
,	O
and	O
then	O
to	O
repeat	O
.	O
a	O
single	O
such	O
pass	O
over	O
the	O
entire	O
data	O
set	O
is	O
called	O
an	O
epoch	B
.	O
see	O
algorithm	O
8	O
for	O
some	O
pseudocode	O
.	O
in	O
this	O
offline	B
case	O
,	O
it	O
is	O
often	O
better	O
to	O
compute	O
the	O
gradient	O
of	O
a	O
mini-batch	B
of	O
b	O
data	O
cases	O
.	O
if	O
b	O
=	O
1	O
,	O
this	O
is	O
standard	O
sgd	O
,	O
and	O
if	O
b	O
=	O
n	O
,	O
this	O
is	O
standard	O
steepest	O
descent	O
.	O
typically	O
b	O
∼	O
100	O
is	O
used	O
.	O
although	O
a	O
simple	O
ﬁrst-order	O
method	O
,	O
sgd	O
performs	O
surprisingly	O
well	O
on	O
some	O
problems	O
,	O
especially	O
ones	O
with	O
large	O
data	O
sets	O
(	O
bottou	O
2007	O
)	O
.	O
the	O
intuitive	O
reason	O
for	O
this	O
is	O
that	O
one	O
can	O
get	O
a	O
fairly	O
good	O
estimate	O
of	O
the	O
gradient	O
by	O
looking	O
at	O
just	O
a	O
few	O
examples	O
.	O
carefully	O
evaluating	O
precise	O
gradients	O
using	O
large	O
datasets	O
is	O
often	O
a	O
waste	O
of	O
time	O
,	O
since	O
the	O
algorithm	O
will	O
have	O
to	O
recompute	O
the	O
gradient	O
again	O
anyway	O
at	O
the	O
next	O
step	O
.	O
it	O
is	O
often	O
a	O
better	O
use	O
of	O
computer	O
time	O
to	O
have	O
a	O
noisy	O
estimate	O
and	O
to	O
move	O
rapidly	O
through	O
parameter	B
space	O
.	O
as	O
an	O
extreme	O
example	O
,	O
suppose	O
we	O
double	O
the	O
training	B
set	I
by	O
duplicating	O
every	O
example	O
.	O
batch	B
methods	O
will	O
take	O
twice	O
as	O
long	O
,	O
but	O
online	O
methods	O
will	O
be	O
unaffected	O
,	O
since	O
the	O
direction	O
of	O
the	O
gradient	O
has	O
not	O
changed	O
(	O
doubling	O
the	O
size	O
of	O
the	O
data	O
changes	O
the	O
magnitude	O
of	O
the	O
gradient	O
,	O
but	O
that	O
is	O
irrelevant	O
,	O
since	O
the	O
gradient	O
is	O
being	O
scaled	O
by	O
the	O
step	B
size	I
anyway	O
)	O
.	O
in	O
addition	O
to	O
enhanced	O
speed	O
,	O
sgd	O
is	O
often	O
less	O
prone	O
to	O
getting	O
stuck	O
in	O
shallow	O
local	O
minima	O
,	O
because	O
it	O
adds	O
a	O
certain	O
amount	O
of	O
“	O
noise	O
”	O
.	O
consequently	O
it	O
is	O
quite	O
popular	O
in	O
the	O
machine	B
learning	I
community	O
for	O
ﬁtting	O
models	O
with	O
non-convex	O
objectives	O
,	O
such	O
as	O
neural	B
networks	I
(	O
section	O
16.5	O
)	O
and	O
deep	O
belief	O
networks	O
(	O
section	O
28.1	O
)	O
.	O
8.5.3	O
the	O
lms	O
algorithm	O
as	O
an	O
example	O
of	O
sgd	O
,	O
let	O
us	O
consider	O
how	O
to	O
compute	O
the	O
mle	O
for	O
linear	B
regression	I
in	O
an	O
online	O
fashion	O
.	O
we	O
derived	O
the	O
batch	B
gradient	O
in	O
equation	O
7.14.	O
the	O
online	O
gradient	O
at	O
iteration	O
k	O
is	O
given	O
by	O
gk	O
=	O
xi	O
(	O
θt	O
k	O
xi	O
−	O
yi	O
)	O
(	O
8.86	O
)	O
8.5.	O
online	B
learning	I
and	O
stochastic	B
optimization	I
265	O
3	O
2.5	O
2	O
1.5	O
1	O
w	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1	O
black	O
line	O
=	O
lms	O
trajectory	O
towards	O
ls	O
soln	O
(	O
red	O
cross	O
)	O
rss	O
vs	O
iteration	O
10	O
9	O
8	O
7	O
6	O
5	O
4	O
0	O
1	O
w0	O
(	O
a	O
)	O
2	O
3	O
3	O
0	O
5	O
10	O
20	O
25	O
30	O
15	O
(	O
b	O
)	O
figure	O
8.8	O
illustration	O
of	O
the	O
lms	O
algorithm	O
.	O
left	O
:	O
we	O
start	O
from	O
θ	O
=	O
(	O
−0.5	O
,	O
2	O
)	O
and	O
slowly	O
converging	O
to	O
the	O
least	B
squares	I
solution	O
of	O
ˆθ	O
=	O
(	O
1.45	O
,	O
0.92	O
)	O
(	O
red	O
cross	O
)	O
.	O
right	O
:	O
plot	O
of	O
objective	O
function	O
over	O
time	O
.	O
note	O
that	O
it	O
does	O
not	O
decrease	O
monotonically	O
.	O
figure	O
generated	O
by	O
lmsdemo	O
.	O
where	O
i	O
=	O
i	O
(	O
k	O
)	O
is	O
the	O
training	O
example	O
to	O
use	O
at	O
iteration	O
k.	O
if	O
the	O
data	O
set	O
is	O
streaming	O
,	O
we	O
use	O
i	O
(	O
k	O
)	O
=	O
k	O
;	O
we	O
shall	O
assume	O
this	O
from	O
now	O
on	O
,	O
for	O
notational	O
simplicity	O
.	O
equation	O
8.86	O
is	O
easy	O
to	O
interpret	O
:	O
it	O
is	O
the	O
feature	O
vector	O
xk	O
weighted	O
by	O
the	O
difference	O
between	O
what	O
we	O
predicted	O
,	O
ˆyk	O
=	O
θt	O
k	O
xk	O
,	O
and	O
the	O
true	O
response	O
,	O
yk	O
;	O
hence	O
the	O
gradient	O
acts	O
like	O
an	O
error	B
signal	I
.	O
after	O
computing	O
the	O
gradient	O
,	O
we	O
take	O
a	O
step	O
along	O
it	O
as	O
follows	O
:	O
θk+1	O
=	O
θk	O
−	O
ηk	O
(	O
ˆyk	O
−	O
yk	O
)	O
xk	O
(	O
8.87	O
)	O
(	O
there	O
is	O
no	O
need	O
for	O
a	O
projection	B
step	O
,	O
since	O
this	O
is	O
an	O
unconstrained	O
optimization	B
problem	O
.	O
)	O
this	O
algorithm	O
is	O
called	O
the	O
least	B
mean	I
squares	I
or	O
lms	O
algorithm	O
,	O
and	O
is	O
also	O
known	O
as	O
the	O
delta	B
rule	I
,	O
or	O
thewidrow-hoff	O
rule	O
.	O
start	O
at	O
θ	O
=	O
(	O
−0.5	O
,	O
2	O
)	O
and	O
converge	B
(	O
in	O
the	O
sense	O
that	O
||θk	O
−	O
θk−1||2	O
of	O
10	O
figure	O
8.8	O
shows	O
the	O
results	O
of	O
applying	O
this	O
algorithm	O
to	O
the	O
data	O
shown	O
in	O
figure	O
7.2.	O
we	O
2	O
drops	O
below	O
a	O
threshold	O
−2	O
)	O
in	O
about	O
26	O
iterations	O
.	O
note	O
that	O
lms	O
may	O
require	O
multiple	O
passes	O
through	O
the	O
data	O
to	O
ﬁnd	O
the	O
optimum	O
.	O
by	O
contrast	O
,	O
the	O
recursive	B
least	I
squares	I
algorithm	O
,	O
which	O
is	O
based	O
on	O
the	O
kalman	O
ﬁlter	O
and	O
which	O
uses	O
second-order	O
information	O
,	O
ﬁnds	O
the	O
optimum	O
in	O
a	O
single	O
pass	O
(	O
see	O
section	O
18.2.3	O
)	O
.	O
see	O
also	O
exercise	O
7.7	O
.	O
8.5.4	O
the	O
perceptron	B
algorithm	I
now	O
let	O
us	O
consider	O
how	O
to	O
ﬁt	O
a	O
binary	O
logistic	O
regression	B
model	O
in	O
an	O
online	O
manner	O
.	O
the	O
batch	B
gradient	O
was	O
given	O
in	O
equation	O
8.5.	O
in	O
the	O
online	O
case	O
,	O
the	O
weight	O
update	O
has	O
the	O
simple	O
form	O
θk	O
=	O
θk−1	O
−	O
ηkgi	O
=	O
θk−1	O
−	O
ηk	O
(	O
μi	O
−	O
yi	O
)	O
xi	O
(	O
8.88	O
)	O
where	O
μi	O
=	O
p	O
(	O
yi	O
=	O
1|xi	O
,	O
θk	O
)	O
=	O
e	O
[	O
yi|xi	O
,	O
θk	O
]	O
.	O
we	O
see	O
that	O
this	O
has	O
exactly	O
the	O
same	O
form	O
as	O
the	O
lms	O
algorithm	O
.	O
indeed	O
,	O
this	O
property	O
holds	O
for	O
all	O
generalized	O
linear	O
models	O
(	O
section	O
9.3	O
)	O
.	O
266	O
chapter	O
8.	O
logistic	B
regression	I
we	O
now	O
consider	O
an	O
approximation	O
to	O
this	O
algorithm	O
.	O
speciﬁcally	O
,	O
let	O
y∈	O
{	O
0,1	O
}	O
p	O
(	O
y|xi	O
,	O
θ	O
)	O
ˆyi	O
=	O
arg	O
max	O
(	O
8.89	O
)	O
represent	O
the	O
most	O
probable	O
class	O
label	O
.	O
we	O
replace	O
μi	O
=	O
p	O
(	O
y	O
=	O
1|xi	O
,	O
θ	O
)	O
=	O
sigm	O
(	O
θt	O
xi	O
)	O
in	O
the	O
gradient	O
expression	O
with	O
ˆyi	O
.	O
thus	O
the	O
approximate	O
gradient	O
becomes	O
gi	O
≈	O
(	O
ˆyi	O
−	O
yi	O
)	O
xi	O
it	O
will	O
make	O
the	O
algebra	O
simpler	O
if	O
we	O
assume	O
y	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
rather	O
than	O
y	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
case	O
,	O
our	O
prediction	O
becomes	O
ˆyi	O
=	O
sign	O
(	O
θt	O
xi	O
)	O
(	O
8.90	O
)	O
in	O
this	O
(	O
8.91	O
)	O
then	O
if	O
ˆyiyi	O
=	O
−1	O
,	O
we	O
have	O
made	O
an	O
error	O
,	O
but	O
if	O
ˆyiyi	O
=	O
+1	O
,	O
we	O
guessed	O
the	O
right	O
label	O
.	O
at	O
each	O
step	O
,	O
we	O
update	O
the	O
weight	B
vector	I
by	O
adding	O
on	O
the	O
gradient	O
.	O
the	O
key	O
observation	B
is	O
that	O
,	O
if	O
we	O
predicted	O
correctly	O
,	O
then	O
ˆyi	O
=	O
yi	O
,	O
so	O
the	O
(	O
approximate	O
)	O
gradient	O
is	O
zero	O
and	O
we	O
do	O
not	O
change	O
the	O
weight	B
vector	I
.	O
but	O
if	O
xi	O
is	O
misclassiﬁed	O
,	O
we	O
update	O
the	O
weights	O
as	O
follows	O
:	O
if	O
ˆyi	O
=	O
1	O
but	O
yi	O
=	O
−1	O
,	O
then	O
the	O
negative	O
gradient	O
is	O
−	O
(	O
ˆyi	O
−	O
yi	O
)	O
xi	O
=	O
−2xi	O
;	O
and	O
if	O
ˆyi	O
=	O
−1	O
but	O
yi	O
=	O
1	O
,	O
then	O
the	O
negative	O
gradient	O
is	O
−	O
(	O
ˆyi	O
−	O
yi	O
)	O
xi	O
=	O
2xi	O
.	O
we	O
can	O
absorb	O
the	O
factor	B
of	O
2	O
into	O
the	O
learning	B
rate	I
η	O
and	O
just	O
write	O
the	O
update	O
,	O
in	O
the	O
case	O
of	O
a	O
misclassiﬁcation	O
,	O
as	O
θk	O
=	O
θk−1	O
+	O
ηkyixi	O
(	O
8.92	O
)	O
since	O
it	O
is	O
only	O
the	O
sign	O
of	O
the	O
weights	O
that	O
matter	O
,	O
not	O
the	O
magnitude	O
,	O
we	O
will	O
set	O
ηk	O
=	O
1.	O
see	O
algorithm	O
11	O
for	O
the	O
pseudocode	O
.	O
one	O
can	O
show	O
that	O
this	O
method	O
,	O
known	O
as	O
the	O
perceptron	B
algorithm	I
(	O
rosenblatt	O
1958	O
)	O
,	O
will	O
converge	B
,	O
provided	O
the	O
data	O
is	O
linearly	B
separable	I
,	O
i.e.	O
,	O
that	O
there	O
exist	O
parameters	O
θ	O
such	O
that	O
predicting	O
with	O
sign	O
(	O
θt	O
x	O
)	O
achieves	O
0	O
error	O
on	O
the	O
training	B
set	I
.	O
however	O
,	O
if	O
the	O
data	O
is	O
not	O
linearly	O
separable	O
,	O
the	O
algorithm	O
will	O
not	O
converge	O
,	O
and	O
even	O
if	O
it	O
does	O
converge	B
,	O
it	O
may	O
take	O
a	O
long	O
time	O
.	O
there	O
are	O
much	O
better	O
ways	O
to	O
train	O
logistic	B
regression	I
models	O
(	O
such	O
as	O
using	O
proper	O
sgd	O
,	O
without	O
the	O
gradient	O
approximation	O
,	O
or	O
irls	O
,	O
discussed	O
in	O
section	O
8.3.4	O
)	O
.	O
however	O
,	O
the	O
perceptron	B
algorithm	I
is	O
historically	O
important	O
:	O
it	O
was	O
one	O
of	O
the	O
ﬁrst	O
machine	B
learning	I
algorithms	O
ever	O
derived	O
(	O
by	O
frank	O
rosenblatt	O
in	O
1957	O
)	O
,	O
and	O
was	O
even	O
implemented	O
in	O
analog	O
hardware	O
.	O
in	O
addition	O
,	O
the	O
algorithm	O
can	O
be	O
used	O
to	O
ﬁt	O
models	O
where	O
computing	O
marginals	O
p	O
(	O
yi|x	O
,	O
θ	O
)	O
is	O
more	O
expensive	O
than	O
computing	O
the	O
map	O
output	O
,	O
arg	O
maxy	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
;	O
this	O
arises	O
in	O
some	O
structured-output	B
classiﬁcation	I
problems	I
.	O
see	O
section	O
19.7	O
for	O
details	O
.	O
8.5.5	O
a	O
bayesian	O
view	O
another	O
approach	O
to	O
online	B
learning	I
is	O
to	O
adopt	O
a	O
bayesian	O
view	O
.	O
this	O
is	O
conceptually	O
quite	O
simple	O
:	O
we	O
just	O
apply	O
bayes	O
rule	O
recursively	O
:	O
p	O
(	O
θ|d1	O
:	O
k	O
)	O
∝	O
p	O
(	O
dk|θ	O
)	O
p	O
(	O
θ|d1	O
:	O
k−1	O
)	O
(	O
8.93	O
)	O
this	O
has	O
the	O
obvious	O
advantage	O
of	O
returning	O
a	O
posterior	O
instead	O
of	O
just	O
a	O
point	B
estimate	I
.	O
it	O
also	O
allows	O
for	O
the	O
online	O
adaptation	O
of	O
hyper-parameters	B
,	O
which	O
is	O
important	O
since	O
cross-validation	O
can	O
not	O
be	O
used	O
in	O
an	O
online	O
setting	O
.	O
finally	O
,	O
it	O
has	O
the	O
(	O
less	O
obvious	O
)	O
advantage	O
that	O
it	O
can	O
be	O
8.6.	O
generative	O
vs	O
discriminative	B
classiﬁers	O
267	O
d	O
,	O
yi	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
for	O
i	O
=	O
1	O
:	O
n	O
;	O
algorithm	O
8.4	O
:	O
perceptron	B
algorithm	I
1	O
input	O
:	O
linearly	B
separable	I
data	O
set	O
xi	O
∈	O
r	O
2	O
initialize	O
θ0	O
;	O
3	O
k	O
←	O
0	O
;	O
4	O
repeat	O
5	O
k	O
←	O
k	O
+	O
1	O
;	O
i	O
←	O
k	O
mod	O
n	O
;	O
if	O
ˆyi	O
(	O
cid:4	O
)	O
=	O
yi	O
then	O
θk+1	O
←	O
θk	O
+	O
yixi	O
6	O
7	O
8	O
9	O
10	O
else	O
no-op	O
11	O
until	O
converged	O
;	O
quicker	O
than	O
sgd	O
.	O
to	O
see	O
why	O
,	O
note	O
that	O
by	O
modeling	O
the	O
posterior	O
variance	O
of	O
each	O
parameter	B
in	O
addition	O
to	O
its	O
mean	B
,	O
we	O
effectively	O
associate	O
a	O
different	O
learning	B
rate	I
for	O
each	O
parameter	B
(	O
de	O
freitas	O
et	O
al	O
.	O
2000	O
)	O
,	O
which	O
is	O
a	O
simple	O
way	O
to	O
model	O
the	O
curvature	O
of	O
the	O
space	O
.	O
these	O
variances	O
can	O
then	O
be	O
adapted	O
using	O
the	O
usual	O
rules	B
of	O
probability	O
theory	O
.	O
by	O
contrast	O
,	O
getting	O
second-order	O
optimization	O
methods	O
to	O
work	O
online	O
is	O
more	O
tricky	O
(	O
see	O
e.g.	O
,	O
(	O
schraudolph	O
et	O
al	O
.	O
2007	O
;	O
sunehag	O
et	O
al	O
.	O
2009	O
;	O
bordes	O
et	O
al	O
.	O
2009	O
,	O
2010	O
)	O
)	O
.	O
as	O
a	O
simple	O
example	O
,	O
in	O
section	O
18.2.3	O
we	O
show	O
how	O
to	O
use	O
the	O
kalman	O
ﬁlter	O
to	O
ﬁt	O
a	O
linear	B
regression	I
model	O
online	O
.	O
unlike	O
the	O
lms	O
algorithm	O
,	O
this	O
converges	O
to	O
the	O
optimal	O
(	O
offline	B
)	O
answer	O
in	O
a	O
single	O
pass	O
over	O
the	O
data	O
.	O
an	O
extension	B
which	O
can	O
learn	O
a	O
robust	B
non-linear	O
regression	B
model	O
in	O
an	O
online	O
fashion	O
is	O
described	O
in	O
(	O
ting	O
et	O
al	O
.	O
2010	O
)	O
.	O
for	O
the	O
glm	O
case	O
,	O
we	O
can	O
use	O
an	O
assumed	B
density	I
ﬁlter	I
(	O
section	O
18.5.3	O
)	O
,	O
where	O
we	O
approximate	O
the	O
posterior	O
by	O
a	O
gaussian	O
with	O
a	O
diagonal	O
covariance	O
;	O
the	O
variance	B
terms	O
serve	O
as	O
a	O
per-parameter	O
step-size	O
.	O
see	O
section	O
18.5.3.2	O
for	O
details	O
.	O
another	O
approach	O
is	O
to	O
use	O
particle	B
ﬁltering	I
(	O
section	O
23.5	O
)	O
;	O
this	O
was	O
used	O
in	O
(	O
andrieu	O
et	O
al	O
.	O
2000	O
)	O
for	O
sequentially	O
learning	B
a	O
kernelized	O
linear/logistic	O
regression	B
model	O
.	O
8.6	O
generative	O
vs	O
discriminative	B
classiﬁers	O
in	O
section	O
4.2.2	O
,	O
we	O
showed	O
that	O
the	O
posterior	O
over	O
class	O
labels	O
induced	O
by	O
gaussian	O
discrim-	O
inant	O
analysis	O
(	O
gda	O
)	O
has	O
exactly	O
the	O
same	O
form	O
as	O
logistic	B
regression	I
,	O
namely	O
p	O
(	O
y	O
=	O
1|x	O
)	O
=	O
sigm	O
(	O
wt	O
x	O
)	O
.	O
the	O
decision	B
boundary	I
is	O
therefore	O
a	O
linear	O
function	O
of	O
x	O
in	O
both	O
cases	O
.	O
note	O
,	O
however	O
,	O
that	O
many	O
generative	O
models	O
can	O
give	O
rise	O
to	O
a	O
logistic	B
regression	I
posterior	O
,	O
e.g.	O
,	O
if	O
each	O
class-conditional	B
density	I
is	O
poisson	O
,	O
p	O
(	O
x|y	O
=	O
c	O
)	O
=	O
poi	O
(	O
x|λc	O
)	O
.	O
so	O
the	O
assumptions	O
made	O
by	O
gda	O
are	O
much	O
stronger	O
than	O
the	O
assumptions	O
made	O
by	O
logistic	B
regression	I
.	O
(	O
cid:10	O
)	O
n	O
a	O
further	O
difference	O
between	O
these	O
models	O
is	O
the	O
way	O
they	O
are	O
trained	O
.	O
when	O
ﬁtting	O
a	O
discrim-	O
(	O
cid:10	O
)	O
n	O
i=1	O
log	O
p	O
(	O
yi|xi	O
,	O
θ	O
)	O
,	O
whereas	O
i=1	O
log	O
p	O
(	O
yi	O
,	O
xi|θ	O
)	O
.	O
inative	O
model	O
,	O
we	O
usually	O
maximize	O
the	O
conditional	O
log	O
likelihood	B
when	O
ﬁtting	O
a	O
generative	O
model	O
,	O
we	O
usually	O
maximize	O
the	O
joint	O
log	O
likelihood	B
,	O
it	O
is	O
clear	O
that	O
these	O
can	O
,	O
in	O
general	O
,	O
give	O
different	O
results	O
(	O
see	O
exercise	O
4.20	O
)	O
.	O
when	O
the	O
gaussian	O
assumptions	O
made	O
by	O
gda	O
are	O
correct	O
,	O
the	O
model	O
will	O
need	O
less	O
training	O
data	O
than	O
logistic	B
regression	I
to	O
achieve	O
a	O
certain	O
level	O
of	O
performance	O
,	O
but	O
if	O
the	O
gaussian	O
268	O
chapter	O
8.	O
logistic	B
regression	I
assumptions	O
are	O
incorrect	O
,	O
logistic	B
regression	I
will	O
do	O
better	O
(	O
ng	O
and	O
jordan	O
2002	O
)	O
.	O
this	O
is	O
because	O
discriminative	B
models	O
do	O
not	O
need	O
to	O
model	O
the	O
distribution	O
of	O
the	O
features	B
.	O
this	O
is	O
illustrated	O
in	O
figure	O
8.10.	O
we	O
see	O
that	O
the	O
class	O
conditional	O
densities	O
are	O
rather	O
complex	O
;	O
in	O
particular	O
,	O
p	O
(	O
x|y	O
=	O
1	O
)	O
is	O
a	O
multimodal	O
distribution	O
,	O
which	O
might	O
be	O
hard	O
to	O
estimate	O
.	O
however	O
,	O
the	O
class	O
posterior	O
,	O
p	O
(	O
y	O
=	O
c|x	O
)	O
,	O
is	O
a	O
simple	O
sigmoidal	O
function	O
,	O
centered	O
on	O
the	O
threshold	O
value	O
of	O
0.55.	O
this	O
suggests	O
that	O
,	O
in	O
general	O
,	O
discriminative	B
methods	O
will	O
be	O
more	O
accurate	O
,	O
since	O
their	O
“	O
job	O
”	O
is	O
in	O
some	O
sense	O
easier	O
.	O
however	O
,	O
accuracy	O
is	O
not	O
the	O
only	O
important	O
factor	B
when	O
choosing	O
a	O
method	O
.	O
below	O
we	O
discuss	O
some	O
other	O
advantages	O
and	O
disadvantages	O
of	O
each	O
approach	O
.	O
8.6.1	O
pros	O
and	O
cons	O
of	O
each	O
approach	O
•	O
easy	O
to	O
ﬁt	O
?	O
as	O
we	O
have	O
seen	O
,	O
it	O
is	O
usually	O
very	O
easy	O
to	O
ﬁt	O
generative	O
classiﬁers	O
.	O
for	O
example	O
,	O
in	O
sections	O
3.5.1.1	O
and	O
4.2.4	O
,	O
we	O
show	O
that	O
we	O
can	O
ﬁt	O
a	O
naive	O
bayes	O
model	O
and	O
an	O
lda	O
model	O
by	O
simple	O
counting	O
and	O
averaging	O
.	O
by	O
contrast	O
,	O
logistic	B
regression	I
requires	O
solving	O
a	O
convex	B
optimization	O
problem	O
(	O
see	O
section	O
8.3.4	O
for	O
the	O
details	O
)	O
,	O
which	O
is	O
much	O
slower	O
.	O
•	O
fit	O
classes	O
separately	O
?	O
in	O
a	O
generative	B
classiﬁer	I
,	O
we	O
estimate	O
the	O
parameters	O
of	O
each	O
class	O
conditional	O
density	O
independently	O
,	O
so	O
we	O
do	O
not	O
have	O
to	O
retrain	O
the	O
model	O
when	O
we	O
add	O
more	O
classes	O
.	O
in	O
contrast	O
,	O
in	O
discriminative	B
models	O
,	O
all	O
the	O
parameters	O
interact	O
,	O
so	O
the	O
whole	O
model	O
must	O
be	O
retrained	O
if	O
we	O
add	O
a	O
new	O
class	O
.	O
(	O
this	O
is	O
also	O
the	O
case	O
if	O
we	O
train	O
a	O
generative	O
model	O
to	O
maximize	O
a	O
discriminative	B
objective	O
salojarvi	O
et	O
al	O
.	O
(	O
2005	O
)	O
.	O
)	O
•	O
handle	O
missing	B
features	O
easily	O
?	O
sometimes	O
some	O
of	O
the	O
inputs	O
(	O
components	O
of	O
x	O
)	O
are	O
not	O
observed	O
.	O
in	O
a	O
generative	B
classiﬁer	I
,	O
there	O
is	O
a	O
simple	O
method	O
for	O
dealing	O
with	O
this	O
,	O
as	O
we	O
discuss	O
in	O
section	O
8.6.2.	O
however	O
,	O
in	O
a	O
discriminative	B
classiﬁer	I
,	O
there	O
is	O
no	O
principled	O
solution	O
to	O
this	O
problem	O
,	O
since	O
the	O
model	O
assumes	O
that	O
x	O
is	O
always	O
available	O
to	O
be	O
conditioned	O
on	O
(	O
although	O
see	O
(	O
marlin	O
2008	O
)	O
for	O
some	O
heuristic	O
approaches	O
)	O
.	O
•	O
can	O
handle	O
unlabeled	O
training	O
data	O
?	O
there	O
is	O
much	O
interest	O
in	O
semi-supervised	B
learning	I
,	O
which	O
uses	O
unlabeled	O
data	O
to	O
help	O
solve	O
a	O
supervised	O
task	O
.	O
this	O
is	O
fairly	O
easy	O
to	O
do	O
using	O
generative	O
models	O
(	O
see	O
e.g.	O
,	O
(	O
lasserre	O
et	O
al	O
.	O
2006	O
;	O
liang	O
et	O
al	O
.	O
2007	O
)	O
)	O
,	O
but	O
is	O
much	O
harder	O
to	O
do	O
with	O
discriminative	B
models	O
.	O
•	O
symmetric	B
in	O
inputs	O
and	O
outputs	O
?	O
we	O
can	O
run	O
a	O
generative	O
model	O
“	O
backwards	O
”	O
,	O
and	O
infer	O
probable	O
inputs	O
given	O
the	O
output	O
by	O
computing	O
p	O
(	O
x|y	O
)	O
.	O
this	O
is	O
not	O
possible	O
with	O
a	O
discriminative	B
model	O
.	O
the	O
reason	O
is	O
that	O
a	O
generative	O
model	O
deﬁnes	O
a	O
joint	B
distribution	I
on	O
x	O
and	O
y	O
,	O
and	O
hence	O
treats	O
both	O
inputs	O
and	O
outputs	O
symmetrically	O
.	O
•	O
can	O
handle	O
feature	O
preprocessing	O
?	O
a	O
big	O
advantage	O
of	O
discriminative	B
methods	O
is	O
that	O
they	O
allow	O
us	O
to	O
preprocess	O
the	O
input	O
in	O
arbitrary	O
ways	O
,	O
e.g.	O
,	O
we	O
can	O
replace	O
x	O
with	O
φ	O
(	O
x	O
)	O
,	O
which	O
could	O
be	O
some	O
basis	B
function	I
expansion	I
,	O
as	O
illustrated	O
in	O
figure	O
8.9.	O
it	O
is	O
often	O
hard	O
to	O
deﬁne	O
a	O
generative	O
model	O
on	O
such	O
pre-processed	O
data	O
,	O
since	O
the	O
new	O
features	B
are	O
correlated	O
in	O
complex	O
ways	O
.	O
•	O
well-calibrated	O
probabilities	O
?	O
some	O
generative	O
models	O
,	O
such	O
as	O
naive	O
bayes	O
,	O
make	O
strong	O
independence	O
assumptions	O
which	O
are	O
often	O
not	O
valid	O
.	O
this	O
can	O
result	O
in	O
very	O
extreme	O
poste-	O
rior	O
class	O
probabilities	O
(	O
very	O
near	O
0	O
or	O
1	O
)	O
.	O
discriminative	B
models	O
,	O
such	O
as	O
logistic	B
regression	I
,	O
are	O
usually	O
better	O
calibrated	O
in	O
terms	O
of	O
their	O
probability	O
estimates	O
.	O
we	O
see	O
that	O
there	O
are	O
arguments	O
for	O
and	O
against	O
both	O
kinds	O
of	O
models	O
.	O
it	O
is	O
therefore	O
useful	O
to	O
have	O
both	O
kinds	O
in	O
your	O
“	O
toolbox	O
”	O
.	O
see	O
table	O
8.1	O
for	O
a	O
summary	O
of	O
the	O
classiﬁcation	B
and	O
8.6.	O
generative	O
vs	O
discriminative	B
classiﬁers	O
269	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1	O
linear	O
multinomial	O
logistic	B
regression	I
kernel−rbf	O
multinomial	B
logistic	I
regression	I
1	O
0.5	O
0	O
−0.5	O
−0.5	O
0	O
(	O
a	O
)	O
0.5	O
1	O
−1	O
−1	O
−0.5	O
0.5	O
1	O
0	O
(	O
b	O
)	O
figure	O
8.9	O
(	O
a	O
)	O
multinomial	B
logistic	I
regression	I
for	O
5	O
classes	O
in	O
the	O
original	O
feature	O
space	O
.	O
(	O
b	O
)	O
after	O
basis	B
function	I
expansion	I
,	O
using	O
rbf	O
kernels	O
with	O
a	O
bandwidth	B
of	O
1	O
,	O
and	O
using	O
all	O
the	O
data	O
points	O
as	O
centers	O
.	O
figure	O
generated	O
by	O
logregmultinomkerneldemo	O
.	O
s	O
e	O
i	O
t	O
i	O
s	O
n	O
e	O
d	O
l	O
a	O
n	O
o	O
i	O
t	O
i	O
d	O
n	O
o	O
c	O
s	O
s	O
a	O
c	O
l	O
5	O
4	O
3	O
2	O
1	O
0	O
0	O
p	O
(	O
x|y=1	O
)	O
0.2	O
0.4	O
x	O
(	O
a	O
)	O
p	O
(	O
y=1|x	O
)	O
p	O
(	O
y=2|x	O
)	O
p	O
(	O
x|y=2	O
)	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0.6	O
0.8	O
1	O
0	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
x	O
(	O
b	O
)	O
figure	O
8.10	O
the	O
class-conditional	O
densities	O
p	O
(	O
x|y	O
=	O
c	O
)	O
(	O
left	O
)	O
may	O
be	O
more	O
complex	O
than	O
the	O
class	O
posteriors	O
p	O
(	O
y	O
=	O
c|x	O
)	O
(	O
right	O
)	O
.	O
figure	O
generated	O
by	O
generativevsdiscrim	O
.	O
based	O
on	O
figure	O
1.27	O
of	O
(	O
bishop	O
2006a	O
)	O
.	O
regression	B
techniques	O
we	O
cover	O
in	O
this	O
book	O
.	O
8.6.2	O
dealing	O
with	O
missing	B
data	I
sometimes	O
some	O
of	O
the	O
inputs	O
(	O
components	O
of	O
x	O
)	O
are	O
not	O
observed	O
;	O
this	O
could	O
be	O
due	O
to	O
a	O
sensor	O
failure	O
,	O
or	O
a	O
failure	O
to	O
complete	B
an	O
entry	O
in	O
a	O
survey	O
,	O
etc	O
.	O
this	O
is	O
called	O
the	O
missing	B
data	I
problem	I
(	O
little	O
.	O
and	O
rubin	O
1987	O
)	O
.	O
the	O
ability	O
to	O
handle	O
missing	B
data	I
in	O
a	O
principled	O
way	O
is	O
one	O
of	O
the	O
biggest	O
advantages	O
of	O
generative	O
models	O
.	O
to	O
formalize	O
our	O
assumptions	O
,	O
we	O
can	O
associate	O
a	O
binary	O
response	O
variable	O
ri	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
that	O
speciﬁes	O
whether	O
each	O
value	O
xi	O
the	O
joint	O
model	O
has	O
the	O
form	O
p	O
(	O
xi	O
,	O
ri|θ	O
,	O
φ	O
)	O
=	O
p	O
(	O
ri|xi	O
,	O
φ	O
)	O
p	O
(	O
xi|θ	O
)	O
,	O
where	O
φ	O
are	O
the	O
parameters	O
controlling	O
whether	O
the	O
item	O
is	O
observed	O
or	O
not	O
.	O
270	O
chapter	O
8.	O
logistic	B
regression	I
model	O
discriminant	B
analysis	I
naive	O
bayes	O
classiﬁer	O
tree-augmented	O
naive	O
bayes	O
classiﬁer	O
linear	B
regression	I
logistic	O
regression	B
sparse	O
linear/	O
logistic	B
regression	I
mixture	O
of	O
experts	O
multilayer	O
perceptron	B
(	O
mlp	O
)	O
/	O
neural	B
network	I
conditional	O
random	O
ﬁeld	O
(	O
crf	O
)	O
k	O
nearest	B
neighbor	I
classiﬁer	O
(	O
inﬁnite	O
)	O
mixture	B
discriminant	O
analysis	O
classiﬁcation	O
and	O
regression	B
trees	O
(	O
cart	O
)	O
boosted	O
model	O
sparse	O
kernelized	O
lin/logreg	O
(	O
sklr	O
)	O
relevance	B
vector	I
machine	I
(	O
rvm	O
)	O
support	B
vector	I
machine	I
(	O
svm	O
)	O
gaussian	O
processes	O
(	O
gp	O
)	O
smoothing	O
splines	O
classif/regr	O
classif	O
classif	O
classif	O
regr	O
classif	O
both	O
both	O
both	O
classif	O
classif	O
classif	O
both	O
both	O
both	O
both	O
both	O
both	O
regr	O
gen/discr	O
gen	O
gen	O
gen	O
discrim	O
discrim	O
discrim	O
discrim	O
discrim	O
discrim	O
gen	O
gen	O
discrim	O
discrim	O
discrim	O
discrim	O
discrim	O
discrim	O
discrim	O
param/non	O
param	O
param	O
param	O
param	O
param	O
param	O
param	O
param	O
param	O
non	O
non	O
non	O
non	O
non	O
non	O
non	O
non	O
non	O
section	O
sec	O
.	O
4.2.2	O
,	O
4.2.4	O
sec	O
.	O
3.5	O
,	O
3.5.1.2	O
sec	O
.	O
10.2.1	O
sec	O
.	O
1.4.5	O
,	O
7.3	O
,	O
7.6	O
,	O
sec	O
.	O
1.4.6	O
,	O
8.3.4	O
,	O
8.4.3	O
,	O
21.8.1.1	O
ch	O
.	O
13	O
sec	O
.	O
11.2.4	O
ch	O
.	O
16	O
sec	O
.	O
19.6	O
sec	O
.	O
1.4.2	O
,	O
14.7.3	O
sec	O
.	O
14.7.3	O
sec	O
.	O
16.2	O
sec	O
.	O
16.4	O
sec	O
.	O
14.3.2	O
sec	O
.	O
14.3.2	O
sec	O
.	O
14.5	O
ch	O
.	O
15	O
section	O
15.4.6	O
is	O
the	O
model	O
suitable	O
for	O
classiﬁcation	B
,	O
regression	B
,	O
or	O
both	O
;	O
table	O
8.1	O
list	O
of	O
various	O
models	O
for	O
classiﬁcation	B
and	O
regression	B
which	O
we	O
discuss	O
in	O
this	O
book	O
.	O
columns	O
are	O
as	O
follows	O
:	O
model	O
name	O
;	O
is	O
the	O
model	O
generative	O
or	O
discriminative	B
;	O
is	O
the	O
model	O
parametric	O
or	O
non-parametric	O
;	O
list	O
of	O
sections	O
in	O
book	O
which	O
discuss	O
the	O
model	O
.	O
see	O
also	O
http	O
:	O
//pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tu	O
tsupervised.html	O
for	O
the	O
pmtk	O
equivalents	O
of	O
these	O
models	O
.	O
any	O
generative	O
probabilistic	O
model	O
(	O
e.g.	O
,	O
hmms	O
,	O
boltzmann	O
machines	O
,	O
bayesian	O
networks	O
,	O
etc	O
.	O
)	O
can	O
be	O
turned	O
into	O
a	O
classiﬁer	O
by	O
using	O
it	O
as	O
a	O
class	O
conditional	O
density	O
.	O
is	O
observed	O
or	O
not	O
.	O
if	O
we	O
assume	O
p	O
(	O
ri|xi	O
,	O
φ	O
)	O
=	O
p	O
(	O
ri|φ	O
)	O
,	O
we	O
say	O
the	O
data	O
is	O
missing	B
completely	I
at	I
random	I
or	O
mcar	O
.	O
if	O
we	O
assume	O
p	O
(	O
ri|xi	O
,	O
φ	O
)	O
=	O
p	O
(	O
ri|xo	O
i	O
is	O
the	O
observed	O
part	O
of	O
xi	O
,	O
we	O
say	O
the	O
data	O
is	O
missing	B
at	I
random	I
or	O
mar	O
.	O
if	O
neither	O
of	O
these	O
assumptions	O
hold	O
,	O
we	O
say	O
the	O
data	O
is	O
not	B
missing	I
at	I
random	I
or	O
nmar	O
.	O
in	O
this	O
case	O
,	O
we	O
have	O
to	O
model	O
the	O
missing	B
data	I
mechanism	O
,	O
since	O
the	O
pattern	B
of	O
missingness	O
is	O
informative	O
about	O
the	O
values	O
of	O
the	O
missing	B
data	I
and	O
the	O
corresponding	O
parameters	O
.	O
this	O
is	O
the	O
case	O
in	O
most	O
collaborative	B
ﬁltering	I
problems	O
,	O
for	O
example	O
.	O
see	O
e.g.	O
,	O
(	O
marlin	O
2008	O
)	O
for	O
further	O
discussion	O
.	O
we	O
will	O
henceforth	O
assume	O
the	O
data	O
is	O
mar	O
.	O
i	O
,	O
φ	O
)	O
,	O
where	O
xo	O
when	O
dealing	O
with	O
missing	B
data	I
,	O
it	O
is	O
helpful	O
to	O
distinguish	O
the	O
cases	O
when	O
there	O
is	O
missing-	O
ness	O
only	O
at	O
test	O
time	O
(	O
so	O
the	O
training	O
data	O
is	O
complete	B
data	I
)	O
,	O
from	O
the	O
harder	O
case	O
when	O
there	O
is	O
missingness	O
also	O
at	O
training	O
time	O
.	O
we	O
will	O
discuss	O
these	O
two	O
cases	O
below	O
.	O
note	O
that	O
the	O
class	O
label	O
is	O
always	O
missing	O
at	O
test	O
time	O
,	O
by	O
deﬁnition	O
;	O
if	O
the	O
class	O
label	O
is	O
also	O
sometimes	O
missing	O
at	O
training	O
time	O
,	O
the	O
problem	O
is	O
called	O
semi-supervised	B
learning	I
.	O
8.6.	O
generative	O
vs	O
discriminative	B
classiﬁers	O
271	O
8.6.2.1	O
missing	B
data	I
at	O
test	O
time	O
in	O
a	O
generative	B
classiﬁer	I
,	O
we	O
can	O
handle	O
features	B
that	O
are	O
mar	O
by	O
marginalizing	O
them	O
out	O
.	O
for	O
example	O
,	O
if	O
we	O
are	O
missing	B
the	O
value	O
of	O
x1	O
,	O
we	O
can	O
compute	O
p	O
(	O
y	O
=	O
c|x2	O
:	O
d	O
,	O
θ	O
)	O
∝	O
p	O
(	O
y	O
=	O
c|θ	O
)	O
p	O
(	O
x2	O
:	O
d|y	O
=	O
c	O
,	O
θ	O
)	O
=	O
p	O
(	O
y	O
=	O
c|θ	O
)	O
p	O
(	O
x1	O
,	O
x2	O
:	O
d|y	O
=	O
c	O
,	O
θ	O
)	O
(	O
cid:2	O
)	O
x1	O
(	O
8.94	O
)	O
(	O
8.95	O
)	O
if	O
we	O
make	O
the	O
naive	O
bayes	O
assumption	O
,	O
the	O
marginalization	O
can	O
be	O
performed	O
as	O
follows	O
:	O
(	O
cid:19	O
)	O
d	O
(	O
cid:27	O
)	O
d	O
(	O
cid:27	O
)	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
x1	O
(	O
cid:2	O
)	O
x1	O
p	O
(	O
x1	O
,	O
x2	O
:	O
d|y	O
=	O
c	O
,	O
θ	O
)	O
=	O
p	O
(	O
x1|θ1c	O
)	O
p	O
(	O
xj|θjc	O
)	O
=	O
p	O
(	O
xj|θjc	O
)	O
(	O
8.96	O
)	O
j=2	O
j=2	O
p	O
(	O
x1|y	O
=	O
c	O
,	O
θ	O
)	O
=	O
1.	O
hence	O
in	O
a	O
naive	O
bayes	O
classiﬁer	O
,	O
we	O
where	O
we	O
exploited	O
the	O
fact	O
that	O
can	O
simply	O
ignore	O
missing	B
features	O
at	O
test	O
time	O
.	O
similarly	O
,	O
in	O
discriminant	B
analysis	I
,	O
no	O
matter	O
what	O
regularization	B
method	O
was	O
used	O
to	O
estimate	O
the	O
parameters	O
,	O
we	O
can	O
always	O
analytically	O
marginalize	O
out	O
the	O
missing	B
variables	O
(	O
see	O
section	O
4.3	O
)	O
:	O
p	O
(	O
x2	O
:	O
d|y	O
=	O
c	O
,	O
θ	O
)	O
=	O
n	O
(	O
x2	O
:	O
d|μc,2	O
:	O
d	O
,	O
σc,2	O
:	O
d,2	O
:	O
d	O
)	O
(	O
8.97	O
)	O
x1	O
8.6.2.2	O
missing	B
data	I
at	O
training	O
time	O
missing	B
data	I
at	O
training	O
time	O
is	O
harder	O
to	O
deal	O
with	O
.	O
in	O
particular	O
,	O
computing	O
the	O
mle	O
or	O
map	O
estimate	O
is	O
no	O
longer	O
a	O
simple	O
optimization	O
problem	O
,	O
for	O
reasons	O
discussed	O
in	O
section	O
11.3.2.	O
however	O
,	O
soon	O
we	O
will	O
study	O
are	O
a	O
variety	O
of	O
more	O
sophisticated	O
algorithms	O
(	O
such	O
as	O
em	O
algo-	O
rithm	O
,	O
in	O
section	O
11.4	O
)	O
for	O
ﬁnding	O
approximate	O
ml	O
or	O
map	O
estimates	O
in	O
such	O
cases	O
.	O
8.6.3	O
fisher	O
’	O
s	O
linear	B
discriminant	I
analysis	I
(	O
flda	O
)	O
*	O
discriminant	B
analysis	I
is	O
a	O
generative	B
approach	I
to	O
classiﬁcation	B
,	O
which	O
requires	O
ﬁtting	O
an	O
mvn	O
to	O
the	O
features	B
.	O
as	O
we	O
have	O
discussed	O
,	O
this	O
can	O
be	O
problematic	O
in	O
high	O
dimensions	O
.	O
an	O
alternative	O
approach	O
is	O
to	O
reduce	O
the	O
dimensionality	O
of	O
the	O
features	B
x	O
∈	O
r	O
d	O
and	O
then	O
ﬁt	O
an	O
mvn	O
to	O
the	O
resulting	O
low-dimensional	O
features	B
z	O
∈	O
r	O
l.	O
the	O
simplest	O
approach	O
is	O
to	O
use	O
a	O
linear	O
projection	O
matrix	O
,	O
z	O
=	O
wx	O
,	O
where	O
w	O
is	O
a	O
l	O
×	O
d	O
matrix	O
.	O
one	O
approach	O
to	O
ﬁnding	O
w	O
would	O
be	O
to	O
use	O
pca	O
(	O
section	O
12.2	O
)	O
;	O
the	O
result	O
would	O
be	O
very	O
similar	B
to	O
rda	O
(	O
section	O
4.2.6	O
)	O
,	O
since	O
svd	O
and	O
pca	O
are	O
essentially	O
equivalent	O
.	O
however	O
,	O
pca	O
is	O
an	O
unsupervised	O
technique	O
that	O
does	O
not	O
take	O
class	O
labels	O
into	O
account	O
.	O
thus	O
the	O
resulting	O
low	O
dimensional	O
features	B
are	O
not	O
necessarily	O
optimal	O
for	O
classiﬁcation	B
,	O
as	O
illustrated	O
in	O
figure	O
8.11.	O
an	O
alternative	O
approach	O
is	O
to	O
ﬁnd	O
the	O
matrix	O
w	O
such	O
that	O
the	O
low-dimensional	O
data	O
can	O
be	O
classiﬁed	O
as	O
well	O
as	O
possible	O
using	O
a	O
gaussian	O
class-conditional	B
density	I
model	O
.	O
the	O
assumption	O
of	O
gaussianity	O
is	O
reasonable	O
since	O
we	O
are	O
computing	O
linear	O
combinations	O
of	O
(	O
potentially	O
non-gaussian	O
)	O
features	B
.	O
this	O
approach	O
is	O
called	O
fisher	O
’	O
s	O
linear	B
discriminant	I
analysis	I
,	O
orflda	O
.	O
flda	O
is	O
an	O
interesting	O
hybrid	O
of	O
discriminative	B
and	O
generative	O
techniques	O
.	O
the	O
drawback	O
of	O
this	O
technique	O
is	O
that	O
it	O
is	O
restricted	O
to	O
using	O
l	O
≤	O
c	O
−	O
1	O
dimensions	O
,	O
regardless	O
of	O
d	O
,	O
for	O
reasons	O
that	O
we	O
will	O
explain	O
below	O
.	O
in	O
the	O
two-class	O
case	O
,	O
this	O
means	O
we	O
are	O
seeking	O
a	O
single	O
vector	O
w	O
onto	O
which	O
we	O
can	O
project	O
the	O
data	O
.	O
below	O
we	O
derive	O
the	O
optimal	O
w	O
in	O
the	O
two-class	O
case	O
.	O
we	O
272	O
chapter	O
8.	O
logistic	B
regression	I
4	O
3	O
2	O
1	O
0	O
−4	O
20	O
15	O
10	O
5	O
0	O
−45	O
25	O
20	O
15	O
10	O
5	O
0	O
−8	O
means	O
fisher	O
pca	O
−2	O
0	O
2	O
4	O
6	O
8	O
(	O
a	O
)	O
fisher	O
−40	O
−35	O
−30	O
−25	O
−20	O
−15	O
−10	O
−5	O
0	O
(	O
b	O
)	O
pca	O
−6	O
−4	O
−2	O
0	O
2	O
4	O
(	O
c	O
)	O
figure	O
8.11	O
example	O
of	O
fisher	O
’	O
s	O
linear	O
discriminant	O
.	O
(	O
a	O
)	O
two	O
class	O
data	O
in	O
2d	O
.	O
dashed	O
green	O
line	O
=	O
ﬁrst	O
principal	O
basis	O
vector	O
.	O
dotted	O
red	O
line	O
=	O
fisher	O
’	O
s	O
linear	O
discriminant	O
vector	O
.	O
solid	O
black	O
line	O
joins	O
the	O
class-conditional	O
means	O
.	O
(	O
c	O
)	O
projection	B
of	O
points	O
onto	O
pca	O
vector	O
shows	O
poor	O
class	O
separation	O
.	O
figure	O
generated	O
by	O
fisherldademo	O
.	O
(	O
b	O
)	O
projection	B
of	O
points	O
onto	O
fisher	O
’	O
s	O
vector	O
shows	O
good	O
class	O
separation	O
.	O
8.6.	O
generative	O
vs	O
discriminative	B
classiﬁers	O
273	O
then	O
generalize	B
to	O
the	O
multi-class	O
case	O
,	O
and	O
ﬁnally	O
we	O
give	O
a	O
probabilistic	O
interpretation	O
of	O
this	O
technique	O
.	O
8.6.3.1	O
derivation	O
of	O
the	O
optimal	O
1d	O
projection	B
we	O
now	O
derive	O
this	O
optimal	O
direction	O
w	O
,	O
for	O
the	O
two-class	O
case	O
,	O
following	O
the	O
presentation	O
of	O
(	O
bishop	O
2006b	O
,	O
sec	O
4.1.4	O
)	O
.	O
deﬁne	O
the	O
class-conditional	O
means	O
as	O
xi	O
(	O
8.98	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
μ1	O
=	O
1	O
n1	O
xi	O
,	O
μ2	O
=	O
1	O
n2	O
i	O
:	O
yi=1	O
i	O
:	O
yi=2	O
let	O
mk	O
=	O
wt	O
μk	O
be	O
the	O
projection	B
of	O
each	O
mean	B
onto	O
the	O
line	O
w.	O
also	O
,	O
let	O
zi	O
=	O
wt	O
xi	O
be	O
the	O
projection	B
of	O
the	O
data	O
onto	O
the	O
line	O
.	O
the	O
variance	B
of	O
the	O
projected	O
points	O
is	O
proportional	O
to	O
s2	O
k	O
=	O
(	O
zi	O
−	O
mk	O
)	O
2	O
(	O
8.99	O
)	O
(	O
cid:2	O
)	O
i	O
:	O
yi=k	O
the	O
goal	O
is	O
to	O
ﬁnd	O
w	O
such	O
that	O
we	O
maximize	O
the	O
distance	O
between	O
the	O
means	O
,	O
m2	O
−	O
m1	O
,	O
while	O
also	O
ensuring	O
the	O
projected	O
clusters	O
are	O
“	O
tight	O
”	O
:	O
j	O
(	O
w	O
)	O
=	O
(	O
m2	O
−	O
m1	O
)	O
2	O
1	O
+	O
s2	O
s2	O
2	O
we	O
can	O
rewrite	O
the	O
right	O
hand	O
side	O
of	O
the	O
above	O
in	O
terms	O
of	O
w	O
as	O
follows	O
j	O
(	O
w	O
)	O
=	O
wt	O
sbw	O
wt	O
sw	O
w	O
where	O
sb	O
is	O
the	O
between-class	O
scatter	O
matrix	O
given	O
by	O
sb	O
=	O
(	O
μ2	O
−	O
μ1	O
)	O
(	O
μ2	O
−	O
μ1	O
)	O
t	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
and	O
sw	O
is	O
the	O
within-class	O
scatter	O
matrix	O
,	O
given	O
by	O
sw	O
=	O
(	O
xi	O
−	O
μ1	O
)	O
(	O
xi	O
−	O
μ1	O
)	O
t	O
+	O
(	O
xi	O
−	O
μ2	O
)	O
(	O
xi	O
−	O
μ2	O
)	O
t	O
i	O
:	O
yi=1	O
i	O
:	O
yi=2	O
to	O
see	O
this	O
,	O
note	O
that	O
wt	O
sbw	O
=	O
wt	O
(	O
μ2	O
−	O
μ1	O
)	O
(	O
μ2	O
−	O
μ1	O
)	O
t	O
w	O
=	O
(	O
m2	O
−	O
m1	O
)	O
(	O
m2	O
−	O
m1	O
)	O
and	O
wt	O
sw	O
w	O
=	O
=	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
:	O
yi=1	O
i	O
:	O
yi=1	O
wt	O
(	O
xi	O
−	O
μ1	O
)	O
(	O
xi	O
−	O
μ1	O
)	O
t	O
w	O
+	O
(	O
zi	O
−	O
m1	O
)	O
2	O
+	O
(	O
zi	O
−	O
m2	O
)	O
2	O
(	O
cid:2	O
)	O
i	O
:	O
yi=2	O
i	O
:	O
yi=2	O
(	O
8.100	O
)	O
(	O
8.101	O
)	O
(	O
8.102	O
)	O
(	O
8.103	O
)	O
(	O
8.104	O
)	O
(	O
8.106	O
)	O
(	O
cid:2	O
)	O
wt	O
(	O
xi	O
−	O
μ2	O
)	O
(	O
xi	O
−	O
μ2	O
)	O
t	O
w	O
(	O
8.105	O
)	O
equation	O
8.101	O
is	O
a	O
ratio	O
of	O
two	O
scalars	O
;	O
we	O
can	O
take	O
its	O
derivative	O
with	O
respect	O
to	O
w	O
and	O
equate	O
to	O
zero	O
.	O
one	O
can	O
show	O
(	O
exercise	O
12.6	O
)	O
that	O
that	O
j	O
(	O
w	O
)	O
is	O
maximized	O
when	O
sbw	O
=	O
λsw	O
w	O
(	O
8.107	O
)	O
274	O
chapter	O
8.	O
logistic	B
regression	I
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−2	O
−2.5	O
−3	O
−3.5	O
−3	O
−2	O
−1	O
0	O
(	O
a	O
)	O
1	O
2	O
3	O
4	O
−4.5	O
−4	O
−3.5	O
−3	O
−2.5	O
−2	O
−1.5	O
−1	O
−0.5	O
0	O
(	O
b	O
)	O
figure	O
8.12	O
(	O
a	O
)	O
pca	O
projection	B
of	O
vowel	O
data	O
to	O
2d	O
.	O
(	O
b	O
)	O
flda	O
projection	B
of	O
vowel	O
data	O
to	O
2d	O
.	O
we	O
see	O
there	O
is	O
better	O
class	O
separation	O
in	O
the	O
flda	O
case	O
.	O
based	O
on	O
figure	O
4.11	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
fisherdiscrimvoweldemo	O
,	O
by	O
hannes	O
bretschneider	O
.	O
where	O
λ	O
=	O
wt	O
sbw	O
wt	O
sw	O
w	O
(	O
8.108	O
)	O
equation	O
8.107	O
is	O
called	O
a	O
generalized	B
eigenvalue	I
problem	O
.	O
if	O
sw	O
is	O
invertible	O
,	O
we	O
can	O
convert	O
it	O
to	O
a	O
regular	B
eigenvalue	O
problem	O
:	O
s−1	O
w	O
sbw	O
=	O
λw	O
however	O
,	O
in	O
the	O
two	O
class	O
case	O
,	O
there	O
is	O
a	O
simpler	O
solution	O
.	O
in	O
particular	O
,	O
since	O
sbw	O
=	O
(	O
μ2	O
−	O
μ1	O
)	O
(	O
μ2	O
−	O
μ1	O
)	O
t	O
w	O
=	O
(	O
μ2	O
−	O
μ1	O
)	O
(	O
m2	O
−	O
m1	O
)	O
then	O
,	O
from	O
equation	O
8.109	O
we	O
have	O
λ	O
w	O
=	O
s−1	O
w	O
∝	O
s−1	O
w	O
(	O
μ2	O
−	O
μ1	O
)	O
(	O
m2	O
−	O
m1	O
)	O
w	O
(	O
μ2	O
−	O
μ1	O
)	O
(	O
8.109	O
)	O
(	O
8.110	O
)	O
(	O
8.111	O
)	O
(	O
8.112	O
)	O
since	O
we	O
only	O
care	O
about	O
the	O
directionality	O
,	O
and	O
not	O
the	O
scale	O
factor	O
,	O
we	O
can	O
just	O
set	O
w	O
=	O
s−1	O
w	O
(	O
μ2	O
−	O
μ1	O
)	O
(	O
8.113	O
)	O
this	O
is	O
the	O
optimal	O
solution	O
in	O
the	O
two-class	O
case	O
.	O
if	O
sw	O
∝	O
i	O
,	O
meaning	O
the	O
pooled	B
covariance	O
matrix	O
is	O
isotropic	B
,	O
then	O
w	O
is	O
proportional	O
to	O
the	O
vector	O
that	O
joins	O
the	O
class	O
means	O
.	O
this	O
is	O
an	O
intuitively	O
reasonable	O
direction	O
to	O
project	O
onto	O
,	O
as	O
shown	O
in	O
figure	O
8.11	O
.	O
8.6.3.2	O
extension	B
to	O
higher	O
dimensions	O
and	O
multiple	O
classes	O
we	O
can	O
extend	O
the	O
above	O
idea	O
to	O
multiple	O
classes	O
,	O
and	O
to	O
higher	O
dimensional	O
subspaces	O
,	O
by	O
ﬁnding	O
a	O
projection	B
matrix	O
w	O
which	O
maps	O
from	O
d	O
to	O
l	O
so	O
as	O
to	O
maximize	O
j	O
(	O
w	O
)	O
=	O
|wσbwt|	O
|wσw	O
wt|	O
(	O
8.114	O
)	O
8.6.	O
generative	O
vs	O
discriminative	B
classiﬁers	O
where	O
σb	O
(	O
cid:2	O
)	O
σw	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
c	O
c	O
σc	O
(	O
cid:2	O
)	O
1	O
nc	O
(	O
μc	O
−	O
μ	O
)	O
(	O
μc	O
−	O
μ	O
)	O
t	O
nc	O
n	O
σc	O
nc	O
n	O
(	O
cid:2	O
)	O
(	O
xi	O
−	O
μc	O
)	O
(	O
xi	O
−	O
μc	O
)	O
t	O
i	O
:	O
yi=c	O
the	O
solution	O
can	O
be	O
shown	O
to	O
be	O
w	O
=	O
σ	O
−	O
1	O
w	O
u	O
2	O
275	O
(	O
8.115	O
)	O
(	O
8.116	O
)	O
(	O
8.117	O
)	O
(	O
8.118	O
)	O
−	O
1	O
w	O
,	O
assuming	O
σw	O
is	O
non-singular	O
.	O
(	O
if	O
it	O
2	O
where	O
u	O
are	O
the	O
l	O
leading	O
eigenvectors	O
of	O
σ	O
is	O
singular	O
,	O
we	O
can	O
ﬁrst	O
perform	O
pca	O
on	O
all	O
the	O
data	O
.	O
)	O
2	O
−	O
1	O
w	O
σbς	O
figure	O
8.12	O
gives	O
an	O
example	O
of	O
this	O
method	O
applied	O
to	O
some	O
d	O
=	O
10	O
dimensional	O
speech	O
data	O
,	O
representing	O
c	O
=	O
11	O
different	O
vowel	O
sounds	O
.	O
we	O
see	O
that	O
flda	O
gives	O
better	O
class	O
separation	O
than	O
pca	O
.	O
note	O
that	O
flda	O
is	O
restricted	O
to	O
ﬁnding	O
at	O
most	O
a	O
l	O
≤	O
c	O
−	O
1	O
dimensional	O
linear	O
subspace	O
,	O
no	O
matter	O
how	O
large	O
d	O
,	O
because	O
the	O
rank	O
of	O
the	O
between	O
class	O
covariance	O
matrix	O
σb	O
is	O
c	O
−	O
1	O
.	O
(	O
the	O
-1	O
term	O
arises	O
because	O
of	O
the	O
μ	O
term	O
,	O
which	O
is	O
a	O
linear	O
function	O
of	O
the	O
μc	O
.	O
)	O
this	O
is	O
a	O
rather	O
severe	O
restriction	O
which	O
limits	O
the	O
usefulness	O
of	O
flda	O
.	O
8.6.3.3	O
probabilistic	O
interpretation	O
of	O
flda	O
*	O
to	O
ﬁnd	O
a	O
valid	O
probabilistic	O
interpretation	O
of	O
flda	O
,	O
we	O
follow	O
the	O
approach	O
of	O
(	O
kumar	O
and	O
andreo	O
1998	O
;	O
zhou	O
et	O
al	O
.	O
2009	O
)	O
.	O
they	O
proposed	O
a	O
model	O
known	O
as	O
heteroscedastic	O
lda	O
(	O
hlda	O
)	O
,	O
which	O
works	O
as	O
follows	O
.	O
let	O
w	O
be	O
a	O
d	O
×	O
d	O
invertible	O
matrix	O
,	O
and	O
let	O
zi	O
=	O
wxi	O
be	O
a	O
transformed	O
version	O
of	O
the	O
data	O
.	O
we	O
now	O
ﬁt	O
full	B
covariance	O
gaussians	O
to	O
the	O
transformed	O
data	O
,	O
one	O
per	O
class	O
,	O
but	O
with	O
the	O
constraint	O
that	O
only	O
the	O
ﬁrst	O
l	O
components	O
will	O
be	O
class-speciﬁc	O
;	O
the	O
remaining	O
h	O
=	O
d	O
−	O
l	O
components	O
will	O
be	O
shared	B
across	O
classes	O
,	O
and	O
will	O
thus	O
not	O
be	O
discriminative	B
.	O
that	O
is	O
,	O
we	O
use	O
(	O
8.121	O
)	O
where	O
m0	O
is	O
the	O
shared	B
h	O
dimensional	O
mean	B
and	O
s0	O
is	O
the	O
shared	B
h	O
×	O
h	O
covariace	O
.	O
the	O
pdf	B
of	O
the	O
original	O
(	O
untransformed	O
)	O
data	O
is	O
given	O
by	O
p	O
(	O
xi|yi	O
=	O
c	O
,	O
w	O
,	O
θ	O
)	O
=|w	O
|	O
n	O
(	O
wxi|μc	O
,	O
σc	O
)	O
=	O
|w|	O
n	O
(	O
wlxi|mc	O
,	O
sc	O
)	O
n	O
(	O
wh	O
xi|m0	O
,	O
s0	O
)	O
.	O
for	O
ﬁxed	O
w	O
,	O
it	O
is	O
easy	O
to	O
derive	O
the	O
mle	O
for	O
θ.	O
one	O
can	O
then	O
optimize	O
(	O
8.119	O
)	O
(	O
8.120	O
)	O
(	O
8.122	O
)	O
(	O
8.123	O
)	O
p	O
(	O
zi|θ	O
,	O
yi	O
=	O
c	O
)	O
=n	O
(	O
zi|μc	O
,	O
σc	O
)	O
(	O
cid:9	O
)	O
μc	O
(	O
cid:2	O
)	O
(	O
mc	O
;	O
m0	O
)	O
sc	O
0	O
σc	O
(	O
cid:2	O
)	O
0	O
s0	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
where	O
w	O
=	O
w	O
using	O
gradient	O
methods	O
.	O
wl	O
wh	O
276	O
chapter	O
8.	O
logistic	B
regression	I
in	O
the	O
special	O
case	O
that	O
the	O
σc	O
are	O
diagonal	B
,	O
there	O
is	O
a	O
closed-form	O
solution	O
for	O
w	O
(	O
gales	O
1999	O
)	O
.	O
and	O
in	O
the	O
special	O
case	O
the	O
σc	O
are	O
all	O
equal	O
,	O
we	O
recover	O
classical	B
lda	O
(	O
zhou	O
et	O
al	O
.	O
2009	O
)	O
.	O
in	O
view	O
of	O
this	O
this	O
result	O
,	O
it	O
should	O
be	O
clear	O
that	O
hlda	O
will	O
outperform	O
lda	O
if	O
the	O
class	O
covariances	O
are	O
not	O
equal	O
within	O
the	O
discriminative	B
subspace	O
(	O
i.e.	O
,	O
if	O
the	O
assumption	O
that	O
σc	O
is	O
independent	O
of	O
c	O
is	O
a	O
poor	O
assumption	O
)	O
.	O
this	O
is	O
easy	O
to	O
demonstrate	O
on	O
synthetic	O
data	O
,	O
and	O
is	O
also	O
the	O
case	O
on	O
more	O
challenging	O
tasks	O
such	O
as	O
speech	B
recognition	I
(	O
kumar	O
and	O
andreo	O
1998	O
)	O
.	O
furthermore	O
,	O
we	O
can	O
extend	O
the	O
model	O
by	O
allowing	O
each	O
class	O
to	O
use	O
its	O
own	O
projection	B
matrix	O
;	O
this	O
is	O
known	O
as	O
multiple	O
lda	O
(	O
gales	O
2002	O
)	O
.	O
exercises	O
exercise	O
8.1	O
spam	B
classiﬁcation	O
using	O
logistic	B
regression	I
consider	O
the	O
email	O
spam	O
data	O
set	O
discussed	O
on	O
p300	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
this	O
consists	O
of	O
4601	O
email	O
messages	O
,	O
from	O
which	O
57	O
features	B
have	O
been	O
extracted	O
.	O
these	O
are	O
as	O
follows	O
:	O
•	O
•	O
•	O
•	O
•	O
48	O
features	B
,	O
in	O
[	O
0	O
,	O
100	O
]	O
,	O
giving	O
the	O
percentage	O
of	O
words	O
in	O
a	O
given	O
message	O
which	O
match	O
a	O
given	O
word	O
on	O
the	O
list	O
.	O
the	O
list	O
contains	O
words	O
such	O
as	O
“	O
business	O
”	O
,	O
“	O
free	O
”	O
,	O
“	O
george	O
”	O
,	O
etc	O
.	O
(	O
the	O
data	O
was	O
collected	O
by	O
george	O
forman	O
,	O
so	O
his	O
name	O
occurs	O
quite	O
a	O
lot	O
.	O
)	O
6	O
features	B
,	O
in	O
[	O
0	O
,	O
100	O
]	O
,	O
giving	O
the	O
percentage	O
of	O
characters	O
in	O
the	O
email	O
that	O
match	O
a	O
given	O
character	O
on	O
the	O
list	O
.	O
the	O
characters	O
are	O
#	O
$	O
;	O
(	O
[	O
!	O
feature	O
55	O
:	O
the	O
average	O
length	O
of	O
an	O
uninterrupted	O
sequence	O
of	O
capital	O
letters	O
(	O
max	O
is	O
40.3	O
,	O
mean	B
is	O
4.9	O
)	O
feature	O
56	O
:	O
the	O
length	O
of	O
the	O
longest	O
uninterrupted	O
sequence	O
of	O
capital	O
letters	O
(	O
max	O
is	O
45.0	O
,	O
mean	B
is	O
52.6	O
)	O
feature	O
57	O
:	O
the	O
sum	O
of	O
the	O
lengts	O
of	O
uninterrupted	O
sequence	O
of	O
capital	O
letters	O
(	O
max	O
is	O
25.6	O
,	O
mean	B
is	O
282.2	O
)	O
load	O
the	O
data	O
from	O
spamdata.mat	O
,	O
which	O
contains	O
a	O
training	B
set	I
(	O
of	O
size	O
3065	O
)	O
and	O
a	O
test	O
set	O
(	O
of	O
size	O
1536	O
)	O
.	O
one	O
can	O
imagine	O
performing	O
several	O
kinds	O
of	O
preprocessing	O
to	O
this	O
data	O
.	O
try	O
each	O
of	O
the	O
following	O
separately	O
:	O
a.	O
standardize	O
the	O
columns	O
so	O
they	O
all	O
have	O
mean	B
0	O
and	O
unit	O
variance	O
.	O
b.	O
transform	O
the	O
features	B
using	O
log	O
(	O
xij	O
+	O
0.1	O
)	O
.	O
c.	O
binarize	O
the	O
features	B
using	O
i	O
(	O
xij	O
>	O
0	O
)	O
.	O
for	O
each	O
version	O
of	O
the	O
data	O
,	O
ﬁt	O
a	O
logistic	B
regression	I
model	O
.	O
use	O
cross	B
validation	I
to	O
choose	O
the	O
strength	O
of	O
the	O
(	O
cid:7	O
)	O
2	O
regularizer	O
.	O
report	O
the	O
mean	B
error	O
rate	B
on	O
the	O
training	O
and	O
test	O
sets	O
.	O
you	O
should	O
get	O
numbers	O
similar	B
to	O
this	O
:	O
method	O
stnd	O
log	O
binary	O
train	O
0.082	O
0.052	O
0.065	O
test	O
0.079	O
0.059	O
0.072	O
(	O
the	O
precise	O
values	O
will	O
depend	O
on	O
what	O
regularization	B
value	O
you	O
choose	O
.	O
)	O
turn	O
in	O
your	O
code	O
and	O
numerical	O
results	O
.	O
(	O
see	O
also	O
exercise	O
8.2.	O
exercise	O
8.2	O
spam	B
classiﬁcation	O
using	O
naive	O
bayes	O
we	O
will	O
re-examine	O
the	O
dataset	O
from	O
exercise	O
8.1	O
.	O
8.6.	O
generative	O
vs	O
discriminative	B
classiﬁers	O
277	O
a.	O
use	O
naivebayesfit	O
and	O
naivebayespredict	O
on	O
the	O
binarized	O
spam	B
data	O
.	O
what	O
is	O
the	O
training	O
and	O
(	O
you	O
can	O
try	O
different	O
settings	O
of	O
the	O
pseudocount	O
α	O
if	O
you	O
like	O
(	O
this	O
corresponds	O
to	O
the	O
test	O
error	O
?	O
beta	O
(	O
α	O
,	O
α	O
)	O
prior	O
each	O
θjc	O
)	O
,	O
although	O
the	O
default	O
of	O
α	O
=	O
1	O
is	O
probably	O
ﬁne	O
.	O
)	O
turn	O
in	O
your	O
error	O
rates	O
.	O
b.	O
modify	O
the	O
code	O
so	O
it	O
can	O
handle	O
real-valued	O
features	B
.	O
use	O
a	O
gaussian	O
density	O
for	O
each	O
feature	O
;	O
ﬁt	O
it	O
with	O
maximum	O
likelihood	O
.	O
what	O
are	O
the	O
training	O
and	O
test	O
error	O
rates	O
on	O
the	O
standardized	B
data	O
and	O
the	O
log	O
transformed	O
data	O
?	O
turn	O
in	O
your	O
4	O
error	O
rates	O
and	O
code	O
.	O
exercise	O
8.3	O
gradient	O
and	O
hessian	O
of	O
log-likelihood	O
for	O
logistic	B
regression	I
a.	O
let	O
σ	O
(	O
a	O
)	O
=	O
1	O
1+e−a	O
be	O
the	O
sigmoid	B
function	O
.	O
show	O
that	O
=	O
σ	O
(	O
a	O
)	O
(	O
1	O
−	O
σ	O
(	O
a	O
)	O
)	O
dσ	O
(	O
a	O
)	O
da	O
(	O
8.124	O
)	O
log	O
likelihood	O
(	O
equation	O
8.5	O
)	O
.	O
b.	O
using	O
the	O
previous	O
result	O
and	O
the	O
chain	B
rule	I
of	O
calculus	O
,	O
derive	O
an	O
expression	O
for	O
the	O
gradient	O
of	O
the	O
c.	O
the	O
hessian	O
can	O
be	O
written	O
as	O
h	O
=	O
xt	O
sx	O
,	O
where	O
s	O
(	O
cid:2	O
)	O
diag	O
(	O
μ1	O
(	O
1	O
−	O
μ1	O
)	O
,	O
.	O
.	O
.	O
,	O
μn	O
(	O
1	O
−	O
μn	O
)	O
)	O
.	O
show	O
(	O
you	O
may	O
assume	O
that	O
0	O
<	O
μi	O
<	O
1	O
,	O
so	O
the	O
elements	O
of	O
s	O
will	O
be	O
strictly	O
that	O
h	O
is	O
positive	B
deﬁnite	I
.	O
positive	O
,	O
and	O
that	O
x	O
is	O
full	B
rank	O
.	O
)	O
exercise	O
8.4	O
gradient	O
and	O
hessian	O
of	O
log-likelihood	O
for	O
multinomial	B
logistic	I
regression	I
a.	O
let	O
μik	O
=	O
s	O
(	O
ηi	O
)	O
k.	O
prove	O
that	O
the	O
jacobian	O
of	O
the	O
softmax	B
is	O
∂μik	O
∂ηij	O
=	O
μik	O
(	O
δkj	O
−	O
μij	O
)	O
(	O
8.125	O
)	O
(	O
8.126	O
)	O
(	O
8.127	O
)	O
where	O
δkj	O
=	O
i	O
(	O
k	O
=	O
j	O
)	O
.	O
b.	O
hence	O
show	O
that	O
∇wc	O
(	O
cid:7	O
)	O
=	O
(	O
yic	O
−	O
μic	O
)	O
xi	O
(	O
cid:2	O
)	O
hint	O
:	O
use	O
the	O
chain	B
rule	I
and	O
the	O
fact	O
that	O
c	O
yic	O
=	O
1.	O
c.	O
show	O
that	O
the	O
block	O
submatrix	O
of	O
the	O
hessian	O
for	O
classes	O
c	O
and	O
c	O
(	O
cid:2	O
)	O
is	O
given	O
by	O
hc	O
,	O
c	O
(	O
cid:2	O
)	O
=	O
−	O
μic	O
(	O
δc	O
,	O
c	O
(	O
cid:2	O
)	O
−	O
μi	O
,	O
c	O
(	O
cid:2	O
)	O
)	O
xixt	O
i	O
(	O
cid:12	O
)	O
i	O
(	O
cid:12	O
)	O
i	O
exercise	O
8.5	O
symmetric	B
version	O
of	O
(	O
cid:7	O
)	O
2	O
regularized	O
multinomial	O
logistic	B
regression	I
(	O
source	O
:	O
ex	O
18.3	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
)	O
multiclass	O
logistic	O
regression	B
has	O
the	O
form	O
p	O
(	O
y	O
=	O
c|x	O
,	O
w	O
)	O
=	O
(	O
cid:2	O
)	O
c	O
exp	O
(	O
wc0	O
+	O
wt	O
c	O
x	O
)	O
k=1	O
exp	O
(	O
wk0	O
+	O
wt	O
k	O
x	O
)	O
(	O
8.128	O
)	O
where	O
w	O
is	O
a	O
(	O
d	O
+	O
1	O
)	O
×	O
c	O
weight	O
matrix	O
.	O
we	O
can	O
arbitrarily	O
deﬁne	O
wc	O
=	O
0	O
for	O
one	O
of	O
the	O
classes	O
,	O
say	O
c	O
=	O
c	O
,	O
since	O
p	O
(	O
y	O
=	O
c|x	O
,	O
w	O
)	O
=	O
1	O
−	O
(	O
cid:2	O
)	O
c−1	O
c=1	O
p	O
(	O
y	O
=	O
c|x	O
,	O
w	O
)	O
.	O
in	O
this	O
case	O
,	O
the	O
model	O
has	O
the	O
form	O
c	O
x	O
)	O
p	O
(	O
y	O
=	O
c|x	O
,	O
w	O
)	O
=	O
1	O
+	O
exp	O
(	O
wc0	O
+	O
wt	O
k=1	O
exp	O
(	O
wk0	O
+	O
wt	O
k	O
x	O
)	O
(	O
cid:2	O
)	O
c−1	O
(	O
8.129	O
)	O
278	O
chapter	O
8.	O
logistic	B
regression	I
if	O
we	O
don	O
’	O
t	O
“	O
clamp	O
”	O
one	O
of	O
the	O
vectors	O
to	O
some	O
constant	O
value	O
,	O
the	O
parameters	O
will	O
be	O
unidentiﬁable	B
.	O
however	O
,	O
suppose	O
we	O
don	O
’	O
t	O
clamp	O
wc	O
=	O
0	O
,	O
so	O
we	O
are	O
using	O
equation	O
8.128	O
,	O
but	O
we	O
add	O
(	O
cid:7	O
)	O
2	O
regularization	B
by	O
optimizing	O
n	O
(	O
cid:12	O
)	O
log	O
p	O
(	O
yi|xi	O
,	O
w	O
)	O
−	O
λ	O
i=1	O
c=1	O
c	O
(	O
cid:12	O
)	O
2	O
||wc||2	O
(	O
cid:2	O
)	O
c	O
(	O
8.130	O
)	O
(	O
8.131	O
)	O
(	O
8.132	O
)	O
show	O
that	O
at	O
the	O
optimum	O
we	O
have	O
still	O
need	O
to	O
enforce	O
that	O
w0c	O
=	O
0	O
to	O
ensure	O
identiﬁability	O
of	O
the	O
offset	O
.	O
)	O
c=1	O
ˆwcj	O
=	O
0	O
for	O
j	O
=	O
1	O
:	O
d	O
.	O
(	O
for	O
the	O
unregularized	O
ˆwc0	O
terms	O
,	O
we	O
exercise	O
8.6	O
elementary	O
properties	O
of	O
(	O
cid:7	O
)	O
2	O
regularized	O
logistic	O
regression	B
(	O
source	O
:	O
jaaakkola.	O
)	O
.	O
consider	O
minimizing	O
j	O
(	O
w	O
)	O
=	O
−	O
(	O
cid:7	O
)	O
(	O
w	O
,	O
dtrain	O
)	O
+	O
λ||w||2	O
2	O
where	O
(	O
cid:7	O
)	O
(	O
w	O
,	O
d	O
)	O
=	O
1|d|	O
(	O
cid:12	O
)	O
i∈d	O
log	O
σ	O
(	O
yixt	O
i	O
w	O
)	O
is	O
the	O
average	O
log-likelihood	O
on	O
data	O
set	O
d	O
,	O
for	O
yi	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
.	O
answer	O
the	O
following	O
true/	O
false	O
questions	O
.	O
a.	O
j	O
(	O
w	O
)	O
has	O
multiple	O
locally	O
optimal	O
solutions	O
:	O
t/f	O
?	O
b.	O
let	O
ˆw	O
=	O
arg	O
minw	O
j	O
(	O
w	O
)	O
be	O
a	O
global	O
optimum	O
.	O
ˆw	O
is	O
sparse	B
(	O
has	O
many	O
zero	O
entries	O
)	O
:	O
t/f	O
?	O
c.	O
d.	O
(	O
cid:7	O
)	O
(	O
ˆw	O
,	O
dtrain	O
)	O
always	O
increases	O
as	O
we	O
increase	O
λ	O
:	O
t/f	O
?	O
e.	O
(	O
cid:7	O
)	O
(	O
ˆw	O
,	O
dtest	O
)	O
always	O
increases	O
as	O
we	O
increase	O
λ	O
:	O
t/f	O
?	O
if	O
the	O
training	O
data	O
is	O
linearly	B
separable	I
,	O
then	O
some	O
weights	O
wj	O
might	O
become	O
inﬁnite	O
if	O
λ	O
=	O
0	O
:	O
t/f	O
?	O
exercise	O
8.7	O
regularizing	O
separate	O
terms	O
in	O
2d	O
logistic	B
regression	I
(	O
source	O
:	O
jaaakkola	O
.	O
)	O
a.	O
consider	O
the	O
data	O
in	O
figure	O
8.13	O
,	O
where	O
we	O
ﬁt	O
the	O
model	O
p	O
(	O
y	O
=	O
1|x	O
,	O
w	O
)	O
=σ	O
(	O
w0	O
+	O
w1x1	O
+	O
w2x2	O
)	O
.	O
suppose	O
we	O
ﬁt	O
the	O
model	O
by	O
maximum	O
likelihood	O
,	O
i.e.	O
,	O
we	O
minimize	O
j	O
(	O
w	O
)	O
=	O
−	O
(	O
cid:7	O
)	O
(	O
w	O
,	O
dtrain	O
)	O
(	O
8.133	O
)	O
where	O
(	O
cid:7	O
)	O
(	O
w	O
,	O
dtrain	O
)	O
is	O
the	O
log	O
likelihood	O
on	O
the	O
training	B
set	I
.	O
sketch	O
a	O
possible	O
decision	B
boundary	I
corresponding	O
to	O
ˆw	O
.	O
answer	O
on	O
your	O
copy	O
,	O
since	O
you	O
will	O
need	O
multiple	O
versions	O
of	O
this	O
ﬁgure	O
)	O
.	O
boundary	O
)	O
unique	O
?	O
how	O
many	O
classiﬁcation	B
errors	O
does	O
your	O
method	O
make	O
on	O
the	O
training	B
set	I
?	O
(	O
copy	O
the	O
ﬁgure	O
ﬁrst	O
(	O
a	O
rough	O
sketch	O
is	O
enough	O
)	O
,	O
and	O
then	O
superimpose	O
your	O
is	O
your	O
answer	O
(	O
decision	B
b.	O
now	O
suppose	O
we	O
regularize	O
only	O
the	O
w0	O
parameter	B
,	O
i.e.	O
,	O
we	O
minimize	O
j0	O
(	O
w	O
)	O
=	O
−	O
(	O
cid:7	O
)	O
(	O
w	O
,	O
dtrain	O
)	O
+λw	O
2	O
0	O
(	O
8.134	O
)	O
suppose	O
λ	O
is	O
a	O
very	O
large	O
number	O
,	O
so	O
we	O
regularize	O
w0	O
all	O
the	O
way	O
to	O
0	O
,	O
but	O
all	O
other	O
parameters	O
are	O
unregularized	O
.	O
sketch	O
a	O
possible	O
decision	B
boundary	I
.	O
how	O
many	O
classiﬁcation	B
errors	O
does	O
your	O
method	O
make	O
on	O
the	O
training	B
set	I
?	O
hint	O
:	O
consider	O
the	O
behavior	O
of	O
simple	O
linear	O
regression	B
,	O
w0	O
+	O
w1x1	O
+	O
w2x2	O
when	O
x1	O
=	O
x2	O
=	O
0.	O
c.	O
now	O
suppose	O
we	O
heavily	O
regularize	O
only	O
the	O
w1	O
parameter	B
,	O
i.e.	O
,	O
we	O
minimize	O
j1	O
(	O
w	O
)	O
=	O
−	O
(	O
cid:7	O
)	O
(	O
w	O
,	O
dtrain	O
)	O
+λw	O
2	O
1	O
(	O
8.135	O
)	O
sketch	O
a	O
possible	O
decision	B
boundary	I
.	O
how	O
many	O
classiﬁcation	B
errors	O
does	O
your	O
method	O
make	O
on	O
the	O
training	B
set	I
?	O
8.6.	O
generative	O
vs	O
discriminative	B
classiﬁers	O
279	O
figure	O
8.13	O
data	O
for	O
logistic	B
regression	I
question	O
.	O
d.	O
now	O
suppose	O
we	O
heavily	O
regularize	O
only	O
the	O
w2	O
parameter	B
.	O
sketch	O
a	O
possible	O
decision	B
boundary	I
.	O
how	O
many	O
classiﬁcation	B
errors	O
does	O
your	O
method	O
make	O
on	O
the	O
training	B
set	I
?	O
9	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	I
9.1	O
introduction	O
we	O
have	O
now	O
encountered	O
a	O
wide	O
variety	O
of	O
probability	O
distributions	O
:	O
the	O
gaussian	O
,	O
the	O
bernoulli	O
,	O
the	O
student	O
t	O
,	O
the	O
uniform	O
,	O
the	O
gamma	O
,	O
etc	O
.	O
it	O
turns	O
out	O
that	O
most	O
of	O
these	O
are	O
members	O
of	O
a	O
broader	O
class	O
of	O
distributions	O
known	O
as	O
the	O
exponential	O
family.1	O
in	O
this	O
chapter	O
,	O
we	O
discuss	O
various	O
properties	O
of	O
this	O
family	B
.	O
this	O
allows	O
us	O
to	O
derive	O
theorems	O
and	O
algorithms	O
with	O
very	O
broad	O
applicability	O
.	O
we	O
will	O
see	O
how	O
we	O
can	O
easily	O
use	O
any	O
member	O
of	O
the	O
exponential	B
family	I
as	O
a	O
class-conditional	B
density	I
in	O
order	O
to	O
make	O
a	O
generative	B
classiﬁer	I
.	O
in	O
addition	O
,	O
we	O
will	O
discuss	O
how	O
to	O
build	O
discriminative	B
models	O
,	O
where	O
the	O
response	B
variable	I
has	O
an	O
exponential	B
family	I
distribution	O
,	O
whose	O
mean	B
is	O
a	O
linear	O
function	O
of	O
the	O
inputs	O
;	O
this	O
is	O
known	O
as	O
a	O
generalized	B
linear	I
model	I
,	O
and	O
generalizes	O
the	O
idea	O
of	O
logistic	B
regression	I
to	O
other	O
kinds	O
of	O
response	O
variables	O
.	O
9.2	O
the	O
exponential	B
family	I
before	O
deﬁning	O
the	O
exponential	B
family	I
,	O
we	O
mention	O
several	O
reasons	O
why	O
it	O
is	O
important	O
:	O
•	O
it	O
can	O
be	O
shown	O
that	O
,	O
under	O
certain	O
regularity	O
conditions	O
,	O
the	O
exponential	B
family	I
is	O
the	O
only	O
family	B
of	O
distributions	O
with	O
ﬁnite-sized	O
sufficient	B
statistics	I
,	O
meaning	O
that	O
we	O
can	O
compress	O
the	O
data	O
into	O
a	O
ﬁxed-sized	O
summary	O
without	O
loss	B
of	O
information	B
.	O
this	O
is	O
particularly	O
useful	O
for	O
online	B
learning	I
,	O
as	O
we	O
will	O
see	O
later	O
.	O
•	O
the	O
exponential	B
family	I
is	O
the	O
only	O
family	B
of	O
distributions	O
for	O
which	O
conjugate	B
priors	I
exist	O
,	O
which	O
simpliﬁes	O
the	O
computation	O
of	O
the	O
posterior	O
(	O
see	O
section	O
9.2.5	O
)	O
.	O
•	O
the	O
exponential	B
family	I
can	O
be	O
shown	O
to	O
be	O
the	O
family	B
of	O
distributions	O
that	O
makes	O
the	O
least	O
set	O
of	O
assumptions	O
subject	O
to	O
some	O
user-chosen	O
constraints	O
(	O
see	O
section	O
9.2.6	O
)	O
.	O
•	O
the	O
exponential	B
family	I
is	O
at	O
the	O
core	O
of	O
generalized	B
linear	I
models	I
,	O
as	O
discussed	O
in	O
section	O
9.3	O
.	O
•	O
the	O
exponential	B
family	I
is	O
at	O
the	O
core	O
of	O
variational	B
inference	I
,	O
as	O
discussed	O
in	O
section	O
21.2	O
.	O
1.	O
the	O
exceptions	O
are	O
the	O
student	O
t	O
,	O
which	O
does	O
not	O
have	O
the	O
right	O
form	O
,	O
and	O
the	O
uniform	B
distribution	I
,	O
which	O
does	O
not	O
have	O
ﬁxed	O
support	O
independent	O
of	O
the	O
parameter	B
values	O
.	O
282	O
chapter	O
9.	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	I
9.2.1	O
deﬁnition	O
a	O
pdf	B
or	O
pmf	B
p	O
(	O
x|θ	O
)	O
,	O
for	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xm	O
)	O
∈	O
x	O
m	O
and	O
θ	O
∈	O
θ	O
⊆	O
r	O
exponential	B
family	I
if	O
it	O
is	O
of	O
the	O
form	O
d	O
,	O
is	O
said	O
to	O
be	O
in	O
the	O
p	O
(	O
x|θ	O
)	O
=	O
1	O
z	O
(	O
θ	O
)	O
h	O
(	O
x	O
)	O
exp	O
[	O
θt	O
φ	O
(	O
x	O
)	O
]	O
=	O
h	O
(	O
x	O
)	O
exp	O
[	O
θt	O
φ	O
(	O
x	O
)	O
−	O
a	O
(	O
θ	O
)	O
]	O
(	O
cid:28	O
)	O
x	O
m	O
h	O
(	O
x	O
)	O
exp	O
[	O
θt	O
φ	O
(	O
x	O
)	O
]	O
dx	O
where	O
z	O
(	O
θ	O
)	O
=	O
a	O
(	O
θ	O
)	O
=	O
log	O
z	O
(	O
θ	O
)	O
(	O
9.1	O
)	O
(	O
9.2	O
)	O
(	O
9.3	O
)	O
(	O
9.4	O
)	O
here	O
θ	O
are	O
called	O
the	O
natural	B
parameters	I
or	O
canonical	B
parameters	I
,	O
φ	O
(	O
x	O
)	O
∈	O
r	O
d	O
is	O
called	O
a	O
vector	O
of	O
sufficient	B
statistics	I
,	O
z	O
(	O
θ	O
)	O
is	O
called	O
the	O
partition	B
function	I
,	O
a	O
(	O
θ	O
)	O
is	O
called	O
the	O
log	B
partition	I
function	I
or	O
cumulant	B
function	I
,	O
and	O
h	O
(	O
x	O
)	O
is	O
the	O
a	O
scaling	O
constant	O
,	O
often	O
1.	O
if	O
φ	O
(	O
x	O
)	O
=	O
x	O
,	O
we	O
say	O
it	O
is	O
a	O
natural	B
exponential	I
family	I
.	O
equation	O
9.2	O
can	O
be	O
generalized	O
by	O
writing	O
p	O
(	O
x|θ	O
)	O
=	O
h	O
(	O
x	O
)	O
exp	O
[	O
η	O
(	O
θ	O
)	O
t	O
φ	O
(	O
x	O
)	O
−	O
a	O
(	O
η	O
(	O
θ	O
)	O
)	O
]	O
(	O
9.5	O
)	O
where	O
η	O
is	O
a	O
function	O
that	O
maps	O
the	O
parameters	O
θ	O
to	O
the	O
canonical	B
parameters	I
η	O
=	O
η	O
(	O
θ	O
)	O
.	O
if	O
dim	O
(	O
θ	O
)	O
<	O
dim	O
(	O
η	O
(	O
θ	O
)	O
)	O
,	O
it	O
is	O
called	O
a	O
curved	B
exponential	I
family	I
,	O
which	O
means	O
we	O
have	O
more	O
if	O
η	O
(	O
θ	O
)	O
=θ	O
,	O
the	O
model	O
is	O
said	O
to	O
be	O
in	O
canonical	B
form	I
.	O
sufficient	B
statistics	I
than	O
parameters	O
.	O
we	O
will	O
assume	O
models	O
are	O
in	O
canonical	B
form	I
unless	O
we	O
state	B
otherwise	O
.	O
9.2.2	O
examples	O
let	O
us	O
consider	O
some	O
examples	O
to	O
make	O
things	O
clearer	O
.	O
9.2.2.1	O
bernoulli	O
the	O
bernoulli	O
for	O
x	O
∈	O
{	O
0	O
,	O
1	O
}	O
can	O
be	O
written	O
in	O
exponential	B
family	I
form	O
as	O
follows	O
:	O
ber	O
(	O
x|μ	O
)	O
=	O
μx	O
(	O
1	O
−	O
μ	O
)	O
1−x	O
=	O
exp	O
[	O
x	O
log	O
(	O
μ	O
)	O
+	O
(	O
1−	O
x	O
)	O
log	O
(	O
1	O
−	O
μ	O
)	O
]	O
=	O
exp	O
[	O
φ	O
(	O
x	O
)	O
t	O
θ	O
]	O
(	O
9.6	O
)	O
where	O
φ	O
(	O
x	O
)	O
=	O
[	O
i	O
(	O
x	O
=	O
0	O
)	O
,	O
i	O
(	O
x	O
=	O
1	O
)	O
]	O
and	O
θ	O
=	O
[	O
log	O
(	O
μ	O
)	O
,	O
log	O
(	O
1	O
−	O
μ	O
)	O
]	O
.	O
however	O
,	O
this	O
representation	O
is	O
over-complete	B
since	O
there	O
is	O
a	O
linear	O
dependendence	O
between	O
the	O
features	B
:	O
1t	O
φ	O
(	O
x	O
)	O
=i	O
(	O
x	O
=	O
0	O
)	O
+	O
i	O
(	O
x	O
=	O
1	O
)	O
=	O
1	O
(	O
9.7	O
)	O
consequently	O
θ	O
is	O
not	O
uniquely	O
identiﬁable	B
.	O
it	O
is	O
common	O
to	O
require	O
that	O
the	O
representation	O
be	O
minimal	B
,	O
which	O
means	O
there	O
is	O
a	O
unique	O
θ	O
associated	O
with	O
the	O
distribution	O
.	O
in	O
this	O
case	O
,	O
we	O
can	O
just	O
deﬁne	O
ber	O
(	O
x|μ	O
)	O
=	O
(	O
1	O
−	O
μ	O
)	O
exp	O
x	O
log	O
(	O
cid:29	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
cid:30	O
)	O
μ	O
1	O
−	O
μ	O
(	O
9.8	O
)	O
cat	O
(	O
x|μ	O
)	O
=	O
k	O
=	O
exp	O
k	O
(	O
cid:27	O
)	O
μxk	O
k=1	O
(	O
cid:18	O
)	O
k−1	O
(	O
cid:2	O
)	O
(	O
cid:18	O
)	O
k−1	O
(	O
cid:2	O
)	O
(	O
cid:18	O
)	O
k−1	O
(	O
cid:2	O
)	O
k=1	O
k=1	O
=	O
exp	O
=	O
exp	O
=	O
exp	O
(	O
cid:18	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
xk	O
log	O
μk	O
+	O
(	O
cid:11	O
)	O
xk	O
log	O
(	O
cid:8	O
)	O
xk	O
log	O
μk	O
μk	O
(	O
cid:11	O
)	O
xk	O
log	O
μk	O
1	O
−	O
k−1	O
(	O
cid:2	O
)	O
1	O
−	O
(	O
cid:10	O
)	O
k−1	O
(	O
cid:9	O
)	O
μk	O
j=1	O
μj	O
k=1	O
(	O
cid:13	O
)	O
log	O
(	O
1	O
−	O
k−1	O
(	O
cid:2	O
)	O
+	O
log	O
(	O
1−	O
k−1	O
(	O
cid:2	O
)	O
k=1	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
μk	O
)	O
μk	O
)	O
k=1	O
xk	O
(	O
cid:13	O
)	O
(	O
cid:19	O
)	O
+	O
log	O
μk	O
k=1	O
where	O
μk	O
=	O
1	O
−	O
(	O
cid:10	O
)	O
k−1	O
cat	O
(	O
x|θ	O
)	O
=	O
exp	O
(	O
θt	O
φ	O
(	O
x	O
)	O
−	O
a	O
(	O
θ	O
)	O
)	O
μk−1	O
μk	O
θ	O
=	O
[	O
log	O
,	O
.	O
.	O
.	O
,	O
log	O
μ1	O
μk	O
]	O
φ	O
(	O
x	O
)	O
=	O
[	O
i	O
(	O
x	O
=	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
i	O
(	O
x	O
=	O
k	O
−	O
1	O
)	O
]	O
k=1	O
μk	O
.	O
we	O
can	O
write	O
this	O
in	O
exponential	B
family	I
form	O
as	O
follows	O
:	O
283	O
(	O
9.10	O
)	O
(	O
9.11	O
)	O
(	O
9.12	O
)	O
(	O
9.13	O
)	O
(	O
9.14	O
)	O
(	O
9.15	O
)	O
(	O
9.16	O
)	O
(	O
9.17	O
)	O
(	O
9.18	O
)	O
9.2.	O
the	O
exponential	B
family	I
(	O
cid:5	O
)	O
(	O
cid:6	O
)	O
now	O
we	O
have	O
φ	O
(	O
x	O
)	O
=x	O
,	O
θ	O
=	O
log	O
can	O
recover	O
the	O
mean	B
parameter	O
μ	O
from	O
the	O
canonical	O
parameter	O
using	O
μ	O
1−μ	O
,	O
which	O
is	O
the	O
log-odds	B
ratio	I
,	O
and	O
z	O
=	O
1/	O
(	O
1	O
−	O
μ	O
)	O
.	O
we	O
μ	O
=	O
sigm	O
(	O
θ	O
)	O
=	O
1	O
1	O
+	O
e−θ	O
9.2.2.2	O
multinoulli	O
(	O
9.9	O
)	O
we	O
can	O
represent	O
the	O
multinoulli	O
as	O
a	O
minimal	B
exponential	O
family	B
as	O
follows	O
(	O
where	O
xk	O
=	O
i	O
(	O
x	O
=	O
k	O
)	O
)	O
:	O
(	O
cid:19	O
)	O
we	O
can	O
recover	O
the	O
mean	B
parameters	O
from	O
the	O
canonical	B
parameters	I
using	O
μk	O
=	O
(	O
cid:10	O
)	O
k−1	O
eθk	O
j=1	O
eθj	O
1	O
+	O
from	O
this	O
,	O
we	O
ﬁnd	O
μk	O
=	O
1	O
−	O
(	O
cid:10	O
)	O
k−1	O
(	O
cid:10	O
)	O
k−1	O
j=1	O
eθj	O
1	O
+	O
and	O
hence	O
(	O
cid:11	O
)	O
a	O
(	O
θ	O
)	O
=	O
log	O
1	O
+	O
j=1	O
eθj	O
k−1	O
(	O
cid:2	O
)	O
eθk	O
1	O
(	O
cid:10	O
)	O
k−1	O
j=1	O
eθj	O
=	O
(	O
cid:13	O
)	O
k=1	O
if	O
we	O
deﬁne	O
θk	O
=	O
0	O
,	O
we	O
can	O
write	O
μ	O
=	O
s	O
(	O
θ	O
)	O
and	O
a	O
(	O
θ	O
)	O
=	O
log	O
softmax	O
function	O
in	O
equation	O
4.39	O
.	O
(	O
9.19	O
)	O
(	O
cid:10	O
)	O
k	O
k=1	O
eθk	O
,	O
where	O
s	O
is	O
the	O
284	O
chapter	O
9.	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	I
9.2.2.3	O
univariate	O
gaussian	O
the	O
univariate	O
gaussian	O
can	O
be	O
written	O
in	O
exponential	B
family	I
form	O
as	O
follows	O
:	O
n	O
(	O
x|μ	O
,	O
σ2	O
)	O
=	O
=	O
=	O
2σ2	O
(	O
x	O
−	O
μ	O
)	O
2	O
]	O
1	O
(	O
2πσ2	O
)	O
1	O
2	O
1	O
(	O
2πσ2	O
)	O
1	O
2	O
exp	O
[	O
−	O
1	O
exp	O
[	O
−	O
1	O
2σ2	O
x2	O
+	O
μ	O
σ2	O
x	O
−	O
1	O
2σ2	O
μ2	O
]	O
(	O
9.20	O
)	O
(	O
9.21	O
)	O
(	O
9.22	O
)	O
(	O
9.23	O
)	O
(	O
9.24	O
)	O
(	O
9.25	O
)	O
(	O
9.26	O
)	O
1	O
z	O
(	O
θ	O
)	O
exp	O
(	O
θt	O
φ	O
(	O
x	O
)	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
μ/σ2	O
−1	O
(	O
cid:9	O
)	O
2σ2	O
x	O
x2	O
√	O
2πσ	O
exp	O
[	O
−θ2	O
−	O
1	O
1	O
4θ2	O
2	O
μ2	O
2σ2	O
]	O
log	O
(	O
−2θ2	O
)	O
−	O
1	O
2	O
log	O
(	O
2π	O
)	O
where	O
θ	O
=	O
φ	O
(	O
x	O
)	O
=	O
z	O
(	O
μ	O
,	O
σ2	O
)	O
=	O
a	O
(	O
θ	O
)	O
=	O
9.2.2.4	O
non-examples	O
not	O
all	O
distributions	O
of	O
interest	O
belong	O
to	O
the	O
exponential	B
family	I
.	O
for	O
example	O
,	O
the	O
uniform	B
distribution	I
,	O
x	O
∼	O
unif	O
(	O
a	O
,	O
b	O
)	O
,	O
does	O
not	O
,	O
since	O
the	O
support	B
of	O
the	O
distribution	O
depends	O
on	O
the	O
parameters	O
.	O
also	O
,	O
the	O
student	O
t	O
distribution	O
(	O
section	O
11.4.5	O
)	O
does	O
not	O
belong	O
,	O
since	O
it	O
does	O
not	O
have	O
the	O
required	O
form	O
.	O
9.2.3	O
log	B
partition	I
function	I
an	O
important	O
property	O
of	O
the	O
exponential	B
family	I
is	O
that	O
derivatives	O
of	O
the	O
log	B
partition	I
function	I
for	O
this	O
reason	O
,	O
a	O
(	O
θ	O
)	O
is	O
can	O
be	O
used	O
to	O
generate	O
cumulants	O
of	O
the	O
sufficient	O
statistics.2	O
sometimes	O
called	O
a	O
cumulant	B
function	I
.	O
we	O
will	O
prove	O
this	O
for	O
a	O
1-parameter	O
distribution	O
;	O
this	O
can	O
be	O
generalized	O
to	O
a	O
k-parameter	O
distribution	O
in	O
a	O
straightforward	O
way	O
.	O
for	O
the	O
ﬁrst	O
2.	O
the	O
ﬁrst	O
and	O
second	O
cumulants	O
of	O
a	O
distribution	O
are	O
its	O
mean	B
e	O
[	O
x	O
]	O
and	O
variance	B
var	O
[	O
x	O
]	O
,	O
whereas	O
the	O
ﬁrst	O
and	O
second	O
moments	O
are	O
its	O
mean	B
e	O
[	O
x	O
]	O
and	O
e	O
x	O
2	O
(	O
cid:5	O
)	O
(	O
cid:6	O
)	O
.	O
9.2.	O
the	O
exponential	B
family	I
derivative	O
we	O
have	O
(	O
cid:8	O
)	O
)	O
(	O
cid:28	O
)	O
(	O
cid:9	O
)	O
log	O
exp	O
(	O
θφ	O
(	O
x	O
)	O
)	O
h	O
(	O
x	O
)	O
dx	O
exp	O
(	O
θφ	O
(	O
x	O
)	O
)	O
h	O
(	O
x	O
)	O
dx	O
exp	O
(	O
θφ	O
(	O
x	O
)	O
)	O
h	O
(	O
x	O
)	O
dx	O
φ	O
(	O
x	O
)	O
exp	O
(	O
θφ	O
(	O
x	O
)	O
)	O
h	O
(	O
x	O
)	O
dx	O
exp	O
(	O
a	O
(	O
θ	O
)	O
)	O
φ	O
(	O
x	O
)	O
exp	O
(	O
θφ	O
(	O
x	O
)	O
−	O
a	O
(	O
θ	O
)	O
)	O
h	O
(	O
x	O
)	O
dx	O
φ	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
dx	O
=	O
e	O
[	O
φ	O
(	O
x	O
)	O
]	O
da	O
dθ	O
=	O
=	O
=	O
=	O
=	O
d	O
dθ	O
d	O
dθ	O
)	O
)	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
for	O
the	O
second	O
derivative	O
we	O
have	O
d2a	O
dθ2	O
=	O
φ	O
(	O
x	O
)	O
exp	O
(	O
θφ	O
(	O
x	O
)	O
−	O
a	O
(	O
θ	O
)	O
)	O
h	O
(	O
x	O
)	O
(	O
φ	O
(	O
x	O
)	O
−	O
a	O
(	O
cid:4	O
)	O
φ	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
(	O
φ	O
(	O
x	O
)	O
−	O
a	O
(	O
cid:4	O
)	O
φ2	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
dx	O
−	O
a	O
(	O
cid:4	O
)	O
−	O
e	O
[	O
φ	O
(	O
x	O
)	O
]	O
(	O
cid:31	O
)	O
φ2	O
(	O
x	O
)	O
=	O
var	O
[	O
φ	O
(	O
x	O
)	O
]	O
φ	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
dx	O
(	O
θ	O
)	O
)	O
dx	O
(	O
θ	O
)	O
(	O
cid:28	O
)	O
2	O
=	O
=	O
=	O
e	O
where	O
we	O
used	O
the	O
fact	O
that	O
a	O
(	O
cid:4	O
)	O
(	O
θ	O
)	O
=	O
da	O
in	O
the	O
multivariate	O
case	O
,	O
we	O
have	O
that	O
dθ	O
=	O
e	O
[	O
φ	O
(	O
x	O
)	O
]	O
.	O
∂2a	O
∂θi∂θj	O
=	O
e	O
[	O
φi	O
(	O
x	O
)	O
φj	O
(	O
x	O
)	O
]	O
−	O
e	O
[	O
φi	O
(	O
x	O
)	O
]	O
e	O
[	O
φj	O
(	O
x	O
)	O
]	O
and	O
hence	O
∇2a	O
(	O
θ	O
)	O
=	O
cov	O
[	O
φ	O
(	O
x	O
)	O
]	O
(	O
θ	O
)	O
)	O
dx	O
285	O
(	O
9.27	O
)	O
(	O
9.28	O
)	O
(	O
9.29	O
)	O
(	O
9.30	O
)	O
(	O
9.31	O
)	O
(	O
9.32	O
)	O
(	O
9.33	O
)	O
(	O
9.34	O
)	O
(	O
9.35	O
)	O
(	O
9.36	O
)	O
(	O
9.37	O
)	O
since	O
the	O
covariance	B
is	O
positive	B
deﬁnite	I
,	O
we	O
see	O
that	O
a	O
(	O
θ	O
)	O
is	O
a	O
convex	B
function	O
(	O
see	O
section	O
7.3.3	O
)	O
.	O
9.2.3.1	O
example	O
:	O
the	O
bernoulli	O
distribution	O
for	O
example	O
,	O
consider	O
the	O
bernoulli	O
distribution	O
.	O
we	O
have	O
a	O
(	O
θ	O
)	O
=	O
log	O
(	O
1	O
+	O
eθ	O
)	O
,	O
so	O
the	O
mean	B
is	O
given	O
by	O
da	O
dθ	O
=	O
eθ	O
1	O
+	O
eθ	O
=	O
1	O
1	O
+	O
e−θ	O
=	O
sigm	O
(	O
θ	O
)	O
=	O
μ	O
the	O
variance	B
is	O
given	O
by	O
d2a	O
dθ2	O
=	O
=	O
d	O
dθ	O
(	O
1	O
+	O
e−θ	O
)	O
e−θ	O
1	O
1	O
+	O
e−θ	O
1	O
+	O
e−θ	O
=	O
1	O
eθ	O
+	O
1	O
−1	O
=	O
(	O
1	O
+	O
e−θ	O
)	O
−2.e−θ	O
1	O
+	O
e−θ	O
=	O
(	O
1	O
−	O
μ	O
)	O
μ	O
1	O
(	O
9.38	O
)	O
(	O
9.39	O
)	O
(	O
9.40	O
)	O
286	O
chapter	O
9.	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	I
9.2.4	O
mle	O
for	O
the	O
exponential	B
family	I
the	O
likelihood	B
of	O
an	O
exponential	B
family	I
model	O
has	O
the	O
form	O
(	O
cid:18	O
)	O
n	O
(	O
cid:27	O
)	O
(	O
cid:19	O
)	O
(	O
cid:11	O
)	O
(	O
cid:13	O
)	O
n	O
(	O
cid:2	O
)	O
h	O
(	O
xi	O
)	O
g	O
(	O
θ	O
)	O
n	O
exp	O
η	O
(	O
θ	O
)	O
t	O
[	O
φ	O
(	O
xi	O
)	O
]	O
i=1	O
i=1	O
we	O
see	O
that	O
the	O
sufficient	B
statistics	I
are	O
n	O
and	O
φ1	O
(	O
xi	O
)	O
,	O
.	O
.	O
.	O
,	O
φk	O
(	O
xi	O
)	O
]	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
cid:10	O
)	O
(	O
cid:10	O
)	O
(	O
cid:10	O
)	O
p	O
(	O
d|θ	O
)	O
=	O
φ	O
(	O
d	O
)	O
=	O
[	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
9.41	O
)	O
(	O
9.42	O
)	O
for	O
example	O
,	O
for	O
the	O
bernoulli	O
model	O
we	O
have	O
φ	O
=	O
[	O
gaussian	O
,	O
we	O
have	O
φ	O
=	O
[	O
i	O
xi	O
,	O
i	O
x2	O
i	O
]	O
.	O
(	O
we	O
also	O
need	O
to	O
know	O
the	O
sample	O
size	O
,	O
n	O
.	O
)	O
i	O
i	O
(	O
xi	O
=	O
1	O
)	O
]	O
,	O
and	O
for	O
the	O
univariate	O
the	O
pitman-koopman-darmois	O
theorem	O
states	O
that	O
,	O
under	O
certain	O
regularity	O
conditions	O
,	O
the	O
exponential	B
family	I
is	O
the	O
only	O
family	B
of	O
distributions	O
with	O
ﬁnite	O
sufficient	O
statistics	O
.	O
(	O
here	O
,	O
ﬁnite	O
means	O
of	O
a	O
size	O
independent	O
of	O
the	O
size	O
of	O
the	O
data	O
set	O
.	O
)	O
one	O
of	O
the	O
conditions	O
required	O
in	O
this	O
theorem	O
is	O
that	O
the	O
support	B
of	O
the	O
distribution	O
not	O
be	O
dependent	O
on	O
the	O
parameter	B
.	O
for	O
a	O
simple	O
example	O
of	O
such	O
a	O
distribution	O
,	O
consider	O
the	O
uniform	B
distribution	I
p	O
(	O
x|θ	O
)	O
=	O
u	O
(	O
x|θ	O
)	O
=	O
i	O
(	O
0	O
≤	O
x	O
≤	O
θ	O
)	O
1	O
θ	O
the	O
likelihood	B
is	O
given	O
by	O
(	O
9.43	O
)	O
p	O
(	O
d|θ	O
)	O
=	O
θ−n	O
i	O
(	O
0	O
≤	O
max	O
{	O
xi	O
}	O
≤θ	O
)	O
(	O
9.44	O
)	O
so	O
the	O
sufficient	B
statistics	I
are	O
n	O
and	O
s	O
(	O
d	O
)	O
=	O
maxi	O
xi	O
.	O
this	O
is	O
ﬁnite	O
in	O
size	O
,	O
but	O
the	O
uni-	O
form	O
distribution	O
is	O
not	O
in	O
the	O
exponential	B
family	I
because	O
its	O
support	B
set	O
,	O
x	O
,	O
depends	O
on	O
the	O
parameters	O
.	O
n	O
iid	B
data	O
points	O
d	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
,	O
the	O
log-likelihood	O
is	O
we	O
now	O
descibe	O
how	O
to	O
compute	O
the	O
mle	O
for	O
a	O
canonical	O
exponential	O
family	B
model	O
.	O
given	O
log	O
p	O
(	O
d|θ	O
)	O
=	O
θt	O
φ	O
(	O
d	O
)	O
−	O
n	O
a	O
(	O
θ	O
)	O
(	O
9.45	O
)	O
since	O
−a	O
(	O
θ	O
)	O
is	O
concave	B
in	O
θ	O
,	O
and	O
θt	O
φ	O
(	O
d	O
)	O
is	O
linear	O
in	O
θ	O
,	O
we	O
see	O
that	O
the	O
log	O
likelihood	O
is	O
concave	B
,	O
and	O
hence	O
has	O
a	O
unique	O
global	O
maximum	O
.	O
to	O
derive	O
this	O
maximum	O
,	O
we	O
use	O
the	O
fact	O
that	O
the	O
derivative	O
of	O
the	O
log	B
partition	I
function	I
yields	O
the	O
expected	B
value	I
of	O
the	O
sufficient	O
statistic	O
vector	O
(	O
section	O
9.2.3	O
)	O
:	O
∇θ	O
log	O
p	O
(	O
d|θ	O
)	O
=	O
φ	O
(	O
d	O
)	O
−	O
n	O
e	O
[	O
φ	O
(	O
x	O
)	O
]	O
setting	O
this	O
gradient	O
to	O
zero	O
,	O
we	O
see	O
that	O
at	O
the	O
mle	O
,	O
the	O
empirical	O
average	O
of	O
the	O
sufficient	O
(	O
9.46	O
)	O
statistics	O
must	O
equal	O
the	O
model	O
’	O
s	O
theoretical	O
expected	B
sufficient	I
statistics	I
,	O
i.e.	O
,	O
ˆθ	O
must	O
satisfy	O
φ	O
(	O
xi	O
)	O
(	O
9.47	O
)	O
e	O
[	O
φ	O
(	O
x	O
)	O
]	O
=	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
i=1	O
9.2.	O
the	O
exponential	B
family	I
287	O
this	O
is	O
called	O
moment	B
matching	I
.	O
for	O
example	O
,	O
in	O
the	O
bernoulli	O
distribution	O
,	O
we	O
have	O
φ	O
(	O
x	O
)	O
=	O
i	O
(	O
x	O
=	O
1	O
)	O
,	O
so	O
the	O
mle	O
satisﬁes	O
e	O
[	O
φ	O
(	O
x	O
)	O
]	O
=	O
p	O
(	O
x	O
=	O
1	O
)	O
=	O
ˆμ	O
=	O
1	O
n	O
i	O
(	O
xi	O
=	O
1	O
)	O
(	O
9.48	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
9.2.5	O
bayes	O
for	O
the	O
exponential	B
family	I
*	O
we	O
have	O
seen	O
that	O
exact	O
bayesian	O
analysis	O
is	O
considerably	O
simpliﬁed	O
if	O
the	O
prior	O
is	O
conjugate	O
to	O
the	O
likelihood	B
.	O
informally	O
this	O
means	O
that	O
the	O
prior	O
p	O
(	O
θ|τ	O
)	O
has	O
the	O
same	O
form	O
as	O
the	O
likelihood	B
p	O
(	O
d|θ	O
)	O
.	O
for	O
this	O
to	O
make	O
sense	O
,	O
we	O
require	O
that	O
the	O
likelihood	B
have	O
ﬁnite	O
sufficient	O
statistics	O
,	O
so	O
that	O
we	O
can	O
write	O
p	O
(	O
d|θ	O
)	O
=	O
p	O
(	O
s	O
(	O
d	O
)	O
|θ	O
)	O
.	O
this	O
suggests	O
that	O
the	O
only	O
family	B
of	O
distributions	O
for	O
which	O
conjugate	B
priors	I
exist	O
is	O
the	O
exponential	B
family	I
.	O
we	O
will	O
derive	O
the	O
form	O
of	O
the	O
prior	O
and	O
posterior	O
below	O
.	O
9.2.5.1	O
likelihood	B
the	O
likelihood	B
of	O
the	O
exponential	B
family	I
is	O
given	O
by	O
p	O
(	O
d|θ	O
)	O
∝	O
g	O
(	O
θ	O
)	O
n	O
exp	O
η	O
(	O
θ	O
)	O
t	O
sn	O
(	O
cid:3	O
)	O
(	O
cid:4	O
)	O
(	O
cid:10	O
)	O
n	O
p	O
(	O
d|η	O
)	O
∝	O
exp	O
(	O
n	O
ηt	O
s	O
−	O
n	O
a	O
(	O
η	O
)	O
)	O
where	O
sn	O
=	O
i=1	O
s	O
(	O
xi	O
)	O
.	O
in	O
terms	O
of	O
the	O
canonical	B
parameters	I
this	O
becomes	O
(	O
9.49	O
)	O
(	O
9.50	O
)	O
where	O
s	O
=	O
1	O
n	O
sn	O
.	O
9.2.5.2	O
prior	O
the	O
natural	O
conjugate	O
prior	O
has	O
the	O
form	O
η	O
(	O
θ	O
)	O
t	O
τ	O
0	O
p	O
(	O
θ|ν0	O
,	O
τ	O
0	O
)	O
∝	O
g	O
(	O
θ	O
)	O
ν0	O
exp	O
(	O
cid:3	O
)	O
(	O
cid:4	O
)	O
(	O
9.51	O
)	O
let	O
us	O
write	O
τ	O
0	O
=	O
ν0τ	O
0	O
,	O
to	O
separate	O
out	O
the	O
size	O
of	O
the	O
prior	O
pseudo-data	O
,	O
ν0	O
,	O
from	O
the	O
mean	B
of	O
the	O
sufficient	B
statistics	I
on	O
this	O
pseudo-data	O
,	O
τ	O
0.	O
in	O
canonical	B
form	I
,	O
the	O
prior	O
becomes	O
p	O
(	O
η|ν0	O
,	O
τ	O
0	O
)	O
∝	O
exp	O
(	O
ν0ηt	O
τ	O
0	O
−	O
ν0a	O
(	O
η	O
)	O
)	O
9.2.5.3	O
posterior	O
the	O
posterior	O
is	O
given	O
by	O
p	O
(	O
θ|d	O
)	O
=	O
p	O
(	O
θ|νn	O
,	O
τ	O
n	O
)	O
=	O
p	O
(	O
θ|ν0	O
+	O
n	O
,	O
τ	O
0	O
+	O
sn	O
)	O
so	O
we	O
see	O
that	O
we	O
just	O
update	O
the	O
hyper-parameters	B
by	O
adding	O
.	O
in	O
canonical	B
form	I
,	O
this	O
becomes	O
p	O
(	O
η|d	O
)	O
∝	O
exp	O
ηt	O
(	O
ν0τ	O
0	O
+	O
n	O
s	O
)	O
−	O
(	O
ν0	O
+	O
n	O
)	O
a	O
(	O
η	O
)	O
)	O
(	O
cid:3	O
)	O
(	O
cid:4	O
)	O
=	O
p	O
(	O
η|ν0	O
+	O
n	O
,	O
ν0τ	O
0	O
+	O
n	O
s	O
ν0	O
+	O
n	O
)	O
so	O
we	O
see	O
that	O
the	O
posterior	O
hyper-parameters	O
are	O
a	O
convex	B
combination	I
of	O
the	O
prior	O
mean	B
hyper-parameters	O
and	O
the	O
average	O
of	O
the	O
sufficient	B
statistics	I
.	O
(	O
9.52	O
)	O
(	O
9.53	O
)	O
(	O
9.54	O
)	O
(	O
9.55	O
)	O
288	O
chapter	O
9.	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	I
9.2.5.4	O
posterior	B
predictive	I
density	I
let	O
us	O
derive	O
a	O
generic	O
expression	O
for	O
the	O
predictive	B
density	O
for	O
future	O
observables	O
d	O
(	O
cid:4	O
)	O
=	O
(	O
˜x1	O
,	O
.	O
.	O
.	O
,	O
˜xn	O
(	O
cid:2	O
)	O
)	O
given	O
past	O
data	O
d	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
as	O
follows	O
.	O
for	O
notational	O
brevity	O
,	O
we	O
will	O
combine	O
the	O
sufficient	B
statistics	I
with	O
the	O
size	O
of	O
the	O
data	O
,	O
as	O
follows	O
:	O
˜τ	O
0	O
=	O
(	O
ν0	O
,	O
τ	O
0	O
)	O
,	O
˜s	O
(	O
d	O
)	O
=	O
(	O
n	O
,	O
s	O
(	O
d	O
)	O
)	O
,	O
and	O
˜s	O
(	O
d	O
(	O
cid:4	O
)	O
)	O
)	O
.	O
so	O
the	O
prior	O
becomes	O
)	O
=	O
(	O
n	O
(	O
cid:4	O
)	O
,	O
s	O
(	O
d	O
(	O
cid:4	O
)	O
p	O
(	O
θ|˜τ	O
0	O
)	O
=	O
1	O
z	O
(	O
˜τ	O
0	O
)	O
g	O
(	O
θ	O
)	O
ν0	O
exp	O
(	O
η	O
(	O
θ	O
)	O
t	O
τ	O
0	O
)	O
the	O
likelihood	B
and	O
posterior	O
have	O
a	O
similar	B
form	O
.	O
hence	O
p	O
(	O
d	O
(	O
cid:4	O
)	O
|d	O
)	O
=	O
=	O
=	O
(	O
cid:28	O
)	O
p	O
(	O
d	O
(	O
cid:4	O
)	O
|θ	O
)	O
p	O
(	O
θ|d	O
)	O
dθ	O
⎡	O
⎣	O
n	O
(	O
cid:2	O
)	O
(	O
cid:27	O
)	O
h	O
(	O
˜xi	O
)	O
(	O
cid:28	O
)	O
⎤	O
⎦	O
z	O
(	O
˜τ	O
0	O
+	O
˜s	O
(	O
d	O
)	O
)	O
−1	O
n	O
(	O
cid:2	O
)	O
⎛	O
⎝	O
(	O
cid:2	O
)	O
⎤	O
⎦	O
z	O
(	O
˜τ	O
0	O
+	O
˜s	O
(	O
d	O
)	O
+	O
˜s	O
(	O
d	O
(	O
cid:4	O
)	O
z	O
(	O
˜τ	O
0	O
+	O
˜s	O
(	O
d	O
)	O
)	O
ηk	O
(	O
θ	O
)	O
(	O
τk	O
+	O
i=1	O
k	O
h	O
(	O
˜xi	O
)	O
i=1	O
×	O
exp	O
⎡	O
⎣	O
n	O
(	O
cid:2	O
)	O
(	O
cid:27	O
)	O
i=1	O
)	O
)	O
g	O
(	O
θ	O
)	O
ν0+n	O
+n	O
(	O
cid:2	O
)	O
dθ	O
⎞	O
⎠	O
dθ	O
n	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i=1	O
sk	O
(	O
˜xi	O
)	O
sk	O
(	O
xi	O
)	O
+	O
if	O
n	O
=	O
0	O
,	O
this	O
becomes	O
the	O
marginal	B
likelihood	I
of	O
d	O
(	O
cid:4	O
)	O
normalizer	O
of	O
the	O
posterior	O
divided	O
by	O
the	O
normalizer	O
of	O
the	O
prior	O
,	O
multiplied	O
by	O
a	O
constant	O
.	O
,	O
which	O
reduces	O
to	O
the	O
familiar	O
form	O
of	O
9.2.5.5	O
example	O
:	O
bernoulli	O
distribution	O
as	O
a	O
simple	O
example	O
,	O
let	O
us	O
revisit	O
the	O
beta-bernoulli	O
model	O
in	O
our	O
new	O
notation	O
.	O
(	O
9.56	O
)	O
(	O
9.57	O
)	O
(	O
9.58	O
)	O
(	O
9.59	O
)	O
(	O
9.60	O
)	O
(	O
9.61	O
)	O
(	O
9.62	O
)	O
(	O
9.63	O
)	O
the	O
likelihood	B
is	O
given	O
by	O
p	O
(	O
d|θ	O
)	O
=	O
(	O
1	O
−	O
θ	O
)	O
n	O
exp	O
(	O
cid:11	O
)	O
log	O
(	O
hence	O
the	O
conjugate	B
prior	I
is	O
given	O
by	O
p	O
(	O
θ|ν0	O
,	O
τ0	O
)	O
∝	O
(	O
1	O
−	O
θ	O
)	O
ν0	O
exp	O
=	O
θτ0	O
(	O
1	O
−	O
θ	O
)	O
ν0−τ0	O
log	O
(	O
θ	O
1	O
−	O
θ	O
(	O
cid:8	O
)	O
(	O
cid:13	O
)	O
(	O
cid:2	O
)	O
)	O
i	O
xi	O
(	O
cid:9	O
)	O
θ	O
1	O
−	O
θ	O
)	O
τ0	O
if	O
we	O
deﬁne	O
α	O
=	O
τ0	O
+	O
1	O
and	O
β	O
=	O
ν0	O
−	O
τ0	O
+	O
1	O
,	O
we	O
see	O
that	O
this	O
is	O
a	O
beta	B
distribution	I
.	O
(	O
cid:10	O
)	O
i	O
i	O
(	O
xi	O
=	O
1	O
)	O
is	O
the	O
sufficient	O
statistic	O
:	O
we	O
can	O
derive	O
the	O
posterior	O
as	O
follows	O
,	O
where	O
s	O
=	O
p	O
(	O
θ|d	O
)	O
∝	O
θτ0+s	O
(	O
1	O
−	O
θ	O
)	O
ν0−τ0+n−s	O
=	O
θτn	O
(	O
1	O
−	O
θ	O
)	O
νn−τn	O
(	O
9.64	O
)	O
(	O
9.65	O
)	O
we	O
can	O
derive	O
the	O
posterior	B
predictive	I
distribution	I
as	O
follows	O
.	O
assume	O
p	O
(	O
θ	O
)	O
=	O
beta	O
(	O
θ|α	O
,	O
β	O
)	O
,	O
and	O
let	O
s	O
=	O
s	O
(	O
d	O
)	O
be	O
the	O
number	O
of	O
heads	O
in	O
the	O
past	O
data	O
.	O
we	O
can	O
predict	O
the	O
probability	O
of	O
a	O
9.2.	O
the	O
exponential	B
family	I
given	O
sequence	O
of	O
future	O
heads	O
,	O
d	O
(	O
cid:4	O
)	O
1	O
)	O
,	O
as	O
follows	O
:	O
(	O
cid:28	O
)	O
1	O
p	O
(	O
d	O
(	O
cid:4	O
)	O
|d	O
)	O
=	O
p	O
(	O
d	O
(	O
cid:4	O
)	O
|θ|beta	O
(	O
θ|αn	O
,	O
βn	O
)	O
dθ	O
0	O
γ	O
(	O
αn	O
+	O
βn	O
)	O
γ	O
(	O
αn	O
)	O
γ	O
(	O
βn	O
)	O
γ	O
(	O
αn	O
+	O
βn	O
)	O
γ	O
(	O
αn	O
)	O
γ	O
(	O
βn	O
)	O
=	O
=	O
where	O
(	O
cid:28	O
)	O
1	O
θαn+t	O
(	O
cid:2	O
)	O
−1	O
(	O
1	O
−	O
θ	O
)	O
βn+m−t	O
(	O
cid:2	O
)	O
−1dθ	O
0	O
γ	O
(	O
αn+m	O
)	O
γ	O
(	O
βn+m	O
)	O
γ	O
(	O
αn+m	O
+	O
βn+m	O
)	O
αn+m	O
=	O
αn	O
+	O
s	O
(	O
cid:4	O
)	O
βn+m	O
=	O
βn	O
+	O
(	O
m	O
−	O
s	O
(	O
cid:4	O
)	O
=	O
α	O
+	O
s	O
+	O
s	O
(	O
cid:4	O
)	O
)	O
=	O
β	O
+	O
(	O
n	O
−	O
s	O
)	O
+	O
(	O
m	O
−	O
s	O
(	O
cid:4	O
)	O
)	O
9.2.6	O
maximum	B
entropy	I
derivation	O
of	O
the	O
exponential	B
family	I
*	O
=	O
(	O
˜x1	O
,	O
.	O
.	O
.	O
,	O
˜xm	O
)	O
,	O
with	O
sufficient	O
statistic	O
s	O
(	O
cid:4	O
)	O
=	O
i=1	O
i	O
(	O
˜xi	O
=	O
289	O
(	O
cid:10	O
)	O
m	O
(	O
9.66	O
)	O
(	O
9.67	O
)	O
(	O
9.68	O
)	O
(	O
9.69	O
)	O
(	O
9.70	O
)	O
although	O
the	O
exponential	B
family	I
is	O
convenient	O
,	O
is	O
there	O
any	O
deeper	O
justiﬁcation	O
for	O
its	O
use	O
?	O
it	O
turns	O
out	O
that	O
there	O
is	O
:	O
it	O
is	O
the	O
distribution	O
that	O
makes	O
the	O
least	O
number	O
of	O
assumptions	O
about	O
the	O
data	O
,	O
subject	O
to	O
a	O
speciﬁc	O
set	O
of	O
user-speciﬁed	O
constraints	O
,	O
as	O
we	O
explain	O
below	O
.	O
in	O
particular	O
,	O
suppose	O
all	O
we	O
know	O
is	O
the	O
expected	O
values	O
of	O
certain	O
features	B
or	O
functions	O
:	O
fk	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
=	O
fk	O
(	O
9.71	O
)	O
(	O
cid:2	O
)	O
x	O
where	O
fk	O
are	O
known	O
constants	O
,	O
and	O
fk	O
(	O
x	O
)	O
is	O
an	O
arbitrary	O
function	O
.	O
the	O
principle	O
of	O
maximum	O
entropy	B
or	O
maxent	B
says	O
we	O
should	O
pick	O
the	O
distribution	O
with	O
maximum	B
entropy	I
(	O
closest	O
to	O
uniform	O
)	O
,	O
subject	O
to	O
the	O
constraints	O
that	O
the	O
moments	O
of	O
the	O
distribution	O
match	O
the	O
empirical	O
moments	O
of	O
the	O
speciﬁed	O
functions	O
.	O
p	O
(	O
x	O
)	O
≥	O
0	O
and	O
j	O
(	O
p	O
,	O
λ	O
)	O
=	O
−	O
(	O
cid:10	O
)	O
to	O
maximize	O
entropy	B
subject	O
to	O
the	O
constraints	O
in	O
equation	O
9.71	O
,	O
and	O
the	O
constraints	O
that	O
(	O
cid:2	O
)	O
x	O
p	O
(	O
x	O
)	O
=	O
1	O
,	O
we	O
need	O
to	O
use	O
lagrange	O
multipliers	O
.	O
the	O
lagrangian	O
is	O
given	O
by	O
p	O
(	O
x	O
)	O
log	O
p	O
(	O
x	O
)	O
+λ	O
0	O
(	O
1	O
−	O
λk	O
(	O
fk	O
−	O
p	O
(	O
x	O
)	O
)	O
+	O
p	O
(	O
x	O
)	O
fk	O
(	O
x	O
)	O
)	O
(	O
9.72	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
x	O
x	O
k	O
x	O
we	O
can	O
use	O
the	O
calculus	B
of	I
variations	I
to	O
take	O
derivatives	O
wrt	O
the	O
function	O
p	O
,	O
but	O
we	O
will	O
adopt	O
a	O
simpler	O
approach	O
and	O
treat	O
p	O
as	O
a	O
ﬁxed	O
length	O
vector	O
(	O
since	O
we	O
are	O
assuming	O
x	O
is	O
discrete	B
)	O
.	O
then	O
we	O
have	O
(	O
cid:2	O
)	O
k	O
∂j	O
∂p	O
(	O
x	O
)	O
=	O
−1	O
−	O
log	O
p	O
(	O
x	O
)	O
−	O
λ0	O
−	O
setting	O
∂j	O
∂p	O
(	O
x	O
)	O
=	O
0	O
yields	O
(	O
cid:2	O
)	O
exp	O
(	O
−	O
1	O
z	O
k	O
p	O
(	O
x	O
)	O
=	O
λkfk	O
(	O
x	O
)	O
)	O
λkfk	O
(	O
x	O
)	O
(	O
9.73	O
)	O
(	O
9.74	O
)	O
290	O
chapter	O
9.	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	I
w	O
xi	O
−1	O
g	O
g	O
ηi	O
μi	O
ψ	O
ψ−1	O
θi	O
figure	O
9.1	O
a	O
visualization	O
of	O
the	O
various	O
features	B
of	O
a	O
glm	O
.	O
based	O
on	O
figure	O
8.3	O
of	O
(	O
jordan	O
2007	O
)	O
.	O
where	O
z	O
=	O
e1+λ0	O
.	O
using	O
the	O
sum	O
to	O
one	O
constraint	O
,	O
we	O
have	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
1	O
=	O
x	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
exp	O
(	O
−	O
x	O
k	O
λkfk	O
(	O
x	O
)	O
)	O
hence	O
the	O
normalization	O
constant	O
is	O
given	O
by	O
z	O
=	O
exp	O
(	O
−	O
λkfk	O
(	O
x	O
)	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
9.75	O
)	O
(	O
9.76	O
)	O
x	O
k	O
thus	O
the	O
maxent	B
distribution	O
p	O
(	O
x	O
)	O
has	O
the	O
form	O
of	O
the	O
exponential	B
family	I
(	O
section	O
9.2	O
)	O
,	O
also	O
known	O
as	O
the	O
gibbs	O
distribution	O
.	O
9.3	O
generalized	B
linear	I
models	I
(	O
glms	O
)	O
linear	O
and	O
logistic	B
regression	I
are	O
examples	O
of	O
generalized	B
linear	I
models	I
,	O
or	O
glms	O
(	O
mccullagh	O
and	O
nelder	O
1989	O
)	O
.	O
these	O
are	O
models	O
in	O
which	O
the	O
output	O
density	O
is	O
in	O
the	O
exponential	B
family	I
(	O
section	O
9.2	O
)	O
,	O
and	O
in	O
which	O
the	O
mean	B
parameters	O
are	O
a	O
linear	O
combination	O
of	O
the	O
inputs	O
,	O
passed	O
through	O
a	O
possibly	O
nonlinear	O
function	O
,	O
such	O
as	O
the	O
logistic	B
function	O
.	O
we	O
describe	O
glms	O
in	O
more	O
detail	O
below	O
.	O
we	O
focus	O
on	O
scalar	O
outputs	O
for	O
notational	O
simplicity	O
.	O
(	O
this	O
excludes	O
multinomial	B
logistic	I
regression	I
,	O
but	O
this	O
is	O
just	O
to	O
simplify	O
the	O
presentation	O
.	O
)	O
9.3.1	O
basics	O
to	O
understand	O
glms	O
,	O
let	O
us	O
ﬁrst	O
consider	O
the	O
case	O
of	O
an	O
unconditional	O
dstribution	O
for	O
a	O
scalar	O
response	O
variable	O
:	O
p	O
(	O
yi|θ	O
,	O
σ2	O
)	O
=	O
exp	O
+	O
c	O
(	O
yi	O
,	O
σ2	O
)	O
(	O
9.77	O
)	O
(	O
cid:29	O
)	O
yiθ	O
−	O
a	O
(	O
θ	O
)	O
σ2	O
(	O
cid:30	O
)	O
where	O
σ2	O
is	O
the	O
dispersion	B
parameter	I
(	O
often	O
set	O
to	O
1	O
)	O
,	O
θ	O
is	O
the	O
natural	O
parameter	O
,	O
a	O
is	O
the	O
partition	B
function	I
,	O
and	O
c	O
is	O
a	O
normalization	O
constant	O
.	O
for	O
example	O
,	O
in	O
the	O
case	O
of	O
logistic	B
regression	I
,	O
θ	O
is	O
the	O
log-odds	B
ratio	I
,	O
θ	O
=	O
log	O
(	O
μ	O
1−μ	O
)	O
,	O
where	O
μ	O
=	O
e	O
[	O
y	O
]	O
=p	O
(	O
y	O
=	O
1	O
)	O
is	O
the	O
mean	B
parameter	O
(	O
see	O
section	O
9.2.2.1	O
)	O
.	O
to	O
convert	O
from	O
the	O
mean	B
parameter	O
to	O
the	O
natural	O
parameter	O
9.3.	O
generalized	B
linear	I
models	I
(	O
glms	O
)	O
291	O
distrib	O
.	O
n	O
(	O
μ	O
,	O
σ2	O
)	O
bin	O
(	O
n	O
,	O
μ	O
)	O
poi	O
(	O
μ	O
)	O
link	O
g	O
(	O
μ	O
)	O
identity	O
logit	B
log	O
θ	O
=	O
ψ	O
(	O
μ	O
)	O
θ	O
=	O
μ	O
θ	O
=	O
log	O
(	O
μ	O
θ	O
=	O
log	O
(	O
μ	O
)	O
1−μ	O
)	O
μ	O
=	O
sigm	O
(	O
θ	O
)	O
μ	O
=	O
eθ	O
μ	O
=	O
ψ−1	O
(	O
θ	O
)	O
=	O
e	O
[	O
y	O
]	O
μ	O
=	O
θ	O
table	O
9.1	O
canonical	O
link	O
functions	O
ψ	O
and	O
their	O
inverses	O
for	O
some	O
common	O
glms	O
.	O
we	O
can	O
use	O
a	O
function	O
ψ	O
,	O
so	O
θ	O
=	O
ψ	O
(	O
μ	O
)	O
.	O
this	O
function	O
is	O
uniquely	O
determined	O
by	O
the	O
form	O
of	O
the	O
−1	O
(	O
θ	O
)	O
.	O
exponential	B
family	I
distribution	O
.	O
in	O
fact	O
,	O
this	O
is	O
an	O
invertible	O
mapping	O
,	O
so	O
we	O
have	O
μ	O
=	O
ψ	O
furthermore	O
,	O
we	O
know	O
from	O
section	O
9.2.3	O
that	O
the	O
mean	B
is	O
given	O
by	O
the	O
derivative	O
of	O
the	O
partition	B
function	I
,	O
so	O
we	O
have	O
μ	O
=	O
ψ	O
−1	O
(	O
θ	O
)	O
=	O
a	O
(	O
cid:4	O
)	O
(	O
θ	O
)	O
.	O
now	O
let	O
us	O
add	O
inputs/	O
covariates	B
.	O
we	O
ﬁrst	O
deﬁne	O
a	O
linear	O
function	O
of	O
the	O
inputs	O
:	O
ηi	O
=	O
wt	O
xi	O
(	O
9.78	O
)	O
we	O
now	O
make	O
the	O
mean	B
of	O
the	O
distribution	O
be	O
some	O
invertible	O
monotonic	O
function	O
of	O
this	O
linear	O
combination	O
.	O
by	O
convention	O
,	O
this	O
function	O
,	O
known	O
as	O
the	O
mean	B
function	I
,	O
is	O
denoted	O
by	O
g−1	O
,	O
so	O
(	O
9.79	O
)	O
μi	O
=	O
g−1	O
(	O
ηi	O
)	O
=	O
g−1	O
(	O
wt	O
xi	O
)	O
see	O
figure	O
9.1	O
for	O
a	O
summary	O
of	O
the	O
basic	O
model	O
.	O
the	O
inverse	O
of	O
the	O
mean	B
function	I
,	O
namely	O
g	O
(	O
)	O
,	O
is	O
called	O
the	O
link	B
function	I
.	O
we	O
are	O
free	O
to	O
choose	O
almost	O
any	O
function	O
we	O
like	O
for	O
g	O
,	O
so	O
long	O
as	O
it	O
is	O
invertible	O
,	O
and	O
so	O
long	O
as	O
g−1	O
has	O
the	O
appropriate	O
range	O
.	O
for	O
example	O
,	O
in	O
logistic	B
regression	I
,	O
we	O
set	O
μi	O
=	O
g−1	O
(	O
ηi	O
)	O
=	O
sigm	O
(	O
ηi	O
)	O
.	O
one	O
particularly	O
simple	O
form	O
of	O
link	B
function	I
is	O
to	O
use	O
g	O
=	O
ψ	O
;	O
this	O
is	O
called	O
the	O
canonical	B
link	I
function	I
.	O
in	O
this	O
case	O
,	O
θi	O
=	O
ηi	O
=	O
wt	O
xi	O
,	O
so	O
the	O
model	O
becomes	O
p	O
(	O
yi|xi	O
,	O
w	O
,	O
σ2	O
)	O
=	O
exp	O
+	O
c	O
(	O
yi	O
,	O
σ2	O
)	O
(	O
9.80	O
)	O
(	O
cid:29	O
)	O
yiwt	O
xi	O
−	O
a	O
(	O
wt	O
xi	O
)	O
σ2	O
(	O
cid:30	O
)	O
in	O
table	O
9.1	O
,	O
we	O
list	O
some	O
distributions	O
and	O
their	O
canonical	O
link	O
functions	O
.	O
we	O
see	O
that	O
for	O
the	O
bernoulli/	O
binomial	B
distribution	I
,	O
the	O
canonical	O
link	O
is	O
the	O
logit	B
function	O
,	O
g	O
(	O
μ	O
)	O
=	O
log	O
(	O
η/	O
(	O
1	O
−	O
η	O
)	O
)	O
,	O
whose	O
inverse	O
is	O
the	O
logistic	B
function	O
,	O
μ	O
=	O
sigm	O
(	O
η	O
)	O
.	O
based	O
on	O
the	O
results	O
in	O
section	O
9.2.3	O
,	O
we	O
can	O
show	O
that	O
the	O
mean	B
and	O
variance	B
of	O
the	O
response	B
variable	I
are	O
as	O
follows	O
:	O
(	O
cid:31	O
)	O
y|xi	O
,	O
w	O
,	O
σ2	O
(	O
cid:31	O
)	O
y|xi	O
,	O
w	O
,	O
σ2	O
e	O
var	O
=	O
μi	O
=	O
a	O
(	O
cid:4	O
)	O
i	O
=	O
a	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
=	O
σ2	O
(	O
θi	O
)	O
(	O
θi	O
)	O
σ2	O
to	O
make	O
the	O
notation	O
clearer	O
,	O
let	O
us	O
consider	O
some	O
simple	O
examples	O
.	O
•	O
for	O
linear	B
regression	I
,	O
we	O
have	O
log	O
p	O
(	O
yi|xi	O
,	O
w	O
,	O
σ2	O
)	O
=	O
yiμi	O
−	O
μ2	O
i	O
2	O
σ2	O
−	O
1	O
2	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
y2	O
i	O
σ2	O
+	O
log	O
(	O
2πσ2	O
)	O
(	O
9.81	O
)	O
(	O
9.82	O
)	O
(	O
9.83	O
)	O
where	O
yi	O
∈	O
r	O
,	O
and	O
θi	O
=	O
μi	O
=	O
wt	O
xi	O
here	O
a	O
(	O
θ	O
)	O
=	O
θ2/2	O
,	O
soe	O
[	O
yi	O
]	O
=	O
μi	O
and	O
var	O
[	O
yi	O
]	O
=	O
σ2	O
.	O
292	O
chapter	O
9.	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	I
•	O
for	O
binomial	B
regression	I
,	O
we	O
have	O
log	O
p	O
(	O
yi|xi	O
,	O
w	O
)	O
=y	O
i	O
log	O
(	O
πi	O
1	O
−	O
πi	O
)	O
+n	O
i	O
log	O
(	O
1	O
−	O
πi	O
)	O
+	O
log	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
ni	O
yi	O
(	O
9.84	O
)	O
where	O
yi	O
∈	O
{	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
ni	O
}	O
,	O
πi	O
=	O
sigm	O
(	O
wt	O
xi	O
)	O
,	O
θi	O
=	O
log	O
(	O
πi/	O
(	O
1	O
−	O
πi	O
)	O
)	O
=	O
wt	O
xi	O
,	O
and	O
σ2	O
=	O
1.	O
here	O
a	O
(	O
θ	O
)	O
=	O
ni	O
log	O
(	O
1	O
+	O
eθ	O
)	O
,	O
so	O
e	O
[	O
yi	O
]	O
=	O
niπi	O
=	O
μi	O
,	O
var	O
[	O
yi	O
]	O
=	O
niπi	O
(	O
1	O
−	O
πi	O
)	O
.	O
•	O
for	O
poisson	B
regression	I
,	O
we	O
have	O
log	O
p	O
(	O
yi|xi	O
,	O
w	O
)	O
=y	O
i	O
log	O
μi	O
−	O
μi	O
−	O
log	O
(	O
yi	O
!	O
)	O
(	O
9.85	O
)	O
where	O
yi	O
∈	O
{	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
}	O
,	O
μi	O
=	O
exp	O
(	O
wt	O
xi	O
)	O
,	O
θi	O
=	O
log	O
(	O
μi	O
)	O
=	O
wt	O
xi	O
,	O
and	O
σ2	O
=	O
1.	O
here	O
a	O
(	O
θ	O
)	O
=e	O
θ	O
,	O
so	O
e	O
[	O
yi	O
]	O
=	O
var	O
[	O
yi	O
]	O
=μ	O
i.	O
poisson	B
regression	I
is	O
widely	O
used	O
in	O
bio-statistical	O
applications	O
,	O
where	O
yi	O
might	O
represent	O
the	O
number	O
of	O
diseases	O
of	O
a	O
given	O
person	O
or	O
place	O
,	O
or	O
the	O
number	O
of	O
reads	O
at	O
a	O
genomic	O
location	O
in	O
a	O
high-throughput	O
sequencing	O
context	O
(	O
see	O
e.g.	O
,	O
(	O
kuan	O
et	O
al	O
.	O
2009	O
)	O
)	O
.	O
9.3.2	O
ml	O
and	O
map	O
estimation	O
one	O
of	O
the	O
appealing	O
properties	O
of	O
glms	O
is	O
that	O
they	O
can	O
be	O
ﬁt	O
using	O
exactly	O
the	O
same	O
methods	O
that	O
we	O
used	O
to	O
ﬁt	O
logistic	B
regression	I
.	O
in	O
particular	O
,	O
the	O
log-likelihood	O
has	O
the	O
following	O
form	O
:	O
(	O
cid:6	O
)	O
(	O
w	O
)	O
=	O
log	O
p	O
(	O
d|w	O
)	O
=	O
1	O
σ2	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
cid:6	O
)	O
i	O
(	O
cid:6	O
)	O
i	O
(	O
cid:2	O
)	O
θiyi	O
−	O
a	O
(	O
θi	O
)	O
we	O
can	O
compute	O
the	O
gradient	O
vector	O
using	O
the	O
chain	B
rule	I
as	O
follows	O
:	O
d	O
(	O
cid:6	O
)	O
i	O
dwj	O
d	O
(	O
cid:6	O
)	O
i	O
dθi	O
dθi	O
=	O
dμi	O
=	O
(	O
yi	O
−	O
a	O
(	O
cid:4	O
)	O
=	O
(	O
yi	O
−	O
μi	O
)	O
dμi	O
dηi	O
(	O
θi	O
)	O
)	O
dθi	O
dμi	O
dηi	O
dwj	O
dθi	O
dμi	O
dμi	O
dηi	O
xij	O
dμi	O
dηi	O
xij	O
if	O
we	O
use	O
a	O
canonical	O
link	O
,	O
θi	O
=	O
ηi	O
,	O
this	O
simpliﬁes	O
to	O
(	O
cid:19	O
)	O
∇w	O
(	O
cid:6	O
)	O
(	O
w	O
)	O
=	O
1	O
σ2	O
(	O
yi	O
−	O
μi	O
)	O
xi	O
(	O
cid:18	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
9.86	O
)	O
(	O
9.87	O
)	O
(	O
9.88	O
)	O
(	O
9.89	O
)	O
(	O
9.90	O
)	O
(	O
9.91	O
)	O
which	O
is	O
a	O
sum	O
of	O
the	O
input	O
vectors	O
,	O
weighted	O
by	O
the	O
errors	O
.	O
this	O
can	O
be	O
used	O
inside	O
a	O
(	O
stochastic	O
)	O
gradient	B
descent	I
procedure	O
,	O
discussed	O
in	O
section	O
8.5.2.	O
however	O
,	O
for	O
improved	O
efficiency	O
,	O
we	O
should	O
use	O
a	O
second-order	O
method	O
.	O
if	O
we	O
use	O
a	O
canonical	O
link	O
,	O
the	O
hessian	O
is	O
given	O
by	O
h	O
=	O
−	O
1	O
σ2	O
dμi	O
dθi	O
xixt	O
i	O
=	O
−	O
1	O
σ2	O
xt	O
sx	O
(	O
9.92	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
9.4.	O
probit	B
regression	I
293	O
name	O
logistic	B
probit	O
log-log	O
complementary	O
log-log	O
formula	O
g−1	O
(	O
η	O
)	O
=	O
sigm	O
(	O
η	O
)	O
=	O
eη	O
g−1	O
(	O
η	O
)	O
=	O
φ	O
(	O
η	O
)	O
g−1	O
(	O
η	O
)	O
=	O
exp	O
(	O
−	O
exp	O
(	O
−η	O
)	O
)	O
g−1	O
(	O
η	O
)	O
=	O
1	O
−	O
exp	O
(	O
−	O
exp	O
(	O
η	O
)	O
)	O
1+eη	O
table	O
9.2	O
summary	O
of	O
some	O
possible	O
mean	B
functions	O
for	O
binary	O
regression	O
.	O
where	O
s	O
=	O
diag	O
(	O
dμ1	O
dθ1	O
irls	O
algorithm	O
(	O
section	O
8.3.4	O
)	O
.	O
speciﬁcally	O
,	O
we	O
have	O
the	O
following	O
newton	O
update	O
:	O
)	O
is	O
a	O
diagonal	B
weighting	O
matrix	O
.	O
this	O
can	O
be	O
used	O
inside	O
the	O
,	O
.	O
.	O
.	O
,	O
dμn	O
dθn	O
wt+1	O
=	O
(	O
xt	O
stx	O
)	O
zt	O
=	O
θt	O
+	O
s−1	O
−1xt	O
stzt	O
t	O
(	O
y	O
−	O
μt	O
)	O
where	O
θt	O
=	O
xwt	O
and	O
μt	O
=	O
g−1	O
(	O
ηt	O
)	O
.	O
(	O
9.93	O
)	O
(	O
9.94	O
)	O
if	O
we	O
extend	O
the	O
derivation	O
to	O
handle	O
non-canonical	O
links	O
,	O
we	O
ﬁnd	O
that	O
the	O
hessian	O
has	O
another	O
term	O
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
the	O
expected	O
hessian	O
is	O
the	O
same	O
as	O
in	O
equation	O
9.92	O
;	O
using	O
the	O
expected	O
hessian	O
(	O
known	O
as	O
the	O
fisher	O
information	B
matrix	O
)	O
instead	O
of	O
the	O
actual	O
hessian	O
is	O
known	O
as	O
the	O
fisher	O
scoring	O
method	O
.	O
it	O
is	O
straightforward	O
to	O
modify	O
the	O
above	O
procedure	O
to	O
perform	O
map	O
estimation	O
with	O
a	O
gaus-	O
sian	O
prior	O
:	O
we	O
just	O
modify	O
the	O
objective	O
,	O
gradient	O
and	O
hessian	O
,	O
just	O
as	O
we	O
added	O
(	O
cid:6	O
)	O
2	O
regularization	B
to	O
logistic	B
regression	I
in	O
section	O
8.3.6	O
.	O
9.3.3	O
bayesian	O
inference	B
bayesian	O
inference	B
for	O
glms	O
is	O
usually	O
conducted	O
using	O
mcmc	O
(	O
chapter	O
24	O
)	O
.	O
possible	O
methods	O
include	O
metropolis	O
hastings	O
with	O
an	O
irls-based	O
proposal	O
(	O
gamerman	O
1997	O
)	O
,	O
gibbs	O
sampling	O
using	O
adaptive	B
rejection	I
sampling	I
(	O
ars	O
)	O
for	O
each	O
full-conditional	O
(	O
dellaportas	O
and	O
smith	O
1993	O
)	O
,	O
etc	O
.	O
see	O
e.g.	O
,	O
(	O
dey	O
et	O
al	O
.	O
2000	O
)	O
for	O
futher	O
information	B
.	O
it	O
is	O
also	O
possible	O
to	O
use	O
the	O
gaussian	O
approximation	O
(	O
section	O
8.4.1	O
)	O
or	O
variational	B
inference	I
(	O
section	O
21.8.1.1	O
)	O
.	O
9.4	O
probit	B
regression	I
in	O
(	O
binary	O
)	O
logistic	B
regression	I
,	O
we	O
use	O
a	O
model	O
of	O
the	O
form	O
p	O
(	O
y	O
=	O
1|xi	O
,	O
w	O
)	O
=	O
sigm	O
(	O
wt	O
xi	O
)	O
.	O
in	O
general	O
,	O
we	O
can	O
write	O
p	O
(	O
y	O
=	O
1|xi	O
,	O
w	O
)	O
=	O
g−1	O
(	O
wt	O
xi	O
)	O
,	O
for	O
any	O
function	O
g−1	O
that	O
maps	O
[	O
−∞	O
,	O
∞	O
]	O
to	O
[	O
0	O
,	O
1	O
]	O
.	O
several	O
possible	O
mean	B
functions	O
are	O
listed	O
in	O
table	O
9.2.	O
in	O
this	O
section	O
,	O
we	O
focus	O
on	O
the	O
case	O
where	O
g−1	O
(	O
η	O
)	O
=	O
φ	O
(	O
η	O
)	O
,	O
where	O
φ	O
(	O
η	O
)	O
is	O
the	O
cdf	B
of	O
the	O
standard	B
normal	I
.	O
this	O
is	O
known	O
as	O
probit	B
regression	I
.	O
the	O
probit	B
function	O
is	O
very	O
similar	B
to	O
the	O
logistic	B
function	O
,	O
as	O
shown	O
in	O
figure	O
8.7	O
(	O
b	O
)	O
.	O
however	O
,	O
this	O
model	O
has	O
some	O
advantages	O
over	O
logistic	B
regression	I
,	O
as	O
we	O
will	O
see	O
.	O
294	O
chapter	O
9.	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	I
9.4.1	O
ml/map	O
estimation	O
using	O
gradient-based	O
optimization	B
we	O
can	O
ﬁnd	O
the	O
mle	O
for	O
probit	B
regression	I
using	O
standard	O
gradient	O
methods	O
.	O
let	O
μi	O
=	O
wt	O
xi	O
,	O
and	O
let	O
˜yi	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
.	O
then	O
the	O
gradient	O
of	O
the	O
log-likelihod	O
for	O
a	O
speciﬁc	O
case	O
is	O
given	O
by	O
gi	O
(	O
cid:2	O
)	O
d	O
dw	O
log	O
p	O
(	O
˜yi|wt	O
xi	O
)	O
=	O
dμi	O
dw	O
d	O
dμi	O
log	O
p	O
(	O
˜yi|wt	O
xi	O
)	O
=	O
xi	O
˜yiφ	O
(	O
μi	O
)	O
φ	O
(	O
˜yiμi	O
)	O
(	O
9.95	O
)	O
where	O
φ	O
is	O
the	O
standard	B
normal	I
pdf	O
,	O
and	O
φ	O
is	O
its	O
cdf	B
.	O
similarly	O
,	O
the	O
hessian	O
for	O
a	O
single	O
case	O
is	O
given	O
by	O
hi	O
=	O
d	O
dw2	O
log	O
p	O
(	O
˜yi|wt	O
xi	O
)	O
=	O
−xi	O
xt	O
i	O
(	O
9.96	O
)	O
(	O
cid:9	O
)	O
˜yiμiφ	O
(	O
μi	O
)	O
φ	O
(	O
˜yiμi	O
)	O
(	O
cid:8	O
)	O
φ	O
(	O
μi	O
)	O
2	O
φ	O
(	O
˜yiμi	O
)	O
2	O
+	O
(	O
cid:10	O
)	O
we	O
can	O
modify	O
these	O
expressions	O
to	O
compute	O
the	O
map	O
estimate	O
in	O
a	O
straightforward	O
manner	O
.	O
in	O
particular	O
,	O
if	O
we	O
use	O
the	O
prior	O
p	O
(	O
w	O
)	O
=n	O
(	O
0	O
,	O
v0	O
)	O
,	O
the	O
gradient	O
and	O
hessian	O
of	O
the	O
penalized	O
0	O
.	O
these	O
expressions	O
can	O
be	O
log	O
likelihood	O
have	O
the	O
form	O
passed	O
to	O
any	O
gradient-based	O
optimizer	O
.	O
see	O
probitregdemo	O
for	O
a	O
demo	O
.	O
i	O
hi	O
+	O
2v−1	O
(	O
cid:10	O
)	O
i	O
gi	O
+	O
2v−1	O
0	O
w	O
and	O
9.4.2	O
latent	O
variable	O
interpretation	O
we	O
can	O
interpret	O
the	O
probit	B
(	O
and	O
logistic	B
)	O
model	O
as	O
follows	O
.	O
first	O
,	O
let	O
us	O
associate	O
each	O
item	O
xi	O
with	O
two	O
latent	B
utilities	O
,	O
u0i	O
and	O
u1i	O
,	O
corresponding	O
to	O
the	O
possible	O
choices	O
of	O
yi	O
=	O
0	O
and	O
yi	O
=	O
1.	O
we	O
then	O
assume	O
that	O
the	O
observed	O
choice	O
is	O
whichever	O
action	B
has	O
larger	O
utility	O
.	O
more	O
precisely	O
,	O
the	O
model	O
is	O
as	O
follows	O
:	O
u0i	O
(	O
cid:2	O
)	O
wt	O
0	O
xi	O
+	O
δ0i	O
u1i	O
(	O
cid:2	O
)	O
wt	O
1	O
xi	O
+	O
δ1i	O
yi	O
=	O
i	O
(	O
u1i	O
>	O
u10	O
)	O
(	O
9.97	O
)	O
(	O
9.98	O
)	O
(	O
9.99	O
)	O
where	O
δ	O
’	O
s	O
are	O
error	O
terms	O
,	O
representing	O
all	O
the	O
other	O
factors	B
that	O
might	O
be	O
relevant	O
in	O
decision	B
making	O
that	O
we	O
have	O
chosen	O
not	O
to	O
(	O
or	O
are	O
unable	O
to	O
)	O
model	O
.	O
this	O
is	O
called	O
a	O
random	B
utility	I
model	I
or	O
rum	O
(	O
mcfadden	O
1974	O
;	O
train	O
2009	O
)	O
.	O
since	O
it	O
is	O
only	O
the	O
difference	O
in	O
utilities	B
that	O
matters	O
,	O
let	O
us	O
deﬁne	O
zi	O
=	O
u1i	O
−	O
u0i	O
+	O
i	O
,	O
where	O
i	O
=	O
δ1i	O
−	O
δ0i	O
.	O
if	O
the	O
δ	O
’	O
s	O
have	O
a	O
gaussian	O
distribution	O
,	O
then	O
so	O
does	O
i	O
.	O
thus	O
we	O
can	O
write	O
zi	O
(	O
cid:2	O
)	O
wt	O
xi	O
+	O
i	O
i	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
yi	O
=	O
1	O
=	O
i	O
(	O
zi	O
≥	O
0	O
)	O
(	O
9.100	O
)	O
(	O
9.101	O
)	O
(	O
9.102	O
)	O
following	O
(	O
fruhwirth-schnatter	O
and	O
fruhwirth	O
2010	O
)	O
,	O
we	O
call	O
this	O
the	O
difference	O
rum	O
or	O
drum	O
model	O
.	O
when	O
we	O
marginalize	O
out	O
zi	O
,	O
we	O
recover	O
the	O
probit	B
model	O
:	O
p	O
(	O
yi	O
=	O
1|xi	O
,	O
w	O
)	O
=	O
i	O
(	O
zi	O
≥	O
0	O
)	O
n	O
(	O
zi|wt	O
xi	O
,	O
1	O
)	O
dzi	O
(	O
cid:28	O
)	O
=	O
p	O
(	O
wt	O
xi	O
+	O
	O
≥	O
0	O
)	O
=	O
p	O
(	O
	O
≥	O
−wt	O
xi	O
)	O
=	O
1	O
−	O
φ	O
(	O
−wt	O
xi	O
)	O
=	O
φ	O
(	O
wt	O
xi	O
)	O
(	O
9.103	O
)	O
(	O
9.104	O
)	O
(	O
9.105	O
)	O
9.4.	O
probit	B
regression	I
295	O
where	O
we	O
used	O
the	O
symmetry	O
of	O
the	O
gaussian.3	O
this	O
latent	O
variable	O
interpretation	O
provides	O
an	O
alternative	O
way	O
to	O
ﬁt	O
the	O
model	O
,	O
as	O
discussed	O
in	O
section	O
11.4.6.	O
interestingly	O
,	O
if	O
we	O
use	O
a	O
gumbel	O
distribution	O
for	O
the	O
δ	O
’	O
s	O
,	O
we	O
induce	O
a	O
logistic	B
distibution	O
for	O
i	O
,	O
and	O
the	O
model	O
reduces	O
to	O
logistic	B
regression	I
.	O
see	O
section	O
24.5.1	O
for	O
further	O
details	O
.	O
9.4.3	O
ordinal	B
probit	O
regression	B
*	O
one	O
advantage	O
of	O
the	O
latent	O
variable	O
interpretation	O
of	O
probit	B
regression	I
is	O
that	O
it	O
is	O
easy	O
to	O
extend	O
to	O
the	O
case	O
where	O
the	O
response	B
variable	I
is	O
ordinal	B
,	O
that	O
is	O
,	O
it	O
can	O
take	O
on	O
c	O
discrete	B
values	O
which	O
can	O
be	O
ordered	O
in	O
some	O
way	O
,	O
such	O
as	O
low	O
,	O
medium	O
and	O
high	O
.	O
this	O
is	O
called	O
ordinal	B
regression	I
.	O
the	O
basic	O
idea	O
is	O
as	O
follows	O
.	O
we	O
introduce	O
c	O
+	O
1	O
thresholds	O
γj	O
and	O
set	O
if	O
γj−1	O
<	O
zi	O
≤	O
γj	O
yi	O
=	O
j	O
(	O
9.106	O
)	O
where	O
γ0	O
≤	O
···	O
≤	O
γc	O
.	O
for	O
identiﬁability	O
reasons	O
,	O
we	O
set	O
γ0	O
=	O
−∞	O
,	O
γ1	O
=	O
0	O
and	O
γc	O
=	O
∞	O
.	O
for	O
example	O
,	O
if	O
c	O
=	O
2	O
,	O
this	O
reduces	O
to	O
the	O
standard	O
binary	O
probit	B
model	O
,	O
whereby	O
zi	O
<	O
0	O
produces	O
yi	O
=	O
0	O
and	O
zi	O
≥	O
0	O
produces	O
yi	O
=	O
1.	O
if	O
c	O
=	O
3	O
,	O
we	O
partition	O
the	O
real	O
line	O
into	O
3	O
intervals	O
:	O
(	O
−∞	O
,	O
0	O
]	O
,	O
(	O
0	O
,	O
γ2	O
]	O
,	O
(	O
γ2	O
,	O
∞	O
)	O
.	O
we	O
can	O
vary	O
the	O
parameter	B
γ2	O
to	O
ensure	O
the	O
right	O
relative	O
amount	O
of	O
probability	O
mass	O
falls	O
in	O
each	O
interval	O
,	O
so	O
as	O
to	O
match	O
the	O
empirical	O
frequencies	O
of	O
each	O
class	O
label	O
.	O
finding	O
the	O
mles	O
for	O
this	O
model	O
is	O
a	O
bit	O
trickier	O
than	O
for	O
binary	O
probit	O
regression	B
,	O
since	O
we	O
need	O
to	O
optimize	O
for	O
w	O
and	O
γ	O
,	O
and	O
the	O
latter	O
must	O
obey	O
an	O
ordering	O
constraint	O
.	O
see	O
e.g.	O
,	O
(	O
kawakatsu	O
and	O
largey	O
2009	O
)	O
for	O
an	O
approach	O
based	O
on	O
em	O
.	O
it	O
is	O
also	O
possible	O
to	O
derive	O
a	O
simple	O
gibbs	O
sampling	O
algorithm	O
for	O
this	O
model	O
(	O
see	O
e.g.	O
,	O
(	O
hoff	O
2009	O
,	O
p216	O
)	O
)	O
.	O
9.4.4	O
multinomial	B
probit	I
models	O
*	O
now	O
consider	O
the	O
case	O
where	O
the	O
response	B
variable	I
can	O
take	O
on	O
c	O
unordered	O
categorical	B
values	O
,	O
yi	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
}	O
.	O
the	O
multinomial	B
probit	I
model	O
is	O
deﬁned	O
as	O
follows	O
:	O
zic	O
=	O
wt	O
xic	O
+	O
ic	O
	O
∼	O
n	O
(	O
0	O
,	O
r	O
)	O
yi	O
=	O
arg	O
max	O
zic	O
c	O
(	O
9.107	O
)	O
(	O
9.108	O
)	O
(	O
9.109	O
)	O
(	O
dow	O
and	O
endersby	O
2004	O
;	O
scott	O
2009	O
;	O
fruhwirth-schnatter	O
and	O
fruhwirth	O
2010	O
)	O
for	O
see	O
e.g.	O
,	O
more	O
details	O
on	O
the	O
model	O
and	O
its	O
connection	O
to	O
multinomial	B
logistic	I
regression	I
.	O
(	O
by	O
deﬁning	O
w	O
=	O
[	O
w1	O
,	O
.	O
.	O
.	O
,	O
wc	O
]	O
,	O
and	O
xic	O
=	O
[	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
,	O
xi	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
]	O
,	O
we	O
can	O
recover	O
the	O
more	O
familiar	O
formulation	O
zic	O
=	O
xt	O
i	O
wc	O
.	O
)	O
since	O
only	O
relative	O
utilities	O
matter	O
,	O
we	O
constrain	O
r	O
to	O
be	O
a	O
correlation	B
matrix	I
.	O
if	O
instead	O
of	O
setting	O
yi	O
=	O
argmaxc	O
zic	O
we	O
use	O
yic	O
=	O
i	O
(	O
zic	O
>	O
0	O
)	O
,	O
we	O
get	O
a	O
model	O
known	O
as	O
multivariate	B
probit	I
,	O
which	O
is	O
one	O
way	O
to	O
model	O
c	O
correlated	O
binary	O
outcomes	O
(	O
see	O
e.g.	O
,	O
(	O
talhouk	O
et	O
al	O
.	O
2011	O
)	O
)	O
.	O
3.	O
note	O
that	O
the	O
assumption	O
that	O
the	O
gaussian	O
noise	O
term	O
is	O
zero	O
mean	O
and	O
unit	O
variance	O
is	O
made	O
without	O
loss	B
of	O
generality	O
.	O
to	O
see	O
why	O
,	O
suppose	O
we	O
used	O
some	O
other	O
mean	B
μ	O
and	O
variance	B
σ2	O
.	O
then	O
we	O
could	O
easily	O
rescale	O
w	O
and	O
add	O
an	O
offset	O
term	O
without	O
changing	O
the	O
likelihood	B
.	O
since	O
p	O
(	O
n	O
(	O
0	O
,	O
1	O
)	O
≥	O
−wt	O
x	O
)	O
=	O
p	O
(	O
n	O
(	O
μ	O
,	O
σ2	O
)	O
≥	O
−	O
(	O
wt	O
x	O
+	O
μ	O
)	O
/σ	O
)	O
.	O
296	O
chapter	O
9.	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	I
9.5	O
multi-task	B
learning	I
sometimes	O
we	O
want	O
to	O
ﬁt	O
many	O
related	O
classiﬁcation	B
or	O
regression	B
models	O
.	O
it	O
is	O
often	O
reasonable	O
to	O
assume	O
the	O
input-output	O
mapping	O
is	O
similar	B
across	O
these	O
different	O
models	O
,	O
so	O
we	O
can	O
get	O
better	O
performance	O
by	O
ﬁtting	O
all	O
the	O
parameters	O
at	O
the	O
same	O
time	O
.	O
in	O
machine	B
learning	I
,	O
this	O
setup	O
is	O
often	O
called	O
multi-task	B
learning	I
(	O
caruana	O
1998	O
)	O
,	O
transfer	B
learning	I
(	O
e.g.	O
,	O
(	O
raina	O
et	O
al	O
.	O
2005	O
)	O
)	O
,	O
or	O
learning	B
to	I
learn	I
(	O
thrun	O
and	O
pratt	O
1997	O
)	O
.	O
in	O
statistics	O
,	O
this	O
is	O
usually	O
tackled	O
using	O
hierarchical	O
bayesian	O
models	O
(	O
bakker	O
and	O
heskes	O
2003	O
)	O
,	O
as	O
we	O
discuss	O
below	O
,	O
although	O
there	O
are	O
other	O
possible	O
methods	O
(	O
see	O
e.g.	O
,	O
(	O
chai	O
2010	O
)	O
)	O
.	O
9.5.1	O
hierarchical	O
bayes	O
for	O
multi-task	B
learning	I
let	O
yij	O
be	O
the	O
response	O
of	O
the	O
i	O
’	O
th	O
item	O
in	O
groupj	O
,	O
for	O
i	O
=	O
1	O
:	O
n	O
j	O
and	O
j	O
=	O
1	O
:	O
j	O
.	O
for	O
example	O
,	O
j	O
might	O
index	O
schools	O
,	O
i	O
might	O
index	O
students	O
within	O
a	O
school	O
,	O
and	O
yij	O
might	O
be	O
the	O
test	O
score	O
,	O
as	O
in	O
section	O
5.6.2.	O
or	O
j	O
might	O
index	O
people	O
,	O
and	O
i	O
might	O
index	O
purchaes	O
,	O
and	O
yij	O
might	O
be	O
the	O
identity	O
of	O
the	O
item	O
that	O
was	O
purchased	O
(	O
this	O
is	O
known	O
as	O
discrete	B
choice	I
modeling	I
(	O
train	O
2009	O
)	O
)	O
.	O
let	O
xij	O
be	O
a	O
feature	O
vector	O
associated	O
with	O
yij	O
.	O
the	O
goal	O
is	O
to	O
ﬁt	O
the	O
models	O
p	O
(	O
yj|xj	O
)	O
for	O
all	O
j.	O
although	O
some	O
groups	O
may	O
have	O
lots	O
of	O
data	O
,	O
there	O
is	O
often	O
a	O
long	B
tail	I
,	O
where	O
the	O
majority	O
of	O
groups	O
have	O
little	O
data	O
.	O
thus	O
we	O
can	O
’	O
t	O
reliably	O
ﬁt	O
each	O
model	O
separately	O
,	O
but	O
we	O
don	O
’	O
t	O
want	O
to	O
use	O
the	O
same	O
model	O
for	O
all	O
groups	O
.	O
as	O
a	O
compromise	O
,	O
we	O
can	O
ﬁt	O
a	O
separate	O
model	O
for	O
each	O
group	O
,	O
but	O
encourage	O
the	O
model	O
parameters	O
to	O
be	O
similar	B
across	O
groups	O
.	O
more	O
precisely	O
,	O
suppose	O
e	O
[	O
yij|xij	O
]	O
=	O
g	O
(	O
xt	O
ijβj	O
)	O
,	O
where	O
g	O
is	O
the	O
link	B
function	I
for	O
the	O
glm	O
.	O
furthermore	O
,	O
suppose	O
βj	O
∼	O
n	O
(	O
β∗	O
,	O
σ2	O
in	O
this	O
model	O
,	O
groups	O
with	O
small	O
sample	O
size	O
borrow	O
statistical	O
strength	O
from	O
the	O
groups	O
with	O
larger	O
sample	O
size	O
,	O
because	O
the	O
βj	O
’	O
s	O
are	O
correlated	O
via	O
the	O
latent	B
common	O
parents	B
β∗	O
(	O
see	O
section	O
5.5	O
for	O
further	O
discussion	O
of	O
this	O
point	O
)	O
.	O
the	O
term	O
σ2	O
j	O
controls	O
how	O
much	O
group	O
j	O
depends	O
on	O
the	O
common	O
parents	B
and	O
the	O
σ2∗	O
term	O
controls	O
the	O
strength	O
of	O
the	O
overall	O
prior	O
.	O
j	O
i	O
)	O
,	O
and	O
that	O
β∗	O
∼	O
n	O
(	O
μ	O
,	O
σ2∗i	O
)	O
.	O
suppose	O
,	O
for	O
simplicity	O
,	O
that	O
μ	O
=	O
0	O
,	O
and	O
that	O
σ2	O
j	O
and	O
σ2∗	O
are	O
all	O
known	O
(	O
e.g.	O
,	O
they	O
could	O
be	O
set	O
by	O
cross	B
validation	I
)	O
.	O
the	O
overall	O
log	O
probability	O
has	O
the	O
form	O
(	O
cid:19	O
)	O
log	O
p	O
(	O
d|β	O
)	O
+	O
log	O
p	O
(	O
β	O
)	O
=	O
log	O
p	O
(	O
dj|βj	O
)	O
−	O
||βj	O
−	O
β∗||2	O
2σ2	O
j	O
−	O
||β∗||2	O
2σ2∗	O
(	O
9.110	O
)	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
j	O
we	O
can	O
perform	O
map	O
estimation	O
of	O
β	O
=	O
(	O
β1	O
:	O
j	O
,	O
β∗	O
)	O
using	O
standard	O
gradient	O
methods	O
.	O
alter-	O
natively	O
,	O
we	O
can	O
perform	O
an	O
iterative	O
optimization	O
scheme	O
,	O
alternating	O
between	O
optimizing	O
the	O
βj	O
and	O
the	O
β∗	O
;	O
since	O
the	O
likelihood	B
and	O
prior	O
are	O
convex	B
,	O
this	O
is	O
guaranteed	O
to	O
converge	B
to	O
the	O
global	O
optimum	O
.	O
note	O
that	O
once	O
the	O
models	O
are	O
trained	O
,	O
we	O
can	O
discard	O
β∗	O
,	O
and	O
use	O
each	O
model	O
separately	O
.	O
9.5.2	O
application	O
to	O
personalized	O
email	O
spam	B
ﬁltering	O
an	O
interesting	O
application	O
of	O
multi-task	B
learning	I
is	O
personalized	B
spam	I
ﬁltering	I
.	O
suppose	O
we	O
want	O
to	O
ﬁt	O
one	O
classiﬁer	O
per	O
user	O
,	O
βj	O
.	O
since	O
most	O
users	O
do	O
not	O
label	B
their	O
email	O
as	O
spam	B
or	O
not	O
,	O
it	O
will	O
be	O
hard	O
to	O
estimate	O
these	O
models	O
independently	O
.	O
so	O
we	O
will	O
let	O
the	O
βj	O
have	O
a	O
common	O
prior	O
β∗	O
,	O
representing	O
the	O
parameters	O
of	O
a	O
generic	O
user	O
.	O
9.5.	O
multi-task	B
learning	I
297	O
in	O
this	O
case	O
,	O
we	O
can	O
emulate	O
the	O
behavior	O
of	O
the	O
above	O
model	O
with	O
a	O
simple	O
trick	O
(	O
daume	O
2007b	O
;	O
attenberg	O
et	O
al	O
.	O
2009	O
;	O
weinberger	O
et	O
al	O
.	O
2009	O
)	O
:	O
we	O
make	O
two	O
copies	O
of	O
each	O
feature	O
xi	O
,	O
one	O
concatenated	O
with	O
the	O
user	O
id	O
,	O
and	O
one	O
not	O
.	O
the	O
effect	O
will	O
be	O
to	O
learn	O
a	O
predictor	O
of	O
the	O
form	O
e	O
[	O
yi|xi	O
,	O
u	O
]	O
=	O
(	O
β∗	O
,	O
w1	O
,	O
···	O
,	O
wj	O
)	O
t	O
[	O
xi	O
,	O
i	O
(	O
u	O
=	O
1	O
)	O
xi	O
,	O
···	O
,	O
i	O
(	O
u	O
=	O
j	O
)	O
xi	O
]	O
where	O
u	O
is	O
the	O
user	O
id	O
.	O
in	O
other	O
words	O
,	O
e	O
[	O
yi|xi	O
,	O
u	O
=	O
j	O
]	O
=	O
(	O
βt∗	O
+	O
wj	O
)	O
t	O
xi	O
(	O
9.111	O
)	O
(	O
9.112	O
)	O
thus	O
β∗	O
will	O
be	O
estimated	O
from	O
everyone	O
’	O
s	O
email	O
,	O
whereas	O
wj	O
will	O
just	O
be	O
estimated	O
from	O
user	O
j	O
’	O
s	O
email	O
.	O
to	O
see	O
the	O
correspondence	B
with	O
the	O
above	O
hierarchical	O
bayesian	O
model	O
,	O
deﬁne	O
wj	O
=	O
βj	O
−	O
β∗	O
.	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
then	O
the	O
log	O
probability	O
of	O
the	O
original	O
model	O
can	O
be	O
rewritten	O
as	O
(	O
cid:19	O
)	O
(	O
9.113	O
)	O
log	O
p	O
(	O
dj|β∗	O
+	O
wj	O
)	O
−	O
||wj||2	O
2σ2	O
j	O
−	O
||β∗||2	O
2σ2∗	O
j	O
if	O
we	O
assume	O
σ2	O
j	O
=	O
σ2∗	O
,	O
the	O
effect	O
is	O
the	O
same	O
as	O
using	O
the	O
augmented	O
feature	O
trick	O
,	O
with	O
the	O
same	O
regularizer	O
strength	O
for	O
both	O
wj	O
and	O
β∗	O
.	O
however	O
,	O
one	O
typically	O
gets	O
better	O
performance	O
by	O
not	O
requiring	O
that	O
σ2	O
j	O
be	O
equal	O
to	O
σ2∗	O
(	O
finkel	O
and	O
manning	O
2009	O
)	O
.	O
9.5.3	O
application	O
to	O
domain	B
adaptation	I
domain	O
adaptation	O
is	O
the	O
problem	O
of	O
training	O
a	O
set	O
of	O
classiﬁers	O
on	O
data	O
drawn	O
from	O
different	O
distributions	O
,	O
such	O
as	O
email	O
and	O
newswire	O
text	O
.	O
this	O
problem	O
is	O
obviously	O
a	O
special	O
case	O
of	O
multi-task	B
learning	I
,	O
where	O
the	O
tasks	O
are	O
the	O
same	O
.	O
(	O
finkel	O
and	O
manning	O
2009	O
)	O
used	O
the	O
above	O
hierarchical	O
bayesian	O
model	O
to	O
perform	O
domain	B
adaptation	I
for	O
two	O
nlp	O
tasks	O
,	O
namely	O
named	O
entity	O
recognition	O
and	O
parsing	O
.	O
they	O
report	O
reason-	O
ably	O
large	O
improvements	O
over	O
ﬁtting	O
separate	O
models	O
to	O
each	O
dataset	O
,	O
and	O
small	O
improvements	O
over	O
the	O
approach	O
of	O
pooling	O
all	O
the	O
data	O
and	O
ﬁtting	O
a	O
single	O
model	O
.	O
9.5.4	O
other	O
kinds	O
of	O
prior	O
in	O
multi-task	B
learning	I
,	O
it	O
is	O
common	O
to	O
assume	O
that	O
the	O
prior	O
is	O
gaussian	O
.	O
however	O
,	O
sometimes	O
other	O
priors	O
are	O
more	O
suitable	O
.	O
for	O
example	O
,	O
consider	O
the	O
task	O
of	O
conjoint	B
analysis	I
,	O
which	O
requires	O
ﬁguring	O
out	O
which	O
features	B
of	O
a	O
product	O
customers	O
like	O
best	O
.	O
this	O
can	O
be	O
modelled	O
using	O
the	O
same	O
hierarchical	O
bayesian	O
setup	O
as	O
above	O
,	O
but	O
where	O
we	O
use	O
a	O
sparsity-promoting	B
prior	I
on	O
βj	O
,	O
rather	O
than	O
a	O
gaussian	O
prior	O
.	O
this	O
is	O
called	O
multi-task	B
feature	I
selection	I
.	O
see	O
e.g.	O
,	O
(	O
lenk	O
et	O
al	O
.	O
1996	O
;	O
argyriou	O
et	O
al	O
.	O
2008	O
)	O
for	O
some	O
possible	O
approaches	O
.	O
it	O
is	O
not	O
always	O
reasonable	O
to	O
assume	O
that	O
all	O
tasks	O
are	O
all	O
equally	O
similar	B
.	O
if	O
we	O
pool	O
the	O
parameters	O
across	O
tasks	O
that	O
are	O
qualitatively	O
different	O
,	O
the	O
performance	O
will	O
be	O
worse	O
than	O
not	O
using	O
pooling	O
,	O
because	O
the	O
inductive	B
bias	I
of	O
our	O
prior	O
is	O
wrong	O
.	O
indeed	O
,	O
it	O
has	O
been	O
found	O
experimentally	O
that	O
sometimes	O
multi-task	B
learning	I
does	O
worse	O
than	O
solving	O
each	O
task	O
separately	O
(	O
this	O
is	O
called	O
negative	B
transfer	I
)	O
.	O
298	O
chapter	O
9.	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	I
one	O
way	O
around	O
this	O
problem	O
is	O
to	O
use	O
a	O
more	O
ﬂexible	O
prior	O
,	O
such	O
as	O
a	O
mixture	O
of	O
gaussians	O
.	O
such	O
ﬂexible	O
priors	O
can	O
provide	O
robustness	B
against	O
prior	O
mis-speciﬁcation	O
.	O
see	O
e.g.	O
,	O
(	O
xue	O
et	O
al	O
.	O
2007	O
;	O
jacob	O
et	O
al	O
.	O
2008	O
)	O
for	O
details	O
.	O
one	O
can	O
of	O
course	O
combine	O
mixtures	O
with	O
sparsity-promoting	O
priors	O
(	O
ji	O
et	O
al	O
.	O
2009	O
)	O
.	O
many	O
other	O
variants	O
are	O
possible	O
.	O
9.6	O
generalized	O
linear	O
mixed	O
models	O
*	O
suppose	O
we	O
generalize	B
the	O
multi-task	B
learning	I
scenario	O
to	O
allow	O
the	O
response	O
to	O
include	O
infor-	O
mation	O
at	O
the	O
group	O
level	O
,	O
xj	O
,	O
as	O
well	O
as	O
at	O
the	O
item	O
level	O
,	O
xij	O
.	O
similarly	O
,	O
we	O
can	O
allow	O
the	O
parameters	O
to	O
vary	O
across	O
groups	O
,	O
βj	O
,	O
or	O
to	O
be	O
tied	B
across	O
groups	O
,	O
α.	O
this	O
gives	O
rise	O
to	O
the	O
following	O
model	O
:	O
j	O
+	O
φ3	O
(	O
xij	O
)	O
t	O
α	O
+	O
φ4	O
(	O
xj	O
)	O
t	O
α	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
9.114	O
)	O
e	O
[	O
yij|xij	O
,	O
xj	O
]	O
=	O
g	O
φ1	O
(	O
xij	O
)	O
t	O
βj	O
+	O
φ2	O
(	O
xj	O
)	O
t	O
β	O
(	O
cid:4	O
)	O
(	O
cid:3	O
)	O
where	O
the	O
φk	O
are	O
basis	B
functions	I
.	O
this	O
model	O
can	O
be	O
represented	O
pictorially	O
as	O
shown	O
in	O
(	O
such	O
ﬁgures	O
will	O
be	O
explained	O
in	O
chapter	O
10	O
.	O
)	O
note	O
that	O
the	O
number	O
of	O
βj	O
figure	O
9.2	O
(	O
a	O
)	O
.	O
parameters	O
grows	O
with	O
the	O
number	O
of	O
groups	O
,	O
whereas	O
the	O
size	O
of	O
α	O
is	O
ﬁxed	O
.	O
frequentists	O
call	O
the	O
terms	O
βj	O
random	B
effects	I
,	O
since	O
they	O
vary	O
randomly	O
across	O
groups	O
,	O
but	O
they	O
call	O
α	O
a	O
ﬁxed	B
effect	I
,	O
since	O
it	O
is	O
viewed	O
as	O
a	O
ﬁxed	O
but	O
unknown	B
constant	O
.	O
a	O
model	O
with	O
both	O
ﬁxed	O
and	O
random	B
effects	I
is	O
called	O
a	O
mixed	B
model	I
.	O
if	O
p	O
(	O
y|x	O
)	O
is	O
a	O
glm	O
,	O
the	O
overall	O
model	O
is	O
called	O
a	O
generalized	B
linear	I
mixed	I
effects	I
model	I
or	O
glmm	O
.	O
such	O
models	O
are	O
widely	O
used	O
in	O
statistics	O
.	O
9.6.1	O
example	O
:	O
semi-parametric	O
glmms	O
for	O
medical	O
data	O
consider	O
the	O
following	O
example	O
from	O
(	O
wand	O
2009	O
)	O
.	O
suppose	O
yij	O
is	O
the	O
amount	O
of	O
spinal	O
bone	O
mineral	O
density	O
(	O
sbmd	O
)	O
for	O
person	O
j	O
at	O
measurement	O
i.	O
let	O
xij	O
be	O
the	O
age	O
of	O
person	O
,	O
and	O
let	O
xj	O
be	O
their	O
ethnicity	O
,	O
which	O
can	O
be	O
one	O
of	O
:	O
white	O
,	O
asian	O
,	O
black	O
,	O
or	O
hispanic	O
.	O
the	O
primary	O
goal	O
is	O
to	O
determine	O
if	O
there	O
are	O
signiﬁcant	O
differences	O
in	O
the	O
mean	B
sbmd	O
among	O
the	O
four	O
ethnic	O
groups	O
,	O
after	O
accounting	O
for	O
age	O
.	O
the	O
data	O
is	O
shown	O
in	O
the	O
light	O
gray	O
lines	O
in	O
figure	O
9.2	O
(	O
b	O
)	O
.	O
we	O
see	O
that	O
there	O
is	O
a	O
nonlinear	O
effect	O
of	O
sbmd	O
vs	O
age	O
,	O
so	O
we	O
will	O
use	O
a	O
semi-parametric	B
model	I
which	O
combines	O
linear	B
regression	I
with	O
non-parametric	O
regression	O
(	O
ruppert	O
et	O
al	O
.	O
2003	O
)	O
.	O
we	O
also	O
see	O
that	O
there	O
is	O
variation	O
across	O
individuals	O
within	O
each	O
group	O
,	O
so	O
we	O
will	O
use	O
a	O
mixed	O
effects	O
model	O
.	O
speciﬁcally	O
,	O
we	O
will	O
use	O
φ1	O
(	O
xij	O
)	O
=	O
1	O
to	O
account	O
for	O
the	O
random	O
effect	O
of	O
each	O
person	O
;	O
φ2	O
(	O
xij	O
)	O
=	O
0	O
since	O
no	O
other	O
coefficients	O
are	O
person-speciﬁc	O
;	O
φ3	O
(	O
xij	O
)	O
=	O
[	O
bk	O
(	O
xij	O
)	O
]	O
,	O
where	O
bk	O
is	O
the	O
k	O
’	O
th	O
spline	B
basis	O
functions	O
(	O
see	O
section	O
15.4.6.2	O
)	O
,	O
to	O
account	O
for	O
the	O
nonlinear	O
effect	O
of	O
age	O
;	O
and	O
φ4	O
(	O
xj	O
)	O
=	O
[	O
i	O
(	O
xj	O
=	O
w	O
)	O
,	O
i	O
(	O
xj	O
=	O
a	O
)	O
,	O
i	O
(	O
xj	O
=	O
b	O
)	O
,	O
i	O
(	O
xj	O
=	O
h	O
)	O
]	O
to	O
account	O
for	O
the	O
effect	O
of	O
the	O
different	O
ethnicities	O
.	O
furthermore	O
,	O
we	O
use	O
a	O
linear	O
link	O
function	O
.	O
the	O
overall	O
model	O
is	O
therefore	O
+α	O
(	O
cid:4	O
)	O
e	O
[	O
yij|xij	O
,	O
xj	O
]	O
=β	O
j	O
+	O
αt	O
b	O
(	O
xij	O
)	O
+	O
ij	O
w	O
i	O
(	O
xj	O
=	O
w	O
)	O
+α	O
(	O
cid:4	O
)	O
(	O
9.115	O
)	O
(	O
9.116	O
)	O
where	O
ij	O
∼	O
n	O
(	O
0	O
,	O
σ2	O
y	O
)	O
.	O
α	O
contains	O
the	O
non-parametric	O
part	O
of	O
the	O
model	O
related	O
to	O
age	O
,	O
α	O
(	O
cid:4	O
)	O
contains	O
the	O
parametric	O
part	O
of	O
the	O
model	O
related	O
to	O
ethnicity	O
,	O
and	O
βj	O
is	O
a	O
random	O
offset	O
for	O
person	O
j.	O
we	O
endow	O
all	O
of	O
these	O
regression	B
coefficients	O
with	O
separate	O
gaussian	O
priors	O
.	O
we	O
can	O
then	O
perform	O
posterior	O
inference	O
to	O
compute	O
p	O
(	O
α	O
,	O
α	O
(	O
cid:4	O
)	O
,	O
β	O
,	O
σ2|d	O
)	O
(	O
see	O
section	O
9.6.2	O
for	O
hi	O
(	O
xj	O
=	O
h	O
)	O
ai	O
(	O
xj	O
=	O
a	O
)	O
+α	O
(	O
cid:4	O
)	O
bi	O
(	O
xj	O
=	O
b	O
)	O
+α	O
(	O
cid:4	O
)	O
9.6.	O
generalized	O
linear	O
mixed	O
models	O
*	O
299	O
α	O
μ	O
α	O
σ2	O
α	O
μ	O
β	O
σ2	O
β	O
σ2	O
y	O
β	O
j	O
y	O
ij	O
xij	O
nj	O
j	O
xj	O
(	O
a	O
)	O
hispanic	O
white	O
10	O
15	O
20	O
25	O
1.4	O
1.2	O
1.0	O
0.8	O
0.6	O
)	O
2	O
m	O
c	O
/	O
g	O
(	O
y	O
t	O
i	O
s	O
n	O
e	O
d	O
l	O
i	O
a	O
r	O
e	O
n	O
m	O
e	O
n	O
o	O
b	O
l	O
a	O
n	O
p	O
s	O
i	O
asian	O
black	O
1.4	O
1.2	O
1.0	O
0.8	O
0.6	O
10	O
15	O
20	O
25	O
age	O
in	O
years	O
(	O
b	O
)	O
figure	O
9.2	O
(	O
a	O
)	O
directed	B
graphical	I
model	I
for	O
generalized	B
linear	I
mixed	I
effects	I
model	I
with	O
j	O
groups	O
.	O
(	O
b	O
)	O
spinal	O
bone	O
mineral	O
density	O
vs	O
age	O
for	O
four	O
different	O
ethnic	O
groups	O
.	O
raw	O
data	O
is	O
shown	O
in	O
the	O
light	O
gray	O
lines	O
.	O
fitted	O
model	O
shown	O
in	O
black	O
(	O
solid	O
is	O
the	O
posterior	O
predicted	O
mean	B
,	O
dotted	O
is	O
the	O
posterior	O
predictive	O
variance	O
)	O
.	O
from	O
figure	O
9	O
of	O
(	O
wand	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
matt	O
wand	O
300	O
chapter	O
9.	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	I
computational	O
details	O
)	O
.	O
after	O
ﬁtting	O
the	O
model	O
,	O
we	O
can	O
compute	O
the	O
prediction	O
for	O
each	O
group	O
.	O
see	O
figure	O
9.2	O
(	O
b	O
)	O
for	O
the	O
results	O
.	O
we	O
can	O
also	O
perform	O
signiﬁcance	O
testing	O
,	O
by	O
computing	O
p	O
(	O
αg	O
−	O
αw|d	O
)	O
for	O
each	O
ethnic	O
group	O
g	O
relative	O
to	O
some	O
baseline	O
(	O
say	O
,	O
white	O
)	O
,	O
as	O
we	O
did	O
in	O
section	O
5.2.3	O
.	O
9.6.2	O
computational	O
issues	O
the	O
principle	O
problem	O
with	O
glmms	O
is	O
that	O
they	O
can	O
be	O
difficult	O
to	O
ﬁt	O
,	O
for	O
two	O
reasons	O
.	O
first	O
,	O
p	O
(	O
yij|θ	O
)	O
may	O
not	O
be	O
conjugate	O
to	O
the	O
prior	O
p	O
(	O
θ	O
)	O
where	O
θ	O
=	O
(	O
α	O
,	O
β	O
)	O
.	O
second	O
,	O
there	O
are	O
two	O
levels	O
of	O
unknowns	O
in	O
the	O
model	O
,	O
namely	O
the	O
regression	B
coefficients	O
θ	O
and	O
the	O
means	O
and	O
variances	O
of	O
the	O
priors	O
η	O
=	O
(	O
μ	O
,	O
σ	O
)	O
.	O
one	O
approach	O
is	O
to	O
adopt	O
fully	O
bayesian	O
inference	B
methods	O
,	O
such	O
as	O
variational	O
bayes	O
(	O
hall	O
et	O
al	O
.	O
2011	O
)	O
or	O
mcmc	O
(	O
gelman	O
and	O
hill	O
2007	O
)	O
.	O
we	O
discuss	O
vb	O
in	O
section	O
21.5	O
,	O
and	O
mcmc	O
in	O
section	O
24.1.	O
an	O
alternative	O
approach	O
is	O
to	O
use	O
empirical	O
bayes	O
,	O
which	O
we	O
discuss	O
in	O
general	O
terms	O
in	O
section	O
5.6.	O
in	O
the	O
context	O
of	O
a	O
glmm	O
,	O
we	O
can	O
use	O
the	O
em	O
algorithm	O
(	O
section	O
11.4	O
)	O
,	O
where	O
in	O
the	O
e	O
step	O
we	O
compute	O
p	O
(	O
θ|η	O
,	O
d	O
)	O
,	O
and	O
in	O
the	O
m	O
step	O
we	O
optimize	O
η.	O
if	O
the	O
linear	B
regression	I
setting	O
,	O
the	O
e	O
step	O
can	O
be	O
performed	O
exactly	O
,	O
but	O
in	O
general	O
we	O
need	O
to	O
use	O
approximations	O
.	O
traditional	O
methods	O
use	O
numerical	O
quadrature	O
or	O
monte	O
carlo	O
(	O
see	O
e.g.	O
,	O
(	O
breslow	O
and	O
clayton	O
1993	O
)	O
)	O
.	O
a	O
faster	O
approach	O
is	O
to	O
use	O
variational	O
em	O
;	O
see	O
(	O
braun	O
and	O
mcauliffe	O
2010	O
)	O
for	O
an	O
application	O
of	O
variational	O
em	O
to	O
a	O
multi-level	O
discrete	O
choice	O
modeling	O
problem	O
.	O
in	O
frequentist	B
statistics	I
,	O
there	O
is	O
a	O
popular	O
method	O
for	O
ﬁtting	O
glmms	O
called	O
generalized	B
estimating	I
equations	I
or	O
gee	O
(	O
hardin	O
and	O
hilbe	O
2003	O
)	O
.	O
however	O
,	O
we	O
do	O
not	O
recommend	O
this	O
approach	O
,	O
since	O
it	O
is	O
not	O
as	O
statistically	O
efficient	O
as	O
likelihood-based	O
methods	O
(	O
see	O
section	O
6.4.3	O
)	O
.	O
in	O
addition	O
,	O
it	O
can	O
only	O
provide	O
estimates	O
of	O
the	O
population	O
parameters	O
α	O
,	O
but	O
not	O
the	O
random	B
effects	I
βj	O
,	O
which	O
are	O
sometimes	O
of	O
interest	O
in	O
themselves	O
.	O
9.7	O
learning	B
to	I
rank	I
*	O
in	O
this	O
section	O
,	O
we	O
discuss	O
the	O
learning	B
to	I
rank	I
or	O
letor	O
problem	O
.	O
that	O
is	O
,	O
we	O
want	O
to	O
learn	O
a	O
function	O
that	O
can	O
rank	O
order	O
a	O
set	O
of	O
items	O
(	O
we	O
will	O
be	O
more	O
precise	O
below	O
)	O
.	O
the	O
most	O
common	O
application	O
is	O
to	O
information	B
retrieval	I
.	O
speciﬁcally	O
,	O
suppose	O
we	O
have	O
a	O
query	O
q	O
and	O
a	O
set	O
of	O
documents	O
d1	O
,	O
.	O
.	O
.	O
,	O
dm	O
that	O
might	O
be	O
relevant	O
to	O
q	O
(	O
e.g.	O
,	O
all	O
documents	O
that	O
contain	O
the	O
string	O
q	O
)	O
.	O
we	O
would	O
like	O
to	O
sort	O
these	O
documents	O
in	O
decreasing	O
order	O
of	O
relevance	O
and	O
show	O
the	O
top	O
k	O
to	O
the	O
user	O
.	O
similar	B
problems	O
arise	O
in	O
other	O
areas	O
,	O
such	O
as	O
collaborative	B
ﬁltering	I
.	O
(	O
ranking	B
players	O
in	O
a	O
game	O
or	O
tournament	O
setting	O
is	O
a	O
slightly	O
different	O
kind	O
of	O
problem	O
;	O
see	O
section	O
22.5.5	O
.	O
)	O
below	O
we	O
summarize	O
some	O
methods	O
for	O
solving	O
this	O
problem	O
,	O
following	O
the	O
presentation	O
of	O
(	O
liu	O
2009	O
)	O
.	O
this	O
material	O
is	O
not	O
based	O
on	O
glms	O
,	O
but	O
we	O
include	O
it	O
in	O
this	O
chapter	O
anyway	O
for	O
lack	O
of	O
a	O
better	O
place	O
.	O
a	O
standard	O
way	O
to	O
measure	O
the	O
relevance	O
of	O
a	O
document	O
d	O
to	O
a	O
query	O
q	O
is	O
to	O
use	O
a	O
probabilistic	O
’	O
n	O
language	B
model	I
based	O
on	O
a	O
bag	B
of	I
words	I
model	O
.	O
that	O
is	O
,	O
we	O
deﬁne	O
sim	O
(	O
q	O
,	O
d	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
q|d	O
)	O
=	O
is	O
the	O
i	O
’	O
th	O
word	O
or	O
term	O
,	O
and	O
p	O
(	O
qi|d	O
)	O
is	O
a	O
multinoulli	B
distribution	I
i=1	O
p	O
(	O
qi|d	O
)	O
,	O
where	O
qi	O
estimated	O
from	O
document	O
d.	O
in	O
practice	O
,	O
we	O
need	O
to	O
smooth	O
the	O
estimated	O
distribution	O
,	O
for	O
example	O
by	O
using	O
a	O
dirichlet	O
prior	O
,	O
representing	O
the	O
overall	O
frequency	O
of	O
each	O
word	O
.	O
this	O
can	O
be	O
9.7.	O
learning	B
to	I
rank	I
*	O
estimated	O
from	O
all	O
documents	O
in	O
the	O
system	O
.	O
more	O
precisely	O
,	O
we	O
can	O
use	O
p	O
(	O
t|d	O
)	O
=	O
(	O
1	O
−	O
λ	O
)	O
tf	O
(	O
t	O
,	O
d	O
)	O
len	O
(	O
d	O
)	O
+	O
λp	O
(	O
t|background	O
)	O
301	O
(	O
9.117	O
)	O
where	O
tf	O
(	O
t	O
,	O
d	O
)	O
is	O
the	O
frequency	O
of	O
term	O
t	O
in	O
document	O
d	O
,	O
len	O
(	O
d	O
)	O
is	O
the	O
number	O
of	O
words	O
in	O
d	O
,	O
and	O
0	O
<	O
λ	O
<	O
1	O
is	O
a	O
smoothing	O
parameter	O
(	O
see	O
e.g.	O
,	O
zhai	O
and	O
lafferty	O
(	O
2004	O
)	O
for	O
details	O
)	O
.	O
however	O
,	O
there	O
might	O
be	O
many	O
other	O
signals	O
that	O
we	O
can	O
use	O
to	O
measure	O
relevance	O
.	O
for	O
example	O
,	O
the	O
pagerank	O
of	O
a	O
web	O
document	O
is	O
a	O
measure	O
of	O
its	O
authoritativeness	O
,	O
derived	O
from	O
the	O
web	O
’	O
s	O
link	O
structure	O
(	O
see	O
section	O
17.2.4	O
for	O
details	O
)	O
.	O
we	O
can	O
also	O
compute	O
how	O
often	O
and	O
where	O
the	O
query	O
occurs	O
in	O
the	O
document	O
.	O
below	O
we	O
discuss	O
how	O
to	O
learn	O
how	O
to	O
combine	O
all	O
these	O
signals.4	O
9.7.1	O
the	O
pointwise	B
approach	I
suppose	O
we	O
collect	O
some	O
training	O
data	O
representing	O
the	O
relevance	O
of	O
a	O
set	O
of	O
documents	O
for	O
each	O
query	O
.	O
speciﬁcally	O
,	O
for	O
each	O
query	O
q	O
,	O
suppose	O
that	O
we	O
retrieve	O
m	O
possibly	O
relevant	O
documents	O
dj	O
,	O
for	O
j	O
=	O
1	O
:	O
m.	O
for	O
each	O
query	O
document	O
pair	O
,	O
we	O
deﬁne	O
a	O
feature	O
vector	O
,	O
x	O
(	O
q	O
,	O
d	O
)	O
.	O
for	O
example	O
,	O
this	O
might	O
contain	O
the	O
query-document	O
similarity	O
score	O
and	O
the	O
page	O
rank	O
score	O
of	O
the	O
document	O
.	O
furthermore	O
,	O
suppose	O
we	O
have	O
a	O
set	O
of	O
labels	O
yj	O
representing	O
the	O
degree	B
of	O
relevance	O
of	O
document	O
dj	O
to	O
query	O
q.	O
such	O
labels	O
might	O
be	O
binary	O
(	O
e.g.	O
,	O
relevant	O
or	O
irrelevant	O
)	O
,	O
or	O
they	O
may	O
represent	O
a	O
degree	B
of	O
relevance	O
(	O
e.g.	O
,	O
very	O
relevant	O
,	O
somewhat	O
relevant	O
,	O
irrelevant	O
)	O
.	O
such	O
labels	O
can	O
be	O
obtained	O
from	O
query	B
logs	I
,	O
by	O
thresholding	O
the	O
number	O
of	O
times	O
a	O
document	O
was	O
clicked	O
on	O
for	O
a	O
given	O
query	O
.	O
if	O
we	O
have	O
binary	O
relevance	O
labels	O
,	O
we	O
can	O
solve	O
the	O
problem	O
using	O
a	O
standard	O
binary	O
clas-	O
siﬁcation	O
scheme	O
to	O
estimate	O
,	O
p	O
(	O
y	O
=	O
1|x	O
(	O
q	O
,	O
d	O
)	O
)	O
.	O
if	O
we	O
have	O
ordered	O
relevancy	O
labels	O
,	O
we	O
can	O
use	O
ordinal	B
regression	I
to	O
predict	O
the	O
rating	O
,	O
p	O
(	O
y	O
=	O
r|x	O
(	O
q	O
,	O
d	O
)	O
)	O
.	O
in	O
either	O
case	O
,	O
we	O
can	O
then	O
sort	O
the	O
documents	O
by	O
this	O
scoring	O
metric	B
.	O
this	O
is	O
called	O
the	O
pointwise	B
approach	I
to	O
letor	O
,	O
and	O
is	O
widely	O
used	O
because	O
of	O
its	O
simplicity	O
.	O
however	O
,	O
this	O
method	O
does	O
not	O
take	O
into	O
account	O
the	O
location	O
of	O
each	O
document	O
in	O
the	O
list	O
.	O
thus	O
it	O
penalizes	O
errors	O
at	O
the	O
end	O
of	O
the	O
list	O
just	O
as	O
much	O
as	O
errors	O
at	O
the	O
beginning	O
,	O
which	O
is	O
often	O
not	O
the	O
desired	O
behavior	O
.	O
in	O
addition	O
,	O
each	O
decision	B
about	O
relevance	O
is	O
made	O
very	O
myopically	O
.	O
9.7.2	O
the	O
pairwise	O
approach	O
there	O
is	O
evidence	B
(	O
e.g.	O
,	O
(	O
carterette	O
et	O
al	O
.	O
2008	O
)	O
)	O
that	O
people	O
are	O
better	O
at	O
judging	O
the	O
relative	O
relevance	O
of	O
two	O
items	O
rather	O
than	O
absolute	O
relevance	O
.	O
consequently	O
,	O
the	O
data	O
might	O
tell	O
us	O
that	O
dj	O
is	O
more	O
relevant	O
than	O
dk	O
for	O
a	O
given	O
query	O
,	O
or	O
vice	O
versa	O
.	O
we	O
can	O
model	O
this	O
kind	O
of	O
data	O
using	O
a	O
binary	O
classiﬁer	O
of	O
the	O
form	O
p	O
(	O
yjk|x	O
(	O
q	O
,	O
dj	O
)	O
,	O
x	O
(	O
q	O
,	O
dk	O
)	O
)	O
,	O
where	O
we	O
set	O
yjk	O
=	O
1	O
if	O
rel	O
(	O
dj	O
,	O
q	O
)	O
>	O
rel	O
(	O
dk	O
,	O
q	O
)	O
and	O
yjk	O
=	O
0	O
otherwise	O
.	O
one	O
way	O
to	O
model	O
such	O
a	O
function	O
is	O
as	O
follows	O
:	O
p	O
(	O
yjk	O
=	O
1|xj	O
,	O
xk	O
)	O
=	O
sigm	O
(	O
f	O
(	O
xj	O
)	O
−	O
f	O
(	O
xk	O
)	O
)	O
(	O
9.118	O
)	O
4.	O
rather	O
surprisingly	O
,	O
google	O
does	O
not	O
(	O
or	O
at	O
least	O
,	O
did	O
not	O
as	O
of	O
2008	O
)	O
using	O
such	O
learning	B
methods	O
in	O
its	O
search	O
engine	O
.	O
source	O
:	O
peter	O
norvig	O
,	O
quoted	O
in	O
http	O
:	O
//anand.typepad.com/datawocky/2008/05/are-human-experts-less-p	O
rone-to-catastrophic-errors-than-machine-learned-models.html	O
.	O
302	O
chapter	O
9.	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	I
where	O
f	O
(	O
x	O
)	O
is	O
a	O
scoring	O
function	O
,	O
often	O
taken	O
to	O
be	O
linear	O
,	O
f	O
(	O
x	O
)	O
=w	O
t	O
x.	O
this	O
is	O
a	O
special	O
kind	O
of	O
neural	B
network	I
known	O
as	O
ranknet	O
(	O
burges	O
et	O
al	O
.	O
2005	O
)	O
(	O
see	O
section	O
16.5	O
for	O
a	O
general	O
discussion	O
of	O
neural	B
networks	I
)	O
.	O
we	O
can	O
ﬁnd	O
the	O
mle	O
of	O
w	O
by	O
maximizing	O
the	O
log	O
likelihood	O
,	O
or	O
equivalently	O
,	O
by	O
minimizing	O
the	O
cross	B
entropy	I
loss	O
,	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
mi	O
(	O
cid:2	O
)	O
mi	O
(	O
cid:2	O
)	O
l	O
=	O
lijk	O
(	O
9.119	O
)	O
i=1	O
j=1	O
k=j+1	O
−lijk	O
=	O
i	O
(	O
yijk	O
=	O
1	O
)	O
log	O
p	O
(	O
yijk	O
=	O
1|xij	O
,	O
xik	O
,	O
w	O
)	O
+i	O
(	O
yijk	O
=	O
0	O
)	O
log	O
p	O
(	O
yijk	O
=	O
0|xij	O
,	O
xik	O
,	O
w	O
)	O
(	O
9.120	O
)	O
this	O
can	O
be	O
optimized	O
using	O
gradient	B
descent	I
.	O
a	O
variant	O
of	O
ranknet	O
is	O
used	O
by	O
microsoft	O
’	O
s	O
bing	O
search	O
engine.5	O
9.7.3	O
the	O
listwise	O
approach	O
the	O
pairwise	O
approach	O
suffers	O
from	O
the	O
problem	O
that	O
decisions	O
about	O
relevance	O
are	O
made	O
just	O
based	O
on	O
a	O
pair	O
of	O
items	O
(	O
documents	O
)	O
,	O
rather	O
than	O
considering	O
the	O
full	B
context	O
.	O
we	O
now	O
consider	O
methods	O
that	O
look	O
at	O
the	O
entire	O
list	O
of	O
items	O
at	O
the	O
same	O
time	O
.	O
we	O
can	O
deﬁne	O
a	O
total	O
order	O
on	O
a	O
list	O
by	O
specifying	O
a	O
permutation	O
of	O
its	O
indices	O
,	O
π.	O
to	O
model	O
our	O
uncertainty	B
about	O
π	O
,	O
we	O
can	O
use	O
the	O
plackett-luce	O
distribution	O
,	O
which	O
derives	O
its	O
name	O
from	O
independent	O
work	O
by	O
(	O
plackett	O
1975	O
)	O
and	O
(	O
luce	O
1959	O
)	O
.	O
this	O
has	O
the	O
following	O
form	O
:	O
p	O
(	O
π|s	O
)	O
=	O
m	O
(	O
cid:27	O
)	O
j=1	O
sj	O
(	O
cid:10	O
)	O
m	O
u=j	O
su	O
(	O
9.121	O
)	O
(	O
9.123	O
)	O
(	O
9.124	O
)	O
where	O
sj	O
=	O
s	O
(	O
π−1	O
(	O
j	O
)	O
)	O
is	O
the	O
score	O
of	O
the	O
document	O
ranked	O
at	O
the	O
j	O
’	O
th	O
position	O
.	O
to	O
understand	O
equation	O
9.121	O
,	O
let	O
us	O
consider	O
a	O
simple	O
example	O
.	O
suppose	O
π	O
=	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
.	O
then	O
we	O
have	O
that	O
p	O
(	O
π	O
)	O
is	O
the	O
probability	O
of	O
a	O
being	O
ranked	O
ﬁrst	O
,	O
times	O
the	O
probability	O
of	O
b	O
being	O
ranked	O
second	O
given	O
that	O
a	O
is	O
ranked	O
ﬁrst	O
,	O
times	O
the	O
probabilty	O
of	O
c	O
being	O
ranked	O
third	O
given	O
that	O
a	O
and	O
b	O
are	O
ranked	O
ﬁrst	O
and	O
second	O
.	O
in	O
other	O
words	O
,	O
×	O
sb	O
sa	O
+	O
sb	O
+	O
sc	O
p	O
(	O
π|s	O
)	O
=	O
to	O
incorporate	O
features	B
,	O
we	O
can	O
deﬁne	O
s	O
(	O
d	O
)	O
=f	O
(	O
x	O
(	O
q	O
,	O
d	O
)	O
)	O
,	O
where	O
we	O
often	O
take	O
f	O
to	O
be	O
a	O
linear	O
function	O
,	O
f	O
(	O
x	O
)	O
=w	O
t	O
x.	O
this	O
is	O
known	O
as	O
the	O
listnet	O
model	O
(	O
cao	O
et	O
al	O
.	O
2007	O
)	O
.	O
to	O
train	O
this	O
model	O
,	O
let	O
yi	O
be	O
the	O
relevance	O
scores	O
of	O
the	O
documents	O
for	O
query	O
i.	O
we	O
then	O
minimize	O
the	O
cross	B
entropy	I
term	O
×	O
sc	O
sc	O
sb	O
+	O
sc	O
(	O
9.122	O
)	O
sa	O
of	O
course	O
,	O
as	O
stated	O
,	O
this	O
is	O
intractable	O
,	O
since	O
the	O
i	O
’	O
th	O
term	O
needs	O
to	O
sum	O
over	O
mi	O
!	O
permutations	O
.	O
to	O
make	O
this	O
tractable	O
,	O
we	O
can	O
consider	O
permutations	O
over	O
the	O
top	O
k	O
positions	O
only	O
:	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
−	O
i	O
π	O
p	O
(	O
π|yi	O
)	O
log	O
p	O
(	O
π|si	O
)	O
p	O
(	O
π1	O
:	O
k|s1	O
:	O
m	O
)	O
=	O
k	O
(	O
cid:27	O
)	O
j=1	O
sj	O
(	O
cid:10	O
)	O
m	O
u=1	O
su	O
5.	O
source	O
:	O
eatures-and-the-science-behind-bing.aspx	O
.	O
http	O
:	O
//www.bing.com/community/site_blogs/b/search/archive/2009/06/01/user-needs-f	O
9.7.	O
learning	B
to	I
rank	I
*	O
303	O
there	O
are	O
only	O
m	O
!	O
/	O
(	O
m	O
−	O
k	O
)	O
!	O
such	O
permutations	O
.	O
entropy	B
term	O
(	O
and	O
its	O
derivative	O
)	O
in	O
o	O
(	O
m	O
)	O
time	O
.	O
if	O
we	O
set	O
k	O
=	O
1	O
,	O
we	O
can	O
evaluate	O
each	O
cross	O
in	O
the	O
special	O
case	O
where	O
only	O
one	O
document	O
from	O
the	O
presented	O
list	O
is	O
deemed	O
relevant	O
,	O
say	O
yi	O
=	O
c	O
,	O
we	O
can	O
instead	O
use	O
multinomial	B
logistic	I
regression	I
:	O
p	O
(	O
yi	O
=	O
c|x	O
)	O
=	O
(	O
cid:10	O
)	O
m	O
exp	O
(	O
sc	O
)	O
c	O
(	O
cid:2	O
)	O
=1	O
exp	O
(	O
sc	O
(	O
cid:2	O
)	O
)	O
(	O
9.125	O
)	O
(	O
9.128	O
)	O
this	O
often	O
performs	O
at	O
least	O
as	O
well	O
as	O
ranking	B
methods	O
,	O
at	O
least	O
in	O
the	O
context	O
of	O
collaborative	B
ﬁltering	I
(	O
yang	O
et	O
al	O
.	O
2011	O
)	O
.	O
9.7.4	O
loss	B
functions	O
for	O
ranking	B
there	O
are	O
a	O
variety	O
of	O
ways	O
to	O
measure	O
the	O
performance	O
of	O
a	O
ranking	B
system	O
,	O
which	O
we	O
summa-	O
rize	O
below	O
.	O
•	O
mean	B
reciprocal	I
rank	I
(	O
mrr	O
)	O
.	O
for	O
a	O
query	O
q	O
,	O
let	O
the	O
rank	O
position	O
of	O
its	O
ﬁrst	O
relevant	O
document	O
be	O
denoted	O
by	O
r	O
(	O
q	O
)	O
.	O
then	O
we	O
deﬁne	O
the	O
mean	B
reciprocal	I
rank	I
to	O
be	O
1/r	O
(	O
q	O
)	O
.	O
this	O
is	O
a	O
very	O
simple	O
performance	O
measure	O
.	O
•	O
mean	B
average	I
precision	I
(	O
map	O
)	O
.	O
in	O
the	O
case	O
of	O
binary	O
relevance	O
labels	O
,	O
we	O
can	O
deﬁne	O
the	O
precision	B
at	I
k	I
of	O
some	O
ordering	O
as	O
follows	O
:	O
p	O
@	O
k	O
(	O
π	O
)	O
(	O
cid:2	O
)	O
num	O
.	O
relevant	O
documents	O
in	O
the	O
top	O
k	O
positions	O
of	O
π	O
k	O
we	O
then	O
deﬁne	O
the	O
average	B
precision	I
as	O
follows	O
:	O
(	O
cid:10	O
)	O
k	O
p	O
@	O
k	O
(	O
π	O
)	O
·	O
ik	O
num	O
.	O
relevant	O
documents	O
ap	O
(	O
π	O
)	O
(	O
cid:2	O
)	O
(	O
9.126	O
)	O
(	O
9.127	O
)	O
where	O
ik	O
is	O
1	O
iff	B
document	O
k	O
is	O
relevant	O
.	O
for	O
example	O
,	O
y	O
=	O
(	O
1	O
,	O
0	O
,	O
1	O
,	O
0	O
,	O
1	O
)	O
,	O
then	O
the	O
ap	O
is	O
1	O
3	O
+	O
3	O
precision	B
as	O
the	O
ap	O
averaged	O
over	O
all	O
queries	O
.	O
1	O
+	O
2	O
3	O
(	O
1	O
if	O
we	O
have	O
the	O
relevancy	O
labels	O
5	O
)	O
≈	O
0.76.	O
finally	O
,	O
we	O
deﬁne	O
the	O
mean	O
average	O
•	O
normalized	B
discounted	I
cumulative	I
gain	I
(	O
ndcg	O
)	O
.	O
suppose	O
the	O
relevance	O
labels	O
have	O
multi-	O
ple	O
levels	O
.	O
we	O
can	O
deﬁne	O
the	O
discounted	B
cumulative	I
gain	I
of	O
the	O
ﬁrst	O
k	O
items	O
in	O
an	O
ordering	O
as	O
follows	O
:	O
dcg	O
@	O
k	O
(	O
r	O
)	O
=	O
r1	O
+	O
k	O
(	O
cid:2	O
)	O
ri	O
log2	O
i	O
i=2	O
where	O
ri	O
is	O
the	O
relevance	O
of	O
item	O
i	O
and	O
the	O
log2	O
term	O
is	O
used	O
to	O
discount	O
items	O
later	O
in	O
the	O
list	O
.	O
table	O
9.3	O
gives	O
a	O
simple	O
numerical	O
example	O
.	O
an	O
alternative	O
deﬁnition	O
,	O
that	O
places	O
stronger	O
emphasis	O
on	O
retrieving	O
relevant	O
documents	O
,	O
uses	O
dcg	O
@	O
k	O
(	O
r	O
)	O
=	O
2ri	O
−	O
1	O
log2	O
(	O
1	O
+	O
i	O
)	O
(	O
9.129	O
)	O
k	O
(	O
cid:2	O
)	O
i=1	O
the	O
trouble	O
with	O
dcg	O
is	O
that	O
it	O
varies	O
in	O
magnitude	O
just	O
because	O
the	O
length	O
of	O
a	O
returned	O
list	O
may	O
vary	O
.	O
it	O
is	O
therefore	O
common	O
to	O
normalize	O
this	O
measure	O
by	O
the	O
ideal	O
dcg	O
,	O
which	O
is	O
304	O
chapter	O
9.	O
generalized	B
linear	I
models	I
and	O
the	O
exponential	B
family	I
i	O
ri	O
log2	O
i	O
log2	O
i	O
ri	O
1	O
3	O
0	O
n/a	O
2	O
2	O
1	O
2	O
3	O
3	O
1.59	O
1.887	O
4	O
0	O
2.0	O
0	O
5	O
1	O
2.32	O
0.431	O
6	O
2	O
2.59	O
0.772	O
table	O
9.3	O
illustration	O
of	O
how	O
to	O
compute	O
ndcg	O
,	O
from	O
http	O
:	O
//en.wikipedia.org/wiki/discounted	O
_cumulative_gain	O
.	O
the	O
value	O
ri	O
is	O
the	O
relevance	O
score	O
of	O
the	O
item	O
in	O
position	O
i.	O
from	O
this	O
,	O
we	O
see	O
that	O
dcg	O
@	O
6	O
=	O
3	O
+	O
(	O
2	O
+	O
1.887	O
+	O
0	O
+	O
0.431	O
+	O
0.772	O
)	O
=	O
8.09.	O
the	O
maximum	O
dcg	O
is	O
obtained	O
using	O
the	O
ordering	O
with	O
scores	B
3	O
,	O
3	O
,	O
2	O
,	O
2	O
,	O
1	O
,	O
0.	O
hence	O
the	O
ideal	O
dcg	O
is	O
8.693	O
,	O
and	O
so	O
the	O
normalized	O
dcg	O
is	O
8.09	O
/	O
8.693	O
=	O
0.9306.	O
the	O
dcg	O
obtained	O
by	O
using	O
the	O
optimal	O
ordering	O
:	O
idcg	O
@	O
k	O
(	O
r	O
)	O
=	O
argmaxπ	O
dcg	O
@	O
k	O
(	O
r	O
)	O
.	O
this	O
can	O
be	O
easily	O
computed	O
by	O
sorting	O
r1	O
:	O
m	O
and	O
then	O
computing	O
dcg	O
@	O
k.	O
finally	O
,	O
we	O
deﬁne	O
the	O
normalized	B
discounted	I
cumulative	I
gain	I
or	O
ndcg	O
as	O
dcg/idcg	O
.	O
table	O
9.3	O
gives	O
a	O
simple	O
numerical	O
example	O
.	O
the	O
ndcg	O
can	O
be	O
averaged	O
over	O
queries	O
to	O
give	O
a	O
measure	O
of	O
performance	O
.	O
•	O
rank	O
correlation	O
.	O
we	O
can	O
measure	O
the	O
correlation	O
between	O
the	O
ranked	O
list	O
,	O
π	O
,	O
and	O
the	O
relevance	O
judegment	O
,	O
π∗	O
,	O
using	O
a	O
variety	O
of	O
methods	O
.	O
one	O
approach	O
,	O
known	O
as	O
the	O
(	O
weighted	O
)	O
kendall	O
’	O
s	O
τ	O
statistics	O
,	O
is	O
deﬁned	O
in	O
terms	O
of	O
the	O
weighted	O
pairwise	O
inconsistency	O
between	O
the	O
two	O
lists	O
:	O
(	O
cid:10	O
)	O
τ	O
(	O
π	O
,	O
π∗	O
)	O
=	O
u	O
<	O
v	O
wuv	O
[	O
1	O
+	O
sgn	O
(	O
πu	O
−	O
πv	O
)	O
sgn	O
(	O
π∗	O
u	O
−	O
π∗	O
v	O
)	O
]	O
(	O
cid:10	O
)	O
2	O
u	O
<	O
v	O
wuv	O
(	O
9.130	O
)	O
a	O
variety	O
of	O
other	O
measures	O
are	O
commonly	O
used	O
.	O
these	O
loss	B
functions	O
can	O
be	O
used	O
in	O
different	O
ways	O
.	O
in	O
the	O
bayesian	O
approach	O
,	O
we	O
ﬁrst	O
ﬁt	O
the	O
model	O
using	O
posterior	O
inference	O
;	O
this	O
depends	O
on	O
the	O
likelihood	B
and	O
prior	O
,	O
but	O
not	O
the	O
loss	B
.	O
we	O
then	O
choose	O
our	O
actions	B
at	O
test	O
time	O
to	O
minimize	O
the	O
expected	O
future	O
loss	B
.	O
one	O
way	O
to	O
do	O
this	O
is	O
to	O
sample	O
parameters	O
from	O
the	O
posterior	O
,	O
θs	O
∼	O
p	O
(	O
θ|d	O
)	O
,	O
and	O
then	O
evaluate	O
,	O
say	O
,	O
the	O
precision	B
@	O
k	O
for	O
different	O
thresholds	O
,	O
averaging	O
over	O
θs	O
.	O
see	O
(	O
zhang	O
et	O
al	O
.	O
2010	O
)	O
for	O
an	O
example	O
of	O
such	O
an	O
approach	O
.	O
in	O
the	O
frequentist	B
approach	O
,	O
we	O
try	O
to	O
minimize	O
the	O
empirical	O
loss	O
on	O
the	O
training	B
set	I
.	O
the	O
problem	O
is	O
that	O
these	O
loss	B
functions	O
are	O
not	O
differentiable	O
functions	O
of	O
the	O
model	O
parameters	O
.	O
we	O
can	O
either	O
use	O
gradient-free	O
optimization	B
methods	O
,	O
or	O
we	O
can	O
minimize	O
a	O
surrogate	B
loss	I
function	I
instead	O
.	O
cross	B
entropy	I
loss	O
(	O
i.e.	O
,	O
negative	B
log	I
likelihood	I
)	O
is	O
an	O
example	O
of	O
a	O
widely	O
used	O
surrogate	B
loss	I
function	I
.	O
another	O
loss	B
,	O
known	O
as	O
weighted	B
approximate-rank	I
pairwise	I
or	O
warp	O
loss	B
,	O
proposed	O
in	O
(	O
usunier	O
et	O
al	O
.	O
2009	O
)	O
and	O
extended	O
in	O
(	O
weston	O
et	O
al	O
.	O
2010	O
)	O
,	O
provides	O
a	O
better	O
approximation	O
to	O
the	O
precision	B
@	O
k	O
loss	B
.	O
warp	O
is	O
deﬁned	O
as	O
follows	O
:	O
(	O
9.131	O
)	O
(	O
9.132	O
)	O
(	O
9.133	O
)	O
warp	O
(	O
f	O
(	O
x	O
,	O
:	O
)	O
,	O
y	O
)	O
(	O
cid:2	O
)	O
l	O
(	O
rank	O
(	O
f	O
(	O
x	O
,	O
:	O
)	O
,	O
y	O
)	O
)	O
rank	O
(	O
f	O
(	O
x	O
,	O
:	O
)	O
,	O
y	O
)	O
=	O
i	O
(	O
f	O
(	O
x	O
,	O
y	O
(	O
cid:4	O
)	O
)	O
≥	O
f	O
(	O
x	O
,	O
y	O
)	O
)	O
l	O
(	O
k	O
)	O
(	O
cid:2	O
)	O
αj	O
,	O
with	O
α1	O
≥	O
α2	O
≥	O
···	O
≥0	O
(	O
cid:2	O
)	O
y	O
(	O
cid:2	O
)	O
(	O
cid:7	O
)	O
=y	O
k	O
(	O
cid:2	O
)	O
j=1	O
9.7.	O
learning	B
to	I
rank	I
*	O
305	O
here	O
f	O
(	O
x	O
,	O
:	O
)	O
=	O
[	O
f	O
(	O
x	O
,	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
f	O
(	O
x	O
,	O
|y|	O
)	O
]	O
is	O
the	O
vector	O
of	O
scores	B
for	O
each	O
possible	O
output	O
label	B
,	O
or	O
,	O
in	O
ir	O
terms	O
,	O
for	O
each	O
possible	O
document	O
corresponding	O
to	O
input	O
query	O
x.	O
the	O
expression	O
rank	O
(	O
f	O
(	O
x	O
,	O
:	O
)	O
,	O
y	O
)	O
measures	O
the	O
rank	O
of	O
the	O
true	O
label	O
y	O
assigned	O
by	O
this	O
scoring	O
function	O
.	O
finally	O
,	O
l	O
transforms	O
the	O
integer	O
rank	O
into	O
a	O
real-valued	O
penalty	O
.	O
using	O
α1	O
=	O
1	O
and	O
αj	O
>	O
1	O
=	O
0	O
would	O
optimize	O
the	O
proportion	O
of	O
top-ranked	O
correct	O
labels	O
.	O
setting	O
α1	O
:	O
k	O
to	O
be	O
non-zero	O
values	O
would	O
optimize	O
the	O
top	O
k	O
in	O
the	O
ranked	O
list	O
,	O
which	O
will	O
induce	O
good	O
performance	O
as	O
measured	O
by	O
map	O
or	O
precision	B
@	O
k.	O
as	O
it	O
stands	O
,	O
warp	O
loss	B
is	O
still	O
hard	O
to	O
optimize	O
,	O
but	O
it	O
can	O
be	O
further	O
approximated	O
by	O
monte	O
carlo	O
sampling	O
,	O
and	O
then	O
optimized	O
by	O
gradient	B
descent	I
,	O
as	O
described	O
in	O
(	O
weston	O
et	O
al	O
.	O
2010	O
)	O
.	O
exercises	O
exercise	O
9.1	O
conjugate	B
prior	I
for	O
univariate	O
gaussian	O
in	O
exponential	B
family	I
form	O
derive	O
the	O
conjugate	B
prior	I
for	O
μ	O
and	O
λ	O
=	O
1/σ2	O
for	O
a	O
univariate	O
gaussian	O
using	O
the	O
exponential	B
family	I
,	O
by	O
analogy	O
to	O
section	O
9.2.5.5.	O
by	O
suitable	O
reparameterization	O
,	O
show	O
that	O
the	O
prior	O
has	O
the	O
form	O
p	O
(	O
μ	O
,	O
λ	O
)	O
=	O
n	O
(	O
μ|γ	O
,	O
λ	O
(	O
2α	O
−	O
1	O
)	O
)	O
ga	O
(	O
λ|α	O
,	O
β	O
)	O
,	O
and	O
thus	O
only	O
has	O
3	O
free	O
parameters	O
.	O
exercise	O
9.2	O
the	O
mvn	O
is	O
in	O
the	O
exponential	B
family	I
show	O
that	O
we	O
can	O
write	O
the	O
mvn	O
in	O
exponential	B
family	I
form	O
.	O
hint	O
:	O
use	O
the	O
information	B
form	I
deﬁned	O
in	O
section	O
4.3.3	O
.	O
10	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
10.1	O
introduction	O
i	O
basically	O
know	O
of	O
two	O
principles	O
for	O
treating	O
complicated	O
systems	O
in	O
simple	O
ways	O
:	O
the	O
ﬁrst	O
is	O
the	O
principle	O
of	O
modularity	O
and	O
the	O
second	O
is	O
the	O
principle	O
of	O
abstraction	O
.	O
i	O
am	O
an	O
apologist	O
for	O
computational	O
probability	O
in	O
machine	B
learning	I
because	O
i	O
believe	O
that	O
probability	O
theory	O
implements	O
these	O
two	O
principles	O
in	O
deep	B
and	O
intriguing	O
ways	O
—	O
namely	O
through	O
factorization	O
and	O
through	O
averaging	O
.	O
exploiting	O
these	O
two	O
mechanisms	O
as	O
fully	O
as	O
possible	O
seems	O
to	O
me	O
to	O
be	O
the	O
way	O
forward	O
in	O
machine	B
learning	I
.	O
—	O
michael	O
jordan	O
,	O
1997	O
(	O
quoted	O
in	O
(	O
frey	O
1998	O
)	O
)	O
.	O
suppose	O
we	O
observe	O
multiple	O
correlated	O
variables	O
,	O
such	O
as	O
words	O
in	O
a	O
document	O
,	O
pixels	O
in	O
an	O
image	O
,	O
or	O
genes	O
in	O
a	O
microarray	O
.	O
how	O
can	O
we	O
compactly	O
represent	O
the	O
joint	B
distribution	I
p	O
(	O
x|θ	O
)	O
?	O
how	O
can	O
we	O
use	O
this	O
distribution	O
to	O
infer	O
one	O
set	O
of	O
variables	O
given	O
another	O
in	O
a	O
reasonable	O
amount	O
of	O
computation	O
time	O
?	O
and	O
how	O
can	O
we	O
learn	O
the	O
parameters	O
of	O
this	O
distribution	O
with	O
a	O
reasonable	O
amount	O
of	O
data	O
?	O
these	O
questions	O
are	O
at	O
the	O
core	O
of	O
probabilistic	O
modeling	O
,	O
inference	B
and	O
learning	B
,	O
and	O
form	O
the	O
topic	B
of	O
this	O
chapter	O
.	O
10.1.1	O
chain	B
rule	I
by	O
the	O
chain	B
rule	I
of	O
probability	O
,	O
we	O
can	O
always	O
represent	O
a	O
joint	B
distribution	I
as	O
follows	O
,	O
using	O
any	O
ordering	O
of	O
the	O
variables	O
:	O
p	O
(	O
x1	O
:	O
v	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
x3|x2	O
,	O
x1	O
)	O
p	O
(	O
x4|x1	O
,	O
x2	O
,	O
x3	O
)	O
.	O
.	O
.	O
p	O
(	O
xv	O
|x1	O
:	O
v	O
−1	O
)	O
(	O
10.1	O
)	O
where	O
v	O
is	O
the	O
number	O
of	O
variables	O
,	O
the	O
matlab-like	O
notation	O
1	O
:	O
v	O
denotes	O
the	O
set	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
v	O
}	O
,	O
and	O
where	O
we	O
have	O
dropped	O
the	O
conditioning	B
on	O
the	O
ﬁxed	O
parameters	O
θ	O
for	O
brevity	O
.	O
the	O
problem	O
with	O
this	O
expression	O
is	O
that	O
it	O
becomes	O
more	O
and	O
more	O
complicated	O
to	O
represent	O
the	O
conditional	O
distributions	O
p	O
(	O
xt|x1	O
:	O
t−1	O
)	O
as	O
t	O
gets	O
large	O
.	O
for	O
example	O
,	O
suppose	O
all	O
the	O
variables	O
have	O
k	O
states	O
.	O
we	O
can	O
represent	O
p	O
(	O
x1	O
)	O
as	O
a	O
table	O
of	O
o	O
(	O
k	O
)	O
numbers	O
,	O
representing	O
a	O
discrete	B
distribution	O
(	O
there	O
are	O
actually	O
only	O
k	O
−	O
1	O
free	O
parameters	O
,	O
due	O
to	O
the	O
sum-to-one	O
constraint	O
,	O
but	O
we	O
write	O
o	O
(	O
k	O
)	O
for	O
simplicity	O
)	O
.	O
similarly	O
,	O
we	O
can	O
represent	O
p	O
(	O
x2|x1	O
)	O
as	O
a	O
table	O
of	O
o	O
(	O
k	O
2	O
)	O
numbers	O
by	O
writing	O
p	O
(	O
x2	O
=	O
j|x1	O
=	O
i	O
)	O
=	O
tij	O
;	O
we	O
j	O
tij	O
=	O
1	O
for	O
all	O
rows	O
i	O
,	O
say	O
that	O
t	O
is	O
a	O
stochastic	B
matrix	I
,	O
since	O
it	O
satisﬁes	O
the	O
constraint	O
and	O
0	O
≤	O
tij	O
≤	O
1	O
for	O
all	O
entries	O
.	O
similarly	O
,	O
we	O
can	O
represent	O
p	O
(	O
x3|x1	O
,	O
x2	O
)	O
as	O
a	O
3d	O
table	O
with	O
(	O
cid:10	O
)	O
308	O
chapter	O
10.	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
o	O
(	O
k	O
3	O
)	O
numbers	O
.	O
these	O
are	O
called	O
conditional	B
probability	I
tables	I
or	O
cpts	O
.	O
we	O
see	O
that	O
there	O
are	O
o	O
(	O
k	O
v	O
)	O
parameters	O
in	O
the	O
model	O
.	O
we	O
would	O
need	O
an	O
awful	O
lot	O
of	O
data	O
to	O
learn	O
so	O
many	O
parameters	O
.	O
one	O
solution	O
is	O
to	O
replace	O
each	O
cpt	O
with	O
a	O
more	O
parsimonius	O
conditional	B
probability	I
distri-	O
bution	O
or	O
cpd	O
,	O
such	O
as	O
multinomial	B
logistic	I
regression	I
,	O
i.e.	O
,	O
p	O
(	O
xt	O
=	O
k|x1	O
:	O
t−1	O
)	O
=	O
s	O
(	O
wtx1	O
:	O
t−1	O
)	O
k.	O
the	O
total	O
number	O
of	O
parameters	O
is	O
now	O
only	O
o	O
(	O
k	O
2v	O
2	O
)	O
,	O
making	O
this	O
a	O
compact	O
density	O
model	O
(	O
neal	O
1992	O
;	O
frey	O
1998	O
)	O
.	O
this	O
is	O
adequate	O
if	O
all	O
we	O
want	O
to	O
do	O
is	O
evaluate	O
the	O
probability	O
of	O
a	O
fully	O
observed	O
vector	O
x1	O
:	O
t	O
.	O
for	O
example	O
,	O
we	O
can	O
use	O
this	O
model	O
to	O
deﬁne	O
a	O
class-conditional	B
density	I
,	O
p	O
(	O
x|y	O
=	O
c	O
)	O
,	O
thus	O
making	O
a	O
generative	B
classiﬁer	I
(	O
bengio	O
and	O
bengio	O
2000	O
)	O
.	O
however	O
,	O
this	O
model	O
is	O
not	O
useful	O
for	O
other	O
kinds	O
of	O
prediction	O
tasks	O
,	O
since	O
each	O
variable	O
depends	O
on	O
all	O
the	O
previous	O
variables	O
.	O
so	O
we	O
need	O
another	O
approach	O
.	O
10.1.2	O
conditional	B
independence	I
the	O
key	O
to	O
efficiently	O
representing	O
large	O
joint	O
distributions	O
is	O
to	O
make	O
some	O
assumptions	O
about	O
conditional	B
independence	I
(	O
ci	O
)	O
.	O
recall	B
from	O
section	O
2.2.4	O
that	O
x	O
and	O
y	O
are	O
conditionally	O
inde-	O
pendent	O
given	O
z	O
,	O
denoted	O
x	O
⊥	O
y	O
|z	O
,	O
if	O
and	O
only	O
if	O
(	O
iff	B
)	O
the	O
conditional	O
joint	O
can	O
be	O
written	O
as	O
a	O
product	O
of	O
conditional	O
marginals	O
,	O
i.e.	O
,	O
x	O
⊥	O
y	O
|z	O
⇐⇒	O
p	O
(	O
x	O
,	O
y	O
|z	O
)	O
=	O
p	O
(	O
x|z	O
)	O
p	O
(	O
y	O
|z	O
)	O
(	O
10.2	O
)	O
let	O
us	O
see	O
why	O
this	O
might	O
help	O
.	O
suppose	O
we	O
assume	O
that	O
xt+1	O
⊥	O
x1	O
:	O
t−1|xt	O
,	O
or	O
in	O
words	O
,	O
“	O
the	O
future	O
is	O
independent	O
of	O
the	O
past	O
given	O
the	O
present	O
”	O
.	O
this	O
is	O
called	O
the	O
(	O
ﬁrst	O
order	O
)	O
markov	O
assumption	O
.	O
using	O
this	O
assumption	O
,	O
plus	O
the	O
chain	B
rule	I
,	O
we	O
can	O
write	O
the	O
joint	B
distribution	I
as	O
follows	O
:	O
v	O
(	O
cid:27	O
)	O
p	O
(	O
x1	O
:	O
v	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
xt|xt−1	O
)	O
(	O
10.3	O
)	O
t=1	O
this	O
is	O
called	O
a	O
(	O
ﬁrst-order	O
)	O
markov	O
chain	O
.	O
they	O
can	O
be	O
characterized	O
by	O
an	O
initial	O
distribution	O
over	O
states	O
,	O
p	O
(	O
x1	O
=	O
i	O
)	O
,	O
plus	O
a	O
state	B
transition	I
matrix	I
p	O
(	O
xt	O
=	O
j|xt−1	O
=	O
i	O
)	O
.	O
see	O
section	O
17.2	O
for	O
more	O
information	B
.	O
10.1.3	O
graphical	O
models	O
although	O
the	O
ﬁrst-order	O
markov	O
assumption	O
is	O
useful	O
for	O
deﬁning	O
distributions	O
on	O
1d	O
sequences	O
,	O
how	O
can	O
we	O
deﬁne	O
distributions	O
on	O
2d	O
images	O
,	O
or	O
3d	O
videos	O
,	O
or	O
,	O
in	O
general	O
,	O
arbitrary	O
collections	O
of	O
variables	O
(	O
such	O
as	O
genes	O
belonging	O
to	O
some	O
biological	O
pathway	O
)	O
?	O
this	O
is	O
where	O
graphical	O
models	O
come	O
in	O
.	O
a	O
graphical	B
model	I
(	O
gm	O
)	O
is	O
a	O
way	O
to	O
represent	O
a	O
joint	B
distribution	I
by	O
making	O
ci	O
assumptions	O
.	O
in	O
particular	O
,	O
the	O
nodes	B
in	O
the	O
graph	B
represent	O
random	O
variables	O
,	O
and	O
the	O
(	O
lack	O
of	O
)	O
edges	B
represent	O
ci	O
assumptions	O
.	O
(	O
a	O
better	O
name	O
for	O
these	O
models	O
would	O
in	O
fact	O
be	O
“	O
independence	O
diagrams	O
”	O
,	O
but	O
the	O
term	O
“	O
graphical	O
models	O
”	O
is	O
now	O
entrenched	O
.	O
)	O
there	O
are	O
several	O
kinds	O
of	O
graphical	B
model	I
,	O
depending	O
on	O
whether	O
the	O
graph	B
is	O
directed	B
,	O
undirected	B
,	O
or	O
some	O
combination	O
of	O
directed	B
and	O
undirected	B
.	O
in	O
this	O
chapter	O
,	O
we	O
just	O
study	O
directed	B
graphs	O
.	O
we	O
consider	O
undirected	B
graphs	O
in	O
chapter	O
19	O
.	O
10.1.	O
introduction	O
309	O
2	O
1	O
4	O
3	O
2	O
5	O
1	O
4	O
3	O
5	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
10.1	O
5	O
are	O
the	O
leaves	B
.	O
{	O
3	O
,	O
5	O
}	O
.	O
(	O
a	O
)	O
a	O
simple	O
dag	O
on	O
5	O
nodes	B
,	O
numbered	O
in	O
topological	O
order	O
.	O
node	O
1	O
is	O
the	O
root	B
,	O
nodes	B
4	O
and	O
(	O
b	O
)	O
a	O
simple	O
undirected	O
graph	B
,	O
with	O
the	O
following	O
maximal	O
cliques	O
:	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
,	O
{	O
2	O
,	O
3	O
,	O
4	O
}	O
,	O
10.1.4	O
graph	B
terminology	O
before	O
we	O
continue	O
,	O
we	O
must	O
deﬁne	O
a	O
few	O
basic	O
terms	O
,	O
most	O
of	O
which	O
are	O
very	O
intuitive	O
.	O
a	O
graph	B
g	O
=	O
(	O
v	O
,	O
e	O
)	O
consists	O
of	O
a	O
set	O
of	O
nodes	B
or	O
vertices	B
,	O
v	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
}	O
,	O
and	O
a	O
set	O
of	O
edges	B
,	O
e	O
=	O
{	O
(	O
s	O
,	O
t	O
)	O
:	O
s	O
,	O
t	O
∈	O
v	O
}	O
.	O
we	O
can	O
represent	O
the	O
graph	B
by	O
its	O
adjacency	B
matrix	I
,	O
in	O
which	O
we	O
write	O
g	O
(	O
s	O
,	O
t	O
)	O
=	O
1	O
to	O
denote	O
(	O
s	O
,	O
t	O
)	O
∈	O
e	O
,	O
that	O
is	O
,	O
if	O
s	O
→	O
t	O
is	O
an	O
edge	O
in	O
the	O
graph	B
.	O
if	O
g	O
(	O
s	O
,	O
t	O
)	O
=	O
1	O
iff	B
g	O
(	O
t	O
,	O
s	O
)	O
=	O
1	O
,	O
we	O
say	O
the	O
graph	B
is	O
undirected	B
,	O
otherwise	O
it	O
is	O
directed	B
.	O
we	O
usually	O
assume	O
g	O
(	O
s	O
,	O
s	O
)	O
=	O
0	O
,	O
which	O
means	O
there	O
are	O
no	O
self	O
loops	O
.	O
here	O
are	O
some	O
other	O
terms	O
we	O
will	O
commonly	O
use	O
:	O
•	O
parent	O
for	O
a	O
directed	B
graph	O
,	O
the	O
parents	B
of	O
a	O
node	O
is	O
the	O
set	O
of	O
all	O
nodes	O
that	O
feed	O
into	O
it	O
:	O
pa	O
(	O
s	O
)	O
(	O
cid:2	O
)	O
{	O
t	O
:	O
g	O
(	O
t	O
,	O
s	O
)	O
=	O
1	O
}	O
.	O
ch	O
(	O
s	O
)	O
(	O
cid:2	O
)	O
{	O
t	O
:	O
g	O
(	O
s	O
,	O
t	O
)	O
=	O
1	O
}	O
.	O
{	O
s	O
}	O
∪	O
pa	O
(	O
s	O
)	O
.	O
•	O
child	O
for	O
a	O
directed	B
graph	O
,	O
the	O
children	B
of	O
a	O
node	O
is	O
the	O
set	O
of	O
all	O
nodes	O
that	O
feed	O
out	O
of	O
it	O
:	O
•	O
family	B
for	O
a	O
directed	B
graph	O
,	O
the	O
family	B
of	O
a	O
node	O
is	O
the	O
node	O
and	O
its	O
parents	B
,	O
fam	O
(	O
s	O
)	O
=	O
•	O
root	B
for	O
a	O
directed	B
graph	O
,	O
a	O
root	B
is	O
a	O
node	O
with	O
no	O
parents	O
.	O
•	O
leaf	B
for	O
a	O
directed	B
graph	O
,	O
a	O
leaf	B
is	O
a	O
node	O
with	O
no	O
children	O
.	O
•	O
ancestors	B
for	O
a	O
directed	B
graph	O
,	O
the	O
ancestors	B
are	O
the	O
parents	B
,	O
grand-parents	O
,	O
etc	O
of	O
a	O
node	O
.	O
that	O
is	O
,	O
the	O
ancestors	B
of	O
t	O
is	O
the	O
set	O
of	O
nodes	O
that	O
connect	O
to	O
t	O
via	O
a	O
trail	B
:	O
anc	O
(	O
t	O
)	O
(	O
cid:2	O
)	O
{	O
s	O
:	O
s	O
;	O
t	O
}	O
.	O
•	O
descendants	B
for	O
a	O
directed	B
graph	O
,	O
the	O
descendants	B
are	O
the	O
children	B
,	O
grand-children	O
,	O
etc	O
of	O
a	O
node	O
.	O
that	O
is	O
,	O
the	O
descendants	B
of	O
s	O
is	O
the	O
set	O
of	O
nodes	O
that	O
can	O
be	O
reached	O
via	O
trails	O
from	O
s	O
:	O
desc	O
(	O
s	O
)	O
(	O
cid:2	O
)	O
{	O
t	O
:	O
s	O
;	O
t	O
}	O
.	O
•	O
neighbors	B
for	O
any	O
graph	B
,	O
we	O
deﬁne	O
the	O
neighbors	B
of	O
a	O
node	O
as	O
the	O
set	O
of	O
all	O
immediately	O
connected	O
nodes	B
,	O
nbr	O
(	O
s	O
)	O
(	O
cid:2	O
)	O
{	O
t	O
:	O
g	O
(	O
s	O
,	O
t	O
)	O
=	O
1	O
∨	O
g	O
(	O
t	O
,	O
s	O
)	O
=	O
1	O
}	O
.	O
for	O
an	O
undirected	B
graph	O
,	O
we	O
310	O
chapter	O
10.	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
write	O
s	O
∼	O
t	O
to	O
indicate	O
that	O
s	O
and	O
t	O
are	O
neighbors	B
(	O
so	O
(	O
s	O
,	O
t	O
)	O
∈	O
e	O
is	O
an	O
edge	O
in	O
the	O
graph	B
)	O
.	O
•	O
degree	B
the	O
degree	B
of	O
a	O
node	O
is	O
the	O
number	O
of	O
neighbors	B
.	O
for	O
directed	B
graphs	O
,	O
we	O
speak	O
of	O
the	O
in-degree	B
and	O
out-degree	B
,	O
which	O
count	O
the	O
number	O
of	O
parents	B
and	O
children	B
.	O
•	O
cycle	B
or	O
loop	B
for	O
any	O
graph	B
,	O
we	O
deﬁne	O
a	O
cycle	B
or	O
loop	B
to	O
be	O
a	O
series	O
of	O
nodes	B
such	O
that	O
we	O
can	O
get	O
back	O
to	O
where	O
we	O
started	O
by	O
following	O
edges	B
,	O
s1	O
−	O
s2	O
···	O
−	O
sn	O
−	O
s1	O
,	O
n	O
≥	O
2.	O
if	O
the	O
graph	B
is	O
directed	B
,	O
we	O
may	O
speak	O
of	O
a	O
directed	B
cycle	O
.	O
for	O
example	O
,	O
in	O
figure	O
10.1	O
(	O
a	O
)	O
,	O
there	O
are	O
no	O
directed	O
cycles	O
,	O
but	O
1	O
→	O
2	O
→	O
4	O
→	O
3	O
→	O
1	O
is	O
an	O
undirected	B
cycle	O
.	O
•	O
dag	O
a	O
directed	B
acyclic	I
graph	I
or	O
dag	O
is	O
a	O
directed	B
graph	O
with	O
no	O
directed	O
cycles	O
.	O
see	O
figure	O
10.1	O
(	O
a	O
)	O
for	O
an	O
example	O
.	O
•	O
topological	B
ordering	I
for	O
a	O
dag	O
,	O
a	O
topological	B
ordering	I
or	O
total	B
ordering	I
is	O
a	O
numbering	O
of	O
the	O
nodes	B
such	O
that	O
parents	B
have	O
lower	O
numbers	O
than	O
their	O
children	B
.	O
for	O
example	O
,	O
in	O
figure	O
10.1	O
(	O
a	O
)	O
,	O
we	O
can	O
use	O
(	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
)	O
,	O
or	O
(	O
1	O
,	O
3	O
,	O
2	O
,	O
5	O
,	O
4	O
)	O
,	O
etc	O
.	O
•	O
path	B
or	O
trail	B
a	O
path	B
or	O
trail	B
s	O
;	O
t	O
is	O
a	O
series	O
of	O
directed	B
edges	O
leading	O
from	O
s	O
to	O
t.	O
•	O
tree	B
an	O
undirected	B
tree	O
is	O
an	O
undirectecd	O
graph	B
with	O
no	O
cycles	O
.	O
a	O
directed	B
tree	O
is	O
a	O
dag	O
in	O
which	O
there	O
are	O
no	O
directed	O
cycles	O
.	O
if	O
we	O
allow	O
a	O
node	O
to	O
have	O
multiple	O
parents	O
,	O
we	O
call	O
it	O
a	O
polytree	B
,	O
otherwise	O
we	O
call	O
it	O
a	O
moral	O
directed	B
tree	O
.	O
•	O
forest	B
a	O
forest	B
is	O
a	O
set	O
of	O
trees	O
.	O
•	O
subgraph	B
a	O
(	O
node-induced	O
)	O
subgraph	B
ga	O
is	O
the	O
graph	B
created	O
by	O
using	O
the	O
nodes	B
in	O
a	O
and	O
their	O
corresponding	O
edges	B
,	O
ga	O
=	O
(	O
va	O
,	O
ea	O
)	O
.	O
•	O
clique	B
for	O
an	O
undirected	B
graph	O
,	O
a	O
clique	B
is	O
a	O
set	O
of	O
nodes	O
that	O
are	O
all	O
neighbors	O
of	O
each	O
other	O
.	O
a	O
maximal	B
clique	I
is	O
a	O
clique	B
which	O
can	O
not	O
be	O
made	O
any	O
larger	O
without	O
losing	O
the	O
clique	B
property	O
.	O
for	O
example	O
,	O
in	O
figure	O
10.1	O
(	O
b	O
)	O
,	O
{	O
1	O
,	O
2	O
}	O
is	O
a	O
clique	B
but	O
it	O
is	O
not	O
maximal	O
,	O
since	O
in	O
fact	O
,	O
the	O
maximal	O
cliques	O
are	O
as	O
we	O
can	O
add	O
3	O
and	O
still	O
maintain	O
the	O
clique	B
property	O
.	O
follows	O
:	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
,	O
{	O
2	O
,	O
3	O
,	O
4	O
}	O
,	O
{	O
3	O
,	O
5	O
}	O
.	O
10.1.5	O
directed	O
graphical	O
models	O
a	O
directed	B
graphical	I
model	I
or	O
dgm	O
is	O
a	O
gm	O
whose	O
graph	B
is	O
a	O
dag	O
.	O
these	O
are	O
more	O
commonly	O
known	O
as	O
bayesian	O
networks	O
.	O
however	O
,	O
there	O
is	O
nothing	O
inherently	O
“	O
bayesian	O
”	O
about	O
bayesian	O
networks	O
:	O
they	O
are	O
just	O
a	O
way	O
of	O
deﬁning	O
probability	O
distributions	O
.	O
these	O
models	O
are	O
also	O
called	O
belief	B
networks	I
.	O
the	O
term	O
“	O
belief	O
”	O
here	O
refers	O
to	O
subjective	B
probability	I
.	O
once	O
again	O
,	O
there	O
is	O
nothing	O
inherently	O
subjective	B
about	O
the	O
kinds	O
of	O
probability	O
distributions	O
represented	O
by	O
dgms	O
.	O
finally	O
,	O
these	O
models	O
are	O
sometimes	O
called	O
causal	B
networks	I
,	O
because	O
the	O
directed	B
arrows	O
are	O
sometimes	O
interpreted	O
as	O
representing	O
causal	O
relations	O
.	O
however	O
,	O
there	O
is	O
nothing	O
inherently	O
causal	O
about	O
dgms	O
.	O
(	O
see	O
section	O
26.6.1	O
for	O
a	O
discussion	O
of	O
causal	O
dgms	O
.	O
)	O
for	O
these	O
reasons	O
,	O
we	O
use	O
the	O
more	O
neutral	O
(	O
but	O
less	O
glamorous	O
)	O
term	O
dgm	O
.	O
the	O
key	O
property	O
of	O
dags	O
is	O
that	O
the	O
nodes	B
can	O
be	O
ordered	O
such	O
that	O
parents	B
come	O
before	O
children	B
.	O
this	O
is	O
called	O
a	O
topological	B
ordering	I
,	O
and	O
it	O
can	O
be	O
constructed	O
from	O
any	O
dag	O
.	O
given	O
such	O
an	O
order	O
,	O
we	O
deﬁne	O
the	O
ordered	O
markov	O
property	O
to	O
be	O
the	O
assumption	O
that	O
a	O
node	O
only	O
depends	O
on	O
its	O
immediate	O
parents	B
,	O
not	O
on	O
all	O
predecessors	O
in	O
the	O
ordering	O
,	O
i.e.	O
,	O
xs	O
⊥	O
xpred	O
(	O
s	O
)	O
\pa	O
(	O
s	O
)	O
|xpa	O
(	O
s	O
)	O
(	O
10.4	O
)	O
where	O
pa	O
(	O
s	O
)	O
are	O
the	O
parents	B
of	O
node	O
s	O
,	O
and	O
pred	O
(	O
s	O
)	O
are	O
the	O
predecessors	O
of	O
node	O
s	O
in	O
the	O
ordering	O
.	O
this	O
is	O
a	O
natural	O
generalization	O
of	O
the	O
ﬁrst-order	O
markov	O
property	O
to	O
from	O
chains	O
to	O
general	O
dags	O
.	O
10.2.	O
examples	O
311	O
y	O
y	O
x1	O
x2	O
x3	O
x4	O
(	O
a	O
)	O
x2	O
x4	O
x1	O
(	O
b	O
)	O
x3	O
figure	O
10.2	O
(	O
a	O
)	O
a	O
naive	O
bayes	O
classiﬁer	O
represented	O
as	O
a	O
dgm	O
.	O
we	O
assume	O
there	O
are	O
d	O
=	O
4	O
features	B
,	O
for	O
simplicity	O
.	O
shaded	O
nodes	B
are	O
observed	O
,	O
unshaded	O
nodes	B
are	O
hidden	B
.	O
(	O
b	O
)	O
tree-augmented	O
naive	O
bayes	O
classiﬁer	O
for	O
d	O
=	O
4	O
features	B
.	O
in	O
general	O
,	O
the	O
tree	B
topology	O
can	O
change	O
depending	O
on	O
the	O
value	O
of	O
y.	O
for	O
example	O
,	O
the	O
dag	O
in	O
figure	O
10.1	O
(	O
a	O
)	O
encodes	O
the	O
following	O
joint	B
distribution	I
:	O
p	O
(	O
x1:5	O
)	O
=p	O
(	O
x1	O
)	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
x3|x1	O
,	O
x2	O
)	O
p	O
(	O
x4|x1	O
,	O
x2	O
,	O
x3	O
)	O
p	O
(	O
x5|x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
x3|x1	O
)	O
p	O
(	O
x4|x2	O
,	O
x3	O
)	O
p	O
(	O
x5|x3	O
)	O
(	O
10.5	O
)	O
(	O
10.6	O
)	O
in	O
general	O
,	O
we	O
have	O
p	O
(	O
x1	O
:	O
v	O
|g	O
)	O
=	O
v	O
(	O
cid:27	O
)	O
p	O
(	O
xt|xpa	O
(	O
t	O
)	O
)	O
t=1	O
(	O
10.7	O
)	O
where	O
each	O
term	O
p	O
(	O
xt|xpa	O
(	O
t	O
)	O
)	O
is	O
a	O
cpd	O
.	O
we	O
have	O
written	O
the	O
distribution	O
as	O
p	O
(	O
x|g	O
)	O
to	O
emphasize	O
that	O
this	O
equation	O
only	O
holds	O
if	O
the	O
ci	O
assumptions	O
encoded	O
in	O
dag	O
g	O
are	O
correct	O
.	O
however	O
,	O
if	O
each	O
node	O
has	O
o	O
(	O
f	O
)	O
parents	B
and	O
we	O
will	O
usual	O
drop	O
this	O
explicit	O
conditioning	O
for	O
brevity	O
.	O
k	O
states	O
,	O
the	O
number	O
of	O
parameters	O
in	O
the	O
model	O
is	O
o	O
(	O
v	O
k	O
f	O
)	O
,	O
which	O
is	O
much	O
less	O
than	O
the	O
o	O
(	O
k	O
v	O
)	O
needed	O
by	O
a	O
model	O
which	O
makes	O
no	O
ci	O
assumptions	O
.	O
10.2	O
examples	O
in	O
this	O
section	O
,	O
we	O
show	O
a	O
wide	O
variety	O
of	O
commonly	O
used	O
probabilistic	O
models	O
can	O
be	O
conve-	O
niently	O
represented	O
as	O
dgms	O
.	O
10.2.1	O
naive	O
bayes	O
classiﬁers	O
in	O
section	O
3.5	O
,	O
we	O
introduced	O
the	O
naive	O
bayes	O
classiﬁer	O
.	O
this	O
assumes	O
the	O
features	B
are	O
condi-	O
tionally	O
independent	O
given	O
the	O
class	O
label	O
.	O
this	O
assumption	O
is	O
illustrated	O
in	O
figure	O
10.2	O
(	O
a	O
)	O
.	O
this	O
allows	O
us	O
to	O
write	O
the	O
joint	O
distirbution	O
as	O
follows	O
:	O
d	O
(	O
cid:27	O
)	O
p	O
(	O
y	O
,	O
x	O
)	O
=	O
p	O
(	O
y	O
)	O
p	O
(	O
xj|y	O
)	O
(	O
10.8	O
)	O
j=1	O
the	O
naive	O
bayes	O
assumption	O
is	O
rather	O
naive	O
,	O
since	O
it	O
assumes	O
the	O
features	B
are	O
conditionally	B
independent	I
.	O
one	O
way	O
to	O
capture	O
correlation	O
between	O
the	O
features	B
is	O
to	O
use	O
a	O
graphical	B
model	I
.	O
in	O
particular	O
,	O
if	O
the	O
model	O
is	O
a	O
tree	B
,	O
the	O
method	O
is	O
known	O
as	O
a	O
tree-augmented	O
naive	O
bayes	O
312	O
chapter	O
10.	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
···	O
x1	O
x2	O
x3	O
(	O
a	O
)	O
x1	O
x2	O
x3	O
(	O
b	O
)	O
·	O
·	O
·	O
x4	O
figure	O
10.3	O
a	O
ﬁrst	O
and	O
second	B
order	I
markov	O
chain	O
.	O
z1	O
x1	O
z2	O
x2	O
zt	O
xt	O
figure	O
10.4	O
a	O
ﬁrst-order	O
hmm	O
.	O
classiﬁer	O
or	O
tan	O
model	O
(	O
friedman	O
et	O
al	O
.	O
1997	O
)	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
10.2	O
(	O
b	O
)	O
.	O
the	O
reason	O
to	O
use	O
a	O
tree	B
,	O
as	O
opposed	O
to	O
a	O
generic	O
graph	B
,	O
is	O
two-fold	O
.	O
first	O
,	O
it	O
is	O
easy	O
to	O
ﬁnd	O
the	O
optimal	O
tree	O
structure	O
using	O
the	O
chow-liu	O
algorithm	O
,	O
as	O
explained	O
in	O
section	O
26.3.	O
second	O
,	O
it	O
is	O
easy	O
to	O
handle	O
missing	B
features	O
in	O
a	O
tree-structured	O
model	O
,	O
as	O
we	O
explain	O
in	O
section	O
20.2	O
.	O
10.2.2	O
markov	O
and	O
hidden	B
markov	O
models	O
figure	O
10.3	O
(	O
a	O
)	O
illustrates	O
a	O
ﬁrst-order	O
markov	O
chain	O
as	O
a	O
dag	O
.	O
of	O
course	O
,	O
the	O
assumption	O
that	O
the	O
immediate	O
past	O
,	O
xt−1	O
,	O
captures	O
everything	O
we	O
need	O
to	O
know	O
about	O
the	O
entire	O
history	O
,	O
x1	O
:	O
t−2	O
,	O
is	O
a	O
bit	O
strong	O
.	O
we	O
can	O
relax	O
it	O
a	O
little	O
by	O
adding	O
a	O
dependence	O
from	O
xt−2	O
to	O
xt	O
as	O
well	O
;	O
this	O
is	O
called	O
a	O
second	B
order	I
markov	O
chain	O
,	O
and	O
is	O
illustrated	O
in	O
figure	O
10.3	O
(	O
b	O
)	O
.	O
the	O
corresponding	O
joint	O
has	O
the	O
following	O
form	O
:	O
t	O
(	O
cid:27	O
)	O
p	O
(	O
x1	O
:	O
t	O
)	O
=	O
p	O
(	O
x1	O
,	O
x2	O
)	O
p	O
(	O
x3|x1	O
,	O
x2	O
)	O
p	O
(	O
x4|x2	O
,	O
x3	O
)	O
.	O
.	O
.	O
=	O
p	O
(	O
x1	O
,	O
x2	O
)	O
p	O
(	O
xt|xt−1	O
,	O
xt−2	O
)	O
(	O
10.9	O
)	O
t=3	O
we	O
can	O
create	O
higher-order	O
markov	O
models	O
in	O
a	O
similar	B
way	O
.	O
see	O
section	O
17.2	O
for	O
a	O
more	O
detailed	O
discussion	O
of	O
markov	O
models	O
.	O
unfortunately	O
,	O
even	O
the	O
second-order	O
markov	O
assumption	O
may	O
be	O
inadequate	O
if	O
there	O
are	O
long-	O
range	O
correlations	O
amongst	O
the	O
observations	O
.	O
we	O
can	O
’	O
t	O
keep	O
building	O
ever	O
higher	O
order	O
models	O
,	O
since	O
the	O
number	O
of	O
parameters	O
will	O
blow	O
up	O
.	O
an	O
alternative	O
approach	O
is	O
to	O
assume	O
that	O
there	O
is	O
an	O
underlying	O
hidden	B
process	O
,	O
that	O
can	O
be	O
modeled	O
by	O
a	O
ﬁrst-order	O
markov	O
chain	O
,	O
but	O
that	O
the	O
data	O
is	O
a	O
noisy	O
observation	B
of	O
this	O
process	O
.	O
the	O
result	O
is	O
known	O
as	O
a	O
hidden	B
markov	O
model	O
or	O
hmm	O
,	O
and	O
is	O
illustrated	O
in	O
figure	O
10.4.	O
here	O
zt	O
is	O
known	O
as	O
a	O
hidden	B
variable	I
at	O
“	O
time	O
”	O
t	O
,	O
and	O
xt	O
is	O
the	O
observed	O
variable	O
.	O
(	O
we	O
put	O
“	O
time	O
”	O
in	O
quotation	O
marks	O
,	O
since	O
these	O
models	O
can	O
be	O
applied	O
to	O
any	O
kind	O
of	O
sequence	O
data	O
,	O
such	O
as	O
genomics	O
or	O
language	O
,	O
where	O
t	O
represents	O
location	O
rather	O
than	O
time	O
.	O
)	O
the	O
cpd	O
p	O
(	O
zt|zt−1	O
)	O
is	O
the	O
transition	B
model	I
,	O
and	O
the	O
cpd	O
p	O
(	O
xt|zt	O
)	O
is	O
the	O
observation	B
model	I
.	O
10.2.	O
examples	O
313	O
h0	O
1	O
1	O
1	O
1	O
h1	O
0	O
1	O
0	O
1	O
h2	O
p	O
(	O
v	O
=	O
0|h1	O
,	O
h2	O
)	O
p	O
(	O
v	O
=	O
1|h1	O
,	O
h2	O
)	O
0	O
0	O
1	O
1	O
1	O
−	O
θ0	O
1	O
−	O
θ0θ1	O
1	O
−	O
θ0θ2	O
1	O
−	O
θ0θ1θ2	O
θ0	O
θ0θ1	O
θ0θ2	O
θ0θ1θ2	O
table	O
10.1	O
noisy-or	O
cpd	O
for	O
2	O
parents	B
augmented	O
with	O
leak	B
node	I
.	O
we	O
have	O
omitted	O
the	O
t	O
subscript	O
for	O
brevity	O
.	O
the	O
hidden	B
variables	I
often	O
represent	O
quantities	O
of	O
interest	O
,	O
such	O
as	O
the	O
identity	O
of	O
the	O
word	O
that	O
someone	O
is	O
currently	O
speaking	O
.	O
the	O
observed	O
variables	O
are	O
what	O
we	O
measure	O
,	O
such	O
as	O
the	O
acoustic	O
waveform	O
.	O
what	O
we	O
would	O
like	O
to	O
do	O
is	O
estimate	O
the	O
hidden	B
state	O
given	O
the	O
data	O
,	O
i.e.	O
,	O
to	O
compute	O
p	O
(	O
zt|x1	O
:	O
t	O
,	O
θ	O
)	O
.	O
this	O
is	O
called	O
state	B
estimation	I
,	O
and	O
is	O
just	O
another	O
form	O
of	O
probabilistic	B
inference	I
.	O
see	O
chapter	O
17	O
for	O
further	O
details	O
on	O
hmms	O
.	O
10.2.3	O
medical	O
diagnosis	O
consider	O
modeling	O
the	O
relationship	O
between	O
various	O
variables	O
that	O
are	O
measured	O
in	O
an	O
intensive	B
care	I
unit	I
(	O
icu	O
)	O
,	O
such	O
as	O
the	O
breathing	O
rate	B
of	O
a	O
patient	O
,	O
their	O
blood	O
pressure	O
,	O
etc	O
.	O
the	O
alarm	B
network	I
in	O
figure	O
10.5	O
(	O
a	O
)	O
is	O
one	O
way	O
to	O
represent	O
these	O
(	O
in	O
)	O
dependencies	O
(	O
beinlich	O
et	O
al	O
.	O
1989	O
)	O
.	O
this	O
model	O
has	O
37	O
variables	O
and	O
504	O
parameters	O
.	O
since	O
this	O
model	O
was	O
created	O
by	O
hand	O
,	O
by	O
a	O
process	O
called	O
knowledge	B
engineering	I
,	O
it	O
is	O
known	O
as	O
a	O
probabilistic	B
expert	I
system	I
.	O
in	O
section	O
10.4	O
,	O
we	O
discuss	O
how	O
to	O
learn	O
the	O
parameters	O
of	O
dgms	O
from	O
data	O
,	O
assuming	O
the	O
graph	B
structure	O
is	O
known	O
,	O
and	O
in	O
chapter	O
26	O
,	O
we	O
discuss	O
how	O
to	O
learn	O
the	O
graph	B
structure	O
itself	O
.	O
a	O
different	O
kind	O
of	O
medical	O
diagnosis	O
network	O
,	O
known	O
as	O
the	O
quick	B
medical	I
reference	I
or	O
qmr	O
network	O
(	O
shwe	O
et	O
al	O
.	O
1991	O
)	O
,	O
is	O
shown	O
in	O
figure	O
10.5	O
(	O
b	O
)	O
.	O
this	O
was	O
designed	O
to	O
model	O
infectious	O
diseases	O
.	O
the	O
qmr	O
model	O
is	O
a	O
bipartite	B
graph	I
structure	O
,	O
with	O
diseases	O
(	O
causes	O
)	O
at	O
the	O
top	O
and	O
symptoms	O
or	O
ﬁndings	O
at	O
the	O
bottom	O
.	O
all	O
nodes	O
are	O
binary	O
.	O
we	O
can	O
write	O
the	O
distribution	O
as	O
follows	O
:	O
(	O
cid:27	O
)	O
(	O
cid:27	O
)	O
p	O
(	O
v	O
,	O
h	O
)	O
=	O
p	O
(	O
hs	O
)	O
p	O
(	O
vt|hpa	O
(	O
t	O
)	O
)	O
(	O
10.10	O
)	O
s	O
t	O
where	O
hs	O
represent	O
the	O
hidden	B
nodes	I
(	O
diseases	O
)	O
,	O
and	O
vt	O
represent	O
the	O
visible	B
nodes	I
(	O
symptoms	O
)	O
.	O
the	O
cpd	O
for	O
the	O
root	B
nodes	O
are	O
just	O
bernoulli	O
distributions	O
,	O
representing	O
the	O
prior	O
probability	O
of	O
that	O
disease	O
.	O
representing	O
the	O
cpds	O
for	O
the	O
leaves	B
(	O
symptoms	O
)	O
using	O
cpts	O
would	O
require	O
too	O
many	O
parameters	O
,	O
because	O
the	O
fan-in	B
(	O
number	O
of	O
parents	B
)	O
of	O
many	O
leaf	B
nodes	O
is	O
very	O
high	O
.	O
a	O
natural	O
alternative	O
is	O
to	O
use	O
logistic	B
regression	I
to	O
model	O
the	O
cpd	O
,	O
p	O
(	O
vt	O
=	O
1|hpa	O
(	O
t	O
)	O
)	O
=	O
sigm	O
(	O
wt	O
t	O
hpa	O
(	O
t	O
)	O
)	O
.	O
(	O
a	O
dgm	O
in	O
which	O
the	O
cpds	O
are	O
logistic	B
regression	I
distributions	O
is	O
known	O
as	O
a	O
sigmoid	B
belief	I
net	I
(	O
neal	O
1992	O
)	O
.	O
)	O
however	O
,	O
since	O
the	O
parameters	O
of	O
this	O
model	O
were	O
created	O
by	O
hand	O
,	O
an	O
alternative	O
cpd	O
,	O
known	O
as	O
the	O
noisy-or	O
model	O
,	O
was	O
used	O
.	O
the	O
noisy-or	O
model	O
assumes	O
that	O
if	O
a	O
parent	O
is	O
on	O
,	O
then	O
the	O
child	O
will	O
usually	O
also	O
be	O
on	O
(	O
since	O
it	O
is	O
an	O
or-gate	O
)	O
,	O
but	O
occasionally	O
the	O
“	O
links	O
”	O
from	O
parents	B
to	O
child	O
may	O
fail	O
,	O
independently	O
at	O
random	O
.	O
in	O
this	O
case	O
,	O
even	O
if	O
the	O
parent	O
is	O
on	O
,	O
the	O
child	O
may	O
be	O
off	O
.	O
to	O
model	O
this	O
more	O
precisely	O
,	O
let	O
θst	O
=	O
1−	O
qst	O
be	O
the	O
probability	O
that	O
the	O
s	O
→	O
t	O
link	O
fails	O
,	O
so	O
qst	O
=	O
1−	O
θst	O
=	O
p	O
(	O
vt	O
=	O
314	O
chapter	O
10.	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
minvolset	O
disconnect	O
ventmach	O
pulm	O
embolus	O
intubation	O
venttube	O
kinked	O
tube	B
pap	O
shunt	O
fio2	O
press	O
ventlung	O
hypo	O
volemia	O
anaphy	O
laxis	O
stroke	O
volume	O
lvfailure	O
tpr	O
co	O
minvol	O
ventalv	O
pvsat	O
insuff	O
anesth	O
sao2	O
artco2	O
catechol	O
expco2	O
history	O
cvp	O
lved	O
volume	O
pcwp	O
bp	O
errlow	O
output	O
hrbp	O
hr	O
errcauter	O
hrsat	O
hrekg	O
(	O
a	O
)	O
(	O
cid:24	O
)	O
(	O
cid:26	O
)	O
(	O
cid:19	O
)	O
(	O
cid:3	O
)	O
(	O
cid:71	O
)	O
(	O
cid:76	O
)	O
(	O
cid:86	O
)	O
(	O
cid:72	O
)	O
(	O
cid:68	O
)	O
(	O
cid:86	O
)	O
(	O
cid:72	O
)	O
(	O
cid:86	O
)	O
(	O
cid:73	O
)	O
(	O
cid:79	O
)	O
(	O
cid:88	O
)	O
(	O
cid:75	O
)	O
(	O
cid:72	O
)	O
(	O
cid:68	O
)	O
(	O
cid:85	O
)	O
(	O
cid:87	O
)	O
(	O
cid:3	O
)	O
(	O
cid:71	O
)	O
(	O
cid:76	O
)	O
(	O
cid:86	O
)	O
(	O
cid:72	O
)	O
(	O
cid:68	O
)	O
(	O
cid:86	O
)	O
(	O
cid:72	O
)	O
(	O
cid:69	O
)	O
(	O
cid:82	O
)	O
(	O
cid:87	O
)	O
(	O
cid:88	O
)	O
(	O
cid:79	O
)	O
(	O
cid:76	O
)	O
(	O
cid:86	O
)	O
(	O
cid:80	O
)	O
(	O
cid:86	O
)	O
(	O
cid:72	O
)	O
(	O
cid:91	O
)	O
(	O
cid:32	O
)	O
(	O
cid:41	O
)	O
(	O
cid:58	O
)	O
(	O
cid:37	O
)	O
(	O
cid:38	O
)	O
(	O
cid:3	O
)	O
(	O
cid:70	O
)	O
(	O
cid:82	O
)	O
(	O
cid:88	O
)	O
(	O
cid:81	O
)	O
(	O
cid:87	O
)	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
(	O
cid:71	O
)	O
(	O
cid:82	O
)	O
(	O
cid:80	O
)	O
(	O
cid:72	O
)	O
(	O
cid:81	O
)	O
(	O
cid:3	O
)	O
(	O
cid:83	O
)	O
(	O
cid:68	O
)	O
(	O
cid:76	O
)	O
(	O
cid:81	O
)	O
(	O
cid:23	O
)	O
(	O
cid:19	O
)	O
(	O
cid:26	O
)	O
(	O
cid:24	O
)	O
(	O
cid:3	O
)	O
(	O
cid:86	O
)	O
(	O
cid:92	O
)	O
(	O
cid:80	O
)	O
(	O
cid:83	O
)	O
(	O
cid:87	O
)	O
(	O
cid:82	O
)	O
(	O
cid:80	O
)	O
(	O
cid:86	O
)	O
(	O
b	O
)	O
figure	O
10.5	O
network	O
.	O
(	O
a	O
)	O
the	O
alarm	B
network	I
.	O
figure	O
generated	O
by	O
visualizealarmnetwork	O
.	O
(	O
b	O
)	O
the	O
qmr	O
10.2.	O
examples	O
315	O
gp	O
gm	O
p	O
(	O
x	O
=	O
a	O
)	O
a	O
a	O
a	O
b	O
b	O
b	O
o	O
o	O
o	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
a	O
b	O
o	O
a	O
b	O
o	O
a	O
b	O
o	O
p	O
(	O
x	O
=	O
b	O
)	O
p	O
(	O
x	O
=	O
o	O
)	O
p	O
(	O
x	O
=	O
ab	O
)	O
0	O
0	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
table	O
10.2	O
cpt	O
which	O
encodes	O
a	O
mapping	O
from	O
genotype	B
to	O
phenotype	O
(	O
bloodtype	B
)	O
.	O
this	O
is	O
a	O
determin-	O
istic	O
,	O
but	O
many-to-one	O
,	O
mapping	O
.	O
1|hs	O
=	O
1	O
,	O
h−s	O
=	O
0	O
)	O
is	O
the	O
probability	O
that	O
s	O
can	O
activate	O
t	O
on	O
its	O
own	O
(	O
its	O
“	O
causal	O
power	O
”	O
)	O
.	O
the	O
only	O
way	O
for	O
the	O
child	O
to	O
be	O
off	O
is	O
if	O
all	O
the	O
links	O
from	O
all	O
parents	O
that	O
are	O
on	O
fail	O
independently	O
at	O
random	O
.	O
thus	O
p	O
(	O
vt	O
=	O
0|h	O
)	O
=	O
θi	O
(	O
hs=1	O
)	O
st	O
(	O
10.11	O
)	O
(	O
cid:27	O
)	O
s∈pa	O
(	O
t	O
)	O
obviously	O
,	O
p	O
(	O
vt	O
=	O
1|h	O
)	O
=	O
1	O
−	O
p	O
(	O
vt	O
=	O
0|h	O
)	O
.	O
if	O
we	O
observe	O
that	O
vt	O
=	O
1	O
but	O
all	O
its	O
parents	B
are	O
off	O
,	O
then	O
this	O
contradicts	O
the	O
model	O
.	O
such	O
a	O
data	O
case	O
would	O
get	O
probability	O
zero	O
under	O
the	O
model	O
,	O
which	O
is	O
problematic	O
,	O
because	O
it	O
is	O
possible	O
that	O
someone	O
exhibits	O
a	O
symptom	O
but	O
does	O
not	O
have	O
any	O
of	O
the	O
speciﬁed	O
diseases	O
.	O
to	O
handle	O
this	O
,	O
we	O
add	O
a	O
dummy	O
leak	O
node	O
h0	O
,	O
which	O
is	O
always	O
on	O
;	O
this	O
represents	O
“	O
all	O
other	O
causes	O
”	O
.	O
the	O
parameter	B
q0t	O
represents	O
the	O
probability	O
that	O
the	O
background	O
leak	O
can	O
cause	O
the	O
effect	O
on	O
its	O
own	O
.	O
the	O
modiﬁed	O
cpd	O
becomes	O
p	O
(	O
vt	O
=	O
0|h	O
)	O
=	O
θ0t	O
st	O
.	O
see	O
table	O
10.1	O
for	O
a	O
numerical	O
example	O
.	O
s∈pa	O
(	O
t	O
)	O
θhs	O
’	O
if	O
we	O
deﬁne	O
wst	O
(	O
cid:2	O
)	O
log	O
(	O
θst	O
)	O
,	O
we	O
can	O
rewrite	O
the	O
cpd	O
as	O
(	O
cid:11	O
)	O
(	O
cid:13	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
vt	O
=	O
1|h	O
)	O
=	O
1	O
−	O
exp	O
w0t	O
+	O
hswst	O
(	O
10.12	O
)	O
s	O
we	O
see	O
that	O
this	O
is	O
similar	B
to	O
a	O
logistic	B
regression	I
model	O
.	O
bipartite	O
models	O
with	O
noisy-or	O
cpds	O
are	O
called	O
bn2o	O
models	O
.	O
it	O
is	O
relatively	O
easy	O
to	O
set	O
the	O
θst	O
parameters	O
by	O
hand	O
,	O
based	O
on	O
domain	O
expertise	O
.	O
however	O
,	O
it	O
is	O
also	O
possible	O
to	O
learn	O
them	O
from	O
data	O
(	O
see	O
e.g	O
,	O
(	O
neal	O
1992	O
;	O
meek	O
and	O
heckerman	O
1997	O
)	O
)	O
.	O
noisy-or	O
cpds	O
have	O
also	O
proved	O
useful	O
in	O
modeling	O
human	O
causal	O
learning	O
(	O
griffiths	O
and	O
tenenbaum	O
2005	O
)	O
,	O
as	O
well	O
as	O
general	O
binary	B
classiﬁcation	I
settings	O
(	O
yuille	O
and	O
zheng	O
2009	O
)	O
.	O
10.2.4	O
genetic	B
linkage	I
analysis	I
*	O
another	O
important	O
(	O
and	O
historically	O
very	O
early	O
)	O
application	O
of	O
dgms	O
is	O
to	O
the	O
problem	O
of	O
genetic	B
linkage	I
analysis	I
.	O
we	O
start	O
with	O
a	O
pedigree	B
graph	I
,	O
which	O
is	O
a	O
dag	O
that	O
representing	O
the	O
relationship	O
between	O
parents	B
and	O
children	B
,	O
as	O
shown	O
in	O
figure	O
10.6	O
(	O
a	O
)	O
.	O
we	O
then	O
convert	O
this	O
to	O
a	O
dgm	O
,	O
as	O
we	O
explain	O
below	O
.	O
finally	O
we	O
perform	O
probabilistic	B
inference	I
in	O
the	O
resulting	O
model	O
.	O
316	O
chapter	O
10.	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:24	O
)	O
(	O
cid:23	O
)	O
(	O
cid:25	O
)	O
(	O
cid:47	O
)	O
(	O
cid:82	O
)	O
(	O
cid:70	O
)	O
(	O
cid:88	O
)	O
(	O
cid:86	O
)	O
(	O
cid:3	O
)	O
(	O
cid:6	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:25	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:24	O
)	O
(	O
cid:47	O
)	O
(	O
cid:82	O
)	O
(	O
cid:70	O
)	O
(	O
cid:88	O
)	O
(	O
cid:86	O
)	O
(	O
cid:3	O
)	O
(	O
cid:6	O
)	O
(	O
cid:3	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:21	O
)	O
(	O
cid:25	O
)	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:24	O
)	O
family	B
tree	O
,	O
circles	O
are	O
females	O
,	O
squares	O
are	O
males	O
.	O
figure	O
10.6	O
left	O
:	O
individuals	O
with	O
the	O
disease	O
of	O
interest	O
are	O
highlighted	O
.	O
right	O
:	O
dgm	O
for	O
two	O
loci	O
.	O
blue	O
nodes	B
xij	O
is	O
the	O
observed	O
phenotype	O
for	O
individual	O
i	O
at	O
locus	O
j.	O
all	O
other	O
nodes	B
are	O
hidden	B
.	O
orange	O
nodes	B
gp/m	O
is	O
the	O
paternal/	O
maternal	O
allele	O
.	O
small	O
red	O
nodes	B
zp/m	O
are	O
the	O
paternal/	O
maternal	O
selection	O
switching	O
variables	O
.	O
these	O
are	O
linked	O
across	O
loci	O
,	O
ij	O
→	O
zp	O
ij	O
→	O
zm	O
zm	O
i	O
,	O
j+1	O
.	O
the	O
founder	O
(	O
root	B
)	O
nodes	B
do	O
not	O
have	O
any	O
parents	B
,	O
and	O
hence	O
do	O
no	O
need	O
switching	O
variables	O
.	O
based	O
on	O
figure	O
3	O
from	O
(	O
friedman	O
et	O
al	O
.	O
2000	O
)	O
.	O
ij	O
ijl	O
i	O
,	O
j+1	O
and	O
zp	O
10.2.	O
examples	O
317	O
in	O
more	O
detail	O
,	O
for	O
each	O
person	O
(	O
or	O
animal	O
)	O
i	O
and	O
location	O
or	O
locus	O
j	O
along	O
the	O
genome	B
,	O
we	O
the	O
observed	O
marker	O
xij	O
(	O
which	O
can	O
be	O
a	O
property	O
such	O
as	O
blood	O
type	O
,	O
create	O
three	O
nodes	B
:	O
or	O
just	O
a	O
fragment	O
of	O
dna	O
that	O
can	O
be	O
measured	O
)	O
,	O
and	O
two	O
hidden	B
alleles	O
,	O
gm	O
ij	O
,	O
one	O
inherited	O
from	O
i	O
’	O
s	O
mother	O
(	O
maternal	O
allele	O
)	O
and	O
the	O
other	O
from	O
i	O
’	O
s	O
father	O
(	O
paternal	O
allele	O
)	O
.	O
together	O
,	O
the	O
ordered	O
pair	O
gij	O
=	O
(	O
gm	O
ij	O
)	O
constitutes	O
i	O
’	O
s	O
hidden	B
genotype	O
at	O
locus	O
j.	O
ij	O
→	O
xij	O
arcs	O
representing	O
the	O
fact	O
that	O
genotypes	O
obviously	O
we	O
must	O
add	O
gm	O
cause	O
phenotypes	B
(	O
observed	O
manifestations	O
of	O
genotypes	O
)	O
.	O
the	O
cpd	O
p	O
(	O
xij|gm	O
ij	O
)	O
is	O
called	O
the	O
penetrance	B
model	I
.	O
as	O
a	O
very	O
simple	O
example	O
,	O
suppose	O
xij	O
∈	O
{	O
a	O
,	O
b	O
,	O
o	O
,	O
ab	O
}	O
represents	O
ij	O
∈	O
{	O
a	O
,	O
b	O
,	O
o	O
}	O
is	O
their	O
genotype	B
.	O
we	O
can	O
repre-	O
person	O
i	O
’	O
s	O
observed	O
bloodtype	O
,	O
and	O
gm	O
sent	O
the	O
penetrance	B
model	I
using	O
the	O
deterministic	O
cpd	O
shown	O
in	O
table	O
10.2.	O
for	O
example	O
,	O
a	O
dominates	B
o	O
,	O
so	O
if	O
a	O
person	O
has	O
genotype	B
ao	O
or	O
oa	O
,	O
their	O
phenotype	O
will	O
be	O
a.	O
ij	O
,	O
gp	O
ij	O
→	O
xij	O
and	O
gp	O
ij	O
and	O
gp	O
ij	O
,	O
gp	O
ij	O
,	O
gp	O
in	O
addition	O
,	O
we	O
add	O
arcs	O
from	O
i	O
’	O
s	O
mother	O
and	O
father	O
into	O
gij	O
,	O
reﬂecting	O
the	O
mendelian	O
inheritance	O
of	O
genetic	O
material	O
from	O
one	O
’	O
s	O
parents	B
.	O
more	O
precisely	O
,	O
let	O
mi	O
=	O
k	O
be	O
i	O
’	O
s	O
mother	O
.	O
then	O
gm	O
kj	O
,	O
that	O
is	O
,	O
i	O
’	O
s	O
maternal	O
allele	O
is	O
a	O
copy	O
of	O
one	O
of	O
its	O
mother	O
’	O
s	O
two	O
alleles	B
.	O
let	O
z	O
m	O
ij	O
be	O
a	O
hidden	B
variable	I
than	O
speciﬁes	O
the	O
choice	O
.	O
we	O
can	O
model	O
this	O
(	O
cid:26	O
)	O
using	O
the	O
following	O
cpd	O
,	O
known	O
as	O
the	O
inheritance	B
model	I
:	O
ij	O
could	O
either	O
be	O
equal	O
to	O
gm	O
kj	O
or	O
gp	O
i	O
(	O
gm	O
i	O
(	O
gm	O
ij	O
=	O
gm	O
kj	O
)	O
ij	O
=	O
gp	O
kj	O
)	O
if	O
z	O
m	O
if	O
z	O
m	O
ij	O
=	O
m	O
ij	O
=	O
p	O
(	O
10.13	O
)	O
p	O
(	O
gm	O
ij|gm	O
kj	O
,	O
gp	O
kj	O
,	O
z	O
m	O
ij	O
)	O
=	O
ij|gm	O
kj	O
,	O
z	O
p	O
kj	O
,	O
gp	O
we	O
can	O
deﬁne	O
p	O
(	O
gp	O
are	O
said	O
to	O
specify	O
the	O
phase	B
of	O
the	O
genotype	B
.	O
the	O
values	O
of	O
gp	O
the	O
haplotype	B
of	O
person	O
i	O
at	O
locus	O
j.1	O
ij	O
)	O
similarly	O
,	O
where	O
k	O
=	O
pi	O
is	O
i	O
’	O
s	O
father	O
.	O
the	O
values	O
of	O
the	O
zij	O
i	O
,	O
j	O
constitute	O
i	O
,	O
j	O
and	O
z	O
m	O
i	O
,	O
j	O
,	O
gm	O
i	O
,	O
j	O
,	O
z	O
p	O
next	O
,	O
we	O
need	O
to	O
specify	O
the	O
prior	O
for	O
the	O
root	B
nodes	O
,	O
p	O
(	O
gm	O
ij	O
)	O
.	O
this	O
is	O
called	O
the	O
founder	B
model	I
,	O
and	O
represents	O
the	O
overall	O
prevalence	B
of	O
difference	O
kinds	O
of	O
alleles	B
in	O
the	O
population	O
.	O
we	O
usually	O
assume	O
independence	O
between	O
the	O
loci	O
for	O
these	O
founder	O
alleles	O
.	O
ij	O
)	O
and	O
p	O
(	O
gp	O
finally	O
,	O
we	O
need	O
to	O
specify	O
priors	O
for	O
the	O
switch	O
variables	O
that	O
control	O
the	O
inheritance	O
process	O
.	O
these	O
variables	O
are	O
spatially	O
correlated	O
,	O
since	O
adjacent	O
sites	O
on	O
the	O
genome	B
are	O
typically	O
inherited	O
together	O
(	O
recombination	O
events	O
are	O
rare	O
)	O
.	O
we	O
can	O
model	O
this	O
by	O
imposing	O
a	O
two-state	O
markov	O
2	O
(	O
1	O
−	O
chain	O
on	O
the	O
z	O
’	O
s	O
,	O
where	O
the	O
probability	O
of	O
switching	O
state	B
at	O
locus	O
j	O
is	O
given	O
by	O
θj	O
=	O
1	O
e−2dj	O
)	O
,	O
where	O
dj	O
is	O
the	O
distance	O
between	O
loci	O
j	O
and	O
j	O
+	O
1.	O
this	O
is	O
called	O
the	O
recombination	B
model	I
.	O
the	O
resulting	O
dgm	O
is	O
shown	O
in	O
figure	O
10.6	O
(	O
b	O
)	O
:	O
it	O
is	O
a	O
series	O
of	O
replicated	O
pedigree	O
dags	O
,	O
augmented	O
with	O
switching	O
z	O
variables	O
,	O
which	O
are	O
linked	O
using	O
markov	O
chains	O
.	O
(	O
there	O
is	O
a	O
related	O
model	O
known	O
as	O
phylogenetic	O
hmm	O
(	O
siepel	O
and	O
haussler	O
2003	O
)	O
,	O
which	O
is	O
used	O
to	O
model	O
evolution	O
amongst	O
phylogenies	O
.	O
)	O
as	O
a	O
simpliﬁed	O
example	O
of	O
how	O
this	O
model	O
can	O
be	O
used	O
,	O
suppose	O
we	O
only	O
have	O
one	O
locus	O
,	O
corresponding	O
to	O
blood	O
type	O
.	O
for	O
brevity	O
,	O
we	O
will	O
drop	O
the	O
j	O
index	O
.	O
suppose	O
we	O
observe	O
xi	O
=	O
a.	O
then	O
there	O
are	O
3	O
possible	O
genotypes	O
:	O
gi	O
is	O
(	O
a	O
,	O
a	O
)	O
,	O
(	O
a	O
,	O
o	O
)	O
or	O
(	O
o	O
,	O
a	O
)	O
.	O
there	O
is	O
ambiguity	O
because	O
the	O
genotype	B
to	O
phenotype	O
mapping	O
is	O
many-to-one	O
.	O
we	O
want	O
to	O
reverse	O
this	O
mapping	O
.	O
this	O
is	O
known	O
as	O
an	O
inverse	B
problem	I
.	O
fortunately	O
,	O
we	O
can	O
use	O
the	O
blood	O
types	O
of	O
relatives	O
to	O
help	O
disambiguate	O
the	O
evidence	B
.	O
information	B
will	O
“	O
ﬂow	O
”	O
from	O
the	O
other	O
xi	O
(	O
cid:2	O
)	O
’	O
s	O
up	O
to	O
their	O
gi	O
(	O
cid:2	O
)	O
’	O
s	O
,	O
then	O
across	O
to	O
i	O
’	O
s	O
gi	O
via	O
the	O
pedigree	O
dag	O
.	O
thus	O
we	O
can	O
combine	O
our	O
local	B
evidence	I
p	O
(	O
xi|gi	O
)	O
1.	O
sometimes	O
the	O
observed	O
marker	O
is	O
equal	O
to	O
the	O
unphased	O
genotype	B
,	O
which	O
is	O
the	O
unordered	O
set	O
{	O
gp	O
ij	O
}	O
;	O
however	O
,	O
the	O
phased	O
or	O
hidden	B
genotype	O
is	O
not	O
directly	O
measurable	O
.	O
ij	O
,	O
gm	O
318	O
chapter	O
10.	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
with	O
an	O
informative	O
prior	O
,	O
p	O
(	O
gi|x−i	O
)	O
,	O
conditioned	O
on	O
the	O
other	O
data	O
,	O
to	O
get	O
a	O
less	O
entropic	O
local	O
posterior	O
,	O
p	O
(	O
gi|x	O
)	O
∝	O
p	O
(	O
xi|gi	O
)	O
p	O
(	O
gi|x−i	O
)	O
.	O
in	O
practice	O
,	O
the	O
model	O
is	O
used	O
to	O
try	O
to	O
determine	O
where	O
along	O
the	O
genome	B
a	O
given	O
disease-	O
causing	O
gene	O
is	O
assumed	O
to	O
lie	O
—	O
this	O
is	O
the	O
genetic	B
linkage	I
analysis	I
task	O
.	O
the	O
method	O
works	O
as	O
follows	O
.	O
first	O
,	O
suppose	O
all	O
the	O
parameters	O
of	O
the	O
model	O
,	O
including	O
the	O
distance	O
between	O
all	O
the	O
marker	B
loci	O
,	O
are	O
known	O
.	O
the	O
only	O
unknown	B
is	O
the	O
location	O
of	O
the	O
disease-causing	O
gene	O
.	O
if	O
there	O
are	O
l	O
marker	B
loci	O
,	O
we	O
construct	O
l	O
+	O
1	O
models	O
:	O
in	O
model	O
(	O
cid:6	O
)	O
,	O
we	O
postulate	O
that	O
the	O
disease	O
gene	O
comes	O
after	O
marker	B
(	O
cid:6	O
)	O
,	O
for	O
0	O
<	O
(	O
cid:6	O
)	O
<	O
l	O
+	O
1.	O
we	O
can	O
estimate	O
the	O
markov	O
switching	O
parameter	O
ˆθ	O
(	O
cid:14	O
)	O
,	O
and	O
hence	O
the	O
distance	O
d	O
(	O
cid:14	O
)	O
between	O
the	O
disease	O
gene	O
and	O
its	O
nearest	O
known	O
locus	O
.	O
we	O
measure	O
the	O
quality	O
of	O
that	O
model	O
using	O
its	O
likelihood	B
,	O
p	O
(	O
d|ˆθ	O
(	O
cid:14	O
)	O
)	O
.	O
we	O
then	O
can	O
then	O
pick	O
the	O
model	O
with	O
highest	O
likelihood	O
(	O
which	O
is	O
equivalent	O
to	O
the	O
map	O
model	O
under	O
a	O
uniform	O
prior	O
)	O
.	O
note	O
,	O
however	O
,	O
that	O
computing	O
the	O
likelihood	B
requires	O
marginalizing	B
out	I
all	O
the	O
hidden	B
z	O
and	O
g	O
variables	O
.	O
see	O
(	O
fishelson	O
and	O
geiger	O
2002	O
)	O
and	O
the	O
references	O
therein	O
for	O
some	O
exact	O
methods	O
for	O
this	O
task	O
;	O
these	O
are	O
based	O
on	O
the	O
variable	B
elimination	I
algorithm	O
,	O
which	O
we	O
discuss	O
in	O
section	O
20.3.	O
unfortunately	O
,	O
for	O
reasons	O
we	O
explain	O
in	O
section	O
20.5	O
,	O
exact	O
methods	O
can	O
be	O
computationally	O
intractable	O
if	O
the	O
number	O
of	O
individuals	O
and/or	O
loci	O
is	O
large	O
.	O
see	O
(	O
albers	O
et	O
al	O
.	O
2006	O
)	O
for	O
an	O
approximate	O
method	O
for	O
computing	O
the	O
likelihood	B
;	O
this	O
is	O
based	O
on	O
a	O
form	O
of	O
variational	B
inference	I
,	O
which	O
we	O
will	O
discuss	O
in	O
section	O
22.4.1	O
.	O
10.2.5	O
directed	B
gaussian	O
graphical	O
models	O
*	O
consider	O
a	O
dgm	O
where	O
all	O
the	O
variables	O
are	O
real-valued	O
,	O
and	O
all	O
the	O
cpds	O
have	O
the	O
following	O
form	O
:	O
p	O
(	O
xt|xpa	O
(	O
t	O
)	O
)	O
=	O
n	O
(	O
xt|μt	O
+	O
wt	O
t	O
xpa	O
(	O
t	O
)	O
,	O
σ2	O
t	O
)	O
(	O
10.14	O
)	O
this	O
is	O
called	O
a	O
linear	O
gaussian	O
cpd	O
.	O
as	O
we	O
show	O
below	O
,	O
multiplying	O
all	O
these	O
cpds	O
together	O
results	O
in	O
a	O
large	O
joint	O
gaussian	O
distribution	O
of	O
the	O
form	O
p	O
(	O
x	O
)	O
=n	O
(	O
x|μ	O
,	O
σ	O
)	O
.	O
this	O
is	O
called	O
a	O
directed	B
ggm	O
,	O
or	O
a	O
gaussian	O
bayes	O
net	O
.	O
we	O
now	O
explain	O
how	O
to	O
derive	O
μ	O
and	O
σ	O
from	O
the	O
cpd	O
parameters	O
,	O
following	O
(	O
shachter	O
and	O
kenley	O
1989	O
,	O
app	O
.	O
b	O
)	O
.	O
for	O
convenience	O
,	O
we	O
will	O
rewrite	O
the	O
cpds	O
in	O
the	O
following	O
form	O
:	O
(	O
cid:2	O
)	O
s∈pa	O
(	O
t	O
)	O
xt	O
=	O
μt	O
+	O
wts	O
(	O
xs	O
−	O
μs	O
)	O
+σ	O
tzt	O
(	O
10.15	O
)	O
where	O
zt	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
,	O
σt	O
is	O
the	O
conditional	O
standard	O
deviation	O
of	O
xt	O
given	O
its	O
parents	B
,	O
wts	O
is	O
the	O
strength	O
of	O
the	O
s	O
→	O
t	O
edge	O
,	O
and	O
μt	O
is	O
the	O
local	O
mean.2	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
global	O
mean	O
is	O
just	O
the	O
concatenation	O
of	O
the	O
local	O
means	O
,	O
μ	O
=	O
(	O
μ1	O
,	O
.	O
.	O
.	O
,	O
μd	O
)	O
.	O
we	O
now	O
derive	O
the	O
global	O
covariance	O
,	O
σ.	O
let	O
s	O
(	O
cid:2	O
)	O
diag	O
(	O
σ	O
)	O
be	O
a	O
diagonal	B
matrix	O
containing	O
the	O
standard	O
deviations	O
.	O
we	O
can	O
rewrite	O
equation	O
10.15	O
in	O
matrix-vector	O
form	O
as	O
follows	O
:	O
(	O
x	O
−	O
μ	O
)	O
=	O
w	O
(	O
x	O
−	O
μ	O
)	O
+sz	O
2.	O
if	O
we	O
do	O
not	O
subtract	O
off	O
the	O
parent	O
’	O
s	O
mean	B
(	O
i.e.	O
,	O
if	O
we	O
use	O
xt	O
=	O
μt	O
+	O
is	O
much	O
messier	O
,	O
as	O
can	O
be	O
seen	O
by	O
looking	O
at	O
(	O
bishop	O
2006b	O
,	O
p370	O
)	O
.	O
(	O
cid:2	O
)	O
(	O
10.16	O
)	O
s∈pa	O
(	O
t	O
)	O
wtsxs	O
+	O
σtzt	O
)	O
,	O
the	O
derivation	O
of	O
σ	O
10.3.	O
inference	B
now	O
let	O
e	O
be	O
a	O
vector	O
of	O
noise	O
terms	O
:	O
e	O
(	O
cid:2	O
)	O
sz	O
319	O
(	O
10.17	O
)	O
we	O
can	O
rearrange	O
this	O
to	O
get	O
e	O
=	O
(	O
i	O
−	O
w	O
)	O
(	O
x	O
−	O
μ	O
)	O
(	O
10.18	O
)	O
since	O
w	O
is	O
lower	O
triangular	O
(	O
because	O
wts	O
=	O
0	O
if	O
t	O
>	O
s	O
in	O
the	O
topological	B
ordering	I
)	O
,	O
we	O
have	O
that	O
i	O
−	O
w	O
is	O
lower	O
triangular	O
with	O
1s	O
on	O
the	O
diagonal	B
.	O
hence	O
⎛	O
⎜⎜⎜⎝	O
1−w21	O
1	O
−w32	O
−w31	O
⎞	O
⎟⎟⎟⎠	O
=	O
x1	O
−	O
μ1	O
x2	O
−	O
μ2	O
⎞	O
⎟⎟⎟⎠	O
⎛	O
⎜⎜⎜⎝	O
(	O
10.19	O
)	O
⎞	O
1	O
⎟⎟⎟⎟⎟⎠	O
.	O
.	O
.	O
...	O
xd	O
−	O
μd	O
⎛	O
⎜⎜⎜⎜⎜⎝	O
...	O
e1	O
e2	O
...	O
ed	O
−wd1	O
−wd2	O
.	O
.	O
.	O
−wd	O
,	O
d−1	O
1	O
since	O
i	O
−	O
w	O
is	O
always	O
invertible	O
,	O
we	O
can	O
write	O
x	O
−	O
μ	O
=	O
(	O
i	O
−	O
w	O
)	O
−1e	O
(	O
cid:2	O
)	O
ue	O
=	O
usz	O
where	O
we	O
deﬁned	O
u	O
=	O
(	O
i	O
−	O
w	O
)	O
decomposition	O
of	O
σ	O
,	O
as	O
we	O
now	O
show	O
:	O
σ	O
=	O
cov	O
[	O
x	O
]	O
=	O
cov	O
[	O
x	O
−	O
μ	O
]	O
=	O
cov	O
[	O
usz	O
]	O
=	O
us	O
cov	O
[	O
z	O
]	O
sut	O
=	O
us2ut	O
10.3	O
inference	B
(	O
10.20	O
)	O
−1	O
.	O
thus	O
the	O
regression	B
weights	O
correspond	O
to	O
a	O
cholesky	O
(	O
10.21	O
)	O
(	O
10.22	O
)	O
we	O
have	O
seen	O
that	O
graphical	O
models	O
provide	O
a	O
compact	O
way	O
to	O
deﬁne	O
joint	O
probability	O
distribu-	O
tions	O
.	O
given	O
such	O
a	O
joint	B
distribution	I
,	O
what	O
can	O
we	O
do	O
with	O
it	O
?	O
the	O
main	O
use	O
for	O
such	O
a	O
joint	B
distribution	I
is	O
to	O
perform	O
probabilistic	B
inference	I
.	O
this	O
refers	O
to	O
the	O
task	O
of	O
estimating	O
unknown	B
quantities	O
from	O
known	O
quantities	O
.	O
for	O
example	O
,	O
in	O
section	O
10.2.2	O
,	O
we	O
introduced	O
hmms	O
,	O
and	O
said	O
that	O
one	O
of	O
the	O
goals	O
is	O
to	O
estimate	O
the	O
hidden	B
states	O
(	O
e.g.	O
,	O
words	O
)	O
from	O
the	O
observations	O
(	O
e.g.	O
,	O
speech	O
signal	O
)	O
.	O
and	O
in	O
section	O
10.2.4	O
,	O
we	O
discussed	O
genetic	B
linkage	I
analysis	I
,	O
and	O
said	O
that	O
one	O
of	O
the	O
goals	O
is	O
to	O
estimate	O
the	O
likelihood	B
of	O
the	O
data	O
under	O
various	O
dags	O
,	O
corresponding	O
to	O
different	O
hypotheses	O
about	O
the	O
location	O
of	O
the	O
disease-causing	O
gene	O
.	O
in	O
general	O
,	O
we	O
can	O
pose	O
the	O
inference	B
problem	O
as	O
follows	O
.	O
suppose	O
we	O
have	O
a	O
set	O
of	O
correlated	O
random	O
variables	O
with	O
joint	B
distribution	I
p	O
(	O
x1	O
:	O
v	O
|θ	O
)	O
.	O
(	O
in	O
this	O
section	O
,	O
we	O
are	O
assuming	O
the	O
parameters	O
θ	O
of	O
the	O
model	O
are	O
known	O
.	O
we	O
discuss	O
how	O
to	O
learn	O
the	O
parameters	O
in	O
section	O
10.4	O
.	O
)	O
let	O
us	O
partition	O
this	O
vector	O
into	O
the	O
visible	B
variables	I
xv	O
,	O
which	O
are	O
observed	O
,	O
and	O
the	O
hidden	B
variables	I
,	O
xh	O
,	O
which	O
are	O
unobserved	O
.	O
inference	B
refers	O
to	O
computing	O
the	O
posterior	O
distribution	O
of	O
the	O
unknowns	O
given	O
the	O
knowns	O
:	O
p	O
(	O
xh	O
,	O
xv|θ	O
)	O
p	O
(	O
xh	O
,	O
xv|θ	O
)	O
p	O
(	O
xv|θ	O
)	O
x	O
(	O
cid:2	O
)	O
p	O
(	O
xh|xv	O
,	O
θ	O
)	O
=	O
h	O
,	O
xv|θ	O
)	O
p	O
(	O
x	O
(	O
cid:4	O
)	O
(	O
10.23	O
)	O
(	O
cid:10	O
)	O
=	O
h	O
essentially	O
we	O
are	O
conditioning	B
on	O
the	O
data	O
by	O
clamping	B
the	O
visible	B
variables	I
to	O
their	O
observed	O
values	O
,	O
xv	O
,	O
and	O
then	O
normalizing	O
,	O
to	O
go	O
from	O
p	O
(	O
xh	O
,	O
xv	O
)	O
to	O
p	O
(	O
xh|xv	O
)	O
.	O
the	O
normalization	O
constant	O
p	O
(	O
xv|θ	O
)	O
is	O
the	O
likelihood	B
of	O
the	O
data	O
,	O
also	O
called	O
the	O
probability	B
of	I
the	I
evidence	I
.	O
320	O
chapter	O
10.	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
sometimes	O
only	O
some	O
of	O
the	O
hidden	B
variables	I
are	O
of	O
interest	O
to	O
us	O
.	O
so	O
let	O
us	O
partition	O
the	O
hidden	B
variables	I
into	O
query	B
variables	I
,	O
xq	O
,	O
whose	O
value	O
we	O
wish	O
to	O
know	O
,	O
and	O
the	O
remaining	O
nuisance	B
variables	I
,	O
xn	O
,	O
which	O
we	O
are	O
not	O
interested	O
in	O
.	O
we	O
can	O
compute	O
what	O
we	O
are	O
interested	O
in	O
by	O
marginalizing	B
out	I
the	O
nuisance	B
variables	I
:	O
(	O
cid:2	O
)	O
p	O
(	O
xq|xv	O
,	O
θ	O
)	O
=	O
p	O
(	O
xq	O
,	O
xn|xv	O
,	O
θ	O
)	O
(	O
10.24	O
)	O
xn	O
in	O
section	O
4.3.1	O
,	O
we	O
saw	O
how	O
to	O
perform	O
all	O
these	O
operations	O
for	O
a	O
multivariate	O
gaussian	O
in	O
o	O
(	O
v	O
3	O
)	O
time	O
,	O
where	O
v	O
is	O
the	O
number	O
of	O
variables	O
.	O
what	O
if	O
we	O
have	O
discrete	O
random	O
variables	O
,	O
with	O
say	O
k	O
states	O
each	O
?	O
if	O
the	O
joint	B
distribution	I
is	O
represented	O
as	O
a	O
multi-dimensional	O
table	O
,	O
we	O
can	O
always	O
perform	O
these	O
operations	O
exactly	O
,	O
but	O
this	O
will	O
take	O
o	O
(	O
k	O
v	O
)	O
time	O
.	O
in	O
chapter	O
20	O
,	O
we	O
explain	O
how	O
to	O
exploit	O
the	O
factorization	O
encoded	O
by	O
the	O
gm	O
to	O
perform	O
these	O
operations	O
in	O
o	O
(	O
v	O
k	O
w+1	O
)	O
time	O
,	O
where	O
w	O
is	O
a	O
quantity	O
known	O
as	O
the	O
treewidth	B
of	O
the	O
graph	B
.	O
this	O
measures	O
if	O
the	O
graph	B
is	O
a	O
tree	B
(	O
or	O
a	O
chain	O
)	O
,	O
we	O
have	O
w	O
=	O
1	O
,	O
so	O
for	O
these	O
how	O
“	O
tree-like	O
”	O
the	O
graph	B
is	O
.	O
models	O
,	O
inference	B
takes	O
time	O
linear	O
in	O
the	O
number	O
of	O
nodes	B
.	O
unfortunately	O
,	O
for	O
more	O
general	O
graphs	O
,	O
exact	O
inference	B
can	O
take	O
time	O
exponential	O
in	O
the	O
number	O
of	O
nodes	B
,	O
as	O
we	O
explain	O
in	O
section	O
20.5.	O
we	O
will	O
therefore	O
examine	O
various	O
approximate	B
inference	I
schemes	O
later	O
in	O
the	O
book	O
.	O
10.4	O
learning	B
in	O
the	O
graphical	O
models	O
literature	O
,	O
it	O
is	O
common	O
to	O
distinguish	O
between	O
inference	B
and	O
learning	B
.	O
inference	B
means	O
computing	O
(	O
functions	O
of	O
)	O
p	O
(	O
xh|xv	O
,	O
θ	O
)	O
,	O
where	O
v	O
are	O
the	O
visible	B
nodes	I
,	O
h	O
are	O
the	O
hidden	B
nodes	I
,	O
and	O
θ	O
are	O
the	O
parameters	O
of	O
the	O
model	O
,	O
assumed	O
to	O
be	O
known	O
.	O
learning	B
usually	O
means	O
computing	O
a	O
map	O
estimate	O
of	O
the	O
parameters	O
given	O
data	O
:	O
n	O
(	O
cid:2	O
)	O
log	O
p	O
(	O
xi	O
,	O
v|θ	O
)	O
+	O
log	O
p	O
(	O
θ	O
)	O
ˆθ	O
=	O
argmax	O
θ	O
(	O
10.25	O
)	O
where	O
xi	O
,	O
v	O
are	O
the	O
visible	B
variables	I
in	O
case	O
i.	O
if	O
we	O
have	O
a	O
uniform	O
prior	O
,	O
p	O
(	O
θ	O
)	O
∝	O
1	O
,	O
this	O
reduces	O
to	O
the	O
mle	O
,	O
as	O
usual	O
.	O
i=1	O
if	O
we	O
adopt	O
a	O
bayesian	O
view	O
,	O
the	O
parameters	O
are	O
unknown	B
variables	O
and	O
should	O
also	O
be	O
inferred	O
.	O
thus	O
to	O
a	O
bayesian	O
,	O
there	O
is	O
no	O
distinction	O
between	O
inference	B
and	O
learning	B
.	O
in	O
fact	O
,	O
we	O
can	O
just	O
add	O
the	O
parameters	O
as	O
nodes	B
to	O
the	O
graph	B
,	O
condition	O
on	O
d	O
,	O
and	O
then	O
infer	O
the	O
values	O
of	O
all	O
the	O
nodes	B
.	O
(	O
we	O
discuss	O
this	O
in	O
more	O
detail	O
below	O
.	O
)	O
in	O
this	O
view	O
,	O
the	O
main	O
difference	O
between	O
hidden	B
variables	I
and	O
parameters	O
is	O
that	O
the	O
number	O
of	O
hidden	B
variables	I
grows	O
with	O
the	O
amount	O
of	O
training	O
data	O
(	O
since	O
there	O
is	O
usually	O
a	O
set	O
of	O
hidden	B
variables	I
for	O
each	O
observed	O
data	O
case	O
)	O
,	O
whereas	O
the	O
number	O
of	O
parameters	O
in	O
usually	O
ﬁxed	O
(	O
at	O
least	O
in	O
a	O
parametric	B
model	I
)	O
.	O
this	O
means	O
that	O
we	O
must	O
integrate	B
out	I
the	O
hidden	B
variables	I
to	O
avoid	O
overﬁtting	B
,	O
but	O
we	O
may	O
be	O
able	O
to	O
get	O
away	O
with	O
point	O
estimation	O
techniques	O
for	O
parameters	O
,	O
which	O
are	O
fewer	O
in	O
number	O
.	O
10.4.1	O
plate	O
notation	O
when	O
inferring	O
parameters	O
from	O
data	O
,	O
we	O
often	O
assume	O
the	O
data	O
is	O
iid	B
.	O
we	O
can	O
represent	O
this	O
assumption	O
explicitly	O
using	O
a	O
graphical	B
model	I
,	O
as	O
shown	O
in	O
figure	O
10.7	O
(	O
a	O
)	O
.	O
this	O
illustrates	O
the	O
10.4.	O
learning	B
321	O
θ	O
θ	O
x1	O
xn	O
xi	O
n	O
left	O
:	O
data	O
points	O
xi	O
are	O
conditionally	B
independent	I
given	O
θ.	O
right	O
:	O
plate	O
notation	O
.	O
this	O
figure	O
10.7	O
represents	O
the	O
same	O
model	O
as	O
the	O
one	O
on	O
the	O
left	O
,	O
except	O
the	O
repeated	O
xi	O
nodes	B
are	O
inside	O
a	O
box	O
,	O
known	O
as	O
a	O
plate	O
;	O
the	O
number	O
in	O
the	O
lower	O
right	O
hand	O
corner	O
,	O
n	O
,	O
speciﬁes	O
the	O
number	O
of	O
repetitions	O
of	O
the	O
xi	O
node	O
.	O
assumption	O
that	O
each	O
data	O
case	O
was	O
generated	O
independently	O
but	O
from	O
the	O
same	O
distribution	O
.	O
notice	O
that	O
the	O
data	O
cases	O
are	O
only	O
independent	O
conditional	O
on	O
the	O
parameters	O
θ	O
;	O
marginally	O
,	O
the	O
data	O
cases	O
are	O
dependent	O
.	O
nevertheless	O
,	O
we	O
can	O
see	O
that	O
,	O
in	O
this	O
example	O
,	O
the	O
order	O
in	O
which	O
the	O
data	O
cases	O
arrive	O
makes	O
no	O
difference	O
to	O
our	O
beliefs	O
about	O
θ	O
,	O
since	O
all	O
orderings	O
will	O
have	O
the	O
same	O
sufficient	B
statistics	I
.	O
hence	O
we	O
say	O
the	O
data	O
is	O
exchangeable	B
.	O
to	O
avoid	O
visual	O
clutter	O
,	O
it	O
is	O
common	O
to	O
use	O
a	O
form	O
of	O
syntactic	B
sugar	I
called	O
plates	B
:	O
we	O
simply	O
draw	O
a	O
little	O
box	O
around	O
the	O
repeated	O
variables	O
,	O
with	O
the	O
convention	O
that	O
nodes	B
within	O
the	O
box	O
will	O
get	O
repeated	O
when	O
the	O
model	O
is	O
unrolled	B
.	O
we	O
often	O
write	O
the	O
number	O
of	O
copies	O
or	O
repetitions	O
in	O
the	O
bottom	O
right	O
corner	O
of	O
the	O
box	O
.	O
see	O
figure	O
10.7	O
(	O
b	O
)	O
for	O
a	O
simple	O
example	O
.	O
the	O
corresponding	O
joint	B
distribution	I
has	O
the	O
form	O
(	O
cid:18	O
)	O
n	O
(	O
cid:27	O
)	O
(	O
cid:19	O
)	O
p	O
(	O
θ	O
,	O
d	O
)	O
=	O
p	O
(	O
θ	O
)	O
p	O
(	O
xi|θ	O
)	O
(	O
10.26	O
)	O
i=1	O
this	O
dgm	O
represents	O
the	O
ci	O
assumptions	O
behind	O
the	O
models	O
we	O
considered	O
in	O
chapter	O
5.	O
a	O
slightly	O
more	O
complex	O
example	O
is	O
shown	O
in	O
figure	O
10.8.	O
on	O
the	O
left	O
we	O
show	O
a	O
naive	O
bayes	O
classiﬁer	O
that	O
has	O
been	O
“	O
unrolled	B
”	O
for	O
d	O
features	B
,	O
but	O
uses	O
a	O
plate	O
to	O
represent	O
repetition	O
over	O
cases	O
i	O
=	O
1	O
:	O
n	O
.	O
the	O
version	O
on	O
the	O
right	O
shows	O
the	O
same	O
model	O
using	O
nested	B
plate	I
notation	O
.	O
when	O
a	O
variable	O
is	O
inside	O
two	O
plates	B
,	O
it	O
will	O
have	O
two	O
sub-indices	O
.	O
for	O
example	O
,	O
we	O
write	O
θjc	O
to	O
represent	O
the	O
parameter	B
for	O
feature	O
j	O
in	O
class-conditional	B
density	I
c.	O
note	O
that	O
plates	B
can	O
be	O
nested	O
or	O
crossing	O
.	O
notational	O
devices	O
for	O
modeling	O
more	O
complex	O
parameter	B
tying	I
patterns	O
can	O
be	O
devised	O
(	O
e.g.	O
,	O
(	O
heckerman	O
et	O
al	O
.	O
2004	O
)	O
)	O
,	O
but	O
these	O
are	O
not	O
widely	O
used	O
.	O
what	O
is	O
not	O
clear	O
from	O
the	O
ﬁgure	O
is	O
that	O
θjc	O
is	O
used	O
to	O
generate	O
xij	O
iff	B
yi	O
=	O
c	O
,	O
otherwise	O
it	O
is	O
ignored	O
.	O
this	O
is	O
an	O
example	O
of	O
context	B
speciﬁc	I
independence	I
,	O
since	O
the	O
ci	O
relationship	O
xij	O
⊥	O
θjc	O
only	O
holds	O
if	O
yi	O
(	O
cid:7	O
)	O
=	O
c.	O
322	O
chapter	O
10.	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
π	O
yi	O
.	O
.	O
.	O
.	O
.	O
.	O
(	O
a	O
)	O
xi1	O
θc1	O
xid	O
n	O
θcd	O
c	O
π	O
yi	O
xij	O
n	O
θjc	O
c	O
d	O
(	O
b	O
)	O
figure	O
10.8	O
naive	O
bayes	O
classiﬁer	O
as	O
a	O
dgm	O
.	O
(	O
a	O
)	O
with	O
single	O
plates	O
.	O
(	O
b	O
)	O
with	O
nested	O
plates	O
.	O
10.4.2	O
learning	B
from	O
complete	B
data	I
if	O
all	O
the	O
variables	O
are	O
fully	O
observed	O
in	O
each	O
case	O
,	O
so	O
there	O
is	O
no	O
missing	O
data	O
and	O
there	O
are	O
no	O
hidden	O
variables	O
,	O
we	O
say	O
the	O
data	O
is	O
complete	B
.	O
for	O
a	O
dgm	O
with	O
complete	B
data	I
,	O
the	O
likelihood	B
is	O
given	O
by	O
n	O
(	O
cid:27	O
)	O
i=1	O
n	O
(	O
cid:27	O
)	O
v	O
(	O
cid:27	O
)	O
v	O
(	O
cid:27	O
)	O
p	O
(	O
d|θ	O
)	O
=	O
p	O
(	O
xi|θ	O
)	O
=	O
p	O
(	O
xit|xi	O
,	O
pa	O
(	O
t	O
)	O
,	O
θt	O
)	O
=	O
p	O
(	O
dt|θt	O
)	O
(	O
10.27	O
)	O
where	O
dt	O
is	O
the	O
data	O
associated	O
with	O
node	O
t	O
and	O
its	O
parents	B
,	O
i.e.	O
,	O
the	O
t	O
’	O
th	O
family	B
.	O
this	O
is	O
a	O
product	O
of	O
terms	O
,	O
one	O
per	O
cpd	O
.	O
we	O
say	O
that	O
the	O
likelihood	B
decomposes	O
according	O
to	O
the	O
graph	B
structure	O
.	O
t=1	O
t=1	O
i=1	O
now	O
suppose	O
that	O
the	O
prior	O
factorizes	O
as	O
well	O
:	O
v	O
(	O
cid:27	O
)	O
t=1	O
p	O
(	O
θ	O
)	O
=	O
p	O
(	O
θt	O
)	O
then	O
clearly	O
the	O
posterior	O
also	O
factorizes	O
:	O
p	O
(	O
θ|d	O
)	O
∝	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
=	O
p	O
(	O
dt|θt	O
)	O
p	O
(	O
θt	O
)	O
v	O
(	O
cid:27	O
)	O
t=1	O
this	O
means	O
we	O
can	O
compute	O
the	O
posterior	O
of	O
each	O
cpd	O
independently	O
.	O
in	O
other	O
words	O
,	O
factored	O
prior	O
plus	O
factored	O
likelihood	B
implies	O
factored	O
posterior	O
(	O
10.28	O
)	O
(	O
10.29	O
)	O
(	O
10.30	O
)	O
let	O
us	O
consider	O
an	O
example	O
,	O
where	O
all	O
cpds	O
are	O
tabular	O
,	O
thus	O
extending	O
the	O
earlier	O
results	O
of	O
secion	O
3.5.1.2	O
,	O
where	O
discussed	O
bayesian	O
naive	O
bayes	O
.	O
we	O
have	O
a	O
separate	O
row	O
(	O
i.e.	O
,	O
a	O
separate	O
multinoulli	B
distribution	I
)	O
for	O
each	O
conditioning	B
case	I
,	O
i.e.	O
,	O
for	O
each	O
combination	O
of	O
parent	O
values	O
,	O
as	O
in	O
table	O
10.2.	O
formally	O
,	O
we	O
can	O
write	O
the	O
t	O
’	O
th	O
cpt	O
as	O
xt|xpa	O
(	O
t	O
)	O
=	O
c	O
∼	O
cat	O
(	O
θtc	O
)	O
,	O
where	O
θtck	O
(	O
cid:2	O
)	O
p	O
(	O
xt	O
=	O
k|xpa	O
(	O
t	O
)	O
=	O
c	O
)	O
,	O
for	O
k	O
=	O
1	O
:	O
kt	O
,	O
c	O
=	O
1	O
:	O
ct	O
and	O
t	O
=	O
1	O
:	O
t	O
.	O
here	O
kt	O
is	O
the	O
number	O
10.4.	O
learning	B
of	O
states	O
for	O
node	O
t	O
,	O
ct	O
(	O
cid:2	O
)	O
number	O
of	O
nodes	B
.	O
obviously	O
’	O
(	O
cid:10	O
)	O
s∈pa	O
(	O
t	O
)	O
ks	O
is	O
the	O
number	O
of	O
parent	O
combinations	O
,	O
and	O
t	O
is	O
the	O
k	O
θtck	O
=	O
1	O
for	O
each	O
row	O
of	O
each	O
cpt	O
.	O
let	O
us	O
put	O
a	O
separate	O
dirichlet	O
prior	O
on	O
each	O
row	O
of	O
each	O
cpt	O
,	O
i.e.	O
,	O
θtc	O
∼	O
dir	O
(	O
αtc	O
)	O
.	O
then	O
we	O
can	O
compute	O
the	O
posterior	O
by	O
simply	O
adding	O
the	O
pseudo	B
counts	I
to	O
the	O
empirical	O
counts	O
to	O
get	O
θtc|d	O
∼	O
dir	O
(	O
ntc	O
+	O
αtc	O
)	O
,	O
wheren	O
tck	O
is	O
the	O
number	O
of	O
times	O
that	O
node	O
t	O
is	O
in	O
state	B
k	O
while	O
its	O
parents	B
are	O
in	O
state	B
c	O
:	O
ntck	O
(	O
cid:2	O
)	O
i	O
(	O
xi	O
,	O
t	O
=	O
k	O
,	O
xi	O
,	O
pa	O
(	O
t	O
)	O
=	O
c	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
323	O
(	O
10.31	O
)	O
(	O
10.32	O
)	O
from	O
equation	O
2.77	O
,	O
the	O
mean	B
of	O
this	O
distribution	O
is	O
given	O
by	O
the	O
following	O
:	O
(	O
cid:10	O
)	O
θtck	O
=	O
ntck	O
+	O
αtck	O
k	O
(	O
cid:2	O
)	O
(	O
ntck	O
(	O
cid:2	O
)	O
+	O
αtck	O
(	O
cid:2	O
)	O
)	O
for	O
example	O
,	O
consider	O
the	O
dgm	O
in	O
figure	O
10.1	O
(	O
a	O
)	O
.	O
suppose	O
the	O
training	O
data	O
consists	O
of	O
the	O
following	O
5	O
cases	O
:	O
x1	O
0	O
0	O
1	O
0	O
0	O
x2	O
0	O
1	O
1	O
1	O
1	O
x3	O
1	O
1	O
0	O
1	O
1	O
x4	O
0	O
1	O
1	O
0	O
1	O
x5	O
0	O
1	O
0	O
0	O
0	O
below	O
we	O
list	O
all	O
the	O
sufficient	B
statistics	I
ntck	O
,	O
and	O
the	O
posterior	B
mean	I
parameters	O
θick	O
under	O
a	O
dirichlet	O
prior	O
with	O
αick	O
=	O
1	O
(	O
corresponding	O
to	O
add-one	B
smoothing	I
)	O
for	O
the	O
t	O
=	O
4	O
node	O
:	O
x2	O
0	O
1	O
0	O
1	O
x3	O
ntck=1	O
ntck=0	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
2	O
θtck=1	O
θtck=0	O
1/2	O
2/3	O
1/3	O
3/5	O
1/2	O
1/3	O
2/3	O
2/5	O
it	O
is	O
easy	O
to	O
show	O
that	O
the	O
mle	O
has	O
the	O
same	O
form	O
as	O
equation	O
10.32	O
,	O
except	O
without	O
the	O
αtck	O
terms	O
,	O
i.e.	O
,	O
ˆθtck	O
=	O
ntck	O
(	O
cid:10	O
)	O
k	O
(	O
cid:2	O
)	O
ntck	O
(	O
cid:2	O
)	O
(	O
10.33	O
)	O
of	O
course	O
,	O
the	O
mle	O
suffers	O
from	O
the	O
zero-count	O
problem	O
discussed	O
in	O
section	O
3.3.4.1	O
,	O
so	O
it	O
is	O
important	O
to	O
use	O
a	O
prior	O
to	O
regularize	O
the	O
estimation	O
problem	O
.	O
10.4.3	O
learning	B
with	O
missing	B
and/or	O
latent	B
variables	O
if	O
we	O
have	O
missing	B
data	I
and/or	O
hidden	B
variables	I
,	O
the	O
likelihood	B
no	O
longer	O
factorizes	O
,	O
and	O
indeed	O
it	O
is	O
no	O
longer	O
convex	B
,	O
as	O
we	O
explain	O
in	O
detail	O
in	O
section	O
11.3.	O
this	O
means	O
we	O
will	O
usually	O
can	O
only	O
compute	O
a	O
locally	O
optimal	O
ml	O
or	O
map	O
estimate	O
.	O
bayesian	O
inference	B
of	O
the	O
parameters	O
is	O
even	O
harder	O
.	O
we	O
discuss	O
suitable	O
approximate	B
inference	I
techniques	O
in	O
later	O
chapters	O
.	O
324	O
chapter	O
10.	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
10.5	O
conditional	B
independence	I
properties	O
of	O
dgms	O
we	O
say	O
that	O
g	O
is	O
an	O
i-map	O
(	O
independence	O
map	O
)	O
for	O
p	O
,	O
or	O
that	O
p	O
is	O
markov	O
wrt	O
g	O
,	O
at	O
the	O
heart	O
of	O
any	O
graphical	B
model	I
is	O
a	O
set	O
of	O
conditional	O
indepence	O
(	O
ci	O
)	O
assumptions	O
.	O
we	O
write	O
xa	O
⊥g	O
xb|xc	O
if	O
a	O
is	O
independent	O
of	O
b	O
given	O
c	O
in	O
the	O
graph	B
g	O
,	O
using	O
the	O
semantics	O
to	O
be	O
deﬁned	O
below	O
.	O
let	O
i	O
(	O
g	O
)	O
be	O
the	O
set	O
of	O
all	O
such	O
ci	O
statements	O
encoded	O
by	O
the	O
graph	B
.	O
iff	B
i	O
(	O
g	O
)	O
⊆	O
i	O
(	O
p	O
)	O
,	O
where	O
i	O
(	O
p	O
)	O
is	O
the	O
set	O
of	O
all	O
ci	O
statements	O
that	O
hold	O
for	O
distribution	O
p.	O
in	O
other	O
words	O
,	O
the	O
graph	B
is	O
an	O
i-map	O
if	O
it	O
does	O
not	O
make	O
any	O
assertions	O
of	O
ci	O
that	O
are	O
not	O
true	O
of	O
the	O
distribution	O
.	O
this	O
allows	O
us	O
to	O
use	O
the	O
graph	B
as	O
a	O
safe	O
proxy	O
for	O
p	O
when	O
reasoning	O
about	O
p	O
’	O
s	O
ci	O
properties	O
.	O
this	O
is	O
helpful	O
for	O
designing	O
algorithms	O
that	O
work	O
for	O
large	O
classes	O
of	O
distributions	O
,	O
regardless	O
of	O
their	O
speciﬁc	O
numerical	O
parameters	O
θ.	O
note	O
that	O
the	O
fully	O
connected	O
graph	B
is	O
an	O
i-map	O
of	O
all	O
distributions	O
,	O
since	O
it	O
makes	O
no	O
ci	O
assertions	O
at	O
all	O
(	O
since	O
it	O
is	O
not	O
missing	O
any	O
edges	B
)	O
.	O
we	O
therefore	O
say	O
g	O
is	O
a	O
minimal	B
i-map	O
of	O
p	O
if	O
g	O
is	O
an	O
i-map	O
of	O
p	O
,	O
and	O
if	O
there	O
is	O
no	O
g	O
(	O
cid:4	O
)	O
⊆	O
g	O
which	O
is	O
an	O
i-map	O
of	O
p.	O
it	O
remains	O
to	O
specify	O
how	O
to	O
determine	O
if	O
xa	O
⊥g	O
xb|xc	O
.	O
deriving	O
these	O
independencies	O
for	O
undirected	B
graphs	O
is	O
easy	O
(	O
see	O
section	O
19.2	O
)	O
,	O
but	O
the	O
dag	O
situation	O
is	O
somewhat	O
complicated	O
,	O
because	O
of	O
the	O
need	O
to	O
respect	O
the	O
orientation	O
of	O
the	O
directed	B
edges	O
.	O
we	O
give	O
the	O
details	O
below	O
.	O
10.5.1	O
d-separation	O
and	O
the	O
bayes	O
ball	O
algorithm	O
(	O
global	O
markov	O
properties	O
)	O
first	O
,	O
we	O
introduce	O
some	O
deﬁnitions	O
.	O
we	O
say	O
an	O
undirected	B
path	O
p	O
is	O
d-separated	B
by	O
a	O
set	O
of	O
nodes	B
e	O
(	O
containing	O
the	O
evidence	B
)	O
iff	B
at	O
least	O
one	O
of	O
the	O
following	O
conditions	O
hold	O
:	O
1.	O
p	O
contains	O
a	O
chain	O
,	O
s	O
→	O
m	O
→	O
t	O
or	O
s	O
←	O
m	O
←	O
t	O
,	O
wherem	O
∈	O
e	O
2.	O
p	O
contains	O
a	O
tent	O
or	O
fork	O
,	O
s	O
(	O
cid:10	O
)	O
m	O
(	O
cid:11	O
)	O
t	O
,	O
wherem	O
∈	O
e	O
3.	O
p	O
contains	O
a	O
collider	B
or	O
v-structure	B
,	O
s	O
(	O
cid:11	O
)	O
m	O
(	O
cid:10	O
)	O
t	O
,	O
where	O
m	O
is	O
not	O
in	O
e	O
and	O
nor	O
is	O
any	O
descendant	O
of	O
m.	O
next	O
,	O
we	O
say	O
that	O
a	O
set	O
of	O
nodes	B
a	O
is	O
d-separated	B
from	O
a	O
different	O
set	O
of	O
nodes	B
b	O
given	O
a	O
third	O
observed	O
set	O
e	O
iff	B
each	O
undirected	B
path	O
from	O
every	O
node	O
a	O
∈	O
a	O
to	O
every	O
node	O
b	O
∈	O
b	O
is	O
d-separated	B
by	O
e.	O
finally	O
,	O
we	O
deﬁne	O
the	O
ci	O
properties	O
of	O
a	O
dag	O
as	O
follows	O
:	O
xa	O
⊥g	O
xb|xe	O
⇐⇒	O
a	O
is	O
d-separated	B
from	O
b	O
given	O
e	O
the	O
bayes	O
ball	O
algorithm	O
(	O
shachter	O
1998	O
)	O
is	O
a	O
simple	O
way	O
to	O
see	O
if	O
a	O
is	O
d-separated	B
from	O
b	O
given	O
e	O
,	O
based	O
on	O
the	O
above	O
deﬁnition	O
.	O
the	O
idea	O
is	O
this	O
.	O
we	O
“	O
shade	O
”	O
all	O
nodes	O
in	O
e	O
,	O
indicating	O
that	O
they	O
are	O
observed	O
.	O
we	O
then	O
place	O
“	O
balls	O
”	O
at	O
each	O
node	O
in	O
a	O
,	O
let	O
them	O
“	O
bounce	O
around	O
”	O
according	O
to	O
some	O
rules	B
,	O
and	O
then	O
ask	O
if	O
any	O
of	O
the	O
balls	O
reach	O
any	O
of	O
the	O
nodes	B
in	O
b.	O
the	O
three	O
main	O
rules	B
are	O
shown	O
in	O
figure	O
10.9.	O
notice	O
that	O
balls	O
can	O
travel	O
opposite	O
to	O
edge	O
directions	O
.	O
we	O
see	O
that	O
a	O
ball	O
can	O
pass	O
through	O
a	O
chain	O
,	O
but	O
not	O
if	O
it	O
is	O
shaded	O
in	O
the	O
middle	O
.	O
similarly	O
,	O
a	O
ball	O
can	O
pass	O
through	O
a	O
fork	O
,	O
but	O
not	O
if	O
it	O
is	O
shaded	O
in	O
the	O
middle	O
.	O
however	O
,	O
a	O
ball	O
can	O
not	O
pass	O
through	O
a	O
v-structure	B
,	O
unless	O
it	O
is	O
shaded	O
in	O
the	O
middle	O
.	O
we	O
can	O
justify	O
the	O
3	O
rules	B
of	O
bayes	O
ball	O
as	O
follows	O
.	O
first	O
consider	O
a	O
chain	O
structure	O
x	O
→	O
y	O
→	O
(	O
10.34	O
)	O
z	O
,	O
which	O
encodes	O
p	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y|x	O
)	O
p	O
(	O
z|y	O
)	O
(	O
10.35	O
)	O
10.5.	O
conditional	B
independence	I
properties	O
of	O
dgms	O
325	O
y	O
x	O
z	O
(	O
b	O
)	O
z	O
x	O
y	O
z	O
x	O
z	O
y	O
(	O
c	O
)	O
x	O
(	O
a	O
)	O
y	O
(	O
e	O
)	O
x	O
y	O
(	O
d	O
)	O
x	O
z	O
z	O
y	O
(	O
f	O
)	O
figure	O
10.9	O
bayes	O
ball	O
rules	B
.	O
a	O
shaded	O
node	O
is	O
one	O
we	O
condition	O
on	O
.	O
if	O
there	O
is	O
an	O
arrow	O
hitting	O
a	O
bar	O
,	O
it	O
means	O
the	O
ball	O
can	O
not	O
pass	O
through	O
;	O
otherwise	O
the	O
ball	O
can	O
pass	O
through	O
.	O
based	O
on	O
(	O
jordan	O
2007	O
)	O
.	O
when	O
we	O
condition	O
on	O
y	O
,	O
arex	O
and	O
z	O
independent	O
?	O
we	O
have	O
p	O
(	O
x	O
,	O
z|y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y|x	O
)	O
p	O
(	O
z|y	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
p	O
(	O
z|y	O
)	O
=	O
(	O
10.36	O
)	O
and	O
therefore	O
x	O
⊥	O
z|y	O
.	O
so	O
observing	O
the	O
middle	O
node	O
of	O
chain	O
breaks	O
it	O
in	O
two	O
(	O
as	O
in	O
a	O
markov	O
chain	O
)	O
.	O
p	O
(	O
y	O
)	O
p	O
(	O
y	O
)	O
=	O
p	O
(	O
x|y	O
)	O
p	O
(	O
z|y	O
)	O
now	O
consider	O
the	O
tent	O
structure	O
x	O
←	O
y	O
→	O
z.	O
the	O
joint	O
is	O
p	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
=	O
p	O
(	O
y	O
)	O
p	O
(	O
x|y	O
)	O
p	O
(	O
z|y	O
)	O
(	O
10.37	O
)	O
326	O
chapter	O
10.	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
x	O
y	O
x	O
y	O
x	O
z	O
y	O
y	O
(	O
cid:14	O
)	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
10.10	O
(	O
a-b	O
)	O
bayes	O
ball	O
boundary	O
conditions	O
.	O
(	O
c	O
)	O
example	O
of	O
why	O
we	O
need	O
boundary	O
conditions	O
.	O
y	O
(	O
cid:2	O
)	O
is	O
an	O
observed	O
child	O
of	O
y	O
,	O
rendering	O
y	O
“	O
effectively	O
observed	O
”	O
,	O
so	O
the	O
ball	O
bounces	O
back	O
up	O
on	O
its	O
way	O
from	O
x	O
to	O
z.	O
when	O
we	O
condition	O
on	O
y	O
,	O
arex	O
and	O
z	O
independent	O
?	O
we	O
have	O
p	O
(	O
x	O
,	O
z|y	O
)	O
=	O
p	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
=	O
p	O
(	O
y	O
)	O
p	O
(	O
x|y	O
)	O
p	O
(	O
z|y	O
)	O
=	O
p	O
(	O
x|y	O
)	O
p	O
(	O
z|y	O
)	O
(	O
10.38	O
)	O
and	O
therefore	O
x	O
⊥	O
z|y	O
.	O
so	O
observing	O
a	O
root	B
node	O
separates	O
its	O
children	B
(	O
as	O
in	O
a	O
naive	O
bayes	O
classiﬁer	O
:	O
see	O
section	O
3.5	O
)	O
.	O
p	O
(	O
y	O
)	O
p	O
(	O
y	O
)	O
finally	O
consider	O
a	O
v-structure	B
x	O
→	O
y	O
←	O
z.	O
the	O
joint	O
is	O
p	O
(	O
x	O
,	O
y	O
,	O
z	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
z	O
)	O
p	O
(	O
y|x	O
,	O
z	O
)	O
when	O
we	O
condition	O
on	O
y	O
,	O
arex	O
and	O
z	O
independent	O
?	O
we	O
have	O
(	O
10.39	O
)	O
(	O
10.40	O
)	O
(	O
10.41	O
)	O
p	O
(	O
x	O
,	O
z|y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
z	O
)	O
p	O
(	O
y|x	O
,	O
z	O
)	O
p	O
(	O
y	O
)	O
p	O
(	O
x	O
,	O
z	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
z	O
)	O
so	O
x	O
(	O
cid:7	O
)	O
⊥	O
z|y	O
.	O
however	O
,	O
in	O
the	O
unconditional	O
distribution	O
,	O
we	O
have	O
so	O
we	O
see	O
that	O
x	O
and	O
z	O
are	O
marginally	B
independent	I
.	O
so	O
we	O
see	O
that	O
conditioning	B
on	O
a	O
common	O
child	O
at	O
the	O
bottom	O
of	O
a	O
v-structure	B
makes	O
its	O
parents	B
become	O
dependent	O
.	O
this	O
important	O
effect	O
is	O
called	O
explaining	B
away	I
,	O
inter-causal	B
reasoning	I
,	O
or	O
berkson	O
’	O
s	O
paradox	O
.	O
as	O
an	O
example	O
of	O
explaining	B
away	I
,	O
suppose	O
we	O
toss	O
two	O
coins	O
,	O
representing	O
the	O
binary	O
numbers	O
0	O
and	O
1	O
,	O
and	O
we	O
observe	O
the	O
“	O
sum	O
”	O
of	O
their	O
values	O
.	O
a	O
priori	O
,	O
the	O
coins	O
are	O
independent	O
,	O
but	O
once	O
we	O
observe	O
their	O
sum	O
,	O
they	O
become	O
coupled	O
(	O
e.g.	O
,	O
if	O
the	O
sum	O
is	O
1	O
,	O
and	O
the	O
ﬁrst	O
coin	O
is	O
0	O
,	O
then	O
we	O
know	O
the	O
second	O
coin	O
is	O
1	O
)	O
.	O
finally	O
,	O
bayes	O
ball	O
also	O
needs	O
the	O
“	O
boundary	O
conditions	O
”	O
shown	O
in	O
figure	O
10.10	O
(	O
a-b	O
)	O
.	O
to	O
understand	O
where	O
these	O
rules	B
come	O
from	O
,	O
consider	O
figure	O
10.10	O
(	O
c	O
)	O
.	O
suppose	O
y	O
(	O
cid:4	O
)	O
is	O
a	O
noise-free	O
copy	O
of	O
y	O
.	O
then	O
if	O
we	O
observe	O
y	O
(	O
cid:4	O
)	O
,	O
we	O
effectively	O
observe	O
y	O
as	O
well	O
,	O
so	O
the	O
parents	B
x	O
and	O
z	O
have	O
to	O
compete	O
to	O
explain	O
this	O
.	O
so	O
if	O
we	O
send	O
a	O
ball	O
down	O
x	O
→	O
y	O
→	O
y	O
(	O
cid:4	O
)	O
,	O
it	O
should	O
“	O
bounce	O
back	O
”	O
up	O
along	O
y	O
(	O
cid:4	O
)	O
→	O
y	O
→	O
z.	O
however	O
,	O
if	O
y	O
and	O
all	O
its	O
children	B
are	O
hidden	B
,	O
the	O
ball	O
does	O
not	O
bounce	O
back	O
.	O
10.5.	O
conditional	B
independence	I
properties	O
of	O
dgms	O
327	O
1	O
2	O
3	O
5	O
4	O
6	O
7	O
figure	O
10.11	O
a	O
dgm	O
.	O
for	O
example	O
,	O
in	O
figure	O
10.11	O
,	O
we	O
see	O
that	O
x2	O
⊥	O
x6|x5	O
,	O
since	O
the	O
2	O
→	O
5	O
→	O
6	O
path	B
is	O
blocked	O
by	O
x5	O
(	O
which	O
is	O
observed	O
)	O
,	O
the	O
2	O
→	O
4	O
→	O
7	O
→	O
6	O
path	B
is	O
blocked	O
by	O
x7	O
(	O
which	O
is	O
hidden	B
)	O
,	O
and	O
the	O
2	O
→	O
1	O
→	O
3	O
→	O
6	O
path	B
is	O
blocked	O
by	O
x1	O
(	O
which	O
is	O
hidden	B
)	O
.	O
however	O
,	O
we	O
also	O
see	O
that	O
x2	O
(	O
cid:7	O
)	O
⊥	O
x6|x5	O
,	O
x7	O
,	O
since	O
now	O
the	O
2	O
→	O
4	O
→	O
7	O
→	O
6	O
path	B
is	O
no	O
longer	O
blocked	O
by	O
x7	O
(	O
which	O
is	O
observed	O
)	O
.	O
exercise	O
10.2	O
gives	O
you	O
some	O
more	O
practice	O
in	O
determining	O
ci	O
relationships	O
for	O
dgms	O
.	O
10.5.2	O
other	O
markov	O
properties	O
of	O
dgms	O
from	O
the	O
d-separation	O
criterion	O
,	O
one	O
can	O
conclude	O
that	O
t	O
⊥	O
nd	O
(	O
t	O
)	O
\	O
pa	O
(	O
t	O
)	O
|pa	O
(	O
t	O
)	O
(	O
10.42	O
)	O
where	O
the	O
non-descendants	B
of	O
a	O
node	O
nd	O
(	O
t	O
)	O
are	O
all	O
the	O
nodes	B
except	O
for	O
its	O
descendants	B
,	O
nd	O
(	O
t	O
)	O
=v	O
\	O
{	O
t	O
∪	O
desc	O
(	O
t	O
)	O
}	O
.	O
equation	O
10.42	O
is	O
called	O
the	O
directed	O
local	O
markov	O
property	O
.	O
for	O
example	O
,	O
in	O
figure	O
10.11	O
,	O
we	O
have	O
nd	O
(	O
3	O
)	O
=	O
{	O
2	O
,	O
4	O
}	O
,	O
and	O
pa	O
(	O
3	O
)	O
=	O
1	O
,	O
so3	O
⊥	O
2	O
,	O
4|1	O
.	O
a	O
special	O
case	O
of	O
this	O
property	O
is	O
when	O
we	O
only	O
look	O
at	O
predecessors	O
of	O
a	O
node	O
according	O
to	O
some	O
topological	B
ordering	I
.	O
we	O
have	O
t	O
⊥	O
pred	O
(	O
t	O
)	O
\	O
pa	O
(	O
t	O
)	O
|pa	O
(	O
t	O
)	O
(	O
10.43	O
)	O
which	O
follows	O
since	O
pred	O
(	O
t	O
)	O
⊆	O
nd	O
(	O
t	O
)	O
.	O
this	O
is	O
called	O
the	O
ordered	O
markov	O
property	O
,	O
which	O
justiﬁes	O
equation	O
10.7.	O
for	O
example	O
,	O
in	O
figure	O
10.11	O
,	O
if	O
we	O
use	O
the	O
ordering	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
7.	O
we	O
ﬁnd	O
pred	O
(	O
3	O
)	O
=	O
{	O
1	O
,	O
2	O
}	O
and	O
pa	O
(	O
3	O
)	O
=	O
1	O
,	O
so3	O
⊥	O
2|1	O
.	O
we	O
have	O
now	O
described	O
three	O
markov	O
properties	O
for	O
dags	O
:	O
the	O
directed	B
global	O
markov	O
property	O
g	O
in	O
equation	O
10.34	O
,	O
the	O
ordered	O
markov	O
property	O
o	O
in	O
equation	O
10.43	O
,	O
and	O
the	O
directed	O
local	O
it	O
is	O
obvious	O
that	O
g	O
=⇒	O
l	O
=⇒	O
o.	O
what	O
is	O
less	O
markov	O
property	O
l	O
in	O
equation	O
10.42.	O
obvious	O
,	O
but	O
nevertheless	O
true	O
,	O
is	O
that	O
o	O
=⇒	O
l	O
=⇒	O
g	O
(	O
see	O
e.g.	O
,	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
for	O
the	O
proof	O
)	O
.	O
hence	O
all	O
these	O
properties	O
are	O
equivalent	O
.	O
furthermore	O
,	O
any	O
distribution	O
p	O
that	O
is	O
markov	O
wrt	O
g	O
can	O
be	O
factorized	O
as	O
in	O
equation	O
10.7	O
;	O
this	O
is	O
called	O
the	O
factorization	O
property	O
f.	O
it	O
is	O
obvious	O
that	O
o	O
=⇒	O
f	O
,	O
but	O
one	O
can	O
show	O
that	O
the	O
converse	O
also	O
holds	O
(	O
see	O
e.g.	O
,	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
for	O
the	O
proof	O
)	O
.	O
10.5.3	O
markov	O
blanket	O
and	O
full	B
conditionals	O
the	O
set	O
of	O
nodes	O
that	O
renders	O
a	O
node	O
t	O
conditionally	O
independent	O
of	O
all	O
the	O
other	O
nodes	B
in	O
the	O
graph	B
is	O
called	O
t	O
’	O
s	O
markov	O
blanket	O
;	O
we	O
will	O
denote	O
this	O
by	O
mb	O
(	O
t	O
)	O
.	O
one	O
can	O
show	O
that	O
the	O
markov	O
blanket	O
of	O
a	O
node	O
in	O
a	O
dgm	O
is	O
equal	O
to	O
the	O
parents	B
,	O
the	O
children	B
,	O
and	O
the	O
co-parents	B
,	O
328	O
chapter	O
10.	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
i.e.	O
,	O
other	O
nodes	B
who	O
are	O
also	O
parents	B
of	O
its	O
children	B
:	O
mb	O
(	O
t	O
)	O
(	O
cid:2	O
)	O
ch	O
(	O
t	O
)	O
∪	O
pa	O
(	O
t	O
)	O
∪	O
copa	O
(	O
t	O
)	O
for	O
example	O
,	O
in	O
figure	O
10.11	O
,	O
we	O
have	O
mb	O
(	O
5	O
)	O
=	O
{	O
6	O
,	O
7	O
}	O
∪	O
{	O
2	O
,	O
3	O
}	O
∪	O
{	O
4	O
}	O
=	O
{	O
2	O
,	O
3	O
,	O
4	O
,	O
6	O
,	O
7	O
}	O
(	O
10.44	O
)	O
(	O
10.45	O
)	O
where	O
4	O
is	O
a	O
co-parent	O
of	O
5	O
because	O
they	O
share	O
a	O
common	O
child	O
,	O
namely	O
7.	O
to	O
see	O
why	O
the	O
co-parents	B
are	O
in	O
the	O
markov	O
blanket	O
,	O
note	O
that	O
when	O
we	O
derive	O
p	O
(	O
xt|x−t	O
)	O
=	O
p	O
(	O
xt	O
,	O
x−t	O
)	O
/p	O
(	O
x−t	O
)	O
,	O
all	O
the	O
terms	O
that	O
do	O
not	O
involve	O
xt	O
will	O
cancel	O
out	O
between	O
numerator	O
and	O
denominator	O
,	O
so	O
we	O
are	O
left	O
with	O
a	O
product	O
of	O
cpds	O
which	O
contain	O
xt	O
in	O
their	O
scope	B
.	O
hence	O
p	O
(	O
xt|x−t	O
)	O
∝	O
p	O
(	O
xt|xpa	O
(	O
t	O
)	O
)	O
p	O
(	O
xs|xpa	O
(	O
s	O
)	O
)	O
(	O
10.46	O
)	O
(	O
cid:27	O
)	O
s∈ch	O
(	O
t	O
)	O
for	O
example	O
,	O
in	O
figure	O
10.11	O
we	O
have	O
p	O
(	O
x5|x−5	O
)	O
∝	O
p	O
(	O
x5|x2	O
,	O
x3	O
)	O
p	O
(	O
x6|x3	O
,	O
x5	O
)	O
p	O
(	O
x7|x4	O
,	O
x5	O
,	O
x6	O
)	O
(	O
10.47	O
)	O
the	O
resulting	O
expression	O
is	O
called	O
t	O
’	O
s	O
full	B
conditional	I
,	O
and	O
will	O
prove	O
to	O
be	O
important	O
when	O
we	O
study	O
gibbs	O
sampling	O
(	O
section	O
24.2	O
)	O
.	O
10.6	O
inﬂuence	O
(	O
decision	B
)	O
diagrams	O
*	O
we	O
can	O
represent	O
multi-stage	B
(	O
bayesian	O
)	O
decision	B
problems	O
by	O
using	O
a	O
graphical	O
notation	O
known	O
as	O
a	O
decision	B
diagram	I
or	O
an	O
inﬂuence	B
diagram	I
(	O
howard	O
and	O
matheson	O
1981	O
;	O
kjaerulff	O
and	O
madsen	O
2008	O
)	O
.	O
this	O
extends	O
directed	O
graphical	O
models	O
by	O
adding	O
decision	B
nodes	I
(	O
also	O
called	O
ac-	O
tion	O
nodes	B
)	O
,	O
represented	O
by	O
rectangles	O
,	O
and	O
utility	B
nodes	I
(	O
also	O
called	O
value	B
nodes	I
)	O
,	O
represented	O
by	O
diamonds	O
.	O
the	O
original	O
random	O
variables	O
are	O
called	O
chance	B
nodes	I
,	O
and	O
are	O
represented	O
by	O
ovals	O
,	O
as	O
usual	O
.	O
figure	O
10.12	O
(	O
a	O
)	O
gives	O
a	O
simple	O
example	O
,	O
illustrating	O
the	O
famous	O
oil	B
wild-catter	I
problem.3	O
in	O
this	O
problem	O
,	O
you	O
have	O
to	O
decide	O
whether	O
to	O
drill	O
an	O
oil	O
well	O
or	O
not	O
.	O
you	O
have	O
two	O
possible	O
actions	B
:	O
d	O
=	O
1	O
means	O
drill	O
,	O
d	O
=	O
0	O
means	O
don	O
’	O
t	O
drill	O
.	O
you	O
assume	O
there	O
are	O
3	O
states	O
of	O
nature	O
:	O
o	O
=	O
0	O
means	O
the	O
well	O
is	O
dry	O
,	O
o	O
=	O
1	O
means	O
it	O
is	O
wet	O
(	O
has	O
some	O
oil	O
)	O
,	O
and	O
o	O
=	O
2	O
means	O
it	O
is	O
soaking	O
(	O
has	O
a	O
lot	O
of	O
oil	O
)	O
.	O
suppose	O
your	O
prior	O
beliefs	O
are	O
p	O
(	O
o	O
)	O
=	O
[	O
0.5	O
,	O
0.3	O
,	O
0.2	O
]	O
.	O
finally	O
,	O
you	O
must	O
specify	O
the	O
utility	B
function	I
u	O
(	O
d	O
,	O
o	O
)	O
.	O
since	O
the	O
states	O
and	O
actions	B
are	O
discrete	B
,	O
we	O
can	O
represent	O
it	O
as	O
a	O
table	O
(	O
analogous	O
to	O
a	O
cpt	O
in	O
a	O
dgm	O
)	O
.	O
suppose	O
we	O
use	O
the	O
following	O
numbers	O
,	O
in	O
dollars	O
:	O
o	O
=	O
0	O
o	O
=	O
1	O
o	O
=	O
2	O
d	O
=	O
0	O
d	O
=	O
1	O
0	O
-70	O
0	O
50	O
0	O
200	O
we	O
see	O
that	O
if	O
you	O
don	O
’	O
t	O
drill	O
,	O
you	O
incur	O
no	O
costs	O
,	O
but	O
also	O
make	O
no	O
money	O
.	O
if	O
you	O
drill	O
a	O
dry	O
well	O
,	O
you	O
lose	O
$	O
70	O
;	O
if	O
you	O
drill	O
a	O
wet	O
well	O
,	O
you	O
gain	O
$	O
50	O
;	O
and	O
if	O
you	O
drill	O
a	O
soaking	O
well	O
,	O
you	O
gain	O
$	O
200	O
.	O
your	O
prior	O
expected	O
utility	O
if	O
you	O
drill	O
is	O
given	O
by	O
2	O
(	O
cid:2	O
)	O
eu	O
(	O
d	O
=	O
1	O
)	O
=	O
p	O
(	O
o	O
)	O
u	O
(	O
d	O
,	O
o	O
)	O
=	O
0.5	O
·	O
(	O
−70	O
)	O
+	O
0.3	O
·	O
50	O
+	O
0.2	O
·	O
200	O
=	O
20	O
(	O
10.48	O
)	O
o=0	O
3.	O
this	O
example	O
is	O
originally	O
from	O
(	O
raiffa	O
1968	O
)	O
.	O
our	O
presentation	O
is	O
based	O
on	O
some	O
notes	O
by	O
daphne	O
koller	O
.	O
10.6.	O
inﬂuence	O
(	O
decision	B
)	O
diagrams	O
*	O
329	O
oil	O
oil	O
sound	O
drill	O
drill	O
utility	O
(	O
a	O
)	O
test	O
utility	O
(	O
b	O
)	O
oil	O
sound	O
cost	O
drill	O
utility	O
(	O
c	O
)	O
figure	O
10.12	O
(	O
a	O
)	O
inﬂuence	B
diagram	I
for	O
basic	O
oil	O
wild	O
catter	O
problem	O
.	O
(	O
b	O
)	O
an	O
extension	B
in	O
which	O
we	O
have	O
an	O
information	B
arc	I
from	O
the	O
sound	O
chance	O
node	O
to	O
the	O
drill	O
decision	B
node	O
.	O
(	O
c	O
)	O
an	O
extension	B
in	O
which	O
we	O
get	O
to	O
decide	O
whether	O
to	O
perform	O
the	O
test	O
or	O
not	O
.	O
your	O
expected	O
utility	O
if	O
you	O
don	O
’	O
t	O
drill	O
is	O
0.	O
so	O
your	O
maximum	O
expected	O
utility	O
is	O
m	O
eu	O
=	O
max	O
{	O
eu	O
(	O
d	O
=	O
0	O
)	O
,	O
eu	O
(	O
d	O
=	O
1	O
)	O
}	O
=	O
max	O
{	O
0	O
,	O
20	O
}	O
=	O
20	O
and	O
therefore	O
the	O
optimal	B
action	I
is	O
to	O
drill	O
:	O
d∗	O
=	O
arg	O
max	O
{	O
eu	O
(	O
d	O
=	O
0	O
)	O
,	O
eu	O
(	O
d	O
=	O
1	O
)	O
}	O
=	O
1	O
(	O
10.49	O
)	O
(	O
10.50	O
)	O
now	O
let	O
us	O
consider	O
a	O
slight	O
extension	B
to	O
the	O
model	O
.	O
suppose	O
you	O
perform	O
a	O
sounding	O
to	O
estimate	O
the	O
state	B
of	O
the	O
well	O
.	O
the	O
sounding	O
observation	B
can	O
be	O
in	O
one	O
of	O
3	O
states	O
:	O
s	O
=	O
0	O
is	O
a	O
diffuse	O
reﬂection	O
pattern	B
,	O
suggesting	O
no	O
oil	O
;	O
s	O
=	O
1	O
is	O
an	O
open	O
reﬂection	O
pattern	B
,	O
suggesting	O
some	O
oil	O
;	O
and	O
s	O
=	O
2	O
is	O
a	O
closed	O
reﬂection	O
pattern	B
,	O
indicating	O
lots	O
of	O
oil	O
.	O
since	O
s	O
is	O
caused	O
by	O
o	O
,	O
we	O
add	O
an	O
o	O
→	O
s	O
arc	O
to	O
our	O
model	O
.	O
in	O
addition	O
,	O
we	O
assume	O
that	O
the	O
outcome	O
of	O
the	O
sounding	O
test	O
will	O
be	O
available	O
before	O
we	O
decide	O
whether	O
to	O
drill	O
or	O
not	O
;	O
hence	O
we	O
add	O
an	O
information	B
arc	I
from	O
s	O
to	O
d.	O
this	O
is	O
illustrated	O
in	O
figure	O
10.12	O
(	O
b	O
)	O
.	O
p	O
(	O
s|o	O
)	O
:	O
let	O
us	O
model	O
the	O
reliability	O
of	O
our	O
sensor	O
using	O
the	O
following	O
conditional	O
distribution	O
for	O
330	O
chapter	O
10.	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
s	O
=	O
0	O
0.6	O
0.3	O
0.1	O
s	O
=	O
1	O
0.3	O
0.4	O
0.4	O
s	O
=	O
2	O
0.1	O
0.3	O
0.5	O
o	O
=	O
0	O
o	O
=	O
1	O
o	O
=	O
2	O
suppose	O
we	O
do	O
the	O
sounding	O
test	O
and	O
we	O
observe	O
s	O
=	O
0.	O
the	O
posterior	O
over	O
the	O
oil	O
state	O
is	O
p	O
(	O
o|s	O
=	O
0	O
)	O
=	O
[	O
0.732	O
,	O
0.219	O
,	O
0.049	O
]	O
(	O
10.51	O
)	O
now	O
your	O
posterior	O
expected	O
utility	O
of	O
performing	O
action	B
d	O
is	O
eu	O
(	O
d|s	O
=	O
0	O
)	O
=	O
p	O
(	O
o|s	O
=	O
0	O
)	O
u	O
(	O
o	O
,	O
d	O
)	O
(	O
10.52	O
)	O
2	O
(	O
cid:2	O
)	O
o=0	O
if	O
d	O
=	O
1	O
,	O
this	O
gives	O
(	O
cid:2	O
)	O
s	O
(	O
cid:2	O
)	O
eu	O
(	O
d	O
=	O
1|s	O
=	O
0	O
)	O
=	O
0.732	O
×	O
(	O
−70	O
)	O
+	O
0.219	O
×	O
50	O
+	O
0.049	O
×	O
200	O
=	O
−30.5	O
(	O
10.53	O
)	O
however	O
,	O
if	O
d	O
=	O
0	O
,	O
then	O
eu	O
(	O
d	O
=	O
0|s	O
=	O
0	O
)	O
=	O
0	O
,	O
since	O
not	O
drilling	O
incurs	O
no	O
cost	O
.	O
so	O
if	O
we	O
observe	O
s	O
=	O
0	O
,	O
we	O
are	O
better	O
off	O
not	O
drilling	O
,	O
which	O
makes	O
sense	O
.	O
now	O
suppose	O
we	O
do	O
the	O
sounding	O
test	O
and	O
we	O
observe	O
s	O
=	O
1.	O
by	O
similar	B
reasoning	O
,	O
one	O
can	O
show	O
that	O
eu	O
(	O
d	O
=	O
1|s	O
=	O
1	O
)	O
=	O
32.9	O
,	O
which	O
is	O
higher	O
than	O
eu	O
(	O
d	O
=	O
0|s	O
=	O
1	O
)	O
=	O
0.	O
similarly	O
,	O
if	O
we	O
observe	O
s	O
=	O
2	O
,	O
we	O
have	O
eu	O
(	O
d	O
=	O
1|s	O
=	O
2	O
)	O
=	O
87.5	O
which	O
is	O
much	O
higher	O
than	O
eu	O
(	O
d	O
=	O
0|s	O
=	O
2	O
)	O
=	O
0.	O
hence	O
the	O
optimal	O
policy	O
d∗	O
if	O
s	O
=	O
0	O
,	O
choose	O
d∗	O
(	O
2	O
)	O
=	O
1	O
and	O
get	O
$	O
87.5	O
.	O
(	O
1	O
)	O
=	O
1	O
and	O
get	O
$	O
32.9	O
;	O
and	O
if	O
s	O
=	O
2	O
,	O
choose	O
d∗	O
(	O
0	O
)	O
=	O
0	O
and	O
get	O
$	O
0	O
;	O
if	O
s	O
=	O
1	O
,	O
choose	O
d∗	O
(	O
s	O
)	O
is	O
as	O
follows	O
:	O
you	O
can	O
compute	O
your	O
expected	B
proﬁt	I
or	O
maximum	O
expected	O
utility	O
as	O
follows	O
:	O
m	O
eu	O
=	O
p	O
(	O
s	O
)	O
eu	O
(	O
d∗	O
(	O
s	O
)	O
|s	O
)	O
(	O
10.54	O
)	O
this	O
is	O
the	O
expected	O
utility	O
given	O
possible	O
outcomes	O
of	O
the	O
sounding	O
test	O
,	O
assuming	O
you	O
act	O
optimally	O
given	O
the	O
outcome	O
.	O
the	O
prior	O
marginal	O
on	O
the	O
outcome	O
of	O
the	O
test	O
is	O
p	O
(	O
s	O
)	O
=	O
p	O
(	O
o	O
)	O
p	O
(	O
s|o	O
)	O
=	O
[	O
0.41	O
,	O
0.35	O
,	O
0.24	O
]	O
(	O
10.55	O
)	O
o	O
hence	O
your	O
maximum	O
expected	O
utility	O
is	O
m	O
eu	O
=	O
0.41	O
×	O
0	O
+	O
0.35	O
×	O
32.9	O
+	O
0.24	O
×	O
87.5	O
=	O
32.2	O
now	O
suppose	O
you	O
can	O
choose	O
whether	O
to	O
do	O
the	O
test	O
or	O
not	O
.	O
this	O
can	O
be	O
modelled	O
as	O
shown	O
in	O
figure	O
10.12	O
(	O
c	O
)	O
,	O
where	O
we	O
add	O
a	O
new	O
test	O
node	O
t	O
.	O
if	O
t	O
=	O
1	O
,	O
we	O
do	O
the	O
test	O
,	O
and	O
s	O
can	O
enter	O
1	O
of	O
3	O
states	O
,	O
determined	O
by	O
o	O
,	O
exactly	O
as	O
above	O
.	O
if	O
t	O
=	O
0	O
,	O
we	O
don	O
’	O
t	O
do	O
the	O
test	O
,	O
and	O
s	O
enters	O
a	O
special	O
unknown	B
state	O
.	O
there	O
is	O
also	O
some	O
cost	O
associated	O
with	O
performing	O
the	O
test	O
.	O
(	O
10.56	O
)	O
is	O
it	O
worth	O
doing	O
the	O
test	O
?	O
this	O
depends	O
on	O
how	O
much	O
our	O
meu	O
changes	O
if	O
we	O
know	O
the	O
if	O
you	O
don	O
’	O
t	O
do	O
the	O
test	O
,	O
we	O
have	O
m	O
eu	O
=	O
20	O
outcome	O
of	O
the	O
test	O
(	O
namely	O
the	O
state	B
of	O
s	O
)	O
.	O
if	O
you	O
do	O
the	O
test	O
,	O
you	O
have	O
m	O
eu	O
=	O
32.2	O
from	O
equation	O
10.56.	O
so	O
the	O
from	O
equation	O
10.49.	O
improvement	O
in	O
utility	O
if	O
you	O
do	O
the	O
test	O
(	O
and	O
act	O
optimally	O
on	O
its	O
outcome	O
)	O
is	O
$	O
12.2	O
.	O
this	O
is	O
10.6.	O
inﬂuence	O
(	O
decision	B
)	O
diagrams	O
*	O
331	O
xt	O
zt	O
at	O
at	O
xt+1	O
zt+1	O
xt	O
xt+1	O
rt	O
(	O
a	O
)	O
rt	O
(	O
b	O
)	O
figure	O
10.13	O
(	O
a	O
)	O
a	O
pomdp	O
,	O
shown	O
as	O
an	O
inﬂuence	B
diagram	I
.	O
zt	O
are	O
hidden	B
world	O
states	O
.	O
we	O
implicitly	O
make	O
the	O
no	B
forgetting	I
assumption	O
,	O
which	O
effectively	O
means	O
that	O
at	O
has	O
arrows	O
coming	O
into	O
it	O
from	O
all	O
previous	O
observations	O
,	O
x1	O
:	O
t.	O
(	O
b	O
)	O
an	O
mdp	O
,	O
shown	O
as	O
an	O
inﬂuence	B
diagram	I
.	O
called	O
the	O
value	B
of	I
perfect	I
information	I
(	O
vpi	O
)	O
.	O
so	O
we	O
should	O
do	O
the	O
test	O
as	O
long	O
as	O
it	O
costs	O
less	O
than	O
$	O
12.2	O
.	O
in	O
terms	O
of	O
graphical	O
models	O
,	O
the	O
vpi	O
of	O
a	O
variable	O
t	O
can	O
be	O
determined	O
by	O
computing	O
the	O
meu	O
for	O
the	O
base	O
inﬂuence	O
diagram	O
,	O
i	O
,	O
and	O
then	O
computing	O
the	O
meu	O
for	O
the	O
same	O
inﬂuence	B
diagram	I
where	O
we	O
add	O
information	B
arcs	O
from	O
t	O
to	O
the	O
action	B
nodes	I
,	O
and	O
then	O
computing	O
the	O
difference	O
.	O
in	O
other	O
words	O
,	O
vpi	O
=	O
meu	O
(	O
i	O
+	O
t	O
→	O
d	O
)	O
−	O
meu	O
(	O
i	O
)	O
(	O
10.57	O
)	O
where	O
d	O
is	O
the	O
decision	B
node	O
and	O
t	O
is	O
the	O
variable	O
we	O
are	O
measuring	O
.	O
it	O
is	O
possible	O
to	O
modify	O
the	O
variable	B
elimination	I
algorithm	O
(	O
section	O
20.3	O
)	O
so	O
that	O
it	O
computes	O
the	O
optimal	O
policy	O
given	O
an	O
inﬂuence	B
diagram	I
.	O
these	O
methods	O
essentially	O
work	O
backwards	O
from	O
the	O
ﬁnal	O
time-step	O
,	O
computing	O
the	O
optimal	O
decision	O
at	O
each	O
step	O
assuming	O
all	O
following	O
actions	B
are	O
chosen	O
optimally	O
.	O
see	O
e.g.	O
,	O
(	O
lauritzen	O
and	O
nilsson	O
2001	O
;	O
kjaerulff	O
and	O
madsen	O
2008	O
)	O
for	O
details	O
.	O
we	O
could	O
continue	O
to	O
extend	O
the	O
model	O
in	O
various	O
ways	O
.	O
for	O
example	O
,	O
we	O
could	O
imagine	O
a	O
dynamical	O
system	O
in	O
which	O
we	O
test	O
,	O
observe	O
outcomes	O
,	O
perform	O
actions	B
,	O
move	O
on	O
to	O
the	O
next	O
oil	O
well	O
,	O
and	O
continue	O
drilling	O
(	O
and	O
polluting	O
)	O
in	O
this	O
way	O
.	O
in	O
fact	O
,	O
many	O
problems	O
in	O
robotics	O
,	O
business	O
,	O
medicine	O
,	O
public	O
policy	B
,	O
etc	O
.	O
can	O
be	O
usefully	O
formulated	O
as	O
inﬂuence	O
diagrams	O
unrolled	B
over	O
time	O
(	O
raiffa	O
1968	O
;	O
lauritzen	O
and	O
nilsson	O
2001	O
;	O
kjaerulff	O
and	O
madsen	O
2008	O
)	O
.	O
a	O
generic	O
model	O
of	O
this	O
form	O
is	O
shown	O
in	O
figure	O
10.13	O
(	O
a	O
)	O
.	O
this	O
is	O
known	O
as	O
a	O
partially	O
observed	O
markov	O
decision	B
process	O
or	O
pomdp	O
(	O
pronounced	O
“	O
pom-d-p	O
”	O
)	O
.	O
this	O
is	O
basically	O
a	O
hidden	B
markov	O
model	O
(	O
section	O
17.3	O
)	O
augmented	O
with	O
action	B
and	O
reward	B
nodes	O
.	O
this	O
can	O
be	O
used	O
to	O
model	O
the	O
perception-action	B
cycle	O
that	O
all	O
intelligent	O
agents	O
use	O
(	O
see	O
e.g.	O
,	O
(	O
kaelbling	O
et	O
al	O
.	O
1998	O
)	O
for	O
details	O
)	O
.	O
a	O
special	O
case	O
of	O
a	O
pomdp	O
,	O
in	O
which	O
the	O
states	O
are	O
fully	O
observed	O
,	O
is	O
called	O
a	O
markov	O
decision	B
process	O
or	O
mdp	O
,	O
shown	O
in	O
figure	O
10.13	O
(	O
b	O
)	O
.	O
this	O
is	O
much	O
easier	O
to	O
solve	O
,	O
since	O
we	O
only	O
have	O
to	O
compute	O
a	O
mapping	O
from	O
observed	O
states	O
to	O
actions	B
.	O
this	O
can	O
be	O
solved	O
using	O
dynamic	B
programming	I
(	O
see	O
e.g.	O
,	O
(	O
sutton	O
and	O
barto	O
1998	O
)	O
for	O
details	O
)	O
.	O
in	O
the	O
pomdp	O
case	O
,	O
the	O
information	B
arc	I
from	O
xt	O
to	O
at	O
is	O
not	O
sufficient	O
to	O
uniquely	O
determine	O
332	O
chapter	O
10.	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
g	O
d	O
b	O
h	O
f	O
i	O
e	O
c	O
a	O
(	O
b	O
)	O
b	O
a	O
d	O
e	O
c	O
f	O
h	O
i	O
g	O
j	O
(	O
c	O
)	O
figure	O
10.14	O
some	O
dgms	O
.	O
(	O
a	O
)	O
the	O
best	O
action	B
,	O
since	O
the	O
state	B
is	O
not	O
fully	O
observed	O
.	O
instead	O
,	O
we	O
need	O
to	O
choose	O
actions	B
based	O
on	O
our	O
belief	B
state	I
,	O
p	O
(	O
zt|x1	O
:	O
t	O
,	O
a1	O
:	O
t	O
)	O
.	O
since	O
the	O
belief	B
updating	I
process	O
is	O
deterministic	O
(	O
see	O
section	O
17.4.2	O
)	O
,	O
we	O
can	O
compute	O
a	O
belief	B
state	I
mdp	O
.	O
for	O
details	O
on	O
to	O
compute	O
the	O
policies	O
for	O
such	O
models	O
,	O
see	O
e.g.	O
,	O
(	O
kaelbling	O
et	O
al	O
.	O
1998	O
;	O
spaan	O
and	O
vlassis	O
2005	O
)	O
.	O
exercises	O
exercise	O
10.1	O
marginalizing	O
a	O
node	O
in	O
a	O
dgm	O
(	O
source	O
:	O
koller	O
.	O
)	O
consider	O
the	O
dag	O
g	O
in	O
figure	O
10.14	O
(	O
a	O
)	O
.	O
assume	O
it	O
is	O
a	O
minimal	B
i-map	O
for	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
,	O
x	O
)	O
.	O
now	O
consider	O
marginalizing	B
out	I
x.	O
construct	O
a	O
new	O
dag	O
g	O
(	O
cid:2	O
)	O
which	O
is	O
a	O
minimal	B
i-map	O
for	O
p	O
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
)	O
.	O
specify	O
(	O
and	O
justify	O
)	O
which	O
extra	O
edges	B
need	O
to	O
be	O
added	O
.	O
exercise	O
10.2	O
bayes	O
ball	O
(	O
source	O
:	O
jordan	O
.	O
)	O
here	O
we	O
compute	O
some	O
global	O
independence	O
statements	O
from	O
some	O
directed	O
graphical	O
models	O
.	O
you	O
can	O
use	O
the	O
“	O
bayes	O
ball	O
”	O
algorithm	O
,	O
the	O
d-separation	O
criterion	O
,	O
or	O
the	O
method	O
of	O
converting	O
to	O
an	O
undirected	B
graph	O
(	O
all	O
should	O
give	O
the	O
same	O
results	O
)	O
.	O
a.	O
consider	O
the	O
dag	O
in	O
figure	O
10.14	O
(	O
b	O
)	O
.	O
list	O
all	O
variables	O
that	O
are	O
independent	O
of	O
a	O
given	O
evidence	B
on	O
b.	O
b.	O
consider	O
the	O
dag	O
in	O
figure	O
10.14	O
(	O
c	O
)	O
.	O
list	O
all	O
variables	O
that	O
are	O
independent	O
of	O
a	O
given	O
evidence	B
on	O
j.	O
exercise	O
10.3	O
markov	O
blanket	O
for	O
a	O
dgm	O
prove	O
that	O
the	O
full	B
conditional	I
for	O
node	O
i	O
in	O
a	O
dgm	O
is	O
given	O
by	O
p	O
(	O
yj|p	O
a	O
(	O
yj	O
)	O
)	O
p	O
(	O
xi|x−i	O
)	O
∝	O
p	O
(	O
xi|p	O
a	O
(	O
xi	O
)	O
)	O
(	O
cid:13	O
)	O
yj∈ch	O
(	O
xi	O
)	O
where	O
ch	O
(	O
xi	O
)	O
are	O
the	O
children	B
of	O
xi	O
and	O
p	O
a	O
(	O
yj	O
)	O
are	O
the	O
parents	B
of	O
yj	O
.	O
(	O
10.58	O
)	O
exercise	O
10.4	O
hidden	B
variables	I
in	O
dgms	O
consider	O
the	O
dgms	O
in	O
figure	O
11.1	O
which	O
both	O
deﬁne	O
p	O
(	O
x1:6	O
)	O
,	O
where	O
we	O
number	O
empty	O
nodes	B
left	O
to	O
right	O
,	O
top	O
to	O
bottom	O
.	O
the	O
graph	B
on	O
the	O
left	O
deﬁnes	O
the	O
joint	O
as	O
p	O
(	O
x1:6	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3	O
)	O
p	O
(	O
h	O
=	O
h|x1:3	O
)	O
p	O
(	O
x4|h	O
=	O
h	O
)	O
p	O
(	O
x5|h	O
=	O
h	O
)	O
p	O
(	O
x6|h	O
=	O
h	O
)	O
(	O
10.59	O
)	O
(	O
cid:12	O
)	O
h	O
10.6.	O
inﬂuence	O
(	O
decision	B
)	O
diagrams	O
*	O
333	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
10.15	O
(	O
a	O
)	O
weather	O
bn	O
.	O
(	O
b	O
)	O
fishing	O
bn	O
.	O
where	O
we	O
have	O
marginalized	O
over	O
the	O
hidden	B
variable	I
h.	O
the	O
graph	B
on	O
the	O
right	O
deﬁnes	O
the	O
joint	O
as	O
p	O
(	O
x1:6	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3	O
)	O
p	O
(	O
x4|x1:3	O
)	O
p	O
(	O
x5|x1:4	O
)	O
p	O
(	O
x6|x1:5	O
)	O
(	O
10.60	O
)	O
a.	O
b.	O
c.	O
(	O
5	O
points	O
)	O
assuming	O
all	O
nodes	O
(	O
including	O
h	O
)	O
are	O
binary	O
and	O
all	O
cpds	O
are	O
tabular	O
,	O
prove	O
that	O
the	O
model	O
on	O
the	O
left	O
has	O
17	O
free	O
parameters	O
.	O
(	O
5	O
points	O
)	O
assuming	O
all	O
nodes	O
are	O
binary	O
and	O
all	O
cpds	O
are	O
tabular	O
,	O
prove	O
that	O
the	O
model	O
on	O
the	O
right	O
has	O
59	O
free	O
parameters	O
.	O
(	O
5	O
points	O
)	O
suppose	O
we	O
have	O
a	O
data	O
set	O
d	O
=	O
x	O
n	O
1:6	O
for	O
n	O
=	O
1	O
:	O
n	O
,	O
where	O
we	O
observe	O
the	O
xs	O
but	O
not	O
h	O
,	O
and	O
we	O
want	O
to	O
estimate	O
the	O
parameters	O
of	O
the	O
cpds	O
using	O
maximum	O
likelihood	O
.	O
for	O
which	O
model	O
is	O
this	O
easier	O
?	O
explain	O
your	O
answer	O
.	O
exercise	O
10.5	O
bayes	O
nets	O
for	O
a	O
rainy	O
day	O
(	O
source	O
:	O
nando	O
de	O
freitas.	O
)	O
.	O
in	O
this	O
question	O
you	O
must	O
model	O
a	O
problem	O
with	O
4	O
binary	O
variables	O
:	O
g	O
=	O
”	O
gray	O
”	O
,	O
v	O
=	O
”	O
vancouver	O
”	O
,	O
r	O
=	O
”	O
rain	O
”	O
and	O
s	O
=	O
”	O
sad	O
”	O
.	O
consider	O
the	O
directed	B
graphical	I
model	I
describing	O
the	O
relation-	O
ship	O
between	O
these	O
variables	O
shown	O
in	O
figure	O
10.15	O
(	O
a	O
)	O
.	O
a.	O
write	O
down	O
an	O
expression	O
for	O
p	O
(	O
s	O
=	O
1|v	O
=	O
1	O
)	O
in	O
terms	O
of	O
α	O
,	O
β	O
,	O
γ	O
,	O
δ.	O
b.	O
write	O
down	O
an	O
expression	O
for	O
p	O
(	O
s	O
=	O
1|v	O
=	O
0	O
)	O
.	O
is	O
this	O
the	O
same	O
or	O
different	O
to	O
p	O
(	O
s	O
=	O
1|v	O
=	O
1	O
)	O
?	O
explain	O
why	O
.	O
c.	O
find	O
maximum	O
likelihood	O
estimates	O
of	O
α	O
,	O
β	O
,	O
γ	O
using	O
the	O
following	O
data	O
set	O
,	O
where	O
each	O
row	O
is	O
a	O
training	O
case	O
.	O
(	O
you	O
may	O
state	B
your	O
answers	O
without	O
proof	O
.	O
)	O
v	O
g	O
r	O
s	O
1	O
1	O
1	O
1	O
1	O
0	O
1	O
1	O
0	O
1	O
0	O
0	O
(	O
10.61	O
)	O
exercise	O
10.6	O
fishing	O
nets	O
(	O
source	O
:	O
the	O
following	O
variables	O
(	O
duda	O
et	O
al	O
.	O
2001	O
)	O
..	O
)	O
consider	O
the	O
bayes	O
net	O
shown	O
in	O
figure	O
10.15	O
(	O
b	O
)	O
.	O
here	O
,	O
the	O
nodes	B
represent	O
x1	O
∈	O
{	O
winter	O
,	O
spring	O
,	O
summer	O
,	O
autumn	O
}	O
,	O
x2	O
∈	O
{	O
salmon	O
,	O
sea	O
bass	O
}	O
x3	O
∈	O
{	O
light	O
,	O
medium	O
,	O
dark	O
}	O
,	O
x4	O
∈	O
{	O
wide	O
,	O
thin	O
}	O
(	O
10.62	O
)	O
(	O
10.63	O
)	O
334	O
chapter	O
10.	O
directed	O
graphical	O
models	O
(	O
bayes	O
nets	O
)	O
z1	O
z2	O
z3	O
z1	O
z2	O
z3	O
x1	O
x2	O
x3	O
x4	O
x5	O
x1	O
x2	O
x4	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
10.16	O
(	O
a	O
)	O
a	O
qmr-style	O
network	O
with	O
some	O
hidden	B
leaves	O
.	O
(	O
b	O
)	O
removing	O
the	O
barren	O
nodes	O
.	O
the	O
corresponding	O
conditional	B
probability	I
tables	I
are	O
(	O
cid:24	O
)	O
(	O
cid:3	O
)	O
(	O
cid:25	O
)	O
,	O
p	O
(	O
x2|x1	O
)	O
=	O
(	O
cid:3	O
)	O
(	O
cid:4	O
)	O
⎛	O
⎜⎜⎝	O
.9	O
.3	O
.4	O
.8	O
⎞	O
⎟⎟⎠	O
.1	O
.7	O
.6	O
.2	O
(	O
cid:4	O
)	O
(	O
10.64	O
)	O
p	O
(	O
x1	O
)	O
=	O
.25	O
.25	O
.25	O
.25	O
p	O
(	O
x3|x2	O
)	O
=	O
.33	O
.8	O
.33	O
.1	O
(	O
10.65	O
)	O
note	O
that	O
in	O
p	O
(	O
x4|x2	O
)	O
,	O
the	O
rows	O
represent	O
x2	O
and	O
the	O
columns	O
x4	O
(	O
so	O
each	O
row	O
sums	O
to	O
one	O
and	O
represents	O
the	O
child	O
of	O
the	O
cpd	O
)	O
.	O
thus	O
p	O
(	O
x4	O
=	O
thin|x2	O
=	O
sea	O
bass	O
)	O
=	O
0.05	O
,	O
p	O
(	O
x4	O
=	O
thin|x2	O
=	O
salmon	O
)	O
=	O
0.6	O
,	O
etc	O
.	O
answer	O
the	O
following	O
queries	O
.	O
you	O
may	O
use	O
matlab	O
or	O
do	O
it	O
by	O
hand	O
.	O
in	O
either	O
case	O
,	O
show	O
your	O
work	O
.	O
.34	O
.1	O
,	O
p	O
(	O
x4|x2	O
)	O
=	O
.4	O
.95	O
.6	O
.05	O
a.	O
suppose	O
the	O
ﬁsh	O
was	O
caught	O
on	O
december	O
20	O
—	O
the	O
end	O
of	O
autumn	O
and	O
the	O
beginning	O
of	O
winter	O
—	O
and	O
thus	O
let	O
p	O
(	O
x1	O
)	O
=	O
(	O
.5	O
,	O
0	O
,	O
0	O
,	O
.5	O
)	O
instead	O
of	O
the	O
above	O
prior	O
.	O
(	O
this	O
is	O
called	O
soft	O
evidence	O
,	O
since	O
we	O
do	O
not	O
know	O
the	O
exact	O
value	O
of	O
x1	O
,	O
but	O
we	O
have	O
a	O
distribution	O
over	O
it	O
.	O
)	O
suppose	O
the	O
lightness	O
has	O
not	O
been	O
measured	O
but	O
it	O
is	O
known	O
that	O
the	O
ﬁsh	O
is	O
thin	O
.	O
classify	O
the	O
ﬁsh	O
as	O
salmon	O
or	O
sea	O
bass	O
.	O
b.	O
suppose	O
all	O
we	O
know	O
is	O
that	O
the	O
ﬁsh	O
is	O
thin	O
and	O
medium	O
lightness	O
.	O
what	O
season	O
is	O
it	O
now	O
,	O
most	O
likely	O
?	O
(	O
cid:24	O
)	O
use	O
p	O
(	O
x1	O
)	O
=	O
.25	O
.25	O
.25	O
.25	O
(	O
cid:25	O
)	O
exercise	O
10.7	O
removing	O
leaves	B
in	O
bn20	O
networks	O
a.	O
consider	O
the	O
qmr	O
network	O
,	O
where	O
only	O
some	O
of	O
the	O
symtpoms	O
are	O
observed	O
.	O
for	O
example	O
,	O
in	O
fig-	O
ure	O
10.16	O
(	O
a	O
)	O
,	O
x4	O
and	O
x5	O
are	O
hidden	B
.	O
show	O
that	O
we	O
can	O
safely	O
remove	O
all	O
the	O
hidden	B
leaf	O
nodes	B
without	O
affecting	O
the	O
posterior	O
over	O
the	O
disease	O
nodes	O
,	O
i.e.	O
,	O
prove	O
that	O
we	O
can	O
compute	O
p	O
(	O
z1:3|x1	O
,	O
x2	O
,	O
x4	O
)	O
using	O
the	O
network	O
in	O
figure	O
10.16	O
(	O
b	O
)	O
.	O
this	O
is	O
called	O
barren	B
node	I
removal	I
,	O
and	O
can	O
be	O
applied	O
to	O
any	O
dgm	O
.	O
b.	O
now	O
suppose	O
we	O
partition	O
the	O
leaves	B
into	O
three	O
groups	O
:	O
on	O
,	O
off	O
and	O
unknown	B
.	O
clearly	O
we	O
can	O
remove	O
the	O
unknown	B
leaves	O
,	O
since	O
they	O
are	O
hidden	B
and	O
do	O
not	O
affect	O
their	O
parents	B
.	O
show	O
that	O
we	O
can	O
analytically	O
remove	O
the	O
leaves	B
that	O
are	O
in	O
the	O
“	O
off	O
state	B
”	O
,	O
by	O
absorbing	O
their	O
effect	O
into	O
the	O
prior	O
of	O
the	O
parents	B
.	O
(	O
this	O
trick	O
only	O
works	O
for	O
noisy-or	O
cpds	O
.	O
)	O
exercise	O
10.8	O
handling	O
negative	O
ﬁndings	O
in	O
the	O
qmr	O
network	O
consider	O
the	O
qmr	O
network	O
.	O
let	O
d	O
be	O
the	O
hidden	B
diseases	O
,	O
f−	O
be	O
the	O
negative	O
ﬁndings	O
(	O
leaf	B
nodes	O
that	O
are	O
be	O
the	O
positive	O
ﬁndings	O
(	O
leaf	B
nodes	O
that	O
are	O
on	O
)	O
.	O
we	O
can	O
compute	O
the	O
posterior	O
p	O
(	O
d|f	O
,	O
f	O
+	O
)	O
in	O
off	O
)	O
,	O
and	O
f−	O
)	O
∝	O
p	O
(	O
d	O
)	O
p	O
(	O
f−|d	O
)	O
,	O
then	O
absorb	O
the	O
positive	O
ﬁndings	O
,	O
two	O
steps	O
:	O
ﬁrst	O
absorb	O
the	O
negative	O
ﬁndings	O
,	O
p	O
(	O
d|f−	O
p	O
(	O
d|f−	O
,	O
f	O
+	O
)	O
∝	O
p	O
(	O
d|f−	O
)	O
p	O
(	O
f	O
+|d	O
)	O
.	O
show	O
that	O
the	O
ﬁrst	O
step	O
can	O
be	O
done	O
in	O
o	O
(	O
|d||f−|	O
)	O
time	O
,	O
where	O
|d|	O
is	O
the	O
number	O
of	O
dieases	O
and	O
|f−|	O
is	O
the	O
number	O
of	O
negative	O
ﬁndings	O
.	O
for	O
simplicity	O
,	O
you	O
can	O
ignore	O
leak	O
nodes	O
.	O
(	O
intuitively	O
,	O
the	O
reason	O
for	O
this	O
is	O
that	O
there	O
is	O
no	O
correlation	O
induced	O
amongst	O
the	O
parents	B
when	O
the	O
ﬁnding	O
is	O
off	O
,	O
since	O
there	O
is	O
no	O
explaining	O
away	O
.	O
)	O
10.6.	O
inﬂuence	O
(	O
decision	B
)	O
diagrams	O
*	O
335	O
exercise	O
10.9	O
moralization	B
does	O
not	O
introduce	O
new	O
independence	O
statements	O
recall	B
that	O
the	O
process	O
of	O
moralizing	O
a	O
dag	O
means	O
connecting	O
together	O
all	O
“	O
unmarried	O
”	O
parents	B
that	O
share	O
a	O
common	O
child	O
,	O
and	O
then	O
dropping	O
all	O
the	O
arrows	O
.	O
let	O
m	O
be	O
the	O
moralization	B
of	O
dag	O
g.	O
show	O
that	O
ci	O
(	O
m	O
)	O
⊆	O
ci	O
(	O
g	O
)	O
,	O
where	O
ci	O
are	O
the	O
set	O
of	O
conditional	B
independence	I
statements	O
implied	O
by	O
the	O
model	O
.	O
11	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
11.1	O
latent	B
variable	I
models	I
in	O
chapter	O
10	O
we	O
showed	O
how	O
graphical	O
models	O
can	O
be	O
used	O
to	O
deﬁne	O
high-dimensional	O
joint	O
probability	O
distributions	O
.	O
the	O
basic	O
idea	O
is	O
to	O
model	O
dependence	O
between	O
two	O
variables	O
by	O
adding	O
an	O
edge	O
between	O
them	O
in	O
the	O
graph	B
.	O
(	O
technically	O
the	O
graph	B
represents	O
conditional	B
independence	I
,	O
but	O
you	O
get	O
the	O
point	O
.	O
)	O
an	O
alternative	O
approach	O
is	O
to	O
assume	O
that	O
the	O
observed	O
variables	O
are	O
correlated	O
because	O
they	O
arise	O
from	O
a	O
hidden	B
common	O
“	O
cause	O
”	O
.	O
model	O
with	O
hidden	B
variables	I
are	O
also	O
known	O
as	O
latent	B
variable	I
models	I
or	O
lvms	O
.	O
as	O
we	O
will	O
see	O
in	O
this	O
chapter	O
,	O
such	O
models	O
are	O
harder	O
to	O
ﬁt	O
than	O
models	O
with	O
no	O
latent	O
variables	O
.	O
however	O
,	O
they	O
can	O
have	O
signiﬁcant	O
advantages	O
,	O
for	O
two	O
main	O
reasons	O
.	O
first	O
,	O
lvms	O
often	O
have	O
fewer	O
parameters	O
than	O
models	O
that	O
directly	O
represent	O
correlation	O
in	O
the	O
visible	B
space	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
11.1.	O
if	O
all	O
nodes	O
(	O
including	O
h	O
)	O
are	O
binary	O
and	O
all	O
cpds	O
are	O
tabular	O
,	O
the	O
model	O
on	O
the	O
left	O
has	O
17	O
free	O
parameters	O
,	O
whereas	O
the	O
model	O
on	O
the	O
right	O
has	O
59	O
free	O
parameters	O
.	O
second	O
,	O
the	O
hidden	B
variables	I
in	O
an	O
lvm	O
can	O
serve	O
as	O
a	O
bottleneck	B
,	O
which	O
computes	O
a	O
compressed	O
representation	O
of	O
the	O
data	O
.	O
this	O
forms	O
the	O
basis	O
of	O
unsupervised	B
learning	I
,	O
as	O
we	O
will	O
see	O
.	O
figure	O
11.2	O
illustrates	O
some	O
generic	O
lvm	O
structures	O
that	O
can	O
be	O
used	O
for	O
this	O
purpose	O
.	O
in	O
general	O
there	O
are	O
l	O
latent	B
variables	O
,	O
zi1	O
,	O
.	O
.	O
.	O
,	O
zil	O
,	O
and	O
d	O
visible	B
variables	I
,	O
xi1	O
,	O
.	O
.	O
.	O
,	O
xid	O
,	O
where	O
usually	O
d	O
(	O
cid:16	O
)	O
l.	O
if	O
we	O
have	O
l	O
>	O
1	O
,	O
there	O
are	O
many	O
latent	B
factors	I
contributing	O
to	O
each	O
if	O
l	O
=	O
1	O
,	O
we	O
we	O
only	O
have	O
a	O
single	O
latent	O
observation	B
,	O
so	O
we	O
have	O
a	O
many-to-many	O
mapping	O
.	O
in	O
this	O
case	O
,	O
zi	O
is	O
usually	O
discrete	B
,	O
and	O
we	O
have	O
a	O
one-to-many	O
mapping	O
.	O
we	O
can	O
variable	O
;	O
also	O
have	O
a	O
many-to-one	O
mapping	O
,	O
representing	O
different	O
competing	O
factors	B
or	O
causes	O
for	O
each	O
observed	O
variable	O
;	O
such	O
models	O
form	O
the	O
basis	O
of	O
probabilistic	B
matrix	I
factorization	I
,	O
discussed	O
in	O
section	O
27.6.2.	O
finally	O
,	O
we	O
can	O
have	O
a	O
one-to-one	O
mapping	O
,	O
which	O
can	O
be	O
represented	O
as	O
zi	O
→	O
xi	O
.	O
by	O
allowing	O
zi	O
and/or	O
xi	O
to	O
be	O
vector-valued	O
,	O
this	O
representation	O
can	O
subsume	O
all	O
the	O
others	O
.	O
depending	O
on	O
the	O
form	O
of	O
the	O
likelihood	B
p	O
(	O
xi|zi	O
)	O
and	O
the	O
prior	O
p	O
(	O
zi	O
)	O
,	O
we	O
can	O
generate	O
a	O
variety	O
of	O
different	O
models	O
,	O
as	O
summarized	O
in	O
table	O
11.1	O
.	O
11.2	O
mixture	B
models	O
the	O
simplest	O
form	O
of	O
lvm	O
is	O
when	O
zi	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
representing	O
a	O
discrete	B
latent	O
state	B
.	O
we	O
will	O
use	O
a	O
discrete	B
prior	O
for	O
this	O
,	O
p	O
(	O
zi	O
)	O
=	O
cat	O
(	O
π	O
)	O
.	O
for	O
the	O
likelihood	B
,	O
we	O
use	O
p	O
(	O
xi|zi	O
=	O
k	O
)	O
=	O
pk	O
(	O
xi	O
)	O
,	O
338	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
(	O
cid:43	O
)	O
(	O
cid:20	O
)	O
(	O
cid:26	O
)	O
(	O
cid:3	O
)	O
(	O
cid:83	O
)	O
(	O
cid:68	O
)	O
(	O
cid:85	O
)	O
(	O
cid:68	O
)	O
(	O
cid:80	O
)	O
(	O
cid:72	O
)	O
(	O
cid:87	O
)	O
(	O
cid:72	O
)	O
(	O
cid:85	O
)	O
(	O
cid:86	O
)	O
(	O
cid:24	O
)	O
(	O
cid:28	O
)	O
(	O
cid:3	O
)	O
(	O
cid:83	O
)	O
(	O
cid:68	O
)	O
(	O
cid:85	O
)	O
(	O
cid:68	O
)	O
(	O
cid:80	O
)	O
(	O
cid:72	O
)	O
(	O
cid:87	O
)	O
(	O
cid:72	O
)	O
(	O
cid:85	O
)	O
(	O
cid:86	O
)	O
figure	O
11.1	O
a	O
dgm	O
with	O
and	O
without	O
hidden	B
variables	I
.	O
the	O
leaves	B
represent	O
medical	O
symptoms	O
.	O
the	O
roots	O
represent	O
primary	O
causes	O
,	O
such	O
as	O
smoking	O
,	O
diet	O
and	O
exercise	O
.	O
the	O
hidden	B
variable	I
can	O
represent	O
mediating	O
factors	B
,	O
such	O
as	O
heart	O
disease	O
,	O
which	O
might	O
not	O
be	O
directly	O
visible	B
.	O
zi	O
zi1	O
.	O
.	O
.	O
zil	O
xi1	O
.	O
.	O
.	O
(	O
a	O
)	O
xid	O
xi1	O
xid	O
.	O
.	O
.	O
(	O
b	O
)	O
zi1	O
.	O
.	O
.	O
zil	O
xi	O
(	O
c	O
)	O
zi	O
xi	O
(	O
d	O
)	O
figure	O
11.2	O
a	O
latent	O
variable	O
model	O
represented	O
as	O
a	O
dgm	O
.	O
many-to-one	O
.	O
(	O
d	O
)	O
one-to-one	O
.	O
(	O
a	O
)	O
many-to-many	O
.	O
(	O
b	O
)	O
one-to-many	O
.	O
(	O
c	O
)	O
where	O
pk	O
is	O
the	O
k	O
’	O
th	O
base	B
distribution	I
for	O
the	O
observations	O
;	O
this	O
can	O
be	O
of	O
any	O
type	O
.	O
the	O
overall	O
model	O
is	O
known	O
as	O
a	O
mixture	B
model	I
,	O
since	O
we	O
are	O
mixing	O
together	O
the	O
k	O
base	O
distributions	O
as	O
follows	O
:	O
p	O
(	O
xi|θ	O
)	O
=	O
πkpk	O
(	O
xi|θ	O
)	O
(	O
11.1	O
)	O
k	O
(	O
cid:2	O
)	O
k=1	O
this	O
is	O
a	O
convex	B
combination	I
of	O
the	O
pk	O
’	O
s	O
,	O
since	O
we	O
are	O
taking	O
a	O
weighted	O
sum	O
,	O
where	O
the	O
mixing	B
weights	I
πk	O
satisfy	O
0	O
≤	O
πk	O
≤	O
1	O
and	O
k=1	O
πk	O
=	O
1.	O
we	O
give	O
some	O
examples	O
below	O
.	O
(	O
cid:10	O
)	O
k	O
11.2.	O
mixture	B
models	O
339	O
p	O
(	O
xi|zi	O
)	O
mvn	O
prod	O
.	O
discrete	B
prod	O
.	O
gaussian	O
prod	O
.	O
gaussian	O
prod	O
.	O
discrete	B
prod	O
.	O
discrete	B
prod	O
.	O
noisy-or	O
prod	O
.	O
bernoulli	O
name	O
mixture	O
of	O
gaussians	O
mixture	O
of	O
multinomials	O
factor	B
analysis/	O
probabilistic	O
pca	O
probabilistic	O
ica/	O
sparse	B
coding	I
p	O
(	O
zi	O
)	O
discrete	B
discrete	O
prod	O
.	O
gaussian	O
prod	O
.	O
laplace	O
prod	O
.	O
gaussian	O
multinomial	B
pca	O
dirichlet	O
prod	O
.	O
bernoulli	O
prod	O
.	O
bernoulli	O
latent	B
dirichlet	O
allocation	O
bn20/	O
qmr	O
sigmoid	B
belief	I
net	I
section	O
11.2.1	O
11.2.2	O
12.1.5	O
12.6	O
27.2.3	O
27.3	O
10.2.3	O
27.7	O
table	O
11.1	O
“	O
prod	O
.	O
discrete	B
”	O
in	O
the	O
likelihood	B
means	O
a	O
factored	O
distribution	O
of	O
the	O
form	O
gaussian	O
”	O
means	O
a	O
factored	O
distribution	O
of	O
the	O
form	O
analysis	O
”	O
.	O
“	O
ica	O
”	O
stands	O
for	O
“	O
indepedendent	O
components	O
analysis	O
”	O
.	O
summary	O
of	O
some	O
popular	O
directed	B
latent	O
variable	O
models	O
.	O
here	O
“	O
prod	O
”	O
means	O
product	O
,	O
so	O
j	O
cat	O
(	O
xij|zi	O
)	O
,	O
and	O
“	O
prod	O
.	O
j	O
n	O
(	O
xij|zi	O
)	O
.	O
“	O
pca	O
”	O
stands	O
for	O
“	O
principal	B
components	I
(	O
cid:14	O
)	O
(	O
cid:14	O
)	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
11.3	O
a	O
mixture	O
of	O
3	O
gaussians	O
in	O
2d	O
.	O
(	O
a	O
)	O
we	O
show	O
the	O
contours	O
of	O
constant	O
probability	O
for	O
each	O
component	O
in	O
the	O
mixture	B
.	O
(	O
b	O
)	O
a	O
surface	O
plot	O
of	O
the	O
overall	O
density	O
.	O
based	O
on	O
figure	O
2.23	O
of	O
(	O
bishop	O
2006a	O
)	O
.	O
figure	O
generated	O
by	O
mixgaussplotdemo	O
.	O
11.2.1	O
mixtures	O
of	O
gaussians	O
k	O
(	O
cid:2	O
)	O
the	O
most	O
widely	O
used	O
mixture	B
model	I
is	O
the	O
mixture	O
of	O
gaussians	O
(	O
mog	O
)	O
,	O
also	O
called	O
a	O
gaussian	O
mixture	B
model	I
or	O
gmm	O
.	O
in	O
this	O
model	O
,	O
each	O
base	B
distribution	I
in	O
the	O
mixture	B
is	O
a	O
multivariate	O
gaussian	O
with	O
mean	B
μk	O
and	O
covariance	B
matrix	I
σk	O
.	O
thus	O
the	O
model	O
has	O
the	O
form	O
p	O
(	O
xi|θ	O
)	O
=	O
πkn	O
(	O
xi|μk	O
,	O
σk	O
)	O
(	O
11.2	O
)	O
k=1	O
figure	O
11.3	O
shows	O
a	O
mixture	O
of	O
3	O
gaussians	O
in	O
2d	O
.	O
each	O
mixture	B
component	O
is	O
represented	O
by	O
a	O
different	O
set	O
of	O
eliptical	O
contours	O
.	O
given	O
a	O
sufficiently	O
large	O
number	O
of	O
mixture	B
components	O
,	O
a	O
gmm	O
can	O
be	O
used	O
to	O
approximate	O
any	O
density	O
deﬁned	O
on	O
r	O
d.	O
340	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
11.2.2	O
mixture	O
of	O
multinoullis	O
we	O
can	O
use	O
mixture	B
models	O
to	O
deﬁne	O
density	O
models	O
on	O
many	O
kinds	O
of	O
data	O
.	O
for	O
example	O
,	O
suppose	O
our	O
data	O
consist	O
of	O
d-dimensional	O
bit	O
vectors	O
.	O
in	O
this	O
case	O
,	O
an	O
appropriate	O
class-	O
conditional	O
density	O
is	O
a	O
product	O
of	O
bernoullis	O
:	O
d	O
(	O
cid:27	O
)	O
d	O
(	O
cid:27	O
)	O
p	O
(	O
xi|zi	O
=	O
k	O
,	O
θ	O
)	O
=	O
ber	O
(	O
xij|μjk	O
)	O
=	O
jk	O
(	O
1	O
−	O
μjk	O
)	O
1−xij	O
μxij	O
(	O
11.3	O
)	O
j=1	O
j=1	O
where	O
μjk	O
is	O
the	O
probability	O
that	O
bit	O
j	O
turns	O
on	O
in	O
cluster	O
k.	O
the	O
latent	B
variables	O
do	O
not	O
have	O
to	O
any	O
meaning	O
,	O
we	O
might	O
simply	O
introduce	O
latent	B
variables	O
in	O
order	O
to	O
make	O
the	O
model	O
more	O
powerful	O
.	O
for	O
example	O
,	O
one	O
can	O
show	O
(	O
exercise	O
11.8	O
)	O
that	O
the	O
mean	B
and	O
covariance	B
of	O
the	O
mixture	B
distribution	O
are	O
given	O
by	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
k	O
e	O
[	O
x	O
]	O
=	O
cov	O
[	O
x	O
]	O
=	O
πkμk	O
πk	O
[	O
σk	O
+	O
μkμt	O
k	O
]	O
−	O
e	O
[	O
x	O
]	O
e	O
[	O
x	O
]	O
t	O
(	O
11.4	O
)	O
(	O
11.5	O
)	O
k	O
where	O
σk	O
=	O
diag	O
(	O
μjk	O
(	O
1	O
−	O
μjk	O
)	O
)	O
.	O
so	O
although	O
the	O
component	O
distributions	O
are	O
factorized	O
,	O
the	O
joint	B
distribution	I
is	O
not	O
.	O
thus	O
the	O
mixture	B
distribution	O
can	O
capture	O
correlations	O
between	O
variables	O
,	O
unlike	O
a	O
single	O
product-of-bernoullis	O
model	O
.	O
11.2.3	O
using	O
mixture	B
models	O
for	O
clustering	B
there	O
are	O
two	O
main	O
applications	O
of	O
mixture	B
models	O
.	O
the	O
ﬁrst	O
is	O
to	O
use	O
them	O
as	O
a	O
black-box	B
density	O
model	O
,	O
p	O
(	O
xi	O
)	O
.	O
this	O
can	O
be	O
useful	O
for	O
a	O
variety	O
of	O
tasks	O
,	O
such	O
as	O
data	B
compression	I
,	O
outlier	O
detection	O
,	O
and	O
creating	O
generative	O
classiﬁers	O
,	O
where	O
we	O
model	O
each	O
class-conditional	B
density	I
p	O
(	O
x|y	O
=	O
c	O
)	O
by	O
a	O
mixture	B
distribution	O
(	O
see	O
section	O
14.7.3	O
)	O
.	O
the	O
second	O
,	O
and	O
more	O
common	O
,	O
application	O
of	O
mixture	B
models	O
is	O
to	O
use	O
them	O
for	O
clustering	B
.	O
we	O
discuss	O
this	O
topic	B
in	O
detail	O
in	O
chapter	O
25	O
,	O
but	O
the	O
basic	O
idea	O
is	O
simple	O
.	O
we	O
ﬁrst	O
ﬁt	O
the	O
mixture	B
model	I
,	O
and	O
then	O
compute	O
p	O
(	O
zi	O
=	O
k|xi	O
,	O
θ	O
)	O
,	O
which	O
represents	O
the	O
posterior	O
probability	O
that	O
point	O
i	O
belongs	O
to	O
cluster	O
k.	O
this	O
is	O
known	O
as	O
the	O
responsibility	B
of	O
cluster	O
k	O
for	O
point	O
i	O
,	O
and	O
can	O
be	O
computed	O
using	O
bayes	O
rule	O
as	O
follows	O
:	O
rik	O
(	O
cid:2	O
)	O
p	O
(	O
zi	O
=	O
k|xi	O
,	O
θ	O
)	O
=	O
p	O
(	O
zi	O
=	O
k|θ	O
)	O
p	O
(	O
xi|zi	O
=	O
k	O
,	O
θ	O
)	O
(	O
cid:10	O
)	O
k	O
k	O
(	O
cid:2	O
)	O
=1	O
p	O
(	O
zi	O
=	O
k	O
(	O
cid:4	O
)	O
|θ	O
)	O
p	O
(	O
xi|zi	O
=	O
k	O
(	O
cid:4	O
)	O
,	O
θ	O
)	O
(	O
11.6	O
)	O
this	O
procedure	O
is	O
called	O
soft	B
clustering	I
,	O
and	O
is	O
identical	O
to	O
the	O
computations	O
performed	O
when	O
using	O
a	O
generative	B
classiﬁer	I
.	O
the	O
difference	O
between	O
the	O
two	O
models	O
only	O
arises	O
at	O
training	O
time	O
:	O
in	O
the	O
mixture	B
case	O
,	O
we	O
never	O
observe	O
zi	O
,	O
whereas	O
with	O
a	O
generative	B
classiﬁer	I
,	O
we	O
do	O
observe	O
yi	O
(	O
which	O
plays	O
the	O
role	O
of	O
zi	O
)	O
.	O
we	O
can	O
represent	O
the	O
amount	O
of	O
uncertainty	B
in	O
the	O
cluster	O
assignment	O
by	O
using	O
1−	O
maxk	O
rik	O
.	O
assuming	O
this	O
is	O
small	O
,	O
it	O
may	O
be	O
reasonable	O
to	O
compute	O
a	O
hard	B
clustering	I
using	O
the	O
map	O
estimate	O
,	O
given	O
by	O
z∗	O
i	O
=	O
arg	O
max	O
k	O
rik	O
=	O
arg	O
max	O
k	O
log	O
p	O
(	O
xi|zi	O
=	O
k	O
,	O
θ	O
)	O
+	O
log	O
p	O
(	O
zi	O
=	O
k|θ	O
)	O
(	O
11.7	O
)	O
11.2.	O
mixture	B
models	O
341	O
s	O
e	O
n	O
e	O
g	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
0	O
yeast	O
microarray	O
data	O
k−means	O
centroids	B
9.5	O
11.5	O
13.5	O
15.5	O
18.5	O
20.5	O
time	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
11.4	O
centers	O
produced	O
by	O
k-means	O
.	O
figure	O
generated	O
by	O
kmeansyeastdemo	O
.	O
(	O
a	O
)	O
some	O
yeast	O
gene	O
expression	O
data	O
plotted	O
as	O
a	O
time	O
series	O
.	O
(	O
c	O
)	O
visualizing	B
the	O
16	O
cluster	O
0.12	O
0.14	O
0.12	O
0.06	O
0.13	O
0.07	O
0.05	O
0.15	O
0.07	O
0.09	O
figure	O
11.5	O
we	O
ﬁt	O
a	O
mixture	O
of	O
10	O
bernoullis	O
to	O
the	O
binarized	O
mnist	O
digit	O
data	O
.	O
we	O
show	O
the	O
mle	O
for	O
the	O
corresponding	O
cluster	O
means	O
,	O
μk	O
.	O
the	O
numbers	O
on	O
top	O
of	O
each	O
image	O
represent	O
the	O
mixing	B
weights	I
ˆπk	O
.	O
no	O
labels	O
were	O
used	O
when	O
training	O
the	O
model	O
.	O
figure	O
generated	O
by	O
mixbermnistem	O
.	O
hard	B
clustering	I
using	O
a	O
gmm	O
is	O
illustrated	O
in	O
figure	O
1.8	O
,	O
where	O
we	O
cluster	O
some	O
data	O
rep-	O
resenting	O
the	O
height	O
and	O
weight	O
of	O
people	O
.	O
the	O
colors	O
represent	O
the	O
hard	O
assignments	O
.	O
note	O
that	O
the	O
identity	O
of	O
the	O
labels	O
(	O
colors	O
)	O
used	O
is	O
immaterial	O
;	O
we	O
are	O
free	O
to	O
rename	O
all	O
the	O
clusters	B
,	O
without	O
affecting	O
the	O
partitioning	B
of	O
the	O
data	O
;	O
this	O
is	O
called	O
label	B
switching	I
.	O
another	O
example	O
is	O
shown	O
in	O
figure	O
11.4.	O
here	O
the	O
data	O
vectors	O
xi	O
∈	O
r	O
7	O
represent	O
the	O
expression	O
levels	O
of	O
different	O
genes	O
at	O
7	O
different	O
time	O
points	O
.	O
we	O
clustered	O
them	O
using	O
a	O
gmm	O
.	O
we	O
see	O
that	O
there	O
are	O
several	O
kinds	O
of	O
genes	O
,	O
such	O
as	O
those	O
whose	O
expression	O
level	O
goes	O
up	O
monotonically	O
over	O
time	O
(	O
in	O
response	O
to	O
a	O
given	O
stimulus	O
)	O
,	O
those	O
whose	O
expression	O
level	O
goes	O
down	O
monotonically	O
,	O
and	O
those	O
with	O
more	O
complex	O
response	O
patterns	O
.	O
we	O
have	O
clustered	O
the	O
series	O
into	O
k	O
=	O
16	O
groups	O
.	O
(	O
see	O
section	O
11.5	O
for	O
details	O
on	O
how	O
to	O
choose	O
k.	O
)	O
for	O
example	O
,	O
we	O
can	O
represent	O
each	O
cluster	O
by	O
a	O
prototype	B
or	O
centroid	B
.	O
this	O
is	O
shown	O
in	O
figure	O
11.4	O
(	O
b	O
)	O
.	O
as	O
an	O
example	O
of	O
clustering	B
binary	O
data	O
,	O
consider	O
a	O
binarized	O
version	O
of	O
the	O
mnist	O
handwrit-	O
ten	O
digit	O
dataset	O
(	O
see	O
figure	O
1.5	O
(	O
a	O
)	O
)	O
,	O
where	O
we	O
ignore	O
the	O
class	O
labels	O
.	O
we	O
can	O
ﬁt	O
a	O
mixture	O
of	O
342	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
bernoullis	O
to	O
this	O
,	O
using	O
k	O
=	O
10	O
,	O
and	O
then	O
visualize	O
the	O
resulting	O
centroids	B
,	O
ˆμk	O
,	O
as	O
shown	O
in	O
figure	O
11.5.	O
we	O
see	O
that	O
the	O
method	O
correctly	O
discovered	O
some	O
of	O
the	O
digit	O
classes	O
,	O
but	O
overall	O
the	O
results	O
aren	O
’	O
t	O
great	O
:	O
it	O
has	O
created	O
multiple	O
clusters	O
for	O
some	O
digits	O
,	O
and	O
no	O
clusters	O
for	O
others	O
.	O
there	O
are	O
several	O
possible	O
reasons	O
for	O
these	O
“	O
errors	O
”	O
:	O
•	O
the	O
model	O
is	O
very	O
simple	O
and	O
does	O
not	O
capture	O
the	O
relevant	O
visual	O
characteristics	O
of	O
a	O
digit	O
.	O
for	O
example	O
,	O
each	O
pixel	O
is	O
treated	O
independently	O
,	O
and	O
there	O
is	O
no	O
notion	O
of	O
shape	O
or	O
a	O
stroke	O
.	O
•	O
although	O
we	O
think	O
there	O
should	O
be	O
10	O
clusters	B
,	O
some	O
of	O
the	O
digits	O
actually	O
exhibit	O
a	O
fair	O
degree	B
of	O
visual	O
variety	O
.	O
for	O
example	O
,	O
there	O
are	O
two	O
ways	O
of	O
writing	O
7	O
’	O
s	O
(	O
with	O
and	O
without	O
the	O
cross	O
bar	O
)	O
.	O
figure	O
1.5	O
(	O
a	O
)	O
illustrates	O
some	O
of	O
the	O
range	O
in	O
writing	O
styles	O
.	O
thus	O
we	O
need	O
k	O
(	O
cid:16	O
)	O
10	O
clusters	B
to	O
adequately	O
model	O
this	O
data	O
.	O
however	O
,	O
if	O
we	O
set	O
k	O
to	O
be	O
large	O
,	O
there	O
is	O
nothing	O
in	O
the	O
model	O
or	O
algorithm	O
preventing	O
the	O
extra	O
clusters	B
from	O
being	O
used	O
to	O
create	O
multiple	O
versions	O
of	O
the	O
same	O
digit	O
,	O
and	O
indeed	O
this	O
is	O
what	O
happens	O
.	O
we	O
can	O
use	O
model	B
selection	I
to	O
prevent	O
too	O
many	O
clusters	B
from	O
being	O
chosen	O
but	O
what	O
looks	O
visually	O
appealing	O
and	O
what	O
makes	O
a	O
good	O
density	O
estimator	O
may	O
be	O
quite	O
different	O
.	O
•	O
the	O
likelihood	B
function	O
is	O
not	O
convex	O
,	O
so	O
we	O
may	O
be	O
stuck	O
in	O
a	O
local	O
optimum	O
,	O
as	O
we	O
explain	O
in	O
section	O
11.3.2.	O
this	O
example	O
is	O
typical	O
of	O
mixture	B
modeling	O
,	O
and	O
goes	O
to	O
show	O
one	O
must	O
be	O
very	O
cautious	O
(	O
adding	O
a	O
little	O
bit	O
of	O
trying	O
to	O
“	O
interpret	O
”	O
any	O
clusters	B
that	O
are	O
discovered	O
by	O
the	O
method	O
.	O
supervision	O
,	O
or	O
using	O
informative	O
priors	O
,	O
can	O
help	O
a	O
lot	O
.	O
)	O
11.2.4	O
mixtures	O
of	O
experts	O
section	O
14.7.3	O
described	O
how	O
to	O
use	O
mixture	B
models	O
in	O
the	O
context	O
of	O
generative	O
classiﬁers	O
.	O
we	O
can	O
also	O
use	O
them	O
to	O
create	O
discriminative	B
models	O
for	O
classiﬁcation	B
and	O
regression	B
.	O
for	O
example	O
,	O
consider	O
the	O
data	O
in	O
figure	O
11.6	O
(	O
a	O
)	O
.	O
it	O
seems	O
like	O
a	O
good	O
model	O
would	O
be	O
three	O
different	O
linear	B
regression	I
functions	O
,	O
each	O
applying	O
to	O
a	O
different	O
part	O
of	O
the	O
input	O
space	O
.	O
we	O
can	O
model	O
this	O
by	O
allowing	O
the	O
mixing	B
weights	I
and	O
the	O
mixture	B
densities	O
to	O
be	O
input-dependent	O
:	O
p	O
(	O
yi|xi	O
,	O
zi	O
=	O
k	O
,	O
θ	O
)	O
=n	O
(	O
yi|wt	O
k	O
xi	O
,	O
σ2	O
k	O
)	O
p	O
(	O
zi|xi	O
,	O
θ	O
)	O
=	O
cat	O
(	O
zi|s	O
(	O
vt	O
xi	O
)	O
)	O
see	O
figure	O
11.7	O
(	O
a	O
)	O
for	O
the	O
dgm	O
.	O
(	O
11.8	O
)	O
(	O
11.9	O
)	O
this	O
model	O
is	O
called	O
a	O
mixture	B
of	I
experts	I
or	O
moe	O
(	O
jordan	O
and	O
jacobs	O
1994	O
)	O
.	O
the	O
idea	O
is	O
that	O
each	O
submodel	O
is	O
considered	O
to	O
be	O
an	O
“	O
expert	O
”	O
in	O
a	O
certain	O
region	O
of	O
input	O
space	O
.	O
the	O
function	O
p	O
(	O
zi	O
=	O
k|xi	O
,	O
θ	O
)	O
is	O
called	O
a	O
gating	B
function	I
,	O
and	O
decides	O
which	O
expert	O
to	O
use	O
,	O
depending	O
on	O
the	O
input	O
values	O
.	O
for	O
example	O
,	O
figure	O
11.6	O
(	O
b	O
)	O
shows	O
how	O
the	O
three	O
experts	O
have	O
“	O
carved	O
up	O
”	O
the	O
1d	O
input	O
space	O
,	O
figure	O
11.6	O
(	O
a	O
)	O
shows	O
the	O
predictions	O
of	O
each	O
expert	O
individually	O
(	O
in	O
this	O
case	O
,	O
the	O
experts	O
are	O
just	O
linear	B
regression	I
models	O
)	O
,	O
and	O
figure	O
11.6	O
(	O
c	O
)	O
shows	O
the	O
overall	O
prediction	O
of	O
the	O
model	O
,	O
obtained	O
using	O
p	O
(	O
yi|xi	O
,	O
θ	O
)	O
=	O
p	O
(	O
zi	O
=	O
k|xi	O
,	O
θ	O
)	O
p	O
(	O
yi|xi	O
,	O
zi	O
=	O
k	O
,	O
θ	O
)	O
(	O
11.10	O
)	O
(	O
cid:2	O
)	O
k	O
we	O
discuss	O
how	O
to	O
ﬁt	O
this	O
model	O
in	O
section	O
11.4.3	O
.	O
11.2.	O
mixture	B
models	O
343	O
expert	O
predictions	O
,	O
fixed	O
mixing	O
weights=0	O
gating	O
functions	O
,	O
fixed	O
mixing	O
weights=0	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−1	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−0.5	O
0	O
(	O
a	O
)	O
0.5	O
1	O
−1	O
−0.5	O
0.5	O
1	O
0	O
(	O
b	O
)	O
predicted	O
mean	B
and	O
var	O
,	O
fixed	O
mixing	O
weights=0	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−2	O
−1.5	O
−1	O
−0.5	O
0	O
0.5	O
1	O
(	O
c	O
)	O
figure	O
11.6	O
(	O
a	O
)	O
some	O
data	O
ﬁt	O
with	O
three	O
separate	O
regression	B
lines	O
.	O
(	O
b	O
)	O
gating	O
functions	O
for	O
three	O
different	O
“	O
experts	O
”	O
.	O
(	O
c	O
)	O
the	O
conditionally	O
weighted	O
average	O
of	O
the	O
three	O
expert	O
predictions	O
.	O
figure	O
generated	O
by	O
mixexpdemo	O
.	O
xi	O
zi	O
yi	O
xi	O
z1	O
i	O
z2	O
i	O
yi	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
11.7	O
(	O
a	O
)	O
a	O
mixture	B
of	I
experts	I
.	O
(	O
b	O
)	O
a	O
hierarchical	B
mixture	I
of	I
experts	I
.	O
344	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
forwards	O
problem	O
expert	O
predictions	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
0	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
−0.2	O
−0.2	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
1.2	O
(	O
a	O
)	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−0.2	O
prediction	O
mean	B
mode	O
(	O
b	O
)	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
1.2	O
(	O
c	O
)	O
figure	O
11.8	O
(	O
a	O
)	O
some	O
data	O
from	O
a	O
simple	O
forwards	O
model	O
.	O
(	O
b	O
)	O
some	O
data	O
from	O
the	O
inverse	O
model	O
,	O
ﬁt	O
(	O
c	O
)	O
the	O
with	O
a	O
mixture	O
of	O
3	O
linear	O
regressions	O
.	O
training	O
points	O
are	O
color	O
coded	O
by	O
their	O
responsibilities	O
.	O
predictive	B
mean	O
(	O
red	O
cross	O
)	O
and	O
mode	B
(	O
black	O
square	O
)	O
.	O
based	O
on	O
figures	O
5.20	O
and	O
5.21	O
of	O
(	O
bishop	O
2006b	O
)	O
.	O
figure	O
generated	O
by	O
mixexpdemoonetomany	O
.	O
it	O
should	O
be	O
clear	O
that	O
we	O
can	O
“	O
plug	O
in	O
”	O
any	O
model	O
for	O
the	O
expert	O
.	O
for	O
example	O
,	O
we	O
can	O
use	O
neural	B
networks	I
(	O
chapter	O
16	O
)	O
to	O
represent	O
both	O
the	O
gating	O
functions	O
and	O
the	O
experts	O
.	O
the	O
result	O
is	O
known	O
as	O
a	O
mixture	B
density	I
network	I
.	O
such	O
models	O
are	O
slower	O
to	O
train	O
,	O
but	O
can	O
be	O
more	O
ﬂexible	O
than	O
mixtures	O
of	O
experts	O
.	O
see	O
(	O
bishop	O
1994	O
)	O
for	O
details	O
.	O
it	O
is	O
also	O
possible	O
to	O
make	O
each	O
expert	O
be	O
itself	O
a	O
mixture	B
of	I
experts	I
.	O
this	O
gives	O
rise	O
to	O
a	O
model	O
known	O
as	O
the	O
hierarchical	B
mixture	I
of	I
experts	I
.	O
see	O
figure	O
11.7	O
(	O
b	O
)	O
for	O
the	O
dgm	O
,	O
and	O
section	O
16.2.6	O
for	O
further	O
details	O
.	O
11.2.4.1	O
application	O
to	O
inverse	B
problems	I
mixtures	O
of	O
experts	O
are	O
useful	O
in	O
solving	O
inverse	B
problems	I
.	O
these	O
are	O
problems	O
where	O
we	O
have	O
to	O
invert	O
a	O
many-to-one	O
mapping	O
.	O
a	O
typical	O
example	O
is	O
in	O
robotics	O
,	O
where	O
the	O
location	O
of	O
the	O
end	B
effector	I
(	O
hand	O
)	O
y	O
is	O
uniquely	O
determined	O
by	O
the	O
joint	O
angles	O
of	O
the	O
motors	O
,	O
x.	O
however	O
,	O
for	O
any	O
given	O
location	O
y	O
,	O
there	O
are	O
many	O
settings	O
of	O
the	O
joints	O
x	O
that	O
can	O
produce	O
it	O
.	O
thus	O
the	O
inverse	O
mapping	O
x	O
=	O
f−1	O
(	O
y	O
)	O
is	O
not	O
unique	O
.	O
another	O
example	O
is	O
kinematic	B
tracking	I
of	O
people	O
from	O
video	O
(	O
bo	O
et	O
al	O
.	O
2008	O
)	O
,	O
where	O
the	O
mapping	O
from	O
image	O
appearance	O
to	O
pose	O
is	O
not	O
unique	O
,	O
due	O
to	O
self	O
occlusion	O
,	O
etc	O
.	O
11.3.	O
parameter	B
estimation	O
for	O
mixture	B
models	O
345	O
θz	O
z1	O
x1	O
·	O
·	O
·	O
zn	O
xn	O
θx	O
(	O
a	O
)	O
θz	O
zi	O
xi	O
θx	O
(	O
b	O
)	O
n	O
figure	O
11.9	O
a	O
lvm	O
represented	O
as	O
a	O
dgm	O
.	O
left	O
:	O
model	O
is	O
unrolled	B
for	O
n	O
examples	O
.	O
right	O
:	O
same	O
model	O
using	O
plate	O
notation	O
.	O
a	O
simpler	O
example	O
,	O
for	O
illustration	O
purposes	O
,	O
is	O
shown	O
in	O
figure	O
11.8	O
(	O
a	O
)	O
.	O
we	O
see	O
that	O
this	O
deﬁnes	O
a	O
function	O
,	O
y	O
=	O
f	O
(	O
x	O
)	O
,	O
since	O
for	O
every	O
value	O
x	O
along	O
the	O
horizontal	O
axis	O
,	O
there	O
is	O
a	O
unique	O
response	O
y.	O
this	O
is	O
sometimes	O
called	O
the	O
forwards	B
model	I
.	O
now	O
consider	O
the	O
problem	O
of	O
computing	O
x	O
=	O
f−1	O
(	O
y	O
)	O
.	O
the	O
corresponding	O
inverse	O
model	O
is	O
shown	O
in	O
figure	O
11.8	O
(	O
b	O
)	O
;	O
this	O
is	O
obtained	O
by	O
simply	O
interchanging	O
the	O
x	O
and	O
y	O
axes	O
.	O
now	O
we	O
see	O
that	O
for	O
some	O
values	O
along	O
the	O
horizontal	O
axis	O
,	O
there	O
are	O
multiple	O
possible	O
outputs	O
,	O
so	O
the	O
inverse	O
is	O
not	O
uniquely	O
deﬁned	O
.	O
for	O
example	O
,	O
if	O
y	O
=	O
0.8	O
,	O
then	O
x	O
could	O
be	O
0.2	O
or	O
0.8.	O
consequently	O
,	O
the	O
predictive	B
distribution	O
,	O
p	O
(	O
x|y	O
,	O
θ	O
)	O
is	O
multimodal	O
.	O
we	O
can	O
ﬁt	O
a	O
mixture	O
of	O
linear	O
experts	O
to	O
this	O
data	O
.	O
figure	O
11.8	O
(	O
b	O
)	O
shows	O
the	O
prediction	O
of	O
each	O
expert	O
,	O
and	O
figure	O
11.8	O
(	O
c	O
)	O
shows	O
(	O
a	O
plugin	O
approximation	O
to	O
)	O
the	O
posterior	O
predictive	O
mode	O
and	O
mean	B
.	O
note	O
that	O
the	O
posterior	B
mean	I
does	O
not	O
yield	O
good	O
predictions	O
.	O
in	O
fact	O
,	O
any	O
model	O
which	O
is	O
trained	O
to	O
minimize	O
mean	B
squared	I
error	I
—	O
even	O
if	O
the	O
model	O
is	O
a	O
ﬂexible	O
nonlinear	O
model	O
,	O
such	O
as	O
neural	B
network	I
—	O
will	O
work	O
poorly	O
on	O
inverse	B
problems	I
such	O
as	O
this	O
.	O
however	O
,	O
the	O
posterior	B
mode	I
,	O
where	O
the	O
mode	B
is	O
input	O
dependent	O
,	O
provides	O
a	O
reasonable	O
approximation	O
.	O
11.3	O
parameter	B
estimation	O
for	O
mixture	B
models	O
we	O
have	O
seen	O
how	O
to	O
compute	O
the	O
posterior	O
over	O
the	O
hidden	B
variables	I
given	O
the	O
observed	O
variables	O
,	O
assuming	O
the	O
parameters	O
are	O
known	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
how	O
to	O
learn	O
the	O
parameters	O
.	O
in	O
section	O
10.4.2	O
,	O
we	O
showed	O
that	O
when	O
we	O
have	O
complete	B
data	I
and	O
a	O
factored	O
prior	O
,	O
the	O
posterior	O
over	O
the	O
parameters	O
also	O
factorizes	O
,	O
making	O
computation	O
very	O
simple	O
.	O
unfortunately	O
this	O
is	O
no	O
longer	O
true	O
if	O
we	O
have	O
hidden	B
variables	I
and/or	O
missing	B
data	I
.	O
the	O
reason	O
is	O
apparent	O
from	O
looking	O
at	O
figure	O
11.9.	O
if	O
the	O
zi	O
were	O
observed	O
,	O
then	O
by	O
d-separation	O
,	O
we	O
see	O
that	O
θz	O
⊥	O
θx|d	O
,	O
and	O
hence	O
the	O
posterior	O
will	O
factorize	O
.	O
but	O
since	O
,	O
in	O
an	O
lvm	O
,	O
the	O
zi	O
are	O
hidden	B
,	O
the	O
parameters	O
are	O
no	O
longer	O
independent	O
,	O
and	O
the	O
posterior	O
does	O
not	O
factorize	O
,	O
making	O
it	O
much	O
harder	O
to	O
346	O
35	O
30	O
25	O
20	O
15	O
10	O
5	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
19.5	O
14.5	O
9.5	O
4.5	O
2	O
μ	O
−0.5	O
−5.5	O
−10.5	O
−15.5	O
0	O
−25	O
−20	O
−15	O
−10	O
−5	O
5	O
10	O
15	O
20	O
25	O
0	O
(	O
a	O
)	O
−15.5	O
−10.5	O
−5.5	O
−0.5	O
μ	O
1	O
(	O
b	O
)	O
4.5	O
9.5	O
14.5	O
19.5	O
figure	O
11.10	O
left	O
:	O
n	O
=	O
200	O
data	O
points	O
sampled	O
from	O
a	O
mixture	O
of	O
2	O
gaussians	O
in	O
1d	O
,	O
with	O
πk	O
=	O
0.5	O
,	O
σk	O
=	O
5	O
,	O
μ1	O
=	O
−10	O
and	O
μ2	O
=	O
10.	O
right	O
:	O
likelihood	B
surface	O
p	O
(	O
d|μ1	O
,	O
μ2	O
)	O
,	O
with	O
all	O
other	O
parameters	O
set	O
to	O
their	O
true	O
values	O
.	O
we	O
see	O
the	O
two	O
symmetric	B
modes	O
,	O
reﬂecting	O
the	O
unidentiﬁability	O
of	O
the	O
parameters	O
.	O
figure	O
generated	O
by	O
mixgaussliksurfacedemo	O
.	O
compute	O
.	O
this	O
also	O
complicates	O
the	O
computation	O
of	O
map	O
and	O
ml	O
estimates	O
,	O
as	O
we	O
discus	O
below	O
.	O
11.3.1	O
unidentiﬁability	O
the	O
main	O
problem	O
with	O
computing	O
p	O
(	O
θ|d	O
)	O
for	O
an	O
lvm	O
is	O
that	O
the	O
posterior	O
may	O
have	O
multiple	O
modes	O
.	O
to	O
see	O
why	O
,	O
consider	O
a	O
gmm	O
.	O
if	O
the	O
zi	O
were	O
all	O
observed	O
,	O
we	O
would	O
have	O
a	O
unimodal	O
posterior	O
for	O
the	O
parameters	O
:	O
k	O
(	O
cid:27	O
)	O
p	O
(	O
θ|d	O
)	O
=	O
dir	O
(	O
π|d	O
)	O
niw	O
(	O
μk	O
,	O
σk|d	O
)	O
(	O
11.11	O
)	O
k=1	O
consequently	O
we	O
can	O
easily	O
ﬁnd	O
the	O
globally	O
optimal	O
map	O
estimate	O
(	O
and	O
hence	O
globally	O
optimal	O
mle	O
)	O
.	O
but	O
now	O
suppose	O
the	O
zi	O
’	O
s	O
are	O
hidden	B
.	O
in	O
this	O
case	O
,	O
for	O
each	O
of	O
the	O
possible	O
ways	O
of	O
“	O
ﬁlling	O
in	O
”	O
the	O
zi	O
’	O
s	O
,	O
we	O
get	O
a	O
different	O
unimodal	O
likelihood	B
.	O
thus	O
when	O
we	O
marginalize	O
out	O
over	O
the	O
zi	O
’	O
s	O
,	O
we	O
get	O
a	O
multi-modal	O
posterior	O
for	O
p	O
(	O
θ|d	O
)	O
.1	O
these	O
modes	O
correspond	O
to	O
different	O
labelings	O
of	O
the	O
clusters	B
.	O
this	O
is	O
illustrated	O
in	O
figure	O
11.10	O
(	O
b	O
)	O
,	O
where	O
we	O
plot	O
the	O
likelihood	B
function	O
,	O
p	O
(	O
d|μ1	O
,	O
μ2	O
)	O
,	O
for	O
a	O
2d	O
gmm	O
with	O
k	O
=	O
2	O
for	O
the	O
data	O
is	O
shown	O
in	O
figure	O
11.10	O
(	O
a	O
)	O
.	O
we	O
see	O
two	O
peaks	O
,	O
one	O
corresponding	O
to	O
the	O
case	O
where	O
μ1	O
=	O
−10	O
,	O
μ2	O
=	O
10	O
,	O
and	O
the	O
other	O
to	O
the	O
case	O
where	O
μ1	O
=	O
10	O
,	O
μ2	O
=	O
−10	O
.	O
we	O
say	O
the	O
parameters	O
are	O
not	O
identiﬁable	O
,	O
since	O
there	O
is	O
not	O
a	O
unique	O
mle	O
.	O
therefore	O
there	O
can	O
not	O
be	O
a	O
unique	O
map	O
estimate	O
(	O
assuming	O
the	O
prior	O
does	O
not	O
rule	O
out	O
certain	O
labelings	O
)	O
,	O
and	O
hence	O
the	O
posterior	O
must	O
be	O
multimodal	O
.	O
the	O
question	O
of	O
how	O
many	O
modes	O
there	O
1.	O
do	O
not	O
confuse	O
multimodality	O
of	O
the	O
parameter	B
posterior	O
,	O
p	O
(	O
θ|d	O
)	O
,	O
with	O
the	O
multimodality	O
deﬁned	O
by	O
the	O
model	O
,	O
p	O
(	O
x|θ	O
)	O
.	O
in	O
the	O
latter	O
case	O
,	O
if	O
we	O
have	O
k	O
clusters	B
,	O
we	O
would	O
expect	O
to	O
only	O
get	O
k	O
peaks	O
,	O
although	O
it	O
is	O
theoretically	O
possible	O
to	O
get	O
more	O
than	O
k	O
,	O
at	O
least	O
if	O
d	O
>	O
1	O
(	O
carreira-perpinan	O
and	O
williams	O
2003	O
)	O
.	O
11.3.	O
parameter	B
estimation	O
for	O
mixture	B
models	O
347	O
unidentiﬁability	O
can	O
cause	O
a	O
problem	O
for	O
bayesian	O
inference	B
.	O
are	O
in	O
the	O
parameter	B
posterior	O
is	O
hard	O
to	O
answer	O
.	O
there	O
are	O
k	O
!	O
possible	O
labelings	O
,	O
but	O
some	O
of	O
the	O
peaks	O
might	O
get	O
merged	O
.	O
nevertheless	O
,	O
there	O
can	O
be	O
an	O
exponential	O
number	O
,	O
since	O
ﬁnding	O
the	O
optimal	O
mle	O
for	O
a	O
gmm	O
is	O
np-hard	O
(	O
aloise	O
et	O
al	O
.	O
2009	O
;	O
drineas	O
et	O
al	O
.	O
2004	O
)	O
.	O
for	O
example	O
,	O
suppose	O
we	O
draw	O
some	O
samples	B
from	O
the	O
posterior	O
,	O
θ	O
(	O
s	O
)	O
∼	O
p	O
(	O
θ|d	O
)	O
,	O
and	O
then	O
average	O
them	O
,	O
to	O
try	O
to	O
approximate	O
the	O
posterior	B
mean	I
,	O
θ	O
=	O
1	O
(	O
this	O
kind	O
of	O
monte	O
carlo	O
approach	O
is	O
s	O
explained	O
in	O
more	O
detail	O
in	O
chapter	O
24	O
.	O
)	O
if	O
the	O
samples	B
come	O
from	O
different	O
modes	O
,	O
the	O
average	O
(	O
cid:10	O
)	O
s	O
will	O
be	O
meaningless	O
.	O
note	O
,	O
however	O
,	O
that	O
it	O
is	O
reasonable	O
to	O
average	O
the	O
posterior	O
predictive	O
distributions	O
,	O
p	O
(	O
x	O
)	O
≈	O
1	O
s=1	O
p	O
(	O
x|θ	O
(	O
s	O
)	O
)	O
,	O
since	O
the	O
likelihood	B
function	O
is	O
invariant	B
to	O
which	O
mode	B
the	O
parameters	O
came	O
from	O
.	O
(	O
cid:10	O
)	O
s	O
s=1	O
θ	O
(	O
s	O
)	O
.	O
s	O
a	O
variety	O
of	O
solutions	O
have	O
been	O
proposed	O
to	O
the	O
unidentiﬁability	O
problem	O
.	O
these	O
solutions	O
depend	O
on	O
the	O
details	O
of	O
the	O
model	O
and	O
the	O
inference	B
algorithm	O
that	O
is	O
used	O
.	O
for	O
example	O
,	O
see	O
(	O
stephens	O
2000	O
)	O
for	O
an	O
approach	O
to	O
handling	O
unidentiﬁability	O
in	O
mixture	B
models	O
using	O
mcmc	O
.	O
the	O
approach	O
we	O
will	O
adopt	O
in	O
this	O
chapter	O
is	O
much	O
simpler	O
:	O
we	O
just	O
compute	O
a	O
single	O
(	O
we	O
say	O
“	O
approximate	O
”	O
since	O
ﬁnding	O
local	O
mode	O
,	O
i.e.	O
,	O
we	O
perform	O
approximate	O
map	O
estimation	O
.	O
the	O
globally	O
optimal	O
mle	O
,	O
and	O
hence	O
map	O
estimate	O
,	O
is	O
np-hard	O
,	O
at	O
least	O
for	O
mixture	B
models	O
(	O
aloise	O
et	O
al	O
.	O
2009	O
)	O
.	O
)	O
this	O
is	O
by	O
far	O
the	O
most	O
common	O
approach	O
,	O
because	O
of	O
its	O
simplicity	O
.	O
it	O
is	O
also	O
a	O
reasonable	O
approximation	O
,	O
at	O
least	O
if	O
the	O
sample	O
size	O
is	O
sufficiently	O
large	O
.	O
to	O
see	O
why	O
,	O
consider	O
figure	O
11.9	O
(	O
a	O
)	O
.	O
we	O
see	O
that	O
there	O
are	O
n	O
latent	B
variables	O
,	O
each	O
of	O
which	O
gets	O
to	O
“	O
see	O
”	O
one	O
data	O
point	O
each	O
.	O
however	O
,	O
there	O
are	O
only	O
two	O
latent	B
parameters	O
,	O
each	O
of	O
which	O
gets	O
to	O
see	O
n	O
data	O
points	O
.	O
so	O
the	O
posterior	O
uncertainty	O
about	O
the	O
parameters	O
is	O
typically	O
much	O
less	O
than	O
the	O
posterior	O
uncertainty	O
about	O
the	O
latent	B
variables	O
.	O
this	O
justiﬁes	O
the	O
common	O
strategy	O
of	O
computing	O
p	O
(	O
zi|xi	O
,	O
ˆθ	O
)	O
,	O
but	O
not	O
bothering	O
to	O
compute	O
p	O
(	O
θ|d	O
)	O
.	O
in	O
section	O
5.6	O
,	O
we	O
will	O
study	O
hierarchical	O
bayesian	O
models	O
,	O
which	O
essentially	O
put	O
structure	O
on	O
top	O
of	O
the	O
parameters	O
.	O
in	O
such	O
models	O
,	O
it	O
is	O
important	O
to	O
model	O
p	O
(	O
θ|d	O
)	O
,	O
so	O
that	O
the	O
parameters	O
can	O
send	O
information	B
between	O
themselves	O
.	O
if	O
we	O
used	O
a	O
point	B
estimate	I
,	O
this	O
would	O
not	O
be	O
possible	O
.	O
11.3.2	O
computing	O
a	O
map	O
estimate	O
is	O
non-convex	O
in	O
the	O
previous	O
sections	O
,	O
we	O
have	O
argued	O
,	O
rather	O
heuristically	O
,	O
that	O
the	O
likelihood	B
function	O
has	O
multiple	O
modes	O
,	O
and	O
hence	O
that	O
ﬁnding	O
an	O
map	O
or	O
ml	O
estimate	O
will	O
be	O
hard	O
.	O
in	O
this	O
section	O
,	O
we	O
show	O
this	O
result	O
by	O
more	O
algebraic	O
means	O
,	O
which	O
sheds	O
some	O
additional	O
insight	O
into	O
the	O
problem	O
.	O
our	O
presentation	O
is	O
based	O
in	O
part	O
on	O
(	O
rennie	O
2004	O
)	O
.	O
consider	O
the	O
log-likelihood	O
for	O
an	O
lvm	O
:	O
p	O
(	O
xi	O
,	O
zi|θ	O
)	O
log	O
p	O
(	O
d|θ	O
)	O
=	O
(	O
cid:2	O
)	O
log	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
(	O
cid:19	O
)	O
i	O
zi	O
unfortunately	O
,	O
this	O
objective	O
is	O
hard	O
to	O
maximize	O
.	O
since	O
we	O
can	O
not	O
push	O
the	O
log	O
inside	O
the	O
sum	O
.	O
this	O
precludes	O
certain	O
algebraic	O
simplications	O
,	O
but	O
does	O
not	O
prove	O
the	O
problem	O
is	O
hard	O
.	O
now	O
suppose	O
the	O
joint	B
probability	I
distribution	I
p	O
(	O
zi	O
,	O
xi|θ	O
)	O
is	O
in	O
the	O
exponential	B
family	I
,	O
which	O
means	O
it	O
can	O
be	O
written	O
as	O
follows	O
:	O
p	O
(	O
x	O
,	O
z|θ	O
)	O
=	O
1	O
z	O
(	O
θ	O
)	O
exp	O
[	O
θt	O
φ	O
(	O
x	O
,	O
z	O
)	O
]	O
(	O
11.13	O
)	O
(	O
11.12	O
)	O
348	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
where	O
φ	O
(	O
x	O
,	O
z	O
)	O
are	O
the	O
sufficient	B
statistics	I
,	O
and	O
z	O
(	O
θ	O
)	O
is	O
the	O
normalization	O
constant	O
(	O
see	O
sec-	O
tion	O
9.2	O
for	O
more	O
details	O
)	O
.	O
it	O
can	O
be	O
shown	O
(	O
exercise	O
9.2	O
)	O
that	O
the	O
mvn	O
is	O
in	O
the	O
exponential	B
family	I
,	O
as	O
are	O
nearly	O
all	O
of	O
the	O
distributions	O
we	O
have	O
encountered	O
so	O
far	O
,	O
including	O
dirichlet	O
,	O
multinomial	B
,	O
gamma	O
,	O
wishart	O
,	O
etc	O
.	O
(	O
the	O
student	O
distribution	O
is	O
a	O
notable	O
exception	O
.	O
)	O
further-	O
more	O
,	O
mixtures	O
of	O
exponential	O
families	O
are	O
also	O
in	O
the	O
exponential	B
family	I
,	O
providing	O
the	O
mixing	O
indicator	O
variables	O
are	O
observed	O
(	O
exercise	O
11.11	O
)	O
.	O
with	O
this	O
assumption	O
,	O
the	O
complete	B
data	I
log	I
likelihood	I
can	O
be	O
written	O
as	O
follows	O
:	O
(	O
cid:6	O
)	O
c	O
(	O
θ	O
)	O
=	O
log	O
p	O
(	O
xi	O
,	O
zi|θ	O
)	O
=	O
θt	O
(	O
φ	O
(	O
xi	O
,	O
zi	O
)	O
)	O
−	O
n	O
z	O
(	O
θ	O
)	O
(	O
11.14	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
i	O
the	O
ﬁrst	O
term	O
is	O
clearly	O
linear	O
in	O
θ.	O
one	O
can	O
show	O
that	O
z	O
(	O
θ	O
)	O
is	O
a	O
convex	B
function	O
(	O
boyd	O
and	O
vandenberghe	O
2004	O
)	O
,	O
so	O
the	O
overall	O
objective	O
is	O
concave	B
(	O
due	O
to	O
the	O
minus	O
sign	O
)	O
,	O
and	O
hence	O
has	O
a	O
unique	O
maximum	O
.	O
now	O
consider	O
what	O
happens	O
when	O
we	O
have	O
missing	B
data	I
.	O
the	O
observed	B
data	I
log	I
likelihood	I
is	O
given	O
by	O
(	O
cid:6	O
)	O
(	O
θ	O
)	O
=	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
log	O
i	O
zi	O
p	O
(	O
xi	O
,	O
zi|θ	O
)	O
=	O
(	O
cid:2	O
)	O
log	O
i	O
zi	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
(	O
cid:19	O
)	O
eθt	O
φ	O
(	O
zi	O
,	O
xi	O
)	O
−	O
n	O
log	O
z	O
(	O
θ	O
)	O
(	O
11.15	O
)	O
one	O
can	O
show	O
that	O
the	O
log-sum-exp	B
function	O
is	O
convex	B
(	O
boyd	O
and	O
vandenberghe	O
2004	O
)	O
,	O
and	O
we	O
know	O
that	O
z	O
(	O
θ	O
)	O
is	O
convex	B
.	O
however	O
,	O
the	O
difference	O
of	O
two	O
convex	B
functions	O
is	O
not	O
,	O
in	O
general	O
,	O
convex	B
.	O
so	O
the	O
objective	O
is	O
neither	O
convex	B
nor	O
concave	B
,	O
and	O
has	O
local	O
optima	O
.	O
the	O
disadvantage	O
of	O
non-convex	O
functions	O
is	O
that	O
it	O
is	O
usually	O
hard	O
to	O
ﬁnd	O
their	O
global	O
op-	O
timum	O
.	O
most	O
optimization	B
algorithms	O
will	O
only	O
ﬁnd	O
a	O
local	O
optimum	O
;	O
which	O
one	O
they	O
ﬁnd	O
depends	O
on	O
where	O
they	O
start	O
.	O
there	O
are	O
some	O
algorithms	O
,	O
such	O
as	O
simulated	B
annealing	I
(	O
sec-	O
tion	O
24.6.1	O
)	O
or	O
genetic	B
algorithms	I
,	O
that	O
claim	O
to	O
always	O
ﬁnd	O
the	O
global	O
optimum	O
,	O
but	O
this	O
is	O
only	O
under	O
unrealistic	O
assumptions	O
(	O
e.g.	O
,	O
if	O
they	O
are	O
allowed	O
to	O
be	O
cooled	O
“	O
inﬁnitely	O
slowly	O
”	O
,	O
or	O
al-	O
lowed	O
to	O
run	O
“	O
inﬁnitely	O
long	O
”	O
)	O
.	O
in	O
practice	O
,	O
we	O
will	O
run	O
a	O
local	O
optimizer	O
,	O
perhaps	O
using	O
multiple	B
random	I
restarts	I
to	O
increase	O
out	O
chance	O
of	O
ﬁnding	O
a	O
“	O
good	O
”	O
local	O
optimum	O
.	O
of	O
course	O
,	O
careful	O
initialization	O
can	O
help	O
a	O
lot	O
,	O
too	O
.	O
we	O
give	O
examples	O
of	O
how	O
to	O
do	O
this	O
on	O
a	O
case-by-case	O
basis	O
.	O
note	O
that	O
a	O
convex	B
method	O
for	O
ﬁtting	O
mixtures	O
of	O
gaussians	O
has	O
been	O
proposed	O
.	O
the	O
idea	O
is	O
to	O
assign	O
one	O
cluster	O
per	O
data	O
point	O
,	O
and	O
select	O
from	O
amongst	O
them	O
,	O
using	O
a	O
convex	B
(	O
cid:6	O
)	O
1-type	O
penalty	O
,	O
rather	O
than	O
trying	O
to	O
optimize	O
the	O
locations	O
of	O
the	O
cluster	O
centers	O
.	O
see	O
(	O
lashkari	O
and	O
golland	O
2007	O
)	O
for	O
details	O
.	O
this	O
is	O
essentially	O
an	O
unsupervised	O
version	O
of	O
the	O
approach	O
used	O
in	O
sparse	O
kernel	O
logistic	O
regression	B
,	O
which	O
we	O
will	O
discuss	O
in	O
section	O
14.3.2.	O
note	O
,	O
however	O
,	O
that	O
the	O
(	O
cid:6	O
)	O
1	O
penalty	O
,	O
although	O
convex	B
,	O
is	O
not	O
necessarily	O
a	O
good	O
way	O
to	O
promote	O
sparsity	B
,	O
as	O
discussed	O
in	O
chapter	O
13.	O
in	O
fact	O
,	O
as	O
we	O
will	O
see	O
in	O
that	O
chapter	O
,	O
some	O
of	O
the	O
best	O
sparsity-promoting	O
methods	O
use	O
non-convex	O
penalties	O
,	O
and	O
use	O
em	O
to	O
optimie	O
them	O
!	O
the	O
moral	O
of	O
the	O
story	O
is	O
:	O
do	O
not	O
be	O
afraid	O
of	O
non-convexity	O
.	O
11.4	O
the	O
em	O
algorithm	O
for	O
many	O
models	O
in	O
machine	B
learning	I
and	O
statistics	O
,	O
computing	O
the	O
ml	O
or	O
map	O
parameter	B
estimate	O
is	O
easy	O
provided	O
we	O
observe	O
all	O
the	O
values	O
of	O
all	O
the	O
relevant	O
random	O
variables	O
,	O
i.e.	O
,	O
if	O
11.4.	O
the	O
em	O
algorithm	O
349	O
model	O
mix	O
.	O
gaussians	O
mix	O
.	O
experts	O
factor	B
analysis	I
student	O
t	O
probit	O
regression	B
dgm	O
with	O
hidden	B
variables	I
mvn	O
with	O
missing	B
data	I
hmms	O
shrinkage	B
estimates	O
of	O
gaussian	O
means	O
section	O
11.4.2	O
11.4.3	O
12.1.5	O
11.4.5	O
11.4.6	O
11.4.4	O
11.6.1	O
17.5.2	O
exercise	O
11.13	O
table	O
11.2	O
some	O
models	O
discussed	O
in	O
this	O
book	O
for	O
which	O
em	O
can	O
be	O
easily	O
applied	O
to	O
ﬁnd	O
the	O
ml/	O
map	O
parameter	B
estimate	O
.	O
we	O
have	O
complete	B
data	I
.	O
however	O
,	O
if	O
we	O
have	O
missing	B
data	I
and/or	O
latent	B
variables	O
,	O
then	O
computing	O
the	O
ml/map	O
estimate	O
becomes	O
hard	O
.	O
one	O
approach	O
is	O
to	O
use	O
a	O
generic	O
gradient-based	O
optimizer	O
to	O
ﬁnd	O
a	O
local	O
minimum	O
of	O
the	O
negative	B
log	I
likelihood	I
or	O
nll	O
,	O
given	O
by	O
nll	O
(	O
θ	O
)	O
=	O
−	O
(	O
cid:2	O
)	O
1	O
n	O
log	O
p	O
(	O
d|θ	O
)	O
(	O
11.16	O
)	O
however	O
,	O
we	O
often	O
have	O
to	O
enforce	O
constraints	O
,	O
such	O
as	O
the	O
fact	O
that	O
covariance	B
matrices	O
must	O
be	O
positive	B
deﬁnite	I
,	O
mixing	B
weights	I
must	O
sum	O
to	O
one	O
,	O
etc.	O
,	O
which	O
can	O
be	O
tricky	O
(	O
see	O
exercise	O
11.5	O
)	O
.	O
in	O
such	O
cases	O
,	O
it	O
is	O
often	O
much	O
simpler	O
(	O
but	O
not	O
always	O
faster	O
)	O
to	O
use	O
an	O
algorithm	O
called	O
expectation	B
maximization	I
,	O
or	O
em	O
for	O
short	O
(	O
dempster	O
et	O
al	O
.	O
1977	O
;	O
meng	O
and	O
van	O
dyk	O
1997	O
;	O
mclachlan	O
and	O
krishnan	O
1997	O
)	O
.	O
this	O
is	O
a	O
simple	O
iterative	O
algorithm	O
,	O
often	O
with	O
closed-form	O
updates	O
at	O
each	O
step	O
.	O
furthermore	O
,	O
the	O
algorithm	O
automatically	O
enforce	O
the	O
required	O
constraints	O
.	O
em	O
exploits	O
the	O
fact	O
that	O
if	O
the	O
data	O
were	O
fully	O
observed	O
,	O
then	O
the	O
ml/	O
map	O
estimate	O
would	O
be	O
easy	O
to	O
compute	O
.	O
in	O
particular	O
,	O
em	O
is	O
an	O
iterative	O
algorithm	O
which	O
alternates	O
between	O
inferring	O
the	O
missing	B
values	O
given	O
the	O
parameters	O
(	O
e	O
step	O
)	O
,	O
and	O
then	O
optimizing	O
the	O
parameters	O
given	O
the	O
“	O
ﬁlled	O
in	O
”	O
data	O
(	O
m	O
step	O
)	O
.	O
we	O
give	O
the	O
details	O
below	O
,	O
followed	O
by	O
several	O
examples	O
.	O
we	O
end	O
with	O
a	O
more	O
theoretical	O
discussion	O
,	O
where	O
we	O
put	O
the	O
algorithm	O
in	O
a	O
larger	O
context	O
.	O
see	O
table	O
11.2	O
for	O
a	O
summary	O
of	O
the	O
applications	O
of	O
em	O
in	O
this	O
book	O
.	O
11.4.1	O
basic	O
idea	O
let	O
xi	O
be	O
the	O
visible	B
or	O
observed	O
variables	O
in	O
case	O
i	O
,	O
and	O
let	O
zi	O
be	O
the	O
hidden	B
or	O
missing	B
variables	O
.	O
the	O
goal	O
is	O
to	O
maximize	O
the	O
log	O
likelihood	O
of	O
the	O
observed	O
data	O
:	O
(	O
cid:6	O
)	O
(	O
θ	O
)	O
=	O
log	O
p	O
(	O
xi|θ	O
)	O
=	O
p	O
(	O
xi	O
,	O
zi|θ	O
)	O
(	O
11.17	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
n	O
(	O
cid:2	O
)	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
log	O
i=1	O
zi	O
(	O
cid:19	O
)	O
unfortunately	O
this	O
is	O
hard	O
to	O
optimize	O
,	O
since	O
the	O
log	O
can	O
not	O
be	O
pushed	O
inside	O
the	O
sum	O
.	O
350	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
em	O
gets	O
around	O
this	O
problem	O
as	O
follows	O
.	O
deﬁne	O
the	O
complete	B
data	I
log	I
likelihood	I
to	O
be	O
(	O
cid:6	O
)	O
c	O
(	O
θ	O
)	O
(	O
cid:2	O
)	O
log	O
p	O
(	O
xi	O
,	O
zi|θ	O
)	O
(	O
11.18	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
this	O
can	O
not	O
be	O
computed	O
,	O
since	O
zi	O
is	O
unknown	B
.	O
so	O
let	O
us	O
deﬁne	O
the	O
expected	B
complete	I
data	I
log	I
likelihood	I
as	O
follows	O
:	O
q	O
(	O
θ	O
,	O
θt−1	O
)	O
=	O
e	O
(	O
cid:6	O
)	O
c	O
(	O
θ	O
)	O
(	O
cid:31	O
)	O
(	O
(	O
d	O
,	O
θt−1	O
(	O
11.19	O
)	O
where	O
t	O
is	O
the	O
current	O
iteration	O
number	O
.	O
q	O
is	O
called	O
the	O
auxiliary	B
function	I
.	O
the	O
expectation	O
is	O
taken	O
wrt	O
the	O
old	O
parameters	O
,	O
θt−1	O
,	O
and	O
the	O
observed	O
data	O
d.	O
the	O
goal	O
of	O
the	O
e	O
stepis	O
to	O
compute	O
q	O
(	O
θ	O
,	O
θt−1	O
)	O
,	O
or	O
rather	O
,	O
the	O
terms	O
inside	O
of	O
it	O
which	O
the	O
mle	O
depends	O
on	O
;	O
these	O
are	O
known	O
as	O
the	O
expected	B
sufficient	I
statistics	I
or	O
ess	O
.	O
in	O
the	O
m	O
step	O
,	O
we	O
optimize	O
the	O
q	O
function	O
wrt	O
θ	O
:	O
θt	O
=	O
arg	O
max	O
θ	O
q	O
(	O
θ	O
,	O
θt−1	O
)	O
to	O
perform	O
map	O
estimation	O
,	O
we	O
modify	O
the	O
m	O
step	O
as	O
follows	O
:	O
θt	O
=	O
argmax	O
θ	O
q	O
(	O
θ	O
,	O
θt−1	O
)	O
+	O
log	O
p	O
(	O
θ	O
)	O
the	O
e	O
step	O
remains	O
unchanged	O
.	O
(	O
11.20	O
)	O
(	O
11.21	O
)	O
in	O
section	O
11.4.7	O
we	O
show	O
that	O
the	O
em	O
algorithm	O
monotonically	O
increases	O
the	O
log	O
likelihood	O
of	O
the	O
observed	O
data	O
(	O
plus	O
the	O
log	O
prior	O
,	O
if	O
doing	O
map	O
estimation	O
)	O
,	O
or	O
it	O
stays	O
the	O
same	O
.	O
so	O
if	O
the	O
objective	O
ever	O
goes	O
down	O
,	O
there	O
must	O
be	O
a	O
bug	O
in	O
our	O
math	O
or	O
our	O
code	O
.	O
(	O
this	O
is	O
a	O
surprisingly	O
useful	O
debugging	O
tool	O
!	O
)	O
below	O
we	O
explain	O
how	O
to	O
perform	O
the	O
e	O
and	O
m	O
steps	O
for	O
several	O
simple	O
models	O
,	O
that	O
should	O
make	O
things	O
clearer	O
.	O
11.4.2	O
em	O
for	O
gmms	O
in	O
this	O
section	O
,	O
we	O
discuss	O
how	O
to	O
ﬁt	O
a	O
mixture	O
of	O
gaussians	O
using	O
em	O
.	O
fitting	O
other	O
kinds	O
of	O
mixture	B
models	O
requires	O
a	O
straightforward	O
modiﬁcation	O
—	O
see	O
exercise	O
11.3.	O
we	O
assume	O
the	O
number	O
of	O
mixture	B
components	O
,	O
k	O
,	O
is	O
known	O
(	O
see	O
section	O
11.5	O
for	O
discussion	O
of	O
this	O
point	O
)	O
.	O
11.4.	O
the	O
em	O
algorithm	O
11.4.2.1	O
auxiliary	B
function	I
the	O
expected	B
complete	I
data	I
log	I
likelihood	I
is	O
given	O
by	O
i	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
q	O
(	O
θ	O
,	O
θ	O
(	O
t−1	O
)	O
)	O
(	O
cid:2	O
)	O
e	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
=	O
=	O
=	O
e	O
k	O
k	O
i	O
i	O
i	O
(	O
cid:19	O
)	O
log	O
p	O
(	O
xi	O
,	O
zi|θ	O
)	O
(	O
cid:18	O
)	O
k	O
(	O
cid:27	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
k=1	O
(	O
πkp	O
(	O
xi|θk	O
)	O
)	O
i	O
(	O
zi=k	O
)	O
log	O
e	O
[	O
i	O
(	O
zi	O
=	O
k	O
)	O
]	O
log	O
[	O
πkp	O
(	O
xi|θk	O
)	O
]	O
p	O
(	O
zi	O
=	O
k|xi	O
,	O
θt−1	O
)	O
log	O
[	O
πkp	O
(	O
xi|θk	O
)	O
]	O
rik	O
log	O
p	O
(	O
xi|θk	O
)	O
rik	O
log	O
πk	O
+	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
=	O
(	O
11.26	O
)	O
where	O
rik	O
(	O
cid:2	O
)	O
p	O
(	O
zi	O
=	O
k|xi	O
,	O
θ	O
(	O
t−1	O
)	O
)	O
is	O
the	O
responsibility	B
that	O
cluster	O
k	O
takes	O
for	O
data	O
point	O
i.	O
this	O
is	O
computed	O
in	O
the	O
e	O
step	O
,	O
described	O
below	O
.	O
k	O
k	O
i	O
i	O
351	O
(	O
11.22	O
)	O
(	O
11.23	O
)	O
(	O
11.24	O
)	O
(	O
11.25	O
)	O
(	O
11.27	O
)	O
(	O
11.28	O
)	O
11.4.2.2	O
e	O
step	O
the	O
e	O
step	O
has	O
the	O
following	O
simple	O
form	O
,	O
which	O
is	O
the	O
same	O
for	O
any	O
mixture	B
model	I
:	O
rik	O
=	O
(	O
cid:10	O
)	O
11.4.2.3	O
m	O
step	O
πkp	O
(	O
xi|θ	O
(	O
t−1	O
)	O
k	O
(	O
cid:2	O
)	O
πk	O
(	O
cid:2	O
)	O
p	O
(	O
xi|θ	O
(	O
t−1	O
)	O
k	O
(	O
cid:2	O
)	O
)	O
k	O
)	O
in	O
the	O
m	O
step	O
,	O
we	O
optimize	O
q	O
wrt	O
π	O
and	O
the	O
θk	O
.	O
for	O
π	O
,	O
we	O
obviously	O
have	O
(	O
cid:2	O
)	O
i	O
1	O
n	O
(	O
cid:10	O
)	O
rik	O
=	O
rk	O
n	O
πk	O
=	O
where	O
rk	O
(	O
cid:2	O
)	O
i	O
rik	O
is	O
the	O
weighted	O
number	O
of	O
points	O
assigned	O
to	O
cluster	O
k.	O
to	O
derive	O
the	O
m	O
step	O
for	O
the	O
μk	O
and	O
σk	O
terms	O
,	O
we	O
look	O
at	O
the	O
parts	O
of	O
q	O
that	O
depend	O
on	O
μk	O
and	O
σk	O
.	O
we	O
see	O
that	O
the	O
result	O
is	O
(	O
cid:6	O
)	O
(	O
μk	O
,	O
σk	O
)	O
=	O
rik	O
log	O
p	O
(	O
xi|θk	O
)	O
(	O
cid:31	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
i	O
k	O
=	O
−	O
1	O
2	O
rik	O
log	O
|σk|	O
+	O
(	O
xi	O
−	O
μk	O
)	O
t	O
σ−1	O
k	O
(	O
xi	O
−	O
μk	O
)	O
(	O
11.29	O
)	O
(	O
11.30	O
)	O
(	O
cid:10	O
)	O
(	O
cid:10	O
)	O
this	O
is	O
just	O
a	O
weighted	O
version	O
of	O
the	O
standard	O
problem	O
of	O
computing	O
the	O
mles	O
of	O
an	O
mvn	O
(	O
see	O
section	O
4.1.3	O
)	O
.	O
one	O
can	O
show	O
(	O
exercise	O
11.2	O
)	O
that	O
the	O
new	O
parameter	B
estimates	O
are	O
given	O
by	O
μk	O
=	O
σk	O
=	O
i	O
rikxi	O
rk	O
i	O
rik	O
(	O
xi	O
−	O
μk	O
)	O
(	O
xi	O
−	O
μk	O
)	O
t	O
rk	O
(	O
cid:10	O
)	O
=	O
i	O
rikxixt	O
i	O
rk	O
−	O
μkμt	O
k	O
(	O
11.31	O
)	O
(	O
11.32	O
)	O
352	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
these	O
equations	O
make	O
intuitive	O
sense	O
:	O
the	O
mean	B
of	O
cluster	O
k	O
is	O
just	O
the	O
weighted	B
average	I
of	O
all	O
points	O
assigned	O
to	O
cluster	O
k	O
,	O
and	O
the	O
covariance	B
is	O
proportional	O
to	O
the	O
weighted	O
empirical	O
scatter	O
matrix	O
.	O
after	O
computing	O
the	O
new	O
estimates	O
,	O
we	O
set	O
θt	O
=	O
(	O
πk	O
,	O
μk	O
,	O
σk	O
)	O
for	O
k	O
=	O
1	O
:	O
k	O
,	O
and	O
go	O
to	O
the	O
next	O
e	O
step	O
.	O
11.4.2.4	O
example	O
an	O
example	O
of	O
the	O
algorithm	O
in	O
action	B
is	O
shown	O
in	O
figure	O
11.11.	O
we	O
start	O
with	O
μ1	O
=	O
(	O
−1	O
,	O
1	O
)	O
,	O
σ1	O
=	O
i	O
,	O
μ2	O
=	O
(	O
1	O
,	O
−1	O
)	O
,	O
σ2	O
=	O
i.	O
we	O
color	O
code	O
points	O
such	O
that	O
blue	O
points	O
come	O
from	O
cluster	O
1	O
and	O
red	O
points	O
from	O
cluster	O
2.	O
more	O
precisely	O
,	O
we	O
set	O
the	O
color	O
to	O
color	O
(	O
i	O
)	O
=r	O
i1blue	O
+	O
ri2red	O
(	O
11.33	O
)	O
so	O
ambiguous	O
points	O
appear	O
purple	O
.	O
after	O
20	O
iterations	O
,	O
the	O
algorithm	O
has	O
converged	O
on	O
a	O
good	O
clustering	B
.	O
(	O
the	O
data	O
was	O
standardized	B
,	O
by	O
removing	O
the	O
mean	B
and	O
dividing	O
by	O
the	O
standard	B
deviation	I
,	O
before	O
processing	O
.	O
this	O
often	O
helps	O
convergence	O
.	O
)	O
11.4.2.5	O
k-means	O
algorithm	O
there	O
is	O
a	O
popular	O
variant	O
of	O
the	O
em	O
algorithm	O
for	O
gmms	O
known	O
as	O
the	O
k-means	O
algorithm	O
,	O
which	O
we	O
now	O
discuss	O
.	O
consider	O
a	O
gmm	O
in	O
which	O
we	O
make	O
the	O
following	O
assumptions	O
:	O
σk	O
=	O
σ2id	O
is	O
ﬁxed	O
,	O
and	O
πk	O
=	O
1/k	O
is	O
ﬁxed	O
,	O
so	O
only	O
the	O
cluster	O
centers	O
,	O
μk	O
∈	O
r	O
d	O
,	O
have	O
to	O
be	O
estimated	O
.	O
now	O
consider	O
the	O
following	O
delta-function	O
approximation	O
to	O
the	O
posterior	O
computed	O
during	O
the	O
e	O
step	O
:	O
p	O
(	O
zi	O
=	O
k|xi	O
,	O
θ	O
)	O
≈	O
i	O
(	O
k	O
=	O
z∗	O
i	O
)	O
(	O
11.34	O
)	O
where	O
zi∗	O
=	O
argmaxk	O
p	O
(	O
zi	O
=	O
k|xi	O
,	O
θ	O
)	O
.	O
this	O
is	O
sometimes	O
called	O
hard	O
em	O
,	O
since	O
we	O
are	O
making	O
a	O
hard	O
assignment	O
of	O
points	O
to	O
clusters	B
.	O
since	O
we	O
assumed	O
an	O
equal	O
spherical	O
covariance	B
matrix	I
for	O
each	O
cluster	O
,	O
the	O
most	O
probable	O
cluster	O
for	O
xi	O
can	O
be	O
computed	O
by	O
ﬁnding	O
the	O
nearest	O
prototype	O
:	O
z∗	O
i	O
=	O
arg	O
min	O
k	O
||xi	O
−	O
μk||2	O
2	O
(	O
11.35	O
)	O
hence	O
in	O
each	O
e	O
step	O
,	O
we	O
must	O
ﬁnd	O
the	O
euclidean	O
distance	O
between	O
n	O
data	O
points	O
and	O
k	O
cluster	O
centers	O
,	O
which	O
takes	O
o	O
(	O
n	O
kd	O
)	O
time	O
.	O
however	O
,	O
this	O
can	O
be	O
sped	O
up	O
using	O
various	O
techniques	O
,	O
such	O
as	O
applying	O
the	O
triangle	B
inequality	I
to	O
avoid	O
some	O
redundant	O
computations	O
(	O
elkan	O
2003	O
)	O
.	O
given	O
the	O
hard	O
cluster	O
assignments	O
,	O
the	O
m	O
step	O
updates	O
each	O
cluster	O
center	O
by	O
computing	O
the	O
mean	B
of	O
all	O
points	O
assigned	O
to	O
it	O
:	O
μk	O
=	O
1	O
nk	O
xi	O
(	O
11.36	O
)	O
(	O
cid:2	O
)	O
i	O
:	O
zi=k	O
see	O
algorithm	O
5	O
for	O
the	O
pseudo-code	O
.	O
11.4.	O
the	O
em	O
algorithm	O
353	O
2	O
0	O
−2	O
−2	O
0	O
2	O
(	O
a	O
)	O
(	O
c	O
)	O
(	O
e	O
)	O
(	O
b	O
)	O
(	O
d	O
)	O
(	O
f	O
)	O
figure	O
11.11	O
illustration	O
of	O
the	O
em	O
for	O
a	O
gmm	O
applied	O
to	O
the	O
old	O
faithful	B
data	O
.	O
(	O
a	O
)	O
initial	O
(	O
random	O
)	O
values	O
of	O
the	O
parameters	O
.	O
(	O
b	O
)	O
posterior	O
responsibility	O
of	O
each	O
point	O
computed	O
in	O
the	O
ﬁrst	O
e	O
step	O
.	O
the	O
degree	B
of	O
redness	O
indicates	O
the	O
degree	B
to	O
which	O
the	O
point	O
belongs	O
to	O
the	O
red	O
cluster	O
,	O
and	O
similarly	O
for	O
blue	O
;	O
this	O
purple	O
points	O
have	O
a	O
roughly	O
uniform	O
posterior	O
over	O
clusters	B
.	O
(	O
c	O
)	O
we	O
show	O
the	O
updated	O
parameters	O
after	O
the	O
ﬁrst	O
m	O
step	O
.	O
(	O
d	O
)	O
after	O
3	O
iterations	O
.	O
(	O
e	O
)	O
after	O
5	O
iterations	O
.	O
(	O
f	O
)	O
after	O
16	O
iterations	O
.	O
based	O
on	O
(	O
bishop	O
2006a	O
)	O
figure	O
9.8.	O
figure	O
generated	O
by	O
mixgaussdemofaithful	O
.	O
354	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
algorithm	O
11.1	O
:	O
k-means	O
algorithm	O
1	O
initialize	O
mk	O
;	O
2	O
repeat	O
3	O
4	O
assign	O
each	O
data	O
point	O
to	O
its	O
closest	O
cluster	O
center	O
:	O
zi	O
=	O
arg	O
mink	O
||xi	O
−	O
μk||2	O
2	O
;	O
update	O
each	O
cluster	O
center	O
by	O
computing	O
the	O
mean	B
of	O
all	O
points	O
assigned	O
to	O
it	O
:	O
μk	O
=	O
1	O
nk	O
i	O
:	O
zi=k	O
xi	O
;	O
(	O
cid:10	O
)	O
5	O
until	O
converged	O
;	O
k=2	O
20	O
40	O
60	O
80	O
100	O
120	O
140	O
160	O
180	O
200	O
50	O
100	O
150	O
200	O
250	O
300	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
11.12	O
an	O
image	O
compressed	O
using	O
vector	B
quantization	I
with	O
a	O
codebook	B
of	O
size	O
k.	O
(	O
a	O
)	O
k	O
=	O
2	O
.	O
(	O
b	O
)	O
k	O
=	O
4.	O
figure	O
generated	O
by	O
vqdemo	O
.	O
11.4.2.6	O
vector	B
quantization	I
since	O
k-means	O
is	O
not	O
a	O
proper	O
em	O
algorithm	O
,	O
it	O
is	O
not	O
maximizing	O
likelihood	B
.	O
instead	O
,	O
it	O
can	O
be	O
interpreted	O
as	O
a	O
greedy	O
algorithm	O
for	O
approximately	O
minimizing	O
a	O
loss	B
function	I
related	O
to	O
data	B
compression	I
,	O
as	O
we	O
now	O
explain	O
.	O
suppose	O
we	O
want	O
to	O
perform	O
lossy	B
compression	I
of	O
some	O
real-valued	O
vectors	O
,	O
xi	O
∈	O
r	O
d.	O
a	O
very	O
simple	O
approach	O
to	O
this	O
is	O
to	O
use	O
vector	B
quantization	I
or	O
vq	O
.	O
the	O
basic	O
idea	O
is	O
to	O
replace	O
each	O
real-valued	O
vector	O
xi	O
∈	O
r	O
d	O
with	O
a	O
discrete	B
symbol	O
zi	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
which	O
is	O
an	O
index	O
into	O
a	O
codebook	B
of	O
k	O
prototypes	O
,	O
μk	O
∈	O
r	O
d.	O
each	O
data	O
vector	O
is	O
encoded	O
by	O
using	O
the	O
index	O
of	O
the	O
most	O
similar	B
prototype	O
,	O
where	O
similarity	O
is	O
measured	O
in	O
terms	O
of	O
euclidean	O
distance	O
:	O
encode	O
(	O
xi	O
)	O
=	O
arg	O
min	O
k	O
||xi	O
−	O
μk||2	O
(	O
11.37	O
)	O
we	O
can	O
deﬁne	O
a	O
cost	O
function	O
that	O
measures	O
the	O
quality	O
of	O
a	O
codebook	B
by	O
computing	O
the	O
reconstruction	B
error	I
or	O
distortion	B
it	O
induces	O
:	O
j	O
(	O
μ	O
,	O
z|k	O
,	O
x	O
)	O
(	O
cid:2	O
)	O
1	O
n	O
||xi	O
−	O
decode	O
(	O
encode	O
(	O
xi	O
)	O
)	O
||2	O
=	O
1	O
n	O
||xi	O
−	O
μzi	O
||2	O
(	O
11.38	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
n	O
(	O
cid:2	O
)	O
i=1	O
where	O
decode	O
(	O
k	O
)	O
=	O
μk	O
.	O
the	O
k-means	O
algorithm	O
can	O
be	O
thought	O
of	O
as	O
a	O
simple	O
iterative	O
scheme	O
for	O
minimizing	O
this	O
objective	O
.	O
of	O
course	O
,	O
we	O
can	O
achieve	O
zero	O
distortion	O
if	O
we	O
assign	O
one	O
prototype	B
to	O
every	O
data	O
vector	O
,	O
but	O
that	O
takes	O
o	O
(	O
n	O
dc	O
)	O
space	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
real-valued	O
data	O
vectors	O
,	O
each	O
of	O
11.4.	O
the	O
em	O
algorithm	O
355	O
length	O
d	O
,	O
and	O
c	O
is	O
the	O
number	O
of	O
bits	B
needed	O
to	O
represent	O
a	O
real-valued	O
scalar	O
(	O
the	O
quantization	O
accuracy	O
)	O
.	O
however	O
,	O
in	O
many	O
data	O
sets	O
,	O
we	O
see	O
similar	B
vectors	O
repeatedly	O
,	O
so	O
rather	O
than	O
storing	O
them	O
many	O
times	O
,	O
we	O
can	O
store	O
them	O
once	O
and	O
then	O
create	O
pointers	O
to	O
them	O
.	O
hence	O
we	O
can	O
reduce	O
the	O
space	O
requirement	O
to	O
o	O
(	O
n	O
log2	O
k	O
+	O
kdc	O
)	O
:	O
the	O
o	O
(	O
n	O
log2	O
k	O
)	O
term	O
arises	O
because	O
each	O
of	O
the	O
n	O
data	O
vectors	O
needs	O
to	O
specify	O
which	O
of	O
the	O
k	O
codewords	O
it	O
is	O
using	O
(	O
the	O
pointers	O
)	O
;	O
and	O
the	O
o	O
(	O
kdc	O
)	O
term	O
arises	O
because	O
we	O
have	O
to	O
store	O
each	O
codebook	B
entry	O
,	O
each	O
of	O
which	O
is	O
a	O
d-dimensional	O
vector	O
.	O
typically	O
the	O
ﬁrst	O
term	O
dominates	B
the	O
second	O
,	O
so	O
we	O
can	O
approximate	O
the	O
rate	B
of	O
the	O
encoding	O
scheme	O
(	O
number	O
of	O
bits	B
needed	O
per	O
object	O
)	O
as	O
o	O
(	O
log2	O
k	O
)	O
,	O
which	O
is	O
typically	O
much	O
less	O
than	O
o	O
(	O
dc	O
)	O
.	O
one	O
application	O
of	O
vq	O
is	O
to	O
image	B
compression	I
.	O
consider	O
the	O
n	O
=	O
200×	O
320	O
=	O
64	O
,	O
000	O
pixel	O
image	O
in	O
figure	O
11.12	O
;	O
this	O
is	O
gray-scale	O
,	O
so	O
d	O
=	O
1.	O
if	O
we	O
use	O
one	O
byte	O
to	O
represent	O
each	O
pixel	O
(	O
a	O
gray-scale	O
intensity	O
of	O
0	O
to	O
255	O
)	O
,	O
then	O
c	O
=	O
8	O
,	O
so	O
we	O
need	O
n	O
c	O
=	O
512	O
,	O
000	O
bits	B
to	O
represent	O
the	O
image	O
.	O
for	O
the	O
compressed	O
image	O
,	O
we	O
need	O
n	O
log2	O
k	O
+	O
kc	O
bits	B
.	O
for	O
k	O
=	O
4	O
,	O
this	O
is	O
about	O
128kb	O
,	O
a	O
factor	B
of	O
4	O
compression	O
.	O
for	O
k	O
=	O
8	O
,	O
this	O
is	O
about	O
192kb	O
,	O
a	O
factor	B
of	O
2.6	O
compression	O
,	O
at	O
negligible	O
perceptual	O
loss	O
(	O
see	O
figure	O
11.12	O
(	O
b	O
)	O
)	O
.	O
greater	O
compression	O
could	O
be	O
achieved	O
if	O
we	O
modelled	O
spatial	O
correlation	O
between	O
the	O
pixels	O
,	O
e.g.	O
,	O
if	O
we	O
encoded	O
5x5	O
blocks	O
(	O
as	O
used	O
by	O
jpeg	O
)	O
.	O
this	O
is	O
because	O
the	O
residual	B
errors	O
(	O
differences	O
from	O
the	O
model	O
’	O
s	O
predictions	O
)	O
would	O
be	O
smaller	O
,	O
and	O
would	O
take	O
fewer	O
bits	B
to	O
encode	O
.	O
11.4.2.7	O
initialization	O
and	O
avoiding	O
local	O
minima	O
both	O
k-means	O
and	O
em	O
need	O
to	O
be	O
initialized	O
.	O
it	O
is	O
common	O
to	O
pick	O
k	O
data	O
points	O
at	O
random	O
,	O
and	O
to	O
make	O
these	O
be	O
the	O
initial	O
cluster	O
centers	O
.	O
or	O
we	O
can	O
pick	O
the	O
centers	O
sequentially	O
so	O
as	O
to	O
try	O
to	O
“	O
cover	O
”	O
the	O
data	O
.	O
that	O
is	O
,	O
we	O
pick	O
the	O
initial	O
point	O
uniformly	O
at	O
random	O
.	O
then	O
each	O
subsequent	O
point	O
is	O
picked	O
from	O
the	O
remaining	O
points	O
with	O
probability	O
proportional	O
to	O
its	O
squared	O
distance	O
to	O
the	O
points	O
’	O
s	O
closest	O
cluster	O
center	O
.	O
this	O
is	O
known	O
as	O
farthest	B
point	I
clustering	I
(	O
gonzales	O
1985	O
)	O
,	O
or	O
k-means++	B
(	O
arthur	O
and	O
vassilvitskii	O
2007	O
;	O
bahmani	O
et	O
al	O
.	O
2012	O
)	O
.	O
surprisingly	O
,	O
this	O
simple	O
trick	O
can	O
be	O
shown	O
to	O
guarantee	O
that	O
the	O
distortion	B
is	O
never	O
more	O
than	O
o	O
(	O
log	O
k	O
)	O
worse	O
than	O
optimal	O
(	O
arthur	O
and	O
vassilvitskii	O
2007	O
)	O
.	O
an	O
heuristic	O
that	O
is	O
commonly	O
used	O
in	O
the	O
speech	B
recognition	I
community	O
is	O
to	O
incrementally	O
“	O
grow	O
”	O
gmms	O
:	O
we	O
initially	O
give	O
each	O
cluster	O
a	O
score	O
based	O
on	O
its	O
mixture	B
weight	O
;	O
after	O
each	O
round	O
of	O
training	O
,	O
we	O
consider	O
splitting	O
the	O
cluster	O
with	O
the	O
highest	O
score	O
into	O
two	O
,	O
with	O
the	O
new	O
centroids	B
being	O
random	O
perturbations	O
of	O
the	O
original	O
centroid	B
,	O
and	O
the	O
new	O
scores	B
being	O
half	O
of	O
the	O
old	O
scores	B
.	O
if	O
a	O
new	O
cluster	O
has	O
too	O
small	O
a	O
score	O
,	O
or	O
too	O
narrow	O
a	O
variance	B
,	O
it	O
is	O
removed	O
.	O
we	O
continue	O
in	O
this	O
way	O
until	O
the	O
desired	O
number	O
of	O
clusters	B
is	O
reached	O
.	O
see	O
(	O
figueiredo	O
and	O
jain	O
2002	O
)	O
for	O
a	O
similar	B
incremental	O
approach	O
.	O
11.4.2.8	O
map	O
estimation	O
as	O
usual	O
,	O
the	O
mle	O
may	O
overﬁt	B
.	O
the	O
overﬁtting	B
problem	O
is	O
particularly	O
severe	O
in	O
the	O
case	O
of	O
gmms	O
.	O
to	O
understand	O
the	O
problem	O
,	O
suppose	O
for	O
simplicity	O
that	O
σk	O
=	O
σ2	O
ki	O
,	O
and	O
that	O
k	O
=	O
2.	O
it	O
is	O
possible	O
to	O
get	O
an	O
inﬁnite	O
likelihood	O
by	O
assigning	O
one	O
of	O
the	O
centers	O
,	O
say	O
μ2	O
,	O
to	O
a	O
single	O
data	O
point	O
,	O
say	O
x1	O
,	O
since	O
then	O
the	O
1st	O
term	O
makes	O
the	O
following	O
contribution	O
to	O
the	O
likelihood	B
:	O
n	O
(	O
x1|μ2	O
,	O
σ2	O
2i	O
)	O
=	O
1	O
(	O
cid:17	O
)	O
2πσ2	O
2	O
e0	O
(	O
11.39	O
)	O
356	O
)	O
x	O
(	O
p	O
x	O
(	O
a	O
)	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
s	O
l	O
i	O
a	O
f	O
m	O
m	O
g	O
r	O
o	O
f	O
m	O
e	O
s	O
e	O
m	O
i	O
t	O
f	O
o	O
n	O
o	O
i	O
t	O
c	O
a	O
r	O
f	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
10	O
20	O
30	O
40	O
mle	O
map	O
70	O
80	O
90	O
100	O
50	O
60	O
dimensionality	O
(	O
b	O
)	O
figure	O
11.13	O
(	O
a	O
)	O
illustration	O
of	O
how	O
singularities	O
can	O
arise	O
in	O
the	O
likelihood	B
function	O
of	O
gmms	O
.	O
based	O
on	O
(	O
bishop	O
2006a	O
)	O
figure	O
9.7.	O
figure	O
generated	O
by	O
mixgausssingularity	O
.	O
(	O
b	O
)	O
illustration	O
of	O
the	O
beneﬁt	O
of	O
map	O
estimation	O
vs	O
ml	O
estimation	O
when	O
ﬁtting	O
a	O
gaussian	O
mixture	B
model	I
.	O
we	O
plot	O
the	O
fraction	O
of	O
times	O
(	O
out	O
of	O
5	O
random	O
trials	O
)	O
each	O
method	O
encounters	O
numerical	O
problems	O
vs	O
the	O
dimensionality	O
of	O
the	O
problem	O
,	O
for	O
n	O
=	O
100	O
samples	B
.	O
solid	O
red	O
(	O
upper	O
curve	O
)	O
:	O
mle	O
.	O
dotted	O
black	O
(	O
lower	O
curve	O
)	O
:	O
map	O
.	O
figure	O
generated	O
by	O
mixgaussmlvsmap	O
.	O
hence	O
we	O
can	O
drive	O
this	O
term	O
to	O
inﬁnity	O
by	O
letting	O
σ2	O
→	O
0	O
,	O
as	O
shown	O
in	O
figure	O
11.13	O
(	O
a	O
)	O
.	O
we	O
will	O
call	O
this	O
the	O
“	O
collapsing	O
variance	B
problem	O
”	O
.	O
an	O
easy	O
solution	O
to	O
this	O
is	O
to	O
perform	O
map	O
estimation	O
.	O
the	O
new	O
auxiliary	B
function	I
is	O
the	O
expected	O
complete	O
data	O
log-likelihood	O
plus	O
the	O
log	O
prior	O
:	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
q	O
(	O
cid:4	O
)	O
(	O
θ	O
,	O
θold	O
)	O
=	O
rik	O
log	O
πik	O
+	O
i	O
k	O
i	O
k	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
rik	O
log	O
p	O
(	O
xi|θk	O
)	O
(	O
cid:19	O
)	O
+	O
log	O
p	O
(	O
π	O
)	O
+	O
(	O
cid:2	O
)	O
k	O
log	O
p	O
(	O
θk	O
)	O
(	O
11.40	O
)	O
note	O
that	O
the	O
e	O
step	O
remains	O
unchanged	O
,	O
but	O
the	O
m	O
step	O
needs	O
to	O
be	O
modiﬁed	O
,	O
as	O
we	O
now	O
explain	O
.	O
for	O
the	O
prior	O
on	O
the	O
mixture	B
weights	O
,	O
it	O
is	O
natural	O
to	O
use	O
a	O
dirichlet	O
prior	O
,	O
π	O
∼	O
dir	O
(	O
α	O
)	O
,	O
since	O
this	O
is	O
conjugate	O
to	O
the	O
categorical	B
distribution	O
.	O
the	O
map	O
estimate	O
is	O
given	O
by	O
πk	O
=	O
(	O
cid:10	O
)	O
rk	O
+	O
αk	O
−	O
1	O
k	O
αk	O
−	O
k	O
n	O
+	O
(	O
11.41	O
)	O
if	O
we	O
use	O
a	O
uniform	O
prior	O
,	O
αk	O
=	O
1	O
,	O
this	O
reduces	O
to	O
equation	O
11.28.	O
the	O
prior	O
on	O
the	O
parameters	O
of	O
the	O
class	O
conditional	O
densities	O
,	O
p	O
(	O
θk	O
)	O
,	O
depends	O
on	O
the	O
form	O
of	O
the	O
class	O
conditional	O
densities	O
.	O
we	O
discuss	O
the	O
case	O
of	O
gmms	O
below	O
,	O
and	O
leave	O
map	O
estimation	O
for	O
mixtures	O
of	O
bernoullis	O
to	O
exercise	O
11.3.	O
for	O
simplicity	O
,	O
let	O
us	O
consider	O
a	O
conjugate	B
prior	I
of	O
the	O
form	O
p	O
(	O
μk	O
,	O
σk	O
)	O
=	O
niw	O
(	O
μk	O
,	O
σk|m0	O
,	O
κ0	O
,	O
ν0	O
,	O
s0	O
)	O
(	O
11.42	O
)	O
11.4.	O
the	O
em	O
algorithm	O
from	O
section	O
4.6.3	O
,	O
the	O
map	O
estimate	O
is	O
given	O
by	O
ˆμk	O
=	O
xk	O
(	O
cid:2	O
)	O
ˆσk	O
=	O
sk	O
(	O
cid:2	O
)	O
rkxk	O
+	O
κ0m0	O
(	O
cid:10	O
)	O
rk	O
+	O
κ0	O
i	O
rikxi	O
rk	O
(	O
xk	O
−	O
m0	O
)	O
(	O
xk	O
−	O
m0	O
)	O
t	O
s0	O
+	O
sk	O
+	O
κ0rk	O
κ0+rk	O
(	O
cid:2	O
)	O
ν0	O
+	O
rk	O
+	O
d	O
+	O
2	O
rik	O
(	O
xi	O
−	O
xk	O
)	O
(	O
xi	O
−	O
xk	O
)	O
t	O
357	O
(	O
11.43	O
)	O
(	O
11.44	O
)	O
(	O
11.45	O
)	O
(	O
11.46	O
)	O
(	O
11.47	O
)	O
i	O
we	O
now	O
illustrate	O
the	O
beneﬁts	O
of	O
using	O
map	O
estimation	O
instead	O
of	O
ml	O
estimation	O
in	O
the	O
context	O
of	O
gmms	O
.	O
we	O
apply	O
em	O
to	O
some	O
synthetic	O
data	O
in	O
d	O
dimensions	O
,	O
using	O
either	O
ml	O
or	O
map	O
estimation	O
.	O
we	O
count	O
the	O
trial	O
as	O
a	O
“	O
failure	O
”	O
if	O
there	O
are	O
numerical	O
issues	O
involving	O
singular	O
matrices	O
.	O
for	O
each	O
dimensionality	O
,	O
we	O
conduct	O
5	O
random	O
trials	O
.	O
the	O
results	O
are	O
illustrated	O
in	O
figure	O
11.13	O
(	O
b	O
)	O
using	O
n	O
=	O
100.	O
we	O
see	O
that	O
as	O
soon	O
as	O
d	O
becomes	O
even	O
moderately	O
large	O
,	O
ml	O
estimation	O
crashes	O
and	O
burns	O
,	O
whereas	O
map	O
estimation	O
never	O
encounters	O
numerical	O
problems	O
.	O
when	O
using	O
map	O
estimation	O
,	O
we	O
need	O
to	O
specify	O
the	O
hyper-parameters	B
.	O
here	O
we	O
mention	O
some	O
simple	O
heuristics	O
for	O
setting	O
them	O
(	O
fraley	O
and	O
raftery	O
2007	O
,	O
p163	O
)	O
.	O
we	O
can	O
set	O
κ0	O
=	O
0	O
,	O
so	O
that	O
the	O
μk	O
are	O
unregularized	O
,	O
since	O
the	O
numerical	O
problems	O
only	O
arise	O
from	O
σk	O
.	O
in	O
this	O
case	O
,	O
the	O
map	O
estimates	O
simplify	O
to	O
ˆμk	O
=	O
xk	O
and	O
ˆσk	O
=	O
ν0+rk+d+2	O
,	O
which	O
is	O
not	O
quite	O
so	O
scary-looking	O
.	O
s0+sk	O
now	O
we	O
discuss	O
how	O
to	O
set	O
s0	O
.	O
one	O
possibility	O
is	O
to	O
use	O
1	O
d	O
)	O
s0	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
s2	O
k	O
1/d	O
diag	O
(	O
s2	O
(	O
cid:10	O
)	O
n	O
i=1	O
(	O
xij	O
−	O
xj	O
)	O
2	O
is	O
the	O
pooled	B
variance	O
for	O
dimension	O
j.	O
where	O
sj	O
=	O
(	O
1/n	O
)	O
(	O
the	O
reason	O
k1/d	O
term	O
is	O
that	O
the	O
resulting	O
volume	O
of	O
each	O
ellipsoid	O
is	O
then	O
given	O
by	O
|s0|	O
=	O
for	O
the	O
d	O
)	O
|	O
.	O
)	O
the	O
parameter	B
ν0	O
controls	O
how	O
strongly	O
we	O
believe	O
this	O
prior	O
.	O
the	O
k|diag	O
(	O
s2	O
1	O
1	O
,	O
.	O
.	O
.	O
,	O
s2	O
weakest	O
prior	O
we	O
can	O
use	O
,	O
while	O
still	O
being	O
proper	O
,	O
is	O
to	O
set	O
ν0	O
=	O
d	O
+	O
2	O
,	O
so	O
this	O
is	O
a	O
common	O
choice	O
.	O
(	O
11.48	O
)	O
1	O
11.4.3	O
em	O
for	O
mixture	B
of	I
experts	I
we	O
can	O
ﬁt	O
a	O
mixture	B
of	I
experts	I
model	O
using	O
em	O
in	O
a	O
straightforward	O
manner	O
.	O
the	O
expected	B
complete	I
data	I
log	I
likelihood	I
is	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
k	O
(	O
cid:2	O
)	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
rik	O
log	O
[	O
πikn	O
(	O
yi|wt	O
k	O
xi	O
,	O
σ2	O
k	O
)	O
]	O
i=1	O
k=1	O
πi	O
,	O
k	O
(	O
cid:2	O
)	O
s	O
(	O
vt	O
xi	O
)	O
k	O
rik	O
∝	O
πold	O
ik	O
n	O
(	O
yi|xt	O
i	O
wold	O
k	O
,	O
(	O
σold	O
k	O
)	O
2	O
)	O
so	O
the	O
e	O
step	O
is	O
the	O
same	O
as	O
in	O
a	O
standard	O
mixture	O
model	O
,	O
except	O
we	O
have	O
to	O
replace	O
πk	O
with	O
πi	O
,	O
k	O
when	O
computing	O
rik	O
.	O
(	O
11.49	O
)	O
(	O
11.50	O
)	O
(	O
11.51	O
)	O
358	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
in	O
the	O
m	O
step	O
,	O
we	O
need	O
to	O
maximize	O
q	O
(	O
θ	O
,	O
θold	O
)	O
wrt	O
wk	O
,	O
σ2	O
k	O
and	O
v.	O
for	O
the	O
regression	B
parameters	O
for	O
model	O
k	O
,	O
the	O
objective	O
has	O
the	O
form	O
q	O
(	O
θk	O
,	O
θold	O
)	O
=	O
rik	O
(	O
yi	O
−	O
wt	O
k	O
xi	O
)	O
−	O
1	O
σ2	O
k	O
(	O
cid:26	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
*	O
if	O
rik	O
is	O
we	O
recognize	O
this	O
as	O
a	O
weighted	B
least	I
squares	I
problem	I
,	O
which	O
makes	O
intuitive	O
sense	O
:	O
small	O
,	O
then	O
data	O
point	O
i	O
will	O
be	O
downweighted	O
when	O
estimating	O
model	O
k	O
’	O
s	O
parameters	O
.	O
from	O
section	O
8.3.4	O
we	O
can	O
immediately	O
write	O
down	O
the	O
mle	O
as	O
wk	O
=	O
(	O
xt	O
rkx	O
)	O
−1xt	O
rky	O
where	O
rk	O
=	O
diag	O
(	O
r	O
:	O
,k	O
)	O
.	O
the	O
mle	O
for	O
the	O
variance	B
is	O
given	O
by	O
(	O
cid:10	O
)	O
n	O
i=1	O
rik	O
(	O
yi	O
−	O
wt	O
i=1	O
rik	O
(	O
cid:10	O
)	O
n	O
k	O
xi	O
)	O
2	O
σ2	O
k	O
=	O
we	O
replace	O
the	O
estimate	O
of	O
the	O
unconditional	O
mixing	B
weights	I
π	O
with	O
the	O
estimate	O
of	O
the	O
gating	O
parameters	O
,	O
v.	O
the	O
objective	O
has	O
the	O
form	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:6	O
)	O
(	O
v	O
)	O
=	O
rik	O
log	O
πi	O
,	O
k	O
(	O
11.55	O
)	O
i	O
k	O
we	O
recognize	O
this	O
as	O
equivalent	O
to	O
the	O
log-likelihood	O
for	O
multinomial	B
logistic	I
regression	I
in	O
equation	O
8.34	O
,	O
except	O
we	O
replace	O
the	O
“	O
hard	O
”	O
1-of-c	O
encoding	O
yi	O
with	O
the	O
“	O
soft	O
”	O
1-of-k	O
encoding	O
ri	O
.	O
thus	O
we	O
can	O
estimate	O
v	O
by	O
ﬁtting	O
a	O
logistic	B
regression	I
model	O
to	O
soft	O
target	O
labels	O
.	O
11.4.4	O
em	O
for	O
dgms	O
with	O
hidden	B
variables	I
we	O
can	O
generalize	B
the	O
ideas	O
behind	O
em	O
for	O
mixtures	O
of	O
experts	O
to	O
compute	O
the	O
mle	O
or	O
map	O
estimate	O
for	O
an	O
arbitrary	O
dgm	O
.	O
we	O
could	O
use	O
gradient-based	O
methods	O
(	O
binder	O
et	O
al	O
.	O
1997	O
)	O
,	O
but	O
it	O
is	O
much	O
simpler	O
to	O
use	O
em	O
(	O
lauritzen	O
1995	O
)	O
:	O
in	O
the	O
e	O
step	O
,	O
we	O
just	O
estimate	O
the	O
hidden	B
variables	I
,	O
and	O
in	O
the	O
m	O
step	O
,	O
we	O
will	O
compute	O
the	O
mle	O
using	O
these	O
ﬁlled-in	O
values	O
.	O
we	O
give	O
the	O
details	O
below	O
.	O
for	O
simplicity	O
of	O
presentation	O
,	O
we	O
will	O
assume	O
all	O
cpds	O
are	O
tabular	O
.	O
based	O
on	O
section	O
10.4.2	O
,	O
(	O
11.52	O
)	O
(	O
11.53	O
)	O
(	O
11.54	O
)	O
(	O
11.56	O
)	O
(	O
11.57	O
)	O
let	O
us	O
write	O
each	O
cpt	O
as	O
follows	O
:	O
p	O
(	O
xit|xi	O
,	O
pa	O
(	O
t	O
)	O
,	O
θt	O
)	O
=	O
θi	O
(	O
xit=i	O
,	O
xi	O
,	O
pa	O
(	O
t	O
)	O
=c	O
)	O
tck	O
kpa	O
(	O
t	O
)	O
(	O
cid:27	O
)	O
kt	O
(	O
cid:27	O
)	O
c=1	O
k=1	O
the	O
log-likelihood	O
of	O
the	O
complete	B
data	I
is	O
given	O
by	O
log	O
p	O
(	O
d|θ	O
)	O
=	O
ntck	O
log	O
θtck	O
kpa	O
(	O
t	O
)	O
(	O
cid:2	O
)	O
kt	O
(	O
cid:2	O
)	O
c=1	O
k=1	O
v	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
n	O
t=1	O
where	O
ntck	O
=	O
complete	B
data	I
log-likelihood	O
has	O
the	O
form	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
e	O
[	O
log	O
p	O
(	O
d|θ	O
)	O
]	O
=	O
t	O
c	O
k	O
i=1	O
i	O
(	O
xit	O
=	O
i	O
,	O
xi	O
,	O
pa	O
(	O
t	O
)	O
=	O
c	O
)	O
are	O
the	O
empirical	O
counts	O
.	O
hence	O
the	O
expected	O
n	O
tck	O
log	O
θtck	O
(	O
11.58	O
)	O
11.4.	O
the	O
em	O
algorithm	O
where	O
n	O
(	O
cid:2	O
)	O
(	O
cid:31	O
)	O
p	O
(	O
xit	O
=	O
k	O
,	O
xi	O
,	O
pa	O
(	O
t	O
)	O
=	O
c|di	O
)	O
359	O
(	O
11.59	O
)	O
(	O
cid:2	O
)	O
e	O
n	O
tck	O
=	O
i	O
(	O
xit	O
=	O
i	O
,	O
xi	O
,	O
pa	O
(	O
t	O
)	O
=	O
c	O
)	O
=	O
where	O
di	O
are	O
all	O
the	O
visible	B
variables	I
in	O
case	O
i.	O
i=1	O
i	O
the	O
quantity	O
p	O
(	O
xit	O
,	O
xi	O
,	O
pa	O
(	O
t	O
)	O
|di	O
,	O
θ	O
)	O
is	O
known	O
as	O
a	O
family	B
marginal	I
,	O
and	O
can	O
be	O
computed	O
using	O
any	O
gm	O
inference	B
algorithm	O
.	O
the	O
n	O
tjk	O
are	O
the	O
expected	B
sufficient	I
statistics	I
,	O
and	O
constitute	O
the	O
output	O
of	O
the	O
e	O
step	O
.	O
given	O
these	O
ess	O
,	O
the	O
m	O
step	O
has	O
the	O
simple	O
form	O
ˆθtck	O
=	O
n	O
tck	O
(	O
cid:10	O
)	O
k	O
(	O
cid:2	O
)	O
n	O
tjk	O
(	O
cid:2	O
)	O
k	O
θtjk	O
=	O
1	O
)	O
this	O
can	O
be	O
proved	O
by	O
adding	O
lagrange	O
multipliers	O
(	O
to	O
enforce	O
the	O
constraint	O
to	O
the	O
expected	B
complete	I
data	I
log	I
likelihood	I
,	O
and	O
then	O
optimizing	O
each	O
parameter	B
vector	O
θtc	O
separately	O
.	O
we	O
can	O
modify	O
this	O
to	O
perform	O
map	O
estimation	O
with	O
a	O
dirichlet	O
prior	O
by	O
simply	O
adding	O
pseudo	B
counts	I
to	O
the	O
expected	O
counts	O
.	O
(	O
11.60	O
)	O
(	O
cid:10	O
)	O
11.4.5	O
em	O
for	O
the	O
student	O
distribution	O
*	O
one	O
problem	O
with	O
the	O
gaussian	O
distribution	O
is	O
that	O
it	O
is	O
sensitive	O
to	O
outliers	B
,	O
since	O
the	O
log-	O
probability	O
only	O
decays	O
quadratically	O
with	O
distance	O
from	O
the	O
center	O
.	O
a	O
more	O
robust	B
alternative	O
is	O
the	O
student	O
t	O
distribution	O
,	O
as	O
discussed	O
in	O
section	O
?	O
?	O
.	O
unlike	O
the	O
case	O
of	O
a	O
gaussian	O
,	O
there	O
is	O
no	O
closed	O
form	O
formula	O
for	O
the	O
mle	O
of	O
a	O
student	O
,	O
even	O
if	O
we	O
have	O
no	O
missing	O
data	O
,	O
so	O
we	O
must	O
resort	O
to	O
iterative	O
optimization	O
methods	O
.	O
the	O
easiest	O
one	O
to	O
use	O
is	O
em	O
,	O
since	O
it	O
automatically	O
enforces	O
the	O
constraints	O
that	O
ν	O
is	O
positive	O
and	O
that	O
σ	O
is	O
symmetric	B
positive	O
deﬁnite	O
.	O
in	O
addition	O
,	O
the	O
resulting	O
algorithm	O
turns	O
out	O
to	O
have	O
a	O
simple	O
intuitive	O
form	O
,	O
as	O
we	O
see	O
below	O
.	O
at	O
ﬁrst	O
blush	O
,	O
it	O
might	O
not	O
be	O
apparent	O
why	O
em	O
can	O
be	O
used	O
,	O
since	O
there	O
is	O
no	O
missing	O
data	O
.	O
the	O
key	O
idea	O
is	O
to	O
introduce	O
an	O
“	O
artiﬁcial	O
”	O
hidden	B
or	O
auxiliary	O
variable	O
in	O
order	O
to	O
simplify	O
the	O
algorithm	O
.	O
in	O
particular	O
,	O
we	O
will	O
exploit	O
the	O
fact	O
that	O
a	O
student	O
distribution	O
can	O
be	O
written	O
as	O
a	O
gaussian	O
scale	O
mixture	O
:	O
t	O
(	O
xi|μ	O
,	O
σ	O
,	O
ν	O
)	O
=	O
n	O
(	O
xi|μ	O
,	O
σ/zi	O
)	O
ga	O
(	O
zi|	O
ν	O
2	O
,	O
ν	O
2	O
)	O
dzi	O
(	O
11.61	O
)	O
(	O
cid:28	O
)	O
(	O
see	O
exercise	O
11.1	O
for	O
a	O
proof	O
of	O
this	O
in	O
the	O
1d	O
case	O
.	O
)	O
this	O
can	O
be	O
thought	O
of	O
as	O
an	O
“	O
inﬁnite	O
”	O
mixture	O
of	O
gaussians	O
,	O
each	O
one	O
with	O
a	O
slightly	O
different	O
covariance	B
matrix	I
.	O
treating	O
the	O
zi	O
as	O
missing	B
data	I
,	O
we	O
can	O
write	O
the	O
complete	B
data	I
log	I
likelihood	I
as	O
(	O
cid:6	O
)	O
c	O
(	O
θ	O
)	O
=	O
=	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
i=1	O
ν	O
+	O
2	O
[	O
log	O
n	O
(	O
xi|μ	O
,	O
σ/zi	O
)	O
+	O
log	O
ga	O
(	O
zi|ν/2	O
,	O
ν/2	O
)	O
]	O
(	O
cid:29	O
)	O
−	O
d	O
2	O
log	O
(	O
2π	O
)	O
−	O
1	O
2	O
(	O
log	O
zi	O
−	O
zi	O
)	O
+	O
(	O
d	O
2	O
log	O
|σ|	O
−	O
zi	O
(	O
cid:30	O
)	O
2	O
−	O
1	O
)	O
log	O
zi	O
δi	O
+	O
log	O
ν	O
2	O
ν	O
2	O
−	O
log	O
γ	O
(	O
ν	O
2	O
)	O
(	O
11.62	O
)	O
(	O
11.63	O
)	O
(	O
11.64	O
)	O
360	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
where	O
we	O
have	O
deﬁned	O
the	O
mahalanobis	O
distance	O
to	O
be	O
δi	O
=	O
(	O
xi	O
−	O
μ	O
)	O
t	O
σ−1	O
(	O
xi	O
−	O
μ	O
)	O
(	O
11.65	O
)	O
we	O
can	O
partition	O
this	O
into	O
two	O
terms	O
,	O
one	O
involving	O
μ	O
and	O
σ	O
,	O
and	O
the	O
other	O
involving	O
ν.	O
we	O
have	O
,	O
dropping	O
irrelevant	O
constants	O
,	O
ln	O
(	O
μ	O
,	O
σ	O
)	O
(	O
cid:2	O
)	O
−	O
1	O
2	O
(	O
cid:6	O
)	O
c	O
(	O
θ	O
)	O
=l	O
n	O
(	O
μ	O
,	O
σ	O
)	O
+l	O
g	O
(	O
ν	O
)	O
n	O
log	O
|σ|	O
−	O
1	O
2	O
lg	O
(	O
ν	O
)	O
(	O
cid:2	O
)	O
−n	O
log	O
γ	O
(	O
ν/2	O
)	O
+	O
n	O
(	O
cid:2	O
)	O
i=1	O
ziδi	O
1	O
2	O
n	O
ν	O
log	O
(	O
ν/2	O
)	O
+	O
(	O
11.66	O
)	O
(	O
11.67	O
)	O
(	O
11.68	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
ν	O
1	O
2	O
(	O
log	O
zi	O
−	O
zi	O
)	O
11.4.5.1	O
em	O
with	O
ν	O
known	O
let	O
us	O
ﬁrst	O
derive	O
the	O
algorithm	O
with	O
ν	O
assumed	O
known	O
,	O
for	O
simplicity	O
.	O
in	O
this	O
case	O
,	O
we	O
can	O
ignore	O
the	O
lg	O
term	O
,	O
so	O
we	O
only	O
need	O
to	O
ﬁgure	O
out	O
how	O
to	O
compute	O
e	O
[	O
zi	O
]	O
wrt	O
the	O
old	O
parameters	O
.	O
now	O
if	O
zi	O
∼	O
ga	O
(	O
a	O
,	O
b	O
)	O
,	O
then	O
e	O
[	O
zi	O
]	O
=	O
a/b	O
.	O
hence	O
the	O
e	O
step	O
at	O
iteration	O
t	O
is	O
from	O
section	O
4.6.2.2	O
we	O
have	O
p	O
(	O
zi|xi	O
,	O
θ	O
)	O
=	O
ga	O
(	O
zi|	O
ν	O
+	O
d	O
,	O
ν	O
+	O
δi	O
)	O
2	O
2	O
+	O
,	O
i	O
(	O
cid:2	O
)	O
e	O
z	O
(	O
t	O
)	O
zi|xi	O
,	O
θ	O
(	O
t	O
)	O
=	O
ν	O
(	O
t	O
)	O
+	O
d	O
ν	O
(	O
t	O
)	O
+	O
δ	O
(	O
t	O
)	O
i	O
the	O
m	O
step	O
is	O
obtained	O
by	O
maximizing	O
e	O
[	O
ln	O
(	O
μ	O
,	O
σ	O
)	O
]	O
to	O
yield	O
(	O
11.69	O
)	O
(	O
11.70	O
)	O
(	O
11.71	O
)	O
(	O
11.72	O
)	O
(	O
11.73	O
)	O
(	O
cid:10	O
)	O
(	O
cid:10	O
)	O
i	O
z	O
(	O
t	O
)	O
i	O
xi	O
(	O
cid:2	O
)	O
i	O
z	O
(	O
t	O
)	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
1	O
n	O
i	O
1	O
n	O
ˆμ	O
(	O
t+1	O
)	O
=	O
(	O
t+1	O
)	O
ˆσ	O
=	O
=	O
i	O
i	O
(	O
xi	O
−	O
ˆμ	O
(	O
t+1	O
)	O
)	O
(	O
xi	O
−	O
ˆμ	O
(	O
t+1	O
)	O
)	O
t	O
z	O
(	O
t	O
)	O
(	O
cid:11	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:13	O
)	O
z	O
(	O
t	O
)	O
i	O
xixt	O
i	O
−	O
i	O
i=1	O
z	O
(	O
t	O
)	O
i	O
ˆμ	O
(	O
t+1	O
)	O
(	O
ˆμ	O
(	O
t+1	O
)	O
)	O
t	O
(	O
cid:19	O
)	O
these	O
results	O
are	O
quite	O
intuitive	O
:	O
the	O
quantity	O
zi	O
is	O
the	O
precision	B
of	O
measurement	O
i	O
,	O
so	O
if	O
it	O
is	O
small	O
,	O
the	O
corresponding	O
data	O
point	O
is	O
down-weighted	O
when	O
estimating	O
the	O
mean	B
and	O
covariance	B
.	O
this	O
is	O
how	O
the	O
student	O
achieves	O
robustness	B
to	O
outliers	B
.	O
11.4.5.2	O
em	O
with	O
ν	O
unknown	B
to	O
compute	O
the	O
mle	O
for	O
the	O
degrees	B
of	I
freedom	I
,	O
we	O
ﬁrst	O
need	O
to	O
compute	O
the	O
expectation	O
of	O
lg	O
(	O
ν	O
)	O
,	O
which	O
involves	O
zi	O
and	O
log	O
zi	O
.	O
now	O
if	O
zi	O
∼	O
ga	O
(	O
a	O
,	O
b	O
)	O
,	O
then	O
one	O
can	O
show	O
that	O
i	O
(	O
cid:2	O
)	O
e	O
(	O
cid:6	O
)	O
(	O
t	O
)	O
log	O
zi|θ	O
(	O
t	O
)	O
=	O
ψ	O
(	O
a	O
)	O
−	O
log	O
b	O
(	O
11.74	O
)	O
+	O
,	O
11.4.	O
the	O
em	O
algorithm	O
361	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
−6	O
−7	O
−5	O
14	O
errors	O
using	O
gauss	O
(	O
red=error	O
)	O
−4	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
bankrupt	O
solvent	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
−6	O
−7	O
−5	O
4	O
errors	O
using	O
student	O
(	O
red=error	O
)	O
−4	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
bankrupt	O
solvent	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
11.14	O
mixture	B
modeling	O
on	O
the	O
bankruptcy	O
data	O
set	O
.	O
left	O
:	O
gaussian	O
class	O
conditional	O
densities	O
.	O
right	O
:	O
student	O
class	O
conditional	O
densities	O
.	O
points	O
that	O
belong	O
to	O
class	O
1	O
are	O
shown	O
as	O
triangles	O
,	O
points	O
that	O
belong	O
to	O
class	O
2	O
are	O
shown	O
as	O
circles	O
the	O
estimated	O
labels	O
,	O
based	O
on	O
the	O
posterior	O
probability	O
of	O
belonging	O
to	O
each	O
mixture	B
component	O
,	O
are	O
computed	O
.	O
if	O
these	O
are	O
incorrect	O
,	O
the	O
point	O
is	O
colored	O
red	O
,	O
otherwise	O
it	O
is	O
colored	O
blue	O
.	O
(	O
training	O
data	O
is	O
in	O
black	O
.	O
)	O
figure	O
generated	O
by	O
mixstudentbankruptcydemo	O
.	O
where	O
ψ	O
(	O
x	O
)	O
=	O
d	O
dx	O
log	O
γ	O
(	O
x	O
)	O
is	O
the	O
digamma	B
function	O
.	O
hence	O
,	O
from	O
equation	O
11.69	O
,	O
we	O
have	O
(	O
cid:6	O
)	O
(	O
t	O
)	O
i	O
ν	O
(	O
t	O
)	O
+	O
d	O
)	O
−	O
log	O
(	O
ν	O
(	O
t	O
)	O
+	O
δ	O
(	O
t	O
)	O
i	O
=	O
ψ	O
(	O
2	O
=	O
log	O
(	O
z	O
(	O
t	O
)	O
i	O
)	O
+	O
ψ	O
(	O
ν	O
(	O
t	O
)	O
+	O
d	O
2	O
)	O
2	O
)	O
−	O
log	O
(	O
ν	O
(	O
t	O
)	O
+	O
d	O
2	O
substituting	O
into	O
equation	O
11.68	O
,	O
we	O
have	O
e	O
[	O
lg	O
(	O
ν	O
)	O
]	O
=	O
−n	O
log	O
γ	O
(	O
ν/2	O
)	O
+	O
n	O
ν	O
2	O
log	O
(	O
ν/2	O
)	O
+	O
)	O
ν	O
2	O
(	O
cid:2	O
)	O
i	O
i	O
−	O
z	O
(	O
t	O
)	O
(	O
(	O
cid:6	O
)	O
(	O
t	O
)	O
i	O
)	O
(	O
11.75	O
)	O
(	O
11.76	O
)	O
(	O
11.77	O
)	O
the	O
gradient	O
of	O
this	O
expression	O
is	O
equal	O
to	O
n	O
e	O
[	O
lg	O
(	O
ν	O
)	O
]	O
=	O
−	O
n	O
2	O
d	O
(	O
11.78	O
)	O
dν	O
this	O
has	O
a	O
unique	O
solution	O
in	O
the	O
interval	O
(	O
0	O
,	O
+∞	O
]	O
which	O
can	O
be	O
found	O
using	O
a	O
1d	O
constrained	O
i	O
−	O
z	O
(	O
t	O
)	O
(	O
(	O
cid:6	O
)	O
(	O
t	O
)	O
i	O
)	O
log	O
(	O
ν/2	O
)	O
+	O
ψ	O
(	O
ν/2	O
)	O
+	O
n	O
2	O
+	O
1	O
2	O
2	O
(	O
cid:2	O
)	O
i	O
optimizer	O
.	O
performing	O
a	O
gradient-based	O
optimization	B
in	O
the	O
m	O
step	O
,	O
rather	O
than	O
a	O
closed-form	O
update	O
,	O
is	O
an	O
example	O
of	O
what	O
is	O
known	O
as	O
the	O
generalized	O
em	O
algorithm	O
.	O
one	O
can	O
show	O
that	O
em	O
will	O
still	O
converge	B
to	O
a	O
local	O
optimum	O
even	O
if	O
we	O
only	O
perform	O
a	O
“	O
partial	O
”	O
improvement	O
to	O
the	O
parameters	O
in	O
the	O
m	O
step	O
.	O
11.4.5.3	O
mixtures	O
of	O
student	O
distributions	O
it	O
is	O
easy	O
to	O
extend	O
the	O
above	O
methods	O
to	O
ﬁt	O
a	O
mixture	O
of	O
student	O
distributions	O
.	O
see	O
exercise	O
11.4	O
for	O
the	O
details	O
.	O
let	O
us	O
consider	O
a	O
small	O
example	O
from	O
(	O
lo	O
2009	O
,	O
ch3	O
)	O
.	O
we	O
have	O
a	O
n	O
=	O
66	O
,	O
d	O
=	O
2	O
data	O
set	O
regarding	O
the	O
bankrupty	O
patterns	O
of	O
certain	O
companies	O
.	O
the	O
ﬁrst	O
feature	O
speciﬁes	O
the	O
ratio	O
362	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
of	O
retained	O
earnings	O
(	O
re	O
)	O
to	O
total	O
assets	O
,	O
and	O
the	O
second	O
feature	O
speciﬁes	O
the	O
ratio	O
of	O
earnings	O
before	O
interests	O
and	O
taxes	O
(	O
ebit	O
)	O
to	O
total	O
assets	O
.	O
we	O
ﬁt	O
two	O
models	O
to	O
this	O
data	O
,	O
ignoring	O
the	O
class	O
labels	O
:	O
a	O
mixture	O
of	O
2	O
gaussians	O
,	O
and	O
a	O
mixture	O
of	O
2	O
students	O
.	O
we	O
then	O
use	O
each	O
ﬁtted	O
model	O
to	O
classify	O
the	O
data	O
.	O
we	O
compute	O
the	O
most	O
probable	O
cluster	O
membership	O
and	O
treat	O
this	O
as	O
ˆyi	O
.	O
we	O
then	O
compare	O
ˆyi	O
to	O
the	O
true	O
labels	O
yi	O
and	O
compute	O
an	O
error	O
rate	O
.	O
if	O
this	O
is	O
more	O
than	O
50	O
%	O
,	O
we	O
permute	O
the	O
latent	B
labels	O
(	O
i.e.	O
,	O
we	O
consider	O
cluster	O
1	O
to	O
represent	O
class	O
2	O
and	O
vice	O
versa	O
)	O
,	O
and	O
then	O
recompute	O
the	O
error	O
rate	O
.	O
points	O
which	O
are	O
misclassiﬁed	O
are	O
then	O
shown	O
in	O
red	O
.	O
the	O
result	O
is	O
shown	O
in	O
figure	O
11.14.	O
we	O
see	O
that	O
the	O
student	O
model	O
made	O
4	O
errors	O
,	O
the	O
gaussian	O
model	O
made	O
21.	O
this	O
is	O
because	O
the	O
class-conditional	O
densities	O
contain	O
some	O
extreme	O
values	O
,	O
causing	O
the	O
gaussian	O
to	O
be	O
a	O
poor	O
choice	O
.	O
11.4.6	O
em	O
for	O
probit	B
regression	I
*	O
in	O
section	O
9.4.2	O
,	O
we	O
described	O
the	O
latent	O
variable	O
interpretation	O
of	O
probit	B
regression	I
.	O
recall	B
that	O
this	O
has	O
the	O
form	O
p	O
(	O
yi	O
=	O
1|zi	O
)	O
=i	O
(	O
zi	O
>	O
0	O
)	O
,	O
where	O
zi	O
∼	O
n	O
(	O
wt	O
xi	O
,	O
1	O
)	O
is	O
latent	B
.	O
we	O
now	O
show	O
how	O
to	O
ﬁt	O
this	O
model	O
using	O
em	O
.	O
(	O
although	O
it	O
is	O
possible	O
to	O
ﬁt	O
probit	B
regression	I
models	O
using	O
gradient	O
based	O
methods	O
,	O
as	O
shown	O
in	O
section	O
9.4.1	O
,	O
this	O
em-based	O
approach	O
has	O
the	O
advantage	O
that	O
it	O
generalized	O
to	O
many	O
other	O
kinds	O
of	O
models	O
,	O
as	O
we	O
will	O
see	O
later	O
on	O
.	O
)	O
the	O
complete	B
data	I
log	I
likelihood	I
has	O
the	O
following	O
form	O
,	O
assuming	O
a	O
n	O
(	O
0	O
,	O
v0	O
)	O
prior	O
on	O
w	O
:	O
(	O
cid:6	O
)	O
(	O
z	O
,	O
w|v0	O
)	O
=	O
log	O
p	O
(	O
y|z	O
)	O
+	O
log	O
n	O
(	O
z|xw	O
,	O
i	O
)	O
+	O
log	O
n	O
(	O
w|0	O
,	O
v0	O
)	O
(	O
11.79	O
)	O
(	O
z	O
−	O
xw	O
)	O
t	O
(	O
z	O
−	O
xw	O
)	O
−	O
1	O
2	O
log	O
p	O
(	O
yi|zi	O
)	O
−	O
1	O
2	O
0	O
w	O
+	O
const	O
(	O
11.80	O
)	O
wt	O
v−1	O
(	O
cid:2	O
)	O
i	O
=	O
the	O
posterior	O
in	O
the	O
e	O
step	O
is	O
a	O
truncated	O
gaussian	O
:	O
p	O
(	O
zi|yi	O
,	O
xi	O
,	O
w	O
)	O
=	O
(	O
cid:26	O
)	O
n	O
(	O
zi|wt	O
xi	O
,	O
1	O
)	O
i	O
(	O
zi	O
>	O
0	O
)	O
n	O
(	O
zi|wt	O
xi	O
,	O
1	O
)	O
i	O
(	O
zi	O
<	O
0	O
)	O
if	O
yi	O
=	O
1	O
if	O
yi	O
=	O
0	O
(	O
11.81	O
)	O
in	O
equation	O
11.80	O
,	O
we	O
see	O
that	O
w	O
only	O
depends	O
linearly	O
on	O
z	O
,	O
so	O
we	O
just	O
need	O
to	O
compute	O
e	O
[	O
zi|yi	O
,	O
xi	O
,	O
w	O
]	O
.	O
exercise	O
11.15	O
asks	O
you	O
to	O
show	O
that	O
the	O
posterior	B
mean	I
is	O
given	O
by	O
-	O
e	O
[	O
zi|w	O
,	O
xi	O
]	O
=	O
φ	O
(	O
μi	O
)	O
μi	O
+	O
μi	O
−	O
φ	O
(	O
μi	O
)	O
φ	O
(	O
μi	O
)	O
1−φ	O
(	O
−μi	O
)	O
=	O
μi	O
+	O
φ	O
(	O
μi	O
)	O
φ	O
(	O
−μi	O
)	O
=	O
μi	O
−	O
φ	O
(	O
μi	O
)	O
1−φ	O
(	O
μi	O
)	O
if	O
yi	O
=	O
1	O
if	O
yi	O
=	O
0	O
(	O
11.82	O
)	O
where	O
μi	O
=	O
wt	O
xi	O
.	O
in	O
the	O
m	O
step	O
,	O
we	O
estimate	O
w	O
using	O
ridge	B
regression	I
,	O
where	O
μ	O
=	O
e	O
[	O
z	O
]	O
is	O
the	O
output	O
we	O
are	O
trying	O
to	O
predict	O
.	O
speciﬁcally	O
,	O
we	O
have	O
ˆw	O
=	O
(	O
v−1	O
0	O
+	O
xt	O
x	O
)	O
−1xt	O
μ	O
(	O
11.83	O
)	O
the	O
em	O
algorithm	O
is	O
simple	O
,	O
but	O
can	O
be	O
much	O
slower	O
than	O
direct	O
gradient	O
methods	O
,	O
as	O
illustrated	O
in	O
figure	O
11.15.	O
this	O
is	O
because	O
the	O
posterior	O
entropy	O
in	O
the	O
e	O
step	O
is	O
quite	O
high	O
,	O
since	O
we	O
only	O
observe	O
that	O
z	O
is	O
positive	O
or	O
negative	O
,	O
but	O
are	O
given	O
no	O
information	O
from	O
the	O
likelihood	B
about	O
its	O
magnitude	O
.	O
using	O
a	O
stronger	O
regularizer	O
can	O
help	O
speed	O
convergence	O
,	O
because	O
it	O
constrains	O
the	O
range	O
of	O
plausible	O
z	O
values	O
.	O
in	O
addition	O
,	O
one	O
can	O
use	O
various	O
speedup	O
tricks	O
,	O
such	O
as	O
data	B
augmentation	I
(	O
van	O
dyk	O
and	O
meng	O
2001	O
)	O
,	O
but	O
we	O
do	O
not	O
discuss	O
that	O
here	O
.	O
11.4.	O
the	O
em	O
algorithm	O
363	O
probit	B
regression	I
with	O
l2	O
regularizer	O
of	O
0.100	O
em	O
minfunc	O
70	O
60	O
50	O
40	O
30	O
20	O
10	O
l	O
l	O
n	O
d	O
e	O
z	O
i	O
l	O
a	O
n	O
e	O
p	O
0	O
0	O
20	O
40	O
60	O
iter	O
80	O
100	O
120	O
figure	O
11.15	O
by	O
probitregdemo	O
.	O
fitting	O
a	O
probit	B
regression	I
model	O
in	O
2d	O
using	O
a	O
quasi-newton	O
method	O
or	O
em	O
.	O
figure	O
generated	O
11.4.7	O
theoretical	O
basis	O
for	O
em	O
*	O
in	O
this	O
section	O
,	O
we	O
show	O
that	O
em	O
monotonically	O
increases	O
the	O
observed	B
data	I
log	I
likelihood	I
until	O
it	O
reaches	O
a	O
local	O
maximum	O
(	O
or	O
saddle	O
point	O
,	O
although	O
such	O
points	O
are	O
usually	O
unstable	B
)	O
.	O
our	O
derivation	O
will	O
also	O
serve	O
as	O
the	O
basis	O
for	O
various	O
generalizations	O
of	O
em	O
that	O
we	O
will	O
discuss	O
later	O
.	O
11.4.7.1	O
expected	B
complete	I
data	I
log	I
likelihood	I
is	O
a	O
lower	O
bound	O
consider	O
an	O
arbitrary	O
distribution	O
q	O
(	O
zi	O
)	O
over	O
the	O
hidden	B
variables	I
.	O
the	O
observed	B
data	I
log	I
likelihood	I
can	O
be	O
written	O
as	O
follows	O
:	O
p	O
(	O
xi	O
,	O
zi|θ	O
)	O
p	O
(	O
xi	O
,	O
zi|θ	O
)	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
(	O
cid:6	O
)	O
(	O
θ	O
)	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
=	O
log	O
q	O
(	O
zi	O
)	O
(	O
cid:19	O
)	O
n	O
(	O
cid:2	O
)	O
log	O
(	O
cid:19	O
)	O
(	O
11.84	O
)	O
i=1	O
zi	O
i=1	O
zi	O
q	O
(	O
zi	O
)	O
now	O
log	O
(	O
u	O
)	O
is	O
a	O
concave	B
function	O
,	O
so	O
from	O
jensen	O
’	O
s	O
inequality	O
(	O
equation	O
2.113	O
)	O
we	O
have	O
the	O
following	O
lower	O
bound	O
:	O
(	O
cid:6	O
)	O
(	O
θ	O
)	O
≥	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
zi	O
qi	O
(	O
zi	O
)	O
log	O
p	O
(	O
xi	O
,	O
zi|θ	O
)	O
qi	O
(	O
zi	O
)	O
let	O
us	O
denote	O
this	O
lower	O
bound	O
as	O
follows	O
:	O
q	O
(	O
θ	O
,	O
q	O
)	O
(	O
cid:2	O
)	O
eqi	O
[	O
log	O
p	O
(	O
xi	O
,	O
zi|θ	O
)	O
]	O
+	O
h	O
(	O
qi	O
)	O
(	O
cid:2	O
)	O
(	O
11.85	O
)	O
(	O
11.86	O
)	O
i	O
where	O
h	O
(	O
qi	O
)	O
is	O
the	O
entropy	B
of	O
qi	O
.	O
the	O
above	O
argument	O
holds	O
for	O
any	O
positive	O
distribution	O
q.	O
which	O
one	O
should	O
we	O
choose	O
?	O
intuitively	O
we	O
should	O
pick	O
the	O
q	O
that	O
yields	O
the	O
tightest	O
lower	O
bound	O
.	O
the	O
lower	O
bound	O
is	O
a	O
sum	O
364	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
over	O
i	O
of	O
terms	O
of	O
the	O
following	O
form	O
:	O
l	O
(	O
θ	O
,	O
qi	O
)	O
=	O
=	O
qi	O
(	O
zi	O
)	O
log	O
qi	O
(	O
zi	O
)	O
log	O
zi	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
zi	O
p	O
(	O
xi	O
,	O
zi|θ	O
)	O
qi	O
(	O
zi	O
)	O
p	O
(	O
zi|xi	O
,	O
θ	O
)	O
p	O
(	O
xi|θ	O
)	O
(	O
cid:2	O
)	O
qi	O
(	O
zi	O
)	O
p	O
(	O
zi|xi	O
,	O
θ	O
)	O
(	O
11.87	O
)	O
(	O
11.88	O
)	O
(	O
11.89	O
)	O
(	O
11.91	O
)	O
qi	O
(	O
zi	O
)	O
log	O
=	O
=	O
−kl	O
(	O
qi	O
(	O
zi	O
)	O
||p	O
(	O
zi|xi	O
,	O
θ	O
)	O
)	O
+	O
log	O
p	O
(	O
xi|θ	O
)	O
qi	O
(	O
zi	O
)	O
+	O
zi	O
zi	O
qi	O
(	O
zi	O
)	O
log	O
p	O
(	O
xi|θ	O
)	O
(	O
11.90	O
)	O
the	O
p	O
(	O
xi|θ	O
)	O
term	O
is	O
independent	O
of	O
qi	O
,	O
so	O
we	O
can	O
maximize	O
the	O
lower	O
bound	O
by	O
setting	O
qi	O
(	O
zi	O
)	O
=	O
p	O
(	O
zi|xi	O
,	O
θ	O
)	O
.	O
of	O
course	O
,	O
θ	O
is	O
unknown	B
,	O
so	O
instead	O
we	O
use	O
qt	O
i	O
(	O
zi	O
)	O
=	O
p	O
(	O
zi|xi	O
,	O
θt	O
)	O
,	O
where	O
θt	O
is	O
our	O
estimate	O
of	O
the	O
parameters	O
at	O
iteration	O
t.	O
this	O
is	O
the	O
output	O
of	O
the	O
e	O
step	O
.	O
plugging	O
this	O
in	O
to	O
the	O
lower	O
bound	O
we	O
get	O
q	O
(	O
θ	O
,	O
qt	O
)	O
=	O
[	O
log	O
p	O
(	O
xi	O
,	O
zi|θ	O
)	O
]	O
+	O
h	O
(	O
cid:3	O
)	O
(	O
cid:4	O
)	O
qt	O
i	O
(	O
cid:2	O
)	O
eqt	O
i	O
i	O
we	O
recognize	O
the	O
ﬁrst	O
term	O
as	O
the	O
expected	B
complete	I
data	I
log	I
likelihood	I
.	O
the	O
second	O
term	O
is	O
a	O
constant	O
wrt	O
θ.	O
so	O
the	O
m	O
step	O
becomes	O
θt+1	O
=	O
arg	O
max	O
θ	O
q	O
(	O
θ	O
,	O
θt	O
)	O
=	O
arg	O
max	O
θ	O
[	O
log	O
p	O
(	O
xi	O
,	O
zi|θ	O
)	O
]	O
(	O
11.92	O
)	O
(	O
cid:2	O
)	O
eqt	O
i	O
i	O
as	O
usual	O
.	O
zero	O
,	O
so	O
l	O
(	O
θt	O
,	O
qi	O
)	O
=	O
log	O
p	O
(	O
xi|θt	O
)	O
,	O
and	O
hence	O
now	O
comes	O
the	O
punchline	O
.	O
since	O
we	O
used	O
qt	O
(	O
cid:2	O
)	O
i	O
(	O
zi	O
)	O
=	O
p	O
(	O
zi|xi	O
,	O
θt	O
)	O
,	O
the	O
kl	O
divergence	O
becomes	O
q	O
(	O
θt	O
,	O
θt	O
)	O
=	O
log	O
p	O
(	O
xi|θt	O
)	O
=	O
(	O
cid:6	O
)	O
(	O
θt	O
)	O
(	O
11.93	O
)	O
i	O
we	O
see	O
that	O
the	O
lower	O
bound	O
is	O
tight	O
after	O
the	O
e	O
step	O
.	O
since	O
the	O
lower	O
bound	O
“	O
touches	O
”	O
the	O
function	O
,	O
maximizing	O
the	O
lower	O
bound	O
will	O
also	O
“	O
push	O
up	O
”	O
on	O
the	O
function	O
itself	O
.	O
that	O
is	O
,	O
the	O
m	O
step	O
is	O
guaranteed	O
to	O
modify	O
the	O
parameters	O
so	O
as	O
to	O
increase	O
the	O
likelihood	B
of	O
the	O
observed	O
data	O
(	O
unless	O
it	O
is	O
already	O
at	O
a	O
local	O
maximum	O
)	O
.	O
this	O
process	O
is	O
sketched	O
in	O
figure	O
11.16.	O
the	O
dashed	O
red	O
curve	O
is	O
the	O
original	O
function	O
(	O
the	O
observed	O
data	O
log-likelihood	O
)	O
.	O
the	O
solid	O
blue	O
curve	O
is	O
the	O
lower	O
bound	O
,	O
evaluated	O
at	O
θt	O
;	O
this	O
touches	O
the	O
objective	O
function	O
at	O
θt	O
.	O
we	O
then	O
set	O
θt+1	O
to	O
the	O
maximum	O
of	O
the	O
lower	O
bound	O
(	O
blue	O
curve	O
)	O
,	O
and	O
ﬁt	O
a	O
new	O
bound	O
at	O
that	O
point	O
(	O
dotted	O
green	O
curve	O
)	O
.	O
the	O
maximum	O
of	O
this	O
new	O
bound	O
becomes	O
θt+2	O
,	O
etc	O
.	O
(	O
compare	O
this	O
to	O
newton	O
’	O
s	O
method	O
in	O
figure	O
8.4	O
(	O
a	O
)	O
,	O
which	O
repeatedly	O
ﬁts	O
and	O
then	O
optimizes	O
a	O
quadratic	O
approximation	O
.	O
)	O
11.4.7.2	O
em	O
monotonically	O
increases	O
the	O
observed	B
data	I
log	I
likelihood	I
we	O
now	O
prove	O
that	O
em	O
monotonically	O
increases	O
the	O
observed	B
data	I
log	I
likelihood	I
until	O
it	O
reaches	O
a	O
local	O
optimum	O
.	O
we	O
have	O
(	O
cid:6	O
)	O
(	O
θt+1	O
)	O
≥	O
q	O
(	O
θt+1	O
,	O
θt	O
)	O
≥	O
q	O
(	O
θt	O
,	O
θt	O
)	O
=	O
(	O
cid:6	O
)	O
(	O
θt	O
)	O
(	O
11.94	O
)	O
11.4.	O
the	O
em	O
algorithm	O
365	O
q	O
(	O
θ	O
,	O
θ	O
)	O
t	O
q	O
(	O
θ	O
,	O
θ	O
l	O
(	O
θ	O
)	O
)	O
t+1	O
θ	O
t	O
θ	O
t+1	O
θ	O
t+2	O
figure	O
11.16	O
illustration	O
of	O
em	O
as	O
a	O
bound	B
optimization	I
algorithm	O
.	O
based	O
on	O
figure	O
9.14	O
of	O
(	O
bishop	O
2006a	O
)	O
.	O
figure	O
generated	O
by	O
emloglikelihoodmax	O
.	O
where	O
the	O
ﬁrst	O
inequality	O
follows	O
since	O
q	O
(	O
θ	O
,	O
·	O
)	O
is	O
a	O
lower	O
bound	O
on	O
(	O
cid:6	O
)	O
(	O
θ	O
)	O
;	O
the	O
second	O
inequality	O
follows	O
since	O
,	O
by	O
deﬁnition	O
,	O
q	O
(	O
θt+1	O
,	O
θt	O
)	O
=	O
maxθ	O
q	O
(	O
θ	O
,	O
θt	O
)	O
≥	O
q	O
(	O
θt	O
,	O
θt	O
)	O
;	O
and	O
the	O
ﬁnal	O
equality	O
follows	O
equation	O
11.93.	O
as	O
a	O
consequence	O
of	O
this	O
result	O
,	O
if	O
you	O
do	O
not	O
observe	O
monotonic	O
increase	O
of	O
the	O
observed	O
(	O
if	O
you	O
are	O
performing	O
data	O
log	O
likelihood	B
,	O
you	O
must	O
have	O
an	O
error	O
in	O
your	O
math	O
and/or	O
code	O
.	O
map	O
estimation	O
,	O
you	O
must	O
add	O
on	O
the	O
log	O
prior	O
term	O
to	O
the	O
objective	O
.	O
)	O
this	O
is	O
a	O
surprisingly	O
powerful	O
debugging	O
tool	O
.	O
11.4.8	O
online	O
em	O
when	O
dealing	O
with	O
large	O
or	O
streaming	O
datasets	O
,	O
it	O
is	O
important	O
to	O
be	O
able	O
to	O
learn	O
online	O
,	O
as	O
we	O
discussed	O
in	O
section	O
8.5.	O
there	O
are	O
two	O
main	O
approaches	O
to	O
online	O
em	O
in	O
the	O
literature	O
.	O
the	O
ﬁrst	O
approach	O
,	O
known	O
as	O
incremental	O
em	O
(	O
neal	O
and	O
hinton	O
1998	O
)	O
,	O
optimizes	O
the	O
lower	O
bound	O
q	O
(	O
θ	O
,	O
q1	O
,	O
.	O
.	O
.	O
,	O
qn	O
)	O
one	O
qi	O
at	O
a	O
time	O
;	O
however	O
,	O
this	O
requires	O
storing	O
the	O
expected	B
sufficient	I
statistics	I
for	O
each	O
data	O
case	O
.	O
the	O
second	O
approach	O
,	O
known	O
as	O
stepwise	O
em	O
(	O
sato	O
and	O
ishii	O
2000	O
;	O
cappe	O
and	O
mouline	O
2009	O
;	O
cappe	O
2010	O
)	O
,	O
is	O
based	O
on	O
stochastic	B
approximation	I
theory	O
,	O
and	O
only	O
requires	O
constant	O
memory	O
use	O
.	O
we	O
explain	O
both	O
approaches	O
in	O
more	O
detail	O
below	O
,	O
following	O
the	O
presentation	O
of	O
(	O
liang	O
and	O
klein	O
liang	O
and	O
klein	O
)	O
.	O
11.4.8.1	O
batch	B
em	O
review	O
(	O
cid:10	O
)	O
before	O
explaining	O
online	O
em	O
,	O
we	O
review	O
batch	B
em	O
in	O
a	O
more	O
abstract	O
setting	O
.	O
let	O
φ	O
(	O
x	O
,	O
z	O
)	O
be	O
a	O
vector	O
of	O
sufficient	B
statistics	I
for	O
a	O
single	O
data	O
case	O
.	O
(	O
for	O
example	O
,	O
for	O
a	O
mixture	O
of	O
multinoullis	O
,	O
this	O
would	O
be	O
the	O
count	O
vector	O
a	O
(	O
j	O
)	O
,	O
which	O
is	O
the	O
number	O
of	O
cluster	O
j	O
was	O
used	O
in	O
z	O
,	O
plus	O
the	O
matrix	O
b	O
(	O
j	O
,	O
v	O
)	O
,	O
which	O
is	O
of	O
the	O
number	O
of	O
times	O
the	O
hidden	B
state	O
was	O
j	O
and	O
the	O
observed	O
letter	O
z	O
p	O
(	O
z|xi	O
,	O
θ	O
)	O
φ	O
(	O
xi	O
,	O
z	O
)	O
be	O
the	O
expected	B
sufficient	I
statistics	I
for	O
case	O
i	O
,	O
and	O
was	O
v.	O
)	O
let	O
si	O
=	O
μ	O
=	O
i=1	O
si	O
be	O
the	O
sum	O
of	O
the	O
ess	O
.	O
given	O
μ	O
,	O
we	O
can	O
derive	O
an	O
ml	O
or	O
map	O
estimate	O
of	O
the	O
parameters	O
in	O
the	O
m	O
step	O
;	O
we	O
will	O
denote	O
this	O
operation	O
by	O
θ	O
(	O
μ	O
)	O
.	O
(	O
for	O
example	O
,	O
in	O
the	O
case	O
of	O
mixtures	O
of	O
multinoullis	O
,	O
we	O
just	O
need	O
to	O
normalize	O
a	O
and	O
each	O
row	O
of	O
b	O
.	O
)	O
with	O
this	O
notation	O
under	O
our	O
belt	O
,	O
the	O
pseudo	O
code	O
for	O
batch	B
em	O
is	O
as	O
shown	O
in	O
algorithm	O
8	O
.	O
(	O
cid:10	O
)	O
n	O
366	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
algorithm	O
11.2	O
:	O
batch	B
em	O
algorithm	O
1	O
initialize	O
μ	O
;	O
2	O
repeat	O
3	O
μnew	O
=	O
0	O
;	O
(	O
cid:10	O
)	O
for	O
each	O
example	O
i	O
=	O
1	O
:	O
n	O
do	O
z	O
p	O
(	O
z|xi	O
,	O
θ	O
(	O
μ	O
)	O
)	O
φ	O
(	O
xi	O
,	O
z	O
)	O
;	O
si	O
:	O
=	O
μnew	O
:	O
=	O
μnew	O
+	O
si	O
;	O
;	O
μ	O
:	O
=	O
μnew	O
;	O
7	O
8	O
until	O
converged	O
;	O
11.4.8.2	O
incremental	O
em	O
in	O
incremental	O
em	O
(	O
neal	O
and	O
hinton	O
1998	O
)	O
,	O
we	O
keep	O
track	O
of	O
μ	O
as	O
well	O
as	O
the	O
si	O
.	O
when	O
we	O
come	O
to	O
a	O
data	O
case	O
,	O
we	O
swap	O
out	O
the	O
old	O
si	O
and	O
replace	O
it	O
with	O
the	O
new	O
snew	O
,	O
as	O
shown	O
in	O
the	O
code	O
in	O
algorithm	O
8.	O
note	O
that	O
we	O
can	O
exploit	O
the	O
sparsity	B
of	O
snew	O
to	O
speedup	O
the	O
computation	O
of	O
θ	O
,	O
since	O
most	O
components	O
of	O
μ	O
wil	O
not	O
have	O
changed	O
.	O
i	O
i	O
4	O
5	O
6	O
5	O
6	O
7	O
(	O
cid:10	O
)	O
i	O
si	O
;	O
algorithm	O
11.3	O
:	O
incremental	O
em	O
algorithm	O
1	O
initialize	O
si	O
for	O
i	O
=	O
1	O
:	O
n	O
;	O
2	O
μ	O
=	O
3	O
repeat	O
4	O
(	O
cid:10	O
)	O
for	O
each	O
example	O
i	O
=	O
1	O
:	O
n	O
in	O
a	O
random	O
order	O
do	O
z	O
p	O
(	O
z|xi	O
,	O
θ	O
(	O
μ	O
)	O
)	O
φ	O
(	O
xi	O
,	O
z	O
)	O
;	O
i	O
−	O
si	O
;	O
:	O
=	O
snew	O
i	O
μ	O
:	O
=	O
μ	O
+	O
snew	O
si	O
:	O
=	O
snew	O
;	O
i	O
8	O
until	O
converged	O
;	O
this	O
can	O
be	O
viewed	O
as	O
maximizing	O
the	O
lower	O
bound	O
q	O
(	O
θ	O
,	O
q1	O
,	O
.	O
.	O
.	O
,	O
qn	O
)	O
by	O
optimizing	O
q1	O
,	O
then	O
θ	O
,	O
then	O
q2	O
,	O
then	O
θ	O
,	O
etc	O
.	O
as	O
such	O
,	O
this	O
method	O
is	O
guaranteed	O
to	O
monotonically	O
converge	B
to	O
a	O
local	O
maximum	O
of	O
the	O
lower	O
bound	O
and	O
to	O
the	O
log	O
likelihood	O
itself	O
.	O
11.4.8.3	O
stepwise	O
em	O
in	O
stepwise	O
em	O
,	O
whenever	O
we	O
compute	O
a	O
new	O
si	O
,	O
we	O
move	O
μ	O
towards	O
it	O
,	O
as	O
shown	O
in	O
algorithm	O
7.2	O
at	O
iteration	O
k	O
,	O
the	O
stepsize	O
has	O
value	O
ηk	O
,	O
which	O
must	O
satisfy	O
the	O
robbins-monro	O
conditions	O
in	O
−κ	O
for	O
equation	O
8.82	O
.	O
0.5	O
<	O
κ	O
≤	O
1.	O
we	O
can	O
get	O
somewhat	O
better	O
behavior	O
by	O
using	O
a	O
minibatch	O
of	O
size	O
m	O
before	O
it	O
is	O
possible	O
to	O
optimize	O
m	O
and	O
κ	O
to	O
maximize	O
the	O
training	B
set	I
likelihood	O
,	O
by	O
each	O
update	O
.	O
(	O
liang	O
and	O
klein	O
liang	O
and	O
klein	O
)	O
use	O
ηk	O
=	O
(	O
2	O
+	O
k	O
)	O
for	O
example	O
,	O
2.	O
a	O
detail	O
:	O
as	O
written	O
the	O
update	O
for	O
μ	O
does	O
not	O
exploit	O
the	O
sparsity	B
of	O
si	O
.	O
we	O
can	O
ﬁx	O
this	O
by	O
storing	O
m	O
=	O
instead	O
of	O
μ	O
,	O
and	O
then	O
using	O
the	O
sparse	B
update	O
m	O
:	O
=	O
m	O
+	O
θ	O
(	O
μ	O
)	O
=θ	O
(	O
m	O
)	O
)	O
,	O
since	O
scaling	O
the	O
counts	O
by	O
a	O
global	O
constant	O
has	O
no	O
effect	O
.	O
j	O
<	O
k	O
(	O
1−ηj	O
)	O
j	O
<	O
k	O
(	O
1−ηj	O
)	O
si	O
.	O
this	O
will	O
not	O
affect	O
the	O
results	O
(	O
i.e.	O
,	O
ηk	O
(	O
cid:3	O
)	O
μ	O
(	O
cid:3	O
)	O
11.4.	O
the	O
em	O
algorithm	O
367	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
figure	O
11.17	O
uated_optimization	O
.	O
illustration	O
of	O
deterministic	B
annealing	I
.	O
based	O
on	O
http	O
:	O
//en.wikipedia.org/wiki/grad	O
trying	O
different	O
values	O
in	O
parallel	O
for	O
an	O
initial	O
trial	O
period	B
;	O
this	O
can	O
signiﬁcantly	O
speed	O
up	O
the	O
algorithm	O
.	O
algorithm	O
11.4	O
:	O
stepwise	O
em	O
algorithm	O
1	O
initialize	O
μ	O
;	O
k	O
=	O
0	O
;	O
2	O
repeat	O
3	O
(	O
cid:10	O
)	O
for	O
each	O
example	O
i	O
=	O
1	O
:	O
n	O
in	O
a	O
random	O
order	O
do	O
4	O
5	O
6	O
z	O
p	O
(	O
z|xi	O
,	O
θ	O
(	O
μ	O
)	O
)	O
φ	O
(	O
xi	O
,	O
z	O
)	O
;	O
si	O
:	O
=	O
μ	O
:	O
=	O
(	O
1	O
−	O
ηk	O
)	O
μ	O
+	O
ηksi	O
;	O
k	O
:	O
=	O
k	O
+	O
1	O
7	O
until	O
converged	O
;	O
(	O
liang	O
and	O
klein	O
liang	O
and	O
klein	O
)	O
compare	O
batch	B
em	O
,	O
incremental	O
em	O
,	O
and	O
stepwise	O
em	O
on	O
four	O
different	O
unsupervised	O
language	O
modeling	O
tasks	O
.	O
they	O
found	O
that	O
stepwise	O
em	O
(	O
using	O
κ	O
≈	O
0.7	O
and	O
m	O
≈	O
1000	O
)	O
was	O
faster	O
than	O
incremental	O
em	O
,	O
and	O
both	O
were	O
much	O
faster	O
than	O
batch	B
em	O
.	O
in	O
terms	O
of	O
accuracy	O
,	O
stepwise	O
em	O
was	O
usually	O
as	O
good	O
or	O
sometimes	O
even	O
better	O
than	O
batch	B
em	O
;	O
incremental	O
em	O
was	O
often	O
worse	O
than	O
either	O
of	O
the	O
other	O
methods	O
.	O
11.4.9	O
other	O
em	O
variants	O
*	O
em	O
is	O
one	O
of	O
the	O
most	O
widely	O
used	O
algorithms	O
in	O
statistics	O
and	O
machine	B
learning	I
.	O
not	O
surpris-	O
ingly	O
,	O
many	O
variations	O
have	O
been	O
proposed	O
.	O
we	O
brieﬂy	O
mention	O
a	O
few	O
below	O
,	O
some	O
of	O
which	O
we	O
will	O
use	O
in	O
later	O
chapters	O
.	O
see	O
(	O
mclachlan	O
and	O
krishnan	O
1997	O
)	O
for	O
more	O
information	B
.	O
•	O
annealed	O
em	O
in	O
general	O
,	O
em	O
will	O
only	O
converge	B
to	O
a	O
local	O
maximum	O
.	O
to	O
increase	O
the	O
chance	O
of	O
ﬁnding	O
the	O
global	O
maximum	O
,	O
we	O
can	O
use	O
a	O
variety	O
of	O
methods	O
.	O
one	O
approach	O
is	O
to	O
use	O
a	O
method	O
known	O
as	O
deterministic	B
annealing	I
(	O
rose	O
1998	O
)	O
.	O
the	O
basic	O
idea	O
is	O
to	O
“	O
smooth	O
”	O
the	O
posterior	O
“	O
landscape	O
”	O
by	O
raising	O
it	O
to	O
a	O
temperature	B
,	O
and	O
then	O
gradually	O
cooling	O
it	O
,	O
all	O
the	O
while	O
slowly	O
tracking	B
the	O
global	O
maximum	O
.	O
see	O
figure	O
11.17.	O
for	O
a	O
sketch	O
.	O
(	O
a	O
stochastic	O
version	O
368	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
true	O
log−likelihood	O
lower	O
bound	O
true	O
log−likelihood	O
lower	O
bound	O
training	O
time	O
(	O
a	O
)	O
training	O
time	O
(	O
b	O
)	O
figure	O
11.18	O
illustration	O
of	O
possible	O
behaviors	O
of	O
variational	O
em	O
.	O
(	O
a	O
)	O
the	O
lower	O
bound	O
increases	O
at	O
each	O
iteration	O
,	O
and	O
so	O
does	O
the	O
likelihood	B
.	O
in	O
this	O
case	O
,	O
the	O
algorithm	O
is	O
closing	O
the	O
gap	O
between	O
the	O
approximate	O
and	O
true	O
posterior	O
.	O
this	O
can	O
have	O
a	O
regularizing	O
effect	O
.	O
based	O
on	O
figure	O
6	O
of	O
(	O
saul	O
et	O
al	O
.	O
1996	O
)	O
.	O
figure	O
generated	O
by	O
varembound	O
.	O
(	O
b	O
)	O
the	O
lower	O
bound	O
increases	O
but	O
the	O
likelihood	B
decreases	O
.	O
of	O
this	O
algorithm	O
is	O
described	O
in	O
section	O
24.6.1	O
.	O
)	O
an	O
annealed	O
version	O
of	O
em	O
is	O
described	O
in	O
(	O
ueda	O
and	O
nakano	O
1998	O
)	O
.	O
•	O
variational	O
em	O
in	O
section	O
11.4.7	O
,	O
we	O
showed	O
that	O
the	O
optimal	O
thing	O
to	O
do	O
in	O
the	O
e	O
step	O
is	O
to	O
i	O
(	O
zi	O
)	O
=p	O
(	O
zi|xi	O
,	O
θt	O
)	O
.	O
in	O
this	O
case	O
,	O
make	O
qi	O
be	O
the	O
exact	O
posterior	O
over	O
the	O
latent	B
variables	O
,	O
qt	O
the	O
lower	O
bound	O
on	O
the	O
log	O
likelihood	O
will	O
be	O
tight	O
,	O
so	O
the	O
m	O
step	O
will	O
“	O
push	O
up	O
”	O
on	O
the	O
log-likelihood	O
itself	O
.	O
however	O
,	O
sometimes	O
it	O
is	O
computationally	O
intractable	O
to	O
perform	O
exact	O
inference	B
in	O
the	O
e	O
step	O
,	O
but	O
we	O
may	O
be	O
able	O
to	O
perform	O
approximate	B
inference	I
.	O
if	O
we	O
can	O
ensure	O
that	O
the	O
e	O
step	O
is	O
performing	O
inference	B
based	O
on	O
a	O
a	O
lowerbound	O
to	O
the	O
likelihood	B
,	O
then	O
the	O
m	O
step	O
can	O
be	O
seen	O
as	O
monotonically	O
increasing	O
this	O
lower	O
bound	O
(	O
see	O
figure	O
11.18	O
)	O
.	O
this	O
is	O
called	O
variational	O
em	O
(	O
neal	O
and	O
hinton	O
1998	O
)	O
.	O
see	O
chapter	O
21	O
for	O
some	O
variational	B
inference	I
methods	O
that	O
can	O
be	O
used	O
in	O
the	O
e	O
step	O
.	O
•	O
monte	O
carlo	O
em	O
another	O
approach	O
to	O
handling	O
an	O
intractable	O
e	O
step	O
is	O
to	O
use	O
a	O
monte	O
carlo	O
approximation	O
to	O
the	O
expected	B
sufficient	I
statistics	I
.	O
that	O
is	O
,	O
we	O
draw	O
samples	B
from	O
the	O
i	O
∼	O
p	O
(	O
zi|xi	O
,	O
θt	O
)	O
,	O
and	O
then	O
compute	O
the	O
sufficient	B
statistics	I
for	O
each	O
completed	O
posterior	O
,	O
zs	O
vector	O
,	O
(	O
xi	O
,	O
zs	O
i	O
)	O
,	O
and	O
then	O
average	O
the	O
results	O
.	O
this	O
is	O
called	O
monte	O
carlo	O
em	O
or	O
mcem	O
(	O
wei	O
(	O
if	O
we	O
only	O
draw	O
a	O
single	O
sample	O
,	O
it	O
is	O
called	O
stochastic	O
em	O
(	O
celeux	O
and	O
and	O
tanner	O
1990	O
)	O
.	O
diebolt	O
1985	O
)	O
.	O
)	O
one	O
way	O
to	O
draw	O
samples	B
is	O
to	O
use	O
mcmc	O
(	O
see	O
chapter	O
24	O
)	O
.	O
however	O
,	O
if	O
we	O
have	O
to	O
wait	O
for	O
mcmc	O
to	O
converge	B
inside	O
each	O
e	O
step	O
,	O
the	O
method	O
becomes	O
very	O
slow	O
.	O
an	O
alternative	O
is	O
to	O
use	O
stochastic	B
approximation	I
,	O
and	O
only	O
perform	O
“	O
brief	O
”	O
sampling	O
in	O
the	O
e	O
step	O
,	O
followed	O
by	O
a	O
partial	O
parameter	O
update	O
.	O
this	O
is	O
called	O
stochastic	B
approximation	I
em	O
(	O
delyon	O
et	O
al	O
.	O
1999	O
)	O
and	O
tends	O
to	O
work	O
better	O
than	O
mcem	O
.	O
another	O
alternative	O
is	O
to	O
apply	O
mcmc	O
to	O
infer	O
the	O
parameters	O
as	O
well	O
as	O
the	O
latent	B
variables	O
(	O
a	O
fully	O
bayesian	O
approach	O
)	O
,	O
thus	O
eliminating	O
the	O
distinction	O
between	O
e	O
and	O
m	O
steps	O
.	O
see	O
chapter	O
24	O
for	O
details	O
.	O
•	O
generalized	O
em	O
sometimes	O
we	O
can	O
perform	O
the	O
e	O
step	O
exactly	O
,	O
but	O
we	O
can	O
not	O
perform	O
the	O
m	O
step	O
exactly	O
.	O
however	O
,	O
we	O
can	O
still	O
monotonically	O
increase	O
the	O
log	O
likelihood	O
by	O
performing	O
a	O
“	O
partial	O
”	O
m	O
step	O
,	O
in	O
which	O
we	O
merely	O
increase	O
the	O
expected	B
complete	I
data	I
log	I
likelihood	I
,	O
rather	O
than	O
maximizing	O
it	O
.	O
for	O
example	O
,	O
we	O
might	O
follow	O
a	O
few	O
gradient	O
steps	O
.	O
this	O
is	O
called	O
11.4.	O
the	O
em	O
algorithm	O
369	O
k=5	O
,	O
d=15	O
,	O
n=5000	O
−38.5	O
−39	O
−39.5	O
k	O
i	O
l	O
g	O
o	O
l	O
−40	O
−40.5	O
−41	O
−41.5	O
em	O
(	O
1.080	O
)	O
or	O
(	O
1	O
)	O
(	O
1.358	O
)	O
or	O
(	O
1.25	O
)	O
(	O
1.141	O
)	O
or	O
(	O
2	O
)	O
(	O
1.219	O
)	O
or	O
(	O
5	O
)	O
(	O
1.433	O
)	O
12	O
14	O
16	O
18	O
−42	O
0	O
2	O
4	O
6	O
8	O
10	O
iterations	O
(	O
a	O
)	O
−36	O
−37	O
−38	O
k	O
i	O
l	O
g	O
o	O
l	O
−39	O
−40	O
−41	O
−42	O
0	O
k=5	O
,	O
d=15	O
,	O
n=5000	O
em	O
(	O
1.315	O
)	O
or	O
(	O
1	O
)	O
(	O
1.368	O
)	O
or	O
(	O
1.25	O
)	O
(	O
1.381	O
)	O
or	O
(	O
2	O
)	O
(	O
1.540	O
)	O
or	O
(	O
5	O
)	O
(	O
1.474	O
)	O
12	O
14	O
16	O
18	O
2	O
4	O
6	O
8	O
10	O
iterations	O
(	O
b	O
)	O
figure	O
11.19	O
illustration	O
of	O
adaptive	O
over-relaxed	O
em	O
applied	O
to	O
a	O
mixture	O
of	O
5	O
gaussians	O
in	O
15	O
dimensions	O
.	O
we	O
show	O
the	O
algorithm	O
applied	O
to	O
two	O
different	O
datasets	O
,	O
randomly	O
sampled	O
from	O
a	O
mixture	O
of	O
10	O
gaussians	O
.	O
we	O
plot	O
the	O
convergence	O
for	O
different	O
update	O
rates	O
η.	O
using	O
η	O
=	O
1	O
gives	O
the	O
same	O
results	O
as	O
regular	B
em	O
.	O
the	O
actual	O
running	O
time	O
is	O
printed	O
in	O
the	O
legend	O
.	O
figure	O
generated	O
by	O
mixgaussoverrelaxedemdemo	O
.	O
the	O
generalized	O
em	O
or	O
gem	O
algorithm	O
.	O
ways	O
to	O
generalize	B
em	O
...	O
.	O
)	O
(	O
this	O
is	O
an	O
unfortunate	O
term	O
,	O
since	O
there	O
are	O
many	O
•	O
ecm	O
(	O
e	O
)	O
algorithm	O
the	O
ecm	O
algorithm	O
stands	O
for	O
“	O
expectation	O
conditional	O
maximization	O
”	O
,	O
and	O
refers	O
to	O
optimizing	O
the	O
parameters	O
in	O
the	O
m	O
step	O
sequentially	O
,	O
if	O
they	O
turn	O
out	O
to	O
be	O
dependent	O
.	O
the	O
ecme	O
algorithm	O
,	O
which	O
stands	O
for	O
“	O
ecm	O
either	O
”	O
(	O
liu	O
and	O
rubin	O
1995	O
)	O
,	O
is	O
a	O
variant	O
of	O
ecm	O
in	O
which	O
we	O
maximize	O
the	O
expected	B
complete	I
data	I
log	I
likelihood	I
(	O
the	O
q	O
function	O
)	O
as	O
usual	O
,	O
or	O
the	O
observed	B
data	I
log	I
likelihood	I
,	O
during	O
one	O
or	O
more	O
of	O
the	O
conditional	O
maximization	O
steps	O
.	O
the	O
latter	O
can	O
be	O
much	O
faster	O
,	O
since	O
it	O
ignores	O
the	O
results	O
of	O
the	O
e	O
step	O
,	O
and	O
directly	O
optimizes	O
the	O
objective	O
of	O
interest	O
.	O
a	O
standard	O
example	O
of	O
this	O
is	O
when	O
ﬁtting	O
the	O
student	O
t	O
distribution	O
.	O
for	O
ﬁxed	O
ν	O
,	O
we	O
can	O
update	O
σ	O
as	O
usual	O
,	O
but	O
then	O
to	O
update	O
ν	O
,	O
we	O
replace	O
the	O
standard	O
update	O
of	O
the	O
form	O
νt+1	O
=	O
arg	O
maxν	O
q	O
(	O
(	O
μt+1	O
,	O
σt+1	O
,	O
ν	O
)	O
,	O
θt	O
)	O
with	O
νt+1	O
=	O
arg	O
maxν	O
log	O
p	O
(	O
d|μt+1	O
,	O
σt+1	O
,	O
ν	O
)	O
.	O
see	O
(	O
mclachlan	O
and	O
krishnan	O
1997	O
)	O
for	O
more	O
information	B
.	O
•	O
over-relaxed	O
em	O
vanilla	O
em	O
can	O
be	O
quite	O
slow	O
,	O
especially	O
if	O
there	O
is	O
lots	O
of	O
missing	B
data	I
.	O
the	O
adaptive	O
overrelaxed	O
em	O
algorithm	O
(	O
salakhutdinov	O
and	O
roweis	O
2003	O
)	O
performs	O
an	O
update	O
of	O
the	O
form	O
θt+1	O
=	O
θt	O
+	O
η	O
(	O
m	O
(	O
θt	O
)	O
−	O
θt	O
)	O
,	O
where	O
η	O
is	O
a	O
step-size	O
parameter	B
,	O
and	O
m	O
(	O
θt	O
)	O
is	O
the	O
usual	O
update	O
computed	O
during	O
the	O
m	O
step	O
.	O
obviously	O
this	O
reduces	O
to	O
standard	O
em	O
if	O
η	O
=	O
1	O
,	O
but	O
using	O
larger	O
values	O
of	O
η	O
can	O
result	O
in	O
faster	O
convergence	O
.	O
see	O
figure	O
11.19	O
for	O
an	O
illustration	O
.	O
unfortunately	O
,	O
using	O
too	O
large	O
a	O
value	O
of	O
η	O
can	O
cause	O
the	O
algorithm	O
to	O
fail	O
to	O
converge	B
.	O
finally	O
,	O
note	O
that	O
em	O
is	O
in	O
fact	O
just	O
a	O
special	O
case	O
of	O
a	O
larger	O
class	O
of	O
algorithms	O
known	O
as	O
bound	B
optimization	I
or	O
mm	O
algorithms	O
(	O
mm	O
stands	O
for	O
minorize-maximize	B
)	O
.	O
see	O
(	O
hunter	O
and	O
lange	O
2004	O
)	O
for	O
further	O
discussion	O
.	O
370	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
11.5	O
model	B
selection	I
for	O
latent	B
variable	I
models	I
when	O
using	O
lvms	O
,	O
we	O
must	O
specify	O
the	O
number	O
of	O
latent	B
variables	O
,	O
which	O
controls	O
the	O
model	O
in	O
the	O
case	O
of	O
mixture	B
models	O
,	O
we	O
must	O
specify	O
k	O
,	O
the	O
number	O
complexity	O
.	O
of	O
clusters	B
.	O
choosing	O
these	O
parameters	O
is	O
an	O
example	O
of	O
model	B
selection	I
.	O
we	O
discuss	O
some	O
approaches	O
below	O
.	O
in	O
particuarl	O
,	O
11.5.1	O
model	B
selection	I
for	O
probabilistic	O
models	O
the	O
optimal	O
bayesian	O
approach	O
,	O
discussed	O
in	O
section	O
5.3	O
,	O
is	O
to	O
pick	O
the	O
model	O
with	O
the	O
largest	O
marginal	B
likelihood	I
,	O
k∗	O
=	O
argmaxk	O
p	O
(	O
d|k	O
)	O
.	O
there	O
are	O
two	O
problems	O
with	O
this	O
.	O
likelihood	B
for	O
lvms	O
is	O
quite	O
difficult	O
.	O
in	O
practice	O
,	O
simple	O
approximations	O
,	O
such	O
as	O
bic	O
,	O
can	O
be	O
used	O
(	O
see	O
e.g.	O
,	O
(	O
fraley	O
and	O
raftery	O
2002	O
)	O
)	O
.	O
alternatively	O
,	O
we	O
can	O
use	O
the	O
cross-validated	O
likelihood	B
as	O
a	O
performance	O
measure	O
,	O
although	O
this	O
can	O
be	O
slow	O
,	O
since	O
it	O
requires	O
ﬁtting	O
each	O
model	O
f	O
times	O
,	O
where	O
f	O
is	O
the	O
number	O
of	O
cv	O
folds	B
.	O
first	O
,	O
evaluating	O
the	O
marginal	O
the	O
second	O
issue	O
is	O
the	O
need	O
to	O
search	O
over	O
a	O
potentially	O
large	O
number	O
of	O
models	O
.	O
the	O
usual	O
approach	O
is	O
to	O
perform	O
exhaustive	O
search	O
over	O
all	O
candidate	O
values	O
of	O
k.	O
however	O
,	O
sometimes	O
we	O
can	O
set	O
the	O
model	O
to	O
its	O
maximal	O
size	O
,	O
and	O
then	O
rely	O
on	O
the	O
power	O
of	O
the	O
bayesian	O
occam	O
’	O
s	O
razor	O
to	O
“	O
kill	O
off	O
”	O
unwanted	O
components	O
.	O
an	O
example	O
of	O
this	O
will	O
be	O
shown	O
in	O
section	O
21.6.1.6	O
,	O
when	O
we	O
discuss	O
variational	O
bayes	O
.	O
an	O
alternative	O
approach	O
is	O
to	O
perform	O
stochastic	O
sampling	O
in	O
the	O
space	O
of	O
models	O
.	O
traditional	O
approaches	O
,	O
such	O
as	O
(	O
green	O
1998	O
,	O
2003	O
;	O
lunn	O
et	O
al	O
.	O
2009	O
)	O
,	O
are	O
based	O
on	O
reversible	O
jump	O
mcmc	O
,	O
and	O
use	O
birth	B
moves	I
to	O
propose	B
new	O
centers	O
,	O
and	O
death	B
moves	I
to	O
kill	O
off	O
old	O
centers	O
.	O
however	O
,	O
this	O
can	O
be	O
slow	O
and	O
difficult	O
to	O
implement	O
.	O
a	O
simpler	O
approach	O
is	O
to	O
use	O
a	O
dirichlet	O
process	O
mixture	B
model	I
,	O
which	O
can	O
be	O
ﬁt	O
using	O
gibbs	O
sampling	O
,	O
but	O
still	O
allows	O
for	O
an	O
unbounded	O
number	O
of	O
mixture	B
components	O
;	O
see	O
section	O
25.2	O
for	O
details	O
.	O
perhaps	O
surprisingly	O
,	O
these	O
sampling-based	O
methods	O
can	O
be	O
faster	O
than	O
the	O
simple	O
approach	O
of	O
evaluating	O
the	O
quality	O
of	O
each	O
k	O
separately	O
.	O
the	O
reason	O
is	O
that	O
ﬁtting	O
the	O
model	O
for	O
each	O
k	O
is	O
often	O
slow	O
.	O
by	O
contrast	O
,	O
the	O
sampling	O
methods	O
can	O
often	O
quickly	O
determine	O
that	O
a	O
certain	O
value	O
of	O
k	O
is	O
poor	O
,	O
and	O
thus	O
they	O
need	O
not	O
waste	O
time	O
in	O
that	O
part	O
of	O
the	O
posterior	O
.	O
11.5.2	O
model	B
selection	I
for	O
non-probabilistic	O
methods	O
e	O
(	O
d	O
,	O
k	O
)	O
=	O
1|d|	O
||xi	O
−	O
ˆxi||2	O
(	O
cid:2	O
)	O
i∈d	O
what	O
if	O
we	O
are	O
not	O
using	O
a	O
probabilistic	O
model	O
?	O
for	O
example	O
,	O
how	O
do	O
we	O
choose	O
k	O
for	O
the	O
k-	O
means	O
algorithm	O
?	O
since	O
this	O
does	O
not	O
correspond	O
to	O
a	O
probability	O
model	O
,	O
there	O
is	O
no	O
likelihood	O
,	O
so	O
none	O
of	O
the	O
methods	O
described	O
above	O
can	O
be	O
used	O
.	O
struction	O
error	O
of	O
a	O
data	O
set	O
d	O
,	O
using	O
model	O
complexity	O
k	O
,	O
as	O
follows	O
:	O
an	O
obvious	O
proxy	O
for	O
the	O
likelihood	B
is	O
the	O
reconstruction	B
error	I
.	O
deﬁne	O
the	O
squared	O
recon-	O
(	O
11.95	O
)	O
in	O
the	O
case	O
of	O
k-means	O
,	O
the	O
reconstruction	O
is	O
given	O
by	O
ˆxi	O
=	O
μzi	O
,	O
where	O
zi	O
=	O
argmink	O
||xi	O
−	O
μk||2	O
2	O
,	O
as	O
explained	O
in	O
section	O
11.4.2.6.	O
figure	O
11.20	O
(	O
a	O
)	O
plots	O
the	O
reconstruction	B
error	I
on	O
the	O
test	O
set	O
for	O
k-means	O
.	O
we	O
notice	O
that	O
the	O
error	O
decreases	O
with	O
increasing	O
model	O
complexity	O
!	O
the	O
reason	O
for	O
this	O
behavior	O
is	O
as	O
follows	O
:	O
11.5.	O
model	B
selection	I
for	O
latent	B
variable	I
models	I
371	O
mse	O
on	O
test	O
vs	O
k	O
for	O
k−means	O
nll	O
on	O
test	O
set	O
vs	O
k	O
for	O
gmm	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
2	O
4	O
6	O
8	O
10	O
12	O
14	O
16	O
1245	O
1240	O
1235	O
1230	O
1225	O
1220	O
1215	O
1210	O
1205	O
1200	O
1195	O
2	O
4	O
6	O
8	O
10	O
12	O
14	O
16	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
11.20	O
test	O
set	O
performance	O
vs	O
k	O
for	O
data	O
generated	O
from	O
a	O
mixture	O
of	O
3	O
gaussians	O
in	O
1d	O
(	O
data	O
is	O
shown	O
in	O
figure	O
11.21	O
(	O
a	O
)	O
)	O
.	O
(	O
a	O
)	O
mse	O
on	O
test	O
set	O
for	O
k-means	O
.	O
(	O
b	O
)	O
negative	B
log	I
likelihood	I
on	O
test	O
set	O
for	O
gmm	O
.	O
figure	O
generated	O
by	O
kmeansmodelsel1d	O
.	O
60	O
50	O
40	O
30	O
20	O
10	O
xtrain	O
k=2	O
,	O
mse=0.2023	O
k=3	O
,	O
mse=0.0818	O
k=4	O
,	O
mse=0.0562	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−1	O
0	O
0	O
−2	O
1	O
0	O
0	O
−2	O
2	O
0	O
2	O
k=5	O
,	O
mse=0.0368	O
k=6	O
,	O
mse=0.0275	O
k=10	O
,	O
mse=0.0111	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−3	O
−2	O
−1	O
0	O
(	O
a	O
)	O
1	O
2	O
3	O
0	O
−2	O
0	O
0	O
−2	O
2	O
0	O
−2	O
2	O
0	O
2	O
0	O
(	O
b	O
)	O
k=2	O
,	O
nll=1244.7882	O
k=3	O
,	O
nll=1198.9738	O
k=4	O
,	O
nll=1196.9937	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−2	O
0	O
0	O
−2	O
2	O
0	O
0	O
−2	O
2	O
0	O
2	O
k=5	O
,	O
nll=1202.5869	O
k=6	O
,	O
nll=1199.5574	O
k=10	O
,	O
nll=1203.2931	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−2	O
0	O
0	O
−2	O
2	O
0	O
(	O
c	O
)	O
0	O
−2	O
2	O
0	O
2	O
figure	O
11.21	O
(	O
test	O
data	O
looks	O
essentially	O
the	O
same	O
.	O
)	O
(	O
c	O
)	O
gmm	O
density	O
model	O
estimated	O
by	O
em	O
for	O
for	O
the	O
same	O
values	O
of	O
k.	O
kmeansmodelsel1d	O
.	O
synthetic	O
data	O
generated	O
from	O
a	O
mixture	O
of	O
3	O
gaussians	O
in	O
1d	O
.	O
(	O
a	O
)	O
histogram	B
of	O
training	O
data	O
.	O
(	O
b	O
)	O
centroids	B
estimated	O
by	O
k-means	O
for	O
k	O
∈	O
{	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
6	O
,	O
10	O
}	O
.	O
figure	O
generated	O
by	O
372	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
when	O
we	O
add	O
more	O
and	O
more	O
centroids	B
to	O
k-means	O
,	O
we	O
can	O
“	O
tile	O
”	O
the	O
space	O
more	O
densely	O
,	O
as	O
shown	O
in	O
figure	O
11.21	O
(	O
b	O
)	O
.	O
hence	O
any	O
given	O
test	O
vector	O
is	O
more	O
likely	O
to	O
ﬁnd	O
a	O
close	O
prototype	B
to	O
accurately	O
represent	O
it	O
as	O
k	O
increases	O
,	O
thus	O
decreasing	O
reconstruction	B
error	I
.	O
however	O
,	O
if	O
we	O
use	O
a	O
probabilistic	O
model	O
,	O
such	O
as	O
the	O
gmm	O
,	O
and	O
plot	O
the	O
negative	O
log-likelihood	O
,	O
we	O
get	O
the	O
usual	O
u-shaped	O
curve	O
on	O
the	O
test	O
set	O
,	O
as	O
shown	O
in	O
figure	O
11.20	O
(	O
b	O
)	O
.	O
in	O
supervised	B
learning	I
,	O
we	O
can	O
always	O
use	O
cross	B
validation	I
to	O
select	O
between	O
non-probabilistic	O
models	O
of	O
different	O
complexity	O
,	O
but	O
this	O
is	O
not	O
the	O
case	O
with	O
unsupervised	B
learning	I
.	O
although	O
this	O
is	O
not	O
a	O
novel	O
observation	B
(	O
e.g.	O
,	O
it	O
is	O
mentioned	O
in	O
passing	O
in	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p519	O
)	O
,	O
one	O
of	O
the	O
standard	O
references	O
in	O
this	O
ﬁeld	O
)	O
,	O
it	O
is	O
perhaps	O
not	O
as	O
widely	O
appreciated	O
as	O
it	O
should	O
be	O
.	O
in	O
fact	O
,	O
it	O
is	O
one	O
of	O
the	O
more	O
compelling	O
arguments	O
in	O
favor	O
of	O
probabilistic	O
models	O
.	O
given	O
that	O
cross	B
validation	I
doesn	O
’	O
t	O
work	O
,	O
and	O
supposing	O
one	O
is	O
unwilling	O
to	O
use	O
probabilistic	O
models	O
(	O
for	O
some	O
bizarre	O
reason	O
...	O
)	O
,	O
how	O
can	O
one	O
choose	O
k	O
?	O
the	O
most	O
common	O
approach	O
is	O
to	O
plot	O
the	O
reconstruction	B
error	I
on	O
the	O
training	B
set	I
versus	O
k	O
,	O
and	O
to	O
try	O
to	O
identify	O
a	O
knee	B
or	O
kink	B
in	O
the	O
curve	O
.	O
the	O
idea	O
is	O
that	O
for	O
k	O
<	O
k∗	O
is	O
the	O
“	O
true	O
”	O
number	O
of	O
clusters	B
,	O
the	O
rate	B
of	O
decrease	O
in	O
the	O
error	B
function	I
will	O
be	O
high	O
,	O
since	O
we	O
are	O
splitting	O
apart	O
things	O
that	O
should	O
not	O
be	O
grouped	O
together	O
.	O
however	O
,	O
for	O
k	O
>	O
k∗	O
,	O
we	O
are	O
splitting	O
apart	O
“	O
natural	O
”	O
clusters	B
,	O
which	O
does	O
not	O
reduce	O
the	O
error	O
by	O
as	O
much	O
.	O
,	O
where	O
k∗	O
this	O
kink-ﬁnding	O
process	O
can	O
be	O
automated	O
by	O
use	O
of	O
the	O
gap	B
statistic	I
(	O
tibshirani	O
et	O
al	O
.	O
2001	O
)	O
.	O
nevertheless	O
,	O
identifying	O
such	O
kinks	O
can	O
be	O
hard	O
,	O
as	O
shown	O
in	O
figure	O
11.20	O
(	O
a	O
)	O
,	O
since	O
the	O
loss	B
function	I
usually	O
drops	O
off	O
gradually	O
.	O
a	O
different	O
approach	O
to	O
“	O
kink	B
ﬁnding	O
”	O
is	O
described	O
in	O
section	O
12.3.2.1	O
.	O
11.6	O
fitting	O
models	O
with	O
missing	B
data	I
suppose	O
we	O
want	O
to	O
ﬁt	O
a	O
joint	O
density	O
model	O
by	O
maximum	O
likelihood	O
,	O
but	O
we	O
have	O
“	O
holes	O
”	O
in	O
our	O
data	O
matrix	O
,	O
due	O
to	O
missing	B
data	I
(	O
usually	O
represented	O
by	O
nans	O
)	O
.	O
more	O
formally	O
,	O
let	O
oij	O
=	O
1	O
if	O
component	O
j	O
of	O
data	O
case	O
i	O
is	O
observed	O
,	O
and	O
let	O
oij	O
=	O
0	O
otherwise	O
.	O
let	O
xv	O
=	O
{	O
xij	O
:	O
oij	O
=	O
1	O
}	O
be	O
the	O
visible	B
data	O
,	O
and	O
xh	O
=	O
{	O
xij	O
:	O
oij	O
=	O
0	O
}	O
be	O
the	O
missing	B
or	O
hidden	B
data	O
.	O
our	O
goal	O
is	O
to	O
compute	O
ˆθ	O
=	O
argmax	O
θ	O
p	O
(	O
xv|θ	O
,	O
o	O
)	O
(	O
11.96	O
)	O
(	O
11.98	O
)	O
(	O
11.99	O
)	O
under	O
the	O
missing	B
at	I
random	I
assumption	O
(	O
see	O
section	O
8.6.2	O
)	O
,	O
we	O
have	O
p	O
(	O
xv|θ	O
,	O
o	O
)	O
=	O
p	O
(	O
xiv|θ	O
)	O
(	O
11.97	O
)	O
where	O
xiv	O
is	O
a	O
vector	O
created	O
from	O
row	O
i	O
and	O
the	O
columns	O
indexed	O
by	O
the	O
set	O
{	O
j	O
:	O
oij	O
=	O
1	O
}	O
.	O
hence	O
the	O
log-likelihood	O
has	O
the	O
form	O
i=1	O
n	O
(	O
cid:27	O
)	O
(	O
cid:2	O
)	O
log	O
p	O
(	O
xv|θ	O
)	O
=	O
log	O
p	O
(	O
xiv|θ	O
)	O
where	O
p	O
(	O
xiv|θ	O
)	O
=	O
(	O
cid:2	O
)	O
xih	O
i	O
p	O
(	O
xiv	O
,	O
xih|θ	O
)	O
11.6.	O
fitting	O
models	O
with	O
missing	B
data	I
373	O
and	O
xih	O
is	O
the	O
vector	O
of	O
hidden	B
variables	I
for	O
case	O
i	O
(	O
assumed	O
discrete	O
for	O
notational	O
simplicity	O
)	O
.	O
substituting	O
in	O
,	O
we	O
get	O
(	O
cid:19	O
)	O
log	O
p	O
(	O
xv|θ	O
)	O
=	O
p	O
(	O
xiv	O
,	O
xih|θ	O
)	O
(	O
cid:2	O
)	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
log	O
i	O
xih	O
(	O
11.100	O
)	O
unfortunately	O
,	O
this	O
objective	O
is	O
hard	O
to	O
maximize	O
.	O
since	O
we	O
can	O
not	O
push	O
the	O
log	O
inside	O
the	O
sum	O
.	O
however	O
,	O
we	O
can	O
use	O
the	O
em	O
algorithm	O
to	O
compute	O
a	O
local	O
optimum	O
.	O
we	O
give	O
an	O
example	O
of	O
this	O
below	O
.	O
11.6.1	O
em	O
for	O
the	O
mle	O
of	O
an	O
mvn	O
with	O
missing	B
data	I
suppose	O
we	O
want	O
to	O
ﬁt	O
an	O
mvn	O
by	O
maximum	O
likelihood	O
,	O
but	O
we	O
have	O
missing	B
data	I
.	O
we	O
can	O
use	O
em	O
to	O
ﬁnd	O
a	O
local	O
maximum	O
of	O
the	O
objective	O
,	O
as	O
we	O
explain	O
below	O
.	O
11.6.1.1	O
getting	O
started	O
to	O
get	O
the	O
algorithm	O
started	O
,	O
we	O
can	O
compute	O
the	O
mle	O
based	O
on	O
those	O
rows	O
of	O
the	O
data	O
ma-	O
trix	O
that	O
are	O
fully	O
observed	O
.	O
if	O
there	O
are	O
no	O
such	O
rows	O
,	O
we	O
can	O
use	O
some	O
ad-hoc	O
imputation	B
procedures	O
,	O
and	O
then	O
compute	O
an	O
initial	O
mle	O
.	O
11.6.1.2	O
e	O
step	O
once	O
we	O
have	O
θt−1	O
,	O
we	O
can	O
compute	O
the	O
expected	B
complete	I
data	I
log	I
likelihood	I
at	O
iteration	O
t	O
as	O
follows	O
:	O
(	O
cid:19	O
)	O
q	O
(	O
θ	O
,	O
θt−1	O
)	O
=e	O
log	O
n	O
(	O
xi|μ	O
,	O
σ	O
)	O
|d	O
,	O
θt−1	O
(	O
cid:18	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
cid:2	O
)	O
(	O
cid:31	O
)	O
e	O
i	O
tr	O
(	O
σ−1	O
(	O
xi	O
−	O
μ	O
)	O
t	O
σ−1	O
(	O
xi	O
−	O
μ	O
)	O
(	O
cid:2	O
)	O
(	O
cid:31	O
)	O
(	O
xi	O
−	O
μ	O
)	O
(	O
xi	O
−	O
μ	O
)	O
t	O
e	O
i	O
tr	O
(	O
σ−1	O
e	O
[	O
s	O
(	O
μ	O
)	O
]	O
)	O
(	O
11.101	O
)	O
(	O
11.102	O
)	O
(	O
11.103	O
)	O
(	O
11.104	O
)	O
=	O
−	O
n	O
2	O
=	O
−	O
n	O
2	O
=	O
−	O
n	O
2	O
(	O
cid:5	O
)	O
(	O
cid:2	O
)	O
(	O
cid:31	O
)	O
log	O
|2πς|	O
−	O
1	O
2	O
log	O
|2πς|	O
−	O
1	O
2	O
log	O
|σ|	O
−	O
n	O
d	O
2	O
log	O
(	O
2π	O
)	O
−	O
1	O
2	O
(	O
cid:6	O
)	O
t	O
where	O
e	O
e	O
[	O
s	O
(	O
μ	O
)	O
]	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
(	O
11.105	O
)	O
(	O
we	O
drop	O
the	O
conditioning	B
of	O
the	O
expectation	O
on	O
d	O
and	O
θt−1	O
for	O
brevity	O
.	O
)	O
we	O
see	O
that	O
we	O
need	O
to	O
compute	O
;	O
these	O
are	O
the	O
expected	B
sufficient	I
statistics	I
.	O
+	O
μμt	O
−	O
2μe	O
[	O
xi	O
]	O
(	O
cid:31	O
)	O
xixt	O
i	O
(	O
cid:10	O
)	O
i	O
e	O
[	O
xi	O
]	O
and	O
i	O
e	O
xixt	O
i	O
i	O
i	O
,	O
where	O
components	O
v	O
are	O
observed	O
and	O
components	O
h	O
are	O
unobserved	O
.	O
we	O
have	O
to	O
compute	O
these	O
quantities	O
,	O
we	O
use	O
the	O
results	O
from	O
section	O
4.3.1.	O
speciﬁcally	O
,	O
consider	O
case	O
xih|xiv	O
,	O
θ	O
∼	O
n	O
(	O
mi	O
,	O
vi	O
)	O
mi	O
(	O
cid:2	O
)	O
μh	O
+	O
σhvς−1	O
vi	O
(	O
cid:2	O
)	O
σhh	O
−	O
σhvς−1	O
vv	O
(	O
xiv	O
−	O
μv	O
)	O
vv	O
σvh	O
(	O
11.106	O
)	O
(	O
11.107	O
)	O
(	O
11.108	O
)	O
374	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
hence	O
the	O
expected	B
sufficient	I
statistics	I
are	O
e	O
[	O
xi	O
]	O
=	O
(	O
e	O
[	O
xih	O
]	O
;	O
xiv	O
)	O
=	O
(	O
mi	O
;	O
xiv	O
)	O
(	O
11.109	O
)	O
where	O
we	O
have	O
assumed	O
(	O
without	O
loss	B
of	O
generality	O
)	O
that	O
the	O
unobserved	O
variables	O
come	O
before	O
the	O
observed	O
variables	O
in	O
the	O
node	O
ordering	O
.	O
(	O
cid:31	O
)	O
(	O
cid:29	O
)	O
(	O
cid:8	O
)	O
,	O
we	O
use	O
the	O
result	O
that	O
cov	O
[	O
x	O
]	O
=	O
e	O
xihxt	O
e	O
ih	O
xiv	O
e	O
[	O
xih	O
]	O
xt	O
ih	O
xt	O
iv	O
(	O
cid:9	O
)	O
(	O
cid:3	O
)	O
xih	O
xiv	O
(	O
cid:4	O
)	O
(	O
cid:30	O
)	O
(	O
cid:8	O
)	O
(	O
cid:31	O
)	O
=	O
(	O
cid:31	O
)	O
t	O
(	O
cid:31	O
)	O
−	O
e	O
[	O
x	O
]	O
e	O
(	O
cid:9	O
)	O
xt	O
xxt	O
e	O
[	O
xih	O
]	O
xt	O
iv	O
xivxt	O
iv	O
to	O
compute	O
e	O
xixt	O
i	O
(	O
cid:31	O
)	O
e	O
(	O
cid:31	O
)	O
xixt	O
i	O
=	O
e	O
.	O
hence	O
(	O
11.110	O
)	O
(	O
11.111	O
)	O
e	O
xihxt	O
ih	O
=	O
e	O
[	O
xih	O
]	O
e	O
[	O
xih	O
]	O
t	O
+	O
vi	O
11.6.1.3	O
m	O
step	O
by	O
solving	O
∇q	O
(	O
θ	O
,	O
θ	O
(	O
t−1	O
)	O
)	O
=0	O
,	O
we	O
can	O
show	O
that	O
the	O
m	O
step	O
is	O
equivalent	O
to	O
plugging	O
these	O
ess	O
into	O
the	O
usual	O
mle	O
equations	O
to	O
get	O
μt	O
=	O
σt	O
=	O
1	O
n	O
1	O
n	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
e	O
[	O
xi	O
]	O
(	O
cid:31	O
)	O
xixt	O
i	O
e	O
i	O
−	O
μt	O
(	O
μt	O
)	O
t	O
(	O
11.112	O
)	O
(	O
11.113	O
)	O
thus	O
we	O
see	O
that	O
em	O
is	O
not	O
equivalent	O
to	O
simply	O
replacing	O
variables	O
by	O
their	O
expectations	O
and	O
applying	O
the	O
standard	O
mle	O
formula	O
;	O
that	O
would	O
ignore	O
the	O
posterior	O
variance	O
and	O
would	O
result	O
in	O
an	O
incorrect	O
estimate	O
.	O
instead	O
we	O
must	O
compute	O
the	O
expectation	O
of	O
the	O
sufficient	B
statistics	I
,	O
and	O
plug	O
that	O
into	O
the	O
usual	O
equation	O
for	O
the	O
mle	O
.	O
we	O
can	O
easily	O
modify	O
the	O
algorithm	O
to	O
perform	O
map	O
estimation	O
,	O
by	O
plugging	O
in	O
the	O
ess	O
into	O
the	O
equation	O
for	O
the	O
map	O
estimate	O
.	O
for	O
an	O
implementation	O
,	O
see	O
gaussmissingfitem	O
.	O
11.6.1.4	O
example	O
as	O
an	O
example	O
of	O
this	O
procedure	O
in	O
action	B
,	O
let	O
us	O
reconsider	O
the	O
imputation	B
problem	O
from	O
section	O
4.3.2.3	O
,	O
which	O
had	O
n	O
=	O
100	O
10-dimensional	O
data	O
cases	O
,	O
with	O
50	O
%	O
missing	B
data	I
.	O
let	O
us	O
ﬁt	O
the	O
parameters	O
using	O
em	O
.	O
call	O
the	O
resulting	O
parameters	O
ˆθ	O
.	O
we	O
can	O
use	O
our	O
model	O
for	O
predictions	O
by	O
computing	O
e	O
.	O
figure	O
11.22	O
(	O
a-b	O
)	O
indicates	O
that	O
the	O
results	O
obtained	O
using	O
the	O
learned	O
parameters	O
are	O
almost	O
as	O
good	O
as	O
with	O
the	O
true	O
parameters	O
.	O
not	O
surprisingly	O
,	O
performance	O
improves	O
with	O
more	O
data	O
,	O
or	O
as	O
the	O
fraction	O
of	O
missing	O
data	O
is	O
reduced	O
.	O
xih|xiv	O
,	O
ˆθ	O
,	O
+	O
11.6.1.5	O
extension	B
to	O
the	O
gmm	O
case	O
it	O
is	O
straightforward	O
to	O
ﬁt	O
a	O
mixture	O
of	O
gaussians	O
in	O
the	O
presence	O
of	O
partially	O
observed	O
data	O
vectors	O
xi	O
.	O
we	O
leave	O
the	O
details	O
as	O
an	O
exercise	O
.	O
exercises	O
exercise	O
11.1	O
student	O
t	O
as	O
inﬁnite	O
mixture	O
of	O
gaussians	O
derive	O
equation	O
11.61.	O
for	O
simplicity	O
,	O
assume	O
a	O
one-dimensional	O
distribution	O
.	O
11.6.	O
fitting	O
models	O
with	O
missing	B
data	I
375	O
imputation	B
with	O
true	O
params	O
r2	O
=	O
0.260	O
r2	O
=	O
0.685	O
imputation	B
with	O
em	O
r2	O
=	O
0.220	O
r2	O
=	O
0.609	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
10	O
5	O
0	O
−5	O
−10	O
−10	O
10	O
5	O
0	O
−5	O
−10	O
−10	O
0	O
10	O
truth	O
r2	O
=	O
0.399	O
0	O
truth	O
10	O
(	O
a	O
)	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
20	O
10	O
0	O
−10	O
−20	O
10	O
5	O
0	O
−5	O
−10	O
−10	O
0	O
20	O
truth	O
r2	O
=	O
0.531	O
0	O
truth	O
10	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
10	O
5	O
0	O
−5	O
−10	O
−10	O
10	O
5	O
0	O
−5	O
−10	O
−10	O
0	O
10	O
truth	O
r2	O
=	O
0.113	O
0	O
truth	O
10	O
(	O
b	O
)	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
d	O
e	O
t	O
u	O
p	O
m	O
i	O
20	O
10	O
0	O
−10	O
−20	O
10	O
5	O
0	O
−5	O
−10	O
−10	O
0	O
20	O
truth	O
r2	O
=	O
0.532	O
0	O
truth	O
10	O
figure	O
11.22	O
illustration	O
of	O
data	O
imputation	O
.	O
ing	O
true	O
parameters	O
.	O
gaussimputationdemo	O
.	O
(	O
a	O
)	O
scatter	B
plot	I
of	O
true	O
values	O
vs	O
imputed	O
values	O
us-	O
(	O
b	O
)	O
same	O
as	O
(	O
b	O
)	O
,	O
but	O
using	O
parameters	O
estimated	O
with	O
em	O
.	O
figure	O
generated	O
by	O
exercise	O
11.2	O
em	O
for	O
mixtures	O
of	O
gaussians	O
show	O
that	O
the	O
m	O
step	O
for	O
ml	O
estimation	O
of	O
a	O
mixture	O
of	O
gaussians	O
is	O
given	O
by	O
μk	O
=	O
σk	O
=	O
i	O
rikxi	O
rk	O
i	O
rik	O
(	O
xi	O
−	O
μk	O
)	O
(	O
xi	O
−	O
μk	O
)	O
t	O
rk	O
(	O
cid:2	O
)	O
=	O
i	O
−	O
rkμkμt	O
i	O
rikxixt	O
rk	O
k	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
exercise	O
11.4	O
em	O
for	O
mixture	O
of	O
student	O
distributions	O
derive	O
the	O
em	O
algorithm	O
for	O
ml	O
estimation	O
of	O
a	O
mixture	O
of	O
multivariate	O
student	O
t	O
distributions	O
.	O
exercise	O
11.5	O
gradient	B
descent	I
for	O
ﬁtting	O
gmm	O
consider	O
the	O
gaussian	O
mixture	B
model	I
πkn	O
(	O
x|μk	O
,	O
σk	O
)	O
p	O
(	O
x|θ	O
)	O
=	O
(	O
cid:12	O
)	O
k	O
deﬁne	O
the	O
log	O
likelihood	O
as	O
(	O
cid:7	O
)	O
(	O
θ	O
)	O
=	O
log	O
p	O
(	O
xn|θ	O
)	O
n	O
(	O
cid:12	O
)	O
n=1	O
exercise	O
11.3	O
em	O
for	O
mixtures	O
of	O
bernoullis	O
•	O
show	O
that	O
the	O
m	O
step	O
for	O
ml	O
estimation	O
of	O
a	O
mixture	O
of	O
bernoullis	O
is	O
given	O
by	O
i	O
rik	O
(	O
cid:2	O
)	O
i	O
rikxij	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
i	O
rikxij	O
)	O
+α	O
−	O
1	O
i	O
rik	O
)	O
+α	O
+	O
β	O
−	O
2	O
(	O
(	O
μkj	O
=	O
μkj	O
=	O
•	O
show	O
that	O
the	O
m	O
step	O
for	O
map	O
estimation	O
of	O
a	O
mixture	O
of	O
bernoullis	O
with	O
a	O
β	O
(	O
α	O
,	O
β	O
)	O
prior	O
is	O
given	O
by	O
(	O
11.114	O
)	O
(	O
11.115	O
)	O
(	O
11.116	O
)	O
(	O
11.117	O
)	O
(	O
11.118	O
)	O
(	O
11.119	O
)	O
376	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
p	O
q	O
jn	O
kn	O
xn	O
μj	O
m	O
n	O
σk	O
l	O
figure	O
11.23	O
a	O
mixture	O
of	O
gaussians	O
with	O
two	O
discrete	B
latent	O
indicators	O
.	O
jn	O
speciﬁes	O
which	O
mean	B
to	O
use	O
,	O
and	O
kn	O
speciﬁes	O
which	O
variance	B
to	O
use	O
.	O
deﬁne	O
the	O
posterior	O
responsibility	O
that	O
cluster	O
k	O
has	O
for	O
datapoint	O
n	O
as	O
follows	O
:	O
rnk	O
(	O
cid:2	O
)	O
p	O
(	O
zn	O
=	O
k|xn	O
,	O
θ	O
)	O
=	O
(	O
cid:2	O
)	O
k	O
πkn	O
(	O
xn|μk	O
,	O
σk	O
)	O
k	O
(	O
cid:2	O
)	O
=1	O
πk	O
(	O
cid:2	O
)	O
n	O
(	O
xn|μk	O
(	O
cid:2	O
)	O
,	O
σk	O
(	O
cid:2	O
)	O
)	O
a.	O
show	O
that	O
the	O
gradient	O
of	O
the	O
log-likelihood	O
wrt	O
μk	O
is	O
(	O
11.120	O
)	O
(	O
11.121	O
)	O
(	O
11.122	O
)	O
(	O
11.123	O
)	O
(	O
cid:12	O
)	O
n	O
d	O
dμk	O
(	O
cid:7	O
)	O
(	O
θ	O
)	O
=	O
rnkς−1	O
k	O
(	O
xn	O
−	O
μk	O
)	O
(	O
cid:2	O
)	O
k	O
πk	O
(	O
cid:2	O
)	O
ewk	O
(	O
cid:2	O
)	O
k	O
k	O
(	O
cid:2	O
)	O
=1	O
ewk	O
(	O
cid:2	O
)	O
(	O
cid:12	O
)	O
d	O
dwk	O
(	O
cid:7	O
)	O
(	O
θ	O
)	O
=	O
n	O
rnk	O
−	O
πk	O
b.	O
derive	O
the	O
gradient	O
of	O
the	O
log-likelihood	O
wrt	O
πk	O
.	O
(	O
for	O
now	O
,	O
ignore	O
any	O
constraints	O
on	O
πk	O
.	O
)	O
c.	O
one	O
way	O
to	O
handle	O
the	O
constraint	O
that	O
k=1	O
πk	O
=	O
1	O
is	O
to	O
reparameterize	O
using	O
the	O
softmax	B
function	O
:	O
here	O
wk	O
∈	O
r	O
are	O
unconstrained	O
parameters	O
.	O
show	O
that	O
(	O
there	O
may	O
be	O
a	O
constant	O
factor	O
missing	B
in	O
the	O
above	O
expression	O
...	O
)	O
hint	O
:	O
use	O
the	O
chain	B
rule	I
and	O
the	O
fact	O
that	O
dπj	O
dwk	O
πj	O
(	O
1	O
−	O
πj	O
)	O
−πjπk	O
if	O
j	O
=	O
k	O
if	O
j	O
(	O
cid:8	O
)	O
=	O
k	O
(	O
11.124	O
)	O
=	O
which	O
follows	O
from	O
exercise	O
8.4	O
(	O
1	O
)	O
.	O
d.	O
derive	O
the	O
gradient	O
of	O
the	O
log-likelihood	O
wrt	O
σk	O
.	O
(	O
for	O
now	O
,	O
ignore	O
any	O
constraints	O
on	O
σk	O
.	O
)	O
e.	O
one	O
way	O
to	O
handle	O
the	O
constraint	O
that	O
σk	O
be	O
a	O
symmetric	B
positive	O
deﬁnite	O
matrix	O
is	O
to	O
reparame-	O
k	O
r	O
,	O
where	O
r	O
is	O
an	O
upper-triangular	O
,	O
but	O
otherwise	O
terize	O
using	O
a	O
cholesky	O
decomposition	O
,	O
σk	O
=	O
rt	O
unconstrained	O
matrix	O
.	O
derive	O
the	O
gradient	O
of	O
the	O
log-likelihood	O
wrt	O
rk	O
.	O
exercise	O
11.6	O
em	O
for	O
a	O
ﬁnite	O
scale	O
mixture	O
of	O
gaussians	O
(	O
source	O
:	O
jaakkola..	O
)	O
consider	O
the	O
graphical	B
model	I
in	O
figure	O
11.23	O
which	O
deﬁnes	O
the	O
following	O
:	O
(	O
cid:19	O
)	O
m	O
(	O
cid:12	O
)	O
(	O
cid:18	O
)	O
l	O
(	O
cid:12	O
)	O
j=1	O
k=1	O
p	O
(	O
xn|θ	O
)	O
=	O
pj	O
qkn	O
(	O
xn|μj	O
,	O
σ2	O
k	O
)	O
(	O
11.125	O
)	O
11.6.	O
fitting	O
models	O
with	O
missing	B
data	I
377	O
where	O
θ	O
=	O
{	O
p1	O
,	O
.	O
.	O
.	O
,	O
pm	O
,	O
μ1	O
,	O
.	O
.	O
.	O
,	O
μm	O
,	O
q1	O
,	O
.	O
.	O
.	O
,	O
ql	O
,	O
σ2	O
l	O
}	O
are	O
all	O
the	O
parameters	O
.	O
here	O
pj	O
(	O
cid:2	O
)	O
p	O
(	O
jn	O
=	O
j	O
)	O
and	O
qk	O
(	O
cid:2	O
)	O
p	O
(	O
kn	O
=	O
k	O
)	O
are	O
the	O
equivalent	O
of	O
mixture	B
weights	O
.	O
we	O
can	O
think	O
of	O
this	O
as	O
a	O
mixture	B
(	O
cid:2	O
)	O
l	O
of	O
m	O
non-gaussian	O
components	O
,	O
where	O
each	O
component	O
distribution	O
is	O
a	O
scale	O
mixture	O
,	O
p	O
(	O
x|j	O
;	O
θ	O
)	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
σ2	O
k	O
)	O
,	O
combining	O
gaussians	O
with	O
different	O
variances	O
(	O
scales	O
)	O
.	O
k=1	O
qkn	O
(	O
x	O
;	O
μj	O
,	O
σ2	O
we	O
will	O
now	O
derive	O
a	O
generalized	O
em	O
algorithm	O
for	O
this	O
model	O
.	O
partial	O
update	O
in	O
the	O
m	O
step	O
,	O
rather	O
than	O
ﬁnding	O
the	O
exact	O
maximum	O
.	O
)	O
a.	O
derive	O
an	O
expression	O
for	O
the	O
responsibilities	O
,	O
p	O
(	O
jn	O
=	O
j	O
,	O
kn	O
=	O
k|xn	O
,	O
θ	O
)	O
,	O
needed	O
for	O
the	O
e	O
step	O
.	O
b.	O
write	O
out	O
a	O
full	B
expression	O
for	O
the	O
expected	O
complete	O
log-likelihood	O
(	O
recall	B
that	O
in	O
generalized	O
em	O
,	O
we	O
do	O
a	O
q	O
(	O
θnew	O
,	O
θold	O
)	O
=	O
eθold	O
log	O
p	O
(	O
jn	O
,	O
kn	O
,	O
xn|θnew	O
)	O
(	O
11.126	O
)	O
n=1	O
c.	O
solving	O
the	O
m-step	O
would	O
require	O
us	O
to	O
jointly	O
optimize	O
the	O
means	O
μ1	O
,	O
.	O
.	O
.	O
,	O
μm	O
and	O
the	O
variances	O
l	O
.	O
it	O
will	O
turn	O
out	O
to	O
be	O
simpler	O
to	O
ﬁrst	O
solve	O
for	O
the	O
μj	O
’	O
s	O
given	O
ﬁxed	O
σ2	O
j	O
’	O
s	O
,	O
and	O
subsequently	O
j	O
’	O
s	O
given	O
the	O
new	O
values	O
of	O
μj	O
’	O
s	O
.	O
for	O
brevity	O
,	O
we	O
will	O
just	O
do	O
the	O
ﬁrst	O
part	O
.	O
derive	O
an	O
σ2	O
1	O
,	O
.	O
.	O
.	O
,	O
σ2	O
solve	O
for	O
σ2	O
expression	O
for	O
the	O
maximizing	O
μj	O
’	O
s	O
given	O
ﬁxed	O
σ2	O
1	O
:	O
l	O
,	O
i.e.	O
,	O
solve	O
∂q	O
∂μnew	O
=	O
0.	O
n	O
(	O
cid:12	O
)	O
exercise	O
11.7	O
manual	O
calculation	O
of	O
the	O
m	O
step	O
for	O
a	O
gmm	O
(	O
source	O
:	O
de	O
freitas	O
.	O
)	O
in	O
this	O
question	O
we	O
consider	O
clustering	B
1d	O
data	O
with	O
a	O
mixture	O
of	O
2	O
gaussians	O
using	O
the	O
em	O
algorithm	O
.	O
you	O
are	O
given	O
the	O
1-d	O
data	O
points	O
x	O
=	O
[	O
1	O
20	O
]	O
.	O
suppose	O
the	O
output	O
of	O
the	O
e	O
step	O
is	O
the	O
following	O
matrix	O
:	O
10	O
⎡	O
⎣	O
1	O
0.4	O
0	O
⎤	O
⎦	O
0	O
0.6	O
1	O
r	O
=	O
(	O
11.127	O
)	O
where	O
entry	O
ri	O
,	O
c	O
is	O
the	O
probability	O
of	O
obervation	O
xi	O
belonging	O
to	O
cluster	O
c	O
(	O
the	O
responsibility	B
of	O
cluster	O
c	O
for	O
data	O
point	O
i	O
)	O
.	O
you	O
just	O
have	O
to	O
compute	O
the	O
m	O
step	O
.	O
you	O
may	O
state	B
the	O
equations	O
for	O
maximum	O
likelihood	O
estimates	O
of	O
these	O
quantities	O
(	O
which	O
you	O
should	O
know	O
)	O
without	O
proof	O
;	O
you	O
just	O
have	O
to	O
apply	O
the	O
equations	O
to	O
this	O
data	O
set	O
.	O
you	O
may	O
leave	O
your	O
answer	O
in	O
fractional	O
form	O
.	O
show	O
your	O
work	O
.	O
a.	O
write	O
down	O
the	O
likelihood	B
function	O
you	O
are	O
trying	O
to	O
optimize	O
.	O
b.	O
after	O
performing	O
the	O
m	O
step	O
for	O
the	O
mixing	B
weights	I
π1	O
,	O
π2	O
,	O
what	O
are	O
the	O
new	O
values	O
?	O
c.	O
after	O
performing	O
the	O
m	O
step	O
for	O
the	O
means	O
μ1	O
and	O
μ2	O
,	O
what	O
are	O
the	O
new	O
values	O
?	O
exercise	O
11.8	O
moments	O
of	O
a	O
mixture	O
of	O
gaussians	O
consider	O
a	O
mixture	O
of	O
k	O
gaussians	O
k	O
(	O
cid:12	O
)	O
p	O
(	O
x	O
)	O
=	O
πkn	O
(	O
x|μk	O
,	O
σk	O
)	O
k=1	O
a.	O
show	O
that	O
e	O
[	O
x	O
]	O
=	O
(	O
cid:12	O
)	O
k	O
πkμk	O
(	O
11.128	O
)	O
(	O
11.129	O
)	O
378	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
0	O
2	O
4	O
6	O
8	O
10	O
12	O
14	O
16	O
18	O
figure	O
11.24	O
some	O
data	O
points	O
in	O
2d	O
.	O
circles	O
represent	O
the	O
initial	O
guesses	O
for	O
m1	O
and	O
m2	O
.	O
b.	O
show	O
that	O
cov	O
[	O
x	O
]	O
=	O
(	O
cid:12	O
)	O
k	O
πk	O
[	O
σk	O
+	O
μkμt	O
(	O
cid:22	O
)	O
k	O
]	O
−	O
e	O
[	O
x	O
]	O
e	O
[	O
x	O
]	O
t	O
(	O
cid:23	O
)	O
−	O
e	O
[	O
x	O
]	O
e	O
[	O
x	O
]	O
t	O
.	O
xxt	O
hint	O
:	O
use	O
the	O
fact	O
that	O
cov	O
[	O
x	O
]	O
=	O
e	O
(	O
11.130	O
)	O
exercise	O
11.9	O
k-means	O
clustering	B
by	O
hand	O
(	O
source	O
:	O
jaakkola	O
.	O
)	O
in	O
figure	O
11.24	O
,	O
we	O
show	O
some	O
data	O
points	O
which	O
lie	O
on	O
the	O
integer	O
grid	O
.	O
(	O
note	O
that	O
the	O
x-axis	O
has	O
been	O
compressed	O
;	O
distances	O
should	O
be	O
measured	O
using	O
the	O
actual	O
grid	O
coordinates	O
.	O
)	O
suppose	O
we	O
apply	O
the	O
k-	O
means	O
algorithm	O
to	O
this	O
data	O
,	O
using	O
k	O
=	O
2	O
and	O
with	O
the	O
centers	O
initialized	O
at	O
the	O
two	O
circled	O
data	O
points	O
.	O
draw	O
the	O
ﬁnal	O
clusters	B
obtained	O
after	O
k-means	O
converges	O
(	O
show	O
the	O
approximate	O
location	O
of	O
the	O
new	O
centers	O
and	O
group	O
together	O
all	O
the	O
points	O
assigned	O
to	O
each	O
center	O
)	O
.	O
hint	O
:	O
think	O
about	O
shortest	O
euclidean	O
distance	O
.	O
exercise	O
11.10	O
deriving	O
the	O
k-means	O
cost	O
function	O
show	O
that	O
k	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
k=1	O
i	O
:	O
zi=k	O
i	O
(	O
cid:2	O
)	O
:	O
zi	O
(	O
cid:2	O
)	O
=k	O
(	O
xi	O
−	O
xi	O
(	O
cid:2	O
)	O
)	O
2	O
=	O
k	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
nk	O
(	O
xi	O
−	O
xk	O
)	O
2	O
k=1	O
i	O
:	O
zi=k	O
(	O
11.131	O
)	O
(	O
11.132	O
)	O
(	O
11.133	O
)	O
(	O
11.134	O
)	O
(	O
11.135	O
)	O
jw	O
(	O
z	O
)	O
=	O
1	O
2	O
(	O
cid:12	O
)	O
hint	O
:	O
note	O
that	O
,	O
for	O
any	O
μ	O
,	O
(	O
xi	O
−	O
μ	O
)	O
2	O
=	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
i	O
i	O
i	O
where	O
s2	O
=	O
1	O
n	O
(	O
cid:12	O
)	O
(	O
x	O
−	O
μ	O
)	O
2	O
−	O
2	O
(	O
cid:12	O
)	O
i	O
(	O
xi	O
−	O
x	O
)	O
(	O
μ	O
−	O
x	O
)	O
(	O
cid:12	O
)	O
[	O
(	O
xi	O
−	O
x	O
)	O
−	O
(	O
μ	O
−	O
x	O
)	O
]	O
2	O
(	O
xi	O
−	O
x	O
)	O
2	O
+	O
=	O
=	O
ns2	O
+	O
n	O
(	O
x	O
−	O
μ	O
)	O
2	O
%	O
(	O
cid:2	O
)	O
n	O
i=1	O
(	O
xi	O
−	O
x	O
)	O
2	O
,	O
since	O
i	O
i	O
(	O
cid:12	O
)	O
(	O
i	O
(	O
xi	O
−	O
x	O
)	O
(	O
μ	O
−	O
x	O
)	O
=	O
(	O
μ	O
−	O
x	O
)	O
xi	O
)	O
−	O
nx	O
&	O
=	O
(	O
μ	O
−	O
x	O
)	O
(	O
nx	O
−	O
nx	O
)	O
=	O
0	O
exercise	O
11.11	O
visible	B
mixtures	O
of	O
gaussians	O
are	O
in	O
the	O
exponential	B
family	I
show	O
that	O
the	O
joint	B
distribution	I
p	O
(	O
x	O
,	O
z|θ	O
)	O
for	O
a	O
1d	O
gmm	O
can	O
be	O
represented	O
in	O
exponential	B
family	I
form	O
.	O
11.6.	O
fitting	O
models	O
with	O
missing	B
data	I
379	O
e	O
m	O
i	O
t	O
l	O
a	O
v	O
v	O
r	O
u	O
s	O
i	O
4.6	O
4.4	O
4.2	O
4	O
3.8	O
3.6	O
3.4	O
3.2	O
3	O
2.8	O
2.6	O
2	O
regression	B
with	O
censored	O
data	O
;	O
red	O
x	O
=	O
censored	O
,	O
green	O
*	O
=	O
predicted	O
em	O
ols	O
2.05	O
2.1	O
2.15	O
2.2	O
2.25	O
2.3	O
2.35	O
2.4	O
inverse	O
temperature	O
figure	O
11.25	O
example	O
of	O
censored	O
linear	O
regression	B
.	O
black	O
circles	O
are	O
observed	O
training	O
points	O
,	O
red	O
crosses	O
are	O
observed	O
but	O
censored	O
training	O
points	O
.	O
green	O
stars	O
are	O
predicted	O
values	O
of	O
the	O
censored	O
training	O
points	O
.	O
we	O
also	O
show	O
the	O
lines	O
ﬁt	O
by	O
least	B
squares	I
(	O
ignoring	O
censoring	O
)	O
and	O
by	O
em	O
.	O
based	O
on	O
figure	O
5.6	O
of	O
(	O
tanner	O
1996	O
)	O
.	O
figure	O
generated	O
by	O
linregcensoredschmeehahndemo	O
,	O
written	O
by	O
hannes	O
bretschneider	O
.	O
exercise	O
11.12	O
em	O
for	O
robust	B
linear	O
regression	B
with	O
a	O
student	O
t	O
likelihood	O
consider	O
a	O
model	O
of	O
the	O
form	O
p	O
(	O
yi|xi	O
,	O
w	O
,	O
σ2	O
,	O
ν	O
)	O
=	O
t	O
(	O
yi|wt	O
xi	O
,	O
σ2	O
,	O
ν	O
)	O
(	O
11.136	O
)	O
derive	O
an	O
em	O
algorithm	O
to	O
compute	O
the	O
mle	O
for	O
w.	O
you	O
may	O
assume	O
ν	O
and	O
σ2	O
are	O
ﬁxed	O
,	O
for	O
simplicity	O
.	O
hint	O
:	O
see	O
section	O
11.4.5.	O
exercise	O
11.13	O
em	O
for	O
eb	O
estimation	O
of	O
gaussian	O
shrinkage	B
model	O
extend	O
the	O
results	O
of	O
section	O
5.6.2.2	O
to	O
the	O
case	O
where	O
the	O
σ2	O
j	O
are	O
not	O
equal	O
(	O
but	O
are	O
known	O
)	O
.	O
hint	O
:	O
treat	O
the	O
θj	O
as	O
hidden	B
variables	I
,	O
and	O
then	O
to	O
integrate	O
them	O
out	O
in	O
the	O
e	O
step	O
,	O
and	O
maximize	O
η	O
=	O
(	O
μ	O
,	O
τ	O
2	O
)	O
in	O
the	O
m	O
step	O
.	O
exercise	O
11.14	O
em	O
for	O
censored	O
linear	O
regression	B
censored	O
regression	B
refers	O
to	O
the	O
case	O
where	O
one	O
knows	O
the	O
outcome	O
is	O
at	O
least	O
(	O
or	O
at	O
most	O
)	O
a	O
certain	O
value	O
,	O
but	O
the	O
precise	O
value	O
is	O
unknown	B
.	O
this	O
arises	O
in	O
many	O
different	O
settings	O
.	O
for	O
example	O
,	O
suppose	O
one	O
is	O
trying	O
to	O
learn	O
a	O
model	O
that	O
can	O
predict	O
how	O
long	O
a	O
program	O
will	O
take	O
to	O
run	O
,	O
for	O
different	O
settings	O
of	O
its	O
parameters	O
.	O
one	O
may	O
abort	O
certain	O
runs	O
if	O
they	O
seem	O
to	O
be	O
taking	O
too	O
long	O
;	O
the	O
resulting	O
run	O
times	O
are	O
said	O
to	O
be	O
right	B
censored	I
.	O
for	O
such	O
runs	O
,	O
all	O
we	O
know	O
is	O
that	O
yi	O
≥	O
ci	O
,	O
where	O
ci	O
is	O
the	O
censoring	O
time	O
,	O
that	O
is	O
,	O
yi	O
=	O
min	O
(	O
zi	O
,	O
ci	O
)	O
,	O
where	O
zi	O
is	O
the	O
true	O
running	O
time	O
and	O
yi	O
is	O
the	O
observed	O
running	O
time	O
.	O
we	O
can	O
also	O
deﬁne	O
left	B
censored	I
and	O
interval	B
censored	I
models.3	O
derive	O
an	O
em	O
algorithm	O
for	O
ﬁtting	O
a	O
linear	B
regression	I
model	O
to	O
right-censored	O
data	O
.	O
hint	O
:	O
use	O
the	O
results	O
from	O
exercise	O
11.15.	O
see	O
figure	O
11.25	O
for	O
an	O
example	O
,	O
based	O
on	O
the	O
data	O
from	O
(	O
schmee	O
and	O
hahn	O
1979	O
)	O
.	O
we	O
notice	O
that	O
the	O
em	O
line	O
is	O
tilted	O
upwards	O
more	O
,	O
since	O
the	O
model	O
takes	O
into	O
account	O
the	O
fact	O
that	O
the	O
truncated	O
values	O
are	O
actually	O
higher	O
than	O
the	O
observed	O
values	O
.	O
3.	O
there	O
is	O
a	O
closely	O
related	O
model	O
in	O
econometrics	O
called	O
the	O
tobit	O
model	O
,	O
in	O
which	O
yi	O
=	O
max	O
(	O
zi	O
,	O
0	O
)	O
,	O
so	O
we	O
only	O
get	O
to	O
observe	O
positive	O
outcomes	O
.	O
an	O
example	O
of	O
this	O
is	O
when	O
zi	O
represents	O
“	O
desired	O
investment	O
”	O
,	O
and	O
yi	O
is	O
actual	O
investment	O
.	O
probit	B
regression	I
(	O
section	O
9.4	O
)	O
is	O
another	O
example	O
.	O
380	O
chapter	O
11.	O
mixture	B
models	O
and	O
the	O
em	O
algorithm	O
exercise	O
11.15	O
posterior	B
mean	I
and	O
variance	B
of	O
a	O
truncated	O
gaussian	O
let	O
zi	O
=	O
μi	O
+	O
σi	O
,	O
where	O
i	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
.	O
sometimes	O
,	O
such	O
as	O
in	O
probit	B
regression	I
or	O
censored	B
regression	I
,	O
we	O
do	O
not	O
observe	O
zi	O
,	O
but	O
we	O
observe	O
the	O
fact	O
that	O
it	O
is	O
above	O
some	O
threshold	O
,	O
namely	O
we	O
observe	O
the	O
event	O
e	O
=	O
i	O
(	O
zi	O
≥	O
ci	O
)	O
=i	O
(	O
i	O
≥	O
ci−μi	O
)	O
.	O
(	O
see	O
exercise	O
11.14	O
for	O
details	O
on	O
censored	B
regression	I
,	O
and	O
section	O
11.4.6	O
for	O
probit	B
regression	I
.	O
)	O
show	O
that	O
e	O
[	O
zi|zi	O
≥	O
ci	O
]	O
=μ	O
i	O
+	O
σh	O
(	O
cid:22	O
)	O
(	O
cid:16	O
)	O
ci	O
−	O
μi	O
(	O
11.137	O
)	O
(	O
cid:17	O
)	O
and	O
(	O
cid:16	O
)	O
ci	O
−	O
μi	O
(	O
cid:17	O
)	O
(	O
cid:23	O
)	O
σ	O
σ	O
i	O
|zi	O
≥	O
ci	O
z2	O
e	O
=	O
μ2	O
i	O
+	O
σ2	O
+	O
σ	O
(	O
ci	O
+	O
μi	O
)	O
h	O
σ	O
where	O
we	O
have	O
deﬁned	O
h	O
(	O
u	O
)	O
(	O
cid:2	O
)	O
φ	O
(	O
u	O
)	O
1	O
−	O
φ	O
(	O
u	O
)	O
and	O
where	O
φ	O
(	O
u	O
)	O
is	O
the	O
pdf	B
of	O
a	O
standard	O
gaussian	O
,	O
and	O
φ	O
(	O
u	O
)	O
is	O
its	O
cdf	B
.	O
hint	O
1	O
:	O
we	O
have	O
p	O
(	O
i|e	O
)	O
=	O
p	O
(	O
i	O
,	O
e	O
)	O
p	O
(	O
e	O
)	O
,	O
wheree	O
is	O
some	O
event	O
of	O
interest	O
.	O
hint	O
2	O
:	O
it	O
can	O
be	O
shown	O
that	O
n	O
(	O
w|0	O
,	O
1	O
)	O
=	O
−wn	O
(	O
w|0	O
,	O
1	O
)	O
and	O
hence	O
d	O
dw	O
(	O
cid:5	O
)	O
c	O
b	O
wn	O
(	O
w|0	O
,	O
1	O
)	O
=	O
n	O
(	O
b|0	O
,	O
1	O
)	O
−	O
n	O
(	O
c|0	O
,	O
1	O
)	O
(	O
11.138	O
)	O
(	O
11.139	O
)	O
(	O
11.140	O
)	O
(	O
11.141	O
)	O
12	O
latent	B
linear	O
models	O
12.1	O
factor	B
analysis	I
one	O
problem	O
with	O
mixture	B
models	O
is	O
that	O
they	O
only	O
use	O
a	O
single	O
latent	O
variable	O
to	O
generate	O
the	O
observations	O
.	O
in	O
particular	O
,	O
each	O
observation	B
can	O
only	O
come	O
from	O
one	O
of	O
k	O
prototypes	O
.	O
one	O
can	O
think	O
of	O
a	O
mixture	B
model	I
as	O
using	O
k	O
hidden	B
binary	O
variables	O
,	O
representing	O
a	O
one-hot	B
encoding	I
of	O
the	O
cluster	O
identity	O
.	O
but	O
because	O
these	O
variables	O
are	O
mutually	O
exclusive	O
,	O
the	O
model	O
is	O
still	O
limited	O
in	O
its	O
representational	O
power	O
.	O
to	O
use	O
is	O
a	O
gaussian	O
(	O
we	O
will	O
consider	O
other	O
choices	O
later	O
)	O
:	O
an	O
alternative	O
is	O
to	O
use	O
a	O
vector	O
of	O
real-valued	O
latent	B
variables	O
,	O
zi	O
∈	O
r	O
p	O
(	O
zi	O
)	O
=	O
n	O
(	O
zi|μ0	O
,	O
σ0	O
)	O
(	O
12.1	O
)	O
if	O
the	O
observations	O
are	O
also	O
continuous	O
,	O
so	O
xi	O
∈	O
r	O
d	O
,	O
we	O
may	O
use	O
a	O
gaussian	O
for	O
the	O
likelihood	B
.	O
just	O
as	O
in	O
linear	B
regression	I
,	O
we	O
will	O
assume	O
the	O
mean	B
is	O
a	O
linear	O
function	O
of	O
the	O
(	O
hidden	B
)	O
inputs	O
,	O
thus	O
yielding	O
l.	O
the	O
simplest	O
prior	O
p	O
(	O
xi|zi	O
,	O
θ	O
)	O
=	O
n	O
(	O
wzi	O
+	O
μ	O
,	O
ψ	O
)	O
(	O
12.2	O
)	O
where	O
w	O
is	O
a	O
d×l	O
matrix	O
,	O
known	O
as	O
the	O
factor	B
loading	I
matrix	I
,	O
and	O
ψ	O
is	O
a	O
d×d	O
covariance	B
matrix	I
.	O
we	O
take	O
ψ	O
to	O
be	O
diagonal	B
,	O
since	O
the	O
whole	O
point	O
of	O
the	O
model	O
is	O
to	O
“	O
force	O
”	O
zi	O
to	O
explain	O
the	O
correlation	O
,	O
rather	O
than	O
“	O
baking	O
it	O
in	O
”	O
to	O
the	O
observation	B
’	O
s	O
covariance	B
.	O
this	O
overall	O
model	O
is	O
called	O
factor	B
analysis	I
or	O
fa	O
.	O
the	O
special	O
case	O
in	O
which	O
ψ	O
=	O
σ2i	O
is	O
called	O
probabilistic	B
principal	I
components	I
analysis	I
or	O
ppca	O
.	O
the	O
reason	O
for	O
this	O
name	O
will	O
become	O
apparent	O
later	O
.	O
the	O
generative	O
process	O
,	O
where	O
l	O
=	O
1	O
,	O
d	O
=	O
2	O
and	O
ψ	O
is	O
diagonal	B
,	O
is	O
illustrated	O
in	O
figure	O
12.1.	O
we	O
take	O
an	O
isotropic	B
gaussian	O
“	O
spray	O
can	O
”	O
and	O
slide	O
it	O
along	O
the	O
1d	O
line	O
deﬁned	O
by	O
wzi	O
+	O
μ.	O
this	O
induces	O
an	O
ellongated	O
(	O
and	O
hence	O
correlated	O
)	O
gaussian	O
in	O
2d	O
.	O
12.1.1	O
fa	O
is	O
a	O
low	O
rank	O
parameterization	O
of	O
an	O
mvn	O
fa	O
can	O
be	O
thought	O
of	O
as	O
a	O
way	O
of	O
specifying	O
a	O
joint	O
density	O
model	O
on	O
x	O
using	O
a	O
small	O
number	O
of	O
parameters	O
.	O
to	O
see	O
this	O
,	O
note	O
that	O
from	O
equation	O
4.126	O
,	O
the	O
induced	O
marginal	O
distribution	O
p	O
(	O
xi|θ	O
)	O
is	O
a	O
gaussian	O
:	O
(	O
cid:28	O
)	O
p	O
(	O
xi|θ	O
)	O
=	O
n	O
(	O
xi|wzi	O
+	O
μ	O
,	O
ψ	O
)	O
n	O
(	O
zi|μ0	O
,	O
σ0	O
)	O
dzi	O
=	O
n	O
(	O
xi|wμ0	O
+	O
μ	O
,	O
ψ	O
+	O
wς0wt	O
)	O
(	O
12.3	O
)	O
(	O
12.4	O
)	O
382	O
chapter	O
12.	O
latent	B
linear	O
models	O
(	O
cid:91	O
)	O
(	O
cid:21	O
)	O
(	O
cid:90	O
)	O
(	O
cid:91	O
)	O
(	O
cid:21	O
)	O
(	O
cid:83	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:95	O
)	O
(	O
cid:93	O
)	O
(	O
cid:12	O
)	O
(	O
cid:541	O
)	O
(	O
cid:93	O
)	O
(	O
cid:3	O
)	O
(	O
cid:95	O
)	O
(	O
cid:90	O
)	O
(	O
cid:95	O
)	O
(	O
cid:541	O
)	O
(	O
cid:83	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:12	O
)	O
(	O
cid:91	O
)	O
(	O
cid:20	O
)	O
(	O
cid:91	O
)	O
(	O
cid:20	O
)	O
(	O
cid:83	O
)	O
(	O
cid:11	O
)	O
(	O
cid:93	O
)	O
(	O
cid:12	O
)	O
(	O
cid:83	O
)	O
(	O
cid:11	O
)	O
(	O
cid:93	O
)	O
(	O
cid:12	O
)	O
(	O
cid:93	O
)	O
(	O
cid:3	O
)	O
figure	O
12.1	O
d	O
=	O
2	O
observed	O
dimensions	O
.	O
based	O
on	O
figure	O
12.9	O
of	O
(	O
bishop	O
2006b	O
)	O
.	O
illustration	O
of	O
the	O
ppca	O
generative	O
process	O
,	O
where	O
we	O
have	O
l	O
=	O
1	O
latent	B
dimension	O
generating	O
from	O
this	O
,	O
we	O
see	O
that	O
we	O
can	O
set	O
μ0	O
=	O
0	O
without	O
loss	B
of	O
generality	O
,	O
since	O
we	O
can	O
always	O
absorb	O
wμ0	O
into	O
μ.	O
similarly	O
,	O
we	O
can	O
set	O
σ0	O
=	O
i	O
without	O
loss	B
of	O
generality	O
,	O
because	O
we	O
can	O
always	O
−	O
1	O
“	O
emulate	O
”	O
a	O
correlated	O
prior	O
by	O
using	O
deﬁning	O
a	O
new	O
weight	O
matrix	O
,	O
˜w	O
=	O
wς	O
.	O
then	O
we	O
0	O
ﬁnd	O
cov	O
[	O
x|θ	O
]	O
=	O
˜wt	O
+	O
e	O
we	O
thus	O
see	O
that	O
fa	O
approximates	O
the	O
covariance	B
matrix	I
of	O
the	O
visible	B
vector	O
using	O
a	O
low-rank	O
)	O
t	O
+	O
ψ	O
=	O
wwt	O
+	O
ψ	O
)	O
σ0	O
(	O
wς	O
=	O
(	O
wς	O
−	O
1	O
0	O
−	O
1	O
0	O
2	O
(	O
12.5	O
)	O
(	O
cid:31	O
)	O
t	O
2	O
2	O
decomposition	O
:	O
c	O
(	O
cid:2	O
)	O
cov	O
[	O
x	O
]	O
=	O
wwt	O
+	O
ψ	O
(	O
12.6	O
)	O
this	O
only	O
uses	O
o	O
(	O
ld	O
)	O
parameters	O
,	O
which	O
allows	O
a	O
ﬂexible	O
compromise	O
between	O
a	O
full	B
covariance	O
gaussian	O
,	O
with	O
o	O
(	O
d2	O
)	O
parameters	O
,	O
and	O
a	O
diagonal	O
covariance	O
,	O
with	O
o	O
(	O
d	O
)	O
parameters	O
.	O
note	O
that	O
if	O
we	O
did	O
not	O
restrict	O
ψ	O
to	O
be	O
diagonal	B
,	O
we	O
could	O
trivially	O
set	O
ψ	O
to	O
a	O
full	B
covariance	O
matrix	O
;	O
then	O
we	O
could	O
set	O
w	O
=	O
0	O
,	O
in	O
which	O
case	O
the	O
latent	B
factors	I
would	O
not	O
be	O
required	O
.	O
12.1.2	O
inference	B
of	O
the	O
latent	B
factors	I
although	O
fa	O
can	O
be	O
thought	O
of	O
as	O
just	O
a	O
way	O
to	O
deﬁne	O
a	O
density	O
on	O
x	O
,	O
it	O
is	O
often	O
used	O
because	O
we	O
hope	O
that	O
the	O
latent	B
factors	I
z	O
will	O
reveal	O
something	O
interesting	O
about	O
the	O
data	O
.	O
to	O
do	O
this	O
,	O
we	O
need	O
to	O
compute	O
the	O
posterior	O
over	O
the	O
latent	B
factors	I
.	O
we	O
can	O
use	O
bayes	O
rule	O
for	O
gaussians	O
to	O
give	O
p	O
(	O
zi|xi	O
,	O
θ	O
)	O
=	O
n	O
(	O
zi|mi	O
,	O
σi	O
)	O
0	O
+	O
wt	O
ψ−1w	O
)	O
σi	O
(	O
cid:2	O
)	O
(	O
σ−1	O
mi	O
(	O
cid:2	O
)	O
σi	O
(	O
wt	O
ψ−1	O
(	O
xi	O
−	O
μ	O
)	O
+	O
σ−1	O
(	O
12.7	O
)	O
(	O
12.8	O
)	O
(	O
12.9	O
)	O
note	O
that	O
in	O
the	O
fa	O
model	O
,	O
σi	O
is	O
actually	O
independent	O
of	O
i	O
,	O
so	O
we	O
can	O
denote	O
it	O
by	O
σ.	O
computing	O
this	O
matrix	O
takes	O
o	O
(	O
l3	O
+	O
l2d	O
)	O
time	O
,	O
and	O
computing	O
each	O
mi	O
=	O
e	O
[	O
zi|xi	O
,	O
θ	O
]	O
takes	O
o	O
(	O
l2	O
+	O
ld	O
)	O
time	O
.	O
the	O
mi	O
are	O
sometimes	O
called	O
the	O
latent	B
scores	O
,	O
or	O
latent	B
factors	I
.	O
0	O
μ0	O
)	O
−1	O
12.1.	O
factor	B
analysis	I
383	O
rotation=none	O
2	O
t	O
n	O
e	O
n	O
o	O
p	O
m	O
o	O
c	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−0.6	O
−0.8	O
−1	O
width	O
wheelbase	O
length	O
weight	O
engine	O
cylinders	O
gmc	O
yukon	O
xl	O
2500	O
slt	O
nissan	O
pathfinder	O
armada	O
se	O
horsepower	O
kia	O
sorento	O
lx	O
mercedes−benz	O
g500	O
saturn	O
ion1	O
honda	O
insight	O
citympg	O
highwaympg	O
retail	O
dealer	O
mercedes−benz	O
cl600	O
porsche	O
911	O
−1	O
−0.5	O
0	O
component	O
1	O
0.5	O
1	O
figure	O
12.2	O
2d	O
projection	B
of	O
2004	O
cars	O
data	O
based	O
on	O
factor	B
analysis	I
.	O
the	O
blue	O
text	O
are	O
the	O
names	O
of	O
cars	O
corresponding	O
to	O
certain	O
chosen	O
points	O
.	O
figure	O
generated	O
by	O
fabiplotdemo	O
.	O
let	O
us	O
give	O
a	O
simple	O
example	O
,	O
based	O
(	O
shalizi	O
2009	O
)	O
.	O
we	O
consider	O
a	O
dataset	O
of	O
d	O
=	O
11	O
variables	O
and	O
n	O
=	O
387	O
cases	O
describing	O
various	O
aspects	O
of	O
cars	O
,	O
such	O
as	O
the	O
engine	O
size	O
,	O
the	O
number	O
of	O
cylinders	O
,	O
the	O
miles	O
per	O
gallon	O
(	O
mpg	O
)	O
,	O
the	O
price	O
,	O
etc	O
.	O
we	O
ﬁrst	O
ﬁt	O
a	O
l	O
=	O
2	O
dimensional	O
model	O
.	O
we	O
can	O
plot	O
the	O
mi	O
scores	B
as	O
points	O
in	O
r	O
2	O
,	O
to	O
visualize	O
the	O
data	O
,	O
as	O
shown	O
in	O
figure	O
12.2.	O
to	O
get	O
a	O
better	O
understanding	O
of	O
the	O
“	O
meaning	O
”	O
of	O
the	O
latent	B
factors	I
,	O
we	O
can	O
project	O
unit	O
vectors	O
corresponding	O
to	O
each	O
of	O
the	O
feature	O
dimensions	O
,	O
e1	O
=	O
(	O
1	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
,	O
e2	O
=	O
(	O
0	O
,	O
1	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
,	O
etc	O
.	O
into	O
the	O
low	O
dimensional	O
space	O
.	O
these	O
are	O
shown	O
as	O
blue	O
lines	O
in	O
figure	O
12.2	O
;	O
this	O
is	O
known	O
as	O
a	O
biplot	B
.	O
we	O
see	O
that	O
the	O
horizontal	O
axis	O
represents	O
price	O
,	O
corresponding	O
to	O
the	O
features	B
labeled	O
“	O
dealer	O
”	O
and	O
“	O
retail	O
”	O
,	O
with	O
expensive	O
cars	O
on	O
the	O
right	O
.	O
the	O
vertical	O
axis	O
represents	O
fuel	O
efficiency	O
(	O
measured	O
in	O
terms	O
of	O
mpg	O
)	O
versus	O
size	O
:	O
heavy	O
vehicles	O
are	O
less	O
efficient	O
and	O
are	O
higher	O
up	O
,	O
whereas	O
light	O
vehicles	O
are	O
more	O
efficient	O
and	O
are	O
lower	O
down	O
.	O
we	O
can	O
“	O
verify	O
”	O
this	O
interpretation	O
by	O
clicking	O
on	O
some	O
points	O
,	O
and	O
ﬁnding	O
the	O
closest	O
exemplars	O
in	O
the	O
training	B
set	I
,	O
and	O
printing	O
their	O
names	O
,	O
as	O
in	O
figure	O
12.2.	O
however	O
,	O
in	O
general	O
,	O
interpreting	O
latent	B
variable	I
models	I
is	O
fraught	O
with	O
difficulties	O
,	O
as	O
we	O
discuss	O
in	O
section	O
12.1.3	O
.	O
12.1.3	O
unidentiﬁability	O
just	O
like	O
with	O
mixture	B
models	O
,	O
fa	O
is	O
also	O
unidentiﬁable	B
.	O
to	O
see	O
this	O
,	O
suppose	O
r	O
is	O
an	O
arbitrary	O
orthogonal	O
rotation	O
matrix	O
,	O
satisfying	O
rrt	O
=	O
i.	O
let	O
us	O
deﬁne	O
˜w	O
=	O
wr	O
;	O
then	O
the	O
likelihood	B
384	O
chapter	O
12.	O
latent	B
linear	O
models	O
function	O
of	O
this	O
modiﬁed	O
matrix	O
is	O
the	O
same	O
as	O
for	O
the	O
unmodiﬁed	O
matrix	O
,	O
since	O
(	O
cid:31	O
)	O
(	O
cid:31	O
)	O
cov	O
[	O
x	O
]	O
=	O
˜we	O
zzt	O
˜wt	O
+	O
e	O
t	O
=	O
wrrt	O
wt	O
+	O
ψ	O
=	O
wwt	O
+	O
ψ	O
(	O
12.10	O
)	O
(	O
12.11	O
)	O
geometrically	O
,	O
multiplying	O
w	O
by	O
an	O
orthogonal	O
matrix	O
is	O
like	O
rotating	O
z	O
before	O
generating	O
x	O
;	O
but	O
since	O
z	O
is	O
drawn	O
from	O
an	O
isotropic	B
gaussian	O
,	O
this	O
makes	O
no	O
difference	O
to	O
the	O
likelihood	B
.	O
consequently	O
,	O
we	O
can	O
not	O
unique	O
identify	O
w	O
,	O
and	O
therefore	O
can	O
not	O
uniquely	O
identify	O
the	O
latent	B
factors	I
,	O
either	O
.	O
to	O
ensure	O
a	O
unique	O
solution	O
,	O
we	O
need	O
to	O
remove	O
l	O
(	O
l	O
−	O
1	O
)	O
/2	O
degrees	B
of	I
freedom	I
,	O
since	O
that	O
in	O
total	O
,	O
the	O
fa	O
model	O
has	O
d	O
+	O
ld	O
−	O
is	O
the	O
number	O
of	O
orthonormal	O
matrices	O
of	O
size	O
l	O
×	O
l.1	O
l	O
(	O
l−	O
1	O
)	O
/2	O
free	O
parameters	O
(	O
excluding	O
the	O
mean	B
)	O
,	O
where	O
the	O
ﬁrst	O
term	O
arises	O
from	O
ψ.	O
obviously	O
we	O
require	O
this	O
to	O
be	O
less	O
than	O
or	O
equal	O
to	O
d	O
(	O
d	O
+	O
1	O
)	O
/2	O
,	O
which	O
is	O
the	O
number	O
of	O
parameters	O
in	O
an	O
unconstrained	O
(	O
but	O
symmetric	B
)	O
covariance	B
matrix	I
.	O
this	O
gives	O
us	O
an	O
upper	O
bound	O
on	O
l	O
,	O
as	O
follows	O
:	O
lmax	O
=	O
(	O
cid:13	O
)	O
d	O
+	O
0.5	O
(	O
1	O
−	O
√	O
(	O
12.12	O
)	O
for	O
example	O
,	O
d	O
=	O
6	O
implies	O
l	O
≤	O
3.	O
but	O
we	O
usually	O
never	O
choose	O
this	O
upper	O
bound	O
,	O
since	O
it	O
would	O
result	O
in	O
overﬁtting	B
(	O
see	O
discussion	O
in	O
section	O
12.3	O
on	O
how	O
to	O
choose	O
l	O
)	O
.	O
1	O
+	O
8d	O
)	O
(	O
cid:15	O
)	O
unfortunately	O
,	O
even	O
if	O
we	O
set	O
l	O
<	O
lmax	O
,	O
we	O
still	O
can	O
not	O
uniquely	O
identify	O
the	O
parameters	O
,	O
since	O
the	O
rotational	O
ambiguity	O
still	O
exists	O
.	O
non-identiﬁability	O
does	O
not	O
affect	O
the	O
predictive	B
per-	O
formance	O
of	O
the	O
model	O
.	O
however	O
,	O
it	O
does	O
affect	O
the	O
loading	O
matrix	O
,	O
and	O
hence	O
the	O
interpretation	O
of	O
the	O
latent	B
factors	I
.	O
since	O
factor	B
analysis	I
is	O
often	O
used	O
to	O
uncover	O
structure	O
in	O
the	O
data	O
,	O
this	O
problem	O
needs	O
to	O
be	O
addressed	O
.	O
here	O
are	O
some	O
commonly	O
used	O
solutions	O
:	O
•	O
forcing	O
w	O
to	O
be	O
orthonormal	O
perhaps	O
the	O
cleanest	O
solution	O
to	O
the	O
identiﬁability	O
problem	O
is	O
to	O
force	O
w	O
to	O
be	O
orthonormal	O
,	O
and	O
to	O
order	O
the	O
columns	O
by	O
decreasing	O
variance	B
of	O
the	O
corresponding	O
latent	B
factors	I
.	O
this	O
is	O
the	O
approach	O
adopted	O
by	O
pca	O
,	O
which	O
we	O
will	O
discuss	O
in	O
section	O
12.2.	O
the	O
result	O
is	O
not	O
necessarily	O
more	O
interpretable	O
,	O
but	O
at	O
least	O
it	O
is	O
unique	O
.	O
•	O
forcing	O
w	O
to	O
be	O
lower	O
triangular	O
one	O
way	O
to	O
achieve	O
identiﬁability	O
,	O
which	O
is	O
popular	O
in	O
the	O
bayesian	O
community	O
(	O
e.g.	O
,	O
(	O
lopes	O
and	O
west	O
2004	O
)	O
)	O
,	O
is	O
to	O
ensure	O
that	O
the	O
ﬁrst	O
visible	B
feature	O
is	O
only	O
generated	O
by	O
the	O
ﬁrst	O
latent	B
factor	O
,	O
the	O
second	O
visible	O
feature	O
is	O
only	O
generated	O
by	O
the	O
ﬁrst	O
two	O
latent	B
factors	I
,	O
and	O
so	O
on	O
.	O
for	O
example	O
,	O
if	O
l	O
=	O
3	O
and	O
d	O
=	O
4	O
,	O
the	O
correspond	O
factor	B
loading	I
matrix	I
is	O
given	O
by	O
⎛	O
⎜⎜⎝	O
w	O
=	O
⎞	O
⎟⎟⎠	O
w11	O
0	O
0	O
w21	O
w22	O
0	O
w31	O
w32	O
w33	O
w41	O
w42	O
w43	O
(	O
12.13	O
)	O
we	O
also	O
require	O
that	O
wjj	O
>	O
0	O
for	O
j	O
=	O
1	O
:	O
l.	O
the	O
total	O
number	O
of	O
parameters	O
in	O
this	O
constrained	O
matrix	O
is	O
d	O
+	O
dl	O
−	O
l	O
(	O
l	O
−	O
1	O
)	O
/2	O
,	O
which	O
is	O
equal	O
to	O
the	O
number	O
of	O
uniquely	O
identiﬁable	B
parameters	O
.	O
the	O
disadvantage	O
of	O
this	O
method	O
is	O
that	O
the	O
ﬁrst	O
l	O
visible	B
variables	I
,	O
1.	O
to	O
see	O
this	O
,	O
note	O
that	O
there	O
are	O
l	O
−	O
1	O
free	O
parameters	O
in	O
r	O
in	O
the	O
ﬁrst	O
column	O
(	O
since	O
the	O
column	O
vector	O
must	O
be	O
normalized	O
to	O
unit	O
length	O
)	O
,	O
there	O
are	O
l	O
−	O
2	O
free	O
parameters	O
in	O
the	O
second	O
column	O
(	O
which	O
must	O
be	O
orthogonal	O
to	O
the	O
ﬁrst	O
)	O
,	O
and	O
so	O
on	O
.	O
12.1.	O
factor	B
analysis	I
385	O
π	O
qi	O
zi	O
ψ	O
xi	O
n	O
μk	O
w	O
k	O
k	O
figure	O
12.3	O
mixture	B
of	I
factor	I
analysers	I
as	O
a	O
dgm	O
.	O
known	O
as	O
the	O
founder	B
variables	I
,	O
affect	O
the	O
interpretation	O
of	O
the	O
latent	B
factors	I
,	O
and	O
so	O
must	O
be	O
chosen	O
carefully	O
.	O
•	O
sparsity	B
promoting	O
priors	O
on	O
the	O
weights	O
instead	O
of	O
pre-specifying	O
which	O
entries	O
in	O
w	O
are	O
zero	O
,	O
we	O
can	O
encourage	O
the	O
entries	O
to	O
be	O
zero	O
,	O
using	O
(	O
cid:6	O
)	O
1	O
regularization	B
(	O
zou	O
et	O
al	O
.	O
2006	O
)	O
,	O
ard	O
(	O
bishop	O
1999	O
;	O
archambeau	O
and	O
bach	O
2008	O
)	O
,	O
or	O
spike-and-slab	O
priors	O
(	O
rattray	O
et	O
al	O
.	O
2009	O
)	O
.	O
this	O
is	O
called	O
sparse	B
factor	O
analysis	O
.	O
this	O
does	O
not	O
necessarily	O
ensure	O
a	O
unique	O
map	O
estimate	O
,	O
but	O
it	O
does	O
encourage	O
interpretable	O
solutions	O
.	O
see	O
section	O
13.8	O
.	O
•	O
choosing	O
an	O
informative	O
rotation	O
matrix	O
there	O
are	O
a	O
variety	O
of	O
heuristic	O
methods	O
that	O
try	O
to	O
ﬁnd	O
rotation	O
matrices	O
r	O
which	O
can	O
be	O
used	O
to	O
modify	O
w	O
(	O
and	O
hence	O
the	O
latent	B
factors	I
)	O
so	O
as	O
to	O
try	O
to	O
increase	O
the	O
interpretability	O
,	O
typically	O
by	O
encouraging	O
them	O
to	O
be	O
(	O
approximately	O
)	O
sparse	B
.	O
one	O
popular	O
method	O
is	O
known	O
as	O
varimax	B
(	O
kaiser	O
1958	O
)	O
.	O
•	O
use	O
of	O
non-gaussian	O
priors	O
for	O
the	O
latent	B
factors	I
in	O
section	O
12.6	O
,	O
we	O
will	O
dicuss	O
how	O
re-	O
placing	O
p	O
(	O
zi	O
)	O
with	O
a	O
non-gaussian	O
distribution	O
can	O
enable	O
us	O
to	O
sometimes	O
uniquely	O
identify	O
w	O
as	O
well	O
as	O
the	O
latent	B
factors	I
.	O
this	O
technique	O
is	O
known	O
as	O
ica	O
.	O
12.1.4	O
mixtures	O
of	O
factor	B
analysers	O
the	O
fa	O
model	O
assumes	O
that	O
the	O
data	O
lives	O
on	O
a	O
low	O
dimensional	O
linear	O
manifold	O
.	O
in	O
reality	O
,	O
most	O
data	O
is	O
better	O
modeled	O
by	O
some	O
form	O
of	O
low	O
dimensional	O
curved	O
manifold	O
.	O
we	O
can	O
approximate	O
a	O
curved	O
manifold	O
by	O
a	O
piecewise	O
linear	O
manifold	O
.	O
this	O
suggests	O
the	O
following	O
model	O
:	O
let	O
the	O
k	O
’	O
th	O
linear	O
subspace	O
of	O
dimensionality	O
lk	O
be	O
represented	O
by	O
wk	O
,	O
for	O
k	O
=	O
1	O
:	O
k	O
.	O
suppose	O
we	O
have	O
a	O
latent	B
indicator	O
qi	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
specifying	O
which	O
subspace	O
we	O
should	O
use	O
to	O
generate	O
the	O
data	O
.	O
we	O
then	O
sample	O
zi	O
from	O
a	O
gaussian	O
prior	O
and	O
pass	O
it	O
through	O
the	O
wk	O
matrix	O
(	O
where	O
k	O
=	O
qi	O
)	O
,	O
and	O
add	O
noise	O
.	O
more	O
precisely	O
,	O
the	O
model	O
is	O
as	O
follows	O
:	O
p	O
(	O
xi|zi	O
,	O
qi	O
=	O
k	O
,	O
θ	O
)	O
=n	O
(	O
xi|μk	O
+	O
wkzi	O
,	O
ψ	O
)	O
p	O
(	O
zi|θ	O
)	O
=n	O
(	O
zi|0	O
,	O
i	O
)	O
p	O
(	O
qi|θ	O
)	O
=	O
cat	O
(	O
qi|π	O
)	O
(	O
12.14	O
)	O
(	O
12.15	O
)	O
(	O
12.16	O
)	O
386	O
chapter	O
12.	O
latent	B
linear	O
models	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−2	O
−1.5	O
−1	O
−0.5	O
0	O
0.5	O
1	O
1.5	O
2	O
−2	O
−1.5	O
−1	O
−0.5	O
0	O
0.5	O
1	O
1.5	O
2	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
12.4	O
mixture	O
of	O
mixppcademonetlab	O
.	O
1d	O
ppcas	O
ﬁt	O
to	O
a	O
dataset	O
,	O
for	O
k	O
=	O
1	O
,	O
10.	O
figure	O
generated	O
by	O
this	O
is	O
called	O
a	O
mixture	B
of	I
factor	I
analysers	I
(	O
mfa	O
)	O
(	O
hinton	O
et	O
al	O
.	O
1997	O
)	O
.	O
the	O
ci	O
assumptions	O
are	O
represented	O
in	O
figure	O
12.3.	O
another	O
way	O
to	O
think	O
about	O
this	O
model	O
is	O
as	O
a	O
low-rank	O
version	O
of	O
a	O
mixture	O
of	O
gaussians	O
.	O
in	O
particular	O
,	O
this	O
model	O
needs	O
o	O
(	O
kld	O
)	O
parameters	O
instead	O
of	O
the	O
o	O
(	O
kd2	O
)	O
parameters	O
needed	O
for	O
a	O
mixture	O
of	O
full	O
covariance	B
gaussians	O
.	O
this	O
can	O
reduce	O
overﬁtting	B
.	O
in	O
fact	O
,	O
mfa	O
is	O
a	O
good	O
generic	O
density	O
model	O
for	O
high-dimensional	O
real-valued	O
data	O
.	O
12.1.5	O
em	O
for	O
factor	B
analysis	I
models	O
using	O
the	O
results	O
from	O
chapter	O
4	O
,	O
it	O
is	O
straightforward	O
to	O
derive	O
an	O
em	O
algorithm	O
to	O
ﬁt	O
an	O
fa	O
model	O
.	O
with	O
just	O
a	O
little	O
more	O
work	O
,	O
we	O
can	O
ﬁt	O
a	O
mixture	O
of	O
fas	O
.	O
below	O
we	O
state	B
the	O
results	O
without	O
proof	O
.	O
the	O
derivation	O
can	O
be	O
found	O
in	O
(	O
ghahramani	O
and	O
hinton	O
1996a	O
)	O
;	O
however	O
,	O
deriving	O
these	O
equations	O
yourself	O
is	O
a	O
useful	O
exercise	O
if	O
you	O
want	O
to	O
become	O
proﬁcient	O
at	O
the	O
math	O
.	O
to	O
obtain	O
the	O
results	O
for	O
a	O
single	O
factor	O
analyser	O
,	O
just	O
set	O
ric	O
=	O
1	O
and	O
c	O
=	O
1	O
in	O
the	O
equations	O
below	O
.	O
in	O
section	O
12.2.5	O
we	O
will	O
see	O
a	O
further	O
simpliﬁcation	O
of	O
these	O
equations	O
that	O
arises	O
when	O
ﬁtting	O
a	O
ppca	O
model	O
,	O
where	O
the	O
results	O
will	O
turn	O
out	O
to	O
have	O
a	O
particularly	O
simple	O
and	O
elegant	O
intepretation	O
.	O
in	O
the	O
e	O
step	O
,	O
we	O
compute	O
the	O
posterior	O
responsibility	O
of	O
cluster	O
c	O
for	O
data	O
point	O
i	O
using	O
ric	O
(	O
cid:2	O
)	O
p	O
(	O
qi	O
=	O
c|xi	O
,	O
θ	O
)	O
∝	O
πcn	O
(	O
xi|μc	O
,	O
wcwt	O
c	O
+	O
ψ	O
)	O
(	O
12.17	O
)	O
the	O
conditional	O
posterior	O
for	O
zi	O
is	O
given	O
by	O
p	O
(	O
zi|xi	O
,	O
qi	O
=	O
c	O
,	O
θ	O
)	O
=n	O
(	O
zi|mic	O
,	O
σic	O
)	O
c	O
ψ−1	O
−1	O
c	O
wc	O
)	O
c	O
(	O
xi	O
−	O
μc	O
)	O
)	O
c	O
ψ−1	O
σic	O
(	O
cid:2	O
)	O
(	O
il	O
+	O
wt	O
mic	O
(	O
cid:2	O
)	O
σic	O
(	O
wt	O
(	O
12.18	O
)	O
(	O
12.19	O
)	O
(	O
12.20	O
)	O
in	O
the	O
m	O
step	O
,	O
it	O
is	O
easiest	O
to	O
estimate	O
μc	O
and	O
wc	O
at	O
the	O
same	O
time	O
,	O
by	O
deﬁning	O
˜wc	O
=	O
12.2.	O
principal	B
components	I
analysis	I
(	O
pca	O
)	O
(	O
wc	O
,	O
μc	O
)	O
,	O
˜z	O
=	O
(	O
z	O
,	O
1	O
)	O
,	O
also	O
,	O
deﬁne	O
bic	O
(	O
cid:2	O
)	O
e	O
[	O
˜z|xi	O
,	O
qi	O
=	O
c	O
]	O
=	O
[	O
mic	O
;	O
1	O
]	O
(	O
cid:31	O
)	O
(	O
cid:8	O
)	O
(	O
cid:31	O
)	O
zzt|xi	O
,	O
qi	O
=	O
c	O
˜z˜zt|xi	O
,	O
qi	O
=	O
c	O
cic	O
(	O
cid:2	O
)	O
e	O
e	O
[	O
z|xi	O
,	O
qi	O
=	O
c	O
]	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
(	O
cid:19	O
)	O
−1	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
(	O
cid:5	O
)	O
then	O
the	O
m	O
step	O
is	O
as	O
follows	O
:	O
ˆ˜wc	O
=	O
ricxibt	O
ic	O
-	O
(	O
cid:2	O
)	O
riccic	O
.	O
e	O
=	O
(	O
cid:6	O
)	O
t	O
i	O
xi	O
−	O
ˆ˜wcbic	O
ric	O
xt	O
i	O
i	O
1	O
n	O
ˆψ	O
=	O
ˆπc	O
=	O
1	O
n	O
diag	O
n	O
(	O
cid:2	O
)	O
ic	O
ric	O
i=1	O
(	O
cid:9	O
)	O
e	O
[	O
z|xi	O
,	O
qi	O
=	O
c	O
]	O
1	O
387	O
(	O
12.21	O
)	O
(	O
12.22	O
)	O
(	O
12.23	O
)	O
(	O
12.24	O
)	O
(	O
12.25	O
)	O
note	O
that	O
these	O
updates	O
are	O
for	O
“	O
vanilla	O
”	O
em	O
.	O
a	O
much	O
faster	O
version	O
of	O
this	O
algorithm	O
,	O
based	O
on	O
ecm	O
,	O
is	O
described	O
in	O
(	O
zhao	O
and	O
yu	O
2008	O
)	O
.	O
12.1.6	O
fitting	O
fa	O
models	O
with	O
missing	B
data	I
in	O
many	O
applications	O
,	O
such	O
as	O
collaborative	B
ﬁltering	I
,	O
we	O
have	O
missing	B
data	I
.	O
one	O
virtue	O
of	O
the	O
em	O
approach	O
to	O
ﬁtting	O
an	O
fa/ppca	O
model	O
is	O
that	O
it	O
is	O
easy	O
to	O
extend	O
to	O
this	O
case	O
.	O
however	O
,	O
overﬁtting	B
can	O
be	O
a	O
problem	O
if	O
there	O
is	O
a	O
lot	O
of	O
missing	B
data	I
.	O
consequently	O
it	O
is	O
important	O
to	O
perform	O
map	O
estimation	O
or	O
to	O
use	O
bayesian	O
inference	B
.	O
see	O
e.g.	O
,	O
(	O
ilin	O
and	O
raiko	O
2010	O
)	O
for	O
details	O
.	O
12.2	O
principal	B
components	I
analysis	I
(	O
pca	O
)	O
consider	O
the	O
fa	O
model	O
where	O
we	O
constrain	O
ψ	O
=	O
σ2i	O
,	O
and	O
w	O
to	O
be	O
orthonormal	O
.	O
it	O
can	O
be	O
shown	O
(	O
tipping	O
and	O
bishop	O
1999	O
)	O
that	O
,	O
as	O
σ2	O
→	O
0	O
,	O
this	O
model	O
reduces	O
to	O
classical	B
(	O
non-	O
probabilistic	O
)	O
principal	B
components	I
analysis	I
(	O
pca	O
)	O
,	O
also	O
known	O
as	O
the	O
karhunen	O
loeve	O
transform	O
.	O
the	O
version	O
where	O
σ2	O
>	O
0	O
is	O
known	O
as	O
probabilistic	O
pca	O
(	O
ppca	O
)	O
(	O
tipping	O
and	O
bishop	O
1999	O
)	O
,	O
or	O
sensible	O
pca	O
(	O
roweis	O
1997	O
)	O
.	O
(	O
an	O
equivalent	O
result	O
was	O
derived	O
independently	O
,	O
from	O
a	O
different	O
perspective	O
,	O
in	O
(	O
moghaddam	O
and	O
pentland	O
1995	O
)	O
.	O
)	O
to	O
make	O
sense	O
of	O
this	O
result	O
,	O
we	O
ﬁrst	O
have	O
to	O
learn	O
about	O
classical	B
pca	O
.	O
we	O
then	O
connect	O
pca	O
to	O
the	O
svd	O
.	O
and	O
ﬁnally	O
we	O
return	O
to	O
discuss	O
ppca	O
.	O
12.2.1	O
classical	B
pca	O
:	O
statement	O
of	O
the	O
theorem	O
the	O
synthesis	B
view	I
of	O
classical	B
pca	O
is	O
summarized	O
in	O
the	O
forllowing	O
theorem	O
.	O
theorem	O
12.2.1.	O
suppose	O
we	O
want	O
to	O
ﬁnd	O
an	O
orthogonal	O
set	O
of	O
l	O
linear	O
basis	O
vectors	O
wj	O
∈	O
r	O
d	O
,	O
and	O
the	O
corresponding	O
scores	B
zi	O
∈	O
r	O
l	O
,	O
such	O
that	O
we	O
minimize	O
the	O
average	O
reconstruction	O
error	O
||xi	O
−	O
ˆxi||2	O
j	O
(	O
w	O
,	O
z	O
)	O
=	O
n	O
(	O
cid:2	O
)	O
(	O
12.26	O
)	O
1	O
n	O
i=1	O
388	O
chapter	O
12.	O
latent	B
linear	O
models	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−5	O
5	O
0	O
(	O
a	O
)	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
−4	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
(	O
b	O
)	O
figure	O
12.5	O
an	O
illustration	O
of	O
pca	O
and	O
ppca	O
where	O
d	O
=	O
2	O
and	O
l	O
=	O
1.	O
circles	O
are	O
the	O
original	O
data	O
points	O
,	O
crosses	O
are	O
the	O
reconstructions	O
.	O
the	O
red	O
star	O
is	O
the	O
data	O
mean	O
.	O
(	O
a	O
)	O
pca	O
.	O
the	O
points	O
are	O
orthogonally	O
projected	O
onto	O
the	O
line	O
.	O
figure	O
generated	O
by	O
pcademo2d	O
.	O
(	O
b	O
)	O
ppca	O
.	O
the	O
projection	B
is	O
no	O
longer	O
orthogonal	O
:	O
the	O
reconstructions	O
are	O
shrunk	O
towards	O
the	O
data	O
mean	O
(	O
red	O
star	O
)	O
.	O
based	O
on	O
figure	O
7.6	O
of	O
(	O
nabney	O
2001	O
)	O
.	O
figure	O
generated	O
by	O
ppcademo2d	O
.	O
where	O
ˆxi	O
=	O
wzi	O
,	O
subject	O
to	O
the	O
constraint	O
that	O
w	O
is	O
orthonormal	O
.	O
equivalently	O
,	O
we	O
can	O
write	O
this	O
objective	O
as	O
follows	O
:	O
j	O
(	O
w	O
,	O
z	O
)	O
=	O
||x	O
−	O
wzt||2	O
(	O
12.27	O
)	O
where	O
z	O
is	O
an	O
n	O
×	O
l	O
matrix	O
with	O
the	O
zi	O
in	O
its	O
rows	O
,	O
and	O
||a||f	O
is	O
the	O
frobenius	O
norm	O
of	O
matrix	O
a	O
,	O
deﬁned	O
by	O
f	O
/001	O
m	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
2	O
tr	O
(	O
at	O
a	O
)	O
=	O
||a	O
(	O
:	O
)	O
||2	O
a2	O
ij	O
=	O
||a||f	O
=	O
(	O
12.28	O
)	O
i=1	O
j=1	O
the	O
optimal	O
solution	O
is	O
obtained	O
by	O
setting	O
ˆw	O
=	O
vl	O
,	O
where	O
vl	O
contains	O
the	O
l	O
eigenvectors	O
with	O
largest	O
eigenvalues	O
of	O
the	O
empirical	O
covariance	O
matrix	O
,	O
ˆσ	O
=	O
1	O
i	O
.	O
(	O
we	O
assume	O
the	O
n	O
xi	O
have	O
zero	O
mean	O
,	O
for	O
notational	O
simplicity	O
.	O
)	O
furthermore	O
,	O
the	O
optimal	O
low-dimensional	O
encoding	O
of	O
the	O
data	O
is	O
given	O
by	O
ˆzi	O
=	O
wt	O
xi	O
,	O
which	O
is	O
an	O
orthogonal	B
projection	I
of	O
the	O
data	O
onto	O
the	O
column	O
space	O
spanned	O
by	O
the	O
eigenvectors	O
.	O
(	O
cid:10	O
)	O
n	O
i=1	O
xixt	O
an	O
example	O
of	O
this	O
is	O
shown	O
in	O
figure	O
12.5	O
(	O
a	O
)	O
for	O
d	O
=	O
2	O
and	O
l	O
=	O
1.	O
the	O
diagonal	B
line	O
is	O
the	O
vector	O
w1	O
;	O
this	O
is	O
called	O
the	O
ﬁrst	O
principal	B
component	I
or	O
principal	O
direction	O
.	O
the	O
data	O
points	O
xi	O
∈	O
r	O
2	O
are	O
orthogonally	O
projected	O
onto	O
this	O
line	O
to	O
get	O
zi	O
∈	O
r.	O
this	O
is	O
the	O
best	O
1-dimensional	O
approximation	O
to	O
the	O
data	O
.	O
(	O
we	O
will	O
discuss	O
figure	O
12.5	O
(	O
b	O
)	O
later	O
.	O
)	O
in	O
general	O
,	O
it	O
is	O
hard	O
to	O
visualize	O
higher	O
dimensional	O
data	O
,	O
but	O
if	O
the	O
data	O
happens	O
to	O
be	O
a	O
set	O
of	O
images	O
,	O
it	O
is	O
easy	O
to	O
do	O
so	O
.	O
figure	O
12.6	O
shows	O
the	O
ﬁrst	O
three	O
principal	O
vectors	O
,	O
reshaped	O
as	O
images	O
,	O
as	O
well	O
as	O
the	O
reconstruction	O
of	O
a	O
speciﬁc	O
image	O
using	O
a	O
varying	O
number	O
of	O
basis	O
vectors	O
.	O
(	O
we	O
discuss	O
how	O
to	O
choose	O
l	O
in	O
section	O
11.5	O
.	O
)	O
below	O
we	O
will	O
show	O
that	O
the	O
principal	O
directions	O
are	O
the	O
ones	O
along	O
which	O
the	O
data	O
shows	O
maximal	O
variance	O
.	O
this	O
means	O
that	O
pca	O
can	O
be	O
“	O
misled	O
”	O
by	O
directions	O
in	O
which	O
the	O
variance	B
is	O
high	O
merely	O
because	O
of	O
the	O
measurement	O
scale	O
.	O
figure	O
12.7	O
(	O
a	O
)	O
shows	O
an	O
example	O
,	O
where	O
the	O
vertical	O
axis	O
(	O
weight	O
)	O
uses	O
a	O
large	O
range	O
than	O
the	O
horizontal	O
axis	O
(	O
height	O
)	O
,	O
resulting	O
in	O
a	O
line	O
that	O
it	O
is	O
therefore	O
standard	O
practice	O
to	O
standardize	O
the	O
data	O
ﬁrst	O
,	O
or	O
looks	O
somewhat	O
“	O
unnatural	O
”	O
.	O
12.2.	O
principal	B
components	I
analysis	I
(	O
pca	O
)	O
389	O
mean	B
principal	O
basis	O
1	O
reconstructed	O
with	O
2	O
bases	O
reconstructed	O
with	O
10	O
bases	O
principal	O
basis	O
2	O
principal	O
basis	O
3	O
reconstructed	O
with	O
100	O
bases	O
reconstructed	O
with	O
506	O
bases	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
12.6	O
(	O
a	O
)	O
the	O
mean	B
and	O
the	O
ﬁrst	O
three	O
pc	O
basis	O
vectors	O
(	O
eigendigits	O
)	O
based	O
on	O
25	O
images	O
of	O
the	O
digit	O
3	O
(	O
from	O
the	O
mnist	O
dataset	O
)	O
.	O
(	O
b	O
)	O
reconstruction	O
of	O
an	O
image	O
based	O
on	O
2	O
,	O
10	O
,	O
100	O
and	O
all	O
the	O
basis	O
vectors	O
.	O
figure	O
generated	O
by	O
pcaimagedemo	O
.	O
300	O
250	O
t	O
i	O
h	O
g	O
e	O
w	O
200	O
150	O
100	O
50	O
55	O
60	O
65	O
70	O
height	O
(	O
a	O
)	O
75	O
80	O
85	O
t	O
i	O
h	O
g	O
e	O
w	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−4	O
−3	O
−2	O
−1	O
0	O
height	O
(	O
b	O
)	O
1	O
2	O
3	O
4	O
figure	O
12.7	O
effect	O
of	O
standardization	O
on	O
pca	O
applied	O
to	O
the	O
height/	O
weight	O
dataset	O
.	O
left	O
:	O
pca	O
of	O
raw	O
data	O
.	O
right	O
:	O
pca	O
of	O
standardized	B
data	O
.	O
figure	O
generated	O
by	O
pcademoheightweight	O
.	O
equivalently	O
,	O
to	O
work	O
with	O
correlation	O
matrices	O
instead	O
of	O
covariance	B
matrices	O
.	O
the	O
beneﬁts	O
of	O
this	O
are	O
apparent	O
from	O
figure	O
12.7	O
(	O
b	O
)	O
.	O
12.2.2	O
proof	O
*	O
proof	O
.	O
we	O
use	O
wj	O
∈	O
r	O
high-dimensional	O
observation	B
,	O
zi	O
∈	O
r	O
˜zj	O
∈	O
r	O
vectors	O
.	O
points	O
˜z1	O
∈	O
r	O
d	O
to	O
denote	O
the	O
j	O
’	O
th	O
principal	O
direction	O
,	O
xi	O
∈	O
r	O
d	O
to	O
denote	O
the	O
i	O
’	O
th	O
l	O
to	O
denote	O
the	O
i	O
’	O
th	O
low-dimensional	O
representation	O
,	O
and	O
n	O
to	O
denote	O
the	O
[	O
z1j	O
,	O
.	O
.	O
.	O
,	O
zn	O
j	O
]	O
,	O
which	O
is	O
the	O
j	O
’	O
th	O
component	O
of	O
all	O
the	O
low-dimensional	O
let	O
us	O
start	O
by	O
estimating	O
the	O
best	O
1d	O
solution	O
,	O
w1	O
∈	O
r	O
d	O
,	O
and	O
the	O
corresponding	O
projected	O
n	O
.	O
we	O
will	O
ﬁnd	O
the	O
remaining	O
bases	O
w2	O
,	O
w3	O
,	O
etc	O
.	O
later	O
.	O
the	O
reconstruction	B
error	I
390	O
is	O
given	O
by	O
j	O
(	O
w1	O
,	O
z1	O
)	O
=	O
=	O
=	O
1	O
n	O
1	O
n	O
1	O
n	O
i=1	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
i=1	O
chapter	O
12.	O
latent	B
linear	O
models	O
||xi	O
−	O
zi1w1||2	O
=	O
n	O
(	O
cid:2	O
)	O
i=1	O
1	O
n	O
(	O
xi	O
−	O
zi1w1	O
)	O
t	O
(	O
xi	O
−	O
zi1w1	O
)	O
i	O
xi	O
−	O
2zi1wt	O
[	O
xt	O
1	O
xi	O
+	O
z2	O
i1wt	O
1	O
w1	O
]	O
i	O
xi	O
−	O
2zi1wt	O
[	O
xt	O
1	O
xi	O
+	O
z2	O
i1	O
]	O
(	O
12.29	O
)	O
(	O
12.30	O
)	O
(	O
12.31	O
)	O
1	O
w1	O
=	O
1	O
(	O
by	O
the	O
orthonormality	O
assumption	O
)	O
.	O
taking	O
derivatives	O
wrt	O
zi1	O
and	O
equating	O
since	O
wt	O
to	O
zero	O
gives	O
∂	O
∂zi1	O
j	O
(	O
w1	O
,	O
z1	O
)	O
=	O
1	O
n	O
[	O
−2wt	O
1	O
xi	O
+	O
2zi1	O
]	O
=	O
0	O
⇒	O
zi1	O
=	O
wt	O
1	O
xi	O
(	O
12.32	O
)	O
so	O
the	O
optimal	O
reconstruction	O
weights	O
are	O
obtained	O
by	O
orthogonally	O
projecting	O
the	O
data	O
onto	O
the	O
ﬁrst	O
principal	O
direction	O
,	O
w1	O
(	O
see	O
figure	O
12.5	O
(	O
a	O
)	O
)	O
.	O
plugging	O
back	O
in	O
gives	O
j	O
(	O
w1	O
)	O
=	O
1	O
n	O
i	O
xi	O
−	O
z2	O
[	O
xt	O
i1	O
]	O
=	O
const	O
−	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
i=1	O
n	O
(	O
cid:2	O
)	O
i=1	O
z2	O
i1	O
now	O
the	O
variance	B
of	O
the	O
projected	O
coordinates	O
is	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
i=1	O
1	O
n	O
i1	O
−	O
0	O
z2	O
(	O
cid:31	O
)	O
˜z2	O
1	O
−	O
(	O
e	O
[	O
˜z1	O
]	O
)	O
2	O
=	O
xt	O
i	O
w1	O
=	O
e	O
[	O
xi	O
]	O
t	O
w1	O
=	O
0	O
var	O
[	O
˜z1	O
]	O
=	O
e	O
since	O
e	O
[	O
zi1	O
]	O
=	O
e	O
(	O
cid:31	O
)	O
(	O
12.33	O
)	O
(	O
12.34	O
)	O
(	O
12.35	O
)	O
because	O
the	O
data	O
has	O
been	O
centered	O
.	O
from	O
this	O
,	O
we	O
see	O
that	O
minimizing	O
the	O
reconstruction	B
error	I
is	O
equivalent	O
to	O
maximizing	O
the	O
variance	B
of	O
the	O
projected	O
data	O
,	O
i.e.	O
,	O
arg	O
min	O
w1	O
j	O
(	O
w1	O
)	O
=	O
arg	O
max	O
w1	O
var	O
[	O
˜z1	O
]	O
(	O
12.36	O
)	O
this	O
is	O
why	O
it	O
is	O
often	O
said	O
that	O
pca	O
ﬁnds	O
the	O
directions	O
of	O
maximal	O
variance	O
.	O
this	O
is	O
called	O
the	O
analysis	B
view	I
of	O
pca	O
.	O
the	O
variance	B
of	O
the	O
projected	O
data	O
can	O
be	O
written	O
as	O
z2	O
i1	O
=	O
wt	O
1	O
xixt	O
i	O
w1	O
=	O
wt	O
1	O
ˆσw1	O
(	O
12.37	O
)	O
i	O
xixt	O
i	O
is	O
the	O
empirical	O
covariance	O
matrix	O
(	O
or	O
correlation	B
matrix	I
if	O
the	O
n	O
(	O
cid:2	O
)	O
i=1	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
i=1	O
1	O
n	O
(	O
cid:10	O
)	O
n	O
where	O
ˆσ	O
=	O
1	O
i=1	O
n	O
data	O
is	O
standardized	B
)	O
.	O
12.2.	O
principal	B
components	I
analysis	I
(	O
pca	O
)	O
391	O
we	O
can	O
trivially	O
maximize	O
the	O
variance	B
of	O
the	O
projection	B
(	O
and	O
hence	O
minimize	O
the	O
recon-	O
struction	O
error	O
)	O
by	O
letting	O
||w1||	O
→	O
∞	O
,	O
so	O
we	O
impose	O
the	O
constraint	O
||w1||	O
=	O
1	O
and	O
instead	O
maximize	O
˜j	O
(	O
w1	O
)	O
=	O
wt	O
1	O
ˆσw1	O
+	O
λ1	O
(	O
wt	O
1	O
w1	O
−	O
1	O
)	O
where	O
λ1	O
is	O
the	O
lagrange	O
multiplier	O
.	O
taking	O
derivatives	O
and	O
equating	O
to	O
zero	O
we	O
have	O
∂	O
∂w1	O
˜j	O
(	O
w1	O
)	O
=	O
2	O
ˆσw1	O
−	O
2λ1w1	O
=	O
0	O
ˆσw1	O
=	O
λ1w1	O
(	O
12.38	O
)	O
(	O
12.39	O
)	O
(	O
12.40	O
)	O
hence	O
the	O
direction	O
that	O
maximizes	O
the	O
variance	B
is	O
an	O
eigenvector	O
of	O
the	O
covariance	B
matrix	I
.	O
left	O
multiplying	O
by	O
w1	O
(	O
and	O
using	O
wt	O
1	O
w1	O
=	O
1	O
)	O
we	O
ﬁnd	O
that	O
the	O
variance	B
of	O
the	O
projected	O
data	O
is	O
wt	O
1	O
ˆσw1	O
=	O
λ1	O
(	O
12.41	O
)	O
since	O
we	O
want	O
to	O
maximize	O
the	O
variance	B
,	O
we	O
pick	O
the	O
eigenvector	O
which	O
corresponds	O
to	O
the	O
largest	O
eigenvalue	O
.	O
now	O
let	O
us	O
ﬁnd	O
another	O
direction	O
w2	O
to	O
further	O
minimize	O
the	O
reconstruction	B
error	I
,	O
subject	O
to	O
1	O
w2	O
=	O
0	O
and	O
wt	O
2	O
w2	O
=	O
1.	O
the	O
error	O
is	O
wt	O
j	O
(	O
w1	O
,	O
z1	O
,	O
w2	O
,	O
z2	O
)	O
=	O
1	O
n	O
||xi	O
−	O
zi1w1	O
−	O
zi2w2||2	O
(	O
12.42	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
optimizing	O
wrt	O
w1	O
and	O
z1	O
gives	O
the	O
same	O
solution	O
as	O
before	O
.	O
exercise	O
12.4	O
asks	O
you	O
to	O
show	O
that	O
∂j	O
in	O
other	O
words	O
,	O
the	O
second	O
principal	O
encoding	O
is	O
gotten	O
by	O
∂z2	O
projecting	O
onto	O
the	O
second	O
principal	O
direction	O
.	O
substituting	O
in	O
yields	O
=	O
0	O
yields	O
zi2	O
=	O
wt	O
2	O
xi	O
.	O
j	O
(	O
w2	O
)	O
=	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
i=1	O
i	O
xi	O
−	O
wt	O
[	O
xt	O
1	O
xixt	O
i	O
w1	O
−	O
wt	O
2	O
xixt	O
i	O
w2	O
]	O
=	O
const	O
−	O
wt	O
2	O
ˆσw2	O
dropping	O
the	O
constant	O
term	O
and	O
adding	O
the	O
constraints	O
yields	O
2	O
w1	O
−	O
0	O
)	O
2	O
w2	O
−	O
1	O
)	O
+	O
λ12	O
(	O
wt	O
˜j	O
(	O
w2	O
)	O
=	O
−wt	O
2	O
ˆσw2	O
+	O
λ2	O
(	O
wt	O
(	O
12.43	O
)	O
(	O
12.44	O
)	O
exercise	O
12.4	O
asks	O
you	O
to	O
show	O
that	O
the	O
solution	O
is	O
given	O
by	O
the	O
eigenvector	O
with	O
the	O
second	O
largest	O
eigenvalue	O
:	O
ˆσw2	O
=	O
λ2w2	O
(	O
12.45	O
)	O
the	O
proof	O
continues	O
in	O
this	O
way	O
.	O
(	O
formally	O
one	O
can	O
use	O
induction	B
.	O
)	O
392	O
chapter	O
12.	O
latent	B
linear	O
models	O
12.2.3	O
singular	B
value	I
decomposition	I
(	O
svd	O
)	O
we	O
have	O
deﬁned	O
the	O
solution	O
to	O
pca	O
in	O
terms	O
of	O
eigenvectors	O
of	O
the	O
covariance	B
matrix	I
.	O
however	O
,	O
there	O
is	O
another	O
way	O
to	O
obtain	O
the	O
solution	O
,	O
based	O
on	O
the	O
singular	B
value	I
decomposition	I
,	O
or	O
svd	O
.	O
this	O
basically	O
generalizes	O
the	O
notion	O
of	O
eigenvectors	O
from	O
square	O
matrices	O
to	O
any	O
kind	O
of	O
matrix	O
.	O
in	O
particular	O
,	O
any	O
(	O
real	O
)	O
n	O
×	O
d	O
matrix	O
x	O
can	O
be	O
decomposed	O
as	O
follows	O
x3456	O
n×d	O
=	O
u3456	O
n×n	O
s3456	O
n×d	O
vt3456	O
d×d	O
(	O
12.46	O
)	O
where	O
u	O
is	O
an	O
n	O
×	O
n	O
matrix	O
whose	O
columns	O
are	O
orthornormal	O
(	O
so	O
ut	O
u	O
=	O
in	O
)	O
,	O
v	O
is	O
d	O
×	O
d	O
matrix	O
whose	O
rows	O
and	O
columns	O
are	O
orthonormal	O
(	O
so	O
vt	O
v	O
=	O
vvt	O
=	O
id	O
)	O
,	O
and	O
s	O
is	O
a	O
n	O
×	O
d	O
matrix	O
containing	O
the	O
r	O
=	O
min	O
(	O
n	O
,	O
d	O
)	O
singular	B
values	I
σi	O
≥	O
0	O
on	O
the	O
main	O
diagonal	B
,	O
with	O
0s	O
ﬁlling	O
the	O
rest	O
of	O
the	O
matrix	O
.	O
the	O
columns	O
of	O
u	O
are	O
the	O
left	O
singular	O
vectors	O
,	O
and	O
the	O
columns	O
of	O
v	O
are	O
the	O
right	O
singular	O
vectors	O
.	O
see	O
figure	O
12.8	O
(	O
a	O
)	O
for	O
an	O
example	O
.	O
since	O
there	O
are	O
at	O
most	O
d	O
singular	B
values	I
(	O
assuming	O
n	O
>	O
d	O
)	O
,	O
the	O
last	O
n	O
−	O
d	O
columns	O
of	O
u	O
are	O
irrelevant	O
,	O
since	O
they	O
will	O
be	O
multiplied	O
by	O
0.	O
the	O
economy	O
sized	O
svd	O
,	O
orthin	O
svd	O
,	O
avoids	O
computing	O
these	O
unnecessary	O
elements	O
.	O
let	O
us	O
denote	O
this	O
decomposition	O
by	O
ˆuˆs	O
ˆv	O
.	O
if	O
n	O
>	O
d	O
,	O
we	O
have	O
x3456	O
n×d	O
=	O
ˆu3456	O
n×d	O
ˆs3456	O
d×d	O
ˆvt3456	O
d×d	O
as	O
in	O
figure	O
12.8	O
(	O
a	O
)	O
.	O
if	O
n	O
<	O
d	O
,	O
we	O
have	O
x3456	O
n×d	O
=	O
ˆu3456	O
n×n	O
ˆs3456	O
n×n	O
ˆvt3456	O
n×d	O
(	O
12.47	O
)	O
(	O
12.48	O
)	O
computing	O
the	O
economy-sized	O
svd	O
takes	O
o	O
(	O
n	O
d	O
min	O
(	O
n	O
,	O
d	O
)	O
)	O
time	O
(	O
golub	O
and	O
van	O
loan	O
1996	O
,	O
p254	O
)	O
.	O
the	O
connection	O
between	O
eigenvectors	O
and	O
singular	O
vectors	O
is	O
the	O
following	O
.	O
for	O
an	O
arbitrary	O
real	O
matrix	O
x	O
,	O
ifx	O
=	O
usvt	O
,	O
we	O
have	O
xt	O
x	O
=	O
vst	O
ut	O
usvt	O
=	O
v	O
(	O
st	O
s	O
)	O
vt	O
=	O
vdvt	O
where	O
d	O
=	O
s2	O
is	O
a	O
diagonal	B
matrix	O
containing	O
the	O
squares	O
singular	B
values	I
.	O
hence	O
(	O
xt	O
x	O
)	O
v	O
=	O
vd	O
(	O
12.49	O
)	O
(	O
12.50	O
)	O
so	O
the	O
eigenvectors	O
of	O
xt	O
x	O
are	O
equal	O
to	O
v	O
,	O
the	O
right	O
singular	O
vectors	O
of	O
x	O
,	O
and	O
the	O
eigenvalues	O
of	O
xt	O
x	O
are	O
equal	O
to	O
d	O
,	O
the	O
squared	O
singular	O
values	O
.	O
similarly	O
xxt	O
=	O
usvt	O
vst	O
ut	O
=	O
u	O
(	O
sst	O
)	O
ut	O
(	O
xxt	O
)	O
u	O
=	O
u	O
(	O
sst	O
)	O
=	O
ud	O
(	O
12.51	O
)	O
(	O
12.52	O
)	O
so	O
the	O
eigenvectors	O
of	O
xxt	O
are	O
equal	O
to	O
u	O
,	O
the	O
left	O
singular	O
vectors	O
of	O
x.	O
also	O
,	O
the	O
eigenvalues	O
of	O
xxt	O
are	O
equal	O
to	O
the	O
squared	O
singular	O
values	O
.	O
we	O
can	O
summarize	O
all	O
this	O
as	O
follows	O
:	O
u	O
=	O
evec	O
(	O
xxt	O
)	O
,	O
v	O
=	O
evec	O
(	O
xt	O
x	O
)	O
,	O
s2	O
=	O
eval	O
(	O
xxt	O
)	O
=	O
eval	O
(	O
xt	O
x	O
)	O
(	O
12.53	O
)	O
12.2.	O
principal	B
components	I
analysis	I
(	O
pca	O
)	O
393	O
d	O
d	O
n	O
−	O
d	O
n	O
=	O
x	O
=	O
u	O
σ1	O
σd	O
d	O
.	O
.	O
.	O
0	O
s	O
d	O
d	O
v	O
t	O
n	O
d	O
x	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
a	O
)	O
l	O
l	O
σ1	O
.	O
.	O
.σl	O
d	O
l	O
u	O
l	O
sl	O
v	O
t	O
l	O
(	O
b	O
)	O
figure	O
12.8	O
(	O
a	O
)	O
svd	O
decomposition	O
of	O
non-square	O
matrices	O
x	O
=	O
usvt	O
.	O
the	O
shaded	O
parts	O
of	O
s	O
,	O
and	O
all	O
the	O
off-diagonal	O
terms	O
,	O
are	O
zero	O
.	O
the	O
shaded	O
entries	O
in	O
u	O
and	O
s	O
are	O
not	O
computed	O
in	O
the	O
economy-sized	O
version	O
,	O
since	O
they	O
are	O
not	O
needed	O
.	O
(	O
b	O
)	O
truncated	O
svd	O
approximation	O
of	O
rank	O
l.	O
since	O
the	O
eigenvectors	O
are	O
unaffected	O
by	O
linear	O
scaling	O
of	O
a	O
matrix	O
,	O
we	O
see	O
that	O
the	O
right	O
singular	O
vectors	O
of	O
x	O
are	O
equal	O
to	O
the	O
eigenvectors	O
of	O
the	O
empirical	O
covariance	O
ˆσ	O
.	O
furthermore	O
,	O
the	O
eigenvalues	O
of	O
ˆσ	O
are	O
a	O
scaled	O
version	O
of	O
the	O
squared	O
singular	O
values	O
.	O
this	O
means	O
we	O
can	O
perform	O
pca	O
using	O
just	O
a	O
few	O
lines	O
of	O
code	O
(	O
see	O
pcapmtk	O
)	O
.	O
however	O
,	O
the	O
connection	O
between	O
pca	O
and	O
svd	O
goes	O
deeper	O
.	O
from	O
equation	O
12.46	O
,	O
we	O
can	O
represent	O
a	O
rank	O
r	O
matrix	O
as	O
follows	O
:	O
⎞	O
⎠	O
(	O
cid:3	O
)	O
−	O
vt	O
1	O
−	O
(	O
cid:4	O
)	O
⎛	O
⎝	O
|	O
u1|	O
x	O
=	O
σ1	O
⎞	O
⎠	O
(	O
cid:3	O
)	O
−	O
vt	O
r	O
−	O
(	O
cid:4	O
)	O
⎛	O
⎝	O
|	O
ur|	O
+	O
···	O
+	O
σr	O
(	O
12.54	O
)	O
if	O
the	O
singular	B
values	I
die	O
off	O
quickly	O
as	O
in	O
figure	O
12.10	O
,	O
we	O
can	O
produce	O
a	O
rank	O
l	O
approximation	O
to	O
the	O
matrix	O
as	O
follows	O
:	O
x	O
≈	O
u	O
:	O
,1	O
:	O
l	O
s1	O
:	O
l,1	O
:	O
l	O
vt	O
:	O
,1	O
:	O
l	O
(	O
12.55	O
)	O
this	O
is	O
called	O
a	O
truncated	O
svd	O
(	O
see	O
figure	O
12.8	O
(	O
b	O
)	O
)	O
.	O
the	O
total	O
number	O
of	O
parameters	O
needed	O
to	O
represent	O
an	O
n	O
×	O
d	O
matrix	O
using	O
a	O
rank	O
l	O
approximation	O
is	O
n	O
l	O
+	O
ld	O
+	O
l	O
=	O
l	O
(	O
n	O
+	O
d	O
+	O
1	O
)	O
(	O
12.56	O
)	O
394	O
chapter	O
12.	O
latent	B
linear	O
models	O
rank	O
200	O
rank	O
2	O
(	O
a	O
)	O
rank	O
5	O
(	O
c	O
)	O
(	O
b	O
)	O
rank	O
20	O
(	O
d	O
)	O
figure	O
12.9	O
low	O
rank	O
approximations	O
to	O
an	O
image	O
.	O
top	O
left	O
:	O
the	O
original	O
image	O
is	O
of	O
size	O
200	O
×	O
320	O
,	O
so	O
has	O
rank	O
200.	O
subsequent	O
images	O
have	O
ranks	O
2	O
,	O
5	O
,	O
and	O
20.	O
figure	O
generated	O
by	O
svdimagedemo	O
.	O
original	O
randomized	O
10	O
9	O
8	O
7	O
6	O
5	O
i	O
)	O
σ	O
(	O
g	O
o	O
l	O
4	O
0	O
10	O
20	O
30	O
40	O
60	O
70	O
80	O
90	O
100	O
50	O
i	O
figure	O
12.10	O
first	O
50	O
log	O
singular	O
values	O
for	O
the	O
clown	O
image	O
(	O
solid	O
red	O
line	O
)	O
,	O
and	O
for	O
a	O
data	O
matrix	O
obtained	O
by	O
randomly	O
shuffling	O
the	O
pixels	O
(	O
dotted	O
green	O
line	O
)	O
.	O
figure	O
generated	O
by	O
svdimagedemo	O
.	O
12.2.	O
principal	B
components	I
analysis	I
(	O
pca	O
)	O
395	O
as	O
an	O
example	O
,	O
consider	O
the	O
200	O
×	O
320	O
pixel	O
image	O
in	O
figure	O
12.9	O
(	O
top	O
left	O
)	O
.	O
this	O
has	O
64,000	O
numbers	O
in	O
it	O
.	O
we	O
see	O
that	O
a	O
rank	O
20	O
approximation	O
,	O
with	O
only	O
(	O
200	O
+	O
320	O
+	O
1	O
)	O
×	O
20	O
=	O
10	O
,	O
420	O
numbers	O
is	O
a	O
very	O
good	O
approximation	O
.	O
one	O
can	O
show	O
that	O
the	O
error	O
in	O
this	O
approximation	O
is	O
given	O
by	O
||x	O
−	O
xl||f	O
≈	O
σl+1	O
(	O
12.57	O
)	O
furthermore	O
,	O
one	O
can	O
show	O
that	O
the	O
svd	O
offers	O
the	O
best	O
rank	O
l	O
approximation	O
to	O
a	O
matrix	O
(	O
best	O
in	O
the	O
sense	O
of	O
minimizing	O
the	O
above	O
frobenius	O
norm	O
)	O
.	O
let	O
us	O
connect	O
this	O
back	O
to	O
pca	O
.	O
let	O
x	O
=	O
usvt	O
be	O
a	O
truncated	O
svd	O
of	O
x.	O
we	O
know	O
that	O
ˆw	O
=	O
v	O
,	O
and	O
that	O
ˆz	O
=	O
x	O
ˆw	O
,	O
so	O
ˆz	O
=	O
usvt	O
v	O
=	O
us	O
furthermore	O
,	O
the	O
optimal	O
reconstruction	O
is	O
given	O
by	O
ˆx	O
=	O
z	O
ˆwt	O
,	O
so	O
we	O
ﬁnd	O
ˆx	O
=	O
usvt	O
(	O
12.58	O
)	O
(	O
12.59	O
)	O
this	O
is	O
precisely	O
the	O
same	O
as	O
a	O
truncated	O
svd	O
approximation	O
!	O
this	O
is	O
another	O
illustration	O
of	O
the	O
fact	O
that	O
pca	O
is	O
the	O
best	O
low	O
rank	O
approximation	O
to	O
the	O
data	O
.	O
12.2.4	O
probabilistic	O
pca	O
we	O
are	O
now	O
ready	O
to	O
revisit	O
ppca	O
.	O
one	O
can	O
show	O
the	O
following	O
remarkable	O
result	O
.	O
theorem	O
12.2.2	O
(	O
(	O
tipping	O
and	O
bishop	O
1999	O
)	O
)	O
.	O
consider	O
a	O
factor	B
analysis	I
model	O
in	O
which	O
ψ	O
=	O
σ2i	O
and	O
w	O
is	O
orthogonal	O
.	O
the	O
observed	B
data	I
log	I
likelihood	I
is	O
given	O
by	O
i	O
c−1xi	O
=	O
−	O
n	O
xt	O
2	O
log	O
p	O
(	O
x|w	O
,	O
σ2	O
)	O
=−	O
n	O
2	O
ln|c|	O
+	O
tr	O
(	O
c−1	O
ˆσ	O
)	O
(	O
12.60	O
)	O
where	O
c	O
=	O
wwt	O
+	O
σ2i	O
and	O
s	O
=	O
1	O
n	O
data	O
,	O
for	O
notational	O
simplicity	O
.	O
)	O
the	O
maxima	O
of	O
the	O
log-likelihood	O
are	O
given	O
by	O
i	O
=	O
(	O
1/n	O
)	O
xt	O
x	O
.	O
(	O
we	O
are	O
assuming	O
centered	O
n	O
(	O
cid:2	O
)	O
ln|c|	O
−	O
1	O
2	O
(	O
cid:10	O
)	O
n	O
i=1	O
xixt	O
i=1	O
ˆw	O
=	O
v	O
(	O
λ	O
−	O
σ2i	O
)	O
1	O
2	O
r	O
(	O
12.61	O
)	O
where	O
r	O
is	O
an	O
arbitrary	O
l	O
×	O
l	O
orthogonal	O
matrix	O
,	O
v	O
is	O
the	O
d	O
×	O
l	O
matrix	O
whose	O
columns	O
are	O
the	O
ﬁrst	O
l	O
eigenvectors	O
of	O
s	O
,	O
and	O
λ	O
is	O
the	O
corresponding	O
diagonal	B
matrix	O
of	O
eigenvalues	O
.	O
without	O
loss	B
of	O
generality	O
,	O
we	O
can	O
set	O
r	O
=	O
i.	O
furthermore	O
,	O
the	O
mle	O
of	O
the	O
noise	O
variance	O
is	O
given	O
by	O
λj	O
(	O
12.62	O
)	O
ˆσ2	O
=	O
1	O
d	O
−	O
l	O
d	O
(	O
cid:2	O
)	O
j=l+1	O
which	O
is	O
the	O
average	O
variance	O
associated	O
with	O
the	O
discarded	O
dimensions	O
.	O
thus	O
,	O
as	O
σ2	O
→	O
0	O
,	O
we	O
have	O
ˆw	O
→	O
v	O
,	O
as	O
in	O
classical	B
pca	O
.	O
what	O
about	O
ˆz	O
?	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
posterior	O
over	O
the	O
latent	B
factors	I
is	O
given	O
by	O
p	O
(	O
zi|xi	O
,	O
ˆθ	O
)	O
=n	O
(	O
zi|	O
ˆf−1	O
ˆwt	O
xi	O
,	O
σ2	O
ˆf−1	O
)	O
ˆf	O
(	O
cid:2	O
)	O
ˆwt	O
ˆw	O
+	O
ˆσ2i	O
(	O
12.63	O
)	O
(	O
12.64	O
)	O
396	O
chapter	O
12.	O
latent	B
linear	O
models	O
(	O
do	O
not	O
confuse	O
f	O
=	O
wt	O
w	O
+	O
σ2i	O
with	O
c	O
=	O
wwt	O
+	O
σ2i	O
.	O
)	O
hence	O
,	O
as	O
σ2	O
→	O
0	O
,	O
we	O
ﬁnd	O
ˆw	O
→	O
v	O
,	O
ˆf	O
→	O
i	O
and	O
ˆzi	O
→	O
vt	O
xi	O
.	O
thus	O
the	O
posterior	B
mean	I
is	O
obtained	O
by	O
an	O
orthogonal	B
projection	I
of	O
the	O
data	O
onto	O
the	O
column	O
space	O
of	O
v	O
,	O
as	O
in	O
classical	B
pca	O
.	O
note	O
,	O
however	O
,	O
that	O
if	O
σ2	O
>	O
,	O
the	O
posterior	B
mean	I
is	O
not	O
an	O
orthogonal	B
projection	I
,	O
since	O
it	O
is	O
shrunk	O
somewhat	O
towards	O
the	O
prior	O
mean	B
,	O
as	O
illustrated	O
in	O
figure	O
12.5	O
(	O
b	O
)	O
.	O
this	O
sounds	O
like	O
an	O
undesirable	O
property	O
,	O
but	O
it	O
means	O
that	O
the	O
reconstructions	O
will	O
be	O
closer	O
to	O
the	O
overall	O
data	O
mean	O
,	O
ˆμ	O
=	O
x	O
.	O
12.2.5	O
em	O
algorithm	O
for	O
pca	O
although	O
the	O
usual	O
way	O
to	O
ﬁt	O
a	O
pca	O
model	O
uses	O
eigenvector	O
methods	O
,	O
or	O
the	O
svd	O
,	O
we	O
can	O
also	O
use	O
em	O
,	O
which	O
will	O
turn	O
out	O
to	O
have	O
some	O
advantages	O
that	O
we	O
discuss	O
below	O
.	O
em	O
for	O
pca	O
relies	O
on	O
the	O
probabilistic	O
formulation	O
of	O
pca	O
.	O
however	O
the	O
algorithm	O
continues	O
to	O
work	O
in	O
the	O
zero	O
noise	O
limit	O
,	O
σ2	O
=	O
0	O
,	O
as	O
shown	O
by	O
(	O
roweis	O
1997	O
)	O
.	O
let	O
˜z	O
be	O
a	O
l	O
×	O
n	O
matrix	O
storing	O
the	O
posterior	O
means	O
(	O
low-dimensional	O
representations	O
)	O
let	O
˜x	O
=	O
xt	O
store	O
the	O
original	O
data	O
along	O
its	O
columns	O
.	O
from	O
along	O
its	O
columns	O
.	O
similarly	O
,	O
equation	O
12.63	O
,	O
when	O
σ2	O
=	O
0	O
,	O
we	O
have	O
˜z	O
=	O
(	O
wt	O
w	O
)	O
−1wt	O
˜x	O
this	O
constitutes	O
the	O
e	O
step	O
.	O
notice	O
that	O
this	O
is	O
just	O
an	O
orthogonal	B
projection	I
of	O
the	O
data	O
.	O
from	O
equation	O
12.23	O
,	O
the	O
m	O
step	O
is	O
given	O
by	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
(	O
cid:2	O
)	O
(	O
cid:19	O
)	O
−1	O
(	O
12.65	O
)	O
(	O
12.66	O
)	O
ˆw	O
=	O
xie	O
[	O
zi	O
]	O
t	O
e	O
[	O
zi	O
]	O
e	O
[	O
zi	O
]	O
t	O
i	O
i	O
(	O
cid:10	O
)	O
where	O
we	O
exploited	O
the	O
fact	O
that	O
σ	O
=	O
cov	O
[	O
zi|xi	O
,	O
θ	O
]	O
=	O
0i	O
when	O
σ2	O
=	O
0.	O
it	O
is	O
worth	O
comparing	O
this	O
expression	O
to	O
the	O
mle	O
for	O
multi-output	O
linear	B
regression	I
(	O
equation	O
7.89	O
)	O
,	O
which	O
has	O
the	O
form	O
−1	O
.	O
thus	O
we	O
see	O
that	O
the	O
m	O
step	O
is	O
like	O
linear	B
regression	I
where	O
we	O
w	O
=	O
(	O
replace	O
the	O
observed	O
inputs	O
by	O
the	O
expected	O
values	O
of	O
the	O
latent	B
variables	O
.	O
i	O
xixt	O
i	O
)	O
i	O
yixt	O
i	O
)	O
(	O
(	O
cid:10	O
)	O
in	O
summary	O
,	O
here	O
is	O
the	O
entire	O
algorithm	O
:	O
•	O
e	O
step	O
˜z	O
=	O
(	O
wt	O
w	O
)	O
•	O
m	O
stepw	O
=	O
˜x˜zt	O
(	O
˜z˜zt	O
)	O
−1	O
−1wt	O
˜x	O
(	O
tipping	O
and	O
bishop	O
1999	O
)	O
showed	O
that	O
the	O
only	O
stable	B
ﬁxed	O
point	O
of	O
the	O
em	O
algorithm	O
is	O
the	O
globally	O
optimal	O
solution	O
.	O
that	O
is	O
,	O
the	O
em	O
algorithm	O
converges	O
to	O
a	O
solution	O
where	O
w	O
spans	O
the	O
same	O
linear	O
subspace	O
as	O
that	O
deﬁned	O
by	O
the	O
ﬁrst	O
l	O
eigenvectors	O
.	O
however	O
,	O
if	O
we	O
want	O
w	O
to	O
be	O
orthogonal	O
,	O
and	O
to	O
contain	O
the	O
eigenvectors	O
in	O
descending	O
order	O
of	O
eigenvalue	O
,	O
we	O
have	O
to	O
orthogonalize	O
the	O
resulting	O
matrix	O
(	O
which	O
can	O
be	O
done	O
quite	O
cheaply	O
)	O
.	O
alternatively	O
,	O
we	O
can	O
modify	O
em	O
to	O
give	O
the	O
principal	O
basis	O
directly	O
(	O
ahn	O
and	O
oh	O
2003	O
)	O
.	O
this	O
algorithm	O
has	O
a	O
simple	O
physical	O
analogy	O
in	O
the	O
case	O
d	O
=	O
2	O
and	O
l	O
=	O
1	O
(	O
roweis	O
1997	O
)	O
.	O
2	O
attached	O
by	O
springs	O
to	O
a	O
rigid	O
rod	O
,	O
whose	O
orientation	O
is	O
deﬁned	O
by	O
a	O
consider	O
some	O
points	O
in	O
r	O
vector	O
w.	O
let	O
zi	O
be	O
the	O
location	O
where	O
the	O
i	O
’	O
th	O
spring	O
attaches	O
to	O
the	O
rod	O
.	O
in	O
the	O
e	O
step	O
,	O
we	O
hold	O
the	O
rod	O
ﬁxed	O
,	O
and	O
let	O
the	O
attachment	O
points	O
slide	O
around	O
so	O
as	O
to	O
minimize	O
the	O
spring	O
energy	O
(	O
which	O
is	O
proportional	O
to	O
the	O
sum	O
of	O
squared	O
residuals	O
)	O
.	O
in	O
the	O
m	O
step	O
,	O
we	O
hold	O
the	O
attachment	O
12.2.	O
principal	B
components	I
analysis	I
(	O
pca	O
)	O
397	O
e	O
step	O
1	O
m	O
step	O
1	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−2	O
−2.5	O
−3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−2	O
−2.5	O
−3	O
1	O
2	O
3	O
−2	O
−1	O
0	O
(	O
a	O
)	O
e	O
step	O
2	O
−2	O
−1	O
0	O
(	O
c	O
)	O
1	O
2	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−2	O
−2.5	O
−3	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−3	O
1	O
2	O
3	O
−2	O
−1	O
0	O
(	O
b	O
)	O
m	O
step	O
2	O
−2	O
−1	O
0	O
(	O
d	O
)	O
1	O
2	O
3	O
illustration	O
of	O
em	O
for	O
pca	O
when	O
d	O
=	O
2	O
and	O
l	O
=	O
1.	O
green	O
stars	O
are	O
the	O
original	O
data	O
points	O
,	O
figure	O
12.11	O
black	O
circles	O
are	O
their	O
reconstructions	O
.	O
the	O
weight	B
vector	I
w	O
is	O
represented	O
by	O
blue	O
line	O
.	O
(	O
a	O
)	O
we	O
start	O
with	O
a	O
random	O
initial	O
guess	O
of	O
w.	O
the	O
e	O
step	O
is	O
represented	O
by	O
the	O
orthogonal	O
projections	O
.	O
(	O
b	O
)	O
we	O
update	O
the	O
rod	O
w	O
in	O
the	O
m	O
step	O
,	O
keeping	O
the	O
projections	O
onto	O
the	O
rod	O
(	O
black	O
circles	O
)	O
ﬁxed	O
.	O
(	O
c	O
)	O
another	O
e	O
step	O
.	O
the	O
black	O
circles	O
can	O
’	O
slide	O
’	O
along	O
the	O
rod	O
,	O
but	O
the	O
rod	O
stays	O
ﬁxed	O
.	O
(	O
d	O
)	O
another	O
m	O
step	O
.	O
based	O
on	O
figure	O
12.12	O
of	O
(	O
bishop	O
2006b	O
)	O
.	O
figure	O
generated	O
by	O
pcaemstepbystep	O
.	O
points	O
ﬁxed	O
and	O
let	O
the	O
rod	O
rotate	O
so	O
as	O
to	O
minimize	O
the	O
spring	O
energy	O
.	O
see	O
figure	O
12.11	O
for	O
an	O
illustration	O
.	O
apart	O
from	O
this	O
pleasing	O
intuitive	O
interpretation	O
,	O
em	O
for	O
pca	O
has	O
the	O
following	O
advantages	O
over	O
eigenvector	O
methods	O
:	O
•	O
em	O
can	O
be	O
faster	O
.	O
in	O
particular	O
,	O
assuming	O
n	O
,	O
d	O
(	O
cid:18	O
)	O
l	O
,	O
the	O
dominant	O
cost	O
of	O
em	O
is	O
the	O
pro-	O
jection	O
operation	O
in	O
the	O
e	O
step	O
,	O
so	O
the	O
overall	O
time	O
is	O
o	O
(	O
t	O
ln	O
d	O
)	O
,	O
where	O
t	O
is	O
the	O
number	O
of	O
398	O
chapter	O
12.	O
latent	B
linear	O
models	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
(	O
e	O
)	O
(	O
f	O
)	O
figure	O
12.12	O
illustration	O
of	O
estimating	O
the	O
effective	O
dimensionalities	O
in	O
a	O
mixture	B
of	I
factor	I
analysers	I
using	O
vbem	O
.	O
the	O
blank	O
columns	O
have	O
been	O
forced	O
to	O
0	O
via	O
the	O
ard	O
mechanism	O
.	O
the	O
data	O
was	O
generated	O
from	O
6	O
clusters	B
with	O
intrinsic	O
dimensionalities	O
of	O
7	O
,	O
4	O
,	O
3	O
,	O
2	O
,	O
2	O
,	O
1	O
,	O
which	O
the	O
method	O
has	O
successfully	O
estimated	O
.	O
source	O
:	O
figure	O
4.4	O
of	O
(	O
beal	O
2003	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
matt	O
beal	O
.	O
iterations	O
.	O
(	O
roweis	O
1997	O
)	O
showed	O
experimentally	O
that	O
the	O
number	O
of	O
iterations	O
is	O
usually	O
very	O
small	O
(	O
the	O
mean	B
was	O
3.6	O
)	O
,	O
regardless	O
of	O
n	O
or	O
d.	O
(	O
this	O
results	O
depends	O
on	O
the	O
ratio	O
of	O
eigenval-	O
ues	O
of	O
the	O
empirical	O
covariance	O
matrix	O
.	O
)	O
this	O
is	O
much	O
faster	O
than	O
the	O
o	O
(	O
min	O
(	O
n	O
d2	O
,	O
dn	O
2	O
)	O
)	O
time	O
required	O
by	O
straightforward	O
eigenvector	O
methods	O
,	O
although	O
more	O
sophisticated	O
eigenvec-	O
tor	O
methods	O
,	O
such	O
as	O
the	O
lanczos	O
algorithm	O
,	O
have	O
running	O
times	O
comparable	O
to	O
em	O
.	O
•	O
em	O
can	O
be	O
implemented	O
in	O
an	O
online	O
fashion	O
,	O
i.e.	O
,	O
we	O
can	O
update	O
our	O
estimate	O
of	O
w	O
as	O
the	O
data	O
streams	O
in	O
.	O
•	O
em	O
can	O
handle	O
missing	B
data	I
in	O
a	O
simple	O
way	O
(	O
see	O
section	O
12.1.6	O
)	O
.	O
•	O
em	O
can	O
be	O
extended	O
to	O
handle	O
mixtures	O
of	O
ppca/	O
fa	O
models	O
.	O
•	O
em	O
can	O
be	O
modiﬁed	O
to	O
variational	O
em	O
or	O
to	O
variational	O
bayes	O
em	O
to	O
ﬁt	O
more	O
complex	O
models	O
.	O
12.3	O
choosing	O
the	O
number	O
of	O
latent	B
dimensions	O
in	O
section	O
11.5	O
,	O
we	O
discussed	O
how	O
to	O
choose	O
the	O
number	O
of	O
components	O
k	O
in	O
a	O
mixture	B
model	I
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
how	O
to	O
choose	O
the	O
number	O
of	O
latent	B
dimensions	O
l	O
in	O
a	O
fa/pca	O
model	O
.	O
12.3.1	O
model	B
selection	I
for	O
fa/ppca	O
=	O
argmaxl	O
p	O
(	O
l|d	O
)	O
.	O
however	O
,	O
if	O
we	O
use	O
a	O
probabilistic	O
model	O
,	O
we	O
can	O
in	O
principle	O
compute	O
l∗	O
there	O
are	O
two	O
problems	O
with	O
this	O
.	O
first	O
,	O
evaluating	O
the	O
marginal	B
likelihood	I
for	O
lvms	O
is	O
quite	O
difficult	O
.	O
lower	O
bounds	O
(	O
see	O
section	O
21.5	O
)	O
,	O
can	O
be	O
used	O
(	O
see	O
also	O
(	O
minka	O
2000a	O
)	O
)	O
.	O
alternatively	O
,	O
we	O
can	O
use	O
the	O
cross-validated	O
likelihood	B
as	O
a	O
performance	O
measure	O
,	O
although	O
this	O
can	O
be	O
slow	O
,	O
since	O
it	O
requires	O
ﬁtting	O
each	O
model	O
f	O
times	O
,	O
where	O
f	O
is	O
the	O
number	O
of	O
cv	O
folds	B
.	O
in	O
practice	O
,	O
simple	O
approximations	O
,	O
such	O
as	O
bic	O
or	O
variational	O
the	O
second	O
issue	O
is	O
the	O
need	O
to	O
search	O
over	O
a	O
potentially	O
large	O
number	O
of	O
models	O
.	O
the	O
usual	O
approach	O
is	O
to	O
perform	O
exhaustive	O
search	O
over	O
all	O
candidate	O
values	O
of	O
l.	O
however	O
,	O
sometimes	O
we	O
can	O
set	O
the	O
model	O
to	O
its	O
maximal	O
size	O
,	O
and	O
then	O
use	O
a	O
technique	O
called	O
automatic	B
relevancy	I
determination	I
(	O
section	O
13.7	O
)	O
,	O
combined	O
with	O
em	O
,	O
to	O
automatically	O
prune	O
out	O
irrelevant	O
weights	O
.	O
12.3.	O
choosing	O
the	O
number	O
of	O
latent	B
dimensions	O
399	O
number	O
of	O
points	O
per	O
cluster	O
1	O
intrinsic	O
dimensionalities	O
7	O
4	O
3	O
2	O
8	O
8	O
16	O
32	O
64	O
128	O
1	O
1	O
1	O
1	O
1	O
2	O
6	O
7	O
7	O
1	O
2	O
4	O
3	O
4	O
4	O
3	O
3	O
3	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
figure	O
12.13	O
we	O
show	O
the	O
estimated	O
number	O
of	O
clusters	B
,	O
and	O
their	O
estimated	O
dimensionalities	O
,	O
as	O
a	O
function	O
of	O
sample	O
size	O
.	O
the	O
vbem	O
algorithm	O
found	O
two	O
different	O
solutions	O
when	O
n	O
=	O
8.	O
note	O
that	O
more	O
clusters	B
,	O
with	O
larger	O
effective	O
dimensionalities	O
,	O
are	O
discovered	O
as	O
the	O
sample	O
sizes	O
increases	O
.	O
source	O
:	O
table	O
4.1	O
of	O
(	O
beal	O
2003	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
matt	O
beal	O
.	O
this	O
technique	O
will	O
be	O
described	O
in	O
a	O
supervised	O
context	O
in	O
chapter	O
13	O
,	O
but	O
can	O
be	O
adapted	O
to	O
the	O
(	O
m	O
)	O
fa	O
context	O
as	O
shown	O
in	O
(	O
bishop	O
1999	O
;	O
ghahramani	O
and	O
beal	O
2000	O
)	O
.	O
figure	O
12.12	O
illustrates	O
this	O
approach	O
applied	O
to	O
a	O
mixture	O
of	O
fas	O
ﬁt	O
to	O
a	O
small	O
synthetic	O
dataset	O
.	O
the	O
ﬁgures	O
visualize	O
the	O
weight	O
matrices	O
for	O
each	O
cluster	O
,	O
using	O
hinton	O
diagrams	O
,	O
where	O
where	O
the	O
size	O
of	O
the	O
square	O
is	O
proportional	O
to	O
the	O
value	O
of	O
the	O
entry	O
in	O
the	O
matrix.2	O
we	O
see	O
that	O
many	O
of	O
them	O
are	O
sparse	B
.	O
figure	O
12.13	O
shows	O
that	O
the	O
degree	B
of	O
sparsity	B
depends	O
on	O
the	O
amount	O
of	O
training	O
data	O
,	O
in	O
accord	O
with	O
the	O
bayesian	O
occam	O
’	O
s	O
razor	O
.	O
in	O
particular	O
,	O
when	O
the	O
sample	O
size	O
is	O
small	O
,	O
the	O
method	O
automatically	O
prefers	O
simpler	O
models	O
,	O
but	O
as	O
the	O
sample	O
size	O
gets	O
sufficiently	O
large	O
,	O
the	O
method	O
converges	O
on	O
the	O
“	O
correct	O
”	O
solution	O
,	O
which	O
is	O
one	O
with	O
6	O
subspaces	O
of	O
dimensionality	O
1	O
,	O
2	O
,	O
2	O
,	O
3	O
,	O
4	O
and	O
7.	O
although	O
the	O
ard/	O
em	O
method	O
is	O
elegant	O
,	O
it	O
still	O
needs	O
to	O
perform	O
search	O
over	O
k.	O
this	O
is	O
done	O
using	O
“	O
birth	O
”	O
and	O
“	O
death	O
”	O
moves	O
(	O
ghahramani	O
and	O
beal	O
2000	O
)	O
.	O
an	O
alternative	O
approach	O
is	O
to	O
perform	O
stochastic	O
sampling	O
in	O
the	O
space	O
of	O
models	O
.	O
traditional	O
approaches	O
,	O
such	O
as	O
(	O
lopes	O
and	O
west	O
2004	O
)	O
,	O
are	O
based	O
on	O
reversible	O
jump	O
mcmc	O
,	O
and	O
also	O
use	O
birth	O
and	O
death	B
moves	I
.	O
however	O
,	O
this	O
can	O
be	O
slow	O
and	O
difficult	O
to	O
implement	O
.	O
more	O
recent	O
approaches	O
use	O
non-parametric	O
priors	O
,	O
combined	O
with	O
gibbs	O
sampling	O
,	O
see	O
e.g.	O
,	O
(	O
paisley	O
and	O
carin	O
2009	O
)	O
.	O
12.3.2	O
model	B
selection	I
for	O
pca	O
since	O
pca	O
is	O
not	O
a	O
probabilistic	O
model	O
,	O
we	O
can	O
not	O
use	O
any	O
of	O
the	O
methods	O
described	O
above	O
.	O
an	O
obvious	O
proxy	O
for	O
the	O
likelihood	B
is	O
the	O
reconstruction	B
error	I
:	O
||xi	O
−	O
ˆxi||2	O
(	O
12.67	O
)	O
e	O
(	O
d	O
,	O
l	O
)	O
=	O
1|d|	O
(	O
cid:2	O
)	O
i∈d	O
in	O
the	O
case	O
of	O
pca	O
,	O
the	O
reconstruction	O
is	O
given	O
by	O
by	O
ˆxi	O
=	O
wzi	O
+	O
μ	O
,	O
where	O
zi	O
=	O
wt	O
(	O
xi	O
−	O
μ	O
)	O
and	O
w	O
and	O
μ	O
are	O
estimated	O
from	O
dtrain	O
.	O
2.	O
geoff	O
hinton	O
is	O
an	O
english	O
professor	O
of	O
computer	O
science	O
at	O
the	O
university	O
of	O
toronto	O
.	O
400	O
chapter	O
12.	O
latent	B
linear	O
models	O
train	O
set	O
reconstruction	B
error	I
test	O
set	O
reconstruction	B
error	I
60	O
50	O
40	O
e	O
s	O
m	O
r	O
30	O
20	O
10	O
0	O
0	O
100	O
200	O
300	O
400	O
500	O
num	O
pcs	O
(	O
a	O
)	O
60	O
50	O
40	O
e	O
s	O
m	O
r	O
30	O
20	O
10	O
0	O
0	O
100	O
200	O
300	O
400	O
500	O
num	O
pcs	O
(	O
b	O
)	O
figure	O
12.14	O
reconstruction	B
error	I
on	O
mnist	O
vs	O
number	O
of	O
latent	B
dimensions	O
used	O
by	O
pca	O
.	O
(	O
a	O
)	O
training	B
set	I
.	O
(	O
b	O
)	O
test	O
set	O
.	O
figure	O
generated	O
by	O
pcaoverfitdemo	O
.	O
figure	O
12.14	O
(	O
a	O
)	O
plots	O
e	O
(	O
dtrain	O
,	O
l	O
)	O
vs	O
l	O
on	O
the	O
mnist	O
training	O
data	O
in	O
figure	O
12.6.	O
we	O
see	O
that	O
it	O
drops	O
off	O
quite	O
quickly	O
,	O
indicating	O
that	O
we	O
can	O
capture	O
most	O
of	O
the	O
empirical	O
correlation	O
of	O
the	O
pixels	O
with	O
a	O
small	O
number	O
of	O
factors	B
,	O
as	O
illustrated	O
qualitatively	O
in	O
figure	O
12.6.	O
exercise	O
12.5	O
asks	O
you	O
to	O
prove	O
that	O
the	O
residual	B
error	I
from	O
only	O
using	O
l	O
terms	O
is	O
given	O
by	O
the	O
sum	O
of	O
the	O
discarded	O
eigenvalues	O
:	O
λj	O
(	O
12.68	O
)	O
therefore	O
an	O
alternative	O
to	O
plotting	O
the	O
error	O
is	O
to	O
plot	O
the	O
retained	O
eigenvalues	O
,	O
in	O
decreasing	O
order	O
.	O
this	O
is	O
called	O
a	O
scree	B
plot	I
,	O
because	O
“	O
the	O
plot	O
looks	O
like	O
the	O
side	O
of	O
a	O
mountain	O
,	O
and	O
’	O
scree	O
’	O
refers	O
to	O
the	O
debris	O
fallen	O
from	O
a	O
mountain	O
and	O
lying	O
at	O
its	O
base	O
”	O
.3	O
this	O
will	O
have	O
the	O
same	O
shape	O
as	O
the	O
residual	B
error	I
plot	O
.	O
a	O
related	O
quantity	O
is	O
the	O
fraction	B
of	I
variance	I
explained	I
,	O
deﬁned	O
as	O
e	O
(	O
dtrain	O
,	O
l	O
)	O
=	O
d	O
(	O
cid:2	O
)	O
j=l+1	O
f	O
(	O
dtrain	O
,	O
l	O
)	O
=	O
(	O
cid:10	O
)	O
l	O
(	O
cid:10	O
)	O
lmax	O
j=1	O
λj	O
j	O
(	O
cid:2	O
)	O
=1	O
λj	O
(	O
cid:2	O
)	O
(	O
12.69	O
)	O
this	O
captures	O
the	O
same	O
information	B
as	O
the	O
scree	B
plot	I
.	O
of	O
course	O
,	O
if	O
we	O
use	O
l	O
=	O
rank	O
(	O
x	O
)	O
,	O
we	O
get	O
zero	O
reconstruction	O
error	O
on	O
the	O
training	B
set	I
.	O
to	O
avoid	O
overﬁtting	B
,	O
it	O
is	O
natural	O
to	O
plot	O
reconstruction	B
error	I
on	O
the	O
test	O
set	O
.	O
this	O
is	O
shown	O
in	O
figure	O
12.14	O
(	O
b	O
)	O
.	O
here	O
we	O
see	O
that	O
the	O
error	O
continues	O
to	O
go	O
down	O
even	O
as	O
the	O
model	O
becomes	O
more	O
complex	O
!	O
thus	O
we	O
do	O
not	O
get	O
the	O
usual	O
u-shaped	O
curve	O
that	O
we	O
typically	O
expect	O
to	O
see	O
.	O
what	O
is	O
going	O
on	O
?	O
the	O
problem	O
is	O
that	O
pca	O
is	O
not	O
a	O
proper	O
generative	O
model	O
of	O
the	O
data	O
.	O
if	O
you	O
give	O
it	O
more	O
latent	B
dimensions	O
,	O
it	O
will	O
be	O
able	O
to	O
it	O
is	O
merely	O
a	O
compression	O
technique	O
.	O
approximate	O
the	O
test	O
data	O
more	O
accurately	O
.	O
by	O
contrast	O
,	O
a	O
probabilistic	O
model	O
enjoys	O
a	O
bayesian	O
occam	O
’	O
s	O
razor	O
effect	O
(	O
section	O
5.3.1	O
)	O
,	O
in	O
that	O
it	O
gets	O
“	O
punished	O
”	O
if	O
it	O
wastes	O
probability	O
mass	O
on	O
parts	O
of	O
the	O
space	O
where	O
there	O
is	O
little	O
data	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
12.15	O
,	O
which	O
plots	O
the	O
3.	O
quotation	O
from	O
http	O
:	O
//janda.org/workshop/factoranalysis/spssrun/spss08.htm	O
.	O
12.3.	O
choosing	O
the	O
number	O
of	O
latent	B
dimensions	O
401	O
x	O
106	O
train	O
set	O
negative	O
loglik	O
2.2	O
2.1	O
2	O
1.9	O
k	O
i	O
l	O
l	O
g	O
o	O
g	O
e	O
n	O
1.8	O
1.7	O
1.6	O
1.5	O
1.4	O
1.3	O
0	O
100	O
200	O
300	O
400	O
500	O
num	O
pcs	O
(	O
a	O
)	O
x	O
106	O
test	O
set	O
negative	O
loglik	O
2.5	O
2.4	O
2.3	O
k	O
i	O
l	O
l	O
g	O
o	O
g	O
e	O
n	O
2.2	O
2.1	O
2	O
1.9	O
1.8	O
0	O
100	O
200	O
300	O
400	O
500	O
num	O
pcs	O
(	O
b	O
)	O
figure	O
12.15	O
negative	B
log	I
likelihood	I
on	O
mnist	O
vs	O
number	O
of	O
latent	B
dimensions	O
used	O
by	O
ppca	O
.	O
(	O
a	O
)	O
training	B
set	I
.	O
(	O
b	O
)	O
test	O
set	O
.	O
figure	O
generated	O
by	O
pcaoverfitdemo	O
.	O
negative	B
log	I
likelihood	I
,	O
computed	O
using	O
ppca	O
,	O
vs	O
l.	O
here	O
,	O
on	O
the	O
test	O
set	O
,	O
we	O
see	O
the	O
usual	O
u-shaped	O
curve	O
.	O
these	O
results	O
are	O
analogous	O
to	O
those	O
in	O
section	O
11.5.2	O
,	O
where	O
we	O
discussed	O
the	O
issue	O
of	O
choosing	O
k	O
in	O
the	O
k-means	O
algorithm	O
vs	O
using	O
a	O
gmm	O
.	O
12.3.2.1	O
proﬁle	O
likelihood	O
although	O
there	O
is	O
no	O
u-shape	O
,	O
there	O
is	O
sometimes	O
a	O
“	O
regime	O
change	O
”	O
in	O
the	O
plots	O
,	O
from	O
relatively	O
large	O
errors	O
to	O
relatively	O
small	O
.	O
one	O
way	O
to	O
automate	O
the	O
detection	O
of	O
this	O
is	O
described	O
in	O
(	O
zhu	O
and	O
ghodsi	O
2006	O
)	O
.	O
the	O
idea	O
is	O
this	O
.	O
let	O
λk	O
be	O
some	O
measure	O
of	O
the	O
error	O
incurred	O
by	O
a	O
model	O
of	O
size	O
k	O
,	O
such	O
that	O
λ1	O
≥	O
λ2	O
≥	O
···	O
≥λ	O
lmax	O
.	O
in	O
pca	O
,	O
these	O
are	O
the	O
eigenvalues	O
,	O
but	O
the	O
method	O
can	O
also	O
be	O
applied	O
to	O
k-means	O
.	O
now	O
consider	O
partitioning	B
these	O
values	O
into	O
two	O
groups	O
,	O
depending	O
on	O
whether	O
k	O
<	O
l	O
or	O
k	O
>	O
l	O
,	O
where	O
l	O
is	O
some	O
threshold	O
which	O
we	O
will	O
determine	O
.	O
to	O
measure	O
the	O
quality	O
of	O
l	O
,	O
we	O
will	O
use	O
a	O
simple	O
change-point	O
model	O
,	O
where	O
λk	O
∼	O
n	O
(	O
μ1	O
,	O
σ2	O
)	O
if	O
k	O
≤	O
l	O
,	O
and	O
λk	O
∼	O
n	O
(	O
μ2	O
,	O
σ2	O
)	O
if	O
k	O
>	O
l.	O
(	O
it	O
is	O
important	O
that	O
σ2	O
be	O
the	O
same	O
in	O
both	O
models	O
,	O
to	O
prevent	O
overﬁtting	B
in	O
the	O
case	O
where	O
one	O
regime	O
has	O
less	O
data	O
than	O
the	O
other	O
.	O
)	O
within	O
each	O
of	O
the	O
two	O
regimes	O
,	O
we	O
assume	O
the	O
λk	O
are	O
iid	B
,	O
which	O
is	O
obviously	O
incorrect	O
,	O
but	O
is	O
adequate	O
for	O
our	O
present	O
purposes	O
.	O
we	O
can	O
ﬁt	O
this	O
model	O
for	O
each	O
l	O
=	O
1	O
:	O
lmax	O
by	O
partitioning	B
the	O
data	O
and	O
computing	O
the	O
mles	O
,	O
using	O
a	O
pooled	B
estimate	O
of	O
the	O
variance	B
:	O
(	O
cid:10	O
)	O
(	O
cid:10	O
)	O
,	O
μ2	O
(	O
l	O
)	O
=	O
k≤l	O
λk	O
l	O
k≤l	O
(	O
λk	O
−	O
μ1	O
(	O
l	O
)	O
)	O
2	O
+	O
(	O
cid:10	O
)	O
k	O
>	O
l	O
λk	O
(	O
cid:10	O
)	O
n	O
−	O
l	O
n	O
k	O
(	O
cid:2	O
)	O
μ1	O
(	O
l	O
)	O
=	O
σ2	O
(	O
l	O
)	O
=	O
l	O
(	O
cid:2	O
)	O
k	O
>	O
l	O
(	O
λk	O
−	O
μ2	O
(	O
l	O
)	O
)	O
2	O
we	O
can	O
then	O
evaluate	O
the	O
proﬁle	B
log	I
likelihood	I
(	O
cid:6	O
)	O
(	O
l	O
)	O
=	O
log	O
n	O
(	O
λk|μ1	O
(	O
l	O
)	O
,	O
σ2	O
(	O
l	O
)	O
)	O
+	O
log	O
n	O
(	O
λk|μ2	O
(	O
l	O
)	O
,	O
σ2	O
(	O
l	O
)	O
)	O
k=1	O
k=l+1	O
finally	O
,	O
we	O
choose	O
l∗	O
=	O
arg	O
max	O
(	O
cid:6	O
)	O
(	O
l	O
)	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
12.16.	O
on	O
the	O
left	O
,	O
we	O
plot	O
the	O
scree	B
plot	I
,	O
which	O
has	O
the	O
same	O
shape	O
as	O
in	O
figure	O
12.14	O
(	O
a	O
)	O
.	O
on	O
the	O
right	O
,	O
we	O
plot	O
the	O
proﬁle	O
(	O
12.70	O
)	O
(	O
12.71	O
)	O
(	O
12.72	O
)	O
402	O
chapter	O
12.	O
latent	B
linear	O
models	O
l	O
e	O
u	O
a	O
v	O
n	O
e	O
g	O
e	O
i	O
x	O
105	O
4	O
scree	B
plot	I
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
0	O
10	O
20	O
30	O
num	O
pcs	O
40	O
50	O
(	O
a	O
)	O
d	O
o	O
o	O
h	O
i	O
l	O
e	O
k	O
i	O
l	O
g	O
o	O
l	O
e	O
l	O
i	O
f	O
o	O
r	O
p	O
−5450	O
−5500	O
−5550	O
−5600	O
−5650	O
−5700	O
−5750	O
0	O
10	O
20	O
30	O
40	O
50	O
num	O
pcs	O
(	O
b	O
)	O
figure	O
12.16	O
(	O
a	O
)	O
scree	B
plot	I
for	O
training	B
set	I
,	O
corresponding	O
to	O
figure	O
12.14	O
(	O
a	O
)	O
.	O
generated	O
by	O
pcaoverfitdemo	O
.	O
(	O
b	O
)	O
proﬁle	O
likelihood	O
.	O
figure	O
likelihood	B
.	O
rather	O
miraculously	O
,	O
we	O
see	O
a	O
fairly	O
well-determined	O
peak	O
.	O
12.4	O
pca	O
for	O
categorical	B
data	O
in	O
this	O
section	O
,	O
we	O
consider	O
extending	O
the	O
factor	B
analysis	I
model	O
to	O
the	O
case	O
where	O
the	O
observed	O
data	O
is	O
categorical	B
rather	O
than	O
real-valued	O
.	O
that	O
is	O
,	O
the	O
data	O
has	O
the	O
form	O
yij	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
}	O
,	O
where	O
j	O
=	O
1	O
:	O
r	O
is	O
the	O
number	O
of	O
observed	O
response	O
variables	O
.	O
we	O
assume	O
each	O
yij	O
is	O
generated	O
from	O
a	O
latent	O
variable	O
zi	O
∈	O
r	O
l	O
,	O
with	O
a	O
gaussian	O
prior	O
,	O
which	O
is	O
passed	O
through	O
the	O
softmax	B
function	O
as	O
follows	O
:	O
p	O
(	O
zi	O
)	O
=n	O
(	O
0	O
,	O
i	O
)	O
r	O
(	O
cid:27	O
)	O
(	O
12.73	O
)	O
(	O
12.74	O
)	O
p	O
(	O
yi|zi	O
,	O
θ	O
)	O
=	O
cat	O
(	O
yir|s	O
(	O
wt	O
r	O
zi	O
+	O
w0r	O
)	O
)	O
r=1	O
l×m	O
is	O
the	O
factor	B
loading	I
matrix	I
for	O
response	O
j	O
,	O
and	O
w0r	O
∈	O
r	O
where	O
wr	O
∈	O
r	O
m	O
is	O
the	O
offset	O
term	O
for	O
response	O
r	O
,	O
and	O
θ	O
=	O
(	O
wr	O
,	O
w0r	O
)	O
r	O
r=1	O
.	O
(	O
we	O
need	O
an	O
explicit	O
offset	O
term	O
,	O
since	O
clamping	B
one	O
element	O
of	O
zi	O
to	O
1	O
can	O
cause	O
problems	O
when	O
computing	O
the	O
posterior	O
covariance	O
.	O
)	O
as	O
in	O
factor	B
analysis	I
,	O
we	O
have	O
deﬁned	O
the	O
prior	O
mean	B
to	O
be	O
m0	O
=	O
0	O
and	O
the	O
prior	O
covariance	B
v0	O
=	O
i	O
,	O
since	O
we	O
can	O
capture	O
non-zero	O
mean	B
by	O
changing	O
w0j	O
and	O
non-identity	O
covariance	B
by	O
changing	O
wr	O
.	O
we	O
will	O
call	O
this	O
categorical	B
pca	O
.	O
see	O
chapter	O
27	O
for	O
a	O
discussion	O
of	O
related	O
models	O
.	O
it	O
is	O
interesting	O
to	O
study	O
what	O
kinds	O
of	O
distributions	O
we	O
can	O
induce	O
on	O
the	O
observed	O
variables	O
by	O
varying	O
the	O
parameters	O
.	O
for	O
simplicity	O
,	O
we	O
assume	O
there	O
is	O
a	O
single	O
ternary	O
response	B
variable	I
,	O
so	O
yi	O
lives	O
in	O
the	O
3d	O
probability	B
simplex	I
.	O
figure	O
12.17	O
shows	O
what	O
happens	O
when	O
we	O
vary	O
the	O
parameters	O
of	O
the	O
prior	O
,	O
m0	O
and	O
v0	O
,	O
which	O
is	O
equivalent	O
to	O
varying	O
the	O
parameters	O
of	O
the	O
likelihood	B
,	O
w1	O
and	O
w01	O
.	O
we	O
see	O
that	O
this	O
can	O
deﬁne	O
fairly	O
complex	O
distributions	O
over	O
the	O
simplex	O
.	O
this	O
induced	O
distribution	O
is	O
known	O
as	O
the	O
logistic	B
normal	I
distribution	O
(	O
aitchison	O
1982	O
)	O
.	O
we	O
can	O
ﬁt	O
this	O
model	O
to	O
data	O
using	O
a	O
modiﬁed	O
version	O
of	O
em	O
.	O
the	O
basic	O
idea	O
is	O
to	O
infer	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
p	O
(	O
zi|yi	O
,	O
θ	O
)	O
in	O
the	O
e	O
step	O
,	O
and	O
then	O
to	O
maximize	O
θ	O
in	O
the	O
m	O
step	O
.	O
the	O
details	O
for	O
the	O
multiclass	O
case	O
,	O
can	O
be	O
found	O
in	O
(	O
khan	O
et	O
al	O
.	O
2010	O
)	O
(	O
see	O
12.4.	O
pca	O
for	O
categorical	B
data	O
403	O
some	O
examples	O
of	O
the	O
logistic	B
normal	I
distribution	O
deﬁned	O
on	O
the	O
3d	O
simplex	O
.	O
figure	O
12.17	O
(	O
a	O
)	O
diagonal	O
covariance	O
and	O
non-zero	O
mean	B
.	O
(	O
c	O
)	O
positive	O
correlation	O
between	O
states	O
1	O
and	O
2.	O
source	O
:	O
figure	O
1	O
of	O
(	O
blei	O
and	O
lafferty	O
2007	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
david	O
blei	O
.	O
(	O
b	O
)	O
negative	O
correlation	O
between	O
states	O
1	O
and	O
2	O
.	O
20	O
40	O
60	O
80	O
100	O
120	O
140	O
2	O
4	O
6	O
8	O
10	O
12	O
14	O
16	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−1.5	O
−1	O
−0.5	O
0	O
0.5	O
1	O
1.5	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
12.18	O
left	O
:	O
150	O
synthetic	O
16	O
dimensional	O
bit	O
vectors	O
.	O
right	O
:	O
the	O
2d	O
embedding	B
learned	O
by	O
binary	O
pca	O
,	O
using	O
variational	O
em	O
.	O
we	O
have	O
color	O
coded	O
points	O
by	O
the	O
identity	O
of	O
the	O
true	O
“	O
prototype	B
”	O
that	O
generated	O
them	O
.	O
figure	O
generated	O
by	O
binaryfademotipping	O
.	O
also	O
section	O
21.8.1.1	O
)	O
.	O
the	O
details	O
for	O
the	O
binary	O
case	O
for	O
the	O
the	O
sigmoid	B
link	O
can	O
be	O
found	O
in	O
exercise	O
21.9	O
,	O
and	O
for	O
the	O
probit	B
link	O
in	O
exercise	O
21.10.	O
one	O
application	O
of	O
such	O
a	O
model	O
is	O
to	O
visualize	O
high	O
dimensional	O
categorical	B
data	O
.	O
fig-	O
ure	O
12.18	O
(	O
a	O
)	O
shows	O
a	O
simple	O
example	O
where	O
we	O
have	O
150	O
6-dimensional	O
bit	O
vectors	O
.	O
it	O
is	O
clear	O
that	O
each	O
sample	O
is	O
just	O
a	O
noisy	O
copy	O
of	O
one	O
of	O
three	O
binary	O
prototypes	O
.	O
we	O
ﬁt	O
a	O
2d	O
catfa	O
to	O
this	O
model	O
,	O
yielding	O
approximate	O
mles	O
ˆθ	O
.	O
in	O
figure	O
12.18	O
(	O
b	O
)	O
,	O
we	O
plot	O
e	O
.	O
we	O
see	O
that	O
there	O
are	O
three	O
distinct	O
clusters	B
,	O
as	O
is	O
to	O
be	O
expected	O
.	O
,	O
+	O
zi|xi	O
,	O
ˆθ	O
in	O
(	O
khan	O
et	O
al	O
.	O
2010	O
)	O
,	O
we	O
show	O
that	O
this	O
model	O
outperforms	O
ﬁnite	O
mixture	O
models	O
on	O
the	O
task	O
of	O
imputing	O
missing	B
entries	O
in	O
design	O
matrices	O
consisting	O
of	O
real	O
and	O
categorical	B
data	O
.	O
this	O
is	O
useful	O
for	O
analysing	O
social	O
science	O
survey	O
data	O
,	O
which	O
often	O
has	O
missing	B
data	I
and	O
variables	O
of	O
mixed	O
type	O
.	O
404	O
chapter	O
12.	O
latent	B
linear	O
models	O
wy	O
w	O
x	O
w	O
x	O
w	O
y	O
zi	O
zx	O
i	O
zs	O
i	O
yi	O
xi	O
n	O
bx	O
(	O
a	O
)	O
yi	O
n	O
xi	O
(	O
b	O
)	O
w	O
x	O
w	O
y	O
zx	O
i	O
zs	O
i	O
zy	O
i	O
bx	O
xi	O
yi	O
n	O
by	O
(	O
c	O
)	O
figure	O
12.19	O
gaussian	O
latent	B
factor	O
models	O
for	O
paired	O
data	O
.	O
(	O
c	O
)	O
canonical	O
correlation	O
analysis	O
.	O
(	O
a	O
)	O
supervised	O
pca	O
.	O
(	O
b	O
)	O
partial	B
least	I
squares	I
.	O
12.5	O
pca	O
for	O
paired	O
and	O
multi-view	O
data	O
it	O
is	O
common	O
to	O
have	O
a	O
pair	O
of	O
related	O
datasets	O
,	O
e.g.	O
,	O
gene	O
expression	O
and	O
gene	O
copy	O
number	O
,	O
or	O
movie	O
ratings	O
by	O
users	O
and	O
movie	O
reviews	O
.	O
it	O
is	O
natural	O
to	O
want	O
to	O
combine	O
these	O
together	O
into	O
a	O
low-dimensional	O
embedding	B
.	O
this	O
is	O
an	O
example	O
of	O
data	B
fusion	I
.	O
in	O
some	O
cases	O
,	O
we	O
might	O
want	O
to	O
predict	O
one	O
element	O
of	O
the	O
pair	O
,	O
say	O
xi1	O
,	O
from	O
the	O
other	O
one	O
,	O
xi2	O
,	O
via	O
the	O
low-dimensional	O
“	O
bottleneck	B
”	O
.	O
below	O
we	O
discuss	O
various	O
latent	B
gaussian	O
models	O
for	O
these	O
tasks	O
,	O
following	O
the	O
presentation	O
of	O
(	O
virtanen	O
2010	O
)	O
.	O
the	O
models	O
easily	O
generalize	B
from	O
pairs	O
to	O
sets	O
of	O
data	O
,	O
xim	O
,	O
for	O
m	O
=	O
1	O
:	O
m	O
.	O
we	O
focus	O
on	O
the	O
case	O
where	O
xim	O
∈	O
r	O
in	O
this	O
case	O
,	O
the	O
joint	B
distribution	I
is	O
multivariate	O
gaussian	O
,	O
so	O
we	O
can	O
easily	O
ﬁt	O
the	O
models	O
using	O
em	O
,	O
or	O
gibbs	O
sampling	O
.	O
dm	O
.	O
we	O
can	O
generalize	B
the	O
models	O
to	O
handle	O
discrete	B
and	O
count	O
data	O
by	O
using	O
the	O
exponential	B
family	I
as	O
a	O
response	O
distribution	O
instead	O
of	O
the	O
gaussian	O
,	O
as	O
we	O
explain	O
in	O
section	O
27.2.2.	O
however	O
,	O
this	O
will	O
require	O
the	O
use	O
of	O
approximate	B
inference	I
in	O
the	O
e	O
step	O
(	O
or	O
an	O
analogous	O
modiﬁcation	O
to	O
mcmc	O
)	O
.	O
12.5.	O
pca	O
for	O
paired	O
and	O
multi-view	O
data	O
12.5.1	O
supervised	O
pca	O
(	O
latent	B
factor	O
regression	B
)	O
consider	O
the	O
following	O
model	O
,	O
illustrated	O
in	O
figure	O
12.19	O
(	O
a	O
)	O
:	O
p	O
(	O
zi	O
)	O
=n	O
(	O
0	O
,	O
il	O
)	O
p	O
(	O
yi|zi	O
)	O
=n	O
(	O
wt	O
y	O
zi	O
+	O
μy	O
,	O
σ2	O
y	O
)	O
p	O
(	O
xi|zi	O
)	O
=n	O
(	O
wxzi	O
+	O
μx	O
,	O
σ2	O
xid	O
)	O
405	O
(	O
12.75	O
)	O
(	O
12.76	O
)	O
(	O
12.77	O
)	O
in	O
(	O
yu	O
et	O
al	O
.	O
2006	O
)	O
,	O
this	O
is	O
called	O
supervised	O
pca	O
.	O
in	O
(	O
west	O
2003	O
)	O
,	O
this	O
is	O
called	O
bayesian	O
factor	B
regression	O
.	O
this	O
model	O
is	O
like	O
pca	O
,	O
except	O
that	O
the	O
target	O
variable	O
yi	O
is	O
taken	O
into	O
account	O
when	O
learning	B
the	O
low	O
dimensional	O
embedding	B
.	O
since	O
the	O
model	O
is	O
jointly	O
gaussian	O
,	O
we	O
have	O
yi|xi	O
∼	O
n	O
(	O
xt	O
i	O
w	O
,	O
σ2	O
y	O
+	O
wt	O
y	O
cwy	O
)	O
(	O
12.78	O
)	O
x	O
ψ−1wx	O
.	O
so	O
although	O
this	O
is	O
a	O
g	O
σ2	O
)	O
,	O
j	O
)	O
contains	O
where	O
w	O
=	O
ψ−1wxcwy	O
,	O
ψ	O
=	O
σ2	O
joint	O
density	O
model	O
of	O
(	O
yi	O
,	O
xi	O
)	O
,	O
we	O
can	O
infer	O
the	O
implied	O
conditional	O
distribution	O
.	O
xid	O
,	O
and	O
c−1	O
=	O
i	O
+	O
wt	O
we	O
now	O
show	O
an	O
interesting	O
connection	O
to	O
zellner	O
’	O
s	O
g-prior	B
.	O
suppose	O
p	O
(	O
wy	O
)	O
=	O
n	O
(	O
0	O
,	O
1	O
and	O
let	O
x	O
=	O
rvt	O
be	O
the	O
svd	O
of	O
x	O
,	O
where	O
vt	O
v	O
=	O
i	O
and	O
rt	O
r	O
=	O
σ2	O
=	O
diag	O
(	O
σ2	O
the	O
squared	O
singular	O
values	O
.	O
then	O
one	O
can	O
show	O
(	O
west	O
2003	O
)	O
that	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
0	O
,	O
gv−t	O
σ−2v−1	O
)	O
=	O
n	O
(	O
0	O
,	O
g	O
(	O
xt	O
x	O
)	O
−1	O
)	O
(	O
12.79	O
)	O
so	O
the	O
dependence	O
of	O
the	O
prior	O
for	O
w	O
on	O
x	O
arises	O
from	O
the	O
fact	O
that	O
w	O
is	O
derived	O
indirectly	O
by	O
a	O
joint	O
model	O
of	O
x	O
and	O
y.	O
the	O
above	O
discussion	O
focussed	O
on	O
regression	B
.	O
(	O
guo	O
2009	O
)	O
generalizes	O
cca	O
to	O
the	O
exponential	B
family	I
,	O
which	O
is	O
more	O
appropriate	O
if	O
xi	O
and/or	O
yi	O
are	O
discrete	B
.	O
although	O
we	O
can	O
no	O
longer	O
compute	O
the	O
conditional	O
p	O
(	O
yi|xi	O
,	O
θ	O
)	O
in	O
closed	O
form	O
,	O
the	O
model	O
has	O
a	O
similar	B
interpretation	O
to	O
the	O
regression	B
case	O
,	O
namely	O
that	O
we	O
are	O
predicting	O
the	O
response	O
via	O
a	O
latent	B
“	O
bottleneck	B
”	O
.	O
in	O
particular	O
,	O
we	O
might	O
want	O
to	O
ﬁnd	O
an	O
encoding	O
distribution	O
p	O
(	O
z|x	O
)	O
such	O
that	O
we	O
minimize	O
the	O
basic	O
idea	O
of	O
compressing	O
xi	O
to	O
predict	O
yi	O
can	O
be	O
formulated	O
using	O
information	B
theory	I
.	O
i	O
(	O
x	O
;	O
z	O
)	O
−	O
βi	O
(	O
x	O
;	O
y	O
)	O
(	O
12.80	O
)	O
where	O
β	O
≥	O
0	O
is	O
some	O
parameter	B
controlling	O
the	O
tradeoff	O
between	O
compression	O
and	O
predictive	B
accuracy	O
.	O
this	O
is	O
known	O
as	O
the	O
information	B
bottleneck	I
(	O
tishby	O
et	O
al	O
.	O
1999	O
)	O
.	O
often	O
z	O
is	O
taken	O
to	O
be	O
discrete	B
,	O
as	O
in	O
clustering	B
.	O
however	O
,	O
in	O
the	O
gaussian	O
case	O
,	O
ib	O
is	O
closely	O
related	O
to	O
cca	O
(	O
chechik	O
et	O
al	O
.	O
2005	O
)	O
.	O
we	O
can	O
easily	O
generalize	B
cca	O
to	O
the	O
case	O
where	O
yi	O
is	O
a	O
vector	O
of	O
responses	O
to	O
be	O
predicted	O
,	O
as	O
in	O
multi-label	B
classiﬁcation	I
.	O
(	O
ma	O
et	O
al	O
.	O
2008	O
;	O
williamson	O
and	O
ghahramani	O
2008	O
)	O
used	O
this	O
model	O
to	O
perform	O
collaborative	B
ﬁltering	I
,	O
where	O
the	O
goal	O
is	O
to	O
predict	O
yij	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
5	O
}	O
,	O
the	O
rating	O
person	O
i	O
gives	O
to	O
movie	O
j	O
,	O
where	O
the	O
“	O
side	B
information	I
”	O
xi	O
takes	O
the	O
form	O
of	O
a	O
list	O
of	O
i	O
’	O
s	O
friends	O
.	O
the	O
intuition	O
behind	O
this	O
approach	O
is	O
that	O
knowledge	O
of	O
who	O
your	O
friends	O
are	O
,	O
as	O
well	O
as	O
the	O
ratings	O
of	O
all	O
other	O
users	O
,	O
should	O
help	O
predict	O
which	O
movies	O
you	O
will	O
like	O
.	O
in	O
general	O
,	O
any	O
setting	O
where	O
the	O
tasks	O
are	O
correlated	O
could	O
beneﬁt	O
from	O
cca	O
.	O
once	O
we	O
adopt	O
a	O
probabilistic	O
view	O
,	O
various	O
extensions	O
are	O
straightforward	O
.	O
for	O
example	O
,	O
we	O
can	O
easily	O
generalize	B
to	O
the	O
semi-supervised	B
case	O
,	O
where	O
we	O
do	O
not	O
observe	O
yi	O
for	O
all	O
i	O
(	O
yu	O
et	O
al	O
.	O
2006	O
)	O
.	O
406	O
chapter	O
12.	O
latent	B
linear	O
models	O
12.5.1.1	O
discriminative	B
supervised	O
pca	O
one	O
problem	O
with	O
this	O
model	O
is	O
that	O
it	O
puts	O
as	O
much	O
weight	O
on	O
predicting	O
the	O
inputs	O
xi	O
as	O
the	O
outputs	O
yi	O
.	O
this	O
can	O
be	O
partially	O
alleviated	O
by	O
using	O
a	O
weighted	O
objective	O
of	O
the	O
following	O
form	O
(	O
rish	O
et	O
al	O
.	O
2008	O
)	O
:	O
(	O
cid:6	O
)	O
(	O
θ	O
)	O
=	O
p	O
(	O
yi|ηiy	O
)	O
αy	O
p	O
(	O
xi|ηix	O
)	O
αx	O
(	O
12.81	O
)	O
(	O
cid:27	O
)	O
where	O
the	O
αm	O
control	O
the	O
relative	O
importance	O
of	O
the	O
data	O
sources	O
,	O
and	O
ηim	O
=	O
wmzi	O
.	O
for	O
gaussian	O
data	O
,	O
we	O
can	O
see	O
that	O
αm	O
just	O
controls	O
the	O
noise	O
variance	O
:	O
i	O
−	O
ηiy||2	O
)	O
(	O
cid:6	O
)	O
(	O
θ	O
)	O
∝	O
αx||xt	O
αy||yt	O
(	O
cid:27	O
)	O
(	O
12.82	O
)	O
i	O
−	O
ηix||2	O
)	O
exp	O
(	O
−	O
1	O
2	O
exp	O
(	O
−	O
1	O
2	O
i	O
i	O
this	O
interpretation	O
holds	O
more	O
generally	O
for	O
the	O
exponential	B
family	I
.	O
note	O
,	O
however	O
,	O
that	O
it	O
is	O
hard	O
to	O
estimate	O
the	O
αm	O
parameters	O
,	O
because	O
changing	O
them	O
changes	O
the	O
normalization	O
constant	O
of	O
the	O
likelihood	B
.	O
we	O
give	O
an	O
alternative	O
approach	O
to	O
weighting	O
y	O
more	O
heavily	O
below	O
.	O
12.5.2	O
partial	B
least	I
squares	I
the	O
technique	O
of	O
partial	B
least	I
squares	I
(	O
pls	O
)	O
(	O
gustafsson	O
2001	O
;	O
sun	O
et	O
al	O
.	O
2009	O
)	O
is	O
an	O
asym-	O
metric	B
or	O
more	O
“	O
discriminative	B
”	O
form	O
of	O
supervised	O
pca	O
.	O
the	O
key	O
idea	O
is	O
to	O
allow	O
some	O
of	O
the	O
(	O
co	O
)	O
variance	B
in	O
the	O
input	O
features	B
to	O
be	O
explained	O
by	O
its	O
own	O
subspace	O
,	O
zx	O
i	O
,	O
and	O
to	O
let	O
the	O
rest	O
of	O
the	O
subspace	O
,	O
zs	O
i	O
,	O
be	O
shared	B
between	O
input	O
and	O
output	O
.	O
the	O
model	O
has	O
the	O
form	O
p	O
(	O
zi	O
)	O
=n	O
(	O
zs	O
i|0	O
,	O
ils	O
)	O
n	O
(	O
zx	O
i	O
|0	O
,	O
ilx	O
)	O
p	O
(	O
yi|zi	O
)	O
=n	O
(	O
wyzs	O
p	O
(	O
xi|zi	O
)	O
=n	O
(	O
wxzs	O
i	O
+	O
μy	O
,	O
σ2idy	O
)	O
i	O
+	O
bxzx	O
i	O
+	O
μx	O
,	O
σ2idx	O
)	O
(	O
12.83	O
)	O
(	O
12.84	O
)	O
(	O
12.85	O
)	O
(	O
cid:28	O
)	O
see	O
figure	O
12.19	O
(	O
b	O
)	O
.	O
the	O
corresponding	O
induced	O
distribution	O
on	O
the	O
visible	B
variables	I
has	O
the	O
form	O
p	O
(	O
vi|θ	O
)	O
=	O
n	O
(	O
vi|wzi	O
+	O
μ	O
,	O
σ2i	O
)	O
n	O
(	O
zi|0	O
,	O
i	O
)	O
dzi	O
=	O
n	O
(	O
vi|μ	O
,	O
wwt	O
+	O
σ2i	O
)	O
(	O
12.86	O
)	O
where	O
vi	O
=	O
(	O
xi	O
;	O
yi	O
)	O
,	O
μ	O
=	O
(	O
μy	O
;	O
μx	O
)	O
and	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
0	O
wy	O
(	O
cid:8	O
)	O
wx	O
bx	O
wywt	O
y	O
wxwt	O
x	O
wxwt	O
wxwt	O
x	O
x	O
+	O
bxbt	O
x	O
w	O
=	O
wwt	O
=	O
(	O
cid:9	O
)	O
(	O
12.87	O
)	O
(	O
12.88	O
)	O
we	O
should	O
choose	O
l	O
large	O
enough	O
so	O
that	O
the	O
shared	B
subspace	O
does	O
not	O
capture	O
covariate-	O
speciﬁc	O
variation	O
.	O
this	O
model	O
can	O
be	O
easily	O
generalized	O
to	O
discrete	B
data	O
using	O
the	O
exponential	B
family	I
(	O
virtanen	O
2010	O
)	O
.	O
12.6.	O
independent	B
component	I
analysis	I
(	O
ica	O
)	O
407	O
12.5.3	O
canonical	O
correlation	O
analysis	O
canonical	O
correlation	O
analysis	O
or	O
cca	O
is	O
like	O
a	O
symmetric	B
unsupervised	O
version	O
of	O
pls	O
:	O
it	O
allows	O
each	O
view	O
to	O
have	O
its	O
own	O
“	O
private	O
”	O
subspace	O
,	O
but	O
there	O
is	O
also	O
a	O
shared	B
subspace	O
.	O
if	O
we	O
have	O
two	O
observed	O
variables	O
,	O
xi	O
and	O
yi	O
,	O
then	O
we	O
have	O
three	O
latent	B
variables	O
,	O
zs	O
l0	O
which	O
is	O
i	O
∈	O
r	O
shared	B
,	O
zx	O
ly	O
which	O
are	O
private	O
.	O
we	O
can	O
write	O
the	O
model	O
as	O
follows	O
(	O
bach	O
and	O
jordan	O
2005	O
)	O
:	O
lx	O
and	O
zy	O
i	O
∈	O
r	O
i	O
∈	O
r	O
(	O
12.89	O
)	O
(	O
12.90	O
)	O
(	O
12.91	O
)	O
(	O
12.92	O
)	O
(	O
12.93	O
)	O
(	O
12.94	O
)	O
p	O
(	O
zi	O
)	O
=n	O
(	O
zs	O
p	O
(	O
xi|zi	O
)	O
=n	O
(	O
xi|bxzx	O
p	O
(	O
yi|zi	O
)	O
=n	O
(	O
yi|byzy	O
i|0	O
,	O
ils	O
)	O
n	O
(	O
zx	O
i	O
+	O
wxzs	O
i	O
+	O
wyzs	O
i	O
|0	O
,	O
ilx	O
)	O
n	O
(	O
zy	O
i	O
|0	O
,	O
ily	O
)	O
i	O
+	O
μx	O
,	O
σ2idx	O
)	O
i	O
+	O
μy	O
,	O
σ2idy	O
)	O
see	O
figure	O
12.19	O
(	O
c	O
)	O
.	O
the	O
corresponding	O
observed	O
joint	O
distribution	O
has	O
the	O
form	O
(	O
cid:28	O
)	O
p	O
(	O
vi|θ	O
)	O
=	O
where	O
w	O
=	O
wwt	O
=	O
n	O
(	O
vi|wzi	O
+	O
μ	O
,	O
σ2i	O
)	O
n	O
(	O
zi|0	O
,	O
i	O
)	O
dzi	O
=	O
n	O
(	O
vi|μ	O
,	O
wwt	O
+	O
σ2id	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
wx	O
bx	O
wy	O
wxwt	O
0	O
0	O
by	O
x	O
+	O
bxbt	O
x	O
wxwt	O
y	O
(	O
cid:9	O
)	O
(	O
cid:9	O
)	O
wywt	O
y	O
wywt	O
y	O
+	O
bybt	O
y	O
one	O
can	O
compute	O
the	O
mle	O
for	O
this	O
model	O
using	O
em	O
.	O
(	O
bach	O
and	O
jordan	O
2005	O
)	O
show	O
that	O
the	O
resulting	O
mle	O
is	O
equivalent	O
(	O
up	O
to	O
rotation	O
and	O
scaling	O
)	O
to	O
the	O
classical	B
,	O
non-probabilistic	O
view	O
.	O
however	O
,	O
the	O
advantages	O
of	O
the	O
probabilistic	O
view	O
are	O
many	O
:	O
we	O
can	O
trivially	O
generalize	B
to	O
m	O
>	O
2	O
observed	O
variables	O
;	O
we	O
can	O
create	O
mixtures	O
of	O
cca	O
(	O
viinikanoja	O
et	O
al	O
.	O
2010	O
)	O
;	O
we	O
can	O
create	O
sparse	B
versions	O
of	O
cca	O
using	O
ard	O
(	O
archambeau	O
and	O
bach	O
2008	O
)	O
;	O
we	O
can	O
generalize	B
to	O
the	O
exponential	B
family	I
(	O
klami	O
et	O
al	O
.	O
2010	O
)	O
;	O
we	O
can	O
perform	O
bayesian	O
inference	B
of	O
the	O
parameters	O
(	O
wang	O
2007	O
;	O
klami	O
and	O
kaski	O
2008	O
)	O
;	O
we	O
can	O
handle	O
non-parametric	O
sparsity-promoting	O
priors	O
for	O
w	O
and	O
b	O
(	O
rai	O
and	O
daume	O
2009	O
)	O
;	O
and	O
so	O
on	O
.	O
12.6	O
independent	B
component	I
analysis	I
(	O
ica	O
)	O
consider	O
the	O
following	O
situation	O
.	O
you	O
are	O
in	O
a	O
crowded	O
room	O
and	O
many	O
people	O
are	O
speaking	O
.	O
your	O
ears	O
essentially	O
act	O
as	O
two	O
microphones	O
,	O
which	O
are	O
listening	O
to	O
a	O
linear	O
combination	O
of	O
the	O
different	O
speech	O
signals	O
in	O
the	O
room	O
.	O
your	O
goal	O
is	O
to	O
deconvolve	O
the	O
mixed	O
signals	O
into	O
their	O
constituent	O
parts	O
.	O
this	O
is	O
known	O
as	O
the	O
cocktail	B
party	I
problem	I
,	O
and	O
is	O
an	O
example	O
of	O
blind	B
signal	I
separation	I
(	O
bss	O
)	O
,	O
or	O
blind	B
source	I
separation	I
,	O
where	O
“	O
blind	O
”	O
means	O
we	O
know	O
“	O
nothing	O
”	O
about	O
the	O
source	O
of	O
the	O
signals	O
.	O
besides	O
the	O
obvious	O
applications	O
to	O
acoustic	O
signal	B
processing	I
,	O
this	O
problem	O
also	O
arises	O
when	O
analysing	O
eeg	O
and	O
meg	O
signals	O
,	O
ﬁnancial	O
data	O
,	O
and	O
any	O
other	O
dataset	O
(	O
not	O
necessarily	O
temporal	O
)	O
where	O
latent	B
sources	O
or	O
factors	B
get	O
mixed	O
together	O
in	O
a	O
linear	O
way	O
.	O
at	O
“	O
time	O
”	O
t	O
,	O
and	O
zt	O
∈	O
r	O
we	O
can	O
formalize	O
the	O
problem	O
as	O
follows	O
.	O
let	O
xt	O
∈	O
r	O
l	O
be	O
the	O
vector	O
of	O
source	O
signals	O
.	O
we	O
assume	O
that	O
d	O
be	O
the	O
observed	O
signal	O
at	O
the	O
sensors	O
xt	O
=	O
wzt	O
+	O
t	O
(	O
12.95	O
)	O
408	O
2	O
0	O
−2	O
0	O
5	O
0	O
−5	O
0	O
2	O
0	O
−2	O
10	O
0	O
0	O
−10	O
0	O
10	O
0	O
−10	O
0	O
5	O
0	O
−5	O
0	O
2	O
0	O
−2	O
0	O
1	O
0	O
−1	O
0	O
chapter	O
12.	O
latent	B
linear	O
models	O
truth	O
observed	O
signals	O
100	O
200	O
300	O
400	O
500	O
100	O
200	O
300	O
400	O
500	O
100	O
200	O
300	O
400	O
500	O
100	O
200	O
300	O
400	O
500	O
(	O
a	O
)	O
pca	O
estimate	O
100	O
200	O
300	O
400	O
500	O
100	O
200	O
300	O
400	O
500	O
100	O
200	O
300	O
400	O
500	O
100	O
200	O
300	O
400	O
500	O
10	O
0	O
−10	O
0	O
5	O
0	O
−5	O
10	O
0	O
0	O
−10	O
0	O
5	O
0	O
−5	O
0	O
5	O
0	O
−5	O
10	O
0	O
0	O
−10	O
0	O
2	O
0	O
−2	O
0	O
2	O
0	O
−2	O
0	O
100	O
200	O
300	O
400	O
500	O
100	O
200	O
300	O
400	O
500	O
100	O
200	O
300	O
400	O
500	O
100	O
200	O
300	O
400	O
500	O
(	O
b	O
)	O
ica	O
estimate	O
100	O
200	O
300	O
400	O
500	O
100	O
200	O
300	O
400	O
500	O
100	O
200	O
300	O
400	O
500	O
100	O
200	O
300	O
400	O
500	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
12.20	O
illustration	O
of	O
ica	O
applied	O
to	O
500	O
iid	B
samples	O
of	O
a	O
4d	O
source	O
signal	O
.	O
(	O
b	O
)	O
observations	O
.	O
(	O
c	O
)	O
pca	O
estimate	O
.	O
(	O
d	O
)	O
ica	O
estimate	O
.	O
figure	O
generated	O
by	O
icademo	O
,	O
written	O
by	O
aapo	O
hyvarinen	O
.	O
(	O
a	O
)	O
latent	B
signals	O
.	O
where	O
w	O
is	O
an	O
d	O
×	O
l	O
matrix	O
,	O
and	O
t	O
∼	O
n	O
(	O
0	O
,	O
ψ	O
)	O
.	O
in	O
this	O
section	O
,	O
we	O
treat	O
each	O
time	O
point	O
as	O
an	O
independent	O
observation	O
,	O
i.e.	O
,	O
we	O
do	O
not	O
model	O
temporal	O
correlation	O
(	O
so	O
we	O
could	O
replace	O
the	O
t	O
index	O
with	O
i	O
,	O
but	O
we	O
stick	O
with	O
t	O
to	O
be	O
consistent	B
with	O
much	O
of	O
the	O
ica	O
literature	O
)	O
.	O
the	O
goal	O
is	O
to	O
infer	O
the	O
source	O
signals	O
,	O
p	O
(	O
zt|xt	O
,	O
θ	O
)	O
,	O
as	O
illustrated	O
in	O
figure	O
12.20.	O
in	O
this	O
context	O
,	O
w	O
is	O
called	O
the	O
mixing	B
matrix	I
.	O
if	O
l	O
=	O
d	O
(	O
number	O
of	O
sources	O
=	O
number	O
of	O
sensors	O
)	O
,	O
it	O
will	O
be	O
a	O
square	O
matrix	O
.	O
often	O
we	O
will	O
assume	O
the	O
noise	O
level	O
,	O
|ψ|	O
,	O
is	O
zero	O
,	O
for	O
simplicity	O
.	O
so	O
far	O
,	O
the	O
model	O
is	O
identical	O
to	O
factor	B
analysis	I
(	O
or	O
pca	O
if	O
there	O
is	O
no	O
noise	O
,	O
except	O
we	O
don	O
’	O
t	O
in	O
general	O
require	O
orthogonality	O
of	O
w	O
)	O
.	O
however	O
,	O
we	O
will	O
use	O
a	O
different	O
prior	O
for	O
p	O
(	O
zt	O
)	O
.	O
in	O
pca	O
,	O
we	O
assume	O
each	O
source	O
is	O
independent	O
,	O
and	O
has	O
a	O
gaussian	O
distribution	O
l	O
(	O
cid:27	O
)	O
p	O
(	O
zt	O
)	O
=	O
n	O
(	O
ztj|0	O
,	O
1	O
)	O
(	O
12.96	O
)	O
j=1	O
we	O
will	O
now	O
relax	O
this	O
gaussian	O
assumption	O
and	O
let	O
the	O
source	O
distributions	O
be	O
any	O
non-gaussian	O
12.6.	O
independent	B
component	I
analysis	I
(	O
ica	O
)	O
409	O
uniform	O
data	O
uniform	O
data	O
after	O
linear	O
mixing	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−3	O
−2	O
−1	O
0	O
(	O
a	O
)	O
1	O
2	O
3	O
pca	O
applied	O
to	O
mixed	O
data	O
from	O
uniform	O
source	O
−3	O
−2	O
−1	O
0	O
(	O
c	O
)	O
1	O
2	O
3	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−3	O
−2	O
−1	O
0	O
(	O
b	O
)	O
1	O
2	O
3	O
ica	O
applied	O
to	O
mixed	O
data	O
from	O
uniform	O
source	O
−3	O
−2	O
−1	O
0	O
(	O
d	O
)	O
1	O
2	O
3	O
figure	O
12.21	O
distribution	O
.	O
icademouniform	O
,	O
written	O
by	O
aapo	O
hyvarinen	O
.	O
(	O
a	O
)	O
latent	B
signals	O
.	O
(	O
b	O
)	O
observations	O
.	O
illustration	O
of	O
ica	O
and	O
pca	O
applied	O
to	O
100	O
iid	B
samples	O
of	O
a	O
2d	O
source	O
signal	O
with	O
a	O
uniform	O
(	O
d	O
)	O
ica	O
estimate	O
.	O
figure	O
generated	O
by	O
(	O
c	O
)	O
pca	O
estimate	O
.	O
distribution	O
p	O
(	O
zt	O
)	O
=	O
l	O
(	O
cid:27	O
)	O
j=1	O
pj	O
(	O
ztj	O
)	O
(	O
12.97	O
)	O
without	O
loss	B
of	O
generality	O
,	O
we	O
can	O
constrain	O
the	O
variance	B
of	O
the	O
source	O
distributions	O
to	O
be	O
1	O
,	O
because	O
any	O
other	O
variance	B
can	O
be	O
modelled	O
by	O
scaling	O
the	O
rows	O
of	O
w	O
appropriately	O
.	O
the	O
resulting	O
model	O
is	O
known	O
as	O
independent	B
component	I
analysis	I
or	O
ica	O
.	O
the	O
reason	O
the	O
gaussian	O
distribution	O
is	O
disallowed	O
as	O
a	O
source	O
prior	O
in	O
ica	O
is	O
that	O
it	O
does	O
not	O
permit	O
unique	O
recovery	O
of	O
the	O
sources	O
,	O
as	O
illustrated	O
in	O
figure	O
12.20	O
(	O
c	O
)	O
.	O
this	O
is	O
because	O
the	O
pca	O
likelihood	B
is	O
invariant	B
to	O
any	O
orthogonal	O
transformation	O
of	O
the	O
sources	O
zt	O
and	O
mixing	B
matrix	I
w.	O
pca	O
can	O
recover	O
the	O
best	O
linear	O
subspace	O
in	O
which	O
the	O
signals	O
lie	O
,	O
but	O
can	O
not	O
uniquely	O
recover	O
the	O
signals	O
themselves	O
.	O
410	O
chapter	O
12.	O
latent	B
linear	O
models	O
to	O
illustrate	O
this	O
,	O
suppose	O
we	O
have	O
two	O
independent	O
sources	O
with	O
uniform	O
distributions	O
,	O
as	O
shown	O
in	O
figure	O
12.21	O
(	O
a	O
)	O
.	O
now	O
suppose	O
we	O
have	O
the	O
following	O
mixing	B
matrix	I
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
w	O
=	O
2	O
3	O
2	O
1	O
(	O
12.98	O
)	O
then	O
we	O
observe	O
the	O
data	O
shown	O
in	O
figure	O
12.21	O
(	O
b	O
)	O
(	O
assuming	O
no	O
noise	O
)	O
.	O
if	O
we	O
apply	O
pca	O
followed	O
by	O
scaling	O
to	O
this	O
,	O
we	O
get	O
the	O
result	O
in	O
figure	O
12.21	O
(	O
c	O
)	O
.	O
this	O
corresponds	O
to	O
a	O
whitening	B
of	O
the	O
data	O
.	O
to	O
uniquely	O
recover	O
the	O
sources	O
,	O
we	O
need	O
to	O
perform	O
an	O
additional	O
rotation	O
.	O
the	O
trouble	O
is	O
,	O
there	O
is	O
no	O
information	O
in	O
the	O
symmetric	B
gaussian	O
posterior	O
to	O
tell	O
us	O
which	O
angle	O
to	O
rotate	O
by	O
.	O
in	O
a	O
sense	O
,	O
pca	O
solves	O
“	O
half	O
”	O
of	O
the	O
problem	O
,	O
since	O
it	O
identiﬁes	O
the	O
linear	O
subspace	O
;	O
all	O
that	O
ica	O
has	O
to	O
do	O
is	O
then	O
to	O
identify	O
the	O
appropriate	O
rotation	O
.	O
(	O
hence	O
we	O
see	O
that	O
ica	O
is	O
not	O
that	O
different	O
from	O
methods	O
such	O
as	O
varimax	B
,	O
which	O
seek	O
good	O
rotations	O
of	O
the	O
latent	B
factors	I
to	O
enhance	O
interpretability	O
.	O
)	O
ica	O
requires	O
that	O
w	O
is	O
square	O
and	O
hence	O
invertible	O
.	O
figure	O
12.21	O
(	O
d	O
)	O
shows	O
that	O
ica	O
can	O
recover	O
the	O
source	O
,	O
up	O
to	O
a	O
permutation	O
of	O
the	O
indices	O
and	O
possible	O
sign	O
change	O
.	O
in	O
the	O
non-square	O
case	O
(	O
e.g.	O
,	O
where	O
we	O
have	O
more	O
sources	O
than	O
sensors	O
)	O
,	O
we	O
can	O
not	O
uniquely	O
recover	O
the	O
true	O
signal	O
,	O
but	O
we	O
can	O
compute	O
the	O
posterior	O
p	O
(	O
zt|xt	O
,	O
ˆw	O
)	O
,	O
which	O
represents	O
our	O
beliefs	O
about	O
the	O
source	O
.	O
in	O
both	O
cases	O
,	O
we	O
need	O
to	O
estimate	O
w	O
as	O
well	O
as	O
the	O
source	O
distributions	O
pj	O
.	O
we	O
discuss	O
how	O
to	O
do	O
this	O
below	O
.	O
12.6.1	O
maximum	O
likelihood	O
estimation	O
in	O
this	O
section	O
,	O
we	O
discuss	O
ways	O
to	O
estimate	O
square	O
mixing	O
matrices	O
w	O
for	O
the	O
noise-free	O
ica	O
model	O
.	O
as	O
usual	O
,	O
we	O
will	O
assume	O
that	O
the	O
observations	O
have	O
been	O
centered	O
;	O
hence	O
we	O
can	O
also	O
assume	O
z	O
is	O
zero-mean	O
.	O
in	O
addition	O
,	O
we	O
assume	O
the	O
observations	O
have	O
been	O
whitened	O
,	O
which	O
can	O
be	O
done	O
with	O
pca	O
.	O
(	O
cid:31	O
)	O
if	O
the	O
data	O
is	O
centered	O
and	O
whitened	O
,	O
we	O
have	O
e	O
xxt	O
=	O
i.	O
but	O
in	O
the	O
noise	O
free	O
case	O
,	O
we	O
also	O
have	O
cov	O
[	O
x	O
]	O
=	O
e	O
(	O
cid:31	O
)	O
(	O
cid:31	O
)	O
xxt	O
=	O
we	O
zzt	O
wt	O
=	O
wwt	O
(	O
12.99	O
)	O
hence	O
we	O
see	O
that	O
w	O
must	O
be	O
orthogonal	O
.	O
this	O
reduces	O
the	O
number	O
of	O
parameters	O
we	O
have	O
to	O
estimate	O
from	O
d2	O
to	O
d	O
(	O
d	O
−	O
1	O
)	O
/2	O
.	O
it	O
will	O
also	O
simplify	O
the	O
math	O
and	O
the	O
algorithms	O
.	O
let	O
v	O
=	O
w−1	O
;	O
these	O
are	O
often	O
called	O
the	O
recognition	B
weights	I
,	O
as	O
opposed	O
to	O
w	O
,	O
which	O
are	O
the	O
generative	O
weights.4	O
since	O
x	O
=	O
wz	O
,	O
we	O
have	O
,	O
from	O
equation	O
2.89	O
,	O
px	O
(	O
wzt	O
)	O
=	O
pz	O
(	O
zt	O
)	O
|	O
det	O
(	O
w−1	O
)	O
|	O
=	O
pz	O
(	O
vxt	O
)	O
|	O
det	O
(	O
v	O
)	O
|	O
hence	O
we	O
can	O
write	O
the	O
log-likelihood	O
,	O
assuming	O
t	O
iid	O
samples	B
,	O
as	O
follows	O
:	O
1	O
t	O
log	O
p	O
(	O
d|v	O
)	O
=	O
log	O
|	O
det	O
(	O
v	O
)	O
|	O
+	O
1	O
t	O
log	O
pj	O
(	O
vt	O
j	O
xt	O
)	O
l	O
(	O
cid:2	O
)	O
t	O
(	O
cid:2	O
)	O
j=1	O
t=1	O
(	O
12.100	O
)	O
(	O
12.101	O
)	O
4.	O
in	O
the	O
literature	O
,	O
it	O
is	O
common	O
to	O
denote	O
the	O
generative	B
weights	I
by	O
a	O
and	O
the	O
recognition	B
weights	I
by	O
w	O
,	O
but	O
we	O
are	O
trying	O
to	O
be	O
consistent	B
with	O
the	O
notation	O
used	O
earlier	O
in	O
this	O
chapter	O
.	O
12.6.	O
independent	B
component	I
analysis	I
(	O
ica	O
)	O
411	O
where	O
vj	O
is	O
the	O
j	O
’	O
th	O
row	O
of	O
v.	O
since	O
we	O
are	O
constraining	O
v	O
to	O
be	O
orthogonal	O
,	O
the	O
ﬁrst	O
term	O
is	O
a	O
constant	O
,	O
so	O
we	O
can	O
drop	O
it	O
.	O
we	O
can	O
also	O
replace	O
the	O
average	O
over	O
the	O
data	O
with	O
an	O
expectation	O
operator	O
to	O
get	O
the	O
following	O
objective	O
l	O
(	O
cid:2	O
)	O
nll	O
(	O
v	O
)	O
=	O
e	O
[	O
gj	O
(	O
zj	O
)	O
]	O
(	O
12.102	O
)	O
j=1	O
j	O
x	O
and	O
gj	O
(	O
z	O
)	O
(	O
cid:2	O
)	O
−	O
log	O
pj	O
(	O
z	O
)	O
.	O
we	O
want	O
to	O
minimize	O
this	O
subject	O
to	O
the	O
constraint	O
where	O
zj	O
=	O
vt	O
that	O
the	O
rows	O
of	O
v	O
are	O
orthogonal	O
.	O
we	O
also	O
want	O
them	O
to	O
be	O
unit	O
norm	O
,	O
since	O
this	O
ensures	O
=	O
||vj||2	O
)	O
,	O
which	O
is	O
that	O
the	O
variance	B
of	O
the	O
factors	B
is	O
unity	O
(	O
since	O
,	O
with	O
whitened	O
data	O
,	O
e	O
necessary	O
to	O
ﬁx	O
the	O
scale	O
of	O
the	O
weights	O
.	O
in	O
otherwords	O
,	O
v	O
should	O
be	O
an	O
orthonormal	O
matrix	O
.	O
it	O
is	O
straightforward	O
to	O
derive	O
a	O
gradient	B
descent	I
algorithm	O
to	O
ﬁt	O
this	O
model	O
;	O
however	O
,	O
it	O
is	O
rather	O
slow	O
.	O
one	O
can	O
also	O
derive	O
a	O
faster	O
algorithm	O
that	O
follows	O
the	O
natural	B
gradient	I
;	O
see	O
e.g.	O
,	O
(	O
mackay	O
2003	O
,	O
ch	O
34	O
)	O
for	O
details	O
.	O
a	O
popular	O
alternative	O
is	O
to	O
use	O
an	O
approximate	O
newton	O
method	O
,	O
which	O
we	O
discuss	O
in	O
section	O
12.6.2.	O
another	O
approach	O
is	O
to	O
use	O
em	O
,	O
which	O
we	O
discuss	O
in	O
section	O
12.6.3.	O
vt	O
j	O
x	O
(	O
cid:31	O
)	O
12.6.2	O
the	O
fastica	O
algorithm	O
we	O
now	O
describe	O
the	O
fast	O
ica	O
algorithm	O
,	O
based	O
on	O
(	O
hyvarinen	O
and	O
oja	O
2000	O
)	O
,	O
which	O
we	O
will	O
show	O
is	O
an	O
approximate	O
newton	O
method	O
for	O
ﬁtting	O
ica	O
models	O
.	O
for	O
simplicity	O
of	O
presentation	O
,	O
we	O
initially	O
assume	O
there	O
is	O
only	O
one	O
latent	B
factor	O
.	O
in	O
addition	O
,	O
we	O
initially	O
assume	O
all	O
source	O
distributions	O
are	O
known	O
and	O
are	O
the	O
same	O
,	O
so	O
we	O
can	O
just	O
write	O
g	O
(	O
z	O
)	O
=−	O
log	O
p	O
(	O
z	O
)	O
.	O
let	O
g	O
(	O
z	O
)	O
=	O
d	O
dz	O
g	O
(	O
z	O
)	O
.	O
the	O
constrained	O
objective	O
,	O
and	O
its	O
gradient	O
and	O
hessian	O
,	O
are	O
given	O
by	O
+	O
λ	O
(	O
1	O
−	O
vt	O
v	O
)	O
−	O
βv	O
−	O
βi	O
(	O
cid:31	O
)	O
where	O
β	O
=	O
2λ	O
is	O
a	O
lagrange	O
multiplier	O
.	O
let	O
us	O
make	O
the	O
approximation	O
f	O
(	O
v	O
)	O
=e	O
∇f	O
(	O
v	O
)	O
=e	O
h	O
(	O
v	O
)	O
=e	O
(	O
12.103	O
)	O
(	O
12.105	O
)	O
(	O
12.104	O
)	O
(	O
cid:31	O
)	O
g	O
(	O
cid:4	O
)	O
e	O
(	O
vt	O
x	O
)	O
=	O
e	O
(	O
vt	O
x	O
)	O
(	O
cid:31	O
)	O
g	O
(	O
cid:4	O
)	O
this	O
makes	O
the	O
hessian	O
very	O
easy	O
to	O
invert	O
,	O
giving	O
rise	O
to	O
the	O
following	O
newton	O
update	O
:	O
(	O
cid:31	O
)	O
(	O
cid:31	O
)	O
g	O
(	O
vt	O
x	O
)	O
(	O
cid:31	O
)	O
xg	O
(	O
vt	O
x	O
)	O
xxt	O
g	O
(	O
cid:4	O
)	O
(	O
vt	O
x	O
)	O
(	O
cid:31	O
)	O
≈	O
e	O
(	O
vt	O
x	O
)	O
(	O
cid:31	O
)	O
−	O
βv	O
xg	O
(	O
vt	O
x	O
)	O
e	O
[	O
g	O
(	O
cid:4	O
)	O
(	O
vt	O
x	O
)	O
]	O
−	O
β	O
xxt	O
xxt	O
g	O
(	O
cid:4	O
)	O
e	O
v∗	O
(	O
cid:2	O
)	O
v	O
−	O
e	O
one	O
can	O
rewrite	O
this	O
in	O
the	O
following	O
way	O
v∗	O
(	O
cid:2	O
)	O
e	O
xg	O
(	O
vt	O
x	O
)	O
g	O
(	O
cid:4	O
)	O
(	O
vt	O
x	O
)	O
v	O
(	O
cid:31	O
)	O
−	O
e	O
(	O
cid:31	O
)	O
(	O
in	O
practice	O
,	O
the	O
expectations	O
can	O
be	O
replaced	O
by	O
monte	O
carlo	O
estimates	O
from	O
the	O
training	B
set	I
,	O
which	O
gives	O
an	O
efficient	O
online	O
learning	B
algorithm	O
.	O
)	O
after	O
performing	O
this	O
update	O
,	O
one	O
should	O
project	O
back	O
onto	O
the	O
constraint	O
surface	O
using	O
vnew	O
(	O
cid:2	O
)	O
v∗	O
||v∗||	O
(	O
12.106	O
)	O
(	O
12.107	O
)	O
(	O
12.108	O
)	O
(	O
12.109	O
)	O
412	O
0.5	O
0.45	O
0.4	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
chapter	O
12.	O
latent	B
linear	O
models	O
gaussian	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
0	O
−4	O
−3	O
−2	O
−1	O
0	O
(	O
a	O
)	O
laplace	O
1	O
2	O
3	O
4	O
−4	O
−3	O
−2	O
−1	O
1	O
2	O
3	O
4	O
0	O
(	O
b	O
)	O
uniform	O
10	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−10	O
−5	O
0	O
(	O
c	O
)	O
5	O
10	O
−2	O
−1	O
0	O
(	O
d	O
)	O
1	O
2	O
figure	O
12.22	O
illustration	O
of	O
gaussian	O
,	O
sub-gaussian	O
(	O
uniform	O
)	O
and	O
super-gaussian	O
(	O
laplace	O
)	O
distributions	O
in	O
1d	O
and	O
2d	O
.	O
figure	O
generated	O
by	O
subsupergaussplot	O
,	O
written	O
by	O
kevin	O
swersky	O
.	O
(	O
due	O
to	O
the	O
sign	O
ambiguity	O
of	O
v	O
,	O
the	O
values	O
of	O
v	O
one	O
iterates	O
this	O
algorithm	O
until	O
convergence	O
.	O
may	O
not	O
converge	O
,	O
but	O
the	O
direction	O
deﬁned	O
by	O
this	O
vector	O
should	O
converge	B
,	O
so	O
one	O
can	O
assess	O
convergence	O
by	O
monitoring	O
|vt	O
vnew|	O
,	O
which	O
should	O
approach	O
1	O
.	O
)	O
since	O
the	O
objective	O
is	O
not	O
convex	O
,	O
there	O
are	O
multiple	O
local	O
optima	O
.	O
we	O
can	O
use	O
this	O
fact	O
to	O
learn	O
multiple	O
different	O
weight	O
vectors	O
or	O
features	B
.	O
we	O
can	O
either	O
learn	O
the	O
features	B
sequentially	O
and	O
then	O
project	O
out	O
the	O
part	O
of	O
vj	O
that	O
lies	O
in	O
the	O
subspace	O
deﬁned	O
by	O
earlier	O
features	B
,	O
or	O
we	O
can	O
learn	O
them	O
in	O
parallel	O
,	O
and	O
orthogonalize	O
v	O
in	O
parallel	O
.	O
this	O
latter	O
approach	O
is	O
usually	O
preferred	O
,	O
since	O
,	O
unlike	O
pca	O
,	O
the	O
features	B
are	O
not	O
ordered	O
in	O
any	O
way	O
.	O
so	O
the	O
ﬁrst	O
feature	O
is	O
not	O
“	O
more	O
important	O
”	O
than	O
the	O
second	O
,	O
and	O
hence	O
it	O
is	O
better	O
to	O
treat	O
them	O
symmetrically	O
.	O
12.6.	O
independent	B
component	I
analysis	I
(	O
ica	O
)	O
413	O
12.6.2.1	O
modeling	O
the	O
source	O
densities	O
so	O
far	O
,	O
we	O
have	O
assumed	O
that	O
g	O
(	O
z	O
)	O
=−	O
log	O
p	O
(	O
z	O
)	O
is	O
known	O
.	O
what	O
kinds	O
of	O
models	O
might	O
be	O
reasonable	O
as	O
signal	O
priors	O
?	O
we	O
know	O
that	O
using	O
gaussians	O
(	O
which	O
correspond	O
to	O
quadratic	O
functions	O
for	O
g	O
)	O
won	O
’	O
t	O
work	O
.	O
so	O
we	O
want	O
some	O
kind	O
of	O
non-gaussian	O
distribution	O
.	O
in	O
general	O
,	O
there	O
are	O
several	O
kinds	O
of	O
non-gaussian	O
distributions	O
,	O
such	O
as	O
the	O
following	O
:	O
•	O
super-gaussian	O
distributions	O
these	O
are	O
distributions	O
which	O
have	O
a	O
big	O
spike	O
at	O
the	O
mean	B
,	O
and	O
hence	O
(	O
in	O
order	O
to	O
ensure	O
unit	O
variance	O
)	O
have	O
heavy	B
tails	I
.	O
the	O
laplace	O
distribution	O
is	O
a	O
classic	O
example	O
.	O
see	O
figure	O
12.22.	O
formally	O
,	O
we	O
say	O
a	O
distribution	O
is	O
super-gaussian	O
or	O
leptokurtic	B
(	O
“	O
lepto	O
”	O
coming	O
from	O
the	O
greek	O
for	O
“	O
thin	O
”	O
)	O
if	O
kurt	O
(	O
z	O
)	O
>	O
0	O
,	O
where	O
kurt	O
(	O
z	O
)	O
is	O
the	O
kurtosis	B
of	O
the	O
distribution	O
,	O
deﬁned	O
by	O
kurt	O
(	O
z	O
)	O
(	O
cid:2	O
)	O
μ4	O
σ4	O
−	O
3	O
(	O
12.110	O
)	O
(	O
cid:31	O
)	O
where	O
σ	O
is	O
the	O
standard	B
deviation	I
,	O
and	O
μk	O
is	O
the	O
k	O
’	O
th	O
central	B
moment	I
,	O
or	O
moment	O
about	O
the	O
mean	B
:	O
μk	O
(	O
cid:2	O
)	O
e	O
(	O
12.111	O
)	O
(	O
so	O
μ1	O
=	O
μ	O
is	O
the	O
mean	B
,	O
and	O
μ2	O
=	O
σ2	O
is	O
the	O
variance	B
.	O
)	O
it	O
is	O
conventional	O
to	O
subtract	O
3	O
in	O
the	O
deﬁnition	O
of	O
kurtosis	B
to	O
make	O
the	O
kurtosis	B
of	O
a	O
gaussian	O
variable	O
equal	O
to	O
zero	O
.	O
(	O
x	O
−	O
e	O
[	O
x	O
]	O
)	O
k	O
•	O
sub-gaussian	O
distributions	O
a	O
sub-gaussian	O
or	O
platykurtic	B
(	O
“	O
platy	O
”	O
coming	O
from	O
the	O
greek	O
for	O
“	O
broad	O
”	O
)	O
distribution	O
has	O
negative	O
kurtosis	O
.	O
these	O
are	O
distributions	O
which	O
are	O
much	O
ﬂatter	O
than	O
a	O
gaussian	O
.	O
the	O
uniform	B
distribution	I
is	O
a	O
classic	O
example	O
.	O
see	O
figure	O
12.22	O
.	O
•	O
skewed	O
distributions	O
another	O
way	O
to	O
“	O
be	O
non-gaussian	O
”	O
is	O
to	O
be	O
asymmetric	O
.	O
one	O
measure	O
of	O
this	O
is	O
skewness	B
,	O
deﬁned	O
by	O
skew	O
(	O
z	O
)	O
(	O
cid:2	O
)	O
μ3	O
σ3	O
(	O
12.112	O
)	O
an	O
example	O
of	O
a	O
(	O
right	O
)	O
skewed	O
distribution	O
is	O
the	O
gamma	B
distribution	I
(	O
see	O
figure	O
2.9	O
)	O
.	O
when	O
one	O
looks	O
at	O
the	O
empirical	B
distribution	I
of	O
many	O
natural	O
signals	O
,	O
such	O
as	O
images	O
and	O
speech	O
,	O
when	O
passed	O
through	O
certain	O
linear	O
ﬁlters	O
,	O
they	O
tend	O
to	O
be	O
very	O
super-gaussian	O
.	O
this	O
result	O
holds	O
both	O
for	O
the	O
kind	O
of	O
linear	O
ﬁlters	O
found	O
in	O
certain	O
parts	O
of	O
the	O
brain	O
,	O
such	O
as	O
the	O
simple	B
cells	I
found	O
in	O
the	O
primary	O
visual	O
cortex	O
,	O
as	O
well	O
as	O
for	O
the	O
kinds	O
of	O
linear	O
ﬁlters	O
used	O
in	O
signal	B
processing	I
,	O
such	O
as	O
wavelet	B
transforms	I
.	O
one	O
obvious	O
choice	O
for	O
modeling	O
natural	O
signals	O
with	O
ica	O
is	O
therefore	O
the	O
laplace	O
distribution	O
.	O
for	O
mean	B
zero	O
and	O
variance	B
1	O
,	O
this	O
has	O
a	O
log	O
pdf	O
given	O
by	O
√	O
log	O
p	O
(	O
z	O
)	O
=	O
−	O
2|z|	O
−log	O
(	O
√	O
2	O
)	O
(	O
12.113	O
)	O
since	O
the	O
laplace	O
prior	O
is	O
not	O
differentiable	O
at	O
the	O
origin	O
,	O
it	O
is	O
more	O
common	O
to	O
use	O
other	O
,	O
smoother	O
super-gaussian	O
distributions	O
.	O
one	O
example	O
is	O
the	O
logistic	B
distribution	I
.	O
the	O
corre-	O
sponding	O
log	O
pdf	O
,	O
for	O
the	O
case	O
where	O
the	O
mean	B
is	O
zero	O
and	O
the	O
variance	B
is	O
1	O
(	O
so	O
μ	O
=	O
0	O
and	O
s	O
=	O
√	O
3	O
π	O
)	O
,	O
is	O
given	O
by	O
the	O
following	O
:	O
log	O
p	O
(	O
z	O
)	O
=	O
−2	O
log	O
cosh	O
(	O
√	O
π	O
z	O
)	O
−	O
log	O
2	O
3	O
√	O
4	O
3	O
π	O
(	O
12.114	O
)	O
414	O
chapter	O
12.	O
latent	B
linear	O
models	O
μ1k	O
σ1k	O
π	O
μdk	O
σdk	O
qtd	O
ztd	O
xtd	O
t	O
w	O
qt1	O
zt1	O
xt1	O
ψ	O
figure	O
12.23	O
modeling	O
the	O
source	O
distributions	O
using	O
a	O
mixture	O
of	O
univariate	O
gaussians	O
(	O
the	O
independent	O
factor	O
analysis	O
model	O
of	O
(	O
moulines	O
et	O
al	O
.	O
1997	O
;	O
attias	O
1999	O
)	O
)	O
.	O
various	O
ways	O
of	O
estimating	O
g	O
(	O
z	O
)	O
=	O
−	O
log	O
p	O
(	O
z	O
)	O
are	O
discussed	O
in	O
the	O
seminal	O
paper	O
(	O
pham	O
and	O
garrat	O
1997	O
)	O
.	O
however	O
,	O
when	O
ﬁtting	O
ica	O
by	O
maximum	O
likelihood	O
,	O
it	O
is	O
not	O
critical	O
that	O
the	O
exact	O
shape	O
of	O
the	O
source	O
distribution	O
be	O
known	O
(	O
although	O
it	O
is	O
important	O
to	O
know	O
whether	O
it	O
is	O
sub	O
z	O
or	O
g	O
(	O
z	O
)	O
=	O
log	O
cosh	O
(	O
z	O
)	O
or	O
super	O
gaussian	O
)	O
.	O
consequently	O
,	O
it	O
is	O
common	O
to	O
just	O
use	O
g	O
(	O
z	O
)	O
=	O
instead	O
of	O
the	O
more	O
complex	O
expressions	O
above	O
.	O
√	O
12.6.3	O
using	O
em	O
an	O
alternative	O
to	O
assuming	O
a	O
particular	O
form	O
for	O
g	O
(	O
z	O
)	O
,	O
or	O
equivalently	O
for	O
p	O
(	O
z	O
)	O
,	O
is	O
to	O
use	O
a	O
ﬂexible	O
non-parametric	O
density	O
estimator	B
,	O
such	O
as	O
a	O
mixture	O
of	O
(	O
uni-variate	O
)	O
gaussians	O
:	O
p	O
(	O
qj	O
=	O
k	O
)	O
=π	O
k	O
p	O
(	O
zj|qj	O
=	O
k	O
)	O
=n	O
(	O
μj	O
,	O
k	O
,	O
σ2	O
j	O
,	O
k	O
)	O
p	O
(	O
x|z	O
)	O
=n	O
(	O
wz	O
,	O
ψ	O
)	O
(	O
12.115	O
)	O
(	O
12.116	O
)	O
(	O
12.117	O
)	O
this	O
approach	O
was	O
proposed	O
in	O
(	O
moulines	O
et	O
al	O
.	O
1997	O
;	O
attias	O
1999	O
)	O
,	O
and	O
the	O
corresponding	O
graph-	O
ical	O
model	O
is	O
shown	O
in	O
figure	O
12.23.	O
it	O
is	O
possible	O
to	O
derive	O
an	O
exact	O
em	O
algorithm	O
for	O
this	O
model	O
.	O
the	O
key	O
observation	B
is	O
that	O
it	O
is	O
possible	O
to	O
compute	O
e	O
[	O
zt|xt	O
,	O
θ	O
]	O
exactly	O
by	O
summing	O
over	O
all	O
k	O
l	O
combinations	O
of	O
the	O
qt	O
variables	O
,	O
where	O
k	O
is	O
the	O
number	O
of	O
mixture	B
components	O
per	O
source	O
.	O
(	O
if	O
this	O
is	O
too	O
expensive	O
,	O
one	O
can	O
use	O
a	O
variational	O
mean	O
ﬁeld	O
approximation	O
(	O
attias	O
1999	O
)	O
.	O
)	O
we	O
can	O
then	O
estimate	O
all	O
the	O
source	O
distributions	O
in	O
parallel	O
by	O
ﬁtting	O
a	O
standard	O
gmm	O
to	O
e	O
[	O
zt	O
]	O
.	O
when	O
the	O
source	O
gmms	O
are	O
12.6.	O
independent	B
component	I
analysis	I
(	O
ica	O
)	O
known	O
,	O
we	O
can	O
compute	O
the	O
marginals	O
pj	O
(	O
zj	O
)	O
very	O
easily	O
,	O
using	O
k	O
(	O
cid:2	O
)	O
pj	O
(	O
zj	O
)	O
=	O
πj	O
,	O
kn	O
(	O
zj|μj	O
,	O
k	O
,	O
σ2	O
j	O
,	O
k	O
)	O
415	O
(	O
12.118	O
)	O
k=1	O
given	O
the	O
pj	O
’	O
s	O
,	O
we	O
can	O
then	O
use	O
an	O
ica	O
algorithm	O
to	O
estimate	O
w.	O
of	O
course	O
,	O
these	O
steps	O
should	O
be	O
interleaved	O
.	O
the	O
details	O
can	O
be	O
found	O
in	O
(	O
attias	O
1999	O
)	O
.	O
12.6.4	O
other	O
estimation	O
principles	O
*	O
it	O
is	O
quite	O
common	O
to	O
estimate	O
the	O
parameters	O
of	O
ica	O
models	O
using	O
methods	O
that	O
seem	O
different	O
to	O
maximum	O
likelihood	O
.	O
we	O
will	O
review	O
some	O
of	O
these	O
methods	O
below	O
,	O
because	O
they	O
give	O
additional	O
insight	O
into	O
ica	O
.	O
however	O
,	O
we	O
will	O
also	O
see	O
that	O
these	O
methods	O
in	O
fact	O
are	O
equivalent	O
to	O
maximum	O
likelihood	O
after	O
all	O
.	O
our	O
presentation	O
is	O
based	O
on	O
(	O
hyvarinen	O
and	O
oja	O
2000	O
)	O
.	O
12.6.4.1	O
maximizing	O
non-gaussianity	O
an	O
early	O
approach	O
to	O
ica	O
was	O
to	O
ﬁnd	O
a	O
matrix	O
v	O
such	O
that	O
the	O
distribution	O
z	O
=	O
vx	O
is	O
as	O
far	O
from	O
gaussian	O
as	O
possible	O
.	O
(	O
there	O
is	O
a	O
related	O
approach	O
in	O
statistics	O
called	O
projection	B
pursuit	I
.	O
)	O
one	O
measure	O
of	O
non-gaussianity	O
is	O
kurtosis	B
,	O
but	O
this	O
can	O
be	O
sensitive	O
to	O
outliers	B
.	O
another	O
measure	O
is	O
the	O
negentropy	B
,	O
deﬁned	O
as	O
negentropy	B
(	O
z	O
)	O
(	O
cid:2	O
)	O
h	O
(	O
cid:3	O
)	O
n	O
(	O
μ	O
,	O
σ2	O
)	O
(	O
cid:4	O
)	O
−	O
h	O
(	O
z	O
)	O
where	O
μ	O
=	O
e	O
[	O
z	O
]	O
and	O
σ2	O
=	O
var	O
[	O
z	O
]	O
.	O
since	O
the	O
gaussian	O
is	O
the	O
maximum	B
entropy	I
distribution	O
,	O
this	O
measure	O
is	O
always	O
non-negative	O
and	O
becomes	O
large	O
for	O
distributions	O
that	O
are	O
highly	O
non-	O
gaussian	O
.	O
we	O
can	O
deﬁne	O
our	O
objective	O
as	O
maximizing	O
j	O
(	O
v	O
)	O
=	O
negentropy	B
(	O
zj	O
)	O
=	O
(	O
cid:2	O
)	O
h	O
j	O
(	O
cid:3	O
)	O
n	O
(	O
μj	O
,	O
σ2	O
j	O
)	O
(	O
cid:4	O
)	O
−	O
h	O
(	O
zj	O
)	O
(	O
cid:2	O
)	O
j	O
(	O
12.119	O
)	O
(	O
12.120	O
)	O
where	O
z	O
=	O
vx	O
.	O
(	O
cid:2	O
)	O
will	O
be	O
i	O
independently	O
of	O
v	O
,	O
so	O
the	O
ﬁrst	O
term	O
is	O
a	O
constant	O
.	O
hence	O
(	O
cid:2	O
)	O
if	O
we	O
ﬁx	O
v	O
to	O
be	O
orthogonal	O
,	O
and	O
if	O
we	O
whiten	O
the	O
data	O
,	O
the	O
covariance	B
of	O
z	O
j	O
(	O
v	O
)	O
=	O
−h	O
(	O
zj	O
)	O
+	O
const	O
=	O
e	O
[	O
log	O
p	O
(	O
zj	O
)	O
]	O
+	O
const	O
(	O
12.121	O
)	O
j	O
j	O
which	O
we	O
see	O
is	O
equal	O
(	O
up	O
to	O
a	O
sign	O
change	O
,	O
and	O
irrelevant	O
constants	O
)	O
to	O
the	O
log-likelihood	O
in	O
equation	O
12.102	O
.	O
12.6.4.2	O
minimizing	O
mutual	B
information	I
one	O
measure	O
of	O
dependence	O
of	O
a	O
set	O
of	O
random	O
variables	O
is	O
the	O
multi-information	B
:	O
⎛	O
⎝p	O
(	O
z	O
)	O
||	O
(	O
cid:27	O
)	O
⎞	O
⎠	O
=	O
(	O
cid:2	O
)	O
p	O
(	O
zj	O
)	O
j	O
j	O
i	O
(	O
z	O
)	O
(	O
cid:2	O
)	O
kl	O
h	O
(	O
zj	O
)	O
−	O
h	O
(	O
z	O
)	O
(	O
12.122	O
)	O
416	O
chapter	O
12.	O
latent	B
linear	O
models	O
we	O
would	O
like	O
to	O
minimize	O
this	O
,	O
since	O
we	O
are	O
trying	O
to	O
ﬁnd	O
independent	O
components	O
.	O
put	O
another	O
way	O
,	O
we	O
want	O
the	O
best	O
possible	O
factored	O
approximation	O
to	O
the	O
joint	B
distribution	I
.	O
(	O
cid:2	O
)	O
now	O
since	O
z	O
=	O
vx	O
,	O
we	O
have	O
h	O
(	O
zj	O
)	O
−	O
h	O
(	O
vx	O
)	O
i	O
(	O
z	O
)	O
=	O
j	O
if	O
we	O
constrain	O
v	O
to	O
be	O
orthogonal	O
,	O
we	O
can	O
drop	O
the	O
last	O
term	O
,	O
since	O
then	O
h	O
(	O
vx	O
)	O
=	O
h	O
(	O
x	O
)	O
(	O
since	O
multiplying	O
by	O
v	O
does	O
not	O
change	O
the	O
shape	O
of	O
the	O
distribution	O
)	O
,	O
and	O
h	O
(	O
x	O
)	O
is	O
a	O
constant	O
which	O
is	O
is	O
solely	O
determined	O
by	O
the	O
empirical	B
distribution	I
.	O
hence	O
we	O
have	O
i	O
(	O
z	O
)	O
=	O
j	O
h	O
(	O
zj	O
)	O
.	O
minimizing	O
this	O
is	O
equivalent	O
to	O
maximizing	O
the	O
negentropy	B
,	O
which	O
is	O
equivalent	O
to	O
maximum	O
likelihood	O
.	O
(	O
cid:10	O
)	O
12.6.4.3	O
maximizing	O
mutual	B
information	I
(	O
infomax	B
)	O
(	O
12.123	O
)	O
instead	O
of	O
trying	O
to	O
minimize	O
the	O
mutual	B
information	I
between	O
the	O
components	O
of	O
z	O
,	O
let	O
us	O
imagine	O
a	O
neural	B
network	I
where	O
x	O
is	O
the	O
input	O
and	O
yj	O
=	O
φ	O
(	O
vt	O
j	O
x	O
)	O
+	O
	O
is	O
the	O
noisy	O
output	O
,	O
where	O
φ	O
is	O
some	O
nonlinear	O
scalar	O
function	O
,	O
and	O
	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
.	O
it	O
seems	O
reasonable	O
to	O
try	O
to	O
maximize	O
the	O
information	B
ﬂow	O
through	O
this	O
system	O
,	O
a	O
principle	O
known	O
as	O
infomax	B
.	O
(	O
bell	O
and	O
sejnowski	O
1995	O
)	O
.	O
that	O
is	O
,	O
we	O
want	O
to	O
maximize	O
the	O
mutual	B
information	I
between	O
y	O
(	O
the	O
internal	O
neural	O
representation	O
)	O
and	O
x	O
(	O
the	O
observed	O
input	O
signal	O
)	O
.	O
we	O
have	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
y	O
)	O
−	O
h	O
(	O
y|x	O
)	O
,	O
where	O
the	O
latter	O
term	O
is	O
constant	O
if	O
we	O
assume	O
the	O
noise	O
has	O
constant	O
variance	O
.	O
one	O
can	O
show	O
that	O
we	O
can	O
approximate	O
the	O
former	O
term	O
as	O
follows	O
log	O
φ	O
(	O
cid:4	O
)	O
(	O
vt	O
j	O
x	O
)	O
+	O
log	O
|	O
det	O
(	O
v	O
)	O
|	O
(	O
12.124	O
)	O
(	O
cid:31	O
)	O
l	O
(	O
cid:2	O
)	O
e	O
j=1	O
h	O
(	O
y	O
)	O
=	O
where	O
,	O
as	O
usual	O
,	O
we	O
can	O
drop	O
the	O
last	O
term	O
if	O
v	O
is	O
orthogonal	O
.	O
if	O
we	O
deﬁne	O
φ	O
(	O
z	O
)	O
to	O
be	O
a	O
cdf	B
,	O
then	O
φ	O
(	O
cid:4	O
)	O
(	O
z	O
)	O
is	O
its	O
pdf	B
,	O
and	O
the	O
above	O
expression	O
is	O
equivalent	O
to	O
the	O
log	O
likelihood	O
.	O
in	O
particular	O
,	O
if	O
we	O
use	O
a	O
logistic	B
nonlinearity	O
,	O
φ	O
(	O
z	O
)	O
=	O
sigm	O
(	O
z	O
)	O
,	O
then	O
the	O
corresponding	O
pdf	B
is	O
the	O
logistic	B
distribution	I
,	O
and	O
log	O
φ	O
(	O
cid:4	O
)	O
(	O
z	O
)	O
=	O
log	O
cosh	O
(	O
z	O
)	O
(	O
ignoring	O
irrelevant	O
constants	O
)	O
.	O
thus	O
we	O
see	O
that	O
infomax	B
is	O
equivalent	O
to	O
maximum	O
likelihood	O
.	O
exercises	O
exercise	O
12.1	O
m	O
step	O
for	O
fa	O
for	O
the	O
fa	O
model	O
,	O
show	O
that	O
the	O
mle	O
in	O
the	O
m	O
step	O
for	O
w	O
is	O
given	O
by	O
equation	O
12.23.	O
exercise	O
12.2	O
map	O
estimation	O
for	O
the	O
fa	O
model	O
derive	O
the	O
m	O
step	O
for	O
the	O
fa	O
model	O
using	O
conjugate	B
priors	I
for	O
the	O
parameters	O
.	O
(	O
cid:2	O
)	O
d	O
(	O
press	O
2005	O
,	O
q9.8	O
)	O
.	O
)	O
.	O
let	O
the	O
empirical	O
covariance	O
matrix	O
σ	O
have	O
eigenvalues	O
λ1	O
≥	O
λ2	O
≥	O
···	O
≥	O
i=1	O
(	O
λi	O
−	O
λ	O
)	O
2	O
is	O
a	O
good	O
measure	O
of	O
whether	O
exercise	O
12.3	O
heuristic	O
for	O
assessing	O
applicability	O
of	O
pca	O
(	O
source	O
:	O
λd	O
>	O
0.	O
explain	O
why	O
the	O
variance	B
of	O
the	O
evalues	O
,	O
σ2	O
=	O
1	O
d	O
or	O
not	O
pca	O
would	O
be	O
useful	O
for	O
analysing	O
the	O
data	O
(	O
the	O
higher	O
the	O
value	O
of	O
σ2	O
the	O
more	O
useful	O
pca	O
)	O
.	O
12.6.	O
independent	B
component	I
analysis	I
(	O
ica	O
)	O
exercise	O
12.4	O
deriving	O
the	O
second	O
principal	O
component	O
a.	O
let	O
j	O
(	O
v2	O
,	O
z2	O
)	O
=	O
1	O
n	O
(	O
xi	O
−	O
zi1v1	O
−	O
zi2v2	O
)	O
t	O
(	O
xi	O
−	O
zi1v1	O
−	O
zi2v2	O
)	O
n	O
(	O
cid:12	O
)	O
i=1	O
417	O
(	O
12.125	O
)	O
show	O
that	O
∂j	O
∂z2	O
=	O
0	O
yields	O
zi2	O
=	O
vt	O
2	O
xi	O
.	O
b.	O
show	O
that	O
the	O
value	O
of	O
v2	O
that	O
minimizes	O
˜j	O
(	O
v2	O
)	O
=	O
−vt	O
2	O
cv2	O
+	O
λ2	O
(	O
vt	O
(	O
12.126	O
)	O
is	O
given	O
by	O
the	O
eigenvector	O
of	O
c	O
with	O
the	O
second	O
largest	O
eigenvalue	O
.	O
hint	O
:	O
recall	B
that	O
cv1	O
=	O
λ1v1	O
and	O
∂xt	O
ax	O
2	O
v2	O
−	O
1	O
)	O
+	O
λ12	O
(	O
vt	O
2	O
v1	O
−	O
0	O
)	O
∂x	O
=	O
(	O
a	O
+	O
at	O
)	O
x.	O
exercise	O
12.5	O
deriving	O
the	O
residual	B
error	I
for	O
pca	O
a.	O
prove	O
that	O
||xi	O
−	O
k	O
(	O
cid:12	O
)	O
i	O
xi	O
−	O
k	O
(	O
cid:12	O
)	O
zijvj||2	O
=	O
xt	O
vt	O
j	O
xixt	O
i	O
vj	O
(	O
12.127	O
)	O
j=1	O
j=1	O
j	O
vj	O
=	O
1	O
and	O
vt	O
j	O
vk	O
=	O
0	O
for	O
k	O
(	O
cid:8	O
)	O
=	O
j.	O
also	O
,	O
hint	O
:	O
ﬁrst	O
consider	O
the	O
case	O
k	O
=	O
2.	O
use	O
the	O
fact	O
that	O
vt	O
recall	B
zij	O
=	O
xt	O
b.	O
now	O
show	O
that	O
i	O
vj	O
.	O
&	O
jk	O
(	O
cid:2	O
)	O
1	O
n	O
vt	O
j	O
xixt	O
i	O
vj	O
=	O
1	O
n	O
%	O
n	O
(	O
cid:12	O
)	O
i	O
xi	O
−	O
k	O
(	O
cid:12	O
)	O
xt	O
i=1	O
j=1	O
n	O
(	O
cid:12	O
)	O
i	O
xi	O
−	O
k	O
(	O
cid:12	O
)	O
xt	O
i=1	O
j=1	O
λj	O
(	O
12.128	O
)	O
j	O
cvj	O
=	O
λjvt	O
hint	O
:	O
recall	B
vt	O
if	O
k	O
=	O
d	O
there	O
is	O
no	O
truncation	O
,	O
so	O
jd	O
=	O
0.	O
use	O
this	O
to	O
show	O
that	O
the	O
error	O
from	O
only	O
using	O
k	O
<	O
d	O
terms	O
is	O
given	O
by	O
j	O
vj	O
=	O
λj	O
.	O
c.	O
d	O
(	O
cid:12	O
)	O
jk	O
=	O
λj	O
j=k+1	O
hint	O
:	O
partition	O
the	O
sum	O
(	O
cid:2	O
)	O
d	O
j=1	O
λj	O
into	O
(	O
cid:2	O
)	O
k	O
j=1	O
λj	O
and	O
(	O
cid:2	O
)	O
d	O
j=k+1	O
λj	O
.	O
(	O
12.129	O
)	O
exercise	O
12.6	O
derivation	O
of	O
fisher	O
’	O
s	O
linear	O
discriminant	O
show	O
that	O
the	O
maximum	O
of	O
j	O
(	O
w	O
)	O
=	O
wt	O
sb	O
w	O
where	O
λ	O
=	O
wt	O
sb	O
w	O
wt	O
sw	O
w	O
where	O
f	O
(	O
cid:2	O
)	O
f	O
(	O
x	O
)	O
and	O
g	O
(	O
cid:2	O
)	O
g	O
(	O
x	O
)	O
.	O
also	O
,	O
recall	B
that	O
d	O
dx	O
=	O
d	O
dx	O
=	O
d	O
dx	O
wt	O
sw	O
w	O
is	O
given	O
by	O
sbw	O
=	O
λsw	O
w	O
.	O
hint	O
:	O
recall	B
that	O
the	O
derivative	O
of	O
a	O
ratio	O
of	O
two	O
scalars	O
is	O
given	O
by	O
d	O
dx	O
xt	O
ax	O
=	O
(	O
a	O
+	O
at	O
)	O
x.	O
f	O
(	O
x	O
)	O
g	O
(	O
x	O
)	O
=	O
f	O
(	O
cid:2	O
)	O
g−f	O
g	O
(	O
cid:2	O
)	O
g2	O
,	O
exercise	O
12.7	O
pca	O
via	O
successive	O
deﬂation	O
let	O
v1	O
,	O
v2	O
,	O
.	O
.	O
.	O
,	O
v	O
k	O
be	O
the	O
ﬁrst	O
k	O
eigenvectors	O
with	O
largest	O
eigenvalues	O
of	O
c	O
=	O
1	O
n	O
basis	O
vectors	O
.	O
these	O
satisfy	O
if	O
j	O
(	O
cid:8	O
)	O
=	O
k	O
if	O
j	O
=	O
k	O
j	O
vk	O
=	O
vt	O
0	O
1	O
xt	O
x	O
,	O
i.e.	O
,	O
the	O
principal	O
(	O
12.130	O
)	O
we	O
will	O
construct	O
a	O
method	O
for	O
ﬁnding	O
the	O
vj	O
sequentially	O
.	O
418	O
chapter	O
12.	O
latent	B
linear	O
models	O
as	O
we	O
showed	O
in	O
class	O
,	O
v1	O
is	O
the	O
ﬁrst	O
principal	O
eigenvector	O
of	O
c	O
,	O
and	O
satisﬁes	O
cv1	O
=	O
λ1v1	O
.	O
now	O
deﬁne	O
˜xi	O
as	O
the	O
orthogonal	B
projection	I
of	O
xi	O
onto	O
the	O
space	O
orthogonal	O
to	O
v1	O
:	O
˜xi	O
=	O
p⊥v1	O
xi	O
=	O
(	O
i	O
−	O
v1vt	O
1	O
)	O
xi	O
(	O
12.131	O
)	O
deﬁne	O
˜x	O
=	O
[	O
˜x1	O
;	O
...	O
;	O
˜xn	O
]	O
as	O
the	O
deﬂated	B
matrix	I
of	O
rank	O
d	O
−	O
1	O
,	O
which	O
is	O
obtained	O
by	O
removing	O
from	O
the	O
d	O
dimensional	O
data	O
the	O
component	O
that	O
lies	O
in	O
the	O
direction	O
of	O
the	O
ﬁrst	O
principal	O
direction	O
:	O
˜x	O
=	O
(	O
i	O
−	O
v1vt	O
1	O
)	O
t	O
x	O
=	O
(	O
i	O
−	O
v1vt	O
1	O
)	O
x	O
(	O
12.132	O
)	O
a.	O
using	O
the	O
facts	O
that	O
xt	O
xv1	O
=	O
nλ1v1	O
(	O
and	O
hence	O
vt	O
1	O
xt	O
x	O
=	O
nλ1vt	O
1	O
)	O
and	O
vt	O
1	O
v1	O
=	O
1	O
,	O
show	O
that	O
the	O
covariance	B
of	O
the	O
deﬂated	B
matrix	I
is	O
given	O
by	O
˜c	O
(	O
cid:2	O
)	O
1	O
n	O
˜xt	O
˜x	O
=	O
1	O
n	O
xt	O
x	O
−	O
λ1v1vt	O
1	O
(	O
12.133	O
)	O
b.	O
let	O
u	O
be	O
the	O
principal	O
eigenvector	O
of	O
˜c	O
.	O
explain	O
why	O
u	O
=	O
v2	O
.	O
(	O
you	O
may	O
assume	O
u	O
is	O
unit	O
norm	O
.	O
)	O
c.	O
suppose	O
we	O
have	O
a	O
simple	O
method	O
for	O
ﬁnding	O
the	O
leading	O
eigenvector	O
and	O
eigenvalue	O
of	O
a	O
pd	O
matrix	O
,	O
denoted	O
by	O
[	O
λ	O
,	O
u	O
]	O
=	O
f	O
(	O
c	O
)	O
.	O
write	O
some	O
pseudo	O
code	O
for	O
ﬁnding	O
the	O
ﬁrst	O
k	O
principal	O
basis	O
vectors	O
of	O
x	O
that	O
only	O
uses	O
the	O
special	O
f	O
function	O
and	O
simple	O
vector	O
arithmetic	O
,	O
i.e.	O
,	O
your	O
code	O
should	O
not	O
use	O
svd	O
or	O
theeig	O
function	O
.	O
hint	O
:	O
this	O
should	O
be	O
a	O
simple	O
iterative	O
routine	O
that	O
takes	O
2–3	O
lines	O
to	O
write	O
.	O
the	O
input	O
is	O
c	O
,	O
k	O
and	O
the	O
function	O
f	O
,	O
the	O
output	O
should	O
be	O
vj	O
and	O
λj	O
for	O
j	O
=	O
1	O
:	O
k.	O
do	O
not	O
worry	O
about	O
being	O
syntactically	O
correct	O
.	O
exercise	O
12.8	O
latent	B
semantic	I
indexing	I
(	O
source	O
:	O
de	O
freitas.	O
)	O
.	O
in	O
this	O
exercise	O
,	O
we	O
study	O
a	O
technique	O
called	O
latent	B
semantic	I
indexing	I
,	O
which	O
applies	O
svd	O
to	O
a	O
document	O
by	O
term	O
matrix	O
,	O
to	O
create	O
a	O
low-dimensional	O
embedding	B
of	O
the	O
data	O
that	O
is	O
designed	O
to	O
capture	O
semantic	O
similarity	O
of	O
words	O
.	O
the	O
ﬁle	O
lsidocuments.pdf	O
contains	O
9	O
documents	O
on	O
various	O
topics	O
.	O
a	O
list	O
of	O
all	O
the	O
460	O
unique	O
words/terms	O
that	O
occur	O
in	O
these	O
documents	O
is	O
in	O
lsiwords.txt	O
.	O
a	O
document	O
by	O
term	O
matrix	O
is	O
in	O
lsimatrix.txt	O
.	O
a.	O
let	O
x	O
be	O
the	O
transpose	O
of	O
lsimatrix	O
,	O
so	O
each	O
column	O
represents	O
a	O
document	O
.	O
compute	O
the	O
svd	O
of	O
x	O
and	O
make	O
an	O
approximation	O
to	O
it	O
ˆx	O
using	O
the	O
ﬁrst	O
2	O
singular	O
values/	O
vectors	O
.	O
plot	O
the	O
low	O
dimensional	O
representation	O
of	O
the	O
9	O
documents	O
in	O
2d	O
.	O
you	O
should	O
get	O
something	O
like	O
figure	O
12.24.	O
b.	O
consider	O
ﬁnding	O
documents	O
that	O
are	O
about	O
alien	O
abductions	O
.	O
if	O
if	O
you	O
look	O
at	O
lsiwords.txt	O
,	O
there	O
are	O
3	O
versions	O
of	O
this	O
word	O
,	O
term	O
23	O
(	O
“	O
abducted	O
”	O
)	O
,	O
term	O
24	O
(	O
“	O
abduction	O
”	O
)	O
and	O
term	O
25	O
(	O
“	O
abductions	O
”	O
)	O
.	O
suppose	O
we	O
want	O
to	O
ﬁnd	O
documents	O
containing	O
the	O
word	O
“	O
abducted	O
”	O
.	O
documents	O
2	O
and	O
3	O
contain	O
it	O
,	O
but	O
document	O
1	O
does	O
not	O
.	O
however	O
,	O
document	O
1	O
is	O
clearly	O
related	O
to	O
this	O
topic	B
.	O
thus	O
lsi	O
should	O
also	O
ﬁnd	O
document	O
1.	O
create	O
a	O
test	O
document	O
q	O
containing	O
the	O
one	O
word	O
“	O
abducted	O
”	O
,	O
and	O
project	O
it	O
into	O
the	O
2d	O
subspace	O
to	O
make	O
ˆq	O
.	O
now	O
compute	O
the	O
cosine	B
similarity	I
between	O
ˆq	O
and	O
the	O
low	O
dimensional	O
representation	O
of	O
all	O
the	O
documents	O
.	O
what	O
are	O
the	O
top	O
3	O
closest	O
matches	O
?	O
exercise	O
12.9	O
imputation	B
in	O
a	O
fa	O
model	O
derive	O
an	O
expression	O
for	O
p	O
(	O
xh|xv	O
,	O
θ	O
)	O
for	O
a	O
fa	O
model	O
.	O
exercise	O
12.10	O
efficiently	O
evaluating	O
the	O
ppca	O
density	O
derive	O
an	O
expression	O
for	O
p	O
(	O
x|	O
ˆw	O
,	O
ˆσ2	O
)	O
for	O
the	O
ppca	O
model	O
based	O
on	O
plugging	O
in	O
the	O
mles	O
and	O
using	O
the	O
matrix	B
inversion	I
lemma	I
.	O
12.6.	O
independent	B
component	I
analysis	I
(	O
ica	O
)	O
419	O
6	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−0.6	O
4	O
7	O
8	O
9	O
5	O
2	O
3	O
1	O
−0.8	O
−0.45	O
−0.4	O
−0.35	O
−0.3	O
−0.25	O
−0.2	O
figure	O
12.24	O
projection	B
of	O
9	O
documents	O
into	O
2	O
dimensions	O
.	O
figure	O
generated	O
by	O
lsicode	O
.	O
exercise	O
12.11	O
ppca	O
vs	O
fa	O
(	O
source	O
:	O
exercise	O
14.15	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
,	O
due	O
to	O
hinton.	O
)	O
.	O
generate	O
200	O
observations	O
from	O
the	O
following	O
model	O
,	O
where	O
zi	O
∼	O
n	O
(	O
0	O
,	O
i	O
)	O
:	O
xi1	O
=	O
zi1	O
,	O
xi2	O
=	O
zi1	O
+	O
0.001zi2	O
,	O
xi3	O
=	O
10zi3	O
.	O
fit	O
a	O
fa	O
and	O
pca	O
model	O
with	O
1	O
latent	B
factor	O
.	O
hence	O
show	O
that	O
the	O
corresponding	O
weight	B
vector	I
w	O
aligns	O
with	O
the	O
maximal	O
variance	O
direction	O
(	O
dimension	O
3	O
)	O
in	O
the	O
pca	O
case	O
,	O
but	O
with	O
the	O
maximal	O
correlation	O
direction	O
(	O
dimensions	O
1+2	O
)	O
in	O
the	O
case	O
of	O
fa	O
.	O
13	O
sparse	B
linear	O
models	O
13.1	O
introduction	O
we	O
introduced	O
the	O
topic	B
of	O
feature	B
selection	I
in	O
section	O
3.5.4	O
,	O
where	O
we	O
discussed	O
methods	O
for	O
ﬁnding	O
input	O
variables	O
which	O
had	O
high	O
mutual	O
information	B
with	O
the	O
output	O
.	O
the	O
trouble	O
with	O
this	O
approach	O
is	O
that	O
it	O
is	O
based	O
on	O
a	O
myopic	O
strategy	O
that	O
only	O
looks	O
at	O
one	O
variable	O
at	O
a	O
time	O
.	O
this	O
can	O
fail	O
if	O
there	O
are	O
interaction	B
effects	I
.	O
for	O
example	O
,	O
if	O
y	O
=	O
xor	B
(	O
x1	O
,	O
x2	O
)	O
,	O
then	O
neither	O
x1	O
nor	O
x2	O
on	O
its	O
own	O
can	O
predict	O
the	O
response	O
,	O
but	O
together	O
they	O
perfectly	O
predict	O
the	O
response	O
.	O
for	O
a	O
real-world	O
example	O
of	O
this	O
,	O
consider	O
genetic	O
association	O
studies	O
:	O
sometimes	O
two	O
genes	O
on	O
their	O
own	O
may	O
be	O
harmless	O
,	O
but	O
when	O
present	O
together	O
they	O
cause	O
a	O
recessive	O
disease	O
(	O
balding	O
2006	O
)	O
.	O
in	O
this	O
chapter	O
,	O
we	O
focus	O
on	O
selecting	O
sets	O
of	O
variables	O
at	O
a	O
time	O
using	O
a	O
model-based	O
approach	O
.	O
if	O
the	O
model	O
is	O
a	O
generalized	B
linear	I
model	I
,	O
of	O
the	O
form	O
p	O
(	O
y|x	O
)	O
=p	O
(	O
y|f	O
(	O
wt	O
x	O
)	O
)	O
for	O
some	O
link	B
function	I
f	O
,	O
then	O
we	O
can	O
perform	O
feature	B
selection	I
by	O
encouraging	O
the	O
weight	B
vector	I
w	O
to	O
be	O
sparse	B
,	O
i.e.	O
,	O
to	O
have	O
lots	O
of	O
zeros	O
.	O
this	O
approach	O
turns	O
out	O
to	O
offer	O
signiﬁcant	O
computational	O
advantages	O
,	O
as	O
we	O
will	O
see	O
below	O
.	O
here	O
are	O
some	O
applications	O
where	O
feature	O
selection/	O
sparsity	B
is	O
useful	O
:	O
•	O
•	O
•	O
in	O
many	O
problems	O
,	O
we	O
have	O
many	O
more	O
dimensions	O
d	O
than	O
training	O
cases	O
n	O
.	O
the	O
cor-	O
responding	O
design	B
matrix	I
is	O
short	O
and	O
fat	O
,	O
rather	O
than	O
tall	O
and	O
skinny	O
.	O
this	O
is	O
called	O
the	O
small	O
n	O
,	O
large	O
d	O
problem	O
.	O
this	O
is	O
becoming	O
increasingly	O
prevalent	O
as	O
we	O
develop	O
more	O
high	B
throughput	I
measurement	O
devices	O
,	O
for	O
example	O
,	O
with	O
gene	B
microarrays	I
,	O
it	O
is	O
common	O
to	O
measure	O
the	O
expression	O
levels	O
of	O
d	O
∼	O
10	O
,	O
000	O
genes	O
,	O
but	O
to	O
only	O
get	O
n	O
∼	O
100	O
such	O
examples	O
.	O
(	O
it	O
is	O
perhaps	O
a	O
sign	O
of	O
the	O
times	O
that	O
even	O
our	O
data	O
seems	O
to	O
be	O
getting	O
fatter	O
...	O
)	O
we	O
may	O
want	O
to	O
ﬁnd	O
the	O
smallest	O
set	O
of	O
features	B
that	O
can	O
accurately	O
predict	O
the	O
response	O
(	O
e.g.	O
,	O
growth	O
rate	B
of	O
the	O
cell	O
)	O
in	O
order	O
to	O
prevent	O
overﬁtting	B
,	O
to	O
reduce	O
the	O
cost	O
of	O
building	O
a	O
diagnostic	O
device	O
,	O
or	O
to	O
help	O
with	O
scientiﬁc	O
insight	O
into	O
the	O
problem	O
.	O
in	O
chapter	O
14	O
,	O
we	O
will	O
use	O
basis	B
functions	I
centered	O
on	O
the	O
training	O
examples	O
,	O
so	O
φ	O
(	O
x	O
)	O
=	O
[	O
κ	O
(	O
x	O
,	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
κ	O
(	O
x	O
,	O
xn	O
)	O
]	O
,	O
where	O
κ	O
is	O
a	O
kernel	B
function	I
.	O
the	O
resulting	O
design	B
matrix	I
has	O
size	O
n	O
×	O
n	O
.	O
feature	B
selection	I
in	O
this	O
context	O
is	O
equivalent	O
to	O
selecting	O
a	O
subset	O
of	O
the	O
training	O
examples	O
,	O
which	O
can	O
help	O
reduce	O
overﬁtting	B
and	O
computational	O
cost	O
.	O
this	O
is	O
known	O
as	O
a	O
sparse	B
kernel	I
machine	I
.	O
in	O
terms	O
of	O
in	O
signal	B
processing	I
,	O
it	O
is	O
common	O
to	O
represent	O
signals	O
(	O
images	O
,	O
speech	O
,	O
etc	O
.	O
)	O
wavelet	B
basis	O
functions	O
.	O
to	O
save	O
time	O
and	O
space	O
,	O
it	O
is	O
useful	O
to	O
ﬁnd	O
a	O
sparse	B
representation	I
422	O
chapter	O
13.	O
sparse	B
linear	O
models	O
of	O
the	O
signals	O
,	O
in	O
terms	O
of	O
a	O
small	O
number	O
of	O
such	O
basis	B
functions	I
.	O
this	O
allows	O
us	O
to	O
estimate	O
signals	O
from	O
a	O
small	O
number	O
of	O
measurements	O
,	O
as	O
well	O
as	O
to	O
compress	O
the	O
signal	O
.	O
see	O
section	O
13.8.3	O
for	O
more	O
information	B
.	O
note	O
that	O
the	O
topic	B
of	O
feature	B
selection	I
and	O
sparsity	B
is	O
currently	O
one	O
of	O
the	O
most	O
active	O
areas	O
in	O
this	O
chapter	O
,	O
we	O
only	O
have	O
space	O
to	O
give	O
an	O
overview	O
of	O
the	O
of	O
machine	O
learning/	O
statistics	O
.	O
main	O
results	O
.	O
13.2	O
bayesian	O
variable	O
selection	O
a	O
natural	O
way	O
to	O
pose	O
the	O
variable	O
selection	O
problem	O
is	O
as	O
follows	O
.	O
let	O
γj	O
=	O
1	O
if	O
feature	O
j	O
is	O
“	O
relevant	O
”	O
,	O
and	O
let	O
γj	O
=	O
0	O
otherwise	O
.	O
our	O
goal	O
is	O
to	O
compute	O
the	O
posterior	O
over	O
models	O
p	O
(	O
γ|d	O
)	O
=	O
(	O
cid:10	O
)	O
e−f	O
(	O
γ	O
)	O
γ	O
(	O
cid:2	O
)	O
e−f	O
(	O
γ	O
(	O
cid:2	O
)	O
)	O
(	O
13.1	O
)	O
where	O
f	O
(	O
γ	O
)	O
is	O
the	O
cost	O
function	O
:	O
f	O
(	O
γ	O
)	O
(	O
cid:2	O
)	O
−	O
[	O
log	O
p	O
(	O
d|γ	O
)	O
+	O
log	O
p	O
(	O
γ	O
)	O
]	O
(	O
13.2	O
)	O
for	O
example	O
,	O
suppose	O
we	O
generate	O
n	O
=	O
20	O
samples	B
from	O
a	O
d	O
=	O
10	O
dimensional	O
linear	B
regression	I
model	O
,	O
yi	O
∼	O
n	O
(	O
wt	O
xi	O
,	O
σ2	O
)	O
,	O
in	O
particular	O
,	O
we	O
use	O
w	O
=	O
(	O
0.00	O
,	O
−1.67	O
,	O
0.13	O
,	O
0.00	O
,	O
0.00	O
,	O
1.19	O
,	O
0.00	O
,	O
−0.04	O
,	O
0.33	O
,	O
0.00	O
)	O
and	O
σ2	O
=	O
1.	O
we	O
enumerate	O
all	O
210	O
=	O
1024	O
models	O
and	O
compute	O
p	O
(	O
γ|d	O
)	O
for	O
each	O
one	O
(	O
we	O
give	O
the	O
equations	O
for	O
this	O
below	O
)	O
.	O
we	O
order	O
the	O
models	O
in	O
gray	O
code	O
order	O
,	O
which	O
ensures	O
consecutive	O
vectors	O
differ	O
by	O
exactly	O
1	O
bit	O
(	O
the	O
reasons	O
for	O
this	O
are	O
computational	O
,	O
and	O
are	O
discussed	O
in	O
section	O
13.2.3	O
)	O
.	O
in	O
which	O
k	O
=	O
5	O
elements	O
of	O
w	O
are	O
non-zero	O
.	O
the	O
resulting	O
set	O
of	O
bit	O
patterns	O
is	O
shown	O
in	O
figure	O
13.1	O
(	O
a	O
)	O
.	O
the	O
cost	O
of	O
each	O
model	O
,	O
f	O
(	O
γ	O
)	O
,	O
is	O
shown	O
in	O
figure	O
13.1	O
(	O
b	O
)	O
.	O
we	O
see	O
that	O
this	O
objective	O
function	O
is	O
extremely	O
“	O
bumpy	O
”	O
.	O
the	O
results	O
are	O
easier	O
to	O
interpret	O
if	O
we	O
compute	O
the	O
posterior	O
distribution	O
over	O
models	O
,	O
p	O
(	O
γ|d	O
)	O
.	O
this	O
is	O
shown	O
in	O
figure	O
13.1	O
(	O
c	O
)	O
.	O
the	O
top	O
8	O
models	O
are	O
listed	O
below	O
:	O
model	O
4	O
61	O
452	O
60	O
29	O
68	O
36	O
5	O
prob	O
members	O
0.447	O
0.241	O
0.103	O
0.091	O
0.041	O
0.021	O
0.015	O
0.010	O
2	O
,	O
2	O
,	O
6	O
,	O
2	O
,	O
6	O
,	O
9	O
,	O
2	O
,	O
3	O
,	O
6	O
,	O
2	O
,	O
5	O
,	O
2	O
,	O
6	O
,	O
7	O
,	O
2	O
,	O
5	O
,	O
6	O
,	O
2	O
,	O
3	O
,	O
the	O
“	O
true	O
”	O
model	O
is	O
{	O
2	O
,	O
3	O
,	O
6	O
,	O
8	O
,	O
9	O
}	O
.	O
however	O
,	O
the	O
coefficients	O
associated	O
with	O
features	B
3	O
and	O
8	O
are	O
very	O
small	O
(	O
relative	O
to	O
σ2	O
)	O
.	O
so	O
these	O
variables	O
are	O
harder	O
to	O
detect	O
.	O
given	O
enough	O
data	O
,	O
the	O
method	O
will	O
converge	B
on	O
the	O
true	O
model	O
(	O
assuming	O
the	O
data	O
is	O
generated	O
from	O
a	O
linear	O
model	O
)	O
,	O
but	O
for	O
ﬁnite	O
data	O
sets	O
,	O
there	O
will	O
usually	O
be	O
considerable	O
posterior	O
uncertainty	O
.	O
interpreting	O
the	O
posterior	O
over	O
a	O
large	O
number	O
of	O
models	O
is	O
quite	O
difficult	O
,	O
so	O
we	O
will	O
seek	O
various	O
summary	O
statistics	O
.	O
a	O
natural	O
one	O
is	O
the	O
posterior	B
mode	I
,	O
or	O
map	O
estimate	O
ˆγ	O
=	O
argmax	O
p	O
(	O
γ|d	O
)	O
=	O
argmin	O
f	O
(	O
γ	O
)	O
(	O
13.3	O
)	O
13.2.	O
bayesian	O
variable	O
selection	O
423	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
0.1	O
0.09	O
0.08	O
0.07	O
0.06	O
0.05	O
0.04	O
0.03	O
0.02	O
0.01	O
0	O
0	O
100	O
200	O
300	O
400	O
500	O
(	O
a	O
)	O
600	O
700	O
800	O
900	O
1000	O
p	O
(	O
model|data	O
)	O
200	O
400	O
600	O
800	O
1000	O
(	O
c	O
)	O
−40	O
−60	O
−80	O
−100	O
−120	O
−140	O
−160	O
−180	O
−200	O
−220	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
log	O
p	O
(	O
model	O
,	O
data	O
)	O
0	O
200	O
400	O
600	O
800	O
1000	O
(	O
b	O
)	O
p	O
(	O
gamma	O
(	O
j	O
)	O
|data	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
(	O
d	O
)	O
figure	O
13.1	O
all	O
possible	O
models	O
.	O
(	O
d	O
)	O
marginal	O
inclusion	O
probabilities	O
.	O
figure	O
generated	O
by	O
linregallsubsetsgraycodedemo	O
.	O
(	O
a	O
)	O
all	O
possible	O
bit	O
vectors	O
of	O
length	O
10	O
enumerated	O
in	O
gray	O
code	O
order	O
.	O
(	O
b	O
)	O
score	B
function	I
for	O
(	O
c	O
)	O
posterior	O
over	O
all	O
1024	O
models	O
.	O
vertical	O
scale	O
has	O
been	O
truncated	O
at	O
0.1	O
for	O
clarity	O
.	O
however	O
,	O
the	O
mode	B
is	O
often	O
not	O
representative	O
of	O
the	O
full	B
posterior	O
mass	O
(	O
see	O
section	O
5.2.1.3	O
)	O
.	O
a	O
better	O
summary	O
is	O
the	O
median	B
model	I
(	O
barbieri	O
and	O
berger	O
2004	O
;	O
carvahlo	O
and	O
lawrence	O
2007	O
)	O
,	O
computed	O
using	O
ˆγ	O
=	O
{	O
j	O
:	O
p	O
(	O
γj	O
=	O
1|d	O
)	O
>	O
0.5	O
}	O
(	O
13.4	O
)	O
this	O
requires	O
computing	O
the	O
posterior	O
marginal	O
inclusion	B
probabilities	I
,	O
p	O
(	O
γj	O
=	O
1|d	O
)	O
.	O
these	O
are	O
shown	O
in	O
figure	O
13.1	O
(	O
d	O
)	O
.	O
we	O
see	O
that	O
the	O
model	O
is	O
conﬁdent	O
that	O
variables	O
2	O
and	O
6	O
are	O
included	O
;	O
if	O
we	O
lower	O
the	O
decision	B
threshold	O
to	O
0.1	O
,	O
we	O
would	O
add	O
3	O
and	O
9	O
as	O
well	O
.	O
however	O
,	O
if	O
we	O
wanted	O
to	O
“	O
capture	O
”	O
variable	O
8	O
,	O
we	O
would	O
incur	O
two	O
false	O
positives	O
(	O
5	O
and	O
7	O
)	O
.	O
this	O
tradeoff	O
between	O
false	O
positives	O
and	O
false	O
negatives	O
is	O
discussed	O
in	O
more	O
detail	O
in	O
section	O
5.7.2.1.	O
the	O
above	O
example	O
illustrates	O
the	O
“	O
gold	O
standard	O
”	O
for	O
variable	O
selection	O
:	O
the	O
problem	O
was	O
sufficiently	O
small	O
(	O
only	O
10	O
variables	O
)	O
that	O
we	O
were	O
able	O
to	O
compute	O
the	O
full	B
posterior	O
exactly	O
.	O
of	O
course	O
,	O
variable	O
selection	O
is	O
most	O
useful	O
in	O
the	O
cases	O
where	O
the	O
number	O
of	O
dimensions	O
is	O
large	O
.	O
since	O
there	O
are	O
2d	O
possible	O
models	O
(	O
bit	O
vectors	O
)	O
,	O
it	O
will	O
be	O
impossible	O
to	O
compute	O
the	O
full	B
posterior	O
in	O
general	O
,	O
and	O
even	O
ﬁnding	O
summaries	O
,	O
such	O
as	O
the	O
map	O
estimate	O
or	O
marginal	O
424	O
chapter	O
13.	O
sparse	B
linear	O
models	O
inclusion	B
probabilities	I
,	O
will	O
be	O
intractable	O
.	O
we	O
will	O
therefore	O
spend	O
most	O
of	O
this	O
chapter	O
focussing	O
on	O
algorithmic	O
speedups	O
.	O
but	O
before	O
we	O
do	O
that	O
,	O
we	O
will	O
explain	O
how	O
we	O
computed	O
p	O
(	O
γ|d	O
)	O
in	O
the	O
above	O
example	O
.	O
13.2.1	O
the	O
spike	B
and	I
slab	I
model	O
the	O
posterior	O
is	O
given	O
by	O
p	O
(	O
γ|d	O
)	O
∝	O
p	O
(	O
γ	O
)	O
p	O
(	O
d|γ	O
)	O
we	O
ﬁrst	O
consider	O
the	O
prior	O
,	O
then	O
the	O
likelihood	B
.	O
it	O
is	O
common	O
to	O
use	O
the	O
following	O
prior	O
on	O
the	O
bit	O
vector	O
:	O
p	O
(	O
γ	O
)	O
=	O
ber	O
(	O
γj|π0	O
)	O
=	O
π	O
||γ||0	O
0	O
(	O
1	O
−	O
π0	O
)	O
d−||γ||0	O
d	O
(	O
cid:27	O
)	O
j=1	O
where	O
π0	O
is	O
the	O
probability	O
a	O
feature	O
is	O
relevant	O
,	O
and	O
||γ||0	O
=	O
j=1	O
γj	O
is	O
the	O
(	O
cid:6	O
)	O
0	O
pseudo-norm	O
,	O
that	O
is	O
,	O
the	O
number	O
of	O
non-zero	O
elements	O
of	O
the	O
vector	O
.	O
for	O
comparison	O
with	O
later	O
models	O
,	O
it	O
is	O
useful	O
to	O
write	O
the	O
log	O
prior	O
as	O
follows	O
:	O
(	O
cid:10	O
)	O
d	O
(	O
13.5	O
)	O
(	O
13.6	O
)	O
(	O
13.7	O
)	O
(	O
13.8	O
)	O
(	O
13.9	O
)	O
(	O
13.10	O
)	O
log	O
p	O
(	O
γ|π0	O
)	O
=||γ	O
||0	O
log	O
π0	O
+	O
(	O
d	O
−	O
||γ||0	O
)	O
log	O
(	O
1	O
−	O
π0	O
)	O
=	O
||γ||0	O
(	O
log	O
π0	O
−	O
log	O
(	O
1	O
−	O
π0	O
)	O
)	O
+	O
const	O
=	O
−λ||γ||0	O
+	O
const	O
where	O
λ	O
(	O
cid:2	O
)	O
log	O
1−π0	O
π0	O
we	O
can	O
write	O
the	O
likelihood	B
as	O
follows	O
:	O
p	O
(	O
d|γ	O
)	O
=	O
p	O
(	O
y|x	O
,	O
γ	O
)	O
=	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
controls	O
the	O
sparsity	B
of	O
the	O
model	O
.	O
p	O
(	O
y|x	O
,	O
w	O
,	O
γ	O
)	O
p	O
(	O
w|γ	O
,	O
σ2	O
)	O
p	O
(	O
σ2	O
)	O
dwdσ2	O
for	O
notational	O
simplicity	O
,	O
we	O
have	O
assumed	O
the	O
response	O
is	O
centered	O
,	O
(	O
i.e.	O
,	O
y	O
=	O
0	O
)	O
,	O
so	O
we	O
can	O
ignore	O
any	O
offset	O
term	O
μ.	O
we	O
now	O
discuss	O
the	O
prior	O
p	O
(	O
w|γ	O
,	O
σ2	O
)	O
.	O
if	O
γj	O
=	O
0	O
,	O
feature	O
j	O
is	O
irrelevant	O
,	O
so	O
we	O
expect	O
wj	O
=	O
0.	O
if	O
γj	O
=	O
1	O
,	O
we	O
expect	O
wj	O
to	O
be	O
non-zero	O
.	O
if	O
we	O
standardize	O
the	O
inputs	O
,	O
a	O
reasonable	O
prior	O
is	O
n	O
(	O
0	O
,	O
σ2σ2	O
w	O
controls	O
how	O
big	O
we	O
expect	O
the	O
coefficients	O
associated	O
with	O
the	O
relevant	O
variables	O
to	O
be	O
(	O
which	O
is	O
scaled	O
by	O
the	O
overall	O
noise	O
level	O
σ2	O
)	O
.	O
we	O
can	O
summarize	O
this	O
prior	O
as	O
follows	O
:	O
w	O
)	O
,	O
where	O
σ2	O
(	O
cid:26	O
)	O
δ0	O
(	O
wj	O
)	O
n	O
(	O
wj|0	O
,	O
σ2σ2	O
w	O
)	O
if	O
γj	O
=	O
0	O
if	O
γj	O
=	O
1	O
(	O
13.11	O
)	O
w	O
→	O
∞	O
,	O
the	O
distribution	O
p	O
(	O
wj|γj	O
=	O
1	O
)	O
approaches	O
the	O
ﬁrst	O
term	O
is	O
a	O
“	O
spike	O
”	O
at	O
the	O
origin	O
.	O
as	O
σ2	O
a	O
uniform	B
distribution	I
,	O
which	O
can	O
be	O
thought	O
of	O
as	O
a	O
“	O
slab	O
”	O
of	O
constant	O
height	O
.	O
hence	O
this	O
is	O
called	O
the	O
spike	B
and	I
slab	I
model	O
(	O
mitchell	O
and	O
beauchamp	O
1988	O
)	O
.	O
we	O
can	O
drop	O
the	O
coefficients	O
wj	O
for	O
which	O
wj	O
=	O
0	O
from	O
the	O
model	O
,	O
since	O
they	O
are	O
clamped	O
to	O
zero	O
under	O
the	O
prior	O
.	O
hence	O
equation	O
13.10	O
becomes	O
the	O
following	O
(	O
assuming	O
a	O
gaussian	O
likelihood	B
)	O
:	O
p	O
(	O
wj|σ2	O
,	O
γj	O
)	O
=	O
p	O
(	O
d|γ	O
)	O
=	O
n	O
(	O
y|xγ	O
wγ	O
,	O
σ2in	O
)	O
n	O
(	O
wγ|0dγ	O
,	O
σ2σ2	O
widγ	O
)	O
p	O
(	O
σ2	O
)	O
dwγdσ2	O
(	O
13.12	O
)	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
13.2.	O
bayesian	O
variable	O
selection	O
425	O
where	O
dγ	O
=	O
||γ||0	O
is	O
the	O
number	O
of	O
non-zero	O
elements	O
in	O
γ.	O
in	O
what	O
follows	O
,	O
we	O
will	O
generalize	B
this	O
slightly	O
by	O
deﬁning	O
a	O
prior	O
of	O
the	O
form	O
p	O
(	O
w|γ	O
,	O
σ2	O
)	O
=n	O
(	O
wγ|0dγ	O
,	O
σ2σγ	O
)	O
for	O
any	O
positive	B
deﬁnite	I
matrix	O
σγ.1	O
given	O
these	O
priors	O
,	O
we	O
can	O
now	O
compute	O
the	O
marginal	B
likelihood	I
.	O
if	O
the	O
noise	O
variance	O
is	O
known	O
,	O
we	O
can	O
write	O
down	O
the	O
marginal	B
likelihood	I
(	O
using	O
equation	O
13.151	O
)	O
as	O
follows	O
:	O
p	O
(	O
d|γ	O
,	O
σ2	O
)	O
=	O
n	O
(	O
y|xγwγ	O
,	O
σ2i	O
)	O
n	O
(	O
wγ|0	O
,	O
σ2σγ	O
)	O
dwγ	O
=	O
n	O
(	O
y|0	O
,	O
cγ	O
)	O
(	O
13.13	O
)	O
(	O
cid:28	O
)	O
cγ	O
(	O
cid:2	O
)	O
σ2xγσγxt	O
γ	O
+	O
σ2in	O
(	O
13.14	O
)	O
if	O
the	O
noise	O
is	O
unknown	B
,	O
we	O
can	O
put	O
a	O
prior	O
on	O
it	O
and	O
integrate	O
it	O
out	O
.	O
it	O
is	O
common	O
to	O
use	O
p	O
(	O
σ2	O
)	O
=	O
ig	O
(	O
σ2|aσ	O
,	O
bσ	O
)	O
.	O
some	O
guidelines	O
on	O
setting	O
a	O
,	O
b	O
can	O
be	O
found	O
in	O
(	O
kohn	O
et	O
al	O
.	O
2001	O
)	O
.	O
if	O
we	O
use	O
a	O
=	O
b	O
=	O
0	O
,	O
we	O
recover	O
the	O
jeffrey	O
’	O
s	O
prior	O
,	O
p	O
(	O
σ2	O
)	O
∝	O
σ−2	O
.	O
when	O
we	O
integrate	B
out	I
the	O
noise	O
,	O
we	O
get	O
the	O
following	O
more	O
complicated	O
expression	O
for	O
the	O
marginal	B
likelihood	I
(	O
brown	O
et	O
al	O
.	O
1998	O
)	O
:	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
p	O
(	O
d|γ	O
)	O
=	O
p	O
(	O
y|γ	O
,	O
wγ	O
,	O
σ2	O
)	O
p	O
(	O
wγ|γ	O
,	O
σ2	O
)	O
p	O
(	O
σ2	O
)	O
dwγdσ2	O
∝	O
|xt	O
where	O
s	O
(	O
γ	O
)	O
is	O
the	O
rss	O
:	O
γ	O
xγ	O
+	O
σ−1	O
γ	O
|−	O
1	O
2|σγ|−	O
1	O
2	O
(	O
2bσ	O
+	O
s	O
(	O
γ	O
)	O
)	O
−	O
(	O
2aσ+n−1	O
)	O
/2	O
s	O
(	O
γ	O
)	O
(	O
cid:2	O
)	O
yt	O
y	O
−	O
yt	O
xγ	O
(	O
xt	O
γ	O
xγ	O
+	O
σ−1	O
γ	O
)	O
−1xt	O
γ	O
y	O
(	O
13.15	O
)	O
(	O
13.16	O
)	O
(	O
13.17	O
)	O
see	O
also	O
exercise	O
13.4.	O
when	O
the	O
marginal	B
likelihood	I
can	O
not	O
be	O
computed	O
in	O
closed	O
form	O
(	O
e.g.	O
,	O
if	O
we	O
are	O
using	O
logistic	B
regression	I
or	O
a	O
nonlinear	O
model	O
)	O
,	O
we	O
can	O
approximate	O
it	O
using	O
bic	O
,	O
which	O
has	O
the	O
form	O
log	O
p	O
(	O
d|γ	O
)	O
≈	O
log	O
p	O
(	O
y|x	O
,	O
ˆwγ	O
,	O
ˆσ2	O
)	O
−	O
||γ||0	O
(	O
13.18	O
)	O
where	O
ˆwγ	O
is	O
the	O
ml	O
or	O
map	O
estimate	O
based	O
on	O
xγ	O
,	O
and	O
||γ||0	O
is	O
the	O
“	O
degrees	B
of	I
freedom	I
”	O
of	O
the	O
model	O
(	O
zou	O
et	O
al	O
.	O
2007	O
)	O
.	O
adding	O
the	O
log	O
prior	O
,	O
the	O
overall	O
objective	O
becomes	O
2	O
log	O
n	O
log	O
p	O
(	O
γ|d	O
)	O
≈	O
log	O
p	O
(	O
y|x	O
,	O
ˆwγ	O
,	O
ˆσ2	O
)	O
−	O
||γ||0	O
2	O
log	O
n	O
−	O
λ||γ||0	O
+	O
const	O
(	O
13.19	O
)	O
we	O
see	O
that	O
there	O
are	O
two	O
complexity	O
penalties	O
:	O
one	O
arising	O
from	O
the	O
bic	O
approximation	O
to	O
the	O
marginal	B
likelihood	I
,	O
and	O
the	O
other	O
arising	O
from	O
the	O
prior	O
on	O
p	O
(	O
γ	O
)	O
.	O
obviously	O
these	O
can	O
be	O
combined	O
into	O
one	O
overall	O
complexity	O
parameter	B
,	O
which	O
we	O
will	O
denote	O
by	O
λ	O
.	O
13.2.2	O
from	O
the	O
bernoulli-gaussian	O
model	O
to	O
(	O
cid:9	O
)	O
0	O
regularization	B
another	O
model	O
that	O
is	O
sometimes	O
used	O
(	O
e.g.	O
,	O
(	O
kuo	O
and	O
mallick	O
1998	O
;	O
zhou	O
et	O
al	O
.	O
2009	O
;	O
soussen	O
et	O
al	O
.	O
2010	O
)	O
)	O
is	O
the	O
following	O
:	O
yi|xi	O
,	O
w	O
,	O
γ	O
,	O
σ2	O
∼	O
n	O
(	O
γjwjxij	O
,	O
σ2	O
)	O
(	O
cid:2	O
)	O
(	O
13.20	O
)	O
j	O
γj	O
∼	O
ber	O
(	O
π0	O
)	O
wj	O
∼	O
n	O
(	O
0	O
,	O
σ2	O
w	O
)	O
(	O
13.21	O
)	O
(	O
13.22	O
)	O
−1	O
for	O
reasons	O
explained	O
in	O
section	O
7.6.3.1	O
(	O
see	O
also	O
1.	O
it	O
is	O
common	O
to	O
use	O
a	O
g-prior	B
of	O
the	O
form	O
σγ	O
=	O
g	O
(	O
xt	O
exercise	O
13.4	O
)	O
.	O
various	O
approaches	O
have	O
been	O
proposed	O
for	O
setting	O
g	O
,	O
including	O
cross	B
validation	I
,	O
empirical	O
bayes	O
(	O
minka	O
2000b	O
;	O
george	O
and	O
foster	O
2000	O
)	O
,	O
hierarchical	O
bayes	O
(	O
liang	O
et	O
al	O
.	O
2008	O
)	O
,	O
etc	O
.	O
γ	O
xγ	O
)	O
426	O
chapter	O
13.	O
sparse	B
linear	O
models	O
(	O
soussen	O
et	O
al	O
.	O
2010	O
)	O
)	O
,	O
this	O
is	O
called	O
the	O
bernoulli-	O
in	O
the	O
signal	B
processing	I
literature	O
(	O
e.g.	O
,	O
gaussian	O
model	O
,	O
although	O
we	O
could	O
also	O
call	O
it	O
the	O
binary	B
mask	I
model	O
,	O
since	O
we	O
can	O
think	O
of	O
the	O
γj	O
variables	O
as	O
“	O
masking	O
out	O
”	O
the	O
weights	O
wj	O
.	O
unlike	O
the	O
spike	B
and	I
slab	I
model	O
,	O
we	O
do	O
not	O
integrate	B
out	I
the	O
“	O
irrelevant	O
”	O
coefficients	O
;	O
they	O
always	O
exist	O
.	O
in	O
addition	O
,	O
the	O
binary	B
mask	I
model	O
has	O
the	O
form	O
γj	O
→	O
y	O
←	O
wj	O
,	O
whereas	O
the	O
spike	B
and	I
slab	I
model	O
has	O
the	O
form	O
γj	O
→	O
wj	O
→	O
y.	O
in	O
the	O
binary	B
mask	I
model	O
,	O
only	O
the	O
product	O
γjwj	O
can	O
be	O
identiﬁed	O
from	O
the	O
likelihood	B
.	O
one	O
interesting	O
aspect	O
of	O
this	O
model	O
is	O
that	O
it	O
can	O
be	O
used	O
to	O
derive	O
an	O
objective	O
function	O
that	O
is	O
widely	O
used	O
in	O
the	O
(	O
non-bayesian	O
)	O
subset	O
selection	O
literature	O
.	O
first	O
,	O
note	O
that	O
the	O
joint	O
prior	O
has	O
the	O
form	O
p	O
(	O
γ	O
,	O
w	O
)	O
∝	O
n	O
(	O
w|0	O
,	O
σ2	O
wi	O
)	O
π	O
||γ||0	O
0	O
(	O
1	O
−	O
π0	O
)	O
d−||γ||0	O
hence	O
the	O
scaled	O
unnormalized	O
negative	O
log	O
posterior	O
has	O
the	O
form	O
f	O
(	O
γ	O
,	O
w	O
)	O
(	O
cid:2	O
)	O
−2σ2	O
log	O
p	O
(	O
γ	O
,	O
w	O
,	O
y|x	O
)	O
=||y	O
−	O
x	O
(	O
γ	O
.	O
∗	O
w	O
)	O
||2	O
||w||2	O
+	O
λ||γ||0	O
+	O
const	O
+	O
σ2	O
σ2	O
w	O
where	O
λ	O
(	O
cid:2	O
)	O
2σ2	O
log	O
(	O
1	O
−	O
π0	O
π0	O
)	O
(	O
13.23	O
)	O
(	O
13.24	O
)	O
(	O
13.25	O
)	O
let	O
us	O
split	O
w	O
into	O
two	O
subvectors	O
,	O
w−γ	O
and	O
wγ	O
,	O
indexed	O
by	O
the	O
zero	O
and	O
non-zero	O
entries	O
of	O
γ	O
respectively	O
.	O
since	O
x	O
(	O
γ	O
.	O
∗	O
w	O
)	O
=	O
xγwγ	O
,	O
we	O
can	O
just	O
set	O
w−γ	O
=	O
0.	O
w	O
→	O
∞	O
,	O
so	O
we	O
do	O
not	O
regularize	O
the	O
non-zero	O
weights	O
(	O
so	O
there	O
is	O
no	O
complexity	O
penalty	O
coming	O
from	O
the	O
marginal	B
likelihood	I
or	O
its	O
bic	O
approximation	O
)	O
.	O
in	O
this	O
case	O
,	O
the	O
objective	O
becomes	O
now	O
consider	O
the	O
case	O
where	O
σ2	O
(	O
13.26	O
)	O
f	O
(	O
γ	O
,	O
w	O
)	O
=	O
||y	O
−	O
xγ	O
wγ||2	O
2	O
+	O
λ||γ||0	O
this	O
is	O
similar	B
to	O
the	O
bic	O
objective	O
above	O
.	O
instead	O
of	O
keeping	O
track	O
of	O
the	O
bit	O
vector	O
γ	O
,	O
we	O
can	O
deﬁne	O
the	O
set	O
of	O
relevant	O
variables	O
to	O
be	O
the	O
support	B
,	O
or	O
set	O
of	O
non-zero	O
entries	O
,	O
of	O
w.	O
then	O
we	O
can	O
rewrite	O
the	O
above	O
equation	O
as	O
follows	O
:	O
f	O
(	O
w	O
)	O
=	O
||y	O
−	O
xw||2	O
2	O
+	O
λ||w||0	O
(	O
13.27	O
)	O
this	O
is	O
called	O
(	O
cid:6	O
)	O
0	O
regularization	B
.	O
we	O
have	O
converted	O
the	O
discrete	B
optimization	O
problem	O
(	O
over	O
γ	O
∈	O
{	O
0	O
,	O
1	O
}	O
d	O
)	O
into	O
a	O
continuous	O
one	O
(	O
over	O
w	O
∈	O
r	O
d	O
)	O
;	O
however	O
,	O
the	O
(	O
cid:6	O
)	O
0	O
pseudo-norm	O
makes	O
the	O
objective	O
very	O
non	O
smooth	O
,	O
so	O
this	O
is	O
still	O
hard	O
to	O
optimize	O
.	O
we	O
will	O
discuss	O
different	O
solutions	O
to	O
this	O
in	O
the	O
rest	O
of	O
this	O
chapter	O
.	O
13.2.3	O
algorithms	O
since	O
there	O
are	O
2d	O
models	O
,	O
we	O
can	O
not	O
explore	O
the	O
full	B
posterior	O
,	O
or	O
ﬁnd	O
the	O
globally	O
optimal	O
model	O
.	O
instead	O
we	O
will	O
have	O
to	O
resort	O
to	O
heuristics	B
of	O
one	O
form	O
or	O
another	O
.	O
all	O
of	O
the	O
methods	O
we	O
will	O
discuss	O
involve	O
searching	O
through	O
the	O
space	O
of	O
models	O
,	O
and	O
evaluating	O
the	O
cost	O
f	O
(	O
γ	O
)	O
at	O
13.2.	O
bayesian	O
variable	O
selection	O
427	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
}	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
{	O
2	O
,	O
3	O
,	O
4	O
}	O
{	O
1	O
,	O
3	O
,	O
4	O
}	O
{	O
1	O
,	O
2	O
,	O
4	O
}	O
{	O
1	O
,	O
2	O
}	O
{	O
1	O
,	O
3	O
}	O
{	O
1	O
,	O
4	O
}	O
{	O
2	O
,	O
3	O
}	O
{	O
2	O
,	O
4	O
}	O
{	O
3	O
,	O
4	O
}	O
{	O
1	O
}	O
{	O
2	O
}	O
{	O
3	O
}	O
{	O
4	O
}	O
all	O
subsets	O
on	O
prostate	O
cancer	O
1.4	O
1.2	O
1	O
0.8	O
0.6	O
r	O
o	O
r	O
r	O
e	O
t	O
e	O
s	O
i	O
g	O
n	O
n	O
a	O
r	O
t	O
i	O
{	O
}	O
(	O
a	O
)	O
0.4	O
0	O
1	O
2	O
3	O
4	O
subset	O
size	O
5	O
6	O
7	O
8	O
(	O
b	O
)	O
figure	O
13.2	O
(	O
a	O
)	O
a	O
lattice	B
of	O
subsets	O
of	O
{	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
}	O
.	O
(	O
b	O
)	O
residual	B
sum	I
of	I
squares	I
versus	O
subset	O
size	O
,	O
on	O
the	O
prostate	O
cancer	O
data	O
set	O
.	O
the	O
lower	O
envelope	O
is	O
the	O
best	O
rss	O
achievable	O
for	O
any	O
set	O
of	O
a	O
given	O
size	O
.	O
based	O
on	O
figure	O
3.5	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2001	O
)	O
.	O
figure	O
generated	O
by	O
prostatesubsets.	O
)	O
each	O
point	O
.	O
this	O
requires	O
ﬁtting	O
the	O
model	O
(	O
i.e.	O
,	O
computing	O
argmax	O
p	O
(	O
d|w	O
)	O
)	O
,	O
or	O
evaluating	O
its	O
p	O
(	O
d|w	O
)	O
p	O
(	O
w	O
)	O
dw	O
)	O
at	O
each	O
step	O
.	O
this	O
is	O
sometimes	O
called	O
marginal	B
likelihood	I
(	O
i.e.	O
,	O
computing	O
the	O
wrapper	B
method	I
,	O
since	O
we	O
“	O
wrap	O
”	O
our	O
search	O
for	O
the	O
best	O
model	O
(	O
or	O
set	O
of	O
good	O
models	O
)	O
around	O
a	O
generic	O
model-ﬁtting	O
procedure	O
.	O
in	O
order	O
to	O
make	O
wrapper	O
methods	O
efficient	O
,	O
it	O
is	O
important	O
that	O
we	O
can	O
quickly	O
evaluate	O
the	O
score	B
function	I
for	O
some	O
new	O
model	O
,	O
γ	O
(	O
cid:4	O
)	O
,	O
given	O
the	O
score	O
of	O
a	O
previous	O
model	O
,	O
γ.	O
this	O
can	O
be	O
done	O
provided	O
we	O
can	O
efficiently	O
update	O
the	O
sufficient	B
statistics	I
needed	O
to	O
compute	O
f	O
(	O
γ	O
)	O
.	O
this	O
is	O
possible	O
provided	O
γ	O
(	O
cid:4	O
)	O
only	O
differs	O
from	O
γ	O
in	O
one	O
bit	O
(	O
corresponding	O
to	O
adding	O
or	O
removing	O
a	O
single	O
variable	O
)	O
,	O
and	O
provided	O
f	O
(	O
γ	O
)	O
only	O
depends	O
on	O
the	O
data	O
via	O
xγ	O
.	O
in	O
this	O
case	O
,	O
we	O
can	O
use	O
rank-one	O
matrix	O
updates/	O
downdates	O
to	O
efficiently	O
compute	O
xt	O
γ	O
xγ	O
.	O
these	O
updates	O
are	O
usually	O
applied	O
to	O
the	O
qr	O
decomposition	O
of	O
x.	O
see	O
e.g.	O
,	O
(	O
miller	O
2002	O
;	O
schniter	O
et	O
al	O
.	O
2008	O
)	O
for	O
details	O
.	O
γ	O
(	O
cid:2	O
)	O
xγ	O
(	O
cid:2	O
)	O
from	O
xt	O
13.2.3.1	O
greedy	O
search	O
suppose	O
we	O
want	O
to	O
ﬁnd	O
the	O
map	O
model	O
.	O
if	O
we	O
use	O
the	O
(	O
cid:6	O
)	O
0-regularized	O
objective	O
in	O
equation	O
13.27	O
,	O
we	O
can	O
exploit	O
properties	O
of	O
least	B
squares	I
to	O
derive	O
various	O
efficient	O
greedy	O
forwards	O
search	O
methods	O
,	O
some	O
of	O
which	O
we	O
summarize	O
below	O
.	O
for	O
further	O
details	O
,	O
see	O
(	O
miller	O
2002	O
;	O
soussen	O
et	O
al	O
.	O
2010	O
)	O
.	O
•	O
single	B
best	I
replacement	I
the	O
simplest	O
method	O
is	O
to	O
use	O
greedy	O
hill	O
climbing	O
,	O
where	O
at	O
each	O
step	O
,	O
we	O
deﬁne	O
the	O
neighborhood	O
of	O
the	O
current	O
model	O
to	O
be	O
all	O
models	O
than	O
can	O
be	O
reached	O
by	O
ﬂipping	O
a	O
single	O
bit	O
of	O
γ	O
,	O
i.e.	O
,	O
for	O
each	O
variable	O
,	O
if	O
it	O
is	O
currently	O
out	O
of	O
the	O
model	O
,	O
we	O
consider	O
adding	O
it	O
,	O
and	O
if	O
it	O
is	O
currently	O
in	O
the	O
model	O
,	O
we	O
consider	O
removing	O
it	O
.	O
in	O
(	O
soussen	O
et	O
al	O
.	O
2010	O
)	O
,	O
they	O
call	O
this	O
the	O
single	B
best	I
replacement	I
(	O
sbr	O
)	O
.	O
since	O
we	O
are	O
expecting	O
a	O
sparse	B
solution	O
,	O
we	O
can	O
start	O
with	O
the	O
empty	O
set	O
,	O
γ	O
=	O
0.	O
we	O
are	O
essentially	O
moving	O
through	O
the	O
lattice	B
of	O
subsets	O
,	O
shown	O
in	O
figure	O
13.2	O
(	O
a	O
)	O
.	O
we	O
continue	O
adding	O
or	O
removing	O
until	O
no	O
improvement	O
is	O
possible	O
.	O
•	O
orthogonal	B
least	I
squares	I
if	O
we	O
set	O
λ	O
=	O
0	O
in	O
equation	O
13.27	O
,	O
so	O
there	O
is	O
no	O
complexity	O
penalty	O
,	O
there	O
will	O
be	O
no	O
reason	O
to	O
perform	O
deletion	O
steps	O
.	O
in	O
this	O
case	O
,	O
the	O
sbr	O
algorithm	O
is	O
equivalent	O
to	O
orthogonal	B
least	I
squares	I
(	O
chen	O
and	O
wigger	O
1995	O
)	O
,	O
which	O
in	O
turn	O
is	O
equivalent	O
428	O
chapter	O
13.	O
sparse	B
linear	O
models	O
to	O
greedy	O
forwards	O
selection	O
.	O
in	O
this	O
algorithm	O
,	O
we	O
start	O
with	O
the	O
empty	O
set	O
and	O
add	O
the	O
best	O
feature	O
at	O
each	O
step	O
.	O
the	O
error	O
will	O
go	O
down	O
monotonically	O
with	O
||γ||0	O
,	O
as	O
shown	O
in	O
figure	O
13.2	O
(	O
b	O
)	O
.	O
we	O
can	O
pick	O
the	O
next	O
best	O
feature	O
j∗	O
to	O
add	O
to	O
the	O
current	O
set	O
γt	O
by	O
solving	O
j∗	O
||y	O
−	O
(	O
xγ	O
t∪j	O
)	O
w||2	O
min	O
w	O
=	O
arg	O
min	O
j	O
(	O
cid:7	O
)	O
∈γt	O
(	O
13.28	O
)	O
we	O
then	O
update	O
the	O
active	B
set	I
by	O
setting	O
γ	O
(	O
t+1	O
)	O
=	O
γ	O
(	O
t	O
)	O
∪	O
{	O
j∗	O
}	O
.	O
to	O
choose	O
the	O
next	O
feature	O
to	O
add	O
at	O
step	O
t	O
,	O
we	O
need	O
to	O
solve	O
d	O
−	O
dt	O
least	B
squares	I
problems	O
at	O
step	O
t	O
,	O
where	O
dt	O
=	O
|γt|	O
is	O
the	O
cardinality	O
of	O
the	O
current	O
active	B
set	I
.	O
having	O
chosen	O
the	O
best	O
feature	O
to	O
add	O
,	O
we	O
need	O
to	O
solve	O
an	O
additional	O
least	B
squares	I
problem	O
to	O
compute	O
wt+1	O
)	O
.	O
•	O
orthogonal	B
matching	I
pursuits	I
orthogonal	O
least	B
squares	I
is	O
somewhat	O
expensive	O
.	O
a	O
simpli-	O
ﬁcation	O
is	O
to	O
“	O
freeze	O
”	O
the	O
current	O
weights	O
at	O
their	O
current	O
value	O
,	O
and	O
then	O
to	O
pick	O
the	O
next	O
feature	O
to	O
add	O
by	O
solving	O
j∗	O
=	O
arg	O
min	O
j	O
(	O
cid:7	O
)	O
∈γt	O
min	O
β	O
||y	O
−	O
xwt	O
−	O
βx	O
:	O
,j||2	O
(	O
13.29	O
)	O
:	O
,jrt/||x	O
:	O
,j||2	O
,	O
where	O
rt	O
=	O
this	O
inner	O
optimization	O
is	O
easy	O
to	O
solve	O
:	O
we	O
simply	O
set	O
β	O
=	O
xt	O
y	O
−	O
xwt	O
is	O
the	O
current	O
residual	B
vector	O
.	O
if	O
the	O
columns	O
are	O
unit	O
norm	O
,	O
we	O
have	O
j∗	O
=	O
arg	O
max	O
xt	O
:	O
,jrt	O
(	O
13.30	O
)	O
so	O
we	O
are	O
just	O
looking	O
for	O
the	O
column	O
that	O
is	O
most	O
correlated	O
with	O
the	O
current	O
residual	B
.	O
we	O
then	O
update	O
the	O
active	B
set	I
,	O
and	O
compute	O
the	O
new	O
least	B
squares	I
estimate	O
wt+1	O
using	O
xγ	O
t+1	O
.	O
this	O
method	O
is	O
called	O
orthogonal	B
matching	I
pursuits	I
or	O
omp	O
(	O
mallat	O
et	O
al	O
.	O
1994	O
)	O
.	O
this	O
only	O
requires	O
one	O
least	B
squares	I
calculation	O
per	O
iteration	O
and	O
so	O
is	O
faster	O
than	O
orthogonal	B
least	I
squares	I
,	O
but	O
is	O
not	O
quite	O
as	O
accurate	O
(	O
blumensath	O
and	O
davies	O
2007	O
)	O
.	O
•	O
matching	B
pursuits	I
an	O
even	O
more	O
aggressive	O
approximation	O
is	O
to	O
just	O
greedily	O
add	O
the	O
feature	O
that	O
is	O
most	O
correlated	O
with	O
the	O
current	O
residual	B
.	O
this	O
is	O
called	O
matching	B
pursuits	I
(	O
mallat	O
and	O
zhang	O
1993	O
)	O
.	O
this	O
is	O
also	O
equivalent	O
to	O
a	O
method	O
known	O
as	O
least	B
squares	I
boosting	I
(	O
section	O
16.4.6	O
)	O
.	O
•	O
backwards	O
selection	O
backwards	O
selection	O
starts	O
with	O
all	O
variables	O
in	O
the	O
model	O
(	O
the	O
so-	O
called	O
saturated	B
model	I
)	O
,	O
and	O
then	O
deletes	O
the	O
worst	O
one	O
at	O
each	O
step	O
.	O
this	O
is	O
equivalent	O
to	O
performing	O
a	O
greedy	O
search	O
from	O
the	O
top	O
of	O
the	O
lattice	B
downwards	O
.	O
this	O
can	O
give	O
better	O
results	O
than	O
a	O
bottom-up	O
search	O
,	O
since	O
the	O
decision	B
about	O
whether	O
to	O
keep	O
a	O
variable	O
or	O
not	O
is	O
made	O
in	O
the	O
context	O
of	O
all	O
the	O
other	O
variables	O
that	O
might	O
depende	O
on	O
it	O
.	O
however	O
,	O
this	O
method	O
is	O
typically	O
infeasible	O
for	O
large	O
problems	O
,	O
since	O
the	O
saturated	B
model	I
will	O
be	O
too	O
expensive	O
to	O
ﬁt	O
.	O
•	O
foba	O
the	O
forwards-backwards	B
algorithm	I
of	O
(	O
zhang	O
2008	O
)	O
is	O
similar	B
to	O
the	O
single	B
best	I
replacement	I
algorithm	O
presented	O
above	O
,	O
except	O
it	O
uses	O
an	O
omp-like	O
approximation	O
when	O
choosing	O
the	O
next	O
move	O
to	O
make	O
.	O
a	O
similar	B
“	O
dual-pass	O
”	O
algorithm	O
was	O
described	O
in	O
(	O
moghad-	O
dam	O
et	O
al	O
.	O
2008	O
)	O
.	O
•	O
bayesian	O
matching	B
pursuit	I
the	O
algorithm	O
of	O
(	O
schniter	O
et	O
al	O
.	O
2008	O
)	O
is	O
similiar	O
to	O
omp	O
except	O
it	O
uses	O
a	O
bayesian	O
marginal	B
likelihood	I
scoring	O
criterion	O
(	O
under	O
a	O
spike	B
and	I
slab	I
model	O
)	O
instead	O
of	O
a	O
least	B
squares	I
objective	O
.	O
in	O
addition	O
,	O
it	O
uses	O
a	O
form	O
of	O
beam	B
search	I
to	O
explore	O
multiple	O
paths	O
through	O
the	O
lattice	B
at	O
once	O
.	O
13.3	O
.	O
(	O
cid:6	O
)	O
1	O
regularization	B
:	O
basics	O
429	O
13.2.3.2	O
stochastic	B
search	I
if	O
we	O
want	O
to	O
approximate	O
the	O
posterior	O
,	O
rather	O
than	O
just	O
computing	O
a	O
mode	B
(	O
e.g	O
.	O
because	O
we	O
want	O
to	O
compute	O
marginal	O
inclusion	O
probabilities	O
)	O
,	O
one	O
option	O
is	O
to	O
use	O
mcmc	O
.	O
the	O
standard	O
approach	O
is	O
to	O
use	O
metropolis	O
hastings	O
,	O
where	O
the	O
proposal	B
distribution	I
just	O
ﬂips	O
single	O
bits	O
.	O
this	O
enables	O
us	O
to	O
efficiently	O
compute	O
p	O
(	O
γ	O
(	O
cid:4	O
)	O
|d	O
)	O
given	O
p	O
(	O
γ|d	O
)	O
.	O
the	O
probability	O
of	O
a	O
state	B
(	O
bit	O
conﬁguration	O
)	O
is	O
estimated	O
by	O
counting	O
how	O
many	O
times	O
the	O
random	O
walk	O
visits	O
this	O
state	B
.	O
see	O
(	O
o	O
’	O
hara	O
and	O
sillanpaa	O
2009	O
)	O
for	O
a	O
review	O
of	O
such	O
methods	O
,	O
and	O
(	O
bottolo	O
and	O
richardson	O
2010	O
)	O
for	O
a	O
very	O
recent	O
method	O
based	O
on	O
evolutionary	O
mcmc	O
.	O
however	O
,	O
in	O
a	O
discrete	B
state	O
space	O
,	O
mcmc	O
is	O
needlessly	O
inefficient	O
,	O
since	O
we	O
can	O
compute	O
the	O
(	O
unnormalized	O
)	O
probability	O
of	O
a	O
state	B
directly	O
using	O
p	O
(	O
γ	O
,	O
d	O
)	O
=	O
exp	O
(	O
−f	O
(	O
γ	O
)	O
)	O
;	O
thus	O
there	O
is	O
no	O
need	O
to	O
ever	O
revisit	O
a	O
state	B
.	O
a	O
much	O
more	O
efficient	O
alternative	O
is	O
to	O
use	O
some	O
kind	O
of	O
stochastic	B
search	I
algorithm	O
,	O
to	O
generate	O
a	O
set	O
s	O
of	O
high	O
scoring	O
models	O
,	O
and	O
then	O
to	O
make	O
the	O
following	O
approximation	O
p	O
(	O
γ|d	O
)	O
≈	O
(	O
cid:10	O
)	O
e−f	O
(	O
γ	O
)	O
γ	O
(	O
cid:2	O
)	O
∈s	O
e−f	O
(	O
γ	O
(	O
cid:2	O
)	O
)	O
(	O
13.31	O
)	O
see	O
(	O
heaton	O
and	O
scott	O
2009	O
)	O
for	O
a	O
review	O
of	O
recent	O
methods	O
of	O
this	O
kind	O
.	O
13.2.3.3	O
em	O
and	O
variational	B
inference	I
*	O
it	O
is	O
tempting	O
to	O
apply	O
em	O
to	O
the	O
spike	B
and	I
slab	I
model	O
,	O
which	O
has	O
the	O
form	O
γj	O
→	O
wj	O
→	O
y.	O
we	O
can	O
compute	O
p	O
(	O
γj	O
=	O
1|wj	O
)	O
in	O
the	O
e	O
step	O
,	O
and	O
optimize	O
w	O
in	O
the	O
m	O
step	O
.	O
however	O
,	O
this	O
will	O
not	O
work	O
,	O
because	O
when	O
we	O
compute	O
p	O
(	O
γj	O
=	O
1|wj	O
)	O
,	O
we	O
are	O
comparing	O
a	O
delta-function	O
,	O
δ0	O
(	O
wj	O
)	O
,	O
with	O
a	O
gaussian	O
pdf	B
,	O
n	O
(	O
wj|0	O
,	O
σ2	O
w	O
)	O
.	O
we	O
can	O
replace	O
the	O
delta	O
function	O
with	O
a	O
narrow	O
gaussian	O
,	O
and	O
then	O
the	O
e	O
step	O
amounts	O
to	O
classifying	O
wj	O
under	O
the	O
two	O
possible	O
gaussian	O
models	O
.	O
however	O
,	O
this	O
is	O
likely	O
to	O
suffer	O
from	O
severe	O
local	O
minima	O
.	O
an	O
alternative	O
is	O
to	O
apply	O
em	O
to	O
the	O
bernoulli-gaussian	O
model	O
,	O
which	O
has	O
the	O
form	O
γj	O
→	O
y	O
←	O
wj	O
.	O
in	O
this	O
case	O
,	O
the	O
posterior	O
p	O
(	O
γ|d	O
,	O
w	O
)	O
is	O
intractable	O
to	O
compute	O
because	O
all	O
the	O
bits	B
become	O
correlated	O
due	O
to	O
explaining	B
away	I
.	O
however	O
,	O
it	O
is	O
possible	O
to	O
derive	O
a	O
mean	B
ﬁeld	I
approximation	O
of	O
the	O
form	O
’	O
j	O
q	O
(	O
γj	O
)	O
q	O
(	O
wj	O
)	O
(	O
huang	O
et	O
al	O
.	O
2007	O
;	O
rattray	O
et	O
al	O
.	O
2009	O
)	O
.	O
13.3	O
(	O
cid:9	O
)	O
1	O
regularization	B
:	O
basics	O
when	O
we	O
have	O
many	O
variables	O
,	O
it	O
is	O
computationally	O
difficult	O
to	O
ﬁnd	O
the	O
posterior	B
mode	I
of	O
p	O
(	O
γ|d	O
)	O
.	O
and	O
although	O
greedy	O
algorithms	O
often	O
work	O
well	O
(	O
see	O
e.g.	O
,	O
(	O
zhang	O
2008	O
)	O
for	O
a	O
theoretical	O
analysis	O
)	O
,	O
they	O
can	O
of	O
course	O
get	O
stuck	O
in	O
local	O
optima	O
.	O
part	O
of	O
the	O
problem	O
is	O
due	O
to	O
the	O
fact	O
that	O
the	O
γj	O
variables	O
are	O
discrete	B
,	O
γj	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
in	O
the	O
optimization	B
community	O
,	O
it	O
is	O
common	O
to	O
relax	O
hard	O
constraints	O
of	O
this	O
form	O
by	O
replacing	O
discrete	B
variables	O
with	O
continuous	O
variables	O
.	O
we	O
can	O
do	O
this	O
by	O
replacing	O
the	O
spike-and-slab	O
style	O
prior	O
,	O
that	O
assigns	O
ﬁnite	O
probability	O
mass	O
to	O
the	O
event	O
that	O
wj	O
=	O
0	O
,	O
to	O
continuous	O
priors	O
that	O
“	O
encourage	O
”	O
wj	O
=	O
0	O
by	O
putting	O
a	O
lot	O
of	O
probability	O
density	O
near	O
the	O
origin	O
,	O
such	O
as	O
a	O
zero-mean	O
laplace	O
distribution	O
.	O
this	O
was	O
ﬁrst	O
introduced	O
in	O
section	O
7.4	O
in	O
the	O
context	O
of	O
robust	B
linear	O
regression	B
.	O
there	O
we	O
exploited	O
the	O
fact	O
that	O
the	O
laplace	O
has	O
heavy	B
tails	I
.	O
here	O
we	O
exploit	O
the	O
fact	O
430	O
chapter	O
13.	O
sparse	B
linear	O
models	O
figure	O
13.3	O
illustration	O
of	O
(	O
cid:7	O
)	O
1	O
(	O
left	O
)	O
vs	O
(	O
cid:7	O
)	O
2	O
(	O
right	O
)	O
regularization	B
of	O
a	O
least	B
squares	I
problem	O
.	O
based	O
on	O
figure	O
3.12	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2001	O
)	O
.	O
that	O
it	O
has	O
a	O
spike	O
near	O
μ	O
=	O
0.	O
more	O
precisely	O
,	O
consider	O
a	O
prior	O
of	O
the	O
form	O
d	O
(	O
cid:27	O
)	O
lap	O
(	O
wj|0	O
,	O
1/λ	O
)	O
∝	O
d	O
(	O
cid:27	O
)	O
p	O
(	O
w|λ	O
)	O
=	O
e−λ|wj|	O
(	O
13.32	O
)	O
j=1	O
j=1	O
we	O
will	O
use	O
a	O
uniform	O
prior	O
on	O
the	O
offset	O
term	O
,	O
p	O
(	O
w0	O
)	O
∝	O
1.	O
let	O
us	O
perform	O
map	O
estimation	O
with	O
this	O
prior	O
.	O
the	O
penalized	O
negative	O
log	O
likelihood	O
has	O
the	O
form	O
f	O
(	O
w	O
)	O
=	O
−	O
log	O
p	O
(	O
d|w	O
)	O
−	O
log	O
p	O
(	O
w|λ	O
)	O
=	O
nll	O
(	O
w	O
)	O
+	O
λ||w||1	O
(	O
cid:10	O
)	O
d	O
(	O
13.33	O
)	O
j=1	O
|wj|	O
is	O
the	O
(	O
cid:6	O
)	O
1	O
norm	O
of	O
w.	O
for	O
suitably	O
large	O
λ	O
,	O
the	O
estimate	O
ˆw	O
will	O
be	O
where	O
||w||1	O
=	O
sparse	B
,	O
for	O
reasons	O
we	O
explain	O
below	O
.	O
indeed	O
,	O
this	O
can	O
be	O
thought	O
of	O
as	O
a	O
convex	B
approximation	O
to	O
the	O
non-convex	O
(	O
cid:6	O
)	O
0	O
objective	O
nll	O
(	O
w	O
)	O
+	O
λ||w||0	O
argmin	O
(	O
13.34	O
)	O
w	O
in	O
the	O
case	O
of	O
linear	B
regression	I
,	O
the	O
(	O
cid:6	O
)	O
1	O
objective	O
becomes	O
f	O
(	O
w	O
)	O
=	O
−	O
1	O
2σ2	O
(	O
yi	O
−	O
(	O
w0	O
+	O
wt	O
xi	O
)	O
)	O
2	O
+	O
λ||w||1	O
n	O
(	O
cid:2	O
)	O
i=1	O
=	O
rss	O
(	O
w	O
)	O
+	O
λ	O
(	O
cid:4	O
)	O
||w||1	O
where	O
λ	O
(	O
cid:4	O
)	O
=	O
2λσ2	O
.	O
this	O
method	O
is	O
known	O
as	O
basis	B
pursuit	I
denoising	I
or	O
bpdn	O
(	O
chen	O
et	O
al	O
.	O
1998	O
)	O
.	O
the	O
reason	O
for	O
this	O
term	O
will	O
become	O
clear	O
later	O
.	O
in	O
general	O
,	O
the	O
technique	O
of	O
putting	O
a	O
zero-mean	O
laplace	O
prior	O
on	O
the	O
parameters	O
and	O
performing	O
map	O
estimation	O
is	O
called	O
(	O
cid:6	O
)	O
1	O
regularization	B
.	O
it	O
can	O
be	O
combined	O
with	O
any	O
convex	B
or	O
non-convex	O
nll	O
term	O
.	O
many	O
different	O
algorithms	O
have	O
been	O
devised	O
for	O
solving	O
such	O
problems	O
,	O
some	O
of	O
which	O
we	O
review	O
in	O
section	O
13.4	O
.	O
(	O
13.36	O
)	O
(	O
13.35	O
)	O
13.3.1	O
why	O
does	O
(	O
cid:9	O
)	O
1	O
regularization	B
yield	O
sparse	B
solutions	O
?	O
we	O
now	O
explain	O
why	O
(	O
cid:6	O
)	O
1	O
regularization	B
results	O
in	O
sparse	B
solutions	O
,	O
whereas	O
(	O
cid:6	O
)	O
2	O
regularization	B
does	O
not	O
.	O
we	O
focus	O
on	O
the	O
case	O
of	O
linear	B
regression	I
,	O
although	O
similar	B
arguments	O
hold	O
for	O
logistic	B
regression	I
and	O
other	O
glms	O
.	O
13.3	O
.	O
(	O
cid:6	O
)	O
1	O
regularization	B
:	O
basics	O
the	O
objective	O
is	O
the	O
following	O
non-smooth	B
objective	O
function	O
:	O
rss	O
(	O
w	O
)	O
+λ||w	O
||1	O
min	O
w	O
431	O
(	O
13.37	O
)	O
we	O
can	O
rewrite	O
this	O
as	O
a	O
constrained	O
but	O
smooth	O
objective	O
(	O
a	O
quadratic	O
function	O
with	O
linear	O
constraints	O
)	O
:	O
min	O
w	O
rss	O
(	O
w	O
)	O
s.t	O
.	O
||w||1	O
≤	O
b	O
(	O
13.38	O
)	O
where	O
b	O
is	O
an	O
upper	O
bound	O
on	O
the	O
(	O
cid:6	O
)	O
1-norm	O
of	O
the	O
weights	O
:	O
a	O
small	O
(	O
tight	O
)	O
bound	O
b	O
corresponds	O
to	O
a	O
large	O
penalty	O
λ	O
,	O
and	O
vice	O
versa.2	O
equation	O
13.38	O
is	O
known	O
as	O
lasso	B
,	O
which	O
stands	O
for	O
“	O
least	O
absolute	O
shrinkage	B
and	O
selection	O
operator	O
”	O
(	O
tibshirani	O
1996	O
)	O
.	O
we	O
will	O
see	O
why	O
it	O
has	O
this	O
name	O
later	O
.	O
similarly	O
,	O
we	O
can	O
write	O
ridge	B
regression	I
rss	O
(	O
w	O
)	O
+λ||w	O
||2	O
2	O
min	O
w	O
or	O
as	O
a	O
bound	O
constrained	O
form	O
:	O
||w||2	O
rss	O
(	O
w	O
)	O
min	O
s.t	O
.	O
2	O
≤	O
b	O
w	O
(	O
13.39	O
)	O
(	O
13.40	O
)	O
in	O
figure	O
13.3	O
,	O
we	O
plot	O
the	O
contours	O
of	O
the	O
rss	O
objective	O
function	O
,	O
as	O
well	O
as	O
the	O
contours	O
of	O
the	O
(	O
cid:6	O
)	O
2	O
and	O
(	O
cid:6	O
)	O
1	O
constraint	O
surfaces	O
.	O
from	O
the	O
theory	O
of	O
constrained	O
optimization	B
,	O
we	O
know	O
that	O
the	O
optimal	O
solution	O
occurs	O
at	O
the	O
point	O
where	O
the	O
lowest	O
level	O
set	O
of	O
the	O
objective	O
function	O
intersects	O
the	O
constraint	O
surface	O
(	O
assuming	O
the	O
constraint	O
is	O
active	O
)	O
.	O
it	O
should	O
be	O
geometrically	O
clear	O
that	O
as	O
we	O
relax	O
the	O
constraint	O
b	O
,	O
we	O
“	O
grow	O
”	O
the	O
(	O
cid:6	O
)	O
1	O
“	O
ball	O
”	O
until	O
it	O
meets	O
the	O
objective	O
;	O
the	O
corners	O
of	O
the	O
ball	O
are	O
more	O
likely	O
to	O
intersect	O
the	O
ellipse	O
than	O
one	O
of	O
the	O
sides	O
,	O
especially	O
in	O
high	O
dimensions	O
,	O
because	O
the	O
corners	O
“	O
stick	O
out	O
”	O
more	O
.	O
the	O
corners	O
correspond	O
to	O
sparse	B
solutions	O
,	O
which	O
lie	O
on	O
the	O
coordinate	O
axes	O
.	O
by	O
contrast	O
,	O
when	O
we	O
grow	O
the	O
(	O
cid:6	O
)	O
2	O
ball	O
,	O
it	O
can	O
intersect	O
the	O
objective	O
at	O
any	O
point	O
;	O
there	O
are	O
no	O
“	O
corners	O
”	O
,	O
so	O
there	O
is	O
no	O
preference	O
for	O
sparsity	B
.	O
√	O
to	O
see	O
this	O
another	O
away	O
,	O
notice	O
that	O
,	O
with	O
ridge	B
regression	I
,	O
the	O
prior	O
cost	O
of	O
a	O
sparse	B
solution	O
,	O
2	O
)	O
,	O
such	O
as	O
w	O
=	O
(	O
1	O
,	O
0	O
)	O
,	O
is	O
the	O
same	O
as	O
the	O
cost	O
of	O
a	O
dense	O
solution	O
,	O
such	O
as	O
w	O
=	O
(	O
1/	O
as	O
long	O
as	O
they	O
have	O
the	O
same	O
(	O
cid:6	O
)	O
2	O
norm	O
:	O
√	O
2||2	O
=	O
1	O
||	O
(	O
1	O
,	O
0	O
)	O
||2	O
=	O
||	O
(	O
1/	O
√	O
2	O
,	O
1/	O
2	O
,	O
1/	O
√	O
(	O
13.41	O
)	O
√	O
√	O
2	O
)	O
,	O
since	O
however	O
,	O
for	O
lasso	B
,	O
setting	O
w	O
=	O
(	O
1	O
,	O
0	O
)	O
is	O
cheaper	O
than	O
setting	O
w	O
=	O
(	O
1/	O
2	O
,	O
1/	O
||	O
(	O
1	O
,	O
0	O
)	O
||1	O
=	O
1	O
<	O
||	O
(	O
1/	O
the	O
most	O
rigorous	O
way	O
to	O
see	O
that	O
(	O
cid:6	O
)	O
1	O
regularization	B
results	O
in	O
sparse	B
solutions	O
is	O
to	O
examine	O
(	O
13.42	O
)	O
2	O
,	O
1/	O
√	O
√	O
2||1	O
=	O
√	O
2	O
conditions	O
that	O
hold	O
at	O
the	O
optimum	O
.	O
we	O
do	O
this	O
in	O
section	O
13.3.2	O
.	O
13.3.2	O
optimality	O
conditions	O
for	O
lasso	B
the	O
lasso	B
objective	O
has	O
the	O
form	O
f	O
(	O
θ	O
)	O
=	O
rss	O
(	O
θ	O
)	O
+λ||w	O
||1	O
(	O
13.43	O
)	O
2.	O
equation	O
13.38	O
is	O
an	O
example	O
of	O
a	O
quadratic	B
program	I
or	O
qp	O
,	O
since	O
we	O
have	O
a	O
quadratic	O
objective	O
subject	O
to	O
linear	O
inequality	O
constraints	O
.	O
its	O
lagrangian	O
is	O
given	O
by	O
equation	O
13.37	O
.	O
432	O
chapter	O
13.	O
sparse	B
linear	O
models	O
cʼ	O
c	O
f	O
(	O
x	O
)	O
−	O
f	O
(	O
x0	O
)	O
c	O
(	O
x	O
−	O
x0	O
)	O
x0	O
x	O
figure	O
13.4	O
//en.wikipedia.org/wiki/subderivative	O
.	O
figure	O
generated	O
by	O
subgradientplot	O
.	O
illustration	O
of	O
some	O
sub-derivatives	O
of	O
a	O
function	O
at	O
point	O
x0	O
.	O
based	O
on	O
a	O
ﬁgure	O
at	O
http	O
:	O
unfortunately	O
,	O
the	O
||w||1	O
term	O
is	O
not	O
differentiable	O
whenever	O
wj	O
=	O
0.	O
this	O
is	O
an	O
example	O
of	O
a	O
non-smooth	B
optimization	O
problem	O
.	O
to	O
handle	O
non-smooth	B
functions	O
,	O
we	O
need	O
to	O
extend	O
the	O
notion	O
of	O
a	O
derivative	O
.	O
we	O
deﬁne	O
a	O
subderivative	B
or	O
subgradient	B
of	O
a	O
(	O
convex	B
)	O
function	O
f	O
:	O
i	O
→	O
r	O
at	O
a	O
point	O
θ0	O
to	O
be	O
a	O
scalar	O
g	O
such	O
that	O
f	O
(	O
θ	O
)	O
−	O
f	O
(	O
θ0	O
)	O
≥	O
g	O
(	O
θ	O
−	O
θ0	O
)	O
∀θ	O
∈	O
i	O
(	O
13.44	O
)	O
where	O
i	O
is	O
some	O
interval	O
containing	O
θ0	O
.	O
see	O
figure	O
13.4	O
for	O
an	O
illustration.3	O
we	O
deﬁne	O
the	O
set	O
of	O
subderivatives	O
as	O
the	O
interval	O
[	O
a	O
,	O
b	O
]	O
where	O
a	O
and	O
b	O
are	O
the	O
one-sided	O
limits	O
f	O
(	O
θ	O
)	O
−	O
f	O
(	O
θ0	O
)	O
θ	O
−	O
θ0	O
a	O
=	O
lim	O
θ→θ−	O
0	O
,	O
b	O
=	O
lim	O
θ→θ+	O
0	O
f	O
(	O
θ	O
)	O
−	O
f	O
(	O
θ0	O
)	O
θ	O
−	O
θ0	O
(	O
13.46	O
)	O
the	O
set	O
[	O
a	O
,	O
b	O
]	O
of	O
all	O
subderivatives	O
is	O
called	O
the	O
subdifferential	B
of	O
the	O
function	O
f	O
at	O
θ0	O
and	O
is	O
denoted	O
∂f	O
(	O
θ	O
)	O
|θ0	O
.	O
for	O
example	O
,	O
in	O
the	O
case	O
of	O
the	O
absolute	O
value	O
function	O
f	O
(	O
θ	O
)	O
=|θ	O
|	O
,	O
the	O
subderivative	B
is	O
given	O
by	O
{	O
−1	O
}	O
[	O
−1	O
,	O
1	O
]	O
{	O
+1	O
}	O
if	O
θ	O
<	O
0	O
if	O
θ	O
=	O
0	O
if	O
θ	O
>	O
0	O
⎧⎨	O
⎩	O
∂f	O
(	O
θ	O
)	O
=	O
(	O
13.47	O
)	O
if	O
the	O
function	O
is	O
everywhere	O
differentiable	O
,	O
then	O
∂f	O
(	O
θ	O
)	O
=	O
{	O
df	O
(	O
θ	O
)	O
calculus	O
result	O
,	O
one	O
can	O
show	O
that	O
the	O
point	O
ˆθ	O
is	O
a	O
local	O
minimum	O
of	O
f	O
iff	B
0	O
∈	O
∂f	O
(	O
θ	O
)	O
|ˆθ	O
.	O
dθ	O
}	O
.	O
by	O
analogy	O
to	O
the	O
standard	O
3.	O
in	O
general	O
,	O
for	O
a	O
vector	O
valued	O
function	O
,	O
we	O
say	O
that	O
g	O
is	O
a	O
subgradient	B
of	O
f	O
at	O
θ0	O
if	O
for	O
all	O
vectors	O
θ	O
,	O
f	O
(	O
θ	O
)	O
−	O
f	O
(	O
θ0	O
)	O
≥	O
(	O
θ	O
−	O
θ0	O
)	O
t	O
g	O
so	O
g	O
is	O
a	O
linear	O
lower	O
bound	O
to	O
the	O
function	O
at	O
θ0	O
.	O
(	O
13.45	O
)	O
13.3	O
.	O
(	O
cid:6	O
)	O
1	O
regularization	B
:	O
basics	O
(	O
cid:70	O
)	O
(	O
cid:78	O
)	O
433	O
(	O
cid:70	O
)	O
(	O
cid:78	O
)	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
13.5	O
left	O
:	O
soft	B
thresholding	I
.	O
the	O
ﬂat	O
region	O
is	O
the	O
interval	O
[	O
−λ	O
,	O
+λ	O
]	O
.	O
right	O
:	O
hard	B
thresholding	I
.	O
let	O
us	O
apply	O
these	O
concepts	O
to	O
the	O
lasso	B
problem	O
.	O
let	O
us	O
initially	O
ignore	O
the	O
non-smooth	B
penalty	O
term	O
.	O
one	O
can	O
show	O
(	O
exercise	O
13.1	O
)	O
that	O
rss	O
(	O
w	O
)	O
=a	O
jwj	O
−	O
cj	O
∂	O
∂wj	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
aj	O
=	O
2	O
cj	O
=	O
2	O
x2	O
ij	O
xij	O
(	O
yi	O
−	O
wt−jxi	O
,	O
−j	O
)	O
i=1	O
where	O
w−j	O
is	O
w	O
without	O
component	O
j	O
,	O
and	O
similarly	O
for	O
xi	O
,	O
−j	O
.	O
we	O
see	O
that	O
cj	O
is	O
(	O
proportional	O
to	O
)	O
the	O
correlation	O
between	O
the	O
j	O
’	O
th	O
feature	O
x	O
:	O
,j	O
and	O
the	O
residual	B
due	O
to	O
the	O
other	O
features	B
,	O
r−j	O
=	O
y	O
−	O
x	O
:	O
,−jw−j	O
.	O
hence	O
the	O
magnitude	O
of	O
cj	O
is	O
an	O
indication	O
of	O
how	O
relevant	O
feature	O
j	O
is	O
for	O
predicting	O
y	O
(	O
relative	O
to	O
the	O
other	O
features	B
and	O
the	O
current	O
parameters	O
)	O
.	O
adding	O
in	O
the	O
penalty	O
term	O
,	O
we	O
ﬁnd	O
that	O
the	O
subderivative	B
is	O
given	O
by	O
(	O
13.48	O
)	O
(	O
13.49	O
)	O
(	O
13.50	O
)	O
(	O
13.51	O
)	O
(	O
13.52	O
)	O
(	O
13.53	O
)	O
∂wj	O
f	O
(	O
w	O
)	O
=	O
(	O
ajwj	O
−	O
cj	O
)	O
+λ∂	O
wj	O
{	O
ajwj	O
−	O
cj	O
−	O
λ	O
}	O
[	O
−cj	O
−	O
λ	O
,	O
−cj	O
+	O
λ	O
]	O
{	O
ajwj	O
−	O
cj	O
+	O
λ	O
}	O
⎧⎨	O
⎩	O
=	O
||w||1	O
if	O
wj	O
<	O
0	O
if	O
wj	O
=	O
0	O
if	O
wj	O
>	O
0	O
we	O
can	O
write	O
this	O
in	O
a	O
more	O
compact	O
fashion	O
as	O
follows	O
:	O
xt	O
(	O
xw	O
−	O
y	O
)	O
j	O
∈	O
⎧⎨	O
⎩	O
{	O
−λ	O
}	O
[	O
−λ	O
,	O
λ	O
]	O
{	O
λ	O
}	O
if	O
wj	O
<	O
0	O
if	O
wj	O
=	O
0	O
if	O
wj	O
>	O
0	O
depending	O
on	O
the	O
value	O
of	O
cj	O
,	O
the	O
solution	O
to	O
∂wj	O
of	O
wj	O
,	O
as	O
follows	O
:	O
f	O
(	O
w	O
)	O
=	O
0	O
can	O
occur	O
at	O
3	O
different	O
values	O
434	O
chapter	O
13.	O
sparse	B
linear	O
models	O
1.	O
if	O
cj	O
<	O
−λ	O
,	O
so	O
the	O
feature	O
is	O
strongly	O
negatively	O
correlated	O
with	O
the	O
residual	B
,	O
then	O
the	O
subgradient	B
is	O
zero	O
at	O
ˆwj	O
=	O
<	O
0.	O
cj	O
+λ	O
aj	O
2.	O
if	O
cj	O
∈	O
[	O
−λ	O
,	O
λ	O
]	O
,	O
so	O
the	O
feature	O
is	O
only	O
weakly	O
correlated	O
with	O
the	O
residual	B
,	O
then	O
the	O
subgradient	B
is	O
zero	O
at	O
ˆwj	O
=	O
0	O
.	O
3.	O
if	O
cj	O
>	O
λ	O
,	O
so	O
the	O
feature	O
is	O
strongly	O
positively	O
correlated	O
with	O
the	O
residual	B
,	O
then	O
the	O
subgra-	O
dient	O
is	O
zero	O
at	O
ˆwj	O
=	O
cj−λ	O
aj	O
in	O
summary	O
,	O
we	O
have	O
⎧⎨	O
⎩	O
(	O
cj	O
+	O
λ	O
)	O
/aj	O
(	O
cj	O
−	O
λ	O
)	O
/aj	O
we	O
can	O
write	O
this	O
as	O
follows	O
:	O
ˆwj	O
(	O
cj	O
)	O
=	O
0	O
>	O
0.	O
if	O
cj	O
<	O
−λ	O
if	O
cj	O
∈	O
[	O
−λ	O
,	O
λ	O
]	O
if	O
cj	O
>	O
λ	O
(	O
13.54	O
)	O
(	O
13.55	O
)	O
(	O
13.56	O
)	O
ˆwj	O
=	O
soft	O
(	O
cj	O
aj	O
;	O
λ	O
aj	O
)	O
where	O
soft	O
(	O
a	O
;	O
δ	O
)	O
(	O
cid:2	O
)	O
sign	O
(	O
a	O
)	O
(	O
|a|	O
−δ	O
)	O
+	O
and	O
x+	O
=	O
max	O
(	O
x	O
,	O
0	O
)	O
is	O
the	O
positive	O
part	O
of	O
x.	O
this	O
is	O
called	O
soft	B
thresholding	I
.	O
this	O
is	O
illustrated	O
in	O
figure	O
13.5	O
(	O
a	O
)	O
,	O
where	O
we	O
plot	O
ˆwj	O
vs	O
cj	O
.	O
the	O
dotted	O
line	O
is	O
the	O
line	O
wj	O
=	O
cj/aj	O
corresponding	O
to	O
the	O
least	B
squares	I
ﬁt	O
.	O
the	O
solid	O
line	O
,	O
which	O
represents	O
the	O
regularized	O
estimate	O
ˆwj	O
(	O
cj	O
)	O
,	O
shifts	O
the	O
dotted	O
line	O
down	O
(	O
or	O
up	O
)	O
by	O
λ	O
,	O
except	O
when	O
−λ	O
≤	O
cj	O
≤	O
λ	O
,	O
in	O
which	O
case	O
it	O
sets	O
wj	O
=	O
0.	O
by	O
contrast	O
,	O
in	O
figure	O
13.5	O
(	O
b	O
)	O
,	O
we	O
illustrate	O
hard	B
thresholding	I
.	O
this	O
sets	O
values	O
of	O
wj	O
to	O
0	O
if	O
−λ	O
≤	O
cj	O
≤	O
λ	O
,	O
but	O
it	O
does	O
not	O
shrink	O
the	O
values	O
of	O
wj	O
outside	O
of	O
this	O
interval	O
.	O
the	O
slope	O
of	O
the	O
soft	B
thresholding	I
line	O
does	O
not	O
coincide	O
with	O
the	O
diagonal	B
,	O
which	O
means	O
that	O
even	O
large	O
coefficients	O
are	O
shrunk	O
towards	O
zero	O
;	O
consequently	O
lasso	B
is	O
a	O
biased	O
estimator	B
.	O
this	O
is	O
undesirable	O
,	O
since	O
if	O
the	O
likelihood	B
indicates	O
(	O
via	O
cj	O
)	O
that	O
the	O
coefficient	O
wj	O
should	O
be	O
large	O
,	O
we	O
do	O
not	O
want	O
to	O
shrink	O
it	O
.	O
we	O
will	O
discuss	O
this	O
issue	O
in	O
more	O
detail	O
in	O
section	O
13.6.2.	O
now	O
we	O
ﬁnally	O
can	O
understand	O
why	O
tibshirani	O
invented	O
the	O
term	O
“	O
lasso	B
”	O
in	O
(	O
tibshirani	O
1996	O
)	O
:	O
it	O
stands	O
for	O
“	O
least	O
absolute	O
selection	O
and	O
shrinkage	B
operator	O
”	O
,	O
since	O
it	O
selects	O
a	O
subset	O
of	O
the	O
variables	O
,	O
and	O
shrinks	O
all	O
the	O
coefficients	O
by	O
penalizing	O
the	O
absolute	O
values	O
.	O
if	O
λ	O
=	O
0	O
,	O
we	O
get	O
the	O
ols	O
solution	O
(	O
of	O
minimal	B
(	O
cid:6	O
)	O
1	O
norm	O
)	O
.	O
if	O
λ	O
≥	O
λmax	O
,	O
we	O
get	O
ˆw	O
=	O
0	O
,	O
where	O
λmax	O
=	O
||xt	O
y||∞	O
=	O
max	O
|yt	O
x	O
:	O
,j|	O
(	O
13.57	O
)	O
this	O
value	O
is	O
computed	O
using	O
the	O
fact	O
that	O
0	O
is	O
optimal	O
if	O
(	O
xt	O
y	O
)	O
j	O
∈	O
[	O
−λ	O
,	O
λ	O
]	O
for	O
all	O
j.	O
in	O
general	O
,	O
the	O
maximum	O
penalty	O
for	O
an	O
(	O
cid:6	O
)	O
1	O
regularized	O
objective	O
is	O
j	O
λmax	O
=	O
max	O
j	O
|∇jn	O
ll	O
(	O
0	O
)	O
|	O
(	O
13.58	O
)	O
13.3	O
.	O
(	O
cid:6	O
)	O
1	O
regularization	B
:	O
basics	O
435	O
13.3.3	O
comparison	O
of	O
least	B
squares	I
,	O
lasso	B
,	O
ridge	O
and	O
subset	O
selection	O
we	O
can	O
gain	O
further	O
insight	O
into	O
(	O
cid:6	O
)	O
1	O
regularization	B
by	O
comparing	O
it	O
to	O
least	B
squares	I
,	O
and	O
(	O
cid:6	O
)	O
2	O
and	O
(	O
cid:6	O
)	O
0	O
regularized	O
least	O
squares	O
.	O
for	O
simplicity	O
,	O
assume	O
all	O
the	O
features	B
of	O
x	O
are	O
orthonormal	O
,	O
so	O
xt	O
x	O
=	O
i.	O
in	O
this	O
case	O
,	O
the	O
rss	O
is	O
given	O
by	O
(	O
cid:2	O
)	O
rss	O
(	O
w	O
)	O
=||y	O
−	O
xw||2	O
=	O
yt	O
y	O
+	O
wt	O
xt	O
xw	O
−	O
2wt	O
xt	O
y	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
13.59	O
)	O
=	O
const	O
+	O
k	O
−	O
2	O
w2	O
wkxikyi	O
(	O
13.60	O
)	O
k	O
k	O
i	O
so	O
we	O
see	O
this	O
factorizes	O
into	O
a	O
sum	O
of	O
terms	O
,	O
one	O
per	O
dimension	O
.	O
hence	O
we	O
can	O
write	O
down	O
the	O
map	O
and	O
ml	O
estimates	O
analytically	O
,	O
as	O
follows	O
:	O
•	O
mle	O
the	O
ols	O
solution	O
is	O
given	O
by	O
ˆwols	O
=	O
xt	O
:	O
ky	O
k	O
(	O
13.61	O
)	O
where	O
x	O
:	O
k	O
is	O
the	O
k	O
’	O
th	O
column	O
of	O
x.	O
this	O
follows	O
trivially	O
from	O
equation	O
13.60.	O
we	O
see	O
that	O
ˆwols	O
is	O
just	O
the	O
orthogonal	B
projection	I
of	O
feature	O
k	O
onto	O
the	O
response	O
vector	O
(	O
see	O
section	O
7.3.2	O
)	O
.	O
k	O
•	O
ridge	O
one	O
can	O
show	O
that	O
the	O
ridge	O
estimate	O
is	O
given	O
by	O
•	O
lasso	B
from	O
equation	O
13.55	O
,	O
and	O
using	O
the	O
fact	O
that	O
ak	O
=	O
2	O
and	O
ˆwols	O
k	O
ˆwridge	O
k	O
=	O
k	O
ˆwols	O
1	O
+	O
λ	O
ˆwlasso	O
k	O
=	O
sign	O
(	O
ˆwols	O
k	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
|	O
ˆwols	O
k	O
|	O
−	O
λ	O
2	O
+	O
(	O
13.62	O
)	O
=	O
ck/2	O
,	O
we	O
have	O
(	O
13.63	O
)	O
this	O
corresponds	O
to	O
soft	B
thresholding	I
,	O
shown	O
in	O
figure	O
13.5	O
(	O
a	O
)	O
.	O
•	O
subset	O
selection	O
if	O
we	O
pick	O
the	O
best	O
k	O
features	B
using	O
subset	O
selection	O
,	O
the	O
parameter	B
estimate	O
is	O
as	O
follows	O
(	O
cid:26	O
)	O
ˆwss	O
k	O
=	O
ˆwols	O
k	O
0	O
if	O
rank	O
(	O
|wols	O
otherwise	O
k	O
|	O
)	O
≤	O
k	O
(	O
13.64	O
)	O
where	O
rank	O
refers	O
to	O
the	O
location	O
in	O
the	O
sorted	O
list	O
of	O
weight	O
magnitudes	O
.	O
this	O
corresponds	O
to	O
hard	B
thresholding	I
,	O
shown	O
in	O
figure	O
13.5	O
(	O
b	O
)	O
.	O
figure	O
13.6	O
(	O
a	O
)	O
plots	O
the	O
mse	O
vs	O
λ	O
for	O
lasso	B
for	O
a	O
degree	B
14	O
polynomial	O
,	O
and	O
figure	O
13.6	O
(	O
b	O
)	O
plots	O
the	O
mse	O
vs	O
polynomial	O
order	O
.	O
we	O
see	O
that	O
lasso	B
gives	O
similar	B
results	O
to	O
the	O
subset	O
selection	O
method	O
.	O
as	O
another	O
example	O
,	O
consider	O
a	O
data	O
set	O
concerning	O
prostate	O
cancer	O
.	O
we	O
have	O
d	O
=	O
8	O
features	B
and	O
n	O
=	O
67	O
training	O
cases	O
;	O
the	O
goal	O
is	O
to	O
predict	O
the	O
log	O
prostate-speciﬁc	O
antigen	O
levels	O
(	O
see	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p4	O
)	O
for	O
more	O
biological	O
details	O
)	O
.	O
table	O
13.1	O
shows	O
that	O
lasso	B
gives	O
better	O
prediction	O
accuracy	O
(	O
at	O
least	O
on	O
this	O
particular	O
data	O
set	O
)	O
than	O
least	B
squares	I
,	O
ridge	O
,	O
and	O
best	O
subset	O
regression	B
.	O
(	O
in	O
each	O
case	O
,	O
the	O
strength	O
of	O
the	O
regularizer	O
was	O
chosen	O
by	O
cross	B
validation	I
.	O
)	O
lasso	B
also	O
gives	O
rise	O
to	O
a	O
sparse	B
solution	O
.	O
of	O
course	O
,	O
for	O
other	O
problems	O
,	O
ridge	O
may	O
give	O
better	O
predictive	B
accuracy	O
.	O
in	O
practice	O
,	O
a	O
combination	O
of	O
lasso	B
and	O
ridge	O
,	O
known	O
as	O
the	O
elastic	B
net	I
,	O
often	O
performs	O
best	O
,	O
since	O
it	O
provides	O
a	O
good	O
combination	O
of	O
sparsity	B
and	O
regularization	B
(	O
see	O
section	O
13.5.3	O
)	O
.	O
436	O
chapter	O
13.	O
sparse	B
linear	O
models	O
train	O
test	O
35	O
30	O
25	O
20	O
15	O
10	O
5	O
e	O
s	O
m	O
0	O
103.249	O
10	O
1	O
0.5	O
0.1	O
0.01	O
0.0001	O
0	O
lambda	O
(	O
a	O
)	O
e	O
s	O
m	O
35	O
30	O
25	O
20	O
15	O
10	O
5	O
0	O
0	O
performance	O
of	O
mle	O
train	O
test	O
2	O
4	O
6	O
8	O
degree	B
(	O
b	O
)	O
10	O
12	O
14	O
16	O
figure	O
13.6	O
(	O
a	O
)	O
mse	O
vs	O
λ	O
for	O
lasso	B
for	O
a	O
degree	B
14	O
polynomial	O
.	O
note	O
that	O
λ	O
decreases	O
as	O
we	O
move	O
to	O
the	O
right	O
.	O
figure	O
generated	O
by	O
linregpolylassodemo	O
.	O
(	O
b	O
)	O
mse	O
versus	O
polynomial	O
degree	O
.	O
note	O
that	O
the	O
model	O
order	O
increases	O
as	O
we	O
move	O
to	O
the	O
right	O
.	O
see	O
figure	O
1.18	O
for	O
a	O
plot	O
of	O
some	O
of	O
these	O
polynomial	B
regression	I
models	O
.	O
figure	O
generated	O
by	O
linregpolyvsdegree	O
.	O
term	O
intercept	O
lcavol	O
lweight	O
age	O
lbph	O
svi	O
lcp	O
gleason	O
pgg45	O
test	O
error	O
ls	O
2.452	O
0.716	O
0.293	O
-0.143	O
0.212	O
0.310	O
-0.289	O
-0.021	O
0.277	O
0.586	O
best	O
subset	O
2.481	O
0.651	O
0.380	O
-0.000	O
-0.000	O
-0.000	O
-0.000	O
-0.000	O
0.178	O
0.572	O
ridge	O
2.479	O
0.656	O
0.300	O
-0.129	O
0.208	O
0.301	O
-0.260	O
-0.019	O
0.256	O
0.580	O
lasso	B
2.480	O
0.653	O
0.297	O
-0.119	O
0.200	O
0.289	O
-0.236	O
0.000	O
0.226	O
0.564	O
table	O
13.1	O
results	O
of	O
different	O
methods	O
on	O
the	O
prostate	O
cancer	O
data	O
,	O
which	O
has	O
8	O
features	B
and	O
67	O
training	O
cases	O
.	O
methods	O
are	O
:	O
ls	O
=	O
least	B
squares	I
,	O
subset	O
=	O
best	O
subset	O
regression	B
,	O
ridge	O
,	O
lasso	B
.	O
rows	O
represent	O
the	O
coefficients	O
;	O
we	O
see	O
that	O
subset	O
regression	B
and	O
lasso	B
give	O
sparse	B
solutions	O
.	O
bottom	O
row	O
is	O
the	O
mean	B
squared	I
error	I
on	O
the	O
test	O
set	O
(	O
30	O
cases	O
)	O
.	O
based	O
on	O
table	O
3.3.	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
prostatecomparison	O
.	O
13.3.4	O
regularization	B
path	I
as	O
we	O
increase	O
λ	O
,	O
the	O
solution	O
vector	O
ˆw	O
(	O
λ	O
)	O
will	O
tend	O
to	O
get	O
sparser	O
,	O
although	O
not	O
necessarily	O
monotonically	O
.	O
we	O
can	O
plot	O
the	O
values	O
ˆwj	O
(	O
λ	O
)	O
vs	O
λ	O
for	O
each	O
feature	O
j	O
;	O
this	O
is	O
known	O
as	O
the	O
regularization	B
path	I
.	O
this	O
is	O
illustrated	O
for	O
ridge	B
regression	I
in	O
figure	O
13.7	O
(	O
a	O
)	O
,	O
where	O
we	O
plot	O
ˆwj	O
(	O
λ	O
)	O
as	O
the	O
regularizer	O
λ	O
decreases	O
.	O
we	O
see	O
that	O
when	O
λ	O
=	O
∞	O
,	O
all	O
the	O
coefficients	O
are	O
zero	O
.	O
but	O
for	O
any	O
ﬁnite	O
value	O
of	O
λ	O
,	O
all	O
coefficients	O
are	O
non-zero	O
;	O
furthermore	O
,	O
they	O
increase	O
in	O
magnitude	O
as	O
λ	O
is	O
decreased	O
.	O
in	O
figure	O
13.7	O
(	O
b	O
)	O
,	O
we	O
plot	O
the	O
analogous	O
result	O
for	O
lasso	B
.	O
as	O
we	O
move	O
to	O
the	O
right	O
,	O
the	O
upper	O
bound	O
on	O
the	O
(	O
cid:6	O
)	O
1	O
penalty	O
,	O
b	O
,	O
increases	O
.	O
when	O
b	O
=	O
0	O
,	O
all	O
the	O
coefficients	O
are	O
zero	O
.	O
as	O
we	O
increase	O
13.3	O
.	O
(	O
cid:6	O
)	O
1	O
regularization	B
:	O
basics	O
437	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−0.1	O
−0.2	O
0	O
lcavol	O
lweight	O
age	O
lbph	O
svi	O
lcp	O
gleason	O
pgg45	O
5	O
10	O
15	O
(	O
a	O
)	O
20	O
25	O
30	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−0.1	O
−0.2	O
0	O
lcavol	O
lweight	O
age	O
lbph	O
svi	O
lcp	O
gleason	O
pgg45	O
5	O
10	O
15	O
20	O
25	O
(	O
b	O
)	O
(	O
a	O
)	O
proﬁles	O
of	O
ridge	O
coefficients	O
for	O
the	O
prostate	O
cancer	O
example	O
vs	O
bound	O
on	O
(	O
cid:7	O
)	O
2	O
norm	O
of	O
w	O
,	O
figure	O
13.7	O
so	O
small	O
t	O
(	O
large	O
λ	O
)	O
is	O
on	O
the	O
left	O
.	O
the	O
vertical	O
line	O
is	O
the	O
value	O
chosen	O
by	O
5-fold	O
cv	O
using	O
the	O
1se	O
rule	O
.	O
based	O
on	O
figure	O
3.8	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
ridgepathprostate	O
.	O
(	O
b	O
)	O
proﬁles	O
of	O
lasso	B
coefficients	O
for	O
the	O
prostate	O
cancer	O
example	O
vs	O
bound	O
on	O
(	O
cid:7	O
)	O
1	O
norm	O
of	O
w	O
,	O
so	O
small	O
t	O
(	O
large	O
λ	O
)	O
is	O
on	O
the	O
left	O
.	O
based	O
on	O
figure	O
3.10	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
lassopathprostate	O
.	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−0.1	O
−0.2	O
0	O
lcavol	O
lweight	O
age	O
lbph	O
svi	O
lcp	O
gleason	O
pgg45	O
0.5	O
1	O
τ	O
(	O
a	O
)	O
1.5	O
2	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−0.1	O
−0.2	O
1	O
lcavol	O
lweight	O
age	O
lbph	O
svi	O
lcp	O
gleason	O
pgg45	O
2	O
3	O
4	O
5	O
lars	O
step	O
(	O
b	O
)	O
6	O
7	O
8	O
9	O
figure	O
13.8	O
illustration	O
of	O
piecewise	O
linearity	O
of	O
regularization	B
path	I
for	O
lasso	B
on	O
the	O
prostate	O
cancer	O
example	O
.	O
(	O
a	O
)	O
we	O
plot	O
ˆwj	O
(	O
b	O
)	O
vs	O
b	O
for	O
the	O
critical	O
values	O
of	O
b	O
.	O
(	O
b	O
)	O
we	O
plot	O
vs	O
steps	O
of	O
the	O
lars	O
algorithm	O
.	O
figure	O
generated	O
by	O
lassopathprostate	O
.	O
b	O
,	O
the	O
coefficients	O
gradually	O
“	O
turn	O
on	O
”	O
.	O
but	O
for	O
any	O
value	O
between	O
0	O
and	O
bmax	O
=	O
||	O
ˆwols||1	O
,	O
the	O
solution	O
is	O
sparse.4	O
remarkably	O
,	O
it	O
can	O
be	O
shown	O
that	O
the	O
solution	O
path	B
is	O
a	O
piecewise	O
linear	O
function	O
of	O
b	O
(	O
efron	O
et	O
al	O
.	O
2004	O
)	O
.	O
that	O
is	O
,	O
there	O
are	O
a	O
set	O
of	O
critical	O
values	O
of	O
b	O
where	O
the	O
active	B
set	I
of	O
non-zero	O
coefficients	O
changes	O
.	O
for	O
values	O
of	O
b	O
between	O
these	O
critical	O
values	O
,	O
each	O
non-zero	O
coefficient	O
increases	O
or	O
decreases	O
in	O
a	O
linear	O
fashion	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
13.8	O
(	O
a	O
)	O
.	O
furthermore	O
,	O
one	O
can	O
solve	O
for	O
these	O
critical	O
values	O
analytically	O
.	O
this	O
is	O
the	O
basis	O
of	O
the	O
lars	O
algorithm	O
(	O
efron	O
et	O
al	O
.	O
2004	O
)	O
,	O
which	O
stands	O
for	O
“	O
least	O
angle	O
regression	B
and	O
shrinkage	B
”	O
(	O
see	O
section	O
13.4.2	O
for	O
details	O
)	O
.	O
remarkably	O
,	O
lars	O
can	O
compute	O
the	O
entire	O
regularization	B
path	I
for	O
roughly	O
the	O
same	O
4.	O
it	O
is	O
common	O
to	O
plot	O
the	O
solution	O
versus	O
the	O
shrinkage	B
factor	I
,	O
deﬁned	O
as	O
s	O
(	O
b	O
)	O
=b/b	O
max	O
,	O
rather	O
than	O
against	O
b.	O
this	O
merely	O
affects	O
the	O
scale	O
of	O
the	O
horizontal	O
axis	O
,	O
not	O
the	O
shape	O
of	O
the	O
curves	O
.	O
438	O
chapter	O
13.	O
sparse	B
linear	O
models	O
original	O
(	O
d	O
=	O
4096	O
,	O
number	O
of	O
nonzeros	O
=	O
160	O
)	O
1	O
0	O
−1	O
0	O
l1	O
reconstruction	O
(	O
k0	O
=	O
1024	O
,	O
lambda	O
=	O
0.0516	O
,	O
mse	O
=	O
0.0027	O
)	O
1000	O
2000	O
3000	O
4000	O
1	O
0	O
−1	O
0	O
1	O
0	O
−1	O
0	O
0.5	O
0	O
−0.5	O
0	O
1000	O
3000	O
debiased	O
(	O
mse	O
=	O
3.26e−005	O
)	O
2000	O
1000	O
2000	O
3000	O
minimum	O
norm	O
solution	O
(	O
mse	O
=	O
0.0292	O
)	O
4000	O
4000	O
1000	O
2000	O
3000	O
4000	O
figure	O
13.9	O
example	O
of	O
recovering	O
a	O
sparse	B
signal	O
using	O
lasso	B
.	O
see	O
text	O
for	O
details	O
.	O
based	O
on	O
figure	O
1	O
of	O
(	O
figueiredo	O
et	O
al	O
.	O
2007	O
)	O
.	O
figure	O
generated	O
by	O
sparsesensingdemo	O
,	O
written	O
by	O
mario	O
figueiredo	O
.	O
computational	O
cost	O
as	O
a	O
single	O
least	O
squares	O
ﬁt	O
(	O
namely	O
o	O
(	O
min	O
(	O
n	O
d2	O
,	O
dn	O
2	O
)	O
)	O
.	O
in	O
figure	O
13.8	O
(	O
b	O
)	O
,	O
we	O
plot	O
the	O
coefficients	O
computed	O
at	O
each	O
critical	B
value	I
of	O
b.	O
now	O
the	O
piecewise	O
linearity	O
is	O
more	O
evident	O
.	O
below	O
we	O
display	O
the	O
actual	O
coefficient	O
values	O
at	O
each	O
step	O
along	O
the	O
regularization	B
path	I
(	O
the	O
last	O
line	O
is	O
the	O
least	B
squares	I
solution	O
)	O
:	O
listing	O
13.1	O
output	O
of	O
lassopathprostate	O
0	O
0.4279	O
0.5015	O
0.5610	O
0.5622	O
0.5797	O
0.5864	O
0.6994	O
0.7164	O
0	O
0	O
0.0735	O
0.1878	O
0.1890	O
0.2456	O
0.2572	O
0.2910	O
0.2926	O
0	O
0	O
0	O
0	O
0	O
0	O
-0.0321	O
-0.1337	O
-0.1425	O
0	O
0	O
0	O
0	O
0.0036	O
0.1435	O
0.1639	O
0.2062	O
0.2120	O
0	O
0	O
0	O
0.0930	O
0.0963	O
0.2003	O
0.2082	O
0.3003	O
0.3096	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
-0.2565	O
-0.2890	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
-0.0209	O
0	O
0	O
0	O
0	O
0	O
0.0901	O
0.1066	O
0.2452	O
0.2773	O
by	O
changing	O
b	O
from	O
0	O
to	O
bmax	O
,	O
we	O
can	O
go	O
from	O
a	O
solution	O
in	O
which	O
all	O
the	O
weights	O
are	O
zero	O
to	O
a	O
solution	O
in	O
which	O
all	O
weights	O
are	O
non-zero	O
.	O
unfortunately	O
,	O
not	O
all	O
subset	O
sizes	O
are	O
achievable	O
using	O
lasso	B
.	O
one	O
can	O
show	O
that	O
,	O
if	O
d	O
>	O
n	O
,	O
the	O
optimal	O
solution	O
can	O
have	O
at	O
most	O
n	O
variables	O
in	O
it	O
,	O
before	O
reaching	O
the	O
complete	B
set	O
corresponding	O
to	O
the	O
ols	O
solution	O
of	O
minimal	B
(	O
cid:6	O
)	O
1	O
norm	O
.	O
in	O
section	O
13.5.3	O
,	O
we	O
will	O
see	O
that	O
by	O
using	O
an	O
(	O
cid:6	O
)	O
2	O
regularizer	O
as	O
well	O
as	O
an	O
(	O
cid:6	O
)	O
1	O
regularizer	O
(	O
a	O
method	O
known	O
as	O
the	O
elastic	B
net	I
)	O
,	O
we	O
can	O
achieve	O
sparse	B
solutions	O
which	O
contain	O
more	O
variables	O
than	O
training	O
cases	O
.	O
this	O
lets	O
us	O
explore	O
model	O
sizes	O
between	O
n	O
and	O
d.	O
13.3	O
.	O
(	O
cid:6	O
)	O
1	O
regularization	B
:	O
basics	O
439	O
13.3.5	O
model	B
selection	I
it	O
is	O
tempting	O
to	O
use	O
(	O
cid:6	O
)	O
1	O
regularization	B
to	O
estimate	O
the	O
set	O
of	O
relevant	O
variables	O
.	O
in	O
some	O
cases	O
,	O
we	O
can	O
recover	O
the	O
true	O
sparsity	O
pattern	B
of	O
w∗	O
,	O
the	O
parameter	B
vector	O
that	O
generated	O
the	O
data	O
.	O
a	O
method	O
that	O
can	O
recover	O
the	O
true	O
model	O
in	O
the	O
n	O
→	O
∞	O
limit	O
is	O
called	O
model	B
selection	I
consistent	I
.	O
the	O
details	O
on	O
which	O
methods	O
enjoy	O
this	O
property	O
,	O
and	O
when	O
,	O
are	O
beyond	O
the	O
scope	B
of	O
this	O
book	O
;	O
see	O
e.g.	O
,	O
(	O
buhlmann	O
and	O
van	O
de	O
geer	O
2011	O
)	O
for	O
details	O
.	O
instead	O
of	O
going	O
into	O
a	O
theoretical	O
discussion	O
,	O
we	O
will	O
just	O
show	O
a	O
small	O
example	O
.	O
we	O
ﬁrst	O
of	O
size	O
d	O
=	O
4096	O
,	O
consisting	O
of	O
160	O
randomly	O
placed	O
±1	O
spikes	O
.	O
generate	O
a	O
sparse	B
signal	O
w∗	O
next	O
we	O
generate	O
a	O
random	O
design	O
matrix	O
x	O
of	O
size	O
n	O
×	O
d	O
,	O
where	O
n	O
=	O
1024.	O
finally	O
we	O
+	O
	O
,	O
where	O
i	O
∼	O
n	O
(	O
0	O
,	O
0.012	O
)	O
.	O
we	O
then	O
estimate	O
w	O
from	O
generate	O
a	O
noisy	O
observation	B
y	O
=	O
xw∗	O
y	O
and	O
x.	O
is	O
shown	O
in	O
the	O
ﬁrst	O
row	O
of	O
figure	O
13.9.	O
the	O
second	O
row	O
is	O
the	O
(	O
cid:6	O
)	O
1	O
estimate	O
ˆwl1	O
using	O
λ	O
=	O
0.1λmax	O
.	O
we	O
see	O
that	O
this	O
has	O
“	O
spikes	O
”	O
in	O
the	O
right	O
places	O
,	O
but	O
they	O
are	O
too	O
small	O
.	O
the	O
third	O
row	O
is	O
the	O
least	B
squares	I
estimate	O
of	O
the	O
coefficients	O
which	O
are	O
estimated	O
to	O
be	O
non-zero	O
based	O
on	O
supp	O
(	O
ˆwl1	O
)	O
.	O
this	O
is	O
called	O
debiasing	B
,	O
and	O
is	O
necessary	O
because	O
lasso	B
shrinks	O
the	O
relevant	O
coefficients	O
as	O
well	O
as	O
the	O
irrelevant	O
ones	O
.	O
the	O
last	O
row	O
is	O
the	O
least	B
squares	I
estimate	O
for	O
all	O
the	O
coefficients	O
jointly	O
,	O
ignoring	O
sparsity	B
.	O
we	O
see	O
that	O
the	O
(	O
debiased	O
)	O
sparse	B
estimate	O
is	O
an	O
excellent	O
estimate	O
of	O
the	O
original	O
signal	O
.	O
by	O
contrast	O
,	O
least	B
squares	I
without	O
the	O
sparsity	B
assumption	O
performs	O
very	O
poorly	O
.	O
the	O
original	O
w∗	O
of	O
course	O
,	O
to	O
perform	O
model	B
selection	I
,	O
we	O
have	O
to	O
pick	O
λ.	O
it	O
is	O
common	O
to	O
use	O
cross	B
validation	I
.	O
however	O
,	O
it	O
is	O
important	O
to	O
note	O
that	O
cross	B
validation	I
is	O
picking	O
a	O
value	O
of	O
λ	O
that	O
results	O
in	O
good	O
predictive	B
accuracy	O
.	O
this	O
is	O
not	O
usually	O
the	O
same	O
value	O
as	O
the	O
one	O
that	O
is	O
likely	O
to	O
recover	O
the	O
“	O
true	O
”	O
model	O
.	O
to	O
see	O
why	O
,	O
recall	B
that	O
(	O
cid:6	O
)	O
1	O
regularization	B
performs	O
selection	O
and	O
shrinkage	B
,	O
that	O
is	O
,	O
the	O
chosen	O
coefficients	O
are	O
brought	O
closer	O
to	O
0.	O
in	O
order	O
to	O
prevent	O
relevant	O
coefficients	O
from	O
being	O
shrunk	O
in	O
this	O
way	O
,	O
cross	B
validation	I
will	O
tend	O
to	O
pick	O
a	O
value	O
of	O
λ	O
that	O
is	O
not	O
too	O
large	O
.	O
of	O
course	O
,	O
this	O
will	O
result	O
in	O
a	O
less	O
sparse	B
model	O
which	O
contains	O
irrelevant	O
variables	O
(	O
false	O
positives	O
)	O
.	O
indeed	O
,	O
it	O
was	O
proved	O
in	O
(	O
meinshausen	O
and	O
buhlmann	O
2006	O
)	O
that	O
the	O
prediction-optimal	O
value	O
of	O
λ	O
does	O
not	O
result	O
in	O
model	B
selection	I
consistency	O
.	O
in	O
section	O
13.6.2	O
,	O
we	O
will	O
discuss	O
some	O
adaptive	O
mechanisms	O
for	O
automatically	O
tuning	O
λ	O
on	O
a	O
per-dimension	O
basis	O
that	O
does	O
result	O
in	O
model	B
selection	I
consistency	O
.	O
a	O
downside	O
of	O
using	O
(	O
cid:6	O
)	O
1	O
regularization	B
to	O
select	O
variables	O
is	O
that	O
it	O
can	O
give	O
quite	O
different	O
results	O
if	O
the	O
data	O
is	O
perturbed	O
slightly	O
.	O
the	O
bayesian	O
approach	O
,	O
which	O
estimates	O
posterior	O
marginal	O
inclusion	B
probabilities	I
,	O
p	O
(	O
γj	O
=	O
1|d	O
)	O
,	O
is	O
much	O
more	O
robust	B
.	O
a	O
frequentist	B
solution	O
to	O
this	O
is	O
to	O
use	O
bootstrap	B
resampling	I
(	O
see	O
section	O
6.2.1	O
)	O
,	O
and	O
to	O
rerun	O
the	O
estimator	B
on	O
different	O
versions	O
of	O
the	O
data	O
.	O
by	O
computing	O
how	O
often	O
each	O
variable	O
is	O
selected	O
across	O
different	O
trials	O
,	O
we	O
can	O
approximate	O
the	O
posterior	O
inclusion	O
probabilities	O
.	O
this	O
method	O
is	O
known	O
as	O
stability	B
selection	I
(	O
meinshausen	O
and	O
bãijhlmann	O
2010	O
)	O
.	O
we	O
can	O
threshold	O
the	O
stability	B
selection	I
(	O
bootstrap	B
)	O
inclusion	B
probabilities	I
at	O
some	O
level	O
,	O
say	O
90	O
%	O
,	O
and	O
thus	O
derive	O
a	O
sparse	B
estimator	O
.	O
this	O
is	O
known	O
as	O
bootstrap	B
lasso	I
or	O
bolasso	B
(	O
bach	O
2008	O
)	O
.	O
it	O
will	O
include	O
a	O
variable	O
if	O
it	O
occurs	O
in	O
at	O
least	O
90	O
%	O
of	O
sets	O
returned	O
by	O
lasso	B
(	O
for	O
a	O
ﬁxed	O
λ	O
)	O
.	O
this	O
process	O
of	O
intersecting	O
the	O
sets	O
is	O
a	O
way	O
of	O
eliminating	O
the	O
false	O
positives	O
that	O
vanilla	O
lasso	B
produces	O
.	O
the	O
theoretical	O
results	O
in	O
(	O
bach	O
2008	O
)	O
prove	O
that	O
bolasso	B
is	O
model	B
selection	I
consistent	I
under	O
a	O
wider	O
range	O
of	O
conditions	O
than	O
vanilla	O
lasso	B
.	O
as	O
an	O
illustration	O
,	O
we	O
reproduced	O
the	O
experiments	O
in	O
(	O
bach	O
2008	O
)	O
.	O
in	O
particular	O
,	O
we	O
created	O
440	O
chapter	O
13.	O
sparse	B
linear	O
models	O
x	O
e	O
d	O
n	O
i	O
l	O
e	O
b	O
a	O
i	O
r	O
a	O
v	O
2	O
4	O
6	O
8	O
10	O
12	O
14	O
16	O
0	O
lasso	B
on	O
sign	O
inconsistent	O
data	O
5	O
−log	O
(	O
λ	O
)	O
10	O
15	O
(	O
a	O
)	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
x	O
e	O
d	O
n	O
i	O
l	O
e	O
b	O
a	O
i	O
r	O
a	O
v	O
2	O
4	O
6	O
8	O
10	O
12	O
14	O
16	O
0	O
bolasso	B
on	O
sign	O
inconsistent	O
data	O
128	O
bootstraps	O
lasso	B
vs	O
bolasso	B
on	O
sign	O
inconsistent	O
data	O
nbootstraps	O
=	O
[	O
0,2,4,8,16,32,64,128,256	O
]	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
1	O
0.5	O
)	O
t	O
r	O
o	O
p	O
p	O
u	O
s	O
t	O
c	O
e	O
r	O
r	O
o	O
c	O
(	O
p	O
0	O
0	O
lasso	B
bolasso	O
5	O
−log	O
(	O
λ	O
)	O
10	O
15	O
(	O
c	O
)	O
5	O
−log	O
(	O
λ	O
)	O
10	O
15	O
(	O
b	O
)	O
figure	O
13.10	O
(	O
a	O
)	O
probability	O
of	O
selection	O
of	O
each	O
variable	O
(	O
white	O
=	O
large	O
probabilities	O
,	O
black	O
=	O
small	O
proba-	O
bilities	O
)	O
vs.	O
regularization	B
parameter	O
for	O
lasso	B
.	O
as	O
we	O
move	O
from	O
left	O
to	O
right	O
,	O
we	O
decrease	O
the	O
amount	O
of	O
regularization	B
,	O
and	O
therefore	O
select	O
more	O
variables	O
.	O
(	O
b	O
)	O
same	O
as	O
(	O
a	O
)	O
but	O
for	O
bolasso	B
.	O
(	O
c	O
)	O
probability	O
of	O
correct	O
sign	O
estimation	O
vs.	O
regularization	B
parameter	O
.	O
bolasso	B
(	O
red	O
,	O
dashed	O
)	O
and	O
lasso	B
(	O
black	O
,	O
plain	O
)	O
:	O
the	O
number	O
of	O
bootstrap	B
replications	O
is	O
in	O
{	O
2	O
,	O
4	O
,	O
8	O
,	O
16	O
,	O
32	O
,	O
64	O
,	O
128	O
,	O
256	O
}	O
.	O
based	O
on	O
figures	O
1-3	O
of	O
(	O
bach	O
2008	O
)	O
.	O
figure	O
generated	O
by	O
bolassodemo	O
.	O
256	O
datasets	O
of	O
size	O
n	O
=	O
1000	O
with	O
d	O
=	O
16	O
variables	O
,	O
of	O
which	O
8	O
are	O
relevant	O
.	O
see	O
(	O
bach	O
2008	O
)	O
for	O
more	O
detail	O
on	O
the	O
experimental	O
setup	O
.	O
for	O
dataset	O
n	O
,	O
variable	O
j	O
,	O
and	O
sparsity	B
level	O
k	O
,	O
deﬁne	O
s	O
(	O
j	O
,	O
k	O
,	O
n	O
)	O
=	O
i	O
(	O
ˆwj	O
(	O
λk	O
,	O
dn	O
)	O
(	O
cid:22	O
)	O
=	O
0	O
)	O
.	O
now	O
deﬁnep	O
(	O
j	O
,	O
k	O
)	O
be	O
the	O
average	O
of	O
s	O
(	O
j	O
,	O
k	O
,	O
n	O
)	O
over	O
the	O
256	O
in	O
figure	O
13.10	O
(	O
a-b	O
)	O
,	O
we	O
plot	O
p	O
vs	O
−	O
log	O
(	O
λ	O
)	O
for	O
lasso	B
and	O
bolasso	B
.	O
we	O
see	O
that	O
for	O
datasets	O
.	O
bolasso	B
,	O
there	O
is	O
a	O
large	O
range	O
of	O
λ	O
where	O
the	O
true	O
variables	O
are	O
selected	O
,	O
but	O
this	O
is	O
not	O
the	O
case	O
for	O
lasso	B
.	O
this	O
is	O
emphasized	O
in	O
figure	O
13.10	O
(	O
c	O
)	O
,	O
where	O
we	O
plot	O
the	O
empirical	O
probability	O
that	O
the	O
correct	O
set	O
of	O
variables	O
is	O
recovered	O
,	O
for	O
lasso	B
and	O
for	O
bolasso	B
with	O
an	O
increasing	O
number	O
of	O
bootstrap	B
samples	O
.	O
of	O
course	O
,	O
using	O
more	O
samples	B
takes	O
longer	O
.	O
in	O
practice	O
,	O
32	O
bootstraps	O
seems	O
to	O
be	O
a	O
good	O
compromise	O
between	O
speed	O
and	O
accuracy	O
.	O
with	O
bolasso	B
,	O
there	O
is	O
the	O
usual	O
issue	O
of	O
picking	O
λ.	O
obviously	O
we	O
could	O
use	O
cross	B
validation	I
,	O
but	O
plots	O
such	O
as	O
figure	O
13.10	O
(	O
b	O
)	O
suggest	O
another	O
heuristic	O
:	O
shuffle	O
the	O
rows	O
to	O
create	O
a	O
large	O
black	O
block	O
,	O
and	O
then	O
pick	O
λ	O
to	O
be	O
in	O
the	O
middle	O
of	O
this	O
region	O
.	O
of	O
course	O
,	O
operationalizing	O
this	O
intuition	O
may	O
be	O
tricky	O
,	O
and	O
will	O
require	O
various	O
ad-hoc	O
thresholds	O
(	O
it	O
is	O
reminiscent	O
of	O
the	O
“	O
ﬁnd	O
the	O
knee	B
in	O
the	O
curve	O
”	O
heuristic	O
discussed	O
in	O
section	O
11.5.2	O
when	O
discussing	O
how	O
to	O
pick	O
k	O
for	O
mixture	B
models	O
)	O
.	O
a	O
bayesian	O
approach	O
provides	O
a	O
more	O
principled	O
method	O
for	O
selecting	O
λ	O
.	O
13.3.6	O
bayesian	O
inference	B
for	O
linear	O
models	O
with	O
laplace	O
priors	O
we	O
have	O
been	O
focusing	O
on	O
map	O
estimation	O
in	O
sparse	B
linear	O
models	O
.	O
it	O
is	O
also	O
possible	O
to	O
perform	O
bayesian	O
inference	B
(	O
see	O
e.g.	O
,	O
(	O
park	O
and	O
casella	O
2008	O
;	O
seeger	O
2008	O
)	O
)	O
.	O
however	O
,	O
the	O
posterior	B
mean	I
and	O
median	B
,	O
as	O
well	O
as	O
samples	B
from	O
the	O
posterior	O
,	O
are	O
not	O
sparse	O
;	O
only	O
the	O
mode	B
is	O
sparse	B
.	O
this	O
is	O
another	O
example	O
of	O
the	O
phenomenon	O
discussed	O
in	O
section	O
5.2.1	O
,	O
where	O
we	O
said	O
that	O
the	O
map	O
estimate	O
is	O
often	O
untypical	O
of	O
the	O
bulk	O
of	O
the	O
posterior	O
.	O
another	O
argument	O
in	O
favor	O
of	O
using	O
the	O
posterior	B
mean	I
comes	O
from	O
equation	O
5.108	O
,	O
which	O
showed	O
that	O
that	O
plugging	O
in	O
the	O
posterior	B
mean	I
,	O
rather	O
than	O
the	O
posterior	B
mode	I
,	O
is	O
the	O
optimal	O
thing	O
to	O
do	O
if	O
we	O
want	O
to	O
minimize	O
squared	O
prediction	O
error	O
.	O
(	O
schniter	O
et	O
al	O
.	O
2008	O
)	O
shows	O
experimentally	O
,	O
and	O
(	O
elad	O
and	O
yavnch	O
2009	O
)	O
shows	O
theoretically	O
,	O
that	O
using	O
the	O
posterior	B
mean	I
with	O
a	O
spike-and-slab	O
prior	O
results	O
in	O
better	O
prediction	O
accuracy	O
than	O
using	O
the	O
posterior	B
mode	I
with	O
a	O
laplace	O
prior	O
,	O
albeit	O
at	O
slightly	O
higher	O
computational	O
cost	O
.	O
13.4	O
.	O
(	O
cid:6	O
)	O
1	O
regularization	B
:	O
algorithms	O
441	O
13.4	O
(	O
cid:9	O
)	O
1	O
regularization	B
:	O
algorithms	O
in	O
this	O
section	O
,	O
we	O
give	O
a	O
brief	O
review	O
of	O
some	O
algorithms	O
that	O
can	O
be	O
used	O
to	O
solve	O
(	O
cid:6	O
)	O
1	O
regularized	B
estimation	I
problems	O
.	O
we	O
focus	O
on	O
the	O
lasso	B
case	O
,	O
where	O
we	O
have	O
a	O
quadratic	B
loss	I
.	O
however	O
,	O
most	O
of	O
the	O
algorithms	O
can	O
be	O
extended	O
to	O
more	O
general	O
settings	O
,	O
such	O
as	O
logistic	B
regression	I
(	O
see	O
(	O
yaun	O
et	O
al	O
.	O
2010	O
)	O
for	O
a	O
comprehensive	O
review	O
of	O
(	O
cid:6	O
)	O
1	O
regularized	O
logistic	O
regression	B
)	O
.	O
note	O
that	O
this	O
area	O
of	O
machine	B
learning	I
is	O
advancing	O
very	O
rapidly	O
,	O
so	O
the	O
methods	O
below	O
may	O
not	O
be	O
state	B
of	O
the	O
art	O
by	O
the	O
time	O
you	O
read	O
this	O
chapter	O
.	O
(	O
see	O
(	O
schmidt	O
et	O
al	O
.	O
2009	O
;	O
yaun	O
et	O
al	O
.	O
2010	O
;	O
yang	O
et	O
al	O
.	O
2010	O
)	O
for	O
some	O
recent	O
surveys	O
.	O
)	O
13.4.1	O
coordinate	O
descent	O
sometimes	O
it	O
is	O
hard	O
to	O
optimize	O
all	O
the	O
variables	O
simultaneously	O
,	O
but	O
it	O
easy	O
to	O
optimize	O
them	O
one	O
by	O
one	O
.	O
in	O
particular	O
,	O
we	O
can	O
solve	O
for	O
the	O
j	O
’	O
th	O
coefficient	O
with	O
all	O
the	O
others	O
held	O
ﬁxed	O
:	O
w∗	O
j	O
=	O
argmin	O
z	O
f	O
(	O
w	O
+	O
zej	O
)	O
−	O
f	O
(	O
w	O
)	O
(	O
13.65	O
)	O
where	O
ej	O
is	O
the	O
j	O
’	O
th	O
unit	O
vector	O
.	O
we	O
can	O
either	O
cycle	B
through	O
the	O
coordinates	O
in	O
a	O
deterministic	O
fashion	O
,	O
or	O
we	O
can	O
sample	O
them	O
at	O
random	O
,	O
or	O
we	O
can	O
choose	O
to	O
update	O
the	O
coordinate	O
for	O
which	O
the	O
gradient	O
is	O
steepest	O
.	O
the	O
coordinate	O
descent	O
method	O
is	O
particularly	O
appealing	O
if	O
each	O
one-dimensional	O
optimization	B
problem	O
can	O
be	O
solved	O
analytically	O
for	O
example	O
,	O
the	O
shooting	B
algorithm	O
(	O
fu	O
1998	O
;	O
wu	O
and	O
lange	O
2008	O
)	O
for	O
lasso	B
uses	O
equation	O
13.54	O
to	O
compute	O
the	O
optimal	O
value	O
of	O
wj	O
given	O
all	O
the	O
other	O
coefficients	O
.	O
see	O
algorithm	O
7	O
for	O
the	O
pseudo	O
code	O
(	O
and	O
lassoshooting	O
for	O
some	O
matlab	O
code	O
)	O
.	O
see	O
(	O
yaun	O
et	O
al	O
.	O
2010	O
)	O
for	O
some	O
extensions	O
of	O
this	O
method	O
to	O
the	O
logistic	B
regression	I
case	O
.	O
the	O
resulting	O
algorithm	O
was	O
the	O
fastest	O
method	O
in	O
their	O
experimental	O
comparison	O
,	O
which	O
concerned	O
document	B
classiﬁcation	I
with	O
large	O
sparse	O
feature	O
vectors	O
(	O
representing	O
bags	O
of	O
words	O
)	O
.	O
other	O
types	O
of	O
data	O
(	O
e.g.	O
,	O
dense	O
features	O
and/or	O
regression	B
problems	O
)	O
might	O
call	O
for	O
different	O
algorithms	O
.	O
−1xt	O
y	O
;	O
algorithm	O
13.1	O
:	O
coordinate	O
descent	O
for	O
lasso	B
(	O
aka	O
shooting	B
algorithm	O
)	O
1	O
initialize	O
w	O
=	O
(	O
xt	O
x	O
+	O
λi	O
)	O
2	O
repeat	O
3	O
(	O
cid:10	O
)	O
n	O
for	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
do	O
(	O
cid:10	O
)	O
n	O
i=1	O
x2	O
ij	O
;	O
i=1	O
xij	O
(	O
yi	O
−	O
wt	O
xi	O
+	O
wjxij	O
)	O
;	O
cj	O
aj	O
,	O
λ	O
aj	O
)	O
;	O
4	O
5	O
6	O
aj	O
=	O
2	O
cj	O
=	O
2	O
wj	O
=	O
soft	O
(	O
7	O
until	O
converged	O
;	O
13.4.2	O
lars	O
and	O
other	O
homotopy	B
methods	O
the	O
problem	O
with	O
coordinate	O
descent	O
is	O
that	O
it	O
only	O
updates	O
one	O
variable	O
at	O
a	O
time	O
,	O
so	O
can	O
be	O
slow	O
to	O
converge	B
.	O
active	B
set	I
methods	O
update	O
many	O
variables	O
at	O
a	O
time	O
.	O
unfortunately	O
,	O
they	O
are	O
442	O
chapter	O
13.	O
sparse	B
linear	O
models	O
more	O
complicated	O
,	O
because	O
of	O
the	O
need	O
to	O
identify	O
which	O
variables	O
are	O
constrained	O
to	O
be	O
zero	O
,	O
and	O
which	O
are	O
free	O
to	O
be	O
updated	O
.	O
active	B
set	I
methods	O
typically	O
only	O
add	O
or	O
remove	O
a	O
few	O
variables	O
at	O
a	O
time	O
,	O
so	O
they	O
can	O
take	O
a	O
long	O
if	O
they	O
are	O
started	O
far	O
from	O
the	O
solution	O
.	O
but	O
they	O
are	O
ideally	O
suited	O
for	O
generating	O
a	O
set	O
of	O
solutions	O
for	O
different	O
values	O
of	O
λ	O
,	O
starting	O
with	O
the	O
empty	O
set	O
,	O
i.e.	O
,	O
for	O
generating	O
regularization	B
path	I
.	O
these	O
algorithms	O
exploit	O
the	O
fact	O
that	O
one	O
can	O
quickly	O
compute	O
ˆw	O
(	O
λk	O
)	O
from	O
ˆw	O
(	O
λk−1	O
)	O
if	O
λk	O
≈	O
λk−1	O
;	O
this	O
is	O
known	O
as	O
warm	B
starting	I
.	O
in	O
fact	O
,	O
even	O
if	O
we	O
only	O
want	O
the	O
solution	O
for	O
a	O
single	O
value	O
of	O
λ	O
,	O
call	O
it	O
λ∗	O
,	O
it	O
can	O
sometimes	O
be	O
computationally	O
more	O
efficient	O
to	O
compute	O
a	O
set	O
of	O
solutions	O
,	O
from	O
λmax	O
down	O
to	O
λ∗	O
,	O
using	O
warm-starting	O
;	O
this	O
is	O
called	O
a	O
continuation	B
method	I
or	O
homotopy	B
method	O
.	O
this	O
is	O
often	O
much	O
faster	O
than	O
directly	O
“	O
cold-starting	O
”	O
at	O
λ∗	O
;	O
this	O
is	O
particularly	O
true	O
if	O
λ∗	O
is	O
small	O
.	O
perhaps	O
the	O
most	O
well-known	O
example	O
of	O
a	O
homotopy	B
method	O
in	O
machine	B
learning	I
is	O
the	O
lars	O
algorithm	O
,	O
which	O
stands	O
for	O
“	O
least	O
angle	O
regression	B
and	O
shrinkage	B
”	O
(	O
efron	O
et	O
al	O
.	O
2004	O
)	O
(	O
a	O
similar	B
algorithm	O
was	O
independently	O
invented	O
in	O
(	O
osborne	O
et	O
al	O
.	O
2000b	O
,	O
a	O
)	O
)	O
.	O
this	O
can	O
compute	O
ˆw	O
(	O
λ	O
)	O
for	O
all	O
possible	O
values	O
of	O
λ	O
in	O
an	O
efficient	O
manner	O
.	O
lars	O
works	O
as	O
follows	O
.	O
it	O
starts	O
with	O
a	O
large	O
value	O
of	O
λ	O
,	O
such	O
that	O
only	O
the	O
variable	O
that	O
is	O
most	O
correlated	O
with	O
the	O
response	O
vector	O
y	O
is	O
chosen	O
.	O
then	O
λ	O
is	O
decreased	O
until	O
a	O
second	O
variable	O
is	O
found	O
which	O
has	O
the	O
same	O
correlation	O
(	O
in	O
terms	O
of	O
magnitude	O
)	O
with	O
the	O
current	O
residual	B
as	O
the	O
ﬁrst	O
variable	O
,	O
where	O
the	O
residual	B
at	O
step	O
k	O
is	O
deﬁned	O
as	O
rk	O
=	O
y	O
−	O
x	O
:	O
,fk	O
wk	O
,	O
where	O
fk	O
is	O
the	O
current	O
active	B
set	I
(	O
c.f.	O
,	O
equation	O
13.50	O
)	O
.	O
remarkably	O
,	O
one	O
can	O
solve	O
for	O
this	O
new	O
value	O
of	O
λ	O
analytically	O
,	O
by	O
using	O
a	O
geometric	O
argument	O
(	O
hence	O
the	O
term	O
“	O
least	O
angle	O
”	O
)	O
.	O
this	O
allows	O
the	O
algorithm	O
to	O
quickly	O
“	O
jump	O
”	O
to	O
the	O
next	O
point	O
on	O
the	O
regularization	B
path	I
where	O
the	O
active	B
set	I
changes	O
.	O
this	O
repeats	O
until	O
all	O
the	O
variables	O
are	O
added	O
.	O
it	O
is	O
necessary	O
to	O
allow	O
variables	O
to	O
be	O
removed	O
from	O
the	O
active	B
set	I
if	O
we	O
want	O
the	O
sequence	O
of	O
solutions	O
to	O
correspond	O
to	O
the	O
regularization	B
path	I
of	O
lasso	B
.	O
if	O
we	O
disallow	O
variable	O
removal	O
,	O
we	O
get	O
a	O
slightly	O
different	O
algorithm	O
called	O
lar	O
,	O
which	O
tends	O
to	O
be	O
faster	O
.	O
in	O
particular	O
,	O
lar	O
costs	O
the	O
same	O
as	O
a	O
single	O
ordinary	O
least	B
squares	I
ﬁt	O
,	O
namely	O
o	O
(	O
n	O
d	O
min	O
(	O
n	O
,	O
d	O
)	O
)	O
,	O
which	O
is	O
o	O
(	O
n	O
d2	O
)	O
if	O
n	O
>	O
d	O
,	O
and	O
o	O
(	O
n	O
2d	O
)	O
if	O
d	O
>	O
n	O
.	O
lar	O
is	O
very	O
similar	B
to	O
greedy	O
forward	O
selection	O
,	O
and	O
a	O
method	O
known	O
as	O
least	B
squares	I
boosting	I
(	O
see	O
section	O
16.4.6	O
)	O
.	O
there	O
have	O
been	O
many	O
attempts	O
to	O
extend	O
the	O
lars	O
algorithm	O
to	O
compute	O
the	O
full	B
regulariza-	O
tion	O
path	B
for	O
(	O
cid:6	O
)	O
1	O
regularized	O
glms	O
,	O
such	O
as	O
logistic	B
regression	I
.	O
in	O
general	O
,	O
one	O
can	O
not	O
analytically	O
solve	O
for	O
the	O
critical	O
values	O
of	O
λ.	O
instead	O
,	O
the	O
standard	O
approach	O
is	O
to	O
start	O
at	O
λmax	O
,	O
and	O
then	O
slowly	O
decrease	O
λ	O
,	O
tracking	B
the	O
solution	O
as	O
we	O
go	O
;	O
this	O
is	O
called	O
a	O
continuation	B
method	I
or	O
homotopy	B
method	O
.	O
these	O
methods	O
exploit	O
the	O
fact	O
that	O
we	O
can	O
quickly	O
compute	O
ˆw	O
(	O
λk	O
)	O
from	O
ˆw	O
(	O
λk−1	O
)	O
if	O
λk	O
≈	O
λk−1	O
;	O
this	O
is	O
known	O
as	O
warm	B
starting	I
.	O
even	O
if	O
we	O
don	O
’	O
t	O
want	O
the	O
full	B
path	O
,	O
this	O
method	O
is	O
often	O
much	O
faster	O
than	O
directly	O
“	O
cold-starting	O
”	O
at	O
the	O
desired	O
value	O
of	O
λ	O
(	O
this	O
is	O
particularly	O
true	O
if	O
λ	O
is	O
small	O
)	O
.	O
the	O
method	O
described	O
in	O
(	O
friedman	O
et	O
al	O
.	O
2010	O
)	O
combines	O
coordinate	O
descent	O
with	O
this	O
warm-	O
starting	O
strategy	O
,	O
and	O
computes	O
the	O
full	B
regularization	O
path	B
for	O
any	O
(	O
cid:6	O
)	O
1	O
regularized	O
glm	O
.	O
this	O
has	O
been	O
implemented	O
in	O
the	O
glmnet	B
package	O
,	O
which	O
is	O
bundled	O
with	O
pmtk	O
.	O
13.4.3	O
proximal	O
and	O
gradient	O
projection	O
methods	O
in	O
this	O
section	O
,	O
we	O
consider	O
some	O
methods	O
that	O
are	O
suitable	O
for	O
very	O
large	O
scale	O
problems	O
,	O
where	O
homotopy	B
methods	O
made	O
be	O
too	O
slow	O
.	O
these	O
methods	O
will	O
also	O
be	O
easy	O
to	O
extend	O
to	O
other	O
kinds	O
13.4	O
.	O
(	O
cid:6	O
)	O
1	O
regularization	B
:	O
algorithms	O
443	O
of	O
regularizers	O
,	O
beyond	O
(	O
cid:6	O
)	O
1	O
,	O
as	O
we	O
will	O
see	O
later	O
.	O
our	O
presentation	O
in	O
this	O
section	O
is	O
based	O
on	O
(	O
vandenberghe	O
2011	O
;	O
yang	O
et	O
al	O
.	O
2010	O
)	O
.	O
consider	O
a	O
convex	B
objective	O
of	O
the	O
form	O
f	O
(	O
θ	O
)	O
=	O
l	O
(	O
θ	O
)	O
+r	O
(	O
θ	O
)	O
(	O
13.66	O
)	O
where	O
l	O
(	O
θ	O
)	O
(	O
representing	O
the	O
loss	B
)	O
is	O
convex	B
and	O
differentiable	O
,	O
and	O
r	O
(	O
θ	O
)	O
(	O
representing	O
the	O
regularizer	O
)	O
is	O
convex	B
but	O
not	O
necessarily	O
differentiable	O
.	O
for	O
example	O
,	O
l	O
(	O
θ	O
)	O
=	O
rss	O
(	O
θ	O
)	O
and	O
r	O
(	O
θ	O
)	O
=	O
λ||θ||1	O
corresponds	O
to	O
the	O
bpdn	O
problem	O
.	O
as	O
another	O
example	O
,	O
the	O
lasso	B
problem	O
can	O
be	O
formulated	O
as	O
follows	O
:	O
l	O
(	O
θ	O
)	O
=	O
rss	O
(	O
θ	O
)	O
and	O
r	O
(	O
θ	O
)	O
=i	O
c	O
(	O
θ	O
)	O
,	O
where	O
c	O
=	O
{	O
θ	O
:	O
||θ||1	O
≤	O
b	O
}	O
,	O
and	O
ic	O
(	O
θ	O
)	O
is	O
the	O
indicator	B
function	I
of	O
a	O
convex	B
set	O
c	O
,	O
deﬁned	O
as	O
(	O
cid:26	O
)	O
ic	O
(	O
θ	O
)	O
(	O
cid:2	O
)	O
θ	O
∈	O
c	O
0	O
+∞	O
otherwise	O
(	O
13.67	O
)	O
in	O
some	O
cases	O
,	O
it	O
is	O
easy	O
to	O
optimize	O
functions	O
of	O
the	O
form	O
in	O
equation	O
13.66.	O
for	O
example	O
,	O
suppose	O
l	O
(	O
θ	O
)	O
=	O
rss	O
(	O
θ	O
)	O
,	O
and	O
the	O
design	B
matrix	I
is	O
simply	O
x	O
=	O
i.	O
then	O
the	O
obective	O
becomes	O
2||θ	O
−	O
y|2	O
f	O
(	O
θ	O
)	O
=	O
r	O
(	O
θ	O
)	O
+	O
1	O
2.	O
the	O
minimizer	O
of	O
this	O
is	O
given	O
by	O
proxr	O
(	O
y	O
)	O
,	O
which	O
is	O
the	O
proximal	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
operator	O
for	O
the	O
convex	B
function	O
r	O
,	O
deﬁned	O
by	O
||z	O
−	O
y||2	O
proxr	O
(	O
y	O
)	O
=	O
argmin	O
r	O
(	O
z	O
)	O
+	O
(	O
13.68	O
)	O
2	O
intuitively	O
,	O
we	O
are	O
returning	O
a	O
point	O
that	O
minimizes	O
r	O
but	O
which	O
is	O
also	O
close	O
(	O
proximal	O
)	O
to	O
y.	O
in	O
general	O
,	O
we	O
will	O
use	O
this	O
operator	O
inside	O
an	O
iterative	O
optimizer	O
,	O
in	O
which	O
case	O
we	O
want	O
to	O
stay	O
close	O
to	O
the	O
previous	O
iterate	O
.	O
in	O
this	O
case	O
,	O
we	O
use	O
||z	O
−	O
θk||2	O
proxr	O
(	O
θk	O
)	O
=	O
argmin	O
r	O
(	O
z	O
)	O
+	O
(	O
13.69	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
2	O
z	O
z	O
1	O
2	O
1	O
2	O
the	O
key	O
issues	O
are	O
:	O
how	O
do	O
we	O
efficiently	O
compute	O
the	O
proximal	B
operator	I
for	O
different	O
regu-	O
larizers	O
r	O
,	O
and	O
how	O
do	O
we	O
extend	O
this	O
technique	O
to	O
more	O
general	O
loss	B
functions	O
l	O
?	O
we	O
discuss	O
these	O
issues	O
below	O
.	O
13.4.3.1	O
proximal	O
operators	O
if	O
r	O
(	O
θ	O
)	O
=	O
λ||θ||1	O
,	O
the	O
proximal	B
operator	I
is	O
given	O
by	O
componentwise	O
soft-thresholding	O
:	O
proxr	O
(	O
θ	O
)	O
=	O
soft	O
(	O
θ	O
,	O
λ	O
)	O
(	O
13.70	O
)	O
as	O
we	O
showed	O
in	O
section	O
13.3.2.	O
if	O
r	O
(	O
θ	O
)	O
=	O
λ||θ||0	O
,	O
the	O
proximal	B
operator	I
is	O
given	O
by	O
componen-	O
twise	O
hard-thresholding	O
:	O
√	O
2λ	O
)	O
where	O
hard	O
(	O
u	O
,	O
a	O
)	O
(	O
cid:2	O
)	O
ui	O
(	O
|u|	O
>	O
a	O
)	O
.	O
proxr	O
(	O
θ	O
)	O
=	O
hard	O
(	O
θ	O
,	O
if	O
r	O
(	O
θ	O
)	O
=	O
ic	O
(	O
θ	O
)	O
,	O
the	O
proximal	B
operator	I
is	O
given	O
by	O
the	O
projection	B
onto	O
the	O
set	O
c	O
:	O
proxr	O
(	O
θ	O
)	O
=	O
argmin	O
z∈c	O
||z	O
−	O
θ||2	O
2	O
=	O
projc	O
(	O
θ	O
)	O
(	O
13.71	O
)	O
(	O
13.72	O
)	O
444	O
chapter	O
13.	O
sparse	B
linear	O
models	O
illustration	O
of	O
projected	B
gradient	I
descent	I
.	O
the	O
step	O
along	O
the	O
negative	O
gradient	O
,	O
to	O
θk	O
−	O
gk	O
,	O
figure	O
13.11	O
if	O
we	O
project	O
that	O
point	O
onto	O
the	O
closest	O
point	O
in	O
the	O
set	O
we	O
get	O
takes	O
us	O
outside	O
the	O
feasible	O
set	O
.	O
θk+1	O
=	O
projθ	O
(	O
θk	O
−	O
gk	O
)	O
.	O
we	O
can	O
then	O
derive	O
the	O
implicit	O
update	O
direction	O
using	O
dk	O
=	O
θk+1	O
−	O
θk	O
.	O
used	O
with	O
kind	O
permission	O
of	O
mark	O
schmidt	O
.	O
for	O
some	O
convex	B
sets	O
,	O
it	O
is	O
easy	O
to	O
compute	O
the	O
projection	B
operator	O
.	O
for	O
example	O
,	O
to	O
project	O
onto	O
the	O
rectangular	O
set	O
deﬁned	O
by	O
the	O
box	B
constraints	I
c	O
=	O
{	O
θ	O
:	O
(	O
cid:6	O
)	O
j	O
≤	O
θj	O
≤	O
uj	O
}	O
we	O
can	O
use	O
θj	O
uj	O
⎧⎨	O
⎩	O
(	O
cid:6	O
)	O
j	O
(	O
cid:26	O
)	O
θ||θ||2	O
θj	O
≤	O
(	O
cid:6	O
)	O
j	O
(	O
cid:6	O
)	O
j	O
≤	O
θj	O
≤	O
uj	O
θj	O
≥	O
uj	O
||θ||2	O
>	O
1	O
||θ||2	O
≤	O
1	O
projc	O
(	O
θ	O
)	O
j	O
=	O
projc	O
(	O
θ	O
)	O
=	O
θ	O
to	O
project	O
onto	O
the	O
euclidean	O
ball	O
c	O
=	O
{	O
θ	O
:	O
||θ||2	O
≤	O
1	O
}	O
we	O
can	O
use	O
to	O
project	O
onto	O
the	O
1-norm	O
ball	O
c	O
=	O
{	O
θ	O
:	O
||θ||1	O
≤	O
1	O
}	O
we	O
can	O
use	O
projc	O
(	O
θ	O
)	O
=	O
soft	O
(	O
θ	O
,	O
λ	O
)	O
where	O
λ	O
=	O
0	O
if	O
||θ||1	O
≤	O
1	O
,	O
and	O
otherwise	O
λ	O
is	O
the	O
solution	O
to	O
the	O
equation	O
d	O
(	O
cid:2	O
)	O
max	O
(	O
|θj|	O
−	O
λ	O
,	O
0	O
)	O
=	O
1	O
(	O
13.73	O
)	O
(	O
13.74	O
)	O
(	O
13.75	O
)	O
(	O
13.76	O
)	O
j=1	O
we	O
can	O
implement	O
the	O
whole	O
procedure	O
in	O
o	O
(	O
d	O
)	O
time	O
,	O
as	O
explained	O
in	O
(	O
duchi	O
et	O
al	O
.	O
2008	O
)	O
.	O
we	O
will	O
see	O
an	O
application	O
of	O
these	O
different	O
projection	B
methods	O
in	O
section	O
13.5.1.2	O
.	O
13.4.3.2	O
proximal	O
gradient	O
method	O
we	O
now	O
discuss	O
how	O
to	O
use	O
the	O
proximal	B
operator	I
inside	O
of	O
a	O
gradient	B
descent	I
routine	O
.	O
the	O
basic	O
idea	O
is	O
to	O
minimize	O
a	O
simple	O
quadratic	O
approximation	O
to	O
the	O
loss	B
function	I
,	O
centered	O
on	O
the	O
13.4	O
.	O
(	O
cid:6	O
)	O
1	O
regularization	B
:	O
algorithms	O
445	O
θk	O
:	O
z	O
r	O
(	O
z	O
)	O
+l	O
(	O
θk	O
)	O
+g	O
t	O
θk+1	O
=	O
argmin	O
(	O
13.77	O
)	O
where	O
gk	O
=	O
∇l	O
(	O
θk	O
)	O
is	O
the	O
gradient	O
of	O
the	O
loss	B
,	O
tk	O
is	O
a	O
constant	O
discussed	O
below	O
,	O
and	O
the	O
last	O
term	O
arises	O
from	O
a	O
simple	O
approximation	O
to	O
the	O
hessian	O
of	O
the	O
loss	B
of	O
the	O
form	O
∇2l	O
(	O
θk	O
)	O
≈	O
1	O
i.	O
dropping	O
terms	O
that	O
are	O
independent	O
of	O
z	O
,	O
and	O
multiplying	O
by	O
tk	O
,	O
we	O
can	O
rewrite	O
the	O
above	O
tk	O
k	O
(	O
z	O
−	O
θk	O
)	O
+	O
||z	O
−	O
θk||2	O
2	O
1	O
2tk	O
expression	O
in	O
terms	O
of	O
a	O
proximal	B
operator	I
as	O
follows	O
:	O
θk+1	O
=	O
argmin	O
tkr	O
(	O
z	O
)	O
+	O
=	O
proxtkr	O
(	O
uk	O
)	O
(	O
13.78	O
)	O
z	O
uk	O
=	O
θk	O
−	O
tkgk	O
gk	O
=	O
∇l	O
(	O
θk	O
)	O
(	O
13.79	O
)	O
(	O
13.80	O
)	O
if	O
r	O
(	O
θ	O
)	O
=	O
0	O
,	O
this	O
is	O
equivalent	O
to	O
gradient	B
descent	I
.	O
if	O
r	O
(	O
θ	O
)	O
=i	O
c	O
(	O
θ	O
)	O
,	O
the	O
method	O
is	O
equivalent	O
if	O
r	O
(	O
θ	O
)	O
=λ||θ	O
||1	O
,	O
the	O
method	O
is	O
to	O
projected	B
gradient	I
descent	I
,	O
sketched	O
in	O
figure	O
13.11.	O
known	O
as	O
iterative	B
soft	I
thresholding	I
.	O
mation	O
to	O
the	O
hessian	O
∇2l	O
,	O
we	O
require	O
that	O
there	O
are	O
several	O
ways	O
to	O
pick	O
tk	O
,	O
or	O
equivalently	O
,	O
αk	O
=	O
1/tk	O
.	O
given	O
that	O
αki	O
is	O
an	O
approxi-	O
(	O
cid:29	O
)	O
(	O
cid:30	O
)	O
||z	O
−	O
uk||2	O
2	O
1	O
2	O
αk	O
(	O
θk	O
−	O
θk−1	O
)	O
≈	O
gk	O
−	O
gk−1	O
in	O
the	O
least	B
squares	I
sense	O
.	O
hence	O
αk	O
=	O
argmin	O
α	O
||α	O
(	O
θk	O
−	O
θk−1	O
)	O
−	O
(	O
gk	O
−	O
gk−1	O
)	O
||2	O
2	O
=	O
(	O
θk	O
−	O
θk−1	O
)	O
t	O
(	O
gk	O
−	O
gk−1	O
)	O
(	O
θk	O
−	O
θk−1	O
)	O
t	O
(	O
θk	O
−	O
θk−1	O
)	O
(	O
13.81	O
)	O
(	O
13.82	O
)	O
this	O
is	O
known	O
as	O
the	O
barzilai-borwein	O
(	O
bb	O
)	O
or	O
spectral	B
stepsize	O
(	O
barzilai	O
and	O
borwein	O
1988	O
;	O
fletcher	O
2005	O
;	O
raydan	O
1997	O
)	O
.	O
this	O
stepsize	O
can	O
be	O
used	O
with	O
any	O
gradient	O
method	O
,	O
whether	O
proximal	O
or	O
not	O
.	O
it	O
does	O
not	O
lead	O
to	O
monotonic	O
decrease	O
of	O
the	O
objective	O
,	O
but	O
it	O
is	O
much	O
faster	O
than	O
standard	O
line	O
search	O
techniques	O
.	O
(	O
to	O
ensure	O
convergence	O
,	O
we	O
require	O
that	O
the	O
objective	O
decrease	O
“	O
on	O
average	O
”	O
,	O
where	O
the	O
average	O
is	O
computed	O
over	O
a	O
sliding	O
window	O
of	O
size	O
m	O
+	O
1	O
.	O
)	O
when	O
we	O
combine	O
the	O
bb	O
stepsize	O
with	O
the	O
iterative	B
soft	I
thresholding	I
technique	O
(	O
for	O
r	O
(	O
θ	O
)	O
=	O
λ||θ||1	O
)	O
,	O
plus	O
a	O
continuation	B
method	I
that	O
gradually	O
reduces	O
λ	O
,	O
we	O
get	O
a	O
fast	O
method	O
for	O
the	O
bpdn	O
problem	O
known	O
as	O
the	O
sparsa	O
algorithm	O
,	O
which	O
stands	O
for	O
“	O
sparse	B
reconstruction	O
by	O
separable	O
approximation	O
”	O
(	O
wright	O
et	O
al	O
.	O
2009	O
)	O
.	O
however	O
,	O
we	O
will	O
call	O
it	O
the	O
iterative	B
shrinkage	I
and	I
thresholding	I
algorithm	I
.	O
see	O
algorithm	O
12	O
for	O
some	O
pseudocode	O
,	O
and	O
sparsa	O
for	O
some	O
matlab	O
code	O
.	O
see	O
also	O
exercise	O
13.11	O
for	O
a	O
related	O
approach	O
based	O
on	O
projected	B
gradient	I
descent	I
.	O
13.4.3.3	O
nesterov	O
’	O
s	O
method	O
a	O
faster	O
version	O
of	O
proximal	O
gradient	O
descent	O
can	O
be	O
obtained	O
by	O
epxanding	O
the	O
quadratic	O
approximation	O
around	O
a	O
point	O
other	O
than	O
the	O
most	O
recent	O
parameter	B
value	O
.	O
in	O
particular	O
,	O
consider	O
performing	O
updates	O
of	O
the	O
form	O
θk+1	O
=	O
proxtkr	O
(	O
φk	O
−	O
tkgk	O
)	O
gk	O
=	O
∇l	O
(	O
φk	O
)	O
φk	O
=	O
θk	O
+	O
(	O
θk	O
−	O
θk−1	O
)	O
k	O
−	O
1	O
k	O
+	O
2	O
(	O
13.83	O
)	O
(	O
13.84	O
)	O
(	O
13.85	O
)	O
446	O
chapter	O
13.	O
sparse	B
linear	O
models	O
n×d	O
,	O
y	O
∈	O
r	O
n	O
,	O
parameters	O
λ	O
≥	O
0	O
,	O
m	O
≥	O
1	O
,	O
0	O
<	O
s	O
<	O
1	O
;	O
algorithm	O
13.2	O
:	O
iterative	O
shrinkage-thresholding	O
algorithm	O
(	O
ista	O
)	O
1	O
input	O
:	O
x	O
∈	O
r	O
2	O
initialize	O
θ0	O
=	O
0	O
,	O
α	O
=	O
1	O
,	O
r	O
=	O
y	O
,	O
λ0	O
=	O
∞	O
;	O
3	O
repeat	O
4	O
λt	O
=	O
max	O
(	O
s||xt	O
r||∞	O
,	O
λ	O
)	O
//	O
adapt	O
the	O
regularizer	O
;	O
repeat	O
5	O
6	O
7	O
8	O
9	O
10	O
g	O
=	O
∇l	O
(	O
θ	O
)	O
;	O
u	O
=	O
θ	O
−	O
1	O
α	O
g	O
;	O
θ	O
=	O
soft	O
(	O
u	O
,	O
λt	O
update	O
α	O
using	O
bb	O
stepsize	O
in	O
equation	O
13.82	O
;	O
α	O
)	O
;	O
until	O
f	O
(	O
θ	O
)	O
increased	O
too	O
much	O
within	O
the	O
past	O
m	O
steps	O
;	O
r	O
=	O
y	O
−	O
xθ	O
//	O
update	O
residual	B
;	O
11	O
12	O
until	O
λt	O
=	O
λ	O
;	O
γ	O
τj	O
wj	O
yi	O
xi	O
d	O
n	O
σ2	O
figure	O
13.12	O
representing	O
lasso	B
using	O
a	O
gaussian	O
scale	O
mixture	O
prior	O
.	O
this	O
is	O
known	O
as	O
nesterov	O
’	O
s	O
method	O
(	O
nesterov	O
2004	O
;	O
tseng	O
2008	O
)	O
.	O
as	O
before	O
,	O
there	O
are	O
a	O
variety	O
of	O
ways	O
of	O
setting	O
tk	O
;	O
typically	O
one	O
uses	O
line	B
search	I
.	O
when	O
this	O
method	O
is	O
combined	O
with	O
the	O
iterative	B
soft	I
thresholding	I
technique	O
(	O
for	O
r	O
(	O
θ	O
)	O
=	O
λ||θ||1	O
)	O
,	O
plus	O
a	O
continuation	B
method	I
that	O
gradually	O
reduces	O
λ	O
,	O
we	O
get	O
a	O
fast	O
method	O
for	O
the	O
bpdn	O
problem	O
known	O
as	O
the	O
fast	B
iterative	I
shrinkage	I
thesholding	I
algorithm	I
or	O
fista	O
(	O
beck	O
and	O
teboulle	O
2009	O
)	O
.	O
13.4	O
.	O
(	O
cid:6	O
)	O
1	O
regularization	B
:	O
algorithms	O
447	O
13.4.4	O
em	O
for	O
lasso	B
in	O
this	O
section	O
,	O
we	O
show	O
how	O
to	O
solve	O
the	O
lasso	B
problem	O
using	O
lasso	B
.	O
at	O
ﬁrst	O
sight	O
,	O
this	O
might	O
seem	O
odd	O
,	O
since	O
there	O
are	O
no	O
hidden	O
variables	O
.	O
the	O
key	O
insight	O
is	O
that	O
we	O
can	O
represent	O
the	O
laplace	O
distribution	O
as	O
a	O
gaussian	O
scale	O
mixture	O
(	O
gsm	O
)	O
(	O
andrews	O
and	O
mallows	O
1974	O
;	O
west	O
1987	O
)	O
as	O
follows	O
:	O
lap	O
(	O
wj|0	O
,	O
1/γ	O
)	O
=	O
e−γ|wj|	O
=	O
γ	O
2	O
n	O
(	O
wj|0	O
,	O
τ	O
2	O
j	O
)	O
ga	O
(	O
τ	O
2	O
j	O
|1	O
,	O
γ2	O
2	O
)	O
dτ	O
2	O
j	O
(	O
13.86	O
)	O
(	O
cid:28	O
)	O
j	O
|	O
γ2	O
2	O
=	O
ga	O
(	O
τ	O
2	O
thus	O
the	O
laplace	O
is	O
a	O
gsm	O
where	O
the	O
mixing	O
distibution	O
on	O
the	O
variances	O
is	O
the	O
exponential	O
j	O
|1	O
,	O
γ2	O
distribution	O
,	O
expon	O
(	O
τ	O
2	O
2	O
)	O
.	O
using	O
this	O
decomposition	O
,	O
we	O
can	O
represent	O
the	O
lasso	B
model	O
as	O
shown	O
in	O
figure	O
13.12.	O
the	O
corresponding	O
joint	B
distribution	I
has	O
the	O
form	O
⎡	O
⎣	O
(	O
cid:27	O
)	O
p	O
(	O
y	O
,	O
w	O
,	O
τ	O
,	O
σ2|x	O
)	O
=n	O
(	O
y|xw	O
,	O
σ2in	O
)	O
n	O
(	O
w|0	O
,	O
dτ	O
)	O
⎤	O
⎦	O
ig	O
(	O
σ2|aσ	O
,	O
bσ	O
)	O
ga	O
(	O
τ	O
2	O
j	O
|1	O
,	O
γ2/2	O
)	O
(	O
13.87	O
)	O
j	O
where	O
dτ	O
=	O
diag	O
(	O
τ	O
2	O
j	O
)	O
,	O
and	O
where	O
we	O
have	O
assumed	O
for	O
notational	O
simplicity	O
that	O
x	O
is	O
stan-	O
dardized	O
and	O
that	O
y	O
is	O
centered	O
(	O
so	O
we	O
can	O
ignore	O
the	O
offset	O
term	O
μ	O
)	O
.	O
expanding	O
out	O
,	O
we	O
get	O
p	O
(	O
y	O
,	O
w	O
,	O
τ	O
,	O
σ2|x	O
)	O
∝	O
(	O
cid:3	O
)	O
σ2	O
(	O
cid:8	O
)	O
(	O
cid:4	O
)	O
−n/2	O
(	O
cid:8	O
)	O
exp	O
−	O
1	O
(	O
cid:9	O
)	O
2σ2	O
(	O
cid:9	O
)	O
||y	O
−	O
xw||2	O
2	O
|dτ|−	O
1	O
2	O
−	O
(	O
aσ+1	O
)	O
−	O
1	O
2	O
wt	O
dτ	O
w	O
(	O
cid:27	O
)	O
exp	O
exp	O
(	O
−bσ/σ2	O
)	O
(	O
σ2	O
)	O
exp	O
(	O
−	O
γ2	O
2	O
j	O
τ	O
2	O
j	O
)	O
(	O
13.88	O
)	O
below	O
we	O
describe	O
how	O
to	O
apply	O
the	O
em	O
algorithm	O
to	O
the	O
model	O
in	O
figure	O
13.12.5	O
in	O
brief	O
,	O
in	O
the	O
e	O
step	O
we	O
infer	O
τ	O
2	O
j	O
and	O
σ2	O
,	O
and	O
in	O
the	O
m	O
step	O
we	O
estimate	O
w.	O
the	O
resulting	O
estimate	O
ˆw	O
is	O
the	O
same	O
as	O
the	O
lasso	B
estimator	O
.	O
this	O
approach	O
was	O
ﬁrst	O
proposed	O
in	O
(	O
figueiredo	O
2003	O
)	O
(	O
see	O
also	O
(	O
griffin	O
and	O
brown	O
2007	O
;	O
caron	O
and	O
doucet	O
2008	O
;	O
ding	O
and	O
harrison	O
2010	O
)	O
for	O
some	O
extensions	O
)	O
.	O
13.4.4.1	O
why	O
em	O
?	O
before	O
going	O
into	O
the	O
details	O
of	O
em	O
,	O
it	O
is	O
worthwhile	O
asking	O
why	O
we	O
are	O
presenting	O
this	O
approach	O
at	O
all	O
,	O
given	O
that	O
there	O
are	O
a	O
variety	O
of	O
other	O
(	O
often	O
much	O
faster	O
)	O
algorithms	O
that	O
directly	O
solve	O
the	O
(	O
cid:6	O
)	O
1	O
map	O
estimation	O
problem	O
(	O
see	O
linregfitl1test	O
for	O
an	O
empirical	O
comparison	O
)	O
.	O
the	O
reason	O
is	O
that	O
the	O
latent	O
variable	O
perspective	O
brings	O
several	O
advantages	O
,	O
such	O
as	O
the	O
following	O
:	O
•	O
it	O
provides	O
an	O
easy	O
way	O
to	O
derive	O
an	O
algorithm	O
to	O
ﬁnd	O
(	O
cid:6	O
)	O
1-regularized	O
parameter	B
estimates	O
for	O
a	O
variety	O
of	O
other	O
models	O
,	O
such	O
as	O
robust	B
linear	O
regression	B
(	O
exercise	O
11.12	O
)	O
or	O
probit	B
regression	I
(	O
exercise	O
13.9	O
)	O
.	O
5.	O
to	O
ensure	O
the	O
posterior	O
is	O
unimodal	O
,	O
one	O
can	O
follow	O
(	O
park	O
and	O
casella	O
2008	O
)	O
and	O
slightly	O
modify	O
the	O
model	O
by	O
making	O
the	O
prior	O
variance	B
for	O
the	O
weights	O
depend	O
on	O
the	O
observation	B
noise	O
:	O
p	O
(	O
wj|τ	O
2	O
j	O
)	O
.	O
the	O
em	O
algorithm	O
is	O
easy	O
to	O
modify	O
.	O
j	O
,	O
σ2	O
)	O
=n	O
(	O
wj|0	O
,	O
σ2τ	O
2	O
448	O
chapter	O
13.	O
sparse	B
linear	O
models	O
•	O
•	O
it	O
suggests	O
trying	O
other	O
priors	O
on	O
the	O
variances	O
besides	O
ga	O
(	O
τ	O
2	O
various	O
extensions	O
below	O
.	O
it	O
makes	O
it	O
clear	O
how	O
we	O
can	O
compute	O
the	O
full	B
posterior	O
,	O
p	O
(	O
w|d	O
)	O
,	O
rather	O
than	O
just	O
a	O
map	O
estimate	O
.	O
this	O
technique	O
is	O
known	O
as	O
the	O
bayesian	O
lasso	B
(	O
park	O
and	O
casella	O
2008	O
;	O
hans	O
2009	O
)	O
.	O
j	O
|1	O
,	O
γ2/2	O
)	O
.	O
we	O
will	O
consider	O
13.4.4.2	O
the	O
objective	O
function	O
from	O
equation	O
13.88	O
,	O
the	O
complete	B
data	I
penalized	O
log	O
likelihood	O
is	O
as	O
follows	O
(	O
dropping	O
terms	O
that	O
do	O
not	O
depend	O
on	O
w	O
)	O
(	O
cid:6	O
)	O
c	O
(	O
w	O
)	O
=−	O
1	O
2σ2	O
||y	O
−	O
xw||2	O
2	O
−	O
1	O
2	O
wt	O
λw	O
+	O
const	O
where	O
λ	O
=	O
diag	O
(	O
1	O
τ	O
2	O
j	O
)	O
is	O
the	O
precision	B
matrix	I
for	O
w.	O
(	O
13.89	O
)	O
13.4.4.3	O
the	O
e	O
step	O
+	O
,	O
|wj	O
1	O
τ	O
2	O
j	O
the	O
key	O
is	O
to	O
compute	O
e	O
we	O
can	O
derive	O
the	O
full	B
posterior	O
,	O
which	O
is	O
given	O
by	O
the	O
following	O
(	O
park	O
and	O
casella	O
2008	O
)	O
:	O
.	O
we	O
can	O
derive	O
this	O
directly	O
(	O
see	O
exercise	O
13.8	O
)	O
.	O
alternatively	O
,	O
(	O
cid:11	O
)	O
7	O
(	O
cid:13	O
)	O
p	O
(	O
1/τ	O
2	O
j	O
|w	O
,	O
d	O
)	O
=inversegaussian	O
γ2	O
w2	O
j	O
,	O
γ2	O
(	O
13.90	O
)	O
(	O
note	O
that	O
the	O
inverse	O
gaussian	O
distribution	O
is	O
also	O
known	O
as	O
the	O
wald	O
distribution	O
.	O
)	O
hence	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
e	O
|wj	O
1	O
τ	O
2	O
j	O
γ	O
|wj|	O
=	O
(	O
cid:31	O
)	O
(	O
cid:31	O
)	O
let	O
λ	O
=	O
diag	O
(	O
e	O
1/τ	O
2	O
1	O
1/τ	O
2	O
d	O
,	O
.	O
.	O
.	O
,	O
e	O
)	O
denote	O
the	O
result	O
of	O
this	O
e	O
step	O
.	O
we	O
also	O
need	O
to	O
infer	O
σ2	O
.	O
it	O
is	O
easy	O
to	O
show	O
that	O
that	O
the	O
posterior	O
is	O
p	O
(	O
σ2|d	O
,	O
w	O
)	O
=	O
ig	O
(	O
aσ	O
+	O
(	O
n	O
)	O
/2	O
,	O
bσ	O
+	O
(	O
cid:31	O
)	O
1	O
2	O
hence	O
(	O
y	O
−	O
x	O
ˆw	O
)	O
t	O
(	O
y	O
−	O
x	O
ˆw	O
)	O
)	O
=	O
ig	O
(	O
an	O
,	O
bn	O
)	O
e	O
1/σ2	O
=	O
(	O
cid:2	O
)	O
ω	O
an	O
bn	O
(	O
13.91	O
)	O
(	O
13.92	O
)	O
(	O
13.93	O
)	O
(	O
13.94	O
)	O
(	O
13.95	O
)	O
13.4.4.4	O
the	O
m	O
step	O
the	O
m	O
step	O
consists	O
of	O
computing	O
ω||y	O
−	O
xw||2	O
ˆw	O
=	O
argmax	O
−	O
1	O
2	O
w	O
2	O
−	O
1	O
2	O
wt	O
λw	O
this	O
is	O
just	O
map	O
estimation	O
under	O
a	O
gaussian	O
prior	O
:	O
ˆw	O
=	O
(	O
σ2λ	O
+	O
xt	O
x	O
)	O
−1xt	O
y	O
13.5	O
.	O
(	O
cid:6	O
)	O
1	O
regularization	B
:	O
extensions	O
449	O
however	O
,	O
since	O
we	O
expect	O
many	O
wj	O
=	O
0	O
,	O
we	O
will	O
have	O
τ	O
2	O
j	O
=	O
0	O
for	O
many	O
j	O
,	O
making	O
inverting	O
λ	O
numerically	O
unstable	B
.	O
fortunately	O
,	O
we	O
can	O
use	O
the	O
svd	O
of	O
x	O
,	O
given	O
by	O
x	O
=	O
udvt	O
,	O
as	O
follows	O
:	O
ˆw	O
=	O
ψv	O
(	O
vt	O
ψv	O
+	O
d−2	O
)	O
−1d−1ut	O
y	O
1	O
ω	O
where	O
−1	O
ψ	O
=	O
λ	O
=	O
diag	O
(	O
(	O
cid:31	O
)	O
1	O
1/τ	O
2	O
j	O
e	O
)	O
=	O
diag	O
(	O
|wj|	O
π	O
(	O
cid:4	O
)	O
(	O
wj	O
)	O
)	O
13.4.4.5	O
caveat	O
(	O
13.96	O
)	O
(	O
13.97	O
)	O
since	O
the	O
lasso	B
objective	O
is	O
convex	B
,	O
this	O
method	O
should	O
always	O
ﬁnd	O
the	O
global	O
optimum	O
.	O
unfor-	O
tunately	O
,	O
this	O
sometimes	O
does	O
not	O
happen	O
,	O
for	O
numerical	O
reasons	O
.	O
in	O
particular	O
,	O
suppose	O
that	O
in	O
j	O
(	O
cid:22	O
)	O
=	O
0.	O
further	O
,	O
suppose	O
that	O
we	O
set	O
ˆwj	O
=	O
0	O
in	O
an	O
m	O
step	O
.	O
in	O
the	O
following	O
e	O
the	O
true	O
solution	O
,	O
w∗	O
step	O
we	O
infer	O
that	O
τ	O
2	O
j	O
=	O
0	O
,	O
so	O
then	O
we	O
set	O
ˆwj	O
=	O
0	O
again	O
;	O
thus	O
we	O
can	O
never	O
“	O
undo	O
”	O
our	O
mistake	O
.	O
fortunately	O
,	O
in	O
practice	O
,	O
this	O
situation	O
seems	O
to	O
be	O
rare	O
.	O
see	O
(	O
hunter	O
and	O
li	O
2005	O
)	O
for	O
further	O
discussion	O
.	O
13.5	O
(	O
cid:9	O
)	O
1	O
regularization	B
:	O
extensions	O
in	O
this	O
section	O
,	O
we	O
discuss	O
various	O
extensions	O
of	O
“	O
vanilla	O
”	O
(	O
cid:6	O
)	O
1	O
regularization	B
.	O
13.5.1	O
group	B
lasso	I
in	O
standard	O
(	O
cid:6	O
)	O
1	O
regularization	B
,	O
we	O
assume	O
that	O
there	O
is	O
a	O
1:1	O
correspondence	B
between	O
parameters	O
and	O
variables	O
,	O
so	O
that	O
if	O
ˆwj	O
=	O
0	O
,	O
we	O
interpret	O
this	O
to	O
mean	B
that	O
variable	O
j	O
is	O
excluded	O
.	O
but	O
in	O
more	O
complex	O
models	O
,	O
there	O
may	O
be	O
many	O
parameters	O
associated	O
with	O
a	O
given	O
variable	O
.	O
in	O
particular	O
,	O
we	O
may	O
have	O
a	O
vector	O
of	O
weights	O
for	O
each	O
input	O
,	O
wj	O
.	O
here	O
are	O
some	O
examples	O
:	O
•	O
multinomial	B
logistic	I
regression	I
each	O
feature	O
is	O
associated	O
with	O
c	O
different	O
weights	O
,	O
one	O
per	O
class	O
.	O
•	O
linear	B
regression	I
with	O
categorical	B
inputs	O
each	O
scalar	O
input	O
is	O
one-hot	O
encoded	O
into	O
a	O
vector	O
of	O
length	O
c.	O
•	O
multi-task	B
learning	I
in	O
multi-task	B
learning	I
,	O
we	O
have	O
multiple	O
related	O
prediction	O
problems	O
.	O
for	O
example	O
,	O
we	O
might	O
have	O
c	O
separate	O
regression	B
or	O
binary	B
classiﬁcation	I
problems	O
.	O
thus	O
each	O
feature	O
is	O
associated	O
with	O
c	O
different	O
weights	O
.	O
we	O
may	O
want	O
to	O
use	O
a	O
feature	O
for	O
all	O
of	O
the	O
tasks	O
or	O
none	O
of	O
the	O
tasks	O
,	O
and	O
thus	O
select	O
weights	O
at	O
the	O
group	O
level	O
(	O
obozinski	O
et	O
al	O
.	O
2007	O
)	O
.	O
if	O
we	O
use	O
an	O
(	O
cid:6	O
)	O
1	O
regularizer	O
of	O
the	O
form	O
||w||	O
=	O
c	O
|wjc|	O
,	O
we	O
may	O
end	O
up	O
with	O
with	O
some	O
elements	O
of	O
wj	O
,	O
:	O
being	O
zero	O
and	O
some	O
not	O
.	O
to	O
prevent	O
this	O
kind	O
of	O
situation	O
,	O
we	O
partition	O
the	O
parameter	B
vector	O
into	O
g	O
groups	O
.	O
we	O
now	O
minimize	O
the	O
following	O
objective	O
j	O
(	O
cid:10	O
)	O
(	O
cid:10	O
)	O
j	O
(	O
w	O
)	O
=	O
nll	O
(	O
w	O
)	O
+	O
λg||wg||2	O
(	O
13.98	O
)	O
g	O
(	O
cid:2	O
)	O
g=1	O
450	O
where	O
||wg||2	O
=	O
7	O
(	O
cid:2	O
)	O
j∈g	O
w2	O
j	O
chapter	O
13.	O
sparse	B
linear	O
models	O
(	O
13.99	O
)	O
is	O
the	O
2-norm	O
of	O
the	O
group	O
weight	O
vector	O
.	O
group	B
lasso	I
(	O
yuan	O
and	O
lin	O
2006	O
)	O
.	O
if	O
the	O
nll	O
is	O
least	B
squares	I
,	O
this	O
method	O
is	O
called	O
(	O
cid:17	O
)	O
we	O
often	O
use	O
a	O
larger	O
penalty	O
for	O
larger	O
groups	O
,	O
by	O
setting	O
λg	O
=	O
λ	O
(	O
cid:30	O
)	O
number	O
of	O
elements	O
in	O
group	O
g.	O
for	O
example	O
,	O
objective	O
becomes	O
√	O
dg	O
,	O
where	O
dg	O
is	O
the	O
if	O
we	O
have	O
groups	O
{	O
1	O
,	O
2	O
}	O
and	O
{	O
3	O
,	O
4	O
,	O
5	O
}	O
,	O
the	O
2	O
(	O
cid:29	O
)	O
√	O
2	O
j	O
(	O
w	O
)	O
=	O
nll	O
(	O
w	O
)	O
+λ	O
2	O
(	O
w2	O
1	O
+	O
w2	O
2|	O
)	O
+	O
3	O
(	O
w2	O
3	O
+	O
w2	O
4	O
+	O
w2	O
5	O
)	O
(	O
13.100	O
)	O
note	O
that	O
if	O
we	O
had	O
used	O
the	O
square	O
of	O
the	O
2-norms	O
,	O
the	O
model	O
would	O
become	O
equivalent	O
to	O
ridge	B
regression	I
,	O
since	O
j	O
=	O
||w||2	O
w2	O
2	O
(	O
13.101	O
)	O
g	O
(	O
cid:2	O
)	O
||wg||2	O
2	O
=	O
g=1	O
g	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
j∈g	O
by	O
using	O
the	O
square	O
root	O
,	O
we	O
are	O
penalizing	O
the	O
radius	O
of	O
a	O
ball	O
containing	O
the	O
group	O
’	O
s	O
weight	B
vector	I
:	O
the	O
only	O
way	O
for	O
the	O
radius	O
to	O
be	O
small	O
is	O
if	O
all	O
elements	O
are	O
small	O
.	O
thus	O
the	O
square	O
root	O
results	O
in	O
group	O
sparsity	O
.	O
a	O
variant	O
of	O
this	O
technique	O
replaces	O
the	O
2-norm	O
with	O
the	O
inﬁnity-norm	O
(	O
turlach	O
et	O
al	O
.	O
2005	O
;	O
zhao	O
et	O
al	O
.	O
2005	O
)	O
:	O
||wg||∞	O
=	O
max	O
j∈g	O
|wj|	O
(	O
13.102	O
)	O
it	O
is	O
clear	O
that	O
this	O
will	O
also	O
result	O
in	O
group	O
sparsity	O
.	O
an	O
illustration	O
of	O
the	O
difference	O
is	O
shown	O
in	O
figures	O
13.13	O
and	O
13.14.	O
in	O
both	O
cases	O
,	O
we	O
have	O
a	O
true	O
signal	O
w	O
of	O
size	O
d	O
=	O
212	O
=	O
4096	O
,	O
divided	O
into	O
64	O
groups	O
each	O
of	O
size	O
64.	O
we	O
randomly	O
choose	O
8	O
groups	O
of	O
w	O
and	O
assign	O
them	O
non-zero	O
values	O
.	O
in	O
the	O
ﬁrst	O
example	O
,	O
the	O
values	O
are	O
drawn	O
from	O
a	O
n	O
(	O
0	O
,	O
1	O
)	O
.	O
in	O
the	O
second	O
example	O
,	O
the	O
values	O
are	O
all	O
set	O
to	O
1.	O
we	O
then	O
pick	O
a	O
random	O
design	O
matrix	O
x	O
of	O
size	O
n	O
×	O
d	O
,	O
where	O
n	O
=	O
210	O
=	O
1024.	O
finally	O
,	O
we	O
generate	O
y	O
=	O
xw	O
+	O
	O
,	O
where	O
	O
∼	O
n	O
(	O
0	O
,	O
10	O
−4in	O
)	O
.	O
given	O
this	O
data	O
,	O
we	O
estimate	O
the	O
support	B
of	O
w	O
using	O
(	O
cid:6	O
)	O
1	O
or	O
group	O
(	O
cid:6	O
)	O
1	O
,	O
and	O
then	O
estimate	O
the	O
non-zero	O
values	O
using	O
least	B
squares	I
.	O
we	O
see	O
that	O
group	B
lasso	I
does	O
a	O
much	O
better	O
job	O
than	O
vanilla	O
lasso	B
,	O
since	O
it	O
respects	O
the	O
known	O
group	O
structure.6	O
we	O
also	O
see	O
that	O
the	O
(	O
cid:6	O
)	O
∞	O
norm	O
has	O
a	O
tendency	O
to	O
make	O
all	O
the	O
elements	O
within	O
a	O
block	O
to	O
have	O
similar	B
magnitude	O
.	O
this	O
is	O
appropriate	O
in	O
the	O
second	O
example	O
,	O
but	O
not	O
the	O
ﬁrst	O
.	O
(	O
the	O
value	O
of	O
λ	O
was	O
the	O
same	O
in	O
all	O
examples	O
,	O
and	O
was	O
chosen	O
by	O
hand	O
.	O
)	O
13.5.1.1	O
gsm	O
interpretation	O
of	O
group	B
lasso	I
group	O
lasso	B
is	O
equivalent	O
to	O
map	O
estimation	O
using	O
the	O
following	O
prior	O
p	O
(	O
w|γ	O
,	O
σ2	O
)	O
∝	O
exp	O
(	O
cid:11	O
)	O
g	O
(	O
cid:2	O
)	O
g=1	O
−	O
γ	O
σ	O
(	O
cid:13	O
)	O
||wg||2	O
(	O
13.103	O
)	O
6.	O
the	O
slight	O
non-zero	O
“	O
noise	O
”	O
in	O
the	O
(	O
cid:7	O
)	O
∞	O
group	B
lasso	I
results	O
is	O
presumably	O
due	O
to	O
numerical	O
errors	O
.	O
13.5	O
.	O
(	O
cid:6	O
)	O
1	O
regularization	B
:	O
extensions	O
451	O
original	O
(	O
d	O
=	O
4096	O
,	O
number	O
groups	O
=	O
64	O
,	O
active	O
groups	O
=	O
8	O
)	O
500	O
1000	O
1500	O
2000	O
2500	O
3000	O
3500	O
4000	O
standard	O
l1	O
(	O
debiased	O
1	O
,	O
tau	O
=	O
0.385	O
,	O
mse	O
=	O
0.06929	O
)	O
2	O
0	O
−2	O
0	O
2	O
0	O
−2	O
0	O
500	O
1000	O
1500	O
2000	O
2500	O
3000	O
3500	O
4000	O
(	O
a	O
)	O
block−l2	O
(	O
debiased	O
1	O
,	O
tau	O
=	O
0.385	O
,	O
mse	O
=	O
0.000351	O
)	O
2	O
0	O
−2	O
0	O
500	O
1000	O
1500	O
2000	O
2500	O
3000	O
3500	O
4000	O
block−linf	O
(	O
debiased	O
1	O
,	O
tau	O
=	O
0.385	O
,	O
mse	O
=	O
0.053	O
)	O
2	O
0	O
−2	O
0	O
500	O
1000	O
1500	O
2000	O
2500	O
3000	O
3500	O
4000	O
(	O
b	O
)	O
figure	O
13.13	O
illustration	O
of	O
group	B
lasso	I
where	O
the	O
original	O
signal	O
is	O
piecewise	O
gaussian	O
.	O
top	O
left	O
:	O
original	O
signal	O
.	O
bottom	O
left	O
:	O
:	O
vanilla	O
lasso	B
estimate	O
.	O
top	O
right	O
:	O
group	B
lasso	I
estimate	O
using	O
a	O
(	O
cid:7	O
)	O
2	O
norm	O
on	O
the	O
blocks	O
.	O
bottom	O
right	O
:	O
group	B
lasso	I
estimate	O
using	O
an	O
(	O
cid:7	O
)	O
∞	O
norm	O
on	O
the	O
blocks	O
.	O
based	O
on	O
figures	O
3-4	O
of	O
(	O
wright	O
et	O
al	O
.	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
grouplassodemo	O
,	O
based	O
on	O
code	O
by	O
mario	O
figueiredo	O
.	O
now	O
one	O
can	O
show	O
(	O
exercise	O
13.10	O
)	O
that	O
this	O
prior	O
can	O
be	O
written	O
as	O
a	O
gsm	O
,	O
as	O
follows	O
:	O
wg|σ2	O
,	O
τ	O
2	O
g	O
∼	O
n	O
(	O
0	O
,	O
σ2τ	O
2	O
g|γ	O
∼	O
ga	O
(	O
dg	O
+	O
1	O
τ	O
2	O
g	O
idg	O
)	O
γ	O
,	O
)	O
2	O
2	O
(	O
13.104	O
)	O
(	O
13.105	O
)	O
where	O
dg	O
is	O
the	O
size	O
of	O
group	O
g.	O
so	O
we	O
see	O
that	O
there	O
is	O
one	O
variance	B
term	O
per	O
group	O
,	O
each	O
of	O
which	O
comes	O
from	O
a	O
gamma	O
prior	O
,	O
whose	O
shape	O
parameter	B
depends	O
on	O
the	O
group	O
size	O
,	O
and	O
whose	O
rate	B
parameter	O
is	O
controlled	O
by	O
γ.	O
figure	O
13.15	O
gives	O
an	O
example	O
,	O
where	O
we	O
have	O
2	O
groups	O
,	O
one	O
of	O
size	O
2	O
and	O
one	O
of	O
size	O
3.	O
this	O
picture	O
also	O
makes	O
it	O
clearer	O
why	O
there	O
should	O
be	O
a	O
grouping	B
effect	I
.	O
suppose	O
w1,1	O
is	O
1	O
will	O
be	O
estimated	O
to	O
be	O
small	O
,	O
which	O
will	O
force	O
w1,2	O
to	O
be	O
small	O
.	O
converseley	O
,	O
1	O
will	O
be	O
estimated	O
to	O
be	O
large	O
,	O
which	O
will	O
allow	O
w1,2	O
to	O
be	O
become	O
small	O
;	O
then	O
τ	O
2	O
suppose	O
w1,1	O
is	O
large	O
;	O
then	O
τ	O
2	O
large	O
as	O
well	O
.	O
452	O
chapter	O
13.	O
sparse	B
linear	O
models	O
1	O
0.5	O
0	O
0	O
1	O
0.5	O
0	O
0	O
1	O
0.5	O
0	O
0	O
1	O
0.5	O
0	O
0	O
original	O
(	O
d	O
=	O
4096	O
,	O
number	O
groups	O
=	O
64	O
,	O
active	O
groups	O
=	O
8	O
)	O
500	O
1000	O
1500	O
2000	O
2500	O
3000	O
3500	O
4000	O
standard	O
l1	O
(	O
debiased	O
1	O
,	O
tau	O
=	O
0.356	O
,	O
mse	O
=	O
0.1206	O
)	O
500	O
1000	O
1500	O
2000	O
2500	O
3000	O
3500	O
4000	O
(	O
a	O
)	O
block−l2	O
(	O
debiased	O
1	O
,	O
tau	O
=	O
0.356	O
,	O
mse	O
=	O
0.000342	O
)	O
500	O
1000	O
1500	O
2000	O
2500	O
3000	O
3500	O
4000	O
block−linf	O
(	O
debiased	O
1	O
,	O
tau	O
=	O
0.356	O
,	O
mse	O
=	O
0.000425	O
)	O
500	O
1000	O
1500	O
2000	O
2500	O
3000	O
3500	O
4000	O
(	O
b	O
)	O
figure	O
13.14	O
same	O
as	O
figure	O
13.13	O
,	O
except	O
the	O
original	O
signal	O
is	O
piecewise	O
constant	O
.	O
γ	O
τ1	O
τ2	O
w11	O
w12	O
w21	O
w22	O
w23	O
σ2	O
yi	O
xi	O
figure	O
13.15	O
graphical	B
model	I
for	O
group	B
lasso	I
with	O
2	O
groups	O
,	O
the	O
ﬁrst	O
has	O
size	O
g1	O
=	O
2	O
,	O
the	O
second	O
has	O
size	O
g2	O
=	O
3	O
.	O
13.5	O
.	O
(	O
cid:6	O
)	O
1	O
regularization	B
:	O
extensions	O
453	O
13.5.1.2	O
algorithms	O
for	O
group	B
lasso	I
(	O
cid:10	O
)	O
there	O
are	O
a	O
variety	O
of	O
algorithms	O
for	O
group	B
lasso	I
.	O
here	O
we	O
brieﬂy	O
mention	O
two	O
.	O
the	O
ﬁrst	O
approach	O
is	O
based	O
on	O
proximal	O
gradient	O
descent	O
,	O
discussed	O
in	O
section	O
13.4.3.	O
since	O
the	O
regularizer	O
g	O
||wg||p	O
,	O
the	O
proximal	B
operator	I
decomposes	O
into	O
g	O
separate	O
operators	O
is	O
separable	O
,	O
r	O
(	O
w	O
)	O
=	O
of	O
the	O
form	O
||z	O
−	O
b||2	O
(	O
13.106	O
)	O
where	O
b	O
=	O
θkg	O
−	O
tkgkg	O
.	O
if	O
p	O
=	O
2	O
,	O
one	O
can	O
show	O
(	O
combettes	O
and	O
wajs	O
2005	O
)	O
that	O
this	O
can	O
be	O
implemented	O
as	O
follows	O
proxr	O
(	O
b	O
)	O
=	O
argmin	O
2	O
+	O
λ||z||p	O
z∈r	O
dg	O
where	O
c	O
=	O
{	O
z	O
:	O
||z||2	O
≤	O
1	O
}	O
is	O
the	O
(	O
cid:6	O
)	O
2	O
ball	O
.	O
using	O
equation	O
13.74	O
,	O
if	O
||b||2	O
<	O
λ	O
,	O
we	O
have	O
proxr	O
(	O
b	O
)	O
=	O
b	O
−	O
projλc	O
(	O
b	O
)	O
proxr	O
(	O
b	O
)	O
=	O
b	O
−	O
b	O
=	O
0	O
otherwise	O
we	O
have	O
proxr	O
(	O
b	O
)	O
=	O
b	O
−	O
λ	O
b	O
||b||2	O
=	O
b	O
||b||2	O
−	O
λ	O
||b||2	O
(	O
13.107	O
)	O
(	O
13.108	O
)	O
(	O
13.109	O
)	O
(	O
13.111	O
)	O
(	O
13.112	O
)	O
we	O
can	O
combine	O
these	O
into	O
a	O
vectorial	O
soft-threshold	O
function	O
as	O
follows	O
(	O
wright	O
et	O
al	O
.	O
2009	O
)	O
:	O
proxr	O
(	O
b	O
)	O
=	O
b	O
max	O
(	O
||b||2	O
−	O
λ	O
,	O
0	O
)	O
max	O
(	O
||b||2	O
−	O
λ	O
,	O
0	O
)	O
+	O
λ	O
(	O
13.110	O
)	O
if	O
p	O
=	O
∞	O
,	O
we	O
use	O
c	O
=	O
{	O
z	O
:	O
||z||1	O
≤	O
1	O
}	O
,	O
which	O
is	O
the	O
(	O
cid:6	O
)	O
1	O
ball	O
.	O
we	O
can	O
project	O
onto	O
this	O
in	O
o	O
(	O
dg	O
)	O
time	O
using	O
an	O
algorithm	O
described	O
in	O
(	O
duchi	O
et	O
al	O
.	O
2008	O
)	O
.	O
another	O
approach	O
is	O
to	O
modify	O
the	O
em	O
algorithm	O
.	O
the	O
method	O
is	O
almost	O
the	O
same	O
as	O
for	O
vanilla	O
lasso	B
.	O
if	O
we	O
deﬁne	O
τ	O
2	O
g	O
(	O
j	O
)	O
,	O
where	O
g	O
(	O
j	O
)	O
is	O
the	O
group	O
to	O
which	O
dimension	O
j	O
belongs	O
,	O
we	O
can	O
use	O
the	O
same	O
full	B
conditionals	O
for	O
σ2	O
and	O
w	O
as	O
before	O
.	O
the	O
only	O
changes	O
are	O
as	O
follows	O
:	O
j	O
=	O
τ	O
2	O
•	O
we	O
must	O
modify	O
the	O
full	B
conditional	I
for	O
the	O
weight	O
precisions	O
,	O
which	O
are	O
estimated	O
based	O
on	O
7	O
|γ	O
,	O
w	O
,	O
σ2	O
,	O
y	O
,	O
x	O
∼	O
inversegaussian	O
(	O
γ2σ2	O
||wg||2	O
2	O
,	O
γ2	O
)	O
j∈g	O
w2	O
jg	O
.	O
for	O
the	O
e	O
step	O
,	O
we	O
can	O
use	O
a	O
shared	B
set	O
of	O
weights	O
:	O
1	O
τ	O
2	O
g	O
e	O
(	O
cid:10	O
)	O
where	O
||wg||2	O
(	O
cid:30	O
)	O
(	O
cid:29	O
)	O
2	O
=	O
1	O
τ	O
2	O
g	O
=	O
γσ	O
||wg||2	O
•	O
we	O
must	O
modify	O
the	O
full	B
conditional	I
for	O
the	O
tuning	O
parameter	B
,	O
which	O
is	O
now	O
only	O
estimated	O
based	O
on	O
g	O
values	O
of	O
τ	O
2	O
g	O
:	O
p	O
(	O
γ2|τ	O
)	O
=	O
ga	O
(	O
aγ	O
+	O
g/2	O
,	O
bγ	O
+	O
g	O
(	O
cid:2	O
)	O
g	O
1	O
2	O
τ	O
2	O
g	O
)	O
(	O
13.113	O
)	O
chapter	O
13.	O
sparse	B
linear	O
models	O
●	O
●	O
●	O
●	O
454	O
5	O
0	O
.	O
0	O
.	O
0	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●●	O
●	O
●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●●	O
●	O
●	O
●	O
●	O
●●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●●	O
●●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
●	O
h	O
g	O
c	O
5	O
.	O
0	O
−	O
●	O
●	O
●	O
●	O
●	O
●	O
0	O
.	O
1	O
−	O
5	O
.	O
1	O
−	O
0	O
100	O
300	O
400	O
●	O
200	O
index	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
13.16	O
(	O
a	O
)	O
example	O
of	O
the	O
fused	B
lasso	I
.	O
the	O
vertical	O
axis	O
represents	O
array	O
cgh	O
(	O
chromosomal	O
genome	B
source	O
:	O
figure	O
1	O
of	O
hybridization	O
)	O
intensity	O
,	O
and	O
the	O
horizontal	O
axis	O
represents	O
location	O
along	O
a	O
genome	B
.	O
(	O
hoeﬂing	O
2010	O
)	O
.	O
source	O
:	O
figure	O
2	O
of	O
(	O
hoeﬂing	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
holger	O
hoeﬂing	O
.	O
(	O
c	O
)	O
fused	B
lasso	I
estimate	O
using	O
2d	O
lattice	B
prior	O
.	O
(	O
b	O
)	O
noisy	O
image	O
.	O
13.5.2	O
fused	B
lasso	I
in	O
some	O
problem	O
settings	O
(	O
e.g.	O
,	O
functional	B
data	I
analysis	I
)	O
,	O
we	O
want	O
neighboring	O
coefficients	O
to	O
be	O
similar	B
to	O
each	O
other	O
,	O
in	O
addition	O
to	O
being	O
sparse	B
.	O
an	O
example	O
is	O
given	O
in	O
figure	O
13.16	O
(	O
a	O
)	O
,	O
where	O
we	O
want	O
to	O
ﬁt	O
a	O
signal	O
that	O
is	O
mostly	O
“	O
off	O
”	O
,	O
but	O
in	O
addition	O
has	O
the	O
property	O
that	O
neighboring	O
locations	O
are	O
typically	O
similar	B
in	O
value	O
.	O
we	O
can	O
model	O
this	O
by	O
using	O
a	O
prior	O
of	O
the	O
form	O
⎛	O
⎝−	O
λ1	O
σ	O
d	O
(	O
cid:2	O
)	O
j=1	O
d−1	O
(	O
cid:2	O
)	O
j=1	O
⎞	O
⎠	O
p	O
(	O
w|σ2	O
)	O
∝	O
exp	O
|wj|	O
−	O
λ2	O
σ	O
|wj+1	O
−	O
wj|	O
(	O
13.114	O
)	O
this	O
is	O
known	O
as	O
the	O
fused	B
lasso	I
penalty	O
.	O
in	O
the	O
context	O
of	O
functional	B
data	I
analysis	I
,	O
we	O
often	O
use	O
x	O
=	O
i	O
,	O
so	O
there	O
is	O
one	O
coefficient	O
for	O
each	O
location	O
in	O
the	O
signal	O
(	O
see	O
section	O
4.4.2.3	O
)	O
.	O
in	O
this	O
case	O
,	O
the	O
overall	O
objective	O
has	O
the	O
form	O
j	O
(	O
w	O
,	O
λ1	O
,	O
λ2	O
)	O
=	O
(	O
yi	O
−	O
wi	O
)	O
2	O
+	O
λ1	O
|wi|	O
+	O
λ2	O
|wi+1	O
−	O
wi|	O
(	O
13.115	O
)	O
n	O
(	O
cid:2	O
)	O
n−1	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
s∈v	O
i=1	O
i=1	O
i=1	O
this	O
is	O
a	O
sparse	B
version	O
of	O
equation	O
4.148.	O
it	O
is	O
possible	O
to	O
generalize	B
this	O
idea	O
beyond	O
chains	O
,	O
and	O
to	O
consider	O
other	O
graph	B
structures	O
,	O
using	O
a	O
penalty	O
of	O
the	O
form	O
j	O
(	O
w	O
,	O
λ1	O
,	O
λ2	O
)	O
=	O
(	O
ys	O
−	O
ws	O
)	O
2	O
+	O
λ1	O
|ws|	O
+	O
λ2	O
|ws	O
−	O
wt|	O
(	O
13.116	O
)	O
(	O
cid:2	O
)	O
s∈v	O
(	O
cid:2	O
)	O
(	O
s	O
,	O
t	O
)	O
∈e	O
this	O
is	O
called	O
graph-guided	B
fused	I
lasso	I
(	O
see	O
e.g.	O
,	O
(	O
chen	O
et	O
al	O
.	O
2010	O
)	O
)	O
.	O
the	O
graph	B
might	O
come	O
from	O
some	O
prior	O
knowledge	O
,	O
e.g.	O
,	O
from	O
a	O
database	O
of	O
known	O
biological	O
pathways	O
.	O
another	O
example	O
is	O
shown	O
in	O
figure	O
13.16	O
(	O
b-c	O
)	O
,	O
where	O
the	O
graph	B
structure	O
is	O
a	O
2d	O
lattice	B
.	O
13.5	O
.	O
(	O
cid:6	O
)	O
1	O
regularization	B
:	O
extensions	O
455	O
13.5.2.1	O
gsm	O
interpretation	O
of	O
fused	B
lasso	I
one	O
can	O
show	O
(	O
kyung	O
et	O
al	O
.	O
2010	O
)	O
that	O
the	O
fused	B
lasso	I
model	O
is	O
equivalent	O
to	O
the	O
following	O
hierarchical	O
model	O
w|σ2	O
,	O
τ	O
,	O
ω	O
∼	O
n	O
(	O
0	O
,	O
σ2σ	O
(	O
τ	O
,	O
ω	O
)	O
)	O
(	O
13.117	O
)	O
(	O
13.118	O
)	O
(	O
13.119	O
)	O
(	O
13.120	O
)	O
(	O
13.121	O
)	O
j	O
|γ1	O
∼	O
expon	O
(	O
τ	O
2	O
j|γ2	O
∼	O
expon	O
(	O
ω2	O
γ2	O
1	O
2	O
γ2	O
2	O
2	O
)	O
,	O
j	O
=	O
1	O
:	O
d	O
)	O
,	O
j	O
=	O
1	O
:	O
d	O
−	O
1	O
where	O
σ	O
=	O
ω−1	O
,	O
and	O
ω	O
is	O
a	O
tridiagonal	B
precision	O
matrix	O
with	O
main	O
diagonal	B
=	O
{	O
1	O
+	O
τ	O
2	O
j	O
off	O
diagonal	B
=	O
{	O
−	O
1	O
ω2	O
j	O
1	O
ω2	O
j−1	O
}	O
+	O
}	O
1	O
ω2	O
j	O
0	O
=	O
ω−2	O
where	O
we	O
have	O
deﬁned	O
ω−2	O
d	O
=	O
0.	O
this	O
is	O
very	O
similar	B
to	O
the	O
model	O
in	O
section	O
4.4.2.3	O
,	O
where	O
we	O
used	O
a	O
chain-structured	O
gaussian	O
markov	O
random	O
ﬁeld	O
as	O
the	O
prior	O
,	O
with	O
ﬁxed	O
vari-	O
ance	O
.	O
here	O
we	O
just	O
let	O
the	O
variance	B
be	O
random	O
.	O
in	O
the	O
case	O
of	O
graph-guided	O
lasso	O
,	O
the	O
structure	O
of	O
the	O
graph	B
is	O
reﬂected	O
in	O
the	O
zero	O
pattern	O
of	O
the	O
gaussian	O
precision	B
matrix	I
(	O
see	O
section	O
19.4.4	O
)	O
.	O
13.5.2.2	O
algorithms	O
for	O
fused	B
lasso	I
it	O
is	O
possible	O
to	O
generalize	B
the	O
em	O
algorithm	O
to	O
ﬁt	O
the	O
fused	B
lasso	I
model	O
,	O
by	O
exploiting	O
the	O
markov	O
structure	O
of	O
the	O
gaussian	O
prior	O
for	O
efficiency	O
.	O
direct	O
solvers	O
(	O
which	O
don	O
’	O
t	O
use	O
the	O
latent	O
variable	O
trick	O
)	O
can	O
also	O
be	O
derived	O
(	O
see	O
e.g.	O
,	O
(	O
hoeﬂing	O
2010	O
)	O
)	O
.	O
however	O
,	O
this	O
model	O
is	O
undeniably	O
more	O
expensive	O
to	O
ﬁt	O
than	O
the	O
other	O
variants	O
we	O
have	O
considered	O
.	O
13.5.3	O
elastic	B
net	I
(	O
ridge	O
and	O
lasso	B
combined	O
)	O
although	O
lasso	B
has	O
proved	O
to	O
be	O
effective	O
as	O
a	O
variable	O
selection	O
technique	O
,	O
problems	O
(	O
zou	O
and	O
hastie	O
2005	O
)	O
,	O
such	O
as	O
the	O
following	O
:	O
it	O
has	O
several	O
•	O
•	O
•	O
if	O
there	O
is	O
a	O
group	O
of	O
variables	O
that	O
are	O
highly	O
correlated	O
(	O
e.g.	O
,	O
genes	O
that	O
are	O
in	O
the	O
same	O
pathway	O
)	O
,	O
then	O
the	O
lasso	B
tends	O
to	O
select	O
only	O
one	O
of	O
them	O
,	O
chosen	O
rather	O
arbitrarily	O
.	O
(	O
this	O
is	O
evident	O
from	O
the	O
lars	O
algorithm	O
:	O
once	O
one	O
member	O
of	O
the	O
group	O
has	O
been	O
chosen	O
,	O
the	O
remaining	O
members	O
of	O
the	O
group	O
will	O
not	O
be	O
very	O
correlated	O
with	O
the	O
new	O
residual	B
and	O
hence	O
will	O
not	O
be	O
chosen	O
.	O
)	O
it	O
is	O
usually	O
better	O
to	O
select	O
all	O
the	O
relevant	O
variables	O
in	O
a	O
group	O
.	O
if	O
we	O
know	O
the	O
grouping	O
structure	O
,	O
we	O
can	O
use	O
group	B
lasso	I
,	O
but	O
often	O
we	O
don	O
’	O
t	O
know	O
the	O
grouping	O
structure	O
.	O
in	O
the	O
d	O
>	O
n	O
case	O
,	O
lasso	B
can	O
select	O
at	O
most	O
n	O
variables	O
before	O
it	O
saturates	O
.	O
if	O
n	O
>	O
d	O
,	O
but	O
the	O
variables	O
are	O
correlated	O
,	O
prediction	O
performance	O
of	O
ridge	O
is	O
better	O
than	O
that	O
of	O
lasso	B
.	O
it	O
has	O
been	O
empirically	O
observed	O
that	O
the	O
456	O
chapter	O
13.	O
sparse	B
linear	O
models	O
zou	O
and	O
hastie	O
(	O
zou	O
and	O
hastie	O
2005	O
)	O
proposed	O
an	O
approach	O
called	O
the	O
elastic	B
net	I
,	O
which	O
is	O
a	O
hybrid	O
between	O
lasso	B
and	O
ridge	B
regression	I
,	O
which	O
solves	O
all	O
of	O
these	O
problems	O
.	O
it	O
is	O
apparently	O
called	O
the	O
“	O
elastic	B
net	I
”	O
because	O
it	O
is	O
“	O
like	O
a	O
stretchable	O
ﬁshing	O
net	O
that	O
retains	O
’	O
all	O
the	O
big	O
ﬁsh	O
”	O
’	O
(	O
zou	O
and	O
hastie	O
2005	O
)	O
.	O
13.5.3.1	O
vanilla	O
version	O
the	O
vanilla	O
version	O
of	O
the	O
model	O
deﬁnes	O
the	O
following	O
objective	O
function	O
:	O
j	O
(	O
w	O
,	O
λ1	O
,	O
λ2	O
)	O
=	O
||y	O
−	O
xw||2	O
+	O
λ2||w||2	O
2	O
+	O
λ1||w||1	O
(	O
13.122	O
)	O
notice	O
that	O
this	O
penalty	O
function	O
is	O
strictly	B
convex	I
(	O
assuming	O
λ2	O
>	O
0	O
)	O
so	O
there	O
is	O
a	O
unique	O
global	B
minimum	I
,	O
even	O
if	O
x	O
is	O
not	O
full	O
rank	O
.	O
it	O
can	O
be	O
shown	O
(	O
zou	O
and	O
hastie	O
2005	O
)	O
that	O
any	O
strictly	B
convex	I
penalty	O
on	O
w	O
will	O
exhibit	O
a	O
grouping	B
effect	I
,	O
which	O
means	O
that	O
the	O
regression	B
coefficients	O
of	O
highly	O
correlated	O
variables	O
tend	O
to	O
be	O
equal	O
(	O
up	O
to	O
a	O
change	O
of	O
sign	O
if	O
they	O
are	O
negatively	O
correlated	O
)	O
.	O
for	O
example	O
,	O
if	O
two	O
features	B
are	O
equal	O
,	O
so	O
x	O
:	O
j	O
=	O
x	O
:	O
k	O
,	O
one	O
can	O
show	O
that	O
their	O
estimates	O
are	O
also	O
equal	O
,	O
ˆwj	O
=	O
ˆwk	O
.	O
by	O
contrast	O
,	O
with	O
lasso	B
,	O
we	O
may	O
have	O
that	O
ˆwj	O
=	O
0	O
and	O
ˆwk	O
(	O
cid:22	O
)	O
=	O
0	O
or	O
vice	O
versa	O
.	O
13.5.3.2	O
algorithms	O
for	O
vanilla	O
elastic	B
net	I
it	O
is	O
simple	O
to	O
show	O
(	O
exercise	O
13.5	O
)	O
that	O
the	O
elastic	B
net	I
problem	O
can	O
be	O
reduced	O
to	O
a	O
lasso	B
problem	O
on	O
modiﬁed	O
data	O
.	O
in	O
particular	O
,	O
deﬁne	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
˜x	O
=	O
c	O
x√	O
λ2id	O
,	O
˜y	O
=	O
y	O
0d×1	O
where	O
c	O
=	O
(	O
1	O
+	O
λ2	O
)	O
−	O
1	O
2	O
.	O
then	O
we	O
solve	O
˜w	O
=	O
arg	O
min	O
˜w	O
||˜y	O
−	O
˜x	O
˜w||2	O
+	O
cλ1||	O
˜w||1	O
and	O
set	O
w	O
=	O
c	O
˜w	O
.	O
(	O
13.123	O
)	O
(	O
13.124	O
)	O
we	O
can	O
use	O
lars	O
to	O
solve	O
this	O
subproblem	O
;	O
this	O
is	O
known	O
as	O
the	O
lars-en	O
algorithm	O
.	O
if	O
we	O
stop	O
the	O
algorithm	O
after	O
m	O
variables	O
have	O
been	O
included	O
,	O
the	O
cost	O
is	O
o	O
(	O
m3	O
+	O
dm2	O
)	O
.	O
note	O
that	O
we	O
can	O
use	O
m	O
=	O
d	O
if	O
we	O
wish	O
,	O
since	O
˜x	O
has	O
rank	O
d.	O
this	O
is	O
in	O
contrast	O
to	O
lasso	B
,	O
which	O
can	O
not	O
select	O
more	O
than	O
n	O
variables	O
(	O
before	O
jumping	O
to	O
the	O
ols	O
solution	O
)	O
if	O
n	O
<	O
d.	O
when	O
using	O
lars-en	O
(	O
or	O
other	O
(	O
cid:6	O
)	O
1	O
solvers	O
)	O
,	O
one	O
typically	O
uses	O
cross-validation	O
to	O
select	O
λ1	O
and	O
λ2	O
.	O
13.5.3.3	O
improved	O
version	O
unfortunately	O
it	O
turns	O
out	O
that	O
the	O
“	O
vanilla	O
”	O
elastic	B
net	I
does	O
not	O
produce	O
functions	O
that	O
predict	O
very	O
accurately	O
,	O
unless	O
it	O
is	O
very	O
close	O
to	O
either	O
pure	B
ridge	O
or	O
pure	B
lasso	O
.	O
intuitively	O
the	O
reason	O
is	O
that	O
it	O
performs	O
shrinkage	B
twice	O
:	O
once	O
due	O
to	O
the	O
(	O
cid:6	O
)	O
2	O
penalty	O
and	O
again	O
due	O
to	O
the	O
(	O
cid:6	O
)	O
1	O
penalty	O
.	O
the	O
solution	O
is	O
simple	O
:	O
undo	O
the	O
(	O
cid:6	O
)	O
2	O
shrinkage	B
by	O
scaling	O
up	O
the	O
estimates	O
from	O
the	O
vanilla	O
version	O
.	O
in	O
other	O
words	O
,	O
if	O
w∗	O
is	O
the	O
solution	O
of	O
equation	O
13.124	O
,	O
then	O
a	O
better	O
estimate	O
is	O
(	O
cid:17	O
)	O
ˆw	O
=	O
1	O
+	O
λ2	O
˜w	O
(	O
13.125	O
)	O
13.6.	O
non-convex	O
regularizers	O
we	O
will	O
call	O
this	O
a	O
corrected	O
estimate	O
.	O
one	O
can	O
show	O
that	O
the	O
corrected	O
estimates	O
are	O
given	O
by	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
xt	O
x	O
+	O
λ2i	O
1	O
+	O
λ2	O
w	O
−	O
2yt	O
xw	O
+	O
λ1||w||1	O
ˆw	O
=	O
arg	O
min	O
w	O
wt	O
now	O
xt	O
x	O
+	O
λ2i	O
1	O
+	O
λ2	O
=	O
(	O
1	O
−	O
ρ	O
)	O
ˆσ	O
+	O
ρi	O
457	O
(	O
13.126	O
)	O
(	O
13.127	O
)	O
where	O
ρ	O
=	O
λ2/	O
(	O
1	O
+	O
λ2	O
)	O
.	O
so	O
the	O
the	O
elastic	B
net	I
is	O
like	O
lasso	B
but	O
where	O
we	O
use	O
a	O
version	O
of	O
ˆσ	O
that	O
is	O
shrunk	O
towards	O
i	O
.	O
(	O
see	O
section	O
4.2.6	O
for	O
more	O
discussion	O
of	O
regularized	O
estimates	O
of	O
covariance	B
matrices	O
.	O
)	O
13.5.3.4	O
gsm	O
interpretation	O
of	O
elastic	B
net	I
the	O
implicit	O
prior	O
being	O
used	O
by	O
the	O
elastic	B
net	I
obviously	O
has	O
the	O
form	O
p	O
(	O
w|σ2	O
)	O
∝	O
exp	O
⎛	O
⎝−	O
γ1	O
σ	O
d	O
(	O
cid:2	O
)	O
j=1	O
|wj|	O
−	O
γ2	O
2σ2	O
⎞	O
⎠	O
w2	O
j	O
d	O
(	O
cid:2	O
)	O
j=1	O
(	O
13.128	O
)	O
which	O
is	O
just	O
a	O
product	O
of	O
gaussian	O
and	O
laplace	O
distributions	O
.	O
this	O
can	O
be	O
written	O
as	O
a	O
hierarchical	O
prior	O
as	O
follows	O
(	O
kyung	O
et	O
al	O
.	O
2010	O
;	O
chen	O
et	O
al	O
.	O
2011	O
)	O
:	O
wj|σ2	O
,	O
τ	O
2	O
j	O
∼	O
n	O
(	O
0	O
,	O
σ2	O
(	O
τ−2	O
j	O
+	O
γ2	O
)	O
−1	O
)	O
(	O
13.129	O
)	O
j	O
|γ1	O
∼	O
expon	O
(	O
τ	O
2	O
γ2	O
1	O
2	O
)	O
(	O
13.130	O
)	O
clearly	O
if	O
γ2	O
=	O
0	O
,	O
this	O
reduces	O
to	O
the	O
regular	B
lasso	O
.	O
it	O
is	O
possible	O
to	O
perform	O
map	O
estimation	O
in	O
this	O
model	O
using	O
em	O
,	O
or	O
bayesian	O
inference	B
using	O
mcmc	O
(	O
kyung	O
et	O
al	O
.	O
2010	O
)	O
or	O
variational	O
bayes	O
(	O
chen	O
et	O
al	O
.	O
2011	O
)	O
.	O
13.6	O
non-convex	O
regularizers	O
although	O
the	O
laplace	O
prior	O
results	O
in	O
a	O
convex	B
optimization	O
problem	O
,	O
from	O
a	O
statistical	O
point	O
of	O
view	O
this	O
prior	O
is	O
not	O
ideal	O
.	O
there	O
are	O
two	O
main	O
problems	O
with	O
it	O
.	O
first	O
,	O
it	O
does	O
not	O
put	O
enough	O
probability	O
mass	O
near	O
0	O
,	O
so	O
it	O
does	O
not	O
sufficiently	O
suppress	O
noise	O
.	O
second	O
,	O
it	O
does	O
not	O
put	O
enough	O
probability	O
mass	O
on	O
large	O
values	O
,	O
so	O
it	O
causes	O
shrinkage	B
of	O
relevant	O
coefficients	O
,	O
corresponding	O
to	O
“	O
signal	O
”	O
.	O
(	O
this	O
can	O
be	O
seen	O
in	O
figure	O
13.5	O
(	O
a	O
)	O
:	O
we	O
see	O
that	O
(	O
cid:6	O
)	O
1	O
estimates	O
of	O
large	O
coefficients	O
are	O
signiﬁcantly	O
smaller	O
than	O
their	O
ml	O
estimates	O
,	O
a	O
phenomenon	O
known	O
as	O
bias	B
.	O
)	O
both	O
problems	O
can	O
be	O
solved	O
by	O
going	O
to	O
more	O
ﬂexible	O
kinds	O
of	O
priors	O
which	O
have	O
a	O
larger	O
spike	O
at	O
0	O
and	O
heavier	O
tails	O
.	O
even	O
though	O
we	O
can	O
not	O
ﬁnd	O
the	O
global	O
optimum	O
anymore	O
,	O
these	O
non-convex	O
methods	O
often	O
outperform	O
(	O
cid:6	O
)	O
1	O
regularization	B
,	O
both	O
in	O
terms	O
of	O
predictive	B
accuracy	O
and	O
in	O
detecting	O
relevant	O
variables	O
(	O
fan	O
and	O
li	O
2001	O
;	O
schniter	O
et	O
al	O
.	O
2008	O
)	O
.	O
we	O
give	O
some	O
examples	O
below	O
.	O
458	O
chapter	O
13.	O
sparse	B
linear	O
models	O
13.6.1	O
bridge	B
regression	I
a	O
natural	O
generalization	O
of	O
(	O
cid:6	O
)	O
1	O
regularization	B
,	O
known	O
as	O
bridge	B
regression	I
(	O
frank	O
and	O
friedman	O
1993	O
)	O
,	O
has	O
the	O
form	O
ˆw	O
=	O
nll	O
(	O
w	O
)	O
+λ	O
|wj|b	O
(	O
13.131	O
)	O
(	O
cid:2	O
)	O
j	O
for	O
b	O
≥	O
0.	O
this	O
corresponds	O
to	O
map	O
estimation	O
using	O
a	O
exponential	B
power	I
distribution	I
given	O
by	O
exppower	O
(	O
w|μ	O
,	O
a	O
,	O
b	O
)	O
(	O
cid:2	O
)	O
b	O
2aγ	O
(	O
1	O
+	O
1/b	O
)	O
exp	O
(	O
13.132	O
)	O
(	O
cid:13	O
)	O
b	O
(	O
cid:11	O
)	O
−|x	O
−	O
μ|	O
√	O
a	O
if	O
b	O
=	O
2	O
,	O
we	O
get	O
the	O
gaussian	O
distribution	O
(	O
with	O
a	O
=	O
σ	O
2	O
)	O
,	O
corresonding	O
to	O
ridge	B
regression	I
;	O
if	O
we	O
set	O
b	O
=	O
1	O
,	O
we	O
get	O
the	O
laplace	O
distribution	O
,	O
corresponding	O
to	O
lasso	B
;	O
if	O
we	O
set	O
b	O
=	O
0	O
,	O
we	O
get	O
(	O
cid:6	O
)	O
0	O
regression	B
,	O
which	O
is	O
equivalent	O
to	O
best	O
subset	O
selection	O
.	O
unfortunately	O
,	O
the	O
objective	O
is	O
not	O
convex	O
for	O
b	O
<	O
1	O
,	O
and	O
is	O
not	O
sparsity	O
promoting	O
for	O
b	O
>	O
1	O
.	O
so	O
the	O
(	O
cid:6	O
)	O
1	O
norm	O
is	O
the	O
tightest	O
convex	B
approximation	O
to	O
the	O
(	O
cid:6	O
)	O
0	O
norm	O
.	O
the	O
effect	O
of	O
changing	O
b	O
is	O
illustrated	O
in	O
figure	O
13.17	O
,	O
where	O
we	O
plot	O
the	O
prior	O
for	O
b	O
=	O
2	O
,	O
b	O
=	O
1	O
and	O
b	O
=	O
0.4	O
;	O
we	O
assume	O
p	O
(	O
w	O
)	O
=p	O
(	O
w1	O
)	O
p	O
(	O
w2	O
)	O
.	O
we	O
also	O
plot	O
the	O
posterior	O
after	O
seeing	O
a	O
single	O
observation	O
,	O
(	O
x	O
,	O
y	O
)	O
,	O
which	O
imposes	O
a	O
single	O
linear	O
constraint	O
of	O
the	O
form	O
,	O
y	O
=	O
wt	O
x	O
,	O
with	O
a	O
certain	O
tolerance	O
controlled	O
by	O
the	O
observation	B
noise	O
(	O
compare	O
to	O
figure	O
7.11	O
)	O
.	O
we	O
see	O
see	O
that	O
the	O
mode	B
of	O
the	O
laplace	O
is	O
on	O
the	O
vertical	O
axis	O
,	O
corresponding	O
to	O
w1	O
=	O
0.	O
by	O
contrast	O
,	O
there	O
are	O
two	O
modes	O
when	O
using	O
b	O
=	O
0.4	O
,	O
corresponding	O
to	O
two	O
different	O
sparse	B
solutions	O
.	O
when	O
using	O
the	O
gaussian	O
,	O
the	O
map	O
estimate	O
is	O
not	O
sparse	O
(	O
the	O
mode	B
does	O
not	O
lie	O
on	O
either	O
of	O
the	O
coordinate	O
axes	O
)	O
.	O
13.6.2	O
hierarchical	B
adaptive	I
lasso	I
recall	O
that	O
one	O
of	O
the	O
principal	O
problems	O
with	O
lasso	B
is	O
that	O
it	O
results	O
in	O
biased	O
estimates	O
.	O
this	O
is	O
because	O
it	O
needs	O
to	O
use	O
a	O
large	O
value	O
of	O
λ	O
to	O
“	O
squash	O
”	O
the	O
irrelevant	O
parameters	O
,	O
but	O
this	O
then	O
over-penalizes	O
the	O
relevant	O
parameters	O
.	O
it	O
would	O
be	O
better	O
if	O
we	O
could	O
associate	O
a	O
different	O
penalty	O
parameter	B
with	O
each	O
parameter	B
.	O
of	O
course	O
,	O
it	O
is	O
completely	O
infeasible	O
to	O
tune	O
d	O
parameters	O
by	O
cross	B
validation	I
,	O
but	O
this	O
poses	O
no	O
problem	O
to	O
the	O
bayesian	O
:	O
we	O
simply	O
make	O
each	O
τ	O
2	O
j	O
have	O
its	O
own	O
private	O
tuning	O
parameter	B
,	O
γj	O
,	O
which	O
are	O
now	O
treated	O
as	O
random	O
variables	O
coming	O
from	O
the	O
conjugate	B
prior	I
γj	O
∼	O
ig	O
(	O
a	O
,	O
b	O
)	O
.	O
the	O
full	B
model	O
is	O
as	O
follows	O
:	O
γj	O
∼	O
ig	O
(	O
a	O
,	O
b	O
)	O
j	O
|γj	O
∼	O
ga	O
(	O
1	O
,	O
γ2	O
τ	O
2	O
j	O
/2	O
)	O
wj|τ	O
2	O
j	O
∼	O
n	O
(	O
0	O
,	O
τ	O
2	O
j	O
)	O
(	O
13.133	O
)	O
(	O
13.134	O
)	O
(	O
13.135	O
)	O
see	O
figure	O
13.18	O
(	O
a	O
)	O
.	O
this	O
has	O
been	O
called	O
the	O
hierarchical	B
adaptive	I
lasso	I
(	O
hal	O
)	O
(	O
lee	O
et	O
al	O
.	O
2010	O
)	O
(	O
see	O
also	O
(	O
lee	O
et	O
al	O
.	O
2011	O
;	O
cevher	O
2009	O
;	O
armagan	O
et	O
al	O
.	O
2011	O
)	O
)	O
.	O
we	O
can	O
integrate	B
out	I
τ	O
2	O
j	O
,	O
which	O
induces	O
a	O
lap	O
(	O
wj|0	O
,	O
1/γj	O
)	O
distribution	O
on	O
wj	O
as	O
before	O
.	O
the	O
result	O
is	O
that	O
p	O
(	O
wj	O
)	O
is	O
now	O
a	O
it	O
turns	O
out	O
that	O
we	O
can	O
ﬁt	O
this	O
model	O
(	O
i.e.	O
,	O
compute	O
a	O
local	O
scaled	O
mixture	O
of	O
laplacians	O
.	O
posterior	B
mode	I
)	O
using	O
em	O
,	O
as	O
we	O
explain	O
below	O
.	O
the	O
resulting	O
estimate	O
,	O
ˆwhal	O
,	O
often	O
works	O
13.6.	O
non-convex	O
regularizers	O
459	O
figure	O
13.17	O
top	O
:	O
plot	O
of	O
log	O
prior	O
for	O
three	O
different	O
distributions	O
with	O
unit	O
variance	O
:	O
gaussian	O
,	O
laplace	O
and	O
exponential	O
power	O
.	O
bottom	O
:	O
plot	O
of	O
log	O
posterior	O
after	O
observing	O
a	O
single	O
observation	O
,	O
corresponding	O
to	O
a	O
single	O
linear	O
constraint	O
.	O
the	O
precision	B
of	O
this	O
observation	B
is	O
shown	O
by	O
the	O
diagonal	B
lines	O
in	O
the	O
top	O
ﬁgure	O
.	O
in	O
the	O
case	O
of	O
the	O
laplace	O
prior	O
,	O
the	O
posterior	O
is	O
unimodal	O
and	O
asymmetric	O
(	O
skewed	O
)	O
.	O
in	O
the	O
case	O
of	O
the	O
exponential	O
prior	O
,	O
the	O
posterior	O
is	O
bimodal	O
.	O
based	O
on	O
figure	O
1	O
of	O
(	O
seeger	O
2008	O
)	O
.	O
figure	O
generated	O
by	O
sparsepostplot	O
,	O
written	O
by	O
florian	O
steinke	O
.	O
in	O
the	O
case	O
of	O
the	O
gaussian	O
prior	O
,	O
the	O
posterior	O
is	O
unimodal	O
and	O
symmetric	B
.	O
much	O
better	O
than	O
the	O
estimate	O
returned	O
by	O
lasso	B
,	O
ˆwl1	O
,	O
in	O
the	O
sense	O
that	O
it	O
is	O
more	O
likely	O
to	O
contain	O
zeros	O
in	O
the	O
right	O
places	O
(	O
model	B
selection	I
consistency	O
)	O
and	O
more	O
likely	O
to	O
result	O
in	O
good	O
predictions	O
(	O
prediction	O
consistency	O
)	O
(	O
lee	O
et	O
al	O
.	O
2010	O
)	O
.	O
we	O
give	O
an	O
explanation	O
for	O
this	O
behavior	O
in	O
section	O
13.6.2.2	O
.	O
13.6.2.1	O
em	O
for	O
hal	O
since	O
the	O
inverse	B
gamma	I
is	O
conjugate	O
to	O
the	O
laplace	O
,	O
we	O
ﬁnd	O
that	O
the	O
e	O
step	O
for	O
γj	O
is	O
given	O
by	O
(	O
13.136	O
)	O
(	O
13.137	O
)	O
(	O
13.138	O
)	O
p	O
(	O
γj|wj	O
)	O
=	O
ig	O
(	O
a	O
+	O
1	O
,	O
b	O
+	O
|wj|	O
)	O
the	O
e	O
step	O
for	O
σ2	O
is	O
the	O
same	O
as	O
for	O
vanilla	O
lasso	B
.	O
the	O
prior	O
for	O
w	O
has	O
the	O
following	O
form	O
:	O
p	O
(	O
w|γ	O
)	O
=	O
exp	O
(	O
−|wj|/γj	O
)	O
1	O
2γj	O
(	O
cid:27	O
)	O
j	O
hence	O
the	O
m	O
step	O
must	O
optimize	O
ˆw	O
(	O
t+1	O
)	O
=	O
argmax	O
w	O
log	O
n	O
(	O
y|xw	O
,	O
σ2	O
)	O
−	O
(	O
cid:2	O
)	O
j	O
|wj|e	O
[	O
1/γj	O
]	O
460	O
chapter	O
13.	O
sparse	B
linear	O
models	O
a	O
b	O
hal	O
a=1	O
,	O
b=0.01	O
a=1	O
,	O
b=0.10	O
a=1	O
,	O
b=1.00	O
γj	O
τ	O
2	O
j	O
wj	O
d	O
yi	O
xi	O
n	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−0.6	O
−0.8	O
−1	O
−1	O
σ2	O
aσ	O
bσ	O
−0.8	O
−0.6	O
−0.4	O
−0.2	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
13.18	O
(	O
a	O
)	O
dgm	O
for	O
hierarchical	B
adaptive	I
lasso	I
.	O
(	O
b	O
)	O
contours	O
of	O
hierarchical	O
adpative	O
laplace	O
.	O
based	O
on	O
figure	O
1	O
of	O
(	O
lee	O
et	O
al	O
.	O
2010	O
)	O
.	O
figure	O
generated	O
by	O
normalgammapenaltyplotdemo	O
.	O
the	O
expectation	O
is	O
given	O
by	O
e	O
[	O
1/γj	O
]	O
=	O
a	O
+	O
1	O
b	O
+	O
|w	O
(	O
t	O
)	O
j	O
|	O
(	O
cid:2	O
)	O
s	O
(	O
t	O
)	O
j	O
thus	O
the	O
m	O
step	O
becomes	O
a	O
weighted	O
lasso	O
problem	O
:	O
ˆw	O
(	O
t+1	O
)	O
=	O
argmin	O
w	O
||y	O
−	O
xw||2	O
2	O
+	O
j	O
|wj|	O
s	O
(	O
t	O
)	O
(	O
cid:2	O
)	O
j	O
(	O
13.139	O
)	O
(	O
13.140	O
)	O
j	O
this	O
is	O
easily	O
solved	O
using	O
standard	O
methods	O
(	O
e.g.	O
,	O
lars	O
)	O
.	O
note	O
that	O
if	O
the	O
coefficient	O
was	O
esti-	O
mated	O
to	O
be	O
large	O
in	O
the	O
previous	O
iteration	O
(	O
so	O
w	O
(	O
t	O
)	O
j	O
will	O
be	O
small	O
,	O
so	O
large	O
coefficients	O
are	O
not	O
penalized	O
heavily	O
.	O
conversely	O
,	O
small	O
coefficients	O
do	O
get	O
penalized	O
heavily	O
.	O
this	O
is	O
the	O
way	O
that	O
the	O
algorithm	O
adapts	O
the	O
penalization	O
strength	O
of	O
each	O
coefficient	O
.	O
the	O
result	O
is	O
an	O
estimate	O
that	O
is	O
often	O
much	O
sparser	O
than	O
returned	O
by	O
lasso	B
,	O
but	O
also	O
less	O
biased	O
.	O
is	O
large	O
)	O
,	O
then	O
the	O
scaling	O
factor	B
s	O
(	O
t	O
)	O
note	O
that	O
if	O
we	O
seta	O
=	O
b	O
=	O
0	O
,	O
and	O
we	O
only	O
perform	O
1	O
iteration	O
of	O
em	O
,	O
we	O
get	O
a	O
method	O
that	O
is	O
closely	O
related	O
to	O
the	O
adaptive	B
lasso	I
of	O
(	O
zou	O
2006	O
;	O
zou	O
and	O
li	O
2008	O
)	O
.	O
this	O
em	O
algorithm	O
is	O
also	O
closely	O
related	O
to	O
some	O
iteratively	O
reweighted	O
(	O
cid:6	O
)	O
1	O
methods	O
proposed	O
in	O
the	O
signal	B
processing	I
community	O
(	O
chartrand	O
and	O
yin	O
2008	O
;	O
candes	O
et	O
al	O
.	O
2008	O
)	O
.	O
13.6.2.2	O
understanding	O
the	O
behavior	O
of	O
hal	O
we	O
can	O
get	O
a	O
better	O
understanding	O
of	O
hal	O
by	O
integrating	O
out	O
γj	O
to	O
get	O
the	O
following	O
marginal	B
distribution	I
,	O
p	O
(	O
wj|a	O
,	O
b	O
)	O
=	O
a	O
2b	O
(	O
cid:9	O
)	O
−	O
(	O
a+1	O
)	O
(	O
cid:8	O
)	O
|wj|	O
b	O
+	O
1	O
(	O
13.141	O
)	O
461	O
13.6.	O
non-convex	O
regularizers	O
lasso	B
hal	O
10	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
p	O
a	O
m	O
w	O
10	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
p	O
a	O
m	O
w	O
−10	O
−10	O
−8	O
−6	O
−4	O
−2	O
0	O
wmle	O
(	O
a	O
)	O
2	O
4	O
6	O
8	O
10	O
−10	O
−10	O
−8	O
−6	O
−4	O
−2	O
b	O
=	O
0.010	O
,	O
a=1	O
b	O
=	O
0.100	O
,	O
a=1	O
b	O
=	O
1.000	O
,	O
a=1	O
2	O
4	O
6	O
8	O
10	O
0	O
wmle	O
(	O
b	O
)	O
figure	O
13.19	O
thresholding	O
behavior	O
of	O
(	O
b	O
)	O
hierarchical	O
adaptive	O
laplace	O
.	O
normalgammathresholdplotdemo	O
.	O
two	O
penalty	O
functions	O
(	O
negative	O
log	O
priors	O
)	O
.	O
based	O
on	O
figure	O
2	O
of	O
(	O
lee	O
et	O
al	O
.	O
2010	O
)	O
.	O
(	O
a	O
)	O
laplace	O
.	O
figure	O
generated	O
by	O
this	O
is	O
an	O
instance	O
of	O
the	O
generalized	B
t	I
distribution	I
(	O
mcdonald	O
and	O
newey	O
1988	O
)	O
(	O
in	O
(	O
cevher	O
2009	O
;	O
armagan	O
et	O
al	O
.	O
2011	O
)	O
,	O
this	O
is	O
called	O
the	O
double	O
pareto	O
distribution	O
)	O
deﬁned	O
as	O
gt	O
(	O
w|μ	O
,	O
a	O
,	O
c	O
,	O
q	O
)	O
(	O
cid:2	O
)	O
q	O
2ca1/qb	O
(	O
1/q	O
,	O
a	O
)	O
(	O
cid:8	O
)	O
|w	O
−	O
μ|q	O
acq	O
1	O
+	O
(	O
cid:9	O
)	O
−	O
(	O
a+1/q	O
)	O
(	O
13.142	O
)	O
√	O
where	O
c	O
is	O
the	O
scale	O
parameter	O
(	O
which	O
controls	O
the	O
degree	B
of	O
sparsity	B
)	O
,	O
and	O
a	O
is	O
related	O
to	O
the	O
degrees	B
of	I
freedom	I
.	O
when	O
q	O
=	O
2	O
and	O
c	O
=	O
2	O
we	O
recover	O
the	O
standard	O
t	O
distribution	O
;	O
when	O
a	O
→	O
∞	O
,	O
we	O
recover	O
the	O
exponential	B
power	I
distribution	I
;	O
and	O
when	O
q	O
=	O
1	O
and	O
a	O
=	O
∞	O
we	O
in	O
the	O
context	O
of	O
the	O
current	O
model	O
,	O
we	O
see	O
that	O
p	O
(	O
wj|a	O
,	O
b	O
)	O
=	O
get	O
the	O
laplace	O
distribution	O
.	O
gt	O
(	O
wj|0	O
,	O
a	O
,	O
b/a	O
,	O
1	O
)	O
.	O
the	O
resulting	O
penalty	O
term	O
has	O
the	O
form	O
πλ	O
(	O
wj	O
)	O
(	O
cid:2	O
)	O
−	O
log	O
p	O
(	O
wj	O
)	O
=	O
(	O
a	O
+	O
1	O
)	O
log	O
(	O
1	O
+	O
|wj|	O
b	O
)	O
+	O
const	O
(	O
13.143	O
)	O
where	O
λ	O
=	O
(	O
a	O
,	O
b	O
)	O
are	O
the	O
tuning	O
parameters	O
.	O
we	O
plot	O
this	O
penalty	O
in	O
2d	O
(	O
i.e.	O
,	O
we	O
plot	O
πλ	O
(	O
w1	O
)	O
+	O
πλ	O
(	O
w2	O
)	O
)	O
in	O
figure	O
13.18	O
(	O
b	O
)	O
for	O
various	O
values	O
of	O
b.	O
compared	O
to	O
the	O
diamond-shaped	O
laplace	O
penalty	O
,	O
shown	O
in	O
figure	O
13.3	O
(	O
a	O
)	O
,	O
we	O
see	O
that	O
the	O
hal	O
penalty	O
looks	O
more	O
like	O
a	O
“	O
star	O
ﬁsh	O
”	O
:	O
it	O
puts	O
much	O
more	O
density	O
along	O
the	O
“	O
spines	O
”	O
,	O
thus	O
enforcing	O
sparsity	B
more	O
aggressively	O
.	O
note	O
that	O
this	O
penalty	O
is	O
clearly	O
not	O
convex	O
.	O
we	O
can	O
gain	O
further	O
understanding	O
into	O
the	O
behavior	O
of	O
this	O
penalty	O
function	O
by	O
considering	O
applying	O
it	O
to	O
the	O
problem	O
of	O
linear	B
regression	I
with	O
an	O
orthogonal	O
design	O
matrix	O
.	O
in	O
this	O
case	O
,	O
462	O
p	O
(	O
τ	O
2	O
j	O
)	O
ga	O
(	O
1	O
,	O
γ2	O
2	O
)	O
ga	O
(	O
1	O
,	O
γ2	O
2	O
)	O
ga	O
(	O
1	O
,	O
γ2	O
2	O
)	O
ga	O
(	O
δ	O
,	O
γ2	O
2	O
)	O
j	O
|0	O
,	O
0	O
)	O
ga	O
(	O
τ	O
2	O
2	O
,	O
δγ2	O
ig	O
(	O
δ	O
2	O
)	O
c	O
+	O
(	O
0	O
,	O
γ	O
)	O
p	O
(	O
wj	O
)	O
lap	O
(	O
0	O
,	O
1/γ	O
)	O
p	O
(	O
γj	O
)	O
fixed	O
ig	O
(	O
a	O
,	O
b	O
)	O
gt	O
(	O
0	O
,	O
a	O
,	O
b/a	O
,	O
1	O
)	O
ga	O
(	O
a	O
,	O
b	O
)	O
fixed	O
-	O
fixed	O
c	O
+	O
(	O
0	O
,	O
b	O
)	O
neg	O
(	O
a	O
,	O
b	O
)	O
ng	O
(	O
δ	O
,	O
γ	O
)	O
nj	O
(	O
wj	O
)	O
t	O
(	O
0	O
,	O
δ	O
,	O
γ	O
)	O
horseshoe	O
(	O
b	O
)	O
chapter	O
13.	O
sparse	B
linear	O
models	O
ref	O
(	O
andrews	O
and	O
mallows	O
1974	O
;	O
west	O
1987	O
)	O
(	O
lee	O
et	O
al	O
.	O
2010	O
,	O
2011	O
;	O
cevher	O
2009	O
;	O
armagan	O
et	O
al	O
.	O
2011	O
)	O
(	O
griffin	O
and	O
brown	O
2007	O
,	O
2010	O
;	O
chen	O
et	O
al	O
.	O
2011	O
)	O
(	O
griffin	O
and	O
brown	O
2007	O
,	O
2010	O
)	O
(	O
figueiredo	O
2003	O
)	O
(	O
andrews	O
and	O
mallows	O
1974	O
;	O
west	O
1987	O
)	O
(	O
carvahlo	O
et	O
al	O
.	O
2010	O
)	O
table	O
13.2	O
some	O
scale	O
mixtures	O
of	O
gaussians	O
.	O
abbreviations	O
:	O
c	O
+	O
=	O
half-rectiﬁed	O
cauchy	O
;	O
ga	O
=	O
gamma	O
(	O
shape	O
and	O
rate	B
parameterization	O
)	O
;	O
gt	O
=	O
generalized	O
t	O
;	O
ig	O
=	O
inverse	B
gamma	I
;	O
neg	O
=	O
normal-exponential-	O
gamma	O
;	O
ng	O
=	O
normal-gamma	O
;	O
nj	O
=	O
normal-jeffreys	O
.	O
the	O
horseshoe	O
distribution	O
is	O
the	O
name	O
we	O
give	O
to	O
the	O
distribution	O
induced	O
on	O
wj	O
by	O
the	O
prior	O
described	O
in	O
(	O
carvahlo	O
et	O
al	O
.	O
2010	O
)	O
;	O
this	O
has	O
no	O
simple	O
analytic	O
form	O
.	O
the	O
deﬁnitions	O
of	O
the	O
neg	O
and	O
ng	O
densities	O
are	O
a	O
bit	O
complicated	O
,	O
but	O
can	O
be	O
found	O
in	O
the	O
references	O
.	O
the	O
other	O
distributions	O
are	O
deﬁned	O
in	O
the	O
text	O
.	O
one	O
can	O
show	O
that	O
the	O
objective	O
becomes	O
d	O
(	O
cid:2	O
)	O
πλ	O
(	O
|wj|	O
)	O
j	O
(	O
w	O
)	O
=	O
=	O
||y	O
−	O
xw||2	O
2	O
+	O
||y	O
−	O
ˆy||2	O
+	O
1	O
2	O
1	O
2	O
1	O
2	O
j=1	O
d	O
(	O
cid:2	O
)	O
j	O
−	O
wj	O
)	O
2	O
+	O
(	O
ˆwmle	O
j=1	O
d	O
(	O
cid:2	O
)	O
j=1	O
πλ	O
(	O
|wj|	O
)	O
(	O
13.144	O
)	O
(	O
13.145	O
)	O
where	O
ˆwmle	O
=	O
xt	O
y	O
is	O
the	O
mle	O
and	O
ˆy	O
=	O
x	O
ˆwmle	O
.	O
thus	O
we	O
can	O
compute	O
the	O
map	O
estimate	O
one	O
dimension	O
at	O
a	O
time	O
by	O
solving	O
the	O
following	O
1d	O
optimization	B
problem	O
:	O
ˆwj	O
=	O
argmin	O
wj	O
1	O
2	O
j	O
−	O
wj	O
)	O
2	O
+	O
πλ	O
(	O
wj	O
)	O
(	O
ˆwmle	O
(	O
13.146	O
)	O
in	O
figure	O
13.19	O
(	O
a	O
)	O
we	O
plot	O
the	O
lasso	B
estimate	O
,	O
ˆwl1	O
,	O
vs	O
the	O
ml	O
estimate	O
,	O
ˆwmle	O
.	O
we	O
see	O
that	O
the	O
(	O
cid:6	O
)	O
1	O
estimator	B
has	O
the	O
usual	O
soft-thresholding	O
behavior	O
seen	O
earlier	O
in	O
figure	O
13.5	O
(	O
a	O
)	O
.	O
however	O
,	O
this	O
behavior	O
is	O
undesirable	O
since	O
the	O
large	O
magnitude	O
coefficients	O
are	O
also	O
shrunk	O
towards	O
0	O
,	O
whereas	O
we	O
would	O
like	O
them	O
to	O
be	O
equal	O
to	O
their	O
unshrunken	O
ml	O
estimates	O
.	O
in	O
figure	O
13.19	O
(	O
b	O
)	O
we	O
plot	O
the	O
hal	O
estimate	O
,	O
ˆwhal	O
,	O
vs	O
the	O
ml	O
estimate	O
ˆwmle	O
.	O
we	O
see	O
that	O
this	O
approximates	O
the	O
more	O
desirable	O
hard	B
thresholding	I
behavior	O
seen	O
earlier	O
in	O
figure	O
13.5	O
(	O
b	O
)	O
much	O
more	O
closely	O
.	O
13.6.3	O
other	O
hierarchical	O
priors	O
many	O
other	O
hierarchical	O
sparsity-promoting	O
priors	O
have	O
been	O
proposed	O
;	O
see	O
table	O
13.2	O
for	O
a	O
brief	O
in	O
some	O
cases	O
,	O
we	O
can	O
analytically	O
derive	O
the	O
form	O
of	O
the	O
marginal	O
prior	O
for	O
wj	O
.	O
summary	O
.	O
generally	O
speaking	O
,	O
this	O
prior	O
is	O
not	O
concave	O
.	O
a	O
particularly	O
interesting	O
prior	O
is	O
the	O
improper	O
normal-jeffreys	O
prior	O
,	O
which	O
has	O
been	O
used	O
j	O
|0	O
,	O
0	O
)	O
∝	O
in	O
(	O
figueiredo	O
2003	O
)	O
.	O
this	O
puts	O
a	O
non-informative	B
jeffreys	O
prior	O
on	O
the	O
variance	B
,	O
ga	O
(	O
τ	O
2	O
13.7	O
13.7.	O
automatic	B
relevance	I
determination	I
(	O
ard	O
)	O
/sparse	O
bayesian	O
learning	B
(	O
sbl	O
)	O
463	O
j	O
;	O
the	O
resulting	O
marginal	O
has	O
the	O
form	O
p	O
(	O
wj	O
)	O
=nj	O
(	O
wj	O
)	O
∝	O
1/|wj|	O
.	O
this	O
gives	O
rise	O
to	O
a	O
1/τ	O
2	O
thresholding	O
rule	O
that	O
looks	O
very	O
similar	B
to	O
hal	O
in	O
figure	O
13.19	O
(	O
b	O
)	O
,	O
which	O
in	O
turn	O
is	O
very	O
similar	B
to	O
hard	B
thresholding	I
.	O
however	O
,	O
this	O
prior	O
has	O
no	O
free	O
parameters	O
,	O
which	O
is	O
both	O
a	O
good	O
thing	O
(	O
nothing	O
to	O
tune	O
)	O
and	O
a	O
bad	O
thing	O
(	O
no	O
ability	O
to	O
adapt	O
the	O
level	O
of	O
sparsity	B
)	O
.	O
automatic	B
relevance	I
determination	I
(	O
ard	O
)	O
/sparse	O
bayesian	O
learning	B
(	O
sbl	O
)	O
’	O
all	O
the	O
methods	O
we	O
have	O
considered	O
so	O
far	O
(	O
except	O
for	O
the	O
spike-and-slab	O
methods	O
in	O
sec-	O
tion	O
13.2.1	O
)	O
have	O
used	O
a	O
factorial	B
prior	I
of	O
the	O
form	O
p	O
(	O
w	O
)	O
=	O
j	O
p	O
(	O
wj	O
)	O
.	O
we	O
have	O
seen	O
how	O
these	O
priors	O
can	O
be	O
represented	O
in	O
terms	O
of	O
gaussian	O
scale	O
mixtures	O
of	O
the	O
form	O
wj	O
∼	O
n	O
(	O
0	O
,	O
τ	O
2	O
j	O
)	O
,	O
where	O
τ	O
2	O
j	O
has	O
one	O
of	O
the	O
priors	O
listed	O
in	O
table	O
13.2.	O
using	O
these	O
latent	B
variances	O
,	O
we	O
can	O
represent	O
the	O
j	O
→	O
wj	O
→	O
y	O
←	O
x.	O
we	O
can	O
then	O
use	O
em	O
to	O
perform	O
map	O
estimation	O
,	O
model	O
in	O
the	O
form	O
τ	O
2	O
j	O
|wj	O
)	O
,	O
and	O
in	O
the	O
m	O
step	O
we	O
estimate	O
w	O
from	O
y	O
,	O
x	O
and	O
τ	O
.	O
where	O
in	O
the	O
e	O
step	O
we	O
inferp	O
(	O
τ	O
2	O
this	O
m	O
step	O
either	O
involves	O
a	O
closed-form	O
weighted	O
(	O
cid:6	O
)	O
2	O
optimization	B
(	O
in	O
the	O
case	O
of	O
gaussian	O
scale	O
mixtures	O
)	O
,	O
or	O
a	O
weighted	O
(	O
cid:6	O
)	O
1	O
optimization	B
(	O
in	O
the	O
case	O
of	O
laplacian	O
scale	O
mixtures	O
)	O
.	O
we	O
also	O
discussed	O
how	O
to	O
perform	O
bayesian	O
inference	B
in	O
such	O
models	O
,	O
rather	O
than	O
just	O
computing	O
map	O
estimates	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
an	O
alternative	O
approach	O
based	O
on	O
type	O
ii	O
ml	O
estimation	O
(	O
empirical	O
bayes	O
)	O
,	O
whereby	O
we	O
integrate	B
out	I
w	O
and	O
maximize	O
the	O
marginal	B
likelihood	I
wrt	O
τ	O
.	O
this	O
eb	O
procedure	O
can	O
be	O
implemented	O
via	O
em	O
,	O
or	O
via	O
a	O
reweighted	O
(	O
cid:6	O
)	O
1	O
scheme	O
,	O
as	O
we	O
will	O
explain	O
below	O
.	O
having	O
estimated	O
the	O
variances	O
,	O
we	O
plug	O
them	O
in	O
to	O
compute	O
the	O
posterior	B
mean	I
of	O
the	O
weights	O
,	O
e	O
[	O
w|ˆτ	O
,	O
d	O
]	O
;	O
rather	O
surprisingly	O
(	O
in	O
view	O
of	O
the	O
gaussian	O
prior	O
)	O
,	O
the	O
result	O
is	O
an	O
(	O
approximately	O
)	O
sparse	B
estimate	O
,	O
for	O
reasons	O
we	O
explain	O
below	O
.	O
in	O
the	O
context	O
of	O
neural	B
networks	I
,	O
this	O
this	O
method	O
is	O
called	O
called	O
automatic	B
relevance	I
determination	I
or	O
ard	O
(	O
mackay	O
1995b	O
;	O
neal	O
1996	O
)	O
:	O
see	O
section	O
16.5.7.5.	O
in	O
the	O
context	O
of	O
the	O
linear	O
models	O
we	O
are	O
considering	O
in	O
this	O
chapter	O
,	O
this	O
method	O
is	O
called	O
sparse	B
bayesian	O
learning	B
or	O
sbl	O
(	O
tipping	O
2001	O
)	O
.	O
combining	O
ard/sbl	O
with	O
basis	B
function	I
expansion	I
in	O
a	O
linear	O
model	O
gives	O
rise	O
to	O
a	O
technique	O
called	O
the	O
relevance	B
vector	I
machine	I
(	O
rvm	O
)	O
,	O
which	O
we	O
will	O
discuss	O
in	O
section	O
14.3.2	O
.	O
13.7.1	O
ard	O
for	O
linear	B
regression	I
we	O
will	O
explain	O
the	O
procedure	O
in	O
the	O
context	O
of	O
linear	B
regression	I
;	O
ard	O
for	O
glms	O
requires	O
the	O
use	O
of	O
the	O
laplace	O
(	O
or	O
some	O
other	O
)	O
approximation	O
.	O
case	O
can	O
be	O
it	O
is	O
conventional	O
,	O
when	O
discussing	O
ard	O
/	O
sbl	O
,	O
to	O
denote	O
the	O
weight	O
precisions	O
by	O
αj	O
=	O
1/τ	O
2	O
j	O
,	O
and	O
the	O
measurement	O
precision	B
by	O
β	O
=	O
1/σ2	O
(	O
do	O
not	O
confuse	O
this	O
with	O
the	O
use	O
of	O
β	O
in	O
statistics	O
to	O
represent	O
the	O
regression	B
coefficients	O
!	O
)	O
.	O
in	O
particular	O
,	O
we	O
will	O
assume	O
the	O
following	O
model	O
:	O
p	O
(	O
y|x	O
,	O
w	O
,	O
β	O
)	O
=n	O
(	O
y|wt	O
x	O
,	O
1/β	O
)	O
p	O
(	O
w	O
)	O
=n	O
(	O
w|0	O
,	O
a−1	O
)	O
(	O
13.147	O
)	O
(	O
13.148	O
)	O
464	O
chapter	O
13.	O
sparse	B
linear	O
models	O
where	O
a	O
=	O
diag	O
(	O
α	O
)	O
.	O
the	O
marginal	B
likelihood	I
can	O
be	O
computed	O
analytically	O
as	O
follows	O
:	O
(	O
cid:28	O
)	O
p	O
(	O
y|x	O
,	O
α	O
,	O
β	O
)	O
=	O
n	O
(	O
y|xw	O
,	O
βin	O
)	O
n	O
(	O
w|0	O
,	O
a	O
)	O
dw	O
=	O
n	O
(	O
y|0	O
,	O
βin	O
+	O
xa−1xt	O
)	O
2	O
exp	O
(	O
−	O
1	O
=	O
(	O
2π	O
)	O
2	O
−n/2|cα|−	O
1	O
yt	O
c−1	O
α	O
y	O
)	O
where	O
cα	O
(	O
cid:2	O
)	O
β−1in	O
+	O
xa−1xt	O
(	O
13.149	O
)	O
(	O
13.150	O
)	O
(	O
13.151	O
)	O
(	O
13.152	O
)	O
compare	O
this	O
to	O
the	O
marginal	B
likelihood	I
in	O
equation	O
13.13	O
in	O
the	O
spike	B
and	I
slab	I
model	O
;	O
modulo	O
the	O
β	O
=	O
1/σ2	O
factor	B
missing	O
from	O
the	O
second	O
term	O
,	O
the	O
equations	O
are	O
the	O
same	O
,	O
except	O
we	O
have	O
replaced	O
the	O
binary	O
γj	O
∈	O
{	O
0	O
,	O
1	O
}	O
with	O
continuous	O
αj	O
∈	O
r	O
+	O
.	O
in	O
log	O
form	O
,	O
the	O
objective	O
becomes	O
log	O
p	O
(	O
y|x	O
,	O
α	O
,	O
β	O
)	O
=	O
log	O
|cα|	O
+	O
yt	O
c−1	O
α	O
y	O
(	O
cid:6	O
)	O
(	O
α	O
,	O
β	O
)	O
(	O
cid:2	O
)	O
−	O
1	O
(	O
13.153	O
)	O
2	O
to	O
regularize	O
the	O
problem	O
,	O
we	O
may	O
put	O
a	O
conjugate	B
prior	I
on	O
each	O
precision	B
,	O
αj	O
∼	O
ga	O
(	O
a	O
,	O
b	O
)	O
and	O
β	O
∼	O
ga	O
(	O
c	O
,	O
d	O
)	O
.	O
the	O
modiﬁed	O
objective	O
becomes	O
(	O
cid:6	O
)	O
(	O
α	O
,	O
β	O
)	O
(	O
cid:2	O
)	O
−	O
1	O
2	O
log	O
p	O
(	O
y|x	O
,	O
α	O
,	O
β	O
)	O
+	O
=	O
log	O
|cα|	O
+	O
yt	O
c−1	O
α	O
y	O
+	O
log	O
ga	O
(	O
αj|a	O
,	O
b	O
)	O
+	O
log	O
ga	O
(	O
β|c	O
,	O
d	O
)	O
(	O
a	O
log	O
αj	O
−	O
bαj	O
)	O
+c	O
log	O
β	O
−	O
dβ	O
(	O
13.154	O
)	O
(	O
13.155	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
j	O
j	O
this	O
is	O
useful	O
when	O
performing	O
bayesian	O
inference	B
for	O
α	O
and	O
β	O
(	O
bishop	O
and	O
tipping	O
2000	O
)	O
.	O
however	O
,	O
when	O
performing	O
(	O
type	O
ii	O
)	O
point	O
estimation	O
,	O
we	O
will	O
use	O
the	O
improper	B
prior	I
a	O
=	O
b	O
=	O
c	O
=	O
d	O
=	O
0	O
,	O
which	O
results	O
in	O
maximal	O
sparsity	O
.	O
below	O
we	O
describe	O
how	O
to	O
optimize	O
(	O
cid:6	O
)	O
(	O
α	O
,	O
β	O
)	O
wrt	O
the	O
precision	B
terms	O
α	O
and	O
β.7	O
this	O
is	O
a	O
proxy	O
for	O
ﬁnding	O
the	O
most	O
probable	O
model	O
setting	O
of	O
γ	O
in	O
the	O
spike	B
and	I
slab	I
model	O
,	O
which	O
in	O
turn	O
is	O
closely	O
related	O
to	O
(	O
cid:6	O
)	O
0	O
regularization	B
.	O
in	O
particular	O
,	O
it	O
can	O
be	O
shown	O
(	O
wipf	O
et	O
al	O
.	O
2010	O
)	O
that	O
the	O
objective	O
in	O
equation	O
13.153	O
has	O
many	O
fewer	O
local	O
optima	O
than	O
the	O
(	O
cid:6	O
)	O
0	O
objective	O
,	O
and	O
hence	O
is	O
much	O
easier	O
to	O
optimize	O
.	O
once	O
we	O
have	O
estimated	O
α	O
and	O
β	O
,	O
we	O
can	O
compute	O
the	O
posterior	O
over	O
the	O
parameters	O
using	O
p	O
(	O
w|d	O
,	O
ˆα	O
,	O
ˆβ	O
)	O
=	O
n	O
(	O
μ	O
,	O
σ	O
)	O
(	O
13.156	O
)	O
σ−1	O
=	O
ˆβxt	O
x	O
+	O
a	O
(	O
13.157	O
)	O
μ	O
=	O
ˆβσxt	O
y	O
(	O
13.158	O
)	O
the	O
fact	O
that	O
we	O
compute	O
a	O
posterior	O
over	O
w	O
,	O
while	O
simultaneously	O
encouraging	O
sparsity	B
,	O
is	O
why	O
the	O
method	O
is	O
called	O
“	O
sparse	B
bayesian	O
learning	B
”	O
.	O
nevertheless	O
,	O
since	O
there	O
are	O
many	O
ways	O
to	O
be	O
sparse	B
and	O
bayesian	O
,	O
we	O
will	O
use	O
the	O
“	O
ard	O
”	O
term	O
instead	O
,	O
even	O
in	O
the	O
linear	O
model	O
context	O
.	O
(	O
in	O
addition	O
,	O
sbl	O
is	O
only	O
“	O
being	O
bayesian	O
”	O
about	O
the	O
values	O
of	O
the	O
coefficients	O
,	O
rather	O
than	O
reﬂecting	O
uncertainty	O
about	O
the	O
set	O
of	O
relevant	O
variables	O
,	O
which	O
is	O
typically	O
of	O
more	O
interest	O
.	O
)	O
7.	O
an	O
alternative	O
approach	O
to	O
optimizing	O
β	O
is	O
to	O
put	O
a	O
gamma	O
prior	O
on	O
β	O
and	O
to	O
integrate	O
it	O
out	O
to	O
get	O
a	O
student	O
posterior	O
for	O
w	O
(	O
buntine	O
and	O
weigend	O
1991	O
)	O
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
this	O
results	O
in	O
a	O
less	O
accurate	O
estimate	O
for	O
α	O
(	O
mackay	O
1999	O
)	O
.	O
in	O
addition	O
,	O
working	O
with	O
gaussians	O
is	O
easier	O
than	O
working	O
with	O
the	O
student	O
distribution	O
,	O
and	O
the	O
gaussian	O
case	O
generalizes	O
more	O
easily	O
to	O
other	O
cases	O
such	O
as	O
logistic	B
regression	I
.	O
13.7.	O
automatic	B
relevance	I
determination	I
(	O
ard	O
)	O
/sparse	O
bayesian	O
learning	B
(	O
sbl	O
)	O
465	O
x	O
y	O
c	O
y	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
13.20	O
illustration	O
of	O
why	O
ard	O
results	O
in	O
sparsity	B
.	O
the	O
vector	O
of	O
inputs	O
x	O
does	O
not	O
point	O
towards	O
the	O
vector	O
of	O
outputs	O
y	O
,	O
so	O
the	O
feature	O
should	O
be	O
removed	O
.	O
(	O
a	O
)	O
for	O
ﬁnite	O
α	O
,	O
the	O
probability	O
density	O
is	O
spread	O
in	O
directions	O
away	O
from	O
y	O
.	O
(	O
b	O
)	O
when	O
α	O
=	O
∞	O
,	O
the	O
probability	O
density	O
at	O
y	O
is	O
maximized	O
.	O
based	O
on	O
figure	O
8	O
of	O
(	O
tipping	O
2001	O
)	O
.	O
13.7.2	O
whence	O
sparsity	B
?	O
j	O
if	O
ˆαj	O
≈	O
0	O
,	O
we	O
ﬁnd	O
ˆwj	O
≈	O
ˆwmle	O
,	O
since	O
the	O
gaussian	O
prior	O
shrinking	O
wj	O
towards	O
0	O
has	O
zero	O
precision	O
.	O
however	O
,	O
if	O
we	O
ﬁnd	O
that	O
ˆαj	O
≈	O
∞	O
,	O
then	O
the	O
prior	O
is	O
very	O
conﬁdent	O
that	O
wj	O
=	O
0	O
,	O
and	O
hence	O
that	O
feature	O
j	O
is	O
“	O
irrelevant	O
”	O
.	O
hence	O
the	O
posterior	B
mean	I
will	O
have	O
ˆwj	O
≈	O
0.	O
thus	O
irrelevant	O
features	B
automatically	O
have	O
their	O
weights	O
“	O
turned	O
off	O
”	O
or	O
“	O
pruned	O
out	O
”	O
.	O
we	O
now	O
give	O
an	O
intuitive	O
argument	O
,	O
based	O
on	O
(	O
tipping	O
2001	O
)	O
,	O
about	O
why	O
ml-ii	O
should	O
encour-	O
age	O
αj	O
→	O
∞	O
for	O
irrelevant	O
features	B
.	O
consider	O
a	O
1d	O
linear	B
regression	I
with	O
2	O
training	O
examples	O
,	O
so	O
x	O
=	O
x	O
=	O
(	O
x1	O
,	O
x2	O
)	O
,	O
and	O
y	O
=	O
(	O
y1	O
,	O
y2	O
)	O
.	O
we	O
can	O
plot	O
x	O
and	O
y	O
as	O
vectors	O
in	O
the	O
plane	O
,	O
as	O
shown	O
in	O
figure	O
13.20.	O
suppose	O
the	O
feature	O
is	O
irrelevant	O
for	O
predicting	O
the	O
response	O
,	O
so	O
x	O
points	O
in	O
a	O
nearly	O
orthogonal	O
direction	O
to	O
y.	O
let	O
us	O
see	O
what	O
happens	O
to	O
the	O
marginal	B
likelihood	I
as	O
we	O
change	O
α.	O
the	O
marginal	B
likelihood	I
is	O
given	O
by	O
p	O
(	O
y|x	O
,	O
α	O
,	O
β	O
)	O
=	O
n	O
(	O
y|0	O
,	O
c	O
)	O
,	O
where	O
c	O
=	O
1	O
β	O
i	O
+	O
1	O
α	O
xxt	O
(	O
13.159	O
)	O
if	O
α	O
is	O
ﬁnite	O
,	O
the	O
posterior	O
will	O
be	O
elongated	O
along	O
the	O
direction	O
of	O
x	O
,	O
as	O
in	O
figure	O
13.20	O
(	O
a	O
)	O
.	O
however	O
,	O
if	O
α	O
=	O
∞	O
,	O
we	O
ﬁnd	O
c	O
=	O
1	O
if	O
|c|	O
is	O
held	O
constant	O
,	O
the	O
latter	O
assigns	O
higher	O
probability	O
density	O
to	O
the	O
observed	O
response	O
vector	O
y	O
,	O
so	O
this	O
is	O
the	O
preferred	O
solution	O
.	O
in	O
other	O
words	O
,	O
the	O
marginal	B
likelihood	I
“	O
punishes	O
”	O
solutions	O
where	O
αj	O
is	O
small	O
but	O
x	O
:	O
,j	O
is	O
irrelevant	O
,	O
since	O
these	O
waste	O
probability	O
mass	O
.	O
it	O
is	O
more	O
parsimonious	O
(	O
from	O
the	O
point	O
of	O
view	O
of	O
bayesian	O
occam	O
’	O
s	O
razor	O
)	O
to	O
eliminate	O
redundant	O
dimensions	O
.	O
β	O
i	O
,	O
so	O
c	O
is	O
spherical	B
,	O
as	O
in	O
figure	O
13.20	O
(	O
b	O
)	O
.	O
13.7.3	O
connection	O
to	O
map	O
estimation	O
ard	O
seems	O
quite	O
different	O
from	O
the	O
map	O
estimation	O
methods	O
we	O
have	O
been	O
considering	O
earlier	O
in	O
this	O
chapter	O
.	O
in	O
particular	O
,	O
in	O
ard	O
,	O
we	O
are	O
not	O
integrating	O
out	O
α	O
and	O
optimizing	O
w	O
,	O
but	O
vice	O
466	O
chapter	O
13.	O
sparse	B
linear	O
models	O
versa	O
.	O
because	O
the	O
parameters	O
wj	O
become	O
correlated	O
in	O
the	O
posterior	O
(	O
due	O
to	O
explaining	B
away	I
)	O
,	O
when	O
we	O
estimate	O
αj	O
we	O
are	O
borrowing	O
information	B
from	O
all	O
the	O
features	B
,	O
not	O
just	O
feature	O
j.	O
consequently	O
,	O
the	O
effective	O
prior	O
p	O
(	O
w|	O
ˆα	O
)	O
is	O
non-factorial	B
,	O
and	O
furthermore	O
it	O
depends	O
on	O
the	O
data	O
d	O
(	O
and	O
σ2	O
)	O
.	O
however	O
,	O
in	O
(	O
wipf	O
and	O
nagarajan	O
2007	O
)	O
,	O
it	O
was	O
shown	O
that	O
ard	O
can	O
be	O
viewed	O
as	O
the	O
following	O
map	O
estimation	O
problem	O
:	O
β||y	O
−	O
xw||2	O
2	O
+	O
gard	O
(	O
w	O
)	O
(	O
13.160	O
)	O
αjw2	O
j	O
+	O
log	O
|cα|	O
(	O
13.161	O
)	O
(	O
cid:2	O
)	O
ˆward	O
=	O
arg	O
min	O
w	O
gard	O
(	O
w	O
)	O
(	O
cid:2	O
)	O
min	O
α≥0	O
j	O
the	O
proof	O
,	O
which	O
is	O
based	O
on	O
convex	B
analysis	O
,	O
is	O
a	O
little	O
complicated	O
and	O
hence	O
is	O
omitted	O
.	O
furthermore	O
,	O
(	O
wipf	O
and	O
nagarajan	O
2007	O
;	O
wipf	O
et	O
al	O
.	O
2010	O
)	O
prove	O
that	O
map	O
estimation	O
with	O
non-factorial	B
priors	O
is	O
strictly	O
better	O
than	O
map	O
estimation	O
with	O
any	O
possible	O
factorial	B
prior	I
in	O
the	O
following	O
sense	O
:	O
the	O
non-factorial	B
objective	O
always	O
has	O
fewer	O
local	O
minima	O
than	O
factorial	O
objectives	O
,	O
while	O
still	O
satisfying	O
the	O
property	O
that	O
the	O
global	O
optimum	O
of	O
the	O
non-factorial	B
objec-	O
tive	O
corresponds	O
to	O
the	O
global	O
optimum	O
of	O
the	O
(	O
cid:6	O
)	O
0	O
objective	O
—	O
a	O
property	O
that	O
(	O
cid:6	O
)	O
1	O
regularization	B
,	O
which	O
has	O
no	O
local	O
minima	O
,	O
does	O
not	O
enjoy	O
.	O
13.7.4	O
algorithms	O
for	O
ard	O
*	O
in	O
this	O
section	O
,	O
we	O
review	O
several	O
different	O
algorithms	O
for	O
implementing	O
ard	O
.	O
13.7.4.1	O
em	O
algorithm	O
the	O
easiest	O
way	O
to	O
implement	O
sbl/ard	O
is	O
to	O
use	O
em	O
.	O
the	O
expected	B
complete	I
data	I
log	I
likelihood	I
is	O
given	O
by	O
log	O
n	O
(	O
y|xw	O
,	O
σ2i	O
)	O
+	O
log	O
n	O
(	O
w|0	O
,	O
a−1	O
)	O
⎡	O
⎣n	O
log	O
β	O
−	O
β||y	O
−	O
xw||2	O
+	O
(	O
cid:2	O
)	O
e	O
log	O
αj	O
−	O
tr	O
(	O
awwt	O
)	O
(	O
13.162	O
)	O
⎤	O
⎦	O
+	O
const	O
(	O
13.163	O
)	O
(	O
cid:31	O
)	O
q	O
(	O
α	O
,	O
β	O
)	O
=e	O
=	O
=	O
1	O
2	O
1	O
2	O
(	O
cid:3	O
)	O
||y	O
−	O
xμ||2	O
+	O
tr	O
(	O
xt	O
xς	O
)	O
j	O
(	O
cid:4	O
)	O
n	O
log	O
β	O
−	O
β	O
(	O
cid:2	O
)	O
2	O
1	O
2	O
log	O
αj	O
−	O
1	O
2	O
j	O
+	O
tr	O
[	O
a	O
(	O
μμt	O
+	O
σ	O
)	O
]	O
+	O
const	O
(	O
13.164	O
)	O
where	O
μ	O
and	O
σ	O
are	O
computed	O
in	O
the	O
e	O
step	O
using	O
equation	O
13.158.	O
suppose	O
we	O
put	O
a	O
ga	O
(	O
a	O
,	O
b	O
)	O
prior	O
on	O
αj	O
and	O
a	O
ga	O
(	O
c	O
,	O
d	O
)	O
prior	O
on	O
β.	O
the	O
penalized	O
objective	O
becomes	O
q	O
(	O
cid:4	O
)	O
(	O
α	O
,	O
β	O
)	O
=	O
q	O
(	O
α	O
,	O
β	O
)	O
+	O
(	O
a	O
log	O
αj	O
−	O
bαj	O
)	O
+c	O
log	O
β	O
−	O
dβ	O
(	O
cid:2	O
)	O
j	O
setting	O
dq	O
(	O
cid:2	O
)	O
dαj	O
αj	O
=	O
e	O
=	O
0	O
we	O
get	O
the	O
following	O
m	O
step	O
:	O
(	O
cid:31	O
)	O
1	O
+	O
2a	O
w2	O
j	O
+	O
2b	O
=	O
1	O
+	O
2a	O
m2	O
j	O
+	O
σjj	O
+	O
2b	O
(	O
13.165	O
)	O
(	O
13.166	O
)	O
13.7.	O
automatic	B
relevance	I
determination	I
(	O
ard	O
)	O
/sparse	O
bayesian	O
learning	B
(	O
sbl	O
)	O
467	O
if	O
αj	O
=	O
α	O
,	O
and	O
a	O
=	O
b	O
=	O
0	O
,	O
the	O
update	O
becomes	O
α	O
=	O
d	O
e	O
[	O
wt	O
w	O
]	O
=	O
d	O
μt	O
μ	O
+	O
tr	O
(	O
σ	O
)	O
the	O
update	O
for	O
β	O
is	O
given	O
by	O
||y	O
−	O
xμ||2	O
+	O
β−1	O
(	O
cid:10	O
)	O
j	O
(	O
1	O
−	O
αjσjj	O
)	O
+	O
2d	O
β−1	O
new	O
=	O
n	O
+	O
2c	O
(	O
13.167	O
)	O
(	O
13.168	O
)	O
(	O
deriving	O
this	O
is	O
exercise	O
13.2	O
.	O
)	O
13.7.4.2	O
fixed-point	O
algorithm	O
a	O
faster	O
and	O
more	O
direct	O
approach	O
is	O
to	O
directly	O
optimize	O
the	O
objective	O
in	O
equation	O
13.155.	O
one	O
can	O
show	O
(	O
exercise	O
13.3	O
)	O
that	O
the	O
equations	O
d	O
(	O
cid:14	O
)	O
dβ	O
=	O
0	O
lead	O
to	O
the	O
following	O
ﬁxed	O
dαj	O
point	O
updates	O
:	O
=	O
0	O
and	O
d	O
(	O
cid:14	O
)	O
m2	O
αj	O
←	O
γj	O
+	O
2a	O
j	O
+	O
2b	O
n	O
−	O
(	O
cid:10	O
)	O
β−1	O
←	O
||y	O
−	O
xμ||2	O
+	O
2d	O
j	O
γj	O
+	O
2c	O
γj	O
(	O
cid:2	O
)	O
1	O
−	O
αjσjj	O
(	O
cid:10	O
)	O
the	O
quantity	O
γj	O
is	O
a	O
measure	O
of	O
how	O
well-determined	O
wj	O
is	O
by	O
the	O
data	O
(	O
mackay	O
1992	O
)	O
.	O
hence	O
j	O
γj	O
is	O
the	O
effective	O
degrees	O
of	O
freedom	O
of	O
the	O
model	O
.	O
see	O
section	O
7.5.3	O
for	O
further	O
γ	O
=	O
discussion	O
.	O
since	O
α	O
and	O
β	O
both	O
depend	O
on	O
μ	O
and	O
σ	O
(	O
which	O
can	O
be	O
computed	O
using	O
equation	O
13.158	O
or	O
the	O
laplace	O
approximation	O
)	O
,	O
we	O
need	O
to	O
re-estimate	O
these	O
equations	O
until	O
convergence	O
.	O
(	O
convergence	O
properties	O
of	O
this	O
algorithm	O
have	O
been	O
studied	O
in	O
(	O
wipf	O
and	O
nagarajan	O
2007	O
)	O
.	O
)	O
at	O
convergence	O
,	O
the	O
results	O
are	O
formally	O
identical	O
to	O
those	O
obtained	O
by	O
em	O
,	O
but	O
since	O
the	O
objective	O
is	O
non-convex	O
,	O
the	O
results	O
can	O
depend	O
on	O
the	O
initial	O
values	O
.	O
(	O
13.169	O
)	O
(	O
13.170	O
)	O
(	O
13.171	O
)	O
13.7.4.3	O
iteratively	O
reweighted	O
(	O
cid:9	O
)	O
1	O
algorithm	O
another	O
approach	O
to	O
solving	O
the	O
ard	O
problem	O
is	O
based	O
on	O
the	O
view	O
that	O
it	O
is	O
a	O
map	O
estimation	O
problem	O
.	O
although	O
the	O
log	O
prior	O
g	O
(	O
w	O
)	O
is	O
rather	O
complex	O
in	O
form	O
,	O
it	O
can	O
be	O
shown	O
to	O
be	O
a	O
non-decreasing	O
,	O
concave	B
function	O
of	O
|wj|	O
.	O
this	O
means	O
that	O
it	O
can	O
be	O
solved	O
by	O
an	O
iteratively	O
(	O
cid:2	O
)	O
reweighted	O
(	O
cid:6	O
)	O
1	O
problem	O
of	O
the	O
form	O
wt+1	O
=	O
arg	O
min	O
w	O
nll	O
(	O
w	O
)	O
+	O
j	O
|wj|	O
λ	O
(	O
t	O
)	O
j	O
(	O
13.172	O
)	O
in	O
(	O
wipf	O
and	O
nagarajan	O
2010	O
)	O
,	O
the	O
following	O
procedure	O
for	O
setting	O
the	O
penalty	O
terms	O
is	O
suggested	O
(	O
based	O
on	O
a	O
convex	B
bound	O
to	O
the	O
penalty	O
function	O
)	O
.	O
we	O
initialize	O
with	O
λ	O
(	O
0	O
)	O
j	O
=	O
1	O
,	O
and	O
then	O
at	O
468	O
chapter	O
13.	O
sparse	B
linear	O
models	O
iteration	O
t	O
+	O
1	O
,	O
compute	O
λ	O
(	O
t+1	O
)	O
j	O
by	O
iterating	O
the	O
following	O
equation	O
a	O
few	O
times:8	O
(	O
cid:6	O
)	O
−1	O
(	O
cid:30	O
)	O
1	O
2	O
(	O
cid:29	O
)	O
(	O
cid:5	O
)	O
λj	O
←	O
x	O
:	O
,j	O
σ2i	O
+	O
xdiag	O
(	O
1/λj	O
)	O
diag	O
(	O
|w	O
(	O
t+1	O
)	O
j	O
|	O
)	O
xt	O
)	O
−1x	O
:	O
,j	O
(	O
13.173	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
d	O
n	O
we	O
see	O
that	O
the	O
new	O
penalty	O
λj	O
depends	O
on	O
all	O
the	O
old	O
weights	O
.	O
this	O
is	O
quite	O
different	O
from	O
the	O
adaptive	B
lasso	I
method	O
of	O
section	O
13.6.2.	O
to	O
understand	O
this	O
difference	O
,	O
consider	O
the	O
noiseless	O
case	O
where	O
σ2	O
=	O
0	O
,	O
and	O
assume	O
d	O
(	O
cid:18	O
)	O
n	O
.	O
solutions	O
which	O
perfectly	O
reconstruct	O
the	O
data	O
,	O
xw	O
=	O
y	O
,	O
and	O
which	O
in	O
this	O
case	O
,	O
there	O
are	O
have	O
sparsity	B
||w||0	O
=	O
n	O
;	O
these	O
are	O
called	O
basic	O
feasible	O
solutions	O
or	O
bfs	O
.	O
what	O
we	O
want	O
are	O
solutions	O
that	O
satsify	O
xw	O
=	O
y	O
but	O
which	O
are	O
much	O
sparser	O
than	O
this	O
.	O
suppose	O
the	O
method	O
has	O
found	O
a	O
bfs	O
.	O
we	O
do	O
not	O
want	O
to	O
increase	O
the	O
penalty	O
on	O
a	O
weight	O
just	O
because	O
it	O
is	O
small	O
(	O
as	O
in	O
adaptive	B
lasso	I
)	O
,	O
since	O
that	O
will	O
just	O
reinforce	O
our	O
current	O
local	O
optimum	O
.	O
instead	O
,	O
we	O
want	O
to	O
increase	O
the	O
penalty	O
on	O
a	O
weight	O
if	O
it	O
is	O
small	O
and	O
if	O
we	O
have	O
||w	O
(	O
t+1	O
)	O
||	O
<	O
n	O
.	O
the	O
covariance	B
term	O
(	O
xdiag	O
(	O
1/λj	O
)	O
diag	O
(	O
|w	O
(	O
t+1	O
)	O
−1	O
has	O
this	O
effect	O
:	O
if	O
w	O
is	O
a	O
bfs	O
,	O
this	O
matrix	O
will	O
be	O
full	B
rank	O
,	O
so	O
the	O
penalty	O
will	O
not	O
increase	O
much	O
,	O
but	O
if	O
w	O
is	O
sparser	O
than	O
n	O
,	O
the	O
matrix	O
will	O
not	O
be	O
full	B
rank	O
,	O
so	O
the	O
penalties	O
associated	O
with	O
zero-valued	O
coefficients	O
will	O
increase	O
,	O
thus	O
reinforcing	O
this	O
solution	O
(	O
wipf	O
and	O
nagarajan	O
2010	O
)	O
.	O
|	O
)	O
)	O
j	O
13.7.5	O
ard	O
for	O
logistic	B
regression	I
now	O
consider	O
binary	O
logistic	O
regression	B
,	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
ber	O
(	O
y|sigm	O
(	O
wt	O
x	O
)	O
)	O
,	O
using	O
the	O
same	O
gaussian	O
prior	O
,	O
p	O
(	O
w	O
)	O
=n	O
(	O
w|0	O
,	O
a−1	O
)	O
.	O
we	O
can	O
no	O
longer	O
use	O
em	O
to	O
estimate	O
α	O
,	O
since	O
the	O
gaussian	O
prior	O
is	O
not	O
conjugate	O
to	O
the	O
logistic	B
likelihood	O
,	O
so	O
the	O
e	O
step	O
can	O
not	O
be	O
done	O
exactly	O
.	O
one	O
approach	O
is	O
to	O
use	O
a	O
variational	O
approximation	O
to	O
the	O
e	O
step	O
,	O
as	O
discussed	O
in	O
section	O
21.8.1.1.	O
a	O
simpler	O
approach	O
is	O
to	O
use	O
a	O
laplace	O
approximation	O
(	O
see	O
section	O
8.4.1	O
)	O
in	O
the	O
e	O
step	O
.	O
we	O
can	O
then	O
use	O
this	O
approximation	O
inside	O
the	O
same	O
em	O
procedure	O
as	O
before	O
,	O
except	O
we	O
no	O
longer	O
need	O
to	O
update	O
β.	O
note	O
,	O
however	O
,	O
that	O
this	O
is	O
not	O
guaranteed	O
to	O
converge	B
.	O
an	O
alternative	O
is	O
to	O
use	O
the	O
techniques	O
from	O
section	O
13.7.4.3.	O
in	O
this	O
case	O
,	O
we	O
can	O
use	O
exact	O
methods	O
to	O
compute	O
the	O
inner	O
weighted	O
(	O
cid:6	O
)	O
1	O
regularized	O
logistic	O
regression	B
problem	O
,	O
and	O
no	O
approximations	O
are	O
required	O
.	O
13.8	O
sparse	B
coding	I
*	O
so	O
far	O
,	O
we	O
have	O
been	O
concentrating	O
on	O
sparse	B
priors	O
for	O
supervised	B
learning	I
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
how	O
to	O
use	O
them	O
for	O
unsupervised	B
learning	I
.	O
in	O
section	O
12.6	O
,	O
we	O
discussed	O
ica	O
,	O
which	O
is	O
like	O
pca	O
except	O
it	O
uses	O
a	O
non-gaussian	O
prior	O
for	O
the	O
latent	B
factors	I
zi	O
.	O
if	O
we	O
make	O
the	O
non-gaussian	O
prior	O
be	O
sparsity	B
promoting	O
,	O
such	O
as	O
a	O
laplace	O
distribution	O
,	O
we	O
will	O
be	O
approximating	O
each	O
observed	O
vector	O
xi	O
as	O
a	O
sparse	B
combination	O
of	O
basis	O
vectors	O
(	O
columns	O
of	O
w	O
)	O
;	O
note	O
that	O
the	O
sparsity	B
pattern	O
(	O
controlled	O
by	O
zi	O
)	O
changes	O
from	O
data	O
case	O
to	O
data	O
case	O
.	O
if	O
we	O
relax	O
the	O
constraint	O
that	O
w	O
is	O
orthogonal	O
,	O
we	O
get	O
a	O
method	O
called	O
8.	O
the	O
algorithm	O
in	O
(	O
wipf	O
and	O
nagarajan	O
2007	O
)	O
is	O
equivalent	O
to	O
a	O
single	O
iteration	O
of	O
equation	O
13.173.	O
however	O
,	O
since	O
the	O
equation	O
is	O
cheap	O
to	O
compute	O
(	O
only	O
o	O
(	O
n	O
d||w	O
(	O
t+1	O
)	O
||0	O
)	O
time	O
)	O
,	O
it	O
is	O
worth	O
iterating	O
a	O
few	O
times	O
before	O
solving	O
the	O
more	O
expensive	O
(	O
cid:7	O
)	O
1	O
problem	O
.	O
13.8.	O
sparse	B
coding	I
*	O
469	O
method	O
pca	O
fa	O
ica	O
sparse	B
coding	I
sparse	O
pca	O
sparse	B
mf	O
p	O
(	O
zi	O
)	O
gauss	O
gauss	O
non-gauss	O
laplace	O
gauss	O
laplace	O
p	O
(	O
w	O
)	O
w	O
orthogonal	O
-	O
-	O
-	O
-	O
laplace	O
maybe	O
laplace	O
yes	O
no	O
yes	O
no	O
no	O
table	O
13.3	O
summary	O
of	O
various	O
latent	B
factor	O
models	O
.	O
a	O
dash	O
“	O
-	O
”	O
in	O
the	O
p	O
(	O
w	O
)	O
column	O
means	O
we	O
are	O
performing	O
ml	O
parameter	B
estimation	O
rather	O
than	O
map	O
parameter	B
estimation	O
.	O
summary	O
of	O
abbreviations	O
:	O
pca	O
=	O
principal	B
components	I
analysis	I
;	O
fa	O
=	O
factor	B
analysis	I
;	O
ica	O
=	O
independent	O
components	O
analysis	O
;	O
mf	O
=	O
matrix	B
factorization	I
.	O
sparse	B
coding	I
.	O
in	O
this	O
context	O
,	O
we	O
call	O
the	O
factor	B
loading	I
matrix	I
w	O
a	O
dictionary	B
;	O
each	O
column	O
is	O
referred	O
to	O
as	O
an	O
atom.9	O
in	O
view	O
of	O
the	O
sparse	B
representation	I
,	O
it	O
is	O
common	O
for	O
l	O
>	O
d	O
,	O
in	O
which	O
case	O
we	O
call	O
the	O
representation	O
overcomplete	B
.	O
in	O
sparse	B
coding	I
,	O
the	O
dictionary	B
can	O
be	O
ﬁxed	O
or	O
learned	O
.	O
if	O
it	O
is	O
ﬁxed	O
,	O
it	O
is	O
common	O
to	O
use	O
a	O
wavelet	B
or	O
dct	O
basis	O
,	O
since	O
many	O
natural	O
signals	O
can	O
be	O
well	O
approximated	O
by	O
a	O
small	O
number	O
of	O
such	O
basis	B
functions	I
.	O
however	O
,	O
it	O
is	O
also	O
possible	O
to	O
learn	O
the	O
dictionary	B
,	O
by	O
maximizing	O
the	O
likelihood	B
log	O
p	O
(	O
d|w	O
)	O
=	O
n	O
(	O
xi|wzi	O
,	O
σ2i	O
)	O
p	O
(	O
zi	O
)	O
dzi	O
(	O
13.174	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:28	O
)	O
log	O
i=1	O
zi	O
we	O
discuss	O
ways	O
to	O
optimize	O
this	O
below	O
,	O
and	O
then	O
we	O
present	O
several	O
interesting	O
applications	O
.	O
do	O
not	O
confuse	O
sparse	B
coding	I
with	O
sparse	B
pca	O
(	O
see	O
e.g.	O
,	O
(	O
witten	O
et	O
al	O
.	O
2009	O
;	O
journee	O
et	O
al	O
.	O
this	O
puts	O
a	O
sparsity	B
promoting	O
prior	O
on	O
the	O
regression	B
weights	O
w	O
,	O
whereas	O
in	O
sparse	B
2010	O
)	O
)	O
:	O
coding	O
,	O
we	O
put	O
a	O
sparsity	B
promoting	O
prior	O
on	O
the	O
latent	B
factors	I
zi	O
.	O
of	O
course	O
,	O
the	O
two	O
techniques	O
can	O
be	O
combined	O
;	O
we	O
call	O
the	O
result	O
sparse	B
matrix	I
factorization	I
,	O
although	O
this	O
term	O
is	O
non-	O
standard	O
.	O
see	O
table	O
13.3	O
for	O
a	O
summary	O
of	O
our	O
terminology	O
.	O
13.8.1	O
learning	B
a	O
sparse	B
coding	I
dictionary	O
since	O
equation	O
13.174	O
is	O
a	O
hard	O
objective	O
to	O
maximize	O
,	O
approximation	O
:	O
it	O
is	O
common	O
to	O
make	O
the	O
following	O
log	O
p	O
(	O
d|w	O
)	O
≈	O
n	O
(	O
cid:2	O
)	O
(	O
cid:31	O
)	O
max	O
zi	O
log	O
n	O
(	O
xi|wzi	O
,	O
σ2i	O
)	O
+	O
log	O
p	O
(	O
zi	O
)	O
if	O
p	O
(	O
zi	O
)	O
is	O
laplace	O
,	O
we	O
can	O
rewrite	O
the	O
nll	O
as	O
nll	O
(	O
w	O
,	O
z	O
)	O
=	O
||xi	O
−	O
wzi||2	O
2	O
+	O
λ||zi||1	O
1	O
2	O
i=1	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
13.175	O
)	O
(	O
13.176	O
)	O
9.	O
it	O
is	O
common	O
to	O
denote	O
the	O
dictionary	B
by	O
d	O
,	O
and	O
to	O
denote	O
the	O
latent	B
factors	I
by	O
αi	O
.	O
however	O
,	O
we	O
will	O
stick	O
with	O
the	O
w	O
and	O
zi	O
notation	O
.	O
470	O
chapter	O
13.	O
sparse	B
linear	O
models	O
to	O
prevent	O
w	O
from	O
becoming	O
arbitrarily	O
large	O
,	O
it	O
is	O
common	O
to	O
constrain	O
the	O
(	O
cid:6	O
)	O
2	O
norm	O
of	O
its	O
columns	O
to	O
be	O
less	O
than	O
or	O
equal	O
to	O
1.	O
let	O
us	O
denote	O
this	O
constraint	O
set	O
by	O
c	O
=	O
{	O
w	O
∈	O
r	O
d×l	O
s.t	O
.	O
wt	O
j	O
wj	O
≤	O
1	O
}	O
(	O
13.177	O
)	O
then	O
we	O
want	O
to	O
solve	O
minw∈c	O
,	O
z∈rn×l	O
nll	O
(	O
w	O
,	O
z	O
)	O
.	O
for	O
a	O
ﬁxed	O
zi	O
,	O
the	O
optimization	B
over	O
w	O
is	O
a	O
simple	O
least	O
squares	O
problem	O
.	O
and	O
for	O
a	O
ﬁxed	O
dictionary	O
w	O
,	O
the	O
optimization	B
problem	O
over	O
z	O
is	O
identical	O
to	O
the	O
lasso	B
problem	O
,	O
for	O
which	O
many	O
fast	O
algorithms	O
exist	O
.	O
this	O
suggests	O
an	O
obvious	O
iterative	O
optimization	O
scheme	O
,	O
in	O
which	O
we	O
alternate	O
between	O
optimizing	O
w	O
and	O
z	O
.	O
(	O
mumford	O
1994	O
)	O
called	O
this	O
kind	O
of	O
approach	O
an	O
analysis-synthesis	B
loop	O
,	O
where	O
estimating	O
the	O
basis	O
w	O
is	O
the	O
analysis	O
phase	O
,	O
and	O
estimating	O
the	O
coefficients	O
z	O
is	O
the	O
synthesis	O
phase	O
.	O
in	O
cases	O
where	O
this	O
is	O
too	O
slow	O
,	O
more	O
sophisticated	O
algorithms	O
can	O
be	O
used	O
,	O
see	O
e.g.	O
,	O
(	O
mairal	O
et	O
al	O
.	O
2010	O
)	O
.	O
a	O
variety	O
of	O
other	O
models	O
result	O
in	O
an	O
optimization	B
problem	O
that	O
is	O
similar	B
to	O
equation	O
13.176.	O
for	O
example	O
,	O
non-negative	B
matrix	I
factorization	I
or	O
nmf	O
(	O
paatero	O
and	O
tapper	O
1994	O
;	O
lee	O
and	O
seung	O
2001	O
)	O
requires	O
solving	O
an	O
objective	O
of	O
the	O
form	O
||xi	O
−	O
wzi||2	O
2	O
s.t	O
.	O
w	O
≥	O
0	O
,	O
zi	O
≥	O
0	O
(	O
13.178	O
)	O
min	O
w∈c	O
,	O
z∈rl×n	O
1	O
2	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
note	O
that	O
this	O
has	O
no	O
hyper-parameters	O
to	O
tune	O
.	O
)	O
the	O
intuition	O
behind	O
this	O
constraint	O
is	O
that	O
the	O
learned	O
dictionary	B
may	O
be	O
more	O
interpretable	O
if	O
it	O
is	O
a	O
positive	O
sum	O
of	O
positive	O
“	O
parts	O
”	O
,	O
rather	O
than	O
a	O
sparse	B
sum	O
of	O
atoms	O
that	O
may	O
be	O
positive	O
or	O
negative	O
.	O
of	O
course	O
,	O
we	O
can	O
combine	O
nmf	O
with	O
a	O
sparsity	B
promoting	O
prior	O
on	O
the	O
latent	B
factors	I
.	O
this	O
is	O
called	O
non-negative	B
sparse	I
coding	I
(	O
hoyer	O
2004	O
)	O
.	O
alternatively	O
,	O
we	O
can	O
drop	O
the	O
positivity	O
constraint	O
,	O
but	O
impose	O
a	O
sparsity	B
constraint	O
on	O
both	O
the	O
factors	B
zi	O
and	O
the	O
dictionary	B
w.	O
we	O
call	O
this	O
sparse	B
matrix	I
factorization	I
.	O
to	O
ensure	O
strict	B
convexity	O
,	O
we	O
can	O
use	O
an	O
elastic	B
net	I
type	O
penalty	O
on	O
the	O
weights	O
(	O
mairal	O
et	O
al	O
.	O
2010	O
)	O
resulting	O
in	O
||xi	O
−	O
wzi||2	O
2	O
+	O
λ||zi||1	O
s.t	O
.	O
||wj||2	O
2	O
+	O
γ||wj||1	O
≤	O
1	O
(	O
13.179	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
min	O
w	O
,	O
z	O
1	O
2	O
there	O
are	O
several	O
related	O
objectives	O
one	O
can	O
write	O
down	O
.	O
for	O
example	O
,	O
we	O
can	O
replace	O
the	O
lasso	B
nll	O
with	O
group	B
lasso	I
or	O
fused	B
lasso	I
(	O
witten	O
et	O
al	O
.	O
2009	O
)	O
.	O
we	O
can	O
also	O
use	O
other	O
sparsity-promoting	O
priors	O
besides	O
the	O
laplace	O
.	O
for	O
example	O
,	O
(	O
zhou	O
et	O
al	O
.	O
2009	O
)	O
propose	B
a	O
model	O
in	O
which	O
the	O
latent	B
factors	I
zi	O
are	O
made	O
sparse	B
using	O
the	O
binary	B
mask	I
model	O
of	O
section	O
13.2.2.	O
each	O
bit	O
of	O
the	O
mask	O
can	O
be	O
generated	O
from	O
a	O
bernoulli	O
distribution	O
with	O
parameter	B
π	O
,	O
which	O
can	O
be	O
drawn	O
from	O
a	O
beta	B
distribution	I
.	O
alternatively	O
,	O
we	O
can	O
use	O
a	O
non-parametric	B
prior	I
,	O
such	O
as	O
the	O
beta	B
process	I
.	O
this	O
allows	O
the	O
model	O
to	O
use	O
dictionaries	O
of	O
unbounded	O
size	O
,	O
rather	O
than	O
having	O
to	O
specify	O
l	O
in	O
advance	O
.	O
one	O
can	O
perform	O
bayesian	O
inference	B
in	O
this	O
model	O
using	O
e.g.	O
,	O
gibbs	O
sampling	O
or	O
variational	O
bayes	O
.	O
one	O
ﬁnds	O
that	O
the	O
effective	O
size	O
of	O
the	O
dictionary	B
goes	O
down	O
as	O
the	O
noise	O
level	O
goes	O
up	O
,	O
due	O
to	O
the	O
bayesian	O
occam	O
’	O
s	O
razor	O
.	O
this	O
can	O
prevent	O
overﬁtting	B
.	O
see	O
(	O
zhou	O
et	O
al	O
.	O
2009	O
)	O
for	O
details	O
.	O
13.8.2	O
results	O
of	O
dictionary	B
learning	O
from	O
image	O
patches	O
one	O
reason	O
that	O
sparse	B
coding	I
has	O
generated	O
so	O
much	O
interest	O
recently	O
is	O
because	O
it	O
explains	O
an	O
interesting	O
phenomenon	O
in	O
neuroscience	O
.	O
in	O
particular	O
,	O
the	O
dictionary	B
that	O
is	O
learned	O
by	O
applying	O
13.8.	O
sparse	B
coding	I
*	O
471	O
(	O
a	O
)	O
(	O
c	O
)	O
(	O
e	O
)	O
(	O
b	O
)	O
(	O
d	O
)	O
(	O
f	O
)	O
figure	O
13.21	O
illustration	O
of	O
the	O
ﬁlters	O
learned	O
by	O
various	O
methods	O
when	O
applied	O
to	O
natural	O
image	O
patches	O
.	O
(	O
a	O
)	O
ica	O
.	O
figure	O
generated	O
by	O
icabasisdemo	O
,	O
(	O
each	O
patch	O
is	O
ﬁrst	O
centered	O
and	O
normalized	O
to	O
unit	O
norm	O
.	O
)	O
(	O
e	O
)	O
kindly	O
provided	O
by	O
aapo	O
hyvarinen	O
.	O
sparse	B
pca	O
with	O
low	O
sparsity	O
on	O
weight	O
matrix	O
.	O
(	O
f	O
)	O
sparse	B
pca	O
with	O
high	O
sparsity	O
on	O
weight	O
matrix	O
.	O
figure	O
generated	O
by	O
sparsedictdemo	O
,	O
written	O
by	O
julien	O
mairal	O
.	O
(	O
c	O
)	O
pca	O
.	O
(	O
d	O
)	O
non-negative	B
matrix	I
factorization	I
.	O
(	O
b	O
)	O
sparse	B
coding	I
.	O
472	O
chapter	O
13.	O
sparse	B
linear	O
models	O
sparse	B
coding	I
to	O
patches	O
of	O
natural	O
images	O
consists	O
of	O
basis	O
vectors	O
that	O
look	O
like	O
the	O
ﬁlters	O
that	O
are	O
found	O
in	O
simple	B
cells	I
in	O
the	O
primary	O
visual	O
cortex	O
of	O
the	O
mammalian	O
brain	O
(	O
olshausen	O
and	O
field	O
1996	O
)	O
.	O
in	O
particular	O
,	O
the	O
ﬁlters	O
look	O
like	O
bar	O
and	O
edge	O
detectors	O
,	O
as	O
shown	O
in	O
figure	O
13.21	O
(	O
b	O
)	O
.	O
(	O
in	O
this	O
example	O
,	O
the	O
parameter	B
λ	O
was	O
chosen	O
so	O
that	O
the	O
number	O
of	O
active	O
basis	O
functions	O
(	O
non-zero	O
components	O
of	O
zi	O
)	O
is	O
about	O
10	O
.	O
)	O
interestingly	O
,	O
using	O
ica	O
gives	O
visually	O
similar	B
results	O
,	O
as	O
shown	O
in	O
figure	O
13.21	O
(	O
a	O
)	O
.	O
by	O
contrast	O
,	O
applying	O
pca	O
to	O
the	O
same	O
data	O
results	O
in	O
sinusoidal	O
gratings	O
,	O
as	O
shown	O
in	O
figure	O
13.21	O
(	O
c	O
)	O
;	O
these	O
do	O
not	O
look	O
like	O
cortical	O
cell	O
response	O
patterns.10	O
it	O
has	O
therefore	O
been	O
conjectured	O
that	O
parts	O
of	O
the	O
cortex	O
may	O
be	O
performing	O
sparse	B
coding	I
of	O
the	O
sensory	O
input	O
;	O
the	O
resulting	O
latent	B
representation	O
is	O
then	O
further	O
processed	O
by	O
higher	O
levels	O
of	O
the	O
brain	O
.	O
figure	O
13.21	O
(	O
d	O
)	O
shows	O
the	O
result	O
of	O
using	O
nmf	O
,	O
and	O
figure	O
13.21	O
(	O
e-f	O
)	O
show	O
the	O
results	O
of	O
sparse	B
pca	O
,	O
as	O
we	O
increase	O
the	O
sparsity	B
of	O
the	O
basis	O
vectors	O
.	O
13.8.3	O
compressed	B
sensing	I
imagine	O
that	O
,	O
instead	O
of	O
observing	O
the	O
data	O
x	O
∈	O
r	O
although	O
it	O
is	O
interesting	O
to	O
look	O
at	O
the	O
dictionaries	O
learned	O
by	O
sparse	B
coding	I
,	O
it	O
is	O
not	O
necessarily	O
very	O
useful	O
.	O
however	O
,	O
there	O
are	O
some	O
practical	O
applications	O
of	O
sparse	B
coding	I
,	O
which	O
we	O
discuss	O
below	O
.	O
d	O
,	O
we	O
observe	O
a	O
low-dimensional	O
projection	B
m	O
,	O
r	O
is	O
a	O
m	O
×	O
d	O
matrix	O
,	O
m	O
(	O
cid:23	O
)	O
d	O
,	O
and	O
	O
is	O
a	O
noise	O
term	O
of	O
it	O
,	O
y	O
=	O
rx	O
+	O
	O
where	O
y	O
∈	O
r	O
(	O
usually	O
gaussian	O
)	O
.	O
we	O
assume	O
r	O
is	O
a	O
known	O
sensing	O
matrix	O
,	O
corresponding	O
to	O
different	O
linear	O
projections	O
of	O
x.	O
for	O
example	O
,	O
consider	O
an	O
mri	O
scanner	O
:	O
each	O
beam	O
direction	O
corresponds	O
to	O
a	O
vector	O
,	O
encoded	O
as	O
a	O
row	O
in	O
r.	O
figure	O
13.22	O
illustrates	O
the	O
modeling	O
assumptions	O
.	O
our	O
goal	O
is	O
to	O
infer	O
p	O
(	O
x|y	O
,	O
r	O
)	O
.	O
how	O
can	O
we	O
hope	O
to	O
recover	O
all	O
of	O
x	O
if	O
we	O
do	O
not	O
measure	O
all	O
of	O
x	O
?	O
the	O
answer	O
is	O
:	O
we	O
can	O
use	O
bayesian	O
inference	B
with	O
an	O
appropriate	O
prior	O
,	O
that	O
exploits	O
the	O
fact	O
that	O
natural	O
signals	O
can	O
be	O
expressed	O
as	O
a	O
weighted	O
combination	O
of	O
a	O
small	O
number	O
of	O
suitably	O
chosen	O
basis	B
functions	I
.	O
that	O
is	O
,	O
we	O
assume	O
x	O
=	O
wz	O
,	O
where	O
z	O
has	O
a	O
sparse	B
prior	O
,	O
and	O
w	O
is	O
suitable	O
dictionary	B
.	O
this	O
is	O
called	O
compressed	B
sensing	I
or	O
compressive	B
sensing	I
(	O
candes	O
et	O
al	O
.	O
2006	O
;	O
baruniak	O
2007	O
;	O
candes	O
and	O
wakin	O
2008	O
;	O
bruckstein	O
et	O
al	O
.	O
2009	O
)	O
.	O
for	O
cs	O
to	O
work	O
,	O
it	O
is	O
important	O
to	O
represent	O
the	O
signal	O
in	O
the	O
right	O
basis	O
,	O
otherwise	O
it	O
will	O
not	O
be	O
sparse	B
.	O
in	O
traditional	O
cs	O
applications	O
,	O
the	O
dictionary	B
is	O
ﬁxed	O
to	O
be	O
a	O
standard	O
form	O
,	O
such	O
as	O
wavelets	O
.	O
however	O
,	O
one	O
can	O
get	O
much	O
better	O
performance	O
by	O
learning	B
a	O
domain-speciﬁc	O
dictionary	B
using	O
sparse	B
coding	I
(	O
zhou	O
et	O
al	O
.	O
2009	O
)	O
.	O
as	O
for	O
the	O
sensing	O
matrix	O
r	O
,	O
it	O
is	O
often	O
chosen	O
to	O
be	O
a	O
random	O
matrix	O
,	O
for	O
reasons	O
explained	O
in	O
(	O
candes	O
and	O
wakin	O
2008	O
)	O
.	O
however	O
,	O
one	O
can	O
get	O
better	O
performance	O
by	O
adapting	O
the	O
projection	B
matrix	O
to	O
the	O
dictionary	B
(	O
seeger	O
and	O
nickish	O
2008	O
;	O
chang	O
et	O
al	O
.	O
2009	O
)	O
.	O
13.8.4	O
image	B
inpainting	I
and	O
denoising	O
suppose	O
we	O
have	O
an	O
image	O
which	O
is	O
corrupted	O
in	O
some	O
way	O
,	O
e.g.	O
,	O
by	O
having	O
text	O
or	O
scratches	O
sparsely	O
superimposed	O
on	O
top	O
of	O
it	O
,	O
as	O
in	O
figure	O
13.23.	O
we	O
might	O
want	O
to	O
estimate	O
the	O
underlying	O
(	O
cid:6	O
)	O
10.	O
the	O
reason	O
pca	O
discovers	O
sinusoidal	O
grating	O
patterns	O
is	O
because	O
it	O
is	O
trying	O
to	O
model	O
the	O
covariance	B
of	O
the	O
data	O
,	O
which	O
,	O
in	O
the	O
case	O
of	O
image	O
patches	O
,	O
is	O
translation	B
invariant	I
.	O
this	O
means	O
cov	O
[	O
i	O
(	O
x	O
,	O
y	O
)	O
,	O
i	O
(	O
x	O
(	O
cid:2	O
)	O
,	O
y	O
(	O
cid:2	O
)	O
)	O
2	O
for	O
some	O
function	O
f	O
,	O
where	O
i	O
(	O
x	O
,	O
y	O
)	O
is	O
the	O
image	O
intensity	O
at	O
location	O
(	O
x	O
,	O
y	O
)	O
.	O
one	O
can	O
show	O
(	O
hyvarinen	O
et	O
al	O
.	O
2009	O
,	O
p125	O
)	O
that	O
the	O
eigenvectors	O
of	O
a	O
matrix	O
of	O
this	O
kind	O
are	O
always	O
sinusoids	O
of	O
different	O
phases	O
,	O
i.e.	O
,	O
pca	O
discovers	O
a	O
fourier	O
basis	O
.	O
)	O
2	O
+	O
(	O
y	O
−	O
y	O
(	O
cid:2	O
)	O
(	O
cid:5	O
)	O
)	O
]	O
=	O
f	O
(	O
x	O
−	O
x	O
(	O
cid:2	O
)	O
13.8.	O
sparse	B
coding	I
*	O
473	O
z	O
x	O
y	O
λ	O
w	O
r	O
figure	O
13.22	O
schematic	O
dgm	O
for	O
compressed	B
sensing	I
.	O
we	O
observe	O
a	O
low	O
dimensional	O
measurement	O
y	O
generated	O
by	O
passing	O
x	O
through	O
a	O
measurement	O
matrix	O
r	O
,	O
and	O
possibly	O
subject	O
to	O
observation	B
noise	O
with	O
variance	B
σ2	O
.	O
we	O
assume	O
that	O
x	O
has	O
a	O
sparse	B
decomposition	O
in	O
terms	O
of	O
the	O
dictionary	B
w	O
and	O
the	O
latent	B
variables	O
z.	O
the	O
parameter	B
λ	O
controlls	O
the	O
sparsity	B
level	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
13.23	O
an	O
example	O
of	O
image	B
inpainting	I
using	O
sparse	B
coding	I
.	O
left	O
:	O
original	O
image	O
.	O
right	O
:	O
recon-	O
struction	O
.	O
source	O
:	O
figure	O
13	O
of	O
(	O
mairal	O
et	O
al	O
.	O
2008	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
julien	O
mairal	O
.	O
“	O
clean	O
”	O
image	O
.	O
this	O
is	O
called	O
image	B
inpainting	I
.	O
one	O
can	O
use	O
similar	B
techniques	O
for	O
image	B
denoising	I
.	O
we	O
can	O
model	O
this	O
as	O
a	O
special	O
kind	O
of	O
compressed	B
sensing	I
problem	O
.	O
the	O
basic	O
idea	O
is	O
as	O
follows	O
.	O
we	O
partition	O
the	O
image	O
into	O
overlapping	O
patches	O
,	O
yi	O
,	O
and	O
concatenate	O
them	O
to	O
form	O
y.	O
we	O
deﬁne	O
r	O
so	O
that	O
the	O
i	O
’	O
th	O
row	O
selects	O
out	O
patch	O
i.	O
now	O
deﬁne	O
v	O
to	O
be	O
the	O
visible	B
(	O
uncorrupted	O
)	O
components	O
of	O
y	O
,	O
and	O
h	O
to	O
be	O
the	O
hidden	B
components	O
.	O
to	O
perform	O
image	B
inpainting	I
,	O
we	O
just	O
compute	O
p	O
(	O
yh|yv	O
,	O
θ	O
)	O
,	O
where	O
θ	O
are	O
the	O
model	O
parameters	O
,	O
which	O
specify	O
the	O
dictionary	B
w	O
and	O
the	O
sparsity	B
level	O
λ	O
of	O
z.	O
we	O
can	O
either	O
learn	O
a	O
dictionary	B
offline	O
from	O
a	O
database	O
of	O
images	O
,	O
or	O
we	O
can	O
learn	O
a	O
dictionary	B
just	O
for	O
this	O
image	O
,	O
based	O
on	O
the	O
non-corrupted	O
patches	O
.	O
from	O
7	O
×	O
106	O
undamaged	O
12	O
×	O
12	O
color	O
patches	O
in	O
the	O
12	O
mega-pixel	O
image	O
.	O
figure	O
13.23	O
shows	O
this	O
technique	O
in	O
action	B
.	O
the	O
dictionary	B
(	O
of	O
size	O
256	O
atoms	O
)	O
was	O
learned	O
an	O
alternative	O
approach	O
is	O
to	O
use	O
a	O
graphical	B
model	I
(	O
e.g.	O
,	O
the	O
ﬁelds	B
of	I
experts	I
model	O
(	O
s.	O
474	O
chapter	O
13.	O
sparse	B
linear	O
models	O
and	O
black	O
2009	O
)	O
)	O
which	O
directly	O
encodes	O
correlations	O
between	O
neighboring	O
image	O
patches	O
,	O
rather	O
than	O
using	O
a	O
latent	O
variable	O
model	O
.	O
unfortunately	O
such	O
models	O
tend	O
to	O
be	O
computationally	O
more	O
expensive	O
.	O
exercises	O
exercise	O
13.1	O
partial	O
derivative	O
of	O
the	O
rss	O
deﬁne	O
rss	O
(	O
w	O
)	O
=	O
||xw	O
−	O
y||2	O
2	O
a.	O
show	O
that	O
∂	O
∂wk	O
rss	O
(	O
w	O
)	O
=a	O
ak	O
=	O
2	O
kwk	O
−	O
ck	O
n	O
(	O
cid:12	O
)	O
n	O
(	O
cid:12	O
)	O
i=1	O
ik	O
=	O
2||x	O
:	O
,k||2	O
x2	O
(	O
13.180	O
)	O
(	O
13.181	O
)	O
(	O
13.182	O
)	O
xik	O
(	O
yi	O
−	O
wt−kxi	O
,	O
−k	O
)	O
=	O
2xt	O
ck	O
=	O
2	O
i=1	O
(	O
13.183	O
)	O
where	O
w−k	O
=	O
w	O
without	O
component	O
k	O
,	O
xi	O
,	O
−k	O
is	O
xi	O
without	O
component	O
k	O
,	O
and	O
rk	O
=	O
y	O
−	O
wt−kx	O
:	O
,−k	O
is	O
the	O
residual	B
due	O
to	O
using	O
all	O
the	O
features	B
except	O
feature	O
k.	O
hint	O
:	O
partition	O
the	O
weights	O
into	O
those	O
involving	O
k	O
and	O
those	O
not	O
involving	O
k.	O
rss	O
(	O
w	O
)	O
=	O
0	O
,	O
then	O
∂	O
:	O
,krk	O
b.	O
show	O
that	O
if	O
∂wk	O
ˆwk	O
=	O
xt	O
:	O
,krk	O
||x	O
:	O
,k||2	O
(	O
13.184	O
)	O
hence	O
when	O
we	O
sequentially	O
add	O
features	B
,	O
the	O
optimal	O
weight	O
for	O
feature	O
k	O
is	O
computed	O
by	O
computing	O
orthogonally	O
projecting	O
x	O
:	O
,k	O
onto	O
the	O
current	O
residual	B
.	O
exercise	O
13.2	O
derivation	O
of	O
m	O
step	O
for	O
eb	O
for	O
linear	B
regression	I
derive	O
equations	O
13.166	O
and	O
13.168.	O
hint	O
:	O
the	O
following	O
identity	O
should	O
be	O
useful	O
σxt	O
x	O
=	O
σxt	O
x	O
+	O
β−1σa	O
−	O
β−1σa	O
=	O
σ	O
(	O
xt	O
xβ	O
+	O
a	O
)	O
β−1	O
−	O
β−1σa	O
=	O
(	O
a	O
+	O
βxt	O
x	O
)	O
=	O
(	O
i	O
−	O
aς	O
)	O
β−1	O
−1	O
(	O
xt	O
xβ	O
+	O
a	O
)	O
β−1	O
−	O
β−1σa	O
(	O
13.185	O
)	O
(	O
13.186	O
)	O
(	O
13.187	O
)	O
(	O
13.188	O
)	O
exercise	O
13.3	O
derivation	O
of	O
ﬁxed	B
point	I
updates	O
for	O
eb	O
for	O
linear	B
regression	I
derive	O
equations	O
13.169	O
and	O
13.170.	O
hint	O
:	O
the	O
easiest	O
way	O
to	O
derive	O
this	O
result	O
is	O
to	O
rewrite	O
log	O
p	O
(	O
d|α	O
,	O
β	O
)	O
as	O
in	O
equation	O
8.54.	O
this	O
is	O
exactly	O
equivalent	O
,	O
since	O
in	O
the	O
case	O
of	O
a	O
gaussian	O
prior	O
and	O
likelihood	B
,	O
the	O
posterior	O
is	O
also	O
gaussian	O
,	O
so	O
the	O
laplace	O
“	O
approximation	O
”	O
is	O
exact	O
.	O
in	O
this	O
case	O
,	O
we	O
get	O
log	O
p	O
(	O
d|α	O
,	O
β	O
)	O
=	O
||y	O
−	O
xw||2	O
n	O
2	O
+	O
log	O
β	O
−	O
β	O
(	O
cid:12	O
)	O
2	O
1	O
2	O
j	O
log	O
αj	O
−	O
1	O
2	O
mt	O
am	O
+	O
1	O
2	O
log	O
|σ|	O
−	O
d	O
2	O
log	O
(	O
2π	O
)	O
(	O
13.189	O
)	O
the	O
rest	O
is	O
straightforward	O
algebra	O
.	O
13.8.	O
sparse	B
coding	I
*	O
exercise	O
13.4	O
marginal	B
likelihood	I
for	O
linear	B
regression	I
suppose	O
we	O
use	O
a	O
g-prior	B
of	O
the	O
form	O
σγ	O
=	O
g	O
(	O
xt	O
γ	O
xγ	O
)	O
−1	O
.	O
show	O
that	O
equation	O
13.16	O
simpliﬁes	O
to	O
p	O
(	O
d|γ	O
)	O
∝	O
(	O
1	O
+	O
g	O
)	O
s	O
(	O
γ	O
)	O
=y	O
t	O
y	O
−	O
g	O
1	O
+g	O
−dγ	O
/2	O
(	O
2bσ	O
+	O
s	O
(	O
γ	O
)	O
)	O
γ	O
xγ	O
)	O
yt	O
xγ	O
(	O
xt	O
−	O
(	O
2aσ	O
+n−1	O
)	O
/2	O
−1xt	O
γ	O
y	O
exercise	O
13.5	O
reducing	O
elastic	B
net	I
to	O
lasso	B
deﬁne	O
j1	O
(	O
w	O
)	O
=	O
|y	O
−	O
xw|2	O
+	O
λ2|w|2	O
+	O
λ1|w|1	O
and	O
j2	O
(	O
w	O
)	O
=	O
|˜y	O
−	O
˜x	O
˜w|2	O
+	O
cλ1|w|1	O
where	O
c	O
=	O
(	O
1	O
+	O
λ2	O
)	O
2	O
and	O
(	O
cid:3	O
)	O
−	O
1	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:3	O
)	O
y	O
0d×1	O
x√	O
λ2id	O
,	O
˜y	O
=	O
˜x	O
=	O
c	O
show	O
arg	O
min	O
j1	O
(	O
w	O
)	O
=	O
c	O
(	O
arg	O
min	O
j2	O
(	O
w	O
)	O
)	O
i.e	O
.	O
j1	O
(	O
cw	O
)	O
=	O
j2	O
(	O
w	O
)	O
and	O
hence	O
that	O
one	O
can	O
solve	O
an	O
elastic	B
net	I
problem	O
using	O
a	O
lasso	B
solver	O
on	O
modiﬁed	O
data	O
.	O
475	O
(	O
13.190	O
)	O
(	O
13.191	O
)	O
(	O
13.192	O
)	O
(	O
13.193	O
)	O
(	O
13.194	O
)	O
(	O
13.195	O
)	O
(	O
13.196	O
)	O
exercise	O
13.6	O
shrinkage	B
in	O
linear	B
regression	I
(	O
source	O
:	O
jaakkola	O
.	O
)	O
consider	O
performing	O
linear	B
regression	I
with	O
an	O
orthonormal	O
design	B
matrix	I
,	O
so	O
||x	O
:	O
,k||2	O
1	O
for	O
each	O
column	O
(	O
feature	O
)	O
k	O
,	O
and	O
xt	O
figure	O
13.24	O
plots	O
ˆwk	O
vs	O
ck	O
=	O
2yt	O
x	O
:	O
,k	O
,	O
the	O
correlation	O
of	O
feature	O
k	O
with	O
the	O
response	O
,	O
for	O
3	O
different	O
esimation	O
methods	O
:	O
ordinary	B
least	I
squares	I
(	O
ols	O
)	O
,	O
ridge	B
regression	I
with	O
parameter	B
λ2	O
,	O
and	O
lasso	B
with	O
parameter	B
λ1	O
.	O
:	O
,kx	O
:	O
,j	O
=	O
0	O
,	O
so	O
we	O
can	O
estimate	O
each	O
parameter	B
wk	O
separately	O
.	O
2	O
=	O
a.	O
unfortunately	O
we	O
forgot	O
to	O
label	B
the	O
plots	O
.	O
which	O
method	O
does	O
the	O
solid	O
(	O
1	O
)	O
,	O
dotted	O
(	O
2	O
)	O
and	O
dashed	O
(	O
3	O
)	O
line	O
correspond	O
to	O
?	O
hint	O
:	O
see	O
section	O
13.3.3.	O
b.	O
what	O
is	O
the	O
value	O
of	O
λ1	O
?	O
c.	O
what	O
is	O
the	O
value	O
of	O
λ2	O
?	O
exercise	O
13.7	O
prior	O
for	O
the	O
bernoulli	O
rate	B
parameter	O
in	O
the	O
spike	B
and	I
slab	I
model	O
consider	O
the	O
model	O
in	O
section	O
13.2.1.	O
suppose	O
we	O
put	O
a	O
prior	O
on	O
the	O
sparsity	B
rates	O
,	O
πj	O
∼	O
beta	O
(	O
α1	O
,	O
α2	O
)	O
.	O
derive	O
an	O
expression	O
for	O
p	O
(	O
γ|α	O
)	O
after	O
integrating	O
out	O
the	O
πj	O
’	O
s	O
.	O
discuss	O
some	O
advantages	O
and	O
disadvantages	O
of	O
this	O
approach	O
compared	O
to	O
assuming	O
πj	O
=	O
π0	O
for	O
ﬁxed	O
π0	O
.	O
476	O
chapter	O
13.	O
sparse	B
linear	O
models	O
k	O
w	O
1	O
2	O
3	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−0.6	O
−0.8	O
−1	O
−2	O
−1.5	O
−1	O
−0.5	O
0	O
c	O
k	O
0.5	O
1	O
1.5	O
2	O
figure	O
13.24	O
plot	O
of	O
ˆwk	O
vs	O
amount	O
of	O
correlation	O
ck	O
for	O
three	O
different	O
estimators	O
.	O
exercise	O
13.8	O
deriving	O
e	O
step	O
for	O
gsm	O
prior	O
show	O
that	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
e	O
|wj	O
1	O
τ	O
2	O
j	O
π	O
(	O
cid:2	O
)	O
(	O
wj	O
)	O
|wj|	O
=	O
where	O
π	O
(	O
wj	O
)	O
=	O
−	O
log	O
p	O
(	O
wj	O
)	O
and	O
p	O
(	O
wj	O
)	O
=	O
intn	O
(	O
wj|0	O
,	O
τ	O
2	O
j	O
)	O
p	O
(	O
τ	O
2	O
j	O
)	O
dτ	O
2	O
j	O
.	O
hint	O
1	O
:	O
1	O
τ	O
2	O
j	O
n	O
(	O
wj|0	O
,	O
τ	O
2	O
)	O
exp	O
(	O
−	O
w2	O
j	O
)	O
∝	O
1	O
j	O
2τ	O
2	O
τ	O
2	O
j	O
j	O
−1|wj|	O
−2wj	O
exp	O
(	O
−	O
w2	O
j	O
2τ	O
2	O
2τ	O
2	O
j	O
j	O
−1|wj|	O
d|wj|n	O
(	O
wj|0	O
,	O
τ	O
2	O
d	O
=	O
=	O
)	O
j	O
)	O
hint	O
2	O
:	O
d	O
d|wj|	O
p	O
(	O
wj	O
)	O
=	O
1	O
p	O
(	O
wj	O
)	O
d	O
d|wj|	O
log	O
p	O
(	O
wj	O
)	O
(	O
13.197	O
)	O
(	O
13.198	O
)	O
(	O
13.199	O
)	O
(	O
13.200	O
)	O
(	O
13.201	O
)	O
exercise	O
13.9	O
em	O
for	O
sparse	B
probit	O
regression	B
with	O
laplace	O
prior	O
derive	O
an	O
em	O
algorithm	O
for	O
ﬁtting	O
a	O
binary	O
probit	O
classiﬁer	O
(	O
section	O
9.4	O
)	O
using	O
a	O
laplace	O
prior	O
on	O
the	O
weights	O
.	O
(	O
if	O
you	O
get	O
stuck	O
,	O
see	O
(	O
figueiredo	O
2003	O
;	O
ding	O
and	O
harrison	O
2010	O
)	O
.	O
)	O
exercise	O
13.10	O
gsm	O
representation	O
of	O
group	B
lasso	I
j	O
∼	O
ga	O
(	O
δ	O
,	O
ρ2/2	O
)	O
,	O
ignoring	O
the	O
grouping	O
issue	O
for	O
now	O
.	O
the	O
marginal	B
distribution	I
consider	O
the	O
prior	O
τ	O
2	O
induced	O
on	O
the	O
weights	O
by	O
a	O
gamma	O
mixing	O
distribution	O
is	O
called	O
the	O
normal	B
gamma	O
distribution	O
and	O
is	O
13.8.	O
sparse	B
coding	I
*	O
given	O
by	O
ng	O
(	O
wj|δ	O
,	O
ρ	O
)	O
=	O
=	O
1/z	O
=	O
(	O
cid:5	O
)	O
1	O
z	O
√	O
j	O
)	O
ga	O
(	O
τ	O
2	O
n	O
(	O
wj|0	O
,	O
τ	O
2	O
|wj|δ−1/2	O
kδ−	O
1	O
j	O
|δ	O
,	O
ρ2/2	O
)	O
dτ	O
2	O
(	O
ρ|wj|	O
)	O
j	O
2	O
ρδ+	O
1	O
2	O
π	O
2δ−1/2	O
ρ	O
(	O
δ	O
)	O
477	O
(	O
13.202	O
)	O
(	O
13.203	O
)	O
(	O
13.204	O
)	O
where	O
kα	O
(	O
x	O
)	O
is	O
the	O
modiﬁed	O
bessel	O
function	O
of	O
the	O
second	O
kind	O
(	O
the	O
besselk	O
function	O
in	O
matlab	O
)	O
.	O
now	O
suppose	O
we	O
have	O
the	O
following	O
prior	O
on	O
the	O
variances	O
g	O
(	O
cid:13	O
)	O
g=1	O
(	O
cid:13	O
)	O
j∈g	O
p	O
(	O
σ2	O
1	O
:	O
d	O
)	O
=	O
p	O
(	O
σ2	O
1	O
:	O
dg	O
)	O
,	O
p	O
(	O
σ2	O
1	O
:	O
dg	O
)	O
=	O
ga	O
(	O
τ	O
2	O
j	O
|δg	O
,	O
ρ2/2	O
)	O
the	O
corresponding	O
marginal	O
for	O
each	O
group	O
of	O
weights	O
has	O
the	O
form	O
p	O
(	O
wg	O
)	O
∝	O
|ug|δg−dg	O
/2	O
kδg−dg	O
/2	O
(	O
ρug	O
)	O
where	O
ug	O
(	O
cid:2	O
)	O
’	O
(	O
cid:12	O
)	O
j∈g	O
g	O
,	O
j	O
=	O
||wg||2	O
w2	O
now	O
suppose	O
δg	O
=	O
(	O
dg	O
+	O
1	O
)	O
/2	O
,	O
so	O
δg	O
−	O
dg/2	O
=	O
1	O
that	O
the	O
resulting	O
map	O
estimate	O
is	O
equivalent	O
to	O
group	B
lasso	I
.	O
2	O
.	O
conveniently	O
,	O
we	O
have	O
k	O
1	O
2	O
(	O
13.205	O
)	O
(	O
13.206	O
)	O
(	O
13.207	O
)	O
π	O
2z	O
exp	O
(	O
−z	O
)	O
.	O
show	O
(	O
cid:15	O
)	O
(	O
z	O
)	O
=	O
exercise	O
13.11	O
projected	B
gradient	I
descent	I
for	O
(	O
cid:7	O
)	O
1	O
regularized	O
least	O
squares	O
consider	O
the	O
bpdn	O
problem	O
argminθ	O
rss	O
(	O
θ	O
)	O
+λ	O
||θ||1	O
.	O
by	O
using	O
the	O
split	B
variable	I
trick	O
introducted	O
in	O
section	O
7.4	O
(	O
i.e.	O
,	O
by	O
deﬁning	O
θ	O
=	O
[	O
θ+	O
,	O
θ−	O
]	O
)	O
,	O
rewrite	O
this	O
as	O
a	O
quadratic	B
program	I
with	O
a	O
simple	O
bound	O
constraint	O
.	O
then	O
sketch	O
how	O
to	O
use	O
projected	B
gradient	I
descent	I
to	O
solve	O
this	O
problem	O
.	O
(	O
if	O
you	O
get	O
stuck	O
,	O
consult	O
(	O
figueiredo	O
et	O
al	O
.	O
2007	O
)	O
.	O
)	O
exercise	O
13.12	O
subderivative	B
of	O
the	O
hinge	B
loss	I
function	O
let	O
f	O
(	O
x	O
)	O
=	O
(	O
1	O
−	O
x	O
)	O
+	O
be	O
the	O
hinge	B
loss	I
function	O
,	O
where	O
(	O
z	O
)	O
+	O
=	O
max	O
(	O
0	O
,	O
z	O
)	O
.	O
what	O
are	O
∂f	O
(	O
0	O
)	O
,	O
∂f	O
(	O
1	O
)	O
,	O
and	O
∂f	O
(	O
2	O
)	O
?	O
exercise	O
13.13	O
lower	O
bounds	O
to	O
convex	B
functions	O
let	O
f	O
be	O
a	O
convex	B
function	O
.	O
explain	O
how	O
to	O
ﬁnd	O
a	O
global	O
affine	O
lower	O
bound	O
to	O
f	O
at	O
an	O
arbitrary	O
point	O
x	O
∈	O
dom	O
(	O
f	O
)	O
.	O
14	O
kernels	O
14.1	O
introduction	O
so	O
far	O
in	O
this	O
book	O
,	O
we	O
have	O
been	O
assuming	O
that	O
each	O
object	O
that	O
we	O
wish	O
to	O
classify	O
or	O
cluster	O
or	O
process	O
in	O
anyway	O
can	O
be	O
represented	O
as	O
a	O
ﬁxed-size	O
feature	O
vector	O
,	O
typically	O
of	O
the	O
form	O
xi	O
∈	O
r	O
d.	O
however	O
,	O
for	O
certain	O
kinds	O
of	O
objects	O
,	O
it	O
is	O
not	O
clear	O
how	O
to	O
best	O
represent	O
them	O
as	O
ﬁxed-sized	O
feature	O
vectors	O
.	O
for	O
example	O
,	O
how	O
do	O
we	O
represent	O
a	O
text	O
document	O
or	O
protein	O
sequence	O
,	O
which	O
can	O
be	O
of	O
variable	O
length	O
?	O
or	O
a	O
molecular	O
structure	O
,	O
which	O
has	O
complex	O
3d	O
geometry	O
?	O
or	O
an	O
evolutionary	O
tree	O
,	O
which	O
has	O
variable	O
size	O
and	O
shape	O
?	O
one	O
approach	O
to	O
such	O
problems	O
is	O
to	O
deﬁne	O
a	O
generative	O
model	O
for	O
the	O
data	O
,	O
and	O
use	O
the	O
inferred	O
latent	B
representation	O
and/or	O
the	O
parameters	O
of	O
the	O
model	O
as	O
features	B
,	O
and	O
then	O
to	O
plug	O
these	O
features	B
in	O
to	O
standard	O
methods	O
.	O
for	O
example	O
,	O
in	O
chapter	O
28	O
,	O
we	O
discuss	O
deep	B
learning	I
,	O
which	O
is	O
essentially	O
an	O
unsupervised	O
way	O
to	O
learn	O
good	O
feature	O
representations	O
.	O
another	O
approach	O
is	O
to	O
assume	O
that	O
we	O
have	O
some	O
way	O
of	O
measuring	O
the	O
similarity	O
between	O
objects	O
,	O
that	O
doesn	O
’	O
t	O
require	O
preprocessing	O
them	O
into	O
feature	O
vector	O
format	O
.	O
for	O
example	O
,	O
when	O
)	O
≥	O
0	O
be	O
some	O
comparing	O
strings	O
,	O
we	O
can	O
compute	O
the	O
edit	B
distance	I
between	O
them	O
.	O
let	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
measure	O
of	O
similarity	O
between	O
objects	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
∈	O
x	O
,	O
where	O
x	O
is	O
some	O
abstract	O
space	O
;	O
we	O
will	O
call	O
κ	O
a	O
kernel	B
function	I
.	O
note	O
that	O
the	O
word	O
“	O
kernel	B
”	O
has	O
several	O
meanings	O
;	O
we	O
will	O
discuss	O
a	O
different	O
interpretation	O
in	O
section	O
14.7.1.	O
in	O
this	O
chapter	O
,	O
we	O
will	O
discuss	O
several	O
kinds	O
of	O
kernel	B
functions	O
.	O
we	O
then	O
describe	O
some	O
algorithms	O
that	O
can	O
be	O
written	O
purely	O
in	O
terms	O
of	O
kernel	B
function	I
computations	O
.	O
such	O
methods	O
can	O
be	O
used	O
when	O
we	O
don	O
’	O
t	O
have	O
access	O
to	O
(	O
or	O
choose	O
not	O
to	O
look	O
at	O
)	O
the	O
“	O
inside	O
”	O
of	O
the	O
objects	O
x	O
that	O
we	O
are	O
processing	O
.	O
14.2	O
kernel	B
functions	O
we	O
deﬁne	O
a	O
kernel	B
function	I
to	O
be	O
a	O
real-valued	O
function	O
of	O
two	O
arguments	O
,	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
∈	O
x	O
.	O
typically	O
the	O
function	O
is	O
symmetric	B
(	O
i.e.	O
,	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
give	O
several	O
examples	O
below.	O
)	O
∈	O
r	O
,	O
for	O
)	O
=	O
κ	O
(	O
x	O
(	O
cid:4	O
)	O
,	O
x	O
)	O
)	O
,	O
and	O
non-negative	O
(	O
i.e.	O
,	O
)	O
≥	O
0	O
)	O
,	O
so	O
it	O
can	O
be	O
interpreted	O
as	O
a	O
measure	O
of	O
similarity	O
,	O
but	O
this	O
is	O
not	O
required	O
.	O
we	O
480	O
14.2.1	O
rbf	O
kernels	O
chapter	O
14.	O
kernels	O
the	O
squared	B
exponential	I
kernel	I
(	O
se	O
kernel	B
)	O
or	O
gaussian	O
kernel	B
is	O
deﬁned	O
by	O
)	O
(	O
14.1	O
)	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
exp	O
(	O
x	O
−	O
x	O
(	O
cid:4	O
)	O
if	O
σ	O
is	O
diagonal	B
,	O
this	O
can	O
be	O
written	O
as	O
(	O
xj	O
−	O
x	O
(	O
cid:4	O
)	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
d	O
(	O
cid:2	O
)	O
(	O
cid:8	O
)	O
−	O
1	O
2	O
⎛	O
⎝−	O
1	O
(	O
cid:9	O
)	O
)	O
t	O
σ−1	O
(	O
x	O
−	O
x	O
(	O
cid:4	O
)	O
⎞	O
⎠	O
j	O
)	O
2	O
1	O
σ2	O
j	O
(	O
14.3	O
)	O
(	O
14.5	O
)	O
(	O
14.6	O
)	O
2	O
)	O
=	O
exp	O
(	O
14.2	O
)	O
we	O
can	O
interpret	O
the	O
σj	O
as	O
deﬁning	O
the	O
characteristic	B
length	I
scale	I
of	O
dimension	O
j.	O
if	O
σj	O
=	O
∞	O
,	O
if	O
σ	O
is	O
the	O
corresponding	O
dimension	O
is	O
ignored	O
;	O
hence	O
this	O
is	O
known	O
as	O
the	O
ard	O
kernel	B
.	O
spherical	B
,	O
we	O
get	O
the	O
isotropic	B
kernel	O
j=1	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
exp	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
−||x	O
−	O
x	O
(	O
cid:4	O
)	O
||2	O
2σ2	O
here	O
σ2	O
is	O
known	O
as	O
the	O
bandwidth	B
.	O
equation	O
14.3	O
is	O
an	O
example	O
of	O
a	O
a	O
radial	B
basis	I
function	I
or	O
rbf	O
kernel	B
,	O
since	O
it	O
is	O
only	O
a	O
function	O
of	O
||x	O
−	O
x	O
(	O
cid:4	O
)	O
||	O
.	O
14.2.2	O
kernels	O
for	O
comparing	O
documents	O
when	O
performing	O
document	B
classiﬁcation	I
or	O
retrieval	O
,	O
it	O
is	O
useful	O
to	O
have	O
a	O
way	O
of	O
comparing	O
two	O
documents	O
,	O
xi	O
and	O
xi	O
(	O
cid:2	O
)	O
.	O
if	O
we	O
use	O
a	O
bag	B
of	I
words	I
representation	O
,	O
where	O
xij	O
is	O
the	O
number	O
of	O
times	O
words	O
j	O
occurs	O
in	O
document	O
i	O
,	O
we	O
can	O
use	O
the	O
cosine	B
similarity	I
,	O
which	O
is	O
deﬁned	O
by	O
κ	O
(	O
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
)	O
=	O
xt	O
i	O
xi	O
(	O
cid:2	O
)	O
||xi||2||xi	O
(	O
cid:2	O
)	O
||2	O
(	O
14.4	O
)	O
this	O
quantity	O
measures	O
the	O
cosine	O
of	O
the	O
angle	O
between	O
xi	O
and	O
xi	O
(	O
cid:2	O
)	O
when	O
interpreted	O
as	O
vectors	O
.	O
since	O
xi	O
is	O
a	O
count	O
vector	O
(	O
and	O
hence	O
non-negative	O
)	O
,	O
the	O
cosine	B
similarity	I
is	O
between	O
0	O
and	O
1	O
,	O
where	O
0	O
means	O
the	O
vectors	O
are	O
orthogonal	O
and	O
therefore	O
have	O
no	O
words	O
in	O
common	O
.	O
unfortunately	O
,	O
this	O
simple	O
method	O
does	O
not	O
work	O
very	O
well	O
,	O
for	O
two	O
main	O
reasons	O
.	O
first	O
,	O
if	O
xi	O
has	O
any	O
word	O
in	O
common	O
with	O
xi	O
(	O
cid:2	O
)	O
,	O
it	O
is	O
deemed	O
similar	B
,	O
even	O
though	O
some	O
popular	O
words	O
,	O
such	O
as	O
“	O
the	O
”	O
or	O
“	O
and	O
”	O
occur	O
in	O
many	O
documents	O
,	O
and	O
are	O
therefore	O
not	O
discriminative	O
.	O
(	O
these	O
are	O
known	O
as	O
stop	B
words	I
.	O
)	O
second	O
,	O
if	O
a	O
discriminative	B
word	O
occurs	O
many	O
times	O
in	O
a	O
document	O
,	O
the	O
similarity	O
is	O
artiﬁcially	O
boosted	O
,	O
even	O
though	O
word	O
usage	O
tends	O
to	O
be	O
bursty	B
,	O
meaning	O
that	O
once	O
a	O
word	O
is	O
used	O
in	O
a	O
document	O
it	O
is	O
very	O
likely	O
to	O
be	O
used	O
again	O
(	O
see	O
section	O
3.5.5	O
)	O
.	O
fortunately	O
,	O
we	O
can	O
signiﬁcantly	O
improve	O
performance	O
using	O
some	O
simple	O
preprocessing	O
.	O
the	O
idea	O
is	O
to	O
replace	O
the	O
word	O
count	O
vector	O
with	O
a	O
new	O
feature	O
vector	O
called	O
the	O
tf-idf	O
representa-	O
tion	O
,	O
which	O
stands	O
for	O
“	O
term	O
frequency	O
inverse	O
document	O
frequency	O
”	O
.	O
we	O
deﬁne	O
this	O
as	O
follows	O
.	O
first	O
,	O
the	O
term	O
frequency	O
is	O
deﬁned	O
as	O
a	O
log-transform	O
of	O
the	O
count	O
:	O
this	O
reduces	O
the	O
impact	O
of	O
words	O
that	O
occur	O
many	O
times	O
within	O
one	O
document	O
.	O
second	O
,	O
the	O
inverse	O
document	O
frequency	O
is	O
deﬁned	O
as	O
tf	O
(	O
xij	O
)	O
(	O
cid:2	O
)	O
log	O
(	O
1	O
+	O
xij	O
)	O
idf	O
(	O
j	O
)	O
(	O
cid:2	O
)	O
log	O
1	O
+	O
(	O
cid:10	O
)	O
n	O
n	O
i=1	O
i	O
(	O
xij	O
>	O
0	O
)	O
14.2.	O
kernel	B
functions	O
481	O
where	O
n	O
is	O
the	O
total	O
number	O
of	O
documents	O
,	O
and	O
the	O
denominator	O
counts	O
how	O
many	O
documents	O
contain	O
term	O
j.	O
finally	O
,	O
we	O
deﬁne	O
tf-idf	O
(	O
xi	O
)	O
(	O
cid:2	O
)	O
[	O
tf	O
(	O
xij	O
)	O
×	O
idf	O
(	O
j	O
)	O
]	O
v	O
(	O
14.7	O
)	O
j=1	O
(	O
there	O
are	O
several	O
other	O
ways	O
to	O
deﬁne	O
the	O
tf	O
and	O
idf	O
terms	O
,	O
see	O
(	O
manning	O
et	O
al	O
.	O
2008	O
)	O
for	O
details	O
.	O
)	O
we	O
then	O
use	O
this	O
inside	O
the	O
cosine	B
similarity	I
measure	O
.	O
that	O
is	O
,	O
our	O
new	O
kernel	B
has	O
the	O
form	O
κ	O
(	O
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
)	O
=	O
φ	O
(	O
xi	O
)	O
t	O
φ	O
(	O
xi	O
(	O
cid:2	O
)	O
)	O
||φ	O
(	O
xi	O
)	O
||2||φ	O
(	O
xi	O
(	O
cid:2	O
)	O
)	O
||2	O
(	O
14.8	O
)	O
where	O
φ	O
(	O
x	O
)	O
=	O
tf-idf	O
(	O
x	O
)	O
.	O
this	O
gives	O
good	O
results	O
for	O
information	B
retrieval	I
(	O
manning	O
et	O
al	O
.	O
2008	O
)	O
.	O
a	O
probabilistic	O
interpretation	O
of	O
the	O
tf-idf	O
kernel	B
is	O
given	O
in	O
(	O
elkan	O
2005	O
)	O
.	O
14.2.3	O
mercer	O
(	O
positive	B
deﬁnite	I
)	O
kernels	O
some	O
methods	O
that	O
we	O
will	O
study	O
require	O
that	O
the	O
kernel	B
function	I
satisfy	O
the	O
requirement	O
that	O
the	O
gram	O
matrix	O
,	O
deﬁned	O
by	O
⎛	O
⎜⎝	O
κ	O
(	O
x1	O
,	O
x1	O
)	O
k	O
=	O
κ	O
(	O
x1	O
,	O
xn	O
)	O
⎞	O
⎟⎠	O
···	O
...	O
···	O
(	O
14.9	O
)	O
κ	O
(	O
xn	O
,	O
x1	O
)	O
κ	O
(	O
xn	O
,	O
xn	O
)	O
be	O
positive	B
deﬁnite	I
for	O
any	O
set	O
of	O
inputs	O
{	O
xi	O
}	O
n	O
i=1	O
.	O
we	O
call	O
such	O
a	O
kernel	B
a	O
mercer	O
kernel	B
,	O
or	O
positive	B
deﬁnite	I
kernel	I
.	O
it	O
can	O
be	O
shown	O
(	O
schoelkopf	O
and	O
smola	O
2002	O
)	O
that	O
the	O
gaussian	O
kernel	B
is	O
a	O
mercer	O
kernel	B
as	O
is	O
the	O
cosine	B
similarity	I
kernel	O
(	O
sahami	O
and	O
heilman	O
2006	O
)	O
.	O
the	O
importance	O
of	O
mercer	O
kernels	O
is	O
the	O
following	O
result	O
,	O
known	O
as	O
mercer	O
’	O
s	O
theorem	O
.	O
if	O
the	O
gram	O
matrix	O
is	O
positive	B
deﬁnite	I
,	O
we	O
can	O
compute	O
an	O
eigenvector	O
decomposition	O
of	O
it	O
as	O
follows	O
k	O
=	O
ut	O
λu	O
where	O
λ	O
is	O
a	O
diagonal	B
matrix	O
of	O
eigenvalues	O
λi	O
>	O
0.	O
now	O
consider	O
an	O
element	O
of	O
k	O
:	O
kij	O
=	O
(	O
λ	O
1	O
2	O
u	O
:	O
,i	O
)	O
t	O
(	O
λ	O
1	O
let	O
us	O
deﬁne	O
φ	O
(	O
xi	O
)	O
=	O
λ	O
1	O
2	O
u	O
:	O
j	O
)	O
2	O
u	O
:	O
i.	O
then	O
we	O
can	O
write	O
kij	O
=	O
φ	O
(	O
xi	O
)	O
t	O
φ	O
(	O
xj	O
)	O
(	O
14.10	O
)	O
(	O
14.11	O
)	O
(	O
14.12	O
)	O
thus	O
we	O
see	O
that	O
the	O
entries	O
in	O
the	O
kernel	B
matrix	O
can	O
be	O
computed	O
by	O
performing	O
an	O
inner	O
product	O
of	O
some	O
feature	O
vectors	O
that	O
are	O
implicitly	O
deﬁned	O
by	O
the	O
eigenvectors	O
u.	O
in	O
general	O
,	O
if	O
the	O
kernel	B
is	O
mercer	O
,	O
then	O
there	O
exists	O
a	O
function	O
φ	O
mapping	O
x	O
∈	O
x	O
to	O
r	O
d	O
such	O
that	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
φ	O
(	O
x	O
)	O
t	O
φ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
(	O
14.13	O
)	O
where	O
φ	O
depends	O
on	O
the	O
eigen	O
functions	O
of	O
κ	O
(	O
so	O
d	O
is	O
a	O
potentially	O
inﬁnite	O
dimensional	O
space	O
)	O
.	O
+	O
r	O
)	O
m	O
,	O
where	O
r	O
>	O
0.	O
one	O
can	O
show	O
that	O
the	O
corresponding	O
feature	O
vector	O
φ	O
(	O
x	O
)	O
will	O
contain	O
all	O
terms	O
up	O
to	O
degree	B
m	O
.	O
for	O
example	O
,	O
if	O
m	O
=	O
2	O
,	O
γ	O
=	O
r	O
=	O
1	O
and	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
∈	O
r	O
for	O
example	O
,	O
consider	O
the	O
(	O
non-stationary	O
)	O
polynomial	B
kernel	I
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
(	O
γxt	O
x	O
(	O
cid:4	O
)	O
2	O
,	O
we	O
have	O
(	O
1	O
+	O
xt	O
x	O
(	O
cid:4	O
)	O
)	O
2	O
=	O
(	O
1	O
+	O
x1x	O
(	O
cid:4	O
)	O
=	O
1	O
+	O
2x1x	O
(	O
cid:4	O
)	O
1	O
+	O
x2x	O
(	O
cid:4	O
)	O
2	O
)	O
2	O
2	O
+	O
(	O
x1x1	O
)	O
2	O
+	O
(	O
x2x	O
(	O
cid:4	O
)	O
1	O
+	O
2x2x	O
(	O
cid:4	O
)	O
2	O
)	O
2	O
+	O
2x1x	O
(	O
cid:4	O
)	O
1x2x	O
(	O
cid:4	O
)	O
2	O
(	O
14.14	O
)	O
(	O
14.15	O
)	O
482	O
chapter	O
14.	O
kernels	O
this	O
can	O
be	O
written	O
as	O
φ	O
(	O
x	O
)	O
t	O
φ	O
(	O
x	O
(	O
cid:4	O
)	O
1	O
,	O
x2	O
2	O
,	O
√	O
2x1	O
,	O
φ	O
(	O
x	O
)	O
=	O
[	O
1	O
,	O
2x2	O
,	O
x2	O
√	O
)	O
,	O
where	O
√	O
2x1x2	O
]	O
t	O
(	O
14.16	O
)	O
so	O
using	O
this	O
kernel	B
is	O
equivalent	O
to	O
working	O
in	O
a	O
6	O
dimensional	O
feature	O
space	O
.	O
in	O
the	O
case	O
of	O
a	O
gaussian	O
kernel	B
,	O
the	O
feature	O
map	O
lives	O
in	O
an	O
inﬁnite	O
dimensional	O
space	O
.	O
in	O
such	O
a	O
case	O
,	O
it	O
is	O
clearly	O
infeasible	O
to	O
explicitly	O
represent	O
the	O
feature	O
vectors	O
.	O
an	O
example	O
of	O
a	O
kernel	B
that	O
is	O
not	O
a	O
mercer	O
kernel	B
is	O
the	O
so-called	O
sigmoid	B
kernel	I
,	O
deﬁned	O
by	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
tanh	O
(	O
γxt	O
x	O
(	O
cid:4	O
)	O
+	O
r	O
)	O
(	O
14.17	O
)	O
(	O
note	O
that	O
this	O
uses	O
the	O
tanh	O
function	O
even	O
though	O
it	O
is	O
called	O
a	O
sigmoid	B
kernel	I
.	O
)	O
this	O
kernel	B
was	O
inspired	O
by	O
the	O
multi-layer	B
perceptron	I
(	O
see	O
section	O
16.5	O
)	O
,	O
but	O
there	O
is	O
no	O
real	O
reason	O
to	O
use	O
it	O
.	O
(	O
for	O
a	O
true	O
“	O
neural	O
net	O
kernel	B
”	O
,	O
which	O
is	O
positive	B
deﬁnite	I
,	O
see	O
section	O
15.4.5	O
.	O
)	O
in	O
general	O
,	O
establishing	O
that	O
a	O
kernel	B
is	O
a	O
mercer	O
kernel	B
is	O
difficult	O
,	O
and	O
requires	O
techniques	O
from	O
functional	O
analysis	O
.	O
however	O
,	O
one	O
can	O
show	O
that	O
it	O
is	O
possible	O
to	O
build	O
up	O
new	O
mercer	O
kernels	O
from	O
simpler	O
ones	O
using	O
a	O
set	O
of	O
standard	O
rules	O
.	O
for	O
example	O
,	O
if	O
κ1	O
and	O
κ2	O
are	O
both	O
mercer	O
,	O
so	O
is	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
.	O
see	O
e.g.	O
,	O
(	O
schoelkopf	O
and	O
smola	O
2002	O
)	O
for	O
details.	O
)	O
=	O
κ1	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
+	O
κ2	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
14.2.4	O
linear	O
kernels	O
deriving	O
the	O
feature	O
vector	O
implied	O
by	O
a	O
kernel	B
is	O
in	O
general	O
quite	O
difficult	O
,	O
and	O
only	O
possible	O
if	O
the	O
kernel	B
is	O
mercer	O
.	O
however	O
,	O
deriving	O
a	O
kernel	B
from	O
a	O
feature	O
vector	O
is	O
easy	O
:	O
we	O
just	O
use	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
φ	O
(	O
x	O
)	O
t	O
φ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
(	O
cid:24	O
)	O
φ	O
(	O
x	O
)	O
,	O
φ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
(	O
cid:25	O
)	O
if	O
φ	O
(	O
x	O
)	O
=	O
x	O
,	O
we	O
get	O
thelinear	O
kernel	B
,	O
deﬁned	O
by	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
xt	O
x	O
(	O
cid:4	O
)	O
(	O
14.18	O
)	O
(	O
14.19	O
)	O
this	O
is	O
useful	O
if	O
the	O
original	O
data	O
is	O
already	O
high	O
dimensional	O
,	O
and	O
if	O
the	O
original	O
features	B
are	O
individually	O
informative	O
,	O
e.g.	O
,	O
a	O
bag	B
of	I
words	I
representation	O
where	O
the	O
vocabulary	O
size	O
is	O
large	O
,	O
or	O
the	O
expression	O
level	O
of	O
many	O
genes	O
.	O
in	O
such	O
a	O
case	O
,	O
the	O
decision	B
boundary	I
is	O
likely	O
to	O
be	O
representable	O
as	O
a	O
linear	O
combination	O
of	O
the	O
original	O
features	B
,	O
so	O
it	O
is	O
not	O
necessary	O
to	O
work	O
in	O
some	O
other	O
feature	O
space	O
.	O
of	O
course	O
,	O
not	O
all	O
high	O
dimensional	O
problems	O
are	O
linearly	B
separable	I
.	O
for	O
example	O
,	O
images	O
are	O
high	O
dimensional	O
,	O
but	O
individual	O
pixels	O
are	O
not	O
very	O
informative	O
,	O
so	O
image	B
classiﬁcation	I
typically	O
requires	O
non-linear	O
kernels	O
(	O
see	O
e.g.	O
,	O
section	O
14.2.7	O
)	O
.	O
14.2.5	O
matern	O
kernels	O
the	O
matern	O
kernel	B
,	O
which	O
is	O
commonly	O
used	O
in	O
gaussian	O
process	O
regression	B
(	O
see	O
section	O
15.2	O
)	O
,	O
has	O
the	O
following	O
form	O
(	O
cid:11	O
)	O
√	O
(	O
cid:13	O
)	O
ν	O
(	O
cid:11	O
)	O
√	O
(	O
cid:13	O
)	O
κ	O
(	O
r	O
)	O
=	O
21−ν	O
γ	O
(	O
ν	O
)	O
2νr	O
(	O
cid:6	O
)	O
kν	O
2νr	O
(	O
cid:6	O
)	O
(	O
14.20	O
)	O
14.2.	O
kernel	B
functions	O
483	O
where	O
r	O
=	O
||x	O
−	O
x	O
(	O
cid:4	O
)	O
||	O
,	O
ν	O
>	O
0	O
,	O
(	O
cid:6	O
)	O
>	O
0	O
,	O
and	O
kν	O
is	O
a	O
modiﬁed	O
bessel	O
function	O
.	O
as	O
ν	O
→	O
∞	O
,	O
this	O
approaches	O
the	O
se	O
kernel	B
.	O
if	O
ν	O
=	O
1	O
2	O
,	O
the	O
kernel	B
simpliﬁes	O
to	O
κ	O
(	O
r	O
)	O
=	O
exp	O
(	O
−r/	O
(	O
cid:6	O
)	O
)	O
(	O
14.21	O
)	O
if	O
d	O
=	O
1	O
,	O
and	O
we	O
use	O
this	O
kernel	B
to	O
deﬁne	O
a	O
gaussian	O
process	O
(	O
see	O
chapter	O
15	O
)	O
,	O
we	O
get	O
the	O
ornstein-uhlenbeck	O
process	O
,	O
which	O
describes	O
the	O
velocity	O
of	O
a	O
particle	O
undergoing	O
brownian	O
motion	O
(	O
the	O
corresponding	O
function	O
is	O
continuous	O
but	O
not	O
differentiable	O
,	O
and	O
hence	O
is	O
very	O
“	O
jagged	O
”	O
)	O
.	O
14.2.6	O
string	O
kernels	O
consider	O
two	O
strings	O
x	O
,	O
and	O
x	O
(	O
cid:4	O
)	O
the	O
real	O
power	O
of	O
kernels	O
arises	O
when	O
the	O
inputs	O
are	O
structured	O
objects	O
.	O
as	O
an	O
example	O
,	O
we	O
now	O
describe	O
one	O
way	O
of	O
comparing	O
two	O
variable	O
length	O
strings	O
using	O
a	O
string	B
kernel	I
.	O
we	O
follow	O
the	O
presentation	O
of	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
p100	O
)	O
and	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p668	O
)	O
.	O
of	O
lengths	O
d	O
,	O
d	O
’	O
,	O
each	O
deﬁned	O
over	O
the	O
alphabet	O
a.	O
for	O
example	O
,	O
consider	O
two	O
amino	O
acid	O
sequences	O
,	O
deﬁned	O
over	O
the	O
20	O
letter	O
alphabet	O
a	O
=	O
{	O
a	O
,	O
r	O
,	O
n	O
,	O
d	O
,	O
c	O
,	O
e	O
,	O
q	O
,	O
g	O
,	O
h	O
,	O
i	O
,	O
l	O
,	O
k	O
,	O
m	O
,	O
f	O
,	O
p	O
,	O
s	O
,	O
t	O
,	O
w	O
,	O
y	O
,	O
v	O
}	O
.	O
let	O
x	O
be	O
the	O
following	O
sequence	O
of	O
length	O
110	O
iptsalvketlallsthrtllianetlripvpvhknhqlcteeifqgigtlesqtvqggtv	O
erlfknlslikkyidgqkkkcgeerrrvnqfldylqeflgvmntewi	O
and	O
let	O
x	O
(	O
cid:4	O
)	O
be	O
the	O
following	O
sequence	O
of	O
length	O
153	O
phrrdlcsrsiwlarkirsdltaltesyvkhqglwselteaerlqenlqayrtfhvlla	O
rlledqqvhftptegdfhqaihtlllqvaafayqieelmilleykiprneadgmlfekk	O
lwglkvlqelsqwtvrsihdlrfisshqtgip	O
these	O
strings	O
have	O
the	O
substring	O
lqe	O
in	O
common	O
.	O
we	O
can	O
deﬁne	O
the	O
similarity	O
of	O
two	O
strings	O
to	O
be	O
the	O
number	O
of	O
substrings	O
they	O
have	O
in	O
common	O
.	O
more	O
formally	O
and	O
more	O
generally	O
,	O
let	O
us	O
say	O
that	O
s	O
is	O
a	O
substring	O
of	O
x	O
if	O
we	O
can	O
write	O
x	O
=	O
usv	O
for	O
some	O
(	O
possibly	O
empty	O
)	O
strings	O
u	O
,	O
s	O
and	O
v.	O
now	O
let	O
φs	O
(	O
x	O
)	O
denote	O
the	O
number	O
of	O
times	O
that	O
substring	O
s	O
appears	O
in	O
string	O
x.	O
we	O
deﬁne	O
the	O
kernel	B
between	O
two	O
strings	O
x	O
and	O
x	O
(	O
cid:4	O
)	O
as	O
(	O
cid:2	O
)	O
s∈a∗	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
wsφs	O
(	O
x	O
)	O
φs	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
(	O
14.22	O
)	O
where	O
ws	O
≥	O
0	O
and	O
a∗	O
is	O
the	O
set	O
of	O
all	O
strings	O
(	O
of	O
any	O
length	O
)	O
from	O
the	O
alphabet	O
a	O
(	O
this	O
is	O
known	O
as	O
the	O
kleene	O
star	O
operator	O
)	O
.	O
this	O
is	O
a	O
mercer	O
kernel	B
,	O
and	O
be	O
computed	O
in	O
o	O
(	O
|x|	O
+	O
|x	O
(	O
cid:4	O
)	O
|	O
)	O
time	O
(	O
for	O
certain	O
settings	O
of	O
the	O
weights	O
{	O
ws	O
}	O
)	O
using	O
suffix	B
trees	I
(	O
leslie	O
et	O
al	O
.	O
2003	O
;	O
vishwanathan	O
and	O
smola	O
2003	O
;	O
shawe-taylor	O
and	O
cristianini	O
2004	O
)	O
.	O
there	O
are	O
various	O
cases	O
of	O
interest	O
.	O
if	O
we	O
set	O
ws	O
=	O
0	O
for	O
|s|	O
>	O
1	O
we	O
get	O
a	O
bag-of-characters	B
kernel	O
.	O
this	O
deﬁnes	O
φ	O
(	O
x	O
)	O
to	O
be	O
the	O
number	O
of	O
times	O
each	O
character	O
in	O
a	O
occurs	O
in	O
x.	O
if	O
we	O
require	O
s	O
to	O
be	O
bordered	O
by	O
white-space	O
,	O
we	O
get	O
a	O
bag-of-words	B
kernel	O
,	O
where	O
φ	O
(	O
x	O
)	O
counts	O
how	O
many	O
times	O
each	O
possible	O
word	O
occurs	O
.	O
note	O
that	O
this	O
is	O
a	O
very	O
sparse	O
vector	O
,	O
since	O
most	O
words	O
484	O
chapter	O
14.	O
kernels	O
optimal	O
partial	O
matching	O
matching	O
figure	O
14.1	O
of	O
kristen	O
grauman	O
.	O
illustration	O
of	O
a	O
pyramid	B
match	I
kernel	I
computed	O
from	O
two	O
images	O
.	O
used	O
with	O
kind	O
permission	O
if	O
we	O
only	O
consider	O
strings	O
of	O
a	O
ﬁxed	O
length	O
k	O
,	O
we	O
get	O
the	O
k-spectrum	O
will	O
not	O
be	O
present	O
.	O
kernel	B
.	O
this	O
has	O
been	O
used	O
to	O
classify	O
proteins	O
into	O
scop	O
superfamilies	O
(	O
leslie	O
et	O
al	O
.	O
2003	O
)	O
.	O
for	O
example	O
if	O
k	O
=	O
3	O
,	O
we	O
have	O
φlqe	O
(	O
x	O
)	O
=	O
1	O
and	O
φlqe	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
2	O
for	O
the	O
two	O
strings	O
above	O
.	O
various	O
extensions	O
are	O
possible	O
.	O
for	O
example	O
,	O
we	O
can	O
allow	O
character	O
mismatches	O
(	O
leslie	O
et	O
al	O
.	O
2003	O
)	O
.	O
and	O
we	O
can	O
generalize	B
string	O
kernels	O
to	O
compare	O
trees	O
,	O
as	O
described	O
in	O
(	O
collins	O
and	O
duffy	O
2002	O
)	O
.	O
this	O
is	O
useful	O
for	O
classifying	O
(	O
or	O
ranking	B
)	O
parse	O
trees	O
,	O
evolutionary	O
trees	O
,	O
etc	O
.	O
14.2.7	O
pyramid	O
match	O
kernels	O
in	O
computer	O
vision	O
,	O
it	O
is	O
common	O
to	O
create	O
a	O
bag-of-words	B
representation	O
of	O
an	O
image	O
by	O
computing	O
a	O
feature	O
vector	O
(	O
often	O
using	O
sift	O
(	O
lowe	O
1999	O
)	O
)	O
from	O
a	O
variety	O
of	O
points	O
in	O
the	O
image	O
,	O
commonly	O
chosen	O
by	O
an	O
interest	B
point	I
detector	I
.	O
the	O
feature	O
vectors	O
at	O
the	O
chosen	O
places	O
are	O
then	O
vector-quantized	O
to	O
create	O
a	O
bag	O
of	O
discrete	O
symbols	O
.	O
one	O
way	O
to	O
compare	O
two	O
variable-sized	O
bags	O
of	O
this	O
kind	O
is	O
to	O
use	O
a	O
pyramid	B
match	I
kernel	I
(	O
grauman	O
and	O
darrell	O
2007	O
)	O
.	O
the	O
basic	O
idea	O
is	O
illustrated	O
in	O
figure	O
14.1.	O
each	O
feature	O
set	O
is	O
mapped	O
to	O
a	O
multi-resolution	O
histogram	B
.	O
these	O
are	O
then	O
compared	O
using	O
weighted	O
histogram	O
intersection	O
.	O
it	O
turns	O
out	O
that	O
this	O
provides	O
a	O
good	O
approximation	O
to	O
the	O
similarity	O
measure	O
one	O
would	O
obtain	O
by	O
performing	O
an	O
optimal	O
bipartite	O
match	O
at	O
the	O
ﬁnest	O
spatial	O
resolution	O
,	O
and	O
then	O
summing	O
up	O
pairwise	O
similarities	O
between	O
matched	O
points	O
.	O
however	O
,	O
the	O
histogram	B
method	O
is	O
faster	O
and	O
is	O
more	O
robust	B
to	O
missing	B
and	O
unequal	O
numbers	O
of	O
points	O
.	O
this	O
is	O
a	O
mercer	O
kernel	B
.	O
14.2.	O
kernel	B
functions	O
485	O
14.2.8	O
kernels	O
derived	O
from	O
probabilistic	O
generative	O
models	O
suppose	O
we	O
have	O
a	O
probabilistic	O
generative	O
model	O
of	O
feature	O
vectors	O
,	O
p	O
(	O
x|θ	O
)	O
.	O
then	O
there	O
are	O
several	O
ways	O
we	O
can	O
use	O
this	O
model	O
to	O
deﬁne	O
kernel	B
functions	O
,	O
and	O
thereby	O
make	O
the	O
model	O
suitable	O
for	O
discriminative	B
tasks	O
.	O
we	O
sketch	O
two	O
approaches	O
below	O
.	O
14.2.8.1	O
probability	O
product	O
kernels	O
one	O
approach	O
is	O
to	O
deﬁne	O
a	O
kernel	B
as	O
follows	O
:	O
(	O
cid:28	O
)	O
p	O
(	O
x|xi	O
)	O
ρp	O
(	O
x|xj	O
)	O
ρdx	O
κ	O
(	O
xi	O
,	O
xj	O
)	O
=	O
(	O
14.23	O
)	O
where	O
ρ	O
>	O
0	O
,	O
and	O
p	O
(	O
x|xi	O
)	O
is	O
often	O
approximated	O
by	O
p	O
(	O
x|ˆθ	O
(	O
xi	O
)	O
)	O
,	O
where	O
ˆθ	O
(	O
xi	O
)	O
is	O
a	O
parameter	B
estimate	O
computed	O
using	O
a	O
single	O
data	O
vector	O
.	O
this	O
is	O
called	O
a	O
probability	B
product	I
kernel	I
(	O
jebara	O
et	O
al	O
.	O
2004	O
)	O
.	O
although	O
it	O
seems	O
strange	O
to	O
ﬁt	O
a	O
model	O
to	O
a	O
single	O
data	O
point	O
,	O
it	O
is	O
important	O
to	O
bear	O
in	O
mind	O
that	O
the	O
ﬁtted	O
model	O
is	O
only	O
being	O
used	O
to	O
see	O
how	O
similar	B
two	O
objects	O
are	O
.	O
in	O
particular	O
,	O
if	O
we	O
ﬁt	O
the	O
model	O
to	O
xi	O
and	O
then	O
the	O
model	O
thinks	O
xj	O
is	O
likely	O
,	O
this	O
means	O
that	O
xi	O
and	O
xj	O
are	O
similar	B
.	O
for	O
example	O
,	O
suppose	O
p	O
(	O
x|θ	O
)	O
=n	O
(	O
μ	O
,	O
σ2i	O
)	O
,	O
where	O
σ2	O
is	O
ﬁxed	O
.	O
if	O
ρ	O
=	O
1	O
,	O
and	O
we	O
use	O
ˆμ	O
(	O
xi	O
)	O
=	O
xi	O
and	O
ˆμ	O
(	O
xj	O
)	O
=	O
xj	O
,	O
we	O
ﬁnd	O
(	O
jebara	O
et	O
al	O
.	O
2004	O
,	O
p825	O
)	O
that	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
κ	O
(	O
xi	O
,	O
xj	O
)	O
=	O
1	O
(	O
4πσ2	O
)	O
d/2	O
exp	O
−	O
1	O
4σ2	O
||xi	O
−	O
xj||2	O
which	O
is	O
(	O
up	O
to	O
a	O
constant	O
factor	O
)	O
the	O
rbf	O
kernel	B
.	O
it	O
turns	O
out	O
that	O
one	O
can	O
compute	O
equation	O
14.23	O
for	O
a	O
variety	O
of	O
generative	O
models	O
,	O
including	O
ones	O
with	O
latent	B
variables	O
,	O
such	O
as	O
hmms	O
.	O
this	O
provides	O
one	O
way	O
to	O
deﬁne	O
kernels	O
on	O
variable	O
length	O
sequences	O
.	O
furthermore	O
,	O
this	O
technique	O
works	O
even	O
if	O
the	O
sequences	O
are	O
of	O
real-valued	O
vectors	O
,	O
unlike	O
the	O
string	B
kernel	I
in	O
section	O
14.2.6.	O
see	O
(	O
jebara	O
et	O
al	O
.	O
2004	O
)	O
for	O
further	O
details	O
.	O
14.2.8.2	O
fisher	O
kernels	O
a	O
more	O
efficient	O
way	O
to	O
use	O
generative	O
models	O
to	O
deﬁne	O
kernels	O
is	O
to	O
use	O
a	O
fisher	O
kernel	B
(	O
jaakkola	O
and	O
haussler	O
1998	O
)	O
which	O
is	O
deﬁned	O
as	O
follows	O
:	O
(	O
14.24	O
)	O
(	O
14.25	O
)	O
(	O
14.26	O
)	O
(	O
14.27	O
)	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
)	O
=	O
g	O
(	O
x	O
)	O
t	O
f−1g	O
(	O
x	O
(	O
cid:4	O
)	O
(	O
(	O
g	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
∇θ	O
log	O
p	O
(	O
x|θ	O
)	O
(	O
(	O
f	O
=	O
∇∇	O
log	O
p	O
(	O
x|θ	O
)	O
ˆθ	O
ˆθ	O
where	O
g	O
is	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	O
,	O
or	O
score	B
vector	I
,	O
evaluated	O
at	O
the	O
mle	O
ˆθ	O
and	O
f	O
is	O
the	O
fisher	O
information	B
matrix	O
,	O
which	O
is	O
essentially	O
the	O
hessian	O
:	O
note	O
that	O
ˆθ	O
is	O
a	O
function	O
of	O
all	O
the	O
data	O
,	O
so	O
the	O
similarity	O
of	O
x	O
and	O
x	O
(	O
cid:4	O
)	O
of	O
all	O
the	O
data	O
as	O
well	O
.	O
also	O
,	O
note	O
that	O
we	O
only	O
have	O
to	O
ﬁt	O
one	O
model	O
.	O
is	O
computed	O
in	O
the	O
context	O
the	O
intuition	O
behind	O
the	O
fisher	O
kernel	B
is	O
the	O
following	O
:	O
let	O
g	O
(	O
x	O
)	O
be	O
the	O
direction	O
(	O
in	O
parameter	B
space	O
)	O
in	O
which	O
x	O
would	O
like	O
the	O
parameters	O
to	O
move	O
(	O
from	O
ˆθ	O
)	O
so	O
as	O
to	O
maximize	O
its	O
own	O
486	O
chapter	O
14.	O
kernels	O
poly10	O
rbf	O
prototypes	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
figure	O
14.2	O
(	O
a	O
)	O
xor	B
truth	O
table	O
.	O
(	O
b	O
)	O
fitting	O
a	O
linear	O
logistic	O
regression	B
classiﬁer	O
using	O
degree	B
10	O
polynomial	O
expansion	O
.	O
(	O
c	O
)	O
same	O
model	O
,	O
but	O
using	O
an	O
rbf	O
kernel	B
with	O
centroids	B
speciﬁed	O
by	O
the	O
4	O
black	O
crosses	O
.	O
figure	O
generated	O
by	O
logregxordemo	O
.	O
likelihood	B
;	O
call	O
this	O
the	O
directional	O
gradient	O
.	O
then	O
we	O
say	O
that	O
two	O
vectors	O
x	O
and	O
x	O
(	O
cid:4	O
)	O
are	O
similar	B
if	O
their	O
directional	O
gradients	O
are	O
similar	B
wrt	O
the	O
the	O
geometry	O
encoded	O
by	O
the	O
curvature	O
of	O
the	O
likelihood	B
function	O
(	O
see	O
section	O
7.5.3	O
)	O
.	O
interestingly	O
,	O
it	O
was	O
shown	O
in	O
(	O
saunders	O
et	O
al	O
.	O
2003	O
)	O
that	O
the	O
string	B
kernel	I
of	O
section	O
14.2.6	O
is	O
equivalent	O
to	O
the	O
fisher	O
kernel	B
derived	O
from	O
an	O
l	O
’	O
th	O
order	O
markov	O
chain	O
(	O
see	O
section	O
17.2	O
)	O
.	O
also	O
,	O
it	O
was	O
shown	O
in	O
(	O
elkan	O
2005	O
)	O
that	O
a	O
kernel	B
deﬁned	O
by	O
the	O
inner	O
product	O
of	O
tf-idf	O
vectors	O
(	O
section	O
14.2.2	O
)	O
is	O
approximately	O
equal	O
to	O
the	O
fisher	O
kernel	B
for	O
a	O
certain	O
generative	O
model	O
of	O
text	O
based	O
on	O
the	O
compound	O
dirichlet	O
multinomial	B
model	O
(	O
section	O
3.5.5	O
)	O
.	O
14.3	O
using	O
kernels	O
inside	O
glms	O
in	O
this	O
section	O
,	O
we	O
discuss	O
one	O
simple	O
way	O
to	O
use	O
kernels	O
for	O
classiﬁcation	B
and	O
regression	B
.	O
we	O
will	O
see	O
other	O
approaches	O
later	O
.	O
14.3.1	O
kernel	B
machines	O
we	O
deﬁne	O
a	O
kernel	B
machine	I
to	O
be	O
a	O
glm	O
where	O
the	O
input	O
feature	O
vector	O
has	O
the	O
form	O
φ	O
(	O
x	O
)	O
=	O
[	O
κ	O
(	O
x	O
,	O
μ1	O
)	O
,	O
.	O
.	O
.	O
,	O
κ	O
(	O
x	O
,	O
μk	O
)	O
]	O
(	O
14.28	O
)	O
where	O
μk	O
∈	O
x	O
are	O
a	O
set	O
of	O
k	O
centroids	B
.	O
if	O
κ	O
is	O
an	O
rbf	O
kernel	B
,	O
this	O
is	O
called	O
an	O
rbf	O
network	O
.	O
we	O
discuss	O
ways	O
to	O
choose	O
the	O
μk	O
parameters	O
below	O
.	O
we	O
will	O
call	O
equation	O
14.28	O
a	O
kernelised	B
feature	I
vector	I
.	O
note	O
that	O
in	O
this	O
approach	O
,	O
the	O
kernel	B
need	O
not	O
be	O
a	O
mercer	O
kernel	B
.	O
we	O
can	O
use	O
the	O
kernelized	O
feature	O
vector	O
for	O
logistic	B
regression	I
by	O
deﬁning	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
ber	O
(	O
wt	O
φ	O
(	O
x	O
)	O
)	O
.	O
this	O
provides	O
a	O
simple	O
way	O
to	O
deﬁne	O
a	O
non-linear	O
decision	B
boundary	I
.	O
as	O
an	O
example	O
,	O
consider	O
the	O
data	O
coming	O
from	O
the	O
exclusive	B
or	I
or	O
xor	B
function	O
.	O
this	O
is	O
a	O
binary-	O
valued	O
function	O
of	O
two	O
binary	O
inputs	O
.	O
its	O
truth	O
table	O
is	O
shown	O
in	O
figure	O
14.2	O
(	O
a	O
)	O
.	O
in	O
figure	O
14.2	O
(	O
b	O
)	O
,	O
we	O
have	O
show	O
some	O
data	O
labeled	O
by	O
the	O
xor	B
function	O
,	O
but	O
we	O
have	O
jittered	B
the	O
points	O
to	O
make	O
the	O
picture	O
clearer.1	O
we	O
see	O
we	O
can	O
not	O
separate	O
the	O
data	O
even	O
using	O
a	O
degree	B
10	O
polynomial	O
.	O
1.	O
jittering	O
is	O
a	O
common	O
visualization	O
trick	O
in	O
statistics	O
,	O
wherein	O
points	O
in	O
a	O
plot/display	O
that	O
would	O
otherwise	O
land	O
on	O
top	O
of	O
each	O
other	O
are	O
dispersed	O
with	O
uniform	O
additive	O
noise	O
.	O
14.3.	O
using	O
kernels	O
inside	O
glms	O
487	O
20	O
10	O
0	O
−10	O
0	O
20	O
10	O
0	O
−10	O
0	O
20	O
10	O
0	O
−10	O
0	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0	O
0.04	O
0.03	O
0.02	O
0.01	O
0	O
0	O
x	O
10−3	O
8	O
7.8	O
7.6	O
7.4	O
7.2	O
0	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
2	O
4	O
6	O
8	O
10	O
2	O
4	O
6	O
8	O
10	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
2	O
4	O
6	O
8	O
10	O
figure	O
14.3	O
rbf	O
basis	O
in	O
1d	O
.	O
left	O
column	O
:	O
ﬁtted	O
function	O
.	O
middle	O
column	O
:	O
basis	B
functions	I
evaluated	O
on	O
a	O
grid	O
.	O
right	O
column	O
:	O
design	B
matrix	I
.	O
top	O
to	O
bottom	O
we	O
show	O
different	O
bandwidths	O
:	O
τ	O
=	O
0.1	O
,	O
τ	O
=	O
0.5	O
,	O
τ	O
=	O
50.	O
figure	O
generated	O
by	O
linregrbfdemo	O
.	O
however	O
,	O
using	O
an	O
rbf	O
kernel	B
and	O
just	O
4	O
prototypes	O
easily	O
solves	O
the	O
problem	O
as	O
shown	O
in	O
figure	O
14.2	O
(	O
c	O
)	O
.	O
we	O
can	O
also	O
use	O
the	O
kernelized	O
feature	O
vector	O
inside	O
a	O
linear	B
regression	I
model	O
by	O
deﬁning	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
n	O
(	O
wt	O
φ	O
(	O
x	O
)	O
,	O
σ2	O
)	O
.	O
for	O
example	O
,	O
figure	O
14.3	O
shows	O
a	O
1d	O
data	O
set	O
ﬁt	O
with	O
k	O
=	O
10	O
uniformly	O
spaced	O
rbf	O
prototypes	O
,	O
but	O
with	O
the	O
bandwidth	B
ranging	O
from	O
small	O
to	O
large	O
.	O
small	O
values	O
lead	O
to	O
very	O
wiggly	O
functions	O
,	O
since	O
the	O
predicted	O
function	O
value	O
will	O
only	O
be	O
non-zero	O
for	O
points	O
x	O
that	O
are	O
close	O
to	O
one	O
of	O
the	O
prototypes	O
μk	O
.	O
if	O
the	O
bandwidth	B
is	O
very	O
large	O
,	O
the	O
design	B
matrix	I
reduces	O
to	O
a	O
constant	O
matrix	O
of	O
1	O
’	O
s	O
,	O
since	O
each	O
point	O
is	O
equally	O
close	O
to	O
every	O
prototype	B
;	O
hence	O
the	O
corresponding	O
function	O
is	O
just	O
a	O
straight	O
line	O
.	O
14.3.2	O
l1vms	O
,	O
rvms	O
,	O
and	O
other	O
sparse	O
vector	O
machines	O
the	O
main	O
issue	O
with	O
kernel	B
machines	O
is	O
:	O
how	O
do	O
we	O
choose	O
the	O
centroids	B
μk	O
?	O
if	O
the	O
input	O
is	O
low-dimensional	O
euclidean	O
space	O
,	O
we	O
can	O
uniformly	O
tile	O
the	O
space	O
occupied	O
by	O
the	O
data	O
with	O
prototypes	O
,	O
as	O
we	O
did	O
in	O
figure	O
14.2	O
(	O
c	O
)	O
.	O
however	O
,	O
this	O
approach	O
breaks	O
down	O
in	O
higher	O
numbers	O
if	O
μk	O
∈	O
r	O
d	O
,	O
we	O
can	O
try	O
to	O
perform	O
of	O
dimensions	O
because	O
of	O
the	O
curse	B
of	I
dimensionality	I
.	O
numerical	O
optimization	O
of	O
these	O
parameters	O
(	O
see	O
e.g.	O
,	O
(	O
haykin	O
1998	O
)	O
)	O
,	O
or	O
we	O
can	O
use	O
mcmc	O
inference	B
,	O
(	O
see	O
e.g.	O
,	O
(	O
andrieu	O
et	O
al	O
.	O
2001	O
;	O
kohn	O
et	O
al	O
.	O
2001	O
)	O
)	O
,	O
but	O
the	O
resulting	O
objective	O
function	O
/	O
posterior	O
is	O
highly	O
multimodal	O
.	O
furthermore	O
,	O
these	O
techniques	O
is	O
hard	O
to	O
extend	O
to	O
structured	O
input	O
spaces	O
,	O
where	O
kernels	O
are	O
most	O
useful	O
.	O
another	O
approach	O
is	O
to	O
ﬁnd	O
clusters	B
in	O
the	O
data	O
and	O
then	O
to	O
assign	O
one	O
prototype	B
per	O
cluster	O
488	O
chapter	O
14.	O
kernels	O
center	O
(	O
many	O
clustering	B
algorithms	O
just	O
need	O
a	O
similarity	O
metric	B
as	O
input	O
)	O
.	O
however	O
,	O
the	O
regions	O
of	O
space	O
that	O
have	O
high	O
density	O
are	O
not	O
necessarily	O
the	O
ones	O
where	O
the	O
prototypes	O
are	O
most	O
useful	O
for	O
representing	O
the	O
output	O
,	O
that	O
is	O
,	O
clustering	B
is	O
an	O
unsupervised	O
task	O
that	O
may	O
not	O
yield	O
a	O
representation	O
that	O
is	O
useful	O
for	O
prediction	O
.	O
furthermore	O
,	O
there	O
is	O
the	O
need	O
to	O
pick	O
the	O
number	O
of	O
clusters	B
.	O
a	O
simpler	O
approach	O
is	O
to	O
make	O
each	O
example	O
xi	O
be	O
a	O
prototype	B
,	O
so	O
we	O
get	O
φ	O
(	O
x	O
)	O
=	O
[	O
κ	O
(	O
x	O
,	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
κ	O
(	O
x	O
,	O
xn	O
)	O
]	O
(	O
14.29	O
)	O
now	O
we	O
see	O
d	O
=	O
n	O
,	O
so	O
we	O
have	O
as	O
many	O
parameters	O
as	O
data	O
points	O
.	O
however	O
,	O
we	O
can	O
use	O
any	O
of	O
the	O
sparsity-promoting	O
priors	O
for	O
w	O
discussed	O
in	O
chapter	O
13	O
to	O
efficiently	O
select	O
a	O
subset	O
of	O
the	O
training	O
exemplars	O
.	O
we	O
call	O
this	O
a	O
sparse	B
vector	I
machine	I
.	O
the	O
most	O
natural	O
choice	O
is	O
to	O
use	O
(	O
cid:6	O
)	O
1	O
regularization	B
(	O
krishnapuram	O
et	O
al	O
.	O
2005	O
)	O
.	O
(	O
note	O
that	O
in	O
the	O
multi-class	O
case	O
,	O
it	O
is	O
necessary	O
to	O
use	O
group	B
lasso	I
,	O
since	O
each	O
exemplar	O
is	O
associated	O
with	O
c	O
weights	O
,	O
one	O
per	O
class	O
.	O
)	O
we	O
call	O
this	O
l1vm	O
,	O
which	O
stands	O
for	O
“	O
(	O
cid:6	O
)	O
1-regularized	O
vector	O
machine	O
”	O
.	O
by	O
analogy	O
,	O
we	O
deﬁne	O
the	O
use	O
of	O
an	O
(	O
cid:6	O
)	O
2	O
regularizer	O
to	O
be	O
a	O
l2vm	O
or	O
“	O
(	O
cid:6	O
)	O
2-regularized	O
vector	O
machine	O
”	O
;	O
this	O
of	O
course	O
will	O
not	O
be	O
sparse	B
.	O
we	O
can	O
get	O
even	O
greater	O
sparsity	B
by	O
using	O
ard/sbl	O
,	O
resulting	O
in	O
a	O
method	O
called	O
the	O
rele-	O
vance	O
vector	O
machine	O
or	O
rvm	O
(	O
tipping	O
2001	O
)	O
.	O
one	O
can	O
ﬁt	O
this	O
model	O
using	O
generic	O
ard/sbl	O
algorithms	O
,	O
although	O
in	O
practice	O
the	O
most	O
common	O
method	O
is	O
the	O
greedy	O
algorithm	O
in	O
(	O
tipping	O
and	O
faul	O
2003	O
)	O
(	O
this	O
is	O
the	O
algorithm	O
implemented	O
in	O
mike	O
tipping	O
’	O
s	O
code	O
,	O
which	O
is	O
bundled	O
with	O
pmtk	O
)	O
.	O
another	O
very	O
popular	O
approach	O
to	O
creating	O
a	O
sparse	B
kernel	I
machine	I
is	O
to	O
use	O
a	O
support	B
vector	I
machine	I
or	O
svm	O
.	O
this	O
will	O
be	O
discussed	O
in	O
detail	O
in	O
section	O
14.5.	O
rather	O
than	O
using	O
a	O
sparsity-promoting	B
prior	I
,	O
it	O
essentially	O
modiﬁes	O
the	O
likelihood	B
term	O
,	O
which	O
is	O
rather	O
unnatural	O
from	O
a	O
bayesian	O
point	O
of	O
view	O
.	O
nevertheless	O
,	O
the	O
effect	O
is	O
similar	B
,	O
as	O
we	O
will	O
see	O
.	O
in	O
figure	O
14.4	O
,	O
we	O
compare	O
l2vm	O
,	O
l1vm	O
,	O
rvm	O
and	O
an	O
svm	O
using	O
the	O
same	O
rbf	O
kernel	B
on	O
a	O
binary	B
classiﬁcation	I
problem	O
in	O
2d	O
.	O
for	O
simplicity	O
,	O
λ	O
was	O
chosen	O
by	O
hand	O
for	O
l2vm	O
and	O
l1vm	O
;	O
for	O
rvms	O
,	O
the	O
parameters	O
are	O
estimated	O
using	O
empirical	O
bayes	O
;	O
and	O
for	O
the	O
svm	O
,	O
we	O
use	O
cv	O
to	O
pick	O
c	O
=	O
1/λ	O
,	O
since	O
svm	O
performance	O
is	O
very	O
sensitive	O
to	O
this	O
parameter	B
(	O
see	O
section	O
14.5.3	O
)	O
.	O
we	O
see	O
that	O
all	O
the	O
methods	O
give	O
similar	B
performance	O
.	O
however	O
,	O
rvm	O
is	O
the	O
sparsest	O
(	O
and	O
hence	O
fastest	O
at	O
test	O
time	O
)	O
,	O
then	O
l1vm	O
,	O
and	O
then	O
svm	O
.	O
rvm	O
is	O
also	O
the	O
fastest	O
to	O
train	O
,	O
since	O
cv	O
for	O
an	O
svm	O
is	O
slow	O
.	O
(	O
this	O
is	O
despite	O
the	O
fact	O
that	O
the	O
rvm	O
code	O
is	O
in	O
matlab	O
and	O
the	O
svm	O
code	O
is	O
in	O
c.	O
)	O
this	O
result	O
is	O
fairly	O
typical	O
.	O
in	O
figure	O
14.5	O
,	O
we	O
compare	O
l2vm	O
,	O
l1vm	O
,	O
rvm	O
and	O
an	O
svm	O
using	O
an	O
rbf	O
kernel	B
on	O
a	O
1d	O
regression	B
problem	O
.	O
again	O
,	O
we	O
see	O
that	O
predictions	O
are	O
quite	O
similar	B
,	O
but	O
rvm	O
is	O
the	O
sparsest	O
,	O
then	O
l2vm	O
,	O
then	O
svm	O
.	O
this	O
is	O
further	O
illustrated	O
in	O
figure	O
14.6	O
.	O
14.4	O
the	O
kernel	B
trick	I
rather	O
than	O
deﬁning	O
our	O
feature	O
vector	O
in	O
terms	O
of	O
kernels	O
,	O
φ	O
(	O
x	O
)	O
=	O
[	O
κ	O
(	O
x	O
,	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
κ	O
(	O
x	O
,	O
xn	O
)	O
]	O
,	O
we	O
can	O
instead	O
work	O
with	O
the	O
original	O
feature	O
vectors	O
x	O
,	O
but	O
modify	O
the	O
algorithm	O
so	O
that	O
it	O
replaces	O
all	O
inner	O
products	O
of	O
the	O
form	O
(	O
cid:24	O
)	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
(	O
cid:25	O
)	O
with	O
a	O
call	O
to	O
the	O
kernel	B
function	I
,	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
.	O
this	O
is	O
called	O
the	O
kernel	B
trick	I
.	O
it	O
turns	O
out	O
that	O
many	O
algorithms	O
can	O
be	O
kernelized	O
in	O
this	O
way	O
.	O
we	O
give	O
some	O
examples	O
below	O
.	O
note	O
that	O
we	O
require	O
that	O
the	O
kernel	B
be	O
a	O
mercer	O
kernel	B
for	O
this	O
trick	O
to	O
work	O
.	O
14.4.	O
the	O
kernel	B
trick	I
489	O
logregl2	O
,	O
nerr=174	O
logregl1	O
,	O
nerr=169	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−2	O
−1	O
0	O
(	O
a	O
)	O
rvm	O
,	O
nerr=173	O
1	O
2	O
3	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−2	O
−1	O
0	O
(	O
b	O
)	O
svm	O
,	O
nerr=173	O
1	O
2	O
3	O
−2	O
−1	O
0	O
(	O
c	O
)	O
1	O
2	O
3	O
−2	O
−1	O
1	O
2	O
3	O
0	O
(	O
d	O
)	O
figure	O
14.4	O
example	O
of	O
non-linear	O
binary	B
classiﬁcation	I
using	O
an	O
rbf	O
kernel	B
with	O
bandwidth	B
σ	O
=	O
0.3	O
.	O
(	O
a	O
)	O
l2vm	O
with	O
λ	O
=	O
5	O
.	O
(	O
c	O
)	O
rvm	O
.	O
(	O
d	O
)	O
svm	O
with	O
c	O
=	O
1/λ	O
chosen	O
by	O
cross	B
validation	I
.	O
black	O
circles	O
denote	O
the	O
support	B
vectors	I
.	O
figure	O
generated	O
by	O
kernelbinaryclassifdemo	O
.	O
(	O
b	O
)	O
l1vm	O
with	O
λ	O
=	O
1	O
.	O
14.4.1	O
kernelized	O
nearest	B
neighbor	I
classiﬁcation	O
recall	B
that	O
in	O
a	O
1nn	O
classiﬁer	O
(	O
section	O
1.4.2	O
)	O
,	O
we	O
just	O
need	O
to	O
compute	O
the	O
euclidean	O
distance	O
of	O
a	O
test	O
vector	O
to	O
all	O
the	O
training	O
points	O
,	O
ﬁnd	O
the	O
closest	O
one	O
,	O
and	O
look	O
up	O
its	O
label	B
.	O
this	O
can	O
be	O
kernelized	O
by	O
observing	O
that	O
||xi	O
−	O
xi	O
(	O
cid:2	O
)	O
||2	O
2	O
=	O
(	O
cid:24	O
)	O
xi	O
,	O
xi	O
(	O
cid:25	O
)	O
+	O
(	O
cid:24	O
)	O
xi	O
(	O
cid:2	O
)	O
,	O
xi	O
(	O
cid:2	O
)	O
(	O
cid:25	O
)	O
−2	O
(	O
cid:24	O
)	O
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
(	O
cid:25	O
)	O
(	O
14.30	O
)	O
this	O
allows	O
us	O
to	O
apply	O
the	O
nearest	B
neighbor	I
classiﬁer	O
to	O
structured	O
data	O
objects	O
.	O
14.4.2	O
kernelized	O
k-medoids	O
clustering	B
k-means	O
clustering	B
(	O
section	O
11.4.2.5	O
)	O
uses	O
euclidean	O
distance	O
to	O
measure	O
dissimilarity	O
,	O
which	O
is	O
not	O
always	O
appropriate	O
for	O
structured	O
objects	O
.	O
we	O
now	O
describe	O
how	O
to	O
develop	O
a	O
kernelized	O
490	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−2	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−2	O
linregl2	O
linregl1	O
chapter	O
14.	O
kernels	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−2	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−2	O
−1.5	O
−1	O
−0.5	O
0	O
(	O
b	O
)	O
svm	O
0.5	O
1	O
1.5	O
2	O
−1.5	O
−1	O
−0.5	O
0	O
(	O
d	O
)	O
0.5	O
1	O
1.5	O
2	O
−1.5	O
−1	O
−0.5	O
0	O
(	O
a	O
)	O
rvm	O
0.5	O
1	O
1.5	O
2	O
−1.5	O
−1	O
−0.5	O
0	O
(	O
c	O
)	O
0.5	O
1	O
1.5	O
2	O
figure	O
14.5	O
example	O
of	O
kernel	B
based	O
regression	B
on	O
the	O
noisy	O
sinc	O
function	O
using	O
an	O
rbf	O
kernel	B
with	O
bandwidth	B
σ	O
=	O
0.3	O
.	O
(	O
c	O
)	O
rvm	O
.	O
(	O
d	O
)	O
svm	O
regression	B
with	O
c	O
=	O
1/λ	O
chosen	O
by	O
cross	B
validation	I
,	O
and	O
	O
=	O
0.1	O
(	O
the	O
default	O
for	O
svmlight	O
)	O
.	O
red	O
circles	O
denote	O
the	O
retained	O
training	O
exemplars	O
.	O
figure	O
generated	O
by	O
kernelregrdemo	O
.	O
(	O
a	O
)	O
l2vm	O
with	O
λ	O
=	O
0.5	O
.	O
(	O
b	O
)	O
l1vm	O
with	O
λ	O
=	O
0.5.	O
version	O
of	O
the	O
algorithm	O
.	O
the	O
ﬁrst	O
step	O
is	O
to	O
replace	O
the	O
k-means	O
algorithm	O
with	O
the	O
k-medoids	O
algorothm	O
.	O
this	O
is	O
similar	B
to	O
k-means	O
,	O
but	O
instead	O
of	O
representing	O
each	O
cluster	O
’	O
s	O
centroid	B
by	O
the	O
mean	B
of	O
all	O
data	O
vectors	O
assigned	O
to	O
this	O
cluster	O
,	O
we	O
make	O
each	O
centroid	B
be	O
one	O
of	O
the	O
data	O
vectors	O
themselves	O
.	O
thus	O
we	O
always	O
deal	O
with	O
integer	O
indexes	O
,	O
rather	O
than	O
data	O
objects	O
.	O
we	O
assign	O
objects	O
to	O
their	O
closest	O
centroids	B
as	O
before	O
.	O
when	O
we	O
update	O
the	O
centroids	B
,	O
we	O
look	O
at	O
each	O
object	O
that	O
belongs	O
to	O
the	O
cluster	O
,	O
and	O
measure	O
the	O
sum	O
of	O
its	O
distances	O
to	O
all	O
the	O
others	O
in	O
the	O
same	O
cluster	O
;	O
we	O
then	O
pick	O
the	O
one	O
which	O
has	O
the	O
smallest	O
such	O
sum	O
:	O
mk	O
=	O
argmin	O
i	O
:	O
zi=k	O
(	O
cid:2	O
)	O
i	O
(	O
cid:2	O
)	O
:	O
zi	O
(	O
cid:2	O
)	O
=k	O
d	O
(	O
i	O
,	O
i	O
(	O
cid:4	O
)	O
)	O
(	O
14.31	O
)	O
14.4.	O
the	O
kernel	B
trick	I
491	O
weights	O
for	O
linregl2	O
weights	O
for	O
linregl1	O
0.06	O
0.04	O
0.02	O
0	O
−0.02	O
−0.04	O
−0.06	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
0.12	O
0.1	O
0.08	O
0.06	O
0.04	O
0.02	O
0	O
−0.02	O
−0.04	O
−0.06	O
0	O
20	O
40	O
60	O
80	O
100	O
0	O
20	O
40	O
60	O
80	O
100	O
(	O
a	O
)	O
weights	O
for	O
rvm	O
(	O
b	O
)	O
weights	O
for	O
svm	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
0	O
20	O
40	O
60	O
80	O
100	O
0	O
20	O
40	O
60	O
80	O
100	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
14.6	O
coefficient	O
vectors	O
of	O
length	O
n	O
=	O
100	O
for	O
the	O
models	O
in	O
figure	O
14.6.	O
figure	O
generated	O
by	O
kernelregrdemo	O
.	O
where	O
d	O
(	O
i	O
,	O
i	O
(	O
cid:4	O
)	O
)	O
(	O
cid:2	O
)	O
||xi	O
−	O
xi	O
(	O
cid:2	O
)	O
||2	O
2	O
(	O
14.32	O
)	O
this	O
takes	O
o	O
(	O
n2	O
k	O
)	O
work	O
per	O
cluster	O
,	O
whereas	O
k-means	O
takes	O
o	O
(	O
nkd	O
)	O
to	O
update	O
each	O
cluster	O
.	O
the	O
pseudo-code	O
is	O
given	O
in	O
algorithm	O
5.	O
this	O
method	O
can	O
be	O
modiﬁed	O
to	O
derive	O
a	O
classiﬁer	O
,	O
by	O
computing	O
the	O
nearest	O
medoid	O
for	O
each	O
class	O
.	O
this	O
is	O
known	O
as	O
nearest	B
medoid	I
classiﬁcation	I
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p671	O
)	O
.	O
d	O
(	O
i	O
,	O
i	O
(	O
cid:4	O
)	O
this	O
algorithm	O
can	O
be	O
kernelized	O
by	O
using	O
equation	O
14.30	O
to	O
replace	O
the	O
distance	O
computation	O
,	O
)	O
.	O
492	O
chapter	O
14.	O
kernels	O
algorithm	O
14.1	O
:	O
k-medoids	O
algorithm	O
1	O
initialize	O
m1	O
:	O
k	O
as	O
a	O
random	O
subset	O
of	O
size	O
k	O
from	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
;	O
2	O
repeat	O
3	O
zi	O
=	O
argmink	O
d	O
(	O
i	O
,	O
mk	O
)	O
for	O
i	O
=	O
1	O
:	O
n	O
;	O
mk	O
←	O
argmini	O
:	O
zi=k	O
i	O
(	O
cid:2	O
)	O
:	O
zi	O
(	O
cid:2	O
)	O
=k	O
d	O
(	O
i	O
,	O
i	O
(	O
cid:4	O
)	O
)	O
for	O
k	O
=	O
1	O
:	O
k	O
;	O
(	O
cid:10	O
)	O
4	O
5	O
until	O
converged	O
;	O
14.4.3	O
kernelized	O
ridge	B
regression	I
applying	O
the	O
kernel	B
trick	I
to	O
distance-based	O
methods	O
was	O
straightforward	O
.	O
it	O
is	O
not	O
so	O
obvious	O
how	O
to	O
apply	O
it	O
to	O
parametric	O
models	O
such	O
as	O
ridge	B
regression	I
.	O
however	O
,	O
it	O
can	O
be	O
done	O
,	O
as	O
we	O
now	O
explain	O
.	O
this	O
will	O
serve	O
as	O
a	O
good	O
“	O
warm	O
up	O
”	O
for	O
studying	O
svms	O
.	O
14.4.3.1	O
the	O
primal	O
problem	O
let	O
x	O
∈	O
r	O
want	O
to	O
minimize	O
d	O
be	O
some	O
feature	O
vector	O
,	O
and	O
x	O
be	O
the	O
corresponding	O
n	O
×	O
d	O
design	B
matrix	I
.	O
we	O
j	O
(	O
w	O
)	O
=	O
(	O
y	O
−	O
xw	O
)	O
t	O
(	O
y	O
−	O
xw	O
)	O
+λ||w	O
||2	O
the	O
optimal	O
solution	O
is	O
given	O
by	O
w	O
=	O
(	O
xt	O
x	O
+	O
λid	O
)	O
−1xt	O
y	O
=	O
(	O
(	O
cid:2	O
)	O
i	O
xixt	O
i	O
+	O
λid	O
)	O
−1xt	O
y	O
14.4.3.2	O
the	O
dual	O
problem	O
(	O
14.33	O
)	O
(	O
14.34	O
)	O
−1y	O
equation	O
14.34	O
is	O
not	O
yet	O
in	O
the	O
form	O
of	O
inner	O
products	O
.	O
however	O
,	O
using	O
the	O
matrix	B
inversion	I
lemma	I
(	O
equation	O
4.107	O
)	O
we	O
rewrite	O
the	O
ridge	O
estimate	O
as	O
follows	O
w	O
=	O
xt	O
(	O
xxt	O
+	O
λin	O
)	O
(	O
14.35	O
)	O
which	O
takes	O
o	O
(	O
n	O
3	O
+	O
n	O
2d	O
)	O
time	O
to	O
compute	O
.	O
this	O
can	O
be	O
advantageous	O
if	O
d	O
is	O
large	O
.	O
further-	O
more	O
,	O
we	O
see	O
that	O
we	O
can	O
partially	O
kernelize	O
this	O
,	O
by	O
replacing	O
xxt	O
with	O
the	O
gram	O
matrix	O
k.	O
but	O
what	O
about	O
the	O
leading	O
xt	O
term	O
?	O
let	O
us	O
deﬁne	O
the	O
following	O
dual	B
variables	I
:	O
−1y	O
α	O
(	O
cid:2	O
)	O
(	O
k	O
+	O
λin	O
)	O
n	O
(	O
cid:2	O
)	O
then	O
we	O
can	O
rewrite	O
the	O
primal	B
variables	I
as	O
follows	O
w	O
=	O
xt	O
α	O
=	O
αixi	O
i=1	O
(	O
14.36	O
)	O
(	O
14.37	O
)	O
this	O
tells	O
us	O
that	O
the	O
solution	O
vector	O
is	O
just	O
a	O
linear	O
sum	O
of	O
the	O
n	O
training	O
vectors	O
.	O
when	O
we	O
plug	O
this	O
in	O
at	O
test	O
time	O
to	O
compute	O
the	O
predictive	B
mean	O
,	O
we	O
get	O
ˆf	O
(	O
x	O
)	O
=	O
wt	O
x	O
=	O
αixt	O
i	O
x	O
=	O
αiκ	O
(	O
x	O
,	O
xi	O
)	O
(	O
14.38	O
)	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
i=1	O
14.4.	O
the	O
kernel	B
trick	I
493	O
eigenvalue=20.936	O
eigenvalue=22.558	O
1.5	O
1	O
0.5	O
0	O
1.5	O
1	O
0.5	O
0	O
eigenvalue=3.988	O
eigenvalue=4.648	O
1.5	O
1	O
0.5	O
0	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
0	O
−0.5	O
−1	O
1	O
0	O
−0.5	O
−1	O
1	O
0	O
−0.5	O
−1	O
1	O
0	O
1	O
eigenvalue=2.760	O
eigenvalue=2.956	O
eigenvalue=3.372	O
1.5	O
1	O
0.5	O
0	O
1.5	O
1	O
0.5	O
0	O
1.5	O
1	O
0.5	O
0	O
1.5	O
1	O
0.5	O
0	O
eigenvalue=2.211	O
−0.5	O
−1	O
0	O
−0.5	O
−1	O
1	O
0	O
−0.5	O
−1	O
1	O
0	O
−0.5	O
−1	O
1	O
0	O
1	O
figure	O
14.7	O
visualization	O
of	O
the	O
ﬁrst	O
8	O
kernel	B
principal	O
component	O
basis	B
functions	I
derived	O
from	O
some	O
2d	O
data	O
.	O
we	O
use	O
an	O
rbf	O
kernel	B
with	O
σ2	O
=	O
0.1.	O
figure	O
generated	O
by	O
kpcascholkopf	O
,	O
written	O
by	O
bernhard	O
scholkopf	O
.	O
so	O
we	O
have	O
succesfully	O
kernelized	O
ridge	B
regression	I
by	O
changing	O
from	O
primal	O
to	O
dual	B
variables	I
.	O
this	O
technique	O
can	O
be	O
applied	O
to	O
many	O
other	O
linear	O
models	O
,	O
such	O
as	O
logistic	B
regression	I
.	O
14.4.3.3	O
computational	O
cost	O
the	O
cost	O
of	O
computing	O
the	O
dual	B
variables	I
α	O
is	O
o	O
(	O
n	O
3	O
)	O
,	O
whereas	O
the	O
cost	O
of	O
computing	O
the	O
primal	B
variables	I
w	O
is	O
o	O
(	O
d3	O
)	O
.	O
hence	O
the	O
kernel	B
method	O
can	O
be	O
useful	O
in	O
high	O
dimensional	O
settings	O
,	O
even	O
if	O
we	O
only	O
use	O
a	O
linear	B
kernel	I
(	O
c.f.	O
,	O
the	O
svd	O
trick	O
in	O
equation	O
7.44	O
)	O
.	O
however	O
,	O
prediction	O
using	O
the	O
dual	B
variables	I
takes	O
o	O
(	O
n	O
d	O
)	O
time	O
,	O
while	O
prediction	O
using	O
the	O
primal	B
variables	I
only	O
takes	O
o	O
(	O
d	O
)	O
time	O
.	O
we	O
can	O
speedup	O
prediction	O
by	O
making	O
α	O
sparse	B
,	O
as	O
we	O
discuss	O
in	O
section	O
14.5	O
.	O
14.4.4	O
kernel	B
pca	O
in	O
section	O
12.2	O
,	O
we	O
saw	O
how	O
we	O
could	O
compute	O
a	O
low-dimensional	O
linear	O
embedding	O
of	O
some	O
data	O
using	O
pca	O
.	O
this	O
required	O
ﬁnding	O
the	O
eigenvectors	O
of	O
the	O
sample	O
covariance	O
matrix	O
s	O
=	O
chapter	O
14.	O
kernels	O
494	O
(	O
cid:10	O
)	O
n	O
i=1	O
xixt	O
1	O
i	O
=	O
(	O
1/n	O
)	O
xt	O
x.	O
however	O
,	O
we	O
can	O
also	O
compute	O
pca	O
by	O
ﬁnding	O
the	O
eigenvectors	O
n	O
of	O
the	O
inner	O
product	O
matrix	O
xxt	O
,	O
as	O
we	O
show	O
below	O
.	O
this	O
will	O
allow	O
us	O
to	O
produce	O
a	O
nonlinear	O
embedding	B
,	O
using	O
the	O
kernel	B
trick	I
,	O
a	O
method	O
known	O
as	O
kernel	B
pca	O
(	O
schoelkopf	O
et	O
al	O
.	O
1998	O
)	O
.	O
first	O
,	O
let	O
u	O
be	O
an	O
orthogonal	O
matrix	O
containing	O
the	O
eigenvectors	O
of	O
xxt	O
with	O
corresponding	O
eigenvalues	O
in	O
λ.	O
by	O
deﬁnition	O
we	O
have	O
(	O
xxt	O
)	O
u	O
=	O
uλ	O
.	O
pre-multiplying	O
by	O
xt	O
gives	O
(	O
xt	O
x	O
)	O
(	O
xt	O
u	O
)	O
=	O
(	O
xt	O
u	O
)	O
λ	O
(	O
14.39	O
)	O
from	O
which	O
we	O
see	O
that	O
the	O
eigenvectors	O
of	O
xt	O
x	O
(	O
and	O
hence	O
of	O
s	O
)	O
are	O
v	O
=	O
xt	O
u	O
,	O
with	O
eigen-	O
values	O
given	O
by	O
λ	O
as	O
before	O
.	O
however	O
,	O
these	O
eigenvectors	O
are	O
not	O
normalized	O
,	O
since	O
||vj||2	O
=	O
j	O
uj	O
=	O
λj	O
.	O
so	O
the	O
normalized	O
eigenvectors	O
are	O
given	O
by	O
vpca	O
=	O
xt	O
uλ−	O
1	O
ut	O
j	O
xxt	O
uj	O
=	O
λjut	O
2	O
.	O
this	O
is	O
a	O
useful	O
trick	O
for	O
regular	B
pca	O
if	O
d	O
>	O
n	O
,	O
since	O
xt	O
x	O
has	O
size	O
d	O
×	O
d	O
,	O
whereas	O
xxt	O
has	O
size	O
n	O
×	O
n	O
.	O
it	O
will	O
also	O
allow	O
us	O
to	O
use	O
the	O
kernel	B
trick	I
,	O
as	O
we	O
now	O
show	O
.	O
now	O
let	O
k	O
=	O
xxt	O
be	O
the	O
gram	O
matrix	O
.	O
recall	B
from	O
mercer	O
’	O
s	O
theorem	O
that	O
the	O
use	O
of	O
a	O
kernel	B
implies	O
some	O
underlying	O
feature	O
space	O
,	O
so	O
we	O
are	O
implicitly	O
replacing	O
xi	O
with	O
φ	O
(	O
xi	O
)	O
=	O
φi	O
.	O
let	O
φ	O
be	O
the	O
corresponding	O
(	O
notional	O
)	O
design	B
matrix	I
,	O
and	O
sφ	O
=	O
1	O
i	O
be	O
the	O
corresponding	O
n	O
(	O
notional	O
)	O
covariance	B
matrix	I
in	O
feature	O
space	O
.	O
the	O
eigenvectors	O
are	O
given	O
by	O
vkpca	O
=	O
φt	O
uλ−	O
1	O
2	O
,	O
where	O
u	O
and	O
λ	O
contain	O
the	O
eigenvectors	O
and	O
eigenvalues	O
of	O
k.	O
of	O
course	O
,	O
we	O
can	O
’	O
t	O
actually	O
compute	O
vkpca	O
,	O
since	O
φi	O
is	O
potentially	O
inﬁnite	O
dimensional	O
.	O
however	O
,	O
we	O
can	O
compute	O
the	O
projection	B
of	O
a	O
test	O
vector	O
x∗	O
onto	O
the	O
feature	O
space	O
as	O
follows	O
:	O
i	O
φiφt	O
(	O
cid:10	O
)	O
φt∗	O
vkpca	O
=	O
φt∗	O
φuλ−	O
1	O
2	O
=	O
kt∗	O
uλ−	O
1	O
where	O
k∗	O
=	O
[	O
κ	O
(	O
x∗	O
,	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
κ	O
(	O
x∗	O
,	O
xn	O
)	O
]	O
.	O
2	O
(	O
14.40	O
)	O
there	O
is	O
one	O
ﬁnal	O
detail	O
to	O
worry	O
about	O
.	O
so	O
far	O
,	O
we	O
have	O
assumed	O
the	O
projected	O
data	O
has	O
zero	O
mean	O
,	O
which	O
is	O
not	O
the	O
case	O
in	O
general	O
.	O
we	O
can	O
not	O
simply	O
subtract	O
off	O
the	O
mean	B
in	O
feature	O
space	O
.	O
however	O
,	O
there	O
is	O
a	O
trick	O
we	O
can	O
use	O
.	O
deﬁne	O
the	O
centered	O
feature	O
vector	O
as	O
˜φi	O
=	O
φ	O
(	O
xi	O
)	O
−	O
1	O
j=1	O
φ	O
(	O
xj	O
)	O
.	O
the	O
gram	O
matrix	O
of	O
the	O
centered	O
feature	O
vectors	O
is	O
given	O
by	O
(	O
cid:10	O
)	O
n	O
n	O
˜kij	O
=	O
˜φ	O
t	O
i	O
˜φj	O
i	O
φj	O
−	O
1	O
n	O
n	O
(	O
cid:2	O
)	O
k=1	O
=	O
φt	O
n	O
(	O
cid:2	O
)	O
k=1	O
i	O
φk	O
−	O
1	O
φt	O
n	O
n	O
(	O
cid:2	O
)	O
φt	O
j	O
φk	O
+	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
m	O
(	O
cid:2	O
)	O
k=1	O
l=1	O
1	O
n	O
2	O
(	O
14.41	O
)	O
(	O
14.42	O
)	O
κ	O
(	O
xk	O
,	O
xl	O
)	O
(	O
14.43	O
)	O
m	O
(	O
cid:2	O
)	O
φt	O
k	O
φl	O
n	O
(	O
cid:2	O
)	O
=	O
κ	O
(	O
xi	O
,	O
xj	O
)	O
−	O
1	O
n	O
κ	O
(	O
xi	O
,	O
xk	O
)	O
−	O
1	O
n	O
κ	O
(	O
xj	O
,	O
xk	O
)	O
+	O
1	O
n	O
2	O
k=1	O
k=1	O
k=1	O
l=1	O
this	O
can	O
be	O
expressed	O
in	O
matrix	O
notation	O
as	O
follows	O
:	O
n	O
1n	O
1t	O
n	O
.	O
˜k	O
=	O
hkh	O
where	O
h	O
(	O
cid:2	O
)	O
i	O
−	O
1	O
is	O
the	O
centering	B
matrix	I
.	O
we	O
can	O
convert	O
all	O
this	O
algebra	O
into	O
the	O
pseudocode	O
shown	O
in	O
algorithm	O
9.	O
whereas	O
linear	O
pca	O
is	O
limited	O
to	O
using	O
l	O
≤	O
d	O
components	O
,	O
in	O
kpca	O
,	O
we	O
can	O
use	O
up	O
to	O
n	O
components	O
,	O
since	O
the	O
rank	O
of	O
φ	O
is	O
n	O
×d∗	O
is	O
the	O
(	O
potentially	O
inﬁnite	O
)	O
dimensionality	O
of	O
embedded	O
feature	O
vectors	O
.	O
figure	O
14.7	O
gives	O
an	O
example	O
of	O
the	O
method	O
applied	O
to	O
some	O
d	O
=	O
2	O
dimensional	O
data	O
using	O
an	O
rbf	O
kernel	B
.	O
we	O
project	O
points	O
in	O
the	O
unit	O
grid	O
onto	O
the	O
ﬁrst	O
,	O
where	O
d∗	O
(	O
14.44	O
)	O
14.4.	O
the	O
kernel	B
trick	I
495	O
n	O
/n	O
;	O
algorithm	O
14.2	O
:	O
kernel	B
pca	O
1	O
input	O
:	O
k	O
of	O
size	O
n	O
×	O
n	O
,	O
k∗	O
of	O
size	O
n∗	O
×	O
n	O
,	O
num	O
.	O
latent	B
dimensions	O
l	O
;	O
2	O
o	O
=	O
1n	O
1t	O
3	O
˜k	O
=	O
k	O
−	O
ok	O
−	O
ko	O
+	O
oko	O
;	O
4	O
[	O
u	O
,	O
λ	O
]	O
=eig	O
(	O
˜k	O
)	O
;	O
√	O
5	O
for	O
i	O
=	O
1	O
:	O
n	O
do	O
λi	O
vi	O
=	O
ui/	O
6	O
7	O
o∗	O
=	O
1n∗	O
1t	O
n	O
/n	O
;	O
8	O
˜k∗	O
=	O
k∗	O
−	O
o∗k∗	O
−	O
k∗o∗	O
+	O
o∗k∗o∗	O
;	O
9	O
z	O
=	O
˜k∗v	O
(	O
:	O
,	O
1	O
:	O
l	O
)	O
pca	O
kpca	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−0.6	O
−0.8	O
−0.6	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−0.6	O
−0.4	O
−0.2	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
−0.8	O
−0.8	O
−0.6	O
−0.4	O
−0.2	O
(	O
a	O
)	O
0.2	O
0.4	O
0.6	O
0.8	O
0	O
(	O
b	O
)	O
figure	O
14.8	O
2d	O
visualization	O
of	O
some	O
2d	O
data	O
.	O
generated	O
by	O
kpcademo2	O
,	O
based	O
on	O
code	O
by	O
l.j.p	O
.	O
van	O
der	O
maaten	O
.	O
(	O
a	O
)	O
pca	O
projection	B
.	O
(	O
b	O
)	O
kernel	B
pca	O
projection	B
.	O
figure	O
8	O
components	O
and	O
visualize	O
the	O
corresponding	O
surfaces	O
using	O
a	O
contour	O
plot	O
.	O
we	O
see	O
that	O
the	O
ﬁrst	O
two	O
component	O
separate	O
the	O
three	O
clusters	B
,	O
and	O
following	O
components	O
split	O
the	O
clusters	B
.	O
although	O
the	O
features	B
learned	O
by	O
kpca	O
can	O
be	O
useful	O
for	O
classiﬁcation	B
(	O
schoelkopf	O
et	O
al	O
.	O
1998	O
)	O
,	O
they	O
are	O
not	O
necessarily	O
so	O
useful	O
for	O
data	O
visualization	O
.	O
for	O
example	O
,	O
figure	O
14.8	O
shows	O
the	O
projection	B
of	O
the	O
data	O
from	O
figure	O
14.7	O
onto	O
the	O
ﬁrst	O
2	O
principal	O
bases	O
computed	O
using	O
pca	O
and	O
kpca	O
.	O
obviously	O
pca	O
perfectly	O
represents	O
the	O
data	O
.	O
kpca	O
represents	O
each	O
cluster	O
by	O
a	O
different	O
line	O
.	O
of	O
course	O
,	O
there	O
is	O
no	O
need	O
to	O
project	O
2d	O
data	O
back	O
into	O
2d	O
.	O
so	O
let	O
us	O
consider	O
a	O
different	O
data	O
set	O
.	O
we	O
will	O
use	O
a	O
12	O
dimensional	O
data	O
set	O
representing	O
the	O
three	O
known	O
phases	O
of	O
ﬂow	O
in	O
an	O
oil	O
pipeline	O
.	O
(	O
this	O
data	O
,	O
which	O
is	O
widely	O
used	O
to	O
compare	O
data	O
visualization	O
methods	O
,	O
is	O
synthetic	O
,	O
and	O
comes	O
from	O
(	O
bishop	O
and	O
james	O
1993	O
)	O
.	O
)	O
we	O
project	O
this	O
into	O
2d	O
using	O
pca	O
and	O
kpca	O
(	O
with	O
an	O
rbf	O
kernel	B
)	O
.	O
the	O
results	O
are	O
shown	O
in	O
figure	O
14.9.	O
if	O
we	O
perform	O
nearest	B
neighbor	I
classiﬁcation	O
in	O
the	O
low-dimensional	O
space	O
,	O
kpca	O
makes	O
13	O
errors	O
and	O
pca	O
makes	O
20	O
(	O
lawrence	O
496	O
chapter	O
14.	O
kernels	O
0.5	O
0	O
0.5	O
−1	O
1.5	O
−2	O
2.5	O
−3	O
0.3	O
.25	O
0.2	O
.15	O
0.1	O
.05	O
0	O
−2	O
−1	O
0	O
1	O
2	O
0	O
0.05	O
0.1	O
0.15	O
0.2	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
14.9	O
2d	O
representation	O
of	O
12	O
dimensional	O
oil	O
ﬂow	O
data	O
.	O
the	O
different	O
colors/symbols	O
represent	O
the	O
3	O
phases	O
of	O
oil	O
ﬂow	O
.	O
(	O
a	O
)	O
pca	O
.	O
(	O
b	O
)	O
kernel	B
pca	O
with	O
gaussian	O
kernel	B
.	O
compare	O
to	O
figure	O
15.10	O
(	O
b	O
)	O
.	O
from	O
figure	O
1	O
of	O
(	O
lawrence	O
2005	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
neil	O
lawrence	O
.	O
2005	O
)	O
.	O
nevertheless	O
,	O
the	O
kpca	O
projection	B
is	O
rather	O
unnatural	O
.	O
how	O
to	O
make	O
kernelized	O
versions	O
of	O
probabilistic	O
pca	O
.	O
in	O
section	O
15.5	O
,	O
we	O
will	O
discuss	O
note	O
that	O
there	O
is	O
a	O
close	O
connection	O
between	O
kernel	B
pca	O
and	O
a	O
technique	O
known	O
as	O
mul-	O
tidimensional	O
scaling	O
or	O
mds	O
.	O
this	O
methods	O
ﬁnds	O
a	O
low-dimensional	O
embedding	B
such	O
that	O
euclidean	O
distance	O
in	O
the	O
embedding	B
space	O
approximates	O
the	O
original	O
dissimilarity	B
matrix	I
.	O
see	O
e.g.	O
,	O
(	O
williams	O
2002	O
)	O
for	O
details	O
.	O
14.5	O
support	B
vector	I
machines	I
(	O
svms	O
)	O
in	O
section	O
14.3.2	O
,	O
we	O
saw	O
one	O
way	O
to	O
derive	O
a	O
sparse	B
kernel	I
machine	I
,	O
namely	O
by	O
using	O
a	O
glm	O
with	O
kernel	B
basis	O
functions	O
,	O
plus	O
a	O
sparsity-promoting	B
prior	I
such	O
as	O
(	O
cid:6	O
)	O
1	O
or	O
ard	O
.	O
an	O
alternative	O
approach	O
is	O
to	O
change	O
the	O
objective	O
function	O
from	O
negative	B
log	I
likelihood	I
to	O
some	O
other	O
loss	B
in	O
particular	O
,	O
consider	O
the	O
(	O
cid:6	O
)	O
2	O
regularized	O
empirical	O
function	O
,	O
as	O
we	O
discussed	O
in	O
section	O
6.5.5.	O
risk	B
function	O
n	O
(	O
cid:2	O
)	O
j	O
(	O
w	O
,	O
λ	O
)	O
=	O
l	O
(	O
yi	O
,	O
ˆyi	O
)	O
+λ||w	O
||2	O
(	O
14.45	O
)	O
i=1	O
where	O
ˆyi	O
=	O
wt	O
xi	O
+	O
w0	O
.	O
moment	O
.	O
)	O
deﬁned	O
in	O
equation	O
6.73	O
,	O
this	O
is	O
equivalent	O
to	O
logistic	B
regression	I
.	O
(	O
so	O
far	O
this	O
is	O
in	O
the	O
original	O
feature	O
space	O
;	O
we	O
introduce	O
kernels	O
in	O
a	O
if	O
l	O
is	O
quadratic	B
loss	I
,	O
this	O
is	O
equivalent	O
to	O
ridge	B
regression	I
,	O
and	O
if	O
l	O
is	O
the	O
log-loss	B
in	O
the	O
ridge	B
regression	I
case	O
,	O
we	O
know	O
that	O
the	O
solution	O
to	O
this	O
has	O
the	O
form	O
ˆw	O
=	O
(	O
xt	O
x	O
+	O
−1xt	O
y	O
,	O
and	O
plug-in	B
predictions	O
take	O
the	O
form	O
ˆw0	O
+	O
ˆwt	O
x.	O
as	O
we	O
saw	O
in	O
section	O
14.4.3	O
,	O
λi	O
)	O
we	O
can	O
rewrite	O
these	O
equations	O
in	O
a	O
way	O
that	O
only	O
involves	O
inner	O
products	O
of	O
the	O
form	O
xt	O
x	O
(	O
cid:4	O
)	O
,	O
which	O
we	O
can	O
replace	O
by	O
calls	O
to	O
a	O
kernel	B
function	I
,	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
.	O
this	O
is	O
kernelized	O
,	O
but	O
not	O
sparse	O
.	O
however	O
,	O
if	O
we	O
replace	O
the	O
quadratic/	O
log-loss	B
with	O
some	O
other	O
loss	B
function	I
,	O
to	O
be	O
explained	O
below	O
,	O
we	O
can	O
ensure	O
that	O
the	O
solution	O
is	O
sparse	B
,	O
so	O
that	O
predictions	O
only	O
depend	O
on	O
a	O
subset	O
of	O
the	O
training	O
data	O
,	O
known	O
as	O
support	B
vectors	I
.	O
this	O
combination	O
of	O
the	O
kernel	B
trick	I
plus	O
a	O
modiﬁed	O
loss	B
function	I
is	O
known	O
as	O
a	O
support	B
vector	I
machine	I
or	O
svm	O
.	O
this	O
technique	O
was	O
14.5.	O
support	B
vector	I
machines	I
(	O
svms	O
)	O
497	O
l2	O
ε−insensitive	O
huber	O
y	O
(	O
x	O
)	O
ξ	O
>	O
0	O
ξ∗	O
>	O
0	O
y	O
+	O
	O
y	O
y	O
−	O
	O
5	O
4.5	O
4	O
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−3	O
−2	O
−1	O
1	O
2	O
3	O
0	O
(	O
a	O
)	O
x	O
(	O
b	O
)	O
figure	O
14.10	O
(	O
a	O
)	O
illustration	O
of	O
(	O
cid:7	O
)	O
2	O
,	O
huber	O
and	O
-insensitive	O
loss	B
functions	O
,	O
where	O
	O
=	O
1.5.	O
figure	O
generated	O
(	O
b	O
)	O
illustration	O
of	O
the	O
-tube	O
used	O
in	O
svm	O
regression	B
.	O
points	O
above	O
the	O
tube	B
have	O
by	O
huberlossdemo	O
.	O
ξi	O
>	O
0	O
and	O
ξ∗	O
i	O
>	O
0.	O
points	O
inside	O
the	O
tube	B
have	O
ξi	O
=	O
ξ∗	O
i	O
=	O
0.	O
points	O
below	O
the	O
tube	B
have	O
ξi	O
=	O
0	O
and	O
ξ∗	O
i	O
=	O
0.	O
based	O
on	O
figure	O
7.7	O
of	O
(	O
bishop	O
2006a	O
)	O
.	O
originally	O
designed	O
for	O
binary	B
classiﬁcation	I
,	O
but	O
can	O
be	O
extended	O
to	O
regression	B
and	O
multi-class	O
classiﬁcation	O
as	O
we	O
explain	O
below	O
.	O
note	O
that	O
svms	O
are	O
very	O
unnatural	O
from	O
a	O
probabilistic	O
point	O
of	O
view	O
.	O
first	O
,	O
they	O
encode	O
sparsity	B
in	O
the	O
loss	B
function	I
rather	O
than	O
the	O
prior	O
.	O
second	O
,	O
they	O
encode	O
kernels	O
by	O
using	O
an	O
algorithmic	O
trick	O
,	O
rather	O
than	O
being	O
an	O
explicit	O
part	O
of	O
the	O
model	O
.	O
finally	O
,	O
svms	O
do	O
not	O
result	O
in	O
probabilistic	O
outputs	O
,	O
which	O
causes	O
various	O
difficulties	O
,	O
especially	O
in	O
the	O
multi-class	O
classiﬁcation	O
setting	O
(	O
see	O
section	O
14.5.2.4	O
for	O
details	O
)	O
.	O
it	O
is	O
possible	O
to	O
obtain	O
sparse	B
,	O
probabilistic	O
,	O
multi-class	O
kernel-based	O
classiﬁers	O
,	O
which	O
work	O
as	O
well	O
or	O
better	O
than	O
svms	O
,	O
using	O
techniques	O
such	O
as	O
the	O
l1vm	O
or	O
rvm	O
,	O
discussed	O
in	O
section	O
14.3.2.	O
however	O
,	O
we	O
include	O
a	O
discussion	O
of	O
svms	O
,	O
despite	O
their	O
non-probabilistic	O
nature	O
,	O
for	O
two	O
main	O
reasons	O
.	O
first	O
,	O
they	O
are	O
very	O
popular	O
and	O
widely	O
used	O
,	O
so	O
all	O
students	O
of	O
machine	B
learning	I
should	O
know	O
about	O
them	O
.	O
second	O
,	O
they	O
have	O
some	O
computational	O
advantages	O
over	O
probabilistic	O
methods	O
in	O
the	O
structured	B
output	I
case	O
;	O
see	O
section	O
19.7	O
.	O
14.5.1	O
svms	O
for	O
regression	B
the	O
problem	O
with	O
kernelized	O
ridge	B
regression	I
is	O
that	O
the	O
solution	O
vector	O
w	O
depends	O
on	O
all	O
the	O
training	O
inputs	O
.	O
we	O
now	O
seek	O
a	O
method	O
to	O
produce	O
a	O
sparse	B
estimate	O
.	O
vapnik	O
(	O
vapnik	O
et	O
al	O
.	O
1997	O
)	O
proposed	O
a	O
variant	O
of	O
the	O
huber	O
loss	B
function	I
(	O
section	O
7.4	O
)	O
called	O
the	O
epsilon	B
insensitive	I
loss	I
function	I
,	O
deﬁned	O
by	O
l	O
(	O
y	O
,	O
ˆy	O
)	O
(	O
cid:2	O
)	O
0	O
|y	O
−	O
ˆy|	O
−	O
if	O
|y	O
−	O
ˆy|	O
<	O
	O
otherwise	O
(	O
14.46	O
)	O
(	O
cid:26	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
this	O
means	O
that	O
any	O
point	O
lying	O
inside	O
an	O
-tube	O
around	O
the	O
prediction	O
is	O
not	O
penalized	O
,	O
as	O
in	O
figure	O
14.10.	O
the	O
corresponding	O
objective	O
function	O
is	O
usually	O
written	O
in	O
the	O
following	O
form	O
j	O
=	O
c	O
l	O
(	O
yi	O
,	O
ˆyi	O
)	O
+	O
||w||2	O
1	O
2	O
(	O
14.47	O
)	O
498	O
chapter	O
14.	O
kernels	O
where	O
ˆyi	O
=	O
f	O
(	O
xi	O
)	O
=w	O
t	O
xi	O
+	O
w0	O
and	O
c	O
=	O
1/λ	O
is	O
a	O
regularization	B
constant	O
.	O
this	O
objective	O
is	O
convex	B
and	O
unconstrained	O
,	O
but	O
not	O
differentiable	O
,	O
because	O
of	O
the	O
absolute	O
value	O
function	O
in	O
the	O
loss	B
term	O
.	O
as	O
in	O
section	O
13.4	O
,	O
where	O
we	O
discussed	O
the	O
lasso	B
problem	O
,	O
there	O
are	O
several	O
possible	O
algorithms	O
we	O
could	O
use	O
.	O
one	O
popular	O
approach	O
is	O
to	O
formulate	O
the	O
problem	O
as	O
a	O
constrained	O
in	O
particular	O
,	O
we	O
introduce	O
slack	B
variables	I
to	O
represent	O
the	O
degree	B
to	O
optimization	B
problem	O
.	O
which	O
each	O
point	O
lies	O
outside	O
the	O
tube	B
:	O
yi	O
≤	O
f	O
(	O
xi	O
)	O
+	O
+	O
ξ+	O
yi	O
≥	O
f	O
(	O
xi	O
)	O
−	O
	O
−	O
ξ−	O
i	O
i	O
given	O
this	O
,	O
we	O
can	O
rewrite	O
the	O
objective	O
as	O
follows	O
:	O
j	O
=	O
c	O
i	O
+	O
ξ−	O
(	O
ξ+	O
i	O
)	O
+	O
||w||2	O
1	O
2	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
cid:2	O
)	O
(	O
14.48	O
)	O
(	O
14.49	O
)	O
(	O
14.50	O
)	O
(	O
14.52	O
)	O
(	O
14.53	O
)	O
(	O
14.54	O
)	O
(	O
14.55	O
)	O
this	O
is	O
a	O
quadratic	O
function	O
of	O
w	O
,	O
and	O
must	O
be	O
minimized	O
subject	O
to	O
the	O
linear	O
constraints	O
i	O
≥	O
0.	O
this	O
is	O
a	O
in	O
equations	O
14.48-14.49	O
,	O
as	O
well	O
as	O
the	O
positivity	O
constraints	O
ξ+	O
standard	O
quadratic	O
program	O
in	O
2n	O
+	O
d	O
+	O
1	O
variables	O
.	O
i	O
≥	O
0	O
and	O
ξ−	O
one	O
can	O
show	O
(	O
see	O
e.g.	O
,	O
(	O
schoelkopf	O
and	O
smola	O
2002	O
)	O
)	O
that	O
the	O
optimal	O
solution	O
has	O
the	O
form	O
ˆw	O
=	O
αixi	O
i	O
(	O
14.51	O
)	O
where	O
αi	O
≥	O
0.	O
furthermore	O
,	O
it	O
turns	O
out	O
that	O
the	O
α	O
vector	O
is	O
sparse	B
,	O
because	O
we	O
don	O
’	O
t	O
care	O
about	O
errors	O
which	O
are	O
smaller	O
than	O
	O
.	O
the	O
xi	O
for	O
which	O
αi	O
>	O
0	O
are	O
called	O
the	O
support	B
vectors	I
;	O
thse	O
are	O
points	O
for	O
which	O
the	O
errors	O
lie	O
on	O
or	O
outside	O
the	O
	O
tube	B
.	O
once	O
the	O
model	O
is	O
trained	O
,	O
we	O
can	O
then	O
make	O
predictions	O
using	O
ˆy	O
(	O
x	O
)	O
=	O
ˆw0	O
+	O
ˆwt	O
x	O
(	O
cid:2	O
)	O
plugging	O
in	O
the	O
deﬁnition	O
of	O
ˆw	O
we	O
get	O
ˆy	O
(	O
x	O
)	O
=	O
ˆw0	O
+	O
αixt	O
i	O
x	O
finally	O
,	O
we	O
can	O
replace	O
xt	O
i	O
x	O
with	O
κ	O
(	O
xi	O
,	O
x	O
)	O
to	O
get	O
a	O
kernelized	O
solution	O
:	O
i	O
(	O
cid:2	O
)	O
ˆy	O
(	O
x	O
)	O
=	O
ˆw0	O
+	O
αiκ	O
(	O
xi	O
,	O
x	O
)	O
i	O
14.5.2	O
svms	O
for	O
classiﬁcation	B
we	O
now	O
discuss	O
how	O
to	O
apply	O
svms	O
to	O
classiﬁcation	B
.	O
we	O
ﬁrst	O
focus	O
on	O
the	O
binary	O
case	O
,	O
and	O
then	O
discuss	O
the	O
multi-class	O
case	O
in	O
section	O
14.5.2.4	O
.	O
14.5.2.1	O
hinge	B
loss	I
in	O
section	O
6.5.5	O
,	O
we	O
showed	O
that	O
the	O
negative	B
log	I
likelihood	I
of	O
a	O
logistic	B
regression	I
model	O
,	O
l	O
nll	O
(	O
y	O
,	O
η	O
)	O
=	O
−	O
log	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
log	O
(	O
1	O
+	O
e−yη	O
)	O
14.5.	O
support	B
vector	I
machines	I
(	O
svms	O
)	O
499	O
was	O
a	O
convex	B
upper	O
bound	O
on	O
the	O
0-1	O
risk	B
of	O
a	O
binary	O
classiﬁer	O
,	O
where	O
η	O
=	O
f	O
(	O
x	O
)	O
=w	O
t	O
x	O
+	O
w0	O
is	O
the	O
log	O
odds	O
ratio	O
,	O
and	O
we	O
have	O
assumed	O
the	O
labels	O
are	O
y	O
∈	O
{	O
1	O
,	O
−1	O
}	O
rather	O
than	O
{	O
0	O
,	O
1	O
}	O
.	O
in	O
this	O
section	O
,	O
we	O
replace	O
the	O
nll	O
loss	B
with	O
the	O
hinge	B
loss	I
,	O
deﬁned	O
as	O
lhinge	O
(	O
y	O
,	O
η	O
)	O
=	O
max	O
(	O
0	O
,	O
1	O
−	O
yη	O
)	O
=	O
(	O
1	O
−	O
yη	O
)	O
+	O
(	O
14.56	O
)	O
here	O
η	O
=	O
f	O
(	O
x	O
)	O
is	O
our	O
“	O
conﬁdence	O
”	O
in	O
choosing	O
label	B
y	O
=	O
1	O
;	O
however	O
,	O
it	O
need	O
not	O
have	O
any	O
probabilistic	O
semantics	O
.	O
see	O
figure	O
6.7	O
for	O
a	O
plot	O
.	O
we	O
see	O
that	O
the	O
function	O
looks	O
like	O
a	O
door	O
hinge	O
,	O
hence	O
its	O
name	O
.	O
the	O
overall	O
objective	O
has	O
the	O
form	O
min	O
w	O
,	O
w0	O
1	O
2	O
||w||2	O
+	O
c	O
(	O
1	O
−	O
yif	O
(	O
xi	O
)	O
)	O
+	O
(	O
14.57	O
)	O
once	O
again	O
,	O
this	O
is	O
non-differentiable	O
,	O
because	O
of	O
the	O
max	O
term	O
.	O
however	O
,	O
by	O
introducing	O
slack	B
variables	I
ξi	O
,	O
one	O
can	O
show	O
that	O
this	O
is	O
equivalent	O
to	O
solving	O
min	O
w	O
,	O
w0	O
,	O
ξ	O
1	O
2	O
||w||2	O
+	O
c	O
ξi	O
s.t	O
.	O
ξi	O
≥	O
0	O
,	O
yi	O
(	O
xt	O
i	O
w	O
+	O
w0	O
)	O
≥	O
1	O
−	O
ξi	O
,	O
i	O
=	O
1	O
:	O
n	O
(	O
14.58	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
n	O
(	O
cid:2	O
)	O
i=1	O
this	O
is	O
a	O
quadratic	B
program	I
in	O
n	O
+	O
d	O
+	O
1	O
variables	O
,	O
subjet	O
to	O
o	O
(	O
n	O
)	O
constraints	O
.	O
we	O
can	O
eliminate	O
the	O
primal	B
variables	I
w	O
,	O
w0	O
and	O
ξi	O
,	O
and	O
just	O
solve	O
the	O
n	O
dual	B
variables	I
,	O
which	O
correspond	O
to	O
the	O
lagrange	O
multipliers	O
for	O
the	O
constraints	O
.	O
standard	O
solvers	O
take	O
o	O
(	O
n	O
3	O
)	O
time	O
.	O
however	O
,	O
specialized	O
algorithms	O
,	O
which	O
avoid	O
the	O
use	O
of	O
generic	O
qp	O
solvers	O
,	O
have	O
been	O
developed	O
for	O
this	O
problem	O
,	O
such	O
as	O
the	O
sequential	B
minimal	I
optimization	I
or	O
smo	O
algorithm	O
(	O
platt	O
1998	O
)	O
.	O
in	O
practice	O
this	O
can	O
take	O
o	O
(	O
n	O
2	O
)	O
.	O
however	O
,	O
even	O
this	O
can	O
be	O
too	O
slow	O
if	O
n	O
is	O
large	O
.	O
in	O
such	O
settings	O
,	O
it	O
is	O
common	O
to	O
use	O
linear	O
svms	O
,	O
which	O
take	O
o	O
(	O
n	O
)	O
time	O
to	O
train	O
(	O
joachims	O
2006	O
;	O
bottou	O
et	O
al	O
.	O
2007	O
)	O
.	O
one	O
can	O
show	O
that	O
the	O
solution	O
has	O
the	O
form	O
ˆw	O
=	O
αixi	O
(	O
14.59	O
)	O
(	O
cid:2	O
)	O
i	O
where	O
αi	O
=	O
λiyi	O
and	O
where	O
α	O
is	O
sparse	B
(	O
because	O
of	O
the	O
hinge	B
loss	I
)	O
.	O
the	O
xi	O
for	O
which	O
αi	O
>	O
0	O
are	O
called	O
support	B
vectors	I
;	O
these	O
are	O
points	O
which	O
are	O
either	O
incorrectly	O
classiﬁed	O
,	O
or	O
are	O
classiﬁed	O
correctly	O
but	O
are	O
on	O
or	O
inside	O
the	O
margin	B
(	O
we	O
disuss	O
margins	O
below	O
)	O
.	O
see	O
figure	O
14.12	O
(	O
b	O
)	O
for	O
an	O
illustration	O
.	O
at	O
test	O
time	O
,	O
prediction	O
is	O
done	O
using	O
ˆy	O
(	O
x	O
)	O
=	O
sgn	O
(	O
f	O
(	O
x	O
)	O
)	O
=	O
sgn	O
(	O
cid:4	O
)	O
(	O
cid:3	O
)	O
ˆw0	O
+	O
ˆwt	O
x	O
(	O
cid:13	O
)	O
(	O
cid:11	O
)	O
n	O
(	O
cid:2	O
)	O
ˆy	O
(	O
x	O
)	O
=	O
sgn	O
ˆw0	O
+	O
αiκ	O
(	O
xi	O
,	O
x	O
)	O
using	O
equation	O
14.59	O
and	O
the	O
kernel	B
trick	I
we	O
have	O
(	O
14.60	O
)	O
(	O
14.61	O
)	O
i=1	O
this	O
takes	O
o	O
(	O
sd	O
)	O
time	O
to	O
compute	O
,	O
where	O
s	O
≤	O
n	O
is	O
the	O
number	O
of	O
support	B
vectors	I
.	O
this	O
depends	O
on	O
the	O
sparsity	B
level	O
,	O
and	O
hence	O
on	O
the	O
regularizer	O
c.	O
500	O
chapter	O
14.	O
kernels	O
figure	O
14.11	O
right	O
:	O
a	O
separating	O
hyper-plane	O
with	O
small	O
margin	O
.	O
illustration	O
of	O
the	O
large	B
margin	I
principle	I
.	O
left	O
:	O
a	O
separating	O
hyper-plane	O
with	O
large	O
margin	O
.	O
y	O
>	O
0	O
y	O
=	O
0	O
y	O
<	O
0	O
r1	O
r0	O
w	O
x	O
r	O
=	O
f	O
(	O
x	O
)	O
(	O
cid:6	O
)	O
w	O
(	O
cid:6	O
)	O
x⊥	O
−w0	O
(	O
cid:6	O
)	O
w	O
(	O
cid:6	O
)	O
(	O
a	O
)	O
y	O
=	O
−1	O
ξ	O
>	O
1	O
y	O
=	O
0	O
ξ	O
<	O
1	O
y	O
=	O
1	O
ξ	O
=	O
0	O
ξ	O
=	O
0	O
(	O
b	O
)	O
figure	O
14.12	O
(	O
a	O
)	O
illustration	O
of	O
the	O
geometry	O
of	O
a	O
linear	O
decision	O
boundary	O
in	O
2d	O
.	O
a	O
point	O
x	O
is	O
classiﬁed	O
as	O
belonging	O
in	O
decision	B
region	O
r1	O
if	O
f	O
(	O
x	O
)	O
>	O
0	O
,	O
otherwise	O
it	O
belongs	O
in	O
decision	B
region	O
r2	O
;	O
here	O
f	O
(	O
x	O
)	O
is	O
known	O
as	O
a	O
discriminant	B
function	I
.	O
the	O
decision	B
boundary	I
is	O
the	O
set	O
of	O
points	O
such	O
that	O
f	O
(	O
x	O
)	O
=	O
0.	O
w	O
is	O
a	O
vector	O
which	O
is	O
perpendicular	O
to	O
the	O
decision	B
boundary	I
.	O
the	O
term	O
w0	O
controls	O
the	O
distance	O
of	O
the	O
decision	B
boundary	I
from	O
the	O
origin	O
.	O
the	O
signed	O
distance	O
of	O
x	O
from	O
its	O
orthogonal	B
projection	I
onto	O
the	O
decision	B
boundary	I
,	O
x⊥	O
,	O
is	O
given	O
by	O
f	O
(	O
x	O
)	O
/||w||	O
.	O
based	O
on	O
figure	O
4.1	O
of	O
(	O
bishop	O
2006a	O
)	O
.	O
(	O
b	O
)	O
illustration	O
of	O
the	O
soft	O
margin	O
principle	O
.	O
points	O
with	O
circles	O
around	O
them	O
are	O
support	B
vectors	I
.	O
we	O
also	O
indicate	O
the	O
value	O
of	O
the	O
corresponding	O
slack	B
variables	I
.	O
based	O
on	O
figure	O
7.3	O
of	O
(	O
bishop	O
2006a	O
)	O
.	O
14.5.	O
support	B
vector	I
machines	I
(	O
svms	O
)	O
501	O
14.5.2.2	O
the	O
large	B
margin	I
principle	I
in	O
this	O
section	O
,	O
we	O
derive	O
equation	O
14.58	O
form	O
a	O
completely	O
different	O
perspective	O
.	O
recall	B
that	O
our	O
goal	O
is	O
to	O
derive	O
a	O
discriminant	B
function	I
f	O
(	O
x	O
)	O
which	O
will	O
be	O
linear	O
in	O
the	O
feature	O
space	O
implied	O
by	O
the	O
choice	O
of	O
kernel	B
.	O
consider	O
a	O
point	O
x	O
in	O
this	O
induced	O
space	O
.	O
referring	O
to	O
figure	O
14.12	O
(	O
a	O
)	O
,	O
we	O
see	O
that	O
x	O
=	O
x⊥	O
+	O
r	O
w	O
||w||	O
(	O
14.62	O
)	O
where	O
r	O
is	O
the	O
distance	O
of	O
x	O
from	O
the	O
decision	B
boundary	I
whose	O
normal	B
vector	O
is	O
w	O
,	O
and	O
x⊥	O
is	O
the	O
orthogonal	B
projection	I
of	O
x	O
onto	O
this	O
boundary	O
.	O
hence	O
f	O
(	O
x	O
)	O
=w	O
t	O
x	O
+	O
w0	O
=	O
(	O
wt	O
x⊥	O
+	O
w0	O
)	O
+r	O
wt	O
w	O
||w||	O
(	O
14.63	O
)	O
now	O
f	O
(	O
x⊥	O
)	O
=	O
0	O
so	O
0	O
=w	O
t	O
x⊥	O
+	O
w0	O
.	O
hence	O
f	O
(	O
x	O
)	O
=	O
r	O
wt	O
w√	O
we	O
would	O
like	O
to	O
make	O
this	O
distance	O
r	O
=	O
f	O
(	O
x	O
)	O
/||w||	O
as	O
large	O
as	O
possible	O
,	O
for	O
reasons	O
illustrated	O
in	O
figure	O
14.11.	O
in	O
particular	O
,	O
there	O
might	O
be	O
many	O
lines	O
that	O
perfectly	O
separate	O
the	O
training	O
data	O
(	O
especially	O
if	O
we	O
work	O
in	O
a	O
high	O
dimensional	O
feature	O
space	O
)	O
,	O
but	O
intuitively	O
,	O
the	O
best	O
one	O
to	O
pick	O
is	O
the	O
one	O
that	O
maximizes	O
the	O
margin	B
,	O
i.e.	O
,	O
the	O
perpendicular	O
distance	O
to	O
the	O
closest	O
point	O
.	O
in	O
addition	O
,	O
we	O
want	O
to	O
ensure	O
each	O
point	O
is	O
on	O
the	O
correct	O
side	O
of	O
the	O
boundary	O
,	O
hence	O
we	O
want	O
f	O
(	O
xi	O
)	O
yi	O
>	O
0.	O
so	O
our	O
objective	O
becomes	O
wt	O
w	O
,	O
and	O
r	O
=	O
f	O
(	O
x	O
)	O
||w||	O
.	O
yi	O
(	O
wt	O
xi	O
+	O
w0	O
)	O
n	O
min	O
i=1	O
||w||	O
max	O
w	O
,	O
w0	O
(	O
14.64	O
)	O
note	O
that	O
by	O
rescaling	O
the	O
parameters	O
using	O
w	O
→	O
kw	O
and	O
w0	O
→	O
kw0	O
,	O
we	O
do	O
not	O
change	O
the	O
distance	O
of	O
any	O
point	O
to	O
the	O
boundary	O
,	O
since	O
the	O
k	O
factor	B
cancels	O
out	O
when	O
we	O
divide	O
by	O
||w||	O
.	O
therefore	O
let	O
us	O
deﬁne	O
the	O
scale	O
factor	O
such	O
that	O
yifi	O
=	O
1	O
for	O
the	O
point	O
that	O
is	O
closest	O
to	O
the	O
decision	B
boundary	I
.	O
we	O
therefore	O
want	O
to	O
optimize	O
||w||2	O
min	O
w	O
,	O
w0	O
1	O
2	O
s.t	O
.	O
yi	O
(	O
wt	O
xi	O
+	O
w0	O
)	O
≥	O
1	O
,	O
i	O
=	O
1	O
:	O
n	O
(	O
14.65	O
)	O
(	O
the	O
fact	O
of	O
1	O
2	O
is	O
added	O
for	O
convenience	O
and	O
doesn	O
’	O
t	O
affect	O
the	O
optimal	O
parameters	O
.	O
)	O
the	O
constraint	O
says	O
that	O
we	O
want	O
all	O
points	O
to	O
be	O
on	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	I
with	O
a	O
margin	B
of	O
at	O
least	O
1.	O
for	O
this	O
reason	O
,	O
we	O
say	O
that	O
an	O
svm	O
is	O
an	O
example	O
of	O
a	O
large	B
margin	I
classiﬁer	I
.	O
if	O
the	O
data	O
is	O
not	O
linearly	O
separable	O
(	O
even	O
after	O
using	O
the	O
kernel	B
trick	I
)	O
,	O
there	O
will	O
be	O
no	O
feasible	O
solution	O
in	O
which	O
yifi	O
≥	O
1	O
for	O
all	O
i.	O
we	O
therefore	O
introduce	O
slack	B
variables	I
ξi	O
≥	O
0	O
such	O
that	O
ξi	O
=	O
0	O
if	O
the	O
point	O
is	O
on	O
or	O
inside	O
the	O
correct	O
margin	B
boundary	O
,	O
and	O
ξi	O
=	O
|yi	O
−	O
fi|	O
otherwise	O
.	O
if	O
0	O
<	O
ξi	O
≤	O
1	O
the	O
point	O
lies	O
inside	O
the	O
margin	B
,	O
but	O
on	O
the	O
correct	O
side	O
of	O
the	O
decision	B
boundary	I
.	O
if	O
ξi	O
>	O
1	O
,	O
the	O
point	O
lies	O
on	O
the	O
wrong	O
side	O
of	O
the	O
decision	B
boundary	I
.	O
see	O
figure	O
14.12	O
(	O
b	O
)	O
.	O
we	O
replace	O
the	O
hard	O
constraints	O
that	O
yifi	O
≥	O
0	O
with	O
the	O
soft	B
margin	I
constraints	I
that	O
yifi	O
≥	O
1	O
−	O
ξi	O
.	O
the	O
new	O
objective	O
becomes	O
ξi	O
s.t	O
.	O
ξi	O
≥	O
0	O
,	O
yi	O
(	O
xt	O
i	O
w	O
+	O
w0	O
)	O
≥	O
1	O
−	O
ξi	O
(	O
14.66	O
)	O
min	O
w	O
,	O
w0	O
,	O
ξ	O
1	O
2	O
||w||2	O
+	O
c	O
n	O
(	O
cid:2	O
)	O
i=1	O
502	O
chapter	O
14.	O
kernels	O
correct	O
log−odds	O
rvm	O
y	O
(	O
x	O
)	O
svm	O
y	O
(	O
x	O
)	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
0.4	O
0.6	O
0.8	O
1	O
1.2	O
figure	O
14.13	O
log-odds	O
vs	O
x	O
for	O
3	O
different	O
methods	O
.	O
based	O
on	O
figure	O
10	O
of	O
(	O
tipping	O
2001	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
mike	O
tipping	O
.	O
(	O
cid:10	O
)	O
which	O
is	O
the	O
same	O
as	O
equation	O
14.58.	O
since	O
ξi	O
>	O
1	O
means	O
point	O
i	O
is	O
misclassiﬁed	O
,	O
we	O
can	O
interpret	O
i	O
ξi	O
as	O
an	O
upper	O
bound	O
on	O
the	O
number	O
of	O
misclassiﬁed	O
points	O
.	O
the	O
parameter	B
c	O
is	O
a	O
regularization	B
parameter	O
that	O
controls	O
the	O
number	O
of	O
errors	O
we	O
are	O
it	O
is	O
common	O
to	O
deﬁne	O
this	O
using	O
c	O
=	O
1/	O
(	O
νn	O
)	O
,	O
where	O
willing	O
to	O
tolerate	O
on	O
the	O
training	B
set	I
.	O
0	O
<	O
ν	O
≤	O
1	O
controls	O
the	O
fraction	O
of	O
misclassiﬁed	O
points	O
that	O
we	O
allow	O
during	O
the	O
training	O
phase	O
.	O
this	O
is	O
called	O
a	O
ν-svm	O
classiﬁer	O
.	O
this	O
is	O
usually	O
set	O
using	O
cross-validation	O
(	O
see	O
section	O
14.5.3	O
)	O
.	O
14.5.2.3	O
probabilistic	O
output	O
an	O
svm	O
classiﬁer	O
produces	O
a	O
hard-labeling	O
,	O
ˆy	O
(	O
x	O
)	O
=	O
sign	O
(	O
f	O
(	O
x	O
)	O
)	O
.	O
however	O
,	O
we	O
often	O
want	O
a	O
measure	O
of	O
conﬁdence	O
in	O
our	O
prediction	O
.	O
one	O
heuristic	O
approach	O
is	O
to	O
interpret	O
f	O
(	O
x	O
)	O
as	O
the	O
p	O
(	O
y=1|x	O
)	O
log-odds	B
ratio	I
,	O
log	O
p	O
(	O
y=0|x	O
)	O
.	O
we	O
can	O
then	O
convert	O
the	O
output	O
of	O
an	O
svm	O
to	O
a	O
probability	O
using	O
p	O
(	O
y	O
=	O
1|x	O
,	O
θ	O
)	O
=	O
σ	O
(	O
af	O
(	O
x	O
)	O
+b	O
)	O
(	O
14.67	O
)	O
where	O
a	O
,	O
b	O
can	O
be	O
estimated	O
by	O
maximum	O
likelihood	O
on	O
a	O
separate	O
validation	B
set	I
.	O
(	O
using	O
the	O
training	B
set	I
to	O
estimate	O
a	O
and	O
b	O
leads	O
to	O
severe	O
overﬁtting	B
.	O
)	O
this	O
technique	O
was	O
ﬁrst	O
proposed	O
in	O
(	O
platt	O
2000	O
)	O
.	O
however	O
,	O
the	O
resulting	O
probabilities	O
are	O
not	O
particularly	O
well	O
calibrated	O
,	O
since	O
there	O
is	O
nothing	O
in	O
the	O
svm	O
training	O
procedure	O
that	O
justiﬁes	O
interpreting	O
f	O
(	O
x	O
)	O
as	O
a	O
log-odds	B
ratio	I
.	O
to	O
illustrate	O
this	O
,	O
consider	O
an	O
example	O
from	O
(	O
tipping	O
2001	O
)	O
.	O
suppose	O
we	O
have	O
1d	O
data	O
where	O
p	O
(	O
x|y	O
=	O
0	O
)	O
=	O
unif	O
(	O
0	O
,	O
1	O
)	O
and	O
p	O
(	O
x|y	O
=	O
1	O
)	O
=	O
unif	O
(	O
0.5	O
,	O
1.5	O
)	O
.	O
since	O
the	O
class-conditional	O
distributions	O
overlap	O
in	O
the	O
middle	O
,	O
the	O
log-odds	O
of	O
class	O
1	O
over	O
class	O
0	O
should	O
be	O
zero	O
in	O
[	O
0.5	O
,	O
1.0	O
]	O
,	O
and	O
inﬁnite	O
outside	O
this	O
region	O
.	O
we	O
sampled	O
1000	O
points	O
from	O
the	O
model	O
,	O
and	O
then	O
ﬁt	O
an	O
rvm	O
and	O
an	O
svm	O
with	O
a	O
gaussian	O
kenel	O
of	O
width	O
0.1.	O
both	O
models	O
can	O
perfectly	O
capture	O
the	O
decision	B
boundary	I
,	O
and	O
achieve	O
a	O
generalizaton	O
error	O
of	O
25	O
%	O
,	O
which	O
is	O
bayes	O
optimal	O
in	O
this	O
problem	O
.	O
the	O
probabilistic	O
output	O
from	O
the	O
rvm	O
is	O
a	O
good	O
approximation	O
to	O
the	O
true	O
log-odds	O
,	O
but	O
this	O
is	O
not	O
the	O
case	O
for	O
the	O
svm	O
,	O
as	O
shown	O
in	O
figure	O
14.13	O
.	O
14.5.	O
support	B
vector	I
machines	I
(	O
svms	O
)	O
503	O
?	O
r1	O
r2	O
c1	O
c3	O
r1	O
?	O
r3	O
c2	O
c1	O
r3	O
c2	O
r2	O
c1	O
not	O
c1	O
not	O
c2	O
c2	O
c3	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
a	O
)	O
the	O
one-versus-rest	O
approach	O
.	O
the	O
green	O
region	O
is	O
predicted	O
to	O
be	O
both	O
class	O
1	O
and	O
class	O
(	O
b	O
)	O
the	O
one-versus-one	B
approach	O
.	O
the	O
label	B
of	O
the	O
green	O
region	O
is	O
ambiguous	O
.	O
based	O
on	O
figure	O
4.2	O
of	O
figure	O
14.14	O
2	O
.	O
(	O
bishop	O
2006a	O
)	O
.	O
14.5.2.4	O
svms	O
for	O
multi-class	O
classiﬁcation	O
in	O
section	O
8.3.7	O
,	O
we	O
saw	O
how	O
we	O
could	O
“	O
upgrade	O
”	O
a	O
binary	O
logistic	O
regression	B
model	O
to	O
the	O
multi-	O
class	O
case	O
,	O
by	O
replacing	O
the	O
sigmoid	B
function	O
with	O
the	O
softmax	B
,	O
and	O
the	O
bernoulli	O
distribution	O
with	O
the	O
multinomial	B
.	O
upgrading	O
an	O
svm	O
to	O
the	O
multi-class	O
case	O
is	O
not	O
so	O
easy	O
,	O
since	O
the	O
outputs	O
are	O
not	O
on	O
a	O
calibrated	O
scale	O
and	O
hence	O
are	O
hard	O
to	O
compare	O
to	O
each	O
other	O
.	O
the	O
obvious	O
approach	O
is	O
to	O
use	O
a	O
one-versus-the-rest	B
approach	O
(	O
also	O
called	O
one-vs-all	B
)	O
,	O
in	O
which	O
we	O
train	O
c	O
binary	O
classiﬁers	O
,	O
fc	O
(	O
x	O
)	O
,	O
where	O
the	O
data	O
from	O
class	O
c	O
is	O
treated	O
as	O
positive	O
,	O
and	O
the	O
data	O
from	O
all	O
the	O
other	O
classes	O
is	O
treated	O
as	O
negative	O
.	O
however	O
,	O
this	O
can	O
result	O
in	O
regions	O
of	O
input	O
space	O
which	O
are	O
ambiguously	O
labeled	O
,	O
as	O
shown	O
in	O
figure	O
14.14	O
(	O
a	O
)	O
.	O
a	O
common	O
alternative	O
is	O
to	O
pick	O
ˆy	O
(	O
x	O
)	O
=	O
arg	O
maxc	O
fc	O
(	O
x	O
)	O
.	O
however	O
,	O
this	O
technique	O
may	O
not	O
work	O
either	O
,	O
since	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
different	O
fc	O
functions	O
have	O
comparable	O
magnitudes	O
.	O
in	O
addition	O
,	O
each	O
binary	O
subproblem	O
is	O
likely	O
to	O
suffer	O
from	O
the	O
class	B
imbalance	I
problem	O
.	O
to	O
see	O
this	O
,	O
suppose	O
we	O
have	O
10	O
equally	O
represented	O
classes	O
.	O
when	O
training	O
f1	O
,	O
we	O
will	O
have	O
10	O
%	O
positive	B
examples	I
and	O
90	O
%	O
negative	B
examples	I
,	O
which	O
can	O
hurt	O
performance	O
.	O
it	O
is	O
possible	O
to	O
devise	O
ways	O
to	O
train	O
all	O
c	O
classiﬁers	O
simultaneously	O
(	O
weston	O
and	O
watkins	O
1999	O
)	O
,	O
but	O
the	O
resulting	O
method	O
takes	O
o	O
(	O
c	O
2n	O
2	O
)	O
time	O
,	O
instead	O
of	O
the	O
usual	O
o	O
(	O
cn	O
2	O
)	O
time	O
.	O
another	O
approach	O
is	O
to	O
use	O
the	O
one-versus-one	B
or	O
ovo	O
approach	O
,	O
also	O
called	O
all	B
pairs	I
,	O
in	O
which	O
we	O
train	O
c	O
(	O
c−1	O
)	O
/2	O
classiﬁers	O
to	O
discriminate	O
all	B
pairs	I
fc	O
,	O
c	O
(	O
cid:2	O
)	O
.	O
we	O
then	O
classify	O
a	O
point	O
into	O
the	O
class	O
which	O
has	O
the	O
highest	O
number	O
of	O
votes	O
.	O
however	O
,	O
this	O
can	O
also	O
result	O
in	O
ambiguities	O
,	O
as	O
shown	O
in	O
figure	O
14.14	O
(	O
b	O
)	O
.	O
also	O
,	O
it	O
takes	O
o	O
(	O
c	O
2n	O
2	O
)	O
time	O
to	O
train	O
and	O
o	O
(	O
c	O
2nsv	O
)	O
to	O
test	O
each	O
data	O
point	O
,	O
where	O
nsv	O
is	O
the	O
number	O
of	O
support	B
vectors.2	O
see	O
also	O
(	O
allwein	O
et	O
al	O
.	O
2000	O
)	O
for	O
an	O
approach	O
based	O
on	O
error-correcting	B
output	I
codes	I
.	O
it	O
is	O
worth	O
remembering	O
that	O
all	O
of	O
these	O
difficulties	O
,	O
and	O
the	O
plethora	O
of	O
heuristics	B
that	O
have	O
been	O
proposed	O
to	O
ﬁx	O
them	O
,	O
fundamentally	O
arise	O
because	O
svms	O
do	O
not	O
model	O
uncertainty	O
using	O
probabilities	O
,	O
so	O
their	O
output	O
scores	B
are	O
not	O
comparable	O
across	O
classes	O
.	O
2.	O
we	O
can	O
reduce	O
the	O
test	O
time	O
by	O
structuring	O
the	O
classes	O
into	O
a	O
dag	O
(	O
directed	B
acyclic	I
graph	I
)	O
,	O
and	O
performing	O
o	O
(	O
c	O
)	O
pairwise	O
comparisons	O
(	O
platt	O
et	O
al	O
.	O
2000	O
)	O
.	O
however	O
,	O
the	O
o	O
(	O
c2	O
)	O
factor	B
in	O
the	O
training	O
time	O
is	O
unavoidable	O
.	O
504	O
r	O
o	O
r	O
r	O
e	O
v	O
c	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
101	O
chapter	O
14.	O
kernels	O
γ	O
=	O
5.0	O
0.35	O
r	O
o	O
r	O
r	O
e	O
v	O
c	O
0.3	O
0.25	O
0.2	O
10−2	O
10−1	O
100	O
101	O
c	O
(	O
b	O
)	O
102	O
103	O
104	O
100	O
γ	O
100	O
c	O
10−1	O
10−2	O
(	O
a	O
)	O
104	O
102	O
figure	O
14.15	O
(	O
a	O
)	O
a	O
cross	B
validation	I
estimate	O
of	O
the	O
0-1	O
error	O
for	O
an	O
svm	O
classiﬁer	O
with	O
rbf	O
kernel	B
with	O
different	O
precisions	O
γ	O
=	O
1/	O
(	O
2σ2	O
)	O
and	O
different	O
regularizer	O
λ	O
=	O
1/c	O
,	O
applied	O
to	O
a	O
synthetic	O
data	O
set	O
drawn	O
from	O
a	O
mixture	O
of	O
2	O
gaussians	O
.	O
(	O
b	O
)	O
a	O
slice	O
through	O
this	O
surface	O
for	O
γ	O
=	O
5	O
the	O
red	O
dotted	O
line	O
is	O
the	O
bayes	O
optimal	O
error	O
,	O
computed	O
using	O
bayes	O
rule	O
applied	O
to	O
the	O
model	O
used	O
to	O
generate	O
the	O
data	O
.	O
based	O
on	O
figure	O
12.6	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
svmcgammademo	O
.	O
14.5.3	O
choosing	O
c	O
2σ2	O
.	O
−15	O
,	O
2	O
svms	O
for	O
both	O
classiﬁcation	B
and	O
regression	B
require	O
that	O
you	O
specify	O
the	O
kernel	B
function	I
and	O
the	O
parameter	B
c.	O
typically	O
c	O
is	O
chosen	O
by	O
cross-validation	O
.	O
note	O
,	O
however	O
,	O
that	O
c	O
interacts	O
quite	O
strongly	O
with	O
the	O
kernel	B
parameters	O
.	O
for	O
example	O
,	O
suppose	O
we	O
are	O
using	O
an	O
rbf	O
kernel	B
with	O
precision	B
γ	O
=	O
1	O
if	O
γ	O
=	O
5	O
,	O
corresponding	O
to	O
narrow	O
kernels	O
,	O
we	O
need	O
heavy	O
regularization	O
,	O
and	O
hence	O
small	O
c	O
(	O
so	O
λ	O
=	O
1/c	O
is	O
big	O
)	O
.	O
if	O
γ	O
=	O
1	O
,	O
a	O
larger	O
value	O
of	O
c	O
should	O
be	O
used	O
.	O
so	O
we	O
see	O
that	O
γ	O
and	O
c	O
are	O
tightly	O
coupled	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
14.15	O
,	O
which	O
shows	O
the	O
cv	O
estimate	O
of	O
the	O
0-1	O
risk	B
as	O
a	O
function	O
of	O
c	O
and	O
γ.	O
the	O
authors	O
of	O
libsvm	O
recommend	O
(	O
hsu	O
et	O
al	O
.	O
2009	O
)	O
using	O
cv	O
over	O
a	O
2d	O
grid	O
with	O
values	O
c	O
∈	O
{	O
2	O
−13	O
,	O
.	O
.	O
.	O
,	O
23	O
}	O
.	O
in	O
addition	O
,	O
it	O
is	O
important	O
to	O
standardize	O
−5	O
,	O
2	O
the	O
data	O
ﬁrst	O
,	O
for	O
a	O
spherical	B
gaussian	O
kernel	B
to	O
make	O
sense	O
.	O
to	O
choose	O
c	O
efficiently	O
,	O
one	O
can	O
develop	O
a	O
path	B
following	O
algorithm	O
in	O
the	O
spirit	O
of	O
lars	O
(	O
section	O
13.3.4	O
)	O
.	O
the	O
basic	O
idea	O
is	O
to	O
start	O
with	O
λ	O
large	O
,	O
so	O
that	O
the	O
margin	B
1/||w	O
(	O
λ	O
)	O
||	O
is	O
wide	O
,	O
and	O
hence	O
all	O
points	O
are	O
inside	O
of	O
it	O
and	O
have	O
αi	O
=	O
1.	O
by	O
slowly	O
decreasing	O
λ	O
,	O
a	O
small	O
set	O
of	O
points	O
will	O
move	O
from	O
inside	O
the	O
margin	B
to	O
outside	O
,	O
and	O
their	O
αi	O
values	O
will	O
change	O
from	O
1	O
to	O
0	O
,	O
as	O
they	O
cease	O
to	O
be	O
support	B
vectors	I
.	O
when	O
λ	O
is	O
maximal	O
,	O
the	O
function	O
is	O
completely	O
smoothed	O
,	O
and	O
no	O
support	O
vectors	O
remain	O
.	O
see	O
(	O
hastie	O
et	O
al	O
.	O
2004	O
)	O
for	O
the	O
details	O
.	O
−3	O
,	O
.	O
.	O
.	O
,	O
215	O
}	O
and	O
γ	O
∈	O
{	O
2	O
14.5.4	O
summary	O
of	O
key	O
points	O
summarizing	O
the	O
above	O
discussion	O
,	O
we	O
recognize	O
that	O
svm	O
classiﬁers	O
involve	O
three	O
key	O
ingre-	O
dients	O
:	O
the	O
kernel	B
trick	I
,	O
sparsity	B
,	O
and	O
the	O
large	B
margin	I
principle	I
.	O
the	O
kernel	B
trick	I
is	O
necessary	O
to	O
prevent	O
underﬁtting	O
,	O
i.e.	O
,	O
to	O
ensure	O
that	O
the	O
feature	O
vector	O
is	O
sufficiently	O
rich	O
that	O
a	O
linear	O
classiﬁer	O
can	O
separate	O
the	O
data	O
.	O
(	O
recall	B
from	O
section	O
14.2.3	O
that	O
any	O
mercer	O
kernel	B
can	O
be	O
viewed	O
as	O
implicitly	O
deﬁning	O
a	O
potentially	O
high	O
dimensional	O
feature	O
vector	O
.	O
)	O
if	O
the	O
original	O
features	B
are	O
already	O
high	O
dimensional	O
(	O
as	O
in	O
many	O
gene	O
expression	O
and	O
text	O
classiﬁcation	B
problems	O
)	O
,	O
it	O
suf-	O
ﬁces	O
to	O
use	O
a	O
linear	B
kernel	I
,	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
,	O
which	O
is	O
equivalent	O
to	O
working	O
with	O
the	O
original	O
features.	O
)	O
=x	O
t	O
x	O
(	O
cid:4	O
)	O
14.6.	O
comparison	O
of	O
discriminative	B
kernel	O
methods	O
method	O
opt	O
.	O
w	O
convex	B
l2vm	O
l1vm	O
convex	B
not	O
convex	B
rvm	O
convex	B
svm	O
gp	O
n/a	O
opt	O
.	O
kernel	B
eb	O
cv	O
eb	O
cv	O
eb	O
sparse	B
no	O
yes	O
yes	O
yes	O
no	O
prob	O
.	O
multiclass	O
non-mercer	O
yes	O
yes	O
yes	O
no	O
yes	O
yes	O
yes	O
yes	O
indirectly	O
yes	O
yes	O
yes	O
yes	O
no	O
no	O
505	O
section	O
14.3.2	O
14.3.2	O
14.3.2	O
14.5	O
15	O
table	O
14.1	O
comparison	O
of	O
various	O
kernel	B
based	O
classiﬁers	O
.	O
eb	O
=	O
empirical	O
bayes	O
,	O
cv	O
=	O
cross	B
validation	I
.	O
see	O
text	O
for	O
details	O
.	O
the	O
sparsity	B
and	O
large	O
margin	O
principles	O
are	O
necessary	O
to	O
prevent	O
overﬁtting	B
,	O
i.e.	O
,	O
to	O
ensure	O
that	O
we	O
do	O
not	O
use	O
all	O
the	O
basis	B
functions	I
.	O
these	O
two	O
ideas	O
are	O
closely	O
related	O
to	O
each	O
other	O
,	O
and	O
both	O
arise	O
(	O
in	O
this	O
case	O
)	O
from	O
the	O
use	O
of	O
the	O
hinge	B
loss	I
function	O
.	O
however	O
,	O
there	O
are	O
other	O
methods	O
of	O
achieving	O
sparsity	B
(	O
such	O
as	O
(	O
cid:6	O
)	O
1	O
)	O
,	O
and	O
also	O
other	O
methods	O
of	O
maximizing	O
the	O
margin	B
(	O
such	O
as	O
boosting	B
)	O
.	O
a	O
deeper	O
discussion	O
of	O
this	O
point	O
takes	O
us	O
outside	O
of	O
the	O
scope	B
of	O
this	O
book	O
.	O
see	O
e.g.	O
,	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
for	O
more	O
information	B
.	O
14.5.5	O
a	O
probabilistic	O
interpretation	O
of	O
svms	O
in	O
section	O
14.3	O
,	O
we	O
saw	O
how	O
to	O
use	O
kernels	O
inside	O
glms	O
to	O
derive	O
probabilistic	O
classiﬁers	O
,	O
such	O
as	O
the	O
l1vm	O
and	O
rvm	O
.	O
and	O
in	O
section	O
15.3	O
,	O
we	O
will	O
discuss	O
gaussian	O
process	O
classiﬁers	O
,	O
which	O
also	O
use	O
kernels	O
.	O
however	O
,	O
all	O
of	O
these	O
approaches	O
use	O
a	O
logistic	B
or	O
probit	B
likelihood	O
,	O
as	O
opposed	O
to	O
the	O
hinge	B
loss	I
used	O
by	O
svms	O
.	O
it	O
is	O
natural	O
to	O
wonder	O
if	O
one	O
can	O
interpret	O
the	O
svm	O
more	O
directly	O
as	O
a	O
probabilistic	O
model	O
.	O
to	O
do	O
so	O
,	O
we	O
must	O
interpret	O
cg	O
(	O
m	O
)	O
as	O
a	O
negative	B
log	I
likelihood	I
,	O
where	O
g	O
(	O
m	O
)	O
=	O
(	O
1	O
−	O
m	O
)	O
+	O
,	O
where	O
m	O
=	O
yf	O
(	O
x	O
)	O
is	O
the	O
margin	B
.	O
hence	O
p	O
(	O
y	O
=	O
1|f	O
)	O
=	O
exp	O
(	O
−cg	O
(	O
f	O
)	O
)	O
and	O
p	O
(	O
y	O
=	O
−1|f	O
)	O
=	O
exp	O
(	O
−cg	O
(	O
−f	O
)	O
)	O
.	O
by	O
summing	O
over	O
both	O
values	O
of	O
y	O
,	O
we	O
require	O
that	O
exp	O
(	O
−cg	O
(	O
f	O
)	O
)	O
+	O
exp	O
(	O
−cg	O
(	O
−f	O
)	O
)	O
be	O
a	O
constant	O
independent	O
of	O
f.	O
but	O
it	O
turns	O
out	O
this	O
is	O
not	O
possible	O
for	O
any	O
c	O
>	O
0	O
(	O
sollich	O
2002	O
)	O
.	O
however	O
,	O
if	O
we	O
are	O
willing	O
to	O
relax	O
the	O
sum-to-one	O
condition	O
,	O
and	O
work	O
with	O
a	O
pseudo-	O
likelihood	B
,	O
we	O
can	O
derive	O
a	O
probabilistic	O
interpretation	O
of	O
the	O
hinge	B
loss	I
(	O
polson	O
and	O
scott	O
2011	O
)	O
.	O
in	O
particular	O
,	O
one	O
can	O
show	O
that	O
(	O
cid:28	O
)	O
∞	O
0	O
(	O
cid:8	O
)	O
1√	O
2πλi	O
exp	O
−	O
1	O
2	O
(	O
cid:9	O
)	O
(	O
1	O
+	O
λi	O
−	O
yixt	O
i	O
w	O
)	O
2	O
λi	O
exp	O
(	O
−2	O
(	O
1	O
−	O
yixt	O
i	O
w	O
)	O
+	O
)	O
=	O
dλi	O
(	O
14.68	O
)	O
thus	O
the	O
exponential	O
of	O
the	O
negative	O
hinge	O
loss	B
can	O
be	O
represented	O
as	O
a	O
gaussian	O
scale	O
mixture	O
.	O
this	O
allows	O
one	O
to	O
ﬁt	O
an	O
svm	O
using	O
em	O
or	O
gibbs	O
sampling	O
,	O
where	O
λi	O
are	O
the	O
latent	B
variables	O
.	O
this	O
in	O
turn	O
opens	O
the	O
door	O
to	O
bayesian	O
methods	O
for	O
setting	O
the	O
hyper-parameters	B
for	O
the	O
prior	O
on	O
w.	O
see	O
(	O
polson	O
and	O
scott	O
2011	O
)	O
for	O
details	O
.	O
(	O
see	O
also	O
(	O
franc	O
et	O
al	O
.	O
2011	O
)	O
for	O
a	O
different	O
probabilistic	O
interpretation	O
of	O
svms	O
.	O
)	O
14.6	O
comparison	O
of	O
discriminative	B
kernel	O
methods	O
we	O
have	O
mentioned	O
several	O
different	O
methods	O
for	O
classiﬁcation	B
and	O
regression	B
based	O
on	O
kernels	O
,	O
which	O
we	O
summarize	O
in	O
table	O
14.1	O
.	O
(	O
gp	O
stands	O
for	O
“	O
gaussian	O
process	O
”	O
,	O
which	O
we	O
discuss	O
in	O
chapter	O
15	O
.	O
)	O
the	O
columns	O
have	O
the	O
following	O
meaning	O
:	O
506	O
chapter	O
14.	O
kernels	O
•	O
optimize	O
w	O
:	O
a	O
key	O
question	O
is	O
whether	O
the	O
objective	O
j	O
(	O
w	O
)	O
=	O
−	O
log	O
p	O
(	O
d|w	O
)	O
−	O
log	O
p	O
(	O
w	O
)	O
is	O
convex	B
or	O
not	O
.	O
l2vm	O
,	O
l1vm	O
and	O
svms	O
have	O
convex	B
objectives	O
.	O
rvms	O
do	O
not	O
.	O
gps	O
are	O
bayesian	O
methods	O
that	O
do	O
not	O
perform	O
parameter	B
estimation	O
.	O
•	O
optimize	O
kernel	B
:	O
all	O
the	O
methods	O
require	O
that	O
one	O
“	O
tune	O
”	O
the	O
kernel	B
parameters	O
,	O
such	O
as	O
the	O
bandwidth	B
of	O
the	O
rbf	O
kernel	B
,	O
as	O
well	O
as	O
the	O
level	O
of	O
regularization	B
.	O
for	O
methods	O
based	O
on	O
gaussians	O
,	O
including	O
l2vm	O
,	O
rvms	O
and	O
gps	O
,	O
we	O
can	O
use	O
efficient	O
gradient	O
based	O
optimizers	O
to	O
maximize	O
the	O
marginal	B
likelihood	I
.	O
for	O
svms	O
,	O
and	O
l1vm	O
,	O
we	O
must	O
use	O
cross	B
validation	I
,	O
which	O
is	O
slower	O
(	O
see	O
section	O
14.5.3	O
)	O
.	O
•	O
sparse	B
:	O
l1vm	O
,	O
rvms	O
and	O
svms	O
are	O
sparse	O
kernel	O
methods	O
,	O
in	O
that	O
they	O
only	O
use	O
a	O
subset	O
of	O
the	O
training	O
examples	O
.	O
gps	O
and	O
l2vm	O
are	O
not	O
sparse	O
:	O
they	O
use	O
all	O
the	O
training	O
examples	O
.	O
the	O
principle	O
advantage	O
of	O
sparsity	B
is	O
that	O
prediction	O
at	O
test	O
time	O
is	O
usually	O
faster	O
.	O
in	O
addition	O
,	O
one	O
can	O
sometimes	O
get	O
improved	O
accuracy	O
.	O
•	O
probabilistic	O
:	O
all	O
the	O
methods	O
except	O
for	O
svms	O
produce	O
probabilistic	O
output	O
of	O
the	O
form	O
p	O
(	O
y|x	O
)	O
.	O
svms	O
produce	O
a	O
“	O
conﬁdence	O
”	O
value	O
that	O
can	O
be	O
converted	O
to	O
a	O
probability	O
,	O
but	O
such	O
probabilities	O
are	O
usually	O
very	O
poorly	O
calibrated	O
(	O
see	O
section	O
14.5.2.3	O
)	O
.	O
•	O
multiclass	O
:	O
all	O
the	O
methods	O
except	O
for	O
svms	O
naturally	O
work	O
in	O
the	O
multiclass	O
setting	O
,	O
by	O
using	O
a	O
multinoulli	O
output	O
instead	O
of	O
bernoulli	O
.	O
the	O
svm	O
can	O
be	O
made	O
into	O
a	O
multiclass	O
classiﬁer	O
,	O
but	O
there	O
are	O
various	O
difficulties	O
with	O
this	O
approach	O
,	O
as	O
discussed	O
in	O
section	O
14.5.2.4	O
.	O
•	O
mercer	O
kernel	B
:	O
svms	O
and	O
gps	O
require	O
that	O
the	O
kernel	B
is	O
positive	B
deﬁnite	I
;	O
the	O
other	O
techniques	O
do	O
not	O
.	O
apart	O
from	O
these	O
differences	O
,	O
there	O
is	O
the	O
natural	O
question	O
:	O
which	O
method	O
works	O
best	O
?	O
in	O
a	O
small	O
experiment3	O
,	O
we	O
found	O
that	O
all	O
of	O
these	O
methods	O
had	O
similar	B
accuracy	O
when	O
averaged	O
over	O
a	O
range	O
of	O
problems	O
,	O
provided	O
they	O
have	O
the	O
same	O
kernel	B
,	O
and	O
provided	O
the	O
regularization	B
constants	O
are	O
chosen	O
appropriately	O
.	O
given	O
that	O
the	O
statistical	O
performance	O
is	O
roughly	O
the	O
same	O
,	O
what	O
about	O
the	O
computational	O
performance	O
?	O
gps	O
and	O
l2vm	O
are	O
generally	O
the	O
slowest	O
,	O
taking	O
o	O
(	O
n	O
3	O
)	O
time	O
,	O
since	O
they	O
don	O
’	O
t	O
exploit	O
sparsity	B
(	O
although	O
various	O
speedups	O
are	O
possible	O
,	O
see	O
section	O
15.6	O
)	O
.	O
svms	O
also	O
take	O
o	O
(	O
n	O
3	O
)	O
time	O
to	O
train	O
(	O
unless	O
we	O
use	O
a	O
linear	B
kernel	I
,	O
in	O
which	O
case	O
we	O
only	O
need	O
o	O
(	O
n	O
)	O
time	O
(	O
joachims	O
2006	O
)	O
)	O
.	O
however	O
,	O
the	O
need	O
to	O
use	O
cross	B
validation	I
can	O
make	O
svms	O
slower	O
than	O
rvms	O
.	O
l1vm	O
should	O
be	O
faster	O
than	O
an	O
rvm	O
,	O
since	O
an	O
rvm	O
requires	O
multiple	O
rounds	O
of	O
(	O
cid:6	O
)	O
1	O
minimization	O
(	O
see	O
section	O
13.7.4.3	O
)	O
.	O
however	O
,	O
in	O
practice	O
it	O
is	O
common	O
to	O
use	O
a	O
greedy	O
method	O
to	O
train	O
rvms	O
,	O
which	O
is	O
faster	O
than	O
(	O
cid:6	O
)	O
1	O
minimization	O
.	O
this	O
is	O
reﬂected	O
in	O
our	O
empirical	O
results	O
.	O
the	O
conclusion	O
of	O
all	O
this	O
is	O
as	O
follows	O
:	O
if	O
speed	O
matters	O
,	O
use	O
an	O
rvm	O
,	O
but	O
if	O
well-calibrated	O
probabilistic	O
output	O
matters	O
(	O
e.g.	O
,	O
for	O
active	B
learning	I
or	O
control	O
problems	O
)	O
,	O
use	O
a	O
gp	O
.	O
the	O
only	O
circumstances	O
under	O
which	O
using	O
an	O
svm	O
seems	O
sensible	O
is	O
the	O
structured	B
output	I
case	O
,	O
where	O
likelihood-based	O
methods	O
can	O
be	O
slow	O
.	O
(	O
we	O
attribute	O
the	O
enormous	O
popularity	O
of	O
svms	O
not	O
to	O
their	O
superiority	O
,	O
but	O
to	O
ignorance	O
of	O
the	O
alternatives	O
,	O
and	O
also	O
to	O
the	O
lack	O
of	O
high	O
quality	O
software	O
implementing	O
the	O
alternatives	O
.	O
)	O
section	O
16.7.1	O
gives	O
a	O
more	O
extensive	O
experimental	O
comparison	O
of	O
supervised	B
learning	I
methods	O
,	O
including	O
svms	O
and	O
various	O
non	O
kernel	B
methods	O
.	O
3.	O
see	O
http	O
:	O
//pmtk3.googlecode.com/svn/trunk/docs/tutorial/html/tutkernelclassif.html	O
.	O
14.7.	O
kernels	O
for	O
building	O
generative	O
models	O
507	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−1.5	O
boxcar	O
epanechnikov	O
tricube	O
gaussian	O
−1	O
−0.5	O
0	O
0.5	O
1	O
1.5	O
figure	O
14.16	O
a	O
comparison	O
of	O
some	O
popular	O
smoothing	O
kernels	O
.	O
the	O
boxcar	B
kernel	I
has	O
compact	O
support	B
but	O
is	O
not	O
smooth	O
.	O
the	O
epanechnikov	O
kernel	B
has	O
compact	O
support	B
but	O
is	O
not	O
differentiable	O
at	O
its	O
boundary	O
.	O
the	O
tri-cube	O
has	O
compact	O
support	B
and	O
two	O
continuous	O
derivatives	O
at	O
the	O
boundary	O
of	O
its	O
support	B
.	O
the	O
gaussian	O
is	O
differentiable	O
,	O
but	O
does	O
not	O
have	O
compact	O
support	B
.	O
based	O
on	O
figure	O
6.2	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
smoothingkernelplot	O
.	O
14.7	O
kernels	O
for	O
building	O
generative	O
models	O
there	O
is	O
a	O
different	O
kind	O
of	O
kernel	B
known	O
as	O
a	O
smoothing	B
kernel	I
which	O
can	O
be	O
used	O
to	O
create	O
non-parametric	O
density	O
estimates	O
.	O
this	O
can	O
be	O
used	O
for	O
unsupervised	O
density	O
estimation	O
,	O
p	O
(	O
x	O
)	O
,	O
as	O
well	O
as	O
for	O
creating	O
generative	O
models	O
for	O
classiﬁcation	B
and	O
regression	B
by	O
making	O
models	O
of	O
the	O
form	O
p	O
(	O
y	O
,	O
x	O
)	O
.	O
14.7.1	O
smoothing	O
kernels	O
a	O
smoothing	B
kernel	I
is	O
a	O
function	O
of	O
one	O
argument	O
which	O
satisﬁes	O
the	O
following	O
properties	O
:	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
κ	O
(	O
x	O
)	O
dx	O
=	O
1	O
,	O
xκ	O
(	O
x	O
)	O
dx	O
=	O
0	O
,	O
x2κ	O
(	O
x	O
)	O
dx	O
>	O
0	O
a	O
simple	O
example	O
is	O
the	O
gaussian	O
kernel	B
,	O
κ	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
1	O
(	O
2π	O
)	O
1	O
2	O
e−x2/2	O
we	O
can	O
control	O
the	O
width	O
of	O
the	O
kernel	B
by	O
introducing	O
a	O
bandwidth	B
parameter	O
h	O
:	O
κh	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
1	O
h	O
κ	O
(	O
x	O
h	O
)	O
we	O
can	O
generalize	B
to	O
vector	O
valued	O
inputs	O
by	O
deﬁning	O
an	O
rbf	O
kernel	B
:	O
κh	O
(	O
x	O
)	O
=	O
κh	O
(	O
||x||	O
)	O
in	O
the	O
case	O
of	O
the	O
gaussian	O
kernel	B
,	O
this	O
becomes	O
κh	O
(	O
x	O
)	O
=	O
1	O
hd	O
(	O
2π	O
)	O
d/2	O
j=1	O
exp	O
(	O
−	O
1	O
2h2	O
x2	O
j	O
)	O
d	O
(	O
cid:27	O
)	O
(	O
14.69	O
)	O
(	O
14.70	O
)	O
(	O
14.71	O
)	O
(	O
14.72	O
)	O
(	O
14.73	O
)	O
508	O
chapter	O
14.	O
kernels	O
although	O
gaussian	O
kernels	O
are	O
popular	O
,	O
they	O
have	O
unbounded	O
support	B
.	O
an	O
alternative	O
kernel	O
,	O
with	O
compact	O
support	B
,	O
is	O
the	O
epanechnikov	O
kernel	B
,	O
deﬁned	O
by	O
κ	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
3	O
4	O
(	O
1	O
−	O
x2	O
)	O
i	O
(	O
|x|	O
≤1	O
)	O
(	O
14.74	O
)	O
this	O
is	O
plotted	O
in	O
figure	O
14.16.	O
compact	O
support	B
can	O
be	O
useful	O
for	O
efficiency	O
reasons	O
,	O
since	O
one	O
can	O
use	O
fast	O
nearest	O
neighbor	O
methods	O
to	O
evaluate	O
the	O
density	O
.	O
unfortunately	O
,	O
the	O
epanechnikov	O
kernel	B
is	O
not	O
differentiable	O
at	O
the	O
boundary	O
of	O
its	O
support	B
.	O
an	O
alterative	O
is	O
the	O
tri-cube	B
kernel	I
,	O
deﬁned	O
as	O
follows	O
:	O
κ	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
70	O
81	O
(	O
1	O
−	O
|x|3	O
)	O
3	O
i	O
(	O
|x|	O
≤	O
1	O
)	O
(	O
14.75	O
)	O
this	O
has	O
compact	O
support	B
and	O
has	O
two	O
continuous	O
derivatives	O
at	O
the	O
boundary	O
of	O
its	O
support	B
.	O
see	O
figure	O
14.16.	O
the	O
boxcar	B
kernel	I
is	O
simply	O
the	O
uniform	B
distribution	I
:	O
κ	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
i	O
(	O
|x|	O
≤1	O
)	O
we	O
will	O
use	O
this	O
kernel	B
below	O
.	O
14.7.2	O
kernel	B
density	I
estimation	I
(	O
kde	O
)	O
(	O
14.76	O
)	O
recall	B
the	O
gaussian	O
mixture	B
model	I
from	O
section	O
11.2.1.	O
this	O
is	O
a	O
parametric	O
density	O
estimator	B
for	O
d.	O
however	O
,	O
it	O
requires	O
specifying	O
the	O
number	O
k	O
and	O
locations	O
μk	O
of	O
the	O
clusters	B
.	O
an	O
data	O
in	O
r	O
alternative	O
to	O
estimating	O
the	O
μk	O
is	O
to	O
allocate	O
one	O
cluster	O
center	O
per	O
data	O
point	O
,	O
so	O
μi	O
=	O
xi	O
.	O
in	O
this	O
case	O
,	O
the	O
model	O
becomes	O
n	O
(	O
cid:2	O
)	O
i=1	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
14.77	O
)	O
(	O
14.78	O
)	O
p	O
(	O
x|d	O
)	O
=	O
1	O
n	O
n	O
(	O
x|xi	O
,	O
σ2i	O
)	O
we	O
can	O
generalize	B
the	O
approach	O
by	O
writing	O
ˆp	O
(	O
x	O
)	O
=	O
1	O
n	O
κh	O
(	O
x	O
−	O
xi	O
)	O
this	O
is	O
called	O
a	O
parzen	O
window	O
density	O
estimator	O
,	O
or	O
kernel	B
density	I
estimator	I
(	O
kde	O
)	O
,	O
and	O
is	O
a	O
simple	O
non-parametric	O
density	O
model	O
.	O
the	O
advantage	O
over	O
a	O
parametric	B
model	I
is	O
that	O
no	O
model	O
ﬁtting	O
is	O
required	O
(	O
except	O
for	O
tuning	O
the	O
bandwidth	B
,	O
usually	O
done	O
by	O
cross-validation	O
)	O
.	O
and	O
there	O
is	O
no	O
need	O
to	O
pick	O
k.	O
the	O
disadvantage	O
is	O
that	O
the	O
model	O
takes	O
a	O
lot	O
of	O
memory	O
to	O
store	O
,	O
and	O
a	O
lot	O
of	O
time	O
to	O
evaluate	O
.	O
it	O
is	O
also	O
of	O
no	O
use	O
for	O
clustering	B
tasks	O
.	O
figure	O
14.17	O
illustrates	O
kde	O
in	O
1d	O
for	O
two	O
kinds	O
of	O
kernel	B
.	O
on	O
the	O
top	O
,	O
we	O
use	O
a	O
boxcar	B
kernel	I
,	O
κ	O
(	O
x	O
)	O
=i	O
(	O
−1	O
≤	O
z	O
≤	O
1	O
)	O
.	O
the	O
result	O
is	O
equivalent	O
to	O
a	O
histogram	B
estimate	O
of	O
the	O
density	O
,	O
since	O
we	O
just	O
count	O
how	O
many	O
data	O
points	O
land	O
within	O
an	O
interval	O
of	O
size	O
h	O
around	O
xi	O
.	O
on	O
the	O
bottom	O
,	O
we	O
use	O
a	O
gaussian	O
kernel	B
,	O
which	O
results	O
in	O
a	O
smoother	O
ﬁt	O
.	O
the	O
usual	O
way	O
to	O
pick	O
h	O
is	O
to	O
minimize	O
an	O
estimate	O
(	O
such	O
as	O
cross	B
validation	I
)	O
of	O
the	O
frequentist	B
risk	O
(	O
see	O
e.g.	O
,	O
(	O
bowman	O
and	O
azzalini	O
1997	O
)	O
)	O
.	O
in	O
section	O
25.2	O
,	O
we	O
discuss	O
a	O
bayesian	O
approach	O
to	O
non-parametric	O
density	O
estimation	O
,	O
based	O
on	O
dirichlet	O
process	O
mixture	B
models	O
,	O
which	O
allows	O
us	O
14.7.	O
kernels	O
for	O
building	O
generative	O
models	O
509	O
unif	O
,	O
h=1.000	O
unif	O
,	O
h=2.000	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
−5	O
0.16	O
0.14	O
0.12	O
0.1	O
0.08	O
0.06	O
0.04	O
0.02	O
0	O
−5	O
0.14	O
0.12	O
0.1	O
0.08	O
0.06	O
0.04	O
0.02	O
0	O
−5	O
0.06	O
0.05	O
0.04	O
0.03	O
0.02	O
0.01	O
0	O
−5	O
0	O
5	O
10	O
(	O
a	O
)	O
gauss	O
,	O
h=1.000	O
0	O
5	O
10	O
(	O
c	O
)	O
0	O
5	O
10	O
(	O
b	O
)	O
gauss	O
,	O
h=2.000	O
0	O
5	O
10	O
(	O
d	O
)	O
figure	O
14.17	O
a	O
nonparametric	O
(	O
parzen	O
)	O
density	O
estimator	O
in	O
1d	O
estimated	O
from	O
6	O
data	O
points	O
,	O
denoted	O
by	O
x.	O
top	O
row	O
:	O
uniform	O
kernel	O
.	O
bottom	O
row	O
:	O
gaussian	O
kernel	B
.	O
rows	O
represent	O
increasingly	O
large	O
band-	O
width	O
parameters	O
.	O
based	O
on	O
http	O
:	O
//en.wikipedia.org/wiki/kernel_density_estimation	O
.	O
figure	O
generated	O
by	O
parzenwindowdemo2	O
.	O
to	O
infer	O
h.	O
dp	O
mixtures	O
can	O
also	O
be	O
more	O
efficient	O
than	O
kde	O
,	O
since	O
they	O
do	O
not	O
need	O
to	O
store	O
all	O
the	O
data	O
.	O
see	O
also	O
section	O
15.2.4	O
where	O
we	O
discuss	O
an	O
empirical	O
bayes	O
approach	O
to	O
estimating	O
kernel	B
parameters	O
in	O
a	O
gaussian	O
process	O
model	O
for	O
classiﬁcation/	O
regression	B
.	O
14.7.3	O
from	O
kde	O
to	O
knn	O
we	O
can	O
use	O
kde	O
to	O
deﬁne	O
the	O
class	O
conditional	O
densities	O
in	O
a	O
generative	B
classiﬁer	I
.	O
this	O
turns	O
out	O
to	O
provide	O
an	O
alternative	O
derivation	O
of	O
the	O
nearest	O
neighbors	O
classiﬁer	O
,	O
which	O
we	O
introduced	O
in	O
section	O
1.4.2.	O
to	O
show	O
this	O
,	O
we	O
follow	O
the	O
presentation	O
of	O
(	O
bishop	O
2006a	O
,	O
p125	O
)	O
.	O
in	O
kde	O
with	O
a	O
boxcar	B
kernel	I
,	O
we	O
ﬁxed	O
the	O
bandwidth	B
and	O
count	O
how	O
many	O
data	O
points	O
fall	O
within	O
the	O
hyper-cube	O
centered	O
on	O
a	O
datapoint	O
.	O
suppose	O
that	O
,	O
instead	O
of	O
ﬁxing	O
the	O
bandwidth	B
h	O
,	O
we	O
instead	O
510	O
chapter	O
14.	O
kernels	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−2	O
gaussian	O
kernel	B
regression	I
true	O
data	O
estimate	O
−1.5	O
−1	O
−0.5	O
0	O
0.5	O
1	O
1.5	O
2	O
figure	O
14.18	O
an	O
example	O
of	O
kernel	B
regression	I
in	O
1d	O
using	O
a	O
gaussian	O
kernel	B
.	O
kernelregressiondemo	O
,	O
based	O
on	O
code	O
by	O
yi	O
cao	O
.	O
figure	O
generated	O
by	O
allow	O
the	O
bandwidth	B
or	O
volume	O
to	O
be	O
different	O
for	O
each	O
data	O
point	O
.	O
speciﬁcally	O
,	O
we	O
will	O
“	O
grow	O
”	O
a	O
volume	O
around	O
x	O
until	O
we	O
encounter	O
k	O
data	O
points	O
,	O
regardless	O
of	O
their	O
class	O
label	O
.	O
let	O
the	O
resulting	O
volume	O
have	O
size	O
v	O
(	O
x	O
)	O
(	O
this	O
was	O
previously	O
hd	O
)	O
,	O
and	O
let	O
there	O
be	O
nc	O
(	O
x	O
)	O
examples	O
from	O
class	O
c	O
in	O
this	O
volume	O
.	O
then	O
we	O
can	O
estimate	O
the	O
class	O
conditional	O
density	O
as	O
follows	O
:	O
p	O
(	O
x|y	O
=	O
c	O
,	O
d	O
)	O
=	O
nc	O
(	O
x	O
)	O
ncv	O
(	O
x	O
)	O
(	O
14.79	O
)	O
where	O
nc	O
is	O
the	O
total	O
number	O
of	O
examples	O
in	O
class	O
c	O
in	O
the	O
whole	O
data	O
set	O
.	O
the	O
class	O
prior	O
can	O
be	O
estimated	O
by	O
p	O
(	O
y	O
=	O
c|d	O
)	O
=	O
(	O
14.80	O
)	O
nc	O
n	O
hence	O
the	O
class	O
posterior	O
is	O
given	O
by	O
p	O
(	O
y	O
=	O
c|x	O
,	O
d	O
)	O
=	O
(	O
cid:10	O
)	O
nc	O
(	O
x	O
)	O
ncv	O
(	O
x	O
)	O
nc	O
n	O
nc	O
(	O
cid:2	O
)	O
(	O
x	O
)	O
nc	O
(	O
cid:2	O
)	O
v	O
(	O
x	O
)	O
c	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
(	O
cid:10	O
)	O
=	O
nc	O
(	O
x	O
)	O
c	O
(	O
cid:2	O
)	O
nc	O
(	O
cid:2	O
)	O
(	O
x	O
)	O
=	O
nc	O
(	O
x	O
)	O
k	O
(	O
14.81	O
)	O
nc	O
(	O
cid:2	O
)	O
n	O
where	O
we	O
used	O
the	O
fact	O
that	O
class	O
)	O
around	O
every	O
point	O
.	O
this	O
is	O
equivalent	O
to	O
equation	O
1.2	O
,	O
since	O
nc	O
(	O
x	O
)	O
=	O
c	O
)	O
.	O
c	O
nc	O
(	O
x	O
)	O
=	O
k	O
,	O
since	O
we	O
choose	O
a	O
total	O
of	O
k	O
points	O
(	O
regardless	O
of	O
i∈nk	O
(	O
x	O
,	O
d	O
)	O
i	O
(	O
yi	O
=	O
(	O
cid:10	O
)	O
14.7.4	O
kernel	B
regression	I
in	O
section	O
14.7.2	O
,	O
we	O
discussed	O
the	O
use	O
of	O
kernel	B
density	I
estimation	I
or	O
kde	O
for	O
unsupervised	B
learning	I
.	O
we	O
can	O
also	O
use	O
kde	O
for	O
regression	B
.	O
the	O
goal	O
is	O
to	O
compute	O
the	O
conditional	O
expectation	O
(	O
cid:28	O
)	O
f	O
(	O
x	O
)	O
=	O
e	O
[	O
y|x	O
]	O
=	O
y	O
p	O
(	O
y|x	O
)	O
dy	O
=	O
)	O
)	O
y	O
p	O
(	O
x	O
,	O
y	O
)	O
dy	O
p	O
(	O
x	O
,	O
y	O
)	O
dy	O
(	O
14.82	O
)	O
14.7.	O
kernels	O
for	O
building	O
generative	O
models	O
we	O
can	O
use	O
kde	O
to	O
approximate	O
the	O
joint	O
density	O
p	O
(	O
x	O
,	O
y	O
)	O
as	O
follows	O
:	O
p	O
(	O
x	O
,	O
y	O
)	O
≈	O
1	O
n	O
κh	O
(	O
x	O
−	O
xi	O
)	O
κh	O
(	O
y	O
−	O
yi	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
1	O
n	O
1	O
n	O
)	O
(	O
cid:10	O
)	O
n	O
i=1	O
κh	O
(	O
x	O
−	O
xi	O
)	O
)	O
(	O
cid:10	O
)	O
n	O
i=1	O
κh	O
(	O
x	O
−	O
xi	O
)	O
(	O
cid:10	O
)	O
n	O
i=1	O
κh	O
(	O
x	O
−	O
xi	O
)	O
yi	O
(	O
cid:10	O
)	O
n	O
i=1	O
κh	O
(	O
x	O
−	O
xi	O
)	O
yκh	O
(	O
y	O
−	O
yi	O
)	O
dy	O
κh	O
(	O
y	O
−	O
yi	O
)	O
dy	O
)	O
hence	O
f	O
(	O
x	O
)	O
=	O
=	O
)	O
(	O
cid:28	O
)	O
511	O
(	O
14.83	O
)	O
(	O
14.84	O
)	O
(	O
14.85	O
)	O
to	O
derive	O
this	O
result	O
,	O
we	O
used	O
two	O
properties	O
of	O
smoothing	O
kernels	O
.	O
first	O
,	O
that	O
they	O
integrate	O
to	O
yκh	O
(	O
y	O
−	O
yi	O
)	O
dy	O
=	O
yi	O
.	O
this	O
follows	O
by	O
one	O
,	O
i.e.	O
,	O
deﬁning	O
x	O
=	O
y	O
−	O
yi	O
and	O
using	O
the	O
zero	O
mean	O
property	O
of	O
smoothing	O
kernels	O
:	O
κh	O
(	O
y	O
−	O
yi	O
)	O
dy	O
=	O
1.	O
and	O
second	O
,	O
the	O
fact	O
that	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
(	O
x	O
+	O
yi	O
)	O
κh	O
(	O
x	O
)	O
dx	O
=	O
xκh	O
(	O
x	O
)	O
dx	O
+	O
yi	O
κh	O
(	O
x	O
)	O
dx	O
=	O
0	O
+	O
yi	O
=	O
yi	O
(	O
14.86	O
)	O
we	O
can	O
rewrite	O
the	O
above	O
result	O
as	O
follows	O
:	O
n	O
(	O
cid:2	O
)	O
f	O
(	O
x	O
)	O
=	O
wi	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
i=1	O
wi	O
(	O
x	O
)	O
yi	O
κh	O
(	O
x	O
−	O
xi	O
)	O
(	O
cid:10	O
)	O
n	O
i	O
(	O
cid:2	O
)	O
=1	O
κh	O
(	O
x	O
−	O
xi	O
(	O
cid:2	O
)	O
)	O
(	O
14.87	O
)	O
(	O
14.88	O
)	O
we	O
see	O
that	O
the	O
prediction	O
is	O
just	O
a	O
weighted	O
sum	O
of	O
the	O
outputs	O
at	O
the	O
training	O
points	O
,	O
where	O
the	O
weights	O
depend	O
on	O
how	O
similar	B
x	O
is	O
to	O
the	O
stored	O
training	O
points	O
.	O
this	O
method	O
is	O
called	O
kernel	B
regression	I
,	O
kernel	B
smoothing	I
,	O
or	O
the	O
nadaraya-watson	O
model	O
.	O
see	O
figure	O
14.18	O
for	O
an	O
example	O
,	O
where	O
we	O
use	O
a	O
gaussian	O
kernel	B
.	O
note	O
that	O
this	O
method	O
only	O
has	O
one	O
free	O
parameter	O
,	O
namely	O
h.	O
one	O
can	O
show	O
(	O
bowman	O
and	O
azzalini	O
1997	O
)	O
that	O
for	O
1d	O
data	O
,	O
if	O
the	O
true	O
density	O
is	O
gaussian	O
and	O
we	O
are	O
using	O
gaussian	O
kernels	O
,	O
the	O
optimal	O
bandwidth	O
h	O
is	O
given	O
by	O
ˆσ	O
(	O
14.89	O
)	O
we	O
can	O
compute	O
a	O
robust	B
approximation	O
to	O
the	O
standard	B
deviation	I
by	O
ﬁrst	O
computing	O
the	O
mean	B
absolute	I
deviation	I
mad	O
=	O
median	B
(	O
|x	O
−	O
median	B
(	O
x	O
)	O
|	O
)	O
and	O
then	O
using	O
ˆσ	O
=	O
1.4826	O
mad	O
=	O
1	O
0.6745	O
mad	O
the	O
code	O
used	O
to	O
produce	O
figure	O
14.18	O
estimated	O
hx	O
and	O
hy	O
separately	O
,	O
and	O
then	O
set	O
h	O
=	O
(	O
14.90	O
)	O
(	O
14.91	O
)	O
(	O
cid:17	O
)	O
hxhy	O
.	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
1/5	O
h	O
=	O
4	O
3n	O
512	O
chapter	O
14.	O
kernels	O
although	O
these	O
heuristics	B
seem	O
to	O
work	O
well	O
,	O
their	O
derivation	O
rests	O
on	O
some	O
rather	O
dubious	O
assumptions	O
(	O
such	O
as	O
gaussianity	O
of	O
the	O
true	O
density	O
)	O
.	O
furthermore	O
,	O
these	O
heuristics	B
are	O
limited	O
to	O
tuning	O
just	O
a	O
single	O
parameter	O
.	O
in	O
section	O
15.2.4	O
we	O
discuss	O
an	O
empirical	O
bayes	O
approach	O
to	O
estimating	O
multiple	O
kernel	O
parameters	O
in	O
a	O
gaussian	O
process	O
model	O
for	O
classiﬁcation/	O
regression	B
,	O
which	O
can	O
handle	O
many	O
tuning	O
parameters	O
,	O
and	O
which	O
is	O
based	O
on	O
much	O
more	O
transparent	O
principles	O
(	O
maximizing	O
the	O
marginal	B
likelihood	I
)	O
.	O
14.7.5	O
locally	B
weighted	I
regression	I
if	O
we	O
deﬁne	O
κh	O
(	O
x−	O
xi	O
)	O
=	O
κ	O
(	O
x	O
,	O
xi	O
)	O
,	O
we	O
can	O
rewrite	O
the	O
prediction	O
made	O
by	O
kernel	B
regression	I
as	O
follows	O
n	O
(	O
cid:2	O
)	O
i=1	O
n	O
(	O
cid:2	O
)	O
i=1	O
ˆf	O
(	O
x∗	O
)	O
=	O
yi	O
(	O
cid:10	O
)	O
n	O
κ	O
(	O
x∗	O
,	O
xi	O
)	O
i	O
(	O
cid:2	O
)	O
=1	O
κ	O
(	O
x∗	O
,	O
xi	O
(	O
cid:2	O
)	O
)	O
note	O
that	O
κ	O
(	O
x	O
,	O
xi	O
)	O
need	O
not	O
be	O
a	O
smoothing	B
kernel	I
.	O
normalization	O
term	O
,	O
so	O
we	O
can	O
just	O
write	O
(	O
14.92	O
)	O
if	O
it	O
is	O
not	O
,	O
we	O
no	O
longer	O
need	O
the	O
ˆf	O
(	O
x∗	O
)	O
=	O
yiκ	O
(	O
x∗	O
,	O
xi	O
)	O
(	O
14.93	O
)	O
this	O
model	O
is	O
essentially	O
ﬁtting	O
a	O
constant	O
function	O
locally	O
.	O
we	O
can	O
improve	O
on	O
this	O
by	O
ﬁtting	O
a	O
linear	B
regression	I
model	O
for	O
each	O
point	O
x∗	O
by	O
solving	O
κ	O
(	O
x∗	O
,	O
xi	O
)	O
[	O
yi	O
−	O
β	O
(	O
x∗	O
)	O
t	O
φ	O
(	O
xi	O
)	O
]	O
2	O
(	O
14.94	O
)	O
n	O
(	O
cid:2	O
)	O
min	O
β	O
(	O
x∗	O
)	O
i=1	O
where	O
φ	O
(	O
x	O
)	O
=	O
[	O
1	O
,	O
x	O
]	O
.	O
this	O
is	O
called	O
locally	B
weighted	I
regression	I
.	O
an	O
example	O
of	O
such	O
a	O
method	O
is	O
loess	O
,	O
akalowess	O
,	O
which	O
stands	O
for	O
“	O
locally-weighted	O
scatterplot	O
smoothing	O
”	O
(	O
cleveland	O
and	O
devlin	O
1988	O
)	O
.	O
see	O
also	O
(	O
edakunni	O
et	O
al	O
.	O
2010	O
)	O
for	O
a	O
bayesian	O
version	O
of	O
this	O
model	O
.	O
we	O
can	O
compute	O
the	O
paramters	O
β	O
(	O
x∗	O
)	O
for	O
each	O
test	O
case	O
by	O
solving	O
the	O
following	O
weighted	B
least	I
squares	I
problem	I
:	O
β	O
(	O
x∗	O
)	O
=	O
(	O
φt	O
d	O
(	O
x∗	O
)	O
φ	O
)	O
(	O
14.95	O
)	O
where	O
φ	O
is	O
an	O
n	O
×	O
(	O
d	O
+	O
1	O
)	O
design	B
matrix	I
and	O
d	O
=	O
diag	O
(	O
κ	O
(	O
x∗	O
,	O
xi	O
)	O
)	O
.	O
the	O
corresponding	O
prediction	O
has	O
the	O
form	O
−1φt	O
d	O
(	O
x∗	O
)	O
y	O
ˆf	O
(	O
x∗	O
)	O
=	O
φ	O
(	O
x∗	O
)	O
t	O
β	O
(	O
x∗	O
)	O
=	O
(	O
φt	O
d	O
(	O
x∗	O
)	O
φ	O
)	O
−1φt	O
d	O
(	O
x∗	O
)	O
y	O
=	O
wi	O
(	O
x∗	O
)	O
yi	O
(	O
14.96	O
)	O
the	O
term	O
wi	O
(	O
x∗	O
)	O
,	O
which	O
combines	O
the	O
local	O
smoothing	O
kernel	B
with	O
the	O
effect	O
of	O
linear	B
regression	I
,	O
is	O
called	O
the	O
equivalent	B
kernel	I
.	O
see	O
also	O
section	O
15.4.2.	O
i=1	O
exercises	O
exercise	O
14.1	O
fitting	O
an	O
svm	O
classiﬁer	O
by	O
hand	O
(	O
source	O
:	O
jaakkola	O
.	O
)	O
consider	O
a	O
dataset	O
with	O
2	O
points	O
in	O
1d	O
:	O
(	O
x1	O
=	O
0	O
,	O
y1	O
=	O
−1	O
)	O
and	O
(	O
x2	O
=	O
consider	O
mapping	O
each	O
point	O
to	O
3d	O
using	O
the	O
feature	O
vector	O
φ	O
(	O
x	O
)	O
=	O
[	O
1	O
,	O
√	O
2	O
,	O
y2	O
=	O
1	O
)	O
.	O
2x	O
,	O
x2	O
]	O
t	O
.	O
(	O
this	O
is	O
equivalent	O
to	O
√	O
n	O
(	O
cid:2	O
)	O
14.7.	O
kernels	O
for	O
building	O
generative	O
models	O
using	O
a	O
second	B
order	I
polynomial	O
kernel	B
.	O
)	O
the	O
max	O
margin	O
classiﬁer	O
has	O
the	O
form	O
min||w||2	O
s.t	O
.	O
y1	O
(	O
wt	O
φ	O
(	O
x1	O
)	O
+w	O
0	O
)	O
≥	O
1	O
y2	O
(	O
wt	O
φ	O
(	O
x2	O
)	O
+w	O
0	O
)	O
≥	O
1	O
513	O
(	O
14.97	O
)	O
(	O
14.98	O
)	O
(	O
14.99	O
)	O
a.	O
write	O
down	O
a	O
vector	O
that	O
is	O
parallel	O
to	O
the	O
optimal	O
vector	O
w.	O
hint	O
:	O
recall	B
from	O
figure	O
7.8	O
(	O
12apr10	O
version	O
)	O
that	O
w	O
is	O
perpendicular	O
to	O
the	O
decision	B
boundary	I
between	O
the	O
two	O
points	O
in	O
the	O
3d	O
feature	O
space	O
.	O
b.	O
what	O
is	O
the	O
value	O
of	O
the	O
margin	B
that	O
is	O
achieved	O
by	O
this	O
w	O
?	O
hint	O
:	O
recall	B
that	O
the	O
margin	B
is	O
the	O
distance	O
from	O
each	O
support	O
vector	O
to	O
the	O
decision	B
boundary	I
.	O
hint	O
2	O
:	O
think	O
about	O
the	O
geometry	O
of	O
2	O
points	O
in	O
space	O
,	O
with	O
a	O
line	O
separating	O
one	O
from	O
the	O
other	O
.	O
c.	O
solve	O
for	O
w	O
,	O
using	O
the	O
fact	O
the	O
margin	B
is	O
equal	O
to	O
1/||w||	O
.	O
d.	O
solve	O
for	O
w0	O
using	O
your	O
value	O
for	O
w	O
and	O
equations	O
14.97	O
to	O
14.99.	O
hint	O
:	O
decision	B
boundary	I
,	O
so	O
the	O
inequalities	O
will	O
be	O
tight	O
.	O
the	O
points	O
will	O
be	O
on	O
the	O
e.	O
write	O
down	O
the	O
form	O
of	O
the	O
discriminant	B
function	I
f	O
(	O
x	O
)	O
=	O
w0	O
+	O
wt	O
φ	O
(	O
x	O
)	O
as	O
an	O
explicit	O
function	O
of	O
x.	O
exercise	O
14.2	O
linear	O
separability	O
(	O
source	O
:	O
koller..	O
)	O
consider	O
ﬁtting	O
an	O
svm	O
with	O
c	O
>	O
0	O
to	O
a	O
dataset	O
that	O
is	O
linearly	B
separable	I
.	O
is	O
the	O
resulting	O
decision	B
boundary	I
guaranteed	O
to	O
separate	O
the	O
classes	O
?	O
15	O
gaussian	O
processes	O
15.1	O
introduction	O
in	O
supervised	B
learning	I
,	O
we	O
observe	O
some	O
inputs	O
xi	O
and	O
some	O
outputs	O
yi	O
.	O
we	O
assume	O
that	O
yi	O
=	O
f	O
(	O
xi	O
)	O
,	O
for	O
some	O
unknown	B
function	O
f	O
,	O
possibly	O
corrupted	O
by	O
noise	O
.	O
the	O
optimal	O
approach	O
is	O
to	O
infer	O
a	O
distribution	O
over	O
functions	O
given	O
the	O
data	O
,	O
p	O
(	O
f|x	O
,	O
y	O
)	O
,	O
and	O
then	O
to	O
use	O
this	O
to	O
make	O
predictions	O
given	O
new	O
inputs	O
,	O
i.e.	O
,	O
to	O
compute	O
(	O
cid:28	O
)	O
p	O
(	O
y∗|x∗	O
,	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
y∗|f	O
,	O
x∗	O
)	O
p	O
(	O
f|x	O
,	O
y	O
)	O
df	O
(	O
15.1	O
)	O
up	O
until	O
now	O
,	O
we	O
have	O
focussed	O
on	O
parametric	O
representations	O
for	O
the	O
function	O
f	O
,	O
so	O
that	O
in	O
this	O
chapter	O
,	O
we	O
discuss	O
a	O
way	O
to	O
perform	O
instead	O
of	O
inferring	O
p	O
(	O
f|d	O
)	O
,	O
we	O
infer	O
p	O
(	O
θ|d	O
)	O
.	O
bayesian	O
inference	B
over	O
functions	O
themselves	O
.	O
our	O
approach	O
will	O
be	O
based	O
on	O
gaussian	O
processes	O
or	O
gps	O
.	O
a	O
gp	O
deﬁnes	O
a	O
prior	O
over	O
functions	O
,	O
which	O
can	O
be	O
converted	O
into	O
a	O
posterior	O
over	O
functions	O
once	O
we	O
have	O
seen	O
some	O
data	O
.	O
although	O
it	O
might	O
seem	O
difficult	O
to	O
represent	O
a	O
distribution	O
over	O
a	O
function	O
,	O
it	O
turns	O
out	O
that	O
we	O
only	O
need	O
to	O
be	O
able	O
to	O
deﬁne	O
a	O
distribution	O
over	O
the	O
function	O
’	O
s	O
values	O
at	O
a	O
ﬁnite	O
,	O
but	O
arbitrary	O
,	O
set	O
of	O
points	O
,	O
say	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
a	O
gp	O
assumes	O
that	O
p	O
(	O
f	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
f	O
(	O
xn	O
)	O
)	O
is	O
jointly	O
gaussian	O
,	O
with	O
some	O
mean	B
μ	O
(	O
x	O
)	O
and	O
covariance	B
σ	O
(	O
x	O
)	O
given	O
by	O
σij	O
=	O
κ	O
(	O
xi	O
,	O
xj	O
)	O
,	O
where	O
κ	O
is	O
a	O
positive	B
deﬁnite	I
kernel	I
function	O
(	O
see	O
section	O
14.2	O
information	B
on	O
kernels	O
)	O
.	O
the	O
key	O
idea	O
is	O
that	O
if	O
xi	O
and	O
xj	O
are	O
deemed	O
by	O
the	O
kernel	B
to	O
be	O
similar	B
,	O
then	O
we	O
expect	O
the	O
output	O
of	O
the	O
function	O
at	O
those	O
points	O
to	O
be	O
similar	B
,	O
too	O
.	O
see	O
figure	O
15.1	O
for	O
an	O
illustration	O
.	O
it	O
turns	O
out	O
that	O
,	O
in	O
the	O
regression	B
setting	O
,	O
all	O
these	O
computations	O
can	O
be	O
done	O
in	O
closed	O
form	O
,	O
in	O
o	O
(	O
n	O
3	O
)	O
time	O
.	O
(	O
we	O
discuss	O
faster	O
approximations	O
in	O
section	O
15.6	O
.	O
)	O
in	O
the	O
classiﬁcation	B
setting	O
,	O
we	O
must	O
use	O
approximations	O
,	O
such	O
as	O
the	O
gaussian	O
approximation	O
,	O
since	O
the	O
posterior	O
is	O
no	O
longer	O
exactly	O
gaussian	O
.	O
gps	O
can	O
be	O
thought	O
of	O
as	O
a	O
bayesian	O
alternative	O
to	O
the	O
kernel	B
methods	O
we	O
discussed	O
in	O
chap-	O
ter	O
14	O
,	O
including	O
l1vm	O
,	O
rvm	O
and	O
svm	O
.	O
although	O
those	O
methods	O
are	O
sparser	O
and	O
therefore	O
faster	O
,	O
they	O
do	O
not	O
give	O
well-calibrated	O
probabilistic	O
outputs	O
(	O
see	O
section	O
15.4.4	O
for	O
further	O
discussion	O
)	O
.	O
having	O
properly	O
tuned	O
probabilistic	O
output	O
is	O
important	O
in	O
certain	O
applications	O
,	O
such	O
as	O
online	O
tracking	O
for	O
vision	O
and	O
robotics	O
(	O
ko	O
and	O
fox	O
2009	O
)	O
,	O
reinforcement	B
learning	I
and	O
optimal	O
control	O
(	O
engel	O
et	O
al	O
.	O
2005	O
;	O
deisenroth	O
et	O
al	O
.	O
2009	O
)	O
,	O
global	O
optimization	O
of	O
non-convex	O
functions	O
(	O
mockus	O
et	O
al	O
.	O
1996	O
;	O
lizotte	O
2008	O
;	O
brochu	O
et	O
al	O
.	O
2009	O
)	O
,	O
experiment	O
design	O
(	O
santner	O
et	O
al	O
.	O
2003	O
)	O
,	O
etc	O
.	O
516	O
chapter	O
15.	O
gaussian	O
processes	O
y1	O
f	O
1	O
x1	O
y2	O
f	O
2	O
x2	O
y	O
(	O
cid:2	O
)	O
f	O
(	O
cid:2	O
)	O
x	O
(	O
cid:2	O
)	O
(	O
cid:14	O
)	O
figure	O
15.1	O
a	O
gaussian	O
process	O
for	O
2	O
training	O
points	O
and	O
1	O
testing	O
point	O
,	O
represented	O
as	O
a	O
mixed	O
directed	O
and	O
undirected	B
graphical	I
model	I
representing	O
p	O
(	O
y	O
,	O
f|x	O
)	O
=n	O
(	O
f|0	O
,	O
k	O
(	O
x	O
)	O
)	O
i	O
p	O
(	O
yi|fi	O
)	O
.	O
the	O
hidden	B
nodes	I
fi	O
=	O
f	O
(	O
xi	O
)	O
represent	O
the	O
value	O
of	O
the	O
function	O
at	O
each	O
of	O
the	O
data	O
points	O
.	O
these	O
hidden	B
nodes	I
are	O
fully	O
interconnected	O
by	O
undirected	B
edges	O
,	O
forming	O
a	O
gaussian	O
graphical	B
model	I
;	O
the	O
edge	O
strengths	O
represent	O
the	O
covariance	B
terms	O
σij	O
=	O
κ	O
(	O
xi	O
,	O
xj	O
)	O
.	O
if	O
the	O
test	O
point	O
x∗	O
is	O
similar	B
to	O
the	O
training	O
points	O
x1	O
and	O
x2	O
,	O
then	O
the	O
predicted	O
output	O
y∗	O
will	O
be	O
similar	B
to	O
y1	O
and	O
y2	O
.	O
our	O
presentation	O
is	O
closely	O
based	O
on	O
(	O
rasmussen	O
and	O
williams	O
2006	O
)	O
,	O
which	O
should	O
be	O
con-	O
sulted	O
for	O
futher	O
details	O
.	O
see	O
also	O
(	O
diggle	O
and	O
ribeiro	O
2007	O
)	O
,	O
which	O
discusses	O
the	O
related	O
approach	O
known	O
as	O
kriging	B
,	O
which	O
is	O
widely	O
used	O
in	O
the	O
spatial	O
statistics	O
literature	O
.	O
15.2	O
gps	O
for	O
regression	B
in	O
this	O
section	O
,	O
we	O
discuss	O
gps	O
for	O
regression	B
.	O
let	O
the	O
prior	O
on	O
the	O
regression	B
function	O
be	O
a	O
gp	O
,	O
denoted	O
by	O
f	O
(	O
x	O
)	O
∼	O
gp	O
(	O
m	O
(	O
x	O
)	O
,	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
)	O
where	O
m	O
(	O
x	O
)	O
is	O
the	O
mean	B
function	I
and	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
is	O
the	O
kernel	B
or	O
covariance	B
function	O
,	O
i.e.	O
,	O
m	O
(	O
x	O
)	O
=e	O
[	O
f	O
(	O
x	O
)	O
]	O
(	O
cid:31	O
)	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=e	O
(	O
f	O
(	O
x	O
)	O
−	O
m	O
(	O
x	O
)	O
)	O
(	O
f	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
−	O
m	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
)	O
t	O
(	O
15.2	O
)	O
(	O
15.3	O
)	O
(	O
15.4	O
)	O
(	O
15.5	O
)	O
we	O
obviously	O
require	O
that	O
κ	O
(	O
)	O
be	O
a	O
positive	B
deﬁnite	I
kernel	I
.	O
for	O
any	O
ﬁnite	O
set	O
of	O
points	O
,	O
this	O
process	O
deﬁnes	O
a	O
joint	O
gaussian	O
:	O
p	O
(	O
f|x	O
)	O
=	O
n	O
(	O
f|μ	O
,	O
k	O
)	O
where	O
kij	O
=	O
κ	O
(	O
xi	O
,	O
xj	O
)	O
and	O
μ	O
=	O
(	O
m	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
m	O
(	O
xn	O
)	O
)	O
.	O
note	O
that	O
it	O
is	O
common	O
to	O
use	O
a	O
mean	B
function	I
of	O
m	O
(	O
x	O
)	O
=	O
0	O
,	O
since	O
the	O
gp	O
is	O
ﬂexible	O
enough	O
to	O
model	O
the	O
mean	B
arbitrarily	O
well	O
,	O
as	O
we	O
will	O
see	O
below	O
.	O
however	O
,	O
in	O
section	O
15.2.6	O
we	O
will	O
consider	O
parametric	O
models	O
for	O
the	O
mean	B
function	I
,	O
so	O
the	O
gp	O
just	O
has	O
to	O
model	O
the	O
residual	B
errors	O
.	O
this	O
semi-parametric	O
approach	O
combines	O
the	O
interpretability	O
of	O
parametric	O
models	O
with	O
the	O
accuracy	O
of	O
non-parametric	O
models	O
.	O
15.2.	O
gps	O
for	O
regression	B
517	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−2	O
−5	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−2	O
−5	O
0	O
(	O
a	O
)	O
5	O
0	O
(	O
b	O
)	O
5	O
figure	O
15.2	O
left	O
:	O
some	O
functions	O
sampled	O
from	O
a	O
gp	O
prior	O
with	O
se	O
kernel	B
.	O
right	O
:	O
some	O
samples	B
from	O
a	O
gp	O
posterior	O
,	O
after	O
conditioning	B
on	O
5	O
noise-free	O
observations	O
.	O
the	O
shaded	O
area	O
represents	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
±2std	O
(	O
f	O
(	O
x	O
)	O
.	O
based	O
on	O
figure	O
2.2	O
of	O
(	O
rasmussen	O
and	O
williams	O
2006	O
)	O
.	O
figure	O
generated	O
by	O
gprdemonoisefree	O
.	O
15.2.1	O
predictions	O
using	O
noise-free	O
observations	O
suppose	O
we	O
observe	O
a	O
training	B
set	I
d	O
=	O
{	O
(	O
xi	O
,	O
fi	O
)	O
,	O
i	O
=	O
1	O
:	O
n	O
}	O
,	O
where	O
fi	O
=	O
f	O
(	O
xi	O
)	O
is	O
the	O
noise-free	O
observation	B
of	O
the	O
function	O
evaluated	O
at	O
xi	O
.	O
given	O
a	O
test	O
set	O
x∗	O
of	O
size	O
n∗	O
×	O
d	O
,	O
we	O
want	O
to	O
predict	O
the	O
function	O
outputs	O
f∗	O
.	O
if	O
we	O
ask	O
the	O
gp	O
to	O
predict	O
f	O
(	O
x	O
)	O
for	O
a	O
value	O
of	O
x	O
that	O
it	O
has	O
already	O
seen	O
,	O
we	O
want	O
the	O
gp	O
to	O
return	O
the	O
answer	O
f	O
(	O
x	O
)	O
with	O
no	O
uncertainty	O
.	O
in	O
other	O
words	O
,	O
it	O
should	O
act	O
as	O
an	O
interpolator	B
of	O
the	O
training	O
data	O
.	O
this	O
will	O
only	O
happen	O
if	O
we	O
assume	O
the	O
observations	O
are	O
noiseless	O
.	O
we	O
will	O
consider	O
the	O
case	O
of	O
noisy	O
observations	O
below	O
.	O
now	O
we	O
return	O
to	O
the	O
prediction	O
problem	O
.	O
by	O
deﬁnition	O
of	O
the	O
gp	O
,	O
the	O
joint	B
distribution	I
has	O
the	O
following	O
form	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
f	O
f∗	O
∼	O
n	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
μ	O
μ∗	O
(	O
cid:8	O
)	O
k	O
k∗	O
kt∗	O
k∗∗	O
,	O
(	O
cid:9	O
)	O
(	O
cid:9	O
)	O
(	O
15.6	O
)	O
where	O
k	O
=	O
κ	O
(	O
x	O
,	O
x	O
)	O
is	O
n	O
×n	O
,	O
k∗	O
=	O
κ	O
(	O
x	O
,	O
x∗	O
)	O
is	O
n	O
×n∗	O
,	O
and	O
k∗∗	O
=	O
κ	O
(	O
x∗	O
,	O
x∗	O
)	O
is	O
n∗×n∗	O
.	O
by	O
the	O
standard	O
rules	O
for	O
conditioning	B
gaussians	O
(	O
section	O
4.3	O
)	O
,	O
the	O
posterior	O
has	O
the	O
following	O
form	O
p	O
(	O
f∗|x∗	O
,	O
x	O
,	O
f	O
)	O
=n	O
(	O
f∗|μ∗	O
,	O
σ∗	O
)	O
μ∗	O
=	O
μ	O
(	O
x∗	O
)	O
+k	O
t∗	O
k−1	O
(	O
f	O
−	O
μ	O
(	O
x	O
)	O
)	O
σ∗	O
=	O
k∗∗	O
−	O
kt∗	O
k−1k∗	O
(	O
15.7	O
)	O
(	O
15.8	O
)	O
(	O
15.9	O
)	O
this	O
process	O
is	O
illustrated	O
in	O
figure	O
15.2.	O
on	O
the	O
left	O
we	O
show	O
sample	O
samples	O
from	O
the	O
prior	O
,	O
in	O
p	O
(	O
f|x	O
)	O
,	O
where	O
we	O
use	O
a	O
squared	B
exponential	I
kernel	I
,	O
aka	O
gaussian	O
kernel	B
or	O
rbf	O
kernel	B
.	O
1d	O
,	O
this	O
is	O
given	O
by	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
σ2	O
f	O
exp	O
(	O
−	O
1	O
2	O
(	O
cid:6	O
)	O
2	O
(	O
x	O
−	O
x	O
(	O
cid:4	O
)	O
)	O
2	O
)	O
(	O
15.10	O
)	O
here	O
(	O
cid:6	O
)	O
controls	O
the	O
horizontal	O
length	O
scale	O
over	O
which	O
the	O
function	O
varies	O
,	O
and	O
σ2	O
f	O
controls	O
the	O
vertical	O
variation	O
.	O
(	O
we	O
discuss	O
how	O
to	O
estimate	O
such	O
kernel	B
parameters	O
below	O
.	O
)	O
on	O
the	O
right	O
we	O
518	O
chapter	O
15.	O
gaussian	O
processes	O
show	O
samples	B
from	O
the	O
posterior	O
,	O
p	O
(	O
f∗|x∗	O
,	O
x	O
,	O
f	O
)	O
.	O
we	O
see	O
that	O
the	O
model	O
perfectly	O
interpolates	O
the	O
training	O
data	O
,	O
and	O
that	O
the	O
predictive	B
uncertainty	O
increases	O
as	O
we	O
move	O
further	O
away	O
from	O
the	O
observed	O
data	O
.	O
one	O
application	O
of	O
noise-free	O
gp	O
regression	B
is	O
as	O
a	O
computationally	O
cheap	O
proxy	O
for	O
the	O
behavior	O
of	O
a	O
complex	O
simulator	O
,	O
such	O
as	O
a	O
weather	O
forecasting	O
program	O
.	O
(	O
if	O
the	O
simulator	O
is	O
stochastic	O
,	O
we	O
can	O
deﬁne	O
f	O
to	O
be	O
its	O
mean	B
output	O
;	O
note	O
that	O
there	O
is	O
still	O
no	O
observation	O
noise	O
.	O
)	O
one	O
can	O
then	O
estimate	O
the	O
effect	O
of	O
changing	O
simulator	O
parameters	O
by	O
examining	O
their	O
effect	O
on	O
the	O
gp	O
’	O
s	O
predictions	O
,	O
rather	O
than	O
having	O
to	O
run	O
the	O
simulator	O
many	O
times	O
,	O
which	O
may	O
be	O
prohibitively	O
slow	O
.	O
this	O
strategy	O
is	O
known	O
as	O
dace	O
,	O
which	O
stands	O
for	O
design	O
and	O
analysis	O
of	O
computer	O
experiments	O
(	O
santner	O
et	O
al	O
.	O
2003	O
)	O
.	O
15.2.2	O
predictions	O
using	O
noisy	O
observations	O
now	O
let	O
us	O
consider	O
the	O
case	O
where	O
what	O
we	O
observe	O
is	O
a	O
noisy	O
version	O
of	O
the	O
underlying	O
function	O
,	O
y	O
=	O
f	O
(	O
x	O
)	O
+	O
,	O
where	O
	O
∼	O
n	O
(	O
0	O
,	O
σ2	O
y	O
)	O
.	O
in	O
this	O
case	O
,	O
the	O
model	O
is	O
not	O
required	O
to	O
interpolate	B
the	O
data	O
,	O
but	O
it	O
must	O
come	O
“	O
close	O
”	O
to	O
the	O
observed	O
data	O
.	O
the	O
covariance	B
of	O
the	O
observed	O
noisy	O
responses	O
is	O
cov	O
[	O
yp	O
,	O
yq	O
]	O
=	O
κ	O
(	O
xp	O
,	O
xq	O
)	O
+σ	O
2	O
yδpq	O
where	O
δpq	O
=	O
i	O
(	O
p	O
=	O
q	O
)	O
.	O
in	O
other	O
words	O
cov	O
[	O
y|x	O
]	O
=	O
k	O
+	O
σ2	O
yin	O
(	O
cid:2	O
)	O
ky	O
(	O
15.11	O
)	O
(	O
15.12	O
)	O
the	O
second	O
matrix	O
is	O
diagonal	B
because	O
we	O
assumed	O
the	O
noise	O
terms	O
were	O
independently	O
added	O
to	O
each	O
observation	B
.	O
the	O
joint	O
density	O
of	O
the	O
observed	O
data	O
and	O
the	O
latent	B
,	O
noise-free	O
function	O
on	O
the	O
test	O
points	O
is	O
given	O
by	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
∼	O
n	O
0	O
,	O
y	O
f∗	O
(	O
cid:9	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
ky	O
k∗	O
kt∗	O
k∗∗	O
p	O
(	O
f∗|x∗	O
,	O
x	O
,	O
y	O
)	O
=n	O
(	O
f∗|μ∗	O
,	O
σ∗	O
)	O
μ∗	O
=	O
kt∗	O
k−1	O
y	O
y	O
σ∗	O
=	O
k∗∗	O
−	O
kt∗	O
k−1	O
y	O
k∗	O
where	O
we	O
are	O
assuming	O
the	O
mean	B
is	O
zero	O
,	O
for	O
notational	O
simplicity	O
.	O
hence	O
the	O
posterior	B
predictive	I
density	I
is	O
(	O
15.13	O
)	O
(	O
15.14	O
)	O
(	O
15.15	O
)	O
(	O
15.16	O
)	O
in	O
the	O
case	O
of	O
a	O
single	O
test	O
input	O
,	O
this	O
simpliﬁes	O
as	O
follows	O
y	O
y	O
,	O
k∗∗	O
−	O
kt∗	O
k−1	O
p	O
(	O
f∗|x∗	O
,	O
x	O
,	O
y	O
)	O
=n	O
(	O
f∗|kt∗	O
k−1	O
y	O
k∗	O
)	O
(	O
15.17	O
)	O
where	O
k∗	O
=	O
[	O
κ	O
(	O
x∗	O
,	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
κ	O
(	O
x∗	O
,	O
xn	O
)	O
]	O
and	O
k∗∗	O
=	O
κ	O
(	O
x∗	O
,	O
x∗	O
)	O
.	O
another	O
way	O
to	O
write	O
the	O
posterior	B
mean	I
is	O
as	O
follows	O
:	O
n	O
(	O
cid:2	O
)	O
f∗	O
=	O
kt∗	O
k−1	O
y	O
y	O
=	O
αiκ	O
(	O
xi	O
,	O
x∗	O
)	O
(	O
15.18	O
)	O
where	O
α	O
=	O
k−1	O
y	O
y.	O
we	O
will	O
revisit	O
this	O
expression	O
later	O
.	O
i=1	O
15.2.	O
gps	O
for	O
regression	B
519	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−8	O
−6	O
−4	O
−2	O
0	O
(	O
a	O
)	O
2	O
4	O
6	O
8	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−8	O
−6	O
−4	O
−2	O
0	O
(	O
b	O
)	O
2	O
4	O
6	O
8	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−8	O
−6	O
−4	O
−2	O
0	O
(	O
c	O
)	O
2	O
4	O
6	O
8	O
figure	O
15.3	O
some	O
1d	O
gps	O
with	O
se	O
kernels	O
but	O
different	O
hyper-parameters	B
ﬁt	O
to	O
20	O
noisy	O
observations	O
.	O
the	O
kernel	B
has	O
the	O
form	O
in	O
equation	O
15.19.	O
the	O
hyper-parameters	B
(	O
(	O
cid:7	O
)	O
,	O
σf	O
,	O
σy	O
)	O
are	O
as	O
follows	O
:	O
(	O
a	O
)	O
(	O
1,1,0.1	O
)	O
(	O
b	O
)	O
(	O
0.3	O
,	O
0.1.08	O
,	O
0.00005	O
)	O
,	O
(	O
c	O
)	O
(	O
3.0	O
,	O
1.16	O
,	O
0.89	O
)	O
.	O
based	O
on	O
figure	O
2.5	O
of	O
(	O
rasmussen	O
and	O
williams	O
2006	O
)	O
.	O
figure	O
generated	O
by	O
gprdemochangehparams	O
,	O
written	O
by	O
carl	O
rasmussen	O
.	O
15.2.3	O
effect	O
of	O
the	O
kernel	B
parameters	O
the	O
predictive	B
performance	O
of	O
gps	O
depends	O
exclusively	O
on	O
the	O
suitability	O
of	O
the	O
chosen	O
kernel	B
.	O
suppose	O
we	O
choose	O
the	O
following	O
squared-exponential	O
(	O
se	O
)	O
kernel	B
for	O
the	O
noisy	O
observations	O
κy	O
(	O
xp	O
,	O
xq	O
)	O
=	O
σ2	O
yδpq	O
f	O
exp	O
(	O
−	O
1	O
2	O
(	O
cid:6	O
)	O
2	O
(	O
xp	O
−	O
xq	O
)	O
2	O
)	O
+σ	O
2	O
(	O
15.19	O
)	O
here	O
(	O
cid:6	O
)	O
is	O
the	O
horizontal	O
scale	O
over	O
which	O
the	O
function	O
changes	O
,	O
σ2	O
f	O
controls	O
the	O
vertical	O
scale	O
of	O
the	O
function	O
,	O
and	O
σ2	O
y	O
is	O
the	O
noise	O
variance	O
.	O
figure	O
15.3	O
illustrates	O
the	O
effects	O
of	O
changing	O
these	O
parameters	O
.	O
we	O
sampled	O
20	O
noisy	O
data	O
points	O
from	O
the	O
se	O
kernel	B
using	O
(	O
(	O
cid:6	O
)	O
,	O
σf	O
,	O
σy	O
)	O
=	O
(	O
1	O
,	O
1	O
,	O
0.1	O
)	O
,	O
and	O
then	O
made	O
predictions	O
various	O
parameters	O
,	O
conditional	O
on	O
the	O
data	O
.	O
in	O
figure	O
15.3	O
(	O
a	O
)	O
,	O
we	O
use	O
(	O
(	O
cid:6	O
)	O
,	O
σf	O
,	O
σy	O
)	O
=	O
(	O
1	O
,	O
1	O
,	O
0.1	O
)	O
,	O
and	O
the	O
result	O
is	O
a	O
good	O
ﬁt	O
.	O
in	O
figure	O
15.3	O
(	O
b	O
)	O
,	O
we	O
reduce	O
the	O
length	O
scale	O
to	O
(	O
cid:6	O
)	O
=	O
0.3	O
(	O
the	O
other	O
parameters	O
were	O
optimized	O
by	O
maximum	O
(	O
marginal	O
)	O
likelihood	B
,	O
a	O
technique	O
we	O
discuss	O
below	O
)	O
;	O
now	O
the	O
function	O
looks	O
more	O
“	O
wiggly	O
”	O
.	O
also	O
,	O
the	O
uncertainty	B
goes	O
up	O
faster	O
,	O
since	O
the	O
effective	O
distance	O
from	O
the	O
training	O
points	O
increases	O
more	O
rapidly	O
.	O
in	O
figure	O
15.3	O
(	O
c	O
)	O
,	O
we	O
increase	O
the	O
length	O
scale	O
to	O
(	O
cid:6	O
)	O
=	O
3	O
;	O
now	O
the	O
function	O
looks	O
smoother	O
.	O
520	O
chapter	O
15.	O
gaussian	O
processes	O
y	O
t	O
u	O
p	O
t	O
u	O
o	O
2	O
1	O
0	O
−1	O
−2	O
y	O
t	O
u	O
p	O
t	O
u	O
o	O
2	O
1	O
0	O
−1	O
−2	O
2	O
0	O
input	O
x2	O
2	O
0	O
input	O
x1	O
−2	O
−2	O
(	O
a	O
)	O
2	O
0	O
input	O
x2	O
2	O
0	O
input	O
x1	O
−2	O
−2	O
(	O
b	O
)	O
y	O
t	O
u	O
p	O
u	O
o	O
t	O
2	O
1	O
0	O
−1	O
−2	O
2	O
0	O
input	O
x2	O
2	O
0	O
input	O
x1	O
−2	O
−2	O
(	O
c	O
)	O
figure	O
15.4	O
kernel	B
has	O
the	O
form	O
in	O
equation	O
15.20	O
where	O
(	O
a	O
)	O
m	O
=	O
i	O
,	O
(	O
b	O
)	O
m	O
=	O
diag	O
(	O
1	O
,	O
3	O
)	O
diag	O
(	O
6	O
,	O
6	O
)	O
written	O
by	O
carl	O
rasmussen	O
.	O
some	O
2d	O
functions	O
sampled	O
from	O
a	O
gp	O
with	O
an	O
se	O
kernel	B
but	O
different	O
hyper-parameters	B
.	O
the	O
−2	O
,	O
(	O
c	O
)	O
m	O
=	O
(	O
1	O
,	O
−1	O
;	O
−1	O
,	O
1	O
)	O
+	O
−2	O
.	O
based	O
on	O
figure	O
5.1	O
of	O
(	O
rasmussen	O
and	O
williams	O
2006	O
)	O
.	O
figure	O
generated	O
by	O
gprdemoard	O
,	O
we	O
can	O
extend	O
the	O
se	O
kernel	B
to	O
multiple	O
dimensions	O
as	O
follows	O
:	O
κy	O
(	O
xp	O
,	O
xq	O
)	O
=	O
σ2	O
f	O
exp	O
(	O
−	O
1	O
2	O
(	O
xp	O
−	O
xq	O
)	O
t	O
m	O
(	O
xp	O
−	O
xq	O
)	O
)	O
+	O
σ2	O
yδpq	O
(	O
15.20	O
)	O
−2	O
.	O
we	O
can	O
deﬁne	O
the	O
matrix	O
m	O
in	O
several	O
ways	O
.	O
the	O
simplest	O
is	O
to	O
use	O
an	O
isotropic	B
matrix	O
,	O
m1	O
=	O
(	O
cid:6	O
)	O
−2i	O
.	O
see	O
figure	O
15.4	O
(	O
a	O
)	O
for	O
an	O
example	O
.	O
we	O
can	O
also	O
endow	O
each	O
dimension	O
with	O
its	O
own	O
characteristic	B
length	I
scale	I
,	O
m2	O
=	O
diag	O
(	O
(	O
cid:9	O
)	O
)	O
if	O
any	O
of	O
these	O
length	O
scales	O
become	O
large	O
,	O
the	O
corresponding	O
feature	O
dimension	O
is	O
deemed	O
“	O
irrelevant	O
”	O
,	O
just	O
as	O
in	O
ard	O
(	O
section	O
13.7	O
)	O
.	O
in	O
figure	O
15.4	O
(	O
b	O
)	O
,	O
we	O
use	O
m	O
=	O
m2	O
with	O
(	O
cid:9	O
)	O
=	O
(	O
1	O
,	O
3	O
)	O
,	O
so	O
the	O
function	O
changes	O
faster	O
along	O
the	O
x1	O
direction	O
than	O
the	O
x2	O
direction	O
.	O
−2	O
,	O
where	O
λ	O
is	O
a	O
d×k	O
matrix	O
,	O
where	O
k	O
<	O
d.	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
p107	O
)	O
calls	O
this	O
the	O
factor	B
analysis	I
distance	I
function	O
,	O
by	O
analogy	O
to	O
the	O
fact	O
that	O
factor	B
analysis	I
(	O
section	O
12.1	O
)	O
approximates	O
a	O
covariance	B
matrix	I
as	O
a	O
low	O
rank	O
matrix	O
plus	O
a	O
diagonal	B
matrix	O
.	O
the	O
columns	O
of	O
λ	O
correspond	O
to	O
relevant	O
directions	O
in	O
input	O
space	O
.	O
in	O
figure	O
15.4	O
(	O
c	O
)	O
,	O
we	O
use	O
(	O
cid:9	O
)	O
=	O
(	O
6	O
;	O
6	O
)	O
and	O
λ	O
=	O
(	O
1	O
;	O
−1	O
)	O
,	O
so	O
the	O
function	O
changes	O
mostly	O
rapidly	O
in	O
the	O
direction	O
which	O
is	O
perpendicular	O
to	O
(	O
1,1	O
)	O
.	O
we	O
can	O
also	O
create	O
a	O
matrix	O
of	O
the	O
form	O
m3	O
=	O
λλt	O
+diag	O
(	O
(	O
cid:9	O
)	O
)	O
15.2.	O
gps	O
for	O
regression	B
521	O
15.2.4	O
estimating	O
the	O
kernel	B
parameters	O
to	O
estimate	O
the	O
kernel	B
parameters	O
,	O
we	O
could	O
use	O
exhaustive	O
search	O
over	O
a	O
discrete	B
grid	O
of	O
values	O
,	O
with	O
validation	O
loss	O
as	O
an	O
objective	O
,	O
but	O
this	O
can	O
be	O
quite	O
slow	O
.	O
(	O
this	O
is	O
the	O
approach	O
used	O
to	O
tune	O
kernels	O
used	O
by	O
svms	O
.	O
)	O
here	O
we	O
consider	O
an	O
empirical	O
bayes	O
approach	O
,	O
which	O
will	O
allow	O
us	O
to	O
use	O
continuous	O
optimization	B
methods	O
,	O
which	O
are	O
much	O
faster	O
.	O
in	O
particular	O
,	O
we	O
will	O
maximize	O
the	O
marginal	O
likelihood1	O
(	O
cid:28	O
)	O
p	O
(	O
y|x	O
)	O
=	O
p	O
(	O
y|f	O
,	O
x	O
)	O
p	O
(	O
f|x	O
)	O
df	O
since	O
p	O
(	O
f|x	O
)	O
=	O
n	O
(	O
f|0	O
,	O
k	O
)	O
,	O
and	O
p	O
(	O
y|f	O
)	O
=	O
log	O
p	O
(	O
y|x	O
)	O
=	O
log	O
n	O
(	O
y|0	O
,	O
ky	O
)	O
=	O
−	O
1	O
2	O
’	O
i	O
n	O
(	O
yi|fi	O
,	O
σ2	O
y	O
y	O
−	O
1	O
2	O
yk−1	O
log	O
|ky|	O
−	O
n	O
2	O
y	O
)	O
,	O
the	O
marginal	B
likelihood	I
is	O
given	O
by	O
log	O
(	O
2π	O
)	O
(	O
15.22	O
)	O
(	O
15.21	O
)	O
the	O
ﬁrst	O
term	O
is	O
a	O
data	O
ﬁt	O
term	O
,	O
the	O
second	O
term	O
is	O
a	O
model	O
complexity	O
term	O
,	O
and	O
the	O
third	O
term	O
is	O
just	O
a	O
constant	O
.	O
to	O
understand	O
the	O
tradeoff	O
between	O
the	O
ﬁrst	O
two	O
terms	O
,	O
consider	O
a	O
se	O
kernel	B
y	O
ﬁxed	O
.	O
let	O
j	O
(	O
(	O
cid:6	O
)	O
)	O
=−	O
log	O
p	O
(	O
y|x	O
,	O
(	O
cid:6	O
)	O
)	O
.	O
for	O
short	O
in	O
1d	O
,	O
as	O
we	O
vary	O
the	O
length	O
scale	O
(	O
cid:6	O
)	O
and	O
hold	O
σ2	O
length	O
scales	O
,	O
the	O
ﬁt	O
will	O
be	O
good	O
,	O
so	O
yt	O
k−1	O
y	O
y	O
will	O
be	O
small	O
.	O
however	O
,	O
the	O
model	O
complexity	O
will	O
be	O
high	O
:	O
k	O
will	O
be	O
almost	O
diagonal	B
(	O
as	O
in	O
figure	O
14.3	O
,	O
top	O
right	O
)	O
,	O
since	O
most	O
points	O
will	O
not	O
be	O
considered	O
“	O
near	O
”	O
any	O
others	O
,	O
so	O
the	O
log	O
|ky|	O
will	O
be	O
large	O
.	O
for	O
long	O
length	O
scales	O
,	O
the	O
ﬁt	O
will	O
be	O
poor	O
but	O
the	O
model	O
complexity	O
will	O
be	O
low	O
:	O
k	O
will	O
be	O
almost	O
all	O
1	O
’	O
s	O
(	O
as	O
in	O
figure	O
14.3	O
,	O
bottom	O
right	O
)	O
,	O
so	O
log	O
|ky|	O
will	O
be	O
small	O
.	O
we	O
now	O
discuss	O
how	O
to	O
maximize	O
the	O
marginal	O
likelhiood	O
.	O
let	O
the	O
kernel	B
parameters	O
(	O
also	O
called	O
hyper-parameters	B
)	O
be	O
denoted	O
by	O
θ.	O
one	O
can	O
show	O
that	O
tr	O
(	O
k−1	O
(	O
cid:9	O
)	O
log	O
p	O
(	O
y|x	O
)	O
=	O
yt	O
k−1	O
(	O
cid:8	O
)	O
∂ky	O
∂θj	O
∂	O
∂θj	O
y	O
∂ky	O
∂θj	O
y	O
y	O
y	O
−	O
1	O
k−1	O
2	O
∂ky	O
∂θj	O
(	O
ααt	O
−	O
k−1	O
y	O
)	O
tr	O
=	O
it	O
takes	O
o	O
(	O
n	O
3	O
)	O
time	O
to	O
compute	O
k−1	O
where	O
α	O
=	O
k−1	O
y	O
y.	O
parameter	B
to	O
compute	O
the	O
gradient	O
.	O
1	O
2	O
1	O
2	O
)	O
(	O
15.23	O
)	O
(	O
15.24	O
)	O
y	O
,	O
and	O
then	O
o	O
(	O
n	O
2	O
)	O
time	O
per	O
hyper-	O
the	O
form	O
of	O
∂ky	O
∂θj	O
depends	O
on	O
the	O
form	O
of	O
the	O
kernel	B
,	O
and	O
which	O
parameter	B
we	O
are	O
taking	O
y	O
≥	O
0.	O
derivatives	O
with	O
respect	O
to	O
.	O
often	O
we	O
have	O
constraints	O
on	O
the	O
hyper-parameters	B
,	O
such	O
as	O
σ2	O
in	O
this	O
case	O
,	O
we	O
can	O
deﬁne	O
θ	O
=	O
log	O
(	O
σ2	O
y	O
)	O
,	O
and	O
then	O
use	O
the	O
chain	B
rule	I
.	O
given	O
an	O
expression	O
for	O
the	O
log	O
marginal	O
likelihood	B
and	O
its	O
derivative	O
,	O
we	O
can	O
estimate	O
the	O
kernel	B
parameters	O
using	O
any	O
standard	O
gradient-based	O
optimizer	O
.	O
however	O
,	O
since	O
the	O
objective	O
is	O
not	O
convex	O
,	O
local	O
minima	O
can	O
be	O
a	O
problem	O
,	O
as	O
we	O
illustrate	O
below	O
.	O
15.2.4.1	O
example	O
consider	O
figure	O
15.5.	O
we	O
use	O
the	O
se	O
kernel	B
in	O
equation	O
15.19	O
with	O
σ2	O
(	O
where	O
x	O
and	O
y	O
are	O
the	O
7	O
data	O
points	O
shown	O
in	O
panels	O
b	O
and	O
c	O
)	O
as	O
we	O
vary	O
(	O
cid:6	O
)	O
and	O
σ2	O
f	O
=	O
1	O
,	O
and	O
plot	O
log	O
p	O
(	O
y|x	O
,	O
(	O
cid:6	O
)	O
,	O
σ2	O
y	O
)	O
y.	O
the	O
two	O
1.	O
the	O
reason	O
it	O
is	O
called	O
the	O
marginal	B
likelihood	I
,	O
rather	O
than	O
just	O
likelihood	B
,	O
is	O
because	O
we	O
have	O
marginalized	O
out	O
the	O
latent	B
gaussian	O
vector	O
f	O
.	O
this	O
moves	O
us	O
up	O
one	O
level	O
of	O
the	O
bayesian	O
hierarchy	O
,	O
and	O
reduces	O
the	O
chances	O
of	O
overﬁtting	B
(	O
the	O
number	O
of	O
kernel	B
parameters	O
is	O
usually	O
fairly	O
small	O
compared	O
to	O
a	O
standard	O
parametric	O
model	O
)	O
.	O
522	O
n	O
o	O
i	O
t	O
i	O
a	O
v	O
e	O
d	O
d	O
r	O
a	O
d	O
n	O
a	O
t	O
s	O
e	O
s	O
o	O
n	O
i	O
100	O
10−1	O
chapter	O
15.	O
gaussian	O
processes	O
y	O
,	O
t	O
u	O
p	O
t	O
u	O
o	O
2	O
1	O
0	O
−1	O
−2	O
−5	O
5	O
0	O
input	O
,	O
x	O
(	O
b	O
)	O
−5	O
5	O
0	O
input	O
,	O
x	O
(	O
c	O
)	O
100	O
101	O
characteristic	O
lengthscale	O
(	O
a	O
)	O
y	O
,	O
t	O
u	O
p	O
t	O
u	O
o	O
2	O
1	O
0	O
−1	O
−2	O
illustration	O
of	O
local	O
minima	O
in	O
the	O
marginal	B
likelihood	I
surface	O
.	O
y	O
and	O
(	O
cid:7	O
)	O
,	O
for	O
ﬁxed	O
σ2	O
figure	O
15.5	O
likelihood	B
vs	O
σ2	O
corresponding	O
to	O
the	O
lower	O
left	O
local	O
minimum	O
,	O
(	O
(	O
cid:7	O
)	O
,	O
σ2	O
noise	O
.	O
smooth	O
and	O
has	O
high	O
noise	O
.	O
the	O
data	O
was	O
generated	O
using	O
(	O
(	O
cid:7	O
)	O
,	O
σ2	O
(	O
rasmussen	O
and	O
williams	O
2006	O
)	O
.	O
figure	O
generated	O
by	O
gprdemomarglik	O
,	O
written	O
by	O
carl	O
rasmussen	O
.	O
(	O
a	O
)	O
we	O
plot	O
the	O
log	O
marginal	O
f	O
=	O
1	O
,	O
using	O
the	O
7	O
data	O
points	O
shown	O
in	O
panels	O
b	O
and	O
c.	O
(	O
b	O
)	O
the	O
function	O
n	O
)	O
≈	O
(	O
1	O
,	O
0.2	O
)	O
.	O
this	O
is	O
quite	O
“	O
wiggly	O
”	O
and	O
has	O
low	O
n	O
)	O
≈	O
(	O
10	O
,	O
0.8	O
)	O
.	O
this	O
is	O
quite	O
source	O
:	O
figure	O
5.5	O
of	O
(	O
c	O
)	O
the	O
function	O
corresponding	O
to	O
the	O
top	O
right	O
local	O
minimum	O
,	O
(	O
(	O
cid:7	O
)	O
,	O
σ2	O
n	O
)	O
=	O
(	O
1	O
,	O
0.1	O
)	O
.	O
local	O
optima	O
are	O
indicated	O
by	O
+	O
.	O
the	O
bottom	O
left	O
optimum	O
corresponds	O
to	O
a	O
low-noise	O
,	O
short-	O
length	O
scale	O
solution	O
(	O
shown	O
in	O
panel	O
b	O
)	O
.	O
the	O
top	O
right	O
optimum	O
corresponds	O
to	O
a	O
high-noise	O
,	O
long-length	O
scale	O
solution	O
(	O
shown	O
in	O
panel	O
c	O
)	O
.	O
with	O
only	O
7	O
data	O
points	O
,	O
there	O
is	O
not	O
enough	O
evidence	B
to	O
conﬁdently	O
decide	O
which	O
is	O
more	O
reasonable	O
,	O
although	O
the	O
more	O
complex	O
model	O
(	O
panel	O
b	O
)	O
has	O
a	O
marginal	B
likelihood	I
that	O
is	O
about	O
60	O
%	O
higher	O
than	O
the	O
simpler	O
model	O
(	O
panel	O
c	O
)	O
.	O
with	O
more	O
data	O
,	O
the	O
map	O
estimate	O
should	O
come	O
to	O
dominate	O
.	O
y	O
≈	O
1	O
(	O
top	O
of	O
panel	O
a	O
)	O
corresponds	O
to	O
the	O
case	O
where	O
the	O
noise	O
is	O
very	O
high	O
;	O
in	O
this	O
regime	O
,	O
the	O
marginal	B
likelihood	I
is	O
insensitive	O
to	O
the	O
length	O
scale	O
(	O
indicated	O
by	O
the	O
horizontal	O
contours	O
)	O
,	O
since	O
all	O
the	O
data	O
is	O
explained	O
as	O
noise	O
.	O
the	O
region	O
where	O
(	O
cid:6	O
)	O
≈	O
0.5	O
(	O
left	O
hand	O
side	O
of	O
panel	O
a	O
)	O
corresponds	O
to	O
the	O
case	O
where	O
the	O
length	O
scale	O
is	O
very	O
short	O
;	O
in	O
this	O
regime	O
,	O
the	O
marginal	B
likelihood	I
is	O
insensitive	O
to	O
the	O
noise	O
level	O
,	O
since	O
the	O
data	O
is	O
perfectly	O
interpolated	O
.	O
neither	O
of	O
these	O
regions	O
would	O
be	O
chosen	O
by	O
a	O
good	O
optimizer	O
.	O
figure	O
15.5	O
illustrates	O
some	O
other	O
interesting	O
(	O
and	O
typical	O
)	O
features	B
.	O
the	O
region	O
where	O
σ2	O
15.2.	O
gps	O
for	O
regression	B
523	O
)	O
e	O
d	O
u	O
t	O
i	O
n	O
g	O
a	O
m	O
(	O
g	O
o	O
l	O
−0.5	O
−1	O
−1.5	O
−2	O
−2.5	O
−3	O
2.8	O
z	O
2	O
z	O
1	O
)	O
e	O
d	O
u	O
t	O
i	O
n	O
g	O
a	O
m	O
(	O
g	O
o	O
l	O
3	O
3.2	O
3.4	O
log	O
(	O
length−scale	O
)	O
−0.5	O
−1	O
−1.5	O
−2	O
−2.5	O
−3	O
2.8	O
3	O
3.2	O
3.4	O
log	O
(	O
length−scale	O
)	O
(	O
a	O
)	O
(	O
b	O
)	O
−0.5	O
−1	O
−1.5	O
−2	O
−2.5	O
)	O
e	O
d	O
u	O
t	O
i	O
n	O
g	O
a	O
m	O
(	O
g	O
o	O
l	O
−3	O
2.8	O
3	O
3.2	O
3.4	O
log	O
(	O
length−scale	O
)	O
(	O
c	O
)	O
figure	O
15.6	O
three	O
different	O
approximations	O
to	O
the	O
posterior	O
over	O
hyper-parameters	B
:	O
grid-based	O
,	O
monte	O
carlo	O
,	O
and	O
central	B
composite	I
design	I
.	O
source	O
:	O
figure	O
3.2	O
of	O
(	O
vanhatalo	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
jarno	O
vanhatalo	O
.	O
15.2.4.2	O
bayesian	O
inference	B
for	O
the	O
hyper-parameters	B
an	O
alternative	O
to	O
computing	O
a	O
point	B
estimate	I
of	O
the	O
hyper-parameters	B
is	O
to	O
compute	O
their	O
poste-	O
rior	O
.	O
let	O
θ	O
represent	O
all	O
the	O
kernel	B
parameters	O
,	O
as	O
well	O
as	O
σ2	O
y.	O
if	O
the	O
dimensionality	O
of	O
θ	O
is	O
small	O
,	O
we	O
can	O
compute	O
a	O
discrete	B
grid	O
of	O
possible	O
values	O
,	O
centered	O
on	O
the	O
map	O
estimate	O
ˆθ	O
(	O
computed	O
as	O
above	O
)	O
.	O
we	O
can	O
then	O
approximate	O
the	O
posterior	O
over	O
the	O
latent	B
variables	O
using	O
p	O
(	O
f|d	O
)	O
∝	O
s	O
(	O
cid:2	O
)	O
p	O
(	O
f|d	O
,	O
θs	O
)	O
p	O
(	O
θs|d	O
)	O
δs	O
(	O
15.25	O
)	O
s=1	O
where	O
δs	O
denotes	O
the	O
weight	O
for	O
grid	O
point	O
s.	O
in	O
higher	O
dimensions	O
,	O
a	O
regular	B
grid	O
suffers	O
from	O
the	O
curse	B
of	I
dimensionality	I
.	O
an	O
obvious	O
alternative	O
is	O
monte	O
carlo	O
,	O
but	O
this	O
can	O
be	O
slow	O
.	O
another	O
approach	O
is	O
to	O
use	O
a	O
form	O
of	O
quasi-	O
monte	O
carlo	O
,	O
whereby	O
we	O
place	O
grid	O
points	O
at	O
the	O
mode	B
,	O
and	O
at	O
a	O
distance	O
±1sd	O
from	O
the	O
mode	B
along	O
each	O
dimension	O
,	O
for	O
a	O
total	O
of	O
2|θ|	O
+	O
1	O
points	O
.	O
this	O
is	O
called	O
a	O
central	B
composite	I
design	I
(	O
rue	O
et	O
al	O
.	O
2009	O
)	O
.	O
(	O
this	O
is	O
also	O
used	O
in	O
the	O
unscented	O
kalman	O
ﬁlter	O
,	O
see	O
section	O
18.5.2	O
.	O
)	O
to	O
make	O
this	O
gaussian-like	O
approximation	O
more	O
reasonable	O
,	O
we	O
often	O
log-transform	O
the	O
hyper-parameters	B
.	O
see	O
figure	O
15.6	O
for	O
an	O
illustration	O
.	O
524	O
chapter	O
15.	O
gaussian	O
processes	O
15.2.4.3	O
multiple	B
kernel	I
learning	I
a	O
quite	O
different	O
approach	O
to	O
optimizing	O
kernel	B
parameters	O
known	O
as	O
multiple	B
kernel	I
learning	I
.	O
the	O
idea	O
is	O
to	O
deﬁne	O
the	O
kernel	B
as	O
a	O
weighted	O
sum	O
of	O
base	O
kernels	O
,	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
j	O
wjκj	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
,	O
and	O
then	O
to	O
optimize	O
the	O
weights	O
wj	O
instead	O
of	O
the	O
kernel	B
parameters	O
themselves	O
.	O
this	O
is	O
particularly	O
useful	O
if	O
we	O
have	O
different	O
kinds	O
of	O
data	O
which	O
we	O
wish	O
to	O
fuse	O
together	O
.	O
see	O
e.g.	O
,	O
(	O
rakotomamonjy	O
et	O
al	O
.	O
2008	O
)	O
for	O
an	O
approach	O
based	O
on	O
risk-minimization	O
and	O
convex	B
optimization	O
,	O
and	O
(	O
girolami	O
and	O
rogers	O
2005	O
)	O
for	O
an	O
approach	O
based	O
on	O
variational	O
bayes.	O
)	O
=	O
(	O
cid:10	O
)	O
15.2.5	O
computational	O
and	O
numerical	O
issues	O
*	O
the	O
predictive	B
mean	O
is	O
given	O
by	O
f∗	O
=	O
kt∗	O
k−1	O
y	O
y.	O
for	O
reasons	O
of	O
numerical	O
stability	O
,	O
it	O
is	O
unwise	O
to	O
directly	O
invert	O
ky.	O
a	O
more	O
robust	B
alternative	O
is	O
to	O
compute	O
a	O
cholesky	O
decomposition	O
,	O
ky	O
=	O
llt	O
.	O
we	O
can	O
then	O
compute	O
the	O
predictive	B
mean	O
and	O
variance	B
,	O
and	O
the	O
log	O
marginal	O
likelihood	B
,	O
as	O
shown	O
in	O
the	O
pseudo-code	O
in	O
algorithm	O
6	O
(	O
based	O
on	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
p19	O
)	O
)	O
.	O
it	O
takes	O
o	O
(	O
n	O
3	O
)	O
time	O
to	O
compute	O
the	O
cholesky	O
decomposition	O
,	O
and	O
o	O
(	O
n	O
2	O
)	O
time	O
to	O
solve	O
for	O
α	O
=	O
k−1	O
y	O
y	O
=	O
l−t	O
l−1y	O
.	O
we	O
can	O
then	O
compute	O
the	O
mean	B
using	O
kt∗	O
α	O
in	O
o	O
(	O
n	O
)	O
time	O
and	O
the	O
variance	B
using	O
k∗∗	O
−	O
kt∗	O
l−t	O
l−1k∗	O
in	O
o	O
(	O
n	O
2	O
)	O
time	O
for	O
each	O
test	O
case	O
.	O
an	O
alternative	O
to	O
cholesky	O
decomposition	O
is	O
to	O
solve	O
the	O
linear	O
system	O
kyα	O
=	O
y	O
using	O
conjugate	B
gradients	I
(	O
cg	O
)	O
.	O
if	O
we	O
terminate	O
this	O
algorithm	O
after	O
k	O
iterations	O
,	O
it	O
takes	O
o	O
(	O
kn	O
2	O
)	O
time	O
.	O
it	O
gives	O
the	O
exact	O
solution	O
in	O
o	O
(	O
n	O
3	O
)	O
time	O
.	O
another	O
approach	O
is	O
to	O
if	O
we	O
run	O
for	O
k	O
=	O
n	O
,	O
approximate	O
the	O
matrix-vector	O
multiplies	O
needed	O
by	O
cg	O
using	O
the	O
fast	O
gauss	O
transform	O
.	O
(	O
yang	O
et	O
al	O
.	O
2005	O
)	O
;	O
however	O
,	O
this	O
doesn	O
’	O
t	O
scale	O
to	O
high-dimensional	O
inputs	O
.	O
see	O
also	O
section	O
15.6	O
for	O
a	O
discussion	O
of	O
other	O
speedup	O
techniques	O
.	O
yi	O
)	O
;	O
algorithm	O
15.1	O
:	O
gp	O
regression	B
1	O
l	O
=	O
cholesky	O
(	O
k	O
+	O
σ2	O
2	O
α	O
=	O
lt	O
\	O
(	O
l	O
\	O
y	O
)	O
;	O
3	O
e	O
[	O
f∗	O
]	O
=	O
kt∗	O
α	O
;	O
4	O
v	O
=	O
l	O
\	O
k∗	O
;	O
2	O
yt	O
α	O
−	O
(	O
cid:10	O
)	O
5	O
var	O
[	O
f∗	O
]	O
=	O
κ	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
vt	O
v	O
;	O
6	O
log	O
p	O
(	O
y|x	O
)	O
=	O
−	O
1	O
i	O
log	O
lii	O
−	O
n	O
2	O
log	O
(	O
2π	O
)	O
15.2.6	O
semi-parametric	O
gps	O
*	O
sometimes	O
it	O
is	O
useful	O
to	O
use	O
a	O
linear	O
model	O
for	O
the	O
mean	B
of	O
the	O
process	O
,	O
as	O
follows	O
:	O
f	O
(	O
x	O
)	O
=	O
βt	O
φ	O
(	O
x	O
)	O
+r	O
(	O
x	O
)	O
where	O
r	O
(	O
x	O
)	O
∼	O
gp	O
(	O
0	O
,	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
parametric	B
model	I
,	O
and	O
is	O
known	O
as	O
a	O
semi-parametric	O
model.	O
)	O
)	O
models	O
the	O
residuals	O
.	O
this	O
combines	O
a	O
parametric	O
and	O
a	O
non-	O
if	O
we	O
assume	O
β	O
∼	O
n	O
(	O
b	O
,	O
b	O
)	O
,	O
we	O
can	O
integrate	O
these	O
parameters	O
out	O
to	O
get	O
a	O
new	O
gp	O
(	O
o	O
’	O
hagan	O
(	O
15.26	O
)	O
1978	O
)	O
:	O
f	O
(	O
x	O
)	O
∼	O
gp	O
(	O
cid:3	O
)	O
φ	O
(	O
x	O
)	O
t	O
b	O
,	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
+φ	O
(	O
x	O
)	O
t	O
bφ	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
(	O
cid:4	O
)	O
(	O
15.27	O
)	O
15.3.	O
gps	O
meet	O
glms	O
525	O
log	O
p	O
(	O
yi|fi	O
)	O
log	O
sigm	O
(	O
yifi	O
)	O
log	O
φ	O
(	O
yifi	O
)	O
log	O
p	O
(	O
yi|fi	O
)	O
∂	O
∂fi	O
ti	O
−	O
πi	O
yiφ	O
(	O
fi	O
)	O
φ	O
(	O
yifi	O
)	O
∂2	O
∂f	O
2	O
i	O
log	O
p	O
(	O
yi|fi	O
)	O
−πi	O
(	O
1	O
−	O
πi	O
)	O
−	O
φ2	O
φ	O
(	O
yifi	O
)	O
2	O
−	O
yifiφ	O
(	O
fi	O
)	O
φ	O
(	O
yifi	O
)	O
i	O
likelihood	B
,	O
gradient	O
and	O
hessian	O
for	O
binary	O
logistic/	O
probit	B
gp	O
regression	B
.	O
we	O
assume	O
yi	O
∈	O
table	O
15.1	O
{	O
−1	O
,	O
+1	O
}	O
and	O
deﬁne	O
ti	O
=	O
(	O
yi	O
+1	O
)	O
/2	O
∈	O
{	O
0	O
,	O
1	O
}	O
and	O
πi	O
=	O
sigm	O
(	O
fi	O
)	O
for	O
logistic	B
regression	I
,	O
and	O
πi	O
=	O
φ	O
(	O
fi	O
)	O
for	O
probit	B
regression	I
.	O
also	O
,	O
φ	O
and	O
φ	O
are	O
the	O
pdf	B
and	O
cdf	B
of	O
n	O
(	O
0	O
,	O
1	O
)	O
.	O
from	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
p43	O
)	O
.	O
integrating	O
out	O
β	O
,	O
the	O
corresponding	O
predictive	B
distribution	O
for	O
test	O
inputs	O
x∗	O
has	O
the	O
following	O
form	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
p28	O
)	O
:	O
p	O
(	O
f∗|x∗	O
,	O
x	O
,	O
y	O
)	O
=n	O
(	O
f∗	O
,	O
cov	O
[	O
f∗	O
]	O
)	O
f∗	O
=	O
φt∗	O
β	O
+	O
kt∗	O
k−1	O
β	O
=	O
(	O
φt	O
k−1	O
y	O
(	O
y	O
−	O
φβ	O
)	O
y	O
φ	O
+	O
b−1	O
)	O
(	O
15.28	O
)	O
(	O
15.29	O
)	O
(	O
15.30	O
)	O
−1	O
(	O
φk−1	O
y	O
y	O
+	O
b−1b	O
)	O
y	O
k∗	O
+	O
rt	O
(	O
b−1	O
+	O
φk−1	O
y	O
φt	O
)	O
cov	O
[	O
f∗	O
]	O
=k	O
∗∗	O
−	O
kt∗	O
k−1	O
y	O
φ∗	O
r	O
=	O
φ∗	O
−	O
φk−1	O
−1r	O
(	O
15.31	O
)	O
(	O
15.32	O
)	O
the	O
predictive	B
mean	O
is	O
the	O
output	O
of	O
the	O
linear	O
model	O
plus	O
a	O
correction	O
term	O
due	O
to	O
the	O
gp	O
,	O
and	O
the	O
predictive	B
covariance	O
is	O
the	O
usual	O
gp	O
covariance	B
plus	O
an	O
extra	O
term	O
due	O
to	O
the	O
uncertainty	B
in	O
β	O
.	O
15.3	O
gps	O
meet	O
glms	O
in	O
this	O
section	O
,	O
we	O
extend	O
gps	O
to	O
the	O
glm	O
setting	O
,	O
focussing	O
on	O
the	O
classiﬁcation	B
case	O
.	O
as	O
with	O
bayesian	O
logistic	B
regression	I
,	O
the	O
main	O
difficulty	O
is	O
that	O
the	O
gaussian	O
prior	O
is	O
not	O
conjugate	O
to	O
the	O
bernoulli/	O
multinoulli	O
likelihood	O
.	O
there	O
are	O
several	O
approximations	O
one	O
can	O
adopt	O
:	O
gaussian	O
approximation	O
(	O
section	O
8.4.3	O
)	O
,	O
expectation	B
propagation	I
(	O
kuss	O
and	O
rasmussen	O
2005	O
;	O
nickisch	O
and	O
rasmussen	O
2008	O
)	O
,	O
variational	O
(	O
girolami	O
and	O
rogers	O
2006	O
;	O
opper	O
and	O
archambeau	O
2009	O
)	O
,	O
mcmc	O
(	O
neal	O
1997	O
;	O
christensen	O
et	O
al	O
.	O
2006	O
)	O
,	O
etc	O
.	O
here	O
we	O
focus	O
on	O
the	O
gaussian	O
approximation	O
,	O
since	O
it	O
is	O
the	O
simplest	O
and	O
fastest	O
.	O
15.3.1	O
binary	B
classiﬁcation	I
in	O
the	O
binary	O
case	O
,	O
we	O
deﬁne	O
the	O
model	O
as	O
p	O
(	O
yi|xi	O
)	O
=	O
σ	O
(	O
yif	O
(	O
xi	O
)	O
)	O
,	O
where	O
,	O
following	O
(	O
rasmussen	O
and	O
williams	O
2006	O
)	O
,	O
we	O
assume	O
yi	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
,	O
and	O
we	O
let	O
σ	O
(	O
z	O
)	O
=	O
sigm	O
(	O
z	O
)	O
(	O
logistic	B
regression	I
)	O
or	O
σ	O
(	O
z	O
)	O
=	O
φ	O
(	O
z	O
)	O
(	O
probit	B
regression	I
)	O
.	O
as	O
for	O
gp	O
regression	B
,	O
we	O
assume	O
f	O
∼	O
gp	O
(	O
0	O
,	O
κ	O
)	O
.	O
15.3.1.1	O
computing	O
the	O
posterior	O
deﬁne	O
the	O
log	O
of	O
the	O
unnormalized	O
posterior	O
as	O
follows	O
:	O
(	O
cid:6	O
)	O
(	O
f	O
)	O
=	O
log	O
p	O
(	O
y|f	O
)	O
+	O
log	O
p	O
(	O
f|x	O
)	O
=	O
log	O
p	O
(	O
y|f	O
)	O
−	O
1	O
2	O
f	O
t	O
k−1f	O
−	O
1	O
2	O
log	O
|k|	O
−	O
n	O
2	O
log	O
2π	O
(	O
15.33	O
)	O
526	O
chapter	O
15.	O
gaussian	O
processes	O
let	O
j	O
(	O
f	O
)	O
(	O
cid:2	O
)	O
−	O
(	O
cid:6	O
)	O
(	O
f	O
)	O
be	O
the	O
function	O
we	O
want	O
to	O
minimize	O
.	O
the	O
gradient	O
and	O
hessian	O
of	O
this	O
are	O
given	O
by	O
g	O
=	O
−∇	O
log	O
p	O
(	O
y|f	O
)	O
+k	O
−1f	O
h	O
=	O
−∇∇	O
log	O
p	O
(	O
y|f	O
)	O
+k	O
−1	O
=	O
w	O
+	O
k−1	O
(	O
15.34	O
)	O
(	O
15.35	O
)	O
note	O
that	O
w	O
(	O
cid:2	O
)	O
−∇∇	O
log	O
p	O
(	O
y|f	O
)	O
is	O
a	O
diagonal	B
matrix	O
because	O
the	O
data	O
are	O
iid	B
(	O
conditional	O
on	O
f	O
)	O
.	O
expressions	O
for	O
the	O
gradient	O
and	O
hessian	O
of	O
the	O
log	O
likelihood	O
for	O
the	O
logit	B
and	O
probit	B
case	O
are	O
given	O
in	O
sections	O
8.3.1	O
and	O
9.4.1	O
,	O
and	O
summarized	O
in	O
table	O
15.1.	O
we	O
can	O
use	O
irls	O
to	O
ﬁnd	O
the	O
map	O
estimate	O
.	O
the	O
update	O
has	O
the	O
form	O
f	O
new	O
=	O
f	O
−	O
h−1g	O
=	O
f	O
+	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
(	O
∇	O
log	O
p	O
(	O
y|f	O
)	O
−	O
k−1f	O
)	O
=	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
(	O
wf	O
+	O
∇	O
log	O
p	O
(	O
y|f	O
)	O
)	O
at	O
convergence	O
,	O
the	O
gaussian	O
approximation	O
of	O
the	O
posterior	O
takes	O
the	O
following	O
form	O
:	O
p	O
(	O
f|x	O
,	O
y	O
)	O
≈	O
n	O
(	O
ˆf	O
,	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
)	O
15.3.1.2	O
computing	O
the	O
posterior	O
predictive	O
(	O
15.36	O
)	O
(	O
15.37	O
)	O
(	O
15.38	O
)	O
we	O
now	O
compute	O
the	O
posterior	O
predictive	O
.	O
first	O
we	O
predict	O
the	O
latent	B
function	O
at	O
the	O
test	O
case	O
(	O
cid:28	O
)	O
x∗	O
.	O
for	O
the	O
mean	B
we	O
have	O
(	O
cid:28	O
)	O
e	O
[	O
f∗|x∗	O
,	O
x	O
,	O
y	O
]	O
=	O
(	O
15.39	O
)	O
e	O
[	O
f∗|f	O
,	O
x∗	O
,	O
x	O
,	O
y	O
]	O
p	O
(	O
f|x	O
,	O
y	O
)	O
df	O
kt∗	O
k−1f	O
p	O
(	O
f|x	O
,	O
y	O
)	O
df	O
=	O
=	O
kt∗	O
k−1	O
e	O
[	O
f|x	O
,	O
y	O
]	O
≈	O
kt∗	O
k−1ˆf	O
(	O
15.40	O
)	O
(	O
15.41	O
)	O
(	O
15.42	O
)	O
(	O
15.43	O
)	O
(	O
15.44	O
)	O
where	O
we	O
used	O
equation	O
15.8	O
to	O
get	O
the	O
mean	B
of	O
f∗	O
given	O
noise-free	O
f	O
.	O
to	O
compute	O
the	O
predictive	B
variance	O
,	O
we	O
use	O
the	O
rule	O
of	O
iterated	O
variance	O
:	O
var	O
[	O
f∗	O
]	O
=e	O
[	O
var	O
[	O
f∗|f	O
]	O
]	O
+	O
var	O
[	O
e	O
[	O
f∗|f	O
]	O
]	O
where	O
all	O
probabilities	O
are	O
conditioned	O
on	O
x∗	O
,	O
x	O
,	O
y.	O
from	O
equation	O
15.9	O
we	O
have	O
(	O
cid:31	O
)	O
e	O
[	O
var	O
[	O
f∗|f	O
]	O
]	O
=	O
e	O
from	O
equation	O
15.9	O
we	O
have	O
var	O
[	O
e	O
[	O
f∗|f	O
]	O
]	O
=	O
var	O
combining	O
these	O
we	O
get	O
k∗∗	O
−	O
kt∗	O
k−1k∗	O
(	O
cid:31	O
)	O
k∗k−1f	O
=	O
k∗∗	O
−	O
kt∗	O
k−1k∗	O
=	O
kt∗	O
k−1cov	O
[	O
f	O
]	O
k−1k∗	O
var	O
[	O
f∗	O
]	O
=k	O
∗∗	O
−	O
kt∗	O
(	O
k−1	O
−	O
k−1cov	O
[	O
f	O
]	O
k−1	O
)	O
k∗	O
from	O
equation	O
15.38	O
we	O
have	O
cov	O
[	O
f	O
]	O
≈	O
(	O
k−1	O
+	O
w	O
)	O
get	O
var	O
[	O
f∗	O
]	O
≈	O
k∗∗	O
−	O
kt∗	O
k−1k∗	O
+	O
kt∗	O
k−1	O
(	O
k−1	O
+	O
w	O
)	O
=	O
k∗∗	O
−	O
kt∗	O
(	O
k	O
+	O
w−1	O
)	O
−1k∗	O
(	O
15.45	O
)	O
−1	O
.	O
using	O
the	O
matrix	B
inversion	I
lemma	I
we	O
−1k−1k∗	O
(	O
15.46	O
)	O
(	O
15.47	O
)	O
15.3.	O
gps	O
meet	O
glms	O
so	O
in	O
summary	O
we	O
have	O
p	O
(	O
f∗|x∗	O
,	O
x	O
,	O
y	O
)	O
=	O
n	O
(	O
e	O
[	O
f∗	O
]	O
,	O
var	O
[	O
f∗	O
]	O
)	O
to	O
convert	O
this	O
in	O
to	O
a	O
predictive	B
distribution	O
for	O
binary	O
responses	O
,	O
we	O
use	O
π∗	O
=	O
p	O
(	O
y∗	O
=	O
1|x∗	O
,	O
x	O
,	O
y	O
)	O
≈	O
σ	O
(	O
f∗	O
)	O
p	O
(	O
f∗|x∗	O
,	O
x	O
,	O
y	O
)	O
df∗	O
(	O
cid:28	O
)	O
527	O
(	O
15.48	O
)	O
(	O
15.49	O
)	O
this	O
can	O
be	O
approximated	O
using	O
any	O
of	O
the	O
methods	O
discussed	O
in	O
section	O
8.4.4	O
,	O
where	O
we	O
discussed	O
bayesian	O
logistic	B
regression	I
.	O
for	O
example	O
,	O
using	O
the	O
probit	B
approximation	O
of	O
sec-	O
tion	O
8.4.4.2	O
,	O
we	O
have	O
π∗	O
≈	O
sigm	O
(	O
κ	O
(	O
v	O
)	O
e	O
[	O
f∗	O
]	O
)	O
,	O
wherev	O
=	O
var	O
[	O
f∗	O
]	O
and	O
κ2	O
(	O
v	O
)	O
=	O
(	O
1	O
+	O
πv/8	O
)	O
−1	O
.	O
15.3.1.3	O
computing	O
the	O
marginal	B
likelihood	I
we	O
need	O
the	O
marginal	B
likelihood	I
in	O
order	O
to	O
optimize	O
the	O
kernel	B
parameters	O
.	O
using	O
the	O
laplace	O
approximation	O
in	O
equation	O
8.54	O
we	O
have	O
log	O
p	O
(	O
y|x	O
)	O
≈	O
(	O
cid:6	O
)	O
(	O
ˆf	O
)	O
−	O
1	O
2	O
log	O
|h|	O
+	O
const	O
hence	O
log	O
p	O
(	O
y|x	O
)	O
≈	O
log	O
p	O
(	O
y|ˆf	O
)	O
−	O
1	O
2	O
computing	O
the	O
derivatives	O
∂	O
log	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
ˆf	O
t	O
k−1ˆf	O
−	O
1	O
2	O
log	O
|k|	O
−	O
1	O
2	O
log	O
|k−1	O
+	O
w|	O
(	O
15.50	O
)	O
(	O
15.51	O
)	O
is	O
more	O
complex	O
than	O
in	O
the	O
regression	B
case	O
,	O
since	O
ˆf	O
and	O
w	O
,	O
as	O
well	O
as	O
k	O
,	O
depend	O
on	O
θ.	O
details	O
can	O
be	O
found	O
in	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
p125	O
)	O
.	O
∂θj	O
15.3.1.4	O
numerically	O
stable	B
computation	O
*	O
to	O
implement	O
the	O
above	O
equations	O
in	O
a	O
numerically	O
stable	B
way	O
,	O
it	O
is	O
best	O
to	O
avoid	O
inverting	O
k	O
or	O
w.	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
p45	O
)	O
suggest	O
deﬁning	O
b	O
=	O
in	O
+	O
w	O
1	O
2	O
kw	O
1	O
2	O
(	O
15.52	O
)	O
which	O
has	O
eigenvalues	O
bounded	O
below	O
by	O
1	O
(	O
because	O
of	O
the	O
i	O
)	O
and	O
above	O
by	O
1	O
+	O
n	O
(	O
because	O
wii	O
=	O
πi	O
(	O
1	O
−	O
π	O
)	O
≤	O
0.25	O
)	O
,	O
and	O
hence	O
can	O
be	O
safely	O
inverted	O
.	O
4	O
maxij	O
kij	O
one	O
can	O
use	O
the	O
matrix	B
inversion	I
lemma	I
to	O
show	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
=	O
k	O
−	O
kw	O
1	O
hence	O
the	O
irls	O
update	O
becomes	O
2	O
b−1w	O
1	O
2	O
k	O
f	O
new	O
=	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
(	O
wf	O
+	O
∇	O
log	O
p	O
(	O
y|f	O
)	O
)	O
6	O
45	O
3	O
b	O
=	O
k	O
(	O
i	O
−	O
w	O
1	O
=	O
k	O
(	O
b	O
−	O
w	O
1	O
2	O
k	O
)	O
b	O
2	O
b−1w	O
1	O
2	O
lt	O
\	O
(	O
l	O
\	O
(	O
w	O
1	O
45	O
3	O
2	O
kb	O
)	O
)	O
)	O
6	O
a	O
(	O
15.53	O
)	O
(	O
15.54	O
)	O
(	O
15.55	O
)	O
(	O
15.56	O
)	O
528	O
chapter	O
15.	O
gaussian	O
processes	O
where	O
b	O
=	O
llt	O
is	O
a	O
cholesky	O
decomposition	O
of	O
b.	O
the	O
ﬁtting	O
algorithm	O
takes	O
in	O
o	O
(	O
t	O
n	O
3	O
)	O
time	O
and	O
o	O
(	O
n	O
2	O
)	O
space	O
,	O
where	O
t	O
is	O
the	O
number	O
of	O
newton	O
iterations	O
.	O
at	O
convergence	O
we	O
have	O
a	O
=	O
k−1ˆf	O
,	O
so	O
we	O
can	O
evaluate	O
the	O
log	O
marginal	O
likelihood	B
(	O
equa-	O
tion	O
15.51	O
)	O
using	O
log	O
p	O
(	O
y|x	O
)	O
=	O
log	O
p	O
(	O
y|ˆf	O
)	O
−	O
1	O
2	O
at	O
ˆf	O
−	O
(	O
cid:2	O
)	O
i	O
log	O
lii	O
where	O
we	O
exploited	O
the	O
fact	O
that	O
(	O
15.57	O
)	O
|b|	O
=	O
|k||k−1	O
+	O
w|	O
=	O
|in	O
+	O
w	O
1	O
(	O
15.58	O
)	O
we	O
now	O
compute	O
the	O
predictive	B
distribution	O
.	O
rather	O
than	O
using	O
e	O
[	O
f∗	O
]	O
=	O
kt∗	O
k−1ˆf	O
,	O
we	O
exploit	O
the	O
fact	O
that	O
at	O
the	O
mode	B
,	O
∇	O
(	O
cid:6	O
)	O
=	O
0	O
,	O
so	O
ˆf	O
=	O
k	O
(	O
∇	O
log	O
p	O
(	O
y|ˆf	O
)	O
)	O
.	O
hence	O
we	O
can	O
rewrite	O
the	O
predictive	B
mean	O
as	O
follows:2	O
2|	O
2	O
kw	O
1	O
e	O
[	O
f∗	O
]	O
=	O
kt∗	O
∇	O
log	O
p	O
(	O
y|ˆf	O
)	O
to	O
compute	O
the	O
predictive	B
variance	O
,	O
we	O
exploit	O
the	O
fact	O
that	O
(	O
k	O
+	O
w−1	O
)	O
2	O
(	O
k	O
+	O
w−1	O
)	O
−1	O
=	O
w	O
1	O
−1w−	O
1	O
2	O
w−	O
1	O
2	O
w	O
1	O
2	O
=	O
w	O
1	O
2	O
b−1w	O
1	O
2	O
to	O
get	O
var	O
[	O
f∗	O
]	O
=	O
k∗∗	O
−	O
kt∗	O
w	O
1	O
2	O
(	O
llt	O
)	O
−1w	O
1	O
2	O
k∗	O
=	O
k∗∗	O
−	O
vt	O
v	O
where	O
v	O
=	O
l	O
\	O
(	O
w	O
1	O
2	O
k∗	O
)	O
.	O
we	O
can	O
then	O
compute	O
π∗	O
.	O
(	O
15.59	O
)	O
(	O
15.60	O
)	O
(	O
15.61	O
)	O
the	O
whole	O
algorithm	O
is	O
summarized	O
in	O
algorithm	O
16	O
,	O
based	O
on	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
p46	O
)	O
.	O
fitting	O
takes	O
o	O
(	O
n	O
3	O
)	O
time	O
,	O
and	O
prediction	O
takes	O
o	O
(	O
n	O
2n∗	O
)	O
time	O
,	O
where	O
n∗	O
is	O
the	O
number	O
of	O
test	O
cases	O
.	O
15.3.1.5	O
example	O
in	O
figure	O
15.7	O
,	O
we	O
show	O
a	O
synthetic	O
binary	B
classiﬁcation	I
problem	O
in	O
2d	O
.	O
we	O
use	O
an	O
se	O
kernel	B
.	O
on	O
the	O
left	O
,	O
we	O
show	O
predictions	O
using	O
hyper-parameters	B
set	O
by	O
hand	O
;	O
we	O
use	O
a	O
short	O
length	O
scale	O
,	O
hence	O
the	O
very	O
sharp	O
turns	O
in	O
the	O
decision	B
boundary	I
.	O
on	O
the	O
right	O
,	O
we	O
show	O
the	O
predictions	O
using	O
the	O
learned	O
hyper-parameters	B
;	O
the	O
model	O
favors	O
a	O
more	O
parsimonious	O
explanation	O
of	O
the	O
data	O
.	O
15.3.2	O
multi-class	O
classiﬁcation	O
in	O
this	O
section	O
,	O
we	O
consider	O
a	O
model	O
of	O
the	O
form	O
p	O
(	O
yi|xi	O
)	O
=	O
cat	O
(	O
yi|s	O
(	O
fi	O
)	O
)	O
,	O
where	O
fi	O
=	O
(	O
fi1	O
,	O
.	O
.	O
.	O
,	O
fic	O
)	O
,	O
and	O
we	O
assume	O
f.c	O
∼	O
gp	O
(	O
0	O
,	O
κc	O
)	O
.	O
thus	O
we	O
have	O
one	O
latent	B
function	O
per	O
class	O
,	O
which	O
are	O
a	O
priori	O
independent	O
,	O
and	O
which	O
may	O
use	O
different	O
kernels	O
.	O
as	O
before	O
,	O
we	O
will	O
use	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
.	O
(	O
a	O
similar	B
model	O
,	O
but	O
using	O
the	O
multinomial	B
probit	I
function	O
instead	O
of	O
the	O
multinomial	B
logit	O
,	O
is	O
described	O
in	O
(	O
girolami	O
and	O
rogers	O
2006	O
)	O
.	O
)	O
2.	O
we	O
see	O
that	O
training	O
points	O
that	O
are	O
well-predicted	O
by	O
the	O
model	O
,	O
for	O
which	O
∇i	O
log	O
p	O
(	O
yi|fi	O
)	O
≈	O
0	O
,	O
do	O
not	O
contribute	O
strongly	O
to	O
the	O
prediction	O
at	O
test	O
points	O
;	O
this	O
is	O
similar	B
to	O
the	O
behavior	O
of	O
support	B
vectors	I
in	O
an	O
svm	O
(	O
see	O
section	O
14.5	O
)	O
.	O
15.3.	O
gps	O
meet	O
glms	O
529	O
5	O
6	O
8	O
algorithm	O
15.2	O
:	O
gp	O
binary	B
classiﬁcation	I
using	O
gaussian	O
approximation	O
1	O
//	O
first	O
compute	O
map	O
estimate	O
using	O
irls	O
;	O
2	O
f	O
=	O
0	O
;	O
3	O
repeat	O
4	O
w	O
=	O
−∇∇	O
log	O
p	O
(	O
y|f	O
)	O
;	O
b	O
=	O
in	O
+	O
w	O
1	O
2	O
kw	O
1	O
2	O
;	O
l	O
=	O
cholesky	O
(	O
b	O
)	O
;	O
b	O
=	O
wf	O
+	O
∇	O
log	O
p	O
(	O
y|f	O
)	O
;	O
a	O
=	O
b	O
−	O
w	O
1	O
f	O
=	O
ka	O
;	O
2	O
lt	O
\	O
(	O
l	O
\	O
(	O
w	O
1	O
2	O
kb	O
)	O
)	O
;	O
7	O
2	O
at	O
f	O
−	O
(	O
cid:10	O
)	O
9	O
10	O
until	O
converged	O
;	O
11	O
log	O
p	O
(	O
y|x	O
)	O
=	O
log	O
p	O
(	O
y|f	O
)	O
−	O
1	O
12	O
//	O
now	O
perform	O
prediction	O
;	O
13	O
e	O
[	O
f∗	O
]	O
=	O
kt∗	O
∇	O
log	O
p	O
(	O
y|f	O
)	O
;	O
14	O
v	O
=	O
l	O
\	O
(	O
w	O
1	O
2	O
k∗	O
)	O
;	O
)	O
15	O
var	O
[	O
f∗	O
]	O
=	O
k∗∗	O
−	O
vt	O
v	O
;	O
sigm	O
(	O
z	O
)	O
n	O
(	O
z|e	O
[	O
f∗	O
]	O
,	O
var	O
[	O
f∗	O
]	O
)	O
dz	O
;	O
16	O
p	O
(	O
y∗	O
=	O
1	O
)	O
=	O
i	O
log	O
lii	O
;	O
se	O
kernel	B
,	O
l=0.500	O
,	O
σ2=10.000	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−4	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−4	O
se	O
kernel	B
,	O
l=1.280	O
,	O
σ2=14.455	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
15.7	O
contours	O
of	O
the	O
posterior	O
predictive	O
probability	O
for	O
the	O
red	O
circle	O
class	O
generated	O
by	O
a	O
gp	O
with	O
an	O
se	O
kernel	B
.	O
thick	O
black	O
line	O
is	O
the	O
decision	B
boundary	I
if	O
we	O
threshold	O
at	O
a	O
probability	O
of	O
0.5	O
.	O
(	O
a	O
)	O
manual	O
parameters	O
,	O
short	O
length	O
scale	O
.	O
(	O
b	O
)	O
learned	O
parameters	O
,	O
long	O
length	O
scale	O
.	O
figure	O
generated	O
by	O
gpcdemo2d	O
,	O
based	O
on	O
code	O
by	O
carl	O
rasmussen	O
.	O
530	O
chapter	O
15.	O
gaussian	O
processes	O
15.3.2.1	O
computing	O
the	O
posterior	O
the	O
unnormalized	O
log	O
posterior	O
is	O
given	O
by	O
f	O
t	O
k−1f	O
+	O
yt	O
f	O
−	O
n	O
(	O
cid:2	O
)	O
log	O
(	O
cid:6	O
)	O
(	O
f	O
)	O
=	O
−	O
1	O
2	O
where	O
(	O
cid:11	O
)	O
(	O
cid:13	O
)	O
c	O
(	O
cid:2	O
)	O
exp	O
fic	O
i=1	O
c=1	O
−	O
1	O
2	O
log	O
|k|	O
−	O
cn	O
2	O
log	O
2π	O
(	O
15.62	O
)	O
f	O
=	O
(	O
f11	O
,	O
.	O
.	O
.	O
,	O
fn	O
1	O
,	O
f12	O
,	O
.	O
.	O
.	O
,	O
fn	O
2	O
,	O
···	O
,	O
f1c	O
,	O
.	O
.	O
.	O
,	O
fn	O
c	O
)	O
t	O
(	O
15.63	O
)	O
and	O
y	O
is	O
a	O
dummy	B
encoding	I
of	O
the	O
yi	O
’	O
s	O
which	O
has	O
the	O
same	O
layout	O
as	O
f	O
.	O
also	O
,	O
k	O
is	O
a	O
block	O
diagonal	B
matrix	O
containing	O
kc	O
,	O
where	O
kc	O
=	O
[	O
κc	O
(	O
xi	O
,	O
xj	O
)	O
]	O
models	O
the	O
correlation	O
of	O
the	O
c	O
’	O
th	O
latent	B
function	O
.	O
the	O
gradient	O
and	O
hessian	O
are	O
given	O
by	O
∇	O
(	O
cid:6	O
)	O
=	O
−k−1f	O
+	O
y	O
−	O
π	O
∇∇	O
(	O
cid:6	O
)	O
=	O
−k−1	O
−	O
w	O
(	O
15.64	O
)	O
(	O
15.65	O
)	O
where	O
w	O
(	O
cid:2	O
)	O
diag	O
(	O
π	O
)	O
−	O
ππt	O
,	O
where	O
π	O
is	O
a	O
cn	O
×	O
n	O
matrix	O
obtained	O
by	O
stacking	B
diag	O
(	O
π	O
:	O
c	O
)	O
vertically	O
.	O
(	O
compare	O
these	O
expressions	O
to	O
standard	O
logistic	O
regression	B
in	O
section	O
8.3.7	O
.	O
)	O
we	O
can	O
use	O
irls	O
to	O
compute	O
the	O
mode	B
.	O
the	O
newton	O
step	O
has	O
the	O
form	O
f	O
new	O
=	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
(	O
wf	O
+	O
y	O
−	O
π	O
)	O
(	O
15.66	O
)	O
naively	O
implementing	O
this	O
would	O
take	O
o	O
(	O
c	O
3n	O
3	O
)	O
time	O
.	O
however	O
,	O
we	O
can	O
reduce	O
this	O
to	O
o	O
(	O
cn	O
3	O
)	O
,	O
as	O
shown	O
in	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
p52	O
)	O
.	O
15.3.2.2	O
computing	O
the	O
posterior	O
predictive	O
we	O
can	O
compute	O
the	O
posterior	O
predictive	O
in	O
a	O
manner	O
analogous	O
to	O
section	O
15.3.1.2.	O
for	O
the	O
mean	B
of	O
the	O
latent	B
response	O
we	O
have	O
e	O
[	O
f∗c	O
]	O
=	O
kc	O
(	O
x∗	O
)	O
t	O
k−1	O
c	O
ˆfc	O
=	O
kc	O
(	O
x∗	O
)	O
t	O
(	O
yc	O
−	O
ˆπc	O
)	O
we	O
can	O
put	O
this	O
in	O
vector	O
form	O
by	O
writing	O
e	O
[	O
f∗	O
]	O
=	O
q∗t	O
(	O
y	O
−	O
ˆπ	O
)	O
where	O
q∗	O
=	O
⎛	O
⎜⎝k1	O
(	O
x∗	O
)	O
0	O
⎞	O
⎟⎠	O
0	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
kc	O
(	O
x∗	O
)	O
(	O
15.67	O
)	O
(	O
15.68	O
)	O
(	O
15.69	O
)	O
using	O
a	O
similar	B
argument	O
to	O
equation	O
15.47	O
,	O
we	O
can	O
show	O
that	O
the	O
covariance	B
of	O
the	O
latent	B
response	O
is	O
given	O
by	O
cov	O
[	O
f∗	O
]	O
=σ	O
+	O
qt∗	O
k−1	O
(	O
k−1	O
+	O
w	O
)	O
−1k−1q∗	O
=	O
diag	O
(	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
)	O
−	O
qt∗	O
(	O
k	O
+	O
w−1	O
)	O
−1q∗	O
(	O
15.70	O
)	O
(	O
15.71	O
)	O
15.3.	O
gps	O
meet	O
glms	O
531	O
where	O
σ	O
is	O
a	O
c	O
×	O
c	O
diagonal	B
matrix	O
with	O
σcc	O
=	O
κc	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
kt	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
=	O
[	O
κc	O
(	O
x∗	O
,	O
x∗	O
)	O
]	O
.	O
to	O
compute	O
the	O
posterior	O
predictive	O
for	O
the	O
visible	B
response	O
,	O
we	O
need	O
to	O
use	O
p	O
(	O
y|x∗	O
,	O
x	O
,	O
y	O
)	O
≈	O
cat	O
(	O
y|s	O
(	O
f∗	O
)	O
)	O
n	O
(	O
f∗|e	O
[	O
f∗	O
]	O
,	O
cov	O
[	O
f∗	O
]	O
)	O
df∗	O
(	O
cid:28	O
)	O
c	O
(	O
x∗	O
)	O
k−1	O
c	O
kc	O
(	O
x∗	O
)	O
,	O
and	O
(	O
15.72	O
)	O
we	O
can	O
use	O
any	O
of	O
deterministic	O
approximations	O
to	O
the	O
softmax	B
function	O
discussed	O
in	O
sec-	O
tion	O
21.8.1.1	O
to	O
compute	O
this	O
.	O
alternatively	O
,	O
we	O
can	O
just	O
use	O
monte	O
carlo	O
.	O
15.3.2.3	O
computing	O
the	O
marginal	B
likelihood	I
using	O
arguments	O
similar	B
to	O
the	O
binary	O
case	O
,	O
we	O
can	O
show	O
that	O
ˆf	O
t	O
k−1ˆf	O
+	O
yt	O
ˆf	O
−	O
n	O
(	O
cid:2	O
)	O
(	O
cid:11	O
)	O
c	O
(	O
cid:2	O
)	O
(	O
cid:13	O
)	O
log	O
exp	O
ˆfic	O
i=1	O
c=1	O
log	O
p	O
(	O
y|x	O
)	O
≈	O
−	O
1	O
2	O
−	O
1	O
2	O
log	O
|icn	O
+	O
w	O
1	O
2	O
kw	O
1	O
2|	O
(	O
15.73	O
)	O
this	O
can	O
be	O
optimized	O
numerically	O
in	O
the	O
usual	O
way	O
.	O
15.3.2.4	O
numerical	O
and	O
computational	O
issues	O
one	O
can	O
implement	O
model	O
ﬁtting	O
in	O
o	O
(	O
t	O
cn	O
3	O
)	O
time	O
and	O
o	O
(	O
cn	O
2	O
)	O
space	O
,	O
where	O
t	O
is	O
the	O
number	O
of	O
newton	O
iterations	O
,	O
using	O
the	O
techniques	O
described	O
in	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
p50	O
)	O
.	O
prediction	O
takes	O
o	O
(	O
cn	O
3	O
+	O
cn	O
2n∗	O
)	O
time	O
,	O
where	O
n∗	O
is	O
the	O
number	O
of	O
test	O
cases	O
.	O
15.3.3	O
gps	O
for	O
poisson	B
regression	I
in	O
this	O
section	O
,	O
we	O
illustrate	O
gps	O
for	O
poisson	B
regression	I
.	O
an	O
interesting	O
application	O
of	O
this	O
is	O
to	O
spatial	O
disease	B
mapping	I
.	O
for	O
example	O
,	O
(	O
vanhatalo	O
et	O
al	O
.	O
2010	O
)	O
discuss	O
the	O
problem	O
of	O
modeling	O
the	O
relative	B
risk	I
of	O
heart	O
attack	O
in	O
different	O
regions	O
in	O
finland	O
.	O
the	O
data	O
consists	O
of	O
the	O
heart	O
attacks	O
in	O
finland	O
from	O
1996-2000	O
aggregated	O
into	O
20km	O
x	O
20km	O
lattice	B
cells	O
.	O
the	O
model	O
has	O
the	O
following	O
form	O
:	O
yi	O
∼	O
poi	O
(	O
eiri	O
)	O
(	O
15.74	O
)	O
where	O
ei	O
is	O
the	O
known	O
expected	O
number	O
of	O
deaths	O
(	O
related	O
to	O
the	O
population	O
of	O
cell	O
i	O
and	O
the	O
overall	O
death	O
rate	O
)	O
,	O
and	O
ri	O
is	O
the	O
relative	B
risk	I
of	O
cell	O
i	O
which	O
we	O
want	O
to	O
infer	O
.	O
since	O
the	O
data	O
counts	O
are	O
small	O
,	O
we	O
regularize	O
the	O
problem	O
by	O
sharing	O
information	B
with	O
spatial	O
neighbors	B
.	O
hence	O
we	O
assume	O
f	O
(	O
cid:2	O
)	O
log	O
(	O
r	O
)	O
∼	O
gp	O
(	O
0	O
,	O
κ	O
)	O
,	O
where	O
we	O
use	O
a	O
matern	O
kernel	B
with	O
ν	O
=	O
3/2	O
,	O
and	O
a	O
length	O
scale	O
and	O
magnitude	O
that	O
are	O
estimated	O
from	O
data	O
.	O
figure	O
15.8	O
gives	O
an	O
example	O
of	O
the	O
kind	O
of	O
output	O
one	O
can	O
obtain	O
from	O
this	O
method	O
,	O
based	O
on	O
data	O
from	O
911	O
locations	O
.	O
on	O
the	O
left	O
we	O
plot	O
the	O
posterior	B
mean	I
relative	O
risk	B
(	O
rr	O
)	O
,	O
and	O
on	O
the	O
right	O
,	O
the	O
posterior	O
variance	O
.	O
we	O
see	O
that	O
the	O
rr	O
is	O
higher	O
in	O
eastern	O
finland	O
,	O
which	O
is	O
consistent	B
with	O
other	O
studies	O
.	O
we	O
also	O
see	O
that	O
the	O
variance	B
in	O
the	O
north	O
is	O
higher	O
,	O
since	O
there	O
are	O
fewer	O
people	O
living	O
there	O
.	O
532	O
chapter	O
15.	O
gaussian	O
processes	O
posterior	B
mean	I
of	O
the	O
relative	B
risk	I
,	O
fic	O
60	O
posterior	O
variance	O
of	O
the	O
relative	B
risk	I
,	O
fic	O
60	O
50	O
40	O
30	O
20	O
10	O
0	O
0	O
1.4	O
1.3	O
1.2	O
1.1	O
1	O
0.9	O
0.8	O
0.7	O
50	O
40	O
30	O
20	O
10	O
20	O
30	O
10	O
(	O
a	O
)	O
0	O
0	O
10	O
20	O
30	O
(	O
b	O
)	O
0.035	O
0.03	O
0.025	O
0.02	O
0.015	O
0.01	O
0.005	O
figure	O
15.8	O
we	O
show	O
the	O
relative	B
risk	I
of	O
heart	O
disease	O
in	O
finland	O
using	O
a	O
poisson	O
gp	O
.	O
left	O
:	O
posterior	B
mean	I
.	O
right	O
:	O
posterior	O
variance	O
.	O
figure	O
generated	O
by	O
gpspatialdemolaplace	O
,	O
written	O
by	O
jarno	O
vanhatalo	O
.	O
15.4	O
connection	O
with	O
other	O
methods	O
there	O
are	O
variety	O
of	O
other	O
methods	O
in	O
statistics	O
and	O
machine	B
learning	I
that	O
are	O
closely	O
related	O
to	O
gp	O
regression/	O
classiﬁcation	B
.	O
we	O
give	O
a	O
brief	O
review	O
of	O
some	O
of	O
these	O
below	O
.	O
15.4.1	O
linear	O
models	O
compared	O
to	O
gps	O
consider	O
bayesian	O
linear	B
regression	I
for	O
d-dimensional	O
features	B
,	O
where	O
the	O
prior	O
on	O
the	O
weights	O
is	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
0	O
,	O
σ	O
)	O
.	O
the	O
posterior	B
predictive	I
distribution	I
is	O
given	O
by	O
the	O
following	O
;	O
p	O
(	O
f∗|x∗	O
,	O
x	O
,	O
y	O
)	O
=	O
n	O
(	O
μ	O
,	O
σ2	O
)	O
(	O
15.75	O
)	O
xt∗	O
a−1xt	O
y	O
1	O
σ2	O
y	O
μ	O
=	O
σ2	O
=	O
xt∗	O
a−1x∗	O
(	O
15.77	O
)	O
y	O
xt	O
x	O
+	O
σ−1	O
.	O
one	O
can	O
show	O
that	O
we	O
can	O
rewrite	O
the	O
above	O
distribution	O
as	O
(	O
15.76	O
)	O
where	O
a	O
=	O
σ−2	O
follows	O
μ	O
=	O
xt∗	O
σxt	O
(	O
k	O
+	O
σ2	O
σ2	O
=	O
xt∗	O
σx∗	O
−	O
xt∗	O
σxt	O
(	O
k	O
+	O
σ2i	O
)	O
yi	O
)	O
−1xσx∗	O
−1y	O
(	O
15.78	O
)	O
)	O
=	O
xt	O
σx	O
(	O
cid:4	O
)	O
(	O
15.79	O
)	O
where	O
we	O
have	O
deﬁned	O
k	O
=	O
xσxt	O
,	O
which	O
is	O
of	O
size	O
n	O
×	O
n	O
.	O
since	O
the	O
features	B
only	O
ever	O
appear	O
in	O
the	O
form	O
xσxt	O
,	O
xt∗	O
σxt	O
or	O
xt∗	O
σx∗	O
,	O
we	O
can	O
kernelize	O
the	O
above	O
expression	O
by	O
deﬁning	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
thus	O
we	O
see	O
that	O
bayesian	O
linear	B
regression	I
is	O
equivalent	O
to	O
a	O
gp	O
with	O
covariance	B
function	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
.	O
note	O
,	O
however	O
,	O
that	O
this	O
is	O
a	O
degenerate	B
covariance	O
function	O
,	O
since	O
it	O
has	O
at	O
most	O
d	O
non-zero	O
eigenvalues	O
.	O
intuitively	O
this	O
reﬂects	O
the	O
fact	O
that	O
the	O
model	O
can	O
only	O
represent	O
a	O
limited	O
number	O
of	O
functions	O
.	O
this	O
can	O
result	O
in	O
underﬁtting	O
,	O
since	O
the	O
model	O
is	O
not	O
ﬂexible	O
enough	O
to	O
capture	O
the	O
data	O
.	O
what	O
is	O
perhaps	O
worse	O
,	O
it	O
can	O
result	O
in	O
overconﬁdence	O
,	O
since	O
the	O
)	O
=	O
xt	O
σx	O
(	O
cid:4	O
)	O
.	O
15.4.	O
connection	O
with	O
other	O
methods	O
533	O
model	O
’	O
s	O
prior	O
is	O
so	O
impoverished	O
that	O
its	O
posterior	O
will	O
become	O
too	O
concentrated	O
.	O
so	O
not	O
only	O
is	O
the	O
model	O
wrong	O
,	O
it	O
think	O
it	O
’	O
s	O
right	O
!	O
15.4.2	O
linear	O
smoothers	O
compared	O
to	O
gps	O
(	O
cid:2	O
)	O
a	O
linear	B
smoother	I
is	O
a	O
regression	B
function	O
which	O
is	O
a	O
linear	O
function	O
of	O
the	O
training	O
outputs	O
:	O
ˆf	O
(	O
x∗	O
)	O
=	O
wi	O
(	O
x∗	O
)	O
yi	O
(	O
15.80	O
)	O
i	O
where	O
wi	O
(	O
x∗	O
)	O
is	O
called	O
the	O
weight	B
function	I
(	O
silverman	O
1984	O
)	O
.	O
(	O
do	O
not	O
confuse	O
this	O
with	O
a	O
linear	O
model	O
,	O
where	O
the	O
output	O
is	O
a	O
linear	O
function	O
of	O
the	O
input	O
vector	O
.	O
)	O
there	O
are	O
a	O
variety	O
of	O
linear	O
smoothers	O
,	O
such	O
as	O
kernel	B
regression	I
(	O
section	O
14.7.4	O
)	O
,	O
locally	B
weighted	I
regression	I
(	O
section	O
14.7.5	O
)	O
,	O
smoothing	O
splines	O
(	O
section	O
15.4.6	O
)	O
,	O
and	O
gp	O
regression	B
.	O
to	O
see	O
that	O
gp	O
regession	O
is	O
a	O
linear	B
smoother	I
,	O
note	O
that	O
the	O
mean	B
of	O
the	O
posterior	B
predictive	I
distribution	I
of	O
a	O
gp	O
is	O
given	O
by	O
n	O
(	O
cid:2	O
)	O
f	O
(	O
x∗	O
)	O
=	O
kt∗	O
(	O
k	O
+	O
σ2	O
yin	O
)	O
−1y	O
=	O
yiwi	O
(	O
x∗	O
)	O
(	O
15.81	O
)	O
where	O
wi	O
(	O
x∗	O
)	O
=	O
[	O
(	O
k	O
+	O
σ2	O
yin	O
)	O
i=1	O
−1k∗	O
]	O
i.	O
in	O
kernel	B
regression	I
,	O
we	O
derive	O
the	O
weight	B
function	I
from	O
a	O
smoothing	B
kernel	I
rather	O
than	O
a	O
mercer	O
kernel	B
,	O
so	O
it	O
is	O
clear	O
that	O
the	O
weight	B
function	I
will	O
then	O
have	O
local	O
support	O
.	O
in	O
the	O
case	O
of	O
a	O
gp	O
,	O
things	O
are	O
not	O
as	O
clear	O
,	O
since	O
the	O
weight	B
function	I
depends	O
on	O
the	O
inverse	O
of	O
k.	O
for	O
(	O
cid:10	O
)	O
n	O
certain	O
gp	O
kernel	B
functions	O
,	O
we	O
can	O
analytically	O
derive	O
the	O
form	O
of	O
wi	O
(	O
x	O
)	O
;	O
this	O
is	O
known	O
as	O
the	O
i=1	O
wi	O
(	O
x∗	O
)	O
=	O
1	O
,	O
although	O
we	O
may	O
equivalent	B
kernel	I
(	O
silverman	O
1984	O
)	O
.	O
one	O
can	O
show	O
that	O
have	O
wi	O
(	O
x∗	O
)	O
<	O
0	O
,	O
so	O
we	O
are	O
computing	O
a	O
linear	O
combination	O
but	O
not	O
a	O
convex	B
combination	I
of	O
the	O
yi	O
’	O
s	O
.	O
more	O
interestingly	O
,	O
wi	O
(	O
x∗	O
)	O
is	O
a	O
local	O
function	O
,	O
even	O
if	O
the	O
original	O
kernel	B
used	O
by	O
the	O
gp	O
is	O
not	O
local	O
.	O
futhermore	O
the	O
effective	O
bandwidth	O
of	O
the	O
equivalent	B
kernel	I
of	O
a	O
gp	O
automatically	O
decreases	O
as	O
the	O
sample	O
size	O
n	O
increases	O
,	O
whereas	O
in	O
kernel	B
smoothing	I
,	O
the	O
bandwidth	B
h	O
needs	O
to	O
be	O
set	O
by	O
hand	O
to	O
adapt	O
to	O
n	O
.	O
see	O
e.g.	O
,	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
sec	O
2.6	O
,	O
sec	O
7.1	O
)	O
for	O
details	O
.	O
15.4.2.1	O
degrees	B
of	I
freedom	I
of	O
linear	O
smoothers	O
it	O
is	O
clear	O
why	O
this	O
method	O
is	O
called	O
“	O
linear	O
”	O
,	O
but	O
why	O
is	O
it	O
called	O
a	O
“	O
smoother	O
”	O
?	O
this	O
is	O
best	O
explained	O
in	O
terms	O
of	O
gps	O
.	O
consider	O
the	O
prediction	O
on	O
the	O
training	B
set	I
:	O
f	O
=	O
k	O
(	O
k	O
+	O
σ2	O
y	O
)	O
−1y	O
(	O
cid:10	O
)	O
n	O
i=1	O
λiuiut	O
(	O
15.82	O
)	O
i	O
.	O
since	O
k	O
is	O
real	O
and	O
symmetric	B
now	O
let	O
k	O
have	O
the	O
eigendecomposition	B
k	O
=	O
positive	B
deﬁnite	I
,	O
the	O
eigenvalues	O
λi	O
are	O
real	O
and	O
non-negative	O
,	O
and	O
the	O
eigenvectors	O
ui	O
are	O
orthonormal	O
.	O
now	O
let	O
y	O
=	O
i	O
y.	O
then	O
we	O
can	O
rewrite	O
the	O
above	O
equation	O
as	O
follows	O
:	O
(	O
cid:10	O
)	O
n	O
i=1	O
γiui	O
,	O
where	O
γi	O
=	O
ut	O
γiλi	O
λi	O
+	O
σ2	O
y	O
ui	O
(	O
15.83	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
f	O
=	O
534	O
chapter	O
15.	O
gaussian	O
processes	O
this	O
is	O
the	O
same	O
as	O
equation	O
7.47	O
,	O
except	O
we	O
are	O
working	O
with	O
the	O
eigenvectors	O
of	O
the	O
gram	O
(	O
cid:10	O
)	O
1	O
,	O
matrix	O
k	O
instead	O
of	O
the	O
data	O
matrix	O
x.	O
in	O
any	O
case	O
,	O
the	O
interpretation	O
is	O
similar	B
:	O
if	O
then	O
the	O
corresponding	O
basis	O
function	O
ui	O
will	O
not	O
have	O
much	O
inﬂuence	O
.	O
consequently	O
the	O
high-	O
frequency	O
components	O
in	O
y	O
are	O
smoothed	O
out	O
.	O
the	O
effective	O
degrees	O
of	O
freedom	O
of	O
the	O
linear	B
smoother	I
is	O
deﬁned	O
as	O
dof	O
(	O
cid:2	O
)	O
tr	O
(	O
k	O
(	O
k	O
+	O
σ2	O
−1	O
)	O
=	O
n	O
(	O
cid:2	O
)	O
(	O
15.84	O
)	O
λi+σ2	O
y	O
λi	O
λi	O
yi	O
)	O
λi	O
+	O
σ2	O
y	O
i=1	O
this	O
speciﬁes	O
how	O
“	O
wiggly	O
”	O
the	O
curve	O
is	O
.	O
15.4.3	O
svms	O
compared	O
to	O
gps	O
we	O
saw	O
in	O
section	O
14.5.2	O
that	O
the	O
svm	O
objective	O
for	O
binary	B
classiﬁcation	I
is	O
given	O
by	O
equation	O
14.57	O
j	O
(	O
w	O
)	O
=	O
(	O
1	O
−	O
yifi	O
)	O
+	O
n	O
(	O
cid:2	O
)	O
i=1	O
1	O
2	O
||w||2	O
+	O
c	O
(	O
cid:10	O
)	O
i	O
,	O
j	O
αiαjxt	O
we	O
also	O
know	O
from	O
equation	O
14.59	O
that	O
the	O
optimal	O
solution	O
has	O
the	O
form	O
w	O
=	O
i	O
αixi	O
,	O
i	O
xj	O
.	O
kernelizing	O
we	O
get	O
||w||2	O
=	O
αkα	O
.	O
from	O
equation	O
14.61	O
,	O
and	O
so	O
||w||2	O
=	O
absorbing	O
the	O
ˆw0	O
term	O
into	O
one	O
of	O
the	O
kernels	O
,	O
we	O
have	O
f	O
=	O
kα	O
,	O
so	O
||w||2	O
=	O
f	O
t	O
k−1f	O
.	O
hence	O
the	O
svm	O
objective	O
can	O
be	O
rewritten	O
as	O
(	O
1	O
−	O
yifi	O
)	O
+	O
f	O
t	O
f	O
+	O
c	O
n	O
(	O
cid:2	O
)	O
j	O
(	O
f	O
)	O
=	O
(	O
15.86	O
)	O
(	O
15.85	O
)	O
(	O
cid:10	O
)	O
compare	O
this	O
to	O
map	O
estimation	O
for	O
gp	O
classiﬁer	O
:	O
1	O
2	O
i=1	O
f	O
t	O
f	O
−	O
n	O
(	O
cid:2	O
)	O
i=1	O
j	O
(	O
f	O
)	O
=	O
1	O
2	O
log	O
p	O
(	O
yi|fi	O
)	O
(	O
15.87	O
)	O
it	O
is	O
tempting	O
to	O
think	O
that	O
we	O
can	O
“	O
convert	O
”	O
an	O
svm	O
into	O
a	O
gp	O
by	O
ﬁguring	O
out	O
what	O
likelihood	B
would	O
be	O
equivalent	O
to	O
the	O
hinge	B
loss	I
.	O
however	O
,	O
it	O
turns	O
out	O
there	O
is	O
no	O
such	O
likelihood	B
(	O
sollich	O
2002	O
)	O
,	O
although	O
there	O
is	O
a	O
pseudo-likelihood	B
that	O
matches	O
the	O
svm	O
(	O
see	O
section	O
14.5.5	O
)	O
.	O
from	O
figure	O
6.7	O
we	O
saw	O
that	O
the	O
hinge	B
loss	I
and	O
the	O
logistic	B
loss	O
(	O
as	O
well	O
as	O
the	O
probit	B
loss	O
)	O
are	O
quite	O
similar	B
to	O
each	O
other	O
.	O
the	O
main	O
difference	O
is	O
that	O
the	O
hinge	B
loss	I
is	O
strictly	O
0	O
for	O
errors	O
larger	O
than	O
1.	O
this	O
gives	O
rise	O
to	O
a	O
sparse	B
solution	O
.	O
in	O
section	O
14.3.2	O
,	O
we	O
discussed	O
other	O
ways	O
to	O
derive	O
sparse	O
kernel	O
machines	O
.	O
we	O
discuss	O
the	O
connection	O
between	O
these	O
methods	O
and	O
gps	O
below	O
.	O
15.4.4	O
l1vm	O
and	O
rvms	O
compared	O
to	O
gps	O
sparse	O
kernel	O
machines	O
are	O
just	O
linear	O
models	O
with	O
basis	B
function	I
expansion	I
of	O
the	O
form	O
φ	O
(	O
x	O
)	O
=	O
[	O
κ	O
(	O
x	O
,	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
κ	O
(	O
x	O
,	O
xn	O
)	O
]	O
.	O
from	O
section	O
15.4.1	O
,	O
we	O
know	O
that	O
this	O
is	O
equivalent	O
to	O
a	O
gp	O
with	O
the	O
following	O
kernel	B
:	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
φj	O
(	O
x	O
)	O
φj	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
1	O
αj	O
(	O
15.88	O
)	O
d	O
(	O
cid:2	O
)	O
j=1	O
15.4.	O
connection	O
with	O
other	O
methods	O
535	O
where	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
0	O
,	O
diag	O
(	O
α−1	O
j	O
)	O
)	O
.	O
this	O
kernel	B
function	I
has	O
two	O
interesting	O
properties	O
.	O
first	O
,	O
it	O
is	O
degenerate	B
,	O
meaning	O
it	O
has	O
at	O
most	O
n	O
non-zero	O
eigenvalues	O
,	O
so	O
the	O
joint	B
distribution	I
p	O
(	O
f	O
,	O
f∗	O
)	O
will	O
be	O
highly	O
constrained	O
.	O
second	O
,	O
the	O
kernel	B
depends	O
on	O
the	O
training	O
data	O
.	O
this	O
can	O
cause	O
the	O
model	O
to	O
be	O
overconﬁdent	O
when	O
extrapolating	O
beyond	O
the	O
training	O
data	O
.	O
to	O
see	O
this	O
,	O
consider	O
a	O
point	O
x∗	O
far	O
outside	O
the	O
convex	B
hull	I
of	O
the	O
data	O
.	O
all	O
the	O
basis	B
functions	I
will	O
have	O
values	O
close	O
to	O
0	O
,	O
so	O
the	O
prediction	O
will	O
back	O
off	O
to	O
the	O
mean	B
of	O
the	O
gp	O
.	O
more	O
worryingly	O
,	O
the	O
variance	B
will	O
back	O
off	O
to	O
the	O
noise	O
variance	O
.	O
by	O
contrast	O
,	O
when	O
using	O
a	O
non-degenerate	O
kernel	B
function	I
,	O
the	O
predictive	B
variance	O
increases	O
as	O
we	O
move	O
away	O
from	O
the	O
training	O
data	O
,	O
as	O
desired	O
.	O
see	O
(	O
rasmussen	O
and	O
quiñonero-candela	O
2005	O
)	O
for	O
further	O
discussion	O
.	O
15.4.5	O
neural	B
networks	I
compared	O
to	O
gps	O
in	O
section	O
16.5	O
,	O
we	O
will	O
discuss	O
neural	B
networks	I
,	O
which	O
are	O
a	O
nonlinear	O
generalization	B
of	O
glms	O
.	O
in	O
the	O
binary	B
classiﬁcation	I
case	O
,	O
a	O
neural	B
network	I
is	O
deﬁned	O
by	O
a	O
logistic	B
regression	I
model	O
applied	O
to	O
a	O
logistic	B
regression	I
model	O
:	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
ber	O
y|sigm	O
wt	O
sigm	O
(	O
vx	O
)	O
(	O
15.89	O
)	O
it	O
turns	O
out	O
there	O
is	O
an	O
interesting	O
connection	O
between	O
neural	B
networks	I
and	O
gaussian	O
processes	O
,	O
as	O
ﬁrst	O
pointed	O
out	O
by	O
(	O
neal	O
1996	O
)	O
.	O
to	O
explain	O
the	O
connection	O
,	O
we	O
follow	O
the	O
presentation	O
of	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
p91	O
)	O
.	O
consider	O
a	O
neural	B
network	I
for	O
regression	B
with	O
one	O
hidden	B
layer	I
.	O
this	O
has	O
the	O
form	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
n	O
(	O
y|f	O
(	O
x	O
;	O
θ	O
)	O
,	O
σ2	O
)	O
where	O
f	O
(	O
x	O
)	O
=	O
b	O
+	O
h	O
(	O
cid:2	O
)	O
j=1	O
vjg	O
(	O
x	O
;	O
uj	O
)	O
(	O
15.90	O
)	O
(	O
15.91	O
)	O
where	O
b	O
is	O
the	O
offset	O
of	O
bias	B
term	I
,	O
vj	O
is	O
the	O
output	O
weight	O
from	O
hidden	B
unit	O
j	O
to	O
the	O
response	O
y	O
,	O
uj	O
are	O
the	O
inputs	O
weights	O
to	O
unit	O
j	O
from	O
the	O
input	O
x	O
,	O
and	O
g	O
(	O
)	O
is	O
the	O
hidden	B
unit	O
activation	B
function	O
.	O
this	O
is	O
typically	O
the	O
sigmoid	B
or	O
tanh	O
function	O
,	O
but	O
can	O
be	O
any	O
smooth	O
function	O
.	O
j	O
n	O
(	O
vj|0	O
,	O
σ2	O
w	O
)	O
,	O
u	O
∼	O
’	O
let	O
us	O
use	O
the	O
following	O
priors	O
on	O
the	O
weights	O
:	O
where	O
b	O
∼	O
n	O
(	O
0	O
,	O
σ2	O
j	O
p	O
(	O
uj	O
)	O
for	O
some	O
unspeciﬁed	O
p	O
(	O
uj	O
)	O
.	O
denoting	O
all	O
the	O
weights	O
by	O
θ	O
we	O
have	O
b	O
)	O
v	O
∼	O
’	O
eθ	O
[	O
f	O
(	O
x	O
)	O
]	O
=	O
0	O
)	O
]	O
=	O
σ2	O
eθ	O
[	O
f	O
(	O
x	O
)	O
f	O
(	O
x	O
(	O
cid:4	O
)	O
b	O
+	O
(	O
cid:2	O
)	O
j	O
=	O
σ2	O
b	O
+	O
hσ2	O
v	O
ev	O
[	O
g	O
(	O
x	O
;	O
uj	O
)	O
g	O
(	O
x	O
(	O
cid:4	O
)	O
σ2	O
v	O
eu	O
[	O
g	O
(	O
x	O
;	O
u	O
)	O
g	O
(	O
x	O
(	O
cid:4	O
)	O
;	O
u	O
)	O
]	O
;	O
uj	O
)	O
]	O
(	O
15.92	O
)	O
(	O
15.93	O
)	O
(	O
15.94	O
)	O
v	O
scale	O
as	O
ω2/h	O
where	O
the	O
last	O
equality	O
follows	O
since	O
the	O
h	O
hidden	B
units	I
are	O
iid	B
.	O
(	O
since	O
more	O
hidden	B
units	I
will	O
increase	O
the	O
input	O
to	O
the	O
ﬁnal	O
node	O
,	O
so	O
we	O
should	O
scale	O
down	O
the	O
magnitude	O
of	O
the	O
weights	O
)	O
,	O
then	O
the	O
last	O
term	O
becomes	O
ω2	O
;	O
u	O
)	O
]	O
.	O
this	O
is	O
a	O
sum	O
over	O
h	O
iid	B
random	O
variables	O
.	O
assuming	O
that	O
g	O
is	O
bounded	O
,	O
we	O
can	O
apply	O
the	O
central	B
limit	I
theorem	I
.	O
the	O
result	O
is	O
that	O
as	O
h	O
→	O
∞	O
,	O
we	O
get	O
a	O
gaussian	O
process	O
.	O
if	O
we	O
let	O
σ2	O
eu	O
[	O
g	O
(	O
x	O
;	O
u	O
)	O
g	O
(	O
x	O
(	O
cid:4	O
)	O
536	O
chapter	O
15.	O
gaussian	O
processes	O
4	O
0	O
ʼ	O
x	O
,	O
t	O
u	O
p	O
n	O
i	O
−4	O
−4	O
−0.5	O
0.95	O
0	O
0.5	O
0.5	O
0	O
0.95	O
−0.5	O
0	O
input	O
,	O
x	O
(	O
a	O
)	O
σ	O
=	O
10	O
σ	O
=	O
3	O
σ	O
=	O
1	O
1	O
0	O
−1	O
)	O
x	O
(	O
f	O
,	O
t	O
u	O
p	O
t	O
u	O
o	O
4	O
−4	O
4	O
0	O
input	O
,	O
x	O
(	O
b	O
)	O
figure	O
15.9	O
(	O
a	O
)	O
covariance	B
function	O
κn	O
n	O
(	O
x	O
,	O
x	O
(	O
cid:2	O
)	O
this	O
kernel	B
,	O
using	O
various	O
values	O
of	O
σ.	O
figure	O
generated	O
by	O
gpnndemo	O
,	O
written	O
by	O
chris	O
williams.	O
)	O
for	O
σ0	O
=	O
10	O
,	O
σ	O
=	O
10	O
.	O
(	O
b	O
)	O
samples	B
from	O
from	O
a	O
gp	O
with	O
)	O
z	O
j=1	O
ujxj	O
)	O
,	O
where	O
erf	B
(	O
z	O
)	O
=	O
e−t2	O
dt	O
,	O
and	O
we	O
choose	O
u	O
∼	O
n	O
(	O
0	O
,	O
σ	O
)	O
,	O
then	O
(	O
williams	O
1998	O
)	O
showed	O
that	O
the	O
covariance	B
if	O
we	O
use	O
as	O
activation	B
/	O
transfer	B
function	I
g	O
(	O
x	O
;	O
u	O
)	O
=erf	O
(	O
u0	O
+	O
√	O
π	O
(	O
cid:13	O
)	O
2/	O
kernel	B
has	O
the	O
form	O
(	O
cid:11	O
)	O
0	O
(	O
cid:10	O
)	O
d	O
κn	O
n	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
−1	O
sin	O
2	O
π	O
(	O
cid:17	O
)	O
2˜xt	O
σ˜x	O
(	O
cid:4	O
)	O
(	O
1	O
+	O
2˜xt	O
σ˜x	O
)	O
(	O
1	O
+	O
2	O
(	O
˜x	O
(	O
cid:4	O
)	O
)	O
t	O
σ˜x	O
(	O
cid:4	O
)	O
)	O
(	O
15.95	O
)	O
where	O
˜x	O
=	O
(	O
1	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
.	O
this	O
is	O
a	O
true	O
“	O
neural	B
network	I
”	O
kernel	B
,	O
unlike	O
the	O
“	O
sigmoid	B
”	O
kernel	B
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
tanh	O
(	O
a	O
+	O
bxt	O
x	O
(	O
cid:4	O
)	O
)	O
,	O
which	O
is	O
not	O
positive	O
deﬁnite	O
.	O
figure	O
15.9	O
(	O
a	O
)	O
illustrates	O
this	O
kernel	B
when	O
d	O
=	O
2	O
and	O
σ	O
=	O
diag	O
(	O
σ2	O
0	O
,	O
σ2	O
)	O
.	O
figure	O
15.9	O
(	O
b	O
)	O
shows	O
some	O
functions	O
sampled	O
from	O
the	O
corresponding	O
gp	O
.	O
these	O
are	O
equivalent	O
to	O
functions	O
which	O
are	O
superpositions	O
of	O
erf	B
(	O
u0	O
+	O
ux	O
)	O
where	O
u0	O
and	O
u	O
are	O
random	O
.	O
as	O
σ2	O
increases	O
,	O
the	O
variance	B
of	O
u	O
increases	O
,	O
so	O
the	O
function	O
varies	O
more	O
quickly	O
.	O
unlike	O
the	O
rbf	O
kernel	B
,	O
functions	O
sampled	O
from	O
this	O
kernel	B
do	O
not	O
tend	O
to	O
0	O
away	O
from	O
the	O
data	O
,	O
but	O
rather	O
they	O
tend	O
to	O
remain	O
at	O
the	O
same	O
value	O
they	O
had	O
at	O
the	O
“	O
edge	O
”	O
of	O
the	O
data	O
.	O
of	O
the	O
form	O
g	O
(	O
x	O
;	O
u	O
)	O
=	O
exp	O
(	O
−|x	O
−	O
u|2/	O
(	O
2σ2	O
coresponding	O
kernel	B
is	O
equivalent	O
to	O
the	O
rbf	O
or	O
se	O
kernel	B
.	O
now	O
suppose	O
we	O
use	O
an	O
rbf	O
network	O
,	O
which	O
is	O
equivalent	O
to	O
a	O
hidden	B
unit	O
activation	B
function	O
ui	O
)	O
,	O
one	O
can	O
show	O
that	O
the	O
if	O
u	O
∼	O
n	O
(	O
0	O
,	O
σ2	O
g	O
)	O
)	O
.	O
15.4.6	O
smoothing	O
splines	O
compared	O
to	O
gps	O
*	O
smoothing	O
splines	O
are	O
a	O
widely	O
used	O
non-parametric	O
method	O
for	O
smoothly	O
interpolating	O
data	O
(	O
green	O
and	O
silverman	O
1994	O
)	O
.	O
they	O
are	O
are	O
a	O
special	O
case	O
of	O
gps	O
,	O
as	O
we	O
will	O
see	O
.	O
they	O
are	O
usually	O
used	O
when	O
the	O
input	O
is	O
1	O
or	O
2	O
dimensional	O
.	O
15.4.6.1	O
univariate	O
splines	O
the	O
basic	O
idea	O
is	O
to	O
ﬁt	O
a	O
function	O
f	O
by	O
minimizing	O
the	O
discrepancy	O
to	O
the	O
data	O
plus	O
a	O
smoothing	O
if	O
we	O
penalize	O
the	O
m	O
’	O
th	O
derivative	O
of	O
the	O
term	O
that	O
penalizes	O
functions	O
that	O
are	O
“	O
too	O
wiggly	O
”	O
.	O
15.4.	O
connection	O
with	O
other	O
methods	O
function	O
,	O
the	O
objective	O
becomes	O
j	O
(	O
f	O
)	O
=	O
(	O
f	O
(	O
xi	O
)	O
−	O
yi	O
)	O
2	O
+	O
λ	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
cid:28	O
)	O
dm	O
dxm	O
(	O
f	O
(	O
x	O
)	O
)	O
2dx	O
537	O
(	O
15.96	O
)	O
one	O
can	O
show	O
(	O
green	O
and	O
silverman	O
1994	O
)	O
that	O
the	O
solution	O
is	O
a	O
piecewise	B
polynomial	I
where	O
the	O
polynomials	O
have	O
order	O
2m	O
−	O
1	O
in	O
the	O
interior	O
bins	O
[	O
xi−1	O
,	O
xi	O
]	O
(	O
denoted	O
i	O
)	O
,	O
and	O
order	O
m	O
−	O
1	O
in	O
the	O
two	O
outermost	O
intervals	O
(	O
−∞	O
,	O
x1	O
]	O
and	O
[	O
xn	O
,	O
∞	O
)	O
:	O
(	O
cid:13	O
)	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
(	O
cid:13	O
)	O
f	O
(	O
x	O
)	O
=	O
βjxj	O
+	O
i	O
(	O
x	O
∈	O
i	O
)	O
αi	O
(	O
x	O
−	O
xi	O
)	O
2m−1	O
+	O
+	O
i	O
(	O
x	O
(	O
cid:13	O
)	O
∈	O
i	O
)	O
αi	O
(	O
x	O
−	O
xi	O
)	O
m−1	O
+	O
m−1	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
j=0	O
i=1	O
i=1	O
(	O
cid:13	O
)	O
for	O
example	O
,	O
if	O
m	O
=	O
2	O
,	O
we	O
get	O
the	O
(	O
natural	O
)	O
cubic	B
spline	I
f	O
(	O
x	O
)	O
=β	O
0	O
+	O
β1x	O
+	O
i	O
(	O
x	O
∈	O
i	O
)	O
αi	O
(	O
x	O
−	O
xi	O
)	O
3	O
+	O
+	O
i	O
(	O
x	O
(	O
cid:13	O
)	O
∈	O
i	O
)	O
(	O
cid:11	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
cid:13	O
)	O
αi	O
(	O
x	O
−	O
xi	O
)	O
+	O
(	O
cid:11	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
(	O
15.97	O
)	O
(	O
15.98	O
)	O
which	O
is	O
a	O
series	O
of	O
truncated	O
cubic	O
polynomials	O
,	O
whose	O
left	O
hand	O
sides	O
are	O
located	O
at	O
each	O
of	O
the	O
n	O
training	O
points	O
.	O
(	O
the	O
fact	O
that	O
the	O
model	O
is	O
linear	O
on	O
the	O
edges	B
prevents	O
it	O
from	O
extrapolating	O
too	O
wildly	O
beyond	O
the	O
range	O
of	O
the	O
data	O
;	O
if	O
we	O
drop	O
this	O
requirement	O
,	O
we	O
get	O
an	O
“	O
unrestricted	O
”	O
spline	B
.	O
)	O
−1φt	O
y	O
,	O
where	O
the	O
+	O
for	O
i	O
=	O
2	O
:	O
n	O
−	O
1	O
and	O
(	O
x	O
−	O
xi	O
)	O
+	O
for	O
i	O
=	O
1	O
or	O
i	O
=	O
n	O
.	O
columns	O
of	O
φ	O
are	O
1	O
,	O
xi	O
and	O
(	O
x	O
−	O
xi	O
)	O
3	O
however	O
,	O
we	O
can	O
also	O
derive	O
an	O
o	O
(	O
n	O
)	O
time	O
method	O
(	O
green	O
and	O
silverman	O
1994	O
,	O
sec	O
2.3.3	O
)	O
.	O
we	O
can	O
clearly	O
ﬁt	O
this	O
model	O
using	O
ridge	B
regression	I
:	O
ˆw	O
=	O
(	O
φt	O
φ	O
+	O
λin	O
)	O
15.4.6.2	O
regression	B
splines	O
in	O
general	O
,	O
we	O
can	O
place	O
the	O
polynomials	O
at	O
a	O
ﬁxed	O
set	O
of	O
k	O
locations	O
known	O
as	O
knots	B
,	O
denoted	O
ξk	O
.	O
the	O
result	O
is	O
called	O
a	O
regression	B
spline	I
.	O
this	O
is	O
a	O
parametric	B
model	I
,	O
which	O
uses	O
basis	B
function	I
expansion	I
of	O
the	O
following	O
form	O
(	O
where	O
we	O
drop	O
the	O
interior/	O
exterior	O
distinction	O
for	O
simplicity	O
)	O
:	O
f	O
(	O
x	O
)	O
=β	O
0	O
+	O
β1x	O
+	O
αj	O
(	O
x	O
−	O
ξk	O
)	O
3	O
+	O
(	O
15.99	O
)	O
k=1	O
choosing	O
the	O
number	O
and	O
locations	O
of	O
the	O
knots	B
is	O
just	O
like	O
choosing	O
the	O
number	O
and	O
values	O
of	O
the	O
support	B
vectors	I
in	O
section	O
14.3.2.	O
if	O
we	O
impose	O
an	O
(	O
cid:6	O
)	O
2	O
regularizer	O
on	O
the	O
regression	B
coefficients	O
αj	O
,	O
the	O
method	O
is	O
known	O
as	O
penalized	B
splines	I
.	O
see	O
section	O
9.6.1	O
for	O
a	O
practical	O
example	O
of	O
penalized	B
splines	I
.	O
15.4.6.3	O
the	O
connection	O
with	O
gps	O
one	O
can	O
show	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
p139	O
)	O
that	O
the	O
cubic	B
spline	I
is	O
the	O
map	O
estimate	O
of	O
the	O
following	O
function	O
f	O
(	O
x	O
)	O
=β	O
0	O
+	O
β1x	O
+	O
r	O
(	O
x	O
)	O
(	O
15.100	O
)	O
k	O
(	O
cid:2	O
)	O
538	O
chapter	O
15.	O
gaussian	O
processes	O
where	O
p	O
(	O
βj	O
)	O
∝	O
1	O
(	O
so	O
that	O
we	O
don	O
’	O
t	O
penalize	O
the	O
zero	O
’	O
th	O
and	O
ﬁrst	O
derivatives	O
of	O
f	O
)	O
,	O
and	O
r	O
(	O
x	O
)	O
∼	O
gp	O
(	O
0	O
,	O
σ2	O
f	O
κsp	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
(	O
cid:28	O
)	O
1	O
κsp	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
(	O
cid:2	O
)	O
)	O
)	O
,	O
where	O
(	O
x	O
−	O
u	O
)	O
+	O
(	O
x	O
(	O
cid:4	O
)	O
−	O
u	O
)	O
+du	O
(	O
15.101	O
)	O
0	O
note	O
that	O
the	O
kernel	B
in	O
equation	O
15.101	O
is	O
rather	O
unnatural	O
,	O
and	O
indeed	O
posterior	O
samples	O
from	O
the	O
resulting	O
gp	O
are	O
rather	O
unsmooth	O
.	O
however	O
,	O
the	O
posterior	O
mode/mean	O
is	O
smooth	O
.	O
this	O
shows	O
that	O
regularizers	O
don	O
’	O
t	O
always	O
make	O
good	O
priors	O
.	O
15.4.6.4	O
2d	O
input	O
(	O
thin-plate	O
splines	O
)	O
one	O
can	O
generalize	B
cubic	O
splines	O
to	O
2d	O
input	O
by	O
deﬁning	O
a	O
regularizer	O
of	O
the	O
following	O
form	O
:	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
(	O
cid:18	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
2	O
(	O
cid:8	O
)	O
∂2f	O
(	O
x	O
)	O
∂x2	O
1	O
(	O
cid:19	O
)	O
(	O
cid:9	O
)	O
2	O
(	O
cid:9	O
)	O
2	O
(	O
cid:8	O
)	O
+	O
∂2f	O
(	O
x	O
)	O
∂x2	O
2	O
∂2f	O
(	O
x	O
)	O
∂x1∂x2	O
dx1dx2	O
(	O
15.102	O
)	O
one	O
can	O
show	O
that	O
the	O
solution	O
has	O
the	O
form	O
+	O
2	O
n	O
(	O
cid:2	O
)	O
αiφi	O
(	O
x	O
)	O
f	O
(	O
x	O
)	O
=β	O
0	O
+	O
βt	O
1	O
x	O
+	O
(	O
15.103	O
)	O
where	O
φi	O
(	O
x	O
)	O
=	O
η	O
(	O
||x	O
−	O
xi||	O
)	O
,	O
and	O
η	O
(	O
z	O
)	O
=	O
z2	O
log	O
z2	O
.	O
this	O
is	O
known	O
as	O
a	O
thin	B
plate	I
spline	I
.	O
this	O
is	O
equivalent	O
to	O
map	O
estimation	O
with	O
a	O
gp	O
whose	O
kernel	B
is	O
deﬁned	O
in	O
(	O
williams	O
and	O
fitzgibbon	O
2006	O
)	O
.	O
i=1	O
15.4.6.5	O
higher-dimensional	O
inputs	O
it	O
is	O
hard	O
to	O
analytically	O
solve	O
for	O
the	O
form	O
of	O
the	O
optimal	O
solution	O
when	O
using	O
higher-order	O
inputs	O
.	O
however	O
,	O
in	O
the	O
parametric	O
regression	O
spline	B
setting	O
,	O
where	O
we	O
forego	O
the	O
regularizer	O
on	O
f	O
,	O
we	O
have	O
more	O
freedom	O
in	O
deﬁning	O
our	O
basis	B
functions	I
.	O
one	O
way	O
to	O
handle	O
multiple	O
inputs	O
is	O
to	O
use	O
a	O
tensor	B
product	I
basis	I
,	O
deﬁned	O
as	O
the	O
cross	O
product	O
of	O
1d	O
basis	B
functions	I
.	O
for	O
example	O
,	O
for	O
2d	O
input	O
,	O
we	O
can	O
deﬁne	O
f	O
(	O
x1	O
,	O
x2	O
)	O
=β	O
0	O
+	O
β1m	O
(	O
x1	O
−	O
ξ1m	O
)	O
+	O
+	O
β2m	O
(	O
x2	O
−	O
ξ2m	O
)	O
+	O
(	O
cid:2	O
)	O
m	O
β12m	O
(	O
x1	O
−	O
ξ1m	O
)	O
+	O
(	O
x2	O
−	O
ξ2m	O
)	O
+	O
(	O
15.104	O
)	O
(	O
15.105	O
)	O
(	O
cid:2	O
)	O
m	O
(	O
cid:2	O
)	O
+	O
m	O
it	O
is	O
clear	O
that	O
for	O
high-dimensional	O
data	O
,	O
we	O
can	O
not	O
allow	O
higher-order	O
interactions	O
,	O
because	O
there	O
will	O
be	O
too	O
many	O
parameters	O
to	O
ﬁt	O
.	O
one	O
approach	O
to	O
this	O
problem	O
is	O
to	O
use	O
a	O
search	O
procedure	O
to	O
look	O
for	O
useful	O
interaction	O
terms	O
.	O
this	O
is	O
known	O
as	O
mars	O
,	O
which	O
stands	O
for	O
“	O
multivariate	B
adaptive	I
regression	I
splines	I
”	O
.	O
see	O
section	O
16.3.3	O
for	O
details	O
.	O
15.4.7	O
rkhs	O
methods	O
compared	O
to	O
gps	O
*	O
we	O
can	O
generalize	B
the	O
idea	O
of	O
penalizing	O
derivatives	O
of	O
functions	O
,	O
as	O
used	O
in	O
smoothing	O
splines	O
,	O
to	O
ﬁt	O
functions	O
with	O
a	O
more	O
general	O
notion	O
of	O
smoothness	O
.	O
recall	B
from	O
section	O
14.2.3	O
that	O
15.4.	O
connection	O
with	O
other	O
methods	O
539	O
mercer	O
’	O
s	O
theorem	O
says	O
that	O
any	O
positive	B
deﬁnite	I
kernel	I
function	O
can	O
be	O
represented	O
in	O
terms	O
of	O
eigenfunctions	O
:	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:4	O
)	O
)	O
=	O
λiφi	O
(	O
x	O
)	O
φi	O
(	O
x	O
(	O
cid:4	O
)	O
)	O
∞	O
(	O
cid:2	O
)	O
i=1	O
∞	O
(	O
cid:2	O
)	O
the	O
φi	O
form	O
an	O
orthormal	O
basis	O
for	O
a	O
function	O
space	O
:	O
hk	O
=	O
{	O
f	O
:	O
f	O
(	O
x	O
)	O
=	O
i	O
/λi	O
<	O
∞	O
}	O
f	O
2	O
(	O
cid:10	O
)	O
∞	O
fiφi	O
(	O
x	O
)	O
,	O
∞	O
(	O
cid:2	O
)	O
now	O
deﬁne	O
the	O
inner	O
product	O
between	O
two	O
functions	O
f	O
(	O
x	O
)	O
=	O
i=1	O
i=1	O
i=1	O
giφi	O
(	O
x	O
)	O
in	O
this	O
space	O
as	O
follows	O
:	O
(	O
cid:14	O
)	O
f	O
,	O
g	O
(	O
cid:15	O
)	O
h	O
(	O
cid:2	O
)	O
∞	O
(	O
cid:2	O
)	O
figi	O
λi	O
i=1	O
(	O
cid:10	O
)	O
∞	O
i=1	O
fiφi	O
(	O
x	O
)	O
and	O
g	O
(	O
x	O
)	O
=	O
(	O
15.106	O
)	O
(	O
15.107	O
)	O
(	O
15.108	O
)	O
in	O
exercise	O
15.1	O
,	O
we	O
show	O
that	O
this	O
deﬁnition	O
implies	O
that	O
(	O
cid:14	O
)	O
κ	O
(	O
x1	O
,	O
·	O
)	O
,	O
κ	O
(	O
x2	O
,	O
·	O
)	O
(	O
cid:15	O
)	O
h	O
=	O
κ	O
(	O
x1	O
,	O
x2	O
)	O
(	O
15.109	O
)	O
this	O
is	O
called	O
the	O
reproducing	B
property	I
,	O
and	O
the	O
space	O
of	O
functions	O
hk	O
is	O
called	O
a	O
reproducing	O
kernel	O
hilbert	O
space	O
or	O
rkhs	O
.	O
now	O
consider	O
an	O
optimization	B
problem	O
of	O
the	O
form	O
(	O
15.110	O
)	O
(	O
15.111	O
)	O
(	O
15.113	O
)	O
n	O
(	O
cid:2	O
)	O
j	O
(	O
f	O
)	O
=	O
1	O
2σ2	O
y	O
(	O
yi	O
−	O
f	O
(	O
xi	O
)	O
)	O
2	O
+	O
1	O
2	O
where	O
||f||j	O
is	O
the	O
norm	B
of	I
a	I
function	I
:	O
i=1	O
||f||2	O
h	O
||f||h	O
=	O
(	O
cid:14	O
)	O
f	O
,	O
f	O
(	O
cid:15	O
)	O
h	O
=	O
∞	O
(	O
cid:2	O
)	O
i=1	O
f	O
2	O
i	O
λi	O
the	O
intuition	O
is	O
that	O
functions	O
that	O
are	O
complex	O
wrt	O
the	O
kernel	B
will	O
have	O
large	O
norms	O
,	O
because	O
they	O
will	O
need	O
many	O
eigenfunctions	O
to	O
represent	O
them	O
.	O
we	O
want	O
to	O
pick	O
a	O
simple	O
function	O
that	O
provides	O
a	O
good	O
ﬁt	O
to	O
the	O
data	O
.	O
one	O
can	O
show	O
(	O
see	O
e.g.	O
,	O
(	O
schoelkopf	O
and	O
smola	O
2002	O
)	O
)	O
that	O
the	O
solution	O
must	O
have	O
the	O
form	O
f	O
(	O
x	O
)	O
=	O
αiκ	O
(	O
x	O
,	O
xi	O
)	O
(	O
15.112	O
)	O
n	O
(	O
cid:2	O
)	O
i=1	O
this	O
is	O
known	O
as	O
the	O
representer	B
theorem	I
,	O
and	O
holds	O
for	O
other	O
convex	B
loss	O
functions	O
besides	O
squared	B
error	I
.	O
(	O
cid:10	O
)	O
n	O
i=1	O
αiκ	O
(	O
x	O
,	O
xi	O
)	O
and	O
using	O
the	O
reproducing	O
we	O
can	O
solve	O
for	O
the	O
α	O
by	O
substituting	O
in	O
f	O
(	O
x	O
)	O
=	O
property	O
to	O
get	O
j	O
(	O
α	O
)	O
=	O
|y	O
−	O
kα|2	O
+	O
1	O
2σ2	O
y	O
αt	O
kα	O
1	O
2	O
540	O
minimizing	O
wrt	O
α	O
we	O
ﬁnd	O
−1	O
yi	O
)	O
ˆα	O
=	O
(	O
k	O
+	O
σ2	O
(	O
cid:2	O
)	O
and	O
hence	O
ˆf	O
(	O
x∗	O
)	O
=	O
ˆαiκ	O
(	O
x∗	O
,	O
xi	O
)	O
=	O
kt∗	O
(	O
k	O
+	O
σ2	O
yi	O
)	O
chapter	O
15.	O
gaussian	O
processes	O
−1y	O
(	O
15.114	O
)	O
(	O
15.115	O
)	O
i	O
this	O
is	O
identical	O
to	O
equation	O
15.18	O
,	O
the	O
posterior	B
mean	I
of	O
a	O
gp	O
predictive	B
distribution	O
.	O
indeed	O
,	O
since	O
the	O
mean	B
and	O
mode	B
of	O
a	O
gaussian	O
are	O
the	O
same	O
,	O
we	O
can	O
see	O
that	O
linear	O
regresson	O
with	O
an	O
rkhs	O
regularizer	O
is	O
equivalent	O
to	O
map	O
estimation	O
with	O
a	O
gp	O
.	O
an	O
analogous	O
statement	O
holds	O
for	O
the	O
gp	O
logistic	B
regression	I
case	O
,	O
which	O
also	O
uses	O
a	O
convex	B
likelihood	O
/	O
loss	B
function	I
.	O
15.5	O
gp	O
latent	O
variable	O
model	O
in	O
section	O
14.4.4	O
,	O
we	O
discussed	O
kernel	B
pca	O
,	O
which	O
applies	O
the	O
kernel	B
trick	I
to	O
regular	B
pca	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
a	O
different	O
way	O
to	O
combine	O
kernels	O
with	O
probabilistic	O
pca	O
.	O
the	O
resulting	O
method	O
is	O
known	O
as	O
the	O
gp-lvm	O
,	O
which	O
stands	O
for	O
“	O
gaussian	O
process	O
latent	O
variable	O
model	O
”	O
(	O
lawrence	O
2005	O
)	O
.	O
to	O
explain	O
the	O
method	O
,	O
we	O
start	O
with	O
ppca	O
.	O
recall	B
from	O
section	O
12.2.4	O
that	O
the	O
ppca	O
model	O
is	O
as	O
follows	O
:	O
p	O
(	O
zi	O
)	O
=n	O
(	O
zi|0	O
,	O
i	O
)	O
p	O
(	O
yi|zi	O
,	O
θ	O
)	O
=n	O
(	O
yi|wzi	O
,	O
σ2i	O
)	O
(	O
15.116	O
)	O
(	O
15.117	O
)	O
we	O
can	O
ﬁt	O
this	O
model	O
by	O
maximum	O
likelihood	O
,	O
by	O
integrating	O
out	O
the	O
zi	O
and	O
maximizing	O
w	O
(	O
and	O
σ2	O
)	O
.	O
the	O
objective	O
is	O
given	O
by	O
(	O
cid:3	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
y|w	O
,	O
σ2	O
)	O
=	O
(	O
2π	O
)	O
−dn/2|c|−n/2	O
exp	O
tr	O
(	O
c−1yt	O
y	O
)	O
−	O
1	O
2	O
(	O
15.118	O
)	O
where	O
c	O
=	O
wwt	O
+	O
σ2i	O
.	O
as	O
we	O
showed	O
in	O
theorem	O
12.2.2	O
,	O
the	O
mle	O
for	O
this	O
can	O
be	O
computed	O
in	O
terms	O
of	O
the	O
eigenvectors	O
of	O
yt	O
y	O
.	O
(	O
cid:5	O
)	O
now	O
we	O
consider	O
the	O
dual	O
problem	O
,	O
whereby	O
we	O
maximize	O
z	O
and	O
integrate	B
out	I
w.	O
we	O
will	O
j	O
n	O
(	O
wj|0	O
,	O
i	O
)	O
.	O
the	O
corresponding	O
likelihood	B
becomes	O
use	O
a	O
prior	O
of	O
the	O
form	O
p	O
(	O
w	O
)	O
=	O
p	O
(	O
y|z	O
,	O
σ2	O
)	O
=	O
d	O
(	O
cid:6	O
)	O
d=1	O
n	O
(	O
y	O
:	O
,d|0	O
,	O
zzt	O
+	O
σ2i	O
)	O
(	O
cid:3	O
)	O
−dn/2|kz|−d/2	O
exp	O
=	O
(	O
2π	O
)	O
(	O
cid:4	O
)	O
tr	O
(	O
k−1	O
z	O
yyt	O
)	O
−	O
1	O
2	O
(	O
15.119	O
)	O
(	O
15.120	O
)	O
where	O
kz	O
=	O
zzt	O
+	O
σ2i	O
.	O
based	O
on	O
our	O
discussion	O
of	O
the	O
connection	O
between	O
the	O
eigenvalues	O
of	O
yyt	O
and	O
of	O
yt	O
y	O
in	O
section	O
14.4.4	O
,	O
it	O
should	O
come	O
as	O
no	O
surprise	O
that	O
we	O
can	O
also	O
solve	O
the	O
dual	O
problem	O
using	O
eigenvalue	O
methods	O
(	O
see	O
(	O
lawrence	O
2005	O
)	O
for	O
the	O
details	O
)	O
.	O
if	O
we	O
use	O
a	O
linear	B
kernel	I
,	O
we	O
recover	O
pca	O
.	O
but	O
we	O
can	O
also	O
use	O
a	O
more	O
general	O
kernel	B
:	O
kz	O
=	O
k	O
+	O
σ2i	O
,	O
where	O
k	O
is	O
the	O
gram	O
matrix	O
for	O
z.	O
the	O
mle	O
for	O
ˆz	O
will	O
no	O
longer	O
be	O
available	O
15.5.	O
gp	O
latent	O
variable	O
model	O
541	O
0.3	O
.25	O
0.2	O
.15	O
0.1	O
.05	O
0	O
0.4	O
0.2	O
0	O
0.2	O
0.4	O
0.6	O
0	O
0.05	O
0.1	O
0.15	O
0.2	O
−0.8	O
−0.6	O
−0.4	O
−0.2	O
0	O
0.2	O
0.4	O
0.6	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
15.10	O
2d	O
representation	O
of	O
12	O
dimensional	O
oil	O
ﬂow	O
data	O
.	O
the	O
different	O
colors/symbols	O
represent	O
the	O
3	O
phases	O
of	O
oil	O
ﬂow	O
.	O
(	O
b	O
)	O
gp-lvm	O
with	O
gaussian	O
kernel	B
.	O
the	O
shading	O
represents	O
the	O
precision	B
of	O
the	O
posterior	O
,	O
where	O
lighter	O
pixels	O
have	O
higher	O
precision	B
.	O
from	O
figure	O
1	O
of	O
(	O
lawrence	O
2005	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
neil	O
lawrence	O
.	O
(	O
a	O
)	O
kernel	B
pca	O
with	O
gaussian	O
kernel	B
.	O
via	O
eigenvalue	O
methods	O
;	O
instead	O
we	O
must	O
use	O
gradient-based	O
optimization	B
.	O
the	O
objective	O
is	O
given	O
by	O
(	O
cid:2	O
)	O
=	O
−	O
d	O
2	O
log	O
|kz|	O
−	O
1	O
2	O
tr	O
(	O
k−1	O
z	O
yyt	O
)	O
and	O
the	O
gradient	O
is	O
given	O
by	O
∂	O
(	O
cid:2	O
)	O
∂zij	O
=	O
∂	O
(	O
cid:2	O
)	O
∂kz	O
∂kz	O
∂zij	O
where	O
∂	O
(	O
cid:2	O
)	O
∂kz	O
=	O
k−1	O
z	O
yyt	O
k−1	O
z	O
−	O
dk−1	O
z	O
(	O
15.121	O
)	O
(	O
15.122	O
)	O
(	O
15.123	O
)	O
the	O
form	O
of	O
∂kz	O
where	O
kz	O
=	O
zzt	O
+	O
σ2i	O
,	O
we	O
have	O
∂kz	O
optimizer	O
,	O
such	O
as	O
conjugate	O
gradient	O
descent	O
.	O
∂zij	O
will	O
of	O
course	O
depend	O
on	O
the	O
kernel	B
used	O
.	O
(	O
for	O
example	O
,	O
with	O
a	O
linear	B
kernel	I
,	O
∂z	O
=	O
z	O
.	O
)	O
we	O
can	O
then	O
pass	O
this	O
gradient	O
to	O
any	O
standard	O
let	O
us	O
now	O
compare	O
gp-lvm	O
to	O
kernel	B
pca	O
.	O
in	O
kpca	O
,	O
we	O
learn	O
a	O
kernelized	O
mapping	O
from	O
the	O
observed	O
space	O
to	O
the	O
latent	B
space	O
,	O
whereas	O
in	O
gp-lvm	O
,	O
we	O
learn	O
a	O
kernelized	O
mapping	O
from	O
the	O
latent	B
space	O
to	O
the	O
observed	O
space	O
.	O
figure	O
15.10	O
illustrates	O
the	O
results	O
of	O
applying	O
kpca	O
and	O
gp-lvm	O
to	O
visualize	O
the	O
12	O
dimensional	O
oil	O
ﬂow	O
data	O
shown	O
in	O
in	O
figure	O
14.9	O
(	O
a	O
)	O
.	O
we	O
see	O
that	O
the	O
embedding	B
produced	O
by	O
gp-lvm	O
is	O
far	O
better	O
.	O
if	O
we	O
perform	O
nearest	B
neighbor	I
classiﬁcation	O
in	O
the	O
latent	B
space	O
,	O
gp-lvm	O
makes	O
4	O
errors	O
,	O
while	O
kernel	B
pca	O
(	O
with	O
the	O
same	O
kernel	B
but	O
separately	O
optimized	O
hyper-parameters	B
)	O
makes	O
13	O
errors	O
,	O
and	O
regular	B
pca	O
makes	O
20	O
errors	O
.	O
gp-lvm	O
inherits	O
the	O
usual	O
advantages	O
of	O
probabilistic	O
generative	O
models	O
,	O
such	O
as	O
the	O
ability	O
to	O
handle	O
missing	B
data	I
and	O
data	O
of	O
different	O
types	O
,	O
the	O
ability	O
to	O
use	O
gradient-based	O
methods	O
(	O
instead	O
of	O
grid	O
search	O
)	O
to	O
tune	O
the	O
kernel	B
parameters	O
,	O
the	O
ability	O
to	O
handle	O
prior	O
information	B
,	O
542	O
chapter	O
15.	O
gaussian	O
processes	O
etc	O
.	O
for	O
a	O
discussion	O
of	O
some	O
other	O
probabilistic	O
methods	O
for	O
(	O
spectral	B
)	O
dimensionality	B
reduction	I
,	O
see	O
(	O
lawrence	O
2012	O
)	O
.	O
15.6	O
approximation	O
methods	O
for	O
large	O
datasets	O
the	O
principal	O
drawback	O
of	O
gps	O
is	O
that	O
they	O
take	O
o	O
(	O
n	O
3	O
)	O
time	O
to	O
use	O
.	O
this	O
is	O
because	O
of	O
the	O
need	O
to	O
invert	O
(	O
or	O
compute	O
the	O
cholesky	O
decomposition	O
of	O
)	O
the	O
n	O
×	O
n	O
kernel	B
matrix	O
k.	O
a	O
variety	O
of	O
approximation	O
methods	O
have	O
been	O
devised	O
which	O
take	O
o	O
(	O
m	O
2n	O
)	O
time	O
,	O
where	O
m	O
is	O
a	O
user-speciﬁable	O
parameter	B
.	O
for	O
details	O
,	O
see	O
(	O
quinonero-candela	O
et	O
al	O
.	O
2007	O
)	O
.	O
exercises	O
exercise	O
15.1	O
reproducing	B
property	I
prove	O
equation	O
15.109	O
.	O
16	O
adaptive	O
basis	O
function	O
models	O
16.1	O
introduction	O
in	O
chapters	O
14	O
and	O
15	O
,	O
we	O
discussed	O
kernel	B
methods	O
,	O
which	O
provide	O
a	O
powerful	O
way	O
to	O
create	O
non-	O
linear	O
models	O
for	O
regression	B
and	O
classiﬁcation	B
.	O
the	O
prediction	O
takes	O
the	O
form	O
f	O
(	O
x	O
)	O
=w	O
t	O
φ	O
(	O
x	O
)	O
,	O
where	O
we	O
deﬁne	O
φ	O
(	O
x	O
)	O
=	O
[	O
κ	O
(	O
x	O
,	O
μ1	O
)	O
,	O
.	O
.	O
.	O
,	O
κ	O
(	O
x	O
,	O
μn	O
)	O
]	O
(	O
16.1	O
)	O
and	O
where	O
μk	O
are	O
either	O
all	O
the	O
training	O
data	O
or	O
some	O
subset	O
.	O
models	O
of	O
this	O
form	O
essen-	O
tially	O
perform	O
a	O
form	O
of	O
template	B
matching	I
,	O
whereby	O
they	O
compare	O
the	O
input	O
x	O
to	O
the	O
stored	O
prototypes	O
μk	O
.	O
although	O
this	O
can	O
work	O
well	O
,	O
it	O
relies	O
on	O
having	O
a	O
good	O
kernel	B
function	I
to	O
measure	O
the	O
similarity	O
between	O
data	O
vectors	O
.	O
often	O
coming	O
up	O
with	O
a	O
good	O
kernel	B
function	I
is	O
quite	O
difficult	O
.	O
for	O
example	O
,	O
how	O
do	O
we	O
deﬁne	O
the	O
similarity	O
between	O
two	O
images	O
?	O
pixel-wise	O
comparison	O
of	O
intensities	O
(	O
which	O
is	O
what	O
a	O
gaussian	O
kernel	B
corresponds	O
to	O
)	O
does	O
not	O
work	O
well	O
.	O
although	O
it	O
is	O
possible	O
(	O
and	O
indeed	O
common	O
)	O
to	O
hand-engineer	O
kernels	O
for	O
speciﬁc	O
tasks	O
(	O
see	O
e.g.	O
,	O
the	O
pyramid	B
match	I
kernel	I
in	O
section	O
14.2.7	O
)	O
,	O
it	O
would	O
be	O
more	O
interesting	O
if	O
we	O
could	O
learn	O
the	O
kernel	B
.	O
in	O
section	O
15.2.4	O
,	O
we	O
discussed	O
a	O
way	O
to	O
learn	O
the	O
parameters	O
of	O
a	O
kernel	B
function	I
,	O
by	O
maxi-	O
mizing	O
the	O
marginal	B
likelihood	I
.	O
for	O
example	O
,	O
if	O
we	O
use	O
the	O
ard	O
kernel	B
,	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:2	O
)	O
)	O
=	O
θ0	O
exp	O
θj	O
(	O
xj	O
−	O
x	O
(	O
cid:2	O
)	O
j	O
)	O
2	O
(	O
16.2	O
)	O
⎛	O
⎝−	O
1	O
d	O
(	O
cid:4	O
)	O
2	O
j=1	O
⎞	O
⎠	O
we	O
can	O
can	O
estimate	O
the	O
θj	O
,	O
and	O
thus	O
perform	O
a	O
form	O
of	O
nonlinear	O
feature	B
selection	I
.	O
however	O
,	O
such	O
methods	O
can	O
be	O
computationally	O
expensive	O
.	O
another	O
approach	O
,	O
known	O
as	O
multiple	O
kernel	O
(	O
rakotomamonjy	O
et	O
al	O
.	O
2008	O
)	O
)	O
uses	O
a	O
convex	B
combination	I
of	O
base	O
kernels	O
,	O
learning	B
(	O
see	O
e.g.	O
,	O
κ	O
(	O
x	O
,	O
x	O
(	O
cid:2	O
)	O
)	O
,	O
and	O
then	O
estimates	O
the	O
mixing	B
weights	I
wj	O
.	O
but	O
this	O
relies	O
on	O
having	O
good	O
base	O
kernels	O
(	O
and	O
is	O
also	O
computationally	O
expensive	O
)	O
.	O
j	O
wjκj	O
(	O
x	O
,	O
x	O
(	O
cid:2	O
)	O
(	O
cid:7	O
)	O
)	O
=	O
an	O
alternative	O
approach	O
is	O
to	O
dispense	O
with	O
kernels	O
altogether	O
,	O
and	O
try	O
to	O
learn	O
useful	O
features	B
φ	O
(	O
x	O
)	O
directly	O
from	O
the	O
input	O
data	O
.	O
that	O
is	O
,	O
we	O
will	O
create	O
what	O
we	O
call	O
an	O
adaptive	O
basis-	O
function	O
model	O
(	O
abm	O
)	O
,	O
which	O
is	O
a	O
model	O
of	O
the	O
form	O
f	O
(	O
x	O
)	O
=	O
w0	O
+	O
wmφm	O
(	O
x	O
)	O
(	O
16.3	O
)	O
m	O
(	O
cid:4	O
)	O
m=1	O
544	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
where	O
φm	O
(	O
x	O
)	O
is	O
the	O
m	O
’	O
th	O
basis	O
function	O
,	O
which	O
is	O
learned	O
from	O
data	O
.	O
this	O
framework	O
covers	O
all	O
of	O
the	O
models	O
we	O
will	O
discuss	O
in	O
this	O
chapter	O
.	O
typically	O
the	O
basis	B
functions	I
are	O
parametric	O
,	O
so	O
we	O
can	O
write	O
φm	O
(	O
x	O
)	O
=φ	O
(	O
x	O
;	O
vm	O
)	O
,	O
where	O
vm	O
are	O
the	O
parameters	O
of	O
the	O
basis	O
function	O
itself	O
.	O
we	O
will	O
use	O
θ	O
=	O
(	O
w0	O
,	O
w1	O
:	O
m	O
,	O
{	O
vm	O
}	O
m	O
m=1	O
)	O
to	O
denote	O
the	O
entire	O
parameter	B
set	O
.	O
the	O
resulting	O
model	O
is	O
not	O
linear-in-the-parameters	O
anymore	O
,	O
so	O
we	O
will	O
only	O
be	O
able	O
to	O
compute	O
a	O
locally	O
optimal	O
mle	O
or	O
map	O
estimate	O
of	O
θ.	O
nevertheless	O
,	O
such	O
models	O
often	O
signiﬁcantly	O
outperform	O
linear	O
models	O
,	O
as	O
we	O
will	O
see	O
.	O
16.2	O
classiﬁcation	B
and	O
regression	B
trees	O
(	O
cart	O
)	O
classiﬁcation	B
and	O
regression	B
trees	O
or	O
cart	O
models	O
,	O
also	O
called	O
decision	B
trees	I
(	O
not	O
to	O
be	O
confused	O
with	O
the	O
decision	B
trees	I
used	O
in	O
decision	B
theory	O
)	O
are	O
deﬁned	O
by	O
recursively	O
partitioning	B
the	O
input	O
space	O
,	O
and	O
deﬁning	O
a	O
local	O
model	O
in	O
each	O
resulting	O
region	O
of	O
input	O
space	O
.	O
this	O
can	O
be	O
represented	O
by	O
a	O
tree	B
,	O
with	O
one	O
leaf	B
per	O
region	O
,	O
as	O
we	O
explain	O
below	O
.	O
16.2.1	O
basics	O
to	O
explain	O
the	O
cart	O
approach	O
,	O
consider	O
the	O
tree	B
in	O
figure	O
16.1	O
(	O
a	O
)	O
.	O
the	O
ﬁrst	O
node	O
asks	O
if	O
x1	O
is	O
less	O
than	O
some	O
threshold	O
t1	O
.	O
if	O
yes	O
,	O
we	O
then	O
ask	O
if	O
x2	O
is	O
less	O
than	O
some	O
other	O
threshold	O
t2	O
.	O
if	O
yes	O
,	O
we	O
are	O
in	O
the	O
bottom	O
left	O
quadrant	O
of	O
space	O
,	O
r1	O
.	O
if	O
no	O
,	O
we	O
ask	O
if	O
x1	O
is	O
less	O
than	O
t3	O
.	O
and	O
so	O
on	O
.	O
the	O
result	O
of	O
these	O
axis	B
parallel	I
splits	I
is	O
to	O
partition	O
2d	O
space	O
into	O
5	O
regions	O
,	O
as	O
shown	O
in	O
figure	O
16.1	O
(	O
b	O
)	O
.	O
we	O
can	O
now	O
associate	O
a	O
mean	B
response	O
with	O
each	O
of	O
these	O
regions	O
,	O
resulting	O
in	O
the	O
piecewise	O
constant	O
surface	O
shown	O
in	O
figure	O
16.1	O
(	O
c	O
)	O
.	O
we	O
can	O
write	O
the	O
model	O
in	O
the	O
following	O
form	O
m	O
(	O
cid:4	O
)	O
m	O
(	O
cid:4	O
)	O
f	O
(	O
x	O
)	O
=	O
e	O
[	O
y|x	O
]	O
=	O
wmi	O
(	O
x	O
∈	O
rm	O
)	O
=	O
wmφ	O
(	O
x	O
;	O
vm	O
)	O
(	O
16.4	O
)	O
m=1	O
m=1	O
where	O
rm	O
is	O
the	O
m	O
’	O
th	O
region	O
,	O
wm	O
is	O
the	O
mean	B
response	O
in	O
this	O
region	O
,	O
and	O
vm	O
encodes	O
the	O
choice	O
of	O
variable	O
to	O
split	O
on	O
,	O
and	O
the	O
threshold	O
value	O
,	O
on	O
the	O
path	B
from	O
the	O
root	B
to	O
the	O
m	O
’	O
th	O
leaf	B
.	O
this	O
makes	O
it	O
clear	O
that	O
a	O
cart	O
model	O
is	O
just	O
a	O
an	O
adaptive	B
basis-function	I
model	I
,	O
where	O
the	O
basis	B
functions	I
deﬁne	O
the	O
regions	O
,	O
and	O
the	O
weights	O
specify	O
the	O
response	O
value	O
in	O
each	O
region	O
.	O
we	O
discuss	O
how	O
to	O
ﬁnd	O
these	O
basis	B
functions	I
below	O
.	O
we	O
can	O
generalize	B
this	O
to	O
the	O
classiﬁcation	B
setting	O
by	O
storing	O
the	O
distribution	O
over	O
class	O
labels	O
in	O
each	O
leaf	B
,	O
instead	O
of	O
the	O
mean	B
response	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
16.2.	O
this	O
model	O
can	O
be	O
used	O
to	O
classify	O
the	O
data	O
in	O
figure	O
1.1.	O
for	O
example	O
,	O
we	O
ﬁrst	O
check	O
the	O
color	O
of	O
the	O
object	O
.	O
if	O
it	O
is	O
blue	O
,	O
we	O
follow	O
the	O
left	O
branch	O
and	O
end	O
up	O
in	O
a	O
leaf	B
labeled	O
“	O
4,0	O
”	O
,	O
which	O
means	O
we	O
have	O
4	O
positive	B
examples	I
and	O
0	O
negative	B
examples	I
which	O
match	O
this	O
criterion	O
.	O
hence	O
we	O
predict	O
p	O
(	O
y	O
=	O
1|x	O
)	O
=	O
4/4	O
if	O
x	O
is	O
blue	O
.	O
if	O
it	O
is	O
an	O
ellipse	O
,	O
we	O
end	O
up	O
in	O
a	O
leaf	B
labeled	O
“	O
1,1	O
”	O
,	O
so	O
we	O
predict	O
p	O
(	O
y	O
=	O
1|x	O
)	O
=	O
1/2	O
.	O
if	O
it	O
is	O
red	O
but	O
not	O
an	O
ellipse	O
,	O
we	O
predict	O
p	O
(	O
y	O
=	O
1|x	O
)	O
=	O
0/2	O
;	O
if	O
it	O
is	O
some	O
other	O
colour	O
,	O
we	O
check	O
the	O
size	O
:	O
if	O
less	O
than	O
10	O
,	O
we	O
predict	O
p	O
(	O
y	O
=	O
1|x	O
)	O
=	O
4/4	O
,	O
otherwise	O
p	O
(	O
y	O
=	O
1|x	O
)	O
=	O
0/5	O
.	O
these	O
probabilities	O
are	O
just	O
the	O
empirical	O
fraction	O
of	O
positive	B
examples	I
that	O
satisfy	O
each	O
conjunction	O
of	O
feature	O
values	O
,	O
which	O
deﬁnes	O
a	O
path	B
from	O
the	O
root	B
to	O
a	O
leaf	B
.	O
if	O
it	O
is	O
red	O
,	O
we	O
then	O
check	O
the	O
shape	O
:	O
16.2.	O
classiﬁcation	B
and	O
regression	B
trees	O
(	O
cart	O
)	O
545	O
x1	O
≤	O
t1	O
x2	O
≤	O
t2	O
x2	O
≤	O
t4	O
r1	O
x1	O
≤	O
t3	O
r2	O
r3	O
10	O
9	O
8	O
7	O
6	O
5	O
4	O
3	O
2	O
10	O
r4	O
r5	O
(	O
a	O
)	O
8	O
6	O
4	O
2	O
4	O
2	O
0	O
0	O
(	O
b	O
)	O
10	O
8	O
6	O
figure	O
16.1	O
a	O
simple	O
regression	O
tree	B
on	O
two	O
inputs	O
.	O
based	O
on	O
figure	O
9.2	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
regtreesurfacedemo	O
.	O
blue	O
color	O
red	O
other	O
4,0	O
shape	O
ellipse	O
other	O
size	O
<	O
10	O
yes	O
no	O
1,1	O
0,2	O
4,0	O
0,5	O
figure	O
16.2	O
a	O
simple	O
decision	O
tree	B
for	O
the	O
data	O
in	O
figure	O
1.1.	O
a	O
leaf	B
labeled	O
as	O
(	O
n1	O
,	O
n0	O
)	O
means	O
that	O
there	O
are	O
n1	O
positive	B
examples	I
that	O
match	O
this	O
path	B
,	O
and	O
n0	O
negative	B
examples	I
.	O
in	O
this	O
tree	B
,	O
most	O
of	O
the	O
leaves	B
are	O
“	O
pure	B
”	O
,	O
meaning	O
they	O
only	O
have	O
examples	O
of	O
one	O
class	O
or	O
the	O
other	O
;	O
the	O
only	O
exception	O
is	O
leaf	B
representing	O
red	O
ellipses	O
,	O
which	O
has	O
a	O
label	B
distribution	O
of	O
(	O
1	O
,	O
1	O
)	O
.	O
we	O
could	O
distinguish	O
positive	O
from	O
negative	O
red	O
ellipses	O
by	O
adding	O
a	O
further	O
test	O
based	O
on	O
size	O
.	O
however	O
,	O
it	O
is	O
not	O
always	O
desirable	O
to	O
construct	O
trees	O
that	O
perfectly	O
model	O
the	O
training	O
data	O
,	O
due	O
to	O
overﬁtting	B
.	O
16.2.2	O
growing	O
a	O
tree	B
finding	O
the	O
optimal	O
partitioning	O
of	O
the	O
data	O
is	O
np-complete	O
(	O
hyaﬁl	O
and	O
rivest	O
1976	O
)	O
,	O
so	O
it	O
is	O
common	O
to	O
use	O
the	O
greedy	O
procedure	O
shown	O
in	O
algorithm	O
6	O
to	O
compute	O
a	O
locally	O
optimal	O
mle	O
.	O
this	O
method	O
is	O
used	O
by	O
cart	O
,	O
(	O
breiman	O
et	O
al	O
.	O
1984	O
)	O
c4.5	O
(	O
quinlan	O
1993	O
)	O
,	O
and	O
id3	O
(	O
quinlan	O
1986	O
)	O
,	O
(	O
see	O
dtfit	O
for	O
a	O
simple	O
matlab	O
which	O
are	O
three	O
popular	O
implementations	O
of	O
the	O
method	O
.	O
implementation	O
.	O
)	O
the	O
split	O
function	O
chooses	O
the	O
best	O
feature	O
,	O
and	O
the	O
best	O
value	O
for	O
that	O
feature	O
,	O
as	O
follows	O
:	O
(	O
j∗	O
,	O
t∗	O
cost	O
(	O
{	O
xi	O
,	O
yi	O
:	O
xij	O
≤	O
t	O
}	O
)	O
+cost	O
(	O
{	O
xi	O
,	O
yi	O
:	O
xij	O
>	O
t	O
}	O
)	O
(	O
16.5	O
)	O
)	O
=	O
arg	O
min	O
j∈	O
{	O
1	O
,	O
...	O
,	O
d	O
}	O
min	O
t∈tj	O
546	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
algorithm	O
16.1	O
:	O
recursive	B
procedure	O
to	O
grow	O
a	O
classiﬁcation/	O
regression	B
tree	O
1	O
function	O
ﬁttree	O
(	O
node	O
,	O
d	O
,	O
depth	O
)	O
;	O
2	O
node.prediction	O
=	O
mean	B
(	O
yi	O
:	O
i	O
∈	O
d	O
)	O
//	O
or	O
class	O
label	O
distribution	O
;	O
3	O
(	O
j∗	O
,	O
t∗	O
,	O
dl	O
,	O
dr	O
)	O
=	O
split	O
(	O
d	O
)	O
;	O
4	O
if	O
not	O
worthsplitting	O
(	O
depth	O
,	O
cost	O
,	O
dl	O
,	O
dr	O
)	O
then	O
5	O
return	O
node	O
6	O
else	O
7	O
8	O
9	O
10	O
node.test	O
=	O
λx.xj∗	O
<	O
t∗	O
node.left	O
=	O
ﬁttree	O
(	O
node	O
,	O
dl	O
,	O
depth+1	O
)	O
;	O
node.right	O
=	O
ﬁttree	O
(	O
node	O
,	O
dr	O
,	O
depth+1	O
)	O
;	O
return	O
node	O
;	O
//	O
anonymous	O
function	O
;	O
where	O
the	O
cost	O
function	O
for	O
a	O
given	O
dataset	O
will	O
be	O
deﬁned	O
below	O
.	O
for	O
notational	O
simplicity	O
,	O
we	O
have	O
assumed	O
all	O
inputs	O
are	O
real-valued	O
or	O
ordinal	B
,	O
so	O
it	O
makes	O
sense	O
to	O
compare	O
a	O
feature	O
xij	O
to	O
a	O
numeric	O
value	O
t.	O
the	O
set	O
of	O
possible	O
thresholds	O
tj	O
for	O
feature	O
j	O
can	O
be	O
obtained	O
by	O
sorting	O
the	O
unique	O
values	O
of	O
xij	O
.	O
for	O
example	O
,	O
if	O
feature	O
1	O
has	O
the	O
values	O
{	O
4.5	O
,	O
−12	O
,	O
72	O
,	O
−12	O
}	O
,	O
then	O
we	O
set	O
t1	O
=	O
{	O
−12	O
,	O
4.5	O
,	O
72	O
}	O
.	O
in	O
the	O
case	O
of	O
categorical	B
inputs	O
,	O
the	O
most	O
common	O
approach	O
is	O
to	O
consider	O
splits	O
of	O
the	O
form	O
xij	O
=	O
ck	O
and	O
xij	O
(	O
cid:4	O
)	O
=	O
ck	O
,	O
for	O
each	O
possible	O
class	O
label	O
ck	O
.	O
although	O
we	O
could	O
allow	O
for	O
multi-way	O
splits	O
(	O
resulting	O
in	O
non-binary	O
trees	O
)	O
,	O
this	O
would	O
result	O
in	O
data	B
fragmentation	I
,	O
meaning	O
too	O
little	O
data	O
might	O
“	O
fall	O
”	O
into	O
each	O
subtree	O
,	O
resulting	O
in	O
overﬁtting	B
.	O
the	O
function	O
that	O
checks	O
if	O
a	O
node	O
is	O
worth	O
splitting	O
can	O
use	O
several	O
stopping	O
heuristics	O
,	O
such	O
as	O
the	O
following	O
:	O
•	O
is	O
the	O
reduction	O
in	O
cost	O
too	O
small	O
?	O
typically	O
we	O
deﬁne	O
the	O
gain	O
of	O
using	O
a	O
feature	O
to	O
be	O
a	O
normalized	O
measure	O
of	O
the	O
reduction	O
in	O
cost	O
:	O
|dr|	O
|d|	O
cost	O
(	O
dr	O
)	O
(	O
cid:8	O
)	O
|dl|	O
|d|	O
cost	O
(	O
dl	O
)	O
+	O
δ	O
(	O
cid:2	O
)	O
cost	O
(	O
d	O
)	O
−	O
(	O
cid:9	O
)	O
(	O
16.6	O
)	O
•	O
•	O
has	O
the	O
tree	B
exceeded	O
the	O
maximum	O
desired	O
depth	O
?	O
•	O
is	O
the	O
distribution	O
of	O
the	O
response	O
in	O
either	O
dl	O
or	O
dr	O
sufficiently	O
homogeneous	B
(	O
e.g.	O
,	O
all	O
labels	O
are	O
the	O
same	O
,	O
so	O
the	O
distribution	O
is	O
pure	B
)	O
?	O
is	O
the	O
number	O
of	O
examples	O
in	O
either	O
dl	O
or	O
dr	O
too	O
small	O
?	O
all	O
that	O
remains	O
is	O
to	O
specify	O
the	O
cost	O
measure	O
used	O
to	O
evaluate	O
the	O
quality	O
of	O
a	O
proposed	O
split	O
.	O
this	O
depends	O
on	O
whether	O
our	O
goal	O
is	O
regression	B
or	O
classiﬁcation	B
.	O
we	O
discuss	O
both	O
cases	O
below	O
.	O
16.2.2.1	O
regression	B
cost	O
(	O
cid:4	O
)	O
i∈d	O
in	O
the	O
regression	B
setting	O
,	O
we	O
deﬁne	O
the	O
cost	O
as	O
follows	O
:	O
cost	O
(	O
d	O
)	O
=	O
(	O
yi	O
−	O
y	O
)	O
2	O
(	O
16.7	O
)	O
(	O
cid:7	O
)	O
16.2.	O
classiﬁcation	B
and	O
regression	B
trees	O
(	O
cart	O
)	O
547	O
where	O
y	O
=	O
1|d|	O
i∈d	O
yi	O
is	O
the	O
mean	B
of	O
the	O
response	B
variable	I
in	O
the	O
speciﬁed	O
set	O
of	O
data	O
.	O
alternatively	O
,	O
we	O
can	O
ﬁt	O
a	O
linear	B
regression	I
model	O
for	O
each	O
leaf	B
,	O
using	O
as	O
inputs	O
the	O
features	B
that	O
were	O
chosen	O
on	O
the	O
path	B
from	O
the	O
root	B
,	O
and	O
then	O
measure	O
the	O
residual	B
error	I
.	O
16.2.2.2	O
classiﬁcation	B
cost	O
(	O
cid:4	O
)	O
i∈d	O
ˆπc	O
=	O
1|d|	O
in	O
the	O
classiﬁcation	B
setting	O
,	O
there	O
are	O
several	O
ways	O
to	O
measure	O
the	O
quality	O
of	O
a	O
split	O
.	O
first	O
,	O
we	O
ﬁt	O
a	O
multinoulli	O
model	O
to	O
the	O
data	O
in	O
the	O
leaf	B
satisfying	O
the	O
test	O
xj	O
<	O
t	O
by	O
estimating	O
the	O
class-conditional	O
probabilities	O
as	O
follows	O
:	O
i	O
(	O
yi	O
=	O
c	O
)	O
(	O
16.8	O
)	O
where	O
d	O
is	O
the	O
data	O
in	O
the	O
leaf	B
.	O
given	O
this	O
,	O
there	O
are	O
several	O
common	O
error	O
measures	O
for	O
evaluating	O
a	O
proposed	O
partition	O
:	O
•	O
misclassiﬁcation	B
rate	I
.	O
we	O
deﬁne	O
the	O
most	O
probable	O
class	O
label	O
as	O
ˆyc	O
=	O
argmaxc	O
ˆπc	O
.	O
the	O
corresponding	O
error	O
rate	O
is	O
then	O
i	O
(	O
yi	O
(	O
cid:4	O
)	O
=	O
ˆy	O
)	O
=	O
1	O
−	O
ˆπˆy	O
1|d|	O
(	O
cid:4	O
)	O
i∈d	O
•	O
entropy	B
,	O
ordeviance	O
:	O
h	O
(	O
ˆπ	O
)	O
=	O
−	O
c	O
(	O
cid:4	O
)	O
c=1	O
ˆπc	O
log	O
ˆπc	O
(	O
16.9	O
)	O
(	O
16.10	O
)	O
(	O
16.11	O
)	O
(	O
16.12	O
)	O
(	O
16.13	O
)	O
note	O
that	O
minimizing	O
the	O
entropy	B
is	O
equivalent	O
to	O
maximizing	O
the	O
information	B
gain	I
(	O
quinlan	O
1986	O
)	O
between	O
test	O
xj	O
<	O
t	O
and	O
the	O
class	O
label	O
y	O
,	O
deﬁned	O
by	O
infogain	O
(	O
xj	O
<	O
t	O
,	O
y	O
)	O
(	O
cid:2	O
)	O
h	O
(	O
y	O
)	O
−	O
h	O
(	O
y	O
|xj	O
<	O
t	O
)	O
(	O
cid:11	O
)	O
=	O
(	O
cid:10	O
)	O
(	O
cid:4	O
)	O
(	O
cid:10	O
)	O
(	O
cid:4	O
)	O
−	O
c	O
+	O
c	O
p	O
(	O
y	O
=	O
c	O
)	O
log	O
p	O
(	O
y	O
=	O
c	O
)	O
p	O
(	O
y	O
=	O
c|xj	O
<	O
t	O
)	O
log	O
p	O
(	O
c|xj	O
<	O
t	O
)	O
(	O
cid:11	O
)	O
since	O
ˆπc	O
is	O
an	O
mle	O
for	O
the	O
distribution	O
p	O
(	O
c|xj	O
<	O
t	O
)	O
.1	O
1.	O
if	O
xj	O
is	O
categorical	B
,	O
and	O
we	O
use	O
tests	O
of	O
the	O
form	O
xj	O
=	O
k	O
,	O
then	O
taking	O
expectations	O
over	O
values	O
of	O
xj	O
gives	O
k	O
p	O
(	O
xj	O
=	O
k	O
)	O
infogain	O
(	O
xj	O
=	O
k	O
,	O
y	O
)	O
=	O
the	O
mutual	O
h	O
(	O
y	O
)	O
−	O
h	O
(	O
y	O
|xj	O
)	O
=	O
i	O
(	O
y	O
;	O
xj	O
)	O
.	O
information	B
between	O
xj	O
and	O
y	O
:	O
e	O
[	O
infogain	O
(	O
xj	O
,	O
y	O
)	O
]	O
=	O
(	O
cid:2	O
)	O
548	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
0.5	O
0.45	O
0.4	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
0	O
error	O
rate	O
gini	O
entropy	B
0.2	O
0.4	O
0.6	O
0.8	O
1	O
figure	O
16.3	O
node	O
impurity	O
measures	O
for	O
binary	B
classiﬁcation	I
.	O
the	O
horizontal	O
axis	O
corresponds	O
to	O
p	O
,	O
the	O
probability	O
of	O
class	O
1.	O
the	O
entropy	B
measure	O
has	O
been	O
rescaled	O
to	O
pass	O
through	O
(	O
0.5,0.5	O
)	O
.	O
based	O
on	O
figure	O
9.3	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
ginidemo	O
.	O
•	O
gini	O
index	O
c	O
(	O
cid:4	O
)	O
ˆπc	O
(	O
1	O
−	O
ˆπc	O
)	O
=	O
(	O
cid:4	O
)	O
ˆπc	O
−	O
(	O
cid:4	O
)	O
c	O
=	O
1	O
−	O
ˆπ2	O
c=1	O
c	O
c	O
(	O
cid:4	O
)	O
c	O
ˆπ2	O
c	O
(	O
16.14	O
)	O
this	O
is	O
the	O
expected	O
error	O
rate	B
.	O
to	O
see	O
this	O
,	O
note	O
that	O
ˆπc	O
is	O
the	O
probability	O
a	O
random	O
entry	O
in	O
the	O
leaf	B
belongs	O
to	O
class	O
c	O
,	O
and	O
(	O
1	O
−	O
ˆπc	O
is	O
the	O
probability	O
it	O
would	O
be	O
misclassiﬁed	O
.	O
in	O
the	O
two-class	O
case	O
,	O
where	O
p	O
=	O
πm	O
(	O
1	O
)	O
,	O
the	O
misclassiﬁcation	B
rate	I
is	O
1	O
−	O
max	O
(	O
p	O
,	O
1	O
−	O
p	O
)	O
,	O
the	O
entropy	B
is	O
h2	O
(	O
p	O
)	O
,	O
and	O
the	O
gini	O
index	O
is	O
2p	O
(	O
1	O
−	O
p	O
)	O
.	O
these	O
are	O
plotted	O
in	O
figure	O
16.3.	O
we	O
see	O
that	O
the	O
cross-entropy	B
and	O
gini	O
measures	O
are	O
very	O
similar	B
,	O
and	O
are	O
more	O
sensitive	O
to	O
changes	O
in	O
class	O
probability	O
than	O
is	O
the	O
misclassiﬁcation	B
rate	I
.	O
for	O
example	O
,	O
consider	O
a	O
two-class	O
problem	O
with	O
400	O
cases	O
in	O
each	O
class	O
.	O
suppose	O
one	O
split	O
created	O
the	O
nodes	B
(	O
300,100	O
)	O
and	O
(	O
100,300	O
)	O
,	O
while	O
the	O
other	O
created	O
the	O
nodes	B
(	O
200,400	O
)	O
and	O
(	O
200,0	O
)	O
.	O
both	O
splits	O
produce	O
a	O
misclassiﬁcation	B
rate	I
of	O
0.25.	O
however	O
,	O
the	O
latter	O
seems	O
preferable	O
,	O
since	O
one	O
of	O
the	O
nodes	B
is	O
pure	B
,	O
i.e.	O
,	O
it	O
only	O
contains	O
one	O
class	O
.	O
the	O
cross-entropy	B
and	O
gini	O
measures	O
will	O
favor	O
this	O
latter	O
choice	O
.	O
16.2.2.3	O
example	O
as	O
an	O
example	O
,	O
consider	O
two	O
of	O
the	O
four	O
features	B
from	O
the	O
3-class	O
iris	B
dataset	O
,	O
shown	O
in	O
fig-	O
ure	O
16.4	O
(	O
a	O
)	O
.	O
the	O
resulting	O
tree	B
is	O
shown	O
in	O
figure	O
16.5	O
(	O
a	O
)	O
,	O
and	O
the	O
decision	B
boundaries	O
are	O
shown	O
in	O
figure	O
16.4	O
(	O
b	O
)	O
.	O
we	O
see	O
that	O
the	O
tree	B
is	O
quite	O
complex	O
,	O
as	O
are	O
the	O
resulting	O
decision	B
boundaries	O
.	O
in	O
figure	O
16.5	O
(	O
b	O
)	O
,	O
we	O
show	O
that	O
the	O
cv	O
estimate	O
of	O
the	O
error	O
is	O
much	O
higher	O
than	O
the	O
training	B
set	I
error	O
,	O
indicating	O
overﬁtting	B
.	O
below	O
we	O
discuss	O
how	O
to	O
perform	O
a	O
tree-pruning	O
stage	O
to	O
simplify	O
the	O
tree	B
.	O
4.5	O
4	O
3.5	O
t	O
h	O
d	O
w	O
i	O
l	O
a	O
p	O
e	O
s	O
3	O
2.5	O
2	O
4	O
setosa	O
versicolor	O
virginica	O
4.5	O
4	O
3.5	O
3	O
2.5	O
y	O
16.2.	O
classiﬁcation	B
and	O
regression	B
trees	O
(	O
cart	O
)	O
unpruned	O
decision	B
tree	O
549	O
versicolor	O
setosa	O
virginica	O
4.5	O
5	O
5.5	O
6	O
6.5	O
7	O
7.5	O
8	O
sepal	O
length	O
(	O
a	O
)	O
2	O
4	O
4.5	O
5	O
5.5	O
6.5	O
7	O
7.5	O
8	O
6	O
x	O
(	O
b	O
)	O
figure	O
16.4	O
petal	O
length	O
and	O
petal	O
width	O
.	O
(	O
b	O
)	O
decision	B
boundaries	O
induced	O
by	O
the	O
decision	B
tree	O
in	O
figure	O
16.5	O
(	O
a	O
)	O
.	O
(	O
a	O
)	O
iris	B
data	O
.	O
we	O
only	O
show	O
the	O
ﬁrst	O
two	O
features	B
,	O
sepal	O
length	O
and	O
sepal	O
width	O
,	O
and	O
ignore	O
sl	O
<	O
5.45	O
sl	O
>	O
=	O
5.45	O
w	O
<	O
2.8	O
sw	O
>	O
=	O
2.8	O
sl	O
<	O
6.15	O
sl	O
>	O
=	O
6.15	O
versicolorsetosa	O
sw	O
<	O
3.45	O
sw	O
>	O
=	O
3.45	O
sl	O
<	O
7.05	O
sl	O
>	O
=	O
7.05	O
sl	O
<	O
5.75	O
sl	O
>	O
=	O
5.75	O
setosa	O
sw	O
<	O
2.4	O
sw	O
>	O
=	O
2.4	O
virginica	O
sw	O
<	O
3.1	O
sw	O
>	O
=	O
3.1	O
versicolor	O
versicolor	O
sw	O
<	O
2.95	O
sw	O
>	O
=	O
2.95	O
versicolor	O
sl	O
<	O
6.95	O
sl	O
>	O
=	O
6.95	O
sw	O
<	O
3.15	O
sw	O
>	O
=	O
3.15	O
versicolor	O
versicolorvirginica	O
sl	O
<	O
6.55	O
sl	O
>	O
=	O
6.55	O
virginica	O
sw	O
<	O
2.95	O
sw	O
>	O
=	O
2.95	O
sl	O
<	O
6.65	O
sl	O
>	O
=	O
6.65	O
sl	O
<	O
6.45	O
sl	O
>	O
=	O
6.45	O
virginicaversicolor	O
sw	O
<	O
2.65	O
sw	O
>	O
=	O
2.65	O
sw	O
<	O
2.85	O
sw	O
>	O
=	O
2.85	O
versicolor	O
sw	O
<	O
2.9	O
virginica	O
sw	O
>	O
=	O
2.9	O
virginicaversicolor	O
versicolorvirginica	O
(	O
a	O
)	O
cross−validation	O
training	B
set	I
min	O
+	O
1	O
std	O
.	O
err	O
.	O
best	O
choice	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
)	O
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
c	O
i	O
f	O
i	O
s	O
s	O
a	O
c	O
s	O
m	O
i	O
l	O
(	O
t	O
s	O
o	O
c	O
0.3	O
0.2	O
0.1	O
0	O
5	O
10	O
15	O
20	O
number	O
of	O
terminal	O
nodes	B
(	O
b	O
)	O
figure	O
16.5	O
tree	B
.	O
figure	O
generated	O
by	O
dtreedemoiris	O
.	O
(	O
a	O
)	O
unpruned	O
decision	B
tree	O
for	O
iris	B
data	O
.	O
(	O
b	O
)	O
plot	O
of	O
misclassiﬁcation	O
error	O
rate	B
vs	O
depth	O
of	O
16.2.3	O
pruning	B
a	O
tree	B
to	O
prevent	O
overﬁtting	B
,	O
we	O
can	O
stop	O
growing	O
the	O
tree	B
if	O
the	O
decrease	O
in	O
the	O
error	O
is	O
not	O
sufficient	O
to	O
justify	O
the	O
extra	O
complexity	O
of	O
adding	O
an	O
extra	O
subtree	O
.	O
however	O
,	O
this	O
tends	O
to	O
be	O
too	O
myopic	O
.	O
for	O
example	O
,	O
on	O
the	O
xor	B
data	O
in	O
figure	O
14.2	O
(	O
c	O
)	O
,	O
it	O
would	O
might	O
never	O
make	O
any	O
splits	O
,	O
since	O
each	O
feature	O
on	O
its	O
own	O
has	O
little	O
predictive	B
power	O
.	O
the	O
standard	O
approach	O
is	O
therefore	O
to	O
grow	O
a	O
“	O
full	B
”	O
tree	B
,	O
and	O
then	O
to	O
perform	O
pruning	B
.	O
this	O
can	O
be	O
done	O
using	O
a	O
scheme	O
that	O
prunes	O
the	O
branches	O
giving	O
the	O
least	O
increase	O
in	O
the	O
error	O
.	O
see	O
(	O
breiman	O
et	O
al	O
.	O
1984	O
)	O
for	O
details	O
.	O
to	O
determine	O
how	O
far	O
to	O
prune	O
back	O
,	O
we	O
can	O
evaluate	O
the	O
cross-validated	O
error	O
on	O
each	O
such	O
subtree	O
,	O
and	O
then	O
pick	O
the	O
tree	B
whose	O
cv	O
error	O
is	O
within	O
1	O
standard	O
error	O
of	O
the	O
minimum	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
16.4	O
(	O
b	O
)	O
.	O
the	O
point	O
with	O
the	O
minimum	O
cv	O
error	O
corresponds	O
to	O
the	O
simple	O
tree	O
in	O
figure	O
16.6	O
(	O
a	O
)	O
.	O
550	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
sl	O
<	O
5.45	O
sl	O
>	O
=	O
5.45	O
sw	O
<	O
2.8	O
sw	O
>	O
=	O
2.8	O
sl	O
<	O
6.15	O
sl	O
>	O
=	O
6.15	O
versicolor	O
setosa	O
sw	O
<	O
3.45	O
sw	O
>	O
=	O
3.45	O
virginica	O
4.5	O
4	O
3.5	O
3	O
2.5	O
y	O
pruned	O
decision	B
tree	O
versicolor	O
setosa	O
virginica	O
versicolor	O
setosa	O
(	O
a	O
)	O
2	O
4	O
4.5	O
5	O
5.5	O
6.5	O
7	O
7.5	O
8	O
6	O
x	O
(	O
b	O
)	O
figure	O
16.6	O
pruned	O
decision	B
tree	O
for	O
iris	B
data	O
.	O
figure	O
generated	O
by	O
dtreedemoiris	O
.	O
16.2.4	O
pros	O
and	O
cons	O
of	O
trees	O
cart	O
models	O
are	O
popular	O
for	O
several	O
reasons	O
:	O
they	O
are	O
easy	O
to	O
interpret2	O
,	O
they	O
can	O
easily	O
handle	O
mixed	O
discrete	O
and	O
continuous	O
inputs	O
,	O
they	O
are	O
insensitive	O
to	O
monotone	O
transformations	O
of	O
the	O
inputs	O
(	O
because	O
the	O
split	O
points	O
are	O
based	O
on	O
ranking	B
the	O
data	O
points	O
)	O
,	O
they	O
perform	O
automatic	O
variable	O
selection	O
,	O
they	O
are	O
relatively	O
robust	B
to	O
outliers	B
,	O
they	O
scale	O
well	O
to	O
large	O
data	O
sets	O
,	O
and	O
they	O
can	O
be	O
modiﬁed	O
to	O
handle	O
missing	B
inputs.3	O
however	O
,	O
cart	O
models	O
also	O
have	O
some	O
disadvantages	O
.	O
the	O
primary	O
one	O
is	O
that	O
they	O
do	O
not	O
predict	O
very	O
accurately	O
compared	O
to	O
other	O
kinds	O
of	O
model	O
.	O
this	O
is	O
in	O
part	O
due	O
to	O
the	O
greedy	O
nature	O
of	O
the	O
tree	B
construction	O
algorithm	O
.	O
a	O
related	O
problem	O
is	O
that	O
trees	O
are	O
unstable	B
:	O
small	O
changes	O
to	O
the	O
input	O
data	O
can	O
have	O
large	O
effects	O
on	O
the	O
structure	O
of	O
the	O
tree	B
,	O
due	O
to	O
the	O
hierarchical	O
nature	O
of	O
the	O
tree-growing	O
process	O
,	O
causing	O
errors	O
at	O
the	O
top	O
to	O
affect	O
the	O
rest	O
of	O
the	O
tree	B
.	O
in	O
frequentist	B
terminology	O
,	O
we	O
say	O
that	O
trees	O
are	O
high	B
variance	I
estimators	I
.	O
we	O
discuss	O
a	O
solution	O
to	O
this	O
below	O
.	O
16.2.5	O
random	B
forests	I
one	O
way	O
to	O
reduce	O
the	O
variance	B
of	O
an	O
estimate	O
is	O
to	O
average	O
together	O
many	O
estimates	O
.	O
for	O
example	O
,	O
we	O
can	O
train	O
m	O
different	O
trees	O
on	O
different	O
subsets	O
of	O
the	O
data	O
,	O
chosen	O
randomly	O
with	O
2.	O
we	O
can	O
postprocess	O
the	O
tree	B
to	O
derive	O
a	O
series	O
of	O
logical	O
rules	O
such	O
as	O
“	O
if	O
x1	O
<	O
5.45	O
then	O
...	O
”	O
(	O
quinlan	O
1990	O
)	O
.	O
3.	O
the	O
standard	O
heuristic	O
for	O
handling	O
missing	B
inputs	O
in	O
decision	B
trees	I
is	O
to	O
look	O
for	O
a	O
series	O
of	O
”	O
backup	O
”	O
variables	O
,	O
which	O
can	O
induce	O
a	O
similar	B
partition	O
to	O
the	O
chosen	O
variable	O
at	O
any	O
given	O
split	O
;	O
these	O
can	O
be	O
used	O
in	O
case	O
the	O
chosen	O
variable	O
is	O
unobserved	O
at	O
test	O
time	O
.	O
these	O
are	O
called	O
surrogate	B
splits	I
.	O
this	O
method	O
ﬁnds	O
highly	O
correlated	O
features	O
,	O
and	O
can	O
be	O
thought	O
of	O
as	O
learning	B
a	O
local	O
joint	O
model	O
of	O
the	O
input	O
.	O
this	O
has	O
the	O
advantage	O
over	O
a	O
generative	O
model	O
of	O
not	O
modeling	O
the	O
entire	O
joint	B
distribution	I
of	O
inputs	O
,	O
but	O
it	O
has	O
the	O
disadvantage	O
of	O
being	O
entirely	O
ad	O
hoc	O
.	O
a	O
simpler	O
approach	O
,	O
applicable	O
to	O
categorical	B
variables	I
,	O
is	O
to	O
code	O
“	O
missing	B
”	O
as	O
a	O
new	O
value	O
,	O
and	O
then	O
to	O
treat	O
the	O
data	O
as	O
fully	O
observed	O
.	O
16.2.	O
classiﬁcation	B
and	O
regression	B
trees	O
(	O
cart	O
)	O
replacement	O
,	O
and	O
then	O
compute	O
the	O
ensemble	B
m	O
(	O
cid:4	O
)	O
m=1	O
f	O
(	O
x	O
)	O
=	O
1	O
m	O
fm	O
(	O
x	O
)	O
551	O
(	O
16.15	O
)	O
where	O
fm	O
is	O
the	O
m	O
’	O
th	O
tree	B
.	O
this	O
technique	O
is	O
called	O
bagging	B
(	O
breiman	O
1996	O
)	O
,	O
which	O
stands	O
for	O
“	O
bootstrap	B
aggregating	O
”	O
.	O
unfortunately	O
,	O
simply	O
re-running	O
the	O
same	O
learning	B
algorithm	O
on	O
different	O
subsets	O
of	O
the	O
data	O
can	O
result	O
in	O
highly	O
correlated	O
predictors	O
,	O
which	O
limits	O
the	O
amount	O
of	O
variance	B
reduction	O
that	O
is	O
possible	O
.	O
the	O
technique	O
known	O
as	O
random	B
forests	I
(	O
breiman	O
2001a	O
)	O
tries	O
to	O
decorrelate	O
the	O
base	O
learners	O
by	O
learning	B
trees	O
based	O
on	O
a	O
randomly	O
chosen	O
subset	O
of	O
input	O
variables	O
,	O
as	O
well	O
as	O
a	O
randomly	O
chosen	O
subset	O
of	O
data	O
cases	O
.	O
such	O
models	O
often	O
have	O
very	O
good	O
predictive	B
accuracy	O
(	O
caruana	O
and	O
niculescu-mizil	O
2006	O
)	O
,	O
and	O
have	O
been	O
widely	O
used	O
in	O
many	O
applications	O
(	O
e.g.	O
,	O
for	O
body	O
pose	O
recognition	O
using	O
microsoft	O
’	O
s	O
popular	O
kinect	B
sensor	O
(	O
shotton	O
et	O
al	O
.	O
2011	O
)	O
)	O
.	O
bagging	B
is	O
a	O
frequentist	B
concept	O
.	O
it	O
is	O
also	O
possible	O
to	O
adopt	O
a	O
bayesian	O
approach	O
to	O
learning	B
trees	O
.	O
in	O
particular	O
,	O
(	O
chipman	O
et	O
al	O
.	O
1998	O
;	O
denison	O
et	O
al	O
.	O
1998	O
;	O
wu	O
et	O
al	O
.	O
2007	O
)	O
perform	O
approximate	B
inference	I
over	O
the	O
space	O
of	O
trees	O
(	O
structure	O
and	O
parameters	O
)	O
using	O
mcmc	O
.	O
this	O
reduces	O
the	O
variance	B
of	O
the	O
predictions	O
.	O
we	O
can	O
also	O
perform	O
bayesian	O
inference	B
over	O
the	O
space	O
of	O
ensembles	O
of	O
trees	O
,	O
which	O
tends	O
to	O
work	O
much	O
better	O
.	O
this	O
is	O
known	O
as	O
bayesian	O
adaptive	O
regression	O
trees	O
or	O
bart	O
(	O
chipman	O
et	O
al	O
.	O
2010	O
)	O
.	O
note	O
that	O
the	O
cost	O
of	O
these	O
sampling-based	O
bayesian	O
methods	O
is	O
comparable	O
to	O
the	O
sampling-based	O
random	O
forest	O
method	O
.	O
that	O
is	O
,	O
both	O
approaches	O
are	O
farily	O
slow	O
to	O
train	O
,	O
but	O
produce	O
high	O
quality	O
classiﬁers	O
.	O
unfortunately	O
,	O
methods	O
that	O
use	O
multiple	O
trees	O
(	O
whether	O
derived	O
from	O
a	O
bayesian	O
or	O
frequen-	O
tist	O
standpoint	O
)	O
lose	O
their	O
nice	O
interpretability	O
properties	O
.	O
fortunately	O
,	O
various	O
post-processing	O
measures	O
can	O
be	O
applied	O
,	O
as	O
discussed	O
in	O
section	O
16.8	O
.	O
16.2.6	O
cart	O
compared	O
to	O
hierarchical	B
mixture	I
of	I
experts	I
*	O
an	O
interesting	O
alternative	O
to	O
a	O
decision	B
tree	O
is	O
known	O
as	O
the	O
hierarchical	B
mixture	I
of	I
experts	I
.	O
figure	O
11.7	O
(	O
b	O
)	O
gives	O
an	O
illustration	O
where	O
we	O
have	O
two	O
levels	O
of	O
experts	O
.	O
this	O
can	O
be	O
thought	O
of	O
as	O
a	O
probabilistic	B
decision	I
tree	I
of	O
depth	O
2	O
,	O
since	O
we	O
recursively	O
partition	O
the	O
space	O
,	O
and	O
apply	O
a	O
different	O
expert	O
to	O
each	O
partition	O
.	O
hastie	O
et	O
al	O
.	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p331	O
)	O
write	O
that	O
“	O
the	O
hme	O
approach	O
is	O
a	O
promising	O
competitor	O
to	O
cart	O
trees	O
”	O
.	O
some	O
of	O
the	O
advantages	O
include	O
the	O
following	O
:	O
•	O
the	O
model	O
can	O
partition	O
the	O
input	O
space	O
using	O
any	O
set	O
of	O
nested	O
linear	O
decision	B
boundaries	O
.	O
by	O
contrast	O
,	O
standard	O
decision	O
trees	O
are	O
constrained	O
to	O
use	O
axis-parallel	O
splits	O
.	O
•	O
the	O
model	O
makes	O
predictions	O
by	O
averaging	O
over	O
all	O
experts	O
.	O
by	O
contrast	O
,	O
in	O
a	O
standard	O
decision	O
tree	B
,	O
predictions	O
are	O
made	O
only	O
based	O
on	O
the	O
model	O
in	O
the	O
corresponding	O
leaf	B
.	O
since	O
leaves	B
often	O
contain	O
few	O
training	O
examples	O
,	O
this	O
can	O
result	O
in	O
overﬁtting	B
.	O
•	O
fitting	O
an	O
hme	O
involves	O
solving	O
a	O
smooth	O
continuous	O
optimization	B
problem	O
(	O
usually	O
using	O
em	O
)	O
,	O
which	O
is	O
likely	O
to	O
be	O
less	O
prone	O
to	O
local	O
optima	O
than	O
the	O
standard	O
greedy	O
discrete	B
optimization	O
methods	O
used	O
to	O
ﬁt	O
decision	B
trees	I
.	O
for	O
similar	B
reasons	O
,	O
it	O
is	O
computationally	O
easier	O
to	O
“	O
be	O
bayesian	O
”	O
about	O
the	O
parameters	O
of	O
an	O
hme	O
(	O
see	O
e.g.	O
,	O
(	O
peng	O
et	O
al	O
.	O
1996	O
;	O
bishop	O
552	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
and	O
svensén	O
2003	O
)	O
)	O
than	O
about	O
the	O
structure	O
and	O
parameters	O
of	O
a	O
decision	B
tree	O
(	O
see	O
e.g.	O
,	O
(	O
wu	O
et	O
al	O
.	O
2007	O
)	O
)	O
.	O
16.3	O
generalized	O
additive	O
models	O
a	O
simple	O
way	O
to	O
create	O
a	O
nonlinear	O
model	O
with	O
multiple	O
inputs	O
is	O
to	O
use	O
a	O
generalized	B
additive	I
model	I
(	O
hastie	O
and	O
tibshirani	O
1990	O
)	O
,	O
which	O
is	O
a	O
model	O
of	O
the	O
form	O
f	O
(	O
x	O
)	O
=	O
α	O
+	O
f1	O
(	O
x1	O
)	O
+···	O
+	O
fd	O
(	O
xd	O
)	O
(	O
16.16	O
)	O
here	O
each	O
fj	O
can	O
be	O
modeled	O
by	O
some	O
scatterplot	O
smoother	O
,	O
and	O
f	O
(	O
x	O
)	O
can	O
be	O
mapped	O
to	O
p	O
(	O
y|x	O
)	O
using	O
a	O
link	B
function	I
,	O
as	O
in	O
a	O
glm	O
(	O
hence	O
the	O
term	O
generalized	B
additive	I
model	I
)	O
.	O
if	O
we	O
use	O
regression	B
splines	O
(	O
or	O
some	O
other	O
ﬁxed	O
basis	O
function	O
expansion	O
approach	O
)	O
for	O
the	O
fj	O
,	O
then	O
each	O
fj	O
(	O
xj	O
)	O
can	O
be	O
written	O
as	O
βt	O
j	O
φj	O
(	O
xj	O
)	O
,	O
so	O
the	O
whole	O
model	O
can	O
be	O
written	O
as	O
f	O
(	O
x	O
)	O
=	O
βt	O
φ	O
(	O
x	O
)	O
,	O
where	O
φ	O
(	O
x	O
)	O
=	O
[	O
1	O
,	O
φ1	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
φd	O
(	O
xd	O
)	O
]	O
.	O
however	O
,	O
it	O
is	O
more	O
common	O
to	O
use	O
smoothing	O
splines	O
(	O
section	O
15.4.6	O
)	O
for	O
the	O
fj	O
.	O
in	O
this	O
case	O
,	O
the	O
objective	O
(	O
in	O
the	O
regression	B
setting	O
)	O
becomes	O
n	O
(	O
cid:4	O
)	O
⎛	O
⎝yi	O
−	O
α	O
−	O
d	O
(	O
cid:4	O
)	O
⎞	O
⎠2	O
d	O
(	O
cid:4	O
)	O
(	O
cid:12	O
)	O
λj	O
+	O
j=1	O
f	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
j	O
(	O
tj	O
)	O
2dtj	O
(	O
16.17	O
)	O
j	O
(	O
α	O
,	O
f1	O
,	O
.	O
.	O
.	O
,	O
fd	O
)	O
=	O
fj	O
(	O
xij	O
)	O
i=1	O
j=1	O
where	O
λj	O
is	O
the	O
strength	O
of	O
the	O
regularizer	O
for	O
fj	O
.	O
16.3.1	O
backﬁtting	B
(	O
cid:7	O
)	O
n	O
we	O
now	O
discuss	O
how	O
to	O
ﬁt	O
the	O
model	O
using	O
mle	O
.	O
the	O
constant	O
α	O
is	O
not	O
uniquely	O
identiﬁable	B
,	O
since	O
we	O
can	O
always	O
add	O
or	O
subtract	O
constants	O
to	O
any	O
of	O
the	O
fj	O
functions	O
.	O
the	O
convention	O
is	O
to	O
assume	O
i=1	O
fj	O
(	O
xij	O
)	O
=	O
0	O
for	O
all	O
j.	O
in	O
this	O
case	O
,	O
the	O
mle	O
for	O
α	O
is	O
just	O
ˆα	O
=	O
1	O
n	O
to	O
ﬁt	O
the	O
rest	O
of	O
the	O
model	O
,	O
we	O
can	O
center	O
the	O
responses	O
(	O
by	O
subtracting	O
ˆα	O
)	O
,	O
and	O
then	O
iteratively	O
update	O
each	O
fj	O
in	O
turn	O
,	O
using	O
as	O
a	O
target	O
vector	O
the	O
residuals	O
obtained	O
by	O
omitting	O
term	O
fj	O
:	O
i=1	O
yi	O
.	O
(	O
cid:7	O
)	O
n	O
(	O
cid:4	O
)	O
k	O
(	O
cid:5	O
)	O
=j	O
ˆfk	O
(	O
xik	O
)	O
}	O
n	O
i=1	O
)	O
ˆfj	O
:	O
=	O
smoother	O
(	O
{	O
yi	O
−	O
n	O
(	O
cid:4	O
)	O
ˆfj	O
:	O
=	O
ˆfj	O
−	O
1	O
n	O
i=1	O
ˆfj	O
(	O
xij	O
)	O
we	O
should	O
then	O
ensure	O
the	O
output	O
is	O
zero	O
mean	O
using	O
(	O
16.18	O
)	O
(	O
16.19	O
)	O
this	O
is	O
called	O
the	O
backﬁtting	B
algorithm	O
(	O
hastie	O
and	O
tibshirani	O
1990	O
)	O
.	O
if	O
x	O
has	O
full	B
column	O
rank	O
,	O
then	O
the	O
above	O
objective	O
is	O
convex	B
(	O
since	O
each	O
smoothing	O
spline	O
is	O
a	O
linear	O
operator	O
,	O
as	O
shown	O
in	O
section	O
15.4.2	O
)	O
,	O
so	O
this	O
procedure	O
is	O
guaranteed	O
to	O
converge	B
to	O
the	O
global	O
optimum	O
.	O
in	O
the	O
glm	O
case	O
,	O
we	O
need	O
to	O
modify	O
the	O
method	O
somewhat	O
.	O
the	O
basic	O
idea	O
is	O
to	O
replace	O
the	O
weighted	B
least	I
squares	I
step	O
of	O
irls	O
(	O
see	O
section	O
8.3.4	O
)	O
with	O
a	O
weighted	O
backﬁtting	O
algorithm	O
.	O
in	O
the	O
logistic	B
regression	I
case	O
,	O
each	O
response	O
has	O
weight	O
si	O
=	O
μi	O
(	O
1	O
−	O
μi	O
)	O
associated	O
with	O
it	O
,	O
where	O
μi	O
=	O
sigm	O
(	O
ˆα	O
+	O
(	O
cid:7	O
)	O
d	O
ˆfj	O
(	O
xij	O
)	O
)	O
.	O
)	O
j=1	O
16.3.	O
generalized	O
additive	O
models	O
553	O
16.3.2	O
computational	O
efficiency	O
each	O
call	O
to	O
the	O
smoother	O
takes	O
o	O
(	O
n	O
)	O
time	O
,	O
so	O
the	O
total	O
cost	O
is	O
o	O
(	O
n	O
dt	O
)	O
,	O
where	O
t	O
is	O
the	O
number	O
of	O
iterations	O
.	O
if	O
we	O
have	O
high-dimensional	O
inputs	O
,	O
ﬁtting	O
a	O
gam	O
is	O
expensive	O
.	O
one	O
approach	O
is	O
to	O
combine	O
it	O
with	O
a	O
sparsity	B
penalty	O
,	O
see	O
e.g.	O
,	O
the	O
spam	B
(	O
sparse	B
additive	O
model	O
)	O
approach	O
of	O
(	O
ravikumar	O
et	O
al	O
.	O
2009	O
)	O
.	O
alternatively	O
,	O
we	O
can	O
use	O
a	O
greedy	O
approach	O
,	O
such	O
as	O
boosting	B
(	O
see	O
section	O
16.4.6	O
)	O
16.3.3	O
multivariate	B
adaptive	I
regression	I
splines	I
(	O
mars	O
)	O
in	O
general	O
,	O
we	O
can	O
create	O
an	O
anova	O
we	O
can	O
extend	O
gams	O
by	O
allowing	O
for	O
interaction	B
effects	I
.	O
decomposition	O
:	O
d	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
f	O
(	O
x	O
)	O
=	O
β0	O
+	O
fj	O
(	O
xj	O
)	O
+	O
fjk	O
(	O
xj	O
,	O
xk	O
)	O
+	O
fjkl	O
(	O
xj	O
,	O
xk	O
,	O
xl	O
)	O
+···	O
(	O
16.20	O
)	O
j=1	O
j	O
,	O
k	O
j	O
,	O
k	O
,	O
l	O
of	O
course	O
,	O
we	O
can	O
not	O
allow	O
for	O
too	O
many	O
higher-order	O
interactions	O
,	O
because	O
there	O
will	O
be	O
too	O
many	O
parameters	O
to	O
ﬁt	O
.	O
it	O
is	O
common	O
to	O
use	O
greedy	O
search	O
to	O
decide	O
which	O
variables	O
to	O
add	O
.	O
the	O
multivariate	B
adaptive	I
regression	I
splines	I
or	O
mars	O
algorithm	O
is	O
one	O
example	O
of	O
this	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
sec9.4	O
)	O
.	O
it	O
ﬁts	O
models	O
of	O
the	O
form	O
in	O
equation	O
16.20	O
,	O
where	O
it	O
uses	O
a	O
tensor	B
product	I
basis	I
of	O
regression	B
splines	O
to	O
represent	O
the	O
multidimensional	O
regression	O
functions	O
.	O
for	O
example	O
,	O
for	O
2d	O
input	O
,	O
we	O
might	O
use	O
f	O
(	O
x1	O
,	O
x2	O
)	O
=β	O
0	O
+	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
+	O
m	O
β1m	O
(	O
x1	O
−	O
t1m	O
)	O
+	O
(	O
cid:4	O
)	O
β2m	O
(	O
t2m	O
−	O
x2	O
)	O
+	O
+	O
m	O
m	O
β12m	O
(	O
x1	O
−	O
t1m	O
)	O
+	O
(	O
t2m	O
−	O
x2	O
)	O
+	O
(	O
16.21	O
)	O
to	O
create	O
such	O
a	O
function	O
,	O
we	O
start	O
with	O
a	O
set	O
of	O
candidate	O
basis	B
functions	I
of	O
the	O
form	O
c	O
=	O
{	O
(	O
xj	O
−	O
t	O
)	O
+	O
,	O
(	O
t	O
−	O
xj	O
)	O
+	O
:	O
t	O
∈	O
{	O
x1j	O
,	O
.	O
.	O
.	O
,	O
xn	O
j	O
}	O
,	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
(	O
16.22	O
)	O
these	O
are	O
1d	O
linear	O
splines	O
where	O
the	O
knots	B
are	O
at	O
all	O
the	O
observed	O
values	O
for	O
that	O
variable	O
.	O
we	O
consider	O
splines	O
sloping	O
up	O
in	O
both	O
directions	O
;	O
this	O
is	O
called	O
a	O
reﬂecting	B
pair	I
.	O
see	O
figure	O
16.7	O
(	O
a	O
)	O
.	O
let	O
m	O
represent	O
the	O
current	O
set	O
of	O
basis	B
functions	I
.	O
we	O
initialize	O
by	O
using	O
m	O
=	O
{	O
1	O
}	O
.	O
we	O
consider	O
creating	O
a	O
new	O
basis	O
function	O
pair	O
by	O
multplying	O
an	O
hm	O
∈	O
m	O
with	O
one	O
of	O
the	O
reﬂecting	O
pairs	O
in	O
c.	O
for	O
example	O
,	O
we	O
might	O
initially	O
get	O
f	O
(	O
x	O
)	O
=	O
25	O
−	O
4	O
(	O
x1	O
−	O
5	O
)	O
+	O
+	O
20	O
(	O
5	O
−	O
x1	O
)	O
+	O
(	O
16.23	O
)	O
obtained	O
by	O
multiplying	O
h0	O
(	O
x	O
)	O
=	O
1	O
with	O
a	O
reﬂecting	B
pair	I
involving	O
x1	O
with	O
knot	O
t	O
=	O
5.	O
this	O
pair	O
is	O
added	O
to	O
m.	O
see	O
figure	O
16.7	O
(	O
b	O
)	O
.	O
at	O
the	O
next	O
step	O
,	O
we	O
might	O
create	O
a	O
model	O
such	O
as	O
f	O
(	O
x	O
)	O
=	O
=	O
2	O
−	O
2	O
(	O
x1	O
−	O
5	O
)	O
+	O
+	O
3	O
(	O
5−	O
x1	O
)	O
+	O
−	O
(	O
x2	O
−	O
10	O
)	O
+	O
×	O
(	O
5	O
−	O
x1	O
)	O
+−	O
1.2	O
(	O
10	O
−	O
x2	O
)	O
+	O
×	O
(	O
5	O
−	O
x1	O
)	O
+	O
(	O
16.24	O
)	O
obtained	O
by	O
multiplying	O
(	O
5−x1	O
)	O
+	O
from	O
m	O
by	O
the	O
new	O
reﬂecting	B
pair	I
(	O
x2−10	O
)	O
+	O
and	O
(	O
10−x2	O
)	O
+	O
.	O
this	O
new	O
function	O
is	O
shown	O
in	O
figure	O
16.7	O
(	O
c	O
)	O
.	O
554	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
15	O
10	O
5	O
0	O
0	O
5	O
10	O
(	O
a	O
)	O
15	O
20	O
120	O
100	O
80	O
60	O
40	O
20	O
0	O
−20	O
−40	O
20	O
0	O
−20	O
−40	O
−60	O
20	O
0	O
5	O
10	O
(	O
b	O
)	O
15	O
20	O
15	O
10	O
5	O
0	O
0	O
10	O
5	O
20	O
15	O
(	O
c	O
)	O
(	O
a	O
)	O
linear	O
spline	O
function	O
with	O
a	O
knot	O
at	O
5.	O
solid	O
blue	O
:	O
(	O
x−	O
5	O
)	O
+	O
.	O
dotted	O
red	O
:	O
(	O
5−	O
x	O
)	O
+	O
.	O
(	O
b	O
)	O
a	O
figure	O
16.7	O
mars	O
model	O
in	O
1d	O
given	O
by	O
equation	O
16.23	O
.	O
(	O
c	O
)	O
a	O
simple	O
mars	O
model	O
in	O
2d	O
given	O
by	O
equation	O
16.24.	O
figure	O
generated	O
by	O
marsdemo	O
.	O
we	O
proceed	O
in	O
this	O
way	O
until	O
the	O
model	O
becomes	O
very	O
large	O
.	O
(	O
we	O
may	O
impose	O
an	O
upper	O
bound	O
on	O
the	O
order	O
of	O
interactions	O
.	O
)	O
then	O
we	O
prune	O
backwards	O
,	O
at	O
each	O
step	O
eliminating	O
the	O
basis	O
function	O
that	O
causes	O
the	O
smallest	O
increase	O
in	O
the	O
residual	B
error	I
,	O
until	O
the	O
cv	O
error	O
stops	O
improving	O
.	O
the	O
whole	O
procedure	O
is	O
closely	O
related	O
to	O
cart	O
.	O
to	O
see	O
this	O
,	O
suppose	O
we	O
replace	O
the	O
piecewise	O
linear	O
basis	B
functions	I
by	O
step	O
functions	O
i	O
(	O
xj	O
>	O
t	O
)	O
and	O
i	O
(	O
xj	O
<	O
t	O
)	O
.	O
multiplying	O
by	O
a	O
pair	O
of	O
reﬂected	O
step	O
functions	O
is	O
equivalent	O
to	O
splitting	O
a	O
node	O
.	O
now	O
suppose	O
we	O
impose	O
the	O
constraint	O
that	O
once	O
a	O
variable	O
is	O
involved	O
in	O
a	O
multiplication	O
by	O
a	O
candidate	O
term	O
,	O
that	O
variable	O
gets	O
replaced	O
by	O
the	O
interaction	O
,	O
so	O
the	O
original	O
variable	O
is	O
no	O
longer	O
available	O
.	O
this	O
ensures	O
that	O
a	O
variable	O
can	O
not	O
be	O
split	O
more	O
than	O
once	O
,	O
thus	O
guaranteeing	O
that	O
the	O
resulting	O
model	O
can	O
be	O
represented	O
as	O
a	O
tree	B
.	O
in	O
this	O
case	O
,	O
the	O
mars	O
growing	O
strategy	O
is	O
the	O
same	O
as	O
the	O
cart	O
growing	O
strategy	O
.	O
16.4	O
boosting	B
boosting	O
(	O
schapire	O
and	O
freund	O
2012	O
)	O
is	O
a	O
greedy	O
algorithm	O
for	O
ﬁtting	O
adaptive	O
basis-function	O
models	O
of	O
the	O
form	O
in	O
equation	O
16.3	O
,	O
where	O
the	O
φm	O
are	O
generated	O
by	O
an	O
algorithm	O
called	O
a	O
weak	B
learner	I
or	O
a	O
base	B
learner	I
.	O
the	O
algorithm	O
works	O
by	O
applying	O
the	O
weak	B
learner	I
sequentially	O
to	O
weighted	O
versions	O
of	O
the	O
data	O
,	O
where	O
more	O
weight	O
is	O
given	O
to	O
examples	O
that	O
were	O
misclassiﬁed	O
by	O
earlier	O
rounds	O
.	O
this	O
weak	B
learner	I
can	O
be	O
any	O
classiﬁcation	B
or	O
regression	B
algorithm	O
,	O
but	O
it	O
is	O
common	O
to	O
use	O
a	O
cart	O
model	O
.	O
in	O
1998	O
,	O
the	O
late	O
leo	O
breiman	O
called	O
boosting	B
,	O
where	O
the	O
weak	B
learner	I
is	O
a	O
shallow	O
decision	O
tree	B
,	O
the	O
“	O
best	O
off-the-shelf	O
classiﬁer	O
in	O
the	O
world	O
”	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p340	O
)	O
.	O
this	O
is	O
supported	O
by	O
an	O
extensive	O
empirical	O
comparison	O
of	O
10	O
different	O
classiﬁers	O
in	O
(	O
caruana	O
and	O
niculescu-mizil	O
2006	O
)	O
,	O
who	O
showed	O
that	O
boosted	O
decision	B
trees	I
were	O
the	O
best	O
both	O
in	O
terms	O
of	O
misclassiﬁcation	O
error	O
and	O
in	O
terms	O
of	O
producing	O
well-calibrated	O
probabilities	O
,	O
as	O
judged	O
by	O
roc	O
curves	O
.	O
(	O
the	O
second	O
best	O
method	O
was	O
random	B
forests	I
,	O
invented	O
by	O
breiman	O
;	O
see	O
section	O
16.2.5	O
.	O
)	O
by	O
contrast	O
,	O
single	O
decision	O
trees	O
performed	O
very	O
poorly	O
.	O
boosting	B
was	O
originally	O
derived	O
in	O
the	O
computational	B
learning	I
theory	I
literature	O
(	O
schapire	O
1990	O
;	O
freund	O
and	O
schapire	O
1996	O
)	O
,	O
where	O
the	O
focus	O
is	O
binary	B
classiﬁcation	I
.	O
in	O
these	O
papers	O
,	O
it	O
was	O
proved	O
that	O
one	O
could	O
boost	O
the	O
performance	O
(	O
on	O
the	O
training	B
set	I
)	O
of	O
any	O
weak	B
learner	I
arbitrarily	O
16.4.	O
boosting	B
555	O
0.16	O
0.14	O
0.12	O
0.1	O
0.08	O
0.06	O
0.04	O
0.02	O
0	O
0	O
train	O
test	O
20	O
40	O
60	O
80	O
100	O
120	O
140	O
figure	O
16.8	O
performance	O
of	O
adaboost	O
using	O
a	O
decision	B
stump	O
as	O
a	O
weak	B
learner	I
on	O
the	O
data	O
in	O
figure	O
16.10.	O
training	O
(	O
solid	O
blue	O
)	O
and	O
test	O
(	O
dotted	O
red	O
)	O
error	O
vs	O
number	O
of	O
iterations	O
.	O
figure	O
generated	O
by	O
boostingdemo	O
,	O
written	O
by	O
richard	O
stapenhurst	O
.	O
high	O
,	O
provided	O
the	O
weak	B
learner	I
could	O
always	O
perform	O
slightly	O
better	O
than	O
chance	O
.	O
for	O
example	O
,	O
in	O
figure	O
16.8	O
,	O
we	O
plot	O
the	O
training	O
and	O
test	O
error	O
for	O
boosted	O
decision	B
stumps	O
on	O
a	O
2d	O
dataset	O
shown	O
in	O
figure	O
16.10.	O
we	O
see	O
that	O
the	O
training	B
set	I
error	O
rapidly	O
goes	O
to	O
near	O
zero	O
.	O
what	O
is	O
more	O
surprising	O
is	O
that	O
the	O
test	O
set	O
error	O
continues	O
to	O
decline	O
even	O
after	O
the	O
training	B
set	I
error	O
has	O
reached	O
zero	O
(	O
although	O
the	O
test	O
set	O
error	O
will	O
eventually	O
go	O
up	O
)	O
.	O
thus	O
boosting	B
is	O
very	O
resistant	O
to	O
overﬁtting	B
.	O
(	O
boosted	O
decision	B
stumps	O
form	O
the	O
basis	O
of	O
a	O
very	O
successful	O
face	B
detector	I
(	O
viola	O
and	O
jones	O
2001	O
)	O
,	O
which	O
was	O
used	O
to	O
generate	O
the	O
results	O
in	O
figure	O
1.6	O
,	O
and	O
which	O
is	O
used	O
in	O
many	O
digital	B
cameras	I
.	O
)	O
in	O
view	O
of	O
its	O
stunning	O
empirical	O
success	O
,	O
statisticians	O
started	O
to	O
become	O
interested	O
in	O
this	O
method	O
.	O
breiman	O
(	O
breiman	O
1998	O
)	O
showed	O
that	O
boosting	B
can	O
be	O
interpreted	O
as	O
a	O
form	O
of	O
gradient	B
descent	I
in	O
function	O
space	O
.	O
this	O
view	O
was	O
then	O
extended	O
in	O
(	O
friedman	O
et	O
al	O
.	O
2000	O
)	O
,	O
who	O
showed	O
how	O
boosting	B
could	O
be	O
extended	O
to	O
handle	O
a	O
variety	O
of	O
loss	B
functions	O
,	O
including	O
for	O
regression	B
,	O
robust	B
regression	O
,	O
poisson	B
regression	I
,	O
etc	O
.	O
in	O
this	O
section	O
,	O
we	O
shall	O
present	O
this	O
statistical	O
inter-	O
pretation	O
of	O
boosting	B
,	O
drawing	O
on	O
the	O
reviews	O
in	O
(	O
buhlmann	O
and	O
hothorn	O
2007	O
)	O
and	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
ch10	O
)	O
,	O
which	O
should	O
be	O
consulted	O
for	O
further	O
details	O
.	O
16.4.1	O
forward	B
stagewise	I
additive	I
modeling	I
the	O
goal	O
of	O
boosting	B
is	O
to	O
solve	O
the	O
following	O
optimization	B
problem	O
:	O
l	O
(	O
yi	O
,	O
f	O
(	O
xi	O
)	O
)	O
(	O
16.25	O
)	O
n	O
(	O
cid:4	O
)	O
min	O
f	O
i=1	O
and	O
l	O
(	O
y	O
,	O
ˆy	O
)	O
is	O
some	O
loss	B
function	I
,	O
and	O
f	O
is	O
assumed	O
to	O
be	O
an	O
abm	O
model	O
as	O
in	O
equation	O
16.3.	O
common	O
choices	O
for	O
the	O
loss	B
function	I
are	O
listed	O
in	O
table	O
16.1.	O
if	O
we	O
use	O
squared	B
error	I
loss	O
,	O
the	O
optimal	O
estimate	O
is	O
given	O
by	O
f∗	O
(	O
y	O
−	O
f	O
(	O
x	O
)	O
)	O
2	O
=	O
e	O
[	O
y	O
|x	O
]	O
(	O
x	O
)	O
=	O
argmin	O
=	O
ey|x	O
f	O
(	O
x	O
)	O
(	O
16.26	O
)	O
(	O
cid:13	O
)	O
(	O
cid:14	O
)	O
556	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
name	O
squared	B
error	I
absolute	O
error	O
exponential	O
loss	B
logloss	O
loss	B
2	O
(	O
yi	O
−	O
f	O
(	O
xi	O
)	O
)	O
2	O
1	O
|yi	O
−	O
f	O
(	O
xi	O
)	O
|	O
exp	O
(	O
−˜yif	O
(	O
xi	O
)	O
)	O
−˜yi	O
exp	O
(	O
−˜yif	O
(	O
xi	O
)	O
)	O
log	O
(	O
1	O
+	O
e−˜yifi	O
)	O
derivative	O
yi	O
−	O
f	O
(	O
xi	O
)	O
sgn	O
(	O
yi	O
−	O
f	O
(	O
xi	O
)	O
)	O
yi	O
−	O
πi	O
algorithm	O
l2boosting	O
f∗	O
e	O
[	O
y|xi	O
]	O
median	B
(	O
y|xi	O
)	O
gradient	B
boosting	I
2	O
log	O
πi	O
1−πi	O
2	O
log	O
πi	O
1−πi	O
adaboost	O
logitboost	O
1	O
1	O
some	O
commonly	O
used	O
loss	B
functions	O
,	O
their	O
gradients	O
,	O
their	O
population	O
minimizers	O
f∗	O
table	O
16.1	O
,	O
and	O
some	O
algorithms	O
to	O
minimize	O
the	O
loss	B
.	O
for	O
binary	B
classiﬁcation	I
problems	O
,	O
we	O
assume	O
˜yi	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
,	O
yi	O
∈	O
{	O
0	O
,	O
1	O
}	O
and	O
πi	O
=	O
sigm	O
(	O
2f	O
(	O
xi	O
)	O
)	O
.	O
for	O
regression	B
problems	O
,	O
we	O
assume	O
yi	O
∈	O
r.	O
adapted	O
from	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p360	O
)	O
and	O
(	O
buhlmann	O
and	O
hothorn	O
2007	O
,	O
p483	O
)	O
.	O
0−1	O
logloss	O
exp	O
s	O
s	O
o	O
l	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
0	O
−2	O
−1.5	O
−1	O
−0.5	O
0.5	O
1	O
1.5	O
2	O
0	O
y−	O
f	O
figure	O
16.9	O
illustration	O
of	O
various	O
loss	B
functions	O
for	O
binary	B
classiﬁcation	I
.	O
the	O
horizontal	O
axis	O
is	O
the	O
margin	B
yη	O
,	O
the	O
vertical	O
axis	O
is	O
the	O
loss	B
.	O
the	O
log	O
loss	O
uses	O
log	O
base	O
2.	O
figure	O
generated	O
by	O
hingelossplot	O
.	O
as	O
we	O
showed	O
in	O
section	O
5.7.1.3.	O
of	O
course	O
,	O
this	O
can	O
not	O
be	O
computed	O
in	O
practice	O
since	O
it	O
requires	O
knowing	O
the	O
true	O
conditional	O
distribution	O
p	O
(	O
y|x	O
)	O
.	O
hence	O
this	O
is	O
sometimes	O
called	O
the	O
population	B
minimizer	I
,	O
where	O
the	O
expectation	O
is	O
interpreted	O
in	O
a	O
frequentist	B
sense	O
.	O
below	O
we	O
will	O
see	O
that	O
boosting	B
will	O
try	O
to	O
approximate	O
this	O
conditional	O
expectation	O
.	O
for	O
binary	B
classiﬁcation	I
,	O
the	O
obvious	O
loss	B
is	O
0-1	O
loss	B
,	O
but	O
this	O
is	O
not	O
differentiable	O
.	O
instead	O
it	O
is	O
common	O
to	O
use	O
logloss	O
,	O
which	O
is	O
a	O
convex	B
upper	O
bound	O
on	O
0-1	O
loss	B
,	O
as	O
we	O
showed	O
in	O
section	O
6.5.5.	O
in	O
this	O
case	O
,	O
one	O
can	O
show	O
that	O
the	O
optimal	O
estimate	O
is	O
given	O
by	O
log	O
(	O
16.27	O
)	O
where	O
˜y	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
.	O
one	O
can	O
generalize	B
this	O
framework	O
to	O
the	O
multiclass	O
case	O
,	O
but	O
we	O
will	O
not	O
discuss	O
that	O
here	O
.	O
an	O
alternative	O
convex	O
upper	O
bound	O
is	O
exponential	B
loss	I
,	O
deﬁned	O
by	O
l	O
(	O
˜y	O
,	O
f	O
)	O
=	O
exp	O
(	O
−˜yf	O
)	O
(	O
16.28	O
)	O
see	O
figure	O
16.9	O
for	O
a	O
plot	O
.	O
this	O
will	O
have	O
some	O
computational	O
advantages	O
over	O
the	O
logloss	O
,	O
(	O
x	O
)	O
=	O
to	O
be	O
discussed	O
below	O
.	O
it	O
turns	O
out	O
that	O
the	O
optimal	O
estimate	O
for	O
this	O
loss	B
is	O
also	O
f∗	O
f∗	O
(	O
x	O
)	O
=	O
1	O
2	O
p	O
(	O
˜y	O
=	O
1|x	O
)	O
p	O
(	O
˜y	O
=	O
−1|x	O
)	O
16.4.	O
boosting	B
557	O
p	O
(	O
˜y=1|x	O
)	O
p	O
(	O
˜y=−1|x	O
)	O
.	O
to	O
see	O
this	O
,	O
we	O
can	O
just	O
set	O
the	O
derivative	O
of	O
the	O
expected	O
loss	O
(	O
for	O
each	O
x	O
)	O
to	O
1	O
2	O
log	O
zero	O
:	O
(	O
cid:15	O
)	O
∂	O
∂f	O
(	O
x	O
)	O
e	O
e−˜yf	O
(	O
x	O
)	O
|x	O
∂	O
[	O
p	O
(	O
˜y	O
=	O
1|x	O
)	O
e−f	O
(	O
x	O
)	O
+	O
p	O
(	O
˜y	O
=	O
−1|x	O
)	O
ef	O
(	O
x	O
)	O
]	O
∂f	O
(	O
x	O
)	O
=	O
=	O
−p	O
(	O
˜y	O
=	O
1|x	O
)	O
e−f	O
(	O
x	O
)	O
+	O
p	O
(	O
˜y	O
=	O
−1|x	O
)	O
ef	O
(	O
x	O
)	O
=	O
0	O
⇒	O
p	O
(	O
˜y	O
=	O
1|x	O
)	O
p	O
(	O
˜y	O
=	O
1	O
−	O
|x	O
)	O
=	O
e2f	O
(	O
x	O
)	O
(	O
16.29	O
)	O
(	O
16.30	O
)	O
(	O
16.31	O
)	O
so	O
in	O
both	O
cases	O
,	O
we	O
can	O
see	O
that	O
boosting	B
should	O
try	O
to	O
approximate	O
(	O
half	O
)	O
the	O
log-odds	B
ratio	I
.	O
since	O
ﬁnding	O
the	O
optimal	O
f	O
is	O
hard	O
,	O
we	O
shall	O
tackle	O
it	O
sequentially	O
.	O
we	O
initialise	O
by	O
deﬁning	O
f0	O
(	O
x	O
)	O
=	O
arg	O
min	O
γ	O
l	O
(	O
yi	O
,	O
f	O
(	O
xi	O
;	O
γ	O
)	O
)	O
for	O
example	O
,	O
if	O
we	O
use	O
squared	B
error	I
,	O
we	O
can	O
set	O
f0	O
(	O
x	O
)	O
=	O
y	O
,	O
and	O
if	O
we	O
use	O
log-loss	B
or	O
exponential	B
loss	I
,	O
we	O
can	O
set	O
f0	O
(	O
x	O
)	O
=	O
1	O
i=1	O
i	O
(	O
yi	O
=	O
1	O
)	O
.	O
we	O
could	O
also	O
use	O
a	O
more	O
powerful	O
model	O
for	O
our	O
baseline	O
,	O
such	O
as	O
a	O
glm	O
.	O
1−ˆπ	O
,	O
where	O
ˆπ	O
=	O
1	O
2	O
log	O
ˆπ	O
n	O
then	O
at	O
iteration	O
m	O
,	O
we	O
compute	O
(	O
cid:7	O
)	O
n	O
(	O
cid:16	O
)	O
n	O
(	O
cid:4	O
)	O
i=1	O
n	O
(	O
cid:4	O
)	O
i=1	O
(	O
16.32	O
)	O
(	O
16.33	O
)	O
(	O
16.34	O
)	O
l	O
(	O
yi	O
,	O
fm−1	O
(	O
xi	O
)	O
+βφ	O
(	O
xi	O
;	O
γ	O
)	O
)	O
(	O
βm	O
,	O
γm	O
)	O
=	O
argmin	O
β	O
,	O
γ	O
and	O
then	O
we	O
set	O
fm	O
(	O
x	O
)	O
=	O
fm−1	O
(	O
x	O
)	O
+β	O
mφ	O
(	O
x	O
;	O
γm	O
)	O
the	O
key	O
point	O
is	O
that	O
we	O
do	O
not	O
go	O
back	O
and	O
adjust	O
earlier	O
parameters	O
.	O
this	O
is	O
why	O
the	O
method	O
is	O
called	O
forward	B
stagewise	I
additive	I
modeling	I
.	O
we	O
continue	O
this	O
for	O
a	O
ﬁxed	O
number	O
of	O
iterations	O
m	O
.	O
in	O
fact	O
m	O
is	O
the	O
main	O
tuning	O
parameter	B
of	O
the	O
method	O
.	O
often	O
we	O
pick	O
it	O
by	O
monitoring	O
the	O
performance	O
on	O
a	O
separate	O
validation	B
set	I
,	O
and	O
then	O
stopping	O
once	O
performance	O
starts	O
to	O
decrease	O
;	O
this	O
is	O
called	O
early	B
stopping	I
.	O
alternatively	O
,	O
we	O
can	O
use	O
model	B
selection	I
criteria	O
such	O
as	O
aic	O
or	O
bic	O
(	O
see	O
e.g.	O
,	O
(	O
buhlmann	O
and	O
hothorn	O
2007	O
)	O
for	O
details	O
)	O
.	O
in	O
practice	O
,	O
better	O
(	O
test	O
set	O
)	O
performance	O
can	O
be	O
obtained	O
by	O
performing	O
“	O
partial	O
updates	O
”	O
of	O
the	O
form	O
fm	O
(	O
x	O
)	O
=	O
fm−1	O
(	O
x	O
)	O
+νβ	O
mφ	O
(	O
x	O
;	O
γm	O
)	O
(	O
16.35	O
)	O
here	O
0	O
<	O
ν	O
≤	O
1	O
is	O
a	O
step-size	O
parameter	B
.	O
in	O
practice	O
it	O
is	O
common	O
to	O
use	O
a	O
small	O
value	O
such	O
as	O
ν	O
=	O
0.1.	O
this	O
is	O
called	O
shrinkage	B
.	O
below	O
we	O
discuss	O
how	O
to	O
solve	O
the	O
suproblem	O
in	O
equation	O
16.33.	O
this	O
will	O
depend	O
on	O
the	O
form	O
of	O
loss	B
function	I
.	O
however	O
,	O
it	O
is	O
independent	O
of	O
the	O
form	O
of	O
weak	B
learner	I
.	O
16.4.2	O
l2boosting	O
suppose	O
we	O
used	O
squared	B
error	I
loss	O
.	O
then	O
at	O
step	O
m	O
the	O
loss	B
has	O
the	O
form	O
l	O
(	O
yi	O
,	O
fm−1	O
(	O
xi	O
)	O
+βφ	O
(	O
xi	O
;	O
γ	O
)	O
)	O
=	O
(	O
rim	O
−	O
φ	O
(	O
xi	O
;	O
γ	O
)	O
)	O
2	O
(	O
16.36	O
)	O
558	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
16.10	O
example	O
of	O
adaboost	O
using	O
a	O
decision	B
stump	O
as	O
a	O
weak	B
learner	I
.	O
the	O
degree	B
of	O
blackness	O
represents	O
the	O
conﬁdence	O
in	O
the	O
red	O
class	O
.	O
the	O
degree	B
of	O
whiteness	O
represents	O
the	O
conﬁdence	O
in	O
the	O
blue	O
class	O
.	O
the	O
size	O
of	O
the	O
datapoints	O
represents	O
their	O
weight	O
.	O
decision	B
boundary	I
is	O
in	O
yellow	O
.	O
(	O
a	O
)	O
after	O
1	O
(	O
c	O
)	O
after	O
120	O
rounds	O
.	O
figure	O
generated	O
by	O
boostingdemo	O
,	O
written	O
by	O
richard	O
round	O
.	O
stapenhurst	O
.	O
(	O
b	O
)	O
after	O
3	O
rounds	O
.	O
where	O
rim	O
(	O
cid:2	O
)	O
yi	O
−	O
fm−1	O
(	O
xi	O
)	O
is	O
the	O
current	O
residual	B
,	O
and	O
we	O
have	O
set	O
β	O
=	O
1	O
without	O
loss	B
of	O
generality	O
.	O
hence	O
we	O
can	O
ﬁnd	O
the	O
new	O
basis	O
function	O
by	O
using	O
the	O
weak	B
learner	I
to	O
predict	O
rm	O
.	O
this	O
is	O
called	O
l2boosting	O
,	O
or	O
least	B
squares	I
boosting	I
(	O
buhlmann	O
and	O
yu	O
2003	O
)	O
.	O
in	O
section	O
16.4.6	O
,	O
we	O
will	O
see	O
that	O
this	O
method	O
,	O
with	O
a	O
suitable	O
choice	O
of	O
weak	B
learner	I
,	O
can	O
be	O
made	O
to	O
give	O
the	O
same	O
results	O
as	O
lars	O
,	O
which	O
can	O
be	O
used	O
to	O
perform	O
variable	O
selection	O
(	O
see	O
section	O
13.4.2	O
)	O
.	O
16.4.3	O
adaboost	O
n	O
(	O
cid:4	O
)	O
i=1	O
consider	O
a	O
binary	B
classiﬁcation	I
problem	O
with	O
exponential	B
loss	I
.	O
at	O
step	O
m	O
we	O
have	O
to	O
minimize	O
lm	O
(	O
φ	O
)	O
=	O
(	O
16.37	O
)	O
where	O
wi	O
,	O
m	O
(	O
cid:2	O
)	O
exp	O
(	O
−˜yifm−1	O
(	O
xi	O
)	O
)	O
is	O
a	O
weight	O
applied	O
to	O
datacase	O
i	O
,	O
and	O
˜yi	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
.	O
we	O
can	O
rewrite	O
this	O
objective	O
as	O
follows	O
:	O
wi	O
,	O
m	O
+	O
eβ	O
exp	O
[	O
−˜yi	O
(	O
fm−1	O
(	O
xi	O
)	O
+	O
βφ	O
(	O
xi	O
)	O
)	O
]	O
=	O
(	O
cid:4	O
)	O
wi	O
,	O
m	O
exp	O
(	O
−β	O
˜yiφ	O
(	O
xi	O
)	O
)	O
lm	O
=	O
e−β	O
(	O
cid:4	O
)	O
(	O
16.38	O
)	O
n	O
(	O
cid:4	O
)	O
i=1	O
n	O
(	O
cid:4	O
)	O
βm	O
=	O
1	O
2	O
log	O
1	O
−	O
errm	O
errm	O
˜yi=φ	O
(	O
xi	O
)	O
n	O
(	O
cid:4	O
)	O
wi	O
,	O
m	O
˜yi	O
(	O
cid:5	O
)	O
=φ	O
(	O
xi	O
)	O
=	O
(	O
eβ	O
−	O
e−β	O
)	O
wi	O
,	O
mi	O
(	O
˜yi	O
(	O
cid:4	O
)	O
=	O
φ	O
(	O
xi	O
)	O
)	O
+	O
e−β	O
i=1	O
i=1	O
consequently	O
the	O
optimal	O
function	O
to	O
add	O
is	O
φm	O
=	O
argmin	O
φ	O
wi	O
,	O
mi	O
(	O
˜yi	O
(	O
cid:4	O
)	O
=	O
φ	O
(	O
xi	O
)	O
)	O
wi	O
,	O
m	O
(	O
16.39	O
)	O
(	O
16.40	O
)	O
this	O
can	O
be	O
found	O
by	O
applying	O
the	O
weak	B
learner	I
to	O
a	O
weighted	O
version	O
of	O
the	O
dataset	O
,	O
with	O
weights	O
wi	O
,	O
m	O
.	O
subsituting	O
φm	O
into	O
lm	O
and	O
solving	O
for	O
β	O
we	O
ﬁnd	O
(	O
16.41	O
)	O
16.4.	O
boosting	B
where	O
errm	O
=	O
(	O
cid:7	O
)	O
n	O
(	O
cid:7	O
)	O
n	O
i=1	O
wii	O
(	O
˜yi	O
(	O
cid:4	O
)	O
=	O
φm	O
(	O
xi	O
)	O
)	O
i=1	O
wi	O
,	O
m	O
the	O
overall	O
update	O
becomes	O
fm	O
(	O
x	O
)	O
=	O
fm−1	O
(	O
x	O
)	O
+β	O
mφm	O
(	O
x	O
)	O
with	O
this	O
,	O
the	O
weights	O
at	O
the	O
next	O
iteration	O
become	O
wi	O
,	O
m+1	O
=	O
wi	O
,	O
me−βm	O
˜yiφm	O
(	O
xi	O
)	O
=	O
wi	O
,	O
meβm	O
(	O
2i	O
(	O
˜yi	O
(	O
cid:5	O
)	O
=φm	O
(	O
xi	O
)	O
)	O
−1	O
)	O
=	O
wi	O
,	O
me2βm	O
i	O
(	O
˜yi	O
(	O
cid:5	O
)	O
=φm	O
(	O
xi	O
)	O
)	O
e−βm	O
559	O
(	O
16.42	O
)	O
(	O
16.43	O
)	O
(	O
16.44	O
)	O
(	O
16.45	O
)	O
(	O
16.46	O
)	O
where	O
we	O
exploited	O
the	O
fact	O
that	O
−˜yiφm	O
(	O
xi	O
)	O
=−1	O
if	O
˜yi	O
=	O
φm	O
(	O
xi	O
)	O
and	O
−˜yiφm	O
(	O
xi	O
)	O
=	O
+1	O
otherwise	O
.	O
since	O
e−βm	O
will	O
cancel	O
out	O
in	O
the	O
normalization	O
step	O
,	O
we	O
can	O
drop	O
it	O
.	O
the	O
result	O
is	O
the	O
algorithm	O
shown	O
in	O
algorithm	O
7	O
,	O
known	O
adaboost.m1.4	O
an	O
example	O
of	O
this	O
algorithm	O
in	O
action	B
,	O
using	O
decision	B
stumps	O
as	O
the	O
weak	B
learner	I
,	O
is	O
given	O
in	O
figure	O
16.10.	O
we	O
see	O
that	O
after	O
many	O
iterations	O
,	O
we	O
can	O
“	O
carve	O
out	O
”	O
a	O
complex	O
decision	B
boundary	I
.	O
what	O
is	O
rather	O
surprising	O
is	O
that	O
adaboost	O
is	O
very	O
slow	O
to	O
overﬁt	B
,	O
as	O
is	O
apparent	O
in	O
figure	O
16.8.	O
see	O
section	O
16.4.8	O
for	O
a	O
discussion	O
of	O
this	O
point	O
.	O
algorithm	O
16.2	O
:	O
adaboost.m1	O
,	O
for	O
binary	B
classiﬁcation	I
with	O
exponential	B
loss	I
1	O
wi	O
=	O
1/n	O
;	O
2	O
for	O
m	O
=	O
1	O
:	O
m	O
do	O
3	O
4	O
5	O
6	O
(	O
cid:2	O
)	O
n	O
fit	O
a	O
classiﬁer	O
φm	O
(	O
x	O
)	O
to	O
the	O
training	B
set	I
using	O
weights	O
w	O
;	O
i=1	O
wi	O
,	O
m	O
i	O
(	O
˜yi	O
(	O
cid:5	O
)	O
=φm	O
(	O
xi	O
)	O
)	O
compute	O
errm	O
=	O
compute	O
αm	O
=	O
log	O
[	O
(	O
1	O
−	O
errm	O
)	O
/errm	O
]	O
;	O
(	O
cid:16	O
)	O
set	O
wi	O
←	O
wi	O
exp	O
[	O
αmi	O
(	O
˜yi	O
(	O
cid:4	O
)	O
=	O
φm	O
(	O
xi	O
)	O
)	O
]	O
;	O
;	O
(	O
cid:15	O
)	O
(	O
cid:7	O
)	O
m	O
m=1	O
αmφm	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
n	O
i=1	O
wi	O
,	O
m	O
)	O
;	O
7	O
return	O
f	O
(	O
x	O
)	O
=	O
sgn	O
16.4.4	O
logitboost	O
the	O
trouble	O
with	O
exponential	B
loss	I
is	O
that	O
it	O
puts	O
a	O
lot	O
of	O
weight	O
on	O
misclassiﬁed	O
examples	O
,	O
as	O
is	O
apparent	O
from	O
the	O
exponential	O
blowup	O
on	O
the	O
left	O
hand	O
side	O
of	O
figure	O
16.9.	O
this	O
makes	O
the	O
method	O
very	O
sensitive	O
to	O
outliers	B
(	O
mislabeled	O
examples	O
)	O
.	O
in	O
addition	O
,	O
e−˜yf	O
is	O
not	O
the	O
logarithm	O
of	O
any	O
pmf	B
for	O
binary	O
variables	O
˜y	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
;	O
consequently	O
we	O
can	O
not	O
recover	O
probability	O
estimates	O
from	O
f	O
(	O
x	O
)	O
.	O
4.	O
in	O
(	O
friedman	O
et	O
al	O
.	O
2000	O
)	O
,	O
this	O
is	O
called	O
discrete	B
adaboost	O
,	O
since	O
it	O
assumes	O
that	O
the	O
base	O
classiﬁer	O
φm	O
returns	O
a	O
binary	O
class	O
label	B
.	O
if	O
φm	O
returns	O
a	O
probability	O
instead	O
,	O
a	O
modiﬁed	O
algorithm	O
,	O
known	O
as	O
real	O
adaboost	O
,	O
can	O
be	O
used	O
.	O
see	O
(	O
friedman	O
et	O
al	O
.	O
2000	O
)	O
for	O
details	O
.	O
560	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
a	O
natural	O
alternative	O
is	O
to	O
use	O
logloss	O
instead	O
.	O
this	O
only	O
punishes	O
mistakes	O
linearly	O
,	O
as	O
is	O
clear	O
from	O
figure	O
16.9.	O
furthermore	O
,	O
it	O
means	O
that	O
we	O
will	O
be	O
able	O
to	O
extract	O
probabilities	O
from	O
the	O
ﬁnal	O
learned	O
function	O
,	O
using	O
p	O
(	O
y	O
=	O
1|x	O
)	O
=	O
1	O
1	O
+	O
e−2f	O
(	O
x	O
)	O
ef	O
(	O
x	O
)	O
e−f	O
(	O
x	O
)	O
+	O
ef	O
(	O
x	O
)	O
=	O
n	O
(	O
cid:4	O
)	O
the	O
goal	O
is	O
to	O
minimze	O
the	O
expected	O
log-loss	O
,	O
given	O
by	O
lm	O
(	O
φ	O
)	O
=	O
log	O
[	O
1	O
+	O
exp	O
(	O
−2˜yi	O
(	O
fm−1	O
(	O
x	O
)	O
+φ	O
(	O
xi	O
)	O
)	O
)	O
]	O
(	O
16.47	O
)	O
(	O
16.48	O
)	O
i=1	O
by	O
performing	O
a	O
newton	O
upate	O
on	O
this	O
objective	O
(	O
similar	B
to	O
irls	O
)	O
,	O
one	O
can	O
derive	O
the	O
algorithm	O
shown	O
in	O
algorithm	O
8.	O
this	O
is	O
known	O
as	O
logitboost	O
(	O
friedman	O
et	O
al	O
.	O
2000	O
)	O
.	O
it	O
can	O
be	O
generalized	O
to	O
the	O
multi-class	O
setting	O
,	O
as	O
explained	O
in	O
(	O
friedman	O
et	O
al	O
.	O
2000	O
)	O
.	O
algorithm	O
16.3	O
:	O
logitboost	O
,	O
for	O
binary	B
classiﬁcation	I
with	O
log-loss	B
1	O
wi	O
=	O
1/n	O
,	O
πi	O
=	O
1/2	O
;	O
2	O
for	O
m	O
=	O
1	O
:	O
m	O
do	O
3	O
i	O
−πi	O
y∗	O
(	O
cid:7	O
)	O
n	O
compute	O
the	O
working	B
response	I
zi	O
=	O
πi	O
(	O
1−πi	O
)	O
;	O
compute	O
the	O
weights	O
wi	O
=	O
πi	O
(	O
1	O
−	O
πi	O
)	O
;	O
i=1	O
wi	O
(	O
zi	O
−	O
φ	O
(	O
xi	O
)	O
)	O
2	O
;	O
φm	O
=	O
argminφ	O
update	O
f	O
(	O
x	O
)	O
←	O
f	O
(	O
x	O
)	O
+	O
1	O
(	O
cid:15	O
)	O
(	O
cid:7	O
)	O
m	O
compute	O
πi	O
=	O
1/	O
(	O
1	O
+	O
exp	O
(	O
−2f	O
(	O
xi	O
)	O
)	O
)	O
;	O
2	O
φm	O
(	O
x	O
)	O
;	O
(	O
cid:16	O
)	O
8	O
return	O
f	O
(	O
x	O
)	O
=	O
sgn	O
m=1	O
φm	O
(	O
x	O
)	O
;	O
4	O
5	O
6	O
7	O
16.4.5	O
boosting	B
as	O
functional	B
gradient	I
descent	I
rather	O
than	O
deriving	O
new	O
versions	O
of	O
boosting	B
for	O
every	O
different	O
loss	B
function	I
,	O
it	O
is	O
possible	O
to	O
derive	O
a	O
generic	O
version	O
,	O
known	O
as	O
gradient	B
boosting	I
(	O
friedman	O
2001	O
;	O
mason	O
et	O
al	O
.	O
2000	O
)	O
.	O
to	O
explain	O
this	O
,	O
imagine	O
minimizing	O
ˆf	O
=	O
argmin	O
f	O
l	O
(	O
f	O
)	O
(	O
16.49	O
)	O
where	O
f	O
=	O
(	O
f	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
f	O
(	O
xn	O
)	O
)	O
are	O
the	O
“	O
parameters	O
”	O
.	O
we	O
will	O
solve	O
this	O
stagewise	O
,	O
using	O
gradient	B
descent	I
.	O
at	O
step	O
m	O
,	O
letg	O
m	O
be	O
the	O
gradient	O
of	O
l	O
(	O
f	O
)	O
evaluated	O
at	O
f	O
=	O
fm−1	O
:	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
gim	O
=	O
∂l	O
(	O
yi	O
,	O
f	O
(	O
xi	O
)	O
)	O
∂f	O
(	O
xi	O
)	O
f	O
=fm−1	O
(	O
16.50	O
)	O
gradients	O
of	O
some	O
common	O
loss	B
functions	O
are	O
given	O
in	O
table	O
16.1.	O
we	O
then	O
make	O
the	O
update	O
fm	O
=	O
fm−1	O
−	O
ρmgm	O
(	O
16.51	O
)	O
16.4.	O
boosting	B
where	O
ρm	O
is	O
the	O
step	O
length	O
,	O
chosen	O
by	O
ρm	O
=	O
argmin	O
ρ	O
l	O
(	O
fm−1	O
−	O
ρgm	O
)	O
561	O
(	O
16.52	O
)	O
this	O
is	O
called	O
functional	B
gradient	I
descent	I
.	O
in	O
its	O
current	O
form	O
,	O
this	O
is	O
not	O
much	O
use	O
,	O
since	O
it	O
only	O
optimizes	O
f	O
at	O
a	O
ﬁxed	O
set	O
of	O
n	O
points	O
,	O
so	O
we	O
do	O
not	O
learn	O
a	O
function	O
that	O
can	O
generalize	B
.	O
however	O
,	O
we	O
can	O
modify	O
the	O
algorithm	O
by	O
ﬁtting	O
a	O
weak	B
learner	I
to	O
approximate	O
the	O
negative	O
gradient	O
signal	O
.	O
that	O
is	O
,	O
we	O
use	O
this	O
update	O
(	O
−gim	O
−	O
φ	O
(	O
xi	O
;	O
γ	O
)	O
)	O
2	O
(	O
16.53	O
)	O
n	O
(	O
cid:4	O
)	O
i=1	O
γm	O
=	O
argmin	O
γ	O
the	O
overall	O
algorithm	O
is	O
summarized	O
in	O
algorithm	O
6.	O
which	O
is	O
not	O
strictly	O
necessary	O
,	O
as	O
argued	O
in	O
(	O
buhlmann	O
and	O
hothorn	O
2007	O
)	O
.	O
)	O
(	O
we	O
have	O
omitted	O
the	O
line	B
search	I
step	O
,	O
(	O
cid:7	O
)	O
n	O
algorithm	O
16.4	O
:	O
gradient	B
boosting	I
1	O
initialize	O
f0	O
(	O
x	O
)	O
=	O
argminγ	O
2	O
for	O
m	O
=	O
1	O
:	O
m	O
do	O
i=1	O
l	O
(	O
yi	O
,	O
φ	O
(	O
xi	O
;	O
γ	O
)	O
)	O
;	O
compute	O
the	O
gradient	O
residual	O
using	O
rim	O
=	O
−	O
∂f	O
(	O
xi	O
)	O
use	O
the	O
weak	B
learner	I
to	O
compute	O
γm	O
which	O
minimizes	O
update	O
fm	O
(	O
x	O
)	O
=	O
fm−1	O
(	O
x	O
)	O
+νφ	O
(	O
x	O
;	O
γm	O
)	O
;	O
(	O
cid:15	O
)	O
3	O
4	O
∂l	O
(	O
yi	O
,	O
f	O
(	O
xi	O
)	O
)	O
(	O
cid:16	O
)	O
(	O
cid:7	O
)	O
n	O
i=1	O
(	O
rim	O
−	O
φ	O
(	O
xi	O
;	O
γm	O
)	O
)	O
2	O
;	O
f	O
(	O
xi	O
)	O
=fm−1	O
(	O
xi	O
)	O
;	O
5	O
6	O
return	O
f	O
(	O
x	O
)	O
=	O
fm	O
(	O
x	O
)	O
if	O
we	O
apply	O
this	O
algorithm	O
using	O
squared	B
loss	I
,	O
we	O
recover	O
l2boosting	O
.	O
if	O
we	O
apply	O
this	O
algorithm	O
to	O
log-loss	B
,	O
we	O
get	O
an	O
algorithm	O
known	O
as	O
binomialboost	O
(	O
buhlmann	O
and	O
hothorn	O
2007	O
)	O
.	O
the	O
advantage	O
of	O
this	O
over	O
logitboost	O
is	O
that	O
it	O
does	O
not	O
need	O
to	O
be	O
able	O
to	O
do	O
weighted	O
ﬁtting	O
:	O
it	O
just	O
applies	O
any	O
black-box	B
regression	O
model	O
to	O
the	O
gradient	O
vector	O
.	O
also	O
,	O
it	O
is	O
relatively	O
easy	O
to	O
extend	O
to	O
the	O
multi-class	O
case	O
(	O
see	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p387	O
)	O
)	O
.	O
we	O
can	O
also	O
apply	O
this	O
algorithm	O
to	O
other	O
loss	B
functions	O
,	O
such	O
as	O
the	O
huber	O
loss	B
(	O
section	O
7.4	O
)	O
,	O
which	O
is	O
more	O
robust	B
to	O
outliers	B
than	O
squared	B
error	I
loss	O
.	O
16.4.6	O
sparse	B
boosting	I
suppose	O
we	O
use	O
as	O
our	O
weak	B
learner	I
the	O
following	O
algorithm	O
:	O
search	O
over	O
all	O
possible	O
variables	O
j	O
=	O
1	O
:	O
d	O
,	O
and	O
pick	O
the	O
one	O
j	O
(	O
m	O
)	O
that	O
best	O
predicts	O
the	O
residual	B
vector	O
:	O
j	O
(	O
m	O
)	O
=	O
argmin	O
(	O
rim	O
−	O
ˆβjmxij	O
)	O
2	O
n	O
(	O
cid:4	O
)	O
i=1	O
j	O
(	O
cid:7	O
)	O
n	O
(	O
cid:7	O
)	O
n	O
i=1	O
xijrim	O
i=1	O
x2	O
ij	O
ˆβjm	O
=	O
φm	O
(	O
x	O
)	O
=	O
ˆβj	O
(	O
m	O
)	O
,	O
m	O
xj	O
(	O
m	O
)	O
(	O
16.54	O
)	O
(	O
16.55	O
)	O
(	O
16.56	O
)	O
562	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
this	O
method	O
,	O
which	O
is	O
known	O
as	O
sparse	B
boosting	I
(	O
buhlmann	O
and	O
yu	O
2006	O
)	O
,	O
is	O
identical	O
to	O
the	O
matching	B
pursuit	I
algorithm	O
discussed	O
in	O
section	O
13.2.3.1.	O
it	O
is	O
clear	O
that	O
this	O
will	O
result	O
in	O
a	O
sparse	B
estimate	O
,	O
at	O
least	O
if	O
m	O
is	O
small	O
.	O
to	O
see	O
this	O
,	O
let	O
us	O
rewrite	O
the	O
update	O
as	O
follows	O
:	O
βm	O
:	O
=	O
βm−1	O
+	O
ν	O
(	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
,	O
ˆβj	O
(	O
m	O
)	O
,	O
m	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
0	O
)	O
(	O
16.57	O
)	O
where	O
the	O
non-zero	O
entry	O
occurs	O
in	O
location	O
j	O
(	O
m	O
)	O
.	O
this	O
is	O
known	O
as	O
forward	B
stagewise	I
linear	I
regression	I
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p608	O
)	O
,	O
which	O
becomes	O
equivalent	O
to	O
the	O
lar	O
algorithm	O
discussed	O
in	O
section	O
13.4.2	O
as	O
ν	O
→	O
0.	O
increasing	O
the	O
number	O
of	O
steps	O
m	O
in	O
boosting	B
is	O
analogous	O
to	O
decreasing	O
the	O
regularization	B
penalty	O
λ.	O
if	O
we	O
modify	O
boosting	B
to	O
allow	O
some	O
variable	O
deletion	O
steps	O
(	O
zhao	O
and	O
yu	O
2007	O
)	O
,	O
we	O
can	O
make	O
it	O
equivalent	O
to	O
the	O
lars	O
algorithm	O
,	O
which	O
computes	O
the	O
full	B
regularization	O
path	B
for	O
the	O
lasso	B
problem	O
.	O
the	O
same	O
algorithm	O
can	O
be	O
used	O
for	O
sparse	B
logistic	O
regression	B
,	O
by	O
simply	O
modifying	O
the	O
residual	B
to	O
be	O
the	O
appropriate	O
negative	O
gradient	O
.	O
now	O
consider	O
a	O
weak	B
learner	I
that	O
is	O
similar	B
to	O
the	O
above	O
,	O
except	O
it	O
uses	O
a	O
smoothing	O
spline	O
instead	O
of	O
linear	B
regression	I
when	O
mapping	O
from	O
xj	O
to	O
the	O
residual	B
.	O
the	O
result	O
is	O
a	O
sparse	B
generalized	O
additive	O
model	O
(	O
see	O
section	O
16.3	O
)	O
.	O
it	O
can	O
obviously	O
be	O
extended	O
to	O
pick	O
pairs	O
of	O
variables	O
at	O
a	O
time	O
.	O
the	O
resulting	O
method	O
often	O
works	O
much	O
better	O
than	O
mars	O
(	O
buhlmann	O
and	O
yu	O
2006	O
)	O
.	O
16.4.7	O
multivariate	O
adaptive	O
regression	O
trees	O
(	O
mart	O
)	O
it	O
is	O
quite	O
common	O
to	O
use	O
cart	O
models	O
as	O
weak	O
learners	O
.	O
it	O
is	O
usually	O
advisable	O
to	O
use	O
a	O
shallow	O
tree	O
,	O
so	O
that	O
the	O
variance	B
is	O
low	O
.	O
even	O
though	O
the	O
bias	B
will	O
be	O
high	O
(	O
since	O
a	O
shallow	O
tree	O
is	O
likely	O
to	O
be	O
far	O
from	O
the	O
“	O
truth	O
”	O
)	O
,	O
this	O
will	O
compensated	O
for	O
in	O
subsequent	O
rounds	O
of	O
boosting	B
.	O
the	O
height	O
of	O
the	O
tree	B
is	O
an	O
additional	O
tuning	O
parameter	B
(	O
in	O
addition	O
to	O
m	O
,	O
the	O
number	O
of	O
rounds	O
of	O
boosting	B
,	O
and	O
ν	O
,	O
the	O
shrinkage	B
factor	I
)	O
.	O
suppose	O
we	O
restrict	O
to	O
trees	O
with	O
j	O
leaves	B
.	O
if	O
j	O
=	O
2	O
,	O
we	O
get	O
a	O
stump	O
,	O
which	O
can	O
only	O
split	O
on	O
a	O
single	O
variable	O
.	O
if	O
j	O
=	O
3	O
,	O
we	O
allow	O
for	O
in	O
general	O
,	O
it	O
is	O
recommended	O
(	O
e.g.	O
,	O
in	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p363	O
)	O
two-variable	O
interactions	O
,	O
etc	O
.	O
and	O
(	O
caruana	O
and	O
niculescu-mizil	O
2006	O
)	O
)	O
to	O
use	O
j	O
≈	O
6.	O
if	O
we	O
combine	O
the	O
gradient	B
boosting	I
algorithm	O
with	O
(	O
shallow	O
)	O
regression	B
trees	O
,	O
we	O
get	O
a	O
model	O
known	O
as	O
mart	O
,	O
which	O
stands	O
for	O
“	O
multivariate	O
adaptive	O
regression	O
trees	O
”	O
.	O
this	O
actually	O
includes	O
a	O
slight	O
reﬁnement	O
to	O
the	O
basic	O
gradient	O
boosting	B
algorithm	O
:	O
after	O
ﬁtting	O
a	O
regression	B
tree	O
to	O
the	O
residual	B
(	O
negative	O
gradient	O
)	O
,	O
we	O
re-estimate	O
the	O
parameters	O
at	O
the	O
leaves	B
of	O
the	O
tree	B
to	O
minimize	O
the	O
loss	B
:	O
(	O
cid:4	O
)	O
γjm	O
=	O
argmin	O
γ	O
xi∈rjm	O
l	O
(	O
yi	O
,	O
fm−1	O
(	O
xi	O
)	O
+γ	O
)	O
(	O
16.58	O
)	O
where	O
rjm	O
is	O
the	O
region	O
for	O
leaf	B
j	O
in	O
the	O
m	O
’	O
th	O
tree	B
,	O
and	O
γjm	O
is	O
the	O
corresponding	O
parameter	B
(	O
the	O
mean	B
response	O
of	O
y	O
for	O
regression	B
problems	O
,	O
or	O
the	O
most	O
probable	O
class	O
label	O
for	O
classiﬁcation	B
problems	O
)	O
.	O
16.4.8	O
why	O
does	O
boosting	B
work	O
so	O
well	O
?	O
we	O
have	O
seen	O
that	O
boosting	B
works	O
very	O
well	O
,	O
especially	O
for	O
classiﬁers	O
.	O
there	O
are	O
two	O
main	O
reasons	O
for	O
this	O
.	O
first	O
,	O
it	O
can	O
be	O
seen	O
as	O
a	O
form	O
of	O
(	O
cid:2	O
)	O
1	O
regularization	B
,	O
which	O
is	O
known	O
to	O
help	O
16.5.	O
feedforward	O
neural	O
networks	O
(	O
multilayer	O
perceptrons	O
)	O
563	O
prevent	O
overﬁtting	B
by	O
eliminating	O
“	O
irrelevant	O
”	O
features	B
.	O
to	O
see	O
this	O
,	O
imagine	O
pre-computing	O
all	O
possible	O
weak-learners	O
,	O
and	O
deﬁning	O
a	O
feature	O
vector	O
of	O
the	O
form	O
φ	O
(	O
x	O
)	O
=	O
[	O
φ1	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
φk	O
(	O
x	O
)	O
]	O
.	O
we	O
could	O
use	O
(	O
cid:2	O
)	O
1	O
regularization	B
to	O
select	O
a	O
subset	O
of	O
these	O
.	O
alternatively	O
we	O
can	O
use	O
boosting	B
,	O
where	O
at	O
each	O
step	O
,	O
the	O
weak	B
learner	I
creates	O
a	O
new	O
φk	O
on	O
the	O
ﬂy	O
.	O
it	O
is	O
possible	O
to	O
combine	O
boosting	B
and	O
(	O
cid:2	O
)	O
1	O
regularization	B
,	O
to	O
get	O
an	O
algorithm	O
known	O
as	O
l1-adaboost	O
(	O
duchi	O
and	O
singer	O
2009	O
)	O
.	O
essentially	O
this	O
method	O
greedily	O
adds	O
the	O
best	O
features	B
(	O
weak	O
learners	O
)	O
using	O
boosting	B
,	O
and	O
then	O
prunes	O
off	O
irrelevant	O
ones	O
using	O
(	O
cid:2	O
)	O
1	O
regularization	B
.	O
another	O
explanation	O
has	O
to	O
do	O
with	O
the	O
concept	B
of	O
margin	B
,	O
which	O
we	O
introduced	O
in	O
sec-	O
tion	O
14.5.2.2	O
.	O
(	O
schapire	O
et	O
al	O
.	O
1998	O
;	O
ratsch	O
et	O
al	O
.	O
2001	O
)	O
proved	O
that	O
adaboost	O
maximizes	O
the	O
margin	B
on	O
the	O
training	O
data	O
.	O
(	O
rosset	O
et	O
al	O
.	O
2004	O
)	O
generalized	O
this	O
to	O
other	O
loss	B
functions	O
,	O
such	O
as	O
log-loss	B
.	O
16.4.9	O
a	O
bayesian	O
view	O
m	O
(	O
cid:4	O
)	O
so	O
far	O
,	O
our	O
presentation	O
of	O
boosting	B
has	O
been	O
very	O
frequentist	B
,	O
since	O
it	O
has	O
focussed	O
on	O
greedily	O
minimizing	O
loss	B
functions	O
.	O
a	O
likelihood	B
interpretation	O
of	O
the	O
algorithm	O
was	O
given	O
in	O
(	O
neal	O
and	O
mackay	O
1998	O
;	O
meek	O
et	O
al	O
.	O
2002	O
)	O
.	O
the	O
idea	O
is	O
to	O
consider	O
a	O
mixture	B
of	I
experts	I
model	O
of	O
the	O
form	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
πmp	O
(	O
y|x	O
,	O
γm	O
)	O
m=1	O
(	O
16.59	O
)	O
where	O
each	O
expert	O
p	O
(	O
y|x	O
,	O
γm	O
)	O
is	O
like	O
a	O
weak	B
learner	I
.	O
we	O
usually	O
ﬁt	O
all	O
m	O
experts	O
at	O
once	O
using	O
em	O
,	O
but	O
we	O
can	O
imagine	O
a	O
sequential	B
scheme	O
,	O
whereby	O
we	O
only	O
update	O
the	O
parameters	O
in	O
the	O
e	O
step	O
,	O
the	O
posterior	O
responsibilities	O
will	O
reﬂect	O
how	O
well	O
the	O
for	O
one	O
expert	O
at	O
a	O
time	O
.	O
existing	O
experts	O
explain	O
a	O
given	O
data	O
point	O
;	O
if	O
this	O
is	O
a	O
poor	O
ﬁt	O
,	O
these	O
data	O
points	O
will	O
have	O
more	O
inﬂuence	O
on	O
the	O
next	O
expert	O
that	O
is	O
ﬁtted	O
.	O
(	O
this	O
view	O
naturally	O
suggest	O
a	O
way	O
to	O
use	O
a	O
boosting-like	O
algorithm	O
for	O
unsupervised	B
learning	I
:	O
we	O
simply	O
sequentially	O
ﬁt	O
mixture	B
models	O
,	O
instead	O
of	O
mixtures	O
of	O
experts	O
.	O
)	O
notice	O
that	O
this	O
is	O
a	O
rather	O
“	O
broken	O
”	O
mle	O
procedure	O
,	O
since	O
it	O
never	O
goes	O
back	O
to	O
update	O
the	O
parameters	O
of	O
an	O
old	O
expert	O
.	O
similarly	O
,	O
if	O
boosting	B
ever	O
wants	O
to	O
change	O
the	O
weight	O
assigned	O
to	O
a	O
weak	B
learner	I
,	O
the	O
only	O
way	O
to	O
do	O
this	O
is	O
to	O
add	O
the	O
weak	B
learner	I
again	O
with	O
a	O
new	O
weight	O
.	O
this	O
can	O
result	O
in	O
unnecessarily	O
large	O
models	O
.	O
by	O
contrast	O
,	O
the	O
bart	O
model	O
(	O
chipman	O
et	O
al	O
.	O
2006	O
,	O
2010	O
)	O
uses	O
a	O
bayesian	O
version	O
of	O
backﬁtting	B
to	O
ﬁt	O
a	O
small	O
sum	O
of	O
weak	O
learners	O
(	O
typically	O
trees	O
)	O
.	O
16.5	O
feedforward	O
neural	O
networks	O
(	O
multilayer	O
perceptrons	O
)	O
a	O
feedforward	B
neural	I
network	I
,	O
aka	O
multi-layer	B
perceptron	I
(	O
mlp	O
)	O
,	O
is	O
a	O
series	O
of	O
logistic	B
regression	I
models	O
stacked	O
on	O
top	O
of	O
each	O
other	O
,	O
with	O
the	O
ﬁnal	O
layer	O
being	O
either	O
another	O
logistic	B
regression	I
or	O
a	O
linear	B
regression	I
model	O
,	O
depending	O
on	O
whether	O
we	O
are	O
solving	O
a	O
classiﬁcation	B
or	O
regression	B
problem	O
.	O
for	O
example	O
,	O
if	O
we	O
have	O
two	O
layers	O
,	O
and	O
we	O
are	O
solving	O
a	O
regression	B
problem	O
,	O
the	O
model	O
has	O
the	O
form	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=n	O
(	O
y|wt	O
z	O
(	O
x	O
)	O
,	O
σ2	O
)	O
z	O
(	O
x	O
)	O
=g	O
(	O
16.60	O
)	O
(	O
16.61	O
)	O
where	O
g	O
is	O
a	O
non-linear	O
activation	B
or	O
transfer	B
function	I
(	O
commonly	O
the	O
logistic	B
function	O
)	O
,	O
z	O
(	O
x	O
)	O
=φ	O
(	O
x	O
,	O
v	O
)	O
is	O
called	O
the	O
hidden	B
layer	I
(	O
a	O
deterministic	O
function	O
of	O
the	O
input	O
)	O
,	O
h	O
is	O
the	O
1	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
g	O
(	O
vt	O
(	O
vx	O
)	O
=	O
[	O
g	O
(	O
vt	O
h	O
x	O
)	O
]	O
564	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
xnd	O
xni	O
xn1	O
vij	O
znh	O
...	O
znj	O
...	O
zn1	O
wjk	O
ync	O
ynk	O
yn1	O
figure	O
16.11	O
a	O
neural	B
network	I
with	O
one	O
hidden	B
layer	I
.	O
number	O
of	O
hidden	B
units	I
,	O
v	O
is	O
the	O
weight	O
matrix	O
from	O
the	O
inputs	O
to	O
the	O
hidden	B
nodes	I
,	O
and	O
w	O
is	O
the	O
weight	B
vector	I
from	O
the	O
hidden	B
nodes	I
to	O
the	O
output	O
.	O
it	O
is	O
important	O
that	O
g	O
be	O
non-	O
linear	O
,	O
otherwise	O
the	O
whole	O
model	O
collapses	O
into	O
a	O
large	O
linear	O
regression	B
model	O
of	O
the	O
form	O
y	O
=	O
wt	O
(	O
vx	O
)	O
.	O
one	O
can	O
show	O
that	O
an	O
mlp	O
is	O
a	O
universal	B
approximator	I
,	O
meaning	O
it	O
can	O
model	O
any	O
suitably	O
smooth	O
function	O
,	O
given	O
enough	O
hidden	B
units	I
,	O
to	O
any	O
desired	O
level	O
of	O
accuracy	O
(	O
hornik	O
1991	O
)	O
.	O
to	O
handle	O
binary	B
classiﬁcation	I
,	O
we	O
pass	O
the	O
output	O
through	O
a	O
sigmoid	B
,	O
as	O
in	O
a	O
glm	O
:	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
ber	O
(	O
y|sigm	O
(	O
wt	O
z	O
(	O
x	O
)	O
)	O
)	O
(	O
16.62	O
)	O
we	O
can	O
easily	O
extend	O
the	O
mlp	O
to	O
predict	O
multiple	O
outputs	O
.	O
for	O
example	O
,	O
in	O
the	O
regression	B
case	O
,	O
we	O
have	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
n	O
(	O
y|w	O
φ	O
(	O
x	O
,	O
v	O
)	O
,	O
σ2i	O
)	O
(	O
16.63	O
)	O
see	O
figure	O
16.11	O
for	O
an	O
illustration	O
.	O
if	O
we	O
add	O
mutual	B
inhibition	I
arcs	O
between	O
the	O
output	O
units	O
,	O
ensuring	O
that	O
only	O
one	O
of	O
them	O
turns	O
on	O
,	O
we	O
can	O
enforce	O
a	O
sum-to-one	O
constraint	O
,	O
which	O
can	O
be	O
used	O
for	O
multi-class	O
classiﬁcation	O
.	O
the	O
resulting	O
model	O
has	O
the	O
form	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
cat	O
(	O
y|s	O
(	O
wz	O
(	O
x	O
)	O
)	O
16.5.1	O
convolutional	O
neural	O
networks	O
(	O
16.64	O
)	O
the	O
purpose	O
of	O
the	O
hidden	B
units	I
is	O
to	O
learn	O
non-linear	O
combinations	O
of	O
the	O
original	O
inputs	O
;	O
this	O
is	O
called	O
feature	B
extraction	I
or	O
feature	B
construction	I
.	O
these	O
hidden	B
features	O
are	O
then	O
passed	O
as	O
input	O
to	O
the	O
ﬁnal	O
glm	O
.	O
this	O
approach	O
is	O
particularly	O
useful	O
for	O
problems	O
where	O
the	O
original	O
input	O
features	B
are	O
not	O
very	O
individually	O
informative	O
.	O
for	O
example	O
,	O
each	O
pixel	O
in	O
an	O
image	O
is	O
not	O
very	O
informative	O
;	O
it	O
is	O
the	O
combination	O
of	O
pixels	O
that	O
tells	O
us	O
what	O
objects	O
are	O
present	O
.	O
conversely	O
,	O
for	O
a	O
task	O
such	O
as	O
document	B
classiﬁcation	I
using	O
a	O
bag	B
of	I
words	I
representation	O
,	O
each	O
feature	O
(	O
word	O
count	O
)	O
is	O
informative	O
on	O
its	O
own	O
,	O
so	O
extracting	O
“	O
higher	O
order	O
”	O
features	B
is	O
less	O
important	O
.	O
not	O
suprisingly	O
,	O
then	O
,	O
much	O
of	O
the	O
work	O
in	O
neural	B
networks	I
has	O
been	O
motivated	O
by	O
visual	O
pattern	O
16.5.	O
feedforward	O
neural	O
networks	O
(	O
multilayer	O
perceptrons	O
)	O
565	O
source	O
:	O
http	O
:	O
//www.codep	O
figure	O
16.12	O
the	O
convolutional	B
neural	I
network	I
from	O
(	O
simard	O
et	O
al	O
.	O
2003	O
)	O
.	O
roject.com/kb/library/neuralnetrecognition.aspx	O
.	O
used	O
with	O
kind	O
permission	O
of	O
mike	O
o	O
’	O
neill	O
.	O
recognition	O
(	O
e.g.	O
,	O
(	O
lecun	O
et	O
al	O
.	O
1989	O
)	O
)	O
,	O
although	O
they	O
have	O
also	O
been	O
applied	O
to	O
other	O
types	O
of	O
data	O
,	O
including	O
text	O
(	O
e.g.	O
,	O
(	O
collobert	O
and	O
weston	O
2008	O
)	O
)	O
.	O
a	O
form	O
of	O
mlp	O
which	O
is	O
particularly	O
well	O
suited	O
to	O
1d	O
signals	O
like	O
speech	O
or	O
text	O
,	O
or	O
2d	O
signals	O
like	O
images	O
,	O
is	O
the	O
convolutional	B
neural	I
network	I
.	O
this	O
is	O
an	O
mlp	O
in	O
which	O
the	O
hidden	B
units	I
have	O
local	O
receptive	O
ﬁelds	O
(	O
as	O
in	O
the	O
primary	O
visual	O
cortex	O
)	O
,	O
and	O
in	O
which	O
the	O
weights	O
are	O
tied	B
or	O
shared	B
across	O
the	O
image	O
,	O
in	O
order	O
to	O
reduce	O
the	O
number	O
of	O
parameters	O
.	O
intuitively	O
,	O
the	O
effect	O
of	O
such	O
spatial	O
parameter	B
tying	I
is	O
that	O
any	O
useful	O
features	B
that	O
are	O
“	O
discovered	O
”	O
in	O
some	O
portion	O
of	O
the	O
image	O
can	O
be	O
re-used	O
everywhere	O
else	O
without	O
having	O
to	O
be	O
independently	O
learned	O
.	O
the	O
resulting	O
network	O
then	O
exhibits	O
translation	B
invariance	I
,	O
meaning	O
it	O
can	O
classify	O
patterns	O
no	O
matter	O
where	O
they	O
occur	O
inside	O
the	O
input	O
image	O
.	O
figure	O
16.12	O
gives	O
an	O
example	O
of	O
a	O
convolutional	O
network	O
,	O
designed	O
by	O
simard	O
and	O
colleagues	O
(	O
simard	O
et	O
al	O
.	O
2003	O
)	O
,	O
with	O
5	O
layers	O
(	O
4	O
layers	O
of	O
adjustable	O
parameters	O
)	O
designed	O
to	O
classify	O
29×	O
29	O
gray-scale	O
images	O
of	O
handwritten	O
digits	O
from	O
the	O
mnist	O
dataset	O
(	O
see	O
section	O
1.2.1.3	O
)	O
.	O
in	O
layer	O
1	O
,	O
we	O
have	O
6	O
feature	B
maps	I
each	O
of	O
which	O
has	O
size	O
13	O
×	O
13.	O
each	O
hidden	B
node	O
in	O
one	O
of	O
these	O
feature	B
maps	I
is	O
computed	O
by	O
convolving	O
the	O
image	O
with	O
a	O
5×	O
5	O
weight	O
matrix	O
(	O
sometimes	O
called	O
a	O
kernel	B
)	O
,	O
adding	O
a	O
bias	B
,	O
and	O
then	O
passing	O
the	O
result	O
through	O
some	O
form	O
of	O
nonlinearity	O
.	O
there	O
are	O
therefore	O
13	O
×	O
13	O
×	O
6	O
=	O
1014	O
neurons	O
in	O
layer	O
1	O
,	O
and	O
(	O
5	O
×	O
5	O
+	O
1	O
)	O
×	O
6	O
=	O
156	O
weights	O
.	O
(	O
the	O
''	O
+1	O
''	O
is	O
for	O
the	O
bias	B
.	O
)	O
if	O
we	O
did	O
not	O
share	O
these	O
parameters	O
,	O
there	O
would	O
be	O
1014	O
×	O
26	O
=	O
26	O
,	O
364	O
weights	O
at	O
the	O
ﬁrst	O
layer	O
.	O
in	O
layer	O
2	O
,	O
we	O
have	O
50	O
feature	B
maps	I
,	O
each	O
of	O
which	O
is	O
obtained	O
by	O
convolving	O
each	O
feature	O
map	O
in	O
layer	O
1	O
with	O
a	O
5	O
×	O
5	O
weight	O
matrix	O
,	O
adding	O
them	O
up	O
,	O
adding	O
a	O
bias	B
,	O
and	O
passing	O
through	O
a	O
nonlinearity	O
.	O
there	O
are	O
therefore	O
5	O
×	O
5	O
×	O
50	O
=	O
1250	O
neurons	O
in	O
layer	O
2	O
,	O
(	O
5	O
×	O
5	O
+	O
1	O
)	O
×	O
6	O
×	O
50	O
=	O
7800	O
adjustable	O
weights	O
(	O
one	O
kernel	B
for	O
each	O
pair	O
of	O
feature	O
566	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
maps	O
in	O
layers	O
1	O
and	O
2	O
)	O
,	O
and	O
1250	O
×	O
26	O
=	O
32	O
,	O
500	O
connections	O
.	O
layer	O
3	O
is	O
fully	O
connected	O
to	O
layer	O
2	O
,	O
and	O
has	O
100	O
neurons	O
and	O
100	O
×	O
(	O
1250	O
+	O
1	O
)	O
=	O
125	O
,	O
100	O
weights	O
.	O
finally	O
,	O
layer	O
4	O
is	O
also	O
fully	O
connected	O
,	O
and	O
has	O
10	O
neurons	O
,	O
and	O
10	O
×	O
(	O
100	O
+	O
1	O
)	O
=	O
1010	O
weights	O
.	O
adding	O
the	O
above	O
numbers	O
,	O
there	O
are	O
a	O
total	O
of	O
3,215	O
neurons	O
,	O
134,066	O
adjustable	O
weights	O
,	O
and	O
184,974	O
connections	O
.	O
this	O
model	O
is	O
usually	O
trained	O
using	O
stochastic	B
gradient	I
descent	I
(	O
see	O
section	O
16.5.4	O
for	O
details	O
)	O
.	O
a	O
single	O
pass	O
over	O
the	O
data	O
set	O
is	O
called	O
an	O
epoch	B
.	O
when	O
mike	O
o	O
’	O
neill	O
did	O
these	O
experiments	O
in	O
2006	O
,	O
he	O
found	O
that	O
a	O
single	O
epoch	O
took	O
about	O
40	O
minutes	O
(	O
recall	B
that	O
there	O
are	O
60,000	O
training	O
examples	O
in	O
mnist	O
)	O
.	O
since	O
it	O
took	O
about	O
30	O
epochs	O
for	O
the	O
error	O
rate	O
to	O
converge	B
,	O
the	O
total	O
training	O
time	O
was	O
about	O
20	O
hours.5	O
using	O
this	O
technique	O
,	O
he	O
obtained	O
a	O
misclassiﬁcation	B
rate	I
on	O
the	O
10,000	O
test	O
cases	O
of	O
about	O
1.40	O
%	O
.	O
to	O
further	O
reduce	O
the	O
error	O
rate	O
,	O
a	O
standard	O
trick	O
is	O
to	O
expand	O
the	O
training	B
set	I
by	O
including	O
distorted	B
versions	O
of	O
the	O
original	O
data	O
,	O
to	O
encourage	O
the	O
network	O
to	O
be	O
invariant	B
to	O
small	O
changes	O
that	O
don	O
’	O
t	O
affect	O
the	O
identity	O
of	O
the	O
digit	O
.	O
these	O
can	O
be	O
created	O
by	O
applying	O
a	O
random	O
ﬂow	O
ﬁeld	O
to	O
shift	O
pixels	O
around	O
.	O
see	O
figure	O
16.13	O
for	O
some	O
examples	O
.	O
(	O
if	O
we	O
use	O
online	O
training	O
,	O
such	O
as	O
stochastic	B
gradient	I
descent	I
,	O
we	O
can	O
create	O
these	O
distortions	O
on	O
the	O
ﬂy	O
,	O
rather	O
than	O
having	O
to	O
store	O
them	O
.	O
)	O
using	O
this	O
technique	O
,	O
mike	O
o	O
’	O
neill	O
obtained	O
a	O
misclassiﬁcation	B
rate	I
on	O
the	O
10,000	O
test	O
cases	O
of	O
about	O
0.74	O
%	O
,	O
which	O
is	O
close	O
to	O
the	O
current	O
state	B
of	O
the	O
art.6	O
yann	O
le	O
cun	O
and	O
colleagues	O
(	O
lecun	O
et	O
al	O
.	O
1998	O
)	O
obtained	O
similar	B
performance	O
using	O
a	O
slightly	O
more	O
complicated	O
architecture	O
shown	O
in	O
figure	O
16.14.	O
this	O
model	O
is	O
known	O
as	O
lenet5	O
,	O
and	O
historically	O
it	O
came	O
before	O
the	O
model	O
in	O
figure	O
16.12.	O
there	O
are	O
two	O
main	O
differences	O
.	O
first	O
,	O
lenet5	O
has	O
a	O
subsampling	B
layer	O
between	O
each	O
convolutional	O
layer	O
,	O
which	O
either	O
averages	O
or	O
computes	O
the	O
max	O
over	O
each	O
small	O
window	O
in	O
the	O
previous	O
layer	O
,	O
in	O
order	O
to	O
reduce	O
the	O
size	O
,	O
and	O
to	O
obtain	O
a	O
small	O
amount	O
of	O
shift	O
invariance	O
.	O
the	O
convolution	O
and	O
sub-sampling	O
combination	O
was	O
inspired	O
by	O
hubel	O
and	O
wiesel	O
’	O
s	O
model	O
of	O
simple	O
and	O
complex	O
cells	O
in	O
the	O
visual	O
cortex	O
(	O
hubel	O
and	O
wiesel	O
1962	O
)	O
,	O
and	O
it	O
continues	O
to	O
be	O
popular	O
in	O
neurally-inspired	O
models	O
of	O
visual	O
object	O
recognition	O
(	O
riesenhuber	O
and	O
poggio	O
1999	O
)	O
.	O
a	O
similar	B
idea	O
ﬁrst	O
appeared	O
in	O
fukushima	O
’	O
s	O
neocognitron	B
(	O
fukushima	O
1975	O
)	O
,	O
though	O
no	O
globally	O
supervised	O
training	O
algorithm	O
was	O
available	O
at	O
that	O
time	O
.	O
the	O
second	O
difference	O
between	O
lenet5	O
and	O
the	O
simard	O
architecture	O
is	O
that	O
the	O
ﬁnal	O
layer	O
is	O
actually	O
an	O
rbf	O
network	O
rather	O
than	O
a	O
more	O
standard	O
sigmoidal	O
or	O
softmax	B
layer	O
.	O
this	O
model	O
gets	O
a	O
test	O
error	O
rate	B
of	O
about	O
0.95	O
%	O
when	O
trained	O
with	O
no	O
distortions	O
,	O
and	O
0.8	O
%	O
when	O
trained	O
with	O
distortions	O
.	O
figure	O
16.15	O
shows	O
all	O
82	O
errors	O
made	O
by	O
the	O
system	O
.	O
some	O
are	O
genuinely	O
ambiguous	O
,	O
but	O
several	O
are	O
errors	O
that	O
a	O
person	O
would	O
never	O
make	O
.	O
a	O
web-based	O
demo	O
of	O
the	O
lenet5	O
can	O
be	O
found	O
at	O
http	O
:	O
//yann.lecun.com/exdb/lenet/index.html	O
.	O
of	O
course	O
,	O
classifying	O
isolated	O
digits	O
is	O
of	O
limited	O
applicability	O
:	O
in	O
the	O
real	O
world	O
,	O
people	O
usually	O
write	O
strings	O
of	O
digits	O
or	O
other	O
letters	O
.	O
this	O
requires	O
both	O
segmentation	O
and	O
classiﬁcation	B
.	O
le	O
cun	O
and	O
colleagues	O
devised	O
a	O
way	O
to	O
combine	O
convolutional	O
neural	O
networks	O
with	O
a	O
model	O
similar	O
to	O
a	O
conditional	B
random	I
ﬁeld	I
(	O
described	O
in	O
section	O
19.6	O
)	O
to	O
solve	O
this	O
problem	O
.	O
the	O
system	O
was	O
eventually	O
deployed	O
by	O
the	O
us	O
postal	O
service	O
.	O
(	O
see	O
(	O
lecun	O
et	O
al	O
.	O
1998	O
)	O
for	O
a	O
more	O
detailed	O
account	O
of	O
the	O
system	O
,	O
which	O
remains	O
one	O
of	O
the	O
best	O
performing	O
systems	O
for	O
this	O
task	O
.	O
)	O
5.	O
implementation	O
details	O
:	O
mike	O
used	O
c++	O
code	O
and	O
a	O
variety	O
of	O
speedup	O
tricks	O
.	O
he	O
was	O
using	O
standard	O
2006	O
era	O
hardware	O
(	O
an	O
intel	O
pentium	O
4	O
hyperthreaded	O
processor	O
running	O
at	O
2.8ghz	O
)	O
.	O
see	O
http	O
:	O
//www.codeproject.com/kb/	O
library/neuralnetrecognition.aspx	O
for	O
details	O
.	O
6.	O
a	O
list	O
of	O
various	O
methods	O
,	O
along	O
with	O
their	O
misclassiﬁcation	O
rates	O
on	O
the	O
mnist	O
test	O
set	O
,	O
is	O
available	O
from	O
http	O
:	O
//yann.lecun.com/exdb/mnist/	O
.	O
error	O
rates	O
within	O
0.1–0.2	O
%	O
of	O
each	O
other	O
are	O
not	O
statistically	O
signiﬁcantly	O
different	O
.	O
16.5.	O
feedforward	O
neural	O
networks	O
(	O
multilayer	O
perceptrons	O
)	O
567	O
(	O
a	O
)	O
(	O
c	O
)	O
(	O
e	O
)	O
(	O
b	O
)	O
(	O
d	O
)	O
(	O
f	O
)	O
figure	O
16.13	O
several	O
synthetic	O
warpings	O
of	O
a	O
handwritten	O
digit	O
.	O
based	O
on	O
figure	O
5.14	O
of	O
(	O
bishop	O
2006a	O
)	O
.	O
figure	O
generated	O
by	O
elasticdistortionsdemo	O
,	O
written	O
by	O
kevin	O
swersky	O
.	O
input	O
32x32	O
c1	O
:	O
feature	B
maps	I
6	O
@	O
28x28	O
c3	O
:	O
f.	O
maps	O
16	O
@	O
10x10	O
s4	O
:	O
f.	O
maps	O
16	O
@	O
5x5	O
s2	O
:	O
f.	O
maps	O
6	O
@	O
14x14	O
c5	O
:	O
layer	O
120	O
f6	O
:	O
layer	O
84	O
output	O
10	O
convolutions	O
subsampling	B
convolutions	O
subsampling	B
full	O
connection	O
full	B
connection	O
gaussian	O
connections	O
figure	O
16.14	O
(	O
lecun	O
et	O
al	O
.	O
1998	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
yann	O
lecun	O
.	O
lenet5	O
,	O
a	O
convolutional	O
neural	O
net	O
for	O
classifying	O
handwritten	O
digits	O
.	O
source	O
:	O
figure	O
2	O
from	O
568	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
4−	O
>	O
6	O
3−	O
>	O
5	O
8−	O
>	O
2	O
2−	O
>	O
1	O
5−	O
>	O
3	O
4−	O
>	O
8	O
2−	O
>	O
8	O
3−	O
>	O
5	O
6−	O
>	O
5	O
7−	O
>	O
3	O
9−	O
>	O
4	O
8−	O
>	O
0	O
7−	O
>	O
8	O
5−	O
>	O
3	O
8−	O
>	O
7	O
0−	O
>	O
6	O
3−	O
>	O
7	O
2−	O
>	O
7	O
8−	O
>	O
3	O
9−	O
>	O
4	O
8−	O
>	O
2	O
5−	O
>	O
3	O
4−	O
>	O
8	O
3−	O
>	O
9	O
6−	O
>	O
0	O
9−	O
>	O
8	O
4−	O
>	O
9	O
6−	O
>	O
1	O
9−	O
>	O
4	O
9−	O
>	O
1	O
9−	O
>	O
4	O
2−	O
>	O
0	O
6−	O
>	O
1	O
3−	O
>	O
5	O
3−	O
>	O
2	O
9−	O
>	O
5	O
6−	O
>	O
0	O
6−	O
>	O
0	O
6−	O
>	O
0	O
6−	O
>	O
8	O
4−	O
>	O
6	O
7−	O
>	O
3	O
9−	O
>	O
4	O
4−	O
>	O
6	O
2−	O
>	O
7	O
9−	O
>	O
7	O
4−	O
>	O
3	O
9−	O
>	O
4	O
9−	O
>	O
4	O
9−	O
>	O
4	O
8−	O
>	O
7	O
4−	O
>	O
2	O
8−	O
>	O
4	O
3−	O
>	O
5	O
8−	O
>	O
4	O
6−	O
>	O
5	O
8−	O
>	O
5	O
3−	O
>	O
8	O
3−	O
>	O
8	O
9−	O
>	O
8	O
1−	O
>	O
5	O
9−	O
>	O
8	O
6−	O
>	O
3	O
0−	O
>	O
2	O
6−	O
>	O
5	O
9−	O
>	O
5	O
0−	O
>	O
7	O
1−	O
>	O
6	O
4−	O
>	O
9	O
2−	O
>	O
1	O
2−	O
>	O
8	O
8−	O
>	O
5	O
4−	O
>	O
9	O
7−	O
>	O
2	O
7−	O
>	O
2	O
6−	O
>	O
5	O
9−	O
>	O
7	O
6−	O
>	O
1	O
5−	O
>	O
6	O
5−	O
>	O
0	O
4−	O
>	O
9	O
2−	O
>	O
8	O
figure	O
16.15	O
these	O
are	O
the	O
82	O
errors	O
made	O
by	O
lenet5	O
on	O
the	O
10,000	O
test	O
cases	O
of	O
mnist	O
.	O
below	O
each	O
image	O
is	O
a	O
label	B
of	O
the	O
form	O
correct-label	O
→	O
estimated-label	O
.	O
source	O
:	O
figure	O
8	O
of	O
(	O
lecun	O
et	O
al	O
.	O
1998	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
yann	O
lecun	O
.	O
(	O
compare	O
to	O
figure	O
28.4	O
(	O
b	O
)	O
which	O
shows	O
the	O
results	O
of	O
a	O
deep	B
generative	O
model	O
.	O
)	O
16.5.2	O
other	O
kinds	O
of	O
neural	B
networks	I
other	O
network	O
topologies	O
are	O
possible	O
besides	O
the	O
ones	O
discussed	O
above	O
.	O
for	O
example	O
,	O
we	O
can	O
have	O
skip	B
arcs	I
that	O
go	O
directly	O
from	O
the	O
input	O
to	O
the	O
output	O
,	O
skipping	O
the	O
hidden	B
layer	I
;	O
we	O
can	O
have	O
sparse	B
connections	O
between	O
the	O
layers	O
;	O
etc	O
.	O
however	O
,	O
the	O
mlp	O
always	O
requires	O
that	O
the	O
weights	O
form	O
a	O
directed	B
acyclic	I
graph	I
.	O
if	O
we	O
allow	O
feedback	O
connections	O
,	O
the	O
model	O
is	O
known	O
as	O
a	O
recurrent	B
neural	I
network	I
;	O
this	O
deﬁnes	O
a	O
nonlinear	O
dynamical	O
system	O
,	O
but	O
does	O
not	O
have	O
a	O
simple	O
probabilistic	O
interpretation	O
.	O
such	O
rnn	O
models	O
are	O
currently	O
the	O
best	O
approach	O
for	O
language	B
modeling	I
(	O
i.e.	O
,	O
performing	O
word	O
prediction	O
in	O
natural	O
language	O
)	O
(	O
tomas	O
et	O
al	O
.	O
2011	O
)	O
,	O
signiﬁcantly	O
outperforming	O
the	O
standard	O
n-gram-based	O
methods	O
discussed	O
in	O
section	O
17.2.2.	O
if	O
we	O
allow	O
symmetric	B
connections	O
between	O
the	O
hidden	B
units	I
,	O
the	O
model	O
is	O
known	O
as	O
a	O
hop-	O
ﬁeld	O
network	O
or	O
associative	B
memory	I
;	O
its	O
probabilistic	O
counterpart	O
is	O
known	O
as	O
a	O
boltzmann	O
machine	O
(	O
see	O
section	O
27.7	O
)	O
and	O
can	O
be	O
used	O
for	O
unsupervised	B
learning	I
.	O
16.5.3	O
a	O
brief	O
history	O
of	O
the	O
ﬁeld	O
neural	B
networks	I
have	O
been	O
the	O
subject	O
of	O
great	O
interest	O
for	O
many	O
decades	O
,	O
due	O
to	O
the	O
desire	O
to	O
understand	O
the	O
brain	O
,	O
and	O
to	O
build	O
learning	B
machines	O
.	O
it	O
is	O
not	O
possible	O
to	O
review	O
the	O
entire	O
history	O
here	O
.	O
instead	O
,	O
we	O
just	O
give	O
a	O
few	O
“	O
edited	O
highlights	O
”	O
.	O
the	O
ﬁeld	O
is	O
generally	O
viewed	O
as	O
starting	O
with	O
mcculloch	O
and	O
pitts	O
(	O
mccullich	O
and	O
pitts	O
1943	O
)	O
,	O
who	O
devised	O
a	O
simple	O
mathematical	O
model	O
of	O
the	O
neuron	O
in	O
1943	O
,	O
in	O
which	O
they	O
approximated	O
the	O
16.5.	O
feedforward	O
neural	O
networks	O
(	O
multilayer	O
perceptrons	O
)	O
569	O
(	O
cid:7	O
)	O
output	O
as	O
a	O
weighted	O
sum	O
of	O
inputs	O
passed	O
through	O
a	O
threshold	O
function	O
,	O
y	O
=	O
i	O
(	O
i	O
wixi	O
>	O
θ	O
)	O
,	O
for	O
some	O
threshold	O
θ.	O
this	O
is	O
similar	B
to	O
a	O
sigmoidal	O
activation	B
function	O
.	O
frank	O
rosenblatt	O
invented	O
the	O
perceptron	B
learning	O
algorithm	O
in	O
1957	O
,	O
which	O
is	O
a	O
way	O
to	O
estimate	O
the	O
parameters	O
of	O
a	O
mcculloch-pitts	O
neuron	O
(	O
see	O
section	O
8.5.4	O
for	O
details	O
)	O
.	O
a	O
very	O
similar	B
model	O
called	O
the	O
adaline	B
(	O
for	O
adaptive	O
linear	O
element	O
)	O
was	O
invented	O
in	O
1960	O
by	O
widrow	O
and	O
hoff	O
.	O
in	O
1969	O
,	O
minsky	O
and	O
papert	O
(	O
minsky	O
and	O
papert	O
1969	O
)	O
published	O
a	O
famous	O
book	O
called	O
“	O
percep-	O
trons	O
”	O
in	O
which	O
they	O
showed	O
that	O
such	O
linear	O
models	O
,	O
with	O
no	O
hidden	O
layers	O
,	O
were	O
very	O
limited	O
in	O
their	O
power	O
,	O
since	O
they	O
can	O
not	O
classify	O
data	O
that	O
is	O
not	O
linearly	O
separable	O
.	O
this	O
considerably	O
reduced	O
interest	O
in	O
the	O
ﬁeld	O
.	O
in	O
1986	O
,	O
rumelhart	O
,	O
hinton	O
and	O
williams	O
(	O
rumelhart	O
et	O
al	O
.	O
1986	O
)	O
discovered	O
the	O
backpropa-	O
(	O
the	O
gation	O
algorithm	O
(	O
see	O
section	O
16.5.4	O
)	O
,	O
which	O
allows	O
one	O
to	O
ﬁt	O
models	O
with	O
hidden	B
layers	O
.	O
backpropagation	B
algorithm	I
was	O
originally	O
discovered	O
in	O
(	O
bryson	O
and	O
ho	O
1969	O
)	O
,	O
and	O
independently	O
in	O
(	O
werbos	O
1974	O
)	O
;	O
however	O
,	O
it	O
was	O
(	O
rumelhart	O
et	O
al	O
.	O
1986	O
)	O
that	O
brought	O
the	O
algorithm	O
to	O
people	O
’	O
s	O
attention	O
.	O
)	O
this	O
spawned	O
a	O
decade	O
of	O
intense	O
interest	O
in	O
these	O
models	O
.	O
in	O
1987	O
,	O
sejnowski	O
and	O
rosenberg	O
(	O
sejnowski	O
and	O
rosenberg	O
1987	O
)	O
created	O
the	O
famous	O
nettalk	O
system	O
,	O
that	O
learned	O
a	O
mapping	O
from	O
english	O
words	O
to	O
phonetic	O
symbols	O
which	O
could	O
be	O
fed	O
into	O
a	O
speech	O
synthesizer	O
.	O
an	O
audio	O
demo	O
of	O
the	O
system	O
as	O
it	O
learns	O
over	O
time	O
can	O
be	O
found	O
at	O
http	O
:	O
//www.cnl.salk.edu/parallelnetspronounce/nettalk.mp3	O
.	O
the	O
systems	O
starts	O
by	O
“	O
babbling	O
”	O
and	O
then	O
gradually	O
learns	O
to	O
pronounce	O
english	O
words	O
.	O
nettalk	O
learned	O
a	O
distributed	B
representation	I
(	O
via	O
its	O
hidden	B
layer	I
)	O
of	O
various	O
sounds	O
,	O
and	O
its	O
success	O
spawned	O
a	O
big	O
debate	O
in	O
psychology	O
between	O
connectionism	B
,	O
based	O
on	O
neural	B
networks	I
,	O
and	O
computationalism	B
,	O
based	O
on	O
syntactic	O
rules	O
.	O
this	O
debate	O
lives	O
on	O
to	O
some	O
extent	O
in	O
the	O
machine	B
learning	I
community	O
,	O
where	O
there	O
are	O
still	O
arguments	O
about	O
whether	O
learning	B
is	O
best	O
performed	O
using	O
low-level	O
,	O
“	O
neural-	O
like	O
”	O
representations	O
,	O
or	O
using	O
more	O
structured	O
models	O
.	O
in	O
1989	O
,	O
yann	O
le	O
cun	O
and	O
others	O
(	O
lecun	O
et	O
al	O
.	O
1989	O
)	O
created	O
the	O
famous	O
lenet	O
system	O
described	O
in	O
section	O
16.5.1.	O
in	O
1992	O
,	O
the	O
support	B
vector	I
machine	I
(	O
see	O
section	O
14.5	O
)	O
was	O
invented	O
(	O
boser	O
et	O
al	O
.	O
1992	O
)	O
.	O
svms	O
provide	O
similar	B
prediction	O
accuracy	O
to	O
neural	B
networks	I
while	O
being	O
considerably	O
easier	O
to	O
train	O
(	O
since	O
they	O
use	O
a	O
convex	B
objective	O
function	O
)	O
.	O
this	O
spawned	O
a	O
decade	O
of	O
interest	O
in	O
kernel	B
methods	O
in	O
general.7	O
note	O
,	O
however	O
,	O
that	O
svms	O
do	O
not	O
use	O
adaptive	O
basis	O
functions	O
,	O
so	O
they	O
require	O
a	O
fair	O
amount	O
of	O
human	O
expertise	O
to	O
design	O
the	O
right	O
kernel	O
function	O
.	O
in	O
2002	O
,	O
geoff	O
hinton	O
invented	O
the	O
contrastive	B
divergence	I
training	O
procedure	O
(	O
hinton	O
2002	O
)	O
,	O
which	O
provided	O
a	O
way	O
,	O
for	O
the	O
ﬁrst	O
time	O
,	O
to	O
learn	O
deep	B
networks	I
,	O
by	O
training	O
one	O
layer	O
at	O
a	O
time	O
in	O
an	O
unsupervised	O
fashion	O
(	O
see	O
section	O
27.7.2.4	O
for	O
details	O
)	O
.	O
this	O
in	O
turn	O
has	O
spawned	O
renewed	O
interest	O
in	O
neural	B
networks	I
over	O
the	O
last	O
few	O
years	O
(	O
see	O
chapter	O
28	O
)	O
.	O
16.5.4	O
the	O
backpropagation	B
algorithm	I
unlike	O
a	O
glm	O
,	O
the	O
nll	O
of	O
an	O
mlp	O
is	O
a	O
non-convex	O
function	O
of	O
its	O
parameters	O
.	O
nevertheless	O
,	O
we	O
can	O
ﬁnd	O
a	O
locally	O
optimal	O
ml	O
or	O
map	O
estimate	O
using	O
standard	O
gradient-based	O
optimization	B
methods	O
.	O
since	O
mlps	O
have	O
lots	O
of	O
parameters	O
,	O
they	O
are	O
often	O
trained	O
on	O
very	O
large	O
data	O
sets	O
.	O
7.	O
it	O
became	O
part	O
of	O
the	O
folklore	O
during	O
the	O
1990s	O
that	O
to	O
get	O
published	O
in	O
the	O
top	O
machine	B
learning	I
conference	O
known	O
as	O
nips	O
,	O
which	O
stands	O
for	O
“	O
neural	O
information	O
processing	O
systems	O
”	O
,	O
it	O
was	O
important	O
to	O
ensure	O
your	O
paper	O
did	O
not	O
contain	O
the	O
word	O
“	O
neural	B
network	I
”	O
!	O
570	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−0.6	O
−0.8	O
−1	O
−10	O
tanh	O
sigmoid	B
−5	O
0	O
5	O
10	O
figure	O
16.16	O
two	O
possible	O
activation	B
functions	O
.	O
tanh	O
maps	O
r	O
to	O
[	O
−1	O
,	O
+1	O
]	O
and	O
is	O
the	O
preferred	O
nonlin-	O
earity	O
for	O
the	O
hidden	B
nodes	I
.	O
sigm	O
maps	O
r	O
to	O
[	O
0	O
,	O
1	O
]	O
and	O
is	O
the	O
preferred	O
nonlinearity	O
for	O
binary	O
nodes	O
at	O
the	O
output	O
layer	O
.	O
figure	O
generated	O
by	O
tanhplot	O
.	O
consequently	O
it	O
is	O
common	O
to	O
use	O
ﬁrst-order	O
online	O
methods	O
,	O
such	O
as	O
stochastic	B
gradient	I
descent	I
(	O
section	O
8.5.2	O
)	O
,	O
whereas	O
glms	O
are	O
usually	O
ﬁt	O
with	O
irls	O
,	O
which	O
is	O
a	O
second-order	O
offline	O
method	O
.	O
we	O
now	O
discuss	O
how	O
to	O
compute	O
the	O
gradient	O
vector	O
of	O
the	O
nll	O
by	O
applying	O
the	O
chain	B
rule	I
of	O
calculus	O
.	O
the	O
resulting	O
algorithm	O
is	O
known	O
as	O
backpropagation	B
,	O
for	O
reasons	O
that	O
will	O
become	O
apparent	O
.	O
for	O
notational	O
simplicity	O
,	O
we	O
shall	O
assume	O
a	O
model	O
with	O
just	O
one	O
hidden	B
layer	I
.	O
it	O
is	O
helpful	O
to	O
distinguish	O
the	O
pre-	O
and	O
post-synaptic	O
values	O
of	O
a	O
neuron	O
,	O
that	O
is	O
,	O
before	O
and	O
after	O
we	O
apply	O
the	O
nonlinearity	O
.	O
let	O
xn	O
be	O
the	O
n	O
’	O
th	O
input	O
,	O
an	O
=	O
vxn	O
be	O
the	O
pre-synaptic	O
hidden	B
layer	I
,	O
and	O
zn	O
=	O
g	O
(	O
an	O
)	O
be	O
the	O
post-synaptic	O
hidden	B
layer	I
,	O
where	O
g	O
is	O
some	O
transfer	B
function	I
.	O
we	O
typically	O
use	O
g	O
(	O
a	O
)	O
=	O
sigm	O
(	O
a	O
)	O
,	O
but	O
we	O
may	O
also	O
use	O
g	O
(	O
a	O
)	O
=	O
tanh	O
(	O
a	O
)	O
:	O
see	O
figure	O
16.16	O
for	O
a	O
comparison	O
.	O
(	O
when	O
the	O
input	O
to	O
sigm	O
or	O
tanh	O
is	O
a	O
vector	O
,	O
we	O
assume	O
it	O
is	O
applied	O
component-wise	O
.	O
)	O
we	O
now	O
convert	O
this	O
hidden	B
layer	I
to	O
the	O
output	O
layer	O
as	O
follows	O
.	O
let	O
bn	O
=	O
wzn	O
be	O
the	O
pre-synaptic	O
output	O
layer	O
,	O
and	O
ˆyn	O
=	O
h	O
(	O
bn	O
)	O
be	O
the	O
post-synaptic	O
output	O
layer	O
,	O
where	O
h	O
is	O
another	O
nonlinearity	O
,	O
corresponding	O
to	O
the	O
canonical	O
link	O
for	O
the	O
glm	O
.	O
(	O
we	O
reserve	O
the	O
notation	O
yn	O
,	O
without	O
the	O
hat	O
,	O
for	O
the	O
output	O
corresponding	O
to	O
the	O
n	O
’	O
th	O
training	O
case	O
.	O
)	O
for	O
a	O
regression	B
model	O
,	O
we	O
use	O
h	O
(	O
b	O
)	O
=	O
b	O
;	O
for	O
binary	O
classifcation	O
,	O
we	O
use	O
h	O
(	O
b	O
)	O
=	O
[	O
sigm	O
(	O
b1	O
)	O
,	O
.	O
.	O
.	O
,	O
sigm	O
(	O
bc	O
)	O
]	O
;	O
for	O
multi-class	O
classiﬁcation	O
,	O
we	O
use	O
h	O
(	O
b	O
)	O
=	O
s	O
(	O
b	O
)	O
.	O
we	O
can	O
write	O
the	O
overall	O
model	O
as	O
follows	O
:	O
v→	O
an	O
g→	O
zn	O
w→	O
bn	O
h→	O
ˆyn	O
xn	O
(	O
16.65	O
)	O
the	O
parameters	O
of	O
the	O
model	O
are	O
θ	O
=	O
(	O
v	O
,	O
w	O
)	O
,	O
the	O
ﬁrst	O
and	O
second	O
layer	O
weight	O
matrices	O
.	O
offset	O
or	O
bias	B
terms	O
can	O
be	O
accomodated	O
by	O
clamping	B
an	O
element	O
of	O
xn	O
and	O
zn	O
to	O
1.8	O
8.	O
in	O
the	O
regression	B
setting	O
,	O
we	O
can	O
easily	O
estimate	O
the	O
variance	B
of	O
the	O
output	O
noise	O
using	O
the	O
empirical	O
variance	O
of	O
the	O
||ˆy	O
(	O
ˆθ	O
)	O
−	O
y||2	O
,	O
after	O
training	O
is	O
complete	B
.	O
there	O
will	O
be	O
one	O
value	O
of	O
σ2	O
for	O
each	O
output	O
node	O
,	O
residual	B
errors	O
,	O
ˆσ2	O
=	O
1	O
n	O
if	O
we	O
are	O
performing	O
multi-target	O
regression	O
,	O
as	O
we	O
usually	O
assume	O
.	O
16.5.	O
feedforward	O
neural	O
networks	O
(	O
multilayer	O
perceptrons	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
k	O
in	O
the	O
regression	B
case	O
,	O
with	O
k	O
outputs	O
,	O
the	O
nll	O
is	O
given	O
by	O
the	O
squared	B
error	I
:	O
j	O
(	O
θ	O
)	O
=	O
−	O
(	O
ˆynk	O
(	O
θ	O
)	O
−	O
ynk	O
)	O
2	O
in	O
the	O
classiﬁcation	B
case	O
,	O
with	O
k	O
classes	O
,	O
the	O
nll	O
is	O
given	O
by	O
the	O
cross	B
entropy	I
j	O
(	O
θ	O
)	O
=	O
−	O
ynk	O
log	O
ˆynk	O
(	O
θ	O
)	O
(	O
16.67	O
)	O
our	O
task	O
is	O
to	O
compute	O
∇θj	O
.	O
we	O
will	O
derive	O
this	O
for	O
each	O
n	O
separately	O
;	O
the	O
overall	O
gradient	O
is	O
obtained	O
by	O
summing	O
over	O
n	O
,	O
although	O
often	O
we	O
just	O
use	O
a	O
mini-batch	B
(	O
see	O
section	O
8.5.2	O
)	O
.	O
n	O
k	O
let	O
us	O
start	O
by	O
considering	O
the	O
output	O
layer	O
weights	O
.	O
we	O
have	O
∇wk	O
∇wk	O
bnk	O
=	O
jn	O
=	O
zn	O
∂jn	O
∂bnk	O
∂jn	O
∂bnk	O
(	O
16.68	O
)	O
since	O
bnk	O
=	O
wt	O
equation	O
9.91	O
tells	O
us	O
that	O
k	O
zn	O
.	O
assuming	O
h	O
is	O
the	O
canonical	B
link	I
function	I
for	O
the	O
output	O
glm	O
,	O
then	O
∂jn	O
∂bnk	O
(	O
cid:2	O
)	O
δw	O
nk	O
=	O
(	O
ˆynk	O
−	O
ynk	O
)	O
(	O
16.69	O
)	O
which	O
is	O
the	O
error	B
signal	I
.	O
so	O
the	O
overall	O
gradient	O
is	O
∇wk	O
jn	O
=	O
δw	O
nkzn	O
(	O
16.70	O
)	O
which	O
is	O
the	O
pre-synaptic	O
input	O
to	O
the	O
output	O
layer	O
,	O
namely	O
zn	O
,	O
times	O
the	O
error	B
signal	I
,	O
namely	O
δw	O
nk	O
.	O
for	O
the	O
input	O
layer	O
weights	O
,	O
we	O
have	O
∇vj	O
jn	O
=	O
∇vj	O
anj	O
(	O
cid:2	O
)	O
δv	O
njxn	O
(	O
16.71	O
)	O
∂jn	O
∂anj	O
j	O
xn	O
.	O
all	O
that	O
remains	O
is	O
to	O
compute	O
the	O
ﬁrst	O
level	O
571	O
(	O
16.66	O
)	O
where	O
we	O
exploited	O
the	O
fact	O
that	O
anj	O
=	O
vt	O
error	B
signal	I
δv	O
nj	O
.	O
we	O
have	O
δv	O
nj	O
=	O
∂jn	O
∂anj	O
=	O
∂jn	O
∂bnk	O
∂bnk	O
∂anj	O
=	O
k	O
(	O
cid:4	O
)	O
k=1	O
k	O
(	O
cid:4	O
)	O
k=1	O
(	O
cid:4	O
)	O
j	O
now	O
bnk	O
=	O
so	O
wkjg	O
(	O
anj	O
)	O
=	O
wkjg	O
(	O
cid:2	O
)	O
∂bnk	O
(	O
anj	O
)	O
∂anj	O
da	O
g	O
(	O
a	O
)	O
.	O
for	O
tanh	O
units	O
,	O
g	O
(	O
cid:2	O
)	O
where	O
g	O
(	O
cid:2	O
)	O
for	O
sigmoid	B
units	O
,	O
g	O
(	O
cid:2	O
)	O
k	O
(	O
cid:4	O
)	O
(	O
a	O
)	O
=	O
d	O
(	O
a	O
)	O
=	O
d	O
δv	O
nj	O
=	O
nkwkjg	O
(	O
cid:2	O
)	O
δw	O
(	O
anj	O
)	O
k=1	O
δw	O
nk	O
∂bnk	O
∂anj	O
(	O
16.72	O
)	O
(	O
16.73	O
)	O
(	O
16.74	O
)	O
da	O
tanh	O
(	O
a	O
)	O
=	O
1	O
−	O
tanh2	O
(	O
a	O
)	O
=	O
sech2	O
(	O
a	O
)	O
,	O
and	O
(	O
a	O
)	O
=	O
d	O
da	O
σ	O
(	O
a	O
)	O
=	O
σ	O
(	O
a	O
)	O
(	O
1	O
−	O
σ	O
(	O
a	O
)	O
)	O
.	O
hence	O
(	O
16.75	O
)	O
572	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
thus	O
the	O
layer	O
1	O
errors	O
can	O
be	O
computed	O
by	O
passing	O
the	O
layer	O
2	O
errors	O
back	O
through	O
the	O
w	O
matrix	O
;	O
hence	O
the	O
term	O
“	O
backpropagation	B
”	O
.	O
the	O
key	O
property	O
is	O
that	O
we	O
can	O
compute	O
the	O
gradients	O
locally	O
:	O
each	O
node	O
only	O
needs	O
to	O
know	O
about	O
its	O
immediate	O
neighbors	B
.	O
this	O
is	O
supposed	O
to	O
make	O
the	O
algorithm	O
“	O
neurally	O
plausible	O
”	O
,	O
although	O
this	O
interpretation	O
is	O
somewhat	O
controversial	O
.	O
putting	O
it	O
all	O
together	O
,	O
we	O
can	O
compute	O
all	O
the	O
gradients	O
as	O
follows	O
:	O
we	O
ﬁrst	O
perform	O
a	O
forwards	O
pass	O
to	O
compute	O
an	O
,	O
zn	O
,	O
bn	O
and	O
ˆyn	O
.	O
we	O
then	O
compute	O
the	O
error	O
for	O
the	O
output	O
layer	O
,	O
n	O
=	O
ˆyn	O
−	O
yn	O
,	O
which	O
we	O
pass	O
backwards	O
through	O
w	O
using	O
equation	O
16.75	O
to	O
compute	O
the	O
δ	O
(	O
2	O
)	O
error	O
for	O
the	O
hidden	B
layer	I
,	O
δ	O
(	O
1	O
)	O
n	O
.	O
we	O
then	O
compute	O
the	O
overall	O
gradient	O
as	O
follows	O
:	O
(	O
cid:4	O
)	O
∇θj	O
(	O
θ	O
)	O
=	O
v2	O
ij	O
+	O
w2	O
jk	O
]	O
(	O
16.77	O
)	O
[	O
δv	O
nxn	O
,	O
δw	O
n	O
zn	O
]	O
(	O
16.76	O
)	O
n	O
16.5.5	O
identiﬁability	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
parameters	O
of	O
a	O
neural	B
network	I
are	O
not	O
identiﬁable	O
.	O
for	O
example	O
,	O
we	O
can	O
change	O
the	O
sign	O
of	O
the	O
weights	O
going	O
into	O
one	O
of	O
the	O
hidden	B
units	I
,	O
so	O
long	O
as	O
we	O
change	O
the	O
sign	O
of	O
all	O
the	O
weights	O
going	O
out	O
of	O
it	O
;	O
these	O
effects	O
cancel	O
,	O
since	O
tanh	O
is	O
an	O
odd	O
function	O
,	O
so	O
tanh	O
(	O
−a	O
)	O
=	O
−	O
tanh	O
(	O
a	O
)	O
.	O
there	O
will	O
be	O
h	O
such	O
sign	O
ﬂip	O
symmetries	O
,	O
leading	O
to	O
2h	O
equivalent	O
settings	O
of	O
the	O
parameters	O
.	O
similarly	O
,	O
we	O
can	O
change	O
the	O
identity	O
of	O
the	O
hidden	B
units	I
without	O
affecting	O
the	O
likelihood	B
.	O
there	O
are	O
h	O
!	O
such	O
permutations	O
.	O
the	O
total	O
number	O
of	O
equivalent	O
parameter	O
settings	O
(	O
with	O
the	O
same	O
likelihood	B
)	O
is	O
therefore	O
h	O
!	O
2h	O
.	O
in	O
addition	O
,	O
there	O
may	O
be	O
local	O
minima	O
due	O
to	O
the	O
non-convexity	O
of	O
the	O
nll	O
.	O
this	O
can	O
be	O
a	O
more	O
serious	O
problem	O
,	O
although	O
with	O
enough	O
data	O
,	O
these	O
local	O
optima	O
are	O
often	O
quite	O
“	O
shallow	O
”	O
,	O
and	O
simple	O
stochastic	O
optimization	B
methods	O
can	O
avoid	O
them	O
.	O
in	O
addition	O
,	O
it	O
is	O
common	O
to	O
perform	O
multiple	B
restarts	I
,	O
and	O
to	O
pick	O
the	O
best	O
solution	O
,	O
or	O
to	O
average	O
over	O
the	O
resulting	O
predictions	O
.	O
(	O
it	O
does	O
not	O
make	O
sense	O
to	O
average	O
the	O
parameters	O
themselves	O
,	O
since	O
they	O
are	O
not	O
identiﬁable	O
.	O
)	O
16.5.6	O
regularization	B
as	O
usual	O
,	O
the	O
mle	O
can	O
overﬁt	B
,	O
especially	O
if	O
the	O
number	O
of	O
nodes	B
is	O
large	O
.	O
a	O
simple	O
way	O
to	O
prevent	O
this	O
is	O
called	O
early	B
stopping	I
,	O
which	O
means	O
stopping	O
the	O
training	O
procedure	O
when	O
the	O
error	O
on	O
the	O
validation	B
set	I
ﬁrst	O
starts	O
to	O
increase	O
.	O
this	O
method	O
works	O
because	O
we	O
usually	O
initialize	O
from	O
small	O
random	O
weights	O
,	O
so	O
the	O
model	O
is	O
initially	O
simple	O
(	O
since	O
the	O
tanh	O
and	O
sigm	O
functions	O
are	O
nearly	O
linear	O
near	O
the	O
origin	O
)	O
.	O
as	O
training	O
progresses	O
,	O
the	O
weights	O
become	O
larger	O
,	O
and	O
the	O
model	O
becomes	O
nonlinear	O
.	O
eventually	O
it	O
will	O
overﬁt	B
.	O
another	O
way	O
to	O
prevent	O
overﬁtting	B
,	O
that	O
is	O
more	O
in	O
keeping	O
with	O
the	O
approaches	O
used	O
elsewhere	O
in	O
this	O
book	O
,	O
is	O
to	O
impose	O
a	O
prior	O
on	O
the	O
parameters	O
,	O
and	O
then	O
use	O
map	O
estimation	O
.	O
it	O
is	O
standard	O
to	O
use	O
a	O
n	O
(	O
0	O
,	O
α−1i	O
)	O
prior	O
(	O
equivalent	O
to	O
(	O
cid:2	O
)	O
2	O
regularization	B
)	O
,	O
where	O
α	O
is	O
the	O
precision	B
(	O
strength	O
)	O
of	O
the	O
prior	O
.	O
in	O
the	O
neural	B
networks	I
literature	O
,	O
this	O
is	O
called	O
weight	B
decay	I
,	O
since	O
it	O
encourages	O
small	O
weights	O
,	O
and	O
hence	O
simpler	O
models	O
.	O
the	O
penalized	O
nll	O
objective	O
becomes	O
j	O
(	O
θ	O
)	O
=	O
−	O
n	O
(	O
cid:4	O
)	O
n=1	O
log	O
p	O
(	O
yn|xn	O
,	O
θ	O
)	O
+	O
α	O
2	O
[	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
ij	O
jk	O
16.5.	O
feedforward	O
neural	O
networks	O
(	O
multilayer	O
perceptrons	O
)	O
573	O
(	O
cid:4	O
)	O
∇θj	O
(	O
θ	O
)	O
=	O
[	O
(	O
cid:4	O
)	O
(	O
note	O
that	O
we	O
don	O
’	O
t	O
penalize	O
the	O
bias	B
terms	O
.	O
)	O
the	O
gradient	O
of	O
the	O
modiﬁed	O
objective	O
becomes	O
δv	O
nxn	O
+	O
αv	O
,	O
δw	O
n	O
zn	O
+	O
αw	O
]	O
(	O
16.78	O
)	O
n	O
n	O
as	O
in	O
section	O
8.3.6.	O
if	O
the	O
regularization	B
is	O
sufficiently	O
strong	O
,	O
it	O
does	O
not	O
matter	O
if	O
we	O
have	O
too	O
many	O
hidden	B
units	I
(	O
apart	O
from	O
wasted	O
computation	O
)	O
.	O
hence	O
it	O
is	O
advisable	O
to	O
set	O
h	O
to	O
be	O
as	O
large	O
as	O
you	O
can	O
afford	O
(	O
say	O
10–100	O
)	O
,	O
and	O
then	O
to	O
choose	O
an	O
appropriate	O
regularizer	O
.	O
we	O
can	O
set	O
the	O
α	O
parameter	B
by	O
cross	B
validation	I
or	O
empirical	O
bayes	O
(	O
see	O
section	O
16.5.7.5	O
)	O
.	O
as	O
with	O
ridge	B
regression	I
,	O
it	O
is	O
good	O
practice	O
to	O
standardize	O
the	O
inputs	O
to	O
zero	O
mean	O
and	O
unit	O
variance	O
,	O
so	O
that	O
the	O
spherical	B
gaussian	O
prior	O
makes	O
sense	O
.	O
16.5.6.1	O
consistent	B
gaussian	O
priors	O
*	O
one	O
can	O
show	O
(	O
mackay	O
1992	O
)	O
that	O
using	O
the	O
same	O
regularization	B
parameter	O
for	O
both	O
the	O
ﬁrst	O
and	O
second	O
layer	O
weights	O
results	O
in	O
the	O
lack	O
of	O
a	O
certain	O
desirable	O
invariance	O
property	O
.	O
in	O
particular	O
,	O
suppose	O
we	O
linearly	O
scale	O
and	O
shift	O
the	O
inputs	O
and/or	O
outputs	O
to	O
a	O
neural	B
network	I
regression	O
model	O
.	O
then	O
we	O
would	O
like	O
the	O
model	O
to	O
learn	O
to	O
predict	O
the	O
same	O
function	O
,	O
by	O
suitably	O
scaling	O
its	O
internal	O
weights	O
and	O
bias	B
terms	O
.	O
however	O
,	O
the	O
amount	O
of	O
scaling	O
needed	O
by	O
the	O
ﬁrst	O
and	O
second	O
layer	O
weights	O
to	O
compensate	O
for	O
a	O
change	O
in	O
the	O
inputs	O
and/or	O
outputs	O
is	O
not	O
the	O
same	O
.	O
therefore	O
we	O
need	O
to	O
use	O
a	O
different	O
regularization	B
strength	O
for	O
the	O
ﬁrst	O
and	O
second	O
layer	O
.	O
fortunately	O
,	O
this	O
is	O
easy	O
to	O
do	O
—	O
we	O
just	O
use	O
the	O
following	O
prior	O
:	O
i	O
)	O
n	O
(	O
c|0	O
,	O
1	O
αc	O
p	O
(	O
θ	O
)	O
=	O
n	O
(	O
w|0	O
,	O
1	O
αw	O
(	O
16.79	O
)	O
i	O
)	O
n	O
(	O
v|0	O
,	O
1	O
αv	O
i	O
)	O
n	O
(	O
b|0	O
,	O
1	O
αb	O
i	O
)	O
where	O
b	O
and	O
c	O
are	O
the	O
bias	B
terms.9	O
to	O
get	O
a	O
feeling	O
for	O
the	O
effect	O
of	O
these	O
hyper-parameters	B
,	O
we	O
can	O
sample	O
mlp	O
parameters	O
from	O
this	O
prior	O
and	O
plot	O
the	O
resulting	O
random	O
functions	O
.	O
figure	O
16.17	O
shows	O
some	O
examples	O
.	O
decreasing	O
αv	O
allows	O
the	O
ﬁrst	O
layer	O
weights	O
to	O
get	O
bigger	O
,	O
making	O
the	O
sigmoid-like	O
shape	O
of	O
the	O
functions	O
steeper	O
.	O
decreasing	O
αb	O
allows	O
the	O
ﬁrst	O
layer	O
biases	O
to	O
get	O
bigger	O
,	O
which	O
allows	O
the	O
center	O
of	O
the	O
sigmoid	B
to	O
shift	O
left	O
and	O
right	O
more	O
.	O
decreasing	O
αw	O
allows	O
the	O
second	O
layer	O
weights	O
to	O
get	O
bigger	O
,	O
making	O
the	O
functions	O
more	O
“	O
wiggly	O
”	O
(	O
greater	O
sensitivity	B
to	O
change	O
in	O
the	O
input	O
,	O
and	O
hence	O
larger	O
dynamic	O
range	O
)	O
.	O
and	O
decreasing	O
αc	O
allows	O
the	O
second	O
layer	O
biases	O
to	O
get	O
bigger	O
,	O
allowing	O
the	O
mean	B
level	O
of	O
the	O
function	O
to	O
move	O
up	O
and	O
down	O
more	O
.	O
(	O
in	O
chapter	O
15	O
,	O
we	O
will	O
see	O
an	O
easier	O
way	O
to	O
deﬁne	O
priors	O
over	O
functions	O
.	O
)	O
16.5.6.2	O
weight	O
pruning	O
since	O
there	O
are	O
many	O
weights	O
in	O
a	O
neural	B
network	I
,	O
it	O
is	O
often	O
helpful	O
to	O
encourage	O
sparsity	B
.	O
various	O
ad-hoc	O
methods	O
for	O
doing	O
this	O
,	O
with	O
names	O
such	O
as	O
“	O
optimal	O
brain	O
damage	O
”	O
,	O
were	O
devised	O
in	O
the	O
1990s	O
;	O
see	O
e.g.	O
,	O
(	O
bishop	O
1995	O
)	O
for	O
details	O
.	O
9.	O
since	O
we	O
are	O
regularizing	O
the	O
output	O
bias	B
terms	O
,	O
it	O
is	O
helpful	O
,	O
in	O
the	O
case	O
of	O
regression	B
,	O
to	O
normalize	O
the	O
target	O
responses	O
in	O
the	O
training	B
set	I
to	O
zero	O
mean	O
,	O
to	O
be	O
consistent	B
with	O
the	O
fact	O
that	O
the	O
prior	O
on	O
the	O
output	O
bias	B
has	O
zero	O
mean	O
.	O
574	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
aw1=0.010	O
,	O
ab1=0.100	O
,	O
aw2=1.000	O
,	O
ab2=1.000	O
aw1=0.001	O
,	O
ab1=0.100	O
,	O
aw2=1.000	O
,	O
ab2=1.000	O
10	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
10	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
−10	O
−1	O
−0.5	O
0	O
(	O
a	O
)	O
0.5	O
1	O
−10	O
−1	O
−0.5	O
0.5	O
1	O
0	O
(	O
b	O
)	O
aw1=0.010	O
,	O
ab1=0.010	O
,	O
aw2=1.000	O
,	O
ab2=1.000	O
aw1=0.010	O
,	O
ab1=0.100	O
,	O
aw2=0.100	O
,	O
ab2=1.000	O
10	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
10	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
−10	O
−1	O
−0.5	O
0	O
(	O
c	O
)	O
0.5	O
1	O
−10	O
−1	O
−0.5	O
0.5	O
1	O
0	O
(	O
d	O
)	O
aw1=0.010	O
,	O
ab1=0.100	O
,	O
aw2=1.000	O
,	O
ab2=0.100	O
10	O
8	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
−10	O
−1	O
−0.5	O
0.5	O
1	O
0	O
(	O
e	O
)	O
figure	O
16.17	O
the	O
effects	O
of	O
changing	O
the	O
hyper-parameters	B
on	O
an	O
mlp	O
.	O
αv	O
=	O
0.01	O
,	O
αb	O
=	O
0.1	O
,	O
αw	O
=	O
1	O
,	O
αc	O
=	O
1.	O
factor	B
of	O
10.	O
mlppriorsdemo	O
.	O
(	O
a	O
)	O
default	O
parameter	B
values	O
(	O
c	O
)	O
decreasing	O
αb	O
by	O
(	O
e	O
)	O
decreasing	O
αc	O
by	O
factor	B
of	O
10.	O
figure	O
generated	O
by	O
(	O
d	O
)	O
decreasing	O
αw	O
by	O
factor	B
of	O
10	O
.	O
(	O
b	O
)	O
decreasing	O
αv	O
by	O
factor	B
of	O
10	O
.	O
16.5.	O
feedforward	O
neural	O
networks	O
(	O
multilayer	O
perceptrons	O
)	O
575	O
h40	O
h30	O
h20	O
h10	O
h41	O
h31	O
h21	O
h11	O
neural	B
network	I
y	O
h42	O
h43	O
h44	O
h32	O
h22	O
h12	O
h33	O
h34	O
h23	O
h24	O
h13	O
h14	O
x0	O
x1	O
(	O
a	O
)	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
data	O
deep	O
neural	O
net	O
0	O
(	O
b	O
)	O
5	O
figure	O
16.18	O
(	O
a	O
)	O
a	O
deep	B
but	O
sparse	B
neural	O
network	O
.	O
the	O
connections	O
are	O
pruned	O
using	O
(	O
cid:7	O
)	O
1	O
regularization	B
.	O
at	O
each	O
level	O
,	O
nodes	B
numbered	O
0	O
are	O
clamped	O
to	O
1	O
,	O
so	O
their	O
outgoing	O
weights	O
correspond	O
to	O
the	O
offset/bias	O
(	O
b	O
)	O
predictions	O
made	O
by	O
the	O
model	O
on	O
the	O
training	B
set	I
.	O
figure	O
generated	O
by	O
sparsennetdemo	O
,	O
terms	O
.	O
written	O
by	O
mark	O
schmidt	O
.	O
however	O
,	O
we	O
can	O
also	O
use	O
the	O
more	O
principled	O
sparsity-promoting	O
techniques	O
we	O
discussed	O
in	O
chapter	O
13.	O
one	O
approach	O
is	O
to	O
use	O
an	O
(	O
cid:2	O
)	O
1	O
regularizer	O
.	O
see	O
figure	O
16.18	O
for	O
an	O
example	O
.	O
another	O
approach	O
is	O
to	O
use	O
ard	O
;	O
this	O
is	O
discussed	O
in	O
more	O
detail	O
in	O
section	O
16.5.7.5	O
.	O
16.5.6.3	O
soft	O
weight	O
sharing*	O
another	O
way	O
to	O
regularize	O
the	O
parameters	O
is	O
to	O
encourage	O
similar	B
weights	O
to	O
share	O
statistical	O
strength	O
.	O
but	O
how	O
do	O
we	O
know	O
which	O
parameters	O
to	O
group	O
together	O
?	O
we	O
can	O
learn	O
this	O
,	O
by	O
using	O
a	O
mixture	B
model	I
.	O
that	O
is	O
,	O
we	O
model	O
p	O
(	O
θ	O
)	O
as	O
a	O
mixture	O
of	O
(	O
diagonal	B
)	O
gaussians	O
.	O
parameters	O
that	O
are	O
assigned	O
to	O
the	O
same	O
cluster	O
will	O
share	O
the	O
same	O
mean	B
and	O
variance	B
and	O
thus	O
will	O
have	O
similar	B
values	O
(	O
assuming	O
the	O
variance	B
for	O
that	O
cluster	O
is	O
low	O
)	O
.	O
this	O
is	O
called	O
soft	B
weight	I
sharing	I
(	O
nowlan	O
and	O
hinton	O
1992	O
)	O
.	O
in	O
practice	O
,	O
this	O
technique	O
is	O
not	O
widely	O
used	O
.	O
see	O
e.g.	O
,	O
(	O
bishop	O
2006a	O
,	O
p271	O
)	O
if	O
you	O
want	O
to	O
know	O
the	O
details	O
.	O
16.5.6.4	O
semi-supervised	B
embedding	I
*	O
an	O
interesting	O
way	O
to	O
regularize	O
“	O
deep	B
”	O
feedforward	O
neural	O
networks	O
is	O
to	O
encourage	O
the	O
hidden	B
layers	O
to	O
assign	O
similar	B
objects	O
to	O
similar	B
representations	O
.	O
this	O
is	O
useful	O
because	O
it	O
is	O
often	O
easy	O
to	O
obtain	O
“	O
side	O
”	O
information	B
consisting	O
of	O
sets	O
of	O
pairs	O
of	O
similar	B
and	O
dissimilar	O
objects	O
.	O
for	O
example	O
,	O
in	O
a	O
video	O
classiﬁcation	B
task	O
,	O
neighboring	O
frames	O
can	O
be	O
deemed	O
similar	B
,	O
but	O
frames	O
that	O
are	O
distant	O
in	O
time	O
can	O
be	O
deemed	O
dis-similar	O
(	O
mobahi	O
et	O
al	O
.	O
2009	O
)	O
.	O
note	O
that	O
this	O
can	O
be	O
done	O
without	O
collecting	O
any	O
labels	O
.	O
let	O
sij	O
=	O
1	O
if	O
examples	O
i	O
and	O
j	O
are	O
similar	B
,	O
and	O
sij	O
=	O
0	O
otherwise	O
.	O
let	O
f	O
(	O
xi	O
)	O
be	O
some	O
embedding	B
of	O
item	O
xi	O
,	O
e.g.	O
,	O
f	O
(	O
xi	O
)	O
=	O
z	O
(	O
xi	O
,	O
θ	O
)	O
,	O
where	O
z	O
is	O
the	O
hidden	B
layer	I
of	O
a	O
neural	B
network	I
.	O
now	O
deﬁne	O
a	O
loss	B
function	I
l	O
(	O
f	O
(	O
xi	O
)	O
,	O
f	O
(	O
xj	O
)	O
,	O
sij	O
)	O
that	O
depends	O
on	O
the	O
embedding	B
of	O
two	O
objects	O
,	O
576	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
and	O
the	O
observed	O
similarity	O
measure	O
.	O
for	O
example	O
,	O
we	O
might	O
want	O
to	O
force	O
similar	B
objects	O
to	O
have	O
similar	B
embeddings	O
,	O
and	O
to	O
force	O
the	O
embeddings	O
of	O
dissimilar	O
objects	O
to	O
be	O
a	O
minimal	B
distance	O
apart	O
:	O
l	O
(	O
fi	O
,	O
fj	O
,	O
sij	O
)	O
=	O
max	O
(	O
0	O
,	O
m	O
−	O
||fi	O
−	O
fj||2	O
)	O
if	O
sij	O
=	O
1	O
if	O
sij	O
=	O
0	O
(	O
16.80	O
)	O
(	O
cid:19	O
)	O
||fi	O
−	O
fj||2	O
(	O
cid:4	O
)	O
i	O
,	O
j∈u	O
where	O
m	O
is	O
some	O
minimal	B
margin	O
.	O
we	O
can	O
now	O
deﬁne	O
an	O
augmented	O
loss	O
function	O
for	O
training	O
the	O
neural	B
network	I
:	O
nll	O
(	O
f	O
(	O
xi	O
)	O
,	O
yi	O
)	O
+λ	O
l	O
(	O
f	O
(	O
xi	O
)	O
,	O
f	O
(	O
xj	O
)	O
,	O
sij	O
)	O
(	O
16.81	O
)	O
(	O
cid:4	O
)	O
i∈l	O
where	O
l	O
is	O
the	O
labeled	O
training	O
set	O
,	O
u	O
is	O
the	O
unlabeled	O
training	B
set	I
,	O
and	O
λ	O
≥	O
0	O
is	O
some	O
tradeoff	O
parameter	B
.	O
this	O
is	O
called	O
semi-supervised	B
embedding	I
(	O
weston	O
et	O
al	O
.	O
2008	O
)	O
.	O
such	O
an	O
objective	O
can	O
be	O
easily	O
optimized	O
by	O
stochastic	B
gradient	I
descent	I
.	O
at	O
each	O
itera-	O
tion	O
,	O
pick	O
a	O
random	O
labeled	O
training	O
example	O
,	O
(	O
xn	O
,	O
yn	O
)	O
,	O
and	O
take	O
a	O
gradient	O
step	O
to	O
optimize	O
nll	O
(	O
f	O
(	O
xi	O
)	O
,	O
yi	O
)	O
.	O
then	O
pick	O
a	O
random	O
pair	O
of	O
similar	B
unlabeled	O
examples	O
xi	O
,	O
xj	O
(	O
these	O
can	O
sometimes	O
be	O
generated	O
on	O
the	O
ﬂy	O
rather	O
than	O
stored	O
in	O
advance	O
)	O
,	O
and	O
make	O
a	O
gradient	O
step	O
to	O
optimize	O
λl	O
(	O
f	O
(	O
xi	O
)	O
,	O
f	O
(	O
xj	O
)	O
,	O
1	O
)	O
,	O
finally	O
,	O
pick	O
a	O
random	O
unlabeled	O
example	O
xk	O
,	O
which	O
with	O
high	O
probability	O
is	O
dissimilar	O
to	O
xi	O
,	O
and	O
make	O
a	O
gradient	O
step	O
to	O
optimize	O
λl	O
(	O
f	O
(	O
xi	O
)	O
,	O
f	O
(	O
xk	O
)	O
,	O
0	O
)	O
.	O
note	O
that	O
this	O
technique	O
is	O
effective	O
because	O
it	O
can	O
leverage	O
massive	O
amounts	O
of	O
data	O
.	O
in	O
a	O
related	O
approach	O
,	O
(	O
collobert	O
and	O
weston	O
2008	O
)	O
trained	O
a	O
neural	B
network	I
to	O
distinguish	O
valid	O
english	O
sentences	O
from	O
invalid	O
ones	O
.	O
this	O
was	O
done	O
by	O
taking	O
all	O
631	O
million	O
words	O
from	O
english	O
wikipedia	O
(	O
en.wikipedia.org	O
)	O
,	O
and	O
then	O
creating	O
windows	O
of	O
length	O
11	O
containing	O
neighboring	O
words	O
.	O
this	O
constitutes	O
the	O
positive	B
examples	I
.	O
to	O
create	O
negative	B
examples	I
,	O
the	O
middle	O
word	O
of	O
each	O
window	O
was	O
replaced	O
by	O
a	O
random	O
english	O
word	O
(	O
this	O
is	O
likely	O
to	O
be	O
an	O
“	O
invalid	O
”	O
sentence	O
—	O
either	O
grammatically	O
and/or	O
semantically	O
—	O
with	O
high	O
probability	O
)	O
.	O
this	O
neural	B
network	I
was	O
then	O
trained	O
over	O
the	O
course	O
of	O
1	O
week	O
,	O
and	O
its	O
latent	B
representation	O
was	O
then	O
used	O
as	O
the	O
input	O
to	O
a	O
supervised	O
semantic	O
role	O
labeling	O
task	O
,	O
for	O
which	O
very	O
little	O
labeled	O
training	O
data	O
is	O
available	O
.	O
(	O
see	O
also	O
(	O
ando	O
and	O
zhang	O
2005	O
)	O
for	O
related	O
work	O
.	O
)	O
16.5.7	O
bayesian	O
inference	B
*	O
although	O
map	O
estimation	O
is	O
a	O
succesful	O
way	O
to	O
reduce	O
overﬁtting	B
,	O
there	O
are	O
still	O
some	O
good	O
reasons	O
to	O
want	O
to	O
adopt	O
a	O
fully	O
bayesian	O
approach	O
to	O
“	O
ﬁtting	O
”	O
neural	B
networks	I
:	O
•	O
integrating	O
out	O
the	O
parameters	O
instead	O
of	O
optimizing	O
them	O
is	O
a	O
much	O
stronger	O
form	O
of	O
regu-	O
larization	O
than	O
map	O
estimation	O
.	O
•	O
we	O
can	O
use	O
bayesian	O
model	B
selection	I
to	O
determine	O
things	O
like	O
the	O
hyper-parameter	O
settings	O
and	O
the	O
number	O
of	O
hidden	B
units	I
.	O
this	O
is	O
likely	O
to	O
be	O
much	O
faster	O
than	O
cross	B
validation	I
,	O
especially	O
if	O
we	O
have	O
many	O
hyper-parameters	B
(	O
e.g.	O
,	O
as	O
in	O
ard	O
)	O
.	O
•	O
modelling	O
uncertainty	B
in	O
the	O
parameters	O
will	O
induce	O
uncertainty	B
in	O
our	O
predictive	B
distribu-	O
tions	O
,	O
which	O
is	O
important	O
for	O
certain	O
problems	O
such	O
as	O
active	B
learning	I
and	O
risk-averse	O
decision	B
making	O
.	O
16.5.	O
feedforward	O
neural	O
networks	O
(	O
multilayer	O
perceptrons	O
)	O
577	O
•	O
we	O
can	O
use	O
online	O
inference	O
methods	O
,	O
such	O
as	O
the	O
extended	O
kalman	O
ﬁlter	O
,	O
to	O
do	O
online	O
learning	B
(	O
haykin	O
2001	O
)	O
.	O
one	O
can	O
adopt	O
a	O
variety	O
of	O
approximate	O
bayesian	O
inference	B
techniques	O
in	O
this	O
context	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
the	O
laplace	O
approximation	O
,	O
ﬁrst	O
suggested	O
in	O
(	O
mackay	O
1992	O
,	O
1995b	O
)	O
.	O
one	O
can	O
also	O
use	O
hybrid	O
monte	O
carlo	O
(	O
neal	O
1996	O
)	O
,	O
or	O
variational	O
bayes	O
(	O
hinton	O
and	O
camp	O
1993	O
;	O
barber	O
and	O
bishop	O
1998	O
)	O
.	O
16.5.7.1	O
parameter	B
posterior	O
for	O
regression	B
we	O
start	O
by	O
considering	O
regression	B
,	O
following	O
the	O
presentation	O
of	O
(	O
bishop	O
2006a	O
,	O
sec	O
5.7	O
)	O
,	O
which	O
summarizes	O
the	O
work	O
of	O
(	O
mackay	O
1992	O
,	O
1995b	O
)	O
.	O
we	O
will	O
use	O
a	O
prior	O
of	O
the	O
form	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
w|0	O
,	O
(	O
1/α	O
)	O
i	O
)	O
,	O
where	O
w	O
represents	O
all	O
the	O
weights	O
combined	O
.	O
we	O
will	O
denote	O
the	O
precision	B
of	O
the	O
noise	O
by	O
β	O
=	O
1/σ2	O
.	O
the	O
posterior	O
can	O
be	O
approximated	O
as	O
follows	O
:	O
p	O
(	O
w|d	O
,	O
α	O
,	O
β	O
)	O
∝	O
exp	O
(	O
−e	O
(	O
w	O
)	O
)	O
e	O
(	O
w	O
)	O
(	O
cid:2	O
)	O
βed	O
(	O
w	O
)	O
+αe	O
w	O
(	O
w	O
)	O
(	O
yn	O
−	O
f	O
(	O
xn	O
,	O
w	O
)	O
)	O
2	O
ed	O
(	O
w	O
)	O
(	O
cid:2	O
)	O
1	O
2	O
ew	O
(	O
w	O
)	O
(	O
cid:2	O
)	O
1	O
2	O
wt	O
w	O
n	O
(	O
cid:4	O
)	O
n=1	O
(	O
16.82	O
)	O
(	O
16.83	O
)	O
(	O
16.84	O
)	O
(	O
16.85	O
)	O
where	O
ed	O
is	O
the	O
data	O
error	O
,	O
ew	O
is	O
the	O
prior	O
error	O
,	O
and	O
e	O
is	O
the	O
overall	O
error	O
(	O
negative	O
log	O
prior	O
plus	O
log	O
likelihood	O
)	O
.	O
now	O
let	O
us	O
make	O
a	O
second-order	O
taylor	O
series	O
approximation	O
of	O
e	O
(	O
w	O
)	O
around	O
its	O
minimum	O
(	O
the	O
map	O
estimate	O
)	O
e	O
(	O
w	O
)	O
≈	O
e	O
(	O
wm	O
p	O
)	O
+	O
1	O
2	O
(	O
w	O
−	O
wm	O
p	O
)	O
t	O
a	O
(	O
w	O
−	O
wm	O
p	O
)	O
where	O
a	O
is	O
the	O
hessian	O
of	O
e	O
:	O
(	O
16.86	O
)	O
a	O
=	O
∇∇e	O
(	O
wm	O
p	O
)	O
=	O
βh	O
+	O
αi	O
(	O
16.87	O
)	O
where	O
h	O
=	O
∇∇ed	O
(	O
wm	O
p	O
)	O
is	O
the	O
hessian	O
of	O
the	O
data	O
error	O
.	O
this	O
can	O
be	O
computed	O
exactly	O
in	O
o	O
(	O
d2	O
)	O
time	O
,	O
where	O
d	O
is	O
the	O
number	O
of	O
parameters	O
,	O
using	O
a	O
variant	O
of	O
backpropagation	B
(	O
see	O
if	O
we	O
use	O
a	O
quasi-newton	O
method	O
to	O
ﬁnd	O
(	O
bishop	O
2006a	O
,	O
sec	O
5.4	O
)	O
for	O
details	O
)	O
.	O
alternatively	O
,	O
the	O
mode	B
,	O
we	O
can	O
use	O
its	O
internally	O
computed	O
(	O
low-rank	O
)	O
approximation	O
to	O
h.	O
(	O
note	O
that	O
diagonal	B
approximations	O
of	O
h	O
are	O
usually	O
very	O
inaccurate	O
.	O
)	O
in	O
either	O
case	O
,	O
using	O
this	O
quadratic	O
approximation	O
,	O
the	O
posterior	O
becomes	O
gaussian	O
:	O
p	O
(	O
w|α	O
,	O
β	O
,	O
d	O
)	O
≈	O
n	O
(	O
w|wm	O
p	O
,	O
a−1	O
)	O
(	O
16.88	O
)	O
578	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
16.5.7.2	O
parameter	B
posterior	O
for	O
classiﬁcation	B
the	O
classiﬁcation	B
case	O
is	O
the	O
same	O
as	O
the	O
regression	B
case	O
,	O
except	O
β	O
=	O
1	O
and	O
ed	O
is	O
a	O
cross-	O
entropy	B
error	O
of	O
the	O
form	O
ed	O
(	O
w	O
)	O
(	O
cid:2	O
)	O
[	O
yn	O
ln	O
f	O
(	O
xn	O
,	O
w	O
)	O
+	O
(	O
1−	O
yn	O
)	O
ln	O
f	O
(	O
xn	O
,	O
w	O
)	O
]	O
n	O
(	O
cid:4	O
)	O
n=1	O
(	O
cid:12	O
)	O
(	O
16.89	O
)	O
(	O
16.90	O
)	O
(	O
16.91	O
)	O
16.5.7.3	O
16.5.7.4	O
predictive	B
posterior	O
for	O
regression	B
the	O
posterior	B
predictive	I
density	I
is	O
given	O
by	O
p	O
(	O
y|x	O
,	O
d	O
,	O
α	O
,	O
β	O
)	O
=	O
n	O
(	O
y|f	O
(	O
x	O
,	O
w	O
)	O
,	O
1/β	O
)	O
n	O
(	O
w|wm	O
p	O
,	O
a−1	O
)	O
dw	O
this	O
is	O
not	O
analytically	O
tractable	O
because	O
of	O
the	O
nonlinearity	O
of	O
f	O
(	O
x	O
,	O
w	O
)	O
.	O
let	O
us	O
therefore	O
construct	O
a	O
ﬁrst-order	O
taylor	O
series	O
approximation	O
around	O
the	O
mode	B
:	O
f	O
(	O
x	O
,	O
w	O
)	O
≈	O
f	O
(	O
x	O
,	O
wm	O
p	O
)	O
+g	O
t	O
(	O
w	O
−	O
wm	O
p	O
)	O
where	O
g	O
=	O
∇wf	O
(	O
x	O
,	O
w	O
)	O
|w=wm	O
p	O
(	O
16.92	O
)	O
(	O
16.93	O
)	O
we	O
now	O
have	O
a	O
linear-gaussian	O
model	O
with	O
a	O
gaussian	O
prior	O
on	O
the	O
weights	O
.	O
from	O
equation	O
4.126	O
we	O
have	O
p	O
(	O
y|x	O
,	O
d	O
,	O
α	O
,	O
β	O
)	O
≈	O
n	O
(	O
y|f	O
(	O
x	O
,	O
wm	O
p	O
)	O
,	O
σ2	O
(	O
x	O
)	O
)	O
where	O
the	O
predictive	B
variance	O
depends	O
on	O
the	O
input	O
x	O
as	O
follows	O
:	O
σ2	O
(	O
x	O
)	O
=	O
β−1	O
+	O
gt	O
a−1g	O
(	O
16.94	O
)	O
(	O
16.95	O
)	O
the	O
error	O
bars	O
will	O
be	O
larger	O
in	O
regions	O
of	O
input	O
space	O
where	O
we	O
have	O
little	O
training	O
data	O
.	O
see	O
figure	O
16.19	O
for	O
an	O
example	O
.	O
predictive	B
posterior	O
for	O
classiﬁcation	B
in	O
this	O
section	O
,	O
we	O
discuss	O
how	O
to	O
approximate	O
p	O
(	O
y|x	O
,	O
d	O
)	O
in	O
the	O
case	O
of	O
binary	B
classiﬁcation	I
.	O
the	O
situation	O
is	O
similar	B
to	O
the	O
case	O
of	O
logistic	B
regression	I
,	O
discussed	O
in	O
section	O
8.4.4	O
,	O
except	O
in	O
addition	O
the	O
posterior	O
predictive	O
mean	O
is	O
a	O
non-linear	O
function	O
of	O
w.	O
speciﬁcally	O
,	O
we	O
have	O
μ	O
=	O
e	O
[	O
y|x	O
,	O
w	O
]	O
=	O
sigm	O
(	O
a	O
(	O
x	O
,	O
w	O
)	O
)	O
,	O
where	O
a	O
(	O
x	O
,	O
w	O
)	O
is	O
the	O
pre-synaptic	O
output	O
of	O
the	O
ﬁnal	O
layer	O
.	O
let	O
us	O
make	O
a	O
linear	O
approximation	O
to	O
this	O
:	O
a	O
(	O
x	O
,	O
w	O
)	O
≈	O
am	O
p	O
(	O
x	O
)	O
+g	O
t	O
(	O
w	O
−	O
wm	O
p	O
)	O
(	O
16.96	O
)	O
where	O
am	O
p	O
(	O
x	O
)	O
=a	O
(	O
x	O
,	O
wm	O
p	O
)	O
and	O
g	O
=	O
∇xa	O
(	O
x	O
,	O
wm	O
p	O
)	O
can	O
be	O
found	O
by	O
a	O
modiﬁed	O
version	O
of	O
backpropagation	B
.	O
clearly	O
p	O
(	O
a|x	O
,	O
d	O
)	O
≈	O
n	O
(	O
a	O
(	O
x	O
,	O
wm	O
p	O
)	O
,	O
g	O
(	O
x	O
)	O
t	O
a−1g	O
(	O
x	O
)	O
)	O
(	O
16.97	O
)	O
16.5.	O
feedforward	O
neural	O
networks	O
(	O
multilayer	O
perceptrons	O
)	O
579	O
t	O
e	O
g	O
r	O
a	O
t	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
0	O
data	O
function	O
network	O
error	O
bars	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
input	O
(	O
a	O
)	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
0	O
data	O
function	O
prediction	O
samples	B
0.2	O
0.4	O
0.6	O
0.8	O
1	O
(	O
b	O
)	O
figure	O
16.19	O
the	O
posterior	B
predictive	I
density	I
for	O
an	O
mlp	O
with	O
3	O
hidden	B
nodes	I
,	O
trained	O
on	O
16	O
data	O
points	O
.	O
the	O
dashed	O
green	O
line	O
is	O
the	O
true	O
function	O
.	O
(	O
a	O
)	O
result	O
of	O
using	O
a	O
laplace	O
approximation	O
,	O
after	O
performing	O
empirical	O
bayes	O
to	O
optimize	O
the	O
hyperparameters	O
.	O
the	O
solid	O
red	O
line	O
is	O
the	O
posterior	B
mean	I
prediction	O
,	O
and	O
the	O
dotted	O
blue	O
lines	O
are	O
1	O
standard	B
deviation	I
above	O
and	O
below	O
the	O
mean	B
.	O
figure	O
generated	O
by	O
mlpregevidencedemo	O
.	O
(	O
b	O
)	O
result	O
of	O
using	O
hybrid	O
monte	O
carlo	O
,	O
using	O
the	O
same	O
trained	O
hyperparameters	O
as	O
in	O
(	O
a	O
)	O
.	O
the	O
solid	O
red	O
line	O
is	O
the	O
posterior	B
mean	I
prediction	O
,	O
and	O
the	O
dotted	O
blue	O
lines	O
are	O
samples	B
from	O
the	O
posterior	O
predictive	O
.	O
figure	O
generated	O
by	O
mlpreghmcdemo	O
,	O
written	O
by	O
ian	O
nabney	O
.	O
(	O
cid:12	O
)	O
hence	O
the	O
posterior	O
predictive	O
for	O
the	O
output	O
is	O
p	O
(	O
y	O
=	O
1|x	O
,	O
d	O
)	O
=	O
sigm	O
(	O
a	O
)	O
p	O
(	O
a|x	O
,	O
d	O
)	O
da	O
≈	O
sigm	O
(	O
κ	O
(	O
σ2	O
a	O
)	O
bt	O
wm	O
p	O
)	O
where	O
κ	O
is	O
deﬁned	O
by	O
equation	O
8.70	O
,	O
which	O
we	O
repeat	O
here	O
for	O
convenience	O
:	O
κ	O
(	O
σ2	O
)	O
(	O
cid:2	O
)	O
(	O
1	O
+	O
πσ2/8	O
)	O
−	O
1	O
2	O
(	O
16.98	O
)	O
(	O
16.99	O
)	O
of	O
course	O
,	O
a	O
simpler	O
(	O
and	O
potentially	O
more	O
accurate	O
)	O
alternative	O
to	O
this	O
is	O
to	O
draw	O
a	O
few	O
samples	B
from	O
the	O
gaussian	O
posterior	O
and	O
to	O
approximate	O
the	O
posterior	O
predictive	O
using	O
monte	O
carlo	O
.	O
in	O
either	O
case	O
,	O
the	O
effect	O
of	O
taking	O
uncertainty	B
of	O
the	O
parameters	O
into	O
account	O
,	O
as	O
in	O
sec-	O
tion	O
8.4.4	O
,	O
is	O
to	O
“	O
moderate	O
”	O
the	O
conﬁdence	O
of	O
the	O
output	O
;	O
the	O
decision	B
boundary	I
itself	O
is	O
unaf-	O
fected	O
,	O
however	O
.	O
16.5.7.5	O
ard	O
for	O
neural	B
networks	I
once	O
we	O
have	O
made	O
the	O
laplace	O
approximation	O
to	O
the	O
posterior	O
,	O
we	O
can	O
optimize	O
the	O
marginal	B
likelihood	I
wrt	O
the	O
hyper-parameters	B
α	O
using	O
the	O
same	O
ﬁxed-point	O
equations	O
as	O
in	O
section	O
13.7.4.2.	O
typically	O
we	O
use	O
one	O
hyper-parameter	O
for	O
the	O
weight	B
vector	I
leaving	O
each	O
node	O
,	O
to	O
achieve	O
an	O
effect	O
similar	B
to	O
group	B
lasso	I
(	O
section	O
13.5.1	O
)	O
.	O
that	O
is	O
,	O
the	O
prior	O
has	O
the	O
form	O
n	O
(	O
v	O
:	O
,i|0	O
,	O
1	O
αv	O
,	O
i	O
i	O
)	O
n	O
(	O
w	O
:	O
,j|0	O
,	O
1	O
αw	O
,	O
j	O
i	O
)	O
(	O
16.100	O
)	O
if	O
we	O
ﬁnd	O
αv	O
,	O
i	O
=	O
∞	O
,	O
then	O
input	O
feature	O
i	O
is	O
irrelevant	O
,	O
and	O
its	O
weight	B
vector	I
v	O
:	O
,i	O
is	O
pruned	O
out	O
.	O
similarly	O
,	O
if	O
we	O
ﬁnd	O
αw	O
,	O
j	O
=	O
∞	O
,	O
then	O
hidden	B
feature	O
j	O
is	O
irrelevant	O
.	O
this	O
is	O
known	O
as	O
automatic	O
d	O
(	O
cid:20	O
)	O
i=1	O
p	O
(	O
θ	O
)	O
=	O
h	O
(	O
cid:20	O
)	O
j=1	O
580	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
relevancy	O
determination	O
or	O
ard	O
,	O
which	O
was	O
discussed	O
in	O
detail	O
in	O
section	O
13.7.	O
applying	O
this	O
to	O
neural	B
networks	I
gives	O
us	O
an	O
efficient	O
means	O
of	O
variable	O
selection	O
in	O
non-linear	O
models	O
.	O
the	O
software	O
package	O
netlab	O
contains	O
a	O
simple	O
example	O
of	O
ard	O
applied	O
to	O
a	O
neural	B
network	I
,	O
called	O
demard	O
.	O
this	O
demo	O
creates	O
some	O
data	O
according	O
to	O
a	O
nonlinear	O
regression	B
function	O
f	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
=	O
sin	O
(	O
2πx1	O
)	O
+	O
	O
,	O
where	O
x2	O
is	O
a	O
noisy	O
copy	O
of	O
x1	O
.	O
we	O
see	O
that	O
x2	O
and	O
x3	O
are	O
irrelevant	O
for	O
predicting	O
the	O
target	O
.	O
however	O
,	O
x2	O
is	O
correlated	O
with	O
x1	O
,	O
which	O
is	O
relevant	O
.	O
using	O
ard	O
,	O
the	O
ﬁnal	O
hyper-parameters	B
are	O
as	O
follows	O
:	O
α	O
=	O
[	O
0.2	O
,	O
21.4	O
,	O
249001.8	O
]	O
(	O
16.101	O
)	O
this	O
clearly	O
indicates	O
that	O
feature	O
3	O
is	O
irrelevant	O
,	O
feature	O
2	O
is	O
only	O
weakly	O
relevant	O
,	O
and	O
feature	O
1	O
is	O
very	O
relevant	O
.	O
16.6	O
ensemble	B
learning	I
(	O
cid:4	O
)	O
m∈m	O
ensemble	B
learning	I
refers	O
to	O
learning	B
a	O
weighted	O
combination	O
of	O
base	O
models	O
of	O
the	O
form	O
f	O
(	O
y|x	O
,	O
π	O
)	O
=	O
wmfm	O
(	O
y|x	O
)	O
(	O
16.102	O
)	O
where	O
the	O
wm	O
are	O
tunable	O
parameters	O
.	O
ensemble	B
learning	I
is	O
sometimes	O
called	O
a	O
committee	B
method	I
,	O
since	O
each	O
base	O
model	O
fm	O
gets	O
a	O
weighted	O
“	O
vote	O
”	O
.	O
clearly	O
ensemble	B
learning	I
is	O
closely	O
related	O
to	O
learning	B
adaptive-basis	O
function	O
models	O
.	O
in	O
fact	O
,	O
one	O
can	O
argue	O
that	O
a	O
neural	O
net	O
is	O
an	O
ensemble	B
method	O
,	O
where	O
fm	O
represents	O
the	O
m	O
’	O
th	O
hidden	B
unit	O
,	O
and	O
wm	O
are	O
the	O
output	O
layer	O
weights	O
.	O
also	O
,	O
we	O
can	O
think	O
of	O
boosting	B
as	O
kind	O
of	O
ensemble	B
learning	I
,	O
where	O
the	O
weights	O
on	O
the	O
base	O
models	O
are	O
determined	O
sequentially	O
.	O
below	O
we	O
describe	O
some	O
other	O
forms	O
of	O
ensemble	B
learning	I
.	O
16.6.1	O
stacking	B
an	O
obvious	O
way	O
to	O
estimate	O
the	O
weights	O
in	O
equation	O
16.102	O
is	O
to	O
use	O
n	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
m	O
(	O
cid:4	O
)	O
m	O
(	O
cid:4	O
)	O
ˆw	O
=	O
argmin	O
w	O
i=1	O
m=1	O
l	O
(	O
yi	O
,	O
wmfm	O
(	O
x	O
)	O
)	O
(	O
16.103	O
)	O
however	O
,	O
this	O
will	O
result	O
in	O
overﬁtting	B
,	O
with	O
wm	O
being	O
large	O
for	O
the	O
most	O
complex	O
model	O
.	O
a	O
simple	O
solution	O
to	O
this	O
is	O
to	O
use	O
cross-validation	O
.	O
in	O
particular	O
,	O
we	O
can	O
use	O
the	O
loocv	O
estimate	O
ˆw	O
=	O
argmin	O
w	O
l	O
(	O
yi	O
,	O
i=1	O
m=1	O
wm	O
ˆf−i	O
m	O
(	O
x	O
)	O
)	O
(	O
16.104	O
)	O
where	O
ˆf−i	O
m	O
(	O
x	O
)	O
is	O
the	O
predictor	O
obtained	O
by	O
training	O
on	O
data	O
excluding	O
(	O
xi	O
,	O
yi	O
)	O
.	O
this	O
is	O
known	O
as	O
stacking	B
,	O
which	O
stands	O
for	O
“	O
stacked	O
generalization	O
”	O
(	O
wolpert	O
1992	O
)	O
.	O
this	O
technique	O
is	O
more	O
robust	B
to	O
the	O
case	O
where	O
the	O
“	O
true	O
”	O
model	O
is	O
not	O
in	O
the	O
model	O
class	O
than	O
standard	O
bma	O
(	O
clarke	O
2003	O
)	O
.	O
this	O
approach	O
was	O
used	O
by	O
the	O
netﬂix	O
team	O
known	O
as	O
“	O
the	O
ensemble	B
”	O
,	O
which	O
tied	B
the	O
submission	O
of	O
the	O
winning	O
team	O
(	O
bellkor	O
’	O
s	O
pragmatic	O
chaos	O
)	O
in	O
terms	O
of	O
accuracy	O
(	O
sill	O
et	O
al	O
.	O
2009	O
)	O
.	O
stacking	B
has	O
also	O
been	O
used	O
for	O
problems	O
such	O
as	O
image	B
segmentation	I
and	O
labeling	O
.	O
16.6.	O
ensemble	B
learning	I
581	O
class	O
c1	O
c2	O
c3	O
c4	O
c5	O
c6	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
···	O
c15	O
···	O
1	O
···	O
0	O
···	O
0	O
0	O
0	O
1	O
...	O
0	O
9	O
0	O
1	O
1	O
1	O
table	O
16.2	O
part	O
of	O
a	O
15-bit	O
error-correcting	O
output	O
code	O
for	O
a	O
10-class	O
problem	O
.	O
each	O
row	O
deﬁnes	O
a	O
two-class	O
problem	O
.	O
based	O
on	O
table	O
16.1	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
16.6.2	O
error-correcting	B
output	I
codes	I
an	O
interesting	O
form	O
of	O
ensemble	B
learning	I
is	O
known	O
as	O
error-correcting	B
output	I
codes	I
or	O
ecoc	O
(	O
dietterich	O
and	O
bakiri	O
1995	O
)	O
,	O
which	O
can	O
be	O
used	O
in	O
the	O
context	O
of	O
multi-class	O
classiﬁcation	O
.	O
the	O
idea	O
is	O
that	O
we	O
are	O
trying	O
to	O
decode	O
a	O
symbol	O
(	O
namely	O
the	O
class	O
label	O
)	O
which	O
has	O
c	O
possible	O
states	O
.	O
we	O
could	O
use	O
a	O
bit	O
vector	O
of	O
length	O
b	O
=	O
(	O
cid:13	O
)	O
log2	O
c	O
(	O
cid:14	O
)	O
to	O
encode	O
the	O
class	O
label	O
,	O
and	O
train	O
b	O
separate	O
binary	O
classiﬁers	O
to	O
predict	O
each	O
bit	O
.	O
however	O
,	O
by	O
using	O
more	O
bits	B
,	O
and	O
by	O
designing	O
the	O
codewords	O
to	O
have	O
maximal	O
hamming	O
distance	O
from	O
each	O
other	O
,	O
we	O
get	O
a	O
method	O
that	O
is	O
more	O
resistant	O
to	O
individual	O
bit-ﬂipping	O
errors	O
(	O
misclassiﬁcation	O
)	O
.	O
for	O
example	O
,	O
in	O
table	O
16.2	O
,	O
we	O
use	O
b	O
=	O
15	O
bits	B
to	O
encode	O
a	O
c	O
=	O
10	O
class	O
problem	O
.	O
the	O
minimum	O
hamming	O
distance	O
between	O
any	O
pair	O
of	O
rows	O
is	O
7.	O
the	O
decoding	B
rule	O
is	O
|ccb	O
−	O
ˆpb	O
(	O
x	O
)	O
|	O
(	O
16.105	O
)	O
b	O
(	O
cid:4	O
)	O
b=1	O
ˆc	O
(	O
x	O
)	O
=	O
min	O
c	O
where	O
ccb	O
is	O
the	O
b	O
’	O
th	O
bit	O
of	O
the	O
codeword	O
for	O
class	O
c.	O
(	O
james	O
and	O
hastie	O
1998	O
)	O
showed	O
that	O
a	O
random	O
code	O
worked	O
just	O
as	O
well	O
as	O
the	O
optimal	O
code	O
:	O
both	O
methods	O
work	O
by	O
averaging	O
the	O
results	O
of	O
multiple	O
classiﬁers	O
,	O
thereby	O
reducing	O
variance	B
.	O
16.6.3	O
ensemble	B
learning	I
is	O
not	O
equivalent	O
to	O
bayes	O
model	O
averaging	O
in	O
section	O
5.3	O
,	O
we	O
discussed	O
bayesian	O
model	B
selection	I
.	O
an	O
alternative	O
to	O
picking	O
the	O
best	O
model	O
,	O
and	O
then	O
using	O
this	O
to	O
make	O
predictions	O
,	O
is	O
to	O
make	O
a	O
weighted	B
average	I
of	O
the	O
predictions	O
made	O
by	O
each	O
model	O
,	O
i.e.	O
,	O
we	O
compute	O
p	O
(	O
y|x	O
,	O
d	O
)	O
=	O
p	O
(	O
y|x	O
,	O
m	O
,	O
d	O
)	O
p	O
(	O
m|d	O
)	O
(	O
16.106	O
)	O
(	O
cid:4	O
)	O
m∈m	O
this	O
is	O
called	O
bayes	O
model	O
averaging	O
(	O
bma	O
)	O
,	O
and	O
can	O
sometimes	O
give	O
better	O
performance	O
than	O
using	O
any	O
single	O
model	O
(	O
hoeting	O
et	O
al	O
.	O
1999	O
)	O
.	O
of	O
course	O
,	O
averaging	O
over	O
all	O
models	O
is	O
typically	O
computationally	O
infeasible	O
(	O
analytical	O
integration	O
is	O
obviously	O
not	O
possible	O
in	O
a	O
discrete	B
space	O
,	O
although	O
one	O
can	O
sometimes	O
use	O
dynamic	B
programming	I
to	O
perform	O
the	O
computation	O
exactly	O
,	O
e.g.	O
,	O
(	O
meila	O
and	O
jaakkola	O
2006	O
)	O
)	O
.	O
a	O
simple	O
approximation	O
is	O
to	O
sample	O
a	O
few	O
models	O
from	O
the	O
posterior	O
.	O
an	O
even	O
simpler	O
approximation	O
(	O
and	O
the	O
one	O
most	O
widely	O
used	O
in	O
practice	O
)	O
is	O
to	O
just	O
use	O
the	O
map	O
model	O
.	O
it	O
is	O
important	O
to	O
note	O
that	O
bma	O
is	O
not	O
equivalent	O
to	O
ensemble	B
learning	I
(	O
minka	O
2000c	O
)	O
.	O
this	O
latter	O
technique	O
corresponds	O
to	O
enlarging	O
the	O
model	O
space	O
,	O
by	O
deﬁning	O
a	O
single	O
new	O
model	O
582	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
model	O
bst-dt	O
rf	O
bag-dt	O
svm	O
ann	O
knn	O
bst-stmp	O
dt	O
logreg	O
nb	O
1st	O
0.580	O
0.390	O
0.030	O
0.000	O
0.000	O
0.000	O
0.000	O
0.000	O
0.000	O
0.000	O
2nd	O
0.228	O
0.525	O
0.232	O
0.008	O
0.007	O
0.000	O
0.000	O
0.000	O
0.000	O
0.000	O
3rd	O
0.160	O
0.084	O
0.571	O
0.148	O
0.035	O
0.000	O
0.002	O
0.000	O
0.000	O
0.000	O
4th	O
0.023	O
0.001	O
0.150	O
0.574	O
0.230	O
0.009	O
0.013	O
0.000	O
0.000	O
0.000	O
5th	O
0.009	O
0.000	O
0.017	O
0.240	O
0.606	O
0.114	O
0.014	O
0.000	O
0.000	O
0.000	O
6th	O
0.000	O
0.000	O
0.000	O
0.029	O
0.122	O
0.592	O
0.257	O
0.000	O
0.000	O
0.000	O
7th	O
0.000	O
0.000	O
0.000	O
0.001	O
0.000	O
0.245	O
0.710	O
0.004	O
0.040	O
0.000	O
8th	O
0.000	O
0.000	O
0.000	O
0.000	O
0.000	O
0.038	O
0.004	O
0.616	O
0.312	O
0.030	O
9th	O
0.000	O
0.000	O
0.000	O
0.000	O
0.000	O
0.002	O
0.000	O
0.291	O
0.423	O
0.284	O
10th	O
0.000	O
0.000	O
0.000	O
0.000	O
0.000	O
0.000	O
0.000	O
0.089	O
0.225	O
0.686	O
table	O
16.3	O
fraction	O
of	O
time	O
each	O
method	O
achieved	O
a	O
speciﬁed	O
rank	O
,	O
when	O
sorting	O
by	O
mean	B
performance	O
across	O
11	O
datasets	O
and	O
8	O
metrics	O
.	O
based	O
on	O
table	O
4	O
of	O
(	O
caruana	O
and	O
niculescu-mizil	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
alexandru	O
niculescu-mizil	O
.	O
which	O
is	O
a	O
convex	B
combination	I
of	O
base	O
models	O
,	O
as	O
follows	O
:	O
p	O
(	O
y|x	O
,	O
π	O
)	O
=	O
πmp	O
(	O
y|x	O
,	O
m	O
)	O
(	O
cid:4	O
)	O
m∈m	O
(	O
cid:21	O
)	O
in	O
principle	O
,	O
we	O
can	O
now	O
perform	O
bayesian	O
inference	B
to	O
compute	O
p	O
(	O
π|d	O
)	O
;	O
we	O
then	O
make	O
pre-	O
dictions	O
using	O
p	O
(	O
y|x	O
,	O
d	O
)	O
=	O
p	O
(	O
y|x	O
,	O
π	O
)	O
p	O
(	O
π|d	O
)	O
dπ	O
.	O
however	O
,	O
it	O
is	O
much	O
more	O
common	O
to	O
use	O
point	O
estimation	O
methods	O
for	O
π	O
,	O
as	O
we	O
saw	O
above	O
.	O
(	O
16.107	O
)	O
16.7	O
experimental	O
comparison	O
we	O
have	O
described	O
many	O
different	O
methods	O
for	O
classiﬁcation	B
and	O
regression	B
.	O
which	O
one	O
should	O
you	O
use	O
?	O
that	O
depends	O
on	O
which	O
inductive	B
bias	I
you	O
think	O
is	O
most	O
appropriate	O
for	O
your	O
domain	O
.	O
usually	O
this	O
is	O
hard	O
to	O
assess	O
,	O
so	O
it	O
is	O
common	O
to	O
just	O
try	O
several	O
different	O
methods	O
,	O
and	O
see	O
how	O
they	O
perform	O
empirically	O
.	O
below	O
we	O
summarize	O
two	O
such	O
comparisons	O
that	O
were	O
carefully	O
conducted	O
(	O
although	O
the	O
data	O
sets	O
that	O
were	O
used	O
are	O
relatively	O
small	O
)	O
.	O
see	O
the	O
website	O
mlcomp.org	O
for	O
a	O
distributed	O
way	O
to	O
perform	O
large	O
scale	O
comparisons	O
of	O
this	O
kind	O
.	O
of	O
course	O
,	O
we	O
must	O
always	O
remember	O
the	O
no	B
free	I
lunch	I
theorem	I
(	O
section	O
1.4.9	O
)	O
,	O
which	O
tells	O
us	O
that	O
there	O
is	O
no	O
universally	O
best	O
learning	B
method	O
.	O
16.7.1	O
low-dimensional	O
features	B
in	O
2006	O
,	O
rich	O
caruana	O
and	O
alex	O
niculescu-mizil	O
(	O
caruana	O
and	O
niculescu-mizil	O
2006	O
)	O
conducted	O
a	O
very	O
extensive	O
experimental	O
comparison	O
of	O
10	O
different	O
binary	B
classiﬁcation	I
methods	O
,	O
on	O
11	O
different	O
data	O
sets	O
.	O
the	O
11	O
data	O
sets	O
all	O
had	O
5000	O
training	O
cases	O
,	O
and	O
had	O
test	O
sets	O
containing	O
∼	O
10	O
,	O
000	O
examples	O
on	O
average	O
.	O
the	O
number	O
of	O
features	B
ranged	O
from	O
9	O
to	O
200	O
,	O
so	O
this	O
is	O
much	O
lower	O
dimensional	O
than	O
the	O
nips	O
2003	O
feature	B
selection	I
challenge	O
.	O
5-fold	O
cross	B
validation	I
was	O
used	O
to	O
assess	O
average	O
test	O
error	O
.	O
(	O
this	O
is	O
separate	O
from	O
any	O
internal	O
cv	O
a	O
method	O
may	O
need	O
to	O
use	O
for	O
model	B
selection	I
.	O
)	O
16.7.	O
experimental	O
comparison	O
583	O
the	O
methods	O
they	O
compared	O
are	O
as	O
follows	O
(	O
listed	O
in	O
roughly	O
decreasing	O
order	O
of	O
performance	O
,	O
as	O
assessed	O
by	O
table	O
16.3	O
)	O
:	O
•	O
bst-dt	O
:	O
boosted	O
decision	B
trees	I
•	O
rf	O
:	O
random	O
forest	O
•	O
bag-dt	O
:	O
bagged	O
decision	B
trees	I
•	O
svm	O
:	O
support	B
vector	I
machine	I
•	O
ann	O
:	O
artiﬁcial	O
neural	B
network	I
•	O
knn	O
:	O
k-nearest	O
neighbors	B
•	O
bst-stmp	O
:	O
boosted	O
stumps	O
•	O
dt	O
:	O
decision	B
tree	O
•	O
logreg	O
:	O
logistic	B
regression	I
•	O
nb	O
:	O
naive	O
bayes	O
they	O
used	O
8	O
different	O
performance	O
measures	O
,	O
which	O
can	O
be	O
divided	O
into	O
three	O
groups	O
.	O
thresh-	O
old	O
metrics	O
just	O
require	O
a	O
point	B
estimate	I
as	O
output	O
.	O
these	O
include	O
accuracy	O
,	O
f-score	O
(	O
sec-	O
tion	O
5.7.2.3	O
)	O
,	O
etc	O
.	O
ordering/	O
ranking	B
metrics	O
measure	O
how	O
well	O
positive	O
cases	O
are	O
ordered	O
before	O
the	O
negative	O
cases	O
.	O
these	O
include	O
area	O
under	O
the	O
roc	O
curve	O
(	O
section	O
5.7.2.1	O
)	O
,	O
average	B
precision	I
,	O
and	O
the	O
precision/recall	O
break	O
even	O
point	O
.	O
finally	O
,	O
the	O
probability	O
metrics	O
included	O
cross-entropy	B
(	O
log-loss	B
)	O
and	O
squared	B
error	I
,	O
(	O
y	O
−	O
ˆp	O
)	O
2.	O
methods	O
such	O
as	O
svms	O
that	O
do	O
not	O
produce	O
calibrated	O
probabilities	O
were	O
post-processed	O
using	O
platt	O
’	O
s	O
logistic	B
regression	I
trick	O
(	O
section	O
14.5.2.3	O
)	O
,	O
or	O
using	O
isotonic	O
regression	B
.	O
performance	O
measures	O
were	O
standardized	B
to	O
a	O
0:1	O
scale	O
so	O
they	O
could	O
be	O
compared	O
.	O
obviously	O
the	O
results	O
vary	O
by	O
dataset	O
and	O
by	O
metric	B
.	O
therefore	O
just	O
averaging	O
the	O
performance	O
does	O
not	O
necessarily	O
give	O
reliable	O
conclusions	O
.	O
however	O
,	O
one	O
can	O
perform	O
a	O
bootstrap	B
analysis	O
,	O
which	O
shows	O
how	O
robust	B
the	O
conclusions	O
are	O
to	O
such	O
changes	O
.	O
the	O
results	O
are	O
shown	O
in	O
table	O
16.3.	O
we	O
see	O
that	O
most	O
of	O
the	O
time	O
,	O
boosted	O
decision	B
trees	I
are	O
the	O
best	O
method	O
,	O
followed	O
by	O
random	B
forests	I
,	O
bagged	O
decision	B
trees	I
,	O
svms	O
and	O
neural	B
networks	I
.	O
however	O
,	O
the	O
following	O
methods	O
all	O
did	O
relatively	O
poorly	O
:	O
knn	O
,	O
stumps	O
,	O
single	O
decision	O
trees	O
,	O
logistic	B
regression	I
and	O
naive	O
bayes	O
.	O
these	O
results	O
are	O
generally	O
consistent	B
with	O
conventional	O
wisdom	O
of	O
practioners	O
in	O
the	O
ﬁeld	O
.	O
of	O
course	O
,	O
the	O
conclusions	O
may	O
change	O
if	O
there	O
the	O
features	B
are	O
high	O
dimensional	O
and/	O
or	O
there	O
are	O
lots	O
of	O
irrelevant	O
features	B
(	O
as	O
in	O
section	O
16.7.2	O
)	O
,	O
or	O
if	O
there	O
is	O
lots	O
of	O
noise	O
,	O
etc	O
.	O
16.7.2	O
high-dimensional	O
features	B
in	O
2003	O
,	O
the	O
nips	O
conference	O
ran	O
a	O
competition	O
where	O
the	O
goal	O
was	O
to	O
solve	O
binary	B
classiﬁcation	I
problems	O
with	O
large	O
numbers	O
of	O
(	O
mostly	O
irrelevant	O
)	O
features	B
,	O
given	O
small	O
training	O
sets	O
.	O
(	O
this	O
was	O
called	O
a	O
“	O
feature	B
selection	I
”	O
challenge	O
,	O
but	O
performance	O
was	O
measured	O
in	O
terms	O
of	O
predictive	B
accuracy	O
,	O
not	O
in	O
terms	O
of	O
the	O
ability	O
to	O
select	O
features	B
.	O
)	O
the	O
ﬁve	O
datasets	O
that	O
were	O
used	O
are	O
summarized	O
in	O
table	O
16.4.	O
the	O
term	O
probe	B
refers	O
to	O
artiﬁcal	O
variables	O
that	O
were	O
added	O
to	O
the	O
problem	O
to	O
make	O
it	O
harder	O
.	O
these	O
have	O
no	O
predictive	O
power	O
,	O
but	O
are	O
correlated	O
with	O
the	O
original	O
features	B
.	O
results	O
of	O
the	O
competition	O
are	O
discussed	O
in	O
(	O
guyon	O
et	O
al	O
.	O
2006	O
)	O
.	O
the	O
overall	O
winner	O
was	O
an	O
in	O
a	O
follow-up	O
study	O
approach	O
based	O
on	O
bayesian	O
neural	B
networks	I
(	O
neal	O
and	O
zhang	O
2006	O
)	O
.	O
584	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
type	O
dataset	O
aracene	O
dexter	O
dorothea	O
drug	O
discovery	O
gisette	O
madelon	O
domain	O
mass	O
spectrometry	O
dense	O
text	O
classiﬁcation	B
sparse	O
sparse	B
dense	O
dense	O
digit	O
recognition	O
artiﬁcial	O
d	O
10,000	O
20,000	O
100,000	O
5000	O
500	O
%	O
probes	O
ntrain	O
nval	O
ntest	O
30	O
50	O
50	O
30	O
96	O
100	O
300	O
800	O
6000	O
2000	O
700	O
2000	O
800	O
6500	O
1800	O
100	O
300	O
350	O
1000	O
600	O
table	O
16.4	O
datasets	O
,	O
the	O
features	B
are	O
binary	O
.	O
for	O
the	O
others	O
,	O
the	O
features	B
are	O
real-valued	O
.	O
summary	O
of	O
the	O
data	O
used	O
in	O
the	O
nips	O
2003	O
“	O
feature	B
selection	I
”	O
challenge	O
.	O
for	O
the	O
dorothea	O
method	O
hmc	O
mlp	O
boosted	O
mlp	O
bagged	O
mlp	O
boosted	O
trees	O
random	B
forests	I
screened	O
features	B
ard	O
avg	O
rank	O
1.5	O
3.8	O
3.6	O
3.4	O
2.7	O
avg	O
time	O
384	O
(	O
138	O
)	O
9.4	O
(	O
8.6	O
)	O
3.5	O
(	O
1.1	O
)	O
3.03	O
(	O
2.5	O
)	O
1.9	O
(	O
1.7	O
)	O
avg	O
rank	O
1.6	O
2.2	O
4.0	O
4.0	O
3.2	O
avg	O
time	O
600	O
(	O
186	O
)	O
35.6	O
(	O
33.5	O
)	O
6.4	O
(	O
4.4	O
)	O
34.1	O
(	O
32.4	O
)	O
11.2	O
(	O
9.3	O
)	O
performance	O
of	O
different	O
methods	O
on	O
the	O
nips	O
2003	O
“	O
feature	B
selection	I
”	O
challenge	O
.	O
table	O
16.5	O
(	O
hmc	O
stands	O
for	O
hybrid	O
monte	O
carlo	O
;	O
see	O
section	O
24.5.4	O
.	O
)	O
we	O
report	O
the	O
average	O
rank	O
(	O
lower	O
is	O
better	O
)	O
across	O
the	O
5	O
datasets	O
.	O
we	O
also	O
report	O
the	O
average	O
training	O
time	O
in	O
minutes	O
(	O
standard	B
error	I
in	O
brackets	O
)	O
.	O
the	O
mcmc	O
and	O
bagged	O
mlps	O
use	O
two	O
hidden	B
layers	O
of	O
20	O
and	O
8	O
units	O
.	O
the	O
boosted	O
mlps	O
use	O
one	O
hidden	B
layer	I
with	O
2	O
or	O
4	O
hidden	B
units	I
.	O
the	O
boosted	O
trees	O
used	O
depths	O
between	O
2	O
and	O
9	O
,	O
and	O
shrinkage	B
between	O
0.001	O
and	O
0.1.	O
each	O
tree	B
was	O
trained	O
on	O
80	O
%	O
of	O
the	O
data	O
chosen	O
at	O
random	O
at	O
each	O
step	O
(	O
so-called	O
stochastic	B
gradient	I
boosting	I
)	O
.	O
from	O
table	O
11.3	O
of	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
)	O
.	O
(	O
johnson	O
2009	O
)	O
,	O
bayesian	O
neural	O
nets	O
(	O
mlps	O
with	O
2	O
hidden	B
layers	O
)	O
were	O
compared	O
to	O
several	O
other	O
methods	O
based	O
on	O
bagging	B
and	O
boosting	B
.	O
note	O
that	O
all	O
of	O
these	O
methods	O
are	O
quite	O
similar	B
:	O
in	O
each	O
case	O
,	O
the	O
prediction	O
has	O
the	O
form	O
m	O
(	O
cid:4	O
)	O
ˆf	O
(	O
x∗	O
)	O
=	O
wme	O
[	O
y|x∗	O
,	O
θm	O
]	O
(	O
16.108	O
)	O
m=1	O
the	O
bayesian	O
mlp	O
was	O
ﬁt	O
by	O
mcmc	O
(	O
hybrid	O
monte	O
carlo	O
)	O
,	O
so	O
we	O
set	O
wm	O
=	O
1/m	O
and	O
set	O
θm	O
in	O
bagging	B
,	O
we	O
set	O
wm	O
=	O
1/m	O
and	O
θm	O
is	O
estimated	O
by	O
ﬁtting	O
to	O
a	O
draw	O
from	O
the	O
posterior	O
.	O
in	O
boosting	B
,	O
we	O
set	O
wm	O
=	O
1	O
and	O
the	O
θm	O
are	O
the	O
model	O
to	O
a	O
bootstrap	B
sample	O
from	O
the	O
data	O
.	O
estimated	O
sequentially	O
.	O
to	O
improve	O
computational	O
and	O
statistical	O
performance	O
,	O
some	O
feature	B
selection	I
was	O
performed	O
.	O
two	O
methods	O
were	O
considered	O
:	O
simple	O
uni-variate	O
screening	B
using	O
t-tests	O
,	O
and	O
a	O
method	O
based	O
on	O
mlp+ard	O
.	O
results	O
of	O
this	O
follow-up	O
study	O
are	O
shown	O
in	O
table	O
16.5.	O
we	O
see	O
that	O
bayesian	O
mlps	O
are	O
again	O
the	O
winner	O
.	O
in	O
second	O
place	O
are	O
either	O
random	B
forests	I
or	O
boosted	O
mlps	O
,	O
depending	O
on	O
the	O
preprocessing	O
.	O
however	O
,	O
it	O
is	O
not	O
clear	O
how	O
statistically	B
signiﬁcant	I
these	O
differences	O
are	O
,	O
since	O
the	O
test	O
sets	O
are	O
relatively	O
small	O
.	O
in	O
terms	O
of	O
training	O
time	O
,	O
we	O
see	O
that	O
mcmc	O
is	O
much	O
slower	O
than	O
the	O
other	O
methods	O
.	O
it	O
would	O
be	O
interesting	O
to	O
see	O
how	O
well	O
deterministic	O
bayesian	O
inference	B
(	O
e.g.	O
,	O
laplace	O
approximation	O
)	O
would	O
perform	O
.	O
(	O
obviously	O
it	O
will	O
be	O
much	O
faster	O
,	O
but	O
the	O
question	O
is	O
:	O
how	O
much	O
would	O
one	O
lose	O
16.8.	O
interpreting	O
black-box	B
models	O
585	O
e	O
c	O
n	O
e	O
d	O
n	O
e	O
p	O
e	O
d	O
l	O
a	O
i	O
t	O
r	O
a	O
p	O
e	O
c	O
n	O
e	O
d	O
n	O
e	O
p	O
e	O
d	O
l	O
a	O
i	O
t	O
r	O
a	O
p	O
0	O
2	O
8	O
1	O
6	O
1	O
4	O
1	O
2	O
1	O
0	O
1	O
8	O
0	O
2	O
8	O
1	O
6	O
1	O
4	O
1	O
2	O
1	O
0	O
1	O
8	O
0	O
2	O
8	O
1	O
6	O
1	O
4	O
1	O
2	O
1	O
0	O
1	O
8	O
0.2	O
0.4	O
0.6	O
0.8	O
0.2	O
0.4	O
0.6	O
0.8	O
x1	O
x2	O
0	O
2	O
8	O
1	O
6	O
1	O
4	O
1	O
2	O
1	O
0	O
1	O
8	O
0.2	O
0.4	O
0.6	O
0.8	O
x3	O
0	O
2	O
8	O
1	O
6	O
1	O
4	O
1	O
2	O
1	O
0	O
1	O
8	O
0	O
2	O
8	O
1	O
6	O
1	O
4	O
1	O
2	O
1	O
0	O
1	O
8	O
0	O
2	O
8	O
1	O
6	O
1	O
4	O
1	O
2	O
1	O
0	O
1	O
8	O
0	O
2	O
8	O
1	O
6	O
1	O
4	O
1	O
2	O
1	O
0	O
1	O
8	O
0	O
2	O
8	O
1	O
6	O
1	O
4	O
1	O
2	O
1	O
0	O
1	O
8	O
0.2	O
0.4	O
0.6	O
0.8	O
1.0	O
0.2	O
0.4	O
0.6	O
0.8	O
x4	O
x5	O
0	O
2	O
8	O
1	O
6	O
1	O
4	O
1	O
2	O
1	O
0	O
1	O
8	O
0.2	O
0.4	O
0.6	O
0.8	O
0.2	O
0.4	O
0.6	O
0.8	O
1.0	O
0.2	O
0.4	O
0.6	O
0.8	O
0.2	O
0.4	O
0.6	O
0.8	O
0.2	O
0.4	O
0.6	O
0.8	O
x6	O
x7	O
x8	O
x9	O
x10	O
figure	O
16.20	O
partial	O
dependence	O
plots	O
for	O
the	O
10	O
predictors	O
in	O
friedman	O
’	O
s	O
synthetic	O
5-dimensional	O
re-	O
gression	O
problem	O
.	O
source	O
:	O
figure	O
4	O
of	O
(	O
chipman	O
et	O
al	O
.	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
hugh	O
chipman	O
.	O
in	O
statistical	O
performance	O
?	O
)	O
16.8	O
interpreting	O
black-box	B
models	O
linear	O
models	O
are	O
popular	O
in	O
part	O
because	O
they	O
are	O
easy	O
to	O
interpet	O
.	O
however	O
,	O
they	O
often	O
are	O
poor	O
predictors	O
,	O
which	O
makes	O
them	O
a	O
poor	O
proxy	O
for	O
“	O
nature	O
’	O
s	O
mechanism	O
”	O
.	O
thus	O
any	O
conclusions	O
about	O
the	O
importance	O
of	O
particular	O
variables	O
should	O
only	O
be	O
based	O
on	O
models	O
that	O
have	O
good	O
predictive	B
accuracy	O
(	O
breiman	O
2001b	O
)	O
.	O
(	O
interestingly	O
,	O
many	O
standard	O
statistical	O
tests	O
of	O
“	O
goodness	O
of	O
ﬁt	O
”	O
do	O
not	O
test	O
the	O
predictive	B
accuracy	O
of	O
a	O
model	O
.	O
)	O
in	O
this	O
chapter	O
,	O
we	O
studied	O
black-box	B
models	O
,	O
which	O
do	O
have	O
good	O
predictive	B
accuracy	O
.	O
unfortunately	O
,	O
they	O
are	O
hard	O
to	O
interpret	O
directly	O
.	O
fortunately	O
,	O
there	O
are	O
various	O
heuristics	B
we	O
can	O
use	O
to	O
“	O
probe	B
”	O
such	O
models	O
,	O
in	O
order	O
to	O
assess	O
which	O
input	O
variables	O
are	O
the	O
most	O
important	O
.	O
as	O
a	O
simple	O
example	O
,	O
consider	O
the	O
following	O
non-linear	O
function	O
,	O
ﬁrst	O
proposed	O
(	O
friedman	O
1991	O
)	O
to	O
illustrate	O
the	O
power	O
of	O
mars	O
:	O
f	O
(	O
x	O
)	O
=	O
10	O
sin	O
(	O
πx1x2	O
)	O
+	O
20	O
(	O
x3	O
−	O
0.5	O
)	O
2	O
+	O
10x4	O
+	O
5x5	O
+	O
	O
(	O
16.109	O
)	O
where	O
	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
.	O
we	O
see	O
that	O
the	O
output	O
is	O
a	O
complex	O
function	O
of	O
the	O
inputs	O
.	O
by	O
augmenting	O
the	O
x	O
vector	O
with	O
additional	O
irrelevant	O
random	O
variables	O
,	O
all	O
drawn	O
uniform	O
on	O
[	O
0	O
,	O
1	O
]	O
,	O
we	O
can	O
create	O
a	O
challenging	O
feature	B
selection	I
problem	O
.	O
in	O
the	O
experiments	O
below	O
,	O
we	O
add	O
5	O
extra	O
dummy	O
variables	O
.	O
586	O
chapter	O
16.	O
adaptive	O
basis	O
function	O
models	O
2	O
3	O
4	O
1	O
5	O
e	O
g	O
a	O
s	O
u	O
5	O
2	O
.	O
0	O
0	O
2	O
.	O
0	O
5	O
1	O
.	O
0	O
0	O
1	O
.	O
0	O
5	O
0	O
.	O
0	O
0	O
0	O
.	O
0	O
1	O
2	O
3	O
4	O
5	O
1	O
2	O
3	O
4	O
5	O
4	O
2	O
1	O
3	O
4	O
5	O
1	O
3	O
4	O
2	O
5	O
2	O
5	O
4	O
3	O
2	O
1	O
5	O
4	O
3	O
2	O
1	O
6	O
5	O
4	O
3	O
2	O
1	O
8	O
5	O
4	O
3	O
2	O
1	O
5	O
4	O
3	O
2	O
1	O
10	O
figure	O
16.21	O
average	O
usage	O
of	O
each	O
variable	O
in	O
a	O
bart	O
model	O
ﬁt	O
to	O
data	O
where	O
only	O
the	O
ﬁrst	O
5	O
features	B
are	O
relevant	O
.	O
the	O
different	O
coloured	O
lines	O
correspond	O
to	O
different	O
numbers	O
of	O
trees	O
in	O
the	O
ensemble	B
.	O
source	O
:	O
figure	O
5	O
of	O
(	O
chipman	O
et	O
al	O
.	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
hugh	O
chipman	O
.	O
one	O
useful	O
way	O
to	O
measure	O
the	O
effect	O
of	O
a	O
set	O
s	O
of	O
variables	O
on	O
the	O
output	O
is	O
to	O
compute	O
a	O
partial	B
dependence	I
plot	I
(	O
friedman	O
2001	O
)	O
.	O
this	O
is	O
a	O
plot	O
of	O
f	O
(	O
xs	O
)	O
vs	O
xs	O
,	O
where	O
f	O
(	O
xs	O
)	O
is	O
deﬁned	O
as	O
the	O
response	O
to	O
xs	O
with	O
the	O
other	O
predictors	O
averaged	O
out	O
:	O
n	O
(	O
cid:4	O
)	O
i=1	O
f	O
(	O
xs	O
)	O
=	O
1	O
n	O
f	O
(	O
xs	O
,	O
xi	O
,	O
−s	O
)	O
(	O
16.110	O
)	O
figure	O
16.20	O
shows	O
an	O
example	O
where	O
we	O
use	O
sets	O
corresponding	O
to	O
each	O
single	O
variable	O
.	O
the	O
data	O
was	O
generated	O
from	O
equation	O
16.109	O
,	O
with	O
5	O
irrelevant	O
variables	O
added	O
.	O
we	O
then	O
ﬁt	O
a	O
bart	O
model	O
(	O
section	O
16.2.5	O
)	O
and	O
computed	O
the	O
partial	O
dependence	O
plots	O
.	O
we	O
see	O
that	O
the	O
predicted	O
response	O
is	O
invariant	B
for	O
s	O
∈	O
{	O
6	O
,	O
.	O
.	O
.	O
,	O
10	O
}	O
,	O
indicating	O
that	O
these	O
variables	O
are	O
(	O
marginally	O
)	O
irrelevant	O
.	O
the	O
response	O
is	O
roughly	O
linear	O
in	O
x4	O
and	O
x5	O
,	O
and	O
roughly	O
quadratic	O
in	O
x3	O
.	O
(	O
the	O
error	O
bars	O
are	O
obtained	O
by	O
computing	O
empirical	O
quantiles	O
of	O
f	O
(	O
x	O
,	O
θ	O
)	O
based	O
on	O
posterior	O
samples	O
of	O
θ	O
;	O
alternatively	O
,	O
we	O
can	O
use	O
bootstrap	B
.	O
)	O
another	O
very	O
useful	O
summary	O
computes	O
the	O
relative	B
importance	I
of	I
predictor	I
variables	I
.	O
this	O
can	O
be	O
thought	O
of	O
as	O
a	O
nonlinear	O
,	O
or	O
even	O
“	O
model	O
free	O
”	O
,	O
way	O
of	O
performing	O
variable	O
selection	O
,	O
(	O
cid:7	O
)	O
m	O
although	O
the	O
technique	O
is	O
restricted	O
to	O
ensembles	O
of	O
trees	O
.	O
the	O
basic	O
idea	O
,	O
originally	O
proposed	O
in	O
(	O
breiman	O
et	O
al	O
.	O
1984	O
)	O
,	O
is	O
to	O
count	O
how	O
often	O
variable	O
j	O
is	O
used	O
as	O
a	O
node	O
in	O
any	O
of	O
the	O
trees	O
.	O
m=1	O
i	O
(	O
j	O
∈	O
tm	O
)	O
be	O
the	O
proportion	O
of	O
all	O
splitting	O
rules	B
that	O
use	O
xj	O
,	O
in	O
particular	O
,	O
let	O
vj	O
=	O
1	O
where	O
tm	O
is	O
the	O
m	O
’	O
th	O
tree	B
.	O
if	O
we	O
can	O
sample	O
the	O
posterior	O
of	O
trees	O
,	O
p	O
(	O
t1	O
:	O
m|d	O
)	O
,	O
we	O
can	O
easily	O
m	O
compute	O
the	O
posterior	O
for	O
vj	O
.	O
alternatively	O
,	O
we	O
can	O
use	O
bootstrap	B
.	O
figure	O
16.21	O
gives	O
an	O
example	O
,	O
using	O
bart	O
.	O
we	O
see	O
that	O
the	O
ﬁve	O
relevant	O
variables	O
are	O
chosen	O
much	O
more	O
than	O
the	O
ﬁve	O
irrelevant	O
variables	O
.	O
as	O
we	O
increase	O
the	O
number	O
m	O
of	O
trees	O
,	O
all	O
the	O
variables	O
are	O
more	O
likely	O
to	O
be	O
chosen	O
,	O
reducing	O
the	O
sensitivity	B
of	O
this	O
method	O
,	O
but	O
for	O
small	O
m	O
,	O
the	O
method	O
is	O
farily	O
diagnostic	O
.	O
16.8.	O
interpreting	O
black-box	B
models	O
587	O
exercises	O
exercise	O
16.1	O
nonlinear	O
regression	B
for	O
inverse	O
dynamics	O
in	O
this	O
question	O
,	O
we	O
ﬁt	O
a	O
model	O
which	O
can	O
predict	O
what	O
torques	O
a	O
robot	O
needs	O
to	O
apply	O
in	O
order	O
to	O
make	O
its	O
arm	O
reach	O
a	O
desired	O
point	O
in	O
space	O
.	O
the	O
data	O
was	O
collected	O
from	O
a	O
sarcos	O
robot	O
arm	O
with	O
7	O
degrees	B
of	I
freedom	I
.	O
the	O
input	O
vector	O
x	O
∈	O
r	O
21	O
encodes	O
the	O
desired	O
position	O
,	O
velocity	O
and	O
accelaration	O
of	O
the	O
7	O
joints	O
.	O
the	O
output	O
vector	O
y	O
∈	O
r	O
7	O
encodes	O
the	O
torques	O
that	O
should	O
be	O
applied	O
to	O
the	O
joints	O
to	O
reach	O
that	O
point	O
.	O
the	O
mapping	O
from	O
x	O
to	O
y	O
is	O
highly	O
nonlinear	O
.	O
we	O
have	O
n	O
=	O
48	O
,	O
933	O
training	O
points	O
and	O
ntest	O
=	O
4	O
,	O
449	O
testing	O
points	O
.	O
for	O
simplicity	O
,	O
we	O
following	O
standard	O
practice	O
and	O
focus	O
on	O
just	O
predicting	O
a	O
scalar	O
output	O
,	O
namely	O
the	O
torque	O
for	O
the	O
ﬁrst	O
joint	O
.	O
download	O
the	O
data	O
from	O
http	O
:	O
//www.gaussianprocess.org/gpml	O
.	O
standardize	O
the	O
inputs	O
so	O
they	O
have	O
zero	O
mean	O
and	O
unit	O
variance	O
on	O
the	O
training	B
set	I
,	O
and	O
center	O
the	O
outputs	O
so	O
they	O
have	O
zero	O
mean	O
on	O
the	O
training	B
set	I
.	O
apply	O
the	O
corresponding	O
transformations	O
to	O
the	O
test	O
data	O
.	O
below	O
we	O
will	O
describe	O
various	O
models	O
which	O
you	O
should	O
ﬁt	O
to	O
this	O
transformed	O
data	O
.	O
then	O
make	O
predictions	O
and	O
compute	O
the	O
standardized	B
mean	O
squared	B
error	I
on	O
the	O
test	O
set	O
as	O
follows	O
:	O
sm	O
se	O
=	O
1	O
ntest	O
(	O
cid:2	O
)	O
ntest	O
i=1	O
(	O
yi	O
−	O
ˆyi	O
)	O
2	O
(	O
cid:2	O
)	O
n	O
train	O
σ2	O
where	O
σ2	O
=	O
1	O
ntrain	O
i=1	O
(	O
yi	O
−	O
y	O
)	O
2	O
is	O
the	O
variance	B
of	O
the	O
output	O
computed	O
on	O
the	O
training	B
set	I
.	O
(	O
16.111	O
)	O
a.	O
the	O
ﬁrst	O
method	O
you	O
should	O
try	O
is	O
standard	O
linear	O
regression	B
.	O
turn	O
in	O
your	O
numbers	O
and	O
code	O
.	O
(	O
according	O
to	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
p24	O
)	O
,	O
you	O
should	O
be	O
able	O
to	O
achieve	O
a	O
smse	O
of	O
0.075	O
using	O
this	O
method	O
.	O
)	O
b.	O
now	O
try	O
running	O
k-means	O
clustering	B
(	O
using	O
cross	B
validation	I
to	O
pick	O
k	O
)	O
.	O
then	O
ﬁt	O
an	O
rbf	O
network	O
to	O
the	O
data	O
,	O
using	O
the	O
μk	O
estimated	O
by	O
k-means	O
.	O
use	O
cv	O
to	O
estimate	O
the	O
rbf	O
bandwidth	B
.	O
what	O
smse	O
do	O
you	O
get	O
?	O
turn	O
in	O
your	O
numbers	O
and	O
code	O
.	O
(	O
according	O
to	O
(	O
rasmussen	O
and	O
williams	O
2006	O
,	O
p24	O
)	O
,	O
gaussian	O
process	O
regression	B
can	O
get	O
an	O
smse	O
of	O
0.011	O
,	O
so	O
the	O
goal	O
is	O
to	O
get	O
close	O
to	O
that	O
.	O
)	O
c.	O
now	O
try	O
ﬁtting	O
a	O
feedforward	B
neural	I
network	I
.	O
use	O
cv	O
to	O
pick	O
the	O
number	O
of	O
hidden	B
units	I
and	O
the	O
strength	O
of	O
the	O
(	O
cid:7	O
)	O
2	O
regularizer	O
.	O
what	O
smse	O
do	O
you	O
get	O
?	O
turn	O
in	O
your	O
numbers	O
and	O
code	O
.	O
17	O
markov	O
and	O
hidden	B
markov	O
models	O
17.1	O
introduction	O
in	O
this	O
chapter	O
,	O
we	O
discuss	O
probabilistic	O
models	O
for	O
sequences	O
of	O
observations	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xt	O
,	O
of	O
arbitrary	O
length	O
t	O
.	O
such	O
models	O
have	O
applications	O
in	O
computational	O
biology	O
,	O
natural	O
language	O
processing	O
,	O
time	O
series	O
forecasting	O
,	O
etc	O
.	O
we	O
focus	O
on	O
the	O
case	O
where	O
we	O
the	O
observations	O
occur	O
at	O
discrete	B
“	O
time	O
steps	O
”	O
,	O
although	O
“	O
time	O
”	O
may	O
also	O
refer	O
to	O
locations	O
within	O
a	O
sequence	O
.	O
17.2	O
markov	O
models	O
recall	B
from	O
section	O
10.2.2	O
that	O
the	O
basic	O
idea	O
behind	O
a	O
markov	O
chain	O
is	O
to	O
assume	O
that	O
xt	O
captures	O
all	O
the	O
relevant	O
information	B
for	O
predicting	O
the	O
future	O
(	O
i.e.	O
,	O
we	O
assume	O
it	O
is	O
a	O
sufficient	O
statistic	O
)	O
.	O
if	O
we	O
assume	O
discrete	B
time	O
steps	O
,	O
we	O
can	O
write	O
the	O
joint	B
distribution	I
as	O
follows	O
:	O
t	O
(	O
cid:20	O
)	O
p	O
(	O
x1	O
:	O
t	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
x3|x2	O
)	O
.	O
.	O
.	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
xt|xt−1	O
)	O
(	O
17.1	O
)	O
t=2	O
this	O
is	O
called	O
a	O
markov	O
chain	O
or	O
markov	O
model	O
.	O
if	O
we	O
assume	O
the	O
transition	O
function	O
p	O
(	O
xt|xt−1	O
)	O
is	O
independent	O
of	O
time	O
,	O
then	O
the	O
chain	O
is	O
called	O
homogeneous	B
,	O
stationary	B
,	O
or	O
time-invariant	B
.	O
this	O
is	O
an	O
example	O
of	O
parameter	B
tying	I
,	O
since	O
the	O
same	O
parameter	B
is	O
shared	B
by	O
multiple	O
variables	O
.	O
this	O
assumption	O
allows	O
us	O
to	O
model	O
an	O
arbitrary	O
number	O
of	O
variables	O
using	O
a	O
ﬁxed	O
number	O
of	O
parameters	O
;	O
such	O
models	O
are	O
called	O
stochastic	B
processes	I
.	O
if	O
we	O
assume	O
that	O
the	O
observed	O
variables	O
are	O
discrete	B
,	O
so	O
xt	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
this	O
is	O
called	O
a	O
discrete-state	O
or	O
ﬁnite-state	O
markov	O
chain	O
.	O
we	O
will	O
make	O
this	O
assumption	O
throughout	O
the	O
rest	O
of	O
this	O
section	O
.	O
17.2.1	O
transition	B
matrix	I
when	O
xt	O
is	O
discrete	B
,	O
so	O
xt	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
the	O
conditional	O
distribution	O
p	O
(	O
xt|xt−1	O
)	O
can	O
be	O
written	O
as	O
a	O
k	O
×	O
k	O
matrix	O
,	O
known	O
as	O
the	O
transition	B
matrix	I
a	O
,	O
where	O
aij	O
=	O
p	O
(	O
xt	O
=	O
j|xt−1	O
=	O
i	O
)	O
is	O
the	O
probability	O
of	O
going	O
from	O
state	B
i	O
to	O
state	B
j.	O
each	O
row	O
of	O
the	O
matrix	O
sums	O
to	O
one	O
,	O
j	O
aij	O
=	O
1	O
,	O
so	O
this	O
is	O
called	O
a	O
stochastic	B
matrix	I
.	O
(	O
cid:7	O
)	O
590	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
1	O
−	O
α	O
1	O
1	O
−	O
β	O
2	O
α	O
β	O
(	O
a	O
)	O
a11	O
1	O
a33	O
3	O
a22	O
a12	O
a23	O
2	O
(	O
b	O
)	O
figure	O
17.1	O
3-state	O
left-to-right	B
chain	O
.	O
state	O
transition	O
diagrams	O
for	O
some	O
simple	O
markov	O
chains	O
.	O
left	O
:	O
a	O
2-state	O
chain	O
.	O
right	O
:	O
a	O
a	O
stationary	B
,	O
ﬁnite-state	O
markov	O
chain	O
is	O
equivalent	O
to	O
a	O
stochastic	B
automaton	I
.	O
it	O
is	O
common	O
to	O
visualize	O
such	O
automata	O
by	O
drawing	O
a	O
directed	B
graph	O
,	O
where	O
nodes	B
represent	O
states	O
and	O
arrows	O
represent	O
legal	O
transitions	O
,	O
i.e.	O
,	O
non-zero	O
elements	O
of	O
a.	O
this	O
is	O
known	O
as	O
a	O
state	B
transition	I
diagram	I
.	O
the	O
weights	O
associated	O
with	O
the	O
arcs	O
are	O
the	O
probabilities	O
.	O
for	O
example	O
,	O
the	O
following	O
2-state	O
chain	O
(	O
17.2	O
)	O
(	O
17.3	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
1	O
−	O
α	O
α	O
1	O
−	O
β	O
β	O
⎛	O
⎝a11	O
a12	O
0	O
a22	O
a23	O
1	O
0	O
0	O
0	O
⎞	O
⎠	O
a	O
=	O
a	O
=	O
is	O
illustrated	O
in	O
figure	O
17.1	O
(	O
left	O
)	O
.	O
the	O
following	O
3-state	O
chain	O
is	O
illustrated	O
in	O
figure	O
17.1	O
(	O
right	O
)	O
.	O
this	O
is	O
called	O
a	O
left-to-right	B
transition	I
matrix	I
,	O
and	O
is	O
com-	O
monly	O
used	O
in	O
speech	B
recognition	I
(	O
section	O
17.6.2	O
)	O
.	O
the	O
aij	O
element	O
of	O
the	O
transition	B
matrix	I
speciﬁes	O
the	O
probability	O
of	O
getting	O
from	O
i	O
to	O
j	O
in	O
one	O
step	O
.	O
the	O
n-step	O
transition	B
matrix	I
a	O
(	O
n	O
)	O
is	O
deﬁned	O
as	O
aij	O
(	O
n	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
xt+n	O
=	O
j|xt	O
=	O
i	O
)	O
(	O
17.4	O
)	O
which	O
is	O
the	O
probability	O
of	O
getting	O
from	O
i	O
to	O
j	O
in	O
exactly	O
n	O
steps	O
.	O
obviously	O
a	O
(	O
1	O
)	O
=	O
a.	O
the	O
chapman-kolmogorov	O
equations	O
state	B
that	O
k	O
(	O
cid:4	O
)	O
aij	O
(	O
m	O
+	O
n	O
)	O
=	O
aik	O
(	O
m	O
)	O
akj	O
(	O
n	O
)	O
(	O
17.5	O
)	O
k=1	O
in	O
words	O
,	O
the	O
probability	O
of	O
getting	O
from	O
i	O
to	O
j	O
in	O
m	O
+	O
n	O
steps	O
is	O
just	O
the	O
probability	O
of	O
getting	O
from	O
i	O
to	O
k	O
in	O
m	O
steps	O
,	O
and	O
then	O
from	O
k	O
to	O
j	O
in	O
n	O
steps	O
,	O
summed	O
up	O
over	O
all	O
k.	O
we	O
can	O
write	O
the	O
above	O
as	O
a	O
matrix	O
multiplication	O
a	O
(	O
m	O
+	O
n	O
)	O
=	O
a	O
(	O
m	O
)	O
a	O
(	O
n	O
)	O
hence	O
a	O
(	O
n	O
)	O
=	O
a	O
a	O
(	O
n	O
−	O
1	O
)	O
=	O
a	O
a	O
a	O
(	O
n	O
−	O
2	O
)	O
=	O
···	O
=	O
an	O
(	O
17.6	O
)	O
(	O
17.7	O
)	O
thus	O
we	O
can	O
simulate	O
multiple	O
steps	O
of	O
a	O
markov	O
chain	O
by	O
“	O
powering	O
up	O
”	O
the	O
transition	B
matrix	I
.	O
17.2.	O
markov	O
models	O
591	O
says	O
it	O
’	O
s	O
not	O
in	O
the	O
cards	O
legendary	O
reconnaissance	O
by	O
rollie	O
democracies	O
unsustainable	O
could	O
strike	O
redlining	O
visits	O
to	O
profit	O
booking	O
wait	O
here	O
at	O
madison	O
square	O
garden	O
county	O
courthouse	O
where	O
he	O
had	O
been	O
done	O
in	O
three	O
already	O
in	O
any	O
way	O
in	O
which	O
a	O
teacher	O
table	O
17.1	O
example	O
output	O
from	O
an	O
4-gram	O
word	O
model	O
,	O
trained	O
using	O
backoff	B
smoothing	I
on	O
the	O
broadcast	O
news	O
corpus	B
.	O
the	O
ﬁrst	O
4	O
words	O
are	O
speciﬁed	O
by	O
hand	O
,	O
the	O
model	O
generates	O
the	O
5th	O
word	O
,	O
and	O
then	O
the	O
results	O
are	O
fed	O
back	O
into	O
the	O
model	O
.	O
source	O
:	O
http	O
:	O
//www.fit.vutbr.cz/~imikolov/rnnlm/gen-4gra	O
m.txt	O
.	O
17.2.2	O
application	O
:	O
language	B
modeling	I
one	O
important	O
application	O
of	O
markov	O
models	O
is	O
to	O
make	O
statistical	O
language	O
models	O
,	O
which	O
are	O
probability	O
distributions	O
over	O
sequences	O
of	O
words	O
.	O
we	O
deﬁne	O
the	O
state	B
space	I
to	O
be	O
all	O
the	O
words	O
in	O
english	O
(	O
or	O
some	O
other	O
language	O
)	O
.	O
the	O
marginal	O
probabilities	O
p	O
(	O
xt	O
=	O
k	O
)	O
are	O
called	O
unigram	B
statistics	I
.	O
if	O
we	O
use	O
a	O
ﬁrst-order	O
markov	O
model	O
,	O
then	O
p	O
(	O
xt	O
=	O
k|xt−1	O
=	O
j	O
)	O
is	O
called	O
a	O
bigram	O
if	O
we	O
use	O
a	O
second-order	O
markov	O
model	O
,	O
then	O
p	O
(	O
xt	O
=	O
k|xt−1	O
=	O
j	O
,	O
xt−2	O
=	O
i	O
)	O
is	O
model	O
.	O
called	O
a	O
trigram	B
model	I
.	O
and	O
so	O
on	O
.	O
in	O
general	O
these	O
are	O
called	O
n-gram	B
models	I
.	O
for	O
example	O
,	O
figure	O
17.2	O
shows	O
1-gram	O
and	O
2-grams	O
counts	O
for	O
the	O
letters	O
{	O
a	O
,	O
.	O
.	O
.	O
,	O
z	O
,	O
−	O
}	O
(	O
where	O
-	O
represents	O
space	O
)	O
estimated	O
from	O
darwin	O
’	O
s	O
on	O
the	O
origin	O
of	O
species	O
.	O
language	B
models	I
can	O
be	O
used	O
for	O
several	O
things	O
,	O
such	O
as	O
the	O
following	O
:	O
•	O
sentence	O
completion	O
a	O
language	B
model	I
can	O
predict	O
the	O
next	O
word	O
given	O
the	O
previous	O
words	O
in	O
a	O
sentence	O
.	O
this	O
can	O
be	O
used	O
to	O
reduce	O
the	O
amount	O
of	O
typing	O
required	O
,	O
which	O
is	O
particularly	O
important	O
for	O
disabled	O
users	O
(	O
see	O
e.g.	O
,	O
david	O
mackay	O
’	O
s	O
dasher	O
system1	O
)	O
,	O
or	O
uses	O
of	O
mobile	O
devices	O
.	O
•	O
data	B
compression	I
any	O
density	O
model	O
can	O
be	O
used	O
to	O
deﬁne	O
an	O
encoding	O
scheme	O
,	O
by	O
assigning	O
short	O
codewords	O
to	O
more	O
probable	O
strings	O
.	O
the	O
more	O
accurate	O
the	O
predictive	B
model	O
,	O
the	O
fewer	O
the	O
number	O
of	O
bits	B
it	O
requires	O
to	O
store	O
the	O
data	O
.	O
•	O
text	O
classiﬁcation	B
any	O
density	O
model	O
can	O
be	O
used	O
as	O
a	O
class-conditional	B
density	I
and	O
hence	O
turned	O
into	O
a	O
(	O
generative	O
)	O
classiﬁer	O
.	O
note	O
that	O
using	O
a	O
0-gram	O
class-conditional	B
density	I
(	O
i.e.	O
,	O
only	O
unigram	B
statistics	I
)	O
would	O
be	O
equivalent	O
to	O
a	O
naive	O
bayes	O
classiﬁer	O
(	O
see	O
section	O
3.5	O
)	O
.	O
•	O
automatic	O
essay	O
writing	O
one	O
can	O
sample	O
from	O
p	O
(	O
x1	O
:	O
t	O
)	O
to	O
generate	O
artiﬁcial	O
text	O
.	O
this	O
is	O
in	O
table	O
17.1	O
,	O
we	O
give	O
an	O
example	O
of	O
text	O
one	O
way	O
of	O
assessing	O
the	O
quality	O
of	O
the	O
model	O
.	O
generated	O
from	O
a	O
4-gram	O
model	O
,	O
trained	O
on	O
a	O
corpus	B
with	O
400	O
million	O
words	O
.	O
(	O
(	O
tomas	O
et	O
al	O
.	O
2011	O
)	O
describes	O
a	O
much	O
better	O
language	B
model	I
,	O
based	O
on	O
recurrent	B
neural	I
networks	I
,	O
which	O
generates	O
much	O
more	O
semantically	O
plausible	O
text	O
.	O
)	O
1.	O
http	O
:	O
//www.inference.phy.cam.ac.uk/dasher/	O
592	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
unigrams	B
bigrams	O
_	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
1	O
0.16098	O
_	O
2	O
0.06687	O
a	O
3	O
0.01414	O
b	O
4	O
0.02938	O
c	O
5	O
0.03107	O
d	O
6	O
0.11055	O
e	O
7	O
0.02325	O
f	O
8	O
0.01530	O
g	O
9	O
0.04174	O
h	O
10	O
0.06233	O
i	O
11	O
0.00060	O
j	O
12	O
0.00309	O
k	O
13	O
0.03515	O
l	O
14	O
0.02107	O
m	O
15	O
0.06007	O
n	O
16	O
0.06066	O
o	O
17	O
0.01594	O
p	O
18	O
0.00077	O
q	O
19	O
0.05265	O
r	O
20	O
0.05761	O
s	O
21	O
0.07566	O
t	O
22	O
0.02149	O
u	O
23	O
0.00993	O
v	O
24	O
0.01341	O
w	O
25	O
0.00208	O
x	O
26	O
0.01381	O
y	O
27	O
0.00039	O
z	O
_	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
figure	O
17.2	O
unigram	O
and	O
bigram	O
counts	O
from	O
darwin	O
’	O
s	O
on	O
the	O
origin	O
of	O
species	O
.	O
the	O
2d	O
picture	O
on	O
the	O
right	O
is	O
a	O
hinton	O
diagram	O
of	O
the	O
joint	B
distribution	I
.	O
the	O
size	O
of	O
the	O
white	O
squares	O
is	O
proportional	O
to	O
the	O
value	O
of	O
the	O
entry	O
in	O
the	O
corresponding	O
vector/	O
matrix	O
.	O
based	O
on	O
(	O
mackay	O
2003	O
,	O
p22	O
)	O
.	O
figure	O
generated	O
by	O
ngramplot	O
.	O
17.2.2.1	O
mle	O
for	O
markov	O
language	B
models	I
we	O
now	O
discuss	O
a	O
simple	O
way	O
to	O
estimate	O
the	O
transition	B
matrix	I
from	O
training	O
data	O
.	O
the	O
proba-	O
bility	O
of	O
any	O
particular	O
sequence	O
of	O
length	O
t	O
is	O
given	O
by	O
p	O
(	O
x1	O
:	O
t|θ	O
)	O
=π	O
(	O
x1	O
)	O
a	O
(	O
x1	O
,	O
x2	O
)	O
.	O
.	O
.	O
a	O
(	O
xt−1	O
,	O
xt	O
)	O
t	O
(	O
cid:20	O
)	O
k	O
(	O
cid:20	O
)	O
k	O
(	O
cid:20	O
)	O
=	O
(	O
πj	O
)	O
i	O
(	O
x1=j	O
)	O
(	O
ajk	O
)	O
i	O
(	O
xt=k	O
,	O
xt−1=j	O
)	O
j=1	O
t=2	O
j=1	O
k=1	O
(	O
17.8	O
)	O
(	O
17.9	O
)	O
hence	O
the	O
log-likelihood	O
of	O
a	O
set	O
of	O
sequences	O
d	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
)	O
,	O
where	O
xi	O
=	O
(	O
xi1	O
,	O
.	O
.	O
.	O
,	O
xi	O
,	O
ti	O
)	O
is	O
a	O
sequence	O
of	O
length	O
ti	O
,	O
is	O
given	O
by	O
k	O
(	O
cid:20	O
)	O
n	O
(	O
cid:4	O
)	O
i=1	O
log	O
p	O
(	O
d|θ	O
)	O
=	O
n	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
log	O
p	O
(	O
xi|θ	O
)	O
=	O
ti−1	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
j	O
where	O
we	O
deﬁne	O
the	O
following	O
counts	O
:	O
j	O
(	O
cid:2	O
)	O
n	O
1	O
i	O
(	O
xi1	O
=	O
j	O
)	O
,	O
njk	O
(	O
cid:2	O
)	O
i=1	O
i=1	O
t=1	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
n	O
1	O
j	O
log	O
πj	O
+	O
njk	O
log	O
ajk	O
(	O
17.10	O
)	O
j	O
k	O
i	O
(	O
xi	O
,	O
t	O
=	O
j	O
,	O
xi	O
,	O
t+1	O
=	O
k	O
)	O
(	O
17.11	O
)	O
17.2.	O
markov	O
models	O
hence	O
we	O
can	O
write	O
the	O
mle	O
as	O
the	O
normalized	O
counts	O
:	O
ˆπj	O
=	O
,	O
ˆajk	O
=	O
j	O
(	O
cid:7	O
)	O
n	O
1	O
j	O
n	O
1	O
j	O
njk	O
(	O
cid:7	O
)	O
k	O
njk	O
593	O
(	O
17.12	O
)	O
these	O
results	O
can	O
be	O
extended	O
in	O
a	O
straightforward	O
way	O
to	O
higher	O
order	O
markov	O
models	O
.	O
however	O
,	O
the	O
problem	O
of	O
zero-counts	O
becomes	O
very	O
acute	O
whenever	O
the	O
number	O
of	O
states	O
k	O
,	O
and/or	O
the	O
order	O
of	O
the	O
chain	O
,	O
n	O
,	O
is	O
large	O
.	O
an	O
n-gram	B
models	I
has	O
o	O
(	O
k	O
n	O
)	O
parameters	O
.	O
if	O
we	O
have	O
k	O
∼	O
50	O
,	O
000	O
words	O
in	O
our	O
vocabulary	O
,	O
then	O
a	O
bi-gram	O
model	O
will	O
have	O
about	O
2.5	O
billion	O
free	O
parameters	O
,	O
corresponding	O
to	O
all	O
possible	O
word	O
pairs	O
.	O
it	O
is	O
very	O
unlikely	O
we	O
will	O
see	O
all	O
of	O
these	O
in	O
our	O
training	O
data	O
.	O
however	O
,	O
we	O
do	O
not	O
want	O
to	O
predict	O
that	O
a	O
particular	O
word	O
string	O
is	O
totally	O
impossible	O
just	O
because	O
we	O
happen	O
not	O
to	O
have	O
seen	O
it	O
in	O
our	O
training	O
text	O
—	O
that	O
would	O
be	O
a	O
severe	O
form	O
of	O
overﬁtting.2	O
a	O
simple	O
solution	O
to	O
this	O
is	O
to	O
use	O
add-one	B
smoothing	I
,	O
where	O
we	O
simply	O
add	O
one	O
to	O
all	O
the	O
empirical	O
counts	O
before	O
normalizing	O
.	O
the	O
bayesian	O
justiﬁcation	O
for	O
this	O
is	O
given	O
in	O
section	O
3.3.4.1.	O
however	O
add-one	B
smoothing	I
assumes	O
all	O
n-grams	O
are	O
equally	O
likely	O
,	O
which	O
is	O
not	O
very	O
realistic	O
.	O
a	O
more	O
sophisticated	O
bayesian	O
approach	O
is	O
discussed	O
in	O
section	O
17.2.2.2.	O
an	O
alternative	O
to	O
using	O
smart	O
priors	O
is	O
to	O
gather	O
lots	O
and	O
lots	O
of	O
data	O
.	O
for	O
example	O
,	O
google	O
has	O
ﬁt	O
n-gram	B
models	I
(	O
for	O
n	O
=	O
1	O
:	O
5	O
)	O
based	O
on	O
one	O
trillion	O
words	O
extracted	O
from	O
the	O
web	O
.	O
their	O
data	O
,	O
which	O
is	O
over	O
100gb	O
when	O
uncompressed	O
,	O
is	O
publically	O
available.3	O
an	O
example	O
of	O
their	O
data	O
,	O
for	O
a	O
set	O
of	O
4-grams	O
,	O
is	O
shown	O
below	O
.	O
serve	O
as	O
the	O
incoming	O
92	O
serve	O
as	O
the	O
incubator	O
99	O
serve	O
as	O
the	O
independent	O
794	O
serve	O
as	O
the	O
index	O
223	O
serve	O
as	O
the	O
indication	O
72	O
serve	O
as	O
the	O
indicator	O
120	O
serve	O
as	O
the	O
indicators	O
45	O
serve	O
as	O
the	O
indispensable	O
111	O
serve	O
as	O
the	O
indispensible	O
40	O
serve	O
as	O
the	O
individual	O
234	O
...	O
although	O
such	O
an	O
approach	O
,	O
based	O
on	O
“	O
brute	O
force	O
and	O
ignorance	O
”	O
,	O
can	O
be	O
successful	O
,	O
it	O
is	O
rather	O
unsatisfying	O
,	O
since	O
it	O
is	O
clear	O
that	O
this	O
is	O
not	O
how	O
humans	O
learn	O
(	O
see	O
e.g.	O
,	O
(	O
tenenbaum	O
and	O
xu	O
2000	O
)	O
)	O
.	O
a	O
more	O
reﬁned	O
bayesian	O
approach	O
,	O
that	O
needs	O
much	O
less	O
data	O
,	O
is	O
described	O
in	O
section	O
17.2.2.2	O
.	O
17.2.2.2	O
empirical	O
bayes	O
version	O
of	O
deleted	B
interpolation	I
a	O
common	O
heuristic	O
used	O
to	O
ﬁx	O
the	O
sparse	B
data	I
problem	I
is	O
called	O
deleted	B
interpolation	I
(	O
chen	O
and	O
goodman	O
1996	O
)	O
.	O
this	O
deﬁnes	O
the	O
transition	B
matrix	I
as	O
a	O
convex	B
combination	I
of	O
the	O
bigram	O
2.	O
a	O
famous	O
example	O
of	O
an	O
improbable	O
,	O
but	O
syntactically	O
valid	O
,	O
english	O
word	O
string	O
,	O
due	O
to	O
noam	O
chomsky	O
,	O
is	O
“	O
colourless	O
green	O
ideas	O
sleep	O
furiously	O
”	O
.	O
we	O
would	O
not	O
want	O
our	O
model	O
to	O
predict	O
that	O
this	O
string	O
is	O
impossible	O
.	O
even	O
ungrammatical	O
constructs	O
should	O
be	O
allowed	O
by	O
our	O
model	O
with	O
a	O
certain	O
probability	O
,	O
since	O
people	O
frequently	O
violate	O
grammatical	O
rules	B
,	O
especially	O
in	O
spoken	O
language	O
.	O
3.	O
see	O
http	O
:	O
//googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html	O
for	O
de-	O
tails	O
.	O
594	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
frequencies	O
fjk	O
=	O
njk/nj	O
and	O
the	O
unigram	O
frequencies	O
fk	O
=	O
nk/n	O
:	O
ajk	O
=	O
(	O
1	O
−	O
λ	O
)	O
fjk	O
+	O
λfk	O
(	O
17.13	O
)	O
the	O
term	O
λ	O
is	O
usually	O
set	O
by	O
cross	B
validation	I
.	O
there	O
is	O
also	O
a	O
closely	O
related	O
technique	O
called	O
backoff	B
smoothing	I
;	O
the	O
idea	O
is	O
that	O
if	O
fjk	O
is	O
too	O
small	O
,	O
we	O
“	O
back	O
off	O
”	O
to	O
a	O
more	O
reliable	O
estimate	O
,	O
namely	O
fk	O
.	O
we	O
will	O
now	O
show	O
that	O
the	O
deleted	B
interpolation	I
heuristic	O
is	O
an	O
approximation	O
to	O
the	O
predic-	O
tions	O
made	O
by	O
a	O
simple	O
hierarchical	O
bayesian	O
model	O
.	O
our	O
presentation	O
follows	O
(	O
mckay	O
and	O
peto	O
1995	O
)	O
.	O
first	O
,	O
let	O
us	O
use	O
an	O
independent	O
dirichlet	O
prior	O
on	O
each	O
row	O
of	O
the	O
transition	B
matrix	I
:	O
aj	O
∼	O
dir	O
(	O
α0m1	O
,	O
.	O
.	O
.	O
,	O
α0mk	O
)	O
=	O
dir	O
(	O
α0m	O
)	O
=	O
dir	O
(	O
α	O
)	O
(	O
cid:7	O
)	O
(	O
17.14	O
)	O
where	O
aj	O
is	O
row	O
j	O
of	O
the	O
transition	B
matrix	I
,	O
m	O
is	O
the	O
prior	O
mean	B
(	O
satisfying	O
α0	O
is	O
the	O
prior	O
strength	O
.	O
we	O
will	O
use	O
the	O
same	O
prior	O
for	O
each	O
row	O
:	O
see	O
figure	O
17.3.	O
k	O
mk	O
=	O
1	O
)	O
and	O
the	O
posterior	O
is	O
given	O
by	O
aj	O
∼	O
dir	O
(	O
α	O
+	O
nj	O
)	O
,	O
where	O
nj	O
=	O
(	O
nj1	O
,	O
.	O
.	O
.	O
,	O
njk	O
)	O
is	O
the	O
vector	O
that	O
records	O
the	O
number	O
of	O
times	O
we	O
have	O
transitioned	O
out	O
of	O
state	B
j	O
to	O
each	O
of	O
the	O
other	O
states	O
.	O
from	O
equation	O
3.51	O
,	O
the	O
posterior	B
predictive	I
density	I
is	O
p	O
(	O
xt+1	O
=	O
k|xt	O
=	O
j	O
,	O
d	O
)	O
=	O
ajk	O
=	O
where	O
ajk	O
=	O
e	O
[	O
ajk|d	O
,	O
α	O
]	O
and	O
=	O
(	O
1	O
−	O
λj	O
)	O
fjk	O
+	O
λjmk	O
(	O
17.15	O
)	O
nj	O
+	O
α0	O
njk	O
+	O
αmk	O
fjknj	O
+	O
αmk	O
nj	O
+	O
α0	O
=	O
λj	O
=	O
α	O
nj	O
+	O
α0	O
(	O
17.16	O
)	O
this	O
is	O
very	O
similar	B
to	O
equation	O
17.13	O
but	O
not	O
identical	O
.	O
the	O
main	O
difference	O
is	O
that	O
the	O
bayesian	O
model	O
uses	O
a	O
context-dependent	O
weight	O
λj	O
to	O
combine	O
mk	O
with	O
the	O
empirical	O
frequency	O
fjk	O
,	O
rather	O
than	O
a	O
ﬁxed	O
weight	O
λ.	O
this	O
is	O
like	O
adaptive	O
deleted	O
interpolation	O
.	O
furthermore	O
,	O
rather	O
than	O
backing	O
off	O
to	O
the	O
empirical	O
marginal	O
frequencies	O
fk	O
,	O
we	O
back	O
off	O
to	O
the	O
model	O
parameter	O
mk	O
.	O
the	O
only	O
remaining	O
question	O
is	O
:	O
what	O
values	O
should	O
we	O
use	O
for	O
α	O
and	O
m	O
?	O
let	O
’	O
s	O
use	O
empirical	O
bayes	O
.	O
since	O
we	O
assume	O
each	O
row	O
of	O
the	O
transition	B
matrix	I
is	O
a	O
priori	O
independent	O
given	O
α	O
,	O
the	O
marginal	B
likelihood	I
for	O
our	O
markov	O
model	O
is	O
found	O
by	O
applying	O
equation	O
5.24	O
to	O
each	O
row	O
:	O
p	O
(	O
d|α	O
)	O
=	O
b	O
(	O
nj	O
+	O
α	O
)	O
b	O
(	O
α	O
)	O
(	O
17.17	O
)	O
(	O
cid:20	O
)	O
j	O
where	O
nj	O
=	O
(	O
nj1	O
,	O
.	O
.	O
.	O
,	O
njk	O
)	O
are	O
the	O
counts	O
for	O
leaving	O
state	B
j	O
and	O
b	O
(	O
α	O
)	O
is	O
the	O
generalized	O
beta	O
function	O
.	O
we	O
can	O
ﬁt	O
this	O
using	O
the	O
methods	O
discussed	O
in	O
(	O
minka	O
2000e	O
)	O
.	O
however	O
,	O
we	O
can	O
also	O
use	O
the	O
following	O
approximation	O
(	O
mckay	O
and	O
peto	O
1995	O
,	O
p12	O
)	O
:	O
mk	O
∝	O
|	O
{	O
j	O
:	O
njk	O
>	O
0	O
}	O
|	O
(	O
17.18	O
)	O
this	O
says	O
that	O
the	O
prior	O
probability	O
of	O
word	O
k	O
is	O
given	O
by	O
the	O
number	O
of	O
different	O
contexts	O
in	O
which	O
it	O
occurs	O
,	O
rather	O
than	O
the	O
number	O
of	O
times	O
it	O
occurs	O
.	O
to	O
justify	O
the	O
reasonableness	O
of	O
this	O
result	O
,	O
mackay	O
and	O
peto	O
(	O
mckay	O
and	O
peto	O
1995	O
)	O
give	O
the	O
following	O
example	O
.	O
17.2.	O
markov	O
models	O
595	O
figure	O
17.3	O
a	O
markov	O
chain	O
in	O
which	O
we	O
put	O
a	O
different	O
dirichlet	O
prior	O
on	O
every	O
row	O
of	O
the	O
transition	B
matrix	I
a	O
,	O
but	O
the	O
hyperparameters	O
of	O
the	O
dirichlet	O
are	O
shared	B
.	O
imagine	O
,	O
you	O
see	O
,	O
that	O
the	O
language	O
,	O
you	O
see	O
,	O
has	O
,	O
you	O
see	O
,	O
a	O
frequently	O
occuring	O
couplet	O
’	O
you	O
see	O
’	O
,	O
you	O
see	O
,	O
in	O
which	O
the	O
second	O
word	O
of	O
the	O
couplet	O
,	O
see	O
,	O
follows	O
the	O
first	O
word	O
,	O
you	O
,	O
with	O
very	O
high	O
probability	O
,	O
you	O
see	O
.	O
then	O
the	O
marginal	O
statistics	O
,	O
you	O
see	O
,	O
are	O
going	O
to	O
become	O
hugely	O
dominated	O
,	O
you	O
see	O
,	O
by	O
the	O
words	O
you	O
and	O
see	O
,	O
with	O
equal	O
frequency	O
,	O
you	O
see	O
.	O
if	O
we	O
use	O
the	O
standard	O
smoothing	O
formula	O
,	O
equation	O
17.13	O
,	O
then	O
p	O
(	O
you|novel	O
)	O
and	O
p	O
(	O
see|novel	O
)	O
,	O
for	O
some	O
novel	O
context	O
word	O
not	O
seen	O
before	O
,	O
would	O
turn	O
out	O
to	O
be	O
the	O
same	O
,	O
since	O
the	O
marginal	O
frequencies	O
of	O
’	O
you	O
’	O
and	O
’	O
see	O
’	O
are	O
the	O
same	O
(	O
11	O
times	O
each	O
)	O
.	O
however	O
,	O
this	O
seems	O
unreasonable	O
.	O
’	O
you	O
’	O
appears	O
in	O
many	O
contexts	O
,	O
so	O
p	O
(	O
you|novel	O
)	O
should	O
be	O
high	O
,	O
but	O
’	O
see	O
’	O
only	O
follows	O
’	O
you	O
’	O
,	O
so	O
p	O
(	O
see|novel	O
)	O
should	O
be	O
low	O
.	O
if	O
we	O
use	O
the	O
bayesian	O
formula	O
equation	O
17.15	O
,	O
we	O
will	O
get	O
this	O
effect	O
for	O
free	O
,	O
since	O
we	O
back	O
off	O
to	O
mk	O
not	O
fk	O
,	O
and	O
mk	O
will	O
be	O
large	O
for	O
’	O
you	O
’	O
and	O
small	O
for	O
’	O
see	O
’	O
by	O
equation	O
17.18.	O
unfortunately	O
,	O
although	O
elegant	O
,	O
this	O
bayesian	O
model	O
does	O
not	O
beat	O
the	O
state-of-the-art	O
lan-	O
guage	O
model	O
,	O
known	O
as	O
interpolated	O
kneser-ney	O
(	O
kneser	O
and	O
ney	O
1995	O
;	O
chen	O
and	O
goodman	O
1998	O
)	O
.	O
however	O
,	O
in	O
(	O
teh	O
2006	O
)	O
,	O
it	O
was	O
shown	O
how	O
one	O
can	O
build	O
a	O
non-parametric	O
bayesian	O
model	O
which	O
outperforms	O
interpolated	O
kneser-ney	O
,	O
by	O
using	O
variable-length	O
contexts	O
.	O
in	O
(	O
wood	O
et	O
al	O
.	O
2009	O
)	O
,	O
this	O
method	O
was	O
extended	O
to	O
create	O
the	O
“	O
sequence	O
memoizer	O
”	O
,	O
which	O
is	O
currently	O
(	O
2010	O
)	O
the	O
best-performing	O
language	O
model.4	O
17.2.2.3	O
handling	O
out-of-vocabulary	O
words	O
while	O
the	O
above	O
smoothing	O
methods	O
handle	O
the	O
case	O
where	O
the	O
counts	O
are	O
small	O
or	O
even	O
zero	O
,	O
none	O
of	O
them	O
deal	O
with	O
the	O
case	O
where	O
the	O
test	O
set	O
may	O
contain	O
a	O
completely	O
novel	O
word	O
.	O
in	O
particular	O
,	O
they	O
all	O
assume	O
that	O
the	O
words	O
in	O
the	O
vocabulary	O
(	O
i.e.	O
,	O
the	O
state	B
space	I
of	O
xt	O
)	O
is	O
ﬁxed	O
and	O
known	O
(	O
typically	O
it	O
is	O
the	O
set	O
of	O
unique	O
words	O
in	O
the	O
training	O
data	O
,	O
or	O
in	O
some	O
dictionary	B
)	O
.	O
4.	O
interestingly	O
,	O
these	O
non-parametric	O
methods	O
are	O
based	O
on	O
posterior	O
inference	O
using	O
mcmc	O
(	O
section	O
24.1	O
)	O
and/or	O
particle	B
ﬁltering	I
(	O
section	O
23.5	O
)	O
,	O
rather	O
than	O
optimization	B
methods	O
such	O
as	O
eb	O
.	O
despite	O
this	O
,	O
they	O
are	O
quite	O
efficient	O
.	O
596	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
0.5	O
3	O
1	O
1.0	O
0.5	O
1.0	O
2	O
(	O
a	O
)	O
0.1	O
0.1	O
1.0	O
1	O
0.9	O
0.9	O
2	O
3	O
0.5	O
0.5	O
4	O
(	O
b	O
)	O
figure	O
17.4	O
some	O
markov	O
chains	O
.	O
(	O
a	O
)	O
a	O
3-state	O
aperiodic	B
chain	O
.	O
(	O
b	O
)	O
a	O
reducible	O
4-state	O
chain	O
.	O
even	O
if	O
all	O
ajk	O
’	O
s	O
are	O
non-zero	O
,	O
none	O
of	O
these	O
models	O
will	O
predict	O
a	O
novel	O
word	O
outside	O
of	O
this	O
set	O
,	O
and	O
hence	O
will	O
assign	O
zero	O
probability	O
to	O
a	O
test	O
sentence	O
with	O
an	O
unfamiliar	O
word	O
.	O
(	O
unfamiliar	O
words	O
are	O
bound	O
to	O
occur	O
,	O
because	O
the	O
set	O
of	O
words	O
is	O
an	O
open	B
class	I
.	O
for	O
example	O
,	O
the	O
set	O
of	O
proper	O
nouns	O
(	O
names	O
of	O
people	O
and	O
places	O
)	O
is	O
unbounded	O
.	O
)	O
a	O
standard	O
heuristic	O
to	O
solve	O
this	O
problem	O
is	O
to	O
replace	O
all	O
novel	O
words	O
with	O
the	O
special	O
symbol	O
unk	O
,	O
which	O
stands	O
for	O
“	O
unknown	B
”	O
.	O
a	O
certain	O
amount	O
of	O
probability	O
mass	O
is	O
held	O
aside	O
for	O
this	O
event	O
.	O
a	O
more	O
principled	O
solution	O
would	O
be	O
to	O
use	O
a	O
dirichlet	O
process	O
,	O
which	O
can	O
generate	O
a	O
countably	O
inﬁnite	O
state	O
space	O
,	O
as	O
the	O
amount	O
of	O
data	O
increases	O
(	O
see	O
section	O
25.2.2	O
)	O
.	O
if	O
all	O
novel	O
words	O
are	O
“	O
accepted	O
”	O
as	O
genuine	O
words	O
,	O
then	O
the	O
system	O
has	O
no	O
predictive	O
power	O
,	O
since	O
any	O
misspelling	O
will	O
be	O
considered	O
a	O
new	O
word	O
.	O
so	O
the	O
novel	O
word	O
has	O
to	O
be	O
seen	O
frequently	O
enough	O
to	O
warrant	O
being	O
added	O
to	O
the	O
vocabulary	O
.	O
see	O
e.g.	O
,	O
(	O
friedman	O
and	O
singer	O
1999	O
;	O
griffiths	O
and	O
tenenbaum	O
2001	O
)	O
for	O
details	O
.	O
17.2.3	O
stationary	B
distribution	I
of	O
a	O
markov	O
chain	O
*	O
we	O
have	O
been	O
focussing	O
on	O
markov	O
models	O
as	O
a	O
way	O
of	O
deﬁning	O
joint	O
probability	O
distributions	O
over	O
sequences	O
.	O
however	O
,	O
we	O
can	O
also	O
interpret	O
them	O
as	O
stochastic	O
dynamical	O
systems	O
,	O
where	O
we	O
“	O
hop	O
”	O
from	O
one	O
state	B
to	O
another	O
at	O
each	O
time	O
step	O
.	O
in	O
this	O
case	O
,	O
we	O
are	O
often	O
interested	O
in	O
the	O
long	O
term	O
distribution	O
over	O
states	O
,	O
which	O
is	O
known	O
as	O
the	O
stationary	B
distribution	I
of	O
the	O
chain	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
some	O
of	O
the	O
relevant	O
theory	O
.	O
later	O
we	O
will	O
consider	O
two	O
important	O
applications	O
:	O
google	O
’	O
s	O
pagerank	O
algorithm	O
for	O
ranking	B
web	O
pages	O
(	O
section	O
17.2.4	O
)	O
,	O
and	O
the	O
mcmc	O
algorithm	O
for	O
generating	O
samples	B
from	O
hard-to-normalize	O
probability	O
distributions	O
(	O
chapter	O
24	O
)	O
.	O
17.2.3.1	O
what	O
is	O
a	O
stationary	B
distribution	I
?	O
let	O
aij	O
=	O
p	O
(	O
xt	O
=	O
j|xt−1	O
=	O
i	O
)	O
be	O
the	O
one-step	O
transition	B
matrix	I
,	O
and	O
let	O
πt	O
(	O
j	O
)	O
=p	O
(	O
xt	O
=	O
j	O
)	O
be	O
the	O
probability	O
of	O
being	O
in	O
state	B
j	O
at	O
time	O
t.	O
it	O
is	O
conventional	O
in	O
this	O
context	O
to	O
assume	O
that	O
π	O
is	O
a	O
row	O
vector	O
.	O
if	O
we	O
have	O
an	O
initial	O
distribution	O
over	O
states	O
of	O
π0	O
,	O
then	O
at	O
time	O
1	O
we	O
have	O
(	O
cid:4	O
)	O
π1	O
(	O
j	O
)	O
=	O
π0	O
(	O
i	O
)	O
aij	O
i	O
or	O
,	O
in	O
matrix	O
notation	O
,	O
π1	O
=	O
π0a	O
(	O
17.19	O
)	O
(	O
17.20	O
)	O
17.2.	O
markov	O
models	O
we	O
can	O
imagine	O
iterating	O
these	O
equations	O
.	O
if	O
we	O
ever	O
reach	O
a	O
stage	O
where	O
π	O
=	O
πa	O
597	O
(	O
17.21	O
)	O
then	O
we	O
say	O
we	O
have	O
reached	O
the	O
stationary	B
distribution	I
(	O
also	O
called	O
the	O
invariant	B
distribution	I
or	O
equilibrium	B
distribution	I
)	O
.	O
once	O
we	O
enter	O
the	O
stationary	B
distribution	I
,	O
we	O
will	O
never	O
leave	O
.	O
for	O
example	O
,	O
consider	O
the	O
chain	O
in	O
figure	O
17.4	O
(	O
a	O
)	O
.	O
to	O
ﬁnd	O
its	O
stationary	B
distribution	I
,	O
we	O
write	O
(	O
cid:23	O
)	O
⎛	O
⎝1	O
−	O
a12	O
−	O
a13	O
a21	O
a31	O
a12	O
1	O
−	O
a21	O
−	O
a23	O
a32	O
a13	O
a23	O
1	O
−	O
a31	O
−	O
a32	O
(	O
cid:22	O
)	O
(	O
cid:23	O
)	O
(	O
cid:22	O
)	O
π1	O
π2	O
π3	O
=	O
π1	O
π2	O
π3	O
π1	O
=	O
π1	O
(	O
1	O
−	O
a12	O
−	O
a12	O
)	O
+π	O
2a21	O
+	O
π3a31	O
so	O
or	O
π1	O
(	O
a12	O
+	O
a13	O
)	O
=	O
π2a21	O
+	O
π3a31	O
in	O
general	O
,	O
we	O
have	O
πi	O
aij	O
=	O
πjaji	O
(	O
cid:4	O
)	O
j	O
(	O
cid:5	O
)	O
=i	O
(	O
cid:4	O
)	O
j	O
(	O
cid:5	O
)	O
=i	O
⎞	O
⎠	O
(	O
17.22	O
)	O
(	O
17.23	O
)	O
(	O
17.24	O
)	O
(	O
17.25	O
)	O
in	O
other	O
words	O
,	O
the	O
probability	O
of	O
being	O
in	O
state	B
i	O
times	O
the	O
net	O
ﬂow	O
out	O
of	O
state	B
i	O
must	O
equal	O
the	O
probability	O
of	O
being	O
in	O
each	O
other	O
state	B
j	O
times	O
the	O
net	O
ﬂow	O
from	O
that	O
state	B
into	O
i.	O
these	O
are	O
called	O
the	O
global	B
balance	I
equations	I
.	O
we	O
can	O
then	O
solve	O
these	O
equations	O
,	O
subject	O
to	O
the	O
constraint	O
that	O
(	O
cid:7	O
)	O
j	O
πj	O
=	O
1	O
.	O
17.2.3.2	O
computing	O
the	O
stationary	B
distribution	I
to	O
ﬁnd	O
the	O
stationary	B
distribution	I
,	O
we	O
can	O
just	O
solve	O
the	O
eigenvector	O
equation	O
at	O
v	O
=	O
v	O
,	O
and	O
then	O
to	O
set	O
π	O
=	O
vt	O
,	O
where	O
v	O
is	O
an	O
eigenvector	O
with	O
eigenvalue	O
1	O
.	O
(	O
we	O
can	O
be	O
sure	O
such	O
an	O
eigenvector	O
exists	O
,	O
since	O
a	O
is	O
a	O
row-stochastic	O
matrix	O
,	O
so	O
a1	O
=	O
1	O
;	O
also	O
recall	B
that	O
the	O
eigenvalues	O
of	O
a	O
and	O
at	O
are	O
the	O
same	O
.	O
)	O
of	O
course	O
,	O
since	O
eigenvectors	O
are	O
unique	O
only	O
up	O
to	O
constants	O
of	O
proportionality	O
,	O
we	O
must	O
normalize	O
v	O
at	O
the	O
end	O
to	O
ensure	O
it	O
sums	O
to	O
one	O
.	O
note	O
,	O
however	O
,	O
that	O
the	O
eigenvectors	O
are	O
only	O
guaranteed	O
to	O
be	O
real-valued	O
if	O
the	O
matrix	O
is	O
positive	O
,	O
aij	O
>	O
0	O
(	O
and	O
hence	O
aij	O
<	O
1	O
,	O
due	O
to	O
the	O
sum-to-one	O
constraint	O
)	O
.	O
a	O
more	O
general	O
approach	O
,	O
which	O
can	O
handle	O
chains	O
where	O
some	O
transition	O
probabilities	O
are	O
0	O
or	O
1	O
(	O
such	O
as	O
figure	O
17.4	O
(	O
a	O
)	O
)	O
,	O
is	O
as	O
follows	O
(	O
resnick	O
1992	O
,	O
p138	O
)	O
.	O
we	O
have	O
k	O
constraints	O
from	O
π	O
(	O
i−	O
a	O
)	O
=	O
0k×1	O
and	O
1	O
constraint	O
from	O
π1k×1	O
=	O
0.	O
since	O
we	O
only	O
have	O
k	O
unknowns	O
,	O
this	O
is	O
overconstrained	O
.	O
so	O
let	O
us	O
replace	O
any	O
column	O
(	O
e.g.	O
,	O
the	O
last	O
)	O
of	O
i	O
−	O
a	O
with	O
1	O
,	O
to	O
get	O
a	O
new	O
matrix	O
,	O
call	O
it	O
m.	O
next	O
we	O
deﬁne	O
r	O
=	O
[	O
0	O
,	O
0	O
,	O
.	O
.	O
.	O
,	O
1	O
]	O
,	O
where	O
the	O
1	O
in	O
the	O
last	O
position	O
corresponds	O
to	O
the	O
column	O
of	O
all	O
1s	O
in	O
m.	O
we	O
then	O
solve	O
πm	O
=	O
r.	O
for	O
example	O
,	O
for	O
a	O
3	O
state	B
chain	O
we	O
have	O
to	O
solve	O
this	O
linear	O
system	O
:	O
(	O
cid:22	O
)	O
π1	O
π2	O
π3	O
(	O
cid:23	O
)	O
⎛	O
⎝1	O
−	O
a11	O
−a12	O
1	O
−	O
a22	O
−a21	O
−a32	O
−a31	O
⎞	O
⎠	O
=	O
(	O
cid:22	O
)	O
1	O
1	O
1	O
(	O
cid:23	O
)	O
0	O
0	O
1	O
(	O
17.26	O
)	O
598	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
for	O
the	O
chain	O
in	O
figure	O
17.4	O
(	O
a	O
)	O
we	O
ﬁnd	O
π	O
=	O
[	O
0.4	O
,	O
0.4	O
,	O
0.2	O
]	O
.	O
we	O
can	O
easily	O
verify	O
this	O
is	O
correct	O
,	O
since	O
π	O
=	O
πa	O
.	O
see	O
mcstatdist	O
for	O
some	O
matlab	O
code	O
.	O
unfortunately	O
,	O
not	O
all	O
chains	O
have	O
a	O
stationary	B
distribution	I
.	O
as	O
we	O
explain	O
below	O
.	O
17.2.3.3	O
when	O
does	O
a	O
stationary	B
distribution	I
exist	O
?	O
*	O
consider	O
the	O
4-state	O
chain	O
in	O
figure	O
17.4	O
(	O
b	O
)	O
.	O
if	O
we	O
start	O
in	O
state	B
4	O
,	O
we	O
will	O
stay	O
there	O
forever	O
,	O
since	O
4	O
is	O
anabsorbing	O
state	B
.	O
thus	O
π	O
=	O
(	O
0	O
,	O
0	O
,	O
0	O
,	O
1	O
)	O
is	O
one	O
possible	O
stationary	B
distribution	I
.	O
however	O
,	O
if	O
we	O
start	O
in	O
1	O
or	O
2	O
,	O
we	O
will	O
oscillate	O
between	O
those	O
two	O
states	O
for	O
ever	O
.	O
so	O
π	O
=	O
(	O
0.5	O
,	O
0.5	O
,	O
0	O
,	O
0	O
)	O
is	O
another	O
possible	O
stationary	B
distribution	I
.	O
if	O
we	O
start	O
in	O
state	B
3	O
,	O
we	O
could	O
end	O
up	O
in	O
either	O
of	O
the	O
above	O
stationary	B
distributions	O
.	O
we	O
see	O
from	O
this	O
example	O
that	O
a	O
necessary	O
condition	O
to	O
have	O
a	O
unique	O
stationary	B
distribution	I
is	O
that	O
the	O
state	B
transition	I
diagram	I
be	O
a	O
singly	O
connected	O
component	O
,	O
i.e.	O
,	O
we	O
can	O
get	O
from	O
any	O
state	B
to	O
any	O
other	O
state	B
.	O
such	O
chains	O
are	O
called	O
irreducible	B
.	O
now	O
consider	O
the	O
2-state	O
chain	O
in	O
figure	O
17.1	O
(	O
a	O
)	O
.	O
this	O
is	O
irreducible	B
provided	O
α	O
,	O
β	O
>	O
0.	O
suppose	O
α	O
=	O
β	O
=	O
0.9.	O
it	O
is	O
clear	O
by	O
symmetry	O
that	O
this	O
chain	O
will	O
spend	O
50	O
%	O
of	O
its	O
time	O
in	O
each	O
state	B
.	O
thus	O
π	O
=	O
(	O
0.5	O
,	O
0.5	O
)	O
.	O
but	O
now	O
suppose	O
α	O
=	O
β	O
=	O
1.	O
in	O
this	O
case	O
,	O
the	O
chain	O
will	O
oscillate	O
between	O
the	O
two	O
states	O
,	O
but	O
the	O
long-term	O
distribution	O
on	O
states	O
depends	O
on	O
where	O
you	O
start	O
from	O
.	O
if	O
we	O
start	O
in	O
state	B
1	O
,	O
then	O
on	O
every	O
odd	O
time	O
step	O
(	O
1,3,5	O
,	O
...	O
)	O
we	O
will	O
be	O
in	O
state	B
1	O
;	O
but	O
if	O
we	O
start	O
in	O
state	B
2	O
,	O
then	O
on	O
every	O
odd	O
time	O
step	O
we	O
will	O
be	O
in	O
state	B
2.	O
this	O
example	O
motivates	O
the	O
following	O
deﬁnition	O
.	O
let	O
us	O
say	O
that	O
a	O
chain	O
has	O
a	O
limiting	O
if	O
this	O
holds	O
,	O
then	O
distribution	O
if	O
πj	O
=	O
limn→∞	O
an	O
ij	O
exists	O
and	O
is	O
independent	O
of	O
i	O
,	O
for	O
all	O
j.	O
the	O
long-run	O
distribution	O
over	O
states	O
will	O
be	O
independent	O
of	O
the	O
starting	O
state	B
:	O
(	O
cid:4	O
)	O
p	O
(	O
xt	O
=	O
j	O
)	O
=	O
p	O
(	O
x0	O
=	O
i	O
)	O
aij	O
(	O
t	O
)	O
→	O
πj	O
as	O
t	O
→	O
∞	O
(	O
17.27	O
)	O
i	O
let	O
us	O
now	O
characterize	O
when	O
a	O
limiting	B
distribution	I
exists	O
.	O
deﬁne	O
the	O
period	B
of	O
state	B
i	O
to	O
be	O
d	O
(	O
i	O
)	O
=gcd	O
{	O
t	O
:	O
aii	O
(	O
t	O
)	O
>	O
0	O
}	O
(	O
17.28	O
)	O
where	O
gcd	O
stands	O
for	O
greatest	B
common	I
divisor	I
,	O
i.e.	O
,	O
the	O
largest	O
integer	O
that	O
divides	O
all	O
the	O
members	O
of	O
the	O
set	O
.	O
for	O
example	O
,	O
in	O
figure	O
17.4	O
(	O
a	O
)	O
,	O
we	O
have	O
d	O
(	O
1	O
)	O
=	O
d	O
(	O
2	O
)	O
=	O
gcd	O
(	O
2	O
,	O
3	O
,	O
4	O
,	O
6	O
,	O
...	O
)	O
=	O
1	O
and	O
d	O
(	O
3	O
)	O
=	O
gcd	O
(	O
3	O
,	O
5	O
,	O
6	O
,	O
...	O
)	O
=	O
1.	O
we	O
say	O
a	O
state	B
i	O
is	O
aperiodic	B
if	O
d	O
(	O
i	O
)	O
=	O
1	O
.	O
(	O
a	O
sufficient	O
condition	O
to	O
ensure	O
this	O
is	O
if	O
state	B
i	O
has	O
a	O
self-loop	O
,	O
but	O
this	O
is	O
not	O
a	O
necessary	O
condition	O
.	O
)	O
we	O
say	O
a	O
chain	O
is	O
aperiodic	B
if	O
all	O
its	O
states	O
are	O
aperiodic	B
.	O
one	O
can	O
show	O
the	O
following	O
important	O
result	O
:	O
theorem	O
17.2.1.	O
every	O
irreducible	B
(	O
singly	O
connected	O
)	O
,	O
aperiodic	B
ﬁnite	O
state	B
markov	O
chain	O
has	O
a	O
limiting	B
distribution	I
,	O
which	O
is	O
equal	O
to	O
π	O
,	O
its	O
unique	O
stationary	B
distribution	I
.	O
a	O
special	O
case	O
of	O
this	O
result	O
says	O
that	O
every	O
regular	B
ﬁnite	O
state	B
chain	O
has	O
a	O
unique	O
stationary	B
distribution	I
,	O
where	O
a	O
regular	B
chain	O
is	O
one	O
whose	O
transition	B
matrix	I
satisﬁes	O
an	O
ij	O
>	O
0	O
for	O
some	O
integer	O
n	O
and	O
all	O
i	O
,	O
j	O
,	O
i.e.	O
,	O
it	O
is	O
possible	O
to	O
get	O
from	O
any	O
state	B
to	O
any	O
other	O
state	B
in	O
n	O
steps	O
.	O
consequently	O
,	O
after	O
n	O
steps	O
,	O
the	O
chain	O
could	O
be	O
in	O
any	O
state	B
,	O
no	O
matter	O
where	O
it	O
started	O
.	O
one	O
can	O
show	O
that	O
sufficient	O
conditions	O
to	O
ensure	O
regularity	O
are	O
that	O
the	O
chain	O
be	O
irreducible	B
(	O
singly	O
connected	O
)	O
and	O
that	O
every	O
state	B
have	O
a	O
self-transition	O
.	O
to	O
handle	O
the	O
case	O
of	O
markov	O
chains	O
whose	O
state-space	O
is	O
not	O
ﬁnite	O
(	O
e.g	O
,	O
the	O
countable	O
set	O
of	O
all	O
integers	O
,	O
or	O
all	O
the	O
uncountable	O
set	O
of	O
all	O
reals	O
)	O
,	O
we	O
need	O
to	O
generalize	B
some	O
of	O
the	O
earlier	O
17.2.	O
markov	O
models	O
599	O
deﬁnitions	O
.	O
since	O
the	O
details	O
are	O
rather	O
technical	O
,	O
we	O
just	O
brieﬂy	O
state	B
the	O
main	O
results	O
without	O
proof	O
.	O
see	O
e.g.	O
,	O
(	O
grimmett	O
and	O
stirzaker	O
1992	O
)	O
for	O
details	O
.	O
for	O
a	O
stationary	B
distribution	I
to	O
exist	O
,	O
we	O
require	O
irreducibility	O
(	O
singly	O
connected	O
)	O
and	O
aperiod-	O
icity	O
,	O
as	O
before	O
.	O
but	O
we	O
also	O
require	O
that	O
each	O
state	B
is	O
recurrent	B
.	O
(	O
a	O
chain	O
in	O
which	O
all	O
states	O
are	O
recurrent	B
is	O
called	O
a	O
recurrent	B
chain	O
.	O
)	O
recurrent	B
means	O
that	O
you	O
will	O
return	O
to	O
that	O
state	B
with	O
probability	O
1.	O
as	O
a	O
simple	O
example	O
of	O
a	O
non-recurrent	O
state	B
(	O
i.e.	O
,	O
a	O
transient	B
state	O
)	O
,	O
consider	O
figure	O
17.4	O
(	O
b	O
)	O
:	O
states	O
3	O
is	O
transient	B
because	O
one	O
immediately	O
leaves	B
it	O
and	O
either	O
spins	O
around	O
state	B
4	O
forever	O
,	O
or	O
oscillates	O
between	O
states	O
1	O
and	O
2	O
forever	O
.	O
there	O
is	O
no	O
way	O
to	O
return	O
to	O
state	B
3.	O
it	O
is	O
clear	O
that	O
any	O
ﬁnite-state	O
irreducible	B
chain	O
is	O
recurrent	B
,	O
since	O
you	O
can	O
always	O
get	O
back	O
to	O
where	O
you	O
started	O
from	O
.	O
but	O
now	O
consider	O
an	O
example	O
with	O
an	O
inﬁnite	O
state	O
space	O
.	O
suppose	O
we	O
perform	O
a	O
random	B
walk	I
on	I
the	I
integers	I
,	O
x	O
=	O
{	O
.	O
.	O
.	O
,	O
−	O
2	O
,	O
−1	O
,	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
}	O
.	O
let	O
ai	O
,	O
i+1	O
=	O
p	O
be	O
the	O
probability	O
of	O
moving	O
right	O
,	O
and	O
ai	O
,	O
i−1	O
=	O
1	O
−	O
p	O
be	O
the	O
probability	O
of	O
moving	O
left	O
.	O
suppose	O
we	O
start	O
at	O
x1	O
=	O
0.	O
if	O
p	O
>	O
0	O
.	O
5	O
,	O
we	O
will	O
shoot	O
off	O
to	O
+∞	O
;	O
we	O
are	O
not	O
guaranteed	O
to	O
return	O
.	O
similarly	O
,	O
if	O
p	O
<	O
0	O
.	O
5	O
,	O
we	O
will	O
shoot	O
off	O
to	O
−∞	O
.	O
so	O
in	O
both	O
cases	O
,	O
the	O
chain	O
is	O
not	O
recurrent	O
,	O
even	O
though	O
it	O
is	O
irreducible	B
.	O
it	O
should	O
be	O
intuitively	O
obvious	O
that	O
we	O
require	O
all	O
states	O
to	O
be	O
recurrent	B
for	O
a	O
stationary	B
distribution	I
to	O
exist	O
.	O
however	O
,	O
this	O
is	O
not	O
sufficient	O
.	O
to	O
see	O
this	O
,	O
consider	O
the	O
random	B
walk	I
on	I
the	I
integers	I
again	O
,	O
and	O
suppose	O
p	O
=	O
0.5.	O
in	O
this	O
case	O
,	O
we	O
can	O
return	O
to	O
the	O
origin	O
an	O
inﬁnite	O
number	O
of	O
times	O
,	O
so	O
the	O
chain	O
is	O
recurrent	B
.	O
however	O
,	O
it	O
takes	O
inﬁnitely	O
long	O
to	O
do	O
so	O
.	O
this	O
prohibits	O
it	O
from	O
having	O
a	O
stationary	B
distribution	I
.	O
the	O
intuitive	O
reason	O
is	O
that	O
the	O
distribution	O
keeps	O
spreading	O
out	O
over	O
a	O
larger	O
and	O
larger	O
set	O
of	O
the	O
integers	O
,	O
and	O
never	O
converges	O
to	O
a	O
stationary	B
distribution	I
.	O
more	O
formally	O
,	O
we	O
deﬁne	O
a	O
state	B
to	O
be	O
non-null	B
recurrent	I
if	O
the	O
expected	O
time	O
to	O
return	O
to	O
this	O
state	B
is	O
ﬁnite	O
.	O
a	O
chain	O
in	O
which	O
all	O
states	O
are	O
non-null	O
is	O
called	O
a	O
non-null	O
chain	O
.	O
for	O
brevity	O
,	O
we	O
we	O
say	O
that	O
a	O
state	B
is	O
ergodic	B
if	O
it	O
is	O
aperiodic	B
,	O
recurrent	B
and	O
non-null	O
,	O
and	O
we	O
say	O
a	O
chain	O
is	O
ergodic	B
if	O
all	O
its	O
states	O
are	O
ergodic	B
.	O
we	O
can	O
now	O
state	B
our	O
main	O
theorem	O
:	O
theorem	O
17.2.2.	O
every	O
irreducible	B
(	O
singly	O
connected	O
)	O
,	O
ergodic	B
markov	O
chain	O
has	O
a	O
limiting	O
distri-	O
bution	O
,	O
which	O
is	O
equal	O
to	O
π	O
,	O
its	O
unique	O
stationary	B
distribution	I
.	O
this	O
generalizes	O
theorem	O
17.2.1	O
,	O
since	O
for	O
irreducible	B
ﬁnite-state	O
chains	O
,	O
all	O
states	O
are	O
recurrent	B
and	O
non-null	O
.	O
17.2.3.4	O
detailed	B
balance	I
establishing	O
ergodicity	O
can	O
be	O
difficult	O
.	O
we	O
now	O
give	O
an	O
alternative	O
condition	O
that	O
is	O
easier	O
to	O
verify	O
.	O
we	O
say	O
that	O
a	O
markov	O
chain	O
a	O
is	O
time	B
reversible	I
if	O
there	O
exists	O
a	O
distribution	O
π	O
such	O
that	O
πiaij	O
=	O
πjaji	O
(	O
17.29	O
)	O
these	O
are	O
called	O
the	O
detailed	B
balance	I
equations	I
.	O
this	O
says	O
that	O
the	O
ﬂow	O
from	O
i	O
to	O
j	O
must	O
equal	O
the	O
ﬂow	O
from	O
j	O
to	O
i	O
,	O
weighted	O
by	O
the	O
appropriate	O
source	O
probabilities	O
.	O
we	O
have	O
the	O
following	O
important	O
result	O
.	O
theorem	O
17.2.3.	O
balance	O
wrt	O
distribution	O
π	O
,	O
then	O
π	O
is	O
a	O
stationary	B
distribution	I
of	O
the	O
chain	O
.	O
if	O
a	O
markov	O
chain	O
with	O
transition	B
matrix	I
a	O
is	O
regular	B
and	O
satisﬁes	O
detailed	O
600	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
x1	O
x4	O
x2	O
x3	O
x6	O
x5	O
figure	O
17.5	O
a	O
very	O
small	O
world	O
wide	O
web	O
.	O
figure	O
generated	O
by	O
pagerankdemo	O
,	O
written	O
by	O
tim	O
davis	O
.	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
proof	O
.	O
to	O
see	O
this	O
,	O
note	O
that	O
πiaij	O
=	O
πjaji	O
=	O
πj	O
i	O
i	O
and	O
hence	O
π	O
=	O
aπ	O
.	O
(	O
cid:4	O
)	O
i	O
aji	O
=	O
πj	O
(	O
17.30	O
)	O
note	O
that	O
this	O
condition	O
is	O
sufficient	O
but	O
not	O
necessary	O
(	O
see	O
figure	O
17.4	O
(	O
a	O
)	O
for	O
an	O
example	O
of	O
a	O
chain	O
with	O
a	O
stationary	B
distribution	I
which	O
does	O
not	O
satisfy	O
detailed	B
balance	I
)	O
.	O
in	O
section	O
24.1	O
,	O
we	O
will	O
discuss	O
markov	O
chain	O
monte	O
carlo	O
or	O
mcmc	O
methods	O
.	O
these	O
take	O
as	O
input	O
a	O
desired	O
distribution	O
π	O
and	O
construct	O
a	O
transition	B
matrix	I
(	O
or	O
in	O
general	O
,	O
a	O
transition	O
kernel	O
)	O
a	O
which	O
satisﬁes	O
detailed	B
balance	I
wrt	O
π.	O
thus	O
by	O
sampling	O
states	O
from	O
such	O
a	O
chain	O
,	O
we	O
will	O
eventually	O
enter	O
the	O
stationary	B
distribution	I
,	O
and	O
will	O
visit	O
states	O
with	O
probabilities	O
given	O
by	O
π	O
.	O
17.2.4	O
application	O
:	O
google	O
’	O
s	O
pagerank	O
algorithm	O
for	O
web	O
page	O
ranking	B
*	O
the	O
results	O
in	O
section	O
17.2.3	O
form	O
the	O
theoretical	O
underpinnings	O
to	O
google	O
’	O
s	O
pagerank	O
algorithm	O
,	O
which	O
is	O
used	O
for	O
information	B
retrieval	I
on	O
the	O
world-wide	O
web	O
.	O
we	O
sketch	O
the	O
basic	O
idea	O
below	O
;	O
see	O
(	O
byran	O
and	O
leise	O
2006	O
)	O
for	O
a	O
more	O
detailed	O
explanation	O
.	O
we	O
will	O
treat	O
the	O
web	O
as	O
a	O
giant	O
directed	B
graph	O
,	O
where	O
nodes	B
represent	O
web	O
pages	O
(	O
documents	O
)	O
and	O
edges	B
represent	O
hyper-links.5	O
we	O
then	O
perform	O
a	O
process	O
called	O
web	B
crawling	I
.	O
we	O
start	O
at	O
a	O
few	O
designated	O
root	B
nodes	O
,	O
such	O
as	O
dmoz.org	O
,	O
the	O
home	O
of	O
the	O
open	O
directory	O
project	O
,	O
and	O
then	O
follows	O
the	O
links	O
,	O
storing	O
all	O
the	O
pages	O
that	O
we	O
encounter	O
,	O
until	O
we	O
run	O
out	O
of	O
time	O
.	O
next	O
,	O
all	O
of	O
the	O
words	O
in	O
each	O
web	O
page	O
are	O
entered	O
into	O
a	O
data	O
structure	O
called	O
an	O
inverted	B
index	I
.	O
that	O
is	O
,	O
for	O
each	O
word	O
,	O
we	O
store	O
a	O
list	O
of	O
the	O
documents	O
where	O
this	O
word	O
occurs	O
.	O
(	O
in	O
practice	O
,	O
we	O
store	O
a	O
list	O
of	O
hash	O
codes	O
representing	O
the	O
urls	O
.	O
)	O
at	O
test	O
time	O
,	O
when	O
a	O
user	O
enters	O
5.	O
in	O
2008	O
,	O
google	O
said	O
it	O
had	O
indexed	O
1	O
trillion	O
(	O
1012	O
)	O
unique	O
urls	O
.	O
if	O
we	O
assume	O
there	O
are	O
about	O
10	O
urls	O
per	O
page	O
(	O
on	O
average	O
)	O
,	O
this	O
means	O
there	O
were	O
about	O
100	O
billion	O
unique	O
web	O
pages	O
.	O
estimates	O
for	O
2010	O
are	O
about	O
121	O
billion	O
unique	O
web	O
pages	O
.	O
source	O
:	O
thenextweb.com/shareables/2011/01/11/infographic-how-big-is-the-internet	O
.	O
17.2.	O
markov	O
models	O
601	O
a	O
query	O
,	O
we	O
can	O
just	O
look	O
up	O
all	O
the	O
documents	O
containing	O
each	O
word	O
,	O
and	O
intersect	O
these	O
lists	O
(	O
since	O
queries	O
are	O
deﬁned	O
by	O
a	O
conjunction	O
of	O
search	O
terms	O
)	O
.	O
we	O
can	O
get	O
a	O
reﬁned	O
search	O
by	O
storing	O
the	O
location	O
of	O
each	O
word	O
in	O
each	O
document	O
.	O
we	O
can	O
then	O
test	O
if	O
the	O
words	O
in	O
a	O
document	O
occur	O
in	O
the	O
same	O
order	O
as	O
in	O
the	O
query	O
.	O
let	O
us	O
give	O
an	O
example	O
,	O
from	O
http	O
:	O
//en.wikipedia.org/wiki/inverted_index	O
.	O
we	O
have	O
3	O
documents	O
,	O
t0	O
=	O
“	O
it	O
is	O
what	O
it	O
is	O
”	O
,	O
t1	O
=	O
“	O
what	O
is	O
it	O
”	O
and	O
t2	O
=	O
“	O
it	O
is	O
a	O
banana	O
”	O
.	O
then	O
we	O
can	O
create	O
the	O
following	O
inverted	B
index	I
,	O
where	O
each	O
pair	O
represents	O
a	O
document	O
and	O
word	O
location	O
:	O
''	O
a	O
''	O
:	O
{	O
(	O
2	O
,	O
2	O
)	O
}	O
''	O
banana	O
''	O
:	O
{	O
(	O
2	O
,	O
3	O
)	O
}	O
''	O
is	O
''	O
:	O
''	O
it	O
''	O
:	O
''	O
what	O
''	O
:	O
{	O
(	O
0	O
,	O
1	O
)	O
,	O
(	O
0	O
,	O
4	O
)	O
,	O
(	O
1	O
,	O
1	O
)	O
,	O
(	O
2	O
,	O
1	O
)	O
}	O
{	O
(	O
0	O
,	O
0	O
)	O
,	O
(	O
0	O
,	O
3	O
)	O
,	O
(	O
1	O
,	O
2	O
)	O
,	O
(	O
2	O
,	O
0	O
)	O
}	O
{	O
(	O
0	O
,	O
2	O
)	O
,	O
(	O
1	O
,	O
0	O
)	O
}	O
for	O
example	O
,	O
we	O
see	O
that	O
the	O
word	O
“	O
what	O
”	O
occurs	O
at	O
location	O
2	O
(	O
counting	O
from	O
0	O
)	O
in	O
document	O
0	O
,	O
and	O
location	O
0	O
in	O
document	O
1.	O
suppose	O
we	O
search	O
for	O
“	O
what	O
is	O
it	O
”	O
.	O
if	O
we	O
ignore	O
word	O
order	O
,	O
we	O
retrieve	O
the	O
following	O
documents	O
:	O
{	O
t0	O
,	O
t1	O
}	O
∩	O
{	O
t0	O
,	O
t1	O
,	O
t2	O
}	O
∩	O
{	O
t0	O
,	O
t1	O
,	O
t2	O
}	O
=	O
{	O
t0	O
,	O
t1	O
}	O
(	O
17.31	O
)	O
if	O
we	O
require	O
that	O
the	O
word	O
order	O
matches	O
,	O
only	O
document	O
t1	O
would	O
be	O
returned	O
.	O
more	O
generally	O
,	O
we	O
can	O
allow	O
out-of-order	O
matches	O
,	O
but	O
can	O
give	O
“	O
bonus	O
points	O
”	O
to	O
documents	O
whose	O
word	O
order	O
matches	O
the	O
query	O
’	O
s	O
word	O
order	O
,	O
or	O
to	O
other	O
features	B
,	O
such	O
as	O
if	O
the	O
words	O
occur	O
in	O
the	O
title	O
of	O
a	O
document	O
.	O
we	O
can	O
then	O
return	O
the	O
matching	O
documents	O
in	O
decreasing	O
order	O
of	O
their	O
score/	O
relevance	O
.	O
this	O
is	O
called	O
document	O
ranking	O
.	O
so	O
far	O
,	O
we	O
have	O
described	O
the	O
standard	O
process	O
of	O
information	B
retrieval	I
.	O
but	O
the	O
link	O
structure	O
of	O
the	O
web	O
provides	O
an	O
additional	O
source	O
of	O
information	B
.	O
the	O
basic	O
idea	O
is	O
that	O
some	O
web	O
pages	O
are	O
more	O
authoritative	O
than	O
others	O
,	O
so	O
these	O
should	O
be	O
ranked	O
higher	O
(	O
assuming	O
they	O
match	O
the	O
query	O
)	O
.	O
a	O
web	O
page	O
is	O
an	O
authority	O
if	O
it	O
is	O
linked	O
to	O
by	O
many	O
other	O
pages	O
.	O
but	O
to	O
protect	O
against	O
the	O
effect	O
of	O
so-called	O
link	B
farms	I
,	O
which	O
are	O
dummy	O
pages	O
which	O
just	O
link	O
to	O
a	O
given	O
site	O
to	O
boost	O
its	O
apparent	O
relevance	O
,	O
we	O
will	O
weight	O
each	O
incoming	O
link	O
by	O
the	O
source	O
’	O
s	O
authority	O
.	O
thus	O
we	O
get	O
the	O
following	O
recursive	B
deﬁnition	O
for	O
the	O
authoritativeness	O
of	O
page	O
j	O
,	O
also	O
called	O
its	O
pagerank	O
:	O
(	O
cid:4	O
)	O
πj	O
=	O
aijπi	O
(	O
17.32	O
)	O
i	O
where	O
aij	O
is	O
the	O
probability	O
of	O
following	O
a	O
link	O
from	O
i	O
to	O
j.	O
we	O
recognize	O
equation	O
17.32	O
as	O
the	O
stationary	B
distribution	I
of	O
a	O
markov	O
chain	O
.	O
in	O
the	O
simplest	O
setting	O
,	O
we	O
deﬁne	O
ai	O
.	O
as	O
a	O
uniform	B
distribution	I
over	O
all	O
states	O
that	O
i	O
is	O
connected	O
to	O
.	O
however	O
,	O
to	O
ensure	O
the	O
distribution	O
is	O
unique	O
,	O
we	O
need	O
to	O
make	O
the	O
chain	O
into	O
a	O
regular	B
chain	O
.	O
this	O
can	O
be	O
done	O
by	O
allowing	O
each	O
state	B
i	O
to	O
jump	O
to	O
any	O
other	O
state	B
(	O
including	O
itself	O
)	O
with	O
some	O
small	O
probability	O
.	O
this	O
effectively	O
makes	O
the	O
transition	B
matrix	I
aperiodic	O
and	O
fully	O
connected	O
(	O
although	O
the	O
adjacency	B
matrix	I
gij	O
of	O
the	O
web	O
itself	O
is	O
highly	O
sparse	B
)	O
.	O
we	O
discuss	O
efficient	O
methods	O
for	O
computing	O
the	O
leading	O
eigenvector	O
of	O
this	O
giant	O
matrix	O
below	O
.	O
but	O
ﬁrst	O
,	O
let	O
us	O
give	O
an	O
example	O
of	O
the	O
pagerank	O
algorithm	O
.	O
consider	O
the	O
small	O
web	O
in	O
figure	O
17.5	O
.	O
602	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
0	O
50	O
100	O
150	O
200	O
250	O
300	O
350	O
400	O
450	O
0.02	O
0.018	O
0.016	O
0.014	O
0.012	O
0.01	O
0.008	O
0.006	O
0.004	O
0.002	O
500	O
0	O
100	O
200	O
300	O
nz	O
=	O
2636	O
400	O
500	O
0	O
0	O
100	O
200	O
300	O
400	O
500	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
17.6	O
(	O
a	O
)	O
web	O
graph	O
of	O
500	O
sites	O
rooted	O
at	O
www.harvard.edu	O
.	O
(	O
b	O
)	O
corresponding	O
page	O
rank	O
vector	O
.	O
figure	O
generated	O
by	O
pagerankdemopmtk	O
,	O
based	O
on	O
code	O
by	O
cleve	O
moler	O
(	O
moler	O
2004	O
)	O
.	O
we	O
ﬁnd	O
that	O
the	O
stationary	B
distribution	I
is	O
π	O
=	O
(	O
0.3209	O
,	O
0.1706	O
,	O
0.1065	O
,	O
0.1368	O
,	O
0.0643	O
,	O
0.2008	O
)	O
(	O
17.33	O
)	O
so	O
a	O
random	O
surfer	O
will	O
visit	O
site	O
1	O
about	O
32	O
%	O
of	O
the	O
time	O
.	O
we	O
see	O
that	O
node	O
1	O
has	O
a	O
higher	O
pagerank	O
than	O
nodes	B
4	O
or	O
6	O
,	O
even	O
though	O
they	O
all	O
have	O
the	O
same	O
number	O
of	O
in-links	O
.	O
this	O
is	O
because	O
being	O
linked	O
to	O
from	O
an	O
inﬂuential	O
nodehelps	O
increase	O
your	O
pagerank	O
score	O
more	O
than	O
being	O
linked	O
to	O
by	O
a	O
less	O
inﬂuential	O
node	O
.	O
as	O
a	O
slightly	O
larger	O
example	O
,	O
figure	O
17.6	O
(	O
a	O
)	O
shows	O
a	O
web	O
graph	O
,	O
derived	O
from	O
the	O
root	B
of	O
harvard.edu	O
.	O
figure	O
17.6	O
(	O
b	O
)	O
shows	O
the	O
corresponding	O
pagerank	O
vector	O
.	O
17.2.4.1	O
efficiently	O
computing	O
the	O
pagerank	O
vector	O
(	O
cid:19	O
)	O
let	O
gij	O
=	O
1	O
iff	B
there	O
is	O
a	O
link	O
from	O
j	O
to	O
i.	O
now	O
imagine	O
performing	O
a	O
random	O
walk	O
on	O
this	O
graph	B
,	O
where	O
at	O
every	O
time	O
step	O
,	O
with	O
probability	O
p	O
=	O
0.85	O
you	O
follow	O
one	O
of	O
the	O
outlinks	O
uniformly	O
at	O
random	O
,	O
and	O
with	O
probability	O
1	O
−	O
p	O
you	O
jump	O
to	O
a	O
random	O
node	O
,	O
again	O
chosen	O
uniformly	O
at	O
random	O
.	O
if	O
there	O
are	O
no	O
outlinks	O
,	O
you	O
just	O
jump	O
to	O
a	O
random	O
page	O
.	O
(	O
these	O
random	O
jumps	O
,	O
including	O
self-transitions	O
,	O
ensure	O
the	O
chain	O
is	O
irreducible	B
(	O
singly	O
connected	O
)	O
and	O
regular	B
.	O
hence	O
we	O
can	O
solve	O
for	O
its	O
unique	O
stationary	B
distribution	I
using	O
eigenvector	O
methods	O
.	O
)	O
this	O
deﬁnes	O
the	O
following	O
transition	B
matrix	I
:	O
if	O
cj	O
(	O
cid:4	O
)	O
=	O
0	O
if	O
cj	O
=	O
0	O
(	O
17.34	O
)	O
(	O
cid:7	O
)	O
where	O
n	O
is	O
the	O
number	O
of	O
nodes	B
,	O
δ	O
=	O
(	O
1	O
−	O
p	O
)	O
/n	O
is	O
the	O
probability	O
of	O
jumping	O
from	O
one	O
page	O
i	O
gij	O
represents	O
the	O
out-degree	B
of	O
page	O
j.	O
to	O
another	O
without	O
following	O
a	O
link	O
and	O
cj	O
=	O
(	O
if	O
n	O
=	O
4	O
·	O
109	O
and	O
p	O
=	O
0.85	O
,	O
then	O
δ	O
=	O
3.75	O
·	O
10	O
−11	O
.	O
)	O
here	O
m	O
is	O
a	O
stochastic	B
matrix	I
in	O
which	O
columns	O
sum	O
to	O
one	O
.	O
note	O
that	O
m	O
=	O
at	O
in	O
our	O
earlier	O
notation	O
.	O
mij	O
=	O
pgij/cj	O
+	O
δ	O
1/n	O
we	O
can	O
represent	O
the	O
transition	B
matrix	I
compactly	O
as	O
follows	O
.	O
deﬁne	O
the	O
diagonal	B
matrix	O
d	O
(	O
cid:19	O
)	O
with	O
entries	O
djj	O
=	O
1/cj	O
0	O
if	O
cj	O
(	O
cid:4	O
)	O
=	O
0	O
if	O
cj	O
=	O
0	O
(	O
17.35	O
)	O
17.3.	O
hidden	B
markov	O
models	O
deﬁne	O
the	O
vector	O
z	O
with	O
components	O
(	O
cid:19	O
)	O
zj	O
=	O
δ	O
1/n	O
if	O
cj	O
(	O
cid:4	O
)	O
=	O
0	O
if	O
cj	O
=	O
0	O
then	O
we	O
can	O
rewrite	O
equation	O
17.34	O
as	O
follows	O
:	O
m	O
=	O
pgd	O
+	O
1zt	O
603	O
(	O
17.36	O
)	O
(	O
17.37	O
)	O
the	O
matrix	O
m	O
is	O
not	O
sparse	O
,	O
but	O
it	O
is	O
a	O
rank	O
one	O
modiﬁcation	O
of	O
a	O
sparse	O
matrix	O
.	O
most	O
of	O
the	O
elements	O
of	O
m	O
are	O
equal	O
to	O
the	O
small	O
constant	O
δ.	O
obviously	O
these	O
do	O
not	O
need	O
to	O
be	O
stored	O
explicitly	O
.	O
our	O
goal	O
is	O
to	O
solve	O
v	O
=	O
mv	O
,	O
where	O
v	O
=	O
πt	O
.	O
one	O
efficient	O
method	O
to	O
ﬁnd	O
the	O
leading	O
eigenvector	O
of	O
a	O
large	O
matrix	O
is	O
known	O
as	O
the	O
power	B
method	I
.	O
this	O
simply	O
consists	O
of	O
repeated	O
matrix-vector	O
multiplication	O
,	O
followed	O
by	O
normalization	O
:	O
v	O
∝	O
mv	O
=	O
pgdv	O
+	O
1zt	O
v	O
it	O
is	O
possible	O
to	O
implement	O
the	O
power	B
method	I
without	O
using	O
any	O
matrix	O
multiplications	O
,	O
by	O
simply	O
sampling	O
from	O
the	O
transition	B
matrix	I
and	O
counting	O
how	O
often	O
you	O
visit	O
each	O
state	B
.	O
this	O
is	O
essentially	O
a	O
monte	O
carlo	O
approximation	O
to	O
the	O
sum	O
implied	O
by	O
v	O
=	O
mv	O
.	O
applying	O
this	O
to	O
the	O
data	O
in	O
figure	O
17.6	O
(	O
a	O
)	O
yields	O
the	O
stationary	B
distribution	I
in	O
figure	O
17.6	O
(	O
b	O
)	O
.	O
this	O
took	O
13	O
iterations	O
to	O
(	O
see	O
also	O
the	O
function	O
pagerankdemo	O
,	O
by	O
tim	O
converge	B
,	O
starting	O
from	O
a	O
uniform	B
distribution	I
.	O
davis	O
,	O
for	O
an	O
animation	O
of	O
the	O
algorithm	O
in	O
action	B
,	O
applied	O
to	O
the	O
small	O
web	O
example	O
.	O
)	O
to	O
handle	O
changing	O
web	O
structure	O
,	O
we	O
can	O
re-run	O
this	O
algorithm	O
every	O
day	O
or	O
every	O
week	O
,	O
starting	O
v	O
off	O
at	O
the	O
old	O
distribution	O
(	O
langville	O
and	O
meyer	O
2006	O
)	O
.	O
(	O
17.38	O
)	O
for	O
details	O
on	O
how	O
to	O
perform	O
this	O
monte	O
carlo	O
power	B
method	I
in	O
a	O
parallel	O
distributed	O
computing	O
environment	O
,	O
see	O
e.g.	O
,	O
(	O
rajaraman	O
and	O
ullman	O
2010	O
)	O
.	O
17.2.4.2	O
web	B
spam	I
pagerank	O
is	O
not	O
foolproof	O
.	O
for	O
example	O
,	O
consider	O
the	O
strategy	O
adopted	O
by	O
jc	O
penney	O
,	O
a	O
depart-	O
ment	O
store	O
in	O
the	O
usa	O
.	O
during	O
the	O
christmas	O
season	O
of	O
2010	O
,	O
it	O
planted	O
many	O
links	O
to	O
its	O
home	O
page	O
on	O
1000s	O
of	O
irrelevant	O
web	O
pages	O
,	O
thus	O
increasing	O
its	O
ranking	B
on	O
google	O
’	O
s	O
search	O
engine	O
(	O
segal	O
2011	O
)	O
.	O
even	O
though	O
each	O
of	O
these	O
source	O
pages	O
has	O
low	O
pagerank	O
,	O
there	O
were	O
so	O
many	O
of	O
them	O
that	O
their	O
effect	O
added	O
up	O
.	O
businesses	O
call	O
this	O
search	B
engine	I
optimization	I
;	O
google	O
calls	O
it	O
web	B
spam	I
.	O
when	O
google	O
was	O
notiﬁed	O
of	O
this	O
scam	O
(	O
by	O
the	O
new	O
york	O
times	O
)	O
,	O
it	O
manually	O
downweighted	O
jc	O
penney	O
,	O
since	O
such	O
behavior	O
violates	O
google	O
’	O
s	O
code	O
of	O
conduct	O
.	O
the	O
result	O
was	O
that	O
jc	O
penney	O
dropped	O
from	O
rank	O
1	O
to	O
rank	O
65	O
,	O
essentially	O
making	O
it	O
disappear	O
from	O
view	O
.	O
automatically	O
detecting	O
such	O
scams	O
relies	O
on	O
various	O
techniques	O
which	O
are	O
beyond	O
the	O
scope	B
of	O
this	O
chapter	O
.	O
17.3	O
hidden	B
markov	O
models	O
as	O
we	O
mentioned	O
in	O
section	O
10.2.2	O
,	O
a	O
hidden	B
markov	O
model	O
or	O
hmm	O
consists	O
of	O
a	O
discrete-time	O
,	O
discrete-state	O
markov	O
chain	O
,	O
with	O
hidden	B
states	O
zt	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
plus	O
an	O
observation	B
model	I
604	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
20	O
15	O
10	O
5	O
0	O
−5	O
13	O
12	O
9	O
10	O
11	O
14	O
8	O
20	O
18	O
19	O
7	O
17	O
456	O
16	O
2	O
3	O
15	O
1	O
3	O
2.8	O
2.6	O
2.4	O
2.2	O
2	O
1.8	O
1.6	O
1.4	O
1.2	O
1	O
−10	O
−20	O
−15	O
−10	O
−5	O
0	O
(	O
a	O
)	O
5	O
10	O
15	O
20	O
2	O
4	O
6	O
8	O
10	O
12	O
14	O
16	O
18	O
20	O
(	O
b	O
)	O
figure	O
17.7	O
hidden	B
state	O
sequence	O
.	O
based	O
on	O
figure	O
13.8	O
of	O
(	O
bishop	O
2006b	O
)	O
.	O
figure	O
generated	O
by	O
hmmlillypaddemo	O
.	O
(	O
a	O
)	O
some	O
2d	O
data	O
sampled	O
from	O
a	O
3	O
state	B
hmm	O
.	O
each	O
state	B
emits	O
from	O
a	O
2d	O
gaussian	O
.	O
(	O
b	O
)	O
the	O
p	O
(	O
xt|zt	O
)	O
.	O
the	O
corresponding	O
joint	B
distribution	I
has	O
the	O
form	O
t	O
(	O
cid:20	O
)	O
p	O
(	O
z1	O
:	O
t	O
,	O
x1	O
:	O
t	O
)	O
=p	O
(	O
z1	O
:	O
t	O
)	O
p	O
(	O
x1	O
:	O
t|z1	O
:	O
t	O
)	O
=	O
(	O
cid:24	O
)	O
p	O
(	O
z1	O
)	O
p	O
(	O
zt|zt−1	O
)	O
(	O
cid:25	O
)	O
(	O
cid:24	O
)	O
t	O
(	O
cid:20	O
)	O
(	O
cid:25	O
)	O
p	O
(	O
xt|zt	O
)	O
(	O
17.39	O
)	O
t=2	O
t=1	O
the	O
observations	O
in	O
an	O
hmm	O
can	O
be	O
discrete	B
or	O
continuous	O
.	O
if	O
they	O
are	O
discrete	B
,	O
it	O
is	O
common	O
for	O
the	O
observation	B
model	I
to	O
be	O
an	O
observation	B
matrix	O
:	O
p	O
(	O
xt	O
=	O
l|zt	O
=	O
k	O
,	O
θ	O
)	O
=	O
b	O
(	O
k	O
,	O
l	O
)	O
(	O
17.40	O
)	O
if	O
the	O
observations	O
are	O
continuous	O
,	O
it	O
is	O
common	O
for	O
the	O
observation	B
model	I
to	O
be	O
a	O
conditional	O
gaussian	O
:	O
p	O
(	O
xt|zt	O
=	O
k	O
,	O
θ	O
)	O
=	O
n	O
(	O
xt|μk	O
,	O
σk	O
)	O
(	O
17.41	O
)	O
figure	O
17.7	O
shows	O
an	O
example	O
where	O
we	O
have	O
3	O
states	O
,	O
each	O
of	O
which	O
emits	O
a	O
different	O
gaussian	O
.	O
the	O
resulting	O
model	O
is	O
similar	B
to	O
a	O
gaussian	O
mixture	B
model	I
,	O
except	O
the	O
cluster	O
membership	O
(	O
indeed	O
,	O
hmms	O
are	O
sometimes	O
called	O
markov	O
switching	O
models	O
has	O
markovian	O
dynamics	O
.	O
(	O
fruhwirth-schnatter	O
2007	O
)	O
.	O
)	O
we	O
see	O
that	O
we	O
tend	O
to	O
get	O
multiple	O
observations	O
in	O
the	O
same	O
location	O
,	O
and	O
then	O
a	O
sudden	O
jump	O
to	O
a	O
new	O
cluster	O
.	O
17.3.1	O
applications	O
of	O
hmms	O
hmms	O
can	O
be	O
used	O
as	O
black-box	B
density	O
models	O
on	O
sequences	O
.	O
they	O
have	O
the	O
advantage	O
over	O
markov	O
models	O
in	O
that	O
they	O
can	O
represent	O
long-range	O
dependencies	O
between	O
observations	O
,	O
mediated	O
via	O
the	O
latent	B
variables	O
.	O
in	O
particular	O
,	O
note	O
that	O
they	O
do	O
not	O
assume	O
the	O
markov	O
property	O
holds	O
for	O
the	O
observations	O
themselves	O
.	O
such	O
black-box	B
models	O
are	O
useful	O
for	O
time-	O
series	O
prediction	O
(	O
fraser	O
2008	O
)	O
.	O
they	O
can	O
also	O
be	O
used	O
to	O
deﬁne	O
class-conditional	O
densities	O
inside	O
a	O
generative	B
classiﬁer	I
.	O
however	O
,	O
it	O
is	O
more	O
common	O
to	O
imbue	O
the	O
hidden	B
states	O
with	O
some	O
desired	O
meaning	O
,	O
and	O
to	O
then	O
try	O
to	O
estimate	O
the	O
hidden	B
states	O
from	O
the	O
observations	O
,	O
i.e.	O
,	O
to	O
compute	O
p	O
(	O
zt|x1	O
:	O
t	O
)	O
if	O
we	O
are	O
17.3.	O
hidden	B
markov	O
models	O
605	O
bat	O
rat	O
cat	O
gnat	O
goat	O
d	O
i	O
m	O
m	O
1	O
i	O
begin	O
0	O
0	O
.	O
-	O
x	O
x	O
.	O
.	O
x	O
a	O
g	O
-	O
-	O
c	O
a	O
-	O
a	O
g	O
-	O
c	O
a	O
g	O
-	O
a	O
a	O
-	O
-	O
a	O
a	O
a	O
c	O
-	O
-	O
c	O
a	O
g	O
-	O
1	O
2	O
.	O
.	O
3	O
-	O
.	O
(	O
a	O
)	O
d	O
i	O
m	O
m	O
2	O
(	O
b	O
)	O
d	O
i	O
m	O
3	O
end	O
4	O
figure	O
17.8	O
(	O
a	O
)	O
some	O
dna	O
sequences	O
.	O
(	O
b	O
)	O
state	B
transition	I
diagram	I
for	O
a	O
proﬁle	O
hmm	O
.	O
source	O
:	O
figure	O
5.7	O
of	O
(	O
durbin	O
et	O
al	O
.	O
1998	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
richard	O
durbin	O
.	O
in	O
an	O
online	O
scenario	O
,	O
or	O
p	O
(	O
zt|x1	O
:	O
t	O
)	O
if	O
we	O
are	O
in	O
an	O
offline	B
scenario	O
(	O
see	O
section	O
17.4.1	O
for	O
further	O
discussion	O
of	O
the	O
differences	O
between	O
these	O
two	O
approaches	O
)	O
.	O
below	O
we	O
give	O
some	O
examples	O
of	O
applications	O
which	O
use	O
hmms	O
in	O
this	O
way	O
:	O
•	O
automatic	B
speech	I
recognition	I
.	O
here	O
xt	O
represents	O
features	B
extracted	O
from	O
the	O
speech	O
signal	O
,	O
and	O
zt	O
represents	O
the	O
word	O
that	O
is	O
being	O
spoken	O
.	O
the	O
transition	B
model	I
p	O
(	O
zt|zt−1	O
)	O
represents	O
the	O
language	B
model	I
,	O
and	O
the	O
observation	B
model	I
p	O
(	O
xt|zt	O
)	O
represents	O
the	O
acoustic	O
model	O
.	O
see	O
e.g.	O
,	O
(	O
jelinek	O
1997	O
;	O
jurafsky	O
and	O
martin	O
2008	O
)	O
for	O
details	O
.	O
•	O
activity	O
recognition	O
.	O
here	O
xt	O
represents	O
features	B
extracted	O
from	O
a	O
video	O
frame	O
,	O
and	O
zt	O
is	O
the	O
class	O
of	O
activity	O
the	O
person	O
is	O
engaged	O
in	O
(	O
e.g.	O
,	O
running	O
,	O
walking	O
,	O
sitting	O
,	O
etc	O
.	O
)	O
see	O
e.g.	O
,	O
(	O
szeliski	O
2010	O
)	O
for	O
details	O
.	O
•	O
part	B
of	I
speech	I
tagging	O
.	O
here	O
xt	O
represents	O
a	O
word	O
,	O
and	O
zt	O
represents	O
its	O
part	B
of	I
speech	I
(	O
noun	O
,	O
verb	O
,	O
adjective	O
,	O
etc	O
.	O
)	O
see	O
section	O
19.6.2.1	O
for	O
more	O
information	B
on	O
pos	O
tagging	O
and	O
606	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
related	O
tasks	O
.	O
•	O
gene	B
ﬁnding	I
.	O
here	O
xt	O
represents	O
the	O
dna	O
nucleotides	O
(	O
a	O
,	O
c	O
,	O
g	O
,	O
t	O
)	O
,	O
and	O
zt	O
represents	O
whether	O
we	O
are	O
inside	O
a	O
gene-coding	O
region	O
or	O
not	O
.	O
see	O
e.g.	O
,	O
(	O
schweikerta	O
et	O
al	O
.	O
2009	O
)	O
for	O
details	O
.	O
•	O
protein	O
sequence	O
alignment	O
.	O
here	O
xt	O
represents	O
an	O
amino	O
acid	O
,	O
and	O
zt	O
represents	O
whether	O
this	O
matches	O
the	O
latent	B
consensus	O
sequence	O
at	O
this	O
location	O
.	O
this	O
model	O
is	O
called	O
a	O
proﬁle	O
hmm	O
and	O
is	O
illustrated	O
in	O
figure	O
17.8.	O
the	O
hmm	O
has	O
3	O
states	O
,	O
called	O
match	O
,	O
insert	O
and	O
delete	O
.	O
if	O
zt	O
is	O
a	O
match	O
state	B
,	O
then	O
xt	O
is	O
equal	O
to	O
the	O
t	O
’	O
th	O
value	O
of	O
the	O
consensus	O
.	O
if	O
zt	O
is	O
an	O
insert	O
state	B
,	O
then	O
xt	O
is	O
generated	O
from	O
a	O
uniform	B
distribution	I
that	O
is	O
unrelated	O
to	O
the	O
consensus	B
sequence	I
.	O
if	O
zt	O
is	O
a	O
delete	O
state	B
,	O
then	O
xt	O
=	O
−	O
.	O
in	O
this	O
way	O
,	O
we	O
can	O
generate	O
noisy	O
copies	O
of	O
the	O
consensus	B
sequence	I
of	O
different	O
lengths	O
.	O
in	O
figure	O
17.8	O
(	O
a	O
)	O
,	O
the	O
consensus	O
is	O
“	O
agc	O
”	O
,	O
and	O
we	O
see	O
various	O
versions	O
of	O
this	O
below	O
.	O
a	O
path	B
through	O
the	O
state	B
transition	I
diagram	I
,	O
shown	O
in	O
figure	O
17.8	O
(	O
b	O
)	O
,	O
speciﬁes	O
how	O
to	O
align	O
a	O
sequence	O
to	O
the	O
consensus	O
,	O
e.g.	O
,	O
for	O
the	O
gnat	O
,	O
the	O
most	O
probable	O
path	B
is	O
d	O
,	O
d	O
,	O
i	O
,	O
i	O
,	O
i	O
,	O
m	O
.	O
this	O
means	O
we	O
delete	O
the	O
a	O
and	O
g	O
parts	O
of	O
the	O
consensus	B
sequence	I
,	O
we	O
insert	O
3	O
a	O
’	O
s	O
,	O
and	O
then	O
we	O
match	O
the	O
ﬁnal	O
c.	O
we	O
can	O
estimate	O
the	O
model	O
parameters	O
by	O
counting	O
the	O
number	O
of	O
such	O
transitions	O
,	O
and	O
the	O
number	O
of	O
emissions	O
from	O
each	O
kind	O
of	O
state	B
,	O
as	O
shown	O
in	O
figure	O
17.8	O
(	O
c	O
)	O
.	O
see	O
section	O
17.5	O
for	O
more	O
information	B
on	O
training	O
an	O
hmm	O
,	O
and	O
(	O
durbin	O
et	O
al	O
.	O
1998	O
)	O
for	O
details	O
on	O
proﬁle	O
hmms	O
.	O
note	O
that	O
for	O
some	O
of	O
these	O
tasks	O
,	O
conditional	B
random	I
ﬁelds	I
,	O
which	O
are	O
essentially	O
discrimi-	O
native	O
versions	O
of	O
hmms	O
,	O
may	O
be	O
more	O
suitable	O
;	O
see	O
chapter	O
19	O
for	O
details	O
.	O
17.4	O
inference	B
in	O
hmms	O
we	O
now	O
discuss	O
how	O
to	O
infer	O
the	O
hidden	B
state	O
sequence	O
of	O
an	O
hmm	O
,	O
assuming	O
the	O
parameters	O
are	O
known	O
.	O
exactly	O
the	O
same	O
algorithms	O
apply	O
to	O
other	O
chain-structured	O
graphical	O
models	O
,	O
such	O
as	O
chain	O
crfs	O
(	O
see	O
section	O
19.6.1	O
)	O
.	O
in	O
chapter	O
20	O
,	O
we	O
generalize	B
these	O
methods	O
to	O
arbitrary	O
graphs	O
.	O
and	O
in	O
section	O
17.5.2	O
,	O
we	O
show	O
how	O
we	O
can	O
use	O
the	O
output	O
of	O
inference	B
in	O
the	O
context	O
of	O
parameter	B
estimation	O
.	O
17.4.1	O
types	O
of	O
inference	B
problems	O
for	O
temporal	O
models	O
there	O
are	O
several	O
different	O
kinds	O
of	O
inferential	O
tasks	O
for	O
an	O
hmm	O
(	O
and	O
ssm	O
in	O
general	O
)	O
.	O
to	O
illustrate	O
the	O
differences	O
,	O
we	O
will	O
consider	O
an	O
example	O
called	O
the	O
occasionally	B
dishonest	I
casino	I
,	O
in	O
this	O
model	O
,	O
xt	O
∈	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
6	O
}	O
represents	O
which	O
dice	O
face	O
shows	O
from	O
(	O
durbin	O
et	O
al	O
.	O
1998	O
)	O
.	O
up	O
,	O
and	O
zt	O
represents	O
the	O
identity	O
of	O
the	O
dice	O
that	O
is	O
being	O
used	O
.	O
most	O
of	O
the	O
time	O
the	O
casino	O
uses	O
a	O
fair	O
dice	O
,	O
z	O
=	O
1	O
,	O
but	O
occasionally	O
it	O
switches	O
to	O
a	O
loaded	O
dice	O
,	O
z	O
=	O
2	O
,	O
for	O
a	O
short	O
period	B
.	O
if	O
z	O
=	O
1	O
the	O
observation	B
distribution	O
is	O
a	O
uniform	O
multinoulli	O
over	O
the	O
symbols	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
6	O
}	O
.	O
if	O
z	O
=	O
2	O
,	O
the	O
observation	B
distribution	O
is	O
skewed	O
towards	O
face	O
6	O
(	O
see	O
figure	O
17.9	O
)	O
.	O
if	O
we	O
sample	O
from	O
this	O
model	O
,	O
we	O
may	O
observe	O
data	O
such	O
as	O
the	O
following	O
:	O
rolls	O
:	O
die	O
:	O
listing	O
17.1	O
example	O
output	O
of	O
casinodemo	O
664153216162115234653214356634261655234232315142464156663246	O
llllllllllllllffffffllllllllllllllffffffffffffffffffllllllll	O
here	O
“	O
rolls	O
”	O
refers	O
to	O
the	O
observed	O
symbol	O
and	O
“	O
die	O
”	O
refers	O
to	O
the	O
hidden	B
state	O
(	O
l	O
is	O
loaded	O
and	O
f	O
is	O
fair	O
)	O
.	O
thus	O
we	O
see	O
that	O
the	O
model	O
generates	O
a	O
sequence	O
of	O
symbols	O
,	O
but	O
the	O
statistics	O
of	O
the	O
17.4.	O
inference	B
in	O
hmms	O
607	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:28	O
)	O
(	O
cid:24	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:28	O
)	O
(	O
cid:19	O
)	O
(	O
cid:20	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:18	O
)	O
(	O
cid:25	O
)	O
(	O
cid:21	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:18	O
)	O
(	O
cid:25	O
)	O
(	O
cid:22	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:18	O
)	O
(	O
cid:25	O
)	O
(	O
cid:23	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:18	O
)	O
(	O
cid:25	O
)	O
(	O
cid:24	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:18	O
)	O
(	O
cid:25	O
)	O
(	O
cid:25	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:18	O
)	O
(	O
cid:25	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:24	O
)	O
(	O
cid:20	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:18	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:21	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:18	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:22	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:18	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:23	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:18	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:24	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:18	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:25	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:24	O
)	O
(	O
cid:18	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
figure	O
17.9	O
an	O
hmm	O
for	O
the	O
occasionally	B
dishonest	I
casino	I
.	O
the	O
blue	O
arrows	O
visualize	O
the	O
state	B
transition	I
diagram	I
a.	O
based	O
on	O
(	O
durbin	O
et	O
al	O
.	O
1998	O
,	O
p54	O
)	O
.	O
filtered	O
smoothed	O
viterbi	O
1	O
)	O
d	O
e	O
d	O
a	O
o	O
l	O
(	O
p	O
0.5	O
1	O
)	O
d	O
e	O
d	O
a	O
o	O
l	O
(	O
p	O
0.5	O
0	O
0	O
50	O
100	O
150	O
roll	O
number	O
200	O
250	O
300	O
0	O
0	O
50	O
100	O
150	O
roll	O
number	O
200	O
250	O
300	O
1	O
0.5	O
)	O
d	O
e	O
d	O
a	O
o	O
=	O
1	O
l	O
f	O
,	O
r	O
i	O
a	O
=	O
0	O
(	O
e	O
t	O
a	O
t	O
s	O
p	O
a	O
m	O
0	O
0	O
50	O
100	O
150	O
roll	O
number	O
200	O
250	O
300	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
17.10	O
inference	B
in	O
the	O
dishonest	O
casino	O
.	O
vertical	O
gray	O
bars	O
denote	O
the	O
samples	B
that	O
we	O
generated	O
using	O
a	O
loaded	O
die	O
.	O
(	O
c	O
)	O
map	O
trajectory	O
.	O
figure	O
generated	O
by	O
casinodemo	O
.	O
(	O
a	O
)	O
filtered	O
estimate	O
of	O
probability	O
of	O
using	O
a	O
loaded	O
dice	O
.	O
(	O
b	O
)	O
smoothed	O
estimates	O
.	O
distribution	O
changes	O
abruptly	O
every	O
now	O
and	O
then	O
.	O
in	O
a	O
typical	O
application	O
,	O
we	O
just	O
see	O
the	O
rolls	O
and	O
want	O
to	O
infer	O
which	O
dice	O
is	O
being	O
used	O
.	O
but	O
there	O
are	O
different	O
kinds	O
of	O
inference	B
,	O
which	O
we	O
summarize	O
below	O
.	O
•	O
filtering	O
means	O
to	O
compute	O
the	O
belief	B
state	I
p	O
(	O
zt|x1	O
:	O
t	O
)	O
online	O
,	O
or	O
recursively	O
,	O
as	O
the	O
data	O
streams	O
in	O
.	O
this	O
is	O
called	O
“	O
ﬁltering	B
”	O
because	O
it	O
reduces	O
the	O
noise	O
more	O
than	O
simply	O
estimating	O
the	O
hidden	B
state	O
using	O
just	O
the	O
current	O
estimate	O
,	O
p	O
(	O
zt|xt	O
)	O
.	O
we	O
will	O
see	O
below	O
that	O
we	O
can	O
perform	O
ﬁltering	B
by	O
simply	O
applying	O
bayes	O
rule	O
in	O
a	O
sequential	B
fashion	O
.	O
see	O
figure	O
17.10	O
(	O
a	O
)	O
for	O
an	O
example	O
.	O
•	O
smoothing	O
means	O
to	O
compute	O
p	O
(	O
zt|x1	O
:	O
t	O
)	O
offline	B
,	O
given	O
all	O
the	O
evidence	B
.	O
see	O
figure	O
17.10	O
(	O
b	O
)	O
for	O
an	O
example	O
.	O
by	O
conditioning	B
on	O
past	O
and	O
future	O
data	O
,	O
our	O
uncertainty	B
will	O
be	O
signiﬁcantly	O
reduced	O
.	O
to	O
understand	O
this	O
intuitively	O
,	O
consider	O
a	O
detective	O
trying	O
to	O
ﬁgure	O
out	O
who	O
com-	O
mitted	O
a	O
crime	O
.	O
as	O
he	O
moves	O
through	O
the	O
crime	O
scene	O
,	O
his	O
uncertainty	B
is	O
high	O
until	O
he	O
ﬁnds	O
the	O
key	O
clue	O
;	O
then	O
he	O
has	O
an	O
“	O
aha	B
”	O
moment	O
,	O
his	O
uncertainty	B
is	O
reduced	O
,	O
and	O
all	O
the	O
previously	O
confusing	O
observations	O
are	O
,	O
in	O
hindsight	B
,	O
easy	O
to	O
explain	O
.	O
608	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
(	O
cid:73	O
)	O
(	O
cid:76	O
)	O
(	O
cid:79	O
)	O
(	O
cid:87	O
)	O
(	O
cid:72	O
)	O
(	O
cid:85	O
)	O
(	O
cid:76	O
)	O
(	O
cid:81	O
)	O
(	O
cid:74	O
)	O
(	O
cid:83	O
)	O
(	O
cid:85	O
)	O
(	O
cid:72	O
)	O
(	O
cid:71	O
)	O
(	O
cid:76	O
)	O
(	O
cid:70	O
)	O
(	O
cid:87	O
)	O
(	O
cid:76	O
)	O
(	O
cid:82	O
)	O
(	O
cid:81	O
)	O
(	O
cid:73	O
)	O
(	O
cid:76	O
)	O
(	O
cid:91	O
)	O
(	O
cid:72	O
)	O
(	O
cid:71	O
)	O
(	O
cid:16	O
)	O
(	O
cid:79	O
)	O
(	O
cid:68	O
)	O
(	O
cid:74	O
)	O
(	O
cid:3	O
)	O
(	O
cid:86	O
)	O
(	O
cid:80	O
)	O
(	O
cid:82	O
)	O
(	O
cid:82	O
)	O
(	O
cid:87	O
)	O
(	O
cid:75	O
)	O
(	O
cid:76	O
)	O
(	O
cid:81	O
)	O
(	O
cid:74	O
)	O
(	O
cid:73	O
)	O
(	O
cid:76	O
)	O
(	O
cid:91	O
)	O
(	O
cid:72	O
)	O
(	O
cid:71	O
)	O
(	O
cid:16	O
)	O
(	O
cid:79	O
)	O
(	O
cid:68	O
)	O
(	O
cid:74	O
)	O
(	O
cid:3	O
)	O
(	O
cid:86	O
)	O
(	O
cid:80	O
)	O
(	O
cid:82	O
)	O
(	O
cid:82	O
)	O
(	O
cid:87	O
)	O
(	O
cid:75	O
)	O
(	O
cid:76	O
)	O
(	O
cid:81	O
)	O
(	O
cid:74	O
)	O
(	O
cid:3	O
)	O
(	O
cid:11	O
)	O
(	O
cid:82	O
)	O
(	O
cid:73	O
)	O
(	O
cid:73	O
)	O
(	O
cid:79	O
)	O
(	O
cid:76	O
)	O
(	O
cid:81	O
)	O
(	O
cid:72	O
)	O
(	O
cid:12	O
)	O
(	O
cid:87	O
)	O
(	O
cid:87	O
)	O
(	O
cid:87	O
)	O
(	O
cid:87	O
)	O
(	O
cid:79	O
)	O
(	O
cid:75	O
)	O
(	O
cid:55	O
)	O
figure	O
17.11	O
the	O
main	O
kinds	O
of	O
inference	B
for	O
state-space	O
models	O
.	O
the	O
shaded	O
region	O
is	O
the	O
interval	O
for	O
which	O
we	O
have	O
data	O
.	O
the	O
arrow	O
represents	O
the	O
time	O
step	O
at	O
which	O
we	O
want	O
to	O
perform	O
inference	B
.	O
t	O
is	O
the	O
current	O
time	O
,	O
t	O
is	O
the	O
sequence	O
length	O
,	O
(	O
cid:7	O
)	O
is	O
the	O
lag	B
and	O
h	O
is	O
the	O
prediction	O
horizon	B
.	O
see	O
text	O
for	O
details	O
.	O
•	O
fixed	O
lag	B
smoothing	O
is	O
an	O
interesting	O
compromise	O
between	O
online	O
and	O
offline	B
estimation	O
;	O
it	O
involves	O
computing	O
p	O
(	O
zt−	O
(	O
cid:8	O
)	O
|x1	O
:	O
t	O
)	O
,	O
where	O
(	O
cid:2	O
)	O
>	O
0	O
is	O
called	O
the	O
lag	B
.	O
this	O
gives	O
better	O
performance	O
than	O
ﬁltering	B
,	O
but	O
incurs	O
a	O
slight	O
delay	O
.	O
by	O
changing	O
the	O
size	O
of	O
the	O
lag	B
,	O
one	O
can	O
trade	O
off	O
accuracy	O
vs	O
delay	O
.	O
•	O
prediction	O
instead	O
of	O
predicting	O
the	O
past	O
given	O
the	O
future	O
,	O
as	O
in	O
ﬁxed	O
lag	O
smoothing	O
,	O
we	O
might	O
want	O
to	O
predict	O
the	O
future	O
given	O
the	O
past	O
,	O
i.e.	O
,	O
to	O
compute	O
p	O
(	O
zt+h|x1	O
:	O
t	O
)	O
,	O
where	O
h	O
>	O
0	O
is	O
called	O
the	O
prediction	O
horizon	B
.	O
for	O
example	O
,	O
suppose	O
h	O
=	O
2	O
;	O
then	O
we	O
have	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
zt+1	O
zt	O
p	O
(	O
zt+2|x1	O
:	O
t	O
)	O
=	O
p	O
(	O
zt+2|zt+1	O
)	O
p	O
(	O
zt+1|zt	O
)	O
p	O
(	O
zt|x1	O
:	O
t	O
)	O
(	O
17.42	O
)	O
it	O
is	O
straightforward	O
to	O
perform	O
this	O
computation	O
:	O
we	O
just	O
power	O
up	O
the	O
transition	B
matrix	I
and	O
apply	O
it	O
to	O
the	O
current	O
belief	B
state	I
.	O
the	O
quantity	O
p	O
(	O
zt+h|x1	O
:	O
t	O
)	O
is	O
a	O
prediction	O
about	O
future	O
hidden	B
states	O
;	O
it	O
can	O
be	O
converted	O
into	O
a	O
prediction	O
about	O
future	O
observations	O
using	O
p	O
(	O
xt+h|x1	O
:	O
t	O
)	O
=	O
p	O
(	O
xt+h|zt+h	O
)	O
p	O
(	O
zt+h|x1	O
:	O
t	O
)	O
(	O
17.43	O
)	O
(	O
cid:4	O
)	O
zt+h	O
this	O
is	O
the	O
posterior	B
predictive	I
density	I
,	O
and	O
can	O
be	O
used	O
for	O
time-series	B
forecasting	I
(	O
see	O
(	O
fraser	O
2008	O
)	O
for	O
details	O
)	O
.	O
see	O
figure	O
17.11	O
for	O
a	O
sketch	O
of	O
the	O
relationship	O
between	O
ﬁltering	B
,	O
smoothing	O
,	O
and	O
prediction	O
.	O
p	O
(	O
z1	O
:	O
t|x1	O
:	O
t	O
)	O
,	O
which	O
is	O
a	O
most	O
prob-	O
in	O
the	O
context	O
of	O
hmms	O
,	O
this	O
is	O
known	O
as	O
viterbi	O
decoding	B
(	O
see	O
•	O
map	O
estimation	O
this	O
means	O
computing	O
arg	O
maxz1	O
:	O
t	O
able	O
state	B
sequence	O
.	O
17.4.	O
inference	B
in	O
hmms	O
609	O
section	O
17.4.4	O
)	O
.	O
figure	O
17.10	O
illustrates	O
the	O
difference	O
between	O
ﬁltering	B
,	O
smoothing	O
and	O
map	O
decoding	B
for	O
the	O
occasionally	B
dishonest	I
casino	I
hmm	O
.	O
we	O
see	O
that	O
the	O
smoothed	O
(	O
offline	B
)	O
estimate	O
is	O
indeed	O
smoother	O
than	O
the	O
ﬁltered	O
(	O
online	O
)	O
estimate	O
.	O
if	O
we	O
threshold	O
the	O
estimates	O
at	O
0.5	O
and	O
compare	O
to	O
the	O
true	O
sequence	O
,	O
we	O
ﬁnd	O
that	O
the	O
ﬁltered	O
method	O
makes	O
71	O
errors	O
out	O
of	O
300	O
,	O
and	O
the	O
smoothed	O
method	O
makes	O
49/300	O
;	O
the	O
map	O
path	B
makes	O
60/300	O
errors	O
.	O
it	O
is	O
not	O
surprising	O
that	O
smoothing	O
makes	O
fewer	O
errors	O
than	O
viterbi	O
,	O
since	O
the	O
optimal	O
way	O
to	O
min-	O
imize	O
bit-error	O
rate	B
is	O
to	O
threshold	O
the	O
posterior	O
marginals	O
(	O
see	O
section	O
5.7.1.1	O
)	O
.	O
nevertheless	O
,	O
for	O
some	O
applications	O
,	O
we	O
may	O
prefer	O
the	O
viterbi	O
decoding	B
,	O
as	O
we	O
discuss	O
in	O
section	O
17.4.4	O
.	O
•	O
posterior	O
samples	O
if	O
there	O
is	O
more	O
than	O
one	O
plausible	O
interpretation	O
of	O
the	O
data	O
,	O
it	O
can	O
be	O
useful	O
to	O
sample	O
from	O
the	O
posterior	O
,	O
z1	O
:	O
t	O
∼	O
p	O
(	O
z1	O
:	O
t|x1	O
:	O
t	O
)	O
.	O
these	O
sample	O
paths	O
contain	O
much	O
more	O
information	B
than	O
the	O
sequence	O
of	O
marginals	O
computed	O
by	O
smoothing	O
.	O
•	O
probability	B
of	I
the	I
evidence	I
we	O
can	O
compute	O
the	O
probability	B
of	I
the	I
evidence	I
,	O
p	O
(	O
x1	O
:	O
t	O
)	O
,	O
p	O
(	O
z1	O
:	O
t	O
,	O
x1	O
:	O
t	O
)	O
.	O
this	O
can	O
be	O
used	O
to	O
by	O
summing	O
up	O
over	O
all	O
hidden	O
paths	O
,	O
p	O
(	O
x1	O
:	O
t	O
)	O
=	O
classify	O
sequences	O
(	O
e.g.	O
,	O
if	O
the	O
hmm	O
is	O
used	O
as	O
a	O
class	O
conditional	O
density	O
)	O
,	O
for	O
model-based	B
clustering	I
,	O
for	O
anomaly	O
detection	O
,	O
etc	O
.	O
z1	O
:	O
t	O
(	O
cid:7	O
)	O
17.4.2	O
the	O
forwards	O
algorithm	O
we	O
now	O
describe	O
how	O
to	O
recursively	O
compute	O
the	O
ﬁltered	O
marginals	O
,	O
p	O
(	O
zt|x1	O
:	O
t	O
)	O
in	O
an	O
hmm	O
.	O
the	O
algorithm	O
has	O
two	O
steps	O
.	O
first	O
comes	O
the	O
prediction	O
step	O
,	O
in	O
which	O
we	O
compute	O
the	O
one-step-ahead	B
predictive	I
density	I
;	O
this	O
acts	O
as	O
the	O
new	O
prior	O
for	O
time	O
t	O
:	O
p	O
(	O
zt	O
=	O
j|x1	O
:	O
t−1	O
)	O
=	O
p	O
(	O
zt	O
=	O
j|zt−1	O
=	O
i	O
)	O
p	O
(	O
zt−1	O
=	O
i|x1	O
:	O
t−1	O
)	O
(	O
17.44	O
)	O
i	O
next	O
comes	O
the	O
update	O
step	O
,	O
in	O
which	O
we	O
absorb	O
the	O
observed	O
data	O
from	O
time	O
t	O
using	O
bayes	O
rule	O
:	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
αt	O
(	O
j	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
zt	O
=	O
j|x1	O
:	O
t	O
)	O
=	O
p	O
(	O
zt	O
=	O
j|xt	O
,	O
x1	O
:	O
t−1	O
)	O
p	O
(	O
xt|zt	O
=	O
j	O
,	O
x1	O
:	O
t−1	O
)	O
p	O
(	O
zt	O
=	O
j|x1	O
:	O
t−1	O
)	O
=	O
1	O
zt	O
where	O
the	O
normalization	O
constant	O
is	O
given	O
by	O
zt	O
(	O
cid:2	O
)	O
p	O
(	O
xt|x1	O
:	O
t−1	O
)	O
=	O
p	O
(	O
zt	O
=	O
j|x1	O
:	O
t−1	O
)	O
p	O
(	O
xt|zt	O
=	O
j	O
)	O
(	O
17.45	O
)	O
(	O
17.46	O
)	O
(	O
17.47	O
)	O
j	O
this	O
process	O
is	O
known	O
as	O
the	O
predict-update	B
cycle	I
.	O
the	O
distribution	O
p	O
(	O
zt|x1	O
:	O
t	O
)	O
is	O
called	O
the	O
(	O
ﬁltered	O
)	O
belief	B
state	I
at	O
time	O
t	O
,	O
and	O
is	O
a	O
vector	O
of	O
k	O
numbers	O
,	O
often	O
denoted	O
by	O
αt	O
.	O
in	O
matrix-	O
vector	O
notation	O
,	O
we	O
can	O
write	O
the	O
update	O
in	O
the	O
following	O
simple	O
form	O
:	O
αt	O
∝	O
ψt	O
(	O
cid:17	O
)	O
(	O
ψt	O
αt−1	O
)	O
(	O
17.48	O
)	O
where	O
ψt	O
(	O
j	O
)	O
=p	O
(	O
xt|zt	O
=	O
j	O
)	O
is	O
the	O
local	B
evidence	I
at	O
time	O
t	O
,	O
ψ	O
(	O
i	O
,	O
j	O
)	O
=p	O
(	O
zt	O
=	O
j|zt−1	O
=	O
i	O
)	O
is	O
the	O
transition	B
matrix	I
,	O
and	O
u	O
(	O
cid:17	O
)	O
v	O
is	O
the	O
hadamard	O
product	O
,	O
representing	O
elementwise	O
vector	O
multiplication	O
.	O
see	O
algorithm	O
6	O
for	O
the	O
pseudo-code	O
,	O
and	O
hmmfilter	O
for	O
some	O
matlab	O
code	O
.	O
610	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
in	O
addition	O
to	O
computing	O
the	O
hidden	B
states	O
,	O
we	O
can	O
use	O
this	O
algorithm	O
to	O
compute	O
the	O
log	O
probability	O
of	O
the	O
evidence	B
:	O
t	O
(	O
cid:4	O
)	O
t	O
(	O
cid:4	O
)	O
log	O
p	O
(	O
x1	O
:	O
t|θ	O
)	O
=	O
log	O
p	O
(	O
xt|x1	O
:	O
t−1	O
)	O
=	O
log	O
zt	O
(	O
17.49	O
)	O
t=1	O
t=1	O
(	O
we	O
need	O
to	O
work	O
in	O
the	O
log	O
domain	O
to	O
avoid	O
numerical	B
underﬂow	I
.	O
)	O
algorithm	O
17.1	O
:	O
forwards	O
algorithm	O
1	O
input	O
:	O
transition	O
matrices	O
ψ	O
(	O
i	O
,	O
j	O
)	O
=	O
p	O
(	O
zt	O
=	O
j|zt−1	O
=	O
i	O
)	O
,	O
local	B
evidence	I
vectors	O
ψt	O
(	O
j	O
)	O
=	O
p	O
(	O
xt|zt	O
=	O
j	O
)	O
,	O
initial	O
state	B
distribution	O
π	O
(	O
j	O
)	O
=	O
p	O
(	O
z1	O
=	O
j	O
)	O
;	O
2	O
[	O
α1	O
,	O
z1	O
]	O
=	O
normalize	O
(	O
ψ1	O
(	O
cid:17	O
)	O
π	O
)	O
;	O
3	O
for	O
t	O
=	O
2	O
:	O
t	O
do	O
4	O
5	O
return	O
α1	O
:	O
t	O
and	O
log	O
p	O
(	O
y1	O
:	O
t	O
)	O
=	O
[	O
αt	O
,	O
zt	O
]	O
=	O
normalize	O
(	O
ψt	O
(	O
cid:17	O
)	O
(	O
ψt	O
αt−1	O
)	O
)	O
;	O
(	O
cid:7	O
)	O
t	O
log	O
zt	O
;	O
(	O
cid:7	O
)	O
6	O
subroutine	O
:	O
[	O
v	O
,	O
z	O
]	O
=	O
normalize	O
(	O
u	O
)	O
:	O
z	O
=	O
j	O
uj	O
;	O
vj	O
=	O
uj/z	O
;	O
17.4.3	O
the	O
forwards-backwards	B
algorithm	I
in	O
section	O
17.4.2	O
,	O
we	O
explained	O
how	O
to	O
compute	O
the	O
ﬁltered	O
marginals	O
p	O
(	O
zt	O
=	O
j|x1	O
:	O
t	O
)	O
using	O
online	O
inference	O
.	O
we	O
now	O
discuss	O
how	O
to	O
compute	O
the	O
smoothed	O
marginals	O
,	O
p	O
(	O
zt	O
=	O
j|x1	O
:	O
t	O
)	O
,	O
using	O
offline	B
inference	O
.	O
17.4.3.1	O
basic	O
idea	O
the	O
key	O
decomposition	O
relies	O
on	O
the	O
fact	O
that	O
we	O
can	O
break	O
the	O
chain	O
into	O
two	O
parts	O
,	O
the	O
past	O
and	O
the	O
future	O
,	O
by	O
conditioning	B
on	O
zt	O
:	O
p	O
(	O
zt	O
=	O
j|x1	O
:	O
t	O
)	O
∝	O
p	O
(	O
zt	O
=	O
j	O
,	O
xt+1	O
:	O
t|x1	O
:	O
t	O
)	O
∝	O
p	O
(	O
zt	O
=	O
j|x1	O
:	O
t	O
)	O
p	O
(	O
xt+1	O
:	O
t|zt	O
=	O
j	O
,	O
x1	O
:	O
t	O
)	O
let	O
αt	O
(	O
j	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
zt	O
=	O
j|x1	O
:	O
t	O
)	O
be	O
the	O
ﬁltered	O
belief	B
state	I
as	O
before	O
.	O
also	O
,	O
deﬁne	O
βt	O
(	O
j	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
xt+1	O
:	O
t|zt	O
=	O
j	O
)	O
(	O
17.50	O
)	O
(	O
17.51	O
)	O
(	O
cid:7	O
)	O
likelihood	B
of	O
future	O
evidence	B
given	O
that	O
the	O
hidden	B
state	O
at	O
time	O
t	O
is	O
j.	O
as	O
the	O
conditional	O
(	O
note	O
that	O
this	O
is	O
not	O
a	O
probability	O
distribution	O
over	O
states	O
,	O
since	O
it	O
does	O
not	O
need	O
to	O
satisfy	O
j	O
βt	O
(	O
j	O
)	O
=	O
1	O
.	O
)	O
finally	O
,	O
deﬁne	O
γt	O
(	O
j	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
zt	O
=	O
j|x1	O
:	O
t	O
)	O
as	O
the	O
desired	O
smoothed	O
posterior	O
marginal	O
.	O
from	O
equation	O
17.50	O
,	O
we	O
have	O
γt	O
(	O
j	O
)	O
∝	O
αt	O
(	O
j	O
)	O
βt	O
(	O
j	O
)	O
(	O
17.52	O
)	O
(	O
17.53	O
)	O
17.4.	O
inference	B
in	O
hmms	O
611	O
we	O
have	O
already	O
described	O
how	O
to	O
recursively	O
compute	O
the	O
α	O
’	O
s	O
in	O
a	O
left-to-right	B
fashion	O
in	O
section	O
17.4.2.	O
we	O
now	O
describe	O
how	O
to	O
recursively	O
compute	O
the	O
β	O
’	O
s	O
in	O
a	O
right-to-left	O
fashion	O
.	O
if	O
we	O
have	O
already	O
computed	O
βt	O
,	O
we	O
can	O
compute	O
βt−1	O
as	O
follows	O
:	O
βt−1	O
(	O
i	O
)	O
=p	O
(	O
xt	O
:	O
t|zt−1	O
=	O
i	O
)	O
p	O
(	O
zt	O
=	O
j	O
,	O
xt	O
,	O
xt+1	O
:	O
t|zt−1	O
=	O
i	O
)	O
p	O
(	O
xt+1	O
:	O
t|zt	O
=	O
j	O
,	O
	O
p	O
(	O
xt+1	O
:	O
t|zt	O
=	O
j	O
)	O
p	O
(	O
xt|zt	O
=	O
j	O
,	O
	O
zt−1	O
=	O
i	O
,	O
xt	O
)	O
p	O
(	O
zt	O
=	O
j	O
,	O
xt|zt−1	O
=	O
i	O
)	O
zt−1	O
=	O
i	O
)	O
p	O
(	O
zt	O
=	O
j|zt−1	O
=	O
i	O
)	O
j	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
j	O
j	O
=	O
=	O
=	O
=	O
(	O
17.54	O
)	O
(	O
17.55	O
)	O
(	O
17.56	O
)	O
(	O
17.57	O
)	O
(	O
17.58	O
)	O
(	O
17.59	O
)	O
(	O
17.60	O
)	O
(	O
17.62	O
)	O
(	O
17.63	O
)	O
(	O
17.64	O
)	O
(	O
17.65	O
)	O
(	O
17.66	O
)	O
βt	O
(	O
j	O
)	O
ψt	O
(	O
j	O
)	O
ψ	O
(	O
i	O
,	O
j	O
)	O
j	O
we	O
can	O
write	O
the	O
resulting	O
equation	O
in	O
matrix-vector	O
form	O
as	O
βt−1	O
=	O
ψ	O
(	O
ψt	O
(	O
cid:17	O
)	O
βt	O
)	O
the	O
base	O
case	O
is	O
βt	O
(	O
i	O
)	O
=	O
p	O
(	O
xt	O
+1	O
:	O
t|zt	O
=	O
i	O
)	O
=p	O
(	O
∅|zt	O
=	O
i	O
)	O
=	O
1	O
which	O
is	O
the	O
probability	O
of	O
a	O
non-event	O
.	O
having	O
computed	O
the	O
forwards	O
and	O
backwards	O
messages	O
,	O
we	O
can	O
combine	O
them	O
to	O
compute	O
γt	O
(	O
j	O
)	O
∝	O
αt	O
(	O
j	O
)	O
βt	O
(	O
j	O
)	O
.	O
the	O
overall	O
algorithm	O
is	O
known	O
as	O
the	O
forwards-backwards	B
algorithm	I
.	O
the	O
pseudo	O
code	O
is	O
very	O
similar	B
to	O
the	O
forwards	O
case	O
;	O
see	O
hmmfwdback	O
for	O
an	O
implementation	O
.	O
we	O
can	O
think	O
of	O
this	O
algorithm	O
as	O
passing	O
“	O
messages	O
”	O
from	O
left	O
to	O
right	O
,	O
and	O
then	O
from	O
right	O
to	O
left	O
,	O
and	O
then	O
combining	O
them	O
at	O
each	O
node	O
.	O
we	O
will	O
generalize	B
this	O
intuition	O
in	O
section	O
20.2	O
,	O
when	O
we	O
discuss	O
belief	B
propagation	I
.	O
17.4.3.2	O
two-slice	O
smoothed	O
marginals	O
t−1	O
(	O
cid:4	O
)	O
when	O
we	O
estimate	O
the	O
parameters	O
of	O
the	O
transition	B
matrix	I
using	O
em	O
(	O
see	O
section	O
17.5	O
)	O
,	O
we	O
will	O
need	O
to	O
compute	O
the	O
expected	O
number	O
of	O
transitions	O
from	O
state	B
i	O
to	O
state	B
j	O
:	O
p	O
(	O
zt	O
=	O
i	O
,	O
zt+1	O
=	O
j|x1	O
:	O
t	O
)	O
(	O
17.61	O
)	O
the	O
term	O
p	O
(	O
zt	O
=	O
i	O
,	O
zt+1	O
=	O
j|x1	O
:	O
t	O
)	O
is	O
called	O
a	O
(	O
smoothed	O
)	O
two-slice	B
marginal	I
,	O
and	O
can	O
be	O
computed	O
as	O
follows	O
e	O
[	O
i	O
(	O
zt	O
=	O
i	O
,	O
zt+1	O
=	O
j	O
)	O
|x1	O
:	O
t	O
]	O
=	O
t−1	O
(	O
cid:4	O
)	O
t=1	O
nij	O
=	O
t=1	O
ξt	O
,	O
t+1	O
(	O
i	O
,	O
j	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
zt	O
=	O
i	O
,	O
zt+1	O
=	O
j|x1	O
:	O
t	O
)	O
∝	O
p	O
(	O
zt|x1	O
:	O
t	O
)	O
p	O
(	O
zt+1|zt	O
,	O
xt+1	O
:	O
t	O
)	O
∝	O
p	O
(	O
zt|x1	O
:	O
t	O
)	O
p	O
(	O
xt+1	O
:	O
t|zt	O
,	O
zt+1	O
)	O
p	O
(	O
zt+1|zt	O
)	O
∝	O
p	O
(	O
zt|x1	O
:	O
t	O
)	O
p	O
(	O
xt+1|zt+1	O
)	O
p	O
(	O
xt+2	O
:	O
t|zt+1	O
)	O
p	O
(	O
zt+1|zt	O
)	O
=	O
αt	O
(	O
i	O
)	O
φt+1	O
(	O
j	O
)	O
βt+1	O
(	O
j	O
)	O
ψ	O
(	O
i	O
,	O
j	O
)	O
612	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
in	O
matrix-vector	O
form	O
,	O
we	O
have	O
ξt	O
,	O
t+1	O
∝	O
ψ	O
(	O
cid:17	O
)	O
(	O
αt	O
(	O
φt+1	O
(	O
cid:17	O
)	O
βt+1	O
)	O
t	O
)	O
for	O
another	O
interpretation	O
of	O
these	O
equations	O
,	O
see	O
section	O
20.2.4.3	O
.	O
(	O
17.67	O
)	O
17.4.3.3	O
time	O
and	O
space	O
complexity	O
it	O
is	O
clear	O
that	O
a	O
straightforward	O
implementation	O
of	O
fb	O
takes	O
o	O
(	O
k	O
2t	O
)	O
time	O
,	O
since	O
we	O
must	O
perform	O
a	O
k	O
×	O
k	O
matrix	O
multiplication	O
at	O
each	O
step	O
.	O
for	O
some	O
applications	O
,	O
such	O
as	O
speech	B
recognition	I
,	O
k	O
is	O
very	O
large	O
,	O
so	O
the	O
o	O
(	O
k	O
2	O
)	O
term	O
becomes	O
prohibitive	O
.	O
fortunately	O
,	O
if	O
the	O
in	O
a	O
left-to-right	B
transition	I
matrix	I
is	O
sparse	B
,	O
we	O
can	O
reduce	O
this	O
substantially	O
.	O
for	O
example	O
,	O
transition	B
matrix	I
,	O
the	O
algorithm	O
takes	O
o	O
(	O
t	O
k	O
)	O
time	O
.	O
in	O
some	O
cases	O
,	O
we	O
can	O
exploit	O
special	O
properties	O
of	O
the	O
state	B
space	I
,	O
even	O
if	O
the	O
transition	B
matrix	I
is	O
not	O
sparse	O
.	O
in	O
particular	O
,	O
suppose	O
the	O
states	O
represent	O
a	O
discretization	O
of	O
an	O
underlying	O
continuous	O
state-space	O
,	O
and	O
the	O
transition	B
matrix	I
has	O
the	O
form	O
ψ	O
(	O
i	O
,	O
j	O
)	O
∝	O
exp	O
(	O
−σ2|zi	O
−	O
zj|	O
)	O
,	O
where	O
zi	O
is	O
the	O
continuous	O
vector	O
represented	O
by	O
state	B
i.	O
then	O
one	O
can	O
implement	O
the	O
forwards-	O
backwards	O
algorithm	O
in	O
o	O
(	O
t	O
k	O
log	O
k	O
)	O
time	O
.	O
this	O
is	O
very	O
useful	O
for	O
models	O
with	O
large	O
state	O
spaces	O
.	O
see	O
section	O
22.2.6.1	O
for	O
details	O
.	O
in	O
some	O
cases	O
,	O
the	O
bottleneck	B
is	O
memory	O
,	O
not	O
time	O
.	O
the	O
expected	B
sufficient	I
statistics	I
needed	O
t	O
ξt−1	O
,	O
t	O
(	O
i	O
,	O
j	O
)	O
;	O
this	O
takes	O
constant	O
space	O
(	O
independent	O
of	O
t	O
)	O
;	O
however	O
,	O
to	O
compute	O
by	O
em	O
are	O
them	O
,	O
we	O
need	O
o	O
(	O
kt	O
)	O
working	O
space	O
,	O
since	O
we	O
must	O
store	O
αt	O
for	O
t	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
t	O
until	O
we	O
do	O
the	O
backwards	O
pass	O
.	O
it	O
is	O
possible	O
to	O
devise	O
a	O
simple	O
divide-and-conquer	O
algorithm	O
that	O
reduces	O
the	O
space	O
complexity	O
from	O
o	O
(	O
kt	O
)	O
to	O
o	O
(	O
k	O
log	O
t	O
)	O
at	O
the	O
cost	O
of	O
increasing	O
the	O
running	O
time	O
from	O
o	O
(	O
k2t	O
)	O
to	O
o	O
(	O
k	O
2t	O
log	O
t	O
)	O
:	O
see	O
(	O
binder	O
et	O
al	O
.	O
1997	O
;	O
zweig	O
and	O
padmanabhan	O
2000	O
)	O
for	O
details	O
.	O
(	O
cid:7	O
)	O
17.4.4	O
the	O
viterbi	O
algorithm	O
the	O
viterbi	O
algorithm	O
(	O
viterbi	O
1967	O
)	O
can	O
be	O
used	O
to	O
compute	O
the	O
most	O
probable	O
sequence	O
of	O
states	O
in	O
a	O
chain-structured	O
graphical	B
model	I
,	O
i.e.	O
,	O
it	O
can	O
compute	O
z∗	O
=	O
arg	O
max	O
z1	O
:	O
t	O
p	O
(	O
z1	O
:	O
t|x1	O
:	O
t	O
)	O
this	O
is	O
equivalent	O
to	O
computing	O
a	O
shortest	O
path	B
through	O
the	O
trellis	B
diagram	I
in	O
figure	O
17.12	O
,	O
where	O
the	O
nodes	B
are	O
possible	O
states	O
at	O
each	O
time	O
step	O
,	O
and	O
the	O
node	O
and	O
edge	O
weights	O
are	O
log	O
probabilities	O
.	O
that	O
is	O
,	O
the	O
weight	O
of	O
a	O
path	B
z1	O
,	O
z2	O
,	O
.	O
.	O
.	O
,	O
zt	O
is	O
given	O
by	O
t	O
(	O
cid:4	O
)	O
log	O
π1	O
(	O
z1	O
)	O
+	O
log	O
φ1	O
(	O
z1	O
)	O
+	O
[	O
log	O
ψ	O
(	O
zt−1	O
,	O
zt	O
)	O
+	O
log	O
φt	O
(	O
zt	O
)	O
]	O
(	O
17.69	O
)	O
17.4.4.1	O
map	O
vs	O
mpe	O
t=2	O
before	O
discussing	O
how	O
the	O
algorithm	O
works	O
,	O
let	O
us	O
make	O
one	O
important	O
remark	O
:	O
the	O
(	O
jointly	O
)	O
most	O
probable	O
sequence	O
of	O
states	O
is	O
not	O
necessarily	O
the	O
same	O
as	O
the	O
sequence	O
of	O
(	O
marginally	O
)	O
most	O
probable	O
states	O
.	O
the	O
former	O
is	O
given	O
by	O
equation	O
17.68	O
,	O
and	O
is	O
what	O
viterbi	O
computes	O
,	O
whereas	O
the	O
latter	O
is	O
given	O
by	O
the	O
maximizer	B
of	I
the	I
posterior	I
marginals	I
or	O
mpm	O
:	O
ˆz	O
=	O
(	O
arg	O
max	O
z1	O
p	O
(	O
z1|x1	O
:	O
t	O
)	O
,	O
.	O
.	O
.	O
,	O
arg	O
max	O
p	O
(	O
zt|x1	O
:	O
t	O
)	O
)	O
zt	O
(	O
17.68	O
)	O
(	O
17.70	O
)	O
17.4.	O
inference	B
in	O
hmms	O
613	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:49	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
(	O
cid:40	O
)	O
(	O
cid:55	O
)	O
(	O
cid:36	O
)	O
(	O
cid:55	O
)	O
(	O
cid:54	O
)	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:17	O
)	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
(	O
cid:55	O
)	O
(	O
cid:50	O
)	O
(	O
cid:37	O
)	O
(	O
cid:54	O
)	O
(	O
cid:40	O
)	O
(	O
cid:53	O
)	O
(	O
cid:57	O
)	O
(	O
cid:36	O
)	O
(	O
cid:55	O
)	O
(	O
cid:44	O
)	O
(	O
cid:50	O
)	O
(	O
cid:49	O
)	O
figure	O
17.12	O
the	O
trellis	B
of	O
states	O
vs	O
time	O
for	O
a	O
markov	O
chain	O
.	O
based	O
on	O
(	O
rabiner	O
1989	O
)	O
.	O
as	O
a	O
simple	O
example	O
of	O
the	O
difference	O
,	O
consider	O
a	O
chain	O
with	O
two	O
time	O
steps	O
,	O
deﬁning	O
the	O
following	O
joint	O
:	O
x2	O
=	O
0	O
x2	O
=	O
1	O
x1	O
=	O
0	O
x1	O
=	O
1	O
0.04	O
0.36	O
0.4	O
0.34	O
0.66	O
0.3	O
0.3	O
0.6	O
the	O
joint	O
map	O
estimate	O
is	O
(	O
0	O
,	O
1	O
)	O
,	O
whereas	O
the	O
sequence	O
of	O
marginal	O
mpms	O
is	O
(	O
1	O
,	O
1	O
)	O
.	O
the	O
advantage	O
of	O
the	O
joint	O
map	O
estimate	O
is	O
that	O
is	O
is	O
always	O
globally	O
consistent	O
.	O
for	O
example	O
,	O
suppose	O
we	O
are	O
performing	O
speech	B
recognition	I
and	O
someones	O
says	O
“	O
recognize	O
speech	O
”	O
.	O
this	O
could	O
be	O
mis-heard	O
as	O
“	O
wreck	O
a	O
nice	O
beach	O
”	O
.	O
locally	O
it	O
may	O
appear	O
that	O
“	O
beach	O
”	O
is	O
the	O
most	O
probable	O
interpretation	O
of	O
that	O
particular	O
window	O
of	O
sound	O
,	O
but	O
when	O
we	O
add	O
the	O
requirement	O
that	O
the	O
data	O
be	O
explained	O
by	O
a	O
single	O
linguistically	O
plausible	O
path	B
,	O
this	O
interpretation	O
becomes	O
less	O
likely	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
mpm	O
estimates	O
can	O
be	O
more	O
robust	B
(	O
marroquin	O
et	O
al	O
.	O
1987	O
)	O
.	O
to	O
see	O
why	O
,	O
note	O
that	O
in	O
viterbi	O
,	O
when	O
we	O
estimate	O
zt	O
,	O
we	O
“	O
max	O
out	O
”	O
the	O
other	O
variables	O
:	O
z∗	O
t	O
=	O
arg	O
max	O
zt	O
z1	O
:	O
t−1	O
,	O
zt+1	O
:	O
t	O
p	O
(	O
z1	O
:	O
t−1	O
,	O
zt	O
,	O
zt+1	O
:	O
t|x1	O
:	O
t	O
)	O
max	O
(	O
cid:4	O
)	O
(	O
17.71	O
)	O
(	O
17.72	O
)	O
whereas	O
we	O
when	O
we	O
use	O
forwards-backwards	B
,	O
we	O
sum	O
out	O
the	O
other	O
variables	O
:	O
p	O
(	O
zt|x1	O
:	O
t	O
)	O
=	O
p	O
(	O
z1	O
:	O
t−1	O
,	O
zt	O
,	O
zt+1	O
:	O
t|x1	O
:	O
t	O
)	O
z1	O
:	O
t−1	O
,	O
zt+1	O
:	O
t	O
this	O
makes	O
the	O
mpm	O
in	O
equation	O
17.70	O
more	O
robust	B
,	O
since	O
we	O
estimate	O
each	O
node	O
averaging	O
over	O
its	O
neighbors	B
,	O
rather	O
than	O
conditioning	B
on	O
a	O
speciﬁc	O
value	O
of	O
its	O
neighbors.6	O
6.	O
in	O
general	O
,	O
we	O
may	O
want	O
to	O
mix	O
max	O
and	O
sum	O
.	O
for	O
example	O
,	O
consider	O
a	O
joint	B
distribution	I
where	O
we	O
observe	O
614	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
17.4.4.2	O
details	O
of	O
the	O
algorithm	O
it	O
is	O
tempting	O
to	O
think	O
that	O
we	O
can	O
implement	O
viterbi	O
by	O
just	O
replacing	O
the	O
sum-operator	O
in	O
forwards-backwards	B
with	O
a	O
max-operator	O
.	O
the	O
former	O
is	O
called	O
the	O
sum-product	B
,	O
and	O
the	O
latter	O
the	O
max-product	B
algorithm	O
.	O
if	O
there	O
is	O
a	O
unique	O
mode	B
,	O
running	O
max-product	O
and	O
then	O
computing	O
using	O
equation	O
17.70	O
will	O
give	O
the	O
same	O
result	O
as	O
using	O
equation	O
17.68	O
(	O
weiss	O
and	O
freeman	O
2001b	O
)	O
,	O
but	O
in	O
general	O
,	O
it	O
can	O
lead	O
to	O
incorrect	O
results	O
if	O
there	O
are	O
multiple	O
equally	O
probably	O
joint	O
assignments	O
.	O
the	O
reasons	O
is	O
that	O
each	O
node	O
breaks	O
ties	O
independently	O
and	O
hence	O
may	O
do	O
so	O
in	O
a	O
manner	O
that	O
is	O
inconsistent	O
with	O
its	O
neighbors	B
.	O
the	O
viterbi	O
algorithm	O
is	O
therefore	O
not	O
quite	O
as	O
simple	O
as	O
replacing	O
sum	O
with	O
max	O
.	O
in	O
particular	O
,	O
the	O
forwards	O
pass	O
does	O
use	O
max-	O
product	O
,	O
but	O
the	O
backwards	O
pass	O
uses	O
a	O
traceback	B
procedure	O
to	O
recover	O
the	O
most	O
probable	O
path	B
through	O
the	O
trellis	B
of	O
states	O
.	O
essentially	O
,	O
once	O
zt	O
picks	O
its	O
most	O
probable	O
state	B
,	O
the	O
previous	O
nodes	B
condition	O
on	O
this	O
event	O
,	O
and	O
therefore	O
they	O
will	O
break	O
ties	O
consistently	O
.	O
in	O
more	O
detail	O
,	O
deﬁne	O
δt	O
(	O
j	O
)	O
(	O
cid:2	O
)	O
max	O
z1	O
,	O
...	O
,	O
zt−1	O
p	O
(	O
z1	O
:	O
t−1	O
,	O
zt	O
=	O
j|x1	O
:	O
t	O
)	O
(	O
17.73	O
)	O
this	O
is	O
the	O
probability	O
of	O
ending	O
up	O
in	O
state	B
j	O
at	O
time	O
t	O
,	O
given	O
that	O
we	O
take	O
the	O
most	O
probable	O
path	B
.	O
the	O
key	O
insight	O
is	O
that	O
the	O
most	O
probable	O
path	B
to	O
state	B
j	O
at	O
time	O
t	O
must	O
consist	O
of	O
the	O
most	O
probable	O
path	B
to	O
some	O
other	O
state	B
i	O
at	O
time	O
t	O
−	O
1	O
,	O
followed	O
by	O
a	O
transition	O
from	O
i	O
to	O
j.	O
hence	O
δt	O
(	O
j	O
)	O
=	O
max	O
i	O
δt−1	O
(	O
i	O
)	O
ψ	O
(	O
i	O
,	O
j	O
)	O
φt	O
(	O
j	O
)	O
(	O
17.74	O
)	O
we	O
also	O
keep	O
track	O
of	O
the	O
most	O
likely	O
previous	O
state	B
,	O
for	O
each	O
possible	O
state	B
that	O
we	O
end	O
up	O
in	O
:	O
at	O
(	O
j	O
)	O
=	O
argmax	O
i	O
δt−1	O
(	O
i	O
)	O
ψ	O
(	O
i	O
,	O
j	O
)	O
φt	O
(	O
j	O
)	O
(	O
17.75	O
)	O
that	O
is	O
,	O
at	O
(	O
j	O
)	O
tells	O
us	O
the	O
most	O
likely	O
previous	O
state	B
on	O
the	O
most	O
probable	O
path	B
to	O
zt	O
=	O
j.	O
we	O
initialize	O
by	O
setting	O
δ1	O
(	O
j	O
)	O
=	O
πjφ1	O
(	O
j	O
)	O
and	O
we	O
terminate	O
by	O
computing	O
the	O
most	O
probable	O
ﬁnal	O
state	B
z∗	O
t	O
:	O
z∗	O
t	O
=	O
arg	O
max	O
δt	O
(	O
i	O
)	O
i	O
(	O
17.76	O
)	O
(	O
17.77	O
)	O
we	O
can	O
then	O
compute	O
the	O
most	O
probable	O
sequence	O
of	O
states	O
using	O
traceback	B
:	O
t+1	O
)	O
z∗	O
t	O
=	O
at+1	O
(	O
z∗	O
as	O
usual	O
,	O
we	O
have	O
to	O
worry	O
about	O
numerical	B
underﬂow	I
.	O
we	O
are	O
free	O
to	O
normalize	O
the	O
δt	O
terms	O
at	O
each	O
step	O
;	O
this	O
will	O
not	O
affect	O
the	O
maximum	O
.	O
however	O
,	O
unlike	O
the	O
forwards-backwards	B
case	O
,	O
(	O
17.78	O
)	O
xn	O
(	O
cid:2	O
)	O
let	O
n	O
be	O
the	O
remaining	O
nuisance	B
variables	I
.	O
we	O
deﬁne	O
the	O
map	O
estimate	O
as	O
x	O
∗	O
v	O
and	O
we	O
want	O
to	O
query	O
q	O
;	O
q	O
=	O
p	O
(	O
xq	O
,	O
xn|xv	O
)	O
,	O
where	O
we	O
max	O
over	O
xq	O
and	O
sum	O
over	O
xn	O
.	O
by	O
contrast	O
,	O
we	O
deﬁne	O
the	O
mpe	O
or	O
arg	O
maxxq	O
n	O
)	O
=	O
arg	O
maxxq	O
,	O
xn	O
p	O
(	O
xq	O
,	O
xn|xv	O
)	O
,	O
where	O
we	O
max	O
over	O
both	O
xq	O
and	O
xn	O
.	O
this	O
∗	O
most	O
probable	O
explanation	O
as	O
(	O
x	O
terminology	O
is	O
due	O
to	O
(	O
pearl	O
1988	O
)	O
,	O
although	O
it	O
is	O
not	O
widely	O
used	O
outside	O
the	O
bayes	O
net	O
literatire	O
.	O
obviously	O
map=mpe	O
if	O
n	O
=	O
∅	O
.	O
however	O
,	O
if	O
n	O
(	O
cid:3	O
)	O
=	O
∅	O
,	O
then	O
summing	O
out	O
the	O
nuisance	B
variables	I
can	O
give	O
different	O
results	O
than	O
maxing	O
them	O
out	O
.	O
summing	O
out	O
nuisance	B
variables	I
is	O
more	O
sensible	O
,	O
but	O
computationally	O
harder	O
,	O
because	O
of	O
the	O
need	O
to	O
combine	O
max	O
and	O
sum	O
operations	O
(	O
lerner	O
and	O
parr	O
2001	O
)	O
.	O
∗	O
q	O
,	O
x	O
17.4.	O
inference	B
in	O
hmms	O
615	O
(	O
cid:38	O
)	O
(	O
cid:20	O
)	O
(	O
cid:38	O
)	O
(	O
cid:21	O
)	O
(	O
cid:38	O
)	O
(	O
cid:22	O
)	O
(	O
cid:38	O
)	O
(	O
cid:23	O
)	O
(	O
cid:38	O
)	O
(	O
cid:24	O
)	O
(	O
cid:38	O
)	O
(	O
cid:25	O
)	O
(	O
cid:38	O
)	O
(	O
cid:26	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:24	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:22	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:21	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:21	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:26	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
a	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:24	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:23	O
)	O
(	O
cid:54	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:24	O
)	O
(	O
cid:54	O
)	O
(	O
cid:21	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:54	O
)	O
(	O
cid:22	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:22	O
)	O
(	O
cid:15	O
)	O
(	O
cid:3	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:22	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:22	O
)	O
(	O
cid:15	O
)	O
(	O
cid:3	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:54	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:23	O
)	O
(	O
cid:24	O
)	O
(	O
cid:54	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:54	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:26	O
)	O
(	O
cid:15	O
)	O
(	O
cid:3	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:21	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:26	O
)	O
(	O
cid:15	O
)	O
(	O
cid:3	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:26	O
)	O
(	O
cid:54	O
)	O
(	O
cid:21	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:26	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:28	O
)	O
(	O
cid:15	O
)	O
(	O
cid:3	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:26	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:28	O
)	O
(	O
cid:15	O
)	O
(	O
cid:3	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:54	O
)	O
(	O
cid:21	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:23	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:54	O
)	O
(	O
cid:21	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:20	O
)	O
(	O
cid:15	O
)	O
(	O
cid:3	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:20	O
)	O
(	O
cid:15	O
)	O
(	O
cid:3	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:24	O
)	O
(	O
cid:54	O
)	O
(	O
cid:22	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:23	O
)	O
(	O
cid:15	O
)	O
(	O
cid:3	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:24	O
)	O
(	O
cid:54	O
)	O
(	O
cid:22	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:26	O
)	O
(	O
cid:54	O
)	O
(	O
cid:22	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
(	O
b	O
)	O
figure	O
17.13	O
illustration	O
of	O
viterbi	O
decoding	B
in	O
a	O
simple	O
hmm	O
for	O
speech	B
recognition	I
.	O
(	O
a	O
)	O
a	O
3-state	O
hmm	O
for	O
a	O
single	O
phone	O
.	O
we	O
are	O
visualizing	B
the	O
state	B
transition	I
diagram	I
.	O
we	O
assume	O
the	O
observations	O
have	O
been	O
vector	O
quantized	O
into	O
7	O
possible	O
symbols	O
,	O
c1	O
,	O
.	O
.	O
.	O
,	O
c7	O
.	O
each	O
state	B
z1	O
,	O
z2	O
,	O
z3	O
has	O
a	O
different	O
distribution	O
over	O
these	O
symbols	O
.	O
based	O
on	O
figure	O
15.20	O
of	O
(	O
russell	O
and	O
norvig	O
2002	O
)	O
.	O
(	O
b	O
)	O
illustration	O
of	O
the	O
viterbi	O
algorithm	O
applied	O
to	O
this	O
model	O
,	O
with	O
data	O
sequence	O
c1	O
,	O
c3	O
,	O
c4	O
,	O
c6	O
.	O
the	O
columns	O
represent	O
time	O
,	O
and	O
the	O
rows	O
represent	O
states	O
.	O
an	O
arrow	O
from	O
state	B
i	O
at	O
t	O
−	O
1	O
to	O
state	B
j	O
at	O
t	O
is	O
annotated	O
with	O
two	O
numbers	O
:	O
the	O
ﬁrst	O
is	O
the	O
probability	O
of	O
the	O
i	O
→	O
j	O
transition	O
,	O
and	O
the	O
second	O
is	O
the	O
probability	O
of	O
generating	O
observation	B
xt	O
from	O
state	B
j.	O
the	O
bold	O
lines/	O
circles	O
represent	O
the	O
most	O
probable	O
sequence	O
of	O
states	O
.	O
based	O
on	O
figure	O
24.27	O
of	O
(	O
russell	O
and	O
norvig	O
1995	O
)	O
.	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:4	O
)	O
=	O
we	O
can	O
also	O
easily	O
work	O
in	O
the	O
log	O
domain	O
.	O
the	O
key	O
difference	O
is	O
that	O
log	O
max	O
=	O
max	O
log	O
,	O
whereas	O
log	O
log	O
.	O
hence	O
we	O
can	O
use	O
log	O
δt	O
(	O
j	O
)	O
(	O
cid:2	O
)	O
max	O
z1	O
:	O
t−1	O
=	O
max	O
i	O
log	O
p	O
(	O
z1	O
:	O
t−1	O
,	O
zt	O
=	O
j|x1	O
:	O
t	O
)	O
log	O
δt−1	O
(	O
i	O
)	O
+	O
log	O
ψ	O
(	O
i	O
,	O
j	O
)	O
+	O
log	O
φt	O
(	O
j	O
)	O
(	O
17.79	O
)	O
(	O
17.80	O
)	O
in	O
the	O
case	O
of	O
gaussian	O
observation	B
models	O
,	O
this	O
can	O
result	O
in	O
a	O
signiﬁcant	O
(	O
constant	O
factor	O
)	O
speedup	O
,	O
since	O
computing	O
log	O
p	O
(	O
xt|zt	O
)	O
can	O
be	O
much	O
faster	O
than	O
computing	O
p	O
(	O
xt|zt	O
)	O
for	O
a	O
high-	O
dimensional	O
gaussian	O
.	O
this	O
is	O
one	O
reason	O
why	O
the	O
viterbi	O
algorithm	O
is	O
widely	O
used	O
in	O
the	O
e	O
step	O
of	O
em	O
(	O
section	O
17.5.2	O
)	O
when	O
training	O
large	O
speech	B
recognition	I
systems	O
based	O
on	O
hmms	O
.	O
17.4.4.3	O
example	O
figure	O
17.13	O
gives	O
a	O
worked	O
example	O
of	O
the	O
viterbi	O
algorithm	O
,	O
based	O
on	O
(	O
russell	O
et	O
al	O
.	O
1995	O
)	O
.	O
suppose	O
we	O
observe	O
the	O
discrete	B
sequence	O
of	O
observations	O
x1:4	O
=	O
(	O
c1	O
,	O
c3	O
,	O
c4	O
,	O
c6	O
)	O
,	O
representing	O
codebook	B
entries	O
in	O
a	O
vector-quantized	O
version	O
of	O
a	O
speech	O
signal	O
.	O
the	O
model	O
starts	O
in	O
state	B
z1	O
.	O
the	O
probability	O
of	O
generating	O
c1	O
in	O
z1	O
is	O
0.5	O
,	O
so	O
we	O
have	O
δ1	O
(	O
1	O
)	O
=	O
0.5	O
,	O
and	O
δ1	O
(	O
i	O
)	O
=	O
0	O
for	O
all	O
other	O
states	O
.	O
next	O
we	O
can	O
self-transition	O
to	O
z1	O
with	O
probability	O
0.3	O
,	O
or	O
transition	O
to	O
z2	O
with	O
proabability	O
0.7.	O
if	O
we	O
end	O
up	O
in	O
z1	O
,	O
the	O
probability	O
of	O
generating	O
c3	O
is	O
0.3	O
;	O
if	O
we	O
end	O
up	O
in	O
z2	O
,	O
616	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
the	O
probability	O
of	O
generating	O
c3	O
is	O
0.2.	O
hence	O
we	O
have	O
δ2	O
(	O
1	O
)	O
=	O
δ1	O
(	O
1	O
)	O
ψ	O
(	O
1	O
,	O
1	O
)	O
φ2	O
(	O
1	O
)	O
=	O
0.5	O
·	O
0.3	O
·	O
0.3	O
=	O
0.045	O
δ2	O
(	O
2	O
)	O
=	O
δ1	O
(	O
1	O
)	O
ψ	O
(	O
1	O
,	O
2	O
)	O
φ2	O
(	O
2	O
)	O
=	O
0.5	O
·	O
0.7	O
·	O
0.2	O
=	O
0.07	O
(	O
17.81	O
)	O
(	O
17.82	O
)	O
thus	O
state	B
2	O
is	O
more	O
probable	O
at	O
t	O
=	O
2	O
;	O
see	O
the	O
second	O
column	O
of	O
figure	O
17.13	O
(	O
b	O
)	O
.	O
in	O
time	O
step	O
3	O
,	O
we	O
see	O
that	O
there	O
are	O
two	O
paths	O
into	O
z2	O
,	O
from	O
z1	O
and	O
from	O
z2	O
.	O
the	O
bold	O
arrow	O
indicates	O
that	O
the	O
latter	O
is	O
more	O
probable	O
.	O
hence	O
this	O
is	O
the	O
only	O
one	O
we	O
have	O
to	O
remember	O
.	O
the	O
algorithm	O
continues	O
in	O
this	O
way	O
until	O
we	O
have	O
reached	O
the	O
end	O
of	O
the	O
sequence	O
.	O
one	O
we	O
have	O
reached	O
the	O
end	O
,	O
we	O
can	O
follow	O
the	O
black	O
arrows	O
back	O
to	O
recover	O
the	O
map	O
path	B
(	O
which	O
is	O
1,2,2,3	O
)	O
.	O
17.4.4.4	O
time	O
and	O
space	O
complexity	O
is	O
clearly	O
o	O
(	O
k	O
2t	O
)	O
in	O
general	O
,	O
and	O
the	O
space	O
complexity	O
the	O
time	O
complexity	O
of	O
viterbi	O
is	O
o	O
(	O
kt	O
)	O
,	O
both	O
the	O
same	O
as	O
forwards-backwards	B
.	O
if	O
the	O
transition	B
matrix	I
has	O
the	O
form	O
ψ	O
(	O
i	O
,	O
j	O
)	O
∝	O
exp	O
(	O
−σ2||zi	O
−	O
zj||2	O
)	O
,	O
where	O
zi	O
is	O
the	O
continuous	O
vector	O
represented	O
by	O
state	B
i	O
,	O
we	O
can	O
implement	O
viterbi	O
in	O
o	O
(	O
t	O
k	O
)	O
time	O
,	O
instead	O
of	O
o	O
(	O
t	O
k	O
log	O
k	O
)	O
needed	O
by	O
forwards-backwards	B
.	O
see	O
section	O
22.2.6.1	O
for	O
details	O
.	O
17.4.4.5	O
n-best	O
list	O
the	O
viterbi	O
algorithm	O
returns	O
one	O
of	O
the	O
most	O
probable	O
paths	O
.	O
it	O
can	O
be	O
extended	O
to	O
return	O
the	O
top	O
n	O
paths	O
(	O
schwarz	O
and	O
chow	O
1990	O
;	O
nilsson	O
and	O
goldberger	O
2001	O
)	O
.	O
this	O
is	O
called	O
the	O
n-best	O
list	O
.	O
once	O
can	O
then	O
use	O
a	O
discriminative	B
method	O
to	O
rerank	B
the	O
paths	O
based	O
on	O
global	O
features	O
derived	O
from	O
the	O
fully	O
observed	O
state	O
sequence	O
(	O
as	O
well	O
as	O
the	O
visible	B
features	O
)	O
.	O
this	O
technique	O
is	O
widely	O
used	O
in	O
speech	B
recognition	I
.	O
for	O
example	O
,	O
consider	O
the	O
sentence	O
“	O
recognize	O
speech	O
”	O
.	O
it	O
is	O
possible	O
that	O
the	O
most	O
probable	O
interpretation	O
by	O
the	O
system	O
of	O
this	O
acoustic	O
signal	O
is	O
“	O
wreck	O
a	O
nice	O
speech	O
”	O
,	O
or	O
maybe	O
“	O
wreck	O
a	O
nice	O
beach	O
”	O
.	O
maybe	O
the	O
correct	O
interpretation	O
is	O
much	O
lower	O
down	O
on	O
the	O
list	O
.	O
however	O
,	O
by	O
using	O
a	O
re-ranking	O
system	O
,	O
we	O
may	O
be	O
able	O
to	O
improve	O
the	O
score	O
of	O
the	O
correct	O
interpretation	O
based	O
on	O
a	O
more	O
global	O
context	O
.	O
one	O
problem	O
with	O
the	O
n	O
-best	O
list	O
is	O
that	O
often	O
the	O
top	O
n	O
paths	O
are	O
very	O
similar	B
to	O
each	O
other	O
,	O
rather	O
than	O
representing	O
qualitatively	O
different	O
interpretations	O
of	O
the	O
data	O
.	O
instead	O
we	O
might	O
want	O
to	O
generate	O
a	O
more	O
diverse	O
set	O
of	O
paths	O
to	O
more	O
accurately	O
represent	O
posterior	O
uncertainty	O
.	O
one	O
way	O
to	O
do	O
this	O
is	O
to	O
sample	O
paths	O
from	O
the	O
posterior	O
,	O
as	O
we	O
discuss	O
below	O
.	O
for	O
some	O
other	O
ways	O
to	O
generate	O
diverse	O
map	O
estimates	O
,	O
see	O
e.g.	O
,	O
(	O
yadollahpour	O
et	O
al	O
.	O
2011	O
;	O
kulesza	O
and	O
taskar	O
2011	O
)	O
.	O
17.4.5	O
forwards	O
ﬁltering	O
,	O
backwards	O
sampling	O
it	O
is	O
often	O
useful	O
to	O
sample	O
paths	O
from	O
the	O
posterior	O
:	O
1	O
:	O
t	O
∼	O
p	O
(	O
z1	O
:	O
t|x1	O
:	O
t	O
)	O
zs	O
(	O
17.83	O
)	O
we	O
can	O
do	O
this	O
is	O
as	O
follow	O
:	O
run	O
forwards	O
backwards	O
,	O
to	O
compute	O
the	O
two-slice	O
smoothed	O
posteri-	O
ors	O
,	O
p	O
(	O
zt−1	O
,	O
t|x1	O
:	O
t	O
)	O
;	O
next	O
compute	O
the	O
conditionals	O
p	O
(	O
zt|zt−1	O
,	O
x1	O
:	O
t	O
)	O
by	O
normalizing	O
;	O
sample	O
from	O
the	O
initial	O
pair	O
of	O
states	O
,	O
z∗	O
t−1	O
,	O
x1	O
:	O
t	O
)	O
.	O
note	O
that	O
the	O
above	O
solution	O
requires	O
a	O
forwards-backwards	B
pass	O
,	O
and	O
then	O
an	O
additional	O
forwards	O
sampling	O
pass	O
.	O
an	O
alternative	O
is	O
to	O
do	O
the	O
forwards	O
pass	O
,	O
and	O
then	O
perform	O
sampling	O
1,2	O
∼	O
p	O
(	O
z1,2|x1	O
:	O
t	O
)	O
;	O
ﬁnally	O
,	O
recursively	O
sample	O
z∗	O
t	O
∼	O
p	O
(	O
zt|z∗	O
17.5.	O
learning	B
for	O
hmms	O
617	O
in	O
the	O
backwards	O
pass	O
.	O
the	O
key	O
insight	O
into	O
how	O
to	O
do	O
this	O
is	O
that	O
we	O
can	O
write	O
the	O
joint	O
from	O
right	O
to	O
left	O
using	O
p	O
(	O
z1	O
:	O
t|x1	O
:	O
t	O
)	O
=	O
p	O
(	O
zt|x1	O
:	O
t	O
)	O
p	O
(	O
zt|zt+1	O
,	O
x1	O
:	O
t	O
)	O
1	O
(	O
cid:20	O
)	O
t=t−1	O
we	O
can	O
then	O
sample	O
zt	O
given	O
future	O
sampled	O
states	O
using	O
t	O
∼	O
p	O
(	O
zt|zt+1	O
:	O
t	O
,	O
x1	O
:	O
t	O
)	O
=	O
p	O
(	O
zt|zt+1	O
,	O
zt+2	O
:	O
t	O
,	O
x1	O
:	O
t	O
,	O
xt+1	O
:	O
t	O
)	O
=	O
p	O
(	O
zt|zs	O
zs	O
t+1	O
,	O
x1	O
:	O
t	O
)	O
the	O
sampling	B
distribution	I
is	O
given	O
by	O
p	O
(	O
zt	O
=	O
i|zt+1	O
=	O
j	O
,	O
x1	O
:	O
t	O
)	O
=p	O
(	O
zt|zt+1	O
,	O
x1	O
:	O
t	O
,	O
xt+1	O
)	O
p	O
(	O
zt+1	O
,	O
zt|x1	O
:	O
t+1	O
)	O
p	O
(	O
zt+1|x1	O
:	O
t+1	O
)	O
=	O
∝	O
p	O
(	O
xt+1|zt+1	O
,	O
zt	O
,	O
x1	O
:	O
t	O
)	O
p	O
(	O
zt+1	O
,	O
zt|x1	O
:	O
t	O
)	O
p	O
(	O
xt+1|zt+1	O
)	O
p	O
(	O
zt+1|zt	O
,	O
x1	O
:	O
t	O
)	O
p	O
(	O
zt|x1	O
:	O
t	O
)	O
p	O
(	O
zt+1|x1	O
:	O
t+1	O
)	O
p	O
(	O
zt+1|x1	O
:	O
t+1	O
)	O
=	O
=	O
φt+1	O
(	O
j	O
)	O
ψ	O
(	O
i	O
,	O
j	O
)	O
αt	O
(	O
i	O
)	O
αt+1	O
(	O
j	O
)	O
(	O
17.84	O
)	O
(	O
17.85	O
)	O
(	O
17.86	O
)	O
(	O
17.87	O
)	O
(	O
17.88	O
)	O
(	O
17.89	O
)	O
(	O
17.90	O
)	O
the	O
base	O
case	O
is	O
t	O
∼	O
p	O
(	O
zt	O
=	O
i|x1	O
:	O
t	O
)	O
=	O
αt	O
(	O
i	O
)	O
zs	O
this	O
algorithm	O
forms	O
the	O
basis	O
of	O
blocked-gibbs	O
sampling	O
methods	O
for	O
parameter	B
inference	O
,	O
(	O
17.91	O
)	O
as	O
we	O
will	O
see	O
below	O
.	O
17.5	O
learning	B
for	O
hmms	O
we	O
now	O
discuss	O
how	O
to	O
estimate	O
the	O
parameters	O
θ	O
=	O
(	O
π	O
,	O
a	O
,	O
b	O
)	O
,	O
where	O
π	O
(	O
i	O
)	O
=	O
p	O
(	O
z1	O
=	O
i	O
)	O
is	O
the	O
initial	O
state	B
distribution	O
,	O
a	O
(	O
i	O
,	O
j	O
)	O
=p	O
(	O
zt	O
=	O
j|zt−1	O
=	O
i	O
)	O
is	O
the	O
transition	B
matrix	I
,	O
and	O
b	O
are	O
the	O
parameters	O
of	O
the	O
class-conditional	O
densities	O
p	O
(	O
xt|zt	O
=	O
j	O
)	O
.	O
we	O
ﬁrst	O
consider	O
the	O
case	O
where	O
z1	O
:	O
t	O
is	O
observed	O
in	O
the	O
training	B
set	I
,	O
and	O
then	O
the	O
harder	O
case	O
where	O
z1	O
:	O
t	O
is	O
hidden	B
.	O
17.5.1	O
training	O
with	O
fully	O
observed	O
data	O
if	O
we	O
observe	O
the	O
hidden	B
state	O
sequences	O
,	O
we	O
can	O
compute	O
the	O
mles	O
for	O
a	O
and	O
π	O
exactly	O
as	O
in	O
section	O
17.2.2.1.	O
if	O
we	O
use	O
a	O
conjugate	B
prior	I
,	O
we	O
can	O
also	O
easily	O
compute	O
the	O
posterior	O
.	O
the	O
details	O
on	O
how	O
to	O
estimate	O
b	O
depend	O
on	O
the	O
form	O
of	O
the	O
observation	B
model	I
.	O
the	O
situation	O
is	O
identical	O
to	O
ﬁtting	O
a	O
generative	B
classiﬁer	I
.	O
for	O
example	O
,	O
if	O
each	O
state	B
has	O
a	O
multinoulli	B
distribution	I
associated	O
with	O
it	O
,	O
with	O
parameters	O
bjl	O
=	O
p	O
(	O
xt	O
=	O
l|zt	O
=	O
j	O
)	O
,	O
where	O
l	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
l	O
}	O
represents	O
the	O
observed	O
symbol	O
,	O
the	O
mle	O
is	O
given	O
by	O
i	O
(	O
zi	O
,	O
t	O
=	O
j	O
,	O
xi	O
,	O
t	O
=	O
l	O
)	O
(	O
17.92	O
)	O
ˆbjl	O
=	O
n	O
x	O
jl	O
nj	O
,	O
n	O
x	O
jl	O
(	O
cid:2	O
)	O
n	O
(	O
cid:4	O
)	O
ti	O
(	O
cid:4	O
)	O
i=1	O
t=1	O
618	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
this	O
result	O
is	O
quite	O
intuitive	O
:	O
we	O
simply	O
add	O
up	O
the	O
number	O
of	O
times	O
we	O
are	O
in	O
state	B
j	O
and	O
we	O
see	O
a	O
symbol	O
l	O
,	O
and	O
divide	O
by	O
the	O
number	O
of	O
times	O
we	O
are	O
in	O
state	B
j.	O
similarly	O
,	O
if	O
each	O
state	B
has	O
a	O
gaussian	O
distribution	O
associated	O
with	O
it	O
,	O
we	O
have	O
(	O
from	O
sec-	O
tion	O
4.2.4	O
)	O
the	O
following	O
mles	O
:	O
ˆμk	O
=	O
xk	O
nk	O
,	O
ˆσk	O
=	O
(	O
xx	O
)	O
t	O
k	O
−	O
nk	O
ˆμk	O
ˆμt	O
k	O
nk	O
where	O
the	O
sufficient	B
statistics	I
are	O
given	O
by	O
n	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
i=1	O
ti	O
(	O
cid:4	O
)	O
ti	O
(	O
cid:4	O
)	O
t=1	O
xk	O
(	O
cid:2	O
)	O
(	O
xx	O
)	O
t	O
k	O
(	O
cid:2	O
)	O
i	O
(	O
zi	O
,	O
t	O
=	O
k	O
)	O
xi	O
,	O
t	O
i	O
(	O
zi	O
,	O
t	O
=	O
k	O
)	O
xi	O
,	O
txt	O
i	O
,	O
t	O
(	O
17.93	O
)	O
(	O
17.94	O
)	O
(	O
17.95	O
)	O
i=1	O
t=1	O
analogous	O
results	O
can	O
be	O
derived	O
for	O
other	O
kinds	O
of	O
distributions	O
.	O
one	O
can	O
also	O
easily	O
extend	O
all	O
of	O
these	O
results	O
to	O
compute	O
map	O
estimates	O
,	O
or	O
even	O
full	B
posteriors	O
over	O
the	O
parameters	O
.	O
17.5.2	O
em	O
for	O
hmms	O
(	O
the	O
baum-welch	O
algorithm	O
)	O
if	O
the	O
zt	O
variables	O
are	O
not	O
observed	O
,	O
we	O
are	O
in	O
a	O
situation	O
analogous	O
to	O
ﬁtting	O
a	O
mixture	B
model	I
.	O
the	O
most	O
common	O
approach	O
is	O
to	O
use	O
the	O
em	O
algorithm	O
to	O
ﬁnd	O
the	O
mle	O
or	O
map	O
parameters	O
,	O
although	O
of	O
course	O
one	O
could	O
use	O
other	O
gradient-based	O
methods	O
(	O
see	O
e.g.	O
,	O
(	O
baldi	O
and	O
chauvin	O
1994	O
)	O
)	O
.	O
in	O
this	O
section	O
,	O
we	O
derive	O
the	O
em	O
algorithm	O
.	O
when	O
applied	O
to	O
hmms	O
,	O
this	O
is	O
also	O
known	O
as	O
the	O
baum-welch	O
algorithm	O
(	O
baum	O
et	O
al	O
.	O
1970	O
)	O
.	O
17.5.2.1	O
e	O
step	O
it	O
is	O
straightforward	O
to	O
show	O
that	O
the	O
expected	B
complete	I
data	I
log	I
likelihood	I
is	O
given	O
by	O
q	O
(	O
θ	O
,	O
θold	O
)	O
=	O
k	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
k=1	O
e	O
(	O
cid:13	O
)	O
ti	O
(	O
cid:4	O
)	O
n	O
1	O
k	O
(	O
cid:14	O
)	O
k	O
(	O
cid:4	O
)	O
+	O
i=1	O
t=1	O
k=1	O
k	O
(	O
cid:4	O
)	O
k	O
(	O
cid:4	O
)	O
log	O
πk	O
+	O
e	O
[	O
njk	O
]	O
log	O
ajk	O
j=1	O
k=1	O
p	O
(	O
zt	O
=	O
k|xi	O
,	O
θold	O
)	O
log	O
p	O
(	O
xi	O
,	O
t|φk	O
)	O
e	O
=	O
(	O
cid:14	O
)	O
(	O
cid:13	O
)	O
n	O
1	O
k	O
where	O
the	O
expected	O
counts	O
are	O
given	O
by	O
p	O
(	O
zi1	O
=	O
k|xi	O
,	O
θold	O
)	O
ti	O
(	O
cid:4	O
)	O
ti	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
e	O
[	O
njk	O
]	O
=	O
t=2	O
i=1	O
i=1	O
e	O
[	O
nj	O
]	O
=	O
i=1	O
t=1	O
p	O
(	O
zi	O
,	O
t	O
=	O
j|xi	O
,	O
θold	O
)	O
p	O
(	O
zi	O
,	O
t−1	O
=	O
j	O
,	O
zi	O
,	O
t	O
=	O
k|xi	O
,	O
θold	O
)	O
(	O
17.96	O
)	O
(	O
17.97	O
)	O
(	O
17.98	O
)	O
(	O
17.99	O
)	O
(	O
17.100	O
)	O
17.5.	O
learning	B
for	O
hmms	O
619	O
these	O
expected	B
sufficient	I
statistics	I
can	O
be	O
computed	O
by	O
running	O
the	O
forwards-backwards	B
algo-	O
rithm	O
on	O
each	O
sequence	O
.	O
in	O
particular	O
,	O
this	O
algorithm	O
computes	O
the	O
following	O
smoothed	O
node	O
and	O
edge	O
marginals	O
:	O
γi	O
,	O
t	O
(	O
j	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
zt	O
=	O
j|xi,1	O
:	O
ti	O
ξi	O
,	O
t	O
(	O
j	O
,	O
k	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
zt−1	O
=	O
j	O
,	O
zt	O
=	O
k|xi,1	O
:	O
ti	O
,	O
θ	O
)	O
,	O
θ	O
)	O
(	O
17.101	O
)	O
(	O
17.102	O
)	O
(	O
17.103	O
)	O
(	O
17.106	O
)	O
(	O
17.107	O
)	O
(	O
17.108	O
)	O
17.5.2.2	O
m	O
step	O
based	O
on	O
section	O
11.3	O
,	O
we	O
have	O
that	O
the	O
m	O
step	O
for	O
a	O
and	O
π	O
is	O
to	O
just	O
normalize	O
the	O
expected	O
counts	O
:	O
(	O
cid:7	O
)	O
ˆajk	O
=	O
e	O
[	O
njk	O
]	O
k	O
(	O
cid:2	O
)	O
e	O
[	O
njk	O
(	O
cid:2	O
)	O
]	O
e	O
,	O
ˆπk	O
=	O
n	O
1	O
k	O
n	O
(	O
cid:13	O
)	O
(	O
cid:14	O
)	O
this	O
result	O
is	O
quite	O
intuitive	O
:	O
we	O
simply	O
add	O
up	O
the	O
expected	O
number	O
of	O
transitions	O
from	O
j	O
to	O
k	O
,	O
and	O
divide	O
by	O
the	O
expected	O
number	O
of	O
times	O
we	O
transition	O
from	O
j	O
to	O
anything	O
else	O
.	O
for	O
a	O
multinoulli	O
observation	O
model	O
,	O
the	O
expected	B
sufficient	I
statistics	I
are	O
n	O
(	O
cid:4	O
)	O
ti	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
e	O
[	O
mjl	O
]	O
=	O
γi	O
,	O
t	O
(	O
j	O
)	O
i	O
(	O
xi	O
,	O
t	O
=	O
l	O
)	O
=	O
i=1	O
t=1	O
i=1	O
t	O
:	O
xi	O
,	O
t=l	O
the	O
m	O
step	O
has	O
the	O
form	O
ˆbjl	O
=	O
e	O
[	O
mjl	O
]	O
e	O
[	O
nj	O
]	O
γi	O
,	O
t	O
(	O
j	O
)	O
(	O
17.104	O
)	O
(	O
17.105	O
)	O
this	O
result	O
is	O
quite	O
intuitive	O
:	O
we	O
simply	O
add	O
up	O
the	O
expected	O
number	O
of	O
times	O
we	O
are	O
in	O
state	B
j	O
and	O
we	O
see	O
a	O
symbol	O
l	O
,	O
and	O
divide	O
by	O
the	O
expected	O
number	O
of	O
times	O
we	O
are	O
in	O
state	B
j.	O
for	O
a	O
gaussian	O
observation	B
model	I
,	O
the	O
expected	B
sufficient	I
statistics	I
are	O
given	O
by	O
n	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
i=1	O
t=1	O
ti	O
(	O
cid:4	O
)	O
ti	O
(	O
cid:4	O
)	O
(	O
cid:13	O
)	O
t=1	O
e	O
[	O
xk	O
]	O
=	O
γi	O
,	O
t	O
(	O
k	O
)	O
xi	O
,	O
t	O
(	O
cid:13	O
)	O
e	O
(	O
cid:14	O
)	O
(	O
xx	O
)	O
t	O
k	O
=	O
i=1	O
γi	O
,	O
t	O
(	O
k	O
)	O
xi	O
,	O
txt	O
i	O
,	O
t	O
the	O
m	O
step	O
becomes	O
ˆμk	O
=	O
e	O
[	O
xk	O
]	O
e	O
[	O
nk	O
]	O
e	O
,	O
ˆσk	O
=	O
(	O
cid:14	O
)	O
−	O
e	O
[	O
nk	O
]	O
ˆμk	O
ˆμt	O
k	O
e	O
[	O
nk	O
]	O
(	O
xx	O
)	O
t	O
k	O
this	O
can	O
(	O
and	O
should	O
)	O
be	O
regularized	O
in	O
the	O
same	O
way	O
we	O
regularize	O
gmms	O
.	O
17.5.2.3	O
initialization	O
as	O
usual	O
with	O
em	O
,	O
we	O
must	O
take	O
care	O
to	O
ensure	O
that	O
we	O
initialize	O
the	O
parameters	O
carefully	O
,	O
to	O
minimize	O
the	O
chance	O
of	O
getting	O
stuck	O
in	O
poor	O
local	O
optima	O
.	O
there	O
are	O
several	O
ways	O
to	O
do	O
this	O
,	O
such	O
as	O
620	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
•	O
use	O
some	O
fully	O
labeled	O
data	O
to	O
initialize	O
the	O
parameters	O
.	O
•	O
initially	O
ignore	O
the	O
markov	O
dependencies	O
,	O
and	O
estimate	O
the	O
observation	B
parameters	O
using	O
the	O
standard	O
mixture	O
model	O
estimation	O
methods	O
,	O
such	O
as	O
k-means	O
or	O
em	O
.	O
•	O
randomly	O
initialize	O
the	O
parameters	O
,	O
use	O
multiple	B
restarts	I
,	O
and	O
pick	O
the	O
best	O
solution	O
.	O
techniques	O
such	O
as	O
deterministic	B
annealing	I
(	O
ueda	O
and	O
nakano	O
1998	O
;	O
rao	O
and	O
rose	O
2001	O
)	O
can	O
help	O
mitigate	O
the	O
effect	O
of	O
local	O
minima	O
.	O
also	O
,	O
just	O
as	O
k-means	O
is	O
often	O
used	O
to	O
initialize	O
em	O
for	O
gmms	O
,	O
so	O
it	O
is	O
common	O
to	O
initialize	O
em	O
for	O
hmms	O
using	O
viterbi	O
training	O
,	O
which	O
means	O
approximating	O
the	O
posterior	O
over	O
paths	O
with	O
the	O
single	O
most	O
probable	O
path	B
.	O
(	O
this	O
is	O
not	O
necessarily	O
a	O
good	O
idea	O
,	O
since	O
initially	O
the	O
parameters	O
are	O
often	O
poorly	O
estimated	O
,	O
so	O
the	O
viterbi	O
path	B
will	O
be	O
fairly	O
arbitrary	O
.	O
a	O
safer	O
option	O
is	O
to	O
start	O
training	O
using	O
forwards-backwards	B
,	O
and	O
to	O
switch	O
to	O
viterbi	O
near	O
convergence	O
.	O
)	O
17.5.3	O
bayesian	O
methods	O
for	O
“	O
ﬁtting	O
”	O
hmms	O
*	O
em	O
returns	O
a	O
map	O
estimate	O
of	O
the	O
parameters	O
.	O
in	O
this	O
section	O
,	O
we	O
brieﬂy	O
discuss	O
some	O
methods	O
for	O
bayesian	O
parameter	B
estimation	O
in	O
hmms	O
.	O
(	O
these	O
methods	O
rely	O
on	O
material	O
that	O
we	O
will	O
cover	O
later	O
in	O
the	O
book	O
.	O
)	O
one	O
approach	O
is	O
to	O
use	O
variational	O
bayes	O
em	O
(	O
vbem	O
)	O
,	O
which	O
we	O
discuss	O
in	O
general	O
terms	O
in	O
section	O
21.6.	O
the	O
details	O
for	O
the	O
hmm	O
case	O
can	O
be	O
found	O
in	O
(	O
mackay	O
1997	O
;	O
beal	O
2003	O
)	O
,	O
but	O
the	O
basic	O
idea	O
is	O
this	O
:	O
the	O
e	O
step	O
uses	O
forwards-backwards	B
,	O
but	O
where	O
(	O
roughly	O
speaking	O
)	O
we	O
plug	O
in	O
the	O
posterior	B
mean	I
parameters	O
instead	O
of	O
the	O
map	O
estimates	O
.	O
the	O
m	O
step	O
updates	O
the	O
parameters	O
of	O
the	O
conjugate	O
posteriors	O
,	O
instead	O
of	O
updating	O
the	O
parameters	O
themselves	O
.	O
an	O
alternative	O
to	O
vbem	O
is	O
to	O
use	O
mcmc	O
.	O
a	O
particularly	O
appealing	O
algorithm	O
is	O
block	O
gibbs	O
sampling	O
,	O
which	O
we	O
discuss	O
in	O
general	O
terms	O
in	O
section	O
24.2.8.	O
the	O
details	O
for	O
the	O
hmm	O
case	O
can	O
be	O
found	O
in	O
(	O
fruhwirth-schnatter	O
2007	O
)	O
,	O
but	O
the	O
basic	O
idea	O
is	O
this	O
:	O
we	O
sample	O
z1	O
:	O
t	O
given	O
the	O
data	O
and	O
parameters	O
using	O
forwards-ﬁltering	O
,	O
backwards-sampling	O
,	O
and	O
we	O
then	O
sample	O
the	O
parameters	O
from	O
their	O
posteriors	O
,	O
conditional	O
on	O
the	O
sampled	O
latent	B
paths	O
.	O
this	O
is	O
simple	O
to	O
implement	O
,	O
but	O
one	O
does	O
need	O
to	O
take	O
care	O
of	O
unidentiﬁability	O
(	O
label	B
switching	I
)	O
,	O
just	O
as	O
with	O
mixture	B
models	O
(	O
see	O
section	O
11.3.1	O
)	O
.	O
17.5.4	O
discriminative	B
training	O
(	O
cid:26	O
)	O
n	O
sometimes	O
hmms	O
are	O
used	O
as	O
the	O
class	O
conditional	O
density	O
inside	O
a	O
generative	B
classiﬁer	I
.	O
in	O
this	O
case	O
,	O
p	O
(	O
x|y	O
=	O
c	O
,	O
θ	O
)	O
can	O
be	O
computed	O
using	O
the	O
forwards	O
algorithm	O
.	O
we	O
can	O
easily	O
maximize	O
the	O
i=1	O
p	O
(	O
xi	O
,	O
yi|θ	O
)	O
by	O
using	O
em	O
(	O
or	O
some	O
other	O
method	O
)	O
to	O
ﬁt	O
the	O
hmm	O
for	O
each	O
joint	O
likelihood	O
class-conditional	B
density	I
separately	O
.	O
(	O
cid:20	O
)	O
however	O
,	O
we	O
might	O
like	O
to	O
ﬁnd	O
the	O
parameters	O
that	O
maximize	O
the	O
conditional	B
likelihood	I
n	O
(	O
cid:20	O
)	O
(	O
17.109	O
)	O
p	O
(	O
yi|xi	O
,	O
θ	O
)	O
=	O
i=1	O
i	O
(	O
cid:7	O
)	O
p	O
(	O
yi|θ	O
)	O
p	O
(	O
xi|yi	O
,	O
θ	O
)	O
c	O
p	O
(	O
yi	O
=	O
c|θ	O
)	O
p	O
(	O
xi|c	O
,	O
θ	O
)	O
this	O
is	O
more	O
expensive	O
than	O
maximizing	O
the	O
joint	O
likelihood	O
,	O
since	O
the	O
denominator	O
couples	O
all	O
c	O
class-conditional	O
hmms	O
together	O
.	O
furthermore	O
,	O
em	O
can	O
no	O
longer	O
be	O
used	O
,	O
and	O
one	O
must	O
resort	O
17.6.	O
generalizations	O
of	O
hmms	O
621	O
to	O
generic	O
gradient	O
based	O
methods	O
.	O
nevertheless	O
,	O
discriminative	B
training	O
can	O
result	O
in	O
improved	O
accuracies	O
.	O
the	O
standard	O
practice	O
in	O
speech	B
recognition	I
is	O
to	O
initially	O
train	O
the	O
generative	O
models	O
separately	O
using	O
em	O
,	O
and	O
then	O
to	O
ﬁne	O
tune	O
them	O
discriminatively	O
(	O
jelinek	O
1997	O
)	O
.	O
17.5.5	O
model	B
selection	I
in	O
hmms	O
,	O
the	O
two	O
main	O
model	B
selection	I
issues	O
are	O
:	O
how	O
many	O
states	O
,	O
and	O
what	O
topology	O
to	O
use	O
for	O
the	O
state	B
transition	I
diagram	I
.	O
we	O
discuss	O
both	O
of	O
these	O
issues	O
below	O
.	O
17.5.5.1	O
choosing	O
the	O
number	O
of	O
hidden	B
states	O
choosing	O
the	O
number	O
of	O
hidden	B
states	O
k	O
in	O
an	O
hmm	O
is	O
analogous	O
to	O
the	O
problem	O
of	O
choosing	O
the	O
number	O
of	O
mixture	B
components	O
.	O
here	O
are	O
some	O
possible	O
solutions	O
:	O
•	O
use	O
grid-search	O
over	O
a	O
range	O
of	O
k	O
’	O
s	O
,	O
using	O
as	O
an	O
objective	O
function	O
cross-validated	O
likelihood	B
,	O
the	O
bic	O
score	O
,	O
or	O
a	O
variational	O
lower	O
bound	O
to	O
the	O
log-marginal	O
likelihood	B
.	O
•	O
use	O
reversible	O
jump	O
mcmc	O
.	O
see	O
(	O
fruhwirth-schnatter	O
2007	O
)	O
for	O
details	O
.	O
note	O
that	O
this	O
is	O
very	O
slow	O
and	O
is	O
not	O
widely	O
used	O
.	O
•	O
use	O
variational	O
bayes	O
to	O
“	O
extinguish	O
”	O
unwanted	O
components	O
,	O
by	O
analogy	O
to	O
the	O
gmm	O
case	O
discussed	O
in	O
section	O
21.6.1.6.	O
see	O
(	O
mackay	O
1997	O
;	O
beal	O
2003	O
)	O
for	O
details	O
.	O
•	O
use	O
an	O
“	O
inﬁnite	O
hmm	O
”	O
,	O
which	O
is	O
based	O
on	O
the	O
hierarchical	O
dirichlet	O
process	O
.	O
see	O
e.g.	O
,	O
(	O
beal	O
et	O
al	O
.	O
2002	O
;	O
teh	O
et	O
al	O
.	O
2006	O
)	O
for	O
details	O
.	O
17.5.5.2	O
structure	B
learning	I
the	O
term	O
structure	B
learning	I
in	O
the	O
context	O
of	O
hmms	O
refers	O
to	O
learning	B
a	O
sparse	B
transition	O
matrix	O
.	O
that	O
is	O
,	O
we	O
want	O
to	O
learn	O
the	O
structure	O
of	O
the	O
state	B
transition	I
diagram	I
,	O
not	O
the	O
structure	O
of	O
the	O
graphical	B
model	I
(	O
which	O
is	O
ﬁxed	O
)	O
.	O
a	O
large	O
number	O
of	O
heuristic	O
methods	O
have	O
been	O
proposed	O
.	O
most	O
alternate	O
between	O
parameter	B
estimation	O
and	O
some	O
kind	O
of	O
heuristic	O
split	B
merge	I
method	O
(	O
see	O
e.g.	O
,	O
(	O
stolcke	O
and	O
omohundro	O
1992	O
)	O
)	O
.	O
alternatively	O
,	O
one	O
can	O
pose	O
the	O
problem	O
as	O
map	O
estimation	O
using	O
a	O
minimum	B
entropy	I
prior	I
,	O
of	O
the	O
form	O
p	O
(	O
ai	O
,	O
:	O
)	O
∝	O
exp	O
(	O
−h	O
(	O
ai	O
,	O
:	O
)	O
)	O
(	O
17.110	O
)	O
this	O
prior	O
prefers	O
states	O
whose	O
outgoing	O
distribution	O
is	O
nearly	O
deterministic	O
,	O
and	O
hence	O
has	O
low	O
entropy	O
(	O
brand	O
1999	O
)	O
.	O
the	O
corresponding	O
m	O
step	O
can	O
not	O
be	O
solved	O
in	O
closed	O
form	O
,	O
but	O
numerical	O
methods	O
can	O
be	O
used	O
.	O
the	O
trouble	O
with	O
this	O
is	O
that	O
we	O
might	O
prune	O
out	O
all	O
incoming	O
transitions	O
to	O
a	O
state	B
,	O
creating	O
isolated	O
“	O
islands	O
”	O
in	O
state-space	O
.	O
the	O
inﬁnite	O
hmm	O
presents	O
an	O
interesting	O
alternative	O
to	O
these	O
methods	O
.	O
see	O
e.g.	O
,	O
(	O
beal	O
et	O
al	O
.	O
2002	O
;	O
teh	O
et	O
al	O
.	O
2006	O
)	O
for	O
details	O
.	O
17.6	O
generalizations	O
of	O
hmms	O
many	O
variants	O
of	O
the	O
basic	O
hmm	O
model	O
have	O
been	O
proposed	O
.	O
we	O
brieﬂy	O
discuss	O
some	O
of	O
them	O
below	O
.	O
622	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
dt−1	O
dt	O
dt+1	O
qt	O
−	O
1	O
qt	O
qt	O
−	O
1	O
xt−1	O
xt	O
xt+1	O
figure	O
17.14	O
encoding	O
a	O
hidden	B
semi-markov	O
model	O
as	O
a	O
dgm	O
.	O
dt	O
are	O
deterministic	O
duration	O
counters	O
.	O
17.6.1	O
variable	O
duration	O
(	O
semi-markov	O
)	O
hmms	O
in	O
a	O
standard	O
hmm	O
,	O
the	O
probability	O
we	O
remain	O
in	O
state	B
i	O
for	O
exactly	O
d	O
steps	O
is	O
p	O
(	O
ti	O
=	O
d	O
)	O
=	O
(	O
1	O
−	O
aii	O
)	O
ad	O
ii	O
∝	O
exp	O
(	O
d	O
log	O
aii	O
)	O
(	O
17.111	O
)	O
where	O
aii	O
is	O
the	O
self-loop	O
probability	O
.	O
this	O
is	O
called	O
the	O
geometric	B
distribution	I
.	O
however	O
,	O
this	O
kind	O
of	O
exponentially	O
decaying	O
function	O
of	O
d	O
is	O
sometimes	O
unrealistic	O
.	O
to	O
allow	O
for	O
more	O
general	O
durations	O
,	O
one	O
can	O
use	O
a	O
semi-markov	O
model	O
.	O
it	O
is	O
called	O
semi-	O
markov	O
because	O
to	O
predict	O
the	O
next	O
state	B
,	O
it	O
is	O
not	O
sufficient	O
to	O
condition	O
on	O
the	O
past	O
state	B
:	O
we	O
also	O
need	O
to	O
know	O
how	O
long	O
we	O
’	O
ve	O
been	O
in	O
that	O
state	B
.	O
when	O
the	O
state	B
space	I
is	O
not	O
observed	O
directly	O
,	O
the	O
result	O
is	O
called	O
a	O
hidden	B
semi-markov	O
model	O
(	O
hsmm	O
)	O
,	O
a	O
variable	O
duration	O
hmm	O
,	O
or	O
an	O
explicit	O
duration	O
hmm	O
.	O
hsmms	O
are	O
widely	O
used	O
in	O
many	O
gene	B
ﬁnding	I
programs	O
,	O
since	O
the	O
length	O
distribution	O
of	O
exons	O
and	O
introns	O
is	O
not	O
geometric	O
(	O
see	O
e.g.	O
,	O
(	O
schweikerta	O
et	O
al	O
.	O
2009	O
)	O
)	O
,	O
and	O
in	O
some	O
chip-seq	O
data	O
analysis	O
programs	O
(	O
see	O
e.g.	O
,	O
(	O
kuan	O
et	O
al	O
.	O
2009	O
)	O
)	O
.	O
hsmms	O
are	O
useful	O
not	O
only	O
because	O
they	O
can	O
model	O
the	O
waiting	O
time	O
of	O
each	O
state	B
more	O
accurately	O
,	O
but	O
also	O
because	O
they	O
can	O
model	O
the	O
distribution	O
of	O
a	O
whole	O
batch	B
of	O
observations	O
at	O
once	O
,	O
instead	O
of	O
assuming	O
all	O
observations	O
are	O
conditionally	O
iid	O
.	O
that	O
is	O
,	O
they	O
can	O
use	O
likelihood	B
models	O
of	O
the	O
form	O
p	O
(	O
xt	O
:	O
t+l|zt	O
=	O
k	O
,	O
dt	O
=	O
l	O
)	O
,	O
which	O
generate	O
l	O
correlated	O
observations	O
if	O
the	O
duration	O
in	O
state	B
k	O
is	O
for	O
l	O
time	O
steps	O
.	O
this	O
is	O
useful	O
for	O
modeling	O
data	O
that	O
is	O
piecewise	O
linear	O
,	O
or	O
shows	O
other	O
local	O
trends	O
(	O
ostendorf	O
et	O
al	O
.	O
1996	O
)	O
.	O
17.6.1.1	O
hsmm	O
as	O
augmented	O
hmms	O
one	O
way	O
to	O
represent	O
a	O
hsmm	O
is	O
to	O
use	O
the	O
graphical	B
model	I
shown	O
in	O
figure	O
17.14	O
.	O
(	O
in	O
this	O
ﬁgure	O
,	O
we	O
have	O
assumed	O
the	O
observations	O
are	O
iid	B
within	O
each	O
state	B
,	O
but	O
this	O
is	O
not	O
required	O
,	O
as	O
mentioned	O
above	O
.	O
)	O
the	O
dt	O
∈	O
{	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
node	O
is	O
a	O
state	B
duration	O
counter	O
,	O
where	O
d	O
is	O
the	O
maximum	O
duration	O
of	O
any	O
state	B
.	O
when	O
we	O
ﬁrst	O
enter	O
state	B
j	O
,	O
we	O
sample	O
dt	O
from	O
the	O
duration	O
distribution	O
for	O
that	O
state	B
,	O
dt	O
∼	O
pj	O
(	O
·	O
)	O
.	O
thereafer	O
,	O
dt	O
deterministically	O
counts	O
down	O
17.6.	O
generalizations	O
of	O
hmms	O
0.012	O
0.01	O
0.008	O
0.006	O
0.004	O
0.002	O
0	O
0	O
p	O
1	O
1	O
−	O
p	O
p	O
2	O
p	O
3	O
1	O
−	O
p	O
p	O
4	O
1	O
−	O
p	O
(	O
a	O
)	O
623	O
n=1	O
n=2	O
n=5	O
100	O
200	O
300	O
400	O
500	O
600	O
(	O
b	O
)	O
figure	O
17.15	O
over	O
sequence	O
lengths	O
,	O
for	O
p	O
=	O
0.99	O
and	O
various	O
n.	O
figure	O
generated	O
by	O
hmmselfloopdist	O
.	O
(	O
a	O
)	O
a	O
markov	O
chain	O
with	O
n	O
=	O
4	O
repeated	O
states	O
and	O
self	B
loops	I
.	O
(	O
b	O
)	O
the	O
resulting	O
distribution	O
until	O
dt	O
=	O
0.	O
while	O
dt	O
>	O
0	O
,	O
the	O
state	B
zt	O
is	O
not	O
allowed	O
to	O
change	O
.	O
when	O
dt	O
=	O
0	O
,	O
we	O
make	O
a	O
stochastic	O
transition	O
to	O
a	O
new	O
state	B
.	O
more	O
precisely	O
,	O
we	O
deﬁne	O
the	O
cpds	O
as	O
follows	O
:	O
⎧⎨	O
⎩	O
pj	O
(	O
d	O
(	O
cid:2	O
)	O
⎧⎨	O
⎩	O
1	O
ajk	O
0	O
1	O
0	O
p	O
(	O
dt	O
=	O
d	O
(	O
cid:2	O
)	O
|dt−1	O
=	O
d	O
,	O
zt	O
=	O
j	O
)	O
=	O
p	O
(	O
zt	O
=	O
k|zt−1	O
=	O
j	O
,	O
dt−1	O
=	O
d	O
)	O
=	O
)	O
=	O
d	O
−	O
1	O
and	O
d	O
≥	O
1	O
if	O
d	O
=	O
0	O
if	O
d	O
(	O
cid:2	O
)	O
otherwise	O
if	O
d	O
>	O
0	O
and	O
j	O
=	O
k	O
if	O
d	O
=	O
0	O
otherwise	O
(	O
17.112	O
)	O
(	O
17.113	O
)	O
note	O
that	O
pj	O
(	O
d	O
)	O
could	O
be	O
represented	O
as	O
a	O
table	O
(	O
a	O
non-parametric	O
approach	O
)	O
or	O
as	O
some	O
kind	O
of	O
parametric	O
distribution	O
,	O
such	O
as	O
a	O
gamma	B
distribution	I
.	O
if	O
pj	O
(	O
d	O
)	O
is	O
a	O
geometric	B
distribution	I
,	O
this	O
emulates	O
a	O
standard	O
hmm	O
.	O
one	O
can	O
perform	O
inference	B
in	O
this	O
model	O
by	O
deﬁning	O
a	O
mega-variable	O
yt	O
=	O
(	O
dt	O
,	O
zt	O
)	O
.	O
however	O
,	O
this	O
is	O
rather	O
inefficient	O
,	O
since	O
dt	O
is	O
deterministic	O
.	O
it	O
is	O
possible	O
to	O
marginalize	O
dt	O
out	O
,	O
and	O
derive	O
special	O
purpose	O
inference	B
procedures	O
.	O
see	O
(	O
guedon	O
2003	O
;	O
yu	O
and	O
kobayashi	O
2006	O
)	O
for	O
details	O
.	O
unfortunately	O
,	O
all	O
these	O
methods	O
take	O
o	O
(	O
t	O
k	O
2d	O
)	O
time	O
,	O
where	O
t	O
is	O
the	O
sequence	O
length	O
,	O
k	O
is	O
the	O
number	O
of	O
states	O
,	O
and	O
d	O
is	O
the	O
maximum	O
duration	O
of	O
any	O
state	B
.	O
17.6.1.2	O
approximations	O
to	O
semi-markov	O
models	O
a	O
more	O
efficient	O
,	O
but	O
less	O
ﬂexible	O
,	O
way	O
to	O
model	O
non-geometric	O
waiting	O
times	O
is	O
to	O
replace	O
each	O
state	B
with	O
n	O
new	O
states	O
,	O
each	O
with	O
the	O
same	O
emission	O
probabilities	O
as	O
the	O
original	O
state	B
.	O
for	O
example	O
,	O
consider	O
the	O
model	O
in	O
figure	O
17.15	O
(	O
a	O
)	O
.	O
obviously	O
the	O
smallest	O
sequence	O
this	O
can	O
generate	O
is	O
of	O
length	O
n	O
=	O
4.	O
any	O
path	B
of	O
length	O
d	O
through	O
the	O
model	O
has	O
probability	O
pd−n	O
(	O
1	O
−	O
p	O
)	O
n	O
;	O
multiplying	O
by	O
the	O
number	O
of	O
possible	O
paths	O
we	O
ﬁnd	O
that	O
the	O
total	O
probability	O
of	O
a	O
path	B
of	O
length	O
d	O
is	O
p	O
(	O
d	O
)	O
=	O
pd−n	O
(	O
1	O
−	O
p	O
)	O
n	O
(	O
17.114	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
d	O
−	O
1	O
n	O
−	O
1	O
624	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
words	O
on	O
need	O
phones	O
aa	O
n	O
end	O
n	O
iy	O
d	O
end	O
the	O
dh	O
n	O
ax	O
iy	O
end	O
sub-	O
phones	O
end	O
end	O
figure	O
17.16	O
an	O
example	O
of	O
an	O
hhmm	O
for	O
an	O
asr	O
system	O
which	O
can	O
recognize	O
3	O
words	O
.	O
the	O
top	O
level	O
represents	O
bigram	O
word	O
probabilities	O
.	O
the	O
middle	O
level	O
represents	O
the	O
phonetic	O
spelling	O
of	O
each	O
word	O
.	O
the	O
bottom	O
level	O
represents	O
the	O
subphones	O
of	O
each	O
phone	B
.	O
(	O
it	O
is	O
traditional	O
to	O
represent	O
a	O
phone	B
as	O
a	O
3	O
state	B
hmm	O
,	O
representing	O
the	O
beginning	O
,	O
middle	O
and	O
end	O
.	O
)	O
based	O
on	O
figure	O
7.5	O
of	O
(	O
jurafsky	O
and	O
martin	O
2000	O
)	O
.	O
this	O
is	O
equivalent	O
to	O
the	O
negative	B
binomial	I
distribution	I
.	O
by	O
adjusting	O
n	O
and	O
the	O
self-loop	O
probabilities	O
p	O
of	O
each	O
state	B
,	O
we	O
can	O
model	O
a	O
wide	O
range	O
of	O
waiting	O
times	O
:	O
see	O
figure	O
17.15	O
(	O
b	O
)	O
.	O
let	O
e	O
be	O
the	O
number	O
of	O
expansions	O
of	O
each	O
state	B
needed	O
to	O
approximate	O
pj	O
(	O
d	O
)	O
.	O
forwards-	O
backwards	O
on	O
this	O
model	O
takes	O
o	O
(	O
t	O
(	O
ke	O
)	O
fin	O
)	O
time	O
,	O
where	O
fin	O
is	O
the	O
average	O
number	O
of	O
predecessor	O
states	O
,	O
compared	O
to	O
o	O
(	O
t	O
k	O
(	O
fin	O
+d	O
)	O
)	O
for	O
the	O
hsmm	O
.	O
for	O
typical	O
speech	B
recognition	I
applications	O
,	O
fin	O
∼	O
3	O
,	O
d	O
∼	O
50	O
,	O
k	O
∼	O
106	O
,	O
t	O
∼	O
105	O
.	O
(	O
similar	B
ﬁgures	O
apply	O
to	O
problems	O
such	O
as	O
gene	B
ﬁnding	I
,	O
which	O
also	O
often	O
uses	O
hsmms	O
.	O
)	O
since	O
fin	O
+	O
d	O
(	O
cid:19	O
)	O
efin	O
,	O
the	O
expanded	O
state	B
method	O
is	O
much	O
faster	O
than	O
an	O
hsmm	O
.	O
see	O
(	O
johnson	O
2005	O
)	O
for	O
details	O
.	O
17.6.2	O
hierarchical	O
hmms	O
a	O
hierarchical	O
hmm	O
(	O
hhmm	O
)	O
(	O
fine	O
et	O
al	O
.	O
1998	O
)	O
is	O
an	O
extension	B
of	O
the	O
hmm	O
that	O
is	O
designed	O
to	O
model	O
domains	O
with	O
hierarchical	O
structure	O
.	O
figure	O
17.16	O
gives	O
an	O
example	O
of	O
an	O
hhmm	O
used	O
in	O
automatic	B
speech	I
recognition	I
.	O
the	O
phone	B
and	O
subphone	O
models	O
can	O
be	O
“	O
called	O
”	O
from	O
different	O
higher	O
level	O
contexts	O
.	O
we	O
can	O
always	O
“	O
ﬂatten	O
”	O
an	O
hhmm	O
to	O
a	O
regular	B
hmm	O
,	O
but	O
a	O
factored	O
representation	O
is	O
often	O
easier	O
to	O
interpret	O
,	O
and	O
allows	O
for	O
more	O
efficient	O
inference	O
and	O
model	O
ﬁtting	O
.	O
hhmms	O
have	O
been	O
used	O
in	O
many	O
application	O
domains	O
,	O
e.g.	O
,	O
speech	B
recognition	I
(	O
bilmes	O
2001	O
)	O
,	O
gene	B
ﬁnding	I
(	O
hu	O
et	O
al	O
.	O
2000	O
)	O
,	O
plan	O
recognition	O
(	O
bui	O
et	O
al	O
.	O
2002	O
)	O
,	O
monitoring	O
transportation	O
patterns	O
(	O
liao	O
et	O
al	O
.	O
2007	O
)	O
,	O
indoor	O
robot	O
localization	O
(	O
theocharous	O
et	O
al	O
.	O
2004	O
)	O
,	O
etc	O
.	O
hhmms	O
are	O
less	O
expressive	O
than	O
stochastic	B
context	I
free	I
grammars	I
(	O
scfgs	O
)	O
,	O
since	O
they	O
only	O
allow	O
hierarchies	O
of	O
bounded	O
depth	O
,	O
but	O
they	O
support	B
more	O
efficient	O
inference	O
.	O
in	O
particular	O
,	O
inference	B
in	O
scfgs	O
(	O
using	O
the	O
inside	B
outside	I
algorithm	O
,	O
(	O
jurafsky	O
and	O
martin	O
2008	O
)	O
)	O
takes	O
o	O
(	O
t	O
3	O
)	O
whereas	O
inference	B
in	O
an	O
hhmm	O
takes	O
o	O
(	O
t	O
)	O
time	O
(	O
murphy	O
and	O
paskin	O
2001	O
)	O
.	O
we	O
can	O
represent	O
an	O
hhmm	O
as	O
a	O
directed	B
graphical	I
model	I
as	O
shown	O
in	O
figure	O
17.17.	O
q	O
(	O
cid:8	O
)	O
t	O
represents	O
the	O
state	B
at	O
time	O
t	O
and	O
level	O
(	O
cid:2	O
)	O
.	O
a	O
state	O
transition	O
at	O
level	O
(	O
cid:2	O
)	O
is	O
only	O
“	O
allowed	O
”	O
if	O
the	O
17.6.	O
generalizations	O
of	O
hmms	O
625	O
f	O
1	O
3	O
f	O
2	O
3	O
f	O
3	O
3	O
f	O
1	O
2	O
f	O
2	O
2	O
f	O
3	O
2	O
f	O
1	O
1	O
f	O
2	O
1	O
f	O
3	O
1	O
q1	O
1	O
q2	O
1	O
q3	O
1	O
y1	O
q1	O
2	O
q2	O
2	O
q3	O
2	O
y2	O
q1	O
3	O
q2	O
3	O
q3	O
3	O
y3	O
figure	O
17.17	O
an	O
hhmm	O
represented	O
as	O
a	O
dgm	O
.	O
q	O
(	O
cid:12	O
)	O
level	O
(	O
cid:7	O
)	O
has	O
ﬁnished	O
(	O
entered	O
its	O
exit	O
state	B
)	O
,	O
otherwise	O
f	O
(	O
cid:12	O
)	O
nodes	B
are	O
hidden	B
.	O
we	O
may	O
optionally	O
clamp	O
f	O
(	O
cid:12	O
)	O
to	O
ensure	O
all	O
models	O
have	O
ﬁnished	O
by	O
the	O
end	O
of	O
the	O
sequence	O
.	O
2001	O
)	O
.	O
t	O
is	O
the	O
state	B
at	O
time	O
t	O
,	O
level	O
(	O
cid:7	O
)	O
;	O
f	O
(	O
cid:12	O
)	O
t	O
=	O
1	O
if	O
the	O
hmm	O
at	O
t	O
=	O
0.	O
shaded	O
nodes	B
are	O
observed	O
;	O
the	O
remaining	O
t	O
=	O
1	O
,	O
where	O
t	O
is	O
the	O
length	O
of	O
the	O
observation	B
sequence	O
,	O
source	O
:	O
figure	O
2	O
of	O
(	O
murphy	O
and	O
paskin	O
chain	O
at	O
the	O
level	O
below	O
has	O
“	O
ﬁnished	O
”	O
,	O
as	O
determined	O
by	O
the	O
f	O
(	O
cid:8	O
)	O
−1	O
(	O
the	O
chain	O
below	O
ﬁnishes	O
when	O
it	O
chooses	O
to	O
enter	O
its	O
end	O
state	O
.	O
)	O
this	O
mechanism	O
ensures	O
that	O
higher	O
level	O
chains	O
evolve	O
more	O
slowly	O
than	O
lower	O
level	O
chains	O
,	O
i.e.	O
,	O
lower	O
levels	O
are	O
nested	O
within	O
higher	O
levels	O
.	O
node	O
.	O
t	O
a	O
variable	O
duration	O
hmm	O
can	O
be	O
thought	O
of	O
as	O
a	O
special	O
case	O
of	O
an	O
hhmm	O
,	O
where	O
the	O
top	O
level	O
is	O
a	O
deterministic	O
counter	O
,	O
and	O
the	O
bottom	O
level	O
is	O
a	O
regular	B
hmm	O
,	O
which	O
can	O
only	O
change	O
states	O
once	O
the	O
counter	O
has	O
“	O
timed	O
out	O
”	O
.	O
see	O
(	O
murphy	O
and	O
paskin	O
2001	O
)	O
for	O
further	O
details	O
.	O
17.6.3	O
input-output	O
hmms	O
it	O
is	O
straightforward	O
to	O
extend	O
an	O
hmm	O
to	O
handle	O
inputs	O
,	O
as	O
shown	O
in	O
figure	O
17.18	O
(	O
a	O
)	O
.	O
this	O
deﬁnes	O
a	O
conditional	O
density	O
model	O
for	O
sequences	O
of	O
the	O
form	O
p	O
(	O
y1	O
:	O
t	O
,	O
z1	O
:	O
t|u1	O
:	O
t	O
,	O
θ	O
)	O
where	O
ut	O
is	O
the	O
input	O
at	O
time	O
t	O
;	O
this	O
is	O
sometimes	O
called	O
a	O
control	B
signal	I
.	O
outputs	O
are	O
continuous	O
,	O
a	O
typical	O
parameterization	O
would	O
be	O
p	O
(	O
zt|xt	O
,	O
zt−1	O
=	O
i	O
,	O
θ	O
)	O
=	O
cat	O
(	O
zt|s	O
(	O
wiut	O
)	O
)	O
p	O
(	O
yt|xt	O
,	O
zt	O
=	O
j	O
,	O
θ	O
)	O
=n	O
(	O
yt|vjut	O
,	O
σj	O
)	O
(	O
17.115	O
)	O
if	O
the	O
inputs	O
and	O
(	O
17.116	O
)	O
(	O
17.117	O
)	O
thus	O
the	O
transition	B
matrix	I
is	O
a	O
logistic	B
regression	I
model	O
whose	O
parameters	O
depend	O
on	O
the	O
previous	O
state	B
.	O
the	O
observation	B
model	I
is	O
a	O
gaussian	O
whose	O
parameters	O
depend	O
on	O
the	O
current	O
626	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
ut−1	O
ut	O
zt−1	O
zt	O
yt−1	O
yt	O
z1	O
x1	O
z2	O
x2	O
zt	O
xt	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
17.18	O
(	O
a	O
)	O
input-output	O
hmm	O
.	O
(	O
b	O
)	O
first-order	O
auto-regressive	O
hmm	O
.	O
(	O
c	O
)	O
a	O
second-order	O
buried	O
markov	O
model	O
.	O
depending	O
on	O
the	O
value	O
of	O
the	O
hidden	B
variables	I
,	O
the	O
effective	O
graph	O
structure	O
between	O
the	O
com-	O
ponents	O
of	O
the	O
observed	O
variables	O
(	O
i.e.	O
,	O
the	O
non-zero	O
elements	O
of	O
the	O
regression	B
matrix	O
and	O
the	O
precision	B
matrix	I
)	O
can	O
change	O
,	O
although	O
this	O
is	O
not	O
shown	O
.	O
state	B
.	O
the	O
whole	O
model	O
can	O
be	O
thought	O
of	O
as	O
a	O
hidden	B
version	O
of	O
a	O
maximum	B
entropy	I
markov	O
model	O
(	O
section	O
19.6.1	O
)	O
.	O
conditional	O
on	O
the	O
inputs	O
u1	O
:	O
t	O
and	O
the	O
parameters	O
θ	O
,	O
one	O
can	O
apply	O
the	O
standard	O
forwards-	O
it	O
is	O
also	O
straightforward	O
to	O
derive	O
an	O
em	O
backwards	O
algorithm	O
to	O
estimate	O
the	O
hidden	B
states	O
.	O
algorithm	O
to	O
estimate	O
the	O
parameters	O
(	O
see	O
(	O
bengio	O
and	O
frasconi	O
1996	O
)	O
for	O
details	O
)	O
.	O
17.6.4	O
auto-regressive	O
and	O
buried	O
hmms	O
the	O
standard	O
hmm	O
assumes	O
the	O
observations	O
are	O
conditionally	B
independent	I
given	O
the	O
hidden	B
state	O
.	O
in	O
practice	O
this	O
is	O
often	O
not	O
the	O
case	O
.	O
however	O
,	O
it	O
is	O
straightforward	O
to	O
have	O
direct	O
arcs	O
from	O
xt−1	O
to	O
xt	O
as	O
well	O
as	O
from	O
zt	O
to	O
xt	O
,	O
as	O
in	O
figure	O
17.18	O
(	O
b	O
)	O
.	O
this	O
is	O
known	O
as	O
an	O
auto-regressive	O
hmm	O
,	O
or	O
aregime	O
switching	O
markov	O
model	O
.	O
for	O
continuous	O
data	O
,	O
the	O
observation	B
model	I
becomes	O
p	O
(	O
xt|xt−1	O
,	O
zt	O
=	O
j	O
,	O
θ	O
)	O
=	O
n	O
(	O
xt|wjxt−1	O
+	O
μj	O
,	O
σj	O
)	O
(	O
17.118	O
)	O
this	O
is	O
a	O
linear	B
regression	I
model	O
,	O
where	O
the	O
parameters	O
are	O
chosen	O
according	O
to	O
the	O
current	O
hidden	B
state	O
.	O
we	O
can	O
also	O
consider	O
higher-order	O
extensions	O
,	O
where	O
we	O
condition	O
on	O
the	O
last	O
l	O
observations	O
:	O
p	O
(	O
xt|xt−l	O
:	O
t−1	O
,	O
zt	O
=	O
j	O
,	O
θ	O
)	O
=	O
n	O
(	O
xt|	O
l	O
(	O
cid:4	O
)	O
wj	O
,	O
(	O
cid:8	O
)	O
xt−	O
(	O
cid:8	O
)	O
+	O
μj	O
,	O
σj	O
)	O
(	O
17.119	O
)	O
(	O
cid:8	O
)	O
=1	O
such	O
models	O
are	O
widely	O
used	O
in	O
econometrics	O
(	O
hamilton	O
1990	O
)	O
.	O
similar	B
models	O
can	O
be	O
deﬁned	O
for	O
discrete	B
observations	O
.	O
the	O
ar-hmm	O
essentially	O
combines	O
two	O
markov	O
chains	O
,	O
one	O
on	O
the	O
hidden	B
variables	I
,	O
to	O
capture	O
long	O
range	O
dependencies	O
,	O
and	O
one	O
on	O
the	O
observed	O
variables	O
,	O
to	O
capture	O
short	O
range	O
dependen-	O
cies	O
(	O
berchtold	O
1999	O
)	O
.	O
since	O
the	O
x	O
nodes	B
are	O
observed	O
,	O
the	O
connections	O
between	O
them	O
only	O
17.6.	O
generalizations	O
of	O
hmms	O
627	O
z1,1	O
z1,2	O
z1,3	O
z11	O
z12	O
z13	O
x11	O
x12	O
x13	O
z2,1	O
z2,2	O
z2,3	O
z21	O
z22	O
z23	O
z3,1	O
z3,2	O
z3,3	O
x21	O
x22	O
x23	O
z31	O
z32	O
z33	O
x1	O
x2	O
(	O
a	O
)	O
x3	O
x31	O
x32	O
x33	O
(	O
b	O
)	O
figure	O
17.19	O
(	O
a	O
)	O
a	O
factorial	O
hmm	O
with	O
3	O
chains	O
.	O
(	O
b	O
)	O
a	O
coupled	O
hmm	O
with	O
3	O
chains	O
.	O
change	O
the	O
computation	O
of	O
the	O
local	B
evidence	I
;	O
inference	B
can	O
still	O
be	O
performed	O
using	O
the	O
stan-	O
dard	O
forwards-backwards	B
algorithm	I
.	O
parameter	B
estimation	O
using	O
em	O
is	O
also	O
straightforward	O
:	O
the	O
e	O
step	O
is	O
unchanged	O
,	O
as	O
is	O
the	O
m	O
step	O
for	O
the	O
transition	B
matrix	I
.	O
if	O
we	O
assume	O
scalar	O
observations	O
for	O
notational	O
simplicty	O
,	O
the	O
m	O
step	O
involves	O
minimizing	O
(	O
cid:18	O
)	O
t−l	O
:	O
t−1w	O
(	O
st	O
)	O
)	O
2	O
+	O
log	O
σ2	O
(	O
st	O
)	O
(	O
17.120	O
)	O
(	O
cid:17	O
)	O
(	O
cid:4	O
)	O
e	O
t	O
1	O
σ2	O
(	O
st	O
)	O
(	O
yt	O
−	O
yt	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
γt	O
(	O
j	O
)	O
σ2	O
(	O
j	O
)	O
j	O
t	O
focussing	O
on	O
the	O
w	O
terms	O
,	O
we	O
see	O
that	O
this	O
requires	O
solving	O
k	O
weighted	B
least	I
squares	I
problems	O
:	O
j	O
(	O
w1	O
:	O
k	O
)	O
=	O
(	O
yt	O
−	O
yt	O
t−l	O
:	O
t−1wj	O
)	O
2	O
(	O
17.121	O
)	O
where	O
γt	O
(	O
j	O
)	O
=p	O
(	O
zt	O
=	O
k|x1	O
:	O
t	O
)	O
is	O
the	O
smoothed	O
posterior	O
marginal	O
.	O
this	O
is	O
a	O
weighted	O
linear	O
regression	B
problem	O
,	O
where	O
the	O
design	B
matrix	I
has	O
a	O
toeplitz	O
form	O
.	O
this	O
subproblem	O
can	O
be	O
solved	O
efficiently	O
using	O
the	O
levinson-durbin	O
method	O
(	O
durbin	O
and	O
koopman	O
2001	O
)	O
.	O
buried	O
markov	O
models	O
generalize	B
ar-hmms	O
by	O
allowing	O
the	O
dependency	O
structure	O
between	O
the	O
observable	O
nodes	B
to	O
change	O
based	O
on	O
the	O
hidden	B
state	O
,	O
as	O
in	O
figure	O
17.18	O
(	O
c	O
)	O
.	O
such	O
a	O
model	O
is	O
called	O
a	O
dynamic	O
bayesian	O
multi	B
net	I
,	O
since	O
it	O
is	O
a	O
mixture	O
of	O
different	O
networks	O
.	O
in	O
the	O
linear-gaussian	O
setting	O
,	O
we	O
can	O
change	O
the	O
structure	O
of	O
the	O
of	O
xt−1	O
→	O
xt	O
arcs	O
by	O
using	O
sparse	B
regression	O
matrices	O
,	O
wj	O
,	O
and	O
we	O
can	O
change	O
the	O
structure	O
of	O
the	O
connections	O
within	O
the	O
components	O
of	O
xt	O
by	O
using	O
sparse	B
gaussian	O
graphical	O
models	O
,	O
either	O
directed	B
or	O
undirected	B
.	O
see	O
(	O
bilmes	O
2000	O
)	O
for	O
details	O
.	O
17.6.5	O
factorial	O
hmm	O
an	O
hmm	O
represents	O
the	O
hidden	B
state	O
using	O
a	O
single	O
discrete	O
random	O
variable	O
zt	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
.	O
to	O
represent	O
10	O
bits	B
of	O
information	B
would	O
require	O
k	O
=	O
210	O
=	O
1024	O
states	O
.	O
by	O
contrast	O
,	O
consider	O
a	O
distributed	B
representation	I
of	O
the	O
hidden	B
state	O
,	O
where	O
each	O
zc	O
,	O
t	O
∈	O
{	O
0	O
,	O
1	O
}	O
represents	O
the	O
c	O
’	O
th	O
628	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
bit	O
of	O
the	O
t	O
’	O
th	O
hidden	B
state	O
.	O
now	O
we	O
can	O
represent	O
10	O
bits	B
using	O
just	O
10	O
binary	O
variables	O
,	O
as	O
illustrated	O
in	O
figure	O
17.19	O
(	O
a	O
)	O
.	O
this	O
model	O
is	O
called	O
a	O
factorial	O
hmm	O
(	O
ghahramani	O
and	O
jordan	O
1997	O
)	O
.	O
the	O
hope	O
is	O
that	O
this	O
kind	O
of	O
model	O
could	O
capture	O
different	O
aspects	O
of	O
a	O
signal	O
,	O
e.g.	O
,	O
one	O
chain	O
would	O
represent	O
speaking	O
style	O
,	O
another	O
the	O
words	O
that	O
are	O
being	O
spoken	O
.	O
unfortunately	O
,	O
conditioned	O
on	O
xt	O
,	O
all	O
the	O
hidden	B
variables	I
are	O
correlated	O
(	O
due	O
to	O
explaining	B
away	I
the	O
common	O
observed	O
child	O
xt	O
)	O
.	O
this	O
make	O
exact	O
state	B
estimation	I
intractable	O
.	O
however	O
,	O
we	O
can	O
derive	O
efficient	O
approximate	O
inference	B
algorithms	O
,	O
as	O
we	O
discuss	O
in	O
section	O
21.4.1	O
.	O
17.6.6	O
coupled	O
hmm	O
and	O
the	O
inﬂuence	B
model	I
if	O
we	O
have	O
multiple	O
related	O
data	O
streams	O
,	O
we	O
can	O
use	O
a	O
coupled	O
hmm	O
(	O
brand	O
1996	O
)	O
,	O
as	O
illustrated	O
in	O
figure	O
17.19	O
(	O
b	O
)	O
.	O
this	O
is	O
a	O
series	O
of	O
hmms	O
where	O
the	O
state	B
transitions	O
depend	O
on	O
the	O
states	O
of	O
neighboring	O
chains	O
.	O
that	O
is	O
,	O
we	O
represent	O
the	O
joint	O
conditional	O
distribution	O
as	O
(	O
cid:20	O
)	O
p	O
(	O
zct|zt−1	O
)	O
zct|zc	O
,	O
t−1	O
,	O
zc−1	O
,	O
t−1	O
,	O
zc+1	O
,	O
t−1	O
)	O
c	O
p	O
(	O
zt|zt−1	O
)	O
=	O
p	O
(	O
zct|zt−1	O
)	O
=p	O
(	O
(	O
17.122	O
)	O
(	O
17.123	O
)	O
this	O
has	O
been	O
used	O
for	O
various	O
tasks	O
,	O
such	O
as	O
audio-visual	B
speech	I
recognition	I
(	O
neﬁan	O
et	O
al	O
.	O
2002	O
)	O
and	O
modeling	O
freeway	O
traffic	O
ﬂows	O
(	O
kwon	O
and	O
murphy	O
2000	O
)	O
.	O
the	O
trouble	O
with	O
the	O
above	O
model	O
is	O
that	O
it	O
requires	O
o	O
(	O
ck	O
4	O
)	O
parameters	O
to	O
specify	O
,	O
if	O
there	O
are	O
c	O
chains	O
with	O
k	O
states	O
per	O
chain	O
,	O
because	O
each	O
state	B
depends	O
on	O
its	O
own	O
past	O
plus	O
the	O
past	O
of	O
its	O
two	O
neighbors	B
.	O
there	O
is	O
a	O
closely	O
related	O
model	O
,	O
known	O
as	O
the	O
inﬂuence	B
model	I
(	O
asavathiratham	O
2000	O
)	O
,	O
which	O
uses	O
fewer	O
parameters	O
.	O
it	O
models	O
the	O
joint	O
conditional	O
distribution	O
as	O
αc	O
,	O
c	O
(	O
cid:2	O
)	O
p	O
(	O
zct|zc	O
(	O
cid:2	O
)	O
,	O
t−1	O
)	O
(	O
17.124	O
)	O
c	O
(	O
cid:4	O
)	O
c	O
(	O
cid:2	O
)	O
=1	O
p	O
(	O
zct|zt−1	O
)	O
=	O
(	O
cid:7	O
)	O
c	O
(	O
cid:2	O
)	O
αc	O
,	O
c	O
(	O
cid:2	O
)	O
=	O
1	O
for	O
each	O
c.	O
that	O
is	O
,	O
we	O
use	O
a	O
convex	B
combination	I
of	O
pairwise	O
transition	O
where	O
matrices	O
.	O
the	O
αc	O
,	O
c	O
(	O
cid:2	O
)	O
parameter	B
speciﬁes	O
how	O
much	O
inﬂuence	O
chain	O
c	O
has	O
on	O
chain	O
c	O
(	O
cid:2	O
)	O
.	O
this	O
model	O
only	O
takes	O
o	O
(	O
c	O
2	O
+	O
ck	O
2	O
)	O
parameters	O
to	O
specify	O
.	O
furthermore	O
,	O
it	O
allows	O
each	O
chain	O
to	O
be	O
inﬂuenced	O
by	O
all	O
the	O
other	O
chains	O
,	O
not	O
just	O
its	O
nearest	O
neighbors	O
.	O
(	O
hence	O
the	O
corresponding	O
graphical	B
model	I
is	O
similar	B
to	O
figure	O
17.19	O
(	O
b	O
)	O
,	O
except	O
that	O
each	O
node	O
has	O
incoming	O
edges	B
from	O
all	O
the	O
previous	O
nodes	B
.	O
)	O
this	O
has	O
been	O
used	O
for	O
various	O
tasks	O
,	O
such	O
as	O
modeling	O
conversational	O
interactions	O
between	O
people	O
(	O
basu	O
et	O
al	O
.	O
2001	O
)	O
.	O
unfortunately	O
,	O
inference	B
in	O
both	O
of	O
these	O
models	O
takes	O
o	O
(	O
t	O
(	O
k	O
c	O
)	O
2	O
)	O
time	O
,	O
since	O
all	O
the	O
chains	O
become	O
fully	O
correlated	O
even	O
if	O
the	O
interaction	O
graph	O
is	O
sparse	B
.	O
various	O
approximate	B
inference	I
methods	O
can	O
be	O
applied	O
,	O
as	O
we	O
discuss	O
later	O
.	O
17.6.7	O
dynamic	O
bayesian	O
networks	O
(	O
dbns	O
)	O
a	O
dynamic	O
bayesian	O
network	O
is	O
just	O
a	O
way	O
to	O
represent	O
a	O
stochastic	B
process	I
using	O
a	O
directed	O
graphical	O
model.7	O
note	O
that	O
the	O
network	O
is	O
not	O
dynamic	O
(	O
the	O
structure	O
and	O
parameters	O
are	O
ﬁxed	O
)	O
,	O
7.	O
the	O
acronym	O
dbn	O
can	O
stand	O
for	O
either	O
“	O
dynamic	O
bayesian	O
network	O
”	O
or	O
“	O
deep	B
belief	I
network	I
”	O
(	O
section	O
28.1	O
)	O
depending	O
on	O
the	O
context	O
.	O
geoff	O
hinton	O
(	O
who	O
invented	O
the	O
term	O
“	O
deep	B
belief	I
network	I
”	O
)	O
has	O
suggested	O
the	O
acronyms	O
dybn	O
and	O
deebn	O
to	O
avoid	O
this	O
ambiguity	O
.	O
17.6.	O
generalizations	O
of	O
hmms	O
629	O
27	O
25	O
21	O
23	O
20	O
16	O
17	O
19	O
7	O
leftclr0	O
rightclr0	O
leftclr1	O
rightclr1	O
lataction0	O
lataction1	O
xdot0	O
inlane0	O
xdot1	O
inlane1	O
sensorvalid1	O
6	O
12	O
fydotdiff1	O
leftclrsens1	O
28	O
rightclrsens1	O
26	O
turnsignal1	O
22	O
xdotsens1	O
24	O
ydotsens1	O
18	O
fwdaction0	O
fwdaction1	O
fydotdiffsens1	O
15	O
ydot0	O
stopped0	O
ydot1	O
stopped1	O
engstatus0	O
engstatus1	O
13	O
fcloseslow1	O
10	O
fclr1	O
fclrsens1	O
11	O
bxdotsens1	O
9	O
8	O
bxdot1	O
bclosefast1	O
4	O
1	O
bclr1	O
3	O
bclrsens1	O
5	O
2	O
14	O
frontbackstatus0	O
frontbackstatus1	O
bydotdiff1	O
bydotdiffsens1	O
slice	O
t	O
slice	O
t+1	O
evidence	B
figure	O
17.20	O
the	O
batnet	O
dbn	O
.	O
the	O
transient	B
nodes	O
are	O
only	O
shown	O
for	O
the	O
second	O
slice	O
,	O
to	O
minimize	O
clutter	O
.	O
the	O
dotted	O
lines	O
can	O
be	O
ignored	O
.	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
.	O
rather	O
it	O
is	O
a	O
network	O
representation	O
of	O
a	O
dynamical	O
system	O
.	O
all	O
of	O
the	O
hmm	O
variants	O
we	O
have	O
seen	O
above	O
could	O
be	O
considered	O
to	O
be	O
dbns	O
.	O
however	O
,	O
we	O
prefer	O
to	O
reserve	O
the	O
term	O
“	O
dbn	O
”	O
for	O
graph	B
structures	O
that	O
are	O
more	O
“	O
irregular	O
”	O
and	O
problem-speciﬁc	O
.	O
an	O
example	O
is	O
shown	O
in	O
figure	O
17.20	O
,	O
which	O
is	O
a	O
dbn	O
designed	O
to	O
monitor	O
the	O
state	B
of	O
a	O
simulated	O
autonomous	O
car	O
known	O
as	O
the	O
“	O
bayesian	O
automated	O
taxi	O
”	O
,	O
or	O
“	O
batmobile	O
”	O
(	O
forbes	O
et	O
al	O
.	O
1995	O
)	O
.	O
deﬁning	O
dbns	O
is	O
straightforward	O
:	O
you	O
just	O
need	O
to	O
specify	O
the	O
structure	O
of	O
the	O
ﬁrst	O
time-slice	O
,	O
the	O
structure	O
between	O
two	O
time-slices	O
,	O
and	O
the	O
form	O
of	O
the	O
cpds	O
.	O
learning	B
is	O
also	O
easy	O
.	O
the	O
main	O
problem	O
is	O
that	O
exact	O
inference	B
can	O
be	O
computationally	O
expensive	O
,	O
because	O
all	O
the	O
hidden	B
variables	I
become	O
correlated	O
over	O
time	O
(	O
this	O
is	O
known	O
as	O
entanglement	B
—	O
see	O
e.g.	O
,	O
(	O
koller	O
and	O
15.2.4	O
)	O
for	O
details	O
)	O
.	O
thus	O
a	O
sparse	B
graph	O
does	O
not	O
necessarily	O
result	O
in	O
friedman	O
2009	O
,	O
sec	O
.	O
tractable	O
exact	O
inference	B
.	O
however	O
,	O
later	O
we	O
will	O
see	O
algorithms	O
that	O
can	O
exploit	O
the	O
graph	B
structure	O
for	O
efficient	O
approximate	O
inference	B
.	O
exercises	O
exercise	O
17.1	O
derivation	O
of	O
q	O
function	O
for	O
hmm	O
derive	O
equation	O
17.97.	O
exercise	O
17.2	O
two	O
ﬁlter	O
approach	O
to	O
smoothing	O
in	O
hmms	O
assuming	O
that	O
πt	O
(	O
i	O
)	O
=p	O
(	O
st	O
=	O
i	O
)	O
>	O
0	O
for	O
all	O
i	O
and	O
t	O
,	O
derive	O
a	O
recursive	B
algorithm	O
for	O
updating	O
rt	O
(	O
i	O
)	O
=	O
p	O
(	O
st	O
=	O
i|xt+1	O
:	O
t	O
)	O
.	O
hint	O
:	O
it	O
should	O
be	O
very	O
similar	B
to	O
the	O
standard	O
forwards	O
algorithm	O
,	O
but	O
using	O
a	O
time-	O
reversed	O
transition	B
matrix	I
.	O
then	O
show	O
how	O
to	O
compute	O
the	O
posterior	O
marginals	O
γt	O
(	O
i	O
)	O
=p	O
(	O
st	O
=	O
i|x1	O
:	O
t	O
)	O
630	O
chapter	O
17.	O
markov	O
and	O
hidden	B
markov	O
models	O
from	O
the	O
backwards	O
ﬁltered	O
messages	O
rt	O
(	O
i	O
)	O
,	O
the	O
forwards	O
ﬁltered	O
messages	O
αt	O
(	O
i	O
)	O
,	O
and	O
the	O
stationary	B
distribution	I
πt	O
(	O
i	O
)	O
.	O
exercise	O
17.3	O
em	O
for	O
for	O
hmms	O
with	O
mixture	O
of	O
gaussian	O
observations	O
consider	O
an	O
hmm	O
where	O
the	O
observation	B
model	I
has	O
the	O
form	O
wjkn	O
(	O
xt|μjk	O
,	O
σjk	O
)	O
(	O
17.125	O
)	O
(	O
cid:12	O
)	O
k	O
p	O
(	O
xt|zt	O
=	O
j	O
,	O
θ	O
)	O
=	O
•	O
draw	O
the	O
dgm	O
.	O
•	O
derive	O
the	O
e	O
step	O
.	O
•	O
derive	O
the	O
m	O
step	O
.	O
exercise	O
17.4	O
em	O
for	O
for	O
hmms	O
with	O
tied	B
mixtures	O
in	O
many	O
applications	O
,	O
it	O
is	O
common	O
that	O
the	O
observations	O
are	O
high-dimensional	O
vectors	O
(	O
e.g.	O
,	O
in	O
speech	B
recognition	I
,	O
xt	O
is	O
often	O
a	O
vector	O
of	O
cepstral	O
coefficients	O
and	O
their	O
derivatives	O
,	O
so	O
xt	O
∈	O
r	O
39	O
)	O
,	O
so	O
estimating	O
a	O
full	B
covariance	O
matrix	O
for	O
km	O
values	O
(	O
where	O
m	O
is	O
the	O
number	O
of	O
mixture	B
components	O
per	O
hidden	B
state	O
)	O
,	O
as	O
in	O
exercise	O
17.3	O
,	O
requires	O
a	O
lot	O
of	O
data	O
.	O
an	O
alternative	O
is	O
to	O
use	O
just	O
m	O
gaussians	O
,	O
rather	O
than	O
m	O
k	O
gaussians	O
,	O
and	O
to	O
let	O
the	O
state	B
inﬂuence	O
the	O
mixing	B
weights	I
but	O
not	O
the	O
means	O
and	O
covariances	O
.	O
this	O
is	O
called	O
a	O
semi-continuous	O
hmm	O
or	O
tied-mixture	O
hmm	O
.	O
•	O
draw	O
the	O
corresponding	O
graphical	B
model	I
.	O
•	O
derive	O
the	O
e	O
step	O
.	O
•	O
derive	O
the	O
m	O
step	O
.	O
18	O
state	B
space	I
models	O
18.1	O
introduction	O
a	O
state	B
space	I
model	I
or	O
ssm	O
is	O
just	O
like	O
an	O
hmm	O
,	O
except	O
the	O
hidden	B
states	O
are	O
continuous	O
.	O
the	O
model	O
can	O
be	O
written	O
in	O
the	O
following	O
generic	O
form	O
:	O
zt	O
=	O
g	O
(	O
ut	O
,	O
zt−1	O
,	O
t	O
)	O
yt	O
=	O
h	O
(	O
zt	O
,	O
ut	O
,	O
δt	O
)	O
(	O
18.1	O
)	O
(	O
18.2	O
)	O
where	O
zt	O
is	O
the	O
hidden	B
state	O
,	O
ut	O
is	O
an	O
optional	O
input	O
or	O
control	B
signal	I
,	O
yt	O
is	O
the	O
observation	B
,	O
g	O
is	O
the	O
transition	B
model	I
,	O
h	O
is	O
the	O
observation	B
model	I
,	O
t	O
is	O
the	O
system	O
noise	O
at	O
time	O
t	O
,	O
and	O
δt	O
is	O
the	O
observation	B
noise	O
at	O
time	O
t.	O
we	O
assume	O
that	O
all	O
parameters	O
of	O
the	O
model	O
,	O
θ	O
,	O
are	O
known	O
;	O
if	O
not	O
,	O
they	O
can	O
be	O
included	O
into	O
the	O
hidden	B
state	O
,	O
as	O
we	O
discuss	O
below	O
.	O
one	O
of	O
the	O
primary	O
goals	O
in	O
using	O
ssms	O
is	O
to	O
recursively	O
estimate	O
the	O
belief	B
state	I
,	O
p	O
(	O
zt|y1	O
:	O
t	O
,	O
u1	O
:	O
t	O
,	O
θ	O
)	O
.	O
(	O
note	O
:	O
we	O
will	O
often	O
drop	O
the	O
conditioning	B
on	O
u	O
and	O
θ	O
for	O
brevity	O
.	O
)	O
we	O
will	O
discuss	O
algorithms	O
for	O
this	O
later	O
in	O
this	O
chapter	O
.	O
we	O
will	O
also	O
discuss	O
how	O
to	O
convert	O
our	O
beliefs	O
about	O
the	O
hidden	B
state	O
into	O
predictions	O
about	O
future	O
observables	O
by	O
computing	O
the	O
posterior	O
predictive	O
p	O
(	O
yt+1|y1	O
:	O
t	O
)	O
.	O
in	O
other	O
an	O
important	O
special	O
case	O
of	O
an	O
ssm	O
is	O
where	O
all	O
the	O
cpds	O
are	O
linear-gaussian	O
.	O
words	O
,	O
we	O
assume	O
•	O
the	O
transition	B
model	I
is	O
a	O
linear	O
function	O
zt	O
=	O
atzt−1	O
+	O
btut	O
+	O
t	O
•	O
the	O
observation	B
model	I
is	O
a	O
linear	O
function	O
yt	O
=	O
ctzt	O
+	O
dtut	O
+	O
δt	O
•	O
the	O
system	O
noise	O
is	O
gaussian	O
t	O
∼	O
n	O
(	O
0	O
,	O
qt	O
)	O
•	O
the	O
observation	B
noise	O
is	O
gaussian	O
δt	O
∼	O
n	O
(	O
0	O
,	O
rt	O
)	O
(	O
18.3	O
)	O
(	O
18.4	O
)	O
(	O
18.5	O
)	O
(	O
18.6	O
)	O
this	O
model	O
is	O
called	O
a	O
linear-gaussian	O
ssm	O
(	O
lg-ssm	O
)	O
or	O
alinear	O
dynamical	O
system	O
(	O
lds	O
)	O
.	O
if	O
the	O
parameters	O
θt	O
=	O
(	O
at	O
,	O
bt	O
,	O
ct	O
,	O
dt	O
,	O
qt	O
,	O
rt	O
)	O
are	O
independent	O
of	O
time	O
,	O
the	O
model	O
is	O
called	O
stationary	B
.	O
632	O
14	O
12	O
10	O
8	O
6	O
4	O
observed	O
truth	O
10	O
12	O
14	O
16	O
18	O
20	O
22	O
(	O
a	O
)	O
16	O
14	O
12	O
10	O
8	O
6	O
4	O
8	O
chapter	O
18.	O
state	B
space	I
models	O
observed	O
smoothed	O
observed	O
filtered	O
14	O
12	O
10	O
8	O
6	O
4	O
10	O
12	O
14	O
16	O
18	O
20	O
22	O
24	O
10	O
15	O
20	O
25	O
(	O
b	O
)	O
(	O
c	O
)	O
illustration	O
of	O
kalman	O
ﬁltering	B
and	O
smoothing	O
.	O
figure	O
18.1	O
(	O
a	O
)	O
observations	O
(	O
green	O
cirles	O
)	O
are	O
generated	O
(	O
b	O
)	O
filtered	O
estimated	O
is	O
shown	O
by	O
an	O
object	O
moving	O
to	O
the	O
right	O
(	O
true	O
location	O
denoted	O
by	O
black	O
squares	O
)	O
.	O
by	O
dotted	O
red	O
line	O
.	O
red	O
cross	O
is	O
the	O
posterior	B
mean	I
,	O
blue	O
circles	O
are	O
95	O
%	O
conﬁdence	O
ellipses	O
derived	O
from	O
the	O
posterior	O
covariance	O
.	O
for	O
clarity	O
,	O
we	O
only	O
plot	O
the	O
ellipses	O
every	O
other	O
time	O
step	O
.	O
(	O
c	O
)	O
same	O
as	O
(	O
b	O
)	O
,	O
but	O
using	O
offline	B
kalman	O
smoothing	O
.	O
figure	O
generated	O
by	O
kalmantrackingdemo	O
.	O
the	O
lg-ssm	O
is	O
important	O
because	O
it	O
supports	O
exact	O
inference	B
,	O
as	O
we	O
will	O
see	O
.	O
in	O
particular	O
,	O
if	O
the	O
initial	O
belief	B
state	I
is	O
gaussian	O
,	O
p	O
(	O
z1	O
)	O
=n	O
(	O
μ1|0	O
,	O
σ1|0	O
)	O
,	O
then	O
all	O
subsequent	O
belief	O
states	O
will	O
also	O
be	O
gaussian	O
;	O
we	O
will	O
denote	O
them	O
by	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
=	O
n	O
(	O
μt|t	O
,	O
σt|t	O
)	O
.	O
(	O
the	O
notation	O
μt|τ	O
denotes	O
e	O
[	O
zt|y1	O
:	O
τ	O
]	O
,	O
and	O
similarly	O
for	O
σt|t	O
;	O
thus	O
μt|0	O
denotes	O
the	O
prior	O
for	O
z1	O
before	O
we	O
have	O
seen	O
any	O
data	O
.	O
for	O
brevity	O
we	O
will	O
denote	O
the	O
posterior	O
belief	O
states	O
using	O
μt|t	O
=	O
μt	O
and	O
σt|t	O
=	O
σt	O
.	O
)	O
we	O
can	O
compute	O
these	O
quantities	O
efficiently	O
using	O
the	O
celebrated	O
kalman	O
ﬁlter	O
,	O
as	O
we	O
show	O
in	O
section	O
18.3.1.	O
but	O
before	O
discussing	O
algorithms	O
,	O
we	O
discuss	O
some	O
important	O
applications	O
.	O
18.2	O
applications	O
of	O
ssms	O
ssms	O
have	O
many	O
applications	O
,	O
some	O
of	O
which	O
we	O
discuss	O
in	O
the	O
sections	O
below	O
.	O
we	O
mostly	O
focus	O
on	O
lg-ssms	O
,	O
for	O
simplicity	O
,	O
although	O
non-linear	O
and/or	O
non-gaussian	O
ssms	O
are	O
even	O
more	O
widely	O
used	O
.	O
18.2.1	O
ssms	O
for	O
object	O
tracking	O
one	O
of	O
the	O
earliest	O
applications	O
of	O
kalman	O
ﬁltering	B
was	O
for	O
tracking	B
objects	O
,	O
such	O
as	O
airplanes	O
and	O
missiles	O
,	O
from	O
noisy	O
measurements	O
,	O
such	O
as	O
radar	B
.	O
here	O
we	O
give	O
a	O
simpliﬁed	O
example	O
to	O
illustrate	O
the	O
key	O
ideas	O
.	O
consider	O
an	O
object	O
moving	O
in	O
a	O
2d	O
plane	O
.	O
let	O
z1t	O
and	O
z2t	O
be	O
the	O
horizontal	O
and	O
vertical	O
locations	O
of	O
the	O
object	O
,	O
and	O
˙z1t	O
and	O
˙z2t	O
be	O
the	O
corresponding	O
velocity	O
.	O
we	O
can	O
represent	O
this	O
as	O
a	O
state	B
vector	O
zt	O
∈	O
r	O
4	O
as	O
follows	O
:	O
z1t	O
z2t	O
˙z1t	O
˙z2t	O
.	O
(	O
18.7	O
)	O
(	O
cid:22	O
)	O
zt	O
t	O
=	O
(	O
cid:23	O
)	O
18.2.	O
applications	O
of	O
ssms	O
633	O
let	O
us	O
assume	O
that	O
the	O
object	O
is	O
moving	O
at	O
constant	O
velocity	O
,	O
but	O
is	O
“	O
perturbed	O
”	O
by	O
random	O
gaussian	O
noise	O
(	O
e.g.	O
,	O
due	O
to	O
the	O
wind	O
)	O
.	O
thus	O
we	O
can	O
model	O
the	O
system	O
dynamics	O
as	O
follows	O
:	O
zt	O
=	O
atzt−1	O
+	O
t	O
⎞	O
⎟⎟⎠	O
=	O
⎛	O
⎜⎜⎝z1t	O
z2t	O
˙z1t	O
˙z2t	O
⎛	O
⎜⎜⎝1	O
0	O
0	O
0	O
⎞	O
⎟⎟⎠	O
⎞	O
⎟⎟⎠	O
+	O
⎛	O
⎜⎜⎝z1	O
,	O
t−1	O
z2	O
,	O
t−1	O
˙z1	O
,	O
t−1	O
˙z2	O
,	O
t−1	O
⎞	O
⎟⎟⎠	O
⎛	O
⎜⎜⎝1t	O
2t	O
3t	O
4t	O
0	O
δ	O
0	O
0	O
δ	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
(	O
18.8	O
)	O
(	O
18.9	O
)	O
(	O
18.10	O
)	O
where	O
t	O
∼	O
n	O
(	O
0	O
,	O
q	O
)	O
is	O
the	O
system	O
noise	O
,	O
and	O
δ	O
is	O
the	O
sampling	B
period	I
.	O
this	O
says	O
that	O
the	O
new	O
location	O
zj	O
,	O
t	O
is	O
the	O
old	O
location	O
zj	O
,	O
t−1	O
plus	O
δ	O
times	O
the	O
old	O
velocity	O
˙zj	O
,	O
t−1	O
,	O
plus	O
random	O
noise	O
,	O
jt	O
,	O
for	O
j	O
=	O
1	O
:	O
2.	O
also	O
,	O
the	O
new	O
velocity	O
˙zj	O
,	O
t	O
is	O
the	O
old	O
velocity	O
˙zj	O
,	O
t−1	O
plus	O
random	O
noise	O
,	O
jt	O
,	O
for	O
j	O
=	O
3	O
:	O
4.	O
this	O
is	O
called	O
a	O
random	B
accelerations	I
model	I
,	O
since	O
the	O
object	O
moves	O
according	O
to	O
newton	O
’	O
s	O
laws	O
,	O
but	O
is	O
subject	O
to	O
random	O
changes	O
in	O
velocity	O
.	O
now	O
suppose	O
that	O
we	O
can	O
observe	O
the	O
location	O
of	O
the	O
object	O
but	O
not	O
its	O
velocity	O
.	O
let	O
yt	O
∈	O
r	O
represent	O
our	O
observation	B
,	O
which	O
we	O
assume	O
is	O
subject	O
to	O
gaussian	O
noise	O
.	O
we	O
can	O
model	O
this	O
as	O
follows	O
:	O
2	O
yt	O
=	O
ctzt	O
+	O
δt	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
⎛	O
⎜⎜⎝z1t	O
⎞	O
⎟⎟⎠	O
+	O
⎛	O
⎜⎜⎝δ1t	O
⎞	O
⎟⎟⎠	O
=	O
0	O
0	O
0	O
0	O
y1t	O
y2t	O
z2t	O
˙z1t	O
˙z2t	O
1	O
0	O
0	O
1	O
δ2t	O
δ3t	O
δ4t	O
where	O
δt	O
∼	O
n	O
(	O
0	O
,	O
r	O
)	O
is	O
the	O
measurement	O
noise	O
.	O
finally	O
,	O
we	O
need	O
to	O
specify	O
our	O
initial	O
(	O
prior	O
)	O
beliefs	O
about	O
the	O
state	B
of	O
the	O
object	O
,	O
p	O
(	O
z1	O
)	O
.	O
we	O
will	O
assume	O
this	O
is	O
a	O
gaussian	O
,	O
p	O
(	O
z1	O
)	O
=	O
n	O
(	O
z1|μ1|0	O
,	O
σ1|0	O
)	O
.	O
we	O
can	O
represent	O
prior	O
ignorance	O
by	O
making	O
σ1|0	O
suitably	O
“	O
broad	O
”	O
,	O
e.g.	O
,	O
σ1|0	O
=	O
∞i	O
.	O
we	O
have	O
now	O
fully	O
speciﬁed	O
the	O
model	O
and	O
can	O
perform	O
sequential	B
bayesian	O
updating	O
to	O
compute	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
using	O
an	O
algorithm	O
known	O
as	O
the	O
kalman	O
ﬁlter	O
,	O
to	O
be	O
described	O
in	O
section	O
18.3.1	O
.	O
(	O
18.11	O
)	O
figure	O
18.1	O
(	O
a	O
)	O
gives	O
an	O
example	O
.	O
the	O
object	O
moves	O
to	O
the	O
right	O
and	O
generates	O
an	O
observation	B
at	O
each	O
time	O
step	O
(	O
think	O
of	O
“	O
blips	O
”	O
on	O
a	O
radar	B
screen	O
)	O
.	O
we	O
observe	O
these	O
blips	O
and	O
ﬁlter	O
out	O
the	O
noise	O
by	O
using	O
the	O
kalman	O
ﬁlter	O
.	O
at	O
every	O
step	O
,	O
we	O
have	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
,	O
from	O
which	O
we	O
can	O
compute	O
p	O
(	O
z1t	O
,	O
z2t|y1	O
:	O
t	O
)	O
by	O
marginalizing	B
out	I
the	O
dimensions	O
corresponding	O
to	O
the	O
velocities	O
.	O
(	O
this	O
is	O
easy	O
to	O
do	O
since	O
the	O
posterior	O
is	O
gaussian	O
.	O
)	O
our	O
“	O
best	O
guess	O
”	O
about	O
the	O
location	O
of	O
the	O
object	O
is	O
the	O
posterior	B
mean	I
,	O
e	O
[	O
zt|y1	O
:	O
t	O
]	O
,	O
denoted	O
as	O
a	O
red	O
cross	O
in	O
figure	O
18.1	O
(	O
b	O
)	O
.	O
our	O
uncertainty	B
associated	O
with	O
this	O
is	O
represented	O
as	O
an	O
ellipse	O
,	O
which	O
contains	O
95	O
%	O
of	O
the	O
probability	O
mass	O
.	O
we	O
see	O
that	O
our	O
uncertainty	B
goes	O
down	O
over	O
time	O
,	O
as	O
the	O
effects	O
of	O
the	O
initial	O
uncertainty	B
get	O
“	O
washed	O
out	O
”	O
.	O
we	O
also	O
see	O
that	O
the	O
estimated	O
trajectory	O
has	O
“	O
ﬁltered	O
out	O
”	O
some	O
of	O
the	O
noise	O
.	O
to	O
obtain	O
the	O
much	O
smoother	O
plot	O
in	O
figure	O
18.1	O
(	O
c	O
)	O
,	O
we	O
need	O
to	O
use	O
the	O
kalman	O
smoother	O
,	O
which	O
computes	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
;	O
this	O
depends	O
on	O
“	O
future	O
”	O
as	O
well	O
as	O
“	O
past	O
”	O
data	O
,	O
as	O
discussed	O
in	O
section	O
18.3.2	O
.	O
18.2.2	O
robotic	O
slam	O
consider	O
a	O
robot	O
moving	O
around	O
an	O
unknown	B
2d	O
world	O
.	O
it	O
needs	O
to	O
learn	O
a	O
map	O
and	O
keep	O
track	O
of	O
its	O
location	O
within	O
that	O
map	O
.	O
this	O
problem	O
is	O
known	O
as	O
simultaneous	O
localization	O
and	O
634	O
chapter	O
18.	O
state	B
space	I
models	O
l1	O
x1	O
l2	O
y1	O
x2	O
x3	O
y3	O
.	O
.	O
.	O
xt	O
y1	O
y2	O
yt	O
figure	O
18.2	O
illustration	O
of	O
graphical	B
model	I
underlying	O
slam	O
.	O
li	O
is	O
the	O
ﬁxed	O
location	O
of	O
landmark	O
i	O
,	O
xt	O
is	O
the	O
location	O
of	O
the	O
robot	O
,	O
and	O
yt	O
is	O
the	O
observation	B
.	O
in	O
this	O
trace	B
,	O
the	O
robot	O
sees	O
landmarks	O
1	O
and	O
2	O
at	O
time	O
step	O
1	O
,	O
then	O
just	O
landmark	O
2	O
,	O
then	O
just	O
landmark	O
1	O
,	O
etc	O
.	O
based	O
on	O
figure	O
15.a.3	O
of	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
robot	O
pose	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
18.3	O
illustration	O
of	O
the	O
slam	O
problem	O
.	O
(	O
a	O
)	O
a	O
robot	O
starts	O
at	O
the	O
top	O
left	O
and	O
moves	O
clockwise	O
in	O
a	O
circle	O
back	O
to	O
where	O
it	O
started	O
.	O
we	O
see	O
how	O
the	O
posterior	O
uncertainty	O
about	O
the	O
robot	O
’	O
s	O
location	O
increases	O
and	O
then	O
decreases	O
as	O
it	O
returns	O
to	O
a	O
familar	O
location	O
,	O
closing	B
the	I
loop	I
.	O
if	O
we	O
performed	O
smoothing	O
,	O
this	O
new	O
information	B
would	O
propagate	O
backwards	O
in	O
time	O
to	O
disambiguate	O
the	O
entire	O
trajectory	O
.	O
(	O
b	O
)	O
we	O
show	O
the	O
precision	B
matrix	I
,	O
representing	O
sparse	B
correlations	O
between	O
the	O
landmarks	O
,	O
and	O
between	O
the	O
landmarks	O
and	O
the	O
robot	O
’	O
s	O
position	O
(	O
pose	O
)	O
.	O
this	O
sparse	B
precision	O
matrix	O
can	O
be	O
visualized	O
as	O
a	O
gaussian	O
graphical	B
model	I
,	O
as	O
shown	O
.	O
source	O
:	O
figure	O
15.a.3	O
of	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
.	O
18.2.	O
applications	O
of	O
ssms	O
635	O
mapping	O
,	O
or	O
slam	O
for	O
short	O
,	O
and	O
is	O
widely	O
used	O
in	O
mobile	O
robotics	O
,	O
as	O
well	O
as	O
other	O
applications	O
such	O
as	O
indoor	O
navigation	O
using	O
cellphones	O
(	O
since	O
gps	O
does	O
not	O
work	O
inside	O
buildings	O
)	O
.	O
let	O
us	O
assume	O
we	O
can	O
represent	O
the	O
map	O
as	O
the	O
2d	O
locations	O
of	O
a	O
ﬁxed	O
set	O
of	O
k	O
landmarks	O
,	O
denote	O
them	O
by	O
l1	O
,	O
.	O
.	O
.	O
,	O
lk	O
(	O
each	O
is	O
a	O
vector	O
in	O
r	O
2	O
)	O
.	O
for	O
simplicity	O
,	O
we	O
will	O
assume	O
these	O
are	O
uniquely	O
identiﬁable	B
.	O
let	O
xt	O
represent	O
the	O
unknown	B
location	O
of	O
the	O
robot	O
at	O
time	O
t.	O
we	O
deﬁne	O
the	O
state	B
space	I
to	O
be	O
zt	O
=	O
(	O
xt	O
,	O
l1	O
:	O
k	O
)	O
;	O
we	O
assume	O
the	O
landmarks	O
are	O
static	O
,	O
so	O
their	O
motion	O
if	O
yt	O
measures	O
the	O
distance	O
from	O
xt	O
to	O
model	O
is	O
a	O
constant	O
,	O
and	O
they	O
have	O
no	O
system	O
noise	O
.	O
the	O
set	O
of	O
closest	O
landmarks	O
,	O
then	O
the	O
robot	O
can	O
update	O
its	O
estimate	O
of	O
the	O
landmark	O
locations	O
based	O
on	O
what	O
it	O
sees	O
.	O
figure	O
18.2	O
shows	O
the	O
corresponding	O
graphical	B
model	I
for	O
the	O
case	O
where	O
k	O
=	O
2	O
,	O
and	O
where	O
on	O
the	O
ﬁrst	O
step	O
it	O
sees	O
landmarks	O
1	O
and	O
2	O
,	O
then	O
just	O
landmark	O
2	O
,	O
then	O
just	O
landmark	O
1	O
,	O
etc	O
.	O
if	O
we	O
assume	O
the	O
observation	B
model	I
p	O
(	O
yt|zt	O
,	O
l	O
)	O
is	O
linear-gaussian	O
,	O
and	O
we	O
use	O
a	O
gaussian	O
motion	O
model	O
for	O
p	O
(	O
xt|xt−1	O
,	O
ut	O
)	O
,	O
we	O
can	O
use	O
a	O
kalman	O
ﬁlter	O
to	O
maintain	O
our	O
belief	B
state	I
about	O
the	O
location	O
of	O
the	O
robot	O
and	O
the	O
location	O
of	O
the	O
landmarks	O
(	O
smith	O
and	O
cheeseman	O
1986	O
;	O
choset	O
and	O
nagatani	O
2001	O
)	O
.	O
over	O
time	O
,	O
the	O
uncertainty	B
in	O
the	O
robot	O
’	O
s	O
location	O
will	O
increase	O
,	O
due	O
to	O
wheel	O
slippage	B
etc.	O
,	O
but	O
when	O
the	O
robot	O
returns	O
to	O
a	O
familiar	O
location	O
,	O
its	O
uncertainty	B
will	O
decrease	O
again	O
.	O
this	O
is	O
called	O
closing	B
the	I
loop	I
,	O
and	O
is	O
illustrated	O
in	O
figure	O
18.3	O
(	O
a	O
)	O
,	O
where	O
we	O
see	O
the	O
uncertainty	B
ellipses	O
,	O
representing	O
cov	O
[	O
xt|y1	O
:	O
t	O
,	O
u1	O
:	O
t	O
]	O
,	O
grow	O
and	O
then	O
shrink	O
.	O
(	O
note	O
that	O
in	O
this	O
section	O
,	O
we	O
assume	O
that	O
a	O
human	O
is	O
joysticking	O
the	O
robot	O
through	O
the	O
environment	O
,	O
so	O
u1	O
:	O
t	O
is	O
given	O
as	O
input	O
,	O
i.e.	O
,	O
we	O
do	O
not	O
address	O
the	O
decision-theoretic	O
issue	O
of	O
choosing	O
where	O
to	O
explore	O
.	O
)	O
since	O
the	O
belief	B
state	I
is	O
gaussian	O
,	O
we	O
can	O
visualize	O
the	O
posterior	O
covariance	O
matrix	O
σt	O
.	O
ac-	O
tually	O
,	O
it	O
is	O
more	O
interesting	O
to	O
visualize	O
the	O
posterior	O
precision	O
matrix	O
,	O
λt	O
=	O
σ−1	O
,	O
since	O
that	O
is	O
fairly	O
sparse	B
,	O
as	O
shown	O
in	O
figure	O
18.3	O
(	O
b	O
)	O
.	O
the	O
reason	O
for	O
this	O
is	O
that	O
zeros	O
in	O
the	O
precision	B
matrix	I
correspond	O
to	O
absent	O
edges	B
in	O
the	O
corresponding	O
undirected	B
gaussian	O
graphical	B
model	I
(	O
see	O
section	O
19.4.4	O
)	O
.	O
initially	O
all	O
the	O
landmarks	O
are	O
uncorrelated	O
(	O
assuming	O
we	O
have	O
a	O
diagonal	B
prior	O
on	O
l	O
)	O
,	O
so	O
the	O
ggm	O
is	O
a	O
disconnected	O
graph	B
,	O
and	O
λt	O
is	O
diagonal	B
.	O
however	O
,	O
as	O
the	O
robot	O
moves	O
about	O
,	O
it	O
will	O
induce	O
correlation	O
between	O
nearby	O
landmarks	O
.	O
intuitively	O
this	O
is	O
because	O
the	O
robot	O
is	O
estimating	O
its	O
position	O
based	O
on	O
distance	O
to	O
the	O
landmarks	O
,	O
but	O
the	O
landmarks	O
’	O
locations	O
are	O
being	O
estimated	O
based	O
on	O
the	O
robot	O
’	O
s	O
position	O
,	O
so	O
they	O
all	O
become	O
inter-dependent	O
.	O
this	O
can	O
be	O
seen	O
more	O
clearly	O
from	O
the	O
graphical	B
model	I
in	O
figure	O
18.2	O
:	O
it	O
is	O
clear	O
that	O
l1	O
and	O
l2	O
are	O
not	O
d-separated	O
by	O
y1	O
:	O
t	O
,	O
because	O
there	O
is	O
a	O
path	B
between	O
them	O
via	O
the	O
unknown	B
sequence	O
of	O
x1	O
:	O
t	O
nodes	O
.	O
as	O
a	O
consequence	O
of	O
the	O
precision	B
matrix	I
becoming	O
denser	O
,	O
exact	O
inference	B
takes	O
o	O
(	O
k	O
3	O
)	O
time	O
.	O
(	O
this	O
is	O
an	O
example	O
of	O
the	O
entanglement	B
problem	I
for	O
inference	B
in	O
dbns	O
.	O
)	O
this	O
prevents	O
the	O
method	O
from	O
being	O
applied	O
to	O
large	O
maps	O
.	O
t	O
there	O
are	O
two	O
main	O
solutions	O
to	O
this	O
problem	O
.	O
the	O
ﬁrst	O
is	O
to	O
notice	O
that	O
the	O
correlation	O
pattern	O
moves	O
along	O
with	O
the	O
location	O
of	O
the	O
robot	O
(	O
see	O
figure	O
18.3	O
(	O
b	O
)	O
)	O
.	O
the	O
remaining	O
correlations	O
become	O
weaker	O
over	O
time	O
.	O
consequently	O
we	O
can	O
dynamically	O
“	O
prune	O
out	O
”	O
weak	O
edges	O
from	O
the	O
ggm	O
using	O
a	O
technique	O
called	O
the	O
thin	B
junction	I
tree	I
ﬁlter	I
(	O
paskin	O
2003	O
)	O
(	O
junction	B
trees	I
are	O
(	O
cid:26	O
)	O
k	O
explained	O
in	O
section	O
20.4	O
)	O
.	O
a	O
second	O
approach	O
is	O
to	O
notice	O
that	O
,	O
conditional	O
on	O
knowing	O
the	O
robot	O
’	O
s	O
path	B
,	O
x1	O
:	O
t	O
,	O
the	O
landmark	O
locations	O
are	O
independent	O
.	O
that	O
is	O
,	O
p	O
(	O
l|x1	O
:	O
t	O
,	O
y1	O
:	O
t	O
)	O
=	O
k=1	O
p	O
(	O
lk|x1	O
:	O
t	O
,	O
y1	O
:	O
t	O
)	O
.	O
this	O
forms	O
the	O
basis	O
of	O
a	O
method	O
known	O
as	O
fastslam	O
,	O
which	O
combines	O
kalman	O
ﬁltering	B
and	O
particle	B
ﬁltering	I
,	O
as	O
discussed	O
in	O
section	O
23.6.3	O
.	O
(	O
thrun	O
et	O
al	O
.	O
2006	O
)	O
provides	O
a	O
more	O
detailed	O
account	O
of	O
slam	O
and	O
mobile	O
robotics	O
.	O
636	O
chapter	O
18.	O
state	B
space	I
models	O
θt−1	O
θt	O
yt−1	O
yt	O
xt−1	O
xt	O
(	O
a	O
)	O
s	O
t	O
h	O
g	O
e	O
w	O
i	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
0	O
online	O
linear	O
regression	B
w0	O
w1	O
w0	O
batch	B
w1	O
batch	B
5	O
10	O
15	O
20	O
25	O
time	O
(	O
b	O
)	O
figure	O
18.4	O
(	O
a	O
)	O
a	O
dynamic	O
generalization	O
of	O
linear	B
regression	I
.	O
(	O
b	O
)	O
illustration	O
of	O
the	O
recursive	B
least	I
squares	I
algorithm	O
applied	O
to	O
the	O
model	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=n	O
(	O
y|w0	O
+	O
w1x	O
,	O
σ2	O
)	O
.	O
we	O
plot	O
the	O
marginal	O
posterior	O
of	O
w0	O
var	O
[	O
wj|y1	O
:	O
t	O
]	O
.	O
)	O
after	O
seeing	O
all	O
and	O
w1	O
vs	O
number	O
of	O
data	O
points	O
.	O
the	O
data	O
,	O
we	O
converge	B
to	O
the	O
offline	B
ml	O
(	O
least	B
squares	I
)	O
solution	O
,	O
represented	O
by	O
the	O
horizontal	O
lines	O
.	O
figure	O
generated	O
by	O
linregonlinedemokalman	O
.	O
(	O
error	O
bars	O
represent	O
e	O
[	O
wj|y1	O
:	O
t	O
]	O
±	O
(	O
cid:15	O
)	O
18.2.3	O
online	O
parameter	O
learning	B
using	O
recursive	B
least	I
squares	I
we	O
can	O
perform	O
online	O
bayesian	O
inference	B
for	O
the	O
parameters	O
of	O
various	O
statistical	O
models	O
using	O
ssms	O
.	O
in	O
section	O
18.5.3.2	O
,	O
we	O
discuss	O
logistic	B
regression	I
.	O
in	O
this	O
section	O
,	O
we	O
focus	O
on	O
linear	B
regression	I
;	O
the	O
basic	O
idea	O
is	O
to	O
let	O
the	O
hidden	B
state	O
represent	O
the	O
regression	B
parameters	O
,	O
and	O
to	O
let	O
the	O
(	O
time-varying	O
)	O
observation	B
model	I
represent	O
the	O
current	O
data	O
vector	O
.	O
in	O
more	O
detail	O
,	O
deﬁne	O
the	O
prior	O
to	O
be	O
p	O
(	O
θ	O
)	O
=n	O
(	O
θ|θ0	O
,	O
σ0	O
)	O
.	O
(	O
if	O
we	O
want	O
to	O
do	O
online	O
ml	O
estimation	O
,	O
we	O
can	O
just	O
set	O
σ0	O
=	O
∞i	O
.	O
)	O
let	O
the	O
hidden	B
state	O
be	O
zt	O
=	O
θ	O
;	O
if	O
we	O
assume	O
the	O
regression	B
parameters	O
do	O
not	O
change	O
,	O
we	O
can	O
set	O
at	O
=	O
i	O
and	O
qt	O
=	O
0i	O
,	O
so	O
p	O
(	O
θt|θt−1	O
)	O
=	O
n	O
(	O
θt|θt−1	O
,	O
0i	O
)	O
=	O
δθt−1	O
(	O
θt	O
)	O
(	O
18.12	O
)	O
(	O
if	O
we	O
do	O
let	O
the	O
parameters	O
change	O
over	O
time	O
,	O
we	O
get	O
a	O
so-called	O
dynamic	B
linear	I
model	I
(	O
harvey	O
1990	O
;	O
west	O
and	O
harrison	O
1997	O
;	O
petris	O
et	O
al	O
.	O
2009	O
)	O
.	O
)	O
let	O
ct	O
=	O
xt	O
t	O
,	O
and	O
rt	O
=	O
σ2	O
,	O
so	O
the	O
(	O
non-stationary	O
)	O
observation	B
model	I
has	O
the	O
form	O
n	O
(	O
yt|ctzt	O
,	O
rt	O
)	O
=	O
n	O
(	O
yt|xt	O
t	O
θt	O
,	O
σ2	O
)	O
(	O
18.13	O
)	O
applying	O
the	O
kalman	O
ﬁlter	O
to	O
this	O
model	O
provides	O
a	O
way	O
to	O
update	O
our	O
posterior	O
beliefs	O
about	O
the	O
parameters	O
as	O
the	O
data	O
streams	O
in	O
.	O
this	O
is	O
known	O
as	O
the	O
recursive	B
least	I
squares	I
or	O
rls	O
algorithm	O
.	O
we	O
can	O
derive	O
an	O
explicit	O
form	O
for	O
the	O
updates	O
as	O
follows	O
.	O
in	O
section	O
18.3.1	O
,	O
we	O
show	O
that	O
the	O
kalman	O
update	O
for	O
the	O
posterior	B
mean	I
has	O
the	O
form	O
μt	O
=	O
atμt−1	O
+	O
kt	O
(	O
yt	O
−	O
ctatμt−1	O
)	O
(	O
18.14	O
)	O
18.2.	O
applications	O
of	O
ssms	O
637	O
where	O
kt	O
is	O
known	O
as	O
the	O
kalman	O
gain	O
matrix	O
.	O
based	O
on	O
equation	O
18.39	O
,	O
one	O
can	O
show	O
that	O
t	O
r−1	O
in	O
this	O
context	O
,	O
we	O
have	O
kt	O
=	O
σtxt/σ2	O
.	O
hence	O
the	O
update	O
for	O
the	O
kt	O
=	O
σtct	O
.	O
parameters	O
becomes	O
t	O
ˆθt	O
=	O
ˆθt−1	O
+	O
σt|t	O
(	O
yt	O
−	O
xt	O
t	O
ˆθt−1	O
)	O
xt	O
1	O
σ2	O
(	O
18.15	O
)	O
if	O
we	O
approximate	O
1	O
σ2	O
σt|t−1	O
with	O
ηti	O
,	O
we	O
recover	O
the	O
least	B
mean	I
squares	I
or	O
lms	O
algorithm	O
,	O
discussed	O
in	O
section	O
8.5.3.	O
in	O
lms	O
,	O
we	O
need	O
to	O
specify	O
how	O
to	O
adapt	O
the	O
update	O
parameter	B
ηt	O
to	O
ensure	O
convergence	O
to	O
the	O
mle	O
.	O
furthermore	O
,	O
the	O
algorithm	O
may	O
take	O
multiple	O
passes	O
through	O
the	O
data	O
.	O
by	O
contrast	O
,	O
the	O
rls	O
algorithm	O
automatically	O
performs	O
step-size	O
adaptation	O
,	O
and	O
converges	O
to	O
the	O
optimal	O
posterior	O
in	O
one	O
pass	O
over	O
the	O
data	O
.	O
see	O
figure	O
18.4	O
for	O
an	O
example	O
.	O
18.2.4	O
ssm	O
for	O
time	O
series	O
forecasting	O
*	O
ssms	O
are	O
very	O
well	O
suited	O
for	O
time-series	B
forecasting	I
,	O
as	O
we	O
explain	O
below	O
.	O
we	O
focus	O
on	O
the	O
case	O
of	O
scalar	O
(	O
one	O
dimensional	O
)	O
time	O
series	O
,	O
for	O
simplicity	O
.	O
our	O
presentation	O
is	O
based	O
on	O
(	O
varian	O
2011	O
)	O
.	O
see	O
also	O
(	O
aoki	O
1987	O
;	O
harvey	O
1990	O
;	O
west	O
and	O
harrison	O
1997	O
;	O
durbin	O
and	O
koopman	O
2001	O
;	O
petris	O
et	O
al	O
.	O
2009	O
;	O
prado	O
and	O
west	O
2010	O
)	O
for	O
good	O
books	O
on	O
this	O
topic	B
.	O
at	O
ﬁrst	O
sight	O
,	O
it	O
might	O
not	O
be	O
apparent	O
why	O
ssms	O
are	O
useful	O
,	O
since	O
the	O
goal	O
in	O
forecasting	O
is	O
to	O
predict	O
future	O
visible	B
variables	I
,	O
not	O
to	O
estimate	O
hidden	B
states	O
of	O
some	O
system	O
.	O
indeed	O
,	O
most	O
classical	B
methods	O
for	O
time	O
series	O
forecasting	O
are	O
just	O
functions	O
of	O
the	O
form	O
ˆyt+1	O
=	O
f	O
(	O
y1	O
:	O
t	O
,	O
θ	O
)	O
,	O
where	O
hidden	B
variables	I
play	O
no	O
role	O
(	O
see	O
section	O
18.2.4.4	O
)	O
.	O
the	O
idea	O
in	O
the	O
state-space	O
approach	O
to	O
time	O
series	O
is	O
to	O
create	O
a	O
generative	O
model	O
of	O
the	O
data	O
in	O
terms	O
of	O
latent	B
processes	O
,	O
which	O
capture	O
different	O
aspects	O
of	O
the	O
signal	O
.	O
we	O
can	O
then	O
integrate	B
out	I
the	O
hidden	B
variables	I
to	O
compute	O
the	O
posterior	O
predictive	O
of	O
the	O
visibles	O
.	O
since	O
the	O
model	O
is	O
linear-gaussian	O
,	O
we	O
can	O
just	O
add	O
these	O
processes	O
together	O
to	O
explain	O
the	O
observed	O
data	O
.	O
this	O
is	O
called	O
a	O
structural	B
time	I
series	I
model	O
.	O
below	O
we	O
explain	O
some	O
of	O
the	O
basic	O
building	O
blocks	O
.	O
18.2.4.1	O
local	B
level	I
model	I
the	O
simplest	O
latent	B
process	O
is	O
known	O
as	O
the	O
local	B
level	I
model	I
,	O
which	O
has	O
the	O
form	O
t	O
∼	O
n	O
(	O
0	O
,	O
r	O
)	O
y	O
yt	O
=	O
at	O
+	O
y	O
t	O
,	O
t	O
∼	O
n	O
(	O
0	O
,	O
q	O
)	O
a	O
at	O
=	O
at−1	O
+	O
a	O
t	O
,	O
(	O
18.16	O
)	O
(	O
18.17	O
)	O
where	O
the	O
hidden	B
state	O
is	O
just	O
zt	O
=	O
at	O
.	O
this	O
model	O
asserts	O
that	O
the	O
observed	O
data	O
yt	O
∈	O
r	O
is	O
equal	O
to	O
some	O
unknown	B
level	O
term	O
at	O
∈	O
r	O
,	O
plus	O
observation	B
noise	O
with	O
variance	B
r.	O
in	O
addition	O
,	O
the	O
level	O
at	O
evolves	O
over	O
time	O
subject	O
to	O
system	O
noise	O
with	O
variance	B
q.	O
see	O
figure	O
18.5	O
for	O
some	O
examples	O
.	O
638	O
at−1	O
at	O
yt	O
6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
−8	O
0	O
chapter	O
18.	O
state	B
space	I
models	O
local	O
level	O
,	O
a=1.000	O
q=0.0	O
,	O
r=0.1	O
q=0.1	O
,	O
r=0.0	O
q=0.1	O
,	O
r=0.1	O
20	O
40	O
60	O
80	O
100	O
120	O
140	O
160	O
180	O
200	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
a	O
)	O
local	B
level	I
model	I
.	O
(	O
b	O
)	O
sample	O
output	O
,	O
for	O
a0	O
=	O
10.	O
black	O
solid	O
line	O
:	O
q	O
=	O
0	O
,	O
r	O
=	O
1	O
figure	O
18.5	O
(	O
deterministic	O
system	O
,	O
noisy	O
observations	O
)	O
.	O
red	O
dotted	O
line	O
:	O
q	O
=	O
0.1	O
,	O
r	O
=	O
0	O
(	O
noisy	O
system	O
,	O
deterministic	O
observation	O
)	O
.	O
blue	O
dot-dash	O
line	O
:	O
q	O
=	O
0.1	O
,	O
r	O
=	O
1	O
(	O
noisy	O
system	O
and	O
observations	O
)	O
.	O
figure	O
generated	O
by	O
ssmtimeseriessimple	O
.	O
at−1	O
bt−1	O
at	O
bt	O
yt	O
200	O
0	O
−200	O
−400	O
−600	O
−800	O
−1000	O
0	O
local	O
trend	O
,	O
a=10.000	O
,	O
b=1.000	O
q=0.0	O
,	O
r=100.0	O
q=1.0	O
,	O
r=0.0	O
q=1.0	O
,	O
r=100.0	O
10	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
90	O
100	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
18.6	O
(	O
a	O
)	O
local	O
trend	O
.	O
(	O
b	O
)	O
sample	O
output	O
,	O
for	O
a0	O
=	O
10	O
,	O
b0	O
=	O
1.	O
color	O
code	O
as	O
in	O
figure	O
18.5.	O
figure	O
generated	O
by	O
ssmtimeseriessimple	O
.	O
18.2.4.2	O
local	O
linear	O
trend	O
many	O
time	O
series	O
exhibit	O
linear	O
trends	O
upwards	O
or	O
downwards	O
,	O
at	O
least	O
locally	O
.	O
we	O
can	O
model	O
this	O
by	O
letting	O
the	O
level	O
at	O
change	O
by	O
an	O
amount	O
bt	O
at	O
each	O
step	O
as	O
follows	O
:	O
see	O
figure	O
18.6	O
(	O
a	O
)	O
.	O
we	O
can	O
write	O
this	O
in	O
standard	O
form	O
by	O
deﬁning	O
zt	O
=	O
(	O
at	O
,	O
bt	O
)	O
and	O
when	O
qb	O
=	O
0	O
,	O
we	O
have	O
bt	O
=	O
b0	O
,	O
which	O
is	O
some	O
constant	O
deﬁning	O
the	O
slope	O
of	O
the	O
line	O
.	O
if	O
in	O
addition	O
we	O
have	O
qa	O
=	O
0	O
,	O
we	O
havea	O
t	O
=	O
at−1	O
+	O
b0t	O
.	O
unrolling	O
this	O
,	O
we	O
have	O
at	O
=	O
a0	O
+	O
b0t	O
,	O
and	O
yt	O
=	O
at	O
+	O
y	O
t	O
,	O
at	O
=	O
at−1	O
+	O
bt−1	O
+	O
a	O
t	O
,	O
bt	O
=	O
bt−1	O
+	O
b	O
t	O
,	O
t	O
∼	O
n	O
(	O
0	O
,	O
r	O
)	O
y	O
t	O
∼	O
n	O
(	O
0	O
,	O
qa	O
)	O
a	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
a	O
=	O
1	O
1	O
0	O
1	O
,	O
c	O
=	O
1	O
t	O
∼	O
n	O
(	O
0	O
,	O
qb	O
)	O
b	O
(	O
cid:22	O
)	O
(	O
cid:23	O
)	O
0	O
,	O
q	O
=	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
qa	O
0	O
0	O
qb	O
(	O
18.18	O
)	O
(	O
18.19	O
)	O
(	O
18.20	O
)	O
(	O
18.21	O
)	O
18.2.	O
applications	O
of	O
ssms	O
639	O
at−1	O
bt−1	O
c3	O
t−1	O
c2	O
t−1	O
c1	O
t−1	O
-1	O
-1	O
-1	O
at	O
bt	O
c3	O
t	O
c2	O
t	O
c1	O
t	O
yt	O
seasonal	O
model	O
,	O
s=4	O
,	O
a=0.000	O
,	O
b=0.000	O
q=0.0	O
,	O
r=1.0	O
q=1.0	O
,	O
r=0.0	O
q=1.0	O
,	O
r=1.0	O
2	O
4	O
6	O
8	O
10	O
12	O
14	O
16	O
18	O
20	O
40	O
30	O
20	O
10	O
0	O
−10	O
−20	O
−30	O
0	O
figure	O
18.7	O
color	O
code	O
as	O
in	O
figure	O
18.5.	O
figure	O
generated	O
by	O
ssmtimeseriessimple	O
.	O
(	O
a	O
)	O
seasonal	O
model	O
.	O
(	O
b	O
)	O
sample	O
output	O
,	O
for	O
a0	O
=	O
b0	O
=	O
0	O
,	O
c0	O
=	O
(	O
1	O
,	O
1	O
,	O
1	O
)	O
,	O
with	O
a	O
period	B
of	O
4.	O
hence	O
e	O
[	O
yt|y1	O
:	O
t−1	O
]	O
=	O
a0	O
+	O
tb0	O
.	O
this	O
is	O
thus	O
a	O
generalization	B
of	O
the	O
classic	O
constant	O
linear	O
trend	O
model	O
,	O
an	O
example	O
of	O
which	O
is	O
shown	O
in	O
the	O
black	O
line	O
of	O
figure	O
18.6	O
(	O
b	O
)	O
.	O
18.2.4.3	O
seasonality	O
many	O
time	O
series	O
ﬂuctuate	O
periodically	O
,	O
as	O
illustrated	O
in	O
figure	O
18.7	O
(	O
b	O
)	O
.	O
this	O
can	O
be	O
modeled	O
by	O
adding	O
a	O
latent	B
process	O
consisting	O
of	O
a	O
series	O
offset	O
terms	O
,	O
ct	O
,	O
which	O
sum	O
to	O
zero	O
(	O
on	O
average	O
)	O
over	O
a	O
complete	B
cycle	O
of	O
s	O
steps	O
:	O
ct−s	O
+	O
c	O
t	O
,	O
c	O
t	O
∼	O
n	O
(	O
0	O
,	O
qc	O
)	O
(	O
18.22	O
)	O
s=1	O
see	O
figure	O
18.7	O
(	O
a	O
)	O
for	O
the	O
graphical	B
model	I
for	O
the	O
case	O
s	O
=	O
4	O
(	O
we	O
only	O
need	O
3	O
seasonal	O
vari-	O
able	O
because	O
of	O
the	O
sum-to-zero	O
constraint	O
)	O
.	O
writing	O
this	O
in	O
standard	O
lg-ssm	O
form	O
is	O
left	O
to	O
exercise	O
18.2	O
.	O
18.2.4.4	O
arma	O
models	O
*	O
the	O
classical	B
approach	O
to	O
time-series	B
forecasting	I
is	O
based	O
on	O
arma	O
models	O
.	O
“	O
arma	O
”	O
stands	O
for	O
auto-regressive	O
moving-average	O
,	O
and	O
refers	O
to	O
a	O
model	O
of	O
the	O
form	O
ct	O
=	O
−	O
s−1	O
(	O
cid:4	O
)	O
p	O
(	O
cid:4	O
)	O
i=1	O
xt	O
=	O
q	O
(	O
cid:4	O
)	O
j=1	O
αixt−i	O
+	O
βjwt−j	O
+	O
vt	O
(	O
18.23	O
)	O
where	O
vt	O
,	O
wt	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
are	O
independent	O
gaussian	O
noise	O
terms	O
.	O
if	O
q	O
=	O
0	O
,	O
we	O
have	O
a	O
pure	B
ar	O
model	O
,	O
where	O
xt	O
⊥	O
xi|xt−1	O
:	O
t−p	O
,	O
for	O
i	O
<	O
t	O
−	O
p.	O
for	O
example	O
,	O
if	O
p	O
=	O
1	O
,	O
we	O
have	O
the	O
ar	O
(	O
1	O
)	O
model	O
640	O
chapter	O
18.	O
state	B
space	I
models	O
x1	O
x2	O
x3	O
x4	O
x1	O
x2	O
x3	O
x4	O
w1	O
w2	O
w3	O
(	O
a	O
)	O
(	O
b	O
)	O
w1	O
w2	O
w3	O
x1	O
x2	O
x3	O
x4	O
(	O
c	O
)	O
figure	O
18.8	O
(	O
a	O
)	O
an	O
ar	O
(	O
1	O
)	O
model	O
.	O
(	O
b	O
)	O
an	O
ma	O
(	O
1	O
)	O
model	O
represented	O
as	O
a	O
bi-directed	B
graph	I
.	O
(	O
c	O
)	O
an	O
arma	O
(	O
1,1	O
)	O
model	O
.	O
source	O
:	O
figure	O
5.14	O
of	O
(	O
choi	O
2011	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
myung	O
choi	O
.	O
(	O
the	O
vt	O
nodes	B
are	O
implicit	O
in	O
the	O
gaussian	O
cpd	O
for	O
xt	O
.	O
)	O
this	O
is	O
just	O
a	O
shown	O
in	O
figure	O
18.8	O
(	O
a	O
)	O
.	O
if	O
p	O
=	O
0	O
,	O
we	O
have	O
a	O
pure	B
ma	O
model	O
,	O
where	O
xt	O
⊥	O
xi	O
,	O
for	O
i	O
<	O
t	O
−	O
q.	O
ﬁrst-order	O
markov	O
chain	O
.	O
for	O
example	O
,	O
if	O
q	O
=	O
1	O
,	O
we	O
have	O
the	O
ma	O
(	O
1	O
)	O
model	O
shown	O
in	O
figure	O
18.8	O
(	O
b	O
)	O
.	O
here	O
the	O
wt	O
nodes	B
are	O
hidden	B
common	O
causes	O
,	O
which	O
induces	O
dependencies	O
between	O
adjacent	O
time	O
steps	O
.	O
this	O
models	O
short-range	O
correlation	O
.	O
if	O
p	O
=	O
q	O
=	O
1	O
,	O
we	O
get	O
the	O
arma	O
(	O
1,1	O
)	O
model	O
shown	O
in	O
figure	O
18.8	O
(	O
c	O
)	O
,	O
which	O
captures	O
correlation	O
at	O
short	O
and	O
long	O
time	O
scales	O
.	O
it	O
turns	O
out	O
that	O
arma	O
models	O
can	O
be	O
represented	O
as	O
ssms	O
,	O
as	O
explained	O
in	O
(	O
aoki	O
1987	O
;	O
harvey	O
1990	O
;	O
west	O
and	O
harrison	O
1997	O
;	O
durbin	O
and	O
koopman	O
2001	O
;	O
petris	O
et	O
al	O
.	O
2009	O
;	O
prado	O
and	O
west	O
2010	O
)	O
.	O
however	O
,	O
the	O
structural	O
approach	O
to	O
time	O
series	O
is	O
often	O
easier	O
to	O
understand	O
than	O
the	O
arma	O
approach	O
.	O
in	O
addition	O
,	O
it	O
allows	O
the	O
parameters	O
to	O
evolve	O
over	O
time	O
,	O
which	O
makes	O
the	O
models	O
more	O
adaptive	O
to	O
non-stationarity	O
.	O
18.3	O
inference	B
in	O
lg-ssm	O
in	O
this	O
section	O
,	O
we	O
discuss	O
exact	O
inference	B
in	O
lg-ssm	O
models	O
.	O
we	O
ﬁrst	O
consider	O
the	O
online	O
case	O
,	O
which	O
is	O
analogous	O
to	O
the	O
forwards	O
algorithm	O
for	O
hmms	O
.	O
we	O
then	O
consider	O
the	O
offline	B
case	O
,	O
which	O
is	O
analogous	O
to	O
the	O
forwards-backwards	B
algorithm	I
for	O
hmms	O
.	O
18.3.1	O
the	O
kalman	O
ﬁltering	B
algorithm	O
the	O
kalman	O
ﬁlter	O
is	O
an	O
algorithm	O
for	O
exact	O
bayesian	O
ﬁltering	B
for	O
linear-gaussian	O
state	B
space	I
models	O
.	O
we	O
will	O
represent	O
the	O
marginal	O
posterior	O
at	O
time	O
t	O
by	O
p	O
(	O
zt|y1	O
:	O
t	O
,	O
u1	O
:	O
t	O
)	O
=	O
n	O
(	O
zt|μt	O
,	O
σt	O
)	O
(	O
18.24	O
)	O
since	O
everything	O
is	O
gaussian	O
,	O
we	O
can	O
perform	O
the	O
prediction	O
and	O
update	O
steps	O
in	O
closed	O
form	O
,	O
as	O
we	O
explain	O
below	O
.	O
the	O
resulting	O
algorithm	O
is	O
the	O
gaussian	O
analog	O
of	O
the	O
hmm	O
ﬁlter	O
in	O
section	O
17.4.2	O
.	O
18.3.	O
inference	B
in	O
lg-ssm	O
18.3.1.1	O
prediction	O
step	O
the	O
prediction	O
step	O
is	O
straightforward	O
to	O
derive	O
:	O
p	O
(	O
zt|y1	O
:	O
t−1	O
,	O
u1	O
:	O
t	O
)	O
=	O
n	O
(	O
zt|atzt−1	O
+	O
btut	O
,	O
qt	O
)	O
n	O
(	O
zt−1|μt−1	O
,	O
σt−1	O
)	O
dzt−1	O
(	O
cid:12	O
)	O
=	O
n	O
(	O
zt|μt|t−1	O
,	O
σt|t−1	O
)	O
μt|t−1	O
(	O
cid:2	O
)	O
atμt−1	O
+	O
btut	O
σt|t−1	O
(	O
cid:2	O
)	O
atσt−1at	O
t	O
+	O
qt	O
18.3.1.2	O
measurement	O
step	O
the	O
measurement	O
step	O
can	O
be	O
computed	O
using	O
bayes	O
rule	O
,	O
as	O
follows	O
p	O
(	O
zt|yt	O
,	O
y1	O
:	O
t−1	O
,	O
u1	O
:	O
t	O
)	O
∝	O
p	O
(	O
yt|zt	O
,	O
ut	O
)	O
p	O
(	O
zt|y1	O
:	O
t−1	O
,	O
u1	O
:	O
t	O
)	O
in	O
section	O
18.3.1.6	O
,	O
we	O
show	O
that	O
this	O
is	O
given	O
by	O
p	O
(	O
zt|y1	O
:	O
t	O
,	O
ut	O
)	O
=n	O
(	O
zt|μt	O
,	O
σt	O
)	O
μt	O
=	O
μt|t−1	O
+	O
ktrt	O
σt	O
=	O
(	O
i	O
−	O
ktct	O
)	O
σt|t−1	O
641	O
(	O
18.25	O
)	O
(	O
18.26	O
)	O
(	O
18.27	O
)	O
(	O
18.28	O
)	O
(	O
18.29	O
)	O
(	O
18.30	O
)	O
(	O
18.31	O
)	O
(	O
18.32	O
)	O
where	O
rt	O
is	O
the	O
residual	B
or	O
innovation	B
,	O
given	O
by	O
the	O
difference	O
between	O
our	O
predicted	O
observa-	O
tion	O
and	O
the	O
actual	O
observation	B
:	O
rt	O
(	O
cid:2	O
)	O
yt	O
−	O
ˆyt	O
ˆyt	O
(	O
cid:2	O
)	O
e	O
[	O
yt|y1	O
:	O
t−1	O
,	O
u1	O
:	O
t	O
]	O
=	O
ctμt|t−1	O
+	O
dtut	O
and	O
kt	O
is	O
the	O
kalman	O
gain	O
matrix	O
,	O
given	O
by	O
kt	O
(	O
cid:2	O
)	O
σt|t−1ct	O
t	O
s−1	O
t	O
(	O
18.33	O
)	O
(	O
18.34	O
)	O
(	O
18.35	O
)	O
(	O
18.36	O
)	O
(	O
18.37	O
)	O
(	O
cid:14	O
)	O
where	O
st	O
(	O
cid:2	O
)	O
cov	O
[	O
rt|y1	O
:	O
t−1	O
,	O
u1	O
:	O
t	O
]	O
(	O
cid:13	O
)	O
=	O
e	O
=	O
ctσt|t−1ct	O
(	O
ctzt	O
+	O
δt	O
−	O
ˆyt	O
)	O
(	O
ctzt	O
+	O
δt	O
−	O
ˆyt	O
)	O
t|y1	O
:	O
t−1	O
,	O
u1	O
:	O
t	O
t	O
+	O
rt	O
(	O
18.38	O
)	O
where	O
δt	O
∼	O
n	O
(	O
0	O
,	O
rt	O
)	O
is	O
an	O
observation	B
noise	O
term	O
which	O
is	O
independent	O
of	O
all	O
other	O
noise	O
sources	O
.	O
note	O
that	O
by	O
using	O
the	O
matrix	B
inversion	I
lemma	I
,	O
the	O
kalman	O
gain	O
matrix	O
can	O
also	O
be	O
written	O
as	O
kt	O
=	O
σt|t−1ct	O
(	O
cσt|t−1ct	O
+	O
r	O
)	O
−1	O
=	O
(	O
σ−1	O
t|t−1	O
+	O
ct	O
rc	O
)	O
−1ct	O
r−1	O
(	O
18.39	O
)	O
we	O
now	O
have	O
all	O
the	O
quantities	O
we	O
need	O
to	O
implement	O
the	O
algorithm	O
;	O
see	O
kalmanfilter	O
for	O
some	O
matlab	O
code	O
.	O
let	O
us	O
try	O
to	O
make	O
sense	O
of	O
these	O
equations	O
.	O
in	O
particular	O
,	O
consider	O
the	O
equation	O
for	O
the	O
mean	B
update	O
:	O
μt	O
=	O
μt|t−1	O
+	O
ktrt	O
.	O
this	O
says	O
that	O
the	O
new	O
mean	B
is	O
the	O
old	O
mean	B
plus	O
a	O
642	O
chapter	O
18.	O
state	B
space	I
models	O
correction	O
factor	B
,	O
which	O
is	O
kt	O
times	O
the	O
error	B
signal	I
rt	O
.	O
the	O
amount	O
of	O
weight	O
placed	O
on	O
the	O
error	B
signal	I
depends	O
on	O
the	O
kalman	O
gain	O
matrix	O
.	O
,	O
which	O
is	O
the	O
ratio	O
between	O
the	O
covariance	B
of	O
the	O
prior	O
(	O
from	O
the	O
dynamic	O
model	O
)	O
and	O
the	O
covariance	B
if	O
we	O
have	O
a	O
strong	O
prior	O
and/or	O
very	O
noisy	O
sensors	O
,	O
|kt|	O
will	O
be	O
of	O
the	O
measurement	O
error	O
.	O
small	O
,	O
and	O
we	O
will	O
place	O
little	O
weight	O
on	O
the	O
correction	O
term	O
.	O
conversely	O
,	O
if	O
we	O
have	O
a	O
weak	O
prior	O
and/or	O
high	O
precision	O
sensors	O
,	O
then	O
|kt|	O
will	O
be	O
large	O
,	O
and	O
we	O
will	O
place	O
a	O
lot	O
of	O
weight	O
on	O
the	O
correction	O
term	O
.	O
if	O
ct	O
=	O
i	O
,	O
then	O
kt	O
=	O
σt|t−1s−1	O
t	O
18.3.1.3	O
marginal	B
likelihood	I
as	O
a	O
byproduct	O
of	O
the	O
algorithm	O
,	O
we	O
can	O
also	O
compute	O
the	O
log-likelihood	O
of	O
the	O
sequence	O
using	O
log	O
p	O
(	O
y1	O
:	O
t|u1	O
:	O
t	O
)	O
=	O
log	O
p	O
(	O
yt|y1	O
:	O
t−1	O
,	O
u1	O
:	O
t	O
)	O
t	O
where	O
p	O
(	O
yt|y1	O
:	O
t−1	O
,	O
u1	O
:	O
t	O
)	O
=	O
n	O
(	O
yt|ctμt|t−1	O
,	O
st	O
)	O
(	O
cid:4	O
)	O
(	O
cid:12	O
)	O
(	O
18.40	O
)	O
(	O
18.41	O
)	O
(	O
18.42	O
)	O
(	O
18.43	O
)	O
18.3.1.4	O
posterior	O
predictive	O
the	O
one-step-ahead	O
posterior	O
predictive	B
density	O
for	O
the	O
observations	O
can	O
be	O
computed	O
as	O
follows	O
p	O
(	O
yt|y1	O
:	O
t−1	O
,	O
u1	O
:	O
t	O
)	O
=	O
n	O
(	O
yt|czt	O
,	O
r	O
)	O
n	O
(	O
zt|μt|t−1	O
,	O
σt|t−1	O
)	O
dzt	O
=	O
n	O
(	O
yt|cμt|t−1	O
,	O
cσt|t−1ct	O
+	O
r	O
)	O
this	O
is	O
useful	O
for	O
time	O
series	O
forecasting	O
.	O
18.3.1.5	O
computational	O
issues	O
there	O
are	O
two	O
dominant	O
costs	O
in	O
the	O
kalman	O
ﬁlter	O
:	O
the	O
matrix	O
inversion	O
to	O
compute	O
the	O
kalman	O
gain	O
matrix	O
,	O
kt	O
,	O
which	O
takes	O
o	O
(	O
|yt|3	O
)	O
time	O
;	O
and	O
the	O
matrix-matrix	O
multiply	O
to	O
compute	O
σt	O
,	O
which	O
takes	O
o	O
(	O
|zt|2	O
)	O
time	O
.	O
in	O
some	O
applications	O
(	O
e.g.	O
,	O
robotic	O
mapping	O
)	O
,	O
we	O
have	O
|zt|	O
(	O
cid:19	O
)	O
|yt|	O
,	O
so	O
the	O
latter	O
cost	O
dominates	B
.	O
however	O
,	O
in	O
such	O
cases	O
,	O
we	O
can	O
sometimes	O
use	O
sparse	B
approximations	O
(	O
see	O
(	O
thrun	O
et	O
al	O
.	O
2006	O
)	O
)	O
.	O
in	O
cases	O
where	O
|yt|	O
(	O
cid:19	O
)	O
|zt|	O
,	O
we	O
can	O
precompute	O
kt	O
,	O
since	O
,	O
suprisingly	O
,	O
it	O
does	O
not	O
depend	O
on	O
the	O
actual	O
observations	O
y1	O
:	O
t	O
(	O
an	O
unusual	O
property	O
that	O
is	O
speciﬁc	O
to	O
linear	O
gaussian	O
systems	O
)	O
.	O
the	O
iterative	O
equations	O
for	O
updating	O
σt	O
are	O
called	O
the	O
ricatti	O
equations	O
,	O
and	O
for	O
time	O
invariant	O
systems	O
(	O
i.e.	O
,	O
where	O
θt	O
=	O
θ	O
)	O
,	O
they	O
converge	B
to	O
a	O
ﬁxed	B
point	I
.	O
this	O
steady	O
state	B
solution	O
can	O
then	O
be	O
used	O
instead	O
of	O
using	O
a	O
time-speciﬁc	O
gain	O
matrix	O
.	O
in	O
practice	O
,	O
more	O
sophisticated	O
implementations	O
of	O
the	O
kalman	O
ﬁlter	O
should	O
be	O
used	O
,	O
for	O
rea-	O
sons	O
of	O
numerical	O
stability	O
.	O
one	O
approach	O
is	O
the	O
information	B
ﬁlter	I
,	O
which	O
recursively	O
updates	O
the	O
canonical	B
parameters	I
of	O
the	O
gaussian	O
,	O
λt	O
=	O
σ−1	O
and	O
ηt	O
=	O
λtμt	O
,	O
instead	O
of	O
the	O
moment	B
parameters	I
.	O
another	O
approach	O
is	O
the	O
square	B
root	I
ﬁlter	I
,	O
which	O
works	O
with	O
the	O
cholesky	O
de-	O
composition	O
or	O
the	O
utdtut	O
decomposition	O
of	O
σt	O
.	O
this	O
is	O
much	O
more	O
numerically	O
stable	B
than	O
directly	O
updating	O
σt	O
.	O
further	O
details	O
can	O
be	O
found	O
at	O
http	O
:	O
//www.cs.unc.edu/~welch/kal	O
man/	O
and	O
in	O
various	O
books	O
,	O
such	O
as	O
(	O
simon	O
2006	O
)	O
.	O
t	O
18.3.	O
inference	B
in	O
lg-ssm	O
643	O
18.3.1.6	O
derivation	O
*	O
we	O
now	O
derive	O
the	O
kalman	O
ﬁlter	O
equations	O
.	O
for	O
notational	O
simplicity	O
,	O
we	O
will	O
ignore	O
the	O
input	O
terms	O
u1	O
:	O
t.	O
from	O
bayes	O
rule	O
for	O
gaussians	O
(	O
equation	O
4.125	O
)	O
,	O
we	O
have	O
that	O
the	O
posterior	O
precision	O
is	O
given	O
by	O
σ−1	O
t	O
=	O
σ−1	O
t|t−1	O
+	O
ct	O
t	O
r−1	O
t	O
ct	O
from	O
the	O
matrix	B
inversion	I
lemma	I
(	O
equation	O
4.106	O
)	O
we	O
can	O
rewrite	O
this	O
as	O
σt	O
=	O
σt|t−1	O
−	O
σt|t−1ct	O
=	O
(	O
i	O
−	O
ktct	O
)	O
σt|t−1	O
t	O
(	O
rt	O
+	O
ctσt|t−1ct	O
t	O
)	O
−1ctσt|t−1	O
from	O
bayes	O
rule	O
for	O
gaussians	O
(	O
equation	O
4.125	O
)	O
,	O
the	O
posterior	B
mean	I
is	O
given	O
by	O
μt	O
=	O
σtctr−1	O
t	O
yt	O
+	O
σtς−1	O
t|t−1μt|t−1	O
(	O
18.44	O
)	O
(	O
18.45	O
)	O
(	O
18.46	O
)	O
(	O
18.47	O
)	O
we	O
will	O
now	O
massage	O
this	O
into	O
the	O
form	O
stated	O
earlier	O
.	O
applying	O
the	O
second	O
matrix	O
inversion	O
lemma	O
(	O
equation	O
4.107	O
)	O
to	O
the	O
ﬁrst	O
term	O
of	O
equation	O
18.47	O
we	O
have	O
σtctr−1	O
t	O
yt	O
=	O
(	O
σ−1	O
t|t−1	O
+	O
ct	O
t	O
ct	O
)	O
t	O
r−1	O
−1ctr−1	O
t	O
(	O
rt	O
+	O
ctσt|t−1ct	O
t	O
)	O
t	O
yt	O
−1yt	O
=	O
ktyt	O
=	O
σt|t−1ct	O
(	O
18.48	O
)	O
(	O
18.49	O
)	O
now	O
applying	O
the	O
matrix	B
inversion	I
lemma	I
(	O
equation	O
4.106	O
)	O
to	O
the	O
second	O
term	O
of	O
equation	O
18.47	O
we	O
have	O
σtς−1	O
putting	O
the	O
two	O
together	O
we	O
get	O
μt	O
=	O
μt|t−1	O
+	O
kt	O
(	O
yt	O
−	O
ctμt|t−1	O
)	O
18.3.2	O
the	O
kalman	O
smoothing	O
algorithm	O
in	O
section	O
18.3.1	O
,	O
we	O
described	O
the	O
kalman	O
ﬁlter	O
,	O
which	O
sequentially	O
computes	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
for	O
each	O
t.	O
this	O
is	O
useful	O
for	O
online	O
inference	O
problems	O
,	O
such	O
as	O
tracking	B
.	O
however	O
,	O
in	O
an	O
offline	B
setting	O
,	O
we	O
can	O
wait	O
until	O
all	O
the	O
data	O
has	O
arrived	O
,	O
and	O
then	O
compute	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
.	O
by	O
conditioning	B
on	O
past	O
and	O
future	O
data	O
,	O
our	O
uncertainty	B
will	O
be	O
signiﬁcantly	O
reduced	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
18.1	O
(	O
c	O
)	O
,	O
where	O
we	O
see	O
that	O
the	O
posterior	O
covariance	O
ellipsoids	O
are	O
smaller	O
for	O
the	O
smoothed	O
trajectory	O
than	O
for	O
the	O
ﬁltered	O
trajectory	O
.	O
(	O
the	O
ellipsoids	O
are	O
larger	O
at	O
the	O
beginning	O
and	O
end	O
of	O
the	O
trajectory	O
,	O
since	O
states	O
near	O
the	O
boundary	O
do	O
not	O
have	O
as	O
many	O
useful	O
neighbors	B
from	O
which	O
to	O
borrow	O
information	O
.	O
)	O
t	O
ct	O
)	O
t	O
r−1	O
t|t−1μt|t−1	O
(	O
cid:13	O
)	O
=	O
(	O
σ−1	O
=	O
=	O
(	O
σt|t−1	O
−	O
ktct	O
=	O
μt|t−1	O
−	O
ktct	O
−1σ−1	O
t|t−1	O
+	O
ct	O
σt|t−1	O
−	O
σt|t−1ct	O
(	O
rt	O
+	O
ct	O
t	O
σt|t−1	O
)	O
σ−1	O
t	O
μt|t−1	O
t|t−1μt|t−1	O
t|t−1μt|t−1	O
t	O
σt|t−1ct	O
t	O
)	O
ctσt|t−1	O
(	O
cid:14	O
)	O
σ−1	O
t|t−1μt|t−1	O
(	O
18.50	O
)	O
(	O
18.51	O
)	O
(	O
18.52	O
)	O
(	O
18.53	O
)	O
(	O
18.54	O
)	O
(	O
18.55	O
)	O
644	O
chapter	O
18.	O
state	B
space	I
models	O
we	O
now	O
explain	O
how	O
to	O
compute	O
the	O
smoothed	O
estimates	O
,	O
using	O
an	O
algorithm	O
called	O
the	O
rts	O
smoother	O
,	O
named	O
after	O
its	O
inventors	O
,	O
rauch	O
,	O
tung	O
and	O
striebel	O
(	O
rauch	O
et	O
al	O
.	O
1965	O
)	O
.	O
it	O
is	O
also	O
known	O
as	O
the	O
kalman	O
smoothing	O
algorithm	O
.	O
the	O
algorithm	O
is	O
analogous	O
to	O
the	O
forwards-	O
backwards	O
algorithm	O
for	O
hmms	O
,	O
although	O
there	O
are	O
some	O
small	O
differences	O
which	O
we	O
discuss	O
below	O
.	O
18.3.2.1	O
algorithm	O
kalman	O
ﬁltering	B
can	O
be	O
regarded	O
as	O
message	B
passing	I
on	O
a	O
graph	B
,	O
from	O
left	O
to	O
right	O
.	O
when	O
the	O
messages	O
have	O
reached	O
the	O
end	O
of	O
the	O
graph	B
,	O
we	O
have	O
successfully	O
computed	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
.	O
now	O
we	O
work	O
backwards	O
,	O
from	O
right	O
to	O
left	O
,	O
sending	O
information	B
from	O
the	O
future	O
back	O
to	O
the	O
past	O
,	O
and	O
them	O
combining	O
the	O
two	O
information	B
sources	O
.	O
the	O
question	O
is	O
:	O
how	O
do	O
we	O
compute	O
these	O
backwards	O
equations	O
?	O
we	O
ﬁrst	O
give	O
the	O
equations	O
,	O
then	O
the	O
derivation	O
.	O
we	O
have	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
=n	O
(	O
μt|t	O
,	O
σt|t	O
)	O
μt|t	O
=	O
μt|t	O
+	O
jt	O
(	O
μt+1|t	O
−	O
μt+1|t	O
)	O
σt|t	O
=	O
σt|t	O
+	O
jt	O
(	O
σt+1|t	O
−	O
σt+1|t	O
)	O
jt	O
t	O
jt	O
(	O
cid:2	O
)	O
σt|tat	O
t+1σ−1	O
t+1|t	O
(	O
18.56	O
)	O
(	O
18.57	O
)	O
(	O
18.58	O
)	O
(	O
18.59	O
)	O
where	O
jt	O
is	O
the	O
backwards	O
kalman	O
gain	O
matrix	O
.	O
the	O
algorithm	O
can	O
be	O
initialized	O
from	O
μt|t	O
and	O
σt|t	O
from	O
the	O
kalman	O
ﬁlter	O
.	O
note	O
that	O
this	O
backwards	O
pass	O
does	O
not	O
need	O
access	O
to	O
the	O
data	O
,	O
that	O
is	O
,	O
it	O
does	O
not	O
need	O
y1	O
:	O
t	O
.	O
this	O
allows	O
us	O
to	O
“	O
throw	O
away	O
”	O
potentially	O
high	O
dimensional	O
observation	B
vectors	O
,	O
and	O
just	O
keep	O
the	O
ﬁltered	O
belief	O
states	O
,	O
which	O
usually	O
requires	O
less	O
memory	O
.	O
18.3.2.2	O
derivation	O
*	O
we	O
now	O
derive	O
the	O
kalman	O
smoother	O
,	O
following	O
the	O
presentation	O
of	O
(	O
jordan	O
2007	O
,	O
sec	O
15.7	O
)	O
.	O
the	O
key	O
idea	O
is	O
to	O
leverage	O
the	O
markov	O
property	O
,	O
which	O
says	O
that	O
zt	O
is	O
independent	O
of	O
future	O
data	O
,	O
yt+1	O
:	O
t	O
,	O
as	O
long	O
as	O
zt+1	O
is	O
known	O
.	O
of	O
course	O
,	O
zt+1	O
is	O
not	O
known	O
,	O
but	O
we	O
have	O
a	O
distribution	O
over	O
it	O
.	O
so	O
we	O
condition	O
on	O
zt+1	O
and	O
then	O
integrate	O
it	O
out	O
,	O
as	O
follows	O
.	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
=	O
=	O
p	O
(	O
zt|y1	O
:	O
t	O
,	O
zt+1	O
)	O
p	O
(	O
zt+1|y1	O
:	O
t	O
)	O
dzt+1	O
p	O
(	O
zt|y1	O
:	O
t	O
,	O
yt+1	O
:	O
t	O
,	O
zt+1	O
)	O
p	O
(	O
zt+1|y1	O
:	O
t	O
)	O
dzt+1	O
by	O
induction	B
,	O
assume	O
we	O
have	O
already	O
computed	O
the	O
smoothed	O
distribution	O
for	O
t	O
+	O
1	O
:	O
the	O
question	O
is	O
:	O
how	O
do	O
we	O
perform	O
the	O
integration	O
?	O
p	O
(	O
zt+1|y1	O
:	O
t	O
)	O
=	O
n	O
(	O
zt+1|μt+1|t	O
,	O
σt+1|t	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
first	O
,	O
we	O
compute	O
the	O
ﬁltered	O
two-slice	O
distribution	O
p	O
(	O
zt	O
,	O
zt+1|y1	O
:	O
t	O
)	O
as	O
follows	O
:	O
p	O
(	O
zt	O
,	O
zt+1|y1	O
:	O
t	O
)	O
=	O
n	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
cid:9	O
)	O
σt|t	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
at+1σt|t	O
σt|tat	O
σt+1|t	O
t+1	O
zt	O
zt+1	O
|	O
μt|t	O
μt+1|t	O
(	O
18.60	O
)	O
(	O
18.61	O
)	O
(	O
18.62	O
)	O
(	O
18.63	O
)	O
18.3.	O
inference	B
in	O
lg-ssm	O
645	O
now	O
we	O
use	O
gaussian	O
conditioning	B
to	O
compute	O
p	O
(	O
zt|zt+1	O
,	O
y1	O
:	O
t	O
)	O
as	O
follows	O
:	O
p	O
(	O
zt|zt+1	O
,	O
y1	O
:	O
t	O
)	O
=n	O
(	O
zt|μt|t	O
+	O
jt	O
(	O
zt+1	O
−	O
μt+1|t	O
)	O
,	O
σt|t	O
−	O
jtσt+1|tjt	O
(	O
18.64	O
)	O
t	O
)	O
we	O
can	O
compute	O
the	O
smoothed	O
distribution	O
for	O
t	O
using	O
the	O
rules	B
of	O
iterated	O
expectation	O
and	O
(	O
cid:16	O
)	O
μt|t	O
=	O
e	O
=	O
e	O
y1	O
:	O
t	O
(	O
cid:14	O
)	O
(	O
cid:13	O
)	O
iterated	O
covariance	O
.	O
first	O
,	O
the	O
mean	B
:	O
y1	O
:	O
t	O
(	O
cid:13	O
)	O
(	O
cid:14	O
)	O
e	O
[	O
zt|zt+1	O
,	O
y1	O
:	O
t	O
]	O
(	O
cid:15	O
)	O
y1	O
:	O
t	O
e	O
[	O
zt|zt+1	O
,	O
y1	O
:	O
t	O
]	O
μt|t	O
+	O
jt	O
(	O
zt+1	O
−	O
μt+1|t	O
)	O
=	O
e	O
=	O
μt|t	O
+	O
jt	O
(	O
μt+1|t	O
−	O
μt+1|t	O
)	O
y1	O
:	O
t	O
(	O
cid:13	O
)	O
(	O
cid:14	O
)	O
(	O
cid:13	O
)	O
e	O
[	O
zt|zt+1	O
,	O
y1	O
:	O
t	O
]	O
(	O
cid:15	O
)	O
+	O
e	O
=	O
cov	O
[	O
e	O
[	O
zt|zt+1	O
,	O
y1	O
:	O
t	O
]	O
|y1	O
:	O
t	O
]	O
+e	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
μt|t	O
+	O
jt	O
(	O
zt+1	O
−	O
μt+1|t	O
)	O
|y1	O
:	O
t	O
=	O
cov	O
zt+1	O
−	O
μt+1|t|y1	O
:	O
t	O
now	O
the	O
covariance	B
:	O
σt|t	O
=	O
cov	O
=	O
jtcov	O
t	O
+	O
σt|t	O
−	O
jtσt+1|tjt	O
=	O
jtσt+1|t	O
jt	O
=	O
σt|t	O
+	O
jt	O
(	O
σt+1|t	O
−	O
σt+1|t	O
)	O
jt	O
t	O
(	O
cid:13	O
)	O
cov	O
[	O
zt|zt+1	O
,	O
y1	O
:	O
t	O
]	O
(	O
cid:16	O
)	O
cov	O
[	O
zt|zt+1	O
,	O
y1	O
:	O
t	O
]	O
y1	O
:	O
t	O
y1	O
:	O
t	O
(	O
cid:14	O
)	O
(	O
cid:13	O
)	O
(	O
cid:14	O
)	O
σt|t	O
−	O
jtσt+1|tjt	O
t	O
|y1	O
:	O
t	O
+	O
e	O
t	O
+	O
σt|t	O
−	O
jtσt+1|tjt	O
jt	O
t	O
(	O
18.65	O
)	O
(	O
18.66	O
)	O
(	O
18.67	O
)	O
(	O
18.68	O
)	O
(	O
18.69	O
)	O
(	O
18.70	O
)	O
(	O
18.71	O
)	O
(	O
18.72	O
)	O
(	O
18.73	O
)	O
(	O
cid:14	O
)	O
(	O
18.75	O
)	O
(	O
18.76	O
)	O
(	O
18.77	O
)	O
(	O
18.78	O
)	O
(	O
18.74	O
)	O
the	O
algorithm	O
can	O
be	O
initialized	O
from	O
μt|t	O
and	O
σt|t	O
from	O
the	O
last	O
step	O
of	O
the	O
ﬁltering	B
algo-	O
rithm	O
.	O
t	O
18.3.2.3	O
comparison	O
to	O
the	O
forwards-backwards	B
algorithm	I
for	O
hmms	O
*	O
note	O
that	O
in	O
both	O
the	O
forwards	O
and	O
backwards	O
passes	O
for	O
lds	O
,	O
we	O
always	O
worked	O
with	O
normalized	O
distributions	O
,	O
either	O
conditioned	O
on	O
the	O
past	O
data	O
or	O
conditioned	O
on	O
all	O
the	O
data	O
.	O
furthermore	O
,	O
the	O
backwards	O
pass	O
depends	O
on	O
the	O
results	O
of	O
the	O
forwards	O
pass	O
.	O
this	O
is	O
different	O
from	O
the	O
usual	O
presentation	O
of	O
forwards-backwards	B
for	O
hmms	O
,	O
where	O
the	O
backwards	O
pass	O
can	O
be	O
computed	O
independently	O
of	O
the	O
forwards	O
pass	O
(	O
see	O
section	O
17.4.3	O
)	O
.	O
it	O
turns	O
out	O
that	O
we	O
can	O
rewrite	O
the	O
kalman	O
smoother	O
in	O
a	O
modiﬁed	O
form	O
which	O
makes	O
it	O
more	O
similar	B
to	O
forwards-backwards	B
for	O
hmms	O
.	O
in	O
particular	O
,	O
we	O
have	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
=	O
=	O
p	O
(	O
zt|y1	O
:	O
t	O
,	O
zt+1	O
)	O
p	O
(	O
zt+1|y1	O
:	O
t	O
)	O
dzt+1	O
p	O
(	O
zt	O
,	O
zt+1|y1	O
:	O
t	O
)	O
p	O
(	O
zt+1|y1	O
:	O
t	O
)	O
p	O
(	O
zt+1|y1	O
:	O
t	O
)	O
dzt+1	O
now	O
p	O
(	O
zt+1|y1	O
:	O
t	O
)	O
=	O
p	O
(	O
yt+1	O
:	O
t|zt+1	O
,	O
y1	O
:	O
t	O
)	O
p	O
(	O
zt+1|y1	O
:	O
t	O
)	O
p	O
(	O
yt+1	O
:	O
t|y1	O
:	O
t	O
)	O
so	O
p	O
(	O
zt+1|y1	O
:	O
t	O
)	O
p	O
(	O
zt+1|y1	O
:	O
t	O
)	O
p	O
(	O
zt+1|y1	O
:	O
t	O
)	O
p	O
(	O
yt+1	O
:	O
t|zt+1	O
)	O
p	O
(	O
zt+1|y1	O
:	O
t	O
)	O
p	O
(	O
yt+1	O
:	O
t|y1	O
:	O
t	O
)	O
=	O
∝	O
p	O
(	O
yt+1	O
:	O
t|zt+1	O
)	O
646	O
chapter	O
18.	O
state	B
space	I
models	O
which	O
is	O
the	O
conditional	B
likelihood	I
of	O
the	O
future	O
data	O
.	O
this	O
backwards	O
message	O
can	O
be	O
computed	O
independently	O
of	O
the	O
forwards	O
message	O
.	O
however	O
,	O
this	O
approach	O
has	O
several	O
disadvantages	O
:	O
(	O
1	O
)	O
it	O
needs	O
access	O
to	O
the	O
original	O
observation	B
sequence	O
;	O
(	O
2	O
)	O
the	O
backwards	O
message	O
is	O
a	O
likelihood	B
,	O
not	O
a	O
posterior	O
,	O
so	O
it	O
need	O
not	O
to	O
integrate	O
to	O
1	O
over	O
zt	O
–	O
in	O
fact	O
,	O
it	O
may	O
not	O
always	O
be	O
possible	O
to	O
represent	O
p	O
(	O
yt+1	O
:	O
t|zt+1	O
)	O
as	O
a	O
gaussian	O
with	O
positive	B
deﬁnite	I
covariance	O
(	O
this	O
problem	O
does	O
not	O
arise	O
in	O
discrete	B
state-spaces	O
,	O
as	O
used	O
in	O
hmms	O
)	O
;	O
(	O
3	O
)	O
when	O
exact	O
inference	B
is	O
not	O
possible	O
,	O
it	O
makes	O
more	O
sense	O
to	O
try	O
to	O
approximate	O
the	O
smoothed	O
distribution	O
rather	O
than	O
the	O
backwards	O
likelihood	B
term	O
(	O
see	O
section	O
22.5	O
)	O
.	O
there	O
is	O
yet	O
another	O
variant	O
,	O
known	O
as	O
two-ﬁlter	B
smoothing	I
,	O
whereby	O
we	O
compute	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
in	O
the	O
forwards	O
pass	O
as	O
usual	O
,	O
and	O
the	O
ﬁltered	O
posterior	O
p	O
(	O
zt|yt+1	O
:	O
t	O
)	O
in	O
the	O
backwards	O
pass	O
.	O
these	O
can	O
then	O
be	O
easily	O
combined	O
to	O
compute	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
.	O
see	O
(	O
kitagawa	O
2004	O
;	O
briers	O
et	O
al	O
.	O
2010	O
)	O
for	O
details	O
.	O
18.4	O
learning	B
for	O
lg-ssm	O
in	O
this	O
section	O
,	O
we	O
brieﬂy	O
discuss	O
how	O
to	O
estimate	O
the	O
parameters	O
of	O
an	O
lg-ssm	O
.	O
in	O
the	O
control	O
theory	O
community	O
,	O
this	O
is	O
known	O
as	O
systems	B
identiﬁcation	I
(	O
ljung	O
1987	O
)	O
.	O
when	O
using	O
ssms	O
for	O
time	O
series	O
forecasting	O
,	O
and	O
also	O
in	O
some	O
physical	O
state	B
estimation	I
problems	O
,	O
the	O
observation	B
matrix	O
c	O
and	O
the	O
transition	B
matrix	I
a	O
are	O
both	O
known	O
and	O
ﬁxed	O
,	O
by	O
deﬁnition	O
of	O
the	O
model	O
.	O
in	O
such	O
cases	O
,	O
all	O
that	O
needs	O
to	O
be	O
learned	O
are	O
the	O
noise	O
covariances	O
q	O
and	O
r.	O
(	O
the	O
initial	O
state	B
estimate	O
μ0	O
is	O
often	O
less	O
important	O
,	O
since	O
it	O
will	O
get	O
“	O
washed	O
away	O
”	O
by	O
the	O
data	O
after	O
a	O
few	O
time	O
steps	O
.	O
this	O
can	O
be	O
encouraged	O
by	O
setting	O
the	O
initial	O
state	B
covariance	O
to	O
be	O
large	O
,	O
representing	O
a	O
weak	O
prior	O
.	O
)	O
although	O
we	O
can	O
estimate	O
q	O
and	O
r	O
offline	B
,	O
using	O
the	O
methods	O
described	O
below	O
,	O
it	O
is	O
also	O
possible	O
to	O
derive	O
a	O
recursive	B
procedure	O
to	O
exactly	O
compute	O
the	O
posterior	O
p	O
(	O
zt	O
,	O
r	O
,	O
q|y1	O
:	O
t	O
)	O
,	O
which	O
has	O
the	O
form	O
of	O
a	O
normal-inverse-wishart	O
;	O
see	O
(	O
west	O
and	O
harrison	O
1997	O
;	O
prado	O
and	O
west	O
2010	O
)	O
for	O
details	O
.	O
18.4.1	O
identiﬁability	O
and	O
numerical	O
stability	O
in	O
the	O
more	O
general	O
setting	O
,	O
where	O
the	O
hidden	B
states	O
have	O
no	O
pre-speciﬁed	O
meaning	O
,	O
we	O
need	O
to	O
learn	O
a	O
and	O
c.	O
however	O
,	O
in	O
this	O
case	O
we	O
can	O
set	O
q	O
=	O
i	O
without	O
loss	B
of	O
generality	O
,	O
since	O
an	O
arbitrary	O
noise	O
covariance	O
can	O
be	O
modeled	O
by	O
appropriately	O
modifying	O
a.	O
also	O
,	O
by	O
analogy	O
with	O
factor	B
analysis	I
,	O
we	O
can	O
require	O
r	O
to	O
be	O
diagonal	B
without	O
loss	B
of	O
generality	O
.	O
doing	O
this	O
reduces	O
the	O
number	O
of	O
free	O
parameters	O
and	O
improves	O
numerical	O
stability	O
.	O
another	O
constraint	O
that	O
is	O
useful	O
to	O
impose	O
is	O
on	O
the	O
eigenvalues	O
of	O
the	O
dynamics	O
matrix	O
a.	O
in	O
this	O
case	O
,	O
the	O
hidden	B
to	O
see	O
why	O
this	O
is	O
important	O
,	O
consider	O
the	O
case	O
of	O
no	O
system	O
noise	O
.	O
state	B
at	O
time	O
t	O
is	O
given	O
by	O
zt	O
=	O
atz1	O
=	O
uλtu−1z1	O
(	O
18.79	O
)	O
where	O
u	O
is	O
the	O
matrix	O
of	O
eigenvectors	O
for	O
a	O
,	O
and	O
λ	O
=	O
diag	O
(	O
λi	O
)	O
contains	O
the	O
eigenvalues	O
.	O
if	O
any	O
λi	O
>	O
1	O
,	O
then	O
for	O
large	O
t	O
,	O
zt	O
will	O
blow	O
up	O
in	O
magnitude	O
.	O
consequently	O
,	O
to	O
ensure	O
stability	O
,	O
it	O
is	O
useful	O
to	O
require	O
that	O
all	O
the	O
eigenvalues	O
are	O
less	O
than	O
1	O
(	O
siddiqi	O
et	O
al	O
.	O
2007	O
)	O
.	O
of	O
course	O
,	O
if	O
all	O
the	O
eigenvalues	O
are	O
less	O
than	O
1	O
,	O
then	O
e	O
[	O
zt	O
]	O
=0	O
for	O
large	O
t	O
,	O
so	O
the	O
state	B
will	O
return	O
to	O
the	O
origin	O
.	O
fortunately	O
,	O
when	O
we	O
add	O
noise	O
,	O
the	O
state	B
become	O
non-zero	O
,	O
so	O
the	O
model	O
does	O
not	O
degenerate	O
.	O
18.5.	O
approximate	O
online	O
inference	B
for	O
non-linear	O
,	O
non-gaussian	O
ssms	O
647	O
below	O
we	O
discuss	O
how	O
to	O
estimate	O
the	O
parameters	O
.	O
however	O
,	O
for	O
simplicity	O
of	O
presentation	O
,	O
we	O
do	O
not	O
impose	O
any	O
of	O
the	O
constraints	O
mentioned	O
above	O
.	O
18.4.2	O
training	O
with	O
fully	O
observed	O
data	O
if	O
we	O
observe	O
the	O
hidden	B
state	O
sequences	O
,	O
we	O
can	O
ﬁt	O
the	O
model	O
by	O
computing	O
the	O
mles	O
(	O
or	O
even	O
(	O
cid:7	O
)	O
2	O
the	O
full	B
posteriors	O
)	O
for	O
the	O
parameters	O
by	O
solving	O
a	O
multivariate	O
linear	O
regression	B
problem	O
for	O
zt−1	O
→	O
zt	O
and	O
for	O
zt	O
→	O
yt	O
.	O
that	O
is	O
,	O
we	O
can	O
estimate	O
a	O
by	O
solving	O
the	O
least	B
squares	I
problem	O
t=1	O
(	O
zt−azt−1	O
)	O
2	O
,	O
and	O
similarly	O
for	O
c.	O
we	O
can	O
estimate	O
the	O
system	O
noise	O
covariance	O
j	O
(	O
a	O
)	O
=	O
q	O
from	O
the	O
residuals	O
in	O
predicting	O
zt	O
from	O
zt−1	O
,	O
and	O
estimate	O
the	O
observation	B
noise	O
covariance	B
r	O
from	O
the	O
residuals	O
in	O
predicting	O
yt	O
from	O
zt	O
.	O
18.4.3	O
em	O
for	O
lg-ssm	O
if	O
we	O
only	O
observe	O
the	O
output	O
sequence	O
,	O
we	O
can	O
compute	O
ml	O
or	O
map	O
estimates	O
of	O
the	O
parameters	O
using	O
em	O
.	O
the	O
method	O
is	O
conceptually	O
quite	O
similar	B
to	O
the	O
baum-welch	O
algorithm	O
for	O
hmms	O
(	O
section	O
17.5	O
)	O
,	O
except	O
we	O
use	O
kalman	O
smoothing	O
instead	O
of	O
forwards-backwards	B
in	O
the	O
e	O
step	O
,	O
and	O
use	O
different	O
calculations	O
in	O
the	O
m	O
step	O
.	O
we	O
leave	O
the	O
details	O
to	O
exercise	O
18.1	O
.	O
18.4.4	O
subspace	O
methods	O
em	O
does	O
not	O
always	O
give	O
satisfactory	O
results	O
,	O
because	O
it	O
is	O
sensitive	O
to	O
the	O
initial	O
parameter	B
estimates	O
.	O
one	O
way	O
to	O
avoid	O
this	O
is	O
to	O
use	O
a	O
different	O
approach	O
known	O
as	O
a	O
subspace	B
method	I
(	O
overschee	O
and	O
moor	O
1996	O
;	O
katayama	O
2005	O
)	O
.	O
to	O
understand	O
this	O
approach	O
,	O
let	O
us	O
initially	O
assume	O
there	O
is	O
no	O
observation	O
noise	O
and	O
no	O
in	O
this	O
case	O
,	O
we	O
have	O
zt	O
=	O
azt−1	O
and	O
yt	O
=	O
czt	O
,	O
and	O
hence	O
yt	O
=	O
cat−1z1	O
.	O
system	O
noise	O
.	O
consequently	O
all	O
the	O
observations	O
must	O
be	O
generated	O
from	O
a	O
dim	O
(	O
zt	O
)	O
-dimensional	O
linear	O
mani-	O
fold	O
or	O
subspace	O
.	O
we	O
can	O
identify	O
this	O
subspace	O
using	O
pca	O
(	O
see	O
the	O
above	O
references	O
for	O
details	O
)	O
.	O
once	O
we	O
have	O
an	O
estimate	O
of	O
the	O
zt	O
’	O
s	O
,	O
we	O
can	O
ﬁt	O
the	O
model	O
as	O
if	O
it	O
were	O
fully	O
observed	O
.	O
we	O
can	O
either	O
use	O
these	O
estimates	O
in	O
their	O
own	O
right	O
,	O
or	O
use	O
them	O
to	O
initialize	O
em	O
.	O
18.4.5	O
bayesian	O
methods	O
for	O
“	O
ﬁtting	O
”	O
lg-ssms	O
there	O
are	O
various	O
offline	B
bayesian	O
alternatives	O
to	O
the	O
em	O
algorithm	O
,	O
including	O
variational	O
bayes	O
em	O
(	O
beal	O
2003	O
;	O
barber	O
and	O
chiappa	O
2007	O
)	O
and	O
blocked	O
gibbs	O
sampling	O
(	O
carter	O
and	O
kohn	O
1994	O
;	O
cappe	O
et	O
al	O
.	O
2005	O
;	O
fruhwirth-schnatter	O
2007	O
)	O
.	O
the	O
bayesian	O
approach	O
can	O
also	O
be	O
used	O
to	O
perform	O
online	B
learning	I
,	O
as	O
we	O
discussed	O
in	O
section	O
18.2.3.	O
unfortunately	O
,	O
once	O
we	O
add	O
the	O
ssm	O
parameters	O
to	O
the	O
state	B
space	I
,	O
the	O
model	O
is	O
generally	O
no	O
longer	O
linear	O
gaussian	O
.	O
consequently	O
we	O
must	O
use	O
some	O
of	O
the	O
approximate	O
online	O
inference	B
methods	O
to	O
be	O
discussed	O
below	O
.	O
18.5	O
approximate	O
online	O
inference	B
for	O
non-linear	O
,	O
non-gaussian	O
ssms	O
in	O
section	O
18.3.1	O
,	O
we	O
discussed	O
how	O
to	O
perform	O
exact	O
online	O
inference	O
for	O
lg-ssms	O
.	O
however	O
,	O
many	O
models	O
are	O
non	O
linear	O
.	O
for	O
example	O
,	O
most	O
moving	O
objects	O
do	O
not	O
move	O
in	O
straight	O
lines	O
.	O
and	O
even	O
if	O
they	O
did	O
,	O
if	O
we	O
assume	O
the	O
parameters	O
of	O
the	O
model	O
are	O
unknown	B
and	O
add	O
them	O
648	O
chapter	O
18.	O
state	B
space	I
models	O
to	O
the	O
state	B
space	I
,	O
the	O
model	O
becomes	O
nonlinear	O
.	O
furthermore	O
,	O
non-gaussian	O
noise	O
is	O
also	O
very	O
common	O
,	O
e.g.	O
,	O
due	O
to	O
outliers	B
,	O
or	O
when	O
inferring	O
parameters	O
for	O
glms	O
instead	O
of	O
just	O
linear	B
regression	I
.	O
for	O
these	O
more	O
general	O
models	O
,	O
we	O
need	O
to	O
use	O
approximate	B
inference	I
.	O
the	O
approximate	B
inference	I
algorithms	O
we	O
discuss	O
below	O
approximate	O
the	O
posterior	O
by	O
a	O
gaus-	O
in	O
general	O
,	O
if	O
y	O
=	O
f	O
(	O
x	O
)	O
,	O
where	O
x	O
has	O
a	O
gaussian	O
distribution	O
and	O
f	O
is	O
a	O
non-linear	O
sian	O
.	O
function	O
,	O
there	O
are	O
two	O
main	O
ways	O
to	O
approximate	O
p	O
(	O
y	O
)	O
by	O
a	O
gaussian	O
.	O
the	O
ﬁrst	O
is	O
to	O
use	O
a	O
ﬁrst-order	O
approximation	O
of	O
f.	O
the	O
second	O
is	O
to	O
use	O
the	O
exact	O
f	O
,	O
but	O
to	O
project	O
f	O
(	O
x	O
)	O
onto	O
the	O
space	O
of	O
gaussians	O
by	O
moment	B
matching	I
.	O
we	O
discuss	O
each	O
of	O
these	O
methods	O
in	O
turn	O
.	O
(	O
see	O
also	O
section	O
23.5	O
,	O
where	O
we	O
discuss	O
particle	B
ﬁltering	I
,	O
which	O
is	O
a	O
stochastic	B
algorithm	I
for	O
approximate	O
online	O
inference	B
,	O
which	O
uses	O
a	O
non-parametric	O
approximation	O
to	O
the	O
posterior	O
,	O
which	O
is	O
often	O
more	O
accurate	O
but	O
slower	O
to	O
compute	O
.	O
)	O
18.5.1	O
extended	O
kalman	O
ﬁlter	O
(	O
ekf	O
)	O
in	O
this	O
section	O
,	O
we	O
focus	O
on	O
non-linear	O
models	O
,	O
but	O
we	O
assume	O
the	O
noise	O
is	O
gaussian	O
.	O
that	O
is	O
,	O
we	O
consider	O
models	O
of	O
the	O
form	O
zt	O
=	O
g	O
(	O
ut	O
,	O
zt−1	O
)	O
+n	O
(	O
0	O
,	O
qt	O
)	O
yt	O
=	O
h	O
(	O
zt	O
)	O
+n	O
(	O
0	O
,	O
rt	O
)	O
(	O
18.80	O
)	O
(	O
18.81	O
)	O
where	O
the	O
transition	B
model	I
g	O
and	O
the	O
observation	B
model	I
h	O
are	O
nonlinear	O
but	O
differentiable	O
functions	O
.	O
furthermore	O
,	O
we	O
focus	O
on	O
the	O
case	O
where	O
we	O
approximate	O
the	O
posterior	O
by	O
a	O
single	O
gaussian	O
.	O
(	O
the	O
simplest	O
way	O
to	O
handle	O
more	O
general	O
posteriors	O
(	O
e.g.	O
,	O
multi-modal	O
,	O
discrete	B
,	O
etc	O
)	O
.	O
is	O
to	O
use	O
particle	B
ﬁltering	I
,	O
which	O
we	O
discuss	O
in	O
section	O
23.5	O
.	O
)	O
the	O
extended	O
kalman	O
ﬁlter	O
or	O
ekf	O
can	O
be	O
applied	O
to	O
nonlinear	O
gaussian	O
dynamical	O
systems	O
of	O
this	O
form	O
.	O
the	O
basic	O
idea	O
is	O
to	O
linearize	O
g	O
and	O
h	O
about	O
the	O
previous	O
state	B
estimate	O
using	O
a	O
ﬁrst	O
order	O
taylor	O
series	O
expansion	O
,	O
and	O
then	O
to	O
apply	O
the	O
standard	O
kalman	O
ﬁlter	O
equations	O
.	O
(	O
the	O
noise	O
variance	O
in	O
the	O
equations	O
(	O
q	O
and	O
r	O
)	O
is	O
not	O
changed	O
,	O
i.e.	O
,	O
the	O
additional	O
error	O
due	O
to	O
linearization	O
is	O
not	O
modeled	O
.	O
)	O
thus	O
we	O
approximate	O
the	O
stationary	B
non-linear	O
dynamical	O
system	O
with	O
a	O
non-stationary	O
linear	B
dynamical	I
system	I
.	O
the	O
intuition	O
behind	O
the	O
approach	O
is	O
shown	O
in	O
figure	O
18.9	O
,	O
which	O
shows	O
what	O
happens	O
when	O
we	O
pass	O
a	O
gaussian	O
distribution	O
p	O
(	O
x	O
)	O
,	O
shown	O
on	O
the	O
bottom	O
right	O
,	O
through	O
a	O
nonlinear	O
function	O
y	O
=	O
g	O
(	O
x	O
)	O
,	O
shown	O
on	O
the	O
top	O
right	O
.	O
the	O
resulting	O
distribution	O
(	O
approximated	O
by	O
monte	O
carlo	O
)	O
is	O
shown	O
in	O
the	O
shaded	O
gray	O
area	O
in	O
the	O
top	O
left	O
corner	O
.	O
the	O
best	O
gaussian	O
approximation	O
to	O
this	O
,	O
computed	O
from	O
e	O
[	O
g	O
(	O
x	O
)	O
]	O
and	O
var	O
[	O
g	O
(	O
x	O
)	O
]	O
by	O
monte	O
carlo	O
,	O
is	O
shown	O
by	O
the	O
solid	O
black	O
line	O
.	O
the	O
ekf	O
approximates	O
this	O
gaussian	O
as	O
follows	O
:	O
it	O
linearizes	O
the	O
g	O
function	O
at	O
the	O
current	O
mode	B
,	O
μ	O
,	O
and	O
then	O
passes	O
the	O
gaussian	O
distribution	O
p	O
(	O
x	O
)	O
through	O
this	O
linearized	O
function	O
.	O
in	O
this	O
example	O
,	O
the	O
result	O
is	O
quite	O
a	O
good	O
approximation	O
to	O
the	O
ﬁrst	O
and	O
second	O
moments	O
of	O
p	O
(	O
y	O
)	O
,	O
for	O
much	O
less	O
cost	O
than	O
an	O
mc	O
approximation	O
.	O
in	O
more	O
detail	O
,	O
the	O
method	O
works	O
as	O
follows	O
.	O
we	O
approximate	O
the	O
measurement	O
model	O
using	O
p	O
(	O
yt|zt	O
)	O
≈	O
n	O
(	O
yt|h	O
(	O
μt|t−1	O
)	O
+h	O
t	O
(	O
yt	O
−	O
μt|t−1	O
)	O
,	O
rt	O
)	O
(	O
18.82	O
)	O
where	O
ht	O
is	O
the	O
jacobian	O
matrix	O
of	O
h	O
evaluated	O
at	O
the	O
prior	O
mode	B
:	O
hij	O
(	O
cid:2	O
)	O
∂hi	O
(	O
z	O
)	O
∂zj	O
ht	O
(	O
cid:2	O
)	O
h|z=μt|t−1	O
(	O
18.83	O
)	O
(	O
18.84	O
)	O
18.5.	O
approximate	O
online	O
inference	B
for	O
non-linear	O
,	O
non-gaussian	O
ssms	O
649	O
p	O
(	O
y	O
)	O
gaussian	O
of	O
p	O
(	O
y	O
)	O
mean	B
of	O
p	O
(	O
y	O
)	O
ekf	O
gaussian	O
mean	B
of	O
ekf	O
y	O
p	O
(	O
y	O
)	O
)	O
x	O
(	O
g	O
=	O
y	O
)	O
x	O
(	O
p	O
function	O
g	O
(	O
x	O
)	O
taylor	O
approx	O
.	O
mean	B
μ	O
g	O
(	O
μ	O
)	O
p	O
(	O
x	O
)	O
mean	B
μ	O
x	O
x	O
figure	O
18.9	O
nonlinear	O
transformation	O
of	O
a	O
gaussian	O
random	O
variable	O
.	O
the	O
prior	O
p	O
(	O
x	O
)	O
is	O
shown	O
on	O
the	O
bottom	O
right	O
.	O
the	O
function	O
y	O
=	O
g	O
(	O
x	O
)	O
is	O
shown	O
on	O
the	O
top	O
right	O
.	O
the	O
transformed	O
distribution	O
p	O
(	O
y	O
)	O
is	O
shown	O
in	O
the	O
top	O
left	O
.	O
a	O
linear	O
function	O
induces	O
a	O
gaussian	O
distribution	O
,	O
but	O
a	O
non-linear	O
function	O
induces	O
a	O
complex	O
distribution	O
.	O
the	O
solid	O
line	O
is	O
the	O
best	O
gaussian	O
approximation	O
to	O
this	O
;	O
the	O
dotted	O
line	O
is	O
the	O
ekf	O
approximation	O
to	O
this	O
.	O
source	O
:	O
figure	O
3.4	O
of	O
(	O
thrun	O
et	O
al	O
.	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
sebastian	O
thrun	O
.	O
similarly	O
,	O
we	O
approximate	O
the	O
system	O
model	O
using	O
p	O
(	O
zt|zt−1	O
,	O
ut	O
)	O
≈	O
n	O
(	O
zt|g	O
(	O
ut	O
,	O
μt−1	O
)	O
+g	O
t	O
(	O
zt−1	O
−	O
μt−1	O
)	O
,	O
qt	O
)	O
where	O
gij	O
(	O
u	O
)	O
(	O
cid:2	O
)	O
∂gi	O
(	O
u	O
,	O
z	O
)	O
∂zj	O
gt	O
(	O
cid:2	O
)	O
g	O
(	O
ut	O
)	O
|z=μt−1	O
so	O
g	O
is	O
the	O
jacobian	O
matrix	O
of	O
g	O
evaluated	O
at	O
the	O
prior	O
mode	B
.	O
given	O
this	O
,	O
we	O
can	O
then	O
apply	O
the	O
kalman	O
ﬁlter	O
to	O
compute	O
the	O
posterior	O
as	O
follows	O
:	O
μt|t−1	O
=	O
g	O
(	O
ut	O
,	O
μt−1	O
)	O
vt|t−1	O
=	O
gtvt−1gt	O
t	O
+	O
qt	O
t	O
(	O
htvt|t−1ht	O
kt	O
=	O
vt|t−1ht	O
μt	O
=	O
μt|t−1	O
+	O
kt	O
(	O
yt	O
−	O
h	O
(	O
μt|t−1	O
)	O
)	O
vt	O
=	O
(	O
i	O
−	O
ktht	O
)	O
vt|t−1	O
t	O
+	O
rt	O
)	O
−1	O
(	O
18.85	O
)	O
(	O
18.86	O
)	O
(	O
18.87	O
)	O
(	O
18.88	O
)	O
(	O
18.89	O
)	O
(	O
18.90	O
)	O
(	O
18.91	O
)	O
(	O
18.92	O
)	O
650	O
chapter	O
18.	O
state	B
space	I
models	O
actual	O
(	O
sampling	O
)	O
sigma-point	O
linearized	O
(	O
ekf	O
)	O
covariance	B
sigma	O
points	O
mean	B
(	O
)	O
=y	O
f	O
x	O
ϒ	O
=	O
f	O
i	O
(	O
)	O
iχ	O
y	O
=	O
true	O
mean	O
s-p	O
mean	B
f	O
x	O
(	O
)	O
y	O
a	O
p	O
=	O
p	O
at	O
x	O
true	O
covariance	O
s-p	O
covariance	B
transformed	O
sigma	B
points	I
a	O
xp	O
at	O
f	O
x	O
(	O
)	O
figure	O
18.10	O
an	O
example	O
of	O
the	O
unscented	B
transform	I
in	O
two	O
dimensions	O
.	O
source	O
:	O
(	O
wan	O
and	O
der	O
merwe	O
2001	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
eric	O
wan	O
.	O
we	O
see	O
that	O
the	O
only	O
difference	O
from	O
the	O
regular	B
kalman	O
ﬁlter	O
is	O
that	O
,	O
when	O
we	O
compute	O
the	O
state	B
prediction	O
,	O
we	O
use	O
g	O
(	O
ut	O
,	O
μt−1	O
)	O
instead	O
of	O
atμt−1	O
+	O
btut	O
,	O
and	O
when	O
we	O
compute	O
the	O
measurement	O
update	O
we	O
use	O
h	O
(	O
μt|t−1	O
)	O
instead	O
of	O
ctμt|t−1	O
.	O
it	O
is	O
possible	O
to	O
improve	O
performance	O
by	O
repeatedly	O
re-linearizing	O
the	O
equations	O
around	O
μt	O
instead	O
of	O
μt|t−1	O
;	O
this	O
is	O
called	O
the	O
iterated	O
ekf	O
,	O
and	O
yields	O
better	O
results	O
,	O
although	O
it	O
is	O
of	O
course	O
slower	O
.	O
there	O
are	O
two	O
cases	O
when	O
the	O
ekf	O
works	O
poorly	O
.	O
the	O
ﬁrst	O
is	O
when	O
the	O
prior	O
covariance	B
is	O
large	O
.	O
in	O
this	O
case	O
,	O
the	O
prior	O
distribution	O
is	O
broad	O
,	O
so	O
we	O
end	O
up	O
sending	O
a	O
lot	O
of	O
probability	O
mass	O
through	O
different	O
parts	O
of	O
the	O
function	O
that	O
are	O
far	O
from	O
the	O
mean	B
,	O
where	O
the	O
function	O
has	O
been	O
linearized	O
.	O
the	O
other	O
setting	O
where	O
the	O
ekf	O
works	O
poorly	O
is	O
when	O
the	O
function	O
is	O
highly	O
nonlinear	O
near	O
the	O
current	O
mean	B
.	O
in	O
section	O
18.5.2	O
,	O
we	O
will	O
discuss	O
an	O
algorithm	O
called	O
the	O
ukf	O
which	O
works	O
better	O
than	O
the	O
ekf	O
in	O
both	O
of	O
these	O
settings	O
.	O
18.5.2	O
unscented	O
kalman	O
ﬁlter	O
(	O
ukf	O
)	O
the	O
unscented	O
kalman	O
ﬁlter	O
(	O
ukf	O
)	O
is	O
a	O
better	O
version	O
of	O
the	O
ekf	O
(	O
julier	O
and	O
uhlmann	O
1997	O
)	O
.	O
(	O
apparently	O
it	O
is	O
so-called	O
because	O
it	O
“	O
doesn	O
’	O
t	O
stink	O
”	O
!	O
)	O
the	O
key	O
intuition	O
is	O
this	O
:	O
it	O
is	O
easier	O
to	O
approximate	O
a	O
gaussian	O
than	O
to	O
approximate	O
a	O
function	O
.	O
so	O
instead	O
of	O
performing	O
a	O
linear	O
approximation	O
to	O
the	O
function	O
,	O
and	O
passing	O
a	O
gaussian	O
through	O
it	O
,	O
instead	O
pass	O
a	O
deterministically	O
chosen	O
set	O
of	O
points	O
,	O
known	O
as	O
sigma	B
points	I
,	O
through	O
the	O
function	O
,	O
and	O
ﬁt	O
a	O
gaussian	O
to	O
the	O
resulting	O
transformed	O
points	O
.	O
this	O
is	O
known	O
as	O
the	O
unscented	B
transform	I
,	O
and	O
is	O
sketched	O
in	O
figure	O
18.10	O
.	O
(	O
we	O
explain	O
this	O
ﬁgure	O
in	O
detail	O
below	O
.	O
)	O
18.5.	O
approximate	O
online	O
inference	B
for	O
non-linear	O
,	O
non-gaussian	O
ssms	O
651	O
the	O
ukf	O
basically	O
uses	O
the	O
unscented	B
transform	I
twice	O
,	O
once	O
to	O
approximate	O
passing	O
through	O
the	O
system	O
model	O
g	O
,	O
and	O
once	O
to	O
approximate	O
passing	O
through	O
the	O
measurement	O
model	O
h.	O
we	O
give	O
the	O
details	O
below	O
.	O
note	O
that	O
the	O
ukf	O
and	O
ekf	O
both	O
perform	O
o	O
(	O
d3	O
)	O
operations	O
per	O
time	O
step	O
where	O
d	O
is	O
the	O
size	O
of	O
the	O
latent	B
state-space	O
.	O
however	O
,	O
the	O
ukf	O
is	O
accurate	O
to	O
at	O
least	O
second	O
order	O
,	O
whereas	O
the	O
ekf	O
is	O
only	O
a	O
ﬁrst	O
order	O
approximation	O
(	O
although	O
both	O
the	O
ekf	O
and	O
ukf	O
can	O
be	O
extended	O
to	O
capture	O
higher	O
order	O
terms	O
)	O
.	O
furthermore	O
,	O
the	O
unscented	B
transform	I
does	O
not	O
require	O
the	O
analytic	O
evaluation	O
of	O
any	O
derivatives	O
or	O
jacobians	O
(	O
a	O
so-called	O
derivative	B
free	I
ﬁlter	I
)	O
,	O
making	O
it	O
simpler	O
to	O
implement	O
and	O
more	O
widely	O
applicable	O
.	O
18.5.2.1	O
!	O
''	O
(	O
d	O
+	O
λ	O
)	O
σ	O
)	O
:	O
i	O
}	O
d	O
μ	O
,	O
{	O
μ	O
+	O
(	O
the	O
unscented	B
transform	I
before	O
explaining	O
the	O
ukf	O
,	O
we	O
ﬁrst	O
explain	O
the	O
unscented	B
transform	I
.	O
assume	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x|μ	O
,	O
σ	O
)	O
,	O
and	O
consider	O
estimating	O
p	O
(	O
y	O
)	O
,	O
where	O
y	O
=	O
f	O
(	O
x	O
)	O
for	O
some	O
nonlinear	O
function	O
f	O
.	O
the	O
unscented	B
transform	I
does	O
this	O
as	O
follows	O
.	O
first	O
we	O
create	O
a	O
set	O
of	O
2d	O
+	O
1	O
sigma	B
points	I
xi	O
,	O
given	O
by	O
i=1	O
,	O
{	O
μ	O
−	O
(	O
(	O
d	O
+	O
λ	O
)	O
σ	O
)	O
:	O
i	O
}	O
d	O
x	O
=	O
(	O
18.93	O
)	O
where	O
λ	O
=	O
α2	O
(	O
d	O
+	O
κ	O
)	O
−	O
d	O
is	O
a	O
scaling	O
parameter	B
to	O
be	O
speciﬁed	O
below	O
,	O
and	O
the	O
notation	O
m	O
:	O
i	O
means	O
the	O
i	O
’	O
th	O
column	O
of	O
matrix	O
m.	O
i=1	O
these	O
sigma	B
points	I
are	O
propagated	O
through	O
the	O
nonlinear	O
function	O
to	O
yield	O
yi	O
=	O
f	O
(	O
xi	O
)	O
,	O
and	O
''	O
#	O
the	O
mean	B
and	O
covariance	B
for	O
y	O
is	O
computed	O
as	O
follows	O
:	O
2d	O
(	O
cid:4	O
)	O
2d	O
(	O
cid:4	O
)	O
i=0	O
μy	O
=	O
σy	O
=	O
wi	O
myi	O
c	O
(	O
yi	O
−	O
μy	O
)	O
(	O
yi	O
−	O
μy	O
)	O
t	O
wi	O
i=0	O
where	O
the	O
w	O
’	O
s	O
are	O
weighting	O
terms	O
,	O
given	O
by	O
wi	O
m	O
=	O
wi	O
c	O
=	O
λ	O
d	O
+	O
λ	O
λ	O
d	O
+	O
λ	O
wi	O
m	O
=	O
wi	O
c	O
=	O
+	O
(	O
1−	O
α2	O
+	O
β	O
)	O
1	O
2	O
(	O
d	O
+	O
λ	O
)	O
(	O
18.94	O
)	O
(	O
18.95	O
)	O
(	O
18.96	O
)	O
(	O
18.97	O
)	O
(	O
18.98	O
)	O
see	O
figure	O
18.10	O
for	O
an	O
illustration	O
.	O
√	O
in	O
general	O
,	O
the	O
optimal	O
values	O
of	O
α	O
,	O
β	O
and	O
κ	O
are	O
problem	O
dependent	O
,	O
but	O
when	O
d	O
=	O
1	O
,	O
they	O
3σ	O
are	O
α	O
=	O
1	O
,	O
β	O
=	O
0	O
,	O
κ	O
=	O
2.	O
thus	O
in	O
the	O
1d	O
case	O
,	O
λ	O
=	O
2	O
,	O
so	O
the	O
3	O
sigma	B
points	I
are	O
μ	O
,	O
μ	O
+	O
and	O
μ	O
−	O
√	O
3σ	O
.	O
18.5.2.2	O
the	O
ukf	O
algorithm	O
the	O
ukf	O
algorithm	O
is	O
simply	O
two	O
applications	O
of	O
the	O
unscented	O
tranform	O
,	O
one	O
to	O
compute	O
p	O
(	O
zt|y1	O
:	O
t−1	O
,	O
u1	O
:	O
t	O
)	O
and	O
the	O
other	O
to	O
compute	O
p	O
(	O
zt|y1	O
:	O
t	O
,	O
u1	O
:	O
t	O
)	O
.	O
we	O
give	O
the	O
details	O
below	O
.	O
652	O
chapter	O
18.	O
state	B
space	I
models	O
the	O
ﬁrst	O
step	O
is	O
to	O
approximate	O
the	O
predictive	B
density	O
p	O
(	O
zt|y1	O
:	O
t−1	O
,	O
u1	O
:	O
t	O
)	O
≈	O
n	O
(	O
zt|μt	O
,	O
σt	O
)	O
by	O
passing	O
the	O
old	O
belief	B
state	I
n	O
(	O
zt−1|μt−1	O
,	O
σt−1	O
)	O
through	O
the	O
system	O
model	O
g	O
as	O
follows	O
:	O
''	O
#	O
σt−1	O
)	O
:	O
i	O
}	O
d	O
i=1	O
,	O
{	O
μt−1	O
−	O
γ	O
(	O
σt−1	O
)	O
:	O
i	O
}	O
d	O
i=1	O
''	O
z0	O
t−1	O
=	O
z∗i	O
t	O
=	O
g	O
(	O
ut	O
,	O
z0i	O
t−1	O
)	O
mz∗i	O
wi	O
μt	O
=	O
!	O
μt−1	O
,	O
{	O
μt−1	O
+	O
γ	O
(	O
2d	O
(	O
cid:4	O
)	O
2d	O
(	O
cid:4	O
)	O
i=0	O
t	O
c	O
(	O
z∗i	O
wi	O
t	O
−	O
μt	O
)	O
(	O
z∗i	O
t	O
−	O
μt	O
)	O
+q	O
t	O
σt	O
=	O
√	O
d	O
+	O
λ.	O
i=0	O
z0	O
t	O
=	O
y∗i	O
t	O
=	O
h	O
(	O
z0i	O
t	O
)	O
my∗i	O
wi	O
ˆyt	O
=	O
(	O
cid:8	O
)	O
μt	O
,	O
{	O
μt	O
+	O
γ	O
(	O
2d	O
(	O
cid:4	O
)	O
2d	O
(	O
cid:4	O
)	O
i=0	O
t	O
σz	O
,	O
y	O
t	O
=	O
t	O
−	O
μt	O
)	O
(	O
y∗i	O
t	O
−	O
ˆyt	O
)	O
t	O
2d	O
(	O
cid:4	O
)	O
i=0	O
c	O
(	O
z∗i	O
wi	O
t	O
s−1	O
t	O
kt	O
=	O
σz	O
,	O
y	O
μt	O
=	O
μt	O
+	O
kt	O
(	O
yt	O
−	O
ˆyt	O
)	O
σt	O
=	O
σt	O
−	O
ktstkt	O
t	O
(	O
18.99	O
)	O
(	O
18.100	O
)	O
(	O
18.101	O
)	O
(	O
18.102	O
)	O
(	O
18.103	O
)	O
(	O
18.104	O
)	O
(	O
18.105	O
)	O
(	O
18.107	O
)	O
(	O
18.108	O
)	O
(	O
18.109	O
)	O
(	O
18.110	O
)	O
the	O
second	O
step	O
is	O
to	O
approximate	O
the	O
likelihood	B
p	O
(	O
yt|zt	O
)	O
≈	O
n	O
(	O
yt|ˆyt	O
,	O
st	O
)	O
by	O
passing	O
the	O
where	O
γ	O
=	O
$	O
prior	O
n	O
(	O
zt|μt	O
,	O
σt	O
)	O
through	O
the	O
observation	B
model	I
h	O
:	O
$	O
(	O
cid:9	O
)	O
σt	O
)	O
:	O
i	O
}	O
d	O
i=1	O
,	O
{	O
μt	O
−	O
γ	O
(	O
σt	O
)	O
:	O
i	O
}	O
d	O
i=1	O
c	O
(	O
y∗i	O
wi	O
t	O
−	O
ˆyt	O
)	O
(	O
y∗i	O
t	O
−	O
ˆyt	O
)	O
t	O
+	O
rt	O
st	O
=	O
(	O
18.106	O
)	O
finally	O
,	O
we	O
use	O
bayes	O
rule	O
for	O
gaussians	O
to	O
get	O
the	O
posterior	O
p	O
(	O
zt|y1	O
:	O
t	O
,	O
u1	O
:	O
t	O
)	O
≈	O
n	O
(	O
zt|μt	O
,	O
σt	O
)	O
:	O
i=0	O
18.5.3	O
assumed	B
density	I
ﬁltering	I
(	O
adf	O
)	O
in	O
this	O
section	O
,	O
we	O
discuss	O
inference	B
where	O
we	O
perform	O
an	O
exact	O
update	O
step	O
,	O
but	O
then	O
approx-	O
imate	O
the	O
posterior	O
by	O
a	O
distribution	O
of	O
a	O
certain	O
convenient	O
form	O
,	O
such	O
as	O
a	O
gaussian	O
.	O
more	O
precisely	O
,	O
let	O
the	O
unknowns	O
that	O
we	O
want	O
to	O
infer	O
be	O
denoted	O
by	O
θt	O
.	O
suppose	O
that	O
q	O
is	O
a	O
set	O
of	O
tractable	O
distributions	O
,	O
e.g.	O
,	O
gaussians	O
with	O
a	O
diagonal	O
covariance	O
matrix	O
,	O
or	O
a	O
product	O
of	O
discrete	O
distributions	O
.	O
suppose	O
that	O
we	O
have	O
an	O
approximate	O
prior	O
qt−1	O
(	O
θt−1	O
)	O
≈	O
p	O
(	O
θt−1|y1	O
:	O
t−1	O
)	O
,	O
where	O
qt−1	O
∈	O
q.	O
we	O
can	O
update	O
this	O
with	O
the	O
new	O
measurement	O
to	O
get	O
the	O
approximate	O
posterior	O
ˆp	O
(	O
θt	O
)	O
=	O
1	O
zt	O
p	O
(	O
yt|θt	O
)	O
qt|t−1	O
(	O
θt	O
)	O
(	O
18.111	O
)	O
18.5.	O
approximate	O
online	O
inference	B
for	O
non-linear	O
,	O
non-gaussian	O
ssms	O
653	O
u	O
pdate	O
qt|t−1	O
predict	O
qt−1	O
u	O
pdate	O
qt+1|t	O
predict	O
ˆpt+1	O
p	O
r	O
o	O
j	O
e	O
c	O
t	O
qt+1	O
ˆpt	O
p	O
r	O
o	O
j	O
e	O
c	O
t	O
qt	O
(	O
a	O
)	O
θt−1	O
θt	O
st−1	O
st	O
yt−1	O
yt	O
xt−1	O
xt	O
(	O
b	O
)	O
figure	O
18.11	O
ical	O
logistic	B
regression	I
model	O
.	O
compare	O
to	O
figure	O
18.4	O
(	O
a	O
)	O
.	O
(	O
a	O
)	O
illustration	O
of	O
the	O
predict-update-project	B
cycle	O
of	O
assumed	B
density	I
ﬁltering	I
.	O
(	O
b	O
)	O
a	O
dynam-	O
(	O
cid:12	O
)	O
where	O
zt	O
=	O
p	O
(	O
yt|θt	O
)	O
qt|t−1	O
(	O
θt	O
)	O
dθt	O
(	O
cid:12	O
)	O
is	O
the	O
normalization	O
constant	O
and	O
qt|t−1	O
(	O
θt	O
)	O
=	O
p	O
(	O
θt|θt−1	O
)	O
qt−1	O
(	O
θt−1	O
)	O
dθt−1	O
(	O
18.112	O
)	O
(	O
18.113	O
)	O
(	O
18.114	O
)	O
is	O
the	O
one	O
step	O
ahead	O
predictive	B
distribution	O
.	O
if	O
the	O
prior	O
is	O
from	O
a	O
suitably	O
restricted	O
family	O
,	O
this	O
one-step	O
update	O
process	O
is	O
usually	O
tractable	O
.	O
however	O
,	O
we	O
often	O
ﬁnd	O
that	O
the	O
resulting	O
posterior	O
is	O
no	O
longer	O
in	O
our	O
tractable	O
family	O
,	O
ˆp	O
(	O
θt	O
)	O
(	O
cid:4	O
)	O
∈	O
q.	O
so	O
after	O
updating	O
we	O
seek	O
the	O
best	O
tractable	O
approximation	O
by	O
computing	O
q	O
(	O
θt	O
)	O
=	O
argmin	O
q∈q	O
kl	O
(	O
ˆp	O
(	O
θt	O
)	O
||q	O
(	O
θt	O
)	O
)	O
this	O
minimizes	O
the	O
the	O
kullback-leibler	O
divergence	O
(	O
section	O
2.8.2	O
)	O
from	O
the	O
approximation	O
q	O
(	O
θt	O
)	O
to	O
the	O
“	O
exact	O
”	O
posterior	O
ˆp	O
(	O
θt	O
)	O
,	O
and	O
can	O
be	O
thought	O
of	O
as	O
projecting	O
ˆp	O
onto	O
the	O
space	O
of	O
tractable	O
distributions	O
.	O
the	O
whole	O
algorithm	O
consists	O
of	O
predict-update-project	B
cycles	O
.	O
this	O
is	O
known	O
as	O
assumed	B
density	I
ﬁltering	I
or	O
adf	O
(	O
maybeck	O
1979	O
)	O
.	O
see	O
figure	O
18.11	O
(	O
a	O
)	O
for	O
a	O
sketch	O
.	O
if	O
q	O
is	O
in	O
the	O
exponential	B
family	I
,	O
one	O
can	O
show	O
that	O
this	O
kl	O
minimization	O
can	O
be	O
done	O
by	O
moment	B
matching	I
.	O
we	O
give	O
some	O
examples	O
of	O
this	O
below	O
.	O
18.5.3.1	O
boyen-koller	O
algorithm	O
for	O
online	O
inference	O
in	O
dbns	O
if	O
we	O
are	O
performing	O
inference	B
in	O
a	O
discrete-state	O
dynamic	O
bayes	O
net	O
(	O
section	O
17.6.7	O
)	O
,	O
where	O
θtj	O
(	O
cid:26	O
)	O
d	O
is	O
the	O
j	O
’	O
th	O
hidden	B
variable	I
at	O
time	O
t	O
,	O
then	O
the	O
exact	O
posterior	O
p	O
(	O
θt	O
)	O
becomes	O
intractable	O
to	O
compute	O
because	O
of	O
the	O
entanglement	B
problem	I
.	O
suppose	O
we	O
use	O
a	O
fully	O
factored	O
approximation	O
j=1	O
cat	O
(	O
θt	O
,	O
j|πt	O
,	O
j	O
)	O
,	O
where	O
πtjk	O
=	O
q	O
(	O
θt	O
,	O
j	O
=	O
k	O
)	O
is	O
the	O
probability	O
variable	O
of	O
the	O
form	O
q	O
(	O
θt	O
)	O
=	O
j	O
is	O
in	O
state	B
k	O
,	O
and	O
d	O
is	O
the	O
number	O
of	O
variables	O
.	O
in	O
this	O
case	O
,	O
the	O
moment	B
matching	I
operation	O
becomes	O
πtjk	O
=	O
ˆp	O
(	O
θt	O
,	O
j	O
=	O
k	O
)	O
(	O
18.115	O
)	O
654	O
chapter	O
18.	O
state	B
space	I
models	O
this	O
can	O
be	O
computed	O
by	O
performing	O
a	O
predict-update	O
step	O
using	O
the	O
factored	O
prior	O
,	O
and	O
then	O
computing	O
the	O
posterior	O
marginals	O
.	O
this	O
is	O
known	O
as	O
the	O
boyen-koller	O
algorithm	O
,	O
named	O
after	O
the	O
authors	O
of	O
(	O
boyen	O
and	O
koller	O
1998	O
)	O
,	O
who	O
demonstrated	O
that	O
the	O
error	O
incurred	O
by	O
this	O
series	O
of	O
repeated	O
approximations	O
remains	O
bounded	O
(	O
under	O
certain	O
assumptions	O
about	O
the	O
stochasticity	O
of	O
the	O
system	O
)	O
.	O
18.5.3.2	O
gaussian	O
approximation	O
for	O
online	O
inference	O
in	O
glms	O
(	O
cid:26	O
)	O
d	O
j=1	O
n	O
(	O
θt	O
,	O
j|μt	O
,	O
j	O
,	O
τt	O
,	O
j	O
)	O
,	O
where	O
τt	O
,	O
j	O
is	O
the	O
variance	B
.	O
then	O
the	O
optimal	O
now	O
suppose	O
q	O
(	O
θt	O
)	O
=	O
parameters	O
of	O
the	O
tractable	O
approximation	O
to	O
the	O
posterior	O
are	O
μt	O
,	O
j	O
=	O
e	O
ˆp	O
[	O
θt	O
,	O
j	O
]	O
,	O
τt	O
,	O
j	O
=	O
var	O
ˆp	O
[	O
θt	O
,	O
j	O
]	O
(	O
18.116	O
)	O
this	O
method	O
can	O
be	O
used	O
to	O
do	O
online	O
inference	B
for	O
the	O
parameters	O
of	O
many	O
statistical	O
models	O
.	O
for	O
example	O
,	O
thetrueskill	O
system	O
,	O
used	O
in	O
microsoft	O
’	O
s	O
xbox	O
to	O
rank	O
players	O
over	O
time	O
,	O
uses	O
this	O
form	O
of	O
approximation	O
(	O
herbrich	O
et	O
al	O
.	O
2007	O
)	O
.	O
we	O
can	O
also	O
apply	O
this	O
method	O
to	O
simpler	O
models	O
,	O
such	O
as	O
glm	O
,	O
which	O
have	O
the	O
advantage	O
that	O
the	O
posterior	O
is	O
log-concave	O
.	O
below	O
we	O
explain	O
how	O
to	O
do	O
this	O
for	O
binary	O
logistic	O
regression	B
,	O
following	O
the	O
presentation	O
of	O
(	O
zoeter	O
2007	O
)	O
.	O
the	O
model	O
has	O
the	O
form	O
p	O
(	O
yt|xt	O
,	O
θt	O
)	O
=	O
ber	O
(	O
yt|sigm	O
(	O
xt	O
p	O
(	O
θt|θt−1	O
)	O
=n	O
(	O
θt|θt−1	O
,	O
σ2i	O
)	O
t	O
θt	O
)	O
)	O
(	O
18.117	O
)	O
(	O
18.118	O
)	O
(	O
cid:26	O
)	O
where	O
σ2	O
is	O
some	O
process	O
noise	O
which	O
allows	O
the	O
parameters	O
to	O
change	O
slowly	O
over	O
time	O
.	O
(	O
this	O
can	O
be	O
set	O
to	O
0	O
,	O
as	O
in	O
the	O
recursive	B
least	I
squares	I
method	O
(	O
section	O
18.2.3	O
)	O
,	O
if	O
desired	O
.	O
)	O
we	O
will	O
j	O
n	O
(	O
θt−1	O
,	O
j|μt−1	O
,	O
j	O
,	O
τt−1	O
,	O
j	O
)	O
is	O
the	O
tractable	O
prior	O
.	O
we	O
can	O
compute	O
the	O
assume	O
qt−1	O
(	O
θt−1	O
)	O
=	O
one-step-ahead	B
predictive	I
density	I
qt|t−1	O
(	O
θt	O
)	O
using	O
the	O
standard	O
linear-gaussian	O
update	O
.	O
so	O
now	O
we	O
concentrate	O
on	O
the	O
measurement	O
update	O
step	O
.	O
deﬁne	O
the	O
deterministic	O
quantity	O
st	O
=	O
θt	O
if	O
qt|t−1	O
(	O
θt	O
)	O
=	O
j	O
n	O
(	O
θt	O
,	O
j|μt|t−1	O
,	O
j	O
,	O
τt|t−1	O
,	O
j	O
)	O
,	O
then	O
we	O
can	O
compute	O
the	O
predictive	B
distribution	O
for	O
st	O
as	O
follows	O
:	O
qt|t−1	O
(	O
st	O
)	O
=n	O
(	O
st|mt|t−1	O
,	O
vt|t−1	O
)	O
(	O
18.119	O
)	O
t	O
xt	O
,	O
as	O
shown	O
in	O
figure	O
18.11	O
(	O
b	O
)	O
.	O
(	O
cid:26	O
)	O
(	O
18.120	O
)	O
(	O
18.121	O
)	O
(	O
18.122	O
)	O
(	O
18.123	O
)	O
(	O
18.124	O
)	O
(	O
18.125	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
j	O
j	O
mt|t−1	O
=	O
vt|t−1	O
=	O
xt	O
,	O
jμt|t−1	O
,	O
j	O
x2	O
t	O
,	O
jτt|t−1	O
,	O
j	O
the	O
posterior	O
for	O
st	O
is	O
given	O
by	O
qt	O
(	O
st	O
)	O
=n	O
(	O
st|mt	O
,	O
vt	O
)	O
mt	O
=	O
vt	O
=	O
zt	O
=	O
st	O
1	O
zt	O
1	O
zt	O
p	O
(	O
yt|st	O
)	O
qt|t−1	O
(	O
st	O
)	O
dst	O
p	O
(	O
yt|st	O
)	O
qt|t−1	O
(	O
st	O
)	O
dst	O
−	O
m2	O
t	O
s2	O
t	O
p	O
(	O
yt|st	O
)	O
qt|t−1	O
(	O
st	O
)	O
dst	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
18.6.	O
hybrid	O
discrete/continuous	O
ssms	O
655	O
where	O
p	O
(	O
yt|st	O
)	O
=	O
ber	O
(	O
yt|st	O
)	O
.	O
these	O
integrals	O
are	O
one	O
dimensional	O
,	O
and	O
so	O
can	O
be	O
computed	O
using	O
gaussian	O
quadrature	O
(	O
see	O
(	O
zoeter	O
2007	O
)	O
for	O
details	O
)	O
.	O
this	O
is	O
the	O
same	O
as	O
one	O
step	O
of	O
the	O
ukf	O
algorithm	O
.	O
having	O
inferred	O
q	O
(	O
st	O
)	O
,	O
we	O
need	O
to	O
compute	O
q	O
(	O
θ|st	O
)	O
.	O
this	O
can	O
be	O
done	O
as	O
follows	O
.	O
deﬁne	O
δm	O
as	O
the	O
change	O
in	O
the	O
mean	B
of	O
st	O
and	O
δv	O
as	O
the	O
change	O
in	O
the	O
variance	B
:	O
mt	O
=	O
mt|t−1	O
+	O
δm	O
,	O
vt	O
=	O
vt|t−1	O
+	O
δv	O
(	O
18.126	O
)	O
then	O
one	O
can	O
show	O
that	O
the	O
new	O
factored	O
posterior	O
over	O
the	O
model	O
parameters	O
is	O
given	O
by	O
q	O
(	O
θt	O
,	O
j	O
)	O
=n	O
(	O
θt	O
,	O
j|μt	O
,	O
j	O
,	O
τt	O
,	O
j	O
)	O
μt	O
,	O
j	O
=	O
μt|t−1	O
,	O
j	O
+	O
ajδm	O
τt	O
,	O
j	O
=	O
τt|t−1	O
,	O
j	O
+	O
a2	O
j	O
δv	O
xt	O
,	O
jτt|t−1	O
,	O
j	O
aj	O
(	O
cid:2	O
)	O
j	O
(	O
cid:2	O
)	O
x2	O
(	O
cid:7	O
)	O
t	O
,	O
j	O
(	O
cid:2	O
)	O
τ	O
2	O
(	O
18.127	O
)	O
(	O
18.128	O
)	O
(	O
18.129	O
)	O
(	O
18.130	O
)	O
thus	O
we	O
see	O
that	O
the	O
parameters	O
which	O
correspond	O
to	O
inputs	O
with	O
larger	O
magnitude	O
(	O
big	O
|xt	O
,	O
j|	O
)	O
or	O
larger	O
uncertainty	B
(	O
big	O
τt|t−1	O
,	O
j	O
)	O
get	O
updated	O
most	O
,	O
which	O
makes	O
intuitive	O
sense	O
.	O
t|t−1	O
,	O
j	O
in	O
(	O
opper	O
1998	O
)	O
a	O
version	O
of	O
this	O
algorithm	O
is	O
derived	O
using	O
a	O
probit	B
likelihood	O
(	O
see	O
section	O
9.4	O
)	O
.	O
in	O
this	O
case	O
,	O
the	O
measurement	O
update	O
can	O
be	O
done	O
in	O
closed	O
form	O
,	O
without	O
the	O
need	O
for	O
numerical	O
in	O
either	O
case	O
,	O
the	O
algorithm	O
only	O
takes	O
o	O
(	O
d	O
)	O
operations	O
per	O
time	O
step	O
,	O
so	O
it	O
can	O
integration	O
.	O
be	O
applied	O
to	O
models	O
with	O
large	O
numbers	O
of	O
parameters	O
.	O
and	O
since	O
it	O
is	O
an	O
online	O
algorithm	O
,	O
it	O
can	O
also	O
handle	O
massive	O
datasets	O
.	O
for	O
example	O
(	O
zhang	O
et	O
al	O
.	O
2010	O
)	O
use	O
a	O
version	O
of	O
this	O
algorithm	O
to	O
ﬁt	O
a	O
multi-class	O
classiﬁer	O
online	O
to	O
very	O
large	O
datasets	O
.	O
they	O
beat	O
alternative	O
(	O
non	O
bayesian	O
)	O
online	B
learning	I
algorithms	O
,	O
and	O
sometimes	O
even	O
outperform	O
state	B
of	O
the	O
art	O
batch	B
(	O
offline	B
)	O
learning	B
methods	O
such	O
as	O
svms	O
(	O
described	O
in	O
section	O
14.5	O
)	O
.	O
18.6	O
hybrid	O
discrete/continuous	O
ssms	O
many	O
systems	O
contain	O
both	O
discrete	B
and	O
continuous	O
hidden	B
variables	I
;	O
these	O
are	O
known	O
as	O
hybrid	B
systems	I
.	O
for	O
example	O
,	O
the	O
discrete	B
variables	O
may	O
indicate	O
whether	O
a	O
measurement	O
sensor	O
is	O
faulty	O
or	O
not	O
,	O
or	O
which	O
“	O
regime	O
”	O
the	O
system	O
is	O
in	O
.	O
we	O
will	O
see	O
some	O
other	O
examples	O
below	O
.	O
a	O
special	O
case	O
of	O
a	O
hybrid	O
system	O
is	O
when	O
we	O
combine	O
an	O
hmm	O
and	O
an	O
lg-ssm	O
.	O
this	O
is	O
called	O
a	O
switching	B
linear	I
dynamical	I
system	I
(	O
slds	O
)	O
,	O
a	O
jump	O
markov	O
linear	O
system	O
(	O
jmls	O
)	O
,	O
or	O
a	O
switching	B
state	I
space	I
model	I
(	O
sssm	O
)	O
.	O
more	O
precisely	O
,	O
we	O
have	O
a	O
discrete	B
latent	O
variable	O
,	O
qt	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
a	O
continuous	O
latent	O
variable	O
,	O
zt	O
∈	O
r	O
l	O
,	O
an	O
continuous	O
observed	O
response	O
yt	O
∈	O
r	O
u	O
.	O
we	O
then	O
assume	O
that	O
the	O
continuous	O
variables	O
have	O
linear	O
gaussian	O
cpds	O
,	O
conditional	O
on	O
the	O
discrete	B
states	O
:	O
d	O
and	O
an	O
optional	O
continuous	O
observed	O
input	O
or	O
control	O
ut	O
∈	O
r	O
p	O
(	O
qt	O
=	O
k|qt−1	O
=	O
j	O
,	O
θ	O
)	O
=a	O
ij	O
p	O
(	O
zt|zt−1	O
,	O
qt	O
=	O
k	O
,	O
ut	O
,	O
θ	O
)	O
=n	O
(	O
zt|akzt−1	O
+	O
bkut	O
,	O
qk	O
)	O
p	O
(	O
yt|zt	O
,	O
qt	O
=	O
k	O
,	O
ut	O
,	O
θ	O
)	O
=n	O
(	O
yt|ckzt	O
+	O
dkut	O
,	O
rk	O
)	O
see	O
figure	O
18.12	O
(	O
a	O
)	O
for	O
the	O
dgm	O
representation	O
.	O
(	O
18.131	O
)	O
(	O
18.132	O
)	O
(	O
18.133	O
)	O
656	O
chapter	O
18.	O
state	B
space	I
models	O
ut−1	O
qt−1	O
ut	O
qt	O
zt−1	O
zt	O
yt−1	O
yt	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
18.12	O
a	O
switching	B
linear	I
dynamical	I
system	I
.	O
(	O
a	O
)	O
squares	O
represent	O
discrete	B
nodes	O
,	O
circles	O
represent	O
continuous	O
nodes	B
.	O
(	O
b	O
)	O
illustration	O
of	O
how	O
the	O
number	O
of	O
modes	O
in	O
the	O
belief	B
state	I
grows	O
exponentially	O
over	O
time	O
.	O
we	O
assume	O
there	O
are	O
two	O
binary	O
states	O
.	O
18.6.1	O
inference	B
unfortunately	O
inference	B
(	O
i.e.	O
,	O
state	B
estimation	I
)	O
in	O
hybrid	O
models	O
,	O
including	O
the	O
switching	O
lg-	O
is	O
intractable	O
.	O
to	O
see	O
why	O
,	O
suppose	O
qt	O
is	O
binary	O
,	O
but	O
that	O
only	O
the	O
dynamics	O
ssm	O
model	O
,	O
a	O
depend	O
on	O
qt	O
,	O
not	O
the	O
observation	B
matrix	O
.	O
our	O
initial	O
belief	B
state	I
will	O
be	O
a	O
mixture	O
of	O
2	O
gaussians	O
,	O
corresponding	O
to	O
p	O
(	O
z1|y1	O
,	O
q1	O
=	O
1	O
)	O
and	O
p	O
(	O
z1|y1	O
,	O
q1	O
=	O
2	O
)	O
.	O
the	O
one-step-ahead	B
predictive	I
density	I
will	O
be	O
a	O
mixture	O
of	O
4	O
gaussians	O
p	O
(	O
z2|y1	O
,	O
q1	O
=	O
1	O
,	O
q2	O
=	O
1	O
)	O
,	O
p	O
(	O
z2|y1	O
,	O
q1	O
=	O
1	O
,	O
q2	O
=	O
2	O
)	O
,	O
p	O
(	O
z2|y1	O
,	O
q1	O
=	O
2	O
,	O
q2	O
=	O
1	O
)	O
,	O
and	O
p	O
(	O
z2|y1	O
,	O
q1	O
=	O
2	O
,	O
q2	O
=	O
2	O
)	O
,	O
obtained	O
by	O
passing	O
each	O
of	O
the	O
prior	O
modes	O
through	O
the	O
2	O
possible	O
transition	O
models	O
.	O
the	O
belief	B
state	I
at	O
step	O
2	O
will	O
also	O
be	O
a	O
mixture	O
of	O
4	O
gaussians	O
,	O
obtained	O
by	O
updating	O
each	O
of	O
the	O
above	O
distributions	O
with	O
y2	O
.	O
at	O
step	O
3	O
,	O
the	O
belief	B
state	I
will	O
be	O
a	O
mixture	O
of	O
8	O
gaussians	O
.	O
and	O
so	O
on	O
.	O
so	O
we	O
see	O
there	O
is	O
an	O
exponential	O
explosion	O
in	O
the	O
number	O
of	O
modes	O
(	O
see	O
figure	O
18.12	O
(	O
b	O
)	O
)	O
.	O
various	O
approximate	B
inference	I
methods	O
have	O
been	O
proposed	O
for	O
this	O
model	O
,	O
such	O
as	O
the	O
following	O
:	O
•	O
prune	O
off	O
low	O
probability	O
trajectories	O
in	O
the	O
discrete	B
tree	O
;	O
this	O
is	O
the	O
basis	O
of	O
multiple	B
hypothesis	I
tracking	I
(	O
bar-shalom	O
and	O
fortmann	O
1988	O
;	O
bar-shalom	O
and	O
li	O
1993	O
)	O
.	O
•	O
use	O
monte	O
carlo	O
.	O
essentially	O
we	O
just	O
sample	O
discrete	O
trajectories	O
,	O
and	O
apply	O
an	O
analytical	O
ﬁlter	O
to	O
the	O
continuous	O
variables	O
conditional	O
on	O
a	O
trajectory	O
.	O
see	O
section	O
23.6	O
for	O
details	O
.	O
•	O
use	O
adf	O
,	O
where	O
we	O
approximate	O
the	O
exponentially	O
large	O
mixture	O
of	O
gaussians	O
with	O
a	O
smaller	O
mixture	O
of	O
gaussians	O
.	O
see	O
section	O
18.6.1.1	O
for	O
details	O
.	O
18.6.1.1	O
a	O
gaussian	O
sum	O
ﬁlter	O
for	O
switching	O
ssms	O
a	O
gaussian	O
sum	O
ﬁlter	O
(	O
sorenson	O
and	O
alspach	O
1971	O
)	O
approximates	O
the	O
belief	B
state	I
at	O
each	O
step	O
by	O
a	O
mixture	O
of	O
k	O
gaussians	O
.	O
this	O
can	O
be	O
implemented	O
by	O
running	O
k	O
kalman	O
ﬁlters	O
in	O
18.6.	O
hybrid	O
discrete/continuous	O
ssms	O
657	O
filter	O
1	O
-	O
b1,1	O
t	O
@	O
merge	O
-	O
b1	O
t	O
b	O
b	O
bbn	O
  	O
merge	O
-	O
b2	O
t	O
b1	O
t−1	O
b2	O
t−1	O
  	O
 	O
@	O
  	O
 	O
@	O
@	O
@	O
r	O
	O
	O
	O
b	O
	O
b	O
	O
@	O
@	O
r	O
filter	O
2	O
-	O
b1,2	O
t	O
b	O
filter	O
1	O
-	O
b2,1	O
	O
t	O
@	O
@	O
r	O
filter	O
2	O
-	O
b2,2	O
t	O
 	O
(	O
a	O
)	O
b1	O
t−1	O
b2	O
t−1	O
-	O
-	O
merge	O
-	O
˜b1	O
t−1	O
-	O
filter	O
1	O
-	O
b1	O
t	O
-	O
˜b2	O
t−1	O
-	O
filter	O
2	O
-	O
b2	O
t	O
figure	O
18.13	O
adf	O
for	O
a	O
switching	B
linear	I
dynamical	I
system	I
.	O
for	O
details	O
.	O
(	O
a	O
)	O
gpb2	O
method	O
.	O
(	O
b	O
)	O
imm	O
method	O
.	O
see	O
text	O
(	O
b	O
)	O
parallel	O
.	O
this	O
is	O
particularly	O
well	O
suited	O
to	O
switching	O
ssms	O
.	O
we	O
now	O
describe	O
one	O
version	O
of	O
this	O
algorithm	O
,	O
known	O
as	O
the	O
“	O
second	B
order	I
generalized	O
pseudo	O
bayes	O
ﬁlter	O
”	O
(	O
gpb2	O
)	O
(	O
bar-shalom	O
and	O
fortmann	O
1988	O
)	O
.	O
we	O
assume	O
that	O
the	O
prior	O
belief	B
state	I
bt−1	O
is	O
a	O
mixture	O
of	O
k	O
gaussians	O
,	O
one	O
per	O
discrete	B
state	O
:	O
t−1	O
(	O
cid:2	O
)	O
p	O
(	O
zt−1	O
,	O
qt−1	O
=	O
i|y1	O
:	O
t−1	O
)	O
=	O
πt−1	O
,	O
in	O
(	O
zt−1|μt−1	O
,	O
i	O
,	O
σt−1	O
,	O
i	O
)	O
bi	O
(	O
18.134	O
)	O
we	O
then	O
pass	O
this	O
through	O
the	O
k	O
different	O
linear	O
models	O
to	O
get	O
t	O
(	O
cid:2	O
)	O
p	O
(	O
zt	O
,	O
qt−1	O
=	O
i	O
,	O
qt	O
=	O
j|y1	O
:	O
t	O
)	O
=	O
πtijn	O
(	O
zt|μt	O
,	O
ij	O
,	O
σt	O
,	O
ij	O
)	O
bij	O
(	O
18.135	O
)	O
where	O
πtij	O
=	O
πt−1	O
,	O
ip	O
(	O
qt	O
=	O
j|qt−1	O
=	O
i	O
)	O
.	O
finally	O
,	O
for	O
each	O
value	O
of	O
j	O
,	O
we	O
collapse	O
the	O
k	O
gaussian	O
mixtures	O
down	O
to	O
a	O
single	O
mixture	O
to	O
give	O
t	O
(	O
cid:2	O
)	O
p	O
(	O
zt	O
,	O
qt	O
=	O
j|y1	O
:	O
t	O
)	O
=	O
πtjn	O
(	O
zt|μt	O
,	O
j	O
,	O
σt	O
,	O
j	O
)	O
bj	O
(	O
18.136	O
)	O
658	O
chapter	O
18.	O
state	B
space	I
models	O
see	O
figure	O
18.13	O
(	O
a	O
)	O
for	O
a	O
sketch	O
.	O
q	O
=	O
arg	O
minq	O
kl	O
(	O
q||p	O
)	O
,	O
where	O
p	O
(	O
z	O
)	O
=	O
be	O
solved	O
by	O
moment	B
matching	I
,	O
that	O
is	O
,	O
(	O
cid:7	O
)	O
the	O
optimal	O
way	O
to	O
approximate	O
a	O
mixture	O
of	O
gaussians	O
with	O
a	O
single	O
gaussian	O
is	O
given	O
by	O
k	O
πkn	O
(	O
z|μk	O
,	O
σk	O
)	O
and	O
q	O
(	O
z	O
)	O
=	O
n	O
(	O
z|μ	O
,	O
σ	O
)	O
.	O
this	O
can	O
μ	O
=	O
e	O
[	O
z	O
]	O
=	O
πkμk	O
k	O
σ	O
=	O
cov	O
[	O
z	O
]	O
=	O
(	O
cid:22	O
)	O
σk	O
+	O
(	O
μk	O
−	O
μ	O
)	O
(	O
μk	O
−	O
μ	O
)	O
t	O
(	O
cid:23	O
)	O
(	O
18.137	O
)	O
(	O
18.138	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
πk	O
k	O
in	O
the	O
graphical	B
model	I
literature	O
,	O
this	O
is	O
called	O
weak	B
marginalization	I
(	O
lauritzen	O
1992	O
)	O
,	O
since	O
it	O
preserves	O
the	O
ﬁrst	O
two	O
moments	O
.	O
applying	O
these	O
equations	O
to	O
our	O
model	O
,	O
we	O
can	O
go	O
from	O
bij	O
to	O
t	O
bj	O
t	O
as	O
follows	O
(	O
where	O
we	O
drop	O
the	O
t	O
subscript	O
for	O
brevity	O
)	O
:	O
πij	O
i	O
(	O
cid:4	O
)	O
πij	O
(	O
cid:7	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
i	O
j	O
(	O
cid:2	O
)	O
πij	O
(	O
cid:2	O
)	O
πj|iμij	O
(	O
cid:22	O
)	O
πj	O
=	O
πj|i	O
=	O
μj	O
=	O
σj	O
=	O
(	O
cid:23	O
)	O
(	O
18.139	O
)	O
(	O
18.140	O
)	O
(	O
18.141	O
)	O
(	O
18.142	O
)	O
πj|i	O
σij	O
+	O
(	O
μij	O
−	O
μj	O
)	O
(	O
μij	O
−	O
μj	O
)	O
t	O
18.6.2	O
i	O
this	O
algorithm	O
requires	O
running	O
k	O
2	O
ﬁlters	O
at	O
each	O
step	O
.	O
a	O
cheaper	O
alternative	O
is	O
to	O
represent	O
the	O
belief	B
state	I
by	O
a	O
single	O
gaussian	O
,	O
marginalizing	O
over	O
the	O
discrete	B
switch	O
at	O
each	O
step	O
.	O
this	O
is	O
a	O
straightforward	O
application	O
of	O
adf	O
.	O
an	O
offline	B
extension	O
to	O
this	O
method	O
,	O
called	O
expectation	B
correction	I
,	O
is	O
described	O
in	O
(	O
barber	O
2006	O
;	O
mesot	O
and	O
barber	O
2009	O
)	O
.	O
another	O
heuristic	O
approach	O
,	O
known	O
as	O
interactive	B
multiple	I
models	I
or	O
imm	O
(	O
bar-shalom	O
and	O
fortmann	O
1988	O
)	O
,	O
can	O
be	O
obtained	O
by	O
ﬁrst	O
collapsing	O
the	O
prior	O
to	O
a	O
single	O
gaussian	O
(	O
by	O
moment	B
matching	I
)	O
,	O
and	O
then	O
updating	O
it	O
using	O
k	O
different	O
kalman	O
ﬁlters	O
,	O
one	O
per	O
value	O
of	O
qt	O
.	O
see	O
figure	O
18.13	O
(	O
b	O
)	O
for	O
a	O
sketch	O
.	O
application	O
:	O
data	B
association	I
and	O
multi-target	B
tracking	I
suppose	O
we	O
are	O
tracking	B
k	O
objects	O
,	O
such	O
as	O
airplanes	O
,	O
and	O
at	O
time	O
t	O
,	O
we	O
observe	O
k	O
(	O
cid:2	O
)	O
detection	O
events	O
,	O
e.g.	O
,	O
“	O
blips	O
”	O
on	O
a	O
radar	B
screen	O
.	O
we	O
can	O
have	O
k	O
(	O
cid:2	O
)	O
<	O
k	O
due	O
to	O
occlusion	O
or	O
missed	O
detections	O
.	O
we	O
can	O
have	O
k	O
(	O
cid:2	O
)	O
>	O
k	O
due	O
to	O
clutter	O
or	O
false	O
alarms	O
.	O
or	O
we	O
can	O
have	O
k	O
(	O
cid:2	O
)	O
=	O
k.	O
in	O
any	O
case	O
,	O
we	O
need	O
to	O
ﬁgure	O
out	O
the	O
correspondence	B
between	O
the	O
k	O
(	O
cid:2	O
)	O
detections	O
ytk	O
and	O
the	O
k	O
objects	O
ztj	O
.	O
this	O
is	O
called	O
the	O
problem	O
of	O
data	B
association	I
,	O
and	O
it	O
arises	O
in	O
many	O
application	O
domains	O
.	O
figure	O
18.14	O
gives	O
an	O
example	O
in	O
which	O
we	O
are	O
tracking	B
k	O
=	O
2	O
objects	O
.	O
at	O
each	O
time	O
step	O
,	O
qt	O
is	O
the	O
unknown	B
mapping	O
which	O
speciﬁes	O
which	O
objects	O
caused	O
which	O
observations	O
.	O
it	O
speciﬁes	O
the	O
“	O
wiring	O
diagram	O
”	O
for	O
time	O
slice	O
t.	O
the	O
standard	O
way	O
to	O
solve	O
this	O
problem	O
is	O
to	O
compute	O
a	O
weight	O
which	O
measures	O
the	O
“	O
compatibility	O
”	O
between	O
object	O
j	O
and	O
measurement	O
k	O
,	O
typically	O
based	O
on	O
how	O
close	O
k	O
is	O
to	O
where	O
the	O
model	O
thinks	O
j	O
should	O
be	O
(	O
the	O
so-called	O
nearest	B
neighbor	I
data	I
association	I
heuristic	O
)	O
.	O
this	O
gives	O
us	O
a	O
k	O
×	O
k	O
(	O
cid:2	O
)	O
weight	O
matrix	O
.	O
we	O
can	O
make	O
this	O
into	O
a	O
18.6.	O
hybrid	O
discrete/continuous	O
ssms	O
659	O
zt−1,1	O
zt−1,2	O
yt−1,1	O
yt−1,2	O
yt−1,3	O
zt,1	O
zt,2	O
yt,1	O
zt+1,1	O
zt+1,2	O
yt+1,1	O
yt+1,2	O
qt−1	O
qt	O
qt+1	O
figure	O
18.14	O
a	O
model	O
for	O
tracking	B
two	O
objects	O
in	O
the	O
presence	O
of	O
data-assocation	O
ambiguity	O
.	O
we	O
observe	O
3	O
,	O
1	O
and	O
2	O
detections	O
in	O
the	O
ﬁrst	O
three	O
time	O
steps	O
.	O
square	O
matrix	O
of	O
size	O
n	O
×	O
n	O
,	O
wheren	O
=	O
max	O
(	O
k	O
,	O
k	O
(	O
cid:2	O
)	O
)	O
,	O
by	O
adding	O
dummy	O
background	O
objects	O
,	O
which	O
can	O
explain	O
all	O
the	O
false	O
alarms	O
,	O
and	O
adding	O
dummy	O
observations	O
,	O
which	O
can	O
explain	O
all	O
the	O
missed	O
detections	O
.	O
we	O
can	O
then	O
compute	O
the	O
maximal	B
weight	I
bipartite	I
matching	I
using	O
the	O
hungarian	O
algorithm	O
,	O
which	O
takes	O
o	O
(	O
n	O
3	O
)	O
time	O
(	O
see	O
e.g.	O
,	O
(	O
burkard	O
et	O
al	O
.	O
2009	O
)	O
)	O
.	O
conditional	O
on	O
this	O
,	O
we	O
can	O
perform	O
a	O
kalman	O
ﬁlter	O
update	O
,	O
where	O
objects	O
that	O
are	O
assigned	O
to	O
dummy	O
observations	O
do	O
not	O
perform	O
a	O
measurement	O
update	O
.	O
an	O
extension	B
of	O
this	O
method	O
,	O
to	O
handle	O
a	O
variable	O
and/or	O
unknown	B
number	O
of	O
objects	O
,	O
is	O
known	O
as	O
multi-target	B
tracking	I
.	O
this	O
requires	O
dealing	O
with	O
a	O
variable-sized	O
state	B
space	I
.	O
there	O
are	O
many	O
ways	O
to	O
do	O
this	O
,	O
but	O
perhaps	O
the	O
simplest	O
and	O
most	O
robust	B
methods	O
are	O
based	O
on	O
sequential	B
monte	O
carlo	O
(	O
e.g.	O
,	O
(	O
ristic	O
et	O
al	O
.	O
2004	O
)	O
)	O
or	O
mcmc	O
(	O
e.g.	O
,	O
(	O
khan	O
et	O
al	O
.	O
2006	O
;	O
oh	O
et	O
al	O
.	O
2009	O
)	O
)	O
.	O
18.6.3	O
application	O
:	O
fault	B
diagnosis	I
consider	O
the	O
model	O
in	O
figure	O
18.15	O
(	O
a	O
)	O
.	O
this	O
represents	O
an	O
industrial	O
plant	O
consisting	O
of	O
various	O
tanks	O
of	O
liquid	O
,	O
interconnected	O
by	O
pipes	O
.	O
in	O
this	O
example	O
,	O
we	O
just	O
have	O
two	O
tanks	O
,	O
for	O
simplicity	O
.	O
we	O
want	O
to	O
estimate	O
the	O
pressure	O
inside	O
each	O
tank	O
,	O
based	O
on	O
a	O
noisy	O
measurement	O
of	O
the	O
ﬂow	O
into	O
and	O
out	O
of	O
each	O
tank	O
.	O
however	O
,	O
the	O
measurement	O
devices	O
can	O
sometimes	O
fail	O
.	O
furthermore	O
,	O
pipes	O
can	O
burst	O
or	O
get	O
blocked	O
;	O
we	O
call	O
this	O
a	O
“	O
resistance	O
failure	O
”	O
.	O
this	O
model	O
is	O
widely	O
used	O
as	O
a	O
benchmark	O
in	O
the	O
fault	B
diagnosis	I
community	O
(	O
mosterman	O
and	O
biswas	O
1999	O
)	O
.	O
we	O
can	O
create	O
a	O
probabilistic	O
model	O
of	O
the	O
system	O
as	O
shown	O
in	O
figure	O
18.15	O
(	O
b	O
)	O
.	O
the	O
square	O
nodes	O
represent	O
discrete	B
variables	O
,	O
such	O
as	O
measurement	O
failures	O
and	O
resistance	O
failures	O
.	O
the	O
remaining	O
variables	O
are	O
continuous	O
.	O
a	O
variety	O
of	O
approximate	B
inference	I
algorithms	O
can	O
be	O
applied	O
to	O
this	O
model	O
.	O
see	O
(	O
koller	O
and	O
lerner	O
2001	O
)	O
for	O
one	O
approach	O
,	O
based	O
on	O
rao-blackwellized	O
particle	B
ﬁltering	I
(	O
which	O
is	O
explained	O
in	O
section	O
23.6	O
)	O
.	O
660	O
chapter	O
18.	O
state	B
space	I
models	O
rf	O
1	O
1	O
r1	O
1	O
rf	O
1	O
2	O
r1	O
2	O
m	O
f	O
1	O
1	O
m	O
f	O
1	O
2	O
f	O
1	O
1	O
m	O
1	O
1	O
f	O
1	O
2	O
m	O
1	O
2	O
p	O
1	O
1	O
p	O
1	O
2	O
m	O
f	O
12	O
1	O
m	O
f	O
12	O
2	O
f	O
12	O
1	O
m	O
12	O
1	O
f	O
12	O
2	O
m	O
12	O
2	O
r12	O
1	O
rf	O
12	O
1	O
p	O
2	O
1	O
r2	O
1	O
rf	O
2	O
1	O
r12	O
2	O
rf	O
12	O
2	O
p	O
2	O
2	O
f	O
2	O
1	O
m	O
2	O
1	O
f	O
2	O
2	O
m	O
2	O
2	O
m	O
f	O
2	O
1	O
m	O
f	O
2	O
2	O
r2	O
2	O
rf	O
2	O
2	O
(	O
b	O
)	O
(	O
a	O
)	O
figure	O
18.15	O
(	O
a	O
)	O
the	O
two-tank	O
system	O
.	O
the	O
goal	O
is	O
to	O
infer	O
when	O
pipes	O
are	O
blocked	O
or	O
have	O
burst	O
,	O
or	O
sensors	O
have	O
broken	O
,	O
from	O
(	O
noisy	O
)	O
observations	O
of	O
the	O
ﬂow	O
out	O
of	O
tank	O
1	O
,	O
f	O
1o	O
,	O
out	O
of	O
tank	O
2	O
,	O
f	O
2o	O
,	O
or	O
between	O
tanks	O
1	O
and	O
2	O
,	O
f	O
12.	O
r1o	O
is	O
a	O
hidden	B
variable	I
representing	O
the	O
resistance	O
of	O
the	O
pipe	O
out	O
of	O
tank	O
1	O
,	O
p	O
1	O
is	O
a	O
hidden	B
variable	I
representing	O
the	O
pressure	O
in	O
tank	O
1	O
,	O
etc	O
.	O
source	O
:	O
figure	O
11	O
of	O
(	O
koller	O
and	O
lerner	O
2001	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
.	O
(	O
b	O
)	O
dynamic	O
bayes	O
net	O
representation	O
of	O
the	O
two-tank	O
system	O
.	O
discrete	B
nodes	O
are	O
squares	O
,	O
continuous	O
nodes	B
are	O
circles	O
.	O
abbreviations	O
:	O
r	O
=	O
resistance	O
,	O
p	O
=	O
pressure	O
,	O
f	O
=	O
ﬂow	O
,	O
m	O
=	O
measurement	O
,	O
rf	O
=	O
resistance	O
failure	O
,	O
mf	O
=	O
measurement	O
failure	O
.	O
based	O
on	O
figure	O
12	O
of	O
(	O
koller	O
and	O
lerner	O
2001	O
)	O
.	O
18.6.4	O
application	O
:	O
econometric	B
forecasting	I
the	O
switching	O
lg-ssm	O
model	O
is	O
widely	O
used	O
in	O
econometric	B
forecasting	I
,	O
where	O
it	O
is	O
called	O
a	O
regime	B
switching	I
model	O
.	O
for	O
example	O
,	O
we	O
can	O
combine	O
two	O
linear	B
trend	I
models	O
(	O
see	O
sec-	O
tion	O
18.2.4.2	O
)	O
,	O
one	O
in	O
which	O
bt	O
>	O
0	O
reﬂects	O
a	O
growing	O
economy	O
,	O
and	O
one	O
in	O
which	O
bt	O
<	O
0	O
reﬂects	O
a	O
shrinking	O
economy	O
.	O
see	O
(	O
west	O
and	O
harrison	O
1997	O
)	O
for	O
further	O
details	O
.	O
exercises	O
exercise	O
18.1	O
derivation	O
of	O
em	O
for	O
lg-ssm	O
derive	O
the	O
e	O
and	O
m	O
steps	O
for	O
computing	O
a	O
(	O
locally	O
optimal	O
)	O
mle	O
for	O
an	O
lg-ssm	O
model	O
.	O
hint	O
:	O
the	O
results	O
are	O
in	O
(	O
ghahramani	O
and	O
hinton	O
1996b	O
)	O
;	O
your	O
task	O
is	O
to	O
derive	O
these	O
results	O
.	O
exercise	O
18.2	O
seasonal	O
lg-ssm	O
model	O
in	O
standard	O
form	O
write	O
the	O
seasonal	O
model	O
in	O
figure	O
18.7	O
(	O
a	O
)	O
as	O
an	O
lg-ssm	O
.	O
deﬁne	O
the	O
matrices	O
a	O
,	O
c	O
,	O
q	O
and	O
r.	O
19	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
19.1	O
introduction	O
in	O
chapter	O
10	O
,	O
we	O
discussed	O
directed	O
graphical	O
models	O
(	O
dgms	O
)	O
,	O
commonly	O
known	O
as	O
bayes	O
nets	O
.	O
however	O
,	O
for	O
some	O
domains	O
,	O
being	O
forced	O
to	O
choose	O
a	O
direction	O
for	O
the	O
edges	B
,	O
as	O
required	O
by	O
a	O
dgm	O
,	O
is	O
rather	O
awkward	O
.	O
for	O
example	O
,	O
consider	O
modeling	O
an	O
image	O
.	O
we	O
might	O
suppose	O
that	O
the	O
intensity	O
values	O
of	O
neighboring	O
pixels	O
are	O
correlated	O
.	O
we	O
can	O
create	O
a	O
dag	O
model	O
with	O
a	O
2d	O
lattice	B
topology	O
as	O
shown	O
in	O
figure	O
19.1	O
(	O
a	O
)	O
.	O
this	O
is	O
known	O
as	O
a	O
causal	O
mrf	O
or	O
a	O
markov	O
mesh	O
(	O
abend	O
et	O
al	O
.	O
1965	O
)	O
.	O
however	O
,	O
its	O
conditional	B
independence	I
properties	O
are	O
rather	O
unnatural	O
.	O
in	O
particular	O
,	O
the	O
markov	O
blanket	O
(	O
deﬁned	O
in	O
section	O
10.5	O
)	O
of	O
the	O
node	O
x8	O
in	O
the	O
middle	O
is	O
the	O
other	O
colored	O
nodes	B
(	O
3	O
,	O
4	O
,	O
7	O
,	O
9	O
,	O
12	O
and	O
13	O
)	O
rather	O
than	O
just	O
its	O
4	O
nearest	O
neighbors	O
as	O
one	O
might	O
expect	O
.	O
an	O
alternative	O
is	O
to	O
use	O
an	O
undirected	B
graphical	I
model	I
(	O
ugm	O
)	O
,	O
also	O
called	O
a	O
markov	O
random	O
ﬁeld	O
(	O
mrf	O
)	O
or	O
markov	O
network	O
.	O
these	O
do	O
not	O
require	O
us	O
to	O
specify	O
edge	O
orientations	O
,	O
and	O
are	O
much	O
more	O
natural	O
for	O
some	O
problems	O
such	O
as	O
image	O
analysis	O
and	O
spatial	O
statistics	O
.	O
for	O
example	O
,	O
an	O
undirected	B
2d	O
lattice	B
is	O
shown	O
in	O
figure	O
19.1	O
(	O
b	O
)	O
;	O
now	O
the	O
markov	O
blanket	O
of	O
each	O
node	O
is	O
just	O
its	O
nearest	O
neighbors	O
,	O
as	O
we	O
show	O
in	O
section	O
19.2.	O
roughly	O
speaking	O
,	O
the	O
main	O
advantages	O
of	O
ugms	O
over	O
dgms	O
are	O
:	O
(	O
1	O
)	O
they	O
are	O
symmetric	B
and	O
therefore	O
more	O
“	O
natural	O
”	O
for	O
certain	O
domains	O
,	O
such	O
as	O
spatial	O
or	O
relational	O
data	O
;	O
and	O
(	O
2	O
)	O
discrimi-	O
nativel	O
ugms	O
(	O
aka	O
conditional	B
random	I
ﬁelds	I
,	O
or	O
crfs	O
)	O
,	O
which	O
deﬁne	O
conditional	O
densities	O
of	O
the	O
form	O
p	O
(	O
y|x	O
)	O
,	O
work	O
better	O
than	O
discriminative	B
dgms	O
,	O
for	O
reasons	O
we	O
explain	O
in	O
section	O
19.6.1.	O
the	O
main	O
disadvantages	O
of	O
ugms	O
compared	O
to	O
dgms	O
are	O
:	O
(	O
1	O
)	O
the	O
parameters	O
are	O
less	O
interpretable	O
and	O
less	O
modular	O
,	O
for	O
reasons	O
we	O
explain	O
in	O
section	O
19.3	O
;	O
and	O
(	O
2	O
)	O
parameter	B
estimation	O
is	O
com-	O
putationally	O
more	O
expensive	O
,	O
for	O
reasons	O
we	O
explain	O
in	O
section	O
19.5.	O
see	O
(	O
domke	O
et	O
al	O
.	O
2008	O
)	O
for	O
an	O
empirical	O
comparison	O
of	O
the	O
two	O
approaches	O
for	O
an	O
image	O
processing	O
task	O
.	O
19.2	O
conditional	B
independence	I
properties	O
of	O
ugms	O
19.2.1	O
key	O
properties	O
ugms	O
deﬁne	O
ci	O
relationships	O
via	O
simple	O
graph	O
separation	O
as	O
follows	O
:	O
for	O
sets	O
of	O
nodes	B
a	O
,	O
b	O
,	O
and	O
c	O
,	O
we	O
say	O
xa	O
⊥g	O
xb|xc	O
iff	B
c	O
separates	O
a	O
from	O
b	O
in	O
the	O
graph	B
g.	O
this	O
means	O
that	O
,	O
when	O
we	O
remove	O
all	O
the	O
nodes	B
in	O
c	O
,	O
if	O
there	O
are	O
no	O
paths	O
connecting	O
any	O
node	O
in	O
a	O
to	O
any	O
node	O
in	O
b	O
,	O
then	O
the	O
ci	O
property	O
holds	O
.	O
this	O
is	O
called	O
the	O
global	O
markov	O
property	O
for	O
ugms	O
.	O
for	O
example	O
,	O
in	O
figure	O
19.2	O
(	O
b	O
)	O
,	O
we	O
have	O
that	O
{	O
1	O
,	O
2	O
}	O
⊥	O
{	O
6	O
,	O
7	O
}	O
|	O
{	O
3	O
,	O
4	O
,	O
5	O
}	O
.	O
662	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
x1	O
x2	O
x3	O
x4	O
x5	O
x1	O
x2	O
x3	O
x4	O
x5	O
x6	O
x7	O
x8	O
x9	O
x10	O
x6	O
x7	O
x8	O
x9	O
x10	O
x11	O
x12	O
x13	O
x14	O
x15	O
x11	O
x12	O
x13	O
x14	O
x15	O
x16	O
x17	O
x18	O
x19	O
x20	O
x16	O
x17	O
x18	O
x19	O
x20	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
a	O
)	O
a	O
2d	O
lattice	B
represented	O
as	O
a	O
dag	O
.	O
the	O
dotted	O
red	O
node	O
x8	O
is	O
independent	O
of	O
all	O
other	O
figure	O
19.1	O
nodes	B
(	O
black	O
)	O
given	O
its	O
markov	O
blanket	O
,	O
which	O
include	O
its	O
parents	B
(	O
blue	O
)	O
,	O
children	B
(	O
green	O
)	O
and	O
co-parents	B
(	O
orange	O
)	O
.	O
(	O
b	O
)	O
the	O
same	O
model	O
represented	O
as	O
a	O
ugm	O
.	O
the	O
red	O
node	O
x8	O
is	O
independent	O
of	O
the	O
other	O
black	O
nodes	O
given	O
its	O
neighbors	B
(	O
blue	O
nodes	B
)	O
.	O
1	O
2	O
3	O
4	O
6	O
5	O
(	O
a	O
)	O
7	O
1	O
2	O
3	O
5	O
(	O
b	O
)	O
4	O
6	O
7	O
figure	O
19.2	O
(	O
a	O
)	O
a	O
dgm	O
.	O
(	O
b	O
)	O
its	O
moralized	O
version	O
,	O
represented	O
as	O
a	O
ugm	O
.	O
the	O
set	O
of	O
nodes	O
that	O
renders	O
a	O
node	O
t	O
conditionally	O
independent	O
of	O
all	O
the	O
other	O
nodes	B
in	O
the	O
graph	B
is	O
called	O
t	O
’	O
s	O
markov	O
blanket	O
;	O
we	O
will	O
denote	O
this	O
by	O
mb	O
(	O
t	O
)	O
.	O
formally	O
,	O
the	O
markov	O
blanket	O
satisﬁes	O
the	O
following	O
property	O
:	O
t	O
⊥	O
v	O
\	O
cl	O
(	O
t	O
)	O
|mb	O
(	O
t	O
)	O
(	O
19.1	O
)	O
where	O
cl	O
(	O
t	O
)	O
(	O
cid:2	O
)	O
mb	O
(	O
t	O
)	O
∪	O
{	O
t	O
}	O
is	O
the	O
closure	B
of	O
node	O
t.	O
one	O
can	O
show	O
that	O
,	O
in	O
a	O
ugm	O
,	O
a	O
node	O
’	O
s	O
markov	O
blanket	O
is	O
its	O
set	O
of	O
immediate	O
neighbors	B
.	O
this	O
is	O
called	O
the	O
undirected	O
local	O
markov	O
property	O
.	O
for	O
example	O
,	O
in	O
figure	O
19.2	O
(	O
b	O
)	O
,	O
we	O
have	O
mb	O
(	O
5	O
)	O
=	O
{	O
2	O
,	O
3	O
,	O
4	O
,	O
6	O
}	O
.	O
from	O
the	O
local	O
markov	O
property	O
,	O
we	O
can	O
easily	O
see	O
that	O
two	O
nodes	B
are	O
conditionally	O
indepen-	O
dent	O
given	O
the	O
rest	O
if	O
there	O
is	O
no	O
direct	O
edge	O
between	O
them	O
.	O
this	O
is	O
called	O
the	O
pairwise	O
markov	O
property	O
.	O
in	O
symbols	O
,	O
this	O
is	O
written	O
as	O
s	O
⊥	O
t|v	O
\	O
{	O
s	O
,	O
t	O
}	O
⇐⇒g	O
st	O
=	O
0	O
using	O
the	O
three	O
markov	O
properties	O
we	O
have	O
discussed	O
,	O
we	O
can	O
derive	O
the	O
following	O
ci	O
properties	O
(	O
19.2	O
)	O
(	O
amongst	O
others	O
)	O
from	O
the	O
ugm	O
in	O
figure	O
19.2	O
(	O
b	O
)	O
:	O
•	O
pairwise	O
1	O
⊥	O
7|rest	O
•	O
local	O
1	O
⊥	O
rest|2	O
,	O
3	O
19.2.	O
conditional	B
independence	I
properties	O
of	O
ugms	O
663	O
g	O
l	O
p	O
p	O
(	O
x	O
)	O
>	O
0	O
figure	O
19.3	O
relationship	O
between	O
markov	O
properties	O
of	O
ugms	O
.	O
4	O
5	O
1	O
1	O
2	O
3	O
(	O
a	O
)	O
4	O
2	O
3	O
5	O
(	O
b	O
)	O
(	O
a	O
)	O
the	O
ancestral	B
graph	I
induced	O
by	O
the	O
dag	O
in	O
figure	O
19.2	O
(	O
a	O
)	O
wrt	O
u	O
=	O
{	O
2	O
,	O
4	O
,	O
5	O
}	O
.	O
figure	O
19.4	O
moralized	O
version	O
of	O
(	O
a	O
)	O
.	O
(	O
b	O
)	O
the	O
•	O
global	O
1	O
,	O
2	O
⊥	O
6	O
,	O
7|3	O
,	O
4	O
,	O
5	O
it	O
is	O
obvious	O
that	O
global	O
markov	O
implies	O
local	O
markov	O
which	O
implies	O
pairwise	O
markov	O
.	O
what	O
is	O
less	O
obvious	O
,	O
but	O
nevertheless	O
true	O
(	O
assuming	O
p	O
(	O
x	O
)	O
>	O
0	O
for	O
all	O
x	O
,	O
i.e.	O
,	O
that	O
p	O
is	O
a	O
positive	O
density	O
)	O
,	O
is	O
that	O
pairwise	O
implies	O
global	O
,	O
and	O
hence	O
that	O
all	O
these	O
markov	O
properties	O
are	O
the	O
same	O
,	O
as	O
illustrated	O
in	O
figure	O
19.3	O
(	O
see	O
e.g.	O
,	O
(	O
koller	O
and	O
friedman	O
2009	O
,	O
p119	O
)	O
for	O
a	O
proof	O
)	O
.1	O
the	O
importance	O
of	O
this	O
result	O
is	O
that	O
it	O
is	O
usually	O
easier	O
to	O
empirically	O
assess	O
pairwise	O
conditional	O
independence	O
;	O
such	O
pairwise	O
ci	O
statements	O
can	O
be	O
used	O
to	O
construct	O
a	O
graph	B
from	O
which	O
global	O
ci	O
statements	O
can	O
be	O
extracted	O
.	O
19.2.2	O
an	O
undirected	B
alternative	O
to	O
d-separation	O
we	O
have	O
seen	O
that	O
determinining	O
ci	O
relationships	O
in	O
ugms	O
is	O
much	O
easier	O
than	O
in	O
dgms	O
,	O
because	O
we	O
do	O
not	O
have	O
to	O
worry	O
about	O
the	O
directionality	O
of	O
the	O
edges	B
.	O
in	O
this	O
section	O
,	O
we	O
show	O
how	O
to	O
determine	O
ci	O
relationships	O
for	O
a	O
dgm	O
using	O
a	O
ugm	O
.	O
it	O
is	O
tempting	O
to	O
simply	O
convert	O
the	O
dgm	O
to	O
a	O
ugm	O
by	O
dropping	O
the	O
orientation	O
of	O
the	O
edges	B
,	O
but	O
this	O
is	O
clearly	O
incorrect	O
,	O
since	O
a	O
v-structure	B
a	O
→	O
b	O
←	O
c	O
has	O
quite	O
different	O
ci	O
properties	O
than	O
the	O
corresponding	O
undirected	B
chain	O
a	O
−	O
b	O
−	O
c.	O
the	O
latter	O
graph	B
incorrectly	O
states	O
that	O
a	O
⊥	O
c|b	O
.	O
to	O
avoid	O
such	O
incorrect	O
ci	O
statements	O
,	O
we	O
can	O
add	O
edges	B
between	O
the	O
“	O
unmarried	O
”	O
parents	B
a	O
and	O
c	O
,	O
and	O
then	O
drop	O
the	O
arrows	O
from	O
the	O
edges	B
,	O
forming	O
(	O
in	O
this	O
case	O
)	O
a	O
fully	O
connected	O
undirected	B
graph	O
.	O
this	O
process	O
is	O
called	O
moralization	B
.	O
figure	O
19.2	O
(	O
b	O
)	O
gives	O
a	O
larger	O
1.	O
the	O
restriction	O
to	O
positive	O
densities	O
arises	O
because	O
deterministic	O
constraints	O
can	O
result	O
in	O
independencies	O
present	O
in	O
the	O
distribution	O
that	O
are	O
not	O
explicitly	O
represented	O
in	O
the	O
graph	B
.	O
see	O
e.g.	O
,	O
(	O
koller	O
and	O
friedman	O
2009	O
,	O
p120	O
)	O
for	O
some	O
examples	O
.	O
distributions	O
with	O
non-graphical	O
ci	O
properties	O
are	O
said	O
to	O
be	O
unfaithful	B
to	O
the	O
graph	B
,	O
so	O
i	O
(	O
p	O
)	O
(	O
cid:3	O
)	O
=	O
i	O
(	O
g	O
)	O
.	O
664	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
(	O
cid:51	O
)	O
(	O
cid:85	O
)	O
(	O
cid:82	O
)	O
(	O
cid:69	O
)	O
(	O
cid:68	O
)	O
(	O
cid:69	O
)	O
(	O
cid:76	O
)	O
(	O
cid:79	O
)	O
(	O
cid:76	O
)	O
(	O
cid:86	O
)	O
(	O
cid:87	O
)	O
(	O
cid:76	O
)	O
(	O
cid:70	O
)	O
(	O
cid:3	O
)	O
(	O
cid:48	O
)	O
(	O
cid:82	O
)	O
(	O
cid:71	O
)	O
(	O
cid:72	O
)	O
(	O
cid:79	O
)	O
(	O
cid:86	O
)	O
(	O
cid:42	O
)	O
(	O
cid:85	O
)	O
(	O
cid:68	O
)	O
(	O
cid:83	O
)	O
(	O
cid:75	O
)	O
(	O
cid:76	O
)	O
(	O
cid:70	O
)	O
(	O
cid:68	O
)	O
(	O
cid:79	O
)	O
(	O
cid:3	O
)	O
(	O
cid:48	O
)	O
(	O
cid:82	O
)	O
(	O
cid:71	O
)	O
(	O
cid:72	O
)	O
(	O
cid:79	O
)	O
(	O
cid:86	O
)	O
(	O
cid:39	O
)	O
(	O
cid:76	O
)	O
(	O
cid:85	O
)	O
(	O
cid:72	O
)	O
(	O
cid:70	O
)	O
(	O
cid:87	O
)	O
(	O
cid:72	O
)	O
(	O
cid:71	O
)	O
(	O
cid:38	O
)	O
(	O
cid:75	O
)	O
(	O
cid:82	O
)	O
(	O
cid:85	O
)	O
(	O
cid:71	O
)	O
(	O
cid:68	O
)	O
(	O
cid:79	O
)	O
(	O
cid:56	O
)	O
(	O
cid:81	O
)	O
(	O
cid:71	O
)	O
(	O
cid:76	O
)	O
(	O
cid:85	O
)	O
(	O
cid:72	O
)	O
(	O
cid:70	O
)	O
(	O
cid:87	O
)	O
(	O
cid:72	O
)	O
(	O
cid:71	O
)	O
figure	O
19.5	O
dgms	O
and	O
ugms	O
can	O
perfectly	O
represent	O
different	O
sets	O
of	O
distributions	O
.	O
some	O
distributions	O
can	O
be	O
perfectly	O
represented	O
by	O
either	O
dgms	O
or	O
ugms	O
;	O
the	O
corresponding	O
graph	B
must	O
be	O
chordal	B
.	O
example	O
of	O
moralization	B
:	O
we	O
interconnect	O
2	O
and	O
3	O
,	O
since	O
they	O
have	O
a	O
common	O
child	O
5	O
,	O
and	O
we	O
interconnect	O
4	O
,	O
5	O
and	O
6	O
,	O
since	O
they	O
have	O
a	O
common	O
child	O
7.	O
unfortunately	O
,	O
moralization	B
loses	O
some	O
ci	O
information	B
,	O
and	O
therefore	O
we	O
can	O
not	O
use	O
the	O
moralized	O
ugm	O
to	O
determine	O
ci	O
properties	O
of	O
the	O
dgm	O
.	O
for	O
example	O
,	O
in	O
figure	O
19.2	O
(	O
a	O
)	O
,	O
using	O
d-separation	O
,	O
we	O
see	O
that	O
4	O
⊥	O
5|2	O
.	O
adding	O
a	O
moralization	B
arc	O
4	O
−	O
5	O
would	O
lose	O
this	O
fact	O
(	O
see	O
figure	O
19.2	O
(	O
b	O
)	O
)	O
.	O
however	O
,	O
notice	O
that	O
the	O
4-5	O
moralization	B
edge	O
,	O
due	O
to	O
the	O
common	O
child	O
7	O
,	O
is	O
not	O
needed	O
if	O
we	O
do	O
not	O
observe	O
7	O
or	O
any	O
of	O
its	O
descendants	B
.	O
this	O
suggests	O
the	O
following	O
approach	O
to	O
determining	O
if	O
a	O
⊥	O
b|c	O
.	O
first	O
we	O
form	O
the	O
ancestral	B
graph	I
of	O
dag	O
g	O
with	O
respect	O
to	O
u	O
=	O
a	O
∪	O
b	O
∪	O
c.	O
this	O
means	O
we	O
remove	O
all	O
nodes	O
from	O
g	O
that	O
are	O
not	O
in	O
u	O
or	O
are	O
not	O
ancestors	O
of	O
u	O
.	O
we	O
then	O
moralize	O
this	O
ancestral	B
graph	I
,	O
and	O
apply	O
the	O
simple	O
graph	O
separation	O
rules	O
for	O
ugms	O
.	O
for	O
example	O
,	O
in	O
figure	O
19.4	O
(	O
a	O
)	O
,	O
we	O
show	O
the	O
ancestral	B
graph	I
for	O
figure	O
19.2	O
(	O
a	O
)	O
using	O
u	O
=	O
{	O
2	O
,	O
4	O
,	O
5	O
}	O
.	O
in	O
figure	O
19.4	O
(	O
b	O
)	O
,	O
we	O
show	O
the	O
moralized	O
version	O
of	O
this	O
graph	B
.	O
it	O
is	O
clear	O
that	O
we	O
now	O
correctly	O
conclude	O
that	O
4	O
⊥	O
5|2	O
.	O
19.2.3	O
comparing	O
directed	B
and	O
undirected	O
graphical	O
models	O
which	O
model	O
has	O
more	O
“	O
expressive	O
power	O
”	O
,	O
a	O
dgm	O
or	O
a	O
ugm	O
?	O
to	O
formalize	O
this	O
question	O
,	O
recall	B
that	O
we	O
say	O
that	O
g	O
is	O
an	O
i-map	O
of	O
a	O
distribution	O
p	O
if	O
i	O
(	O
g	O
)	O
⊆	O
i	O
(	O
p	O
)	O
.	O
now	O
deﬁne	O
g	O
to	O
be	O
perfect	B
map	I
of	O
p	O
if	O
i	O
(	O
g	O
)	O
=i	O
(	O
p	O
)	O
,	O
in	O
other	O
words	O
,	O
the	O
graph	B
can	O
represent	O
all	O
(	O
and	O
only	O
)	O
the	O
ci	O
properties	O
of	O
the	O
distribution	O
.	O
it	O
turns	O
out	O
that	O
dgms	O
and	O
ugms	O
are	O
perfect	O
maps	O
for	O
different	O
sets	O
of	O
distributions	O
(	O
see	O
figure	O
19.5	O
)	O
.	O
in	O
this	O
sense	O
,	O
neither	O
is	O
more	O
powerful	O
than	O
the	O
other	O
as	O
a	O
representation	O
language	O
.	O
as	O
an	O
example	O
of	O
some	O
ci	O
relationships	O
that	O
can	O
be	O
perfectly	O
modeled	O
by	O
a	O
dgm	O
but	O
not	O
a	O
ugm	O
,	O
consider	O
a	O
v-structure	B
a	O
→	O
c	O
←	O
b.	O
this	O
asserts	O
that	O
a	O
⊥	O
b	O
,	O
and	O
a	O
(	O
cid:4	O
)	O
⊥	O
b|c	O
.	O
if	O
we	O
drop	O
the	O
arrows	O
,	O
we	O
get	O
a	O
−	O
c	O
−	O
b	O
,	O
which	O
asserts	O
a	O
⊥	O
b|c	O
and	O
a	O
(	O
cid:4	O
)	O
⊥	O
b	O
,	O
which	O
is	O
incorrect	O
.	O
in	O
fact	O
,	O
there	O
is	O
no	O
ugm	O
that	O
can	O
precisely	O
represent	O
all	O
and	O
only	O
the	O
two	O
ci	O
statements	O
encoded	O
by	O
a	O
v-	O
structure	O
.	O
in	O
general	O
,	O
ci	O
properties	O
in	O
ugms	O
are	O
monotonic	O
,	O
in	O
the	O
following	O
sense	O
:	O
if	O
a	O
⊥	O
b|c	O
,	O
then	O
a	O
⊥	O
b|	O
(	O
c	O
∪	O
d	O
)	O
.	O
but	O
in	O
dgms	O
,	O
ci	O
properties	O
can	O
be	O
non-monotonic	O
,	O
since	O
conditioning	B
19.3.	O
parameterization	O
of	O
mrfs	O
a	O
a	O
d	O
b	O
d	O
b	O
c	O
(	O
a	O
)	O
c	O
(	O
b	O
)	O
665	O
b	O
a	O
d	O
c	O
(	O
c	O
)	O
figure	O
19.6	O
a	O
ugm	O
and	O
two	O
failed	O
attempts	O
to	O
represent	O
it	O
as	O
a	O
dgm	O
.	O
source	O
:	O
figure	O
3.10	O
of	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
.	O
on	O
extra	O
variables	O
can	O
eliminate	O
conditional	O
independencies	O
due	O
to	O
explaining	B
away	I
.	O
as	O
an	O
example	O
of	O
some	O
ci	O
relationships	O
that	O
can	O
be	O
perfectly	O
modeled	O
by	O
a	O
ugm	O
but	O
not	O
a	O
dgm	O
,	O
consider	O
the	O
4-cycle	O
shown	O
in	O
figure	O
19.6	O
(	O
a	O
)	O
.	O
one	O
attempt	O
to	O
model	O
this	O
with	O
a	O
dgm	O
is	O
shown	O
in	O
figure	O
19.6	O
(	O
b	O
)	O
.	O
this	O
correctly	O
asserts	O
that	O
a	O
⊥	O
c|b	O
,	O
d.	O
however	O
,	O
it	O
incorrectly	O
asserts	O
that	O
b	O
⊥	O
d|a	O
.	O
figure	O
19.6	O
(	O
c	O
)	O
is	O
another	O
incorrect	O
dgm	O
:	O
it	O
correctly	O
encodes	O
a	O
⊥	O
c|b	O
,	O
d	O
,	O
but	O
incorrectly	O
encodes	O
b	O
⊥	O
d.	O
in	O
fact	O
there	O
is	O
no	O
dgm	O
that	O
can	O
precisely	O
represent	O
all	O
and	O
only	O
the	O
ci	O
statements	O
encoded	O
by	O
this	O
ugm	O
.	O
some	O
distributions	O
can	O
be	O
perfectly	O
modeled	O
by	O
either	O
a	O
dgm	O
or	O
a	O
ugm	O
;	O
the	O
resulting	O
graphs	O
are	O
called	O
decomposable	B
or	O
chordal	B
.	O
roughly	O
speaking	O
,	O
this	O
means	O
the	O
following	O
:	O
if	O
we	O
collapse	O
together	O
all	O
the	O
variables	O
in	O
each	O
maximal	B
clique	I
,	O
to	O
make	O
“	O
mega-variables	O
”	O
,	O
the	O
resulting	O
graph	B
will	O
be	O
a	O
tree	B
.	O
of	O
course	O
,	O
if	O
the	O
graph	B
is	O
already	O
a	O
tree	B
(	O
which	O
includes	O
chains	O
as	O
a	O
special	O
case	O
)	O
,	O
it	O
will	O
be	O
chordal	B
.	O
see	O
section	O
20.4.1	O
for	O
further	O
details	O
.	O
19.3	O
parameterization	O
of	O
mrfs	O
although	O
the	O
ci	O
properties	O
of	O
ugm	O
are	O
simpler	O
and	O
more	O
natural	O
than	O
for	O
dgms	O
,	O
representing	O
the	O
joint	B
distribution	I
for	O
a	O
ugm	O
is	O
less	O
natural	O
than	O
for	O
a	O
dgm	O
,	O
as	O
we	O
see	O
below	O
.	O
19.3.1	O
the	O
hammersley-clifford	O
theorem	O
since	O
there	O
is	O
no	O
topological	O
ordering	O
associated	O
with	O
an	O
undirected	B
graph	O
,	O
we	O
can	O
’	O
t	O
use	O
the	O
chain	B
rule	I
to	O
represent	O
p	O
(	O
y	O
)	O
.	O
so	O
instead	O
of	O
associating	O
cpds	O
with	O
each	O
node	O
,	O
we	O
associate	O
potential	O
functions	O
orfactors	O
with	O
each	O
maximal	B
clique	I
in	O
the	O
graph	B
.	O
we	O
will	O
denote	O
the	O
potential	B
function	I
for	O
clique	B
c	O
by	O
ψc	O
(	O
yc|θc	O
)	O
.	O
a	O
potential	B
function	I
can	O
be	O
any	O
non-negative	O
function	O
of	O
its	O
arguments	O
.	O
the	O
joint	B
distribution	I
is	O
then	O
deﬁned	O
to	O
be	O
proportional	O
to	O
the	O
product	O
of	O
clique	O
potentials	O
.	O
rather	O
surprisingly	O
,	O
one	O
can	O
show	O
that	O
any	O
positive	O
distribution	O
whose	O
ci	O
properties	O
can	O
be	O
represented	O
by	O
a	O
ugm	O
can	O
be	O
represented	O
in	O
this	O
way	O
.	O
we	O
state	B
this	O
result	O
more	O
formally	O
below	O
.	O
666	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
theorem	O
19.3.1	O
(	O
hammersley-clifford	O
)	O
.	O
a	O
positive	O
distribution	O
p	O
(	O
y	O
)	O
>	O
0	O
satisﬁes	O
the	O
ci	O
prop-	O
erties	O
of	O
an	O
undirected	B
graph	O
g	O
iff	B
p	O
can	O
be	O
represented	O
as	O
a	O
product	O
of	O
factors	O
,	O
one	O
per	O
maximal	B
clique	I
,	O
i.e.	O
,	O
ψc	O
(	O
yc|θc	O
)	O
(	O
19.3	O
)	O
where	O
c	O
is	O
the	O
set	O
of	O
all	O
the	O
(	O
maximal	O
)	O
cliques	B
of	O
g	O
,	O
and	O
z	O
(	O
θ	O
)	O
is	O
the	O
partition	B
function	I
given	O
by	O
ψc	O
(	O
yc|θc	O
)	O
(	O
19.4	O
)	O
(	O
cid:20	O
)	O
c∈c	O
p	O
(	O
y|θ	O
)	O
=	O
(	O
cid:4	O
)	O
z	O
(	O
θ	O
)	O
(	O
cid:2	O
)	O
1	O
z	O
(	O
θ	O
)	O
(	O
cid:20	O
)	O
c∈c	O
x	O
note	O
that	O
the	O
partition	B
function	I
is	O
what	O
ensures	O
the	O
overall	O
distribution	O
sums	O
to	O
1.2	O
the	O
proof	O
was	O
never	O
published	O
,	O
but	O
can	O
be	O
found	O
in	O
e.g.	O
,	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
for	O
example	O
,	O
consider	O
the	O
mrf	O
in	O
figure	O
10.1	O
(	O
b	O
)	O
.	O
if	O
p	O
satisﬁes	O
the	O
ci	O
properties	O
of	O
this	O
graph	B
then	O
we	O
can	O
write	O
p	O
as	O
follows	O
:	O
p	O
(	O
y|θ	O
)	O
=	O
(	O
cid:4	O
)	O
z	O
=	O
y	O
where	O
1	O
z	O
(	O
θ	O
)	O
ψ123	O
(	O
y1	O
,	O
y2	O
,	O
y3	O
)	O
ψ234	O
(	O
y2	O
,	O
y3	O
,	O
y4	O
)	O
ψ35	O
(	O
y3	O
,	O
y5	O
)	O
ψ123	O
(	O
y1	O
,	O
y2	O
,	O
y3	O
)	O
ψ234	O
(	O
y2	O
,	O
y3	O
,	O
y4	O
)	O
ψ35	O
(	O
y3	O
,	O
y5	O
)	O
(	O
19.5	O
)	O
(	O
19.6	O
)	O
in	O
particular	O
,	O
there	O
is	O
a	O
there	O
is	O
a	O
deep	B
connection	O
between	O
ugms	O
and	O
statistical	O
physics	O
.	O
model	O
known	O
as	O
the	O
gibbs	O
distribution	O
,	O
which	O
can	O
be	O
written	O
as	O
follows	O
:	O
p	O
(	O
y|θ	O
)	O
=	O
exp	O
(	O
−	O
1	O
z	O
(	O
θ	O
)	O
e	O
(	O
yc|θc	O
)	O
)	O
(	O
19.7	O
)	O
(	O
cid:4	O
)	O
c	O
where	O
e	O
(	O
yc	O
)	O
>	O
0	O
is	O
the	O
energy	O
associated	O
with	O
the	O
variables	O
in	O
clique	B
c.	O
we	O
can	O
convert	O
this	O
to	O
a	O
ugm	O
by	O
deﬁning	O
ψc	O
(	O
yc|θc	O
)	O
=	O
exp	O
(	O
−e	O
(	O
yc|θc	O
)	O
)	O
(	O
19.8	O
)	O
we	O
see	O
that	O
high	O
probability	O
states	O
correspond	O
to	O
low	O
energy	O
conﬁgurations	O
.	O
models	O
of	O
this	O
form	O
are	O
known	O
as	O
energy	B
based	I
models	I
,	O
and	O
are	O
commonly	O
used	O
in	O
physics	O
and	O
biochemistry	O
,	O
as	O
well	O
as	O
some	O
branches	O
of	O
machine	B
learning	I
(	O
lecun	O
et	O
al	O
.	O
2006	O
)	O
.	O
note	O
that	O
we	O
are	O
free	O
to	O
restrict	O
the	O
parameterization	O
to	O
the	O
edges	B
of	O
the	O
graph	B
,	O
rather	O
than	O
the	O
maximal	O
cliques	O
.	O
this	O
is	O
called	O
a	O
pairwise	O
mrf	O
.	O
in	O
figure	O
10.1	O
(	O
b	O
)	O
,	O
we	O
get	O
p	O
(	O
y|θ	O
)	O
∝	O
ψ12	O
(	O
y1	O
,	O
y2	O
)	O
ψ13	O
(	O
y1	O
,	O
y3	O
)	O
ψ23	O
(	O
y2	O
,	O
y3	O
)	O
ψ24	O
(	O
y2	O
,	O
y4	O
)	O
ψ34	O
(	O
y3	O
,	O
y4	O
)	O
ψ35	O
(	O
y3	O
,	O
y5	O
)	O
(	O
19.9	O
)	O
(	O
19.10	O
)	O
(	O
cid:20	O
)	O
s∼t	O
∝	O
ψst	O
(	O
ys	O
,	O
yt	O
)	O
this	O
form	O
is	O
widely	O
used	O
due	O
to	O
its	O
simplicity	O
,	O
although	O
it	O
is	O
not	O
as	O
general	O
.	O
2.	O
the	O
partition	B
function	I
is	O
denoted	O
by	O
z	O
because	O
of	O
the	O
german	O
word	O
zustandssumme	O
,	O
which	O
means	O
“	O
sum	O
over	O
states	O
”	O
.	O
this	O
reﬂects	O
the	O
fact	O
that	O
a	O
lot	O
of	O
pioneering	O
working	O
in	O
statistical	O
physics	O
was	O
done	O
by	O
germans	O
.	O
19.3.	O
parameterization	O
of	O
mrfs	O
667	O
19.3.2	O
representing	O
potential	O
functions	O
if	O
the	O
variables	O
are	O
discrete	B
,	O
we	O
can	O
represent	O
the	O
potential	O
or	O
energy	O
functions	O
as	O
tables	O
of	O
(	O
non-negative	O
)	O
numbers	O
,	O
just	O
as	O
we	O
did	O
with	O
cpts	O
.	O
however	O
,	O
the	O
potentials	O
are	O
not	O
probabilities	O
.	O
rather	O
,	O
they	O
represent	O
the	O
relative	O
“	O
compatibility	O
”	O
between	O
the	O
different	O
assignments	O
to	O
the	O
potential	O
.	O
we	O
will	O
see	O
some	O
examples	O
of	O
this	O
below	O
.	O
a	O
more	O
general	O
approach	O
is	O
to	O
deﬁne	O
the	O
log	O
potentials	O
as	O
a	O
linear	O
function	O
of	O
the	O
parameters	O
:	O
log	O
ψc	O
(	O
yc	O
)	O
(	O
cid:2	O
)	O
φc	O
(	O
yc	O
)	O
t	O
θc	O
(	O
19.11	O
)	O
where	O
φc	O
(	O
xc	O
)	O
is	O
a	O
feature	O
vector	O
derived	O
from	O
the	O
values	O
of	O
the	O
variables	O
yc	O
.	O
the	O
resulting	O
log	O
probability	O
has	O
the	O
form	O
log	O
p	O
(	O
y|θ	O
)	O
=	O
φc	O
(	O
yc	O
)	O
t	O
θc	O
−	O
z	O
(	O
θ	O
)	O
(	O
19.12	O
)	O
(	O
cid:4	O
)	O
c	O
this	O
is	O
also	O
known	O
as	O
a	O
maximum	B
entropy	I
or	O
a	O
log-linear	B
model	O
.	O
for	O
example	O
,	O
consider	O
a	O
pairwise	O
mrf	O
,	O
where	O
for	O
each	O
edge	O
,	O
we	O
associate	O
a	O
feature	O
vector	O
of	O
length	O
k	O
2	O
as	O
follows	O
:	O
φst	O
(	O
ys	O
,	O
yt	O
)	O
=	O
[	O
.	O
.	O
.	O
,	O
i	O
(	O
ys	O
=	O
j	O
,	O
yt	O
=	O
k	O
)	O
,	O
.	O
.	O
.	O
]	O
(	O
19.13	O
)	O
if	O
we	O
have	O
a	O
weight	O
for	O
each	O
feature	O
,	O
we	O
can	O
convert	O
this	O
into	O
a	O
k	O
×	O
k	O
potential	B
function	I
as	O
follows	O
:	O
ψst	O
(	O
ys	O
=	O
j	O
,	O
yt	O
=	O
k	O
)	O
=	O
exp	O
(	O
[	O
θt	O
stφst	O
]	O
jk	O
)	O
=	O
exp	O
(	O
θst	O
(	O
j	O
,	O
k	O
)	O
)	O
(	O
19.14	O
)	O
so	O
we	O
see	O
that	O
we	O
can	O
easily	O
represent	O
tabular	O
potentials	O
using	O
a	O
log-linear	B
form	O
.	O
but	O
the	O
log-linear	B
form	O
is	O
more	O
general	O
.	O
to	O
see	O
why	O
this	O
is	O
useful	O
,	O
suppose	O
we	O
are	O
interested	O
in	O
making	O
a	O
probabilistic	O
model	O
of	O
english	O
spelling	O
.	O
since	O
certain	O
letter	O
combinations	O
occur	O
together	O
quite	O
frequently	O
(	O
e.g.	O
,	O
“	O
ing	O
”	O
)	O
,	O
we	O
will	O
need	O
higher	O
order	O
factors	B
to	O
capture	O
this	O
.	O
suppose	O
we	O
limit	O
ourselves	O
to	O
letter	O
trigrams	O
.	O
a	O
tabular	O
potential	O
still	O
has	O
263	O
=	O
17	O
,	O
576	O
parameters	O
in	O
it	O
.	O
however	O
,	O
most	O
of	O
these	O
triples	O
will	O
never	O
occur	O
.	O
an	O
alternative	O
approach	O
is	O
to	O
deﬁne	O
indicator	O
functions	O
that	O
look	O
for	O
certain	O
“	O
special	O
”	O
triples	O
,	O
such	O
as	O
“	O
ing	O
”	O
,	O
“	O
qu-	O
”	O
,	O
etc	O
.	O
then	O
we	O
can	O
deﬁne	O
the	O
potential	O
on	O
each	O
trigram	O
as	O
follows	O
:	O
ψ	O
(	O
yt−1	O
,	O
yt	O
,	O
yt+1	O
)	O
=	O
exp	O
(	O
θkφk	O
(	O
yt−1	O
,	O
yt	O
,	O
yt+1	O
)	O
)	O
(	O
19.15	O
)	O
(	O
cid:4	O
)	O
k	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
y|θ	O
)	O
∝	O
exp	O
(	O
where	O
k	O
indexes	O
the	O
different	O
features	B
,	O
corresponding	O
to	O
“	O
ing	O
”	O
,	O
“	O
qu-	O
”	O
,	O
etc.	O
,	O
and	O
φk	O
is	O
the	O
corre-	O
sponding	O
binary	O
feature	O
function	O
.	O
by	O
tying	O
the	O
parameters	O
across	O
locations	O
,	O
we	O
can	O
deﬁne	O
the	O
probability	O
of	O
a	O
word	O
of	O
any	O
length	O
using	O
θkφk	O
(	O
yt−1	O
,	O
yt	O
,	O
yt+1	O
)	O
)	O
(	O
19.16	O
)	O
t	O
k	O
this	O
raises	O
the	O
question	O
of	O
where	O
these	O
feature	O
functions	O
come	O
from	O
.	O
in	O
many	O
applications	O
,	O
they	O
are	O
created	O
by	O
hand	O
to	O
reﬂect	O
domain	O
knowledge	O
(	O
we	O
will	O
see	O
examples	O
later	O
)	O
,	O
but	O
it	O
is	O
also	O
possible	O
to	O
learn	O
them	O
from	O
data	O
,	O
as	O
we	O
discuss	O
in	O
section	O
19.5.6	O
.	O
668	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
19.4	O
examples	O
of	O
mrfs	O
in	O
this	O
section	O
,	O
we	O
show	O
how	O
several	O
popular	O
probability	O
models	O
can	O
be	O
conveniently	O
expressed	O
as	O
ugms	O
.	O
19.4.1	O
ising	O
model	O
the	O
ising	O
model	O
is	O
an	O
example	O
of	O
an	O
mrf	O
that	O
arose	O
from	O
statistical	O
physics.3	O
it	O
was	O
originally	O
used	O
for	O
modeling	O
the	O
behavior	O
of	O
magnets	O
.	O
in	O
particular	O
,	O
let	O
ys	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
represent	O
the	O
spin	B
in	O
some	O
magnets	O
,	O
called	O
ferro-magnets	B
,	O
of	O
an	O
atom	B
,	O
which	O
can	O
either	O
be	O
spin	B
down	O
or	O
up	O
.	O
neighboring	O
spins	O
tend	O
to	O
line	O
up	O
in	O
the	O
same	O
direction	O
,	O
whereas	O
in	O
other	O
kinds	O
of	O
magnets	O
,	O
called	O
anti-ferromagnets	B
,	O
the	O
spins	O
“	O
want	O
”	O
to	O
be	O
different	O
from	O
their	O
neighbors	B
.	O
we	O
can	O
model	O
this	O
as	O
an	O
mrf	O
as	O
follows	O
.	O
we	O
create	O
a	O
graph	B
in	O
the	O
form	O
of	O
a	O
2d	O
or	O
3d	O
lattice	B
,	O
and	O
connect	O
neighboring	O
variables	O
,	O
as	O
in	O
figure	O
19.1	O
(	O
b	O
)	O
.	O
we	O
then	O
deﬁne	O
the	O
following	O
pairwise	O
clique	O
potential	O
:	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
ψst	O
(	O
ys	O
,	O
yt	O
)	O
=	O
ewst	O
e−wst	O
e−wst	O
ewst	O
(	O
19.17	O
)	O
here	O
wst	O
is	O
the	O
coupling	O
strength	O
between	O
nodes	B
s	O
and	O
t.	O
if	O
two	O
nodes	B
are	O
not	O
connected	O
in	O
the	O
graph	B
,	O
we	O
set	O
wst	O
=	O
0.	O
we	O
assume	O
that	O
the	O
weight	O
matrix	O
w	O
is	O
symmetric	B
,	O
so	O
wst	O
=	O
wts	O
.	O
often	O
we	O
assume	O
all	O
edges	O
have	O
the	O
same	O
strength	O
,	O
so	O
wst	O
=	O
j	O
(	O
assuming	O
wst	O
(	O
cid:4	O
)	O
=	O
0	O
)	O
.	O
if	O
all	O
the	O
weights	O
are	O
positive	O
,	O
j	O
>	O
0	O
,	O
then	O
neighboring	O
spins	O
are	O
likely	O
to	O
be	O
in	O
the	O
same	O
state	B
;	O
this	O
can	O
be	O
used	O
to	O
model	O
ferromagnets	O
,	O
and	O
is	O
an	O
example	O
of	O
an	O
associative	B
markov	O
network	O
.	O
if	O
the	O
weights	O
are	O
sufficiently	O
strong	O
,	O
the	O
corresponding	O
probability	O
distribution	O
will	O
have	O
two	O
modes	O
,	O
corresponding	O
to	O
the	O
all	O
+1	O
’	O
s	O
state	B
and	O
the	O
all	O
-1	O
’	O
s	O
state	B
.	O
these	O
are	O
called	O
the	O
ground	B
states	I
of	O
the	O
system	O
.	O
if	O
all	O
of	O
the	O
weights	O
are	O
negative	O
,	O
j	O
<	O
0	O
,	O
then	O
the	O
spins	O
want	O
to	O
be	O
different	O
from	O
their	O
neighbors	B
;	O
this	O
can	O
be	O
used	O
to	O
model	O
an	O
anti-ferromagnet	O
,	O
and	O
results	O
in	O
a	O
frustrated	B
system	I
,	O
in	O
which	O
not	O
all	O
the	O
constraints	O
can	O
be	O
satisﬁed	O
at	O
the	O
same	O
time	O
.	O
the	O
corresponding	O
probability	O
distribution	O
will	O
have	O
multiple	O
modes	O
.	O
interestingly	O
,	O
computing	O
the	O
partition	B
function	I
z	O
(	O
j	O
)	O
can	O
be	O
done	O
in	O
polynomial	O
time	O
for	O
associative	B
markov	O
networks	O
,	O
but	O
is	O
np-hard	O
in	O
general	O
(	O
cipra	O
2000	O
)	O
.	O
there	O
is	O
an	O
interesting	O
analogy	O
between	O
ising	O
models	O
and	O
gaussian	O
graphical	O
models	O
.	O
first	O
,	O
assuming	O
yt	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
,	O
we	O
can	O
write	O
the	O
unnormalized	O
log	O
probability	O
of	O
an	O
ising	O
model	O
as	O
follows	O
:	O
log	O
˜p	O
(	O
y	O
)	O
=	O
−	O
yswstyt	O
=	O
−	O
1	O
2	O
yt	O
wy	O
(	O
19.18	O
)	O
(	O
cid:4	O
)	O
s∼t	O
2	O
arises	O
because	O
we	O
sum	O
each	O
edge	O
twice	O
.	O
)	O
if	O
wst	O
=	O
j	O
>	O
0	O
,	O
we	O
get	O
a	O
low	O
energy	O
(	O
the	O
factor	B
of	O
1	O
(	O
and	O
hence	O
high	O
probability	O
)	O
if	O
neighboring	O
states	O
agree	O
.	O
sometimes	O
there	O
is	O
an	O
external	B
ﬁeld	I
,	O
which	O
is	O
an	O
energy	O
term	O
which	O
is	O
added	O
to	O
each	O
spin	B
.	O
this	O
can	O
be	O
modelled	O
using	O
a	O
local	O
energy	O
term	O
of	O
the	O
form	O
−bt	O
y	O
,	O
where	O
b	O
is	O
sometimes	O
called	O
3.	O
ernst	O
ising	O
was	O
a	O
german-american	O
physicist	O
,	O
1900–1998	O
.	O
19.4.	O
examples	O
of	O
mrfs	O
a	O
bias	B
term	I
.	O
the	O
modiﬁed	O
distribution	O
is	O
given	O
by	O
log	O
˜p	O
(	O
y	O
)	O
=	O
wstysyt	O
+	O
bsys	O
=	O
1	O
2	O
yt	O
wy	O
+	O
bt	O
y	O
(	O
cid:4	O
)	O
s∼t	O
(	O
cid:4	O
)	O
s	O
669	O
(	O
19.19	O
)	O
where	O
θ	O
=	O
(	O
w	O
,	O
b	O
)	O
.	O
if	O
we	O
deﬁne	O
μ	O
(	O
cid:2	O
)	O
−	O
1	O
that	O
looks	O
similar	B
to	O
a	O
gaussian	O
:	O
2	O
σ−1b	O
,	O
σ−1	O
=	O
−w	O
,	O
and	O
c	O
(	O
cid:2	O
)	O
1	O
2	O
μt	O
σ−1μ	O
,	O
we	O
can	O
rewrite	O
this	O
in	O
a	O
form	O
˜p	O
(	O
y	O
)	O
∝	O
exp	O
(	O
−	O
1	O
2	O
(	O
y	O
−	O
μ	O
)	O
t	O
σ−1	O
(	O
y	O
−	O
μ	O
)	O
+c	O
)	O
(	O
19.20	O
)	O
one	O
very	O
important	O
difference	O
is	O
that	O
,	O
in	O
the	O
case	O
of	O
gaussians	O
,	O
the	O
normalization	O
constant	O
,	O
z	O
=	O
|2πς|	O
,	O
requires	O
the	O
computation	O
of	O
a	O
matrix	O
determinant	O
,	O
which	O
can	O
be	O
computed	O
in	O
o	O
(	O
d3	O
)	O
time	O
,	O
whereas	O
in	O
the	O
case	O
of	O
the	O
ising	O
model	O
,	O
the	O
normalization	O
constant	O
requires	O
summing	O
over	O
all	O
2d	O
bit	O
vectors	O
;	O
this	O
is	O
equivalent	O
to	O
computing	O
the	O
matrix	B
permanent	I
,	O
which	O
is	O
np-hard	O
in	O
general	O
(	O
jerrum	O
et	O
al	O
.	O
2004	O
)	O
.	O
19.4.2	O
hopﬁeld	O
networks	O
a	O
hopﬁeld	O
network	O
(	O
hopﬁeld	O
1982	O
)	O
is	O
a	O
fully	O
connected	O
ising	O
model	O
with	O
a	O
symmetric	B
weight	O
matrix	O
,	O
w	O
=	O
wt	O
.	O
these	O
weights	O
,	O
plus	O
the	O
bias	B
terms	O
b	O
,	O
can	O
be	O
learned	O
from	O
training	O
data	O
using	O
(	O
approximate	O
)	O
maximum	O
likelihood	O
,	O
as	O
described	O
in	O
section	O
19.5.4	O
the	O
main	O
application	O
of	O
hopﬁeld	O
networks	O
is	O
as	O
an	O
associative	B
memory	I
or	O
content	O
ad-	O
dressable	O
memory	O
.	O
the	O
idea	O
is	O
this	O
:	O
suppose	O
we	O
train	O
on	O
a	O
set	O
of	O
fully	O
observed	O
bit	O
vectors	O
,	O
corresponding	O
to	O
patterns	O
we	O
want	O
to	O
memorize	O
.	O
then	O
,	O
at	O
test	O
time	O
,	O
we	O
present	O
a	O
partial	O
pattern	O
to	O
the	O
network	O
.	O
we	O
would	O
like	O
to	O
estimate	O
the	O
missing	B
variables	O
;	O
this	O
is	O
called	O
pattern	B
com-	O
pletion	O
.	O
see	O
figure	O
19.7	O
for	O
an	O
example	O
.	O
this	O
can	O
be	O
thought	O
of	O
as	O
retrieving	O
an	O
example	O
from	O
memory	O
based	O
on	O
a	O
piece	O
of	O
the	O
example	O
itself	O
,	O
hence	O
the	O
term	O
“	O
associative	B
memory	I
”	O
.	O
since	O
exact	O
inference	B
is	O
intractable	O
in	O
this	O
model	O
,	O
it	O
is	O
standard	O
to	O
use	O
a	O
coordinate	O
descent	O
algorithm	O
known	O
as	O
iterative	B
conditional	I
modes	I
(	O
icm	O
)	O
,	O
which	O
just	O
sets	O
each	O
node	O
to	O
its	O
most	O
likely	O
(	O
lowest	O
energy	O
)	O
state	B
,	O
given	O
all	O
its	O
neighbors	B
.	O
the	O
full	B
conditional	I
can	O
be	O
shown	O
to	O
be	O
(	O
cid:7	O
)	O
(	O
19.21	O
)	O
p	O
(	O
ys	O
=	O
1|y−s	O
,	O
θ	O
)	O
=	O
sigm	O
(	O
wt	O
s	O
,	O
:y−s	O
+	O
bs	O
)	O
picking	O
the	O
most	O
probable	O
state	B
amounts	O
to	O
using	O
the	O
rule	O
y∗	O
y∗	O
s	O
=	O
0	O
otherwise	O
.	O
(	O
much	O
better	O
inference	B
algorithms	O
will	O
be	O
discussed	O
later	O
in	O
this	O
book	O
.	O
)	O
since	O
inference	B
is	O
deterministic	O
,	O
it	O
is	O
also	O
possible	O
to	O
interpret	O
this	O
model	O
as	O
a	O
recurrent	B
neural	I
network	I
.	O
(	O
this	O
is	O
quite	O
different	O
from	O
the	O
feedforward	O
neural	O
nets	O
studied	O
in	O
section	O
16.5	O
;	O
they	O
are	O
univariate	O
conditional	O
density	O
models	O
of	O
the	O
form	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
which	O
can	O
only	O
be	O
used	O
for	O
supervised	B
learning	I
.	O
)	O
see	O
hertz	O
et	O
al	O
.	O
(	O
1991	O
)	O
for	O
further	O
details	O
on	O
hopﬁeld	O
networks	O
.	O
t	O
wstyt	O
>	O
bs	O
and	O
using	O
s	O
=	O
1	O
if	O
a	O
boltzmann	O
machine	O
generalizes	O
the	O
hopﬁeld	O
/	O
ising	O
model	O
by	O
including	O
some	O
hidden	B
inference	O
in	O
such	O
models	O
nodes	B
,	O
which	O
makes	O
the	O
model	O
representationally	O
more	O
powerful	O
.	O
often	O
uses	O
gibbs	O
sampling	O
,	O
which	O
is	O
a	O
stochastic	O
version	O
of	O
icm	O
(	O
see	O
section	O
24.2	O
for	O
details	O
)	O
.	O
4.	O
ml	O
estimation	O
works	O
much	O
better	O
than	O
the	O
outer	O
product	O
rule	O
proposed	O
in	O
in	O
(	O
hopﬁeld	O
1982	O
)	O
,	O
because	O
it	O
not	O
only	O
lowers	O
the	O
energy	O
of	O
the	O
observed	O
patterns	O
,	O
but	O
it	O
also	O
raises	O
the	O
energy	O
of	O
the	O
non-observed	O
patterns	O
,	O
in	O
order	O
to	O
make	O
the	O
distribution	O
sum	O
to	O
one	O
(	O
hillar	O
et	O
al	O
.	O
2012	O
)	O
.	O
670	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
figure	O
19.7	O
examples	O
of	O
how	O
an	O
associative	B
memory	I
can	O
reconstruct	O
images	O
.	O
these	O
are	O
binary	O
images	O
of	O
size	O
50	O
×	O
50	O
pixels	O
.	O
top	O
:	O
training	O
images	O
.	O
row	O
2	O
:	O
partially	O
visible	O
test	O
images	O
.	O
row	O
3	O
:	O
estimate	O
after	O
5	O
iterations	O
.	O
bottom	O
:	O
ﬁnal	O
state	B
estimate	O
.	O
based	O
on	O
figure	O
2.1	O
of	O
hertz	O
et	O
al	O
.	O
(	O
1991	O
)	O
.	O
figure	O
generated	O
by	O
hopfielddemo	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
19.8	O
visualizing	B
a	O
sample	O
from	O
a	O
10-state	O
potts	O
model	O
of	O
size	O
128	O
×	O
128	O
for	O
different	O
association	O
(	O
a	O
)	O
j	O
=	O
1.42	O
,	O
(	O
b	O
)	O
j	O
=	O
1.44	O
,	O
(	O
c	O
)	O
j	O
=	O
1.46.	O
the	O
regions	O
are	O
labeled	O
according	O
to	O
size	O
:	O
blue	O
is	O
strengths	O
:	O
largest	O
,	O
red	O
is	O
smallest	O
.	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
.	O
see	O
gibbsdemoising	O
for	O
matlab	O
code	O
to	O
produce	O
a	O
similar	B
plot	O
for	O
the	O
ising	O
model	O
.	O
however	O
,	O
we	O
could	O
equally	O
well	O
apply	O
gibbs	O
to	O
a	O
hopﬁeld	O
net	O
and	O
icm	O
to	O
a	O
boltzmann	O
machine	O
:	O
the	O
inference	B
algorithm	O
is	O
not	O
part	O
of	O
the	O
model	O
deﬁnition	O
.	O
see	O
section	O
27.7	O
for	O
further	O
details	O
on	O
boltzmann	O
machines	O
.	O
19.4.	O
examples	O
of	O
mrfs	O
671	O
ys	O
xs	O
yt	O
xt	O
figure	O
19.9	O
a	O
grid-structured	O
mrf	O
with	O
local	B
evidence	I
nodes	O
.	O
19.4.3	O
potts	O
model	O
it	O
is	O
easy	O
to	O
generalize	B
the	O
ising	O
model	O
to	O
multiple	O
discrete	O
states	O
,	O
yt	O
∈	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
.	O
common	O
to	O
use	O
a	O
potential	B
function	I
of	O
the	O
following	O
form	O
:	O
it	O
is	O
⎛	O
⎝ej	O
0	O
0	O
⎞	O
⎠	O
0	O
ej	O
0	O
0	O
0	O
ej	O
ψst	O
(	O
ys	O
,	O
yt	O
)	O
=	O
(	O
19.22	O
)	O
this	O
is	O
called	O
the	O
potts	O
model.5	O
if	O
j	O
>	O
0	O
,	O
then	O
neighboring	O
nodes	B
are	O
encouraged	O
to	O
have	O
the	O
same	O
label	B
.	O
some	O
samples	B
from	O
this	O
model	O
are	O
shown	O
in	O
figure	O
19.8.	O
we	O
see	O
that	O
for	O
j	O
>	O
1.44	O
,	O
large	O
clusters	O
occur	O
,	O
for	O
j	O
<	O
1.44	O
,	O
many	O
small	O
clusters	O
occur	O
,	O
and	O
at	O
the	O
critical	B
value	I
of	O
k	O
=	O
1.44	O
,	O
there	O
is	O
a	O
mix	O
of	O
small	O
and	O
large	O
clusters	O
.	O
this	O
rapid	O
change	O
in	O
behavior	O
as	O
we	O
vary	O
a	O
parameter	B
of	O
the	O
system	O
is	O
called	O
a	O
phase	B
transition	I
,	O
and	O
has	O
been	O
widely	O
studied	O
in	O
the	O
physics	O
community	O
.	O
an	O
analogous	O
phenomenon	O
occurs	O
in	O
the	O
ising	O
model	O
;	O
see	O
(	O
mackay	O
2003	O
,	O
ch	O
31	O
)	O
for	O
details	O
.	O
the	O
potts	O
model	O
can	O
be	O
used	O
as	O
a	O
prior	O
for	O
image	B
segmentation	I
,	O
since	O
it	O
says	O
that	O
neighboring	O
pixels	O
are	O
likely	O
to	O
have	O
the	O
same	O
discrete	B
label	O
and	O
hence	O
belong	O
to	O
the	O
same	O
segment	O
.	O
we	O
can	O
combine	O
this	O
prior	O
with	O
a	O
likelihood	B
term	O
as	O
follows	O
:	O
p	O
(	O
y	O
,	O
x|θ	O
)	O
=	O
p	O
(	O
y|j	O
)	O
p	O
(	O
xt|yt	O
,	O
θ	O
)	O
=	O
1	O
t	O
(	O
19.23	O
)	O
where	O
p	O
(	O
xt|yt	O
=	O
k	O
,	O
θ	O
)	O
is	O
the	O
probability	O
of	O
observing	O
pixel	O
xt	O
given	O
that	O
the	O
corresponding	O
segment	O
belongs	O
to	O
class	O
k.	O
this	O
observation	B
model	I
can	O
be	O
modeled	O
using	O
a	O
gaussian	O
or	O
a	O
non-parametric	O
density	O
.	O
(	O
note	O
that	O
we	O
label	B
the	O
hidden	B
nodes	I
yt	O
and	O
the	O
observed	O
nodes	O
xt	O
,	O
to	O
be	O
compatible	O
with	O
section	O
19.6	O
.	O
)	O
z	O
(	O
j	O
)	O
ψ	O
(	O
ys	O
,	O
yt	O
;	O
j	O
)	O
t	O
p	O
(	O
xt|yt	O
,	O
θ	O
)	O
the	O
corresponding	O
graphical	B
model	I
is	O
a	O
mix	O
of	O
undirected	B
and	O
directed	B
edges	O
,	O
as	O
shown	O
in	O
figure	O
19.9.	O
the	O
undirected	B
2d	O
lattice	B
represents	O
the	O
prior	O
p	O
(	O
y	O
)	O
;	O
in	O
addition	O
,	O
there	O
are	O
directed	B
edge	O
from	O
each	O
yt	O
to	O
its	O
corresponding	O
xt	O
,	O
representing	O
the	O
local	B
evidence	I
.	O
technically	O
speak-	O
ing	O
,	O
this	O
combination	O
of	O
an	O
undirected	B
and	O
directed	B
graph	O
is	O
called	O
a	O
chain	B
graph	I
.	O
however	O
,	O
5.	O
renfrey	O
potts	O
was	O
an	O
australian	O
mathematician	O
,	O
1925–2005	O
.	O
(	O
cid:20	O
)	O
(	O
cid:24	O
)	O
(	O
cid:20	O
)	O
s∼t	O
(	O
cid:25	O
)	O
(	O
cid:20	O
)	O
672	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
since	O
the	O
xt	O
nodes	B
are	O
observed	O
,	O
they	O
can	O
be	O
“	O
absorbed	O
”	O
into	O
the	O
model	O
,	O
thus	O
leaving	O
behind	O
an	O
undirected	B
“	O
backbone	O
”	O
.	O
this	O
model	O
is	O
a	O
2d	O
analog	O
of	O
an	O
hmm	O
,	O
and	O
could	O
be	O
called	O
a	O
partially	O
observed	O
mrf	O
.	O
as	O
in	O
an	O
hmm	O
,	O
the	O
goal	O
is	O
to	O
perform	O
posterior	O
inference	O
,	O
i.e.	O
,	O
to	O
compute	O
(	O
some	O
function	O
of	O
)	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
.	O
unfortunately	O
,	O
the	O
2d	O
case	O
is	O
provably	O
much	O
harder	O
than	O
the	O
1d	O
case	O
,	O
and	O
we	O
must	O
resort	O
to	O
approximate	O
methods	O
,	O
as	O
we	O
discuss	O
in	O
later	O
chapters	O
.	O
although	O
the	O
potts	O
prior	O
is	O
adequate	O
for	O
regularizing	O
supervised	B
learning	I
problems	O
,	O
it	O
is	O
not	O
sufficiently	O
accurate	O
to	O
perform	O
image	B
segmentation	I
in	O
an	O
unsupervised	O
way	O
,	O
since	O
the	O
segments	O
produced	O
by	O
this	O
model	O
do	O
not	O
accurately	O
represent	O
the	O
kinds	O
of	O
segments	O
one	O
sees	O
in	O
natural	O
images	O
(	O
morris	O
et	O
al	O
.	O
1996	O
)	O
.6	O
for	O
the	O
unsupervised	O
case	O
,	O
one	O
needs	O
to	O
use	O
more	O
sophisticated	O
priors	O
,	O
such	O
as	O
the	O
truncated	O
gaussian	O
process	O
prior	O
of	O
(	O
sudderth	O
and	O
jordan	O
2008	O
)	O
.	O
19.4.4	O
gaussian	O
mrfs	O
an	O
undirected	B
ggm	O
,	O
also	O
called	O
a	O
gaussian	O
mrf	O
(	O
see	O
e.g.	O
,	O
(	O
rue	O
and	O
held	O
2005	O
)	O
)	O
,	O
is	O
a	O
pairwise	O
mrf	O
of	O
the	O
following	O
form	O
:	O
(	O
19.24	O
)	O
(	O
19.25	O
)	O
(	O
19.26	O
)	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
s∼t	O
p	O
(	O
y|θ	O
)	O
∝	O
ψst	O
(	O
ys	O
,	O
yt	O
)	O
ψt	O
(	O
yt	O
)	O
ψst	O
(	O
ys	O
,	O
yt	O
)	O
=	O
exp	O
(	O
−	O
1	O
2	O
ψt	O
(	O
yt	O
)	O
=	O
exp	O
(	O
−	O
1	O
2	O
t	O
ysλstyt	O
)	O
λtty2	O
t	O
+	O
ηtyt	O
)	O
(	O
note	O
that	O
we	O
could	O
easily	O
absorb	O
the	O
node	O
potentials	O
ψt	O
into	O
the	O
edge	O
potentials	O
,	O
but	O
we	O
have	O
kept	O
them	O
separate	O
for	O
clarity	O
.	O
)	O
the	O
joint	B
distribution	I
can	O
be	O
written	O
as	O
follows	O
:	O
p	O
(	O
y|θ	O
)	O
∝	O
exp	O
[	O
ηt	O
y	O
−	O
1	O
2	O
yt	O
λy	O
]	O
(	O
19.27	O
)	O
we	O
recognize	O
this	O
as	O
a	O
multivariate	O
gaussian	O
written	O
in	O
information	B
form	I
where	O
λ	O
=	O
σ−1	O
and	O
η	O
=	O
λμ	O
.	O
if	O
λst	O
=	O
0	O
,	O
then	O
there	O
is	O
no	O
pairwise	O
term	O
connecting	O
s	O
and	O
t	O
,	O
so	O
by	O
the	O
factorization	O
theorem	O
(	O
theorem	O
2.2.1	O
)	O
,	O
we	O
conclude	O
that	O
ys	O
⊥	O
yt|y−	O
(	O
st	O
)	O
⇐⇒	O
λst	O
=	O
0	O
(	O
19.28	O
)	O
the	O
zero	O
entries	O
in	O
λ	O
are	O
called	O
structural	B
zeros	I
,	O
since	O
they	O
represent	O
the	O
absent	O
edges	B
in	O
the	O
graph	B
.	O
thus	O
undirected	B
ggms	O
correspond	O
to	O
sparse	B
precision	O
matrices	O
,	O
a	O
fact	O
which	O
we	O
exploit	O
in	O
section	O
26.7.2	O
to	O
efficiently	O
learn	O
the	O
structure	O
of	O
the	O
graph	B
.	O
19.4.4.1	O
comparing	O
gaussian	O
dgms	O
and	O
ugms	O
*	O
in	O
section	O
10.2.5	O
,	O
we	O
saw	O
that	O
directed	B
ggms	O
correspond	O
to	O
sparse	B
regression	O
matrices	O
,	O
and	O
hence	O
sparse	B
cholesky	O
factorizations	O
of	O
covariance	B
matrices	O
,	O
whereas	O
undirected	B
ggms	O
correspond	O
to	O
6.	O
an	O
inﬂuential	O
paper	O
(	O
geman	O
and	O
geman	O
1984	O
)	O
,	O
which	O
introduced	O
the	O
idea	O
of	O
a	O
gibbs	O
sampler	O
(	O
section	O
24.2	O
)	O
,	O
proposed	O
using	O
the	O
potts	O
model	O
as	O
a	O
prior	O
for	O
image	B
segmentation	I
,	O
but	O
the	O
results	O
in	O
their	O
paper	O
are	O
misleading	O
because	O
they	O
did	O
not	O
run	O
their	O
gibbs	O
sampler	O
for	O
long	O
enough	O
.	O
see	O
figure	O
24.10	O
for	O
a	O
vivid	O
illustration	O
of	O
this	O
point	O
.	O
19.4.	O
examples	O
of	O
mrfs	O
673	O
figure	O
19.10	O
a	O
var	O
(	O
2	O
)	O
process	O
represented	O
as	O
a	O
dynamic	O
chain	O
graph	B
.	O
source	O
:	O
2000	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
rainer	O
dahlhaus	O
and	O
oxford	O
university	O
press	O
.	O
(	O
dahlhaus	O
and	O
eichler	O
sparse	B
precision	O
matrices	O
.	O
the	O
advantage	O
of	O
the	O
dag	O
formulation	O
is	O
that	O
we	O
can	O
make	O
the	O
regression	B
weights	O
w	O
,	O
and	O
hence	O
σ	O
,	O
be	O
conditional	O
on	O
covariate	O
information	B
(	O
pourahmadi	O
2004	O
)	O
,	O
without	O
worrying	O
about	O
positive	B
deﬁnite	I
constraints	O
.	O
the	O
disadavantage	O
of	O
the	O
dag	O
formulation	O
is	O
its	O
dependence	O
on	O
the	O
order	O
,	O
although	O
in	O
certain	O
domains	O
,	O
such	O
as	O
time	O
series	O
,	O
there	O
is	O
already	O
a	O
natural	O
ordering	O
of	O
the	O
variables	O
.	O
it	O
is	O
actually	O
possible	O
to	O
combine	O
both	O
representations	O
,	O
resulting	O
in	O
a	O
gaussian	O
chain	B
graph	I
.	O
for	O
example	O
,	O
consider	O
a	O
a	O
discrete-time	O
,	O
second-order	O
markov	O
chain	O
in	O
which	O
the	O
states	O
are	O
continuous	O
,	O
yt	O
∈	O
r	O
d.	O
the	O
transition	O
function	O
can	O
be	O
represented	O
as	O
a	O
(	O
vector-valued	O
)	O
linear-	O
gaussian	O
cpd	O
:	O
p	O
(	O
yt|yt−1	O
,	O
yt−2	O
,	O
θ	O
)	O
=	O
n	O
(	O
yt|a1yt−1	O
+	O
a2yt−2	O
,	O
σ	O
)	O
(	O
19.29	O
)	O
this	O
is	O
called	O
vector	B
auto-regressive	I
or	O
var	O
process	O
of	O
order	O
2.	O
such	O
models	O
are	O
widely	O
used	O
in	O
econometrics	O
for	O
time-series	B
forecasting	I
.	O
the	O
time	O
series	O
aspect	O
is	O
most	O
naturally	O
modeled	O
using	O
a	O
dgm	O
.	O
however	O
,	O
if	O
σ−1	O
is	O
sparse	B
,	O
then	O
the	O
correlation	O
amongst	O
the	O
components	O
within	O
a	O
time	O
slice	O
is	O
most	O
naturally	O
modeled	O
using	O
a	O
ugm	O
.	O
for	O
example	O
,	O
suppose	O
we	O
have	O
⎛	O
⎜⎜⎜⎜⎝	O
⎛	O
⎜⎜⎜⎜⎝	O
1	O
1	O
2	O
1	O
0	O
0	O
a1	O
=	O
and	O
σ	O
=	O
3	O
5	O
0	O
2	O
5	O
0	O
0	O
0	O
3	O
5	O
1	O
3	O
0	O
0	O
1	O
5	O
3	O
5	O
0	O
0	O
−	O
1	O
5	O
0	O
0	O
−	O
1	O
2	O
0	O
1	O
5	O
0	O
0	O
0	O
1	O
5	O
2	O
5	O
1	O
2	O
3	O
−	O
1	O
1	O
3	O
1	O
−	O
1	O
3	O
1	O
0	O
0	O
3	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
⎞	O
⎛	O
⎟⎟⎟⎟⎠	O
,	O
a2	O
=	O
⎜⎜⎜⎜⎝	O
⎛	O
⎞	O
⎜⎜⎜⎜⎝	O
⎟⎟⎟⎟⎠	O
,	O
σ−1	O
=	O
⎞	O
⎟⎟⎟⎟⎠	O
0	O
−	O
1	O
5	O
0	O
0	O
0	O
0	O
1	O
0	O
5	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
−	O
1	O
3	O
5	O
2.13	O
−1.47	O
−1.2	O
−1.47	O
1.2	O
−1.2	O
1.8	O
0	O
0	O
0	O
0	O
2.13	O
1.2	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
(	O
19.30	O
)	O
(	O
19.31	O
)	O
⎞	O
⎟⎟⎟⎟⎠	O
0	O
0	O
0	O
0	O
1	O
674	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
x1	O
w1	O
x1	O
x2	O
(	O
a	O
)	O
x2	O
(	O
b	O
)	O
x3	O
w2	O
x3	O
figure	O
19.11	O
based	O
on	O
figures	O
5.12-5.13	O
of	O
(	O
choi	O
2011	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
myung	O
choi	O
.	O
(	O
a	O
)	O
a	O
bi-directed	B
graph	I
.	O
(	O
b	O
)	O
the	O
equivalent	O
dag	O
.	O
here	O
the	O
w	O
nodes	B
are	O
latent	B
confounders	O
.	O
the	O
resulting	O
graphical	B
model	I
is	O
illustrated	O
in	O
figure	O
19.10.	O
zeros	O
in	O
the	O
transition	O
matrices	O
a1	O
and	O
a2	O
correspond	O
to	O
absent	O
directed	B
arcs	O
from	O
yt−1	O
and	O
yt−2	O
into	O
yt	O
.	O
zeros	O
in	O
the	O
precision	B
matrix	I
σ−1	O
correspond	O
to	O
absent	O
undirected	B
arcs	O
between	O
nodes	B
in	O
yt	O
.	O
sometimes	O
we	O
have	O
a	O
sparse	B
covariance	O
matrix	O
rather	O
than	O
a	O
sparse	B
precision	O
matrix	O
.	O
this	O
can	O
be	O
represented	O
using	O
a	O
bi-directed	B
graph	I
,	O
where	O
each	O
edge	O
has	O
arrows	O
in	O
both	O
directions	O
,	O
as	O
in	O
figure	O
19.11	O
(	O
a	O
)	O
.	O
here	O
nodes	O
that	O
are	O
not	O
connected	O
are	O
unconditionally	B
independent	I
.	O
for	O
example	O
in	O
figure	O
19.11	O
(	O
a	O
)	O
we	O
see	O
that	O
y1	O
⊥	O
y3	O
.	O
in	O
the	O
gaussian	O
case	O
,	O
this	O
means	O
σ1,3	O
=	O
σ3,1	O
=	O
0	O
.	O
(	O
a	O
graph	B
representing	O
a	O
sparse	B
covariance	O
matrix	O
is	O
called	O
a	O
covariance	B
graph	I
.	O
)	O
by	O
contrast	O
,	O
if	O
this	O
were	O
an	O
undirected	B
model	O
,	O
we	O
would	O
have	O
that	O
y1	O
⊥	O
y3|y2	O
,	O
and	O
λ1,3	O
=	O
λ3,1	O
=	O
0	O
,	O
where	O
λ	O
=	O
σ−1	O
.	O
a	O
bidirected	O
graph	B
can	O
be	O
converted	O
to	O
a	O
dag	O
with	O
latent	B
variables	O
,	O
where	O
each	O
bidirected	O
edge	O
is	O
replaced	O
with	O
a	O
hidden	B
variable	I
representing	O
a	O
hidden	B
common	O
cause	O
,	O
or	O
confounder	B
,	O
as	O
illustrated	O
in	O
figure	O
19.11	O
(	O
b	O
)	O
.	O
the	O
relevant	O
ci	O
properties	O
can	O
then	O
be	O
determined	O
using	O
d-	O
separation	O
.	O
we	O
can	O
combine	O
bidirected	O
and	O
directed	B
edges	O
to	O
get	O
a	O
directed	B
mixed	I
graphical	I
model	I
.	O
this	O
is	O
useful	O
for	O
representing	O
a	O
variety	O
of	O
models	O
,	O
such	O
as	O
arma	O
models	O
(	O
section	O
18.2.4.4	O
)	O
,	O
structural	B
equation	I
models	I
(	O
section	O
26.5.5	O
)	O
,	O
etc	O
.	O
19.4.5	O
markov	O
logic	O
networks	O
*	O
in	O
section	O
10.2.2	O
,	O
we	O
saw	O
how	O
we	O
could	O
“	O
unroll	O
”	O
markov	O
models	O
and	O
hmms	O
for	O
an	O
arbitrary	O
number	O
of	O
time	O
steps	O
in	O
order	O
to	O
model	O
variable-length	O
sequences	O
.	O
similarly	O
,	O
in	O
section	O
19.4.1	O
,	O
we	O
saw	O
how	O
we	O
could	O
expand	O
a	O
lattice	B
ugm	O
to	O
model	O
images	O
of	O
any	O
size	O
.	O
what	O
about	O
more	O
complex	O
domains	O
,	O
where	O
we	O
have	O
a	O
variable	O
number	O
of	O
objects	O
and	O
relationships	O
between	O
them	O
?	O
creating	O
models	O
for	O
such	O
scenarios	O
is	O
often	O
done	O
using	O
ﬁrst-order	B
logic	I
(	O
see	O
e.g.	O
,	O
(	O
russell	O
and	O
norvig	O
2010	O
)	O
)	O
.	O
for	O
example	O
,	O
consider	O
the	O
sentences	O
“	O
smoking	O
causes	O
cancer	O
”	O
and	O
“	O
if	O
two	O
people	O
are	O
friends	O
,	O
and	O
one	O
smokes	O
,	O
then	O
so	O
does	O
the	O
other	O
”	O
.	O
we	O
can	O
write	O
these	O
sentences	O
in	O
ﬁrst-order	O
19.4.	O
examples	O
of	O
mrfs	O
675	O
friends	O
(	O
a	O
,	O
b	O
)	O
friends	O
(	O
a	O
,	O
a	O
)	O
smokes	O
(	O
a	O
)	O
smokes	O
(	O
b	O
)	O
friends	O
(	O
b	O
,	O
b	O
)	O
cancer	O
(	O
a	O
)	O
cancer	O
(	O
b	O
)	O
friends	O
(	O
b	O
,	O
a	O
)	O
figure	O
19.12	O
an	O
example	O
of	O
a	O
ground	O
markov	O
logic	O
network	O
represented	O
as	O
a	O
pairwise	O
mrf	O
for	O
2	O
people	O
.	O
based	O
on	O
figure	O
2.1	O
from	O
(	O
domingos	O
and	O
lowd	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
pedro	O
domingos	O
.	O
logic	O
as	O
follows	O
:	O
∀x.sm	O
(	O
x	O
)	O
=⇒	O
ca	O
(	O
x	O
)	O
∀x.∀y.f	O
r	O
(	O
x	O
,	O
y	O
)	O
∧	O
sm	O
(	O
x	O
)	O
=⇒	O
sm	O
(	O
y	O
)	O
(	O
19.32	O
)	O
(	O
19.33	O
)	O
where	O
sm	O
and	O
ca	O
are	O
predicates	O
,	O
and	O
f	O
r	O
is	O
a	O
relation.7	O
of	O
course	O
,	O
such	O
rules	B
are	O
not	O
always	O
true	O
.	O
indeed	O
,	O
this	O
brittleness	O
is	O
the	O
main	O
reason	O
why	O
logical	O
approaches	O
to	O
ai	O
are	O
no	O
longer	O
widely	O
used	O
,	O
at	O
least	O
not	O
in	O
their	O
pure	B
form	O
.	O
there	O
have	O
been	O
a	O
variety	O
of	O
attempts	O
to	O
combine	O
ﬁrst	O
order	O
logic	O
with	O
probability	O
theory	O
,	O
an	O
area	O
known	O
as	O
statistical	O
relational	O
ai	O
or	O
probabilistic	B
relational	I
modeling	I
(	O
kersting	O
et	O
al	O
.	O
2011	O
)	O
.	O
one	O
simple	O
approach	O
is	O
to	O
take	O
logical	O
rules	O
and	O
attach	O
weights	O
(	O
known	O
as	O
certainty	B
factors	I
)	O
to	O
them	O
,	O
and	O
then	O
to	O
interpret	O
them	O
as	O
conditional	B
probability	I
distributions	O
.	O
for	O
example	O
,	O
we	O
might	O
say	O
p	O
(	O
ca	O
(	O
x	O
)	O
=	O
1|sm	O
(	O
x	O
)	O
=	O
1	O
)	O
=	O
0.9.	O
unfortunately	O
,	O
the	O
rule	O
does	O
not	O
say	O
what	O
to	O
predict	O
if	O
sm	O
(	O
x	O
)	O
=	O
0.	O
furthermore	O
,	O
combining	O
cpds	O
in	O
this	O
way	O
is	O
not	O
guaranteed	O
to	O
deﬁne	O
a	O
consistent	B
joint	O
distribution	O
,	O
because	O
the	O
resulting	O
graph	B
may	O
not	O
be	O
a	O
dag	O
.	O
an	O
alternative	O
approach	O
is	O
to	O
treat	O
these	O
rules	B
as	O
a	O
way	O
of	O
deﬁning	O
potential	O
functions	O
in	O
an	O
unrolled	B
ugm	O
.	O
the	O
result	O
is	O
known	O
as	O
a	O
markov	O
logic	O
network	O
(	O
domingos	O
and	O
lowd	O
2009	O
)	O
.	O
to	O
specify	O
the	O
network	O
,	O
we	O
ﬁrst	O
rewrite	O
all	O
the	O
rules	B
in	O
conjunctive	B
normal	I
form	I
(	O
cnf	O
)	O
,	O
also	O
known	O
as	O
clausal	B
form	I
.	O
in	O
this	O
case	O
,	O
we	O
get	O
¬sm	O
(	O
x	O
)	O
∨	O
ca	O
(	O
x	O
)	O
¬f	O
r	O
(	O
x	O
,	O
y	O
)	O
∨	O
¬sm	O
(	O
x	O
)	O
∨	O
sm	O
(	O
y	O
)	O
(	O
19.34	O
)	O
(	O
19.35	O
)	O
the	O
ﬁrst	O
clause	B
can	O
be	O
read	O
as	O
“	O
either	O
x	O
does	O
not	O
smoke	O
or	O
he	O
has	O
cancer	O
”	O
,	O
which	O
is	O
logically	O
equivalent	O
to	O
equation	O
19.32	O
.	O
(	O
note	O
that	O
in	O
a	O
clause	B
,	O
any	O
unbound	O
variable	O
,	O
such	O
as	O
x	O
,	O
is	O
assumed	O
to	O
be	O
universally	O
quantiﬁed	O
.	O
)	O
7.	O
a	O
predicate	O
is	O
just	O
a	O
function	O
of	O
one	O
argument	O
,	O
known	O
as	O
an	O
object	O
,	O
that	O
evaluates	O
to	O
true	O
or	O
false	O
,	O
depending	O
on	O
whether	O
the	O
property	O
holds	O
or	O
not	O
of	O
that	O
object	O
.	O
a	O
(	O
logical	O
)	O
relation	B
is	O
just	O
a	O
function	O
of	O
two	O
or	O
more	O
arguments	O
(	O
objects	O
)	O
that	O
evaluates	O
to	O
true	O
or	O
false	O
,	O
depending	O
on	O
whether	O
the	O
relationship	O
holds	O
between	O
that	O
set	O
of	O
objects	O
or	O
not	O
.	O
676	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
inference	B
in	O
ﬁrst-order	B
logic	I
is	O
only	O
semi-decidable	O
,	O
so	O
it	O
is	O
common	O
to	O
use	O
a	O
restricted	O
subset	O
.	O
a	O
common	O
approach	O
(	O
as	O
used	O
in	O
prolog	O
)	O
is	O
to	O
restrict	O
the	O
language	O
to	O
horn	O
clauses	O
,	O
which	O
are	O
clauses	O
that	O
contain	O
at	O
most	O
one	O
positive	O
literal	O
.	O
essentially	O
this	O
means	O
the	O
model	O
is	O
a	O
series	O
of	O
if-then	O
rules	B
,	O
where	O
the	O
right	O
hand	O
side	O
of	O
the	O
rules	B
(	O
the	O
“	O
then	O
”	O
part	O
,	O
or	O
consequence	O
)	O
has	O
only	O
a	O
single	O
term	O
.	O
once	O
we	O
have	O
encoded	O
our	O
knowledge	B
base	I
as	O
a	O
set	O
of	O
clauses	O
,	O
we	O
can	O
attach	O
weights	O
to	O
each	O
one	O
;	O
these	O
weights	O
are	O
the	O
parameter	B
of	O
the	O
model	O
,	O
and	O
they	O
deﬁne	O
the	O
clique	B
potentials	O
as	O
follows	O
:	O
ψc	O
(	O
xc	O
)	O
=	O
exp	O
(	O
wcφc	O
(	O
xc	O
)	O
)	O
(	O
19.36	O
)	O
where	O
φc	O
(	O
xc	O
)	O
is	O
a	O
logical	O
expression	O
which	O
evaluates	O
clause	B
c	O
applied	O
to	O
the	O
variables	O
xc	O
,	O
and	O
wc	O
is	O
the	O
weight	O
we	O
attach	O
to	O
this	O
clause	B
.	O
roughly	O
speaking	O
,	O
the	O
weight	O
of	O
a	O
clause	B
speciﬁes	O
the	O
probability	O
of	O
a	O
world	O
in	O
which	O
this	O
clause	B
is	O
satsiﬁed	O
relative	O
to	O
a	O
world	O
in	O
which	O
it	O
is	O
not	O
satisﬁed	O
.	O
now	O
suppose	O
there	O
are	O
two	O
objects	O
(	O
people	O
)	O
in	O
the	O
world	O
,	O
anna	O
and	O
bob	O
,	O
which	O
we	O
will	O
denote	O
by	O
constant	B
symbols	I
a	O
and	O
b.	O
we	O
can	O
make	O
a	O
ground	B
network	I
from	O
the	O
above	O
clauses	O
by	O
creating	O
binary	O
random	O
variables	O
sx	O
,	O
cx	O
,	O
and	O
fx	O
,	O
y	O
for	O
x	O
,	O
y	O
∈	O
{	O
a	O
,	O
b	O
}	O
,	O
and	O
then	O
“	O
wiring	O
these	O
up	O
”	O
according	O
to	O
the	O
clauses	O
above	O
.	O
the	O
result	O
is	O
the	O
ugm	O
in	O
figure	O
19.12	O
with	O
8	O
binary	O
nodes	O
.	O
note	O
that	O
we	O
have	O
not	O
encoded	O
the	O
fact	O
that	O
f	O
r	O
is	O
a	O
symmetric	B
relation	O
,	O
so	O
f	O
r	O
(	O
a	O
,	O
b	O
)	O
and	O
f	O
r	O
(	O
b	O
,	O
a	O
)	O
might	O
have	O
different	O
values	O
.	O
similarly	O
,	O
we	O
have	O
the	O
“	O
degenerate	B
”	O
nodes	B
f	O
r	O
(	O
a	O
,	O
a	O
)	O
and	O
f	O
r	O
(	O
b	O
,	O
b	O
)	O
,	O
since	O
we	O
did	O
not	O
enforce	O
x	O
(	O
cid:4	O
)	O
=	O
y	O
in	O
equation	O
19.33	O
.	O
(	O
if	O
we	O
add	O
such	O
constraints	O
,	O
then	O
the	O
model	O
compiler	O
,	O
which	O
generates	O
the	O
ground	B
network	I
,	O
could	O
avoid	O
creating	O
redundant	O
nodes	B
.	O
)	O
in	O
summary	O
,	O
we	O
can	O
think	O
of	O
mlns	O
as	O
a	O
convenient	O
way	O
of	O
specifying	O
a	O
ugm	O
template	B
,	O
that	O
can	O
get	O
unrolled	B
to	O
handle	O
data	O
of	O
arbitrary	O
size	O
.	O
there	O
are	O
several	O
other	O
ways	O
to	O
deﬁne	O
relational	B
probabilistic	I
models	I
;	O
see	O
e.g.	O
,	O
(	O
koller	O
and	O
friedman	O
2009	O
;	O
kersting	O
et	O
al	O
.	O
2011	O
)	O
for	O
details	O
.	O
in	O
some	O
cases	O
,	O
there	O
is	O
uncertainty	B
about	O
the	O
number	O
or	O
existence	O
of	O
objects	O
or	O
relations	O
(	O
the	O
so-called	O
open	B
universe	I
problem	O
)	O
.	O
section	O
18.6.2	O
gives	O
a	O
concrete	O
example	O
in	O
the	O
context	O
of	O
multi-object	O
tracking	B
.	O
see	O
e.g.	O
,	O
(	O
russell	O
and	O
norvig	O
2010	O
;	O
kersting	O
et	O
al	O
.	O
2011	O
)	O
and	O
references	O
therein	O
for	O
further	O
details	O
.	O
19.5	O
learning	B
in	O
this	O
section	O
,	O
we	O
discuss	O
how	O
to	O
perform	O
ml	O
and	O
map	O
parameter	B
estimation	O
for	O
mrfs	O
.	O
we	O
will	O
see	O
that	O
this	O
is	O
quite	O
computationally	O
expensive	O
.	O
for	O
this	O
reason	O
,	O
it	O
is	O
rare	O
to	O
perform	O
bayesian	O
inference	B
for	O
the	O
parameters	O
of	O
mrfs	O
(	O
although	O
see	O
(	O
qi	O
et	O
al	O
.	O
2005	O
)	O
)	O
.	O
19.5.1	O
training	O
maxent	O
models	O
using	O
gradient	O
methods	O
consider	O
an	O
mrf	O
in	O
log-linear	B
form	O
:	O
p	O
(	O
y|θ	O
)	O
=	O
1	O
z	O
(	O
θ	O
)	O
exp	O
θt	O
c	O
φc	O
(	O
y	O
)	O
(	O
19.37	O
)	O
(	O
cid:11	O
)	O
(	O
cid:10	O
)	O
(	O
cid:4	O
)	O
c	O
19.5.	O
learning	B
where	O
c	O
indexes	O
the	O
cliques	B
.	O
the	O
scaled	O
log-likelihood	O
is	O
given	O
by	O
c	O
φc	O
(	O
yi	O
)	O
−	O
log	O
z	O
(	O
θ	O
)	O
θt	O
log	O
p	O
(	O
yi|θ	O
)	O
=	O
(	O
cid:2	O
)	O
(	O
θ	O
)	O
(	O
cid:2	O
)	O
1	O
n	O
1	O
n	O
(	O
cid:4	O
)	O
i	O
(	O
cid:24	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
i	O
c	O
(	O
cid:25	O
)	O
677	O
(	O
19.38	O
)	O
since	O
mrfs	O
are	O
in	O
the	O
exponential	B
family	I
,	O
we	O
know	O
that	O
this	O
function	O
is	O
convex	B
in	O
θ	O
(	O
see	O
section	O
9.2.3	O
)	O
,	O
so	O
it	O
has	O
a	O
unique	O
global	O
maximum	O
which	O
we	O
can	O
ﬁnd	O
using	O
gradient-based	O
optimizers	O
.	O
in	O
particular	O
,	O
the	O
derivative	O
for	O
the	O
weights	O
of	O
a	O
particular	O
clique	B
,	O
c	O
,	O
is	O
given	O
by	O
(	O
cid:18	O
)	O
∂	O
(	O
cid:2	O
)	O
∂θc	O
=	O
1	O
n	O
φc	O
(	O
yi	O
)	O
−	O
∂	O
∂θc	O
log	O
z	O
(	O
θ	O
)	O
(	O
cid:17	O
)	O
(	O
cid:4	O
)	O
i	O
(	O
19.39	O
)	O
(	O
19.41	O
)	O
exercise	O
19.1	O
asks	O
you	O
to	O
show	O
that	O
the	O
derivative	O
of	O
the	O
log	B
partition	I
function	I
wrt	O
θc	O
is	O
the	O
expectation	O
of	O
the	O
c	O
’	O
th	O
feature	O
under	O
the	O
model	O
,	O
i.e.	O
,	O
φc	O
(	O
y	O
)	O
p	O
(	O
y|θ	O
)	O
∂	O
log	O
z	O
(	O
θ	O
)	O
(	O
cid:4	O
)	O
(	O
19.40	O
)	O
∂θc	O
(	O
cid:24	O
)	O
=	O
e	O
[	O
φc	O
(	O
y	O
)	O
|θ	O
]	O
=	O
(	O
cid:4	O
)	O
(	O
cid:25	O
)	O
y	O
hence	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	O
is	O
−	O
e	O
[	O
φc	O
(	O
y	O
)	O
]	O
φc	O
(	O
yi	O
)	O
=	O
∂	O
(	O
cid:2	O
)	O
∂θc	O
1	O
n	O
i	O
in	O
the	O
ﬁrst	O
term	O
,	O
we	O
ﬁx	O
y	O
to	O
its	O
observed	O
values	O
;	O
this	O
is	O
sometimes	O
called	O
the	O
clamped	B
term	I
.	O
in	O
the	O
second	O
term	O
,	O
y	O
is	O
free	O
;	O
this	O
is	O
sometimes	O
called	O
the	O
unclamped	B
term	I
or	O
contrastive	B
term	I
.	O
note	O
that	O
computing	O
the	O
unclamped	B
term	I
requires	O
inference	B
in	O
the	O
model	O
,	O
and	O
this	O
must	O
be	O
done	O
once	O
per	O
gradient	O
step	O
.	O
this	O
makes	O
ugm	O
training	O
much	O
slower	O
than	O
dgm	O
training	O
.	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	O
can	O
be	O
rewritten	O
as	O
the	O
expected	O
feature	O
vector	O
according	O
to	O
the	O
empirical	B
distribution	I
minus	O
the	O
model	O
’	O
s	O
expectation	O
of	O
the	O
feature	O
vector	O
:	O
∂	O
(	O
cid:2	O
)	O
∂θc	O
=	O
epemp	O
[	O
φc	O
(	O
y	O
)	O
]	O
−	O
ep	O
(	O
·|θ	O
)	O
[	O
φc	O
(	O
y	O
)	O
]	O
(	O
19.42	O
)	O
at	O
the	O
optimum	O
,	O
the	O
gradient	O
will	O
be	O
zero	O
,	O
so	O
the	O
empirical	B
distribution	I
of	O
the	O
features	B
will	O
match	O
the	O
model	O
’	O
s	O
predictions	O
:	O
epemp	O
[	O
φc	O
(	O
y	O
)	O
]	O
=	O
ep	O
(	O
·|θ	O
)	O
[	O
φc	O
(	O
y	O
)	O
]	O
(	O
19.43	O
)	O
this	O
is	O
called	O
moment	B
matching	I
.	O
this	O
observation	B
motivates	O
a	O
different	O
optimization	B
algorithm	O
which	O
we	O
discuss	O
in	O
section	O
19.5.7	O
.	O
19.5.2	O
training	O
partially	O
observed	O
maxent	O
models	O
suppose	O
we	O
have	O
missing	B
data	I
and/or	O
hidden	B
variables	I
in	O
our	O
model	O
.	O
represent	O
such	O
models	O
as	O
follows	O
:	O
in	O
general	O
,	O
we	O
can	O
θt	O
c	O
φc	O
(	O
h	O
,	O
y	O
)	O
)	O
(	O
19.44	O
)	O
p	O
(	O
y	O
,	O
h|θ	O
)	O
=	O
1	O
z	O
(	O
θ	O
)	O
exp	O
(	O
(	O
cid:4	O
)	O
c	O
678	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
the	O
log	O
likelihood	O
has	O
the	O
form	O
(	O
cid:2	O
)	O
(	O
θ	O
)	O
=	O
1	O
n	O
where	O
(	O
cid:10	O
)	O
(	O
cid:4	O
)	O
hi	O
(	O
cid:11	O
)	O
(	O
cid:4	O
)	O
i	O
=	O
1	O
n	O
(	O
cid:10	O
)	O
log	O
1	O
z	O
(	O
θ	O
)	O
(	O
cid:4	O
)	O
hi	O
(	O
cid:11	O
)	O
˜p	O
(	O
yi	O
,	O
hi|θ	O
)	O
(	O
19.45	O
)	O
p	O
(	O
yi	O
,	O
hi|θ	O
)	O
(	O
cid:11	O
)	O
log	O
(	O
cid:4	O
)	O
(	O
cid:10	O
)	O
(	O
cid:4	O
)	O
i	O
˜p	O
(	O
y	O
,	O
h|θ	O
)	O
(	O
cid:2	O
)	O
exp	O
θt	O
c	O
φc	O
(	O
h	O
,	O
y	O
)	O
(	O
19.46	O
)	O
hi	O
˜p	O
(	O
yi	O
,	O
hi|θ	O
)	O
is	O
the	O
same	O
as	O
the	O
partition	B
function	I
is	O
the	O
unnormalized	O
distribution	O
.	O
the	O
term	O
for	O
the	O
whole	O
model	O
,	O
except	O
that	O
y	O
is	O
ﬁxed	O
at	O
yi	O
.	O
hence	O
the	O
gradient	O
is	O
just	O
the	O
expected	O
features	O
where	O
we	O
clamp	O
yi	O
,	O
but	O
average	O
over	O
h	O
:	O
c	O
(	O
cid:7	O
)	O
(	O
cid:10	O
)	O
(	O
cid:4	O
)	O
hi	O
∂	O
∂θc	O
log	O
(	O
cid:11	O
)	O
˜p	O
(	O
yi	O
,	O
hi|θ	O
)	O
(	O
cid:4	O
)	O
so	O
the	O
overall	O
gradient	O
is	O
given	O
by	O
=	O
e	O
[	O
φc	O
(	O
h	O
,	O
yi	O
)	O
|θ	O
]	O
∂	O
(	O
cid:2	O
)	O
∂θc	O
=	O
1	O
n	O
i	O
{	O
e	O
[	O
φc	O
(	O
h	O
,	O
yi	O
)	O
|θ	O
]	O
−	O
e	O
[	O
φc	O
(	O
h	O
,	O
y	O
)	O
|θ	O
]	O
}	O
(	O
19.47	O
)	O
(	O
19.48	O
)	O
the	O
ﬁrst	O
set	O
of	O
expectations	O
are	O
computed	O
by	O
“	O
clamping	B
”	O
the	O
visible	B
nodes	I
to	O
their	O
observed	O
values	O
,	O
and	O
the	O
second	O
set	O
are	O
computed	O
by	O
letting	O
the	O
visible	B
nodes	I
be	O
free	O
.	O
in	O
both	O
cases	O
,	O
we	O
marginalize	O
over	O
hi	O
.	O
an	O
alternative	O
approach	O
is	O
to	O
use	O
generalized	O
em	O
,	O
where	O
we	O
use	O
gradient	O
methods	O
in	O
the	O
m	O
step	O
.	O
see	O
(	O
koller	O
and	O
friedman	O
2009	O
,	O
p956	O
)	O
for	O
details	O
.	O
19.5.3	O
approximate	O
methods	O
for	O
computing	O
the	O
mles	O
of	O
mrfs	O
when	O
ﬁtting	O
a	O
ugm	O
there	O
is	O
(	O
in	O
general	O
)	O
no	O
closed	O
form	O
solution	O
for	O
the	O
ml	O
or	O
the	O
map	O
estimate	O
of	O
the	O
parameters	O
,	O
so	O
we	O
need	O
to	O
use	O
gradient-based	O
optimizers	O
.	O
this	O
gradient	O
requires	O
inference	B
.	O
in	O
models	O
where	O
inference	B
is	O
intractable	O
,	O
learning	B
also	O
becomes	O
intractable	O
.	O
this	O
has	O
motivated	O
various	O
computationally	O
faster	O
alternatives	O
to	O
ml/map	O
estimation	O
,	O
which	O
we	O
list	O
in	O
table	O
19.1.	O
we	O
dicsuss	O
some	O
of	O
these	O
alternatives	O
below	O
,	O
and	O
defer	O
others	O
to	O
later	O
sections	O
.	O
19.5.4	O
pseudo	B
likelihood	I
one	O
alternative	O
to	O
mle	O
is	O
to	O
maximize	O
the	O
pseudo	B
likelihood	I
(	O
besag	O
1975	O
)	O
,	O
deﬁned	O
as	O
follows	O
:	O
(	O
cid:2	O
)	O
p	O
l	O
(	O
θ	O
)	O
(	O
cid:2	O
)	O
pemp	O
(	O
y	O
log	O
p	O
(	O
yd|y−d	O
)	O
=	O
1	O
n	O
log	O
p	O
(	O
yid|yi	O
,	O
−d	O
,	O
θ	O
)	O
(	O
19.49	O
)	O
n	O
(	O
cid:4	O
)	O
d	O
(	O
cid:4	O
)	O
i=1	O
d=1	O
that	O
is	O
,	O
we	O
optimize	O
the	O
product	O
of	O
the	O
full	B
conditionals	O
,	O
also	O
known	O
as	O
the	O
composite	O
likeli-	O
hood	O
(	O
lindsay	O
1988	O
)	O
,	O
compare	O
this	O
to	O
the	O
objective	O
for	O
maximum	O
likelihood	O
:	O
(	O
cid:2	O
)	O
m	O
l	O
(	O
θ	O
)	O
=	O
pemp	O
(	O
y	O
log	O
p	O
(	O
y|θ	O
)	O
=	O
log	O
p	O
(	O
yi|θ	O
)	O
(	O
19.50	O
)	O
y	O
,	O
x	O
i=1	O
n	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
d	O
(	O
cid:4	O
)	O
y	O
d=1	O
(	O
cid:4	O
)	O
19.5.	O
learning	B
679	O
method	O
closed	O
form	O
ipf	O
gradient-based	O
optimization	B
max-margin	O
training	O
pseudo-likelihood	O
stochastic	O
ml	O
contrastive	B
divergence	I
minimum	O
probability	O
ﬂow	O
restriction	O
only	O
chordal	B
mrf	O
only	O
tabular	O
/	O
gaussian	O
mrf	O
low	O
tree	O
width	O
only	O
crfs	O
no	O
hidden	O
variables	O
-	O
-	O
can	O
integrate	B
out	I
the	O
hiddens	O
exact	O
mle	O
?	O
exact	O
exact	O
exact	O
n/a	O
approximate	O
exact	O
(	O
up	O
to	O
mc	O
error	O
)	O
approximate	O
approximate	O
section	O
section	O
19.5.7.4	O
section	O
19.5.7	O
section	O
19.5.1	O
section	O
19.7	O
section	O
19.5.4	O
section	O
19.5.5	O
section	O
27.7.2.4	O
sohl-dickstein	O
et	O
al	O
.	O
(	O
2011	O
)	O
table	O
19.1	O
some	O
methods	O
that	O
can	O
be	O
used	O
to	O
compute	O
approximate	O
ml/	O
map	O
parameter	B
estimates	O
for	O
mrfs/	O
crfs	O
.	O
low	O
tree-width	O
means	O
that	O
,	O
in	O
order	O
for	O
the	O
method	O
to	O
be	O
efficient	O
,	O
the	O
graph	B
must	O
“	O
tree-like	O
”	O
;	O
see	O
section	O
20.5	O
for	O
details	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
19.13	O
(	O
a	O
)	O
a	O
small	O
2d	O
lattice	B
.	O
observed	O
neighbors	O
.	O
based	O
on	O
figure	O
2.2	O
of	O
(	O
carbonetto	O
2003	O
)	O
.	O
(	O
b	O
)	O
the	O
representation	O
used	O
by	O
pseudo	B
likelihood	I
.	O
solid	O
nodes	B
are	O
in	O
the	O
case	O
of	O
gaussian	O
mrfs	O
,	O
pl	O
is	O
equivalent	O
to	O
ml	O
(	O
besag	O
1975	O
)	O
,	O
but	O
this	O
is	O
not	O
true	O
in	O
general	O
(	O
liang	O
and	O
jordan	O
2008	O
)	O
.	O
the	O
pl	O
approach	O
is	O
illustrated	O
in	O
figure	O
19.13	O
for	O
a	O
2d	O
grid	O
.	O
we	O
learn	O
to	O
predict	O
each	O
node	O
,	O
given	O
all	O
of	O
its	O
neighbors	B
.	O
this	O
objective	O
is	O
generally	O
fast	O
to	O
compute	O
since	O
each	O
full	B
conditional	I
p	O
(	O
yid|yi	O
,	O
−d	O
,	O
θ	O
)	O
only	O
requires	O
summing	O
over	O
the	O
states	O
of	O
a	O
single	O
node	O
,	O
yid	O
,	O
in	O
order	O
to	O
compute	O
the	O
local	O
normalization	O
constant	O
.	O
the	O
pl	O
approach	O
is	O
similar	B
to	O
ﬁtting	O
each	O
full	B
conditional	I
separately	O
(	O
which	O
is	O
the	O
method	O
used	O
to	O
train	O
dependency	B
networks	I
,	O
discussed	O
in	O
section	O
26.2.2	O
)	O
,	O
except	O
that	O
the	O
parameters	O
are	O
tied	B
between	O
adjacent	O
nodes	B
.	O
one	O
problem	O
with	O
pl	O
is	O
that	O
it	O
is	O
hard	O
to	O
apply	O
to	O
models	O
with	O
hidden	B
variables	I
(	O
parise	O
and	O
welling	O
2005	O
)	O
.	O
another	O
more	O
subtle	O
problem	O
is	O
that	O
each	O
node	O
assumes	O
that	O
its	O
neighbors	B
have	O
if	O
node	O
t	O
∈	O
nbr	O
(	O
s	O
)	O
is	O
a	O
perfect	O
predictor	O
for	O
node	O
s	O
,	O
then	O
s	O
will	O
learn	O
to	O
rely	O
known	O
values	O
.	O
completely	O
on	O
node	O
t	O
,	O
even	O
at	O
the	O
expense	O
of	O
ignoring	O
other	O
potentially	O
useful	O
information	B
,	O
such	O
as	O
its	O
local	B
evidence	I
.	O
however	O
,	O
experiments	O
in	O
(	O
parise	O
and	O
welling	O
2005	O
;	O
hoeﬂing	O
and	O
tibshirani	O
2009	O
)	O
suggest	O
that	O
pl	O
works	O
as	O
well	O
as	O
exact	O
ml	O
for	O
fully	O
observed	O
ising	O
models	O
,	O
and	O
of	O
course	O
pl	O
is	O
much	O
faster	O
.	O
19.5.5	O
stochastic	B
maximum	I
likelihood	I
recall	O
that	O
the	O
gradient	O
of	O
the	O
log-likelihood	O
for	O
a	O
fully	O
observed	O
mrf	O
is	O
given	O
by	O
(	O
cid:4	O
)	O
i	O
∇θ	O
(	O
cid:2	O
)	O
(	O
θ	O
)	O
=	O
1	O
n	O
[	O
φ	O
(	O
yi	O
)	O
−	O
e	O
[	O
φ	O
(	O
y	O
)	O
]	O
]	O
(	O
19.51	O
)	O
680	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
the	O
gradient	O
for	O
a	O
partially	O
observed	O
mrf	O
is	O
similar	B
.	O
in	O
both	O
cases	O
,	O
we	O
can	O
approximate	O
the	O
model	O
expectations	O
using	O
monte	O
carlo	O
sampling	O
.	O
we	O
can	O
combine	O
this	O
with	O
stochastic	B
gradient	I
descent	I
(	O
section	O
8.5.2	O
)	O
,	O
which	O
takes	O
samples	B
from	O
the	O
empirical	B
distribution	I
.	O
pseudocode	O
for	O
the	O
resulting	O
method	O
is	O
shown	O
in	O
algorithm	O
3.	O
algorithm	O
19.1	O
:	O
stochastic	B
maximum	I
likelihood	I
for	O
ﬁtting	O
an	O
mrf	O
1	O
initialize	O
weights	O
θ	O
randomly	O
;	O
2	O
k	O
=	O
0	O
,	O
η	O
=	O
1	O
;	O
3	O
for	O
each	O
epoch	B
do	O
4	O
for	O
each	O
minibatch	O
of	O
size	O
b	O
do	O
for	O
each	O
sample	O
s	O
=	O
1	O
:	O
s	O
do	O
sample	O
ys	O
,	O
k	O
∼	O
p	O
(	O
y|θk	O
)	O
;	O
s=1	O
φ	O
(	O
ys	O
,	O
k	O
)	O
;	O
gik	O
=	O
φ	O
(	O
yi	O
)	O
−	O
ˆe	O
(	O
φ	O
(	O
y	O
)	O
)	O
;	O
ˆe	O
(	O
φ	O
(	O
y	O
)	O
)	O
=	O
1	O
s	O
for	O
each	O
training	O
case	O
i	O
in	O
minibatch	O
do	O
(	O
cid:7	O
)	O
s	O
(	O
cid:7	O
)	O
gk	O
=	O
1	O
i∈b	O
gik	O
;	O
θk+1	O
=	O
θk	O
−	O
ηgk	O
;	O
b	O
k	O
=	O
k	O
+	O
1	O
;	O
decrease	O
step	B
size	I
η	O
;	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
13	O
typically	O
we	O
use	O
mcmc	O
to	O
generate	O
the	O
samples	B
.	O
of	O
course	O
,	O
running	O
mcmc	O
to	O
convergence	O
at	O
each	O
step	O
of	O
the	O
inner	O
loop	O
would	O
be	O
extremely	O
slow	O
.	O
fortunately	O
,	O
it	O
was	O
shown	O
in	O
(	O
younes	O
1989	O
)	O
that	O
we	O
can	O
start	O
the	O
mcmc	O
chain	O
at	O
its	O
previous	O
value	O
,	O
and	O
just	O
take	O
a	O
few	O
steps	O
.	O
in	O
otherwords	O
,	O
we	O
sample	O
ys	O
,	O
k	O
by	O
initializing	O
the	O
mcmc	O
chain	O
at	O
ys	O
,	O
k−1	O
,	O
and	O
then	O
run	O
for	O
a	O
few	O
iterations	O
.	O
this	O
is	O
valid	O
since	O
p	O
(	O
y|θk	O
)	O
is	O
likely	O
to	O
be	O
close	O
to	O
p	O
(	O
y|θk−1	O
)	O
,	O
since	O
we	O
only	O
changed	O
the	O
parameters	O
a	O
small	O
amount	O
.	O
we	O
call	O
this	O
algorithm	O
stochastic	B
maximum	I
likelihood	I
or	O
sml	O
.	O
(	O
there	O
is	O
a	O
closely	O
related	O
algorithm	O
called	O
persistent	B
contrastive	I
divergence	I
which	O
we	O
discuss	O
in	O
section	O
27.7.2.5	O
.	O
)	O
19.5.6	O
feature	B
induction	I
for	O
maxent	B
models	O
*	O
mrfs	O
require	O
a	O
good	O
set	O
of	O
features	B
.	O
one	O
unsupervised	O
way	O
to	O
learn	O
such	O
features	B
,	O
known	O
as	O
feature	B
induction	I
,	O
is	O
to	O
start	O
with	O
a	O
base	O
set	O
of	O
features	B
,	O
and	O
then	O
to	O
continually	O
create	O
new	O
feature	O
combinations	O
out	O
of	O
old	O
ones	O
,	O
greedily	O
adding	O
the	O
best	O
ones	O
to	O
the	O
model	O
.	O
this	O
approach	O
was	O
ﬁrst	O
proposed	O
in	O
(	O
pietra	O
et	O
al	O
.	O
1997	O
;	O
zhu	O
et	O
al	O
.	O
1997	O
)	O
,	O
and	O
was	O
later	O
extended	O
to	O
the	O
crf	O
case	O
in	O
(	O
mccallum	O
2003	O
)	O
.	O
to	O
illustrate	O
the	O
basic	O
idea	O
,	O
we	O
present	O
an	O
example	O
from	O
(	O
pietra	O
et	O
al	O
.	O
1997	O
)	O
,	O
which	O
described	O
how	O
to	O
build	O
unconditional	O
probabilistic	O
models	O
to	O
represent	O
english	O
spelling	O
.	O
initially	O
the	O
model	O
has	O
no	O
features	O
,	O
which	O
represents	O
the	O
uniform	B
distribution	I
.	O
the	O
algorithm	O
starts	O
by	O
choosing	O
to	O
add	O
the	O
feature	O
φ1	O
(	O
y	O
)	O
=	O
i	O
(	O
yt	O
∈	O
{	O
a	O
,	O
.	O
.	O
.	O
,	O
z	O
}	O
)	O
(	O
19.52	O
)	O
(	O
cid:4	O
)	O
t	O
19.5.	O
learning	B
681	O
which	O
checks	O
if	O
any	O
letter	O
is	O
lower	O
case	O
or	O
not	O
.	O
after	O
the	O
feature	O
is	O
added	O
,	O
the	O
parameters	O
are	O
(	O
re	O
)	O
-ﬁt	O
by	O
maximum	O
likelihood	O
.	O
for	O
this	O
feature	O
,	O
it	O
turns	O
out	O
that	O
ˆθ1	O
=	O
1.944	O
,	O
which	O
means	O
that	O
a	O
word	O
with	O
a	O
lowercase	O
letter	O
in	O
any	O
position	O
is	O
about	O
e1.944	O
≈	O
7	O
times	O
more	O
likely	O
than	O
the	O
same	O
word	O
without	O
a	O
lowercase	O
letter	O
in	O
that	O
position	O
.	O
some	O
samples	B
from	O
this	O
model	O
,	O
generated	O
using	O
(	O
annealed	O
)	O
gibbs	O
sampling	O
(	O
section	O
24.2	O
)	O
,	O
are	O
shown	O
below.8	O
m	O
,	O
r	O
,	O
xevo	O
,	O
ijjiir	O
,	O
b	O
,	O
to	O
,	O
jz	O
,	O
gsr	O
,	O
wq	O
,	O
vf	O
,	O
x	O
,	O
ga	O
,	O
msmgh	O
,	O
pcp	O
,	O
d	O
,	O
ozivlal	O
,	O
hzagh	O
,	O
yzop	O
,	O
io	O
,	O
advzmxnv	O
,	O
ijv_bolft	O
,	O
x	O
,	O
emx	O
,	O
kayerf	O
,	O
mlj	O
,	O
rawzyb	O
,	O
jp	O
,	O
ag	O
,	O
ctdnnnbg	O
,	O
wgdw	O
,	O
t	O
,	O
kguv	O
,	O
cy	O
,	O
spxcq	O
,	O
uzflbbf	O
,	O
dxtkkn	O
,	O
cxwx	O
,	O
jpd	O
,	O
ztzh	O
,	O
lv	O
,	O
zhpkvnu	O
,	O
l^	O
,	O
r	O
,	O
qee	O
,	O
nynrx	O
,	O
atze4n	O
,	O
ik	O
,	O
se	O
,	O
w	O
,	O
lrh	O
,	O
hp+	O
,	O
yrqyka	O
’	O
h	O
,	O
zcngotcnx	O
,	O
igcump	O
,	O
zjcjs	O
,	O
lqpwiqu	O
,	O
cefmfhc	O
,	O
o	O
,	O
lb	O
,	O
fdcy	O
,	O
tzby	O
,	O
yopxmvk	O
,	O
by	O
,	O
fz„	O
t	O
,	O
govyccm	O
,	O
ijyiduwfzo	O
,	O
6xr	O
,	O
duh	O
,	O
ejv	O
,	O
pk	O
,	O
pjw	O
,	O
l	O
,	O
fl	O
,	O
w	O
the	O
second	O
feature	O
added	O
by	O
the	O
algorithm	O
checks	O
if	O
two	O
adjacent	O
characters	O
are	O
lower	O
case	O
:	O
φ2	O
(	O
y	O
)	O
=	O
i	O
(	O
ys	O
∈	O
{	O
a	O
,	O
.	O
.	O
.	O
,	O
z	O
}	O
,	O
yt	O
∈	O
{	O
a	O
,	O
.	O
.	O
.	O
,	O
z	O
}	O
)	O
(	O
cid:4	O
)	O
s∼t	O
now	O
the	O
model	O
has	O
the	O
form	O
p	O
(	O
y	O
)	O
=	O
1	O
z	O
exp	O
(	O
θ1φ1	O
(	O
y	O
)	O
+θ	O
2φ2	O
(	O
y	O
)	O
)	O
(	O
19.53	O
)	O
(	O
19.54	O
)	O
continuing	O
in	O
this	O
way	O
,	O
the	O
algorithm	O
adds	O
features	B
for	O
the	O
strings	O
s	O
>	O
and	O
ing	O
>	O
,	O
where	O
>	O
represents	O
the	O
end	O
of	O
word	O
,	O
and	O
for	O
various	O
regular	B
expressions	O
such	O
as	O
[	O
0-9	O
]	O
,	O
etc	O
.	O
some	O
samples	B
from	O
the	O
model	O
with	O
1000	O
features	B
,	O
generated	O
using	O
(	O
annealed	O
)	O
gibbs	O
sampling	O
,	O
are	O
shown	O
below	O
.	O
was	O
,	O
reaser	O
,	O
in	O
,	O
there	O
,	O
to	O
,	O
will	O
,	O
„	O
was	O
,	O
by	O
,	O
homes	O
,	O
thing	O
,	O
be	O
,	O
reloverated	O
,	O
ther	O
,	O
which	O
,	O
conists	O
,	O
at	O
,	O
fores	O
,	O
anditing	O
,	O
with	O
,	O
mr.	O
,	O
proveral	O
,	O
the	O
,	O
„	O
***	O
,	O
on	O
’	O
t	O
,	O
prolling	O
,	O
prothere	O
,	O
„	O
mento	O
,	O
at	O
,	O
yaou	O
,	O
1	O
,	O
chestraing	O
,	O
for	O
,	O
have	O
,	O
to	O
,	O
intrally	O
,	O
of	O
,	O
qut	O
,	O
.	O
,	O
best	O
,	O
compers	O
,	O
***	O
,	O
cluseliment	O
,	O
uster	O
,	O
of	O
,	O
is	O
,	O
deveral	O
,	O
this	O
,	O
thise	O
,	O
of	O
,	O
offect	O
,	O
inatever	O
,	O
thifer	O
,	O
constranded	O
,	O
stater	O
,	O
vill	O
,	O
in	O
,	O
thase	O
,	O
in	O
,	O
youse	O
,	O
menttering	O
,	O
and	O
,	O
.	O
,	O
of	O
,	O
in	O
,	O
verate	O
,	O
of	O
,	O
to	O
this	O
approach	O
of	O
feature	O
learning	O
can	O
be	O
thought	O
of	O
as	O
a	O
form	O
of	O
graphical	B
model	I
structure	O
learning	B
(	O
chapter	O
26	O
)	O
,	O
except	O
it	O
is	O
more	O
ﬁne-grained	O
:	O
we	O
add	O
features	B
that	O
are	O
useful	O
,	O
regardless	O
of	O
the	O
resulting	O
graph	B
structure	O
.	O
however	O
,	O
the	O
resulting	O
graphs	O
can	O
become	O
densely	O
connected	O
,	O
which	O
makes	O
inference	B
(	O
and	O
hence	O
parameter	B
estimation	O
)	O
intractable	O
.	O
19.5.7	O
iterative	B
proportional	I
ﬁtting	I
(	O
ipf	O
)	O
*	O
consider	O
a	O
pairwise	O
mrf	O
where	O
the	O
potentials	O
are	O
represented	O
as	O
tables	O
,	O
with	O
one	O
parameter	B
per	O
variable	O
setting	O
.	O
we	O
can	O
represent	O
this	O
in	O
log-linear	B
form	O
using	O
!	O
#	O
ψst	O
(	O
ys	O
,	O
yt	O
)	O
=	O
exp	O
θt	O
st	O
[	O
i	O
(	O
ys	O
=	O
1	O
,	O
yt	O
=	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
i	O
(	O
ys	O
=	O
k	O
,	O
yt	O
=	O
k	O
)	O
]	O
(	O
19.55	O
)	O
and	O
similarly	O
for	O
ψt	O
(	O
yt	O
)	O
.	O
thus	O
the	O
feature	O
vectors	O
are	O
just	O
indicator	O
functions	O
.	O
8.	O
we	O
thank	O
john	O
lafferty	O
for	O
sharing	O
this	O
example	O
.	O
682	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
from	O
equation	O
19.43	O
,	O
we	O
have	O
that	O
,	O
at	O
the	O
maximum	O
of	O
the	O
likelihood	B
,	O
the	O
empirical	O
expectation	O
of	O
the	O
features	B
equals	O
the	O
model	O
’	O
s	O
expectation	O
:	O
epemp	O
[	O
i	O
(	O
ys	O
=	O
j	O
,	O
yt	O
=	O
k	O
)	O
]	O
=	O
ep	O
(	O
·|θ	O
)	O
[	O
i	O
(	O
ys	O
=	O
j	O
,	O
yt	O
=	O
k	O
)	O
]	O
(	O
19.56	O
)	O
(	O
19.57	O
)	O
(	O
19.58	O
)	O
(	O
19.59	O
)	O
pemp	O
(	O
ys	O
=	O
j	O
,	O
yt	O
=	O
k	O
)	O
=p	O
(	O
where	O
pemp	O
is	O
the	O
empirical	O
probability	O
:	O
ys	O
=	O
j	O
,	O
yt	O
=	O
k|θ	O
)	O
(	O
cid:7	O
)	O
n	O
pemp	O
(	O
ys	O
=	O
j	O
,	O
yt	O
=	O
k	O
)	O
=	O
nst	O
,	O
jk	O
n	O
=	O
n=1	O
i	O
(	O
yns	O
=	O
j	O
,	O
ynt	O
=	O
k	O
)	O
n	O
for	O
a	O
general	O
graph	B
,	O
the	O
condition	O
that	O
must	O
hold	O
at	O
the	O
optimum	O
is	O
pemp	O
(	O
yc	O
)	O
=	O
p	O
(	O
yc|θ	O
)	O
for	O
a	O
special	O
family	B
of	O
graphs	O
known	O
as	O
decomposable	B
graphs	I
(	O
deﬁned	O
in	O
section	O
20.4.1	O
)	O
,	O
one	O
can	O
show	O
that	O
p	O
(	O
yc|θ	O
)	O
=ψ	O
c	O
(	O
yc	O
)	O
.	O
however	O
,	O
even	O
if	O
the	O
graph	B
is	O
not	O
decomposable	O
,	O
we	O
can	O
imagine	O
trying	O
to	O
enforce	O
this	O
condition	O
.	O
this	O
suggests	O
an	O
iterative	O
coordinate	O
ascent	O
scheme	O
where	O
at	O
each	O
step	O
we	O
compute	O
ψt+1	O
c	O
(	O
yc	O
)	O
=	O
ψt	O
c	O
(	O
yc	O
)	O
×	O
pemp	O
(	O
yc	O
)	O
p	O
(	O
yc|ψt	O
)	O
(	O
19.60	O
)	O
where	O
the	O
multiplication	O
is	O
elementwise	O
.	O
this	O
is	O
known	O
as	O
iterative	B
proportional	I
ﬁtting	I
or	O
ipf	O
(	O
fienberg	O
1970	O
;	O
bishop	O
et	O
al	O
.	O
1975	O
)	O
.	O
see	O
algorithm	O
7	O
for	O
the	O
pseudocode	O
.	O
algorithm	O
19.2	O
:	O
iterative	O
proportional	O
fitting	O
algorithm	O
for	O
tabular	O
mrfs	O
1	O
initialize	O
ψc	O
=	O
1	O
for	O
c	O
=	O
1	O
:	O
c	O
;	O
2	O
repeat	O
3	O
for	O
c	O
=	O
1	O
:	O
c	O
do	O
pc	O
=	O
p	O
(	O
yc|ψ	O
)	O
;	O
ˆpc	O
=	O
pemp	O
(	O
yc	O
)	O
;	O
ψc	O
=	O
ψc	O
∗	O
ˆpc	O
pc	O
;	O
4	O
5	O
6	O
7	O
until	O
converged	O
;	O
19.5.7.1	O
example	O
let	O
us	O
consider	O
a	O
simple	O
example	O
from	O
http	O
:	O
//en.wikipedia.org/wiki/iterative_propo	O
rtional_fitting	O
.	O
we	O
have	O
two	O
binary	O
variables	O
,	O
y1	O
and	O
y2	O
,	O
where	O
yn1	O
=	O
1	O
if	O
man	O
n	O
is	O
left	O
handed	O
,	O
and	O
yn1	O
=	O
0	O
otherwise	O
;	O
similarly	O
,	O
yn2	O
=	O
1	O
if	O
woman	O
n	O
is	O
left	O
handed	O
,	O
and	O
yn2	O
=	O
0	O
otherwise	O
.	O
we	O
can	O
summarize	O
the	O
data	O
using	O
the	O
following	O
2	O
×	O
2	O
contingency	B
table	I
:	O
male	O
female	O
total	O
right-handed	O
43	O
44	O
87	O
left-handed	O
9	O
4	O
13	O
total	O
52	O
48	O
100	O
19.5.	O
learning	B
683	O
suppose	O
we	O
want	O
to	O
ﬁt	O
a	O
disconnected	O
graphical	B
model	I
containing	O
nodes	B
y1	O
and	O
y2	O
but	O
with	O
2	O
≈	O
c	O
,	O
no	O
edge	O
between	O
them	O
.	O
that	O
is	O
,	O
we	O
want	O
to	O
ﬁnd	O
vectors	O
ψ1	O
and	O
ψ2	O
such	O
that	O
m	O
(	O
cid:2	O
)	O
ψ1ψt	O
where	O
m	O
are	O
the	O
model	O
’	O
s	O
expected	O
counts	O
,	O
and	O
c	O
are	O
the	O
empirical	O
counts	O
.	O
by	O
moment	B
matching	I
,	O
we	O
ﬁnd	O
that	O
the	O
row	O
and	O
column	O
sums	O
of	O
the	O
model	O
must	O
exactly	O
match	O
the	O
row	O
and	O
column	O
sums	O
of	O
the	O
data	O
.	O
one	O
possible	O
solution	O
is	O
to	O
use	O
ψ1	O
=	O
[	O
0.5200	O
,	O
0.4800	O
]	O
and	O
ψ2	O
=	O
[	O
87	O
,	O
13	O
]	O
.	O
below	O
we	O
show	O
the	O
model	O
’	O
s	O
predictions	O
,	O
m	O
=	O
ψ1ψt	O
2	O
.	O
total	O
52	O
48	O
100	O
right-handed	O
45.24	O
41.76	O
87	O
left-handed	O
6.76	O
6.24	O
13	O
male	O
female	O
total	O
it	O
is	O
easy	O
to	O
see	O
that	O
this	O
matches	O
the	O
required	O
constraints	O
.	O
see	O
ipfdemo2x2	O
for	O
some	O
matlab	O
code	O
that	O
computes	O
these	O
numbers	O
.	O
this	O
method	O
is	O
easily	O
to	O
generalized	O
to	O
arbitrary	O
graphs	O
.	O
19.5.7.2	O
speed	O
of	O
ipf	O
ipf	O
is	O
a	O
ﬁxed	B
point	I
algorithm	O
for	O
enforcing	O
the	O
moment	B
matching	I
constraints	O
and	O
is	O
guaranteed	O
to	O
converge	B
to	O
the	O
global	O
optimum	O
(	O
bishop	O
et	O
al	O
.	O
1975	O
)	O
.	O
the	O
number	O
of	O
iterations	O
depends	O
on	O
the	O
form	O
of	O
the	O
model	O
.	O
if	O
the	O
graph	B
is	O
decomposable	B
,	O
then	O
ipf	O
converges	O
in	O
a	O
single	O
iteration	O
,	O
but	O
in	O
general	O
,	O
ipf	O
may	O
require	O
many	O
iterations	O
.	O
it	O
is	O
clear	O
that	O
the	O
dominant	O
cost	O
of	O
ipf	O
is	O
computing	O
the	O
required	O
marginals	O
under	O
the	O
model	O
.	O
efficient	O
methods	O
,	O
such	O
as	O
the	O
junction	B
tree	I
algorithm	I
(	O
section	O
20.4	O
)	O
,	O
can	O
be	O
used	O
,	O
resulting	O
in	O
something	O
called	O
efficient	O
ipf	O
(	O
jirousek	O
and	O
preucil	O
1995	O
)	O
.	O
nevertheless	O
,	O
coordinate	O
descent	O
can	O
be	O
slow	O
.	O
an	O
alternative	O
method	O
is	O
to	O
update	O
all	O
the	O
parameters	O
at	O
once	O
,	O
by	O
simply	O
following	O
the	O
gradient	O
of	O
the	O
likelihood	B
.	O
this	O
gradient	O
approach	O
has	O
the	O
further	O
signiﬁcant	O
advantage	O
that	O
it	O
works	O
for	O
models	O
in	O
which	O
the	O
clique	B
potentials	O
may	O
not	O
be	O
fully	O
parameterized	O
,	O
i.e.	O
,	O
the	O
features	B
may	O
not	O
consist	O
of	O
all	O
possible	O
indicators	O
for	O
each	O
clique	B
,	O
but	O
instead	O
can	O
be	O
arbitrary	O
.	O
although	O
it	O
is	O
possible	O
to	O
adapt	O
ipf	O
to	O
this	O
setting	O
of	O
general	O
features	B
,	O
resulting	O
in	O
a	O
method	O
known	O
as	O
iterative	B
scaling	I
,	O
in	O
practice	O
the	O
gradient	O
method	O
is	O
much	O
faster	O
(	O
malouf	O
2002	O
;	O
minka	O
2003	O
)	O
.	O
19.5.7.3	O
generalizations	O
of	O
ipf	O
we	O
can	O
use	O
ipf	O
to	O
ﬁt	O
gaussian	O
graphical	O
models	O
:	O
instead	O
of	O
working	O
with	O
empirical	O
counts	O
,	O
we	O
work	O
with	O
empirical	O
means	O
and	O
covariances	O
(	O
speed	O
and	O
kiiveri	O
1986	O
)	O
.	O
it	O
is	O
also	O
possible	O
to	O
create	O
a	O
bayesian	O
ipf	O
algorithm	O
for	O
sampling	O
from	O
the	O
posterior	O
of	O
the	O
model	O
’	O
s	O
parameters	O
(	O
see	O
e.g.	O
,	O
(	O
dobra	O
and	O
massam	O
2010	O
)	O
)	O
.	O
19.5.7.4	O
ipf	O
for	O
decomposable	B
graphical	O
models	O
there	O
is	O
a	O
special	O
family	B
of	O
undirected	O
graphical	O
models	O
known	O
as	O
decomposable	B
graphical	O
models	O
.	O
this	O
is	O
formally	O
deﬁned	O
in	O
section	O
20.4.1	O
,	O
but	O
the	O
basic	O
idea	O
is	O
that	O
it	O
contains	O
graphs	O
which	O
are	O
“	O
tree-like	O
”	O
.	O
such	O
graphs	O
can	O
be	O
represented	O
by	O
ugms	O
or	O
dgms	O
without	O
any	O
loss	B
of	O
information	B
.	O
in	O
the	O
case	O
of	O
decomposable	B
graphical	O
models	O
,	O
ipf	O
converges	O
in	O
one	O
iteration	O
.	O
in	O
fact	O
,	O
the	O
684	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
mle	O
has	O
a	O
closed	O
form	O
solution	O
(	O
lauritzen	O
1996	O
)	O
.	O
in	O
particular	O
,	O
for	O
tabular	O
potentials	O
we	O
have	O
(	O
cid:7	O
)	O
n	O
ˆψc	O
(	O
yc	O
=	O
k	O
)	O
=	O
i=1	O
i	O
(	O
yi	O
,	O
c	O
=	O
k	O
)	O
(	O
cid:7	O
)	O
n	O
n	O
(	O
cid:7	O
)	O
and	O
for	O
gaussian	O
potentials	O
,	O
we	O
have	O
ˆμc	O
=	O
i=1	O
yic	O
n	O
,	O
ˆσc	O
=	O
i	O
(	O
yic	O
−	O
ˆμc	O
)	O
(	O
xic	O
−	O
ˆμc	O
)	O
t	O
n	O
(	O
19.61	O
)	O
(	O
19.62	O
)	O
by	O
using	O
conjugate	B
priors	I
,	O
we	O
can	O
also	O
easily	O
compute	O
the	O
full	B
posterior	O
over	O
the	O
model	O
pa-	O
rameters	O
in	O
the	O
decomposable	B
case	O
,	O
just	O
as	O
we	O
did	O
in	O
the	O
dgm	O
case	O
.	O
see	O
(	O
lauritzen	O
1996	O
)	O
for	O
details	O
.	O
19.6	O
conditional	B
random	I
ﬁelds	I
(	O
crfs	O
)	O
a	O
conditional	B
random	I
ﬁeld	I
or	O
crf	O
(	O
lafferty	O
et	O
al	O
.	O
2001	O
)	O
,	O
sometimes	O
a	O
discriminative	B
random	I
ﬁeld	I
(	O
kumar	O
and	O
hebert	O
2003	O
)	O
,	O
is	O
just	O
a	O
version	O
of	O
an	O
mrf	O
where	O
all	O
the	O
clique	B
potentials	O
are	O
conditioned	O
on	O
input	O
features	B
:	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
1	O
ψc	O
(	O
yc|x	O
,	O
w	O
)	O
(	O
19.63	O
)	O
(	O
cid:20	O
)	O
z	O
(	O
x	O
,	O
w	O
)	O
c	O
a	O
crf	O
can	O
be	O
thought	O
of	O
as	O
a	O
structured	B
output	I
extension	O
of	O
logistic	B
regression	I
.	O
we	O
will	O
usually	O
assume	O
a	O
log-linear	B
representation	O
of	O
the	O
potentials	O
:	O
ψc	O
(	O
yc|x	O
,	O
w	O
)	O
=	O
exp	O
(	O
wt	O
c	O
φ	O
(	O
x	O
,	O
yc	O
)	O
)	O
(	O
19.64	O
)	O
where	O
φ	O
(	O
x	O
,	O
yc	O
)	O
is	O
a	O
feature	O
vector	O
derived	O
from	O
the	O
global	O
inputs	O
x	O
and	O
the	O
local	O
set	O
of	O
labels	O
yc	O
.	O
we	O
will	O
give	O
some	O
examples	O
below	O
which	O
will	O
make	O
this	O
notation	O
clearer	O
.	O
the	O
advantage	O
of	O
a	O
crf	O
over	O
an	O
mrf	O
is	O
analogous	O
to	O
the	O
advantage	O
of	O
a	O
discriminative	B
classiﬁer	I
over	O
a	O
generative	B
classiﬁer	I
(	O
see	O
section	O
8.6	O
)	O
,	O
namely	O
,	O
we	O
don	O
’	O
t	O
need	O
to	O
“	O
waste	O
resources	O
”	O
modeling	O
things	O
that	O
we	O
always	O
observe	O
.	O
instead	O
we	O
can	O
focus	O
our	O
attention	O
on	O
modeling	O
what	O
we	O
care	O
about	O
,	O
namely	O
the	O
distribution	O
of	O
labels	O
given	O
the	O
data	O
.	O
another	O
important	O
advantage	O
of	O
crfs	O
is	O
that	O
we	O
can	O
make	O
the	O
potentials	O
(	O
or	O
factors	B
)	O
of	O
the	O
model	O
be	O
data-dependent	O
.	O
for	O
example	O
,	O
in	O
image	O
processing	O
applications	O
,	O
we	O
may	O
“	O
turn	O
off	O
”	O
the	O
label	B
smoothing	O
between	O
two	O
neighboring	O
nodes	B
s	O
and	O
t	O
if	O
there	O
is	O
an	O
observed	O
discontinuity	O
in	O
the	O
image	O
intensity	O
between	O
pixels	O
s	O
and	O
t.	O
similarly	O
,	O
in	O
natural	O
language	O
processing	O
problems	O
,	O
we	O
can	O
make	O
the	O
latent	B
labels	O
depend	O
on	O
global	O
properties	O
of	O
the	O
sentence	O
,	O
such	O
as	O
which	O
language	O
it	O
is	O
written	O
in	O
.	O
it	O
is	O
hard	O
to	O
incorporate	O
global	O
features	O
into	O
generative	O
models	O
.	O
the	O
disadvantage	O
of	O
crfs	O
over	O
mrfs	O
is	O
that	O
they	O
require	O
labeled	O
training	O
data	O
,	O
and	O
they	O
are	O
slower	O
to	O
train	O
,	O
as	O
we	O
explain	O
in	O
section	O
19.6.3.	O
this	O
is	O
analogous	O
to	O
the	O
strengths	O
and	O
weaknesses	O
of	O
logistic	B
regression	I
vs	O
naive	O
bayes	O
,	O
discussed	O
in	O
section	O
8.6	O
.	O
19.6.1	O
chain-structured	O
crfs	O
,	O
memms	O
and	O
the	O
label-bias	O
problem	O
the	O
most	O
widely	O
used	O
kind	O
of	O
crf	O
uses	O
a	O
chain-structured	O
graph	B
to	O
model	O
correlation	O
amongst	O
neighboring	O
labels	O
.	O
such	O
models	O
are	O
useful	O
for	O
a	O
variety	O
of	O
sequence	O
labeling	O
tasks	O
(	O
see	O
sec-	O
tion	O
19.6.2	O
)	O
.	O
19.6.	O
conditional	B
random	I
ﬁelds	I
(	O
crfs	O
)	O
yt−1	O
yt	O
yt+1	O
yt−1	O
xt−1	O
xt	O
(	O
a	O
)	O
xt+1	O
xt−1	O
xg	O
yt	O
xt	O
(	O
b	O
)	O
685	O
xg	O
yt+1	O
yt−1	O
yt+1	O
xt+1	O
xt−1	O
yt	O
xt	O
(	O
c	O
)	O
xt+1	O
figure	O
19.14	O
various	O
models	O
for	O
sequential	B
data	O
.	O
directed	B
memm	O
.	O
(	O
c	O
)	O
a	O
discriminative	B
undirected	O
crf	O
.	O
(	O
a	O
)	O
a	O
generative	O
directed	O
hmm	O
.	O
(	O
b	O
)	O
a	O
discriminative	B
traditionally	O
,	O
hmms	O
(	O
discussed	O
in	O
detail	O
in	O
chapter	O
17	O
)	O
have	O
been	O
used	O
for	O
such	O
tasks	O
.	O
these	O
are	O
joint	O
density	O
models	O
of	O
the	O
form	O
t	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
p	O
(	O
x	O
,	O
y|w	O
)	O
=	O
p	O
(	O
yt|yt−1	O
,	O
w	O
)	O
p	O
(	O
xt|yt	O
,	O
w	O
)	O
(	O
19.65	O
)	O
t=1	O
where	O
we	O
have	O
dropped	O
the	O
initial	O
p	O
(	O
y1	O
)	O
term	O
for	O
simplicity	O
.	O
see	O
figure	O
19.14	O
(	O
a	O
)	O
.	O
if	O
we	O
observe	O
both	O
xt	O
and	O
yt	O
for	O
all	O
t	O
,	O
it	O
is	O
very	O
easy	O
to	O
train	O
such	O
models	O
,	O
using	O
techniques	O
described	O
in	O
section	O
17.5.1.	O
an	O
hmm	O
requires	O
specifying	O
a	O
generative	O
observation	O
model	O
,	O
p	O
(	O
xt|yt	O
,	O
w	O
)	O
,	O
which	O
can	O
be	O
difficult	O
.	O
furthemore	O
,	O
each	O
xt	O
is	O
required	O
to	O
be	O
local	O
,	O
since	O
it	O
is	O
hard	O
to	O
deﬁne	O
a	O
generative	O
model	O
for	O
the	O
whole	O
stream	O
of	O
observations	O
,	O
x	O
=	O
x1	O
:	O
t	O
.	O
an	O
obvious	O
way	O
to	O
make	O
a	O
discriminative	B
version	O
of	O
an	O
hmm	O
is	O
to	O
“	O
reverse	O
the	O
arrows	O
”	O
from	O
yt	O
to	O
xt	O
,	O
as	O
in	O
figure	O
19.14	O
(	O
b	O
)	O
.	O
this	O
deﬁnes	O
a	O
directed	B
discriminative	O
model	O
of	O
the	O
form	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
p	O
(	O
yt|yt−1	O
,	O
x	O
,	O
w	O
)	O
(	O
19.66	O
)	O
t	O
where	O
x	O
=	O
(	O
x1	O
:	O
t	O
,	O
xg	O
)	O
,	O
xg	O
are	O
global	O
features	O
,	O
and	O
xt	O
are	O
features	B
speciﬁc	O
to	O
node	O
t.	O
(	O
this	O
partition	O
into	O
local	O
and	O
global	O
is	O
not	O
necessary	O
,	O
but	O
helps	O
when	O
comparing	O
to	O
hmms	O
.	O
)	O
this	O
is	O
called	O
a	O
maximum	B
entropy	I
markov	O
model	O
or	O
memm	O
(	O
mccallum	O
et	O
al	O
.	O
2000	O
;	O
kakade	O
et	O
al	O
.	O
2002	O
)	O
.	O
an	O
memm	O
is	O
simply	O
a	O
markov	O
chain	O
in	O
which	O
the	O
state	O
transition	O
probabilities	O
are	O
conditioned	O
(	O
it	O
is	O
therefore	O
a	O
special	O
case	O
of	O
an	O
input-output	O
hmm	O
,	O
discussed	O
in	O
on	O
the	O
input	O
features	B
.	O
section	O
17.6.3	O
.	O
)	O
this	O
seems	O
like	O
the	O
natural	O
generalization	O
of	O
logistic	B
regression	I
to	O
the	O
structured-	O
output	O
setting	O
,	O
but	O
it	O
suffers	O
from	O
a	O
subtle	O
problem	O
known	O
(	O
rather	O
obscurely	O
)	O
as	O
the	O
label	B
bias	I
problem	O
(	O
lafferty	O
et	O
al	O
.	O
2001	O
)	O
.	O
the	O
problem	O
is	O
that	O
local	O
features	O
at	O
time	O
t	O
do	O
not	O
inﬂuence	O
states	O
prior	O
to	O
time	O
t.	O
this	O
follows	O
by	O
examining	O
the	O
dag	O
,	O
which	O
shows	O
that	O
xt	O
is	O
d-separated	B
from	O
yt−1	O
(	O
and	O
all	O
earlier	O
time	O
points	O
)	O
by	O
the	O
v-structure	B
at	O
yt	O
,	O
which	O
is	O
a	O
hidden	B
child	O
,	O
thus	O
blocking	O
the	O
information	B
ﬂow	O
.	O
to	O
understand	O
what	O
this	O
means	O
in	O
practice	O
,	O
consider	O
the	O
part	B
of	I
speech	I
(	O
pos	O
)	O
tagging	O
task	O
.	O
suppose	O
we	O
see	O
the	O
word	O
“	O
banks	O
”	O
;	O
this	O
could	O
be	O
a	O
verb	O
(	O
as	O
in	O
“	O
he	O
banks	O
at	O
boa	O
”	O
)	O
,	O
or	O
a	O
noun	O
(	O
as	O
in	O
“	O
the	O
river	O
banks	O
were	O
overﬂowing	O
”	O
)	O
.	O
locally	O
the	O
pos	O
tag	O
for	O
the	O
word	O
is	O
ambiguous	O
.	O
however	O
,	O
686	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
(	O
e	O
)	O
figure	O
19.15	O
example	O
of	O
handwritten	O
letter	O
recognition	O
.	O
in	O
the	O
word	O
’	O
brace	O
’	O
,	O
the	O
’	O
r	O
’	O
and	O
the	O
’	O
c	O
’	O
look	O
very	O
similar	B
,	O
but	O
can	O
be	O
disambiguated	O
using	O
context	O
.	O
source	O
:	O
(	O
taskar	O
et	O
al	O
.	O
2003	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
ben	O
taskar	O
.	O
suppose	O
that	O
later	O
in	O
the	O
sentence	O
,	O
we	O
see	O
the	O
word	O
“	O
ﬁshing	O
”	O
;	O
this	O
gives	O
us	O
enough	O
context	O
to	O
infer	O
that	O
the	O
sense	O
of	O
“	O
banks	O
”	O
is	O
“	O
river	O
banks	O
”	O
.	O
however	O
,	O
in	O
an	O
memm	O
(	O
unlike	O
in	O
an	O
hmm	O
and	O
crf	O
)	O
,	O
the	O
“	O
ﬁshing	O
”	O
evidence	B
will	O
not	O
ﬂow	O
backwards	O
,	O
so	O
we	O
will	O
not	O
be	O
able	O
to	O
disambiguate	O
“	O
banks	O
”	O
.	O
now	O
consider	O
a	O
chain-structured	O
crf	O
.	O
this	O
model	O
has	O
the	O
form	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
ψ	O
(	O
yt	O
,	O
yt+1|x	O
,	O
w	O
)	O
ψ	O
(	O
yt|x	O
,	O
w	O
)	O
t−1	O
(	O
cid:20	O
)	O
t	O
(	O
cid:20	O
)	O
1	O
z	O
(	O
x	O
,	O
w	O
)	O
t=1	O
t=1	O
(	O
19.67	O
)	O
from	O
the	O
graph	B
in	O
figure	O
19.14	O
(	O
c	O
)	O
,	O
we	O
see	O
that	O
the	O
label	B
bias	I
problem	O
no	O
longer	O
exists	O
,	O
since	O
yt	O
does	O
not	O
block	O
the	O
information	B
from	O
xt	O
from	O
reaching	O
other	O
yt	O
(	O
cid:2	O
)	O
nodes	B
.	O
the	O
label	B
bias	I
problem	O
in	O
memms	O
occurs	O
because	O
directed	B
models	O
are	O
locally	B
normalized	I
,	O
meaning	O
each	O
cpd	O
sums	O
to	O
1.	O
by	O
contrast	O
,	O
mrfs	O
and	O
crfs	O
are	O
globally	B
normalized	I
,	O
which	O
means	O
that	O
local	O
factors	O
do	O
not	O
need	O
to	O
sum	O
to	O
1	O
,	O
since	O
the	O
partition	B
function	I
z	O
,	O
which	O
sums	O
over	O
all	O
joint	O
conﬁgurations	O
,	O
will	O
ensure	O
the	O
model	O
deﬁnes	O
a	O
valid	O
distribution	O
.	O
however	O
,	O
this	O
solution	O
comes	O
at	O
a	O
price	O
:	O
we	O
do	O
not	O
get	O
a	O
valid	O
probability	O
distribution	O
over	O
y	O
until	O
we	O
have	O
seen	O
the	O
whole	O
sentence	O
,	O
since	O
only	O
then	O
can	O
we	O
normalize	O
over	O
all	O
conﬁgurations	O
.	O
consequently	O
,	O
crfs	O
are	O
not	O
as	O
useful	O
as	O
dgms	O
(	O
whether	O
discriminative	B
or	O
generative	O
)	O
for	O
online	O
or	O
real-time	O
inference	B
.	O
furthermore	O
,	O
the	O
fact	O
that	O
z	O
depends	O
on	O
all	O
the	O
nodes	B
,	O
and	O
hence	O
all	O
their	O
parameters	O
,	O
makes	O
crfs	O
much	O
slower	O
to	O
train	O
than	O
dgms	O
,	O
as	O
we	O
will	O
see	O
in	O
section	O
19.6.3	O
.	O
19.6.2	O
applications	O
of	O
crfs	O
crfs	O
have	O
been	O
applied	O
to	O
many	O
interesting	O
problems	O
;	O
we	O
give	O
a	O
representative	O
sample	O
below	O
.	O
these	O
applications	O
illustrate	O
several	O
useful	O
modeling	O
tricks	O
,	O
and	O
will	O
also	O
provide	O
motivation	O
for	O
some	O
of	O
the	O
inference	B
techniques	O
we	O
will	O
discuss	O
in	O
chapter	O
20	O
.	O
19.6.2.1	O
handwriting	B
recognition	I
a	O
natural	O
application	O
of	O
crfs	O
is	O
to	O
classify	O
hand-written	O
digit	O
strings	O
,	O
as	O
illustrated	O
in	O
figure	O
19.15.	O
the	O
key	O
observation	B
is	O
that	O
locally	O
a	O
letter	O
may	O
be	O
ambiguous	O
,	O
but	O
by	O
depending	O
on	O
the	O
(	O
un-	O
known	O
)	O
labels	O
of	O
one	O
’	O
s	O
neighbors	B
,	O
it	O
is	O
possible	O
to	O
use	O
context	O
to	O
reduce	O
the	O
error	O
rate	O
.	O
note	O
that	O
the	O
node	O
potential	O
,	O
ψt	O
(	O
yt|xt	O
)	O
,	O
is	O
often	O
taken	O
to	O
be	O
a	O
probabilistic	O
discriminative	O
classiﬁer	O
,	O
19.6.	O
conditional	B
random	I
ﬁelds	I
(	O
crfs	O
)	O
687	O
b	O
adj	O
i	O
n	O
o	O
v	O
o	O
in	O
o	O
v	O
b	O
prp	O
i	O
n	O
o	O
b	O
in	O
dt	O
i	O
n	O
i	O
np	O
n	O
pos	O
british	O
airways	O
rose	O
after	O
announcing	O
its	O
withdrawal	O
from	O
the	O
ual	O
deal	O
key	O
b	O
i	O
o	O
n	O
adj	O
begin	O
noun	O
phrase	O
within	O
noun	O
phrase	O
not	O
a	O
noun	O
phrase	O
noun	O
adjective	O
v	O
in	O
prp	O
dt	O
verb	O
preposition	O
possesive	O
pronoun	O
determiner	O
(	O
e.g.	O
,	O
a	O
,	O
an	O
,	O
the	O
)	O
figure	O
19.16	O
a	O
crf	O
for	O
joint	O
pos	O
tagging	O
and	O
np	O
segmentation	O
.	O
friedman	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
.	O
source	O
:	O
figure	O
4.e.1	O
of	O
(	O
koller	O
and	O
such	O
as	O
a	O
neural	B
network	I
or	O
rvm	O
,	O
that	O
is	O
trained	O
on	O
isolated	O
letters	O
,	O
and	O
the	O
edge	O
potentials	O
,	O
ψst	O
(	O
ys	O
,	O
yt	O
)	O
,	O
are	O
often	O
taken	O
to	O
be	O
a	O
language	O
bigram	O
model	O
.	O
later	O
we	O
will	O
discuss	O
how	O
to	O
train	O
all	O
the	O
potentials	O
jointly	O
.	O
19.6.2.2	O
noun	B
phrase	I
chunking	I
one	O
common	O
nlp	O
task	O
is	O
noun	B
phrase	I
chunking	I
,	O
which	O
refers	O
to	O
the	O
task	O
of	O
segmenting	O
a	O
sentence	O
into	O
its	O
distinct	O
noun	O
phrases	O
(	O
nps	O
)	O
.	O
this	O
is	O
a	O
simple	O
example	O
of	O
a	O
technique	O
known	O
as	O
shallow	B
parsing	I
.	O
in	O
more	O
detail	O
,	O
we	O
tag	O
each	O
word	O
in	O
the	O
sentence	O
with	O
b	O
(	O
meaning	O
beginning	O
of	O
a	O
new	O
np	O
)	O
,	O
i	O
(	O
meaning	O
inside	O
a	O
np	O
)	O
,	O
or	O
o	O
(	O
meaning	O
outside	O
an	O
np	O
)	O
.	O
this	O
is	O
called	O
bio	O
notation	O
.	O
for	O
example	O
,	O
in	O
the	O
following	O
sentence	O
,	O
the	O
nps	O
are	O
marked	O
with	O
brackets	O
:	O
b	O
i	O
o	O
o	O
o	O
b	O
i	O
o	O
b	O
i	O
i	O
(	O
british	O
airways	O
)	O
rose	O
after	O
announcing	O
(	O
its	O
withdrawl	O
)	O
from	O
(	O
the	O
uai	O
deal	O
)	O
(	O
we	O
need	O
the	O
b	O
symbol	O
so	O
that	O
we	O
can	O
distinguish	O
i	O
i	O
,	O
meaning	O
two	O
words	O
within	O
a	O
single	O
np	O
,	O
from	O
b	O
b	O
,	O
meaning	O
two	O
separate	O
nps	O
.	O
)	O
a	O
standard	O
approach	O
to	O
this	O
problem	O
would	O
ﬁrst	O
convert	O
the	O
string	O
of	O
words	O
into	O
a	O
string	O
of	O
pos	O
tags	O
,	O
and	O
then	O
convert	O
the	O
pos	O
tags	O
to	O
a	O
string	O
of	O
bios	O
.	O
however	O
,	O
such	O
a	O
pipeline	B
method	O
can	O
propagate	O
errors	O
.	O
a	O
more	O
robust	B
approach	O
is	O
to	O
build	O
a	O
joint	O
probabilistic	O
model	O
of	O
the	O
form	O
p	O
(	O
np1	O
:	O
t	O
,	O
pos1	O
:	O
t|words1	O
:	O
t	O
)	O
.	O
one	O
way	O
to	O
do	O
this	O
is	O
to	O
use	O
the	O
crf	O
in	O
figure	O
19.16.	O
the	O
connections	O
between	O
adjacent	O
labels	O
encode	O
the	O
probability	O
of	O
transitioning	O
between	O
the	O
b	O
,	O
i	O
and	O
o	O
states	O
,	O
and	O
can	O
enforce	O
constraints	O
such	O
as	O
the	O
fact	O
that	O
b	O
must	O
preceed	O
i.	O
the	O
features	B
are	O
usually	O
hand	O
engineered	O
and	O
include	O
things	O
like	O
:	O
does	O
this	O
word	O
begin	O
with	O
a	O
capital	O
letter	O
,	O
is	O
this	O
word	O
followed	O
by	O
a	O
full	B
stop	O
,	O
is	O
this	O
word	O
a	O
noun	O
,	O
etc	O
.	O
typically	O
there	O
are	O
∼	O
1	O
,	O
000−	O
10	O
,	O
000	O
features	B
per	O
node	O
.	O
the	O
number	O
of	O
features	B
has	O
minimal	B
impact	O
on	O
the	O
inference	B
time	O
,	O
since	O
the	O
features	B
are	O
increase	O
in	O
the	O
cost	O
of	O
observed	O
and	O
do	O
not	O
need	O
to	O
be	O
summed	O
over	O
.	O
(	O
there	O
is	O
a	O
small	O
688	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
b-per	O
i-per	O
oth	O
oth	O
oth	O
b-loc	O
i-loc	O
b-per	O
oth	O
oth	O
oth	O
oth	O
mrs.	O
green	O
spoke	O
today	O
in	O
new	O
york	O
green	O
chairs	O
the	O
ﬁnance	O
committee	O
key	O
b-per	O
i-per	O
b-loc	O
begin	O
person	O
name	O
within	O
person	O
name	O
begin	O
location	O
name	O
i-loc	O
oth	O
within	O
location	O
name	O
not	O
an	O
entitiy	O
(	O
)	O
figure	O
19.17	O
a	O
skip-chain	O
crf	O
for	O
named	O
entity	O
recognition	O
.	O
source	O
:	O
figure	O
4.e.1	O
of	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
.	O
evaluating	O
potential	O
functions	O
with	O
many	O
features	B
,	O
but	O
this	O
is	O
usually	O
negligible	O
;	O
if	O
not	O
,	O
one	O
can	O
use	O
(	O
cid:2	O
)	O
1	O
regularization	B
to	O
prune	O
out	O
irrelevant	O
features	B
.	O
)	O
however	O
,	O
the	O
graph	B
structure	O
can	O
have	O
a	O
dramatic	O
effect	O
on	O
inference	B
time	O
.	O
the	O
model	O
in	O
figure	O
19.16	O
is	O
tractable	O
,	O
since	O
it	O
is	O
essentially	O
a	O
“	O
fat	O
chain	O
”	O
,	O
so	O
we	O
can	O
use	O
the	O
forwards-backwards	B
algorithm	I
(	O
section	O
17.4.3	O
)	O
for	O
exact	O
inference	B
in	O
o	O
(	O
t|pos|2|np|2	O
)	O
time	O
,	O
where	O
|pos|	O
is	O
the	O
number	O
of	O
pos	O
tags	O
,	O
and	O
|np|	O
is	O
the	O
number	O
of	O
np	O
tags	O
.	O
however	O
,	O
the	O
seemingly	O
similar	B
graph	O
in	O
figure	O
19.17	O
,	O
to	O
be	O
explained	O
below	O
,	O
is	O
computationally	O
intractable	O
.	O
19.6.2.3	O
named	O
entity	O
recognition	O
a	O
task	O
that	O
is	O
related	O
to	O
np	O
chunking	O
is	O
named	B
entity	I
extraction	I
.	O
instead	O
of	O
just	O
segmenting	O
out	O
noun	O
phrases	O
,	O
we	O
can	O
segment	O
out	O
phrases	O
to	O
do	O
with	O
people	O
and	O
locations	O
.	O
similar	B
techniques	O
are	O
used	O
to	O
automatically	O
populate	O
your	O
calendar	O
from	O
your	O
email	O
messages	O
;	O
this	O
is	O
called	O
information	B
extraction	I
.	O
a	O
simple	O
approach	O
to	O
this	O
is	O
to	O
use	O
a	O
chain-structured	O
crf	O
,	O
but	O
to	O
expand	O
the	O
state	B
space	I
from	O
bio	O
to	O
b-per	O
,	O
i-per	O
,	O
b-loc	O
,	O
i-loc	O
,	O
and	O
other	O
.	O
however	O
,	O
sometimes	O
it	O
is	O
ambiguous	O
whether	O
a	O
word	O
is	O
a	O
person	O
,	O
location	O
,	O
or	O
something	O
else	O
.	O
(	O
proper	O
nouns	O
are	O
particularly	O
difficult	O
to	O
deal	O
with	O
because	O
they	O
belong	O
to	O
an	O
open	B
class	I
,	O
that	O
is	O
,	O
there	O
is	O
an	O
unbounded	O
number	O
of	O
possible	O
names	O
,	O
unlike	O
the	O
set	O
of	O
nouns	O
and	O
verbs	O
,	O
which	O
is	O
large	O
but	O
essentially	O
ﬁxed	O
.	O
)	O
we	O
can	O
get	O
better	O
performance	O
by	O
considering	O
long-range	O
correlations	O
between	O
words	O
.	O
for	O
example	O
,	O
we	O
might	O
add	O
a	O
link	O
between	O
all	O
occurrences	O
of	O
the	O
same	O
word	O
,	O
and	O
force	O
the	O
word	O
to	O
have	O
the	O
same	O
tag	O
in	O
each	O
occurence	O
.	O
(	O
the	O
same	O
technique	O
can	O
also	O
be	O
helpful	O
for	O
resolving	O
the	O
identity	O
of	O
pronouns	O
.	O
)	O
this	O
is	O
known	O
as	O
a	O
skip-chain	O
crf	O
.	O
see	O
figure	O
19.17	O
for	O
an	O
illustration	O
.	O
we	O
see	O
that	O
the	O
graph	B
structure	O
itself	O
changes	O
depending	O
on	O
the	O
input	O
,	O
which	O
is	O
an	O
additional	O
advantage	O
of	O
crfs	O
over	O
generative	O
models	O
.	O
unfortunately	O
,	O
inference	B
in	O
this	O
model	O
is	O
gener-	O
ally	O
more	O
expensive	O
than	O
in	O
a	O
simple	O
chain	O
with	O
local	O
connections	O
,	O
for	O
reasons	O
explained	O
in	O
section	O
20.5	O
.	O
19.6.	O
conditional	B
random	I
ﬁelds	I
(	O
crfs	O
)	O
689	O
figure	O
19.18	O
illustration	O
of	O
a	O
simple	O
parse	O
tree	B
based	O
on	O
a	O
context	B
free	I
grammar	I
in	O
chomsky	O
normal	B
form	O
.	O
the	O
feature	O
vector	O
φ	O
(	O
x	O
,	O
y	O
)	O
=	O
ψ	O
(	O
x	O
,	O
y	O
)	O
counts	O
the	O
number	O
of	O
times	O
each	O
production	O
rule	O
was	O
used	O
.	O
source	O
:	O
figure	O
5.2	O
of	O
(	O
altun	O
et	O
al	O
.	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
yasemin	O
altun	O
.	O
19.6.2.4	O
natural	O
language	O
parsing	O
a	O
generalization	B
of	O
chain-structured	O
models	O
for	O
language	O
is	O
to	O
use	O
probabilistic	O
grammars	O
.	O
in	O
particular	O
,	O
a	O
probabilistic	O
context	O
free	O
grammar	O
or	O
pcfg	O
is	O
a	O
set	O
of	O
re-write	O
or	O
production	B
rules	I
of	O
the	O
form	O
σ	O
→	O
σ	O
(	O
cid:2	O
)	O
σ	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
or	O
σ	O
→	O
x	O
,	O
where	O
σ	O
,	O
σ	O
(	O
cid:2	O
)	O
,	O
σ	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
∈	O
σ	O
are	O
non-terminals	B
(	O
analogous	O
to	O
parts	O
of	O
speech	O
)	O
,	O
and	O
x	O
∈	O
x	O
are	O
terminals	B
,	O
i.e.	O
,	O
words	O
.	O
see	O
figure	O
19.18	O
for	O
an	O
example	O
.	O
each	O
such	O
rule	O
has	O
an	O
associated	O
probability	O
.	O
the	O
resulting	O
model	O
deﬁnes	O
a	O
probability	O
distribution	O
over	O
sequences	O
of	O
words	O
.	O
we	O
can	O
compute	O
the	O
probability	O
of	O
observing	O
a	O
particular	O
sequence	O
x	O
=	O
x1	O
.	O
.	O
.	O
xt	O
by	O
summing	O
over	O
all	O
trees	O
that	O
generate	O
it	O
.	O
this	O
can	O
be	O
done	O
in	O
o	O
(	O
t	O
3	O
)	O
time	O
using	O
the	O
inside-outside	B
algorithm	I
;	O
see	O
e.g.	O
,	O
(	O
jurafsky	O
and	O
martin	O
2008	O
;	O
manning	O
and	O
schuetze	O
1999	O
)	O
for	O
details	O
.	O
pcfgs	O
are	O
generative	O
models	O
.	O
it	O
is	O
possible	O
to	O
make	O
discriminative	B
versions	O
which	O
encode	O
the	O
probability	O
of	O
a	O
labeled	O
tree	O
,	O
y	O
,	O
given	O
a	O
sequence	O
of	O
words	O
,	O
x	O
,	O
by	O
using	O
a	O
crf	O
of	O
the	O
form	O
p	O
(	O
y|x	O
)	O
∝	O
exp	O
(	O
wt	O
φ	O
(	O
x	O
,	O
y	O
)	O
)	O
.	O
for	O
example	O
,	O
we	O
might	O
deﬁne	O
φ	O
(	O
x	O
,	O
y	O
)	O
to	O
count	O
the	O
number	O
of	O
times	O
each	O
production	O
rule	O
was	O
used	O
(	O
which	O
is	O
analogous	O
to	O
the	O
number	O
of	O
state	B
transitions	O
in	O
a	O
chain-structured	O
model	O
)	O
.	O
see	O
e.g.	O
,	O
(	O
taskar	O
et	O
al	O
.	O
2004	O
)	O
for	O
details	O
.	O
19.6.2.5	O
hierarchical	O
classiﬁcation	O
suppose	O
we	O
are	O
performing	O
multi-class	O
classiﬁcation	O
,	O
where	O
we	O
have	O
a	O
label	B
taxonomy	I
,	O
which	O
groups	O
the	O
classes	O
into	O
a	O
hierarchy	O
.	O
we	O
can	O
encode	O
the	O
position	O
of	O
y	O
within	O
this	O
hierarchy	O
by	O
deﬁning	O
a	O
binary	O
vector	O
φ	O
(	O
y	O
)	O
,	O
where	O
we	O
turn	O
on	O
the	O
bit	O
for	O
component	O
y	O
and	O
for	O
all	O
its	O
children	B
.	O
this	O
can	O
be	O
combined	O
with	O
input	O
features	B
φ	O
(	O
x	O
)	O
using	O
a	O
tensor	B
product	I
,	O
φ	O
(	O
x	O
,	O
y	O
)	O
=	O
φ	O
(	O
x	O
)	O
⊗	O
φ	O
(	O
y	O
)	O
.	O
see	O
figure	O
19.19	O
for	O
an	O
example	O
.	O
this	O
method	O
is	O
widely	O
used	O
for	O
text	O
classiﬁcation	B
,	O
where	O
manually	O
constructed	O
taxnomies	O
(	O
such	O
as	O
the	O
open	O
directory	O
project	O
at	O
www.dmoz.org	O
)	O
are	O
quite	O
common	O
.	O
the	O
beneﬁt	O
is	O
that	O
information	B
can	O
be	O
shared	B
between	O
the	O
parameters	O
for	O
nearby	O
categories	O
,	O
enabling	O
generalization	B
across	O
classes	O
.	O
690	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
(	O
cid:4	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
2	O
)	O
(	O
cid:5	O
)	O
=	O
(	O
cid:4	O
)	O
w2	O
,	O
x	O
(	O
cid:5	O
)	O
+	O
(	O
cid:4	O
)	O
w6	O
,	O
x	O
(	O
cid:5	O
)	O
+	O
(	O
cid:4	O
)	O
w9	O
,	O
x	O
(	O
cid:5	O
)	O
figure	O
19.19	O
illustration	O
of	O
a	O
simple	O
label	O
taxonomy	O
,	O
and	O
how	O
it	O
can	O
be	O
used	O
to	O
compute	O
a	O
distributed	B
representation	I
for	O
the	O
label	B
for	O
class	O
2.	O
in	O
this	O
ﬁgure	O
,	O
φ	O
(	O
x	O
)	O
=	O
x	O
,	O
φ	O
(	O
y	O
=	O
2	O
)	O
=	O
λ	O
(	O
2	O
)	O
,	O
φ	O
(	O
x	O
,	O
y	O
)	O
is	O
denoted	O
by	O
ψ	O
(	O
x	O
,	O
2	O
)	O
,	O
and	O
wt	O
φ	O
(	O
x	O
,	O
y	O
)	O
is	O
denoted	O
by	O
(	O
cid:4	O
)	O
w	O
,	O
ψ	O
(	O
x	O
,	O
2	O
)	O
(	O
cid:5	O
)	O
.	O
source	O
:	O
figure	O
5.1	O
of	O
(	O
altun	O
et	O
al	O
.	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
yasemin	O
altun	O
.	O
19.6.2.6	O
protein	O
side-chain	O
prediction	O
an	O
interesting	O
analog	O
to	O
the	O
skip-chain	O
model	O
arises	O
in	O
the	O
problem	O
of	O
predicting	O
the	O
structure	O
of	O
protein	O
side	B
chains	I
.	O
each	O
residue	O
in	O
the	O
side	O
chain	O
has	O
4	O
dihedral	O
angles	O
,	O
which	O
are	O
usually	O
discretized	O
into	O
3	O
values	O
called	O
rotamers	B
.	O
the	O
goal	O
is	O
to	O
predict	O
this	O
discrete	B
sequence	O
of	O
angles	O
,	O
y	O
,	O
from	O
the	O
discrete	B
sequence	O
of	O
amino	O
acids	O
,	O
x	O
.	O
(	O
cid:7	O
)	O
d	O
we	O
can	O
deﬁne	O
an	O
energy	B
function	I
e	O
(	O
x	O
,	O
y	O
)	O
,	O
where	O
we	O
include	O
various	O
pairwise	O
interaction	O
terms	O
between	O
nearby	O
residues	O
(	O
elements	O
of	O
the	O
y	O
vector	O
)	O
.	O
this	O
energy	O
is	O
usually	O
deﬁned	O
as	O
a	O
weighted	O
sum	O
of	O
individual	O
energy	O
terms	O
,	O
e	O
(	O
x	O
,	O
y|w	O
)	O
=	O
j=1	O
θjej	O
(	O
x	O
,	O
y	O
)	O
,	O
where	O
the	O
ej	O
are	O
energy	O
contribution	O
due	O
to	O
various	O
electrostatic	O
charges	O
,	O
hydrogen	O
bonding	O
potentials	O
,	O
etc	O
,	O
and	O
w	O
are	O
the	O
parameters	O
of	O
the	O
model	O
.	O
see	O
(	O
yanover	O
et	O
al	O
.	O
2007	O
)	O
for	O
details	O
.	O
=	O
argmin	O
e	O
(	O
x	O
,	O
y|w	O
)	O
.	O
in	O
general	O
,	O
this	O
problem	O
is	O
np-hard	O
,	O
depending	O
on	O
the	O
nature	O
of	O
the	O
graph	B
induced	O
by	O
the	O
ej	O
terms	O
,	O
due	O
to	O
long-range	O
connections	O
between	O
the	O
variables	O
.	O
nevertheless	O
,	O
some	O
special	O
cases	O
can	O
be	O
efficiently	O
handled	O
,	O
using	O
methods	O
discussed	O
in	O
section	O
22.6.	O
given	O
the	O
model	O
,	O
we	O
can	O
compute	O
the	O
most	O
probable	O
side	O
chain	O
conﬁguration	O
using	O
y∗	O
19.6.2.7	O
stereo	O
vision	O
low-level	O
vision	O
problems	O
are	O
problems	O
where	O
the	O
input	O
is	O
an	O
image	O
(	O
or	O
set	O
of	O
images	O
)	O
,	O
and	O
the	O
output	O
is	O
a	O
processed	O
version	O
of	O
the	O
image	O
.	O
in	O
such	O
cases	O
,	O
it	O
is	O
common	O
to	O
use	O
2d	O
lattice-	O
structured	O
models	O
;	O
the	O
models	O
are	O
similar	B
to	O
figure	O
19.9	O
,	O
except	O
that	O
the	O
features	B
can	O
be	O
global	O
,	O
and	O
are	O
not	O
generated	O
by	O
the	O
model	O
.	O
we	O
will	O
assume	O
a	O
pairwise	O
crf	O
.	O
a	O
classic	O
low-level	O
vision	O
problem	O
is	O
dense	B
stereo	I
reconstruction	I
,	O
where	O
the	O
goal	O
is	O
to	O
estimate	O
the	O
depth	O
of	O
every	O
pixel	O
given	O
two	O
images	O
taken	O
from	O
slightly	O
different	O
angles	O
.	O
in	O
this	O
section	O
(	O
based	O
on	O
(	O
sudderth	O
and	O
freeman	O
2008	O
)	O
)	O
,	O
we	O
give	O
a	O
sketch	O
of	O
how	O
a	O
simple	O
crf	O
can	O
be	O
used	O
to	O
solve	O
this	O
task	O
.	O
see	O
e.g.	O
,	O
(	O
sun	O
et	O
al	O
.	O
2003	O
)	O
for	O
a	O
more	O
sophisticated	O
model	O
.	O
by	O
using	O
some	O
standard	O
preprocessing	O
techniques	O
,	O
one	O
can	O
convert	O
depth	O
estimation	O
into	O
a	O
19.6.	O
conditional	B
random	I
ﬁelds	I
(	O
crfs	O
)	O
691	O
problem	O
of	O
estimating	O
the	O
disparity	B
ys	O
between	O
the	O
pixel	O
at	O
location	O
(	O
is	O
,	O
js	O
)	O
in	O
the	O
left	O
image	O
and	O
the	O
corresponding	O
pixel	O
at	O
location	O
(	O
is	O
+	O
ys	O
,	O
js	O
)	O
in	O
the	O
right	O
image	O
.	O
we	O
typically	O
assume	O
that	O
corresponding	O
pixels	O
have	O
similar	B
intensity	O
,	O
so	O
we	O
deﬁne	O
a	O
local	O
node	O
potential	O
of	O
the	O
form	O
%	O
ψs	O
(	O
ys|x	O
)	O
∝	O
exp	O
−	O
1	O
2σ2	O
(	O
xl	O
(	O
is	O
,	O
js	O
)	O
−	O
xr	O
(	O
is	O
+	O
ys	O
,	O
js	O
)	O
)	O
2	O
(	O
cid:19	O
)	O
(	O
cid:8	O
)	O
(	O
cid:19	O
)	O
(	O
19.68	O
)	O
(	O
19.69	O
)	O
(	O
19.70	O
)	O
where	O
xl	O
is	O
the	O
left	O
image	O
and	O
xr	O
is	O
the	O
right	O
image	O
.	O
this	O
equation	O
can	O
be	O
generalized	O
to	O
model	O
the	O
intensity	O
of	O
small	O
windows	O
around	O
each	O
location	O
.	O
in	O
highly	O
textured	O
regions	O
,	O
it	O
is	O
usually	O
possible	O
to	O
ﬁnd	O
the	O
corresponding	O
patch	O
using	O
cross	O
correlation	O
,	O
but	O
in	O
regions	O
of	O
low	O
texture	O
,	O
there	O
will	O
be	O
considerable	O
ambiguity	O
about	O
the	O
correct	O
value	O
of	O
ys	O
.	O
we	O
can	O
easily	O
add	O
a	O
gaussian	O
prior	O
on	O
the	O
edges	B
of	O
the	O
mrf	O
that	O
encodes	O
the	O
assumption	O
that	O
neighboring	O
disparities	O
ys	O
,	O
yt	O
should	O
be	O
similar	B
,	O
as	O
follows	O
:	O
(	O
cid:9	O
)	O
ψst	O
(	O
ys	O
,	O
yt	O
)	O
∝	O
exp	O
−	O
1	O
2γ2	O
(	O
ys	O
−	O
yt	O
)	O
2	O
the	O
resulting	O
model	O
is	O
a	O
gaussian	O
crf	O
.	O
however	O
,	O
using	O
gaussian	O
edge-potentials	O
will	O
oversmooth	O
the	O
estimate	O
,	O
since	O
this	O
prior	O
fails	O
to	O
account	O
for	O
the	O
occasional	O
large	O
changes	O
in	O
disparity	B
that	O
occur	O
between	O
neighboring	O
pixels	O
which	O
are	O
on	O
different	O
sides	O
of	O
an	O
occlusion	O
boundary	O
.	O
one	O
gets	O
much	O
better	O
results	O
using	O
a	O
truncated	O
gaussian	O
potential	O
of	O
the	O
form	O
(	O
cid:22	O
)	O
(	O
cid:23	O
)	O
%	O
ψst	O
(	O
ys	O
,	O
yt	O
)	O
∝	O
exp	O
−	O
1	O
2γ2	O
min	O
(	O
ys	O
−	O
yt	O
)	O
2	O
,	O
δ2	O
0	O
where	O
γ	O
encodes	O
the	O
expected	O
smoothness	O
,	O
and	O
δ0	O
encodes	O
the	O
maximum	O
penalty	O
that	O
will	O
be	O
imposed	O
if	O
disparities	O
are	O
signiﬁcantly	O
different	O
.	O
this	O
is	O
called	O
a	O
discontinuity	B
preserving	I
potential	O
;	O
note	O
that	O
such	O
penalties	O
are	O
not	O
convex	O
.	O
the	O
local	B
evidence	I
potential	O
can	O
be	O
made	O
robust	B
in	O
a	O
similar	B
way	O
,	O
in	O
order	O
to	O
handle	O
outliers	B
due	O
to	O
specularities	O
,	O
occlusions	O
,	O
etc	O
.	O
figure	O
19.20	O
illustrates	O
the	O
difference	O
between	O
these	O
two	O
forms	O
of	O
prior	O
.	O
on	O
the	O
top	O
left	O
is	O
an	O
image	O
from	O
the	O
standard	O
middlebury	O
stereo	O
benchmark	O
dataset	O
(	O
scharstein	O
and	O
szeliski	O
2002	O
)	O
.	O
on	O
the	O
bottom	O
left	O
is	O
the	O
corresponding	O
true	O
disparity	O
values	O
.	O
the	O
remaining	O
columns	O
represent	O
the	O
estimated	O
disparity	O
after	O
0	O
,	O
1	O
and	O
an	O
“	O
inﬁnite	O
”	O
number	O
of	O
rounds	O
of	O
loopy	B
belief	I
propagation	I
(	O
see	O
section	O
22.2	O
)	O
,	O
where	O
by	O
“	O
inﬁnite	O
”	O
we	O
mean	B
the	O
results	O
at	O
convergence	O
.	O
the	O
top	O
row	O
shows	O
the	O
results	O
using	O
a	O
gaussian	O
edge	O
potential	O
,	O
and	O
the	O
bottom	O
row	O
shows	O
the	O
results	O
using	O
the	O
truncated	O
potential	O
.	O
the	O
latter	O
is	O
clearly	O
better	O
.	O
unfortunately	O
,	O
performing	O
inference	B
with	O
real-valued	O
variables	O
is	O
computationally	O
difficult	O
,	O
unless	O
the	O
model	O
is	O
jointly	O
gaussian	O
.	O
consequently	O
,	O
it	O
is	O
common	O
to	O
discretize	B
the	O
variables	O
.	O
(	O
for	O
example	O
,	O
figure	O
19.20	O
(	O
bottom	O
)	O
used	O
50	O
states	O
.	O
)	O
the	O
edge	O
potentials	O
still	O
have	O
the	O
form	O
given	O
in	O
equation	O
19.69.	O
the	O
resulting	O
model	O
is	O
called	O
a	O
metric	B
crf	O
,	O
since	O
the	O
potentials	O
form	O
a	O
metric	B
.	O
inference	B
in	O
metric	B
crfs	O
is	O
more	O
efficient	O
than	O
in	O
crfs	O
where	O
the	O
discrete	B
labels	O
have	O
no	O
natural	O
ordering	O
,	O
as	O
we	O
explain	O
in	O
section	O
22.6.3.3.	O
see	O
section	O
22.6.4	O
for	O
a	O
comparison	O
of	O
various	O
approximate	B
inference	I
methods	O
applied	O
to	O
low-level	O
crfs	O
,	O
and	O
see	O
(	O
blake	O
et	O
al	O
.	O
2011	O
;	O
prince	O
2012	O
)	O
for	O
more	O
details	O
on	O
probabilistic	O
models	O
for	O
computer	O
vision	O
.	O
9	O
9.	O
a	O
function	O
f	O
is	O
said	O
to	O
be	O
a	O
metric	B
if	O
it	O
satisﬁes	O
the	O
following	O
three	O
properties	O
:	O
reﬂexivity	O
:	O
f	O
(	O
a	O
,	O
b	O
)	O
=	O
0	O
iff	B
a	O
=	O
b	O
;	O
symmetry	O
:	O
f	O
(	O
a	O
,	O
b	O
)	O
=f	O
(	O
b	O
,	O
a	O
)	O
;	O
and	O
triangle	B
inequality	I
:	O
f	O
(	O
a	O
,	O
b	O
)	O
+	O
f	O
(	O
b	O
,	O
c	O
)	O
≥	O
f	O
(	O
a	O
,	O
c	O
)	O
.	O
if	O
f	O
satisﬁes	O
only	O
the	O
ﬁrst	O
two	O
properties	O
,	O
it	O
is	O
called	O
a	O
semi-metric	B
.	O
692	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
	O
	O
	O
	O
	O
	O
	O
(	O
cid:1	O
)	O
figure	O
19.20	O
illustration	O
of	O
belief	B
propagation	I
for	O
stereo	O
depth	O
estimation	O
.	O
left	O
column	O
:	O
image	O
and	O
true	O
disparities	O
.	O
remaining	O
columns	O
:	O
initial	O
estimate	O
,	O
estimate	O
after	O
1	O
iteration	O
,	O
and	O
estimate	O
at	O
convergence	O
.	O
top	O
row	O
:	O
gaussian	O
edge	O
potentials	O
.	O
bottom	O
row	O
:	O
robust	B
edge	O
potentials	O
.	O
source	O
:	O
figure	O
4	O
of	O
(	O
sudderth	O
and	O
freeman	O
2008	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
.	O
19.6.3	O
crf	O
training	O
we	O
can	O
modify	O
the	O
gradient	O
based	O
optimization	B
of	O
mrfs	O
described	O
in	O
section	O
19.5.1	O
to	O
the	O
crf	O
case	O
in	O
a	O
straightforward	O
way	O
.	O
in	O
particular	O
,	O
the	O
scaled	O
log-likelihood	O
becomes	O
(	O
cid:25	O
)	O
and	O
the	O
gradient	O
becomes	O
(	O
cid:4	O
)	O
i	O
(	O
cid:2	O
)	O
(	O
w	O
)	O
(	O
cid:2	O
)	O
1	O
n	O
∂	O
(	O
cid:2	O
)	O
∂wc	O
=	O
=	O
1	O
n	O
1	O
n	O
i	O
(	O
cid:24	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
i	O
c	O
1	O
n	O
log	O
p	O
(	O
yi|xi	O
,	O
w	O
)	O
=	O
(	O
cid:17	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
i	O
φc	O
(	O
yi	O
,	O
xi	O
)	O
−	O
∂	O
∂wc	O
[	O
φc	O
(	O
yi	O
,	O
xi	O
)	O
−	O
e	O
[	O
φc	O
(	O
y	O
,	O
xi	O
)	O
]	O
]	O
c	O
φc	O
(	O
yi	O
,	O
xi	O
)	O
−	O
log	O
z	O
(	O
w	O
,	O
xi	O
)	O
wt	O
(	O
cid:18	O
)	O
log	O
z	O
(	O
w	O
,	O
xi	O
)	O
(	O
19.71	O
)	O
(	O
19.72	O
)	O
(	O
19.73	O
)	O
(	O
19.74	O
)	O
note	O
that	O
we	O
now	O
have	O
to	O
perform	O
inference	B
for	O
every	O
single	O
training	O
case	O
inside	O
each	O
gradient	O
step	O
,	O
which	O
is	O
o	O
(	O
n	O
)	O
times	O
slower	O
than	O
the	O
mrf	O
case	O
.	O
this	O
is	O
because	O
the	O
partition	B
function	I
depends	O
on	O
the	O
inputs	O
xi	O
.	O
in	O
most	O
applications	O
of	O
crfs	O
(	O
and	O
some	O
applications	O
of	O
mrfs	O
)	O
,	O
the	O
size	O
of	O
the	O
graph	B
structure	O
can	O
vary	O
.	O
hence	O
we	O
need	O
to	O
use	O
parameter	B
tying	I
to	O
ensure	O
we	O
can	O
deﬁne	O
a	O
distribution	O
of	O
arbitrary	O
size	O
.	O
in	O
the	O
pairwise	O
case	O
,	O
we	O
can	O
write	O
the	O
model	O
as	O
follows	O
:	O
(	O
cid:22	O
)	O
(	O
cid:23	O
)	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
1	O
z	O
(	O
w	O
,	O
x	O
)	O
exp	O
wt	O
φ	O
(	O
y	O
,	O
x	O
)	O
19.7.	O
structural	O
svms	O
where	O
w	O
=	O
[	O
wn	O
,	O
we	O
]	O
are	O
the	O
node	O
and	O
edge	O
parameters	O
,	O
and	O
φ	O
(	O
y	O
,	O
x	O
)	O
(	O
cid:2	O
)	O
[	O
φt	O
(	O
yt	O
,	O
x	O
)	O
,	O
φst	O
(	O
ys	O
,	O
yt	O
,	O
x	O
)	O
]	O
(	O
cid:4	O
)	O
s∼t	O
are	O
the	O
summed	O
node	O
and	O
edge	O
features	O
(	O
these	O
are	O
the	O
sufficient	B
statistics	I
)	O
.	O
the	O
gradient	O
expression	O
is	O
easily	O
modiﬁed	O
to	O
handle	O
this	O
case	O
.	O
in	O
practice	O
,	O
it	O
is	O
important	O
to	O
use	O
a	O
prior/	O
regularization	B
to	O
prevent	O
overﬁtting	B
.	O
693	O
(	O
19.75	O
)	O
if	O
we	O
use	O
a	O
(	O
19.76	O
)	O
(	O
cid:4	O
)	O
t	O
(	O
cid:4	O
)	O
i	O
i	O
(	O
cid:4	O
)	O
gaussian	O
prior	O
,	O
the	O
new	O
objective	O
becomes	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
w	O
)	O
(	O
cid:2	O
)	O
1	O
n	O
log	O
p	O
(	O
yi|xi	O
,	O
w	O
)	O
−	O
λ||w||2	O
2	O
it	O
is	O
simple	O
to	O
modify	O
the	O
gradient	O
expression	O
.	O
alternatively	O
,	O
we	O
can	O
use	O
(	O
cid:2	O
)	O
1	O
regularization	B
.	O
for	O
example	O
,	O
we	O
could	O
use	O
(	O
cid:2	O
)	O
1	O
for	O
the	O
edge	O
weights	O
we	O
to	O
learn	O
a	O
sparse	B
graph	O
structure	O
,	O
and	O
(	O
cid:2	O
)	O
2	O
for	O
the	O
node	O
weights	O
wn	O
,	O
as	O
in	O
(	O
schmidt	O
et	O
al	O
.	O
2008	O
)	O
.	O
in	O
other	O
words	O
,	O
the	O
objective	O
becomes	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
w	O
)	O
(	O
cid:2	O
)	O
1	O
n	O
log	O
p	O
(	O
yi|xi	O
,	O
w	O
)	O
−	O
λ1||we||1	O
−	O
λ2||wn||2	O
2	O
(	O
19.77	O
)	O
unfortunately	O
,	O
the	O
optimization	B
algorithms	O
are	O
more	O
complicated	O
when	O
we	O
use	O
(	O
cid:2	O
)	O
1	O
(	O
see	O
sec-	O
tion	O
13.4	O
)	O
,	O
although	O
the	O
problem	O
is	O
still	O
convex	B
.	O
to	O
handle	O
large	O
datasets	O
,	O
we	O
can	O
use	O
stochastic	B
gradient	I
descent	I
(	O
sgd	O
)	O
,	O
as	O
described	O
in	O
section	O
8.5.2.	O
it	O
is	O
possible	O
(	O
and	O
useful	O
)	O
to	O
deﬁne	O
crfs	O
with	O
hidden	B
variables	I
,	O
for	O
example	O
to	O
allow	O
for	O
an	O
unknown	B
alignment	O
between	O
the	O
visible	B
features	O
and	O
the	O
hidden	B
labels	O
(	O
see	O
e.g.	O
,	O
(	O
schnitzspan	O
et	O
al	O
.	O
2010	O
)	O
)	O
.	O
in	O
this	O
case	O
,	O
the	O
objective	O
function	O
is	O
no	O
longer	O
convex	B
.	O
nevertheless	O
,	O
we	O
can	O
ﬁnd	O
a	O
locally	O
optimal	O
ml	O
or	O
map	O
parameter	B
estimate	O
using	O
em	O
and/	O
or	O
gradient	O
methods	O
.	O
19.7	O
structural	O
svms	O
we	O
have	O
seen	O
that	O
training	O
a	O
crf	O
requires	O
inference	B
,	O
in	O
order	O
to	O
compute	O
the	O
expected	B
sufficient	I
statistics	I
needed	O
to	O
evaluate	O
the	O
gradient	O
.	O
for	O
certain	O
models	O
,	O
computing	O
a	O
joint	O
map	O
estimate	O
of	O
the	O
states	O
is	O
provably	O
simpler	O
than	O
computing	O
marginals	O
,	O
as	O
we	O
discuss	O
in	O
section	O
22.6.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
a	O
way	O
to	O
train	O
structured	B
output	I
classiﬁers	O
that	O
that	O
leverages	O
the	O
existence	O
of	O
fast	O
map	O
solvers	O
.	O
(	O
to	O
avoid	O
confusion	O
with	O
map	O
estimation	O
of	O
parameters	O
,	O
we	O
will	O
often	O
refer	O
to	O
map	O
estimation	O
of	O
states	O
as	O
decoding	B
.	O
)	O
these	O
methods	O
are	O
known	O
as	O
structural	B
support	I
vector	I
machines	I
or	O
ssvms	O
(	O
tsochantaridis	O
et	O
al	O
.	O
2005	O
)	O
.	O
(	O
there	O
is	O
also	O
a	O
very	O
similar	B
class	O
of	O
methods	O
known	O
as	O
max	O
margin	O
markov	O
networks	O
or	O
m3nets	O
(	O
taskar	O
et	O
al	O
.	O
2003	O
)	O
;	O
see	O
section	O
19.7.2	O
for	O
a	O
discussion	O
of	O
the	O
differences	O
.	O
)	O
19.7.1	O
ssvms	O
:	O
a	O
probabilistic	O
view	O
in	O
this	O
book	O
,	O
we	O
have	O
mostly	O
concentrated	O
on	O
ﬁtting	O
models	O
using	O
map	O
parameter	B
estimation	O
,	O
i.e.	O
,	O
by	O
minimizing	O
functions	O
of	O
the	O
form	O
log	O
p	O
(	O
yi|xi	O
,	O
w	O
)	O
(	O
19.78	O
)	O
rm	O
ap	O
(	O
w	O
)	O
=	O
−	O
log	O
p	O
(	O
w	O
)	O
−	O
n	O
(	O
cid:4	O
)	O
i=1	O
694	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
however	O
,	O
at	O
test	O
time	O
,	O
we	O
pick	O
the	O
label	B
so	O
as	O
to	O
minimize	O
the	O
posterior	B
expected	I
loss	I
(	O
deﬁned	O
in	O
section	O
5.7	O
)	O
:	O
ˆy	O
(	O
x|w	O
)	O
=	O
argmin	O
l	O
(	O
ˆy	O
,	O
y	O
)	O
p	O
(	O
y|x	O
,	O
w	O
)	O
(	O
cid:4	O
)	O
ˆy	O
y	O
where	O
l	O
(	O
y∗	O
,	O
ˆy	O
)	O
is	O
the	O
loss	B
we	O
incur	O
when	O
we	O
estimate	O
ˆy	O
but	O
the	O
truth	O
is	O
y∗	O
.	O
it	O
therefore	O
seems	O
reasonable	O
to	O
take	O
the	O
loss	B
function	I
into	O
account	O
when	O
performing	O
parameter	B
estimation.10	O
so	O
,	O
following	O
(	O
yuille	O
and	O
he	O
2011	O
)	O
,	O
let	O
us	O
instead	O
minimized	O
the	O
posterior	B
expected	I
loss	I
on	O
the	O
training	B
set	I
:	O
(	O
cid:25	O
)	O
n	O
(	O
cid:4	O
)	O
(	O
cid:24	O
)	O
(	O
cid:4	O
)	O
rel	O
(	O
w	O
)	O
(	O
cid:2	O
)	O
−	O
log	O
p	O
(	O
w	O
)	O
+	O
log	O
l	O
(	O
yi	O
,	O
y	O
)	O
p	O
(	O
y|xi	O
,	O
w	O
)	O
in	O
the	O
special	O
case	O
of	O
0-1	O
loss	B
,	O
l	O
(	O
yi	O
,	O
y	O
)	O
=	O
1	O
−	O
δy	O
,	O
yi	O
,	O
this	O
reduces	O
to	O
rm	O
ap	O
.	O
i=1	O
y	O
we	O
will	O
assume	O
that	O
we	O
can	O
write	O
our	O
model	O
in	O
the	O
following	O
form	O
:	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
(	O
19.79	O
)	O
(	O
19.80	O
)	O
(	O
19.81	O
)	O
(	O
19.82	O
)	O
exp	O
(	O
wt	O
φ	O
(	O
x	O
,	O
y	O
)	O
)	O
z	O
(	O
x	O
,	O
w	O
)	O
exp	O
(	O
−e	O
(	O
w	O
)	O
)	O
(	O
cid:7	O
)	O
z	O
p	O
(	O
w	O
)	O
=	O
where	O
z	O
(	O
x	O
,	O
w	O
)	O
=	O
this	O
,	O
we	O
can	O
rewrite	O
our	O
objective	O
as	O
follows	O
:	O
(	O
cid:4	O
)	O
(	O
cid:24	O
)	O
(	O
cid:4	O
)	O
y	O
exp	O
(	O
wt	O
φ	O
(	O
x	O
,	O
y	O
)	O
)	O
.	O
also	O
,	O
let	O
us	O
deﬁne	O
l	O
(	O
yi	O
,	O
y	O
)	O
=	O
exp	O
˜l	O
(	O
yi	O
,	O
y	O
)	O
.	O
with	O
(	O
cid:25	O
)	O
rel	O
(	O
w	O
)	O
=−	O
log	O
p	O
(	O
w	O
)	O
+	O
(	O
cid:4	O
)	O
=	O
e	O
(	O
w	O
)	O
+	O
log	O
i	O
y	O
exp	O
˜l	O
(	O
yi	O
,	O
y	O
)	O
(	O
cid:4	O
)	O
!	O
exp	O
(	O
wt	O
φ	O
(	O
x	O
,	O
y	O
)	O
)	O
z	O
(	O
x	O
,	O
w	O
)	O
˜l	O
(	O
vyi	O
,	O
y	O
)	O
+w	O
t	O
φ	O
(	O
xi	O
,	O
y	O
)	O
(	O
19.83	O
)	O
#	O
(	O
19.84	O
)	O
i	O
−	O
log	O
z	O
(	O
xi	O
,	O
w	O
)	O
+	O
log	O
(	O
cid:8	O
)	O
(	O
cid:17	O
)	O
exp	O
y	O
(	O
cid:9	O
)	O
(	O
cid:18	O
)	O
(	O
cid:4	O
)	O
y∈y	O
we	O
will	O
now	O
consider	O
various	O
bounds	O
in	O
order	O
to	O
simplify	O
this	O
objective	O
.	O
first	O
note	O
that	O
for	O
any	O
function	O
f	O
(	O
y	O
)	O
we	O
have	O
y∈y	O
f	O
(	O
y	O
)	O
≤	O
log	O
max	O
exp	O
[	O
f	O
(	O
y	O
)	O
]	O
≤	O
log	O
|y|	O
exp	O
max	O
y	O
f	O
(	O
y	O
)	O
=	O
log	O
|y|	O
+	O
max	O
y	O
f	O
(	O
y	O
)	O
(	O
19.85	O
)	O
for	O
example	O
,	O
suppose	O
y	O
=	O
{	O
0	O
,	O
1	O
,	O
2	O
}	O
and	O
f	O
(	O
y	O
)	O
=	O
y.	O
then	O
we	O
have	O
2	O
=	O
log	O
[	O
exp	O
(	O
2	O
)	O
]	O
≤	O
log	O
[	O
exp	O
(	O
0	O
)	O
+	O
exp	O
(	O
1	O
)	O
+	O
exp	O
(	O
2	O
)	O
]	O
≤	O
log	O
[	O
3	O
×	O
exp	O
(	O
2	O
)	O
]	O
=	O
log	O
(	O
3	O
)	O
+	O
2	O
(	O
19.86	O
)	O
we	O
can	O
ignore	O
the	O
log	O
|y|	O
term	O
,	O
which	O
is	O
independent	O
of	O
y	O
,	O
and	O
treat	O
maxy∈y	O
f	O
(	O
y	O
)	O
as	O
both	O
a	O
lower	O
and	O
upper	O
bound	O
.	O
hence	O
we	O
see	O
that	O
rel	O
(	O
w	O
)	O
∼	O
e	O
(	O
w	O
)	O
+	O
˜l	O
(	O
yi	O
,	O
y	O
)	O
+w	O
t	O
φ	O
(	O
xi	O
,	O
y	O
)	O
wt	O
φ	O
(	O
xi	O
,	O
y	O
)	O
(	O
19.87	O
)	O
’	O
−	O
max	O
y	O
(	O
cid:18	O
)	O
&	O
(	O
cid:17	O
)	O
n	O
(	O
cid:4	O
)	O
max	O
y	O
i=1	O
10.	O
note	O
that	O
this	O
violates	O
the	O
fundamental	O
bayesian	O
distinction	O
between	O
inference	B
and	O
decision	B
making	O
.	O
however	O
,	O
performing	O
these	O
tasks	O
separately	O
will	O
only	O
result	O
in	O
an	O
optimal	O
decision	O
if	O
we	O
can	O
compute	O
the	O
exact	O
posterior	O
.	O
in	O
most	O
cases	O
,	O
this	O
is	O
intractable	O
,	O
so	O
we	O
need	O
to	O
perform	O
loss-calibrated	B
inference	I
(	O
lacoste-julien	O
et	O
al	O
.	O
2011	O
)	O
.	O
in	O
this	O
section	O
,	O
we	O
just	O
perform	O
loss-calibrated	O
map	O
parameter	B
estimation	O
,	O
which	O
is	O
computationally	O
simpler	O
.	O
(	O
see	O
also	O
(	O
stoyanov	O
et	O
al	O
.	O
2011	O
)	O
.	O
)	O
19.7.	O
structural	O
svms	O
695	O
where	O
x	O
∼	O
y	O
means	O
c1	O
+	O
x	O
≤	O
y	O
+	O
c2	O
for	O
some	O
constants	O
c1	O
,	O
c2	O
.	O
unfortunately	O
,	O
this	O
objective	O
is	O
not	O
convex	O
in	O
w.	O
however	O
,	O
we	O
can	O
devise	O
a	O
convex	B
upper	O
bound	O
by	O
exploiting	O
the	O
following	O
looser	O
lower	O
bound	O
on	O
the	O
log-sum-exp	B
function	O
:	O
exp	O
[	O
f	O
(	O
y	O
)	O
]	O
(	O
19.88	O
)	O
f	O
(	O
y	O
(	O
cid:2	O
)	O
)	O
≤	O
log	O
(	O
cid:4	O
)	O
y	O
for	O
any	O
y	O
(	O
cid:2	O
)	O
∈	O
y.	O
applying	O
this	O
equation	O
to	O
our	O
earlier	O
example	O
,	O
for	O
f	O
(	O
y	O
)	O
=	O
y	O
and	O
y	O
(	O
cid:2	O
)	O
1	O
=	O
log	O
[	O
exp	O
(	O
1	O
)	O
]	O
≤	O
log	O
[	O
exp	O
(	O
0	O
)	O
+	O
exp	O
(	O
1	O
)	O
+	O
exp	O
(	O
2	O
)	O
]	O
.	O
and	O
applying	O
this	O
bound	O
to	O
rel	O
we	O
get	O
(	O
cid:18	O
)	O
(	O
cid:17	O
)	O
=	O
1	O
,	O
we	O
get	O
’	O
˜l	O
(	O
yi	O
,	O
y	O
)	O
+w	O
t	O
φ	O
(	O
xi	O
,	O
y	O
)	O
−	O
wt	O
φ	O
(	O
xi	O
,	O
yi	O
)	O
(	O
19.89	O
)	O
rel	O
(	O
w	O
)	O
≤	O
e	O
(	O
w	O
)	O
+	O
&	O
n	O
(	O
cid:4	O
)	O
max	O
y	O
i=1	O
if	O
we	O
set	O
e	O
(	O
w	O
)	O
=	O
−	O
1	O
2c||w||2	O
2	O
(	O
corresponding	O
to	O
a	O
spherical	B
gaussian	O
prior	O
)	O
,	O
we	O
get	O
’	O
(	O
cid:18	O
)	O
rssv	O
m	O
(	O
w	O
)	O
(	O
cid:2	O
)	O
1	O
2	O
||w||2	O
+	O
c	O
˜l	O
(	O
yi	O
,	O
y	O
)	O
+w	O
t	O
φ	O
(	O
xi	O
,	O
y	O
)	O
−	O
wt	O
φ	O
(	O
xi	O
,	O
yi	O
)	O
(	O
19.90	O
)	O
&	O
(	O
cid:17	O
)	O
n	O
(	O
cid:4	O
)	O
max	O
y	O
i=1	O
this	O
is	O
the	O
same	O
objective	O
as	O
used	O
in	O
the	O
ssvm	O
approach	O
of	O
(	O
tsochantaridis	O
et	O
al	O
.	O
2005	O
)	O
.	O
in	O
the	O
special	O
case	O
that	O
y	O
=	O
{	O
−1	O
,	O
+1	O
}	O
l	O
(	O
y∗	O
,	O
y	O
)	O
=	O
1	O
−	O
δy	O
,	O
y∗	O
,	O
and	O
φ	O
(	O
x	O
,	O
y	O
)	O
=	O
1	O
criterion	O
reduces	O
to	O
the	O
following	O
(	O
by	O
considering	O
the	O
two	O
cases	O
that	O
y	O
=	O
yi	O
and	O
y	O
(	O
cid:8	O
)	O
=	O
yi	O
)	O
:	O
2	O
yx	O
,	O
this	O
rsv	O
m	O
(	O
w	O
)	O
(	O
cid:2	O
)	O
1	O
2	O
||w||2	O
+	O
c	O
(	O
cid:13	O
)	O
max	O
{	O
0	O
,	O
1	O
−	O
yiwt	O
xi	O
}	O
(	O
cid:14	O
)	O
n	O
(	O
cid:4	O
)	O
i=1	O
(	O
19.91	O
)	O
which	O
is	O
the	O
standard	O
binary	O
svm	O
objective	O
(	O
see	O
equation	O
14.57	O
)	O
.	O
so	O
we	O
see	O
that	O
the	O
ssvm	O
criterion	O
can	O
be	O
seen	O
as	O
optimizing	O
an	O
upper	O
bound	O
on	O
the	O
bayesian	O
objective	O
,	O
a	O
result	O
ﬁrst	O
shown	O
in	O
(	O
yuille	O
and	O
he	O
2011	O
)	O
.	O
this	O
bound	O
will	O
be	O
tight	O
(	O
and	O
hence	O
the	O
approximation	O
will	O
be	O
a	O
good	O
one	O
)	O
when	O
||w||	O
is	O
large	O
,	O
since	O
in	O
that	O
case	O
,	O
p	O
(	O
y|x	O
,	O
w	O
)	O
will	O
concentrate	O
its	O
mass	O
on	O
argmaxy	O
p	O
(	O
y|x	O
,	O
w	O
)	O
.	O
unfortunately	O
,	O
a	O
large	O
||w||	O
corresponds	O
to	O
a	O
model	O
that	O
is	O
likely	O
to	O
overﬁt	B
,	O
so	O
it	O
is	O
unlikely	O
that	O
we	O
will	O
be	O
working	O
in	O
this	O
regime	O
(	O
because	O
we	O
will	O
tune	O
the	O
strength	O
of	O
the	O
regularizer	O
to	O
avoid	O
this	O
situation	O
)	O
.	O
an	O
alternative	O
justiﬁcation	O
for	O
the	O
svm	O
criterion	O
is	O
that	O
it	O
focusses	O
effort	O
on	O
ﬁtting	O
parameters	O
that	O
affect	O
the	O
decision	B
boundary	I
.	O
this	O
is	O
a	O
better	O
use	O
of	O
computational	O
resources	O
than	O
ﬁtting	O
the	O
full	B
distribution	O
,	O
especially	O
when	O
the	O
model	O
is	O
wrong	O
.	O
19.7.2	O
ssvms	O
:	O
a	O
non-probabilistic	O
view	O
we	O
now	O
present	O
ssvms	O
in	O
a	O
more	O
traditional	O
(	O
non-probabilistic	O
)	O
way	O
,	O
following	O
(	O
tsochantaridis	O
et	O
al	O
.	O
2005	O
)	O
.	O
the	O
resulting	O
objective	O
will	O
be	O
the	O
same	O
as	O
the	O
one	O
above	O
.	O
however	O
,	O
this	O
derivation	O
will	O
set	O
the	O
stage	O
for	O
the	O
algorithms	O
we	O
discuss	O
below	O
.	O
let	O
f	O
(	O
x	O
;	O
w	O
)	O
=	O
argmaxy∈y	O
wt	O
φ	O
(	O
x	O
,	O
y	O
)	O
be	O
the	O
prediction	O
function	O
.	O
we	O
can	O
obtain	O
zero	O
loss	O
on	O
the	O
training	B
set	I
using	O
this	O
predictor	O
if	O
wt	O
φ	O
(	O
xi	O
,	O
y	O
)	O
≤	O
wt	O
φ	O
(	O
xi	O
,	O
yi	O
)	O
∀i	O
.	O
max	O
y∈y\yi	O
(	O
19.92	O
)	O
696	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
each	O
one	O
of	O
these	O
nonlinear	O
inequalities	O
can	O
be	O
equivalently	O
replaced	O
by	O
|y|	O
−	O
1	O
linear	O
inequal-	O
ities	O
,	O
resulting	O
in	O
a	O
total	O
of	O
n|y|	O
−	O
n	O
linear	O
constraints	O
of	O
the	O
following	O
form	O
:	O
∀i.∀y	O
∈	O
y	O
\	O
yi.wt	O
φ	O
(	O
xi	O
,	O
yi	O
)	O
−	O
wt	O
φ	O
(	O
xi	O
,	O
y	O
)	O
≥	O
0	O
for	O
brevity	O
,	O
we	O
introduce	O
the	O
notation	O
δi	O
(	O
y	O
)	O
(	O
cid:2	O
)	O
φ	O
(	O
xi	O
,	O
yi	O
)	O
−	O
φ	O
(	O
xi	O
,	O
y	O
)	O
so	O
we	O
can	O
rewrite	O
these	O
constraints	O
as	O
wt	O
δi	O
(	O
y	O
)	O
≥	O
0	O
.	O
(	O
19.93	O
)	O
(	O
19.94	O
)	O
(	O
19.96	O
)	O
(	O
19.97	O
)	O
if	O
we	O
can	O
achieve	O
zero	O
loss	O
,	O
there	O
will	O
typically	O
be	O
multiple	O
solution	O
vectors	O
w.	O
we	O
pick	O
the	O
one	O
that	O
maximizes	O
the	O
margin	B
,	O
deﬁned	O
as	O
f	O
(	O
x	O
,	O
y	O
(	O
cid:2	O
)	O
f	O
(	O
x	O
,	O
yi	O
;	O
w	O
)	O
−	O
max	O
y	O
(	O
cid:2	O
)	O
∈y\y	O
γ	O
(	O
cid:2	O
)	O
min	O
i	O
;	O
w	O
)	O
(	O
19.95	O
)	O
since	O
the	O
margin	B
can	O
be	O
made	O
arbitrarily	O
large	O
by	O
rescaling	O
w	O
,	O
we	O
ﬁx	O
its	O
norm	O
to	O
be	O
1	O
,	O
resulting	O
in	O
the	O
optimization	B
problem	O
max	O
γ	O
,	O
w	O
:	O
||w||=1	O
s.t	O
.	O
∀i.∀y	O
∈	O
y	O
\	O
yi	O
.	O
wt	O
δi	O
(	O
y	O
)	O
≥	O
γ	O
equivalently	O
,	O
we	O
can	O
write	O
||w||2	O
min	O
w	O
1	O
2	O
s.t	O
.	O
∀i.∀y	O
∈	O
y	O
\	O
yi	O
.	O
wt	O
δi	O
(	O
y	O
)	O
≥	O
1	O
to	O
allow	O
for	O
the	O
case	O
where	O
zero	O
loss	O
can	O
not	O
be	O
achieved	O
(	O
equivalent	O
to	O
the	O
data	O
being	O
inseparable	O
in	O
the	O
case	O
of	O
binary	B
classiﬁcation	I
)	O
,	O
we	O
relax	O
the	O
constraints	O
by	O
introducing	O
slack	O
terms	O
ξi	O
,	O
one	O
per	O
data	O
case	O
.	O
this	O
yields	O
||w||2	O
+	O
c	O
min	O
w	O
,	O
ξ	O
1	O
2	O
s.t	O
.	O
∀i.∀y	O
∈	O
y	O
\	O
yi	O
.	O
wt	O
δi	O
(	O
y	O
)	O
≥	O
1	O
−	O
ξi	O
,	O
ξi	O
≥	O
0	O
ξi	O
(	O
19.98	O
)	O
in	O
the	O
case	O
of	O
structured	O
outputs	O
,	O
we	O
don	O
’	O
t	O
want	O
to	O
treat	O
all	O
constraint	O
violations	O
equally	O
.	O
for	O
example	O
,	O
in	O
a	O
segmentation	O
problem	O
,	O
getting	O
one	O
position	O
wrong	O
should	O
be	O
punished	O
less	O
than	O
getting	O
many	O
positions	O
wrong	O
.	O
one	O
way	O
to	O
achieve	O
this	O
is	O
to	O
divide	O
the	O
slack	O
variable	O
by	O
the	O
size	O
of	O
the	O
loss	B
(	O
this	O
is	O
called	O
slack	B
re-scaling	I
)	O
.	O
this	O
yields	O
||w||2	O
+	O
c	O
min	O
w	O
,	O
ξ	O
1	O
2	O
s.t	O
.	O
∀i.∀y	O
∈	O
y	O
\	O
yi	O
.	O
wt	O
δi	O
(	O
y	O
)	O
≥	O
1	O
−	O
ξi	O
ξi	O
l	O
(	O
yi	O
,	O
y	O
)	O
,	O
ξi	O
≥	O
0	O
(	O
19.99	O
)	O
alternatively	O
,	O
we	O
can	O
deﬁne	O
the	O
margin	B
to	O
be	O
proportional	O
to	O
the	O
loss	B
(	O
this	O
is	O
called	O
margin	B
re-rescaling	O
)	O
.	O
this	O
yields	O
||w||2	O
+	O
c	O
min	O
w	O
,	O
ξ	O
1	O
2	O
ξi	O
s.t	O
.	O
∀i.∀y	O
∈	O
y	O
\	O
yi	O
.	O
wt	O
δi	O
(	O
y	O
)	O
≥	O
l	O
(	O
yi	O
,	O
y	O
)	O
−	O
ξi	O
,	O
ξi	O
≥	O
0	O
(	O
19.100	O
)	O
(	O
in	O
fact	O
,	O
we	O
can	O
write	O
∀y	O
∈	O
y	O
instead	O
of	O
∀y	O
∈	O
y	O
\	O
yi	O
,	O
since	O
if	O
y	O
=	O
yi	O
,	O
then	O
wt	O
δi	O
(	O
y	O
)	O
=	O
0	O
and	O
ξi	O
=	O
0.	O
by	O
using	O
the	O
simpler	O
notation	O
,	O
which	O
doesn	O
’	O
t	O
exclude	O
yi	O
,	O
we	O
add	O
an	O
extra	O
but	O
redundant	O
constraint	O
.	O
)	O
this	O
latter	O
approach	O
is	O
used	O
in	O
m3nets	O
.	O
n	O
(	O
cid:4	O
)	O
i=1	O
n	O
(	O
cid:4	O
)	O
i=1	O
n	O
(	O
cid:4	O
)	O
i=1	O
19.7.	O
structural	O
svms	O
for	O
future	O
reference	O
,	O
note	O
that	O
we	O
can	O
solve	O
for	O
the	O
ξ∗	O
i	O
(	O
w	O
)	O
=	O
max	O
{	O
0	O
,	O
max	O
(	O
l	O
(	O
yi	O
,	O
y	O
)	O
−	O
wt	O
δi	O
)	O
)	O
}	O
=	O
max	O
ξ∗	O
(	O
cid:4	O
)	O
y	O
y	O
(	O
l	O
(	O
yi	O
,	O
y	O
)	O
+w	O
t	O
φ	O
(	O
xi	O
,	O
y	O
)	O
||w||2	O
+	O
c	O
min	O
w	O
1	O
2	O
max	O
y	O
i	O
i	O
terms	O
as	O
follows	O
:	O
(	O
l	O
(	O
yi	O
,	O
y	O
)	O
−	O
wt	O
δi	O
)	O
)	O
)	O
−	O
wt	O
φ	O
(	O
xi	O
,	O
yi	O
)	O
substituting	O
in	O
,	O
and	O
dropping	O
the	O
constraints	O
,	O
we	O
get	O
the	O
following	O
equivalent	O
problem	O
:	O
697	O
(	O
19.101	O
)	O
(	O
19.102	O
)	O
19.7.2.1	O
empirical	B
risk	I
minimization	I
let	O
us	O
pause	O
and	O
consider	O
whether	O
the	O
above	O
objective	O
is	O
reasonable	O
.	O
recall	B
that	O
in	O
the	O
frequen-	O
tist	O
approach	O
to	O
machine	B
learning	I
(	O
section	O
6.5	O
)	O
,	O
the	O
goal	O
is	O
to	O
minimize	O
the	O
regularized	O
empirical	O
risk	B
,	O
deﬁned	O
by	O
r	O
(	O
w	O
)	O
+	O
c	O
n	O
l	O
(	O
yi	O
,	O
f	O
(	O
xi	O
,	O
w	O
)	O
)	O
(	O
19.103	O
)	O
where	O
r	O
(	O
w	O
)	O
is	O
the	O
regularizer	O
,	O
and	O
f	O
(	O
xi	O
,	O
w	O
)	O
=	O
argmaxy	O
wt	O
φ	O
(	O
xi	O
,	O
y	O
)	O
=	O
ˆyi	O
is	O
the	O
prediction	O
.	O
since	O
this	O
objective	O
is	O
hard	O
to	O
optimize	O
,	O
because	O
the	O
loss	B
is	O
not	O
differentiable	O
,	O
we	O
will	O
construct	O
a	O
convex	B
upper	O
bound	O
instead	O
.	O
n	O
(	O
cid:4	O
)	O
i=1	O
(	O
cid:4	O
)	O
i	O
we	O
can	O
show	O
that	O
r	O
(	O
w	O
)	O
+	O
c	O
n	O
max	O
y	O
(	O
l	O
(	O
yi	O
,	O
y	O
)	O
−	O
wt	O
δi	O
)	O
)	O
is	O
such	O
a	O
convex	B
upper	O
bound	O
.	O
to	O
see	O
this	O
,	O
note	O
that	O
l	O
(	O
yi	O
,	O
f	O
(	O
xi	O
,	O
w	O
)	O
)	O
≤	O
l	O
(	O
yi	O
,	O
f	O
(	O
xi	O
,	O
w	O
)	O
)	O
−	O
wt	O
φ	O
(	O
xi	O
,	O
yi	O
)	O
+w	O
t	O
φ	O
(	O
xi	O
,	O
ˆyi	O
)	O
≤	O
max	O
using	O
this	O
bound	O
and	O
r	O
(	O
w	O
)	O
=	O
1	O
l	O
(	O
yi	O
,	O
y	O
)	O
−	O
wt	O
φ	O
(	O
xi	O
,	O
yi	O
)	O
+w	O
t	O
φ	O
(	O
xi	O
,	O
y	O
)	O
2||w||2	O
yields	O
equation	O
19.102.	O
y	O
(	O
19.104	O
)	O
(	O
19.105	O
)	O
(	O
19.106	O
)	O
19.7.2.2	O
computational	O
issues	O
although	O
the	O
above	O
objectives	O
are	O
simple	O
quadratic	O
programs	O
(	O
qp	O
)	O
,	O
they	O
have	O
o	O
(	O
n|y|	O
)	O
con-	O
straints	O
.	O
this	O
is	O
intractable	O
,	O
since	O
y	O
is	O
usually	O
exponentially	O
large	O
.	O
in	O
the	O
case	O
of	O
the	O
margin	B
rescaling	O
formulation	O
,	O
it	O
is	O
possible	O
to	O
reduce	O
the	O
exponential	O
number	O
of	O
constraints	O
to	O
a	O
poly-	O
nomial	O
number	O
,	O
provided	O
the	O
loss	B
function	I
and	O
the	O
feature	O
vector	O
decompose	O
according	O
to	O
a	O
graphical	B
model	I
.	O
this	O
is	O
the	O
approach	O
used	O
in	O
m3nets	O
(	O
taskar	O
et	O
al	O
.	O
2003	O
)	O
.	O
an	O
alternative	O
approach	O
is	O
to	O
work	O
directly	O
with	O
the	O
exponentially	O
sized	O
qp	O
.	O
this	O
allows	O
for	O
the	O
use	O
of	O
more	O
general	O
loss	B
functions	O
.	O
there	O
are	O
several	O
possible	O
methods	O
to	O
make	O
this	O
feasible	O
.	O
one	O
is	O
to	O
use	O
cutting	B
plane	I
methods	O
.	O
another	O
is	O
to	O
use	O
stochastic	O
subgradient	O
methods	O
.	O
we	O
discuss	O
both	O
of	O
these	O
below	O
.	O
698	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
fi	O
l	O
l	O
c	O
i	O
illustration	O
of	O
the	O
cutting	B
plane	I
algorithm	O
in	O
2d	O
.	O
we	O
start	O
with	O
the	O
estimate	O
w	O
=	O
w0	O
=	O
0.	O
figure	O
19.21	O
(	O
a	O
)	O
we	O
add	O
the	O
ﬁrst	O
constraint	O
;	O
the	O
shaded	O
region	O
is	O
the	O
new	O
feasible	O
set	O
.	O
the	O
new	O
minimum	O
norm	O
solution	O
is	O
w1	O
.	O
(	O
c	O
)	O
we	O
add	O
a	O
third	O
constraint	O
.	O
source	O
:	O
figure	O
5.3	O
of	O
(	O
altun	O
et	O
al	O
.	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
yasemin	O
altun	O
.	O
(	O
b	O
)	O
we	O
add	O
another	O
constraint	O
;	O
the	O
dark	O
shaded	O
region	O
is	O
the	O
new	O
feasible	O
set	O
.	O
f	O
h	O
h	O
s	O
l	O
h	O
l	O
19.7.3	O
cutting	B
plane	I
methods	O
for	O
ﬁtting	O
ssvms	O
in	O
this	O
section	O
,	O
we	O
discuss	O
an	O
efficient	O
algorithm	O
for	O
ﬁtting	O
ssvms	O
due	O
to	O
(	O
joachims	O
et	O
al	O
.	O
2009	O
)	O
.	O
this	O
method	O
can	O
handle	O
general	O
loss	B
functions	O
,	O
and	O
is	O
implemented	O
in	O
the	O
popular	O
svmstruct	O
package11	O
.	O
the	O
method	O
is	O
based	O
on	O
the	O
cutting	B
plane	I
method	O
from	O
convex	B
optimization	O
(	O
kelley	O
1960	O
)	O
.	O
the	O
basic	O
idea	O
is	O
as	O
follows	O
.	O
we	O
start	O
with	O
an	O
initial	O
guess	O
w	O
and	O
no	O
constraints	O
.	O
at	O
each	O
iteration	O
,	O
we	O
then	O
do	O
the	O
following	O
:	O
for	O
each	O
example	O
i	O
,	O
we	O
ﬁnd	O
the	O
“	O
most	O
violated	O
”	O
constraint	O
involving	O
xi	O
and	O
ˆyi	O
.	O
if	O
the	O
loss-augmented	O
margin	O
violation	O
exceeds	O
the	O
current	O
value	O
of	O
ξi	O
by	O
more	O
than	O
	O
,	O
we	O
add	O
ˆyi	O
to	O
the	O
working	O
set	O
of	O
constraints	O
for	O
this	O
training	O
case	O
,	O
wi	O
,	O
and	O
then	O
solve	O
the	O
resulting	O
new	O
qp	O
to	O
ﬁnd	O
the	O
new	O
w	O
,	O
ξ.	O
see	O
figure	O
19.21	O
for	O
a	O
sketch	O
,	O
and	O
algorithm	O
11	O
for	O
the	O
pseudo	O
code	O
.	O
(	O
since	O
at	O
each	O
step	O
we	O
only	O
add	O
one	O
new	O
constraint	O
,	O
we	O
can	O
warm-start	O
the	O
qp	O
solver	O
.	O
)	O
we	O
can	O
can	O
easily	O
modify	O
the	O
algorithm	O
to	O
optimize	O
the	O
slack	O
rescaling	O
version	O
by	O
replacing	O
the	O
expression	O
l	O
(	O
yi	O
,	O
y	O
)	O
−	O
wt	O
δi	O
(	O
ˆyi	O
)	O
with	O
l	O
(	O
yi	O
,	O
y	O
)	O
(	O
1	O
−	O
wt	O
δi	O
(	O
ˆyi	O
)	O
)	O
.	O
the	O
key	O
to	O
the	O
efficiency	O
of	O
this	O
method	O
is	O
that	O
only	O
polynomially	O
many	O
constraints	O
need	O
to	O
be	O
added	O
,	O
and	O
as	O
soon	O
as	O
they	O
are	O
,	O
the	O
exponential	O
number	O
of	O
other	O
constraints	O
are	O
guaranteed	O
to	O
also	O
be	O
satisﬁed	O
to	O
within	O
a	O
tolerance	O
of	O
	O
(	O
see	O
(	O
tsochantaridis	O
et	O
al	O
.	O
2005	O
)	O
for	O
the	O
proof	O
)	O
.	O
19.7.3.1	O
loss-augmented	B
decoding	I
the	O
other	O
key	O
to	O
efficiency	O
is	O
the	O
ability	O
to	O
ﬁnd	O
the	O
most	O
violated	O
constraint	O
in	O
line	O
5	O
of	O
the	O
algorithm	O
,	O
i.e.	O
,	O
to	O
compute	O
argmax	O
y∈y	O
l	O
(	O
yi	O
,	O
y	O
)	O
−	O
wt	O
δi	O
(	O
y	O
)	O
=	O
argmax	O
y∈y	O
l	O
(	O
yi	O
,	O
y	O
)	O
+	O
wt	O
φ	O
(	O
xi	O
,	O
y	O
)	O
(	O
19.107	O
)	O
11.	O
http	O
:	O
//svmlight.joachims.org/svm_struct.html	O
19.7.	O
structural	O
svms	O
699	O
algorithm	O
19.3	O
:	O
cutting	B
plane	I
algorithm	O
for	O
ssvms	O
(	O
margin	B
rescaling	O
,	O
n	O
-slack	O
version	O
)	O
1	O
input	O
d	O
=	O
{	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
}	O
,	O
c	O
,	O
	O
;	O
2	O
wi	O
=	O
∅	O
,	O
ξi	O
=	O
0	O
for	O
i	O
=	O
1	O
:	O
n	O
;	O
3	O
repeat	O
4	O
for	O
i	O
=	O
1	O
:	O
n	O
do	O
ˆyi	O
=	O
argmaxˆyi∈y	O
l	O
(	O
yi	O
,	O
y	O
)	O
−	O
wt	O
δi	O
(	O
ˆyi	O
)	O
;	O
if	O
l	O
(	O
yi	O
,	O
y	O
)	O
−	O
wt	O
δi	O
(	O
ˆyi	O
)	O
>	O
ξi	O
+	O
	O
then	O
(	O
cid:7	O
)	O
n	O
wi	O
=	O
wi	O
∪	O
{	O
ˆyi	O
}	O
;	O
(	O
w	O
,	O
ξ	O
)	O
=	O
argminw	O
,	O
ξ≥0	O
2||w||2	O
2	O
+	O
c	O
1	O
i=1	O
ξi	O
;	O
5	O
6	O
7	O
8	O
9	O
s.t	O
.	O
∀i	O
=	O
1	O
:	O
n	O
,	O
∀y	O
(	O
cid:2	O
)	O
∈	O
wi	O
:	O
wt	O
δi	O
(	O
ˆyi	O
)	O
≥	O
l	O
(	O
yi	O
,	O
y	O
(	O
cid:2	O
)	O
)	O
−	O
ξi	O
;	O
10	O
until	O
no	O
wi	O
has	O
changed	O
;	O
11	O
return	O
(	O
w	O
,	O
ξ	O
)	O
we	O
call	O
this	O
process	O
loss-augmented	B
decoding	I
.	O
(	O
in	O
(	O
joachims	O
et	O
al	O
.	O
2009	O
)	O
,	O
this	O
procedure	O
is	O
called	O
the	O
separation	B
oracle	I
.	O
)	O
if	O
the	O
loss	B
function	I
has	O
an	O
additive	O
decomposition	O
of	O
the	O
same	O
form	O
as	O
the	O
features	B
,	O
then	O
we	O
can	O
fold	O
the	O
loss	B
into	O
the	O
weight	B
vector	I
,	O
i.e.	O
,	O
we	O
can	O
ﬁnd	O
a	O
new	O
set	O
of	O
parameters	O
w	O
(	O
cid:2	O
)	O
)	O
t	O
δi	O
(	O
y	O
)	O
=	O
wt	O
δi	O
(	O
y	O
)	O
.	O
we	O
can	O
then	O
use	O
a	O
standard	O
decoding	O
algorithm	O
,	O
such	O
as	O
viterbi	O
,	O
on	O
the	O
model	O
p	O
(	O
y|x	O
,	O
w	O
(	O
cid:2	O
)	O
with	O
a	O
value	O
of	O
of	O
0	O
−	O
wt	O
δi	O
(	O
ˆy	O
)	O
,	O
or	O
it	O
will	O
be	O
the	O
second	O
best	O
solution	O
,	O
i.e.	O
,	O
in	O
the	O
special	O
case	O
of	O
0-1	O
loss	B
,	O
the	O
optimum	O
will	O
either	O
be	O
the	O
best	O
solution	O
,	O
argmaxy	O
wt	O
φ	O
(	O
xi	O
,	O
y	O
)	O
,	O
such	O
that	O
(	O
w	O
(	O
cid:2	O
)	O
)	O
.	O
wt	O
φ	O
(	O
xi	O
,	O
y	O
)	O
˜y	O
=	O
argmax	O
y	O
(	O
cid:5	O
)	O
=ˆy	O
(	O
19.108	O
)	O
which	O
achieves	O
an	O
overall	O
value	O
of	O
1	O
−	O
wt	O
δi	O
(	O
˜y	O
)	O
.	O
for	O
chain	O
structured	O
crfs	O
,	O
we	O
can	O
use	O
the	O
viterbi	O
algorithm	O
to	O
do	O
decoding	O
;	O
the	O
second	O
best	O
path	B
will	O
differ	O
from	O
the	O
best	O
path	B
in	O
a	O
single	O
position	O
,	O
which	O
can	O
be	O
obtained	O
by	O
changing	O
the	O
variable	O
whose	O
max	O
marginal	O
is	O
closest	O
to	O
its	O
decision	B
boundary	I
to	O
its	O
second	O
best	O
value	O
.	O
we	O
can	O
generalize	B
this	O
(	O
with	O
a	O
bit	O
more	O
work	O
)	O
to	O
ﬁnd	O
the	O
n	O
-best	O
list	O
(	O
schwarz	O
and	O
chow	O
1990	O
;	O
nilsson	O
and	O
goldberger	O
2001	O
)	O
.	O
t	O
(	O
cid:8	O
)	O
=	O
yt	O
)	O
,	O
and	O
for	O
the	O
f1	O
score	O
(	O
deﬁned	O
in	O
section	O
5.7.2.3	O
)	O
,	O
we	O
can	O
devise	O
a	O
dynamic	B
programming	I
algorithm	O
to	O
compute	O
equation	O
19.107.	O
see	O
(	O
altun	O
et	O
al	O
.	O
2006	O
)	O
for	O
details	O
.	O
other	O
models	O
and	O
loss	B
function	I
combinations	O
will	O
require	O
different	O
methods	O
.	O
for	O
hamming	O
loss	O
,	O
l	O
(	O
y∗	O
,	O
y	O
)	O
=	O
t	O
i	O
(	O
y∗	O
(	O
cid:7	O
)	O
19.7.3.2	O
a	O
linear	O
time	O
algorithm	O
although	O
the	O
above	O
algorithm	O
takes	O
polynomial	O
time	O
,	O
we	O
can	O
do	O
better	O
,	O
and	O
devise	O
an	O
algorithm	O
that	O
runs	O
in	O
linear	O
time	O
,	O
assuming	O
we	O
use	O
a	O
linear	B
kernel	I
(	O
i.e.	O
,	O
we	O
work	O
with	O
the	O
original	O
features	B
φ	O
(	O
x	O
,	O
y	O
)	O
and	O
do	O
not	O
apply	O
the	O
kernel	B
trick	I
)	O
.	O
the	O
basic	O
idea	O
,	O
as	O
explained	O
in	O
(	O
joachims	O
et	O
al	O
.	O
2009	O
)	O
,	O
is	O
to	O
have	O
a	O
single	O
slack	O
variable	O
,	O
ξ	O
,	O
instead	O
of	O
n	O
,	O
but	O
to	O
use	O
|y|n	O
constraints	O
,	O
instead	O
of	O
700	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
just	O
n|y|	O
.	O
speciﬁcally	O
,	O
we	O
optimize	O
the	O
following	O
(	O
assuming	O
the	O
margin	B
rescaling	O
formulation	O
)	O
:	O
min	O
w	O
,	O
ξ≥0	O
||w||2	O
2	O
+	O
cξ	O
1	O
2	O
s.t	O
.	O
∀	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
)	O
∈	O
y	O
n	O
:	O
1	O
n	O
wt	O
n	O
(	O
cid:4	O
)	O
i=1	O
δi	O
(	O
yi	O
)	O
≥	O
1	O
n	O
n	O
(	O
cid:4	O
)	O
i=1	O
l	O
(	O
yi	O
,	O
yi	O
)	O
−	O
ξ	O
(	O
19.109	O
)	O
compare	O
this	O
to	O
the	O
original	O
version	O
,	O
which	O
was	O
min	O
w	O
,	O
ξ≥0	O
||w||2	O
2	O
+	O
c	O
n	O
ξ	O
1	O
2	O
s.t	O
.	O
∀i	O
=	O
1	O
:	O
n	O
,	O
∀y	O
∈	O
y	O
:	O
wt	O
δi	O
(	O
y	O
)	O
≥	O
l	O
(	O
yi	O
,	O
yi	O
)	O
−	O
ξi	O
(	O
19.110	O
)	O
one	O
can	O
show	O
that	O
any	O
solution	O
w∗	O
vice	O
versa	O
,	O
with	O
ξ∗	O
=	O
1	O
n	O
ξ∗	O
i	O
.	O
of	O
equation	O
19.109	O
is	O
also	O
a	O
solution	O
of	O
equation	O
19.110	O
and	O
(	O
w	O
,	O
ξ	O
)	O
=	O
argminw	O
,	O
ξ≥0	O
algorithm	O
19.4	O
:	O
cutting	B
plane	I
algorithm	O
for	O
ssvms	O
(	O
margin	B
rescaling	O
,	O
1-slack	O
version	O
)	O
1	O
input	O
d	O
=	O
{	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
}	O
,	O
c	O
,	O
	O
;	O
2	O
w	O
=	O
∅	O
;	O
(	O
cid:7	O
)	O
n	O
3	O
repeat	O
2	O
+	O
c	O
4	O
s.t	O
.	O
∀	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
)	O
∈	O
w	O
:	O
1	O
for	O
i	O
=	O
1	O
:	O
n	O
do	O
(	O
cid:7	O
)	O
n	O
(	O
cid:7	O
)	O
n	O
w	O
=	O
w	O
∪	O
{	O
(	O
ˆy1	O
,	O
.	O
.	O
.	O
,	O
ˆyn	O
)	O
}	O
;	O
i=1	O
δi	O
(	O
ˆyi	O
)	O
≤	O
ξ	O
+	O
	O
;	O
i=1	O
l	O
(	O
yi	O
,	O
ˆyi	O
)	O
−	O
1	O
n	O
wt	O
(	O
cid:7	O
)	O
n	O
i=1	O
δi	O
(	O
yi	O
)	O
≥	O
1	O
(	O
cid:7	O
)	O
n	O
i=1	O
l	O
(	O
yi	O
,	O
yi	O
)	O
−	O
ξ	O
;	O
ˆyi	O
=	O
argmaxˆyi∈y	O
l	O
(	O
yi	O
,	O
ˆyi	O
)	O
+w	O
t	O
φ	O
(	O
xi	O
,	O
ˆyi	O
)	O
1	O
2||w||2	O
i=1	O
ξ	O
;	O
5	O
6	O
7	O
n	O
wt	O
n	O
8	O
9	O
until	O
10	O
return	O
(	O
w	O
,	O
ξ	O
)	O
1	O
n	O
we	O
can	O
optimize	O
equation	O
19.109	O
using	O
the	O
cutting	B
plane	I
algorithm	O
in	O
algorithm	O
10	O
.	O
(	O
this	O
is	O
what	O
is	O
implemented	O
in	O
svmstruct	O
.	O
)	O
the	O
inner	O
qp	O
in	O
line	O
4	O
can	O
be	O
solved	O
in	O
o	O
(	O
n	O
)	O
time	O
using	O
the	O
method	O
of	O
(	O
joachims	O
2006	O
)	O
.	O
in	O
line	O
7	O
we	O
make	O
n	O
calls	O
to	O
the	O
loss-augmented	O
decoder	O
.	O
finally	O
,	O
it	O
can	O
be	O
shown	O
that	O
the	O
number	O
of	O
iterations	O
is	O
a	O
constant	O
independent	O
on	O
n	O
.	O
thus	O
the	O
overall	O
running	O
time	O
is	O
linear	O
.	O
19.7.4	O
online	O
algorithms	O
for	O
ﬁtting	O
ssvms	O
although	O
the	O
cutting	B
plane	I
algorithm	O
can	O
be	O
made	O
to	O
run	O
in	O
time	O
linear	O
in	O
the	O
number	O
of	O
data	O
points	O
,	O
that	O
can	O
still	O
be	O
slow	O
if	O
we	O
have	O
a	O
large	O
dataset	O
.	O
in	O
such	O
cases	O
,	O
it	O
is	O
preferable	O
to	O
use	O
online	B
learning	I
.	O
we	O
brieﬂy	O
mention	O
a	O
few	O
possible	O
algorithms	O
below	O
.	O
19.7.4.1	O
the	O
structured	B
perceptron	I
algorithm	I
a	O
very	O
simple	O
algorithm	O
for	O
ﬁtting	O
ssvms	O
is	O
the	O
structured	B
perceptron	I
algorithm	I
(	O
collins	O
2002	O
)	O
.	O
this	O
method	O
is	O
an	O
extension	B
of	O
the	O
regular	B
perceptron	O
algorithm	O
of	O
section	O
8.5.4.	O
at	O
each	O
19.7.	O
structural	O
svms	O
701	O
step	O
,	O
we	O
compute	O
ˆy	O
=	O
argmax	O
p	O
(	O
y|x	O
)	O
(	O
e.g.	O
,	O
using	O
the	O
viterbi	O
algorithm	O
)	O
for	O
the	O
current	O
training	O
sample	O
x.	O
if	O
ˆy	O
=	O
y	O
,	O
we	O
do	O
nothing	O
,	O
otherwise	O
we	O
update	O
the	O
weight	B
vector	I
using	O
wk+1	O
=	O
wk	O
+	O
φ	O
(	O
y	O
,	O
x	O
)	O
−	O
φ	O
(	O
ˆy	O
,	O
x	O
)	O
(	O
19.111	O
)	O
to	O
get	O
good	O
performance	O
,	O
it	O
is	O
necessary	O
to	O
average	O
the	O
parameters	O
over	O
the	O
last	O
few	O
updates	O
(	O
see	O
section	O
8.5.2	O
for	O
details	O
)	O
,	O
rather	O
than	O
using	O
the	O
most	O
recent	O
value	O
.	O
19.7.4.2	O
stochastic	O
subgradient	O
descent	O
the	O
disadvantage	O
of	O
the	O
structured	B
perceptron	I
algorithm	I
is	O
that	O
it	O
implicitly	O
assumes	O
0-1	O
loss	B
,	O
and	O
it	O
does	O
not	O
enforce	O
any	O
kind	O
of	O
margin	B
.	O
an	O
alternative	O
approach	O
is	O
to	O
perform	O
stochastic	O
subgradient	O
descent	O
.	O
a	O
speciﬁc	O
instance	O
of	O
this	O
the	O
pegasos	O
algorithm	O
(	O
shalev-shwartz	O
et	O
al	O
.	O
2007	O
)	O
,	O
which	O
stands	O
for	O
“	O
primal	O
estimated	O
sub-gradient	O
solver	O
for	O
svm	O
”	O
.	O
pegasos	O
was	O
designed	O
for	O
binary	O
svms	O
,	O
but	O
can	O
be	O
extended	O
to	O
ssvms	O
.	O
let	O
us	O
start	O
by	O
considering	O
the	O
objective	O
function	O
:	O
l	O
(	O
yi	O
,	O
ˆyi	O
)	O
+w	O
t	O
φ	O
(	O
xi	O
,	O
ˆyi	O
)	O
(	O
cid:14	O
)	O
−	O
wt	O
φ	O
(	O
xi	O
,	O
yi	O
)	O
+λ||w	O
||2	O
n	O
(	O
cid:4	O
)	O
(	O
cid:13	O
)	O
f	O
(	O
w	O
)	O
=	O
max	O
ˆyi	O
i=1	O
n	O
(	O
cid:4	O
)	O
(	O
19.112	O
)	O
(	O
19.113	O
)	O
letting	O
ˆyi	O
be	O
the	O
argmax	O
of	O
this	O
max	O
.	O
then	O
the	O
subgradient	B
of	O
this	O
objective	O
function	O
is	O
g	O
(	O
w	O
)	O
=	O
φ	O
(	O
xi	O
,	O
ˆyi	O
)	O
−	O
φ	O
(	O
xi	O
,	O
yi	O
)	O
+	O
2λw	O
i=1	O
in	O
stochastic	O
subgradient	O
descent	O
,	O
we	O
approximate	O
this	O
gradient	O
with	O
a	O
single	O
term	O
,	O
i	O
,	O
and	O
then	O
perform	O
an	O
update	O
:	O
wk+1	O
=	O
wk	O
−	O
ηkgi	O
(	O
wk	O
)	O
=	O
wk	O
−	O
ηk	O
[	O
φ	O
(	O
xi	O
,	O
ˆyi	O
)	O
−	O
φ	O
(	O
xi	O
,	O
yi	O
)	O
+	O
(	O
2/n	O
)	O
λw	O
]	O
(	O
19.114	O
)	O
where	O
ηk	O
is	O
the	O
step	B
size	I
parameter	O
,	O
which	O
should	O
satisfy	O
the	O
robbins-monro	O
conditions	O
(	O
sec-	O
(	O
notice	O
that	O
the	O
perceptron	B
algorithm	I
is	O
just	O
a	O
special	O
case	O
where	O
λ	O
=	O
0	O
and	O
tion	O
8.5.2.1	O
)	O
.	O
ηk	O
=	O
1	O
.	O
)	O
to	O
ensure	O
that	O
w	O
has	O
unit	O
norm	O
,	O
we	O
can	O
project	O
it	O
onto	O
the	O
(	O
cid:2	O
)	O
2	O
ball	O
after	O
each	O
update	O
.	O
19.7.5	O
latent	B
structural	O
svms	O
in	O
many	O
applications	O
of	O
interest	O
,	O
we	O
have	O
latent	B
or	O
hidden	B
variables	I
h.	O
for	O
example	O
,	O
in	O
object	O
detections	O
problems	O
,	O
we	O
may	O
be	O
told	O
that	O
the	O
image	O
contains	O
an	O
object	O
,	O
so	O
y	O
=	O
1	O
,	O
but	O
we	O
may	O
not	O
know	O
where	O
it	O
is	O
.	O
the	O
location	O
of	O
the	O
object	O
,	O
or	O
its	O
pose	O
,	O
can	O
be	O
considered	O
a	O
hidden	B
variable	I
.	O
or	O
in	O
machine	O
translation	O
,	O
we	O
may	O
know	O
the	O
source	O
text	O
x	O
(	O
say	O
english	O
)	O
and	O
the	O
target	O
text	O
y	O
(	O
say	O
french	O
)	O
,	O
but	O
we	O
typically	O
do	O
not	O
know	O
the	O
alignment	B
between	O
the	O
words	O
.	O
we	O
will	O
extend	O
our	O
model	O
as	O
follows	O
,	O
to	O
get	O
a	O
latent	B
crf	O
:	O
p	O
(	O
y	O
,	O
h|x	O
,	O
w	O
)	O
=	O
exp	O
(	O
wt	O
φ	O
(	O
x	O
,	O
y	O
,	O
h	O
)	O
)	O
z	O
(	O
x	O
,	O
w	O
)	O
z	O
(	O
x	O
,	O
w	O
)	O
=	O
exp	O
(	O
wt	O
φ	O
(	O
x	O
,	O
y	O
,	O
h	O
)	O
)	O
(	O
cid:4	O
)	O
y	O
,	O
h	O
(	O
19.115	O
)	O
(	O
19.116	O
)	O
702	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
in	O
addition	O
,	O
we	O
introduce	O
the	O
loss	B
function	I
l	O
(	O
y∗	O
,	O
y	O
,	O
h	O
)	O
;	O
this	O
measures	O
the	O
loss	B
when	O
the	O
“	O
action	B
”	O
that	O
we	O
take	O
is	O
to	O
predict	O
y	O
using	O
latent	B
variables	O
h.	O
we	O
could	O
just	O
use	O
l	O
(	O
y∗	O
,	O
y	O
)	O
as	O
before	O
,	O
since	O
h	O
is	O
usually	O
a	O
nuisance	O
variable	O
and	O
not	O
of	O
direct	O
interest	O
.	O
however	O
,	O
h	O
can	O
sometimes	O
play	O
a	O
useful	O
role	O
in	O
deﬁning	O
a	O
loss	B
function.12	O
given	O
the	O
loss	B
function	I
,	O
we	O
deﬁne	O
our	O
objective	O
as	O
⎤	O
⎦	O
(	O
19.117	O
)	O
(	O
19.118	O
)	O
rel	O
(	O
w	O
)	O
=−	O
log	O
p	O
(	O
w	O
)	O
+	O
exp	O
˜l	O
(	O
yi	O
,	O
y	O
,	O
h	O
)	O
exp	O
(	O
wt	O
φ	O
(	O
x	O
,	O
y	O
,	O
h	O
)	O
)	O
z	O
(	O
x	O
,	O
w	O
)	O
using	O
the	O
same	O
loose	O
lower	O
bound	O
as	O
before	O
,	O
we	O
get	O
rel	O
(	O
w	O
)	O
≤	O
e	O
(	O
w	O
)	O
+	O
˜l	O
(	O
yi	O
,	O
y	O
,	O
h	O
)	O
+w	O
t	O
φ	O
(	O
xi	O
,	O
y	O
,	O
h	O
)	O
’	O
(	O
cid:4	O
)	O
⎡	O
⎣	O
(	O
cid:4	O
)	O
i	O
y	O
,	O
h	O
log	O
&	O
n	O
(	O
cid:4	O
)	O
max	O
y	O
,	O
h	O
i=1	O
wt	O
φ	O
(	O
xi	O
,	O
yi	O
,	O
h	O
)	O
max	O
h	O
−	O
n	O
(	O
cid:4	O
)	O
i=1	O
if	O
we	O
set	O
e	O
(	O
w	O
)	O
=−	O
1	O
and	O
joachims	O
2009	O
)	O
.	O
2c||w||2	O
2	O
,	O
we	O
get	O
the	O
same	O
objective	O
as	O
is	O
optimized	O
in	O
latent	B
svms	O
(	O
yu	O
unfortunately	O
,	O
this	O
objective	O
is	O
no	O
longer	O
convex	B
.	O
however	O
,	O
it	O
is	O
a	O
difference	O
of	O
convex	B
functions	O
,	O
and	O
hence	O
can	O
be	O
solved	O
efficiently	O
using	O
the	O
cccp	O
or	O
concave-convex	B
procedure	I
(	O
yuille	O
and	O
rangarajan	O
2003	O
)	O
.	O
this	O
is	O
a	O
method	O
for	O
minimizing	O
functions	O
of	O
the	O
form	O
f	O
(	O
w	O
)	O
−	O
g	O
(	O
w	O
)	O
,	O
where	O
f	O
and	O
g	O
are	O
convex	B
.	O
the	O
method	O
alternates	O
between	O
ﬁnding	O
a	O
linear	O
upper	O
bound	O
u	O
on	O
−g	O
,	O
and	O
then	O
minimizing	O
the	O
convex	B
function	O
f	O
(	O
w	O
)	O
+u	O
(	O
w	O
)	O
;	O
see	O
algorithm	O
6	O
for	O
the	O
pseudocode	O
.	O
cccp	O
is	O
guaranteed	O
to	O
decrease	O
the	O
objective	O
at	O
every	O
iteration	O
,	O
and	O
to	O
converge	B
to	O
a	O
local	O
minimum	O
or	O
a	O
saddle	O
point	O
.	O
algorithm	O
19.5	O
:	O
concave-convex	B
procedure	I
(	O
cccp	O
)	O
1	O
set	O
t	O
=	O
0	O
and	O
initialize	O
w0	O
;	O
2	O
repeat	O
3	O
find	O
hyperplane	O
vt	O
such	O
that	O
−g	O
(	O
w	O
)	O
≤	O
−g	O
(	O
wt	O
)	O
+	O
(	O
w	O
−	O
wt	O
)	O
t	O
vt	O
for	O
all	O
w	O
;	O
solve	O
wt+1	O
=	O
argminw	O
f	O
(	O
w	O
)	O
+w	O
t	O
vt	O
;	O
set	O
t	O
=	O
t	O
+	O
1	O
5	O
6	O
until	O
converged	O
;	O
4	O
when	O
applied	O
to	O
latent	B
ssvms	O
,	O
cccp	O
is	O
very	O
similar	B
to	O
(	O
hard	O
)	O
em	O
.	O
in	O
the	O
“	O
e	O
step	O
”	O
,	O
we	O
compute	O
12.	O
for	O
example	O
,	O
consider	O
the	O
problem	O
of	O
learning	O
to	O
classify	O
a	O
set	O
of	O
documents	O
as	O
relevant	O
or	O
not	O
to	O
a	O
query	O
.	O
that	O
is	O
,	O
given	O
n	O
documents	O
x1	O
,	O
.	O
.	O
.	O
,	O
x	O
n	O
for	O
a	O
single	O
query	O
q	O
,	O
we	O
want	O
to	O
produce	O
a	O
labeling	O
yj	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
,	O
representing	O
whether	O
document	O
j	O
is	O
relevant	O
to	O
q	O
or	O
not	O
.	O
suppose	O
our	O
goal	O
is	O
to	O
maximize	O
the	O
precision	B
at	I
k	I
,	O
which	O
is	O
a	O
metric	B
widely	O
used	O
in	O
ranking	B
(	O
see	O
section	O
9.7.4	O
)	O
.	O
we	O
will	O
introduce	O
a	O
latent	O
variable	O
for	O
each	O
document	O
hj	O
representing	O
its	O
degree	B
(	O
cid:2	O
)	O
k	O
of	O
relevance	O
.	O
this	O
corresponds	O
to	O
a	O
latent	B
total	O
ordering	O
,	O
that	O
has	O
to	O
be	O
consistent	B
with	O
the	O
observed	O
partial	O
ordering	O
y.	O
given	O
this	O
,	O
we	O
can	O
deﬁne	O
the	O
following	O
loss	B
function	I
:	O
l	O
(	O
y	O
,	O
ˆy	O
,	O
ˆh	O
)	O
=	O
min	O
{	O
1	O
,	O
n	O
(	O
y	O
)	O
j=1	O
i	O
(	O
yhj	O
=	O
1	O
)	O
,	O
where	O
n	O
(	O
y	O
)	O
is	O
the	O
total	O
number	O
of	O
relevant	O
documents	O
.	O
this	O
loss	B
is	O
essentially	O
just	O
1	O
minus	O
the	O
precision	B
@	O
k	O
,	O
except	O
we	O
replace	O
1	O
with	O
n	O
(	O
y	O
)	O
/k	O
so	O
that	O
the	O
loss	B
will	O
have	O
a	O
minimum	O
of	O
zero	O
.	O
see	O
(	O
yu	O
and	O
joachims	O
2009	O
)	O
for	O
details.	O
}	O
−	O
1	O
k	O
k	O
19.7.	O
structural	O
svms	O
the	O
linear	O
upper	O
bound	O
by	O
setting	O
vt	O
=	O
−c	O
hi	O
=	O
argmax	O
h	O
wt	O
t	O
φ	O
(	O
xi	O
,	O
yi	O
,	O
h	O
)	O
(	O
cid:7	O
)	O
n	O
i=1	O
φ	O
(	O
xi	O
,	O
yi	O
,	O
h∗	O
i	O
)	O
,	O
where	O
703	O
(	O
19.119	O
)	O
in	O
the	O
“	O
m	O
step	O
”	O
,	O
we	O
estimate	O
w	O
using	O
techniques	O
for	O
solving	O
fully	O
visible	B
ssvms	O
.	O
speciﬁcally	O
,	O
we	O
minimize	O
l	O
(	O
yi	O
,	O
y	O
,	O
h	O
)	O
+w	O
t	O
φ	O
(	O
xi	O
,	O
y	O
,	O
h	O
)	O
wt	O
φ	O
(	O
xi	O
,	O
yi	O
,	O
h∗	O
i	O
)	O
(	O
19.120	O
)	O
(	O
n	O
(	O
cid:4	O
)	O
max	O
y	O
,	O
h	O
i=1	O
||w||2	O
+	O
c	O
1	O
2	O
exercises	O
)	O
−	O
c	O
n	O
(	O
cid:4	O
)	O
i=1	O
exercise	O
19.1	O
derivative	O
of	O
the	O
log	B
partition	I
function	I
derive	O
equation	O
19.40.	O
exercise	O
19.2	O
ci	O
properties	O
of	O
gaussian	O
graphical	O
models	O
(	O
source	O
:	O
jordan	O
.	O
)	O
in	O
this	O
question	O
,	O
we	O
study	O
the	O
relationship	O
between	O
sparse	B
matrices	O
and	O
sparse	B
graphs	O
for	O
gaussian	O
graphical	O
models	O
.	O
consider	O
a	O
multivariate	O
gaussian	O
n	O
(	O
x|μ	O
,	O
σ	O
)	O
in	O
3	O
dimensions	O
.	O
suppose	O
μ	O
=	O
(	O
0	O
,	O
0	O
,	O
0	O
)	O
t	O
throughout	O
.	O
recall	B
that	O
for	O
jointly	O
gaussian	O
random	O
variables	O
,	O
we	O
know	O
that	O
xi	O
and	O
xj	O
are	O
independent	O
iff	O
they	O
are	O
uncorrelated	O
,	O
ie	O
.	O
σij	O
=	O
0	O
.	O
(	O
this	O
is	O
not	O
true	O
in	O
general	O
,	O
or	O
even	O
if	O
xi	O
and	O
xj	O
are	O
gaussian	O
but	O
not	O
jointly	O
gaussian	O
.	O
)	O
also	O
,	O
xi	O
is	O
conditionally	B
independent	I
of	O
xj	O
given	O
all	O
the	O
other	O
variables	O
iff	B
σ	O
−1	O
ij	O
=	O
0.	O
a.	O
suppose	O
σ	O
=	O
⎛	O
⎝0.75	O
0.5	O
0.25	O
⎞	O
⎠	O
0.5	O
1.0	O
0.5	O
0.25	O
0.5	O
0.75	O
are	O
there	O
any	O
marginal	O
independencies	O
amongst	O
x1	O
,	O
x2	O
and	O
x3	O
?	O
what	O
about	O
conditional	O
indepen-	O
−1x	O
:	O
which	O
pairwise	O
terms	O
xixj	O
are	O
missing	B
?	O
draw	O
dencies	O
?	O
hint	O
:	O
compute	O
σ	O
an	O
undirected	B
graphical	I
model	I
that	O
captures	O
as	O
many	O
of	O
these	O
independence	O
statements	O
(	O
marginal	O
and	O
conditional	O
)	O
as	O
possible	O
,	O
but	O
does	O
not	O
make	O
any	O
false	O
independence	O
assertions	O
.	O
−1	O
and	O
expand	O
out	O
xt	O
σ	O
b.	O
suppose	O
σ	O
=	O
⎛	O
⎝2	O
1	O
0	O
⎞	O
⎠	O
1	O
2	O
1	O
0	O
1	O
2	O
are	O
there	O
any	O
marginal	O
independencies	O
amongst	O
x1	O
,	O
x2	O
and	O
x3	O
?	O
are	O
there	O
any	O
conditional	O
inde-	O
pendencies	O
amongst	O
x1	O
,	O
x2	O
and	O
x3	O
?	O
draw	O
an	O
undirected	B
graphical	I
model	I
that	O
captures	O
as	O
many	O
of	O
these	O
independence	O
statements	O
(	O
marginal	O
and	O
conditional	O
)	O
as	O
possible	O
,	O
but	O
does	O
not	O
make	O
any	O
false	O
independence	O
assertions	O
.	O
c.	O
now	O
suppose	O
the	O
distribution	O
on	O
x	O
can	O
be	O
represented	O
by	O
the	O
following	O
dag	O
:	O
x1	O
→	O
x2	O
→	O
x3	O
let	O
the	O
cpds	O
be	O
as	O
follows	O
:	O
p	O
(	O
x1	O
)	O
=	O
n	O
(	O
x1	O
;	O
0	O
,	O
1	O
)	O
,	O
p	O
(	O
x2|x1	O
)	O
=	O
n	O
(	O
x2	O
;	O
x1	O
,	O
1	O
)	O
,	O
p	O
(	O
x3|x2	O
)	O
=	O
n	O
(	O
x3	O
;	O
x2	O
,	O
1	O
)	O
(	O
19.121	O
)	O
multiply	O
these	O
3	O
cpds	O
together	O
and	O
complete	B
the	O
square	O
(	O
bishop	O
p101	O
)	O
to	O
ﬁnd	O
the	O
corresponding	O
joint	B
distribution	I
n	O
(	O
x1:3|μ	O
,	O
σ	O
)	O
.	O
(	O
you	O
may	O
ﬁnd	O
it	O
easier	O
to	O
solve	O
for	O
σ	O
−1	O
rather	O
than	O
σ	O
.	O
)	O
704	O
chapter	O
19.	O
undirected	O
graphical	O
models	O
(	O
markov	O
random	O
ﬁelds	O
)	O
d.	O
for	O
the	O
dag	O
model	O
in	O
the	O
previous	O
question	O
:	O
are	O
there	O
any	O
marginal	O
independencies	O
amongst	O
x1	O
,	O
x2	O
and	O
x3	O
?	O
what	O
about	O
conditional	O
independencies	O
?	O
draw	O
an	O
undirected	B
graphical	I
model	I
that	O
captures	O
as	O
many	O
of	O
these	O
independence	O
statements	O
as	O
possible	O
,	O
but	O
does	O
not	O
make	O
any	O
false	O
independence	O
assertions	O
(	O
either	O
marginal	O
or	O
conditional	O
)	O
.	O
exercise	O
19.3	O
independencies	O
in	O
gaussian	O
graphical	O
models	O
(	O
source	O
:	O
mackay	O
.	O
)	O
a.	O
consider	O
the	O
dag	O
x1	O
←	O
x2	O
→	O
x3	O
.	O
assume	O
that	O
all	O
the	O
cpds	O
are	O
linear-gaussian	O
.	O
which	O
of	O
the	O
⎞	O
⎠	O
(	O
19.122	O
)	O
following	O
matrices	O
could	O
be	O
the	O
covariance	B
matrix	I
?	O
⎞	O
⎠	O
,	O
d	O
=	O
⎞	O
⎠	O
,	O
b	O
=	O
⎞	O
⎠	O
,	O
c	O
=	O
⎛	O
⎝9	O
⎛	O
⎝9	O
a	O
=	O
⎛	O
⎝	O
9	O
−3	O
−3	O
10−3	O
0	O
−3	O
0	O
9	O
⎛	O
⎝	O
8	O
−3	O
−3	O
9−3	O
1	O
−3	O
1	O
8	O
3	O
9	O
3	O
0	O
3	O
9	O
3	O
0	O
3	O
9	O
3	O
1	O
3	O
9	O
3	O
1	O
b.	O
which	O
of	O
the	O
above	O
matrices	O
could	O
be	O
inverse	O
covariance	O
matrix	O
?	O
c.	O
consider	O
the	O
dag	O
x1	O
→	O
x2	O
←	O
x3	O
.	O
assume	O
that	O
all	O
the	O
cpds	O
are	O
linear-gaussian	O
.	O
which	O
of	O
the	O
above	O
matrices	O
could	O
be	O
the	O
covariance	B
matrix	I
?	O
d.	O
which	O
of	O
the	O
above	O
matrices	O
could	O
be	O
the	O
inverse	O
covariance	O
matrix	O
?	O
e.	O
let	O
three	O
variables	O
x1	O
,	O
x2	O
,	O
x4	O
have	O
covariance	B
matrix	I
σ	O
(	O
1:3	O
)	O
and	O
precision	B
matrix	I
ω	O
(	O
1:3	O
)	O
=	O
σ−1	O
(	O
1:3	O
)	O
as	O
follows	O
σ	O
(	O
1:3	O
)	O
=	O
⎛	O
⎝	O
1	O
0.5	O
0	O
(	O
cid:3	O
)	O
0.5	O
1	O
0.5	O
0	O
0.5	O
1	O
(	O
cid:4	O
)	O
⎞	O
⎠	O
,	O
ω	O
(	O
1:3	O
)	O
=	O
⎞	O
⎠	O
0.5	O
1.5	O
⎛	O
⎝1.5	O
−1	O
−1	O
2−1	O
0.5	O
−1	O
(	O
cid:3	O
)	O
1.5	O
−1	O
−1	O
2	O
(	O
cid:4	O
)	O
(	O
19.123	O
)	O
(	O
19.124	O
)	O
now	O
focus	O
on	O
x1	O
and	O
x2	O
.	O
which	O
of	O
the	O
following	O
statements	O
about	O
their	O
covariance	B
matrix	I
σ	O
(	O
1:2	O
)	O
and	O
precision	B
matrix	I
ω	O
(	O
1:2	O
)	O
are	O
true	O
?	O
a	O
:	O
σ	O
(	O
1:2	O
)	O
=	O
,	O
b	O
:	O
ω	O
(	O
1:2	O
)	O
=	O
1	O
0.5	O
0.5	O
1	O
exercise	O
19.4	O
cost	O
of	O
training	O
mrfs	O
and	O
crfs	O
(	O
source	O
:	O
koller	O
.	O
)	O
consider	O
the	O
process	O
of	O
gradient-ascent	O
training	O
for	O
a	O
log-linear	B
model	O
with	O
k	O
features	B
,	O
given	O
a	O
data	O
set	O
with	O
n	O
training	O
instances	O
.	O
assume	O
for	O
simplicity	O
that	O
the	O
cost	O
of	O
computing	O
a	O
single	O
feature	O
over	O
a	O
single	O
instance	O
in	O
our	O
data	O
set	O
is	O
constant	O
,	O
as	O
is	O
the	O
cost	O
of	O
computing	O
the	O
expected	B
value	I
of	O
each	O
feature	O
once	O
we	O
compute	O
a	O
marginal	O
over	O
the	O
variables	O
in	O
its	O
scope	B
.	O
assume	O
that	O
it	O
takes	O
c	O
time	O
to	O
compute	O
all	O
the	O
marginals	O
for	O
each	O
data	O
case	O
.	O
also	O
,	O
assume	O
that	O
we	O
need	O
r	O
iterations	O
for	O
the	O
gradient	O
process	O
to	O
converge	B
.	O
•	O
using	O
this	O
notation	O
,	O
what	O
is	O
the	O
time	O
required	O
to	O
train	O
an	O
mrf	O
in	O
big-o	O
notation	O
?	O
•	O
using	O
this	O
notation	O
,	O
what	O
is	O
the	O
time	O
required	O
to	O
train	O
a	O
crf	O
in	O
big-o	O
notation	O
?	O
exercise	O
19.5	O
full	B
conditional	I
in	O
an	O
ising	O
model	O
consider	O
an	O
ising	O
model	O
p	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn|θ	O
)	O
=	O
exp	O
(	O
jijxixj	O
)	O
(	O
cid:13	O
)	O
1	O
z	O
(	O
θ	O
)	O
<	O
ij	O
>	O
n	O
(	O
cid:13	O
)	O
i=1	O
exp	O
(	O
hixi	O
)	O
(	O
19.125	O
)	O
where	O
<	O
ij	O
>	O
denotes	O
all	O
unique	O
pairs	O
(	O
i.e.	O
,	O
all	O
edges	O
)	O
,	O
jij	O
∈	O
r	O
is	O
the	O
coupling	O
strength	O
(	O
weight	O
)	O
on	O
edge	O
i	O
−	O
j	O
,	O
hi	O
∈	O
r	O
is	O
the	O
local	B
evidence	I
(	O
bias	B
term	I
)	O
,	O
and	O
θ	O
=	O
(	O
j	O
,	O
h	O
)	O
are	O
all	O
the	O
parameters	O
.	O
19.7.	O
structural	O
svms	O
if	O
xi	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
derive	O
an	O
expression	O
for	O
the	O
full	B
conditional	I
p	O
(	O
xi	O
=	O
1|x−i	O
,	O
θ	O
)	O
=	O
p	O
(	O
xi	O
=	O
1|xnbi	O
,	O
θ	O
)	O
705	O
(	O
19.126	O
)	O
where	O
x−i	O
are	O
all	O
nodes	O
except	O
i	O
,	O
and	O
nbi	O
are	O
the	O
neighbors	B
of	O
i	O
in	O
the	O
graph	B
.	O
hint	O
:	O
you	O
answer	O
should	O
use	O
the	O
sigmoid/	O
logistic	B
function	O
σ	O
(	O
z	O
)	O
=	O
1/	O
(	O
1	O
+	O
e−z	O
)	O
.	O
now	O
suppose	O
xi	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
.	O
derive	O
a	O
related	O
expression	O
for	O
p	O
(	O
xi|x−i	O
,	O
θ	O
)	O
in	O
this	O
case	O
.	O
(	O
this	O
result	O
can	O
be	O
used	O
when	O
applying	O
gibbs	O
sampling	O
to	O
the	O
model	O
.	O
)	O
20	O
exact	O
inference	B
for	O
graphical	O
models	O
20.1	O
introduction	O
in	O
section	O
17.4.3	O
,	O
we	O
discussed	O
the	O
forwards-backwards	B
algorithm	I
,	O
which	O
can	O
exactly	O
compute	O
the	O
posterior	O
marginals	O
p	O
(	O
xt|v	O
,	O
θ	O
)	O
in	O
any	O
chain-structured	O
graphical	B
model	I
,	O
where	O
x	O
are	O
the	O
hidden	B
variables	I
(	O
assumed	O
discrete	O
)	O
and	O
v	O
are	O
the	O
visible	B
variables	I
.	O
this	O
algorithm	O
can	O
be	O
modiﬁed	O
to	O
compute	O
the	O
posterior	B
mode	I
and	O
posterior	O
samples	O
.	O
a	O
similar	B
algorithm	O
for	O
linear-gaussian	O
chains	O
,	O
known	O
as	O
the	O
kalman	O
smoother	O
,	O
was	O
discussed	O
in	O
section	O
18.3.2.	O
our	O
goal	O
in	O
this	O
chapter	O
is	O
to	O
generalize	B
these	O
exact	O
inference	B
algorithms	O
to	O
arbitrary	O
graphs	O
.	O
the	O
resulting	O
methods	O
apply	O
to	O
both	O
directed	B
and	O
undirected	O
graphical	O
models	O
.	O
we	O
will	O
describe	O
a	O
variety	O
of	O
algorithms	O
,	O
but	O
we	O
omit	O
their	O
derivations	O
for	O
brevity	O
.	O
see	O
e.g.	O
,	O
(	O
darwiche	O
2009	O
;	O
koller	O
and	O
friedman	O
2009	O
)	O
for	O
a	O
detailed	O
exposition	O
of	O
exact	O
inference	B
techniques	O
for	O
discrete	B
directed	O
graphical	O
models	O
.	O
20.2	O
belief	B
propagation	I
for	O
trees	O
in	O
this	O
section	O
,	O
we	O
generalize	B
the	O
forwards-backwards	B
algorithm	I
from	O
chains	O
to	O
trees	O
.	O
the	O
resulting	O
algorithm	O
is	O
known	O
as	O
belief	B
propagation	I
(	O
bp	O
)	O
(	O
pearl	O
1988	O
)	O
,	O
or	O
the	O
sum-product	B
algorithm	I
.	O
20.2.1	O
serial	O
protocol	O
(	O
cid:20	O
)	O
1	O
z	O
(	O
v	O
)	O
s∈v	O
(	O
cid:20	O
)	O
(	O
s	O
,	O
t	O
)	O
∈e	O
we	O
initially	O
assume	O
(	O
for	O
notational	O
simplicity	O
)	O
that	O
the	O
model	O
is	O
a	O
pairwise	O
mrf	O
(	O
or	O
crf	O
)	O
,	O
i.e.	O
,	O
p	O
(	O
x|v	O
)	O
=	O
ψs	O
(	O
xs	O
)	O
ψs	O
,	O
t	O
(	O
xs	O
,	O
xt	O
)	O
(	O
20.1	O
)	O
where	O
ψs	O
is	O
the	O
local	B
evidence	I
for	O
node	O
s	O
,	O
and	O
ψst	O
is	O
the	O
potential	O
for	O
edge	O
s	O
−	O
t.	O
we	O
will	O
consider	O
the	O
case	O
of	O
models	O
with	O
higher	O
order	O
cliques	B
(	O
such	O
as	O
directed	B
trees	O
)	O
later	O
on	O
.	O
one	O
way	O
to	O
implement	O
bp	O
for	O
undirected	B
trees	O
is	O
as	O
follows	O
.	O
pick	O
an	O
arbitrary	O
node	O
and	O
call	O
it	O
the	O
root	B
,	O
r.	O
now	O
orient	O
all	O
edges	O
away	O
from	O
r	O
(	O
intuitively	O
,	O
we	O
can	O
imagine	O
“	O
picking	O
up	O
the	O
graph	B
”	O
at	O
node	O
r	O
and	O
letting	O
all	O
the	O
edges	B
“	O
dangle	O
”	O
down	O
)	O
.	O
this	O
gives	O
us	O
a	O
well-deﬁned	O
notion	O
of	O
parent	O
and	O
child	O
.	O
now	O
we	O
send	O
messages	O
up	O
from	O
the	O
leaves	B
to	O
the	O
root	B
(	O
the	O
collect	B
evidence	I
phase	O
)	O
and	O
then	O
back	O
down	O
from	O
the	O
root	B
(	O
the	O
distribute	B
evidence	I
phase	O
)	O
,	O
in	O
a	O
manner	O
analogous	O
to	O
forwards-backwards	B
on	O
chains	O
.	O
708	O
chapter	O
20.	O
exact	O
inference	B
for	O
graphical	O
models	O
root	O
v−	O
st	O
t	O
v+	O
st	O
t	O
root	O
s	O
u	O
s	O
u	O
s1	O
s2	O
u1	O
u2	O
s1	O
s2	O
u1	O
u2	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
20.1	O
message	B
passing	I
on	O
a	O
tree	B
.	O
(	O
a	O
)	O
collect-to-root	B
phase	O
.	O
(	O
b	O
)	O
distribute-from-root	B
phase	O
.	O
to	O
explain	O
the	O
process	O
in	O
more	O
detail	O
,	O
consider	O
the	O
example	O
in	O
figure	O
20.1.	O
suppose	O
we	O
want	O
to	O
compute	O
the	O
belief	B
state	I
at	O
node	O
t.	O
we	O
will	O
initially	O
condition	O
the	O
belief	O
only	O
on	O
evidence	B
that	O
is	O
at	O
or	O
below	O
t	O
in	O
the	O
graph	B
,	O
i.e.	O
,	O
we	O
want	O
to	O
compute	O
bel	O
t	O
)	O
.	O
we	O
will	O
call	O
this	O
a	O
“	O
bottom-up	O
belief	B
state	I
”	O
.	O
suppose	O
,	O
by	O
induction	B
,	O
that	O
we	O
have	O
computed	O
“	O
messages	O
”	O
from	O
t	O
’	O
s	O
two	O
children	B
,	O
summarizing	O
what	O
they	O
think	O
t	O
should	O
know	O
about	O
the	O
evidence	B
in	O
their	O
subtrees	O
,	O
i.e.	O
,	O
we	O
have	O
computed	O
m−	O
st	O
is	O
all	O
the	O
evidence	B
on	O
the	O
downstream	O
side	O
of	O
the	O
s	O
−	O
t	O
edge	O
(	O
see	O
figure	O
20.1	O
(	O
a	O
)	O
)	O
,	O
and	O
similarly	O
we	O
have	O
computed	O
mu→t	O
(	O
xt	O
)	O
.	O
then	O
we	O
can	O
compute	O
the	O
bottom-up	O
belief	B
state	I
at	O
t	O
as	O
follows	O
:	O
s→t	O
(	O
xt	O
)	O
=p	O
(	O
xt|v−	O
st	O
)	O
,	O
where	O
v−	O
(	O
cid:20	O
)	O
t	O
(	O
xt	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
xt|v−	O
−	O
t	O
(	O
xt	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
xt|v−	O
−	O
t	O
)	O
=	O
bel	O
1	O
zt	O
ψt	O
(	O
xt	O
)	O
c∈ch	O
(	O
t	O
)	O
m−	O
c→t	O
(	O
xt	O
)	O
(	O
20.2	O
)	O
where	O
ψt	O
(	O
xt	O
)	O
∝	O
p	O
(	O
xt|vt	O
)	O
is	O
the	O
local	B
evidence	I
for	O
node	O
t	O
,	O
and	O
zt	O
is	O
the	O
local	O
normalization	O
in	O
words	O
,	O
we	O
multiply	O
all	O
the	O
incoming	O
messages	O
from	O
our	O
children	B
,	O
as	O
well	O
as	O
the	O
constant	O
.	O
incoming	O
message	O
from	O
our	O
local	B
evidence	I
,	O
and	O
then	O
normalize	O
.	O
how	O
do	O
we	O
compute	O
the	O
messages	O
themselves	O
?	O
consider	O
computing	O
m−	O
of	O
t	O
’	O
s	O
children	B
.	O
assume	O
,	O
by	O
recursion	O
,	O
that	O
we	O
have	O
computed	O
bel	O
can	O
compute	O
the	O
message	O
as	O
follows	O
:	O
−	O
s	O
(	O
xs	O
)	O
we	O
have	O
explained	O
how	O
to	O
compute	O
the	O
bottom-up	O
belief	O
states	O
from	O
the	O
bottom-up	O
messages	O
.	O
s→t	O
(	O
xt	O
)	O
,	O
where	O
s	O
is	O
one	O
s	O
)	O
.	O
then	O
we	O
s	O
(	O
xs	O
)	O
=	O
p	O
(	O
xs|v−	O
−	O
ψst	O
(	O
xs	O
,	O
xt	O
)	O
bel	O
s→t	O
(	O
xt	O
)	O
=	O
(	O
cid:4	O
)	O
m−	O
(	O
20.3	O
)	O
xs	O
essentially	O
we	O
convert	O
beliefs	O
about	O
xs	O
into	O
beliefs	O
about	O
xt	O
by	O
using	O
the	O
edge	O
potential	O
ψst	O
.	O
we	O
continue	O
in	O
this	O
way	O
up	O
the	O
tree	B
until	O
we	O
reach	O
the	O
root	B
.	O
once	O
at	O
the	O
root	B
,	O
we	O
have	O
“	O
seen	O
”	O
all	O
the	O
evidence	B
in	O
the	O
tree	B
,	O
so	O
we	O
can	O
compute	O
our	O
local	O
belief	O
state	B
at	O
the	O
root	B
using	O
belr	O
(	O
xr	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
xr|v	O
)	O
=	O
p	O
(	O
xt|v−	O
r	O
)	O
∝	O
ψr	O
(	O
xr	O
)	O
m−	O
c→r	O
(	O
xr	O
)	O
(	O
20.4	O
)	O
(	O
cid:20	O
)	O
c∈ch	O
(	O
r	O
)	O
this	O
completes	O
the	O
end	O
of	O
the	O
upwards	O
pass	O
,	O
which	O
is	O
analogous	O
to	O
the	O
forwards	O
pass	O
in	O
an	O
hmm	O
.	O
as	O
a	O
“	O
side	O
effect	O
”	O
,	O
we	O
can	O
compute	O
the	O
probability	B
of	I
the	I
evidence	I
by	O
collecting	O
the	O
20.2.	O
belief	B
propagation	I
for	O
trees	O
normalization	O
constants	O
:	O
p	O
(	O
v	O
)	O
=	O
zt	O
(	O
cid:20	O
)	O
t	O
709	O
(	O
20.5	O
)	O
we	O
can	O
now	O
pass	O
messages	O
down	O
from	O
the	O
root	B
.	O
for	O
example	O
,	O
consider	O
node	O
s	O
,	O
with	O
parent	O
t	O
,	O
as	O
shown	O
in	O
figure	O
20.1	O
(	O
b	O
)	O
.	O
to	O
compute	O
the	O
belief	B
state	I
for	O
s	O
,	O
we	O
need	O
to	O
combine	O
the	O
bottom-up	O
belief	O
for	O
s	O
together	O
with	O
a	O
top-down	O
message	O
from	O
t	O
,	O
which	O
summarizes	O
all	O
the	O
information	B
in	O
the	O
rest	O
of	O
the	O
graph	B
,	O
m+	O
st	O
is	O
all	O
the	O
evidence	B
on	O
the	O
upstream	O
(	O
root	B
)	O
side	O
of	O
the	O
s	O
−	O
t	O
edge	O
,	O
as	O
shown	O
in	O
figure	O
20.1	O
(	O
b	O
)	O
.	O
we	O
then	O
have	O
st	O
)	O
,	O
where	O
v+	O
t→s	O
(	O
xs	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
xt|v+	O
(	O
cid:20	O
)	O
bels	O
(	O
xs	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
xs|v	O
)	O
∝	O
bel	O
−	O
s	O
(	O
xs	O
)	O
m+	O
t→s	O
(	O
xt	O
)	O
t∈pa	O
(	O
s	O
)	O
how	O
do	O
we	O
compute	O
these	O
downward	O
messages	O
?	O
for	O
example	O
,	O
consider	O
the	O
message	O
from	O
t	O
to	O
s.	O
suppose	O
t	O
’	O
s	O
parent	O
is	O
r	O
,	O
and	O
t	O
’	O
s	O
children	B
are	O
s	O
and	O
u	O
,	O
as	O
shown	O
in	O
figure	O
20.1	O
(	O
b	O
)	O
.	O
we	O
want	O
to	O
include	O
in	O
m+	O
t→s	O
all	O
the	O
information	B
that	O
t	O
has	O
received	O
,	O
except	O
for	O
the	O
information	B
that	O
s	O
sent	O
it	O
:	O
(	O
20.6	O
)	O
(	O
20.7	O
)	O
(	O
cid:4	O
)	O
xt	O
(	O
cid:4	O
)	O
xt	O
(	O
cid:4	O
)	O
t→s	O
(	O
xs	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
xs|v+	O
m+	O
st	O
)	O
=	O
ψst	O
(	O
xs	O
,	O
xt	O
)	O
belt	O
(	O
xt	O
)	O
m−	O
s→t	O
(	O
xt	O
)	O
rather	O
than	O
dividing	O
out	O
the	O
message	O
sent	O
up	O
to	O
t	O
,	O
we	O
can	O
plug	O
in	O
the	O
equation	O
of	O
belt	O
to	O
get	O
m+	O
t→s	O
(	O
xs	O
)	O
=	O
ψst	O
(	O
xs	O
,	O
xt	O
)	O
ψt	O
(	O
xt	O
)	O
m+	O
p→t	O
(	O
xt	O
)	O
(	O
20.8	O
)	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
m−	O
c→t	O
(	O
xt	O
)	O
c∈ch	O
(	O
t	O
)	O
,	O
c	O
(	O
cid:5	O
)	O
=s	O
p∈pa	O
(	O
t	O
)	O
in	O
other	O
words	O
,	O
we	O
multiply	O
together	O
all	O
the	O
messages	O
coming	O
into	O
t	O
from	O
all	O
nodes	O
except	O
for	O
the	O
recipient	O
s	O
,	O
combine	O
together	O
,	O
and	O
then	O
pass	O
through	O
the	O
edge	O
potential	O
ψst	O
.	O
in	O
the	O
case	O
of	O
a	O
chain	O
,	O
t	O
only	O
has	O
one	O
child	O
s	O
and	O
one	O
parent	O
p	O
,	O
so	O
the	O
above	O
simpliﬁes	O
to	O
m+	O
t→s	O
(	O
xs	O
)	O
=	O
ψst	O
(	O
xs	O
,	O
xt	O
)	O
ψt	O
(	O
xt	O
)	O
m+	O
p→t	O
(	O
xt	O
)	O
(	O
20.9	O
)	O
xt	O
the	O
version	O
of	O
bp	O
in	O
which	O
we	O
use	O
division	O
is	O
called	O
belief	B
updating	I
,	O
and	O
the	O
version	O
in	O
which	O
we	O
multiply	O
all-but-one	O
of	O
the	O
messages	O
is	O
called	O
sum-product	B
.	O
the	O
belief	B
updating	I
version	O
is	O
analogous	O
to	O
how	O
we	O
formulated	O
the	O
kalman	O
smoother	O
in	O
section	O
18.3.2	O
:	O
the	O
top-	O
down	O
messages	O
depend	O
on	O
the	O
bottom-up	O
messages	O
.	O
this	O
means	O
they	O
can	O
be	O
interpreted	O
as	O
conditional	O
posterior	O
probabilities	O
.	O
the	O
sum-product	B
version	O
is	O
analogous	O
to	O
how	O
we	O
formulated	O
the	O
backwards	O
algorithm	O
in	O
section	O
17.4.3	O
:	O
the	O
top-down	O
messages	O
are	O
completely	O
independent	O
of	O
the	O
bottom-up	O
messages	O
,	O
which	O
means	O
they	O
can	O
only	O
be	O
interpreted	O
as	O
conditional	O
likelihoods	O
.	O
see	O
section	O
18.3.2.3	O
for	O
a	O
more	O
detailed	O
discussion	O
of	O
this	O
subtle	O
difference	O
.	O
20.2.2	O
parallel	O
protocol	O
so	O
far	O
,	O
we	O
have	O
presented	O
a	O
serial	O
version	O
of	O
the	O
algorithm	O
,	O
in	O
which	O
we	O
send	O
messages	O
up	O
to	O
the	O
root	B
and	O
back	O
.	O
this	O
is	O
the	O
optimal	O
approach	O
for	O
a	O
tree	B
,	O
and	O
is	O
a	O
natural	O
extension	O
of	O
forwards-backwards	B
on	O
chains	O
.	O
however	O
,	O
as	O
a	O
prelude	O
to	O
handling	O
general	O
graphs	O
with	O
loops	O
,	O
we	O
now	O
consider	O
a	O
parallel	O
version	O
of	O
bp	O
.	O
this	O
gives	O
equivalent	O
results	O
to	O
the	O
serial	O
version	O
but	O
is	O
less	O
efficient	O
when	O
implemented	O
on	O
a	O
serial	O
machine	O
.	O
710	O
chapter	O
20.	O
exact	O
inference	B
for	O
graphical	O
models	O
the	O
basic	O
idea	O
is	O
that	O
all	O
nodes	O
receive	O
messages	O
from	O
their	O
neighbors	B
in	O
parallel	O
,	O
they	O
then	O
updates	O
their	O
belief	O
states	O
,	O
and	O
ﬁnally	O
they	O
send	O
new	O
messages	O
back	O
out	O
to	O
their	O
neighbors	B
.	O
this	O
process	O
repeats	O
until	O
convergence	O
.	O
this	O
kind	O
of	O
computing	O
architecture	O
is	O
called	O
a	O
systolic	B
array	I
,	O
due	O
to	O
its	O
resemblance	O
to	O
a	O
beating	O
heart	O
.	O
more	O
precisely	O
,	O
we	O
initialize	O
all	O
messages	O
to	O
the	O
all	O
1	O
’	O
s	O
vector	O
.	O
then	O
,	O
in	O
parallel	O
,	O
each	O
node	O
absorbs	O
messages	O
from	O
all	O
its	O
neighbors	B
using	O
(	O
20.10	O
)	O
(	O
20.11	O
)	O
t∈nbrs	O
mt→s	O
(	O
xs	O
)	O
(	O
cid:20	O
)	O
⎛	O
⎝ψs	O
(	O
xs	O
)	O
ψst	O
(	O
xs	O
,	O
xt	O
)	O
(	O
cid:20	O
)	O
bels	O
(	O
xs	O
)	O
∝	O
ψs	O
(	O
xs	O
)	O
(	O
cid:4	O
)	O
ms→t	O
(	O
xt	O
)	O
=	O
xs	O
then	O
,	O
in	O
parallel	O
,	O
each	O
node	O
sends	O
messages	O
to	O
each	O
of	O
its	O
neighbors	B
:	O
⎞	O
⎠	O
mu→s	O
(	O
xs	O
)	O
u∈nbrs\t	O
the	O
ms→t	O
message	O
is	O
computed	O
by	O
multiplying	O
together	O
all	O
incoming	O
messages	O
,	O
except	O
the	O
one	O
sent	O
by	O
the	O
recipient	O
,	O
and	O
then	O
passing	O
through	O
the	O
ψst	O
potential	O
.	O
at	O
iteration	O
t	O
of	O
the	O
algorithm	O
,	O
bels	O
(	O
xs	O
)	O
represents	O
the	O
posterior	O
belief	O
of	O
xs	O
conditioned	O
on	O
the	O
evidence	B
that	O
is	O
t	O
steps	O
away	O
in	O
the	O
graph	B
.	O
after	O
d	O
(	O
g	O
)	O
steps	O
,	O
where	O
d	O
(	O
g	O
)	O
is	O
the	O
diameter	B
of	O
the	O
graph	B
(	O
the	O
largest	O
distance	O
between	O
any	O
two	O
pairs	O
of	O
nodes	B
)	O
,	O
every	O
node	O
has	O
obtained	O
information	B
from	O
all	O
the	O
other	O
nodes	B
.	O
its	O
local	O
belief	O
state	B
is	O
then	O
the	O
correct	O
posterior	O
marginal	O
.	O
since	O
the	O
diameter	B
of	O
a	O
tree	B
is	O
at	O
most	O
|v|	O
−	O
1	O
,	O
the	O
algorithm	O
converges	O
in	O
a	O
linear	O
number	O
of	O
steps	O
.	O
we	O
can	O
actually	O
derive	O
the	O
up-down	B
version	O
of	O
the	O
algorithm	O
by	O
imposing	O
the	O
condition	O
that	O
a	O
node	O
can	O
only	O
send	O
a	O
message	O
once	O
it	O
has	O
received	O
messages	O
from	O
all	O
its	O
other	O
neighbors	B
.	O
this	O
means	O
we	O
must	O
start	O
with	O
the	O
leaf	B
nodes	O
,	O
which	O
only	O
have	O
one	O
neighbor	O
.	O
the	O
messages	O
then	O
propagate	O
up	O
to	O
the	O
root	B
and	O
back	O
.	O
we	O
can	O
also	O
update	O
the	O
nodes	B
in	O
a	O
random	O
order	O
.	O
the	O
only	O
requirement	O
is	O
that	O
each	O
node	O
get	O
updated	O
d	O
(	O
g	O
)	O
times	O
.	O
this	O
is	O
just	O
enough	O
time	O
for	O
information	B
to	O
spread	O
throughout	O
the	O
whole	O
tree	B
.	O
similar	B
parallel	O
,	O
distributed	O
algorithms	O
for	O
solving	O
linear	O
systems	O
of	O
equations	O
are	O
discussed	O
in	O
(	O
bertsekas	O
1997	O
)	O
.	O
in	O
particular	O
,	O
the	O
gauss-seidel	O
algorithm	O
is	O
analogous	O
to	O
the	O
serial	O
up-down	B
version	O
of	O
bp	O
,	O
and	O
the	O
jacobi	O
algorithm	O
is	O
analogous	O
to	O
the	O
parallel	O
version	O
of	O
bp	O
.	O
20.2.3	O
gaussian	O
bp	O
*	O
now	O
consider	O
the	O
case	O
where	O
p	O
(	O
x|v	O
)	O
is	O
jointly	O
gaussian	O
,	O
so	O
it	O
can	O
be	O
represented	O
as	O
a	O
gaussian	O
pairwise	O
mrf	O
,	O
as	O
in	O
section	O
19.4.4.	O
we	O
now	O
present	O
the	O
belief	B
propagation	I
algorithm	O
for	O
this	O
class	O
of	O
models	O
,	O
follow	O
the	O
presentation	O
of	O
(	O
bickson	O
2009	O
)	O
(	O
see	O
also	O
(	O
malioutov	O
et	O
al	O
.	O
2006	O
)	O
)	O
.	O
we	O
will	O
assume	O
the	O
following	O
node	O
and	O
edge	O
potentials	O
:	O
ψt	O
(	O
xt	O
)	O
=	O
exp	O
(	O
−	O
1	O
2	O
ψst	O
(	O
xs	O
,	O
xt	O
)	O
=	O
exp	O
(	O
−	O
1	O
2	O
attx2	O
t	O
+	O
btxt	O
)	O
xsastxt	O
)	O
so	O
the	O
overall	O
model	O
has	O
the	O
form	O
p	O
(	O
x|v	O
)	O
∝	O
exp	O
(	O
−	O
1	O
2	O
xt	O
ax	O
+	O
bt	O
x	O
)	O
(	O
20.12	O
)	O
(	O
20.13	O
)	O
(	O
20.14	O
)	O
20.2.	O
belief	B
propagation	I
for	O
trees	O
711	O
this	O
is	O
the	O
information	B
form	I
of	O
the	O
mvn	O
(	O
see	O
exercise	O
9.2	O
)	O
,	O
where	O
a	O
is	O
the	O
precision	B
matrix	I
.	O
note	O
that	O
by	O
completing	B
the	I
square	I
,	O
the	O
local	B
evidence	I
can	O
be	O
rewritten	O
as	O
a	O
gaussian	O
:	O
tt	O
)	O
(	O
cid:2	O
)	O
n	O
(	O
mt	O
,	O
(	O
cid:2	O
)	O
−1	O
t	O
)	O
ψt	O
(	O
xt	O
)	O
∝	O
n	O
(	O
bt/att	O
,	O
a−1	O
below	O
we	O
describe	O
how	O
to	O
use	O
bp	O
to	O
compute	O
the	O
posterior	O
node	O
marginals	O
,	O
p	O
(	O
xt|v	O
)	O
=	O
n	O
(	O
μt	O
,	O
λ−1	O
t	O
)	O
(	O
20.15	O
)	O
(	O
20.16	O
)	O
if	O
the	O
graph	B
is	O
a	O
tree	B
,	O
the	O
method	O
is	O
exact	O
.	O
if	O
the	O
graph	B
is	O
loopy	O
,	O
the	O
posterior	O
means	O
may	O
still	O
be	O
exact	O
,	O
but	O
the	O
posterior	O
variances	O
are	O
often	O
too	O
small	O
(	O
weiss	O
and	O
freeman	O
1999	O
)	O
.	O
although	O
the	O
precision	B
matrix	I
a	O
is	O
often	O
sparse	B
,	O
computing	O
the	O
posterior	B
mean	I
requires	O
inverting	O
it	O
,	O
since	O
μ	O
=	O
a−1b	O
.	O
bp	O
provides	O
a	O
way	O
to	O
exploit	O
graph	B
structure	O
to	O
perform	O
this	O
computation	O
in	O
o	O
(	O
d	O
)	O
time	O
instead	O
of	O
o	O
(	O
d3	O
)	O
.	O
this	O
is	O
related	O
to	O
various	O
methods	O
from	O
linear	O
algebra	O
,	O
as	O
discussed	O
in	O
(	O
bickson	O
2009	O
)	O
.	O
since	O
the	O
model	O
is	O
jointly	O
gaussian	O
,	O
all	O
marginals	O
and	O
all	O
messages	O
will	O
be	O
gaussian	O
.	O
the	O
key	O
operations	O
we	O
need	O
are	O
to	O
multiply	O
together	O
two	O
gaussian	O
factors	B
,	O
and	O
to	O
marginalize	O
out	O
a	O
variable	O
from	O
a	O
joint	O
gaussian	O
factor	B
.	O
for	O
multiplication	O
,	O
we	O
can	O
use	O
the	O
fact	O
that	O
the	O
product	O
of	O
two	O
gaussians	O
is	O
gaussian	O
:	O
n	O
(	O
x|μ1	O
,	O
λ−1	O
1	O
)	O
×	O
n	O
(	O
x|μ2	O
,	O
λ−1	O
2	O
)	O
=c	O
n	O
(	O
x|μ	O
,	O
λ−1	O
)	O
λ	O
=	O
λ1	O
+	O
λ2	O
μ	O
=	O
λ−1	O
(	O
μ1λ1	O
+	O
μ2λ2	O
)	O
which	O
follows	O
from	O
the	O
normalization	O
constant	O
of	O
a	O
gaussian	O
(	O
exercise	O
2.11	O
)	O
.	O
we	O
now	O
have	O
all	O
the	O
pieces	O
we	O
need	O
.	O
in	O
particular	O
,	O
let	O
the	O
message	O
ms→t	O
(	O
xt	O
)	O
be	O
a	O
gaussian	O
with	O
mean	B
μst	O
and	O
precision	B
λst	O
.	O
from	O
equation	O
20.10	O
,	O
the	O
belief	O
at	O
node	O
s	O
is	O
given	O
by	O
the	O
product	O
of	O
incoming	O
messages	O
times	O
the	O
local	B
evidence	I
(	O
equation	O
20.15	O
)	O
and	O
hence	O
bels	O
(	O
xs	O
)	O
=ψ	O
s	O
(	O
xs	O
)	O
mts	O
(	O
xs	O
)	O
=	O
n	O
(	O
xs|μs	O
,	O
λ−1	O
s	O
)	O
t∈nbr	O
(	O
s	O
)	O
(	O
cid:20	O
)	O
(	O
cid:4	O
)	O
⎛	O
⎝	O
(	O
cid:2	O
)	O
sms	O
+	O
t∈nbr	O
(	O
s	O
)	O
λs	O
=	O
(	O
cid:2	O
)	O
s	O
+	O
λts	O
μs	O
=	O
λ−1	O
s	O
⎞	O
⎠	O
λtsμts	O
(	O
cid:4	O
)	O
t∈nbr	O
(	O
s	O
)	O
.	O
where	O
c	O
=	O
(	O
cid:8	O
)	O
1	O
2	O
λ	O
λ1λ2	O
exp	O
(	O
cid:12	O
)	O
see	O
exercise	O
20.2	O
for	O
the	O
proof	O
.	O
for	O
marginalization	O
,	O
we	O
have	O
the	O
following	O
result	O
:	O
exp	O
(	O
−ax2	O
+	O
bx	O
)	O
dx	O
=	O
π/a	O
exp	O
(	O
b2/4a	O
)	O
''	O
(	O
λ1μ2	O
1	O
(	O
λ−1λ1	O
−	O
1	O
)	O
+	O
λ2μ2	O
2	O
(	O
λ−1λ2	O
−	O
1	O
)	O
+	O
2λ−1λ1λ2μ1μ2	O
)	O
(	O
cid:9	O
)	O
(	O
20.17	O
)	O
(	O
20.18	O
)	O
(	O
20.19	O
)	O
(	O
20.20	O
)	O
(	O
20.21	O
)	O
(	O
20.22	O
)	O
(	O
20.23	O
)	O
(	O
20.24	O
)	O
712	O
chapter	O
20.	O
exact	O
inference	B
for	O
graphical	O
models	O
to	O
compute	O
the	O
messages	O
themselves	O
,	O
we	O
use	O
equation	O
20.11	O
,	O
which	O
is	O
given	O
by	O
⎛	O
⎝ψst	O
(	O
xs	O
,	O
xt	O
)	O
ψs	O
(	O
xs	O
)	O
(	O
cid:20	O
)	O
ψst	O
(	O
xs	O
,	O
xt	O
)	O
fs\t	O
(	O
xs	O
)	O
dxs	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
xs	O
xs	O
mu→s	O
(	O
xs	O
)	O
u∈nbrs\t	O
⎞	O
⎠	O
dxs	O
ms→t	O
(	O
xt	O
)	O
=	O
=	O
where	O
fs\t	O
(	O
xs	O
)	O
is	O
the	O
product	O
of	O
the	O
local	B
evidence	I
and	O
all	O
incoming	O
messages	O
excluding	O
the	O
message	O
from	O
t	O
:	O
fs\t	O
(	O
xs	O
)	O
(	O
cid:2	O
)	O
ψs	O
(	O
xs	O
)	O
mu→s	O
(	O
xs	O
)	O
u∈nbrs\t	O
=	O
n	O
(	O
xs|μs\t	O
,	O
λ−1	O
s\t	O
)	O
λs\t	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
s	O
+	O
(	O
cid:20	O
)	O
(	O
cid:4	O
)	O
⎛	O
⎝	O
(	O
cid:2	O
)	O
sms	O
+	O
u∈nbr	O
(	O
s	O
)	O
\t	O
⎞	O
⎠	O
λusμus	O
λus	O
(	O
cid:4	O
)	O
u∈nbr	O
(	O
s	O
)	O
\t	O
returning	O
to	O
equation	O
20.26	O
we	O
have	O
μs\t	O
(	O
cid:2	O
)	O
λ−1	O
s\t	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
xs	O
ms→t	O
(	O
xt	O
)	O
=	O
xs	O
=	O
∝	O
exp	O
∝	O
n	O
(	O
μst	O
,	O
λ−1	O
st	O
)	O
λst	O
=	O
a2	O
μst	O
=	O
astμs\t/λst	O
st/λs\t	O
(	O
20.25	O
)	O
(	O
20.26	O
)	O
(	O
20.27	O
)	O
(	O
20.28	O
)	O
(	O
20.29	O
)	O
(	O
20.30	O
)	O
(	O
20.31	O
)	O
(	O
20.32	O
)	O
(	O
20.33	O
)	O
(	O
20.34	O
)	O
(	O
20.35	O
)	O
(	O
20.36	O
)	O
ψst	O
(	O
xs	O
,	O
xt	O
)	O
01	O
(	O
−λs\tx2	O
/	O
2	O
/	O
2	O
exp	O
(	O
−xsastxt	O
)	O
exp	O
(	O
−λs\t/2	O
(	O
xs	O
−	O
μs\t	O
)	O
2	O
)	O
(	O
cid:23	O
)	O
(	O
cid:22	O
)	O
s/2	O
)	O
+	O
(	O
λs\tμs\t	O
−	O
astxt	O
)	O
xs	O
(	O
cid:22	O
)	O
exp	O
(	O
λs\tμs\t	O
−	O
astxt	O
)	O
2/	O
(	O
2λs\t	O
)	O
01	O
(	O
cid:23	O
)	O
fs\t	O
(	O
xs	O
)	O
dxs	O
dxs	O
+	O
const	O
one	O
can	O
generalize	B
these	O
equations	O
to	O
the	O
case	O
where	O
each	O
node	O
is	O
a	O
vector	O
,	O
and	O
the	O
messages	O
become	O
small	O
mvns	O
instead	O
of	O
scalar	O
gaussians	O
(	O
alag	O
and	O
agogino	O
1996	O
)	O
.	O
if	O
we	O
apply	O
the	O
resulting	O
algorithm	O
to	O
a	O
linear	B
dynamical	I
system	I
,	O
we	O
recover	O
the	O
kalman	O
smoothing	O
algorithm	O
of	O
section	O
18.3.2.	O
to	O
perform	O
message	B
passing	I
in	O
models	O
with	O
non-gaussian	O
potentials	O
,	O
one	O
can	O
use	O
sampling	O
methods	O
to	O
approximate	O
the	O
relevant	O
integrals	O
.	O
this	O
is	O
called	O
non-parametric	O
bp	O
(	O
sudderth	O
et	O
al	O
.	O
2003	O
;	O
isard	O
2003	O
;	O
sudderth	O
et	O
al	O
.	O
2010	O
)	O
.	O
20.2.4	O
other	O
bp	O
variants	O
*	O
in	O
this	O
section	O
,	O
we	O
brieﬂy	O
discuss	O
several	O
variants	O
of	O
the	O
main	O
algorithm	O
.	O
20.2.	O
belief	B
propagation	I
for	O
trees	O
713	O
ft	O
ψt	O
,	O
t+1	O
ψt	O
xt	O
vt	O
βt+1	O
ψt+1	O
xt+1	O
vt+1	O
figure	O
20.2	O
illustration	O
of	O
how	O
to	O
compute	O
the	O
two-slice	O
distribution	O
for	O
an	O
hmm	O
.	O
the	O
ψt	O
and	O
ψt+1	O
terms	O
are	O
the	O
local	B
evidence	I
messages	O
from	O
the	O
visible	B
nodes	I
vt	O
,	O
vt+1	O
to	O
the	O
hidde	O
nodes	B
xt	O
,	O
xt+1	O
respectively	O
;	O
ft	O
is	O
the	O
forwards	O
message	O
from	O
xt−1	O
and	O
βt+1	O
is	O
the	O
backwards	O
message	O
from	O
xt+2	O
.	O
20.2.4.1	O
max-product	B
algorithm	O
it	O
is	O
possible	O
to	O
devise	O
a	O
max-product	B
version	O
of	O
the	O
bp	O
algorithm	O
,	O
by	O
replacing	O
the	O
operator	O
with	O
the	O
max	O
operator	O
.	O
we	O
can	O
then	O
compute	O
the	O
local	O
map	O
marginal	O
of	O
each	O
node	O
.	O
however	O
,	O
if	O
there	O
are	O
ties	O
,	O
this	O
might	O
not	O
be	O
globally	O
consistent	O
,	O
as	O
discussed	O
in	O
section	O
17.4.4.	O
fortunately	O
,	O
we	O
can	O
generalize	B
the	O
viterbi	O
algorithm	O
to	O
trees	O
,	O
where	O
we	O
use	O
max	O
and	O
argmax	O
in	O
the	O
collect-	O
to-root	O
phase	B
,	O
and	O
perform	O
traceback	B
in	O
the	O
distribute-from-root	B
phase	O
.	O
see	O
(	O
dawid	O
1992	O
)	O
for	O
details	O
.	O
(	O
cid:7	O
)	O
20.2.4.2	O
sampling	O
from	O
a	O
tree	B
it	O
is	O
possible	O
to	O
draw	O
samples	B
from	O
a	O
tree	B
structured	O
model	O
by	O
generalizing	O
the	O
forwards	O
ﬁltering	O
/	O
backwards	O
sampling	O
algorithm	O
discussed	O
in	O
section	O
17.4.5.	O
see	O
(	O
dawid	O
1992	O
)	O
for	O
details	O
.	O
20.2.4.3	O
computing	O
posteriors	O
on	O
sets	O
of	O
variables	O
in	O
section	O
17.4.3.2	O
,	O
we	O
explained	O
how	O
to	O
compute	O
the	O
“	O
two-slice	O
”	O
distribution	O
ξt	O
,	O
t+1	O
(	O
i	O
,	O
j	O
)	O
=	O
p	O
(	O
xt	O
=	O
i	O
,	O
xt+1	O
=	O
j|v	O
)	O
in	O
an	O
hmm	O
,	O
namely	O
by	O
using	O
ξt	O
,	O
t+1	O
(	O
i	O
,	O
j	O
)	O
=α	O
t	O
(	O
i	O
)	O
ψt+1	O
(	O
j	O
)	O
βt+1	O
(	O
j	O
)	O
ψt	O
,	O
t+1	O
(	O
i	O
,	O
j	O
)	O
(	O
20.37	O
)	O
since	O
αt	O
(	O
i	O
)	O
∝	O
ψt	O
(	O
i	O
)	O
ft	O
(	O
i	O
)	O
,	O
where	O
ft	O
=	O
p	O
(	O
xt|v1	O
:	O
t−1	O
)	O
is	O
the	O
forwards	O
message	O
,	O
we	O
can	O
think	O
of	O
this	O
as	O
sending	O
messages	O
ft	O
and	O
ψt	O
into	O
xt	O
,	O
βt+1	O
and	O
φt+1	O
into	O
xt+1	O
,	O
and	O
then	O
combining	O
them	O
with	O
the	O
ψ	O
matrix	O
,	O
as	O
shown	O
in	O
figure	O
20.2.	O
this	O
is	O
like	O
treating	O
xt	O
and	O
xt+1	O
as	O
a	O
single	O
“	O
mega	O
node	O
”	O
,	O
and	O
then	O
multiplying	O
all	O
the	O
incoming	O
messages	O
as	O
well	O
as	O
all	O
the	O
local	O
factors	O
(	O
here	O
,	O
ψt	O
,	O
t+1	O
)	O
.	O
714	O
chapter	O
20.	O
exact	O
inference	B
for	O
graphical	O
models	O
coherence	O
coherence	O
diﬃculty	O
intelligence	O
diﬃculty	O
intelligence	O
grade	O
sat	O
grade	O
sat	O
letter	O
job	O
letter	O
job	O
happy	O
(	O
a	O
)	O
happy	O
(	O
b	O
)	O
figure	O
20.3	O
left	O
:	O
the	O
“	O
student	O
”	O
dgm	O
.	O
right	O
:	O
the	O
equivalent	O
ugm	O
.	O
we	O
add	O
moralization	B
arcs	O
d-i	O
,	O
g-j	O
and	O
l-s.	O
based	O
on	O
figure	O
9.8	O
of	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
20.3	O
the	O
variable	B
elimination	I
algorithm	O
we	O
have	O
seen	O
how	O
to	O
use	O
bp	O
to	O
compute	O
exact	O
marginals	O
on	O
chains	O
and	O
trees	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
an	O
algorithm	O
to	O
compute	O
p	O
(	O
xq|xv	O
)	O
for	O
any	O
kind	O
of	O
graph	B
.	O
we	O
will	O
explain	O
the	O
algorithm	O
by	O
example	O
.	O
consider	O
the	O
dgm	O
in	O
figure	O
20.3	O
(	O
a	O
)	O
.	O
this	O
model	O
,	O
from	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
,	O
is	O
a	O
hypothetical	O
model	O
relating	O
various	O
variables	O
pertaining	O
to	O
a	O
typical	O
student	O
.	O
the	O
corresponding	O
joint	O
has	O
the	O
following	O
form	O
:	O
p	O
(	O
c	O
,	O
d	O
,	O
i	O
,	O
g	O
,	O
s	O
,	O
l	O
,	O
j	O
,	O
h	O
)	O
=	O
p	O
(	O
c	O
)	O
p	O
(	O
d|c	O
)	O
p	O
(	O
i	O
)	O
p	O
(	O
g|i	O
,	O
d	O
)	O
p	O
(	O
s|i	O
)	O
p	O
(	O
l|g	O
)	O
p	O
(	O
j|l	O
,	O
s	O
)	O
p	O
(	O
h|g	O
,	O
j	O
)	O
(	O
20.38	O
)	O
(	O
20.39	O
)	O
note	O
that	O
the	O
forms	O
of	O
the	O
cpds	O
do	O
not	O
matter	O
,	O
since	O
all	O
our	O
calculations	O
will	O
be	O
symbolic	O
.	O
however	O
,	O
for	O
illustration	O
purposes	O
,	O
we	O
will	O
assume	O
all	O
variables	O
are	O
binary	O
.	O
before	O
proceeding	O
,	O
we	O
convert	O
our	O
model	O
to	O
undirected	B
form	O
.	O
this	O
is	O
not	O
required	O
,	O
but	O
it	O
makes	O
for	O
a	O
more	O
uniﬁed	O
presentation	O
,	O
since	O
the	O
resulting	O
method	O
can	O
then	O
be	O
applied	O
to	O
both	O
dgms	O
and	O
ugms	O
(	O
and	O
,	O
as	O
we	O
will	O
see	O
in	O
section	O
20.3.1	O
,	O
to	O
a	O
variety	O
of	O
other	O
problems	O
that	O
have	O
nothing	O
to	O
do	O
with	O
graphical	O
models	O
)	O
.	O
since	O
the	O
computational	O
complexity	O
of	O
inference	B
in	O
dgms	O
and	O
ugms	O
is	O
,	O
generally	O
speaking	O
,	O
the	O
same	O
,	O
nothing	O
is	O
lost	O
in	O
this	O
transformation	O
from	O
a	O
computational	O
point	O
of	O
view.1	O
to	O
convert	O
the	O
dgm	O
to	O
a	O
ugm	O
,	O
we	O
simply	O
deﬁne	O
a	O
potential	O
or	O
factor	B
for	O
every	O
cpd	O
,	O
yielding	O
p	O
(	O
c	O
,	O
d	O
,	O
i	O
,	O
g	O
,	O
s	O
,	O
l	O
,	O
j	O
,	O
h	O
)	O
(	O
20.40	O
)	O
=	O
ψc	O
(	O
c	O
)	O
ψd	O
(	O
d	O
,	O
c	O
)	O
ψi	O
(	O
i	O
)	O
ψg	O
(	O
g	O
,	O
i	O
,	O
d	O
)	O
ψs	O
(	O
s	O
,	O
i	O
)	O
ψl	O
(	O
l	O
,	O
g	O
)	O
ψj	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
ψh	O
(	O
h	O
,	O
g	O
,	O
j	O
)	O
(	O
20.41	O
)	O
1.	O
there	O
are	O
a	O
few	O
“	O
tricks	O
”	O
one	O
can	O
exploit	O
in	O
the	O
directed	B
case	O
that	O
can	O
not	O
easily	O
be	O
exploited	O
in	O
the	O
undirected	B
case	O
.	O
one	O
important	O
example	O
is	O
barren	B
node	I
removal	I
.	O
to	O
explain	O
this	O
,	O
consider	O
a	O
naive	O
bayes	O
classiﬁer	O
,	O
as	O
in	O
figure	O
10.2.	O
suppose	O
we	O
want	O
to	O
infer	O
y	O
and	O
we	O
observe	O
x1	O
and	O
x2	O
,	O
but	O
not	O
x3	O
and	O
x4	O
.	O
it	O
is	O
clear	O
that	O
we	O
can	O
safely	O
remove	O
x3	O
and	O
x4	O
,	O
since	O
in	O
general	O
,	O
once	O
we	O
have	O
removed	O
hidden	B
leaves	O
,	O
we	O
can	O
apply	O
this	O
process	O
recursively	O
.	O
since	O
potential	O
functions	O
do	O
not	O
necessary	O
sum	O
to	O
one	O
,	O
we	O
can	O
not	O
use	O
this	O
trick	O
in	O
the	O
undirected	B
case	O
.	O
see	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
for	O
a	O
variety	O
of	O
other	O
speedup	O
tricks	O
.	O
p	O
(	O
x3|y	O
)	O
=	O
1	O
,	O
and	O
similarly	O
for	O
x4	O
.	O
(	O
cid:2	O
)	O
x3	O
20.3.	O
the	O
variable	B
elimination	I
algorithm	O
715	O
since	O
all	O
the	O
potentials	O
are	O
locally	B
normalized	I
,	O
since	O
they	O
are	O
cpds	O
,	O
there	O
is	O
no	O
need	O
for	O
a	O
global	O
normalization	O
constant	O
,	O
so	O
z	O
=	O
1.	O
the	O
corresponding	O
undirected	B
graph	O
is	O
shown	O
in	O
figure	O
20.3	O
(	O
b	O
)	O
.	O
note	O
that	O
it	O
has	O
more	O
edges	B
than	O
the	O
dag	O
.	O
in	O
particular	O
,	O
any	O
“	O
unmarried	O
”	O
nodes	O
that	O
share	O
a	O
child	O
must	O
get	O
“	O
married	O
”	O
,	O
by	O
adding	O
an	O
edge	O
between	O
them	O
;	O
this	O
process	O
is	O
known	O
as	O
moralization	B
.	O
only	O
then	O
can	O
the	O
arrows	O
be	O
dropped	O
.	O
in	O
this	O
example	O
,	O
we	O
added	O
d-i	O
,	O
g-j	O
,	O
and	O
l-s	O
moralization	B
arcs	O
.	O
the	O
reason	O
this	O
operation	O
is	O
required	O
is	O
to	O
ensure	O
that	O
the	O
ci	O
properties	O
of	O
the	O
ugm	O
match	O
those	O
of	O
the	O
dgm	O
,	O
as	O
explained	O
in	O
section	O
19.2.2.	O
it	O
also	O
ensures	O
there	O
is	O
a	O
clique	B
that	O
can	O
“	O
store	O
”	O
the	O
cpds	O
of	O
each	O
family	B
.	O
now	O
suppose	O
we	O
want	O
to	O
compute	O
p	O
(	O
j	O
=	O
1	O
)	O
,	O
the	O
marginal	O
probability	O
that	O
a	O
person	O
will	O
get	O
a	O
job	O
.	O
since	O
we	O
have	O
8	O
binary	O
variables	O
,	O
we	O
could	O
simply	O
enumerate	O
over	O
all	O
possible	O
assignments	O
to	O
all	O
the	O
variables	O
(	O
except	O
for	O
j	O
)	O
,	O
adding	O
up	O
the	O
probability	O
of	O
each	O
joint	O
instantiation	O
:	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
j	O
)	O
=	O
p	O
(	O
c	O
,	O
d	O
,	O
i	O
,	O
g	O
,	O
s	O
,	O
l	O
,	O
j	O
,	O
h	O
)	O
(	O
20.42	O
)	O
l	O
s	O
g	O
h	O
i	O
d	O
c	O
however	O
,	O
this	O
would	O
take	O
o	O
(	O
27	O
)	O
time	O
.	O
we	O
can	O
be	O
smarter	O
by	O
pushing	B
sums	I
inside	I
products	I
.	O
this	O
is	O
the	O
key	O
idea	O
behind	O
the	O
variable	B
elimination	I
algorithm	O
(	O
zhang	O
and	O
poole	O
1996	O
)	O
,	O
also	O
called	O
bucket	B
elimination	I
(	O
dechter	O
1996	O
)	O
,	O
or	O
,	O
in	O
the	O
context	O
of	O
genetic	O
pedigree	O
trees	O
,	O
the	O
peeling	B
algorithm	I
(	O
cannings	O
et	O
al	O
.	O
1978	O
)	O
.	O
in	O
our	O
example	O
,	O
we	O
get	O
(	O
cid:7	O
)	O
we	O
now	O
evaluate	O
this	O
expression	O
,	O
working	O
right	O
to	O
left	O
as	O
shown	O
in	O
table	O
20.1.	O
first	O
we	O
multiply	O
together	O
all	O
the	O
terms	O
in	O
the	O
scope	B
of	O
the	O
c	O
operator	O
to	O
create	O
the	O
temporary	O
factor	B
then	O
we	O
marginalize	O
out	O
c	O
to	O
get	O
the	O
new	O
factor	B
τ	O
(	O
cid:2	O
)	O
1	O
(	O
c	O
,	O
d	O
)	O
=	O
ψc	O
(	O
c	O
)	O
ψd	O
(	O
d	O
,	O
c	O
)	O
(	O
cid:4	O
)	O
c	O
τ1	O
(	O
d	O
)	O
=	O
τ	O
(	O
cid:2	O
)	O
1	O
(	O
c	O
,	O
d	O
)	O
next	O
we	O
multiply	O
together	O
all	O
the	O
terms	O
in	O
the	O
scope	B
of	O
the	O
out	O
to	O
create	O
τ	O
(	O
cid:2	O
)	O
2	O
(	O
g	O
,	O
i	O
,	O
d	O
)	O
=ψ	O
g	O
(	O
g	O
,	O
i	O
,	O
d	O
)	O
τ1	O
(	O
d	O
)	O
τ2	O
(	O
g	O
,	O
i	O
)	O
=	O
τ	O
(	O
cid:2	O
)	O
2	O
(	O
g	O
,	O
i	O
,	O
d	O
)	O
(	O
cid:4	O
)	O
d	O
(	O
cid:7	O
)	O
(	O
20.43	O
)	O
(	O
20.44	O
)	O
d	O
operator	O
and	O
then	O
marginalize	O
(	O
20.45	O
)	O
(	O
20.46	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
j	O
)	O
=	O
=	O
=	O
p	O
(	O
c	O
,	O
d	O
,	O
i	O
,	O
g	O
,	O
s	O
,	O
l	O
,	O
j	O
,	O
h	O
)	O
l	O
,	O
s	O
,	O
g	O
,	O
h	O
,	O
i	O
,	O
d	O
,	O
c	O
ψc	O
(	O
c	O
)	O
ψd	O
(	O
d	O
,	O
c	O
)	O
ψi	O
(	O
i	O
)	O
ψg	O
(	O
g	O
,	O
i	O
,	O
d	O
)	O
ψs	O
(	O
s	O
,	O
i	O
)	O
ψl	O
(	O
l	O
,	O
g	O
)	O
(	O
cid:4	O
)	O
l	O
,	O
s	O
,	O
g	O
,	O
h	O
,	O
i	O
,	O
d	O
,	O
c	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
×ψj	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
ψh	O
(	O
h	O
,	O
g	O
,	O
j	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
ψj	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
l	O
,	O
s	O
g	O
×	O
d	O
c	O
ψg	O
(	O
g	O
,	O
i	O
,	O
d	O
)	O
ψc	O
(	O
c	O
)	O
ψd	O
(	O
d	O
,	O
c	O
)	O
(	O
cid:4	O
)	O
ψl	O
(	O
l	O
,	O
g	O
)	O
ψh	O
(	O
h	O
,	O
g	O
,	O
j	O
)	O
ψs	O
(	O
s	O
,	O
i	O
)	O
ψi	O
(	O
i	O
)	O
h	O
i	O
chapter	O
20.	O
exact	O
inference	B
for	O
graphical	O
models	O
716	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
l	O
s	O
ψj	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
(	O
cid:7	O
)	O
g	O
ψl	O
(	O
l	O
,	O
g	O
)	O
(	O
cid:7	O
)	O
h	O
(	O
cid:7	O
)	O
ψh	O
(	O
h	O
,	O
g	O
,	O
j	O
)	O
ψs	O
(	O
s	O
,	O
i	O
)	O
ψi	O
(	O
i	O
)	O
(	O
cid:7	O
)	O
d	O
ψg	O
(	O
g	O
,	O
i	O
,	O
d	O
)	O
i	O
(	O
cid:7	O
)	O
ψh	O
(	O
h	O
,	O
g	O
,	O
j	O
)	O
(	O
cid:7	O
)	O
i	O
ψs	O
(	O
s	O
,	O
i	O
)	O
ψi	O
(	O
i	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
l	O
s	O
ψj	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
(	O
cid:7	O
)	O
g	O
ψl	O
(	O
l	O
,	O
g	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
l	O
s	O
ψj	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
ψl	O
(	O
l	O
,	O
g	O
)	O
(	O
cid:7	O
)	O
h	O
ψh	O
(	O
h	O
,	O
g	O
,	O
j	O
)	O
h	O
(	O
cid:7	O
)	O
g	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
l	O
s	O
ψj	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
(	O
cid:7	O
)	O
g	O
ψl	O
(	O
l	O
,	O
g	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
l	O
s	O
ψj	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
c	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
d	O
ψc	O
(	O
c	O
)	O
ψd	O
(	O
d	O
,	O
c	O
)	O
(	O
cid:11	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
τ1	O
(	O
d	O
)	O
ψg	O
(	O
g	O
,	O
i	O
,	O
d	O
)	O
τ1	O
(	O
d	O
)	O
(	O
cid:11	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
τ2	O
(	O
g	O
,	O
i	O
)	O
ψs	O
(	O
s	O
,	O
i	O
)	O
ψi	O
(	O
i	O
)	O
τ2	O
(	O
g	O
,	O
i	O
)	O
(	O
cid:11	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
τ3	O
(	O
g	O
,	O
s	O
)	O
ψh	O
(	O
h	O
,	O
g	O
,	O
j	O
)	O
(	O
cid:11	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
τ4	O
(	O
g	O
,	O
j	O
)	O
τ3	O
(	O
g	O
,	O
s	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
i	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
h	O
ψl	O
(	O
l	O
,	O
g	O
)	O
τ4	O
(	O
g	O
,	O
j	O
)	O
τ3	O
(	O
g	O
,	O
s	O
)	O
(	O
cid:11	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
τ5	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
ψj	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
τ5	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
(	O
cid:11	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
τ6	O
(	O
j	O
,	O
l	O
)	O
(	O
cid:7	O
)	O
τ6	O
(	O
j	O
,	O
l	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
(	O
cid:8	O
)	O
l	O
τ7	O
(	O
j	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
g	O
l	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
s	O
table	O
20.1	O
eliminating	O
variables	O
from	O
figure	O
20.3	O
in	O
the	O
order	O
c	O
,	O
d	O
,	O
i	O
,	O
h	O
,	O
g	O
,	O
s	O
,	O
l	O
to	O
compute	O
p	O
(	O
j	O
)	O
.	O
(	O
cid:7	O
)	O
next	O
we	O
multiply	O
together	O
all	O
the	O
terms	O
in	O
the	O
scope	B
of	O
the	O
out	O
to	O
create	O
τ	O
(	O
cid:2	O
)	O
3	O
(	O
g	O
,	O
i	O
,	O
s	O
)	O
=ψ	O
s	O
(	O
s	O
,	O
i	O
)	O
ψi	O
(	O
i	O
)	O
τ2	O
(	O
g	O
,	O
i	O
)	O
τ3	O
(	O
g	O
,	O
s	O
)	O
=	O
τ	O
(	O
cid:2	O
)	O
3	O
(	O
g	O
,	O
i	O
,	O
s	O
)	O
(	O
cid:4	O
)	O
i	O
i	O
operator	O
and	O
then	O
marginalize	O
(	O
20.47	O
)	O
(	O
20.48	O
)	O
and	O
so	O
on	O
.	O
the	O
above	O
technique	O
can	O
be	O
used	O
to	O
compute	O
any	O
marginal	O
of	O
interest	O
,	O
such	O
as	O
p	O
(	O
j	O
)	O
or	O
p	O
(	O
j	O
,	O
h	O
)	O
.	O
to	O
compute	O
a	O
conditional	O
,	O
we	O
can	O
take	O
a	O
ratio	O
of	O
two	O
marginals	O
,	O
where	O
the	O
visible	B
variables	I
have	O
been	O
clamped	O
to	O
their	O
known	O
values	O
(	O
and	O
hence	O
don	O
’	O
t	O
need	O
to	O
be	O
summed	O
over	O
)	O
.	O
for	O
example	O
,	O
p	O
(	O
j	O
=	O
j|i	O
=	O
1	O
,	O
h	O
=	O
0	O
)	O
=	O
in	O
general	O
,	O
we	O
can	O
write	O
p	O
(	O
xq|xv	O
)	O
=	O
p	O
(	O
xq	O
,	O
xv	O
)	O
p	O
(	O
xv	O
)	O
(	O
cid:7	O
)	O
p	O
(	O
j	O
=	O
j	O
,	O
i	O
=	O
1	O
,	O
h	O
=	O
0	O
)	O
j	O
(	O
cid:2	O
)	O
p	O
(	O
j	O
=	O
j	O
(	O
cid:2	O
)	O
,	O
i	O
=	O
1	O
,	O
h	O
=	O
0	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
p	O
(	O
xh	O
,	O
xq	O
,	O
xv	O
)	O
x	O
(	O
cid:2	O
)	O
p	O
(	O
xh	O
,	O
x	O
(	O
cid:2	O
)	O
q	O
,	O
xv	O
)	O
xh	O
xh	O
=	O
q	O
(	O
20.49	O
)	O
(	O
20.50	O
)	O
20.3.	O
the	O
variable	B
elimination	I
algorithm	O
717	O
the	O
normalization	O
constant	O
in	O
the	O
denominator	O
,	O
p	O
(	O
xv	O
)	O
,	O
is	O
called	O
the	O
probability	O
of	O
the	O
evi-	O
dence	O
.	O
see	O
variableelimination	O
for	O
a	O
simple	O
matlab	O
implementation	O
of	O
this	O
algorithm	O
,	O
which	O
works	O
for	O
arbitrary	O
graphs	O
,	O
and	O
arbitrary	O
discrete	B
factors	O
.	O
but	O
before	O
you	O
go	O
too	O
crazy	O
,	O
please	O
read	O
section	O
20.3.2	O
,	O
which	O
points	O
out	O
that	O
ve	O
can	O
be	O
exponentially	O
slow	O
in	O
the	O
worst	O
case	O
.	O
20.3.1	O
the	O
generalized	O
distributive	O
law	O
*	O
(	O
cid:4	O
)	O
(	O
cid:20	O
)	O
p	O
(	O
xq|xv	O
)	O
∝	O
abstractly	O
,	O
ve	O
can	O
be	O
thought	O
of	O
as	O
computing	O
the	O
following	O
expression	O
:	O
ψc	O
(	O
xc	O
)	O
(	O
20.51	O
)	O
x	O
c	O
it	O
is	O
understood	O
that	O
the	O
visible	B
variables	I
xv	O
are	O
clamped	O
,	O
and	O
not	O
summed	O
over	O
.	O
ve	O
uses	O
non-serial	B
dynamic	I
programming	I
(	O
bertele	O
and	O
brioschi	O
1972	O
)	O
,	O
caching	O
intermediate	O
results	O
to	O
avoid	O
redundant	O
computation	O
.	O
however	O
,	O
there	O
are	O
other	O
tasks	O
we	O
might	O
like	O
to	O
solve	O
for	O
any	O
given	O
graphical	B
model	I
.	O
for	O
example	O
,	O
we	O
might	O
want	O
the	O
map	O
estimate	O
:	O
(	O
cid:20	O
)	O
x∗	O
=	O
argmax	O
x	O
c	O
ψc	O
(	O
xc	O
)	O
(	O
20.52	O
)	O
fortunately	O
,	O
essentially	O
the	O
same	O
algorithm	O
can	O
also	O
be	O
used	O
to	O
solve	O
this	O
task	O
:	O
we	O
just	O
replace	O
sum	O
with	O
max	O
.	O
(	O
we	O
also	O
need	O
a	O
traceback	B
step	O
,	O
which	O
actually	O
recovers	O
the	O
argmax	O
,	O
as	O
opposed	O
to	O
just	O
the	O
value	O
of	O
max	O
;	O
these	O
details	O
are	O
explained	O
in	O
section	O
17.4.4	O
.	O
)	O
two	O
binary	O
operations	O
called	O
“	O
+	O
”	O
and	O
“	O
×	O
”	O
,	O
which	O
satisfy	O
the	O
following	O
three	O
axioms	O
:	O
1.	O
the	O
operation	O
“	O
+	O
”	O
is	O
associative	B
and	O
commutative	O
,	O
and	O
there	O
is	O
an	O
additive	O
identity	O
element	O
in	O
general	O
,	O
ve	O
can	O
be	O
applied	O
to	O
any	O
commutative	B
semi-ring	I
.	O
this	O
is	O
a	O
set	O
k	O
,	O
together	O
with	O
called	O
“	O
0	O
”	O
such	O
that	O
k	O
+	O
0	O
=	O
k	O
for	O
all	O
k	O
∈	O
k.	O
2.	O
the	O
operation	O
“	O
×	O
”	O
is	O
associative	B
and	O
commutative	O
,	O
and	O
there	O
is	O
a	O
multiplicative	O
identity	O
element	O
called	O
“	O
1	O
”	O
such	O
that	O
k	O
×	O
1	O
=k	O
for	O
all	O
k	O
∈	O
k.	O
3.	O
the	O
distributive	B
law	I
holds	O
,	O
i.e.	O
,	O
(	O
a	O
×	O
b	O
)	O
+	O
(	O
a	O
×	O
c	O
)	O
=	O
a	O
×	O
(	O
b	O
+	O
c	O
)	O
for	O
all	O
triples	O
(	O
a	O
,	O
b	O
,	O
c	O
)	O
from	O
k.	O
(	O
20.53	O
)	O
this	O
framework	O
covers	O
an	O
extremely	O
wide	O
range	O
of	O
important	O
applications	O
,	O
including	O
constraint	B
satisfaction	I
problems	I
(	O
bistarelli	O
et	O
al	O
.	O
1997	O
;	O
dechter	O
2003	O
)	O
,	O
the	O
fast	O
fourier	O
transform	O
(	O
aji	O
and	O
mceliece	O
2000	O
)	O
,	O
etc	O
.	O
see	O
table	O
20.2	O
for	O
some	O
examples	O
.	O
20.3.2	O
computational	O
complexity	O
of	O
ve	O
the	O
running	O
time	O
of	O
ve	O
is	O
clearly	O
exponential	O
in	O
the	O
size	O
of	O
the	O
largest	O
factor	B
,	O
since	O
we	O
have	O
sum	O
over	O
all	O
of	O
the	O
corresponding	O
variables	O
.	O
some	O
of	O
the	O
factors	B
come	O
from	O
the	O
original	O
model	O
(	O
and	O
are	O
thus	O
unavoidable	O
)	O
,	O
but	O
new	O
factors	B
are	O
created	O
in	O
the	O
process	O
of	O
summing	O
out	O
.	O
for	O
example	O
,	O
718	O
chapter	O
20.	O
exact	O
inference	B
for	O
graphical	O
models	O
domain	O
[	O
0	O
,	O
∞	O
)	O
[	O
0	O
,	O
∞	O
)	O
(	O
−∞	O
,	O
∞	O
]	O
{	O
t	O
,	O
f	O
}	O
+	O
(	O
+	O
,	O
0	O
)	O
(	O
max	O
,	O
0	O
)	O
(	O
min	O
,	O
∞	O
)	O
(	O
∨	O
,	O
f	O
)	O
×	O
name	O
(	O
×	O
,	O
1	O
)	O
sum-product	B
(	O
×	O
,	O
1	O
)	O
max-product	B
(	O
+	O
,	O
0	O
)	O
min-sum	O
(	O
∧	O
,	O
t	O
)	O
boolean	O
satisﬁability	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
ψd	O
(	O
d	O
,	O
c	O
)	O
table	O
20.2	O
some	O
commutative	O
semirings	O
.	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
ψj	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
ψi	O
(	O
i	O
)	O
ψs	O
(	O
s	O
,	O
i	O
)	O
d	O
c	O
h	O
l	O
s	O
i	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
ψd	O
(	O
d	O
,	O
c	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
d	O
c	O
h	O
l	O
s	O
ψj	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
d	O
c	O
(	O
cid:7	O
)	O
ψd	O
(	O
d	O
,	O
c	O
)	O
h	O
l	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
g	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
(	O
cid:7	O
)	O
i	O
ψg	O
(	O
g	O
,	O
i	O
,	O
d	O
)	O
ψl	O
(	O
l	O
,	O
)	O
ψh	O
(	O
h	O
,	O
g	O
,	O
j	O
)	O
(	O
cid:11	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
τ1	O
(	O
i	O
,	O
d	O
,	O
l	O
,	O
j	O
,	O
h	O
)	O
ψi	O
(	O
i	O
)	O
ψs	O
(	O
s	O
,	O
i	O
)	O
τ1	O
(	O
i	O
,	O
d	O
,	O
l	O
,	O
j	O
,	O
h	O
)	O
(	O
cid:11	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
τ2	O
(	O
d	O
,	O
l	O
,	O
s	O
,	O
j	O
,	O
h	O
)	O
ψj	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
τ2	O
(	O
d	O
,	O
l	O
,	O
s	O
,	O
j	O
,	O
h	O
)	O
(	O
cid:11	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
s	O
ψd	O
(	O
d	O
,	O
c	O
)	O
d	O
c	O
h	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
d	O
c	O
ψd	O
(	O
d	O
,	O
c	O
)	O
τ3	O
(	O
d	O
,	O
l	O
,	O
j	O
,	O
h	O
)	O
(	O
cid:7	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
l	O
τ3	O
(	O
d	O
,	O
l	O
,	O
j	O
,	O
h	O
)	O
(	O
cid:11	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
τ4	O
(	O
d	O
,	O
j	O
,	O
h	O
)	O
τ4	O
(	O
d	O
,	O
j	O
,	O
h	O
)	O
(	O
cid:11	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
τ5	O
(	O
d	O
,	O
j	O
)	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
h	O
ψd	O
(	O
d	O
,	O
c	O
)	O
τ5	O
(	O
d	O
,	O
j	O
)	O
(	O
cid:11	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
τ6	O
(	O
d	O
,	O
j	O
)	O
(	O
cid:7	O
)	O
τ6	O
(	O
d	O
,	O
j	O
)	O
(	O
cid:9	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
(	O
cid:8	O
)	O
d	O
(	O
cid:7	O
)	O
d	O
(	O
cid:7	O
)	O
(	O
cid:8	O
)	O
c	O
table	O
20.3	O
eliminating	O
variables	O
from	O
figure	O
20.3	O
in	O
the	O
order	O
g	O
,	O
i	O
,	O
s	O
,	O
l	O
,	O
h	O
,	O
c	O
,	O
d.	O
τ7	O
(	O
j	O
)	O
in	O
equation	O
20.47	O
,	O
we	O
created	O
a	O
factor	B
involving	O
g	O
,	O
i	O
and	O
s	O
;	O
but	O
these	O
nodes	B
were	O
not	O
originally	O
present	O
together	O
in	O
any	O
factor	B
.	O
the	O
order	O
in	O
which	O
we	O
perform	O
the	O
summation	O
is	O
known	O
as	O
the	O
elimination	B
order	I
.	O
this	O
can	O
have	O
a	O
large	O
impact	O
on	O
the	O
size	O
of	O
the	O
intermediate	O
factors	B
that	O
are	O
created	O
.	O
for	O
example	O
,	O
consider	O
the	O
ordering	O
in	O
table	O
20.1	O
:	O
the	O
largest	O
created	O
factor	B
(	O
beyond	O
the	O
original	O
ones	O
in	O
the	O
model	O
)	O
has	O
size	O
3	O
,	O
corresponding	O
to	O
τ5	O
(	O
j	O
,	O
l	O
,	O
s	O
)	O
.	O
now	O
consider	O
the	O
ordering	O
in	O
table	O
20.3	O
:	O
now	O
the	O
largest	O
factors	B
are	O
τ1	O
(	O
i	O
,	O
d	O
,	O
l	O
,	O
j	O
,	O
h	O
)	O
and	O
τ2	O
(	O
d	O
,	O
l	O
,	O
s	O
,	O
j	O
,	O
h	O
)	O
,	O
which	O
are	O
much	O
bigger	O
.	O
we	O
can	O
determine	O
the	O
size	O
of	O
the	O
largest	O
factor	B
graphically	O
,	O
without	O
worrying	O
about	O
the	O
actual	O
numerical	O
values	O
of	O
the	O
factors	B
.	O
when	O
we	O
eliminate	O
a	O
variable	O
xt	O
,	O
we	O
connect	O
it	O
to	O
all	O
variables	O
20.3.	O
the	O
variable	B
elimination	I
algorithm	O
719	O
coherence	O
coherence	O
coherence	O
diﬃculty	O
intelligence	O
diﬃculty	O
intelligence	O
diﬃculty	O
intelligence	O
grade	O
sat	O
grade	O
sat	O
letter	O
job	O
letter	O
job	O
happy	O
(	O
a	O
)	O
happy	O
(	O
b	O
)	O
grade	O
sat	O
letter	O
job	O
happy	O
(	O
c	O
)	O
figure	O
20.4	O
example	O
of	O
the	O
elimination	O
process	O
,	O
in	O
the	O
order	O
c	O
,	O
d	O
,	O
i	O
,	O
etc	O
.	O
when	O
we	O
eliminate	O
i	O
(	O
ﬁgure	O
c	O
)	O
,	O
we	O
add	O
a	O
ﬁll-in	O
edge	O
between	O
g	O
and	O
s	O
,	O
since	O
they	O
are	O
not	O
connected	O
.	O
based	O
on	O
figure	O
9.10	O
of	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
that	O
share	O
a	O
factor	B
with	O
xt	O
(	O
to	O
reﬂect	O
the	O
new	O
temporary	O
factor	B
τ	O
(	O
cid:2	O
)	O
t	O
)	O
.	O
the	O
edges	B
created	O
by	O
this	O
process	O
are	O
called	O
ﬁll-in	B
edges	I
.	O
for	O
example	O
,	O
figure	O
20.4	O
shows	O
the	O
ﬁll-in	B
edges	I
introduced	O
when	O
we	O
eliminate	O
in	O
the	O
order	O
c	O
,	O
d	O
,	O
i	O
,	O
.	O
.	O
..	O
the	O
ﬁrst	O
two	O
steps	O
do	O
not	O
introduce	O
any	O
ﬁll-ins	O
,	O
but	O
when	O
we	O
eliminate	O
i	O
,	O
we	O
connect	O
g	O
and	O
s	O
,	O
since	O
they	O
co-occur	O
in	O
equation	O
20.48.	O
let	O
g	O
(	O
≺	O
)	O
be	O
the	O
(	O
undirected	B
)	O
graph	B
induced	O
by	O
applying	O
variable	B
elimination	I
to	O
g	O
using	O
elimination	O
ordering	O
≺	O
.	O
the	O
temporary	O
factors	B
generated	O
by	O
ve	O
correspond	O
to	O
maximal	O
cliques	O
in	O
the	O
graph	B
g	O
(	O
≺	O
)	O
.	O
for	O
example	O
,	O
with	O
ordering	O
(	O
c	O
,	O
d	O
,	O
i	O
,	O
h	O
,	O
g	O
,	O
s	O
,	O
l	O
)	O
,	O
the	O
maximal	O
cliques	O
are	O
as	O
follows	O
:	O
{	O
c	O
,	O
d	O
}	O
,	O
{	O
d	O
,	O
i	O
,	O
g	O
}	O
,	O
{	O
g	O
,	O
l	O
,	O
s	O
,	O
j	O
}	O
,	O
{	O
g	O
,	O
j	O
,	O
h	O
}	O
,	O
{	O
g	O
,	O
i	O
,	O
s	O
}	O
(	O
cid:20	O
)	O
it	O
is	O
clear	O
that	O
the	O
time	O
complexity	O
of	O
ve	O
is	O
k|c|	O
c∈c	O
(	O
g	O
(	O
≺	O
)	O
)	O
(	O
20.54	O
)	O
(	O
20.55	O
)	O
where	O
c	O
are	O
the	O
cliques	B
that	O
are	O
created	O
,	O
|c|	O
is	O
the	O
size	O
of	O
the	O
clique	B
c	O
,	O
and	O
we	O
assume	O
for	O
notational	O
simplicity	O
that	O
all	O
the	O
variables	O
have	O
k	O
states	O
each	O
.	O
let	O
us	O
deﬁne	O
the	O
induced	B
width	I
of	O
a	O
graph	B
given	O
elimination	O
ordering	O
≺	O
,	O
denoted	O
w	O
(	O
≺	O
)	O
,	O
as	O
the	O
size	O
of	O
the	O
largest	O
factor	B
(	O
i.e.	O
,	O
the	O
largest	O
clique	B
in	O
the	O
induced	O
graph	O
)	O
minus	O
1.	O
then	O
it	O
is	O
easy	O
to	O
see	O
that	O
the	O
complexity	O
of	O
ve	O
with	O
ordering	O
≺	O
is	O
o	O
(	O
kw	O
(	O
≺	O
)	O
+1	O
)	O
.	O
obviously	O
we	O
would	O
like	O
to	O
minimize	O
the	O
running	O
time	O
,	O
and	O
hence	O
the	O
induced	B
width	I
.	O
let	O
us	O
deﬁne	O
the	O
treewidth	B
of	O
a	O
graph	B
as	O
the	O
minimal	B
induced	O
width	O
.	O
w	O
(	O
cid:2	O
)	O
min≺	O
max	O
c∈g	O
(	O
≺	O
)	O
|c|	O
−1	O
(	O
20.56	O
)	O
then	O
clearly	O
the	O
best	O
possible	O
running	O
time	O
for	O
ve	O
is	O
o	O
(	O
dk	O
w+1	O
)	O
.	O
unfortunately	O
,	O
one	O
can	O
show	O
that	O
for	O
arbitrary	O
graphs	O
,	O
ﬁnding	O
an	O
elimination	O
ordering	O
≺	O
that	O
minimizes	O
w	O
(	O
≺	O
)	O
is	O
np-hard	O
(	O
arnborg	O
et	O
al	O
.	O
1987	O
)	O
.	O
in	O
practice	O
greedy	O
search	O
techniques	O
are	O
used	O
to	O
ﬁnd	O
reasonable	O
orderings	O
(	O
kjaerulff	O
1990	O
)	O
,	O
although	O
people	O
have	O
tried	O
other	O
heuristic	O
methods	O
for	O
discrete	B
optimization	O
,	O
720	O
chapter	O
20.	O
exact	O
inference	B
for	O
graphical	O
models	O
such	O
as	O
genetic	B
algorithms	I
(	O
larranaga	O
et	O
al	O
.	O
1997	O
)	O
.	O
algorithms	O
with	O
provable	O
performance	O
guarantees	O
(	O
amir	O
2010	O
)	O
.	O
it	O
is	O
also	O
possible	O
to	O
derive	O
approximate	O
in	O
some	O
cases	O
,	O
the	O
optimal	O
elimination	O
ordering	O
is	O
clear	O
.	O
for	O
example	O
,	O
for	O
chains	O
,	O
we	O
should	O
work	O
forwards	O
or	O
backwards	O
in	O
time	O
.	O
for	O
trees	O
,	O
we	O
should	O
work	O
from	O
the	O
leaves	B
to	O
the	O
root	B
.	O
these	O
orderings	O
do	O
not	O
introduce	O
any	O
ﬁll-in	B
edges	I
,	O
so	O
w	O
=	O
1.	O
consequently	O
,	O
inference	B
in	O
chains	O
and	O
trees	O
takes	O
o	O
(	O
v	O
k	O
2	O
)	O
time	O
.	O
this	O
is	O
one	O
reason	O
why	O
markov	O
chains	O
and	O
markov	O
trees	O
are	O
so	O
widely	O
used	O
.	O
unfortunately	O
,	O
for	O
other	O
graphs	O
,	O
the	O
treewidth	B
is	O
large	O
.	O
for	O
example	O
,	O
for	O
an	O
m	O
×	O
n	O
2d	O
lattice	B
,	O
the	O
treewidth	B
is	O
o	O
(	O
min	O
{	O
m	O
,	O
n	O
}	O
)	O
(	O
lipton	O
and	O
tarjan	O
1979	O
)	O
.	O
so	O
ve	O
on	O
a	O
100	O
×	O
100	O
ising	O
model	O
would	O
take	O
o	O
(	O
2100	O
)	O
time	O
.	O
of	O
course	O
,	O
just	O
because	O
ve	O
is	O
slow	O
doesn	O
’	O
t	O
mean	O
that	O
there	O
isn	O
’	O
t	O
some	O
smarter	O
algorithm	O
out	O
there	O
.	O
we	O
discuss	O
this	O
issue	O
in	O
section	O
20.5	O
.	O
20.3.3	O
a	O
weakness	O
of	O
ve	O
the	O
main	O
disadvantage	O
of	O
the	O
variable	B
elimination	I
algorithm	O
(	O
apart	O
from	O
its	O
exponential	O
depen-	O
dence	O
on	O
treewidth	B
)	O
is	O
that	O
it	O
is	O
inefficient	O
if	O
we	O
want	O
to	O
compute	O
multiple	O
queries	O
conditioned	O
on	O
the	O
same	O
evidence	B
.	O
for	O
example	O
,	O
consider	O
computing	O
all	O
the	O
marginals	O
in	O
a	O
chain-structured	O
graphical	B
model	I
such	O
as	O
an	O
hmm	O
.	O
we	O
can	O
easily	O
compute	O
the	O
ﬁnal	O
marginal	O
p	O
(	O
xt|v	O
)	O
by	O
elimi-	O
nating	O
all	O
the	O
nodes	B
x1	O
to	O
xt−1	O
in	O
order	O
.	O
this	O
is	O
equivalent	O
to	O
the	O
forwards	O
algorithm	O
,	O
and	O
takes	O
o	O
(	O
k	O
2t	O
)	O
time	O
.	O
but	O
now	O
suppose	O
we	O
want	O
to	O
compute	O
p	O
(	O
xt−1|v	O
)	O
.	O
we	O
have	O
to	O
run	O
ve	O
again	O
,	O
at	O
a	O
cost	O
of	O
o	O
(	O
k	O
2t	O
)	O
time	O
.	O
so	O
the	O
total	O
cost	O
to	O
compute	O
all	O
the	O
marginals	O
is	O
o	O
(	O
k	O
2t	O
2	O
)	O
.	O
however	O
,	O
we	O
know	O
that	O
we	O
can	O
solve	O
this	O
problem	O
in	O
o	O
(	O
k	O
2t	O
)	O
using	O
forwards-backwards	B
.	O
the	O
difference	O
is	O
that	O
fb	O
caches	O
the	O
messages	O
computed	O
on	O
the	O
forwards	O
pass	O
,	O
so	O
it	O
can	O
reuse	O
them	O
later	O
.	O
the	O
same	O
argument	O
holds	O
for	O
bp	O
on	O
trees	O
.	O
for	O
example	O
,	O
consider	O
the	O
4-node	O
tree	B
in	O
fig-	O
ure	O
20.5.	O
we	O
can	O
compute	O
p	O
(	O
x1|v	O
)	O
by	O
eliminating	O
x2:4	O
;	O
this	O
is	O
equivalent	O
to	O
sending	O
messages	O
up	O
to	O
x1	O
(	O
the	O
messages	O
correspond	O
to	O
the	O
τ	O
factors	B
created	O
by	O
ve	O
)	O
.	O
similarly	O
we	O
can	O
compute	O
p	O
(	O
x2|v	O
)	O
,	O
p	O
(	O
x3|v	O
)	O
and	O
then	O
p	O
(	O
x4|v	O
)	O
.	O
we	O
see	O
that	O
some	O
of	O
the	O
messages	O
used	O
to	O
compute	O
the	O
marginal	O
on	O
one	O
node	O
can	O
be	O
re-used	O
to	O
compute	O
the	O
marginals	O
on	O
the	O
other	O
nodes	B
.	O
by	O
storing	O
the	O
messages	O
for	O
later	O
re-use	O
,	O
we	O
can	O
compute	O
all	O
the	O
marginals	O
in	O
o	O
(	O
dk	O
2	O
)	O
time	O
.	O
this	O
is	O
what	O
the	O
up-down	B
(	O
collect-distribute	O
)	O
algorithm	O
on	O
trees	O
does	O
.	O
the	O
question	O
is	O
:	O
how	O
can	O
we	O
combine	O
the	O
efficiency	O
of	O
bp	O
on	O
trees	O
with	O
the	O
generality	O
of	O
ve	O
?	O
the	O
answer	O
is	O
given	O
in	O
section	O
20.4	O
.	O
20.4	O
the	O
junction	B
tree	I
algorithm	I
*	O
the	O
junction	B
tree	I
algorithm	I
or	O
jta	O
generalizes	O
bp	O
from	O
trees	O
to	O
arbitrary	O
graphs	O
.	O
we	O
sketch	O
the	O
basic	O
idea	O
below	O
;	O
for	O
details	O
,	O
see	O
e.g.	O
,	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
20.4.1	O
creating	O
a	O
junction	B
tree	I
the	O
basic	O
idea	O
behind	O
the	O
jta	O
is	O
this	O
.	O
we	O
ﬁrst	O
run	O
the	O
ve	O
algorithm	O
“	O
symbolically	O
”	O
,	O
adding	O
ﬁll-in	B
edges	I
as	O
we	O
go	O
,	O
according	O
to	O
a	O
given	O
elimination	O
ordering	O
.	O
the	O
resulting	O
graph	B
will	O
be	O
a	O
chordal	B
graph	I
,	O
which	O
means	O
that	O
every	O
undirected	B
cycle	O
x1	O
−	O
x2	O
···x	O
k	O
−	O
x1	O
of	O
length	O
k	O
≥	O
4	O
has	O
a	O
20.4.	O
the	O
junction	B
tree	I
algorithm	I
*	O
721	O
(	O
cid:59	O
)	O
(	O
cid:20	O
)	O
(	O
cid:59	O
)	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
(	O
cid:68	O
)	O
(	O
cid:12	O
)	O
(	O
cid:59	O
)	O
(	O
cid:20	O
)	O
(	O
cid:59	O
)	O
(	O
cid:21	O
)	O
(	O
cid:59	O
)	O
(	O
cid:20	O
)	O
(	O
cid:59	O
)	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
(	O
cid:69	O
)	O
(	O
cid:12	O
)	O
(	O
cid:59	O
)	O
(	O
cid:20	O
)	O
(	O
cid:59	O
)	O
(	O
cid:21	O
)	O
(	O
cid:80	O
)	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:21	O
)	O
(	O
cid:12	O
)	O
(	O
cid:80	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:21	O
)	O
(	O
cid:12	O
)	O
(	O
cid:59	O
)	O
(	O
cid:22	O
)	O
(	O
cid:80	O
)	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:21	O
)	O
(	O
cid:12	O
)	O
(	O
cid:80	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:21	O
)	O
(	O
cid:12	O
)	O
(	O
cid:59	O
)	O
(	O
cid:22	O
)	O
(	O
cid:80	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:22	O
)	O
(	O
cid:12	O
)	O
(	O
cid:80	O
)	O
(	O
cid:23	O
)	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:23	O
)	O
(	O
cid:12	O
)	O
(	O
cid:59	O
)	O
(	O
cid:23	O
)	O
(	O
cid:80	O
)	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:20	O
)	O
(	O
cid:12	O
)	O
(	O
cid:80	O
)	O
(	O
cid:23	O
)	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:21	O
)	O
(	O
cid:12	O
)	O
(	O
cid:59	O
)	O
(	O
cid:23	O
)	O
(	O
cid:80	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:23	O
)	O
(	O
cid:12	O
)	O
(	O
cid:80	O
)	O
(	O
cid:23	O
)	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:21	O
)	O
(	O
cid:12	O
)	O
(	O
cid:59	O
)	O
(	O
cid:23	O
)	O
(	O
cid:59	O
)	O
(	O
cid:23	O
)	O
(	O
cid:80	O
)	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:20	O
)	O
(	O
cid:12	O
)	O
(	O
cid:80	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:21	O
)	O
(	O
cid:12	O
)	O
(	O
cid:59	O
)	O
(	O
cid:22	O
)	O
(	O
cid:80	O
)	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:21	O
)	O
(	O
cid:12	O
)	O
(	O
cid:80	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:21	O
)	O
(	O
cid:12	O
)	O
(	O
cid:59	O
)	O
(	O
cid:22	O
)	O
(	O
cid:80	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:11	O
)	O
(	O
cid:91	O
)	O
(	O
cid:23	O
)	O
(	O
cid:12	O
)	O
figure	O
20.5	O
of	O
the	O
messages	O
needed	O
to	O
compute	O
all	O
singleton	O
marginals	O
.	O
based	O
on	O
figure	O
4.3	O
of	O
(	O
jordan	O
2007	O
)	O
.	O
sending	O
multiple	O
messages	O
along	O
a	O
tree	B
.	O
(	O
a	O
)	O
x1	O
is	O
root	B
.	O
(	O
b	O
)	O
x2	O
is	O
root	B
.	O
(	O
c	O
)	O
x4	O
is	O
root	B
.	O
(	O
d	O
)	O
all	O
(	O
cid:20	O
)	O
(	O
cid:24	O
)	O
(	O
cid:21	O
)	O
(	O
cid:25	O
)	O
(	O
a	O
)	O
(	O
cid:22	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:24	O
)	O
(	O
cid:21	O
)	O
(	O
cid:25	O
)	O
(	O
b	O
)	O
(	O
cid:22	O
)	O
(	O
cid:23	O
)	O
figure	O
20.6	O
left	O
:	O
this	O
graph	B
is	O
not	O
triangulated	O
,	O
despite	O
appearances	O
,	O
since	O
it	O
contains	O
a	O
chordless	O
5-cycle	O
1-2-3-4-5-1.	O
right	O
:	O
one	O
possible	O
triangulation	O
,	O
by	O
adding	O
the	O
1-3	O
and	O
1-4	O
ﬁll-in	B
edges	I
.	O
based	O
on	O
(	O
armstrong	O
2005	O
,	O
p46	O
)	O
722	O
chapter	O
20.	O
exact	O
inference	B
for	O
graphical	O
models	O
chord	O
,	O
i.e.	O
,	O
an	O
edge	O
connects	O
xi	O
,	O
xj	O
for	O
all	O
non-adjacent	O
nodes	B
i	O
,	O
j	O
in	O
the	O
cycle.2	O
having	O
created	O
a	O
chordal	B
graph	I
,	O
we	O
can	O
extract	O
its	O
maximal	O
cliques	O
.	O
in	O
general	O
,	O
ﬁnding	O
max	O
cliques	O
is	O
computationally	O
hard	O
,	O
but	O
it	O
turns	O
out	O
that	O
it	O
can	O
be	O
done	O
efficiently	O
from	O
this	O
special	O
kind	O
of	O
graph	B
.	O
figure	O
20.7	O
(	O
b	O
)	O
gives	O
an	O
example	O
,	O
where	O
the	O
max	O
cliques	O
are	O
as	O
follows	O
:	O
{	O
c	O
,	O
d	O
}	O
,	O
{	O
g	O
,	O
i	O
,	O
d	O
}	O
,	O
{	O
g	O
,	O
s	O
,	O
i	O
}	O
,	O
{	O
g	O
,	O
j	O
,	O
s	O
,	O
l	O
}	O
,	O
{	O
h	O
,	O
g	O
,	O
j	O
}	O
(	O
20.57	O
)	O
note	O
that	O
if	O
the	O
original	O
graphical	B
model	I
was	O
already	O
chordal	B
,	O
the	O
elimination	O
process	O
would	O
not	O
add	O
any	O
extra	O
ﬁll-in	B
edges	I
(	O
assuming	O
the	O
optimal	O
elimination	O
ordering	O
was	O
used	O
)	O
.	O
we	O
call	O
such	O
models	O
decomposable	B
,	O
since	O
they	O
break	O
into	O
little	O
pieces	O
deﬁned	O
by	O
the	O
cliques	B
.	O
it	O
turns	O
out	O
that	O
the	O
cliques	B
of	O
a	O
chordal	B
graph	I
can	O
be	O
arranged	O
into	O
a	O
special	O
kind	O
of	O
tree	B
known	O
as	O
a	O
junction	B
tree	I
.	O
this	O
enjoys	O
the	O
running	B
intersection	I
property	I
(	O
rip	O
)	O
,	O
which	O
means	O
that	O
any	O
subset	O
of	O
nodes	B
containing	O
a	O
given	O
variable	O
forms	O
a	O
connected	O
component	O
.	O
figure	O
20.7	O
(	O
c	O
)	O
gives	O
an	O
example	O
of	O
such	O
a	O
tree	B
.	O
we	O
see	O
that	O
the	O
node	O
i	O
occurs	O
in	O
two	O
adjacent	O
tree	B
nodes	O
,	O
so	O
they	O
can	O
share	O
information	B
about	O
this	O
variable	O
.	O
a	O
similar	B
situation	O
holds	O
for	O
all	O
the	O
other	O
variables	O
.	O
one	O
can	O
show	O
that	O
if	O
a	O
tree	B
that	O
satisﬁes	O
the	O
running	B
intersection	I
property	I
,	O
then	O
applying	O
bp	O
to	O
this	O
tree	B
(	O
as	O
we	O
explain	O
below	O
)	O
will	O
return	O
the	O
exact	O
values	O
of	O
p	O
(	O
xc|v	O
)	O
for	O
each	O
node	O
c	O
in	O
the	O
tree	B
(	O
i.e.	O
,	O
clique	B
in	O
the	O
induced	O
graph	O
)	O
.	O
from	O
this	O
,	O
we	O
can	O
easily	O
extract	O
the	O
node	O
and	O
edge	O
marginals	O
,	O
p	O
(	O
xt|v	O
)	O
and	O
p	O
(	O
xs	O
,	O
xt|v	O
)	O
from	O
the	O
original	O
model	O
,	O
by	O
marginalizing	O
the	O
clique	B
distributions.3	O
20.4.2	O
message	B
passing	I
on	O
a	O
junction	B
tree	I
having	O
constructed	O
a	O
junction	B
tree	I
,	O
we	O
can	O
use	O
it	O
for	O
inference	B
.	O
the	O
process	O
is	O
very	O
similar	B
to	O
belief	B
propagation	I
on	O
a	O
tree	B
.	O
as	O
in	O
section	O
20.2	O
,	O
there	O
are	O
two	O
versions	O
:	O
the	O
sum-product	B
form	O
,	O
also	O
known	O
as	O
the	O
shafer-shenoy	O
algorithm	O
,	O
named	O
after	O
(	O
shafer	O
and	O
shenoy	O
1990	O
)	O
;	O
and	O
the	O
belief	B
updating	I
form	O
(	O
which	O
involves	O
division	O
)	O
,	O
also	O
known	O
as	O
the	O
hugin	O
(	O
named	O
after	O
a	O
company	O
)	O
or	O
the	O
lauritzen-spiegelhalter	O
algorithm	O
(	O
named	O
after	O
(	O
lauritzen	O
and	O
spiegelhalter	O
1988	O
)	O
)	O
.	O
see	O
(	O
lepar	O
and	O
shenoy	O
1998	O
)	O
for	O
a	O
detailed	O
comparison	O
of	O
these	O
methods	O
.	O
below	O
we	O
sketch	O
how	O
the	O
hugin	O
algorithm	O
works	O
.	O
we	O
assume	O
the	O
original	O
model	O
has	O
the	O
following	O
form	O
:	O
ψc	O
(	O
xc	O
)	O
p	O
(	O
x	O
)	O
=	O
c∈c	O
(	O
g	O
)	O
(	O
20.58	O
)	O
where	O
c	O
(	O
g	O
)	O
are	O
the	O
cliques	B
of	O
the	O
original	O
graph	B
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
tree	B
deﬁnes	O
a	O
distribution	O
of	O
the	O
following	O
form	O
:	O
(	O
cid:20	O
)	O
1	O
z	O
(	O
cid:26	O
)	O
(	O
cid:26	O
)	O
p	O
(	O
x	O
)	O
=	O
c∈c	O
(	O
t	O
)	O
ψc	O
(	O
xc	O
)	O
s∈s	O
(	O
t	O
)	O
ψs	O
(	O
xs	O
)	O
(	O
20.59	O
)	O
2.	O
the	O
largest	O
loop	B
in	O
a	O
chordal	B
graph	I
is	O
length	O
3.	O
consequently	O
chordal	B
graphs	O
are	O
sometimes	O
called	O
triangulated	B
.	O
however	O
,	O
it	O
is	O
not	O
enough	O
for	O
the	O
graph	B
to	O
look	O
like	O
it	O
is	O
made	O
of	O
little	O
triangles	O
.	O
for	O
example	O
,	O
figure	O
20.6	O
(	O
a	O
)	O
is	O
not	O
chordal	O
,	O
even	O
though	O
it	O
is	O
made	O
of	O
little	O
triangles	O
,	O
since	O
it	O
contains	O
the	O
chordless	O
5-cycle	O
1-2-3-4-5-1	O
.	O
3.	O
if	O
we	O
want	O
the	O
joint	B
distribution	I
of	O
some	O
variables	O
that	O
are	O
not	O
in	O
the	O
same	O
clique	B
—	O
a	O
so-called	O
out-of-clique	B
query	I
—	O
we	O
can	O
adapt	O
the	O
technique	O
described	O
in	O
section	O
20.2.4.3	O
as	O
follows	O
:	O
create	O
a	O
mega	O
node	O
containing	O
the	O
query	B
variables	I
and	O
any	O
other	O
nuisance	B
variables	I
that	O
lie	O
on	O
the	O
path	B
between	O
them	O
,	O
multiply	O
in	O
messages	O
onto	O
the	O
boundary	O
of	O
the	O
mega	O
node	O
,	O
and	O
then	O
marginalize	O
out	O
the	O
internal	O
nuisance	B
variables	I
.	O
this	O
internal	O
marginalization	O
may	O
require	O
the	O
use	O
of	O
bp	O
or	O
ve	O
.	O
see	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
for	O
details	O
.	O
20.4.	O
the	O
junction	B
tree	I
algorithm	I
*	O
723	O
coherence	O
coherence	O
difﬁculty	O
intelligence	O
difﬁculty	O
intelligence	O
grade	O
sat	O
grade	O
sat	O
letter	O
letter	O
job	O
job	O
happy	O
happy	O
(	O
a	O
)	O
(	O
b	O
)	O
c	O
,	O
d	O
g	O
,	O
i	O
,	O
d	O
g	O
,	O
s	O
,	O
i	O
g	O
,	O
j	O
,	O
s	O
,	O
l	O
h	O
,	O
g	O
,	O
j	O
d	O
g	O
,	O
i	O
g	O
,	O
s	O
g	O
,	O
j	O
(	O
c	O
)	O
(	O
a	O
)	O
the	O
student	O
graph	B
with	O
ﬁll-in	B
edges	I
added	O
.	O
figure	O
20.7	O
(	O
c	O
)	O
the	O
junction	B
tree	I
.	O
an	O
edge	O
between	O
nodes	B
s	O
and	O
t	O
is	O
labeled	O
by	O
the	O
intersection	O
of	O
the	O
sets	O
on	O
nodes	B
s	O
and	O
t	O
;	O
this	O
is	O
called	O
the	O
separating	B
set	I
.	O
from	O
figure	O
9.11	O
of	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
.	O
(	O
b	O
)	O
the	O
maximal	O
cliques	O
.	O
where	O
c	O
(	O
t	O
)	O
are	O
the	O
nodes	B
of	O
the	O
junction	B
tree	I
(	O
which	O
are	O
the	O
cliques	B
of	O
the	O
chordal	B
graph	I
)	O
,	O
and	O
s	O
(	O
t	O
)	O
are	O
the	O
separators	O
of	O
the	O
tree	B
.	O
to	O
make	O
these	O
equal	O
,	O
we	O
initialize	O
by	O
deﬁning	O
ψs	O
=	O
1	O
for	O
all	O
separators	O
and	O
ψc	O
=	O
1	O
for	O
all	O
cliques	O
.	O
then	O
,	O
for	O
each	O
clique	B
in	O
the	O
original	O
model	O
,	O
c	O
∈	O
c	O
(	O
g	O
)	O
,	O
we	O
ﬁnd	O
a	O
clique	B
in	O
the	O
tree	B
c	O
(	O
cid:2	O
)	O
∈	O
c	O
(	O
t	O
)	O
which	O
contains	O
it	O
,	O
c	O
(	O
cid:2	O
)	O
⊇	O
c.	O
we	O
then	O
multiply	O
ψc	O
onto	O
ψc	O
(	O
cid:2	O
)	O
by	O
computing	O
ψc	O
(	O
cid:2	O
)	O
=	O
ψc	O
(	O
cid:2	O
)	O
ψc	O
.	O
after	O
doing	O
this	O
for	O
all	O
the	O
cliques	B
in	O
the	O
original	O
graph	B
,	O
we	O
have	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
ψc	O
(	O
xc	O
)	O
=	O
ψc	O
(	O
xc	O
)	O
(	O
20.60	O
)	O
c∈c	O
(	O
t	O
)	O
c∈c	O
(	O
g	O
)	O
as	O
in	O
section	O
20.2.1	O
,	O
we	O
now	O
send	O
messages	O
from	O
the	O
leaves	B
to	O
the	O
root	B
and	O
back	O
,	O
as	O
sketched	O
in	O
figure	O
20.1.	O
in	O
the	O
upwards	O
pass	O
,	O
also	O
known	O
as	O
the	O
collect-to-root	B
phase	O
,	O
node	O
i	O
sends	O
to	O
its	O
parent	O
j	O
the	O
following	O
message	O
:	O
mi→j	O
(	O
sij	O
)	O
=	O
ψi	O
(	O
ci	O
)	O
(	O
20.61	O
)	O
(	O
cid:4	O
)	O
ci\sij	O
(	O
cid:20	O
)	O
j∈chi	O
that	O
is	O
,	O
we	O
marginalize	O
out	O
the	O
variables	O
that	O
node	O
i	O
“	O
knows	O
about	O
”	O
which	O
are	O
irrelevant	O
to	O
j	O
,	O
and	O
then	O
we	O
send	O
what	O
is	O
left	O
over	O
.	O
once	O
a	O
node	O
has	O
received	O
messages	O
from	O
all	O
its	O
children	B
,	O
it	O
updates	O
its	O
belief	B
state	I
using	O
ψi	O
(	O
ci	O
)	O
∝	O
ψi	O
(	O
ci	O
)	O
mj→i	O
(	O
sij	O
)	O
(	O
20.62	O
)	O
724	O
chapter	O
20.	O
exact	O
inference	B
for	O
graphical	O
models	O
|v	O
)	O
,	O
which	O
is	O
the	O
posterior	O
over	O
the	O
nodes	B
in	O
clique	B
at	O
the	O
root	B
,	O
ψr	O
(	O
cr	O
)	O
represents	O
p	O
(	O
xcr	O
cr	O
conditioned	O
on	O
all	O
the	O
evidence	B
.	O
its	O
normalization	O
constant	O
is	O
p	O
(	O
v	O
)	O
/z0	O
,	O
where	O
z0	O
is	O
the	O
normalization	O
constant	O
for	O
the	O
unconditional	O
prior	O
,	O
p	O
(	O
x	O
)	O
.	O
(	O
we	O
have	O
z0	O
=	O
1	O
if	O
the	O
original	O
model	O
was	O
a	O
dgm	O
.	O
)	O
in	O
the	O
downwards	O
pass	O
,	O
also	O
known	O
as	O
the	O
distribute-from-root	B
phase	O
,	O
node	O
i	O
sends	O
to	O
its	O
(	O
cid:7	O
)	O
children	B
j	O
the	O
following	O
message	O
:	O
ψi	O
(	O
ci	O
)	O
mi→j	O
(	O
sij	O
)	O
=	O
ci\sij	O
mj→i	O
(	O
sij	O
)	O
we	O
divide	O
out	O
by	O
what	O
j	O
sent	O
to	O
i	O
to	O
avoid	O
double	O
counting	O
the	O
evidence	B
.	O
this	O
requires	O
that	O
we	O
store	O
the	O
messages	O
from	O
the	O
upwards	O
pass	O
.	O
once	O
a	O
node	O
has	O
received	O
a	O
top-down	O
message	O
from	O
its	O
parent	O
,	O
it	O
can	O
compute	O
its	O
ﬁnal	O
belief	B
state	I
using	O
ψj	O
(	O
cj	O
)	O
∝	O
ψj	O
(	O
cj	O
)	O
mi→j	O
(	O
sij	O
)	O
an	O
equivalent	O
way	O
to	O
present	O
this	O
algorithm	O
is	O
based	O
on	O
storing	O
the	O
messages	O
inside	O
the	O
separator	O
potentials	O
.	O
so	O
on	O
the	O
way	O
up	O
,	O
sending	O
from	O
i	O
to	O
j	O
we	O
compute	O
the	O
separator	O
potential	O
(	O
20.64	O
)	O
(	O
20.63	O
)	O
(	O
20.65	O
)	O
(	O
20.66	O
)	O
(	O
20.67	O
)	O
(	O
20.68	O
)	O
and	O
then	O
update	O
the	O
recipient	O
potential	O
:	O
ψ∗	O
ij	O
(	O
sij	O
)	O
=	O
ψi	O
(	O
ci	O
)	O
(	O
cid:4	O
)	O
ci\sij	O
j	O
(	O
cj	O
)	O
∝	O
ψj	O
(	O
cj	O
)	O
ψ∗	O
(	O
cid:4	O
)	O
ψ∗∗	O
ij	O
(	O
sij	O
)	O
=	O
ci\sij	O
ψ∗	O
ij	O
(	O
sij	O
)	O
ψij	O
(	O
sij	O
)	O
ψ∗	O
i	O
(	O
ci	O
)	O
and	O
then	O
update	O
the	O
recipient	O
potential	O
:	O
j	O
(	O
cj	O
)	O
∝	O
ψ∗	O
ψ∗∗	O
j	O
(	O
cj	O
)	O
ψ∗∗	O
ij	O
(	O
sij	O
)	O
ψ∗	O
ij	O
(	O
sij	O
)	O
(	O
recall	B
that	O
we	O
initialize	O
ψij	O
(	O
sij	O
)	O
=	O
1	O
.	O
)	O
this	O
is	O
sometimes	O
called	O
passing	B
a	I
ﬂow	I
from	O
i	O
to	O
j.	O
on	O
the	O
way	O
down	O
,	O
from	O
i	O
to	O
j	O
,	O
we	O
compute	O
the	O
separator	O
potential	O
this	O
process	O
is	O
known	O
as	O
junction	B
tree	I
calibration	O
.	O
see	O
figure	O
20.1	O
for	O
an	O
illustration	O
.	O
its	O
correctness	O
follows	O
from	O
the	O
fact	O
that	O
each	O
edge	O
partitions	O
the	O
evidence	B
into	O
two	O
distinct	O
groups	O
,	O
plus	O
the	O
fact	O
that	O
the	O
tree	B
satisﬁes	O
rip	O
,	O
which	O
ensures	O
that	O
no	O
information	O
is	O
lost	O
by	O
only	O
performing	O
local	O
computations	O
.	O
20.4.2.1	O
example	O
:	O
jtree	O
algorithm	O
on	O
a	O
chain	O
it	O
is	O
interesting	O
to	O
see	O
what	O
happens	O
if	O
we	O
apply	O
this	O
process	O
to	O
a	O
chain	O
structured	O
graph	B
such	O
as	O
an	O
hmm	O
.	O
a	O
detailed	O
discussion	O
can	O
be	O
found	O
in	O
(	O
smyth	O
et	O
al	O
.	O
1997	O
)	O
,	O
but	O
the	O
basic	O
idea	O
is	O
this	O
.	O
the	O
cliques	B
are	O
the	O
edges	B
,	O
and	O
the	O
separators	O
are	O
the	O
nodes	B
,	O
as	O
shown	O
in	O
figure	O
20.8.	O
we	O
initialize	O
the	O
potentials	O
as	O
follows	O
:	O
we	O
set	O
ψs	O
=	O
1	O
for	O
all	O
the	O
separators	O
,	O
we	O
set	O
ψc	O
(	O
xt−1	O
,	O
xt	O
)	O
=	O
p	O
(	O
xt|xt−1	O
)	O
for	O
clique	B
c	O
=	O
(	O
xt−1	O
,	O
xt	O
)	O
,	O
and	O
we	O
set	O
ψc	O
(	O
xt	O
,	O
yt	O
)	O
=	O
p	O
(	O
yt|xt	O
)	O
for	O
clique	B
c	O
=	O
(	O
xt	O
,	O
yt	O
)	O
.	O
20.4.	O
the	O
junction	B
tree	I
algorithm	I
*	O
725	O
x1	O
x2	O
x2	O
x2	O
x3	O
x3	O
x3	O
x4	O
x1	O
x2	O
x1	O
y1	O
x2	O
y2	O
x3	O
x3	O
y3	O
x4	O
x4	O
y4	O
figure	O
20.8	O
the	O
junction	B
tree	I
derived	O
from	O
an	O
hmm/ssm	O
of	O
length	O
t	O
=	O
4	O
.	O
(	O
cid:7	O
)	O
next	O
we	O
send	O
messages	O
from	O
left	O
to	O
right	O
.	O
consider	O
clique	B
(	O
xt−1	O
,	O
xt	O
)	O
with	O
potential	O
p	O
(	O
xt|xt−1	O
)	O
.	O
it	O
receives	O
a	O
message	O
from	O
clique	B
(	O
xt−2	O
,	O
xt−1	O
)	O
via	O
separator	O
xt−1	O
of	O
the	O
form	O
p	O
(	O
xt−2	O
,	O
xt−1|v1	O
:	O
t−1	O
)	O
=p	O
(	O
xt−1|v1	O
:	O
t−1	O
)	O
.	O
when	O
combined	O
with	O
the	O
clique	B
potential	O
,	O
xt−2	O
this	O
becomes	O
the	O
two-slice	O
predictive	O
density	O
p	O
(	O
xt|xt−1	O
)	O
p	O
(	O
xt−1|v1	O
:	O
t−1	O
)	O
=	O
p	O
(	O
xt−1	O
,	O
xt|v1	O
:	O
t−1	O
(	O
20.69	O
)	O
the	O
clique	B
(	O
xt−1	O
,	O
xt	O
)	O
also	O
receives	O
a	O
message	O
from	O
(	O
xt	O
,	O
yt	O
)	O
via	O
separator	O
xt	O
of	O
the	O
form	O
p	O
(	O
yt|xt	O
)	O
,	O
which	O
corresponds	O
to	O
its	O
local	B
evidence	I
.	O
when	O
combined	O
with	O
the	O
updated	O
clique	B
potential	O
,	O
this	O
becomes	O
the	O
two-slice	O
ﬁltered	O
posterior	O
p	O
(	O
xt−1	O
,	O
xt|v1	O
:	O
t−1	O
)	O
p	O
(	O
vt|xt	O
)	O
=	O
p	O
(	O
xt−1	O
,	O
xt|v1	O
:	O
t	O
)	O
(	O
20.70	O
)	O
thus	O
the	O
messages	O
in	O
the	O
forwards	O
pass	O
are	O
the	O
ﬁltered	O
belief	O
states	O
αt	O
,	O
and	O
the	O
clique	B
potentials	O
are	O
the	O
two-slice	O
distributions	O
.	O
in	O
the	O
backwards	O
pass	O
,	O
the	O
messages	O
are	O
the	O
update	O
factors	B
γt	O
αt	O
,	O
where	O
γt	O
(	O
k	O
)	O
=p	O
(	O
xt	O
=	O
k|v1	O
:	O
t	O
)	O
and	O
αt	O
(	O
k	O
)	O
=p	O
(	O
xt	O
=	O
k|v1	O
:	O
t	O
)	O
.	O
by	O
multiplying	O
by	O
this	O
message	O
,	O
we	O
“	O
swap	O
out	O
”	O
the	O
old	O
αt	O
message	O
and	O
“	O
swap	O
in	O
”	O
the	O
new	O
γt	O
message	O
.	O
we	O
see	O
that	O
the	O
backwards	O
pass	O
involves	O
working	O
with	O
posterior	O
beliefs	O
,	O
not	O
conditional	O
likelihoods	O
.	O
see	O
section	O
18.3.2.3	O
for	O
further	O
discussion	O
of	O
this	O
difference	O
.	O
20.4.3	O
computational	O
complexity	O
of	O
jta	O
if	O
all	O
nodes	O
are	O
discrete	O
with	O
k	O
states	O
each	O
,	O
it	O
is	O
clear	O
that	O
the	O
jta	O
takes	O
o	O
(	O
|c|k	O
w+1	O
)	O
time	O
and	O
space	O
,	O
where	O
|c|	O
is	O
the	O
number	O
of	O
cliques	B
and	O
w	O
is	O
the	O
treewidth	B
of	O
the	O
graph	B
,	O
i.e.	O
,	O
the	O
size	O
of	O
the	O
largest	O
clique	B
minus	O
1.	O
unfortunately	O
,	O
choosing	O
a	O
triangulation	O
so	O
as	O
to	O
minimize	O
the	O
treewidth	B
is	O
np-hard	O
,	O
as	O
explained	O
in	O
section	O
20.3.2.	O
the	O
jta	O
can	O
be	O
modiﬁed	O
to	O
handle	O
the	O
case	O
of	O
gaussian	O
graphical	O
models	O
.	O
the	O
graph-theoretic	O
steps	O
remain	O
unchanged	O
.	O
only	O
the	O
message	O
computation	O
differs	O
.	O
we	O
just	O
need	O
to	O
deﬁne	O
how	O
to	O
multiply	O
,	O
divide	O
,	O
and	O
marginalize	O
gaussian	O
potential	O
functions	O
.	O
this	O
is	O
most	O
easily	O
done	O
in	O
information	B
form	I
.	O
see	O
e.g.	O
,	O
(	O
lauritzen	O
1992	O
;	O
murphy	O
1998	O
;	O
cemgil	O
2001	O
)	O
for	O
the	O
details	O
.	O
the	O
algorithm	O
takes	O
o	O
(	O
|c|w3	O
)	O
time	O
and	O
o	O
(	O
|c|w2	O
)	O
space	O
.	O
when	O
applied	O
to	O
a	O
chain	O
structured	O
graph	B
,	O
the	O
algorithm	O
is	O
equivalent	O
to	O
the	O
kalman	O
smoother	O
in	O
section	O
18.3.2	O
.	O
726	O
chapter	O
20.	O
exact	O
inference	B
for	O
graphical	O
models	O
q1	O
q2	O
q3	O
q4	O
qn	O
c1	O
c2	O
c3	O
.	O
.	O
.	O
cm–1	O
a1	O
a2	O
.	O
.	O
.	O
am–2	O
cm	O
x	O
figure	O
20.9	O
encoding	O
a	O
3-sat	O
problem	O
on	O
n	O
variables	O
and	O
m	O
clauses	O
as	O
a	O
dgm	O
.	O
the	O
qs	O
variables	O
are	O
binary	O
random	O
variables	O
.	O
the	O
ct	O
variables	O
are	O
deterministic	O
functions	O
of	O
the	O
qs	O
’	O
s	O
,	O
and	O
compute	O
the	O
truth	O
value	O
of	O
each	O
clause	B
.	O
the	O
at	O
nodes	B
are	O
a	O
chain	O
of	O
and	O
gates	O
,	O
to	O
ensure	O
that	O
the	O
cpt	O
for	O
the	O
ﬁnal	O
x	O
node	O
has	O
bounded	O
size	O
.	O
the	O
double	O
rings	O
denote	O
nodes	B
with	O
deterministic	O
cpds	O
.	O
source	O
:	O
figure	O
9.1	O
of	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
.	O
20.4.4	O
jta	O
generalizations	O
*	O
we	O
have	O
seen	O
how	O
to	O
use	O
the	O
jta	O
algorithm	O
to	O
compute	O
posterior	O
marginals	O
in	O
a	O
graphical	B
model	I
.	O
there	O
are	O
several	O
possible	O
generalizations	O
of	O
this	O
algorithm	O
,	O
some	O
of	O
which	O
we	O
mention	O
below	O
.	O
all	O
of	O
these	O
exploit	O
graph	B
decomposition	O
in	O
some	O
form	O
or	O
other	O
.	O
they	O
only	O
differ	O
in	O
terms	O
of	O
how	O
they	O
deﬁne/	O
compute	O
messages	O
and	O
“	O
beliefs	O
”	O
.	O
the	O
key	O
requirement	O
is	O
that	O
the	O
operators	O
which	O
compute	O
messages	O
form	O
a	O
commutative	B
semiring	I
(	O
see	O
section	O
20.3.1	O
)	O
.	O
•	O
computing	O
the	O
map	O
estimate	O
.	O
we	O
just	O
replace	O
the	O
sum-product	B
with	O
max-product	B
in	O
the	O
collect	O
phase	O
,	O
and	O
use	O
traceback	B
in	O
the	O
distribute	O
phase	O
,	O
as	O
in	O
the	O
viterbi	O
algorithm	O
(	O
sec-	O
tion	O
17.4.4	O
)	O
.	O
see	O
(	O
dawid	O
1992	O
)	O
for	O
details	O
.	O
•	O
computing	O
the	O
n-most	O
probable	O
conﬁgurations	O
(	O
nilsson	O
1998	O
)	O
.	O
•	O
computing	O
posterior	O
samples	O
.	O
the	O
collect	O
pass	O
is	O
the	O
same	O
as	O
usual	O
,	O
but	O
in	O
the	O
distribute	O
pass	O
,	O
we	O
sample	O
variables	O
given	O
the	O
values	O
higher	O
up	O
in	O
the	O
tree	B
,	O
thus	O
generalizing	O
forwards-	O
ﬁltering	B
backwards-sampling	O
for	O
hmms	O
described	O
in	O
section	O
17.4.5.	O
see	O
(	O
dawid	O
1992	O
)	O
for	O
details	O
.	O
•	O
solving	O
constraint	B
satisfaction	I
problems	I
(	O
dechter	O
2003	O
)	O
.	O
•	O
solving	O
logical	B
reasoning	I
problems	I
(	O
amir	O
and	O
mcilraith	O
2005	O
)	O
.	O
20.5	O
computational	O
intractability	O
of	O
exact	O
inference	B
in	O
the	O
worst	O
case	O
as	O
we	O
saw	O
in	O
sections	O
20.3.2	O
and	O
20.4.3	O
,	O
ve	O
and	O
jta	O
take	O
time	O
that	O
is	O
exponential	O
in	O
the	O
treewidth	B
of	O
a	O
graph	B
.	O
since	O
the	O
treewidth	B
can	O
be	O
o	O
(	O
number	O
of	O
nodes	B
)	O
in	O
the	O
worst	O
case	O
,	O
this	O
means	O
these	O
algorithms	O
can	O
be	O
exponential	O
in	O
the	O
problem	O
size	O
.	O
of	O
course	O
,	O
just	O
because	O
ve	O
and	O
jta	O
are	O
slow	O
doesn	O
’	O
t	O
mean	O
that	O
there	O
isn	O
’	O
t	O
some	O
smarter	O
algo-	O
rithm	O
out	O
there	O
.	O
unfortunately	O
,	O
this	O
seems	O
unlikely	O
,	O
since	O
it	O
is	O
easy	O
to	O
show	O
that	O
exact	O
inference	B
is	O
np-hard	O
(	O
dagum	O
and	O
luby	O
1993	O
)	O
.	O
the	O
proof	O
is	O
a	O
simple	O
reduction	O
from	O
the	O
satisﬁability	O
prob-	O
20.5.	O
computational	O
intractability	O
of	O
exact	O
inference	B
in	O
the	O
worst	O
case	O
727	O
method	O
forwards-backwards	B
belief	O
propagation	O
variable	B
elimination	I
junction	O
tree	B
algorithm	O
loopy	B
belief	I
propagation	I
convex	O
belief	B
propagation	I
mean	O
ﬁeld	O
gibbs	O
sampling	O
restriction	O
chains	O
,	O
d	O
or	O
lg	O
trees	O
,	O
d	O
or	O
lg	O
low	O
treewidth	O
,	O
d	O
or	O
lg	O
,	O
single	O
query	O
low	O
treewidth	O
,	O
d	O
or	O
lg	O
approximate	O
,	O
d	O
or	O
lg	O
approximate	O
,	O
d	O
or	O
lg	O
approximate	O
,	O
c-e	O
approximate	O
section	O
section	O
17.4.3	O
section	O
20.2	O
section	O
20.3	O
section	O
20.4	O
section	O
22.2	O
section	O
22.4.2	O
section	O
21.3	O
section	O
24.2	O
table	O
20.4	O
summary	O
of	O
some	O
methods	O
that	O
can	O
be	O
used	O
for	O
inference	B
in	O
graphical	O
models	O
.	O
“	O
d	O
”	O
means	O
that	O
all	O
the	O
hidden	B
variables	I
must	O
be	O
discrete	B
.	O
“	O
l-g	O
”	O
means	O
that	O
all	O
the	O
factors	B
must	O
be	O
linear-gaussian	O
.	O
the	O
term	O
“	O
single	O
query	O
”	O
refers	O
to	O
the	O
restriction	O
that	O
ve	O
only	O
computes	O
one	O
marginal	O
p	O
(	O
xq|xv	O
)	O
at	O
a	O
time	O
.	O
see	O
section	O
20.3.3	O
for	O
a	O
discussion	O
of	O
this	O
point	O
.	O
“	O
c-e	O
”	O
stands	O
for	O
“	O
conjugate	O
exponential	O
”	O
;	O
this	O
means	O
that	O
variational	O
mean	O
ﬁeld	O
only	O
applies	O
to	O
models	O
where	O
the	O
likelihood	B
is	O
in	O
the	O
exponential	B
family	I
,	O
and	O
the	O
prior	O
is	O
conjugate	O
.	O
this	O
includes	O
the	O
d	O
and	O
lg	O
case	O
,	O
but	O
many	O
others	O
as	O
well	O
,	O
as	O
we	O
will	O
see	O
in	O
section	O
21.5.	O
in	O
particular	O
,	O
note	O
that	O
we	O
can	O
encode	O
any	O
3-sat	O
problem4	O
as	O
a	O
dgm	O
with	O
deterministic	O
lem	O
.	O
links	O
,	O
as	O
shown	O
in	O
figure	O
20.9.	O
we	O
clamp	O
the	O
ﬁnal	O
node	O
,	O
x	O
,	O
to	O
be	O
on	O
,	O
and	O
we	O
arrange	O
the	O
cpts	O
so	O
that	O
p	O
(	O
x	O
=	O
1	O
)	O
>	O
0	O
iff	B
there	O
a	O
satisfying	B
assignment	I
.	O
computing	O
any	O
posterior	O
marginal	O
requires	O
evaluating	O
the	O
normalization	O
constant	O
p	O
(	O
x	O
=	O
1	O
)	O
,	O
which	O
represents	O
the	O
probability	B
of	I
the	I
evidence	I
,	O
so	O
inference	B
in	O
this	O
model	O
implicitly	O
solves	O
the	O
sat	O
problem	O
.	O
in	O
fact	O
,	O
exact	O
inference	B
is	O
#	O
p-hard	O
(	O
roth	O
1996	O
)	O
,	O
which	O
is	O
even	O
harder	O
than	O
np-hard	O
.	O
(	O
see	O
e.g.	O
,	O
(	O
arora	O
and	O
barak	O
2009	O
)	O
for	O
deﬁnitions	O
of	O
these	O
terms	O
.	O
)	O
the	O
intuitive	O
reason	O
for	O
this	O
is	O
that	O
to	O
compute	O
the	O
normalizing	O
constant	O
z	O
,	O
we	O
have	O
tocount	O
how	O
many	O
satisfying	O
assignments	O
there	O
are	O
.	O
by	O
contrast	O
,	O
map	O
estimation	O
is	O
provably	O
easier	O
for	O
some	O
model	O
classes	O
(	O
greig	O
et	O
al	O
.	O
1989	O
)	O
,	O
since	O
,	O
intuitively	O
speaking	O
,	O
it	O
only	O
requires	O
ﬁnding	O
one	O
satisfying	B
assignment	I
,	O
not	O
counting	O
all	O
of	O
them	O
.	O
20.5.1	O
approximate	B
inference	I
many	O
popular	O
probabilistic	O
models	O
support	B
efficient	O
exact	O
inference	B
,	O
since	O
they	O
are	O
based	O
on	O
chains	O
,	O
trees	O
or	O
low	O
treewidth	O
graphs	O
.	O
but	O
there	O
are	O
many	O
other	O
models	O
for	O
which	O
exact	O
in	O
fact	O
,	O
even	O
simple	O
two	O
node	O
models	O
of	O
the	O
form	O
θ	O
→	O
x	O
may	O
not	O
inference	O
is	O
intractable	O
.	O
support	B
exact	O
inference	B
if	O
the	O
prior	O
on	O
θ	O
is	O
not	O
conjugate	O
to	O
the	O
likelihood	B
p	O
(	O
x|θ	O
)	O
.5	O
therefore	O
we	O
will	O
need	O
to	O
turn	O
to	O
approximate	B
inference	I
methods	O
.	O
see	O
table	O
20.4	O
for	O
a	O
summary	O
of	O
coming	O
attractions	O
.	O
for	O
the	O
most	O
part	O
,	O
these	O
methods	O
do	O
not	O
come	O
with	O
any	O
guarantee	O
as	O
to	O
their	O
accuracy	O
or	O
running	O
time	O
.	O
theoretical	O
computer	O
scientists	O
would	O
therefore	O
describe	O
them	O
as	O
heuristics	B
rather	O
than	O
approximation	O
algorithms	O
.	O
in	O
fact	O
,	O
one	O
can	O
prove	O
that	O
4.	O
a	O
3-sat	O
problem	O
is	O
a	O
logical	O
expression	O
of	O
the	O
form	O
(	O
q1	O
∧	O
q2	O
∧	O
¬q3	O
)	O
∨	O
(	O
q1	O
∧	O
¬q4	O
∧	O
q5	O
)	O
·	O
·	O
·	O
,	O
where	O
the	O
qi	O
are	O
binary	O
variables	O
,	O
and	O
each	O
clause	B
consists	O
of	O
the	O
conjunction	O
of	O
three	O
variables	O
(	O
or	O
their	O
negation	O
)	O
.	O
the	O
goal	O
is	O
to	O
ﬁnd	O
a	O
satisfying	B
assignment	I
,	O
which	O
is	O
a	O
set	O
of	O
values	O
for	O
the	O
qi	O
variables	O
such	O
that	O
the	O
expression	O
evaluates	O
to	O
true	O
.	O
5.	O
for	O
discrete	O
random	O
variables	O
,	O
conjugacy	O
is	O
not	O
a	O
concern	O
,	O
since	O
discrete	B
distributions	O
are	O
always	O
closed	O
under	O
conditioning	B
and	O
marginalization	O
.	O
consequently	O
,	O
graph-theoretic	O
considerations	O
are	O
of	O
more	O
importance	O
when	O
discussing	O
inference	B
in	O
models	O
with	O
discrete	B
hidden	O
states	O
.	O
728	O
chapter	O
20.	O
exact	O
inference	B
for	O
graphical	O
models	O
it	O
is	O
not	O
possible	O
to	O
construct	O
polynomial	B
time	I
approximation	I
schemes	I
for	O
inference	B
in	O
general	O
discrete	B
gms	O
(	O
dagum	O
and	O
luby	O
1993	O
;	O
roth	O
1996	O
)	O
.	O
fortunately	O
,	O
we	O
will	O
see	O
that	O
for	O
many	O
of	O
these	O
heuristic	O
methods	O
often	O
perform	O
well	O
in	O
practice	O
.	O
exercises	O
exercise	O
20.1	O
variable	B
elimination	I
consider	O
the	O
mrf	O
in	O
figure	O
10.14	O
(	O
b	O
)	O
.	O
a.	O
suppose	O
we	O
want	O
to	O
compute	O
the	O
partition	B
function	I
using	O
the	O
elimination	O
ordering	O
≺=	O
(	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
6	O
)	O
,	O
i.e.	O
,	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
ψ12	O
(	O
x1	O
,	O
x2	O
)	O
ψ13	O
(	O
x1	O
,	O
x3	O
)	O
ψ24	O
(	O
x2	O
,	O
x4	O
)	O
ψ34	O
(	O
x3	O
,	O
x4	O
)	O
ψ45	O
(	O
x4	O
,	O
x5	O
)	O
ψ56	O
(	O
x5	O
,	O
x6	O
)	O
(	O
20.71	O
)	O
x6	O
x5	O
x4	O
x3	O
x2	O
x1	O
if	O
we	O
use	O
the	O
variable	B
elimination	I
algorithm	O
,	O
we	O
will	O
create	O
new	O
intermediate	O
factors	B
.	O
what	O
is	O
the	O
largest	O
intermediate	O
factor	B
?	O
b.	O
add	O
an	O
edge	O
to	O
the	O
original	O
mrf	O
between	O
every	O
pair	O
of	O
variables	O
that	O
end	O
up	O
in	O
the	O
same	O
factor	B
.	O
(	O
these	O
are	O
called	O
ﬁll	O
in	O
edges	B
.	O
)	O
draw	O
the	O
resulting	O
mrf	O
.	O
what	O
is	O
the	O
size	O
of	O
the	O
largest	O
maximal	B
clique	I
in	O
this	O
graph	B
?	O
(	O
cid:12	O
)	O
c.	O
now	O
consider	O
elimination	O
ordering	O
≺=	O
(	O
4	O
,	O
1	O
,	O
2	O
,	O
3	O
,	O
5	O
,	O
6	O
)	O
,	O
i.e.	O
,	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
ψ12	O
(	O
x1	O
,	O
x2	O
)	O
ψ13	O
(	O
x1	O
,	O
x3	O
)	O
ψ24	O
(	O
x2	O
,	O
x4	O
)	O
ψ34	O
(	O
x3	O
,	O
x4	O
)	O
ψ45	O
(	O
x4	O
,	O
x5	O
)	O
ψ56	O
(	O
x5	O
,	O
x6	O
)	O
(	O
20.72	O
)	O
x6	O
x5	O
x3	O
x2	O
x1	O
x4	O
if	O
we	O
use	O
the	O
variable	B
elimination	I
algorithm	O
,	O
we	O
will	O
create	O
new	O
intermediate	O
factors	B
.	O
what	O
is	O
the	O
largest	O
intermediate	O
factor	B
?	O
d.	O
add	O
an	O
edge	O
to	O
the	O
original	O
mrf	O
between	O
every	O
pair	O
of	O
variables	O
that	O
end	O
up	O
in	O
the	O
same	O
factor	B
.	O
(	O
these	O
are	O
called	O
ﬁll	O
in	O
edges	B
.	O
)	O
draw	O
the	O
resulting	O
mrf	O
.	O
what	O
is	O
the	O
size	O
of	O
the	O
largest	O
maximal	B
clique	I
in	O
this	O
graph	B
?	O
exercise	O
20.2	O
gaussian	O
times	O
gaussian	O
is	O
gaussian	O
prove	O
equation	O
20.17.	O
hint	O
:	O
use	O
completing	B
the	I
square	I
.	O
exercise	O
20.3	O
message	B
passing	I
on	O
a	O
tree	B
consider	O
the	O
dgm	O
in	O
figure	O
20.10	O
which	O
represents	O
the	O
following	O
ﬁctitious	O
biological	O
model	O
.	O
each	O
gi	O
represents	O
the	O
genotype	B
of	O
a	O
person	O
:	O
gi	O
=	O
1	O
if	O
they	O
have	O
a	O
healthy	O
gene	O
and	O
gi	O
=	O
2	O
if	O
they	O
have	O
an	O
unhealthy	O
gene	O
.	O
g2	O
and	O
g3	O
may	O
inherit	O
the	O
unhealthy	O
gene	O
from	O
their	O
parent	O
g1	O
.	O
xi	O
∈	O
r	O
is	O
a	O
continuous	O
measure	O
of	O
blood	O
pressure	O
,	O
which	O
is	O
low	O
if	O
you	O
are	O
healthy	O
and	O
high	O
if	O
you	O
are	O
unhealthy	O
.	O
we	O
deﬁne	O
the	O
cpds	O
as	O
follows	O
p	O
(	O
xi|gi	O
=	O
1	O
)	O
=	O
n	O
(	O
xi|μ	O
=	O
50	O
,	O
σ2	O
=	O
10	O
)	O
p	O
(	O
xi|gi	O
=	O
2	O
)	O
=	O
n	O
(	O
xi|μ	O
=	O
60	O
,	O
σ2	O
=	O
10	O
)	O
(	O
20.76	O
)	O
(	O
20.77	O
)	O
the	O
meaning	O
of	O
the	O
matrix	O
for	O
p	O
(	O
g2|g1	O
)	O
is	O
that	O
p	O
(	O
g2	O
=	O
1|g1	O
=	O
1	O
)	O
=	O
0.9	O
,	O
p	O
(	O
g2	O
=	O
1|g1	O
=	O
2	O
)	O
=	O
0.1	O
,	O
etc	O
.	O
p	O
(	O
g1	O
)	O
=	O
[	O
0.5	O
,	O
0.5	O
]	O
p	O
(	O
g2|g1	O
)	O
=	O
p	O
(	O
g3|g1	O
)	O
=	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
0.9	O
0.1	O
0.9	O
0.1	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
0.1	O
0.9	O
0.1	O
0.9	O
(	O
20.73	O
)	O
(	O
20.74	O
)	O
(	O
20.75	O
)	O
20.5.	O
computational	O
intractability	O
of	O
exact	O
inference	B
in	O
the	O
worst	O
case	O
729	O
x1	O
g1	O
g2	O
g3	O
x2	O
x3	O
figure	O
20.10	O
a	O
simple	O
dag	O
representing	O
inherited	O
diseases	O
.	O
p	O
(	O
g1|x2	O
=	O
50	O
)	O
?	O
a.	O
suppose	O
you	O
observe	O
x2	O
=	O
50	O
,	O
and	O
x1	O
is	O
unobserved	O
.	O
what	O
is	O
the	O
posterior	O
belief	O
on	O
g1	O
,	O
i.e.	O
,	O
b.	O
now	O
suppose	O
you	O
observe	O
x2	O
=	O
50	O
amd	O
x3	O
=	O
50.	O
what	O
is	O
p	O
(	O
g1|x2	O
,	O
x3	O
)	O
?	O
explain	O
your	O
answer	O
c.	O
now	O
suppose	O
x2	O
=	O
60	O
,	O
x3	O
=	O
60.	O
what	O
is	O
p	O
(	O
g1|x2	O
,	O
x3	O
)	O
?	O
explain	O
your	O
answer	O
intuitively	O
.	O
d.	O
now	O
suppose	O
x2	O
=	O
50	O
,	O
x3	O
=	O
60.	O
what	O
is	O
p	O
(	O
g1|x1	O
,	O
x2	O
)	O
?	O
explain	O
your	O
answer	O
intuitively	O
.	O
intuitively	O
.	O
exercise	O
20.4	O
inference	B
in	O
2d	O
lattice	B
mrfs	O
consider	O
an	O
mrf	O
with	O
a	O
2d	O
m	O
×	O
n	O
lattice	B
graph	O
structure	O
,	O
so	O
each	O
hidden	B
node	O
,	O
xij	O
,	O
is	O
connected	O
to	O
its	O
4	O
nearest	O
neighbors	O
,	O
as	O
in	O
an	O
ising	O
model	O
.	O
in	O
addition	O
,	O
each	O
hidden	B
node	O
has	O
its	O
own	O
local	B
evidence	I
,	O
yij	O
.	O
assume	O
all	O
hidden	O
nodes	B
have	O
k	O
>	O
2	O
states	O
.	O
in	O
general	O
,	O
exact	O
inference	B
in	O
such	O
models	O
is	O
intractable	O
,	O
because	O
the	O
maximum	O
cliques	O
of	O
the	O
corresponding	O
triangulated	B
graph	O
have	O
size	O
o	O
(	O
max	O
{	O
m	O
,	O
n	O
}	O
)	O
.	O
suppose	O
m	O
(	O
cid:8	O
)	O
n	O
i.e.	O
,	O
the	O
lattice	B
is	O
short	O
and	O
fat	O
.	O
a.	O
how	O
can	O
one	O
efficiently	O
perform	O
exact	O
inference	B
(	O
using	O
a	O
deterministic	O
algorithm	O
)	O
in	O
such	O
models	O
?	O
(	O
by	O
exact	O
inference	B
,	O
i	O
mean	B
computing	O
marginal	O
probabilities	O
p	O
(	O
xij|	O
(	O
cid:22	O
)	O
y	O
)	O
exactly	O
,	O
where	O
(	O
cid:22	O
)	O
y	O
is	O
all	O
the	O
evidence	B
.	O
)	O
give	O
a	O
brief	O
description	O
of	O
your	O
method	O
.	O
b.	O
what	O
is	O
the	O
asymptotic	O
complexity	O
(	O
running	O
time	O
)	O
of	O
your	O
algorithm	O
?	O
c.	O
now	O
suppose	O
the	O
lattice	B
is	O
large	O
and	O
square	O
,	O
so	O
m	O
=	O
n	O
,	O
but	O
all	O
hidden	O
states	O
are	O
binary	O
(	O
ie	O
k	O
=	O
2	O
)	O
.	O
in	O
this	O
case	O
,	O
how	O
can	O
one	O
efficiently	O
exactly	O
compute	O
(	O
using	O
a	O
deterministic	O
algorithm	O
)	O
the	O
map	O
estimate	O
arg	O
maxx	O
p	O
(	O
x|y	O
)	O
,	O
wherex	O
is	O
the	O
joint	O
assignment	O
to	O
all	O
hidden	O
nodes	B
?	O
21	O
variational	B
inference	I
21.1	O
introduction	O
we	O
have	O
now	O
seen	O
several	O
algorithms	O
for	O
computing	O
(	O
functions	O
of	O
)	O
a	O
posterior	O
distribution	O
.	O
for	O
discrete	B
graphical	O
models	O
,	O
we	O
can	O
use	O
the	O
junction	B
tree	I
algorithm	I
to	O
perform	O
exact	O
inference	B
,	O
as	O
explained	O
in	O
section	O
20.4.	O
however	O
,	O
this	O
takes	O
time	O
exponential	O
in	O
the	O
treewidth	B
of	O
the	O
graph	B
,	O
rendering	O
exact	O
inference	B
often	O
impractical	O
.	O
for	O
the	O
case	O
of	O
gaussian	O
graphical	O
models	O
,	O
exact	O
inference	B
is	O
cubic	O
in	O
the	O
treewidth	B
.	O
however	O
,	O
even	O
this	O
can	O
be	O
too	O
slow	O
if	O
we	O
have	O
many	O
variables	O
.	O
in	O
addition	O
,	O
the	O
jta	O
does	O
not	O
work	O
for	O
continuous	O
random	O
variables	O
outside	O
of	O
the	O
gaussian	O
case	O
,	O
nor	O
for	O
mixed	O
discrete-continuous	O
variables	O
,	O
outside	O
of	O
the	O
conditionally	O
gaussian	O
case	O
.	O
for	O
some	O
simple	O
two	O
node	O
graphical	O
models	O
,	O
of	O
the	O
form	O
x	O
→	O
d	O
,	O
we	O
can	O
compute	O
the	O
exact	O
posterior	O
p	O
(	O
x|d	O
)	O
in	O
closed	O
form	O
,	O
provided	O
the	O
prior	O
p	O
(	O
x	O
)	O
is	O
conjugate	O
to	O
the	O
likelihood	B
,	O
p	O
(	O
d|x	O
)	O
(	O
which	O
means	O
the	O
likelihood	B
must	O
be	O
in	O
the	O
exponential	B
family	I
)	O
.	O
see	O
chapter	O
5	O
for	O
some	O
(	O
note	O
that	O
in	O
this	O
chapter	O
,	O
x	O
represent	O
the	O
unknown	B
variables	O
,	O
whereas	O
in	O
examples	O
of	O
this	O
.	O
chapter	O
5	O
,	O
we	O
used	O
θ	O
to	O
represent	O
the	O
unknowns	O
.	O
)	O
in	O
more	O
general	O
settings	O
,	O
we	O
must	O
use	O
approximate	B
inference	I
methods	O
.	O
in	O
section	O
8.4.1	O
,	O
we	O
discussed	O
the	O
gaussian	O
approximation	O
,	O
which	O
is	O
useful	O
for	O
inference	B
in	O
two	O
node	O
models	O
of	O
the	O
form	O
x	O
→	O
d	O
,	O
where	O
the	O
prior	O
is	O
not	O
conjugate	O
.	O
(	O
for	O
example	O
,	O
section	O
8.4.3	O
applied	O
the	O
method	O
to	O
logistic	B
regression	I
.	O
)	O
the	O
gaussian	O
approximation	O
is	O
simple	O
.	O
however	O
,	O
some	O
posteriors	O
are	O
not	O
naturally	O
modelled	O
using	O
gaussians	O
.	O
for	O
example	O
,	O
when	O
inferring	O
multinomial	B
parameters	O
,	O
a	O
dirichlet	O
distribution	O
is	O
a	O
better	O
choice	O
,	O
and	O
when	O
inferring	O
states	O
in	O
a	O
discrete	B
graphical	O
model	O
,	O
a	O
categorical	B
distribution	O
is	O
a	O
better	O
choice	O
.	O
in	O
this	O
chapter	O
,	O
we	O
will	O
study	O
a	O
more	O
general	O
class	O
of	O
deterministic	O
approximate	O
inference	B
algorithms	O
based	O
on	O
variational	B
inference	I
(	O
jordan	O
et	O
al	O
.	O
1998	O
;	O
jaakkola	O
and	O
jordan	O
2000	O
;	O
jaakkola	O
2001	O
;	O
wainwright	O
and	O
jordan	O
2008a	O
)	O
.	O
the	O
basic	O
idea	O
is	O
to	O
pick	O
an	O
approximation	O
q	O
(	O
x	O
)	O
to	O
the	O
distribution	O
from	O
some	O
tractable	O
family	O
,	O
and	O
then	O
to	O
try	O
to	O
make	O
this	O
approximation	O
as	O
close	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
x|d	O
)	O
.	O
this	O
reduces	O
inference	B
to	O
an	O
optimization	B
as	O
possible	O
to	O
the	O
true	O
posterior	O
,	O
p∗	O
problem	O
.	O
by	O
relaxing	O
the	O
constraints	O
and/or	O
approximating	O
the	O
objective	O
,	O
we	O
can	O
trade	O
accuracy	O
for	O
speed	O
.	O
the	O
bottom	O
line	O
is	O
that	O
variational	B
inference	I
often	O
gives	O
us	O
the	O
speed	O
beneﬁts	O
of	O
map	O
estimation	O
but	O
the	O
statistical	O
beneﬁts	O
of	O
the	O
bayesian	O
approach	O
.	O
732	O
chapter	O
21.	O
variational	B
inference	I
21.2	O
variational	B
inference	I
suppose	O
p∗	O
(	O
x	O
)	O
is	O
our	O
true	O
but	O
intractable	O
distribution	O
and	O
q	O
(	O
x	O
)	O
is	O
some	O
approximation	O
,	O
chosen	O
from	O
some	O
tractable	O
family	O
,	O
such	O
as	O
a	O
multivariate	O
gaussian	O
or	O
a	O
factored	O
distribution	O
.	O
we	O
assume	O
q	O
has	O
some	O
free	O
parameters	O
which	O
we	O
want	O
to	O
optimize	O
so	O
as	O
to	O
make	O
q	O
“	O
similar	B
to	O
”	O
p∗	O
.	O
an	O
obvious	O
cost	O
function	O
to	O
try	O
to	O
minimize	O
is	O
the	O
kl	O
divergence	O
:	O
kl	O
(	O
p∗||q	O
)	O
=	O
(	O
x	O
)	O
log	O
p∗	O
p∗	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
however	O
,	O
this	O
is	O
hard	O
to	O
compute	O
,	O
since	O
taking	O
expectations	O
wrt	O
p∗	O
a	O
natural	O
alternative	O
is	O
the	O
reverse	O
kl	O
divergence	O
:	O
(	O
cid:4	O
)	O
x	O
(	O
cid:4	O
)	O
kl	O
(	O
q||p∗	O
)	O
=	O
q	O
(	O
x	O
)	O
log	O
q	O
(	O
x	O
)	O
p∗	O
(	O
x	O
)	O
x	O
(	O
21.1	O
)	O
is	O
assumed	O
to	O
be	O
intractable	O
.	O
(	O
21.2	O
)	O
the	O
main	O
advantage	O
of	O
this	O
objective	O
is	O
that	O
computing	O
expectations	O
wrt	O
q	O
is	O
tractable	O
(	O
by	O
choos-	O
ing	O
a	O
suitable	O
form	O
for	O
q	O
)	O
.	O
we	O
discuss	O
the	O
statistical	O
differences	O
between	O
these	O
two	O
objectives	O
in	O
section	O
21.2.2.	O
unfortunately	O
,	O
equation	O
21.2	O
is	O
still	O
not	O
tractable	O
as	O
written	O
,	O
since	O
even	O
evaluating	O
p∗	O
(	O
x	O
)	O
=	O
p	O
(	O
x|d	O
)	O
pointwise	O
is	O
hard	O
,	O
since	O
it	O
requires	O
evaluating	O
the	O
intractable	O
normalization	O
constant	O
z	O
=	O
p	O
(	O
d	O
)	O
.	O
however	O
,	O
usually	O
the	O
unnormalized	O
distribution	O
˜p	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
x	O
,	O
d	O
)	O
=p	O
∗	O
(	O
x	O
)	O
z	O
is	O
tractable	O
to	O
compute	O
.	O
we	O
therefore	O
deﬁne	O
our	O
new	O
objective	O
function	O
as	O
follows	O
:	O
j	O
(	O
q	O
)	O
(	O
cid:2	O
)	O
kl	O
(	O
q||˜p	O
)	O
(	O
21.3	O
)	O
where	O
we	O
are	O
slightly	O
abusing	O
notation	O
,	O
since	O
˜p	O
is	O
not	O
a	O
normalized	O
distribution	O
.	O
plugging	O
in	O
the	O
deﬁnition	O
of	O
kl	O
,	O
we	O
get	O
j	O
(	O
q	O
)	O
=	O
=	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
x	O
x	O
q	O
(	O
x	O
)	O
log	O
q	O
(	O
x	O
)	O
˜p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
log	O
q	O
(	O
x	O
)	O
zp∗	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
p∗	O
(	O
x	O
)	O
)	O
−	O
log	O
z	O
q	O
(	O
x	O
)	O
log	O
=	O
=	O
kl	O
(	O
q||p∗	O
x	O
−	O
log	O
z	O
(	O
21.4	O
)	O
(	O
21.5	O
)	O
(	O
21.6	O
)	O
(	O
21.7	O
)	O
since	O
z	O
is	O
a	O
constant	O
,	O
by	O
minimizing	O
j	O
(	O
q	O
)	O
,	O
we	O
will	O
force	O
q	O
to	O
become	O
close	O
to	O
p∗	O
.	O
(	O
negative	B
log	I
likelihood	I
)	O
:	O
since	O
kl	O
divergence	O
is	O
always	O
non-negative	O
,	O
we	O
see	O
that	O
j	O
(	O
q	O
)	O
is	O
an	O
upper	O
bound	O
on	O
the	O
nll	O
j	O
(	O
q	O
)	O
=	O
kl	O
(	O
q||p∗	O
)	O
−	O
log	O
z	O
≥	O
−	O
log	O
z	O
=	O
−	O
log	O
p	O
(	O
d	O
)	O
(	O
21.8	O
)	O
alternatively	O
,	O
we	O
can	O
try	O
to	O
maximize	O
the	O
following	O
quantity	O
(	O
in	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
,	O
this	O
is	O
referred	O
to	O
as	O
the	O
energy	B
functional	I
)	O
,	O
which	O
is	O
a	O
lower	O
bound	O
on	O
the	O
log	O
likelihood	O
of	O
the	O
data	O
:	O
l	O
(	O
q	O
)	O
(	O
cid:2	O
)	O
−j	O
(	O
q	O
)	O
=	O
−kl	O
(	O
q||p∗	O
)	O
+	O
log	O
z	O
≤	O
log	O
z	O
=	O
log	O
p	O
(	O
d	O
)	O
(	O
21.9	O
)	O
,	O
we	O
see	O
that	O
variational	B
inference	I
is	O
closely	O
related	O
to	O
em	O
since	O
this	O
bound	O
is	O
tight	O
when	O
q	O
=	O
p∗	O
(	O
see	O
section	O
11.4.7	O
)	O
.	O
21.2.	O
variational	B
inference	I
733	O
21.2.1	O
alternative	O
interpretations	O
of	O
the	O
variational	O
objective	O
there	O
are	O
several	O
equivalent	O
ways	O
of	O
writing	O
this	O
objective	O
that	O
provide	O
different	O
insights	O
.	O
one	O
formulation	O
is	O
as	O
follows	O
:	O
j	O
(	O
q	O
)	O
=	O
eq	O
[	O
log	O
q	O
(	O
x	O
)	O
]	O
+	O
eq	O
[	O
−	O
log	O
˜p	O
(	O
x	O
)	O
]	O
=	O
−h	O
(	O
q	O
)	O
+e	O
q	O
[	O
e	O
(	O
x	O
)	O
]	O
(	O
21.10	O
)	O
which	O
is	O
the	O
expected	O
energy	O
(	O
recall	B
e	O
(	O
x	O
)	O
=	O
−	O
log	O
˜p	O
(	O
x	O
)	O
)	O
minus	O
the	O
entropy	B
of	O
the	O
system	O
.	O
statistical	O
physics	O
,	O
j	O
(	O
q	O
)	O
is	O
called	O
the	O
variational	B
free	I
energy	I
or	O
the	O
helmholtz	O
free	O
energy.1	O
in	O
another	O
formulation	O
of	O
the	O
objective	O
is	O
as	O
follows	O
:	O
j	O
(	O
q	O
)	O
=e	O
q	O
[	O
log	O
q	O
(	O
x	O
)	O
−	O
log	O
p	O
(	O
x	O
)	O
p	O
(	O
d|x	O
)	O
]	O
=	O
eq	O
[	O
log	O
q	O
(	O
x	O
)	O
−	O
log	O
p	O
(	O
x	O
)	O
−	O
log	O
p	O
(	O
d|x	O
)	O
]	O
=	O
eq	O
[	O
−	O
log	O
p	O
(	O
d|x	O
)	O
]	O
+	O
kl	O
(	O
q	O
(	O
x	O
)	O
||p	O
(	O
x	O
)	O
)	O
(	O
21.11	O
)	O
(	O
21.12	O
)	O
(	O
21.13	O
)	O
this	O
is	O
the	O
expected	O
nll	O
,	O
plus	O
a	O
penalty	O
term	O
that	O
measures	O
how	O
far	O
the	O
approximate	O
posterior	O
is	O
from	O
the	O
exact	O
prior	O
.	O
we	O
can	O
also	O
interpret	O
the	O
variational	O
objective	O
from	O
the	O
point	O
of	O
view	O
of	O
information	B
theory	I
(	O
the	O
so-called	O
bits-back	B
argument	O
)	O
.	O
see	O
(	O
hinton	O
and	O
camp	O
1993	O
;	O
honkela	O
and	O
valpola	O
2004	O
)	O
,	O
for	O
details	O
.	O
21.2.2	O
forward	O
or	O
reverse	O
kl	O
?	O
*	O
since	O
the	O
kl	O
divergence	O
is	O
not	O
symmetric	O
in	O
its	O
arguments	O
,	O
minimizing	O
kl	O
(	O
q||p	O
)	O
wrt	O
q	O
will	O
give	O
different	O
behavior	O
than	O
minimizing	O
kl	O
(	O
p||q	O
)	O
.	O
below	O
we	O
discuss	O
these	O
two	O
different	O
methods	O
.	O
first	O
,	O
consider	O
the	O
reverse	O
kl	O
,	O
kl	O
(	O
q||p	O
)	O
,	O
also	O
known	O
as	O
an	O
i-projection	O
or	O
information	B
projection	I
.	O
by	O
deﬁnition	O
,	O
we	O
have	O
kl	O
(	O
q||p	O
)	O
=	O
q	O
(	O
x	O
)	O
ln	O
q	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
(	O
21.14	O
)	O
(	O
cid:4	O
)	O
x	O
(	O
cid:4	O
)	O
x	O
this	O
is	O
inﬁnite	O
if	O
p	O
(	O
x	O
)	O
=	O
0	O
and	O
q	O
(	O
x	O
)	O
>	O
0.	O
thus	O
if	O
p	O
(	O
x	O
)	O
=	O
0	O
we	O
must	O
ensure	O
q	O
(	O
x	O
)	O
=	O
0.	O
we	O
say	O
that	O
the	O
reverse	O
kl	O
is	O
zero	B
forcing	I
for	O
q.	O
hence	O
q	O
will	O
typically	O
under-estimate	O
the	O
support	B
of	O
p.	O
now	O
consider	O
the	O
forwards	O
kl	O
,	O
also	O
known	O
as	O
an	O
m-projection	O
or	O
moment	B
projection	I
:	O
kl	O
(	O
p||q	O
)	O
=	O
p	O
(	O
x	O
)	O
ln	O
(	O
21.15	O
)	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
this	O
is	O
inﬁnite	O
if	O
q	O
(	O
x	O
)	O
=	O
0	O
and	O
p	O
(	O
x	O
)	O
>	O
0.	O
so	O
if	O
p	O
(	O
x	O
)	O
>	O
0	O
we	O
must	O
ensure	O
q	O
(	O
x	O
)	O
>	O
0.	O
we	O
say	O
that	O
the	O
forwards	O
kl	O
is	O
zero	B
avoiding	I
for	O
q.	O
hence	O
q	O
will	O
typically	O
over-estimate	O
the	O
support	B
of	O
p.	O
the	O
difference	O
between	O
these	O
methods	O
is	O
illustrated	O
in	O
figure	O
21.1.	O
we	O
see	O
that	O
when	O
the	O
true	O
distribution	O
is	O
multimodal	O
,	O
using	O
the	O
forwards	O
kl	O
is	O
a	O
bad	O
idea	O
(	O
assuming	O
q	O
is	O
constrained	O
to	O
be	O
unimodal	O
)	O
,	O
since	O
the	O
resulting	O
posterior	O
mode/mean	O
will	O
be	O
in	O
a	O
region	O
of	O
low	O
density	O
,	O
right	O
between	O
the	O
two	O
peaks	O
.	O
in	O
such	O
contexts	O
,	O
the	O
reverse	O
kl	O
is	O
not	O
only	O
more	O
tractable	O
to	O
compute	O
,	O
but	O
also	O
more	O
sensible	O
statistically	O
.	O
734	O
chapter	O
21.	O
variational	B
inference	I
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
21.1	O
illustrating	O
forwards	O
vs	O
reverse	O
kl	O
on	O
a	O
bimodal	O
distribution	O
.	O
the	O
blue	O
curves	O
are	O
the	O
contours	O
of	O
the	O
true	O
distribution	O
p.	O
the	O
red	O
curves	O
are	O
the	O
contours	O
of	O
the	O
unimodal	O
approximation	O
q	O
.	O
(	O
a	O
)	O
minimizing	O
forwards	O
kl	O
:	O
q	O
tends	O
to	O
“	O
cover	O
”	O
p.	O
(	O
b-c	O
)	O
minimizing	O
reverse	O
kl	O
:	O
q	O
locks	O
on	O
to	O
one	O
of	O
the	O
two	O
modes	O
.	O
based	O
on	O
figure	O
10.3	O
of	O
(	O
bishop	O
2006b	O
)	O
.	O
figure	O
generated	O
by	O
klfwdreversemixgauss	O
.	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−0.6	O
−0.8	O
−1	O
−1	O
−0.8	O
−0.6	O
−0.4	O
−0.2	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−0.6	O
−0.8	O
−1	O
−1	O
−0.8	O
−0.6	O
−0.4	O
−0.2	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
21.2	O
illustrating	O
forwards	O
vs	O
reverse	O
kl	O
on	O
a	O
symmetric	B
gaussian	O
.	O
the	O
blue	O
curves	O
are	O
the	O
contours	O
of	O
the	O
true	O
distribution	O
p.	O
the	O
red	O
curves	O
are	O
the	O
contours	O
of	O
a	O
factorized	O
approximation	O
q	O
.	O
(	O
a	O
)	O
minimizing	O
kl	O
(	O
q||p	O
)	O
.	O
(	O
b	O
)	O
minimizing	O
kl	O
(	O
p||q	O
)	O
.	O
based	O
on	O
figure	O
10.2	O
of	O
(	O
bishop	O
2006b	O
)	O
.	O
figure	O
generated	O
by	O
klpqgauss	O
.	O
μ	O
=	O
μ1	O
μ2	O
λ11	O
λ12	O
λ21	O
λ22	O
another	O
example	O
of	O
the	O
difference	O
is	O
shown	O
in	O
figure	O
21.2	O
,	O
where	O
the	O
target	O
distribution	O
is	O
(	O
cid:9	O
)	O
an	O
elongated	O
2d	O
gaussian	O
and	O
the	O
approximating	O
distribution	O
is	O
a	O
product	O
of	O
two	O
1d	O
gaussians	O
.	O
that	O
is	O
,	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x|μ	O
,	O
λ−1	O
)	O
,	O
where	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
,	O
λ	O
=	O
(	O
21.16	O
)	O
in	O
figure	O
21.2	O
(	O
a	O
)	O
we	O
show	O
the	O
result	O
of	O
minimizing	O
kl	O
(	O
q||p	O
)	O
.	O
in	O
this	O
simple	O
example	O
,	O
one	O
can	O
show	O
that	O
the	O
solution	O
has	O
the	O
form	O
q	O
(	O
x	O
)	O
=n	O
(	O
x1|m1	O
,	O
λ	O
m1	O
=	O
μ1	O
−	O
λ	O
m2	O
=	O
μ2	O
−	O
λ	O
11	O
λ12	O
(	O
m2	O
−	O
μ2	O
)	O
−1	O
22	O
λ21	O
(	O
m1	O
−	O
μ1	O
)	O
−1	O
11	O
)	O
n	O
(	O
x2|m2	O
,	O
λ	O
−1	O
−1	O
22	O
)	O
(	O
21.17	O
)	O
(	O
21.18	O
)	O
(	O
21.19	O
)	O
1.	O
it	O
is	O
called	O
“	O
free	O
”	O
because	O
the	O
variables	O
x	O
are	O
free	O
to	O
vary	O
,	O
rather	O
than	O
being	O
ﬁxed	O
.	O
the	O
variational	B
free	I
energy	I
is	O
a	O
function	O
of	O
the	O
distribution	O
q	O
,	O
whereas	O
the	O
regular	B
energy	O
is	O
a	O
function	O
of	O
the	O
state	B
vector	O
x	O
.	O
21.3.	O
the	O
mean	B
ﬁeld	I
method	O
735	O
its	O
variance	B
is	O
controlled	O
by	O
the	O
direction	O
of	O
smallest	O
variance	B
of	O
p.	O
figure	O
21.2	O
(	O
a	O
)	O
shows	O
that	O
we	O
have	O
correctly	O
captured	O
the	O
mean	B
,	O
but	O
the	O
approximation	O
is	O
too	O
compact	O
:	O
in	O
fact	O
,	O
it	O
is	O
often	O
the	O
case	O
(	O
although	O
not	O
always	O
(	O
turner	O
et	O
al	O
.	O
2008	O
)	O
)	O
that	O
minimizing	O
kl	O
(	O
q||p	O
)	O
,	O
where	O
q	O
is	O
factorized	O
,	O
results	O
in	O
an	O
approximation	O
that	O
is	O
overconﬁdent	O
.	O
in	O
figure	O
21.2	O
(	O
b	O
)	O
,	O
we	O
show	O
the	O
result	O
of	O
minimizing	O
kl	O
(	O
p||q	O
)	O
.	O
as	O
we	O
show	O
in	O
exercise	O
21.7	O
,	O
the	O
optimal	O
solution	O
when	O
minimizing	O
the	O
forward	O
kl	O
wrt	O
a	O
factored	O
approximation	O
is	O
to	O
set	O
q	O
to	O
be	O
the	O
product	O
of	O
marginals	O
.	O
thus	O
the	O
solution	O
has	O
the	O
form	O
q	O
(	O
x	O
)	O
=	O
n	O
(	O
x1|μ1	O
,	O
λ	O
11	O
)	O
n	O
(	O
x2|μ2	O
,	O
λ	O
−1	O
−1	O
22	O
)	O
(	O
21.20	O
)	O
figure	O
21.2	O
(	O
b	O
)	O
shows	O
that	O
this	O
is	O
too	O
broad	O
,	O
since	O
it	O
is	O
an	O
over-estimate	O
of	O
the	O
support	B
of	O
p.	O
for	O
the	O
rest	O
of	O
this	O
chapter	O
,	O
and	O
for	O
most	O
of	O
the	O
next	O
,	O
we	O
will	O
focus	O
on	O
minimizing	O
kl	O
(	O
q||p	O
)	O
.	O
in	O
section	O
22.5	O
,	O
when	O
we	O
discuss	O
expectation	B
proagation	I
,	O
we	O
will	O
discuss	O
ways	O
to	O
locally	O
optimize	O
kl	O
(	O
p||q	O
)	O
.	O
one	O
can	O
create	O
a	O
family	B
of	O
divergence	O
measures	O
indexed	O
by	O
a	O
parameter	B
α	O
∈	O
r	O
by	O
deﬁning	O
the	O
alpha	B
divergence	I
as	O
follows	O
:	O
(	O
cid:8	O
)	O
(	O
cid:12	O
)	O
(	O
cid:9	O
)	O
dα	O
(	O
p||q	O
)	O
(	O
cid:2	O
)	O
4	O
1	O
−	O
α2	O
1	O
−	O
p	O
(	O
x	O
)	O
(	O
1+α	O
)	O
/2q	O
(	O
x	O
)	O
(	O
1−α	O
)	O
/2dx	O
(	O
21.21	O
)	O
this	O
measure	O
satisﬁes	O
dα	O
(	O
p||q	O
)	O
=	O
0	O
iff	B
p	O
=	O
q	O
,	O
but	O
is	O
obviously	O
not	O
symmetric	O
,	O
and	O
hence	O
is	O
not	O
a	O
metric	B
.	O
kl	O
(	O
p||q	O
)	O
corresponds	O
to	O
the	O
limit	O
α	O
→	O
1	O
,	O
whereas	O
kl	O
(	O
q||p	O
)	O
corresponds	O
to	O
the	O
limit	O
α	O
→	O
−1	O
.	O
when	O
α	O
=	O
0	O
,	O
we	O
get	O
a	O
symmetric	B
divergence	O
measure	O
that	O
is	O
linearly	O
related	O
to	O
the	O
hellinger	O
distance	O
,	O
deﬁned	O
by	O
2	O
−	O
q	O
(	O
x	O
)	O
(	O
cid:12	O
)	O
!	O
#	O
2	O
(	O
21.22	O
)	O
1	O
p	O
(	O
x	O
)	O
1	O
2	O
dx	O
dh	O
(	O
p||q	O
)	O
is	O
a	O
valid	O
distance	O
metric	O
,	O
that	O
is	O
,	O
note	O
that	O
satisﬁes	O
the	O
triangle	B
inequality	I
.	O
see	O
(	O
minka	O
2005	O
)	O
for	O
details	O
.	O
it	O
is	O
symmetric	B
,	O
non-negative	O
and	O
dh	O
(	O
p||q	O
)	O
(	O
cid:2	O
)	O
''	O
21.3	O
the	O
mean	B
ﬁeld	I
method	O
one	O
of	O
the	O
most	O
popular	O
forms	O
of	O
variational	B
inference	I
is	O
called	O
the	O
mean	B
ﬁeld	I
approxima-	O
tion	O
(	O
opper	O
and	O
saad	O
2001	O
)	O
.	O
in	O
this	O
approach	O
,	O
we	O
assume	O
the	O
posterior	O
is	O
a	O
fully	O
factorized	O
approximation	O
of	O
the	O
form	O
(	O
cid:20	O
)	O
q	O
(	O
x	O
)	O
=	O
qi	O
(	O
xi	O
)	O
i	O
our	O
goal	O
is	O
to	O
solve	O
this	O
optimization	B
problem	O
:	O
kl	O
(	O
q||p	O
)	O
min	O
q1	O
,	O
...	O
,	O
qd	O
where	O
we	O
optimize	O
over	O
the	O
parameters	O
of	O
each	O
marginal	B
distribution	I
qi	O
.	O
derive	O
a	O
coordinate	O
descent	O
method	O
,	O
where	O
at	O
each	O
step	O
we	O
make	O
the	O
following	O
update	O
:	O
in	O
section	O
21.3.1	O
,	O
we	O
log	O
qj	O
(	O
xj	O
)	O
=	O
e−qj	O
[	O
log	O
˜p	O
(	O
x	O
)	O
]	O
+	O
const	O
(	O
21.25	O
)	O
(	O
21.23	O
)	O
(	O
21.24	O
)	O
736	O
chapter	O
21.	O
variational	B
inference	I
model	O
ising	O
model	O
factorial	O
hmm	O
univariate	O
gaussian	O
linear	B
regression	I
logistic	O
regression	B
mixtures	O
of	O
gaussians	O
latent	B
dirichlet	O
allocation	O
section	O
section	O
21.3.2	O
section	O
21.4.1	O
section	O
21.5.1	O
section	O
21.5.2	O
section	O
21.8.1.1	O
section	O
21.6.1	O
section	O
27.3.6.3	O
table	O
21.1	O
algorithm	O
.	O
some	O
models	O
in	O
this	O
book	O
for	O
which	O
we	O
provide	O
detailed	O
derivations	O
of	O
the	O
mean	B
ﬁeld	I
inference	O
where	O
˜p	O
(	O
x	O
)	O
=p	O
(	O
x	O
,	O
d	O
)	O
is	O
the	O
unnormalized	O
posterior	O
and	O
the	O
notation	O
e−qj	O
[	O
f	O
(	O
x	O
)	O
]	O
means	O
to	O
take	O
the	O
expectation	O
over	O
f	O
(	O
x	O
)	O
with	O
respect	O
to	O
all	O
the	O
variables	O
except	O
for	O
xj	O
.	O
for	O
example	O
,	O
if	O
we	O
have	O
three	O
variables	O
,	O
then	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
e−q2	O
[	O
f	O
(	O
x	O
)	O
]	O
=	O
q	O
(	O
x1	O
)	O
q3	O
(	O
x3	O
)	O
f	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
(	O
21.26	O
)	O
x1	O
x3	O
where	O
sums	O
get	O
replaced	O
by	O
integrals	O
where	O
necessary	O
.	O
when	O
updating	O
qj	O
,	O
we	O
only	O
need	O
to	O
reason	O
about	O
the	O
variables	O
which	O
share	O
a	O
factor	B
with	O
xj	O
,	O
i.e.	O
,	O
the	O
terms	O
in	O
j	O
’	O
s	O
markov	O
blanket	O
(	O
see	O
section	O
10.5.3	O
)	O
;	O
the	O
other	O
terms	O
get	O
absorbed	O
into	O
the	O
constant	O
term	O
.	O
since	O
we	O
are	O
replacing	O
the	O
neighboring	O
values	O
by	O
their	O
mean	B
value	O
,	O
the	O
method	O
is	O
known	O
as	O
mean	B
ﬁeld	I
.	O
this	O
is	O
very	O
similar	B
to	O
gibbs	O
sampling	O
(	O
section	O
24.2	O
)	O
,	O
except	O
instead	O
of	O
sending	O
sampled	O
values	O
between	O
neighboring	O
nodes	B
,	O
we	O
send	O
mean	B
values	O
between	O
nodes	B
.	O
this	O
tends	O
to	O
be	O
more	O
efficient	O
,	O
since	O
the	O
mean	B
can	O
be	O
used	O
as	O
a	O
proxy	O
for	O
a	O
large	O
number	O
of	O
samples	B
.	O
(	O
on	O
the	O
other	O
hand	O
,	O
mean	B
ﬁeld	I
messages	O
are	O
dense	O
,	O
whereas	O
samples	B
are	O
sparse	B
;	O
this	O
can	O
make	O
sampling	O
more	O
scalable	O
to	O
very	O
large	O
models	O
.	O
)	O
of	O
course	O
,	O
updating	O
one	O
distribution	O
at	O
a	O
time	O
can	O
be	O
slow	O
,	O
since	O
it	O
is	O
a	O
form	O
of	O
coordinate	O
descent	O
.	O
several	O
methods	O
have	O
been	O
proposed	O
to	O
speed	O
up	O
this	O
basic	O
approach	O
,	O
including	O
using	O
pattern	B
search	I
(	O
honkela	O
et	O
al	O
.	O
2003	O
)	O
,	O
and	O
techniques	O
based	O
on	O
parameter	B
expansion	I
(	O
qi	O
and	O
jaakkola	O
2008	O
)	O
.	O
however	O
,	O
we	O
will	O
not	O
consider	O
these	O
methods	O
in	O
this	O
chapter	O
.	O
it	O
is	O
important	O
to	O
note	O
that	O
the	O
mean	B
ﬁeld	I
method	O
can	O
be	O
used	O
to	O
infer	O
discrete	B
or	O
continuous	O
latent	B
quantities	O
,	O
using	O
a	O
variety	O
of	O
parametric	O
forms	O
for	O
qi	O
,	O
as	O
we	O
will	O
see	O
below	O
.	O
this	O
is	O
in	O
contrast	O
to	O
some	O
of	O
the	O
other	O
variational	O
methods	O
we	O
will	O
encounter	O
later	O
,	O
which	O
are	O
more	O
restricted	O
in	O
their	O
applicability	O
.	O
table	O
21.1	O
lists	O
some	O
of	O
the	O
examples	O
of	O
mean	B
ﬁeld	I
that	O
we	O
cover	O
in	O
this	O
book	O
.	O
21.3.1	O
derivation	O
of	O
the	O
mean	B
ﬁeld	I
update	O
equations	O
recall	B
that	O
the	O
goal	O
of	O
variational	B
inference	I
is	O
to	O
minimize	O
the	O
upper	O
bound	O
j	O
(	O
q	O
)	O
≥	O
−	O
log	O
p	O
(	O
d	O
)	O
.	O
equivalently	O
,	O
we	O
can	O
try	O
to	O
maximize	O
the	O
lower	O
bound	O
l	O
(	O
q	O
)	O
(	O
cid:2	O
)	O
−j	O
(	O
q	O
)	O
=	O
q	O
(	O
x	O
)	O
log	O
˜p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
≤	O
log	O
p	O
(	O
d	O
)	O
(	O
21.27	O
)	O
(	O
cid:4	O
)	O
x	O
we	O
will	O
do	O
this	O
one	O
term	O
at	O
a	O
time	O
.	O
21.3.	O
the	O
mean	B
ﬁeld	I
method	O
737	O
if	O
we	O
write	O
the	O
objective	O
singling	O
out	O
the	O
terms	O
that	O
involve	O
qj	O
,	O
and	O
regarding	O
all	O
the	O
other	O
terms	O
as	O
constants	O
,	O
we	O
get	O
l	O
(	O
qj	O
)	O
=	O
=	O
=	O
(	O
cid:20	O
)	O
(	O
cid:4	O
)	O
i	O
x	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
−	O
(	O
cid:4	O
)	O
xj	O
xj	O
xj	O
x−j	O
log	O
qk	O
(	O
xk	O
)	O
(	O
cid:25	O
)	O
(	O
cid:4	O
)	O
k	O
(	O
cid:4	O
)	O
k	O
qi	O
(	O
xi	O
)	O
qj	O
(	O
xj	O
)	O
(	O
cid:24	O
)	O
log	O
˜p	O
(	O
x	O
)	O
−	O
(	O
cid:24	O
)	O
(	O
cid:20	O
)	O
(	O
cid:4	O
)	O
(	O
cid:20	O
)	O
(	O
cid:4	O
)	O
(	O
cid:20	O
)	O
qi	O
(	O
xi	O
)	O
i	O
(	O
cid:5	O
)	O
=j	O
i	O
(	O
cid:5	O
)	O
=j	O
x−j	O
log	O
˜p	O
(	O
x	O
)	O
−	O
⎡	O
⎣	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
k	O
(	O
cid:5	O
)	O
=j	O
qj	O
(	O
xj	O
)	O
qi	O
(	O
xi	O
)	O
log	O
˜p	O
(	O
x	O
)	O
log	O
qk	O
(	O
xk	O
)	O
(	O
cid:25	O
)	O
⎤	O
⎦	O
qj	O
(	O
xj	O
)	O
qi	O
(	O
xi	O
)	O
log	O
qk	O
(	O
xk	O
)	O
+q	O
j	O
(	O
xj	O
)	O
qj	O
(	O
xj	O
)	O
log	O
qj	O
(	O
xj	O
)	O
+	O
const	O
=	O
xj	O
where	O
log	O
fj	O
(	O
xj	O
)	O
(	O
cid:2	O
)	O
x−j	O
i	O
(	O
cid:5	O
)	O
=j	O
qj	O
(	O
xj	O
)	O
log	O
fj	O
(	O
xj	O
)	O
−	O
(	O
cid:4	O
)	O
(	O
cid:20	O
)	O
x−j	O
i	O
(	O
cid:5	O
)	O
=j	O
l	O
(	O
qj	O
)	O
=	O
−kl	O
(	O
qj||fj	O
)	O
(	O
cid:22	O
)	O
qj	O
(	O
xj	O
)	O
=	O
1	O
zj	O
exp	O
e−qj	O
[	O
log	O
˜p	O
(	O
x	O
)	O
]	O
xj	O
(	O
cid:23	O
)	O
(	O
21.28	O
)	O
(	O
21.29	O
)	O
(	O
21.30	O
)	O
(	O
21.31	O
)	O
(	O
21.32	O
)	O
(	O
21.33	O
)	O
(	O
21.34	O
)	O
qi	O
(	O
xi	O
)	O
log	O
˜p	O
(	O
x	O
)	O
=	O
e−qj	O
[	O
log	O
˜p	O
(	O
x	O
)	O
]	O
so	O
we	O
average	O
out	O
all	O
the	O
hidden	B
variables	I
except	O
for	O
xj	O
.	O
thus	O
we	O
can	O
rewrite	O
l	O
(	O
qj	O
)	O
as	O
follows	O
:	O
we	O
can	O
maximize	O
l	O
by	O
minimizing	O
this	O
kl	O
,	O
which	O
we	O
can	O
do	O
by	O
setting	O
qj	O
=	O
fj	O
,	O
as	O
follows	O
:	O
we	O
can	O
usually	O
ignore	O
the	O
local	O
normalization	O
constant	O
zj	O
,	O
since	O
we	O
know	O
qj	O
must	O
be	O
a	O
normalized	O
distribution	O
.	O
hence	O
we	O
usually	O
work	O
with	O
the	O
form	O
log	O
qj	O
(	O
xj	O
)	O
=	O
e−qj	O
[	O
log	O
˜p	O
(	O
x	O
)	O
]	O
+	O
const	O
(	O
21.35	O
)	O
the	O
functional	O
form	O
of	O
the	O
qj	O
distributions	O
will	O
be	O
determined	O
by	O
the	O
type	O
of	O
variables	O
xj	O
,	O
as	O
if	O
xj	O
is	O
a	O
well	O
as	O
the	O
form	O
of	O
the	O
model	O
.	O
discrete	B
random	I
variable	I
,	O
then	O
qj	O
will	O
be	O
a	O
discrete	B
distribution	O
;	O
if	O
xj	O
is	O
a	O
continuous	O
random	O
variable	O
,	O
then	O
qj	O
will	O
be	O
some	O
kind	O
of	O
pdf	B
.	O
we	O
will	O
see	O
examples	O
of	O
this	O
below	O
.	O
(	O
this	O
is	O
sometimes	O
called	O
free-form	B
optimization	I
.	O
)	O
21.3.2	O
example	O
:	O
mean	B
ﬁeld	I
for	O
the	O
ising	O
model	O
consider	O
the	O
image	B
denoising	I
example	O
from	O
section	O
19.4.1	O
,	O
where	O
xi	O
∈	O
{	O
−1	O
,	O
+1	O
}	O
are	O
the	O
hidden	B
pixel	O
values	O
of	O
the	O
“	O
clean	O
”	O
image	O
.	O
we	O
have	O
a	O
joint	O
model	O
of	O
the	O
form	O
p	O
(	O
x	O
,	O
y	O
)	O
=p	O
(	O
x	O
)	O
p	O
(	O
y|x	O
)	O
(	O
21.36	O
)	O
738	O
chapter	O
21.	O
variational	B
inference	I
where	O
the	O
prior	O
has	O
the	O
form	O
1	O
z0	O
p	O
(	O
x	O
)	O
=	O
e0	O
(	O
x	O
)	O
=−	O
d	O
(	O
cid:4	O
)	O
exp	O
(	O
−e0	O
(	O
x	O
)	O
)	O
(	O
cid:4	O
)	O
wijxixj	O
i=1	O
j∈nbri	O
(	O
cid:20	O
)	O
(	O
cid:4	O
)	O
and	O
the	O
likelihood	B
has	O
the	O
form	O
p	O
(	O
y|x	O
)	O
=	O
p	O
(	O
yi|xi	O
)	O
=	O
exp	O
(	O
−li	O
(	O
xi	O
)	O
)	O
i	O
i	O
therefore	O
the	O
posterior	O
has	O
the	O
form	O
p	O
(	O
x|y	O
)	O
=	O
e	O
(	O
x	O
)	O
=e	O
0	O
(	O
x	O
)	O
−	O
1	O
z	O
(	O
cid:4	O
)	O
exp	O
(	O
−e	O
(	O
x	O
)	O
)	O
li	O
(	O
xi	O
)	O
i	O
(	O
cid:20	O
)	O
i	O
q	O
(	O
x	O
)	O
=	O
q	O
(	O
xi	O
,	O
μi	O
)	O
we	O
will	O
now	O
approximate	O
this	O
by	O
a	O
fully	O
factored	O
approximation	O
(	O
21.37	O
)	O
(	O
21.38	O
)	O
(	O
21.39	O
)	O
(	O
21.40	O
)	O
(	O
21.41	O
)	O
(	O
21.42	O
)	O
(	O
21.44	O
)	O
(	O
21.45	O
)	O
where	O
μi	O
is	O
the	O
mean	B
value	O
of	O
node	O
i.	O
to	O
derive	O
the	O
update	O
for	O
the	O
variational	O
parameter	O
μi	O
,	O
we	O
ﬁrst	O
write	O
out	O
log	O
˜p	O
(	O
x	O
)	O
=	O
−e	O
(	O
x	O
)	O
,	O
dropping	O
terms	O
that	O
do	O
not	O
involve	O
xi	O
:	O
log	O
˜p	O
(	O
x	O
)	O
=	O
xi	O
wijxj	O
+	O
li	O
(	O
xi	O
)	O
+	O
const	O
(	O
21.43	O
)	O
this	O
only	O
depends	O
on	O
the	O
states	O
of	O
the	O
neighboring	O
nodes	B
.	O
now	O
we	O
take	O
expectations	O
of	O
this	O
wrt	O
(	O
cid:4	O
)	O
j∈nbri	O
⎛	O
⎝xi	O
(	O
cid:26	O
)	O
j	O
(	O
cid:5	O
)	O
=i	O
qj	O
(	O
xj	O
)	O
to	O
get	O
qi	O
(	O
xi	O
)	O
∝	O
exp	O
(	O
cid:4	O
)	O
mi	O
=	O
j∈nbri	O
wijμj	O
(	O
cid:4	O
)	O
j∈nbri	O
wijμj	O
+	O
li	O
(	O
xi	O
)	O
⎞	O
⎠	O
thus	O
we	O
replace	O
the	O
states	O
of	O
the	O
neighbors	B
by	O
their	O
average	O
values	O
.	O
let	O
be	O
the	O
mean	B
ﬁeld	I
inﬂuence	O
on	O
node	O
i.	O
also	O
,	O
approximate	O
marginal	O
posterior	O
is	O
given	O
by	O
let	O
l+	O
i	O
(	O
cid:2	O
)	O
li	O
(	O
+1	O
)	O
and	O
l−	O
i	O
(	O
cid:2	O
)	O
li	O
(	O
−1	O
)	O
.	O
the	O
qi	O
(	O
xi	O
=	O
1	O
)	O
=	O
emi+l+	O
ai	O
(	O
cid:2	O
)	O
mi	O
+	O
0.5	O
(	O
l+	O
i	O
emi+l+	O
i	O
+	O
e−mi+l−	O
i	O
−	O
l−	O
i	O
)	O
i	O
=	O
1	O
1	O
+	O
e−2mi+l−	O
i	O
−l+	O
i	O
=	O
sigm	O
(	O
2ai	O
)	O
(	O
21.46	O
)	O
(	O
21.47	O
)	O
21.4.	O
structured	B
mean	I
ﬁeld	I
*	O
739	O
sample	O
1	O
,	O
meanfieldh	O
sample	O
3	O
,	O
meanfieldh	O
mean	B
after	O
15	O
sweeps	O
of	O
meanfieldh	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−0.6	O
−0.8	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−0.6	O
−0.8	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−0.2	O
−0.4	O
−0.6	O
−0.8	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
21.3	O
example	O
of	O
image	B
denoising	I
using	O
mean	B
ﬁeld	I
(	O
with	O
parallel	O
updates	O
and	O
a	O
damping	B
factor	O
of	O
0.5	O
)	O
.	O
we	O
use	O
an	O
ising	O
prior	O
with	O
wij	O
=	O
1	O
and	O
a	O
gaussian	O
noise	O
model	O
with	O
σ	O
=	O
2.	O
we	O
show	O
the	O
results	O
after	O
1	O
,	O
3	O
and	O
15	O
iterations	O
across	O
the	O
image	O
.	O
compare	O
to	O
figure	O
24.1.	O
figure	O
generated	O
by	O
isingimagedenoisedemo	O
.	O
similarly	O
,	O
we	O
have	O
qi	O
(	O
xi	O
=	O
−1	O
)	O
=	O
sigm	O
(	O
−2ai	O
)	O
.	O
from	O
this	O
we	O
can	O
compute	O
the	O
new	O
mean	B
for	O
site	O
i	O
:	O
μi	O
=	O
eqi	O
[	O
xi	O
]	O
=	O
qi	O
(	O
xi	O
=	O
+1	O
)	O
·	O
(	O
+1	O
)	O
+	O
qi	O
(	O
xi	O
=	O
−1	O
)	O
·	O
(	O
−1	O
)	O
=	O
−	O
1	O
1	O
+	O
e2ai	O
=	O
hence	O
the	O
update	O
equation	O
becomes	O
μi	O
=	O
tanh	O
wijμj	O
+	O
0.5	O
(	O
l+	O
eai	O
eai	O
+	O
e−ai	O
⎞	O
⎠	O
i	O
−	O
l−	O
i	O
)	O
−	O
e−ai	O
e−ai	O
+	O
eai	O
=	O
tanh	O
(	O
ai	O
)	O
1	O
1	O
+	O
e−2ai	O
⎛	O
⎝	O
(	O
cid:4	O
)	O
j∈nbri	O
⎛	O
⎝	O
(	O
cid:4	O
)	O
j∈nbri	O
(	O
21.48	O
)	O
(	O
21.49	O
)	O
(	O
21.50	O
)	O
(	O
21.51	O
)	O
(	O
21.52	O
)	O
see	O
also	O
exercise	O
21.6	O
for	O
an	O
alternative	O
derivation	O
of	O
these	O
equations	O
.	O
we	O
can	O
turn	O
the	O
above	O
equations	O
in	O
to	O
a	O
ﬁxed	B
point	I
algorithm	O
by	O
writing	O
μt	O
i	O
=	O
tanh	O
wijμt−1	O
j	O
+	O
0.5	O
(	O
l+	O
i	O
−	O
l−	O
i	O
)	O
⎞	O
⎠	O
it	O
is	O
usually	O
better	O
to	O
use	O
damped	B
updates	I
of	O
the	O
form	O
i	O
=	O
(	O
1	O
−	O
λ	O
)	O
μt−1	O
μt	O
i	O
+	O
λ	O
tanh	O
wijμt−1	O
j	O
+	O
0.5	O
(	O
l+	O
i	O
−	O
l−	O
i	O
)	O
⎞	O
⎠	O
⎛	O
⎝	O
(	O
cid:4	O
)	O
j∈nbri	O
for	O
0	O
<	O
λ	O
<	O
1.	O
we	O
can	O
update	O
all	O
the	O
nodes	B
in	O
parallel	O
,	O
or	O
update	O
them	O
asychronously	O
.	O
figure	O
21.3	O
shows	O
the	O
method	O
in	O
action	B
,	O
applied	O
to	O
a	O
2d	O
ising	O
model	O
with	O
homogeneous	B
attractive	O
potentials	O
,	O
wij	O
=	O
1.	O
we	O
use	O
parallel	O
updates	O
with	O
a	O
damping	B
factor	O
of	O
λ	O
=	O
0.5	O
.	O
(	O
if	O
we	O
don	O
’	O
t	O
use	O
damping	B
,	O
we	O
tend	O
to	O
get	O
“	O
checkerboard	O
”	O
artefacts	O
.	O
)	O
21.4	O
structured	B
mean	I
ﬁeld	I
*	O
assuming	O
that	O
all	O
the	O
variables	O
are	O
independent	O
in	O
the	O
posterior	O
is	O
a	O
very	O
strong	O
assumption	O
that	O
can	O
lead	O
to	O
poor	O
results	O
.	O
sometimes	O
we	O
can	O
exploit	O
tractable	B
substructure	I
in	O
our	O
problem	O
,	O
so	O
740	O
chapter	O
21.	O
variational	B
inference	I
x1,1	O
x1,2	O
x1,3	O
x2,1	O
x2,2	O
x2,3	O
x3,1	O
x3,2	O
x3,3	O
y1	O
y2	O
y3	O
x1,1	O
x1,2	O
x1,3	O
x1,1	O
x1,2	O
x1,3	O
x2,1	O
x2,2	O
x2,3	O
x3,1	O
x3,2	O
x3,3	O
x2,1	O
x2,2	O
x2,3	O
x3,1	O
x3,2	O
x3,3	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
21.4	O
chains	O
approximation	O
.	O
based	O
on	O
figure	O
2	O
of	O
(	O
ghahramani	O
and	O
jordan	O
1997	O
)	O
.	O
(	O
a	O
)	O
a	O
factorial	O
hmm	O
with	O
3	O
chains	O
.	O
(	O
b	O
)	O
a	O
fully	O
factorized	O
approximation	O
.	O
(	O
c	O
)	O
a	O
product-of-	O
that	O
we	O
can	O
efficiently	O
handle	O
some	O
kinds	O
of	O
dependencies	O
.	O
this	O
is	O
called	O
the	O
structured	B
mean	I
ﬁeld	I
approach	O
(	O
saul	O
and	O
jordan	O
1995	O
)	O
.	O
the	O
approach	O
is	O
the	O
same	O
as	O
before	O
,	O
except	O
we	O
group	O
sets	O
of	O
variables	O
together	O
,	O
and	O
we	O
update	O
them	O
simultaneously	O
.	O
(	O
this	O
follows	O
by	O
simply	O
treating	O
all	O
the	O
variables	O
in	O
the	O
i	O
’	O
th	O
group	O
as	O
a	O
single	O
“	O
mega-variable	O
”	O
,	O
and	O
then	O
repeating	O
the	O
derivation	O
in	O
section	O
21.3.1	O
.	O
)	O
as	O
long	O
as	O
we	O
can	O
perform	O
efficient	O
inference	O
in	O
each	O
qi	O
,	O
the	O
method	O
is	O
tractable	O
overall	O
.	O
we	O
give	O
an	O
example	O
below	O
.	O
see	O
(	O
bouchard-cote	O
and	O
jordan	O
2009	O
)	O
for	O
some	O
more	O
recent	O
work	O
in	O
this	O
area	O
.	O
21.4.1	O
example	O
:	O
factorial	O
hmm	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
consider	O
the	O
factorial	O
hmm	O
model	O
(	O
ghahramani	O
and	O
jordan	O
1997	O
)	O
introduced	O
in	O
section	O
17.6.5.	O
suppose	O
there	O
are	O
m	O
chains	O
,	O
each	O
of	O
length	O
t	O
,	O
and	O
suppose	O
each	O
hidden	B
node	O
has	O
k	O
states	O
.	O
the	O
model	O
is	O
deﬁned	O
as	O
follows	O
p	O
(	O
xtm|xt−1	O
,	O
m	O
)	O
p	O
(	O
yt|xtm	O
)	O
p	O
(	O
x	O
,	O
y	O
)	O
=	O
(	O
21.53	O
)	O
where	O
p	O
(	O
xtm	O
=	O
k|xt−1	O
,	O
m	O
=	O
j	O
)	O
=a	O
mjk	O
is	O
an	O
entry	O
in	O
the	O
transition	B
matrix	I
for	O
chain	O
m	O
,	O
p	O
(	O
x1m	O
=	O
k|x0m	O
)	O
=	O
p	O
(	O
x1m	O
=	O
k	O
)	O
=	O
πmk	O
,	O
is	O
the	O
initial	O
state	B
distribution	O
for	O
chain	O
m	O
,	O
and	O
m	O
t	O
(	O
cid:10	O
)	O
yt|	O
m	O
(	O
cid:4	O
)	O
(	O
cid:11	O
)	O
p	O
(	O
yt|xt	O
)	O
=	O
n	O
wmxtm	O
,	O
σ	O
m=1	O
(	O
21.54	O
)	O
is	O
the	O
observation	B
model	I
,	O
where	O
xtm	O
is	O
a	O
1-of-k	O
encoding	O
of	O
xtm	O
and	O
wm	O
is	O
a	O
d	O
×	O
k	O
matrix	O
(	O
assuming	O
yt	O
∈	O
r	O
d	O
)	O
.	O
figure	O
21.4	O
(	O
a	O
)	O
illustrates	O
the	O
model	O
for	O
the	O
case	O
where	O
m	O
=	O
3.	O
even	O
though	O
each	O
chain	O
is	O
a	O
priori	O
independent	O
,	O
they	O
become	O
coupled	O
in	O
the	O
posterior	O
due	O
to	O
having	O
an	O
observed	O
common	O
child	O
,	O
yt	O
.	O
the	O
junction	B
tree	I
algorithm	I
applied	O
to	O
this	O
graph	B
takes	O
o	O
(	O
t	O
m	O
km	O
+1	O
)	O
time	O
.	O
below	O
we	O
will	O
derive	O
a	O
structured	B
mean	I
ﬁeld	I
algorithm	O
that	O
takes	O
o	O
(	O
t	O
m	O
k	O
2i	O
)	O
time	O
,	O
where	O
i	O
is	O
the	O
number	O
of	O
mean	B
ﬁeld	I
iterations	O
(	O
typically	O
i	O
∼	O
10	O
suffices	O
for	O
good	O
performance	O
)	O
.	O
21.4.	O
structured	B
mean	I
ﬁeld	I
*	O
we	O
can	O
write	O
the	O
exact	O
posterior	O
in	O
the	O
following	O
form	O
:	O
p	O
(	O
x|y	O
)	O
=	O
1	O
z	O
(	O
cid:10	O
)	O
exp	O
(	O
−e	O
(	O
x	O
,	O
y	O
)	O
)	O
(	O
cid:4	O
)	O
t	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
1m	O
˜πm	O
−	O
t	O
(	O
cid:4	O
)	O
yt	O
−	O
xt	O
t=1	O
m	O
wmxtm	O
(	O
cid:4	O
)	O
(	O
cid:11	O
)	O
t	O
(	O
cid:10	O
)	O
σ−1	O
yt	O
−	O
xt	O
tm	O
˜amxt−1	O
,	O
m	O
e	O
(	O
x	O
,	O
y	O
)	O
=	O
1	O
2	O
−	O
(	O
cid:11	O
)	O
wmxtm	O
(	O
cid:4	O
)	O
m	O
741	O
(	O
21.55	O
)	O
(	O
21.56	O
)	O
m	O
t=2	O
m	O
where	O
˜am	O
(	O
cid:2	O
)	O
log	O
am	O
and	O
˜πm	O
(	O
cid:2	O
)	O
log	O
πm	O
(	O
both	O
interpreted	O
elementwise	O
)	O
.	O
we	O
can	O
approximate	O
the	O
posterior	O
as	O
a	O
product	O
of	O
marginals	O
,	O
as	O
in	O
figure	O
21.4	O
(	O
b	O
)	O
,	O
but	O
a	O
better	O
approximation	O
is	O
to	O
use	O
a	O
product	O
of	O
chains	O
,	O
as	O
in	O
figure	O
21.4	O
(	O
c	O
)	O
.	O
each	O
chain	O
can	O
be	O
tractably	O
updated	O
individually	O
,	O
using	O
the	O
forwards-backwards	B
algorithm	I
.	O
more	O
precisely	O
,	O
we	O
assume	O
m	O
(	O
cid:20	O
)	O
t	O
(	O
cid:20	O
)	O
q	O
(	O
x1m|ξ1m	O
)	O
q	O
(	O
xtm|xt−1	O
,	O
m	O
,	O
ξtm	O
)	O
q	O
(	O
x|y	O
)	O
=	O
q	O
(	O
x1m|ξ1m	O
)	O
=	O
q	O
(	O
xtm|xt−1	O
,	O
m	O
,	O
ξtm	O
)	O
=	O
1	O
zq	O
k	O
(	O
cid:20	O
)	O
k	O
(	O
cid:20	O
)	O
k=1	O
m=1	O
t=2	O
(	O
ξ1mkπmk	O
)	O
x1mk	O
⎛	O
⎝ξtmk	O
k	O
(	O
cid:20	O
)	O
(	O
amjk	O
)	O
xt−1	O
,	O
m	O
,	O
j	O
⎞	O
⎠xtmk	O
k=1	O
j=1	O
(	O
21.57	O
)	O
(	O
21.58	O
)	O
(	O
21.59	O
)	O
(	O
21.60	O
)	O
(	O
21.61	O
)	O
we	O
see	O
that	O
the	O
ξtmk	O
parameters	O
play	O
the	O
role	O
of	O
an	O
approximate	O
local	O
evidence	B
,	O
averaging	O
out	O
the	O
effects	O
of	O
the	O
other	O
chains	O
.	O
this	O
is	O
contrast	O
to	O
the	O
exact	O
local	B
evidence	I
,	O
which	O
couples	O
all	O
the	O
chains	O
together	O
.	O
we	O
can	O
rewrite	O
the	O
approximate	O
posterior	O
as	O
q	O
(	O
x	O
)	O
=	O
1	O
zq	O
t	O
(	O
cid:4	O
)	O
m	O
(	O
cid:4	O
)	O
tm˜ξtm	O
−	O
m	O
(	O
cid:4	O
)	O
xt	O
1m	O
˜πm	O
−	O
t	O
(	O
cid:4	O
)	O
xt	O
eq	O
(	O
x	O
)	O
=−	O
exp	O
(	O
−eq	O
(	O
x	O
)	O
)	O
,	O
where	O
m	O
(	O
cid:4	O
)	O
xt	O
tm	O
˜amxt−1	O
,	O
m	O
t=1	O
m=1	O
m=1	O
t=2	O
m=1	O
where	O
˜ξtm	O
=	O
log	O
ξtm	O
.	O
we	O
see	O
that	O
this	O
has	O
the	O
same	O
temporal	O
factors	B
as	O
the	O
exact	O
posterior	O
,	O
but	O
the	O
local	B
evidence	I
term	O
is	O
different	O
.	O
the	O
objective	O
function	O
is	O
given	O
by	O
where	O
the	O
expectations	O
are	O
taken	O
wrt	O
q.	O
one	O
can	O
show	O
(	O
exercise	O
21.8	O
)	O
that	O
the	O
update	O
has	O
the	O
form	O
kl	O
(	O
q||p	O
)	O
=e	O
[	O
e	O
]	O
−	O
e	O
[	O
eq	O
]	O
−	O
log	O
zq	O
+	O
log	O
z	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
˜ytm	O
(	O
cid:2	O
)	O
yt	O
−	O
m	O
(	O
cid:4	O
)	O
wt	O
ξtm	O
=	O
exp	O
δm	O
(	O
cid:2	O
)	O
diag	O
(	O
wt	O
mς−1	O
˜ytm	O
−	O
1	O
2	O
mς−1wm	O
)	O
w	O
(	O
cid:8	O
)	O
e	O
[	O
xt	O
,	O
(	O
cid:8	O
)	O
]	O
(	O
cid:8	O
)	O
(	O
cid:5	O
)	O
=m	O
δm	O
(	O
21.62	O
)	O
(	O
21.63	O
)	O
(	O
21.64	O
)	O
742	O
chapter	O
21.	O
variational	B
inference	I
the	O
ξtm	O
parameter	B
plays	O
the	O
role	O
of	O
the	O
local	B
evidence	I
,	O
averaging	O
over	O
the	O
neighboring	O
chains	O
.	O
having	O
computed	O
this	O
for	O
each	O
chain	O
,	O
we	O
can	O
perform	O
forwards-backwards	B
in	O
parallel	O
,	O
using	O
these	O
approximate	O
local	O
evidence	B
terms	O
to	O
compute	O
q	O
(	O
xt	O
,	O
m|y1	O
:	O
t	O
)	O
for	O
each	O
m	O
and	O
t.	O
the	O
update	O
cost	O
is	O
o	O
(	O
t	O
m	O
k	O
2	O
)	O
for	O
a	O
full	B
“	O
sweep	O
”	O
over	O
all	O
the	O
variational	O
parameters	O
,	O
since	O
we	O
have	O
to	O
run	O
forwards-backwards	B
m	O
times	O
,	O
for	O
each	O
chain	O
independently	O
.	O
this	O
is	O
the	O
same	O
cost	O
as	O
a	O
fully	O
factorized	O
approximation	O
,	O
but	O
is	O
much	O
more	O
accurate	O
.	O
21.5	O
variational	O
bayes	O
make	O
a	O
fully	O
factorized	O
(	O
i.e.	O
,	O
mean	B
ﬁeld	I
)	O
approximation	O
,	O
p	O
(	O
θ|d	O
)	O
≈	O
(	O
cid:26	O
)	O
so	O
far	O
we	O
have	O
been	O
concentrating	O
on	O
inferring	O
latent	B
variables	O
zi	O
assuming	O
the	O
parameters	O
θ	O
of	O
the	O
model	O
are	O
known	O
.	O
now	O
suppose	O
we	O
want	O
to	O
infer	O
the	O
parameters	O
themselves	O
.	O
if	O
we	O
k	O
q	O
(	O
θk	O
)	O
,	O
we	O
get	O
a	O
method	O
known	O
as	O
variational	O
bayes	O
or	O
vb	O
(	O
hinton	O
and	O
camp	O
1993	O
;	O
mackay	O
1995a	O
;	O
attias	O
2000	O
;	O
beal	O
and	O
ghahramani	O
2006	O
;	O
smidl	O
and	O
quinn	O
2005	O
)	O
.2	O
we	O
give	O
some	O
examples	O
of	O
vb	O
below	O
,	O
assuming	O
that	O
there	O
are	O
no	O
latent	O
variables	O
.	O
if	O
we	O
want	O
to	O
infer	O
both	O
latent	B
variables	O
and	O
parameters	O
,	O
and	O
we	O
make	O
an	O
approximation	O
of	O
the	O
form	O
p	O
(	O
θ	O
,	O
z1	O
:	O
n|d	O
)	O
≈	O
q	O
(	O
θ	O
)	O
i	O
qi	O
(	O
zi	O
)	O
,	O
we	O
get	O
a	O
method	O
known	O
as	O
variational	O
bayes	O
em	O
,	O
which	O
we	O
described	O
in	O
section	O
21.6	O
.	O
(	O
cid:26	O
)	O
21.5.1	O
example	O
:	O
vb	O
for	O
a	O
univariate	O
gaussian	O
following	O
(	O
mackay	O
2003	O
,	O
p429	O
)	O
,	O
let	O
us	O
consider	O
how	O
to	O
apply	O
vb	O
to	O
infer	O
the	O
posterior	O
over	O
the	O
parameters	O
for	O
a	O
1d	O
gaussian	O
,	O
p	O
(	O
μ	O
,	O
λ|d	O
)	O
,	O
where	O
λ	O
=	O
1/σ2	O
is	O
the	O
precision	B
.	O
for	O
convenience	O
,	O
we	O
will	O
use	O
a	O
conjugate	B
prior	I
of	O
the	O
form	O
p	O
(	O
μ	O
,	O
λ	O
)	O
=	O
n	O
(	O
μ|μ0	O
,	O
(	O
κ0λ	O
)	O
−1	O
)	O
ga	O
(	O
λ|a0	O
,	O
b0	O
)	O
however	O
,	O
we	O
will	O
use	O
an	O
approximate	O
factored	O
posterior	O
of	O
the	O
form	O
q	O
(	O
μ	O
,	O
λ	O
)	O
=	O
qμ	O
(	O
μ	O
)	O
qλ	O
(	O
λ	O
)	O
(	O
21.65	O
)	O
(	O
21.66	O
)	O
we	O
do	O
not	O
need	O
to	O
specify	O
the	O
forms	O
for	O
the	O
distributions	O
qμ	O
and	O
qλ	O
;	O
the	O
optimal	O
forms	O
will	O
“	O
fall	O
out	O
”	O
automatically	O
during	O
the	O
derivation	O
(	O
and	O
conveniently	O
,	O
they	O
turn	O
out	O
to	O
be	O
gaussian	O
and	O
gamma	O
respectively	O
)	O
.	O
you	O
might	O
wonder	O
why	O
we	O
would	O
want	O
to	O
do	O
this	O
,	O
since	O
we	O
know	O
how	O
to	O
compute	O
the	O
exact	O
posterior	O
for	O
this	O
model	O
(	O
section	O
4.6.3.7	O
)	O
.	O
there	O
are	O
two	O
reasons	O
.	O
first	O
,	O
it	O
is	O
a	O
useful	O
pedagogical	O
exercise	O
,	O
since	O
we	O
can	O
compare	O
the	O
quality	O
of	O
our	O
approximation	O
to	O
the	O
exact	O
posterior	O
.	O
second	O
,	O
it	O
is	O
simple	O
to	O
modify	O
the	O
method	O
to	O
handle	O
a	O
semi-conjugate	B
prior	O
of	O
the	O
form	O
p	O
(	O
μ	O
,	O
λ	O
)	O
=	O
n	O
(	O
μ|μ0	O
,	O
τ0	O
)	O
ga	O
(	O
λ|a0	O
,	O
b0	O
)	O
,	O
for	O
which	O
exact	O
inference	B
is	O
no	O
longer	O
possible	O
.	O
2.	O
this	O
method	O
was	O
originally	O
called	O
ensemble	B
learning	I
(	O
mackay	O
1995a	O
)	O
,	O
since	O
we	O
are	O
using	O
an	O
ensemble	B
of	O
parameters	O
(	O
a	O
distribution	O
)	O
instead	O
of	O
a	O
point	B
estimate	I
.	O
however	O
,	O
the	O
term	O
“	O
ensemble	B
learning	I
”	O
is	O
also	O
used	O
to	O
describe	O
methods	O
such	O
as	O
boosting	B
,	O
so	O
we	O
prefer	O
the	O
term	O
vb	O
.	O
21.5.	O
variational	O
bayes	O
21.5.1.1	O
target	O
distribution	O
the	O
unnormalized	O
log	O
posterior	O
has	O
the	O
form	O
log	O
˜p	O
(	O
μ	O
,	O
λ	O
)	O
=	O
log	O
p	O
(	O
μ	O
,	O
λ	O
,	O
d	O
)	O
=	O
log	O
p	O
(	O
d|μ	O
,	O
λ	O
)	O
+	O
log	O
p	O
(	O
μ|λ	O
)	O
+	O
log	O
p	O
(	O
λ	O
)	O
=	O
n	O
2	O
log	O
λ	O
−	O
λ	O
2	O
(	O
xi	O
−	O
μ	O
)	O
2	O
−	O
κ0λ	O
2	O
(	O
μ	O
−	O
μ0	O
)	O
2	O
n	O
(	O
cid:4	O
)	O
i=1	O
log	O
(	O
κ0λ	O
)	O
+	O
(	O
a0	O
−	O
1	O
)	O
log	O
λ	O
−	O
b0λ	O
+	O
const	O
+	O
1	O
2	O
21.5.1.2	O
updating	O
qμ	O
(	O
μ	O
)	O
the	O
optimal	O
form	O
for	O
qμ	O
(	O
μ	O
)	O
is	O
obtained	O
by	O
averaging	O
over	O
λ	O
:	O
4	O
log	O
qμ	O
(	O
μ	O
)	O
=e	O
qλ	O
[	O
log	O
p	O
(	O
d|μ	O
,	O
λ	O
)	O
+	O
log	O
p	O
(	O
μ|λ	O
)	O
]	O
+	O
const	O
(	O
xi	O
−	O
μ	O
)	O
2	O
κ0	O
(	O
μ	O
−	O
μ0	O
)	O
2	O
+	O
=	O
−	O
eqλ	O
[	O
λ	O
]	O
n	O
(	O
cid:4	O
)	O
3	O
2	O
i=1	O
+	O
const	O
by	O
completing	B
the	I
square	I
one	O
can	O
show	O
that	O
qμ	O
(	O
μ	O
)	O
=	O
n	O
(	O
μ|μn	O
,	O
κ−1	O
n	O
)	O
,	O
where	O
μn	O
=	O
κ0μ0	O
+	O
n	O
x	O
κ0	O
+	O
n	O
,	O
κn	O
=	O
(	O
κ0	O
+	O
n	O
)	O
eqλ	O
[	O
λ	O
]	O
743	O
(	O
21.67	O
)	O
(	O
21.68	O
)	O
(	O
21.69	O
)	O
(	O
21.70	O
)	O
(	O
21.71	O
)	O
at	O
this	O
stage	O
we	O
don	O
’	O
t	O
know	O
what	O
qλ	O
(	O
λ	O
)	O
is	O
,	O
and	O
hence	O
we	O
can	O
not	O
compute	O
e	O
[	O
λ	O
]	O
,	O
but	O
we	O
will	O
derive	O
this	O
below	O
.	O
21.5.1.3	O
updating	O
qλ	O
(	O
λ	O
)	O
the	O
optimal	O
form	O
for	O
qλ	O
(	O
λ	O
)	O
is	O
given	O
by	O
log	O
qλ	O
(	O
λ	O
)	O
=e	O
qμ	O
[	O
log	O
p	O
(	O
d|μ	O
,	O
λ	O
)	O
+	O
log	O
p	O
(	O
μ|λ	O
)	O
+	O
log	O
p	O
(	O
λ	O
)	O
]	O
+	O
const	O
=	O
(	O
a0	O
−	O
1	O
)	O
log	O
λ	O
−	O
b0λ	O
+	O
(	O
cid:24	O
)	O
1	O
2	O
κ0	O
(	O
μ	O
−	O
μ0	O
)	O
2	O
+	O
n	O
(	O
cid:4	O
)	O
−	O
λ	O
2	O
eqμ	O
log	O
λ	O
+	O
log	O
λ	O
n	O
2	O
(	O
cid:25	O
)	O
(	O
xi	O
−	O
μ	O
)	O
2	O
+	O
const	O
i=1	O
(	O
21.72	O
)	O
(	O
21.73	O
)	O
we	O
recognize	O
this	O
as	O
the	O
log	O
of	O
a	O
gamma	B
distribution	I
,	O
hence	O
qλ	O
(	O
λ	O
)	O
=	O
ga	O
(	O
λ|an	O
,	O
bn	O
)	O
,	O
where	O
an	O
=	O
a0	O
+	O
bn	O
=	O
b0	O
+	O
n	O
+	O
1	O
2	O
1	O
2	O
eqμ	O
(	O
cid:24	O
)	O
(	O
cid:25	O
)	O
n	O
(	O
cid:4	O
)	O
i=1	O
(	O
xi	O
−	O
μ	O
)	O
2	O
κ0	O
(	O
μ	O
−	O
μ0	O
)	O
2	O
+	O
(	O
21.74	O
)	O
(	O
21.75	O
)	O
744	O
chapter	O
21.	O
variational	B
inference	I
21.5.1.4	O
computing	O
the	O
expectations	O
n	O
)	O
,	O
we	O
have	O
to	O
implement	O
the	O
updates	O
,	O
we	O
have	O
to	O
specify	O
how	O
to	O
compute	O
the	O
various	O
expectations	O
.	O
since	O
q	O
(	O
μ	O
)	O
=	O
n	O
(	O
μ|μn	O
,	O
κ−1	O
eq	O
(	O
μ	O
)	O
[	O
μ	O
]	O
=μ	O
n	O
1	O
κn	O
+	O
μ2	O
n	O
(	O
21.76	O
)	O
eq	O
(	O
μ	O
)	O
μ2	O
(	O
21.77	O
)	O
(	O
cid:13	O
)	O
(	O
cid:14	O
)	O
=	O
since	O
q	O
(	O
λ	O
)	O
=	O
ga	O
(	O
λ|an	O
,	O
bn	O
)	O
,	O
we	O
have	O
eq	O
(	O
λ	O
)	O
[	O
λ	O
]	O
=	O
an	O
bn	O
we	O
can	O
now	O
give	O
explicit	O
forms	O
for	O
the	O
update	O
equations	O
.	O
for	O
q	O
(	O
μ	O
)	O
we	O
have	O
κ0μ0	O
+	O
n	O
x	O
μn	O
=	O
κ0	O
+	O
n	O
κn	O
=	O
(	O
κ0	O
+	O
n	O
)	O
an	O
bn	O
and	O
for	O
q	O
(	O
λ	O
)	O
we	O
have	O
an	O
=	O
a0	O
+	O
n	O
+	O
1	O
2	O
(	O
cid:13	O
)	O
bn	O
=	O
b0	O
+	O
κ0	O
(	O
e	O
μ2	O
(	O
cid:14	O
)	O
0	O
−	O
2e	O
[	O
μ	O
]	O
μ0	O
)	O
+	O
+	O
μ2	O
n	O
(	O
cid:4	O
)	O
(	O
cid:22	O
)	O
i=1	O
1	O
2	O
(	O
cid:13	O
)	O
μ2	O
(	O
cid:14	O
)	O
−	O
2e	O
[	O
μ	O
]	O
xi	O
(	O
cid:23	O
)	O
x2	O
i	O
+	O
e	O
(	O
21.78	O
)	O
(	O
21.79	O
)	O
(	O
21.80	O
)	O
(	O
21.81	O
)	O
(	O
21.82	O
)	O
we	O
see	O
that	O
μn	O
and	O
an	O
are	O
in	O
fact	O
ﬁxed	O
constants	O
,	O
and	O
only	O
κn	O
and	O
bn	O
need	O
to	O
be	O
updated	O
(	O
in	O
fact	O
,	O
one	O
can	O
solve	O
for	O
the	O
ﬁxed	O
points	O
of	O
κn	O
and	O
bn	O
analytically	O
,	O
but	O
we	O
don	O
’	O
t	O
iteratively	O
.	O
do	O
this	O
here	O
in	O
order	O
to	O
illustrate	O
the	O
iterative	O
updating	O
scheme	O
.	O
)	O
21.5.1.5	O
illustration	O
figure	O
21.5	O
gives	O
an	O
example	O
of	O
this	O
method	O
in	O
action	B
.	O
the	O
green	O
contours	O
represent	O
the	O
exact	O
posterior	O
,	O
which	O
is	O
gaussian-gamma	O
.	O
the	O
dotted	O
red	O
contours	O
represent	O
the	O
variational	O
approximation	O
over	O
several	O
iterations	O
.	O
we	O
see	O
that	O
the	O
ﬁnal	O
approximation	O
is	O
reasonably	O
close	O
to	O
the	O
exact	O
solution	O
.	O
however	O
,	O
it	O
is	O
more	O
“	O
compact	O
”	O
than	O
the	O
true	O
distribution	O
.	O
it	O
is	O
often	O
the	O
case	O
that	O
mean	B
ﬁeld	I
inference	O
underestimates	O
the	O
posterior	O
uncertainty	O
;	O
see	O
section	O
21.2.2	O
for	O
more	O
discussion	O
of	O
this	O
point	O
.	O
21.5.1.6	O
lower	O
bound	O
*	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
in	O
vb	O
,	O
we	O
are	O
maximizing	O
l	O
(	O
q	O
)	O
,	O
which	O
is	O
a	O
lower	O
bound	O
on	O
the	O
log	O
marginal	O
likelihood	B
:	O
l	O
(	O
q	O
)	O
≤	O
log	O
p	O
(	O
d	O
)	O
=	O
log	O
p	O
(	O
d|μ	O
,	O
λ	O
)	O
p	O
(	O
μ	O
,	O
λ	O
)	O
dμdλ	O
(	O
21.83	O
)	O
it	O
is	O
very	O
useful	O
to	O
compute	O
the	O
lower	O
bound	O
itself	O
,	O
for	O
three	O
reasons	O
.	O
first	O
,	O
it	O
can	O
be	O
used	O
to	O
assess	O
convergence	O
of	O
the	O
algorithm	O
.	O
second	O
,	O
it	O
can	O
be	O
used	O
to	O
assess	O
the	O
correctness	O
of	O
one	O
’	O
s	O
21.5.	O
variational	O
bayes	O
745	O
2	O
1.8	O
1.6	O
1.4	O
1.2	O
λ	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−1	O
−0.5	O
0.5	O
1	O
0	O
μ	O
(	O
a	O
)	O
2	O
1.8	O
1.6	O
1.4	O
1.2	O
λ	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
−1	O
−0.5	O
0	O
μ	O
(	O
c	O
)	O
0.5	O
1	O
exact	O
vb	O
exact	O
vb	O
2	O
1.8	O
1.6	O
1.4	O
1.2	O
λ	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
2	O
1.8	O
1.6	O
1.4	O
1.2	O
λ	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
exact	O
vb	O
exact	O
vb	O
−1	O
−0.5	O
0	O
μ	O
(	O
b	O
)	O
0.5	O
1	O
−1	O
−0.5	O
0	O
μ	O
(	O
d	O
)	O
0.5	O
1	O
factored	O
variational	O
approximation	O
(	O
red	O
)	O
to	O
the	O
gaussian-gamma	O
distribution	O
(	O
green	O
)	O
.	O
figure	O
21.5	O
(	O
a	O
)	O
initial	O
guess	O
.	O
(	O
b	O
)	O
after	O
updating	O
qμ	O
.	O
(	O
c	O
)	O
after	O
updating	O
qλ	O
.	O
(	O
d	O
)	O
at	O
convergence	O
(	O
after	O
5	O
iterations	O
)	O
.	O
based	O
on	O
10.4	O
of	O
(	O
bishop	O
2006b	O
)	O
.	O
figure	O
generated	O
by	O
unigaussvbdemo	O
.	O
code	O
:	O
as	O
with	O
em	O
,	O
if	O
the	O
bound	O
does	O
not	O
increase	O
monotonically	O
,	O
there	O
must	O
be	O
a	O
bug	O
.	O
third	O
,	O
the	O
bound	O
can	O
be	O
used	O
as	O
an	O
approximation	O
to	O
the	O
marginal	B
likelihood	I
,	O
which	O
can	O
be	O
used	O
for	O
bayesian	O
model	B
selection	I
.	O
unfortunately	O
,	O
computing	O
this	O
lower	O
bound	O
involves	O
a	O
fair	O
amount	O
of	O
tedious	O
algebra	O
.	O
we	O
work	O
out	O
the	O
details	O
for	O
this	O
example	O
,	O
but	O
for	O
other	O
models	O
,	O
we	O
will	O
just	O
state	B
the	O
results	O
without	O
proof	O
,	O
or	O
even	O
omit	O
discussion	O
of	O
the	O
bound	O
altogether	O
,	O
for	O
brevity	O
.	O
for	O
this	O
model	O
,	O
l	O
(	O
q	O
)	O
can	O
be	O
computed	O
as	O
follows	O
:	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
l	O
(	O
q	O
)	O
=	O
q	O
(	O
μ	O
,	O
λ	O
)	O
log	O
p	O
(	O
d	O
,	O
μ	O
,	O
λ	O
)	O
q	O
(	O
μ	O
,	O
λ	O
)	O
dμdλ	O
=	O
e	O
[	O
log	O
p	O
(	O
d|μ	O
,	O
λ	O
)	O
]	O
+	O
e	O
[	O
log	O
p	O
(	O
μ|λ	O
)	O
]	O
+	O
e	O
[	O
log	O
p	O
(	O
λ	O
)	O
]	O
−e	O
[	O
log	O
q	O
(	O
μ	O
)	O
]	O
−	O
e	O
[	O
log	O
q	O
(	O
λ	O
)	O
]	O
(	O
21.84	O
)	O
(	O
21.85	O
)	O
(	O
21.86	O
)	O
(	O
21.87	O
)	O
where	O
all	O
expectations	O
are	O
wrt	O
q	O
(	O
μ	O
,	O
λ	O
)	O
.	O
we	O
recognize	O
the	O
last	O
two	O
terms	O
as	O
the	O
entropy	B
of	O
a	O
gaussian	O
and	O
the	O
entropy	B
of	O
a	O
gamma	B
distribution	I
,	O
which	O
are	O
given	O
by	O
(	O
cid:22	O
)	O
n	O
(	O
μn	O
,	O
κ−1	O
n	O
)	O
(	O
cid:23	O
)	O
h	O
=	O
−	O
1	O
2	O
log	O
κn	O
+	O
(	O
1	O
+	O
log	O
(	O
2π	O
)	O
)	O
1	O
2	O
h	O
(	O
ga	O
(	O
an	O
,	O
bn	O
)	O
)	O
=	O
log	O
γ	O
(	O
an	O
)	O
−	O
(	O
an	O
−	O
1	O
)	O
ψ	O
(	O
an	O
)	O
−	O
log	O
(	O
bn	O
)	O
+a	O
n	O
746	O
chapter	O
21.	O
variational	B
inference	I
where	O
ψ	O
(	O
)	O
is	O
the	O
digamma	B
function	O
.	O
to	O
compute	O
the	O
other	O
terms	O
,	O
we	O
need	O
the	O
following	O
facts	O
:	O
e	O
[	O
log	O
x|x	O
∼	O
ga	O
(	O
a	O
,	O
b	O
)	O
]	O
=	O
ψ	O
(	O
a	O
)	O
−	O
log	O
(	O
b	O
)	O
(	O
cid:13	O
)	O
e	O
[	O
x|x	O
∼	O
ga	O
(	O
a	O
,	O
b	O
)	O
]	O
=	O
(	O
cid:13	O
)	O
x|x	O
∼	O
n	O
(	O
μ	O
,	O
σ2	O
)	O
x2|x	O
∼	O
n	O
(	O
μ	O
,	O
σ2	O
)	O
a	O
b	O
=	O
μ	O
=	O
μ	O
+	O
σ2	O
(	O
cid:14	O
)	O
(	O
cid:14	O
)	O
e	O
e	O
for	O
the	O
expected	O
log	O
likelihood	B
,	O
one	O
can	O
show	O
that	O
eq	O
(	O
μ	O
,	O
λ	O
)	O
[	O
log	O
p	O
(	O
d|μ	O
,	O
λ	O
)	O
]	O
=	O
−	O
n	O
2	O
=	O
−	O
n	O
2	O
log	O
(	O
2π	O
)	O
+	O
log	O
(	O
2π	O
)	O
+	O
(	O
cid:8	O
)	O
n	O
2	O
n	O
2	O
eq	O
(	O
λ	O
)	O
[	O
log	O
λ	O
]	O
−	O
e	O
[	O
λ	O
]	O
q	O
(	O
λ	O
)	O
(	O
cid:9	O
)	O
(	O
ψ	O
(	O
an	O
)	O
−	O
log	O
bn	O
)	O
2	O
−	O
n	O
an	O
2bn	O
ˆσ2	O
+	O
x2	O
−	O
2μn	O
x	O
+	O
μ2	O
n	O
+	O
1	O
κn	O
(	O
cid:13	O
)	O
eq	O
(	O
μ	O
)	O
n	O
(	O
cid:4	O
)	O
i=1	O
(	O
cid:14	O
)	O
(	O
xi	O
−	O
μ	O
)	O
2	O
where	O
x	O
and	O
ˆσ2	O
are	O
the	O
empirical	O
mean	O
and	O
variance	B
.	O
for	O
the	O
expected	O
log	O
prior	O
of	O
λ	O
,	O
we	O
have	O
eq	O
(	O
λ	O
)	O
[	O
log	O
p	O
(	O
λ	O
)	O
]	O
=	O
(	O
a0	O
−	O
1	O
)	O
e	O
[	O
log	O
λ	O
]	O
−	O
b0e	O
[	O
λ	O
]	O
+a	O
0	O
log	O
b0	O
−	O
log	O
γ	O
(	O
a0	O
)	O
=	O
(	O
a0	O
−	O
1	O
)	O
(	O
ψ	O
(	O
an	O
)	O
−	O
log	O
bn	O
)	O
−	O
b0	O
an	O
bn	O
+	O
a0	O
log	O
b0	O
−	O
log	O
γ	O
(	O
a0	O
)	O
(	O
cid:14	O
)	O
(	O
μ	O
−	O
μ0	O
)	O
2κ0λ	O
eq	O
(	O
μ	O
,	O
λ	O
)	O
(	O
cid:13	O
)	O
for	O
the	O
expected	O
log	O
prior	O
of	O
μ	O
,	O
one	O
can	O
show	O
that	O
eq	O
(	O
μ	O
,	O
λ	O
)	O
[	O
log	O
p	O
(	O
μ|λ	O
)	O
]	O
=	O
log	O
+	O
=	O
1	O
2	O
1	O
log	O
2	O
−	O
κ0	O
2	O
κ0	O
2π	O
κ0	O
2π	O
an	O
bn	O
(	O
cid:17	O
)	O
+	O
1	O
2	O
1	O
2	O
1	O
κn	O
e	O
[	O
log	O
λ	O
]	O
q	O
(	O
λ	O
)	O
−	O
1	O
2	O
(	O
cid:18	O
)	O
(	O
ψ	O
(	O
an	O
)	O
−	O
log	O
bn	O
)	O
+	O
(	O
μn	O
−	O
μ0	O
)	O
2	O
putting	O
it	O
altogether	O
,	O
one	O
can	O
show	O
that	O
l	O
(	O
q	O
)	O
=	O
1	O
2	O
log	O
1	O
κn	O
+	O
log	O
γ	O
(	O
an	O
)	O
−	O
an	O
log	O
bn	O
+	O
const	O
this	O
quantity	O
monotonically	O
increases	O
after	O
each	O
vb	O
update	O
.	O
(	O
21.88	O
)	O
(	O
21.89	O
)	O
(	O
21.90	O
)	O
(	O
21.91	O
)	O
(	O
21.92	O
)	O
(	O
21.93	O
)	O
(	O
21.94	O
)	O
(	O
21.95	O
)	O
(	O
21.96	O
)	O
(	O
21.97	O
)	O
(	O
21.98	O
)	O
21.5.2	O
example	O
:	O
vb	O
for	O
linear	B
regression	I
in	O
section	O
7.6.4	O
,	O
we	O
discussed	O
an	O
empirical	O
bayes	O
approach	O
to	O
setting	O
the	O
hyper-parameters	B
for	O
ridge	B
regression	I
known	O
as	O
the	O
evidence	B
procedure	I
.	O
in	O
particular	O
,	O
we	O
assumed	O
a	O
likelihood	B
of	O
the	O
form	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=n	O
(	O
xw	O
,	O
λ−1	O
)	O
and	O
a	O
prior	O
of	O
the	O
form	O
p	O
(	O
w	O
)	O
=n	O
(	O
w|0	O
,	O
α−1i	O
)	O
.	O
we	O
then	O
21.5.	O
variational	O
bayes	O
747	O
computed	O
a	O
type	O
ii	O
estimate	O
of	O
α	O
and	O
λ.	O
the	O
same	O
approach	O
was	O
extended	O
in	O
section	O
13.7	O
to	O
handle	O
a	O
prior	O
of	O
the	O
form	O
n	O
(	O
w|0	O
,	O
diag	O
(	O
α	O
)	O
−1	O
)	O
,	O
which	O
allows	O
one	O
hyper-parameter	O
per	O
feature	O
,	O
a	O
technique	O
known	O
as	O
automatic	B
relevancy	I
determination	I
.	O
in	O
this	O
section	O
,	O
we	O
derive	O
a	O
vb	O
algorithm	O
for	O
this	O
model	O
.	O
we	O
follow	O
the	O
presentation	O
of	O
(	O
drugowitsch	O
2008	O
)	O
.3	O
initially	O
we	O
will	O
use	O
the	O
following	O
prior	O
:	O
0	O
)	O
ga	O
(	O
α|aα	O
p	O
(	O
w	O
,	O
λ	O
,	O
α	O
)	O
=n	O
(	O
w|0	O
,	O
(	O
λα	O
)	O
−1i	O
)	O
ga	O
(	O
λ|aλ	O
0	O
,	O
bλ	O
0	O
,	O
bα	O
0	O
)	O
we	O
choose	O
to	O
use	O
the	O
following	O
factorized	O
approximation	O
to	O
the	O
posterior	O
:	O
q	O
(	O
w	O
,	O
α	O
,	O
λ	O
)	O
=	O
q	O
(	O
w	O
,	O
λ	O
)	O
q	O
(	O
α	O
)	O
(	O
21.99	O
)	O
(	O
21.100	O
)	O
given	O
these	O
assumptions	O
,	O
one	O
can	O
show	O
(	O
see	O
(	O
drugowitsch	O
2008	O
)	O
)	O
that	O
the	O
optimal	O
form	O
for	O
the	O
posterior	O
is	O
q	O
(	O
w	O
,	O
α	O
,	O
λ	O
)	O
=n	O
(	O
w|wn	O
,	O
λ−1vn	O
)	O
ga	O
(	O
λ|aλ	O
n	O
)	O
ga	O
(	O
α|aα	O
n	O
,	O
bλ	O
n	O
,	O
bα	O
n	O
)	O
where	O
v−1	O
n	O
=	O
a	O
+	O
xx	O
wn	O
=	O
vn	O
xt	O
y	O
n	O
aλ	O
n	O
=	O
aλ	O
0	O
+	O
bλ	O
n	O
=	O
bλ	O
0	O
+	O
aα	O
n	O
=	O
aα	O
0	O
+	O
2	O
1	O
n	O
=	O
bα	O
bα	O
0	O
+	O
2	O
a	O
=	O
(	O
cid:19	O
)	O
α	O
(	O
cid:20	O
)	O
i	O
=	O
aλ	O
n	O
bλ	O
n	O
aα	O
n	O
bα	O
n	O
i	O
2	O
(	O
||y	O
−	O
xw||2	O
+	O
wt	O
1	O
2	O
(	O
cid:8	O
)	O
d	O
n	O
awn	O
)	O
(	O
cid:9	O
)	O
wt	O
n	O
wn	O
+	O
tr	O
(	O
vn	O
)	O
(	O
21.101	O
)	O
(	O
21.102	O
)	O
(	O
21.103	O
)	O
(	O
21.104	O
)	O
(	O
21.105	O
)	O
(	O
21.106	O
)	O
(	O
21.107	O
)	O
(	O
21.108	O
)	O
this	O
method	O
can	O
be	O
extended	O
to	O
the	O
ard	O
case	O
in	O
a	O
straightforward	O
way	O
,	O
by	O
using	O
the	O
following	O
priors	O
:	O
p	O
(	O
w	O
)	O
=n	O
(	O
0	O
,	O
diag	O
(	O
α	O
)	O
ga	O
(	O
αj|aα	O
p	O
(	O
α	O
)	O
=	O
d	O
(	O
cid:20	O
)	O
−1	O
)	O
0	O
,	O
bα	O
0	O
)	O
(	O
21.109	O
)	O
(	O
21.110	O
)	O
j=1	O
the	O
posterior	O
for	O
w	O
and	O
λ	O
is	O
computed	O
as	O
before	O
,	O
except	O
we	O
use	O
a	O
=	O
diag	O
(	O
aα	O
n	O
/bα	O
nj	O
)	O
instead	O
of	O
3.	O
note	O
that	O
drugowitsch	O
uses	O
a0	O
,	O
b0	O
as	O
the	O
hyper-parameters	B
for	O
p	O
(	O
λ	O
)	O
and	O
c0	O
,	O
d0	O
as	O
the	O
hyper-parameters	B
for	O
p	O
(	O
α	O
)	O
,	O
whereas	O
(	O
bishop	O
2006b	O
,	O
sec	O
10.3	O
)	O
uses	O
a0	O
,	O
b0	O
as	O
the	O
hyper-parameters	B
for	O
p	O
(	O
α	O
)	O
and	O
treats	O
λ	O
as	O
ﬁxed	O
.	O
to	O
(	O
hopefully	O
)	O
avoid	O
confusion	O
,	O
i	O
use	O
aλ	O
0	O
as	O
the	O
hyper-parameters	B
for	O
p	O
(	O
λ	O
)	O
,	O
and	O
aα	O
0	O
as	O
the	O
hyper-parameters	B
for	O
p	O
(	O
α	O
)	O
.	O
0	O
,	O
bα	O
0	O
,	O
bλ	O
748	O
chapter	O
21.	O
variational	B
inference	I
aα	O
n	O
/bα	O
n	O
i.	O
the	O
posterior	O
for	O
α	O
has	O
the	O
form	O
q	O
(	O
α	O
)	O
=	O
n	O
,	O
bα	O
nj	O
)	O
aα	O
n	O
=	O
aα	O
0	O
+	O
bα	O
nj	O
=	O
bα	O
0	O
+	O
w2	O
n	O
,	O
j	O
+	O
(	O
vn	O
)	O
jj	O
(	O
cid:20	O
)	O
j	O
ga	O
(	O
αj|aα	O
(	O
cid:8	O
)	O
1	O
2	O
1	O
2	O
aλ	O
n	O
bλ	O
n	O
(	O
cid:9	O
)	O
(	O
21.111	O
)	O
(	O
21.112	O
)	O
(	O
21.113	O
)	O
the	O
algorithm	O
alternates	O
between	O
updating	O
q	O
(	O
w	O
,	O
λ	O
)	O
and	O
q	O
(	O
α	O
)	O
.	O
once	O
w	O
and	O
λ	O
have	O
been	O
inferred	O
,	O
the	O
posterior	O
predictive	O
is	O
a	O
student	O
distribution	O
,	O
as	O
shown	O
in	O
equation	O
7.76.	O
speciﬁcally	O
,	O
for	O
a	O
single	O
data	O
case	O
,	O
we	O
have	O
(	O
y|wt	O
(	O
1	O
+	O
xt	O
vn	O
x	O
)	O
,	O
2aλ	O
(	O
21.114	O
)	O
n	O
x	O
,	O
n	O
)	O
p	O
(	O
y|x	O
,	O
d	O
)	O
=t	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
bλ	O
n	O
aλ	O
n	O
(	O
cid:8	O
)	O
n	O
(	O
cid:4	O
)	O
i=1	O
the	O
exact	O
marginal	B
likelihood	I
,	O
which	O
can	O
be	O
used	O
for	O
model	B
selection	I
,	O
is	O
given	O
by	O
p	O
(	O
d	O
)	O
=	O
p	O
(	O
y|x	O
,	O
w	O
,	O
λ	O
)	O
p	O
(	O
w|α	O
)	O
p	O
(	O
λ	O
)	O
dwdαdλ	O
we	O
can	O
compute	O
a	O
lower	O
bound	O
on	O
log	O
p	O
(	O
d	O
)	O
as	O
follows	O
:	O
(	O
cid:9	O
)	O
l	O
(	O
q	O
)	O
=−	O
n	O
2	O
log	O
(	O
2π	O
)	O
−	O
1	O
2	O
(	O
yi	O
−	O
wt	O
n	O
xi	O
)	O
2	O
+	O
xt	O
i	O
vn	O
xi	O
aλ	O
n	O
bλ	O
n	O
log	O
|vn|	O
+	O
d	O
1	O
+	O
2	O
−	O
log	O
γ	O
(	O
aλ	O
−	O
log	O
γ	O
(	O
aα	O
2	O
0	O
)	O
+a	O
λ	O
0	O
log	O
bλ	O
0	O
)	O
+a	O
α	O
0	O
log	O
bα	O
0	O
aλ	O
n	O
bλ	O
n	O
0	O
−	O
bλ	O
0	O
+	O
log	O
γ	O
(	O
aα	O
+	O
log	O
γ	O
(	O
aλ	O
n	O
)	O
−	O
aα	O
n	O
)	O
−	O
aλ	O
n	O
log	O
bα	O
n	O
n	O
log	O
bλ	O
n	O
+	O
aλ	O
n	O
in	O
the	O
ard	O
case	O
,	O
the	O
last	O
line	O
becomes	O
d	O
(	O
cid:4	O
)	O
(	O
cid:15	O
)	O
−	O
log	O
γ	O
(	O
aα	O
0	O
)	O
+a	O
α	O
0	O
log	O
bα	O
0	O
+	O
log	O
γ	O
(	O
aα	O
n	O
)	O
−	O
aα	O
n	O
log	O
bα	O
nj	O
(	O
cid:16	O
)	O
(	O
21.115	O
)	O
(	O
21.116	O
)	O
(	O
21.117	O
)	O
j=1	O
figure	O
21.6	O
compare	O
vb	O
and	O
eb	O
on	O
a	O
model	B
selection	I
problem	O
for	O
polynomial	B
regression	I
.	O
we	O
see	O
that	O
vb	O
gives	O
similar	B
results	O
to	O
eb	O
,	O
but	O
the	O
precise	O
behavior	O
depends	O
on	O
the	O
sample	O
size	O
.	O
when	O
n	O
=	O
5	O
,	O
vb	O
’	O
s	O
estimate	O
of	O
the	O
posterior	O
over	O
models	O
is	O
more	O
diffuse	O
than	O
eb	O
’	O
s	O
,	O
since	O
vb	O
models	O
uncertainty	B
in	O
the	O
hyper-parameters	B
.	O
when	O
n	O
=	O
30	O
,	O
the	O
posterior	O
estimate	O
of	O
the	O
hyper-	O
indeed	O
,	O
if	O
we	O
compute	O
e	O
[	O
α|d	O
]	O
when	O
we	O
have	O
an	O
parameters	O
becomes	O
more	O
well-determined	O
.	O
uninformative	B
prior	O
,	O
aα	O
0	O
=	O
0	O
,	O
we	O
get	O
0	O
=	O
bα	O
α	O
=	O
aα	O
n	O
bα	O
n	O
=	O
d/2	O
1	O
2	O
(	O
aλ	O
n	O
bλ	O
n	O
wt	O
n	O
wn	O
+	O
tr	O
(	O
vn	O
)	O
)	O
(	O
21.118	O
)	O
21.6.	O
variational	O
bayes	O
em	O
749	O
n=5	O
,	O
method=vb	O
n=5	O
,	O
method=eb	O
)	O
|	O
d	O
m	O
p	O
(	O
)	O
|	O
d	O
m	O
p	O
(	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
)	O
|	O
d	O
m	O
p	O
(	O
2	O
3	O
1	O
m	O
(	O
a	O
)	O
n=30	O
,	O
method=vb	O
)	O
|	O
d	O
m	O
p	O
(	O
2	O
3	O
1	O
m	O
(	O
c	O
)	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
2	O
3	O
1	O
m	O
(	O
b	O
)	O
n=30	O
,	O
method=eb	O
2	O
3	O
1	O
m	O
(	O
d	O
)	O
figure	O
21.6	O
we	O
plot	O
the	O
posterior	O
over	O
models	O
(	O
polynomials	O
of	O
degree	B
1	O
,	O
2	O
and	O
3	O
)	O
assuming	O
a	O
uniform	O
prior	O
p	O
(	O
m	O
)	O
∝	O
1.	O
we	O
approximate	O
the	O
marginal	B
likelihood	I
using	O
(	O
a	O
,	O
c	O
)	O
vb	O
and	O
(	O
b	O
,	O
d	O
)	O
eb	O
.	O
in	O
(	O
a-b	O
)	O
,	O
we	O
use	O
n	O
=	O
5	O
data	O
points	O
(	O
shown	O
in	O
figure	O
5.7	O
)	O
.	O
in	O
(	O
c-d	O
)	O
,	O
we	O
use	O
n	O
=	O
30	O
data	O
points	O
(	O
shown	O
in	O
figure	O
5.8	O
)	O
.	O
figure	O
generated	O
by	O
linregebmodelselvsn	O
.	O
compare	O
this	O
to	O
equation	O
13.167	O
for	O
eb	O
:	O
ˆα	O
=	O
d	O
e	O
[	O
wt	O
w	O
]	O
=	O
d	O
wt	O
n	O
wn	O
+	O
tr	O
(	O
vn	O
)	O
(	O
21.119	O
)	O
n	O
and	O
bλ	O
modulo	O
the	O
aλ	O
in	O
hindsight	B
this	O
is	O
perhaps	O
not	O
that	O
surprising	O
,	O
since	O
eb	O
is	O
trying	O
to	O
maximize	O
log	O
p	O
(	O
d	O
)	O
,	O
and	O
vb	O
is	O
trying	O
to	O
maximize	O
a	O
lower	O
bound	O
on	O
log	O
p	O
(	O
d	O
)	O
.	O
n	O
terms	O
,	O
these	O
are	O
the	O
same	O
.	O
21.6	O
variational	O
bayes	O
em	O
now	O
consider	O
latent	B
variable	I
models	I
of	O
the	O
form	O
zi	O
→	O
xi	O
←	O
θ.	O
this	O
includes	O
mixtures	O
models	O
,	O
pca	O
,	O
hmms	O
,	O
etc	O
.	O
there	O
are	O
now	O
two	O
kinds	O
of	O
unknowns	O
:	O
parameters	O
,	O
θ	O
,	O
and	O
latent	B
variables	O
,	O
zi	O
.	O
as	O
we	O
saw	O
in	O
section	O
11.4	O
,	O
it	O
is	O
common	O
to	O
ﬁt	O
such	O
models	O
using	O
em	O
,	O
where	O
in	O
the	O
e	O
step	O
we	O
infer	O
the	O
posterior	O
over	O
the	O
latent	B
variables	O
,	O
p	O
(	O
zi|xi	O
,	O
θ	O
)	O
,	O
and	O
in	O
the	O
m	O
step	O
,	O
we	O
compute	O
a	O
point	B
estimate	I
of	O
the	O
parameters	O
,	O
θ.	O
the	O
justiﬁcation	O
for	O
this	O
is	O
two-fold	O
.	O
first	O
,	O
it	O
results	O
in	O
simple	O
algorithms	O
.	O
second	O
,	O
the	O
posterior	O
uncertainty	O
in	O
θ	O
is	O
usually	O
less	O
than	O
in	O
zi	O
,	O
since	O
the	O
θ	O
are	O
informed	O
by	O
all	O
n	O
data	O
cases	O
,	O
whereas	O
zi	O
is	O
only	O
informed	O
by	O
xi	O
;	O
this	O
makes	O
a	O
map	O
estimate	O
of	O
750	O
chapter	O
21.	O
variational	B
inference	I
θ	O
more	O
reasonable	O
than	O
a	O
map	O
estimate	O
of	O
zi	O
.	O
however	O
,	O
vb	O
provides	O
a	O
way	O
to	O
be	O
“	O
more	O
bayesian	O
”	O
,	O
by	O
modeling	O
uncertainty	B
in	O
the	O
parameters	O
θ	O
as	O
well	O
in	O
the	O
latent	B
variables	O
zi	O
,	O
at	O
a	O
computational	O
cost	O
that	O
is	O
essentially	O
the	O
same	O
as	O
em	O
.	O
this	O
method	O
is	O
known	O
as	O
variational	O
bayes	O
em	O
or	O
vbem	O
.	O
the	O
basic	O
idea	O
is	O
to	O
use	O
mean	B
ﬁeld	I
,	O
where	O
the	O
approximate	O
posterior	O
has	O
the	O
form	O
p	O
(	O
θ	O
,	O
z1	O
:	O
n|d	O
)	O
≈	O
q	O
(	O
θ	O
)	O
q	O
(	O
z	O
)	O
=	O
q	O
(	O
θ	O
)	O
q	O
(	O
zi	O
)	O
(	O
21.120	O
)	O
(	O
cid:20	O
)	O
i	O
the	O
ﬁrst	O
factorization	O
,	O
between	O
θ	O
and	O
z	O
,	O
is	O
a	O
crucial	O
assumption	O
to	O
make	O
the	O
algorithm	O
tractable	O
.	O
the	O
second	O
factorization	O
follows	O
from	O
the	O
model	O
,	O
since	O
the	O
latent	B
variables	O
are	O
iid	B
conditional	O
on	O
θ.	O
in	O
vbem	O
,	O
we	O
alternate	O
between	O
updating	O
q	O
(	O
zi|d	O
)	O
(	O
the	O
variational	O
e	O
step	O
)	O
and	O
updating	O
q	O
(	O
θ|d	O
)	O
(	O
the	O
variational	O
m	O
step	O
)	O
.	O
we	O
can	O
recover	O
standard	O
em	O
from	O
vbem	O
by	O
approximating	O
the	O
param-	O
eter	O
posterior	O
using	O
a	O
delta	O
function	O
,	O
q	O
(	O
θ|d	O
)	O
≈	O
δˆθ	O
(	O
θ	O
)	O
.	O
the	O
variational	O
e	O
step	O
is	O
similar	B
to	O
a	O
standard	O
e	O
step	O
,	O
except	O
instead	O
of	O
plugging	O
in	O
a	O
map	O
estimate	O
of	O
the	O
parameters	O
and	O
computing	O
p	O
(	O
zi|d	O
,	O
ˆθ	O
)	O
,	O
we	O
need	O
to	O
average	O
over	O
the	O
parameters	O
.	O
roughly	O
speaking	O
,	O
this	O
can	O
be	O
computed	O
by	O
plugging	O
in	O
the	O
posterior	B
mean	I
of	O
the	O
parameters	O
instead	O
of	O
the	O
map	O
estimate	O
,	O
and	O
then	O
computing	O
p	O
(	O
zi|d	O
,	O
θ	O
)	O
using	O
standard	O
algorithms	O
,	O
such	O
as	O
forwards-backwards	B
.	O
unfortunately	O
,	O
things	O
are	O
not	O
quite	O
this	O
simple	O
,	O
but	O
this	O
is	O
the	O
basic	O
idea	O
.	O
the	O
details	O
depend	O
on	O
the	O
form	O
of	O
the	O
model	O
;	O
we	O
give	O
some	O
examples	O
below	O
.	O
the	O
variational	O
m	O
step	O
is	O
similar	B
to	O
a	O
standard	O
m	O
step	O
,	O
except	O
instead	O
of	O
computing	O
a	O
point	B
estimate	I
of	O
the	O
parameters	O
,	O
we	O
update	O
the	O
hyper-parameters	B
,	O
using	O
the	O
expected	O
sufficient	O
statis-	O
tics	O
.	O
this	O
process	O
is	O
usually	O
very	O
similar	B
to	O
map	O
estimation	O
in	O
regular	B
em	O
.	O
again	O
,	O
the	O
details	O
on	O
how	O
to	O
do	O
this	O
depend	O
on	O
the	O
form	O
of	O
the	O
model	O
.	O
the	O
principle	O
advantage	O
of	O
vbem	O
over	O
regular	B
em	O
is	O
that	O
by	O
marginalizing	B
out	I
the	O
parameters	O
,	O
we	O
can	O
compute	O
a	O
lower	O
bound	O
on	O
the	O
marginal	B
likelihood	I
,	O
which	O
can	O
be	O
used	O
for	O
model	B
selection	I
.	O
we	O
will	O
see	O
an	O
example	O
of	O
this	O
in	O
section	O
21.6.1.6.	O
vbem	O
is	O
also	O
“	O
egalitarian	O
”	O
,	O
since	O
it	O
treats	O
parameters	O
as	O
“	O
ﬁrst	O
class	O
citizens	O
”	O
,	O
just	O
like	O
any	O
other	O
unknown	B
quantity	O
,	O
whereas	O
em	O
makes	O
an	O
artiﬁcial	O
distinction	O
between	O
parameters	O
and	O
latent	B
variables	O
.	O
21.6.1	O
example	O
:	O
vbem	O
for	O
mixtures	O
of	O
gaussians	O
*	O
let	O
us	O
consider	O
how	O
to	O
“	O
ﬁt	O
”	O
a	O
mixture	O
of	O
gaussians	O
using	O
vbem	O
.	O
(	O
we	O
use	O
scare	O
quotes	O
since	O
we	O
are	O
not	O
estimating	O
the	O
model	O
parameters	O
,	O
but	O
inferring	O
a	O
posterior	O
over	O
them	O
.	O
)	O
we	O
will	O
follow	O
the	O
presentation	O
of	O
(	O
bishop	O
2006b	O
,	O
sec	O
10.2	O
)	O
.	O
unfortunately	O
,	O
the	O
details	O
are	O
rather	O
complicated	O
.	O
fortunately	O
,	O
as	O
with	O
em	O
,	O
one	O
gets	O
used	O
to	O
it	O
after	O
a	O
bit	O
of	O
practice	O
.	O
(	O
as	O
usual	O
with	O
math	O
,	O
simply	O
reading	O
the	O
equations	O
won	O
’	O
t	O
help	O
much	O
,	O
you	O
should	O
really	O
try	O
deriving	O
these	O
results	O
yourself	O
(	O
or	O
try	O
some	O
of	O
the	O
exercises	O
)	O
if	O
you	O
want	O
to	O
learn	O
this	O
stuff	O
in	O
depth	O
.	O
)	O
21.6.1.1	O
the	O
variational	O
posterior	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
the	O
likelihood	B
function	O
is	O
the	O
usual	O
one	O
for	O
gaussian	O
mixture	B
models	O
:	O
p	O
(	O
z	O
,	O
x|θ	O
)	O
=	O
k	O
n	O
(	O
xi|μk	O
,	O
λ−1	O
πzik	O
k	O
)	O
zik	O
i	O
k	O
where	O
zik	O
=	O
1	O
if	O
data	O
point	O
i	O
belongs	O
to	O
cluster	O
k	O
,	O
and	O
zik	O
=	O
0	O
otherwise	O
.	O
(	O
21.121	O
)	O
21.6.	O
variational	O
bayes	O
em	O
we	O
will	O
assume	O
the	O
following	O
factored	O
conjugate	B
prior	I
p	O
(	O
θ	O
)	O
=	O
dir	O
(	O
π|α0	O
)	O
n	O
(	O
μk|m0	O
,	O
(	O
β0λk	O
)	O
−1	O
)	O
wi	O
(	O
λk|l0	O
,	O
ν0	O
)	O
(	O
cid:20	O
)	O
k	O
751	O
(	O
21.122	O
)	O
where	O
λk	O
is	O
the	O
precision	B
matrix	I
for	O
cluster	O
k.	O
the	O
subscript	O
0	O
means	O
these	O
are	O
parameters	O
of	O
the	O
prior	O
;	O
we	O
assume	O
all	O
the	O
prior	O
parameters	O
are	O
the	O
same	O
for	O
all	O
clusters	O
.	O
for	O
the	O
mixing	B
weights	I
,	O
we	O
usually	O
use	O
a	O
symmetric	B
prior	O
,	O
α0	O
=	O
α01	O
.	O
the	O
exact	O
posterior	O
p	O
(	O
z	O
,	O
θ|d	O
)	O
is	O
a	O
mixture	O
of	O
k	O
n	O
distributions	O
,	O
corresponding	O
to	O
all	O
possible	O
labelings	O
z.	O
we	O
will	O
try	O
to	O
approximate	O
the	O
volume	O
around	O
one	O
of	O
these	O
modes	O
.	O
we	O
will	O
use	O
the	O
standard	O
vb	O
approximation	O
to	O
the	O
posterior	O
:	O
p	O
(	O
θ	O
,	O
z1	O
:	O
n|d	O
)	O
≈	O
q	O
(	O
θ	O
)	O
q	O
(	O
zi	O
)	O
(	O
21.123	O
)	O
(	O
cid:20	O
)	O
i	O
at	O
this	O
stage	O
we	O
have	O
not	O
speciﬁed	O
the	O
forms	O
of	O
the	O
q	O
functions	O
;	O
these	O
will	O
be	O
determined	O
by	O
the	O
form	O
of	O
the	O
likelihood	B
and	O
prior	O
.	O
below	O
we	O
will	O
show	O
that	O
the	O
optimal	O
form	O
is	O
as	O
follows	O
:	O
q	O
(	O
z	O
,	O
θ	O
)	O
=q	O
(	O
cid:25	O
)	O
cat	O
(	O
zi|ri	O
)	O
(	O
cid:24	O
)	O
(	O
cid:20	O
)	O
(	O
z|θ	O
)	O
q	O
(	O
θ	O
)	O
=	O
(	O
cid:24	O
)	O
(	O
cid:20	O
)	O
i	O
dir	O
(	O
π|α	O
)	O
n	O
(	O
μk|mk	O
,	O
(	O
βkλk	O
)	O
(	O
cid:25	O
)	O
−1	O
)	O
wi	O
(	O
λk|lk	O
,	O
νk	O
)	O
(	O
21.124	O
)	O
(	O
21.125	O
)	O
k	O
(	O
the	O
lack	O
of	O
0	O
subscript	O
means	O
these	O
are	O
parameters	O
of	O
the	O
posterior	O
,	O
not	O
the	O
prior	O
.	O
)	O
below	O
we	O
will	O
derive	O
the	O
update	O
equations	O
for	O
these	O
variational	O
parameters	O
.	O
21.6.1.2	O
derivation	O
of	O
q	O
(	O
z	O
)	O
(	O
variational	O
e	O
step	O
)	O
the	O
form	O
for	O
q	O
(	O
z	O
)	O
can	O
be	O
obtained	O
by	O
looking	O
at	O
the	O
complete	O
data	O
log	O
joint	O
,	O
ignoring	O
terms	O
that	O
do	O
not	O
involve	O
z	O
,	O
and	O
taking	O
expectations	O
of	O
what	O
’	O
s	O
left	O
over	O
wrt	O
all	O
the	O
hidden	B
variables	I
except	O
for	O
z.	O
we	O
have	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
log	O
q	O
(	O
z	O
)	O
=e	O
q	O
(	O
θ	O
)	O
[	O
log	O
p	O
(	O
x	O
,	O
z	O
,	O
θ	O
)	O
]	O
+	O
const	O
zik	O
log	O
ρik	O
+	O
const	O
=	O
i	O
i	O
where	O
we	O
deﬁne	O
log	O
ρik	O
(	O
cid:2	O
)	O
eq	O
(	O
θ	O
)	O
[	O
log	O
πk	O
]	O
+	O
(	O
cid:13	O
)	O
1	O
2	O
eq	O
(	O
θ	O
)	O
[	O
log	O
|λk|	O
]	O
−	O
d	O
(	O
cid:14	O
)	O
2	O
(	O
xi	O
−	O
μk	O
)	O
t	O
λk	O
(	O
xi	O
−	O
μk	O
)	O
−	O
1	O
2	O
eq	O
(	O
θ	O
)	O
using	O
the	O
fact	O
that	O
q	O
(	O
π	O
)	O
=	O
dir	O
(	O
π	O
)	O
,	O
we	O
have	O
log	O
˜πk	O
(	O
cid:2	O
)	O
e	O
[	O
log	O
πk	O
]	O
=	O
ψ	O
(	O
αk	O
)	O
−	O
ψ	O
(	O
αk	O
(	O
cid:2	O
)	O
)	O
(	O
cid:4	O
)	O
k	O
(	O
cid:2	O
)	O
log	O
(	O
2π	O
)	O
(	O
21.126	O
)	O
(	O
21.127	O
)	O
(	O
21.128	O
)	O
(	O
21.129	O
)	O
752	O
chapter	O
21.	O
variational	B
inference	I
where	O
ψ	O
(	O
)	O
is	O
the	O
digamma	B
function	O
.	O
(	O
see	O
exercise	O
21.5	O
for	O
the	O
detailed	O
derivation	O
.	O
)	O
next	O
,	O
we	O
use	O
the	O
fact	O
that	O
q	O
(	O
μk	O
,	O
λk	O
)	O
=	O
n	O
(	O
μk|mk	O
,	O
(	O
βkλk	O
)	O
(	O
cid:8	O
)	O
d	O
(	O
cid:4	O
)	O
to	O
get	O
log	O
˜λk	O
(	O
cid:2	O
)	O
e	O
[	O
log	O
|λk|	O
]	O
=	O
(	O
cid:13	O
)	O
j=1	O
ψ	O
(	O
cid:14	O
)	O
−1	O
)	O
wi	O
(	O
λk|lk	O
,	O
νk	O
)	O
(	O
cid:9	O
)	O
+	O
d	O
log	O
2	O
+	O
log	O
|λk|	O
νk	O
+	O
1−	O
j	O
2	O
finally	O
,	O
for	O
the	O
expected	B
value	I
of	O
the	O
quadratic	O
form	O
,	O
we	O
get	O
(	O
xi	O
−	O
μk	O
)	O
t	O
λk	O
(	O
xi	O
−	O
μk	O
)	O
=	O
dβ−1	O
k	O
+	O
νk	O
(	O
xi	O
−	O
mk	O
)	O
t	O
λk	O
(	O
xi	O
−	O
mk	O
)	O
e	O
putting	O
it	O
altogether	O
,	O
we	O
get	O
that	O
the	O
posterior	O
responsibility	O
of	O
cluster	O
k	O
for	O
datapoint	O
i	O
is	O
rik	O
∝	O
˜πk	O
˜λ	O
1	O
2	O
k	O
exp	O
−	O
νk	O
2	O
(	O
xi	O
−	O
mk	O
)	O
t	O
λk	O
(	O
xi	O
−	O
mk	O
)	O
compare	O
this	O
to	O
the	O
expression	O
used	O
in	O
regular	B
em	O
:	O
rem	O
ik	O
∝	O
ˆπk|	O
ˆλ|	O
1	O
2	O
k	O
exp	O
(	O
xi	O
−	O
ˆμk	O
)	O
t	O
ˆλk	O
(	O
xi	O
−	O
ˆμk	O
)	O
(	O
cid:9	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
−	O
d	O
2βk	O
(	O
cid:8	O
)	O
−	O
1	O
2	O
(	O
cid:4	O
)	O
k	O
(	O
cid:4	O
)	O
i	O
+	O
(	O
cid:4	O
)	O
(	O
cid:20	O
)	O
k	O
(	O
21.130	O
)	O
(	O
21.131	O
)	O
(	O
21.132	O
)	O
(	O
21.133	O
)	O
(	O
21.134	O
)	O
(	O
21.135	O
)	O
(	O
21.136	O
)	O
(	O
21.137	O
)	O
(	O
21.138	O
)	O
(	O
21.139	O
)	O
(	O
21.140	O
)	O
the	O
signiﬁcance	O
of	O
this	O
difference	O
is	O
discussed	O
further	O
in	O
section	O
21.6.1.7	O
.	O
21.6.1.3	O
derivation	O
of	O
q	O
(	O
θ	O
)	O
(	O
variational	O
m	O
step	O
)	O
using	O
the	O
mean	B
ﬁeld	I
recipe	O
,	O
we	O
have	O
log	O
q	O
(	O
θ	O
)	O
=	O
log	O
p	O
(	O
π	O
)	O
+	O
log	O
p	O
(	O
μk	O
,	O
λk	O
)	O
+	O
(	O
cid:4	O
)	O
eq	O
(	O
z	O
)	O
[	O
log	O
p	O
(	O
zi|π	O
)	O
]	O
eq	O
(	O
z	O
)	O
[	O
zik	O
]	O
log	O
n	O
(	O
xi|μk	O
,	O
λ−1	O
k	O
)	O
+	O
const	O
i	O
we	O
see	O
this	O
factorizes	O
into	O
the	O
form	O
q	O
(	O
θ	O
)	O
=q	O
(	O
π	O
)	O
q	O
(	O
μk	O
,	O
λk	O
)	O
k	O
for	O
the	O
π	O
term	O
,	O
we	O
have	O
log	O
q	O
(	O
π	O
)	O
=	O
(	O
α0	O
−	O
1	O
)	O
(	O
cid:4	O
)	O
k	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
k	O
i	O
log	O
πk	O
+	O
rik	O
log	O
πk	O
+	O
const	O
exponentiating	O
,	O
we	O
recognize	O
this	O
as	O
a	O
dirichlet	O
distribution	O
:	O
q	O
(	O
π	O
)	O
=	O
dir	O
(	O
π|α	O
)	O
αk	O
=	O
α0	O
+	O
nk	O
nk	O
=	O
(	O
cid:4	O
)	O
rik	O
i	O
21.6.	O
variational	O
bayes	O
em	O
753	O
variational	O
bayes	O
objective	O
for	O
gmm	O
on	O
old	O
faithful	B
data	O
d	O
o	O
o	O
h	O
i	O
l	O
e	O
k	O
i	O
i	O
l	O
l	O
a	O
n	O
g	O
r	O
a	O
m	O
g	O
o	O
l	O
n	O
o	O
d	O
n	O
u	O
o	O
b	O
r	O
e	O
w	O
o	O
l	O
−600	O
−650	O
−700	O
−750	O
−800	O
−850	O
−900	O
−950	O
−1000	O
−1050	O
−1100	O
0	O
20	O
40	O
60	O
80	O
100	O
iter	O
figure	O
21.7	O
lower	O
bound	O
vs	O
iterations	O
for	O
the	O
vb	O
algorithm	O
in	O
figure	O
21.8.	O
the	O
steep	O
parts	O
of	O
the	O
curve	O
correspond	O
to	O
places	O
where	O
the	O
algorithm	O
ﬁgures	O
out	O
that	O
it	O
can	O
increase	O
the	O
bound	O
by	O
“	O
killing	O
off	O
”	O
unnecessary	O
mixture	B
components	O
,	O
as	O
described	O
in	O
section	O
21.6.1.6.	O
the	O
plateaus	O
correspond	O
to	O
slowly	O
moving	O
the	O
clusters	B
around	O
.	O
figure	O
generated	O
by	O
mixgaussvbdemofaithful	O
.	O
for	O
the	O
μk	O
and	O
λk	O
terms	O
,	O
we	O
have	O
q	O
(	O
μk	O
,	O
λk	O
)	O
=n	O
(	O
μk|mk	O
,	O
(	O
βkλk	O
)	O
−1	O
)	O
wi	O
(	O
λk|lk	O
,	O
νk	O
)	O
β0nk	O
β0	O
+	O
nk	O
(	O
xk	O
−	O
m0	O
)	O
(	O
xk	O
−	O
m0	O
)	O
t	O
βk	O
=	O
β0	O
+	O
nk	O
mk	O
=	O
(	O
β0m0	O
+	O
nkxk	O
)	O
/βk	O
l−1	O
0	O
+	O
nksk	O
+	O
k	O
νk	O
=	O
ν0	O
+	O
nk	O
+	O
1	O
=	O
l−1	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
1	O
nk	O
i	O
1	O
nk	O
i	O
xk	O
=	O
sk	O
=	O
rikxi	O
rik	O
(	O
xi	O
−	O
xk	O
)	O
(	O
xi	O
−	O
xk	O
)	O
t	O
(	O
21.141	O
)	O
(	O
21.142	O
)	O
(	O
21.143	O
)	O
(	O
21.144	O
)	O
(	O
21.145	O
)	O
(	O
21.146	O
)	O
(	O
21.147	O
)	O
this	O
is	O
very	O
similar	B
to	O
the	O
m	O
step	O
for	O
map	O
estimation	O
discussed	O
in	O
section	O
11.4.2.8	O
,	O
except	O
here	O
we	O
are	O
computing	O
the	O
parameters	O
of	O
the	O
posterior	O
over	O
θ	O
,	O
rather	O
than	O
map	O
estimates	O
of	O
θ	O
.	O
21.6.1.4	O
lower	O
bound	O
on	O
the	O
marginal	B
likelihood	I
the	O
algorithm	O
is	O
trying	O
to	O
maximize	O
the	O
following	O
lower	O
bound	O
(	O
cid:12	O
)	O
(	O
cid:4	O
)	O
z	O
l	O
=	O
q	O
(	O
z	O
,	O
θ	O
)	O
log	O
p	O
(	O
x	O
,	O
z	O
,	O
θ	O
)	O
q	O
(	O
z	O
,	O
θ	O
)	O
dθ	O
≤	O
log	O
p	O
(	O
d	O
)	O
(	O
21.148	O
)	O
this	O
quantity	O
should	O
increase	O
monotonically	O
with	O
each	O
iteration	O
,	O
as	O
shown	O
in	O
figure	O
21.7.	O
un-	O
fortunately	O
,	O
deriving	O
the	O
bound	O
is	O
a	O
bit	O
messy	O
,	O
because	O
we	O
need	O
to	O
compute	O
expectations	O
of	O
the	O
unnormalized	O
log	O
posterior	O
as	O
well	O
as	O
entropies	O
of	O
the	O
q	O
distribution	O
.	O
we	O
leave	O
the	O
details	O
(	O
which	O
are	O
similar	B
to	O
section	O
21.5.1.6	O
)	O
to	O
exercise	O
21.4	O
.	O
754	O
chapter	O
21.	O
variational	B
inference	I
21.6.1.5	O
posterior	B
predictive	I
distribution	I
(	O
cid:20	O
)	O
k	O
we	O
showed	O
that	O
the	O
approximate	O
posterior	O
has	O
the	O
form	O
q	O
(	O
θ	O
)	O
=	O
dir	O
(	O
π|α	O
)	O
n	O
(	O
μk|mk	O
,	O
(	O
βkλk	O
)	O
−1	O
)	O
wi	O
(	O
λk|lk	O
,	O
νk	O
)	O
(	O
21.149	O
)	O
consequently	O
the	O
posterior	B
predictive	I
density	I
can	O
be	O
approximated	O
as	O
follows	O
,	O
using	O
the	O
results	O
from	O
section	O
4.6.3.6	O
:	O
p	O
(	O
x|z	O
,	O
θ	O
)	O
p	O
(	O
z|θ	O
)	O
q	O
(	O
θ	O
)	O
dθ	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
πkn	O
(	O
x|μk	O
,	O
λ−1	O
αk	O
(	O
cid:7	O
)	O
(	O
νk	O
+	O
1−	O
d	O
)	O
βk	O
k	O
(	O
cid:2	O
)	O
αk	O
(	O
cid:2	O
)	O
z	O
k	O
k	O
lk	O
1	O
+	O
βk	O
k	O
)	O
q	O
(	O
θ	O
)	O
dθ	O
t	O
(	O
x|mk	O
,	O
mk	O
,	O
νk	O
+	O
1−	O
d	O
)	O
p	O
(	O
x|d	O
)	O
≈	O
=	O
=	O
mk	O
=	O
(	O
21.150	O
)	O
(	O
21.151	O
)	O
(	O
21.152	O
)	O
(	O
21.153	O
)	O
this	O
is	O
just	O
a	O
weighted	O
sum	O
of	O
student	O
distributions	O
.	O
if	O
instead	O
we	O
used	O
a	O
plug-in	B
approximation	I
,	O
we	O
would	O
get	O
a	O
weighted	O
sum	O
of	O
gaussian	O
distributions	O
.	O
21.6.1.6	O
model	B
selection	I
using	O
vbem	O
the	O
simplest	O
way	O
to	O
select	O
k	O
when	O
using	O
vb	O
is	O
to	O
ﬁt	O
several	O
models	O
,	O
and	O
then	O
to	O
use	O
the	O
variational	O
lower	O
bound	O
to	O
the	O
log	O
marginal	O
likelihood	B
,	O
l	O
(	O
k	O
)	O
≤	O
log	O
p	O
(	O
d|k	O
)	O
,	O
to	O
approximate	O
p	O
(	O
k|d	O
)	O
:	O
el	O
(	O
k	O
)	O
k	O
(	O
cid:2	O
)	O
el	O
(	O
k	O
(	O
cid:2	O
)	O
)	O
(	O
21.154	O
)	O
(	O
cid:7	O
)	O
p	O
(	O
k|d	O
)	O
=	O
however	O
,	O
the	O
lower	O
bound	O
needs	O
to	O
be	O
modiﬁed	O
somewhat	O
to	O
take	O
into	O
account	O
the	O
lack	O
of	O
identiﬁability	O
of	O
the	O
parameters	O
(	O
section	O
11.3.1	O
)	O
.	O
in	O
particular	O
,	O
although	O
vb	O
will	O
approximate	O
the	O
volume	O
occupied	O
by	O
the	O
parameter	B
posterior	O
,	O
it	O
will	O
only	O
do	O
so	O
around	O
one	O
of	O
the	O
local	O
modes	O
.	O
with	O
k	O
components	O
,	O
there	O
are	O
k	O
!	O
equivalent	O
modes	O
,	O
which	O
differ	O
merely	O
by	O
permuting	O
the	O
labels	O
.	O
therefore	O
we	O
should	O
use	O
log	O
p	O
(	O
d|k	O
)	O
≈	O
l	O
(	O
k	O
)	O
+	O
log	O
(	O
k	O
!	O
)	O
.	O
21.6.1.7	O
automatic	O
sparsity	O
inducing	O
effects	O
of	O
vbem	O
although	O
vb	O
provides	O
a	O
reasonable	O
approximation	O
to	O
the	O
marginal	B
likelihood	I
(	O
better	O
than	O
bic	O
(	O
beal	O
and	O
ghahramani	O
2006	O
)	O
)	O
,	O
this	O
method	O
still	O
requires	O
ﬁtting	O
multiple	O
models	O
,	O
one	O
for	O
each	O
value	O
of	O
k	O
being	O
considered	O
.	O
a	O
faster	O
alternative	O
is	O
to	O
ﬁt	O
a	O
single	O
model	O
,	O
where	O
k	O
is	O
set	O
large	O
,	O
but	O
where	O
α0	O
is	O
set	O
very	O
small	O
,	O
α0	O
(	O
cid:22	O
)	O
1.	O
from	O
figure	O
2.14	O
(	O
d	O
)	O
,	O
we	O
see	O
that	O
the	O
resulting	O
prior	O
for	O
the	O
mixing	B
weights	I
π	O
has	O
“	O
spikes	O
”	O
near	O
the	O
corners	O
of	O
the	O
simplex	O
,	O
encouraging	O
a	O
sparse	B
mixing	O
weight	B
vector	I
.	O
in	O
regular	B
em	O
,	O
the	O
map	O
estimate	O
of	O
the	O
mixing	B
weights	I
will	O
have	O
the	O
form	O
ˆπk	O
∝	O
(	O
αk	O
−	O
1	O
)	O
,	O
where	O
αk	O
=	O
α0	O
+	O
nk	O
.	O
unforuntately	O
,	O
this	O
can	O
be	O
negative	O
if	O
α0	O
=	O
0	O
and	O
nk	O
=	O
0	O
(	O
figueiredo	O
21.6.	O
variational	O
bayes	O
em	O
755	O
iter	O
1	O
iter	O
94	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−2	O
−2.5	O
−2	O
5	O
6	O
1	O
2	O
3	O
4	O
−1.5	O
−1	O
−0.5	O
0	O
0.5	O
1	O
1.5	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−2	O
−2.5	O
−2	O
5	O
4	O
−1.5	O
−1	O
−0.5	O
0	O
0.5	O
1	O
1.5	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
21.8	O
we	O
visualize	O
the	O
posterior	B
mean	I
parameters	O
at	O
various	O
stages	O
of	O
the	O
vbem	O
algorithm	O
applied	O
to	O
a	O
mixture	O
of	O
gaussians	O
model	O
on	O
the	O
old	O
faithful	B
data	O
.	O
shading	O
intensity	O
is	O
proportional	O
to	O
the	O
mixing	O
weight	O
.	O
we	O
initialize	O
with	O
k-means	O
and	O
use	O
α0	O
=	O
0.001	O
as	O
the	O
dirichlet	O
hyper-parameter	O
.	O
based	O
on	O
figure	O
10.6	O
of	O
(	O
bishop	O
2006b	O
)	O
.	O
figure	O
generated	O
by	O
mixgaussvbdemofaithful	O
,	O
based	O
on	O
code	O
by	O
emtiyaz	O
khan	O
.	O
iter	O
1	O
iter	O
94	O
90	O
80	O
70	O
60	O
50	O
40	O
30	O
20	O
10	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
(	O
a	O
)	O
180	O
160	O
140	O
120	O
100	O
80	O
60	O
40	O
20	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
(	O
b	O
)	O
figure	O
21.9	O
we	O
visualize	O
the	O
posterior	O
values	O
of	O
αk	O
for	O
the	O
model	O
in	O
figure	O
21.8.	O
we	O
see	O
that	O
unnecessary	O
components	O
get	O
“	O
killed	O
off	O
”	O
.	O
figure	O
generated	O
by	O
mixgaussvbdemofaithful	O
.	O
and	O
jain	O
2002	O
)	O
.	O
however	O
,	O
in	O
vbem	O
,	O
we	O
use	O
(	O
cid:7	O
)	O
exp	O
[	O
ψ	O
(	O
αk	O
)	O
]	O
exp	O
[	O
ψ	O
(	O
˜πk	O
=	O
k	O
(	O
cid:2	O
)	O
αk	O
(	O
cid:2	O
)	O
)	O
]	O
(	O
21.155	O
)	O
now	O
exp	O
(	O
ψ	O
(	O
x	O
)	O
)	O
≈	O
x	O
−	O
0.5	O
for	O
x	O
>	O
1	O
.	O
so	O
if	O
αk	O
=	O
0	O
,	O
when	O
we	O
compute	O
˜πk	O
,	O
it	O
’	O
s	O
like	O
we	O
substract	O
0.5	O
from	O
the	O
posterior	O
counts	O
.	O
this	O
will	O
hurt	O
small	O
clusters	O
more	O
than	O
large	O
clusters	O
(	O
like	O
a	O
regressive	O
tax	O
)	O
.4	O
the	O
effect	O
is	O
that	O
clusters	B
which	O
have	O
very	O
few	O
(	O
weighted	O
)	O
members	O
become	O
more	O
and	O
more	O
empty	O
over	O
successive	O
iterations	O
,	O
whereas	O
the	O
popular	O
clusters	B
get	O
more	O
and	O
more	O
members	O
.	O
this	O
is	O
called	O
the	O
rich	B
get	I
richer	I
phenomenon	O
;	O
we	O
will	O
encounter	O
it	O
again	O
in	O
section	O
25.2	O
,	O
when	O
we	O
discuss	O
dirichlet	O
process	O
mixture	B
models	O
.	O
this	O
automatic	O
pruning	O
method	O
is	O
demonstrated	O
in	O
figure	O
21.8.	O
we	O
ﬁt	O
a	O
mixture	O
of	O
6	O
gaussians	O
to	O
the	O
old	O
faithful	B
dataset	O
,	O
but	O
the	O
data	O
only	O
really	O
“	O
needs	O
”	O
2	O
clusters	B
,	O
so	O
the	O
rest	O
get	O
“	O
killed	O
off	O
”	O
.	O
4.	O
for	O
more	O
details	O
,	O
see	O
(	O
liang	O
et	O
al	O
.	O
2007	O
)	O
.	O
756	O
chapter	O
21.	O
variational	B
inference	I
in	O
this	O
example	O
,	O
we	O
used	O
α0	O
=	O
0.001	O
;	O
if	O
we	O
use	O
a	O
larger	O
α0	O
,	O
we	O
do	O
not	O
get	O
a	O
sparsity	B
effect	O
.	O
in	O
figure	O
21.9	O
,	O
we	O
plot	O
q	O
(	O
α|d	O
)	O
at	O
various	O
iterations	O
;	O
we	O
see	O
that	O
the	O
unwanted	O
components	O
get	O
extinguished	O
.	O
this	O
provides	O
an	O
efficient	O
alternative	O
to	O
performing	O
a	O
discrete	B
search	O
over	O
the	O
number	O
of	O
clusters	B
.	O
21.7	O
variational	B
message	I
passing	I
and	O
vibes	O
we	O
have	O
seen	O
that	O
mean	B
ﬁeld	I
methods	O
,	O
at	O
least	O
of	O
the	O
fully-factorized	O
variety	O
,	O
are	O
all	O
very	O
similar	B
:	O
just	O
compute	O
each	O
node	O
’	O
s	O
full	B
conditional	I
,	O
and	O
average	O
out	O
the	O
neighbors	B
.	O
this	O
is	O
very	O
similar	B
to	O
gibbs	O
sampling	O
(	O
section	O
24.2	O
)	O
,	O
except	O
the	O
derivation	O
of	O
the	O
equations	O
is	O
usually	O
a	O
bit	O
more	O
work	O
.	O
fortunately	O
it	O
is	O
possible	O
to	O
derive	O
a	O
general	O
purpose	O
set	O
of	O
update	O
equations	O
that	O
work	O
for	O
any	O
dgm	O
for	O
which	O
all	O
cpds	O
are	O
in	O
the	O
exponential	B
family	I
,	O
and	O
for	O
which	O
all	O
parent	O
nodes	B
have	O
conjugate	O
distributions	O
(	O
ghahramani	O
and	O
beal	O
2001	O
)	O
.	O
(	O
see	O
(	O
wand	O
et	O
al	O
.	O
2011	O
)	O
for	O
a	O
recent	O
extension	B
to	O
handle	O
non-conjugate	O
priors	O
.	O
)	O
one	O
can	O
then	O
sweep	O
over	O
the	O
graph	B
,	O
updating	O
nodes	B
one	O
at	O
a	O
time	O
,	O
in	O
a	O
manner	O
similar	B
to	O
gibbs	O
sampling	O
.	O
this	O
is	O
known	O
as	O
variational	B
message	I
passing	I
or	O
vmp	O
(	O
winn	O
and	O
bishop	O
2005	O
)	O
,	O
and	O
has	O
been	O
implemented	O
in	O
the	O
open-source	O
program	O
vibes5	O
.	O
this	O
is	O
a	O
vb	O
analog	O
to	O
bugs	O
,	O
which	O
is	O
a	O
popular	O
generic	O
program	O
for	O
gibbs	O
sampling	O
discussed	O
in	O
section	O
24.2.6.	O
vmp/	O
mean	B
ﬁeld	I
is	O
best-suited	O
to	O
inference	B
where	O
one	O
or	O
more	O
of	O
the	O
hidden	B
nodes	I
are	O
continuous	O
(	O
e.g.	O
,	O
when	O
performing	O
“	O
bayesian	O
learning	B
”	O
)	O
.	O
for	O
models	O
where	O
all	O
the	O
hidden	B
nodes	I
are	O
discrete	B
,	O
more	O
accurate	O
approximate	B
inference	I
algorithms	O
can	O
be	O
used	O
,	O
as	O
we	O
discuss	O
in	O
chapter	O
22	O
.	O
21.8	O
local	O
variational	O
bounds	O
*	O
so	O
far	O
,	O
we	O
have	O
been	O
focusing	O
on	O
mean	B
ﬁeld	I
inference	O
,	O
which	O
is	O
a	O
form	O
of	O
variational	B
inference	I
based	O
on	O
minimizing	O
kl	O
(	O
q||˜p	O
)	O
,	O
where	O
q	O
is	O
the	O
approximate	O
posterior	O
,	O
assumed	O
to	O
be	O
factorized	O
,	O
and	O
˜p	O
is	O
the	O
exact	O
(	O
but	O
unnormalized	O
)	O
posterior	O
.	O
however	O
,	O
there	O
is	O
another	O
kind	O
of	O
variational	B
inference	I
,	O
where	O
we	O
replace	O
a	O
speciﬁc	O
term	O
in	O
the	O
joint	B
distribution	I
with	O
a	O
simpler	O
function	O
,	O
to	O
simplify	O
computation	O
of	O
the	O
posterior	O
.	O
such	O
an	O
approach	O
is	O
sometimes	O
called	O
a	O
local	B
variational	I
approximation	I
,	O
since	O
we	O
are	O
only	O
modifying	O
one	O
piece	O
of	O
the	O
model	O
,	O
unlike	O
mean	B
ﬁeld	I
,	O
which	O
is	O
a	O
global	O
approximation	O
.	O
in	O
this	O
section	O
,	O
we	O
study	O
several	O
examples	O
of	O
this	O
method	O
.	O
21.8.1	O
motivating	O
applications	O
before	O
we	O
explain	O
how	O
to	O
derive	O
local	O
variational	O
bounds	O
,	O
we	O
give	O
some	O
examples	O
of	O
where	O
this	O
is	O
useful	O
.	O
21.8.1.1	O
variational	O
logistic	O
regression	B
consider	O
the	O
problem	O
of	O
how	O
to	O
approximate	O
the	O
parameter	B
posterior	O
for	O
multiclass	O
logistic	O
regression	B
model	O
under	O
a	O
gaussian	O
prior	O
.	O
one	O
approach	O
is	O
to	O
use	O
a	O
gaussian	O
(	O
laplace	O
)	O
approx-	O
imation	O
,	O
as	O
discussed	O
in	O
section	O
8.4.3.	O
however	O
,	O
a	O
variational	O
approach	O
can	O
produce	O
a	O
more	O
5.	O
available	O
at	O
http	O
:	O
//vibes.sourceforge.net/	O
.	O
21.8.	O
local	O
variational	O
bounds	O
*	O
757	O
accurate	O
approximation	O
to	O
the	O
posterior	O
,	O
since	O
it	O
has	O
tunable	O
parameters	O
.	O
another	O
advantage	O
is	O
that	O
the	O
variational	O
approach	O
monotonically	O
optimizes	O
a	O
lower	O
bound	O
on	O
the	O
likelihood	B
of	O
the	O
data	O
,	O
as	O
we	O
will	O
see	O
.	O
to	O
see	O
why	O
we	O
need	O
a	O
bound	O
,	O
note	O
that	O
the	O
likelihood	B
can	O
be	O
written	O
as	O
follows	O
:	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
i	O
ηi	O
−	O
lse	B
(	O
ηi	O
)	O
yt	O
(	O
21.156	O
)	O
(	O
cid:14	O
)	O
(	O
cid:13	O
)	O
n	O
(	O
cid:20	O
)	O
exp	O
i=1	O
(	O
cid:10	O
)	O
m	O
(	O
cid:4	O
)	O
(	O
cid:11	O
)	O
where	O
ηi	O
=	O
xiwi	O
=	O
[	O
xt	O
identiﬁability	O
)	O
,	O
and	O
where	O
we	O
deﬁne	O
the	O
log-sum-exp	B
or	O
lse	B
function	O
as	O
follows	O
:	O
i	O
w1	O
,	O
.	O
.	O
.	O
,	O
xt	O
i	O
wm	O
]	O
,	O
where	O
m	O
=	O
c	O
−	O
1	O
(	O
since	O
we	O
set	O
wc	O
=	O
0	O
for	O
lse	B
(	O
ηi	O
)	O
(	O
cid:2	O
)	O
log	O
1	O
+	O
eηim	O
(	O
21.157	O
)	O
m=1	O
the	O
main	O
problem	O
is	O
that	O
this	O
likelihood	B
is	O
not	O
conjugate	O
to	O
the	O
gaussian	O
prior	O
.	O
below	O
we	O
discuss	O
how	O
to	O
compute	O
“	O
gaussian-like	O
”	O
lower	O
bounds	O
to	O
this	O
likelihood	B
,	O
which	O
give	O
rise	O
to	O
approximate	O
gaussian	O
posteriors	O
.	O
21.8.1.2	O
multi-task	B
learning	I
one	O
important	O
application	O
of	O
bayesian	O
inference	B
for	O
logistic	B
regression	I
is	O
where	O
we	O
have	O
multiple	O
related	O
classiﬁers	O
we	O
want	O
to	O
ﬁt	O
.	O
in	O
this	O
case	O
,	O
we	O
want	O
to	O
share	O
information	B
between	O
the	O
parameters	O
for	O
each	O
classiﬁer	O
;	O
this	O
requires	O
that	O
we	O
maintain	O
a	O
posterior	O
distibution	O
over	O
the	O
parameters	O
,	O
so	O
we	O
have	O
a	O
measure	O
of	O
conﬁdence	O
as	O
well	O
as	O
an	O
estimate	O
of	O
the	O
value	O
.	O
we	O
can	O
embed	O
the	O
above	O
variational	O
method	O
inside	O
of	O
a	O
larger	O
hierarchical	O
model	O
in	O
order	O
to	O
perform	O
such	O
multi-task	B
learning	I
,	O
as	O
described	O
in	O
e.g.	O
,	O
(	O
braun	O
and	O
mcauliffe	O
2010	O
)	O
.	O
21.8.1.3	O
discrete	B
factor	O
analysis	O
another	O
situation	O
where	O
variational	O
bounds	O
are	O
useful	O
arises	O
when	O
we	O
ﬁt	O
a	O
factor	B
analysis	I
model	O
to	O
discrete	B
data	O
.	O
this	O
model	O
is	O
just	O
like	O
multinomial	B
logistic	I
regression	I
,	O
except	O
the	O
input	O
variables	O
are	O
hidden	B
factors	O
.	O
we	O
need	O
to	O
perform	O
inference	B
on	O
the	O
hidden	B
variables	I
as	O
well	O
as	O
the	O
regression	B
weights	O
.	O
for	O
simplicity	O
,	O
we	O
might	O
perform	O
point	O
estimation	O
of	O
the	O
weights	O
,	O
and	O
just	O
integrate	B
out	I
the	O
hidden	B
variables	I
.	O
we	O
can	O
do	O
this	O
using	O
variational	O
em	O
,	O
where	O
we	O
use	O
the	O
variational	O
bound	O
in	O
the	O
e	O
step	O
.	O
see	O
section	O
12.4	O
for	O
details	O
.	O
21.8.1.4	O
correlated	B
topic	I
model	I
a	O
topic	B
model	I
is	O
a	O
latent	O
variable	O
model	O
for	O
text	O
documents	O
and	O
other	O
forms	O
of	O
discrete	B
data	O
;	O
see	O
section	O
27.3	O
for	O
details	O
.	O
often	O
we	O
assume	O
the	O
distribution	O
over	O
topics	O
has	O
a	O
dirichlet	O
prior	O
,	O
but	O
a	O
more	O
powerful	O
model	O
,	O
known	O
as	O
the	O
correlated	B
topic	I
model	I
,	O
uses	O
a	O
gaussian	O
prior	O
,	O
which	O
can	O
model	O
correlations	O
more	O
easily	O
(	O
see	O
section	O
27.4.1	O
for	O
details	O
)	O
.	O
unfortunately	O
,	O
this	O
also	O
involves	O
the	O
lse	B
function	O
.	O
however	O
,	O
we	O
can	O
use	O
our	O
variational	O
bounds	O
in	O
the	O
context	O
of	O
a	O
variational	O
em	O
algorithm	O
,	O
as	O
we	O
will	O
see	O
later	O
.	O
758	O
chapter	O
21.	O
variational	B
inference	I
21.8.2	O
bohning	O
’	O
s	O
quadratic	O
bound	O
to	O
the	O
log-sum-exp	B
function	O
all	O
of	O
the	O
above	O
examples	O
require	O
dealing	O
with	O
multiplying	O
a	O
gaussian	O
prior	O
by	O
a	O
multinomial	B
likelihood	O
;	O
this	O
is	O
difficult	O
because	O
of	O
the	O
log-sum-exp	B
(	O
lse	B
)	O
term	O
.	O
in	O
this	O
section	O
,	O
we	O
derive	O
a	O
way	O
to	O
derive	O
a	O
“	O
gaussian-like	O
”	O
lower	O
bound	O
on	O
this	O
likelihood	B
.	O
consider	O
a	O
taylor	O
series	O
expansion	O
of	O
the	O
lse	B
function	O
around	O
ψi	O
∈	O
r	O
lse	B
(	O
ηi	O
)	O
=	O
lse	B
(	O
ψi	O
)	O
+	O
(	O
ηi	O
−	O
ψi	O
)	O
t	O
g	O
(	O
ψi	O
)	O
+	O
g	O
(	O
ψi	O
)	O
=	O
exp	O
[	O
ψi	O
−	O
lse	B
(	O
ψi	O
)	O
]	O
=	O
s	O
(	O
ψi	O
)	O
h	O
(	O
ψi	O
)	O
=	O
diag	O
(	O
g	O
(	O
ψi	O
)	O
)	O
−	O
g	O
(	O
ψi	O
)	O
g	O
(	O
ψi	O
)	O
t	O
1	O
2	O
(	O
ηi	O
−	O
ψi	O
)	O
t	O
h	O
(	O
ψi	O
)	O
(	O
ηi	O
−	O
ψi	O
)	O
m	O
:	O
(	O
21.158	O
)	O
(	O
21.159	O
)	O
(	O
21.160	O
)	O
(	O
cid:15	O
)	O
where	O
g	O
and	O
h	O
are	O
the	O
gradient	O
and	O
hessian	O
of	O
lse	B
,	O
and	O
ψi	O
∈	O
r	O
m	O
is	O
chosen	O
such	O
that	O
equality	O
holds	O
.	O
an	O
upper	O
bound	O
to	O
lse	B
can	O
be	O
found	O
by	O
replacing	O
the	O
hessian	O
matrix	O
h	O
(	O
ψi	O
)	O
with	O
a	O
matrix	O
ai	O
such	O
that	O
ai	O
≺	O
h	O
(	O
ψi	O
)	O
.	O
(	O
bohning	O
1992	O
)	O
showed	O
that	O
this	O
can	O
be	O
achieved	O
if	O
we	O
use	O
the	O
matrix	O
ai	O
=	O
1	O
(	O
recall	B
that	O
m	O
+	O
1	O
=	O
c	O
is	O
the	O
number	O
of	O
classes	O
.	O
)	O
2	O
note	O
that	O
ai	O
is	O
independent	O
of	O
ψi	O
;	O
however	O
,	O
we	O
still	O
write	O
it	O
as	O
ai	O
(	O
rather	O
than	O
dropping	O
the	O
i	O
subscript	O
)	O
,	O
since	O
other	O
bounds	O
that	O
we	O
consider	O
below	O
will	O
have	O
a	O
data-dependent	O
curvature	O
term	O
.	O
the	O
upper	O
bound	O
on	O
lse	B
therefore	O
becomes	O
im	O
−	O
1	O
m	O
+1	O
1m	O
1t	O
(	O
cid:16	O
)	O
m	O
.	O
(	O
cid:18	O
)	O
i	O
ηi	O
+	O
ci	O
1m	O
1t	O
m	O
lse	B
(	O
ηi	O
)	O
≤	O
1	O
2	O
1	O
2	O
(	O
cid:17	O
)	O
i	O
aiηi	O
−	O
bt	O
ηt	O
im	O
−	O
1	O
ai	O
=	O
m	O
+	O
1	O
bi	O
=	O
aiψi	O
−	O
g	O
(	O
ψi	O
)	O
ci	O
=	O
where	O
ψi	O
∈	O
r	O
1	O
2	O
i	O
aiψi	O
−	O
g	O
(	O
ψi	O
)	O
t	O
ψi	O
+	O
lse	B
(	O
ψi	O
)	O
ψt	O
m	O
is	O
a	O
vector	O
of	O
variational	O
parameters	O
.	O
(	O
cid:17	O
)	O
we	O
can	O
use	O
the	O
above	O
result	O
to	O
get	O
the	O
following	O
lower	O
bound	O
on	O
the	O
softmax	B
likelihood	O
:	O
log	O
p	O
(	O
yi	O
=	O
c|xi	O
,	O
w	O
)	O
≥	O
wt	O
xiaixiw	O
+	O
bt	O
i	O
xiw	O
−	O
ci	O
(	O
21.165	O
)	O
i	O
xiw	O
−	O
1	O
yt	O
2	O
(	O
21.161	O
)	O
(	O
21.162	O
)	O
(	O
21.163	O
)	O
(	O
21.164	O
)	O
(	O
cid:18	O
)	O
c	O
(	O
21.166	O
)	O
(	O
21.167	O
)	O
to	O
simplify	O
notation	O
,	O
deﬁne	O
the	O
pseudo-measurement	O
˜yi	O
(	O
cid:2	O
)	O
a−1	O
i	O
(	O
bi	O
+	O
yi	O
)	O
then	O
we	O
can	O
get	O
a	O
“	O
gaussianized	O
”	O
version	O
of	O
the	O
observation	B
model	I
:	O
p	O
(	O
yi|xi	O
,	O
w	O
)	O
≥	O
f	O
(	O
xi	O
,	O
ψi	O
)	O
n	O
(	O
˜yi|xiw	O
,	O
a−1	O
i	O
)	O
where	O
f	O
(	O
xi	O
,	O
ψi	O
)	O
is	O
some	O
function	O
that	O
does	O
not	O
depend	O
on	O
w.	O
given	O
this	O
,	O
it	O
is	O
easy	O
to	O
compute	O
the	O
posterior	O
q	O
(	O
w	O
)	O
=	O
n	O
(	O
mn	O
,	O
vn	O
)	O
,	O
using	O
bayes	O
rule	O
for	O
gaussians	O
.	O
below	O
we	O
will	O
explain	O
how	O
to	O
update	O
the	O
variational	O
parameters	O
ψi	O
.	O
21.8.	O
local	O
variational	O
bounds	O
*	O
759	O
21.8.2.1	O
applying	O
bohning	O
’	O
s	O
bound	O
to	O
multinomial	B
logistic	I
regression	I
let	O
us	O
see	O
how	O
to	O
apply	O
this	O
bound	O
to	O
multinomial	B
logistic	I
regression	I
.	O
from	O
equation	O
21.13	O
,	O
we	O
can	O
deﬁne	O
the	O
goal	O
of	O
variational	B
inference	I
as	O
maximizing	O
i=1	O
(	O
cid:25	O
)	O
(	O
cid:25	O
)	O
log	O
p	O
(	O
yi|xi	O
,	O
w	O
)	O
l	O
(	O
q	O
)	O
(	O
cid:2	O
)	O
−kl	O
(	O
q	O
(	O
w	O
)	O
||p	O
(	O
w|d	O
)	O
)	O
+	O
eq	O
(	O
cid:24	O
)	O
(	O
cid:24	O
)	O
=	O
−kl	O
(	O
q	O
(	O
w	O
)	O
||p	O
(	O
w|d	O
)	O
)	O
+	O
eq	O
n	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
i	O
eq	O
[	O
ηi	O
]	O
−	O
n	O
(	O
cid:4	O
)	O
(	O
cid:13	O
)	O
−kl	O
(	O
n	O
(	O
m0	O
,	O
v0	O
)	O
||n	O
(	O
mn	O
,	O
vn	O
)	O
)	O
=	O
−	O
1	O
2	O
+	O
(	O
mn	O
−	O
m0	O
)	O
t	O
v−1	O
=	O
−kl	O
(	O
q	O
(	O
w	O
)	O
||p	O
(	O
w|d	O
)	O
)	O
+	O
i	O
ηi	O
−	O
lse	B
(	O
ηi	O
)	O
yt	O
i=1	O
yt	O
i=1	O
i=1	O
eq	O
[	O
lse	B
(	O
ηi	O
)	O
]	O
where	O
q	O
(	O
w	O
)	O
=n	O
(	O
w|mn	O
,	O
vn	O
)	O
is	O
the	O
approximate	O
posterior	O
.	O
the	O
ﬁrst	O
term	O
is	O
just	O
the	O
kl	O
divergence	O
between	O
two	O
gaussians	O
,	O
which	O
is	O
given	O
by	O
tr	O
(	O
vn	O
v−1	O
0	O
)	O
−	O
log	O
|vn	O
v−1	O
0	O
|	O
0	O
(	O
mn	O
−	O
m0	O
)	O
−	O
dm	O
(	O
cid:14	O
)	O
(	O
21.171	O
)	O
(	O
21.168	O
)	O
(	O
21.169	O
)	O
(	O
21.170	O
)	O
(	O
21.172	O
)	O
(	O
21.173	O
)	O
(	O
cid:14	O
)	O
(	O
21.174	O
)	O
where	O
dm	O
is	O
the	O
dimensionality	O
of	O
the	O
gaussian	O
,	O
and	O
we	O
assume	O
a	O
prior	O
of	O
the	O
form	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
m0	O
,	O
v0	O
)	O
,	O
where	O
typically	O
μ0	O
=	O
0dm	O
,	O
and	O
v0	O
is	O
block	O
diagonal	B
.	O
the	O
second	O
term	O
is	O
simply	O
n	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
yt	O
i	O
eq	O
[	O
ηi	O
]	O
=	O
yt	O
i	O
˜mi	O
i=1	O
i=1	O
where	O
˜mi	O
(	O
cid:2	O
)	O
ximn	O
.	O
the	O
ﬁnal	O
term	O
can	O
be	O
lower	O
bounded	O
by	O
taking	O
expectations	O
of	O
our	O
quadratic	O
upper	O
bound	O
on	O
lse	B
as	O
follows	O
:	O
−	O
n	O
(	O
cid:4	O
)	O
i=1	O
eq	O
[	O
lse	B
(	O
ηi	O
)	O
]	O
≥	O
−	O
1	O
2	O
tr	O
(	O
ai	O
˜vi	O
)	O
−	O
1	O
2	O
˜miai	O
˜mi	O
+	O
bt	O
i	O
˜mi	O
−	O
ci	O
(	O
cid:13	O
)	O
where	O
˜vi	O
(	O
cid:2	O
)	O
xivn	O
xt	O
lqj	O
(	O
q	O
)	O
≥	O
−	O
1	O
2	O
−	O
1	O
2	O
i	O
.	O
putting	O
it	O
altogether	O
,	O
we	O
have	O
0	O
)	O
−	O
log	O
|vn	O
v−1	O
tr	O
(	O
vn	O
v−1	O
n	O
(	O
cid:4	O
)	O
dm	O
+	O
i=1	O
i	O
˜mi	O
−	O
1	O
yt	O
2	O
tr	O
(	O
ai	O
˜vi	O
)	O
−	O
1	O
2	O
0	O
|	O
+	O
(	O
mn	O
−	O
m0	O
)	O
t	O
v−1	O
0	O
(	O
mn	O
−	O
m0	O
)	O
i	O
˜mi	O
−	O
ci	O
˜miai	O
˜mi	O
+	O
bt	O
this	O
lower	O
bound	O
combines	O
jensen	O
’	O
s	O
inequality	O
(	O
as	O
in	O
mean	B
ﬁeld	I
inference	O
)	O
,	O
plus	O
the	O
quadratic	O
lower	O
bound	O
due	O
to	O
the	O
lse	B
term	O
,	O
so	O
we	O
write	O
it	O
as	O
lqj	O
.	O
we	O
will	O
use	O
coordinate	O
ascent	O
to	O
optimize	O
this	O
lower	O
bound	O
.	O
that	O
is	O
,	O
we	O
update	O
the	O
variational	O
posterior	O
parameters	O
vn	O
and	O
mn	O
,	O
and	O
then	O
the	O
variational	O
likelihood	O
parameters	O
ψi	O
.	O
we	O
leave	O
760	O
chapter	O
21.	O
variational	B
inference	I
(	O
cid:11	O
)	O
−1	O
n	O
(	O
cid:4	O
)	O
i=1	O
xt	O
i	O
aixi	O
n	O
(	O
cid:4	O
)	O
i=1	O
(	O
cid:10	O
)	O
(	O
cid:10	O
)	O
the	O
detailed	O
derivation	O
as	O
an	O
exercise	O
,	O
and	O
just	O
state	B
the	O
results	O
.	O
we	O
have	O
vn	O
=	O
v0	O
+	O
(	O
cid:10	O
)	O
mn	O
=	O
vn	O
v−1	O
0	O
m0	O
+	O
xt	O
i	O
(	O
yi	O
+	O
bi	O
)	O
(	O
cid:11	O
)	O
(	O
21.175	O
)	O
(	O
21.176	O
)	O
ψi	O
=	O
˜mi	O
=	O
ximn	O
(	O
21.177	O
)	O
we	O
can	O
exploit	O
the	O
fact	O
that	O
ai	O
is	O
a	O
constant	O
matrix	O
,	O
plus	O
the	O
fact	O
that	O
xi	O
has	O
block	O
structure	O
,	O
to	O
simplify	O
the	O
ﬁrst	O
two	O
terms	O
as	O
follows	O
:	O
vn	O
=	O
(	O
cid:11	O
)	O
−1	O
v0	O
+	O
a	O
⊗	O
n	O
(	O
cid:4	O
)	O
(	O
cid:10	O
)	O
i=1	O
v−1	O
xixt	O
i	O
n	O
(	O
cid:4	O
)	O
(	O
cid:11	O
)	O
(	O
21.178	O
)	O
0	O
m0	O
+	O
mn	O
=	O
vn	O
(	O
21.179	O
)	O
where	O
⊗	O
denotes	O
the	O
kronecker	B
product	I
.	O
see	O
algorithm	O
15	O
for	O
some	O
pseudocode	O
,	O
and	O
http	O
:	O
//www.cs.ubc.ca/~emtiyaz/software/catlgm.html	O
for	O
some	O
matlab	O
code	O
.	O
i=1	O
(	O
yi	O
+	O
bi	O
)	O
⊗	O
xi	O
inference	B
for	O
multi-class	B
logistic	I
regression	I
using	O
bohning	O
’	O
s	O
d	O
,	O
i	O
=	O
1	O
:	O
n	O
,	O
prior	O
m0	O
,	O
v0	O
;	O
algorithm	O
21.1	O
:	O
variational	O
bound	O
1	O
input	O
:	O
yi	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
}	O
,	O
xi	O
∈	O
r	O
2	O
deﬁne	O
m	O
:	O
=	O
c	O
−	O
1	O
;	O
dummy	O
encode	O
yi	O
∈	O
{	O
0	O
,	O
1	O
}	O
m	O
;	O
deﬁne	O
xi	O
=	O
blockdiag	O
(	O
xt	O
i	O
)	O
;	O
m	O
+1	O
1m	O
1t	O
3	O
deﬁne	O
y	O
:	O
=	O
[	O
y1	O
;	O
.	O
.	O
.	O
;	O
yn	O
]	O
,	O
x	O
:	O
=	O
[	O
x1	O
;	O
.	O
.	O
.	O
;	O
xn	O
]	O
and	O
a	O
:	O
=	O
1	O
2	O
4	O
vn	O
:	O
=	O
5	O
initialize	O
mn	O
:	O
=	O
m0	O
;	O
6	O
repeat	O
7	O
im	O
−	O
1	O
(	O
cid:7	O
)	O
n	O
(	O
cid:23	O
)	O
−1	O
i=1	O
xt	O
i	O
axi	O
v−1	O
0	O
+	O
(	O
cid:15	O
)	O
(	O
cid:22	O
)	O
m	O
;	O
(	O
cid:16	O
)	O
;	O
8	O
9	O
10	O
11	O
12	O
ψ	O
:	O
=	O
xmn	O
;	O
ψ	O
:	O
=	O
reshape	O
(	O
m	O
,	O
m	O
,	O
n	O
)	O
;	O
g	O
:	O
=	O
exp	O
(	O
ψ	O
−	O
lse	B
(	O
ψ	O
)	O
)	O
;	O
b	O
:	O
=	O
aψ	O
−	O
g	O
;	O
(	O
cid:22	O
)	O
b	O
:	O
=	O
(	O
b	O
)	O
;	O
v−1	O
mn	O
:	O
=	O
vn	O
compute	O
the	O
lower	O
bound	O
lqj	O
using	O
equation	O
21.174	O
;	O
0	O
m0	O
+	O
xt	O
(	O
y	O
+	O
b	O
)	O
(	O
cid:23	O
)	O
;	O
13	O
14	O
until	O
converged	O
;	O
15	O
return	O
mn	O
and	O
vn	O
;	O
21.8.3	O
bounds	O
for	O
the	O
sigmoid	B
function	O
in	O
many	O
models	O
,	O
we	O
just	O
have	O
binary	O
data	O
.	O
ηi	O
=	O
wt	O
xi	O
where	O
w	O
∈	O
r	O
d	O
is	O
a	O
weight	B
vector	I
(	O
not	O
matrix	O
)	O
.	O
in	O
this	O
case	O
,	O
we	O
have	O
yi	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
m	O
=	O
1	O
and	O
in	O
this	O
case	O
,	O
the	O
bohning	O
bound	O
21.8.	O
local	O
variational	O
bounds	O
*	O
761	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−6	O
bohning	O
bound	O
,	O
χ=−2.5	O
−4	O
−2	O
0	O
2	O
4	O
6	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−6	O
jj	O
bound	O
,	O
χ=2.5	O
−4	O
−2	O
0	O
2	O
4	O
6	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
21.10	O
quadratic	O
lower	O
bounds	O
on	O
the	O
sigmoid	B
(	O
logistic	B
)	O
function	O
.	O
in	O
solid	O
red	O
,	O
we	O
plot	O
sigm	O
(	O
x	O
)	O
vs	O
x.	O
in	O
dotted	O
blue	O
,	O
we	O
plot	O
the	O
lower	O
bound	O
l	O
(	O
x	O
,	O
ξ	O
)	O
vs	O
x	O
for	O
ξ	O
=	O
2.5	O
.	O
(	O
a	O
)	O
bohning	O
bound	O
.	O
this	O
is	O
tight	O
at	O
−ξ	O
=	O
2.5	O
.	O
(	O
b	O
)	O
jj	O
bound	O
.	O
this	O
is	O
tight	O
at	O
ξ	O
=	O
±2.5	O
.	O
figure	O
generated	O
by	O
sigmoidlowerbounds	O
.	O
becomes	O
aη2	O
−	O
bη	O
+	O
c	O
log	O
(	O
1	O
+	O
eη	O
)	O
≤	O
1	O
2	O
1	O
4	O
a	O
=	O
b	O
=	O
aψ	O
−	O
(	O
1	O
+	O
e−ψ	O
)	O
c	O
=	O
−1	O
aψ2	O
−	O
(	O
1	O
+	O
e−ψ	O
)	O
1	O
2	O
−1ψ	O
+	O
log	O
(	O
1	O
+	O
eψ	O
)	O
(	O
21.180	O
)	O
(	O
21.181	O
)	O
(	O
21.182	O
)	O
(	O
21.183	O
)	O
it	O
is	O
possible	O
to	O
derive	O
an	O
alternative	O
quadratic	O
bound	O
for	O
this	O
case	O
,	O
as	O
shown	O
in	O
(	O
jaakkola	O
and	O
jordan	O
1996b	O
,	O
2000	O
)	O
.	O
this	O
has	O
the	O
following	O
form	O
log	O
(	O
1	O
+	O
eη	O
)	O
≤	O
λ	O
(	O
ξ	O
)	O
(	O
η2	O
−	O
ξ2	O
)	O
+	O
λ	O
(	O
ξ	O
)	O
(	O
cid:2	O
)	O
1	O
4ξ	O
tanh	O
(	O
ξ/2	O
)	O
=	O
(	O
cid:17	O
)	O
(	O
η	O
−	O
ξ	O
)	O
+	O
log	O
(	O
1	O
+e	O
ξ	O
)	O
1	O
2	O
1	O
2ξ	O
sigm	O
(	O
ξ	O
)	O
−	O
1	O
2	O
(	O
cid:18	O
)	O
we	O
shall	O
refer	O
to	O
this	O
as	O
the	O
jj	O
bound	O
,	O
after	O
its	O
inventors	O
,	O
(	O
jaakkola	O
and	O
jordan	O
1996b	O
,	O
2000	O
)	O
.	O
to	O
facilitate	O
comparison	O
with	O
bohning	O
’	O
s	O
bound	O
,	O
let	O
us	O
rewrite	O
the	O
jj	O
bound	O
as	O
a	O
quadratic	O
form	O
as	O
follows	O
(	O
21.184	O
)	O
(	O
21.185	O
)	O
(	O
21.186	O
)	O
(	O
21.187	O
)	O
(	O
21.188	O
)	O
(	O
21.189	O
)	O
log	O
(	O
1	O
+	O
eη	O
)	O
≤	O
1	O
2	O
a	O
(	O
ξ	O
)	O
η2	O
−	O
b	O
(	O
ξ	O
)	O
η	O
+	O
c	O
(	O
ξ	O
)	O
a	O
(	O
ξ	O
)	O
=	O
2λ	O
(	O
ξ	O
)	O
b	O
(	O
ξ	O
)	O
=−	O
1	O
2	O
c	O
(	O
ξ	O
)	O
=−λ	O
(	O
ξ	O
)	O
ξ2	O
−	O
1	O
2	O
ξ	O
+	O
log	O
(	O
1	O
+	O
eξ	O
)	O
the	O
jj	O
bound	O
has	O
an	O
adaptive	O
curvature	O
term	O
,	O
since	O
a	O
depends	O
on	O
ξ.	O
in	O
addition	O
,	O
it	O
is	O
tight	O
at	O
two	O
points	O
,	O
as	O
is	O
evident	O
from	O
figure	O
21.10	O
(	O
b	O
)	O
.	O
by	O
contrast	O
,	O
the	O
bohning	O
bound	O
is	O
a	O
constant	O
curvature	O
bound	O
,	O
and	O
is	O
only	O
tight	O
at	O
one	O
point	O
,	O
as	O
is	O
evident	O
from	O
figure	O
21.10	O
(	O
a	O
)	O
.	O
762	O
chapter	O
21.	O
variational	B
inference	I
if	O
we	O
wish	O
to	O
use	O
the	O
jj	O
bound	O
for	O
binary	O
logistic	O
regression	B
,	O
we	O
can	O
make	O
some	O
small	O
modiﬁcations	O
to	O
algorithm	O
15.	O
first	O
,	O
we	O
use	O
the	O
new	O
deﬁnitions	O
for	O
ai	O
,	O
bi	O
and	O
ci	O
.	O
the	O
fact	O
that	O
ai	O
is	O
not	O
constant	O
when	O
using	O
the	O
jj	O
bound	O
,	O
unlike	O
when	O
using	O
the	O
bohning	O
bound	O
,	O
means	O
we	O
can	O
not	O
compute	O
vn	O
outside	O
of	O
the	O
main	O
loop	B
,	O
making	O
the	O
method	O
a	O
constant	O
factor	O
slower	O
.	O
next	O
we	O
note	O
that	O
xi	O
=	O
xt	O
i	O
,	O
so	O
the	O
updates	O
for	O
the	O
posterior	O
become	O
n	O
(	O
cid:4	O
)	O
i=1	O
n	O
=	O
v−1	O
v−1	O
(	O
cid:10	O
)	O
0	O
+	O
2	O
mn	O
=	O
vn	O
v−1	O
0	O
m0	O
+	O
λ	O
(	O
ξi	O
)	O
xixt	O
i	O
n	O
(	O
cid:4	O
)	O
i=1	O
(	O
yi	O
−	O
1	O
2	O
)	O
xi	O
(	O
cid:11	O
)	O
(	O
21.190	O
)	O
(	O
21.191	O
)	O
finally	O
,	O
to	O
compute	O
the	O
update	O
for	O
ξi	O
,	O
we	O
isolate	O
the	O
terms	O
in	O
lqj	O
that	O
depend	O
on	O
ξi	O
to	O
get	O
(	O
cid:13	O
)	O
(	O
cid:14	O
)	O
)	O
i	O
eq	O
wwt	O
xi	O
−	O
ξ2	O
i	O
)	O
+	O
const	O
(	O
21.192	O
)	O
n	O
(	O
cid:4	O
)	O
(	O
i=1	O
l	O
(	O
ξ	O
)	O
=	O
optimizing	O
this	O
wrt	O
ξi	O
gives	O
the	O
equation	O
ln	O
sigm	O
(	O
ξi	O
)	O
−	O
ξi/2	O
−	O
λ	O
(	O
ξi	O
)	O
(	O
xt	O
(	O
cid:13	O
)	O
(	O
cid:14	O
)	O
0	O
=λ	O
(	O
cid:2	O
)	O
(	O
ξi	O
)	O
(	O
xt	O
i	O
eq	O
wwt	O
(	O
21.193	O
)	O
(	O
ξi	O
)	O
is	O
monotonic	O
for	O
ξi	O
≥	O
0	O
,	O
and	O
we	O
do	O
not	O
need	O
to	O
consider	O
negative	O
values	O
of	O
ξi	O
by	O
now	O
λ	O
(	O
cid:2	O
)	O
symmetry	O
of	O
the	O
bound	O
around	O
ξi	O
=	O
0	O
(	O
see	O
figure	O
21.10	O
)	O
.	O
hence	O
the	O
only	O
way	O
to	O
make	O
the	O
above	O
expression	O
0	O
is	O
if	O
we	O
have	O
(	O
xt	O
i	O
)	O
=	O
0.	O
hence	O
the	O
update	O
becomes	O
xi	O
−	O
ξ2	O
wwt	O
(	O
cid:14	O
)	O
i	O
e	O
xi	O
−	O
ξ2	O
i	O
)	O
(	O
cid:13	O
)	O
(	O
ξnew	O
i	O
)	O
2	O
=	O
xt	O
i	O
(	O
vn	O
+	O
mn	O
mt	O
n	O
)	O
xi	O
(	O
21.194	O
)	O
although	O
the	O
jj	O
bound	O
is	O
tighter	O
than	O
the	O
bohning	O
bound	O
,	O
sometimes	O
it	O
is	O
not	O
tight	O
enough	O
in	O
order	O
to	O
estimate	O
the	O
posterior	O
covariance	O
accurately	O
.	O
a	O
more	O
accurate	O
approach	O
,	O
which	O
uses	O
a	O
piecewise	O
quadratic	O
upper	O
bound	O
to	O
lse	B
,	O
is	O
described	O
in	O
(	O
marlin	O
et	O
al	O
.	O
2011	O
)	O
.	O
by	O
increasing	O
the	O
number	O
of	O
pieces	O
,	O
the	O
bound	O
can	O
be	O
made	O
arbitrarily	O
tight	O
.	O
21.8.4	O
other	O
bounds	O
and	O
approximations	O
to	O
the	O
log-sum-exp	B
function	O
*	O
there	O
are	O
several	O
other	O
bounds	O
and	O
approximations	O
to	O
the	O
multiclass	O
lse	O
function	O
which	O
we	O
can	O
use	O
,	O
which	O
we	O
brieﬂy	O
summarize	O
below	O
.	O
note	O
,	O
however	O
,	O
that	O
all	O
of	O
these	O
require	O
numerical	O
optimization	O
methods	O
to	O
compute	O
mn	O
and	O
vn	O
,	O
making	O
them	O
more	O
complicated	O
to	O
implement	O
.	O
21.8.4.1	O
product	O
of	O
sigmoids	O
(	O
cid:10	O
)	O
k	O
(	O
cid:4	O
)	O
(	O
cid:11	O
)	O
k	O
(	O
cid:4	O
)	O
the	O
approach	O
in	O
(	O
bouchard	O
2007	O
)	O
exploits	O
the	O
fact	O
that	O
log	O
eηk	O
≤	O
α	O
+	O
log	O
(	O
1	O
+	O
eηk−α	O
)	O
(	O
21.195	O
)	O
k=1	O
k=1	O
it	O
then	O
applies	O
the	O
jj	O
bound	O
to	O
the	O
term	O
on	O
the	O
right	O
.	O
21.8.	O
local	O
variational	O
bounds	O
*	O
21.8.4.2	O
jensen	O
’	O
s	O
inequality	O
the	O
approach	O
in	O
(	O
blei	O
and	O
lafferty	O
2006a	O
,	O
2007	O
)	O
uses	O
jensen	O
’	O
s	O
inequality	O
as	O
follows	O
:	O
eq	O
[	O
lse	B
(	O
ηi	O
)	O
]	O
=	O
eq	O
log	O
exp	O
(	O
xt	O
i	O
wc	O
)	O
(	O
cid:11	O
)	O
(	O
cid:25	O
)	O
(	O
cid:14	O
)	O
(	O
cid:11	O
)	O
(	O
cid:24	O
)	O
(	O
cid:10	O
)	O
(	O
cid:10	O
)	O
m	O
(	O
cid:4	O
)	O
(	O
cid:13	O
)	O
c=1	O
1	O
+	O
(	O
cid:10	O
)	O
m	O
(	O
cid:4	O
)	O
m	O
(	O
cid:4	O
)	O
c=1	O
≤	O
log	O
≤	O
log	O
1	O
+	O
eq	O
exp	O
(	O
xt	O
i	O
wc	O
)	O
1	O
+	O
c=1	O
exp	O
(	O
xt	O
i	O
mn	O
,	O
c	O
+	O
xt	O
i	O
vn	O
,	O
ccxi	O
)	O
1	O
2	O
(	O
cid:11	O
)	O
763	O
(	O
21.196	O
)	O
(	O
21.197	O
)	O
(	O
21.198	O
)	O
where	O
the	O
last	O
term	O
follows	O
from	O
the	O
mean	B
of	O
a	O
log-normal	O
distribution	O
,	O
which	O
is	O
eμ+σ2/2	O
.	O
21.8.4.3	O
multivariate	B
delta	I
method	I
the	O
approach	O
in	O
(	O
ahmed	O
and	O
xing	O
2007	O
;	O
braun	O
and	O
mcauliffe	O
2010	O
)	O
uses	O
the	O
multivariate	B
delta	I
method	I
,	O
which	O
is	O
a	O
way	O
to	O
approximate	O
moments	O
of	O
a	O
function	O
using	O
a	O
taylor	O
series	O
expansion	O
.	O
in	O
more	O
detail	O
,	O
let	O
f	O
(	O
w	O
)	O
be	O
the	O
function	O
of	O
interest	O
.	O
using	O
a	O
second-order	O
approximation	O
around	O
m	O
we	O
have	O
f	O
(	O
w	O
)	O
≈	O
f	O
(	O
m	O
)	O
+	O
(	O
w	O
−	O
m	O
)	O
t	O
g	O
(	O
w	O
−	O
m	O
)	O
+	O
(	O
21.199	O
)	O
where	O
g	O
and	O
h	O
are	O
the	O
gradient	O
and	O
hessian	O
evaluated	O
at	O
m.	O
if	O
q	O
(	O
w	O
)	O
=	O
n	O
(	O
w|m	O
,	O
v	O
)	O
,	O
we	O
have	O
(	O
w	O
−	O
m	O
)	O
t	O
h	O
(	O
w	O
−	O
m	O
)	O
1	O
2	O
eq	O
[	O
f	O
(	O
w	O
)	O
]	O
≈	O
f	O
(	O
m	O
)	O
+	O
1	O
2	O
tr	O
[	O
hv	O
]	O
if	O
we	O
use	O
f	O
(	O
w	O
)	O
=	O
lse	B
(	O
xiw	O
)	O
,	O
we	O
get	O
eq	O
[	O
lse	B
(	O
xiw	O
)	O
]	O
≈	O
lse	B
(	O
xim	O
)	O
+	O
tr	O
[	O
xihxt	O
i	O
v	O
]	O
1	O
2	O
(	O
21.200	O
)	O
(	O
21.201	O
)	O
where	O
g	O
and	O
h	O
for	O
the	O
lse	B
function	O
are	O
deﬁned	O
in	O
equations	O
21.159	O
and	O
21.160	O
.	O
21.8.5	O
variational	B
inference	I
based	O
on	O
upper	O
bounds	O
so	O
far	O
,	O
we	O
have	O
been	O
concentrating	O
on	O
lower	O
bounds	O
.	O
however	O
,	O
sometimes	O
we	O
need	O
to	O
use	O
an	O
upper	O
bound	O
.	O
for	O
example	O
,	O
(	O
saul	O
et	O
al	O
.	O
1996	O
)	O
derives	O
a	O
mean	B
ﬁeld	I
algorithm	O
for	O
sigmoid	B
belief	I
nets	I
,	O
which	O
are	O
dgms	O
in	O
which	O
each	O
cpd	O
is	O
a	O
logistic	B
regression	I
function	O
(	O
neal	O
1992	O
)	O
.	O
unlike	O
the	O
case	O
of	O
ising	O
models	O
,	O
the	O
resulting	O
mrf	O
is	O
not	O
pairwise	O
,	O
but	O
contains	O
higher	O
order	O
interactions	O
.	O
this	O
makes	O
the	O
standard	O
mean	O
ﬁeld	O
updates	O
intractable	O
.	O
in	O
particular	O
,	O
they	O
turn	O
out	O
to	O
involve	O
computing	O
an	O
expression	O
which	O
requires	O
evaluating	O
(	O
cid:15	O
)	O
log	O
(	O
1	O
+	O
e−	O
(	O
cid:2	O
)	O
j∈pai	O
wij	O
xj	O
)	O
e	O
=	O
e	O
(	O
cid:16	O
)	O
(	O
cid:13	O
)	O
−	O
log	O
sigm	O
(	O
wt	O
i	O
xpa	O
(	O
i	O
)	O
)	O
(	O
cid:14	O
)	O
(	O
21.202	O
)	O
(	O
notice	O
the	O
minus	O
sign	O
in	O
front	O
.	O
)	O
(	O
saul	O
et	O
al	O
.	O
1996	O
)	O
show	O
how	O
to	O
derive	O
an	O
upper	O
bound	O
on	O
the	O
sigmoid	B
function	O
so	O
as	O
to	O
make	O
this	O
update	O
tractable	O
,	O
resulting	O
in	O
a	O
monotonically	O
convergent	O
inference	B
procedure	O
.	O
764	O
chapter	O
21.	O
variational	B
inference	I
exercises	O
exercise	O
21.1	O
laplace	O
approximation	O
to	O
p	O
(	O
μ	O
,	O
log	O
σ|d	O
)	O
for	O
a	O
univariate	O
gaussian	O
compute	O
a	O
laplace	O
approximation	O
of	O
p	O
(	O
μ	O
,	O
log	O
σ|d	O
)	O
for	O
a	O
gaussian	O
,	O
using	O
an	O
uninformative	B
prior	O
p	O
(	O
μ	O
,	O
log	O
σ	O
)	O
∝	O
1.	O
exercise	O
21.2	O
laplace	O
approximation	O
to	O
normal-gamma	O
consider	O
estimating	O
μ	O
and	O
(	O
cid:7	O
)	O
=	O
log	O
σ	O
for	O
a	O
gaussian	O
using	O
an	O
uniformative	O
normal-gamma	O
prior	O
.	O
the	O
log	O
posterior	O
is	O
log	O
p	O
(	O
μ	O
,	O
(	O
cid:7	O
)	O
|d	O
)	O
=	O
−n	O
log	O
σ	O
−	O
1	O
2σ2	O
[	O
ns2	O
+	O
n	O
(	O
y	O
−	O
μ	O
)	O
2	O
]	O
a.	O
show	O
that	O
the	O
ﬁrst	O
derivatives	O
are	O
n	O
(	O
y	O
−	O
μ	O
)	O
log	O
p	O
(	O
μ	O
,	O
(	O
cid:7	O
)	O
|d	O
)	O
=	O
σ2	O
log	O
p	O
(	O
μ	O
,	O
(	O
cid:7	O
)	O
|d	O
)	O
=−n	O
+	O
∂	O
∂μ	O
∂	O
∂	O
(	O
cid:7	O
)	O
ns2	O
+	O
n	O
(	O
y	O
−	O
μ	O
)	O
2	O
σ2	O
b.	O
show	O
that	O
the	O
hessian	O
matrix	O
is	O
given	O
by	O
∂2	O
∂2	O
%	O
∂μ∂	O
(	O
cid:12	O
)	O
log	O
p	O
(	O
μ	O
,	O
(	O
cid:7	O
)	O
|d	O
)	O
∂μ2	O
log	O
p	O
(	O
μ	O
,	O
(	O
cid:7	O
)	O
|d	O
)	O
(	O
cid:4	O
)	O
(	O
cid:3	O
)	O
−	O
n	O
∂	O
(	O
cid:12	O
)	O
2	O
log	O
p	O
(	O
μ	O
,	O
(	O
cid:7	O
)	O
|d	O
)	O
∂	O
(	O
cid:12	O
)	O
2	O
log	O
p	O
(	O
μ	O
,	O
(	O
cid:7	O
)	O
|d	O
)	O
−2n	O
y−μ	O
σ2	O
(	O
ns2	O
+	O
n	O
(	O
y	O
−	O
μ	O
)	O
2	O
)	O
−	O
2	O
−2n	O
y−μ	O
∂2	O
∂2	O
σ2	O
σ2	O
σ2	O
h	O
=	O
=	O
&	O
(	O
21.203	O
)	O
(	O
21.204	O
)	O
(	O
21.205	O
)	O
(	O
21.206	O
)	O
(	O
21.207	O
)	O
c.	O
use	O
this	O
to	O
derive	O
a	O
laplace	O
approximation	O
to	O
the	O
posterior	O
p	O
(	O
μ	O
,	O
(	O
cid:7	O
)	O
|d	O
)	O
.	O
exercise	O
21.3	O
variational	O
lower	O
bound	O
for	O
vb	O
for	O
univariate	O
gaussian	O
fill	O
in	O
the	O
details	O
of	O
the	O
derivation	O
in	O
section	O
21.5.1.6.	O
exercise	O
21.4	O
variational	O
lower	O
bound	O
for	O
vb	O
for	O
gmms	O
consider	O
vbem	O
for	O
gmms	O
as	O
in	O
section	O
21.6.1.4.	O
show	O
that	O
the	O
lower	O
bound	O
has	O
the	O
following	O
form	O
l	O
=	O
e	O
[	O
ln	O
p	O
(	O
x|z	O
,	O
μ	O
,	O
λ	O
)	O
]	O
+	O
e	O
[	O
ln	O
p	O
(	O
z|π	O
)	O
]	O
+	O
e	O
[	O
ln	O
p	O
(	O
π	O
)	O
]	O
+	O
e	O
[	O
ln	O
p	O
(	O
μ	O
,	O
λ	O
)	O
]	O
−e	O
[	O
ln	O
q	O
(	O
z	O
)	O
]	O
−	O
e	O
[	O
ln	O
q	O
(	O
π	O
)	O
]	O
−	O
e	O
[	O
ln	O
q	O
(	O
μ	O
,	O
λ	O
)	O
]	O
(	O
21.208	O
)	O
21.8.	O
local	O
variational	O
bounds	O
*	O
where	O
e	O
[	O
ln	O
p	O
(	O
x|z	O
,	O
μ	O
,	O
λ	O
)	O
]	O
=	O
(	O
cid:12	O
)	O
(	O
k	O
nk	O
ln	O
˜λk	O
−	O
dβ−1	O
k	O
−	O
νktr	O
(	O
sklk	O
)	O
1	O
2	O
(	O
cid:12	O
)	O
−νk	O
(	O
xk	O
−	O
mk	O
)	O
t	O
lk	O
(	O
xk	O
−	O
mk	O
)	O
−	O
d	O
ln	O
(	O
2π	O
)	O
(	O
cid:12	O
)	O
e	O
[	O
ln	O
p	O
(	O
z|π	O
)	O
]	O
=	O
e	O
[	O
ln	O
p	O
(	O
π	O
)	O
]	O
=	O
ln	O
cdir	O
(	O
α0	O
)	O
+	O
(	O
α0	O
−	O
1	O
)	O
rik	O
ln	O
˜πk	O
k	O
i	O
(	O
cid:12	O
)	O
k	O
ln	O
˜πk	O
(	O
cid:12	O
)	O
e	O
[	O
ln	O
p	O
(	O
μ	O
,	O
λ	O
)	O
]	O
=	O
e	O
[	O
ln	O
q	O
(	O
z	O
)	O
]	O
=	O
e	O
[	O
ln	O
q	O
(	O
π	O
)	O
]	O
=	O
e	O
[	O
ln	O
q	O
(	O
μ	O
,	O
λ	O
)	O
]	O
=	O
k	O
(	O
cid:12	O
)	O
ν0	O
−	O
d	O
−	O
1	O
+	O
ln	O
cw	O
i	O
(	O
l0	O
,	O
ν0	O
)	O
+	O
d	O
ln	O
(	O
β0/2π	O
)	O
+	O
ln	O
˜λk	O
−	O
dβ0	O
1	O
βk	O
2	O
−β0νk	O
(	O
mk	O
−	O
m0	O
)	O
t	O
lk	O
(	O
mk	O
−	O
m0	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
αk	O
−	O
1	O
)	O
ln	O
˜πk	O
+	O
ln	O
cdir	O
(	O
α	O
)	O
(	O
cid:4	O
)	O
(	O
cid:12	O
)	O
rik	O
ln	O
rik	O
(	O
cid:3	O
)	O
2	O
k	O
k	O
i	O
ln	O
˜λk	O
+	O
1	O
2	O
d	O
2	O
ln	O
βk	O
2π	O
−	O
d	O
2	O
k	O
ln	O
˜λk	O
−	O
1	O
2	O
*	O
−	O
h	O
(	O
q	O
(	O
λk	O
)	O
)	O
)	O
*	O
νktr	O
(	O
l−1	O
0	O
lk	O
)	O
where	O
the	O
normalization	O
constant	O
for	O
the	O
dirichlet	O
and	O
wishart	O
is	O
given	O
by	O
(	O
cid:14	O
)	O
cdir	O
(	O
α	O
)	O
(	O
cid:2	O
)	O
γ	O
(	O
(	O
cid:2	O
)	O
k	O
αk	O
)	O
k	O
γ	O
(	O
αk	O
)	O
(	O
cid:16	O
)	O
cw	O
i	O
(	O
l	O
,	O
ν	O
)	O
(	O
cid:2	O
)	O
|l|−ν/2	O
2νd/2γd	O
(	O
ν/2	O
)	O
γd	O
(	O
α	O
)	O
(	O
cid:2	O
)	O
πd	O
(	O
d−1	O
)	O
/4	O
γ	O
(	O
α	O
+	O
(	O
1	O
−	O
j	O
)	O
/2	O
)	O
d	O
(	O
cid:13	O
)	O
j=1	O
(	O
cid:17	O
)	O
−1	O
2	O
where	O
γd	O
(	O
ν	O
)	O
is	O
the	O
multivariate	B
gamma	I
function	I
.	O
finally	O
,	O
the	O
entropy	B
of	O
the	O
wishart	O
is	O
given	O
by	O
h	O
(	O
wi	O
(	O
l	O
,	O
ν	O
)	O
)	O
=	O
−	O
ln	O
cw	O
i	O
(	O
l	O
,	O
ν	O
)	O
−	O
ν	O
−	O
d	O
−	O
1	O
e	O
[	O
ln|λ|	O
]	O
+	O
where	O
e	O
[	O
ln|λ|	O
]	O
is	O
given	O
in	O
equation	O
21.131.	O
exercise	O
21.5	O
derivation	O
of	O
e	O
[	O
log	O
πk	O
]	O
under	O
a	O
dirichlet	O
distribution	O
show	O
that	O
νd	O
2	O
(	O
cid:2	O
)	O
exp	O
(	O
ψ	O
(	O
αk	O
)	O
)	O
exp	O
(	O
ψ	O
(	O
k	O
(	O
cid:2	O
)	O
αk	O
(	O
cid:2	O
)	O
)	O
)	O
exp	O
(	O
e	O
[	O
log	O
πk	O
]	O
)	O
=	O
where	O
π	O
∼	O
dir	O
(	O
α	O
)	O
.	O
exercise	O
21.6	O
alternative	O
derivation	O
of	O
the	O
mean	B
ﬁeld	I
updates	O
for	O
the	O
ising	O
model	O
derive	O
equation	O
21.50	O
by	O
directly	O
optimizing	O
the	O
variational	B
free	I
energy	I
one	O
term	O
at	O
a	O
time	O
.	O
765	O
(	O
21.209	O
)	O
(	O
21.210	O
)	O
(	O
21.211	O
)	O
(	O
21.212	O
)	O
(	O
21.213	O
)	O
(	O
21.214	O
)	O
(	O
21.215	O
)	O
(	O
21.216	O
)	O
(	O
21.217	O
)	O
(	O
21.218	O
)	O
(	O
21.219	O
)	O
(	O
21.220	O
)	O
766	O
chapter	O
21.	O
variational	B
inference	I
exercise	O
21.7	O
forwards	O
vs	O
reverse	O
kl	O
divergence	O
(	O
source	O
:	O
exercise	O
33.7	O
of	O
(	O
mackay	O
2003	O
)	O
.	O
)	O
consider	O
a	O
factored	O
approximation	O
q	O
(	O
x	O
,	O
y	O
)	O
=	O
q	O
(	O
x	O
)	O
q	O
(	O
y	O
)	O
to	O
a	O
joint	B
distribution	I
p	O
(	O
x	O
,	O
y	O
)	O
.	O
show	O
that	O
to	O
minimize	O
the	O
forwards	O
kl	O
kl	O
(	O
p||q	O
)	O
we	O
should	O
set	O
q	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
)	O
and	O
q	O
(	O
y	O
)	O
=	O
p	O
(	O
y	O
)	O
,	O
i.e.	O
,	O
the	O
optimal	O
approximation	O
is	O
a	O
product	O
of	O
marginals	O
now	O
consider	O
the	O
following	O
joint	B
distribution	I
,	O
where	O
the	O
rows	O
represent	O
y	O
and	O
the	O
columns	O
x	O
.	O
1	O
1/8	O
1/8	O
0	O
0	O
1	O
2	O
3	O
4	O
x	O
2	O
1/8	O
1/8	O
0	O
0	O
3	O
0	O
0	O
1/4	O
0	O
4	O
0	O
0	O
0	O
1/4	O
show	O
that	O
the	O
reverse	O
kl	O
kl	O
(	O
q||p	O
)	O
for	O
this	O
p	O
has	O
three	O
distinct	O
minima	O
.	O
evaluate	O
kl	O
(	O
q||p	O
)	O
at	O
each	O
of	O
them	O
.	O
what	O
is	O
the	O
value	O
of	O
kl	O
(	O
q||p	O
)	O
if	O
we	O
set	O
q	O
(	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
?	O
exercise	O
21.8	O
derivation	O
of	O
the	O
structured	B
mean	I
ﬁeld	I
updates	O
for	O
fhmm	O
derive	O
the	O
updates	O
in	O
section	O
21.4.1.	O
identify	O
those	O
minima	O
and	O
exercise	O
21.9	O
variational	O
em	O
for	O
binary	O
fa	O
with	O
sigmoid	B
link	O
consider	O
the	O
binary	O
fa	O
model	O
:	O
d	O
(	O
cid:13	O
)	O
d	O
(	O
cid:13	O
)	O
p	O
(	O
xi|zi	O
,	O
θ	O
)	O
=	O
ber	O
(	O
xij|sigm	O
(	O
wt	O
j	O
zi	O
+	O
βj	O
)	O
)	O
=	O
j=1	O
ηi	O
=	O
˜w˜zi	O
˜zi	O
(	O
cid:2	O
)	O
(	O
zi	O
;	O
1	O
)	O
˜w	O
(	O
cid:2	O
)	O
(	O
w	O
,	O
β	O
)	O
p	O
(	O
zi	O
)	O
=n	O
(	O
0	O
,	O
i	O
)	O
j=1	O
ber	O
(	O
xij|sigm	O
(	O
ηij	O
)	O
)	O
(	O
21.221	O
)	O
(	O
21.222	O
)	O
(	O
21.223	O
)	O
(	O
21.224	O
)	O
(	O
21.225	O
)	O
derive	O
an	O
em	O
algorithm	O
to	O
ﬁt	O
this	O
model	O
,	O
using	O
the	O
jaakkola-jordan	O
bound	O
.	O
hint	O
:	O
the	O
answer	O
is	O
in	O
(	O
tipping	O
1998	O
)	O
,	O
but	O
the	O
exercise	O
asks	O
you	O
to	O
derive	O
these	O
equations	O
.	O
exercise	O
21.10	O
vb	O
for	O
binary	O
fa	O
with	O
probit	B
link	O
in	O
section	O
11.4.6	O
,	O
we	O
showed	O
how	O
to	O
use	O
em	O
to	O
ﬁt	O
probit	B
regression	I
,	O
using	O
a	O
model	O
of	O
the	O
form	O
p	O
(	O
yi	O
=	O
1|zi	O
)	O
=i	O
(	O
zi	O
>	O
0	O
)	O
,	O
where	O
zi	O
∼	O
n	O
(	O
wt	O
xi	O
,	O
1	O
)	O
is	O
latent	B
.	O
now	O
consider	O
the	O
case	O
where	O
the	O
inputs	O
xi	O
are	O
also	O
unknown	B
,	O
as	O
in	O
binary	O
factor	O
analysis	O
.	O
show	O
how	O
to	O
ﬁt	O
this	O
model	O
using	O
variational	O
bayes	O
,	O
making	O
an	O
l=1	O
q	O
(	O
wl	O
)	O
.	O
hint	O
:	O
q	O
(	O
xi	O
)	O
and	O
i=1	O
q	O
(	O
xi	O
)	O
q	O
(	O
zi	O
)	O
approximation	O
to	O
the	O
posterior	O
of	O
the	O
form	O
q	O
(	O
x	O
,	O
z	O
,	O
w	O
)	O
=	O
q	O
(	O
wi	O
)	O
will	O
be	O
gaussian	O
,	O
and	O
q	O
(	O
zi	O
)	O
will	O
be	O
a	O
truncated	O
univariate	O
gaussian	O
.	O
(	O
cid:14	O
)	O
n	O
(	O
cid:14	O
)	O
l	O
22	O
more	O
variational	B
inference	I
22.1	O
introduction	O
in	O
chapter	O
21	O
,	O
we	O
discussed	O
mean	B
ﬁeld	I
inference	O
,	O
which	O
approximates	O
the	O
posterior	O
by	O
a	O
product	O
of	O
marginal	O
distributions	O
.	O
this	O
allows	O
us	O
to	O
use	O
different	O
parametric	O
forms	O
for	O
each	O
variable	O
,	O
which	O
is	O
particularly	O
useful	O
when	O
performing	O
bayesian	O
inference	B
for	O
the	O
parameters	O
of	O
statistical	O
models	O
(	O
such	O
as	O
the	O
mean	B
and	O
variance	B
of	O
a	O
gaussian	O
or	O
gmm	O
,	O
or	O
the	O
regression	B
weights	O
in	O
a	O
glm	O
)	O
,	O
as	O
we	O
saw	O
when	O
we	O
discussed	O
variational	O
bayes	O
and	O
vb-em	O
.	O
in	O
this	O
chapter	O
,	O
we	O
discuss	O
a	O
slightly	O
different	O
kind	O
of	O
variational	B
inference	I
.	O
the	O
basic	O
idea	O
is	O
to	O
minimize	O
j	O
(	O
q	O
)	O
=	O
kl	O
(	O
q||˜p	O
)	O
,	O
where	O
˜p	O
is	O
the	O
exact	O
but	O
unnormalized	O
posterior	O
as	O
before	O
,	O
but	O
where	O
we	O
no	O
longer	O
require	O
q	O
to	O
be	O
factorized	O
.	O
in	O
fact	O
,	O
we	O
do	O
not	O
even	O
require	O
q	O
to	O
be	O
a	O
globally	O
instead	O
,	O
we	O
only	O
require	O
that	O
q	O
is	O
locally	O
consistent	O
,	O
meaning	O
that	O
the	O
valid	O
joint	B
distribution	I
.	O
joint	B
distribution	I
of	O
two	O
adjacent	O
nodes	B
agrees	O
with	O
the	O
corresponding	O
marginals	O
(	O
we	O
will	O
deﬁne	O
this	O
more	O
precisely	O
below	O
)	O
.	O
in	O
addition	O
to	O
this	O
new	O
kind	O
of	O
inference	B
,	O
we	O
will	O
discuss	O
approximate	O
methods	O
for	O
map	O
state	B
estimation	I
in	O
discrete	B
graphical	O
models	O
.	O
it	O
turns	O
out	O
that	O
algorithms	O
for	O
solving	O
the	O
map	O
problem	O
are	O
very	O
similar	B
to	O
some	O
approximate	O
methods	O
for	O
computing	O
marginals	O
,	O
as	O
we	O
will	O
see	O
.	O
22.2	O
loopy	B
belief	I
propagation	I
:	O
algorithmic	O
issues	O
there	O
is	O
a	O
very	O
simple	O
approximate	O
inference	B
algorithm	O
for	O
discrete	B
(	O
or	O
gaussian	O
)	O
graphical	O
models	O
known	O
as	O
loopy	B
belief	I
propagation	I
or	O
lbp	O
.	O
the	O
basic	O
idea	O
is	O
extremely	O
simple	O
:	O
we	O
apply	O
the	O
belief	B
propagation	I
algorithm	O
of	O
section	O
20.2	O
to	O
the	O
graph	B
,	O
even	O
if	O
it	O
has	O
loops	O
(	O
i.e.	O
,	O
even	O
if	O
it	O
is	O
not	O
a	O
tree	B
)	O
.	O
this	O
method	O
is	O
simple	O
and	O
efficient	O
,	O
and	O
often	O
works	O
well	O
in	O
practice	O
,	O
outperforming	O
mean	B
ﬁeld	I
(	O
weiss	O
2001	O
)	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
the	O
algorithm	O
in	O
more	O
detail	O
.	O
in	O
the	O
next	O
section	O
,	O
we	O
analyse	O
this	O
algorithm	O
in	O
terms	O
of	O
variational	B
inference	I
.	O
22.2.1	O
a	O
brief	O
history	O
when	O
applied	O
to	O
loopy	O
graphs	O
,	O
bp	O
is	O
not	O
guaranteed	O
to	O
give	O
correct	O
results	O
,	O
and	O
may	O
not	O
even	O
converge	B
.	O
indeed	O
,	O
judea	O
pearl	O
,	O
who	O
invented	O
belief	B
propagation	I
for	O
trees	O
,	O
wrote	O
the	O
following	O
about	O
loopy	O
bp	O
in	O
1988	O
:	O
when	O
loops	O
are	O
present	O
,	O
the	O
network	O
is	O
no	O
longer	O
singly	O
connected	O
and	O
local	O
propagation	O
768	O
chapter	O
22.	O
more	O
variational	B
inference	I
schemes	O
will	O
invariably	O
run	O
into	O
trouble	O
.	O
.	O
.	O
if	O
we	O
ignore	O
the	O
existence	O
of	O
loops	O
and	O
permit	O
the	O
nodes	B
to	O
continue	O
communicating	O
with	O
each	O
other	O
as	O
if	O
the	O
network	O
were	O
singly	O
connected	O
,	O
messages	O
may	O
circulate	O
indeﬁnitely	O
around	O
the	O
loops	O
and	O
the	O
process	O
may	O
not	O
converge	O
to	O
a	O
stable	B
equilibrium	O
.	O
.	O
.	O
such	O
oscillations	O
do	O
not	O
normally	O
occur	O
in	O
probabilistic	O
networks	O
.	O
.	O
.	O
which	O
tend	O
to	O
bring	O
all	O
messages	O
to	O
some	O
stable	B
equilibrium	O
as	O
time	O
goes	O
on	O
.	O
however	O
,	O
this	O
asymptotic	O
equilibrium	O
is	O
not	O
coherent	O
,	O
in	O
the	O
sense	O
that	O
it	O
does	O
not	O
represent	O
the	O
posterior	O
probabilities	O
of	O
all	O
nodes	O
of	O
the	O
network	O
—	O
(	O
pearl	O
1988	O
,	O
p.195	O
)	O
despite	O
these	O
reservations	O
,	O
pearl	O
advocated	O
the	O
use	O
of	O
belief	B
propagation	I
in	O
loopy	O
networks	O
as	O
an	O
approximation	O
scheme	O
(	O
j.	O
pearl	O
,	O
personal	O
communication	O
)	O
and	O
exercise	O
4.7	O
in	O
(	O
pearl	O
1988	O
)	O
investigates	O
the	O
quality	O
of	O
the	O
approximation	O
when	O
it	O
is	O
applied	O
to	O
a	O
particular	O
loopy	O
belief	O
network	O
.	O
however	O
,	O
the	O
main	O
impetus	O
behind	O
the	O
interest	O
in	O
bp	O
arose	O
when	O
mceliece	O
et	O
al	O
.	O
(	O
1998	O
)	O
showed	O
that	O
a	O
popular	O
algorithm	O
for	O
error	B
correcting	I
codes	I
known	O
as	O
turbo	B
codes	I
(	O
berrou	O
et	O
al	O
.	O
1993	O
)	O
could	O
be	O
viewed	O
as	O
an	O
instance	O
of	O
bp	O
applied	O
to	O
a	O
certain	O
kind	O
of	O
graph	B
.	O
this	O
was	O
an	O
important	O
observation	B
since	O
turbo	B
codes	I
have	O
gotten	O
very	O
close	O
to	O
the	O
theoretical	O
lower	O
bound	O
on	O
coding	O
efficiency	O
proved	O
by	O
shannon	O
.	O
(	O
another	O
approach	O
,	O
known	O
as	O
low	B
density	I
parity	I
check	I
or	O
ldpc	O
codes	O
,	O
has	O
achieved	O
comparable	O
performance	O
;	O
it	O
also	O
uses	O
lbp	O
for	O
decoding	B
—	O
see	O
figure	O
22.1	O
for	O
an	O
example	O
.	O
)	O
in	O
(	O
murphy	O
et	O
al	O
.	O
1999	O
)	O
,	O
lbp	O
was	O
experimentally	O
shown	O
to	O
also	O
work	O
well	O
for	O
inference	B
in	O
other	O
kinds	O
of	O
graphical	O
models	O
beyond	O
the	O
error-correcting	O
code	O
context	O
,	O
and	O
since	O
then	O
,	O
the	O
method	O
has	O
been	O
widely	O
used	O
in	O
many	O
different	O
applications	O
.	O
22.2.2	O
lbp	O
on	O
pairwise	O
models	O
we	O
now	O
discuss	O
how	O
to	O
apply	O
lbp	O
to	O
an	O
undirected	B
graphical	I
model	I
with	O
pairwise	O
factors	O
(	O
we	O
discuss	O
the	O
directed	B
case	O
,	O
which	O
can	O
involve	O
higher	O
order	O
factors	B
,	O
in	O
the	O
next	O
section	O
)	O
.	O
the	O
method	O
is	O
simple	O
:	O
see	O
algorithm	O
8	O
for	O
the	O
pseudocode	O
,	O
and	O
beliefpropagation	O
for	O
some	O
matlab	O
code	O
.	O
we	O
will	O
discuss	O
issues	O
such	O
as	O
convergence	O
and	O
accuracy	O
of	O
this	O
method	O
shortly	O
.	O
just	O
continually	O
apply	O
equations	O
20.11	O
and	O
20.10	O
until	O
convergence	O
.	O
algorithm	O
22.1	O
:	O
loopy	B
belief	I
propagation	I
for	O
a	O
pairwise	O
mrf	O
1	O
input	O
:	O
node	O
potentials	O
ψs	O
(	O
xs	O
)	O
,	O
edge	O
potentials	O
ψst	O
(	O
xs	O
,	O
xt	O
)	O
;	O
2	O
initialize	O
messages	O
ms→t	O
(	O
xt	O
)	O
=	O
1	O
for	O
all	O
edges	O
s	O
−	O
t	O
;	O
3	O
initialize	O
beliefs	O
bels	O
(	O
xs	O
)	O
=	O
1	O
for	O
all	O
nodes	O
s	O
;	O
4	O
repeat	O
5	O
send	O
message	O
on	O
each	O
edge	O
ms→t	O
(	O
xt	O
)	O
=	O
update	O
belief	O
of	O
each	O
node	O
bels	O
(	O
xs	O
)	O
∝	O
ψs	O
(	O
xs	O
)	O
ψs	O
(	O
xs	O
)	O
ψst	O
(	O
xs	O
,	O
xt	O
)	O
(	O
cid:26	O
)	O
!	O
(	O
cid:7	O
)	O
xs	O
6	O
7	O
until	O
beliefs	O
don	O
’	O
t	O
change	O
signiﬁcantly	O
;	O
8	O
return	O
marginal	O
beliefs	O
bels	O
(	O
xs	O
)	O
;	O
#	O
;	O
(	O
cid:26	O
)	O
u∈nbrs\t	O
mu→s	O
(	O
xs	O
)	O
t∈nbrs	O
mt→s	O
(	O
xs	O
)	O
;	O
22.2.	O
loopy	B
belief	I
propagation	I
:	O
algorithmic	O
issues	O
769	O
figure	O
22.1	O
(	O
a	O
)	O
a	O
simple	O
factor	O
graph	B
representation	O
of	O
a	O
(	O
2,3	O
)	O
low-density	O
parity	O
check	O
code	O
(	O
factor	B
graphs	O
are	O
deﬁned	O
in	O
section	O
22.2.3.1	O
)	O
.	O
each	O
message	O
bit	O
(	O
hollow	O
round	O
circle	O
)	O
is	O
connected	O
to	O
two	O
parity	O
factors	B
(	O
solid	O
black	O
squares	O
)	O
,	O
and	O
each	O
parity	O
factor	B
is	O
connected	O
to	O
three	O
bits	B
.	O
each	O
parity	O
factor	B
has	O
the	O
form	O
ψstu	O
(	O
xs	O
,	O
xt	O
,	O
xu	O
)	O
=i	O
(	O
xs	O
⊗	O
xt	O
⊗	O
xu	O
=	O
1	O
)	O
,	O
where	O
⊗	O
is	O
the	O
xor	B
operator	O
.	O
the	O
local	B
evidence	I
factors	O
for	O
each	O
hidden	B
node	O
are	O
not	O
shown	O
.	O
(	O
b	O
)	O
a	O
larger	O
example	O
of	O
a	O
random	O
ldpc	O
code	O
.	O
we	O
see	O
that	O
this	O
graph	B
is	O
“	O
locally	O
tree-like	O
”	O
,	O
meaning	O
there	O
are	O
no	O
short	O
cycles	O
;	O
rather	O
,	O
each	O
cycle	B
has	O
length	O
∼	O
log	O
m	O
,	O
where	O
m	O
is	O
the	O
number	O
of	O
nodes	B
.	O
this	O
gives	O
us	O
a	O
hint	O
as	O
to	O
why	O
loopy	O
bp	O
works	O
so	O
well	O
on	O
such	O
graphs	O
.	O
(	O
note	O
,	O
however	O
,	O
that	O
some	O
error	O
correcting	O
code	O
graphs	O
have	O
short	O
loops	O
,	O
so	O
this	O
is	O
not	O
the	O
full	B
explanation	O
.	O
)	O
source	O
:	O
figure	O
2.9	O
from	O
(	O
wainwright	O
and	O
jordan	O
2008b	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
martin	O
wainwright	O
.	O
22.2.3	O
lbp	O
on	O
a	O
factor	B
graph	I
to	O
handle	O
models	O
with	O
higher-order	O
clique	B
potentials	O
(	O
which	O
includes	O
directed	B
models	O
where	O
some	O
nodes	B
have	O
more	O
than	O
one	O
parent	O
)	O
,	O
it	O
is	O
useful	O
to	O
use	O
a	O
representation	O
known	O
as	O
a	O
factor	B
graph	I
.	O
we	O
explain	O
this	O
representation	O
below	O
,	O
and	O
then	O
describe	O
how	O
to	O
apply	O
lbp	O
to	O
such	O
models	O
.	O
22.2.3.1	O
factor	B
graphs	O
a	O
factor	B
graph	I
(	O
kschischang	O
et	O
al	O
.	O
2001	O
;	O
frey	O
2003	O
)	O
is	O
a	O
graphical	O
representation	O
that	O
uniﬁes	O
directed	B
and	O
undirected	B
models	O
,	O
and	O
which	O
simpliﬁes	O
certain	O
message	B
passing	I
algorithms	O
.	O
more	O
precisely	O
,	O
a	O
factor	B
graph	I
is	O
an	O
undirected	B
bipartite	O
graph	B
with	O
two	O
kinds	O
of	O
nodes	B
.	O
round	O
nodes	B
represent	O
variables	O
,	O
square	O
nodes	O
represent	O
factors	B
,	O
and	O
there	O
is	O
an	O
edge	O
from	O
each	O
variable	O
to	O
every	O
factor	B
that	O
mentions	O
it	O
.	O
for	O
example	O
,	O
consider	O
the	O
mrf	O
in	O
figure	O
22.2	O
(	O
a	O
)	O
.	O
if	O
we	O
assume	O
one	O
potential	O
per	O
maximal	B
clique	I
,	O
we	O
get	O
the	O
factor	B
graph	I
in	O
figure	O
22.2	O
(	O
b	O
)	O
,	O
which	O
represents	O
the	O
function	O
f	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
f124	O
(	O
x1	O
,	O
x2	O
,	O
x4	O
)	O
f234	O
(	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
(	O
22.1	O
)	O
if	O
we	O
assume	O
one	O
potential	O
per	O
edge	O
.	O
we	O
get	O
the	O
factor	B
graph	I
in	O
figure	O
22.2	O
(	O
c	O
)	O
,	O
which	O
represents	O
the	O
function	O
f	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
)	O
=	O
f14	O
(	O
x1	O
,	O
x4	O
)	O
f12	O
(	O
x1	O
,	O
x2	O
)	O
f34	O
(	O
x3	O
,	O
x4	O
)	O
f23	O
(	O
x2	O
,	O
x3	O
)	O
f24	O
(	O
x2	O
,	O
x4	O
)	O
(	O
22.2	O
)	O
770	O
chapter	O
22.	O
more	O
variational	B
inference	I
2	O
4	O
4	O
1	O
3	O
(	O
a	O
)	O
1	O
3	O
(	O
b	O
)	O
2	O
4	O
2	O
1	O
3	O
(	O
c	O
)	O
figure	O
22.2	O
(	O
a	O
)	O
a	O
simple	O
ugm	O
.	O
(	O
b	O
)	O
a	O
factor	B
graph	I
representation	O
assuming	O
one	O
potential	O
per	O
maximal	B
clique	I
.	O
(	O
c	O
)	O
a	O
factor	B
graph	I
representation	O
assuming	O
one	O
potential	O
per	O
edge	O
.	O
x1	O
x2	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
x3	O
x4	O
x5	O
x2	O
x1	O
p	O
(	O
x3|x1	O
,	O
x2	O
)	O
x3	O
p	O
(	O
x4|x3	O
)	O
p	O
(	O
x5|x3	O
)	O
x4	O
x5	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
22.3	O
(	O
a	O
)	O
a	O
simple	O
dgm	O
.	O
(	O
b	O
)	O
its	O
corresponding	O
factor	B
graph	I
.	O
based	O
on	O
figure	O
5	O
of	O
(	O
yedidia	O
et	O
al	O
.	O
2001	O
)	O
..	O
we	O
can	O
also	O
convert	O
a	O
dgm	O
to	O
a	O
factor	B
graph	I
:	O
just	O
create	O
one	O
factor	B
per	O
cpd	O
,	O
and	O
connect	O
that	O
factor	B
to	O
all	O
the	O
variables	O
that	O
use	O
that	O
cpd	O
.	O
for	O
example	O
,	O
figure	O
22.3	O
represents	O
the	O
following	O
factorization	O
:	O
f	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4	O
,	O
x5	O
)	O
=	O
f1	O
(	O
x1	O
)	O
f2	O
(	O
x2	O
)	O
f123	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
f34	O
(	O
x3	O
,	O
x4	O
)	O
f35	O
(	O
x3	O
,	O
x5	O
)	O
(	O
22.3	O
)	O
where	O
we	O
deﬁne	O
f123	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
)	O
=	O
p	O
(	O
x3|x1	O
,	O
x2	O
)	O
,	O
etc	O
.	O
if	O
each	O
node	O
has	O
at	O
most	O
one	O
parent	O
(	O
and	O
hence	O
the	O
graph	B
is	O
a	O
chain	O
or	O
simple	O
tree	O
)	O
,	O
then	O
there	O
will	O
be	O
one	O
factor	B
per	O
edge	O
(	O
root	B
nodes	O
can	O
have	O
their	O
prior	O
cpds	O
absorvbed	O
into	O
their	O
children	B
’	O
s	O
factors	B
)	O
.	O
such	O
models	O
are	O
equivalent	O
to	O
pairwise	O
mrfs	O
.	O
22.2.	O
loopy	B
belief	I
propagation	I
:	O
algorithmic	O
issues	O
771	O
figure	O
22.4	O
message	B
passing	I
on	O
a	O
bipartite	O
factor	O
graph	B
.	O
square	O
nodes	O
represent	O
factors	B
,	O
and	O
circles	O
represent	O
variables	O
.	O
source	O
:	O
figure	O
6	O
of	O
(	O
kschischang	O
et	O
al	O
.	O
2001	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
brendan	O
frey	O
.	O
22.2.3.2	O
bp	O
on	O
a	O
factor	B
graph	I
we	O
now	O
derive	O
a	O
version	O
of	O
bp	O
that	O
sends	O
messages	O
on	O
a	O
factor	B
graph	I
,	O
as	O
proposed	O
in	O
(	O
kschis-	O
chang	O
et	O
al	O
.	O
2001	O
)	O
.	O
speciﬁcally	O
,	O
we	O
now	O
have	O
two	O
kinds	O
of	O
messages	O
:	O
variables	O
to	O
factors	B
mx→f	O
(	O
x	O
)	O
=	O
h∈nbr	O
(	O
x	O
)	O
\	O
{	O
f	O
}	O
and	O
factors	B
to	O
variables	O
:	O
mf→x	O
(	O
x	O
)	O
=	O
f	O
(	O
x	O
,	O
y	O
)	O
mh→x	O
(	O
x	O
)	O
(	O
cid:20	O
)	O
y∈nbr	O
(	O
f	O
)	O
\	O
{	O
x	O
}	O
my→f	O
(	O
y	O
)	O
(	O
22.4	O
)	O
(	O
22.5	O
)	O
here	O
nbr	O
(	O
x	O
)	O
are	O
all	O
the	O
factors	B
that	O
are	O
connected	O
to	O
variable	O
x	O
,	O
and	O
nbr	O
(	O
f	O
)	O
are	O
all	O
the	O
variables	O
that	O
are	O
connected	O
to	O
factor	B
f.	O
these	O
messages	O
are	O
illustrated	O
in	O
figure	O
22.4.	O
at	O
convergence	O
,	O
we	O
can	O
compute	O
the	O
ﬁnal	O
beliefs	O
as	O
a	O
product	O
of	O
incoming	O
messages	O
:	O
(	O
cid:20	O
)	O
(	O
cid:4	O
)	O
y	O
(	O
cid:20	O
)	O
bel	O
(	O
x	O
)	O
∝	O
f∈nbr	O
(	O
x	O
)	O
mf→x	O
(	O
x	O
)	O
(	O
22.6	O
)	O
in	O
the	O
following	O
sections	O
,	O
we	O
will	O
focus	O
on	O
lbp	O
for	O
pairwise	O
models	O
,	O
rather	O
than	O
for	O
factor	B
graphs	O
,	O
but	O
this	O
is	O
just	O
for	O
notational	O
simplicity	O
.	O
22.2.4	O
convergence	O
lbp	O
does	O
not	O
always	O
converge	B
,	O
and	O
even	O
when	O
it	O
does	O
,	O
it	O
may	O
converge	B
to	O
the	O
wrong	O
answers	O
.	O
this	O
raises	O
several	O
questions	O
:	O
how	O
can	O
we	O
predict	O
when	O
convergence	O
will	O
occur	O
?	O
what	O
can	O
we	O
do	O
to	O
increase	O
the	O
probability	O
of	O
convergence	O
?	O
what	O
can	O
we	O
do	O
to	O
increase	O
the	O
rate	B
of	O
convergence	O
?	O
we	O
brieﬂy	O
discuss	O
these	O
issues	O
below	O
.	O
we	O
then	O
discuss	O
the	O
issue	O
of	O
accuracy	O
of	O
the	O
results	O
at	O
convergence	O
.	O
772	O
chapter	O
22.	O
more	O
variational	B
inference	I
d	O
e	O
g	O
r	O
e	O
v	O
n	O
o	O
c	O
s	O
e	O
g	O
a	O
s	O
s	O
e	O
m	O
f	O
o	O
%	O
)	O
0	O
=	O
1	O
6	O
x	O
(	O
p	O
1.0	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
1.0	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
)	O
0	O
=	O
5	O
1	O
1	O
x	O
(	O
p	O
)	O
0	O
=	O
7	O
x	O
(	O
p	O
1.0	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
1.0	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
time	O
(	O
seconds	O
)	O
(	O
b	O
)	O
0	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
time	O
(	O
seconds	O
)	O
(	O
e	O
)	O
)	O
0	O
=	O
0	O
1	O
x	O
(	O
p	O
)	O
0	O
=	O
7	O
1	O
x	O
(	O
p	O
1.0	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
1.0	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
time	O
(	O
seconds	O
)	O
(	O
c	O
)	O
0	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
time	O
(	O
seconds	O
)	O
(	O
f	O
)	O
0	O
10	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
90	O
100	O
time	O
(	O
seconds	O
)	O
(	O
a	O
)	O
0	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
time	O
(	O
seconds	O
)	O
(	O
d	O
)	O
synchronous	O
asynchronous	O
no	O
smoothing	O
true	O
illustration	O
of	O
the	O
behavior	O
of	O
loopy	B
belief	I
propagation	I
on	O
an	O
11	O
×	O
11	O
ising	O
grid	O
with	O
figure	O
22.5	O
random	O
potentials	O
,	O
wij	O
∼	O
unif	O
(	O
−c	O
,	O
c	O
)	O
,	O
where	O
c	O
=	O
11.	O
for	O
larger	O
c	O
,	O
inference	B
becomes	O
harder	O
.	O
(	O
a	O
)	O
percentage	O
of	O
messasges	O
that	O
have	O
converged	O
vs	O
time	O
for	O
3	O
different	O
update	O
schedules	O
:	O
dotted	O
=	O
damped	O
sychronous	O
(	O
few	O
nodes	B
converge	O
)	O
,	O
dashed	O
=	O
undamped	O
asychnronous	O
(	O
half	O
the	O
nodes	B
converge	O
)	O
,	O
solid	O
=	O
(	O
b-f	O
)	O
marginal	O
beliefs	O
of	O
certain	O
nodes	B
vs	O
time	O
.	O
solid	O
straight	O
damped	O
asychnronous	O
(	O
all	O
nodes	O
converge	B
)	O
.	O
line	O
=	O
truth	O
,	O
dashed	O
=	O
sychronous	O
,	O
solid	O
=	O
damped	O
asychronous	O
.	O
source	O
:	O
figure	O
11.c.1	O
of	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
.	O
22.2.4.1	O
when	O
will	O
lbp	O
converge	B
?	O
the	O
details	O
of	O
the	O
analysis	O
of	O
when	O
lbp	O
will	O
converge	B
are	O
beyond	O
the	O
scope	B
of	O
this	O
chapter	O
,	O
but	O
we	O
brieﬂy	O
sketch	O
the	O
basic	O
idea	O
.	O
the	O
key	O
analysis	O
tool	O
is	O
the	O
computation	B
tree	I
,	O
which	O
visualizes	O
the	O
messages	O
that	O
are	O
passed	O
as	O
the	O
algorithm	O
proceeds	O
.	O
figure	O
22.6	O
gives	O
a	O
simple	O
example	O
.	O
in	O
the	O
ﬁrst	O
iteration	O
,	O
node	O
1	O
receives	O
messages	O
from	O
nodes	B
2	O
and	O
3.	O
in	O
the	O
second	O
iteration	O
,	O
it	O
receives	O
one	O
message	O
from	O
node	O
3	O
(	O
via	O
node	O
2	O
)	O
,	O
one	O
from	O
node	O
2	O
(	O
via	O
node	O
3	O
)	O
,	O
and	O
two	O
messages	O
from	O
node	O
4	O
(	O
via	O
nodes	B
2	O
and	O
3	O
)	O
.	O
and	O
so	O
on	O
.	O
the	O
key	O
insight	O
is	O
that	O
t	O
iterations	O
of	O
lbp	O
is	O
equivalent	O
to	O
exact	O
computation	O
in	O
a	O
computation	B
tree	I
of	O
height	O
t	O
+	O
1.	O
if	O
the	O
strengths	O
of	O
the	O
connections	O
on	O
the	O
edges	B
is	O
sufficiently	O
weak	O
,	O
then	O
the	O
inﬂuence	O
of	O
the	O
leaves	B
on	O
the	O
root	B
will	O
diminish	O
over	O
time	O
,	O
and	O
convergence	O
will	O
occur	O
.	O
see	O
(	O
wainwright	O
and	O
jordan	O
2008b	O
)	O
and	O
references	O
therein	O
for	O
more	O
information	B
.	O
22.2.	O
loopy	B
belief	I
propagation	I
:	O
algorithmic	O
issues	O
773	O
figure	O
22.6	O
(	O
a	O
)	O
a	O
simple	O
loopy	O
graph	B
.	O
(	O
b	O
)	O
the	O
computation	B
tree	I
,	O
rooted	O
at	O
node	O
1	O
,	O
after	O
4	O
rounds	O
of	O
message	B
passing	I
.	O
nodes	B
2	O
and	O
3	O
occur	O
more	O
often	O
in	O
the	O
tree	B
because	O
they	O
have	O
higher	O
degree	B
than	O
nodes	B
1	O
and	O
2.	O
source	O
:	O
figure	O
8.2	O
of	O
(	O
wainwright	O
and	O
jordan	O
2008b	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
martin	O
wainwright	O
.	O
22.2.4.2	O
making	O
lbp	O
converge	B
although	O
the	O
theoretical	O
convergence	O
analysis	O
is	O
very	O
interesting	O
,	O
in	O
practice	O
,	O
when	O
faced	O
with	O
a	O
model	O
where	O
lbp	O
is	O
not	O
converging	O
,	O
what	O
should	O
we	O
do	O
?	O
one	O
simple	O
way	O
to	O
reduce	O
the	O
chance	O
of	O
oscillation	O
is	O
to	O
use	O
damping	B
.	O
that	O
is	O
,	O
instead	O
of	O
sending	O
the	O
message	O
m	O
k	O
ts	O
,	O
we	O
send	O
a	O
damped	O
message	O
of	O
the	O
form	O
ts	O
(	O
xs	O
)	O
=	O
λmts	O
(	O
xs	O
)	O
+	O
(	O
1−	O
λ	O
)	O
˜m	O
k−1	O
˜m	O
k	O
ts	O
(	O
xs	O
)	O
(	O
22.7	O
)	O
where	O
0	O
≤	O
λ	O
≤	O
1	O
is	O
the	O
damping	B
factor	O
clearly	O
if	O
λ	O
=	O
1	O
this	O
reduces	O
to	O
the	O
standard	O
scheme	O
,	O
but	O
for	O
λ	O
<	O
1	O
,	O
this	O
partial	O
updating	O
scheme	O
can	O
help	O
improve	O
convergence	O
.	O
using	O
a	O
value	O
such	O
as	O
λ	O
∼	O
0.5	O
is	O
standard	O
practice	O
.	O
the	O
beneﬁts	O
of	O
this	O
approach	O
are	O
shown	O
in	O
figure	O
22.5	O
,	O
where	O
we	O
see	O
that	O
damped	O
updating	O
results	O
in	O
convergence	O
much	O
more	O
often	O
than	O
undamped	O
updating	O
.	O
it	O
is	O
possible	O
to	O
devise	O
methods	O
,	O
known	O
as	O
double	B
loop	I
algorithms	I
,	O
which	O
are	O
guaranteed	O
to	O
converge	B
to	O
a	O
local	O
minimum	O
of	O
the	O
same	O
objective	O
that	O
lbp	O
is	O
minimizing	O
(	O
yuille	O
2001	O
;	O
welling	O
and	O
teh	O
2001	O
)	O
.	O
unfortunately	O
,	O
these	O
methods	O
are	O
rather	O
slow	O
and	O
complicated	O
,	O
and	O
the	O
accuracy	O
of	O
the	O
resulting	O
marginals	O
is	O
usually	O
not	O
much	O
greater	O
than	O
with	O
standard	O
lbp	O
.	O
(	O
indeed	O
,	O
oscillating	O
marginals	O
is	O
sometimes	O
a	O
sign	O
that	O
the	O
lbp	O
approximation	O
itself	O
is	O
a	O
poor	O
one	O
.	O
)	O
consequently	O
,	O
these	O
techniques	O
are	O
not	O
very	O
widely	O
used	O
.	O
in	O
section	O
22.4.2	O
,	O
we	O
will	O
see	O
a	O
different	O
convergent	O
version	O
of	O
bp	O
that	O
is	O
widely	O
used	O
.	O
22.2.4.3	O
increasing	O
the	O
convergence	O
rate	B
:	O
message	O
scheduling	O
even	O
if	O
lbp	O
converges	O
,	O
it	O
may	O
take	O
a	O
long	O
time	O
.	O
the	O
standard	O
approach	O
when	O
implementing	O
lbp	O
is	O
to	O
perform	O
synchronous	B
updates	I
,	O
where	O
all	O
nodes	O
absorb	O
messages	O
in	O
parallel	O
,	O
and	O
then	O
send	O
out	O
messages	O
in	O
parallel	O
.	O
that	O
is	O
,	O
the	O
new	O
messages	O
at	O
iteration	O
k	O
+	O
1	O
are	O
computed	O
in	O
parallel	O
using	O
mk+1	O
=	O
(	O
f1	O
(	O
mk	O
)	O
,	O
.	O
.	O
.	O
,	O
fe	O
(	O
mk	O
)	O
)	O
(	O
22.8	O
)	O
where	O
e	O
is	O
the	O
number	O
of	O
edges	B
,	O
and	O
fst	O
(	O
m	O
)	O
is	O
the	O
function	O
that	O
computes	O
the	O
message	O
for	O
edge	O
s	O
→	O
t	O
given	O
all	O
the	O
old	O
messages	O
.	O
this	O
is	O
analogous	O
to	O
the	O
jacobi	O
method	O
for	O
solving	O
linear	O
774	O
chapter	O
22.	O
more	O
variational	B
inference	I
systems	O
of	O
equations	O
.	O
it	O
is	O
well	O
known	O
(	O
bertsekas	O
1997	O
)	O
that	O
the	O
gauss-seidel	O
method	O
,	O
which	O
performs	O
asynchronous	B
updates	I
in	O
a	O
ﬁxed	O
round-robin	O
fashion	O
,	O
converges	O
faster	O
when	O
solving	O
linear	O
systems	O
of	O
equations	O
.	O
we	O
can	O
apply	O
the	O
same	O
idea	O
to	O
lbp	O
,	O
using	O
updates	O
of	O
the	O
form	O
(	O
cid:22	O
)	O
{	O
mk+1	O
j	O
j	O
:	O
j	O
>	O
i	O
}	O
(	O
cid:23	O
)	O
mk+1	O
i	O
=	O
fi	O
:	O
j	O
<	O
i	O
}	O
,	O
{	O
mk	O
(	O
22.9	O
)	O
where	O
the	O
message	O
for	O
edge	O
i	O
is	O
computed	O
using	O
new	O
messages	O
(	O
iteration	O
k	O
+	O
1	O
)	O
from	O
edges	B
earlier	O
in	O
the	O
ordering	O
,	O
and	O
using	O
old	O
messages	O
(	O
iteration	O
k	O
)	O
from	O
edges	B
later	O
in	O
the	O
ordering	O
.	O
this	O
raises	O
the	O
question	O
of	O
what	O
order	O
to	O
update	O
the	O
messages	O
in	O
.	O
one	O
simple	O
idea	O
is	O
to	O
use	O
a	O
ﬁxed	O
or	O
random	O
order	O
.	O
the	O
beneﬁts	O
of	O
this	O
approach	O
are	O
shown	O
in	O
figure	O
22.5	O
,	O
where	O
we	O
see	O
that	O
(	O
damped	O
)	O
asynchronous	O
updating	O
results	O
in	O
convergence	O
much	O
more	O
often	O
than	O
synchronous	O
updating	O
.	O
a	O
smarter	O
approach	O
is	O
to	O
pick	O
a	O
set	O
of	O
spanning	O
trees	O
,	O
and	O
then	O
to	O
perform	O
an	O
up-down	B
sweep	O
on	O
one	O
tree	B
at	O
a	O
time	O
,	O
keeping	O
all	O
the	O
other	O
messages	O
ﬁxed	O
.	O
this	O
is	O
known	O
as	O
tree	B
reparameterization	I
(	O
trp	O
)	O
(	O
wainwright	O
et	O
al	O
.	O
2001	O
)	O
,	O
which	O
should	O
not	O
be	O
confused	O
with	O
the	O
more	O
sophisticated	O
tree-reweighted	O
bp	O
(	O
often	O
abbreviated	O
to	O
trw	O
)	O
to	O
be	O
discussed	O
in	O
section	O
22.4.2.1.	O
however	O
,	O
we	O
can	O
do	O
even	O
better	O
by	O
using	O
an	O
adaptive	O
ordering	O
.	O
the	O
intuition	O
is	O
that	O
we	O
should	O
focus	O
our	O
computational	O
efforts	O
on	O
those	O
variables	O
that	O
are	O
most	O
uncertain	O
.	O
(	O
elidan	O
et	O
al	O
.	O
2006	O
)	O
proposed	O
a	O
technique	O
known	O
as	O
residual	B
belief	I
propagation	I
,	O
in	O
which	O
messages	O
are	O
scheduled	O
to	O
be	O
sent	O
according	O
to	O
the	O
norm	O
of	O
the	O
difference	O
from	O
their	O
previous	O
value	O
.	O
that	O
is	O
,	O
we	O
deﬁne	O
the	O
residual	B
of	O
new	O
message	O
mst	O
at	O
iteration	O
k	O
to	O
be	O
|	O
log	O
r	O
(	O
s	O
,	O
t	O
,	O
k	O
)	O
=	O
||	O
log	O
mst	O
−	O
log	O
mk	O
st||∞	O
=	O
max	O
(	O
22.10	O
)	O
|	O
mst	O
(	O
i	O
)	O
mk	O
st	O
(	O
i	O
)	O
i	O
we	O
can	O
store	O
messages	O
in	O
a	O
priority	O
queue	O
,	O
and	O
always	O
send	O
the	O
one	O
with	O
highest	O
residual	O
.	O
when	O
a	O
message	O
is	O
sent	O
from	O
s	O
to	O
t	O
,	O
all	O
of	O
the	O
other	O
messages	O
that	O
depend	O
on	O
mst	O
(	O
i.e.	O
,	O
messages	O
of	O
the	O
form	O
mtu	O
where	O
u	O
∈	O
nbr	O
(	O
t	O
)	O
\	O
s	O
)	O
need	O
to	O
be	O
recomputed	O
;	O
their	O
residual	B
is	O
recomputed	O
,	O
and	O
they	O
are	O
added	O
back	O
to	O
the	O
queue	O
.	O
in	O
(	O
elidan	O
et	O
al	O
.	O
2006	O
)	O
,	O
they	O
showed	O
(	O
experimentally	O
)	O
that	O
this	O
method	O
converges	O
more	O
often	O
,	O
and	O
much	O
faster	O
,	O
than	O
using	O
sychronous	O
updating	O
,	O
asynchronous	O
updating	O
with	O
a	O
ﬁxed	O
order	O
,	O
and	O
the	O
trp	O
approach	O
.	O
a	O
reﬁnement	O
of	O
residual	B
bp	O
was	O
presented	O
in	O
(	O
sutton	O
and	O
mccallum	O
2007	O
)	O
.	O
in	O
this	O
paper	O
,	O
they	O
use	O
an	O
upper	O
bound	O
on	O
the	O
residual	B
of	O
a	O
message	O
instead	O
of	O
the	O
actual	O
residual	B
.	O
this	O
means	O
that	O
messages	O
are	O
only	O
computed	O
if	O
they	O
are	O
going	O
to	O
be	O
sent	O
;	O
they	O
are	O
not	O
just	O
computed	O
for	O
the	O
purposes	O
of	O
evaluating	O
the	O
residual	B
.	O
this	O
was	O
observed	O
to	O
be	O
about	O
ﬁve	O
times	O
faster	O
than	O
residual	B
bp	O
,	O
although	O
the	O
quality	O
of	O
the	O
ﬁnal	O
results	O
is	O
similar	B
.	O
22.2.5	O
accuracy	O
of	O
lbp	O
for	O
a	O
graph	B
with	O
a	O
single	O
loop	O
,	O
one	O
can	O
show	O
that	O
the	O
max-product	B
version	O
of	O
lbp	O
will	O
ﬁnd	O
the	O
correct	O
map	O
estimate	O
,	O
if	O
it	O
converges	O
(	O
weiss	O
2000	O
)	O
.	O
for	O
more	O
general	O
graphs	O
,	O
one	O
can	O
bound	O
the	O
error	O
in	O
the	O
approximate	O
marginals	O
computed	O
by	O
lbp	O
,	O
as	O
shown	O
in	O
(	O
wainwright	O
et	O
al	O
.	O
2003	O
;	O
vinyals	O
et	O
al	O
.	O
2010	O
)	O
.	O
much	O
stronger	O
results	O
are	O
available	O
in	O
the	O
case	O
of	O
gaussian	O
models	O
(	O
weiss	O
and	O
freeman	O
2001a	O
;	O
johnson	O
et	O
al	O
.	O
2006	O
;	O
bickson	O
2009	O
)	O
.	O
in	O
particular	O
,	O
in	O
the	O
gaussian	O
case	O
,	O
if	O
the	O
method	O
converges	O
,	O
the	O
means	O
are	O
exact	O
,	O
although	O
the	O
variances	O
are	O
not	O
(	O
typically	O
the	O
beliefs	O
are	O
over	O
conﬁdent	O
)	O
.	O
22.2.	O
loopy	B
belief	I
propagation	I
:	O
algorithmic	O
issues	O
775	O
22.2.6	O
other	O
speedup	O
tricks	O
for	O
lbp	O
*	O
there	O
are	O
several	O
tricks	O
one	O
can	O
use	O
to	O
make	O
bp	O
run	O
faster	O
.	O
we	O
discuss	O
some	O
of	O
them	O
below	O
.	O
22.2.6.1	O
fast	O
message	O
computation	O
for	O
large	O
state	O
spaces	O
the	O
cost	O
of	O
computing	O
each	O
message	O
in	O
bp	O
(	O
whether	O
in	O
a	O
tree	B
or	O
a	O
loopy	O
graph	O
)	O
is	O
o	O
(	O
k	O
f	O
)	O
,	O
where	O
k	O
is	O
the	O
number	O
of	O
states	O
,	O
and	O
f	O
is	O
the	O
size	O
of	O
the	O
largest	O
factor	B
(	O
f	O
=	O
2	O
for	O
pairwise	O
in	O
many	O
vision	O
problems	O
(	O
e.g.	O
,	O
image	B
denoising	I
)	O
,	O
k	O
is	O
quite	O
large	O
(	O
say	O
256	O
)	O
,	O
because	O
ugms	O
)	O
.	O
it	O
represents	O
the	O
discretization	O
of	O
some	O
underlying	O
continuous	O
space	O
,	O
so	O
o	O
(	O
k	O
2	O
)	O
per	O
message	O
is	O
too	O
expensive	O
.	O
fortunately	O
,	O
for	O
certain	O
kinds	O
of	O
pairwise	O
potential	O
functions	O
of	O
the	O
form	O
ψst	O
(	O
xs	O
,	O
xt	O
)	O
=ψ	O
(	O
xs	O
−	O
xt	O
)	O
,	O
one	O
can	O
compute	O
the	O
sum-product	B
messages	O
in	O
o	O
(	O
k	O
log	O
k	O
)	O
time	O
using	O
the	O
fast	O
fourier	O
transform	O
or	O
fft	O
,	O
as	O
explained	O
in	O
(	O
felzenszwalb	O
and	O
huttenlocher	O
2006	O
)	O
.	O
the	O
key	O
insight	O
is	O
that	O
message	O
computation	O
is	O
just	O
convolution	O
:	O
(	O
cid:4	O
)	O
xs	O
(	O
cid:26	O
)	O
m	O
k	O
st	O
(	O
xt	O
)	O
=	O
ψ	O
(	O
xs	O
−	O
xt	O
)	O
h	O
(	O
xs	O
)	O
(	O
22.11	O
)	O
v∈nbr	O
(	O
s	O
)	O
\t	O
m	O
k−1	O
vs	O
where	O
h	O
(	O
xs	O
)	O
=	O
ψs	O
(	O
xs	O
)	O
(	O
xs	O
)	O
.	O
if	O
the	O
potential	B
function	I
ψ	O
(	O
z	O
)	O
is	O
a	O
gaussian-like	O
potential	O
,	O
we	O
can	O
compute	O
the	O
convolution	O
in	O
o	O
(	O
k	O
)	O
time	O
by	O
sequentially	O
convolving	O
with	O
a	O
small	O
number	O
of	O
box	O
ﬁlters	O
(	O
felzenszwalb	O
and	O
huttenlocher	O
2006	O
)	O
.	O
for	O
the	O
max-product	B
case	O
,	O
a	O
technique	O
called	O
the	O
distance	B
transform	I
can	O
be	O
used	O
to	O
compute	O
messages	O
in	O
o	O
(	O
k	O
)	O
time	O
.	O
however	O
,	O
this	O
only	O
works	O
if	O
ψ	O
(	O
z	O
)	O
=	O
exp	O
(	O
−e	O
(	O
z	O
)	O
)	O
and	O
where	O
e	O
(	O
z	O
)	O
has	O
one	O
the	O
following	O
forms	O
:	O
quadratic	O
,	O
e	O
(	O
z	O
)	O
=	O
z2	O
;	O
truncated	O
linear	O
,	O
e	O
(	O
z	O
)	O
=	O
min	O
(	O
c1|z|	O
,	O
c2	O
)	O
;	O
or	O
potts	O
model	O
,	O
e	O
(	O
z	O
)	O
=	O
c	O
i	O
(	O
z	O
(	O
cid:8	O
)	O
=	O
0	O
)	O
.	O
see	O
(	O
felzenszwalb	O
and	O
huttenlocher	O
2006	O
)	O
for	O
details	O
.	O
22.2.6.2	O
multi-scale	O
methods	O
a	O
method	O
which	O
is	O
speciﬁc	O
to	O
2d	O
lattice	B
structures	O
,	O
which	O
commonly	O
arise	O
in	O
computer	O
vision	O
,	O
is	O
based	O
on	O
multi-grid	B
techniques	I
.	O
such	O
methods	O
are	O
widely	O
used	O
in	O
numerical	O
linear	O
algebra	O
,	O
where	O
one	O
of	O
the	O
core	O
problems	O
is	O
the	O
fast	O
solution	O
of	O
linear	O
systems	O
of	O
equations	O
;	O
this	O
is	O
equivalent	O
to	O
map	O
estimation	O
in	O
a	O
gaussian	O
mrf	O
.	O
in	O
the	O
computer	O
vision	O
context	O
,	O
(	O
felzenszwalb	O
and	O
huttenlocher	O
2006	O
)	O
suggested	O
using	O
the	O
following	O
heuristic	O
to	O
signiﬁcantly	O
speedup	O
bp	O
:	O
construct	O
a	O
coarse-to-ﬁne	B
grid	I
,	O
compute	O
messages	O
at	O
the	O
coarse	O
level	O
,	O
and	O
use	O
this	O
to	O
initialize	O
messages	O
at	O
the	O
level	O
below	O
;	O
when	O
we	O
reach	O
the	O
bottom	O
level	O
,	O
just	O
a	O
few	O
iterations	O
of	O
standard	O
bp	O
are	O
required	O
,	O
since	O
long-range	O
communication	O
has	O
already	O
been	O
achieved	O
via	O
the	O
initialization	O
process	O
.	O
the	O
beliefs	O
at	O
the	O
coarse	O
level	O
are	O
computed	O
over	O
a	O
small	O
number	O
of	O
large	O
blocks	O
.	O
the	O
local	B
evidence	I
is	O
computed	O
from	O
the	O
average	O
log-probability	O
each	O
possible	O
block	O
label	B
assigns	O
to	O
all	O
the	O
pixels	O
in	O
the	O
block	O
.	O
the	O
pairwise	O
potential	O
is	O
based	O
on	O
the	O
discrepancy	O
between	O
labels	O
of	O
neighboring	O
blocks	O
,	O
taking	O
into	O
account	O
their	O
size	O
.	O
we	O
can	O
then	O
run	O
lbp	O
at	O
the	O
coarse	O
level	O
,	O
and	O
then	O
use	O
this	O
to	O
initialize	O
the	O
messages	O
one	O
level	O
down	O
.	O
note	O
that	O
the	O
model	O
is	O
still	O
a	O
ﬂat	O
grid	O
;	O
however	O
,	O
the	O
initialization	O
process	O
exploits	O
the	O
multi-scale	O
nature	O
of	O
the	O
problem	O
.	O
see	O
(	O
felzenszwalb	O
and	O
huttenlocher	O
2006	O
)	O
for	O
details	O
.	O
776	O
chapter	O
22.	O
more	O
variational	B
inference	I
22.2.6.3	O
cascades	O
another	O
trick	O
for	O
handling	O
high-dimensional	O
state-spaces	O
,	O
that	O
can	O
also	O
be	O
used	O
with	O
exact	O
inference	B
(	O
e.g.	O
,	O
for	O
chain-structured	O
crfs	O
)	O
,	O
is	O
to	O
prune	O
out	O
improbable	O
states	O
based	O
on	O
a	O
com-	O
in	O
fact	O
,	O
one	O
can	O
create	O
a	O
hierarchy	O
of	O
models	O
which	O
tradeoff	O
putationally	O
cheap	O
ﬁltering	B
step	O
.	O
speed	O
and	O
accuracy	O
.	O
this	O
is	O
called	O
a	O
computational	O
cascade	O
.	O
in	O
the	O
case	O
of	O
chains	O
,	O
one	O
can	O
guarantee	O
that	O
the	O
cascade	B
will	O
never	O
ﬁlter	O
out	O
the	O
true	O
map	O
solution	O
(	O
weiss	O
et	O
al	O
.	O
2010	O
)	O
.	O
22.3	O
loopy	B
belief	I
propagation	I
:	O
theoretical	O
issues	O
*	O
we	O
now	O
attempt	O
to	O
understand	O
the	O
lbp	O
algorithm	O
from	O
a	O
variational	O
point	O
of	O
view	O
.	O
our	O
presen-	O
tation	O
is	O
closely	O
based	O
on	O
an	O
excellent	O
300-page	O
review	O
article	O
(	O
wainwright	O
and	O
jordan	O
2008a	O
)	O
.	O
this	O
paper	O
is	O
sometimes	O
called	O
“	O
the	O
monster	O
”	O
(	O
by	O
its	O
own	O
authors	O
!	O
)	O
in	O
view	O
of	O
its	O
length	O
and	O
technical	O
difficulty	O
.	O
this	O
section	O
just	O
sketches	O
some	O
of	O
the	O
main	O
results	O
.	O
to	O
simplify	O
the	O
presentation	O
,	O
we	O
focus	O
on	O
the	O
special	O
case	O
of	O
pairwise	O
ugms	O
with	O
discrete	B
variables	O
and	O
tabular	O
potentials	O
.	O
many	O
of	O
the	O
results	O
generalize	B
to	O
ugms	O
with	O
higher-order	O
clique	B
potentials	O
(	O
which	O
includes	O
dgms	O
)	O
,	O
but	O
this	O
makes	O
the	O
notation	O
more	O
complex	O
(	O
see	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
for	O
details	O
of	O
the	O
general	O
case	O
)	O
.	O
22.3.1	O
ugms	O
represented	O
in	O
exponential	B
family	I
form	O
we	O
assume	O
the	O
distribution	O
has	O
the	O
following	O
form	O
:	O
⎧⎨	O
⎩	O
(	O
cid:4	O
)	O
s∈v	O
(	O
cid:4	O
)	O
(	O
s	O
,	O
t	O
)	O
∈e	O
⎫⎬	O
⎭	O
(	O
22.12	O
)	O
p	O
(	O
x|θ	O
,	O
g	O
)	O
=	O
1	O
z	O
(	O
θ	O
)	O
exp	O
θs	O
(	O
xs	O
)	O
+	O
θst	O
(	O
xs	O
,	O
xt	O
)	O
1	O
p	O
(	O
x|θ	O
)	O
=	O
e	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
−θt	O
φ	O
(	O
x	O
)	O
z	O
(	O
θ	O
)	O
where	O
graph	B
g	O
has	O
nodes	B
v	O
and	O
edges	B
e.	O
(	O
henceforth	O
we	O
will	O
drop	O
the	O
explicit	O
conditioning	O
on	O
θ	O
and	O
g	O
for	O
brevity	O
,	O
since	O
we	O
assume	O
both	O
are	O
known	O
and	O
ﬁxed	O
.	O
)	O
we	O
can	O
rewrite	O
this	O
in	O
exponential	B
family	I
form	O
as	O
follows	O
:	O
exp	O
(	O
−e	O
(	O
x	O
)	O
)	O
(	O
22.13	O
)	O
(	O
22.14	O
)	O
where	O
θ	O
=	O
(	O
{	O
θs	O
;	O
j	O
}	O
,	O
{	O
θs	O
,	O
t	O
;	O
j	O
,	O
k	O
}	O
)	O
are	O
all	O
the	O
node	O
and	O
edge	O
parameters	O
(	O
the	O
canonical	B
parameters	I
)	O
,	O
and	O
φ	O
(	O
x	O
)	O
=	O
(	O
{	O
i	O
(	O
xs	O
=	O
j	O
)	O
}	O
,	O
{	O
i	O
(	O
xs	O
=	O
j	O
,	O
xt	O
=	O
k	O
)	O
}	O
)	O
are	O
all	O
the	O
node	O
and	O
edge	O
indicator	O
functions	O
(	O
the	O
sufficient	B
statistics	I
)	O
.	O
note	O
:	O
we	O
use	O
s	O
,	O
t	O
∈	O
v	O
to	O
index	O
nodes	B
and	O
j	O
,	O
k	O
∈	O
x	O
to	O
index	O
states	O
.	O
the	O
mean	B
of	O
the	O
sufficient	B
statistics	I
are	O
known	O
as	O
the	O
mean	B
parameters	O
of	O
the	O
model	O
,	O
and	O
are	O
given	O
by	O
μ	O
=	O
e	O
[	O
φ	O
(	O
x	O
)	O
]	O
=	O
(	O
{	O
p	O
(	O
xs	O
=	O
j	O
)	O
}	O
s	O
,	O
{	O
p	O
(	O
xs	O
=	O
j	O
,	O
xt	O
=	O
k	O
)	O
}	O
s	O
(	O
cid:5	O
)	O
=t	O
)	O
=	O
(	O
{	O
μs	O
;	O
j	O
}	O
s	O
,	O
{	O
μst	O
;	O
jk	O
}	O
s	O
(	O
cid:5	O
)	O
=t	O
)	O
(	O
22.15	O
)	O
this	O
is	O
a	O
vector	O
of	O
length	O
d	O
=	O
|x||v	O
|	O
+	O
|x|2|e|	O
,	O
containing	O
the	O
node	O
and	O
edge	O
marginals	O
.	O
it	O
completely	O
characterizes	O
the	O
distribution	O
p	O
(	O
x|θ	O
)	O
,	O
so	O
we	O
sometimes	O
treat	O
μ	O
as	O
a	O
distribution	O
itself	O
.	O
equation	O
22.12	O
is	O
called	O
the	O
standard	B
overcomplete	I
representation	I
.	O
it	O
is	O
called	O
“	O
overcom-	O
plete	O
”	O
because	O
it	O
ignores	O
the	O
sum-to-one	O
constraints	O
.	O
in	O
some	O
cases	O
,	O
it	O
is	O
convenient	O
to	O
remove	O
22.3.	O
loopy	B
belief	I
propagation	I
:	O
theoretical	O
issues	O
*	O
777	O
this	O
redundancy	O
.	O
for	O
example	O
,	O
consider	O
an	O
ising	O
model	O
where	O
xs	O
∈	O
{	O
0	O
,	O
1	O
}	O
.	O
the	O
model	O
can	O
be	O
written	O
as	O
⎧⎨	O
⎩	O
(	O
cid:4	O
)	O
s∈v	O
(	O
cid:4	O
)	O
(	O
s	O
,	O
t	O
)	O
∈e	O
⎫⎬	O
⎭	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
(	O
θ	O
)	O
exp	O
θsxs	O
+	O
θstxsxt	O
(	O
22.16	O
)	O
hence	O
we	O
can	O
use	O
the	O
following	O
minimal	B
parameterization	O
φ	O
(	O
x	O
)	O
=	O
(	O
xs	O
,	O
s	O
∈	O
v	O
;	O
xsxt	O
,	O
(	O
s	O
,	O
t	O
)	O
∈	O
e	O
)	O
∈	O
r	O
d	O
(	O
22.17	O
)	O
where	O
d	O
=	O
|v	O
|	O
+	O
|e|	O
.	O
the	O
corresponding	O
mean	B
parameters	O
are	O
μs	O
=	O
p	O
(	O
xs	O
=	O
1	O
)	O
and	O
μst	O
=	O
p	O
(	O
xs	O
=	O
1	O
,	O
xt	O
=	O
1	O
)	O
.	O
22.3.2	O
the	O
marginal	B
polytope	I
(	O
cid:4	O
)	O
x	O
the	O
space	O
of	O
allowable	O
μ	O
vectors	O
is	O
called	O
the	O
marginal	B
polytope	I
,	O
and	O
is	O
denoted	O
m	O
(	O
g	O
)	O
,	O
where	O
g	O
is	O
the	O
structure	O
of	O
the	O
graph	B
deﬁning	O
the	O
ugm	O
.	O
this	O
is	O
deﬁned	O
to	O
be	O
the	O
set	O
of	O
all	O
mean	O
parameters	O
for	O
the	O
given	O
model	O
that	O
can	O
be	O
generated	O
from	O
a	O
valid	O
probability	O
distribution	O
:	O
d	O
:	O
∃p	O
s.t	O
.	O
μ	O
=	O
m	O
(	O
g	O
)	O
(	O
cid:2	O
)	O
{	O
μ	O
∈	O
r	O
for	O
example	O
,	O
consider	O
an	O
ising	O
model	O
.	O
p	O
(	O
x	O
)	O
=	O
1	O
}	O
(	O
22.18	O
)	O
φ	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
for	O
some	O
p	O
(	O
x	O
)	O
≥	O
0	O
,	O
if	O
we	O
have	O
just	O
two	O
nodes	B
connected	O
as	O
x1	O
−	O
x2	O
,	O
one	O
can	O
show	O
that	O
we	O
have	O
the	O
following	O
minimal	B
set	O
of	O
constraints	O
:	O
0	O
≤	O
μ12	O
,	O
0	O
≤	O
μ12	O
≤	O
μ1	O
,	O
⎛	O
0	O
≤	O
μ12	O
≤	O
μ2	O
,	O
and	O
1	O
+	O
μ12	O
−	O
μ1	O
−	O
μ2	O
≥	O
0.	O
we	O
can	O
write	O
these	O
in	O
matrix-vector	O
form	O
as	O
⎜⎜⎝	O
0	O
⎛	O
⎜⎜⎝	O
0	O
0	O
⎛	O
⎝	O
μ1	O
⎞	O
⎠	O
≥	O
⎞	O
⎟⎟⎠	O
⎞	O
⎟⎟⎠	O
(	O
22.19	O
)	O
(	O
cid:4	O
)	O
x	O
0	O
1	O
0	O
−1	O
1	O
−1	O
1	O
1	O
0	O
−1	O
−1	O
μ2	O
μ12	O
0−1	O
these	O
four	O
constraints	O
deﬁne	O
a	O
series	O
of	O
half-planes	O
,	O
whose	O
intersection	O
deﬁnes	O
a	O
polytope	O
,	O
as	O
shown	O
in	O
figure	O
22.7	O
(	O
a	O
)	O
.	O
since	O
m	O
(	O
g	O
)	O
is	O
obtained	O
by	O
taking	O
a	O
convex	B
combination	I
of	O
the	O
φ	O
(	O
x	O
)	O
vectors	O
,	O
it	O
can	O
also	O
be	O
written	O
as	O
the	O
convex	B
hull	I
of	O
the	O
feature	O
set	O
:	O
m	O
(	O
g	O
)	O
=	O
conv	O
{	O
φ1	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
φd	O
(	O
x	O
)	O
}	O
for	O
example	O
,	O
for	O
a	O
2	O
node	O
mrf	O
x1	O
−	O
x2	O
with	O
binary	O
states	O
,	O
we	O
have	O
m	O
(	O
g	O
)	O
=	O
conv	O
{	O
(	O
0	O
,	O
0	O
,	O
0	O
)	O
,	O
(	O
1	O
,	O
0	O
,	O
0	O
)	O
,	O
(	O
0	O
,	O
1	O
,	O
0	O
)	O
,	O
(	O
1	O
,	O
1	O
,	O
1	O
)	O
}	O
(	O
22.20	O
)	O
(	O
22.21	O
)	O
these	O
are	O
the	O
four	O
black	O
dots	O
in	O
figure	O
22.7	O
(	O
a	O
)	O
.	O
we	O
see	O
that	O
the	O
convex	B
hull	I
deﬁnes	O
the	O
same	O
volume	O
as	O
the	O
intersection	O
of	O
half-spaces	O
.	O
the	O
marginal	B
polytope	I
will	O
play	O
a	O
crucial	O
role	O
in	O
the	O
approximate	B
inference	I
algorithms	O
we	O
discuss	O
in	O
the	O
rest	O
of	O
this	O
chapter	O
.	O
778	O
chapter	O
22.	O
more	O
variational	B
inference	I
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
a	O
)	O
illustration	O
of	O
the	O
marginal	B
polytope	I
for	O
an	O
ising	O
model	O
with	O
two	O
variables	O
.	O
figure	O
22.7	O
(	O
b	O
)	O
cartoon	O
illustration	O
of	O
the	O
set	O
mf	O
(	O
g	O
)	O
,	O
which	O
is	O
a	O
nonconvex	O
inner	O
bound	O
on	O
the	O
marginal	B
polytope	I
m	O
(	O
g	O
)	O
.	O
mf	O
(	O
g	O
)	O
is	O
used	O
by	O
mean	B
ﬁeld	I
.	O
(	O
c	O
)	O
cartoon	O
illustration	O
of	O
the	O
relationship	O
between	O
m	O
(	O
g	O
)	O
and	O
l	O
(	O
g	O
)	O
,	O
which	O
is	O
used	O
by	O
loopy	O
bp	O
.	O
the	O
set	O
l	O
(	O
g	O
)	O
is	O
always	O
an	O
outer	O
bound	O
on	O
m	O
(	O
g	O
)	O
,	O
and	O
the	O
inclusion	O
m	O
(	O
g	O
)	O
⊂	O
l	O
(	O
g	O
)	O
is	O
strict	B
whenever	O
g	O
has	O
loops	O
.	O
both	O
sets	O
are	O
polytopes	O
,	O
which	O
can	O
be	O
deﬁned	O
as	O
an	O
intersection	O
of	O
half-planes	O
(	O
deﬁned	O
by	O
facets	O
)	O
,	O
or	O
as	O
the	O
convex	B
hull	I
of	O
the	O
vertices	B
.	O
l	O
(	O
g	O
)	O
actually	O
has	O
fewer	O
facets	O
than	O
m	O
(	O
g	O
)	O
,	O
despite	O
the	O
picture	O
.	O
in	O
fact	O
,	O
l	O
(	O
g	O
)	O
has	O
o	O
(	O
|x||v	O
|	O
+|x|2|e|	O
)	O
facets	O
,	O
where	O
|x|	O
is	O
the	O
number	O
of	O
states	O
per	O
variable	O
,	O
|v	O
|	O
is	O
the	O
number	O
of	O
variables	O
,	O
and	O
|e|	O
is	O
the	O
number	O
of	O
edges	B
.	O
by	O
contrast	O
,	O
m	O
(	O
g	O
)	O
has	O
o	O
(	O
|x||v	O
|	O
)	O
facets	O
.	O
on	O
the	O
other	O
hand	O
,	O
l	O
(	O
g	O
)	O
has	O
more	O
vertices	B
than	O
m	O
(	O
g	O
)	O
,	O
despite	O
the	O
picture	O
,	O
since	O
l	O
(	O
g	O
)	O
contains	O
all	O
the	O
binary	O
vector	O
extreme	O
points	O
μ	O
∈	O
m	O
(	O
g	O
)	O
,	O
plus	O
additional	O
fractional	O
extreme	O
points	O
.	O
source	O
:	O
figures	O
3.6	O
,	O
5.4	O
and	O
4.2	O
of	O
(	O
wainwright	O
and	O
jordan	O
2008a	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
martin	O
wainwright	O
.	O
22.3.3	O
exact	O
inference	B
as	O
a	O
variational	O
optimization	O
problem	O
recall	B
from	O
section	O
21.2	O
that	O
the	O
goal	O
of	O
variational	B
inference	I
is	O
to	O
ﬁnd	O
the	O
distribution	O
q	O
that	O
maximizes	O
the	O
energy	B
functional	I
l	O
(	O
q	O
)	O
=	O
−kl	O
(	O
q||p	O
)	O
+	O
log	O
z	O
=	O
eq	O
[	O
log	O
˜p	O
(	O
x	O
)	O
]	O
+	O
h	O
(	O
q	O
)	O
≤	O
log	O
z	O
where	O
˜p	O
(	O
x	O
)	O
=zp	O
(	O
x	O
)	O
is	O
the	O
unnormalized	O
posterior	O
.	O
let	O
q	O
=	O
p	O
,	O
then	O
the	O
exact	O
energy	B
functional	I
becomes	O
max	O
μ∈m	O
(	O
g	O
)	O
θt	O
μ	O
+	O
h	O
(	O
μ	O
)	O
(	O
22.22	O
)	O
if	O
we	O
write	O
log	O
˜p	O
(	O
x	O
)	O
=θ	O
t	O
φ	O
(	O
x	O
)	O
,	O
and	O
we	O
(	O
22.23	O
)	O
where	O
μ	O
=	O
ep	O
[	O
φ	O
(	O
x	O
)	O
]	O
is	O
a	O
joint	B
distribution	I
over	O
all	O
state	O
conﬁgurations	O
x	O
(	O
so	O
it	O
is	O
valid	O
to	O
write	O
h	O
(	O
μ	O
)	O
)	O
.	O
since	O
the	O
kl	O
divergence	O
is	O
zero	O
when	O
p	O
=	O
q	O
,	O
we	O
know	O
that	O
max	O
μ∈m	O
(	O
g	O
)	O
θt	O
μ	O
+	O
h	O
(	O
μ	O
)	O
=	O
log	O
z	O
(	O
θ	O
)	O
(	O
22.24	O
)	O
this	O
is	O
a	O
way	O
to	O
cast	O
exact	O
inference	B
as	O
a	O
variational	O
optimization	O
problem	O
.	O
equation	O
22.24	O
seems	O
easy	O
to	O
optimize	O
:	O
the	O
objective	O
is	O
concave	B
,	O
since	O
it	O
is	O
the	O
sum	O
of	O
a	O
linear	O
function	O
and	O
a	O
concave	B
function	O
(	O
see	O
figure	O
2.21	O
to	O
see	O
why	O
entropy	B
is	O
concave	B
)	O
;	O
furthermore	O
,	O
we	O
are	O
maximizing	O
this	O
over	O
a	O
convex	B
set	O
.	O
however	O
,	O
the	O
marginal	B
polytope	I
m	O
(	O
g	O
)	O
has	O
exponentially	O
many	O
facets	O
.	O
in	O
some	O
cases	O
,	O
there	O
is	O
structure	O
to	O
this	O
polytope	O
that	O
can	O
be	O
exploited	O
by	O
dynamic	B
programming	I
(	O
as	O
we	O
saw	O
in	O
chapter	O
20	O
)	O
,	O
but	O
in	O
general	O
,	O
exact	O
inference	B
takes	O
exponential	O
time	O
.	O
most	O
of	O
the	O
existing	O
deterministic	O
approximate	O
inference	B
schemes	O
that	O
have	O
been	O
proposed	O
in	O
the	O
literature	O
can	O
be	O
seen	O
as	O
different	O
approximations	O
to	O
the	O
marginal	B
polytope	I
,	O
as	O
we	O
explain	O
below	O
.	O
22.3.	O
loopy	B
belief	I
propagation	I
:	O
theoretical	O
issues	O
*	O
779	O
22.3.4	O
mean	B
ﬁeld	I
as	O
a	O
variational	O
optimization	O
problem	O
we	O
discussed	O
mean	B
ﬁeld	I
at	O
length	O
in	O
chapter	O
21.	O
let	O
us	O
re-interpret	O
mean	B
ﬁeld	I
inference	O
in	O
our	O
new	O
more	O
abstract	O
framework	O
.	O
this	O
will	O
help	O
us	O
compare	O
it	O
to	O
other	O
approximate	O
methods	O
which	O
we	O
discuss	O
below	O
.	O
first	O
,	O
let	O
f	O
be	O
an	O
edge	O
subgraph	O
of	O
the	O
original	O
graph	B
g	O
,	O
and	O
let	O
i	O
(	O
f	O
)	O
⊆	O
i	O
be	O
the	O
subset	O
of	O
sufficient	B
statistics	I
associated	O
with	O
the	O
cliques	B
of	O
f	O
.	O
let	O
ω	O
be	O
the	O
set	O
of	O
canonical	B
parameters	I
for	O
the	O
full	B
model	O
,	O
and	O
deﬁne	O
the	O
canonical	O
parameter	O
space	O
for	O
the	O
submodel	O
as	O
follows	O
:	O
ω	O
(	O
f	O
)	O
(	O
cid:2	O
)	O
{	O
θ	O
∈	O
ω	O
:	O
θ	O
α	O
=	O
0	O
∀α	O
∈	O
i	O
\	O
i	O
(	O
f	O
)	O
}	O
in	O
other	O
words	O
,	O
we	O
require	O
that	O
the	O
natural	B
parameters	I
associated	O
with	O
the	O
sufficient	B
statistics	I
α	O
outside	O
of	O
our	O
chosen	O
class	O
to	O
be	O
zero	O
.	O
in	O
the	O
case	O
of	O
a	O
fully	O
factorized	O
approximation	O
,	O
f0	O
,	O
we	O
remove	O
all	O
edges	O
from	O
the	O
graph	B
,	O
giving	O
for	O
example	O
,	O
ω	O
(	O
f0	O
)	O
(	O
cid:2	O
)	O
{	O
θ	O
∈	O
ω	O
:	O
θ	O
st	O
=	O
0	O
∀	O
(	O
s	O
,	O
t	O
)	O
∈	O
e	O
}	O
in	O
the	O
case	O
of	O
structured	B
mean	I
ﬁeld	I
(	O
section	O
21.4	O
)	O
,	O
we	O
set	O
θst	O
=	O
0	O
for	O
edges	B
which	O
are	O
not	O
in	O
our	O
tractable	O
subgraph	O
.	O
next	O
,	O
we	O
deﬁne	O
the	O
mean	B
parameter	O
space	O
of	O
the	O
restricted	O
model	O
as	O
follows	O
:	O
mf	O
(	O
g	O
)	O
(	O
cid:2	O
)	O
{	O
μ	O
∈	O
r	O
(	O
22.27	O
)	O
this	O
is	O
called	O
an	O
inner	B
approximation	I
to	O
the	O
marginal	B
polytope	I
,	O
since	O
mf	O
(	O
g	O
)	O
⊆	O
m	O
(	O
g	O
)	O
.	O
see	O
figure	O
22.7	O
(	O
b	O
)	O
for	O
a	O
sketch	O
.	O
note	O
that	O
mf	O
(	O
g	O
)	O
is	O
a	O
non-convex	O
polytope	O
,	O
which	O
results	O
in	O
multiple	O
local	O
optima	O
.	O
by	O
contrast	O
,	O
some	O
of	O
the	O
approximations	O
we	O
will	O
consider	O
later	O
will	O
be	O
convex	B
.	O
for	O
some	O
θ	O
∈	O
ω	O
(	O
f	O
)	O
}	O
d	O
:	O
μ	O
=	O
eθ	O
[	O
φ	O
(	O
x	O
)	O
]	O
we	O
deﬁne	O
the	O
entropy	B
of	O
our	O
approximation	O
h	O
(	O
μ	O
(	O
f	O
)	O
)	O
as	O
the	O
entropy	B
of	O
the	O
distribution	O
μ	O
deﬁned	O
on	O
submodel	O
f	O
.	O
then	O
we	O
deﬁne	O
the	O
mean	B
ﬁeld	I
energy	I
functional	I
optimization	O
problem	O
as	O
follows	O
:	O
(	O
22.25	O
)	O
(	O
22.26	O
)	O
(	O
22.28	O
)	O
max	O
μ∈mf	O
(	O
g	O
)	O
θt	O
μ	O
+	O
h	O
(	O
μ	O
)	O
≤	O
log	O
z	O
(	O
θ	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
θs	O
(	O
xs	O
)	O
μs	O
(	O
xs	O
)	O
+	O
max	O
μ∈p	O
d	O
s∈v	O
xs	O
(	O
cid:4	O
)	O
(	O
s	O
,	O
t	O
)	O
∈e	O
xs	O
,	O
xt	O
in	O
the	O
case	O
of	O
the	O
fully	O
factorized	O
mean	B
ﬁeld	I
approximation	O
for	O
pairwise	O
ugms	O
,	O
we	O
can	O
write	O
this	O
objective	O
as	O
follows	O
:	O
θst	O
(	O
xs	O
,	O
xt	O
)	O
μs	O
(	O
xs	O
)	O
μt	O
(	O
xt	O
)	O
+	O
h	O
(	O
μs	O
)	O
(	O
22.29	O
)	O
(	O
cid:4	O
)	O
s∈v	O
where	O
μs	O
∈	O
p	O
,	O
and	O
p	O
is	O
the	O
probability	B
simplex	I
over	O
x	O
.	O
mean	B
ﬁeld	I
involves	O
a	O
concave	B
objective	O
being	O
maximized	O
over	O
a	O
non-convex	O
set	O
.	O
it	O
is	O
typically	O
optimized	O
using	O
coordinate	O
ascent	O
,	O
since	O
it	O
is	O
easy	O
to	O
optimize	O
a	O
scalar	O
concave	O
function	O
over	O
p	O
for	O
each	O
μs	O
.	O
for	O
example	O
,	O
for	O
a	O
pairwise	O
ugm	O
we	O
get	O
μs	O
(	O
xs	O
)	O
∝	O
exp	O
(	O
θs	O
(	O
xs	O
)	O
)	O
exp	O
μt	O
(	O
xt	O
)	O
θst	O
(	O
xs	O
,	O
xt	O
)	O
(	O
22.30	O
)	O
⎞	O
⎠	O
⎛	O
⎝	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
t∈nbr	O
(	O
s	O
)	O
xt	O
22.3.5	O
lbp	O
as	O
a	O
variational	O
optimization	O
problem	O
in	O
this	O
section	O
,	O
we	O
explain	O
how	O
lbp	O
can	O
be	O
viewed	O
as	O
a	O
variational	B
inference	I
problem	O
.	O
780	O
chapter	O
22.	O
more	O
variational	B
inference	I
figure	O
22.8	O
(	O
a	O
)	O
illustration	O
of	O
pairwise	O
ugm	O
on	O
binary	O
nodes	O
,	O
together	O
with	O
a	O
set	O
of	O
pseudo	B
marginals	I
that	O
are	O
not	O
globally	O
consistent	B
.	O
(	O
b	O
)	O
a	O
slice	O
of	O
the	O
marginal	B
polytope	I
illustrating	O
the	O
set	O
of	O
feasible	O
edge	O
marginals	O
,	O
assuming	O
the	O
node	O
marginals	O
are	O
clamped	O
at	O
μ1	O
=	O
μ2	O
=	O
μ3	O
=	O
0.5.	O
source	O
:	O
figure	O
4.1	O
of	O
(	O
wainwright	O
and	O
jordan	O
2008a	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
martin	O
wainwright	O
.	O
22.3.5.1	O
an	O
outer	B
approximation	I
to	O
the	O
marginal	B
polytope	I
if	O
we	O
want	O
to	O
consider	O
all	O
possible	O
probability	O
distributions	O
which	O
are	O
markov	O
wrt	O
our	O
model	O
,	O
we	O
need	O
to	O
consider	O
all	O
vectors	O
μ	O
∈	O
m	O
(	O
g	O
)	O
.	O
since	O
the	O
set	O
m	O
(	O
g	O
)	O
is	O
exponentially	O
large	O
,	O
it	O
is	O
usually	O
infeasible	O
to	O
optimize	O
over	O
.	O
a	O
standard	O
strategy	O
in	O
combinatorial	O
optimization	B
is	O
to	O
relax	O
the	O
constraints	O
.	O
in	O
this	O
case	O
,	O
instead	O
of	O
requiring	O
probability	O
vector	O
μ	O
to	O
live	O
in	O
m	O
(	O
g	O
)	O
,	O
we	O
consider	O
a	O
vector	O
τ	O
that	O
only	O
satisﬁes	O
the	O
following	O
local	B
consistency	I
constraints	O
:	O
(	O
22.31	O
)	O
(	O
22.32	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
xt	O
τs	O
(	O
xs	O
)	O
=	O
1	O
xs	O
τst	O
(	O
xs	O
,	O
xt	O
)	O
=	O
τs	O
(	O
xs	O
)	O
the	O
ﬁrst	O
constraint	O
is	O
called	O
the	O
normalization	O
constraint	O
,	O
and	O
the	O
second	O
is	O
called	O
the	O
marginal-	O
ization	O
constraint	O
.	O
we	O
then	O
deﬁne	O
the	O
set	O
l	O
(	O
g	O
)	O
(	O
cid:2	O
)	O
{	O
τ	O
≥	O
0	O
:	O
(	O
22.31	O
)	O
holds	O
∀s	O
∈	O
v	O
and	O
(	O
22.32	O
)	O
holds	O
∀	O
(	O
s	O
,	O
t	O
)	O
∈	O
e	O
}	O
(	O
22.33	O
)	O
the	O
set	O
l	O
(	O
g	O
)	O
is	O
also	O
a	O
polytope	O
,	O
but	O
it	O
only	O
has	O
o	O
(	O
|v	O
|	O
+	O
|e|	O
)	O
constraints	O
.	O
it	O
is	O
a	O
convex	B
outer	O
approximation	O
on	O
m	O
(	O
g	O
)	O
,	O
as	O
shown	O
in	O
figure	O
22.7	O
(	O
c	O
)	O
.	O
we	O
call	O
the	O
terms	O
τs	O
,	O
τst	O
∈	O
l	O
(	O
g	O
)	O
pseudo	B
marginals	I
,	O
since	O
they	O
may	O
not	O
correspond	O
to	O
marginals	O
of	O
any	O
valid	O
probability	O
distribution	O
.	O
as	O
an	O
example	O
of	O
this	O
,	O
consider	O
figure	O
22.8	O
(	O
a	O
)	O
.	O
the	O
picture	O
shows	O
a	O
set	O
of	O
pseudo	O
node	O
and	O
edge	O
marginals	O
,	O
which	O
satisfy	O
the	O
local	B
consistency	I
requirements	O
.	O
however	O
,	O
they	O
are	O
not	O
globally	O
consistent	B
.	O
to	O
see	O
why	O
,	O
note	O
that	O
τ12	O
implies	O
p	O
(	O
x1	O
=	O
x2	O
)	O
=	O
0.8	O
,	O
τ23	O
implies	O
p	O
(	O
x2	O
=	O
x3	O
)	O
=	O
0.8	O
,	O
but	O
τ13	O
implies	O
p	O
(	O
x1	O
=	O
x3	O
)	O
=	O
0.2	O
,	O
which	O
is	O
not	O
possible	O
(	O
see	O
(	O
wainwright	O
and	O
jordan	O
2008b	O
,	O
p81	O
)	O
for	O
a	O
formal	O
proof	O
)	O
.	O
indeed	O
,	O
figure	O
22.8	O
(	O
b	O
)	O
shows	O
that	O
l	O
(	O
g	O
)	O
contains	O
points	O
that	O
are	O
not	O
in	O
m	O
(	O
g	O
)	O
.	O
we	O
claim	O
that	O
m	O
(	O
g	O
)	O
⊆	O
l	O
(	O
g	O
)	O
,	O
with	O
equality	O
iff	B
g	O
is	O
a	O
tree	B
.	O
to	O
see	O
this	O
,	O
ﬁrst	O
consider	O
22.3.	O
loopy	B
belief	I
propagation	I
:	O
theoretical	O
issues	O
*	O
781	O
an	O
element	O
μ	O
∈	O
m	O
(	O
g	O
)	O
.	O
any	O
such	O
vector	O
must	O
satisfy	O
the	O
normalization	O
and	O
marginalization	O
constraints	O
,	O
hence	O
m	O
(	O
g	O
)	O
⊆	O
l	O
(	O
g	O
)	O
.	O
now	O
consider	O
the	O
converse	O
.	O
suppose	O
t	O
is	O
a	O
tree	B
,	O
and	O
let	O
μ	O
∈	O
l	O
(	O
t	O
)	O
.	O
by	O
deﬁnition	O
,	O
this	O
satisﬁes	O
the	O
normalization	O
and	O
marginalization	O
constraints	O
.	O
however	O
,	O
any	O
tree	B
can	O
be	O
represented	O
in	O
the	O
form	O
pμ	O
(	O
x	O
)	O
=	O
μs	O
(	O
xs	O
)	O
μst	O
(	O
xs	O
,	O
xt	O
)	O
μs	O
(	O
xs	O
)	O
μt	O
(	O
xt	O
)	O
(	O
22.34	O
)	O
(	O
cid:20	O
)	O
s∈v	O
(	O
cid:20	O
)	O
(	O
s	O
,	O
t	O
)	O
∈e	O
hence	O
satsifying	O
normalization	O
and	O
local	B
consistency	I
is	O
enough	O
to	O
deﬁne	O
a	O
valid	O
distribution	O
for	O
any	O
tree	B
.	O
hence	O
μ	O
∈	O
m	O
(	O
t	O
)	O
as	O
well	O
.	O
in	O
contrast	O
,	O
if	O
the	O
graph	B
has	O
loops	O
,	O
we	O
have	O
that	O
m	O
(	O
g	O
)	O
(	O
cid:8	O
)	O
=	O
l	O
(	O
g	O
)	O
.	O
see	O
figure	O
22.8	O
(	O
b	O
)	O
for	O
an	O
example	O
of	O
this	O
fact	O
.	O
22.3.5.2	O
the	O
entropy	B
approximation	O
from	O
equation	O
22.34	O
,	O
we	O
can	O
write	O
the	O
exact	O
entropy	B
of	O
any	O
tree	B
structured	O
distribution	O
μ	O
∈	O
m	O
(	O
t	O
)	O
as	O
follows	O
:	O
(	O
cid:4	O
)	O
(	O
s	O
,	O
t	O
)	O
∈e	O
ist	O
(	O
μst	O
)	O
μs	O
(	O
xs	O
)	O
log	O
μs	O
(	O
xs	O
)	O
(	O
cid:4	O
)	O
hs	O
(	O
μs	O
)	O
−	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
xs∈xs	O
s∈v	O
(	O
xs	O
,	O
xt	O
)	O
∈xs×xt	O
h	O
(	O
μ	O
)	O
=	O
hs	O
(	O
μs	O
)	O
=−	O
ist	O
(	O
μst	O
)	O
=	O
μst	O
(	O
xs	O
,	O
xt	O
)	O
log	O
μst	O
(	O
xs	O
,	O
xt	O
)	O
μs	O
(	O
xs	O
)	O
μt	O
(	O
xt	O
)	O
(	O
22.35	O
)	O
(	O
22.36	O
)	O
(	O
22.37	O
)	O
note	O
that	O
we	O
can	O
rewrite	O
the	O
mutual	B
information	I
term	O
in	O
the	O
form	O
ist	O
(	O
μst	O
)	O
=	O
hs	O
(	O
μs	O
)	O
+ht	O
(	O
μt	O
)	O
−	O
hst	O
(	O
μst	O
)	O
,	O
and	O
hence	O
we	O
get	O
the	O
following	O
alternative	O
but	O
equivalent	O
expression	O
:	O
h	O
(	O
μ	O
)	O
=−	O
(	O
ds	O
−	O
1	O
)	O
hs	O
(	O
μs	O
)	O
+	O
hst	O
(	O
μst	O
)	O
(	O
22.38	O
)	O
(	O
cid:4	O
)	O
(	O
s	O
,	O
t	O
)	O
∈e	O
where	O
ds	O
is	O
the	O
degree	B
(	O
number	O
of	O
neighbors	B
)	O
for	O
node	O
s.	O
the	O
bethe1	O
approximation	O
to	O
the	O
entropy	B
is	O
simply	O
the	O
use	O
of	O
equation	O
22.35	O
even	O
when	O
we	O
(	O
cid:4	O
)	O
s∈v	O
(	O
cid:4	O
)	O
s∈v	O
(	O
cid:15	O
)	O
don	O
’	O
t	O
have	O
a	O
tree	B
:	O
hbethe	O
(	O
τ	O
)	O
=	O
hs	O
(	O
τs	O
)	O
−	O
we	O
deﬁne	O
the	O
bethe	O
free	B
energy	I
as	O
fbethe	O
(	O
τ	O
)	O
(	O
cid:2	O
)	O
−	O
θt	O
τ	O
+	O
hbethe	O
(	O
τ	O
)	O
(	O
cid:4	O
)	O
(	O
cid:16	O
)	O
(	O
s	O
,	O
t	O
)	O
∈e	O
ist	O
(	O
τst	O
)	O
(	O
22.39	O
)	O
(	O
22.40	O
)	O
we	O
deﬁne	O
the	O
bethe	O
energy	B
functional	I
as	O
the	O
negative	O
of	O
the	O
bethe	O
free	B
energy	I
.	O
1.	O
hans	O
bethe	O
was	O
a	O
german-american	O
physicist	O
,	O
1906–2005	O
.	O
782	O
chapter	O
22.	O
more	O
variational	B
inference	I
22.3.5.3	O
the	O
lbp	O
objective	O
combining	O
the	O
outer	B
approximation	I
l	O
(	O
g	O
)	O
with	O
the	O
bethe	O
approximation	O
to	O
the	O
entropy	B
,	O
we	O
get	O
the	O
following	O
bethe	O
variational	O
problem	O
(	O
bvp	O
)	O
:	O
min	O
τ∈l	O
(	O
g	O
)	O
fbethe	O
(	O
τ	O
)	O
=	O
max	O
τ∈l	O
(	O
g	O
)	O
θt	O
τ	O
+	O
hbethe	O
(	O
τ	O
)	O
(	O
22.41	O
)	O
the	O
space	O
we	O
are	O
optimizing	O
over	O
is	O
a	O
convex	B
set	O
,	O
but	O
the	O
objective	O
itself	O
is	O
not	O
concave	O
(	O
since	O
hbethe	O
is	O
not	O
concave	O
)	O
.	O
thus	O
there	O
can	O
be	O
multiple	O
local	O
optima	O
of	O
the	O
bvp	O
.	O
the	O
value	O
obtained	O
by	O
the	O
bvp	O
is	O
an	O
approximation	O
to	O
log	O
z	O
(	O
θ	O
)	O
.	O
in	O
the	O
case	O
of	O
trees	O
,	O
the	O
approximation	O
is	O
exact	O
,	O
and	O
in	O
the	O
case	O
of	O
models	O
with	O
attractive	O
potentials	O
,	O
the	O
approximation	O
turns	O
out	O
to	O
be	O
an	O
upper	O
bound	O
(	O
sudderth	O
et	O
al	O
.	O
2008	O
)	O
.	O
22.3.5.4	O
message	B
passing	I
and	O
lagrange	O
multipliers	O
xs	O
1	O
−	O
(	O
cid:7	O
)	O
τs	O
(	O
xs	O
)	O
,	O
and	O
the	O
marginalization	O
constraint	O
as	O
cts	O
(	O
xs	O
;	O
τ	O
)	O
(	O
cid:2	O
)	O
τs	O
(	O
xs	O
)	O
−	O
(	O
cid:7	O
)	O
(	O
cid:25	O
)	O
in	O
this	O
subsection	O
,	O
we	O
will	O
show	O
that	O
any	O
ﬁxed	B
point	I
of	O
the	O
lbp	O
algorithm	O
deﬁnes	O
a	O
stationary	B
point	O
of	O
the	O
above	O
constrained	O
objective	O
.	O
let	O
us	O
deﬁne	O
the	O
normalization	O
constraint	O
at	O
css	O
(	O
τ	O
)	O
(	O
cid:2	O
)	O
τst	O
(	O
xs	O
,	O
xt	O
)	O
for	O
each	O
edge	O
t	O
→	O
s.	O
we	O
can	O
now	O
write	O
the	O
lagrangian	O
as	O
λsscss	O
(	O
τ	O
)	O
(	O
cid:4	O
)	O
l	O
(	O
τ	O
,	O
λ	O
;	O
θ	O
)	O
(	O
cid:2	O
)	O
θt	O
τ	O
+	O
hbethe	O
(	O
τ	O
)	O
+	O
(	O
cid:24	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
xt	O
s	O
+	O
(	O
22.42	O
)	O
(	O
the	O
constraint	O
that	O
τ	O
≥	O
0	O
is	O
not	O
explicitly	O
enforced	O
,	O
but	O
one	O
can	O
show	O
that	O
it	O
will	O
hold	O
at	O
the	O
optimum	O
since	O
θ	O
>	O
0	O
.	O
)	O
some	O
simple	O
algebra	O
then	O
shows	O
that	O
∇τl	O
=	O
0	O
yields	O
λts	O
(	O
xs	O
)	O
cts	O
(	O
xs	O
;	O
τ	O
)	O
+	O
λst	O
(	O
xt	O
)	O
cst	O
(	O
xt	O
;	O
τ	O
)	O
s	O
,	O
t	O
xs	O
xt	O
τ	O
(	O
xs	O
,	O
xt	O
)	O
.	O
using	O
the	O
fact	O
that	O
the	O
marginalization	O
con-	O
(	O
cid:4	O
)	O
t∈nbr	O
(	O
s	O
)	O
log	O
τs	O
(	O
xs	O
)	O
=λ	O
ss	O
+	O
θs	O
(	O
xs	O
)	O
+	O
λts	O
(	O
xs	O
)	O
log	O
τst	O
(	O
xs	O
,	O
xt	O
)	O
˜τs	O
(	O
xs	O
)	O
˜τt	O
(	O
xt	O
)	O
=	O
θst	O
(	O
xs	O
,	O
xt	O
)	O
−	O
λts	O
(	O
xs	O
)	O
−	O
λst	O
(	O
xt	O
)	O
(	O
cid:7	O
)	O
where	O
we	O
have	O
deﬁned	O
˜τs	O
(	O
xs	O
)	O
(	O
cid:2	O
)	O
xt	O
straint	O
implies	O
˜τs	O
(	O
xs	O
)	O
=	O
τs	O
(	O
xs	O
)	O
,	O
we	O
get	O
(	O
cid:4	O
)	O
log	O
τst	O
(	O
xs	O
,	O
xt	O
)	O
=λ	O
ss	O
+	O
λtt	O
+	O
θst	O
(	O
xs	O
,	O
xt	O
)	O
+θ	O
s	O
(	O
xs	O
)	O
+θ	O
t	O
(	O
xt	O
)	O
λut	O
(	O
xt	O
)	O
λus	O
(	O
xs	O
)	O
+	O
+	O
u∈nbr	O
(	O
s	O
)	O
\t	O
u∈nbr	O
(	O
t	O
)	O
\s	O
(	O
cid:4	O
)	O
τs	O
(	O
xs	O
)	O
∝	O
exp	O
(	O
θs	O
(	O
xs	O
)	O
)	O
(	O
cid:20	O
)	O
×	O
τst	O
(	O
xs	O
,	O
xt	O
)	O
∝	O
exp	O
(	O
θst	O
(	O
xs	O
,	O
xt	O
)	O
+θ	O
s	O
(	O
xs	O
)	O
+θ	O
t	O
(	O
xt	O
)	O
)	O
(	O
cid:20	O
)	O
t∈nbr	O
(	O
s	O
)	O
mts	O
(	O
xs	O
)	O
(	O
cid:20	O
)	O
mus	O
(	O
xs	O
)	O
mut	O
(	O
xt	O
)	O
u∈nbr	O
(	O
s	O
)	O
\t	O
u∈nbr	O
(	O
t	O
)	O
\s	O
to	O
make	O
the	O
connection	O
to	O
message	B
passing	I
,	O
deﬁne	O
mts	O
(	O
xs	O
)	O
=	O
exp	O
(	O
λts	O
(	O
xs	O
)	O
)	O
.	O
with	O
this	O
notation	O
,	O
we	O
can	O
rewrite	O
the	O
above	O
equations	O
(	O
after	O
taking	O
exponents	O
of	O
both	O
sides	O
)	O
as	O
follows	O
:	O
(	O
22.43	O
)	O
(	O
22.44	O
)	O
(	O
22.45	O
)	O
(	O
22.46	O
)	O
(	O
22.47	O
)	O
22.4.	O
extensions	O
of	O
belief	B
propagation	I
*	O
783	O
where	O
the	O
λ	O
terms	O
are	O
absorbed	O
into	O
the	O
constant	O
of	O
proportionality	O
.	O
we	O
see	O
that	O
this	O
is	O
equivalent	O
to	O
the	O
usual	O
expression	O
for	O
the	O
node	O
and	O
edge	O
marginals	O
in	O
lbp	O
.	O
to	O
derive	O
an	O
equation	O
for	O
the	O
messages	O
in	O
terms	O
of	O
other	O
messages	O
(	O
rather	O
than	O
in	O
terms	O
of	O
τst	O
(	O
xs	O
,	O
xt	O
)	O
=τ	O
s	O
(	O
xs	O
)	O
.	O
then	O
one	O
can	O
show	O
λts	O
)	O
,	O
we	O
enforce	O
the	O
marginalization	O
condition	O
that	O
(	O
cid:7	O
)	O
⎡	O
⎣exp	O
{	O
θst	O
(	O
xs	O
,	O
xt	O
)	O
+θ	O
t	O
(	O
xt	O
)	O
}	O
xt	O
(	O
cid:20	O
)	O
⎤	O
⎦	O
mts	O
(	O
xs	O
)	O
∝	O
(	O
cid:4	O
)	O
xt	O
mut	O
(	O
xt	O
)	O
u∈nbr	O
(	O
t	O
)	O
\s	O
(	O
22.48	O
)	O
we	O
see	O
that	O
this	O
is	O
equivalent	O
to	O
the	O
usual	O
expression	O
for	O
the	O
messages	O
in	O
lbp	O
.	O
22.3.6	O
loopy	O
bp	O
vs	O
mean	B
ﬁeld	I
it	O
is	O
interesting	O
to	O
compare	O
the	O
naive	O
mean	O
ﬁeld	O
(	O
mf	O
)	O
and	O
lbp	O
approximations	O
.	O
there	O
are	O
several	O
obvious	O
differences	O
.	O
first	O
,	O
lbp	O
is	O
exact	O
for	O
trees	O
whereas	O
mf	O
is	O
not	O
,	O
suggesting	O
lbp	O
will	O
in	O
general	O
be	O
more	O
accurate	O
(	O
see	O
(	O
wainwright	O
et	O
al	O
.	O
2003	O
)	O
for	O
an	O
analysis	O
)	O
.	O
second	O
,	O
lbp	O
optimizes	O
over	O
node	O
and	O
edge	O
marginals	O
,	O
whereas	O
mf	O
only	O
optimizes	O
over	O
node	O
marginals	O
,	O
again	O
suggesting	O
lbp	O
will	O
be	O
more	O
accurate	O
.	O
third	O
,	O
in	O
the	O
case	O
that	O
the	O
true	O
edge	O
marginals	O
factorize	O
,	O
so	O
μst	O
=	O
μsμt	O
,	O
the	O
free	B
energy	I
approximations	O
will	O
be	O
the	O
same	O
in	O
both	O
cases	O
.	O
what	O
is	O
less	O
obvious	O
,	O
but	O
which	O
nevertheless	O
seems	O
to	O
be	O
true	O
,	O
is	O
that	O
the	O
mf	O
objective	O
has	O
many	O
more	O
local	O
optima	O
than	O
the	O
lbp	O
objective	O
,	O
so	O
optimizing	O
the	O
mf	O
objective	O
seems	O
to	O
be	O
harder	O
.	O
in	O
particular	O
,	O
(	O
weiss	O
2001	O
)	O
,	O
shows	O
empirically	O
that	O
optimizing	O
mf	O
starting	O
from	O
uniform	O
or	O
random	O
initial	O
conditions	O
often	O
leads	O
to	O
poor	O
results	O
,	O
whereas	O
optimizing	O
bp	O
from	O
uniform	O
initial	O
messages	O
often	O
leads	O
to	O
good	O
results	O
.	O
furthermore	O
,	O
initializing	O
mf	O
with	O
the	O
bp	O
marginals	O
also	O
leads	O
to	O
good	O
results	O
(	O
although	O
mf	O
tends	O
to	O
be	O
more	O
overconﬁdent	O
than	O
bp	O
)	O
,	O
indicating	O
that	O
the	O
problem	O
is	O
caused	O
not	O
by	O
the	O
inaccuracy	O
of	O
the	O
mf	O
approximation	O
,	O
but	O
rather	O
by	O
the	O
severe	O
non-convexity	O
of	O
the	O
mf	O
objective	O
,	O
and	O
by	O
the	O
weakness	O
of	O
the	O
standard	O
coordinate	O
descent	O
optimization	B
method	O
used	O
by	O
mf.2	O
however	O
,	O
the	O
advantage	O
of	O
mf	O
is	O
that	O
it	O
gives	O
a	O
lower	O
bound	O
on	O
the	O
partition	B
function	I
,	O
unlike	O
bp	O
,	O
which	O
is	O
useful	O
when	O
using	O
it	O
as	O
a	O
subroutine	O
inside	O
a	O
learning	B
algorithm	O
.	O
also	O
,	O
mf	O
is	O
easier	O
to	O
extend	O
to	O
other	O
distributions	O
besides	O
discrete	B
and	O
gaussian	O
,	O
as	O
we	O
saw	O
in	O
chapter	O
21.	O
intuitively	O
,	O
this	O
is	O
because	O
mf	O
only	O
works	O
with	O
marginal	O
distributions	O
,	O
which	O
have	O
a	O
single	O
type	O
,	O
rather	O
than	O
needing	O
to	O
deﬁne	O
pairwise	O
distributions	O
,	O
which	O
may	O
need	O
to	O
have	O
two	O
different	O
types	O
.	O
22.4	O
extensions	O
of	O
belief	B
propagation	I
*	O
in	O
this	O
section	O
,	O
we	O
discuss	O
various	O
extensions	O
of	O
lbp	O
.	O
22.4.1	O
generalized	B
belief	I
propagation	I
we	O
can	O
improve	O
the	O
accuracy	O
of	O
loopy	O
bp	O
by	O
clustering	B
together	O
nodes	O
that	O
form	O
a	O
tight	O
loop	B
.	O
this	O
is	O
known	O
as	O
the	O
cluster	B
variational	I
method	I
.	O
the	O
result	O
is	O
a	O
hyper-graph	O
,	O
which	O
is	O
a	O
graph	B
2	O
.	O
(	O
honkela	O
et	O
al	O
.	O
2003	O
)	O
discusses	O
the	O
use	O
of	O
the	O
pattern	B
search	I
algorithm	O
to	O
speedup	O
mean	B
ﬁeld	I
inference	O
in	O
the	O
case	O
of	O
continuous	O
random	O
variables	O
.	O
it	O
is	O
possible	O
that	O
similar	B
ideas	O
could	O
be	O
adapted	O
to	O
the	O
discrete	B
case	O
,	O
although	O
there	O
may	O
be	O
no	O
reason	O
to	O
do	O
this	O
,	O
given	O
that	O
lbp	O
already	O
works	O
well	O
in	O
the	O
discrete	B
case	O
.	O
784	O
chapter	O
22.	O
more	O
variational	B
inference	I
1	O
4	O
7	O
2	O
5	O
8	O
3	O
6	O
9	O
1	O
2	O
4	O
5	O
2	O
3	O
5	O
6	O
52	O
4	O
5	O
5	O
5	O
6	O
5	O
8	O
54	O
7	O
8	O
5	O
6	O
8	O
9	O
figure	O
22.9	O
(	O
a	O
)	O
kikuchi	O
clusters	B
superimposed	O
on	O
a	O
3	O
×	O
3	O
lattice	B
graph	O
.	O
source	O
:	O
figure	O
4.5	O
of	O
(	O
wainwright	O
and	O
jordan	O
2008b	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
martin	O
wainwright	O
.	O
(	O
b	O
)	O
corresponding	O
hyper-graph	O
.	O
where	O
there	O
are	O
hyper-edges	O
between	O
sets	O
of	O
vertices	B
instead	O
of	O
between	O
single	O
vertices	O
.	O
note	O
that	O
a	O
junction	B
tree	I
(	O
section	O
20.4.1	O
)	O
is	O
a	O
kind	O
of	O
hyper-graph	O
.	O
we	O
can	O
represent	O
hyper-graph	O
using	O
a	O
poset	O
(	O
partially	O
ordered	O
set	O
)	O
diagram	O
,	O
where	O
each	O
node	O
represents	O
a	O
hyper-edge	O
,	O
and	O
there	O
is	O
an	O
arrow	O
e1	O
→	O
e2	O
if	O
e2	O
⊂	O
e1	O
.	O
see	O
figure	O
22.9	O
for	O
an	O
example	O
.	O
let	O
t	O
be	O
the	O
size	O
of	O
the	O
largest	O
hyper-edge	O
in	O
the	O
hyper-graph	O
.	O
if	O
we	O
allow	O
t	O
to	O
be	O
as	O
large	O
as	O
the	O
treewidth	B
of	O
the	O
graph	B
,	O
then	O
we	O
can	O
represent	O
the	O
hyper-graph	O
as	O
a	O
tree	B
,	O
and	O
the	O
method	O
will	O
be	O
exact	O
,	O
just	O
as	O
lbp	O
is	O
exact	O
on	O
regular	B
trees	O
(	O
with	O
treewidth	B
1	O
)	O
.	O
in	O
this	O
way	O
,	O
we	O
can	O
deﬁne	O
a	O
continuum	O
of	O
approximations	O
,	O
from	O
lbp	O
all	O
the	O
way	O
to	O
exact	O
inference	B
.	O
deﬁne	O
lt	O
(	O
g	O
)	O
to	O
be	O
the	O
set	O
of	O
all	O
pseudo-marginals	O
such	O
that	O
normalization	O
and	O
marginaliza-	O
tion	O
constraints	O
hold	O
on	O
a	O
hyper-graph	O
whose	O
largest	O
hyper-edge	O
is	O
of	O
size	O
t	O
+	O
1.	O
for	O
example	O
,	O
in	O
figure	O
22.9	O
,	O
we	O
impose	O
constraints	O
of	O
the	O
form	O
τ1245	O
(	O
x1	O
,	O
x2	O
,	O
x4	O
,	O
x5	O
)	O
=	O
τ45	O
(	O
x4	O
,	O
x5	O
)	O
,	O
τ56	O
(	O
x5	O
,	O
x6	O
)	O
=	O
τ5	O
(	O
x5	O
)	O
,	O
.	O
.	O
.	O
(	O
22.49	O
)	O
(	O
cid:4	O
)	O
x1	O
,	O
x2	O
(	O
cid:4	O
)	O
x6	O
furthermore	O
,	O
we	O
approximate	O
the	O
entropy	B
as	O
follows	O
:	O
hkikuchi	O
(	O
τ	O
)	O
(	O
cid:2	O
)	O
c	O
(	O
g	O
)	O
hg	O
(	O
τg	O
)	O
(	O
22.50	O
)	O
where	O
hg	O
(	O
τg	O
)	O
is	O
the	O
entropy	B
of	O
the	O
joint	O
(	O
pseudo	O
)	O
distribution	O
on	O
the	O
vertices	B
in	O
set	O
g	O
,	O
and	O
c	O
(	O
g	O
)	O
is	O
called	O
the	O
overcounting	B
number	I
of	O
set	O
g.	O
these	O
are	O
related	O
to	O
mobious	O
numbers	O
in	O
set	O
theory	O
.	O
rather	O
than	O
giving	O
a	O
precise	O
deﬁnition	O
,	O
we	O
just	O
give	O
a	O
simple	O
example	O
.	O
for	O
the	O
graph	B
in	O
figure	O
22.9	O
,	O
we	O
have	O
(	O
cid:4	O
)	O
g∈e	O
(	O
cid:15	O
)	O
putting	O
these	O
two	O
approximations	O
together	O
,	O
we	O
can	O
deﬁne	O
the	O
kikuchi	O
free	O
energy3	O
as	O
follows	O
:	O
hkikuchi	O
(	O
τ	O
)	O
=	O
[	O
h1245	O
+	O
h2356	O
+	O
h4578	O
+	O
h5689	O
]	O
−	O
[	O
h25	O
+	O
h45	O
+	O
h56	O
+	O
h58	O
]	O
+h	O
5	O
(	O
cid:16	O
)	O
fkikuchi	O
(	O
τ	O
)	O
(	O
cid:2	O
)	O
−	O
θt	O
τ	O
+	O
hkikuchi	O
(	O
τ	O
)	O
3.	O
ryoichi	O
kikuchi	O
is	O
a	O
japanese	O
physicist	O
.	O
(	O
22.51	O
)	O
(	O
22.52	O
)	O
22.4.	O
extensions	O
of	O
belief	B
propagation	I
*	O
785	O
our	O
variational	O
problem	O
becomes	O
fkikuchi	O
(	O
τ	O
)	O
=	O
max	O
τ∈lt	O
(	O
g	O
)	O
min	O
θt	O
τ	O
+	O
hkikuchi	O
(	O
τ	O
)	O
τ∈lt	O
(	O
g	O
)	O
just	O
as	O
with	O
the	O
bethe	O
free	B
energy	I
,	O
this	O
is	O
not	O
a	O
concave	B
objective	O
.	O
there	O
are	O
several	O
possible	O
algorithms	O
for	O
ﬁnding	O
a	O
local	O
optimum	O
of	O
this	O
objective	O
,	O
including	O
a	O
message	B
passing	I
algorithm	O
known	O
as	O
generalized	B
belief	I
propagation	I
.	O
however	O
,	O
the	O
details	O
are	O
beyond	O
the	O
scope	B
of	O
this	O
chapter	O
.	O
see	O
e.g.	O
,	O
(	O
wainwright	O
and	O
jordan	O
2008b	O
,	O
sec	O
4.2	O
)	O
or	O
(	O
koller	O
and	O
friedman	O
2009	O
,	O
sec	O
11.3.2	O
)	O
for	O
more	O
information	B
.	O
suffice	O
it	O
to	O
say	O
that	O
the	O
method	O
gives	O
more	O
accurate	O
results	O
than	O
lbp	O
,	O
but	O
at	O
increased	O
computational	O
cost	O
(	O
because	O
of	O
the	O
need	O
to	O
handle	O
clusters	B
of	O
nodes	B
)	O
.	O
this	O
cost	O
,	O
plus	O
the	O
complexity	O
of	O
the	O
approach	O
,	O
have	O
precluded	O
it	O
from	O
widespread	O
use	O
.	O
(	O
22.53	O
)	O
22.4.2	O
convex	B
belief	I
propagation	I
the	O
mean	B
ﬁeld	I
energy	I
functional	I
is	O
concave	B
,	O
but	O
it	O
is	O
maximized	O
over	O
a	O
non-convex	O
inner	B
approximation	I
to	O
the	O
marginal	B
polytope	I
.	O
the	O
bethe	O
and	O
kikuchi	O
energy	O
functionals	O
are	O
not	O
concave	O
,	O
but	O
they	O
are	O
maximized	O
over	O
a	O
convex	B
outer	O
approximation	O
to	O
the	O
marginal	B
polytope	I
.	O
consequently	O
,	O
for	O
both	O
mf	O
and	O
lbp	O
,	O
the	O
optimization	B
problem	O
has	O
multiple	O
optima	O
,	O
so	O
the	O
methods	O
are	O
sensitive	O
to	O
the	O
initial	O
conditions	O
.	O
given	O
that	O
the	O
exact	O
formulation	O
(	O
equation	O
22.24	O
)	O
a	O
concave	B
objective	O
maximized	O
over	O
a	O
convex	B
set	O
,	O
it	O
is	O
natural	O
to	O
try	O
to	O
come	O
up	O
with	O
an	O
appproximation	O
which	O
involves	O
a	O
concave	B
objective	O
being	O
maximized	O
over	O
a	O
convex	B
set	O
.	O
we	O
now	O
describe	O
one	O
method	O
,	O
known	O
as	O
convex	B
belief	I
propagation	I
.	O
this	O
involves	O
working	O
with	O
a	O
set	O
of	O
tractable	O
submodels	O
,	O
f	O
,	O
such	O
as	O
trees	O
or	O
planar	O
graphs	O
.	O
for	O
each	O
model	O
f	O
⊂	O
g	O
,	O
the	O
entropy	B
is	O
higher	O
,	O
h	O
(	O
μ	O
(	O
f	O
)	O
)	O
≥	O
h	O
(	O
μ	O
(	O
g	O
)	O
)	O
,	O
since	O
f	O
has	O
fewer	O
constraints	O
.	O
consequently	O
,	O
any	O
convex	B
combination	I
of	O
such	O
subgraphs	O
will	O
have	O
higher	O
entropy	B
,	O
too	O
:	O
(	O
cid:4	O
)	O
ρ	O
(	O
f	O
)	O
h	O
(	O
μ	O
(	O
f	O
)	O
)	O
(	O
cid:2	O
)	O
h	O
(	O
μ	O
,	O
ρ	O
)	O
h	O
(	O
μ	O
(	O
g	O
)	O
)	O
≤	O
f∈f	O
where	O
ρ	O
(	O
f	O
)	O
≥	O
0	O
and	O
deﬁne	O
the	O
convex	B
free	O
energy	O
as	O
(	O
cid:7	O
)	O
fconvex	O
(	O
μ	O
,	O
ρ	O
)	O
(	O
cid:2	O
)	O
−	O
(	O
cid:13	O
)	O
μt	O
θ	O
+	O
h	O
(	O
μ	O
,	O
ρ	O
)	O
(	O
cid:14	O
)	O
f	O
ρ	O
(	O
f	O
)	O
=	O
1.	O
furthermore	O
,	O
h	O
(	O
μ	O
,	O
ρ	O
)	O
is	O
a	O
concave	B
function	O
of	O
μ.	O
we	O
now	O
(	O
22.54	O
)	O
(	O
22.55	O
)	O
we	O
deﬁne	O
the	O
concave	B
energy	O
functional	O
as	O
the	O
negative	O
of	O
the	O
convex	B
free	O
energy	O
.	O
we	O
discuss	O
how	O
to	O
optimize	O
ρ	O
below	O
.	O
having	O
deﬁned	O
an	O
upper	O
bound	O
on	O
the	O
entropy	B
,	O
we	O
now	O
consider	O
a	O
convex	B
outerbound	O
on	O
the	O
marginal	B
polytope	I
of	O
mean	B
parameters	O
.	O
we	O
want	O
to	O
ensure	O
we	O
can	O
evaluate	O
the	O
entropy	B
of	O
any	O
vector	O
τ	O
in	O
this	O
set	O
,	O
so	O
we	O
restrict	O
it	O
so	O
that	O
the	O
projection	B
of	O
τ	O
onto	O
the	O
subgraph	B
g	O
lives	O
in	O
the	O
projection	B
of	O
m	O
onto	O
f	O
:	O
l	O
(	O
g	O
;	O
f	O
)	O
(	O
cid:2	O
)	O
{	O
τ	O
∈	O
r	O
d	O
:	O
τ	O
(	O
f	O
)	O
∈	O
m	O
(	O
f	O
)	O
∀f	O
∈	O
f	O
}	O
(	O
22.56	O
)	O
this	O
is	O
a	O
convex	B
set	O
since	O
each	O
m	O
(	O
f	O
)	O
is	O
a	O
projection	B
of	O
a	O
convex	B
set	O
.	O
hence	O
we	O
deﬁne	O
our	O
problem	O
as	O
min	O
τ∈l	O
(	O
g	O
;	O
f	O
)	O
fconvex	O
(	O
τ	O
,	O
ρ	O
)	O
=	O
max	O
τ∈l	O
(	O
g	O
;	O
f	O
)	O
τ	O
t	O
θ	O
+	O
h	O
(	O
τ	O
,	O
ρ	O
)	O
(	O
22.57	O
)	O
this	O
is	O
a	O
concave	B
objective	O
being	O
maximized	O
over	O
a	O
convex	B
set	O
,	O
and	O
hence	O
has	O
a	O
unique	O
maxi-	O
mum	O
.	O
we	O
give	O
a	O
speciﬁc	O
example	O
below	O
.	O
786	O
chapter	O
22.	O
more	O
variational	B
inference	I
f	O
e	O
b	O
f	O
e	O
b	O
f	O
e	O
b	O
f	O
e	O
b	O
figure	O
22.10	O
(	O
a	O
)	O
a	O
graph	B
.	O
(	O
b-d	O
)	O
some	O
of	O
its	O
spanning	O
trees	O
.	O
source	O
:	O
figure	O
7.1	O
of	O
(	O
wainwright	O
and	O
jordan	O
2008b	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
martin	O
wainwright	O
.	O
22.4.2.1	O
tree-reweighted	O
belief	B
propagation	I
consider	O
the	O
speciﬁc	O
case	O
where	O
f	O
is	O
all	O
spanning	O
trees	O
of	O
a	O
graph	B
.	O
for	O
any	O
given	O
tree	B
,	O
the	O
entropy	B
is	O
given	O
by	O
equation	O
22.35.	O
to	O
compute	O
the	O
upper	O
bound	O
,	O
obtained	O
by	O
averaging	O
over	O
f	O
ρ	O
(	O
f	O
)	O
h	O
(	O
μ	O
(	O
f	O
)	O
s	O
)	O
for	O
single	O
nodes	O
will	O
just	O
be	O
hs	O
,	O
since	O
node	O
s	O
all	O
trees	O
,	O
note	O
that	O
the	O
terms	O
f	O
ρ	O
(	O
f	O
)	O
=	O
1.	O
but	O
the	O
mutual	B
information	I
term	O
ist	O
receives	O
weight	O
appears	O
in	O
every	O
tree	B
,	O
and	O
ρst	O
=	O
eρ	O
[	O
i	O
(	O
(	O
s	O
,	O
t	O
)	O
∈	O
e	O
(	O
t	O
)	O
)	O
]	O
,	O
known	O
as	O
the	O
edge	B
appearance	I
probability	I
.	O
hence	O
we	O
have	O
the	O
following	O
upper	O
bound	O
on	O
the	O
entropy	B
:	O
h	O
(	O
μ	O
)	O
≤	O
hs	O
(	O
μs	O
)	O
−	O
ρstist	O
(	O
μst	O
)	O
(	O
22.58	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:4	O
)	O
(	O
s	O
,	O
t	O
)	O
∈e	O
(	O
cid:4	O
)	O
s∈v	O
the	O
edge	O
appearance	O
probabilities	O
live	O
in	O
a	O
space	O
called	O
the	O
spanning	B
tree	I
polytope	I
.	O
this	O
is	O
because	O
they	O
are	O
constrained	O
to	O
arise	O
from	O
a	O
distribution	O
over	O
trees	O
.	O
figure	O
22.10	O
gives	O
an	O
example	O
of	O
a	O
graph	B
and	O
three	O
of	O
its	O
spanning	O
trees	O
.	O
suppose	O
each	O
tree	B
has	O
equal	O
weight	O
under	O
ρ.	O
the	O
edge	O
f	O
occurs	O
in	O
1	O
of	O
the	O
3	O
trees	O
,	O
so	O
ρf	O
=	O
1/3	O
.	O
the	O
edge	O
e	O
occurs	O
in	O
2	O
of	O
the	O
3	O
trees	O
,	O
so	O
ρe	O
=	O
2/3	O
.	O
the	O
edge	O
b	O
appears	O
in	O
all	O
of	O
the	O
trees	O
,	O
so	O
ρb	O
=	O
1.	O
and	O
so	O
on	O
.	O
ideally	O
we	O
can	O
ﬁnd	O
a	O
distribution	O
ρ	O
,	O
or	O
equivalently	O
edge	O
probabilities	O
in	O
the	O
spanning	B
tree	I
polytope	I
,	O
that	O
make	O
the	O
above	O
bound	O
as	O
tight	O
as	O
possible	O
.	O
an	O
algorithm	O
to	O
do	O
this	O
is	O
described	O
in	O
(	O
wainwright	O
et	O
al	O
.	O
(	O
a	O
simpler	O
approach	O
is	O
to	O
generate	O
spanning	O
trees	O
of	O
g	O
at	O
random	O
until	O
all	O
edges	O
are	O
2005	O
)	O
.	O
covered	O
,	O
or	O
use	O
all	O
single	O
edges	B
with	O
weight	O
ρe	O
=	O
1/e	O
.	O
)	O
what	O
about	O
the	O
set	O
we	O
are	O
optimizing	O
over	O
?	O
we	O
require	O
μ	O
(	O
t	O
)	O
∈	O
m	O
(	O
t	O
)	O
for	O
each	O
tree	B
t	O
,	O
which	O
means	O
enforcing	O
normalization	O
and	O
local	B
consistency	I
.	O
since	O
we	O
have	O
to	O
do	O
this	O
for	O
every	O
tree	B
,	O
we	O
are	O
enforcing	O
normalization	O
and	O
local	B
consistency	I
on	O
every	O
edge	O
.	O
hence	O
l	O
(	O
g	O
;	O
f	O
)	O
=	O
l	O
(	O
g	O
)	O
.	O
so	O
our	O
ﬁnal	O
optimization	B
problem	O
is	O
as	O
follows	O
:	O
⎧⎨	O
⎩τ	O
t	O
θ	O
+	O
(	O
cid:4	O
)	O
s∈v	O
max	O
τ∈l	O
(	O
g	O
)	O
(	O
cid:4	O
)	O
hs	O
(	O
τs	O
)	O
−	O
ρstist	O
(	O
τst	O
)	O
(	O
s	O
,	O
t	O
)	O
∈e	O
(	O
g	O
)	O
⎫⎬	O
⎭	O
which	O
is	O
the	O
same	O
as	O
the	O
lbp	O
objective	O
except	O
for	O
the	O
crucial	O
ρst	O
weights	O
.	O
so	O
long	O
as	O
ρst	O
>	O
0	O
for	O
all	O
edges	O
(	O
s	O
,	O
t	O
)	O
,	O
this	O
problem	O
is	O
strictly	O
concave	O
with	O
a	O
unique	O
maximum	O
.	O
how	O
can	O
we	O
ﬁnd	O
this	O
global	O
optimum	O
?	O
as	O
for	O
lbp	O
,	O
there	O
are	O
several	O
algorithms	O
,	O
but	O
perhaps	O
the	O
simplest	O
is	O
a	O
modiﬁcation	O
of	O
belief	B
propagation	I
known	O
as	O
tree	B
reweighted	I
belief	I
propagation	I
,	O
(	O
22.59	O
)	O
22.5.	O
expectation	B
propagation	I
787	O
also	O
called	O
trw	O
or	O
trbp	O
for	O
short	O
.	O
the	O
message	O
from	O
t	O
to	O
s	O
is	O
now	O
a	O
function	O
of	O
all	O
messages	O
sent	O
from	O
other	O
neighbors	B
v	O
to	O
t	O
,	O
as	O
before	O
,	O
but	O
now	O
it	O
is	O
also	O
a	O
function	O
of	O
the	O
message	O
sent	O
from	O
s	O
to	O
t.	O
speciﬁcally	O
(	O
cid:9	O
)	O
(	O
cid:26	O
)	O
(	O
cid:8	O
)	O
(	O
cid:4	O
)	O
exp	O
xt	O
mts	O
(	O
xs	O
)	O
∝	O
1	O
ρst	O
θst	O
(	O
xs	O
,	O
xt	O
)	O
+θ	O
t	O
(	O
xt	O
)	O
v∈nbr	O
(	O
t	O
)	O
\s	O
[	O
mvt	O
(	O
xt	O
)	O
]	O
ρvt	O
[	O
mst	O
(	O
xt	O
)	O
]	O
1−ρts	O
(	O
cid:20	O
)	O
at	O
convergence	O
,	O
the	O
node	O
and	O
edge	O
pseudo	O
marginals	O
are	O
given	O
by	O
τs	O
(	O
xs	O
)	O
∝	O
exp	O
(	O
θs	O
(	O
xs	O
)	O
)	O
(	O
cid:26	O
)	O
τst	O
(	O
xs	O
,	O
xt	O
)	O
∝	O
ϕst	O
(	O
xs	O
,	O
xt	O
)	O
[	O
mvs	O
(	O
xs	O
)	O
]	O
ρvs	O
v∈nbr	O
(	O
s	O
)	O
v∈nbr	O
(	O
s	O
)	O
\t	O
[	O
mvs	O
(	O
xs	O
)	O
]	O
ρvs	O
[	O
mts	O
(	O
xs	O
)	O
]	O
1−ρst	O
(	O
cid:26	O
)	O
(	O
cid:9	O
)	O
v∈nbr	O
(	O
t	O
)	O
\s	O
[	O
mvt	O
(	O
xt	O
)	O
]	O
ρvt	O
[	O
mst	O
(	O
xt	O
)	O
]	O
1−ρts	O
ϕst	O
(	O
xs	O
,	O
xt	O
)	O
(	O
cid:2	O
)	O
exp	O
θst	O
(	O
xs	O
,	O
xt	O
)	O
+θ	O
s	O
(	O
xs	O
)	O
+θ	O
t	O
(	O
xt	O
)	O
(	O
cid:8	O
)	O
1	O
ρst	O
(	O
22.60	O
)	O
(	O
22.61	O
)	O
(	O
22.62	O
)	O
(	O
22.63	O
)	O
this	O
algorithm	O
can	O
be	O
derived	O
using	O
a	O
method	O
similar	B
to	O
that	O
described	O
in	O
section	O
22.3.5.4.	O
if	O
ρst	O
=	O
1	O
for	O
all	O
edges	O
(	O
s	O
,	O
t	O
)	O
∈	O
e	O
,	O
the	O
algorithm	O
reduces	O
to	O
the	O
standard	O
lbp	O
algorithm	O
.	O
however	O
,	O
the	O
condition	O
ρst	O
=	O
1	O
implies	O
every	O
edge	O
is	O
present	O
in	O
every	O
spanning	O
tree	O
with	O
probability	O
1	O
,	O
which	O
is	O
only	O
possible	O
if	O
the	O
original	O
graph	B
is	O
a	O
tree	B
.	O
hence	O
the	O
method	O
is	O
only	O
equivalent	O
to	O
standard	O
lbp	O
on	O
trees	O
,	O
when	O
the	O
method	O
is	O
of	O
course	O
exact	O
.	O
in	O
general	O
,	O
this	O
message	B
passing	I
scheme	O
is	O
not	O
guaranteed	O
to	O
converge	B
to	O
the	O
unique	O
global	O
optimum	O
.	O
one	O
can	O
devise	O
double-loop	O
methods	O
that	O
are	O
guaranteed	O
to	O
converge	B
(	O
hazan	O
and	O
shashua	O
2008	O
)	O
,	O
but	O
in	O
practice	O
,	O
using	O
damped	B
updates	I
as	O
in	O
equation	O
22.7	O
is	O
often	O
sufficient	O
to	O
ensure	O
convergence	O
.	O
it	O
is	O
also	O
possible	O
to	O
produce	O
a	O
convex	B
version	O
of	O
the	O
kikuchi	O
free	B
energy	I
,	O
which	O
one	O
can	O
optimize	O
with	O
a	O
modiﬁed	O
version	O
of	O
generalized	B
belief	I
propagation	I
.	O
see	O
(	O
wainwright	O
and	O
jordan	O
2008b	O
,	O
sec	O
7.2.2	O
)	O
for	O
details	O
.	O
from	O
equation	O
22.59	O
,	O
and	O
using	O
the	O
fact	O
that	O
the	O
trbp	O
entropy	B
approximation	O
is	O
an	O
upper	O
bound	O
on	O
the	O
true	O
entropy	O
,	O
wee	O
see	O
that	O
the	O
trbp	O
objective	O
is	O
an	O
upper	O
bound	O
on	O
log	O
z.	O
using	O
the	O
fact	O
that	O
ist	O
=	O
hs	O
+	O
ht	O
−	O
hst	O
,	O
we	O
can	O
rewrite	O
the	O
upper	O
bound	O
as	O
follows	O
:	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
cshs	O
(	O
τs	O
)	O
≤	O
log	O
z	O
(	O
θ	O
)	O
(	O
22.64	O
)	O
log	O
ˆz	O
(	O
θ	O
)	O
(	O
cid:2	O
)	O
τ	O
t	O
θ	O
+	O
where	O
cs	O
(	O
cid:2	O
)	O
1	O
−	O
(	O
cid:7	O
)	O
ρsthst	O
(	O
τst	O
)	O
+	O
st	O
t	O
ρst	O
.	O
s	O
22.5	O
expectation	B
propagation	I
expectation	O
propagation	O
(	O
ep	O
)	O
(	O
minka	O
2001c	O
)	O
is	O
a	O
form	O
of	O
belief	B
propagation	I
where	O
the	O
mes-	O
sages	O
are	O
approximated	O
.	O
it	O
is	O
a	O
generalization	B
of	O
the	O
assumed	B
density	I
ﬁltering	I
(	O
adf	O
)	O
algorithm	O
,	O
discussed	O
in	O
section	O
18.5.3.	O
in	O
that	O
method	O
,	O
we	O
approximated	O
the	O
posterior	O
at	O
each	O
step	O
using	O
an	O
assumed	O
functional	O
form	O
,	O
such	O
as	O
a	O
gaussian	O
.	O
this	O
posterior	O
can	O
be	O
computed	O
using	O
mo-	O
ment	O
matching	O
,	O
which	O
locally	O
optimizes	O
kl	O
(	O
p||q	O
)	O
for	O
a	O
single	O
term	O
.	O
from	O
this	O
,	O
we	O
derived	O
the	O
message	O
to	O
send	O
to	O
the	O
next	O
time	O
step	O
.	O
788	O
chapter	O
22.	O
more	O
variational	B
inference	I
adf	O
works	O
well	O
for	O
sequential	B
bayesian	O
updating	O
,	O
but	O
the	O
answer	O
it	O
gives	O
depends	O
on	O
the	O
order	O
in	O
which	O
the	O
data	O
is	O
seen	O
.	O
ep	O
essentially	O
corrects	O
this	O
ﬂaw	O
by	O
making	O
multiple	O
passes	O
over	O
the	O
data	O
(	O
thus	O
ep	O
is	O
an	O
offline	B
or	O
batch	B
inference	O
algorithm	O
)	O
.	O
22.5.1	O
ep	O
as	O
a	O
variational	B
inference	I
problem	O
we	O
now	O
explain	O
how	O
to	O
view	O
ep	O
in	O
terms	O
of	O
variational	B
inference	I
.	O
we	O
follow	O
the	O
presentation	O
of	O
(	O
wainwright	O
and	O
jordan	O
2008b	O
,	O
sec	O
4.3	O
)	O
,	O
which	O
should	O
be	O
consulted	O
for	O
further	O
details	O
.	O
suppose	O
the	O
joint	B
distribution	I
can	O
be	O
written	O
in	O
exponential	B
family	I
form	O
as	O
follows	O
:	O
p	O
(	O
x|θ	O
,	O
˜θ	O
)	O
∝	O
f0	O
(	O
x	O
)	O
exp	O
(	O
θt	O
φ	O
(	O
x	O
)	O
)	O
di	O
(	O
cid:20	O
)	O
t	O
i	O
φi	O
(	O
x	O
)	O
)	O
exp	O
(	O
˜θ	O
(	O
22.65	O
)	O
i=1	O
where	O
we	O
have	O
partitioned	O
the	O
parameters	O
and	O
the	O
sufficient	B
statistics	I
into	O
a	O
tractable	O
term	O
θ	O
of	O
size	O
dt	O
and	O
di	O
intractable	O
terms	O
˜θi	O
,	O
each	O
of	O
size	O
b.	O
for	O
example	O
,	O
consider	O
the	O
problem	O
of	O
inferring	O
an	O
unknown	B
vector	O
x	O
,	O
when	O
the	O
observation	B
model	I
is	O
a	O
mixture	O
of	O
two	O
gaussians	O
,	O
one	O
centered	O
at	O
x	O
and	O
one	O
centered	O
at	O
0	O
.	O
(	O
this	O
can	O
be	O
used	O
to	O
represent	O
outliers	B
,	O
for	O
example	O
.	O
)	O
minka	O
(	O
who	O
invented	O
ep	O
)	O
calls	O
this	O
the	O
clutter	B
problem	I
.	O
more	O
formally	O
,	O
we	O
assume	O
an	O
observation	B
model	I
of	O
the	O
form	O
p	O
(	O
y|x	O
)	O
=	O
(	O
1	O
−	O
w	O
)	O
n	O
(	O
y|x	O
,	O
i	O
)	O
+w	O
n	O
(	O
y|0	O
,	O
ai	O
)	O
where	O
0	O
<	O
w	O
<	O
1	O
is	O
the	O
known	O
mixing	O
weight	O
(	O
fraction	O
of	O
outliers	O
)	O
,	O
and	O
a	O
>	O
0	O
is	O
the	O
variance	B
of	O
the	O
background	O
distribution	O
.	O
assuming	O
a	O
ﬁxed	O
prior	O
of	O
the	O
form	O
p	O
(	O
x	O
)	O
=n	O
(	O
x|0	O
,	O
σ	O
)	O
,	O
we	O
can	O
write	O
our	O
model	O
in	O
the	O
required	O
form	O
as	O
follows	O
:	O
p	O
(	O
x|y1	O
:	O
n	O
)	O
∝	O
n	O
(	O
x|0	O
,	O
σ	O
)	O
n	O
(	O
cid:20	O
)	O
i=1	O
p	O
(	O
yi|x	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
=	O
exp	O
xt	O
σ−1x	O
exp	O
−	O
1	O
2	O
(	O
cid:11	O
)	O
log	O
p	O
(	O
yi|x	O
)	O
(	O
cid:10	O
)	O
n	O
(	O
cid:4	O
)	O
i=1	O
(	O
22.66	O
)	O
(	O
22.67	O
)	O
(	O
cid:22	O
)	O
−	O
1	O
(	O
22.68	O
)	O
(	O
cid:23	O
)	O
2	O
xt	O
σ−1x	O
,	O
this	O
matches	O
our	O
canonical	B
form	I
where	O
f0	O
(	O
x	O
)	O
exp	O
(	O
θt	O
φ	O
(	O
x	O
)	O
)	O
corresponds	O
to	O
exp	O
using	O
φ	O
(	O
x	O
)	O
=	O
(	O
x	O
,	O
xxt	O
)	O
,	O
and	O
we	O
set	O
φi	O
(	O
x	O
)	O
=	O
log	O
p	O
(	O
yi|x	O
)	O
,	O
˜θi	O
=	O
1	O
,	O
and	O
di	O
=	O
n	O
.	O
the	O
exact	O
inference	B
problem	O
corresponds	O
to	O
max	O
τ	O
t	O
θ	O
+	O
˜τ	O
t	O
˜θ	O
+	O
h	O
(	O
(	O
τ	O
,	O
˜τ	O
)	O
)	O
(	O
τ	O
,	O
˜τ	O
)	O
∈m	O
(	O
φ	O
,	O
φ	O
)	O
(	O
22.69	O
)	O
where	O
m	O
(	O
φ	O
,	O
φ	O
)	O
is	O
the	O
set	O
of	O
mean	B
parameters	O
realizable	O
by	O
any	O
probability	O
distribution	O
as	O
seen	O
through	O
the	O
eyes	O
of	O
the	O
sufficient	B
statistics	I
:	O
dt	O
×	O
r	O
m	O
(	O
φ	O
,	O
φ	O
)	O
=	O
{	O
(	O
μ	O
,	O
˜μ	O
)	O
∈	O
r	O
as	O
it	O
stands	O
,	O
it	O
is	O
intractable	O
to	O
perform	O
inference	B
in	O
this	O
distribution	O
.	O
for	O
example	O
,	O
in	O
our	O
clutter	O
example	O
,	O
the	O
posterior	O
contains	O
2n	O
modes	O
.	O
but	O
suppose	O
we	O
incorporate	O
just	O
one	O
of	O
the	O
intractable	O
terms	O
,	O
say	O
the	O
i	O
’	O
th	O
one	O
;	O
we	O
will	O
call	O
this	O
the	O
φi-augmented	O
distribution	O
:	O
di	O
b	O
:	O
(	O
μ	O
,	O
˜μ	O
)	O
=	O
e	O
[	O
(	O
φ	O
(	O
x	O
)	O
,	O
φ1	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
φdi	O
(	O
x	O
)	O
)	O
]	O
}	O
(	O
22.70	O
)	O
p	O
(	O
x|θ	O
,	O
˜θi	O
)	O
∝	O
f0	O
(	O
x	O
)	O
exp	O
(	O
θt	O
φ	O
(	O
x	O
)	O
)	O
exp	O
(	O
˜θ	O
t	O
i	O
φi	O
(	O
x	O
)	O
)	O
(	O
22.71	O
)	O
22.5.	O
expectation	B
propagation	I
789	O
(	O
cid:8	O
)	O
in	O
our	O
clutter	O
example	O
,	O
this	O
becomes	O
p	O
(	O
x|θ	O
,	O
˜θi	O
)	O
=	O
exp	O
xt	O
σ−1x	O
−	O
1	O
2	O
(	O
cid:9	O
)	O
[	O
wn	O
(	O
yi|0	O
,	O
ai	O
)	O
+	O
(	O
1−	O
w	O
)	O
n	O
(	O
yi|x	O
,	O
i	O
)	O
]	O
(	O
22.72	O
)	O
this	O
is	O
tractable	O
to	O
compute	O
,	O
since	O
it	O
is	O
just	O
a	O
mixture	O
of	O
2	O
gaussians	O
.	O
fashion	O
.	O
first	O
,	O
we	O
approximate	O
the	O
convex	B
set	O
m	O
(	O
φ	O
,	O
φ	O
)	O
with	O
another	O
,	O
larger	O
convex	B
set	O
:	O
the	O
key	O
idea	O
behind	O
ep	O
is	O
to	O
work	O
with	O
these	O
the	O
φi-augmented	O
distributions	O
in	O
an	O
iterative	O
l	O
(	O
φ	O
,	O
φ	O
)	O
(	O
cid:2	O
)	O
{	O
(	O
τ	O
,	O
˜τ	O
)	O
:	O
τ	O
∈	O
m	O
(	O
φ	O
)	O
,	O
(	O
τ	O
,	O
˜τ	O
i	O
)	O
∈	O
m	O
(	O
φ	O
,	O
φi	O
)	O
}	O
(	O
22.73	O
)	O
dt	O
:	O
μ	O
=	O
e	O
[	O
φ	O
(	O
x	O
)	O
]	O
}	O
and	O
m	O
(	O
φ	O
,	O
φi	O
)	O
=	O
{	O
(	O
μ	O
,	O
˜μi	O
)	O
∈	O
r	O
where	O
m	O
(	O
φ	O
)	O
=	O
{	O
μ	O
∈	O
r	O
b	O
:	O
(	O
μ	O
,	O
˜μi	O
)	O
=e	O
[	O
(	O
φ	O
(	O
x	O
)	O
,	O
φi	O
(	O
x	O
)	O
)	O
]	O
.	O
next	O
we	O
approximate	O
the	O
entropy	B
by	O
the	O
following	O
term-by-term	O
di	O
(	O
cid:4	O
)	O
approximation	O
:	O
dt	O
×	O
r	O
hep	O
(	O
τ	O
,	O
˜τ	O
)	O
(	O
cid:2	O
)	O
h	O
(	O
τ	O
)	O
+	O
[	O
h	O
(	O
τ	O
,	O
˜τ	O
i	O
)	O
−	O
h	O
(	O
τ	O
)	O
]	O
then	O
the	O
ep	O
problem	O
becomes	O
i=1	O
max	O
(	O
τ	O
,	O
˜τ	O
)	O
∈l	O
(	O
φ	O
,	O
φ	O
)	O
τ	O
t	O
θ	O
+	O
˜τ	O
t	O
˜θ	O
+	O
hep	O
(	O
τ	O
,	O
˜τ	O
)	O
(	O
22.74	O
)	O
(	O
22.75	O
)	O
22.5.2	O
optimizing	O
the	O
ep	O
objective	O
using	O
moment	B
matching	I
we	O
now	O
discuss	O
how	O
to	O
maximize	O
the	O
ep	O
objective	O
in	O
equation	O
22.75.	O
let	O
us	O
duplicate	O
τ	O
di	O
times	O
to	O
yield	O
ηi	O
=	O
τ	O
.	O
the	O
augmented	O
set	O
of	O
parameters	O
we	O
need	O
to	O
optimize	O
is	O
now	O
(	O
τ	O
,	O
(	O
ηi	O
,	O
˜τ	O
i	O
)	O
di	O
i=1	O
)	O
∈	O
r	O
dt	O
×	O
(	O
r	O
dt	O
×	O
r	O
b	O
)	O
di	O
(	O
22.76	O
)	O
subject	O
to	O
the	O
constraints	O
that	O
ηi	O
=	O
τ	O
and	O
(	O
ηi	O
,	O
˜τ	O
i	O
)	O
∈	O
m	O
(	O
φ	O
;	O
φi	O
)	O
.	O
let	O
us	O
associate	O
a	O
vector	O
of	O
lagrange	O
multipliers	O
λi	O
∈	O
r	O
dt	O
with	O
the	O
ﬁrst	O
set	O
of	O
constraints	O
.	O
then	O
the	O
partial	O
lagrangian	O
becomes	O
(	O
cid:16	O
)	O
l	O
(	O
τ	O
;	O
λ	O
)	O
=τ	O
t	O
θ	O
+	O
h	O
(	O
τ	O
)	O
+	O
i	O
˜θi	O
+	O
h	O
(	O
(	O
ηi	O
,	O
˜τ	O
i	O
)	O
)	O
−	O
h	O
(	O
ηi	O
)	O
+λ	O
t	O
˜τ	O
t	O
i	O
(	O
τ	O
−	O
ηi	O
)	O
(	O
22.77	O
)	O
by	O
solving	O
∇τ	O
l	O
(	O
τ	O
;	O
λ	O
)	O
=	O
0	O
,	O
we	O
can	O
show	O
that	O
the	O
corresponding	O
distribution	O
in	O
m	O
(	O
φ	O
)	O
has	O
the	O
form	O
q	O
(	O
x|θ	O
,	O
λ	O
)	O
∝	O
f0	O
(	O
x	O
)	O
exp	O
{	O
(	O
θ	O
+	O
λi	O
)	O
t	O
φ	O
(	O
x	O
)	O
}	O
(	O
22.78	O
)	O
di	O
(	O
cid:4	O
)	O
(	O
cid:15	O
)	O
i=1	O
di	O
(	O
cid:4	O
)	O
i=1	O
the	O
λt	O
i	O
φ	O
(	O
x	O
)	O
terms	O
represents	O
an	O
approximation	O
to	O
the	O
i	O
’	O
th	O
intractable	O
term	O
using	O
the	O
sufficient	B
statistics	I
from	O
the	O
base	B
distribution	I
,	O
as	O
we	O
will	O
see	O
below	O
.	O
similarly	O
,	O
by	O
solving	O
∇	O
(	O
ηi	O
,	O
˜τ	O
i	O
)	O
l	O
(	O
τ	O
;	O
λ	O
)	O
=	O
0	O
,	O
we	O
ﬁnd	O
that	O
the	O
corresponding	O
distribution	O
in	O
m	O
(	O
φ	O
,	O
φi	O
)	O
has	O
the	O
form	O
qi	O
(	O
x|θ	O
,	O
˜θi	O
,	O
λ	O
)	O
∝	O
f0	O
(	O
x	O
)	O
exp	O
{	O
(	O
θ	O
+	O
λj	O
)	O
t	O
φ	O
(	O
x	O
)	O
+	O
˜θ	O
t	O
i	O
φi	O
(	O
x	O
)	O
}	O
(	O
22.79	O
)	O
(	O
cid:4	O
)	O
j	O
(	O
cid:5	O
)	O
=i	O
790	O
chapter	O
22.	O
more	O
variational	B
inference	I
this	O
corresponds	O
to	O
removing	O
the	O
approximation	O
to	O
the	O
i	O
’	O
th	O
term	O
,	O
λi	O
,	O
from	O
the	O
base	B
distribution	I
,	O
and	O
adding	O
in	O
the	O
correct	O
i	O
’	O
th	O
term	O
,	O
φi	O
.	O
finally	O
,	O
∇λl	O
(	O
τ	O
;	O
λ	O
)	O
=0	O
just	O
enforces	O
the	O
constraints	O
(	O
cid:12	O
)	O
that	O
τ	O
=	O
eq	O
[	O
φ	O
(	O
x	O
)	O
]	O
and	O
ηi	O
=	O
eqi	O
[	O
φ	O
(	O
x	O
)	O
]	O
are	O
equal	O
.	O
in	O
other	O
words	O
,	O
we	O
get	O
the	O
following	O
moment	B
matching	I
constraints	O
:	O
(	O
cid:12	O
)	O
q	O
(	O
x|θ	O
,	O
λ	O
)	O
φ	O
(	O
x	O
)	O
dx	O
=	O
qi	O
(	O
x|θ	O
,	O
˜θi	O
,	O
λ	O
)	O
φ	O
(	O
x	O
)	O
dx	O
(	O
22.80	O
)	O
thus	O
the	O
overall	O
algorithm	O
is	O
as	O
follows	O
.	O
first	O
we	O
initialize	O
the	O
λi	O
.	O
then	O
we	O
iterate	O
the	O
following	O
to	O
convergence	O
:	O
pick	O
a	O
term	O
i	O
;	O
compute	O
qi	O
(	O
corresponding	O
to	O
removing	O
the	O
old	O
approximation	O
to	O
φi	O
and	O
adding	O
in	O
the	O
new	O
one	O
)	O
;	O
then	O
update	O
the	O
λi	O
term	O
in	O
q	O
by	O
solving	O
the	O
moment	B
matching	I
equation	O
eqi	O
[	O
φ	O
(	O
x	O
)	O
]	O
=	O
eq	O
[	O
φ	O
(	O
x	O
)	O
]	O
.	O
(	O
note	O
that	O
this	O
particular	O
optimization	B
scheme	O
is	O
not	O
guaranteed	O
to	O
converge	B
to	O
a	O
ﬁxed	B
point	I
.	O
)	O
an	O
equivalent	O
way	O
of	O
stating	O
the	O
algorithm	O
is	O
as	O
follows	O
.	O
let	O
us	O
assume	O
the	O
true	O
distribution	O
is	O
given	O
by	O
p	O
(	O
x|d	O
)	O
=	O
1	O
z	O
(	O
cid:20	O
)	O
i	O
fi	O
(	O
x	O
)	O
(	O
cid:20	O
)	O
i	O
q	O
(	O
x	O
)	O
=	O
1	O
z	O
˜fi	O
(	O
x	O
)	O
we	O
approximate	O
each	O
fi	O
by	O
˜fi	O
and	O
set	O
now	O
we	O
repeat	O
the	O
following	O
until	O
convergence	O
:	O
1.	O
choose	O
a	O
factor	B
˜fi	O
to	O
reﬁne	O
.	O
2.	O
remove	O
˜fi	O
from	O
the	O
posterior	O
by	O
dividing	O
it	O
out	O
:	O
q−i	O
(	O
x	O
)	O
=	O
q	O
(	O
x	O
)	O
˜fi	O
(	O
x	O
)	O
(	O
22.81	O
)	O
(	O
22.82	O
)	O
(	O
22.83	O
)	O
(	O
cid:8	O
)	O
1	O
zi	O
(	O
cid:12	O
)	O
this	O
can	O
be	O
implemented	O
by	O
substracting	O
off	O
the	O
natural	B
parameters	I
of	O
˜fi	O
from	O
q	O
.	O
3.	O
compute	O
the	O
new	O
posterior	O
qnew	O
(	O
x	O
)	O
by	O
solving	O
(	O
cid:9	O
)	O
fi	O
(	O
x	O
)	O
q−i	O
(	O
x	O
)	O
||qnew	O
(	O
x	O
)	O
kl	O
min	O
qnew	O
(	O
x	O
)	O
(	O
22.84	O
)	O
this	O
can	O
be	O
done	O
by	O
equating	O
the	O
moments	O
of	O
qnew	O
(	O
x	O
)	O
with	O
those	O
of	O
qi	O
(	O
x	O
)	O
∝	O
q−i	O
(	O
x	O
)	O
fi	O
(	O
x	O
)	O
.	O
the	O
corresponding	O
normalization	O
constant	O
has	O
the	O
form	O
zi	O
=	O
q−i	O
(	O
x	O
)	O
fi	O
(	O
x	O
)	O
dx	O
(	O
22.85	O
)	O
4.	O
compute	O
the	O
new	O
factor	B
(	O
message	O
)	O
that	O
was	O
implicitly	O
used	O
(	O
so	O
it	O
can	O
be	O
later	O
removed	O
)	O
:	O
˜fi	O
(	O
x	O
)	O
=	O
zi	O
qnew	O
(	O
x	O
)	O
q−i	O
(	O
x	O
)	O
(	O
22.86	O
)	O
22.5.	O
expectation	B
propagation	I
after	O
convergence	O
,	O
we	O
can	O
approximate	O
the	O
marginal	B
likelihood	I
using	O
(	O
cid:12	O
)	O
(	O
cid:20	O
)	O
p	O
(	O
d	O
)	O
≈	O
˜fi	O
(	O
x	O
)	O
dx	O
791	O
(	O
22.87	O
)	O
i	O
we	O
will	O
give	O
some	O
examples	O
of	O
this	O
below	O
which	O
will	O
make	O
things	O
clearer	O
.	O
22.5.3	O
ep	O
for	O
the	O
clutter	B
problem	I
let	O
us	O
return	O
to	O
considering	O
the	O
clutter	B
problem	I
.	O
our	O
presentation	O
is	O
based	O
on	O
(	O
bishop	O
2006b	O
)	O
.4	O
for	O
simplicity	O
,	O
we	O
will	O
assume	O
that	O
the	O
prior	O
is	O
a	O
spherical	B
gaussian	O
,	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
0	O
,	O
bi	O
)	O
.	O
also	O
,	O
we	O
choose	O
to	O
approximate	O
the	O
posterior	O
by	O
a	O
spherical	B
gaussian	O
,	O
q	O
(	O
x	O
)	O
=	O
n	O
(	O
m	O
,	O
vi	O
)	O
.	O
we	O
set	O
f0	O
(	O
x	O
)	O
to	O
be	O
the	O
prior	O
;	O
this	O
can	O
be	O
held	O
ﬁxed	O
.	O
the	O
factor	B
approximations	O
will	O
be	O
“	O
gaussian	O
like	O
”	O
terms	O
of	O
the	O
form	O
˜fi	O
(	O
x	O
)	O
=	O
sin	O
(	O
x|mi	O
,	O
vii	O
)	O
(	O
22.88	O
)	O
note	O
,	O
however	O
,	O
that	O
in	O
the	O
ep	O
updates	O
,	O
the	O
variances	O
may	O
be	O
negative	O
!	O
thus	O
these	O
terms	O
should	O
(	O
if	O
the	O
variance	B
is	O
be	O
interpreted	O
as	O
functions	O
,	O
but	O
not	O
necessarily	O
probability	O
distributions	O
.	O
negative	O
,	O
it	O
means	O
the	O
that	O
˜fi	O
curves	O
upwards	O
instead	O
of	O
downwards	O
.	O
)	O
first	O
we	O
remove	O
˜fi	O
(	O
x	O
)	O
from	O
q	O
(	O
x	O
)	O
by	O
division	O
,	O
which	O
yields	O
q−i	O
(	O
x	O
)	O
=	O
n	O
(	O
m−i	O
,	O
v−ii	O
)	O
,	O
where	O
v−1−i	O
=	O
v−1	O
−	O
v−1	O
(	O
22.89	O
)	O
m−i	O
=	O
m	O
+	O
v−iv−1	O
(	O
22.90	O
)	O
i	O
(	O
m	O
−	O
mi	O
)	O
the	O
normalization	O
constant	O
is	O
given	O
by	O
i	O
zi	O
=	O
(	O
1	O
−	O
w	O
)	O
n	O
(	O
yi|m−i	O
,	O
(	O
v−i	O
+	O
1	O
)	O
i	O
)	O
+w	O
n	O
(	O
yi|0	O
,	O
ai	O
)	O
(	O
22.91	O
)	O
next	O
we	O
compute	O
qnew	O
(	O
x	O
)	O
by	O
computing	O
the	O
mean	B
and	O
variance	B
of	O
q−i	O
(	O
x	O
)	O
fi	O
(	O
x	O
)	O
as	O
follows	O
:	O
m	O
=	O
m−i	O
+	O
ρi	O
v	O
=	O
v−i	O
−	O
ρi	O
ρi	O
=	O
1	O
−	O
w	O
zi	O
(	O
yi	O
−	O
m−i	O
)	O
+	O
ρi	O
(	O
1	O
−	O
ρi	O
)	O
v2−i||yi	O
−	O
mi||2	O
d	O
(	O
v−i	O
+	O
1	O
)	O
2	O
v−i	O
+	O
1	O
n	O
(	O
yi|0	O
,	O
ai	O
)	O
v−i	O
+	O
1	O
v2−i	O
(	O
22.93	O
)	O
(	O
22.92	O
)	O
(	O
22.94	O
)	O
v−i	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
x	O
and	O
ρi	O
can	O
be	O
interpreted	O
as	O
the	O
probability	O
that	O
yi	O
is	O
not	O
clutter	O
.	O
i	O
=	O
v−1	O
−	O
v−1−i	O
finally	O
,	O
we	O
compute	O
the	O
new	O
factor	B
˜fi	O
whose	O
parameters	O
are	O
given	O
by	O
v−1	O
mi	O
=	O
m−i	O
+	O
(	O
vi	O
+	O
v−i	O
)	O
v−1−i	O
(	O
m	O
−	O
m−i	O
)	O
(	O
2πvi	O
)	O
d/2n	O
(	O
mi|m−i	O
,	O
(	O
vi	O
+	O
v−i	O
)	O
i	O
)	O
si	O
=	O
zi	O
(	O
22.95	O
)	O
(	O
22.96	O
)	O
(	O
22.97	O
)	O
4.	O
for	O
a	O
handy	O
“	O
crib	O
sheet	O
”	O
,	O
containing	O
many	O
of	O
the	O
standard	O
equations	O
needed	O
for	O
deriving	O
gaussian	O
ep	O
algorithms	O
,	O
see	O
http	O
:	O
//research.microsoft.com/en-us/um/people/minka/papers/ep/minka-ep-quickref.pdf	O
.	O
792	O
chapter	O
22.	O
more	O
variational	B
inference	I
at	O
convergence	O
,	O
we	O
can	O
approximate	O
the	O
marginal	B
likelihood	I
as	O
follows	O
:	O
p	O
(	O
d	O
)	O
≈	O
(	O
2πv	O
)	O
d/2	O
exp	O
(	O
c/2	O
)	O
c	O
(	O
cid:2	O
)	O
mt	O
m	O
v	O
n	O
(	O
cid:20	O
)	O
−	O
n	O
(	O
cid:4	O
)	O
i=1	O
i=1	O
mt	O
i	O
mi	O
vi	O
−d/2	O
si	O
(	O
2πvi	O
)	O
(	O
22.98	O
)	O
(	O
22.99	O
)	O
in	O
(	O
minka	O
2001d	O
)	O
,	O
it	O
is	O
shown	O
that	O
,	O
at	O
least	O
on	O
this	O
example	O
,	O
ep	O
gives	O
better	O
accuracy	O
per	O
unit	O
of	O
cpu	O
time	O
than	O
vb	O
and	O
mcmc	O
.	O
22.5.4	O
lbp	O
is	O
a	O
special	O
case	O
of	O
ep	O
we	O
now	O
show	O
that	O
loopy	B
belief	I
propagation	I
is	O
a	O
special	O
case	O
of	O
ep	O
,	O
where	O
the	O
base	B
distribution	I
contains	O
the	O
node	O
marginals	O
and	O
the	O
“	O
intractable	O
”	O
terms	O
correspond	O
to	O
the	O
edge	O
potentials	O
.	O
we	O
if	O
there	O
are	O
m	O
nodes	B
,	O
the	O
assume	O
the	O
model	O
has	O
the	O
pairwise	O
form	O
shown	O
in	O
equation	O
22.12.	O
base	B
distribution	I
takes	O
the	O
form	O
p	O
(	O
x|θ1	O
,	O
.	O
.	O
.	O
,	O
θm	O
,	O
0	O
)	O
∝	O
exp	O
(	O
θs	O
(	O
xs	O
)	O
)	O
the	O
entropy	B
of	O
this	O
distribution	O
is	O
simply	O
h	O
(	O
τ	O
1	O
:	O
m	O
)	O
=	O
h	O
(	O
τ	O
s	O
)	O
(	O
cid:4	O
)	O
s	O
if	O
we	O
add	O
in	O
the	O
u	O
−	O
v	O
edge	O
,	O
the	O
φuv	O
augmented	O
distribution	O
has	O
the	O
form	O
(	O
cid:25	O
)	O
p	O
(	O
x|θ1	O
:	O
m	O
,	O
θuv	O
)	O
∝	O
exp	O
(	O
θs	O
(	O
xs	O
)	O
)	O
exp	O
(	O
θuv	O
(	O
xu	O
,	O
xv	O
)	O
)	O
(	O
22.100	O
)	O
(	O
22.101	O
)	O
(	O
22.102	O
)	O
(	O
cid:20	O
)	O
s∈v	O
(	O
cid:24	O
)	O
(	O
cid:20	O
)	O
(	O
cid:4	O
)	O
s∈v	O
since	O
this	O
graph	B
is	O
a	O
tree	B
,	O
the	O
exact	O
entropy	B
of	O
this	O
distribution	O
is	O
given	O
by	O
h	O
(	O
τ	O
s	O
)	O
−	O
i	O
(	O
˜τ	O
uv	O
)	O
h	O
(	O
τ	O
1	O
:	O
m	O
,	O
˜τ	O
uv	O
)	O
=	O
(	O
22.103	O
)	O
where	O
i	O
(	O
τ	O
uv	O
)	O
=h	O
(	O
τ	O
u	O
)	O
+h	O
(	O
τ	O
v	O
)	O
−	O
h	O
(	O
τ	O
uv	O
)	O
is	O
the	O
mutual	B
information	I
.	O
thus	O
the	O
ep	O
approxi-	O
mation	O
to	O
the	O
entropy	B
of	O
the	O
full	B
distribution	O
is	O
given	O
by	O
s	O
hep	O
(	O
τ	O
,	O
˜τ	O
)	O
=h	O
(	O
τ	O
)	O
+	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
s	O
s	O
=	O
=	O
h	O
(	O
τ	O
s	O
)	O
+	O
h	O
(	O
τ	O
s	O
)	O
−	O
(	O
cid:24	O
)	O
(	O
cid:4	O
)	O
[	O
h	O
(	O
τ	O
1	O
:	O
m	O
,	O
˜τ	O
uv	O
)	O
−	O
h	O
(	O
τ	O
)	O
]	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
s	O
h	O
(	O
τ	O
s	O
)	O
−	O
i	O
(	O
˜τ	O
uv	O
)	O
−	O
(	O
u	O
,	O
v	O
)	O
∈e	O
i	O
(	O
˜τ	O
uv	O
)	O
(	O
u	O
,	O
v	O
)	O
∈e	O
(	O
22.104	O
)	O
(	O
cid:25	O
)	O
h	O
(	O
τ	O
s	O
)	O
(	O
22.105	O
)	O
(	O
cid:4	O
)	O
s	O
(	O
22.106	O
)	O
(	O
cid:4	O
)	O
(	O
u	O
,	O
v	O
)	O
∈e	O
which	O
is	O
precisely	O
the	O
bethe	O
approximation	O
to	O
the	O
entropy	B
.	O
22.5.	O
expectation	B
propagation	I
793	O
we	O
now	O
show	O
that	O
the	O
convex	B
set	O
that	O
ep	O
is	O
optimizing	O
over	O
,	O
l	O
(	O
φ	O
,	O
φ	O
)	O
given	O
by	O
equation	O
22.73	O
,	O
is	O
the	O
same	O
as	O
the	O
one	O
that	O
lbp	O
is	O
optimizing	O
over	O
,	O
l	O
(	O
g	O
)	O
given	O
in	O
equation	O
22.33.	O
first	O
,	O
let	O
us	O
consider	O
the	O
set	O
m	O
(	O
φ	O
)	O
.	O
this	O
consists	O
of	O
all	O
marginal	O
distributions	O
(	O
τ	O
s	O
,	O
s	O
∈	O
v	O
)	O
,	O
realizable	O
by	O
a	O
factored	O
distribution	O
.	O
this	O
is	O
therefore	O
equivalent	O
to	O
the	O
set	O
of	O
all	O
distributions	O
which	O
satisfy	O
non-negativity	O
τs	O
(	O
xs	O
)	O
≥	O
0	O
and	O
the	O
local	O
normalization	O
constraint	O
τ	O
(	O
xs	O
)	O
=	O
1.	O
now	O
consider	O
the	O
set	O
m	O
(	O
φ	O
,	O
φuv	O
)	O
for	O
a	O
single	O
u−v	O
edge	O
.	O
this	O
is	O
equivalent	O
to	O
the	O
marginal	B
polytope	I
m	O
(	O
guv	O
)	O
,	O
where	O
guv	O
is	O
the	O
graph	B
with	O
the	O
single	O
u	O
−	O
v	O
edge	O
added	O
.	O
since	O
this	O
graph	B
corresponds	O
to	O
a	O
(	O
cid:4	O
)	O
tree	B
,	O
this	O
set	O
also	O
satisﬁes	O
the	O
marginalization	O
conditions	O
(	O
cid:4	O
)	O
(	O
cid:7	O
)	O
xs	O
τuv	O
(	O
xu	O
,	O
xv	O
)	O
=	O
τu	O
(	O
xu	O
)	O
,	O
τuv	O
(	O
xu	O
,	O
xv	O
)	O
=	O
τv	O
(	O
xv	O
)	O
xv	O
xu	O
(	O
22.107	O
)	O
(	O
22.108	O
)	O
(	O
22.109	O
)	O
(	O
22.110	O
)	O
since	O
l	O
(	O
φ	O
,	O
φ	O
)	O
is	O
the	O
union	O
of	O
such	O
sets	O
,	O
as	O
we	O
sweep	O
over	O
all	O
edges	O
in	O
the	O
graph	B
,	O
we	O
recover	O
the	O
same	O
set	O
as	O
l	O
(	O
g	O
)	O
.	O
we	O
have	O
shown	O
that	O
the	O
bethe	O
approximation	O
is	O
equivalent	O
to	O
the	O
ep	O
approximation	O
.	O
we	O
now	O
show	O
how	O
the	O
ep	O
algorithm	O
reduces	O
to	O
lbp	O
.	O
associated	O
with	O
each	O
intractable	O
term	O
i	O
=	O
(	O
u	O
,	O
v	O
)	O
will	O
be	O
a	O
pair	O
of	O
lagrange	O
multipliers	O
,	O
(	O
λuv	O
(	O
xv	O
)	O
,	O
λvu	O
(	O
xu	O
)	O
)	O
.	O
recalling	O
that	O
θt	O
φ	O
(	O
x	O
)	O
=	O
[	O
θs	O
(	O
xs	O
)	O
]	O
s	O
,	O
the	O
base	B
distribution	I
in	O
equation	O
22.78	O
has	O
the	O
form	O
q	O
(	O
x|θ	O
,	O
λ	O
)	O
∝	O
exp	O
(	O
θs	O
(	O
xs	O
)	O
)	O
exp	O
(	O
λuv	O
(	O
xv	O
)	O
+λ	O
vu	O
(	O
xu	O
)	O
)	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
s	O
(	O
cid:20	O
)	O
⎛	O
(	O
cid:4	O
)	O
⎝θs	O
(	O
xs	O
)	O
+	O
(	O
u	O
,	O
v	O
)	O
∈e	O
=	O
exp	O
s	O
λts	O
(	O
xs	O
)	O
t∈n	O
(	O
s	O
)	O
⎞	O
⎠	O
similarly	O
,	O
the	O
augmented	O
distribution	O
in	O
equation	O
22.79	O
has	O
the	O
form	O
quv	O
(	O
x|θ	O
,	O
λ	O
)	O
∝	O
q	O
(	O
x|θ	O
,	O
λ	O
)	O
exp	O
(	O
θuv	O
(	O
xu	O
,	O
xv	O
)	O
−	O
λuv	O
(	O
xv	O
)	O
−	O
λvu	O
(	O
xu	O
)	O
)	O
we	O
now	O
need	O
to	O
update	O
τu	O
(	O
xu	O
)	O
and	O
τv	O
(	O
xv	O
)	O
to	O
enforce	O
the	O
moment	B
matching	I
constraints	O
:	O
(	O
eq	O
[	O
xs	O
]	O
,	O
eq	O
[	O
xt	O
]	O
)	O
=	O
(	O
equv	O
[	O
xs	O
]	O
,	O
equv	O
[	O
xt	O
]	O
)	O
(	O
22.111	O
)	O
it	O
can	O
be	O
shown	O
that	O
this	O
can	O
be	O
done	O
by	O
performing	O
the	O
usual	O
sum-product	B
message	O
passing	O
step	O
along	O
the	O
u	O
−	O
v	O
edge	O
(	O
in	O
both	O
directions	O
)	O
,	O
where	O
the	O
messages	O
are	O
given	O
by	O
muv	O
(	O
xv	O
)	O
=	O
exp	O
(	O
λuv	O
(	O
xv	O
)	O
)	O
,	O
and	O
mvu	O
(	O
xu	O
)	O
=	O
exp	O
(	O
λvu	O
(	O
xu	O
)	O
)	O
.	O
once	O
we	O
have	O
updated	O
q	O
,	O
we	O
can	O
derive	O
the	O
corresponding	O
messages	O
λuv	O
and	O
λvu	O
.	O
the	O
above	O
analysis	O
suggests	O
a	O
natural	O
extension	O
,	O
where	O
we	O
make	O
the	O
base	B
distribution	I
be	O
a	O
tree	B
structure	O
instead	O
of	O
a	O
fully	O
factored	O
distribution	O
.	O
we	O
then	O
add	O
in	O
one	O
edge	O
at	O
a	O
time	O
,	O
absorb	O
its	O
effect	O
,	O
and	O
approximate	O
the	O
resulting	O
distribution	O
by	O
a	O
new	O
tree	B
.	O
this	O
is	O
known	O
as	O
tree	B
ep	O
(	O
minka	O
and	O
qi	O
2003	O
)	O
,	O
and	O
is	O
more	O
accurate	O
than	O
lbp	O
,	O
and	O
sometimes	O
faster	O
.	O
by	O
considering	O
other	O
kinds	O
of	O
structured	O
base	O
distributions	O
,	O
we	O
can	O
derive	O
algorothms	O
that	O
outperform	O
generalization	B
belief	O
propagation	O
(	O
welling	O
et	O
al	O
.	O
2005	O
)	O
.	O
22.5.5	O
ranking	B
players	O
using	O
trueskill	O
we	O
now	O
present	O
an	O
interesting	O
application	O
of	O
ep	O
to	O
the	O
problem	O
of	O
ranking	B
players	O
who	O
compete	O
in	O
games	O
.	O
microsoft	O
uses	O
this	O
method	O
—	O
known	O
as	O
trueskill	O
(	O
herbrich	O
et	O
al	O
.	O
2007	O
)	O
—	O
to	O
rank	O
794	O
chapter	O
22.	O
more	O
variational	B
inference	I
(	O
cid:54	O
)	O
(	O
cid:20	O
)	O
(	O
cid:51	O
)	O
(	O
cid:20	O
)	O
(	O
cid:87	O
)	O
(	O
cid:20	O
)	O
(	O
cid:54	O
)	O
(	O
cid:21	O
)	O
(	O
cid:54	O
)	O
(	O
cid:22	O
)	O
(	O
cid:51	O
)	O
(	O
cid:21	O
)	O
(	O
cid:51	O
)	O
(	O
cid:22	O
)	O
(	O
cid:71	O
)	O
(	O
cid:20	O
)	O
(	O
cid:92	O
)	O
(	O
cid:20	O
)	O
(	O
cid:87	O
)	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
cid:41	O
)	O
(	O
cid:21	O
)	O
(	O
cid:41	O
)	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
(	O
cid:54	O
)	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
(	O
cid:54	O
)	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
(	O
cid:25	O
)	O
(	O
cid:25	O
)	O
(	O
cid:21	O
)	O
(	O
cid:24	O
)	O
(	O
cid:22	O
)	O
(	O
cid:23	O
)	O
(	O
cid:75	O
)	O
(	O
cid:20	O
)	O
(	O
cid:71	O
)	O
(	O
cid:20	O
)	O
(	O
cid:78	O
)	O
(	O
cid:20	O
)	O
(	O
cid:92	O
)	O
(	O
cid:20	O
)	O
(	O
b	O
)	O
(	O
cid:54	O
)	O
(	O
cid:23	O
)	O
(	O
cid:51	O
)	O
(	O
cid:23	O
)	O
(	O
cid:87	O
)	O
(	O
cid:22	O
)	O
(	O
cid:41	O
)	O
(	O
cid:22	O
)	O
(	O
cid:54	O
)	O
(	O
cid:22	O
)	O
(	O
cid:71	O
)	O
(	O
cid:21	O
)	O
(	O
cid:92	O
)	O
(	O
cid:21	O
)	O
(	O
cid:75	O
)	O
(	O
cid:21	O
)	O
(	O
cid:71	O
)	O
(	O
cid:21	O
)	O
(	O
cid:78	O
)	O
(	O
cid:21	O
)	O
(	O
cid:92	O
)	O
(	O
cid:21	O
)	O
figure	O
22.11	O
(	O
a	O
)	O
a	O
dgm	O
representing	O
the	O
trueskill	O
model	O
for	O
4	O
players	O
and	O
3	O
teams	O
,	O
where	O
team	O
1	O
is	O
player	O
1	O
,	O
team	O
2	O
is	O
players	O
2	O
and	O
3	O
,	O
and	O
team	O
3	O
is	O
player	O
4.	O
we	O
assume	O
there	O
are	O
two	O
games	O
,	O
team	O
1	O
vs	O
team	O
2	O
,	O
and	O
team	O
2	O
vs	O
team	O
3.	O
nodes	B
with	O
double	O
circles	O
are	O
deterministic	O
.	O
(	O
b	O
)	O
a	O
factor	B
graph	I
representation	O
of	O
the	O
model	O
where	O
we	O
assume	O
there	O
are	O
3	O
players	O
(	O
and	O
no	O
teams	O
)	O
.	O
there	O
are	O
2	O
games	O
,	O
player	O
1	O
vs	O
player	O
2	O
,	O
and	O
player	O
2	O
vs	O
player	O
3.	O
the	O
numbers	O
inside	O
circles	O
represent	O
steps	O
in	O
the	O
message	B
passing	I
algorithm	O
.	O
22.5.	O
expectation	B
propagation	I
795	O
i|st−1	O
i	O
players	O
who	O
use	O
the	O
xbox	O
360	O
live	O
online	O
gaming	O
system	O
;	O
this	O
system	O
process	O
over	O
105	O
games	O
per	O
day	O
,	O
making	O
this	O
one	O
of	O
the	O
largest	O
application	O
of	O
bayesian	O
statistics	O
to	O
date.5	O
the	O
same	O
method	O
can	O
also	O
be	O
applied	O
to	O
other	O
games	O
,	O
such	O
as	O
tennis	O
or	O
chess.6	O
the	O
basic	O
idea	O
is	O
shown	O
in	O
figure	O
22.11	O
(	O
a	O
)	O
.	O
we	O
assume	O
each	O
player	O
i	O
has	O
a	O
latent	B
or	O
true	O
underlying	O
skill	O
level	O
si	O
∈	O
r.	O
these	O
skill	O
levels	O
can	O
evolve	O
over	O
time	O
according	O
to	O
a	O
simple	O
,	O
γ2	O
)	O
.	O
in	O
any	O
given	O
game	O
,	O
we	O
deﬁne	O
the	O
performance	O
dynamical	O
model	O
,	O
p	O
(	O
st	O
of	O
player	O
i	O
to	O
be	O
pi	O
,	O
which	O
has	O
the	O
conditional	O
distribution	O
p	O
(	O
pi|si	O
)	O
=n	O
(	O
pi|si	O
,	O
β2	O
)	O
.	O
we	O
then	O
deﬁne	O
the	O
performance	O
of	O
a	O
team	O
to	O
be	O
the	O
sum	O
of	O
the	O
performance	O
of	O
its	O
constituent	O
players	O
.	O
for	O
example	O
,	O
in	O
figure	O
22.11	O
(	O
a	O
)	O
,	O
we	O
assume	O
team	O
2	O
is	O
composed	O
of	O
players	O
2	O
and	O
3	O
,	O
so	O
we	O
deﬁne	O
t2	O
=	O
p2	O
+	O
p3	O
.	O
finally	O
,	O
we	O
assume	O
that	O
the	O
outcome	O
of	O
a	O
game	O
depends	O
on	O
the	O
difference	O
in	O
performance	O
levels	O
of	O
the	O
two	O
teams	O
.	O
for	O
example	O
,	O
in	O
figure	O
22.11	O
(	O
a	O
)	O
,	O
we	O
assume	O
y1	O
=	O
sign	O
(	O
d1	O
)	O
,	O
where	O
d1	O
=	O
t1	O
−	O
t2	O
,	O
and	O
where	O
y1	O
=	O
+1	O
means	O
team	O
1	O
won	O
,	O
and	O
y1	O
=	O
−1	O
means	O
team	O
2	O
won	O
.	O
thus	O
the	O
prior	O
probability	O
that	O
team	O
1	O
wins	O
is	O
)	O
=	O
n	O
(	O
st	O
i|st−1	O
i	O
p	O
(	O
y1	O
=	O
+1|s	O
)	O
=	O
p	O
(	O
d1	O
>	O
0|t1	O
,	O
t2	O
)	O
p	O
(	O
t1|s1	O
)	O
p	O
(	O
t2|s2	O
)	O
dt1dt2	O
(	O
22.112	O
)	O
where	O
t1	O
∼	O
n	O
(	O
s1	O
,	O
β2	O
)	O
and	O
t2	O
∼	O
n	O
(	O
s2	O
+	O
s3	O
,	O
β2	O
)	O
.7	O
to	O
simplify	O
the	O
presentation	O
of	O
the	O
algorithm	O
,	O
we	O
will	O
ignore	O
the	O
dynamical	O
model	O
and	O
assume	O
a	O
common	O
static	O
factored	O
gaussian	O
prior	O
,	O
n	O
(	O
μ0	O
,	O
σ2	O
0	O
)	O
,	O
on	O
the	O
skills	O
.	O
also	O
,	O
we	O
will	O
assume	O
that	O
each	O
team	O
consists	O
of	O
1	O
player	O
,	O
so	O
ti	O
=	O
pi	O
,	O
and	O
that	O
there	O
can	O
be	O
no	O
ties	O
.	O
finally	O
,	O
we	O
will	O
integrate	B
out	I
the	O
performance	O
variables	O
pi	O
,	O
and	O
assume	O
β2	O
=	O
1	O
,	O
leading	O
to	O
a	O
ﬁnal	O
model	O
of	O
the	O
form	O
(	O
cid:12	O
)	O
(	O
cid:20	O
)	O
p	O
(	O
s	O
)	O
=	O
i	O
n	O
(	O
si|μ0	O
,	O
σ2	O
)	O
−	O
sjg	O
(	O
yg	O
=	O
sign	O
(	O
dg	O
)	O
)	O
,	O
1	O
)	O
p	O
(	O
dg|s	O
)	O
=n	O
(	O
dg|sig	O
p	O
(	O
yg|dg	O
)	O
=i	O
(	O
22.113	O
)	O
(	O
22.114	O
)	O
(	O
22.115	O
)	O
where	O
ig	O
is	O
the	O
ﬁrst	O
player	O
of	O
game	O
g	O
,	O
and	O
jg	O
is	O
the	O
second	O
player	O
.	O
this	O
is	O
represented	O
in	O
factor	B
graph	I
form	O
in	O
in	O
figure	O
22.11	O
(	O
b	O
)	O
.	O
we	O
have	O
3	O
kinds	O
of	O
factors	B
:	O
the	O
prior	O
factor	B
,	O
fi	O
(	O
si	O
)	O
=	O
n	O
(	O
si|μ0	O
,	O
σ2	O
0	O
)	O
,	O
the	O
game	O
factor	O
,	O
hg	O
(	O
sig	O
,	O
sjg	O
,	O
dg	O
)	O
=n	O
(	O
dg|sig	O
−	O
sjg	O
,	O
1	O
)	O
,	O
and	O
the	O
outcome	O
factor	B
,	O
kg	O
(	O
dg	O
,	O
yg	O
)	O
=	O
i	O
(	O
yg	O
=	O
sign	O
(	O
dg	O
)	O
)	O
.	O
since	O
the	O
likelihood	B
term	O
(	O
yg|dg	O
)	O
is	O
not	O
conjugate	O
to	O
the	O
gaussian	O
priors	O
,	O
we	O
will	O
have	O
to	O
perform	O
approximate	B
inference	I
.	O
thus	O
even	O
when	O
the	O
graph	B
is	O
a	O
tree	B
,	O
we	O
will	O
need	O
to	O
iterate	O
.	O
(	O
if	O
there	O
were	O
an	O
additional	O
game	O
,	O
say	O
between	O
player	O
1	O
and	O
player	O
3	O
,	O
then	O
the	O
graph	B
would	O
no	O
longer	O
be	O
a	O
tree	B
.	O
)	O
we	O
will	O
represent	O
all	O
messages	O
and	O
marginal	O
beliefs	O
by	O
1d	O
gaussians	O
.	O
we	O
will	O
use	O
the	O
notation	O
μ	O
and	O
v	O
for	O
the	O
mean	B
and	O
variance	B
(	O
the	O
moment	B
parameters	I
)	O
,	O
and	O
λ	O
=	O
1/v	O
and	O
η	O
=	O
λμ	O
for	O
the	O
precision	B
and	O
precision-adjusted	O
mean	B
(	O
the	O
natural	B
parameters	I
)	O
.	O
5.	O
naive	O
bayes	O
classiﬁers	O
,	O
which	O
are	O
widely	O
used	O
in	O
spam	B
ﬁlters	O
,	O
are	O
often	O
described	O
as	O
the	O
most	O
common	O
application	O
of	O
bayesian	O
methods	O
.	O
however	O
,	O
the	O
parameters	O
of	O
such	O
models	O
are	O
usually	O
ﬁt	O
using	O
non-bayesian	O
methods	O
,	O
such	O
as	O
penalized	O
maximum	O
likelihood	B
.	O
6.	O
our	O
presentation	O
of	O
this	O
algorithm	O
is	O
based	O
in	O
part	O
on	O
lecture	O
notes	O
by	O
carl	O
rasmussen	O
joaquin	O
quinonero-candela	O
,	O
available	O
at	O
http	O
:	O
//mlg.eng.cam.ac.uk/teaching/4f13/1112/lect13.pdf	O
.	O
7.	O
note	O
that	O
this	O
is	O
very	O
similar	B
to	O
probit	B
regression	I
,	O
discussed	O
in	O
section	O
9.4	O
,	O
except	O
the	O
inputs	O
are	O
(	O
the	O
differences	O
of	O
)	O
latent	B
1	O
dimensional	O
factors	B
.	O
if	O
we	O
assume	O
a	O
logistic	B
noise	O
model	O
instead	O
of	O
a	O
gaussian	O
noise	O
model	O
,	O
we	O
recover	O
the	O
bradley	O
terry	O
model	O
of	O
ranking	B
.	O
796	O
chapter	O
22.	O
more	O
variational	B
inference	I
we	O
initialize	O
by	O
assuming	O
that	O
at	O
iteration	O
0	O
,	O
the	O
initial	O
upward	O
messages	O
from	O
factors	B
hg	O
to	O
variables	O
si	O
are	O
uniform	O
,	O
i.e.	O
,	O
m0	O
hg→sig	O
(	O
sig	O
)	O
=	O
1	O
,	O
λ0	O
hg→sig	O
=	O
0	O
,	O
η0	O
hg→sig	O
=	O
0	O
(	O
22.116	O
)	O
and	O
similarly	O
m0	O
as	O
illustrated	O
in	O
figure	O
22.11	O
(	O
b	O
)	O
.	O
we	O
give	O
the	O
details	O
of	O
these	O
steps	O
below	O
.	O
(	O
sjg	O
)	O
=	O
1.	O
the	O
messages	O
passing	O
algorithm	O
consists	O
of	O
6	O
steps	O
per	O
game	O
,	O
hg→sjg	O
1.	O
compute	O
the	O
posterior	O
over	O
the	O
skills	O
variables	O
:	O
qt	O
(	O
si	O
)	O
=f	O
(	O
si	O
)	O
λt	O
i	O
=	O
λ0	O
+	O
hg→si	O
(	O
si	O
)	O
=	O
nc	O
(	O
si|ηt	O
mt−1	O
(	O
cid:4	O
)	O
λt−1	O
hg→si	O
i	O
=	O
η0	O
+	O
,	O
ηt	O
i	O
,	O
λt	O
i	O
)	O
ηt−1	O
hg→si	O
(	O
cid:20	O
)	O
(	O
cid:4	O
)	O
g	O
g	O
g	O
2.	O
compute	O
the	O
message	O
from	O
the	O
skills	O
variables	O
down	O
to	O
the	O
game	O
factor	O
hg	O
:	O
mt	O
sig→hg	O
(	O
sig	O
)	O
=	O
qt	O
(	O
sig	O
)	O
hg→sig	O
mt	O
(	O
sig	O
)	O
,	O
mt	O
sjg→hg	O
(	O
sjg	O
)	O
=	O
qt	O
(	O
sjg	O
)	O
hg→sjg	O
mt	O
(	O
sjg	O
)	O
where	O
the	O
division	O
is	O
implemented	O
by	O
subtracting	O
the	O
natural	B
parameters	I
as	O
follows	O
:	O
λt	O
sig→hg	O
=	O
λt	O
sig	O
−	O
λt	O
hg→sig	O
,	O
ηt	O
sig→hg	O
=	O
ηt	O
sig	O
−	O
ηt	O
hg→sig	O
(	O
22.117	O
)	O
(	O
22.118	O
)	O
(	O
22.119	O
)	O
(	O
22.120	O
)	O
and	O
similarly	O
for	O
sjg	O
.	O
3.	O
compute	O
the	O
message	O
from	O
the	O
game	O
factor	O
hg	O
down	O
to	O
the	O
difference	O
variable	O
dg	O
:	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
mt	O
hg→dg	O
(	O
dg	O
)	O
=	O
=	O
sig→hg	O
(	O
sig	O
)	O
mt	O
sjg→hg	O
(	O
sjg	O
)	O
dsig	O
sig→hg	O
,	O
vt	O
sig→hg	O
)	O
,	O
sjg	O
)	O
mt	O
hg	O
(	O
dg	O
,	O
sig	O
n	O
(	O
dg|sig	O
−	O
sjg	O
,	O
1	O
)	O
n	O
(	O
sig|μt	O
sjg→hg	O
)	O
dsig	O
dsjg	O
hg→dg	O
)	O
sjg→hg	O
,	O
vt	O
sig→hg	O
+	O
vt	O
sjg→hg	O
hg→dg	O
,	O
vt	O
n	O
(	O
sjg|μt	O
=	O
n	O
(	O
dg|μt	O
vt	O
hg→dg	O
=	O
1	O
+	O
vt	O
μt	O
hg→dg	O
=	O
μt	O
sig→hg	O
−	O
μt	O
sjg→hg	O
dsjg	O
(	O
22.121	O
)	O
(	O
22.122	O
)	O
(	O
22.123	O
)	O
(	O
22.124	O
)	O
(	O
22.125	O
)	O
(	O
22.126	O
)	O
(	O
22.127	O
)	O
(	O
22.128	O
)	O
(	O
22.129	O
)	O
4.	O
compute	O
the	O
posterior	O
over	O
the	O
difference	O
variables	O
:	O
qt	O
(	O
dg	O
)	O
∝	O
mt	O
hg→dg	O
(	O
dg	O
)	O
mkg→dg	O
(	O
dg	O
)	O
=	O
n	O
(	O
dg|μt	O
≈	O
n	O
(	O
dg|μt	O
hg→dg	O
g	O
,	O
vt	O
g	O
)	O
,	O
vt	O
hg→dg	O
)	O
i	O
(	O
yg	O
=	O
sign	O
(	O
dg	O
)	O
)	O
22.5.	O
expectation	B
propagation	I
797	O
ψ	O
function	O
λ	O
function	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−6	O
−4	O
−2	O
0	O
(	O
a	O
)	O
2	O
4	O
6	O
0	O
−6	O
−4	O
−2	O
2	O
4	O
6	O
0	O
(	O
b	O
)	O
figure	O
22.12	O
(	O
a	O
)	O
ψ	O
function	O
.	O
(	O
b	O
)	O
λ	O
function	O
.	O
based	O
on	O
figure	O
2	O
of	O
(	O
herbrich	O
et	O
al	O
.	O
2007	O
)	O
.	O
figure	O
generated	O
by	O
trueskillplot	O
.	O
(	O
note	O
that	O
the	O
upward	O
message	O
from	O
the	O
kg	O
factor	B
is	O
constant	O
.	O
)	O
we	O
can	O
ﬁnd	O
these	O
parameters	O
by	O
moment	B
matching	I
as	O
follows	O
:	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
ygμt	O
hg→dg	O
σt	O
hg→dg	O
(	O
cid:11	O
)	O
(	O
cid:25	O
)	O
ygμt	O
hg→dg	O
σt	O
hg→dg	O
g	O
=	O
ygμt	O
μt	O
hg→dg	O
+	O
σt	O
hg→dg	O
ψ	O
(	O
cid:24	O
)	O
(	O
cid:10	O
)	O
1	O
−	O
λ	O
vt	O
g	O
=	O
vt	O
hg→dg	O
ψ	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
n	O
(	O
x|0	O
,	O
1	O
)	O
λ	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
ψ	O
(	O
x	O
)	O
(	O
ψ	O
(	O
x	O
)	O
+x	O
)	O
φ	O
(	O
x	O
)	O
(	O
22.130	O
)	O
(	O
22.131	O
)	O
(	O
22.132	O
)	O
(	O
22.133	O
)	O
(	O
the	O
derivation	O
of	O
these	O
equations	O
is	O
left	O
as	O
a	O
modiﬁcation	O
to	O
exercise	O
11.15	O
.	O
)	O
these	O
functions	O
are	O
plotted	O
in	O
figure	O
22.12.	O
let	O
us	O
try	O
to	O
understand	O
these	O
equations	O
.	O
suppose	O
μt	O
hg→dg	O
is	O
a	O
large	O
positive	O
number	O
.	O
that	O
means	O
we	O
expect	O
,	O
based	O
on	O
the	O
current	O
estimate	O
of	O
the	O
skills	O
,	O
that	O
dg	O
will	O
be	O
large	O
and	O
positive	O
.	O
consequently	O
,	O
if	O
we	O
observe	O
yg	O
=	O
+1	O
,	O
we	O
will	O
not	O
be	O
surprised	O
that	O
ig	O
is	O
the	O
winner	O
,	O
which	O
is	O
reﬂected	O
in	O
the	O
fact	O
that	O
the	O
update	O
factor	B
for	O
the	O
hg→dg	O
)	O
≈	O
0.	O
similarly	O
,	O
the	O
update	O
factor	B
for	O
the	O
variance	B
is	O
small	O
,	O
mean	B
is	O
small	O
,	O
ψ	O
(	O
ygμt	O
hg→dg	O
)	O
≈	O
0.	O
however	O
,	O
if	O
we	O
observe	O
yg	O
=	O
−1	O
,	O
then	O
the	O
update	O
factor	B
for	O
the	O
mean	B
λ	O
(	O
ygμt	O
and	O
variance	B
becomes	O
quite	O
large	O
.	O
5.	O
compute	O
the	O
upward	O
message	O
from	O
the	O
difference	O
variable	O
to	O
the	O
game	O
factor	O
hg	O
:	O
mt	O
dg→hg	O
(	O
dg	O
)	O
=	O
qt	O
(	O
dg	O
)	O
mt	O
dg→hg	O
(	O
dg	O
)	O
g	O
−	O
λt	O
λt	O
dg→hh	O
=	O
λt	O
hg→dg	O
,	O
ηt	O
dg→hh	O
=	O
ηt	O
g	O
−	O
ηt	O
hg→dg	O
(	O
22.134	O
)	O
(	O
22.135	O
)	O
6.	O
compute	O
the	O
upward	O
messages	O
from	O
the	O
game	O
factor	O
to	O
the	O
skill	O
variables	O
.	O
let	O
us	O
assume	O
798	O
chapter	O
22.	O
more	O
variational	B
inference	I
2	O
5	O
1	O
4	O
(	O
a	O
)	O
3	O
6	O
2	O
1.5	O
1	O
0.5	O
0	O
−0.5	O
−1	O
−1.5	O
−2	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
(	O
b	O
)	O
figure	O
22.13	O
(	O
a	O
)	O
a	O
dag	O
representing	O
a	O
partial	O
ordering	O
of	O
players	O
.	O
(	O
b	O
)	O
posterior	B
mean	I
plus/minus	O
1	O
standard	B
deviation	I
for	O
the	O
latent	B
skills	O
of	O
each	O
player	O
based	O
on	O
26	O
games	O
.	O
figure	O
generated	O
by	O
trueskilldemo	O
.	O
that	O
ig	O
is	O
the	O
winner	O
,	O
and	O
jg	O
is	O
the	O
loser	O
.	O
then	O
we	O
have	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
mt	O
hg→sig	O
(	O
sig	O
)	O
=	O
,	O
sjg	O
)	O
mt	O
dg→hg	O
(	O
dg	O
)	O
mt	O
sjg→hg	O
(	O
sjg	O
)	O
ddgdsjg	O
vt	O
hg→sig	O
μt	O
hg→sig	O
and	O
similarly	O
hg	O
(	O
dg	O
,	O
sig	O
|μt	O
hg→sig	O
dg→hg	O
+	O
vt	O
)	O
,	O
vt	O
hg→sig	O
sjg→hg	O
dg→hg	O
+	O
μt	O
sjg→hg	O
=	O
n	O
(	O
sig	O
=	O
1	O
+	O
vt	O
=	O
μt	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
22.136	O
)	O
(	O
22.137	O
)	O
(	O
22.138	O
)	O
(	O
22.139	O
)	O
(	O
22.140	O
)	O
(	O
22.141	O
)	O
(	O
22.142	O
)	O
(	O
22.143	O
)	O
mt	O
hg→sjg	O
(	O
sjg	O
)	O
=	O
,	O
sjg	O
)	O
mt	O
dg→hg	O
(	O
dg	O
)	O
mt	O
sig→hg	O
(	O
sig	O
)	O
ddgdsig	O
=	O
n	O
(	O
sjg	O
=	O
1	O
+	O
vt	O
=	O
μt	O
dg→hg	O
hg	O
(	O
dg	O
,	O
sig	O
|μt	O
hg→sjg	O
dg→hg	O
+	O
vt	O
−	O
μt	O
sig→hg	O
,	O
vt	O
hg→sjg	O
)	O
sig→hg	O
vt	O
hg→sjg	O
μt	O
hg→sjg	O
when	O
we	O
compute	O
qt+1	O
(	O
sig	O
)	O
at	O
the	O
next	O
iteration	O
,	O
by	O
combining	O
mt	O
(	O
sig	O
)	O
with	O
the	O
prior	O
factor	B
,	O
we	O
will	O
see	O
that	O
the	O
posterior	B
mean	I
of	O
sig	O
goes	O
up	O
.	O
similarly	O
,	O
the	O
posterior	B
mean	I
of	O
sjg	O
goes	O
down	O
.	O
hg→sig	O
it	O
is	O
straightforward	O
to	O
combine	O
ep	O
with	O
adf	O
to	O
perform	O
online	O
inference	O
,	O
which	O
is	O
necessary	O
for	O
most	O
practical	O
applications	O
.	O
let	O
us	O
consider	O
a	O
simple	O
example	O
of	O
this	O
method	O
.	O
we	O
create	O
a	O
partial	O
ordering	O
of	O
5	O
players	O
as	O
shown	O
in	O
figure	O
22.13	O
(	O
a	O
)	O
.	O
we	O
then	O
sample	O
some	O
game	O
outcomes	O
from	O
this	O
graph	B
,	O
where	O
a	O
22.6.	O
map	O
state	B
estimation	I
799	O
parent	O
always	O
beats	O
a	O
child	O
.	O
we	O
pass	O
this	O
data	O
into	O
(	O
5	O
iterations	O
of	O
)	O
the	O
ep	O
algorithm	O
and	O
infer	O
the	O
posterior	B
mean	I
and	O
variance	B
for	O
each	O
player	O
’	O
s	O
skill	O
level	O
.	O
the	O
results	O
are	O
shown	O
in	O
figure	O
22.13	O
(	O
b	O
)	O
.	O
we	O
see	O
that	O
the	O
method	O
has	O
correctly	O
inferred	O
the	O
rank	O
ordering	O
of	O
the	O
players	O
.	O
22.5.6	O
other	O
applications	O
of	O
ep	O
the	O
trueskill	O
model	O
was	O
developed	O
by	O
researchers	O
at	O
microsoft	O
.	O
they	O
and	O
others	O
have	O
extended	O
the	O
model	O
to	O
a	O
variety	O
of	O
other	O
interesting	O
applications	O
,	O
including	O
personalized	O
ad	O
recommenda-	O
tion	O
(	O
stern	O
et	O
al	O
.	O
2009	O
)	O
,	O
predicting	O
click-through-rate	O
on	O
ads	O
in	O
the	O
bing	O
search	O
engine	O
(	O
graepel	O
et	O
al	O
.	O
2010	O
)	O
,	O
etc	O
.	O
they	O
have	O
also	O
developed	O
a	O
general	O
purpose	O
bayesian	O
inference	B
toolbox	O
based	O
on	O
ep	O
called	O
infer.net	B
(	O
minka	O
et	O
al	O
.	O
2010	O
)	O
.	O
ep	O
has	O
also	O
been	O
used	O
for	O
a	O
variety	O
of	O
other	O
models	O
,	O
such	O
as	O
gaussian	O
process	O
classiﬁcation	B
(	O
nickisch	O
and	O
rasmussen	O
2008	O
)	O
.	O
see	O
http	O
:	O
//research.microsoft.com/en-us/um/people/	O
minka/papers/ep/roadmap.html	O
for	O
a	O
list	O
of	O
other	O
ep	O
applications	O
.	O
22.6	O
map	O
state	B
estimation	I
in	O
this	O
section	O
,	O
we	O
consider	O
the	O
problem	O
of	O
ﬁnding	O
the	O
most	O
probable	O
conﬁguration	O
of	O
variables	O
in	O
a	O
discrete-state	O
graphical	B
model	I
,	O
i.e.	O
,	O
our	O
goal	O
is	O
to	O
ﬁnd	O
a	O
map	O
assignment	O
of	O
the	O
following	O
form	O
:	O
x∗	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
θi	O
(	O
xi	O
)	O
+	O
p	O
(	O
x|θ	O
)	O
=	O
arg	O
max	O
x∈x	O
m	O
θf	O
(	O
xf	O
)	O
=	O
arg	O
max	O
x∈x	O
m	O
θt	O
φ	O
(	O
x	O
)	O
(	O
22.144	O
)	O
=	O
arg	O
max	O
x∈x	O
m	O
i∈v	O
f∈f	O
where	O
θi	O
are	O
the	O
singleton	O
node	O
potentials	O
,	O
and	O
θf	O
are	O
the	O
factor	B
potentials	O
.	O
(	O
in	O
this	O
section	O
,	O
we	O
follow	O
the	O
notation	O
of	O
(	O
sontag	O
et	O
al	O
.	O
2011	O
)	O
,	O
which	O
considers	O
the	O
case	O
of	O
general	O
potentials	O
,	O
not	O
just	O
pairwise	O
ones	O
.	O
)	O
note	O
that	O
the	O
partition	B
function	I
z	O
(	O
θ	O
)	O
plays	O
no	O
role	O
in	O
map	O
estimation	O
.	O
if	O
the	O
treewidth	B
is	O
low	O
,	O
we	O
can	O
solve	O
this	O
problem	O
with	O
the	O
junction	B
tree	I
algorithm	I
(	O
sec-	O
tion	O
20.4	O
)	O
,	O
but	O
in	O
general	O
this	O
problem	O
is	O
intractable	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
various	O
approxi-	O
mations	O
,	O
building	O
on	O
the	O
material	O
from	O
section	O
22.3	O
.	O
22.6.1	O
linear	O
programming	O
relaxation	O
we	O
can	O
rewrite	O
the	O
objective	O
in	O
terms	O
of	O
the	O
variational	O
parameters	O
as	O
follows	O
:	O
θt	O
μ	O
arg	O
max	O
x∈x	O
m	O
θt	O
φ	O
(	O
x	O
)	O
=	O
arg	O
max	O
μ∈m	O
(	O
g	O
)	O
(	O
22.145	O
)	O
where	O
φ	O
(	O
x	O
)	O
=	O
[	O
{	O
i	O
(	O
xs	O
=	O
j	O
)	O
}	O
,	O
{	O
i	O
(	O
xf	O
=	O
k	O
)	O
}	O
)	O
and	O
μ	O
is	O
a	O
probability	O
vector	O
in	O
the	O
marginal	B
polytope	I
.	O
to	O
see	O
why	O
this	O
equation	O
is	O
true	O
,	O
note	O
that	O
we	O
can	O
just	O
set	O
μ	O
to	O
be	O
a	O
degenerate	B
distribution	O
with	O
μ	O
(	O
xs	O
)	O
=	O
i	O
(	O
xs	O
=	O
x∗	O
s	O
is	O
the	O
optimal	O
assigment	O
of	O
node	O
s.	O
so	O
instead	O
of	O
optimizing	O
over	O
discrete	B
assignments	O
,	O
we	O
now	O
optimize	O
over	O
probability	O
distributions	O
μ.	O
s	O
)	O
,	O
where	O
x∗	O
it	O
seems	O
like	O
we	O
have	O
an	O
easy	O
problem	O
to	O
solve	O
,	O
since	O
the	O
objective	O
in	O
equation	O
22.145	O
is	O
linear	O
in	O
μ	O
,	O
and	O
the	O
constraint	O
set	O
m	O
(	O
g	O
)	O
is	O
convex	B
.	O
the	O
trouble	O
is	O
,	O
m	O
(	O
g	O
)	O
in	O
general	O
has	O
a	O
number	O
of	O
facets	O
that	O
is	O
exponential	O
in	O
the	O
number	O
of	O
nodes	B
.	O
a	O
standard	O
strategy	O
in	O
combinatorial	O
optimization	B
is	O
to	O
relax	O
the	O
constraints	O
.	O
in	O
this	O
case	O
,	O
instead	O
of	O
requiring	O
probability	O
vector	O
μ	O
to	O
live	O
in	O
the	O
marginal	B
polytope	I
m	O
(	O
g	O
)	O
,	O
we	O
allow	O
it	O
to	O
800	O
chapter	O
22.	O
more	O
variational	B
inference	I
live	O
inside	O
a	O
convex	B
outer	O
bound	O
l	O
(	O
g	O
)	O
.	O
having	O
deﬁned	O
this	O
relaxed	O
constraint	O
set	O
,	O
we	O
have	O
max	O
x∈x	O
m	O
θt	O
φ	O
(	O
x	O
)	O
=	O
max	O
μ∈m	O
(	O
g	O
)	O
θt	O
μ	O
≤	O
max	O
τ∈l	O
(	O
g	O
)	O
θt	O
τ	O
(	O
22.146	O
)	O
if	O
the	O
solution	O
is	O
integral	O
,	O
it	O
is	O
exact	O
;	O
if	O
it	O
is	O
fractional	O
,	O
it	O
is	O
an	O
approximation	O
.	O
this	O
is	O
called	O
a	O
(	O
ﬁrst	O
order	O
)	O
linear	B
programming	I
relaxtion	I
.	O
the	O
reason	O
it	O
is	O
called	O
ﬁrst-order	O
is	O
that	O
the	O
constraints	O
that	O
are	O
enforced	O
are	O
those	O
that	O
correspond	O
to	O
consistency	O
on	O
a	O
tree	B
,	O
which	O
is	O
a	O
graph	B
of	O
treewidth	B
1.	O
it	O
is	O
possible	O
to	O
enforce	O
higher-order	O
consistency	O
,	O
using	O
graphs	O
with	O
larger	O
treewidth	B
(	O
see	O
(	O
wainwright	O
and	O
jordan	O
2008b	O
,	O
sec	O
8.5	O
)	O
for	O
details	O
)	O
.	O
how	O
should	O
we	O
actually	O
perform	O
the	O
optimization	B
?	O
we	O
can	O
use	O
a	O
generic	O
linear	O
programming	O
package	O
,	O
but	O
this	O
is	O
often	O
very	O
slow	O
.	O
fortunately	O
,	O
in	O
the	O
case	O
of	O
graphical	O
models	O
,	O
it	O
is	O
possible	O
to	O
devise	O
specialised	O
distributed	O
message	O
passing	O
algorithms	O
for	O
solving	O
this	O
optimization	B
problem	O
,	O
as	O
we	O
explain	O
below	O
.	O
22.6.2	O
max-product	B
belief	I
propagation	I
the	O
map	O
objective	O
in	O
equation	O
22.145	O
,	O
maxμ∈m	O
(	O
g	O
)	O
θt	O
μ	O
,	O
is	O
almost	O
identical	O
to	O
the	O
inference	B
objective	O
in	O
equation	O
22.23	O
,	O
maxμ∈m	O
(	O
g	O
)	O
θt	O
μ	O
+	O
h	O
(	O
μ	O
)	O
,	O
apart	O
from	O
the	O
entropy	B
term	O
.	O
one	O
heuristic	O
way	O
to	O
proceed	O
would	O
be	O
to	O
consider	O
the	O
zero	B
temperature	I
limit	I
of	O
the	O
probability	O
distribution	O
μ	O
,	O
where	O
the	O
probability	O
distribution	O
has	O
all	O
its	O
mass	O
centered	O
on	O
its	O
mode	B
(	O
see	O
section	O
4.2.2	O
)	O
.	O
in	O
such	O
a	O
setting	O
,	O
the	O
entropy	B
term	O
becomes	O
zero	O
.	O
we	O
can	O
then	O
modify	O
the	O
message	B
passing	I
methods	O
used	O
to	O
solve	O
the	O
inference	B
problem	O
so	O
that	O
they	O
solve	O
the	O
map	O
estimation	O
problem	O
instead	O
.	O
in	O
the	O
zero	B
temperature	I
limit	I
,	O
the	O
sum	O
operator	O
becomes	O
the	O
max	O
operator	O
,	O
which	O
results	O
in	O
a	O
method	O
called	O
max-product	B
belief	I
propagation	I
.	O
in	O
particular	O
,	O
in	O
more	O
detail	O
,	O
let	O
a	O
(	O
θ	O
)	O
(	O
cid:2	O
)	O
max	O
μ∈m	O
(	O
g	O
)	O
θt	O
μ	O
+	O
h	O
(	O
μ	O
)	O
(	O
1	O
β	O
(	O
cid:19	O
)	O
max	O
μ∈m	O
(	O
g	O
)	O
)	O
%	O
now	O
consider	O
an	O
inverse	O
temperature	O
β	O
going	O
to	O
inﬁnity	O
.	O
we	O
have	O
lim	O
β→+∞	O
a	O
(	O
βθ	O
)	O
β	O
=	O
lim	O
β→+∞	O
(	O
βθ	O
)	O
t	O
μ	O
+	O
h	O
(	O
μ	O
)	O
=	O
max	O
μ∈m	O
(	O
g	O
)	O
θt	O
μ	O
+	O
lim	O
β→+∞	O
1	O
β	O
h	O
(	O
μ	O
)	O
=	O
max	O
μ∈m	O
(	O
g	O
)	O
θt	O
μ	O
(	O
22.147	O
)	O
(	O
22.148	O
)	O
(	O
22.149	O
)	O
(	O
22.150	O
)	O
it	O
is	O
the	O
concavity	O
of	O
the	O
objective	O
function	O
that	O
allows	O
us	O
to	O
interchange	O
the	O
lim	O
and	O
max	O
operators	O
(	O
see	O
(	O
wainwright	O
and	O
jordan	O
2008b	O
,	O
p274	O
)	O
for	O
details	O
)	O
.	O
now	O
consider	O
the	O
bethe	O
approximation	O
,	O
which	O
has	O
the	O
form	O
maxτ∈l	O
(	O
g	O
)	O
θt	O
τ	O
+	O
hbethe	O
(	O
τ	O
)	O
.	O
we	O
showed	O
that	O
loopy	O
bp	O
ﬁnds	O
a	O
local	O
optimum	O
of	O
this	O
objective	O
.	O
in	O
the	O
zero	B
temperature	I
limit	I
,	O
this	O
objective	O
is	O
equivalent	O
to	O
the	O
lp	O
relaxation	O
of	O
the	O
map	O
problem	O
.	O
unfortunately	O
,	O
max-product	B
loopy	O
bp	O
does	O
not	O
solve	O
this	O
lp	O
relaxation	O
unless	O
the	O
graph	B
is	O
a	O
tree	B
(	O
wainwright	O
and	O
jordan	O
2008b	O
,	O
p211	O
)	O
.	O
the	O
reason	O
is	O
that	O
bethe	O
energy	B
functional	I
is	O
not	O
concave	O
(	O
except	O
on	O
trees	O
)	O
,	O
so	O
we	O
are	O
not	O
licensed	O
to	O
swap	O
the	O
limit	O
and	O
max	O
operators	O
in	O
the	O
above	O
zero-temperature	O
derivation	O
.	O
however	O
,	O
if	O
we	O
use	O
tree-reweighted	O
bp	O
,	O
or	O
trbp/	O
trw	O
,	O
we	O
have	O
a	O
concave	B
objective	O
.	O
in	O
this	O
case	O
,	O
22.6.	O
map	O
state	B
estimation	I
801	O
one	O
can	O
show	O
(	O
kolmogorov	O
and	O
wainwright	O
2005	O
)	O
that	O
the	O
max-product	B
version	O
of	O
trbp	O
does	O
solve	O
the	O
above	O
lp	O
relaxation	O
.	O
a	O
certain	O
scheduling	O
of	O
this	O
algorithm	O
,	O
known	O
as	O
sequential	B
trbp	O
,	O
trbp-s	O
,	O
ortrw-s	O
,	O
can	O
be	O
shown	O
to	O
always	O
converge	B
(	O
kolmogorov	O
2006	O
)	O
,	O
and	O
furthermore	O
,	O
it	O
typically	O
does	O
so	O
faster	O
than	O
the	O
standard	O
parallel	O
updates	O
.	O
the	O
idea	O
is	O
to	O
pick	O
an	O
arbitrary	O
node	O
ordering	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
we	O
then	O
consider	O
a	O
set	O
of	O
trees	O
which	O
is	O
a	O
subsequence	O
of	O
this	O
ordering	O
.	O
at	O
each	O
iteration	O
,	O
we	O
perform	O
max-product	B
bp	O
from	O
x1	O
towards	O
xn	O
and	O
back	O
along	O
one	O
of	O
these	O
trees	O
.	O
it	O
can	O
be	O
shown	O
that	O
this	O
monotonically	O
minimizes	O
a	O
lower	O
bound	O
on	O
the	O
energy	O
,	O
and	O
thus	O
is	O
guaranteed	O
to	O
converge	B
to	O
the	O
global	O
optimum	O
of	O
the	O
lp	O
relaxation	O
.	O
22.6.3	O
graphcuts	B
in	O
this	O
section	O
,	O
we	O
show	O
how	O
to	O
ﬁnd	O
map	O
state	B
estimates	O
,	O
or	O
equivalently	O
,	O
minimum	O
energy	O
conﬁgurations	O
,	O
by	O
using	O
the	O
max	B
ﬂow/min	I
cut	I
algorithm	O
for	O
graphs.8	O
this	O
class	O
of	O
methods	O
is	O
known	O
as	O
graphcuts	B
and	O
is	O
very	O
widely	O
used	O
,	O
especially	O
in	O
computer	O
vision	O
applications	O
.	O
we	O
will	O
start	O
by	O
considering	O
the	O
case	O
of	O
mrfs	O
with	O
binary	O
nodes	O
and	O
a	O
restricted	O
class	O
of	O
potentials	O
;	O
in	O
this	O
case	O
,	O
graphcuts	B
will	O
ﬁnd	O
the	O
exact	O
global	O
optimum	O
.	O
we	O
then	O
consider	O
the	O
case	O
of	O
multiple	O
states	O
per	O
node	O
,	O
which	O
are	O
assumed	O
to	O
have	O
some	O
underlying	O
ordering	O
;	O
we	O
can	O
approximately	O
solve	O
this	O
case	O
by	O
solving	O
a	O
series	O
of	O
binary	O
subproblems	O
,	O
as	O
we	O
will	O
see	O
.	O
22.6.3.1	O
graphcuts	B
for	O
the	O
generalized	O
ising	O
model	O
(	O
cid:19	O
)	O
0	O
λst	O
if	O
xu	O
=	O
xv	O
if	O
xu	O
(	O
cid:8	O
)	O
=	O
xv	O
let	O
us	O
start	O
by	O
considering	O
a	O
binary	O
mrf	O
where	O
the	O
edge	O
energies	O
have	O
the	O
following	O
form	O
:	O
euv	O
(	O
xu	O
,	O
xv	O
)	O
=	O
(	O
22.151	O
)	O
where	O
λst	O
≥	O
0	O
is	O
the	O
edge	O
cost	O
.	O
this	O
encourages	O
neighboring	O
nodes	B
to	O
have	O
the	O
same	O
value	O
(	O
since	O
we	O
are	O
trying	O
to	O
minimize	O
energy	O
)	O
.	O
since	O
we	O
are	O
free	O
to	O
add	O
any	O
constant	O
we	O
like	O
to	O
the	O
overall	O
energy	O
without	O
affecting	O
the	O
map	O
state	B
estimate	O
,	O
let	O
us	O
rescale	O
the	O
local	O
energy	O
terms	O
such	O
that	O
either	O
eu	O
(	O
1	O
)	O
=	O
0	O
or	O
eu	O
(	O
0	O
)	O
=	O
0.	O
now	O
let	O
us	O
construct	O
a	O
graph	B
which	O
has	O
the	O
same	O
set	O
of	O
nodes	B
as	O
the	O
mrf	O
,	O
plus	O
two	O
distin-	O
guished	O
nodes	B
:	O
the	O
source	O
s	O
and	O
the	O
sinkt	O
.	O
if	O
eu	O
(	O
1	O
)	O
=	O
0	O
,	O
we	O
add	O
the	O
edge	O
xu	O
→	O
t	O
with	O
cost	O
(	O
this	O
ensures	O
that	O
if	O
u	O
is	O
not	O
in	O
partition	O
xt	O
,	O
meaning	O
u	O
is	O
assigned	O
to	O
state	B
0	O
,	O
we	O
will	O
eu	O
(	O
0	O
)	O
.	O
pay	O
a	O
cost	O
of	O
eu	O
(	O
0	O
)	O
in	O
the	O
cut	O
.	O
)	O
similarly	O
,	O
if	O
eu	O
(	O
0	O
)	O
=	O
0	O
,	O
we	O
add	O
the	O
edge	O
xu	O
→	O
s	O
with	O
cost	O
eu	O
(	O
1	O
)	O
.	O
finally	O
,	O
for	O
every	O
pair	O
of	O
variables	O
that	O
are	O
connected	O
in	O
the	O
mrf	O
,	O
we	O
add	O
edges	B
xu	O
→	O
xv	O
and	O
xv	O
→	O
xu	O
,	O
both	O
with	O
cost	O
λu	O
,	O
v	O
≥	O
0.	O
figure	O
22.14	O
illustrates	O
this	O
construction	O
for	O
an	O
mrf	O
with	O
4	O
nodes	B
,	O
and	O
with	O
the	O
following	O
non-zero	O
energy	O
values	O
:	O
e1	O
(	O
0	O
)	O
=	O
7	O
,	O
e2	O
(	O
1	O
)	O
=	O
2	O
,	O
e3	O
(	O
1	O
)	O
=	O
1	O
,	O
e4	O
(	O
1	O
)	O
=	O
6	O
λ1,2	O
=	O
6	O
,	O
λ2,3	O
=	O
6	O
,	O
λ3,4	O
=	O
2	O
,	O
λ1,4	O
=	O
1	O
(	O
22.152	O
)	O
(	O
22.153	O
)	O
having	O
constructed	O
the	O
graph	B
,	O
we	O
compute	O
a	O
minimal	B
s	O
−	O
t	O
cut	O
.	O
this	O
is	O
a	O
partition	O
of	O
the	O
nodes	B
into	O
two	O
sets	O
,	O
xs	O
,	O
which	O
are	O
nodes	B
connected	O
to	O
s	O
,	O
and	O
xt	O
,	O
which	O
are	O
nodes	B
connected	O
to	O
t.	O
we	O
8.	O
there	O
are	O
a	O
variety	O
of	O
ways	O
to	O
implement	O
this	O
algorithm	O
,	O
see	O
e.g.	O
,	O
o	O
(	O
ev	O
log	O
v	O
)	O
or	O
o	O
(	O
v	O
3	O
)	O
time	O
,	O
where	O
e	O
is	O
the	O
number	O
of	O
edges	B
and	O
v	O
is	O
the	O
number	O
of	O
nodes	B
.	O
(	O
sedgewick	O
and	O
wayne	O
2011	O
)	O
.	O
the	O
best	O
take	O
802	O
chapter	O
22.	O
more	O
variational	B
inference	I
z1	O
1	O
z4	O
7	O
t	O
6	O
2	O
1	O
2	O
s	O
6	O
z2	O
6	O
z3	O
figure	O
22.14	O
illustration	O
of	O
graphcuts	B
applied	O
to	O
an	O
mrf	O
with	O
4	O
nodes	B
.	O
dashed	O
lines	O
are	O
ones	O
which	O
contribute	O
to	O
the	O
cost	O
of	O
the	O
cut	O
(	O
for	O
bidirected	O
edges	B
,	O
we	O
only	O
count	O
one	O
of	O
the	O
costs	O
)	O
.	O
here	O
the	O
min	O
cut	O
has	O
cost	O
6.	O
source	O
:	O
figure	O
13.5	O
from	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
.	O
pick	O
the	O
partition	O
which	O
minimizes	O
the	O
sum	O
of	O
the	O
cost	O
of	O
the	O
edges	B
between	O
nodes	B
on	O
different	O
sides	O
of	O
the	O
partition	O
:	O
(	O
cid:4	O
)	O
cost	O
(	O
xs	O
,	O
xt	O
)	O
=	O
xu∈xs	O
,	O
xv∈xt	O
cost	O
(	O
xu	O
,	O
sv	O
)	O
(	O
22.154	O
)	O
in	O
figure	O
22.14	O
,	O
we	O
see	O
that	O
the	O
min-cut	O
has	O
cost	O
6.	O
minimizing	O
the	O
cost	O
in	O
this	O
graph	B
is	O
equivalent	O
to	O
minimizing	O
the	O
energy	O
in	O
the	O
mrf	O
.	O
hence	O
nodes	O
that	O
are	O
assigned	O
to	O
s	O
have	O
an	O
optimal	O
state	O
of	O
0	O
,	O
and	O
the	O
nodes	O
that	O
are	O
assigned	O
to	O
t	O
have	O
an	O
optimal	O
state	O
of	O
1.	O
in	O
figure	O
22.14	O
,	O
we	O
see	O
that	O
the	O
optimal	O
map	O
estimate	O
is	O
(	O
1	O
,	O
1	O
,	O
1	O
,	O
0	O
)	O
.	O
22.6.3.2	O
graphcuts	B
for	O
binary	O
mrfs	O
with	O
submodular	B
potentials	O
we	O
now	O
discuss	O
how	O
to	O
extend	O
the	O
graphcuts	B
construction	O
to	O
binary	O
mrfs	O
with	O
more	O
general	O
kinds	O
of	O
potential	O
functions	O
.	O
in	O
particular	O
,	O
suppose	O
each	O
pairwise	O
energy	O
satisﬁes	O
the	O
following	O
condition	O
:	O
euv	O
(	O
1	O
,	O
1	O
)	O
+	O
euv	O
(	O
0	O
,	O
0	O
)	O
≤	O
euv	O
(	O
1	O
,	O
0	O
)	O
+	O
euv	O
(	O
0	O
,	O
1	O
)	O
(	O
22.155	O
)	O
in	O
other	O
words	O
,	O
the	O
sum	O
of	O
the	O
diagonal	B
energies	O
is	O
less	O
than	O
the	O
sum	O
of	O
the	O
off-diagonal	O
energies	O
.	O
in	O
this	O
case	O
,	O
we	O
say	O
the	O
energies	O
are	O
submodular	B
(	O
kolmogorov	O
and	O
zabin	O
2004	O
)	O
.9	O
an	O
example	O
of	O
a	O
submodular	B
energy	O
is	O
an	O
ising	O
model	O
where	O
λuv	O
>	O
0.	O
this	O
is	O
also	O
known	O
as	O
an	O
attractive	O
mrf	O
or	O
associative	B
mrf	O
,	O
since	O
the	O
model	O
“	O
wants	O
”	O
neighboring	O
states	O
to	O
be	O
the	O
same	O
.	O
9.	O
submodularity	O
is	O
the	O
discrete	B
analog	O
of	O
convexity	O
.	O
intuitively	O
,	O
it	O
corresponds	O
to	O
the	O
“	O
law	O
of	O
diminishing	O
returns	O
”	O
,	O
that	O
is	O
,	O
the	O
extra	O
value	O
of	O
adding	O
one	O
more	O
element	O
to	O
a	O
set	O
is	O
reduced	O
if	O
the	O
set	O
is	O
already	O
large	O
.	O
more	O
formally	O
,	O
we	O
say	O
that	O
f	O
:	O
2s	O
→	O
r	O
is	O
submodular	B
if	O
for	O
any	O
a	O
⊂	O
b	O
⊂	O
s	O
and	O
x	O
∈	O
s	O
,	O
we	O
havef	O
(	O
a	O
∪	O
{	O
x	O
}	O
)	O
−	O
f	O
(	O
a	O
)	O
≥	O
f	O
(	O
b	O
∪	O
{	O
x	O
}	O
)	O
−	O
f	O
(	O
b	O
)	O
.	O
if	O
−f	O
is	O
submodular	B
,	O
then	O
f	O
is	O
supermodular	B
.	O
22.6.	O
map	O
state	B
estimation	I
803	O
to	O
apply	O
graphcuts	B
to	O
a	O
binary	O
mrf	O
with	O
submodular	B
potentials	O
,	O
we	O
construct	O
the	O
pairwise	O
edge	O
weights	O
as	O
follows	O
:	O
u	O
,	O
v	O
(	O
0	O
,	O
1	O
)	O
=	O
eu	O
,	O
v	O
(	O
1	O
,	O
0	O
)	O
+	O
eu	O
,	O
v	O
(	O
0	O
,	O
1	O
)	O
−	O
eu	O
,	O
v	O
(	O
0	O
,	O
0	O
)	O
−	O
eu	O
,	O
v	O
(	O
1	O
,	O
1	O
)	O
e	O
(	O
cid:2	O
)	O
(	O
22.156	O
)	O
this	O
is	O
guaranteed	O
to	O
be	O
non-negative	O
by	O
virtue	O
of	O
the	O
submodularity	O
assumption	O
.	O
in	O
addition	O
,	O
we	O
construct	O
new	O
local	O
edge	O
weights	O
as	O
follows	O
:	O
ﬁrst	O
we	O
initialize	O
e	O
(	O
cid:2	O
)	O
(	O
u	O
)	O
=	O
e	O
(	O
u	O
)	O
,	O
and	O
then	O
for	O
each	O
edge	O
pair	O
(	O
u	O
,	O
v	O
)	O
,	O
we	O
update	O
these	O
values	O
as	O
follows	O
:	O
u	O
(	O
1	O
)	O
+	O
(	O
eu	O
,	O
v	O
(	O
1	O
,	O
0	O
)	O
−	O
eu	O
,	O
v	O
(	O
0	O
,	O
0	O
)	O
)	O
u	O
(	O
1	O
)	O
+	O
(	O
eu	O
,	O
v	O
(	O
1	O
,	O
1	O
)	O
−	O
eu	O
,	O
v	O
(	O
1	O
,	O
0	O
)	O
)	O
e	O
(	O
cid:2	O
)	O
u	O
(	O
1	O
)	O
=	O
e	O
(	O
cid:2	O
)	O
e	O
(	O
cid:2	O
)	O
v	O
(	O
1	O
)	O
=	O
e	O
(	O
cid:2	O
)	O
we	O
now	O
construct	O
a	O
graph	B
in	O
a	O
similar	B
way	O
to	O
before	O
.	O
speciﬁcally	O
,	O
if	O
e	O
(	O
cid:2	O
)	O
add	O
the	O
edge	O
u	O
→	O
s	O
with	O
cost	O
e	O
(	O
cid:2	O
)	O
u	O
(	O
0	O
)	O
−	O
e	O
(	O
cid:2	O
)	O
e	O
(	O
cid:2	O
)	O
xu	O
−	O
xv	O
with	O
cost	O
e	O
(	O
cid:2	O
)	O
u	O
(	O
1	O
)	O
.	O
finally	O
for	O
every	O
mrf	O
edge	O
for	O
which	O
e	O
(	O
cid:2	O
)	O
u	O
,	O
v	O
(	O
0	O
,	O
1	O
)	O
.	O
(	O
we	O
don	O
’	O
t	O
need	O
to	O
add	O
the	O
edge	O
in	O
both	O
directions	O
.	O
)	O
u	O
(	O
1	O
)	O
>	O
e	O
(	O
cid:2	O
)	O
(	O
22.157	O
)	O
(	O
22.158	O
)	O
u	O
(	O
1	O
)	O
−	O
e	O
(	O
cid:2	O
)	O
u	O
(	O
0	O
)	O
,	O
we	O
u	O
(	O
0	O
)	O
,	O
otherwise	O
we	O
add	O
the	O
edge	O
u	O
→	O
t	O
with	O
cost	O
u	O
,	O
v	O
(	O
0	O
,	O
1	O
)	O
>	O
0	O
,	O
we	O
add	O
a	O
graphcuts	B
edge	O
one	O
can	O
show	O
(	O
exercise	O
22.1	O
)	O
that	O
the	O
min	O
cut	O
in	O
this	O
graph	B
is	O
the	O
same	O
as	O
the	O
minimum	O
energy	O
conﬁguration	O
.	O
thus	O
we	O
can	O
use	O
max	B
ﬂow/min	I
cut	I
to	O
ﬁnd	O
the	O
globally	O
optimal	O
map	O
estimate	O
(	O
greig	O
et	O
al	O
.	O
1989	O
)	O
.	O
22.6.3.3	O
graphcuts	B
for	O
nonbinary	O
metric	B
mrfs	O
we	O
now	O
discuss	O
how	O
to	O
use	O
graphcuts	B
for	O
approximate	O
map	O
estimation	O
in	O
mrfs	O
where	O
each	O
node	O
can	O
have	O
multiple	O
states	O
(	O
boykov	O
et	O
al	O
.	O
2001	O
)	O
.	O
however	O
,	O
we	O
require	O
that	O
the	O
pairwise	O
energies	O
form	O
a	O
metric	B
.	O
we	O
call	O
such	O
a	O
model	O
a	O
metric	B
mrf	O
.	O
for	O
example	O
,	O
suppose	O
the	O
states	O
have	O
a	O
natural	O
ordering	O
,	O
as	O
commonly	O
arises	O
if	O
they	O
are	O
a	O
discretization	O
of	O
an	O
underlying	O
continuous	O
in	O
this	O
case	O
,	O
we	O
can	O
deﬁne	O
a	O
metric	B
of	O
the	O
form	O
e	O
(	O
xs	O
,	O
xt	O
)	O
=	O
min	O
(	O
δ	O
,	O
||xs	O
−	O
xt||	O
)	O
or	O
a	O
space	O
.	O
semi-metric	B
of	O
the	O
form	O
e	O
(	O
xs	O
,	O
xt	O
)	O
=	O
min	O
(	O
δ	O
,	O
(	O
xs	O
−	O
xt	O
)	O
2	O
)	O
,	O
for	O
some	O
constant	O
δ	O
>	O
0.	O
this	O
energy	O
encourages	O
neighbors	B
to	O
have	O
similar	B
labels	O
,	O
but	O
never	O
“	O
punishes	O
”	O
them	O
by	O
more	O
than	O
δ.	O
this	O
δ	O
term	O
prevents	O
over-smoothing	O
,	O
which	O
we	O
illustrate	O
in	O
figure	O
19.20.	O
one	O
version	O
of	O
graphcuts	B
is	O
the	O
alpha	B
expansion	I
.	O
at	O
each	O
step	O
,	O
it	O
picks	O
one	O
of	O
the	O
available	O
labels	O
or	O
states	O
and	O
calls	O
it	O
α	O
;	O
then	O
it	O
solves	O
a	O
binary	O
subproblem	O
where	O
each	O
variable	O
can	O
choose	O
to	O
remain	O
in	O
its	O
current	O
state	B
,	O
or	O
to	O
become	O
state	B
α	O
(	O
see	O
figure	O
22.15	O
(	O
d	O
)	O
for	O
an	O
illustration	O
)	O
.	O
more	O
precisely	O
,	O
we	O
deﬁne	O
a	O
new	O
mrf	O
on	O
binary	O
nodes	O
,	O
and	O
we	O
deﬁne	O
the	O
energies	O
of	O
this	O
new	O
model	O
,	O
relative	O
to	O
the	O
current	O
assignment	O
x	O
,	O
as	O
follows	O
:	O
e	O
(	O
cid:2	O
)	O
u	O
,	O
v	O
(	O
0	O
,	O
1	O
)	O
=	O
eu	O
,	O
v	O
(	O
xu	O
,	O
α	O
)	O
,	O
e	O
(	O
cid:2	O
)	O
e	O
(	O
cid:2	O
)	O
u	O
(	O
0	O
)	O
=	O
eu	O
(	O
xu	O
)	O
,	O
e	O
(	O
cid:2	O
)	O
u	O
(	O
1	O
)	O
=	O
eu	O
(	O
α	O
)	O
,	O
e	O
(	O
cid:2	O
)	O
u	O
,	O
v	O
(	O
1	O
,	O
0	O
)	O
=	O
eu	O
,	O
v	O
(	O
α	O
,	O
xv	O
)	O
,	O
e	O
(	O
cid:2	O
)	O
u	O
,	O
v	O
(	O
0	O
,	O
0	O
)	O
=	O
eu	O
,	O
v	O
(	O
xu	O
,	O
xv	O
)	O
u	O
,	O
v	O
(	O
1	O
,	O
1	O
)	O
=	O
eu	O
,	O
v	O
(	O
α	O
,	O
α	O
)	O
(	O
22.159	O
)	O
(	O
22.160	O
)	O
to	O
optimize	O
e	O
(	O
cid:2	O
)	O
using	O
graph	B
cuts	I
(	O
and	O
thus	O
ﬁgure	O
out	O
the	O
optimal	O
alpha	O
expansion	O
move	O
)	O
,	O
we	O
require	O
that	O
the	O
energies	O
be	O
submodular	B
.	O
plugging	O
in	O
the	O
deﬁnition	O
we	O
get	O
the	O
following	O
constraint	O
:	O
eu	O
,	O
v	O
(	O
xu	O
,	O
xv	O
)	O
+e	O
u	O
,	O
v	O
(	O
α	O
,	O
α	O
)	O
≤	O
eu	O
,	O
v	O
(	O
xu	O
,	O
α	O
)	O
+e	O
u	O
,	O
v	O
(	O
α	O
,	O
xv	O
)	O
(	O
22.161	O
)	O
for	O
any	O
distance	O
function	O
,	O
eu	O
,	O
v	O
(	O
α	O
,	O
α	O
)	O
=	O
0	O
,	O
and	O
the	O
remaining	O
inequality	O
follows	O
from	O
the	O
triangle	B
inequality	I
.	O
thus	O
we	O
can	O
apply	O
the	O
alpha	B
expansion	I
move	O
to	O
any	O
metric	B
mrf	O
.	O
804	O
chapter	O
22.	O
more	O
variational	B
inference	I
(	O
a	O
)	O
initial	O
labeling	O
(	O
b	O
)	O
standard	O
move	O
(	O
c	O
)	O
α-β-swap	O
(	O
d	O
)	O
α-expansion	O
(	O
a	O
)	O
an	O
image	O
with	O
3	O
labels	O
.	O
figure	O
22.15	O
just	O
ﬂips	O
the	O
label	B
of	O
one	O
pixel	O
.	O
be	O
relabeled	O
as	O
β	O
if	O
this	O
decreases	O
the	O
energy	O
.	O
labeled	O
as	O
α	O
to	O
be	O
relabeled	O
as	O
α	O
if	O
this	O
decreases	O
the	O
energy	O
.	O
used	O
with	O
kind	O
permission	O
of	O
ramin	O
zabih	O
.	O
(	O
b	O
)	O
a	O
standard	O
local	O
move	O
(	O
e.g.	O
,	O
by	O
iterative	B
conditional	I
modes	I
)	O
(	O
c	O
)	O
an	O
α	O
−	O
β	O
swap	O
allows	O
all	O
nodes	O
that	O
are	O
currently	O
labeled	O
as	O
α	O
to	O
(	O
d	O
)	O
an	O
α	O
expansion	O
allows	O
all	O
nodes	O
that	O
are	O
not	O
currently	O
source	O
:	O
figure	O
2	O
of	O
(	O
boykov	O
et	O
al	O
.	O
2001	O
)	O
.	O
at	O
each	O
step	O
of	O
alpha	B
expansion	I
,	O
we	O
ﬁnd	O
the	O
optimal	O
move	O
from	O
amongst	O
an	O
exponentially	O
large	O
set	O
;	O
thus	O
we	O
reach	O
a	O
strong	B
local	I
optimum	I
,	O
of	O
much	O
lower	O
energy	O
than	O
the	O
local	O
optima	O
found	O
by	O
standard	O
greedy	O
label	B
ﬂipping	O
methods	O
such	O
as	O
iterative	B
conditional	I
modes	I
.	O
in	O
fact	O
,	O
one	O
can	O
show	O
that	O
,	O
once	O
the	O
algorithm	O
has	O
converged	O
,	O
the	O
energy	O
of	O
the	O
resulting	O
solution	O
is	O
at	O
most	O
2c	O
times	O
the	O
optimal	O
energy	O
,	O
where	O
c	O
=	O
max	O
(	O
u	O
,	O
v	O
)	O
∈e	O
maxα	O
(	O
cid:5	O
)	O
=β	O
euv	O
(	O
α	O
,	O
β	O
)	O
minα	O
(	O
cid:5	O
)	O
=β	O
euv	O
(	O
α	O
,	O
β	O
)	O
see	O
exercise	O
22.3	O
for	O
the	O
proof	O
.	O
approximation	O
.	O
in	O
the	O
case	O
of	O
the	O
potts	O
model	O
,	O
c	O
=	O
1	O
,	O
so	O
we	O
have	O
a	O
2-	O
(	O
22.162	O
)	O
another	O
version	O
of	O
graphcuts	B
is	O
the	O
alpha-beta	B
swap	I
.	O
at	O
each	O
step	O
,	O
two	O
labels	O
are	O
chosen	O
,	O
call	O
them	O
α	O
and	O
β.	O
all	O
the	O
nodes	B
currently	O
labeled	O
α	O
can	O
change	O
to	O
β	O
(	O
and	O
vice	O
versa	O
)	O
if	O
this	O
reduces	O
the	O
energy	O
(	O
see	O
figure	O
22.15	O
(	O
c	O
)	O
for	O
an	O
illustration	O
)	O
.	O
the	O
resulting	O
binary	O
subproblem	O
can	O
be	O
solved	O
exactly	O
,	O
even	O
if	O
the	O
energies	O
are	O
only	O
semi-metric	B
(	O
that	O
is	O
,	O
the	O
triangle	B
inequality	I
need	O
not	O
hold	O
;	O
see	O
exercise	O
22.2	O
)	O
.	O
although	O
the	O
α	O
−	O
β	O
swap	O
version	O
can	O
be	O
applied	O
to	O
a	O
broader	O
class	O
of	O
models	O
than	O
the	O
α-expansion	O
version	O
,	O
it	O
is	O
theoretically	O
not	O
as	O
powerful	O
.	O
indeed	O
,	O
in	O
various	O
low-level	O
vision	O
problems	O
,	O
(	O
szeliski	O
et	O
al	O
.	O
2008	O
)	O
show	O
empirically	O
that	O
the	O
expansion	O
version	O
is	O
usually	O
better	O
than	O
the	O
swap	O
version	O
(	O
see	O
section	O
22.6.4	O
)	O
.	O
22.6.4	O
experimental	O
comparison	O
of	O
graphcuts	B
and	O
bp	O
in	O
section	O
19.6.2.7	O
,	O
we	O
described	O
lattice-structured	O
crfs	O
for	O
various	O
low-level	O
vision	O
problems	O
.	O
(	O
szeliski	O
et	O
al	O
.	O
2008	O
)	O
performed	O
an	O
extensive	O
comparison	O
of	O
different	O
approximate	O
optimization	O
techniques	O
for	O
this	O
class	O
of	O
problems	O
.	O
some	O
of	O
the	O
results	O
,	O
for	O
the	O
problem	O
of	O
stereo	O
depth	O
estimation	O
,	O
are	O
shown	O
in	O
figure	O
22.16.	O
we	O
see	O
that	O
the	O
graphcut	O
and	O
tree-reweighted	O
max-	O
product	O
bp	O
(	O
trw	O
)	O
give	O
the	O
best	O
results	O
,	O
with	O
regular	B
max-product	O
bp	O
being	O
much	O
worse	O
.	O
in	O
terms	O
of	O
speed	O
,	O
graphcuts	B
is	O
the	O
fastest	O
,	O
with	O
trw	O
a	O
close	O
second	O
.	O
other	O
algorithms	O
,	O
such	O
as	O
icm	O
,	O
simulated	B
annealing	I
or	O
a	O
standard	O
domain-speciﬁc	O
heuristic	O
known	O
as	O
normalize	O
correlation	O
,	O
are	O
22.6.	O
map	O
state	B
estimation	I
805	O
×106	O
2	O
1.9	O
1.8	O
1.7	O
1.6	O
1.5	O
1.4	O
y	O
g	O
r	O
e	O
n	O
e	O
max-product	B
bp	O
a-expansion	O
a-b	O
swap	O
trw	O
×105	O
4.2	O
4.1	O
4	O
3.9	O
3.8	O
3.7	O
y	O
g	O
r	O
e	O
n	O
e	O
max-product	B
bp	O
a-expansion	O
a-b	O
swap	O
trw	O
1.3	O
100	O
101	O
running	O
time	O
(	O
s	O
)	O
102	O
103	O
3.6	O
100	O
101	O
running	O
time	O
(	O
s	O
)	O
102	O
figure	O
22.16	O
energy	O
minimization	O
on	O
a	O
crf	O
for	O
stereo	O
depth	O
estimation	O
.	O
top	O
row	O
:	O
two	O
input	O
images	O
along	O
with	O
the	O
ground	O
truth	O
depth	O
values	O
.	O
bottom	O
row	O
:	O
energy	O
vs	O
time	O
for	O
4	O
different	O
optimization	B
algorithms	O
.	O
bottom	O
left	O
:	O
results	O
are	O
for	O
the	O
teddy	O
image	O
(	O
shown	O
in	O
top	O
row	O
)	O
.	O
bottom	O
right	O
:	O
results	O
are	O
for	O
the	O
tsukuba	O
image	O
(	O
shown	O
in	O
figure	O
22.17	O
(	O
a	O
)	O
)	O
.	O
source	O
:	O
figure	O
13.b.1	O
of	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
.	O
even	O
worse	O
,	O
as	O
shown	O
qualitatively	O
in	O
figure	O
22.17.	O
since	O
trw	O
is	O
optimizing	O
the	O
dual	O
of	O
the	O
relaxed	O
lp	O
problem	O
,	O
we	O
can	O
use	O
its	O
value	O
at	O
conver-	O
gence	O
to	O
evaluate	O
the	O
optimal	O
energy	O
.	O
it	O
turns	O
out	O
that	O
for	O
many	O
of	O
the	O
images	O
in	O
the	O
stereo	O
benchmark	O
dataset	O
,	O
the	O
ground	O
truth	O
has	O
higher	O
energy	O
(	O
lower	O
probability	O
)	O
than	O
the	O
globally	O
op-	O
timal	O
estimate	O
(	O
meltzer	O
et	O
al	O
.	O
2005	O
)	O
.	O
this	O
indicates	O
that	O
we	O
are	O
optimizing	O
the	O
wrong	O
model	O
.	O
this	O
is	O
not	O
surprising	O
,	O
since	O
the	O
pairwise	O
crf	O
ignores	O
known	O
long-range	O
constraints	O
.	O
unfortunately	O
,	O
if	O
we	O
add	O
these	O
constraints	O
to	O
the	O
model	O
,	O
the	O
graph	B
either	O
becomes	O
too	O
dense	O
(	O
making	O
bp	O
slow	O
)	O
,	O
and/or	O
the	O
potentials	O
become	O
non-submodular	O
(	O
making	O
graphcuts	B
inapplicable	O
)	O
.	O
one	O
way	O
around	O
this	O
is	O
to	O
generate	O
a	O
diverse	O
set	O
of	O
local	O
modes	O
,	O
using	O
repeated	O
applications	O
of	O
graph	B
cuts	I
,	O
as	O
described	O
in	O
(	O
yadollahpour	O
et	O
al	O
.	O
2011	O
)	O
.	O
we	O
can	O
then	O
apply	O
a	O
more	O
sophisticated	O
model	O
,	O
which	O
uses	O
global	O
features	O
,	O
to	O
rerank	B
the	O
solutions	O
.	O
806	O
chapter	O
22.	O
more	O
variational	B
inference	I
(	O
a	O
)	O
left	O
image	O
:	O
384x288	O
,	O
15	O
labels	O
(	O
b	O
)	O
ground	O
truth	O
(	O
c	O
)	O
swap	O
algorithm	O
(	O
d	O
)	O
expansion	O
algorithm	O
(	O
e	O
)	O
normalized	O
correlation	O
(	O
f	O
)	O
simulated	B
annealing	I
figure	O
22.17	O
an	O
example	O
of	O
stereo	O
depth	O
estimation	O
using	O
an	O
mrf	O
.	O
(	O
a	O
)	O
left	O
image	O
,	O
of	O
size	O
384	O
×	O
288	O
pixels	O
,	O
from	O
the	O
university	O
of	O
tsukuba	O
.	O
(	O
b	O
)	O
(	O
c-f	O
)	O
:	O
map	O
estimates	O
using	O
different	O
methods	O
:	O
(	O
c	O
)	O
α	O
−	O
β	O
ground	O
truth	O
depth	O
map	O
,	O
quantized	O
to	O
15	O
levels	O
.	O
swap	O
,	O
(	O
d	O
)	O
α	O
expansion	O
,	O
(	O
e	O
)	O
normalized	O
cross	O
correlation	O
,	O
(	O
f	O
)	O
simulated	B
annealing	I
.	O
source	O
:	O
figure	O
10	O
of	O
(	O
boykov	O
et	O
al	O
.	O
2001	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
ramin	O
zabih	O
.	O
(	O
the	O
corresponding	O
right	O
image	O
is	O
similar	B
,	O
but	O
not	O
shown	O
.	O
)	O
22.6.5	O
dual	B
decomposition	I
we	O
are	O
interested	O
in	O
computing	O
(	O
cid:4	O
)	O
f∈f	O
(	O
cid:4	O
)	O
p∗	O
=	O
max	O
x∈x	O
m	O
i∈v	O
θi	O
(	O
xi	O
)	O
+	O
θf	O
(	O
xf	O
)	O
(	O
22.163	O
)	O
where	O
f	O
represents	O
a	O
set	O
of	O
factors	B
.	O
we	O
will	O
assume	O
that	O
we	O
can	O
tractably	O
optimize	O
each	O
local	O
factor	O
,	O
but	O
the	O
combination	O
of	O
all	O
of	O
these	O
factors	B
makes	O
the	O
problem	O
intractable	O
.	O
one	O
way	O
to	O
proceed	O
is	O
to	O
optimize	O
each	O
term	O
independently	O
,	O
but	O
then	O
to	O
introduce	O
constraints	O
that	O
force	O
all	O
the	O
local	O
estimates	O
of	O
the	O
variables	O
’	O
values	O
to	O
agree	O
with	O
each	O
other	O
.	O
we	O
explain	O
this	O
in	O
more	O
detail	O
below	O
,	O
following	O
the	O
presentation	O
of	O
(	O
sontag	O
et	O
al	O
.	O
2011	O
)	O
.	O
22.6.	O
map	O
state	B
estimation	I
807	O
θf	O
(	O
x1	O
,	O
x2	O
)	O
x1	O
x2	O
θg	O
(	O
x1	O
,	O
x3	O
)	O
θh	O
(	O
x2	O
,	O
x4	O
)	O
x3	O
x4	O
θk	O
(	O
x3	O
,	O
x4	O
)	O
θf	O
(	O
xf	O
1	O
,	O
xf	O
2	O
)	O
xf	O
1	O
=	O
xg	O
1	O
=	O
x1	O
xf	O
2	O
=	O
x2	O
=	O
xh	O
2	O
θh	O
(	O
xh	O
2	O
,	O
xh	O
4	O
)	O
θg	O
(	O
xg	O
1	O
,	O
xg	O
3	O
)	O
xg	O
3	O
=	O
x3	O
x4	O
=	O
xh	O
4	O
=	O
xk	O
3	O
=	O
xk	O
4	O
3	O
,	O
xk	O
4	O
)	O
θk	O
(	O
xk	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
22.18	O
(	O
a	O
)	O
a	O
pairwise	O
mrf	O
with	O
4	O
different	O
edge	O
factors	O
.	O
(	O
b	O
)	O
we	O
have	O
4	O
separate	O
variables	O
,	O
plus	O
a	O
copy	O
of	O
each	O
variable	O
for	O
each	O
factor	B
it	O
participates	O
in	O
.	O
source	O
:	O
figure	O
1.2-1.3	O
of	O
(	O
sontag	O
et	O
al	O
.	O
2011	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
david	O
sontag	O
.	O
22.6.5.1	O
basic	O
idea	O
let	O
us	O
duplicate	O
the	O
variables	O
xi	O
,	O
once	O
for	O
each	O
factor	B
,	O
and	O
then	O
force	O
them	O
to	O
be	O
equal	O
.	O
i	O
}	O
i∈f	O
be	O
the	O
set	O
of	O
variables	O
used	O
by	O
factor	B
f.	O
this	O
construction	O
is	O
speciﬁcally	O
,	O
let	O
xf	O
(	O
cid:4	O
)	O
illustrated	O
in	O
figure	O
22.18.	O
we	O
can	O
reformulate	O
the	O
objective	O
as	O
follows	O
:	O
i	O
=	O
xi	O
∀f	O
,	O
i	O
∈	O
f	O
f	O
=	O
{	O
xf	O
(	O
cid:4	O
)	O
s.t	O
.	O
xf	O
θi	O
(	O
xi	O
)	O
+	O
θf	O
(	O
xf	O
f	O
)	O
(	O
22.164	O
)	O
p∗	O
=	O
max	O
x	O
,	O
xf	O
i∈v	O
f∈f	O
let	O
us	O
now	O
introduce	O
lagrange	O
multipliers	O
,	O
or	O
dual	B
variables	I
,	O
δf	O
i	O
(	O
k	O
)	O
,	O
to	O
enforce	O
these	O
constraints	O
.	O
the	O
lagrangian	O
becomes	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
i∈v	O
θi	O
(	O
xi	O
)	O
+	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
f∈f	O
+	O
f∈f	O
i∈f	O
ˆxi	O
l	O
(	O
δ	O
,	O
x	O
,	O
xf	O
)	O
=	O
!	O
θf	O
(	O
xf	O
f	O
)	O
i	O
(	O
xi	O
=	O
ˆxi	O
)	O
−	O
i	O
(	O
xf	O
δf	O
i	O
(	O
ˆxi	O
)	O
i	O
=	O
ˆxi	O
)	O
#	O
(	O
22.165	O
)	O
(	O
22.166	O
)	O
this	O
is	O
equivalent	O
to	O
our	O
original	O
problem	O
in	O
the	O
following	O
sense	O
:	O
for	O
any	O
value	O
of	O
δ	O
,	O
we	O
have	O
p∗	O
=	O
max	O
x	O
,	O
xf	O
l	O
(	O
δ	O
,	O
x	O
,	O
xf	O
)	O
s.t	O
.	O
xf	O
i	O
=	O
xi	O
∀f	O
,	O
i	O
∈	O
f	O
(	O
22.167	O
)	O
since	O
if	O
the	O
constraints	O
hold	O
,	O
the	O
last	O
term	O
is	O
zero	O
.	O
we	O
can	O
get	O
an	O
upper	O
bound	O
by	O
dropping	O
the	O
consistency	O
constraints	O
,	O
and	O
just	O
optimizing	O
the	O
following	O
upper	O
bound	O
:	O
l	O
(	O
δ	O
)	O
(	O
cid:2	O
)	O
max	O
x	O
,	O
xf	O
(	O
cid:4	O
)	O
=	O
max	O
xi	O
i	O
l	O
(	O
δ	O
,	O
x	O
,	O
xf	O
)	O
⎛	O
⎝θi	O
(	O
xi	O
)	O
+	O
(	O
cid:4	O
)	O
f	O
:	O
i∈f	O
δf	O
i	O
(	O
xi	O
)	O
⎞	O
⎠	O
+	O
(	O
cid:4	O
)	O
f	O
max	O
xf	O
⎛	O
⎝θf	O
(	O
xf	O
)	O
−	O
(	O
22.168	O
)	O
⎞	O
⎠	O
(	O
22.169	O
)	O
(	O
cid:4	O
)	O
i∈f	O
δf	O
i	O
(	O
xi	O
)	O
see	O
figure	O
22.19	O
for	O
an	O
illustration	O
.	O
808	O
chapter	O
22.	O
more	O
variational	B
inference	I
θg	O
(	O
x1	O
,	O
x3	O
)	O
−	O
δg3	O
(	O
x3	O
)	O
−	O
δg1	O
(	O
x1	O
)	O
x1	O
x3	O
−	O
θf	O
(	O
x1	O
,	O
x2	O
)	O
x1	O
δf	O
1	O
(	O
x1	O
)	O
−	O
x2	O
δf	O
2	O
(	O
x2	O
)	O
δf	O
1	O
(	O
x1	O
)	O
+	O
δg1	O
(	O
x1	O
)	O
δg3	O
(	O
x3	O
)	O
+	O
δk3	O
(	O
x3	O
)	O
x1	O
x2	O
x3	O
x4	O
δf	O
2	O
(	O
x2	O
)	O
+	O
δh2	O
(	O
x2	O
)	O
δk4	O
(	O
x4	O
)	O
+	O
δh4	O
(	O
x4	O
)	O
x3	O
−	O
θk	O
(	O
x3	O
,	O
x4	O
)	O
x4	O
−	O
δk3	O
(	O
x3	O
)	O
δk4	O
(	O
x4	O
)	O
x2	O
x4	O
θh	O
(	O
x2	O
,	O
x4	O
)	O
−	O
δh2	O
(	O
x2	O
)	O
−	O
δh4	O
(	O
x4	O
)	O
figure	O
22.19	O
illustration	O
of	O
dual	B
decomposition	I
.	O
kind	O
permission	O
of	O
david	O
sontag	O
.	O
source	O
:	O
figure	O
1.2	O
of	O
(	O
sontag	O
et	O
al	O
.	O
2011	O
)	O
.	O
used	O
with	O
this	O
objective	O
is	O
tractable	O
to	O
optimize	O
,	O
since	O
each	O
xf	O
term	O
is	O
decoupled	O
.	O
furthermore	O
,	O
we	O
see	O
,	O
since	O
by	O
relaxing	O
the	O
consistency	O
constraints	O
,	O
we	O
are	O
optimizing	O
over	O
a	O
larger	O
that	O
l	O
(	O
δ	O
)	O
≥	O
p∗	O
space	O
.	O
furthermore	O
,	O
we	O
have	O
the	O
property	O
that	O
l	O
(	O
δ	O
)	O
=	O
p∗	O
min	O
δ	O
(	O
22.170	O
)	O
so	O
the	O
upper	O
bound	O
is	O
tight	O
at	O
the	O
optimal	O
value	O
of	O
δ	O
,	O
which	O
enforces	O
the	O
original	O
constraints	O
.	O
minimizing	O
this	O
upper	O
bound	O
is	O
known	O
as	O
dual	B
decomposition	I
or	O
lagrangian	O
relaxation	O
(	O
komodakis	O
et	O
al	O
.	O
2011	O
;	O
sontag	O
et	O
al	O
.	O
2011	O
;	O
rush	O
and	O
collins	O
2012	O
)	O
.	O
furthemore	O
,	O
it	O
can	O
be	O
shown	O
that	O
l	O
(	O
δ	O
)	O
is	O
the	O
dual	O
to	O
the	O
same	O
lp	O
relaxation	O
we	O
saw	O
before	O
.	O
we	O
will	O
discuss	O
several	O
possible	O
optimization	B
algorithms	O
below	O
.	O
the	O
main	O
advantage	O
of	O
dual	B
decomposition	I
from	O
a	O
practical	O
point	O
of	O
view	O
is	O
that	O
it	O
allows	O
one	O
to	O
mix	O
and	O
match	O
different	O
kinds	O
of	O
optimization	B
algorithms	O
in	O
a	O
convenient	O
way	O
.	O
for	O
example	O
,	O
we	O
can	O
combine	O
a	O
grid	O
structured	O
graph	O
with	O
local	O
submodular	O
factors	B
to	O
perform	O
image	B
segmentation	I
,	O
together	O
with	O
a	O
tree	B
structured	O
model	O
to	O
perform	O
pose	O
estimation	O
(	O
see	O
exercise	O
22.4	O
)	O
.	O
analogous	O
methods	O
can	O
be	O
used	O
in	O
natural	O
language	O
processing	O
,	O
where	O
we	O
often	O
have	O
a	O
mix	O
of	O
local	O
and	O
global	O
constraints	O
(	O
see	O
e.g.	O
,	O
(	O
koo	O
et	O
al	O
.	O
2010	O
;	O
rush	O
and	O
collins	O
2012	O
)	O
)	O
.	O
22.6.5.2	O
theoretical	O
guarantees	O
what	O
can	O
we	O
say	O
about	O
the	O
quality	O
of	O
the	O
solutions	O
obtained	O
in	O
this	O
way	O
?	O
to	O
understand	O
this	O
,	O
let	O
us	O
ﬁrst	O
introduce	O
some	O
more	O
notation	O
:	O
δf	O
i	O
(	O
xi	O
)	O
δf	O
i	O
(	O
xi	O
)	O
(	O
22.171	O
)	O
(	O
22.172	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
f	O
:	O
i∈f	O
i	O
(	O
xi	O
)	O
(	O
cid:2	O
)	O
θi	O
(	O
xi	O
)	O
+	O
θδ	O
f	O
(	O
xf	O
)	O
(	O
cid:2	O
)	O
θf	O
(	O
xf	O
)	O
−	O
θδ	O
i∈f	O
this	O
represents	O
a	O
reparameterization	O
of	O
the	O
original	O
problem	O
,	O
in	O
the	O
sense	O
that	O
22.6.	O
map	O
state	B
estimation	I
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
θi	O
(	O
xi	O
)	O
+	O
i	O
and	O
hence	O
l	O
(	O
δ	O
)	O
=	O
f	O
(	O
cid:4	O
)	O
max	O
xi	O
i	O
θf	O
(	O
xf	O
)	O
=	O
θδ	O
i	O
(	O
xi	O
)	O
+	O
f	O
θδ	O
f	O
(	O
xf	O
)	O
max	O
xf	O
f	O
(	O
cid:4	O
)	O
θδ	O
i	O
(	O
xi	O
)	O
+	O
θδ	O
f	O
(	O
xf	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
i	O
809	O
(	O
22.173	O
)	O
(	O
22.174	O
)	O
and	O
an	O
assignment	O
x∗	O
θδ∗	O
f	O
(	O
xf	O
)	O
.	O
in	O
this	O
case	O
,	O
we	O
have	O
now	O
suppose	O
there	O
is	O
a	O
set	O
of	O
dual	B
variables	I
δ∗	O
(	O
cid:4	O
)	O
f	O
∈	O
argmaxxf	O
θi	O
(	O
x∗	O
such	O
that	O
the	O
maxi-	O
mizing	O
assignments	O
to	O
the	O
singleton	O
terms	O
agrees	O
with	O
the	O
assignments	O
to	O
the	O
factor	B
terms	O
,	O
i.e.	O
,	O
so	O
that	O
x∗	O
l	O
(	O
δ∗	O
now	O
since	O
(	O
cid:4	O
)	O
θi	O
(	O
x∗	O
(	O
cid:4	O
)	O
i	O
∈	O
argmaxxi	O
θδ∗	O
i	O
(	O
x∗	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
θδ∗	O
i	O
(	O
xi	O
)	O
and	O
x∗	O
θδ∗	O
f	O
(	O
x∗	O
f	O
)	O
≤	O
p∗	O
≤	O
l	O
(	O
δ∗	O
θf	O
(	O
x∗	O
f	O
)	O
(	O
cid:4	O
)	O
θf	O
(	O
x∗	O
(	O
22.176	O
)	O
(	O
22.175	O
)	O
f	O
)	O
=	O
i	O
)	O
+	O
i	O
)	O
+	O
i	O
)	O
+	O
)	O
=	O
)	O
i	O
f	O
i	O
f	O
i	O
f	O
we	O
conclude	O
that	O
l	O
(	O
δ∗	O
)	O
=	O
p∗	O
,	O
sox	O
∗	O
is	O
the	O
map	O
assignment	O
.	O
so	O
if	O
we	O
can	O
ﬁnd	O
a	O
solution	O
where	O
all	O
the	O
subproblems	O
agree	O
,	O
we	O
can	O
be	O
assured	O
that	O
it	O
is	O
the	O
global	O
optimum	O
.	O
this	O
happens	O
surprisingly	O
often	O
in	O
practical	O
problems	O
.	O
22.6.5.3	O
subgradient	B
descent	O
l	O
(	O
δ	O
)	O
is	O
a	O
convex	B
and	O
continuous	O
objective	O
,	O
but	O
it	O
is	O
non-differentiable	O
at	O
points	O
δ	O
where	O
θδ	O
or	O
θδ	O
the	O
elements	O
of	O
δ	O
at	O
the	O
same	O
time	O
,	O
as	O
follows	O
:	O
i	O
(	O
xi	O
)	O
f	O
(	O
xf	O
)	O
have	O
multiple	O
optima	O
.	O
one	O
approach	O
is	O
to	O
use	O
subgradient	B
descent	O
.	O
this	O
updates	O
all	O
δt+1	O
f	O
i	O
(	O
xi	O
)	O
=	O
δt	O
f	O
i	O
(	O
xi	O
)	O
−	O
αtgt	O
f	O
i	O
(	O
xi	O
)	O
(	O
22.177	O
)	O
one	O
can	O
show	O
that	O
the	O
gradient	O
is	O
given	O
by	O
the	O
following	O
sparse	O
vector	O
.	O
first	O
let	O
xs	O
where	O
gt	O
the	O
subgradient	B
of	O
l	O
(	O
δ	O
)	O
at	O
δt	O
.	O
tion	O
8.5.2.1	O
)	O
,	O
this	O
method	O
is	O
guaranteed	O
to	O
converge	B
to	O
a	O
global	O
optimum	O
of	O
the	O
dual	O
.	O
(	O
komodakis	O
et	O
al	O
.	O
2011	O
)	O
for	O
details	O
.	O
)	O
if	O
the	O
step	O
sizes	O
αt	O
are	O
set	O
appropriately	O
(	O
see	O
sec-	O
(	O
see	O
i	O
∈	O
θδt	O
f	O
(	O
xf	O
)	O
.	O
next	O
let	O
gf	O
i	O
(	O
xi	O
)	O
=	O
0	O
for	O
all	O
elements	O
.	O
finally	O
,	O
argmaxxi	O
i	O
(	O
cid:8	O
)	O
=	O
xs	O
if	O
xf	O
i	O
)	O
=	O
+1	O
and	O
gf	O
i	O
(	O
xf	O
i	O
)	O
,	O
bringing	O
them	O
closer	O
to	O
agreement	O
.	O
similarly	O
,	O
the	O
subgradient	B
update	O
will	O
decrease	O
the	O
value	O
of	O
θδt	O
f	O
(	O
xf	O
to	O
compute	O
the	O
gradient	O
,	O
we	O
need	O
to	O
be	O
able	O
to	O
solve	O
subproblems	O
of	O
the	O
following	O
form	O
:	O
θδt	O
i	O
(	O
xi	O
)	O
and	O
xf	O
i	O
(	O
so	O
factor	B
f	O
disagrees	O
with	O
the	O
local	O
term	O
on	O
how	O
to	O
set	O
variable	O
i	O
)	O
,	O
we	O
set	O
gf	O
i	O
(	O
xs	O
i	O
)	O
and	O
increasing	O
θδt	O
i	O
)	O
=−1	O
.	O
this	O
has	O
the	O
effect	O
of	O
decreasing	O
θδt	O
f	O
∈	O
argmaxxf	O
i	O
,	O
xf\i	O
)	O
.	O
i	O
(	O
xf	O
f	O
(	O
xs	O
i	O
(	O
xs	O
i	O
,	O
xf\i	O
)	O
and	O
increasing	O
the	O
value	O
of	O
θδt	O
(	O
cid:4	O
)	O
⎡	O
⎣θf	O
(	O
xf	O
)	O
−	O
θδt	O
f	O
(	O
xf	O
)	O
=	O
argmax	O
xf	O
argmax	O
xf	O
δt	O
f	O
i	O
(	O
xi	O
)	O
i∈f	O
⎤	O
⎦	O
(	O
22.178	O
)	O
810	O
chapter	O
22.	O
more	O
variational	B
inference	I
(	O
in	O
(	O
komodakis	O
et	O
al	O
.	O
2011	O
)	O
,	O
these	O
subproblems	O
are	O
called	O
slaves	B
,	O
whereas	O
l	O
(	O
δ	O
)	O
is	O
called	O
the	O
master	B
.	O
)	O
obviously	O
if	O
the	O
scope	B
of	O
factor	B
f	O
is	O
small	O
,	O
this	O
is	O
simple	O
.	O
for	O
example	O
,	O
if	O
each	O
factor	B
is	O
pairwise	O
,	O
and	O
each	O
variable	O
has	O
k	O
states	O
,	O
the	O
cost	O
is	O
just	O
k	O
2.	O
however	O
,	O
there	O
are	O
some	O
kinds	O
of	O
global	O
factors	O
that	O
also	O
support	B
exact	O
and	O
efficient	O
maximization	O
,	O
including	O
the	O
following	O
:	O
•	O
graphical	O
models	O
with	O
low	O
tree	O
width	O
.	O
•	O
factors	B
that	O
correspond	O
to	O
bipartite	B
graph	I
matchings	O
(	O
see	O
e.g.	O
,	O
(	O
duchi	O
et	O
al	O
.	O
2007	O
)	O
)	O
.	O
this	O
is	O
useful	O
for	O
data	B
association	I
problems	O
,	O
where	O
we	O
must	O
match	O
up	O
a	O
sensor	O
reading	O
with	O
an	O
unknown	B
source	O
.	O
we	O
can	O
ﬁnd	O
the	O
maximal	O
matching	O
using	O
the	O
so-called	O
hungarian	O
algorithm	O
in	O
o	O
(	O
|f|3	O
)	O
time	O
(	O
see	O
e.g.	O
,	O
(	O
padadimitriou	O
and	O
steiglitz	O
1982	O
)	O
)	O
.	O
•	O
supermodular	B
functions	O
.	O
we	O
discuss	O
this	O
case	O
in	O
more	O
detail	O
in	O
section	O
22.6.3.2	O
.	O
•	O
cardinality	O
constraints	O
.	O
for	O
example	O
,	O
we	O
might	O
have	O
a	O
factor	B
over	O
a	O
large	O
set	O
of	O
binary	O
variables	O
that	O
enforces	O
that	O
a	O
certain	O
number	O
of	O
bits	B
are	O
turned	O
on	O
;	O
this	O
can	O
be	O
useful	O
in	O
problems	O
such	O
as	O
image	B
segmentation	I
.	O
in	O
particular	O
,	O
suppose	O
θf	O
(	O
xf	O
)	O
=	O
0	O
if	O
i∈f	O
xi	O
=	O
l	O
and	O
θf	O
(	O
xf	O
)	O
=	O
−∞	O
otherwise	O
.	O
we	O
can	O
ﬁnd	O
the	O
maximizing	O
assignment	O
in	O
o	O
(	O
|f|	O
log	O
|f|	O
)	O
time	O
as	O
follows	O
:	O
ﬁrst	O
deﬁne	O
ei	O
=	O
δf	O
i	O
(	O
1	O
)	O
−	O
δf	O
i	O
(	O
0	O
)	O
;	O
now	O
sort	O
the	O
ei	O
;	O
ﬁnally	O
set	O
xi	O
=	O
1	O
for	O
the	O
ﬁrst	O
l	O
values	O
,	O
and	O
xi	O
=	O
0	O
for	O
the	O
rest	O
(	O
tarlow	O
et	O
al	O
.	O
2010	O
)	O
.	O
(	O
cid:7	O
)	O
•	O
factors	B
which	O
are	O
constant	O
for	O
all	O
but	O
a	O
small	O
set	O
s	O
of	O
distinguished	O
values	O
of	O
xf	O
.	O
then	O
we	O
can	O
optimize	O
over	O
the	O
factor	B
in	O
o	O
(	O
|s|	O
)	O
time	O
(	O
rother	O
et	O
al	O
.	O
2009	O
)	O
.	O
22.6.5.4	O
coordinate	O
descent	O
an	O
alternative	O
to	O
updating	O
the	O
entire	O
δ	O
vector	O
at	O
once	O
(	O
albeit	O
sparsely	O
)	O
is	O
to	O
update	O
it	O
using	O
block	O
coordinate	O
descent	O
.	O
by	O
choosing	O
the	O
size	O
of	O
the	O
blocks	O
,	O
we	O
can	O
trade	O
off	O
convergence	O
speed	O
with	O
ease	O
of	O
the	O
local	O
optimization	O
problem	O
.	O
one	O
approach	O
,	O
which	O
optimizes	O
δf	O
i	O
(	O
xi	O
)	O
for	O
all	O
i	O
∈	O
f	O
and	O
all	O
xi	O
at	O
the	O
same	O
time	O
(	O
for	O
a	O
ﬁxed	O
factor	O
f	O
)	O
,	O
is	O
known	O
as	O
max	B
product	I
linear	I
programming	I
(	O
globerson	O
and	O
jaakkola	O
2008	O
)	O
.	O
algorithmically	O
,	O
this	O
is	O
similar	B
to	O
belief	B
propagation	I
on	O
a	O
factor	B
graph	I
.	O
in	O
particular	O
,	O
we	O
deﬁne	O
δf→i	O
as	O
messages	O
sent	O
from	O
factor	B
f	O
to	O
variable	O
i	O
,	O
and	O
we	O
deﬁne	O
δi→f	O
as	O
messages	O
sent	O
from	O
variable	O
i	O
to	O
factor	B
f.	O
these	O
messages	O
can	O
be	O
computed	O
as	O
follows	O
(	O
see	O
(	O
globerson	O
and	O
jaakkola	O
2008	O
)	O
for	O
the	O
derivation	O
)	O
:10	O
δi→f	O
(	O
xi	O
)	O
=θ	O
i	O
(	O
xi	O
)	O
+	O
δg→i	O
(	O
xi	O
)	O
(	O
cid:4	O
)	O
g	O
(	O
cid:5	O
)	O
=f	O
δf→i	O
(	O
xi	O
)	O
=−δ	O
i→f	O
(	O
xi	O
)	O
+	O
1|f|	O
max	O
xf\i	O
⎡	O
⎣θf	O
(	O
xf	O
)	O
+	O
(	O
cid:4	O
)	O
j∈f	O
⎤	O
⎦	O
δj→f	O
(	O
xj	O
)	O
(	O
22.179	O
)	O
(	O
22.180	O
)	O
we	O
then	O
set	O
the	O
dual	B
variables	I
δf	O
i	O
(	O
xi	O
)	O
to	O
be	O
the	O
messages	O
δf→i	O
(	O
xi	O
)	O
.	O
for	O
example	O
,	O
consider	O
a	O
2	O
×	O
2	O
grid	O
mrf	O
,	O
with	O
the	O
following	O
pairwise	O
factors	O
:	O
θf	O
(	O
x1	O
,	O
x2	O
)	O
,	O
θg	O
(	O
x1	O
,	O
x3	O
)	O
,	O
θh	O
(	O
x2	O
,	O
x4	O
)	O
,	O
and	O
θk	O
(	O
x3	O
,	O
x4	O
)	O
.	O
the	O
outgoing	O
message	O
from	O
factor	B
f	O
to	O
variable	O
2	O
is	O
a	O
10.	O
note	O
that	O
we	O
denote	O
their	O
δ−f	O
(	O
xi	O
)	O
by	O
δi→f	O
(	O
xi	O
)	O
.	O
i	O
22.6.	O
map	O
state	B
estimation	I
811	O
function	O
of	O
all	O
messages	O
coming	O
into	O
f	O
,	O
and	O
f	O
’	O
s	O
local	O
factor	O
:	O
δf→2	O
(	O
x2	O
)	O
=	O
−δ2→f	O
(	O
x2	O
)	O
+	O
1	O
2	O
max	O
x1	O
[	O
θf	O
(	O
x1	O
,	O
x2	O
)	O
+δ	O
1→f	O
(	O
x1	O
)	O
+δ	O
2→f	O
(	O
x2	O
)	O
]	O
(	O
22.181	O
)	O
similarly	O
,	O
the	O
outgoing	O
message	O
from	O
variable	O
2	O
to	O
factor	B
f	O
is	O
a	O
function	O
of	O
all	O
the	O
messages	O
sent	O
into	O
variable	O
2	O
from	O
other	O
connected	O
factors	B
(	O
in	O
this	O
example	O
,	O
just	O
factor	B
h	O
)	O
and	O
the	O
local	O
potential	O
:	O
δ2→f	O
(	O
x2	O
)	O
=	O
θ2	O
(	O
2	O
)	O
+δ	O
h2	O
(	O
x2	O
)	O
(	O
22.182	O
)	O
the	O
key	O
computational	O
bottleneck	O
is	O
computing	O
the	O
max	O
marginals	O
of	O
each	O
factor	B
,	O
where	O
we	O
max	O
out	O
all	O
the	O
variables	O
from	O
xf	O
except	O
for	O
xi	O
,	O
i.e.	O
,	O
we	O
need	O
to	O
be	O
able	O
to	O
compute	O
the	O
following	O
max	O
marginals	O
efficiently	O
:	O
(	O
cid:4	O
)	O
j∈f	O
h	O
(	O
xf\i	O
,	O
xi	O
)	O
,	O
h	O
(	O
xf\i	O
,	O
xi	O
)	O
(	O
cid:2	O
)	O
θf	O
(	O
xf	O
)	O
+	O
max	O
xf\i	O
δjf	O
(	O
xj	O
)	O
(	O
22.183	O
)	O
22.6.5.5	O
the	O
difference	O
from	O
equation	O
22.178	O
is	O
that	O
we	O
are	O
maxing	O
over	O
all	O
but	O
one	O
of	O
the	O
variables	O
.	O
we	O
can	O
solve	O
this	O
efficiently	O
for	O
low	O
treewidth	O
graphical	O
models	O
using	O
message	B
passing	I
;	O
we	O
can	O
also	O
solve	O
this	O
efficiently	O
for	O
factors	B
corresponding	O
to	O
bipartite	O
matchings	O
(	O
duchi	O
et	O
al	O
.	O
2007	O
)	O
or	O
to	O
cardinality	O
constraints	O
(	O
tarlow	O
et	O
al	O
.	O
2010	O
)	O
.	O
however	O
,	O
there	O
are	O
cases	O
where	O
maximizing	O
over	O
all	O
the	O
variables	O
in	O
a	O
factor	B
’	O
s	O
scope	B
is	O
computationally	O
easier	O
than	O
maximizing	O
over	O
all-but-one	O
(	O
see	O
(	O
sontag	O
et	O
al	O
.	O
2011	O
,	O
sec	O
1.5.4	O
)	O
for	O
an	O
example	O
)	O
;	O
in	O
such	O
cases	O
,	O
we	O
may	O
prefer	O
to	O
use	O
a	O
subgradient	B
method	O
.	O
coordinate	O
descent	O
is	O
a	O
simple	O
algorithm	O
that	O
is	O
often	O
much	O
faster	O
at	O
minimizing	O
the	O
dual	O
than	O
gradient	B
descent	I
,	O
especially	O
in	O
the	O
early	O
iterations	O
.	O
it	O
also	O
reduces	O
the	O
objective	O
monotonically	O
,	O
and	O
does	O
not	O
need	O
any	O
step	B
size	I
parameters	O
.	O
unfortunately	O
,	O
it	O
is	O
not	O
guaranteed	O
to	O
converge	B
to	O
the	O
global	O
optimum	O
,	O
since	O
l	O
(	O
δ	O
)	O
is	O
convex	B
but	O
not	O
strictly	O
convex	B
(	O
which	O
implies	O
there	O
may	O
be	O
more	O
than	O
one	O
globally	O
optimizing	O
value	O
)	O
.	O
one	O
way	O
to	O
ensure	O
convergence	O
is	O
to	O
replace	O
the	O
max	O
function	O
in	O
the	O
deﬁnition	O
of	O
l	O
(	O
δ	O
)	O
with	O
the	O
soft-max	O
function	O
,	O
which	O
makes	O
the	O
objective	O
strictly	B
convex	I
(	O
see	O
e.g.	O
,	O
(	O
hazan	O
and	O
shashua	O
2010	O
)	O
for	O
details	O
)	O
.	O
.	O
in	O
general	O
,	O
computing	O
x∗	O
from	O
δ∗	O
recovering	O
the	O
map	O
assignment	O
so	O
far	O
,	O
we	O
have	O
been	O
focussing	O
on	O
ﬁnding	O
the	O
optimal	O
value	O
of	O
δ∗	O
.	O
but	O
what	O
we	O
really	O
want	O
is	O
the	O
optimal	O
value	O
of	O
x∗	O
is	O
np-hard	O
,	O
even	O
if	O
the	O
lp	O
relaxation	O
is	O
tight	O
and	O
the	O
map	O
assignment	O
is	O
unique	O
(	O
sontag	O
et	O
al	O
.	O
2011	O
,	O
theorem	O
1.4	O
)	O
.	O
(	O
the	O
troublesome	O
cases	O
arise	O
when	O
there	O
are	O
fractional	O
assignments	O
with	O
the	O
same	O
optimal	O
value	O
as	O
the	O
map	O
estimate	O
.	O
)	O
however	O
,	O
suppose	O
that	O
each	O
θδ∗	O
is	O
locally	B
decodable	I
to	O
x∗	O
.	O
one	O
can	O
show	O
than	O
in	O
this	O
case	O
,	O
the	O
lp	O
relaxation	O
is	O
unique	O
and	O
its	O
solution	O
is	O
indeed	O
x∗	O
.	O
if	O
many	O
,	O
but	O
not	O
all	O
,	O
of	O
the	O
nodes	B
are	O
uniquely	O
decodable	O
,	O
we	O
can	O
“	O
clamp	O
”	O
the	O
uniquely	O
decodable	O
ones	O
to	O
their	O
map	O
value	O
,	O
and	O
then	O
use	O
exact	O
inference	B
algorithms	O
to	O
ﬁgure	O
out	O
the	O
optimal	O
assignment	O
to	O
the	O
remaining	O
variables	O
.	O
using	O
this	O
method	O
,	O
(	O
meltzer	O
et	O
al	O
.	O
2005	O
)	O
was	O
able	O
to	O
optimally	O
solve	O
various	O
stereo	O
vision	O
crf	O
estimation	O
problems	O
,	O
and	O
(	O
yanover	O
et	O
al	O
.	O
2007	O
)	O
was	O
able	O
to	O
optimally	O
solve	O
various	O
protein	O
side-chain	O
structure	O
predicition	O
problems	O
.	O
another	O
approach	O
is	O
to	O
use	O
the	O
upper	O
bound	O
provided	O
by	O
the	O
dual	O
in	O
a	O
branch	B
and	I
bound	I
i	O
;	O
in	O
this	O
case	O
,	O
we	O
say	O
that	O
δ∗	O
i	O
has	O
a	O
unique	O
maximum	O
,	O
x∗	O
search	O
procedure	O
(	O
geoffrion	O
1974	O
)	O
.	O
812	O
exercises	O
chapter	O
22.	O
more	O
variational	B
inference	I
exercise	O
22.1	O
graphcuts	B
for	O
map	O
estimation	O
in	O
binary	O
submodular	O
mrfs	O
(	O
source	O
:	O
ex	O
.	O
13.14	O
of	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
)	O
.	O
show	O
that	O
using	O
the	O
graph	B
construction	O
described	O
in	O
section	O
22.6.3.2	O
,	O
the	O
cost	O
of	O
the	O
cut	O
is	O
equal	O
to	O
the	O
energy	O
of	O
the	O
corresponding	O
assignment	O
,	O
up	O
to	O
an	O
irrelevant	O
constant	O
.	O
(	O
warning	O
:	O
this	O
exercise	O
involves	O
a	O
lot	O
of	O
algebraic	O
book-keeping	O
.	O
)	O
(	O
t	O
)	O
=	O
e	O
(	O
x	O
)	O
+	O
const	O
.	O
i	O
=	O
α	O
,	O
ti	O
=	O
1	O
if	O
x	O
(	O
cid:2	O
)	O
i	O
=	O
β	O
,	O
and	O
is	O
submodular	B
if	O
e	O
is	O
a	O
semimetric	O
.	O
i	O
=	O
xi	O
is	O
unchanged	O
f	O
xi	O
(	O
cid:13	O
)	O
=	O
α	O
and	O
xi	O
(	O
cid:13	O
)	O
=	O
β.	O
x	O
(	O
cid:2	O
)	O
exercise	O
22.2	O
graphcuts	B
for	O
alpha-beta	B
swap	I
(	O
source	O
:	O
ex	O
.	O
13.15	O
of	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
)	O
.	O
show	O
how	O
the	O
optimal	O
alpha-beta	O
swap	O
can	O
be	O
found	O
by	O
running	O
min-cut	O
on	O
an	O
appropriately	O
constructed	O
graph	B
.	O
more	O
precisely	O
,	O
a.	O
deﬁne	O
a	O
set	O
of	O
binary	O
variables	O
t1	O
,	O
.	O
.	O
.	O
,	O
tn	O
such	O
that	O
ti	O
=	O
0	O
means	O
x	O
(	O
cid:2	O
)	O
b.	O
deﬁne	O
an	O
energy	B
function	I
over	O
the	O
new	O
variables	O
such	O
that	O
e	O
(	O
cid:2	O
)	O
c.	O
show	O
that	O
e	O
(	O
cid:2	O
)	O
exercise	O
22.3	O
constant	O
factor	O
optimality	O
for	O
alpha-expansion	O
(	O
source	O
:	O
daphne	O
koller.	O
)	O
.	O
let	O
x	O
be	O
a	O
pairwise	O
metric	O
markov	O
random	O
ﬁeld	O
over	O
a	O
graph	B
g	O
=	O
(	O
v	O
,	O
e	O
)	O
.	O
suppose	O
that	O
the	O
variables	O
are	O
nonbinary	O
and	O
that	O
the	O
node	O
potentials	O
are	O
nonnegative	O
.	O
let	O
a	O
denote	O
the	O
set	O
of	O
labels	O
for	O
each	O
x	O
∈	O
x	O
.	O
though	O
it	O
is	O
not	O
possible	O
to	O
(	O
tractably	O
)	O
ﬁnd	O
the	O
globally	O
optimal	O
assignment	O
x	O
(	O
cid:15	O
)	O
in	O
general	O
,	O
the	O
α-expansion	O
algorithm	O
provides	O
a	O
method	O
for	O
ﬁnding	O
assignments	O
ˆx	O
that	O
are	O
locally	O
optimal	O
with	O
respect	O
to	O
a	O
large	O
set	O
of	O
transformations	O
,	O
i.e.	O
,	O
the	O
possible	O
α-expansion	O
moves	O
.	O
despite	O
the	O
fact	O
that	O
α-expansion	O
only	O
produces	O
a	O
locally	O
optimal	O
map	O
assignment	O
,	O
it	O
is	O
possible	O
to	O
prove	O
that	O
the	O
energy	O
of	O
this	O
assignment	O
is	O
within	O
a	O
known	O
factor	B
of	O
the	O
energy	O
of	O
the	O
globally	O
optimal	O
solution	O
x	O
(	O
cid:15	O
)	O
.	O
in	O
fact	O
,	O
this	O
is	O
a	O
special	O
case	O
of	O
a	O
more	O
general	O
principle	O
that	O
applies	O
to	O
a	O
wide	O
variety	O
of	O
algorithms	O
,	O
including	O
max-product	B
belief	I
propagation	I
and	O
more	O
general	O
move-making	O
algorithms	O
:	O
if	O
one	O
can	O
prove	O
that	O
the	O
solutions	O
obtained	O
by	O
the	O
algorithm	O
are	O
‘	O
strong	O
local	O
minima	O
’	O
,	O
i.e.	O
,	O
local	O
minima	O
with	O
respect	O
to	O
a	O
large	O
set	O
of	O
potential	O
moves	O
,	O
then	O
it	O
is	O
possible	O
to	O
derive	O
bounds	O
on	O
the	O
(	O
global	O
)	O
suboptimality	O
of	O
these	O
solutions	O
,	O
and	O
the	O
quality	O
of	O
the	O
bounds	O
will	O
depend	O
on	O
the	O
nature	O
of	O
the	O
moves	O
considered	O
.	O
(	O
there	O
is	O
a	O
precise	O
deﬁnition	O
of	O
‘	O
large	O
set	O
of	O
moves	O
’	O
.	O
)	O
consider	O
the	O
following	O
approach	O
to	O
proving	O
the	O
suboptimality	O
bound	O
for	O
α-expansion	O
.	O
a.	O
let	O
ˆx	O
be	O
a	O
local	O
minimum	O
with	O
respect	O
to	O
expansion	O
moves	O
.	O
for	O
each	O
α	O
∈	O
a	O
,	O
let	O
v	O
α	O
=	O
{	O
s	O
∈	O
v	O
|	O
x	O
(	O
cid:15	O
)	O
s	O
=	O
α	O
}	O
,	O
i.e.	O
,	O
the	O
set	O
of	O
nodes	B
labelled	O
α	O
in	O
the	O
global	B
minimum	I
.	O
let	O
x	O
(	O
cid:2	O
)	O
be	O
an	O
assignment	O
that	O
is	O
equal	O
to	O
x	O
(	O
cid:15	O
)	O
on	O
v	O
α	O
and	O
equal	O
to	O
ˆx	O
elsewhere	O
;	O
this	O
is	O
an	O
α-expansion	O
of	O
ˆx	O
.	O
verify	O
that	O
e	O
(	O
x	O
(	O
cid:15	O
)	O
)	O
≤	O
e	O
(	O
ˆx	O
)	O
≤	O
e	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
.	O
b.	O
building	O
on	O
the	O
previous	O
part	O
,	O
show	O
that	O
e	O
(	O
ˆx	O
)	O
≤	O
2ce	O
(	O
x	O
(	O
cid:15	O
)	O
)	O
,	O
where	O
c	O
=	O
max	O
(	O
s	O
,	O
t	O
)	O
∈e	O
and	O
e	O
denotes	O
the	O
energy	O
of	O
an	O
assignment	O
.	O
hint	O
.	O
think	O
about	O
where	O
x	O
(	O
cid:2	O
)	O
agrees	O
with	O
ˆx	O
and	O
where	O
it	O
agrees	O
with	O
x	O
(	O
cid:15	O
)	O
.	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
maxα	O
(	O
cid:5	O
)	O
=β	O
εst	O
(	O
α	O
,	O
β	O
)	O
minα	O
(	O
cid:5	O
)	O
=β	O
εst	O
(	O
α	O
,	O
β	O
)	O
exercise	O
22.4	O
dual	B
decomposition	I
for	O
pose	O
segmentation	O
(	O
source	O
:	O
daphne	O
koller.	O
)	O
.	O
two	O
important	O
problems	O
in	O
computer	O
vision	O
are	O
that	O
of	O
parsing	O
articulated	O
objects	O
(	O
e.g.	O
,	O
the	O
human	O
body	O
)	O
,	O
called	O
pose	O
estimation	O
,	O
and	O
segmenting	O
the	O
foreground	O
and	O
the	O
background	O
,	O
called	O
segmentation	O
.	O
intuitively	O
,	O
these	O
two	O
problems	O
are	O
linked	O
,	O
in	O
that	O
solving	O
either	O
one	O
would	O
be	O
easier	O
if	O
the	O
solution	O
to	O
the	O
other	O
were	O
available	O
.	O
we	O
consider	O
solving	O
these	O
problems	O
simultaneously	O
using	O
a	O
joint	O
model	O
over	O
human	O
poses	O
and	O
foreground/background	O
labels	O
and	O
then	O
using	O
dual	B
decomposition	I
for	O
map	O
inference	B
in	O
this	O
model	O
.	O
we	O
construct	O
a	O
two-level	O
model	O
,	O
where	O
the	O
high	O
level	O
handles	O
pose	O
estimation	O
and	O
the	O
low	O
level	O
handles	O
pixel-level	O
background	O
segmentation	O
.	O
let	O
g	O
=	O
(	O
v	O
,	O
e	O
)	O
be	O
an	O
undirected	B
grid	O
over	O
the	O
pixels	O
.	O
each	O
node	O
i	O
∈	O
v	O
represents	O
a	O
pixel	O
.	O
suppose	O
we	O
have	O
one	O
binary	O
variable	O
xi	O
for	O
each	O
pixel	O
,	O
where	O
xi	O
=	O
1	O
means	O
that	O
pixel	O
i	O
is	O
in	O
the	O
foreground	O
.	O
denote	O
the	O
full	B
set	O
of	O
these	O
variables	O
by	O
x	O
=	O
(	O
xi	O
)	O
.	O
22.6.	O
map	O
state	B
estimation	I
813	O
in	O
addition	O
,	O
suppose	O
we	O
have	O
an	O
undirected	B
tree	O
structure	O
t	O
=	O
(	O
v	O
(	O
cid:2	O
)	O
,	O
e	O
(	O
cid:2	O
)	O
)	O
on	O
the	O
parts	O
.	O
for	O
each	O
body	O
part	O
,	O
we	O
have	O
a	O
discrete	B
set	O
of	O
candidate	O
poses	O
that	O
the	O
part	O
can	O
be	O
in	O
,	O
where	O
each	O
pose	O
is	O
characterized	O
by	O
parameters	O
specifying	O
its	O
position	O
and	O
orientation	O
.	O
(	O
these	O
candidates	O
are	O
generated	O
by	O
a	O
procedure	O
external	O
to	O
the	O
algorithm	O
described	O
here	O
.	O
)	O
deﬁne	O
yjk	O
to	O
be	O
a	O
binary	O
variable	O
indicating	O
whether	O
body	O
part	O
is	O
in	O
conﬁguration	O
k.	O
then	O
the	O
full	B
set	O
of	O
part	O
variables	O
is	O
given	O
by	O
y	O
=	O
(	O
yjk	O
)	O
,	O
with	O
j	O
∈	O
v	O
(	O
cid:2	O
)	O
j	O
∈	O
v	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
k	O
and	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
,	O
where	O
j	O
is	O
the	O
total	O
number	O
of	O
body	O
parts	O
and	O
k	O
is	O
the	O
number	O
of	O
candidate	O
poses	O
for	O
each	O
part	O
.	O
note	O
that	O
in	O
order	O
to	O
describe	O
a	O
valid	O
conﬁguration	O
,	O
y	O
must	O
satisfy	O
the	O
constraint	O
that	O
k=1	O
yjk	O
=	O
1	O
for	O
each	O
j.	O
suppose	O
we	O
have	O
the	O
following	O
energy	B
function	I
on	O
pixels	O
:	O
1	O
[	O
xi	O
(	O
cid:13	O
)	O
=	O
xj	O
]	O
·	O
θij	O
.	O
1	O
[	O
xi	O
=	O
1	O
]	O
·	O
θi	O
+	O
e1	O
(	O
x	O
)	O
=	O
(	O
cid:12	O
)	O
(	O
i	O
,	O
j	O
)	O
∈e	O
(	O
cid:12	O
)	O
i∈v	O
(	O
cid:12	O
)	O
p∈v	O
(	O
cid:2	O
)	O
assume	O
that	O
the	O
θij	O
arises	O
from	O
a	O
metric	B
(	O
e.g.	O
,	O
based	O
on	O
differences	O
in	O
pixel	O
intensities	O
)	O
,	O
so	O
this	O
can	O
be	O
viewed	O
as	O
the	O
energy	O
for	O
a	O
pairwise	O
metric	O
mrf	O
with	O
respect	O
to	O
g.	O
we	O
then	O
have	O
the	O
following	O
energy	B
function	I
for	O
parts	O
:	O
e2	O
(	O
y	O
)	O
=	O
θp	O
(	O
yp	O
)	O
+	O
θpq	O
(	O
yp	O
,	O
yq	O
)	O
.	O
(	O
cid:12	O
)	O
(	O
p	O
,	O
q	O
)	O
∈e	O
(	O
cid:2	O
)	O
since	O
each	O
part	O
candidate	O
yjk	O
is	O
assumed	O
to	O
come	O
with	O
a	O
position	O
and	O
orientation	O
,	O
we	O
can	O
compute	O
a	O
jk	O
}	O
i∈v	O
,	O
where	O
binary	B
mask	I
in	O
the	O
image	O
plane	O
.	O
the	O
mask	O
assigns	O
a	O
value	O
to	O
each	O
pixel	O
,	O
denoted	O
by	O
{	O
wi	O
wi	O
jk	O
=	O
1	O
if	O
pixel	O
i	O
lies	O
on	O
the	O
skeleton	O
and	O
decreases	O
as	O
we	O
move	O
away	O
.	O
we	O
can	O
use	O
this	O
to	O
deﬁne	O
an	O
energy	B
function	I
relating	O
the	O
parts	O
and	O
the	O
pixels	O
:	O
e3	O
(	O
x	O
,	O
y	O
)	O
=	O
1	O
[	O
xi	O
=	O
0	O
,	O
yjk	O
=	O
1	O
]	O
·	O
wi	O
jk	O
.	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
k	O
(	O
cid:12	O
)	O
i∈v	O
j∈v	O
(	O
cid:2	O
)	O
k=1	O
in	O
other	O
words	O
,	O
this	O
energy	O
term	O
only	O
penalizes	O
the	O
case	O
where	O
a	O
part	O
candidate	O
is	O
active	O
but	O
the	O
pixel	O
underneath	O
is	O
labeled	O
as	O
background	O
.	O
formulate	O
the	O
minimization	O
of	O
e1	O
+	O
e2	O
+	O
e3	O
as	O
an	O
integer	O
program	O
and	O
show	O
how	O
you	O
can	O
use	O
dual	B
decomposition	I
to	O
solve	O
the	O
dual	O
of	O
this	O
integer	O
program	O
.	O
your	O
solution	O
should	O
describe	O
the	O
decomposition	O
into	O
slaves	B
,	O
the	O
method	O
for	O
solving	O
each	O
one	O
,	O
and	O
the	O
update	O
rules	B
for	O
the	O
overall	O
algorithm	O
.	O
brieﬂy	O
justify	O
your	O
design	O
choices	O
,	O
particularly	O
your	O
choice	O
of	O
inference	B
algorithms	O
for	O
the	O
slaves	B
.	O
23	O
monte	O
carlo	O
inference	B
23.1	O
introduction	O
so	O
far	O
,	O
we	O
discussed	O
various	O
deterministic	O
algorithms	O
for	O
posterior	O
inference	O
.	O
these	O
meth-	O
ods	O
enjoy	O
many	O
of	O
the	O
beneﬁts	O
of	O
the	O
bayesian	O
approach	O
,	O
while	O
still	O
being	O
about	O
as	O
fast	O
as	O
optimization-based	O
point-estimation	O
methods	O
.	O
the	O
trouble	O
with	O
these	O
methods	O
is	O
that	O
they	O
can	O
be	O
rather	O
complicated	O
to	O
derive	O
,	O
and	O
they	O
are	O
somewhat	O
limited	O
in	O
their	O
domain	O
of	O
applicabil-	O
ity	O
(	O
e.g.	O
,	O
they	O
usually	O
assume	O
conjugate	B
priors	I
and	O
exponential	B
family	I
likelihoods	O
,	O
although	O
see	O
(	O
wand	O
et	O
al	O
.	O
2011	O
)	O
for	O
some	O
recent	O
extensions	O
of	O
mean	B
ﬁeld	I
to	O
more	O
complex	O
distributions	O
)	O
.	O
fur-	O
thermore	O
,	O
although	O
they	O
are	O
fast	O
,	O
their	O
accuracy	O
is	O
often	O
limited	O
by	O
the	O
form	O
of	O
the	O
approximation	O
which	O
we	O
choose	O
.	O
(	O
cid:7	O
)	O
s	O
s	O
in	O
this	O
chapter	O
,	O
we	O
discuss	O
an	O
alternative	O
class	O
of	O
algorithms	O
based	O
on	O
the	O
idea	O
of	O
monte	O
carlo	O
approximation	O
,	O
which	O
we	O
ﬁrst	O
introduced	O
in	O
section	O
2.7.	O
the	O
idea	O
is	O
very	O
simple	O
:	O
generate	O
some	O
(	O
unweighted	O
)	O
samples	B
from	O
the	O
posterior	O
,	O
xs	O
∼	O
p	O
(	O
x|d	O
)	O
,	O
and	O
then	O
use	O
these	O
to	O
compute	O
any	O
quantity	O
of	O
interest	O
,	O
such	O
as	O
a	O
posterior	O
marginal	O
,	O
p	O
(	O
x1|d	O
)	O
,	O
or	O
the	O
posterior	O
of	O
the	O
difference	O
of	O
two	O
quantities	O
,	O
p	O
(	O
x1	O
−	O
x2|d	O
)	O
,	O
or	O
the	O
posterior	O
predictive	O
,	O
p	O
(	O
y|d	O
)	O
,	O
etc	O
.	O
all	O
of	O
these	O
quantities	O
can	O
be	O
approximated	O
by	O
e	O
[	O
f|d	O
]	O
≈	O
1	O
s=1	O
f	O
(	O
xs	O
)	O
for	O
some	O
suitable	O
function	O
f.	O
by	O
generating	O
enough	O
samples	B
,	O
we	O
can	O
achieve	O
any	O
desired	O
level	O
of	O
accuracy	O
we	O
like	O
.	O
the	O
main	O
issue	O
is	O
:	O
how	O
do	O
we	O
efficiently	O
generate	O
samples	B
from	O
a	O
probability	O
distribution	O
,	O
particularly	O
in	O
high	O
dimensions	O
?	O
in	O
this	O
chapter	O
,	O
we	O
discuss	O
non-iterative	O
methods	O
for	O
generating	O
independent	O
samples	O
.	O
in	O
the	O
next	O
chapter	O
,	O
we	O
discuss	O
an	O
iterative	O
method	O
known	O
as	O
markov	O
chain	O
monte	O
carlo	O
,	O
or	O
mcmc	O
for	O
short	O
,	O
which	O
produces	O
dependent	O
samples	B
but	O
which	O
works	O
well	O
in	O
high	O
dimensions	O
.	O
note	O
that	O
sampling	O
is	O
a	O
large	O
topic	O
.	O
the	O
reader	O
should	O
consult	O
other	O
books	O
,	O
such	O
as	O
(	O
liu	O
2001	O
;	O
robert	O
and	O
casella	O
2004	O
)	O
,	O
for	O
more	O
information	B
.	O
23.2	O
sampling	O
from	O
standard	O
distributions	O
we	O
brieﬂy	O
discuss	O
some	O
ways	O
to	O
sample	O
from	O
1	O
or	O
2	O
dimensional	O
distributions	O
of	O
standard	O
form	O
.	O
these	O
methods	O
are	O
often	O
used	O
as	O
subroutines	O
by	O
more	O
complex	O
methods	O
.	O
23.2.1	O
using	O
the	O
cdf	B
the	O
simplest	O
method	O
for	O
sampling	O
from	O
a	O
univariate	O
distribution	O
is	O
based	O
on	O
the	O
inverse	O
prob-	O
ability	O
transform	O
.	O
let	O
f	O
be	O
a	O
cdf	B
of	O
some	O
distribution	O
we	O
want	O
to	O
sample	O
from	O
,	O
and	O
let	O
f	O
−1	O
816	O
chapter	O
23.	O
monte	O
carlo	O
inference	B
f	O
1	O
u	O
0	O
0	O
x	O
figure	O
23.1	O
sampling	O
using	O
an	O
inverse	O
cdf	O
.	O
figure	O
generated	O
by	O
samplecdf	O
.	O
be	O
its	O
inverse	O
.	O
then	O
we	O
have	O
the	O
following	O
result	O
.	O
theorem	O
23.2.1.	O
if	O
u	O
∼	O
u	O
(	O
0	O
,	O
1	O
)	O
is	O
a	O
uniform	O
rv	O
,	O
then	O
f	O
−1	O
(	O
u	O
)	O
∼	O
f	O
.	O
proof	O
.	O
pr	O
(	O
f	O
−1	O
(	O
u	O
)	O
≤	O
x	O
)	O
=	O
pr	O
(	O
u	O
≤	O
f	O
(	O
x	O
)	O
)	O
(	O
applying	O
f	O
to	O
both	O
sides	O
)	O
=	O
f	O
(	O
x	O
)	O
(	O
because	O
pr	O
(	O
u	O
≤	O
y	O
)	O
=	O
y	O
(	O
23.1	O
)	O
(	O
23.2	O
)	O
where	O
the	O
ﬁrst	O
line	O
follows	O
since	O
f	O
is	O
a	O
monotonic	O
function	O
,	O
and	O
the	O
second	O
line	O
follows	O
since	O
u	O
is	O
uniform	O
on	O
the	O
unit	O
interval	O
.	O
hence	O
we	O
can	O
sample	O
from	O
any	O
univariate	O
distribution	O
,	O
for	O
which	O
we	O
can	O
evaluate	O
its	O
inverse	O
cdf	O
,	O
as	O
follows	O
:	O
generate	O
a	O
random	O
number	O
u	O
∼	O
u	O
(	O
0	O
,	O
1	O
)	O
using	O
a	O
pseudo	B
random	I
number	I
generator	I
(	O
see	O
e.g.	O
,	O
(	O
press	O
et	O
al	O
.	O
1988	O
)	O
for	O
details	O
)	O
.	O
let	O
u	O
represent	O
the	O
height	O
up	O
the	O
y	O
axis	O
.	O
then	O
“	O
slide	O
along	O
”	O
the	O
x	O
axis	O
until	O
you	O
intersect	O
the	O
f	O
curve	O
,	O
and	O
then	O
“	O
drop	O
down	O
”	O
and	O
return	O
the	O
corresponding	O
x	O
value	O
.	O
this	O
corresponds	O
to	O
computing	O
x	O
=	O
f	O
−1	O
(	O
u	O
)	O
.	O
see	O
figure	O
23.1	O
for	O
an	O
illustration	O
.	O
for	O
example	O
,	O
consider	O
the	O
exponential	O
distribution	O
expon	O
(	O
x|λ	O
)	O
(	O
cid:2	O
)	O
λe−λx	O
i	O
(	O
x	O
≥	O
0	O
)	O
the	O
cdf	B
is	O
f	O
(	O
x	O
)	O
=	O
1	O
−	O
e−λx	O
i	O
(	O
x	O
≥	O
0	O
)	O
(	O
23.3	O
)	O
(	O
23.4	O
)	O
whose	O
inverse	O
is	O
the	O
quantile	B
function	O
f	O
−1	O
(	O
p	O
)	O
=	O
−	O
ln	O
(	O
1	O
−	O
p	O
)	O
λ	O
(	O
23.5	O
)	O
by	O
the	O
above	O
theorem	O
,	O
if	O
u	O
∼	O
unif	O
(	O
0	O
,	O
1	O
)	O
,	O
we	O
know	O
that	O
f	O
−1	O
(	O
u	O
)	O
∼	O
expon	O
(	O
λ	O
)	O
.	O
furthermore	O
,	O
since	O
1	O
−	O
u	O
∼	O
unif	O
(	O
0	O
,	O
1	O
)	O
as	O
well	O
,	O
we	O
can	O
sample	O
from	O
the	O
exponential	O
distribution	O
by	O
ﬁrst	O
sampling	O
from	O
the	O
uniform	O
and	O
then	O
transforming	O
the	O
results	O
using	O
−	O
ln	O
(	O
u	O
)	O
/λ	O
.	O
23.3.	O
rejection	B
sampling	I
817	O
23.2.2	O
sampling	O
from	O
a	O
gaussian	O
(	O
box-muller	O
method	O
)	O
we	O
now	O
describe	O
a	O
method	O
to	O
sample	O
from	O
a	O
gaussian	O
.	O
the	O
idea	O
is	O
we	O
sample	O
uniformly	O
from	O
a	O
unit	O
radius	O
circle	O
,	O
and	O
then	O
use	O
the	O
change	B
of	I
variables	I
formula	O
to	O
derive	O
samples	B
from	O
a	O
spherical	B
2d	O
gaussian	O
.	O
this	O
can	O
be	O
thought	O
of	O
as	O
two	O
samples	B
from	O
a	O
1d	O
gaussian	O
.	O
in	O
more	O
detail	O
,	O
sample	O
z1	O
,	O
z2	O
∈	O
(	O
−1	O
,	O
1	O
)	O
uniformly	O
,	O
and	O
then	O
discard	O
pairs	O
that	O
do	O
not	O
satisfy	O
2	O
≤	O
1.	O
the	O
result	O
will	O
be	O
points	O
uniformly	O
distributed	O
inside	O
the	O
unit	O
circle	O
,	O
so	O
p	O
(	O
z	O
)	O
=	O
z2	O
1	O
+	O
z2	O
1	O
π	O
i	O
(	O
z	O
inside	O
circle	O
)	O
.	O
now	O
deﬁne	O
(	O
cid:8	O
)	O
−2	O
lnr	O
2	O
(	O
cid:9	O
)	O
1	O
2	O
r2	O
xi	O
=	O
zi	O
for	O
i	O
=	O
1	O
:	O
2	O
,	O
wherer	O
2	O
=	O
z2	O
1	O
+	O
z2	O
p	O
(	O
x1	O
,	O
x2	O
)	O
=	O
p	O
(	O
z1	O
,	O
z2	O
)	O
|	O
∂	O
(	O
z1	O
,	O
z2	O
)	O
∂	O
(	O
x1	O
,	O
x2	O
)	O
2.	O
using	O
the	O
multivariate	O
change	O
of	O
variables	O
formula	O
,	O
we	O
have	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
(	O
cid:17	O
)	O
|	O
=	O
1√	O
2π	O
exp	O
(	O
−	O
1	O
2	O
x2	O
1	O
)	O
1√	O
2π	O
exp	O
(	O
−	O
1	O
2	O
x2	O
2	O
)	O
(	O
cid:18	O
)	O
(	O
23.6	O
)	O
(	O
23.7	O
)	O
hence	O
x1	O
and	O
x2	O
are	O
two	O
independent	O
samples	O
from	O
a	O
univariate	O
gaussian	O
.	O
this	O
is	O
known	O
as	O
the	O
box-muller	O
method	O
.	O
to	O
sample	O
from	O
a	O
multivariate	O
gaussian	O
,	O
we	O
ﬁrst	O
compute	O
the	O
cholesky	O
decomposition	O
of	O
its	O
covariance	B
matrix	I
,	O
σ	O
=	O
llt	O
,	O
where	O
l	O
is	O
lower	O
triangular	O
.	O
next	O
we	O
sample	O
x	O
∼	O
n	O
(	O
0	O
,	O
i	O
)	O
using	O
the	O
box-muller	O
method	O
.	O
finally	O
we	O
set	O
y	O
=	O
lx	O
+	O
μ.	O
this	O
is	O
valid	O
since	O
cov	O
[	O
y	O
]	O
=	O
lcov	O
[	O
x	O
]	O
lt	O
=	O
l	O
i	O
lt	O
=	O
σ	O
(	O
23.8	O
)	O
23.3	O
rejection	B
sampling	I
when	O
the	O
inverse	O
cdf	O
method	O
can	O
not	O
be	O
used	O
,	O
one	O
simple	O
alternative	O
is	O
to	O
use	O
rejection	O
sam-	O
pling	O
,	O
which	O
we	O
now	O
explain	O
.	O
23.3.1	O
basic	O
idea	O
in	O
rejection	B
sampling	I
,	O
we	O
create	O
a	O
proposal	B
distribution	I
q	O
(	O
x	O
)	O
which	O
satisifes	O
m	O
q	O
(	O
x	O
)	O
≥	O
˜p	O
(	O
x	O
)	O
,	O
for	O
some	O
constant	O
m	O
,	O
where	O
˜p	O
(	O
x	O
)	O
is	O
an	O
unnormalized	O
version	O
of	O
p	O
(	O
x	O
)	O
(	O
i.e.	O
,	O
p	O
(	O
x	O
)	O
=	O
˜p	O
(	O
x	O
)	O
/zp	O
for	O
some	O
possibly	O
unknown	B
constant	O
zp	O
)	O
.	O
the	O
function	O
m	O
q	O
(	O
x	O
)	O
provides	O
an	O
upper	O
envelope	O
for	O
˜p	O
.	O
we	O
then	O
sample	O
x	O
∼	O
q	O
(	O
x	O
)	O
,	O
which	O
corresponds	O
to	O
picking	O
a	O
random	O
x	O
location	O
,	O
and	O
then	O
we	O
sample	O
u	O
∼	O
u	O
(	O
0	O
,	O
1	O
)	O
,	O
which	O
corresponds	O
to	O
picking	O
a	O
random	O
height	O
(	O
y	O
location	O
)	O
under	O
the	O
envelope	O
.	O
if	O
u	O
>	O
˜p	O
(	O
x	O
)	O
m	O
q	O
(	O
x	O
)	O
,	O
we	O
reject	O
the	O
sample	O
,	O
otherwise	O
we	O
accept	B
it	O
.	O
see	O
figure	O
23.2	O
(	O
a	O
)	O
.	O
where	O
the	O
acceptance	O
region	O
is	O
shown	O
shaded	O
,	O
and	O
the	O
rejection	O
region	O
is	O
the	O
white	O
region	O
between	O
the	O
shaded	O
zone	O
and	O
the	O
upper	O
envelope	O
.	O
we	O
now	O
prove	O
that	O
this	O
procedure	O
is	O
correct	O
.	O
let	O
s	O
=	O
{	O
(	O
x	O
,	O
u	O
)	O
:	O
u	O
≤	O
˜p	O
(	O
x	O
)	O
/m	O
q	O
(	O
x	O
)	O
}	O
,	O
s0	O
=	O
{	O
(	O
x	O
,	O
u	O
)	O
:	O
x	O
≤	O
x0	O
,	O
u	O
≤	O
˜p	O
(	O
x	O
)	O
/m	O
q	O
(	O
x	O
)	O
}	O
(	O
23.9	O
)	O
818	O
chapter	O
23.	O
monte	O
carlo	O
inference	B
(	O
i	O
)	O
mq	O
(	O
x	O
)	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
                              	O
	O
accept	B
region	O
(	O
i	O
)	O
umq	O
(	O
x	O
)	O
(	O
i	O
)	O
p	O
(	O
x	O
)	O
reject	O
region	O
target	O
p	O
(	O
x	O
)	O
comparison	O
function	O
mq	O
(	O
x	O
)	O
1.4	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
(	O
i	O
)	O
x	O
~	O
q	O
(	O
x	O
)	O
(	O
a	O
)	O
x	O
0	O
0	O
2	O
4	O
6	O
8	O
10	O
(	O
b	O
)	O
figure	O
23.2	O
(	O
a	O
)	O
schematic	O
illustration	O
of	O
rejection	B
sampling	I
.	O
source	O
:	O
figure	O
2	O
of	O
(	O
andrieu	O
et	O
al	O
.	O
2003	O
)	O
.	O
(	O
b	O
)	O
rejection	B
sampling	I
from	O
a	O
ga	O
(	O
α	O
=	O
5.7	O
,	O
λ	O
=	O
2	O
)	O
used	O
with	O
kind	O
permission	O
of	O
nando	O
de	O
freitas	O
.	O
distribution	O
(	O
solid	O
blue	O
)	O
using	O
a	O
proposal	O
of	O
the	O
form	O
m	O
ga	O
(	O
k	O
,	O
λ	O
−	O
1	O
)	O
(	O
dotted	O
red	O
)	O
,	O
where	O
k	O
=	O
(	O
cid:15	O
)	O
5.7	O
(	O
cid:16	O
)	O
=	O
5.	O
the	O
curves	O
touch	O
at	O
α	O
−	O
k	O
=	O
0.7.	O
figure	O
generated	O
by	O
rejectionsamplingdemo	O
.	O
then	O
the	O
cdf	B
of	O
the	O
accepted	O
points	O
is	O
given	O
by	O
p	O
(	O
x	O
≤	O
x0|x	O
accepted	O
)	O
=	O
=	O
p	O
(	O
x	O
≤	O
x0	O
,	O
x	O
accepted	O
)	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
p	O
(	O
x	O
accepted	O
)	O
i	O
(	O
(	O
x	O
,	O
u	O
)	O
∈	O
s0	O
)	O
q	O
(	O
x	O
)	O
dudx	O
i	O
(	O
(	O
x	O
,	O
u	O
)	O
∈	O
s	O
)	O
q	O
(	O
x	O
)	O
dudx	O
(	O
cid:21	O
)	O
x0−∞	O
˜p	O
(	O
x	O
)	O
dx	O
(	O
cid:21	O
)	O
∞	O
−∞	O
˜p	O
(	O
x	O
)	O
dx	O
=	O
(	O
23.10	O
)	O
(	O
23.11	O
)	O
which	O
is	O
the	O
cdf	B
of	O
p	O
(	O
x	O
)	O
,	O
as	O
desired	O
.	O
how	O
efficient	O
is	O
this	O
method	O
?	O
since	O
we	O
generate	O
with	O
probability	O
q	O
(	O
x	O
)	O
and	O
accept	B
with	O
probability	O
˜p	O
(	O
x	O
)	O
m	O
q	O
(	O
x	O
)	O
,	O
the	O
probability	O
of	O
acceptance	O
is	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
p	O
(	O
accept	B
)	O
=	O
˜p	O
(	O
x	O
)	O
m	O
q	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
dx	O
=	O
1	O
m	O
hence	O
we	O
want	O
to	O
choose	O
m	O
as	O
small	O
as	O
possible	O
while	O
still	O
satisfying	O
m	O
q	O
(	O
x	O
)	O
≥	O
˜p	O
(	O
x	O
)	O
.	O
˜p	O
(	O
x	O
)	O
dx	O
(	O
23.12	O
)	O
23.3.2	O
example	O
for	O
example	O
,	O
suppose	O
we	O
want	O
to	O
sample	O
from	O
a	O
gamma	O
distribution:1	O
ga	O
(	O
x|α	O
,	O
λ	O
)	O
=	O
1	O
γ	O
(	O
α	O
)	O
xα−1λα	O
exp	O
(	O
−λx	O
)	O
(	O
23.13	O
)	O
iid∼	O
expon	O
(	O
λ	O
)	O
,	O
and	O
y	O
=	O
x1	O
+	O
···	O
+	O
xk	O
,	O
then	O
y	O
∼	O
ga	O
(	O
k	O
,	O
λ	O
)	O
.	O
for	O
one	O
can	O
show	O
that	O
if	O
xi	O
non-integer	O
shape	O
parameters	O
,	O
we	O
can	O
not	O
use	O
this	O
trick	O
.	O
however	O
,	O
we	O
can	O
use	O
rejection	B
sampling	I
1.	O
this	O
section	O
is	O
based	O
on	O
notes	O
by	O
ioana	O
a.	O
cosma	O
,	O
available	O
at	O
http	O
:	O
//users.aims.ac.za/~ioana/cp2.pdf	O
.	O
23.3.	O
rejection	B
sampling	I
819	O
f	O
(	O
x	O
)	O
half−gaussian	O
samples	B
from	O
f	O
(	O
x	O
)	O
(	O
by	O
ars	O
)	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
−8	O
−6	O
−4	O
−2	O
0	O
2	O
4	O
6	O
8	O
1000	O
900	O
800	O
700	O
600	O
500	O
400	O
300	O
200	O
100	O
0	O
−8	O
−6	O
−4	O
−2	O
0	O
2	O
4	O
6	O
8	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
23.3	O
(	O
a	O
)	O
idea	O
behind	O
adaptive	B
rejection	I
sampling	I
.	O
we	O
place	O
piecewise	O
linear	O
upper	O
(	O
and	O
lower	O
)	O
bounds	O
on	O
the	O
log-concave	O
density	O
.	O
based	O
on	O
figure	O
1	O
of	O
(	O
gilks	O
and	O
wild	O
1992	O
)	O
.	O
figure	O
generated	O
by	O
arsenvelope	O
.	O
(	O
b-c	O
)	O
using	O
ars	O
to	O
sample	O
from	O
a	O
half-gaussian	O
.	O
figure	O
generated	O
by	O
arsdemo	O
,	O
written	O
by	O
daniel	O
eaton	O
.	O
using	O
a	O
ga	O
(	O
k	O
,	O
λ	O
−	O
1	O
)	O
distribution	O
as	O
a	O
proposal	O
,	O
where	O
k	O
=	O
(	O
cid:28	O
)	O
α	O
(	O
cid:29	O
)	O
.	O
the	O
ratio	O
has	O
the	O
form	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
=	O
=	O
ga	O
(	O
x|α	O
,	O
λ	O
)	O
ga	O
(	O
x|k	O
,	O
λ	O
−	O
1	O
)	O
γ	O
(	O
α	O
)	O
(	O
λ	O
−	O
1	O
)	O
k	O
γ	O
(	O
k	O
)	O
λα	O
=	O
xα−k	O
exp	O
(	O
−x	O
)	O
xα−1λα	O
exp	O
(	O
−λx	O
)	O
/γ	O
(	O
α	O
)	O
xk−1	O
(	O
λ	O
−	O
1	O
)	O
k	O
exp	O
(	O
−	O
(	O
λ	O
−	O
1	O
)	O
x	O
)	O
/γ	O
(	O
k	O
)	O
this	O
ratio	O
attains	O
its	O
maximum	O
when	O
x	O
=	O
α	O
−	O
k.	O
hence	O
m	O
=	O
ga	O
(	O
α	O
−	O
k|α	O
,	O
λ	O
)	O
ga	O
(	O
α	O
−	O
k|k	O
,	O
λ	O
−	O
1	O
)	O
see	O
figure	O
23.2	O
(	O
b	O
)	O
for	O
a	O
plot	O
.	O
based	O
on	O
the	O
cauchy	O
distribution	O
.	O
)	O
(	O
exercise	O
23.2	O
asks	O
you	O
to	O
devise	O
a	O
better	O
proposal	B
distribution	I
23.3.3	O
application	O
to	O
bayesian	O
statistics	O
suppose	O
we	O
want	O
to	O
draw	O
(	O
unweighted	O
)	O
samples	B
from	O
the	O
posterior	O
,	O
p	O
(	O
θ|d	O
)	O
=	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
/p	O
(	O
d	O
)	O
.	O
we	O
can	O
use	O
rejection	B
sampling	I
with	O
˜p	O
(	O
θ	O
)	O
=	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
as	O
the	O
target	O
distribution	O
,	O
q	O
(	O
θ	O
)	O
=	O
p	O
(	O
θ	O
)	O
as	O
our	O
proposal	O
,	O
and	O
m	O
=	O
p	O
(	O
d|ˆθ	O
)	O
,	O
where	O
ˆθ	O
=	O
arg	O
max	O
p	O
(	O
d|θ	O
)	O
is	O
the	O
mle	O
;	O
this	O
was	O
ﬁrst	O
suggested	O
in	O
(	O
smith	O
and	O
gelfand	O
1992	O
)	O
.	O
we	O
accept	B
points	O
with	O
probability	O
(	O
23.14	O
)	O
(	O
23.15	O
)	O
(	O
23.16	O
)	O
(	O
23.17	O
)	O
˜p	O
(	O
θ	O
)	O
m	O
q	O
(	O
θ	O
)	O
=	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
d|ˆθ	O
)	O
thus	O
samples	B
from	O
the	O
prior	O
that	O
have	O
high	O
likelihood	O
are	O
more	O
likely	O
to	O
be	O
retained	O
in	O
the	O
posterior	O
.	O
of	O
course	O
,	O
if	O
there	O
is	O
a	O
big	O
mismatch	O
between	O
prior	O
and	O
posterior	O
(	O
which	O
will	O
be	O
the	O
case	O
if	O
the	O
prior	O
is	O
vague	O
and	O
the	O
likelihood	B
is	O
informative	O
)	O
,	O
this	O
procedure	O
is	O
very	O
inefficient	O
.	O
we	O
discuss	O
better	O
algorithms	O
later	O
.	O
23.3.4	O
adaptive	B
rejection	I
sampling	I
we	O
now	O
describe	O
a	O
method	O
that	O
can	O
automatically	O
come	O
up	O
with	O
a	O
tight	O
upper	O
envelope	O
q	O
(	O
x	O
)	O
to	O
any	O
log	O
concave	O
density	O
p	O
(	O
x	O
)	O
.	O
the	O
idea	O
is	O
to	O
upper	O
bound	O
the	O
log	O
density	O
with	O
a	O
piecewise	O
820	O
chapter	O
23.	O
monte	O
carlo	O
inference	B
linear	O
function	O
,	O
as	O
illustrated	O
in	O
figure	O
23.3	O
(	O
a	O
)	O
.	O
we	O
choose	O
the	O
initial	O
locations	O
for	O
the	O
pieces	O
based	O
on	O
a	O
ﬁxed	O
grid	O
over	O
the	O
support	B
of	O
the	O
distribution	O
.	O
we	O
then	O
evaluate	O
the	O
gradient	O
of	O
the	O
log	O
density	O
at	O
these	O
locations	O
,	O
and	O
make	O
the	O
lines	O
be	O
tangent	O
at	O
these	O
points	O
.	O
since	O
the	O
log	O
of	O
the	O
envelope	O
is	O
piecewise	O
linear	O
,	O
the	O
envelope	O
itself	O
is	O
piecewise	O
exponential	O
:	O
q	O
(	O
x	O
)	O
=m	O
iλi	O
exp	O
(	O
−λi	O
(	O
x	O
−	O
xi−1	O
)	O
)	O
,	O
xi−1	O
<	O
x	O
≤	O
xi	O
(	O
23.18	O
)	O
where	O
xi	O
are	O
the	O
grid	O
points	O
.	O
it	O
is	O
relatively	O
straightforward	O
to	O
sample	O
from	O
this	O
distribution	O
.	O
if	O
the	O
sample	O
x	O
is	O
rejected	O
,	O
we	O
create	O
a	O
new	O
grid	O
point	O
at	O
x	O
,	O
and	O
thereby	O
reﬁne	O
the	O
envelope	O
.	O
as	O
the	O
number	O
of	O
grid	O
points	O
is	O
increased	O
,	O
the	O
tightness	O
of	O
the	O
envelope	O
improves	O
,	O
and	O
the	O
rejection	O
rate	O
goes	O
down	O
.	O
this	O
is	O
known	O
as	O
adaptive	B
rejection	I
sampling	I
(	O
ars	O
)	O
(	O
gilks	O
and	O
wild	O
1992	O
)	O
.	O
figure	O
23.3	O
(	O
b-c	O
)	O
gives	O
an	O
example	O
of	O
the	O
method	O
in	O
action	B
.	O
as	O
with	O
standard	O
rejection	O
sampling	O
,	O
it	O
can	O
be	O
applied	O
to	O
unnormalized	O
distributions	O
.	O
23.3.5	O
rejection	B
sampling	I
in	O
high	O
dimensions	O
it	O
is	O
clear	O
that	O
we	O
want	O
to	O
make	O
our	O
proposal	O
q	O
(	O
x	O
)	O
as	O
close	O
as	O
possible	O
to	O
the	O
target	O
distribution	O
p	O
(	O
x	O
)	O
,	O
while	O
still	O
being	O
an	O
upper	O
bound	O
.	O
but	O
this	O
is	O
quite	O
hard	O
to	O
achieve	O
,	O
especially	O
in	O
high	O
dimensions	O
.	O
to	O
see	O
this	O
,	O
consider	O
sampling	O
from	O
p	O
(	O
x	O
)	O
=n	O
(	O
0	O
,	O
σ2	O
pi	O
)	O
using	O
as	O
a	O
proposal	O
q	O
(	O
x	O
)	O
=	O
n	O
(	O
0	O
,	O
σ2	O
in	O
d	O
dimensions	O
,	O
the	O
optimum	O
value	O
is	O
given	O
by	O
m	O
=	O
(	O
σq/σp	O
)	O
d.	O
the	O
acceptance	O
rate	B
is	O
1/m	O
(	O
since	O
both	O
p	O
and	O
q	O
are	O
normalized	O
)	O
,	O
which	O
decreases	O
exponentially	O
fast	O
with	O
dimension	O
.	O
for	O
example	O
,	O
if	O
σq	O
exceeds	O
σp	O
by	O
just	O
1	O
%	O
,	O
then	O
in	O
1000	O
dimensions	O
the	O
acceptance	O
ratio	O
will	O
be	O
about	O
1/20,000	O
.	O
this	O
is	O
a	O
fundamental	O
weakness	O
of	O
rejection	B
sampling	I
.	O
q	O
i	O
)	O
.	O
obviously	O
we	O
must	O
have	O
σ2	O
q	O
≥	O
σ2	O
p	O
in	O
order	O
to	O
be	O
an	O
upper	O
bound	O
.	O
in	O
chapter	O
24	O
,	O
we	O
will	O
describe	O
mcmc	O
sampling	O
,	O
which	O
is	O
a	O
more	O
efficient	O
way	O
to	O
sample	O
from	O
high	O
dimensional	O
distributions	O
.	O
sometimes	O
this	O
uses	O
(	O
adaptive	O
)	O
rejection	B
sampling	I
as	O
a	O
subroutine	O
,	O
which	O
is	O
known	O
as	O
adaptive	O
rejection	O
metropolis	O
sampling	O
(	O
gilks	O
et	O
al	O
.	O
1995	O
)	O
.	O
23.4	O
importance	B
sampling	I
we	O
now	O
describe	O
a	O
monte	O
carlo	O
method	O
known	O
as	O
importance	B
sampling	I
for	O
approximating	O
integrals	O
of	O
the	O
form	O
(	O
cid:12	O
)	O
i	O
=	O
e	O
[	O
f	O
]	O
=	O
f	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
dx	O
(	O
23.19	O
)	O
23.4.1	O
basic	O
idea	O
the	O
idea	O
is	O
to	O
draw	O
samples	B
x	O
in	O
regions	O
which	O
have	O
high	O
probability	O
,	O
p	O
(	O
x	O
)	O
,	O
but	O
also	O
where	O
|f	O
(	O
x	O
)	O
|	O
is	O
large	O
.	O
the	O
result	O
can	O
be	O
super	B
efficient	I
,	O
meaning	O
it	O
needs	O
less	O
samples	B
than	O
if	O
we	O
were	O
to	O
sample	O
from	O
the	O
exact	O
distribution	O
p	O
(	O
x	O
)	O
.	O
the	O
reason	O
is	O
that	O
the	O
samples	B
are	O
focussed	O
on	O
the	O
important	O
parts	O
of	O
space	O
.	O
for	O
example	O
,	O
suppose	O
we	O
want	O
to	O
estimate	O
the	O
probability	O
of	O
a	O
rare	B
event	I
.	O
deﬁne	O
f	O
(	O
x	O
)	O
=i	O
(	O
x	O
∈	O
e	O
)	O
,	O
for	O
some	O
set	O
e.	O
then	O
it	O
is	O
better	O
to	O
sample	O
from	O
a	O
proposal	O
of	O
the	O
form	O
q	O
(	O
x	O
)	O
∝	O
f	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
than	O
to	O
sample	O
from	O
p	O
(	O
x	O
)	O
itself	O
.	O
importance	B
sampling	I
samples	O
from	O
any	O
proposal	O
,	O
q	O
(	O
x	O
)	O
.	O
it	O
then	O
uses	O
these	O
samples	B
to	O
estimate	O
23.4.	O
importance	B
sampling	I
the	O
integral	O
as	O
follows	O
:	O
(	O
cid:12	O
)	O
e	O
[	O
f	O
]	O
=	O
f	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
dx	O
≈	O
1	O
s	O
s	O
(	O
cid:4	O
)	O
s=1	O
wsf	O
(	O
xs	O
)	O
=	O
ˆi	O
821	O
(	O
23.20	O
)	O
q	O
(	O
xs	O
)	O
are	O
the	O
importance	B
weights	I
.	O
note	O
that	O
,	O
unlike	O
rejection	B
sampling	I
,	O
we	O
use	O
all	O
how	O
should	O
we	O
choose	O
the	O
proposal	O
?	O
a	O
natural	O
criterion	O
is	O
to	O
minimize	O
the	O
variance	B
of	O
the	O
where	O
ws	O
(	O
cid:2	O
)	O
p	O
(	O
xs	O
)	O
(	O
cid:7	O
)	O
the	O
samples	B
.	O
estimate	O
ˆi	O
=	O
s	O
wsf	O
(	O
xs	O
)	O
.	O
now	O
varq	O
(	O
x	O
)	O
[	O
f	O
(	O
x	O
)	O
w	O
(	O
x	O
)	O
]	O
=	O
eq	O
(	O
x	O
)	O
(	O
cid:13	O
)	O
eq	O
(	O
x	O
)	O
f	O
2	O
(	O
x	O
)	O
w2	O
(	O
x	O
)	O
(	O
cid:13	O
)	O
f	O
2	O
(	O
x	O
)	O
w2	O
(	O
x	O
)	O
(	O
cid:14	O
)	O
−	O
i	O
2	O
(	O
cid:14	O
)	O
≥	O
(	O
eq	O
(	O
x	O
)	O
[	O
|f	O
(	O
x	O
)	O
w	O
(	O
x	O
)	O
|	O
]	O
)	O
2	O
=	O
(	O
cid:8	O
)	O
(	O
cid:12	O
)	O
(	O
cid:9	O
)	O
2	O
|f	O
(	O
x	O
)	O
|p	O
(	O
x	O
)	O
dx	O
since	O
the	O
last	O
term	O
is	O
independent	O
of	O
q	O
,	O
we	O
can	O
ignore	O
it	O
.	O
by	O
jensen	O
’	O
s	O
inequality	O
,	O
we	O
have	O
the	O
following	O
lower	O
bound	O
:	O
the	O
lower	O
bound	O
is	O
obtained	O
when	O
we	O
use	O
the	O
optimal	O
importance	O
distribution	O
:	O
q∗	O
(	O
x	O
)	O
=	O
(	O
cid:21	O
)	O
|f	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
|p	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
dx	O
(	O
cid:2	O
)	O
|f	O
(	O
x	O
)	O
|p	O
(	O
x	O
)	O
(	O
23.21	O
)	O
(	O
23.22	O
)	O
(	O
23.23	O
)	O
when	O
we	O
don	O
’	O
t	O
have	O
a	O
particular	O
target	O
function	O
f	O
(	O
x	O
)	O
in	O
mind	O
,	O
we	O
often	O
just	O
try	O
to	O
make	O
q	O
(	O
x	O
)	O
as	O
close	O
as	O
possible	O
to	O
p	O
(	O
x	O
)	O
.	O
in	O
general	O
,	O
this	O
is	O
difficult	O
,	O
especially	O
in	O
high	O
dimensions	O
,	O
but	O
it	O
is	O
possible	O
to	O
adapt	O
the	O
proposal	B
distribution	I
to	O
improve	O
the	O
approximation	O
.	O
this	O
is	O
known	O
as	O
adaptive	B
importance	I
sampling	I
(	O
oh	O
and	O
berger	O
1992	O
)	O
.	O
23.4.2	O
handling	O
unnormalized	O
distributions	O
it	O
is	O
frequently	O
the	O
case	O
that	O
we	O
can	O
evaluate	O
the	O
unnormalized	O
target	O
distribution	O
,	O
˜p	O
(	O
x	O
)	O
,	O
but	O
not	O
its	O
normalization	O
constant	O
,	O
zp	O
.	O
we	O
may	O
also	O
want	O
to	O
use	O
an	O
unnormalized	O
proposal	O
,	O
˜q	O
(	O
x	O
)	O
,	O
with	O
possibly	O
unknown	B
normlization	O
constant	O
zq	O
.	O
we	O
can	O
do	O
this	O
as	O
follows	O
.	O
first	O
we	O
evaluate	O
(	O
cid:12	O
)	O
e	O
[	O
f	O
]	O
=	O
zq	O
zp	O
f	O
(	O
x	O
)	O
˜p	O
(	O
x	O
)	O
˜q	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
dx	O
≈	O
zq	O
zp	O
1	O
s	O
˜wsf	O
(	O
xs	O
)	O
(	O
23.24	O
)	O
˜q	O
(	O
xs	O
)	O
is	O
the	O
unnormalized	O
importance	O
weight	O
.	O
we	O
can	O
use	O
the	O
same	O
set	O
of	O
samples	B
s	O
(	O
cid:4	O
)	O
s=1	O
s	O
(	O
cid:4	O
)	O
s=1	O
where	O
˜ws	O
(	O
cid:2	O
)	O
˜p	O
(	O
xs	O
)	O
(	O
cid:12	O
)	O
to	O
evaluate	O
the	O
ratio	O
zp/zq	O
as	O
follows	O
:	O
(	O
cid:12	O
)	O
˜p	O
(	O
x	O
)	O
dx	O
=	O
˜p	O
(	O
x	O
)	O
˜q	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
dx	O
≈	O
1	O
s	O
zp	O
zq	O
=	O
hence	O
1	O
s	O
ˆi	O
=	O
1	O
zq	O
(	O
cid:7	O
)	O
1	O
s	O
(	O
cid:7	O
)	O
s	O
˜wsf	O
(	O
xs	O
)	O
s	O
˜ws	O
s	O
(	O
cid:4	O
)	O
s=1	O
=	O
wsf	O
(	O
xs	O
)	O
˜ws	O
(	O
23.25	O
)	O
(	O
23.26	O
)	O
822	O
where	O
ws	O
(	O
cid:2	O
)	O
˜ws	O
(	O
cid:7	O
)	O
s	O
(	O
cid:2	O
)	O
˜ws	O
(	O
cid:2	O
)	O
chapter	O
23.	O
monte	O
carlo	O
inference	B
(	O
23.27	O
)	O
are	O
the	O
normalized	O
importance	O
weights	O
.	O
the	O
resulting	O
estimate	O
is	O
a	O
ratio	O
of	O
two	O
estimates	O
,	O
and	O
hence	O
is	O
biased	O
.	O
however	O
,	O
as	O
s	O
→	O
∞	O
,	O
we	O
have	O
that	O
ˆi	O
→	O
i	O
,	O
under	O
weak	O
assumptions	O
(	O
see	O
e.g.	O
,	O
(	O
robert	O
and	O
casella	O
2004	O
)	O
for	O
details	O
)	O
.	O
23.4.3	O
importance	B
sampling	I
for	O
a	O
dgm	O
:	O
likelihood	B
weighting	I
we	O
now	O
describe	O
a	O
way	O
to	O
use	O
importance	B
sampling	I
to	O
generate	O
samples	O
from	O
a	O
distribution	O
which	O
can	O
be	O
represented	O
as	O
a	O
directed	B
graphical	I
model	I
(	O
chapter	O
10	O
)	O
.	O
if	O
we	O
have	O
no	O
evidence	O
,	O
we	O
can	O
sample	O
from	O
the	O
unconditional	O
joint	B
distribution	I
of	O
a	O
dgm	O
p	O
(	O
x	O
)	O
as	O
follows	O
:	O
ﬁrst	O
sample	O
the	O
root	B
nodes	O
,	O
then	O
sample	O
their	O
children	B
,	O
then	O
sample	O
their	O
children	B
,	O
etc	O
.	O
this	O
is	O
known	O
as	O
ancestral	B
sampling	I
.	O
it	O
works	O
because	O
,	O
in	O
a	O
dag	O
,	O
we	O
can	O
always	O
topologically	O
order	O
the	O
nodes	B
so	O
that	O
parents	B
preceed	O
children	B
.	O
(	O
note	O
that	O
there	O
is	O
no	O
equivalent	O
easy	O
method	O
for	O
sampling	O
from	O
an	O
unconditional	O
undirected	B
graphical	I
model	I
.	O
)	O
now	O
suppose	O
we	O
have	O
some	O
evidence	B
,	O
so	O
some	O
nodes	B
are	O
“	O
clamped	O
”	O
to	O
observed	O
values	O
,	O
and	O
we	O
want	O
to	O
sample	O
from	O
the	O
posterior	O
p	O
(	O
x|d	O
)	O
.	O
if	O
all	O
the	O
variables	O
are	O
discrete	B
,	O
we	O
can	O
use	O
the	O
following	O
simple	O
procedure	O
:	O
perform	O
ancestral	B
sampling	I
,	O
but	O
as	O
soon	O
as	O
we	O
sample	O
a	O
value	O
that	O
is	O
inconsistent	O
with	O
an	O
observed	O
value	O
,	O
reject	O
the	O
whole	O
sample	O
and	O
start	O
again	O
.	O
this	O
is	O
known	O
as	O
logic	B
sampling	I
(	O
henrion	O
1988	O
)	O
.	O
needless	O
to	O
say	O
,	O
logic	B
sampling	I
is	O
very	O
inefficient	O
,	O
and	O
it	O
can	O
not	O
be	O
applied	O
when	O
we	O
have	O
real-valued	O
evidence	B
.	O
however	O
,	O
it	O
can	O
be	O
modiﬁed	O
as	O
follows	O
.	O
sample	O
unobserved	O
variables	O
as	O
before	O
,	O
conditional	O
on	O
their	O
parents	B
.	O
but	O
don	O
’	O
t	O
sample	O
observed	O
variables	O
;	O
instead	O
we	O
just	O
use	O
their	O
observed	O
values	O
.	O
this	O
is	O
equivalent	O
to	O
using	O
a	O
proposal	O
of	O
the	O
form	O
q	O
(	O
x	O
)	O
=	O
p	O
(	O
xt|xpa	O
(	O
t	O
)	O
)	O
δx∗	O
t	O
(	O
xt	O
)	O
(	O
23.28	O
)	O
(	O
cid:20	O
)	O
t∈e	O
(	O
cid:20	O
)	O
t	O
(	O
cid:5	O
)	O
∈e	O
where	O
e	O
is	O
the	O
set	O
of	O
observed	O
nodes	O
,	O
and	O
x∗	O
therefore	O
give	O
the	O
overall	O
sample	O
an	O
importance	O
weight	O
as	O
follows	O
:	O
t	O
is	O
the	O
observed	O
value	O
for	O
node	O
t.	O
we	O
should	O
w	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
=	O
p	O
(	O
xt|xpa	O
(	O
t	O
)	O
)	O
p	O
(	O
xt|xpa	O
(	O
t	O
)	O
)	O
p	O
(	O
xt|xpa	O
(	O
t	O
)	O
)	O
t∈e	O
1	O
(	O
cid:20	O
)	O
t	O
(	O
cid:5	O
)	O
∈e	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
=	O
t∈e	O
p	O
(	O
xt|xpa	O
(	O
t	O
)	O
)	O
(	O
23.29	O
)	O
this	O
technique	O
is	O
known	O
as	O
likelihood	B
weighting	I
(	O
fung	O
and	O
chang	O
1989	O
;	O
shachter	O
and	O
peot	O
1989	O
)	O
.	O
23.4.4	O
sampling	B
importance	I
resampling	I
(	O
sir	O
)	O
we	O
can	O
draw	O
unweighted	O
samples	B
from	O
p	O
(	O
x	O
)	O
by	O
ﬁrst	O
using	O
importance	B
sampling	I
(	O
with	O
proposal	O
q	O
)	O
to	O
generate	O
a	O
distribution	O
of	O
the	O
form	O
wsδxs	O
(	O
x	O
)	O
(	O
23.30	O
)	O
(	O
cid:4	O
)	O
s	O
p	O
(	O
x	O
)	O
≈	O
23.5.	O
particle	B
ﬁltering	I
823	O
where	O
ws	O
are	O
the	O
normalized	O
importance	O
weights	O
.	O
we	O
then	O
sample	O
with	O
replacement	O
from	O
equation	O
23.30	O
,	O
where	O
the	O
probability	O
that	O
we	O
pick	O
xs	O
is	O
ws	O
.	O
let	O
this	O
procedure	O
induce	O
a	O
distribution	O
denoted	O
by	O
ˆp	O
.	O
to	O
see	O
that	O
this	O
is	O
valid	O
,	O
note	O
that	O
(	O
cid:7	O
)	O
s	O
i	O
(	O
xs	O
≤	O
x0	O
)	O
˜p	O
(	O
xs	O
)	O
/q	O
(	O
xs	O
)	O
s	O
˜p	O
(	O
xs	O
)	O
/q	O
(	O
xs	O
)	O
(	O
cid:4	O
)	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
s	O
(	O
cid:7	O
)	O
(	O
cid:12	O
)	O
˜p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
dx	O
i	O
(	O
xs	O
≤	O
x0	O
)	O
ws	O
=	O
(	O
cid:21	O
)	O
˜p	O
(	O
x	O
)	O
i	O
(	O
x	O
≤	O
x0	O
)	O
q	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
dx	O
(	O
cid:21	O
)	O
i	O
(	O
x	O
≤	O
x0	O
)	O
˜p	O
(	O
x	O
)	O
dx	O
˜p	O
(	O
x	O
)	O
dx	O
i	O
(	O
x	O
≤	O
x0	O
)	O
p	O
(	O
x	O
)	O
dx	O
=	O
p	O
(	O
x	O
≤	O
x0	O
)	O
=	O
ˆp	O
(	O
x	O
≤	O
x0	O
)	O
=	O
→	O
=	O
s	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
s=1	O
p	O
(	O
x	O
)	O
≈	O
1	O
s	O
(	O
cid:2	O
)	O
(	O
23.31	O
)	O
(	O
23.32	O
)	O
(	O
23.33	O
)	O
(	O
23.35	O
)	O
this	O
is	O
known	O
as	O
sampling	B
importance	I
resampling	I
(	O
sir	O
)	O
(	O
rubin	O
1998	O
)	O
.	O
the	O
result	O
is	O
an	O
un-	O
weighted	O
approximation	O
of	O
the	O
form	O
δxs	O
(	O
x	O
)	O
(	O
23.34	O
)	O
note	O
that	O
we	O
typically	O
take	O
s	O
(	O
cid:2	O
)	O
(	O
cid:22	O
)	O
s.	O
this	O
algorithm	O
can	O
be	O
used	O
to	O
perform	O
bayesian	O
inference	B
in	O
low-dimensional	O
settings	O
(	O
smith	O
and	O
gelfand	O
1992	O
)	O
.	O
that	O
is	O
,	O
suppose	O
we	O
want	O
to	O
draw	O
(	O
unweighted	O
)	O
samples	B
from	O
the	O
posterior	O
,	O
p	O
(	O
θ|d	O
)	O
=	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
/p	O
(	O
d	O
)	O
.	O
we	O
can	O
use	O
importance	B
sampling	I
with	O
˜p	O
(	O
θ	O
)	O
=	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
as	O
the	O
unnormalized	O
posterior	O
,	O
and	O
q	O
(	O
θ	O
)	O
=	O
p	O
(	O
θ	O
)	O
as	O
our	O
proposal	O
.	O
the	O
normalized	O
weights	O
have	O
the	O
form	O
(	O
cid:7	O
)	O
ws	O
=	O
p	O
(	O
d|θs	O
)	O
s	O
(	O
cid:2	O
)	O
p	O
(	O
d|θs	O
(	O
cid:2	O
)	O
)	O
we	O
can	O
then	O
use	O
sir	O
to	O
sample	O
from	O
p	O
(	O
θ|d	O
)	O
.	O
˜p	O
(	O
θs	O
)	O
/q	O
(	O
θs	O
)	O
s	O
(	O
cid:2	O
)	O
˜p	O
(	O
θs	O
(	O
cid:2	O
)	O
)	O
/q	O
(	O
θs	O
(	O
cid:2	O
)	O
)	O
=	O
(	O
cid:7	O
)	O
of	O
course	O
,	O
if	O
there	O
is	O
a	O
big	O
discrepancy	O
between	O
our	O
proposal	O
(	O
the	O
prior	O
)	O
and	O
the	O
target	O
(	O
the	O
posterior	O
)	O
,	O
we	O
will	O
need	O
a	O
huge	O
number	O
of	O
importance	O
samples	O
for	O
this	O
technique	O
to	O
work	O
reliably	O
,	O
since	O
otherwise	O
the	O
variance	B
of	O
the	O
importance	B
weights	I
will	O
be	O
very	O
large	O
,	O
implying	O
that	O
most	O
samples	B
carry	O
no	O
useful	O
information	B
.	O
(	O
this	O
issue	O
will	O
come	O
up	O
again	O
in	O
section	O
23.5	O
,	O
when	O
we	O
discuss	O
particle	B
ﬁltering	I
.	O
)	O
23.5	O
particle	B
ﬁltering	I
particle	O
ﬁltering	B
(	O
pf	O
)	O
is	O
a	O
monte	O
carlo	O
,	O
or	O
simulation	B
based	I
,	O
algorithm	O
for	O
recursive	B
bayesian	O
inference	B
.	O
that	O
is	O
,	O
it	O
approximates	O
the	O
predict-update	B
cycle	I
described	O
in	O
section	O
18.3.1.	O
it	O
is	O
very	O
widely	O
used	O
in	O
many	O
areas	O
,	O
including	O
tracking	B
,	O
time-series	B
forecasting	I
,	O
online	O
parameter	O
learning	B
,	O
etc	O
.	O
we	O
explain	O
the	O
basic	O
algorithm	O
below	O
.	O
for	O
a	O
book-length	O
treatment	O
,	O
see	O
(	O
doucet	O
et	O
al	O
.	O
2001	O
)	O
;	O
for	O
a	O
good	O
tutorial	O
,	O
see	O
(	O
arulampalam	O
et	O
al	O
.	O
2002	O
)	O
,	O
or	O
just	O
read	O
on	O
.	O
824	O
chapter	O
23.	O
monte	O
carlo	O
inference	B
23.5.1	O
sequential	B
importance	O
sampling	O
the	O
basic	O
idea	O
is	O
to	O
appproximate	O
the	O
belief	B
state	I
(	O
of	O
the	O
entire	O
state	B
trajectory	O
)	O
using	O
a	O
weighted	O
set	O
of	O
particles	O
:	O
p	O
(	O
z1	O
:	O
t|y1	O
:	O
t	O
)	O
≈	O
s	O
(	O
cid:4	O
)	O
ˆws	O
t	O
δzs	O
1	O
:	O
t	O
(	O
z1	O
:	O
t	O
)	O
(	O
23.36	O
)	O
s=1	O
where	O
ˆws	O
t	O
is	O
the	O
normalized	O
weight	O
of	O
sample	O
s	O
at	O
time	O
t.	O
from	O
this	O
representation	O
,	O
we	O
can	O
easily	O
compute	O
the	O
marginal	B
distribution	I
over	O
the	O
most	O
recent	O
state	B
,	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
,	O
by	O
simply	O
ignoring	O
the	O
previous	O
parts	O
of	O
the	O
trajectory	O
,	O
z1	O
:	O
t−1	O
.	O
(	O
the	O
fact	O
that	O
pf	O
samples	B
in	O
the	O
space	O
of	O
entire	O
trajectories	O
has	O
various	O
implications	O
which	O
we	O
will	O
discuss	O
later	O
.	O
)	O
if	O
the	O
proposal	O
has	O
the	O
form	O
q	O
(	O
zs	O
we	O
update	O
this	O
belief	B
state	I
using	O
importance	B
sampling	I
.	O
1	O
:	O
t|y1	O
:	O
t	O
)	O
,	O
then	O
the	O
importance	B
weights	I
are	O
given	O
by	O
1	O
:	O
t|y1	O
:	O
t	O
)	O
t	O
∝	O
p	O
(	O
zs	O
ws	O
1	O
:	O
t|y1	O
:	O
t	O
)	O
q	O
(	O
zs	O
t	O
(	O
cid:7	O
)	O
ws	O
s	O
(	O
cid:2	O
)	O
ws	O
(	O
cid:2	O
)	O
which	O
can	O
be	O
normalized	O
as	O
follows	O
:	O
t	O
=	O
ˆws	O
t	O
p	O
(	O
z1	O
:	O
t|y1	O
:	O
t	O
)	O
=	O
we	O
can	O
rewrite	O
the	O
numerator	O
recursively	O
as	O
follows	O
:	O
p	O
(	O
yt|z1	O
:	O
t	O
,	O
y1	O
:	O
t−1	O
)	O
p	O
(	O
z1	O
:	O
t|y1	O
:	O
t−1	O
)	O
p	O
(	O
yt|zt	O
)	O
p	O
(	O
zt|z1	O
:	O
t−1	O
,	O
y1	O
:	O
t−1	O
)	O
p	O
(	O
z1	O
:	O
t−1|y1	O
:	O
t−1	O
)	O
p	O
(	O
yt|y1	O
:	O
t−1	O
)	O
=	O
∝	O
p	O
(	O
yt|zt	O
)	O
p	O
(	O
zt|zt−1	O
)	O
p	O
(	O
z1	O
:	O
t−1|y1	O
:	O
t−1	O
)	O
p	O
(	O
yt|y1	O
:	O
t−1	O
)	O
(	O
23.37	O
)	O
(	O
23.38	O
)	O
(	O
23.39	O
)	O
(	O
23.40	O
)	O
(	O
23.41	O
)	O
(	O
23.45	O
)	O
where	O
we	O
have	O
made	O
the	O
usual	O
markov	O
assumptions	O
.	O
we	O
will	O
restrict	O
attention	O
to	O
proposal	O
densities	O
of	O
the	O
following	O
form	O
:	O
q	O
(	O
z1	O
:	O
t|y1	O
:	O
t	O
)	O
=	O
q	O
(	O
zt|z1	O
:	O
t−1	O
,	O
y1	O
:	O
t	O
)	O
q	O
(	O
z1	O
:	O
t−1|y1	O
:	O
t−1	O
)	O
so	O
that	O
we	O
can	O
“	O
grow	O
”	O
the	O
trajectory	O
by	O
adding	O
the	O
new	O
state	B
zt	O
to	O
the	O
end	O
.	O
importance	B
weights	I
simplify	O
to	O
t|zs	O
ws	O
1	O
:	O
t−1|y1	O
:	O
t−1	O
)	O
t	O
∝	O
p	O
(	O
yt|zs	O
t	O
)	O
p	O
(	O
zs	O
t−1	O
)	O
p	O
(	O
zs	O
1	O
:	O
t−1|y1	O
:	O
t−1	O
)	O
t|zs	O
1	O
:	O
t−1	O
,	O
y1	O
:	O
t	O
)	O
q	O
(	O
zs	O
p	O
(	O
yt|zs	O
t|zs	O
t	O
)	O
p	O
(	O
zs	O
t−1	O
)	O
t|zs	O
q	O
(	O
zs	O
1	O
:	O
t−1	O
,	O
y1	O
:	O
t	O
)	O
=	O
ws	O
q	O
(	O
zs	O
t−1	O
(	O
23.44	O
)	O
if	O
we	O
further	O
assume	O
that	O
q	O
(	O
zt|z1	O
:	O
t−1	O
,	O
y1	O
:	O
t	O
)	O
=q	O
(	O
zt|zt−1	O
,	O
yt	O
)	O
,	O
then	O
we	O
only	O
need	O
to	O
keep	O
the	O
most	O
recent	O
part	O
of	O
the	O
trajectory	O
and	O
observation	B
sequence	O
,	O
rather	O
than	O
the	O
whole	O
history	O
,	O
in	O
order	O
to	O
compute	O
the	O
new	O
sample	O
.	O
in	O
this	O
case	O
,	O
the	O
weight	O
becomes	O
(	O
23.42	O
)	O
in	O
this	O
case	O
,	O
the	O
(	O
23.43	O
)	O
t	O
∝	O
ws	O
ws	O
t−1	O
p	O
(	O
yt|zs	O
q	O
(	O
zs	O
t|zs	O
t−1	O
,	O
yt	O
)	O
t	O
)	O
p	O
(	O
zs	O
t|zs	O
t−1	O
)	O
23.5.	O
particle	B
ﬁltering	I
hence	O
we	O
can	O
approximate	O
the	O
posterior	O
ﬁltered	O
density	O
using	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
≈	O
s	O
(	O
cid:4	O
)	O
ˆws	O
t	O
δzs	O
t	O
(	O
zt	O
)	O
825	O
(	O
23.46	O
)	O
s=1	O
as	O
s	O
→	O
∞	O
,	O
one	O
can	O
show	O
that	O
this	O
approaches	O
the	O
true	O
posterior	O
(	O
crisan	O
et	O
al	O
.	O
1999	O
)	O
.	O
the	O
basic	O
algorithm	O
is	O
now	O
very	O
simple	O
:	O
for	O
each	O
old	O
sample	O
s	O
,	O
propose	B
an	O
extension	B
using	O
t	O
∼	O
q	O
(	O
zt|zs	O
zs	O
t	O
using	O
equation	O
23.45.	O
unfortunately	O
,	O
this	O
basic	O
algorithm	O
does	O
not	O
work	O
very	O
well	O
,	O
as	O
we	O
discuss	O
below	O
.	O
t−1	O
,	O
yt	O
)	O
,	O
and	O
give	O
this	O
new	O
particle	O
weight	O
ws	O
23.5.2	O
the	O
degeneracy	B
problem	I
the	O
basic	O
sequential	O
importance	B
sampling	I
algorithm	O
fails	O
after	O
a	O
few	O
steps	O
because	O
most	O
of	O
the	O
particles	O
will	O
have	O
negligible	O
weight	O
.	O
this	O
is	O
called	O
the	O
degeneracy	B
problem	I
,	O
and	O
occurs	O
because	O
we	O
are	O
sampling	O
in	O
a	O
high-dimensional	O
space	O
(	O
in	O
fact	O
,	O
the	O
space	O
is	O
growing	O
in	O
size	O
over	O
time	O
)	O
,	O
using	O
a	O
myopic	O
proposal	B
distribution	I
.	O
we	O
can	O
quantify	O
the	O
degree	B
of	O
degeneracy	O
using	O
the	O
effective	B
sample	I
size	I
,	O
deﬁned	O
by	O
where	O
w∗s	O
t−1	O
,	O
yt	O
)	O
is	O
the	O
“	O
true	O
weight	O
”	O
of	O
particle	O
s.	O
this	O
quantity	O
can	O
not	O
be	O
computed	O
exactly	O
,	O
since	O
we	O
don	O
’	O
t	O
know	O
the	O
true	O
posterior	O
,	O
but	O
we	O
can	O
approximate	O
it	O
using	O
t	O
=	O
p	O
(	O
zs	O
t|y1	O
:	O
t	O
)	O
/q	O
(	O
zs	O
t|zs	O
(	O
23.47	O
)	O
(	O
23.48	O
)	O
(	O
cid:2	O
)	O
s	O
eff	O
s	O
1	O
+	O
var	O
[	O
w∗s	O
t	O
]	O
1	O
(	O
cid:7	O
)	O
s	O
s=1	O
(	O
ws	O
t	O
)	O
2	O
ˆs	O
eff	O
=	O
if	O
the	O
variance	B
of	O
the	O
weights	O
is	O
large	O
,	O
then	O
we	O
are	O
wasting	O
our	O
resources	O
updating	O
particles	O
with	O
low	O
weight	O
,	O
which	O
do	O
not	O
contribute	O
much	O
to	O
our	O
posterior	O
estimate	O
.	O
there	O
are	O
two	O
main	O
solutions	O
to	O
the	O
degeneracy	B
problem	I
:	O
adding	O
a	O
resampling	O
step	O
,	O
and	O
using	O
a	O
good	O
proposal	B
distribution	I
.	O
we	O
discuss	O
both	O
of	O
these	O
in	O
turn	O
.	O
23.5.3	O
the	O
resampling	O
step	O
the	O
main	O
improvement	O
to	O
the	O
basic	O
sis	O
algorithm	O
is	O
to	O
monitor	O
the	O
effective	O
sampling	O
size	O
,	O
and	O
whenever	O
it	O
drops	O
below	O
a	O
threshold	O
,	O
to	O
eliminate	O
particles	O
with	O
low	O
weight	O
,	O
and	O
then	O
(	O
hence	O
pf	O
is	O
sometimes	O
called	O
survival	O
of	O
the	O
to	O
create	O
replicates	O
of	O
the	O
surviving	O
particles	O
.	O
ﬁttest	O
(	O
kanazawa	O
et	O
al	O
.	O
1995	O
)	O
.	O
)	O
in	O
particular	O
,	O
we	O
generate	O
a	O
new	O
set	O
{	O
zs∗	O
s=1	O
by	O
sampling	O
with	O
replacement	O
s	O
times	O
from	O
the	O
weighted	O
distribution	O
t	O
}	O
s	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
≈	O
s	O
(	O
cid:4	O
)	O
ˆws	O
t	O
δzs	O
t	O
(	O
zt	O
)	O
(	O
23.49	O
)	O
s=1	O
where	O
the	O
probability	O
of	O
choosing	O
particle	O
j	O
for	O
replication	O
is	O
wj	O
(	O
this	O
is	O
sometimes	O
called	O
t	O
.	O
rejuvenation	B
.	O
)	O
the	O
result	O
is	O
an	O
iid	B
unweighted	O
sample	O
from	O
the	O
discrete	B
density	O
equation	O
23.49	O
,	O
so	O
we	O
set	O
the	O
new	O
weights	O
to	O
ws	O
t	O
=	O
1/s	O
.	O
this	O
scheme	O
is	O
illustrated	O
in	O
figure	O
23.4	O
.	O
826	O
chapter	O
23.	O
monte	O
carlo	O
inference	B
(	O
cid:83	O
)	O
(	O
cid:85	O
)	O
(	O
cid:82	O
)	O
(	O
cid:83	O
)	O
(	O
cid:82	O
)	O
(	O
cid:86	O
)	O
(	O
cid:68	O
)	O
(	O
cid:79	O
)	O
(	O
cid:90	O
)	O
(	O
cid:72	O
)	O
(	O
cid:76	O
)	O
(	O
cid:74	O
)	O
(	O
cid:75	O
)	O
(	O
cid:87	O
)	O
(	O
cid:76	O
)	O
(	O
cid:81	O
)	O
(	O
cid:74	O
)	O
(	O
cid:85	O
)	O
(	O
cid:72	O
)	O
(	O
cid:86	O
)	O
(	O
cid:68	O
)	O
(	O
cid:80	O
)	O
(	O
cid:83	O
)	O
(	O
cid:79	O
)	O
(	O
cid:72	O
)	O
(	O
cid:51	O
)	O
(	O
cid:11	O
)	O
(	O
cid:93	O
)	O
(	O
cid:11	O
)	O
(	O
cid:87	O
)	O
(	O
cid:16	O
)	O
(	O
cid:20	O
)	O
(	O
cid:12	O
)	O
(	O
cid:3	O
)	O
(	O
cid:95	O
)	O
(	O
cid:3	O
)	O
(	O
cid:92	O
)	O
(	O
cid:11	O
)	O
(	O
cid:20	O
)	O
(	O
cid:29	O
)	O
(	O
cid:87	O
)	O
(	O
cid:16	O
)	O
(	O
cid:20	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:51	O
)	O
(	O
cid:11	O
)	O
(	O
cid:93	O
)	O
(	O
cid:11	O
)	O
(	O
cid:87	O
)	O
(	O
cid:12	O
)	O
(	O
cid:3	O
)	O
(	O
cid:95	O
)	O
(	O
cid:3	O
)	O
(	O
cid:92	O
)	O
(	O
cid:11	O
)	O
(	O
cid:20	O
)	O
(	O
cid:29	O
)	O
(	O
cid:87	O
)	O
(	O
cid:16	O
)	O
(	O
cid:20	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:51	O
)	O
(	O
cid:11	O
)	O
(	O
cid:92	O
)	O
(	O
cid:11	O
)	O
(	O
cid:87	O
)	O
(	O
cid:12	O
)	O
(	O
cid:3	O
)	O
(	O
cid:95	O
)	O
(	O
cid:3	O
)	O
(	O
cid:93	O
)	O
(	O
cid:11	O
)	O
(	O
cid:87	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:51	O
)	O
(	O
cid:11	O
)	O
(	O
cid:93	O
)	O
(	O
cid:11	O
)	O
(	O
cid:87	O
)	O
(	O
cid:12	O
)	O
(	O
cid:3	O
)	O
(	O
cid:95	O
)	O
(	O
cid:3	O
)	O
(	O
cid:92	O
)	O
(	O
cid:11	O
)	O
(	O
cid:20	O
)	O
(	O
cid:29	O
)	O
(	O
cid:87	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
figure	O
23.4	O
illustration	O
of	O
particle	B
ﬁltering	I
.	O
there	O
are	O
a	O
variety	O
of	O
algorithms	O
for	O
peforming	O
the	O
resampling	O
step	O
.	O
the	O
simplest	O
is	O
multi-	O
nomial	O
resampling	O
,	O
which	O
computes	O
(	O
k1	O
,	O
.	O
.	O
.	O
,	O
ks	O
)	O
∼	O
mu	O
(	O
s	O
,	O
(	O
w1	O
we	O
then	O
make	O
ks	O
copies	O
of	O
zs	O
t	O
.	O
various	O
improvements	O
exist	O
,	O
such	O
as	O
systematic	B
resampling	I
residual	O
resampling	O
,	O
and	O
stratiﬁed	B
sampling	I
,	O
which	O
can	O
reduce	O
the	O
variance	B
of	O
the	O
weights	O
.	O
all	O
these	O
methods	O
take	O
o	O
(	O
s	O
)	O
time	O
.	O
see	O
(	O
doucet	O
et	O
al	O
.	O
2001	O
)	O
for	O
details	O
.	O
t	O
,	O
.	O
.	O
.	O
,	O
ws	O
(	O
23.50	O
)	O
t	O
)	O
)	O
the	O
overall	O
particle	B
ﬁltering	I
algorithm	O
is	O
summarized	O
in	O
algorithm	O
6	O
.	O
(	O
note	O
that	O
if	O
an	O
estimate	O
of	O
the	O
state	B
is	O
required	O
,	O
it	O
should	O
be	O
computed	O
before	O
the	O
resampling	O
step	O
,	O
since	O
this	O
will	O
result	O
in	O
lower	O
variance	B
.	O
)	O
algorithm	O
23.1	O
:	O
one	O
step	O
of	O
a	O
generic	O
particle	O
ﬁlter	O
1	O
for	O
s	O
=	O
1	O
:	O
s	O
do	O
2	O
t	O
∼	O
q	O
(	O
zt|zs	O
draw	O
zs	O
compute	O
weight	O
ws	O
p	O
(	O
yt|zs	O
q	O
(	O
zs	O
t|zs	O
t−1	O
,	O
yt	O
)	O
t	O
)	O
p	O
(	O
zs	O
t|zs	O
t−1	O
)	O
;	O
t−1	O
,	O
yt	O
)	O
;	O
t	O
∝	O
ws	O
t−1	O
ws	O
t	O
(	O
cid:2	O
)	O
s	O
(	O
cid:2	O
)	O
ws	O
(	O
cid:2	O
)	O
t	O
=	O
t	O
;	O
3	O
8	O
9	O
s=1	O
(	O
ws	O
t	O
)	O
2	O
;	O
4	O
normalize	O
weights	O
:	O
ws	O
1	O
(	O
cid:2	O
)	O
s	O
5	O
compute	O
ˆseff	O
=	O
6	O
if	O
ˆs	O
7	O
<	O
smin	O
then	O
eff	O
resample	O
s	O
indices	O
π	O
∼	O
wt	O
;	O
t	O
=	O
zπ	O
z	O
:	O
t	O
;	O
ws	O
t	O
=	O
1/s	O
;	O
although	O
the	O
resampling	O
step	O
helps	O
with	O
the	O
degeneracy	B
problem	I
,	O
it	O
introduces	O
problems	O
of	O
its	O
own	O
.	O
in	O
particular	O
,	O
since	O
the	O
particles	O
with	O
high	O
weight	O
will	O
be	O
selected	O
many	O
times	O
,	O
there	O
is	O
a	O
loss	B
of	O
diversity	O
amongst	O
the	O
population	O
.	O
this	O
is	O
known	O
as	O
sample	B
impoverishment	I
.	O
in	O
the	O
23.5.	O
particle	B
ﬁltering	I
827	O
extreme	O
case	O
of	O
no	O
process	O
noise	O
(	O
e.g.	O
,	O
if	O
we	O
have	O
static	O
but	O
unknown	B
parameters	O
as	O
part	O
of	O
the	O
state	B
space	I
)	O
,	O
then	O
all	O
the	O
particles	O
will	O
collapse	O
to	O
a	O
single	O
point	O
within	O
a	O
few	O
iterations	O
.	O
to	O
mitigate	O
this	O
problem	O
,	O
several	O
solutions	O
have	O
been	O
proposed	O
.	O
(	O
1	O
)	O
only	O
resample	O
when	O
(	O
the	O
original	O
bootstrap	B
ﬁlter	I
(	O
gordon	O
1993	O
)	O
resampled	O
at	O
necessary	O
,	O
not	O
at	O
every	O
time	O
step	O
.	O
every	O
step	O
,	O
but	O
this	O
is	O
suboptimal	O
.	O
)	O
(	O
2	O
)	O
after	O
replicating	O
old	O
particles	O
,	O
sample	O
new	O
values	O
using	O
an	O
mcmc	O
step	O
which	O
leaves	B
the	O
posterior	O
distribution	O
invariant	B
(	O
see	O
e.g.	O
,	O
the	O
resample-move	B
algorithm	O
in	O
(	O
gilks	O
and	O
berzuini	O
2001	O
)	O
)	O
.	O
(	O
3	O
)	O
create	O
a	O
kernel	O
density	O
estimate	O
on	O
top	O
of	O
the	O
particles	O
,	O
p	O
(	O
zt|y1	O
:	O
t	O
)	O
≈	O
s	O
(	O
cid:4	O
)	O
t	O
κ	O
(	O
zt	O
−	O
zs	O
ws	O
t	O
)	O
(	O
23.51	O
)	O
s=1	O
where	O
κ	O
is	O
some	O
smoothing	B
kernel	I
.	O
we	O
then	O
sample	O
from	O
this	O
smoothed	O
distribution	O
.	O
this	O
is	O
known	O
as	O
a	O
regularized	B
particle	I
ﬁlter	I
(	O
musso	O
et	O
al	O
.	O
2001	O
)	O
.	O
(	O
4	O
)	O
when	O
performing	O
inference	B
on	O
static	O
parameters	O
,	O
add	O
some	O
artiﬁcial	O
process	O
noise	O
.	O
(	O
if	O
this	O
is	O
undesirable	O
,	O
other	O
algorithms	O
must	O
be	O
used	O
for	O
online	O
parameter	O
estimation	O
,	O
e.g.	O
,	O
(	O
andrieu	O
et	O
al	O
.	O
2005	O
)	O
)	O
.	O
23.5.4	O
the	O
proposal	B
distribution	I
the	O
simplest	O
and	O
most	O
widely	O
used	O
proposal	B
distribution	I
is	O
to	O
sample	O
from	O
the	O
prior	O
:	O
q	O
(	O
zt|zs	O
t−1	O
,	O
yt	O
)	O
=	O
p	O
(	O
zt|zs	O
t−1	O
)	O
in	O
this	O
case	O
,	O
the	O
weight	O
update	O
simpliﬁes	O
to	O
t	O
∝	O
ws	O
ws	O
t−1p	O
(	O
yt|zs	O
t	O
)	O
(	O
23.52	O
)	O
(	O
23.53	O
)	O
this	O
can	O
be	O
thought	O
of	O
a	O
“	O
generate	B
and	I
test	I
”	O
approach	O
:	O
we	O
sample	O
values	O
from	O
the	O
dynamic	O
model	O
,	O
and	O
then	O
evaluate	O
how	O
good	O
they	O
are	O
after	O
we	O
see	O
the	O
data	O
(	O
see	O
figure	O
23.4	O
)	O
.	O
this	O
is	O
the	O
approach	O
used	O
in	O
the	O
condensation	B
algorithm	O
(	O
which	O
stands	O
for	O
“	O
conditional	O
density	O
propagation	O
”	O
)	O
used	O
for	O
visual	O
tracking	O
(	O
isard	O
and	O
blake	O
1998	O
)	O
.	O
however	O
,	O
if	O
the	O
likelihood	B
is	O
narrower	O
than	O
the	O
dynamical	O
prior	O
(	O
meaning	O
the	O
sensor	O
is	O
more	O
informative	O
than	O
the	O
motion	O
model	O
,	O
which	O
is	O
often	O
the	O
case	O
)	O
,	O
this	O
is	O
a	O
very	O
inefficient	O
approach	O
,	O
since	O
most	O
particles	O
will	O
be	O
assigned	O
very	O
low	O
weight	O
.	O
it	O
is	O
much	O
better	O
to	O
actually	O
look	O
at	O
the	O
data	O
yt	O
when	O
generating	O
a	O
proposal	O
.	O
q	O
(	O
zt|zs	O
t−1	O
,	O
yt	O
)	O
=	O
p	O
(	O
zt|zs	O
optimal	O
proposal	O
distribution	O
has	O
the	O
following	O
form	O
:	O
p	O
(	O
yt|zt	O
)	O
p	O
(	O
zt|zs	O
t−1	O
)	O
p	O
(	O
yt|zs	O
if	O
we	O
use	O
this	O
proposal	O
,	O
the	O
new	O
weight	O
is	O
given	O
by	O
t|zs	O
t	O
)	O
p	O
(	O
z	O
(	O
cid:2	O
)	O
t−1p	O
(	O
yt|zs	O
t	O
∝	O
ws	O
ws	O
t−1	O
)	O
=	O
ws	O
t−1	O
p	O
(	O
yt|z	O
(	O
cid:2	O
)	O
t−1	O
,	O
yt	O
)	O
=	O
(	O
cid:12	O
)	O
t−1	O
)	O
t−1	O
)	O
dz	O
(	O
cid:2	O
)	O
t	O
in	O
fact	O
,	O
the	O
(	O
23.54	O
)	O
(	O
23.55	O
)	O
this	O
proposal	O
is	O
optimal	O
since	O
,	O
for	O
any	O
given	O
zs	O
regardless	O
of	O
the	O
value	O
drawn	O
for	O
zs	O
true	O
weights	O
var	O
[	O
w∗s	O
t	O
]	O
,	O
is	O
zero	O
.	O
t−1	O
,	O
the	O
new	O
weight	O
ws	O
t	O
.	O
hence	O
,	O
conditional	O
on	O
the	O
old	O
values	O
z.	O
t	O
takes	O
the	O
same	O
value	O
t−1	O
,	O
the	O
variance	B
of	O
828	O
chapter	O
23.	O
monte	O
carlo	O
inference	B
in	O
general	O
,	O
it	O
is	O
intractable	O
to	O
sample	O
from	O
p	O
(	O
zt|zs	O
t−1	O
,	O
yt	O
)	O
and	O
to	O
evaluate	O
the	O
integral	O
needed	O
to	O
compute	O
the	O
predictive	B
density	O
p	O
(	O
yt|zs	O
t−1	O
)	O
.	O
however	O
,	O
there	O
are	O
two	O
cases	O
when	O
the	O
optimal	O
proposal	O
distribution	O
can	O
be	O
used	O
.	O
the	O
ﬁrst	O
setting	O
is	O
when	O
zt	O
is	O
discrete	B
,	O
so	O
the	O
integral	O
becomes	O
a	O
sum	O
.	O
of	O
course	O
,	O
if	O
the	O
entire	O
state	B
space	I
is	O
discrete	B
,	O
we	O
can	O
use	O
an	O
hmm	O
ﬁlter	O
instead	O
,	O
but	O
in	O
some	O
cases	O
,	O
some	O
parts	O
of	O
the	O
state	B
are	O
discrete	B
,	O
and	O
some	O
continuous	O
.	O
the	O
second	O
setting	O
is	O
when	O
p	O
(	O
zt|zs	O
t−1	O
,	O
yt	O
)	O
is	O
gaussian	O
.	O
this	O
occurs	O
when	O
the	O
dynamics	O
are	O
nonlinear	O
but	O
the	O
observations	O
are	O
linear	O
.	O
see	O
exercise	O
23.3	O
for	O
the	O
details	O
.	O
in	O
cases	O
where	O
the	O
model	O
is	O
not	O
linear-gaussian	O
,	O
we	O
may	O
still	O
compute	O
a	O
gaussian	O
approxima-	O
tion	O
to	O
p	O
(	O
zt|zs	O
t−1	O
,	O
yt	O
)	O
using	O
the	O
unscented	B
transform	I
(	O
section	O
18.5.2	O
)	O
and	O
use	O
this	O
as	O
a	O
proposal	O
.	O
this	O
is	O
known	O
as	O
the	O
unscented	B
particle	I
ﬁlter	I
(	O
van	O
der	O
merwe	O
et	O
al	O
.	O
2000	O
)	O
.	O
in	O
more	O
general	O
settings	O
,	O
we	O
can	O
use	O
other	O
kinds	O
of	O
data-driven	B
proposals	I
,	O
perhaps	O
based	O
on	O
discriminative	B
models	O
.	O
unlike	O
mcmc	O
,	O
we	O
do	O
not	O
need	O
to	O
worry	O
about	O
the	O
proposals	O
being	O
reversible	O
.	O
23.5.5	O
application	O
:	O
robot	O
localization	O
consider	O
a	O
mobile	O
robot	O
wandering	O
around	O
an	O
office	O
environment	O
.	O
we	O
will	O
assume	O
that	O
it	O
already	O
has	O
a	O
map	O
of	O
the	O
world	O
,	O
represented	O
in	O
the	O
form	O
of	O
an	O
occupancy	B
grid	I
,	O
which	O
just	O
speciﬁes	O
whether	O
each	O
grid	O
cell	O
is	O
empty	O
space	O
or	O
occupied	O
by	O
an	O
something	O
solid	O
like	O
a	O
wall	O
.	O
the	O
goal	O
is	O
for	O
the	O
robot	O
to	O
estimate	O
its	O
location	O
.	O
this	O
can	O
be	O
solved	O
optimally	O
using	O
an	O
hmm	O
ﬁlter	O
,	O
since	O
we	O
are	O
assuming	O
the	O
state	B
space	I
is	O
discrete	B
.	O
however	O
,	O
since	O
the	O
number	O
of	O
states	O
,	O
k	O
,	O
is	O
often	O
very	O
large	O
,	O
the	O
o	O
(	O
k	O
2	O
)	O
time	O
complexity	O
per	O
update	O
is	O
prohibitive	O
.	O
we	O
can	O
use	O
a	O
particle	O
ﬁlter	O
as	O
a	O
sparse	B
approximation	O
to	O
the	O
belief	B
state	I
.	O
this	O
is	O
known	O
as	O
monte	O
carlo	O
localization	O
,	O
and	O
is	O
described	O
in	O
detail	O
in	O
(	O
thrun	O
et	O
al	O
.	O
2006	O
)	O
.	O
figure	O
23.5	O
gives	O
an	O
example	O
of	O
the	O
method	O
in	O
action	B
.	O
the	O
robot	O
uses	O
a	O
sonar	O
range	O
ﬁnder	O
,	O
so	O
it	O
can	O
only	O
sense	O
distance	O
to	O
obstacles	O
.	O
it	O
starts	O
out	O
with	O
a	O
uniform	O
prior	O
,	O
reﬂecting	O
the	O
fact	O
that	O
the	O
owner	O
of	O
the	O
robot	O
may	O
have	O
turned	O
it	O
on	O
in	O
an	O
arbitrary	O
location	O
.	O
(	O
figuring	O
out	O
where	O
you	O
are	O
,	O
starting	O
from	O
a	O
uniform	O
prior	O
,	O
is	O
called	O
global	B
localization	I
.	O
)	O
after	O
the	O
ﬁrst	O
scan	O
,	O
which	O
indicates	O
two	O
walls	O
on	O
either	O
side	O
,	O
the	O
belief	B
state	I
is	O
shown	O
in	O
(	O
b	O
)	O
.	O
the	O
posterior	O
is	O
still	O
fairly	O
broad	O
,	O
since	O
the	O
robot	O
could	O
be	O
in	O
any	O
location	O
where	O
the	O
walls	O
are	O
fairly	O
close	O
by	O
,	O
such	O
as	O
a	O
corridor	O
or	O
any	O
of	O
the	O
narrow	O
rooms	O
.	O
after	O
moving	O
to	O
location	O
2	O
,	O
the	O
robot	O
is	O
pretty	O
sure	O
it	O
must	O
be	O
in	O
the	O
corridor	O
,	O
as	O
shown	O
in	O
(	O
c	O
)	O
.	O
after	O
moving	O
to	O
location	O
3	O
,	O
the	O
sensor	O
is	O
able	O
to	O
detect	O
the	O
end	O
of	O
the	O
corridor	O
.	O
however	O
,	O
due	O
to	O
symmetry	O
,	O
it	O
is	O
not	O
sure	O
if	O
it	O
is	O
in	O
location	O
i	O
(	O
the	O
true	O
location	O
)	O
or	O
location	O
ii	O
.	O
(	O
this	O
is	O
an	O
example	O
of	O
perceptual	B
aliasing	I
,	O
which	O
refers	O
to	O
the	O
fact	O
that	O
different	O
things	O
may	O
look	O
the	O
same	O
.	O
)	O
after	O
moving	O
to	O
locations	O
4	O
and	O
5	O
,	O
it	O
is	O
ﬁnally	O
able	O
to	O
ﬁgure	O
out	O
precisely	O
where	O
it	O
is	O
.	O
the	O
whole	O
process	O
is	O
analogous	O
to	O
someone	O
getting	O
lost	O
in	O
an	O
office	O
building	O
,	O
and	O
wandering	O
the	O
corridors	O
until	O
they	O
see	O
a	O
sign	O
they	O
recognize	O
.	O
in	O
section	O
23.6.3	O
,	O
we	O
discuss	O
how	O
to	O
estimate	O
location	O
and	O
the	O
map	O
at	O
the	O
same	O
time	O
.	O
23.5.6	O
application	O
:	O
visual	O
object	O
tracking	B
our	O
next	O
example	O
is	O
concerned	O
with	O
tracking	B
an	O
object	O
(	O
in	O
this	O
case	O
,	O
a	O
remote-controlled	O
heli-	O
copter	O
)	O
in	O
a	O
video	O
sequence	O
.	O
the	O
method	O
uses	O
a	O
simple	O
linear	O
motion	O
model	O
for	O
the	O
centroid	B
of	O
the	O
object	O
,	O
and	O
a	O
color	O
histogram	B
for	O
the	O
likelihood	B
model	O
,	O
using	O
bhattacharya	O
distance	O
to	O
compare	O
histograms	O
.	O
the	O
proposal	B
distribution	I
is	O
obtained	O
by	O
sampling	O
from	O
the	O
likelihood	B
.	O
see	O
(	O
nummiaro	O
et	O
al	O
.	O
2003	O
)	O
for	O
further	O
details	O
.	O
23.5.	O
particle	B
ﬁltering	I
829	O
(	O
a	O
)	O
path	B
and	O
reference	O
poses	O
(	O
b	O
)	O
belief	O
at	O
reference	O
pose	O
1	O
room	O
a	O
5	O
4	O
3	O
start	O
1	O
2	O
room	O
b	O
room	O
c	O
(	O
c	O
)	O
belief	O
at	O
reference	O
pose	O
2	O
(	O
d	O
)	O
belief	O
at	O
reference	O
pose	O
3	O
ii	O
i	O
(	O
e	O
)	O
belief	O
at	O
reference	O
pose	O
4	O
(	O
f	O
)	O
belief	O
at	O
reference	O
pose	O
5	O
i	O
ii	O
figure	O
23.5	O
with	O
kind	O
permission	O
of	O
sebastian	O
thrun	O
.	O
illustration	O
of	O
monte	O
carlo	O
localization	O
.	O
source	O
:	O
figure	O
8.7	O
of	O
(	O
thrun	O
et	O
al	O
.	O
2006	O
)	O
.	O
used	O
figure	O
23.6	O
shows	O
some	O
example	O
frames	O
.	O
the	O
system	O
uses	O
s	O
=	O
250	O
particles	O
,	O
with	O
an	O
effective	B
sample	I
size	I
of	O
ˆs	O
eff	O
=	O
134	O
.	O
(	O
a	O
)	O
shows	O
the	O
belief	B
state	I
at	O
frame	O
1.	O
the	O
system	O
has	O
had	O
to	O
resample	O
5	O
times	O
to	O
keep	O
the	O
effective	B
sample	I
size	I
above	O
the	O
threshold	O
of	O
150	O
;	O
(	O
b	O
)	O
shows	O
the	O
belief	B
state	I
at	O
frame	O
251	O
;	O
the	O
red	O
lines	O
show	O
the	O
estimated	O
location	O
of	O
the	O
center	O
of	O
the	O
object	O
over	O
the	O
last	O
250	O
frames	O
.	O
(	O
c	O
)	O
shows	O
that	O
the	O
system	O
can	O
handle	O
visual	O
clutter	O
,	O
as	O
long	O
as	O
it	O
does	O
not	O
have	O
the	O
same	O
color	O
as	O
the	O
target	O
object	O
.	O
(	O
d	O
)	O
shows	O
that	O
the	O
system	O
is	O
confused	O
between	O
the	O
grey	O
of	O
the	O
helicopter	O
and	O
the	O
grey	O
of	O
the	O
building	O
.	O
the	O
posterior	O
is	O
bimodal	O
.	O
the	O
green	O
ellipse	O
,	O
representing	O
the	O
posterior	B
mean	I
and	O
covariance	B
,	O
is	O
in	O
between	O
the	O
two	O
modes	O
.	O
(	O
e	O
)	O
shows	O
that	O
the	O
probability	O
mass	O
has	O
shifted	O
to	O
the	O
wrong	O
mode	B
:	O
the	O
system	O
has	O
lost	O
track	O
.	O
(	O
f	O
)	O
shows	O
the	O
particles	O
spread	O
out	O
over	O
the	O
gray	O
building	O
;	O
recovery	O
of	O
the	O
object	O
is	O
very	O
unlikely	O
from	O
this	O
state	B
using	O
this	O
830	O
chapter	O
23.	O
monte	O
carlo	O
inference	B
(	O
a	O
)	O
(	O
c	O
)	O
(	O
e	O
)	O
(	O
b	O
)	O
(	O
d	O
)	O
(	O
f	O
)	O
figure	O
23.6	O
example	O
of	O
particle	B
ﬁltering	I
applied	O
to	O
visual	O
object	O
tracking	B
,	O
based	O
on	O
color	O
histograms	O
.	O
(	O
a-c	O
)	O
succesful	O
tracking	B
:	O
green	O
ellipse	O
is	O
on	O
top	O
of	O
the	O
helicopter	O
.	O
(	O
d-f	O
)	O
:	O
tracker	O
gets	O
distracted	O
by	O
gray	O
clutter	O
in	O
the	O
background	O
.	O
see	O
text	O
for	O
details	O
.	O
figure	O
generated	O
by	O
pfcolortrackerdemo	O
,	O
written	O
by	O
sebastien	O
paris	O
.	O
proposal	O
.	O
we	O
see	O
that	O
the	O
method	O
is	O
able	O
to	O
keep	O
track	O
for	O
a	O
fairly	O
long	O
time	O
,	O
despite	O
the	O
presence	O
of	O
clutter	O
.	O
however	O
,	O
eventually	O
it	O
loses	O
track	O
of	O
the	O
object	O
.	O
note	O
that	O
since	O
the	O
algorithm	O
is	O
stochastic	O
,	O
simply	O
re-running	O
the	O
demo	O
may	O
ﬁx	O
the	O
problem	O
.	O
but	O
in	O
the	O
real	O
world	O
,	O
this	O
is	O
not	O
an	O
option	O
.	O
the	O
simplest	O
way	O
to	O
improve	O
performance	O
is	O
to	O
use	O
more	O
particles	O
.	O
an	O
alternative	O
is	O
to	O
perform	O
tracking	B
by	I
detection	I
,	O
by	O
running	O
an	O
object	O
detector	O
over	O
the	O
image	O
every	O
few	O
frames	O
.	O
see	O
(	O
forsyth	O
and	O
ponce	O
2002	O
;	O
szeliski	O
2010	O
;	O
prince	O
2012	O
)	O
for	O
details	O
.	O
23.6.	O
rao-blackwellised	O
particle	B
ﬁltering	I
(	O
rbpf	O
)	O
831	O
23.5.7	O
application	O
:	O
time	O
series	O
forecasting	O
in	O
section	O
18.2.4	O
,	O
we	O
discussed	O
how	O
to	O
use	O
the	O
kalman	O
ﬁlter	O
to	O
perform	O
time	O
series	O
forecasting	O
.	O
this	O
assumes	O
that	O
the	O
model	O
is	O
a	O
linear-gaussian	O
state-space	O
model	O
.	O
there	O
are	O
many	O
models	O
which	O
are	O
either	O
non-linear	O
and/or	O
non-gaussian	O
.	O
for	O
example	O
,	O
stochastic	B
volatility	I
models	O
,	O
which	O
are	O
widely	O
used	O
in	O
ﬁnance	O
,	O
assume	O
that	O
the	O
variance	B
of	O
the	O
system	O
and/or	O
observation	B
noise	O
changes	O
over	O
time	O
.	O
particle	B
ﬁltering	I
is	O
widely	O
used	O
in	O
such	O
settings	O
.	O
see	O
e.g.	O
,	O
(	O
doucet	O
et	O
al	O
.	O
2001	O
)	O
and	O
references	O
therein	O
for	O
details	O
.	O
23.6	O
rao-blackwellised	O
particle	B
ﬁltering	I
(	O
rbpf	O
)	O
in	O
some	O
models	O
,	O
we	O
can	O
partition	O
the	O
hidden	B
variables	I
into	O
two	O
kinds	O
,	O
qt	O
and	O
zt	O
,	O
such	O
that	O
we	O
can	O
analytically	O
integrate	B
out	I
zt	O
provided	O
we	O
know	O
the	O
values	O
of	O
q1	O
:	O
t.	O
this	O
means	O
we	O
only	O
have	O
sample	O
q1	O
:	O
t	O
,	O
and	O
can	O
represent	O
p	O
(	O
zt|q1	O
:	O
t	O
)	O
parametrically	O
.	O
thus	O
each	O
particle	O
s	O
represents	O
a	O
value	O
for	O
qs	O
1	O
:	O
t	O
)	O
.	O
these	O
hybrid	O
particles	O
are	O
are	O
sometimes	O
called	O
distributional	B
particles	I
or	O
collapsed	B
particles	I
(	O
koller	O
and	O
friedman	O
2009	O
,	O
sec	O
12.4	O
)	O
.	O
1	O
:	O
t	O
and	O
a	O
distribution	O
of	O
the	O
form	O
p	O
(	O
zt|y1	O
:	O
t	O
,	O
qs	O
the	O
advantage	O
of	O
this	O
approach	O
is	O
that	O
we	O
reduce	O
the	O
dimensionality	O
of	O
the	O
space	O
in	O
which	O
we	O
are	O
sampling	O
,	O
which	O
reduces	O
the	O
variance	B
of	O
our	O
estimate	O
.	O
hence	O
this	O
technique	O
is	O
known	O
as	O
rao-blackwellised	O
particle	B
ﬁltering	I
or	O
rbpf	O
for	O
short	O
,	O
named	O
after	O
theorem	O
24.20.	O
the	O
method	O
is	O
best	O
explained	O
using	O
a	O
speciﬁc	O
example	O
.	O
23.6.1	O
rbpf	O
for	O
switching	O
lg-ssms	O
a	O
canonical	O
example	O
for	O
which	O
rbpf	O
can	O
be	O
applied	O
is	O
the	O
switching	B
linear	I
dynamical	I
system	I
(	O
slds	O
)	O
model	O
discussed	O
in	O
section	O
18.6	O
(	O
chen	O
and	O
liu	O
2000	O
;	O
doucet	O
et	O
al	O
.	O
2001	O
)	O
.	O
we	O
can	O
represent	O
p	O
(	O
zt|y1	O
:	O
t	O
,	O
qs	O
1	O
:	O
t	O
)	O
using	O
a	O
mean	B
and	O
covariance	B
matrix	I
for	O
each	O
particle	O
s	O
,	O
where	O
qt	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
.	O
t−1p	O
(	O
yt|qt	O
=	O
k	O
,	O
qs	O
(	O
cid:12	O
)	O
if	O
we	O
propose	B
from	O
the	O
prior	O
,	O
q	O
(	O
qt	O
=	O
k|qs	O
t	O
∝	O
ws	O
ws	O
t−1	O
)	O
,	O
the	O
weight	O
update	O
becomes	O
1	O
:	O
t−1	O
,	O
y1	O
:	O
t−1	O
)	O
=	O
ws	O
t−1ls	O
t	O
,	O
k	O
(	O
23.56	O
)	O
where	O
p	O
(	O
yt|qt	O
=	O
k	O
,	O
zt	O
,	O
y1	O
:	O
t−1	O
,	O
qs	O
tk	O
is	O
the	O
predictive	B
density	O
for	O
the	O
new	O
observation	B
yt	O
conditioned	O
on	O
qt	O
=	O
k	O
and	O
1	O
:	O
t−1	O
.	O
in	O
the	O
case	O
of	O
slds	O
models	O
,	O
this	O
can	O
be	O
computed	O
using	O
the	O
normalization	O
1	O
:	O
t−1	O
)	O
p	O
(	O
zt|qt	O
=	O
k	O
,	O
y1	O
:	O
t−1qs	O
1	O
:	O
t−1	O
,	O
)	O
dzt	O
(	O
23.57	O
)	O
the	O
quantity	O
ls	O
the	O
history	O
qs	O
constant	O
of	O
the	O
kalman	O
ﬁlter	O
,	O
equation	O
18.41.	O
ls	O
tk	O
=	O
we	O
give	O
some	O
pseudo-code	O
in	O
algorithm	O
8	O
.	O
(	O
the	O
step	O
marked	O
“	O
kfupdate	O
”	O
refers	O
to	O
the	O
kalman	O
ﬁlter	O
update	O
equations	O
in	O
section	O
18.3.1	O
.	O
)	O
this	O
is	O
known	O
as	O
a	O
mixture	O
of	O
kalman	O
ﬁlters	O
.	O
if	O
k	O
is	O
small	O
,	O
we	O
can	O
compute	O
the	O
optimal	O
proposal	O
distribution	O
,	O
which	O
is	O
p	O
(	O
qt	O
=	O
k|y1	O
:	O
t	O
,	O
qs	O
t−1	O
(	O
qt	O
=	O
k|yt	O
)	O
1	O
:	O
t−1	O
)	O
=	O
ˆps	O
t−1	O
(	O
yt|qt	O
=	O
k	O
)	O
ˆps	O
ˆps	O
(	O
cid:7	O
)	O
ˆps	O
t−1	O
(	O
yt	O
)	O
tkp	O
(	O
qt	O
=	O
k|qs	O
ls	O
t−1	O
)	O
tk	O
(	O
cid:2	O
)	O
p	O
(	O
qt	O
=	O
k	O
(	O
cid:2	O
)	O
|qs	O
k	O
(	O
cid:2	O
)	O
ls	O
=	O
=	O
t−1	O
)	O
t−1	O
(	O
qt	O
=	O
k	O
)	O
(	O
23.58	O
)	O
(	O
23.59	O
)	O
(	O
23.60	O
)	O
832	O
chapter	O
23.	O
monte	O
carlo	O
inference	B
algorithm	O
23.2	O
:	O
one	O
step	O
of	O
rbpf	O
for	O
slds	O
using	O
prior	O
as	O
proposal	O
1	O
for	O
s	O
=	O
1	O
:	O
s	O
do	O
k	O
∼	O
p	O
(	O
qt|qs	O
2	O
qs	O
t	O
:	O
=	O
k	O
;	O
t	O
,	O
σs	O
(	O
μs	O
ws	O
t	O
=	O
ws	O
tk	O
)	O
=	O
kfupdate	O
(	O
μs	O
t	O
,	O
ls	O
t−1lk	O
ts	O
;	O
t−1	O
,	O
yt	O
,	O
θk	O
)	O
;	O
t−1	O
,	O
σs	O
t−1	O
)	O
;	O
3	O
4	O
5	O
6	O
normalize	O
weights	O
:	O
ws	O
1	O
(	O
cid:2	O
)	O
s	O
7	O
compute	O
ˆs	O
8	O
if	O
ˆs	O
9	O
<	O
smin	O
then	O
eff	O
=	O
t	O
=	O
ws	O
t	O
(	O
cid:2	O
)	O
s	O
(	O
cid:2	O
)	O
ws	O
(	O
cid:2	O
)	O
t	O
;	O
s=1	O
(	O
ws	O
t	O
)	O
2	O
;	O
eff	O
resample	O
s	O
indices	O
π	O
∼	O
wt	O
;	O
t	O
=	O
σπ	O
t	O
=	O
qπ	O
q	O
:	O
t	O
,	O
μ	O
:	O
ws	O
t	O
=	O
1/s	O
;	O
t	O
=	O
μπ	O
t	O
,	O
σ	O
:	O
t	O
,	O
;	O
10	O
11	O
where	O
we	O
use	O
the	O
following	O
shorthand	O
:	O
t−1	O
(	O
·	O
)	O
=	O
p	O
(	O
·|y1	O
:	O
t−1	O
,	O
qs	O
ˆps	O
1	O
:	O
t−1	O
)	O
we	O
then	O
sample	O
from	O
p	O
(	O
qt|qs	O
t	O
∝	O
ws	O
ws	O
t−1p	O
(	O
yt|qs	O
1	O
:	O
t−1	O
,	O
y1	O
:	O
t	O
)	O
and	O
give	O
the	O
resulting	O
particle	O
weight	O
1	O
:	O
t−1	O
,	O
y1	O
:	O
t−1	O
)	O
=	O
ws	O
t−1	O
tkp	O
(	O
qt	O
=	O
k|qs	O
ls	O
t−1	O
)	O
(	O
cid:4	O
)	O
(	O
cid:13	O
)	O
(	O
cid:14	O
)	O
(	O
23.61	O
)	O
(	O
23.62	O
)	O
k	O
since	O
the	O
weights	O
of	O
the	O
particles	O
in	O
equation	O
23.62	O
are	O
independent	O
of	O
the	O
new	O
value	O
that	O
is	O
actually	O
sampled	O
for	O
qt	O
,	O
we	O
can	O
compute	O
these	O
weights	O
ﬁrst	O
,	O
and	O
use	O
them	O
to	O
decide	O
which	O
particles	O
to	O
propagate	O
.	O
that	O
is	O
,	O
we	O
choose	O
the	O
ﬁttest	O
particles	O
at	O
time	O
t	O
−	O
1	O
using	O
information	B
from	O
time	O
t.	O
this	O
is	O
called	O
look-ahead	O
rbpf	O
(	O
de	O
freitas	O
et	O
al	O
.	O
2004	O
)	O
.	O
in	O
more	O
detail	O
,	O
the	O
idea	O
is	O
this	O
.	O
we	O
pass	O
each	O
sample	O
in	O
the	O
prior	O
through	O
all	O
k	O
models	O
to	O
get	O
k	O
posteriors	O
,	O
one	O
per	O
sample	O
.	O
the	O
normalization	O
constants	O
of	O
this	O
process	O
allow	O
us	O
to	O
compute	O
the	O
optimal	O
weights	O
in	O
equation	O
23.62.	O
we	O
then	O
resample	O
s	O
indices	O
.	O
finally	O
,	O
for	O
each	O
old	O
particle	O
s	O
that	O
is	O
chosen	O
,	O
we	O
sample	O
one	O
new	O
state	B
qs	O
t	O
=	O
k	O
,	O
and	O
use	O
the	O
corresponding	O
posterior	O
from	O
the	O
k	O
possible	O
alternative	O
that	O
we	O
have	O
already	O
computed	O
.	O
the	O
pseudo-code	O
is	O
shown	O
in	O
algorithm	O
7.	O
this	O
method	O
needs	O
o	O
(	O
ks	O
)	O
storage	O
,	O
but	O
has	O
the	O
advantage	O
that	O
each	O
particle	O
is	O
chosen	O
using	O
the	O
latest	O
information	B
,	O
yt	O
.	O
a	O
further	O
improvement	O
can	O
be	O
obtained	O
by	O
exploiting	O
the	O
fact	O
that	O
the	O
state	B
space	I
is	O
discrete	B
.	O
hence	O
we	O
can	O
use	O
the	O
resampling	O
method	O
of	O
(	O
fearnhead	O
2004	O
)	O
which	O
avoids	O
duplicating	O
particles	O
.	O
23.6.2	O
application	O
:	O
tracking	B
a	O
maneuvering	O
target	O
one	O
application	O
of	O
slds	O
is	O
to	O
track	O
moving	O
objects	O
that	O
have	O
piecewise	O
linear	O
dynamics	O
.	O
for	O
example	O
,	O
suppose	O
we	O
want	O
to	O
track	O
an	O
airplane	O
or	O
missile	O
;	O
qt	O
can	O
specify	O
if	O
the	O
object	O
is	O
ﬂying	O
normally	O
or	O
is	O
taking	O
evasive	O
action	B
.	O
this	O
is	O
called	O
maneuvering	B
target	I
tracking	I
.	O
figure	O
23.7	O
gives	O
an	O
example	O
of	O
an	O
object	O
moving	O
in	O
2d	O
.	O
the	O
setup	O
is	O
essentially	O
the	O
same	O
as	O
in	O
section	O
18.2.1	O
,	O
except	O
that	O
we	O
add	O
a	O
three-state	O
discrete	B
markov	O
chain	O
which	O
controls	O
the	O
23.6.	O
rao-blackwellised	O
particle	B
ﬁltering	I
(	O
rbpf	O
)	O
833	O
algorithm	O
23.3	O
:	O
one	O
step	O
of	O
look-ahead	O
rbpf	O
for	O
slds	O
using	O
optimal	O
proposal	O
1	O
for	O
s	O
=	O
1	O
:	O
s	O
do	O
2	O
t−1	O
,	O
σs	O
t−1	O
,	O
yt	O
,	O
θk	O
)	O
;	O
1	O
:	O
t−1	O
,	O
y1	O
:	O
t	O
)	O
=	O
(	O
cid:2	O
)	O
tkp	O
(	O
qt=k|qs	O
ls	O
k	O
(	O
cid:2	O
)	O
ls	O
t−1	O
)	O
tkp	O
(	O
qt=k|qs	O
t−1	O
)	O
;	O
(	O
cid:7	O
)	O
ws	O
tk	O
,	O
σs	O
t−1	O
[	O
(	O
μs	O
t	O
=	O
ws	O
for	O
k	O
=	O
1	O
:	O
k	O
do	O
tk	O
,	O
lk	O
k	O
lk	O
4	O
5	O
normalize	O
weights	O
:	O
ws	O
6	O
resample	O
s	O
indices	O
π	O
∼	O
wt	O
;	O
7	O
for	O
s	O
∈	O
π	O
do	O
ts	O
)	O
=	O
kfupdate	O
(	O
μs	O
tsp	O
(	O
qt	O
=	O
k|qs	O
t−1	O
)	O
]	O
;	O
ws	O
;	O
t	O
=	O
t	O
(	O
cid:2	O
)	O
s	O
(	O
cid:2	O
)	O
ws	O
(	O
cid:2	O
)	O
t	O
compute	O
optimal	O
proposal	O
p	O
(	O
k|qs	O
sample	O
k	O
∼	O
p	O
(	O
k|qs	O
t	O
=	O
k	O
,	O
μs	O
qs	O
t	O
=	O
μs	O
ws	O
t	O
=	O
1/s	O
;	O
1	O
:	O
t−1	O
,	O
y1	O
:	O
t	O
)	O
;	O
t	O
=	O
σs	O
tk	O
;	O
tk	O
,	O
σs	O
3	O
8	O
9	O
10	O
11	O
method	O
misclassiﬁcation	B
rate	I
mse	O
21.051	O
pf	O
rbpf	O
18.168	O
0.440	O
0.340	O
time	O
(	O
seconds	O
)	O
6.086	O
10.986	O
table	O
23.1	O
comparison	O
of	O
pf	O
an	O
rbpf	O
on	O
the	O
maneuvering	O
target	O
problem	O
in	O
figure	O
23.7.	O
input	O
to	O
the	O
system	O
.	O
we	O
deﬁne	O
ut	O
=	O
1	O
and	O
set	O
b1	O
=	O
(	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
)	O
t	O
,	O
b2	O
=	O
(	O
−1.225	O
,	O
−0.35	O
,	O
1.225	O
,	O
0.35	O
)	O
t	O
,	O
b3	O
=	O
(	O
1.225	O
,	O
0.35	O
,	O
−1.225	O
,	O
−0.35	O
)	O
t	O
so	O
the	O
system	O
will	O
turn	O
in	O
different	O
directions	O
depending	O
on	O
the	O
discrete	B
state	O
.	O
figure	O
23.7	O
(	O
a	O
)	O
shows	O
the	O
true	O
state	O
of	O
the	O
system	O
from	O
a	O
sample	O
run	O
,	O
starting	O
at	O
(	O
0	O
,	O
0	O
)	O
:	O
the	O
colored	O
symbols	O
denote	O
the	O
discrete	B
state	O
,	O
and	O
the	O
location	O
of	O
the	O
symbol	O
denotes	O
the	O
(	O
x	O
,	O
y	O
)	O
location	O
.	O
the	O
small	O
dots	O
represent	O
noisy	O
observations	O
.	O
figure	O
23.7	O
(	O
b	O
)	O
shows	O
the	O
estimate	O
of	O
the	O
state	B
computed	O
using	O
particle	B
ﬁltering	I
with	O
500	O
particles	O
,	O
where	O
the	O
proposal	O
is	O
to	O
sample	O
from	O
the	O
prior	O
.	O
the	O
colored	O
symbols	O
denote	O
the	O
map	O
estimate	O
of	O
the	O
state	B
,	O
and	O
the	O
location	O
of	O
the	O
symbol	O
denotes	O
the	O
mmse	O
(	O
minimum	O
mean	O
square	O
error	O
)	O
estimate	O
of	O
the	O
location	O
,	O
which	O
is	O
given	O
by	O
the	O
posterior	B
mean	I
.	O
figure	O
23.7	O
(	O
c	O
)	O
shows	O
the	O
estimate	O
computing	O
using	O
rbpf	O
with	O
500	O
particles	O
,	O
using	O
the	O
optimal	O
proposal	O
distribution	O
.	O
a	O
more	O
quantitative	O
comparison	O
is	O
shown	O
in	O
table	O
23.1.	O
we	O
see	O
that	O
rbpf	O
has	O
slightly	O
better	O
performance	O
,	O
although	O
it	O
is	O
also	O
slightly	O
slower	O
.	O
figure	O
23.8	O
visualizes	O
the	O
belief	B
state	I
of	O
the	O
system	O
.	O
in	O
(	O
a	O
)	O
we	O
show	O
the	O
distribution	O
over	O
the	O
discrete	B
states	O
.	O
we	O
see	O
that	O
the	O
particle	O
ﬁlter	O
estimate	O
of	O
the	O
belief	B
state	I
(	O
second	O
column	O
)	O
is	O
not	O
as	O
accurate	O
as	O
the	O
rbpf	O
estimate	O
(	O
third	O
column	O
)	O
in	O
the	O
beginning	O
,	O
although	O
after	O
the	O
ﬁrst	O
few	O
observations	O
performance	O
is	O
similar	B
for	O
both	O
methods	O
.	O
in	O
(	O
b	O
)	O
,	O
we	O
plot	O
the	O
posterior	O
over	O
the	O
x	O
locations	O
.	O
for	O
simplicity	O
,	O
we	O
use	O
the	O
pf	O
estimate	O
,	O
which	O
is	O
a	O
set	O
of	O
weighted	O
samples	O
,	O
but	O
we	O
could	O
also	O
have	O
used	O
the	O
rbpf	O
estimate	O
,	O
which	O
is	O
a	O
set	O
of	O
weighted	O
gaussians	O
.	O
834	O
50	O
0	O
−50	O
−100	O
−150	O
−200	O
−250	O
−90	O
chapter	O
23.	O
monte	O
carlo	O
inference	B
data	O
pf	O
,	O
mse	O
21.051	O
50	O
0	O
−50	O
−100	O
−150	O
−200	O
−30	O
−20	O
−10	O
0	O
−40	O
(	O
b	O
)	O
−80	O
−70	O
−60	O
−50	O
−40	O
−30	O
−20	O
−10	O
0	O
−250	O
−80	O
−70	O
−60	O
−50	O
rbpf	O
,	O
mse	O
18.168	O
(	O
a	O
)	O
50	O
0	O
−50	O
−100	O
−150	O
−200	O
−250	O
−80	O
−70	O
−60	O
−50	O
−30	O
−20	O
−10	O
0	O
−40	O
(	O
c	O
)	O
figure	O
23.7	O
particle	O
ﬁlter	O
estimate	O
.	O
nando	O
de	O
freitas	O
.	O
(	O
a	O
)	O
a	O
maneuvering	O
target	O
.	O
the	O
colored	O
symbols	O
represent	O
the	O
hidden	B
discrete	O
state	B
.	O
(	O
b	O
)	O
(	O
c	O
)	O
rbpf	O
estimate	O
.	O
figure	O
generated	O
by	O
rbpfmaneuverdemo	O
,	O
based	O
on	O
code	O
by	O
23.6.3	O
application	O
:	O
fast	O
slam	O
in	O
section	O
18.2.2	O
,	O
we	O
introduced	O
the	O
problem	O
of	O
simultaneous	B
localization	I
and	I
mapping	I
or	O
slam	O
for	O
mobile	O
robotics	O
.	O
the	O
main	O
problem	O
with	O
the	O
kalman	O
ﬁlter	O
implementation	O
is	O
that	O
it	O
is	O
cubic	O
in	O
the	O
number	O
of	O
landmarks	O
.	O
however	O
,	O
by	O
looking	O
at	O
the	O
dgm	O
in	O
figure	O
18.2	O
,	O
we	O
see	O
that	O
,	O
conditional	O
on	O
knowing	O
the	O
robot	O
’	O
s	O
path	B
,	O
q1	O
:	O
t	O
,	O
where	O
qt	O
∈	O
r	O
2	O
,	O
the	O
landmark	O
locations	O
z	O
∈	O
r	O
(	O
cid:26	O
)	O
l	O
2l	O
(	O
we	O
assume	O
the	O
landmarks	O
don	O
’	O
t	O
move	O
,	O
so	O
we	O
drop	O
the	O
t	O
subscript	O
)	O
.	O
that	O
is	O
,	O
are	O
independent	O
.	O
p	O
(	O
z|q1	O
:	O
t	O
,	O
y1	O
:	O
t	O
)	O
=	O
l=1	O
p	O
(	O
zl|q1	O
:	O
t	O
,	O
y1	O
:	O
t	O
)	O
.	O
consequently	O
we	O
can	O
use	O
rbpf	O
,	O
where	O
we	O
sample	O
the	O
robot	O
’	O
s	O
trajectory	O
,	O
q1	O
:	O
t	O
,	O
and	O
we	O
run	O
l	O
independent	O
2d	O
kalman	O
ﬁlters	O
inside	O
each	O
particle	O
.	O
this	O
takes	O
o	O
(	O
l	O
)	O
time	O
per	O
particle	O
.	O
fortunately	O
,	O
the	O
number	O
of	O
particles	O
needed	O
for	O
good	O
performance	O
is	O
quite	O
small	O
(	O
this	O
partly	O
depends	O
on	O
the	O
control	O
/	O
exploration	O
policy	B
)	O
,	O
so	O
the	O
algorithm	O
is	O
essentially	O
linear	O
in	O
the	O
number	O
of	O
particles	O
.	O
this	O
technique	O
has	O
the	O
additional	O
advantage	O
that	O
23.6.	O
rao-blackwellised	O
particle	B
ﬁltering	I
(	O
rbpf	O
)	O
835	O
truth	O
pf	O
,	O
error	O
rate	O
0.440	O
rbpf	O
,	O
error	O
rate	O
0.340	O
pf	O
10	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
90	O
10	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
90	O
10	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
90	O
100	O
1	O
2	O
3	O
100	O
1	O
2	O
3	O
100	O
1	O
2	O
3	O
(	O
a	O
)	O
)	O
t	O
:	O
1	O
1	O
0.5	O
0	O
100	O
y	O
|	O
t	O
,	O
1	O
x	O
(	O
p	O
80	O
60	O
40	O
t	O
20	O
0	O
−20	O
−40	O
−60	O
x1	O
,	O
t	O
0	O
−80	O
(	O
b	O
)	O
figure	O
23.8	O
belief	O
states	O
corresponding	O
to	O
figure	O
23.7	O
.	O
(	O
a	O
)	O
discrete	B
state	O
.	O
the	O
system	O
starts	O
in	O
state	B
2	O
(	O
red	O
x	O
in	O
figure	O
23.7	O
)	O
,	O
then	O
moves	O
to	O
state	B
3	O
(	O
black	O
*	O
in	O
figure	O
23.7	O
)	O
,	O
returns	O
brieﬂy	O
to	O
state	B
2	O
,	O
then	O
switches	O
to	O
state	B
1	O
(	O
blue	O
circle	O
in	O
figure	O
23.7	O
)	O
,	O
etc	O
.	O
(	O
b	O
)	O
horizontal	O
location	O
(	O
pf	O
estimate	O
)	O
.	O
figure	O
generated	O
by	O
rbpfmaneuverdemo	O
,	O
based	O
on	O
code	O
by	O
nando	O
de	O
freitas	O
.	O
it	O
is	O
easy	O
to	O
use	O
sampling	O
to	O
handle	O
the	O
data	B
association	I
ambiguity	O
,	O
and	O
that	O
it	O
allows	O
for	O
other	O
representations	O
of	O
the	O
map	O
,	O
such	O
as	O
occupancy	O
grids	O
.	O
this	O
idea	O
was	O
ﬁrst	O
suggested	O
in	O
(	O
murphy	O
2000	O
)	O
,	O
and	O
was	O
subsequently	O
extended	O
and	O
made	O
practical	O
in	O
(	O
thrun	O
et	O
al	O
.	O
2004	O
)	O
,	O
who	O
christened	O
the	O
technique	O
fastslam	O
.	O
see	O
rbpfslamdemo	O
for	O
a	O
simple	O
demo	O
in	O
a	O
discrete	B
grid	O
world	O
.	O
exercises	O
exercise	O
23.1	O
sampling	O
from	O
a	O
cauchy	O
show	O
how	O
to	O
use	O
inverse	B
probability	I
transform	I
to	O
sample	O
from	O
a	O
standard	O
cauchy	O
,	O
t	O
(	O
x|0	O
,	O
1	O
,	O
1	O
)	O
.	O
exercise	O
23.2	O
rejection	B
sampling	I
from	O
a	O
gamma	O
using	O
a	O
cauchy	O
proposal	O
show	O
how	O
to	O
use	O
a	O
cauchy	O
proposal	O
to	O
perform	O
rejection	B
sampling	I
from	O
a	O
gamma	B
distribution	I
.	O
derive	O
the	O
optimal	O
constant	O
m	O
,	O
and	O
plot	O
the	O
density	O
and	O
its	O
upper	O
envelope	O
.	O
exercise	O
23.3	O
optimal	O
proposal	O
for	O
particle	B
ﬁltering	I
with	O
linear-gaussian	O
measurement	O
model	O
consider	O
a	O
state-space	O
model	O
of	O
the	O
following	O
form	O
:	O
zt	O
=	O
ft	O
(	O
zt−1	O
)	O
+	O
n	O
(	O
0	O
,	O
qt−1	O
)	O
yt	O
=	O
htzt	O
+	O
n	O
(	O
0	O
,	O
rt	O
)	O
(	O
23.63	O
)	O
(	O
23.64	O
)	O
derive	O
expressions	O
for	O
p	O
(	O
zt|zt−1	O
,	O
yt	O
)	O
and	O
p	O
(	O
yt|zt−1	O
)	O
,	O
which	O
are	O
needed	O
to	O
compute	O
the	O
optimal	O
(	O
minimum	O
variance	O
)	O
proposal	B
distribution	I
.	O
hint	O
:	O
use	O
bayes	O
rule	O
for	O
gaussians	O
.	O
24	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
24.1	O
introduction	O
in	O
chapter	O
23	O
,	O
we	O
introduced	O
some	O
simple	O
monte	O
carlo	O
methods	O
,	O
including	O
rejection	B
sampling	I
and	O
importance	B
sampling	I
.	O
the	O
trouble	O
with	O
these	O
methods	O
is	O
that	O
they	O
do	O
not	O
work	O
well	O
in	O
high	O
dimensional	O
spaces	O
.	O
the	O
most	O
popular	O
method	O
for	O
sampling	O
from	O
high-dimensional	O
distributions	O
is	O
markov	O
chain	O
monte	O
carlo	O
or	O
mcmc	O
.	O
in	O
a	O
survey	O
bysiam	O
news	O
1	O
,	O
mcmc	O
was	O
placed	O
in	O
the	O
top	O
10	O
most	O
important	O
algorithms	O
of	O
the	O
20th	O
century	O
.	O
the	O
basic	O
idea	O
behind	O
mcmc	O
is	O
to	O
construct	O
a	O
markov	O
chain	O
(	O
section	O
17.2	O
)	O
on	O
the	O
state	B
space	I
x	O
whose	O
stationary	B
distribution	I
is	O
the	O
target	O
density	O
p∗	O
(	O
x	O
)	O
of	O
interest	O
(	O
this	O
may	O
be	O
a	O
prior	O
or	O
a	O
posterior	O
)	O
.	O
that	O
is	O
,	O
we	O
perform	O
a	O
random	O
walk	O
on	O
the	O
state	O
space	O
,	O
in	O
such	O
a	O
way	O
that	O
the	O
fraction	O
of	O
time	O
we	O
spend	O
in	O
each	O
state	B
x	O
is	O
proportional	O
to	O
p∗	O
(	O
x	O
)	O
.	O
by	O
drawing	O
(	O
correlated	O
!	O
)	O
samples	B
x0	O
,	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
,	O
from	O
the	O
chain	O
,	O
we	O
can	O
perform	O
monte	O
carlo	O
integration	O
wrt	O
p∗	O
.	O
we	O
give	O
the	O
details	O
below	O
.	O
the	O
mcmc	O
algorithm	O
has	O
an	O
interesting	O
history	O
.	O
it	O
was	O
discovered	O
by	O
physicists	O
working	O
on	O
the	O
atomic	B
bomb	I
at	O
los	O
alamos	O
during	O
world	O
war	O
ii	O
,	O
and	O
was	O
ﬁrst	O
published	O
in	O
the	O
open	O
literature	O
in	O
(	O
metropolis	O
et	O
al	O
.	O
1953	O
)	O
in	O
a	O
chemistry	O
journal	O
.	O
an	O
extension	B
was	O
published	O
in	O
the	O
statistics	O
literature	O
in	O
(	O
hastings	O
1970	O
)	O
,	O
but	O
was	O
largely	O
unnoticed	O
.	O
a	O
special	O
case	O
(	O
gibbs	O
sampling	O
,	O
section	O
24.2	O
)	O
was	O
independently	O
invented	O
in	O
1984	O
in	O
the	O
context	O
of	O
ising	O
models	O
and	O
was	O
published	O
in	O
(	O
geman	O
and	O
geman	O
1984	O
)	O
.	O
but	O
it	O
was	O
not	O
until	O
(	O
gelfand	O
and	O
smith	O
1990	O
)	O
that	O
the	O
algorithm	O
became	O
well-known	O
to	O
the	O
wider	O
statistical	O
community	O
.	O
since	O
then	O
it	O
has	O
become	O
wildly	O
popular	O
in	O
bayesian	O
statistics	O
,	O
and	O
is	O
becoming	O
increasingly	O
popular	O
in	O
machine	B
learning	I
.	O
it	O
is	O
worth	O
brieﬂy	O
comparing	O
mcmc	O
to	O
variational	B
inference	I
(	O
chapter	O
21	O
)	O
.	O
the	O
advantages	O
of	O
variational	O
it	O
is	O
deterministic	O
;	O
(	O
3	O
)	O
is	O
it	O
easy	O
to	O
determine	O
when	O
to	O
stop	O
;	O
(	O
4	O
)	O
it	O
often	O
provides	O
a	O
lower	O
bound	O
on	O
the	O
log	O
likelihood	O
.	O
the	O
advantages	O
of	O
sampling	O
are	O
:	O
(	O
1	O
)	O
it	O
is	O
often	O
easier	O
to	O
implement	O
;	O
(	O
2	O
)	O
it	O
is	O
applicable	O
to	O
a	O
broader	O
range	O
of	O
models	O
,	O
such	O
as	O
models	O
whose	O
size	O
or	O
structure	O
changes	O
depending	O
on	O
the	O
values	O
of	O
certain	O
variables	O
(	O
e.g.	O
,	O
as	O
happens	O
in	O
matching	O
problems	O
)	O
,	O
or	O
models	O
without	O
nice	O
conjugate	B
priors	I
;	O
(	O
3	O
)	O
sampling	O
can	O
be	O
faster	O
than	O
variational	O
methods	O
when	O
applied	O
to	O
really	O
huge	O
models	O
or	O
datasets.2	O
inference	B
are	O
(	O
1	O
)	O
for	O
small	O
to	O
medium	O
problems	O
,	O
it	O
is	O
usually	O
faster	O
;	O
(	O
2	O
)	O
1.	O
source	O
:	O
http	O
:	O
//www.siam.org/pdf/news/637.pdf	O
.	O
2.	O
the	O
reason	O
is	O
that	O
sampling	O
passes	O
speciﬁc	O
values	O
of	O
variables	O
(	O
or	O
sets	O
of	O
variables	O
)	O
,	O
whereas	O
in	O
variational	B
inference	I
,	O
we	O
pass	O
around	O
distributions	O
.	O
thus	O
sampling	O
passes	O
sparse	B
messages	O
,	O
whereas	O
variational	B
inference	I
passes	O
dense	O
messages	O
for	O
comparisons	O
of	O
the	O
two	O
approaches	O
,	O
see	O
e.g.	O
,	O
(	O
yoshida	O
and	O
west	O
2010	O
)	O
and	O
articles	O
in	O
(	O
bekkerman	O
et	O
al	O
.	O
838	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
24.2	O
gibbs	O
sampling	O
in	O
this	O
section	O
,	O
we	O
present	O
one	O
of	O
the	O
most	O
popular	O
mcmc	O
algorithms	O
,	O
known	O
as	O
gibbs	O
sam-	O
pling.3	O
(	O
in	O
physics	O
,	O
this	O
method	O
is	O
known	O
as	O
glauber	O
dynamics	O
or	O
the	O
heat	B
bath	I
method	O
.	O
)	O
this	O
is	O
the	O
mcmc	O
analog	O
of	O
coordinate	O
descent	O
.	O
24.2.1	O
basic	O
idea	O
the	O
idea	O
behind	O
gibbs	O
sampling	O
is	O
that	O
we	O
sample	O
each	O
variable	O
in	O
turn	O
,	O
conditioned	O
on	O
the	O
values	O
of	O
all	O
the	O
other	O
variables	O
in	O
the	O
distribution	O
.	O
that	O
is	O
,	O
given	O
a	O
joint	O
sample	O
xs	O
of	O
all	O
the	O
variables	O
,	O
we	O
generate	O
a	O
new	O
sample	O
xs+1	O
by	O
sampling	O
each	O
component	O
in	O
turn	O
,	O
based	O
on	O
the	O
most	O
recent	O
values	O
of	O
the	O
other	O
variables	O
.	O
for	O
example	O
,	O
if	O
we	O
have	O
d	O
=	O
3	O
variables	O
,	O
we	O
use	O
•	O
xs+1	O
•	O
xs+1	O
•	O
xs+1	O
1	O
∼	O
p	O
(	O
x1|xs	O
2	O
,	O
xs	O
3	O
)	O
2	O
∼	O
p	O
(	O
x2|xs+1	O
3	O
∼	O
p	O
(	O
x3|xs+1	O
,	O
xs+1	O
,	O
xs	O
3	O
)	O
1	O
1	O
2	O
)	O
this	O
readily	O
generalizes	O
to	O
d	O
variables	O
.	O
if	O
xi	O
is	O
a	O
visible	B
variable	O
,	O
we	O
do	O
not	O
sample	O
it	O
,	O
since	O
its	O
value	O
is	O
already	O
known	O
.	O
the	O
expression	O
p	O
(	O
xi|x−i	O
)	O
is	O
called	O
the	O
full	B
conditional	I
for	O
variable	O
i.	O
in	O
general	O
,	O
xi	O
may	O
only	O
depend	O
on	O
some	O
of	O
the	O
other	O
variables	O
.	O
if	O
we	O
represent	O
p	O
(	O
x	O
)	O
as	O
a	O
graphical	B
model	I
,	O
we	O
can	O
infer	O
the	O
dependencies	O
by	O
looking	O
at	O
i	O
’	O
s	O
markov	O
blanket	O
,	O
which	O
are	O
its	O
neighbors	B
in	O
the	O
graph	B
.	O
thus	O
to	O
sample	O
xi	O
,	O
we	O
only	O
need	O
to	O
know	O
the	O
values	O
of	O
i	O
’	O
s	O
neighbors	B
.	O
in	O
this	O
sense	O
,	O
gibbs	O
sampling	O
is	O
a	O
distributed	O
algorithm	O
.	O
however	O
,	O
it	O
is	O
not	O
a	O
parallel	O
algorithm	O
,	O
since	O
the	O
samples	B
must	O
be	O
generated	O
sequentially	O
.	O
for	O
reasons	O
that	O
we	O
will	O
explain	O
in	O
section	O
24.4.1	O
,	O
it	O
is	O
necessary	O
to	O
discard	O
some	O
of	O
the	O
initial	O
samples	B
until	O
the	O
markov	O
chain	O
has	O
burned	B
in	I
,	O
or	O
entered	O
its	O
stationary	B
distribution	I
.	O
we	O
discuss	O
how	O
to	O
estimate	O
when	O
burnin	O
has	O
occured	O
in	O
section	O
24.4.1.	O
in	O
the	O
examples	O
below	O
,	O
we	O
just	O
discard	O
the	O
initial	O
25	O
%	O
of	O
the	O
samples	B
,	O
for	O
simplicity	O
.	O
24.2.2	O
example	O
:	O
gibbs	O
sampling	O
for	O
the	O
ising	O
model	O
in	O
section	O
21.3.2	O
,	O
we	O
applied	O
mean	B
ﬁeld	I
to	O
an	O
ising	O
model	O
.	O
here	O
we	O
apply	O
gibbs	O
sampling	O
.	O
gibbs	O
sampling	O
in	O
pairwise	O
mrf/crf	O
takes	O
the	O
form	O
p	O
(	O
xt|x−t	O
,	O
θ	O
)	O
∝	O
ψst	O
(	O
xs	O
,	O
xt	O
)	O
(	O
cid:20	O
)	O
s∈nbr	O
(	O
t	O
)	O
in	O
the	O
case	O
of	O
an	O
ising	O
model	O
with	O
edge	O
potentials	O
ψ	O
(	O
xs	O
,	O
xt	O
)	O
=	O
exp	O
(	O
jxsxt	O
)	O
,	O
where	O
xt	O
∈	O
2011	O
)	O
3.	O
josiah	O
willard	O
gibbs	O
,	O
1839–1903	O
,	O
was	O
an	O
american	O
physicist	O
.	O
(	O
24.1	O
)	O
24.2.	O
gibbs	O
sampling	O
839	O
sample	O
1	O
,	O
gibbs	O
1	O
sample	O
5	O
,	O
gibbs	O
1	O
mean	B
after	O
15	O
sweeps	O
of	O
gibbs	O
1	O
0.5	O
0	O
−0.5	O
−1	O
0.5	O
0	O
−0.5	O
−1	O
(	O
b	O
)	O
(	O
a	O
)	O
0.5	O
0	O
−0.5	O
−1	O
(	O
c	O
)	O
figure	O
24.1	O
example	O
of	O
image	B
denoising	I
.	O
we	O
use	O
an	O
ising	O
prior	O
with	O
wij	O
=	O
j	O
=	O
1	O
and	O
a	O
gaussian	O
noise	O
model	O
with	O
σ	O
=	O
2.	O
we	O
use	O
gibbs	O
sampling	O
(	O
section	O
24.2	O
)	O
to	O
perform	O
approximate	B
inference	I
.	O
(	O
a	O
)	O
sample	O
from	O
the	O
posterior	O
after	O
one	O
sweep	O
over	O
the	O
image	O
.	O
(	O
b	O
)	O
sample	O
after	O
5	O
sweeps	O
.	O
(	O
c	O
)	O
posterior	B
mean	I
,	O
computed	O
by	O
averaging	O
over	O
15	O
sweeps	O
.	O
compare	O
to	O
figure	O
21.3	O
which	O
shows	O
the	O
results	O
of	O
using	O
mean	B
ﬁeld	I
inference	O
.	O
figure	O
generated	O
by	O
isingimagedenoisedemo	O
.	O
{	O
−1	O
,	O
+1	O
}	O
,	O
the	O
full	B
conditional	I
becomes	O
(	O
cid:26	O
)	O
p	O
(	O
xt	O
=	O
+1|x−t	O
,	O
θ	O
)	O
=	O
=	O
=	O
(	O
cid:26	O
)	O
(	O
cid:7	O
)	O
s∈nbr	O
(	O
t	O
)	O
ψst	O
(	O
xt	O
=	O
+1	O
,	O
xs	O
)	O
(	O
cid:26	O
)	O
s∈nbr	O
(	O
t	O
)	O
ψ	O
(	O
xt	O
=	O
−1	O
,	O
xs	O
)	O
(	O
cid:7	O
)	O
s∈nbr	O
(	O
t	O
)	O
xs	O
]	O
(	O
cid:7	O
)	O
s∈nbr	O
(	O
t	O
)	O
ψ	O
(	O
st	O
=	O
+1	O
,	O
xs	O
)	O
+	O
exp	O
[	O
j	O
exp	O
[	O
j	O
s∈nbr	O
(	O
t	O
)	O
xs	O
]	O
s∈nbr	O
(	O
t	O
)	O
xs	O
]	O
+	O
exp	O
[	O
−j	O
exp	O
[	O
jηt	O
]	O
(	O
cid:7	O
)	O
exp	O
[	O
jηt	O
]	O
+	O
exp	O
[	O
−jηt	O
]	O
=	O
sigm	O
(	O
2jηt	O
)	O
(	O
24.2	O
)	O
(	O
24.3	O
)	O
(	O
24.4	O
)	O
s∈nbr	O
(	O
t	O
)	O
xt	O
and	O
sigm	O
(	O
u	O
)	O
=	O
1/	O
(	O
1	O
+	O
e−u	O
)	O
is	O
the	O
sigmoid	B
where	O
j	O
is	O
the	O
coupling	O
strength	O
,	O
ηt	O
(	O
cid:2	O
)	O
function	O
.	O
it	O
is	O
easy	O
to	O
see	O
that	O
ηt	O
=	O
xt	O
(	O
at	O
−	O
dt	O
)	O
,	O
where	O
at	O
is	O
the	O
number	O
of	O
neighbors	B
that	O
agree	O
with	O
(	O
have	O
the	O
same	O
sign	O
as	O
)	O
t	O
,	O
and	O
dt	O
is	O
the	O
number	O
of	O
neighbors	B
who	O
disagree	O
.	O
if	O
this	O
number	O
is	O
equal	O
,	O
the	O
“	O
forces	O
”	O
on	O
xt	O
cancel	O
out	O
,	O
so	O
the	O
full	B
conditional	I
is	O
uniform	O
.	O
observation	B
model	I
,	O
we	O
have	O
ψt	O
(	O
xt	O
)	O
=	O
n	O
(	O
yt|xt	O
,	O
σ2	O
)	O
.	O
the	O
full	B
conditional	I
becomes	O
we	O
can	O
combine	O
an	O
ising	O
prior	O
with	O
a	O
local	B
evidence	I
term	O
ψt	O
.	O
for	O
example	O
,	O
with	O
a	O
gaussian	O
p	O
(	O
xt	O
=	O
+1|x−t	O
,	O
y	O
,	O
θ	O
)	O
=	O
exp	O
[	O
jηt	O
]	O
ψt	O
(	O
+1	O
)	O
exp	O
[	O
jηt	O
]	O
ψt	O
(	O
+1	O
)	O
+	O
exp	O
[	O
−jηt	O
]	O
ψt	O
(	O
−1	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
2jηt	O
−	O
log	O
ψt	O
(	O
+1	O
)	O
ψt	O
(	O
−1	O
)	O
=	O
sigm	O
(	O
24.5	O
)	O
(	O
24.6	O
)	O
now	O
the	O
probability	O
of	O
xt	O
entering	O
each	O
state	B
is	O
determined	O
both	O
by	O
compatibility	O
with	O
its	O
neighbors	B
(	O
the	O
ising	O
prior	O
)	O
and	O
compatibility	O
with	O
the	O
data	O
(	O
the	O
local	O
likelihood	O
term	O
)	O
.	O
see	O
figure	O
24.1	O
for	O
an	O
example	O
of	O
this	O
algorithm	O
applied	O
to	O
a	O
simple	O
image	O
denoising	O
problem	O
.	O
the	O
results	O
are	O
similar	B
to	O
mean	B
ﬁeld	I
(	O
figure	O
21.3	O
)	O
except	O
that	O
the	O
ﬁnal	O
estimate	O
(	O
based	O
on	O
averaging	O
the	O
samples	B
)	O
is	O
somewhat	O
“	O
blurrier	O
”	O
,	O
due	O
to	O
the	O
fact	O
that	O
mean	B
ﬁeld	I
tends	O
to	O
be	O
over-conﬁdent	O
.	O
840	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
24.2.3	O
example	O
:	O
gibbs	O
sampling	O
for	O
inferring	O
the	O
parameters	O
of	O
a	O
gmm	O
it	O
is	O
straightforward	O
to	O
derive	O
a	O
gibbs	O
sampling	O
algorithm	O
to	O
“	O
ﬁt	O
”	O
a	O
mixture	B
model	I
,	O
especially	O
if	O
we	O
use	O
conjugate	B
priors	I
.	O
we	O
will	O
focus	O
on	O
the	O
case	O
of	O
mixture	O
of	O
gaussians	O
,	O
although	O
the	O
results	O
are	O
easily	O
extended	O
to	O
other	O
kinds	O
of	O
mixture	B
models	O
.	O
(	O
the	O
derivation	O
,	O
which	O
follows	O
from	O
the	O
results	O
of	O
section	O
4.6	O
,	O
is	O
much	O
easier	O
than	O
the	O
corresponding	O
variational	O
bayes	O
algorithm	O
in	O
section	O
21.6.1	O
.	O
)	O
suppose	O
we	O
use	O
a	O
semi-conjugate	B
prior	O
.	O
then	O
the	O
full	B
joint	O
distribution	O
is	O
given	O
by	O
p	O
(	O
x	O
,	O
z	O
,	O
μ	O
,	O
σ	O
,	O
π	O
)	O
=p	O
(	O
x|z	O
,	O
μ	O
,	O
σ	O
)	O
p	O
(	O
z|π	O
)	O
p	O
(	O
π	O
)	O
p	O
(	O
μk	O
)	O
p	O
(	O
σk	O
)	O
k	O
(	O
cid:20	O
)	O
k=1	O
(	O
cid:10	O
)	O
n	O
(	O
cid:20	O
)	O
k	O
(	O
cid:20	O
)	O
=	O
i=1	O
k=1	O
(	O
πkn	O
(	O
xi|μk	O
,	O
σk	O
)	O
)	O
i	O
(	O
zi=k	O
)	O
k	O
(	O
cid:20	O
)	O
dir	O
(	O
π|α	O
)	O
n	O
(	O
μk|m0	O
,	O
v0	O
)	O
iw	O
(	O
σk|s0	O
,	O
ν0	O
)	O
k=1	O
(	O
cid:11	O
)	O
×	O
we	O
use	O
the	O
same	O
prior	O
for	O
each	O
mixture	B
component	O
.	O
the	O
full	B
conditionals	O
are	O
as	O
follows	O
.	O
for	O
the	O
discrete	B
indicators	O
,	O
we	O
have	O
(	O
24.7	O
)	O
(	O
24.8	O
)	O
(	O
24.9	O
)	O
(	O
24.10	O
)	O
(	O
24.11	O
)	O
(	O
24.12	O
)	O
(	O
24.13	O
)	O
(	O
24.14	O
)	O
(	O
24.15	O
)	O
(	O
24.16	O
)	O
(	O
24.17	O
)	O
(	O
24.18	O
)	O
(	O
24.19	O
)	O
p	O
(	O
zi	O
=	O
k|xi	O
,	O
μ	O
,	O
σ	O
,	O
π	O
)	O
∝	O
πkn	O
(	O
xi|μk	O
,	O
σk	O
)	O
for	O
the	O
mixing	B
weights	I
,	O
we	O
have	O
(	O
using	O
results	O
from	O
section	O
3.4	O
)	O
p	O
(	O
π|z	O
)	O
=	O
dir	O
(	O
{	O
αk	O
+	O
i	O
(	O
zi	O
=	O
k	O
)	O
}	O
k	O
k=1	O
)	O
n	O
(	O
cid:4	O
)	O
i=1	O
for	O
the	O
means	O
,	O
we	O
have	O
(	O
using	O
results	O
from	O
section	O
4.6.1	O
)	O
k	O
p	O
(	O
μk|σk	O
,	O
z	O
,	O
x	O
)	O
=n	O
(	O
μk|mk	O
,	O
vk	O
)	O
0	O
+	O
nkς−1	O
=	O
v−1	O
n	O
(	O
cid:4	O
)	O
(	O
cid:7	O
)	O
n	O
v−1	O
mk	O
=	O
vk	O
(	O
σ−1	O
nk	O
(	O
cid:2	O
)	O
i	O
(	O
zi	O
=	O
k	O
)	O
i=1	O
xk	O
(	O
cid:2	O
)	O
i=1	O
i	O
(	O
zi	O
=	O
k	O
)	O
xi	O
k	O
k	O
nkxk	O
+	O
v−1	O
0	O
m0	O
)	O
nk	O
n	O
(	O
cid:4	O
)	O
for	O
the	O
covariances	O
,	O
we	O
have	O
(	O
using	O
results	O
from	O
section	O
4.6.2	O
)	O
p	O
(	O
σk|μk	O
,	O
z	O
,	O
x	O
)	O
=	O
iw	O
(	O
σk|sk	O
,	O
νk	O
)	O
i	O
(	O
zi	O
=	O
k	O
)	O
(	O
xi	O
−	O
μk	O
)	O
(	O
xi	O
−	O
μk	O
)	O
t	O
sk	O
=	O
s0	O
+	O
i=1	O
νk	O
=	O
ν0	O
+	O
nk	O
see	O
gaussmissingfitgibbs	O
for	O
some	O
matlab	O
code	O
.	O
values	O
for	O
x	O
,	O
if	O
necessary	O
.	O
)	O
(	O
this	O
code	O
can	O
also	O
sample	O
missing	O
24.2.	O
gibbs	O
sampling	O
841	O
24.2.3.1	O
label	B
switching	I
although	O
it	O
is	O
simple	O
to	O
implement	O
,	O
gibbs	O
sampling	O
for	O
mixture	B
models	O
has	O
a	O
fundamental	O
weakness	O
.	O
the	O
problem	O
is	O
that	O
the	O
parameters	O
of	O
the	O
model	O
θ	O
,	O
and	O
the	O
indicator	O
functions	O
z	O
,	O
are	O
unidentiﬁable	B
,	O
since	O
we	O
can	O
arbitrarily	O
permute	O
the	O
hidden	B
labels	O
without	O
affecting	O
the	O
likelihood	B
(	O
see	O
section	O
11.3.1	O
)	O
.	O
consequently	O
,	O
we	O
can	O
not	O
just	O
take	O
a	O
monte	O
carlo	O
average	O
of	O
the	O
samples	B
to	O
compute	O
posterior	O
means	O
,	O
since	O
what	O
one	O
sample	O
considers	O
the	O
parameters	O
for	O
cluster	O
1	O
may	O
be	O
what	O
another	O
sample	O
considers	O
the	O
parameters	O
for	O
cluster	O
2.	O
indeed	O
,	O
if	O
we	O
could	O
average	O
over	O
all	O
modes	O
,	O
we	O
would	O
ﬁnd	O
e	O
[	O
μk|d	O
]	O
is	O
the	O
same	O
for	O
all	O
k	O
(	O
assuming	O
a	O
symmetric	B
prior	O
)	O
.	O
this	O
is	O
called	O
the	O
label	B
switching	I
problem	O
.	O
this	O
problem	O
does	O
not	O
arise	O
in	O
em	O
or	O
vbem	O
,	O
which	O
just	O
“	O
lock	O
on	O
”	O
to	O
a	O
single	O
mode	O
.	O
however	O
,	O
it	O
arises	O
in	O
any	O
method	O
that	O
visits	O
multiple	O
modes	O
.	O
in	O
1d	O
problems	O
,	O
one	O
can	O
try	O
to	O
prevent	O
this	O
problem	O
by	O
introducing	O
constraints	O
on	O
the	O
parameters	O
to	O
ensure	O
identiﬁability	O
,	O
e.g.	O
,	O
μ1	O
<	O
μ2	O
<	O
μ3	O
(	O
richardson	O
and	O
green	O
1997	O
)	O
.	O
however	O
,	O
this	O
does	O
not	O
always	O
work	O
,	O
since	O
the	O
likelihood	B
might	O
overwhelm	O
the	O
prior	O
and	O
cause	O
label	B
switching	I
anyway	O
.	O
furthermore	O
,	O
this	O
technique	O
does	O
not	O
scale	O
to	O
higher	O
dimensions	O
.	O
another	O
approach	O
is	O
to	O
post-process	O
the	O
samples	B
by	O
searching	O
for	O
a	O
global	O
label	O
permutation	O
to	O
apply	O
to	O
each	O
sample	O
that	O
minimizes	O
some	O
loss	B
function	I
(	O
stephens	O
2000	O
)	O
;	O
however	O
,	O
this	O
can	O
be	O
slow	O
.	O
perhaps	O
the	O
best	O
solution	O
is	O
simply	O
to	O
“	O
not	O
ask	O
”	O
questions	O
that	O
can	O
not	O
be	O
uniquely	O
identiﬁed	O
.	O
for	O
example	O
,	O
instead	O
of	O
asking	O
for	O
the	O
probability	O
that	O
data	O
point	O
i	O
belongs	O
to	O
cluster	O
k	O
,	O
ask	O
for	O
the	O
probability	O
that	O
data	O
points	O
i	O
and	O
j	O
belong	O
to	O
the	O
same	O
cluster	O
.	O
the	O
latter	O
question	O
is	O
invariant	B
to	O
the	O
labeling	O
.	O
furthermore	O
,	O
it	O
only	O
refers	O
to	O
observable	O
quantities	O
(	O
are	O
i	O
and	O
j	O
grouped	O
together	O
or	O
not	O
)	O
,	O
rather	O
than	O
referring	O
to	O
unobservable	O
quantities	O
,	O
such	O
as	O
latent	B
clusters	O
.	O
this	O
approach	O
has	O
the	O
further	O
advantage	O
that	O
it	O
extends	O
to	O
inﬁnite	B
mixture	I
models	I
,	O
discussed	O
in	O
section	O
25.2	O
,	O
where	O
k	O
is	O
unbounded	O
;	O
in	O
such	O
models	O
,	O
the	O
notion	O
of	O
a	O
hidden	B
cluster	O
is	O
not	O
well	O
deﬁned	O
,	O
but	O
the	O
notion	O
of	O
a	O
partitioning	B
of	O
the	O
data	O
is	O
well	O
deﬁned	O
24.2.4	O
collapsed	O
gibbs	O
sampling	O
*	O
in	O
some	O
cases	O
,	O
we	O
can	O
analytically	O
integrate	B
out	I
some	O
of	O
the	O
unknown	B
quantities	O
,	O
and	O
just	O
sample	O
the	O
rest	O
.	O
this	O
is	O
called	O
a	O
collapsed	O
gibbs	O
sampler	O
,	O
and	O
it	O
tends	O
to	O
be	O
much	O
more	O
efficient	O
,	O
since	O
it	O
is	O
sampling	O
in	O
a	O
lower	O
dimensional	O
space	O
.	O
more	O
precisely	O
,	O
suppose	O
we	O
sample	O
z	O
and	O
integrate	B
out	I
θ.	O
thus	O
the	O
θ	O
parameters	O
do	O
not	O
participate	O
in	O
the	O
markov	O
chain	O
;	O
consequently	O
we	O
can	O
draw	O
conditionally	B
independent	I
samples	O
θs	O
∼	O
p	O
(	O
θ|zs	O
,	O
d	O
)	O
,	O
which	O
will	O
have	O
much	O
lower	O
variance	B
than	O
samples	B
drawn	O
from	O
the	O
joint	O
state	O
space	O
(	O
liu	O
et	O
al	O
.	O
1994	O
)	O
.	O
this	O
process	O
is	O
called	O
rao-blackwellisation	O
,	O
named	O
after	O
the	O
following	O
theorem	O
:	O
theorem	O
24.2.1	O
(	O
rao-blackwell	O
)	O
.	O
let	O
z	O
and	O
θ	O
be	O
dependent	O
random	O
variables	O
,	O
and	O
f	O
(	O
z	O
,	O
θ	O
)	O
be	O
some	O
scalar	O
function	O
.	O
then	O
varz	O
,	O
θ	O
[	O
f	O
(	O
z	O
,	O
θ	O
)	O
]	O
≥	O
varz	O
[	O
eθ	O
[	O
f	O
(	O
z	O
,	O
θ	O
)	O
|z	O
]	O
]	O
this	O
theorem	O
guarantees	O
that	O
the	O
variance	B
of	O
the	O
estimate	O
created	O
by	O
analytically	O
integrating	O
out	O
θ	O
will	O
always	O
be	O
lower	O
(	O
or	O
rather	O
,	O
will	O
never	O
be	O
higher	O
)	O
than	O
the	O
variance	B
of	O
a	O
direct	O
mc	O
in	O
collapsed	O
gibbs	O
,	O
we	O
sample	O
z	O
with	O
θ	O
integrated	O
out	O
;	O
the	O
above	O
rao-blackwell	O
estimate	O
.	O
theorem	O
still	O
applies	O
in	O
this	O
case	O
(	O
liu	O
et	O
al	O
.	O
1994	O
)	O
.	O
(	O
24.20	O
)	O
842	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
zn	O
xn	O
z1	O
x1	O
z1	O
x1	O
α	O
π	O
zi	O
xi	O
θk	O
β	O
(	O
a	O
)	O
α	O
β	O
zn	O
xn	O
zi	O
xi	O
(	O
b	O
)	O
figure	O
24.2	O
(	O
a	O
)	O
a	O
mixture	B
model	I
.	O
(	O
b	O
)	O
after	O
integrating	O
out	O
the	O
parameters	O
.	O
we	O
will	O
encounter	O
rao-blackwellisation	O
again	O
in	O
section	O
23.6.	O
although	O
it	O
can	O
reduce	O
statistical	O
variance	O
,	O
it	O
is	O
only	O
worth	O
doing	O
if	O
the	O
integrating	O
out	O
can	O
be	O
done	O
quickly	O
,	O
otherwise	O
we	O
will	O
not	O
be	O
able	O
to	O
produce	O
as	O
many	O
samples	B
per	O
second	O
as	O
the	O
naive	O
method	O
.	O
we	O
give	O
an	O
example	O
of	O
this	O
below	O
.	O
24.2.4.1	O
example	O
:	O
collapsed	O
gibbs	O
for	O
ﬁtting	O
a	O
gmm	O
consider	O
a	O
gmm	O
with	O
a	O
fully	O
conjugate	B
prior	I
.	O
in	O
this	O
case	O
we	O
can	O
analytically	O
integrate	B
out	I
the	O
model	O
parameters	O
μk	O
,	O
σk	O
and	O
π	O
,	O
and	O
just	O
sample	O
the	O
indicators	O
z.	O
once	O
we	O
integrate	B
out	I
π	O
,	O
all	O
the	O
zi	O
nodes	B
become	O
inter-dependent	O
.	O
similarly	O
,	O
once	O
we	O
integrate	B
out	I
θk	O
,	O
all	O
the	O
xi	O
nodes	B
become	O
inter-dependent	O
,	O
as	O
shown	O
in	O
figure	O
24.2	O
(	O
b	O
)	O
.	O
nevertheless	O
,	O
we	O
can	O
easily	O
compute	O
the	O
full	B
conditionals	O
as	O
follows	O
:	O
p	O
(	O
zi	O
=	O
k|z−i	O
,	O
x	O
,	O
α	O
,	O
β	O
)	O
∝	O
p	O
(	O
zi	O
=	O
k|z−i	O
,	O
α	O
,	O
β	O
)	O
p	O
(	O
x|zi	O
=	O
k	O
,	O
z−i	O
,	O
α	O
,	O
β	O
)	O
∝	O
p	O
(	O
zi	O
=	O
k|z−i	O
,	O
α	O
)	O
p	O
(	O
xi|x−i	O
,	O
zi	O
=	O
k	O
,	O
z−i	O
,	O
β	O
)	O
p	O
(	O
x−i|zi	O
=	O
k	O
,	O
z−i	O
,	O
β	O
)	O
∝	O
p	O
(	O
zi	O
=	O
k|z−i	O
,	O
α	O
)	O
p	O
(	O
xi|x−i	O
,	O
zi	O
=	O
k	O
,	O
z−i	O
,	O
β	O
)	O
(	O
24.21	O
)	O
(	O
24.22	O
)	O
(	O
24.23	O
)	O
where	O
β	O
=	O
(	O
m0	O
,	O
v0	O
,	O
s0	O
,	O
ν0	O
)	O
are	O
the	O
hyper-parameters	B
for	O
the	O
class-conditional	O
densities	O
.	O
the	O
ﬁrst	O
term	O
can	O
be	O
obtained	O
by	O
integrating	O
out	O
π.	O
suppose	O
we	O
use	O
a	O
symmetric	B
prior	O
of	O
the	O
form	O
π	O
∼	O
dir	O
(	O
α	O
)	O
,	O
whereα	O
k	O
=	O
α/k	O
.	O
from	O
equation	O
5.26	O
we	O
have	O
p	O
(	O
z1	O
,	O
.	O
.	O
.	O
,	O
zn|α	O
)	O
=	O
k	O
(	O
cid:20	O
)	O
γ	O
(	O
α	O
)	O
γ	O
(	O
nk	O
+	O
α/k	O
)	O
γ	O
(	O
n	O
+	O
α	O
)	O
k=1	O
γ	O
(	O
α/k	O
)	O
(	O
24.24	O
)	O
24.2.	O
gibbs	O
sampling	O
hence	O
p	O
(	O
zi	O
=	O
k|z−i	O
,	O
α	O
)	O
=	O
(	O
cid:7	O
)	O
where	O
nk	O
,	O
−i	O
(	O
cid:2	O
)	O
xγ	O
(	O
x	O
)	O
.	O
843	O
(	O
24.25	O
)	O
p	O
(	O
z1	O
:	O
n|α	O
)	O
p	O
(	O
z−i|α	O
)	O
=	O
γ	O
(	O
n	O
+	O
α	O
−	O
1	O
)	O
γ	O
(	O
n	O
+	O
α	O
)	O
1	O
1	O
×	O
γ	O
(	O
nk	O
+	O
α/k	O
)	O
γ	O
(	O
nk	O
,	O
−i	O
+	O
α/k	O
)	O
γ	O
(	O
n	O
+α	O
)	O
γ	O
(	O
n	O
+α−1	O
)	O
γ	O
(	O
nk	O
,	O
−i	O
+	O
1	O
+	O
α/k	O
)	O
=	O
(	O
24.26	O
)	O
n	O
(	O
cid:5	O
)	O
=i	O
i	O
(	O
zn	O
=	O
k	O
)	O
=n	O
k	O
−	O
1	O
,	O
and	O
where	O
we	O
exploited	O
the	O
fact	O
that	O
γ	O
(	O
x	O
+	O
1	O
)	O
=	O
γ	O
(	O
nk	O
,	O
−i	O
+	O
α/k	O
)	O
=	O
nk	O
,	O
−i	O
+	O
α/k	O
n	O
+	O
α	O
−	O
1	O
to	O
obtain	O
the	O
second	O
term	O
in	O
equation	O
24.23	O
,	O
which	O
is	O
the	O
posterior	B
predictive	I
distribution	I
for	O
xi	O
given	O
all	O
the	O
other	O
data	O
and	O
all	O
the	O
assignments	O
,	O
we	O
use	O
the	O
fact	O
that	O
p	O
(	O
xi|x−i	O
,	O
z−i	O
,	O
zi	O
=	O
k	O
,	O
β	O
)	O
=	O
p	O
(	O
xi|d−i	O
,	O
k	O
)	O
(	O
24.27	O
)	O
where	O
d−i	O
,	O
k	O
=	O
{	O
xj	O
:	O
zj	O
=	O
k	O
,	O
j	O
(	O
cid:8	O
)	O
=	O
i	O
}	O
is	O
all	O
the	O
data	O
assigned	O
to	O
cluster	O
k	O
except	O
for	O
xi	O
.	O
if	O
we	O
use	O
a	O
conjugate	B
prior	I
for	O
θk	O
,	O
we	O
can	O
compute	O
p	O
(	O
xi|d−i	O
,	O
k	O
)	O
in	O
closed	O
form	O
.	O
furthermore	O
,	O
we	O
can	O
efficiently	O
update	O
these	O
predictive	B
likelihoods	O
by	O
caching	O
the	O
sufficient	B
statistics	I
for	O
each	O
cluster	O
.	O
to	O
compute	O
the	O
above	O
expression	O
,	O
we	O
remove	O
xi	O
’	O
s	O
statistics	O
from	O
its	O
current	O
cluster	O
(	O
namely	O
zi	O
)	O
,	O
and	O
then	O
evaluate	O
xi	O
under	O
each	O
cluster	O
’	O
s	O
posterior	O
predictive	O
.	O
once	O
we	O
have	O
picked	O
a	O
new	O
cluster	O
,	O
we	O
add	O
xi	O
’	O
s	O
statistics	O
to	O
this	O
new	O
cluster	O
.	O
some	O
pseudo-code	O
for	O
one	O
step	O
of	O
the	O
algorithm	O
is	O
shown	O
in	O
algorithm	O
1	O
,	O
based	O
on	O
(	O
sud-	O
derth	O
2006	O
,	O
p94	O
)	O
.	O
(	O
we	O
update	O
the	O
nodes	B
in	O
random	O
order	O
to	O
improve	O
the	O
mixing	B
time	I
,	O
as	O
suggested	O
in	O
(	O
roberts	O
and	O
sahu	O
1997	O
)	O
.	O
)	O
we	O
can	O
initialize	O
the	O
sample	O
by	O
sequentially	O
sampling	O
from	O
p	O
(	O
zi|z1	O
:	O
i−1	O
,	O
x1	O
:	O
i	O
)	O
.	O
(	O
see	O
fmgibbs	O
for	O
some	O
matlab	O
code	O
,	O
by	O
yee-whye	O
teh	O
.	O
)	O
in	O
the	O
case	O
of	O
gmms	O
,	O
both	O
the	O
naive	O
sampler	O
and	O
collapsed	O
sampler	O
take	O
o	O
(	O
n	O
kd	O
)	O
time	O
per	O
step	O
.	O
algorithm	O
24.1	O
:	O
collapsed	O
gibbs	O
sampler	O
for	O
a	O
mixture	B
model	I
1	O
for	O
each	O
i	O
=	O
1	O
:	O
n	O
in	O
random	O
order	O
do	O
2	O
remove	O
xi	O
’	O
s	O
sufficient	B
statistics	I
from	O
old	O
cluster	O
zi	O
;	O
for	O
each	O
k	O
=	O
1	O
:	O
k	O
do	O
compute	O
pk	O
(	O
xi	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
xi|	O
{	O
xj	O
:	O
zj	O
=	O
k	O
,	O
j	O
(	O
cid:8	O
)	O
=	O
i	O
}	O
)	O
;	O
compute	O
p	O
(	O
zi	O
=	O
k|z−i	O
,	O
d	O
)	O
∝	O
(	O
nk	O
,	O
−i	O
+	O
α/k	O
)	O
pk	O
(	O
xi	O
)	O
;	O
sample	O
zi	O
∼	O
p	O
(	O
zi|·	O
)	O
;	O
add	O
xi	O
’	O
s	O
sufficient	B
statistics	I
to	O
new	O
cluster	O
zi	O
3	O
4	O
5	O
6	O
7	O
n	O
(	O
cid:4	O
)	O
a	O
comparison	O
of	O
this	O
method	O
with	O
the	O
standard	O
gibbs	O
sampler	O
is	O
shown	O
in	O
figure	O
24.3.	O
the	O
vertical	O
axis	O
is	O
the	O
data	O
log	O
probability	O
at	O
each	O
iteration	O
,	O
computed	O
using	O
log	O
p	O
(	O
d|z	O
,	O
θ	O
)	O
=	O
log	O
[	O
πzi	O
p	O
(	O
xi|θzi	O
)	O
]	O
(	O
24.28	O
)	O
i=1	O
to	O
compute	O
this	O
quantity	O
using	O
the	O
collapsed	O
sampler	O
,	O
we	O
have	O
to	O
sample	O
θ	O
=	O
(	O
π	O
,	O
θ1	O
:	O
k	O
)	O
given	O
the	O
data	O
and	O
the	O
current	O
assignment	O
z.	O
in	O
figure	O
24.3	O
we	O
see	O
that	O
the	O
collapsed	O
sampler	O
does	O
indeed	O
generally	O
work	O
better	O
than	O
the	O
vanilla	O
sampler	O
.	O
occasionally	O
,	O
however	O
,	O
both	O
methods	O
can	O
get	O
stuck	O
in	O
poor	O
local	O
modes	O
.	O
(	O
note	O
844	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
)	O
θ	O
,	O
π	O
|	O
x	O
(	O
p	O
g	O
o	O
l	O
−350	O
−400	O
−450	O
−500	O
−550	O
−600	O
100	O
standard	O
gibbs	O
sampler	O
rao−blackwellized	O
sampler	O
102	O
103	O
101	O
iteration	O
(	O
a	O
)	O
)	O
θ	O
,	O
π	O
|	O
x	O
(	O
p	O
g	O
o	O
l	O
−350	O
−400	O
−450	O
−500	O
−550	O
−600	O
100	O
standard	O
gibbs	O
sampler	O
rao−blackwellized	O
sampler	O
102	O
103	O
101	O
iteration	O
(	O
b	O
)	O
figure	O
24.3	O
comparison	O
of	O
collapsed	O
(	O
red	O
)	O
and	O
vanilla	O
(	O
blue	O
)	O
gibbs	O
sampling	O
for	O
a	O
mixture	O
of	O
k	O
=	O
4	O
two-	O
dimensional	O
gaussians	O
applied	O
to	O
n	O
=	O
300	O
data	O
points	O
(	O
shown	O
in	O
figure	O
25.7	O
)	O
.	O
we	O
plot	O
log	O
probability	O
of	O
the	O
data	O
vs	O
iteration	O
.	O
(	O
a	O
)	O
20	O
different	O
random	O
initializations	O
.	O
(	O
b	O
)	O
logprob	O
averaged	O
over	O
100	O
different	O
random	O
initializations	O
.	O
solid	O
line	O
is	O
the	O
median	B
,	O
thick	O
dashed	O
in	O
the	O
0.25	O
and	O
0.75	O
quantiles	O
,	O
and	O
thin	O
dashed	O
are	O
the	O
0.05	O
and	O
0.95	O
quintiles	O
.	O
source	O
:	O
figure	O
2.20	O
of	O
(	O
sudderth	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
.	O
e	O
r	O
o	O
c	O
s	O
h	O
t	O
a	O
m	O
80	O
70	O
60	O
50	O
40	O
30	O
20	O
10	O
5	O
0	O
−5	O
e	O
p	O
o	O
s	O
l	O
−2	O
−1	O
0	O
ses	O
(	O
a	O
)	O
1	O
2	O
5	O
10	O
20	O
15	O
sample	O
size	O
25	O
30	O
(	O
b	O
)	O
e	O
r	O
o	O
c	O
s	O
h	O
t	O
a	O
m	O
80	O
70	O
60	O
50	O
40	O
30	O
20	O
1	O
2	O
−2	O
−1	O
0	O
ses	O
(	O
c	O
)	O
figure	O
24.4	O
(	O
a	O
)	O
least	B
squares	I
regression	O
lines	O
for	O
math	O
scores	B
vs	O
socio-economic	O
status	O
for	O
100	O
schools	O
.	O
(	O
b	O
)	O
plot	O
of	O
ˆw2j	O
(	O
the	O
slope	O
)	O
vs	O
nj	O
(	O
sample	O
size	O
)	O
for	O
the	O
100	O
population	O
mean	O
(	O
pooled	B
estimate	O
)	O
is	O
in	O
bold	O
.	O
schools	O
.	O
the	O
extreme	O
slopes	O
tend	O
to	O
correspond	O
to	O
schools	O
with	O
smaller	O
sample	O
sizes	O
.	O
(	O
c	O
)	O
predictions	O
from	O
the	O
hierarchical	O
model	O
.	O
population	O
mean	O
is	O
in	O
bold	O
.	O
based	O
on	O
figure	O
11.1	O
of	O
(	O
hoff	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
multilevellinregdemo	O
,	O
written	O
by	O
emtiyaz	O
khan	O
.	O
that	O
the	O
error	O
bars	O
in	O
figure	O
24.3	O
(	O
b	O
)	O
are	O
averaged	O
over	O
starting	O
values	O
,	O
whereas	O
the	O
theorem	O
refers	O
to	O
mc	O
samples	B
in	O
a	O
single	O
run	O
.	O
)	O
24.2.5	O
gibbs	O
sampling	O
for	O
hierarchical	O
glms	O
often	O
we	O
have	O
data	O
from	O
multiple	O
related	O
sources	O
.	O
if	O
some	O
sources	O
are	O
more	O
reliable	O
and/or	O
data-rich	O
than	O
others	O
,	O
it	O
makes	O
sense	O
to	O
model	O
all	O
the	O
data	O
simultaneously	O
,	O
so	O
as	O
to	O
enable	O
the	O
borrowing	O
of	O
statistical	O
strength	O
.	O
one	O
of	O
the	O
most	O
natural	O
way	O
to	O
solve	O
such	O
problems	O
is	O
to	O
use	O
hierarchical	O
bayesian	O
modeling	O
,	O
also	O
called	O
multi-level	B
modeling	I
.	O
in	O
section	O
9.6	O
,	O
we	O
discussed	O
a	O
way	O
to	O
perform	O
approximate	B
inference	I
in	O
such	O
models	O
using	O
variational	O
methods	O
.	O
here	O
we	O
discuss	O
how	O
to	O
use	O
gibbs	O
sampling	O
.	O
to	O
explain	O
the	O
method	O
,	O
consider	O
the	O
following	O
example	O
.	O
suppose	O
we	O
have	O
data	O
on	O
students	O
24.2.	O
gibbs	O
sampling	O
845	O
μw	O
σw	O
wj	O
yij	O
σ2	O
xij	O
nj	O
j	O
figure	O
24.5	O
multi-level	B
model	I
for	O
linear	B
regression	I
.	O
in	O
different	O
schools	O
.	O
such	O
data	O
is	O
naturally	O
modeled	O
in	O
a	O
two-level	O
hierarchy	O
:	O
we	O
let	O
yij	O
be	O
the	O
response	B
variable	I
we	O
want	O
to	O
predict	O
for	O
student	O
i	O
in	O
school	O
j.	O
this	O
prediction	O
can	O
be	O
based	O
on	O
school	O
and	O
student	O
speciﬁc	O
covariates	B
,	O
xij	O
.	O
since	O
the	O
quality	O
of	O
schools	O
varies	O
,	O
we	O
want	O
to	O
use	O
a	O
separate	O
parameter	B
for	O
each	O
school	O
.	O
so	O
our	O
model	O
becomes	O
yij	O
=	O
xt	O
ijwj	O
+	O
ij	O
(	O
24.29	O
)	O
we	O
will	O
illustrate	O
this	O
model	O
below	O
,	O
using	O
a	O
dataset	O
from	O
(	O
hoff	O
2009	O
,	O
p197	O
)	O
,	O
where	O
xij	O
is	O
the	O
socio-economic	O
status	O
(	O
ses	O
)	O
of	O
student	O
i	O
in	O
school	O
y	O
,	O
and	O
yij	O
is	O
their	O
math	O
score	O
.	O
we	O
could	O
ﬁt	O
each	O
wj	O
separately	O
,	O
but	O
this	O
can	O
give	O
poor	O
results	O
if	O
the	O
sample	O
size	O
of	O
a	O
given	O
school	O
is	O
small	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
24.4	O
(	O
a	O
)	O
,	O
which	O
plots	O
the	O
least	B
squares	I
regression	O
line	O
estimated	O
separately	O
for	O
each	O
of	O
the	O
j	O
=	O
100	O
schools	O
.	O
we	O
see	O
that	O
most	O
of	O
the	O
slopes	O
are	O
positive	O
,	O
but	O
there	O
are	O
a	O
few	O
“	O
errant	O
”	O
cases	O
where	O
the	O
slope	O
is	O
negative	O
.	O
it	O
turns	O
out	O
that	O
the	O
lines	O
with	O
extreme	O
slopes	O
tend	O
to	O
be	O
in	O
schools	O
with	O
small	O
sample	O
size	O
,	O
as	O
shown	O
in	O
figure	O
24.4	O
(	O
b	O
)	O
.	O
thus	O
we	O
may	O
not	O
necessarily	O
trust	O
these	O
ﬁts	O
.	O
we	O
can	O
get	O
better	O
results	O
if	O
we	O
construct	O
a	O
hierarchical	O
bayesian	O
model	O
,	O
in	O
which	O
the	O
wj	O
are	O
assumed	O
to	O
come	O
from	O
a	O
common	O
prior	O
:	O
wj	O
∼	O
n	O
(	O
μw	O
,	O
σw	O
)	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
24.5.	O
in	O
this	O
model	O
,	O
the	O
schools	O
with	O
small	O
sample	O
size	O
borrow	O
statistical	O
strength	O
from	O
the	O
schools	O
with	O
larger	O
sample	O
size	O
,	O
because	O
the	O
wj	O
’	O
s	O
are	O
correlated	O
via	O
the	O
latent	B
common	O
parents	B
(	O
μw	O
,	O
σw	O
)	O
.	O
(	O
it	O
is	O
crucial	O
that	O
these	O
hyper-parameters	B
be	O
inferrred	O
from	O
data	O
;	O
if	O
they	O
were	O
ﬁxed	O
constants	O
,	O
the	O
wj	O
would	O
be	O
conditionally	B
independent	I
,	O
and	O
there	O
would	O
be	O
no	O
information	O
sharing	O
between	O
them	O
.	O
)	O
to	O
complete	B
the	O
model	O
speciﬁcation	O
,	O
we	O
must	O
specify	O
priors	O
for	O
the	O
shared	B
parameters	O
.	O
fol-	O
lowing	O
(	O
hoff	O
2009	O
,	O
p198	O
)	O
,	O
we	O
will	O
use	O
the	O
following	O
semi-conjugate	B
forms	O
,	O
for	O
convenience	O
:	O
μw	O
∼	O
n	O
(	O
μ0	O
,	O
v0	O
)	O
σw	O
∼	O
iw	O
(	O
η0	O
,	O
s−1	O
0	O
)	O
σ2	O
∼	O
ig	O
(	O
ν0/2	O
,	O
ν0σ2	O
given	O
this	O
,	O
it	O
is	O
simple	O
to	O
show	O
that	O
the	O
full	B
conditionals	O
needed	O
for	O
gibbs	O
sampling	O
have	O
the	O
(	O
24.30	O
)	O
(	O
24.31	O
)	O
(	O
24.32	O
)	O
0/2	O
)	O
846	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
following	O
forms	O
.	O
for	O
the	O
group-speciﬁc	O
weights	O
:	O
p	O
(	O
wj|dj	O
,	O
θ	O
)	O
=n	O
(	O
wj|μj	O
,	O
σj	O
)	O
j	O
=	O
σ−1	O
+	O
xt	O
σ−1	O
j	O
xj/σ2	O
μj	O
=	O
σj	O
(	O
σ−1μ	O
+	O
xt	O
j	O
yj/σ2	O
)	O
for	O
the	O
overall	O
mean	B
:	O
p	O
(	O
μw|w1	O
:	O
j	O
,	O
σw	O
)	O
=n	O
(	O
μ|μn	O
,	O
σn	O
)	O
0	O
+	O
jς−1	O
σ−1	O
n	O
=	O
v−1	O
(	O
cid:7	O
)	O
μn	O
=	O
σn	O
(	O
v−1	O
j	O
wj	O
.	O
for	O
the	O
overall	O
covariance	B
:	O
0	O
μ0	O
+	O
jς−1w	O
)	O
where	O
w	O
=	O
1	O
j	O
p	O
(	O
σw|μw	O
,	O
w1	O
:	O
j	O
)	O
=	O
iw	O
(	O
(	O
s0	O
+	O
sμ	O
)	O
(	O
cid:4	O
)	O
−1	O
,	O
η0	O
+	O
j	O
)	O
(	O
wj	O
−	O
μw	O
)	O
(	O
wj	O
−	O
μw	O
)	O
t	O
sμ	O
=	O
for	O
the	O
noise	O
variance	O
:	O
j	O
p	O
(	O
σ2|d	O
,	O
w1	O
:	O
j	O
)	O
=	O
ig	O
(	O
[	O
ν0	O
+	O
n	O
]	O
/2	O
,	O
[	O
ν0σ2	O
0	O
+	O
ssr	O
(	O
w1	O
:	O
j	O
)	O
]	O
/2	O
)	O
j	O
(	O
cid:4	O
)	O
nj	O
(	O
cid:4	O
)	O
ssr	O
(	O
w1	O
:	O
j	O
)	O
=	O
(	O
yij	O
−	O
wt	O
j	O
xij	O
)	O
2	O
(	O
24.33	O
)	O
(	O
24.34	O
)	O
(	O
24.35	O
)	O
(	O
24.36	O
)	O
(	O
24.37	O
)	O
(	O
24.38	O
)	O
(	O
24.39	O
)	O
(	O
24.40	O
)	O
(	O
24.41	O
)	O
(	O
24.42	O
)	O
j=1	O
i=1	O
applying	O
gibbs	O
sampling	O
to	O
our	O
hierarchical	O
model	O
,	O
we	O
get	O
the	O
results	O
shown	O
in	O
figure	O
24.4	O
(	O
c	O
)	O
.	O
the	O
light	O
gray	O
lines	O
plot	O
the	O
mean	B
of	O
the	O
posterior	B
predictive	I
distribution	I
for	O
each	O
school	O
:	O
e	O
[	O
yj|xij	O
]	O
=	O
xt	O
ij	O
ˆwj	O
where	O
ˆwj	O
=	O
e	O
[	O
wj|d	O
]	O
≈	O
1	O
s	O
s	O
(	O
cid:4	O
)	O
s=1	O
w	O
(	O
s	O
)	O
j	O
(	O
24.43	O
)	O
(	O
24.44	O
)	O
the	O
dark	O
gray	O
line	O
in	O
the	O
middle	O
plots	O
the	O
prediction	O
using	O
the	O
overall	O
mean	B
parameters	O
,	O
xt	O
ij	O
ˆμw	O
.	O
we	O
see	O
that	O
the	O
method	O
has	O
regularized	O
the	O
ﬁts	O
quite	O
nicely	O
,	O
without	O
enforcing	O
too	O
much	O
(	O
the	O
amount	O
of	O
shrinkage	B
is	O
controlled	O
by	O
σw	O
,	O
which	O
in	O
turns	O
depends	O
on	O
the	O
uniformity	O
.	O
hyper-parameters	B
;	O
in	O
this	O
example	O
,	O
we	O
used	O
vague	O
values	O
.	O
)	O
24.2.6	O
bugs	O
and	O
jags	O
one	O
reason	O
gibbs	O
sampling	O
is	O
so	O
popular	O
is	O
that	O
it	O
is	O
possible	O
to	O
design	O
general	O
purpose	O
software	O
that	O
will	O
work	O
for	O
almost	O
any	O
model	O
.	O
this	O
software	O
just	O
needs	O
a	O
model	O
speciﬁcation	O
,	O
usually	O
in	O
the	O
form	O
a	O
directed	B
graphical	I
model	I
(	O
speciﬁed	O
in	O
a	O
ﬁle	O
,	O
or	O
created	O
with	O
a	O
graphical	O
user	O
interface	O
)	O
,	O
and	O
a	O
library	O
of	O
methods	O
for	O
sampling	O
from	O
different	O
kinds	O
of	O
full	B
conditionals	O
.	O
(	O
this	O
can	O
often	O
be	O
done	O
using	O
adaptive	B
rejection	I
sampling	I
,	O
described	O
in	O
section	O
23.3.4	O
.	O
)	O
an	O
example	O
24.2.	O
gibbs	O
sampling	O
847	O
of	O
such	O
a	O
package	O
is	O
bugs	O
(	O
lunn	O
et	O
al	O
.	O
2000	O
)	O
,	O
which	O
stands	O
for	O
“	O
bayesian	O
updating	O
using	O
gibbs	O
sampling	O
”	O
.	O
bugs	O
is	O
very	O
widely	O
used	O
in	O
biostatistics	O
and	O
social	O
science	O
.	O
another	O
more	O
recent	O
,	O
but	O
very	O
similar	B
,	O
package	O
is	O
jags	O
(	O
plummer	O
2003	O
)	O
,	O
which	O
stands	O
for	O
“	O
just	O
another	O
gibbs	O
sampler	O
”	O
.	O
this	O
uses	O
a	O
similar	B
model	O
speciﬁcation	O
language	O
to	O
bugs	O
.	O
for	O
example	O
,	O
we	O
can	O
describe	O
the	O
model	O
in	O
figure	O
24.5	O
as	O
follows	O
:	O
model	O
{	O
for	O
(	O
i	O
in	O
1	O
:	O
n	O
)	O
{	O
for	O
(	O
j	O
in	O
1	O
:	O
j	O
)	O
{	O
y	O
[	O
i	O
,	O
j	O
]	O
~	O
dnorm	O
(	O
y.hat	O
[	O
i	O
,	O
j	O
]	O
,	O
tau.y	O
)	O
y.hat	O
[	O
i	O
,	O
j	O
]	O
<	O
-	O
inprod	O
(	O
w	O
[	O
j	O
,	O
]	O
,	O
x	O
[	O
i	O
,	O
j	O
,	O
]	O
)	O
}	O
}	O
tau.y	O
<	O
-	O
pow	O
(	O
sigma.y	O
,	O
-2	O
)	O
sigma.y	O
~	O
dunif	O
(	O
0,100	O
)	O
for	O
(	O
j	O
in	O
1	O
:	O
j	O
)	O
{	O
w	O
[	O
j	O
,	O
]	O
~	O
dmnorm	O
(	O
mu	O
,	O
sigmainv	O
)	O
}	O
sigmainv	O
~	O
dwish	O
(	O
s0	O
[	O
,	O
]	O
,	O
eta0	O
)	O
mu	O
~	O
dmnorm	O
(	O
mu0	O
,	O
v0inv	O
)	O
}	O
we	O
can	O
then	O
just	O
pass	O
this	O
model	O
to	O
bugs	O
or	O
jags	O
,	O
which	O
will	O
generate	O
samples	O
for	O
us	O
.	O
see	O
the	O
webpages	O
for	O
details	O
.	O
although	O
this	O
approach	O
is	O
appealing	O
,	O
unfortunately	O
it	O
can	O
be	O
much	O
slower	O
than	O
using	O
hand-	O
written	O
code	O
,	O
especially	O
for	O
complex	O
models	O
.	O
there	O
has	O
been	O
some	O
work	O
on	O
automatically	O
deriving	O
model-speciﬁc	O
optimized	O
inference	B
code	O
(	O
fischer	O
and	O
schumann	O
2003	O
)	O
,	O
but	O
fast	O
code	O
still	O
typically	O
requires	O
human	O
expertise	O
.	O
24.2.7	O
the	O
imputation	B
posterior	O
(	O
ip	O
)	O
algorithm	O
the	O
imputation	B
posterior	O
or	O
ip	O
algorithm	O
(	O
tanner	O
and	O
wong	O
1987	O
)	O
is	O
a	O
special	O
case	O
of	O
gibbs	O
sampling	O
in	O
which	O
we	O
group	O
the	O
variables	O
into	O
two	O
classes	O
:	O
hidden	B
variables	I
z	O
and	O
parameters	O
θ.	O
this	O
should	O
sound	O
familiar	O
:	O
it	O
is	O
basically	O
an	O
mcmc	O
version	O
of	O
em	O
,	O
where	O
the	O
e	O
step	O
gets	O
replaced	O
by	O
the	O
i	O
step	O
,	O
and	O
the	O
m	O
step	O
gets	O
replaced	O
the	O
p	O
step	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
more	O
general	O
strategy	O
called	O
data	B
augmentation	I
,	O
whereby	O
we	O
introduce	O
auxiliary	B
variables	I
in	O
order	O
to	O
simplify	O
the	O
posterior	O
computations	O
(	O
here	O
the	O
computation	O
of	O
p	O
(	O
θ|d	O
)	O
)	O
.	O
see	O
(	O
tanner	O
1996	O
;	O
van	O
dyk	O
and	O
meng	O
2001	O
)	O
for	O
more	O
information	B
.	O
24.2.8	O
blocking	O
gibbs	O
sampling	O
gibbs	O
sampling	O
can	O
be	O
quite	O
slow	O
,	O
since	O
it	O
only	O
updates	O
one	O
variable	O
at	O
a	O
time	O
(	O
so-called	O
single	B
site	I
updating	I
)	O
.	O
if	O
the	O
variables	O
are	O
highly	O
correlated	O
,	O
it	O
will	O
take	O
a	O
long	O
time	O
to	O
move	O
away	O
from	O
the	O
current	O
state	B
.	O
this	O
is	O
illustrated	O
in	O
figure	O
24.6	O
,	O
where	O
we	O
illustrate	O
sampling	O
from	O
a	O
2d	O
gaussian	O
(	O
see	O
exercise	O
24.1	O
for	O
the	O
details	O
)	O
.	O
if	O
the	O
variables	O
are	O
highly	O
correlated	O
,	O
the	O
algorithm	O
848	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
figure	O
24.6	O
illustration	O
of	O
potentially	O
slow	O
sampling	O
when	O
using	O
gibbs	O
sampling	O
for	O
a	O
skewed	O
2d	O
gaus-	O
sian	O
.	O
based	O
on	O
figure	O
11.11	O
of	O
(	O
bishop	O
2006b	O
)	O
.	O
figure	O
generated	O
by	O
gibbsgaussdemo	O
.	O
will	O
move	O
very	O
slowly	O
through	O
the	O
state	B
space	I
.	O
in	O
particular	O
,	O
the	O
size	O
of	O
the	O
moves	O
is	O
controlled	O
by	O
the	O
variance	B
of	O
the	O
conditional	O
distributions	O
.	O
if	O
this	O
is	O
(	O
cid:2	O
)	O
in	O
the	O
x1	O
direction	O
,	O
and	O
the	O
support	B
of	O
the	O
distribution	O
is	O
l	O
along	O
this	O
dimension	O
,	O
then	O
we	O
need	O
o	O
(	O
(	O
l/	O
(	O
cid:2	O
)	O
)	O
2	O
)	O
steps	O
to	O
obtain	O
an	O
independent	O
sample	O
.	O
in	O
some	O
cases	O
we	O
can	O
efficiently	O
sample	O
groups	O
of	O
variables	O
at	O
a	O
time	O
.	O
this	O
is	O
called	O
blocking	O
gibbs	O
sampling	O
or	O
blocked	O
gibbs	O
sampling	O
(	O
jensen	O
et	O
al	O
.	O
1995	O
;	O
wilkinson	O
and	O
yeung	O
2002	O
)	O
,	O
and	O
can	O
make	O
much	O
bigger	O
moves	O
through	O
the	O
state	B
space	I
.	O
24.3	O
metropolis	O
hastings	O
algorithm	O
although	O
gibbs	O
sampling	O
is	O
simple	O
,	O
it	O
is	O
somewhat	O
restricted	O
in	O
the	O
set	O
of	O
models	O
to	O
which	O
it	O
can	O
be	O
applied	O
.	O
for	O
example	O
,	O
it	O
is	O
not	O
much	O
help	O
in	O
computing	O
p	O
(	O
w|d	O
)	O
for	O
a	O
logistic	B
regression	I
model	O
,	O
since	O
the	O
corresponding	O
graphical	B
model	I
has	O
no	O
useful	O
markov	O
structure	O
.	O
in	O
addition	O
,	O
gibbs	O
sampling	O
can	O
be	O
quite	O
slow	O
,	O
as	O
we	O
mentioned	O
above	O
.	O
fortunately	O
,	O
there	O
is	O
a	O
more	O
general	O
algorithm	O
that	O
can	O
be	O
used	O
,	O
known	O
as	O
the	O
metropolis	O
hastings	O
or	O
mh	O
algorithm	O
,	O
which	O
we	O
describe	O
below	O
.	O
24.3.1	O
basic	O
idea	O
the	O
basic	O
idea	O
in	O
mh	O
is	O
that	O
at	O
each	O
step	O
,	O
we	O
propose	B
to	O
move	O
from	O
the	O
current	O
state	B
x	O
to	O
a	O
with	O
probability	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
,	O
where	O
q	O
is	O
called	O
the	O
proposal	B
distribution	I
(	O
also	O
called	O
new	O
state	B
x	O
(	O
cid:2	O
)	O
the	O
kernel	B
)	O
.	O
the	O
user	O
is	O
free	O
to	O
use	O
any	O
kind	O
of	O
proposal	O
they	O
want	O
,	O
subject	O
to	O
some	O
conditions	O
which	O
we	O
explain	O
below	O
.	O
this	O
makes	O
mh	O
quite	O
a	O
ﬂexible	O
method	O
.	O
a	O
commonly	O
used	O
proposal	O
is	O
a	O
symmetric	B
gaussian	O
distribution	O
centered	O
on	O
the	O
current	O
state	B
,	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
=n	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
,	O
σ	O
)	O
;	O
this	O
is	O
called	O
a	O
random	O
walk	O
metropolis	O
algorithm	O
.	O
we	O
discuss	O
how	O
to	O
choose	O
σ	O
in	O
section	O
24.3.3.	O
if	O
we	O
use	O
a	O
proposal	O
of	O
the	O
form	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
=q	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
,	O
where	O
the	O
new	O
state	B
is	O
independent	O
of	O
the	O
old	O
state	B
,	O
we	O
get	O
a	O
method	O
known	O
as	O
the	O
independence	B
sampler	I
,	O
which	O
is	O
similar	B
to	O
importance	B
sampling	I
(	O
section	O
23.4	O
)	O
.	O
having	O
proposed	O
a	O
move	O
to	O
x	O
(	O
cid:2	O
)	O
,	O
we	O
then	O
decide	O
whether	O
to	O
accept	B
this	O
proposal	O
or	O
not	O
according	O
to	O
some	O
formula	O
,	O
which	O
ensures	O
that	O
the	O
fraction	O
of	O
time	O
spent	O
in	O
each	O
state	B
is	O
proportional	O
to	O
p∗	O
,	O
otherwise	O
the	O
new	O
state	B
(	O
x	O
)	O
.	O
if	O
the	O
proposal	O
is	O
accepted	O
,	O
the	O
new	O
state	B
is	O
x	O
(	O
cid:2	O
)	O
24.3.	O
metropolis	O
hastings	O
algorithm	O
849	O
is	O
the	O
same	O
as	O
the	O
current	O
state	B
,	O
x	O
(	O
i.e.	O
,	O
we	O
repeat	O
the	O
sample	O
)	O
.	O
if	O
the	O
proposal	O
is	O
symmetric	B
,	O
so	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
=	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
following	O
formula	O
:	O
p∗	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
p∗	O
(	O
x	O
)	O
r	O
=	O
min	O
(	O
1	O
,	O
)	O
)	O
,	O
the	O
acceptance	O
probability	O
is	O
given	O
by	O
the	O
(	O
24.45	O
)	O
is	O
more	O
probable	O
than	O
x	O
,	O
we	O
deﬁnitely	O
move	O
there	O
(	O
since	O
p∗	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
we	O
see	O
that	O
if	O
x	O
(	O
cid:2	O
)	O
p∗	O
(	O
x	O
)	O
>	O
1	O
)	O
,	O
but	O
if	O
x	O
(	O
cid:2	O
)	O
is	O
less	O
probable	O
,	O
we	O
may	O
still	O
move	O
there	O
anyway	O
,	O
depending	O
on	O
the	O
relative	O
probabilities	O
.	O
so	O
instead	O
of	O
greedily	O
moving	O
to	O
only	O
more	O
probable	O
states	O
,	O
we	O
occasionally	O
allow	O
“	O
downhill	O
”	O
moves	O
to	O
less	O
probable	O
states	O
.	O
in	O
section	O
24.3.6	O
,	O
we	O
prove	O
that	O
this	O
procedure	O
ensures	O
that	O
the	O
fraction	O
of	O
time	O
we	O
spend	O
in	O
each	O
state	B
x	O
is	O
proportional	O
to	O
p∗	O
if	O
the	O
proposal	O
is	O
asymmetric	O
,	O
so	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
(	O
cid:8	O
)	O
=	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
(	O
x	O
)	O
.	O
)	O
,	O
we	O
need	O
the	O
hastings	O
correction	O
,	O
given	O
by	O
the	O
following	O
:	O
r	O
=	O
min	O
(	O
1	O
,	O
α	O
)	O
)	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
p∗	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
p∗	O
(	O
x	O
)	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
)	O
/q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
p∗	O
(	O
x	O
(	O
cid:2	O
)	O
p∗	O
(	O
x	O
)	O
/q	O
(	O
x|x	O
(	O
cid:2	O
)	O
)	O
=	O
(	O
24.46	O
)	O
(	O
24.47	O
)	O
α	O
=	O
this	O
correction	O
is	O
needed	O
to	O
compensate	O
for	O
the	O
fact	O
that	O
the	O
proposal	B
distribution	I
itself	O
(	O
rather	O
than	O
just	O
the	O
target	O
distribution	O
)	O
might	O
favor	O
certain	O
states	O
.	O
know	O
the	O
target	O
density	O
up	O
to	O
a	O
normalization	O
constant	O
.	O
in	O
particular	O
,	O
suppose	O
p∗	O
where	O
˜p	O
(	O
x	O
)	O
is	O
an	O
unnormalized	O
distribution	O
and	O
z	O
is	O
the	O
normalization	O
constant	O
.	O
then	O
an	O
important	O
reason	O
why	O
mh	O
is	O
a	O
useful	O
algorithm	O
is	O
that	O
,	O
when	O
evaluating	O
α	O
,	O
we	O
only	O
need	O
to	O
z	O
˜p	O
(	O
x	O
)	O
,	O
(	O
x	O
)	O
=	O
1	O
)	O
/z	O
)	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
(	O
˜p	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
(	O
˜p	O
(	O
x	O
)	O
/z	O
)	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
α	O
=	O
so	O
the	O
z	O
’	O
s	O
cancel	O
.	O
hence	O
we	O
can	O
sample	O
from	O
p∗	O
have	O
to	O
do	O
is	O
evaluate	O
˜p	O
pointwise	O
,	O
where	O
˜p	O
(	O
x	O
)	O
=	O
p∗	O
even	O
if	O
z	O
is	O
unknown	B
.	O
(	O
x	O
)	O
z.	O
the	O
overall	O
algorithm	O
is	O
summarized	O
in	O
algorithm	O
2	O
.	O
(	O
24.48	O
)	O
in	O
particular	O
,	O
all	O
we	O
24.3.2	O
gibbs	O
sampling	O
is	O
a	O
special	O
case	O
of	O
mh	O
it	O
turns	O
out	O
that	O
gibbs	O
sampling	O
,	O
which	O
we	O
discussed	O
in	O
section	O
24.2	O
,	O
is	O
a	O
special	O
case	O
of	O
mh	O
.	O
in	O
particular	O
,	O
it	O
is	O
equivalent	O
to	O
using	O
mh	O
with	O
a	O
sequence	O
of	O
proposals	O
of	O
the	O
form	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
=	O
p	O
(	O
x	O
(	O
cid:2	O
)	O
i|x−i	O
)	O
i	O
(	O
x	O
(	O
cid:2	O
)	O
−i	O
=	O
x−i	O
)	O
(	O
24.49	O
)	O
that	O
is	O
,	O
we	O
move	O
to	O
a	O
new	O
state	B
where	O
xi	O
is	O
sampled	O
from	O
its	O
full	B
conditional	I
,	O
but	O
x−i	O
is	O
left	O
unchanged	O
.	O
we	O
now	O
prove	O
that	O
the	O
acceptance	O
rate	B
of	O
each	O
such	O
proposal	O
is	O
1	O
,	O
so	O
the	O
overall	O
algorithm	O
also	O
has	O
an	O
acceptance	O
rate	B
of	O
100	O
%	O
.	O
we	O
have	O
α	O
=	O
=	O
−i	O
)	O
p	O
(	O
x	O
(	O
cid:2	O
)	O
p	O
(	O
x	O
(	O
cid:2	O
)	O
p	O
(	O
xi|x−i	O
)	O
p	O
(	O
x−i	O
)	O
p	O
(	O
x	O
(	O
cid:2	O
)	O
−i	O
)	O
p	O
(	O
xi|x	O
(	O
cid:2	O
)	O
−i	O
)	O
i|x−i	O
)	O
=	O
i|x	O
(	O
cid:2	O
)	O
)	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
p	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
i|x−i	O
)	O
p	O
(	O
x−i	O
)	O
p	O
(	O
xi|x−i	O
)	O
p	O
(	O
x	O
(	O
cid:2	O
)	O
i|x−i	O
)	O
p	O
(	O
xi|x−i	O
)	O
p	O
(	O
x−i	O
)	O
p	O
(	O
x	O
(	O
cid:2	O
)	O
=	O
1	O
(	O
24.50	O
)	O
(	O
24.51	O
)	O
850	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
algorithm	O
24.2	O
:	O
metropolis	O
hastings	O
algorithm	O
1	O
initialize	O
x0	O
;	O
2	O
for	O
s	O
=	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
do	O
3	O
deﬁne	O
x	O
=	O
xs	O
;	O
sample	O
x	O
(	O
cid:2	O
)	O
∼	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
;	O
compute	O
acceptance	O
probability	O
4	O
5	O
6	O
7	O
)	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
˜p	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
˜p	O
(	O
x	O
)	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
α	O
=	O
compute	O
r	O
=	O
min	O
(	O
1	O
,	O
α	O
)	O
;	O
sample	O
u	O
∼	O
u	O
(	O
0	O
,	O
1	O
)	O
;	O
set	O
new	O
sample	O
to	O
x	O
(	O
cid:2	O
)	O
xs	O
xs+1	O
=	O
(	O
cid:19	O
)	O
if	O
u	O
<	O
r	O
if	O
u	O
≥	O
r	O
where	O
we	O
exploited	O
the	O
fact	O
that	O
x	O
(	O
cid:2	O
)	O
−i	O
=	O
x−i	O
,	O
and	O
that	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
=	O
p	O
(	O
x	O
(	O
cid:2	O
)	O
i|x−i	O
)	O
.	O
the	O
fact	O
that	O
the	O
acceptance	O
rate	B
is	O
100	O
%	O
does	O
not	O
necessarily	O
mean	B
that	O
gibbs	O
will	O
converge	B
rapidly	O
,	O
since	O
it	O
only	O
updates	O
one	O
coordinate	O
at	O
a	O
time	O
(	O
see	O
section	O
24.2.8	O
)	O
.	O
fortunately	O
,	O
there	O
are	O
many	O
other	O
kinds	O
of	O
proposals	O
we	O
can	O
use	O
,	O
as	O
we	O
discuss	O
below	O
.	O
24.3.3	O
proposal	O
distributions	O
for	O
a	O
given	O
target	O
distribution	O
p∗	O
,	O
a	O
proposal	B
distribution	I
q	O
is	O
valid	O
or	O
admissible	B
if	O
it	O
gives	O
a	O
non-zero	O
probability	O
of	O
moving	O
to	O
the	O
states	O
that	O
have	O
non-zero	O
probability	O
in	O
the	O
target	O
.	O
formally	O
,	O
we	O
can	O
write	O
this	O
as	O
)	O
⊆	O
∪xsupp	O
(	O
q	O
(	O
·|x	O
)	O
)	O
supp	O
(	O
p∗	O
(	O
24.52	O
)	O
for	O
example	O
,	O
a	O
gaussian	O
random	B
walk	I
proposal	I
has	O
non-zero	O
probability	O
density	O
on	O
the	O
entire	O
state	B
space	I
,	O
and	O
hence	O
is	O
a	O
valid	O
proposal	O
for	O
any	O
continuous	O
state	B
space	I
.	O
of	O
course	O
,	O
in	O
practice	O
,	O
it	O
is	O
important	O
that	O
the	O
proposal	O
spread	O
its	O
probability	O
mass	O
in	O
just	O
the	O
right	O
way	O
.	O
figure	O
24.7	O
shows	O
an	O
example	O
where	O
we	O
use	O
mh	O
to	O
sample	O
from	O
a	O
mixture	O
of	O
two	O
1d	O
gaussians	O
using	O
a	O
random	B
walk	I
proposal	I
,	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
=n	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
,	O
v	O
)	O
.	O
this	O
is	O
a	O
somewhat	O
tricky	O
target	O
distribution	O
,	O
since	O
it	O
consists	O
of	O
two	O
well	O
separated	O
modes	O
.	O
it	O
is	O
very	O
important	O
to	O
set	O
the	O
variance	B
of	O
the	O
proposal	O
v	O
correctly	O
:	O
if	O
the	O
variance	B
is	O
too	O
low	O
,	O
the	O
chain	O
will	O
only	O
explore	O
one	O
of	O
the	O
modes	O
,	O
as	O
shown	O
in	O
figure	O
24.7	O
(	O
a	O
)	O
,	O
but	O
if	O
the	O
variance	B
is	O
too	O
large	O
,	O
most	O
of	O
the	O
moves	O
will	O
be	O
rejected	O
,	O
and	O
the	O
chain	O
will	O
be	O
very	O
sticky	B
,	O
i.e.	O
,	O
it	O
will	O
stay	O
in	O
the	O
same	O
state	B
for	O
a	O
long	O
time	O
.	O
this	O
is	O
evident	O
from	O
the	O
long	O
stretches	O
of	O
repeated	O
values	O
in	O
figure	O
24.7	O
(	O
b	O
)	O
.	O
if	O
we	O
set	O
the	O
proposal	O
’	O
s	O
variance	B
just	O
right	O
,	O
we	O
get	O
the	O
trace	B
in	O
figure	O
24.7	O
(	O
c	O
)	O
,	O
where	O
the	O
samples	B
clearly	O
explore	O
the	O
support	B
of	O
the	O
target	O
distribution	O
.	O
we	O
discuss	O
how	O
to	O
tune	O
the	O
proposal	O
below	O
.	O
one	O
big	O
advantage	O
of	O
gibbs	O
sampling	O
is	O
that	O
one	O
does	O
not	O
need	O
to	O
choose	O
the	O
proposal	O
24.3.	O
metropolis	O
hastings	O
algorithm	O
851	O
mh	O
with	O
n	O
(	O
0,1.0002	O
)	O
proposal	O
mh	O
with	O
n	O
(	O
0,500.0002	O
)	O
proposal	O
0.2	O
0.1	O
0	O
0	O
200	O
400	O
600	O
800	O
0.06	O
0.04	O
0.02	O
0	O
0	O
200	O
400	O
600	O
800	O
100	O
50	O
0	O
−50	O
100	O
50	O
0	O
−50	O
iterations	O
1000	O
−100	O
samples	B
iterations	O
1000	O
−100	O
samples	B
(	O
a	O
)	O
(	O
b	O
)	O
mh	O
with	O
n	O
(	O
0,8.0002	O
)	O
proposal	O
0.03	O
0.02	O
0.01	O
0	O
0	O
200	O
400	O
600	O
800	O
100	O
50	O
0	O
−50	O
iterations	O
1000	O
−100	O
samples	B
(	O
c	O
)	O
figure	O
24.7	O
an	O
example	O
of	O
the	O
metropolis	O
hastings	O
algorithm	O
for	O
sampling	O
from	O
a	O
mixture	O
of	O
two	O
1d	O
gaussians	O
(	O
μ	O
=	O
(	O
−20	O
,	O
20	O
)	O
,	O
π	O
=	O
(	O
0.3	O
,	O
0.7	O
)	O
,	O
σ	O
=	O
(	O
100	O
,	O
100	O
)	O
)	O
,	O
using	O
a	O
gaussian	O
proposal	O
with	O
variances	O
of	O
v	O
∈	O
{	O
1	O
,	O
500	O
,	O
8	O
}	O
.	O
(	O
a	O
)	O
when	O
v	O
=	O
1	O
,	O
the	O
chain	O
gets	O
trapped	O
near	O
the	O
starting	O
state	B
and	O
fails	O
to	O
sample	O
from	O
the	O
mode	B
at	O
μ	O
=	O
−20	O
.	O
(	O
b	O
)	O
when	O
v	O
=	O
500	O
,	O
the	O
chain	O
is	O
very	O
“	O
sticky	B
”	O
,	O
so	O
its	O
effective	B
sample	I
size	I
is	O
low	O
(	O
as	O
reﬂected	O
by	O
the	O
rough	O
histogram	B
approximation	O
at	O
the	O
end	O
)	O
.	O
(	O
c	O
)	O
using	O
a	O
variance	B
of	O
v	O
=	O
8	O
is	O
just	O
right	O
and	O
leads	O
to	O
a	O
good	O
approximation	O
of	O
the	O
true	O
distribution	O
(	O
shown	O
in	O
red	O
)	O
.	O
figure	O
generated	O
by	O
mcmcgmmdemo	O
.	O
based	O
on	O
code	O
by	O
christophe	O
andrieu	O
and	O
nando	O
de	O
freitas	O
.	O
distribution	O
,	O
and	O
furthermore	O
,	O
the	O
acceptance	O
rate	B
is	O
100	O
%	O
.	O
of	O
course	O
,	O
a	O
100	O
%	O
acceptance	O
can	O
trivially	O
be	O
achieved	O
by	O
using	O
a	O
proposal	O
with	O
variance	B
0	O
(	O
assuming	O
we	O
start	O
at	O
a	O
mode	B
)	O
,	O
but	O
this	O
is	O
obviously	O
not	O
exploring	O
the	O
posterior	O
.	O
so	O
having	O
a	O
high	O
acceptance	O
is	O
not	O
the	O
ultimate	O
goal	O
.	O
we	O
can	O
increase	O
the	O
amount	O
of	O
exploration	O
by	O
increasing	O
the	O
variance	B
of	O
the	O
gaussian	O
kernel	B
.	O
often	O
one	O
experiments	O
with	O
different	O
parameters	O
until	O
the	O
acceptance	O
rate	B
is	O
between	O
25	O
%	O
and	O
40	O
%	O
,	O
which	O
theory	O
suggests	O
is	O
optimal	O
,	O
at	O
least	O
for	O
gaussian	O
target	O
distributions	O
.	O
these	O
short	O
initial	O
runs	O
,	O
used	O
to	O
tune	O
the	O
proposal	O
,	O
are	O
called	O
pilot	B
runs	I
.	O
852	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
0.2	O
0.18	O
0.16	O
0.14	O
0.12	O
1	O
w	O
0.1	O
0.08	O
0.06	O
0.04	O
0.02	O
w0	O
intercept	O
w1	O
slope	O
1500	O
1000	O
500	O
1500	O
1000	O
500	O
0	O
−120	O
−100	O
−80	O
−60	O
w0	O
(	O
a	O
)	O
−40	O
−20	O
0	O
0	O
−120	O
−100	O
−80	O
−60	O
−40	O
−20	O
0	O
0	O
0	O
0.05	O
(	O
b	O
)	O
0.15	O
0.2	O
0.1	O
(	O
c	O
)	O
figure	O
24.8	O
(	O
a	O
)	O
joint	O
posterior	O
of	O
the	O
parameters	O
for	O
1d	O
logistic	B
regression	I
when	O
applied	O
to	O
some	O
sat	O
data	O
.	O
(	O
b	O
)	O
marginal	O
for	O
the	O
offset	O
w0	O
.	O
(	O
c	O
)	O
marginal	O
for	O
the	O
slope	O
w1	O
.	O
we	O
see	O
that	O
the	O
marginals	O
do	O
not	O
capture	O
the	O
fact	O
that	O
the	O
parameters	O
are	O
highly	O
correlated	O
.	O
figure	O
generated	O
by	O
logregsatmhdemo	O
.	O
24.3.3.1	O
gaussian	O
proposals	O
if	O
we	O
have	O
a	O
continuous	O
state	B
space	I
,	O
the	O
hessian	O
h	O
at	O
a	O
local	O
mode	O
ˆw	O
can	O
be	O
used	O
to	O
deﬁne	O
the	O
covariance	B
of	O
a	O
gaussian	O
proposal	B
distribution	I
.	O
this	O
approach	O
has	O
the	O
advantage	O
that	O
the	O
hessian	O
models	O
the	O
local	O
curvature	O
and	O
length	O
scales	O
of	O
each	O
dimension	O
;	O
this	O
approach	O
therefore	O
avoids	O
some	O
of	O
the	O
slow	O
mixing	O
behavior	O
of	O
gibbs	O
sampling	O
shown	O
in	O
figure	O
24.6.	O
there	O
are	O
two	O
obvious	O
approaches	O
:	O
(	O
1	O
)	O
an	O
independence	O
proposal	O
,	O
q	O
(	O
w	O
(	O
cid:2	O
)	O
|w	O
)	O
=	O
n	O
(	O
w	O
(	O
cid:2	O
)	O
|	O
ˆw	O
,	O
h−1	O
)	O
or	O
(	O
2	O
)	O
,	O
a	O
random	B
walk	I
proposal	I
,	O
q	O
(	O
w	O
(	O
cid:2	O
)	O
|w	O
)	O
=	O
n	O
(	O
w	O
(	O
cid:2	O
)	O
|w	O
,	O
s2h−1	O
)	O
,	O
where	O
s2	O
is	O
a	O
scale	O
factor	O
chosen	O
to	O
facilitate	O
rapid	O
mixing	O
.	O
(	O
roberts	O
and	O
rosenthal	O
2001	O
)	O
prove	O
that	O
,	O
if	O
the	O
posterior	O
is	O
gaussian	O
,	O
the	O
asymptotically	B
optimal	I
value	O
is	O
to	O
use	O
s2	O
=	O
2.382/d	O
,	O
where	O
d	O
is	O
the	O
dimensionality	O
of	O
w	O
;	O
this	O
results	O
in	O
an	O
acceptance	O
rate	B
of	O
0.234.	O
for	O
example	O
,	O
consider	O
mh	O
for	O
binary	O
logistic	O
regression	B
.	O
from	O
equation	O
8.7	O
,	O
we	O
have	O
that	O
the	O
hessian	O
of	O
the	O
log-likelihood	O
is	O
hl	O
=	O
xt	O
dx	O
,	O
where	O
d	O
=	O
diag	O
(	O
μi	O
(	O
1	O
−	O
μi	O
)	O
)	O
and	O
μi	O
=	O
sigm	O
(	O
ˆwt	O
xi	O
)	O
.	O
if	O
we	O
assume	O
a	O
gaussian	O
prior	O
,	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
0	O
,	O
v0	O
)	O
,	O
we	O
have	O
h	O
=	O
v−1	O
0	O
+	O
hl	O
,	O
so	O
the	O
asymptotically	B
optimal	I
gaussian	O
proposal	O
has	O
the	O
form	O
(	O
cid:8	O
)	O
(	O
cid:22	O
)	O
(	O
cid:9	O
)	O
(	O
cid:23	O
)	O
−1	O
q	O
(	O
w	O
(	O
cid:2	O
)	O
|w	O
)	O
=	O
n	O
w	O
,	O
2.382	O
d	O
v−1	O
0	O
+	O
xt	O
dx	O
see	O
(	O
gamerman	O
1997	O
;	O
rossi	O
et	O
al	O
.	O
2006	O
;	O
fruhwirth-schnatter	O
and	O
fruhwirth	O
2010	O
)	O
for	O
further	O
details	O
.	O
the	O
approach	O
is	O
illustrated	O
in	O
figure	O
24.8	O
,	O
where	O
we	O
sample	O
parameters	O
from	O
a	O
1d	O
logistic	B
regression	I
model	O
ﬁt	O
to	O
some	O
sat	O
data	O
.	O
we	O
initialize	O
the	O
chain	O
at	O
the	O
mode	B
,	O
computed	O
using	O
irls	O
,	O
and	O
then	O
use	O
the	O
above	O
random	O
walk	O
metropolis	O
sampler	O
.	O
if	O
you	O
can	O
not	O
afford	O
to	O
compute	O
the	O
mode	B
or	O
its	O
hessian	O
xdx	O
,	O
an	O
alternative	O
approach	O
,	O
suggested	O
in	O
(	O
scott	O
2009	O
)	O
,	O
is	O
to	O
approximate	O
the	O
above	O
proposal	O
as	O
follows	O
:	O
(	O
cid:10	O
)	O
(	O
cid:8	O
)	O
(	O
cid:11	O
)	O
(	O
cid:9	O
)	O
−1	O
q	O
(	O
w	O
(	O
cid:2	O
)	O
|w	O
)	O
=	O
n	O
w	O
,	O
v−1	O
0	O
+	O
6	O
π2	O
xt	O
x	O
(	O
24.53	O
)	O
(	O
24.54	O
)	O
24.3.	O
metropolis	O
hastings	O
algorithm	O
853	O
24.3.3.2	O
mixture	B
proposals	O
if	O
one	O
doesn	O
’	O
t	O
know	O
what	O
kind	O
of	O
proposal	O
to	O
use	O
,	O
one	O
can	O
try	O
a	O
mixture	B
proposal	I
,	O
which	O
is	O
a	O
convex	B
combination	I
of	O
base	O
proposals	O
:	O
k	O
(	O
cid:4	O
)	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
=	O
wkqk	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
(	O
24.55	O
)	O
k=1	O
where	O
wk	O
are	O
the	O
mixing	B
weights	I
.	O
as	O
long	O
as	O
each	O
qk	O
is	O
individually	O
valid	O
,	O
the	O
overall	O
proposal	O
will	O
also	O
be	O
valid	O
.	O
24.3.3.3	O
data-driven	O
mcmc	O
the	O
most	O
efficient	O
proposals	O
depend	O
not	O
just	O
on	O
the	O
previous	O
hidden	B
state	O
,	O
but	O
also	O
the	O
visible	B
data	O
,	O
i.e.	O
,	O
they	O
have	O
the	O
form	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
,	O
d	O
)	O
.	O
this	O
is	O
called	O
data-driven	O
mcmc	O
(	O
see	O
e.g.	O
,	O
(	O
tu	O
and	O
zhu	O
2002	O
)	O
)	O
.	O
to	O
create	O
such	O
proposals	O
,	O
one	O
can	O
sample	O
(	O
x	O
,	O
d	O
)	O
pairs	O
from	O
the	O
forwards	B
model	I
and	O
then	O
train	O
a	O
discriminative	B
classiﬁer	I
to	O
predict	O
p	O
(	O
x|f	O
(	O
d	O
)	O
)	O
,	O
where	O
f	O
(	O
d	O
)	O
are	O
some	O
features	B
extracted	O
from	O
the	O
visible	B
data	O
.	O
typically	O
x	O
is	O
a	O
high-dimensional	O
vector	O
(	O
e.g.	O
,	O
position	O
and	O
orientation	O
of	O
all	O
the	O
limbs	O
of	O
a	O
person	O
in	O
a	O
visual	O
object	O
detector	O
)	O
,	O
so	O
it	O
is	O
hard	O
to	O
predict	O
the	O
entire	O
state	B
vector	O
,	O
p	O
(	O
x|f	O
(	O
d	O
)	O
)	O
.	O
instead	O
we	O
might	O
train	O
a	O
discriminative	B
detector	O
to	O
predict	O
parts	O
of	O
the	O
state-space	O
,	O
p	O
(	O
xk|fk	O
(	O
d	O
)	O
)	O
,	O
such	O
as	O
the	O
location	O
of	O
just	O
the	O
face	O
of	O
a	O
person	O
.	O
we	O
can	O
then	O
use	O
a	O
proposal	O
of	O
the	O
form	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
,	O
d	O
)	O
=	O
π0q0	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
+	O
πkqk	O
(	O
x	O
(	O
cid:2	O
)	O
k|fk	O
(	O
d	O
)	O
)	O
(	O
24.56	O
)	O
(	O
cid:4	O
)	O
k	O
where	O
q0	O
is	O
a	O
standard	O
data-independent	O
proposal	O
(	O
e.g.	O
,	O
random	O
walk	O
)	O
,	O
and	O
qk	O
updates	O
the	O
k	O
’	O
th	O
component	O
of	O
the	O
state	B
space	I
.	O
for	O
added	O
efficiency	O
,	O
the	O
discriminative	B
proposals	O
should	O
suggest	O
joint	O
changes	O
to	O
multiple	O
variables	O
,	O
but	O
this	O
is	O
often	O
hard	O
to	O
do	O
.	O
the	O
overall	O
procedure	O
is	O
a	O
form	O
of	O
generate	B
and	I
test	I
:	O
the	O
discriminative	B
proposals	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
generate	O
new	O
hypotheses	O
,	O
which	O
are	O
then	O
“	O
tested	O
”	O
by	O
computing	O
the	O
posterior	O
ratio	O
p	O
(	O
x	O
(	O
cid:2	O
)	O
|d	O
)	O
p	O
(	O
x|d	O
)	O
,	O
to	O
see	O
if	O
the	O
new	O
hypothesis	O
is	O
better	O
or	O
worse	O
.	O
by	O
adding	O
an	O
annealing	B
step	O
,	O
one	O
can	O
modify	O
the	O
algorithm	O
to	O
ﬁnd	O
posterior	O
modes	O
;	O
this	O
is	O
called	O
simulated	B
annealing	I
,	O
and	O
is	O
described	O
in	O
section	O
24.6.1.	O
one	O
advantage	O
of	O
using	O
the	O
mode-seeking	O
version	O
of	O
the	O
algorithm	O
is	O
that	O
we	O
do	O
not	O
need	O
to	O
ensure	O
the	O
proposal	B
distribution	I
is	O
reversible	O
.	O
24.3.4	O
adaptive	O
mcmc	O
one	O
can	O
change	O
the	O
parameters	O
of	O
the	O
proposal	O
as	O
the	O
algorithm	O
is	O
running	O
to	O
increase	O
efficiency	O
.	O
this	O
is	O
called	O
adaptive	O
mcmc	O
.	O
this	O
allows	O
one	O
to	O
start	O
with	O
a	O
broad	O
covariance	B
(	O
say	O
)	O
,	O
allowing	O
large	O
moves	O
through	O
the	O
space	O
until	O
a	O
mode	B
is	O
found	O
,	O
followed	O
by	O
a	O
narrowing	O
of	O
the	O
covariance	B
to	O
ensure	O
careful	O
exploration	O
of	O
the	O
region	O
around	O
the	O
mode	B
.	O
however	O
,	O
one	O
must	O
be	O
careful	O
not	O
to	O
violate	O
the	O
markov	O
property	O
;	O
thus	O
the	O
parameters	O
of	O
the	O
proposal	O
should	O
not	O
depend	O
on	O
the	O
entire	O
history	O
of	O
the	O
chain	O
.	O
it	O
turns	O
out	O
that	O
a	O
sufficient	O
condition	O
to	O
ensure	O
this	O
is	O
that	O
the	O
adaption	O
is	O
“	O
faded	O
out	O
”	O
gradually	O
over	O
time	O
.	O
see	O
e.g.	O
,	O
(	O
andrieu	O
and	O
thoms	O
2008	O
)	O
for	O
details	O
.	O
854	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
24.3.5	O
initialization	O
and	O
mode	B
hopping	O
it	O
is	O
necessary	O
to	O
start	O
mcmc	O
in	O
an	O
initial	O
state	B
that	O
has	O
non-zero	O
probability	O
.	O
if	O
the	O
model	O
has	O
deterministic	O
constraints	O
,	O
ﬁnding	O
such	O
a	O
legal	O
conﬁguration	O
may	O
be	O
a	O
hard	O
problem	O
in	O
itself	O
.	O
it	O
is	O
therefore	O
common	O
to	O
initialize	O
mcmc	O
methods	O
at	O
a	O
local	O
mode	O
,	O
found	O
using	O
an	O
optimizer	O
.	O
in	O
some	O
domains	O
(	O
especially	O
with	O
discrete	B
state	O
spaces	O
)	O
,	O
it	O
is	O
a	O
more	O
effective	O
use	O
of	O
computa-	O
tion	O
time	O
to	O
perform	O
multiple	B
restarts	I
of	O
an	O
optimizer	O
,	O
and	O
to	O
average	O
over	O
these	O
modes	O
,	O
rather	O
than	O
exploring	O
similar	B
points	O
around	O
a	O
local	O
mode	O
.	O
however	O
,	O
in	O
continuous	O
state	B
spaces	O
,	O
the	O
mode	B
contains	O
negligible	O
volume	O
(	O
section	O
5.2.1.3	O
)	O
,	O
so	O
it	O
is	O
necessary	O
to	O
locally	O
explore	O
around	O
each	O
mode	B
,	O
in	O
order	O
to	O
visit	O
enough	O
posterior	O
probability	O
mass	O
.	O
24.3.6	O
why	O
mh	O
works	O
*	O
to	O
prove	O
that	O
the	O
mh	O
procedure	O
generates	O
samples	B
from	O
p∗	O
chain	O
theory	O
,	O
so	O
be	O
sure	O
to	O
read	O
section	O
17.2.3	O
ﬁrst	O
.	O
,	O
we	O
have	O
to	O
use	O
a	O
bit	O
of	O
markov	O
(	O
cid:19	O
)	O
the	O
mh	O
algorithm	O
deﬁnes	O
a	O
markov	O
chain	O
with	O
the	O
following	O
transition	B
matrix	I
:	O
p	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
=	O
(	O
cid:7	O
)	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
r	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
q	O
(	O
x|x	O
)	O
+	O
x	O
(	O
cid:2	O
)	O
(	O
cid:5	O
)	O
=x	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
(	O
1	O
−	O
r	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
)	O
if	O
x	O
(	O
cid:2	O
)	O
(	O
cid:8	O
)	O
=	O
x	O
otherwise	O
(	O
24.57	O
)	O
this	O
follows	O
from	O
a	O
case	B
analysis	I
:	O
if	O
you	O
move	O
to	O
x	O
(	O
cid:2	O
)	O
from	O
x	O
,	O
you	O
must	O
have	O
proposed	O
it	O
(	O
with	O
probability	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
)	O
and	O
it	O
must	O
have	O
been	O
accepted	O
(	O
with	O
probability	O
r	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
)	O
;	O
otherwise	O
you	O
stay	O
in	O
state	B
x	O
,	O
either	O
because	O
that	O
is	O
what	O
you	O
proposed	O
(	O
with	O
probability	O
q	O
(	O
x|x	O
)	O
)	O
,	O
or	O
because	O
you	O
proposed	O
something	O
else	O
(	O
with	O
probability	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
)	O
but	O
it	O
was	O
rejected	O
(	O
with	O
probability	O
1	O
−	O
r	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
)	O
.	O
let	O
us	O
analyse	O
this	O
markov	O
chain	O
.	O
recall	B
from	O
section	O
17.2.3.4	O
that	O
a	O
chain	O
satisﬁes	O
detailed	B
balance	I
if	O
p	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
p∗	O
(	O
x	O
)	O
=	O
p	O
(	O
x|x	O
(	O
cid:2	O
)	O
)	O
p∗	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
(	O
24.58	O
)	O
we	O
also	O
showed	O
that	O
if	O
a	O
chain	O
satisﬁes	O
detailed	B
balance	I
,	O
then	O
p∗	O
is	O
its	O
stationary	B
distribution	I
.	O
our	O
goal	O
is	O
to	O
show	O
that	O
the	O
mh	O
algorithm	O
deﬁnes	O
a	O
transition	O
function	O
that	O
satisﬁes	O
detailed	B
balance	I
and	O
hence	O
that	O
p∗	O
is	O
its	O
stationary	B
distribution	I
.	O
(	O
if	O
equation	O
24.58	O
holds	O
,	O
we	O
say	O
that	O
p∗	O
is	O
an	O
invariant	B
distribution	I
wrt	O
the	O
markov	O
transition	O
kernel	O
q	O
.	O
)	O
theorem	O
24.3.1.	O
if	O
the	O
transition	B
matrix	I
deﬁned	O
by	O
the	O
mh	O
algorithm	O
(	O
given	O
by	O
equation	O
24.57	O
)	O
is	O
ergodic	B
and	O
irreducible	B
,	O
then	O
p∗	O
proof	O
.	O
consider	O
two	O
states	O
x	O
and	O
x	O
(	O
cid:2	O
)	O
)	O
is	O
its	O
unique	O
limiting	B
distribution	I
.	O
(	O
x	O
)	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
<	O
p∗	O
)	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
.	O
either	O
(	O
x	O
(	O
cid:2	O
)	O
(	O
24.59	O
)	O
p∗	O
or	O
p∗	O
(	O
x	O
)	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
>	O
p∗	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
)	O
(	O
24.60	O
)	O
we	O
will	O
ignore	O
ties	O
(	O
which	O
occur	O
with	O
probability	O
zero	O
for	O
continuous	O
distributions	O
)	O
.	O
without	O
loss	B
(	O
x	O
)	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
>	O
p∗	O
of	O
generality	O
,	O
assume	O
that	O
p∗	O
)	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
(	O
x	O
(	O
cid:2	O
)	O
p∗	O
)	O
p∗	O
(	O
x	O
)	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
α	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
=	O
)	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
)	O
.	O
hence	O
(	O
24.61	O
)	O
(	O
x	O
(	O
cid:2	O
)	O
<	O
1	O
24.3.	O
metropolis	O
hastings	O
algorithm	O
hence	O
we	O
have	O
r	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
=	O
α	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
and	O
r	O
(	O
x|x	O
(	O
cid:2	O
)	O
now	O
to	O
move	O
fromx	O
to	O
x	O
(	O
cid:2	O
)	O
p	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
=	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
r	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
=	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
)	O
=	O
1.	O
we	O
must	O
ﬁrst	O
propose	B
x	O
(	O
cid:2	O
)	O
)	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
p∗	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
p∗	O
(	O
x	O
)	O
q	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
hence	O
p∗	O
(	O
x	O
)	O
p	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
=	O
p∗	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
)	O
and	O
then	O
accept	B
it	O
.	O
hence	O
p∗	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
p∗	O
(	O
x	O
)	O
=	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
)	O
the	O
backwards	O
probability	O
is	O
)	O
r	O
(	O
x|x	O
(	O
cid:2	O
)	O
)	O
=	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
p	O
(	O
x|x	O
(	O
cid:2	O
)	O
)	O
=	O
q	O
(	O
x|x	O
(	O
cid:2	O
)	O
)	O
since	O
r	O
(	O
x|x	O
(	O
cid:2	O
)	O
855	O
(	O
24.62	O
)	O
(	O
24.63	O
)	O
(	O
24.64	O
)	O
)	O
=	O
1.	O
inserting	O
this	O
into	O
equation	O
24.63	O
we	O
get	O
p∗	O
(	O
x	O
(	O
cid:2	O
)	O
(	O
x	O
)	O
p	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
)	O
=	O
p∗	O
)	O
p	O
(	O
x|x	O
(	O
cid:2	O
)	O
so	O
detailed	B
balance	I
holds	O
wrt	O
p∗	O
is	O
a	O
stationary	B
distribution	I
.	O
furthermore	O
,	O
from	O
theorem	O
17.2.2	O
,	O
this	O
distribution	O
is	O
unique	O
,	O
since	O
the	O
chain	O
is	O
ergodic	B
and	O
irreducible.	O
)	O
.	O
hence	O
,	O
from	O
theorem	O
17.2.3	O
,	O
p∗	O
(	O
24.65	O
)	O
24.3.7	O
reversible	O
jump	O
(	O
trans-dimensional	O
)	O
mcmc	O
*	O
suppose	O
we	O
have	O
a	O
set	O
of	O
models	O
with	O
different	O
numbers	O
of	O
parameters	O
,	O
e.g.	O
,	O
mixture	B
models	O
in	O
which	O
the	O
number	O
of	O
mixture	B
components	O
is	O
unknown	B
.	O
let	O
the	O
model	O
be	O
denoted	O
by	O
m	O
,	O
and	O
let	O
its	O
unknowns	O
(	O
e.g.	O
,	O
parameters	O
)	O
be	O
denoted	O
by	O
xm	O
∈	O
xm	O
(	O
e.g.	O
,	O
xm	O
=	O
r	O
nm	O
,	O
where	O
nm	O
is	O
(	O
cid:26	O
)	O
m	O
the	O
dimensionality	O
of	O
model	O
m	O
)	O
.	O
sampling	O
in	O
spaces	O
of	O
differing	O
dimensionality	O
is	O
called	O
trans-	O
dimensional	O
mcmc	O
(	O
green	O
2003	O
)	O
.	O
we	O
could	O
sample	O
the	O
model	O
indicator	O
m	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
}	O
and	O
m=1	O
xm	O
,	O
but	O
this	O
is	O
very	O
inefficient	O
.	O
it	O
is	O
sample	O
all	O
the	O
parameters	O
from	O
the	O
product	O
space	O
m=1	O
{	O
m	O
}	O
×	O
xm	O
,	O
where	O
we	O
only	O
worry	O
more	O
parsimonious	O
to	O
sample	O
in	O
the	O
union	O
space	O
x	O
=	O
∪m	O
about	O
parameters	O
for	O
the	O
currently	O
active	O
model	O
.	O
the	O
difficulty	O
with	O
this	O
approach	O
arises	O
when	O
we	O
move	O
between	O
models	O
of	O
different	O
dimen-	O
sionality	O
.	O
the	O
trouble	O
is	O
that	O
when	O
we	O
compute	O
the	O
mh	O
acceptance	O
ratio	O
,	O
we	O
are	O
comparing	O
densities	O
deﬁned	O
in	O
different	O
dimensionality	O
spaces	O
,	O
which	O
is	O
meaningless	O
.	O
it	O
is	O
like	O
trying	O
to	O
compare	O
a	O
sphere	O
with	O
a	O
circle	O
.	O
the	O
solution	O
,	O
proposed	O
by	O
(	O
green	O
1998	O
)	O
and	O
known	O
as	O
reversible	O
jump	O
mcmc	O
or	O
rjmcmc	O
,	O
is	O
to	O
augment	O
the	O
low	O
dimensional	O
space	O
with	O
extra	O
random	O
variables	O
so	O
that	O
the	O
two	O
spaces	O
have	O
a	O
common	O
measure	O
.	O
unfortunately	O
,	O
we	O
do	O
not	O
have	O
space	O
to	O
go	O
into	O
details	O
here	O
.	O
suffice	O
it	O
to	O
say	O
that	O
the	O
method	O
can	O
be	O
made	O
to	O
work	O
in	O
theory	O
,	O
although	O
it	O
is	O
a	O
bit	O
tricky	O
in	O
practice	O
.	O
if	O
,	O
however	O
,	O
the	O
continuous	O
parameters	O
can	O
be	O
integrated	O
out	O
(	O
resulting	O
in	O
a	O
method	O
called	O
collapsed	O
rjmcmc	O
)	O
,	O
much	O
of	O
the	O
difficulty	O
goes	O
away	O
,	O
since	O
we	O
are	O
just	O
left	O
with	O
a	O
discrete	B
state	O
space	O
,	O
where	O
there	O
is	O
no	O
need	O
to	O
worry	O
about	O
change	O
of	O
measure	O
.	O
for	O
example	O
,	O
(	O
denison	O
et	O
al	O
.	O
2002	O
)	O
includes	O
many	O
examples	O
of	O
applications	O
of	O
collapsed	O
rjmcmc	O
applied	O
to	O
bayesian	O
inference	B
fro	O
adaptive	O
basis-function	O
models	O
.	O
they	O
sample	O
basis	O
functions	O
from	O
a	O
ﬁxed	O
set	O
of	O
candidates	O
(	O
e.g.	O
,	O
centered	O
on	O
the	O
data	O
points	O
)	O
,	O
and	O
integrate	B
out	I
the	O
other	O
parameters	O
analytically	O
.	O
this	O
provides	O
a	O
bayesian	O
alternative	O
to	O
using	O
rvms	O
or	O
svms	O
.	O
856	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
initial	O
condition	O
x	O
=	O
10	O
0	O
initial	O
condition	O
x	O
=	O
17	O
0	O
p	O
(	O
0	O
)	O
(	O
x	O
)	O
p	O
(	O
1	O
)	O
(	O
x	O
)	O
p	O
(	O
2	O
)	O
(	O
x	O
)	O
p	O
(	O
3	O
)	O
(	O
x	O
)	O
p	O
(	O
10	O
)	O
(	O
x	O
)	O
p	O
(	O
100	O
)	O
(	O
x	O
)	O
p	O
(	O
200	O
)	O
(	O
x	O
)	O
p	O
(	O
400	O
)	O
(	O
x	O
)	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
5	O
5	O
5	O
5	O
5	O
5	O
5	O
5	O
10	O
10	O
10	O
10	O
10	O
10	O
10	O
10	O
15	O
15	O
15	O
15	O
15	O
15	O
15	O
15	O
20	O
20	O
20	O
20	O
20	O
20	O
20	O
20	O
p	O
(	O
0	O
)	O
(	O
x	O
)	O
p	O
(	O
1	O
)	O
(	O
x	O
)	O
p	O
(	O
2	O
)	O
(	O
x	O
)	O
p	O
(	O
3	O
)	O
(	O
x	O
)	O
p	O
(	O
10	O
)	O
(	O
x	O
)	O
p	O
(	O
100	O
)	O
(	O
x	O
)	O
p	O
(	O
200	O
)	O
(	O
x	O
)	O
p	O
(	O
400	O
)	O
(	O
x	O
)	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
5	O
5	O
5	O
5	O
5	O
5	O
5	O
5	O
10	O
10	O
10	O
10	O
10	O
10	O
10	O
10	O
15	O
15	O
15	O
15	O
15	O
15	O
15	O
15	O
20	O
20	O
20	O
20	O
20	O
20	O
20	O
20	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
24.9	O
illustration	O
of	O
convergence	O
to	O
the	O
uniform	B
distribution	I
over	O
{	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,20	O
}	O
using	O
a	O
symmetric	B
random	O
walk	O
starting	O
from	O
(	O
left	O
)	O
state	B
10	O
,	O
and	O
(	O
right	O
)	O
state	B
17.	O
based	O
on	O
figures	O
29.14	O
and	O
29.15	O
of	O
(	O
mackay	O
2003	O
)	O
.	O
figure	O
generated	O
by	O
randomwalk0to20demo	O
.	O
24.4	O
speed	O
and	O
accuracy	O
of	O
mcmc	O
in	O
this	O
section	O
,	O
we	O
discuss	O
a	O
number	O
of	O
important	O
theoretical	O
and	O
practical	O
issues	O
to	O
do	O
with	O
mcmc	O
.	O
24.4.1	O
the	O
burn-in	B
phase	I
we	O
start	O
mcmc	O
from	O
an	O
arbitrary	O
initial	O
state	B
.	O
as	O
we	O
explained	O
in	O
section	O
17.2.3	O
,	O
only	O
when	O
the	O
chain	O
has	O
“	O
forgotten	O
”	O
where	O
it	O
started	O
from	O
will	O
the	O
samples	B
be	O
coming	O
from	O
the	O
chain	O
’	O
s	O
stationary	B
distribution	I
.	O
samples	B
collected	O
before	O
the	O
chain	O
has	O
reached	O
its	O
stationary	B
distribution	I
do	O
not	O
come	O
from	O
p∗	O
,	O
and	O
are	O
usually	O
thrown	O
away	O
.	O
the	O
initial	O
period	B
,	O
whose	O
samples	B
will	O
be	O
ignored	O
,	O
is	O
called	O
the	O
burn-in	B
phase	I
.	O
for	O
example	O
,	O
consider	O
a	O
uniform	B
distribution	I
on	O
the	O
integers	O
{	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
20	O
}	O
.	O
suppose	O
we	O
sample	O
from	O
this	O
using	O
a	O
symmetric	B
random	O
walk	O
.	O
in	O
figure	O
24.9	O
,	O
we	O
show	O
two	O
runs	O
of	O
the	O
algorithm	O
.	O
on	O
the	O
left	O
,	O
we	O
start	O
in	O
state	B
10	O
;	O
on	O
the	O
right	O
,	O
we	O
start	O
in	O
state	B
17.	O
even	O
in	O
this	O
small	O
problem	O
it	O
takes	O
over	O
100	O
steps	O
until	O
the	O
chain	O
has	O
“	O
forgotten	O
”	O
where	O
it	O
started	O
from	O
.	O
it	O
is	O
difficult	O
to	O
diagnose	O
when	O
the	O
chain	O
has	O
burned	B
in	I
,	O
an	O
issue	O
we	O
discuss	O
in	O
more	O
detail	O
below	O
.	O
(	O
this	O
is	O
one	O
of	O
the	O
fundamental	O
weaknesses	O
of	O
mcmc	O
.	O
)	O
as	O
an	O
interesting	O
example	O
of	O
what	O
can	O
happen	O
if	O
you	O
start	O
collecting	O
samples	B
too	O
early	O
,	O
consider	O
the	O
potts	O
model	O
.	O
figure	O
24.10	O
(	O
a	O
)	O
,	O
shows	O
a	O
sample	O
after	O
500	O
iterations	O
of	O
gibbs	O
sampling	O
.	O
this	O
suggests	O
that	O
the	O
model	O
likes	O
24.4.	O
speed	O
and	O
accuracy	O
of	O
mcmc	O
857	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
24.10	O
illustration	O
of	O
problems	O
caused	O
by	O
poor	O
mixing	O
.	O
(	O
a	O
)	O
one	O
sample	O
from	O
a	O
5-state	O
potts	O
model	O
on	O
a	O
128	O
×	O
128	O
grid	O
with	O
8	O
nearest	B
neighbor	I
connectivity	O
and	O
j	O
=	O
2/3	O
(	O
as	O
in	O
(	O
geman	O
and	O
geman	O
1984	O
)	O
)	O
,	O
after	O
200	O
iterations	O
.	O
(	O
b	O
)	O
one	O
sample	O
from	O
the	O
same	O
model	O
after	O
10,000	O
iterations	O
.	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
.	O
medium-sized	O
regions	O
where	O
the	O
label	B
is	O
the	O
same	O
,	O
implying	O
the	O
model	O
would	O
make	O
a	O
good	O
prior	O
for	O
image	B
segmentation	I
.	O
indeed	O
,	O
this	O
was	O
suggested	O
in	O
the	O
original	O
gibbs	O
sampling	O
paper	O
(	O
geman	O
and	O
geman	O
1984	O
)	O
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
if	O
you	O
run	O
the	O
chain	O
long	O
enough	O
,	O
you	O
get	O
isolated	O
speckles	O
,	O
as	O
in	O
figure	O
24.10	O
(	O
b	O
)	O
.	O
the	O
results	O
depend	O
on	O
the	O
coupling	O
strength	O
,	O
but	O
in	O
general	O
,	O
it	O
is	O
very	O
hard	O
to	O
ﬁnd	O
a	O
setting	O
which	O
produces	O
nice	O
medium-sized	O
blobs	O
:	O
most	O
parameters	O
result	O
in	O
a	O
few	O
super-clusters	O
,	O
or	O
lots	O
of	O
small	O
fragments	O
.	O
in	O
fact	O
,	O
there	O
is	O
a	O
rapid	O
phase	B
transition	I
between	O
these	O
two	O
regimes	O
.	O
this	O
led	O
to	O
a	O
paper	O
called	O
“	O
the	O
ising/potts	O
model	O
is	O
not	O
well	O
suited	O
to	O
segmentation	O
tasks	O
”	O
(	O
morris	O
et	O
al	O
.	O
1996	O
)	O
.	O
it	O
is	O
possible	O
to	O
create	O
priors	O
more	O
suited	O
to	O
image	B
segmentation	I
(	O
e.g.	O
,	O
(	O
sudderth	O
and	O
jordan	O
2008	O
)	O
)	O
,	O
but	O
the	O
main	O
point	O
here	O
is	O
that	O
sampling	O
before	O
reaching	O
convergence	O
can	O
lead	O
to	O
erroneous	O
conclusions	O
.	O
24.4.2	O
mixing	O
rates	O
of	O
markov	O
chains	O
*	O
the	O
amount	O
of	O
time	O
it	O
takes	O
for	O
a	O
markov	O
chain	O
to	O
converge	B
to	O
the	O
stationary	B
distribution	I
,	O
and	O
forget	O
its	O
initial	O
state	B
,	O
is	O
called	O
the	O
mixing	B
time	I
.	O
more	O
formally	O
,	O
we	O
say	O
that	O
the	O
mixing	B
time	I
from	O
state	B
x0	O
is	O
the	O
minimal	B
time	O
such	O
that	O
,	O
for	O
any	O
constant	O
	O
>	O
0	O
,	O
we	O
have	O
that	O
τ	O
(	O
x0	O
)	O
(	O
cid:2	O
)	O
min	O
{	O
t	O
:	O
||δx0	O
(	O
x	O
)	O
t	O
t	O
−	O
p∗||1	O
≤	O
	O
}	O
(	O
24.66	O
)	O
where	O
δx0	O
(	O
x	O
)	O
is	O
a	O
distribution	O
with	O
all	O
its	O
mass	O
in	O
state	B
x0	O
,	O
t	O
is	O
the	O
transition	B
matrix	I
of	O
the	O
chain	O
(	O
which	O
depends	O
on	O
the	O
target	O
p∗	O
and	O
the	O
proposal	O
q	O
)	O
,	O
and	O
δx0	O
(	O
x	O
)	O
t	O
t	O
is	O
the	O
distribution	O
after	O
t	O
steps	O
.	O
the	O
mixing	B
time	I
of	O
the	O
chain	O
is	O
deﬁned	O
as	O
τ	O
(	O
x0	O
)	O
τ	O
(	O
cid:2	O
)	O
max	O
(	O
24.67	O
)	O
the	O
mixing	B
time	I
is	O
determined	O
by	O
the	O
eigengap	B
γ	O
=	O
λ1	O
−	O
λ2	O
,	O
which	O
is	O
the	O
difference	O
of	O
the	O
x0	O
858	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
x1	O
x2	O
x3	O
x5	O
x4	O
x6	O
x7	O
figure	O
24.11	O
a	O
markov	O
chain	O
with	O
low	O
conductance	O
.	O
the	O
dotted	O
arcs	O
represent	O
transitions	O
with	O
very	O
low	O
probability	O
.	O
source	O
:	O
figure	O
12.6	O
of	O
(	O
koller	O
and	O
friedman	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
daphne	O
koller	O
.	O
ﬁrst	O
and	O
second	O
eigenvalues	O
of	O
the	O
transition	B
matrix	I
.	O
in	O
particular	O
,	O
one	O
can	O
show	O
that	O
τ	O
≤	O
o	O
(	O
1	O
γ	O
log	O
n	O
	O
)	O
(	O
24.68	O
)	O
where	O
n	O
is	O
the	O
number	O
of	O
states	O
.	O
since	O
computing	O
the	O
transition	B
matrix	I
can	O
be	O
hard	O
to	O
do	O
,	O
especially	O
for	O
high	O
dimensional	O
and/or	O
continuous	O
state	B
spaces	O
,	O
it	O
is	O
useful	O
to	O
ﬁnd	O
other	O
ways	O
to	O
estimate	O
the	O
mixing	B
time	I
.	O
an	O
alternative	O
approach	O
is	O
to	O
examine	O
the	O
geometry	O
of	O
the	O
state	B
space	I
.	O
for	O
example	O
,	O
consider	O
the	O
chain	O
in	O
figure	O
24.11.	O
we	O
see	O
that	O
the	O
state	B
space	I
consists	O
of	O
two	O
“	O
islands	O
”	O
,	O
each	O
of	O
which	O
is	O
connected	O
via	O
a	O
narrow	O
“	O
bottleneck	B
”	O
.	O
(	O
if	O
they	O
were	O
completely	O
disconnected	O
,	O
the	O
chain	O
would	O
not	O
be	O
ergodic	B
,	O
and	O
there	O
would	O
no	O
longer	O
be	O
a	O
unique	O
stationary	B
distribution	I
.	O
)	O
we	O
deﬁne	O
the	O
conductance	B
φ	O
of	O
a	O
chain	O
as	O
the	O
minimum	O
probability	O
,	O
over	O
all	O
subsets	O
of	O
states	O
,	O
of	O
transitioning	O
from	O
that	O
set	O
to	O
its	O
complement	O
:	O
(	O
cid:7	O
)	O
x∈s	O
,	O
x	O
(	O
cid:2	O
)	O
∈sc	O
t	O
(	O
x	O
→	O
x	O
(	O
cid:2	O
)	O
p∗	O
(	O
s	O
)	O
φ	O
(	O
cid:2	O
)	O
min	O
s:0≤p∗	O
(	O
s	O
)	O
≤0.5	O
one	O
can	O
show	O
that	O
τ	O
≤	O
o	O
(	O
1	O
φ2	O
log	O
n	O
	O
)	O
)	O
,	O
(	O
24.69	O
)	O
(	O
24.70	O
)	O
hence	O
chains	O
with	O
low	O
conductance	O
have	O
high	O
mixing	O
time	O
.	O
for	O
example	O
,	O
distributions	O
with	O
well-separated	O
modes	O
usually	O
have	O
high	O
mixing	O
time	O
.	O
simple	O
mcmc	O
methods	O
often	O
do	O
not	O
work	O
well	O
in	O
such	O
cases	O
,	O
and	O
more	O
advanced	O
algorithms	O
,	O
such	O
as	O
parallel	B
tempering	I
,	O
are	O
necessary	O
(	O
see	O
e.g.	O
,	O
(	O
liu	O
2001	O
)	O
)	O
.	O
24.4.3	O
practical	O
convergence	O
diagnostics	O
computing	O
the	O
mixing	B
time	I
of	O
a	O
chain	O
is	O
in	O
general	O
quite	O
difficult	O
,	O
since	O
the	O
transition	B
matrix	I
is	O
usually	O
very	O
hard	O
to	O
compute	O
.	O
in	O
practice	O
various	O
heuristics	B
have	O
been	O
proposed	O
to	O
diagnose	O
24.4.	O
speed	O
and	O
accuracy	O
of	O
mcmc	O
859	O
convergence	O
—	O
see	O
(	O
geyer	O
1992	O
;	O
cowles	O
and	O
carlin	O
1996	O
;	O
brooks	O
and	O
roberts	O
1998	O
)	O
for	O
a	O
review	O
.	O
strictly	O
speaking	O
,	O
these	O
methods	O
do	O
not	O
diagnose	O
convergence	O
,	O
but	O
rather	O
non-convergence	O
.	O
that	O
is	O
,	O
the	O
method	O
may	O
claim	O
the	O
chain	O
has	O
converged	O
when	O
in	O
fact	O
it	O
has	O
not	O
.	O
this	O
is	O
a	O
ﬂaw	O
common	O
to	O
all	O
convergence	O
diagnostics	O
,	O
since	O
diagnosing	O
convergence	O
is	O
computationally	O
intractable	O
in	O
general	O
(	O
bhatnagar	O
et	O
al	O
.	O
2010	O
)	O
.	O
one	O
of	O
the	O
simplest	O
approaches	O
to	O
assessing	O
when	O
the	O
method	O
has	O
converged	O
is	O
to	O
run	O
multiple	O
chains	O
from	O
very	O
different	O
overdispersed	B
starting	O
points	O
,	O
and	O
to	O
plot	O
the	O
samples	B
of	O
some	O
variables	O
of	O
interest	O
.	O
this	O
is	O
called	O
a	O
trace	B
plot	I
.	O
if	O
the	O
chain	O
has	O
mixed	O
,	O
it	O
should	O
have	O
“	O
forgotten	O
”	O
where	O
it	O
started	O
from	O
,	O
so	O
the	O
trace	B
plots	O
should	O
converge	B
to	O
the	O
same	O
distribution	O
,	O
and	O
thus	O
overlap	O
with	O
each	O
other	O
.	O
figure	O
24.12	O
gives	O
an	O
example	O
.	O
we	O
show	O
the	O
traceplot	O
for	O
x	O
which	O
was	O
sampled	O
from	O
a	O
mixture	O
of	O
two	O
1d	O
gaussians	O
using	O
four	O
different	O
methods	O
:	O
mh	O
with	O
a	O
symmetric	B
gaussian	O
proposal	O
of	O
variance	B
σ2	O
∈	O
{	O
1	O
,	O
8	O
,	O
500	O
}	O
,	O
and	O
gibbs	O
sampling	O
.	O
we	O
see	O
that	O
σ2	O
=	O
1	O
has	O
not	O
mixed	O
,	O
which	O
is	O
also	O
evident	O
from	O
figure	O
24.7	O
(	O
a	O
)	O
,	O
which	O
shows	O
that	O
a	O
single	O
chain	O
never	O
leaves	B
the	O
area	O
where	O
it	O
started	O
.	O
the	O
results	O
for	O
the	O
other	O
methods	O
indicate	O
that	O
the	O
chains	O
rapidly	O
converge	B
to	O
(	O
the	O
sticky	B
nature	O
of	O
the	O
σ2	O
=	O
500	O
the	O
stationary	B
distribution	I
,	O
no	O
matter	O
where	O
they	O
started	O
.	O
proposal	O
is	O
very	O
evident	O
.	O
this	O
reduces	O
the	O
computational	O
efficiency	O
,	O
as	O
we	O
discuss	O
below	O
,	O
but	O
not	O
the	O
statistical	O
validity	O
.	O
)	O
24.4.3.1	O
estimated	B
potential	I
scale	I
reduction	I
(	O
epsr	O
)	O
we	O
can	O
assess	O
convergence	O
more	O
quantitatively	O
as	O
follows	O
.	O
the	O
basic	O
idea	O
is	O
to	O
compare	O
the	O
variance	B
of	O
a	O
quantity	O
within	O
each	O
chain	O
to	O
its	O
variance	B
across	O
chains	O
.	O
more	O
precisely	O
,	O
suppose	O
we	O
collect	O
s	O
samples	B
(	O
after	O
burn-in	O
)	O
from	O
each	O
of	O
c	O
chains	O
of	O
d	O
variables	O
,	O
xisc	O
,	O
i	O
=	O
1	O
:	O
d	O
,	O
s	O
=	O
1	O
:	O
s	O
,	O
c	O
=	O
1	O
:	O
c	O
.	O
let	O
ysc	O
be	O
a	O
scalar	O
quantity	O
of	O
interest	O
derived	O
from	O
x1	O
:	O
d	O
,	O
s	O
,	O
c	O
(	O
e.g.	O
,	O
ysc	O
=	O
xisc	O
for	O
some	O
chosen	O
i	O
)	O
.	O
deﬁne	O
the	O
within-sequence	O
mean	B
and	O
overall	O
mean	B
as	O
s	O
(	O
cid:4	O
)	O
s=1	O
y·c	O
(	O
cid:2	O
)	O
1	O
s	O
c	O
(	O
cid:4	O
)	O
c=1	O
y·c	O
ysc	O
,	O
y··	O
(	O
cid:2	O
)	O
1	O
c	O
c	O
(	O
cid:4	O
)	O
deﬁne	O
the	O
between-sequence	O
and	O
within-sequence	O
variance	B
as	O
b	O
(	O
cid:2	O
)	O
s	O
c	O
−	O
1	O
c=1	O
(	O
y·c	O
−	O
y··	O
)	O
2	O
,	O
w	O
(	O
cid:2	O
)	O
1	O
c	O
1	O
s	O
−	O
1	O
(	O
ysc	O
−	O
y·c	O
)	O
2	O
(	O
cid:24	O
)	O
c	O
(	O
cid:4	O
)	O
c=1	O
s	O
(	O
cid:4	O
)	O
s=1	O
(	O
cid:25	O
)	O
(	O
24.71	O
)	O
(	O
24.72	O
)	O
we	O
can	O
now	O
construct	O
two	O
estimates	O
of	O
the	O
variance	B
of	O
y.	O
the	O
ﬁrst	O
estimate	O
is	O
w	O
:	O
this	O
should	O
underestimate	O
var	O
[	O
y	O
]	O
if	O
the	O
chains	O
have	O
not	O
ranged	O
over	O
the	O
full	B
posterior	O
.	O
the	O
second	O
estimate	O
is	O
w	O
+	O
1	O
s	O
b	O
(	O
24.73	O
)	O
s	O
−	O
1	O
s	O
ˆv	O
=	O
this	O
is	O
an	O
estimate	O
of	O
var	O
[	O
y	O
]	O
that	O
is	O
unbiased	B
under	O
stationarity	O
,	O
but	O
is	O
an	O
overestimate	O
if	O
the	O
starting	O
points	O
were	O
overdispersed	B
(	O
gelman	O
and	O
rubin	O
1992	O
)	O
.	O
from	O
this	O
,	O
we	O
can	O
deﬁne	O
the	O
following	O
convergence	O
diagnostic	O
statistic	O
,	O
known	O
as	O
the	O
estimated	B
potential	I
scale	I
reduction	I
or	O
epsr	O
:	O
8	O
ˆr	O
(	O
cid:2	O
)	O
ˆv	O
w	O
(	O
24.74	O
)	O
860	O
50	O
40	O
30	O
20	O
10	O
0	O
−10	O
0	O
50	O
40	O
30	O
20	O
10	O
0	O
−10	O
−20	O
−30	O
−40	O
−50	O
0	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
mh	O
n	O
(	O
0,1.0002	O
)	O
,	O
rhat	O
=	O
1.493	O
mh	O
n	O
(	O
0,8.0002	O
)	O
,	O
rhat	O
=	O
1.039	O
60	O
40	O
20	O
0	O
−20	O
−40	O
−60	O
0	O
60	O
40	O
20	O
0	O
−20	O
−40	O
−60	O
0	O
200	O
400	O
600	O
800	O
1000	O
(	O
a	O
)	O
mh	O
n	O
(	O
0,500.0002	O
)	O
,	O
rhat	O
=	O
1.005	O
200	O
400	O
600	O
800	O
1000	O
200	O
400	O
600	O
800	O
1000	O
(	O
b	O
)	O
gibbs	O
,	O
rhat	O
=	O
1.007	O
200	O
400	O
600	O
800	O
1000	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
24.12	O
traceplots	O
for	O
mcmc	O
samplers	O
.	O
each	O
color	O
represents	O
the	O
samples	B
from	O
a	O
different	O
starting	O
point	O
.	O
(	O
d	O
)	O
gibbs	O
sampling	O
.	O
figure	O
generated	O
by	O
mcmcgmmdemo	O
.	O
(	O
a-c	O
)	O
mh	O
with	O
proposal	O
n	O
(	O
x	O
(	O
cid:2	O
)	O
|x	O
,	O
σ2	O
)	O
for	O
σ2	O
∈	O
{	O
1	O
,	O
8	O
,	O
500	O
}	O
,	O
corresponding	O
to	O
figure	O
24.7.	O
this	O
quantity	O
,	O
which	O
was	O
ﬁrst	O
proposed	O
in	O
(	O
gelman	O
and	O
rubin	O
1992	O
)	O
,	O
measures	O
the	O
degree	B
to	O
which	O
the	O
posterior	O
variance	O
would	O
decrease	O
if	O
we	O
were	O
to	O
continue	O
sampling	O
in	O
the	O
s	O
→	O
∞	O
limit	O
.	O
if	O
ˆr	O
≈	O
1	O
for	O
any	O
given	O
quantity	O
,	O
then	O
that	O
estimate	O
is	O
reliable	O
(	O
or	O
at	O
least	O
is	O
not	O
unreliable	O
)	O
.	O
the	O
ˆr	O
values	O
for	O
the	O
four	O
samplers	O
in	O
figure	O
24.12	O
are	O
1.493	O
,	O
1.039	O
,	O
1.005	O
and	O
1.007.	O
so	O
this	O
diagnostic	O
has	O
correctly	O
identiﬁed	O
that	O
the	O
sampler	O
using	O
the	O
ﬁrst	O
(	O
σ2	O
=	O
1	O
)	O
proposal	O
is	O
untrustworthy	O
.	O
24.4.4	O
accuracy	O
of	O
mcmc	O
the	O
samples	B
produced	O
by	O
mcmc	O
are	O
auto-correlated	O
,	O
and	O
this	O
reduces	O
their	O
information	B
content	O
relative	O
to	O
independent	O
or	O
“	O
perfect	O
”	O
samples	B
.	O
we	O
can	O
quantify	O
this	O
as	O
follows.4	O
suppose	O
we	O
want	O
4.	O
this	O
section	O
is	O
based	O
on	O
(	O
hoff	O
2009	O
,	O
sec	O
6.6	O
)	O
.	O
24.4.	O
speed	O
and	O
accuracy	O
of	O
mcmc	O
861	O
mh	O
n	O
(	O
0,1.0002	O
)	O
mh	O
n	O
(	O
0,8.0002	O
)	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
5	O
10	O
15	O
20	O
25	O
30	O
35	O
40	O
45	O
(	O
a	O
)	O
mh	O
n	O
(	O
0,500.0002	O
)	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
5	O
10	O
15	O
20	O
25	O
30	O
35	O
40	O
45	O
(	O
b	O
)	O
gibbs	O
−0.2	O
0	O
5	O
10	O
15	O
20	O
25	O
30	O
35	O
40	O
45	O
−0.2	O
0	O
5	O
10	O
15	O
20	O
25	O
30	O
35	O
40	O
45	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
24.13	O
autocorrelation	O
functions	O
corresponding	O
to	O
figure	O
24.12.	O
figure	O
generated	O
by	O
mcmcgmmdemo	O
.	O
to	O
estimate	O
the	O
mean	B
of	O
f	O
(	O
x	O
)	O
,	O
for	O
some	O
function	O
f	O
,	O
wherex	O
∼	O
p	O
(	O
)	O
.	O
denote	O
the	O
true	O
mean	O
by	O
(	O
24.75	O
)	O
(	O
24.76	O
)	O
f∗	O
(	O
cid:2	O
)	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
s	O
(	O
cid:4	O
)	O
f	O
=	O
1	O
s	O
a	O
monte	O
carlo	O
estimate	O
is	O
given	O
by	O
fs	O
s=1	O
862	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
where	O
fs	O
(	O
cid:2	O
)	O
f	O
(	O
xs	O
)	O
and	O
xs	O
∼	O
p	O
(	O
x	O
)	O
.	O
an	O
mcmc	O
estimate	O
of	O
the	O
variance	B
of	O
this	O
estimate	O
is	O
given	O
by	O
(	O
24.77	O
)	O
(	O
24.78	O
)	O
(	O
24.79	O
)	O
(	O
24.80	O
)	O
e	O
[	O
(	O
fs	O
−	O
f∗	O
)	O
(	O
ft	O
−	O
f∗	O
)	O
]	O
)	O
(	O
ft	O
−	O
f∗	O
)	O
]	O
varm	O
cm	O
c	O
[	O
f	O
]	O
=e	O
=	O
e	O
(	O
cid:14	O
)	O
(	O
cid:13	O
)	O
⎡	O
⎣3	O
(	O
f	O
−	O
f∗	O
s	O
(	O
cid:4	O
)	O
(	O
cid:24	O
)	O
s	O
(	O
cid:4	O
)	O
1	O
s	O
s=1	O
e	O
s=1	O
)	O
2	O
(	O
fs	O
−	O
f∗	O
)	O
2	O
(	O
fs	O
−	O
f∗	O
(	O
cid:4	O
)	O
1	O
s2	O
s	O
(	O
cid:5	O
)	O
=t	O
⎤	O
⎦	O
42	O
(	O
cid:25	O
)	O
)	O
(	O
cid:4	O
)	O
+	O
1	O
s2	O
s	O
(	O
cid:5	O
)	O
=t	O
e	O
[	O
(	O
fs	O
−	O
f∗	O
=	O
1	O
s2	O
=	O
varm	O
c	O
(	O
f	O
)	O
+	O
where	O
the	O
ﬁrst	O
term	O
is	O
the	O
monte	O
carlo	O
estimate	O
of	O
the	O
variance	B
if	O
the	O
samples	B
weren	O
’	O
t	O
correlated	O
,	O
and	O
the	O
second	O
term	O
depends	O
on	O
the	O
correlation	O
of	O
the	O
samples	B
.	O
we	O
can	O
measure	O
this	O
as	O
follows	O
.	O
deﬁne	O
the	O
sample-based	O
auto-correlation	O
at	O
lag	B
t	O
of	O
a	O
set	O
of	O
samples	B
f1	O
,	O
.	O
.	O
.	O
,	O
fs	O
as	O
follows	O
:	O
(	O
cid:7	O
)	O
s−t	O
(	O
cid:7	O
)	O
s	O
s=1	O
(	O
fs	O
−	O
f	O
)	O
(	O
fs+t	O
−	O
f	O
)	O
s=1	O
(	O
fs	O
−	O
f	O
)	O
2	O
1	O
s−1	O
1	O
s−t	O
ρt	O
(	O
cid:2	O
)	O
(	O
24.81	O
)	O
this	O
is	O
called	O
the	O
autocorrelation	B
function	I
(	O
acf	O
)	O
.	O
this	O
is	O
plotted	O
in	O
figure	O
24.13	O
for	O
our	O
four	O
samplers	O
for	O
the	O
gaussian	O
mixture	B
model	I
.	O
we	O
see	O
that	O
the	O
acf	O
of	O
the	O
gibbs	O
sampler	O
(	O
bottom	O
right	O
)	O
dies	O
off	O
to	O
0	O
much	O
more	O
rapidly	O
than	O
the	O
mh	O
samplers	O
,	O
indicating	O
that	O
each	O
gibbs	O
sample	O
is	O
“	O
worth	O
”	O
more	O
than	O
each	O
mh	O
sample	O
.	O
a	O
simple	O
method	O
to	O
reduce	O
the	O
autocorrelation	O
is	O
to	O
use	O
thinning	B
,	O
in	O
which	O
we	O
keep	O
every	O
n	O
’	O
th	O
sample	O
.	O
this	O
does	O
not	O
increase	O
the	O
efficiency	O
of	O
the	O
underlying	O
sampler	O
,	O
but	O
it	O
does	O
save	O
space	O
,	O
since	O
it	O
avoids	O
storing	O
highly	O
correlated	O
samples	O
.	O
we	O
can	O
estimate	O
the	O
information	B
content	O
of	O
a	O
set	O
of	O
samples	B
by	O
computing	O
the	O
effective	B
sample	I
size	I
(	O
ess	O
)	O
s	O
eff	O
,	O
deﬁned	O
by	O
s	O
eff	O
(	O
cid:2	O
)	O
varm	O
c	O
(	O
f	O
)	O
varm	O
cm	O
c	O
(	O
f	O
)	O
(	O
24.82	O
)	O
from	O
figure	O
24.12	O
,	O
it	O
is	O
clear	O
that	O
the	O
effective	B
sample	I
size	I
of	O
the	O
gibbs	O
sampler	O
is	O
higher	O
than	O
that	O
of	O
the	O
other	O
samplers	O
(	O
in	O
this	O
example	O
)	O
.	O
24.4.5	O
how	O
many	O
chains	O
?	O
a	O
natural	O
question	O
to	O
ask	O
is	O
:	O
how	O
many	O
chains	O
should	O
we	O
run	O
?	O
we	O
could	O
either	O
run	O
one	O
long	O
chain	O
to	O
ensure	O
convergence	O
,	O
and	O
then	O
collect	O
samples	O
spaced	O
far	O
apart	O
,	O
or	O
we	O
could	O
run	O
many	O
short	O
chains	O
,	O
but	O
that	O
wastes	O
the	O
burnin	O
time	O
.	O
in	O
practice	O
it	O
is	O
common	O
to	O
run	O
a	O
medium	O
number	O
of	O
chains	O
(	O
say	O
3	O
)	O
of	O
medium	O
length	O
(	O
say	O
100,000	O
steps	O
)	O
,	O
and	O
to	O
take	O
samples	B
from	O
each	O
after	O
discarding	O
the	O
ﬁrst	O
half	O
of	O
the	O
samples	B
.	O
if	O
we	O
initialize	O
at	O
a	O
local	O
mode	O
,	O
we	O
may	O
be	O
able	O
to	O
use	O
all	O
the	O
samples	B
,	O
and	O
not	O
wait	O
for	O
burn-in	O
.	O
24.5.	O
auxiliary	O
variable	O
mcmc	O
*	O
863	O
em	O
ep	O
gibbs+	O
gibbs	O
with	O
ars	O
model	O
goal	O
method	O
probit	B
map	O
gradient	O
probit	O
map	O
post	O
probit	B
post	O
probit	B
post	O
probit	B
probit	O
post	O
mh	O
using	O
irls	O
proposal	O
map	O
gradient	O
logit	O
post	O
logit	B
logit	O
post	O
gibbs+	O
with	O
student	O
gibbs+	O
with	O
ks	O
reference	O
section	O
9.4.1	O
section	O
11.4.6	O
(	O
nickisch	O
and	O
rasmussen	O
2008	O
)	O
exercise	O
24.6	O
(	O
dellaportas	O
and	O
smith	O
1993	O
)	O
(	O
gamerman	O
1997	O
)	O
section	O
8.3.4	O
(	O
fruhwirth-schnatter	O
and	O
fruhwirth	O
2010	O
)	O
(	O
holmes	O
and	O
held	O
2006	O
)	O
table	O
24.1	O
summary	O
of	O
some	O
possible	O
algorithms	O
for	O
estimation	O
and	O
inference	B
for	O
binary	B
classiﬁcation	I
problems	O
using	O
gaussian	O
priors	O
.	O
abbreviations	O
:	O
aux	O
.	O
=	O
auxiliary	O
variable	O
sampling	O
,	O
ars	O
=	O
adaptive	B
rejection	I
sampling	I
,	O
ep	O
=	O
expectation	B
propagation	I
,	O
gibbs+	O
=	O
gibbs	O
sampling	O
with	O
auxiliary	B
variables	I
,	O
irls	O
=	O
iterative	O
reweighted	O
least	B
squares	I
,	O
ks	O
=	O
kolmogorov	O
smirnov	O
,	O
map	O
=	O
maximum	B
a	I
posteriori	I
,	O
mh	O
=	O
metropolis	O
hastings	O
,	O
post	O
=	O
posterior	O
.	O
24.5	O
auxiliary	O
variable	O
mcmc	O
*	O
sometimes	O
we	O
can	O
dramatically	O
improve	O
the	O
efficiency	O
of	O
sampling	O
by	O
introducing	O
dummy	O
auxiliary	O
variables	O
,	O
in	O
order	O
to	O
reduce	O
correlation	O
between	O
the	O
original	O
variables	O
.	O
if	O
the	O
original	O
variables	O
are	O
denoted	O
by	O
x	O
,	O
and	O
the	O
auxiliary	B
variables	I
by	O
z	O
,	O
we	O
require	O
that	O
z	O
p	O
(	O
x	O
,	O
z	O
)	O
=	O
p	O
(	O
x	O
)	O
,	O
and	O
that	O
p	O
(	O
x	O
,	O
z	O
)	O
is	O
easier	O
to	O
sample	O
from	O
than	O
just	O
p	O
(	O
x	O
)	O
.	O
if	O
we	O
meet	O
these	O
two	O
conditions	O
,	O
we	O
can	O
sample	O
in	O
the	O
enlarged	O
model	O
,	O
and	O
then	O
throw	O
away	O
the	O
sampled	O
z	O
values	O
,	O
thereby	O
recovering	O
samples	B
from	O
p	O
(	O
x	O
)	O
.	O
we	O
give	O
some	O
examples	O
below	O
.	O
(	O
cid:7	O
)	O
24.5.1	O
auxiliary	O
variable	O
sampling	O
for	O
logistic	B
regression	I
in	O
section	O
9.4.2	O
,	O
we	O
discussed	O
the	O
latent	O
variable	O
interpretation	O
of	O
probit	B
regression	I
.	O
recall	B
that	O
this	O
had	O
the	O
form	O
zi	O
(	O
cid:2	O
)	O
wt	O
xi	O
+	O
i	O
i	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
yi	O
=	O
1	O
=	O
i	O
(	O
zi	O
≥	O
0	O
)	O
(	O
24.83	O
)	O
(	O
24.84	O
)	O
(	O
24.85	O
)	O
we	O
exploited	O
this	O
representation	O
in	O
section	O
11.4.6	O
,	O
where	O
we	O
used	O
em	O
to	O
ﬁnd	O
an	O
ml	O
estimate	O
.	O
it	O
is	O
straightforward	O
to	O
convert	O
this	O
into	O
an	O
auxiliary	O
variable	O
gibbs	O
sampler	O
(	O
exercise	O
24.6	O
)	O
,	O
since	O
p	O
(	O
w|d	O
)	O
is	O
gaussian	O
and	O
p	O
(	O
zi|xi	O
,	O
yi	O
,	O
w	O
)	O
is	O
truncated	O
gaussian	O
,	O
both	O
of	O
which	O
are	O
easy	O
to	O
sample	O
from	O
.	O
now	O
let	O
us	O
discuss	O
how	O
to	O
derive	O
an	O
auxiliary	O
variable	O
gibbs	O
sampler	O
for	O
logistic	B
regression	I
.	O
let	O
i	O
follow	O
a	O
logistic	B
distribution	I
,	O
with	O
pdf	B
plogistic	O
(	O
	O
)	O
=	O
e−	O
(	O
1	O
+	O
e−	O
)	O
2	O
(	O
24.86	O
)	O
with	O
mean	B
e	O
[	O
	O
]	O
=	O
0	O
and	O
variance	B
var	O
[	O
	O
]	O
=	O
π2/3	O
.	O
the	O
cdf	B
has	O
the	O
form	O
f	O
(	O
	O
)	O
=	O
sigm	O
(	O
	O
)	O
,	O
which	O
p	O
(	O
yi	O
=	O
1|xi	O
,	O
w	O
)	O
=	O
−wt	O
xi	O
f	O
(	O
	O
)	O
d	O
=	O
−∞	O
f	O
(	O
	O
)	O
d	O
=	O
f	O
(	O
wt	O
xi	O
)	O
=	O
sigm	O
(	O
wt	O
xi	O
)	O
(	O
24.87	O
)	O
as	O
required	O
.	O
we	O
can	O
derive	O
an	O
auxiliary	O
variable	O
gibbs	O
sampler	O
by	O
sampling	O
from	O
p	O
(	O
z|w	O
,	O
d	O
)	O
and	O
p	O
(	O
w|z	O
,	O
d	O
)	O
.	O
unfortunately	O
,	O
sampling	O
directly	O
from	O
p	O
(	O
w|z	O
,	O
d	O
)	O
is	O
not	O
possible	O
.	O
one	O
approach	O
is	O
to	O
deﬁne	O
i	O
∼	O
n	O
(	O
0	O
,	O
λi	O
)	O
,	O
where	O
λi	O
=	O
(	O
2ψi	O
)	O
2	O
and	O
ψi	O
∼	O
ks	O
,	O
the	O
kolmogorov	O
smirnov	O
distribution	O
,	O
and	O
then	O
to	O
sample	O
w	O
,	O
z	O
,	O
λ	O
and	O
ψ	O
(	O
holmes	O
and	O
held	O
2006	O
)	O
.	O
a	O
simpler	O
approach	O
is	O
to	O
approximate	O
the	O
logistic	B
distribution	I
by	O
the	O
student	O
distribution	O
(	O
albert	O
and	O
chib	O
1993	O
)	O
.	O
speciﬁcally	O
,	O
we	O
will	O
make	O
the	O
approximation	O
i	O
∼	O
t	O
(	O
0	O
,	O
1	O
,	O
ν	O
)	O
,	O
where	O
ν	O
≈	O
8.	O
we	O
can	O
now	O
use	O
the	O
scale	O
mixture	O
of	O
gaussians	O
representation	O
of	O
the	O
student	O
to	O
simplify	O
inference	B
.	O
in	O
particular	O
,	O
we	O
write	O
λi	O
∼	O
ga	O
(	O
ν/2	O
,	O
ν/2	O
)	O
i	O
∼	O
n	O
(	O
0	O
,	O
λ−1	O
)	O
zi	O
(	O
cid:2	O
)	O
wt	O
xi	O
+	O
i	O
yi	O
=	O
1|zi	O
=	O
i	O
(	O
zi	O
≥	O
0	O
)	O
(	O
24.88	O
)	O
(	O
24.89	O
)	O
(	O
24.90	O
)	O
(	O
24.91	O
)	O
i	O
all	O
of	O
the	O
full	B
conditionals	O
now	O
have	O
a	O
simple	O
form	O
;	O
see	O
exercise	O
24.7	O
for	O
the	O
details	O
.	O
note	O
that	O
if	O
we	O
set	O
ν	O
=	O
1	O
,	O
then	O
zi	O
∼	O
n	O
(	O
wt	O
xi	O
,	O
1	O
)	O
,	O
which	O
is	O
equivalent	O
to	O
probit	B
regression	I
(	O
see	O
section	O
9.4	O
)	O
.	O
rather	O
than	O
choosing	O
between	O
probit	B
or	O
logit	B
regression	O
,	O
we	O
can	O
simply	O
estimate	O
the	O
ν	O
parameter	B
.	O
there	O
is	O
no	O
convenient	O
conjugate	B
prior	I
,	O
but	O
we	O
can	O
consider	O
a	O
ﬁnite	O
range	O
of	O
possible	O
values	O
and	O
evaluate	O
the	O
posterior	O
as	O
follows	O
:	O
n	O
(	O
cid:20	O
)	O
p	O
(	O
ν|λ	O
)	O
∝	O
p	O
(	O
ν	O
)	O
1	O
λν/2−1	O
i	O
e−νλi/2	O
γ	O
(	O
ν/2	O
)	O
(	O
ν/2	O
)	O
ν/2	O
i=1	O
(	O
24.92	O
)	O
(	O
cid:7	O
)	O
d	O
864	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
is	O
the	O
logistic	B
function	O
.	O
since	O
yi	O
=	O
1	O
iff	B
wt	O
xi	O
+	O
	O
>	O
0	O
,	O
we	O
have	O
,	O
by	O
symmetry	O
,	O
that	O
(	O
cid:12	O
)	O
∞	O
(	O
cid:12	O
)	O
wt	O
xi	O
furthermore	O
,	O
if	O
we	O
deﬁne	O
v0	O
=	O
v0i	O
,	O
we	O
can	O
sample	O
v0	O
as	O
well	O
.	O
for	O
example	O
,	O
suppose	O
we	O
use	O
a	O
ig	O
(	O
δ1	O
,	O
δ2	O
)	O
prior	O
for	O
v0	O
.	O
the	O
posterior	O
is	O
given	O
by	O
p	O
(	O
v0|w	O
)	O
=	O
ig	O
(	O
δ1	O
+	O
1	O
j=1	O
w2	O
j	O
)	O
.	O
this	O
can	O
be	O
interleaved	O
with	O
the	O
other	O
gibbs	O
sampling	O
steps	O
,	O
and	O
provides	O
an	O
appealing	O
bayesian	O
alternative	O
to	O
cross	B
validation	I
for	O
setting	O
the	O
strength	O
of	O
the	O
regularizer	O
.	O
2	O
d	O
,	O
δ2	O
+	O
1	O
2	O
see	O
table	O
24.1	O
for	O
a	O
summary	O
of	O
various	O
algorithms	O
for	O
ﬁtting	O
probit	B
and	O
logit	B
models	O
.	O
many	O
of	O
these	O
methods	O
can	O
also	O
be	O
extended	O
to	O
the	O
multinomial	B
logistic	I
regression	I
case	O
.	O
for	O
details	O
,	O
see	O
(	O
scott	O
2009	O
;	O
fruhwirth-schnatter	O
and	O
fruhwirth	O
2010	O
)	O
.	O
24.5.2	O
slice	B
sampling	I
consider	O
sampling	O
from	O
a	O
univariate	O
,	O
but	O
multimodal	O
,	O
distribution	O
˜p	O
(	O
x	O
)	O
.	O
we	O
can	O
sometimes	O
improve	O
the	O
ability	O
to	O
make	O
large	O
moves	O
by	O
adding	O
an	O
auxiliary	O
variable	O
u.	O
we	O
deﬁne	O
the	O
joint	B
distribution	I
as	O
follows	O
:	O
1/zp	O
0	O
if	O
0	O
≤	O
u	O
≤	O
˜p	O
(	O
x	O
)	O
otherwise	O
ˆp	O
(	O
x	O
,	O
u	O
)	O
=	O
(	O
24.93	O
)	O
(	O
cid:19	O
)	O
24.5.	O
auxiliary	O
variable	O
mcmc	O
*	O
865	O
f	O
(	O
x	O
)	O
(	O
i	O
)	O
(	O
i+1	O
)	O
u	O
(	O
i+1	O
)	O
x	O
(	O
i	O
)	O
x	O
(	O
a	O
)	O
x	O
180	O
160	O
140	O
120	O
100	O
80	O
60	O
40	O
20	O
0	O
−5	O
−4	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
(	O
b	O
)	O
(	O
a	O
)	O
illustration	O
of	O
the	O
principle	O
behind	O
slice	B
sampling	I
.	O
given	O
a	O
previous	O
sample	O
xi	O
,	O
we	O
figure	O
24.14	O
sample	O
ui+1	O
uniformly	O
on	O
[	O
0	O
,	O
f	O
(	O
xi	O
)	O
]	O
,	O
where	O
f	O
is	O
the	O
target	O
density	O
.	O
we	O
then	O
sample	O
xi+1	O
along	O
the	O
slice	O
where	O
f	O
(	O
x	O
)	O
≥	O
ui+1	O
.	O
source	O
:	O
figure	O
15	O
of	O
(	O
andrieu	O
et	O
al	O
.	O
2003	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
nando	O
de	O
freitas	O
.	O
(	O
b	O
)	O
slice	B
sampling	I
in	O
action	B
.	O
figure	O
generated	O
by	O
slicesamplingdemo1d	O
.	O
x	O
10−11	O
y	O
t	O
i	O
s	O
n	O
e	O
d	O
r	O
o	O
i	O
r	O
e	O
t	O
s	O
o	O
p	O
4	O
3	O
2	O
1	O
0	O
6	O
5	O
4	O
slope	O
3	O
−1	O
−1.5	O
intercept	O
(	O
a	O
)	O
−2	O
−2.5	O
(	O
b	O
)	O
figure	O
24.15	O
binomial	B
regression	I
for	O
1d	O
data	O
.	O
approximation	O
.	O
figure	O
generated	O
by	O
slicesamplingdemo2d	O
.	O
(	O
a	O
)	O
grid	O
approximation	O
to	O
posterior	O
.	O
(	O
b	O
)	O
slice	B
sampling	I
(	O
24.94	O
)	O
(	O
cid:21	O
)	O
where	O
zp	O
=	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
˜p	O
(	O
x	O
)	O
˜p	O
(	O
x	O
)	O
dx	O
.	O
the	O
marginal	B
distribution	I
over	O
x	O
is	O
given	O
by	O
ˆp	O
(	O
x	O
,	O
u	O
)	O
du	O
=	O
0	O
1	O
zp	O
du	O
=	O
˜p	O
(	O
x	O
)	O
zp	O
=	O
p	O
(	O
x	O
)	O
so	O
we	O
can	O
sample	O
from	O
p	O
(	O
x	O
)	O
by	O
sampling	O
from	O
ˆp	O
(	O
x	O
,	O
u	O
)	O
and	O
then	O
ignoring	O
u.	O
the	O
full	B
conditionals	O
have	O
the	O
form	O
p	O
(	O
u|x	O
)	O
=	O
u	O
[	O
0	O
,	O
˜p	O
(	O
x	O
)	O
]	O
(	O
u	O
)	O
p	O
(	O
x|u	O
)	O
=	O
ua	O
(	O
x	O
)	O
(	O
24.95	O
)	O
(	O
24.96	O
)	O
where	O
a	O
=	O
{	O
x	O
:	O
˜p	O
(	O
x	O
)	O
≥	O
u	O
}	O
is	O
the	O
set	O
of	O
points	O
on	O
or	O
above	O
the	O
chosen	O
height	O
u.	O
this	O
corresponds	O
to	O
a	O
slice	O
through	O
the	O
distribution	O
,	O
hence	O
the	O
term	O
slice	B
sampling	I
(	O
neal	O
2003a	O
)	O
.	O
see	O
figure	O
24.14	O
(	O
a	O
)	O
.	O
in	O
practice	O
,	O
it	O
can	O
be	O
difficult	O
to	O
identify	O
the	O
set	O
a.	O
so	O
we	O
can	O
use	O
the	O
following	O
approach	O
:	O
construct	O
an	O
interval	O
xmin	O
≤	O
x	O
≤	O
xmax	O
around	O
the	O
current	O
point	O
xs	O
of	O
some	O
width	O
.	O
we	O
then	O
866	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
test	O
to	O
see	O
if	O
each	O
end	O
point	O
lies	O
within	O
the	O
slice	O
.	O
if	O
it	O
does	O
,	O
we	O
keep	O
extending	O
in	O
that	O
direction	O
until	O
it	O
lies	O
outside	O
the	O
slice	O
.	O
this	O
is	O
called	O
stepping	B
out	I
.	O
a	O
candidate	O
value	O
x	O
(	O
cid:2	O
)	O
is	O
then	O
chosen	O
if	O
it	O
lies	O
within	O
the	O
slice	O
,	O
it	O
is	O
kept	O
,	O
so	O
xs+1	O
=	O
x	O
(	O
cid:2	O
)	O
.	O
otherwise	O
we	O
uniformly	O
from	O
this	O
region	O
.	O
shrink	O
the	O
region	O
such	O
that	O
x	O
(	O
cid:2	O
)	O
forms	O
one	O
end	O
and	O
such	O
that	O
the	O
region	O
still	O
contains	O
xs	O
.	O
then	O
another	O
sample	O
is	O
drawn	O
.	O
we	O
continue	O
in	O
this	O
way	O
until	O
a	O
sample	O
is	O
accepted	O
.	O
to	O
apply	O
the	O
method	O
to	O
multivariate	O
distributions	O
,	O
we	O
can	O
sample	O
one	O
extra	O
auxiliary	O
variable	O
for	O
each	O
dimension	O
.	O
the	O
advantage	O
of	O
slice	B
sampling	I
over	O
gibbs	O
is	O
that	O
it	O
does	O
not	O
need	O
a	O
speciﬁcation	O
of	O
the	O
full-conditionals	O
,	O
just	O
the	O
unnormalized	O
joint	O
.	O
the	O
advantage	O
of	O
slice	B
sampling	I
over	O
mh	O
is	O
that	O
it	O
does	O
not	O
need	O
a	O
user-speciﬁed	O
proposal	B
distribution	I
(	O
although	O
it	O
does	O
require	O
a	O
speciﬁcation	O
of	O
the	O
width	O
of	O
the	O
stepping	B
out	I
interval	O
)	O
.	O
figure	O
24.14	O
(	O
b	O
)	O
illustrates	O
the	O
algorithm	O
in	O
action	B
on	O
a	O
synthetic	O
1d	O
problem	O
.	O
figure	O
24.15	O
illustrates	O
its	O
behavior	O
on	O
a	O
slightly	O
harder	O
problem	O
,	O
namely	O
binomial	B
logistic	O
regression	B
.	O
the	O
model	O
has	O
the	O
form	O
yi	O
∼	O
bin	O
(	O
ni	O
,	O
logit	B
(	O
β1	O
+	O
β2xi	O
)	O
)	O
(	O
24.97	O
)	O
we	O
use	O
a	O
vague	O
gaussian	O
prior	O
for	O
the	O
βj	O
’	O
s	O
.	O
figure	O
24.15	O
(	O
a	O
)	O
shows	O
a	O
grid-based	O
approximation	O
to	O
the	O
posterior	O
,	O
and	O
figure	O
24.15	O
(	O
b	O
)	O
shows	O
a	O
sample-based	O
approximation	O
.	O
in	O
this	O
example	O
,	O
the	O
grid	O
is	O
faster	O
to	O
compute	O
,	O
but	O
for	O
any	O
problem	O
with	O
more	O
than	O
2	O
dimensions	O
,	O
the	O
grid	O
approach	O
is	O
infeasible	O
.	O
24.5.3	O
swendsen	O
wang	O
consider	O
an	O
ising	O
model	O
of	O
the	O
following	O
form	O
:	O
1	O
z	O
(	O
cid:20	O
)	O
(	O
cid:9	O
)	O
e	O
fe	O
(	O
xe	O
)	O
p	O
(	O
x	O
)	O
=	O
(	O
24.98	O
)	O
(	O
cid:8	O
)	O
where	O
xe	O
=	O
(	O
xi	O
,	O
xj	O
)	O
for	O
edge	O
e	O
=	O
(	O
i	O
,	O
j	O
)	O
,	O
xi	O
∈	O
{	O
+1	O
,	O
−1	O
}	O
,	O
and	O
the	O
edge	O
factor	O
fe	O
is	O
deﬁned	O
by	O
ej	O
,	O
where	O
j	O
is	O
the	O
edge	O
strength	O
.	O
gibbs	O
sampling	O
in	O
such	O
models	O
can	O
be	O
slow	O
when	O
e−j	O
j	O
is	O
large	O
in	O
absolute	O
value	O
,	O
because	O
neighboring	O
states	O
can	O
be	O
highly	O
correlated	O
.	O
the	O
swendsen	O
wang	O
algorithm	O
(	O
swendsen	O
and	O
wang	O
1987	O
)	O
is	O
a	O
auxiliary	O
variable	O
mcmc	O
sampler	O
which	O
mixes	O
much	O
faster	O
,	O
at	O
least	O
for	O
the	O
case	O
of	O
attractive	O
or	O
ferromagnetic	O
models	O
,	O
with	O
j	O
>	O
0.	O
e−j	O
ej	O
suppose	O
we	O
introduce	O
auxiliary	O
binary	O
variables	O
,	O
one	O
per	O
edge	O
.	O
5	O
these	O
are	O
called	O
bond	B
variables	I
,	O
and	O
will	O
be	O
denoted	O
by	O
z.	O
we	O
then	O
deﬁne	O
an	O
extended	O
model	O
p	O
(	O
x	O
,	O
z	O
)	O
of	O
the	O
form	O
p	O
(	O
x	O
,	O
z	O
)	O
=	O
1	O
z	O
(	O
cid:2	O
)	O
ge	O
(	O
xe	O
,	O
ze	O
)	O
(	O
cid:20	O
)	O
e	O
(	O
cid:8	O
)	O
where	O
ze	O
∈	O
{	O
0	O
,	O
1	O
}	O
,	O
and	O
we	O
deﬁne	O
the	O
new	O
factor	B
as	O
follows	O
:	O
ge	O
(	O
xe	O
,	O
ze	O
=	O
0	O
)	O
=	O
(	O
cid:9	O
)	O
ej	O
−	O
e−j	O
0	O
0	O
ej	O
−	O
e−j	O
(	O
cid:8	O
)	O
(	O
24.99	O
)	O
(	O
cid:9	O
)	O
,	O
e−j	O
e−j	O
e−j	O
e−j	O
(	O
cid:7	O
)	O
1	O
and	O
ge	O
(	O
xe	O
,	O
ze	O
=	O
1	O
)	O
=	O
.	O
it	O
is	O
clear	O
that	O
ze=0	O
ge	O
(	O
xe	O
,	O
ze	O
)	O
=	O
fe	O
(	O
xe	O
)	O
,	O
5.	O
our	O
presentation	O
of	O
the	O
method	O
is	O
based	O
on	O
some	O
notes	O
by	O
david	O
mackay	O
,	O
available	O
from	O
http	O
:	O
//www.inference	O
.phy.cam.ac.uk/mackay/itila/swendsen.pdf	O
.	O
24.5.	O
auxiliary	O
variable	O
mcmc	O
*	O
867	O
figure	O
24.16	O
illustration	O
of	O
the	O
swendsen	O
wang	O
algorithm	O
on	O
a	O
2d	O
grid	O
.	O
used	O
with	O
kind	O
permission	O
of	O
kevin	O
tang	O
.	O
(	O
cid:7	O
)	O
z	O
p	O
(	O
x	O
,	O
z	O
)	O
=	O
p	O
(	O
x	O
)	O
.	O
so	O
if	O
we	O
can	O
sample	O
from	O
this	O
extended	O
model	O
,	O
we	O
can	O
just	O
and	O
hence	O
that	O
throw	O
away	O
the	O
z	O
samples	B
and	O
get	O
valid	O
x	O
samples	B
from	O
the	O
original	O
distribution	O
.	O
fortunately	O
,	O
it	O
is	O
easy	O
to	O
apply	O
gibbs	O
sampling	O
to	O
this	O
extended	O
model	O
.	O
the	O
full	B
conditional	I
p	O
(	O
z|x	O
)	O
factorizes	O
over	O
the	O
edges	B
,	O
since	O
the	O
bond	B
variables	I
are	O
conditionally	B
independent	I
given	O
the	O
node	O
variables	O
.	O
furthermore	O
,	O
the	O
full	B
conditional	I
p	O
(	O
ze|xe	O
)	O
is	O
simple	O
to	O
compute	O
:	O
if	O
the	O
nodes	B
on	O
either	O
end	O
of	O
the	O
edge	O
are	O
in	O
the	O
same	O
state	B
(	O
xi	O
=	O
xj	O
)	O
,	O
we	O
set	O
the	O
bond	O
ze	O
to	O
1	O
with	O
probability	O
p	O
=	O
1	O
−	O
e−2j	O
,	O
otherwise	O
we	O
set	O
it	O
to	O
0.	O
in	O
figure	O
24.16	O
(	O
top	O
right	O
)	O
,	O
the	O
bonds	O
that	O
could	O
be	O
turned	O
on	O
(	O
because	O
their	O
corresponding	O
nodes	B
are	O
in	O
the	O
same	O
state	B
)	O
are	O
represented	O
in	O
figure	O
24.16	O
(	O
bottom	O
right	O
)	O
,	O
the	O
bonds	O
that	O
are	O
randomly	O
turned	O
on	O
are	O
by	O
dotted	O
edges	B
.	O
represented	O
by	O
solid	O
edges	B
.	O
to	O
sample	O
p	O
(	O
x|z	O
)	O
,	O
we	O
proceed	O
as	O
follows	O
.	O
find	O
the	O
connected	O
components	O
deﬁned	O
by	O
the	O
graph	B
induced	O
by	O
the	O
bonds	O
that	O
are	O
turned	O
on	O
.	O
(	O
note	O
that	O
a	O
connected	O
component	O
may	O
consist	O
of	O
a	O
singleton	O
node	O
.	O
)	O
pick	O
one	O
of	O
these	O
components	O
uniformly	O
at	O
random	O
.	O
all	O
the	O
nodes	B
in	O
each	O
such	O
component	O
must	O
have	O
the	O
same	O
state	B
,	O
since	O
the	O
off-diagonal	O
terms	O
in	O
the	O
ge	O
(	O
xe	O
,	O
ze	O
=	O
1	O
)	O
factor	B
are	O
0.	O
pick	O
a	O
state	B
±1	O
uniformly	O
at	O
random	O
,	O
and	O
force	O
all	O
the	O
variables	O
in	O
this	O
component	O
to	O
adopt	O
this	O
new	O
state	B
.	O
this	O
is	O
illustrated	O
in	O
figure	O
24.16	O
(	O
bottom	O
left	O
)	O
,	O
where	O
the	O
green	O
square	O
868	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
denotes	O
the	O
selected	O
connected	O
component	O
,	O
and	O
we	O
choose	O
to	O
force	O
all	O
nodes	O
within	O
in	O
to	O
enter	O
the	O
white	O
state	B
.	O
the	O
validity	O
of	O
this	O
algorithm	O
is	O
left	O
as	O
an	O
exercise	O
,	O
as	O
is	O
the	O
extension	B
to	O
handle	O
local	B
evidence	I
and	O
non-stationary	O
potentials	O
.	O
it	O
should	O
be	O
intuitively	O
clear	O
that	O
swendsen	O
wang	O
makes	O
much	O
larger	O
moves	O
through	O
the	O
state	B
space	I
than	O
gibbs	O
sampling	O
.	O
in	O
fact	O
,	O
sw	O
mixes	O
much	O
faster	O
than	O
gibbs	O
sampling	O
on	O
2d	O
lattice	B
ising	O
models	O
for	O
a	O
variety	O
of	O
values	O
of	O
the	O
coupling	O
parameter	B
,	O
provided	O
j	O
>	O
0.	O
more	O
precisely	O
,	O
let	O
the	O
edge	O
strength	O
be	O
parameterized	O
by	O
j/t	O
,	O
where	O
t	O
>	O
0	O
is	O
a	O
computational	O
temperature	O
.	O
for	O
large	O
t	O
,	O
the	O
nodes	B
are	O
roughly	O
independent	O
,	O
so	O
both	O
methods	O
work	O
equally	O
well	O
.	O
however	O
,	O
as	O
t	O
approaches	O
a	O
critical	B
temperature	I
tc	O
,	O
the	O
typical	O
states	O
of	O
the	O
system	O
have	O
very	O
long	O
correlation	O
lengths	O
,	O
and	O
gibbs	O
sampling	O
takes	O
a	O
very	O
long	O
time	O
to	O
generate	O
independent	O
samples	B
.	O
as	O
the	O
temperature	B
continues	O
to	O
drop	O
,	O
the	O
typical	O
states	O
are	O
either	O
all	O
on	O
or	O
all	O
off	O
.	O
the	O
frequency	O
with	O
which	O
gibbs	O
sampling	O
moves	O
between	O
these	O
two	O
modes	O
is	O
exponentiall	O
small	O
.	O
by	O
contrast	O
,	O
sw	O
mixes	O
rapidly	O
at	O
all	O
temperatures	O
.	O
unfortunately	O
,	O
if	O
any	O
of	O
the	O
edge	O
weights	O
are	O
negative	O
,	O
j	O
<	O
0	O
,	O
the	O
system	O
is	O
frustrated	B
,	O
and	O
there	O
are	O
exponentially	O
many	O
modes	O
,	O
even	O
at	O
low	O
temperature	O
.	O
sw	O
does	O
not	O
work	O
very	O
well	O
in	O
this	O
setting	O
,	O
since	O
it	O
tries	O
to	O
force	O
many	O
neighboring	O
variables	O
to	O
have	O
the	O
same	O
state	B
.	O
in	O
fact	O
,	O
computation	O
in	O
this	O
regime	O
is	O
provably	O
hard	O
for	O
any	O
algorithm	O
(	O
jerrum	O
and	O
sinclair	O
1993	O
,	O
1996	O
)	O
.	O
24.5.4	O
hybrid/hamiltonian	O
mcmc	O
*	O
in	O
this	O
section	O
,	O
we	O
brieﬂy	O
mention	O
a	O
way	O
to	O
perform	O
mcmc	O
sampling	O
for	O
continuous	O
state	B
spaces	O
,	O
for	O
which	O
we	O
can	O
compute	O
the	O
gradient	O
of	O
the	O
(	O
unnormalized	O
)	O
log-posterior	O
.	O
this	O
is	O
the	O
case	O
in	O
neural	B
network	I
models	O
,	O
for	O
example	O
.	O
the	O
basic	O
idea	O
is	O
to	O
think	O
of	O
the	O
parameters	O
as	O
a	O
particle	O
in	O
space	O
,	O
and	O
to	O
create	O
auxiliary	B
variables	I
which	O
represent	O
the	O
“	O
momentum	B
”	O
of	O
this	O
particle	O
.	O
we	O
then	O
update	O
this	O
parameter/	O
momentum	B
pair	O
according	O
to	O
certain	O
rules	B
(	O
see	O
e.g.	O
,	O
(	O
duane	O
et	O
al	O
.	O
1987	O
;	O
neal	O
1993	O
;	O
mackay	O
2003	O
;	O
neal	O
2010	O
)	O
for	O
details	O
)	O
.	O
the	O
resulting	O
method	O
is	O
called	O
hybrid	O
mcmc	O
or	O
hamiltonian	O
mcmc	O
.	O
the	O
two	O
main	O
parameters	O
that	O
the	O
user	O
must	O
specify	O
are	O
how	O
many	O
leapfrog	B
steps	I
to	O
take	O
when	O
updating	O
the	O
position/	O
momentum	B
,	O
and	O
how	O
big	O
to	O
make	O
these	O
steps	O
.	O
performance	O
can	O
be	O
quite	O
sensitive	O
to	O
these	O
parameters	O
(	O
although	O
see	O
(	O
hoffman	O
and	O
gelman	O
2011	O
)	O
for	O
a	O
recent	O
way	O
to	O
set	O
them	O
automatically	O
)	O
.	O
this	O
method	O
can	O
be	O
combined	O
with	O
stochastic	B
gradient	I
descent	I
(	O
section	O
8.5.2	O
)	O
in	O
order	O
to	O
handle	O
large	O
datasets	O
,	O
as	O
explained	O
in	O
(	O
ahn	O
et	O
al	O
.	O
2012	O
)	O
.	O
recently	O
,	O
a	O
more	O
powerful	O
extension	B
of	O
this	O
method	O
has	O
been	O
developed	O
,	O
that	O
exploits	O
second-	O
order	O
gradient	O
information	O
.	O
see	O
(	O
girolami	O
et	O
al	O
.	O
2010	O
)	O
for	O
details	O
.	O
24.6	O
annealing	B
methods	O
many	O
distributions	O
are	O
multimodal	O
and	O
hence	O
hard	O
to	O
sample	O
from	O
.	O
however	O
,	O
by	O
analogy	O
to	O
the	O
way	O
metals	O
are	O
heated	O
up	O
and	O
then	O
cooled	O
down	O
in	O
order	O
to	O
make	O
the	O
molecules	O
align	O
,	O
we	O
can	O
imagine	O
using	O
a	O
computational	O
temperature	O
parameter	B
to	O
smooth	O
out	O
a	O
distribution	O
,	O
gradually	O
cooling	O
it	O
to	O
recover	O
the	O
original	O
“	O
bumpy	O
”	O
distribution	O
.	O
we	O
ﬁrst	O
explain	O
this	O
idea	O
in	O
more	O
detail	O
in	O
the	O
context	O
of	O
an	O
algorithm	O
for	O
map	O
estimation	O
.	O
we	O
then	O
discuss	O
extensions	O
to	O
the	O
sampling	O
case	O
.	O
24.6.	O
annealing	B
methods	O
869	O
temp	O
1.000	O
temp	O
0.200	O
10	O
8	O
6	O
4	O
2	O
0	O
60	O
x	O
105	O
10	O
8	O
6	O
4	O
2	O
0	O
60	O
40	O
20	O
y	O
0	O
0	O
10	O
20	O
x	O
(	O
a	O
)	O
50	O
40	O
30	O
40	O
50	O
40	O
30	O
20	O
y	O
0	O
0	O
10	O
20	O
x	O
(	O
b	O
)	O
figure	O
24.17	O
an	O
energy	O
surface	O
at	O
different	O
temperatures	O
.	O
note	O
the	O
different	O
vertical	O
scales	O
.	O
(	O
b	O
)	O
t	O
=	O
0.5.	O
figure	O
generated	O
by	O
sademopeaks	O
.	O
(	O
a	O
)	O
t	O
=	O
1	O
.	O
24.6.1	O
simulated	B
annealing	I
simulated	O
annealing	B
(	O
kirkpatrick	O
et	O
al	O
.	O
1983	O
)	O
is	O
a	O
stochastic	B
algorithm	I
that	O
attempts	O
to	O
ﬁnd	O
the	O
global	O
optimum	O
of	O
a	O
black-box	B
function	O
f	O
(	O
x	O
)	O
.	O
it	O
is	O
closely	O
related	O
to	O
the	O
metropolis-	O
hastings	O
algorithm	O
for	O
generating	O
samples	B
from	O
a	O
probability	O
distribution	O
,	O
which	O
we	O
discussed	O
in	O
section	O
24.3.	O
sa	O
can	O
be	O
used	O
for	O
both	O
discrete	B
and	O
continuous	O
optimization	B
.	O
the	O
method	O
is	O
inspired	O
by	O
statistical	O
physics	O
.	O
the	O
key	O
quantity	O
is	O
the	O
boltzmann	O
distribution	O
,	O
which	O
speciﬁes	O
that	O
the	O
probability	O
of	O
being	O
in	O
any	O
particular	O
state	B
x	O
is	O
given	O
by	O
p	O
(	O
x	O
)	O
∝	O
exp	O
(	O
−f	O
(	O
x	O
)	O
/t	O
)	O
(	O
24.100	O
)	O
where	O
f	O
(	O
x	O
)	O
is	O
the	O
“	O
energy	O
”	O
of	O
the	O
system	O
and	O
t	O
is	O
the	O
computational	O
temperature	O
.	O
as	O
the	O
temperature	B
approaches	O
0	O
(	O
so	O
the	O
system	O
is	O
cooled	O
)	O
,	O
the	O
system	O
spends	O
more	O
and	O
more	O
time	O
in	O
its	O
minimum	O
energy	O
(	O
most	O
probable	O
)	O
state	B
.	O
figure	O
24.17	O
gives	O
an	O
example	O
of	O
a	O
2d	O
function	O
at	O
different	O
temperatures	O
.	O
at	O
high	O
temperatures	O
,	O
t	O
(	O
cid:30	O
)	O
1	O
,	O
the	O
surface	O
is	O
approximately	O
ﬂat	O
,	O
and	O
hence	O
it	O
is	O
easy	O
to	O
move	O
around	O
(	O
i.e.	O
,	O
to	O
avoid	O
local	O
optima	O
)	O
.	O
as	O
the	O
temperature	B
cools	O
,	O
the	O
largest	O
peaks	O
become	O
larger	O
,	O
and	O
the	O
smallest	O
peaks	O
disappear	O
.	O
by	O
cooling	O
slowly	O
enough	O
,	O
it	O
is	O
possible	O
to	O
“	O
track	O
”	O
the	O
largest	O
peak	O
,	O
and	O
thus	O
ﬁnd	O
the	O
global	O
optimum	O
.	O
this	O
is	O
an	O
example	O
of	O
a	O
continuation	B
method	I
.	O
we	O
can	O
generate	O
an	O
algorithm	O
from	O
this	O
as	O
follows	O
.	O
at	O
each	O
step	O
,	O
sample	O
a	O
new	O
state	B
according	O
to	O
some	O
proposal	B
distribution	I
x	O
(	O
cid:2	O
)	O
∼	O
q	O
(	O
·|xk	O
)	O
.	O
for	O
real-valued	O
parameters	O
,	O
this	O
is	O
often	O
simply	O
a	O
=	O
xk	O
+	O
k	O
,	O
where	O
k	O
∼	O
n	O
(	O
0	O
,	O
σ	O
)	O
.	O
for	O
discrete	B
optimization	O
,	O
other	O
random	B
walk	I
proposal	I
,	O
x	O
(	O
cid:2	O
)	O
kinds	O
of	O
local	O
moves	O
must	O
be	O
deﬁned	O
.	O
having	O
proposed	O
a	O
new	O
state	B
,	O
we	O
compute	O
α	O
=	O
exp	O
(	O
(	O
f	O
(	O
x	O
)	O
−	O
f	O
(	O
x	O
(	O
cid:2	O
)	O
)	O
)	O
/t	O
)	O
(	O
24.101	O
)	O
we	O
then	O
accept	B
the	O
new	O
state	B
(	O
i.e.	O
,	O
set	O
xk+1	O
=	O
x	O
(	O
cid:2	O
)	O
)	O
with	O
probability	O
min	O
(	O
1	O
,	O
α	O
)	O
,	O
otherwise	O
we	O
stay	O
in	O
the	O
current	O
state	B
(	O
i.e.	O
,	O
set	O
xk+1	O
=	O
xk	O
)	O
.	O
this	O
means	O
that	O
if	O
the	O
new	O
state	B
has	O
lower	O
energy	O
(	O
is	O
more	O
probable	O
)	O
,	O
we	O
will	O
deﬁnitely	O
accept	B
it	O
,	O
but	O
it	O
it	O
has	O
higher	O
energy	O
(	O
is	O
less	O
probable	O
)	O
,	O
we	O
might	O
still	O
accept	B
,	O
depending	O
on	O
the	O
current	O
temperature	B
.	O
thus	O
the	O
algorithm	O
allows	O
“	O
down-hill	O
”	O
moves	O
in	O
probability	O
space	O
(	O
up-hill	O
in	O
energy	O
space	O
)	O
,	O
but	O
less	O
frequently	O
as	O
the	O
temperature	B
drops	O
.	O
870	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
temperature	O
vs	O
iteration	O
energy	O
vs	O
iteration	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
200	O
400	O
800	O
1000	O
1200	O
600	O
(	O
a	O
)	O
−4.5	O
−5	O
−5.5	O
−6	O
−6.5	O
−7	O
−7.5	O
−8	O
−8.5	O
0	O
200	O
400	O
600	O
800	O
1000	O
(	O
b	O
)	O
figure	O
24.18	O
a	O
run	O
of	O
simulated	B
annealing	I
on	O
the	O
energy	O
surface	O
in	O
figure	O
24.17.	O
iteration	O
.	O
(	O
b	O
)	O
energy	O
vs	O
iteration	O
.	O
figure	O
generated	O
by	O
sademopeaks	O
.	O
(	O
a	O
)	O
temperature	B
vs	O
iter	O
550	O
,	O
temp	O
0.064	O
iter	O
1000	O
,	O
temp	O
0.007	O
60	O
50	O
40	O
30	O
20	O
10	O
0	O
150	O
100	O
50	O
0	O
30	O
28	O
26	O
24	O
y	O
22	O
36	O
35	O
37	O
x	O
40	O
39	O
38	O
30	O
28	O
26	O
24	O
y	O
22	O
36	O
35	O
37	O
x	O
40	O
39	O
38	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
24.19	O
histogram	B
of	O
samples	B
from	O
the	O
annealed	O
“	O
posterior	O
”	O
at	O
2	O
different	O
time	O
points	O
produced	O
by	O
simulated	B
annealing	I
on	O
the	O
energy	O
surface	O
shown	O
in	O
figure	O
24.17.	O
note	O
that	O
at	O
cold	O
temperatures	O
,	O
most	O
of	O
the	O
samples	B
are	O
concentrated	O
near	O
the	O
peak	O
at	O
(	O
38,25	O
)	O
.	O
figure	O
generated	O
by	O
sademopeaks	O
.	O
the	O
rate	B
at	O
which	O
the	O
temperature	B
changes	O
over	O
time	O
is	O
called	O
the	O
cooling	B
schedule	I
.	O
it	O
has	O
been	O
shown	O
(	O
kirkpatrick	O
et	O
al	O
.	O
1983	O
)	O
that	O
if	O
one	O
cools	O
sufficiently	O
slowly	O
,	O
the	O
algorithm	O
will	O
provably	O
ﬁnd	O
the	O
global	O
optimum	O
.	O
however	O
,	O
it	O
is	O
not	O
clear	O
what	O
“	O
sufficient	O
slowly	O
”	O
means	O
.	O
in	O
practice	O
it	O
is	O
common	O
to	O
use	O
an	O
exponential	B
cooling	I
schedule	I
of	O
the	O
following	O
form	O
:	O
tk	O
=	O
t0c	O
k	O
,	O
where	O
t0	O
is	O
the	O
initial	O
temperature	B
(	O
often	O
t0	O
∼	O
1	O
)	O
and	O
c	O
is	O
the	O
cooling	O
rate	O
(	O
often	O
c	O
∼	O
0.8	O
)	O
.	O
see	O
figure	O
24.18	O
(	O
a	O
)	O
for	O
a	O
plot	O
of	O
this	O
cooling	B
schedule	I
.	O
cooling	O
too	O
quickly	O
means	O
one	O
can	O
get	O
stuck	O
in	O
a	O
local	O
maximum	O
,	O
but	O
cooling	O
too	O
slowly	O
just	O
wastes	O
time	O
.	O
the	O
best	O
cooling	B
schedule	I
is	O
difficult	O
to	O
determine	O
;	O
this	O
is	O
one	O
of	O
the	O
main	O
drawbacks	O
of	O
simulated	B
annealing	I
.	O
figure	O
24.18	O
(	O
b	O
)	O
shows	O
an	O
example	O
of	O
simulated	B
annealing	I
applied	O
to	O
the	O
function	O
in	O
figure	O
24.17	O
using	O
a	O
random	B
walk	I
proposal	I
.	O
we	O
see	O
that	O
the	O
method	O
stochastically	O
reduces	O
the	O
energy	O
over	O
time	O
.	O
figures	O
24.19	O
illustrate	O
(	O
a	O
histogram	B
of	O
)	O
samples	B
drawn	O
from	O
the	O
cooled	O
probability	O
distribution	O
over	O
time	O
.	O
we	O
see	O
that	O
most	O
of	O
the	O
samples	B
are	O
concentrated	O
near	O
the	O
global	O
maximum	O
.	O
when	O
the	O
algorithm	O
has	O
converged	O
,	O
we	O
just	O
return	O
the	O
largest	O
value	O
found	O
.	O
24.6.	O
annealing	B
methods	O
871	O
24.6.2	O
annealed	B
importance	I
sampling	I
we	O
now	O
describe	O
a	O
method	O
known	O
as	O
annealed	B
importance	I
sampling	I
(	O
neal	O
2001	O
)	O
that	O
com-	O
bines	O
ideas	O
from	O
simulated	B
annealing	I
and	O
importance	B
sampling	I
in	O
order	O
to	O
draw	O
independent	O
samples	O
from	O
difficult	O
(	O
e.g.	O
,	O
multimodal	O
)	O
distributions	O
.	O
suppose	O
we	O
want	O
to	O
sample	O
from	O
p0	O
(	O
x	O
)	O
∝	O
f0	O
(	O
x	O
)	O
,	O
but	O
we	O
can	O
not	O
do	O
so	O
easily	O
;	O
for	O
example	O
,	O
this	O
might	O
represent	O
a	O
multimodal	O
posterior	O
.	O
suppose	O
however	O
that	O
there	O
is	O
an	O
easier	O
distribution	O
which	O
we	O
can	O
sample	O
from	O
,	O
call	O
it	O
pn	O
(	O
x	O
)	O
∝	O
fn	O
(	O
x	O
)	O
;	O
for	O
example	O
,	O
this	O
might	O
be	O
the	O
prior	O
.	O
we	O
can	O
now	O
construct	O
a	O
sequence	O
of	O
intermediate	O
distributions	O
than	O
move	O
slowly	O
from	O
pn	O
to	O
p0	O
as	O
follows	O
:	O
fj	O
(	O
x	O
)	O
=	O
f0	O
(	O
x	O
)	O
βj	O
fn	O
(	O
x	O
)	O
1−βj	O
(	O
24.102	O
)	O
where	O
1	O
=β	O
0	O
>	O
β1	O
>	O
···	O
>	O
βn	O
=	O
0	O
,	O
where	O
βj	O
is	O
an	O
inverse	O
temperature	O
.	O
(	O
contrast	O
this	O
to	O
the	O
scheme	O
used	O
by	O
simulated	B
annealing	I
which	O
has	O
the	O
form	O
fj	O
(	O
x	O
)	O
=f	O
0	O
(	O
x	O
)	O
βj	O
;	O
this	O
makes	O
it	O
hard	O
to	O
sample	O
from	O
pn	O
.	O
)	O
furthermore	O
,	O
suppose	O
we	O
have	O
a	O
series	O
of	O
markov	O
chains	O
tj	O
(	O
x	O
,	O
x	O
(	O
cid:2	O
)	O
)	O
(	O
from	O
x	O
to	O
x	O
(	O
cid:2	O
)	O
)	O
which	O
leave	O
each	O
pj	O
invariant	B
.	O
given	O
this	O
,	O
we	O
can	O
sample	O
x	O
from	O
p0	O
by	O
ﬁrst	O
sampling	O
a	O
sequence	O
z	O
=	O
(	O
zn−1	O
,	O
.	O
.	O
.	O
,	O
z0	O
)	O
as	O
follows	O
:	O
sample	O
zn−1	O
∼	O
pn	O
;	O
sample	O
zn−2	O
∼	O
tn−1	O
(	O
zn−1	O
,	O
·	O
)	O
;	O
...	O
;	O
sample	O
z0	O
∼	O
t1	O
(	O
z1	O
,	O
·	O
)	O
.	O
finally	O
we	O
set	O
x	O
=	O
z0	O
and	O
give	O
it	O
weight	O
w	O
=	O
fn−1	O
(	O
zn−1	O
)	O
fn	O
(	O
zn−1	O
)	O
fn−2	O
(	O
zn−2	O
)	O
fn−1	O
(	O
zn−2	O
)	O
···	O
f1	O
(	O
z1	O
)	O
f2	O
(	O
z1	O
)	O
f0	O
(	O
z0	O
)	O
f1	O
(	O
z0	O
)	O
(	O
24.103	O
)	O
this	O
can	O
be	O
shown	O
to	O
be	O
correct	O
by	O
viewing	O
the	O
algorithm	O
as	O
a	O
form	O
of	O
importance	B
sampling	I
in	O
an	O
extended	O
state	O
space	O
z	O
=	O
(	O
z0	O
,	O
.	O
.	O
.	O
,	O
zn−1	O
)	O
.	O
consider	O
the	O
following	O
distribution	O
on	O
this	O
state	B
space	I
:	O
p	O
(	O
z	O
)	O
∝	O
f	O
(	O
z	O
)	O
=	O
f0	O
(	O
z0	O
)	O
˜t1	O
(	O
z0	O
,	O
z1	O
)	O
˜t2	O
(	O
z1	O
,	O
z2	O
)	O
···	O
˜tn−1	O
(	O
zn−2	O
,	O
zn−1	O
)	O
(	O
24.104	O
)	O
where	O
˜tj	O
is	O
the	O
reversal	O
of	O
tj	O
:	O
)	O
=	O
tj	O
(	O
z	O
(	O
cid:2	O
)	O
,	O
z	O
)	O
pj	O
(	O
z	O
(	O
cid:2	O
)	O
˜tj	O
(	O
z	O
,	O
z	O
(	O
cid:2	O
)	O
(	O
cid:7	O
)	O
)	O
/pj	O
(	O
z	O
)	O
=	O
tj	O
(	O
z	O
(	O
cid:2	O
)	O
,	O
z	O
)	O
fj	O
(	O
z	O
(	O
cid:2	O
)	O
f	O
(	O
z	O
)	O
=	O
f0	O
(	O
z0	O
)	O
,	O
so	O
we	O
can	O
safely	O
just	O
use	O
the	O
z0	O
part	O
of	O
these	O
)	O
/fj	O
(	O
z	O
)	O
(	O
24.105	O
)	O
it	O
is	O
clear	O
that	O
sequences	O
to	O
recover	O
the	O
original	O
ditribution	O
.	O
z1	O
,	O
...	O
,	O
zn−1	O
now	O
consider	O
the	O
proposal	B
distribution	I
deﬁned	O
by	O
the	O
algorithm	O
:	O
q	O
(	O
z	O
)	O
∝	O
g	O
(	O
z	O
)	O
=	O
fn	O
(	O
zn−1	O
)	O
tn−1	O
(	O
zn−1	O
,	O
zn−2	O
)	O
···t	O
2	O
(	O
z2	O
,	O
z1	O
)	O
t1	O
(	O
z1	O
,	O
z0	O
)	O
(	O
24.106	O
)	O
one	O
can	O
show	O
that	O
the	O
importance	B
weights	I
w	O
=	O
f	O
(	O
z0	O
,	O
...	O
,	O
zn−1	O
)	O
g	O
(	O
z0	O
,	O
...	O
,	O
zn−1	O
)	O
are	O
given	O
by	O
equation	O
24.103	O
.	O
24.6.3	O
parallel	B
tempering	I
another	O
way	O
to	O
combine	O
mcmc	O
and	O
annealing	B
is	O
to	O
run	O
multiple	O
chains	O
in	O
parallel	O
at	O
different	O
temperatures	O
,	O
and	O
allow	O
one	O
chain	O
to	O
sample	O
from	O
another	O
chain	O
at	O
a	O
neighboring	O
temperature	B
.	O
in	O
this	O
way	O
,	O
the	O
high	O
temperature	O
chain	O
can	O
make	O
long	O
distance	O
moves	O
through	O
the	O
state	B
space	I
,	O
and	O
have	O
this	O
inﬂuence	O
lower	O
temperature	B
chains	O
.	O
this	O
is	O
known	O
as	O
parallel	B
tempering	I
.	O
see	O
e.g.	O
,	O
(	O
earl	O
and	O
deem	O
2005	O
)	O
for	O
details	O
.	O
872	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
24.7	O
approximating	O
the	O
marginal	B
likelihood	I
the	O
marginal	B
likelihood	I
p	O
(	O
d|m	O
)	O
is	O
a	O
key	O
quantity	O
for	O
bayesian	O
model	B
selection	I
,	O
and	O
is	O
given	O
by	O
p	O
(	O
d|m	O
)	O
=	O
p	O
(	O
d|θ	O
,	O
m	O
)	O
p	O
(	O
θ|m	O
)	O
dθ	O
(	O
24.107	O
)	O
(	O
cid:12	O
)	O
unfortunately	O
,	O
this	O
integral	O
is	O
often	O
intractable	O
to	O
compute	O
,	O
for	O
example	O
if	O
we	O
have	O
non	O
conjugate	O
in	O
this	O
section	O
,	O
we	O
brieﬂy	O
discuss	O
some	O
ways	O
to	O
priors	O
,	O
and/or	O
we	O
have	O
hidden	B
variables	I
.	O
approximate	O
this	O
expression	O
using	O
monte	O
carlo	O
.	O
see	O
(	O
gelman	O
and	O
meng	O
1998	O
)	O
for	O
a	O
more	O
extensive	O
review	O
.	O
24.7.1	O
the	O
candidate	O
method	O
there	O
is	O
a	O
simple	O
method	O
for	O
approximating	O
the	O
marginal	B
likelihood	I
known	O
as	O
the	O
candidate	O
method	O
(	O
chib	O
1995	O
)	O
.	O
this	O
exploits	O
the	O
following	O
identity	O
:	O
p	O
(	O
d|m	O
)	O
=	O
p	O
(	O
d|θ	O
,	O
m	O
)	O
p	O
(	O
θ|m	O
)	O
p	O
(	O
θ|d	O
,	O
m	O
)	O
(	O
24.108	O
)	O
this	O
holds	O
for	O
any	O
value	O
of	O
θ.	O
once	O
we	O
have	O
picked	O
some	O
value	O
,	O
we	O
can	O
evaluate	O
p	O
(	O
d|θ	O
,	O
m	O
)	O
and	O
p	O
(	O
θ|m	O
)	O
quite	O
easily	O
.	O
if	O
we	O
have	O
some	O
estimate	O
of	O
the	O
posterior	O
near	O
θ	O
,	O
we	O
can	O
then	O
evaluate	O
the	O
denominator	O
as	O
well	O
.	O
this	O
posterior	O
is	O
often	O
approximated	O
using	O
mcmc	O
.	O
the	O
ﬂaw	O
with	O
this	O
method	O
is	O
that	O
it	O
relies	O
on	O
the	O
assumption	O
that	O
p	O
(	O
θ|d	O
,	O
m	O
)	O
has	O
marginalized	O
over	O
all	O
the	O
modes	O
of	O
the	O
posterior	O
,	O
which	O
in	O
practice	O
is	O
rarely	O
possible	O
.	O
consequently	O
the	O
method	O
can	O
give	O
very	O
inaccurate	O
results	O
in	O
practice	O
(	O
neal	O
1998	O
)	O
.	O
24.7.2	O
harmonic	B
mean	I
estimate	O
newton	O
and	O
raftery	O
(	O
1994	O
)	O
proposed	O
a	O
simple	O
method	O
for	O
approximating	O
p	O
(	O
d	O
)	O
using	O
the	O
output	O
of	O
mcmc	O
,	O
as	O
follows	O
:	O
s	O
(	O
cid:4	O
)	O
1	O
1/p	O
(	O
d	O
)	O
≈	O
1	O
s	O
s=1	O
(	O
24.109	O
)	O
where	O
θs	O
∼	O
p	O
(	O
θ|d	O
)	O
.	O
this	O
expression	O
is	O
the	O
harmonic	B
mean	I
of	O
the	O
likelihood	B
of	O
the	O
data	O
under	O
(	O
cid:12	O
)	O
each	O
sample	O
.	O
the	O
theoretical	O
correctness	O
of	O
this	O
expression	O
follows	O
from	O
the	O
following	O
identity	O
:	O
p	O
(	O
d|θs	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
1	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ|d	O
)	O
dθ	O
=	O
1	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
d|θ	O
)	O
p	O
(	O
θ	O
)	O
p	O
(	O
d	O
)	O
dθ	O
=	O
1	O
p	O
(	O
d	O
)	O
p	O
(	O
θ|d	O
)	O
dθ	O
=	O
1	O
p	O
(	O
d	O
)	O
(	O
24.110	O
)	O
unfortunately	O
,	O
in	O
practice	O
this	O
method	O
works	O
very	O
poorly	O
.	O
indeed	O
,	O
radford	O
neal	O
called	O
this	O
“	O
the	O
worst	O
monte	O
carlo	O
method	O
ever	O
”	O
.6	O
.	O
the	O
reason	O
it	O
is	O
so	O
bad	O
is	O
that	O
it	O
depends	O
only	O
on	O
samples	B
drawn	O
from	O
the	O
posterior	O
.	O
but	O
the	O
posterior	O
is	O
often	O
very	O
insensitive	O
to	O
the	O
prior	O
,	O
whereas	O
the	O
marginal	B
likelihood	I
is	O
not	O
.	O
we	O
only	O
mention	O
this	O
method	O
in	O
order	O
to	O
warn	O
against	O
its	O
use	O
.	O
we	O
present	O
a	O
better	O
method	O
below	O
.	O
6.	O
source	O
:	O
radfordneal.wordpress.com/2008/08/17/the-harmonic-mean-of-the-likelihood-worst-mon	O
te-carlo-method-ever	O
.	O
24.7.	O
approximating	O
the	O
marginal	B
likelihood	I
873	O
24.7.3	O
annealed	B
importance	I
sampling	I
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
f	O
(	O
z	O
)	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
z0	O
zn	O
=	O
f	O
(	O
z	O
)	O
dz	O
g	O
(	O
z	O
)	O
dz	O
=	O
g	O
(	O
z	O
)	O
g	O
(	O
z	O
)	O
dz	O
g	O
(	O
z	O
)	O
dz	O
=	O
eq	O
we	O
can	O
use	O
annealed	B
importance	I
sampling	I
(	O
section	O
24.6.2	O
)	O
to	O
evaluate	O
a	O
ratio	O
of	O
partition	O
functions	O
.	O
notice	O
that	O
z0	O
=	O
g	O
(	O
z	O
)	O
dz	O
.	O
hence	O
f	O
(	O
z	O
)	O
dz	O
,	O
and	O
zn	O
=	O
fn	O
(	O
x	O
)	O
dx	O
=	O
f0	O
(	O
x	O
)	O
dx	O
=	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
f	O
(	O
z	O
)	O
g	O
(	O
z	O
)	O
≈	O
1	O
s	O
s	O
(	O
cid:4	O
)	O
s=1	O
ws	O
(	O
24.111	O
)	O
if	O
fn	O
is	O
a	O
prior	O
and	O
f0	O
is	O
the	O
posterior	O
,	O
we	O
can	O
estimate	O
zn	O
=	O
p	O
(	O
d	O
)	O
using	O
the	O
above	O
equation	O
,	O
provided	O
the	O
prior	O
has	O
a	O
known	O
normalization	O
constant	O
z0	O
.	O
this	O
is	O
generally	O
considered	O
the	O
method	O
of	O
choice	O
for	O
evaluating	O
difficult	O
partition	O
functions	O
.	O
exercises	O
exercise	O
24.1	O
gibbs	O
sampling	O
from	O
a	O
2d	O
gaussian	O
suppose	O
x	O
∼	O
n	O
(	O
μ	O
,	O
σ	O
)	O
,	O
where	O
μ	O
=	O
(	O
1	O
,	O
1	O
)	O
and	O
σ	O
=	O
(	O
1	O
,	O
−0.5	O
;	O
−0.5	O
,	O
1	O
)	O
.	O
derive	O
the	O
full	B
condition-	O
als	O
p	O
(	O
x1|x2	O
)	O
and	O
p	O
(	O
x2|x1	O
)	O
.	O
implement	O
the	O
algorithm	O
and	O
plot	O
the	O
1d	O
marginals	O
p	O
(	O
x1	O
)	O
and	O
p	O
(	O
x2	O
)	O
as	O
histograms	O
.	O
superimpose	O
a	O
plot	O
of	O
the	O
exact	O
marginals	O
.	O
exercise	O
24.2	O
gibbs	O
sampling	O
for	O
a	O
1d	O
gaussian	O
mixture	B
model	I
consider	O
applying	O
gibbs	O
sampling	O
to	O
a	O
univariate	O
mixture	O
of	O
gaussians	O
,	O
as	O
in	O
section	O
24.2.3.	O
derive	O
the	O
expressions	O
for	O
the	O
full	B
conditionals	O
.	O
hint	O
:	O
if	O
we	O
know	O
zn	O
=	O
j	O
(	O
say	O
)	O
,	O
then	O
μj	O
gets	O
“	O
connected	O
”	O
to	O
xn	O
,	O
but	O
all	O
other	O
values	O
of	O
μi	O
,	O
for	O
all	O
i	O
(	O
cid:13	O
)	O
=	O
j	O
,	O
are	O
irrelevant	O
.	O
(	O
this	O
is	O
an	O
example	O
of	O
context-speciﬁc	B
independence	I
,	O
where	O
the	O
structure	O
of	O
the	O
graph	B
simpliﬁes	O
once	O
we	O
have	O
assigned	O
values	O
to	O
some	O
of	O
the	O
nodes	B
.	O
)	O
hence	O
,	O
given	O
all	O
the	O
zn	O
values	O
,	O
the	O
posteriors	O
of	O
the	O
μ	O
’	O
s	O
should	O
be	O
independent	O
,	O
so	O
the	O
conditional	O
of	O
μj	O
should	O
be	O
independent	O
of	O
μ−j	O
.	O
(	O
similarly	O
for	O
σj	O
.	O
)	O
exercise	O
24.3	O
gibbs	O
sampling	O
from	O
the	O
potts	O
model	O
modify	O
the	O
code	O
in	O
gibbsdemoising	O
to	O
draw	O
samples	B
from	O
a	O
potts	O
prior	O
at	O
different	O
temperatures	O
,	O
as	O
in	O
figure	O
19.8.	O
exercise	O
24.4	O
full	B
conditionals	O
for	O
hierarchical	O
model	O
of	O
gaussian	O
means	O
let	O
us	O
reconsider	O
the	O
gaussian-gaussian	O
model	O
parameters	O
θj	O
.	O
2009	O
,	O
p134	O
)	O
)	O
,	O
that	O
we	O
use	O
the	O
following	O
conjugate	B
priors	I
on	O
the	O
hyper-parameters	B
:	O
in	O
section	O
5.6.2	O
for	O
modelling	O
multiple	O
related	O
mean	B
in	O
this	O
exercise	O
we	O
derive	O
a	O
gibbs	O
sampler	O
instead	O
of	O
using	O
eb	O
.	O
suppose	O
,	O
following	O
(	O
hoff	O
μ	O
∼	O
n	O
(	O
μ0	O
,	O
γ2	O
0	O
)	O
τ	O
2	O
∼	O
ig	O
(	O
η0/2	O
,	O
η0τ	O
2	O
σ2	O
∼	O
ig	O
(	O
ν0/2	O
,	O
ν0σ2	O
0	O
/2	O
)	O
0/2	O
)	O
(	O
24.112	O
)	O
(	O
24.113	O
)	O
(	O
24.114	O
)	O
874	O
chapter	O
24.	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
inference	B
we	O
can	O
set	O
η	O
=	O
(	O
μ0	O
,	O
γ0	O
,	O
η0	O
,	O
τ0	O
,	O
ν0	O
,	O
σ0	O
)	O
to	O
uninformative	B
values	O
.	O
given	O
this	O
model	O
speciﬁcation	O
,	O
show	O
that	O
the	O
full	B
conditionals	O
for	O
μ	O
,	O
τ	O
,	O
σ	O
and	O
the	O
θj	O
are	O
as	O
follows	O
:	O
p	O
(	O
μ|θ1	O
:	O
d	O
,	O
τ	O
2	O
)	O
=n	O
(	O
μ|	O
dθ/τ	O
2	O
+	O
μ0/γ2	O
d/τ	O
2	O
+	O
1/γ2	O
0	O
0	O
,	O
[	O
d/τ	O
2	O
+	O
1/γ2	O
0	O
]	O
−1	O
)	O
p	O
(	O
θj|μ	O
,	O
τ	O
2	O
,	O
dj	O
,	O
σ2	O
)	O
=n	O
(	O
θj|	O
njxj/σ2	O
+	O
1/τ	O
2	O
nj/σ2	O
+	O
1/τ	O
2	O
η0τ	O
2	O
0	O
+	O
p	O
(	O
τ	O
2|θ1	O
:	O
d	O
,	O
μ	O
)	O
=	O
ig	O
(	O
τ	O
2|	O
η0	O
+	O
d	O
,	O
(	O
cid:2	O
)	O
,	O
[	O
nj/σ2	O
+	O
1/τ	O
2	O
]	O
−1	O
)	O
p	O
(	O
σ2|θ1	O
:	O
d	O
,	O
d	O
)	O
=	O
ig	O
(	O
σ2|	O
1	O
2	O
[	O
ν0	O
+	O
nj	O
]	O
,	O
1	O
2	O
2	O
d	O
(	O
cid:12	O
)	O
j=1	O
j	O
(	O
θj	O
−	O
μ	O
)	O
2	O
d	O
(	O
cid:12	O
)	O
2	O
[	O
ν0σ2	O
0	O
+	O
)	O
nj	O
(	O
cid:12	O
)	O
(	O
xij	O
−	O
θj	O
)	O
2	O
]	O
)	O
j=1	O
i=1	O
(	O
24.115	O
)	O
(	O
24.116	O
)	O
(	O
24.117	O
)	O
(	O
24.118	O
)	O
exercise	O
24.5	O
gibbs	O
sampling	O
for	O
robust	B
linear	O
regression	B
with	O
a	O
student	O
t	O
likelihood	O
modify	O
the	O
em	O
algorithm	O
in	O
exercise	O
11.12	O
to	O
perform	O
gibbs	O
sampling	O
for	O
p	O
(	O
w	O
,	O
σ2	O
,	O
z|d	O
,	O
ν	O
)	O
.	O
exercise	O
24.6	O
gibbs	O
sampling	O
for	O
probit	B
regression	I
modify	O
the	O
em	O
algorithm	O
in	O
section	O
11.4.6	O
to	O
perform	O
gibbs	O
sampling	O
for	O
p	O
(	O
w	O
,	O
z|d	O
)	O
.	O
hint	O
:	O
we	O
can	O
sample	O
from	O
a	O
truncated	O
gaussian	O
,	O
n	O
(	O
z|μ	O
,	O
σ	O
)	O
i	O
(	O
a	O
≤	O
z	O
≤	O
b	O
)	O
in	O
two	O
steps	O
:	O
ﬁrst	O
sample	O
u	O
∼	O
u	O
(	O
φ	O
(	O
(	O
a	O
−	O
μ	O
)	O
/σ	O
)	O
,	O
φ	O
(	O
(	O
b	O
−	O
μ	O
)	O
/σ	O
)	O
)	O
,	O
then	O
set	O
z	O
=	O
μ	O
+	O
σφ	O
exercise	O
24.7	O
gibbs	O
sampling	O
for	O
logistic	B
regression	I
with	O
the	O
student	O
approximation	O
derive	O
the	O
full	B
conditionals	O
for	O
the	O
joint	O
model	O
deﬁned	O
by	O
equations	O
24.88	O
to	O
24.91	O
.	O
−1	O
(	O
u	O
)	O
(	O
robert	O
1995	O
)	O
.	O
25	O
clustering	B
25.1	O
introduction	O
clustering	B
is	O
the	O
process	O
of	O
grouping	O
similar	O
objects	O
together	O
.	O
there	O
are	O
two	O
kinds	O
of	O
inputs	O
we	O
might	O
use	O
.	O
in	O
similarity-based	B
clustering	I
,	O
the	O
input	O
to	O
the	O
algorithm	O
is	O
an	O
n×n	O
dissimilarity	B
matrix	I
or	O
distance	B
matrix	I
d.	O
in	O
feature-based	B
clustering	I
,	O
the	O
input	O
to	O
the	O
algorithm	O
is	O
an	O
n	O
×	O
d	O
feature	B
matrix	I
or	O
design	B
matrix	I
x.	O
similarity-based	B
clustering	I
has	O
the	O
advantage	O
that	O
it	O
allows	O
for	O
easy	O
inclusion	O
of	O
domain-speciﬁc	O
similarity	O
or	O
kernel	B
functions	O
(	O
section	O
14.2	O
)	O
.	O
feature-	O
based	O
clustering	B
has	O
the	O
advantage	O
that	O
it	O
is	O
applicable	O
to	O
“	O
raw	O
”	O
,	O
potentially	O
noisy	O
data	O
.	O
we	O
will	O
see	O
examples	O
of	O
both	O
below	O
.	O
in	O
addition	O
to	O
the	O
two	O
types	O
of	O
input	O
,	O
there	O
are	O
two	O
possible	O
types	O
of	O
output	O
:	O
ﬂat	O
cluster-	O
ing	O
,	O
also	O
called	O
partitional	B
clustering	I
,	O
where	O
we	O
partition	O
the	O
objects	O
into	O
disjoint	O
sets	O
;	O
and	O
hierarchical	B
clustering	I
,	O
where	O
we	O
create	O
a	O
nested	O
tree	O
of	O
partitions	O
.	O
we	O
will	O
discuss	O
both	O
of	O
these	O
below	O
.	O
not	O
surprisingly	O
,	O
ﬂat	O
clusterings	O
are	O
usually	O
faster	O
to	O
create	O
(	O
o	O
(	O
n	O
d	O
)	O
for	O
ﬂat	O
vs	O
o	O
(	O
n	O
2	O
log	O
n	O
)	O
for	O
hierarchical	O
)	O
,	O
but	O
hierarchical	O
clusterings	O
are	O
often	O
more	O
useful	O
.	O
furthermore	O
,	O
most	O
hierarchical	B
clustering	I
algorithms	O
are	O
deterministic	O
and	O
do	O
not	O
require	O
the	O
speciﬁcation	O
of	O
k	O
,	O
the	O
number	O
of	O
clusters	B
,	O
whereas	O
most	O
ﬂat	B
clustering	I
algorithms	O
are	O
sensitive	O
to	O
the	O
initial	O
conditions	O
and	O
require	O
some	O
model	B
selection	I
method	O
for	O
k.	O
(	O
we	O
will	O
discuss	O
how	O
to	O
choose	O
k	O
in	O
more	O
detail	O
below	O
.	O
)	O
the	O
ﬁnal	O
distinction	O
we	O
will	O
make	O
in	O
this	O
chapter	O
is	O
whether	O
the	O
method	O
is	O
based	O
on	O
a	O
probabilistic	O
model	O
or	O
not	O
.	O
one	O
might	O
wonder	O
why	O
we	O
even	O
bother	O
discussing	O
non-probabilistic	O
methods	O
for	O
clustering	B
.	O
the	O
reason	O
is	O
two-fold	O
:	O
ﬁrst	O
,	O
they	O
are	O
widely	O
used	O
,	O
so	O
readers	O
should	O
know	O
about	O
them	O
;	O
second	O
,	O
they	O
often	O
contain	O
good	O
ideas	O
,	O
which	O
can	O
be	O
used	O
to	O
speed	O
up	O
inference	B
in	O
a	O
probabilistic	O
models	O
.	O
25.1.1	O
measuring	O
(	O
dis	O
)	O
similarity	O
a	O
dissimilarity	B
matrix	I
d	O
is	O
a	O
matrix	O
where	O
di	O
,	O
i	O
=	O
0	O
and	O
di	O
,	O
j	O
≥	O
0	O
is	O
a	O
measure	O
of	O
“	O
distance	O
”	O
between	O
objects	O
i	O
and	O
j.	O
subjectively	O
judged	O
dissimilarities	O
are	O
seldom	O
distances	O
in	O
the	O
strict	B
sense	O
,	O
since	O
the	O
triangle	B
inequality	I
,	O
di	O
,	O
j	O
≤	O
di	O
,	O
k	O
+	O
dj	O
,	O
k	O
,	O
often	O
does	O
not	O
hold	O
.	O
some	O
algorithms	O
require	O
d	O
to	O
be	O
a	O
true	O
distance	O
matrix	O
,	O
but	O
many	O
do	O
not	O
.	O
if	O
we	O
have	O
a	O
similarity	O
matrix	O
s	O
,	O
we	O
can	O
convert	O
it	O
to	O
a	O
dissimilarity	B
matrix	I
by	O
applying	O
any	O
monotonically	O
decreasing	O
function	O
,	O
e.g.	O
,	O
d	O
=	O
max	O
(	O
s	O
)	O
−	O
s.	O
the	O
most	O
common	O
way	O
to	O
deﬁne	O
dissimilarity	O
between	O
objects	O
is	O
in	O
terms	O
of	O
the	O
dissimilarity	O
876	O
chapter	O
25.	O
clustering	B
of	O
their	O
attributes	B
:	O
d	O
(	O
cid:4	O
)	O
δ	O
(	O
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
)	O
=	O
δj	O
(	O
xij	O
,	O
xi	O
(	O
cid:2	O
)	O
j	O
)	O
j=1	O
some	O
common	O
attribute	O
dissimilarity	O
functions	O
are	O
as	O
follows	O
:	O
•	O
squared	O
(	O
euclidean	O
)	O
distance	O
:	O
δj	O
(	O
xij	O
,	O
xi	O
(	O
cid:2	O
)	O
j	O
)	O
=	O
(	O
xij	O
−	O
xi	O
(	O
cid:2	O
)	O
j	O
)	O
2	O
(	O
25.1	O
)	O
(	O
25.2	O
)	O
of	O
course	O
,	O
this	O
only	O
makes	O
sense	O
if	O
attribute	O
j	O
is	O
real-valued	O
.	O
•	O
squared	O
distance	O
strongly	O
emphasizes	O
large	O
differences	O
(	O
because	O
differences	O
are	O
squared	O
)	O
.	O
a	O
more	O
robust	B
alternative	O
is	O
to	O
use	O
an	O
(	O
cid:2	O
)	O
1	O
distance	O
:	O
δj	O
(	O
xij	O
,	O
xi	O
(	O
cid:2	O
)	O
j	O
)	O
=	O
|xij	O
−	O
xi	O
(	O
cid:2	O
)	O
j|	O
(	O
25.3	O
)	O
this	O
is	O
also	O
called	O
city	B
block	I
distance	I
,	O
since	O
,	O
in	O
2d	O
,	O
the	O
distance	O
can	O
be	O
computed	O
by	O
counting	O
how	O
many	O
rows	O
and	O
columns	O
we	O
have	O
to	O
move	O
horizontally	O
and	O
vertically	O
to	O
get	O
from	O
xi	O
to	O
xi	O
(	O
cid:2	O
)	O
.	O
(	O
cid:7	O
)	O
•	O
if	O
xi	O
is	O
a	O
vector	O
(	O
e.g.	O
,	O
a	O
time-series	O
of	O
real-valued	O
data	O
)	O
,	O
it	O
is	O
common	O
to	O
use	O
the	O
correlation	O
j	O
xijxi	O
(	O
cid:2	O
)	O
j	O
,	O
coefficient	O
(	O
see	O
section	O
2.5.1	O
)	O
.	O
j	O
(	O
xij	O
−	O
xi	O
(	O
cid:2	O
)	O
j	O
)	O
2	O
=	O
2	O
(	O
1	O
−	O
corr	O
[	O
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
]	O
)	O
.	O
so	O
clustering	B
based	O
on	O
correlation	O
and	O
hence	O
(	O
similarity	O
)	O
is	O
equivalent	O
to	O
clustering	B
based	O
on	O
squared	O
distance	O
(	O
dissimilarity	O
)	O
.	O
if	O
the	O
data	O
is	O
standardized	B
,	O
then	O
corr	O
[	O
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
]	O
=	O
•	O
for	O
ordinal	B
variables	I
,	O
such	O
as	O
{	O
low	O
,	O
medium	O
,	O
high	O
}	O
,	O
it	O
is	O
standard	O
to	O
encode	O
the	O
values	O
as	O
real-valued	O
numbers	O
,	O
say	O
1/3	O
,	O
2/3	O
,	O
3/3	O
if	O
there	O
are	O
3	O
possible	O
values	O
.	O
one	O
can	O
then	O
apply	O
any	O
dissimilarity	O
function	O
for	O
quantitative	O
variables	O
,	O
such	O
as	O
squared	O
distance	O
.	O
•	O
for	O
categorical	B
variables	I
,	O
such	O
as	O
{	O
red	O
,	O
green	O
,	O
blue	O
}	O
,	O
we	O
usually	O
assign	O
a	O
distance	O
of	O
1	O
if	O
the	O
features	B
are	O
different	O
,	O
and	O
a	O
distance	O
of	O
0	O
otherwise	O
.	O
summing	O
up	O
over	O
all	O
the	O
categorical	B
features	O
gives	O
(	O
cid:7	O
)	O
d	O
(	O
cid:4	O
)	O
δ	O
(	O
xi	O
,	O
xi	O
)	O
=	O
i	O
(	O
xij	O
(	O
cid:8	O
)	O
=	O
xi	O
(	O
cid:2	O
)	O
j	O
)	O
(	O
25.4	O
)	O
j=1	O
this	O
is	O
called	O
the	O
hamming	B
distance	I
.	O
25.1.2	O
evaluating	O
the	O
output	O
of	O
clustering	B
methods	O
*	O
the	O
validation	O
of	O
clustering	B
structures	O
is	O
the	O
most	O
difficult	O
and	O
frustrating	O
part	O
of	O
cluster	O
analysis	O
.	O
without	O
a	O
strong	O
effort	O
in	O
this	O
direction	O
,	O
cluster	O
analysis	O
will	O
remain	O
a	O
black	O
art	O
accessible	O
only	O
to	O
those	O
true	O
believers	O
who	O
have	O
experience	O
and	O
great	O
courage	O
.	O
—	O
jain	O
and	O
dubes	O
(	O
jain	O
and	O
dubes	O
1988	O
)	O
25.1.	O
introduction	O
877	O
(	O
cid:36	O
)	O
(	O
cid:3	O
)	O
(	O
cid:36	O
)	O
(	O
cid:3	O
)	O
(	O
cid:36	O
)	O
(	O
cid:3	O
)	O
(	O
cid:36	O
)	O
(	O
cid:3	O
)	O
(	O
cid:36	O
)	O
(	O
cid:3	O
)	O
(	O
cid:37	O
)	O
(	O
cid:36	O
)	O
(	O
cid:3	O
)	O
(	O
cid:37	O
)	O
(	O
cid:3	O
)	O
(	O
cid:37	O
)	O
(	O
cid:37	O
)	O
(	O
cid:3	O
)	O
(	O
cid:37	O
)	O
(	O
cid:3	O
)	O
(	O
cid:38	O
)	O
(	O
cid:36	O
)	O
(	O
cid:3	O
)	O
(	O
cid:36	O
)	O
(	O
cid:3	O
)	O
(	O
cid:38	O
)	O
(	O
cid:3	O
)	O
(	O
cid:38	O
)	O
(	O
cid:3	O
)	O
(	O
cid:38	O
)	O
figure	O
25.1	O
three	O
clusters	B
with	O
labeled	O
objects	O
inside	O
.	O
based	O
on	O
figure	O
16.4	O
of	O
(	O
manning	O
et	O
al	O
.	O
2008	O
)	O
.	O
clustering	B
is	O
an	O
unupervised	O
learning	B
technique	O
,	O
so	O
it	O
is	O
hard	O
to	O
evaluate	O
the	O
quality	O
of	O
the	O
output	O
of	O
any	O
given	O
method	O
.	O
if	O
we	O
use	O
probabilistic	O
models	O
,	O
we	O
can	O
always	O
evaluate	O
the	O
likelihood	B
of	O
a	O
test	O
set	O
,	O
but	O
this	O
has	O
two	O
drawbacks	O
:	O
ﬁrst	O
,	O
it	O
does	O
not	O
directly	O
assess	O
any	O
clustering	B
that	O
is	O
discovered	O
by	O
the	O
model	O
;	O
and	O
second	O
,	O
it	O
does	O
not	O
apply	O
to	O
non-probabilistic	O
methods	O
.	O
so	O
now	O
we	O
discuss	O
some	O
performance	O
measures	O
not	O
based	O
on	O
likelihood	B
.	O
intuitively	O
,	O
the	O
goal	O
of	O
clustering	B
is	O
to	O
assign	O
points	O
that	O
are	O
similar	B
to	O
the	O
same	O
cluster	O
,	O
and	O
to	O
ensure	O
that	O
points	O
that	O
are	O
dissimilar	O
are	O
in	O
different	O
clusters	B
.	O
there	O
are	O
several	O
ways	O
of	O
measuring	O
these	O
quantities	O
e.g.	O
,	O
see	O
(	O
jain	O
and	O
dubes	O
1988	O
;	O
kaufman	O
and	O
rousseeuw	O
1990	O
)	O
.	O
however	O
,	O
these	O
internal	O
criteria	O
may	O
be	O
of	O
limited	O
use	O
.	O
an	O
alternative	O
is	O
to	O
rely	O
on	O
some	O
external	O
form	O
of	O
data	O
with	O
which	O
to	O
validate	O
the	O
method	O
.	O
for	O
example	O
,	O
suppose	O
we	O
have	O
labels	O
for	O
each	O
object	O
,	O
as	O
in	O
figure	O
25.1	O
.	O
(	O
equivalently	O
,	O
we	O
can	O
have	O
a	O
reference	O
clustering	B
;	O
given	O
a	O
clustering	B
,	O
we	O
can	O
induce	O
a	O
set	O
of	O
labels	O
and	O
vice	O
versa	O
.	O
)	O
then	O
we	O
can	O
compare	O
the	O
clustering	B
with	O
the	O
labels	O
using	O
various	O
metrics	O
which	O
we	O
describe	O
below	O
.	O
we	O
will	O
use	O
some	O
of	O
these	O
metrics	O
later	O
,	O
when	O
we	O
compare	O
clustering	B
methods	O
.	O
(	O
cid:7	O
)	O
c	O
25.1.2.1	O
purity	B
(	O
cid:4	O
)	O
i	O
let	O
nij	O
be	O
the	O
number	O
of	O
objects	O
in	O
cluster	O
i	O
that	O
belong	O
to	O
class	O
j	O
,	O
and	O
let	O
ni	O
=	O
j=1	O
nij	O
be	O
the	O
total	O
number	O
of	O
objects	O
in	O
cluster	O
i.	O
deﬁne	O
pij	O
=	O
nij/ni	O
;	O
this	O
is	O
the	O
empirical	B
distribution	I
over	O
class	O
labels	O
for	O
cluster	O
i.	O
we	O
deﬁne	O
the	O
purity	B
of	O
a	O
cluster	O
as	O
pi	O
(	O
cid:2	O
)	O
maxj	O
pij	O
,	O
and	O
the	O
overall	O
purity	B
of	O
a	O
clustering	B
as	O
purity	B
(	O
cid:2	O
)	O
ni	O
n	O
pi	O
(	O
25.5	O
)	O
for	O
example	O
,	O
in	O
figure	O
25.1	O
,	O
we	O
have	O
that	O
the	O
purity	B
is	O
+	O
=	O
5	O
6	O
+	O
6	O
17	O
4	O
6	O
5	O
17	O
3	O
5	O
5	O
+	O
4	O
+	O
3	O
6	O
17	O
the	O
purity	B
ranges	O
between	O
0	O
(	O
bad	O
)	O
and	O
1	O
(	O
good	O
)	O
.	O
however	O
,	O
we	O
can	O
trivially	O
achieve	O
a	O
purity	B
of	O
1	O
by	O
putting	O
each	O
object	O
into	O
its	O
own	O
cluster	O
,	O
so	O
this	O
measure	O
does	O
not	O
penalize	O
for	O
the	O
number	O
of	O
clusters	B
.	O
=	O
0.71	O
(	O
25.6	O
)	O
17	O
25.1.2.2	O
rand	O
index	O
let	O
u	O
=	O
{	O
u1	O
,	O
.	O
.	O
.	O
,	O
ur	O
}	O
and	O
v	O
=	O
{	O
v1	O
,	O
.	O
.	O
.	O
,	O
vc	O
}	O
be	O
two	O
different	O
partitions	O
of	O
the	O
n	O
data	O
points	O
,	O
i.e.	O
,	O
two	O
different	O
(	O
ﬂat	O
)	O
clusterings	O
.	O
for	O
example	O
,	O
u	O
might	O
be	O
the	O
estimated	O
clustering	O
and	O
v	O
is	O
reference	O
clustering	B
derived	O
from	O
the	O
class	O
labels	O
.	O
now	O
deﬁne	O
a	O
2	O
×	O
2	O
contingency	B
table	I
,	O
878	O
chapter	O
25.	O
clustering	B
containing	O
the	O
following	O
numbers	O
:	O
t	O
p	O
is	O
the	O
number	O
of	O
pairs	O
that	O
are	O
in	O
the	O
same	O
cluster	O
in	O
both	O
u	O
and	O
v	O
(	O
true	O
positives	O
)	O
;	O
t	O
n	O
is	O
the	O
number	O
of	O
pairs	O
that	O
are	O
in	O
the	O
different	O
clusters	B
in	O
both	O
u	O
and	O
v	O
(	O
true	O
negatives	O
)	O
;	O
f	O
n	O
is	O
the	O
number	O
of	O
pairs	O
that	O
are	O
in	O
the	O
different	O
clusters	B
in	O
u	O
but	O
the	O
same	O
cluster	O
in	O
v	O
(	O
false	O
negatives	O
)	O
;	O
and	O
f	O
p	O
is	O
the	O
number	O
of	O
pairs	O
that	O
are	O
in	O
the	O
same	O
cluster	O
in	O
u	O
but	O
different	O
clusters	B
in	O
v	O
(	O
false	O
positives	O
)	O
.	O
a	O
common	O
summary	O
statistic	O
is	O
the	O
rand	O
index	O
:	O
r	O
(	O
cid:2	O
)	O
t	O
p	O
+	O
t	O
n	O
t	O
p	O
+	O
f	O
p	O
+	O
f	O
n	O
+	O
t	O
n	O
(	O
25.7	O
)	O
this	O
can	O
be	O
interpreted	O
as	O
the	O
fraction	O
of	O
clustering	O
decisions	O
that	O
are	O
correct	O
.	O
clearly	O
0	O
≤	O
r	O
≤	O
1.	O
for	O
example	O
,	O
consider	O
figure	O
25.1	O
,	O
the	O
three	O
clusters	B
contain	O
6	O
,	O
6	O
and	O
5	O
points	O
,	O
so	O
the	O
number	O
of	O
“	O
positives	O
”	O
(	O
i.e.	O
,	O
pairs	O
of	O
objects	O
put	O
in	O
the	O
same	O
cluster	O
,	O
regardless	O
of	O
label	B
)	O
is	O
t	O
p	O
+	O
f	O
p	O
=	O
+	O
+	O
=	O
40	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
6	O
2	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
5	O
2	O
6	O
2	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
3	O
2	O
5	O
2	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
2	O
2	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
5	O
2	O
of	O
these	O
,	O
the	O
number	O
of	O
true	O
positives	O
is	O
given	O
by	O
(	O
25.8	O
)	O
(	O
25.9	O
)	O
(	O
cid:9	O
)	O
2	O
2	O
t	O
p	O
=	O
+	O
+	O
+	O
=	O
20	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
3	O
2	O
where	O
the	O
last	O
two	O
terms	O
come	O
from	O
cluster	O
3	O
:	O
there	O
are	O
pairs	O
labeled	O
a.	O
so	O
f	O
p	O
=	O
40	O
−	O
20	O
=	O
20.	O
similarly	O
,	O
one	O
can	O
show	O
f	O
n	O
=	O
24	O
and	O
t	O
n	O
=	O
72.	O
so	O
the	O
rand	O
index	O
is	O
(	O
20	O
+	O
72	O
)	O
/	O
(	O
20	O
+	O
20	O
+	O
24	O
+	O
72	O
)	O
=	O
0.68.	O
pairs	O
labeled	O
c	O
and	O
one	O
can	O
deﬁne	O
an	O
adjusted	O
rand	O
index	O
(	O
hubert	O
and	O
arabie	O
1985	O
)	O
as	O
follows	O
:	O
the	O
rand	O
index	O
only	O
achieves	O
its	O
lower	O
bound	O
of	O
0	O
if	O
t	O
p	O
=	O
t	O
n	O
=	O
0	O
,	O
which	O
is	O
a	O
rare	B
event	I
.	O
ar	O
(	O
cid:2	O
)	O
index	O
−	O
expected	O
index	O
max	O
index	O
−	O
expected	O
index	O
(	O
25.10	O
)	O
here	O
the	O
model	O
of	O
randomness	O
is	O
based	O
on	O
using	O
the	O
generalized	O
hyper-geometric	O
distribution	O
,	O
i.e.	O
,	O
the	O
two	O
partitions	O
are	O
picked	O
at	O
random	O
subject	O
to	O
having	O
the	O
original	O
number	O
of	O
classes	O
and	O
objects	O
in	O
each	O
,	O
and	O
then	O
the	O
expected	B
value	I
of	O
t	O
p	O
+	O
t	O
n	O
is	O
computed	O
.	O
this	O
model	O
can	O
be	O
used	O
to	O
compute	O
the	O
statistical	O
signiﬁcance	O
of	O
the	O
rand	O
index	O
.	O
the	O
rand	O
index	O
weights	O
false	O
positives	O
and	O
false	O
negatives	O
equally	O
.	O
various	O
other	O
summary	O
statistics	O
for	O
binary	O
decision	O
problems	O
,	O
such	O
as	O
the	O
f-score	O
(	O
section	O
5.7.2.2	O
)	O
,	O
can	O
also	O
be	O
used	O
.	O
one	O
can	O
compute	O
their	O
frequentist	B
sampling	O
distribution	O
,	O
and	O
hence	O
their	O
statistical	O
signiﬁcance	O
,	O
using	O
methods	O
such	O
as	O
bootstrap	B
.	O
25.1.2.3	O
mutual	B
information	I
another	O
way	O
to	O
measure	O
cluster	O
quality	O
is	O
to	O
compute	O
the	O
mutual	B
information	I
between	O
u	O
and	O
v	O
(	O
vaithyanathan	O
and	O
dom	O
1999	O
)	O
.	O
to	O
do	O
this	O
,	O
let	O
pu	O
v	O
(	O
i	O
,	O
j	O
)	O
=	O
be	O
the	O
probability	O
that	O
a	O
randomly	O
chosen	O
object	O
belongs	O
to	O
cluster	O
ui	O
in	O
u	O
and	O
vj	O
in	O
v	O
.	O
also	O
,	O
let	O
pu	O
(	O
i	O
)	O
=	O
|ui|/n	O
be	O
the	O
be	O
the	O
probability	O
that	O
a	O
randomly	O
chosen	O
object	O
belongs	O
to	O
cluster	O
ui	O
in	O
u	O
;	O
deﬁne	O
|ui∩vj|	O
n	O
25.2.	O
dirichlet	O
process	O
mixture	B
models	O
879	O
pv	O
(	O
j	O
)	O
=	O
|vj|/n	O
similarly	O
.	O
then	O
we	O
have	O
r	O
(	O
cid:4	O
)	O
c	O
(	O
cid:4	O
)	O
pu	O
v	O
(	O
i	O
,	O
j	O
)	O
pu	O
(	O
i	O
)	O
pv	O
(	O
j	O
)	O
i=1	O
j=1	O
i	O
(	O
u	O
,	O
v	O
)	O
=	O
pu	O
v	O
(	O
i	O
,	O
j	O
)	O
log	O
(	O
25.11	O
)	O
this	O
lies	O
between	O
0	O
and	O
min	O
{	O
h	O
(	O
u	O
)	O
,	O
h	O
(	O
v	O
)	O
}	O
.	O
unfortunately	O
,	O
the	O
maximum	O
value	O
can	O
be	O
achieved	O
by	O
using	O
lots	O
of	O
small	O
clusters	O
,	O
which	O
have	O
low	O
entropy	O
.	O
to	O
compensate	O
for	O
this	O
,	O
we	O
can	O
use	O
the	O
normalized	B
mutual	I
information	I
,	O
n	O
m	O
i	O
(	O
u	O
,	O
v	O
)	O
(	O
cid:2	O
)	O
i	O
(	O
u	O
,	O
v	O
)	O
(	O
h	O
(	O
u	O
)	O
+h	O
(	O
v	O
)	O
)	O
/2	O
(	O
25.12	O
)	O
this	O
lies	O
between	O
0	O
and	O
1.	O
a	O
version	O
of	O
this	O
that	O
is	O
adjusted	O
for	O
chance	O
(	O
under	O
a	O
particular	O
random	O
data	O
model	O
)	O
is	O
described	O
in	O
(	O
vinh	O
et	O
al	O
.	O
2009	O
)	O
.	O
another	O
variant	O
,	O
called	O
variation	B
of	I
information	I
,	O
is	O
described	O
in	O
(	O
meila	O
2005	O
)	O
.	O
25.2	O
dirichlet	O
process	O
mixture	B
models	O
the	O
simplest	O
approach	O
to	O
(	O
ﬂat	O
)	O
clustering	B
is	O
to	O
use	O
a	O
ﬁnite	B
mixture	I
model	I
,	O
as	O
we	O
discussed	O
in	O
section	O
11.2.3.	O
this	O
is	O
sometimes	O
called	O
model-based	B
clustering	I
,	O
since	O
we	O
deﬁne	O
a	O
probabilistic	O
model	O
of	O
the	O
data	O
,	O
and	O
optimize	O
a	O
well-deﬁned	O
objective	O
(	O
the	O
likelihood	B
or	O
posterior	O
)	O
,	O
as	O
opposed	O
to	O
just	O
using	O
some	O
heuristic	O
algorithm	O
.	O
the	O
principle	O
problem	O
with	O
ﬁnite	O
mixture	O
models	O
is	O
how	O
to	O
choose	O
the	O
number	O
of	O
components	O
k.	O
we	O
discussed	O
several	O
techniques	O
in	O
section	O
11.5.	O
however	O
,	O
in	O
many	O
cases	O
,	O
there	O
is	O
no	O
well-	O
deﬁned	O
number	O
of	O
clusters	B
.	O
even	O
in	O
the	O
simple	O
2d	O
height-weight	O
data	O
(	O
figure	O
1.8	O
)	O
,	O
it	O
is	O
not	O
clear	O
if	O
the	O
“	O
correct	O
”	O
value	O
of	O
k	O
should	O
be	O
2	O
,	O
3	O
,	O
or	O
4.	O
it	O
would	O
be	O
much	O
better	O
if	O
we	O
did	O
not	O
have	O
to	O
choose	O
k	O
at	O
all	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
inﬁnite	B
mixture	I
models	I
,	O
in	O
which	O
we	O
do	O
not	O
impose	O
any	O
a	O
priori	O
bound	O
on	O
k.	O
to	O
do	O
this	O
,	O
we	O
will	O
use	O
a	O
non-parametric	B
prior	I
based	O
on	O
the	O
dirichlet	O
process	O
(	O
dp	O
)	O
.	O
this	O
allows	O
the	O
number	O
of	O
clusters	B
to	O
grow	O
as	O
the	O
amount	O
of	O
data	O
increases	O
.	O
it	O
will	O
also	O
prove	O
useful	O
later	O
when	O
we	O
discuss	O
hiearchical	O
clustering	B
.	O
the	O
topic	B
of	O
non-parametric	O
bayes	O
is	O
currently	O
very	O
active	O
,	O
and	O
we	O
do	O
not	O
have	O
space	O
to	O
go	O
into	O
details	O
(	O
see	O
(	O
hjort	O
et	O
al	O
.	O
2010	O
)	O
for	O
a	O
recent	O
book	O
on	O
the	O
topic	B
)	O
.	O
instead	O
we	O
just	O
give	O
a	O
brief	O
review	O
of	O
the	O
dp	O
and	O
its	O
application	O
to	O
mixture	B
modeling	O
,	O
based	O
on	O
the	O
presentation	O
in	O
(	O
sudderth	O
2006	O
,	O
sec	O
2.2	O
)	O
.	O
25.2.1	O
from	O
ﬁnite	O
to	O
inﬁnite	B
mixture	I
models	I
consider	O
a	O
ﬁnite	B
mixture	I
model	I
,	O
as	O
shown	O
in	O
figure	O
25.2	O
(	O
a	O
)	O
.	O
the	O
usual	O
representation	O
is	O
as	O
follows	O
:	O
p	O
(	O
xi|zi	O
=	O
k	O
,	O
θ	O
)	O
=p	O
(	O
xi|θk	O
)	O
p	O
(	O
zi	O
=	O
k|π	O
)	O
=π	O
k	O
p	O
(	O
π|α	O
)	O
=	O
dir	O
(	O
π|	O
(	O
α/k	O
)	O
1k	O
)	O
(	O
25.13	O
)	O
(	O
25.14	O
)	O
(	O
25.15	O
)	O
the	O
form	O
of	O
p	O
(	O
θk|λ	O
)	O
is	O
chosen	O
to	O
be	O
conjugate	O
to	O
p	O
(	O
xi|θk	O
)	O
.	O
we	O
can	O
write	O
p	O
(	O
xi|θk	O
)	O
as	O
xi	O
∼	O
f	O
(	O
θzi	O
)	O
,	O
where	O
f	O
is	O
the	O
observation	B
distribution	O
.	O
similarly	O
,	O
we	O
can	O
write	O
θk	O
∼	O
h	O
(	O
λ	O
)	O
,	O
where	O
h	O
is	O
the	O
prior	O
.	O
880	O
chapter	O
25.	O
clustering	B
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
25.2	O
two	O
different	O
representations	O
of	O
a	O
ﬁnite	B
mixture	I
model	I
.	O
left	O
:	O
traditional	O
representation	O
.	O
right	O
:	O
representation	O
where	O
parameters	O
are	O
samples	B
from	O
g	O
,	O
a	O
discrete	B
measure	O
.	O
the	O
picture	O
on	O
the	O
right	O
illustrates	O
the	O
case	O
where	O
k	O
=	O
4	O
,	O
and	O
we	O
sample	O
4	O
gaussian	O
means	O
θk	O
from	O
a	O
gaussian	O
prior	O
h	O
(	O
.|λ	O
)	O
.	O
the	O
height	O
of	O
the	O
spikes	O
reﬂects	O
the	O
mixing	B
weights	I
πk	O
.	O
this	O
weighted	O
sum	O
of	O
delta	O
functions	O
is	O
g.	O
we	O
then	O
generate	O
two	O
parameters	O
,	O
θ1	O
and	O
θ2	O
,	O
from	O
g	O
,	O
one	O
per	O
data	O
point	O
.	O
finally	O
,	O
we	O
generate	O
two	O
data	O
points	O
,	O
x1	O
and	O
x2	O
,	O
from	O
n	O
(	O
θ1	O
,	O
σ2	O
)	O
and	O
n	O
(	O
θ2	O
,	O
σ2	O
)	O
.	O
source	O
:	O
figure	O
2.9	O
of	O
(	O
sudderth	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
.	O
an	O
equivalent	O
representation	O
for	O
this	O
model	O
is	O
the	O
parameter	B
used	O
to	O
generate	O
observation	O
xi	O
;	O
these	O
parameters	O
are	O
sampled	O
from	O
distribution	O
g	O
,	O
which	O
has	O
the	O
form	O
is	O
shown	O
in	O
figure	O
25.2	O
(	O
b	O
)	O
.	O
here	O
θi	O
k	O
(	O
cid:4	O
)	O
k=1	O
g	O
(	O
θ	O
)	O
=	O
πkδθk	O
(	O
θ	O
)	O
(	O
25.16	O
)	O
k	O
1	O
)	O
,	O
and	O
θk	O
∼	O
h.	O
thus	O
we	O
see	O
that	O
g	O
is	O
a	O
ﬁnite	O
mixture	O
of	O
delta	O
functions	O
,	O
where	O
π	O
∼	O
dir	O
(	O
α	O
centered	O
on	O
the	O
cluster	O
parameters	O
θk	O
.	O
the	O
probability	O
that	O
θi	O
is	O
equal	O
to	O
θk	O
is	O
exactly	O
πk	O
,	O
the	O
prior	O
probability	O
for	O
that	O
cluster	O
.	O
if	O
we	O
sample	O
from	O
this	O
model	O
,	O
we	O
will	O
always	O
(	O
with	O
probability	O
one	O
)	O
get	O
exactly	O
k	O
clusters	B
,	O
with	O
data	O
points	O
scattered	O
around	O
the	O
cluster	O
centers	O
.	O
we	O
would	O
like	O
a	O
more	O
ﬂexible	O
model	O
,	O
that	O
can	O
generate	O
a	O
variable	O
number	O
of	O
clusters	B
.	O
furthermore	O
,	O
the	O
more	O
data	O
we	O
generate	O
,	O
the	O
more	O
likely	O
we	O
should	O
be	O
to	O
see	O
a	O
new	O
cluster	O
.	O
the	O
way	O
to	O
do	O
this	O
is	O
to	O
replace	O
the	O
discrete	B
distribution	O
g	O
with	O
a	O
random	B
probability	I
measure	I
.	O
below	O
we	O
will	O
show	O
that	O
the	O
dirichlet	O
process	O
,	O
denoted	O
g	O
∼	O
dp	O
(	O
α	O
,	O
h	O
)	O
,	O
is	O
one	O
way	O
to	O
do	O
this	O
.	O
before	O
we	O
go	O
into	O
the	O
details	O
,	O
we	O
show	O
some	O
samples	B
from	O
this	O
non-parametric	B
model	I
in	O
figure	O
25.3.	O
we	O
see	O
that	O
it	O
has	O
the	O
desired	O
properties	O
of	O
generating	O
a	O
variable	O
number	O
of	O
clusters	B
,	O
with	O
more	O
clusters	B
as	O
the	O
amount	O
of	O
data	O
increases	O
.	O
the	O
resulting	O
samples	B
look	O
much	O
more	O
like	O
real	O
data	O
than	O
samples	B
from	O
a	O
ﬁnite	B
mixture	I
model	I
.	O
of	O
course	O
,	O
working	O
with	O
an	O
“	O
inﬁnite	O
”	O
model	O
sounds	O
scary	O
.	O
fortunately	O
,	O
as	O
we	O
show	O
below	O
,	O
even	O
though	O
this	O
model	O
is	O
potentially	O
inﬁnite	O
,	O
we	O
can	O
perform	O
inference	B
using	O
an	O
amount	O
of	O
computation	O
that	O
is	O
not	O
only	O
tractable	O
,	O
but	O
is	O
often	O
much	O
less	O
than	O
that	O
required	O
to	O
ﬁt	O
a	O
set	O
25.2.	O
dirichlet	O
process	O
mixture	B
models	O
881	O
(	O
a	O
)	O
(	O
c	O
)	O
(	O
e	O
)	O
(	O
b	O
)	O
(	O
d	O
)	O
(	O
f	O
)	O
figure	O
25.3	O
some	O
samples	B
from	O
a	O
dirichlet	O
process	O
mixture	B
model	I
of	O
2d	O
gaussians	O
,	O
with	O
concentration	B
parameter	I
α	O
=	O
1.	O
from	O
left	O
to	O
right	O
,	O
we	O
show	O
n	O
=	O
50	O
,	O
n	O
=	O
500	O
and	O
n	O
=	O
1000	O
samples	B
.	O
each	O
row	O
is	O
a	O
different	O
run	O
.	O
we	O
also	O
show	O
the	O
model	O
parameters	O
as	O
ellipses	O
,	O
which	O
are	O
sampled	O
from	O
a	O
vague	O
niw	O
base	B
distribution	I
.	O
based	O
on	O
figure	O
2.25	O
of	O
(	O
sudderth	O
2006	O
)	O
.	O
figure	O
generated	O
by	O
dpmsampledemo	O
,	O
written	O
by	O
yee-whye	O
teh	O
.	O
of	O
ﬁnite	O
mixture	O
models	O
for	O
different	O
k.	O
the	O
intuitive	O
reason	O
is	O
that	O
we	O
can	O
get	O
evidence	B
that	O
certain	O
values	O
of	O
k	O
are	O
appropriate	O
(	O
have	O
high	O
posterior	O
support	B
)	O
long	O
before	O
we	O
have	O
been	O
able	O
to	O
estimate	O
the	O
parameters	O
,	O
so	O
we	O
can	O
focus	O
our	O
computational	O
efforts	O
on	O
models	O
of	O
appropriate	O
complexity	O
.	O
thus	O
going	O
to	O
the	O
inﬁnite	O
limit	O
can	O
sometimes	O
be	O
faster	O
.	O
this	O
is	O
especially	O
true	O
when	O
we	O
have	O
multiple	O
model	O
selection	O
problems	O
to	O
solve	O
.	O
882	O
chapter	O
25.	O
clustering	B
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
a	O
)	O
a	O
base	B
measure	I
h	O
on	O
a	O
2d	O
space	O
θ.	O
figure	O
25.4	O
where	O
the	O
shading	O
of	O
cell	O
tk	O
is	O
proportional	O
to	O
e	O
[	O
g	O
(	O
tk	O
)	O
]	O
=	O
h	O
(	O
tk	O
)	O
.	O
regions	O
.	O
source	O
:	O
figure	O
2.21	O
of	O
(	O
sudderth	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
.	O
(	O
b	O
)	O
one	O
possible	O
partition	O
into	O
k	O
=	O
3	O
regions	O
,	O
(	O
c	O
)	O
a	O
reﬁned	O
partition	O
into	O
k	O
=	O
5	O
25.2.2	O
the	O
dirichlet	O
process	O
recall	B
from	O
chapter	O
15	O
that	O
a	O
gaussian	O
process	O
is	O
a	O
distribution	O
over	O
functions	O
of	O
the	O
form	O
f	O
:	O
x	O
→	O
r.	O
it	O
is	O
deﬁned	O
implicitly	O
by	O
the	O
requirement	O
that	O
p	O
(	O
f	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
f	O
(	O
xn	O
)	O
)	O
be	O
jointly	O
gaussian	O
,	O
for	O
any	O
set	O
of	O
points	O
xi	O
∈	O
x	O
.	O
the	O
parameters	O
of	O
this	O
gaussian	O
can	O
be	O
computed	O
using	O
a	O
mean	B
function	I
μ	O
(	O
)	O
and	O
covariance	B
(	O
kernel	B
)	O
function	O
k	O
(	O
)	O
.	O
we	O
write	O
f	O
∼	O
gp	O
(	O
μ	O
(	O
)	O
,	O
k	O
(	O
)	O
)	O
.	O
fur-	O
thermore	O
,	O
the	O
gp	O
is	O
consistently	O
deﬁned	O
,	O
so	O
that	O
p	O
(	O
f	O
(	O
x1	O
)	O
)	O
can	O
be	O
derived	O
from	O
p	O
(	O
f	O
(	O
x1	O
)	O
,	O
f	O
(	O
x2	O
)	O
)	O
,	O
etc	O
.	O
require	O
g	O
(	O
θ	O
)	O
≥	O
0	O
and	O
(	O
g	O
(	O
t1	O
)	O
,	O
.	O
.	O
.	O
,	O
g	O
(	O
tk	O
)	O
)	O
has	O
a	O
joint	O
dirichlet	O
distribution	O
+	O
,	O
where	O
we	O
g	O
(	O
θ	O
)	O
dθ	O
=	O
1.	O
the	O
dp	O
is	O
deﬁned	O
implicitly	O
by	O
the	O
requirement	O
that	O
a	O
dirichlet	O
process	O
is	O
a	O
distribution	O
over	O
probability	O
measures	O
g	O
:	O
θ	O
→	O
r	O
(	O
cid:21	O
)	O
θ	O
dir	O
(	O
αh	O
(	O
t1	O
)	O
,	O
.	O
.	O
.	O
,	O
αh	O
(	O
tk	O
)	O
)	O
(	O
25.17	O
)	O
for	O
any	O
ﬁnite	O
partition	O
(	O
t1	O
,	O
.	O
.	O
.	O
,	O
tk	O
)	O
of	O
θ.	O
if	O
this	O
is	O
the	O
case	O
,	O
we	O
write	O
g	O
∼	O
dp	O
(	O
α	O
,	O
h	O
)	O
,	O
where	O
α	O
is	O
called	O
the	O
concentration	B
parameter	I
and	O
h	O
is	O
called	O
the	O
base	O
measure.1	O
an	O
example	O
of	O
a	O
dp	O
is	O
shown	O
in	O
figure	O
25.4	O
,	O
where	O
the	O
base	B
measure	I
is	O
a	O
2d	O
gaussian	O
.	O
the	O
distribution	O
over	O
all	O
the	O
cells	O
,	O
p	O
(	O
g	O
(	O
t1	O
)	O
,	O
.	O
.	O
.	O
,	O
g	O
(	O
tk	O
)	O
)	O
,	O
is	O
dirichlet	O
,	O
so	O
the	O
marginals	O
in	O
each	O
cell	O
are	O
beta	O
distributed	O
:	O
(	O
cid:4	O
)	O
j	O
(	O
cid:5	O
)	O
=i	O
beta	O
(	O
αh	O
(	O
ti	O
)	O
,	O
α	O
h	O
(	O
tj	O
)	O
)	O
(	O
25.18	O
)	O
the	O
dp	O
is	O
consistently	O
deﬁned	O
in	O
the	O
sense	O
that	O
if	O
t1	O
and	O
t2	O
form	O
a	O
partition	O
of	O
˜t1	O
,	O
then	O
g	O
(	O
t1	O
)	O
+	O
g	O
(	O
t2	O
)	O
and	O
g	O
(	O
˜t1	O
)	O
both	O
follow	O
the	O
same	O
beta	B
distribution	I
.	O
recall	B
that	O
if	O
π	O
∼	O
dir	O
(	O
α	O
)	O
,	O
and	O
z|π	O
∼	O
cat	O
(	O
π	O
)	O
,	O
then	O
we	O
can	O
integrate	B
out	I
π	O
to	O
get	O
the	O
predictive	B
distribution	O
for	O
the	O
dirichlet-multinoulli	O
model	O
:	O
z	O
∼	O
cat	O
(	O
α1/α0	O
,	O
.	O
.	O
.	O
,	O
αk/α0	O
)	O
(	O
25.19	O
)	O
1.	O
unlike	O
a	O
gp	O
,	O
knowing	O
something	O
about	O
g	O
(	O
tk	O
)	O
does	O
not	O
tell	O
us	O
anything	O
about	O
g	O
(	O
tk	O
(	O
cid:2	O
)	O
)	O
,	O
beyond	O
the	O
sum-to-one	O
constraint	O
;	O
we	O
say	O
that	O
the	O
dp	O
is	O
a	O
neutral	B
process	I
.	O
other	O
stochastic	B
processes	I
can	O
be	O
deﬁned	O
that	O
do	O
not	O
have	O
this	O
property	O
,	O
but	O
they	O
are	O
not	O
so	O
computationally	O
convenient	O
.	O
25.2.	O
dirichlet	O
process	O
mixture	B
models	O
883	O
β1	O
π	O
1	O
β2	O
π	O
2	O
1−β1	O
1−β2	O
1−β3	O
β4	O
π	O
4	O
1−β4	O
β5	O
π	O
5	O
β3	O
π	O
3	O
(	O
a	O
)	O
α	O
=	O
2	O
α	O
=	O
2	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
0	O
(	O
b	O
)	O
10	O
20	O
30	O
α	O
=	O
5	O
10	O
20	O
30	O
10	O
20	O
30	O
α	O
=	O
5	O
10	O
20	O
30	O
illustration	O
of	O
the	O
stick	O
breaking	O
construction	O
.	O
figure	O
25.5	O
(	O
a	O
)	O
we	O
have	O
a	O
unit	O
length	O
stick	O
,	O
which	O
we	O
break	O
at	O
a	O
random	O
point	O
β1	O
;	O
the	O
length	O
of	O
the	O
piece	O
we	O
keep	O
is	O
called	O
π1	O
;	O
we	O
then	O
recursively	O
break	O
off	O
pieces	O
of	O
the	O
remaining	O
stick	O
,	O
to	O
generate	O
π2	O
,	O
π3	O
,	O
.	O
.	O
..	O
source	O
:	O
figure	O
2.22	O
of	O
(	O
sudderth	O
2006	O
)	O
.	O
used	O
with	O
(	O
b	O
)	O
samples	B
of	O
πk	O
from	O
this	O
process	O
for	O
α	O
=	O
2	O
(	O
top	O
row	O
)	O
and	O
α	O
=	O
5	O
kind	O
permission	O
of	O
erik	O
sudderth	O
.	O
(	O
bottom	O
row	O
)	O
.	O
figure	O
generated	O
by	O
stickbreakingdemo	O
,	O
written	O
by	O
yee-whye	O
teh	O
.	O
(	O
cid:7	O
)	O
where	O
α0	O
=	O
given	O
one	O
observation	B
is	O
given	O
by	O
k	O
αk	O
.	O
in	O
other	O
words	O
,	O
p	O
(	O
z	O
=	O
k|α	O
)	O
=	O
αk/α0	O
.	O
also	O
,	O
the	O
updated	O
posterior	O
for	O
π	O
π|z	O
∼	O
dir	O
(	O
α1	O
+	O
i	O
(	O
z	O
=	O
1	O
)	O
,	O
.	O
.	O
.	O
,	O
αk	O
+	O
i	O
(	O
z	O
=	O
k	O
)	O
)	O
(	O
25.20	O
)	O
the	O
dp	O
generalizes	O
this	O
to	O
arbitrary	O
partitions	O
.	O
if	O
g	O
∼	O
dp	O
(	O
α	O
,	O
h	O
)	O
,	O
then	O
p	O
(	O
θ	O
∈	O
ti	O
)	O
=	O
h	O
(	O
ti	O
)	O
and	O
the	O
posterior	O
is	O
p	O
(	O
g	O
(	O
t1	O
)	O
,	O
.	O
.	O
.	O
,	O
g	O
(	O
tk	O
)	O
|θ	O
,	O
α	O
,	O
h	O
)	O
=	O
dir	O
(	O
αh	O
(	O
t1	O
)	O
+i	O
(	O
θ	O
∈	O
t1	O
)	O
,	O
.	O
.	O
.	O
,	O
αh	O
(	O
tk	O
)	O
+i	O
(	O
θ	O
∈	O
tk	O
)	O
)	O
(	O
25.21	O
)	O
this	O
holds	O
for	O
any	O
set	O
of	O
partitions	O
.	O
hence	O
if	O
we	O
observe	O
multiple	O
samples	O
θi	O
∼	O
g	O
,	O
the	O
new	O
posterior	O
is	O
given	O
by	O
g|θ1	O
,	O
.	O
.	O
.	O
,	O
θn	O
,	O
α	O
,	O
h	O
∼	O
dp	O
α	O
+	O
n	O
,	O
1	O
α	O
+	O
n	O
αh	O
+	O
δθi	O
(	O
25.22	O
)	O
(	O
cid:10	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
n	O
(	O
cid:4	O
)	O
i=1	O
thus	O
we	O
see	O
that	O
the	O
dp	O
effectively	O
deﬁnes	O
a	O
conjugate	B
prior	I
for	O
arbitrary	O
measurable	O
spaces	O
.	O
the	O
concentration	B
parameter	I
α	O
is	O
like	O
the	O
effective	B
sample	I
size	I
of	O
the	O
base	B
measure	I
h.	O
25.2.2.1	O
stick	O
breaking	O
construction	O
of	O
the	O
dp	O
our	O
discussion	O
so	O
far	O
has	O
been	O
very	O
abstract	O
.	O
we	O
now	O
give	O
a	O
constructive	O
deﬁnition	O
for	O
the	O
dp	O
,	O
known	O
as	O
the	O
stick-breaking	B
construction	I
.	O
k=1	O
be	O
an	O
inﬁnite	O
sequence	O
of	O
mixture	B
weights	O
derived	O
from	O
the	O
following	O
process	O
:	O
let	O
π	O
=	O
{	O
πk	O
}	O
∞	O
k−1	O
(	O
cid:20	O
)	O
βk	O
∼	O
beta	O
(	O
1	O
,	O
α	O
)	O
πk	O
=	O
βk	O
(	O
1	O
−	O
βl	O
)	O
=	O
βk	O
(	O
1	O
−	O
k−1	O
(	O
cid:4	O
)	O
l=1	O
l=1	O
πl	O
)	O
(	O
25.23	O
)	O
(	O
25.24	O
)	O
884	O
chapter	O
25.	O
clustering	B
this	O
is	O
often	O
denoted	O
by	O
π	O
∼	O
gem	O
(	O
α	O
)	O
(	O
25.25	O
)	O
where	O
gem	O
stands	O
for	O
griffiths	O
,	O
engen	O
and	O
mccloskey	O
(	O
this	O
term	O
is	O
due	O
to	O
(	O
ewens	O
1990	O
)	O
)	O
.	O
some	O
samples	B
from	O
this	O
process	O
are	O
shown	O
in	O
figure	O
25.5.	O
one	O
can	O
show	O
that	O
this	O
process	O
process	O
will	O
terminate	O
with	O
probability	O
1	O
,	O
although	O
the	O
number	O
of	O
elements	O
it	O
generates	O
increases	O
with	O
α.	O
furthermore	O
,	O
the	O
size	O
of	O
the	O
πk	O
components	O
decreases	O
on	O
average	O
.	O
now	O
deﬁne	O
∞	O
(	O
cid:4	O
)	O
g	O
(	O
θ	O
)	O
=	O
πkδθk	O
(	O
θ	O
)	O
(	O
25.26	O
)	O
k=1	O
where	O
π	O
∼	O
gem	O
(	O
α	O
)	O
and	O
θk	O
∼	O
h.	O
then	O
one	O
can	O
show	O
that	O
g	O
∼	O
dp	O
(	O
α	O
,	O
h	O
)	O
.	O
as	O
a	O
consequence	O
of	O
this	O
construction	O
,	O
we	O
see	O
that	O
samples	B
from	O
a	O
dp	O
are	O
discrete	B
with	I
probability	I
one	I
.	O
in	O
other	O
words	O
,	O
if	O
you	O
keep	O
sampling	O
it	O
,	O
you	O
will	O
get	O
more	O
and	O
more	O
repetitions	O
of	O
previously	O
generated	O
values	O
.	O
so	O
if	O
we	O
sample	O
θi	O
∼	O
g	O
,	O
we	O
will	O
see	O
repeated	O
values	O
;	O
let	O
us	O
number	O
the	O
unique	O
values	O
θ1	O
,	O
θ2	O
,	O
etc	O
.	O
data	O
sampled	O
from	O
θi	O
will	O
therefore	O
cluster	O
around	O
the	O
θk	O
.	O
this	O
is	O
evident	O
in	O
figure	O
25.3	O
,	O
where	O
most	O
data	O
comes	O
from	O
the	O
gaussians	O
with	O
large	O
πk	O
values	O
,	O
represented	O
by	O
ellipses	O
with	O
thick	O
borders	O
.	O
this	O
is	O
our	O
ﬁrst	O
indication	O
that	O
the	O
dp	O
might	O
be	O
useful	O
for	O
clustering	B
.	O
25.2.2.2	O
the	O
chinese	O
restaurant	O
process	O
(	O
crp	O
)	O
(	O
cid:10	O
)	O
(	O
cid:10	O
)	O
(	O
cid:11	O
)	O
(	O
cid:11	O
)	O
k	O
(	O
cid:4	O
)	O
k=1	O
k	O
(	O
cid:4	O
)	O
k=1	O
working	O
with	O
inﬁnite	O
dimensional	O
sticks	O
is	O
problematic	O
.	O
however	O
,	O
we	O
can	O
exploit	O
the	O
clustering	B
property	O
to	O
draw	O
samples	B
form	O
a	O
gp	O
,	O
as	O
we	O
now	O
show	O
.	O
if	O
θi	O
∼	O
g	O
are	O
n	O
observations	O
from	O
g	O
∼	O
dp	O
(	O
α	O
,	O
h	O
)	O
,	O
taking	O
on	O
k	O
the	O
key	O
result	O
is	O
this	O
:	O
distinct	O
values	O
θk	O
,	O
then	O
the	O
predictive	B
distribution	O
of	O
the	O
next	O
observation	B
is	O
given	O
by	O
p	O
(	O
θn	O
+1	O
=	O
θ|θ1	O
:	O
n	O
,	O
α	O
,	O
h	O
)	O
=	O
1	O
α	O
+	O
n	O
αh	O
(	O
θ	O
)	O
+	O
nkδθk	O
(	O
θ	O
)	O
(	O
25.27	O
)	O
where	O
nk	O
is	O
the	O
number	O
of	O
previous	O
observations	O
equal	O
to	O
θk	O
.	O
this	O
is	O
called	O
the	O
polya	O
urn	O
or	O
blackwell-macqueen	O
sampling	O
scheme	O
.	O
this	O
provides	O
a	O
constructive	O
way	O
to	O
sample	O
from	O
a	O
dp	O
.	O
it	O
is	O
much	O
more	O
convenient	O
to	O
work	O
with	O
discrete	B
variables	O
zi	O
which	O
specify	O
which	O
value	O
of	O
θk	O
to	O
use	O
.	O
that	O
is	O
,	O
we	O
deﬁne	O
θi	O
=	O
θzi	O
.	O
based	O
on	O
the	O
above	O
expression	O
,	O
we	O
have	O
p	O
(	O
zn	O
+1	O
=	O
z|z1	O
:	O
n	O
,	O
α	O
)	O
=	O
1	O
α	O
+	O
n	O
αi	O
(	O
z	O
=	O
k∗	O
)	O
+	O
nki	O
(	O
z	O
=	O
k	O
)	O
(	O
25.28	O
)	O
where	O
k∗	O
represents	O
a	O
new	O
cluster	O
index	O
that	O
has	O
not	O
yet	O
been	O
used	O
.	O
this	O
is	O
called	O
the	O
chinese	O
restaurant	O
process	O
or	O
crp	O
,	O
based	O
on	O
the	O
seemingly	O
inﬁnite	O
supply	O
of	O
tables	O
at	O
certain	O
chinese	O
restaurants	O
.	O
the	O
analogy	O
is	O
as	O
follows	O
:	O
the	O
tables	O
are	O
like	O
clusters	B
,	O
and	O
the	O
customers	O
are	O
like	O
observations	O
.	O
when	O
a	O
person	O
enters	O
the	O
restaurant	O
,	O
he	O
may	O
choose	O
to	O
join	O
an	O
existing	O
table	O
with	O
probability	O
proportional	O
to	O
the	O
number	O
of	O
people	O
already	O
sitting	O
at	O
this	O
table	O
(	O
the	O
nk	O
)	O
;	O
otherwise	O
,	O
with	O
a	O
probability	O
that	O
diminishes	O
as	O
more	O
people	O
enter	O
the	O
room	O
(	O
due	O
to	O
the	O
1/	O
(	O
α	O
+	O
n	O
)	O
term	O
)	O
,	O
25.2.	O
dirichlet	O
process	O
mixture	B
models	O
885	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
25.6	O
two	O
views	B
of	O
a	O
dp	O
mixture	B
model	I
.	O
left	O
:	O
π	O
∼	O
gem	O
(	O
α	O
)	O
.	O
right	O
:	O
g	O
is	O
drawn	O
from	O
a	O
dp	O
.	O
compare	O
to	O
figure	O
25.2	O
.	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
.	O
inﬁnite	O
number	O
of	O
clusters	B
parameters	O
,	O
θk	O
,	O
and	O
source	O
:	O
figure	O
2.24	O
of	O
(	O
sudderth	O
he	O
may	O
choose	O
to	O
sit	O
at	O
a	O
new	O
table	O
k∗	O
integers	O
,	O
which	O
is	O
like	O
a	O
distribution	O
of	O
customers	O
to	O
tables	O
.	O
.	O
the	O
result	O
is	O
a	O
distribution	O
over	O
partitions	O
of	O
the	O
the	O
fact	O
that	O
currently	O
occupied	O
tables	O
are	O
more	O
likely	O
to	O
get	O
new	O
customers	O
is	O
sometimes	O
called	O
the	O
rich	B
get	I
richer	I
phenomenon	O
.	O
indeed	O
,	O
one	O
can	O
derive	O
an	O
expression	O
for	O
the	O
distri-	O
bution	O
of	O
cluster	O
sizes	O
induced	O
by	O
this	O
prior	O
process	O
;	O
it	O
is	O
basically	O
a	O
power	B
law	I
.	O
the	O
number	O
of	O
occupied	O
tables	O
k	O
almost	O
surely	O
approaches	O
α	O
log	O
(	O
n	O
)	O
as	O
n	O
→	O
∞	O
,	O
showing	O
that	O
the	O
model	O
complexity	O
will	O
indeed	O
grow	O
logarithmically	O
with	O
dataset	O
size	O
.	O
more	O
ﬂexible	O
priors	O
over	O
cluster	O
sizes	O
can	O
also	O
be	O
deﬁned	O
,	O
such	O
as	O
the	O
two-parameter	O
pitman-yor	O
process	O
.	O
25.2.3	O
applying	O
dirichlet	O
processes	O
to	O
mixture	B
modeling	O
the	O
dp	O
is	O
not	O
particularly	O
useful	O
as	O
a	O
model	O
for	O
data	O
directly	O
,	O
since	O
data	O
vectors	O
rarely	O
repeat	O
exactly	O
.	O
however	O
,	O
it	O
is	O
useful	O
as	O
a	O
prior	O
for	O
the	O
parameters	O
of	O
a	O
stochastic	O
data	O
generating	O
mechanism	O
,	O
such	O
as	O
a	O
mixture	B
model	I
.	O
to	O
create	O
such	O
a	O
model	O
,	O
we	O
follow	O
exactly	O
the	O
same	O
setup	O
as	O
section	O
11.2	O
,	O
but	O
we	O
deﬁne	O
g	O
∼	O
dp	O
(	O
α	O
,	O
h	O
)	O
.	O
equivalently	O
,	O
we	O
can	O
write	O
the	O
model	O
as	O
follows	O
:	O
(	O
25.29	O
)	O
(	O
25.30	O
)	O
(	O
25.31	O
)	O
(	O
25.32	O
)	O
π	O
∼	O
gem	O
(	O
α	O
)	O
zi	O
∼	O
π	O
θk	O
∼	O
h	O
(	O
λ	O
)	O
xi	O
∼	O
f	O
(	O
θzi	O
)	O
this	O
is	O
illustrated	O
in	O
figure	O
25.6.	O
we	O
see	O
that	O
g	O
is	O
now	O
a	O
random	O
draw	O
of	O
an	O
unbounded	O
number	O
of	O
parameters	O
θk	O
from	O
the	O
base	B
distribution	I
h	O
,	O
each	O
with	O
weight	O
πk	O
.	O
each	O
data	O
point	O
xi	O
is	O
generated	O
by	O
sampling	O
its	O
own	O
“	O
private	O
”	O
parameter	B
θi	O
from	O
g.	O
as	O
we	O
get	O
more	O
and	O
more	O
data	O
,	O
it	O
becomes	O
increasingly	O
likely	O
that	O
θi	O
will	O
be	O
equal	O
to	O
one	O
of	O
the	O
θk	O
’	O
s	O
we	O
have	O
seen	O
before	O
,	O
and	O
thus	O
xi	O
will	O
be	O
generated	O
close	O
to	O
an	O
existing	O
datapoint	O
.	O
886	O
chapter	O
25.	O
clustering	B
25.2.4	O
fitting	O
a	O
dp	O
mixture	B
model	I
the	O
simplest	O
way	O
to	O
ﬁt	O
a	O
dpmm	O
is	O
to	O
modify	O
the	O
collapsed	O
gibbs	O
sampler	O
of	O
section	O
24.2.4.	O
from	O
equation	O
24.23	O
we	O
have	O
p	O
(	O
zi	O
=	O
k|z−i	O
,	O
x	O
,	O
α	O
,	O
λ	O
)	O
∝	O
p	O
(	O
zi	O
=	O
k|z−i	O
,	O
α	O
)	O
p	O
(	O
xi|x−i	O
,	O
zi	O
=	O
k	O
,	O
z−i	O
,	O
λ	O
)	O
(	O
25.33	O
)	O
(	O
cid:10	O
)	O
1	O
α	O
+	O
n	O
−	O
1	O
3	O
by	O
exchangeability	O
,	O
we	O
can	O
assume	O
that	O
zi	O
is	O
the	O
last	O
customer	O
to	O
enter	O
the	O
restaurant	O
.	O
hence	O
the	O
ﬁrst	O
term	O
is	O
given	O
by	O
(	O
cid:11	O
)	O
p	O
(	O
zi|z−i	O
,	O
α	O
)	O
=	O
αi	O
(	O
zi	O
=	O
k∗	O
)	O
+	O
nk	O
,	O
−ii	O
(	O
zi	O
=	O
k	O
)	O
(	O
25.34	O
)	O
k	O
(	O
cid:4	O
)	O
k=1	O
where	O
k	O
is	O
the	O
number	O
of	O
clusters	B
used	O
by	O
z−i	O
,	O
and	O
k∗	O
this	O
is	O
as	O
follows	O
:	O
is	O
a	O
new	O
cluster	O
.	O
another	O
way	O
to	O
write	O
nk	O
,	O
−i	O
α+n−1	O
α+n−1	O
α	O
p	O
(	O
zi	O
=	O
k|z−i	O
,	O
α	O
)	O
=	O
if	O
k	O
has	O
been	O
seen	O
before	O
if	O
k	O
is	O
a	O
new	O
cluster	O
nk	O
,	O
−i+α/k	O
(	O
25.35	O
)	O
interestingly	O
,	O
this	O
is	O
equivalent	O
to	O
equation	O
24.26	O
,	O
which	O
has	O
the	O
form	O
p	O
(	O
zi	O
=	O
k|z−i	O
,	O
α	O
)	O
=	O
α+n−1	O
to	O
compute	O
the	O
second	O
term	O
,	O
p	O
(	O
xi|x−i	O
,	O
zi	O
=	O
k	O
,	O
z−i	O
,	O
λ	O
)	O
,	O
let	O
us	O
partition	O
the	O
data	O
x−i	O
into	O
clusters	B
based	O
on	O
z−i	O
.	O
let	O
x−i	O
,	O
c	O
=	O
{	O
xj	O
:	O
zj	O
=	O
c	O
,	O
j	O
(	O
cid:8	O
)	O
=	O
i	O
}	O
be	O
the	O
data	O
assigned	O
to	O
cluster	O
c.	O
if	O
zi	O
=	O
k	O
,	O
then	O
xi	O
is	O
conditionally	B
independent	I
of	O
all	O
the	O
data	O
points	O
except	O
those	O
assigned	O
to	O
cluster	O
k.	O
hence	O
we	O
have	O
,	O
in	O
thek	O
→	O
∞	O
limit	O
(	O
rasmussen	O
2000	O
;	O
neal	O
2000	O
)	O
.	O
p	O
(	O
xi|x−i	O
,	O
z−i	O
,	O
zi	O
=	O
k	O
,	O
λ	O
)	O
=p	O
(	O
xi|x−i	O
,	O
k	O
,	O
λ	O
)	O
=	O
where	O
p	O
(	O
xi	O
,	O
x−i	O
,	O
k|λ	O
)	O
=	O
(	O
cid:12	O
)	O
p	O
(	O
xi|θk	O
)	O
⎡	O
⎣	O
(	O
cid:20	O
)	O
j	O
(	O
cid:5	O
)	O
=i	O
:	O
zj	O
=k	O
p	O
(	O
xi	O
,	O
x−i	O
,	O
k|λ	O
)	O
p	O
(	O
x−i	O
,	O
k|λ	O
)	O
⎤	O
⎦	O
h	O
(	O
θk|λ	O
)	O
dθk	O
p	O
(	O
xj|θk	O
)	O
(	O
25.36	O
)	O
(	O
25.37	O
)	O
is	O
the	O
marginal	B
likelihood	I
of	O
all	O
the	O
data	O
assigned	O
to	O
cluster	O
k	O
,	O
including	O
i	O
,	O
and	O
p	O
(	O
x−i	O
,	O
k|λ	O
)	O
is	O
an	O
analogous	O
expression	O
excluding	O
i.	O
thus	O
we	O
see	O
that	O
the	O
term	O
p	O
(	O
xi|x−i	O
,	O
z−i	O
,	O
zi	O
=	O
k	O
,	O
λ	O
)	O
is	O
the	O
posterior	O
preditive	O
distribution	O
for	O
cluster	O
k	O
evaluated	O
at	O
xi	O
.	O
,	O
corresponding	O
to	O
a	O
new	O
cluster	O
,	O
we	O
have	O
if	O
zi	O
=	O
k∗	O
p	O
(	O
xi|x−i	O
,	O
z−i	O
,	O
zi	O
=	O
k∗	O
,	O
λ	O
)	O
=	O
p	O
(	O
xi|λ	O
)	O
=	O
(	O
cid:12	O
)	O
p	O
(	O
xi|θ	O
)	O
h	O
(	O
θ|λ	O
)	O
dθ	O
(	O
25.38	O
)	O
which	O
is	O
just	O
the	O
prior	O
predictive	B
distribution	O
for	O
a	O
new	O
cluster	O
evaluated	O
at	O
xi	O
.	O
see	O
algorithm	O
1	O
for	O
the	O
pseudocode	O
.	O
(	O
this	O
is	O
called	O
“	O
algorithm	O
3	O
”	O
in	O
(	O
neal	O
2000	O
)	O
.	O
)	O
this	O
is	O
very	O
similar	B
to	O
collapsed	O
gibbs	O
for	O
ﬁnite	O
mixtures	O
except	O
that	O
we	O
have	O
to	O
consider	O
the	O
case	O
zi	O
=	O
k∗	O
.	O
an	O
example	O
of	O
this	O
procedure	O
in	O
action	B
is	O
shown	O
in	O
figure	O
25.7.	O
the	O
sample	O
clusterings	O
,	O
and	O
the	O
induced	O
posterior	O
over	O
k	O
,	O
seems	O
reasonable	O
.	O
the	O
method	O
tends	O
to	O
rapidly	O
discover	O
a	O
good	O
clustering	B
.	O
by	O
contrast	O
,	O
gibbs	O
sampling	O
(	O
and	O
em	O
)	O
for	O
a	O
ﬁnite	B
mixture	I
model	I
often	O
gets	O
stuck	O
in	O
25.3.	O
affinity	B
propagation	I
887	O
algorithm	O
25.1	O
:	O
collapsed	O
gibbs	O
sampler	O
for	O
dp	O
mixtures	O
1	O
for	O
each	O
i	O
=	O
1	O
:	O
n	O
in	O
random	O
order	O
do	O
2	O
remove	O
xi	O
’	O
s	O
sufficient	B
statistics	I
from	O
old	O
cluster	O
zi	O
;	O
for	O
each	O
k	O
=	O
1	O
:	O
k	O
do	O
compute	O
pk	O
(	O
xi	O
)	O
=	O
p	O
(	O
xi|x−i	O
(	O
k	O
)	O
)	O
;	O
set	O
nk	O
,	O
−i	O
=	O
dim	O
(	O
x−i	O
(	O
k	O
)	O
)	O
;	O
compute	O
p	O
(	O
zi	O
=	O
k|z−i	O
,	O
d	O
)	O
=	O
nk	O
,	O
−i	O
α+n−1	O
;	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
compute	O
p∗	O
(	O
xi	O
)	O
=	O
p	O
(	O
xi|λ	O
)	O
;	O
compute	O
p	O
(	O
zi	O
=	O
∗|z−i	O
,	O
d	O
)	O
=	O
normalize	O
p	O
(	O
zi|·	O
)	O
;	O
sample	O
zi	O
∼	O
p	O
(	O
zi|·	O
)	O
;	O
add	O
xi	O
’	O
s	O
sufficient	B
statistics	I
to	O
new	O
cluster	O
zi	O
;	O
if	O
any	O
cluster	O
is	O
empty	O
,	O
remove	O
it	O
and	O
decrease	O
k	O
;	O
α+n−1	O
;	O
α	O
poor	O
local	O
optima	O
(	O
not	O
shown	O
)	O
.	O
this	O
is	O
because	O
the	O
dpmm	O
is	O
able	O
to	O
create	O
extra	O
redundant	O
clusters	B
early	O
on	O
,	O
and	O
to	O
use	O
them	O
to	O
escape	O
local	O
optima	O
.	O
figure	O
25.8	O
shows	O
that	O
most	O
of	O
the	O
time	O
,	O
the	O
dpmm	O
converges	O
more	O
rapidly	O
than	O
a	O
ﬁnite	B
mixture	I
model	I
.	O
a	O
variety	O
of	O
other	O
ﬁtting	O
methods	O
have	O
been	O
proposed	O
.	O
(	O
daume	O
2007a	O
)	O
shows	O
how	O
one	O
can	O
use	O
a	O
star	O
search	O
and	O
beam	B
search	I
to	O
quickly	O
ﬁnd	O
an	O
approximate	O
map	O
estimate	O
.	O
(	O
mansinghka	O
et	O
al	O
.	O
2007	O
)	O
discusses	O
how	O
to	O
ﬁt	O
a	O
dpmm	O
online	O
using	O
particle	B
ﬁltering	I
,	O
which	O
is	O
a	O
like	O
a	O
stochastic	O
version	O
of	O
beam	B
search	I
.	O
this	O
can	O
be	O
more	O
efficient	O
than	O
gibbs	O
sampling	O
,	O
particularly	O
for	O
large	O
datasets	O
.	O
(	O
kurihara	O
et	O
al	O
.	O
2006	O
)	O
develops	O
a	O
variational	O
approximation	O
that	O
is	O
even	O
faster	O
(	O
see	O
also	O
(	O
zobay	O
2009	O
)	O
)	O
.	O
extensions	O
to	O
the	O
case	O
of	O
non-conjugate	O
priors	O
are	O
discussed	O
in	O
(	O
neal	O
2000	O
)	O
.	O
another	O
important	O
issue	O
is	O
how	O
to	O
set	O
the	O
hyper-parameters	B
.	O
for	O
the	O
dp	O
,	O
the	O
value	O
of	O
α	O
does	O
not	O
have	O
much	O
impact	O
on	O
predictive	B
accuracy	O
,	O
but	O
it	O
does	O
affect	O
the	O
number	O
of	O
clusters	B
.	O
one	O
approach	O
is	O
to	O
put	O
a	O
ga	O
(	O
a	O
,	O
b	O
)	O
prior	O
for	O
α	O
,	O
and	O
then	O
to	O
from	O
its	O
posterior	O
,	O
p	O
(	O
α|k	O
,	O
n	O
,	O
a	O
,	O
b	O
)	O
,	O
using	O
auxiliary	O
variable	O
methods	O
(	O
escobar	O
and	O
west	O
1995	O
)	O
.	O
alternatively	O
,	O
one	O
can	O
use	O
empirical	O
bayes	O
(	O
mcauliffe	O
et	O
al	O
.	O
2006	O
)	O
.	O
similarly	O
,	O
for	O
the	O
base	B
distribution	I
,	O
we	O
can	O
either	O
sample	O
the	O
hyper-parameters	B
λ	O
(	O
rasmussen	O
2000	O
)	O
or	O
use	O
empirical	O
bayes	O
(	O
mcauliffe	O
et	O
al	O
.	O
2006	O
)	O
.	O
25.3	O
affinity	B
propagation	I
mixture	O
models	O
,	O
whether	O
ﬁnite	O
or	O
inﬁnite	O
,	O
require	O
access	O
to	O
the	O
raw	O
n	O
×	O
d	O
data	O
matrix	O
,	O
and	O
need	O
to	O
specify	O
a	O
generative	O
model	O
of	O
the	O
data	O
.	O
an	O
alternative	O
approach	O
takes	O
as	O
input	O
an	O
n	O
×n	O
similarity	O
matrix	O
,	O
and	O
then	O
tries	O
to	O
identify	O
examplars	O
,	O
which	O
will	O
act	O
as	O
cluster	O
centers	O
.	O
the	O
k-medoids	O
or	O
k-centers	O
algorithm	O
(	O
section	O
14.4.2	O
)	O
is	O
one	O
approach	O
,	O
but	O
it	O
can	O
suffer	O
from	O
local	O
minima	O
.	O
here	O
we	O
describe	O
an	O
alternative	O
approach	O
called	O
affinity	B
propagation	I
(	O
frey	O
and	O
dueck	O
2007	O
)	O
that	O
works	O
substantially	O
better	O
in	O
practice	O
.	O
the	O
idea	O
is	O
that	O
each	O
data	O
point	O
must	O
choose	O
another	O
data	O
point	O
as	O
its	O
exemplar	O
or	O
centroid	B
;	O
some	O
data	O
points	O
will	O
choose	O
themselves	O
as	O
centroids	B
,	O
and	O
this	O
will	O
automatically	O
determine	O
the	O
number	O
of	O
clusters	B
.	O
more	O
precisely	O
,	O
let	O
ci	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
represent	O
the	O
centroid	B
for	O
datapoint	O
i	O
.	O
888	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
4	O
2	O
0	O
−2	O
−4	O
−6	O
iter	O
#	O
50	O
iter	O
#	O
100	O
chapter	O
25.	O
clustering	B
4	O
2	O
0	O
−2	O
−4	O
−6	O
−6	O
−4	O
−2	O
0	O
2	O
4	O
−6	O
−4	O
−2	O
0	O
2	O
4	O
(	O
a	O
)	O
iter	O
#	O
200	O
−6	O
−4	O
−2	O
0	O
2	O
4	O
(	O
c	O
)	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
(	O
b	O
)	O
1	O
2	O
3	O
4	O
5	O
6	O
(	O
d	O
)	O
figure	O
25.7	O
100	O
data	O
points	O
in	O
2d	O
are	O
clustered	O
using	O
a	O
dp	O
mixture	B
ﬁt	O
with	O
collapsed	O
gibbs	O
sampling	O
.	O
we	O
show	O
samples	B
from	O
the	O
posterior	O
after	O
50,100	O
,	O
200	O
samples	B
.	O
we	O
also	O
show	O
the	O
posterior	O
over	O
k	O
,	O
based	O
on	O
200	O
samples	B
,	O
discarding	O
the	O
ﬁrst	O
50	O
as	O
burnin	O
.	O
figure	O
generated	O
by	O
dpmgauss2ddemo	O
,	O
written	O
by	O
yee	O
whye	O
teh	O
.	O
the	O
goal	O
is	O
to	O
maximize	O
the	O
following	O
function	O
n	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
s	O
(	O
c	O
)	O
=	O
s	O
(	O
i	O
,	O
ci	O
)	O
+	O
δk	O
(	O
c	O
)	O
(	O
25.39	O
)	O
i=1	O
k=1	O
the	O
ﬁrst	O
term	O
measures	O
the	O
similarity	O
of	O
each	O
point	O
to	O
its	O
centroid	B
.	O
the	O
second	O
term	O
is	O
a	O
penalty	O
term	O
that	O
is	O
−∞	O
if	O
some	O
data	O
point	O
i	O
has	O
chosen	O
k	O
as	O
its	O
exemplar	O
(	O
i.e.	O
,	O
ci	O
=	O
k	O
)	O
,	O
but	O
k	O
has	O
not	O
chosen	O
itself	O
as	O
an	O
exemplar	O
(	O
i.e.	O
,	O
we	O
do	O
not	O
have	O
ck	O
=	O
k	O
)	O
.	O
more	O
formally	O
,	O
(	O
cid:19	O
)	O
−∞	O
if	O
ck	O
(	O
cid:8	O
)	O
=	O
k	O
but	O
∃i	O
:	O
ci	O
=	O
k	O
(	O
25.40	O
)	O
δk	O
(	O
c	O
)	O
=	O
0	O
otherwise	O
the	O
objective	O
function	O
can	O
be	O
represented	O
as	O
a	O
factor	B
graph	I
.	O
we	O
can	O
either	O
use	O
n	O
nodes	B
,	O
25.3.	O
affinity	B
propagation	I
)	O
θ	O
,	O
π	O
|	O
x	O
(	O
p	O
g	O
o	O
l	O
−350	O
−400	O
−450	O
−500	O
−550	O
−600	O
100	O
dirichlet	O
process	O
mixture	B
finite	O
mixture	B
102	O
103	O
101	O
iteration	O
(	O
a	O
)	O
)	O
θ	O
,	O
π	O
|	O
x	O
(	O
p	O
g	O
o	O
l	O
−350	O
−400	O
−450	O
−500	O
−550	O
−600	O
100	O
889	O
dirichlet	O
process	O
mixture	B
finite	O
mixture	B
102	O
103	O
101	O
iteration	O
(	O
b	O
)	O
figure	O
25.8	O
comparison	O
of	O
collapsed	O
gibbs	O
samplers	O
for	O
a	O
dp	O
mixture	B
(	O
dark	O
blue	O
)	O
and	O
a	O
ﬁnite	O
mixture	O
(	O
light	O
red	O
)	O
with	O
k	O
=	O
4	O
applied	O
to	O
n	O
=	O
300	O
data	O
points	O
(	O
shown	O
in	O
figure	O
25.7	O
)	O
.	O
left	O
:	O
logprob	O
vs	O
iteration	O
for	O
20	O
different	O
starting	O
values	O
.	O
right	O
:	O
median	B
(	O
thick	O
line	O
)	O
and	O
quantiles	O
(	O
dashed	O
lines	O
)	O
over	O
100	O
different	O
starting	O
values	O
.	O
source	O
:	O
figure	O
2.27	O
of	O
(	O
sudderth	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
erik	O
sudderth	O
.	O
a	O
1	O
2	O
2	O
3	O
…	O
…	O
k	O
n	O
c1	O
c2	O
…	O
ci	O
c3	O
…	O
cn	O
s	O
(	O
1	O
,	O
)	O
s	O
(	O
2	O
,	O
)	O
s	O
(	O
3	O
,	O
)	O
s	O
(	O
i	O
,	O
)	O
s	O
(	O
n	O
,	O
)	O
figure	O
25.9	O
factor	B
graphs	O
for	O
affinity	B
propagation	I
.	O
circles	O
are	O
variables	O
,	O
squares	O
are	O
factors	B
.	O
each	O
ci	O
node	O
has	O
n	O
possible	O
states	O
.	O
from	O
figure	O
s2	O
of	O
(	O
frey	O
and	O
dueck	O
2007	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
brendan	O
frey	O
.	O
each	O
with	O
n	O
possible	O
values	O
,	O
as	O
shown	O
in	O
figure	O
25.9	O
,	O
or	O
we	O
can	O
use	O
n	O
2	O
binary	O
nodes	O
(	O
see	O
(	O
givoni	O
and	O
frey	O
2009	O
)	O
for	O
the	O
details	O
)	O
.	O
we	O
will	O
assume	O
the	O
former	O
representation	O
.	O
we	O
can	O
ﬁnd	O
a	O
strong	O
local	O
maximum	O
of	O
the	O
objective	O
by	O
using	O
max-product	B
loopy	O
belief	B
propagation	I
(	O
section	O
22.2	O
)	O
.	O
referring	O
to	O
the	O
model	O
in	O
figure	O
25.9	O
,	O
each	O
variable	O
nodes	O
ci	O
sends	O
a	O
message	O
to	O
each	O
factor	B
node	O
δk	O
.	O
it	O
turns	O
out	O
that	O
this	O
vector	O
of	O
n	O
numbers	O
can	O
be	O
reduced	O
to	O
a	O
scalar	O
message	O
,	O
denote	O
ri→k	O
,	O
known	O
as	O
the	O
responsibility	B
.	O
this	O
is	O
a	O
measure	O
of	O
how	O
much	O
i	O
thinks	O
k	O
would	O
make	O
a	O
good	O
exemplar	O
,	O
compared	O
to	O
all	O
the	O
other	O
exemplars	O
i	O
has	O
looked	O
at	O
.	O
in	O
addition	O
,	O
each	O
factor	B
node	O
δk	O
sends	O
a	O
message	O
to	O
each	O
variable	O
node	O
ci	O
.	O
again	O
this	O
can	O
be	O
reduced	O
to	O
a	O
scalar	O
message	O
,	O
ai←k	O
,	O
known	O
as	O
the	O
availability	O
.	O
this	O
is	O
a	O
measure	O
of	O
how	O
strongly	O
k	O
believes	O
it	O
should	O
an	O
exemplar	O
for	O
i	O
,	O
based	O
on	O
all	O
the	O
other	O
data	O
points	O
k	O
has	O
looked	O
at	O
.	O
as	O
usual	O
with	O
loopy	O
bp	O
,	O
the	O
method	O
might	O
oscillate	O
,	O
and	O
convergence	O
is	O
not	O
guaranteed	O
.	O
890	O
chapter	O
25.	O
clustering	B
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
	O
	O
	O
	O
	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
	O
	O
	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
	O
	O
	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
	O
	O
	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
	O
	O
	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
	O
	O
	O
	O
,	O
-	O
,	O
	O
$	O
4	O
$	O
+	O
.	O
*	O
/	O
$	O
4	O
$	O
+	O
.	O
*	O
/	O
figure	O
25.10	O
example	O
of	O
affinity	B
propagation	I
.	O
each	O
point	O
is	O
colored	O
coded	O
by	O
how	O
much	O
it	O
wants	O
to	O
be	O
an	O
exemplar	O
(	O
red	O
is	O
the	O
most	O
,	O
green	O
is	O
the	O
least	O
)	O
.	O
this	O
can	O
be	O
computed	O
by	O
summing	O
up	O
all	O
the	O
incoming	O
availability	O
messages	O
and	O
the	O
self-similarity	O
term	O
.	O
the	O
darkness	O
of	O
the	O
i	O
→	O
k	O
arrow	O
reﬂects	O
how	O
much	O
point	O
i	O
wants	O
to	O
belong	O
to	O
exemplar	O
k.	O
from	O
figure	O
1	O
of	O
(	O
frey	O
and	O
dueck	O
2007	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
brendan	O
frey	O
.	O
however	O
,	O
by	O
using	O
damping	B
,	O
the	O
method	O
is	O
very	O
reliable	O
in	O
practice	O
.	O
if	O
the	O
graph	B
is	O
densely	O
connected	O
,	O
message	B
passing	I
takes	O
o	O
(	O
n	O
2	O
)	O
time	O
,	O
but	O
with	O
sparse	B
similarity	O
matrices	O
,	O
it	O
only	O
takes	O
o	O
(	O
e	O
)	O
time	O
,	O
where	O
e	O
is	O
the	O
number	O
of	O
edges	B
or	O
non-zero	O
entries	O
in	O
s.	O
the	O
number	O
of	O
clusters	B
can	O
be	O
controlled	O
by	O
scaling	O
the	O
diagonal	B
terms	O
s	O
(	O
i	O
,	O
i	O
)	O
,	O
which	O
reﬂect	O
how	O
much	O
each	O
data	O
point	O
wants	O
to	O
be	O
an	O
exemplar	O
.	O
figure	O
25.10	O
gives	O
a	O
simple	O
example	O
of	O
some	O
2d	O
data	O
,	O
where	O
the	O
negative	O
euclidean	O
distance	O
was	O
used	O
to	O
measured	O
similarity	O
.	O
the	O
s	O
(	O
i	O
,	O
i	O
)	O
values	O
were	O
set	O
to	O
be	O
the	O
median	B
of	O
all	O
the	O
pairwise	O
similarities	O
.	O
the	O
result	O
is	O
3	O
clusters	B
.	O
many	O
other	O
results	O
are	O
reported	O
in	O
(	O
frey	O
and	O
dueck	O
2007	O
)	O
,	O
who	O
show	O
that	O
the	O
method	O
signiﬁcantly	O
outperforms	O
k-medoids	O
.	O
25.4	O
spectral	B
clustering	I
an	O
alternative	O
view	O
of	O
clustering	B
is	O
in	O
terms	O
of	O
graph	B
cuts	I
.	O
the	O
idea	O
is	O
we	O
create	O
a	O
weighted	O
undirected	O
graph	B
w	O
from	O
the	O
similarity	O
matrix	O
s	O
,	O
typically	O
by	O
using	O
the	O
nearest	O
neighbors	O
of	O
each	O
point	O
;	O
this	O
ensures	O
the	O
graph	B
is	O
sparse	B
,	O
which	O
speeds	O
computation	O
.	O
if	O
we	O
want	O
to	O
ﬁnd	O
a	O
partition	O
into	O
k	O
clusters	B
,	O
say	O
a1	O
,	O
.	O
.	O
.	O
,	O
ak	O
,	O
one	O
natural	O
criterion	O
is	O
to	O
minimize	O
cut	O
(	O
a1	O
,	O
.	O
.	O
.	O
,	O
ak	O
)	O
(	O
cid:2	O
)	O
1	O
2	O
k	O
(	O
cid:4	O
)	O
k=1	O
w	O
(	O
ak	O
,	O
ak	O
)	O
(	O
25.41	O
)	O
25.4.	O
spectral	B
clustering	I
(	O
cid:7	O
)	O
where	O
ak	O
=	O
v	O
\	O
ak	O
is	O
the	O
complement	O
of	O
ak	O
,	O
and	O
w	O
(	O
a	O
,	O
b	O
)	O
(	O
cid:2	O
)	O
i∈a	O
,	O
j∈b	O
wij	O
.	O
for	O
k	O
=	O
2	O
this	O
problem	O
is	O
easy	O
to	O
solve	O
.	O
unfortunately	O
the	O
optimal	O
solution	O
often	O
just	O
partitions	O
off	O
a	O
single	O
data	O
point	O
from	O
the	O
rest	O
.	O
to	O
ensure	O
the	O
sets	O
are	O
reasonably	O
large	O
,	O
we	O
can	O
deﬁne	O
the	O
normalized	B
cut	I
to	O
be	O
891	O
(	O
25.42	O
)	O
k	O
(	O
cid:4	O
)	O
k=1	O
ncut	O
(	O
a1	O
,	O
.	O
.	O
.	O
,	O
ak	O
)	O
(	O
cid:2	O
)	O
1	O
2	O
(	O
cid:7	O
)	O
cut	O
(	O
ak	O
,	O
ak	O
)	O
vol	O
(	O
ak	O
)	O
(	O
cid:7	O
)	O
n	O
i∈a	O
di	O
,	O
and	O
di	O
=	O
where	O
vol	O
(	O
a	O
)	O
(	O
cid:2	O
)	O
j=1	O
wij	O
is	O
the	O
weighted	O
degree	O
of	O
node	O
i.	O
this	O
splits	O
the	O
graph	B
into	O
k	O
clusters	B
such	O
that	O
nodes	B
within	O
each	O
cluster	O
are	O
similar	B
to	O
each	O
other	O
,	O
but	O
are	O
different	O
to	O
nodes	B
in	O
other	O
clusters	B
.	O
we	O
can	O
formulate	O
the	O
ncut	O
problem	O
in	O
terms	O
of	O
searching	O
for	O
binary	O
vectors	O
ci	O
∈	O
{	O
0	O
,	O
1	O
}	O
n	O
,	O
where	O
cik	O
=	O
1	O
if	O
point	O
i	O
belongs	O
to	O
cluster	O
k	O
,	O
that	O
minimize	O
the	O
objective	O
.	O
unfortunately	O
this	O
is	O
np-hard	O
(	O
wagner	O
and	O
wagner	O
1993	O
)	O
.	O
affinity	B
propagation	I
is	O
one	O
way	O
to	O
solve	O
the	O
problem	O
.	O
another	O
is	O
to	O
relax	O
the	O
constraints	O
that	O
ci	O
be	O
binary	O
,	O
and	O
allow	O
them	O
to	O
be	O
real-valued	O
.	O
the	O
result	O
turns	O
into	O
an	O
eigenvector	O
problem	O
known	O
as	O
spectral	B
clustering	I
(	O
see	O
e.g.	O
,	O
(	O
shi	O
and	O
malik	O
in	O
general	O
,	O
the	O
technique	O
of	O
performing	O
eigenalysis	O
of	O
graphs	O
is	O
called	O
spectral	O
graph	O
2000	O
)	O
)	O
.	O
theory	O
(	O
chung	O
1997	O
)	O
.	O
going	O
into	O
the	O
details	O
would	O
take	O
us	O
too	O
far	O
aﬁeld	O
,	O
but	O
below	O
we	O
give	O
a	O
very	O
brief	O
summary	O
,	O
based	O
on	O
(	O
von	O
luxburg	O
2007	O
)	O
,	O
since	O
we	O
will	O
encounter	O
some	O
of	O
these	O
ideas	O
later	O
on	O
.	O
25.4.1	O
graph	B
laplacian	O
let	O
w	O
be	O
a	O
symmetric	B
weight	O
matrix	O
for	O
a	O
graph	B
,	O
where	O
wij	O
=	O
wji	O
≥	O
0.	O
let	O
d	O
=	O
diag	O
(	O
di	O
)	O
be	O
a	O
diaogonal	O
matrix	O
containing	O
the	O
weighted	O
degree	O
of	O
each	O
node	O
.	O
we	O
deﬁne	O
the	O
graph	B
laplacian	O
as	O
follows	O
:	O
l	O
(	O
cid:2	O
)	O
d	O
−	O
w	O
(	O
25.43	O
)	O
this	O
matrix	O
has	O
various	O
important	O
properties	O
.	O
because	O
each	O
row	O
sums	O
to	O
zero	O
,	O
we	O
have	O
(	O
cid:4	O
)	O
that	O
1	O
is	O
an	O
eigenvector	O
with	O
eigenvalue	O
0.	O
furthermore	O
,	O
the	O
matrix	O
is	O
symmetric	B
and	O
positive	O
semi-deﬁnite	O
.	O
to	O
see	O
this	O
,	O
note	O
that	O
f	O
t	O
lf	O
=	O
f	O
t	O
df	O
−	O
f	O
t	O
wf	O
=	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
fifjwij	O
(	O
25.44	O
)	O
i	O
−	O
dif	O
2	O
i	O
,	O
j	O
i	O
⎛	O
⎝	O
(	O
cid:4	O
)	O
wij	O
(	O
fi	O
−	O
fj	O
)	O
2	O
(	O
25.45	O
)	O
=	O
1	O
2	O
i	O
−	O
2	O
dif	O
2	O
fifjwij	O
+	O
djf	O
2	O
j	O
i	O
i	O
,	O
j	O
j	O
⎞	O
⎠	O
=	O
1	O
2	O
i	O
,	O
j	O
hence	O
f	O
t	O
lf	O
≥	O
0	O
for	O
all	O
f	O
∈	O
r	O
eigenvalues	O
,	O
0	O
≤	O
λ1	O
≤	O
λ2	O
≤	O
.	O
.	O
.	O
≤	O
λn	O
.	O
n	O
.	O
consequently	O
we	O
see	O
that	O
l	O
has	O
n	O
non-negative	O
,	O
real-valued	O
to	O
get	O
some	O
intuition	O
as	O
to	O
why	O
l	O
might	O
be	O
useful	O
for	O
graph-based	O
clustering	B
,	O
we	O
note	O
the	O
following	O
result	O
.	O
theorem	O
25.4.1.	O
the	O
set	O
of	O
eigenvectors	O
of	O
l	O
with	O
eigenvalue	O
0	O
is	O
spanned	O
by	O
the	O
indicator	O
vectors	O
1a1	O
,	O
.	O
.	O
.	O
,	O
1ak	O
,	O
where	O
ak	O
are	O
the	O
k	O
connected	O
components	O
of	O
the	O
graph	B
.	O
892	O
chapter	O
25.	O
clustering	B
(	O
cid:7	O
)	O
proof	O
.	O
let	O
us	O
start	O
with	O
the	O
case	O
k	O
=	O
1.	O
if	O
f	O
is	O
an	O
eigenvector	O
with	O
eigenvalue	O
0	O
,	O
then	O
ij	O
wij	O
(	O
fi	O
−	O
fj	O
)	O
2.	O
if	O
two	O
nodes	B
are	O
connected	O
,	O
so	O
wij	O
>	O
0	O
,	O
we	O
must	O
have	O
that	O
fi	O
=	O
fj	O
.	O
0	O
=	O
hence	O
f	O
is	O
constant	O
for	O
all	O
vertices	O
which	O
are	O
connected	O
by	O
a	O
path	B
in	O
the	O
graph	B
.	O
now	O
suppose	O
k	O
>	O
1.	O
in	O
this	O
case	O
,	O
l	O
will	O
be	O
block	O
diagonal	B
.	O
a	O
similar	B
argument	O
to	O
the	O
above	O
shows	O
that	O
we	O
will	O
have	O
k	O
indicator	O
functions	O
,	O
which	O
“	O
select	O
out	O
”	O
the	O
connected	O
components	O
.	O
this	O
suggests	O
the	O
following	O
algorithm	O
.	O
compute	O
the	O
ﬁrst	O
k	O
eigenvectors	O
uk	O
of	O
l.	O
let	O
u	O
=	O
[	O
u1	O
,	O
.	O
.	O
.	O
,	O
uk	O
]	O
be	O
an	O
n	O
×	O
k	O
matrix	O
with	O
the	O
eigenvectors	O
in	O
its	O
columns	O
.	O
let	O
yi	O
∈	O
r	O
k	O
be	O
the	O
i	O
’	O
th	O
row	O
of	O
u.	O
since	O
these	O
yi	O
will	O
be	O
piecewise	O
constant	O
,	O
we	O
can	O
apply	O
k-means	O
clustering	B
to	O
them	O
to	O
recover	O
the	O
connected	O
components	O
.	O
now	O
assign	O
point	O
i	O
to	O
cluster	O
k	O
iff	B
row	O
i	O
of	O
y	O
was	O
assigned	O
to	O
cluster	O
k.	O
in	O
reality	O
,	O
we	O
do	O
not	O
expect	O
a	O
graph	B
derived	O
from	O
a	O
real	O
similarity	O
matrix	O
to	O
have	O
isolated	O
connected	O
components	O
—	O
that	O
would	O
be	O
too	O
easy	O
.	O
but	O
it	O
is	O
reasonable	O
to	O
suppose	O
the	O
graph	B
is	O
a	O
small	O
“	O
perturbation	O
”	O
from	O
such	O
an	O
ideal	O
.	O
in	O
this	O
case	O
,	O
one	O
can	O
use	O
results	O
from	O
perturbation	B
theory	I
to	O
show	O
that	O
the	O
eigenvectors	O
of	O
the	O
perturbed	O
laplacian	O
will	O
be	O
close	O
to	O
these	O
ideal	O
indicator	O
functions	O
(	O
ng	O
et	O
al	O
.	O
2001	O
)	O
.	O
note	O
that	O
this	O
approach	O
is	O
related	O
to	O
kernel	B
pca	O
(	O
section	O
14.4.4	O
)	O
.	O
in	O
particular	O
,	O
kpca	O
uses	O
the	O
largest	O
eigenvectors	O
of	O
w	O
;	O
these	O
are	O
equivalent	O
to	O
the	O
smallest	O
eigenvectors	O
of	O
i	O
−	O
w.	O
this	O
is	O
similar	B
to	O
the	O
above	O
method	O
,	O
which	O
computes	O
the	O
smallest	O
eigenvectors	O
of	O
l	O
=	O
d	O
−	O
w.	O
see	O
in	O
practice	O
,	O
spectral	B
clustering	I
gives	O
much	O
better	O
results	O
than	O
(	O
bengio	O
et	O
al	O
.	O
2004	O
)	O
for	O
details	O
.	O
kpca	O
.	O
25.4.2	O
normalized	O
graph	O
laplacian	O
in	O
practice	O
,	O
it	O
is	O
important	O
to	O
normalize	O
the	O
graph	B
laplacian	O
,	O
to	O
account	O
for	O
the	O
fact	O
that	O
some	O
nodes	B
are	O
more	O
highly	O
connected	O
than	O
others	O
.	O
there	O
are	O
two	O
comon	O
ways	O
to	O
do	O
this	O
.	O
one	O
method	O
,	O
used	O
in	O
e.g.	O
,	O
(	O
shi	O
and	O
malik	O
2000	O
;	O
meila	O
2001	O
)	O
,	O
creates	O
a	O
stochastic	B
matrix	I
where	O
each	O
row	O
sums	O
to	O
one	O
:	O
lrw	O
(	O
cid:2	O
)	O
d−1l	O
=	O
i	O
−	O
d−1w	O
(	O
25.46	O
)	O
the	O
eigenvalues	O
and	O
eigenvectors	O
of	O
l	O
and	O
lrw	O
are	O
closely	O
related	O
to	O
each	O
other	O
(	O
see	O
(	O
von	O
luxburg	O
2007	O
)	O
for	O
details	O
)	O
.	O
furthemore	O
,	O
one	O
can	O
show	O
that	O
for	O
lrw	O
,	O
the	O
eigenspace	O
of	O
0	O
is	O
again	O
spanned	O
by	O
the	O
indicator	O
vectors	O
1ak	O
.	O
this	O
suggests	O
the	O
following	O
algorithm	O
:	O
ﬁnd	O
the	O
smallest	O
k	O
eigenvectors	O
of	O
lrw	O
,	O
create	O
u	O
,	O
cluster	O
the	O
rows	O
of	O
u	O
using	O
k-means	O
,	O
then	O
infer	O
the	O
partitioning	B
of	O
the	O
original	O
points	O
(	O
shi	O
and	O
malik	O
2000	O
)	O
.	O
(	O
note	O
that	O
the	O
eigenvectors/	O
values	O
of	O
lrw	O
are	O
equivalent	O
to	O
the	O
generalized	O
eigenvectors/	O
values	O
of	O
l	O
,	O
which	O
solve	O
lu	O
=	O
λdu	O
.	O
)	O
2	O
ld−	O
1	O
(	O
k	O
u2	O
2	O
wd−	O
1	O
2	O
2	O
=	O
i	O
−	O
d−	O
1	O
another	O
method	O
,	O
used	O
in	O
e.g.	O
,	O
(	O
ng	O
et	O
al	O
.	O
2001	O
)	O
,	O
creates	O
a	O
symmetric	B
matrix	O
lsym	O
(	O
cid:2	O
)	O
d−	O
1	O
''	O
(	O
cid:7	O
)	O
this	O
time	O
the	O
eigenspace	O
of	O
0	O
is	O
spanned	O
by	O
d	O
1	O
2	O
1ak	O
.	O
this	O
suggest	O
the	O
following	O
algorithm	O
:	O
ﬁnd	O
the	O
smallest	O
k	O
eigenvectors	O
of	O
lsym	O
,	O
create	O
u	O
,	O
normalize	O
each	O
row	O
to	O
unit	O
norm	O
by	O
creating	O
tij	O
=	O
uij/	O
ik	O
)	O
,	O
cluster	O
the	O
rows	O
of	O
t	O
using	O
k-means	O
,	O
then	O
infer	O
the	O
partitioning	B
of	O
the	O
original	O
points	O
(	O
ng	O
et	O
al	O
.	O
2001	O
)	O
.	O
there	O
is	O
an	O
interesting	O
connection	O
between	O
ncuts	O
and	O
random	O
walks	O
on	O
a	O
graph	B
(	O
meila	O
2001	O
)	O
.	O
first	O
note	O
that	O
p	O
=	O
d−1w	O
=	O
i	O
−	O
lrw	O
is	O
a	O
stochastic	B
matrix	I
,	O
where	O
pij	O
=	O
wij/di	O
(	O
25.47	O
)	O
25.5.	O
hierarchical	B
clustering	I
893	O
y	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
−6	O
k−means	O
clustering	B
2	O
4	O
6	O
−4	O
−2	O
0	O
x	O
(	O
a	O
)	O
y	O
5	O
4	O
3	O
2	O
1	O
0	O
−1	O
−2	O
−3	O
−4	O
−5	O
−6	O
spectral	B
clustering	I
2	O
4	O
6	O
−4	O
−2	O
0	O
x	O
(	O
b	O
)	O
figure	O
25.11	O
clustering	B
data	O
consisting	O
of	O
2	O
spirals	O
.	O
(	O
a	O
)	O
k-means	O
.	O
(	O
b	O
)	O
spectral	B
clustering	I
.	O
figure	O
generated	O
by	O
spectralclusteringdemo	O
,	O
written	O
by	O
wei-lwun	O
lu	O
.	O
can	O
be	O
interpreted	O
as	O
the	O
probability	O
of	O
going	O
from	O
i	O
to	O
j.	O
if	O
the	O
graph	B
is	O
connected	O
and	O
non-bipartite	O
,	O
it	O
possesses	O
a	O
unique	O
stationary	B
distribution	I
π	O
=	O
(	O
π1	O
,	O
.	O
.	O
.	O
,	O
πn	O
)	O
,	O
where	O
πi	O
=	O
di/vol	O
(	O
v	O
)	O
.	O
furthermore	O
,	O
one	O
can	O
show	O
that	O
ncut	O
(	O
a	O
,	O
a	O
)	O
=	O
p	O
(	O
a|a	O
)	O
+p	O
(	O
a|a	O
)	O
(	O
25.48	O
)	O
this	O
means	O
that	O
we	O
are	O
looking	O
for	O
a	O
cut	O
such	O
that	O
a	O
random	O
walk	O
rarely	O
makes	O
transitions	O
from	O
a	O
to	O
a	O
or	O
vice	O
versa	O
.	O
25.4.3	O
example	O
figure	O
25.11	O
illustrates	O
the	O
method	O
in	O
action	B
.	O
in	O
figure	O
25.11	O
(	O
a	O
)	O
,	O
we	O
see	O
that	O
k-means	O
does	O
a	O
poor	O
job	O
of	O
clustering	B
,	O
since	O
it	O
implicitly	O
assumes	O
each	O
cluster	O
corresponds	O
to	O
a	O
spherical	B
gaussian	O
.	O
next	O
we	O
try	O
spectral	B
clustering	I
.	O
we	O
deﬁne	O
a	O
similarity	O
matrix	O
using	O
the	O
gaussian	O
kernel	B
.	O
we	O
compute	O
the	O
ﬁrst	O
two	O
eigenvectors	O
of	O
the	O
laplacian	O
.	O
from	O
this	O
we	O
can	O
infer	O
the	O
clustering	B
in	O
figure	O
25.11	O
(	O
b	O
)	O
.	O
since	O
the	O
method	O
is	O
based	O
on	O
ﬁnding	O
the	O
smallest	O
k	O
eigenvectors	O
of	O
a	O
sparse	O
matrix	O
,	O
it	O
takes	O
o	O
(	O
n	O
3	O
)	O
time	O
.	O
however	O
,	O
a	O
variety	O
of	O
methods	O
can	O
be	O
used	O
to	O
scale	O
it	O
up	O
for	O
large	O
datasets	O
(	O
see	O
e.g.	O
,	O
(	O
yan	O
et	O
al	O
.	O
2009	O
)	O
)	O
.	O
25.5	O
hierarchical	B
clustering	I
mixture	O
models	O
,	O
whether	O
ﬁnite	O
or	O
inﬁnite	O
,	O
produce	O
a	O
“	O
ﬂat	O
”	O
clustering	B
.	O
often	O
we	O
want	O
to	O
learn	O
a	O
hierarchical	B
clustering	I
,	O
where	O
clusters	B
can	O
be	O
nested	O
inside	O
each	O
other	O
.	O
there	O
are	O
two	O
main	O
approaches	O
to	O
hierarchical	B
clustering	I
:	O
bottom-up	O
or	O
agglomerative	O
clus-	O
tering	O
,	O
and	O
top-down	O
or	O
divisive	B
clustering	I
.	O
both	O
methods	O
take	O
as	O
input	O
a	O
dissimilarity	B
matrix	I
between	O
the	O
objects	O
.	O
in	O
the	O
bottom-up	O
approach	O
,	O
the	O
most	O
similar	B
groups	O
are	O
merged	O
at	O
each	O
894	O
chapter	O
25.	O
clustering	B
2	O
1	O
3	O
5	O
4.5	O
4	O
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
0	O
1	O
2	O
3	O
5	O
4	O
4	O
(	O
a	O
)	O
2.5	O
2	O
1.5	O
1	O
5	O
6	O
7	O
8	O
4	O
5	O
3	O
2	O
1	O
(	O
b	O
)	O
figure	O
25.12	O
(	O
a	O
)	O
an	O
example	O
of	O
single	B
link	I
clustering	I
using	O
city	B
block	I
distance	I
.	O
pairs	O
(	O
1,3	O
)	O
and	O
(	O
4,5	O
)	O
are	O
both	O
distance	O
1	O
apart	O
,	O
so	O
get	O
merged	O
ﬁrst	O
.	O
(	O
b	O
)	O
the	O
resulting	O
dendrogram	B
.	O
based	O
on	O
figure	O
7.5	O
of	O
(	O
alpaydin	O
2004	O
)	O
.	O
figure	O
generated	O
by	O
agglomdemo	O
.	O
hierarchical	B
clustering	I
of	O
profiles	O
0	O
−1	O
−2	O
−3	O
0	O
2	O
0	O
−2	O
0	O
2	O
0	O
−2	O
−4	O
0	O
2	O
1.5	O
1	O
0.5	O
10	O
20	O
10	O
20	O
10	O
20	O
0	O
10	O
20	O
0	O
−1	O
−2	O
0	O
2	O
0	O
−2	O
0	O
4	O
2	O
0	O
−2	O
0	O
2	O
1	O
0	O
0	O
−0.5	O
−1	O
−1.5	O
−2	O
−2.5	O
0	O
0	O
−2	O
−4	O
0	O
2	O
1	O
0	O
−1	O
0	O
0	O
−1	O
−2	O
0	O
10	O
20	O
10	O
20	O
10	O
20	O
10	O
20	O
(	O
b	O
)	O
1	O
0	O
−1	O
−2	O
−3	O
0	O
4	O
2	O
0	O
−2	O
0	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
2	O
1	O
0	O
−1	O
0	O
10	O
20	O
10	O
20	O
10	O
20	O
10	O
20	O
10	O
20	O
10	O
20	O
10	O
20	O
10	O
20	O
(	O
a	O
)	O
figure	O
25.13	O
hierarchical	B
clustering	I
applied	O
to	O
the	O
yeast	O
gene	O
expression	O
data	O
.	O
(	O
a	O
)	O
the	O
rows	O
are	O
permuted	O
according	O
to	O
a	O
hierarchical	B
clustering	I
scheme	O
(	O
average	O
link	O
agglomerative	O
clustering	B
)	O
,	O
in	O
order	O
to	O
bring	O
similar	B
rows	O
close	O
together	O
.	O
(	O
b	O
)	O
16	O
clusters	B
induced	O
by	O
cutting	O
the	O
average	O
linkage	O
tree	B
at	O
a	O
certain	O
height	O
.	O
figure	O
generated	O
by	O
hclustyeastdemo	O
.	O
in	O
the	O
top-down	O
approach	O
,	O
groups	O
are	O
split	O
using	O
various	O
different	O
criteria	O
.	O
we	O
give	O
the	O
step	O
.	O
details	O
below	O
.	O
note	O
that	O
agglomerative	O
and	O
divisive	B
clustering	I
are	O
both	O
just	O
heuristics	B
,	O
which	O
do	O
not	O
optimize	O
any	O
well-deﬁned	O
objective	O
function	O
.	O
thus	O
it	O
is	O
hard	O
to	O
assess	O
the	O
quality	O
of	O
the	O
clustering	B
they	O
produce	O
in	O
any	O
formal	O
sense	O
.	O
furthermore	O
,	O
they	O
will	O
always	O
produce	O
a	O
clustering	B
of	O
the	O
input	O
data	O
,	O
even	O
if	O
the	O
data	O
has	O
no	O
structure	O
at	O
all	O
(	O
e.g.	O
,	O
it	O
is	O
random	O
noise	O
)	O
.	O
later	O
in	O
this	O
section	O
we	O
will	O
discuss	O
a	O
probabilistic	O
version	O
of	O
hierarchical	B
clustering	I
that	O
solves	O
both	O
these	O
problems	O
.	O
25.5.	O
hierarchical	B
clustering	I
895	O
algorithm	O
25.2	O
:	O
agglomerative	B
clustering	I
1	O
initialize	O
clusters	B
as	O
singletons	O
:	O
for	O
i	O
←	O
1	O
to	O
n	O
do	O
ci	O
←	O
{	O
i	O
}	O
;	O
2	O
initialize	O
set	O
of	O
clusters	B
available	O
for	O
merging	O
:	O
s	O
←	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
;	O
3	O
repeat	O
4	O
pick	O
2	O
most	O
similar	B
clusters	O
to	O
merge	O
:	O
(	O
j	O
,	O
k	O
)	O
←	O
arg	O
minj	O
,	O
k∈s	O
dj	O
,	O
k	O
;	O
create	O
new	O
cluster	O
c	O
(	O
cid:8	O
)	O
←	O
cj	O
∪	O
ck	O
;	O
mark	O
j	O
and	O
k	O
as	O
unavailable	O
:	O
s	O
←	O
s	O
\	O
{	O
j	O
,	O
k	O
}	O
;	O
if	O
c	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
then	O
foreach	O
i	O
∈	O
s	O
do	O
mark	O
(	O
cid:2	O
)	O
as	O
available	O
,	O
s	O
←	O
s	O
∪	O
{	O
(	O
cid:2	O
)	O
}	O
;	O
5	O
6	O
7	O
8	O
9	O
10	O
update	O
dissimilarity	B
matrix	I
d	O
(	O
i	O
,	O
(	O
cid:2	O
)	O
)	O
;	O
11	O
until	O
no	O
more	O
clusters	B
are	O
available	O
for	O
merging	O
;	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
25.14	O
illustration	O
of	O
(	O
a	O
)	O
single	O
linkage	O
.	O
(	O
b	O
)	O
complete	B
linkage	O
.	O
(	O
c	O
)	O
average	O
linkage	O
.	O
25.5.1	O
agglomerative	B
clustering	I
agglomerative	O
clustering	B
starts	O
with	O
n	O
groups	O
,	O
each	O
initially	O
containing	O
one	O
object	O
,	O
and	O
then	O
at	O
each	O
step	O
it	O
merges	O
the	O
two	O
most	O
similar	B
groups	O
until	O
there	O
is	O
a	O
single	O
group	O
,	O
containing	O
all	O
the	O
data	O
.	O
see	O
algorithm	O
11	O
for	O
the	O
pseudocode	O
.	O
since	O
picking	O
the	O
two	O
most	O
similar	B
clusters	O
to	O
merge	O
takes	O
o	O
(	O
n	O
2	O
)	O
time	O
,	O
and	O
there	O
are	O
o	O
(	O
n	O
)	O
steps	O
in	O
the	O
algorithm	O
,	O
the	O
total	O
running	O
time	O
is	O
o	O
(	O
n	O
3	O
)	O
.	O
however	O
,	O
by	O
using	O
a	O
priority	O
queue	O
,	O
this	O
can	O
be	O
reduced	O
to	O
o	O
(	O
n	O
2	O
log	O
n	O
)	O
(	O
see	O
e.g.	O
,	O
(	O
manning	O
et	O
al	O
.	O
2008	O
,	O
ch	O
.	O
17	O
)	O
for	O
details	O
)	O
.	O
for	O
large	O
n	O
,	O
a	O
common	O
heuristic	O
is	O
to	O
ﬁrst	O
run	O
k-means	O
,	O
which	O
takes	O
o	O
(	O
kn	O
d	O
)	O
time	O
,	O
and	O
then	O
apply	O
hierarchical	B
clustering	I
to	O
the	O
estimated	O
cluster	O
centers	O
.	O
the	O
merging	O
process	O
can	O
be	O
represented	O
by	O
a	O
binary	B
tree	I
,	O
called	O
a	O
dendrogram	B
,	O
as	O
shown	O
in	O
figure	O
25.12	O
(	O
b	O
)	O
.	O
the	O
initial	O
groups	O
(	O
objects	O
)	O
are	O
at	O
the	O
leaves	B
(	O
at	O
the	O
bottom	O
of	O
the	O
ﬁgure	O
)	O
,	O
and	O
every	O
time	O
two	O
groups	O
are	O
merged	O
,	O
we	O
join	O
them	O
in	O
the	O
tree	B
.	O
the	O
height	O
of	O
the	O
branches	O
represents	O
the	O
dissimilarity	O
between	O
the	O
groups	O
that	O
are	O
being	O
joined	O
.	O
the	O
root	B
of	O
the	O
tree	B
(	O
which	O
is	O
at	O
the	O
top	O
)	O
represents	O
a	O
group	O
containing	O
all	O
the	O
data	O
.	O
if	O
we	O
cut	O
the	O
tree	B
at	O
any	O
given	O
height	O
,	O
we	O
induce	O
a	O
clustering	B
of	O
a	O
given	O
size	O
.	O
for	O
example	O
,	O
if	O
we	O
cut	O
the	O
tree	B
in	O
figure	O
25.12	O
(	O
b	O
)	O
at	O
height	O
2	O
,	O
we	O
get	O
the	O
clustering	B
{	O
{	O
{	O
4	O
,	O
5	O
}	O
,	O
{	O
1	O
,	O
3	O
}	O
}	O
,	O
{	O
2	O
}	O
}	O
.	O
we	O
discuss	O
the	O
issue	O
of	O
how	O
to	O
choose	O
the	O
height/	O
number	O
of	O
clusters	B
below	O
.	O
a	O
more	O
complex	O
example	O
is	O
shown	O
in	O
figure	O
25.13	O
(	O
a	O
)	O
,	O
where	O
we	O
show	O
some	O
gene	O
expression	O
if	O
we	O
cut	O
the	O
tree	B
in	O
figure	O
25.13	O
(	O
a	O
)	O
at	O
a	O
certain	O
height	O
,	O
we	O
get	O
the	O
16	O
clusters	B
shown	O
in	O
data	O
.	O
figure	O
25.13	O
(	O
b	O
)	O
.	O
there	O
are	O
actually	O
three	O
variants	O
of	O
agglomerative	B
clustering	I
,	O
depending	O
on	O
how	O
we	O
deﬁne	O
the	O
dissimilarity	O
between	O
groups	O
of	O
objects	O
.	O
these	O
can	O
give	O
quite	O
different	O
results	O
,	O
as	O
shown	O
in	O
896	O
chapter	O
25.	O
clustering	B
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
2	O
1.8	O
1.6	O
1.4	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
1.8	O
1.6	O
1.4	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
single	O
link	O
(	O
a	O
)	O
complete	O
link	O
(	O
b	O
)	O
average	O
link	O
(	O
c	O
)	O
figure	O
25.15	O
hierarchical	B
clustering	I
of	O
yeast	O
gene	O
expression	O
data	O
.	O
(	O
a	O
)	O
single	O
linkage	O
.	O
(	O
b	O
)	O
complete	B
linkage	O
.	O
(	O
c	O
)	O
average	O
linkage	O
.	O
figure	O
generated	O
by	O
hclustyeastdemo	O
.	O
25.5.	O
hierarchical	B
clustering	I
897	O
figure	O
25.15.	O
we	O
give	O
the	O
details	O
below	O
.	O
25.5.1.1	O
single	O
link	O
in	O
single	B
link	I
clustering	I
,	O
also	O
called	O
nearest	B
neighbor	I
clustering	I
,	O
the	O
distance	O
between	O
two	O
groups	O
g	O
and	O
h	O
is	O
deﬁned	O
as	O
the	O
distance	O
between	O
the	O
two	O
closest	O
members	O
of	O
each	O
group	O
:	O
dsl	O
(	O
g	O
,	O
h	O
)	O
=	O
min	O
i∈g	O
,	O
i	O
(	O
cid:2	O
)	O
∈h	O
di	O
,	O
i	O
(	O
cid:2	O
)	O
see	O
figure	O
25.14	O
(	O
a	O
)	O
.	O
(	O
25.49	O
)	O
the	O
tree	B
built	O
using	O
single	B
link	I
clustering	I
is	O
a	O
minimum	B
spanning	I
tree	I
of	O
the	O
data	O
,	O
which	O
is	O
a	O
tree	B
that	O
connects	O
all	O
the	O
objects	O
in	O
a	O
way	O
that	O
minimizes	O
the	O
sum	O
of	O
the	O
edge	O
weights	O
(	O
distances	O
)	O
.	O
to	O
see	O
this	O
,	O
note	O
that	O
when	O
we	O
merge	O
two	O
clusters	B
,	O
we	O
connect	O
together	O
the	O
two	O
closest	O
members	O
of	O
the	O
clusters	B
;	O
this	O
adds	O
an	O
edge	O
between	O
the	O
corresponding	O
nodes	B
,	O
and	O
this	O
is	O
guaranteed	O
to	O
be	O
the	O
“	O
lightest	O
weight	O
”	O
edge	O
joining	O
these	O
two	O
clusters	B
.	O
and	O
once	O
two	O
clusters	B
have	O
been	O
merged	O
,	O
they	O
will	O
never	O
be	O
considered	O
again	O
,	O
so	O
we	O
can	O
not	O
create	O
cycles	O
.	O
as	O
a	O
consequence	O
of	O
this	O
,	O
we	O
can	O
actually	O
implement	O
single	B
link	I
clustering	I
in	O
o	O
(	O
n	O
2	O
)	O
time	O
,	O
whereas	O
the	O
other	O
variants	O
take	O
o	O
(	O
n	O
3	O
)	O
time	O
.	O
25.5.1.2	O
complete	O
link	O
in	O
complete	B
link	I
clustering	I
,	O
also	O
called	O
furthest	B
neighbor	I
clustering	I
,	O
the	O
distance	O
between	O
two	O
groups	O
is	O
deﬁned	O
as	O
the	O
distance	O
between	O
the	O
two	O
most	O
distant	O
pairs	O
:	O
dcl	O
(	O
g	O
,	O
h	O
)	O
=	O
max	O
i∈g	O
,	O
i	O
(	O
cid:2	O
)	O
∈h	O
di	O
,	O
i	O
(	O
cid:2	O
)	O
see	O
figure	O
25.14	O
(	O
b	O
)	O
.	O
(	O
25.50	O
)	O
single	O
linkage	O
only	O
requires	O
that	O
a	O
single	O
pair	O
of	O
objects	O
be	O
close	O
for	O
the	O
two	O
groups	O
to	O
be	O
considered	O
close	O
together	O
,	O
regardless	O
of	O
the	O
similarity	O
of	O
the	O
other	O
members	O
of	O
the	O
group	O
.	O
thus	O
clusters	B
can	O
be	O
formed	O
that	O
violate	O
the	O
compactness	B
property	O
,	O
which	O
says	O
that	O
all	O
the	O
observations	O
within	O
a	O
group	O
should	O
be	O
similar	B
to	O
each	O
other	O
.	O
in	O
particular	O
if	O
we	O
deﬁne	O
the	O
diameter	B
of	O
a	O
group	O
as	O
the	O
largest	O
dissimilarity	O
of	O
its	O
members	O
,	O
dg	O
=	O
maxi∈g	O
,	O
i	O
(	O
cid:2	O
)	O
∈g	O
di	O
,	O
i	O
(	O
cid:2	O
)	O
,	O
then	O
we	O
can	O
see	O
that	O
single	O
linkage	O
can	O
produce	O
clusters	B
with	O
large	O
diameters	O
.	O
complete	B
linkage	O
represents	O
the	O
opposite	O
extreme	O
:	O
two	O
groups	O
are	O
considered	O
close	O
only	O
if	O
all	O
of	O
the	O
observations	O
in	O
their	O
union	O
are	O
relatively	O
similar	B
.	O
this	O
will	O
tend	O
to	O
produce	O
clusterings	O
with	O
small	O
diameter	O
,	O
i.e.	O
,	O
compact	O
clusters	B
.	O
25.5.1.3	O
average	O
link	O
in	O
practice	O
,	O
the	O
preferred	O
method	O
is	O
average	B
link	I
clustering	I
,	O
which	O
measures	O
the	O
average	O
distance	O
between	O
all	B
pairs	I
:	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
ngnh	O
i∈g	O
i	O
(	O
cid:2	O
)	O
∈h	O
davg	O
(	O
g	O
,	O
h	O
)	O
=	O
1	O
di	O
,	O
i	O
(	O
cid:2	O
)	O
(	O
25.51	O
)	O
where	O
ng	O
and	O
nh	O
are	O
the	O
number	O
of	O
elements	O
in	O
groups	O
g	O
and	O
h.	O
see	O
figure	O
25.14	O
(	O
c	O
)	O
.	O
average	B
link	I
clustering	I
represents	O
a	O
compromise	O
between	O
single	O
and	O
complete	B
link	I
clustering	I
.	O
it	O
tends	O
to	O
produce	O
relatively	O
compact	O
clusters	B
that	O
are	O
relatively	O
far	O
apart	O
.	O
however	O
,	O
since	O
it	O
898	O
chapter	O
25.	O
clustering	B
involves	O
averaging	O
of	O
the	O
di	O
,	O
i	O
(	O
cid:2	O
)	O
’	O
s	O
,	O
any	O
change	O
to	O
the	O
measurement	O
scale	O
can	O
change	O
the	O
result	O
.	O
in	O
contrast	O
,	O
single	O
linkage	O
and	O
complete	B
linkage	O
are	O
invariant	B
to	O
monotonic	O
transformations	O
of	O
di	O
,	O
i	O
(	O
cid:2	O
)	O
,	O
since	O
they	O
leave	O
the	O
relative	O
ordering	O
the	O
same	O
.	O
25.5.2	O
divisive	B
clustering	I
divisive	O
clustering	B
starts	O
with	O
all	O
the	O
data	O
in	O
a	O
single	O
cluster	O
,	O
and	O
then	O
recursively	O
divides	O
each	O
cluster	O
into	O
two	O
daughter	O
clusters	B
,	O
in	O
a	O
top-down	O
fashion	O
.	O
since	O
there	O
are	O
2n−1	O
−	O
1	O
ways	O
to	O
split	O
a	O
group	O
of	O
n	O
items	O
into	O
2	O
groups	O
,	O
it	O
is	O
hard	O
to	O
compute	O
the	O
optimal	O
split	O
,	O
so	O
various	O
heuristics	B
are	O
used	O
.	O
one	O
approach	O
is	O
pick	O
the	O
cluster	O
with	O
the	O
largest	O
diameter	B
,	O
and	O
split	O
it	O
in	O
two	O
using	O
the	O
k-means	O
or	O
k-medoids	O
algorithm	O
with	O
k	O
=	O
2.	O
this	O
is	O
called	O
the	O
bisecting	O
k-means	O
algorithm	O
(	O
steinbach	O
et	O
al	O
.	O
2000	O
)	O
.	O
we	O
can	O
repeat	O
this	O
until	O
we	O
have	O
any	O
desired	O
number	O
of	O
clusters	B
.	O
this	O
can	O
be	O
used	O
as	O
an	O
alternative	O
to	O
regular	B
k-means	O
,	O
but	O
it	O
also	O
induces	O
a	O
hierarchical	B
clustering	I
.	O
another	O
method	O
is	O
to	O
build	O
a	O
minimum	B
spanning	I
tree	I
from	O
the	O
dissimilarity	O
graph	O
,	O
and	O
then	O
(	O
this	O
to	O
make	O
new	O
clusters	B
by	O
breaking	O
the	O
link	O
corresponding	O
to	O
the	O
largest	O
dissimilarity	O
.	O
actually	O
gives	O
the	O
same	O
results	O
as	O
single	O
link	O
agglomerative	O
clustering	B
.	O
)	O
is	O
as	O
follows	O
.	O
we	O
start	O
with	O
a	O
single	O
cluster	O
containing	O
all	O
the	O
data	O
,	O
g	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
.	O
we	O
then	O
measure	O
the	O
average	O
dissimilarity	O
of	O
i	O
∈	O
g	O
to	O
all	O
the	O
other	O
i	O
(	O
cid:2	O
)	O
∈	O
g	O
:	O
yet	O
another	O
method	O
,	O
called	O
dissimilarity	B
analysis	I
(	O
macnaughton-smith	O
et	O
al	O
.	O
1964	O
)	O
,	O
(	O
cid:4	O
)	O
i	O
(	O
cid:2	O
)	O
∈g	O
dg	O
i	O
=	O
1	O
ng	O
di	O
,	O
i	O
(	O
cid:2	O
)	O
(	O
25.52	O
)	O
(	O
25.53	O
)	O
(	O
25.54	O
)	O
we	O
remove	O
the	O
most	O
dissimilar	O
object	O
and	O
put	O
it	O
in	O
its	O
own	O
cluster	O
h	O
:	O
i∗	O
=	O
arg	O
max	O
i∈g	O
i	O
,	O
g	O
=	O
g	O
\	O
{	O
i∗	O
}	O
,	O
h	O
=	O
{	O
i∗	O
}	O
dg	O
we	O
now	O
continue	O
to	O
move	O
objects	O
from	O
g	O
to	O
h	O
until	O
some	O
stopping	O
criterion	O
is	O
met	O
.	O
speciﬁcally	O
,	O
to	O
move	O
that	O
maximizes	O
the	O
average	O
dissimilarity	O
to	O
each	O
i	O
(	O
cid:2	O
)	O
∈	O
g	O
but	O
minimizes	O
we	O
pick	O
a	O
point	O
i∗	O
(	O
cid:4	O
)	O
the	O
average	O
dissimilarity	O
to	O
each	O
i	O
(	O
cid:2	O
)	O
∈	O
h	O
:	O
dh	O
i	O
=	O
1	O
nh	O
i	O
(	O
cid:2	O
)	O
∈h	O
di	O
,	O
i	O
(	O
cid:2	O
)	O
,	O
i∗	O
i	O
−	O
dh	O
dg	O
i	O
=	O
arg	O
max	O
i∈g	O
i	O
−	O
dh	O
i	O
we	O
continue	O
to	O
do	O
this	O
until	O
dg	O
is	O
negative	O
.	O
the	O
ﬁnal	O
result	O
is	O
that	O
we	O
have	O
split	O
g	O
into	O
two	O
daughter	O
clusters	B
,	O
g	O
and	O
h.	O
we	O
can	O
then	O
recursively	O
call	O
the	O
algorithm	O
on	O
g	O
and/or	O
h	O
,	O
or	O
on	O
any	O
other	O
node	O
in	O
the	O
tree	B
.	O
for	O
example	O
,	O
we	O
might	O
choose	O
to	O
split	O
the	O
node	O
g	O
whose	O
average	O
dissimilarity	O
is	O
highest	O
,	O
or	O
whose	O
maximum	O
dissimilarity	O
(	O
i.e.	O
,	O
diameter	B
)	O
is	O
highest	O
.	O
we	O
continue	O
the	O
process	O
until	O
the	O
average	O
dissimilarity	O
within	O
each	O
cluster	O
is	O
below	O
some	O
threshold	O
,	O
and/or	O
all	O
clusters	O
are	O
singletons	O
.	O
divisive	B
clustering	I
is	O
less	O
popular	O
than	O
agglomerative	B
clustering	I
,	O
but	O
it	O
has	O
two	O
advantages	O
.	O
first	O
,	O
it	O
can	O
be	O
faster	O
,	O
since	O
if	O
we	O
only	O
split	O
for	O
a	O
constant	O
number	O
of	O
levels	O
,	O
it	O
takes	O
just	O
o	O
(	O
n	O
)	O
time	O
.	O
second	O
,	O
the	O
splitting	O
decisions	O
are	O
made	O
in	O
the	O
context	O
of	O
seeing	O
all	O
the	O
data	O
,	O
whereas	O
bottom-up	O
methods	O
make	O
myopic	O
merge	O
decisions	O
.	O
25.5.	O
hierarchical	B
clustering	I
899	O
25.5.3	O
choosing	O
the	O
number	O
of	O
clusters	B
it	O
is	O
difficult	O
to	O
choose	O
the	O
“	O
right	O
”	O
number	O
of	O
clusters	B
,	O
since	O
a	O
hierarchical	B
clustering	I
algorithm	O
will	O
always	O
create	O
a	O
hierarchy	O
,	O
even	O
if	O
the	O
data	O
is	O
completely	O
random	O
.	O
but	O
,	O
as	O
with	O
choosing	O
k	O
for	O
k-means	O
,	O
there	O
is	O
the	O
hope	O
that	O
there	O
will	O
be	O
a	O
visible	B
“	O
gap	O
”	O
in	O
the	O
lengths	O
of	O
the	O
links	O
in	O
the	O
dendrogram	B
(	O
represent	O
the	O
dissimilarity	O
between	O
merged	O
groups	O
)	O
between	O
natural	O
clusters	O
and	O
unnatural	O
clusters	B
.	O
of	O
course	O
,	O
on	O
real	O
data	O
,	O
this	O
gap	O
might	O
be	O
hard	O
to	O
detect	O
.	O
in	O
section	O
25.5.4	O
,	O
we	O
will	O
present	O
a	O
bayesian	O
approach	O
to	O
hierarchical	B
clustering	I
that	O
nicely	O
solves	O
this	O
problem	O
.	O
25.5.4	O
bayesian	O
hierarchical	B
clustering	I
there	O
are	O
several	O
ways	O
to	O
make	O
probabilistic	O
models	O
which	O
produce	O
results	O
similar	B
to	O
hierarchical	B
clustering	I
,	O
e.g.	O
,	O
(	O
williams	O
2000	O
;	O
neal	O
2003b	O
;	O
castro	O
et	O
al	O
.	O
2004	O
;	O
lau	O
and	O
green	O
2006	O
)	O
.	O
here	O
we	O
present	O
one	O
particular	O
approach	O
called	O
bayesian	O
hierarchical	B
clustering	I
(	O
heller	O
and	O
ghahra-	O
mani	O
2005	O
)	O
.	O
algorithmically	O
it	O
is	O
very	O
similar	B
to	O
standard	O
bottom-up	O
agglomerative	B
clustering	I
,	O
and	O
takes	O
comparable	O
time	O
,	O
whereas	O
several	O
of	O
the	O
other	O
techniques	O
referenced	O
above	O
are	O
much	O
slower	O
.	O
however	O
,	O
it	O
uses	O
bayesian	O
hypothesis	O
tests	O
to	O
decide	O
which	O
clusters	B
to	O
merge	O
(	O
if	O
any	O
)	O
,	O
rather	O
than	O
computing	O
the	O
similarity	O
between	O
groups	O
of	O
points	O
in	O
some	O
ad-hoc	O
way	O
.	O
these	O
hypothesis	O
tests	O
are	O
closely	O
related	O
to	O
the	O
calculations	O
required	O
to	O
do	O
inference	O
in	O
a	O
dirichlet	O
process	O
mixture	B
model	I
,	O
as	O
we	O
will	O
see	O
.	O
furthermore	O
,	O
the	O
input	O
to	O
the	O
model	O
is	O
a	O
data	O
matrix	O
,	O
not	O
a	O
dissimilarity	B
matrix	I
.	O
25.5.4.1	O
the	O
algorithm	O
let	O
d	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
represent	O
all	O
the	O
data	O
,	O
and	O
let	O
di	O
be	O
the	O
set	O
of	O
datapoints	O
at	O
the	O
leaves	B
of	O
the	O
substree	O
ti	O
.	O
at	O
each	O
step	O
,	O
we	O
compare	O
two	O
trees	O
ti	O
and	O
tj	O
to	O
see	O
if	O
they	O
should	O
be	O
merged	O
into	O
a	O
new	O
tree	B
.	O
deﬁne	O
dij	O
as	O
their	O
merged	O
data	O
,	O
and	O
let	O
mij	O
=	O
1	O
if	O
they	O
should	O
be	O
merged	O
,	O
and	O
mij	O
=	O
0	O
otherwise	O
.	O
the	O
probability	O
of	O
a	O
merge	O
is	O
given	O
by	O
p	O
(	O
dij|tij	O
)	O
rij	O
(	O
cid:2	O
)	O
p	O
(	O
dij|mij	O
=	O
1	O
)	O
p	O
(	O
mij	O
=	O
1	O
)	O
(	O
25.55	O
)	O
p	O
(	O
dij|tij	O
)	O
=p	O
(	O
dij|mij	O
=	O
1	O
)	O
p	O
(	O
mij	O
=	O
1	O
)	O
+	O
p	O
(	O
dij|mij	O
=	O
0	O
)	O
p	O
(	O
mij	O
=	O
0	O
)	O
(	O
25.56	O
)	O
here	O
p	O
(	O
mij	O
=	O
1	O
)	O
is	O
the	O
prior	O
probability	O
of	O
a	O
merge	O
,	O
which	O
can	O
be	O
computed	O
using	O
a	O
bottom-up	O
algorithm	O
described	O
below	O
.	O
we	O
now	O
turn	O
to	O
the	O
likelihood	B
terms	O
.	O
if	O
mij	O
=	O
1	O
,	O
the	O
data	O
in	O
dij	O
is	O
assumed	O
to	O
come	O
from	O
the	O
same	O
model	O
,	O
and	O
hence	O
(	O
cid:12	O
)	O
⎡	O
⎣	O
(	O
cid:20	O
)	O
⎤	O
⎦	O
p	O
(	O
θ|λ	O
)	O
dθ	O
p	O
(	O
dij|mij	O
=	O
1	O
)	O
=	O
p	O
(	O
xn|θ	O
)	O
(	O
25.57	O
)	O
if	O
mij	O
=	O
0	O
,	O
the	O
data	O
in	O
dij	O
is	O
assumed	O
to	O
have	O
been	O
generated	O
by	O
each	O
tree	B
independently	O
,	O
so	O
(	O
25.58	O
)	O
p	O
(	O
dij|mij	O
=	O
0	O
)	O
=	O
p	O
(	O
di|ti	O
)	O
p	O
(	O
dj|tj	O
)	O
xn∈dij	O
these	O
two	O
terms	O
will	O
have	O
already	O
been	O
computed	O
by	O
the	O
bottom-up	O
process	O
.	O
consequently	O
we	O
have	O
all	O
the	O
quantities	O
we	O
need	O
to	O
decide	O
which	O
trees	O
to	O
merge	O
.	O
see	O
algorithm	O
9	O
for	O
the	O
pseudocode	O
,	O
assuming	O
p	O
(	O
mij	O
)	O
is	O
uniform	O
.	O
when	O
ﬁnished	O
,	O
we	O
can	O
cut	O
the	O
tree	B
at	O
points	O
where	O
rij	O
<	O
0.5	O
.	O
900	O
chapter	O
25.	O
clustering	B
algorithm	O
25.3	O
:	O
bayesian	O
hierarchical	B
clustering	I
1	O
initialize	O
di	O
=	O
{	O
xi	O
}	O
,	O
i	O
=	O
1	O
:	O
n	O
;	O
2	O
compute	O
p	O
(	O
di|ti	O
)	O
,	O
i	O
=	O
1	O
:	O
n	O
;	O
3	O
repeat	O
4	O
5	O
6	O
7	O
for	O
each	O
pair	O
of	O
clusters	B
i	O
,	O
j	O
do	O
compute	O
p	O
(	O
dij|tij	O
)	O
find	O
the	O
pair	O
di	O
and	O
dj	O
with	O
highest	O
merge	O
probability	O
rij	O
;	O
merge	O
dk	O
:	O
=	O
di	O
∪	O
dj	O
;	O
delete	O
di	O
,	O
dj	O
;	O
8	O
9	O
until	O
all	O
clusters	O
merged	O
;	O
25.5.4.2	O
the	O
connection	O
with	O
dirichlet	O
process	O
mixture	B
models	O
in	O
this	O
section	O
,	O
we	O
will	O
establish	O
the	O
connection	O
between	O
bhc	O
and	O
dpmms	O
.	O
this	O
will	O
in	O
turn	O
give	O
us	O
an	O
algorithm	O
to	O
compute	O
the	O
prior	O
probabilities	O
p	O
(	O
mij	O
=	O
1	O
)	O
.	O
note	O
that	O
the	O
marginal	B
likelihood	I
of	O
a	O
dpmm	O
,	O
summing	O
over	O
all	O
2n	O
−	O
1	O
partitions	O
,	O
is	O
given	O
by	O
p	O
(	O
dk	O
)	O
=	O
(	O
cid:4	O
)	O
(	O
25.59	O
)	O
p	O
(	O
v	O
)	O
p	O
(	O
dv	O
)	O
(	O
cid:26	O
)	O
mv	O
l=1	O
γ	O
(	O
nv	O
l	O
)	O
v∈v	O
αmv	O
mv	O
(	O
cid:20	O
)	O
γ	O
(	O
nk+α	O
)	O
γ	O
(	O
α	O
)	O
p	O
(	O
dv	O
l	O
)	O
p	O
(	O
v	O
)	O
=	O
p	O
(	O
dv	O
)	O
=	O
(	O
25.60	O
)	O
(	O
25.61	O
)	O
l=1	O
l	O
where	O
v	O
is	O
the	O
set	O
of	O
all	O
possible	O
partitions	O
of	O
dk	O
,	O
p	O
(	O
v	O
)	O
is	O
the	O
probability	O
of	O
partition	O
v	O
,	O
mv	O
is	O
is	O
the	O
number	O
of	O
points	O
in	O
cluster	O
l	O
of	O
partition	O
v	O
,	O
dv	O
the	O
number	O
of	O
clusters	B
in	O
partition	O
v	O
,	O
nv	O
are	O
the	O
points	O
in	O
cluster	O
l	O
of	O
partition	O
v	O
,	O
and	O
nk	O
are	O
the	O
number	O
of	O
points	O
in	O
dk	O
.	O
l	O
one	O
can	O
show	O
(	O
heller	O
and	O
ghahramani	O
2005	O
)	O
that	O
p	O
(	O
dk|tk	O
)	O
computed	O
by	O
the	O
bhc	O
algorithm	O
is	O
similar	B
to	O
p	O
(	O
dk	O
)	O
given	O
above	O
,	O
except	O
for	O
the	O
fact	O
that	O
it	O
only	O
sums	O
over	O
partitions	O
which	O
are	O
consistent	B
with	O
tree	B
tk	O
.	O
(	O
the	O
number	O
of	O
tree-consistent	O
partitions	O
is	O
exponential	O
in	O
the	O
number	O
of	O
data	O
points	O
for	O
balanced	O
binary	O
trees	O
,	O
but	O
this	O
is	O
obviously	O
a	O
subset	O
of	O
all	O
possible	O
partitions	O
.	O
)	O
in	O
this	O
way	O
,	O
we	O
can	O
use	O
the	O
bhc	O
algorithm	O
to	O
compute	O
a	O
lower	O
bound	O
on	O
the	O
marginal	B
likelihood	I
of	O
the	O
data	O
from	O
a	O
dpmm	O
.	O
furthermore	O
,	O
we	O
can	O
interpret	O
the	O
algorithm	O
as	O
greedily	O
searching	O
through	O
the	O
exponentially	O
large	O
space	O
of	O
tree-consistent	O
partitions	O
to	O
ﬁnd	O
the	O
best	O
ones	O
of	O
a	O
given	O
size	O
at	O
each	O
step	O
.	O
we	O
are	O
now	O
in	O
a	O
position	O
to	O
compute	O
πk	O
=	O
p	O
(	O
mk	O
=	O
1	O
)	O
,	O
for	O
each	O
node	O
k	O
with	O
children	B
i	O
and	O
j.	O
this	O
is	O
equal	O
to	O
the	O
probability	O
of	O
cluster	O
dk	O
coming	O
from	O
the	O
dpmm	O
,	O
relative	O
to	O
all	O
other	O
partitions	O
of	O
dk	O
consistent	B
with	O
the	O
current	O
tree	B
.	O
this	O
can	O
be	O
computed	O
as	O
follows	O
:	O
initialize	O
di	O
=	O
α	O
and	O
πi	O
=	O
1	O
for	O
each	O
leaf	B
i	O
;	O
then	O
as	O
we	O
build	O
the	O
tree	B
,	O
for	O
each	O
internal	O
node	O
k	O
,	O
compute	O
dk	O
=	O
αγ	O
(	O
nk	O
)	O
+d	O
idj	O
,	O
and	O
πk	O
=	O
,	O
wherei	O
and	O
j	O
are	O
k	O
’	O
s	O
left	O
and	O
right	O
children	O
.	O
αγ	O
(	O
nk	O
)	O
dk	O
25.6.	O
clustering	B
datapoints	O
and	O
features	B
901	O
data	O
set	O
synthetic	O
newsgroups	O
spambase	O
digits	O
fglass	O
single	O
linkage	O
0.599	O
±	O
0.033	O
0.275	O
±	O
0.001	O
0.598	O
±	O
0.017	O
0.224	O
±	O
0.004	O
0.478	O
±	O
0.009	O
complete	B
linkage	O
0.634	O
±	O
0.024	O
0.315	O
±	O
0.008	O
0.699	O
±	O
0.017	O
0.299	O
±	O
0.006	O
0.476	O
±	O
0.009	O
average	O
linkage	O
0.668	O
±	O
0.040	O
0.282	O
±	O
0.002	O
0.668	O
±	O
0.019	O
0.342	O
±	O
0.005	O
0.491	O
±	O
0.009	O
bhc	O
0.828	O
±	O
0.025	O
0.465	O
±	O
0.016	O
0.728	O
±	O
0.029	O
0.393	O
±	O
0.015	O
0.467	O
±	O
0.011	O
table	O
25.1	O
purity	B
scores	O
for	O
various	O
hierarchical	B
clustering	I
schemes	O
applied	O
to	O
various	O
data	O
sets	O
.	O
the	O
synthetic	O
data	O
has	O
n	O
=	O
200	O
,	O
d	O
=	O
2	O
,	O
c	O
=	O
4	O
and	O
real	O
features	O
.	O
newsgroups	O
is	O
extracted	O
from	O
the	O
20	O
newsgroups	O
dataset	O
(	O
d	O
=	O
500	O
,	O
n	O
=	O
800	O
,	O
c	O
=	O
4	O
,	O
binary	O
features	O
)	O
.	O
spambase	O
has	O
n	O
=	O
100	O
,	O
c	O
=	O
2	O
,	O
d	O
=	O
57	O
,	O
binary	O
features	O
.	O
digits	O
is	O
the	O
cedar	O
buffalo	O
digits	O
(	O
n	O
=	O
200	O
,	O
c	O
=	O
10	O
,	O
d	O
=	O
64	O
,	O
binarized	O
features	B
)	O
.	O
fglass	O
is	O
forensic	O
glass	O
dataset	O
(	O
n	O
=	O
214	O
,	O
c	O
=	O
6	O
,	O
d	O
=	O
9	O
,	O
real	O
features	O
)	O
.	O
source	O
:	O
table	O
1	O
of	O
(	O
heller	O
and	O
ghahramani	O
2005	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
katherine	O
heller	O
.	O
25.5.4.3	O
learning	B
the	O
hyper-parameters	B
the	O
model	O
has	O
two	O
free-parameters	O
:	O
α	O
and	O
λ	O
,	O
where	O
λ	O
are	O
the	O
hyper-parameters	B
for	O
the	O
prior	O
on	O
the	O
parameters	O
θ.	O
in	O
(	O
heller	O
and	O
ghahramani	O
2005	O
)	O
,	O
they	O
show	O
how	O
one	O
can	O
back-propagate	O
gradients	O
of	O
the	O
form	O
∂p	O
(	O
dk|tk	O
)	O
through	O
the	O
tree	B
,	O
and	O
thus	O
perform	O
an	O
empirical	O
bayes	O
estimate	O
of	O
the	O
hyper-parameters	B
.	O
∂λ	O
25.5.4.4	O
experimental	O
results	O
(	O
heller	O
and	O
ghahramani	O
2005	O
)	O
compared	O
bhc	O
with	O
traditional	O
agglomerative	B
clustering	I
algo-	O
rithms	O
on	O
various	O
data	O
sets	O
in	O
terms	O
of	O
purity	B
scores	O
.	O
the	O
results	O
are	O
shown	O
in	O
table	O
25.1.	O
we	O
see	O
that	O
bhc	O
did	O
much	O
better	O
than	O
the	O
other	O
methods	O
on	O
all	O
datasets	O
except	O
the	O
forensic	O
glass	O
one	O
.	O
figure	O
25.16	O
visualizes	O
the	O
tree	B
structure	O
estimated	O
by	O
bhc	O
and	O
agglomerative	B
hierarchical	I
clustering	I
(	O
ahc	O
)	O
on	O
the	O
newsgroup	O
data	O
(	O
using	O
a	O
beta-bernoulli	O
model	O
)	O
.	O
the	O
bhc	O
tree	B
is	O
clearly	O
superior	O
(	O
look	O
at	O
the	O
colors	O
at	O
the	O
leaves	B
,	O
which	O
represent	O
class	O
labels	O
)	O
.	O
figure	O
25.17	O
is	O
a	O
zoom-in	O
on	O
the	O
top	O
few	O
nodes	B
of	O
these	O
two	O
trees	O
.	O
bhc	O
splits	O
off	O
clusters	B
concerning	O
sports	O
from	O
clusters	B
concerning	O
cars	O
and	O
space	O
.	O
ahc	O
keeps	O
sports	O
and	O
cars	O
merged	O
together	O
.	O
although	O
sports	O
and	O
cars	O
both	O
fall	O
under	O
the	O
same	O
“	O
rec	O
”	O
newsgroup	O
heading	O
(	O
as	O
opposed	O
to	O
space	O
,	O
that	O
comes	O
under	O
the	O
“	O
sci	O
”	O
newsgroup	O
heading	O
)	O
,	O
the	O
bhc	O
clustering	B
still	O
seems	O
more	O
reasonable	O
,	O
and	O
this	O
is	O
borne	O
out	O
by	O
the	O
quantitative	O
purity	B
scores	O
.	O
bhc	O
has	O
also	O
been	O
applied	O
to	O
gene	O
expression	O
data	O
,	O
with	O
good	O
results	O
(	O
savage	O
et	O
al	O
.	O
2009	O
)	O
.	O
25.6	O
clustering	B
datapoints	O
and	O
features	B
so	O
far	O
,	O
we	O
have	O
been	O
concentrating	O
on	O
clustering	B
datapoints	O
.	O
but	O
each	O
datapoint	O
is	O
often	O
described	O
by	O
multiple	O
features	O
,	O
and	O
we	O
might	O
be	O
interested	O
in	O
clustering	B
them	O
as	O
well	O
.	O
below	O
we	O
describe	O
some	O
methods	O
for	O
doing	O
this	O
.	O
902	O
chapter	O
25.	O
clustering	B
4	O
newsgroups	O
average	O
linkage	O
clustering	B
4	O
newsgroups	O
bayesian	O
hierarchical	B
clustering	I
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
25.16	O
hierarchical	B
clustering	I
applied	O
to	O
800	O
documents	O
from	O
4	O
newsgroups	O
(	O
red	O
is	O
rec.autos	O
,	O
blue	O
is	O
rec.sport.baseball	O
,	O
green	O
is	O
rec.sport.hockey	O
,	O
and	O
magenta	O
is	O
sci.space	O
)	O
.	O
top	O
:	O
average	O
linkage	O
hierarchical	B
clustering	I
.	O
bottom	O
:	O
bayesian	O
hierarchical	B
clustering	I
.	O
each	O
of	O
the	O
leaves	B
is	O
labeled	O
with	O
a	O
color	O
,	O
according	O
to	O
which	O
newsgroup	O
that	O
document	O
came	O
from	O
.	O
we	O
see	O
that	O
the	O
bayesian	O
method	O
results	O
in	O
a	O
clustering	B
that	O
is	O
more	O
consistent	B
with	O
these	O
labels	O
(	O
which	O
were	O
not	O
used	O
during	O
model	O
ﬁtting	O
)	O
.	O
source	O
:	O
figure	O
7	O
of	O
(	O
heller	O
and	O
ghahramani	O
2005	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
katherine	O
heller	O
.	O
25.6.	O
clustering	B
datapoints	O
and	O
features	B
903	O
all	O
data	O
354	O
game	O
team	O
play	O
446	O
car	O
space	O
nasa	O
205	O
149	O
284	O
162	O
baseball	O
pitch	O
hit	O
nhl	O
hockey	O
round	O
car	O
dealer	O
drive	O
space	O
nasa	O
orbit	O
(	O
a	O
)	O
all	O
data	O
1	O
quebec	O
jet	O
boston	O
799	O
car	O
baseball	O
engine	O
2	O
797	O
pitcher	O
boston	O
ball	O
car	O
player	O
space	O
796	O
team	O
game	O
hockey	O
1	O
vehicle	O
dealer	O
driver	O
(	O
b	O
)	O
figure	O
25.17	O
zoom-in	O
on	O
the	O
top	O
nodes	B
in	O
the	O
trees	O
of	O
figure	O
25.16	O
.	O
(	O
b	O
)	O
average	O
linkage	O
.	O
we	O
show	O
the	O
3	O
most	O
probable	O
words	O
per	O
cluster	O
.	O
the	O
number	O
of	O
documents	O
at	O
each	O
cluster	O
is	O
also	O
given	O
.	O
source	O
:	O
figure	O
5	O
of	O
(	O
heller	O
and	O
ghahramani	O
2005	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
katherine	O
heller	O
.	O
(	O
a	O
)	O
bayesian	O
method	O
.	O
25.6.1	O
biclustering	B
clustering	O
the	O
rows	O
and	O
columns	O
is	O
known	O
as	O
biclustering	B
or	O
coclustering	B
.	O
this	O
is	O
widely	O
used	O
in	O
bioinformatics	O
,	O
where	O
the	O
rows	O
often	O
represent	O
genes	O
and	O
the	O
columns	O
represent	O
conditions	O
.	O
it	O
can	O
also	O
be	O
used	O
for	O
collaborative	B
ﬁltering	I
,	O
where	O
the	O
rows	O
represent	O
users	O
and	O
the	O
columns	O
represent	O
movies	O
.	O
a	O
variety	O
of	O
ad	O
hoc	O
methods	O
for	O
biclustering	B
have	O
been	O
proposed	O
;	O
see	O
(	O
madeira	O
and	O
oliveira	O
2004	O
)	O
for	O
a	O
review	O
.	O
here	O
we	O
present	O
a	O
simple	O
probabilistic	O
generative	O
model	O
,	O
based	O
on	O
(	O
kemp	O
et	O
al	O
.	O
2006	O
)	O
(	O
see	O
also	O
(	O
sheng	O
et	O
al	O
.	O
2003	O
)	O
for	O
a	O
related	O
approach	O
)	O
.	O
the	O
idea	O
is	O
to	O
associate	O
each	O
row	O
and	O
each	O
column	O
with	O
a	O
latent	B
indicator	O
,	O
ri	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
r	O
}	O
,	O
cj	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
c	O
}	O
.	O
we	O
then	O
assume	O
the	O
data	O
are	O
iid	B
across	O
samples	B
and	O
across	O
features	B
within	O
each	O
block	O
:	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
p	O
(	O
x|r	O
,	O
c	O
,	O
θ	O
)	O
=	O
p	O
(	O
xij|ri	O
,	O
cj	O
,	O
θ	O
)	O
=	O
p	O
(	O
xij|θri	O
,	O
cj	O
)	O
(	O
25.62	O
)	O
i	O
j	O
where	O
θa	O
,	O
b	O
are	O
the	O
parameters	O
for	O
row	O
cluster	O
a	O
and	O
column	O
cluster	O
b.	O
rather	O
than	O
using	O
a	O
ﬁnite	O
number	O
of	O
clusters	B
for	O
the	O
rows	O
and	O
columns	O
,	O
we	O
can	O
use	O
a	O
dirchlet	O
process	O
,	O
as	O
in	O
the	O
inﬁnite	B
relational	I
model	I
which	O
we	O
discuss	O
in	O
section	O
27.6.1.	O
we	O
can	O
ﬁt	O
this	O
model	O
using	O
e.g.	O
,	O
(	O
collapsed	O
)	O
gibbs	O
sampling	O
.	O
the	O
behavior	O
of	O
this	O
model	O
is	O
illustrated	O
in	O
figure	O
25.18.	O
the	O
data	O
has	O
the	O
form	O
x	O
(	O
i	O
,	O
j	O
)	O
=	O
1	O
iff	B
animal	O
i	O
has	O
feature	O
j	O
,	O
where	O
i	O
=	O
1	O
:	O
50	O
and	O
j	O
=	O
1	O
:	O
85.	O
the	O
animals	O
represent	O
whales	O
,	O
bears	O
,	O
horses	O
,	O
etc	O
.	O
the	O
features	B
represent	O
properties	O
of	O
the	O
habitat	O
(	O
jungle	O
,	O
tree	B
,	O
coastal	O
)	O
,	O
or	O
anatomical	O
properties	O
(	O
has	O
teeth	O
,	O
quadrapedal	O
)	O
,	O
or	O
behavioral	O
properties	O
(	O
swims	O
,	O
eats	O
meat	O
)	O
,	O
etc	O
.	O
the	O
model	O
,	O
using	O
a	O
bernoulli	O
likelihood	B
,	O
was	O
ﬁt	O
to	O
the	O
data	O
.	O
it	O
discovered	O
12	O
animal	O
clusters	B
and	O
33	O
feature	O
clusters	O
.	O
for	O
example	O
,	O
it	O
discovered	O
a	O
bicluster	O
that	O
represents	O
the	O
fact	O
that	O
mammals	O
tend	O
to	O
have	O
aquatic	O
features	B
.	O
25.6.2	O
multi-view	O
clustering	B
the	O
problem	O
with	O
biclustering	B
is	O
that	O
each	O
object	O
(	O
row	O
)	O
can	O
only	O
belong	O
to	O
one	O
cluster	O
.	O
intuitively	O
,	O
an	O
object	O
can	O
have	O
multiple	O
roles	O
,	O
and	O
can	O
be	O
assigned	O
to	O
different	O
clusters	B
depending	O
on	O
which	O
904	O
chapter	O
25.	O
clustering	B
killer	O
whale	O
,	O
blue	O
whale	O
,	O
humpback	O
,	O
seal	O
,	O
walrus	O
,	O
dolphin	O
antelope	O
,	O
horse	O
,	O
giraffe	O
,	O
zebra	O
,	O
deer	O
o1	O
o2	O
o3	O
monkey	O
,	O
gorilla	O
,	O
chimp	O
o4	O
hippo	O
,	O
elephant	O
,	O
rhino	O
o5	O
grizzly	O
bear	O
,	O
polar	B
bear	O
ﬂippers	O
,	O
strain	O
teeth	O
,	O
swims	O
,	O
arctic	O
,	O
coastal	O
,	O
ocean	O
,	O
water	O
hooves	O
,	O
long	O
neck	O
,	O
horns	O
hands	O
,	O
bipedal	O
,	O
jungle	O
,	O
tree	B
bulbous	O
body	O
shape	O
,	O
slow	O
,	O
inactive	O
f1	O
f2	O
f3	O
f4	O
f5	O
meat	O
teeth	O
,	O
eats	O
meat	O
,	O
hunter	O
,	O
ﬁerce	O
f6	O
walks	O
,	O
quadrapedal	O
,	O
ground	O
f1	O
2	O
43	O
5	O
6	O
o1	O
o2	O
o3	O
o4	O
o5	O
figure	O
25.18	O
illustration	O
of	O
biclustering	B
.	O
we	O
show	O
5	O
of	O
the	O
12	O
animal	O
clusters	B
,	O
and	O
6	O
of	O
the	O
33	O
feature	O
clusters	O
.	O
the	O
original	O
data	O
matrix	O
is	O
shown	O
,	O
partitioned	O
according	O
to	O
the	O
discovered	O
clusters	B
.	O
from	O
figure	O
3	O
of	O
(	O
kemp	O
et	O
al	O
.	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
charles	O
kemp	O
.	O
α	O
γ	O
β	O
cj	O
θjk	O
k	O
=	O
1	O
:	O
∞	O
j	O
=	O
1	O
:	O
d	O
riv	O
v	O
=	O
1	O
:	O
∞	O
xij	O
i	O
=	O
1	O
:	O
n	O
(	O
b	O
)	O
(	O
a	O
)	O
figure	O
25.19	O
(	O
a	O
)	O
illustration	O
of	O
multi-view	O
clustering	B
.	O
here	O
we	O
have	O
3	O
views	B
(	O
column	O
partitions	O
)	O
.	O
in	O
the	O
ﬁrst	O
view	O
,	O
we	O
have	O
2	O
clusters	B
(	O
row	O
partitions	O
)	O
.	O
in	O
the	O
third	O
view	O
,	O
we	O
have	O
2	O
clusters	B
.	O
the	O
number	O
of	O
views	B
and	O
partitions	O
are	O
inferred	O
from	O
data	O
.	O
rows	O
within	O
each	O
colored	O
block	O
are	O
assumed	O
to	O
generated	O
iid	B
;	O
however	O
,	O
each	O
column	O
can	O
have	O
a	O
different	O
distributional	O
form	O
,	O
which	O
is	O
useful	O
for	O
modeling	O
discrete	B
and	O
continuous	O
data	O
.	O
from	O
figure	O
1	O
of	O
(	O
guan	O
et	O
al	O
.	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
jennifer	O
dy	O
.	O
(	O
b	O
)	O
corresponding	O
dgm	O
.	O
in	O
the	O
second	O
view	O
,	O
we	O
have	O
3	O
clusters	B
.	O
subset	O
of	O
features	B
you	O
use	O
.	O
for	O
example	O
,	O
in	O
the	O
animal	O
dataset	O
,	O
we	O
may	O
want	O
to	O
group	O
the	O
animals	O
on	O
the	O
basis	O
of	O
anatomical	O
features	B
(	O
e.g.	O
,	O
mammals	O
are	O
warm	O
blooded	O
,	O
reptiles	O
are	O
not	O
)	O
,	O
or	O
on	O
the	O
basis	O
of	O
behavioral	O
features	B
(	O
e.g.	O
,	O
predators	O
vs	O
prey	O
)	O
.	O
we	O
now	O
present	O
a	O
model	O
that	O
can	O
capture	O
this	O
phenomenon	O
.	O
this	O
model	O
was	O
indepen-	O
dently	O
proposed	O
in	O
(	O
shafto	O
et	O
al	O
.	O
2006	O
;	O
mansinghka	O
et	O
al	O
.	O
2011	O
)	O
,	O
who	O
call	O
it	O
crosscat	B
(	O
for	O
cross-	O
categorization	O
)	O
,	O
and	O
in	O
(	O
guan	O
et	O
al	O
.	O
2010	O
;	O
cui	O
et	O
al	O
.	O
2010	O
)	O
,	O
who	O
call	O
it	O
(	O
non-parametric	O
)	O
multi-clust	B
.	O
(	O
see	O
also	O
(	O
rodriguez	O
and	O
ghosh	O
2011	O
)	O
for	O
a	O
very	O
similar	B
model	O
.	O
)	O
the	O
idea	O
is	O
that	O
we	O
partition	O
the	O
columns	O
(	O
features	B
)	O
into	O
v	O
groups	O
or	O
views	B
,	O
so	O
cj	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
}	O
,	O
where	O
j	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
}	O
indexes	O
25.6.	O
clustering	B
datapoints	O
and	O
features	B
905	O
features	B
.	O
we	O
will	O
use	O
a	O
dirichlet	O
process	O
prior	O
for	O
p	O
(	O
c	O
)	O
,	O
which	O
allows	O
v	O
to	O
grow	O
automatically	O
.	O
then	O
for	O
each	O
partition	O
of	O
the	O
columns	O
(	O
i.e.	O
,	O
each	O
view	O
)	O
,	O
call	O
it	O
v	O
,	O
we	O
partition	O
the	O
rows	O
,	O
again	O
using	O
a	O
dp	O
,	O
as	O
illustrated	O
in	O
figure	O
25.19	O
(	O
a	O
)	O
.	O
let	O
riv	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
(	O
v	O
)	O
}	O
be	O
the	O
cluster	O
to	O
which	O
the	O
i	O
’	O
th	O
row	O
belongs	O
in	O
view	O
v.	O
finally	O
,	O
having	O
partitioned	O
the	O
rows	O
and	O
columns	O
,	O
we	O
generate	O
the	O
data	O
:	O
we	O
assume	O
all	O
the	O
rows	O
and	O
columns	O
within	O
a	O
block	O
are	O
iid	B
.	O
we	O
can	O
deﬁne	O
the	O
model	O
more	O
precisely	O
as	O
follows	O
:	O
p	O
(	O
c	O
,	O
r	O
,	O
d	O
)	O
=p	O
(	O
c	O
)	O
p	O
(	O
r|c	O
)	O
p	O
(	O
d|r	O
,	O
c	O
)	O
p	O
(	O
c	O
)	O
=dp	O
(	O
c|α	O
)	O
p	O
(	O
r|c	O
)	O
=	O
v	O
(	O
c	O
)	O
(	O
cid:20	O
)	O
v	O
(	O
c	O
)	O
(	O
cid:20	O
)	O
v=1	O
p	O
(	O
d|r	O
,	O
c	O
,	O
θ	O
)	O
=	O
dp	O
(	O
rv|β	O
)	O
⎡	O
⎣k	O
(	O
rv	O
)	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
(	O
cid:12	O
)	O
(	O
cid:20	O
)	O
⎤	O
⎦	O
p	O
(	O
xij|θjk	O
)	O
p	O
(	O
θjk	O
)	O
dθjk	O
v=1	O
j	O
:	O
cj	O
=v	O
k=1	O
i	O
:	O
riv=k	O
(	O
25.63	O
)	O
(	O
25.64	O
)	O
(	O
25.65	O
)	O
(	O
25.66	O
)	O
(	O
25.67	O
)	O
see	O
figure	O
25.19	O
(	O
b	O
)	O
for	O
the	O
dgm.2	O
if	O
the	O
data	O
is	O
binary	O
,	O
and	O
we	O
use	O
a	O
beta	O
(	O
γ	O
,	O
γ	O
)	O
prior	O
for	O
θjk	O
,	O
the	O
likelihood	B
reduces	O
to	O
v	O
(	O
c	O
)	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
k	O
(	O
rv	O
)	O
(	O
cid:20	O
)	O
v=1	O
j	O
:	O
cj	O
=v	O
k=1	O
p	O
(	O
d|r	O
,	O
c	O
,	O
γ	O
)	O
=	O
(	O
cid:7	O
)	O
beta	O
(	O
nj	O
,	O
k	O
,	O
v	O
+	O
γ	O
,	O
nj	O
,	O
k	O
,	O
v	O
+	O
γ	O
)	O
beta	O
(	O
γ	O
,	O
γ	O
)	O
where	O
nj	O
,	O
k	O
,	O
v	O
=	O
i	O
:	O
ri	O
,	O
v=k	O
i	O
(	O
xij	O
=	O
1	O
)	O
counts	O
the	O
number	O
of	O
features	B
which	O
are	O
on	O
in	O
the	O
j	O
’	O
th	O
column	O
for	O
view	O
v	O
and	O
for	O
row	O
cluster	O
k.	O
similarly	O
,	O
nj	O
,	O
k	O
,	O
v	O
counts	O
how	O
many	O
features	B
are	O
off	O
.	O
the	O
model	O
is	O
easily	O
extended	O
to	O
other	O
kinds	O
of	O
data	O
,	O
by	O
replacing	O
the	O
beta-bernoulli	O
with	O
,	O
say	O
,	O
the	O
gaussian-gamma-gaussian	O
model	O
,	O
as	O
discussed	O
in	O
(	O
guan	O
et	O
al	O
.	O
2010	O
;	O
mansinghka	O
et	O
al	O
.	O
2011	O
)	O
.	O
approximate	O
map	O
estimation	O
can	O
be	O
done	O
using	O
stochastic	B
search	I
(	O
shafto	O
et	O
al	O
.	O
2006	O
)	O
,	O
and	O
approximate	B
inference	I
can	O
be	O
done	O
using	O
variational	O
bayes	O
(	O
guan	O
et	O
al	O
.	O
2010	O
)	O
or	O
gibbs	O
sampling	O
(	O
mansinghka	O
et	O
al	O
.	O
2011	O
)	O
.	O
the	O
hyper-parameter	O
γ	O
for	O
the	O
likelihood	B
can	O
usually	O
be	O
set	O
in	O
a	O
non-	O
informative	O
way	O
,	O
but	O
results	O
are	O
more	O
sensitive	O
to	O
the	O
other	O
two	O
parameters	O
,	O
since	O
α	O
controls	O
the	O
number	O
of	O
column	O
partitions	O
,	O
and	O
β	O
controls	O
the	O
number	O
of	O
row	O
partitions	O
.	O
hence	O
a	O
more	O
robust	B
technique	O
is	O
to	O
infer	O
the	O
hyper-parameters	B
using	O
mh	O
.	O
this	O
also	O
speeds	O
up	O
convergence	O
(	O
mansinghka	O
et	O
al	O
.	O
2011	O
)	O
.	O
figure	O
25.20	O
illustrates	O
the	O
model	O
applied	O
to	O
some	O
binary	O
data	O
containing	O
22	O
animals	O
and	O
106	O
features	B
.	O
the	O
ﬁgures	O
shows	O
the	O
(	O
approximate	O
)	O
map	O
partition	O
.	O
the	O
ﬁrst	O
partition	O
of	O
the	O
columns	O
contains	O
taxonomic	O
features	B
,	O
such	O
as	O
“	O
has	O
bones	O
”	O
,	O
“	O
is	O
warm-blooded	O
”	O
,	O
“	O
lays	O
eggs	O
”	O
,	O
etc	O
.	O
this	O
divides	O
the	O
animals	O
into	O
birds	O
,	O
reptiles/	O
amphibians	O
,	O
mammals	O
,	O
and	O
invertebrates	O
.	O
the	O
second	O
partition	O
of	O
the	O
columns	O
contains	O
features	B
that	O
are	O
treated	O
as	O
noise	O
,	O
with	O
no	O
apparent	O
structure	O
(	O
except	O
for	O
the	O
single	O
row	O
labeled	O
“	O
frog	O
”	O
)	O
.	O
the	O
third	O
partition	O
of	O
the	O
columns	O
contains	O
ecological	O
features	B
like	O
“	O
dangerous	O
”	O
,	O
“	O
carnivorous	O
”	O
,	O
“	O
lives	O
in	O
water	O
”	O
,	O
etc	O
.	O
this	O
divides	O
the	O
animals	O
into	O
prey	O
,	O
land	O
predators	O
,	O
sea	O
predators	O
and	O
air	O
predators	O
.	O
thus	O
each	O
animal	O
(	O
row	O
)	O
can	O
belong	O
to	O
a	O
different	O
2.	O
the	O
dependence	O
between	O
r	O
and	O
c	O
is	O
not	O
shown	O
,	O
since	O
it	O
is	O
not	O
a	O
dependence	O
between	O
the	O
values	O
of	O
riv	O
and	O
cj	O
,	O
but	O
between	O
the	O
cardinality	O
of	O
v	O
and	O
cj	O
.	O
in	O
other	O
words	O
,	O
the	O
number	O
of	O
row	O
partitions	O
we	O
need	O
to	O
specify	O
(	O
the	O
number	O
of	O
views	B
,	O
indexed	O
by	O
v	O
)	O
depends	O
on	O
the	O
number	O
of	O
column	O
partitions	O
(	O
clusters	B
)	O
that	O
we	O
have	O
.	O
906	O
a	O
leopard	O
sheep	O
seal	O
dolphin	O
monkey	O
bat	O
alligator	O
iguana	O
frog	O
python	O
finch	O
ostrich	O
seagull	O
owl	O
penguin	O
eagle	O
grasshopper	O
ant	O
bee	O
jellyfish	O
octopus	O
dragonfly	O
b	O
c	O
frog	O
chapter	O
25.	O
clustering	B
leopard	O
alligator	O
python	O
seal	O
dolphin	O
frog	O
jellyfish	O
octopus	O
penguin	O
finch	O
seagull	O
owl	O
eagle	O
dragonfly	O
bat	O
grasshopper	O
ant	O
bee	O
sheep	O
monkey	O
iguana	O
ostrich	O
d	O
r	O
a	O
z	O
i	O
l	O
d	O
r	O
o	O
c	O
l	O
n	O
e	O
e	O
r	O
g	O
s	O
i	O
a	O
s	O
i	O
s	O
k	O
w	O
a	O
u	O
q	O
s	O
k	O
a	O
e	O
b	O
a	O
s	O
a	O
h	O
e	O
u	O
g	O
n	O
o	O
t	O
a	O
s	O
a	O
h	O
s	O
r	O
e	O
p	O
p	O
i	O
l	O
f	O
s	O
a	O
h	O
l	O
i	O
a	O
t	O
a	O
s	O
a	O
h	O
i	O
s	O
w	O
a	O
p	O
s	O
a	O
h	O
n	O
a	O
r	O
b	O
e	O
g	O
r	O
a	O
y	O
r	O
r	O
u	O
f	O
s	O
i	O
n	O
w	O
o	O
r	O
b	O
s	O
i	O
e	O
c	O
i	O
m	O
s	O
t	O
a	O
e	O
s	O
t	O
n	O
e	O
d	O
o	O
r	O
s	O
t	O
a	O
e	O
t	O
u	O
o	O
n	O
s	O
a	O
s	O
a	O
h	O
e	O
a	O
n	O
n	O
e	O
t	O
n	O
a	O
s	O
a	O
h	O
i	O
a	O
n	O
p	O
s	O
a	O
s	O
a	O
h	O
l	O
a	O
s	O
a	O
h	O
s	O
g	O
g	O
e	O
s	O
y	O
a	O
s	O
e	O
n	O
o	O
b	O
s	O
a	O
h	O
l	O
l	O
a	O
m	O
m	O
a	O
m	O
a	O
s	O
i	O
l	O
d	O
e	O
d	O
o	O
o	O
b	O
−	O
m	O
r	O
a	O
w	O
s	O
i	O
t	O
e	O
e	O
f	O
s	O
a	O
h	O
h	O
t	O
e	O
e	O
t	O
s	O
a	O
h	O
t	O
r	O
a	O
m	O
s	O
s	O
i	O
s	O
p	O
u	O
o	O
r	O
g	O
n	O
i	O
s	O
l	O
e	O
v	O
a	O
r	O
t	O
s	O
e	O
s	O
i	O
o	O
n	O
d	O
u	O
o	O
l	O
s	O
e	O
k	O
a	O
m	O
l	O
l	O
a	O
t	O
s	O
i	O
h	O
s	O
i	O
f	O
a	O
s	O
i	O
y	O
m	O
i	O
l	O
s	O
s	O
i	O
s	O
r	O
a	O
o	O
r	O
s	O
n	O
i	O
f	O
s	O
a	O
h	O
e	O
n	O
i	O
l	O
e	O
f	O
a	O
s	O
i	O
s	O
n	O
r	O
o	O
h	O
s	O
a	O
h	O
s	O
e	O
v	O
o	O
o	O
h	O
s	O
a	O
h	O
t	O
n	O
e	O
d	O
o	O
r	O
a	O
s	O
i	O
s	O
e	O
k	O
a	O
l	O
n	O
i	O
s	O
e	O
v	O
i	O
l	O
i	O
i	O
n	O
a	O
b	O
h	O
p	O
m	O
a	O
n	O
a	O
s	O
i	O
h	O
t	O
o	O
o	O
m	O
s	O
e	O
e	O
r	O
t	O
n	O
e	O
g	O
r	O
a	O
l	O
s	O
i	O
s	O
t	O
u	O
n	O
s	O
t	O
a	O
e	O
s	O
s	O
i	O
i	O
s	O
e	O
v	O
i	O
l	O
t	O
e	O
e	O
f	O
d	O
e	O
b	O
b	O
e	O
w	O
s	O
a	O
h	O
s	O
e	O
t	O
a	O
m	O
l	O
i	O
l	O
c	O
d	O
o	O
c	O
n	O
i	O
s	O
e	O
v	O
i	O
l	O
i	O
s	O
u	O
o	O
i	O
c	O
o	O
r	O
e	O
f	O
s	O
i	O
s	O
u	O
o	O
r	O
e	O
g	O
n	O
a	O
d	O
s	O
i	O
e	O
r	O
o	O
v	O
n	O
r	O
a	O
c	O
a	O
s	O
i	O
r	O
o	O
t	O
a	O
d	O
e	O
r	O
p	O
a	O
s	O
i	O
r	O
e	O
t	O
a	O
w	O
n	O
i	O
s	O
e	O
v	O
i	O
l	O
s	O
e	O
i	O
l	O
f	O
g	O
n	O
o	O
l	O
s	O
i	O
s	O
s	O
a	O
r	O
g	O
n	O
h	O
s	O
i	O
f	O
s	O
t	O
a	O
e	O
s	O
e	O
t	O
a	O
m	O
s	O
e	O
v	O
a	O
e	O
l	O
s	O
t	O
a	O
e	O
i	O
s	O
l	O
a	O
m	O
n	O
a	O
s	O
t	O
a	O
e	O
i	O
s	O
e	O
v	O
i	O
l	O
i	O
l	O
c	O
t	O
o	O
h	O
n	O
i	O
s	O
e	O
v	O
i	O
l	O
figure	O
25.20	O
map	O
estimate	O
produced	O
by	O
the	O
crosscat	B
system	O
when	O
applied	O
to	O
a	O
binary	O
data	O
matrix	O
of	O
animals	O
(	O
rows	O
)	O
by	O
features	B
(	O
columns	O
)	O
.	O
see	O
text	O
for	O
details	O
.	O
source	O
:	O
figure	O
7	O
of	O
(	O
shafto	O
et	O
al	O
.	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
vikash	O
mansingkha	O
.	O
cluster	O
depending	O
on	O
what	O
set	O
of	O
features	B
are	O
considered	O
.	O
uncertainty	B
about	O
the	O
partitions	O
can	O
be	O
handled	O
by	O
sampling	O
.	O
it	O
is	O
interesting	O
to	O
compare	O
this	O
model	O
to	O
a	O
standard	O
inﬁnite	O
mixture	B
model	I
.	O
while	O
the	O
standard	B
model	I
can	O
represent	O
any	O
density	O
on	O
ﬁxed-sized	O
vectors	O
as	O
n	O
→	O
∞	O
,	O
it	O
can	O
not	O
cope	O
with	O
d	O
→	O
∞	O
,	O
since	O
it	O
has	O
no	O
way	O
to	O
handle	O
irrelevant	O
,	O
noisy	O
or	O
redundant	O
features	B
.	O
by	O
contrast	O
,	O
the	O
crosscat/multi-clust	O
system	O
is	O
robust	B
to	O
irrelevant	O
features	B
:	O
it	O
can	O
just	O
partition	O
them	O
off	O
,	O
and	O
cluster	O
the	O
rows	O
only	O
using	O
the	O
relevant	O
features	B
.	O
note	O
,	O
however	O
,	O
that	O
it	O
does	O
not	O
need	O
a	O
separate	O
“	O
background	O
”	O
model	O
,	O
since	O
everything	O
is	O
modelled	O
using	O
the	O
same	O
mechanism	O
.	O
this	O
is	O
useful	O
,	O
since	O
one	O
’	O
s	O
person	O
’	O
s	O
noise	O
is	O
another	O
person	O
’	O
s	O
signal	O
.	O
(	O
indeed	O
,	O
this	O
symmetry	O
may	O
explain	O
why	O
multi-clust	B
outperformed	O
the	O
sparse	B
mixture	O
model	O
approach	O
of	O
(	O
law	O
et	O
al	O
.	O
2004	O
)	O
in	O
the	O
experiments	O
reported	O
in	O
(	O
guan	O
et	O
al	O
.	O
2010	O
)	O
.	O
)	O
26	O
graphical	B
model	I
structure	O
learning	B
26.1	O
introduction	O
we	O
have	O
seen	O
how	O
graphical	O
models	O
can	O
be	O
used	O
to	O
express	O
conditional	B
independence	I
assump-	O
tions	O
between	O
variables	O
.	O
in	O
this	O
chapter	O
,	O
we	O
discuss	O
how	O
to	O
learn	O
the	O
structure	O
of	O
the	O
graphical	B
model	I
itself	O
.	O
that	O
is	O
,	O
we	O
want	O
to	O
compute	O
p	O
(	O
g|d	O
)	O
,	O
whereg	O
is	O
the	O
graph	B
structure	O
,	O
represented	O
as	O
an	O
v	O
×	O
v	O
adjacency	B
matrix	I
.	O
as	O
we	O
discussed	O
in	O
section	O
1.3.3	O
,	O
there	O
are	O
two	O
main	O
applications	O
of	O
structure	B
learning	I
:	O
knowl-	O
edge	O
discovery	O
and	O
density	B
estimation	I
.	O
the	O
former	O
just	O
requires	O
a	O
graph	B
topology	O
,	O
whereas	O
the	O
latter	O
requires	O
a	O
fully	O
speciﬁed	O
model	O
.	O
the	O
main	O
obstacle	O
in	O
structure	B
learning	I
is	O
that	O
the	O
number	O
of	O
possible	O
graphs	O
is	O
exponential	O
in	O
the	O
number	O
of	O
nodes	B
:	O
a	O
simple	O
upper	O
bound	O
is	O
o	O
(	O
2v	O
(	O
v	O
−1	O
)	O
/2	O
)	O
.	O
thus	O
the	O
full	B
posterior	O
p	O
(	O
g|d	O
)	O
is	O
prohibitively	O
large	O
:	O
even	O
if	O
we	O
could	O
afford	O
to	O
compute	O
it	O
,	O
we	O
could	O
not	O
even	O
store	O
it	O
.	O
so	O
we	O
will	O
seek	O
appropriate	O
summaries	O
of	O
the	O
posterior	O
.	O
these	O
summary	O
statistics	O
depend	O
on	O
our	O
task	O
.	O
is	O
knowledge	B
discovery	I
,	O
we	O
may	O
want	O
to	O
compute	O
posterior	O
edge	O
marginals	O
,	O
p	O
(	O
gst	O
=	O
1|d	O
)	O
;	O
we	O
can	O
then	O
plot	O
the	O
corresponding	O
graph	B
,	O
where	O
the	O
thickness	O
of	O
each	O
edge	O
represents	O
our	O
conﬁdence	O
in	O
its	O
presence	O
.	O
by	O
setting	O
a	O
threshold	O
,	O
we	O
can	O
generate	O
a	O
sparse	B
graph	O
,	O
which	O
can	O
be	O
useful	O
for	O
visualization	O
purposes	O
(	O
see	O
figure	O
1.11	O
)	O
.	O
if	O
our	O
goal	O
is	O
density	B
estimation	I
,	O
we	O
may	O
want	O
to	O
compute	O
the	O
map	O
graph	B
,	O
ˆg	O
∈	O
argmaxg	O
p	O
(	O
g|d	O
)	O
.	O
if	O
our	O
goal	O
in	O
most	O
cases	O
,	O
ﬁnding	O
the	O
globally	O
optimal	O
graph	B
will	O
take	O
exponential	O
time	O
,	O
so	O
we	O
will	O
use	O
dis-	O
crete	O
optimization	B
methods	O
such	O
as	O
heuristic	O
search	O
.	O
however	O
,	O
in	O
the	O
case	O
of	O
trees	O
,	O
we	O
can	O
ﬁnd	O
the	O
globally	O
optimal	O
graph	B
structure	O
quite	O
efficiently	O
using	O
exact	O
methods	O
,	O
as	O
we	O
discuss	O
in	O
section	O
26.3.	O
if	O
density	B
estimation	I
is	O
our	O
only	O
goal	O
,	O
it	O
is	O
worth	O
considering	O
whether	O
it	O
would	O
be	O
more	O
appropriate	O
to	O
learn	O
a	O
latent	O
variable	O
model	O
,	O
which	O
can	O
capture	O
correlation	O
between	O
the	O
visible	B
variables	I
via	O
a	O
set	O
of	O
latent	B
common	O
causes	O
(	O
see	O
chapters	O
12	O
and	O
27	O
)	O
.	O
such	O
models	O
are	O
often	O
easier	O
to	O
learn	O
and	O
,	O
perhaps	O
more	O
importantly	O
,	O
they	O
can	O
be	O
applied	O
(	O
for	O
prediction	O
purposes	O
)	O
much	O
more	O
efficiently	O
,	O
since	O
they	O
do	O
not	O
require	O
performing	O
inference	B
in	O
a	O
learned	O
graph	B
with	O
potentially	O
high	O
treewidth	O
.	O
the	O
downside	O
with	O
such	O
models	O
is	O
that	O
the	O
latent	B
factors	I
are	O
often	O
unidentiﬁable	B
,	O
and	O
hence	O
hard	O
to	O
interpret	O
.	O
of	O
course	O
,	O
we	O
can	O
combine	O
graphical	B
model	I
structure	O
learning	B
and	O
latent	O
variable	O
learning	O
,	O
as	O
we	O
will	O
show	O
later	O
in	O
this	O
chapter	O
.	O
in	O
some	O
cases	O
,	O
we	O
don	O
’	O
t	O
just	O
want	O
to	O
model	O
the	O
observed	O
correlation	O
between	O
variables	O
;	O
instead	O
,	O
we	O
want	O
to	O
model	O
the	O
causal	O
structure	O
behind	O
the	O
data	O
,	O
so	O
we	O
can	O
predict	O
the	O
effects	O
of	O
manipulating	O
variables	O
.	O
this	O
is	O
a	O
much	O
more	O
challenging	O
task	O
,	O
which	O
we	O
brieﬂy	O
discuss	O
in	O
908	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
bible	O
case	O
course	O
evidence	B
children	O
mission	O
launch	O
christian	O
fact	O
israel	O
government	O
earth	O
gun	O
nasa	O
lunar	O
god	O
human	O
jews	O
war	O
president	O
law	O
orbit	O
shuttle	O
moon	O
jesus	O
computer	O
religion	O
world	O
rights	O
solar	O
space	O
science	O
university	O
state	B
figure	O
26.1	O
part	O
of	O
a	O
relevance	B
network	I
constructed	O
from	O
the	O
20-news	O
data	O
shown	O
in	O
figure	O
1.2.	O
we	O
show	O
edges	B
whose	O
mutual	B
information	I
is	O
greater	O
than	O
or	O
equal	O
to	O
20	O
%	O
of	O
the	O
maximum	O
pairwise	O
mi	O
.	O
for	O
clarity	O
,	O
the	O
graph	B
has	O
been	O
cropped	O
,	O
so	O
we	O
only	O
show	O
a	O
subset	O
of	O
the	O
nodes	B
and	O
edges	B
.	O
figure	O
generated	O
by	O
relevancenetworknewsgroupdemo	O
.	O
section	O
26.6	O
.	O
26.2	O
structure	B
learning	I
for	O
knowledge	B
discovery	I
since	O
computing	O
the	O
map	O
graph	B
or	O
the	O
exact	O
posterior	O
edge	O
marginals	O
is	O
in	O
general	O
computa-	O
tionally	O
intractable	O
(	O
chickering	O
1996	O
)	O
,	O
in	O
this	O
section	O
we	O
discuss	O
some	O
“	O
quick	O
and	O
dirty	O
”	O
methods	O
for	O
learning	B
graph	O
structures	O
which	O
can	O
be	O
used	O
to	O
visualize	O
one	O
’	O
s	O
data	O
.	O
the	O
resulting	O
models	O
do	O
not	O
constitute	O
consistent	B
joint	O
probability	O
distributions	O
,	O
so	O
they	O
can	O
not	O
be	O
used	O
for	O
prediction	O
,	O
and	O
they	O
can	O
not	O
even	O
be	O
formally	O
evaluated	O
in	O
terms	O
of	O
goodness	O
of	O
ﬁt	O
.	O
nevertheless	O
,	O
these	O
methods	O
are	O
a	O
useful	O
ad	O
hoc	O
tool	O
to	O
have	O
in	O
one	O
’	O
s	O
data	O
visualization	O
toolbox	O
,	O
in	O
view	O
of	O
their	O
speed	O
and	O
simplicity	O
.	O
26.2.1	O
relevance	O
networks	O
a	O
relevance	B
network	I
is	O
a	O
way	O
of	O
visualizing	B
the	O
pairwise	O
mutual	O
information	B
between	O
multiple	O
random	O
variables	O
:	O
we	O
simply	O
choose	O
a	O
threshold	O
and	O
draw	O
an	O
edge	O
from	O
node	O
i	O
to	O
node	O
j	O
if	O
i	O
(	O
xi	O
;	O
xj	O
)	O
is	O
above	O
this	O
threshold	O
.	O
in	O
the	O
gaussian	O
case	O
,	O
i	O
(	O
xi	O
;	O
xj	O
)	O
=−	O
1	O
ij	O
)	O
,	O
where	O
ρij	O
is	O
the	O
correlation	B
coefficient	I
(	O
see	O
exercise	O
2.13	O
)	O
,	O
so	O
we	O
are	O
essentially	O
visualizing	B
σ	O
;	O
this	O
is	O
known	O
as	O
the	O
covariance	B
graph	I
(	O
section	O
19.4.4.1	O
)	O
.	O
2	O
log	O
(	O
1	O
−	O
ρ2	O
this	O
method	O
is	O
quite	O
popular	O
in	O
systems	B
biology	I
(	O
margolin	O
et	O
al	O
.	O
2006	O
)	O
,	O
where	O
it	O
is	O
used	O
to	O
visualize	O
the	O
interaction	O
between	O
genes	O
.	O
the	O
trouble	O
with	O
biological	O
examples	O
is	O
that	O
they	O
are	O
hard	O
for	O
non-biologists	O
to	O
understand	O
.	O
so	O
let	O
us	O
instead	O
illustrate	O
the	O
idea	O
using	O
natural	O
language	O
text	O
.	O
figure	O
26.1	O
gives	O
an	O
example	O
,	O
where	O
we	O
visualize	O
the	O
mi	O
between	O
words	O
in	O
the	O
newsgroup	O
dataset	O
from	O
figure	O
1.2.	O
the	O
results	O
seem	O
intuitively	O
reasonable	O
.	O
however	O
,	O
relevance	O
networks	O
suffer	O
from	O
a	O
major	O
problem	O
:	O
the	O
graphs	O
are	O
usually	O
very	O
dense	O
,	O
since	O
most	O
variables	O
are	O
dependent	O
on	O
most	O
other	O
variables	O
,	O
even	O
after	O
thresholding	O
the	O
mis	O
.	O
for	O
example	O
,	O
suppose	O
x1	O
directly	O
inﬂuences	O
x2	O
which	O
directly	O
inﬂuences	O
x3	O
(	O
e.g.	O
,	O
these	O
form	O
components	O
of	O
a	O
signalling	O
cascade	B
,	O
x1	O
−	O
x2	O
−	O
x3	O
)	O
.	O
then	O
x1	O
has	O
non-zero	O
mi	O
with	O
x3	O
(	O
and	O
vice	O
versa	O
)	O
,	O
so	O
there	O
will	O
be	O
a	O
1	O
−	O
3	O
edge	O
in	O
the	O
relevance	B
network	I
.	O
indeed	O
,	O
most	O
pairs	O
will	O
be	O
26.2.	O
structure	B
learning	I
for	O
knowledge	B
discovery	I
909	O
children	B
case	O
course	O
fact	O
question	O
earth	O
bible	O
christian	O
food	O
baseball	O
mission	O
god	O
disk	O
mac	O
car	O
aids	O
doctor	O
fans	O
nasa	O
jesus	O
pc	O
dos	O
drive	O
bmw	O
israel	O
government	O
health	O
games	O
hockey	O
hit	O
launch	O
memory	O
scsi	O
jews	O
engine	O
dealer	O
state	B
war	O
computer	O
president	O
medicine	O
season	O
puck	O
nhl	O
shuttle	O
religion	O
data	O
card	O
honda	O
power	O
oil	O
world	O
insurance	O
science	O
studies	O
team	O
software	O
solar	O
graphics	O
driver	O
gun	O
research	O
university	O
water	O
human	O
cancer	O
win	O
league	O
lunar	O
system	O
display	O
video	O
windows	O
law	O
disease	O
won	O
players	O
moon	O
server	O
files	O
rights	O
problem	O
evidence	B
space	O
program	O
format	O
patients	O
msg	O
help	O
mars	O
orbit	O
technology	O
ftp	O
image	O
number	O
vitamin	O
email	O
satellite	O
version	O
phone	O
figure	O
26.2	O
a	O
dependency	B
network	I
constructed	O
from	O
the	O
20-news	O
data	O
.	O
we	O
show	O
all	O
edges	O
with	O
regres-	O
sion	O
weight	O
above	O
0.5	O
in	O
the	O
markov	O
blankets	O
estimated	O
by	O
(	O
cid:7	O
)	O
1	O
penalized	O
logistic	O
regression	B
.	O
undirected	B
edges	O
represent	O
cases	O
where	O
a	O
directed	B
edge	O
was	O
found	O
in	O
both	O
directions	O
.	O
from	O
figure	O
4.9	O
of	O
(	O
schmidt	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
mark	O
schmidt	O
.	O
connected	O
.	O
a	O
better	O
approach	O
is	O
to	O
use	O
graphical	O
models	O
,	O
which	O
represent	O
conditional	B
independence	I
,	O
in	O
the	O
above	O
example	O
,	O
x1	O
is	O
conditionally	B
independent	I
of	O
x3	O
given	O
rather	O
than	O
dependence	O
.	O
x2	O
,	O
so	O
there	O
will	O
not	O
be	O
a	O
1	O
−	O
3	O
edge	O
.	O
consequently	O
graphical	O
models	O
are	O
usually	O
much	O
sparser	O
than	O
relevance	O
networks	O
,	O
and	O
hence	O
are	O
a	O
more	O
useful	O
way	O
of	O
visualizing	B
interactions	O
between	O
multiple	O
variables	O
.	O
26.2.2	O
dependency	B
networks	I
a	O
simple	O
and	O
efficient	O
way	O
to	O
learn	O
a	O
graphical	B
model	I
structure	O
is	O
to	O
independently	O
ﬁt	O
d	O
sparse	B
full-conditional	O
distributions	O
p	O
(	O
xt|x−t	O
)	O
;	O
this	O
is	O
called	O
a	O
dependency	B
network	I
(	O
heckerman	O
et	O
al	O
.	O
2000	O
)	O
.	O
the	O
chosen	O
variables	O
constitute	O
the	O
inputs	O
to	O
the	O
node	O
,	O
i.e.	O
,	O
its	O
markov	O
blanket	O
.	O
we	O
can	O
then	O
visualize	O
the	O
resulting	O
sparse	B
graph	O
.	O
the	O
advantage	O
over	O
relevance	O
networks	O
is	O
that	O
redundant	O
variables	O
will	O
not	O
be	O
selected	O
as	O
inputs	O
.	O
we	O
can	O
use	O
any	O
kind	O
of	O
sparse	B
regression	O
or	O
classiﬁcation	B
method	O
to	O
ﬁt	O
each	O
cpd	O
.	O
(	O
heckerman	O
(	O
meinshausen	O
and	O
buhlmann	O
2006	O
)	O
use	O
(	O
cid:2	O
)	O
1-	O
et	O
al	O
.	O
2000	O
)	O
uses	O
classiﬁcation/	O
regression	B
trees	O
,	O
regularized	O
linear	O
regression	B
,	O
(	O
wainwright	O
et	O
al	O
.	O
2006	O
)	O
use	O
(	O
cid:2	O
)	O
1-regularized	O
logistic	B
regression	I
(	O
see	O
depnetfit	O
for	O
some	O
code	O
)	O
,	O
(	O
dobra	O
2009	O
)	O
uses	O
bayesian	O
variable	O
selection	O
,	O
etc	O
.	O
(	O
meinshausen	O
910	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
and	O
buhlmann	O
2006	O
)	O
discuss	O
theoretical	O
conditions	O
under	O
which	O
(	O
cid:2	O
)	O
1-regularized	O
linear	B
regression	I
can	O
recover	O
the	O
true	O
graph	O
structure	O
,	O
assuming	O
the	O
data	O
was	O
generated	O
from	O
a	O
sparse	B
gaussian	O
graphical	B
model	I
.	O
figure	O
26.2	O
shows	O
a	O
dependency	B
network	I
that	O
was	O
learned	O
from	O
the	O
20-newsgroup	O
data	O
using	O
(	O
cid:2	O
)	O
1	O
regularized	O
logistic	O
regression	B
,	O
where	O
the	O
penalty	O
parameter	B
λ	O
was	O
chosen	O
by	O
bic	O
.	O
many	O
of	O
the	O
words	O
present	O
in	O
these	O
estimated	O
markov	O
blankets	O
represent	O
fairly	O
natural	O
associations	O
(	O
aids	O
:	O
disease	O
,	O
baseball	O
:	O
fans	O
,	O
bible	O
:	O
god	O
,	O
bmw	O
:	O
car	O
,	O
cancer	O
:	O
patients	O
,	O
etc.	O
)	O
.	O
however	O
,	O
some	O
of	O
the	O
esti-	O
mated	O
statistical	O
dependencies	O
seem	O
less	O
intuitive	O
,	O
such	O
as	O
baseball	O
:	O
windows	O
and	O
bmw	O
:	O
christian	O
.	O
we	O
can	O
gain	O
more	O
insight	O
if	O
we	O
look	O
not	O
only	O
at	O
the	O
sparsity	B
pattern	O
,	O
but	O
also	O
the	O
values	O
of	O
the	O
regression	B
weights	O
.	O
for	O
example	O
,	O
here	O
are	O
the	O
incoming	O
weights	O
for	O
the	O
ﬁrst	O
5	O
words	O
:	O
•	O
aids	O
:	O
children	B
(	O
0.53	O
)	O
,	O
disease	O
(	O
0.84	O
)	O
,	O
fact	O
(	O
0.47	O
)	O
,	O
health	O
(	O
0.77	O
)	O
,	O
president	O
(	O
0.50	O
)	O
,	O
research	O
(	O
0.53	O
)	O
•	O
baseball	O
:	O
christian	O
(	O
-0.98	O
)	O
,	O
drive	O
(	O
-0.49	O
)	O
,	O
games	O
(	O
0.81	O
)	O
,	O
god	O
(	O
-0.46	O
)	O
,	O
government	O
(	O
-0.69	O
)	O
,	O
hit	O
(	O
0.62	O
)	O
,	O
memory	O
(	O
-1.29	O
)	O
,	O
players	O
(	O
1.16	O
)	O
,	O
season	O
(	O
0.31	O
)	O
,	O
software	O
(	O
-0.68	O
)	O
,	O
windows	O
(	O
-1.45	O
)	O
•	O
bible	O
:	O
car	O
(	O
-0.72	O
)	O
,	O
card	O
(	O
-0.88	O
)	O
,	O
christian	O
(	O
0.49	O
)	O
,	O
fact	O
(	O
0.21	O
)	O
,	O
god	O
(	O
1.01	O
)	O
,	O
jesus	O
(	O
0.68	O
)	O
,	O
orbit	O
(	O
0.83	O
)	O
,	O
program	O
(	O
-0.56	O
)	O
,	O
religion	O
(	O
0.24	O
)	O
,	O
version	O
(	O
0.49	O
)	O
•	O
bmw	O
:	O
car	O
(	O
0.60	O
)	O
,	O
christian	O
(	O
-11.54	O
)	O
,	O
engine	O
(	O
0.69	O
)	O
,	O
god	O
(	O
-0.74	O
)	O
,	O
government	O
(	O
-1.01	O
)	O
,	O
help	O
(	O
-0.50	O
)	O
,	O
windows	O
(	O
-1.43	O
)	O
•	O
cancer	O
:	O
disease	O
(	O
0.62	O
)	O
,	O
medicine	O
(	O
0.58	O
)	O
,	O
patients	O
(	O
0.90	O
)	O
,	O
research	O
(	O
0.49	O
)	O
,	O
studies	O
(	O
0.70	O
)	O
words	O
in	O
italic	O
red	O
have	O
negative	O
weights	O
,	O
which	O
represents	O
a	O
dissociative	O
relationship	O
.	O
for	O
example	O
,	O
the	O
model	O
reﬂects	O
that	O
baseball	O
:	O
windows	O
is	O
an	O
unlikely	O
combination	O
.	O
it	O
turns	O
out	O
that	O
most	O
of	O
the	O
weights	O
are	O
negative	O
(	O
1173	O
negative	O
,	O
286	O
positive	O
,	O
8541	O
zero	O
)	O
in	O
this	O
model	O
.	O
in	O
addition	O
to	O
visualizing	B
the	O
data	O
,	O
a	O
dependency	B
network	I
can	O
be	O
used	O
for	O
inference	B
.	O
however	O
,	O
the	O
only	O
algorithm	O
we	O
can	O
use	O
is	O
gibbs	O
sampling	O
,	O
where	O
we	O
repeatedly	O
sample	O
the	O
nodes	B
with	O
missing	B
values	O
from	O
their	O
full	B
conditionals	O
.	O
unfortunately	O
,	O
a	O
product	O
of	O
full	O
conditionals	O
does	O
not	O
,	O
in	O
general	O
,	O
constitute	O
a	O
representation	O
of	O
any	O
valid	O
joint	B
distribution	I
(	O
heckerman	O
et	O
al	O
.	O
2000	O
)	O
,	O
so	O
the	O
output	O
of	O
the	O
gibbs	O
sampler	O
may	O
not	O
be	O
meaningful	O
.	O
nevertheless	O
,	O
the	O
method	O
can	O
sometimes	O
give	O
reasonable	O
results	O
if	O
there	O
is	O
not	O
much	O
missing	B
data	I
,	O
and	O
it	O
is	O
a	O
useful	O
method	O
for	O
data	O
imputation	O
(	O
gelman	O
and	O
raghunathan	O
2001	O
)	O
.	O
in	O
addition	O
,	O
the	O
method	O
can	O
be	O
used	O
as	O
an	O
initialization	O
technique	O
for	O
more	O
complex	O
structure	B
learning	I
methods	O
that	O
we	O
discuss	O
below	O
.	O
26.3	O
learning	B
tree	O
structures	O
for	O
the	O
rest	O
of	O
this	O
chapter	O
,	O
we	O
focus	O
on	O
learning	B
fully	O
speciﬁed	O
joint	O
probability	O
models	O
,	O
which	O
can	O
be	O
used	O
for	O
density	B
estimation	I
,	O
prediction	O
and	O
knowledge	B
discovery	I
.	O
since	O
the	O
problem	O
of	O
structure	B
learning	I
for	O
general	O
graphs	O
is	O
np-hard	O
(	O
chickering	O
1996	O
)	O
,	O
we	O
start	O
by	O
considering	O
the	O
special	O
case	O
of	O
trees	O
.	O
trees	O
are	O
special	O
because	O
we	O
can	O
learn	O
their	O
structure	O
efficiently	O
,	O
as	O
we	O
disuscs	O
below	O
,	O
and	O
because	O
,	O
once	O
we	O
have	O
learned	O
the	O
tree	B
,	O
we	O
can	O
use	O
them	O
for	O
efficient	O
exact	O
inference	B
,	O
as	O
discussed	O
in	O
section	O
20.2	O
.	O
26.3.	O
learning	B
tree	O
structures	O
911	O
1	O
4	O
2	O
3	O
1	O
4	O
2	O
3	O
1	O
4	O
2	O
3	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
26.3	O
an	O
undirected	B
tree	O
and	O
two	O
equivalent	O
directed	O
trees	O
.	O
26.3.1	O
directed	B
or	O
undirected	B
tree	O
?	O
before	O
continuing	O
,	O
we	O
need	O
to	O
discuss	O
the	O
issue	O
of	O
whether	O
we	O
should	O
use	O
directed	B
or	O
undirected	B
trees	O
.	O
a	O
directed	B
tree	O
,	O
with	O
a	O
single	O
root	O
node	O
r	O
,	O
deﬁnes	O
a	O
joint	B
distribution	I
as	O
follows	O
:	O
p	O
(	O
x|t	O
)	O
=	O
p	O
(	O
xt|xpa	O
(	O
t	O
)	O
)	O
where	O
we	O
deﬁne	O
pa	O
(	O
r	O
)	O
=	O
∅	O
.	O
for	O
example	O
,	O
in	O
figure	O
26.3	O
(	O
b-c	O
)	O
,	O
we	O
have	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4|t	O
)	O
=p	O
(	O
x1	O
)	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
x3|x2	O
)	O
p	O
(	O
x4|x2	O
)	O
=	O
p	O
(	O
x2	O
)	O
p	O
(	O
x1|x2	O
)	O
p	O
(	O
x3|x2	O
)	O
p	O
(	O
x4|x2	O
)	O
we	O
see	O
that	O
the	O
choice	O
of	O
root	B
does	O
not	O
matter	O
:	O
both	O
of	O
these	O
models	O
are	O
equivalent	O
.	O
to	O
make	O
the	O
model	O
more	O
symmetric	B
,	O
it	O
is	O
preferable	O
to	O
use	O
an	O
undirected	B
tree	O
.	O
this	O
can	O
be	O
represented	O
as	O
follows	O
:	O
p	O
(	O
x|t	O
)	O
=	O
p	O
(	O
xt	O
)	O
(	O
cid:20	O
)	O
(	O
s	O
,	O
t	O
)	O
∈e	O
(	O
cid:20	O
)	O
t∈v	O
(	O
cid:20	O
)	O
t∈v	O
(	O
26.1	O
)	O
(	O
26.2	O
)	O
(	O
26.3	O
)	O
(	O
26.5	O
)	O
(	O
26.6	O
)	O
(	O
26.7	O
)	O
(	O
26.8	O
)	O
p	O
(	O
xs	O
,	O
xt	O
)	O
p	O
(	O
xs	O
)	O
p	O
(	O
xt	O
)	O
(	O
26.4	O
)	O
where	O
p	O
(	O
xs	O
,	O
xt	O
)	O
is	O
an	O
edge	O
marginal	O
and	O
p	O
(	O
xt	O
)	O
is	O
a	O
node	O
marginal	O
.	O
for	O
example	O
,	O
in	O
figure	O
26.3	O
(	O
a	O
)	O
we	O
have	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4|t	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3	O
)	O
p	O
(	O
x4	O
)	O
p	O
(	O
x1	O
,	O
x2	O
)	O
p	O
(	O
x2	O
,	O
x3	O
)	O
p	O
(	O
x2	O
,	O
x4	O
)	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x4	O
)	O
to	O
see	O
the	O
equivalence	O
with	O
the	O
directed	B
representation	O
,	O
let	O
us	O
cancel	O
terms	O
to	O
get	O
p	O
(	O
x1	O
,	O
x2	O
,	O
x3	O
,	O
x4|t	O
)	O
=p	O
(	O
x1	O
,	O
x2	O
)	O
p	O
(	O
x2	O
,	O
x3	O
)	O
p	O
(	O
x2	O
,	O
x4	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x2	O
)	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2|x1	O
)	O
p	O
(	O
x3|x2	O
)	O
p	O
(	O
x4|x2	O
)	O
=	O
p	O
(	O
x2	O
)	O
p	O
(	O
x1|x2	O
)	O
p	O
(	O
x3|x2	O
)	O
p	O
(	O
x4|x2	O
)	O
where	O
p	O
(	O
xt|xs	O
)	O
=	O
p	O
(	O
xs	O
,	O
xt	O
)	O
/p	O
(	O
xs	O
)	O
.	O
thus	O
a	O
tree	B
can	O
be	O
represented	O
as	O
either	O
an	O
undirected	B
or	O
directed	B
graph	O
:	O
the	O
number	O
of	O
parameters	O
is	O
the	O
same	O
,	O
and	O
hence	O
the	O
complexity	O
of	O
learning	B
is	O
the	O
same	O
.	O
and	O
of	O
course	O
,	O
inference	B
is	O
the	O
same	O
in	O
both	O
representations	O
,	O
too	O
.	O
the	O
undirected	B
representation	O
,	O
which	O
is	O
symmetric	B
,	O
is	O
useful	O
for	O
structure	B
learning	I
,	O
but	O
the	O
directed	B
representation	O
is	O
more	O
convenient	O
for	O
parameter	B
learning	O
.	O
912	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
26.3.2	O
chow-liu	O
algorithm	O
for	O
ﬁnding	O
the	O
ml	O
tree	B
structure	O
using	O
equation	O
26.4	O
,	O
we	O
can	O
write	O
the	O
log-likelihood	O
for	O
a	O
tree	B
as	O
follows	O
:	O
log	O
p	O
(	O
d|θ	O
,	O
t	O
)	O
=	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
ntk	O
log	O
p	O
(	O
xt	O
=	O
k|θ	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
k	O
t	O
+	O
s	O
,	O
t	O
j	O
,	O
k	O
nstjk	O
log	O
p	O
(	O
xs	O
=	O
j	O
,	O
xt	O
=	O
k|θ	O
)	O
p	O
(	O
xs	O
=	O
j|θ	O
)	O
p	O
(	O
xt	O
=	O
k|θ	O
)	O
(	O
26.9	O
)	O
where	O
nstjk	O
is	O
the	O
number	O
of	O
times	O
node	O
s	O
is	O
in	O
state	B
j	O
and	O
node	O
t	O
is	O
in	O
state	B
k	O
,	O
and	O
ntk	O
is	O
the	O
number	O
of	O
times	O
node	O
t	O
is	O
in	O
state	B
k.	O
we	O
can	O
rewrite	O
these	O
counts	O
in	O
terms	O
of	O
the	O
empirical	B
distribution	I
:	O
nstjk	O
=	O
n	O
pemp	O
(	O
xs	O
=	O
j	O
,	O
xt	O
=	O
k	O
)	O
and	O
ntk	O
=	O
n	O
pemp	O
(	O
xt	O
=	O
k	O
)	O
.	O
setting	O
θ	O
to	O
the	O
mles	O
,	O
this	O
becomes	O
log	O
p	O
(	O
d|θ	O
,	O
t	O
)	O
(	O
cid:4	O
)	O
pemp	O
(	O
xt	O
=	O
k	O
)	O
log	O
pemp	O
(	O
xt	O
=	O
k	O
)	O
(	O
26.10	O
)	O
n	O
=	O
t∈v	O
i	O
(	O
xs	O
,	O
xt|ˆθst	O
)	O
(	O
26.11	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
k	O
+	O
(	O
s	O
,	O
t	O
)	O
∈e	O
(	O
t	O
)	O
where	O
i	O
(	O
xs	O
,	O
xt|ˆθst	O
)	O
≥	O
0	O
is	O
the	O
mutual	B
information	I
between	O
xs	O
and	O
xt	O
given	O
the	O
empirical	B
distribution	I
:	O
i	O
(	O
xs	O
,	O
xt|ˆθst	O
)	O
=	O
pemp	O
(	O
xs	O
=	O
j	O
,	O
xt	O
=	O
k	O
)	O
log	O
pemp	O
(	O
xs	O
=	O
j	O
,	O
xt	O
=	O
k	O
)	O
pemp	O
(	O
xs	O
=	O
j	O
)	O
pemp	O
(	O
xt	O
=	O
k	O
)	O
(	O
26.12	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
j	O
k	O
since	O
the	O
ﬁrst	O
term	O
in	O
equation	O
26.11	O
is	O
independent	O
of	O
the	O
topology	O
t	O
,	O
we	O
can	O
ignore	O
it	O
when	O
learning	B
structure	O
.	O
thus	O
the	O
tree	B
topology	O
that	O
maximizes	O
the	O
likelihood	B
can	O
be	O
found	O
by	O
computing	O
the	O
maximum	B
weight	I
spanning	I
tree	I
,	O
where	O
the	O
edge	O
weights	O
are	O
the	O
pairwise	O
mutual	O
informations	O
,	O
i	O
(	O
ys	O
,	O
yt|ˆθst	O
)	O
.	O
this	O
is	O
called	O
the	O
chow-liu	O
algorithm	O
(	O
chow	O
and	O
liu	O
1968	O
)	O
.	O
there	O
are	O
several	O
algorithms	O
for	O
ﬁnding	O
a	O
max	O
spanning	O
tree	B
(	O
mst	O
)	O
.	O
the	O
two	O
best	O
known	O
are	O
prim	O
’	O
s	O
algorithm	O
and	O
kruskal	O
’	O
s	O
algorithm	O
.	O
both	O
can	O
be	O
implemented	O
to	O
run	O
in	O
o	O
(	O
e	O
log	O
v	O
)	O
time	O
,	O
where	O
e	O
=	O
v	O
2	O
is	O
the	O
number	O
of	O
edges	B
and	O
v	O
is	O
the	O
number	O
of	O
nodes	B
.	O
see	O
e.g.	O
,	O
(	O
sedgewick	O
and	O
wayne	O
2011	O
,	O
4.3	O
)	O
for	O
details	O
.	O
thus	O
the	O
overall	O
running	O
time	O
is	O
o	O
(	O
n	O
v	O
2	O
+	O
v	O
2	O
log	O
v	O
)	O
,	O
where	O
the	O
ﬁrst	O
term	O
is	O
the	O
cost	O
of	O
computing	O
the	O
sufficient	B
statistics	I
.	O
figure	O
26.4	O
gives	O
an	O
example	O
of	O
the	O
method	O
in	O
action	B
,	O
applied	O
to	O
the	O
binary	O
20	O
newsgroups	O
data	O
shown	O
in	O
figure	O
1.2.	O
the	O
tree	B
has	O
been	O
arbitrarily	O
rooted	O
at	O
the	O
node	O
representing	O
“	O
email	O
”	O
.	O
the	O
connections	O
that	O
are	O
learned	O
seem	O
intuitively	O
reasonable	O
.	O
26.3.3	O
finding	O
the	O
map	O
forest	B
since	O
all	O
trees	O
have	O
the	O
same	O
number	O
of	O
parameters	O
,	O
we	O
can	O
safely	O
used	O
the	O
maximum	O
likelihood	O
score	O
as	O
a	O
model	B
selection	I
criterion	O
without	O
worrying	O
about	O
overﬁtting	B
.	O
however	O
,	O
sometimes	O
we	O
may	O
want	O
to	O
ﬁt	O
a	O
forest	B
rather	O
than	O
a	O
single	O
tree	O
,	O
since	O
inference	B
in	O
a	O
forest	B
is	O
much	O
faster	O
than	O
in	O
a	O
tree	B
(	O
we	O
can	O
run	O
belief	B
propagation	I
in	O
each	O
tree	B
in	O
the	O
forest	B
in	O
parallel	O
)	O
.	O
the	O
mle	O
criterion	O
will	O
never	O
choose	O
to	O
omit	O
an	O
edge	O
.	O
however	O
,	O
if	O
we	O
use	O
the	O
marginal	B
likelihood	I
or	O
a	O
penalized	O
likelihood	O
(	O
such	O
as	O
bic	O
)	O
,	O
the	O
optimal	O
solution	O
may	O
be	O
a	O
forest	B
.	O
below	O
we	O
give	O
the	O
details	O
for	O
the	O
marginal	B
likelihood	I
case	O
.	O
26.3.	O
learning	B
tree	O
structures	O
913	O
email	O
ftp	O
phone	B
files	O
number	O
disk	O
format	O
windows	O
drive	O
memory	O
system	O
image	O
card	O
dos	O
driver	O
pc	O
program	O
version	O
car	O
scsi	O
data	O
problem	O
display	O
graphics	O
video	O
software	O
space	O
win	O
team	O
won	O
bmw	O
dealer	O
engine	O
honda	O
mac	O
help	O
server	O
launch	O
moon	O
nasa	O
orbit	O
shuttle	O
technology	O
fans	O
games	O
hockey	O
league	O
players	O
puck	O
season	O
oil	O
lunar	O
mars	O
earth	O
satellite	O
solar	O
mission	O
nhl	O
baseball	O
god	O
hit	O
bible	O
christian	O
jesus	O
religion	O
jews	O
government	O
israel	O
children	B
power	O
president	O
rights	O
state	B
war	O
health	O
human	O
law	O
university	O
world	O
aids	O
food	O
insurance	O
medicine	O
fact	O
gun	O
research	O
science	O
msg	O
water	O
patients	O
studies	O
case	O
course	O
evidence	B
question	O
computer	O
cancer	O
disease	O
doctor	O
vitamin	O
figure	O
26.4	O
the	O
mle	O
tree	B
on	O
the	O
20-newsgroup	O
data	O
.	O
from	O
figure	O
4.11	O
of	O
(	O
schmidt	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
mark	O
schmidt	O
.	O
(	O
a	O
topologically	O
equivalent	O
tree	O
can	O
be	O
produced	O
using	O
chowliutreedemo	O
.	O
)	O
in	O
section	O
26.4.2.2	O
,	O
we	O
explain	O
how	O
to	O
compute	O
the	O
marginal	B
likelihood	I
of	O
any	O
dag	O
using	O
a	O
dirichlet	O
prior	O
for	O
the	O
cpts	O
.	O
the	O
resulting	O
expression	O
can	O
be	O
written	O
as	O
follows	O
:	O
log	O
p	O
(	O
d|t	O
)	O
=	O
p	O
(	O
xit|xi	O
,	O
pa	O
(	O
t	O
)	O
|θt	O
)	O
p	O
(	O
θt	O
)	O
dθt	O
=	O
score	O
(	O
nt	O
,	O
pa	O
(	O
t	O
)	O
)	O
(	O
26.13	O
)	O
(	O
cid:4	O
)	O
(	O
cid:12	O
)	O
n	O
(	O
cid:20	O
)	O
log	O
t∈v	O
i=1	O
(	O
cid:4	O
)	O
t	O
where	O
nt	O
,	O
pa	O
(	O
t	O
)	O
are	O
the	O
counts	O
(	O
sufficient	B
statistics	I
)	O
for	O
node	O
t	O
and	O
its	O
parents	B
,	O
and	O
score	O
is	O
deﬁned	O
in	O
equation	O
26.28.	O
now	O
suppose	O
we	O
only	O
allow	O
dags	O
with	O
at	O
most	O
one	O
parent	O
.	O
following	O
(	O
heckerman	O
et	O
al	O
.	O
1995	O
,	O
p227	O
)	O
,	O
let	O
us	O
associate	O
a	O
weight	O
with	O
each	O
s	O
→	O
t	O
edge	O
,	O
ws	O
,	O
t	O
(	O
cid:2	O
)	O
score	O
(	O
t|s	O
)	O
−	O
score	O
(	O
t|0	O
)	O
,	O
where	O
score	O
(	O
t|0	O
)	O
is	O
the	O
score	O
when	O
t	O
has	O
no	O
parents	O
.	O
note	O
that	O
the	O
weights	O
might	O
be	O
negative	O
(	O
unlike	O
the	O
mle	O
case	O
,	O
where	O
edge	O
weights	O
are	O
aways	O
non-negative	O
because	O
they	O
correspond	O
to	O
mutual	B
information	I
)	O
.	O
then	O
we	O
can	O
rewrite	O
the	O
objective	O
as	O
follows	O
:	O
log	O
p	O
(	O
d|t	O
)	O
=	O
score	O
(	O
t|pa	O
(	O
t	O
)	O
)	O
=	O
wpa	O
(	O
t	O
)	O
,	O
t	O
+	O
score	O
(	O
t|0	O
)	O
(	O
26.14	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
t	O
t	O
t	O
the	O
last	O
term	O
is	O
the	O
same	O
for	O
all	O
trees	O
t	O
,	O
so	O
we	O
can	O
ignore	O
it	O
.	O
thus	O
ﬁnding	O
the	O
most	O
probable	O
tree	B
amounts	O
to	O
ﬁnding	O
a	O
maximal	B
branching	I
in	O
the	O
corresponding	O
weighted	O
directed	O
graph	B
.	O
this	O
can	O
be	O
found	O
using	O
the	O
algorithm	O
in	O
(	O
gabow	O
et	O
al	O
.	O
1984	O
)	O
.	O
914	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
if	O
the	O
scoring	O
function	O
is	O
prior	O
and	O
likelihood	B
equivalent	I
(	O
these	O
terms	O
are	O
explained	O
in	O
sec-	O
tion	O
26.4.2.3	O
)	O
,	O
we	O
have	O
score	O
(	O
s|t	O
)	O
+score	O
(	O
t|0	O
)	O
=	O
score	O
(	O
t|s	O
)	O
+score	O
(	O
s|0	O
)	O
(	O
26.15	O
)	O
and	O
hence	O
the	O
weight	O
matrix	O
is	O
symmetric	B
.	O
in	O
this	O
case	O
,	O
the	O
maximal	B
branching	I
is	O
the	O
same	O
as	O
the	O
maximal	O
weight	O
forest	O
.	O
we	O
can	O
apply	O
a	O
slightly	O
modiﬁed	O
version	O
of	O
the	O
mst	O
algorithm	O
to	O
ﬁnd	O
this	O
(	O
edwards	O
et	O
al	O
.	O
2010	O
)	O
.	O
to	O
see	O
this	O
,	O
let	O
g	O
=	O
(	O
v	O
,	O
e	O
)	O
be	O
a	O
graph	B
with	O
both	O
positive	O
and	O
negative	O
edge	O
weights	O
.	O
now	O
let	O
g	O
(	O
cid:2	O
)	O
be	O
a	O
graph	B
obtained	O
by	O
omitting	O
all	O
the	O
negative	O
edges	O
from	O
g.	O
this	O
can	O
not	O
reduce	O
the	O
total	O
weight	O
,	O
so	O
we	O
can	O
ﬁnd	O
the	O
maximum	O
weight	O
forest	O
of	O
g	O
by	O
ﬁnding	O
the	O
mst	O
for	O
each	O
connected	O
component	O
of	O
g	O
(	O
cid:2	O
)	O
.	O
we	O
can	O
do	O
this	O
by	O
running	O
kruskal	O
’	O
s	O
algorithm	O
directly	O
on	O
g	O
(	O
cid:2	O
)	O
:	O
there	O
is	O
no	O
need	O
to	O
ﬁnd	O
the	O
connected	O
components	O
explicitly	O
.	O
26.3.4	O
mixtures	O
of	O
trees	O
a	O
single	O
tree	O
is	O
rather	O
limited	O
in	O
its	O
expressive	O
power	O
.	O
later	O
in	O
this	O
chapter	O
we	O
discuss	O
ways	O
to	O
learn	O
more	O
general	O
graphs	O
.	O
however	O
,	O
the	O
resulting	O
graphs	O
can	O
be	O
expensive	O
to	O
do	O
inference	O
in	O
.	O
an	O
interesting	O
alternative	O
is	O
to	O
learn	O
a	O
mixture	B
of	I
trees	I
(	O
meila	O
and	O
jordan	O
2000	O
)	O
,	O
where	O
each	O
mixture	B
component	O
may	O
have	O
a	O
different	O
tree	B
topology	O
.	O
this	O
is	O
like	O
an	O
unsupervised	O
version	O
of	O
the	O
tan	O
classiﬁer	O
discussed	O
in	O
section	O
10.2.1.	O
we	O
can	O
ﬁt	O
a	O
mixture	B
of	I
trees	I
by	O
using	O
em	O
:	O
in	O
the	O
e	O
step	O
,	O
we	O
compute	O
the	O
responsibilities	O
of	O
each	O
cluster	O
for	O
each	O
data	O
point	O
,	O
and	O
in	O
the	O
m	O
step	O
,	O
we	O
use	O
a	O
weighted	O
version	O
of	O
the	O
chow-liu	O
algorithm	O
.	O
see	O
(	O
meila	O
and	O
jordan	O
2000	O
)	O
for	O
details	O
.	O
in	O
fact	O
,	O
it	O
is	O
possible	O
to	O
create	O
an	O
“	O
inﬁnite	O
mixture	O
of	O
trees	O
”	O
,	O
by	O
integrating	O
out	O
over	O
all	O
possible	O
trees	O
.	O
remarkably	O
,	O
this	O
can	O
be	O
done	O
in	O
v	O
3	O
time	O
using	O
the	O
matrix	B
tree	I
theorem	I
.	O
this	O
allows	O
us	O
to	O
perform	O
exact	O
bayesian	O
inference	B
of	O
posterior	O
edge	O
marginals	O
etc	O
.	O
however	O
,	O
it	O
is	O
not	O
tractable	O
to	O
use	O
this	O
inﬁnite	O
mixture	O
for	O
inference	B
of	O
hidden	B
nodes	I
.	O
see	O
(	O
meila	O
and	O
jaakkola	O
2006	O
)	O
for	O
details	O
.	O
26.4	O
learning	B
dag	O
structures	O
in	O
this	O
section	O
,	O
we	O
discuss	O
how	O
to	O
compute	O
(	O
functions	O
of	O
)	O
p	O
(	O
g|d	O
)	O
,	O
where	O
g	O
is	O
constrained	O
to	O
be	O
a	O
dag	O
.	O
this	O
is	O
often	O
called	O
bayesian	O
network	O
structure	B
learning	I
.	O
in	O
this	O
section	O
,	O
we	O
assume	O
there	O
is	O
no	O
missing	O
data	O
,	O
and	O
that	O
there	O
are	O
no	O
hidden	O
variables	O
.	O
this	O
is	O
called	O
the	O
complete	B
data	I
assumption	I
.	O
for	O
simplicity	O
,	O
we	O
will	O
focus	O
on	O
the	O
case	O
where	O
all	O
the	O
variables	O
are	O
categorical	B
and	O
all	O
the	O
cpds	O
are	O
tables	O
,	O
although	O
the	O
results	O
generalize	B
to	O
real-valued	O
data	O
and	O
other	O
kinds	O
of	O
cpds	O
,	O
such	O
as	O
linear-gaussian	O
cpds	O
.	O
our	O
presentation	O
is	O
based	O
in	O
part	O
on	O
(	O
heckerman	O
et	O
al	O
.	O
1995	O
)	O
,	O
although	O
we	O
will	O
follow	O
the	O
notation	O
of	O
section	O
10.4.2.	O
in	O
particular	O
,	O
let	O
xit	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
kt	O
}	O
be	O
the	O
value	O
of	O
node	O
t	O
in	O
case	O
i	O
,	O
where	O
kt	O
is	O
the	O
number	O
of	O
states	O
for	O
node	O
t.	O
let	O
θtck	O
(	O
cid:2	O
)	O
p	O
(	O
xt	O
=	O
k|xpa	O
(	O
t	O
)	O
=	O
c	O
)	O
,	O
for	O
k	O
=	O
1	O
:	O
k	O
t	O
,	O
and	O
c	O
=	O
1	O
:	O
c	O
t	O
,	O
where	O
ct	O
is	O
the	O
number	O
of	O
parent	O
combinations	O
(	O
possible	O
conditioning	B
cases	O
)	O
.	O
for	O
notational	O
simplicity	O
,	O
we	O
will	O
often	O
assume	O
kt	O
=	O
k	O
,	O
so	O
all	O
nodes	O
have	O
the	O
same	O
number	O
of	O
states	O
.	O
we	O
will	O
also	O
let	O
dt	O
=	O
dim	O
(	O
pa	O
(	O
t	O
)	O
)	O
be	O
the	O
degree	B
or	O
fan-in	B
of	O
node	O
t	O
,	O
so	O
that	O
ct	O
=	O
k	O
dt	O
.	O
26.4.1	O
markov	O
equivalence	O
in	O
this	O
section	O
,	O
we	O
discuss	O
some	O
fundamental	O
limits	O
to	O
our	O
ability	O
to	O
learn	O
dag	O
structures	O
from	O
data	O
.	O
26.4.	O
learning	B
dag	O
structures	O
915	O
g1	O
g2	O
g3	O
x1	O
x3	O
x1	O
x3	O
x1	O
x3	O
x2	O
x2	O
x2	O
x5	O
x5	O
x5	O
x4	O
x4	O
x4	O
figure	O
26.5	O
three	O
dags	O
.	O
g1	O
and	O
g3	O
are	O
markov	O
equivalent	O
,	O
g2	O
is	O
not	O
.	O
consider	O
the	O
following	O
3	O
dgms	O
:	O
x	O
→	O
y	O
→	O
z	O
,	O
x	O
←	O
y	O
←	O
z	O
and	O
x	O
←	O
y	O
→	O
z.	O
these	O
all	O
represent	O
the	O
same	O
set	O
of	O
ci	O
statements	O
,	O
namely	O
x	O
⊥	O
z|y	O
,	O
x	O
(	O
cid:8	O
)	O
⊥	O
z	O
(	O
26.16	O
)	O
we	O
say	O
these	O
graphs	O
are	O
markov	O
equivalent	O
,	O
since	O
they	O
encode	O
the	O
same	O
set	O
of	O
ci	O
assumptions	O
.	O
that	O
is	O
,	O
they	O
all	O
belong	O
to	O
the	O
same	O
markov	O
equivalence	B
class	I
.	O
however	O
,	O
the	O
v-structure	B
x	O
→	O
y	O
←	O
z	O
encodes	O
x	O
⊥	O
z	O
and	O
x	O
(	O
cid:8	O
)	O
⊥	O
z|y	O
,	O
which	O
represents	O
the	O
opposite	O
set	O
of	O
ci	O
assumptions	O
.	O
one	O
can	O
prove	O
the	O
following	O
theorem	O
.	O
theorem	O
26.4.1	O
(	O
verma	O
and	O
pearl	O
(	O
verma	O
and	O
pearl	O
1990	O
)	O
)	O
.	O
two	O
structures	O
are	O
markov	O
equivalent	O
iff	O
they	O
have	O
the	O
same	O
undirected	B
skeleton	O
and	O
the	O
same	O
set	O
of	O
v-structures	O
.	O
for	O
example	O
,	O
referring	O
to	O
figure	O
26.5	O
,	O
we	O
see	O
that	O
g1	O
(	O
cid:8	O
)	O
≡	O
g2	O
,	O
since	O
reversing	O
the	O
2	O
→	O
4	O
arc	O
creates	O
a	O
new	O
v-structure	B
.	O
however	O
,	O
g1	O
≡	O
g3	O
,	O
since	O
reversing	O
the	O
1	O
→	O
5	O
arc	O
does	O
not	O
create	O
a	O
new	O
v-structure	B
.	O
we	O
can	O
represent	O
a	O
markov	O
equivalence	B
class	I
using	O
a	O
single	O
partially	O
directed	B
acyclic	I
graph	I
(	O
pdag	O
)	O
,	O
also	O
called	O
an	O
essential	B
graph	I
or	O
pattern	B
,	O
in	O
which	O
some	O
edges	B
are	O
directed	B
and	O
some	O
undirected	B
.	O
the	O
undirected	B
edges	O
represent	O
reversible	O
edges	O
;	O
any	O
combination	O
is	O
possible	O
so	O
long	O
as	O
no	O
new	O
v-structures	O
are	O
created	O
.	O
the	O
directed	B
edges	O
are	O
called	O
compelled	B
edges	I
,	O
since	O
changing	O
their	O
orientation	O
would	O
change	O
the	O
v-structures	O
and	O
hence	O
change	O
the	O
equivalence	B
class	I
.	O
for	O
example	O
,	O
the	O
pdag	O
x	O
−	O
y	O
−	O
z	O
represents	O
{	O
x	O
→	O
y	O
→	O
z	O
,	O
x	O
←	O
y	O
←	O
z	O
,	O
x	O
←	O
y	O
→	O
z	O
}	O
which	O
encodes	O
x	O
(	O
cid:8	O
)	O
⊥	O
z	O
and	O
x	O
⊥	O
z|y	O
.	O
see	O
figure	O
26.6.	O
the	O
signiﬁcance	O
of	O
the	O
above	O
theorem	O
is	O
that	O
,	O
when	O
we	O
learn	O
the	O
dag	O
structure	O
from	O
data	O
,	O
we	O
will	O
not	O
be	O
able	O
to	O
uniquely	O
identify	O
all	O
of	O
the	O
edge	O
directions	O
,	O
even	O
given	O
an	O
inﬁnite	O
amount	O
of	O
data	O
.	O
we	O
say	O
that	O
we	O
can	O
learn	O
dag	O
structure	O
“	O
up	O
to	O
markov	O
equivalence	O
”	O
.	O
this	O
also	O
cautions	O
us	O
not	O
to	O
read	O
too	O
much	O
into	O
the	O
meaning	O
of	O
particular	O
edge	O
orientations	O
,	O
since	O
we	O
can	O
often	O
change	O
them	O
without	O
changing	O
the	O
model	O
in	O
any	O
observable	O
way	O
.	O
916	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
x	O
y	O
z	O
x	O
y	O
z	O
x	O
y	O
z	O
≡	O
x	O
y	O
z	O
x	O
y	O
z	O
≡	O
x	O
y	O
z	O
figure	O
26.6	O
pdag	O
representation	O
of	O
markov	O
equivalent	O
dags	O
.	O
26.4.2	O
exact	O
structural	O
inference	O
in	O
this	O
section	O
,	O
we	O
discuss	O
how	O
to	O
compute	O
the	O
exact	O
posterior	O
over	O
graphs	O
,	O
p	O
(	O
g|d	O
)	O
,	O
ignoring	O
for	O
now	O
the	O
issue	O
of	O
computational	O
tractability	O
.	O
26.4.2.1	O
deriving	O
the	O
likelihood	B
assuming	O
there	O
is	O
no	O
missing	O
data	O
,	O
and	O
that	O
all	O
cpds	O
are	O
tabular	O
,	O
the	O
likelihood	B
can	O
be	O
written	O
as	O
follows	O
:	O
i=1	O
n	O
(	O
cid:20	O
)	O
n	O
(	O
cid:20	O
)	O
n	O
(	O
cid:20	O
)	O
v	O
(	O
cid:20	O
)	O
i=1	O
i=1	O
t=1	O
v	O
(	O
cid:20	O
)	O
v	O
(	O
cid:20	O
)	O
v	O
(	O
cid:20	O
)	O
ct	O
(	O
cid:20	O
)	O
t=1	O
t=1	O
p	O
(	O
d|g	O
,	O
θ	O
)	O
=	O
=	O
=	O
=	O
cat	O
(	O
xit|xi	O
,	O
pa	O
(	O
t	O
)	O
,	O
θt	O
)	O
ct	O
(	O
cid:20	O
)	O
ct	O
(	O
cid:20	O
)	O
kt	O
(	O
cid:20	O
)	O
cat	O
(	O
xit|θtc	O
)	O
i	O
(	O
xi	O
,	O
pa	O
(	O
t	O
)	O
=c	O
)	O
kt	O
(	O
cid:20	O
)	O
θi	O
(	O
xi	O
,	O
t=k	O
,	O
xi	O
,	O
pa	O
(	O
t	O
)	O
=c	O
)	O
tck	O
k=1	O
c=1	O
c=1	O
θntck	O
tck	O
(	O
26.17	O
)	O
(	O
26.18	O
)	O
(	O
26.19	O
)	O
(	O
26.20	O
)	O
t=1	O
c=1	O
k=1	O
where	O
ntck	O
is	O
the	O
number	O
of	O
times	O
node	O
t	O
is	O
in	O
state	B
k	O
and	O
its	O
parents	B
are	O
in	O
state	B
c.	O
(	O
technically	O
these	O
counts	O
depend	O
on	O
the	O
graph	B
structure	O
g	O
,	O
but	O
we	O
drop	O
this	O
from	O
the	O
notation	O
.	O
)	O
26.4.2.2	O
deriving	O
the	O
marginal	B
likelihood	I
of	O
course	O
,	O
choosing	O
the	O
graph	B
with	O
the	O
maximum	O
likelihood	O
will	O
always	O
pick	O
a	O
fully	O
connected	O
graph	B
(	O
subject	O
to	O
the	O
acyclicity	O
constraint	O
)	O
,	O
since	O
this	O
maximizes	O
the	O
number	O
of	O
parameters	O
.	O
to	O
avoid	O
such	O
overﬁtting	B
,	O
we	O
will	O
choose	O
the	O
graph	B
with	O
the	O
maximum	O
marginal	O
likelihood	B
,	O
p	O
(	O
d|g	O
)	O
;	O
the	O
magic	O
of	O
the	O
bayesian	O
occam	O
’	O
s	O
razor	O
will	O
then	O
penalize	O
overly	O
complex	O
graphs	O
.	O
to	O
compute	O
the	O
marginal	B
likelihood	I
,	O
we	O
need	O
to	O
specify	O
priors	O
on	O
the	O
parameters	O
.	O
we	O
will	O
make	O
two	O
standard	O
assumptions	O
.	O
first	O
,	O
we	O
assume	O
global	B
prior	I
parameter	I
independence	I
,	O
which	O
means	O
p	O
(	O
θ	O
)	O
=	O
p	O
(	O
θt	O
)	O
(	O
26.21	O
)	O
v	O
(	O
cid:20	O
)	O
t=1	O
26.4.	O
learning	B
dag	O
structures	O
second	O
,	O
we	O
assume	O
local	B
prior	I
parameter	I
independence	I
,	O
which	O
means	O
ct	O
(	O
cid:20	O
)	O
p	O
(	O
θt	O
)	O
=	O
p	O
(	O
θtc	O
)	O
917	O
(	O
26.22	O
)	O
c=1	O
for	O
each	O
t.	O
must	O
be	O
a	O
dirichlet	O
(	O
geiger	O
and	O
heckerman	O
1997	O
)	O
,	O
that	O
is	O
,	O
it	O
turns	O
out	O
that	O
these	O
assumtions	O
imply	O
that	O
the	O
prior	O
for	O
each	O
row	O
of	O
each	O
cpt	O
p	O
(	O
θtc	O
)	O
=	O
dir	O
(	O
θtc|αtc	O
)	O
(	O
26.23	O
)	O
given	O
these	O
assumptions	O
,	O
and	O
using	O
the	O
results	O
of	O
section	O
5.3.2.2	O
,	O
we	O
can	O
write	O
down	O
the	O
marginal	B
likelihood	I
of	O
any	O
dag	O
as	O
follows	O
:	O
(	O
cid:12	O
)	O
⎡	O
⎣	O
(	O
cid:20	O
)	O
⎤	O
⎦	O
dir	O
(	O
θtc	O
)	O
dθtc	O
cat	O
(	O
xit|θtc	O
)	O
i	O
:	O
xi	O
,	O
pa	O
(	O
t	O
)	O
=c	O
b	O
(	O
ntc	O
+	O
αtc	O
)	O
b	O
(	O
αtc	O
)	O
γ	O
(	O
ntc	O
)	O
kt	O
(	O
cid:20	O
)	O
γ	O
(	O
ntck	O
+	O
αg	O
tck	O
)	O
γ	O
(	O
ntc	O
+	O
αtc	O
)	O
c=1	O
k=1	O
γ	O
(	O
αg	O
ijk	O
)	O
p	O
(	O
d|g	O
)	O
=	O
=	O
=	O
(	O
26.24	O
)	O
(	O
26.25	O
)	O
(	O
26.26	O
)	O
(	O
26.27	O
)	O
c=1	O
ct	O
(	O
cid:20	O
)	O
ct	O
(	O
cid:20	O
)	O
ct	O
(	O
cid:20	O
)	O
c=1	O
t=1	O
v	O
(	O
cid:20	O
)	O
v	O
(	O
cid:20	O
)	O
v	O
(	O
cid:20	O
)	O
v	O
(	O
cid:20	O
)	O
t=1	O
t=1	O
t=1	O
ct	O
(	O
cid:20	O
)	O
c=1	O
=	O
(	O
cid:7	O
)	O
score	O
(	O
nt	O
,	O
pa	O
(	O
t	O
)	O
)	O
(	O
cid:7	O
)	O
where	O
ntc	O
=	O
node	O
t	O
and	O
its	O
parents	B
,	O
and	O
score	O
(	O
)	O
is	O
a	O
local	O
scoring	O
function	O
deﬁned	O
by	O
k	O
αtck	O
,	O
nt	O
,	O
pa	O
(	O
t	O
)	O
is	O
the	O
vector	O
of	O
counts	O
(	O
sufficient	B
statistics	I
)	O
for	O
k	O
ntck	O
,	O
αtc	O
=	O
score	O
(	O
nt	O
,	O
pa	O
(	O
t	O
)	O
)	O
(	O
cid:2	O
)	O
b	O
(	O
ntc	O
+	O
αtc	O
)	O
b	O
(	O
αtc	O
)	O
(	O
26.28	O
)	O
we	O
say	O
that	O
the	O
marginal	B
likelihood	I
decomposes	O
or	O
factorizes	O
according	O
to	O
the	O
graph	B
structure	O
.	O
26.4.2.3	O
setting	O
the	O
prior	O
how	O
should	O
we	O
set	O
the	O
hyper-parameters	B
αtck	O
?	O
it	O
is	O
tempting	O
to	O
use	O
a	O
jeffreys	O
prior	O
of	O
the	O
form	O
αtck	O
=	O
1	O
2	O
(	O
equation	O
5.62	O
)	O
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
this	O
violates	O
a	O
property	O
called	O
likelihood	B
equivalence	I
,	O
which	O
is	O
sometimes	O
considered	O
desirable	O
.	O
this	O
property	O
says	O
that	O
if	O
g1	O
and	O
g2	O
are	O
markov	O
equivalent	O
(	O
section	O
26.4.1	O
)	O
,	O
they	O
should	O
have	O
the	O
same	O
marginal	B
likelihood	I
,	O
since	O
they	O
are	O
essentially	O
equivalent	O
models	O
.	O
geiger	O
and	O
heckerman	O
(	O
1997	O
)	O
proved	O
that	O
,	O
for	O
complete	B
graphs	O
,	O
the	O
only	O
prior	O
that	O
satisﬁes	O
likelihood	B
equivalence	I
and	O
parameter	B
independence	O
is	O
the	O
dirichlet	O
prior	O
,	O
where	O
the	O
pseudo	B
counts	I
have	O
the	O
form	O
αtck	O
=	O
α	O
p0	O
(	O
xt	O
=	O
k	O
,	O
xpa	O
(	O
t	O
)	O
=	O
c	O
)	O
(	O
26.29	O
)	O
where	O
α	O
>	O
0	O
is	O
called	O
the	O
equivalent	B
sample	I
size	I
,	O
and	O
p0	O
is	O
some	O
prior	O
joint	O
probability	O
dis-	O
tribution	O
.	O
this	O
is	O
called	O
the	O
bde	O
prior	O
,	O
which	O
stands	O
for	O
bayesian	O
dirichlet	O
likelihood	B
equivalent	I
.	O
918	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	O
to	O
derive	O
the	O
hyper-parameters	B
for	O
other	O
graph	B
structures	O
,	O
geiger	O
and	O
heckerman	O
(	O
1997	O
)	O
invoked	O
an	O
additional	O
assumption	O
called	O
parameter	B
modularity	I
,	O
which	O
says	O
that	O
if	O
node	O
xt	O
has	O
the	O
same	O
parents	B
in	O
g1	O
and	O
g2	O
,	O
then	O
p	O
(	O
θt|g1	O
)	O
=p	O
(	O
θt|g2	O
)	O
.	O
with	O
this	O
assumption	O
,	O
we	O
can	O
always	O
derive	O
αt	O
for	O
a	O
node	O
t	O
in	O
any	O
other	O
graph	B
by	O
marginalizing	O
the	O
pseudo	B
counts	I
in	O
equation	O
26.29.	O
typically	O
the	O
prior	O
distribution	O
p0	O
is	O
assumed	O
to	O
be	O
uniform	O
over	O
all	O
possible	O
joint	O
conﬁgura-	O
tions	O
.	O
in	O
this	O
case	O
,	O
we	O
have	O
αtck	O
=	O
α	O
ktct	O
(	O
26.30	O
)	O
ktct	O
.	O
thus	O
if	O
we	O
sum	O
the	O
pseudo	B
counts	I
over	O
all	O
ct	O
×	O
kt	O
since	O
p0	O
(	O
xt	O
=	O
k	O
,	O
xpa	O
(	O
t	O
)	O
=	O
c	O
)	O
=	O
1	O
entries	O
in	O
the	O
cpt	O
,	O
we	O
get	O
a	O
total	O
equivalent	O
sample	O
size	O
of	O
α.	O
this	O
is	O
called	O
the	O
bdeu	O
prior	O
,	O
where	O
the	O
“	O
u	O
”	O
stands	O
for	O
uniform	O
.	O
this	O
is	O
the	O
most	O
widely	O
used	O
prior	O
for	O
learning	B
bayes	O
net	O
structures	O
.	O
for	O
advice	O
on	O
setting	O
the	O
global	O
tuning	O
parameter	B
α	O
,	O
see	O
(	O
silander	O
et	O
al	O
.	O
2007	O
)	O
.	O
26.4.2.4	O
simple	O
worked	O
example	O
we	O
now	O
give	O
a	O
very	O
simple	O
worked	O
example	O
from	O
(	O
neapolitan	O
2003	O
,	O
p.438	O
)	O
.	O
suppose	O
we	O
have	O
just	O
2	O
binary	O
nodes	O
,	O
and	O
the	O
following	O
8	O
data	O
cases	O
:	O
x1	O
x2	O
1	O
1	O
1	O
2	O
1	O
2	O
1	O
2	O
suppose	O
we	O
are	O
interested	O
in	O
two	O
possible	O
graphs	O
:	O
g1	O
is	O
x1	O
→	O
x2	O
and	O
g2	O
is	O
the	O
disconnected	O
1	O
2	O
1	O
2	O
1	O
1	O
1	O
2	O
graph	B
.	O
the	O
empirical	O
counts	O
for	O
node	O
1	O
in	O
g1	O
are	O
n1	O
=	O
(	O
5	O
,	O
3	O
)	O
and	O
for	O
node	O
2	O
are	O
x1	O
=	O
1	O
x1	O
=	O
2	O
x2	O
=	O
1	O
x2	O
=	O
2	O
4	O
1	O
1	O
2	O
the	O
bdeu	O
prior	O
for	O
g1	O
is	O
α1	O
=	O
(	O
α/2	O
,	O
α/2	O
)	O
,	O
α2|x1=1	O
=	O
(	O
α/4	O
,	O
α/4	O
)	O
and	O
α2|x1=2	O
=	O
(	O
α/4	O
,	O
α/4	O
)	O
.	O
for	O
g2	O
,	O
the	O
prior	O
for	O
θ1	O
is	O
the	O
same	O
,	O
and	O
for	O
θ2	O
it	O
is	O
α2|x1=1	O
=	O
(	O
α/2	O
,	O
α/2	O
)	O
if	O
we	O
setα	O
=	O
4	O
,	O
and	O
use	O
the	O
bdeu	O
prior	O
,	O
we	O
ﬁnd	O
p	O
(	O
d|g1	O
)	O
=	O
and	O
α2|x1=2	O
=	O
(	O
α/2	O
,	O
α/2	O
)	O
.	O
7.2150	O
×	O
10	O
−6	O
.	O
hence	O
the	O
posterior	O
probabilites	O
,	O
under	O
a	O
uniform	O
graph	O
prior	O
,	O
are	O
p	O
(	O
g1|d	O
)	O
=	O
0.51678	O
and	O
p	O
(	O
g2|d	O
)	O
=	O
0.48322	O
.	O
−6	O
and	O
p	O
(	O
d|g2	O
)	O
=	O
6.7465	O
×	O
10	O
26.4.2.5	O
example	O
:	O
analysis	O
of	O
the	O
college	O
plans	O
dataset	O
we	O
now	O
consider	O
a	O
more	O
interesting	O
example	O
from	O
(	O
heckerman	O
et	O
al	O
.	O
1997	O
)	O
.	O
consider	O
the	O
data	O
set	O
collected	O
in	O
1968	O
by	O
sewell	O
and	O
shah	O
which	O
measured	O
5	O
variables	O
that	O
might	O
inﬂuence	O
the	O
decision	B
of	O
high	O
school	O
students	O
about	O
whether	O
to	O
attend	O
college	O
.	O
speciﬁcally	O
,	O
the	O
variables	O
are	O
as	O
follows	O
:	O
26.4.	O
learning	B
dag	O
structures	O
919	O
figure	O
26.7	O
the	O
two	O
most	O
probable	O
dags	O
learned	O
from	O
the	O
sewell-shah	O
data	O
.	O
source	O
:	O
(	O
heckerman	O
et	O
al	O
.	O
1997	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
david	O
heckerman	O
•	O
sex	O
male	O
or	O
female	O
•	O
ses	O
socio	O
economic	O
status	O
:	O
low	O
,	O
lower	O
middle	O
,	O
upper	O
middle	O
or	O
high	O
.	O
•	O
•	O
pe	O
parental	O
encouragment	O
:	O
low	O
or	O
high	O
•	O
cp	O
college	O
plans	O
:	O
yes	O
or	O
no	O
.	O
iq	O
intelligence	O
quotient	O
:	O
discretized	O
into	O
low	O
,	O
lower	O
middle	O
,	O
upper	O
middle	O
or	O
high	O
.	O
these	O
variables	O
were	O
measured	O
for	O
10,318	O
wisconsin	O
high	O
school	O
seniors	O
.	O
there	O
are	O
2	O
×	O
4	O
×	O
4	O
×	O
2×	O
=	O
128	O
possible	O
joint	O
conﬁgurations	O
.	O
heckerman	O
et	O
al	O
.	O
computed	O
the	O
exact	O
posterior	O
over	O
all	O
29,281	O
possible	O
5	O
node	O
dags	O
,	O
except	O
for	O
ones	O
in	O
which	O
sex	O
and/or	O
ses	O
have	O
parents	B
,	O
and/or	O
cp	O
have	O
children	B
.	O
(	O
the	O
prior	O
probability	O
of	O
these	O
graphs	O
was	O
set	O
to	O
0	O
,	O
based	O
on	O
domain	O
knowledge	O
.	O
)	O
they	O
used	O
the	O
bdeu	O
score	O
with	O
α	O
=	O
5	O
,	O
although	O
they	O
said	O
that	O
the	O
results	O
were	O
robust	B
to	O
any	O
α	O
in	O
the	O
range	O
3	O
to	O
40.	O
the	O
top	O
two	O
graphs	O
are	O
shown	O
in	O
figure	O
26.7.	O
we	O
see	O
that	O
the	O
most	O
probable	O
one	O
has	O
approximately	O
all	O
of	O
the	O
probability	O
mass	O
,	O
so	O
the	O
posterior	O
is	O
extremely	O
peaked	O
.	O
it	O
is	O
tempting	O
to	O
interpret	O
this	O
graph	B
in	O
terms	O
of	O
causality	B
(	O
see	O
section	O
26.6	O
)	O
.	O
in	O
particular	O
,	O
it	O
seems	O
that	O
socio-economic	O
status	O
,	O
iq	O
and	O
parental	O
encouragment	O
all	O
causally	O
inﬂuence	O
the	O
decision	B
about	O
whether	O
to	O
go	O
to	O
college	O
,	O
which	O
makes	O
sense	O
.	O
also	O
,	O
sex	O
inﬂuences	O
college	O
plans	O
only	O
indirectly	O
through	O
parental	O
encouragement	O
,	O
which	O
also	O
makes	O
sense	O
.	O
however	O
,	O
the	O
direct	O
link	O
from	O
socio	O
economic	O
status	O
to	O
iq	O
seems	O
surprising	O
;	O
this	O
may	O
be	O
due	O
to	O
a	O
hidden	B
common	O
cause	O
.	O
in	O
section	O
26.5.1.4	O
we	O
will	O
re-examine	O
this	O
dataset	O
allowing	O
for	O
the	O
presence	O
of	O
hidden	B
variables	I
.	O
26.4.2.6	O
the	O
k2	O
algorithm	O
suppose	O
we	O
know	O
a	O
total	B
ordering	I
of	O
the	O
nodes	B
.	O
then	O
we	O
can	O
compute	O
the	O
distribution	O
over	O
parents	B
for	O
each	O
node	O
independently	O
,	O
without	O
the	O
risk	B
of	O
introducing	O
any	O
directed	B
cycles	O
:	O
we	O
920	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
simply	O
enumerate	O
over	O
all	O
possible	O
subsets	O
of	O
ancestors	B
and	O
compute	O
their	O
marginal	O
likelihoods.1	O
if	O
we	O
just	O
return	O
the	O
best	O
set	O
of	O
parents	B
for	O
each	O
node	O
,	O
we	O
get	O
the	O
the	O
k2	O
algorithm	O
(	O
cooper	O
and	O
herskovits	O
1992	O
)	O
.	O
26.4.2.7	O
handling	O
non-tabular	O
cpds	O
if	O
all	O
cpds	O
are	O
linear	O
gaussian	O
,	O
we	O
can	O
replace	O
the	O
dirichlet-multinomial	O
model	O
with	O
the	O
normal-	O
gamma	O
model	O
,	O
and	O
thus	O
derive	O
a	O
different	O
exact	O
expression	O
for	O
the	O
marginal	B
likelihood	I
.	O
see	O
(	O
geiger	O
and	O
heckerman	O
1994	O
)	O
for	O
the	O
details	O
.	O
in	O
fact	O
,	O
we	O
can	O
easily	O
combine	O
discrete	B
nodes	O
and	O
gaussian	O
nodes	B
,	O
as	O
long	O
as	O
the	O
discrete	B
nodes	O
always	O
have	O
discrete	B
parents	O
;	O
this	O
is	O
called	O
a	O
conditional	O
gaussian	O
dag	O
.	O
again	O
,	O
we	O
can	O
compute	O
the	O
marginal	B
likelihood	I
in	O
closed	O
form	O
.	O
see	O
(	O
bottcher	O
and	O
dethlefsen	O
2003	O
)	O
for	O
the	O
details	O
.	O
in	O
the	O
general	O
case	O
(	O
i.e.	O
,	O
everything	O
except	O
gaussians	O
and	O
cpts	O
)	O
,	O
we	O
need	O
to	O
approximate	O
the	O
marginal	B
likelihood	I
.	O
the	O
simplest	O
approach	O
is	O
to	O
use	O
the	O
bic	O
approximation	O
,	O
which	O
has	O
the	O
form	O
log	O
p	O
(	O
dt|ˆθt	O
)	O
−	O
ktct	O
2	O
log	O
n	O
(	O
26.31	O
)	O
(	O
cid:4	O
)	O
t	O
26.4.3	O
scaling	O
up	O
to	O
larger	O
graphs	O
the	O
main	O
challenge	O
in	O
computing	O
the	O
posterior	O
over	O
dags	O
is	O
that	O
there	O
are	O
so	O
many	O
possible	O
graphs	O
.	O
more	O
precisely	O
,	O
(	O
robinson	O
1973	O
)	O
showed	O
that	O
the	O
number	O
of	O
dags	O
on	O
d	O
nodes	B
satisﬁes	O
the	O
following	O
recurrence	O
:	O
(	O
−1	O
)	O
i+1	O
2i	O
(	O
d−i	O
)	O
f	O
(	O
d	O
−	O
i	O
)	O
f	O
(	O
d	O
)	O
=	O
(	O
26.32	O
)	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
d	O
i	O
d	O
(	O
cid:4	O
)	O
i=1	O
for	O
d	O
>	O
2.	O
the	O
base	O
case	O
is	O
f	O
(	O
1	O
)	O
=	O
1.	O
solving	O
this	O
recurrence	O
yields	O
the	O
following	O
sequence	O
:	O
1	O
,	O
3	O
,	O
25	O
,	O
543	O
,	O
29281	O
,	O
3781503	O
,	O
etc.2	O
in	O
view	O
of	O
the	O
enormous	O
size	O
of	O
the	O
hypothesis	B
space	I
,	O
we	O
are	O
generally	O
forced	O
to	O
use	O
approximate	O
methods	O
,	O
some	O
of	O
which	O
we	O
review	O
below	O
.	O
26.4.3.1	O
approximating	O
the	O
mode	B
of	O
the	O
posterior	O
we	O
can	O
use	O
dynamic	B
programming	I
to	O
ﬁnd	O
the	O
globally	O
optimal	O
map	O
dag	O
(	O
up	O
to	O
markov	O
equiv-	O
alence	O
)	O
(	O
koivisto	O
and	O
sood	O
2004	O
;	O
silander	O
and	O
myllmaki	O
2006	O
)	O
.	O
unfortunately	O
this	O
method	O
takes	O
v	O
2v	O
time	O
and	O
space	O
,	O
making	O
it	O
intractable	O
beyond	O
about	O
16	O
nodes	B
.	O
indeed	O
,	O
the	O
general	O
problem	O
of	O
ﬁnding	O
the	O
globally	O
optimal	O
map	O
dag	O
is	O
provably	O
np-complete	O
(	O
chickering	O
1996	O
)	O
,	O
consequently	O
,	O
we	O
must	O
settle	O
for	O
ﬁnding	O
a	O
locally	O
optimal	O
map	O
dag	O
.	O
the	O
most	O
common	O
method	O
is	O
greedy	O
hill	O
climbing	O
:	O
at	O
each	O
step	O
,	O
the	O
algorithm	O
proposes	O
small	O
changes	O
to	O
the	O
current	O
graph	B
,	O
such	O
as	O
adding	O
,	O
deleting	O
or	O
reversing	O
a	O
single	O
edge	O
;	O
it	O
then	O
moves	O
to	O
the	O
neigh-	O
boring	O
graph	B
which	O
most	O
increases	O
the	O
posterior	O
.	O
the	O
method	O
stops	O
when	O
it	O
reaches	O
a	O
lo-	O
cal	O
maximum	O
.	O
it	O
is	O
important	O
that	O
the	O
method	O
only	O
proposes	O
local	O
changes	O
to	O
the	O
graph	B
,	O
1.	O
we	O
can	O
make	O
this	O
method	O
more	O
efficient	O
by	O
using	O
(	O
cid:7	O
)	O
1-regularization	O
to	O
select	O
the	O
parents	B
(	O
schmidt	O
et	O
al	O
.	O
2007	O
)	O
.	O
in	O
this	O
case	O
,	O
we	O
need	O
to	O
approximate	O
the	O
marginal	O
likelhood	O
as	O
we	O
discuss	O
below	O
.	O
2.	O
a	O
longer	O
list	O
of	O
values	O
can	O
be	O
found	O
at	O
http	O
:	O
//www.research.att.com/~njas/sequences/a003024	O
.	O
interest-	O
ingly	O
,	O
the	O
number	O
of	O
dags	O
is	O
equal	O
to	O
the	O
number	O
of	O
(	O
0,1	O
)	O
matrices	O
all	O
of	O
whose	O
eigenvalues	O
are	O
positive	O
real	O
numbers	O
(	O
mckay	O
et	O
al	O
.	O
2004	O
)	O
.	O
26.4.	O
learning	B
dag	O
structures	O
921	O
evidence	B
case	O
course	O
question	O
msg	O
fact	O
god	O
gun	O
christian	O
nasa	O
shuttle	O
drive	O
scsi	O
disk	O
government	O
religion	O
jesus	O
car	O
disease	O
mission	O
space	O
law	O
jews	O
engine	O
patients	O
orbit	O
games	O
program	O
rights	O
power	O
bible	O
honda	O
computer	O
bmw	O
medicine	O
earth	O
solar	O
season	O
launch	O
technology	O
dos	O
dealer	O
science	O
moon	O
system	O
team	O
satellite	O
files	O
problem	O
studies	O
mars	O
lunar	O
players	O
version	O
human	O
israel	O
war	O
president	O
hockey	O
hit	O
windows	O
university	O
nhl	O
puck	O
baseball	O
won	O
email	O
memory	O
ftp	O
state	B
research	O
league	O
fans	O
win	O
phone	B
format	O
video	O
mac	O
children	B
world	O
oil	O
cancer	O
number	O
image	O
data	O
driver	O
software	O
water	O
health	O
food	O
aids	O
insurance	O
doctor	O
help	O
vitamin	O
pc	O
card	O
server	O
graphics	O
display	O
figure	O
26.8	O
a	O
locally	O
optimal	O
dag	O
learned	O
from	O
the	O
20-newsgroup	O
data	O
.	O
from	O
figure	O
4.10	O
of	O
(	O
schmidt	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
mark	O
schmidt	O
.	O
since	O
this	O
enables	O
the	O
change	O
in	O
marginal	B
likelihood	I
(	O
and	O
hence	O
the	O
posterior	O
)	O
to	O
be	O
computed	O
in	O
constant	O
time	O
(	O
assuming	O
we	O
cache	O
the	O
sufficient	B
statistics	I
)	O
.	O
this	O
is	O
because	O
all	O
but	O
one	O
or	O
two	O
of	O
the	O
terms	O
in	O
equation	O
26.25	O
will	O
cancel	O
out	O
when	O
computing	O
the	O
log	O
bayes	O
factor	B
δ	O
(	O
g	O
→	O
g	O
(	O
cid:2	O
)	O
)	O
=	O
log	O
p	O
(	O
g	O
(	O
cid:2	O
)	O
|d	O
)	O
−	O
log	O
p	O
(	O
g|d	O
)	O
.	O
we	O
can	O
initialize	O
the	O
search	O
from	O
the	O
best	O
tree	B
,	O
which	O
can	O
be	O
found	O
using	O
exact	O
methods	O
discussed	O
in	O
section	O
26.3.	O
for	O
speed	O
,	O
we	O
can	O
restrict	O
the	O
search	O
so	O
it	O
only	O
adds	O
edges	B
which	O
are	O
part	O
of	O
the	O
markov	O
blankets	O
estimated	O
from	O
a	O
dependency	B
network	I
(	O
schmidt	O
2010	O
)	O
.	O
figure	O
26.8	O
gives	O
an	O
example	O
of	O
a	O
dag	O
learned	O
in	O
this	O
way	O
from	O
the	O
20-newsgroup	O
data	O
.	O
we	O
can	O
use	O
techniques	O
such	O
as	O
multiple	B
random	I
restarts	I
to	O
increase	O
the	O
chance	O
of	O
ﬁnding	O
a	O
good	O
local	O
maximum	O
.	O
we	O
can	O
also	O
use	O
more	O
sophisticated	O
local	O
search	O
methods	O
,	O
such	O
as	O
genetic	B
algorithms	I
or	O
simulated	B
annealing	I
,	O
for	O
structure	B
learning	I
.	O
26.4.3.2	O
approximating	O
other	O
functions	O
of	O
the	O
posterior	O
if	O
our	O
goal	O
is	O
knowledge	B
discovery	I
,	O
the	O
map	O
dag	O
can	O
be	O
misleading	O
,	O
for	O
reasons	O
we	O
discussed	O
in	O
section	O
5.2.1.	O
a	O
better	O
approach	O
is	O
to	O
compute	O
the	O
probability	O
that	O
each	O
edge	O
is	O
present	O
,	O
p	O
(	O
gst	O
=	O
1|d	O
)	O
,	O
of	O
the	O
probability	O
there	O
is	O
a	O
path	B
from	O
s	O
to	O
t.	O
we	O
can	O
do	O
this	O
exactly	O
using	O
dynamic	B
programming	I
(	O
koivisto	O
2006	O
;	O
parviainen	O
and	O
koivisto	O
2011	O
)	O
.	O
unfortunately	O
these	O
methods	O
take	O
v	O
2v	O
time	O
in	O
the	O
general	O
case	O
,	O
making	O
them	O
intractable	O
for	O
graphs	O
with	O
more	O
than	O
about	O
16	O
922	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
nodes	O
.	O
an	O
approximate	O
method	O
is	O
to	O
sample	O
dags	O
from	O
the	O
posterior	O
,	O
and	O
then	O
to	O
compute	O
the	O
fraction	O
of	O
times	O
there	O
is	O
an	O
s	O
→	O
t	O
edge	O
or	O
path	B
for	O
each	O
(	O
s	O
,	O
t	O
)	O
pair	O
.	O
the	O
standard	O
way	O
to	O
draw	O
samples	B
is	O
to	O
use	O
the	O
metropolis	O
hastings	O
algorithm	O
(	O
section	O
24.3	O
)	O
,	O
where	O
we	O
use	O
the	O
same	O
local	O
proposal	O
as	O
we	O
did	O
in	O
greedy	O
search	O
(	O
madigan	O
and	O
raftery	O
1994	O
)	O
.	O
a	O
faster-mixing	O
method	O
is	O
to	O
use	O
a	O
collapsed	O
mh	O
sampler	O
,	O
as	O
suggested	O
in	O
(	O
friedman	O
and	O
koller	O
2003	O
)	O
.	O
this	O
exploits	O
the	O
fact	O
that	O
,	O
if	O
a	O
total	B
ordering	I
of	O
the	O
nodes	B
is	O
known	O
,	O
we	O
can	O
select	O
the	O
parents	B
for	O
each	O
node	O
independently	O
,	O
without	O
worrying	O
about	O
cycles	O
,	O
as	O
discussed	O
in	O
section	O
26.4.2.6.	O
by	O
summing	O
over	O
all	O
possible	O
choice	O
of	O
parents	B
,	O
we	O
can	O
marginalize	O
out	O
this	O
part	O
of	O
the	O
problem	O
,	O
and	O
just	O
sample	O
total	O
orders	O
.	O
(	O
ellis	O
and	O
wong	O
2008	O
)	O
also	O
use	O
order-space	O
(	O
collapsed	O
)	O
mcmc	O
,	O
but	O
this	O
time	O
with	O
a	O
parallel	B
tempering	I
mcmc	O
algorithm	O
.	O
26.5	O
learning	B
dag	O
structure	O
with	O
latent	B
variables	O
sometimes	O
the	O
complete	B
data	I
assumption	I
does	O
not	O
hold	O
,	O
either	O
because	O
we	O
have	O
missing	B
data	I
,	O
and/	O
or	O
because	O
we	O
have	O
hidden	B
variables	I
.	O
in	O
this	O
case	O
,	O
the	O
marginal	B
likelihood	I
is	O
given	O
by	O
(	O
cid:12	O
)	O
(	O
cid:4	O
)	O
(	O
cid:12	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
d|g	O
)	O
=	O
p	O
(	O
d	O
,	O
h|θ	O
,	O
g	O
)	O
p	O
(	O
θ|g	O
)	O
dθ	O
=	O
p	O
(	O
d	O
,	O
h|θ	O
,	O
g	O
)	O
p	O
(	O
θ|g	O
)	O
dθ	O
(	O
26.33	O
)	O
h	O
h	O
where	O
h	O
represents	O
the	O
hidden	B
or	O
missing	B
data	I
.	O
in	O
general	O
this	O
is	O
intractable	O
to	O
compute	O
.	O
for	O
example	O
,	O
consider	O
a	O
mixture	B
model	I
,	O
where	O
in	O
this	O
case	O
,	O
there	O
are	O
kn	O
possible	O
completions	O
of	O
the	O
we	O
don	O
’	O
t	O
observe	O
the	O
cluster	O
label	O
.	O
data	O
(	O
assuming	O
we	O
have	O
k	O
clusters	B
)	O
;	O
we	O
can	O
evaluate	O
the	O
inner	O
integral	O
for	O
each	O
one	O
of	O
these	O
assignments	O
to	O
h	O
,	O
but	O
we	O
can	O
not	O
afford	O
to	O
evaluate	O
all	O
of	O
the	O
integrals	O
.	O
(	O
of	O
course	O
,	O
most	O
of	O
these	O
integrals	O
will	O
correspond	O
to	O
hypotheses	O
with	O
little	O
posterior	O
support	O
,	O
such	O
as	O
assigning	O
single	O
data	O
points	O
to	O
isolated	O
clusters	B
,	O
but	O
we	O
don	O
’	O
t	O
know	O
ahead	O
of	O
time	O
the	O
relative	O
weight	O
of	O
these	O
assignments	O
.	O
)	O
in	O
this	O
section	O
,	O
we	O
discuss	O
some	O
ways	O
for	O
learning	B
dag	O
structure	O
when	O
we	O
have	O
latent	B
variables	O
and/or	O
missing	B
data	I
.	O
26.5.1	O
approximating	O
the	O
marginal	B
likelihood	I
when	O
we	O
have	O
missing	B
data	I
the	O
simplest	O
approach	O
is	O
to	O
use	O
standard	O
structure	O
learning	B
methods	O
for	O
fully	O
visible	B
dags	O
,	O
but	O
to	O
approximate	O
the	O
marginal	B
likelihood	I
.	O
in	O
section	O
24.7	O
,	O
we	O
discussed	O
some	O
monte	O
carlo	O
methods	O
for	O
approximating	O
the	O
marginal	B
likelihood	I
.	O
however	O
,	O
these	O
are	O
usually	O
too	O
slow	O
to	O
use	O
inside	O
of	O
a	O
search	O
over	O
models	O
.	O
below	O
we	O
mention	O
some	O
faster	O
deterministic	O
approximations	O
.	O
26.5.1.1	O
bic	O
approximation	O
a	O
simple	O
approximation	O
is	O
to	O
use	O
the	O
bic	O
score	O
,	O
which	O
is	O
given	O
by	O
bic	O
(	O
g	O
)	O
(	O
cid:2	O
)	O
log	O
p	O
(	O
d|ˆθ	O
,	O
g	O
)	O
−	O
log	O
n	O
2	O
dim	O
(	O
g	O
)	O
(	O
26.34	O
)	O
where	O
dim	O
(	O
g	O
)	O
is	O
the	O
number	O
of	O
degrees	B
of	I
freedom	I
in	O
the	O
model	O
and	O
ˆθ	O
is	O
the	O
map	O
or	O
ml	O
estimate	O
.	O
however	O
,	O
the	O
bic	O
score	O
often	O
severely	O
underestimates	O
the	O
true	O
marginal	O
likelihood	B
(	O
chickering	O
and	O
heckerman	O
1997	O
)	O
,	O
resulting	O
in	O
it	O
selecting	O
overly	O
simple	O
models	O
.	O
26.5.	O
learning	B
dag	O
structure	O
with	O
latent	B
variables	O
923	O
26.5.1.2	O
cheeseman-stutz	O
approximation	O
we	O
now	O
present	O
a	O
better	O
method	O
known	O
as	O
the	O
cheeseman-stutz	O
approximation	O
(	O
cs	O
)	O
(	O
cheese-	O
man	O
and	O
stutz	O
1996	O
)	O
.	O
we	O
ﬁrst	O
compute	O
a	O
map	O
estimate	O
of	O
the	O
parameters	O
ˆθ	O
(	O
e.g.	O
,	O
using	O
em	O
)	O
.	O
denote	O
the	O
expected	B
sufficient	I
statistics	I
of	O
the	O
data	O
by	O
d	O
=	O
d	O
(	O
ˆθ	O
)	O
;	O
in	O
the	O
case	O
of	O
discrete	B
variables	O
,	O
we	O
just	O
“	O
ﬁll	O
in	O
”	O
the	O
hidden	B
variables	I
with	O
their	O
expectation	O
.	O
we	O
then	O
use	O
the	O
exact	O
marginal	B
likelihood	I
equation	O
on	O
this	O
ﬁlled-in	O
data	O
:	O
p	O
(	O
d|θ	O
,	O
g	O
)	O
p	O
(	O
θ|g	O
)	O
dθ	O
p	O
(	O
d|g	O
)	O
≈	O
p	O
(	O
d|g	O
)	O
=	O
(	O
cid:12	O
)	O
(	O
26.35	O
)	O
however	O
,	O
comparing	O
this	O
to	O
equation	O
26.33	O
,	O
we	O
can	O
see	O
that	O
the	O
value	O
will	O
be	O
exponentially	O
smaller	O
,	O
since	O
it	O
does	O
not	O
sum	O
over	O
all	O
values	O
of	O
h.	O
to	O
correct	O
for	O
this	O
,	O
we	O
ﬁrst	O
write	O
log	O
p	O
(	O
d|g	O
)	O
=	O
log	O
p	O
(	O
d|g	O
)	O
+	O
log	O
p	O
(	O
d|g	O
)	O
−	O
log	O
p	O
(	O
d|g	O
)	O
and	O
then	O
we	O
apply	O
a	O
bic	O
approximation	O
to	O
the	O
last	O
two	O
terms	O
:	O
(	O
cid:17	O
)	O
log	O
p	O
(	O
d|g	O
)	O
−	O
log	O
p	O
(	O
d|g	O
)	O
≈	O
(	O
cid:18	O
)	O
(	O
cid:18	O
)	O
dim	O
(	O
ˆθ	O
)	O
(	O
cid:17	O
)	O
log	O
p	O
(	O
d|ˆθ	O
,	O
g	O
)	O
−	O
n	O
2	O
−	O
log	O
p	O
(	O
d|ˆθ	O
,	O
g	O
)	O
−	O
n	O
2	O
dim	O
(	O
ˆθ	O
)	O
=	O
log	O
p	O
(	O
d|ˆθ	O
,	O
g	O
)	O
−	O
log	O
p	O
(	O
d|ˆθ	O
,	O
g	O
)	O
(	O
26.36	O
)	O
(	O
26.37	O
)	O
(	O
26.38	O
)	O
(	O
26.39	O
)	O
putting	O
it	O
altogether	O
we	O
get	O
log	O
p	O
(	O
d|g	O
)	O
≈	O
log	O
p	O
(	O
d|g	O
)	O
+	O
log	O
p	O
(	O
d|ˆθ	O
,	O
g	O
)	O
−	O
log	O
p	O
(	O
d|ˆθ	O
,	O
g	O
)	O
(	O
26.40	O
)	O
the	O
ﬁrst	O
term	O
p	O
(	O
d|g	O
)	O
can	O
be	O
computed	O
by	O
plugging	O
in	O
the	O
ﬁlled-in	O
data	O
into	O
the	O
exact	O
marginal	B
likelihood	I
.	O
the	O
second	O
term	O
p	O
(	O
d|ˆθ	O
,	O
g	O
)	O
,	O
which	O
involves	O
an	O
exponential	O
sum	O
(	O
thus	O
matching	O
the	O
“	O
dimensionality	O
”	O
of	O
the	O
left	O
hand	O
side	O
)	O
can	O
be	O
computed	O
using	O
an	O
inference	B
algorithm	O
.	O
the	O
ﬁnal	O
term	O
p	O
(	O
d|ˆθ	O
,	O
g	O
)	O
can	O
be	O
computed	O
by	O
plugging	O
in	O
the	O
ﬁlled-in	O
data	O
into	O
the	O
regular	B
likelihood	O
.	O
26.5.1.3	O
variational	O
bayes	O
em	O
an	O
even	O
more	O
accurate	O
approach	O
is	O
to	O
use	O
the	O
variational	O
bayes	O
em	O
algorithm	O
.	O
recall	B
from	O
section	O
21.6	O
that	O
the	O
key	O
idea	O
is	O
to	O
make	O
the	O
following	O
factorization	O
assumption	O
:	O
p	O
(	O
θ	O
,	O
z1	O
:	O
n|d	O
)	O
≈	O
q	O
(	O
θ	O
)	O
q	O
(	O
z	O
)	O
=	O
q	O
(	O
θ	O
)	O
q	O
(	O
zi	O
)	O
(	O
26.41	O
)	O
(	O
cid:20	O
)	O
i	O
where	O
zi	O
are	O
the	O
hidden	B
variables	I
in	O
case	O
i.	O
in	O
the	O
e	O
step	O
,	O
we	O
update	O
the	O
q	O
(	O
zi	O
)	O
,	O
and	O
in	O
the	O
m	O
step	O
,	O
we	O
update	O
q	O
(	O
θ	O
)	O
.	O
the	O
corresponding	O
variational	B
free	I
energy	I
provides	O
a	O
lower	O
bound	O
on	O
the	O
log	O
marginal	O
likelihood	B
.	O
in	O
(	O
beal	O
and	O
ghahramani	O
2006	O
)	O
,	O
it	O
is	O
shown	O
that	O
this	O
bound	O
is	O
a	O
much	O
better	O
approximation	O
to	O
the	O
true	O
log	O
marginal	B
likelihood	I
(	O
as	O
estimated	O
by	O
a	O
slow	O
annealed	B
importance	I
sampling	I
procedure	O
)	O
than	O
either	O
bic	O
or	O
cs	O
.	O
in	O
fact	O
,	O
one	O
can	O
prove	O
that	O
the	O
variational	O
bound	O
will	O
always	O
be	O
more	O
accurate	O
than	O
cs	O
(	O
which	O
in	O
turn	O
is	O
always	O
more	O
accurate	O
than	O
bic	O
)	O
.	O
924	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
pe	O
low	O
low	O
high	O
high	O
h	O
0	O
1	O
0	O
1	O
p	O
(	O
iq=high|pe	O
,	O
h	O
)	O
0.098	O
0.22	O
0.21	O
0.49	O
p	O
(	O
male	O
)	O
=	O
0.48	O
iq	O
ses	O
low	O
low	O
high	O
high	O
sex	O
p	O
(	O
pe=high|ses	O
,	O
sex	O
)	O
male	O
female	O
male	O
female	O
0.32	O
0.166	O
0.86	O
0.81	O
h	O
p	O
(	O
h=0	O
)	O
=	O
0.63	O
p	O
(	O
h=1	O
)	O
=	O
0.37	O
ses	O
sex	O
pe	O
cp	O
h	O
0	O
1	O
p	O
(	O
ses=high|h	O
)	O
0.088	O
0.51	O
ses	O
low	O
low	O
low	O
low	O
high	O
high	O
high	O
high	O
iq	O
low	O
low	O
high	O
high	O
low	O
low	O
high	O
high	O
pe	O
low	O
high	O
low	O
high	O
low	O
high	O
low	O
high	O
p	O
(	O
cp=yes|ses	O
,	O
iq	O
,	O
pe	O
)	O
0.011	O
0.170	O
0.124	O
0.53	O
0.093	O
0.39	O
0.24	O
0.84	O
figure	O
26.9	O
the	O
most	O
probable	O
dag	O
with	O
a	O
single	O
binary	O
hidden	B
variable	I
learned	O
from	O
the	O
sewell-shah	O
data	O
.	O
map	O
estimates	O
of	O
the	O
cpt	O
entries	O
are	O
shown	O
for	O
some	O
of	O
the	O
nodes	B
.	O
source	O
:	O
(	O
heckerman	O
et	O
al	O
.	O
1997	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
david	O
heckerman	O
.	O
26.5.1.4	O
example	O
:	O
college	O
plans	O
revisited	O
let	O
us	O
revisit	O
the	O
college	O
plans	O
dataset	O
from	O
section	O
26.4.2.5.	O
recall	B
that	O
if	O
we	O
ignore	O
the	O
possibility	O
of	O
hidden	B
variables	I
there	O
was	O
a	O
direct	O
link	O
from	O
socio	O
economic	O
status	O
to	O
iq	O
in	O
the	O
map	O
dag	O
.	O
heckerman	O
et	O
al	O
.	O
decided	O
to	O
see	O
what	O
would	O
happen	O
if	O
they	O
introduced	O
a	O
hidden	B
variable	I
h	O
,	O
which	O
they	O
made	O
a	O
parent	O
of	O
both	O
ses	O
and	O
iq	O
,	O
representing	O
a	O
hidden	B
common	O
cause	O
.	O
they	O
also	O
considered	O
a	O
variant	O
in	O
which	O
h	O
points	O
to	O
ses	O
,	O
iq	O
and	O
pe	O
.	O
for	O
both	O
such	O
cases	O
,	O
they	O
considered	O
dropping	O
none	O
,	O
one	O
,	O
or	O
both	O
of	O
the	O
ses-pe	O
and	O
pe-iq	O
edges	B
.	O
they	O
varied	O
the	O
number	O
of	O
states	O
for	O
the	O
hidden	B
node	O
from	O
2	O
to	O
6.	O
thus	O
they	O
computed	O
the	O
approximate	O
posterior	O
over	O
8	O
×	O
5	O
=	O
40	O
different	O
models	O
,	O
using	O
the	O
cs	O
approximation	O
.	O
the	O
most	O
probable	O
model	O
which	O
they	O
found	O
is	O
shown	O
in	O
figure	O
26.9.	O
this	O
is	O
2	O
·	O
1010	O
times	O
it	O
is	O
also	O
5	O
·	O
109	O
times	O
more	O
more	O
likely	O
than	O
the	O
best	O
model	O
containing	O
no	O
hidden	O
variable	O
.	O
likely	O
than	O
the	O
second	O
most	O
probable	O
model	O
with	O
a	O
hidden	B
variable	I
.	O
so	O
again	O
the	O
posterior	O
is	O
very	O
peaked	O
.	O
these	O
results	O
suggests	O
that	O
there	O
is	O
indeed	O
a	O
hidden	B
common	O
cause	O
underlying	O
both	O
the	O
socio-economic	O
status	O
of	O
the	O
parents	B
and	O
the	O
iq	O
of	O
the	O
children	B
.	O
by	O
examining	O
the	O
cpt	O
entries	O
,	O
we	O
see	O
that	O
both	O
ses	O
and	O
iq	O
are	O
more	O
likely	O
to	O
be	O
high	O
when	O
h	O
takes	O
on	O
the	O
value	O
1.	O
they	O
interpret	O
this	O
to	O
mean	B
that	O
the	O
hidden	B
variable	I
represents	O
“	O
parent	O
quality	O
”	O
(	O
possibly	O
a	O
genetic	O
factor	O
)	O
.	O
note	O
,	O
however	O
,	O
that	O
the	O
arc	O
between	O
h	O
and	O
ses	O
can	O
be	O
reversed	O
without	O
changing	O
the	O
v-	O
structures	O
in	O
the	O
graph	B
,	O
and	O
thus	O
without	O
affecting	O
the	O
likelihood	B
;	O
this	O
underscores	O
the	O
difficulty	O
in	O
interpreting	O
hidden	B
variables	I
.	O
interestingly	O
,	O
the	O
hidden	B
variable	I
model	O
has	O
the	O
same	O
conditional	B
independence	I
assumptions	O
amongst	O
the	O
visible	B
variables	I
as	O
the	O
most	O
probable	O
visible	B
variable	O
model	O
.	O
so	O
it	O
is	O
not	O
pos-	O
sible	O
to	O
distinguish	O
between	O
these	O
hypotheses	O
by	O
merely	O
looking	O
at	O
the	O
empirical	O
conditional	O
independencies	O
in	O
the	O
data	O
(	O
which	O
is	O
the	O
basis	O
of	O
the	O
constraint-based	B
approach	I
to	O
structure	B
learning	I
(	O
pearl	O
and	O
verma	O
1991	O
;	O
spirtes	O
et	O
al	O
.	O
2000	O
)	O
)	O
.	O
instead	O
,	O
by	O
adopting	O
a	O
bayesian	O
approach	O
,	O
which	O
takes	O
parsimony	O
into	O
account	O
(	O
and	O
not	O
just	O
conditional	B
independence	I
)	O
,	O
we	O
can	O
discover	O
26.5.	O
learning	B
dag	O
structure	O
with	O
latent	B
variables	O
925	O
the	O
possible	O
existence	O
of	O
hidden	B
factors	O
.	O
this	O
is	O
the	O
basis	O
of	O
much	O
of	O
scientiﬁc	O
and	O
everday	O
human	O
reasoning	O
(	O
see	O
e.g	O
.	O
(	O
griffiths	O
and	O
tenenbaum	O
2009	O
)	O
for	O
a	O
discussion	O
)	O
.	O
26.5.2	O
structural	O
em	O
one	O
way	O
to	O
perform	O
structural	O
inference	O
in	O
the	O
presence	O
of	O
missing	B
data	I
is	O
to	O
use	O
a	O
standard	O
search	O
procedure	O
(	O
deterministic	O
or	O
stochastic	O
)	O
,	O
and	O
to	O
use	O
the	O
methods	O
from	O
section	O
26.5.1	O
to	O
estimate	O
the	O
marginal	B
likelihood	I
.	O
however	O
,	O
this	O
approach	O
is	O
very	O
efficient	O
,	O
because	O
the	O
marginal	B
likelihood	I
does	O
not	O
decompose	O
when	O
we	O
have	O
missing	B
data	I
,	O
and	O
nor	O
do	O
its	O
approximations	O
.	O
for	O
example	O
,	O
if	O
we	O
use	O
the	O
cs	O
approximation	O
or	O
the	O
vbem	O
approximation	O
,	O
we	O
have	O
to	O
perform	O
inference	B
in	O
every	O
neighboring	O
model	O
,	O
just	O
to	O
evaluate	O
the	O
quality	O
of	O
a	O
single	O
move	O
!	O
(	O
friedman	O
1997b	O
;	O
thiesson	O
et	O
al	O
.	O
1998	O
)	O
presents	O
a	O
much	O
more	O
efficient	O
approach	O
called	O
the	O
structural	O
em	O
algorithm	O
.	O
the	O
basic	O
idea	O
is	O
this	O
:	O
instead	O
of	O
ﬁtting	O
each	O
candidate	O
neighboring	O
graph	B
and	O
then	O
ﬁlling	O
in	O
its	O
data	O
,	O
ﬁll	O
in	O
the	O
data	O
once	O
,	O
and	O
use	O
this	O
ﬁlled-in	O
data	O
to	O
evaluate	O
the	O
score	O
of	O
all	O
the	O
neighbors	B
.	O
although	O
this	O
might	O
be	O
a	O
bad	O
approximation	O
to	O
the	O
marginal	B
likelihood	I
,	O
it	O
can	O
be	O
a	O
good	O
enough	O
approximation	O
of	O
the	O
difference	O
in	O
marginal	O
likelihoods	O
between	O
different	O
models	O
,	O
which	O
is	O
all	O
we	O
need	O
in	O
order	O
to	O
pick	O
the	O
best	O
neighbor	O
.	O
more	O
precisely	O
,	O
deﬁne	O
d	O
(	O
g0	O
,	O
ˆθ0	O
)	O
to	O
be	O
the	O
data	O
ﬁlled	O
in	O
using	O
model	O
g0	O
with	O
map	O
parameters	O
ˆθ0	O
.	O
now	O
deﬁne	O
a	O
modiﬁed	O
bic	O
score	O
as	O
follows	O
:	O
scorebic	O
(	O
g	O
,	O
d	O
)	O
(	O
cid:2	O
)	O
log	O
p	O
(	O
d|ˆθ	O
,	O
g	O
)	O
−	O
log	O
n	O
2	O
dim	O
(	O
g	O
)	O
+	O
log	O
p	O
(	O
g	O
)	O
+	O
log	O
p	O
(	O
ˆθ|g	O
)	O
(	O
26.42	O
)	O
where	O
we	O
have	O
included	O
the	O
log	O
prior	O
for	O
the	O
graph	B
and	O
parameters	O
.	O
one	O
can	O
show	O
(	O
friedman	O
1997b	O
)	O
that	O
if	O
we	O
pick	O
a	O
graph	B
g	O
which	O
increases	O
the	O
bic	O
score	O
relative	O
to	O
g0	O
on	O
the	O
expected	O
data	O
,	O
it	O
will	O
also	O
increase	O
the	O
score	O
on	O
the	O
actual	O
data	O
,	O
i.e.	O
,	O
scorebic	O
(	O
g	O
,	O
d	O
(	O
g0	O
,	O
ˆθ0	O
)	O
)	O
−	O
scorebic	O
(	O
g0	O
,	O
d	O
(	O
g0	O
,	O
ˆθ0	O
)	O
≤	O
scorebic	O
(	O
g	O
,	O
d	O
)	O
−	O
scorebic	O
(	O
g0	O
,	O
d	O
)	O
(	O
26.43	O
)	O
to	O
convert	O
this	O
into	O
an	O
algorithm	O
,	O
we	O
proceed	O
as	O
follows	O
.	O
first	O
we	O
initialize	O
with	O
some	O
graph	B
g0	O
and	O
some	O
set	O
of	O
parameters	O
θ0	O
.	O
then	O
we	O
ﬁll-in	O
the	O
data	O
using	O
the	O
current	O
parameters	O
—	O
in	O
practice	O
,	O
this	O
means	O
when	O
we	O
ask	O
for	O
the	O
expected	O
counts	O
for	O
any	O
particular	O
family	B
,	O
we	O
perform	O
inference	B
using	O
our	O
current	O
model	O
.	O
(	O
if	O
we	O
know	O
which	O
counts	O
we	O
will	O
need	O
,	O
we	O
can	O
precompute	O
all	O
of	O
them	O
,	O
which	O
is	O
much	O
faster	O
.	O
)	O
we	O
then	O
evaluate	O
the	O
bic	O
score	O
of	O
all	O
of	O
our	O
neighbors	B
using	O
the	O
ﬁlled-in	O
data	O
,	O
and	O
we	O
pick	O
the	O
best	O
neighbor	O
.	O
we	O
then	O
reﬁt	O
the	O
model	O
parameters	O
,	O
ﬁll-in	O
the	O
data	O
again	O
,	O
and	O
repeat	O
.	O
for	O
increased	O
speed	O
,	O
we	O
may	O
choose	O
to	O
only	O
reﬁt	O
the	O
model	O
every	O
few	O
steps	O
,	O
since	O
small	O
changes	O
to	O
the	O
structure	O
hopefully	O
won	O
’	O
t	O
invalidate	O
the	O
parameter	B
estimates	O
and	O
the	O
ﬁlled-in	O
data	O
too	O
much	O
.	O
one	O
interesting	O
application	O
is	O
to	O
learn	O
a	O
phylogenetic	B
tree	I
structure	O
.	O
here	O
the	O
observed	O
leaves	O
are	O
the	O
dna	O
or	O
protein	O
sequences	O
of	O
currently	O
alive	O
species	O
,	O
and	O
the	O
goal	O
is	O
to	O
infer	O
the	O
topology	O
of	O
the	O
tree	B
and	O
the	O
values	O
of	O
the	O
missing	B
internal	O
nodes	B
.	O
there	O
are	O
many	O
classical	B
algorithms	O
for	O
this	O
task	O
(	O
see	O
e.g.	O
,	O
(	O
durbin	O
et	O
al	O
.	O
1998	O
)	O
)	O
,	O
but	O
one	O
that	O
uses	O
sem	O
is	O
discussed	O
in	O
(	O
friedman	O
et	O
al	O
.	O
2002	O
)	O
.	O
another	O
interesting	O
application	O
of	O
this	O
method	O
is	O
to	O
learn	O
sparse	B
mixture	O
models	O
(	O
barash	O
and	O
friedman	O
2002	O
)	O
.	O
the	O
idea	O
is	O
that	O
we	O
have	O
one	O
hidden	B
variable	I
c	O
specifying	O
the	O
cluster	O
,	O
and	O
we	O
have	O
to	O
choose	O
whether	O
to	O
add	O
edges	B
c	O
→	O
xt	O
for	O
each	O
possible	O
feature	O
xt	O
.	O
thus	O
some	O
features	B
(	O
see	O
also	O
(	O
law	O
et	O
al	O
.	O
2004	O
)	O
will	O
be	O
dependent	O
on	O
the	O
cluster	O
id	O
,	O
and	O
some	O
will	O
be	O
independent	O
.	O
926	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
(	O
cid:25	O
)	O
(	O
cid:33	O
)	O
(	O
cid:32	O
)	O
(	O
cid:25	O
)	O
(	O
cid:34	O
)	O
(	O
cid:25	O
)	O
(	O
cid:1	O
)	O
(	O
cid:2	O
)	O
(	O
cid:3	O
)	O
(	O
cid:4	O
)	O
(	O
cid:25	O
)	O
(	O
cid:33	O
)	O
(	O
cid:29	O
)	O
(	O
cid:25	O
)	O
(	O
cid:33	O
)	O
(	O
cid:25	O
)	O
(	O
cid:25	O
)	O
(	O
cid:33	O
)	O
(	O
cid:27	O
)	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
(	O
cid:31	O
)	O
(	O
cid:25	O
)	O
(	O
cid:32	O
)	O
(	O
cid:31	O
)	O
(	O
cid:13	O
)	O
(	O
cid:2	O
)	O
(	O
cid:14	O
)	O
(	O
cid:16	O
)	O
(	O
cid:15	O
)	O
(	O
cid:9	O
)	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
(	O
cid:10	O
)	O
(	O
cid:18	O
)	O
(	O
cid:25	O
)	O
(	O
cid:31	O
)	O
(	O
cid:27	O
)	O
(	O
cid:13	O
)	O
(	O
cid:6	O
)	O
(	O
cid:1	O
)	O
(	O
cid:7	O
)	O
(	O
cid:14	O
)	O
(	O
cid:13	O
)	O
(	O
cid:2	O
)	O
(	O
cid:11	O
)	O
(	O
cid:4	O
)	O
(	O
cid:17	O
)	O
(	O
cid:12	O
)	O
(	O
cid:1	O
)	O
(	O
cid:11	O
)	O
(	O
cid:10	O
)	O
(	O
cid:6	O
)	O
(	O
cid:4	O
)	O
(	O
cid:14	O
)	O
(	O
cid:17	O
)	O
(	O
cid:3	O
)	O
(	O
cid:2	O
)	O
(	O
cid:6	O
)	O
(	O
cid:4	O
)	O
(	O
cid:25	O
)	O
(	O
cid:31	O
)	O
(	O
cid:33	O
)	O
(	O
cid:25	O
)	O
(	O
cid:29	O
)	O
(	O
cid:27	O
)	O
(	O
cid:3	O
)	O
(	O
cid:15	O
)	O
(	O
cid:10	O
)	O
(	O
cid:14	O
)	O
(	O
cid:15	O
)	O
(	O
cid:12	O
)	O
(	O
cid:25	O
)	O
(	O
cid:31	O
)	O
(	O
cid:28	O
)	O
(	O
cid:9	O
)	O
(	O
cid:2	O
)	O
(	O
cid:11	O
)	O
(	O
cid:9	O
)	O
(	O
cid:15	O
)	O
(	O
cid:11	O
)	O
(	O
cid:25	O
)	O
(	O
cid:29	O
)	O
(	O
cid:29	O
)	O
(	O
cid:22	O
)	O
(	O
cid:1	O
)	O
(	O
cid:11	O
)	O
(	O
cid:4	O
)	O
(	O
cid:25	O
)	O
(	O
cid:34	O
)	O
(	O
cid:28	O
)	O
(	O
cid:9	O
)	O
(	O
cid:1	O
)	O
(	O
cid:14	O
)	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
(	O
cid:25	O
)	O
(	O
cid:33	O
)	O
(	O
cid:26	O
)	O
(	O
cid:25	O
)	O
(	O
cid:32	O
)	O
(	O
cid:27	O
)	O
(	O
cid:25	O
)	O
(	O
cid:31	O
)	O
(	O
cid:30	O
)	O
(	O
cid:25	O
)	O
(	O
cid:28	O
)	O
(	O
cid:29	O
)	O
(	O
cid:25	O
)	O
(	O
cid:28	O
)	O
(	O
cid:31	O
)	O
(	O
cid:12	O
)	O
(	O
cid:6	O
)	O
(	O
cid:7	O
)	O
(	O
cid:2	O
)	O
(	O
cid:21	O
)	O
(	O
cid:2	O
)	O
(	O
cid:15	O
)	O
(	O
cid:11	O
)	O
(	O
cid:25	O
)	O
(	O
cid:26	O
)	O
(	O
cid:31	O
)	O
(	O
cid:24	O
)	O
(	O
cid:17	O
)	O
(	O
cid:6	O
)	O
(	O
cid:4	O
)	O
(	O
cid:14	O
)	O
(	O
cid:2	O
)	O
(	O
cid:15	O
)	O
(	O
cid:11	O
)	O
(	O
cid:10	O
)	O
(	O
cid:1	O
)	O
(	O
cid:11	O
)	O
(	O
cid:10	O
)	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
(	O
cid:25	O
)	O
(	O
cid:28	O
)	O
(	O
cid:30	O
)	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
(	O
cid:27	O
)	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
(	O
cid:30	O
)	O
(	O
cid:10	O
)	O
(	O
cid:13	O
)	O
(	O
cid:12	O
)	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
(	O
cid:14	O
)	O
(	O
cid:2	O
)	O
(	O
cid:1	O
)	O
(	O
cid:11	O
)	O
(	O
cid:25	O
)	O
(	O
cid:26	O
)	O
(	O
cid:28	O
)	O
(	O
cid:10	O
)	O
(	O
cid:15	O
)	O
(	O
cid:17	O
)	O
(	O
cid:12	O
)	O
(	O
cid:4	O
)	O
(	O
cid:6	O
)	O
(	O
cid:8	O
)	O
(	O
cid:6	O
)	O
(	O
cid:3	O
)	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
(	O
cid:2	O
)	O
(	O
cid:11	O
)	O
(	O
cid:6	O
)	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
(	O
cid:25	O
)	O
(	O
cid:25	O
)	O
(	O
cid:26	O
)	O
(	O
cid:34	O
)	O
(	O
cid:25	O
)	O
(	O
cid:25	O
)	O
(	O
cid:29	O
)	O
(	O
cid:7	O
)	O
(	O
cid:6	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
(	O
cid:17	O
)	O
(	O
cid:6	O
)	O
(	O
cid:25	O
)	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
(	O
cid:5	O
)	O
(	O
cid:2	O
)	O
(	O
cid:5	O
)	O
(	O
cid:7	O
)	O
(	O
cid:6	O
)	O
(	O
cid:25	O
)	O
(	O
cid:26	O
)	O
(	O
cid:25	O
)	O
(	O
cid:3	O
)	O
(	O
cid:2	O
)	O
(	O
cid:4	O
)	O
(	O
cid:6	O
)	O
(	O
cid:1	O
)	O
(	O
cid:4	O
)	O
(	O
cid:6	O
)	O
(	O
cid:16	O
)	O
(	O
cid:1	O
)	O
(	O
cid:14	O
)	O
(	O
cid:2	O
)	O
(	O
cid:6	O
)	O
(	O
cid:11	O
)	O
(	O
cid:14	O
)	O
(	O
cid:4	O
)	O
(	O
cid:13	O
)	O
(	O
cid:15	O
)	O
(	O
cid:10	O
)	O
(	O
cid:18	O
)	O
(	O
cid:6	O
)	O
(	O
cid:19	O
)	O
(	O
cid:11	O
)	O
(	O
cid:13	O
)	O
(	O
cid:7	O
)	O
(	O
cid:21	O
)	O
(	O
cid:1	O
)	O
(	O
cid:8	O
)	O
(	O
cid:6	O
)	O
(	O
cid:4	O
)	O
(	O
cid:25	O
)	O
(	O
cid:26	O
)	O
(	O
cid:29	O
)	O
(	O
cid:5	O
)	O
(	O
cid:1	O
)	O
(	O
cid:4	O
)	O
(	O
cid:6	O
)	O
(	O
cid:5	O
)	O
(	O
cid:1	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:16	O
)	O
(	O
cid:7	O
)	O
(	O
cid:1	O
)	O
(	O
cid:19	O
)	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
(	O
cid:4	O
)	O
(	O
cid:21	O
)	O
(	O
cid:15	O
)	O
(	O
cid:3	O
)	O
(	O
cid:23	O
)	O
(	O
cid:6	O
)	O
(	O
cid:4	O
)	O
(	O
cid:17	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:6	O
)	O
(	O
cid:1	O
)	O
(	O
cid:4	O
)	O
(	O
cid:15	O
)	O
(	O
cid:11	O
)	O
(	O
cid:14	O
)	O
(	O
cid:6	O
)	O
(	O
cid:1	O
)	O
(	O
cid:8	O
)	O
figure	O
26.10	O
part	O
of	O
a	O
hierarchical	O
latent	O
tree	O
learned	O
from	O
the	O
20-newsgroup	O
data	O
.	O
from	O
figure	O
2	O
of	O
(	O
harmeling	O
and	O
williams	O
2011	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
stefan	O
harmeling	O
.	O
for	O
a	O
different	O
way	O
to	O
perform	O
this	O
task	O
,	O
using	O
regular	B
em	O
and	O
a	O
set	O
of	O
bits	B
,	O
one	O
per	O
feature	O
,	O
that	O
are	O
free	O
to	O
change	O
across	O
data	O
cases	O
.	O
)	O
26.5.3	O
discovering	O
hidden	B
variables	I
in	O
section	O
26.5.1.4	O
,	O
we	O
introduced	O
a	O
hidden	B
variable	I
“	O
by	O
hand	O
”	O
,	O
and	O
then	O
ﬁgured	O
out	O
the	O
local	O
topology	O
by	O
ﬁtting	O
a	O
series	O
of	O
different	O
models	O
and	O
computing	O
the	O
one	O
with	O
the	O
best	O
marginal	B
likelihood	I
.	O
how	O
can	O
we	O
automate	O
this	O
process	O
?	O
figure	O
11.1	O
provides	O
one	O
useful	O
intuition	O
:	O
if	O
there	O
is	O
a	O
hidden	B
variable	I
in	O
the	O
“	O
true	O
model	O
”	O
,	O
then	O
its	O
children	B
are	O
likely	O
to	O
be	O
densely	O
connected	O
.	O
this	O
suggest	O
the	O
following	O
heuristic	O
(	O
elidan	O
et	O
al	O
.	O
2000	O
)	O
:	O
perform	O
structure	B
learning	I
in	O
the	O
visible	B
domain	O
,	O
and	O
then	O
look	O
for	O
structural	B
signatures	I
,	O
such	O
as	O
sets	O
of	O
densely	O
connected	O
nodes	B
(	O
near-cliques	O
)	O
;	O
introduce	O
a	O
hidden	B
variable	I
and	O
connect	O
it	O
to	O
all	O
nodes	O
in	O
this	O
near-clique	O
;	O
and	O
then	O
let	O
structural	O
em	O
sort	O
out	O
the	O
details	O
.	O
unfortunately	O
,	O
this	O
technique	O
does	O
not	O
work	O
too	O
well	O
,	O
since	O
structure	B
learning	I
algorithms	O
are	O
biased	O
against	O
ﬁtting	O
models	O
with	O
densely	O
connected	O
cliques	B
.	O
another	O
useful	O
intuition	O
comes	O
from	O
clustering	B
.	O
in	O
a	O
ﬂat	O
mixture	O
model	O
,	O
also	O
called	O
a	O
latent	B
class	I
model	I
,	O
the	O
discrete	B
latent	O
variable	O
provides	O
a	O
compressed	O
representation	O
of	O
its	O
children	B
.	O
thus	O
we	O
want	O
to	O
create	O
hidden	B
variables	I
with	O
high	O
mutual	O
information	B
with	O
their	O
children	B
.	O
one	O
way	O
to	O
do	O
this	O
is	O
to	O
create	O
a	O
tree-structured	O
hierarchy	O
of	O
latent	B
variables	O
,	O
each	O
of	O
which	O
(	O
zhang	O
2004	O
)	O
calls	O
this	O
a	O
hierarchical	O
latent	O
class	O
only	O
has	O
to	O
explain	O
a	O
small	O
set	O
of	O
children	B
.	O
model	O
.	O
they	O
propose	B
a	O
greedy	O
local	O
search	O
algorithm	O
to	O
learn	O
such	O
structures	O
,	O
based	O
on	O
adding	O
or	O
deleting	O
hidden	B
nodes	I
,	O
adding	O
or	O
deleting	O
edges	B
,	O
etc	O
.	O
(	O
note	O
that	O
learning	B
the	O
optimal	O
latent	O
26.5.	O
learning	B
dag	O
structure	O
with	O
latent	B
variables	O
927	O
h3	O
h17	O
president	O
government	O
power	O
h4	O
children	B
war	O
h20	O
religion	O
h14	O
earth	O
lunar	O
orbit	O
satellite	O
solar	O
law	O
state	B
human	O
rights	O
world	O
israel	O
jews	O
h8	O
bible	O
god	O
gun	O
h2	O
christian	O
jesus	O
moon	O
mars	O
technology	O
mission	O
h1	O
space	O
launch	O
shuttle	O
nasa	O
health	O
case	O
course	O
evidence	B
fact	O
question	O
program	O
h9	O
food	O
aids	O
h21	O
insurance	O
msg	O
water	O
studies	O
h13	O
medicine	O
car	O
dealer	O
h15	O
cancer	O
disease	O
doctor	O
patients	O
vitamin	O
bmw	O
engine	O
honda	O
oil	O
h5	O
version	O
h25	O
h12	O
files	O
ftp	O
email	O
format	O
phone	B
windows	O
h18	O
h11	O
image	O
number	O
card	O
driver	O
h10	O
dos	O
h19	O
h26	O
h6	O
puck	O
season	O
team	O
h7	O
win	O
video	O
h16	O
disk	O
memory	O
h22	O
pc	O
software	O
display	O
server	O
games	O
baseball	O
league	O
players	O
fans	O
hockey	O
nhl	O
won	O
graphics	O
h23	O
system	O
data	O
scsi	O
drive	O
computer	O
h24	O
hit	O
problem	O
help	O
mac	O
science	O
university	O
research	O
figure	O
26.11	O
a	O
partially	O
latent	O
tree	B
learned	O
from	O
the	O
20-newsgroup	O
data	O
.	O
note	O
that	O
some	O
words	O
can	O
have	O
multiple	O
meanings	O
,	O
and	O
get	O
connected	O
to	O
different	O
latent	B
variables	O
,	O
representing	O
different	O
“	O
topics	O
”	O
.	O
for	O
example	O
,	O
the	O
word	O
“	O
win	O
”	O
can	O
refer	O
to	O
a	O
sports	O
context	O
(	O
represented	O
by	O
h5	O
)	O
or	O
the	O
microsoft	O
windows	O
context	O
(	O
represented	O
by	O
h25	O
)	O
.	O
from	O
figure	O
12	O
of	O
(	O
choi	O
et	O
al	O
.	O
2011	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
jin	O
choi	O
.	O
tree	B
is	O
np-hard	O
(	O
roch	O
2006	O
)	O
.	O
)	O
recently	O
(	O
harmeling	O
and	O
williams	O
2011	O
)	O
proposed	O
a	O
faster	O
greedy	O
algorithm	O
for	O
learning	B
such	O
models	O
based	O
on	O
agglomerative	B
hierarchical	I
clustering	I
.	O
rather	O
than	O
go	O
into	O
details	O
,	O
we	O
just	O
give	O
an	O
example	O
of	O
what	O
this	O
system	O
can	O
learn	O
.	O
figure	O
26.10	O
shows	O
part	O
of	O
a	O
latent	B
forest	O
learned	O
from	O
the	O
20-newsgroup	O
data	O
.	O
the	O
algorithm	O
imposes	O
the	O
constraint	O
that	O
each	O
latent	B
node	O
has	O
exactly	O
two	O
children	B
,	O
for	O
speed	O
reasons	O
.	O
nevertheless	O
,	O
we	O
see	O
interpretable	O
clusters	B
arising	O
.	O
for	O
example	O
,	O
figure	O
26.10	O
shows	O
separate	O
clusters	B
concerning	O
medicine	O
,	O
sports	O
and	O
religion	O
.	O
this	O
provides	O
an	O
alternative	O
to	O
lda	O
and	O
other	O
topic	B
models	O
(	O
section	O
4.2.2	O
)	O
,	O
with	O
the	O
added	O
advantage	O
that	O
inference	B
in	O
latent	B
trees	O
is	O
exact	O
and	O
takes	O
time	O
linear	O
in	O
the	O
number	O
of	O
nodes	B
.	O
an	O
alternative	O
approach	O
is	O
proposed	O
in	O
(	O
choi	O
et	O
al	O
.	O
2011	O
)	O
,	O
in	O
which	O
the	O
observed	O
data	O
is	O
not	O
constrained	O
to	O
be	O
at	O
the	O
leaves	B
.	O
this	O
method	O
starts	O
with	O
the	O
chow-liu	O
tree	B
on	O
the	O
observed	O
data	O
,	O
and	O
then	O
adds	O
hidden	B
variables	I
to	O
capture	O
higher-order	O
dependencies	O
between	O
internal	O
nodes	B
.	O
this	O
results	O
in	O
much	O
more	O
compact	O
models	O
,	O
as	O
shown	O
in	O
figure	O
26.11.	O
this	O
model	O
also	O
has	O
better	O
predictive	B
accuracy	O
than	O
other	O
approaches	O
,	O
such	O
as	O
mixture	B
models	O
,	O
or	O
trees	O
where	O
all	O
the	O
observed	O
data	O
is	O
forced	O
to	O
be	O
at	O
the	O
leaves	B
.	O
interestingly	O
,	O
one	O
can	O
show	O
that	O
this	O
method	O
can	O
recover	O
the	O
exact	O
latent	B
tree	O
structure	O
,	O
providing	O
the	O
data	O
is	O
generated	O
from	O
a	O
tree	B
.	O
see	O
928	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
figure	O
26.12	O
google	O
’	O
s	O
rephil	O
model	O
.	O
leaves	B
represent	O
presence	O
or	O
absence	O
of	O
words	O
.	O
internal	O
nodes	B
represent	O
clusters	B
of	O
co-occuring	O
words	O
,	O
or	O
“	O
concepts	O
”	O
.	O
all	O
nodes	O
are	O
binary	O
,	O
and	O
all	O
cpds	O
are	O
noisy-or	O
.	O
the	O
model	O
contains	O
12	O
million	O
word	O
nodes	B
,	O
1	O
million	O
latent	B
cluster	O
nodes	B
,	O
and	O
350	O
million	O
edges	B
.	O
used	O
with	O
kind	O
permission	O
of	O
brian	O
milch	O
.	O
(	O
choi	O
et	O
al	O
.	O
2011	O
)	O
for	O
details	O
.	O
note	O
,	O
however	O
,	O
that	O
this	O
approach	O
,	O
unlike	O
(	O
zhang	O
2004	O
;	O
harmeling	O
and	O
williams	O
2011	O
)	O
,	O
requires	O
that	O
the	O
cardinality	O
of	O
all	O
the	O
variables	O
,	O
hidden	B
and	O
observed	O
,	O
be	O
the	O
same	O
.	O
furthermore	O
,	O
if	O
the	O
observed	O
variables	O
are	O
gaussian	O
,	O
the	O
hidden	B
variables	I
must	O
be	O
gaussian	O
also	O
.	O
26.5.4	O
case	O
study	O
:	O
google	O
’	O
s	O
rephil	O
in	O
this	O
section	O
,	O
we	O
describe	O
a	O
huge	O
dgm	O
called	O
rephil	O
,	O
which	O
was	O
automatically	O
learned	O
from	O
data.3	O
the	O
model	O
is	O
widely	O
used	O
inside	O
google	O
for	O
various	O
purposes	O
,	O
including	O
their	O
famous	O
adsense	O
system.4	O
the	O
model	O
structure	O
is	O
shown	O
in	O
figure	O
26.12.	O
the	O
leaves	B
are	O
binary	O
nodes	O
,	O
and	O
represent	O
the	O
presence	O
or	O
absence	O
of	O
words	O
or	O
compounds	O
(	O
such	O
as	O
“	O
new	O
york	O
city	O
”	O
)	O
in	O
a	O
text	O
document	O
or	O
query	O
.	O
the	O
latent	B
variables	O
are	O
also	O
binary	O
,	O
and	O
represent	O
clusters	B
of	O
co-occuring	O
words	O
.	O
all	O
cpds	O
are	O
noisy-or	O
,	O
since	O
some	O
leaf	B
nodes	O
(	O
representing	O
words	O
)	O
can	O
have	O
many	O
parents	B
.	O
this	O
means	O
each	O
edge	O
can	O
be	O
augmented	O
with	O
a	O
hidden	B
variable	I
specifying	O
if	O
the	O
link	O
was	O
activated	O
or	O
not	O
;	O
if	O
the	O
link	O
is	O
not	O
active	O
,	O
then	O
the	O
parent	O
can	O
not	O
turn	O
the	O
child	O
on	O
.	O
(	O
a	O
very	O
similar	B
model	O
was	O
proposed	O
independently	O
in	O
(	O
singliar	O
and	O
hauskrecht	O
2006	O
)	O
.	O
)	O
parameter	B
learning	O
is	O
based	O
on	O
em	O
,	O
where	O
the	O
hidden	B
activation	O
status	O
of	O
each	O
edge	O
needs	O
to	O
be	O
inferred	O
(	O
meek	O
and	O
heckerman	O
1997	O
)	O
.	O
structure	B
learning	I
is	O
based	O
on	O
the	O
old	O
neuroscience	O
3.	O
the	O
original	O
system	O
,	O
called	O
“	O
phil	O
”	O
,	O
was	O
developed	O
by	O
georges	O
harik	O
and	O
noam	O
shazeer	O
,	O
.	O
it	O
has	O
been	O
published	O
as	O
us	O
patent	O
#	O
8024372	O
,	O
“	O
method	O
and	O
apparatus	O
for	O
learning	B
a	O
probabilistic	O
generative	O
model	O
for	O
text	O
”	O
,	O
ﬁled	O
in	O
2004.	O
rephil	O
is	O
a	O
more	O
probabilistically	O
sound	O
version	O
of	O
the	O
method	O
,	O
developed	O
by	O
uri	O
lerner	O
et	O
al	O
.	O
the	O
summary	O
below	O
is	O
based	O
on	O
notes	O
by	O
brian	O
milch	O
(	O
who	O
also	O
works	O
at	O
google	O
)	O
.	O
4.	O
adsense	O
is	O
google	O
’	O
s	O
system	O
for	O
matching	O
web	O
pages	O
with	O
content-appropriate	O
ads	O
in	O
an	O
automatic	O
way	O
,	O
by	O
extracting	O
semantic	O
keywords	O
from	O
web	O
pages	O
.	O
these	O
keywords	O
play	O
a	O
role	O
analogous	O
to	O
the	O
words	O
that	O
users	O
type	O
in	O
when	O
searching	O
;	O
this	O
latter	O
form	O
of	O
information	B
is	O
used	O
by	O
google	O
’	O
s	O
adwords	O
system	O
.	O
the	O
details	O
are	O
secret	O
,	O
but	O
(	O
levy	O
2011	O
)	O
gives	O
an	O
overview	O
.	O
26.5.	O
learning	B
dag	O
structure	O
with	O
latent	B
variables	O
929	O
idea	O
that	O
“	O
nodes	B
that	I
ﬁre	I
together	I
should	I
wire	I
together	I
”	O
.	O
to	O
implement	O
this	O
,	O
we	O
run	O
inference	B
and	O
check	O
for	O
cluster-word	O
and	O
cluster-cluster	O
pairs	O
that	O
frequently	O
turn	O
on	O
together	O
.	O
we	O
then	O
add	O
an	O
edge	O
from	O
parent	O
to	O
child	O
if	O
the	O
link	O
can	O
signiﬁcantly	O
increase	O
the	O
probability	O
of	O
the	O
child	O
.	O
links	O
that	O
are	O
not	O
activated	O
very	O
often	O
are	O
pruned	O
out	O
.	O
we	O
initialize	O
with	O
one	O
cluster	O
per	O
“	O
document	O
”	O
(	O
corresponding	O
to	O
a	O
set	O
of	O
semantically	O
related	O
phrases	O
)	O
.	O
we	O
then	O
merge	O
clusters	B
a	O
and	O
b	O
if	O
a	O
explains	O
b	O
’	O
s	O
top	O
words	O
and	O
vice	O
versa	O
.	O
we	O
can	O
also	O
discard	O
clusters	B
that	O
are	O
used	O
too	O
rarely	O
.	O
the	O
model	O
was	O
trained	O
on	O
about	O
100	O
billion	O
text	O
snippets	O
or	O
search	O
queries	O
;	O
this	O
takes	O
several	O
weeks	O
,	O
even	O
on	O
a	O
parallel	O
distributed	O
computing	O
architecture	O
.	O
the	O
resulting	O
model	O
contains	O
12	O
million	O
word	O
nodes	B
and	O
about	O
1	O
million	O
latent	B
cluster	O
nodes	B
.	O
there	O
are	O
about	O
350	O
million	O
links	O
in	O
the	O
model	O
,	O
including	O
many	O
cluster-cluster	O
dependencies	O
.	O
the	O
longest	O
path	B
in	O
the	O
graph	B
has	O
length	O
555	O
,	O
so	O
the	O
model	O
is	O
quite	O
deep	B
.	O
exact	O
inference	B
in	O
this	O
model	O
is	O
obviously	O
infeasible	O
.	O
however	O
note	O
that	O
most	O
leaves	B
will	O
be	O
off	O
,	O
since	O
most	O
words	O
do	O
not	O
occur	O
in	O
a	O
given	O
query	O
;	O
such	O
leaves	B
can	O
be	O
analytically	O
removed	O
,	O
as	O
shown	O
in	O
exercise	O
10.7.	O
we	O
an	O
also	O
prune	O
out	O
unlikely	O
hidden	B
nodes	I
by	O
following	O
the	O
strongest	O
links	O
from	O
the	O
words	O
that	O
are	O
on	O
up	O
to	O
their	O
parents	B
to	O
get	O
a	O
candidate	O
set	O
of	O
concepts	O
.	O
we	O
then	O
perform	O
iterative	B
conditional	I
modes	I
to	O
ﬁnd	O
a	O
good	O
set	O
of	O
local	O
maxima	O
.	O
at	O
each	O
step	O
of	O
icm	O
,	O
each	O
node	O
sets	O
its	O
value	O
to	O
its	O
most	O
probable	O
state	B
given	O
the	O
values	O
of	O
its	O
neighbors	B
in	O
its	O
markov	O
blanket	O
.	O
this	O
continues	O
until	O
it	O
reaches	O
a	O
local	O
maximum	O
.	O
we	O
can	O
repeat	O
this	O
process	O
a	O
few	O
times	O
from	O
random	O
starting	O
conﬁgurations	O
.	O
at	O
google	O
,	O
this	O
can	O
be	O
made	O
to	O
run	O
in	O
15	O
milliseconds	O
!	O
26.5.5	O
structural	B
equation	I
models	I
*	O
a	O
structural	B
equation	I
model	I
(	O
bollen	O
1989	O
)	O
is	O
a	O
special	O
kind	O
of	O
directed	B
mixed	I
graph	I
(	O
sec-	O
tion	O
19.4.4.1	O
)	O
,	O
possibly	O
cyclic	O
,	O
in	O
which	O
all	O
cpds	O
are	O
linear	O
gaussian	O
,	O
and	O
in	O
which	O
all	O
bidirected	O
edges	B
represent	O
correlated	O
gaussian	O
noise	O
.	O
such	O
models	O
are	O
also	O
called	O
path	B
diagrams	I
.	O
sems	O
are	O
widely	O
used	O
,	O
especially	O
in	O
economics	O
and	O
social	O
science	O
.	O
it	O
is	O
common	O
to	O
interpret	O
the	O
edge	O
directions	O
in	O
terms	O
of	O
causality	B
,	O
where	O
directed	B
cycles	O
are	O
interpreted	O
is	O
in	O
terms	O
of	O
feedback	B
loops	I
(	O
see	O
e.g.	O
,	O
(	O
pearl	O
2000	O
,	O
ch.5	O
)	O
)	O
.	O
however	O
,	O
the	O
model	O
is	O
really	O
just	O
a	O
way	O
of	O
specifying	O
a	O
joint	O
gaussian	O
,	O
as	O
we	O
show	O
below	O
.	O
there	O
is	O
nothing	O
inherently	O
“	O
causal	O
”	O
about	O
it	O
at	O
all	O
.	O
(	O
we	O
discuss	O
causality	B
in	O
section	O
26.6	O
.	O
)	O
we	O
can	O
deﬁne	O
an	O
sem	O
as	O
a	O
series	O
of	O
full	B
conditionals	O
as	O
follows	O
:	O
(	O
cid:4	O
)	O
j	O
(	O
cid:5	O
)	O
=i	O
xi	O
=	O
μi	O
+	O
wijxj	O
+	O
i	O
where	O
	O
∼	O
n	O
(	O
0	O
,	O
ψ	O
)	O
.	O
we	O
can	O
rewrite	O
the	O
model	O
in	O
matrix	O
form	O
as	O
follows	O
:	O
x	O
=	O
wx	O
+	O
μ	O
+	O
	O
⇒	O
x	O
=	O
(	O
i	O
−	O
w	O
)	O
−1	O
(	O
	O
+	O
μ	O
)	O
hence	O
the	O
joint	B
distribution	I
is	O
given	O
by	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
μ	O
,	O
σ	O
)	O
where	O
σ	O
=	O
(	O
i	O
−	O
w	O
)	O
−1ψ	O
(	O
i	O
−	O
w	O
)	O
−t	O
(	O
26.46	O
)	O
we	O
draw	O
an	O
arc	O
xi	O
←	O
xj	O
if	O
|wij|	O
>	O
0.	O
if	O
w	O
is	O
lower	O
triangular	O
then	O
the	O
graph	B
is	O
acyclic	O
.	O
if	O
,	O
in	O
addition	O
,	O
ψ	O
is	O
diagonal	B
,	O
then	O
the	O
model	O
is	O
equivalent	O
to	O
a	O
gaussian	O
dgm	O
,	O
as	O
discussed	O
in	O
section	O
10.2.5	O
;	O
such	O
models	O
are	O
called	O
recursive	B
.	O
if	O
ψ	O
is	O
not	O
diagonal	O
,	O
then	O
we	O
draw	O
a	O
bidirected	O
(	O
26.44	O
)	O
(	O
26.45	O
)	O
930	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
y1	O
z1	O
y2	O
z1	O
z3	O
y3	O
figure	O
26.13	O
a	O
cyclic	O
directed	O
mixed	O
graphical	O
model	O
(	O
non-recursive	O
sem	O
)	O
.	O
note	O
the	O
z1	O
→	O
z2	O
→	O
z3	O
→	O
z1	O
feedback	O
loop	O
.	O
arc	O
xi	O
↔	O
xj	O
for	O
each	O
non-zero	O
off-diagonal	O
term	O
.	O
such	O
edges	B
represent	O
correlation	O
,	O
possibly	O
due	O
to	O
a	O
hidden	B
common	O
cause	O
.	O
when	O
using	O
structural	B
equation	I
models	I
,	O
it	O
is	O
common	O
to	O
partition	O
the	O
variables	O
into	O
latent	B
variables	O
,	O
zt	O
,	O
and	O
observed	O
or	O
manifest	B
variables	O
yt	O
.	O
for	O
example	O
,	O
figure	O
26.13	O
illustrates	O
the	O
following	O
model	O
:	O
⎞	O
⎟⎟⎟⎟⎟⎟⎠	O
⎛	O
⎜⎜⎜⎜⎜⎜⎝	O
0	O
0	O
0	O
0	O
0	O
0	O
z1	O
z2	O
z3	O
y1	O
y2	O
y3	O
⎞	O
⎟⎟⎟⎟⎟⎟⎠	O
+	O
⎛	O
⎜⎜⎜⎜⎜⎜⎝	O
⎞	O
⎟⎟⎟⎟⎟⎟⎠	O
,	O
1	O
2	O
3	O
4	O
5	O
6	O
0	O
0	O
0	O
0	O
0	O
0	O
⎞	O
⎟⎟⎟⎟⎟⎟⎠	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
(	O
26.47	O
)	O
(	O
26.48	O
)	O
⎛	O
⎜⎜⎜⎜⎜⎜⎝	O
x1	O
x2	O
x3	O
x4	O
x5	O
x6	O
where	O
ψ	O
=	O
⎞	O
⎟⎟⎟⎟⎟⎟⎠	O
=	O
⎜⎜⎜⎜⎜⎜⎝	O
⎛	O
0	O
0	O
0	O
0	O
0	O
⎞	O
⎟⎟⎟⎟⎟⎟⎠	O
=	O
⎛	O
⎜⎜⎜⎜⎜⎜⎝	O
⎛	O
⎜⎜⎜⎜⎜⎜⎝	O
z1	O
z2	O
z3	O
y1	O
y2	O
y3	O
0	O
w21	O
0	O
w41	O
0	O
0	O
0	O
0	O
w32	O
0	O
w52	O
0	O
w13	O
0	O
0	O
0	O
0	O
w63	O
ψ11	O
0	O
ψ22	O
0	O
0	O
ψ33	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
ψ44	O
ψ45	O
ψ54	O
ψ55	O
0	O
0	O
ψ66	O
the	O
presence	O
of	O
a	O
feedback	O
loop	O
z1	O
→	O
z2	O
→	O
z3	O
is	O
evident	O
from	O
the	O
fact	O
that	O
w	O
is	O
not	O
lower	O
triangular	O
.	O
also	O
the	O
presence	O
of	O
confounding	O
between	O
y1	O
and	O
y2	O
is	O
evident	O
in	O
the	O
off-diagonal	O
terms	O
in	O
ψ.	O
often	O
we	O
assume	O
there	O
are	O
multiple	O
observations	O
for	O
each	O
latent	O
variable	O
.	O
to	O
ensure	O
identiﬁa-	O
bility	O
,	O
we	O
can	O
set	O
the	O
mean	B
of	O
the	O
latent	B
variables	O
zt	O
to	O
0	O
,	O
and	O
we	O
can	O
set	O
the	O
regression	B
weights	O
of	O
zt	O
→	O
yt	O
to	O
1.	O
this	O
essentially	O
deﬁnes	O
the	O
scale	O
of	O
each	O
latent	O
variable	O
.	O
(	O
in	O
addition	O
to	O
the	O
z	O
’	O
s	O
,	O
there	O
are	O
the	O
extra	O
hidden	B
variables	I
implied	O
by	O
the	O
presence	O
of	O
the	O
bidirected	O
edges	B
.	O
)	O
the	O
standard	O
practice	O
in	O
the	O
sem	O
community	O
,	O
as	O
exempliﬁed	O
by	O
the	O
popular	O
commercial	O
software	O
package	O
called	O
lisrel	O
(	O
available	O
from	O
http	O
:	O
//www.ssicentral.com/lisrel/	O
)	O
,	O
is	O
to	O
26.6.	O
learning	B
causal	O
dags	O
931	O
build	O
the	O
structure	O
by	O
hand	O
,	O
to	O
estimate	O
the	O
parameters	O
by	O
maximum	O
likelihood	O
,	O
and	O
then	O
to	O
test	O
if	O
any	O
of	O
the	O
regression	B
weights	O
are	O
signiﬁcantly	O
different	O
from	O
0	O
,	O
using	O
standard	O
frequentist	O
methods	O
.	O
however	O
,	O
one	O
can	O
also	O
use	O
bayesian	O
inference	B
for	O
the	O
parameters	O
(	O
see	O
e.g.	O
,	O
(	O
dunson	O
et	O
al	O
.	O
2005	O
)	O
)	O
.	O
structure	B
learning	I
in	O
sems	O
is	O
rare	O
,	O
but	O
since	O
recursive	B
sems	O
are	O
equivalent	O
to	O
gaussian	O
dags	O
,	O
many	O
of	O
the	O
techniques	O
we	O
have	O
been	O
discussing	O
in	O
this	O
section	O
can	O
be	O
applied	O
.	O
sems	O
are	O
closely	O
related	O
to	O
factor	B
analysis	I
(	O
fa	O
)	O
models	O
(	O
chapter	O
12	O
)	O
.	O
the	O
basic	O
difference	O
is	O
that	O
in	O
an	O
fa	O
model	O
,	O
the	O
latent	B
gaussian	O
has	O
a	O
low-rank	O
covariance	B
matrix	I
,	O
and	O
the	O
observed	O
noise	O
has	O
a	O
diagonal	O
covariance	O
(	O
hence	O
no	O
bidirected	O
edges	B
)	O
.	O
in	O
an	O
sem	O
,	O
the	O
covariance	B
of	O
the	O
latent	B
gaussian	O
has	O
a	O
sparse	B
cholesky	O
decomposition	O
(	O
at	O
least	O
if	O
w	O
is	O
acyclic	O
)	O
,	O
and	O
the	O
observed	O
noise	O
might	O
have	O
a	O
full	B
covariance	O
matrix	O
.	O
note	O
that	O
sems	O
can	O
be	O
extended	O
in	O
many	O
ways	O
.	O
for	O
example	O
,	O
we	O
can	O
add	O
covariates/	O
input	O
variables	O
(	O
possibly	O
noisily	O
observed	O
)	O
,	O
we	O
can	O
make	O
some	O
of	O
the	O
observations	O
be	O
discrete	B
(	O
e.g.	O
,	O
by	O
using	O
probit	B
links	O
)	O
,	O
and	O
so	O
on	O
.	O
26.6	O
learning	B
causal	O
dags	O
26.6.1	O
causal	O
models	O
are	O
models	O
which	O
can	O
predict	O
the	O
effects	O
of	O
interventions	B
to	O
,	O
or	O
manipulations	O
of	O
,	O
a	O
system	O
.	O
for	O
example	O
,	O
an	O
electronic	O
circuit	O
diagram	O
implicitly	O
provides	O
a	O
compact	O
encoding	O
of	O
what	O
will	O
happen	O
if	O
one	O
removes	O
any	O
given	O
component	O
,	O
or	O
cuts	O
any	O
wire	O
.	O
a	O
causal	O
medical	O
model	O
might	O
predict	O
that	O
if	O
i	O
continue	O
to	O
smoke	O
,	O
i	O
am	O
likely	O
to	O
get	O
lung	O
cancer	O
(	O
and	O
hence	O
if	O
i	O
cease	O
smoking	O
,	O
i	O
am	O
less	O
likely	O
to	O
get	O
lung	O
cancer	O
)	O
.	O
causal	O
claims	O
are	O
inherently	O
stronger	O
,	O
yet	O
more	O
useful	O
,	O
than	O
purely	O
associative	B
claims	O
,	O
such	O
as	O
“	O
people	O
who	O
smoke	O
often	O
have	O
lung	O
cancer	O
”	O
.	O
causal	O
models	O
are	O
often	O
represented	O
by	O
dags	O
(	O
pearl	O
2000	O
)	O
,	O
although	O
this	O
is	O
somewhat	O
contro-	O
versial	O
(	O
dawid	O
2010	O
)	O
.	O
we	O
explain	O
this	O
causal	O
interpretation	O
of	O
dags	O
below	O
.	O
we	O
then	O
show	O
how	O
to	O
use	O
a	O
dag	O
to	O
do	O
causal	O
reasoning	O
.	O
finally	O
,	O
we	O
brieﬂy	O
discuss	O
how	O
to	O
learn	O
the	O
structure	O
of	O
causal	O
dags	O
.	O
a	O
more	O
detailed	O
description	O
of	O
this	O
topic	B
can	O
be	O
found	O
in	O
(	O
pearl	O
2000	O
)	O
and	O
(	O
koller	O
and	O
friedman	O
2009	O
,	O
ch.21	O
)	O
.	O
causal	O
interpretation	O
of	O
dags	O
in	O
this	O
section	O
,	O
we	O
deﬁne	O
a	O
directed	B
edge	O
a	O
→	O
b	O
in	O
a	O
dag	O
to	O
mean	B
that	O
“	O
a	O
directly	O
causes	O
b	O
”	O
,	O
so	O
if	O
we	O
manipulate	O
a	O
,	O
then	O
b	O
will	O
change	O
.	O
this	O
is	O
known	O
as	O
the	O
causal	O
markov	O
assumption	O
.	O
(	O
of	O
course	O
,	O
we	O
have	O
not	O
deﬁned	O
the	O
word	O
“	O
causes	O
”	O
,	O
and	O
we	O
can	O
not	O
do	O
that	O
by	O
appealing	O
to	O
a	O
dag	O
,	O
lest	O
we	O
end	O
up	O
with	O
a	O
cyclic	O
deﬁnition	O
;	O
see	O
(	O
dawid	O
2010	O
)	O
for	O
further	O
disussion	O
of	O
this	O
point	O
.	O
)	O
we	O
will	O
also	O
assume	O
that	O
all	O
relevant	O
variables	O
are	O
included	O
in	O
the	O
model	O
,	O
i.e.	O
,	O
there	O
are	O
no	O
unknown	O
confounders	B
,	O
reﬂecting	O
hidden	O
common	O
causes	O
.	O
this	O
is	O
called	O
the	O
causal	B
sufficiency	I
assumption	O
.	O
(	O
if	O
there	O
are	O
known	O
to	O
be	O
confounders	B
,	O
they	O
should	O
be	O
added	O
to	O
the	O
model	O
,	O
although	O
one	O
can	O
sometimes	O
use	O
mixed	B
directed	I
graphs	I
(	O
section	O
26.5.5	O
)	O
as	O
a	O
way	O
to	O
avoid	O
having	O
to	O
model	O
confounders	O
explicitly	O
.	O
)	O
assuming	O
we	O
are	O
willing	O
to	O
make	O
the	O
causal	O
markov	O
and	O
causal	B
sufficiency	I
assumptions	O
,	O
we	O
can	O
use	O
dags	O
to	O
answer	O
causal	O
questions	O
.	O
the	O
key	O
abstraction	O
is	O
that	O
of	O
a	O
perfect	B
intervention	I
;	O
this	O
represents	O
the	O
act	O
of	O
setting	O
a	O
variable	O
to	O
some	O
known	O
value	O
,	O
say	O
setting	O
xi	O
to	O
xi	O
.	O
a	O
real	O
world	O
example	O
of	O
such	O
a	O
perfect	B
intervention	I
is	O
a	O
gene	B
knockout	I
experiment	I
,	O
in	O
which	O
a	O
gene	O
is	O
“	O
silenced	O
”	O
.	O
we	O
need	O
some	O
notational	O
convention	O
to	O
distinguish	O
this	O
from	O
observing	O
that	O
xi	O
932	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
g	O
gdo	O
(	O
x=x	O
)	O
x	O
x	O
figure	O
26.14	O
surgical	O
intervention	O
on	O
x.	O
based	O
on	O
(	O
pe	O
’	O
er	O
2005	O
)	O
.	O
happens	O
to	O
have	O
value	O
xi	O
.	O
we	O
use	O
pearl	O
’	O
s	O
do	B
calculus	I
notation	O
(	O
as	O
in	O
the	O
verb	O
“	O
to	O
do	O
”	O
)	O
and	O
write	O
do	O
(	O
xi	O
=	O
xi	O
)	O
to	O
denote	O
the	O
event	O
that	O
we	O
set	O
xi	O
to	O
xi	O
.	O
a	O
causal	O
model	O
can	O
be	O
used	O
to	O
make	O
inferences	O
of	O
the	O
form	O
p	O
(	O
x|do	O
(	O
xi	O
=	O
xi	O
)	O
)	O
,	O
which	O
is	O
different	O
from	O
making	O
inferences	O
of	O
the	O
form	O
p	O
(	O
x|xi	O
=	O
xi	O
)	O
.	O
to	O
understand	O
the	O
difference	O
between	O
conditioning	B
on	O
interventions	B
and	O
conditioning	B
on	O
observations	O
(	O
i.e.	O
,	O
the	O
difference	O
between	O
doing	O
and	O
seeing	O
)	O
,	O
consider	O
a	O
2	O
node	O
dgm	O
s	O
→	O
y	O
,	O
in	O
which	O
s	O
=	O
1	O
if	O
you	O
smoke	O
and	O
s	O
=	O
0	O
otherwise	O
,	O
and	O
y	O
=	O
1	O
if	O
you	O
have	O
yellow-stained	O
ﬁngers	O
,	O
and	O
y	O
=	O
0	O
otherwise	O
.	O
if	O
i	O
observe	O
you	O
have	O
yellow	O
ﬁngers	O
,	O
i	O
am	O
licensed	O
to	O
infer	O
that	O
you	O
are	O
probably	O
a	O
smoker	O
(	O
since	O
nicotine	O
causes	O
yellow	O
stains	O
)	O
:	O
p	O
(	O
s	O
=	O
1|y	O
=	O
1	O
)	O
>	O
p	O
(	O
s	O
=	O
1	O
)	O
(	O
26.49	O
)	O
however	O
,	O
if	O
i	O
intervene	O
and	O
paint	O
your	O
ﬁngers	O
yellow	O
,	O
i	O
am	O
no	O
longer	O
licensed	O
to	O
infer	O
this	O
,	O
since	O
i	O
have	O
disrupted	O
the	O
normal	B
causal	O
mechanism	O
.	O
thus	O
p	O
(	O
s	O
=	O
1|do	O
(	O
y	O
=	O
1	O
)	O
)	O
=p	O
(	O
s	O
=	O
1	O
)	O
one	O
way	O
to	O
model	O
perfect	O
interventions	B
is	O
to	O
use	O
graph	B
surgery	I
:	O
represent	O
the	O
joint	O
distri-	O
bution	O
by	O
a	O
dgm	O
,	O
and	O
then	O
cut	O
the	O
arcs	O
coming	O
into	O
any	O
nodes	O
that	O
were	O
set	O
by	O
intervention	O
.	O
see	O
figure	O
26.14	O
for	O
an	O
example	O
.	O
this	O
prevents	O
any	O
information	B
ﬂow	O
from	O
the	O
nodes	O
that	O
were	O
intervened	O
on	O
from	O
being	O
sent	O
back	O
up	O
to	O
their	O
parents	B
.	O
having	O
perform	O
this	O
surgery	O
,	O
we	O
can	O
then	O
perform	O
probabilistic	B
inference	I
in	O
the	O
resulting	O
“	O
mutilated	O
”	O
graph	B
in	O
the	O
usual	O
way	O
to	O
reason	O
about	O
the	O
effects	O
of	O
interventions	B
.	O
we	O
state	B
this	O
formally	O
as	O
follows	O
.	O
theorem	O
26.6.1	O
(	O
manipulation	O
theorem	O
(	O
pearl	O
2000	O
;	O
spirtes	O
et	O
al	O
.	O
2000	O
)	O
)	O
.	O
.	O
to	O
compute	O
p	O
(	O
xi|do	O
(	O
xj	O
)	O
)	O
for	O
sets	O
of	O
nodes	B
i	O
,	O
j	O
,	O
we	O
can	O
perform	O
surgical	O
intervention	O
on	O
the	O
xj	O
nodes	B
and	O
then	O
use	O
standard	O
probabilistic	O
inference	B
in	O
the	O
mutilated	O
graph	B
.	O
(	O
26.50	O
)	O
we	O
can	O
generalize	B
the	O
notion	O
of	O
a	O
perfect	B
intervention	I
by	O
adding	O
interventions	B
as	O
explicit	O
action	O
nodes	B
to	O
the	O
graph	B
.	O
the	O
result	O
is	O
like	O
an	O
inﬂuence	B
diagram	I
,	O
except	O
there	O
are	O
no	O
utility	O
nodes	B
(	O
lauritzen	O
2000	O
;	O
dawid	O
2002	O
)	O
.	O
this	O
has	O
been	O
called	O
the	O
augmented	O
dag	O
(	O
pearl	O
2000	O
)	O
.	O
we	O
26.6.	O
learning	B
causal	O
dags	O
933	O
y	O
x	O
figure	O
26.15	O
illustration	O
of	O
simpson	O
’	O
s	O
paradox	O
.	O
figure	O
generated	O
by	O
simpsonsparadoxgraph	O
.	O
can	O
then	O
deﬁne	O
the	O
cpd	O
p	O
(	O
xi|do	O
(	O
xi	O
)	O
)	O
to	O
be	O
anything	O
we	O
want	O
.	O
we	O
can	O
also	O
allow	O
an	O
action	B
to	O
affect	O
multiple	O
nodes	O
.	O
this	O
is	O
called	O
a	O
fat	B
hand	I
intervention	O
,	O
a	O
reference	O
to	O
someone	O
trying	O
to	O
change	O
a	O
single	O
component	O
of	O
some	O
system	O
(	O
e.g.	O
,	O
an	O
electronic	O
circuit	O
)	O
,	O
but	O
accidently	O
touching	O
multiple	O
components	O
and	O
thereby	O
causing	O
various	O
side	O
effects	O
(	O
see	O
(	O
eaton	O
and	O
murphy	O
2007	O
)	O
for	O
a	O
way	O
to	O
model	O
this	O
using	O
augmented	O
dags	O
)	O
.	O
26.6.2	O
using	O
causal	O
dags	O
to	O
resolve	O
simpson	O
’	O
s	O
paradox	O
in	O
this	O
section	O
,	O
we	O
assume	O
we	O
know	O
the	O
causal	O
dag	O
.	O
we	O
can	O
then	O
do	O
causal	O
reasoning	O
by	O
applying	O
d-separation	O
to	O
the	O
mutilated	O
graph	B
.	O
in	O
this	O
section	O
,	O
we	O
give	O
an	O
example	O
of	O
this	O
,	O
and	O
show	O
how	O
causal	O
reasoning	O
can	O
help	O
resolve	O
a	O
famous	O
paradox	O
,	O
known	O
as	O
simpon	O
’	O
s	O
paradox	O
.	O
simpson	O
’	O
s	O
paradox	O
says	O
that	O
any	O
statistical	O
relationship	O
between	O
two	O
variables	O
can	O
be	O
reversed	O
by	O
including	O
additional	O
factors	B
in	O
the	O
analysis	O
.	O
for	O
example	O
,	O
suppose	O
some	O
cause	O
c	O
(	O
say	O
,	O
taking	O
a	O
drug	O
)	O
makes	O
some	O
effect	O
e	O
(	O
say	O
getting	O
better	O
)	O
more	O
likely	O
p	O
(	O
e|c	O
)	O
>	O
p	O
(	O
e|¬c	O
)	O
and	O
yet	O
,	O
when	O
we	O
condition	O
on	O
the	O
gender	O
of	O
the	O
patient	O
,	O
we	O
ﬁnd	O
that	O
taking	O
the	O
drug	O
makes	O
the	O
effect	O
less	O
likely	O
in	O
both	O
females	O
(	O
f	O
)	O
and	O
males	O
(	O
¬f	O
)	O
:	O
p	O
(	O
e|c	O
,	O
f	O
)	O
<	O
p	O
(	O
e|¬c	O
,	O
f	O
)	O
p	O
(	O
e|c	O
,	O
¬f	O
)	O
<	O
p	O
(	O
e|¬c	O
,	O
¬f	O
)	O
this	O
seems	O
impossible	O
,	O
but	O
by	O
the	O
rules	B
of	O
probability	O
,	O
this	O
is	O
perfectly	O
possible	O
,	O
because	O
the	O
event	O
space	O
where	O
we	O
condition	O
on	O
(	O
¬c	O
,	O
f	O
)	O
or	O
(	O
¬c	O
,	O
¬f	O
)	O
can	O
be	O
completely	O
different	O
to	O
the	O
event	O
space	O
when	O
we	O
just	O
condition	O
on	O
¬c	O
.	O
the	O
table	O
of	O
numbers	O
below	O
shows	O
a	O
concrete	O
example	O
(	O
from	O
(	O
pearl	O
2000	O
,	O
p175	O
)	O
)	O
:	O
e	O
20	O
16	O
36	O
c	O
¬c	O
total	O
combined	O
¬e	O
20	O
24	O
44	O
total	O
40	O
40	O
80	O
rate	B
e	O
18	O
50	O
%	O
40	O
%	O
7	O
25	O
male	O
¬e	O
12	O
3	O
15	O
total	O
30	O
10	O
40	O
rate	B
e	O
2	O
60	O
%	O
70	O
%	O
9	O
11	O
¬e	O
8	O
21	O
29	O
female	O
total	O
10	O
30	O
40	O
rate	B
20	O
%	O
30	O
%	O
934	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
from	O
this	O
table	O
of	O
numbers	O
,	O
we	O
see	O
that	O
p	O
(	O
e|c	O
)	O
=	O
20/40	O
=	O
0.5	O
>	O
p	O
(	O
e|¬c	O
)	O
=	O
16/40	O
=	O
0.4	O
p	O
(	O
e|c	O
,	O
f	O
)	O
=	O
2/10	O
=	O
0.2	O
<	O
p	O
(	O
e|¬c	O
,	O
f	O
)	O
=	O
9/30	O
=	O
0.3	O
p	O
(	O
e|c	O
,	O
¬f	O
)	O
=	O
18/30	O
=	O
0.6	O
<	O
p	O
(	O
e|¬	O
,	O
¬f	O
)	O
=	O
7/10	O
=	O
0.7	O
(	O
26.51	O
)	O
(	O
26.52	O
)	O
(	O
26.53	O
)	O
a	O
visual	O
representation	O
of	O
the	O
paradox	O
is	O
given	O
in	O
in	O
figure	O
26.15.	O
the	O
line	O
which	O
goes	O
up	O
and	O
to	O
the	O
right	O
shows	O
that	O
the	O
effect	O
(	O
y-axis	O
)	O
increases	O
as	O
the	O
cause	O
(	O
x-axis	O
)	O
increases	O
.	O
however	O
,	O
the	O
dots	O
represent	O
the	O
data	O
for	O
females	O
,	O
and	O
the	O
crosses	O
represent	O
the	O
data	O
for	O
males	O
.	O
within	O
each	O
subgroup	O
,	O
we	O
see	O
that	O
the	O
effect	O
decreases	O
as	O
we	O
increase	O
the	O
cause	O
.	O
it	O
is	O
clear	O
that	O
the	O
effect	O
is	O
real	O
,	O
but	O
it	O
is	O
still	O
very	O
counter-intuitive	O
.	O
the	O
reason	O
the	O
paradox	O
arises	O
is	O
that	O
we	O
are	O
interpreting	O
the	O
statements	O
causally	O
,	O
but	O
we	O
are	O
not	O
using	O
proper	O
causal	O
reasoning	O
when	O
performing	O
our	O
calculations	O
.	O
the	O
statement	O
that	O
the	O
drug	O
c	O
causes	O
recovery	O
e	O
is	O
p	O
(	O
e|do	O
(	O
c	O
)	O
)	O
>	O
p	O
(	O
e|do	O
(	O
¬c	O
)	O
)	O
(	O
26.54	O
)	O
whereas	O
the	O
data	O
merely	O
tell	O
us	O
p	O
(	O
e|c	O
)	O
>	O
p	O
(	O
e|¬c	O
)	O
(	O
26.55	O
)	O
this	O
is	O
not	O
a	O
contradiction	O
.	O
observing	O
c	O
is	O
positive	O
evidence	O
for	O
e	O
,	O
since	O
more	O
males	O
than	O
females	O
take	O
the	O
drug	O
,	O
and	O
the	O
male	O
recovery	O
rate	B
is	O
higher	O
(	O
regardless	O
of	O
the	O
drug	O
)	O
.	O
thus	O
equation	O
26.55	O
does	O
not	O
imply	O
equation	O
26.54.	O
nevertheless	O
,	O
we	O
are	O
left	O
with	O
a	O
practical	O
question	O
:	O
should	O
we	O
use	O
the	O
drug	O
or	O
not	O
?	O
it	O
seems	O
like	O
if	O
we	O
don	O
’	O
t	O
know	O
the	O
patient	O
’	O
s	O
gender	O
,	O
we	O
should	O
use	O
the	O
drug	O
,	O
but	O
as	O
soon	O
as	O
we	O
discover	O
if	O
they	O
are	O
male	O
or	O
female	O
,	O
we	O
should	O
stop	O
using	O
it	O
.	O
obviously	O
this	O
conclusion	O
is	O
ridiculous	O
.	O
to	O
answer	O
the	O
question	O
,	O
we	O
need	O
to	O
make	O
our	O
assumptions	O
more	O
explicit	O
.	O
suppose	O
reality	O
can	O
be	O
modeled	O
by	O
the	O
causal	O
dag	O
in	O
figure	O
26.16	O
(	O
a	O
)	O
.	O
to	O
compute	O
the	O
causal	O
effect	O
of	O
c	O
on	O
e	O
,	O
we	O
need	O
to	O
adjust	B
for	I
(	O
i.e.	O
,	O
condition	O
on	O
)	O
the	O
confounding	B
variable	I
f	O
.	O
this	O
is	O
necessary	O
because	O
there	O
is	O
a	O
backdoor	B
path	I
from	O
c	O
to	O
e	O
via	O
f	O
,	O
so	O
we	O
need	O
to	O
check	O
the	O
c	O
→	O
e	O
relationship	O
for	O
each	O
value	O
of	O
f	O
separately	O
,	O
to	O
make	O
sure	O
the	O
relationship	O
between	O
c	O
and	O
e	O
is	O
not	O
affected	O
by	O
any	O
value	O
of	O
f	O
.	O
suppose	O
that	O
for	O
each	O
value	O
of	O
f	O
,	O
taking	O
the	O
drug	O
is	O
harmful	O
,	O
that	O
is	O
,	O
(	O
26.56	O
)	O
(	O
26.57	O
)	O
(	O
26.58	O
)	O
(	O
26.59	O
)	O
(	O
26.60	O
)	O
(	O
26.61	O
)	O
p	O
(	O
e|do	O
(	O
c	O
)	O
,	O
f	O
)	O
<	O
p	O
(	O
e|do	O
(	O
¬c	O
)	O
,	O
f	O
)	O
p	O
(	O
e|do	O
(	O
c	O
)	O
,	O
¬f	O
)	O
<	O
p	O
(	O
e|do	O
(	O
¬c	O
)	O
,	O
¬f	O
)	O
then	O
we	O
can	O
show	O
that	O
taking	O
the	O
drug	O
is	O
harmful	O
overall	O
:	O
p	O
(	O
e|do	O
(	O
c	O
)	O
)	O
<	O
p	O
(	O
e|do	O
(	O
¬c	O
)	O
)	O
the	O
proof	O
is	O
as	O
follows	O
(	O
pearl	O
2000	O
,	O
p181	O
)	O
.	O
first	O
,	O
from	O
our	O
assumptions	O
in	O
figure	O
26.16	O
(	O
a	O
)	O
,	O
we	O
see	O
that	O
drugs	O
have	O
no	O
effect	O
on	O
gender	O
p	O
(	O
f|do	O
(	O
c	O
)	O
)	O
=	O
p	O
(	O
f|do	O
(	O
¬c	O
)	O
)	O
=	O
p	O
(	O
f	O
)	O
now	O
using	O
the	O
law	O
of	O
total	O
probability	O
,	O
p	O
(	O
e|do	O
(	O
c	O
)	O
)	O
=	O
p	O
(	O
e|do	O
(	O
c	O
)	O
,	O
f	O
)	O
p	O
(	O
f|do	O
(	O
c	O
)	O
)	O
+	O
p	O
(	O
e|do	O
(	O
c	O
)	O
,	O
¬f	O
)	O
p	O
(	O
¬f|do	O
(	O
c	O
)	O
)	O
=	O
p	O
(	O
e|do	O
(	O
c	O
)	O
,	O
f	O
)	O
p	O
(	O
f	O
)	O
+p	O
(	O
e	O
|do	O
(	O
c	O
)	O
,	O
¬f	O
)	O
p	O
(	O
¬f	O
)	O
26.6.	O
learning	B
causal	O
dags	O
935	O
treatment	O
gender	O
treatment	O
blood	O
pressure	O
c	O
f	O
c	O
f	O
e	O
recovery	O
(	O
a	O
)	O
e	O
recovery	O
(	O
b	O
)	O
figure	O
26.16	O
two	O
different	O
models	O
uses	O
to	O
illustrate	O
simpson	O
’	O
s	O
paradox	O
.	O
(	O
a	O
)	O
f	O
is	O
gender	O
and	O
is	O
a	O
confounder	B
for	O
c	O
and	O
e.	O
(	O
b	O
)	O
f	O
is	O
blood	O
pressure	O
and	O
is	O
caused	O
by	O
c.	O
similarly	O
,	O
p	O
(	O
e|do	O
(	O
¬c	O
)	O
)	O
=	O
p	O
(	O
e|do	O
(	O
¬c	O
)	O
,	O
f	O
)	O
p	O
(	O
f	O
)	O
+p	O
(	O
e|do	O
(	O
¬c	O
)	O
,	O
¬f	O
)	O
p	O
(	O
¬f	O
)	O
(	O
26.62	O
)	O
since	O
every	O
term	O
in	O
equation	O
26.61	O
is	O
less	O
than	O
the	O
corresponding	O
term	O
in	O
equation	O
26.62	O
,	O
we	O
conclude	O
that	O
p	O
(	O
e|do	O
(	O
c	O
)	O
)	O
<	O
p	O
(	O
e|do	O
(	O
¬c	O
)	O
)	O
(	O
26.63	O
)	O
so	O
if	O
the	O
model	O
in	O
figure	O
26.16	O
(	O
a	O
)	O
is	O
correct	O
,	O
we	O
should	O
not	O
administer	O
the	O
drug	O
,	O
since	O
it	O
reduces	O
the	O
probability	O
of	O
the	O
effect	O
.	O
now	O
consider	O
a	O
different	O
version	O
of	O
this	O
example	O
.	O
suppose	O
we	O
keep	O
the	O
data	O
the	O
same	O
but	O
interpret	O
f	O
as	O
something	O
that	O
is	O
affected	O
by	O
c	O
,	O
such	O
as	O
blood	O
pressure	O
.	O
see	O
figure	O
26.16	O
(	O
b	O
)	O
.	O
in	O
this	O
case	O
,	O
we	O
can	O
no	O
longer	O
assume	O
p	O
(	O
f|do	O
(	O
c	O
)	O
)	O
=	O
p	O
(	O
f|do	O
(	O
¬c	O
)	O
)	O
=	O
p	O
(	O
f	O
)	O
(	O
26.64	O
)	O
and	O
the	O
above	O
proof	O
breaks	O
down	O
.	O
so	O
p	O
(	O
e|do	O
(	O
c	O
)	O
)	O
−	O
p	O
(	O
e|do	O
(	O
¬c	O
)	O
)	O
may	O
be	O
positive	O
or	O
negaitve	O
.	O
in	O
the	O
true	O
model	O
is	O
figure	O
26.16	O
(	O
b	O
)	O
,	O
then	O
we	O
should	O
not	O
condition	O
on	O
f	O
when	O
assessing	O
the	O
effect	O
of	O
c	O
on	O
e	O
,	O
since	O
there	O
is	O
no	O
backdoor	O
path	B
in	O
this	O
case	O
,	O
because	O
of	O
the	O
v-structure	B
at	O
f	O
.	O
that	O
is	O
,	O
conditioning	B
on	O
f	O
might	O
block	O
one	O
of	O
the	O
causal	O
pathways	O
.	O
in	O
other	O
words	O
,	O
by	O
comparing	O
patients	O
with	O
the	O
same	O
post-treatment	O
blood	O
pressure	O
(	O
value	O
of	O
f	O
)	O
,	O
we	O
may	O
mask	O
the	O
effect	O
of	O
one	O
of	O
the	O
two	O
pathways	O
by	O
which	O
the	O
drug	O
operates	O
to	O
bring	O
about	O
recovery	O
.	O
thus	O
we	O
see	O
that	O
different	O
causal	O
assumptions	O
lead	O
to	O
different	O
causal	O
conclusions	O
,	O
and	O
hence	O
different	O
courses	O
of	O
action	B
.	O
this	O
raises	O
the	O
question	O
on	O
whether	O
we	O
can	O
learn	O
the	O
causal	O
model	O
from	O
data	O
.	O
we	O
discuss	O
this	O
issue	O
below	O
.	O
26.6.3	O
learning	B
causal	O
dag	O
structures	O
in	O
this	O
section	O
,	O
we	O
discuss	O
some	O
ways	O
to	O
learn	O
causal	O
dag	O
structures	O
.	O
936	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
26.6.3.1	O
learning	B
from	O
observational	O
data	O
in	O
section	O
26.4	O
,	O
we	O
discussed	O
various	O
methods	O
for	O
learning	B
dag	O
structures	O
from	O
observational	O
data	O
.	O
it	O
is	O
natural	O
to	O
ask	O
whether	O
these	O
methods	O
can	O
recover	O
the	O
“	O
true	O
”	O
dag	O
structure	O
that	O
was	O
used	O
to	O
generate	O
the	O
data	O
.	O
clearly	O
,	O
even	O
if	O
we	O
have	O
inﬁnite	O
data	O
,	O
an	O
optimal	O
method	O
can	O
only	O
identify	O
the	O
dag	O
up	O
to	O
markov	O
equivalence	O
(	O
section	O
26.4.1	O
)	O
.	O
that	O
is	O
,	O
it	O
can	O
identify	O
the	O
pdag	O
(	O
partially	O
directed	O
acylic	O
graph	B
)	O
,	O
but	O
not	O
the	O
complete	B
dag	O
structure	O
,	O
because	O
all	O
dags	O
which	O
are	O
markov	O
equivalent	O
have	O
the	O
same	O
likelihood	B
.	O
there	O
are	O
several	O
algorithms	O
(	O
e.g.	O
,	O
the	O
greedy	B
equivalence	I
search	I
method	O
of	O
(	O
chickering	O
2002	O
)	O
)	O
that	O
are	O
consistent	B
estimators	I
of	O
pdag	O
structure	O
,	O
in	O
the	O
sense	O
that	O
they	O
identify	O
the	O
true	O
markov	O
equivalence	B
class	I
as	O
the	O
sample	O
size	O
goes	O
to	O
inﬁnity	O
,	O
assuming	O
we	O
observe	O
all	O
the	O
variables	O
.	O
however	O
,	O
we	O
also	O
have	O
to	O
assume	O
that	O
the	O
generating	O
distribution	O
p	O
is	O
faithful	B
to	O
the	O
generating	O
dag	O
g.	O
this	O
means	O
that	O
all	O
the	O
conditional	O
indepence	O
(	O
ci	O
)	O
properties	O
of	O
p	O
are	O
exactly	O
captured	O
by	O
the	O
graphical	O
structure	O
,	O
so	O
i	O
(	O
p	O
)	O
=	O
i	O
(	O
g	O
)	O
;	O
this	O
means	O
there	O
can	O
not	O
be	O
any	O
ci	O
properties	O
in	O
p	O
that	O
are	O
due	O
to	O
particular	O
settings	O
of	O
the	O
parameters	O
(	O
such	O
as	O
zeros	O
in	O
a	O
regression	B
matrix	O
)	O
that	O
are	O
not	O
graphically	O
explicit	O
.	O
for	O
this	O
reason	O
,	O
a	O
faithful	B
distribution	O
is	O
also	O
called	O
a	O
stable	B
distribution	O
.	O
suppose	O
the	O
assumptions	O
hold	O
and	O
we	O
learn	O
a	O
pdag	O
.	O
what	O
can	O
we	O
do	O
with	O
it	O
?	O
instead	O
of	O
recovering	O
the	O
full	B
graph	O
,	O
we	O
can	O
focus	O
on	O
the	O
causal	O
analog	O
of	O
edge	O
marginals	O
,	O
by	O
computing	O
the	O
magnitude	O
of	O
the	O
causal	O
effect	O
of	O
one	O
node	O
on	O
another	O
(	O
say	O
a	O
on	O
b	O
)	O
.	O
if	O
we	O
know	O
the	O
dag	O
,	O
we	O
can	O
do	O
this	O
using	O
techniques	O
described	O
in	O
(	O
pearl	O
2000	O
)	O
.	O
if	O
the	O
dag	O
is	O
unknown	B
,	O
we	O
can	O
compute	O
a	O
lower	O
bound	O
on	O
the	O
effect	O
as	O
follows	O
(	O
maathuis	O
et	O
al	O
.	O
2009	O
)	O
:	O
learn	O
an	O
equivalence	B
class	I
(	O
pdag	O
)	O
from	O
data	O
;	O
enumerate	O
all	O
the	O
dags	O
in	O
the	O
equivalence	B
class	I
;	O
apply	O
pearl	O
’	O
s	O
do-calculus	O
to	O
compute	O
the	O
magnitude	O
of	O
the	O
causal	O
effect	O
of	O
a	O
on	O
b	O
in	O
each	O
dag	O
;	O
ﬁnally	O
,	O
take	O
the	O
minimum	O
of	O
these	O
effects	O
as	O
the	O
lower	O
bound	O
.	O
it	O
is	O
usually	O
computationally	O
infeasible	O
to	O
compute	O
all	O
dags	O
in	O
the	O
equivalence	B
class	I
,	O
but	O
fortunately	O
one	O
only	O
needs	O
to	O
be	O
able	O
to	O
identify	O
the	O
local	O
neighborhood	O
of	O
a	O
and	O
b	O
,	O
which	O
can	O
be	O
esimated	O
more	O
efficiently	O
,	O
as	O
described	O
in	O
(	O
maathuis	O
et	O
al	O
.	O
2009	O
)	O
.	O
this	O
technique	O
is	O
called	O
ida	O
,	O
which	O
is	O
short	O
for	O
“	O
intervention-calculus	O
when	O
the	O
dag	O
is	O
absent	O
”	O
.	O
in	O
(	O
maathuis	O
et	O
al	O
.	O
2010	O
)	O
,	O
this	O
technique	O
was	O
applied	O
to	O
some	O
yeast	O
gene	O
expression	O
data	O
.	O
gene	O
knockout	O
data	O
was	O
used	O
to	O
estimate	O
the	O
“	O
ground	O
truth	O
”	O
effect	O
of	O
each	O
234	O
single-gene	O
deletions	O
on	O
the	O
remaining	O
5,361	O
genes	O
.	O
then	O
the	O
algorithm	O
was	O
applied	O
to	O
63	O
unperturbed	O
(	O
wild-type	O
)	O
samples	B
,	O
and	O
was	O
used	O
to	O
rank	O
order	O
the	O
likely	O
targets	O
of	O
each	O
of	O
the	O
234	O
genes	O
.	O
the	O
method	O
had	O
a	O
precision	B
of	O
66	O
%	O
when	O
the	O
recall	B
was	O
set	O
to	O
10	O
%	O
;	O
while	O
low	O
,	O
this	O
is	O
substantially	O
more	O
than	O
rival	O
variable-selection	O
methods	O
,	O
such	O
as	O
lasso	B
and	O
elastic	B
net	I
,	O
which	O
were	O
only	O
slightly	O
above	O
chance	O
.	O
26.6.3.2	O
learning	B
from	O
interventional	B
data	I
if	O
we	O
want	O
to	O
distinguish	O
between	O
dags	O
within	O
the	O
equivalence	B
class	I
,	O
we	O
need	O
to	O
use	O
interven-	O
tional	O
data	O
,	O
where	O
certain	O
variables	O
have	O
been	O
set	O
,	O
and	O
the	O
consequences	O
have	O
been	O
measured	O
.	O
an	O
example	O
of	O
this	O
is	O
the	O
dataset	O
in	O
figure	O
26.17	O
(	O
a	O
)	O
,	O
where	O
proteins	O
in	O
a	O
signalling	O
pathway	O
were	O
perturbed	O
,	O
and	O
their	O
phosphorylation	O
status	O
was	O
measured	O
using	O
a	O
technique	O
called	O
ﬂow	B
cytometry	I
(	O
sachs	O
et	O
al	O
.	O
2005	O
)	O
.	O
it	O
is	O
straightforward	O
to	O
modify	O
the	O
standard	O
bayesian	O
scoring	O
criteria	O
,	O
such	O
as	O
the	O
marginal	B
likelihood	I
or	O
bic	O
score	O
,	O
to	O
handle	O
learning	B
from	O
mixed	O
observational	O
and	O
experimental	O
data	O
:	O
we	O
26.6.	O
learning	B
causal	O
dags	O
937	O
psitect	O
akt	O
inh	O
pip3	O
plcy	O
pip2	O
g06967	O
present	O
missing	B
int	O
.	O
edge	O
(	O
a	O
)	O
pkc	O
u0126	O
pma	O
raf	O
f	O
akt	O
pka	O
mek12	O
erk	O
jnk	O
p38	O
b2camp	O
(	O
b	O
)	O
figure	O
26.17	O
(	O
a	O
)	O
a	O
design	B
matrix	I
consisting	O
of	O
5400	O
data	O
points	O
(	O
rows	O
)	O
measuring	O
the	O
status	O
(	O
using	O
ﬂow	B
cytometry	I
)	O
of	O
11	O
proteins	O
(	O
columns	O
)	O
under	O
different	O
experimental	O
conditions	O
.	O
the	O
data	O
has	O
been	O
discretized	O
low	O
(	O
black	O
)	O
,	O
medium	O
(	O
grey	O
)	O
and	O
high	O
(	O
white	O
)	O
.	O
some	O
proteins	O
were	O
explicitly	O
controlled	O
using	O
into	O
3	O
states	O
:	O
activating	O
or	O
inhibiting	O
chemicals	O
.	O
(	O
b	O
)	O
a	O
directed	B
graphical	I
model	I
representing	O
dependencies	O
between	O
various	O
proteins	O
(	O
blue	O
circles	O
)	O
and	O
various	O
experimental	O
interventions	B
(	O
pink	O
ovals	O
)	O
,	O
which	O
was	O
inferred	O
from	O
this	O
data	O
.	O
we	O
plot	O
all	O
edges	O
for	O
which	O
p	O
(	O
gst	O
=	O
1|d	O
)	O
>	O
0.5.	O
dotted	O
edges	B
are	O
believed	O
to	O
exist	O
in	O
nature	O
but	O
were	O
not	O
discovered	O
by	O
the	O
algorithm	O
(	O
1	O
false	B
negative	I
)	O
.	O
solid	O
edges	B
are	O
true	O
positives	O
.	O
the	O
light	O
colored	O
edges	B
represent	O
the	O
effects	O
of	O
intervention	O
.	O
source	O
:	O
figure	O
6d	O
of	O
(	O
eaton	O
and	O
murphy	O
2007	O
)	O
.	O
this	O
ﬁgure	O
can	O
be	O
reproduced	O
using	O
the	O
code	O
at	O
http	O
:	O
//www.cs.ubc.ca/~murphyk/software/bdagl/index.html	O
.	O
938	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
just	O
compute	O
the	O
sufficient	B
statistics	I
for	O
a	O
cpd	O
’	O
s	O
parameter	B
by	O
skipping	O
over	O
the	O
cases	O
where	O
that	O
node	O
was	O
set	O
by	O
intervention	O
(	O
cooper	O
and	O
yoo	O
1999	O
)	O
.	O
for	O
example	O
,	O
when	O
using	O
tabular	O
cpds	O
,	O
we	O
modify	O
the	O
counts	O
as	O
follows	O
:	O
(	O
cid:4	O
)	O
ntck	O
(	O
cid:2	O
)	O
i	O
(	O
xi	O
,	O
t	O
=	O
k	O
,	O
xi	O
,	O
pa	O
(	O
t	O
)	O
=	O
c	O
)	O
(	O
26.65	O
)	O
i	O
:	O
xit	O
not	O
set	O
the	O
justiﬁcation	O
for	O
this	O
is	O
that	O
in	O
cases	O
where	O
node	O
t	O
is	O
set	O
by	O
force	O
,	O
it	O
is	O
not	O
sampled	O
from	O
its	O
usual	O
mechanism	O
,	O
so	O
such	O
cases	O
should	O
be	O
ignored	O
when	O
inferring	O
the	O
parameter	B
θt	O
.	O
the	O
mod-	O
iﬁed	O
scoring	O
criterion	O
can	O
be	O
combined	O
with	O
any	O
of	O
the	O
standard	O
structure	O
learning	B
algorithms	O
.	O
(	O
he	O
and	O
geng	O
2009	O
)	O
discusses	O
some	O
methods	O
for	O
choosing	O
which	O
interventions	B
to	O
perform	O
,	O
so	O
as	O
to	O
reduce	O
the	O
posterior	O
uncertainty	O
as	O
quickly	O
as	O
possible	O
(	O
a	O
form	O
of	O
active	B
learning	I
)	O
.	O
the	O
preceeding	O
method	O
assumes	O
the	O
interventions	B
are	O
perfect	O
.	O
in	O
reality	O
,	O
experimenters	O
can	O
rarely	O
control	O
the	O
state	B
of	O
individual	O
molecules	O
.	O
instead	O
,	O
they	O
inject	O
various	O
stimulant	O
or	O
inhibitor	O
chemicals	O
which	O
are	O
designed	O
to	O
target	O
speciﬁc	O
molecules	O
,	O
but	O
which	O
may	O
have	O
side	O
effects	O
.	O
we	O
can	O
model	O
this	O
quite	O
simply	O
by	O
adding	O
the	O
intervention	O
nodes	B
to	O
the	O
dag	O
,	O
and	O
then	O
learning	B
a	O
larger	O
augmented	O
dag	O
structure	O
,	O
with	O
the	O
constraint	O
that	O
there	O
are	O
no	O
edges	O
between	O
the	O
intervention	O
nodes	B
,	O
and	O
no	O
edges	O
from	O
the	O
“	O
regular	B
”	O
nodes	B
back	O
to	O
the	O
intervention	O
nodes	B
.	O
figure	O
26.17	O
(	O
b	O
)	O
shows	O
the	O
augmented	O
dag	O
that	O
was	O
learned	O
from	O
the	O
interventional	O
ﬂow	O
in	O
particular	O
,	O
we	O
plot	O
the	O
median	B
graph	O
,	O
which	O
cytometry	O
data	O
depicted	O
in	O
figure	O
26.17	O
(	O
a	O
)	O
.	O
includes	O
all	O
edges	O
for	O
which	O
p	O
(	O
gij	O
=	O
1|d	O
)	O
>	O
0.5.	O
these	O
were	O
computed	O
using	O
the	O
exact	O
it	O
turns	O
out	O
that	O
,	O
in	O
this	O
example	O
,	O
the	O
median	B
model	I
has	O
exactly	O
algorithm	O
of	O
(	O
koivisto	O
2006	O
)	O
.	O
the	O
same	O
structure	O
as	O
the	O
optimal	O
map	O
model	O
,	O
argmaxg	O
p	O
(	O
g|d	O
)	O
,	O
which	O
was	O
computed	O
using	O
the	O
algorithm	O
of	O
(	O
koivisto	O
and	O
sood	O
2004	O
;	O
silander	O
and	O
myllmaki	O
2006	O
)	O
.	O
26.7	O
learning	B
undirected	O
gaussian	O
graphical	O
models	O
learning	O
the	O
structured	O
of	O
undirected	O
graphical	O
models	O
is	O
easier	O
than	O
learning	B
dag	O
structure	O
because	O
we	O
don	O
’	O
t	O
need	O
to	O
worry	O
about	O
acyclicity	O
.	O
on	O
the	O
other	O
hand	O
,	O
it	O
is	O
harder	O
than	O
learning	B
dag	O
structure	O
since	O
the	O
likelihood	B
does	O
not	O
decompose	O
(	O
see	O
section	O
19.5	O
)	O
.	O
this	O
precludes	O
the	O
kind	O
of	O
local	O
search	O
methods	O
(	O
both	O
greedy	O
search	O
and	O
mcmc	O
sampling	O
)	O
we	O
used	O
to	O
learn	O
dag	O
structures	O
,	O
because	O
the	O
cost	O
of	O
evaluating	O
each	O
neighboring	O
graph	B
is	O
too	O
high	O
,	O
since	O
we	O
have	O
to	O
reﬁt	O
each	O
model	O
from	O
scratch	O
(	O
there	O
is	O
no	O
way	O
to	O
incrementally	O
update	O
the	O
score	O
of	O
a	O
model	O
)	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
several	O
solutions	O
to	O
this	O
problem	O
,	O
in	O
the	O
context	O
of	O
gaussian	O
random	O
ﬁelds	O
or	O
undirected	B
gaussian	O
graphical	O
models	O
(	O
ggm	O
)	O
s.	O
we	O
consider	O
structure	B
learning	I
for	O
discrete	B
undirected	O
models	O
in	O
section	O
26.8	O
.	O
26.7.1	O
mle	O
for	O
a	O
ggm	O
before	O
discussing	O
structure	B
learning	I
,	O
we	O
need	O
to	O
discuss	O
parameter	B
estimation	O
.	O
the	O
task	O
of	O
computing	O
the	O
mle	O
for	O
a	O
(	O
non-decomposable	O
)	O
ggm	O
is	O
called	O
covariance	B
selection	I
(	O
dempster	O
1972	O
)	O
.	O
from	O
equation	O
4.19	O
,	O
the	O
log	O
likelihood	O
can	O
be	O
written	O
as	O
(	O
cid:2	O
)	O
(	O
ω	O
)	O
=	O
log	O
det	O
ω	O
−	O
tr	O
(	O
sω	O
)	O
(	O
26.66	O
)	O
26.7.	O
learning	B
undirected	O
gaussian	O
graphical	O
models	O
(	O
cid:7	O
)	O
n	O
i=1	O
(	O
xi	O
−	O
x	O
)	O
(	O
xi	O
−	O
x	O
)	O
t	O
is	O
the	O
empirical	O
(	O
for	O
notational	O
simplicity	O
,	O
we	O
assume	O
we	O
have	O
already	O
estimated	O
ˆμ	O
=	O
x	O
.	O
)	O
n	O
where	O
ω	O
=	O
σ−1	O
is	O
the	O
precision	B
matrix	I
,	O
and	O
s	O
=	O
1	O
covariance	B
matrix	I
.	O
one	O
can	O
show	O
that	O
the	O
gradient	O
of	O
this	O
is	O
given	O
by	O
939	O
∇	O
(	O
cid:2	O
)	O
(	O
ω	O
)	O
=ω	O
−1	O
−	O
s	O
(	O
26.67	O
)	O
however	O
,	O
we	O
have	O
to	O
enforce	O
the	O
constraints	O
that	O
ωst	O
=	O
0	O
if	O
gst	O
=	O
0	O
(	O
structural	B
zeros	I
)	O
,	O
and	O
that	O
ω	O
is	O
positive	B
deﬁnite	I
.	O
the	O
former	O
constraint	O
is	O
easy	O
to	O
enforce	O
,	O
but	O
the	O
latter	O
is	O
somewhat	O
challenging	O
(	O
albeit	O
still	O
a	O
convex	B
constraint	O
)	O
.	O
one	O
approach	O
is	O
to	O
add	O
a	O
penalty	O
term	O
to	O
the	O
objective	O
if	O
ω	O
leaves	B
the	O
positive	B
deﬁnite	I
cone	O
;	O
this	O
is	O
the	O
approach	O
used	O
in	O
ggmfitminfunc	O
(	O
see	O
also	O
(	O
dahl	O
et	O
al	O
.	O
2008	O
)	O
)	O
.	O
another	O
approach	O
is	O
to	O
use	O
a	O
coordinate	O
descent	O
method	O
,	O
described	O
in	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p633	O
)	O
,	O
and	O
implemented	O
in	O
ggmfithtf	O
.	O
yet	O
another	O
approach	O
is	O
to	O
use	O
iterative	B
proportional	I
ﬁtting	I
,	O
described	O
in	O
section	O
19.5.7.	O
however	O
,	O
ipf	O
requires	O
identifying	O
the	O
cliques	B
of	O
the	O
graph	B
,	O
which	O
is	O
np-hard	O
in	O
general	O
.	O
interestingly	O
,	O
one	O
can	O
show	O
that	O
the	O
mle	O
must	O
satisfy	O
the	O
following	O
property	O
:	O
σst	O
=	O
sst	O
if	O
gst	O
=	O
1	O
or	O
s	O
=	O
t	O
,	O
i.e.	O
,	O
the	O
covariance	B
of	O
a	O
pair	O
that	O
are	O
connected	O
by	O
an	O
edge	O
must	O
match	O
the	O
in	O
addition	O
,	O
we	O
have	O
ωst	O
=	O
0	O
if	O
gst	O
=	O
0	O
,	O
by	O
deﬁnition	O
of	O
a	O
ggm	O
,	O
i.e.	O
,	O
empirical	O
covariance	O
.	O
the	O
precision	B
of	O
a	O
pair	O
that	O
are	O
not	O
connected	O
must	O
be	O
0.	O
we	O
say	O
that	O
σ	O
is	O
a	O
positive	B
deﬁnite	I
matrix	O
completion	O
of	O
s	O
,	O
since	O
it	O
retains	O
as	O
many	O
of	O
the	O
entries	O
in	O
s	O
as	O
possible	O
,	O
corresponding	O
to	O
the	O
edges	B
in	O
the	O
graph	B
,	O
subject	O
to	O
the	O
required	O
sparsity	B
pattern	O
on	O
σ−1	O
,	O
corresponding	O
to	O
the	O
absent	O
edges	B
;	O
the	O
remaining	O
entries	O
in	O
σ	O
are	O
ﬁlled	O
in	O
so	O
as	O
to	O
maximize	O
the	O
likelihood	B
.	O
let	O
us	O
consider	O
a	O
worked	O
example	O
from	O
(	O
hastie	O
et	O
al	O
.	O
2009	O
,	O
p652	O
)	O
.	O
we	O
will	O
use	O
the	O
following	O
adjacency	B
matrix	I
,	O
representing	O
the	O
cyclic	O
structure	O
,	O
x1−	O
x2−	O
x3−	O
x4−	O
x1	O
,	O
and	O
the	O
following	O
empirical	O
covariance	O
matrix	O
:	O
4	O
6	O
3	O
10	O
⎞	O
⎟⎟⎠	O
⎛	O
⎜⎜⎝	O
0.12	O
−0.01	O
−0.01	O
−0.05	O
0	O
1	O
10	O
2	O
6	O
5	O
2	O
10	O
3	O
⎞	O
⎟⎟⎠	O
,	O
ω	O
=	O
(	O
26.68	O
)	O
⎞	O
⎟⎟⎠	O
(	O
26.69	O
)	O
−0.05	O
0	O
0.11	O
−0.02	O
−0.02	O
0.11	O
−0.03	O
−0.03	O
0.13	O
0	O
0	O
⎞	O
⎟⎟⎠	O
,	O
s	O
=	O
⎛	O
⎜⎜⎝10	O
1	O
5	O
4	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
⎛	O
⎜⎜⎝0	O
⎛	O
⎜⎜⎝10.00	O
1.00	O
1.31	O
4.00	O
g	O
=	O
σ	O
=	O
the	O
mle	O
is	O
given	O
by	O
1.00	O
10.00	O
2.00	O
0.87	O
1.31	O
2.00	O
10.00	O
3.00	O
4.00	O
0.87	O
3.00	O
10.00	O
(	O
see	O
ggmfitdemo	O
for	O
the	O
code	O
to	O
reproduce	O
these	O
numbers	O
.	O
)	O
the	O
constrained	O
elements	O
in	O
ω	O
,	O
and	O
the	O
free	O
elements	O
in	O
σ	O
,	O
both	O
of	O
which	O
correspond	O
to	O
absent	O
edges	B
,	O
have	O
been	O
highlighted	O
.	O
26.7.2	O
graphical	B
lasso	I
we	O
now	O
discuss	O
one	O
way	O
to	O
learn	O
a	O
sparse	B
grf	O
structure	O
,	O
which	O
exploits	O
the	O
fact	O
that	O
there	O
is	O
a	O
1:1	O
correspondence	B
between	O
zeros	O
in	O
the	O
precision	B
matrix	I
and	O
absent	O
edges	B
in	O
the	O
graph	B
.	O
this	O
suggests	O
that	O
we	O
can	O
learn	O
a	O
sparse	B
graph	O
structure	O
by	O
using	O
an	O
objective	O
that	O
encourages	O
zeros	O
in	O
the	O
precision	B
matrix	I
.	O
by	O
analogy	O
to	O
lasso	B
(	O
see	O
section	O
13.3	O
)	O
,	O
one	O
can	O
deﬁne	O
the	O
following	O
(	O
cid:2	O
)	O
1	O
penalized	O
nll	O
:	O
j	O
(	O
ω	O
)	O
=−	O
log	O
det	O
ω	O
+	O
tr	O
(	O
sω	O
)	O
+λ||ω	O
||1	O
(	O
26.70	O
)	O
940	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
lambda=36.00	O
,	O
nedges=8	O
lambda=27.00	O
,	O
nedges=11	O
(	O
a	O
)	O
(	O
b	O
)	O
lambda=7.00	O
,	O
nedges=18	O
lambda=0.00	O
,	O
nedges=55	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
26.18	O
sparse	B
ggms	O
learned	O
using	O
graphical	B
lasso	I
applied	O
to	O
the	O
ﬂow	B
cytometry	I
data	O
.	O
(	O
a	O
)	O
λ	O
=	O
36	O
.	O
(	O
b	O
)	O
λ	O
=	O
27	O
.	O
(	O
c	O
)	O
λ	O
=	O
7	O
.	O
(	O
d	O
)	O
λ	O
=	O
0.	O
figure	O
generated	O
by	O
ggmlassodemo	O
.	O
(	O
cid:7	O
)	O
where	O
||ω||1	O
=	O
glasso	O
.	O
j	O
,	O
k	O
|ωjk|	O
is	O
the	O
1-norm	O
of	O
the	O
matrix	O
.	O
this	O
is	O
called	O
the	O
graphical	B
lasso	I
or	O
although	O
the	O
objective	O
is	O
convex	B
,	O
it	O
is	O
non-smooth	B
(	O
because	O
of	O
the	O
non-differentiable	O
(	O
cid:2	O
)	O
1	O
penalty	O
)	O
and	O
is	O
constrained	O
(	O
because	O
ω	O
must	O
be	O
a	O
positive	B
deﬁnite	I
matrix	O
)	O
.	O
several	O
algorithms	O
have	O
been	O
proposed	O
for	O
optimizing	O
this	O
objective	O
(	O
yuan	O
and	O
lin	O
2007	O
;	O
banerjee	O
et	O
al	O
.	O
2008	O
;	O
duchi	O
et	O
al	O
.	O
2008	O
)	O
,	O
although	O
arguably	O
the	O
simplest	O
is	O
the	O
one	O
in	O
(	O
friedman	O
et	O
al	O
.	O
2008	O
)	O
,	O
which	O
uses	O
a	O
coordinate	O
descent	O
algorithm	O
similar	B
to	O
the	O
shooting	B
algorithm	O
for	O
lasso	B
.	O
see	O
ggmlassohtf	O
for	O
an	O
implementation	O
.	O
(	O
see	O
also	O
(	O
mazumder	O
and	O
hastie	O
2012	O
)	O
for	O
a	O
more	O
recent	O
version	O
of	O
this	O
algorithm	O
.	O
)	O
as	O
an	O
example	O
,	O
let	O
us	O
apply	O
the	O
method	O
to	O
the	O
ﬂow	B
cytometry	I
dataset	O
from	O
(	O
sachs	O
et	O
al	O
.	O
2005	O
)	O
.	O
a	O
discretized	O
version	O
of	O
the	O
data	O
is	O
shown	O
in	O
figure	O
26.17	O
(	O
a	O
)	O
.	O
here	O
we	O
use	O
the	O
original	O
continuous	O
data	O
.	O
however	O
,	O
we	O
are	O
ignoring	O
the	O
fact	O
that	O
the	O
data	O
was	O
sampled	O
under	O
intervention	O
.	O
in	O
figure	O
26.18	O
,	O
we	O
illustrate	O
the	O
graph	B
structures	O
that	O
are	O
learned	O
as	O
we	O
sweep	O
λ	O
from	O
0	O
to	O
a	O
large	O
value	O
.	O
these	O
represent	O
a	O
range	O
of	O
plausible	O
hypotheses	O
about	O
the	O
connectivity	O
of	O
these	O
proteins	O
.	O
it	O
is	O
worth	O
comparing	O
this	O
with	O
the	O
dag	O
that	O
was	O
learned	O
in	O
figure	O
26.17	O
(	O
b	O
)	O
.	O
the	O
dag	O
has	O
the	O
advantage	O
that	O
it	O
can	O
easily	O
model	O
the	O
interventional	O
nature	O
of	O
the	O
data	O
,	O
but	O
the	O
disadvantage	O
that	O
it	O
can	O
not	O
model	O
the	O
feedback	B
loops	I
that	O
are	O
known	O
to	O
exist	O
in	O
this	O
biological	O
pathway	O
(	O
see	O
the	O
discussion	O
in	O
(	O
schmidt	O
and	O
murphy	O
2009	O
)	O
)	O
.	O
note	O
that	O
the	O
fact	O
that	O
we	O
show	O
many	O
ugms	O
and	O
only	O
one	O
dag	O
is	O
incidental	O
:	O
we	O
could	O
easily	O
use	O
bic	O
to	O
pick	O
the	O
“	O
best	O
”	O
ugm	O
,	O
and	O
conversely	O
,	O
we	O
26.7.	O
learning	B
undirected	O
gaussian	O
graphical	O
models	O
941	O
could	O
easily	O
display	O
several	O
dag	O
structures	O
,	O
sampled	O
from	O
the	O
posterior	O
.	O
26.7.3	O
bayesian	O
inference	B
for	O
ggm	O
structure	O
*	O
although	O
the	O
graphical	B
lasso	I
is	O
reasonably	O
fast	O
,	O
it	O
only	O
gives	O
a	O
point	B
estimate	I
of	O
the	O
structure	O
.	O
furthermore	O
,	O
it	O
is	O
not	O
model-selection	O
consistent	B
(	O
meinshausen	O
2005	O
)	O
,	O
meaning	O
it	O
can	O
not	O
recover	O
the	O
true	O
graph	O
even	O
as	O
n	O
→	O
∞	O
.	O
it	O
would	O
be	O
preferable	O
to	O
integrate	B
out	I
the	O
parameters	O
,	O
and	O
perform	O
posterior	O
inference	O
in	O
the	O
space	O
of	O
graphs	O
,	O
i.e.	O
,	O
to	O
compute	O
p	O
(	O
g|d	O
)	O
.	O
we	O
can	O
then	O
extract	O
summaries	O
of	O
the	O
posterior	O
,	O
such	O
as	O
posterior	O
edge	O
marginals	O
,	O
p	O
(	O
gij	O
=	O
1|d	O
)	O
,	O
just	O
as	O
we	O
did	O
for	O
dags	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
how	O
to	O
do	O
this	O
.	O
note	O
that	O
the	O
situation	O
is	O
analogous	O
to	O
chapter	O
13	O
,	O
where	O
we	O
discussed	O
variable	O
selection	O
.	O
in	O
section	O
13.2	O
,	O
we	O
discussed	O
bayesian	O
variable	O
selection	O
,	O
where	O
we	O
integrated	O
out	O
the	O
regression	B
weights	O
and	O
computed	O
p	O
(	O
γ|d	O
)	O
and	O
the	O
marginal	O
inclusion	O
probabilities	O
p	O
(	O
γj	O
=	O
1|d	O
)	O
.	O
then	O
in	O
section	O
13.3	O
,	O
we	O
discussed	O
methods	O
based	O
on	O
(	O
cid:2	O
)	O
1	O
regularization	B
.	O
here	O
we	O
have	O
the	O
same	O
dichotomy	O
,	O
but	O
we	O
are	O
presenting	O
them	O
in	O
the	O
opposite	O
order	O
.	O
if	O
the	O
graph	B
is	O
decomposable	B
,	O
and	O
if	O
we	O
use	O
conjugate	B
priors	I
,	O
we	O
can	O
compute	O
the	O
marginal	B
likelihood	I
in	O
closed	O
form	O
(	O
dawid	O
and	O
lauritzen	O
1993	O
)	O
.	O
furthermore	O
,	O
we	O
can	O
efficiently	O
identify	O
the	O
decomposable	B
neighbors	O
of	O
a	O
graph	B
(	O
thomas	O
and	O
green	O
2009	O
)	O
,	O
i.e.	O
,	O
the	O
set	O
of	O
legal	O
edge	O
additions	O
and	O
removals	O
.	O
this	O
means	O
that	O
we	O
can	O
perform	O
relatively	O
efficient	O
stochastic	O
local	O
search	O
to	O
approximate	O
the	O
posterior	O
(	O
see	O
e.g	O
.	O
(	O
giudici	O
and	O
green	O
1999	O
;	O
armstrong	O
et	O
al	O
.	O
2008	O
;	O
scott	O
and	O
carvalho	O
2008	O
)	O
)	O
.	O
however	O
,	O
the	O
restriction	O
to	O
decomposable	B
graphs	I
is	O
rather	O
limiting	O
if	O
one	O
’	O
s	O
goal	O
is	O
knowledge	B
discovery	I
,	O
since	O
the	O
number	O
of	O
decomposable	B
graphs	I
is	O
much	O
less	O
than	O
the	O
number	O
of	O
general	O
undirected	B
graphs.5	O
a	O
few	O
authors	O
have	O
looked	O
at	O
bayesian	O
inference	B
for	O
ggm	O
structure	O
in	O
the	O
non-decomposable	O
case	O
(	O
e.g.	O
,	O
(	O
dellaportas	O
et	O
al	O
.	O
2003	O
;	O
wong	O
et	O
al	O
.	O
2003	O
;	O
jones	O
et	O
al	O
.	O
2005	O
)	O
)	O
,	O
but	O
such	O
methods	O
can	O
not	O
scale	O
to	O
large	O
models	O
because	O
they	O
use	O
an	O
expensive	O
monte	O
carlo	O
approximation	O
to	O
the	O
marginal	B
likelihood	I
(	O
atay-kayis	O
and	O
massam	O
2005	O
)	O
.	O
(	O
lenkoski	O
and	O
dobra	O
2008	O
)	O
suggested	O
using	O
a	O
laplace	O
approxmation	O
.	O
this	O
requires	O
computing	O
the	O
map	O
estimate	O
of	O
the	O
parameters	O
for	O
ω	O
under	O
a	O
g-	O
wishart	O
prior	O
(	O
roverato	O
2002	O
)	O
.	O
in	O
(	O
lenkoski	O
and	O
dobra	O
2008	O
)	O
,	O
they	O
used	O
the	O
iterative	O
proportional	O
scaling	O
algorithm	O
(	O
speed	O
and	O
kiiveri	O
1986	O
;	O
hara	O
and	O
takimura	O
2008	O
)	O
to	O
ﬁnd	O
the	O
mode	B
.	O
however	O
,	O
this	O
is	O
very	O
slow	O
,	O
since	O
it	O
requires	O
knowing	O
the	O
maximal	O
cliques	O
of	O
the	O
graph	B
,	O
which	O
is	O
np-hard	O
in	O
general	O
.	O
in	O
(	O
moghaddam	O
et	O
al	O
.	O
2009	O
)	O
,	O
a	O
much	O
faster	O
method	O
is	O
proposed	O
.	O
in	O
particular	O
,	O
they	O
modify	O
the	O
gradient-based	O
methods	O
from	O
section	O
26.7.1	O
to	O
ﬁnd	O
the	O
map	O
estimate	O
;	O
these	O
algorithms	O
do	O
not	O
need	O
to	O
know	O
the	O
cliques	B
of	O
the	O
graph	B
.	O
a	O
further	O
speedup	O
is	O
obtained	O
by	O
just	O
using	O
a	O
diagonal	B
laplace	O
approximation	O
,	O
which	O
is	O
more	O
accurate	O
than	O
bic	O
,	O
but	O
has	O
essentially	O
the	O
same	O
cost	O
.	O
this	O
,	O
plus	O
the	O
lack	O
of	O
restriction	O
to	O
decomposable	B
graphs	I
,	O
enables	O
fairly	O
fast	O
stochastic	O
search	O
methods	O
to	O
be	O
used	O
to	O
approximate	O
p	O
(	O
g|d	O
)	O
and	O
its	O
mode	B
.	O
this	O
approach	O
signiﬁcantly	O
outperfomed	O
graphical	B
lasso	I
,	O
both	O
in	O
terms	O
of	O
predictive	B
accuracy	O
and	O
structural	O
recovery	O
,	O
for	O
a	O
comparable	O
computational	O
cost	O
.	O
5.	O
the	O
number	O
of	O
decomposable	B
graphs	I
on	O
v	O
nodes	B
,	O
for	O
v	O
=	O
2	O
,	O
.	O
.	O
.	O
,	O
8	O
,	O
is	O
as	O
follows	O
(	O
(	O
armstrong	O
2005	O
,	O
p158	O
)	O
)	O
:	O
2	O
;	O
8	O
;	O
61	O
;	O
822	O
;	O
18,154	O
;	O
61,7675	O
;	O
30,888,596.	O
if	O
we	O
divide	O
these	O
numbers	O
by	O
the	O
number	O
of	O
undirected	B
graphs	O
,	O
which	O
is	O
2v	O
(	O
v	O
−1	O
)	O
/2	O
,	O
we	O
ﬁnd	O
the	O
ratios	O
are	O
:	O
1	O
,	O
1	O
,	O
0.95	O
,	O
0.8	O
,	O
0.55	O
,	O
0.29	O
,	O
0.12.	O
so	O
we	O
see	O
that	O
decomposable	B
graphs	I
form	O
a	O
vanishing	O
fraction	O
of	O
the	O
total	O
hypothesis	O
space	O
.	O
942	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
26.7.4	O
handling	O
non-gaussian	O
data	O
using	O
copulas	O
*	O
the	O
graphical	B
lasso	I
and	O
variants	O
is	O
inhertently	O
limited	O
to	O
data	O
that	O
is	O
jointly	O
gaussian	O
,	O
which	O
is	O
a	O
rather	O
severe	O
restriction	O
.	O
fortunately	O
the	O
method	O
can	O
be	O
generalized	O
to	O
handle	O
non-gaussian	O
,	O
but	O
still	O
continuous	O
,	O
data	O
in	O
a	O
fairly	O
simple	O
fashion	O
.	O
the	O
basic	O
idea	O
is	O
to	O
estimate	O
a	O
set	O
of	O
d	O
univariate	O
monotonic	O
transformations	O
fj	O
,	O
one	O
per	O
variable	O
j	O
,	O
such	O
that	O
the	O
resulting	O
transformed	O
data	O
is	O
jointly	O
gaussian	O
.	O
if	O
this	O
is	O
possible	O
,	O
we	O
say	O
the	O
data	O
belongs	O
to	O
the	O
nonparametric	O
normal	B
distribution	O
,	O
or	O
nonparanormal	B
distribution	O
(	O
liu	O
et	O
al	O
.	O
2009	O
)	O
.	O
this	O
is	O
equivalent	O
to	O
the	O
family	B
of	O
gaussian	O
copulas	O
(	O
klaassen	O
and	O
wellner	O
1997	O
)	O
.	O
details	O
on	O
how	O
to	O
estimate	O
the	O
fj	O
transformations	O
from	O
the	O
empirical	O
cdf	O
’	O
s	O
of	O
each	O
variable	O
can	O
be	O
found	O
in	O
(	O
liu	O
et	O
al	O
.	O
2009	O
)	O
.	O
after	O
transforming	O
the	O
data	O
,	O
we	O
can	O
compute	O
the	O
correlation	B
matrix	I
and	O
then	O
apply	O
glasso	O
in	O
the	O
usual	O
way	O
.	O
one	O
can	O
show	O
,	O
under	O
various	O
assumptions	O
,	O
that	O
this	O
is	O
a	O
consistent	B
estimator	I
of	O
the	O
graph	B
structure	O
,	O
representing	O
the	O
ci	O
assumptions	O
of	O
the	O
original	O
distribution	O
(	O
liu	O
et	O
al	O
.	O
2009	O
)	O
.	O
26.8	O
learning	B
undirected	O
discrete	B
graphical	O
models	O
the	O
problem	O
of	O
learning	B
the	O
structure	O
for	O
ugms	O
with	O
discrete	B
variables	O
is	O
harder	O
than	O
the	O
gaussian	O
case	O
,	O
because	O
computing	O
the	O
partition	B
function	I
z	O
(	O
θ	O
)	O
,	O
which	O
is	O
needed	O
for	O
parameter	B
estimation	O
,	O
has	O
complexity	O
comparable	O
to	O
computing	O
the	O
permanent	B
of	O
a	O
matrix	O
,	O
which	O
in	O
general	O
is	O
intractable	O
(	O
jerrum	O
et	O
al	O
.	O
2004	O
)	O
.	O
by	O
contrast	O
,	O
in	O
the	O
gaussian	O
case	O
,	O
computing	O
z	O
only	O
requires	O
computing	O
a	O
matrix	O
determinant	O
,	O
which	O
is	O
at	O
most	O
o	O
(	O
v	O
3	O
)	O
.	O
since	O
stochastic	O
local	O
search	O
is	O
not	O
tractable	O
for	O
general	O
discrete	B
ugms	O
,	O
below	O
we	O
mention	O
some	O
possible	O
alternative	O
approaches	O
that	O
have	O
been	O
tried	O
.	O
26.8.1	O
graphical	B
lasso	I
for	O
mrfs/crfs	O
it	O
is	O
possible	O
to	O
extend	O
the	O
graphical	B
lasso	I
idea	O
to	O
the	O
discrete	B
mrf	O
and	O
crf	O
case	O
.	O
however	O
,	O
now	O
there	O
is	O
a	O
set	O
of	O
parameters	O
associated	O
with	O
each	O
edge	O
in	O
the	O
graph	B
,	O
so	O
we	O
have	O
to	O
use	O
the	O
graph	B
analog	O
of	O
group	B
lasso	I
(	O
see	O
section	O
13.5.1	O
)	O
.	O
for	O
example	O
,	O
consider	O
a	O
pairwise	O
crf	O
with	O
ternary	O
nodes	B
,	O
and	O
node	O
and	O
edge	O
potentials	O
given	O
by	O
⎞	O
⎠	O
,	O
ψst	O
(	O
ys	O
,	O
yt	O
,	O
x	O
)	O
=	O
⎛	O
⎝	O
wt	O
⎛	O
⎝vt	O
t1x	O
vt	O
t2x	O
vt	O
t3x	O
t11x	O
wt	O
st21x	O
wt	O
st31x	O
wt	O
st12x	O
wt	O
st22x	O
wt	O
st32x	O
wt	O
st13x	O
st23x	O
st33x	O
wt	O
wt	O
ψt	O
(	O
yt	O
,	O
x	O
)	O
=	O
(	O
26.71	O
)	O
where	O
we	O
assume	O
x	O
begins	O
with	O
a	O
constant	O
1	O
term	O
,	O
to	O
account	O
for	O
the	O
offset	O
.	O
(	O
if	O
x	O
only	O
contains	O
1	O
,	O
the	O
crf	O
reduces	O
to	O
an	O
mrf	O
.	O
)	O
note	O
that	O
we	O
may	O
choose	O
to	O
set	O
some	O
of	O
the	O
vtk	O
and	O
wstjk	O
weights	O
to	O
0	O
,	O
to	O
ensure	O
identiﬁability	O
,	O
although	O
this	O
can	O
also	O
be	O
taken	O
care	O
of	O
by	O
the	O
prior	O
,	O
as	O
shown	O
in	O
exercise	O
8.5.	O
to	O
learn	O
sparse	B
structure	O
,	O
we	O
can	O
minimize	O
the	O
following	O
objective	O
:	O
⎞	O
⎠	O
(	O
cid:25	O
)	O
(	O
cid:24	O
)	O
(	O
cid:4	O
)	O
j	O
=	O
−	O
n	O
(	O
cid:4	O
)	O
v	O
(	O
cid:4	O
)	O
v	O
(	O
cid:4	O
)	O
i=1	O
t	O
+λ1	O
v	O
(	O
cid:4	O
)	O
v	O
(	O
cid:4	O
)	O
v	O
(	O
cid:4	O
)	O
log	O
ψt	O
(	O
yit	O
,	O
xi	O
,	O
vt	O
)	O
+	O
log	O
ψst	O
(	O
yis	O
,	O
yit	O
,	O
xi	O
,	O
wst	O
)	O
||wst||p	O
+	O
λ2	O
s=1	O
t=s+1	O
||vt||2	O
2	O
(	O
26.72	O
)	O
s=1	O
t=s+1	O
t=1	O
26.8.	O
learning	B
undirected	O
discrete	B
graphical	O
models	O
943	O
case	O
course	O
children	B
bible	O
health	O
christian	O
insurance	O
computer	O
evidence	B
disk	O
email	O
display	O
card	O
fact	O
earth	O
files	O
graphics	O
government	O
god	O
dos	O
format	O
help	O
data	O
image	O
video	O
gun	O
human	O
car	O
president	O
israel	O
jesus	O
drive	O
memory	O
number	O
power	O
law	O
engine	O
dealer	O
jews	O
baseball	O
ftp	O
mac	O
scsi	O
problem	O
rights	O
war	O
religion	O
games	O
fans	O
pc	O
program	O
phone	B
nasa	O
state	B
question	O
software	O
research	O
shuttle	O
launch	O
moon	O
science	O
orbit	O
space	O
university	O
world	O
system	O
driver	O
version	O
technology	O
windows	O
hockey	O
league	O
nhl	O
players	O
season	O
team	O
win	O
won	O
figure	O
26.19	O
an	O
mrf	O
estimated	O
from	O
the	O
20-newsgroup	O
data	O
using	O
group	O
(	O
cid:7	O
)	O
1	O
regularization	B
with	O
λ	O
=	O
256.	O
isolated	O
nodes	B
are	O
not	O
plotted	O
.	O
from	O
figure	O
5.9	O
of	O
(	O
schmidt	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
mark	O
schmidt	O
.	O
where	O
||wst||p	O
is	O
the	O
p-norm	O
;	O
common	O
choices	O
are	O
p	O
=	O
2	O
or	O
p	O
=	O
∞	O
,	O
as	O
explained	O
in	O
sec-	O
tion	O
13.5.1.	O
this	O
method	O
of	O
crf	O
structure	B
learning	I
was	O
ﬁrst	O
suggested	O
in	O
(	O
schmidt	O
et	O
al	O
.	O
2008	O
)	O
.	O
(	O
the	O
use	O
of	O
(	O
cid:2	O
)	O
1	O
regularization	B
for	O
learning	B
the	O
structure	O
of	O
binary	O
mrfs	O
was	O
proposed	O
in	O
(	O
lee	O
et	O
al	O
.	O
2006	O
)	O
.	O
)	O
although	O
this	O
objective	O
is	O
convex	B
,	O
it	O
can	O
be	O
costly	O
to	O
evaluate	O
,	O
since	O
we	O
need	O
to	O
perform	O
inference	B
to	O
compute	O
its	O
gradient	O
,	O
as	O
explained	O
in	O
section	O
19.6.3	O
(	O
this	O
is	O
true	O
also	O
for	O
mrfs	O
)	O
.	O
we	O
should	O
therefore	O
use	O
an	O
optimizer	O
that	O
does	O
not	O
make	O
too	O
many	O
calls	O
to	O
the	O
objective	O
function	O
or	O
its	O
gradient	O
,	O
such	O
as	O
the	O
projected	O
quasi-newton	O
method	O
in	O
(	O
schmidt	O
et	O
al	O
.	O
2009	O
)	O
.	O
in	O
addition	O
,	O
we	O
can	O
use	O
approximate	B
inference	I
,	O
such	O
as	O
convex	B
belief	I
propagation	I
(	O
section	O
22.4.2	O
)	O
,	O
to	O
compute	O
an	O
approximate	O
objective	O
and	O
gradient	O
more	O
quickly	O
.	O
another	O
approach	O
is	O
to	O
apply	O
the	O
group	B
lasso	I
penalty	O
to	O
the	O
pseudo-likelihood	B
discussed	O
in	O
section	O
19.5.4.	O
this	O
is	O
much	O
faster	O
,	O
since	O
inference	B
is	O
no	O
longer	O
required	O
(	O
hoeﬂing	O
and	O
tibshirani	O
2009	O
)	O
.	O
figure	O
26.19	O
shows	O
the	O
result	O
of	O
applying	O
this	O
procedure	O
to	O
the	O
20-newsgroup	O
data	O
,	O
where	O
yit	O
indicates	O
the	O
presence	O
of	O
word	O
t	O
in	O
document	O
i	O
,	O
and	O
xi	O
=	O
1	O
(	O
so	O
the	O
model	O
is	O
an	O
mrf	O
)	O
.	O
944	O
chapter	O
26.	O
graphical	B
model	I
structure	O
learning	B
(	O
cid:38	O
)	O
(	O
cid:41	O
)	O
(	O
cid:55	O
)	O
(	O
cid:51	O
)	O
(	O
cid:11	O
)	O
(	O
cid:54	O
)	O
(	O
cid:32	O
)	O
(	O
cid:41	O
)	O
(	O
cid:12	O
)	O
(	O
cid:51	O
)	O
(	O
cid:11	O
)	O
(	O
cid:54	O
)	O
(	O
cid:32	O
)	O
(	O
cid:55	O
)	O
(	O
cid:12	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:24	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:28	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:24	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:20	O
)	O
(	O
cid:51	O
)	O
(	O
cid:3	O
)	O
(	O
cid:11	O
)	O
(	O
cid:38	O
)	O
(	O
cid:32	O
)	O
(	O
cid:41	O
)	O
(	O
cid:12	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:24	O
)	O
(	O
cid:51	O
)	O
(	O
cid:11	O
)	O
(	O
cid:38	O
)	O
(	O
cid:32	O
)	O
(	O
cid:55	O
)	O
(	O
cid:12	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:24	O
)	O
(	O
cid:38	O
)	O
(	O
cid:79	O
)	O
(	O
cid:82	O
)	O
(	O
cid:88	O
)	O
(	O
cid:71	O
)	O
(	O
cid:92	O
)	O
(	O
cid:54	O
)	O
(	O
cid:83	O
)	O
(	O
cid:85	O
)	O
(	O
cid:76	O
)	O
(	O
cid:81	O
)	O
(	O
cid:78	O
)	O
(	O
cid:79	O
)	O
(	O
cid:72	O
)	O
(	O
cid:85	O
)	O
(	O
cid:53	O
)	O
(	O
cid:68	O
)	O
(	O
cid:76	O
)	O
(	O
cid:81	O
)	O
(	O
cid:58	O
)	O
(	O
cid:72	O
)	O
(	O
cid:87	O
)	O
(	O
cid:3	O
)	O
(	O
cid:42	O
)	O
(	O
cid:85	O
)	O
(	O
cid:68	O
)	O
(	O
cid:86	O
)	O
(	O
cid:86	O
)	O
(	O
cid:51	O
)	O
(	O
cid:11	O
)	O
(	O
cid:58	O
)	O
(	O
cid:32	O
)	O
(	O
cid:41	O
)	O
(	O
cid:12	O
)	O
(	O
cid:51	O
)	O
(	O
cid:11	O
)	O
(	O
cid:58	O
)	O
(	O
cid:32	O
)	O
(	O
cid:55	O
)	O
(	O
cid:12	O
)	O
(	O
cid:20	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:20	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:28	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:28	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
(	O
cid:54	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:53	O
)	O
(	O
cid:41	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:41	O
)	O
(	O
cid:55	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:41	O
)	O
(	O
cid:41	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:55	O
)	O
(	O
cid:55	O
)	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
cid:55	O
)	O
(	O
cid:38	O
)	O
(	O
cid:41	O
)	O
(	O
cid:55	O
)	O
(	O
cid:51	O
)	O
(	O
cid:11	O
)	O
(	O
cid:53	O
)	O
(	O
cid:32	O
)	O
(	O
cid:41	O
)	O
(	O
cid:12	O
)	O
(	O
cid:51	O
)	O
(	O
cid:11	O
)	O
(	O
cid:53	O
)	O
(	O
cid:32	O
)	O
(	O
cid:55	O
)	O
(	O
cid:12	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:27	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:21	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:21	O
)	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
(	O
cid:27	O
)	O
figure	O
26.20	O
water	O
sprinkler	O
dgm	O
with	O
corresponding	O
binary	O
cpts	O
.	O
t	O
and	O
f	O
stand	O
for	O
true	O
and	O
false	O
.	O
26.8.2	O
thin	B
junction	I
trees	I
so	O
far	O
,	O
we	O
have	O
been	O
concerned	O
with	O
learning	B
“	O
sparse	B
”	O
graphs	O
,	O
but	O
these	O
do	O
not	O
necessarily	O
have	O
low	O
treewidth	O
.	O
for	O
example	O
,	O
a	O
d	O
×	O
d	O
grid	O
is	O
sparse	B
,	O
but	O
has	O
treewidth	B
o	O
(	O
d	O
)	O
.	O
this	O
means	O
that	O
the	O
models	O
we	O
learn	O
may	O
be	O
intractable	O
to	O
use	O
for	O
inference	B
purposes	O
,	O
which	O
defeats	O
one	O
of	O
the	O
two	O
main	O
reasons	O
to	O
learn	O
graph	B
structure	O
in	O
the	O
ﬁrst	O
place	O
(	O
the	O
other	O
reason	O
being	O
“	O
knowledge	B
discovery	I
”	O
)	O
.	O
there	O
have	O
been	O
various	O
attempts	O
to	O
learn	O
graphical	O
models	O
with	O
bounded	O
treewidth	B
(	O
e.g.	O
,	O
(	O
bach	O
and	O
jordan	O
2001	O
;	O
srebro	O
2001	O
;	O
elidan	O
and	O
gould	O
2008	O
;	O
shahaf	O
et	O
al	O
.	O
2009	O
)	O
)	O
,	O
also	O
known	O
as	O
thin	B
junction	I
trees	I
,	O
but	O
the	O
exact	O
problem	O
in	O
general	O
is	O
hard	O
.	O
an	O
alternative	O
approach	O
is	O
to	O
learn	O
a	O
model	O
with	O
low	O
circuit	O
complexity	O
(	O
gogate	O
et	O
al	O
.	O
2010	O
;	O
poon	O
and	O
domingos	O
2011	O
)	O
.	O
such	O
models	O
may	O
have	O
high	O
treewidth	O
,	O
but	O
they	O
exploit	O
context-	O
speciﬁc	O
independence	O
and	O
determinism	B
to	O
enable	O
fast	O
exact	O
inference	B
(	O
see	O
e.g.	O
,	O
(	O
darwiche	O
2009	O
)	O
)	O
.	O
exercises	O
exercise	O
26.1	O
causal	O
reasoning	O
in	O
the	O
sprinkler	O
network	O
consider	O
the	O
causal	O
network	O
in	O
figure	O
26.20.	O
let	O
t	O
represent	O
true	O
and	O
f	O
represent	O
false	O
.	O
a.	O
suppose	O
i	O
perform	O
a	O
perfect	B
intervention	I
and	O
make	O
the	O
grass	O
wet	O
.	O
what	O
is	O
the	O
probability	O
the	O
sprinkler	O
b.	O
suppose	O
i	O
perform	O
a	O
perfect	B
intervention	I
and	O
make	O
the	O
grass	O
dry	O
.	O
what	O
is	O
the	O
probability	O
the	O
sprinkler	O
is	O
on	O
,	O
p	O
(	O
s	O
=	O
t|do	O
(	O
w	O
=	O
t	O
)	O
)	O
?	O
is	O
on	O
,	O
p	O
(	O
s	O
=	O
t|do	O
(	O
w	O
=	O
f	O
)	O
)	O
?	O
is	O
the	O
probability	O
the	O
sprinkler	O
is	O
on	O
,	O
p	O
(	O
s	O
=	O
t|do	O
(	O
c	O
=	O
t	O
)	O
)	O
?	O
c.	O
suppose	O
i	O
perform	O
a	O
perfect	B
intervention	I
and	O
make	O
the	O
clouds	O
“	O
turn	O
on	O
”	O
(	O
e.g.	O
,	O
by	O
seeding	O
them	O
)	O
.	O
what	O
27	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
27.1	O
introduction	O
in	O
this	O
chapter	O
,	O
we	O
are	O
concerned	O
with	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
,	O
such	O
as	O
bit	O
vectors	O
,	O
sequences	O
of	O
categorical	B
variables	I
,	O
count	O
vectors	O
,	O
graph	B
structures	O
,	O
relational	O
data	O
,	O
etc	O
.	O
these	O
models	O
can	O
be	O
used	O
to	O
analyze	O
voting	O
records	O
,	O
text	O
and	O
document	O
collections	O
,	O
low-intensity	O
images	O
,	O
movie	O
ratings	O
,	O
etc	O
.	O
however	O
,	O
we	O
will	O
mostly	O
focus	O
on	O
text	O
analysis	O
,	O
and	O
this	O
will	O
be	O
reﬂected	O
in	O
our	O
terminology	O
.	O
since	O
we	O
will	O
be	O
dealing	O
with	O
so	O
many	O
different	O
kinds	O
of	O
data	O
,	O
we	O
need	O
some	O
precise	O
notation	O
to	O
keep	O
things	O
clear	O
.	O
when	O
modeling	O
variable-length	O
sequences	O
of	O
categorical	B
variables	I
(	O
i.e.	O
,	O
let	O
yil	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
}	O
represent	O
symbols	O
or	O
tokens	B
)	O
,	O
such	O
as	O
words	O
in	O
a	O
document	O
,	O
we	O
will	O
the	O
identity	O
of	O
the	O
l	O
’	O
th	O
word	O
in	O
document	O
i	O
,	O
where	O
v	O
is	O
the	O
number	O
of	O
possible	O
words	O
in	O
the	O
vocabulary	O
.	O
we	O
assume	O
l	O
=	O
1	O
:	O
li	O
,	O
where	O
li	O
is	O
the	O
(	O
known	O
)	O
length	O
of	O
document	O
i	O
,	O
and	O
i	O
=	O
1	O
:	O
n	O
,	O
wheren	O
is	O
the	O
number	O
of	O
documents	O
.	O
we	O
will	O
often	O
ignore	O
the	O
word	O
order	O
,	O
resulting	O
in	O
a	O
bag	B
of	I
words	I
.	O
this	O
can	O
be	O
reduced	O
to	O
a	O
ﬁxed	O
length	O
vector	O
of	O
counts	O
(	O
a	O
histogram	B
)	O
.	O
we	O
will	O
use	O
niv	O
∈	O
{	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
li	O
}	O
to	O
denote	O
the	O
number	O
of	O
times	O
word	O
v	O
occurs	O
in	O
document	O
i	O
,	O
for	O
v	O
=	O
1	O
:	O
v	O
.	O
note	O
that	O
the	O
n	O
×	O
v	O
count	O
matrix	O
n	O
is	O
often	O
large	O
but	O
sparse	B
,	O
since	O
we	O
typically	O
have	O
many	O
documents	O
,	O
but	O
most	O
words	O
do	O
not	O
occur	O
in	O
any	O
given	O
document	O
.	O
in	O
some	O
cases	O
,	O
we	O
might	O
have	O
multiple	O
different	O
bags	O
of	O
words	O
,	O
e.g.	O
,	O
bags	O
of	O
text	O
words	O
and	O
bags	O
of	O
visual	B
words	I
.	O
these	O
correspond	O
to	O
different	O
“	O
channels	O
”	O
or	O
types	O
of	O
features	B
.	O
we	O
will	O
denote	O
these	O
by	O
yirl	O
,	O
for	O
r	O
=	O
1	O
:	O
r	O
(	O
the	O
number	O
of	O
responses	O
)	O
and	O
l	O
=	O
1	O
:	O
l	O
ir	O
.	O
if	O
lir	O
=	O
1	O
,	O
it	O
means	O
we	O
have	O
a	O
single	O
token	O
(	O
a	O
bag	O
of	O
length	O
1	O
)	O
;	O
in	O
this	O
case	O
,	O
we	O
just	O
write	O
yir	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
vr	O
}	O
if	O
every	O
channel	O
is	O
just	O
a	O
single	O
token	O
,	O
we	O
write	O
the	O
ﬁxed-size	O
response	O
vector	O
as	O
for	O
brevity	O
.	O
yi,1	O
:	O
r	O
;	O
in	O
this	O
case	O
,	O
the	O
n	O
×	O
r	O
design	B
matrix	I
y	O
will	O
not	O
be	O
sparse	B
.	O
for	O
example	O
,	O
in	O
social	O
science	O
surveys	O
,	O
yir	O
could	O
be	O
the	O
response	O
of	O
person	O
i	O
to	O
the	O
r	O
’	O
th	O
multi-choice	O
question	O
.	O
out	O
goal	O
is	O
to	O
build	O
joint	O
probability	O
models	O
of	O
p	O
(	O
yi	O
)	O
or	O
p	O
(	O
ni	O
)	O
using	O
latent	B
variables	O
to	O
capture	O
the	O
correlations	O
.	O
we	O
will	O
then	O
try	O
to	O
interpret	O
the	O
latent	B
variables	O
,	O
which	O
provide	O
a	O
compressed	O
representation	O
of	O
the	O
data	O
.	O
we	O
provide	O
an	O
overview	O
of	O
some	O
approaches	O
in	O
section	O
27.2	O
,	O
before	O
going	O
into	O
more	O
detail	O
in	O
later	O
sections	O
.	O
towards	O
the	O
end	O
of	O
the	O
chapter	O
,	O
we	O
will	O
consider	O
modeling	O
graphs	O
and	O
relations	O
,	O
which	O
can	O
also	O
be	O
represented	O
as	O
sparse	B
discrete	O
matrices	O
.	O
for	O
example	O
,	O
we	O
might	O
want	O
to	O
model	O
the	O
graph	B
of	O
which	O
papers	O
mycite	O
which	O
other	O
papers	O
.	O
we	O
will	O
denote	O
these	O
relations	O
by	O
r	O
,	O
reserving	O
the	O
symbol	O
y	O
for	O
any	O
categorical	B
data	O
(	O
e.g.	O
,	O
text	O
)	O
associated	O
with	O
the	O
nodes	B
.	O
946	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
27.2	O
distributed	O
state	O
lvms	O
for	O
discrete	B
data	O
in	O
this	O
section	O
,	O
we	O
summarize	O
a	O
variety	O
of	O
possible	O
approaches	O
for	O
constructing	O
models	O
of	O
the	O
form	O
p	O
(	O
yi,1	O
:	O
li	O
)	O
,	O
for	O
bags	O
of	O
tokens	B
;	O
p	O
(	O
y1	O
:	O
r	O
)	O
,	O
for	O
vectors	O
of	O
tokens	B
;	O
and	O
p	O
(	O
ni	O
)	O
,	O
for	O
vectors	O
of	O
integer	O
counts	O
.	O
27.2.1	O
mixture	B
models	O
the	O
simplest	O
approach	O
is	O
to	O
use	O
a	O
ﬁnite	B
mixture	I
model	I
(	O
chapter	O
11	O
)	O
.	O
this	O
associates	O
a	O
single	O
discrete	O
latent	O
variable	O
,	O
qi	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
with	O
every	O
document	O
,	O
where	O
k	O
is	O
the	O
number	O
of	O
clusters	B
.	O
we	O
will	O
use	O
a	O
discrete	B
prior	O
,	O
qi	O
∼	O
cat	O
(	O
π	O
)	O
.	O
for	O
variable	O
length	O
documents	O
,	O
we	O
can	O
deﬁne	O
p	O
(	O
yil|qi	O
=	O
k	O
)	O
=	O
bkv	O
,	O
where	O
bkv	O
is	O
the	O
probability	O
that	O
cluster	O
k	O
generates	O
word	O
v.	O
the	O
value	O
of	O
qi	O
is	O
known	O
as	O
a	O
topic	B
,	O
and	O
the	O
vector	O
bk	O
is	O
the	O
k	O
’	O
th	O
topic	B
’	O
s	O
word	O
distribution	O
.	O
that	O
is	O
,	O
the	O
likelihood	B
has	O
the	O
form	O
li	O
(	O
cid:20	O
)	O
l=1	O
p	O
(	O
yi,1	O
:	O
li	O
|qi	O
=	O
k	O
)	O
=	O
(	O
cid:4	O
)	O
cat	O
(	O
yil|bk	O
)	O
li	O
(	O
cid:20	O
)	O
cat	O
(	O
yil|bk	O
)	O
(	O
cid:24	O
)	O
(	O
cid:25	O
)	O
p	O
(	O
yi,1	O
:	O
li	O
)	O
=	O
πk	O
the	O
induced	O
distribution	O
on	O
the	O
visible	B
data	O
is	O
given	O
by	O
if	O
the	O
sum	O
is	O
unknown	B
,	O
we	O
can	O
use	O
a	O
poisson	O
class-conditional	O
density	O
to	O
give	O
p	O
(	O
ni|qi	O
=	O
k	O
)	O
=	O
v	O
(	O
cid:20	O
)	O
v=1	O
poi	O
(	O
niv|λvk	O
)	O
(	O
cid:7	O
)	O
in	O
this	O
case	O
,	O
li|qi	O
=	O
k	O
∼	O
poi	O
(	O
v	O
λvk	O
)	O
.	O
k	O
l=1	O
the	O
“	O
generative	O
story	O
”	O
which	O
this	O
encodes	O
is	O
as	O
follows	O
:	O
for	O
document	O
i	O
,	O
pick	O
a	O
topic	B
qi	O
from	O
π	O
,	O
call	O
it	O
k	O
,	O
and	O
then	O
for	O
each	O
word	O
l	O
=	O
1	O
:	O
l	O
i	O
,	O
pick	O
a	O
word	O
from	O
bk	O
.	O
we	O
will	O
consider	O
more	O
sophisticated	O
generative	O
models	O
later	O
in	O
this	O
chapter	O
.	O
if	O
we	O
have	O
a	O
ﬁxed	O
set	O
of	O
categorical	B
observations	O
,	O
we	O
can	O
use	O
a	O
different	O
topic	B
matrix	O
for	O
each	O
output	O
variable	O
:	O
p	O
(	O
yi,1	O
:	O
r|qi	O
=	O
k	O
)	O
=	O
r	O
(	O
cid:20	O
)	O
cat	O
(	O
yil|b	O
(	O
r	O
)	O
k	O
)	O
r=1	O
this	O
is	O
an	O
unsupervised	O
analog	O
of	O
naive	O
bayes	O
classiﬁcation	B
.	O
if	O
the	O
sum	O
li	O
=	O
we	O
can	O
also	O
model	O
count	O
vectors	O
.	O
multinomial	B
:	O
p	O
(	O
ni|li	O
,	O
qi	O
=	O
k	O
)	O
=	O
mu	O
(	O
ni|li	O
,	O
bk	O
)	O
(	O
cid:7	O
)	O
(	O
27.3	O
)	O
v	O
niv	O
is	O
known	O
,	O
we	O
can	O
use	O
a	O
(	O
27.1	O
)	O
(	O
27.2	O
)	O
(	O
27.4	O
)	O
(	O
27.5	O
)	O
27.2.	O
distributed	O
state	O
lvms	O
for	O
discrete	B
data	O
947	O
27.2.2	O
exponential	B
family	I
pca	O
unfortunately	O
,	O
ﬁnite	O
mixture	O
models	O
are	O
very	O
limited	O
in	O
their	O
expressive	O
power	O
.	O
a	O
more	O
ﬂexible	O
model	O
is	O
to	O
use	O
a	O
vector	O
of	O
real-valued	O
continuous	O
latent	B
variables	O
,	O
similar	B
to	O
the	O
factor	B
analysis	I
in	O
pca	O
,	O
we	O
use	O
a	O
gaussian	O
prior	O
of	O
the	O
form	O
p	O
(	O
zi	O
)	O
=	O
(	O
fa	O
)	O
and	O
pca	O
models	O
in	O
chapter	O
12.	O
n	O
(	O
μ	O
,	O
σ	O
)	O
,	O
where	O
zi	O
∈	O
r	O
k	O
,	O
and	O
a	O
gaussian	O
likelihood	B
of	O
the	O
form	O
p	O
(	O
yi|zi	O
)	O
=	O
n	O
(	O
wzi	O
,	O
σ2i	O
)	O
.	O
indeed	O
,	O
the	O
method	O
known	O
this	O
method	O
can	O
certainly	O
be	O
applied	O
to	O
discrete	B
or	O
count	O
data	O
.	O
as	O
latent	B
semantic	I
analysis	I
(	O
lsa	O
)	O
orlatent	O
semantic	O
indexing	O
(	O
lsi	O
)	O
(	O
deerwester	O
et	O
al	O
.	O
1990	O
;	O
dumais	O
and	O
landauer	O
1997	O
)	O
is	O
exactly	O
equivalent	O
to	O
applying	O
pca	O
to	O
a	O
term	O
by	O
document	O
count	O
matrix	O
.	O
a	O
better	O
method	O
for	O
modeling	O
categorical	B
data	O
is	O
to	O
use	O
a	O
multinoulli	O
or	O
multinomial	B
distribu-	O
tion	O
.	O
we	O
just	O
have	O
to	O
change	O
the	O
likelihood	B
to	O
p	O
(	O
yi,1	O
:	O
li	O
|zi	O
)	O
=	O
cat	O
(	O
yil|s	O
(	O
wzi	O
)	O
)	O
(	O
27.6	O
)	O
where	O
w	O
∈	O
r	O
of	O
categorical	B
responses	O
,	O
we	O
can	O
use	O
v	O
×k	O
is	O
a	O
weight	O
matrix	O
and	O
s	O
is	O
the	O
softmax	B
function	O
.	O
if	O
we	O
have	O
a	O
ﬁxed	O
number	O
r	O
(	O
cid:20	O
)	O
cat	O
(	O
yir|s	O
(	O
wrzi	O
)	O
)	O
p	O
(	O
y1	O
:	O
r|zi	O
)	O
=	O
where	O
wr	O
∈	O
r	O
v	O
×k	O
is	O
the	O
weight	O
matrix	O
for	O
the	O
r	O
’	O
th	O
response	B
variable	I
.	O
this	O
model	O
is	O
called	O
categorical	B
pca	O
,	O
and	O
is	O
illustrated	O
in	O
figure	O
27.1	O
(	O
a	O
)	O
;	O
see	O
section	O
12.4	O
for	O
further	O
discussion	O
.	O
if	O
we	O
have	O
counts	O
,	O
we	O
can	O
use	O
a	O
multinomial	B
model	O
(	O
27.7	O
)	O
r=1	O
all	O
of	O
these	O
models	O
are	O
examples	O
of	O
exponential	B
family	I
pca	O
or	O
epca	O
(	O
collins	O
et	O
al	O
.	O
2002	O
;	O
mohamed	O
et	O
al	O
.	O
2008	O
)	O
,	O
which	O
is	O
an	O
unsupervised	O
analog	O
of	O
glms	O
.	O
the	O
corresponding	O
induced	O
distribution	O
on	O
the	O
visible	B
variables	I
has	O
the	O
form	O
(	O
cid:25	O
)	O
p	O
(	O
yi,1	O
:	O
li	O
)	O
=	O
p	O
(	O
yil|zi	O
,	O
w	O
)	O
n	O
(	O
zi|μ	O
,	O
σ	O
)	O
dzi	O
(	O
27.10	O
)	O
l=1	O
fitting	O
this	O
model	O
is	O
tricky	O
,	O
due	O
to	O
the	O
lack	O
of	O
conjugacy	O
.	O
(	O
collins	O
et	O
al	O
.	O
2002	O
)	O
proposed	O
a	O
coordinate	O
ascent	O
method	O
that	O
alternates	O
between	O
estimating	O
the	O
zi	O
and	O
w.	O
this	O
can	O
be	O
regarded	O
as	O
a	O
degenerate	B
version	O
of	O
em	O
,	O
that	O
computes	O
a	O
point	B
estimate	I
of	O
zi	O
in	O
the	O
e	O
step	O
.	O
the	O
problem	O
with	O
the	O
degenerate	B
approach	O
is	O
that	O
it	O
is	O
very	O
prone	O
to	O
overﬁtting	B
,	O
since	O
the	O
number	O
of	O
latent	B
variables	O
is	O
proportional	O
to	O
the	O
number	O
of	O
datacases	O
(	O
welling	O
et	O
al	O
.	O
2008	O
)	O
.	O
a	O
true	O
em	O
algorithm	O
would	O
marginalize	O
out	O
the	O
latent	B
variables	O
zi	O
.	O
a	O
way	O
to	O
do	O
this	O
for	O
categorical	B
pca	O
,	O
using	O
variational	O
em	O
,	O
is	O
discussed	O
in	O
section	O
12.4.	O
for	O
more	O
general	O
models	O
,	O
one	O
can	O
use	O
mcmc	O
(	O
mohamed	O
et	O
al	O
.	O
2008	O
)	O
.	O
li	O
(	O
cid:20	O
)	O
l=1	O
v	O
(	O
cid:20	O
)	O
v=1	O
(	O
cid:12	O
)	O
(	O
cid:24	O
)	O
li	O
(	O
cid:20	O
)	O
p	O
(	O
ni|li	O
,	O
zi	O
)	O
=	O
mu	O
(	O
ni|li	O
,	O
s	O
(	O
wzi	O
)	O
)	O
or	O
a	O
poisson	O
model	O
p	O
(	O
ni|zi	O
)	O
=	O
poi	O
(	O
niv|	O
exp	O
(	O
wt	O
v	O
,	O
:zi	O
)	O
)	O
(	O
27.8	O
)	O
(	O
27.9	O
)	O
948	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
μ	O
σ	O
yi,1	O
w	O
1	O
,	O
k	O
,	O
v	O
yi	O
,	O
r	O
n	O
w	O
r	O
,	O
k	O
,	O
v	O
zi	O
.	O
.	O
.	O
.	O
.	O
.	O
γ	O
(	O
a	O
)	O
α	O
πi	O
.	O
.	O
.	O
(	O
b	O
)	O
ni,1	O
bk	O
,	O
v	O
ni	O
,	O
v	O
li	O
n	O
figure	O
27.1	O
two	O
lvms	O
for	O
discrete	B
data	O
.	O
circles	O
are	O
scalar	O
nodes	O
,	O
ellipses	O
are	O
vector	O
nodes	O
,	O
squares	O
are	O
matrix	O
nodes	O
.	O
(	O
a	O
)	O
categorical	B
pca	O
.	O
(	O
b	O
)	O
multinomial	B
pca	O
.	O
27.2.3	O
lda	O
and	O
mpca	O
in	O
epca	O
,	O
the	O
quantity	O
wzi	O
represents	O
the	O
natural	B
parameters	I
of	O
the	O
exponential	B
family	I
.	O
some-	O
times	O
it	O
is	O
more	O
convenient	O
to	O
use	O
the	O
dual	O
parameters	O
.	O
for	O
example	O
,	O
for	O
the	O
multinomial	B
,	O
the	O
dual	O
parameter	O
is	O
the	O
probability	O
vector	O
,	O
whereas	O
the	O
natural	O
parameter	O
is	O
the	O
vector	O
of	O
log	O
odds	O
.	O
if	O
we	O
want	O
to	O
use	O
the	O
dual	O
parameters	O
,	O
we	O
need	O
to	O
constrain	O
the	O
latent	B
variables	O
so	O
they	O
live	O
in	O
the	O
appropriate	O
parameter	B
space	O
.	O
in	O
the	O
case	O
of	O
categorical	B
data	O
,	O
we	O
will	O
need	O
to	O
ensure	O
the	O
latent	B
vector	O
lives	O
in	O
sk	O
,	O
the	O
k-dimensional	O
probability	B
simplex	I
.	O
to	O
avoid	O
confusion	O
with	O
epca	O
,	O
we	O
will	O
denote	O
such	O
a	O
latent	B
vector	O
by	O
πi	O
.	O
in	O
this	O
case	O
,	O
the	O
natural	O
prior	O
for	O
the	O
latent	B
variables	O
is	O
the	O
dirichlet	O
,	O
πi	O
∼	O
dir	O
(	O
α	O
)	O
.	O
typically	O
we	O
set	O
α	O
=	O
α1k	O
.	O
if	O
we	O
set	O
α	O
(	O
cid:22	O
)	O
1	O
,	O
we	O
encourage	O
πi	O
to	O
be	O
sparse	B
,	O
as	O
shown	O
in	O
figure	O
2.14.	O
when	O
we	O
have	O
a	O
count	O
vector	O
whose	O
total	O
sum	O
is	O
known	O
,	O
the	O
likelihood	B
is	O
given	O
by	O
p	O
(	O
ni|li	O
,	O
πi	O
)	O
=	O
mu	O
(	O
ni|li	O
,	O
bπi	O
)	O
(	O
27.11	O
)	O
(	O
cid:7	O
)	O
this	O
model	O
is	O
called	O
multinomial	B
pca	O
or	O
mpca	O
(	O
buntine	O
2002	O
;	O
buntine	O
and	O
jakulin	O
2004	O
,	O
2006	O
)	O
.	O
see	O
figure	O
27.1	O
(	O
b	O
)	O
.	O
since	O
we	O
are	O
assuming	O
niv	O
=	O
k	O
bvkπiv	O
,	O
this	O
can	O
be	O
seen	O
as	O
a	O
form	O
of	O
matrix	B
factorization	I
for	O
the	O
count	O
matrix	O
.	O
note	O
that	O
we	O
use	O
bv	O
,	O
k	O
to	O
denote	O
the	O
parameter	B
vector	O
,	O
rather	O
than	O
wv	O
,	O
k	O
,	O
since	O
we	O
impose	O
the	O
constraints	O
that	O
0	O
≤	O
bv	O
,	O
k	O
≤	O
1	O
and	O
v	O
bv	O
,	O
k	O
=	O
1.	O
the	O
corresponding	O
marginal	B
distribution	I
has	O
the	O
form	O
(	O
cid:7	O
)	O
p	O
(	O
ni|li	O
)	O
=	O
mu	O
(	O
ni|li	O
,	O
bπi	O
)	O
dir	O
(	O
πi|α	O
)	O
dπi	O
(	O
cid:12	O
)	O
li	O
(	O
cid:20	O
)	O
l=1	O
unfortunately	O
,	O
this	O
integral	O
can	O
not	O
be	O
computed	O
analytically	O
.	O
if	O
we	O
have	O
a	O
variable	O
length	O
sequence	O
(	O
of	O
known	O
length	O
)	O
,	O
we	O
can	O
use	O
p	O
(	O
yi,1	O
:	O
li|πi	O
)	O
=	O
cat	O
(	O
yil|bπi	O
)	O
(	O
27.12	O
)	O
(	O
27.13	O
)	O
27.2.	O
distributed	O
state	O
lvms	O
for	O
discrete	B
data	O
949	O
this	O
is	O
called	O
latent	B
dirichlet	O
allocation	O
or	O
lda	O
(	O
blei	O
et	O
al	O
.	O
2003	O
)	O
,	O
and	O
will	O
be	O
described	O
in	O
much	O
greater	O
detail	O
below	O
.	O
lda	O
can	O
be	O
thought	O
of	O
as	O
a	O
probabilistic	O
extension	O
of	O
lsa	O
,	O
where	O
the	O
latent	B
quantities	O
πik	O
are	O
non-negative	O
and	O
sum	O
to	O
one	O
.	O
by	O
contrast	O
,	O
in	O
lsa	O
,	O
zik	O
can	O
be	O
negative	O
which	O
makes	O
interpetation	O
difficult	O
.	O
a	O
predecessor	O
to	O
lda	O
,	O
known	O
as	O
probabilistic	B
latent	I
semantic	I
indexing	I
or	O
plsi	O
(	O
hofmann	O
1999	O
)	O
,	O
uses	O
the	O
same	O
model	O
but	O
computes	O
a	O
point	B
estimate	I
of	O
πi	O
for	O
each	O
document	O
(	O
similar	B
to	O
epca	O
)	O
,	O
rather	O
than	O
integrating	O
it	O
out	O
.	O
thus	O
in	O
plsi	O
,	O
there	O
is	O
no	O
prior	O
for	O
πi	O
.	O
we	O
can	O
modify	O
lda	O
to	O
handle	O
a	O
ﬁxed	O
number	O
of	O
different	O
categorical	B
responses	O
as	O
follows	O
:	O
p	O
(	O
yi,1	O
:	O
r|πi	O
)	O
=	O
cat	O
(	O
yil|b	O
(	O
r	O
)	O
πi	O
)	O
(	O
27.14	O
)	O
r	O
(	O
cid:20	O
)	O
r=1	O
this	O
has	O
been	O
called	O
the	O
user	B
rating	I
proﬁle	I
(	O
urp	O
)	O
model	O
(	O
marlin	O
2003	O
)	O
,	O
and	O
the	O
simplex	B
factor	I
model	I
(	O
bhattacharya	O
and	O
dunson	O
2011	O
)	O
.	O
27.2.4	O
gap	O
model	O
and	O
non-negative	B
matrix	I
factorization	I
now	O
consider	O
modeling	O
count	O
vectors	O
where	O
we	O
do	O
not	O
constrain	O
the	O
sum	O
to	O
be	O
observed	O
.	O
this	O
case	O
,	O
the	O
latent	B
variables	O
just	O
need	O
to	O
be	O
non-negative	O
,	O
so	O
we	O
will	O
denote	O
them	O
by	O
z+	O
can	O
be	O
ensured	O
by	O
using	O
a	O
prior	O
of	O
the	O
form	O
in	O
i	O
.	O
this	O
(	O
27.15	O
)	O
(	O
27.16	O
)	O
v=1	O
this	O
is	O
called	O
the	O
gap	O
(	O
gamma-poisson	O
)	O
model	O
(	O
canny	O
2004	O
)	O
.	O
see	O
figure	O
27.2	O
(	O
a	O
)	O
.	O
in	O
(	O
buntine	O
and	O
jakulin	O
2006	O
)	O
,	O
it	O
is	O
shown	O
that	O
the	O
gap	O
model	O
,	O
when	O
conditioned	O
on	O
a	O
ﬁxed	O
li	O
,	O
reduces	O
to	O
the	O
mpca	O
model	O
.	O
this	O
follows	O
since	O
a	O
set	O
of	O
poisson	O
random	O
variables	O
,	O
when	O
conditioned	O
on	O
their	O
sum	O
,	O
becomes	O
a	O
multinomial	B
distribution	O
(	O
see	O
e.g.	O
,	O
(	O
ross	O
1989	O
)	O
)	O
.	O
if	O
we	O
set	O
αk	O
=	O
βk	O
=	O
0	O
in	O
the	O
gap	O
model	O
,	O
we	O
recover	O
a	O
method	O
known	O
as	O
non-negative	B
matrix	I
factorization	I
or	O
nmf	O
(	O
lee	O
and	O
seung	O
2001	O
)	O
,	O
as	O
shown	O
in	O
(	O
buntine	O
and	O
jakulin	O
2006	O
)	O
.	O
nmf	O
is	O
not	O
a	O
probabilistic	O
generative	O
model	O
,	O
since	O
it	O
does	O
not	O
specify	O
a	O
proper	O
prior	O
for	O
z+	O
i	O
.	O
furthermore	O
,	O
the	O
algorithm	O
proposed	O
in	O
(	O
lee	O
and	O
seung	O
2001	O
)	O
is	O
another	O
degenerate	B
em	O
algo-	O
rithm	O
,	O
so	O
suffers	O
from	O
overﬁtting	B
.	O
some	O
procedures	O
to	O
ﬁt	O
the	O
gap	O
model	O
,	O
which	O
overcome	O
these	O
problems	O
,	O
are	O
given	O
in	O
(	O
buntine	O
and	O
jakulin	O
2006	O
)	O
.	O
to	O
encourage	O
z+	O
i	O
to	O
be	O
sparse	B
,	O
we	O
can	O
modify	O
the	O
prior	O
to	O
be	O
a	O
spike-and-gamma	O
type	O
prior	O
as	O
follows	O
:	O
p	O
(	O
z+	O
ik	O
)	O
=	O
ρki	O
(	O
z+	O
ik	O
=	O
0	O
)	O
+	O
(	O
1	O
−	O
ρk	O
)	O
ga	O
(	O
z+	O
ik|αk	O
,	O
βk	O
)	O
(	O
27.17	O
)	O
where	O
ρk	O
is	O
the	O
probability	O
of	O
the	O
spike	O
at	O
0.	O
this	O
is	O
called	O
the	O
conditional	O
gamma	O
poisson	O
model	O
(	O
buntine	O
and	O
jakulin	O
2006	O
)	O
.	O
it	O
is	O
simple	O
to	O
modify	O
gibbs	O
sampling	O
to	O
handle	O
this	O
kind	O
of	O
prior	O
,	O
although	O
we	O
will	O
not	O
go	O
into	O
detail	O
here	O
.	O
k	O
(	O
cid:20	O
)	O
k=1	O
v	O
(	O
cid:20	O
)	O
p	O
(	O
z+	O
i	O
)	O
=	O
ga	O
(	O
z+	O
ik|αk	O
,	O
βk	O
)	O
the	O
likelihood	B
is	O
given	O
by	O
p	O
(	O
ni|z+	O
i	O
)	O
=	O
poi	O
(	O
niv|bt	O
v	O
,	O
:z+	O
i	O
)	O
950	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
αk	O
βk	O
z+	O
i	O
,	O
k	O
ni	O
,	O
v	O
n	O
α1	O
β1	O
z+	O
i,1	O
ni,1	O
b	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
γ	O
(	O
a	O
)	O
α	O
πi	O
qil	O
yil	O
b	O
γ	O
(	O
b	O
)	O
li	O
n	O
figure	O
27.2	O
(	O
a	O
)	O
gaussian-poisson	O
(	O
gap	O
)	O
model	O
.	O
(	O
b	O
)	O
latent	B
dirichlet	O
allocation	O
(	O
lda	O
)	O
model	O
.	O
27.3	O
latent	B
dirichlet	O
allocation	O
(	O
lda	O
)	O
in	O
this	O
section	O
,	O
we	O
explain	O
the	O
latent	B
dirichlet	O
allocation	O
or	O
lda	O
(	O
blei	O
et	O
al	O
.	O
2003	O
)	O
model	O
in	O
detail	O
.	O
27.3.1	O
basics	O
in	O
a	O
mixture	O
of	O
multinoullis	O
,	O
every	O
document	O
is	O
assigned	O
to	O
a	O
single	O
topic	O
,	O
qi	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
in	O
lda	O
,	O
every	O
word	O
is	O
assigned	O
to	O
its	O
own	O
topic	B
,	O
qil	O
∈	O
drawn	O
from	O
a	O
global	O
distribution	O
π	O
.	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
,	O
drawn	O
from	O
a	O
document-speciﬁc	O
distribution	O
πi	O
.	O
since	O
a	O
document	O
belongs	O
to	O
a	O
distribution	O
over	O
topics	O
,	O
rather	O
than	O
a	O
single	O
topic	O
,	O
the	O
model	O
is	O
called	O
an	O
admixture	B
mixture	I
or	O
mixed	B
membership	I
model	I
(	O
erosheva	O
et	O
al	O
.	O
2004	O
)	O
.	O
this	O
model	O
has	O
many	O
other	O
applications	O
beyond	O
text	O
analysis	O
,	O
e.g.	O
,	O
genetics	O
(	O
pritchard	O
et	O
al	O
.	O
2000	O
)	O
,	O
health	O
science	O
(	O
erosheva	O
et	O
al	O
.	O
2007	O
)	O
,	O
social	O
network	O
analysis	O
(	O
airoldi	O
et	O
al	O
.	O
2008	O
)	O
,	O
etc	O
.	O
adding	O
conjugate	B
priors	I
to	O
the	O
parameters	O
,	O
the	O
full	B
model	O
is	O
as	O
follows:1	O
πi|α	O
∼	O
dir	O
(	O
α1k	O
)	O
qil|πi	O
∼	O
cat	O
(	O
πi	O
)	O
bk|γ	O
∼	O
dir	O
(	O
γ1v	O
)	O
yil|qil	O
=	O
k	O
,	O
b	O
∼	O
cat	O
(	O
bk	O
)	O
(	O
27.18	O
)	O
(	O
27.19	O
)	O
(	O
27.20	O
)	O
(	O
27.21	O
)	O
this	O
is	O
illustrated	O
in	O
figure	O
27.2	O
(	O
b	O
)	O
.	O
we	O
can	O
marginalize	O
out	O
the	O
qi	O
variables	O
,	O
thereby	O
creating	O
a	O
1.	O
our	O
notation	O
is	O
similar	B
to	O
the	O
one	O
we	O
use	O
elsewhere	O
in	O
this	O
book	O
,	O
but	O
is	O
different	O
from	O
that	O
used	O
by	O
most	O
lda	O
papers	O
.	O
they	O
typically	O
use	O
wnd	O
for	O
the	O
identity	O
of	O
word	O
n	O
in	O
document	O
d	O
,	O
znd	O
to	O
represent	O
the	O
discrete	B
indicator	O
,	O
θd	O
as	O
the	O
continuous	O
latent	B
vector	O
for	O
document	O
d	O
,	O
and	O
βk	O
as	O
the	O
k	O
’	O
th	O
topic	B
vector	O
.	O
27.3.	O
latent	B
dirichlet	O
allocation	O
(	O
lda	O
)	O
951	O
1	O
p	O
(	O
word1	O
)	O
0	O
p	O
(	O
word3	O
)	O
1	O
=	O
topic	B
=	O
observed	O
document	O
=	O
generated	O
document	O
1	O
p	O
(	O
word2	O
)	O
figure	O
27.3	O
geometric	O
interpretation	O
of	O
lda	O
.	O
we	O
have	O
k	O
=	O
2	O
topics	O
and	O
v	O
=	O
3	O
words	O
.	O
each	O
document	O
(	O
white	O
dots	O
)	O
,	O
and	O
each	O
topic	B
(	O
black	O
dots	O
)	O
,	O
is	O
a	O
point	O
in	O
the	O
3d	O
simplex	O
.	O
source	O
:	O
figure	O
5	O
of	O
(	O
steyvers	O
and	O
griffiths	O
2007	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
tom	O
griffiths	O
.	O
direct	O
arc	O
from	O
πi	O
to	O
yil	O
,	O
with	O
the	O
following	O
cpd	O
:	O
p	O
(	O
yil	O
=	O
v|πi	O
)	O
=	O
p	O
(	O
yil	O
=	O
v|qil	O
=	O
k	O
)	O
p	O
(	O
qil	O
=	O
k	O
)	O
=	O
(	O
cid:4	O
)	O
k	O
(	O
cid:4	O
)	O
k	O
πikbkv	O
(	O
27.22	O
)	O
as	O
we	O
mentioned	O
in	O
the	O
introduction	O
,	O
this	O
is	O
very	O
similar	B
to	O
the	O
multinomial	B
pca	O
model	O
proposed	O
in	O
(	O
buntine	O
2002	O
)	O
,	O
which	O
in	O
turn	O
is	O
closely	O
related	O
to	O
categorical	B
pca	O
,	O
gap	O
,	O
nmf	O
,	O
etc	O
.	O
lda	O
has	O
an	O
interesting	O
geometric	O
interpretation	O
.	O
each	O
vector	O
bk	O
deﬁnes	O
a	O
distribution	O
over	O
v	O
words	O
;	O
each	O
k	O
is	O
known	O
as	O
a	O
topic	B
.	O
each	O
document	O
vector	O
πi	O
deﬁnes	O
a	O
distribution	O
over	O
k	O
topics	O
.	O
so	O
we	O
model	O
each	O
document	O
as	O
an	O
admixture	O
over	O
topics	O
.	O
equivalently	O
,	O
we	O
can	O
think	O
of	O
lda	O
as	O
a	O
form	O
of	O
dimensionality	B
reduction	I
(	O
assuming	O
k	O
<	O
v	O
,	O
as	O
is	O
usually	O
the	O
case	O
)	O
,	O
where	O
we	O
project	O
a	O
point	O
in	O
the	O
v	O
-dimensional	O
simplex	O
(	O
a	O
normalized	O
document	O
count	O
vector	O
xi	O
)	O
onto	O
the	O
k-dimensional	O
simplex	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
27.3	O
,	O
where	O
we	O
have	O
v	O
=	O
3	O
words	O
and	O
k	O
=	O
2	O
topics	O
.	O
the	O
observed	O
documents	O
(	O
which	O
live	O
in	O
the	O
3d	O
simplex	O
)	O
are	O
approximated	O
as	O
living	O
on	O
a	O
2d	O
simplex	O
spanned	O
by	O
the	O
2	O
topic	B
vectors	O
,	O
each	O
of	O
which	O
lives	O
in	O
the	O
3d	O
simplex	O
.	O
one	O
advantage	O
of	O
using	O
the	O
simplex	O
as	O
our	O
latent	B
space	O
rather	O
than	O
euclidean	O
space	O
is	O
that	O
the	O
simplex	O
can	O
handle	O
ambiguity	O
.	O
this	O
is	O
importance	O
since	O
in	O
natural	O
language	O
,	O
words	O
can	O
often	O
have	O
multiple	O
meanings	O
,	O
a	O
phenomomen	O
known	O
as	O
polysemy	B
.	O
for	O
example	O
,	O
“	O
play	O
”	O
might	O
refer	O
to	O
a	O
verb	O
(	O
e.g.	O
,	O
“	O
to	O
play	O
ball	O
”	O
or	O
“	O
to	O
play	O
the	O
coronet	O
”	O
)	O
,	O
or	O
to	O
a	O
noun	O
(	O
e.g.	O
,	O
“	O
shakespeare	O
’	O
s	O
play	O
”	O
)	O
.	O
in	O
lda	O
,	O
we	O
can	O
have	O
multiple	O
topics	O
,	O
each	O
of	O
which	O
can	O
generate	O
the	O
word	O
“	O
play	O
”	O
,	O
as	O
shown	O
in	O
figure	O
27.4	O
,	O
reﬂecting	O
this	O
ambiguity	O
.	O
given	O
word	O
l	O
in	O
document	O
i	O
,	O
we	O
can	O
compute	O
p	O
(	O
qil	O
=	O
k|yi	O
,	O
θ	O
)	O
,	O
and	O
thus	O
infer	O
its	O
most	O
likely	O
topic	B
.	O
by	O
looking	O
at	O
the	O
word	O
in	O
isolation	O
,	O
it	O
might	O
be	O
hard	O
to	O
know	O
what	O
sense	O
of	O
the	O
word	O
is	O
meant	O
,	O
but	O
we	O
can	O
disambiguate	O
this	O
by	O
looking	O
at	O
other	O
words	O
in	O
the	O
document	O
.	O
in	O
particular	O
,	O
given	O
xi	O
,	O
we	O
can	O
infer	O
the	O
topic	B
distribution	O
πi	O
for	O
the	O
document	O
;	O
this	O
acts	O
as	O
a	O
prior	O
for	O
disambiguating	O
qil	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
27.5	O
,	O
where	O
we	O
show	O
three	O
documents	O
from	O
the	O
tasa	O
corpus.2	O
in	O
the	O
ﬁrst	O
document	O
,	O
there	O
are	O
a	O
variety	O
of	O
music	O
related	O
words	O
,	O
which	O
suggest	O
2.	O
the	O
tasa	O
corpus	B
is	O
a	O
collection	O
of	O
37,000	O
high-school	O
level	O
english	O
documents	O
,	O
comprising	O
over	O
10	O
million	O
words	O
,	O
952	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
topic	B
77	O
topic	B
82	O
topic	B
166	O
word	O
prob	O
.	O
music	O
.090	O
dance	O
.034	O
song	O
.033	O
play	O
.030	O
sing	O
.026	O
singing	O
.026	O
band	O
.026	O
played	O
.023	O
sang	O
.022	O
songs	O
.021	O
dancing	O
.020	O
piano	O
.017	O
playing	O
.016	O
rhythm	O
.015	O
.013	O
albert	O
musical	O
.013	O
word	O
literature	O
prob	O
.	O
.031	O
poem	O
.028	O
poetry	O
.027	O
poet	O
.020	O
plays	O
.019	O
poems	O
.019	O
play	O
.015	O
literary	O
.013	O
.013	O
writers	O
drama	O
.012	O
.012	O
wrote	O
.011	O
poets	O
writer	O
.011	O
.010	O
written	O
.009	O
.009	O
stage	O
shakespeare	O
hit	O
word	O
prob	O
.	O
play	O
.136	O
.129	O
ball	O
game	O
.065	O
playing	O
.042	O
.032	O
played	O
.031	O
.027	O
baseball	O
.025	O
games	O
bat	O
.019	O
run	O
.019	O
throw	O
.016	O
.015	O
balls	O
tennis	O
.011	O
home	O
.010	O
catch	O
.010	O
field	O
.010	O
figure	O
27.4	O
three	O
topics	O
related	O
to	O
the	O
word	O
play	O
.	O
used	O
with	O
kind	O
permission	O
of	O
tom	O
griffiths	O
.	O
source	O
:	O
figure	O
9	O
of	O
(	O
steyvers	O
and	O
griffiths	O
2007	O
)	O
.	O
document	O
#	O
29795	O
bix	O
beiderbecke	O
,	O
at	O
age060	O
fifteen207	O
,	O
sat174	O
on	O
the	O
slope071	O
of	O
a	O
bluff055	O
overlooking027	O
the	O
mississippi137	O
river137	O
.	O
he	O
was	O
listening077	O
to	O
music077	O
coming009	O
from	O
a	O
passing043	O
riverboat	O
.	O
the	O
music077	O
had	O
already	O
captured006	O
his	O
heart157	O
as	O
well	O
as	O
his	O
ear119	O
.	O
it	O
was	O
jazz077	O
.	O
bix	O
beiderbecke	O
had	O
already	O
had	O
music077	O
lessons077	O
.	O
he	O
showed002	O
promise134	O
on	O
the	O
piano077	O
,	O
and	O
his	O
parents035	O
hoped268	O
he	O
might	O
consider118	O
becoming	O
a	O
concert077	O
pianist077	O
.	O
but	O
bix	O
was	O
interested268	O
in	O
another	O
kind050	O
of	O
music077	O
.	O
he	O
wanted268	O
to	O
play077	O
the	O
cornet	O
.	O
and	O
he	O
wanted268	O
to	O
play077	O
jazz077	O
...	O
document	O
#	O
1883	O
there	O
is	O
a	O
simple050	O
reason106	O
why	O
there	O
are	O
so	O
few	O
periods078	O
of	O
really	O
great	O
theater082	O
in	O
our	O
whole	O
western046	O
world	O
.	O
too	O
many	O
things300	O
have	O
to	O
come	O
right	O
at	O
the	O
very	O
same	O
time	O
.	O
the	O
dramatists	O
must	O
have	O
the	O
right	O
actors082	O
,	O
the	O
actors082	O
must	O
have	O
the	O
right	O
playhouses	O
,	O
the	O
playhouses	O
must	O
have	O
the	O
right	O
audiences082	O
.	O
we	O
must	O
remember288	O
that	O
plays082	O
exist143	O
to	O
be	O
performed077	O
,	O
not	O
merely050	O
to	O
be	O
read254	O
.	O
(	O
even	O
when	O
you	O
read254	O
a	O
play082	O
to	O
yourself	O
,	O
try288	O
to	O
perform062	O
it	O
,	O
to	O
put174	O
it	O
on	O
a	O
stage078	O
,	O
as	O
you	O
go	O
along	O
.	O
)	O
as	O
soon028	O
as	O
a	O
play082	O
has	O
to	O
be	O
performed082	O
,	O
then	O
some	O
kind126	O
of	O
theatrical082	O
...	O
document	O
#	O
21359	O
jim296	O
has	O
a	O
game166	O
book254	O
.	O
jim296	O
reads254	O
the	O
book254	O
.	O
jim296	O
sees081	O
a	O
game166	O
for	O
one	O
.	O
jim296	O
plays166	O
the	O
game166	O
.	O
jim296	O
likes081	O
the	O
game166	O
for	O
one	O
.	O
the	O
game166	O
book254	O
helps081	O
jim296	O
.	O
don180	O
comes040	O
into	O
the	O
house038	O
.	O
don180	O
and	O
jim296	O
read254	O
the	O
game166	O
book254	O
.	O
the	O
boys020	O
see	O
a	O
game166	O
for	O
two	O
.	O
the	O
two	O
boys020	O
play166	O
the	O
game166	O
.	O
the	O
boys020	O
play166	O
the	O
game166	O
for	O
two	O
.	O
the	O
boys020	O
like	O
the	O
game166	O
.	O
meg282	O
comes040	O
into	O
the	O
house282	O
.	O
meg282	O
and	O
don180	O
and	O
jim296	O
read254	O
the	O
book254	O
.	O
they	O
see	O
a	O
game166	O
for	O
three	O
.	O
meg282	O
and	O
don180	O
and	O
jim296	O
play166	O
the	O
game166	O
.	O
they	O
play166	O
...	O
figure	O
27.5	O
three	O
documents	O
from	O
the	O
tasa	O
corpus	B
containing	O
different	O
senses	O
of	O
the	O
word	O
play	O
.	O
grayed	O
out	O
words	O
were	O
ignored	O
by	O
the	O
model	O
,	O
because	O
they	O
correspond	O
to	O
uninteresting	O
stop	B
words	I
(	O
such	O
as	O
“	O
and	O
”	O
,	O
“	O
the	O
”	O
,	O
etc	O
.	O
)	O
or	O
very	O
low	O
frequency	O
words	O
.	O
source	O
:	O
figure	O
10	O
of	O
(	O
steyvers	O
and	O
griffiths	O
2007	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
tom	O
griffiths	O
.	O
27.3.	O
latent	B
dirichlet	O
allocation	O
(	O
lda	O
)	O
953	O
πi	O
will	O
put	O
most	O
of	O
its	O
mass	O
on	O
the	O
music	O
topic	B
(	O
number	O
77	O
)	O
;	O
this	O
in	O
turn	O
makes	O
the	O
music	O
interpretation	O
of	O
“	O
play	O
”	O
the	O
most	O
likely	O
,	O
as	O
shown	O
by	O
the	O
superscript	O
.	O
the	O
second	O
document	O
interprets	O
play	O
in	O
the	O
theatrical	O
sense	O
,	O
and	O
the	O
third	O
in	O
the	O
sports	O
sense	O
.	O
note	O
that	O
is	O
crucial	O
that	O
πi	O
be	O
a	O
latent	O
variable	O
,	O
so	O
information	B
can	O
ﬂow	O
between	O
the	O
qil	O
’	O
s	O
,	O
thus	O
enabling	O
local	O
disambiguation	O
to	O
use	O
the	O
full	B
set	O
of	O
words	O
.	O
27.3.2	O
unsupervised	O
discovery	O
of	O
topics	O
one	O
of	O
the	O
main	O
purposes	O
of	O
lda	O
is	O
discover	O
topics	O
in	O
a	O
large	O
collection	O
or	O
corpus	B
of	O
docu-	O
ments	O
(	O
see	O
figure	O
27.12	O
for	O
an	O
example	O
)	O
.	O
unfortunately	O
,	O
since	O
the	O
model	O
is	O
unidentiﬁable	B
,	O
the	O
interpertation	O
of	O
the	O
topics	O
can	O
be	O
difficult	O
(	O
chang	O
et	O
al	O
.	O
2009	O
)	O
..	O
one	O
approach	O
,	O
known	O
as	O
la-	O
beled	O
lda	O
(	O
ramage	O
et	O
al	O
.	O
2009	O
)	O
,	O
exploits	O
the	O
existence	O
of	O
tags	O
on	O
documents	O
as	O
a	O
way	O
to	O
ensure	O
identiﬁability	O
.	O
in	O
particular	O
,	O
it	O
forces	O
the	O
topics	O
to	O
correspond	O
to	O
the	O
tags	O
,	O
and	O
then	O
it	O
learns	O
a	O
distribution	O
over	O
words	O
for	O
each	O
tag	O
.	O
this	O
can	O
make	O
the	O
results	O
easier	O
to	O
interpret	O
.	O
27.3.3	O
quantitatively	O
evaluating	O
lda	O
as	O
a	O
language	B
model	I
in	O
order	O
to	O
evaluate	O
lda	O
quantitatively	O
,	O
we	O
can	O
treat	O
it	O
as	O
a	O
language	B
model	I
,	O
i.e.	O
,	O
a	O
probability	O
distribution	O
over	O
sequences	O
of	O
words	O
.	O
of	O
course	O
,	O
it	O
is	O
not	O
a	O
very	O
good	O
language	B
model	I
,	O
since	O
it	O
ignores	O
word	O
order	O
and	O
just	O
looks	O
at	O
single	O
words	O
(	O
unigrams	B
)	O
,	O
but	O
it	O
is	O
interesting	O
to	O
compare	O
lda	O
to	O
other	O
unigram-based	O
models	O
,	O
such	O
as	O
mixtures	O
of	O
multinoullis	O
,	O
and	O
plsi	O
.	O
such	O
simple	O
language	O
models	O
are	O
sometimes	O
useful	O
for	O
information	B
retrieval	I
purposes	O
.	O
the	O
standard	O
way	O
to	O
measure	O
the	O
quality	O
of	O
a	O
language	B
model	I
is	O
to	O
use	O
perplexity	B
,	O
which	O
we	O
now	O
deﬁne	O
below	O
.	O
27.3.3.1	O
perplexity	B
the	O
perplexity	B
of	O
language	B
model	I
q	O
given	O
a	O
stochastic	O
process3	O
p	O
is	O
deﬁned	O
as	O
perplexity	B
(	O
p	O
,	O
q	O
)	O
(	O
cid:2	O
)	O
2h	O
(	O
p	O
,	O
q	O
)	O
(	O
cid:4	O
)	O
h	O
(	O
p	O
,	O
q	O
)	O
(	O
cid:2	O
)	O
lim	O
n→∞	O
−	O
1	O
n	O
where	O
h	O
(	O
p	O
,	O
q	O
)	O
is	O
the	O
cross-entropy	B
of	O
the	O
two	O
stochastic	B
processes	I
,	O
deﬁned	O
as	O
p	O
(	O
y1	O
:	O
n	O
)	O
log	O
q	O
(	O
y1	O
:	O
n	O
)	O
y1	O
:	O
n	O
(	O
27.23	O
)	O
(	O
27.24	O
)	O
the	O
cross	B
entropy	I
(	O
and	O
hence	O
perplexity	B
)	O
is	O
minimized	O
if	O
q	O
=	O
p	O
;	O
in	O
this	O
case	O
,	O
the	O
model	O
can	O
predict	O
as	O
well	O
as	O
the	O
“	O
true	O
”	O
distribution	O
.	O
we	O
can	O
approximate	O
the	O
stochastic	B
process	I
by	O
using	O
a	O
single	O
long	O
test	O
sequence	O
(	O
composed	O
of	O
multiple	O
documents	O
and	O
multiple	O
sentences	O
,	O
complete	B
with	O
end-of-sentence	O
markers	O
)	O
,	O
call	O
it	O
y∗	O
(	O
this	O
approximation	O
becomes	O
more	O
and	O
more	O
accurate	O
as	O
the	O
sequence	O
gets	O
longer	O
,	O
provided	O
the	O
process	O
is	O
stationary	B
and	O
ergodic	B
(	O
cover	O
and	O
thomas	O
2006	O
)	O
.	O
)	O
deﬁne	O
the	O
empirical	B
distribution	I
(	O
an	O
approximation	O
to	O
the	O
stochastic	B
process	I
)	O
as	O
1	O
:	O
n	O
.	O
pemp	O
(	O
y1	O
:	O
n	O
)	O
=	O
δy∗	O
1	O
:	O
n	O
(	O
y1	O
:	O
n	O
)	O
(	O
27.25	O
)	O
collated	O
by	O
a	O
company	O
formerly	O
known	O
as	O
touchstone	O
applied	O
science	O
associates	O
,	O
but	O
now	O
known	O
as	O
questar	O
assessment	O
inc	O
www.questarai.com	O
.	O
3.	O
a	O
stochastic	B
process	I
is	O
one	O
which	O
can	O
deﬁne	O
a	O
joint	B
distribution	I
over	O
an	O
arbitrary	O
number	O
of	O
random	O
variables	O
.	O
we	O
can	O
think	O
of	O
natural	O
language	O
as	O
a	O
stochastic	B
process	I
,	O
since	O
it	O
can	O
generate	O
an	O
inﬁnite	O
stream	O
of	O
words	O
.	O
954	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
in	O
this	O
case	O
,	O
the	O
cross-entropy	B
becomes	O
h	O
(	O
pemp	O
,	O
q	O
)	O
=	O
−	O
1	O
n	O
log	O
q	O
(	O
y∗	O
1	O
:	O
n	O
)	O
and	O
the	O
perplexity	B
becomes	O
perplexity	B
(	O
pemp	O
,	O
q	O
)	O
=	O
2h	O
(	O
pemp	O
,	O
q	O
)	O
=	O
q	O
(	O
y∗	O
1	O
:	O
n	O
)	O
9	O
:	O
:	O
;	O
n	O
(	O
cid:20	O
)	O
−1/n	O
=	O
n	O
i=1	O
1	O
1	O
:	O
i−1	O
)	O
i	O
|y∗	O
q	O
(	O
y∗	O
(	O
27.26	O
)	O
(	O
27.27	O
)	O
we	O
see	O
that	O
this	O
is	O
the	O
geometric	O
mean	O
of	O
the	O
inverse	O
predictive	O
probabilities	O
,	O
which	O
is	O
the	O
usual	O
deﬁnition	O
of	O
perplexity	B
(	O
jurafsky	O
and	O
martin	O
2008	O
,	O
p96	O
)	O
.	O
in	O
the	O
case	O
of	O
unigram	O
models	O
,	O
the	O
cross	B
entropy	I
term	O
is	O
given	O
by	O
log	O
q	O
(	O
yil	O
)	O
(	O
27.28	O
)	O
n	O
(	O
cid:4	O
)	O
li	O
(	O
cid:4	O
)	O
1	O
li	O
i=1	O
l=1	O
h	O
=	O
−	O
1	O
n	O
where	O
n	O
is	O
the	O
number	O
of	O
documents	O
and	O
li	O
is	O
the	O
number	O
of	O
words	O
in	O
document	O
i.	O
hence	O
the	O
perplexity	B
of	O
model	O
q	O
is	O
given	O
by	O
perplexity	B
(	O
pemp	O
,	O
p	O
)	O
=	O
exp	O
log	O
q	O
(	O
yil	O
)	O
(	O
27.29	O
)	O
(	O
cid:10	O
)	O
n	O
(	O
cid:4	O
)	O
li	O
(	O
cid:4	O
)	O
1	O
li	O
i=1	O
l=1	O
−	O
1	O
n	O
(	O
cid:11	O
)	O
intuitively	O
,	O
perplexity	B
mesures	O
the	O
weighted	B
average	I
branching	O
factor	B
of	O
the	O
model	O
’	O
s	O
predic-	O
tive	O
distribution	O
.	O
suppose	O
the	O
model	O
predicts	O
that	O
each	O
symbol	O
(	O
letter	O
,	O
word	O
,	O
whatever	O
)	O
is	O
equally	O
likely	O
,	O
so	O
p	O
(	O
yi|y1	O
:	O
i−1	O
)	O
=	O
1/k	O
.	O
then	O
the	O
perplexity	B
is	O
(	O
(	O
1/k	O
)	O
n	O
)	O
−1/n	O
=	O
k.	O
if	O
some	O
symbols	O
are	O
more	O
likely	O
than	O
others	O
,	O
and	O
the	O
model	O
correctly	O
reﬂects	O
this	O
,	O
its	O
perplexity	B
will	O
be	O
lower	O
than	O
k.	O
of	O
course	O
,	O
h	O
(	O
p	O
,	O
p	O
)	O
=h	O
(	O
p	O
)	O
≤	O
h	O
(	O
p	O
,	O
q	O
)	O
,	O
so	O
we	O
can	O
never	O
reduce	O
the	O
perplexity	B
below	O
the	O
entropy	B
of	O
the	O
underlying	O
stochastic	B
process	I
.	O
27.3.3.2	O
perplexity	B
of	O
lda	O
the	O
key	O
quantity	O
is	O
p	O
(	O
v	O
)	O
,	O
the	O
predictive	B
distribution	O
of	O
the	O
model	O
over	O
possible	O
words	O
.	O
(	O
it	O
is	O
implicitly	O
conditioned	O
on	O
the	O
training	B
set	I
.	O
)	O
for	O
lda	O
,	O
this	O
can	O
be	O
approximated	O
by	O
plugging	O
in	O
b	O
(	O
e.g.	O
,	O
the	O
posterior	B
mean	I
estimate	O
)	O
and	O
approximately	O
integrating	O
out	O
q	O
using	O
mean	B
ﬁeld	I
inference	O
(	O
see	O
(	O
wallach	O
et	O
al	O
.	O
2009	O
)	O
for	O
a	O
more	O
accurate	O
way	O
to	O
approximate	O
the	O
predictive	B
likelihood	O
)	O
.	O
in	O
figure	O
27.6	O
,	O
we	O
compare	O
lda	O
to	O
several	O
other	O
simple	O
unigram	O
models	O
,	O
namely	O
map	O
estima-	O
tion	O
of	O
a	O
multinoulli	O
,	O
map	O
estimation	O
of	O
a	O
mixture	O
of	O
multinoullis	O
,	O
and	O
plsi	O
.	O
(	O
when	O
performing	O
map	O
estimation	O
,	O
the	O
same	O
dirichlet	O
prior	O
on	O
b	O
was	O
used	O
as	O
in	O
the	O
lda	O
model	O
.	O
)	O
the	O
metric	B
is	O
perplexity	B
,	O
as	O
in	O
equation	O
27.29	O
,	O
and	O
the	O
data	O
is	O
a	O
subset	O
of	O
the	O
trec	O
ap	O
corpus	B
containing	O
16,333	O
newswire	O
articles	O
with	O
23,075	O
unique	O
terms	O
.	O
we	O
see	O
that	O
lda	O
signiﬁcantly	O
outperforms	O
these	O
other	O
methods	O
.	O
27.3.	O
latent	B
dirichlet	O
allocation	O
(	O
lda	O
)	O
955	O
unigram	O
mixtures	O
of	O
unigrams	B
lda	O
fold	O
in	O
plsi	O
7000	O
6500	O
6000	O
5500	O
5000	O
4500	O
4000	O
3500	O
3000	O
l	O
y	O
t	O
i	O
x	O
e	O
p	O
r	O
e	O
p	O
2500	O
0	O
20	O
40	O
60	O
100	O
80	O
120	O
number	O
of	O
topics	O
140	O
160	O
180	O
200	O
figure	O
27.6	O
perplexity	B
vs	O
number	O
of	O
topics	O
on	O
the	O
trec	O
ap	O
corpus	B
for	O
various	O
language	B
models	I
.	O
based	O
on	O
figure	O
9	O
of	O
(	O
blei	O
et	O
al	O
.	O
2003	O
)	O
.	O
figure	O
generated	O
by	O
bleildaperplexityplot	O
.	O
α	O
α	O
π1	O
.	O
.	O
.	O
.	O
.	O
.	O
πn	O
q1,1	O
y1,1	O
q1	O
,	O
l1	O
.	O
.	O
.	O
y1	O
,	O
l1	O
.	O
.	O
.	O
qn,1	O
.	O
.	O
.	O
qn	O
,	O
ln	O
.	O
.	O
.	O
yn,1	O
.	O
.	O
.	O
yn	O
,	O
ln	O
.	O
.	O
.	O
q1,1	O
y1,1	O
q1	O
,	O
l1	O
.	O
.	O
.	O
y1	O
,	O
l1	O
.	O
.	O
.	O
qn,1	O
.	O
.	O
.	O
qn	O
,	O
ln	O
.	O
.	O
.	O
yn,1	O
.	O
.	O
.	O
yn	O
,	O
ln	O
.	O
.	O
.	O
b1	O
.	O
.	O
.b	O
k	O
γ	O
(	O
a	O
)	O
γ	O
(	O
b	O
)	O
figure	O
27.7	O
the	O
bk	O
.	O
(	O
a	O
)	O
lda	O
unrolled	B
for	O
n	O
documents	O
.	O
(	O
b	O
)	O
collapsed	O
lda	O
,	O
where	O
we	O
integrate	B
out	I
the	O
πi	O
and	O
27.3.4	O
fitting	O
using	O
(	O
collapsed	O
)	O
gibbs	O
sampling	O
it	O
is	O
straightforward	O
to	O
derive	O
a	O
gibbs	O
sampling	O
algorithm	O
for	O
lda	O
.	O
the	O
full	B
conditionals	O
are	O
as	O
follows	O
:	O
p	O
(	O
qil	O
=	O
k|·	O
)	O
∝	O
exp	O
[	O
log	O
πik	O
+	O
log	O
bk	O
,	O
xil	O
]	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
l	O
p	O
(	O
πi|·	O
)	O
=	O
dir	O
(	O
{	O
αk	O
+	O
p	O
(	O
bk|·	O
)	O
=	O
dir	O
(	O
{	O
γv	O
+	O
i	O
(	O
zil	O
=	O
k	O
)	O
}	O
)	O
(	O
cid:4	O
)	O
i	O
(	O
xil	O
=	O
v	O
,	O
zil	O
=	O
k	O
)	O
}	O
)	O
(	O
27.30	O
)	O
(	O
27.31	O
)	O
(	O
27.32	O
)	O
however	O
,	O
one	O
can	O
get	O
better	O
performance	O
by	O
analytically	O
integrating	O
out	O
the	O
πi	O
’	O
s	O
and	O
the	O
bk	O
’	O
s	O
,	O
i	O
l	O
956	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
both	O
of	O
which	O
have	O
a	O
dirichlet	O
distribution	O
,	O
and	O
just	O
sampling	O
the	O
discrete	B
qil	O
’	O
s	O
.	O
this	O
approach	O
was	O
ﬁrst	O
suggested	O
in	O
(	O
griffiths	O
and	O
steyvers	O
2004	O
)	O
,	O
and	O
is	O
an	O
example	O
of	O
collapsed	O
gibbs	O
sampling	O
.	O
figure	O
27.7	O
(	O
b	O
)	O
shows	O
that	O
now	O
all	O
the	O
qil	O
variables	O
are	O
fully	O
correlated	O
.	O
however	O
,	O
we	O
can	O
sample	O
them	O
one	O
at	O
a	O
time	O
,	O
as	O
we	O
explain	O
below	O
.	O
first	O
,	O
we	O
need	O
some	O
notation	O
.	O
let	O
civk	O
=	O
word	O
v	O
is	O
assigned	O
to	O
topic	B
k	O
in	O
document	O
i.	O
let	O
cik	O
=	O
word	O
from	O
document	O
i	O
has	O
been	O
assigned	O
to	O
topic	B
k.	O
let	O
cvk	O
=	O
word	O
v	O
has	O
been	O
assigned	O
to	O
topic	B
k	O
in	O
any	O
document	O
.	O
let	O
niv	O
=	O
times	O
word	O
v	O
occurs	O
in	O
document	O
i	O
;	O
this	O
is	O
observed	O
.	O
let	O
ck	O
=	O
assigned	O
to	O
topic	B
k.	O
finally	O
,	O
let	O
li	O
=	O
observed	O
.	O
l=1	O
i	O
(	O
qil	O
=	O
k	O
,	O
yil	O
=	O
v	O
)	O
be	O
the	O
number	O
of	O
times	O
v	O
civk	O
be	O
the	O
number	O
of	O
times	O
any	O
i	O
civk	O
be	O
the	O
number	O
of	O
times	O
k	O
civk	O
be	O
the	O
number	O
of	O
v	O
cvk	O
be	O
the	O
number	O
of	O
words	O
k	O
cik	O
be	O
the	O
number	O
of	O
words	O
in	O
document	O
i	O
;	O
this	O
is	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
(	O
cid:7	O
)	O
li	O
(	O
cid:7	O
)	O
we	O
can	O
now	O
derive	O
the	O
marginal	O
prior	O
.	O
by	O
applying	O
equation	O
5.24	O
,	O
one	O
can	O
show	O
that	O
p	O
(	O
q|α	O
)	O
=	O
dir	O
(	O
πi|α1k	O
)	O
dπi	O
(	O
cid:7	O
)	O
(	O
cid:25	O
)	O
(	O
27.33	O
)	O
(	O
27.34	O
)	O
(	O
27.35	O
)	O
(	O
27.36	O
)	O
(	O
cid:12	O
)	O
(	O
cid:24	O
)	O
(	O
cid:20	O
)	O
(	O
cid:8	O
)	O
i	O
l=1	O
li	O
(	O
cid:20	O
)	O
cat	O
(	O
qil|πi	O
)	O
(	O
cid:26	O
)	O
k	O
(	O
cid:9	O
)	O
n	O
n	O
(	O
cid:20	O
)	O
(	O
cid:12	O
)	O
⎡	O
⎣	O
(	O
cid:20	O
)	O
(	O
cid:9	O
)	O
k	O
k	O
(	O
cid:20	O
)	O
il	O
:	O
qil=k	O
i=1	O
(	O
cid:20	O
)	O
(	O
cid:8	O
)	O
k	O
cat	O
(	O
yil|bk	O
)	O
(	O
cid:26	O
)	O
v	O
γ	O
(	O
v	O
β	O
)	O
γ	O
(	O
β	O
)	O
v	O
v=1	O
γ	O
(	O
cvk	O
+	O
β	O
)	O
γ	O
(	O
ck	O
+	O
v	O
β	O
)	O
k=1	O
p	O
(	O
y|q	O
,	O
γ	O
)	O
=	O
=	O
=	O
γ	O
(	O
kα	O
)	O
γ	O
(	O
α	O
)	O
k	O
k=1	O
γ	O
(	O
cik	O
+	O
α	O
)	O
γ	O
(	O
li	O
+	O
kα	O
)	O
by	O
similar	B
reasoning	O
,	O
one	O
can	O
show	O
⎤	O
⎦	O
dir	O
(	O
bk|γ1v	O
)	O
dbk	O
from	O
the	O
above	O
equations	O
,	O
and	O
using	O
the	O
fact	O
that	O
γ	O
(	O
x	O
+	O
1	O
)	O
/γ	O
(	O
x	O
)	O
=	O
x	O
,	O
we	O
can	O
derive	O
the	O
full	B
conditional	I
for	O
p	O
(	O
qil|q−i	O
,	O
l	O
)	O
.	O
deﬁne	O
c−	O
ivk	O
to	O
be	O
the	O
same	O
as	O
civk	O
except	O
it	O
is	O
compute	O
by	O
summing	O
over	O
all	O
locations	O
in	O
document	O
i	O
except	O
for	O
qil	O
.	O
also	O
,	O
let	O
yil	O
=	O
v.	O
then	O
p	O
(	O
qi	O
,	O
l	O
=	O
k|q−i	O
,	O
l	O
,	O
y	O
,	O
α	O
,	O
γ	O
)	O
∝	O
c−	O
v	O
,	O
k	O
+	O
γ	O
c−	O
k	O
+	O
v	O
γ	O
c−	O
i	O
,	O
k	O
+	O
α	O
li	O
+	O
kα	O
(	O
27.37	O
)	O
we	O
see	O
that	O
a	O
word	O
in	O
a	O
document	O
is	O
assigned	O
to	O
a	O
topic	B
based	O
both	O
on	O
how	O
often	O
that	O
word	O
is	O
generated	O
by	O
the	O
topic	B
(	O
ﬁrst	O
term	O
)	O
,	O
and	O
also	O
on	O
how	O
often	O
that	O
topic	B
is	O
used	O
in	O
that	O
document	O
(	O
second	O
term	O
)	O
.	O
given	O
equation	O
27.37	O
,	O
we	O
can	O
implement	O
the	O
collapsed	O
gibbs	O
sampler	O
as	O
follows	O
.	O
we	O
randomly	O
assign	O
a	O
topic	B
to	O
each	O
word	O
,	O
qil	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
.	O
we	O
can	O
then	O
sample	O
a	O
new	O
topic	B
as	O
follows	O
:	O
for	O
a	O
given	O
word	O
in	O
the	O
corpus	B
,	O
decrement	O
the	O
relevant	O
counts	O
,	O
based	O
on	O
the	O
topic	B
assigned	O
to	O
the	O
current	O
word	O
;	O
draw	O
a	O
new	O
topic	B
from	O
equation	O
27.37	O
,	O
update	O
the	O
count	O
matrices	O
;	O
and	O
repeat	O
.	O
this	O
algorithm	O
can	O
be	O
made	O
efficient	O
since	O
the	O
count	O
matrices	O
are	O
very	O
sparse	B
.	O
27.3.5	O
example	O
this	O
process	O
is	O
illustrated	O
in	O
figure	O
27.8	O
on	O
a	O
small	O
example	O
with	O
two	O
topics	O
,	O
and	O
ﬁve	O
words	O
.	O
the	O
left	O
part	O
of	O
the	O
ﬁgure	O
illustrates	O
16	O
documents	O
that	O
were	O
sampled	O
from	O
the	O
lda	O
model	O
using	O
27.3.	O
latent	B
dirichlet	O
allocation	O
(	O
lda	O
)	O
957	O
river	O
stream	O
bank	O
money	O
loan	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
13	O
14	O
15	O
16	O
river	O
stream	O
bank	O
money	O
loan	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
13	O
14	O
15	O
16	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
27.8	O
illustration	O
of	O
(	O
collapsed	O
)	O
gibbs	O
sampling	O
applied	O
to	O
a	O
small	O
lda	O
example	O
.	O
there	O
are	O
n	O
=	O
16	O
documents	O
,	O
each	O
containing	O
a	O
variable	O
number	O
of	O
words	O
drawn	O
from	O
a	O
vocabulary	O
of	O
v	O
=	O
5	O
words	O
,	O
there	O
are	O
two	O
topics	O
.	O
a	O
white	O
dot	O
means	O
word	O
the	O
word	O
is	O
assigned	O
to	O
topic	B
1	O
,	O
a	O
black	O
dot	O
means	O
the	O
word	O
is	O
assigned	O
to	O
topic	B
2	O
.	O
(	O
b	O
)	O
a	O
sample	O
from	O
the	O
posterior	O
after	O
64	O
steps	O
of	O
gibbs	O
sampling	O
.	O
source	O
:	O
figure	O
7	O
of	O
(	O
steyvers	O
and	O
griffiths	O
2007	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
tom	O
griffiths	O
.	O
(	O
a	O
)	O
the	O
initial	O
random	O
assignment	O
of	O
states	O
.	O
p	O
(	O
money|k	O
=	O
1	O
)	O
=p	O
(	O
loan|	O
k	O
=	O
1	O
)	O
=p	O
(	O
bank|	O
k	O
=	O
1	O
)	O
=	O
1/3	O
and	O
p	O
(	O
river|k	O
=	O
2	O
)	O
=p	O
(	O
stream|	O
k	O
=	O
2	O
)	O
=	O
p	O
(	O
bank|k	O
=	O
2	O
)	O
=	O
1/3	O
.	O
for	O
example	O
,	O
we	O
see	O
that	O
the	O
ﬁrst	O
document	O
contains	O
the	O
word	O
“	O
bank	O
”	O
4	O
times	O
(	O
indicated	O
by	O
the	O
four	O
dots	O
in	O
row	O
1	O
of	O
the	O
“	O
bank	O
”	O
column	O
)	O
,	O
as	O
well	O
as	O
various	O
other	O
ﬁnancial	O
terms	O
.	O
the	O
right	O
part	O
of	O
the	O
ﬁgure	O
shows	O
the	O
state	B
of	O
the	O
gibbs	O
sampler	O
after	O
64	O
iterations	O
.	O
the	O
“	O
correct	O
”	O
topic	B
has	O
been	O
assigned	O
to	O
each	O
token	O
in	O
most	O
cases	O
.	O
for	O
example	O
,	O
in	O
document	O
1	O
,	O
we	O
see	O
that	O
the	O
word	O
“	O
bank	O
”	O
has	O
been	O
correctly	O
assigned	O
to	O
the	O
ﬁnancial	O
topic	B
,	O
based	O
on	O
the	O
presence	O
of	O
the	O
words	O
“	O
money	O
”	O
and	O
“	O
loan	O
”	O
.	O
the	O
posterior	B
mean	I
estimate	O
of	O
the	O
parameters	O
is	O
given	O
by	O
ˆp	O
(	O
money|k	O
=	O
1	O
)	O
=	O
0.32	O
,	O
ˆp	O
(	O
loan|k	O
=	O
1	O
)	O
=	O
0.29	O
,	O
ˆp	O
(	O
bank|k	O
=	O
1	O
)	O
=	O
0.39	O
,	O
ˆp	O
(	O
river|k	O
=	O
2	O
)	O
=	O
0.25	O
,	O
ˆp	O
(	O
stream|k	O
=	O
2	O
)	O
=	O
0.4	O
,	O
and	O
ˆp	O
(	O
bank|k	O
=	O
2	O
)	O
=	O
0.35	O
,	O
which	O
is	O
impressively	O
accurate	O
,	O
given	O
that	O
there	O
are	O
only	O
16	O
training	O
examples	O
.	O
27.3.6	O
fitting	O
using	O
batch	B
variational	O
inference	B
a	O
faster	O
alternative	O
to	O
mcmc	O
is	O
to	O
use	O
variational	O
em	O
.	O
(	O
we	O
can	O
not	O
use	O
exact	O
em	O
since	O
exact	O
inference	B
of	O
πi	O
and	O
qi	O
is	O
intractable	O
.	O
)	O
we	O
give	O
the	O
details	O
below	O
.	O
27.3.6.1	O
sequence	O
version	O
(	O
cid:20	O
)	O
l	O
following	O
(	O
blei	O
et	O
al	O
.	O
2003	O
)	O
,	O
we	O
will	O
use	O
a	O
fully	O
factorized	O
(	O
mean	B
ﬁeld	I
)	O
approximation	O
of	O
the	O
form	O
q	O
(	O
πi	O
,	O
qi	O
)	O
=	O
dir	O
(	O
πi|	O
˜πi	O
)	O
cat	O
(	O
qil|˜qil	O
)	O
(	O
27.38	O
)	O
we	O
will	O
follow	O
the	O
usual	O
mean	B
ﬁeld	I
recipe	O
.	O
for	O
q	O
(	O
qil	O
)	O
,	O
we	O
use	O
bayes	O
’	O
rule	O
,	O
but	O
where	O
we	O
need	O
to	O
take	O
expectations	O
over	O
the	O
prior	O
:	O
˜qilk	O
∝	O
byi	O
,	O
l	O
,	O
k	O
exp	O
(	O
e	O
[	O
log	O
πik	O
]	O
)	O
where	O
e	O
[	O
log	O
πik	O
]	O
=	O
ψk	O
(	O
˜πi	O
.	O
)	O
(	O
cid:2	O
)	O
ψ	O
(	O
˜πik	O
)	O
−	O
ψ	O
(	O
(	O
cid:4	O
)	O
k	O
(	O
cid:2	O
)	O
˜πik	O
(	O
cid:2	O
)	O
)	O
(	O
27.39	O
)	O
(	O
27.40	O
)	O
958	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
where	O
ψ	O
is	O
the	O
digamma	B
function	O
.	O
the	O
update	O
for	O
q	O
(	O
πi	O
)	O
is	O
obtained	O
by	O
adding	O
up	O
the	O
expected	O
counts	O
:	O
the	O
m	O
step	O
is	O
obtained	O
by	O
adding	O
up	O
the	O
expected	O
counts	O
and	O
normalizing	O
:	O
˜πik	O
=	O
αk	O
+	O
˜qilk	O
(	O
cid:4	O
)	O
l	O
n	O
(	O
cid:4	O
)	O
li	O
(	O
cid:4	O
)	O
ˆbvk	O
∝	O
γv	O
+	O
˜qilki	O
(	O
yil	O
=	O
v	O
)	O
i=1	O
l=1	O
27.3.6.2	O
count	O
version	O
(	O
cid:7	O
)	O
(	O
27.41	O
)	O
(	O
27.42	O
)	O
(	O
27.43	O
)	O
(	O
27.44	O
)	O
(	O
27.45	O
)	O
(	O
27.46	O
)	O
i	O
li	O
)	O
v	O
k	O
)	O
space	O
to	O
store	O
the	O
˜qilk	O
.	O
note	O
that	O
the	O
e	O
step	O
takes	O
o	O
(	O
(	O
it	O
is	O
much	O
more	O
space	O
efficient	O
to	O
perform	O
inference	B
in	O
the	O
mpca	O
version	O
of	O
the	O
model	O
,	O
which	O
works	O
with	O
counts	O
;	O
these	O
only	O
take	O
o	O
(	O
n	O
v	O
k	O
)	O
space	O
,	O
which	O
is	O
a	O
big	O
savings	O
if	O
documents	O
are	O
long	O
.	O
(	O
by	O
contrast	O
,	O
the	O
collapsed	O
gibbs	O
sampler	O
must	O
work	O
explicitly	O
with	O
the	O
qil	O
variables	O
.	O
)	O
we	O
will	O
focus	O
on	O
approximating	O
p	O
(	O
πi	O
,	O
ci|ni	O
,	O
li	O
)	O
,	O
where	O
we	O
write	O
ci	O
as	O
shorthand	O
for	O
ci	O
...	O
we	O
will	O
again	O
use	O
a	O
fully	O
factorized	O
(	O
mean	B
ﬁeld	I
)	O
approximation	O
of	O
the	O
form	O
(	O
cid:20	O
)	O
v	O
mu	O
(	O
civ.|niv	O
,	O
˜civ	O
.	O
)	O
q	O
(	O
πi	O
,	O
ci	O
)	O
=	O
dir	O
(	O
πi|	O
˜πi	O
)	O
(	O
cid:4	O
)	O
the	O
new	O
e	O
step	O
becomes	O
˜πik	O
=	O
αk	O
+	O
˜civk	O
∝	O
bvk	O
exp	O
(	O
e	O
[	O
log	O
πik	O
]	O
)	O
niv˜civk	O
v	O
the	O
new	O
m	O
step	O
becomes	O
ˆbvk	O
∝	O
γv	O
+	O
niv˜civk	O
(	O
cid:4	O
)	O
i	O
27.3.6.3	O
vb	O
version	O
we	O
now	O
modify	O
the	O
algorithm	O
to	O
use	O
vb	O
instead	O
of	O
em	O
,	O
so	O
that	O
we	O
infer	O
the	O
parameters	O
as	O
well	O
as	O
the	O
latent	B
variables	O
.	O
there	O
are	O
two	O
advantages	O
to	O
this	O
.	O
first	O
,	O
by	O
setting	O
γ	O
(	O
cid:22	O
)	O
1	O
,	O
vb	O
will	O
encourage	O
b	O
to	O
be	O
sparse	B
(	O
as	O
in	O
section	O
21.6.1.6	O
)	O
.	O
second	O
,	O
we	O
will	O
be	O
able	O
to	O
generalize	B
this	O
to	O
the	O
online	B
learning	I
setting	O
,	O
as	O
we	O
discuss	O
below	O
.	O
(	O
cid:20	O
)	O
our	O
new	O
posterior	O
approximation	O
becomes	O
q	O
(	O
πi	O
,	O
ci	O
,	O
b	O
)	O
=	O
dir	O
(	O
πi|	O
˜πi	O
)	O
mu	O
(	O
civ.|niv	O
,	O
˜civ	O
.	O
)	O
v	O
k	O
the	O
update	O
for	O
˜civk	O
changes	O
,	O
to	O
the	O
following	O
:	O
˜civk	O
∝	O
exp	O
(	O
e	O
[	O
log	O
bvk	O
]	O
+e	O
[	O
log	O
πik	O
]	O
)	O
(	O
cid:20	O
)	O
dir	O
(	O
b.k|˜b.k	O
)	O
(	O
27.47	O
)	O
(	O
27.48	O
)	O
27.3.	O
latent	B
dirichlet	O
allocation	O
(	O
lda	O
)	O
959	O
algorithm	O
27.1	O
:	O
batch	B
vb	O
for	O
lda	O
1	O
input	O
:	O
niv	O
,	O
k	O
,	O
αk	O
,	O
γv	O
;	O
2	O
estimate	O
˜bvk	O
using	O
em	O
for	O
multinomial	B
mixtures	O
;	O
3	O
initialize	O
counts	O
niv	O
;	O
4	O
while	O
not	O
converged	O
do	O
5	O
//	O
e	O
step	O
;	O
svk	O
=	O
0	O
//	O
expected	B
sufficient	I
statistics	I
;	O
for	O
each	O
document	O
i	O
=	O
1	O
:	O
n	O
do	O
(	O
˜πi	O
,	O
˜ci	O
)	O
=	O
estep	O
(	O
ni	O
,	O
˜b	O
,	O
α	O
)	O
;	O
svk+	O
=	O
niv˜civk	O
;	O
//	O
m	O
step	O
;	O
for	O
each	O
topic	B
k	O
=	O
1	O
:	O
k	O
do	O
˜bvk	O
=	O
γv	O
+	O
svk	O
;	O
13	O
function	O
(	O
˜πi	O
,	O
˜ci	O
)	O
=	O
estep	O
(	O
ni	O
,	O
˜b	O
,	O
α	O
)	O
;	O
14	O
initialize	O
˜πik	O
=	O
αk	O
;	O
15	O
repeat	O
˜πold	O
i	O
.	O
=	O
˜πi.	O
,	O
˜πik	O
=	O
αk	O
;	O
16	O
for	O
each	O
word	O
v	O
=	O
1	O
:	O
v	O
do	O
17	O
for	O
each	O
topic	B
k	O
=	O
1	O
:	O
k	O
do	O
!	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
18	O
19	O
20	O
#	O
;	O
˜civk	O
=	O
exp	O
ψk	O
(	O
˜bv	O
.	O
)	O
+ψ	O
k	O
(	O
˜πold	O
i	O
.	O
)	O
(	O
cid:7	O
)	O
˜civ	O
.	O
=	O
normalize	O
(	O
˜civ	O
.	O
)	O
;	O
˜πik+	O
=	O
niv˜civk	O
k	O
|˜πik	O
−	O
˜πold	O
ik	O
|	O
<	O
thresh	O
;	O
21	O
22	O
until	O
1	O
k	O
also	O
,	O
the	O
m	O
step	O
becomes	O
(	O
cid:4	O
)	O
˜bvk	O
=	O
γv	O
+	O
˜civk	O
(	O
27.49	O
)	O
i	O
no	O
normalization	O
is	O
required	O
,	O
since	O
we	O
are	O
just	O
updating	O
the	O
pseudcounts	O
.	O
the	O
overall	O
algorithm	O
is	O
summarized	O
in	O
algorithm	O
22	O
.	O
27.3.7	O
fitting	O
using	O
online	O
variational	O
inference	B
in	O
the	O
bathc	O
version	O
,	O
the	O
e	O
step	O
clearly	O
takes	O
o	O
(	O
n	O
kv	O
t	O
)	O
time	O
,	O
where	O
t	O
is	O
the	O
number	O
of	O
mean	B
ﬁeld	I
updates	O
(	O
typically	O
t	O
∼	O
5	O
)	O
.	O
this	O
can	O
be	O
slow	O
if	O
we	O
have	O
many	O
documents	O
.	O
this	O
can	O
be	O
reduced	O
by	O
using	O
stochastic	B
gradient	I
descent	I
(	O
section	O
8.5.2	O
)	O
to	O
perform	O
online	O
variational	O
inference	B
,	O
as	O
we	O
now	O
explain	O
.	O
we	O
can	O
derive	O
an	O
online	O
version	O
,	O
following	O
(	O
hoffman	O
et	O
al	O
.	O
2010	O
)	O
.	O
we	O
perform	O
an	O
e	O
step	O
in	O
the	O
usual	O
way	O
.	O
we	O
then	O
compute	O
the	O
variational	O
parameters	O
for	O
b	O
treating	O
the	O
expected	B
sufficient	I
statistics	I
from	O
the	O
single	O
data	O
case	O
as	O
if	O
the	O
whole	O
data	O
set	O
had	O
those	O
statistics	O
.	O
finally	O
,	O
we	O
make	O
960	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
algorithm	O
27.2	O
:	O
online	O
variational	O
bayes	O
for	O
lda	O
1	O
input	O
:	O
niv	O
,	O
k	O
,	O
αk	O
,	O
γv	O
,	O
τ0	O
,	O
κ	O
;	O
2	O
initialize	O
˜bvk	O
randomly	O
;	O
3	O
for	O
t	O
=	O
1	O
:	O
∞	O
do	O
4	O
5	O
6	O
7	O
8	O
−κ	O
;	O
set	O
step	B
size	I
ρt	O
=	O
(	O
τ0	O
+	O
t	O
)	O
pick	O
document	O
i	O
=	O
i	O
(	O
t	O
)	O
;	O
;	O
(	O
˜πi	O
,	O
˜ci	O
)	O
=	O
estep	O
(	O
ni	O
,	O
˜b	O
,	O
α	O
)	O
;	O
˜bnew	O
vk	O
=	O
γv	O
+	O
n	O
niv˜civk	O
;	O
˜bvk	O
=	O
(	O
1	O
−	O
ρt	O
)	O
˜bvk	O
+	O
ρt˜bnew	O
vk	O
;	O
900	O
850	O
800	O
750	O
700	O
650	O
600	O
l	O
y	O
t	O
i	O
x	O
e	O
p	O
r	O
e	O
p	O
online	O
98k	O
online	O
3.3m	O
103.5	O
batch	B
98k	O
104	O
documents	O
seen	O
(	O
log	O
scale	O
)	O
104.5	O
105	O
105.5	O
106	O
106.5	O
figure	O
27.9	O
test	O
perplexity	O
vs	O
number	O
of	O
training	O
documents	O
for	O
batch	B
and	O
online	O
vb-lda	O
.	O
from	O
figure	O
1	O
of	O
(	O
hoffman	O
et	O
al	O
.	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
david	O
blei	O
.	O
a	O
partial	O
update	O
for	O
the	O
variational	O
parameters	O
for	O
b	O
,	O
putting	O
weight	O
ρt	O
on	O
the	O
new	O
estimate	O
and	O
weight	O
1	O
−	O
ρt	O
on	O
the	O
old	O
estimate	O
.	O
the	O
step	B
size	I
ρt	O
decays	O
over	O
time	O
,	O
as	O
in	O
equation	O
8.83.	O
in	O
practice	O
,	O
we	O
should	O
use	O
mini-batches	O
,	O
the	O
overall	O
algorithm	O
is	O
summarized	O
in	O
algorithm	O
3.	O
as	O
explained	O
in	O
section	O
8.5.2.3.	O
in	O
(	O
hoffman	O
et	O
al	O
.	O
2010	O
)	O
,	O
they	O
used	O
a	O
batch	B
of	O
size	O
256–4096	O
.	O
figure	O
27.9	O
plots	O
the	O
perplexity	B
on	O
a	O
test	O
set	O
of	O
size	O
1000	O
vs	O
number	O
of	O
analyzed	O
documents	O
(	O
e	O
steps	O
)	O
,	O
where	O
the	O
data	O
is	O
drawn	O
from	O
(	O
english	O
)	O
wikipedia	O
.	O
the	O
ﬁgure	O
shows	O
that	O
online	O
variational	O
inference	B
is	O
much	O
faster	O
than	O
offline	B
inference	O
,	O
yet	O
produces	O
similar	B
results	O
.	O
27.3.8	O
determining	O
the	O
number	O
of	O
topics	O
choosing	O
k	O
,	O
the	O
number	O
of	O
topics	O
,	O
is	O
a	O
standard	B
model	I
selection	O
problem	O
.	O
here	O
are	O
some	O
approaches	O
that	O
have	O
been	O
taken	O
:	O
•	O
use	O
annealed	B
importance	I
sampling	I
(	O
section	O
24.6.2	O
)	O
to	O
approximate	O
the	O
evidence	B
(	O
wallach	O
et	O
al	O
.	O
2009	O
)	O
.	O
•	O
cross	B
validation	I
,	O
using	O
the	O
log	O
likelihood	O
on	O
a	O
test	O
set	O
.	O
27.4.	O
extensions	O
of	O
lda	O
961	O
•	O
use	O
the	O
variational	O
lower	O
bound	O
as	O
a	O
proxy	O
for	O
log	O
p	O
(	O
d|k	O
)	O
.	O
•	O
use	O
non-parametric	O
bayesian	O
methods	O
(	O
teh	O
et	O
al	O
.	O
2006	O
)	O
.	O
27.4	O
extensions	O
of	O
lda	O
many	O
extensions	O
of	O
lda	O
have	O
been	O
proposed	O
since	O
the	O
ﬁrst	O
paper	O
came	O
out	O
in	O
2003.	O
we	O
brieﬂy	O
discuss	O
a	O
few	O
of	O
these	O
below	O
.	O
27.4.1	O
correlated	B
topic	I
model	I
one	O
weakness	O
of	O
lda	O
is	O
that	O
it	O
can	O
not	O
capture	O
correlation	O
between	O
topics	O
.	O
for	O
example	O
,	O
if	O
a	O
document	O
has	O
the	O
“	O
business	O
”	O
topic	B
,	O
it	O
is	O
reasonable	O
to	O
expect	O
the	O
“	O
ﬁnance	O
”	O
topic	B
to	O
co-occcur	O
.	O
the	O
source	O
of	O
the	O
problem	O
is	O
the	O
use	O
of	O
a	O
dirichlet	O
prior	O
for	O
πi	O
.	O
the	O
problem	O
with	O
the	O
dirichelt	O
it	O
that	O
it	O
is	O
characterized	O
by	O
just	O
a	O
mean	B
vector	O
and	O
a	O
strength	O
parameter	B
,	O
but	O
its	O
covariance	B
is	O
ﬁxed	O
(	O
σij	O
=	O
−αiαj	O
)	O
,	O
rather	O
than	O
being	O
a	O
free	O
parameter	O
.	O
one	O
way	O
around	O
this	O
is	O
to	O
replace	O
the	O
dirichlet	O
prior	O
with	O
the	O
logistic	B
normal	I
distribution	O
,	O
as	O
in	O
categorical	B
pca	O
(	O
section	O
27.2.2	O
)	O
.	O
the	O
model	O
becomes	O
bk|γ	O
∼	O
dir	O
(	O
γ1v	O
)	O
zi	O
∼	O
n	O
(	O
μ	O
,	O
σ	O
)	O
πi|zi	O
=	O
s	O
(	O
zi	O
)	O
qil|πi	O
∼	O
cat	O
(	O
πi	O
)	O
yil|qil	O
=	O
k	O
,	O
b	O
∼	O
cat	O
(	O
bk	O
)	O
(	O
27.50	O
)	O
(	O
27.51	O
)	O
(	O
27.52	O
)	O
(	O
27.53	O
)	O
(	O
27.54	O
)	O
this	O
is	O
known	O
as	O
the	O
correlated	B
topic	I
model	I
(	O
blei	O
and	O
lafferty	O
2007	O
)	O
.	O
this	O
is	O
very	O
similar	B
to	O
categorical	B
pca	O
,	O
but	O
slightly	O
different	O
.	O
to	O
see	O
the	O
difference	O
,	O
let	O
us	O
marginalize	O
out	O
the	O
qil	O
and	O
πi	O
.	O
then	O
in	O
the	O
ctm	O
we	O
have	O
(	O
27.55	O
)	O
(	O
27.56	O
)	O
yil	O
∼	O
cat	O
(	O
bs	O
(	O
zi	O
)	O
)	O
where	O
b	O
is	O
a	O
stochastic	B
matrix	I
.	O
by	O
contrast	O
,	O
in	O
catpca	O
we	O
have	O
yil	O
∼	O
cat	O
(	O
s	O
(	O
wzi	O
)	O
)	O
where	O
w	O
is	O
an	O
unconstrained	O
matrix	O
.	O
fitting	O
this	O
model	O
is	O
tricky	O
,	O
since	O
the	O
prior	O
for	O
πi	O
is	O
no	O
longer	O
conjugate	O
to	O
the	O
multinomial	B
likelihood	O
for	O
qil	O
.	O
however	O
,	O
we	O
can	O
use	O
any	O
of	O
the	O
variational	O
methods	O
in	O
section	O
21.8.1.1	O
,	O
where	O
we	O
discussed	O
bayesian	O
multiclass	O
logistic	O
regression	B
.	O
in	O
the	O
ctm	O
case	O
,	O
things	O
are	O
even	O
harder	O
since	O
the	O
categorical	B
response	O
variables	O
qi	O
are	O
hidden	B
,	O
but	O
we	O
can	O
handle	O
this	O
by	O
using	O
an	O
additional	O
mean	B
ﬁeld	I
approximation	O
.	O
see	O
(	O
blei	O
and	O
lafferty	O
2007	O
)	O
for	O
details	O
.	O
having	O
ﬁt	O
the	O
model	O
,	O
one	O
can	O
then	O
convert	O
ˆσ	O
to	O
a	O
sparse	B
precision	O
matrix	O
ˆσ	O
by	O
pruning	B
low-strength	O
edges	B
,	O
to	O
get	O
a	O
sparse	B
gaussian	O
graphical	B
model	I
.	O
this	O
allows	O
you	O
to	O
visualize	O
the	O
correlation	O
between	O
topics	O
.	O
figure	O
27.10	O
shows	O
the	O
result	O
of	O
applying	O
this	O
procedure	O
to	O
articles	O
from	O
science	O
magazine	O
,	O
from	O
1990-1999	O
.	O
(	O
this	O
corpus	B
contains	O
16,351	O
documents	O
,	O
and	O
5.7m	O
words	O
(	O
19,088	O
of	O
them	O
unique	O
)	O
,	O
after	O
stop-word	O
and	O
low-frequency	O
removal	O
.	O
)	O
nodes	B
represent	O
topics	O
,	O
with	O
the	O
top	O
5	O
words	O
per	O
topic	B
listed	O
inside	O
.	O
the	O
font	O
size	O
reﬂects	O
the	O
overall	O
prevalence	B
of	O
the	O
topic	B
in	O
the	O
corpus	B
.	O
edges	B
represent	O
signiﬁcant	O
elements	O
of	O
the	O
precision	B
matrix	I
.	O
−1	O
962	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
activated	O
tyrosine	O
phosphorylation	O
activation	B
phosphorylation	O
kinase	O
research	O
funding	O
support	B
nih	O
program	O
science	O
scientists	O
says	O
research	O
people	O
united	O
states	O
women	O
universities	O
students	O
education	O
receptor	O
receptors	O
ligand	O
ligands	O
apoptosis	O
cells	O
cell	O
expression	O
cell	O
lines	O
bone	O
marrow	O
amino	O
acids	O
cdna	O
sequence	O
isolated	O
protein	O
p53	O
cell	O
cycle	B
activity	O
cyclin	O
regulation	O
wild	O
type	O
mutant	O
mutations	O
mutants	O
mutation	O
mice	O
antigen	O
t	O
cells	O
antigens	O
immune	O
response	O
virus	O
hiv	O
aids	O
infection	O
viruses	O
patients	O
disease	O
treatment	O
drugs	O
clinical	O
bacteria	O
bacterial	O
host	O
resistance	O
parasite	O
gene	O
disease	O
mutations	O
families	O
mutation	O
cells	O
proteins	O
researchers	O
protein	O
found	O
enzyme	O
enzymes	O
iron	O
active	O
site	O
reduction	O
plants	O
plant	O
gene	O
genes	O
arabidopsis	O
development	O
embryos	O
drosophila	O
genes	O
expression	O
proteins	O
protein	O
binding	O
domain	O
domains	O
rna	O
dna	O
rna	O
polymerase	O
cleavage	O
site	O
brain	O
memory	O
subjects	O
left	O
task	O
computer	O
problem	O
information	B
computers	O
problems	O
sequence	O
sequences	O
genome	B
dna	O
sequencing	O
surface	O
liquid	O
surfaces	O
ﬂuid	O
model	O
magnetic	O
magnetic	O
ﬁeld	O
spin	B
superconductivity	O
superconducting	O
fossil	O
record	O
birds	O
fossils	O
dinosaurs	O
fossil	O
species	O
forest	B
forests	O
populations	O
ecosystems	O
genetic	O
population	O
populations	O
differences	O
variation	O
ancient	O
found	O
impact	O
million	O
years	O
ago	O
africa	O
neurons	O
stimulus	O
motor	O
visual	O
cortical	O
materials	O
organic	O
polymer	O
polymers	O
molecules	O
synapses	O
ltp	O
glutamate	O
synaptic	O
neurons	O
physicists	O
particles	O
physics	O
particle	O
experiment	O
surface	O
tip	O
image	O
sample	O
device	O
laser	O
optical	O
light	O
electrons	O
quantum	O
reaction	O
reactions	O
molecule	O
molecules	O
transition	O
state	O
stars	O
astronomers	O
universe	O
galaxies	O
galaxy	O
pressure	O
high	O
pressure	O
pressures	O
core	O
inner	O
core	O
mantle	O
crust	O
upper	O
mantle	O
meteorites	O
ratios	O
sun	O
solar	O
wind	O
earth	O
planets	O
planet	O
earthquake	O
earthquakes	O
fault	O
images	O
data	O
co2	O
carbon	O
carbon	O
dioxide	O
methane	O
water	O
volcanic	O
deposits	O
magma	O
eruption	O
volcanism	O
climate	O
ocean	O
ice	O
changes	O
climate	O
change	O
ozone	O
atmospheric	O
measurements	O
stratosphere	O
concentrations	O
figure	O
27.10	O
output	O
of	O
the	O
correlated	B
topic	I
model	I
(	O
with	O
k	O
=	O
50	O
topics	O
)	O
when	O
applied	O
to	O
articles	O
from	O
science	O
.	O
nodes	B
represent	O
topics	O
,	O
with	O
the	O
5	O
most	O
probable	O
phrases	O
from	O
each	O
topic	B
shown	O
inside	O
.	O
font	O
size	O
reﬂects	O
overall	O
prevalence	B
of	O
the	O
topic	B
.	O
see	O
http	O
:	O
//www.cs.cmu.edu/~lemur/science/	O
for	O
an	O
interactive	O
version	O
of	O
this	O
model	O
with	O
100	O
topics	O
.	O
source	O
:	O
figure	O
2	O
of	O
(	O
blei	O
and	O
lafferty	O
2007	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
david	O
blei	O
.	O
27.4.2	O
dynamic	B
topic	I
model	I
in	O
lda	O
,	O
the	O
topics	O
(	O
distributions	O
over	O
words	O
)	O
are	O
assumed	O
to	O
be	O
static	O
.	O
in	O
some	O
cases	O
,	O
it	O
makes	O
sense	O
to	O
allow	O
these	O
distributions	O
to	O
evolve	O
smoothly	O
over	O
time	O
.	O
for	O
example	O
,	O
an	O
article	O
might	O
use	O
the	O
topic	B
“	O
neuroscience	O
”	O
,	O
but	O
if	O
it	O
was	O
written	O
in	O
the	O
1900s	O
,	O
it	O
is	O
more	O
likely	O
to	O
use	O
words	O
like	O
“	O
nerve	O
”	O
,	O
whereas	O
if	O
it	O
was	O
written	O
in	O
the	O
2000s	O
,	O
it	O
is	O
more	O
likely	O
to	O
use	O
words	O
like	O
“	O
calcium	O
receptor	O
”	O
(	O
this	O
reﬂects	O
the	O
general	O
trend	O
of	O
neuroscience	O
towards	O
molecular	O
biology	O
)	O
.	O
one	O
way	O
to	O
model	O
this	O
is	O
use	O
a	O
dynamic	O
logistic	O
normal	B
model	O
,	O
as	O
illustrated	O
in	O
figure	O
27.11.	O
in	O
particular	O
,	O
we	O
assume	O
the	O
topic	B
distributions	O
evolve	O
according	O
to	O
a	O
gaussian	O
random	O
walk	O
,	O
and	O
then	O
we	O
map	O
these	O
gaussian	O
vectors	O
to	O
probabilities	O
via	O
the	O
softmax	B
function	O
:	O
bt	O
,	O
k|bt−1	O
,	O
k	O
∼	O
n	O
(	O
bt−1	O
,	O
k	O
,	O
σ21v	O
)	O
i	O
∼	O
dir	O
(	O
α1k	O
)	O
πt	O
i	O
∼	O
cat	O
(	O
πt	O
il|πt	O
qt	O
i	O
)	O
il	O
=	O
k	O
,	O
bt	O
∼	O
cat	O
(	O
s	O
(	O
bt	O
k	O
)	O
)	O
il|qt	O
yt	O
this	O
is	O
known	O
as	O
a	O
dynamic	B
topic	I
model	I
(	O
blei	O
and	O
lafferty	O
2006b	O
)	O
.	O
(	O
27.57	O
)	O
(	O
27.58	O
)	O
(	O
27.59	O
)	O
(	O
27.60	O
)	O
27.4.	O
extensions	O
of	O
lda	O
963	O
α	O
πt	O
i	O
qt	O
il	O
yt	O
il	O
πt−1	O
i	O
qt−1	O
il	O
yt−1	O
il	O
πt+1	O
i	O
qt+1	O
il	O
yt+1	O
il	O
n	O
n	O
bt−1	O
k	O
bt	O
k	O
bt+1	O
k	O
n	O
k	O
figure	O
27.11	O
the	O
dynamic	B
topic	I
model	I
.	O
one	O
can	O
perform	O
approximate	O
infernece	O
in	O
this	O
model	O
using	O
a	O
structured	B
mean	I
ﬁeld	I
method	O
(	O
section	O
21.4	O
)	O
,	O
that	O
exploits	O
the	O
kalman	O
smoothing	O
algorithm	O
(	O
section	O
18.3.1	O
)	O
to	O
perform	O
exact	O
inference	B
on	O
the	O
linear-gaussian	O
chain	O
between	O
the	O
bt	O
,	O
k	O
nodes	B
(	O
see	O
(	O
blei	O
and	O
lafferty	O
2006b	O
)	O
for	O
details	O
)	O
.	O
figure	O
27.12	O
illustrates	O
a	O
typical	O
output	O
of	O
the	O
system	O
when	O
applied	O
to	O
100	O
years	O
of	O
articles	O
from	O
science	O
.	O
on	O
the	O
top	O
,	O
we	O
visualize	O
the	O
top	O
10	O
words	O
from	O
a	O
speciﬁc	O
topic	B
(	O
which	O
seems	O
to	O
be	O
related	O
to	O
neuroscience	O
)	O
after	O
10	O
year	O
intervals	O
.	O
on	O
the	O
bottom	O
left	O
,	O
we	O
plot	O
the	O
probability	O
of	O
some	O
speciﬁc	O
words	O
belonging	O
to	O
this	O
topic	B
.	O
on	O
the	O
bottom	O
right	O
,	O
we	O
list	O
the	O
titles	O
of	O
some	O
articles	O
that	O
contained	O
this	O
topic	B
.	O
one	O
interesting	O
application	O
of	O
this	O
model	O
is	O
to	O
perform	O
temporally-corrected	O
document	O
re-	O
trieval	O
.	O
that	O
is	O
,	O
suppose	O
we	O
look	O
for	O
documents	O
about	O
the	O
inheritance	O
of	O
disease	O
.	O
modern	O
articles	O
will	O
use	O
words	O
like	O
“	O
dna	O
”	O
,	O
but	O
older	O
articles	O
(	O
before	O
the	O
discovery	O
of	O
dna	O
)	O
may	O
use	O
other	O
terms	O
such	O
as	O
“	O
heritable	O
unit	O
”	O
.	O
but	O
both	O
articles	O
are	O
likely	O
to	O
use	O
the	O
same	O
topics	O
.	O
similar	B
ideas	O
can	O
be	O
used	O
to	O
perform	O
cross-language	B
information	I
retrieval	I
,	O
see	O
e.g.	O
,	O
(	O
cimiano	O
et	O
al	O
.	O
2009	O
)	O
.	O
27.4.3	O
lda-hmm	O
the	O
lda	O
model	O
assumes	O
words	O
are	O
exchangeable	B
,	O
which	O
is	O
clearly	O
not	O
true	O
.	O
a	O
simple	O
way	O
to	O
model	O
sequential	O
dependence	O
between	O
words	O
is	O
to	O
use	O
a	O
hidden	B
markov	O
model	O
or	O
hmm	O
.	O
the	O
trouble	O
with	O
hmms	O
is	O
that	O
they	O
can	O
only	O
model	O
short-range	O
dependencies	O
,	O
so	O
they	O
can	O
not	O
capture	O
the	O
overall	O
gist	B
of	O
a	O
document	O
.	O
hence	O
they	O
can	O
generate	O
syntactically	O
correct	O
sentences	O
(	O
see	O
e.g.	O
,	O
table	O
17.1	O
)	O
.	O
but	O
not	O
semantically	O
plausible	O
ones	O
.	O
it	O
is	O
possible	O
to	O
combine	O
lda	O
with	O
hmm	O
to	O
create	O
a	O
model	O
called	O
lda-hmm	O
(	O
griffiths	O
et	O
al	O
.	O
964	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
1881	O
brain	O
movement	O
action	B
right	O
eye	O
hand	O
left	O
muscle	O
nerve	O
sound	O
1890	O
movement	O
eye	O
right	O
hand	O
brain	O
left	O
action	O
muscle	O
sound	O
experiment	O
1900	O
brain	O
eye	O
movement	O
right	O
left	O
hand	O
nerve	O
vision	O
sound	O
muscle	O
1910	O
1920	O
movement	O
movement	O
brain	O
sound	O
nerve	O
active	O
muscle	O
left	O
eye	O
right	O
nervous	O
sound	O
muscle	O
active	O
nerve	O
stimulate	O
ﬁber	O
reaction	O
brain	O
response	O
1930	O
stimulate	O
muscle	O
sound	O
movement	O
response	O
nerve	O
frequency	O
ﬁber	O
active	O
brain	O
1940	O
record	O
nerve	O
stimulate	O
response	O
muscle	O
electrode	O
active	O
brain	O
ﬁber	O
potential	O
1950	O
respons	O
record	O
stimulate	O
nerve	O
muscle	O
active	O
frequency	O
electrode	O
potential	O
study	O
1960	O
response	O
stimulate	O
record	O
condition	O
active	O
potential	O
stimulus	O
nerve	O
subject	O
eye	O
1970	O
respons	O
cell	O
potential	O
stimul	O
neuron	O
active	O
nerve	O
eye	O
record	O
abstract	O
1980	O
cell	O
neuron	O
response	O
active	O
brain	O
stimul	O
muscle	O
system	O
nerve	O
receptor	O
1990	O
cell	O
channel	O
neuron	O
ca2	O
active	O
brain	O
receptor	O
muscle	O
respons	O
current	O
2000	O
neuron	O
active	O
brain	O
cell	O
ﬁg	O
response	O
channel	O
receptor	O
synapse	O
signal	O
nerve	O
''	O
neuroscience	O
''	O
ca2	O
neuron	O
1880	O
1900	O
1920	O
1940	O
1960	O
1980	O
2000	O
1887	O
mental	O
science	O
1900	O
hemianopsia	O
in	O
migraine	O
1912	O
a	O
defence	O
of	O
the	O
``	O
new	O
phrenology	O
''	O
1921	O
the	O
synchronal	O
flashing	O
of	O
fireﬂies	O
1932	O
myoesthesis	O
and	O
imageless	O
thought	O
1943	O
acetylcholine	O
and	O
the	O
physiology	O
of	O
the	O
nervous	O
system	O
1952	O
brain	O
waves	O
and	O
unit	O
discharge	O
in	O
cerebral	O
cortex	O
1963	O
errorless	O
discrimination	O
learning	B
in	O
the	O
pigeon	O
1974	O
temporal	O
summation	O
of	O
light	O
by	O
a	O
vertebrate	O
visual	O
receptor	O
1983	O
hysteresis	O
in	O
the	O
force-calcium	O
relation	B
in	O
muscle	O
1993	O
gaba-activated	O
chloride	O
channels	O
in	O
secretory	O
nerve	O
endings	O
figure	O
27.12	O
part	O
of	O
the	O
output	O
of	O
the	O
dynamic	B
topic	I
model	I
when	O
applied	O
to	O
articles	O
from	O
science	O
.	O
we	O
show	O
the	O
top	O
10	O
words	O
for	O
the	O
neuroscience	O
topic	B
over	O
time	O
.	O
we	O
also	O
show	O
the	O
probability	O
of	O
three	O
words	O
within	O
this	O
topic	B
over	O
time	O
,	O
and	O
some	O
articles	O
that	O
contained	O
this	O
topic	B
.	O
source	O
:	O
figure	O
4	O
of	O
(	O
blei	O
and	O
lafferty	O
2006b	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
david	O
blei	O
.	O
2004	O
)	O
.	O
this	O
model	O
uses	O
the	O
hmm	O
states	O
to	O
model	O
function	O
or	O
syntactic	O
words	O
,	O
such	O
as	O
“	O
and	O
”	O
or	O
“	O
however	O
”	O
,	O
and	O
uses	O
the	O
lda	O
to	O
model	O
content	O
or	O
semantic	O
words	O
,	O
which	O
are	O
harder	O
to	O
predict	O
.	O
there	O
is	O
a	O
distinguished	O
hmm	O
state	B
which	O
speciﬁes	O
when	O
the	O
lda	O
model	O
should	O
be	O
used	O
to	O
generate	O
the	O
word	O
;	O
the	O
rest	O
of	O
the	O
time	O
,	O
the	O
hmm	O
generates	O
the	O
word	O
.	O
more	O
formally	O
,	O
for	O
each	O
document	O
i	O
,	O
the	O
model	O
deﬁnes	O
an	O
hmm	O
with	O
states	O
zil	O
∈	O
{	O
0	O
,	O
.	O
.	O
.	O
,	O
c	O
}	O
.	O
in	O
addition	O
,	O
each	O
document	O
has	O
an	O
lda	O
model	O
associated	O
with	O
it	O
.	O
if	O
zil	O
=	O
0	O
,	O
we	O
generate	O
word	O
yil	O
from	O
the	O
semantic	O
lda	O
model	O
,	O
with	O
topic	B
speciﬁed	O
by	O
qil	O
;	O
otherwise	O
we	O
generate	O
word	O
yil	O
from	O
the	O
syntactic	O
hmm	O
model	O
.	O
the	O
dgm	O
is	O
shown	O
in	O
figure	O
27.13.	O
the	O
cpds	O
are	O
as	O
follows	O
:	O
p	O
(	O
πi	O
)	O
=	O
dir	O
(	O
πi|α1k	O
)	O
p	O
(	O
zil	O
=	O
c	O
(	O
cid:2	O
)	O
|zi	O
,	O
l−1	O
=	O
c	O
)	O
=a	O
hmm	O
(	O
c	O
,	O
c	O
(	O
cid:2	O
)	O
p	O
(	O
qil	O
=	O
k|πi	O
)	O
=π	O
ik	O
(	O
cid:19	O
)	O
p	O
(	O
yil	O
=	O
v|qil	O
=	O
k	O
,	O
zil	O
=	O
c	O
)	O
=	O
(	O
27.61	O
)	O
(	O
27.62	O
)	O
(	O
27.63	O
)	O
(	O
27.64	O
)	O
if	O
c	O
=	O
0	O
if	O
c	O
>	O
0	O
)	O
blda	O
(	O
k	O
,	O
v	O
)	O
bhmm	O
(	O
c	O
,	O
v	O
)	O
where	O
blda	O
is	O
the	O
usual	O
topic-word	O
matrix	O
,	O
bhmm	O
is	O
the	O
state-word	O
hmm	O
emission	O
matrix	O
and	O
ahmm	O
is	O
the	O
state-state	O
hmm	O
transition	B
matrix	I
.	O
inference	B
in	O
this	O
model	O
can	O
be	O
done	O
with	O
collapsed	O
gibbs	O
sampling	O
,	O
analytically	O
integrating	O
out	O
all	O
the	O
continuous	O
quantities	O
.	O
see	O
(	O
griffiths	O
et	O
al	O
.	O
2004	O
)	O
for	O
the	O
details	O
.	O
the	O
results	O
of	O
applying	O
this	O
model	O
(	O
with	O
k	O
=	O
200	O
lda	O
topics	O
and	O
c	O
=	O
20	O
hmm	O
states	O
)	O
to	O
the	O
combined	O
brown	O
and	O
tasa	O
corpora4	O
are	O
shown	O
in	O
table	O
27.1.	O
we	O
see	O
that	O
the	O
hmm	O
generally	O
is	O
4.	O
the	O
brown	O
corpus	B
consists	O
of	O
500	O
documents	O
and	O
1,137,466	O
word	O
tokens	B
,	O
with	O
part-of-speech	O
tags	O
for	O
each	O
token	O
.	O
27.4.	O
extensions	O
of	O
lda	O
965	O
α	O
πi	O
qi	O
,	O
l	O
yi	O
,	O
l	O
zi	O
,	O
l	O
qi	O
,	O
l−1	O
yi	O
,	O
l−1	O
zi	O
,	O
l−1	O
qi	O
,	O
l+1	O
.	O
.	O
.	O
yi	O
,	O
l+1	O
.	O
.	O
.	O
blda	O
zi	O
,	O
l+1	O
.	O
.	O
.	O
n	O
ahmm	O
bhmm	O
figure	O
27.13	O
lda-hmm	O
model	O
.	O
in	O
contrast	O
to	O
this	O
approach	O
,	O
we	O
study	O
here	O
how	O
the	O
overall	O
network	O
activity	O
can	O
control	O
single	O
cell	O
parameters	O
such	O
as	O
input	O
resistance	O
,	O
as	O
well	O
as	O
time	O
and	O
space	O
constants	O
,	O
parameters	O
that	O
are	O
crucial	O
for	O
excitability	O
and	O
spariotemporal	O
(	O
sic	O
)	O
integration	O
.	O
the	O
integrated	O
architecture	O
in	O
this	O
paper	O
combines	O
feed	O
forward	O
control	O
and	O
error	O
feedback	O
adaptive	O
control	O
using	O
neural	B
networks	I
.	O
in	O
other	O
words	O
,	O
for	O
our	O
proof	O
of	O
convergence	O
,	O
we	O
require	O
the	O
softassign	O
algorithm	O
to	O
return	O
a	O
doubly	O
stochastic	B
matrix	I
as	O
*sinkhorn	O
theorem	O
guarantees	O
that	O
it	O
will	O
instead	O
of	O
a	O
matrix	O
which	O
is	O
merely	O
close	O
to	O
being	O
doubly	O
stochastic	O
based	O
on	O
some	O
reasonable	O
metric	B
.	O
the	O
aim	O
is	O
to	O
construct	O
a	O
portfolio	O
with	O
a	O
maximal	O
expected	O
return	O
for	O
a	O
given	O
risk	B
level	O
and	O
time	O
horizon	O
while	O
simultaneously	O
obeying	O
*institutional	O
or	O
*legally	O
required	O
constraints	O
.	O
the	O
left	O
graph	O
is	O
the	O
standard	O
experiment	O
the	O
right	O
from	O
a	O
training	O
with	O
#	O
samples	B
.	O
the	O
graph	B
g	O
is	O
called	O
the	O
*guest	O
graph	B
,	O
and	O
h	O
is	O
called	O
the	O
host	O
graph	B
.	O
1	O
.	O
2	O
.	O
3.	O
figure	O
27.14	O
function	O
and	O
content	O
words	O
in	O
the	O
nips	O
corpus	B
,	O
as	O
distinguished	O
by	O
the	O
lda-hmm	O
model	O
.	O
graylevel	O
indicates	O
posterior	O
probability	O
of	O
assignment	O
to	O
lda	O
component	O
,	O
with	O
black	O
being	O
highest	O
.	O
the	O
boxed	O
word	O
appears	O
as	O
a	O
function	O
word	O
in	O
one	O
sentence	O
,	O
and	O
as	O
a	O
content	O
word	O
in	O
another	O
sentence	O
.	O
asterisked	O
words	O
had	O
low	O
frequency	O
,	O
and	O
were	O
treated	O
as	O
a	O
single	O
word	O
type	O
by	O
the	O
model	O
.	O
source	O
:	O
figure	O
4	O
of	O
(	O
griffiths	O
et	O
al	O
.	O
2004	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
tom	O
griffiths	O
.	O
966	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
the	O
blood	O
,	O
of	O
body	O
heart	O
and	O
in	O
to	O
is	O
blood	O
heart	O
pressure	O
body	O
lungs	O
oxygen	O
vessels	O
arteries	O
*	O
breathing	O
the	O
a	O
his	O
this	O
their	O
these	O
your	O
her	O
my	O
some	O
the	O
,	O
and	O
of	O
a	O
in	O
trees	O
tree	B
with	O
on	O
forest	B
trees	O
forests	O
land	O
soil	O
areas	O
park	O
wildlife	O
area	O
rain	O
in	O
for	O
to	O
on	O
with	O
at	O
by	O
from	O
as	O
into	O
the	O
,	O
and	O
of	O
in	O
land	O
to	O
the	O
of	O
,	O
to	O
in	O
and	O
classes	O
farmers	O
government	O
for	O
farm	O
farmers	O
land	O
crops	O
farm	O
food	O
people	O
farming	O
wheat	O
farms	O
corn	O
he	O
it	O
you	O
they	O
i	O
she	O
we	O
there	O
this	O
who	O
a	O
state	B
government	O
state	B
federal	O
public	O
local	O
act	O
states	O
national	O
laws	O
department	O
*	O
new	O
other	O
ﬁrst	O
same	O
great	O
good	O
small	O
little	O
old	O
the	O
a	O
of	O
,	O
in	O
to	O
picture	O
ﬁlm	O
image	O
lens	O
light	O
eye	O
lens	O
image	O
mirror	O
eyes	O
glass	O
object	O
objects	O
lenses	O
be	O
have	O
see	O
make	O
do	O
know	O
get	O
go	O
take	O
ﬁnd	O
a	O
the	O
of	O
,	O
in	O
water	O
is	O
and	O
matter	O
are	O
water	O
matter	O
molecules	O
liquid	O
particles	O
gas	O
solid	O
substance	O
temperature	B
changes	O
said	O
made	O
used	O
came	O
went	O
found	O
called	O
the	O
,	O
a	O
of	O
and	O
drink	O
alcohol	O
to	O
bottle	O
in	O
drugs	O
drug	O
alcohol	O
people	O
drinking	O
person	O
effects	O
marijuana	O
body	O
use	O
time	O
way	O
years	O
day	O
part	O
number	O
kind	O
place	O
the	O
,	O
of	O
a	O
and	O
in	O
story	O
is	O
to	O
as	O
story	O
stories	O
poem	O
characters	O
poetry	O
character	O
author	O
poems	O
life	O
poet	O
can	O
would	O
will	O
could	O
may	O
had	O
must	O
do	O
have	O
did	O
the	O
,	O
a	O
in	O
game	O
ball	O
and	O
team	O
to	O
play	O
ball	O
game	O
team	O
*	O
baseball	O
players	O
football	O
player	O
ﬁeld	O
basketball	O
,	O
;	O
(	O
:	O
)	O
table	O
27.1	O
upper	O
row	O
:	O
topics	O
extracted	O
by	O
the	O
lda	O
model	O
when	O
trained	O
on	O
the	O
combined	O
brown	O
and	O
tasa	O
corpora	O
.	O
middle	O
row	O
:	O
topics	O
extracted	O
by	O
lda	O
part	O
of	O
lda-hmm	O
model	O
.	O
bottom	O
row	O
:	O
topics	O
extracted	O
by	O
hmm	O
part	O
of	O
lda-hmm	O
model	O
.	O
each	O
column	O
represents	O
a	O
single	O
topic/class	O
,	O
and	O
words	O
appear	O
in	O
order	O
of	O
probability	O
in	O
that	O
topic/class	O
.	O
since	O
some	O
classes	O
give	O
almost	O
all	O
probability	O
to	O
only	O
a	O
few	O
words	O
,	O
a	O
list	O
is	O
terminated	O
when	O
the	O
words	O
account	O
for	O
90	O
%	O
of	O
the	O
probability	O
mass	O
.	O
source	O
:	O
figure	O
2	O
of	O
(	O
griffiths	O
et	O
al	O
.	O
2004	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
tom	O
griffiths	O
.	O
responsible	O
for	O
syntactic	O
words	O
,	O
and	O
the	O
lda	O
for	O
semantics	O
words	O
.	O
if	O
we	O
did	O
not	O
have	O
the	O
hmm	O
,	O
the	O
lda	O
topics	O
would	O
get	O
“	O
polluted	O
”	O
by	O
function	O
words	O
(	O
see	O
top	O
of	O
ﬁgure	O
)	O
,	O
which	O
is	O
why	O
such	O
words	O
are	O
normally	O
removed	O
during	O
preprocessing	O
.	O
the	O
model	O
can	O
also	O
help	O
disambiguate	O
when	O
the	O
same	O
word	O
is	O
being	O
used	O
syntactically	O
or	O
semantically	O
.	O
figure	O
27.14	O
shows	O
some	O
examples	O
when	O
the	O
model	O
was	O
applied	O
to	O
the	O
nips	O
corpus.5	O
we	O
see	O
that	O
the	O
roles	O
of	O
words	O
are	O
distinguished	O
,	O
e.g.	O
,	O
“	O
we	O
require	O
the	O
algorithm	O
to	O
return	O
a	O
matrix	O
”	O
(	O
verb	O
)	O
vs	O
“	O
the	O
maximal	O
expected	O
return	O
”	O
(	O
noun	O
)	O
.	O
in	O
principle	O
,	O
a	O
part	B
of	I
speech	I
tagger	O
could	O
disambiguate	O
these	O
two	O
uses	O
,	O
but	O
note	O
that	O
(	O
1	O
)	O
the	O
lda-hmm	O
method	O
is	O
fully	O
unsupervised	O
(	O
no	O
pos	O
tags	O
were	O
used	O
)	O
,	O
and	O
(	O
2	O
)	O
sometimes	O
a	O
word	O
can	O
have	O
the	O
same	O
pos	O
tag	O
,	O
but	O
different	O
senses	O
,	O
e.g.	O
,	O
“	O
the	O
left	O
graph	O
”	O
(	O
a	O
synactic	O
role	O
)	O
vs	O
“	O
the	O
graph	B
g	O
”	O
(	O
a	O
semantic	O
role	O
)	O
.	O
the	O
topic	B
of	O
probabilistic	O
models	O
for	O
syntax	O
and	O
semantics	O
is	O
a	O
vast	O
one	O
,	O
which	O
we	O
do	O
not	O
the	O
tasa	O
corpus	B
is	O
an	O
untagged	O
collection	O
of	O
educational	O
materials	O
consisting	O
of	O
37,651	O
documents	O
and	O
12,190,931	O
word	O
tokens	B
.	O
words	O
appearing	O
in	O
fewer	O
than	O
5	O
documents	O
were	O
replaced	O
with	O
an	O
asterisk	O
,	O
but	O
punctuation	O
was	O
included	O
.	O
the	O
combined	O
vocabulary	O
was	O
of	O
size	O
37,202	O
unique	O
words	O
.	O
5.	O
nips	O
stands	O
for	O
“	O
neural	O
information	O
processing	O
systems	O
”	O
.	O
nips	O
corpus	B
volumes	O
1–12	O
contains	O
1713	O
documents	O
.	O
it	O
is	O
one	O
of	O
the	O
top	O
machine	B
learning	I
conferences	O
.	O
the	O
27.4.	O
extensions	O
of	O
lda	O
967	O
πi	O
α	O
qil	O
yil	O
¯qi	O
ci	O
n	O
b	O
w	O
(	O
a	O
)	O
a	O
πi	O
α	O
qil	O
yil	O
ci	O
n	O
b	O
(	O
b	O
)	O
figure	O
27.15	O
(	O
a	O
)	O
supervised	O
lda	O
.	O
(	O
b	O
)	O
discriminative	B
lda	O
.	O
have	O
space	O
to	O
delve	O
into	O
any	O
more	O
.	O
see	O
e.g.	O
,	O
(	O
jurafsky	O
and	O
martin	O
2008	O
)	O
for	O
further	O
information	B
.	O
27.4.4	O
supervised	O
lda	O
in	O
this	O
section	O
,	O
we	O
discuss	O
extensions	O
of	O
lda	O
to	O
handle	O
side	B
information	I
of	O
various	O
kinds	O
beyond	O
just	O
words	O
.	O
27.4.4.1	O
generative	O
supervised	O
lda	O
suppose	O
we	O
have	O
a	O
variable	O
length	O
sequence	O
of	O
words	O
yil	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
v	O
}	O
as	O
usual	O
,	O
but	O
we	O
also	O
have	O
a	O
class	O
label	O
ci	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
c	O
}	O
.	O
how	O
can	O
we	O
predict	O
ci	O
from	O
yi	O
?	O
there	O
are	O
many	O
possible	O
approaches	O
,	O
but	O
most	O
are	O
direct	O
mappings	O
from	O
the	O
words	O
to	O
the	O
class	O
.	O
in	O
some	O
cases	O
,	O
such	O
as	O
sentiment	B
analysis	I
,	O
we	O
can	O
get	O
better	O
performance	O
by	O
ﬁrst	O
performing	O
inference	B
,	O
to	O
try	O
to	O
disambiguate	O
the	O
meaning	O
of	O
words	O
.	O
for	O
example	O
,	O
suppose	O
the	O
goal	O
is	O
to	O
determine	O
if	O
a	O
document	O
is	O
a	O
favorable	O
review	O
of	O
a	O
movie	O
or	O
not	O
.	O
if	O
we	O
encounter	O
the	O
phrase	O
“	O
brad	O
pitt	O
was	O
excellent	O
until	O
the	O
middle	O
of	O
the	O
movie	O
”	O
,	O
the	O
word	O
“	O
excellent	O
”	O
may	O
lead	O
us	O
to	O
think	O
the	O
review	O
is	O
positive	O
,	O
but	O
clearly	O
the	O
overall	O
sentiment	O
is	O
negative	O
.	O
one	O
way	O
to	O
tackle	O
such	O
problems	O
is	O
to	O
build	O
a	O
joint	O
model	O
of	O
the	O
form	O
p	O
(	O
ci	O
,	O
yi|θ	O
)	O
.	O
(	O
blei	O
and	O
mcauliffe	O
2010	O
)	O
proposes	O
an	O
approach	O
,	O
called	O
supervised	O
lda	O
,	O
where	O
the	O
class	O
label	O
ci	O
is	O
generated	O
from	O
the	O
topics	O
as	O
follows	O
:	O
p	O
(	O
ci|qi	O
)	O
=	O
ber	O
(	O
sigm	O
(	O
wt	O
qi	O
)	O
)	O
here	O
qi	O
is	O
the	O
empirical	O
topic	O
distribution	O
for	O
document	O
i	O
:	O
li	O
(	O
cid:4	O
)	O
i=1	O
qik	O
(	O
cid:2	O
)	O
1	O
li	O
qilk	O
see	O
figure	O
27.15	O
(	O
a	O
)	O
for	O
an	O
illustration	O
.	O
(	O
27.65	O
)	O
(	O
27.66	O
)	O
968	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
w	O
xi	O
πi	O
qil	O
yil	O
n	O
(	O
a	O
)	O
b	O
μ	O
σ	O
xi	O
w	O
i	O
πi	O
qil	O
yil	O
n	O
b	O
(	O
b	O
)	O
w	O
xi	O
αi	O
πi	O
b	O
qil	O
yil	O
n	O
(	O
c	O
)	O
figure	O
27.16	O
discriminative	B
variants	O
of	O
lda	O
.	O
(	O
a	O
)	O
mixture	B
of	I
experts	I
aka	O
mr-lda	O
.	O
the	O
double	O
ring	O
denotes	O
a	O
node	O
that	O
πi	O
a	O
deterministic	O
function	O
of	O
its	O
parents	B
.	O
(	O
c	O
)	O
dmr-lda	O
.	O
(	O
b	O
)	O
mixture	B
of	I
experts	I
with	O
random	B
effects	I
.	O
we	O
can	O
ﬁt	O
this	O
model	O
using	O
monte	O
carlo	O
em	O
:	O
run	O
the	O
collapsed	O
gibbs	O
sampler	O
in	O
the	O
e	O
step	O
,	O
to	O
compute	O
e	O
[	O
qik	O
]	O
,	O
and	O
then	O
use	O
this	O
as	O
the	O
input	O
feature	O
to	O
a	O
standard	O
logistic	O
regression	B
package	O
.	O
27.4.4.2	O
discriminative	B
supervised	O
lda	O
an	O
alternative	O
approach	O
,	O
known	O
as	O
discriminative	B
lda	O
(	O
lacoste-julien	O
et	O
al	O
.	O
2009	O
)	O
,	O
is	O
shown	O
in	O
figure	O
27.15	O
(	O
b	O
)	O
.	O
this	O
is	O
a	O
discriminative	B
model	O
of	O
the	O
form	O
p	O
(	O
yi|ci	O
,	O
θ	O
)	O
.	O
the	O
only	O
change	O
from	O
regular	B
lda	O
is	O
that	O
the	O
topic	B
prior	O
becomes	O
input	O
dependent	O
,	O
as	O
follows	O
:	O
p	O
(	O
qil|πi	O
,	O
ci	O
=	O
c	O
,	O
θ	O
)	O
=	O
cat	O
(	O
acπ	O
)	O
where	O
ac	O
is	O
a	O
k	O
×	O
k	O
stochastic	B
matrix	I
.	O
so	O
far	O
,	O
we	O
have	O
assumed	O
the	O
“	O
side	B
information	I
”	O
is	O
a	O
single	O
categorical	O
variable	O
ci	O
.	O
often	O
we	O
have	O
high	O
dimensional	O
covariates	B
xi	O
∈	O
r	O
d.	O
for	O
example	O
,	O
consider	O
the	O
task	O
of	O
image	B
tagging	I
.	O
the	O
idea	O
is	O
that	O
yil	O
represent	O
correlated	O
tags	O
or	O
labels	O
,	O
which	O
we	O
want	O
to	O
predict	O
given	O
xi	O
.	O
we	O
now	O
discuss	O
several	O
attempts	O
to	O
extend	O
lda	O
so	O
that	O
it	O
can	O
generate	O
tags	O
given	O
the	O
inputs	O
.	O
the	O
simplest	O
approach	O
is	O
to	O
use	O
a	O
mixture	B
of	I
experts	I
(	O
section	O
11.2.4	O
)	O
with	O
multiple	O
outputs	O
.	O
this	O
is	O
just	O
like	O
lda	O
except	O
we	O
replace	O
the	O
dirichlet	O
prior	O
on	O
πi	O
with	O
a	O
deterministic	O
function	O
of	O
the	O
input	O
:	O
πi	O
=	O
s	O
(	O
wxi	O
)	O
(	O
27.67	O
)	O
(	O
27.68	O
)	O
in	O
(	O
law	O
et	O
al	O
.	O
2010	O
)	O
,	O
this	O
is	O
called	O
multinomial	O
regression	O
lda	O
.	O
see	O
figure	O
27.16	O
(	O
a	O
)	O
.	O
eliminating	O
the	O
deterministic	O
πi	O
we	O
have	O
p	O
(	O
qil|xi	O
,	O
w	O
)	O
=	O
cat	O
(	O
s	O
(	O
wxi	O
)	O
)	O
we	O
can	O
ﬁt	O
this	O
with	O
em	O
in	O
the	O
usual	O
way	O
.	O
however	O
,	O
(	O
law	O
et	O
al	O
.	O
2010	O
)	O
suggest	O
an	O
alternative	O
.	O
first	O
ﬁt	O
an	O
unsupervised	O
lda	O
model	O
based	O
only	O
on	O
yi	O
;	O
then	O
treat	O
the	O
inferred	O
πi	O
as	O
data	O
,	O
and	O
(	O
27.69	O
)	O
27.4.	O
extensions	O
of	O
lda	O
969	O
ﬁt	O
a	O
multinomial	B
logistic	I
regression	I
model	O
mapping	O
xi	O
to	O
πi	O
.	O
although	O
this	O
is	O
fast	O
,	O
ﬁtting	O
lda	O
in	O
an	O
unsupervised	O
fashion	O
does	O
not	O
necessarily	O
result	O
in	O
a	O
discriminative	B
set	O
of	O
latent	B
variables	O
,	O
as	O
discussed	O
in	O
(	O
blei	O
and	O
mcauliffe	O
2010	O
)	O
.	O
there	O
is	O
a	O
more	O
subtle	O
problem	O
with	O
this	O
model	O
.	O
since	O
πi	O
is	O
a	O
deterministic	O
function	O
of	O
the	O
inputs	O
,	O
it	O
is	O
effectively	O
observed	O
,	O
rendering	O
the	O
qil	O
(	O
and	O
hence	O
the	O
tags	O
yil	O
)	O
independent	O
.	O
in	O
other	O
words	O
,	O
li	O
(	O
cid:20	O
)	O
li	O
(	O
cid:20	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
yi|xi	O
,	O
θ	O
)	O
=	O
p	O
(	O
yil|xi	O
,	O
θ	O
)	O
=	O
p	O
(	O
yil|qil	O
=	O
k	O
,	O
b	O
)	O
p	O
(	O
qil	O
=	O
k|xi	O
,	O
w	O
)	O
(	O
27.70	O
)	O
l=1	O
l=1	O
k	O
this	O
means	O
that	O
if	O
we	O
observe	O
the	O
value	O
of	O
one	O
tag	O
,	O
it	O
will	O
have	O
no	O
inﬂuence	O
on	O
any	O
of	O
the	O
others	O
.	O
this	O
may	O
explain	O
why	O
the	O
results	O
in	O
(	O
law	O
et	O
al	O
.	O
2010	O
)	O
only	O
show	O
negligible	O
improvement	O
over	O
predicting	O
each	O
tag	O
independently	O
.	O
one	O
way	O
to	O
induce	O
correlations	O
is	O
to	O
make	O
w	O
a	O
random	O
variable	O
.	O
the	O
resulting	O
model	O
is	O
shown	O
in	O
figure	O
27.16	O
(	O
b	O
)	O
.	O
we	O
call	O
this	O
a	O
random	B
effects	I
mixture	I
of	I
experts	I
.	O
we	O
typically	O
assume	O
a	O
gaussian	O
prior	O
on	O
wi	O
.	O
if	O
xi	O
=	O
1	O
,	O
then	O
p	O
(	O
qil|xi	O
,	O
wi	O
)	O
=	O
cat	O
(	O
s	O
(	O
wi	O
)	O
)	O
,	O
so	O
we	O
recover	O
the	O
correlated	B
topic	I
model	I
.	O
it	O
is	O
possible	O
to	O
extend	O
this	O
model	O
by	O
adding	O
markovian	O
dynamics	O
to	O
the	O
qil	O
variables	O
.	O
this	O
is	O
called	O
a	O
conditional	B
topic	I
random	I
ﬁeld	I
(	O
zhu	O
and	O
xing	O
2010	O
)	O
.	O
a	O
closely	O
related	O
approach	O
,	O
known	O
as	O
dirichlet	O
multinomial	O
regression	O
lda	O
(	O
mimno	O
and	O
mccallum	O
2008	O
)	O
,	O
is	O
shown	O
in	O
figure	O
27.16	O
(	O
c	O
)	O
.	O
this	O
is	O
identical	O
to	O
standard	O
lda	O
except	O
we	O
make	O
α	O
a	O
function	O
of	O
the	O
input	O
αi	O
=	O
exp	O
(	O
wxi	O
)	O
where	O
w	O
is	O
a	O
k	O
×	O
d	O
matrix	O
.	O
eliminating	O
the	O
deterministic	O
αi	O
we	O
have	O
πi	O
∼	O
dir	O
(	O
exp	O
(	O
wxi	O
)	O
)	O
(	O
27.71	O
)	O
(	O
27.72	O
)	O
is	O
known	O
as	O
labeled	O
lda	O
(	O
ramage	O
et	O
al	O
.	O
2009	O
)	O
.	O
unlike	O
(	O
law	O
et	O
al	O
.	O
2010	O
)	O
,	O
this	O
model	O
allows	O
information	B
to	O
ﬂow	O
between	O
tags	O
via	O
the	O
latent	B
πi	O
.	O
a	O
variant	O
of	O
this	O
model	O
,	O
where	O
xi	O
corresponds	O
to	O
a	O
bag	O
of	O
discrete	O
labels	O
and	O
πi	O
∼	O
dir	O
(	O
α	O
$	O
in	O
this	O
case	O
,	O
the	O
labels	O
xi	O
are	O
in	O
1:1	O
xi	O
)	O
,	O
correspondence	B
with	O
the	O
latent	B
topics	O
,	O
which	O
makes	O
the	O
resulting	O
topics	O
much	O
more	O
interpretable	O
.	O
an	O
extension	B
,	O
known	O
as	O
partially	O
labeled	O
lda	O
(	O
ramage	O
et	O
al	O
.	O
2011	O
)	O
,	O
allows	O
each	O
label	B
to	O
have	O
multiple	O
latent	O
sub-topics	O
;	O
this	O
model	O
includes	O
lda	O
,	O
labeled	O
lda	O
and	O
a	O
multinomial	B
mixture	O
model	O
as	O
special	O
cases	O
.	O
27.4.4.3	O
discriminative	B
categorical	O
pca	O
an	O
alternative	O
to	O
using	O
lda	O
is	O
to	O
expand	O
the	O
categorical	B
pca	O
model	O
with	O
inputs	O
,	O
as	O
shown	O
in	O
figure	O
27.17	O
(	O
a	O
)	O
.	O
since	O
the	O
latent	B
space	O
is	O
now	O
real-valued	O
,	O
we	O
can	O
use	O
simple	O
linear	O
regression	B
for	O
the	O
input-hidden	O
mapping	O
.	O
for	O
the	O
hidden-output	O
mapping	O
,	O
we	O
use	O
traditional	O
catpca	O
:	O
p	O
(	O
zi|xi	O
,	O
v	O
)	O
=n	O
(	O
vxi	O
,	O
σ	O
)	O
p	O
(	O
yi|zi	O
,	O
w	O
)	O
=	O
cat	O
(	O
yil|s	O
(	O
wzi	O
)	O
)	O
(	O
cid:20	O
)	O
(	O
27.73	O
)	O
(	O
27.74	O
)	O
l	O
this	O
model	O
is	O
essentially	O
a	O
probabilistic	O
neural	O
network	O
with	O
one	O
hidden	B
layer	I
,	O
as	O
shown	O
in	O
figure	O
27.17	O
(	O
b	O
)	O
,	O
but	O
with	O
exchangeable	B
output	O
(	O
e.g.	O
,	O
to	O
handle	O
variable	O
numbers	O
of	O
tags	O
)	O
.	O
the	O
970	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
xi	O
zi	O
yil	O
n	O
(	O
a	O
)	O
v	O
w	O
v1	O
xi1	O
yi1	O
.	O
.	O
.	O
.	O
.	O
.	O
xid	O
zi1	O
.	O
.	O
.	O
zik	O
vk	O
.	O
.	O
.	O
.	O
.	O
.	O
yili	O
n	O
w	O
(	O
b	O
)	O
figure	O
27.17	O
vector	O
nodes	O
expanded	O
out	O
.	O
(	O
a	O
)	O
categorical	B
pca	O
with	O
inputs	O
and	O
exchangeable	B
outputs	O
.	O
(	O
b	O
)	O
same	O
as	O
(	O
a	O
)	O
,	O
but	O
with	O
the	O
key	O
difference	O
from	O
a	O
neural	O
net	O
is	O
that	O
information	B
can	O
ﬂow	O
between	O
the	O
yil	O
’	O
s	O
via	O
the	O
latent	B
bottleneck	O
layer	O
zi	O
.	O
this	O
should	O
work	O
better	O
than	O
a	O
conventional	O
neural	O
net	O
when	O
the	O
output	O
labels	O
are	O
highly	O
correlated	O
,	O
even	O
after	O
conditioning	B
on	O
the	O
features	B
;	O
this	O
problem	O
frequently	O
arises	O
in	O
multi	B
label	I
classiﬁcation	I
.	O
note	O
that	O
we	O
could	O
allow	O
a	O
direct	O
xi	O
to	O
yi	O
arc	O
,	O
but	O
this	O
would	O
require	O
too	O
many	O
parameters	O
if	O
the	O
number	O
of	O
labels	O
is	O
large.6	O
we	O
can	O
ﬁt	O
this	O
model	O
with	O
a	O
small	O
modiﬁcation	O
of	O
the	O
variational	O
em	O
algorithm	O
in	O
section	O
12.4.	O
if	O
we	O
use	O
this	O
model	O
for	O
regression	B
,	O
rather	O
than	O
classiﬁcation	B
,	O
we	O
can	O
perform	O
the	O
e	O
step	O
exactly	O
,	O
by	O
modifying	O
the	O
em	O
algorithm	O
for	O
factor	B
analysis	I
.	O
(	O
ma	O
et	O
al	O
.	O
1997	O
)	O
reports	O
that	O
this	O
method	O
converges	O
faster	O
than	O
standard	O
backpropagation	O
.	O
we	O
can	O
also	O
extend	O
the	O
model	O
so	O
that	O
the	O
prior	O
on	O
zi	O
is	O
a	O
mixture	O
of	O
gaussians	O
using	O
input-	O
if	O
the	O
output	O
is	O
gaussian	O
,	O
this	O
corresponds	O
to	O
a	O
mixture	O
of	O
discriminative	O
dependent	O
means	O
.	O
factor	B
analysers	O
(	O
fokoue	O
2005	O
;	O
zhou	O
and	O
liu	O
2008	O
)	O
.	O
if	O
the	O
output	O
is	O
categorical	B
,	O
this	O
would	O
be	O
an	O
(	O
as	O
yet	O
unpublished	O
)	O
model	O
,	O
which	O
we	O
could	O
call	O
“	O
discriminative	B
mixtures	O
of	O
categorical	B
factor	O
analyzers	O
”	O
.	O
27.5	O
lvms	O
for	O
graph-structured	O
data	O
another	O
source	O
of	O
discrete	B
data	O
is	O
when	O
modeling	O
graph	B
or	O
network	O
structures	O
.	O
to	O
see	O
the	O
connection	O
,	O
recall	B
that	O
any	O
graph	B
on	O
d	O
nodes	B
can	O
be	O
represented	O
as	O
a	O
d	O
×	O
d	O
adjacency	B
matrix	I
g	O
,	O
where	O
g	O
(	O
i	O
,	O
j	O
)	O
=	O
1	O
iff	B
there	O
is	O
an	O
edge	O
from	O
node	O
i	O
to	O
node	O
j.	O
such	O
matrices	O
are	O
binary	O
,	O
and	O
often	O
very	O
sparse	B
.	O
see	O
figure	O
27.19	O
for	O
an	O
example	O
.	O
graphs	O
arise	O
in	O
many	O
application	O
areas	O
,	O
such	O
as	O
modeling	O
social	B
networks	I
,	O
protein-protein	B
interaction	I
networks	I
,	O
or	O
patterns	O
of	O
disease	B
transmission	I
between	O
people	O
or	O
animals	O
.	O
there	O
are	O
usually	O
two	O
primary	O
goals	O
when	O
analysing	O
such	O
data	O
:	O
ﬁrst	O
,	O
try	O
to	O
discover	O
some	O
“	O
interesting	O
6.	O
a	O
non-probabilistic	O
version	O
of	O
this	O
idea	O
,	O
using	O
squared	B
loss	I
,	O
was	O
proposed	O
in	O
(	O
ji	O
et	O
al	O
.	O
2010	O
)	O
.	O
this	O
is	O
similar	B
to	O
a	O
linear	O
feed-forward	O
neural	B
network	I
with	O
an	O
additional	O
edge	O
from	O
xi	O
directly	O
to	O
yi	O
.	O
27.5.	O
lvms	O
for	O
graph-structured	O
data	O
971	O
3	O
6	O
9	O
1	O
4	O
7	O
2	O
5	O
8	O
(	O
a	O
)	O
1	O
6	O
4	O
5	O
8	O
3	O
7	O
2	O
9	O
(	O
b	O
)	O
figure	O
27.18	O
(	O
a	O
)	O
a	O
directed	B
graph	O
.	O
(	O
b	O
)	O
the	O
same	O
graph	B
,	O
with	O
the	O
nodes	B
partitioned	O
into	O
3	O
groups	O
,	O
making	O
the	O
block	O
structure	O
more	O
apparent	O
.	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
1	O
6	O
4	O
8	O
2	O
3	O
5	O
9	O
7	O
z	O
0.1	O
0.1	O
0.9	O
0.9	O
0.1	O
0.1	O
0.1	O
0.9	O
0.1	O
η	O
1	O
6	O
4	O
8	O
2	O
3	O
5	O
9	O
7	O
r	O
1	O
6	O
4	O
8	O
2	O
3	O
5	O
9	O
7	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
27.19	O
(	O
a	O
)	O
adjacency	B
matrix	I
for	O
the	O
graph	B
in	O
figure	O
27.18	O
(	O
a	O
)	O
.	O
(	O
b	O
)	O
rows	O
and	O
columns	O
are	O
shown	O
permuted	O
to	O
show	O
the	O
block	O
structure	O
.	O
we	O
also	O
sketch	O
of	O
how	O
the	O
stochastic	B
block	I
model	I
can	O
generate	O
this	O
graph	B
.	O
from	O
figure	O
1	O
of	O
(	O
kemp	O
et	O
al	O
.	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
charles	O
kemp	O
.	O
structure	O
”	O
in	O
the	O
graph	B
,	O
such	O
as	O
clusters	B
or	O
communities	O
;	O
second	O
,	O
try	O
to	O
predict	O
which	O
links	O
might	O
occur	O
in	O
the	O
future	O
(	O
e.g.	O
,	O
who	O
will	O
make	O
friends	O
with	O
whom	O
)	O
.	O
below	O
we	O
summarize	O
some	O
models	O
that	O
have	O
been	O
proposed	O
for	O
these	O
tasks	O
,	O
some	O
of	O
which	O
are	O
related	O
to	O
lda	O
.	O
futher	O
details	O
on	O
these	O
and	O
other	O
approaches	O
can	O
be	O
found	O
in	O
e.g.	O
,	O
(	O
goldenberg	O
et	O
al	O
.	O
2009	O
)	O
and	O
the	O
references	O
therein	O
.	O
27.5.1	O
stochastic	B
block	I
model	I
in	O
figure	O
27.18	O
(	O
a	O
)	O
we	O
show	O
a	O
directed	B
graph	O
on	O
9	O
nodes	B
.	O
there	O
is	O
no	O
apparent	O
structure	O
.	O
however	O
,	O
if	O
we	O
look	O
more	O
deeply	O
,	O
we	O
see	O
it	O
is	O
possible	O
to	O
partition	O
the	O
nodes	B
into	O
three	O
groups	O
or	O
blocks	O
,	O
b1	O
=	O
{	O
1	O
,	O
4	O
,	O
6	O
}	O
,	O
b2	O
=	O
{	O
2	O
,	O
3	O
,	O
5	O
,	O
8	O
}	O
,	O
and	O
b3	O
=	O
{	O
7	O
,	O
9	O
}	O
,	O
such	O
that	O
most	O
of	O
the	O
connections	O
go	O
from	O
nodes	B
in	O
b1	O
to	O
b2	O
,	O
or	O
from	O
b2	O
to	O
b3	O
,	O
or	O
from	O
b3	O
to	O
b1	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
27.18	O
(	O
b	O
)	O
.	O
972	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
m	O
e	O
t	O
s	O
y	O
s	O
l	O
a	O
n	O
o	O
i	O
t	O
l	O
a	O
e	O
r	O
x	O
i	O
r	O
t	O
a	O
m	O
d	O
e	O
t	O
r	O
o	O
s	O
a	O
b	O
c	O
d	O
a	O
d	O
b	O
c	O
a	O
b	O
c	O
d	O
a	O
b	O
d	O
g	O
e	O
h	O
c	O
f	O
a	O
c	O
b	O
d	O
e	O
a	O
b	O
c	O
d	O
a	O
b	O
c	O
d	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
a	O
edcb	O
a	O
b	O
c	O
d	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
a	O
b	O
c	O
d	O
e	O
figure	O
27.20	O
some	O
examples	O
of	O
graphs	O
generated	O
using	O
the	O
stochastic	B
block	I
model	I
with	O
different	O
kinds	O
of	O
connectivity	O
patterns	O
between	O
the	O
blocks	O
.	O
the	O
abstract	O
graph	B
(	O
between	O
blocks	O
)	O
represent	O
a	O
ring	O
,	O
a	O
dominance	O
hierarchy	O
,	O
a	O
common-cause	O
structure	O
,	O
and	O
a	O
common-effect	O
structure	O
.	O
from	O
figure	O
4	O
of	O
(	O
kemp	O
et	O
al	O
.	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
charles	O
kemp	O
.	O
the	O
problem	O
is	O
easier	O
to	O
understand	O
if	O
we	O
plot	O
the	O
adjacency	O
matrices	O
.	O
figure	O
27.19	O
(	O
a	O
)	O
shows	O
the	O
matrix	O
for	O
the	O
graph	B
with	O
the	O
nodes	B
in	O
their	O
original	O
ordering	O
.	O
figure	O
27.19	O
(	O
b	O
)	O
shows	O
the	O
matrix	O
for	O
the	O
graph	B
with	O
the	O
nodes	B
in	O
their	O
permtuted	O
ordering	O
.	O
it	O
is	O
clear	O
that	O
there	O
is	O
block	O
structure	O
.	O
we	O
can	O
make	O
a	O
generative	O
model	O
of	O
block	O
structured	O
graphs	O
as	O
follows	O
.	O
first	O
,	O
for	O
every	O
node	O
,	O
sample	O
a	O
latent	B
block	O
qi	O
∼	O
cat	O
(	O
π	O
)	O
,	O
where	O
πk	O
is	O
the	O
probability	O
of	O
choosing	O
block	O
k	O
,	O
for	O
k	O
=	O
1	O
:	O
k.	O
second	O
,	O
choose	O
the	O
probability	O
of	O
connecting	O
group	O
a	O
to	O
group	O
b	O
,	O
for	O
all	B
pairs	I
of	O
groups	O
;	O
let	O
us	O
denote	O
this	O
probability	O
by	O
ηa	O
,	O
b	O
.	O
this	O
can	O
come	O
from	O
a	O
beta	O
prior	O
.	O
finally	O
,	O
generate	O
each	O
edge	O
rij	O
using	O
the	O
following	O
model	O
:	O
p	O
(	O
rij	O
=	O
r|qi	O
=	O
a	O
,	O
qj	O
=	O
b	O
,	O
η	O
)	O
=	O
ber	O
(	O
r|ηa	O
,	O
b	O
)	O
(	O
27.75	O
)	O
this	O
is	O
called	O
the	O
stochastic	B
block	I
model	I
(	O
nowicki	O
and	O
snijders	O
2001	O
)	O
.	O
figure	O
27.21	O
(	O
a	O
)	O
illustrates	O
the	O
model	O
as	O
a	O
dgm	O
,	O
and	O
figure	O
27.19	O
(	O
c	O
)	O
illustrates	O
how	O
this	O
model	O
can	O
be	O
used	O
to	O
cluster	O
the	O
nodes	B
in	O
our	O
example	O
.	O
note	O
that	O
this	O
is	O
quite	O
different	O
from	O
a	O
conventional	O
clustering	B
problem	O
.	O
for	O
example	O
,	O
we	O
see	O
that	O
all	O
the	O
nodes	B
in	O
block	O
3	O
are	O
grouped	O
together	O
,	O
even	O
though	O
there	O
are	O
no	O
connections	O
between	O
them	O
.	O
what	O
they	O
share	O
is	O
the	O
property	O
that	O
they	O
“	O
like	O
to	O
”	O
connect	O
to	O
nodes	B
in	O
block	O
1	O
,	O
and	O
to	O
receive	O
connections	O
from	O
nodes	B
in	O
block	O
2.	O
figure	O
27.20	O
illustrates	O
the	O
power	O
of	O
the	O
model	O
for	O
generating	O
many	O
different	O
kinds	O
of	O
graph	B
structure	O
.	O
for	O
example	O
,	O
some	O
social	B
networks	I
have	O
hierarchical	O
structure	O
,	O
which	O
can	O
be	O
modeled	O
by	O
clustering	B
people	O
into	O
different	O
social	O
strata	O
,	O
whereas	O
others	O
consist	O
of	O
a	O
set	O
of	O
cliques	B
.	O
unlike	O
a	O
standard	O
mixture	O
model	O
,	O
it	O
is	O
not	O
possible	O
to	O
ﬁt	O
this	O
model	O
using	O
exact	O
em	O
,	O
because	O
all	O
the	O
latent	B
qi	O
variables	O
become	O
correlated	O
.	O
however	O
,	O
one	O
can	O
use	O
variational	O
em	O
(	O
airoldi	O
et	O
al	O
.	O
27.5.	O
lvms	O
for	O
graph-structured	O
data	O
973	O
π	O
qi	O
qj	O
ri	O
,	O
j	O
i	O
j	O
ηa	O
,	O
b	O
(	O
a	O
)	O
α	O
πj	O
qi←j	O
ri	O
,	O
j	O
i	O
j	O
ηa	O
,	O
b	O
πi	O
qi→j	O
(	O
b	O
)	O
figure	O
27.21	O
(	O
a	O
)	O
stochastic	B
block	I
model	I
.	O
(	O
b	O
)	O
mixed	B
membership	I
stochastic	I
block	I
model	I
.	O
2008	O
)	O
,	O
collapsed	O
gibbs	O
sampling	O
(	O
kemp	O
et	O
al	O
.	O
2006	O
)	O
,	O
etc	O
.	O
we	O
omit	O
the	O
details	O
(	O
which	O
are	O
similar	B
to	O
the	O
lda	O
case	O
)	O
.	O
in	O
(	O
kemp	O
et	O
al	O
.	O
2006	O
)	O
,	O
they	O
lifted	O
the	O
restriction	O
that	O
the	O
number	O
of	O
blocks	O
k	O
be	O
ﬁxed	O
,	O
by	O
replacing	O
the	O
dirichlet	O
prior	O
on	O
π	O
by	O
a	O
dirichlet	O
process	O
(	O
see	O
section	O
25.2.2	O
)	O
.	O
this	O
is	O
known	O
as	O
the	O
inﬁnite	B
relational	I
model	I
.	O
see	O
section	O
27.6.1	O
for	O
details	O
.	O
if	O
we	O
have	O
features	B
associated	O
with	O
each	O
node	O
,	O
we	O
can	O
make	O
a	O
discriminative	B
version	O
of	O
this	O
model	O
,	O
for	O
example	O
by	O
deﬁning	O
p	O
(	O
rij	O
=	O
r|qi	O
=	O
a	O
,	O
qj	O
=	O
b	O
,	O
xi	O
,	O
xj	O
,	O
θ	O
)	O
=	O
ber	O
(	O
r|wt	O
a	O
,	O
bf	O
(	O
xi	O
,	O
xj	O
)	O
)	O
(	O
27.76	O
)	O
where	O
f	O
(	O
xi	O
,	O
xj	O
)	O
is	O
some	O
way	O
of	O
combining	O
the	O
feature	O
vectors	O
.	O
for	O
example	O
,	O
we	O
could	O
use	O
concatenation	O
,	O
[	O
xi	O
,	O
xj	O
]	O
,	O
or	O
elementwise	O
product	O
xi	O
⊗	O
xj	O
as	O
in	O
supervised	O
lda	O
.	O
the	O
overall	O
model	O
is	O
like	O
a	O
relational	O
extension	O
of	O
the	O
mixture	B
of	I
experts	I
model	O
.	O
27.5.2	O
mixed	B
membership	I
stochastic	I
block	I
model	I
in	O
(	O
airoldi	O
et	O
al	O
.	O
2008	O
)	O
,	O
they	O
lifted	O
the	O
restriction	O
that	O
each	O
node	O
only	O
belong	O
to	O
one	O
cluster	O
.	O
that	O
is	O
,	O
they	O
replaced	O
qi	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
with	O
πi	O
∈	O
sk	O
.	O
this	O
is	O
known	O
as	O
the	O
mixed	B
membership	I
stochastic	I
block	I
model	I
,	O
and	O
is	O
similar	B
in	O
spirit	O
to	O
fuzzy	B
clustering	I
or	O
soft	B
clustering	I
.	O
note	O
that	O
πik	O
is	O
not	O
the	O
same	O
as	O
p	O
(	O
zi	O
=	O
k|d	O
)	O
;	O
the	O
former	O
represents	O
ontological	B
uncertainty	I
(	O
to	O
what	O
degree	B
does	O
each	O
object	O
belong	O
to	O
a	O
cluster	O
)	O
wheras	O
the	O
latter	O
represents	O
epistemological	B
uncertainty	I
(	O
which	O
cluster	O
does	O
an	O
object	O
belong	O
to	O
)	O
.	O
if	O
we	O
want	O
to	O
combine	O
epistemological	O
and	O
ontological	B
uncertainty	I
,	O
we	O
can	O
compute	O
p	O
(	O
πi|d	O
)	O
.	O
in	O
more	O
detail	O
,	O
the	O
generative	O
process	O
is	O
as	O
follows	O
.	O
first	O
,	O
each	O
node	O
picks	O
a	O
distribution	O
over	O
blocks	O
,	O
πi	O
∼	O
dir	O
(	O
α	O
)	O
.	O
second	O
,	O
choose	O
the	O
probability	O
of	O
connecting	O
group	O
a	O
to	O
group	O
b	O
,	O
for	O
all	B
pairs	I
of	O
groups	O
,	O
ηa	O
,	O
b	O
∼	O
β	O
(	O
α	O
,	O
β	O
)	O
.	O
third	O
,	O
for	O
each	O
edge	O
,	O
sample	O
two	O
discrete	B
variables	O
,	O
one	O
for	O
each	O
direction	O
:	O
qi→j	O
∼	O
cat	O
(	O
πi	O
)	O
,	O
qi←j	O
∼	O
cat	O
(	O
πj	O
)	O
finally	O
,	O
generate	O
each	O
edge	O
rij	O
using	O
the	O
following	O
model	O
:	O
p	O
(	O
rij	O
=	O
1|qi→j	O
=	O
a	O
,	O
qi←j	O
=	O
b	O
,	O
η	O
)	O
=	O
ηa	O
,	O
b	O
(	O
27.77	O
)	O
(	O
27.78	O
)	O
974	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
12	O
7	O
outcasts	O
5	O
6	O
waverers	O
15	O
loyal	O
opposition	O
13	O
11	O
8	O
1	O
16	O
10	O
young	O
turks	O
9	O
17	O
3	O
14	O
4	O
18	O
2	O
1	O
ambrose	O
2	O
boniface	O
3	O
mark	O
4	O
winfrid	O
5	O
elias	O
6	O
basil	O
7	O
simplicius	O
8	O
berthold	O
9	O
john	O
bosco	O
10	O
victor	O
11	O
bonaventure	O
12	O
amand	O
13	O
louis	O
14	O
albert	O
15ramuald	O
16	O
peter	O
17	O
gregory	O
18	O
hugh	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
27.22	O
(	O
a	O
)	O
who-likes-whom	O
graph	B
for	O
sampson	O
’	O
s	O
monks	B
.	O
one	O
of	O
three	O
groups	O
.	O
from	O
figures	O
2-3	O
of	O
(	O
airoldi	O
et	O
al	O
.	O
2008	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
edo	O
airoldi	O
.	O
(	O
b	O
)	O
mixed	O
membership	O
of	O
each	O
monk	O
in	O
see	O
figure	O
27.21	O
(	O
b	O
)	O
for	O
the	O
dgm	O
.	O
unlike	O
the	O
regular	B
stochastic	O
block	O
model	O
,	O
each	O
node	O
can	O
play	O
a	O
different	O
role	O
,	O
depending	O
on	O
who	O
it	O
is	O
connecting	O
to	O
.	O
as	O
an	O
illustration	O
of	O
this	O
,	O
we	O
will	O
consider	O
a	O
data	O
set	O
that	O
is	O
widely	O
used	O
in	O
the	O
social	B
networks	I
analysis	O
literature	O
.	O
the	O
data	O
concerns	O
who-likes-whom	O
amongst	O
of	O
group	O
of	O
18	O
monks	B
.	O
it	O
was	O
collected	O
by	O
hand	O
in	O
1968	O
by	O
sampson	O
(	O
sampson	O
1968	O
)	O
over	O
a	O
period	B
of	O
months	O
.	O
(	O
these	O
days	O
,	O
in	O
the	O
era	O
of	O
social	O
media	O
such	O
as	O
facebook	O
,	O
a	O
social	O
network	O
with	O
only	O
18	O
people	O
is	O
trivially	O
small	O
,	O
but	O
the	O
methods	O
we	O
are	O
discussing	O
can	O
be	O
made	O
to	O
scale	O
.	O
)	O
figure	O
27.22	O
(	O
a	O
)	O
plots	O
the	O
raw	O
data	O
,	O
and	O
figure	O
27.22	O
(	O
b	O
)	O
plots	O
e	O
[	O
π	O
]	O
i	O
for	O
each	O
monk	O
,	O
where	O
k	O
=	O
3.	O
we	O
see	O
that	O
most	O
of	O
the	O
monk	O
belong	O
to	O
one	O
of	O
the	O
three	O
clusters	B
,	O
known	O
as	O
the	O
“	O
young	O
turks	O
”	O
,	O
the	O
“	O
outcasts	O
”	O
and	O
the	O
“	O
loyal	O
opposition	O
”	O
.	O
however	O
,	O
some	O
individuals	O
,	O
notably	O
monk	O
15	O
,	O
belong	O
to	O
two	O
clusters	B
;	O
sampson	O
called	O
these	O
monks	B
the	O
“	O
waverers	O
”	O
.	O
it	O
is	O
interesting	O
to	O
see	O
that	O
the	O
model	O
can	O
recover	O
the	O
same	O
kinds	O
of	O
insights	O
as	O
sampson	O
derived	O
by	O
hand	O
.	O
one	O
prevalent	O
problem	O
in	O
social	O
network	O
analysis	O
is	O
missing	B
data	I
.	O
for	O
example	O
,	O
if	O
rij	O
=	O
0	O
,	O
it	O
may	O
be	O
due	O
to	O
the	O
fact	O
that	O
person	O
i	O
and	O
j	O
have	O
not	O
had	O
an	O
opportunity	O
to	O
interact	O
,	O
or	O
that	O
data	O
is	O
not	O
available	O
for	O
that	O
interaction	O
,	O
as	O
opposed	O
to	O
the	O
fact	O
that	O
these	O
people	O
don	O
’	O
t	O
want	O
to	O
interact	O
.	O
in	O
other	O
words	O
,	O
absence	O
of	O
evidence	B
is	O
not	O
evidence	O
of	O
absence	O
.	O
we	O
can	O
model	O
this	O
by	O
modifying	O
the	O
observation	B
model	I
so	O
that	O
with	O
probability	O
ρ	O
,	O
we	O
generate	O
a	O
0	O
from	O
the	O
background	O
model	O
,	O
and	O
we	O
only	O
force	O
the	O
model	O
to	O
explain	O
observed	O
0s	O
with	O
probability	O
1	O
−	O
ρ.	O
in	O
other	O
words	O
,	O
we	O
robustify	O
the	O
observation	B
model	I
to	O
allow	O
for	O
outliers	B
,	O
as	O
follows	O
:	O
p	O
(	O
rij	O
=	O
r|qi→j	O
=	O
a	O
,	O
qi←j	O
=	O
b	O
,	O
η	O
)	O
=	O
ρδ0	O
(	O
r	O
)	O
+	O
(	O
1−	O
ρ	O
)	O
ber	O
(	O
r|ηa	O
,	O
b	O
)	O
(	O
27.79	O
)	O
see	O
(	O
airoldi	O
et	O
al	O
.	O
2008	O
)	O
for	O
details	O
.	O
27.5.3	O
relational	B
topic	I
model	I
in	O
many	O
cases	O
,	O
the	O
nodes	B
in	O
our	O
network	O
have	O
atttributes	O
.	O
for	O
example	O
,	O
if	O
the	O
nodes	B
represent	O
academic	O
papers	O
,	O
and	O
the	O
edges	B
represent	O
citations	O
,	O
then	O
the	O
attributes	B
include	O
the	O
text	O
of	O
the	O
document	O
itself	O
.	O
it	O
is	O
therefore	O
desirable	O
to	O
create	O
a	O
model	O
that	O
can	O
explain	O
the	O
text	O
and	O
the	O
link	O
structure	O
concurrently	O
.	O
such	O
a	O
model	O
can	O
predict	O
links	O
given	O
text	O
,	O
or	O
even	O
vice	O
versa	O
.	O
the	O
relational	B
topic	I
model	I
(	O
rtm	O
)	O
(	O
chang	O
and	O
blei	O
2010	O
)	O
is	O
one	O
way	O
to	O
do	O
this	O
.	O
this	O
is	O
a	O
27.6.	O
lvms	O
for	O
relational	O
data	O
975	O
b	O
α	O
πi	O
yil	O
qil	O
¯qi	O
yjl	O
qjl	O
πj	O
¯qj	O
w	O
rij	O
i	O
j	O
figure	O
27.23	O
dgm	O
for	O
the	O
relational	B
topic	I
model	I
.	O
simple	O
extension	O
of	O
supervised	O
lda	O
(	O
section	O
27.4.4.1	O
)	O
,	O
where	O
the	O
response	B
variable	I
rij	O
(	O
which	O
represents	O
whether	O
there	O
is	O
an	O
edge	O
between	O
nodes	B
i	O
and	O
j	O
)	O
is	O
modeled	O
as	O
follows	O
:	O
p	O
(	O
rij	O
=	O
1|qi	O
,	O
qj	O
,	O
θ	O
)	O
=	O
sigm	O
(	O
wt	O
(	O
qi	O
⊗	O
qj	O
)	O
+w	O
0	O
)	O
(	O
cid:7	O
)	O
li	O
(	O
27.80	O
)	O
i=1	O
qilk	O
.	O
see	O
recall	B
that	O
qi	O
is	O
the	O
empirical	O
topic	O
distribution	O
for	O
document	O
i	O
,	O
qik	O
(	O
cid:2	O
)	O
1	O
figure	O
27.23	O
li	O
note	O
that	O
it	O
is	O
important	O
that	O
rij	O
depend	O
on	O
the	O
actual	O
topics	O
chosen	O
,	O
qi	O
and	O
qj	O
,	O
and	O
not	O
on	O
the	O
topic	B
distributions	O
,	O
πi	O
and	O
πj	O
,	O
otherwise	O
predictive	B
performance	O
is	O
not	O
as	O
good	O
.	O
the	O
if	O
rij	O
is	O
a	O
child	O
of	O
πi	O
and	O
πj	O
,	O
it	O
will	O
be	O
treated	O
as	O
just	O
intuitive	O
reason	O
for	O
this	O
is	O
as	O
follows	O
:	O
another	O
word	O
,	O
similar	B
to	O
the	O
qil	O
’	O
s	O
and	O
yil	O
’	O
s	O
;	O
but	O
since	O
there	O
are	O
many	O
more	O
words	O
than	O
edges	B
,	O
the	O
graph	B
structure	O
information	B
will	O
get	O
“	O
washed	O
out	O
”	O
.	O
by	O
making	O
rij	O
a	O
child	O
of	O
qi	O
and	O
qj	O
,	O
the	O
graph	B
information	O
can	O
inﬂuence	O
the	O
choice	O
of	O
topics	O
more	O
directly	O
.	O
one	O
can	O
ﬁt	O
this	O
model	O
in	O
a	O
manner	O
similar	B
to	O
slda	O
.	O
see	O
(	O
chang	O
and	O
blei	O
2010	O
)	O
for	O
details	O
.	O
the	O
method	O
does	O
better	O
at	O
predicting	O
missing	B
links	O
than	O
the	O
simpler	O
approach	O
of	O
ﬁrst	O
ﬁtting	O
an	O
lda	O
model	O
,	O
and	O
then	O
using	O
the	O
qi	O
’	O
s	O
as	O
inputs	O
to	O
a	O
logistic	B
regression	I
problem	O
.	O
the	O
reason	O
is	O
analogous	O
to	O
the	O
superiority	O
of	O
partial	B
least	I
squares	I
(	O
section	O
12.5.2	O
)	O
to	O
pca+	O
linear	B
regression	I
,	O
namely	O
that	O
the	O
rtm	O
learns	O
a	O
latent	B
space	O
that	O
is	O
forced	O
to	O
be	O
predictive	B
of	O
the	O
graph	B
structure	O
and	O
words	O
,	O
whereas	O
lda	O
might	O
learn	O
a	O
latent	B
space	O
that	O
is	O
not	O
useful	O
for	O
predicting	O
the	O
graph	B
.	O
27.6	O
lvms	O
for	O
relational	O
data	O
graphs	O
can	O
be	O
used	O
to	O
represent	O
data	O
which	O
represents	O
the	O
relation	B
amongst	O
variables	O
of	O
a	O
certain	O
type	O
,	O
e.g.	O
,	O
friendship	O
relationships	O
between	O
people	O
.	O
but	O
often	O
we	O
have	O
multiple	O
types	O
of	O
objects	O
,	O
and	O
multiple	O
types	O
of	O
relations	O
.	O
for	O
example	O
,	O
figure	O
27.24	O
illustrates	O
two	O
relations	O
,	O
one	O
between	O
people	O
and	O
people	O
,	O
and	O
one	O
between	O
people	O
and	O
movies	O
.	O
in	O
general	O
,	O
we	O
deﬁne	O
a	O
k-ary	O
relation	B
r	O
as	O
a	O
subset	O
of	O
k-tuples	O
of	O
the	O
appropriate	O
types	O
:	O
r	O
⊆	O
t1	O
×	O
t2	O
×	O
···	O
×t	O
k	O
(	O
27.81	O
)	O
976	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
figure	O
27.24	O
example	O
of	O
relational	O
data	O
.	O
there	O
are	O
two	O
types	O
of	O
objects	O
,	O
people	O
and	O
movies	O
;	O
one	O
2-ary	O
relation	B
,	O
friends	O
:	O
people	O
×	O
people	O
→	O
{	O
0	O
,	O
1	O
}	O
and	O
one	O
2-ary	O
function	O
,	O
rates	O
:	O
people	O
×	O
movie	O
→	O
r.	O
age	O
and	O
sex	O
are	O
attributes	B
(	O
unary	O
functions	O
)	O
of	O
the	O
people	O
class	O
.	O
where	O
ti	O
are	O
sets	O
or	O
types	O
.	O
a	O
binary	O
,	O
pairwise	O
or	O
dyadic	B
relation	O
is	O
a	O
relation	B
deﬁned	O
on	O
pairs	O
of	O
objects	O
.	O
for	O
example	O
,	O
the	O
seen	O
relation	B
between	O
people	O
and	O
movies	O
might	O
be	O
represented	O
as	O
the	O
set	O
of	O
movies	O
that	O
people	O
have	O
seen	O
.	O
we	O
can	O
either	O
represent	O
this	O
explicitly	O
as	O
a	O
set	O
,	O
such	O
as	O
seen	O
=	O
{	O
(	O
bob	O
,	O
starwars	O
)	O
,	O
(	O
bob	O
,	O
tombraider	O
)	O
,	O
(	O
alice	O
,	O
jaws	O
)	O
}	O
or	O
implicitly	O
,	O
using	O
an	O
indicator	B
function	I
for	O
the	O
set	O
:	O
seen	O
(	O
bob	O
,	O
starwars	O
)	O
=1	O
,	O
seen	O
(	O
bob	O
,	O
tombraider	O
)	O
=1	O
,	O
seen	O
(	O
alice	O
,	O
jaws	O
)	O
=1	O
a	O
relation	B
between	O
two	O
entities	O
of	O
types	O
t	O
1	O
and	O
t	O
2	O
can	O
be	O
represented	O
as	O
a	O
binary	O
function	O
r	O
:	O
t	O
1×	O
t	O
2	O
→	O
{	O
0	O
,	O
1	O
}	O
,	O
and	O
hence	O
as	O
a	O
binary	O
matrix	O
.	O
this	O
can	O
also	O
be	O
represented	O
as	O
a	O
bipartite	B
graph	I
,	O
in	O
which	O
we	O
have	O
nodes	B
of	O
two	O
types	O
.	O
if	O
t	O
1	O
=	O
t	O
2	O
,	O
this	O
becomes	O
a	O
regular	B
directed	O
graph	B
,	O
as	O
in	O
section	O
27.5.	O
however	O
,	O
there	O
are	O
some	O
situations	O
that	O
are	O
not	O
so	O
easily	O
modelled	O
by	O
graphs	O
,	O
but	O
which	O
can	O
still	O
be	O
modelled	O
by	O
relations	O
.	O
for	O
example	O
,	O
we	O
might	O
have	O
a	O
ternary	O
relation	B
,	O
r	O
:	O
t	O
1	O
×	O
t	O
1	O
×	O
t	O
2	O
→	O
{	O
0	O
,	O
1	O
}	O
,	O
where	O
,	O
say	O
,	O
r	O
(	O
i	O
,	O
j	O
,	O
k	O
)	O
=	O
1	O
iff	B
protein	O
i	O
interacts	O
with	O
protein	O
j	O
when	O
chemical	O
k	O
is	O
present	O
.	O
this	O
can	O
be	O
modelled	O
by	O
a	O
3d	O
binary	O
matrix	O
.	O
we	O
will	O
give	O
some	O
examples	O
of	O
this	O
in	O
section	O
27.6.1.	O
making	O
probabilistic	O
models	O
of	O
relational	O
data	O
is	O
called	O
statistical	B
relational	I
learning	I
(	O
getoor	O
and	O
taskar	O
2007	O
)	O
.	O
one	O
approach	O
is	O
to	O
directly	O
model	O
the	O
relationship	O
between	O
the	O
variables	O
using	O
graphical	O
models	O
;	O
this	O
is	O
known	O
as	O
probabilistic	B
relational	I
modeling	I
.	O
another	O
approach	O
is	O
to	O
use	O
latent	B
variable	I
models	I
,	O
as	O
we	O
discuss	O
below	O
.	O
27.6.1	O
inﬁnite	B
relational	I
model	I
it	O
is	O
straightforward	O
to	O
extend	O
the	O
stochastic	B
block	I
model	I
to	O
model	O
relational	O
data	O
:	O
we	O
just	O
i	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
kt	O
}	O
with	O
each	O
entity	O
i	O
of	O
each	O
type	O
t.	O
we	O
then	O
deﬁne	O
associate	O
a	O
latent	O
variable	O
qt	O
the	O
probability	O
of	O
the	O
relation	O
holding	O
between	O
speciﬁc	O
entities	O
by	O
looking	O
up	O
the	O
probability	O
of	O
the	O
relation	O
holding	O
between	O
entities	O
of	O
that	O
type	O
.	O
for	O
example	O
,	O
if	O
r	O
:	O
t	O
1	O
×	O
t	O
1	O
×	O
t	O
2	O
→	O
{	O
0	O
,	O
1	O
}	O
,	O
we	O
have	O
p	O
(	O
r	O
(	O
i	O
,	O
j	O
,	O
k	O
)	O
=	O
1|q1	O
k	O
=	O
c	O
,	O
η	O
)	O
=	O
ηa	O
,	O
b	O
,	O
c	O
i	O
=	O
a	O
,	O
q1	O
j	O
=	O
b	O
,	O
q2	O
(	O
27.82	O
)	O
if	O
we	O
allow	O
the	O
number	O
of	O
clusters	B
kt	O
for	O
each	O
type	O
to	O
be	O
unbounded	O
,	O
by	O
using	O
a	O
dirichlet	O
pro-	O
cess	O
,	O
the	O
model	O
is	O
called	O
the	O
inﬁnite	B
relational	I
model	I
(	O
irm	O
)	O
(	O
kemp	O
et	O
al	O
.	O
2006	O
)	O
.	O
an	O
essentially	O
27.6.	O
lvms	O
for	O
relational	O
data	O
977	O
	O
	O
	O
	O
interact	O
with	O
	O
	O
(	O
cid:1	O
)	O
	O
	O
	O
affects	O
,	O
causes	O
causes	O
affects	O
affects	O
,	O
causes	O
,	O
complicates	O
causes	O
,	O
complicates	O
affects	O
,	O
complicates	O
disrupts	O
affects	O
,	O
complicates	O
,	O
process	O
of	O
,	O
manifestation	O
of	O
affects	O
,	O
process	O
of	O
,	O
result	O
of	O
,	O
manifestation	O
of	O
	O
	O
result	O
of	O
	O
	O
	O
	O
	O
	O
	O
	O
affects	O
,	O
process	O
of	O
,	O
result	O
of	O
result	O
of	O
	O
	O
	O
	O
	O
result	O
of	O
affects	O
,	O
process	O
of	O
	O
	O
	O
manifestation	O
of	O
,	O
associated	O
with	O
manifestation	O
of	O
affects	O
,	O
process	O
of	O
manifestation	O
of	O
,	O
associated	O
with	O
	O
	O
	O
figure	O
27.25	O
illustration	O
of	O
an	O
ontology	B
learned	O
by	O
irm	O
applied	O
to	O
the	O
uniﬁed	O
medical	O
language	O
system	O
.	O
the	O
boxes	O
represent	O
7	O
of	O
the	O
14	O
concept	B
clusters	O
.	O
predicates	O
that	O
belong	O
to	O
the	O
same	O
cluster	O
are	O
grouped	O
together	O
,	O
and	O
associated	O
with	O
edges	B
to	O
which	O
they	O
pertain	O
.	O
all	O
links	O
with	O
weight	O
above	O
0.8	O
have	O
been	O
included	O
.	O
from	O
figure	O
9	O
of	O
(	O
kemp	O
et	O
al	O
.	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
charles	O
kemp	O
.	O
identical	O
model	O
,	O
under	O
the	O
name	O
inﬁnite	B
hidden	I
relational	I
model	I
(	O
ihrm	O
)	O
,	O
was	O
concurrently	O
proposed	O
in	O
(	O
xu	O
et	O
al	O
.	O
2006	O
)	O
.	O
we	O
can	O
ﬁt	O
this	O
model	O
with	O
variational	O
bayes	O
(	O
xu	O
et	O
al	O
.	O
2006	O
,	O
2007	O
)	O
or	O
collapsed	O
gibbs	O
sampling	O
(	O
kemp	O
et	O
al	O
.	O
2006	O
)	O
.	O
rather	O
than	O
go	O
into	O
algorithmic	O
detail	O
,	O
we	O
just	O
sketch	O
some	O
interesting	O
applications	O
.	O
27.6.1.1	O
learning	B
ontologies	O
an	O
ontology	B
refers	O
to	O
an	O
organisation	O
of	O
knowledge	O
.	O
(	O
see	O
e.g.	O
,	O
(	O
russell	O
and	O
norvig	O
2010	O
)	O
)	O
,	O
but	O
it	O
is	O
interesting	O
to	O
try	O
and	O
learn	O
them	O
from	O
data	O
.	O
(	O
kemp	O
et	O
al	O
.	O
2006	O
)	O
,	O
they	O
show	O
how	O
this	O
can	O
be	O
done	O
using	O
the	O
irm	O
.	O
in	O
ai	O
,	O
ontologies	O
are	O
often	O
built	O
by	O
hand	O
in	O
the	O
data	O
comes	O
from	O
the	O
uniﬁed	O
medical	O
language	O
system	O
(	O
mccray	O
2003	O
)	O
,	O
which	O
deﬁnes	O
a	O
semantic	B
network	I
with	O
135	O
concepts	O
(	O
such	O
as	O
“	O
disease	O
or	O
syndrome	O
”	O
,	O
“	O
diagnostic	O
procedure	O
”	O
,	O
“	O
animal	O
”	O
)	O
,	O
and	O
49	O
binary	O
predicates	O
(	O
such	O
as	O
“	O
affects	O
”	O
,	O
“	O
prevents	O
”	O
)	O
.	O
we	O
can	O
represent	O
this	O
as	O
a	O
ternary	O
relation	B
r	O
:	O
t	O
1	O
×	O
t	O
1	O
×	O
t	O
2	O
→	O
{	O
0	O
,	O
1	O
}	O
,	O
where	O
t	O
1	O
is	O
the	O
set	O
of	O
concepts	O
and	O
t	O
2	O
is	O
the	O
set	O
of	O
binary	O
predicates	O
.	O
the	O
result	O
is	O
a	O
3d	O
cube	O
.	O
we	O
can	O
then	O
apply	O
the	O
irm	O
to	O
partition	O
the	O
cube	O
into	O
regions	O
of	O
roughly	O
homogoneous	O
response	O
.	O
the	O
system	O
found	O
14	O
concept	B
clusters	O
and	O
21	O
predicate	O
clusters	B
.	O
some	O
of	O
these	O
are	O
shown	O
in	O
figure	O
27.25.	O
the	O
system	O
learns	O
,	O
for	O
example	O
,	O
that	O
biological	O
functions	O
affect	O
organisms	O
(	O
since	O
ηa	O
,	O
b	O
,	O
c	O
≈	O
1	O
where	O
a	O
represents	O
the	O
biological	O
function	O
cluster	O
,	O
b	O
represents	O
the	O
organism	O
cluster	O
,	O
and	O
c	O
represents	O
the	O
affects	O
cluster	O
)	O
.	O
27.6.1.2	O
clustering	B
based	O
on	O
relations	O
and	O
features	B
we	O
can	O
also	O
use	O
irm	O
to	O
cluster	O
objects	O
based	O
on	O
their	O
relations	O
and	O
their	O
features	B
.	O
for	O
example	O
,	O
(	O
kemp	O
et	O
al	O
.	O
2006	O
)	O
consider	O
a	O
political	O
dataset	O
(	O
from	O
1965	O
)	O
consisting	O
of	O
14	O
countries	O
,	O
54	O
binary	O
978	O
a	O
)	O
brazil	O
netherlands	O
uk	O
usa	O
burma	O
indonesia	O
jordan	O
egypt	O
india	O
israel	O
china	O
cuba	O
poland	O
ussr	O
b	O
)	O
military	O
alliance	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
t	O
v	O
o	O
g	O
l	O
a	O
n	O
o	O
i	O
t	O
u	O
t	O
i	O
t	O
s	O
n	O
o	O
c	O
l	O
i	O
c	O
o	O
b	O
t	O
s	O
n	O
u	O
m	O
m	O
o	O
c	O
s	O
n	O
o	O
i	O
t	O
c	O
e	O
e	O
e	O
e	O
r	O
f	O
l	O
i	O
t	O
s	O
n	O
u	O
m	O
m	O
o	O
c	O
n	O
o	O
n	O
l	O
c	O
o	O
b	O
n	O
r	O
e	O
t	O
s	O
e	O
w	O
l	O
i	O
s	O
n	O
o	O
i	O
t	O
c	O
e	O
e	O
e	O
e	O
r	O
f	O
o	O
n	O
p	O
h	O
s	O
r	O
o	O
s	O
n	O
e	O
c	O
h	O
g	O
h	O
i	O
y	O
c	O
a	O
r	O
e	O
t	O
i	O
l	O
l	O
i	O
l	O
i	O
e	O
c	O
n	O
e	O
o	O
v	O
c	O
i	O
t	O
s	O
e	O
m	O
o	O
d	O
i	O
s	O
t	O
s	O
n	O
u	O
m	O
m	O
o	O
c	O
n	O
a	O
i	O
r	O
a	O
t	O
i	O
l	O
a	O
t	O
o	O
t	O
t	O
s	O
i	O
t	O
i	O
l	O
e	O
l	O
$	O
n	O
o	O
i	O
t	O
a	O
c	O
u	O
d	O
e	O
t	O
v	O
o	O
g	O
e	O
n	O
n	O
o	O
s	O
r	O
e	O
p	O
y	O
r	O
a	O
t	O
i	O
l	O
i	O
m	O
s	O
d	O
o	O
o	O
g	O
e	O
n	O
r	O
o	O
b	O
a	O
e	O
s	O
s	O
k	O
o	O
o	O
b	O
s	O
u	O
o	O
g	O
i	O
i	O
l	O
e	O
r	O
p	O
n	O
g	O
/	O
s	O
t	O
r	O
o	O
p	O
x	O
e	O
t	O
n	O
e	O
u	O
q	O
n	O
i	O
l	O
e	O
d	O
n	O
u	O
l	O
c	O
o	O
b	O
l	O
a	O
r	O
t	O
u	O
e	O
n	O
l	O
s	O
n	O
o	O
i	O
t	O
a	O
n	O
s	O
s	O
a	O
s	O
s	O
a	O
n	O
o	O
i	O
t	O
u	O
o	O
v	O
e	O
r	O
t	O
v	O
o	O
g	O
i	O
s	O
n	O
o	O
g	O
i	O
i	O
l	O
e	O
r	O
m	O
u	O
n	O
i	O
s	O
s	O
i	O
r	O
c	O
t	O
v	O
o	O
g	O
y	O
r	O
a	O
t	O
i	O
l	O
i	O
i	O
m	O
g	O
n	O
n	O
e	O
v	O
r	O
e	O
t	O
n	O
i	O
d	O
e	O
m	O
u	O
s	O
n	O
o	O
c	O
y	O
g	O
r	O
e	O
n	O
e	O
p	O
h	O
s	O
r	O
o	O
s	O
n	O
e	O
c	O
e	O
m	O
o	O
s	O
i	O
s	O
u	O
m	O
o	O
r	O
f	O
r	O
a	O
f	O
s	O
e	O
g	O
r	O
u	O
p	O
l	O
l	O
a	O
f	O
n	O
a	O
r	O
i	O
s	O
t	O
n	O
e	O
d	O
u	O
t	O
s	O
n	O
g	O
e	O
r	O
o	O
f	O
i	O
y	O
r	O
t	O
n	O
u	O
o	O
c	O
f	O
o	O
e	O
g	O
a	O
h	O
t	O
g	O
n	O
e	O
l	O
d	O
a	O
o	O
r	O
l	O
i	O
a	O
r	O
s	O
o	O
g	O
n	O
w	O
a	O
l	O
s	O
e	O
g	O
a	O
u	O
g	O
n	O
a	O
l	O
m	O
u	O
n	O
n	O
e	O
k	O
a	O
t	O
$	O
d	O
a	O
i	O
t	O
n	O
e	O
s	O
l	O
i	O
a	O
m	O
n	O
g	O
e	O
r	O
o	O
f	O
i	O
s	O
r	O
e	O
k	O
r	O
o	O
w	O
e	O
a	O
m	O
e	O
f	O
l	O
i	O
t	O
e	O
d	O
n	O
i	O
n	O
e	O
t	O
o	O
r	O
p	O
i	O
s	O
t	O
n	O
e	O
m	O
t	O
s	O
e	O
v	O
n	O
i	O
n	O
e	O
k	O
a	O
t	O
d	O
a	O
s	O
u	O
y	O
t	O
i	O
s	O
n	O
e	O
d	O
.	O
n	O
p	O
o	O
p	O
i	O
a	O
e	O
r	O
a	O
d	O
n	O
a	O
l	O
s	O
o	O
g	O
n	O
s	O
t	O
r	O
a	O
y	O
h	O
c	O
r	O
a	O
n	O
o	O
m	O
h	O
t	O
g	O
n	O
e	O
l	O
d	O
a	O
o	O
r	O
s	O
t	O
n	O
a	O
r	O
g	O
m	O
e	O
i	O
l	O
e	O
b	O
a	O
r	O
a	O
i	O
t	O
e	O
d	O
n	O
i	O
s	O
e	O
i	O
r	O
o	O
a	O
c	O
l	O
l	O
d	O
e	O
y	O
o	O
p	O
m	O
e	O
n	O
u	O
e	O
n	O
o	O
h	O
p	O
e	O
e	O
t	O
n	O
o	O
i	O
t	O
a	O
u	O
p	O
o	O
p	O
$	O
e	O
s	O
n	O
e	O
f	O
e	O
d	O
l	O
l	O
s	O
c	O
i	O
l	O
o	O
h	O
t	O
a	O
c	O
s	O
t	O
s	O
e	O
t	O
o	O
r	O
p	O
s	O
t	O
a	O
e	O
r	O
h	O
t	O
p	O
n	O
g	O
sends	O
tourists	O
to	O
exports	O
books	O
to	O
exports	O
to	O
treaties	O
conferences	O
membership	O
of	O
igos	O
c	O
)	O
d	O
)	O
joint	O
joint	O
membership	O
of	O
ngos	O
e	O
)	O
negative	O
behavior	O
negative	O
communications	O
accusations	O
protests	O
f	O
)	O
book	O
translations	O
g	O
)	O
h	O
)	O
economic	O
aid	O
emigration	O
i	O
)	O
common	O
bloc	O
membership	O
figure	O
27.26	O
illustration	O
of	O
irm	O
applied	O
to	O
some	O
political	O
data	O
containing	O
features	B
and	O
pairwise	O
interac-	O
tions	O
.	O
top	O
row	O
(	O
a	O
)	O
.	O
the	O
partition	O
of	O
the	O
countries	O
into	O
5	O
clusters	B
and	O
the	O
features	B
into	O
5	O
clusters	B
.	O
every	O
second	O
column	O
is	O
labelled	O
with	O
the	O
name	O
of	O
the	O
corresponding	O
feature	O
.	O
small	O
squares	O
at	O
bottom	O
(	O
a-i	O
)	O
:	O
these	O
are	O
8	O
of	O
the	O
18	O
clusters	B
of	O
interaction	O
types	O
.	O
from	O
figure	O
6	O
of	O
(	O
kemp	O
et	O
al	O
.	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
charles	O
kemp	O
.	O
predicates	O
representing	O
interaction	O
types	O
between	O
countries	O
(	O
e.g.	O
,	O
“	O
sends	O
tourists	O
to	O
”	O
,	O
“	O
economic	O
aid	O
”	O
)	O
,	O
and	O
90	O
features	B
(	O
e.g.	O
,	O
“	O
communist	O
”	O
,	O
“	O
monarchy	O
”	O
)	O
.	O
to	O
create	O
a	O
binary	O
dataset	O
,	O
real-valued	O
features	B
were	O
thresholded	O
at	O
their	O
mean	B
,	O
and	O
categorical	B
variables	I
were	O
dummy-encoded	O
.	O
the	O
data	O
has	O
3	O
types	O
:	O
t	O
1	O
represents	O
countries	O
,	O
t	O
2	O
represents	O
interactions	O
,	O
and	O
t	O
3	O
represents	O
features	B
.	O
we	O
have	O
two	O
relations	O
:	O
r1	O
:	O
t	O
1×t	O
1×t	O
2	O
→	O
{	O
0	O
,	O
1	O
}	O
,	O
and	O
r2	O
:	O
t	O
1×t	O
3	O
→	O
{	O
0	O
,	O
1	O
}	O
.	O
(	O
this	O
problem	O
therefore	O
combines	O
aspects	O
of	O
both	O
the	O
biclustering	B
model	O
and	O
the	O
ontology	B
discovery	O
model	O
.	O
)	O
when	O
given	O
multiple	O
relations	O
,	O
the	O
irm	O
treats	O
them	O
as	O
conditionally	B
independent	I
.	O
in	O
this	O
case	O
,	O
we	O
have	O
p	O
(	O
r1	O
,	O
r2	O
,	O
q1	O
,	O
q2	O
,	O
q3|θ	O
)	O
=	O
p	O
(	O
r1|q1	O
,	O
q2	O
,	O
θ	O
)	O
p	O
(	O
r2|q1	O
,	O
q3	O
,	O
θ	O
)	O
the	O
results	O
are	O
shown	O
in	O
figure	O
27.26.	O
the	O
irm	O
divides	O
the	O
90	O
features	B
into	O
5	O
clusters	B
,	O
the	O
ﬁrst	O
of	O
which	O
contains	O
“	O
noncommunist	O
”	O
,	O
which	O
captures	O
one	O
of	O
the	O
most	O
important	O
aspects	O
of	O
this	O
cold-war	O
era	O
dataset	O
.	O
it	O
also	O
clusters	B
the	O
14	O
countries	O
into	O
5	O
clusters	B
,	O
reﬂecting	O
natural	O
geo-political	O
groupings	O
(	O
e.g.	O
,	O
us	O
and	O
uk	O
,	O
or	O
the	O
communist	O
bloc	O
)	O
,	O
and	O
the	O
54	O
predicates	O
into	O
18	O
clusters	B
,	O
reﬂecting	O
similar	O
relationships	O
(	O
e.g.	O
,	O
“	O
negative	O
behavior	O
and	O
“	O
accusations	O
”	O
)	O
.	O
(	O
27.83	O
)	O
27.6.	O
lvms	O
for	O
relational	O
data	O
979	O
27.6.2	O
probabilistic	B
matrix	I
factorization	I
for	O
collaborative	B
ﬁltering	I
as	O
discussed	O
in	O
section	O
1.3.4.2	O
,	O
collaborative	B
ﬁltering	I
(	O
cf	O
)	O
requires	O
predicting	O
entries	O
in	O
a	O
matrix	O
r	O
:	O
t	O
1	O
×	O
t	O
2	O
→	O
r	O
,	O
where	O
for	O
example	O
r	O
(	O
i	O
,	O
j	O
)	O
is	O
the	O
rating	O
that	O
user	O
i	O
gave	O
to	O
movie	O
j.	O
thus	O
we	O
see	O
that	O
cf	O
is	O
a	O
kind	O
of	O
relational	O
learning	O
problem	O
(	O
and	O
one	O
with	O
particular	O
commercial	O
importance	O
)	O
.	O
much	O
of	O
the	O
work	O
in	O
this	O
area	O
makes	O
use	O
of	O
the	O
data	O
that	O
netﬂix	O
made	O
available	O
in	O
their	O
competition	O
.	O
in	O
particular	O
,	O
a	O
large	O
17,770	O
×	O
480,189	O
movie	O
x	O
user	O
ratings	O
matrix	O
is	O
provided	O
.	O
the	O
full	B
matrix	O
would	O
have	O
∼	O
8.6	O
×	O
109	O
entries	O
,	O
but	O
only	O
100,480,507	O
(	O
about	O
1	O
%	O
)	O
of	O
the	O
entries	O
are	O
observed	O
,	O
so	O
the	O
matrix	O
is	O
extremely	O
sparse	B
.	O
in	O
addition	O
the	O
data	O
is	O
quite	O
imbalanced	O
,	O
with	O
many	O
users	O
rating	O
fewer	O
than	O
5	O
movies	O
,	O
and	O
a	O
few	O
users	O
rating	O
over	O
10,000	O
movies	O
.	O
the	O
validation	B
set	I
is	O
1,408,395	O
(	O
movie	O
,	O
user	O
)	O
pairs	O
.	O
finally	O
,	O
there	O
is	O
a	O
separate	O
test	O
set	O
with	O
2,817,131	O
(	O
movie	O
,	O
user	O
)	O
pairs	O
,	O
for	O
which	O
the	O
ranking	B
is	O
known	O
but	O
withheld	O
from	O
contestants	O
.	O
the	O
performance	O
measure	O
is	O
root	B
mean	I
square	I
error	I
:	O
rm	O
se	O
=	O
(	O
x	O
(	O
mi	O
,	O
ui	O
)	O
−	O
ˆx	O
(	O
mi	O
,	O
ui	O
)	O
)	O
2	O
(	O
27.84	O
)	O
9	O
:	O
:	O
;	O
1	O
n	O
(	O
cid:4	O
)	O
n	O
i=1	O
where	O
x	O
(	O
mi	O
,	O
ui	O
)	O
is	O
the	O
true	O
rating	O
of	O
user	O
ui	O
on	O
movie	O
mi	O
,	O
and	O
ˆx	O
(	O
mi	O
,	O
ui	O
)	O
is	O
the	O
prediction	O
.	O
the	O
baseline	O
system	O
,	O
known	O
as	O
cinematch	O
,	O
had	O
an	O
rmse	O
on	O
the	O
training	B
set	I
of	O
0.9514	O
,	O
and	O
on	O
the	O
test	O
set	O
of	O
0.9525.	O
to	O
qualify	O
for	O
the	O
grand	O
prize	O
,	O
teams	O
needed	O
to	O
reduce	O
the	O
test	O
rmse	O
by	O
10	O
%	O
,	O
i.e.	O
,	O
get	O
a	O
test	O
rmse	O
of	O
0.8563	O
or	O
less	O
.	O
we	O
will	O
discuss	O
some	O
of	O
the	O
basic	O
methods	O
used	O
byt	O
the	O
winning	O
team	O
below	O
.	O
since	O
the	O
ratings	O
are	O
drawn	O
from	O
the	O
set	O
{	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
}	O
,	O
it	O
is	O
tempting	O
to	O
use	O
a	O
categorical	B
observation	O
model	O
.	O
however	O
,	O
this	O
does	O
not	O
capture	O
the	O
fact	O
that	O
the	O
ratings	O
are	O
ordered	O
.	O
although	O
we	O
could	O
use	O
an	O
ordinal	B
observation	O
model	O
,	O
in	O
practice	O
people	O
use	O
a	O
gaussian	O
observation	B
model	I
for	O
simplicity	O
.	O
one	O
way	O
to	O
make	O
the	O
model	O
better	O
match	O
the	O
data	O
is	O
to	O
pass	O
the	O
model	O
’	O
s	O
predicted	O
mean	B
response	O
through	O
a	O
sigmoid	B
,	O
and	O
then	O
to	O
map	O
the	O
[	O
0	O
,	O
1	O
]	O
interval	O
to	O
[	O
0	O
,	O
5	O
]	O
(	O
salakhutdinov	O
and	O
mnih	O
2008	O
)	O
.	O
alternatively	O
we	O
can	O
make	O
the	O
data	O
a	O
better	O
match	O
to	O
the	O
gaussian	O
model	O
by	O
transforming	O
the	O
data	O
using	O
rij	O
=	O
''	O
6	O
−	O
rij	O
(	O
aggarwal	O
and	O
merugu	O
2007	O
)	O
.	O
we	O
could	O
use	O
the	O
irm	O
for	O
the	O
cf	O
task	O
,	O
by	O
associating	O
a	O
discrete	B
latent	O
variable	O
for	O
each	O
user	O
i	O
and	O
for	O
each	O
movie	O
or	O
video	O
qv	O
qu	O
p	O
(	O
rij	O
=	O
r|qu	O
j	O
=	O
b	O
,	O
θ	O
)	O
=	O
n	O
(	O
r|μa	O
,	O
b	O
,	O
σ2	O
)	O
j	O
,	O
and	O
then	O
deﬁning	O
i	O
=	O
a	O
,	O
qv	O
(	O
27.85	O
)	O
this	O
is	O
just	O
another	O
example	O
of	O
co-clustering	B
.	O
we	O
can	O
also	O
extend	O
the	O
model	O
to	O
generate	O
side	O
information	B
,	O
such	O
as	O
attributes	B
about	O
each	O
user	O
and/or	O
movie	O
.	O
see	O
figure	O
27.27	O
for	O
an	O
illustration	O
.	O
another	O
possibility	O
is	O
to	O
replace	O
the	O
discrete	B
latent	O
variables	O
with	O
continuous	O
latent	B
variables	O
i	O
∈	O
sku	O
and	O
πv	O
j	O
∈	O
skv	O
.	O
however	O
,	O
it	O
has	O
been	O
found	O
(	O
see	O
e.g.	O
,	O
(	O
shan	O
and	O
banerjee	O
2010	O
)	O
)	O
that	O
πu	O
one	O
obtains	O
much	O
better	O
results	O
by	O
using	O
unconstrained	O
real-valued	O
latent	B
factors	I
for	O
each	O
user	O
ui	O
∈	O
r	O
k.7	O
we	O
then	O
use	O
a	O
likelihood	B
of	O
the	O
form	O
k	O
and	O
each	O
movie	O
vj	O
∈	O
r	O
p	O
(	O
rij	O
=	O
r|ui	O
,	O
vj	O
)	O
=	O
n	O
(	O
r|ut	O
i	O
vj	O
,	O
σ2	O
)	O
(	O
27.86	O
)	O
7.	O
good	O
results	O
with	O
discrete	B
latent	O
variables	O
have	O
been	O
obtained	O
on	O
some	O
datasets	O
that	O
are	O
smaller	O
than	O
netﬂix	O
,	O
such	O
as	O
movielens	O
and	O
eachmovie	O
.	O
however	O
,	O
these	O
datasets	O
are	O
much	O
easier	O
to	O
predict	O
,	O
because	O
there	O
is	O
less	O
imbalance	O
between	O
the	O
number	O
of	O
reviews	O
performed	O
by	O
different	O
users	O
(	O
in	O
netﬂix	O
,	O
some	O
users	O
have	O
rated	O
more	O
than	O
10,000	O
movies	O
,	O
whereas	O
others	O
have	O
rated	O
less	O
than	O
5	O
)	O
.	O
980	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
figure	O
27.27	O
visualization	O
of	O
a	O
small	O
relational	O
dataset	O
,	O
where	O
we	O
have	O
one	O
relation	B
,	O
likes	O
(	O
user	O
,	O
movie	O
)	O
,	O
and	O
features	B
for	O
movies	O
(	O
here	O
,	O
genre	O
)	O
and	O
users	O
(	O
here	O
,	O
occupation	O
)	O
.	O
from	O
figure	O
5	O
of	O
(	O
xu	O
et	O
al	O
.	O
2008	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
zhao	O
xu	O
.	O
2	O
r	O
o	O
t	O
c	O
e	O
v	O
r	O
o	O
t	O
c	O
a	O
f	O
5	O
.	O
1	O
0	O
1	O
.	O
5	O
0	O
.	O
0	O
0	O
.	O
5	O
.	O
0	O
−	O
.	O
0	O
1	O
−	O
.	O
5	O
1	O
−	O
μv	O
σv	O
σ2	O
vj	O
rij	O
t	O
2	O
t	O
1	O
ui	O
(	O
a	O
)	O
μu	O
σu	O
the	O
royal	O
tenenbau	O
m	O
s	O
julien	O
donkey−boy	O
punch−drunk	O
love	O
i	O
heart	O
huckabees	O
lost	O
in	O
translation	O
being	O
john	O
m	O
alkovich	O
belle	O
de	O
jour	O
kill	O
bill	O
:	O
vol	O
.	O
1	O
natural	O
born	O
killers	O
citizen	O
kane	O
scarface	O
freddy	O
g	O
ot	O
fingered	O
half	O
baked	O
freddy	O
vs.	O
jason	O
road	O
trip	O
the	O
longest	O
yard	O
the	O
fast	O
and	O
the	O
furious	O
arm	O
ageddon	O
catwo	O
m	O
an	O
coyote	O
ugly	O
the	O
wizard	O
of	O
oz	O
runaway	O
bride	O
sister	O
act	O
step	O
m	O
o	O
m	O
m	O
aid	O
in	O
m	O
anhattan	O
annie	O
hall	O
sophie	O
’	O
s	O
choice	O
m	O
oonstruck	O
the	O
w	O
ay	O
w	O
e	O
w	O
ere	O
the	O
sound	O
of	O
m	O
usic	O
the	O
w	O
altons	O
:	O
season	O
1	O
−1.5	O
−1.0	O
−0.5	O
0.0	O
0.5	O
1.0	O
factor	B
vector	O
1	O
(	O
b	O
)	O
figure	O
27.28	O
(	O
a	O
)	O
a	O
dgm	O
for	O
probabilistic	B
matrix	I
factorization	I
.	O
(	O
b	O
)	O
visualization	O
of	O
the	O
ﬁrst	O
two	O
factors	B
in	O
the	O
pmf	B
model	O
estimated	O
from	O
the	O
netﬂix	O
challenge	O
data	O
.	O
each	O
movie	O
j	O
is	O
plotted	O
at	O
the	O
location	O
speciﬁed	O
ˆvj	O
.	O
on	O
the	O
left	O
we	O
have	O
low-brow	O
humor	O
and	O
horror	O
movies	O
(	O
half	O
baked	O
,	O
freddy	O
vs	O
jason	O
)	O
,	O
and	O
on	O
the	O
right	O
we	O
have	O
more	O
serious	O
dramas	O
(	O
sophie	O
’	O
s	O
choice	O
,	O
moonstruck	O
)	O
.	O
on	O
the	O
top	O
we	O
have	O
critically	O
acclaimed	O
independent	O
movies	O
(	O
punch-drunk	O
love	O
,	O
i	O
heart	O
huckabees	O
)	O
,	O
and	O
on	O
the	O
bottom	O
we	O
have	O
mainstream	O
hollywood	O
blockbusters	O
(	O
armageddon	O
,	O
runway	O
bride	O
)	O
.	O
the	O
wizard	O
of	O
oz	O
is	O
right	O
in	O
the	O
middle	O
of	O
these	O
axes	O
.	O
from	O
figure	O
3	O
of	O
(	O
koren	O
et	O
al	O
.	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
yehuda	O
koren	O
.	O
this	O
has	O
been	O
called	O
probabilistic	B
matrix	I
factorization	I
(	O
pmf	B
)	O
(	O
salakhutdinov	O
and	O
mnih	O
2008	O
)	O
.	O
see	O
figure	O
27.28	O
(	O
a	O
)	O
for	O
the	O
dgm	O
.	O
the	O
intuition	O
behind	O
this	O
method	O
is	O
that	O
each	O
user	O
and	O
each	O
movie	O
get	O
embedded	O
into	O
the	O
same	O
low-dimensional	O
continuous	O
space	O
(	O
see	O
figure	O
27.28	O
(	O
b	O
)	O
)	O
.	O
if	O
a	O
user	O
is	O
close	O
to	O
a	O
movie	O
in	O
that	O
space	O
,	O
they	O
are	O
likely	O
to	O
rate	B
it	O
highly	O
.	O
all	O
of	O
the	O
best	O
entries	O
in	O
the	O
netﬂix	O
competition	O
used	O
this	O
approach	O
in	O
one	O
form	O
or	O
another.8	O
pmf	B
is	O
closely	O
related	O
to	O
the	O
svd	O
.	O
in	O
particular	O
,	O
if	O
there	O
is	O
no	O
missing	O
data	O
,	O
then	O
computing	O
the	O
mle	O
for	O
the	O
ui	O
’	O
s	O
and	O
the	O
vj	O
’	O
s	O
is	O
equivalent	O
to	O
ﬁnding	O
a	O
rank	O
k	O
approximation	O
to	O
r.	O
however	O
,	O
as	O
soon	O
as	O
we	O
have	O
missing	B
data	I
,	O
the	O
problem	O
becomes	O
non-convex	O
,	O
as	O
shown	O
in	O
8.	O
the	O
winning	O
entry	O
was	O
actually	O
an	O
ensemble	B
of	O
different	O
methods	O
,	O
including	O
pmf	B
,	O
nearest	B
neighbor	I
methods	O
,	O
etc	O
.	O
27.6.	O
lvms	O
for	O
relational	O
data	O
981	O
0.97	O
0.96	O
0.95	O
e	O
s	O
m	O
r	O
0.94	O
0.93	O
0.92	O
0.91	O
0.9	O
0	O
netflix	O
baseline	O
score	O
svd	O
pmf	B
constrained	O
pmf	B
5	O
10	O
15	O
20	O
25	O
30	O
epochs	O
35	O
40	O
45	O
50	O
55	O
60	O
60	O
40	O
50	O
50	O
e	O
s	O
m	O
r	O
0.91	O
0.905	O
0.9	O
0.895	O
0.89	O
0.885	O
0.88	O
0.875	O
10	O
plain	O
90	O
128	O
180	O
100	O
200	O
w/biases	O
100	O
200	O
w/implicit	O
feedback	O
50	O
100	O
200	O
500	O
1000	O
1500	O
w/temporal	O
dynamics	O
100	O
1000	O
millions	O
of	O
parameters	O
10000	O
100000	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
27.29	O
(	O
a	O
)	O
rmse	O
on	O
the	O
validation	B
set	I
for	O
different	O
pmf	B
variants	O
vs	O
number	O
of	O
passes	O
through	O
the	O
data	O
.	O
“	O
svd	O
”	O
is	O
the	O
unregularized	O
version	O
,	O
λu	O
=	O
λv	O
=	O
0	O
.	O
“	O
pmf1	O
”	O
corresponds	O
to	O
λu	O
=	O
0.01	O
and	O
λv	O
=	O
0.001	O
,	O
while	O
“	O
pmf2	O
”	O
corresponds	O
to	O
λu	O
=	O
0.001	O
and	O
λv	O
=	O
0.0001	O
.	O
“	O
pmfa1	O
”	O
corresponds	O
to	O
a	O
version	O
where	O
the	O
mean	B
and	O
diagonal	O
covariance	O
of	O
the	O
gaussian	O
prior	O
were	O
learned	O
from	O
data	O
.	O
from	O
figure	O
2	O
of	O
(	O
salakhutdinov	O
and	O
mnih	O
2008	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
ruslan	O
salakhutdinov	O
.	O
(	O
b	O
)	O
rmse	O
on	O
the	O
test	O
set	O
(	O
quiz	O
portion	O
)	O
vs	O
number	O
of	O
parameters	O
for	O
several	O
different	O
models	O
.	O
“	O
plain	O
”	O
is	O
the	O
baseline	O
pmf	B
with	O
suitably	O
chosen	O
λu	O
,	O
λv	O
.	O
“	O
with	O
biases	O
”	O
adds	O
fi	O
and	O
gj	O
offset	O
terms	O
.	O
“	O
with	O
implicit	B
feedback	I
”	O
“	O
with	O
temporal	O
dynamics	O
”	O
allows	O
the	O
offset	O
terms	O
to	O
change	O
over	O
time	O
.	O
the	O
netﬂix	O
baseline	O
system	O
achieves	O
an	O
rmse	O
of	O
0.9514	O
,	O
and	O
the	O
grand	O
prize	O
’	O
s	O
required	O
accuracy	O
is	O
0.8563	O
(	O
which	O
was	O
obtained	O
on	O
21	O
september	O
2009	O
)	O
.	O
figure	O
generated	O
by	O
netflixresultsplot	O
.	O
from	O
figure	O
4	O
of	O
(	O
koren	O
et	O
al	O
.	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
yehuda	O
koren	O
.	O
(	O
srebro	O
and	O
jaakkola	O
2003	O
)	O
,	O
and	O
standard	O
svd	O
methods	O
can	O
not	O
be	O
applied	O
.	O
netﬂix	O
challenge	O
,	O
only	O
about	O
1	O
%	O
of	O
the	O
matrix	O
is	O
observed	O
.	O
)	O
the	O
most	O
straightforward	O
way	O
to	O
ﬁt	O
the	O
pmf	B
model	O
is	O
to	O
minimize	O
the	O
overall	O
nll	O
:	O
⎛	O
⎝	O
n	O
(	O
cid:20	O
)	O
m	O
(	O
cid:20	O
)	O
(	O
cid:13	O
)	O
n	O
(	O
rij|ut	O
i=1	O
j=1	O
i	O
vj	O
,	O
σ2	O
)	O
(	O
recall	B
that	O
in	O
the	O
(	O
cid:14	O
)	O
i	O
(	O
oij	O
=1	O
)	O
⎞	O
⎠	O
(	O
27.87	O
)	O
j	O
(	O
u	O
,	O
v	O
)	O
=	O
−	O
log	O
p	O
(	O
r|u	O
,	O
v	O
,	O
o	O
)	O
=	O
−	O
log	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
where	O
oij	O
=	O
1	O
if	O
user	O
i	O
has	O
seen	O
movie	O
j.	O
since	O
this	O
is	O
non-convex	O
,	O
we	O
can	O
just	O
ﬁnd	O
a	O
locally	O
optimal	O
mle	O
.	O
since	O
the	O
netﬂix	O
data	O
is	O
so	O
large	O
(	O
about	O
100	O
million	O
observed	O
entries	O
)	O
,	O
it	O
is	O
common	O
to	O
use	O
stochastic	B
gradient	I
descent	I
(	O
section	O
8.5.2	O
)	O
for	O
this	O
task	O
.	O
the	O
gradient	O
for	O
ui	O
is	O
given	O
by	O
dj	O
dui	O
=	O
d	O
dui	O
1	O
2	O
i	O
(	O
oij	O
=	O
1	O
)	O
(	O
rij	O
−	O
ut	O
i	O
vj	O
)	O
2	O
=	O
−	O
ij	O
j	O
:	O
oij	O
=1	O
eijvj	O
(	O
27.88	O
)	O
where	O
eij	O
=	O
rij	O
−	O
ut	O
i	O
has	O
watched	O
,	O
the	O
update	O
takes	O
the	O
following	O
simple	O
form	O
:	O
i	O
vj	O
is	O
the	O
error	O
term	O
.	O
by	O
stochastically	O
sampling	O
a	O
single	O
movie	O
j	O
that	O
user	O
ui	O
=	O
ui	O
+	O
ηeijvj	O
(	O
27.89	O
)	O
where	O
η	O
is	O
the	O
learning	B
rate	I
.	O
the	O
update	O
for	O
vj	O
is	O
similar	B
.	O
982	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
of	O
course	O
,	O
just	O
maximizing	O
the	O
likelihood	B
results	O
in	O
overﬁtting	B
,	O
as	O
shown	O
in	O
figure	O
27.29	O
(	O
a	O
)	O
.	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
we	O
can	O
regularize	O
this	O
by	O
imposing	O
gaussian	O
priors	O
:	O
n	O
(	O
vj|μv	O
,	O
σv	O
)	O
n	O
(	O
ui|μu	O
,	O
σu	O
)	O
p	O
(	O
u	O
,	O
v	O
)	O
=	O
i	O
j	O
(	O
27.90	O
)	O
(	O
27.91	O
)	O
(	O
27.92	O
)	O
if	O
we	O
use	O
μu	O
=	O
μv	O
=	O
0	O
,	O
σu	O
=	O
σ2	O
u	O
ik	O
,	O
and	O
σv	O
=	O
σ2	O
v	O
ik	O
,	O
the	O
new	O
objective	O
becomes	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
j	O
(	O
u	O
,	O
v	O
)	O
=−	O
log	O
p	O
(	O
r	O
,	O
u	O
,	O
v|o	O
,	O
θ	O
)	O
i	O
(	O
oij	O
=	O
1	O
)	O
(	O
rij	O
−	O
ut	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
||ui||2	O
+λu	O
2	O
+	O
λv	O
=	O
i	O
vj	O
)	O
2	O
||vj||2	O
i	O
j	O
2	O
+	O
const	O
i	O
j	O
where	O
we	O
have	O
deﬁned	O
λu	O
=	O
σ2/σ2	O
v	O
.	O
by	O
varying	O
the	O
regularizers	O
,	O
we	O
can	O
reduce	O
the	O
effect	O
of	O
overﬁtting	B
,	O
as	O
shown	O
in	O
figure	O
27.29	O
(	O
a	O
)	O
.	O
we	O
can	O
ﬁnd	O
map	O
estimates	O
using	O
stochastic	B
gradient	I
descent	I
.	O
we	O
can	O
also	O
compute	O
approximate	O
posteriors	O
using	O
variational	O
bayes	O
(	O
ilin	O
and	O
raiko	O
2010	O
)	O
.	O
u	O
and	O
λv	O
=	O
σ2/σ2	O
if	O
we	O
use	O
diagonal	B
covariances	O
for	O
the	O
priors	O
,	O
we	O
can	O
penalize	O
each	O
latent	B
dimension	O
by	O
a	O
different	O
amount	O
.	O
also	O
,	O
if	O
we	O
use	O
non-zero	O
means	O
for	O
the	O
priors	O
,	O
we	O
can	O
account	O
for	O
offset	O
terms	O
.	O
optimizing	O
the	O
prior	O
parameters	O
(	O
μu	O
,	O
σu	O
,	O
μv	O
,	O
σv	O
)	O
at	O
the	O
same	O
time	O
as	O
the	O
model	O
parameters	O
(	O
u	O
,	O
v	O
,	O
σ2	O
)	O
is	O
a	O
way	O
to	O
create	O
an	O
adaptive	O
prior	O
.	O
this	O
avoids	O
the	O
need	O
to	O
search	O
for	O
the	O
optimal	O
values	O
of	O
λu	O
and	O
λv	O
,	O
and	O
gives	O
even	O
better	O
results	O
,	O
as	O
shown	O
in	O
figure	O
27.29	O
(	O
a	O
)	O
.	O
it	O
turns	O
out	O
that	O
much	O
of	O
the	O
variation	O
in	O
the	O
data	O
can	O
be	O
explained	O
by	O
movie-speciﬁc	O
or	O
user-speciﬁc	O
effects	O
.	O
for	O
example	O
,	O
some	O
movies	O
are	O
popular	O
for	O
all	O
types	O
of	O
users	O
.	O
and	O
some	O
users	O
give	O
low	O
scores	O
for	O
all	O
types	O
of	O
movies	O
.	O
we	O
can	O
model	O
this	O
by	O
allowing	O
for	O
user	O
and	O
movie	O
speciﬁc	O
offset	O
or	O
bias	B
terms	O
as	O
follows	O
:	O
p	O
(	O
rij	O
=	O
r|ui	O
,	O
vj	O
,	O
θ	O
)	O
=	O
n	O
(	O
r|ut	O
i	O
vj	O
+	O
μ	O
+	O
fi	O
+	O
gj	O
,	O
σ2	O
)	O
(	O
27.93	O
)	O
where	O
μ	O
is	O
the	O
overall	O
mean	B
,	O
fi	O
is	O
the	O
user	O
bias	O
,	O
gj	O
is	O
the	O
movie	O
bias	B
,	O
and	O
ut	O
i	O
vj	O
is	O
the	O
interaction	O
term	O
.	O
this	O
is	O
equivalent	O
to	O
applying	O
pmf	B
just	O
to	O
the	O
residual	B
matrix	O
,	O
and	O
gives	O
much	O
better	O
results	O
,	O
as	O
shown	O
in	O
figure	O
27.29	O
(	O
b	O
)	O
.	O
we	O
can	O
estimate	O
the	O
fi	O
,	O
gj	O
and	O
μ	O
terms	O
using	O
stochastic	B
gradient	I
descent	I
,	O
just	O
as	O
we	O
estimated	O
u	O
,	O
v	O
and	O
θ.	O
we	O
can	O
also	O
allow	O
the	O
bias	B
terms	O
to	O
evolve	O
over	O
time	O
,	O
to	O
reﬂect	O
the	O
changing	O
preferences	B
of	O
users	O
(	O
koren	O
2009b	O
)	O
.	O
this	O
is	O
important	O
since	O
in	O
the	O
netﬂix	O
competition	O
,	O
the	O
test	O
data	O
was	O
more	O
recent	O
than	O
the	O
training	O
data	O
.	O
figure	O
27.29	O
(	O
b	O
)	O
shows	O
that	O
allowing	O
for	O
temporal	O
dynamics	O
can	O
help	O
a	O
lot	O
.	O
often	O
we	O
also	O
have	O
side	B
information	I
of	O
various	O
kinds	O
.	O
in	O
the	O
netﬂix	O
competition	O
,	O
entrants	O
knew	O
which	O
movies	O
the	O
user	O
had	O
rated	O
in	O
the	O
test	O
set	O
,	O
even	O
though	O
they	O
did	O
not	O
know	O
the	O
values	O
of	O
these	O
ratings	O
.	O
that	O
is	O
,	O
they	O
knew	O
the	O
value	O
of	O
the	O
(	O
dense	O
)	O
o	O
matrix	O
even	O
on	O
the	O
test	O
set	O
.	O
if	O
a	O
user	O
chooses	O
to	O
rate	B
a	O
movie	O
,	O
it	O
is	O
likely	O
because	O
they	O
have	O
seen	O
it	O
,	O
which	O
in	O
turns	O
means	O
they	O
thought	O
they	O
would	O
like	O
it	O
.	O
thus	O
the	O
very	O
act	O
of	O
rating	O
reveals	O
information	B
.	O
conversely	O
,	O
if	O
a	O
user	O
chooses	O
not	O
rate	O
a	O
movie	O
,	O
it	O
suggests	O
they	O
knew	O
they	O
would	O
not	O
like	O
it	O
.	O
so	O
the	O
data	O
is	O
not	B
missing	I
at	I
random	I
(	O
see	O
e.g.	O
,	O
(	O
marlin	O
and	O
zemel	O
2009	O
)	O
)	O
.	O
exploiting	O
this	O
can	O
improve	O
performance	O
,	O
as	O
shown	O
in	O
figure	O
27.29	O
(	O
b	O
)	O
.	O
in	O
real	O
problems	O
,	O
information	B
on	O
the	O
test	O
set	O
is	O
not	O
available	O
.	O
however	O
,	O
we	O
often	O
know	O
which	O
movies	O
the	O
user	O
has	O
watched	O
or	O
declined	O
to	O
27.7.	O
restricted	O
boltzmann	O
machines	O
(	O
rbms	O
)	O
983	O
watch	O
,	O
even	O
if	O
they	O
did	O
not	O
rate	O
them	O
(	O
this	O
is	O
called	O
implicit	B
feedback	I
)	O
,	O
and	O
this	O
can	O
be	O
used	O
as	O
useful	O
side	B
information	I
.	O
another	O
source	O
of	O
side	B
information	I
concerns	O
the	O
content	O
of	O
the	O
movie	O
,	O
such	O
as	O
the	O
movie	O
genre	O
,	O
the	O
list	O
of	O
the	O
actors	O
,	O
or	O
a	O
synopsis	O
of	O
the	O
plot	O
.	O
this	O
can	O
be	O
denoted	O
by	O
xv	O
,	O
the	O
features	B
(	O
in	O
the	O
case	O
where	O
we	O
just	O
have	O
the	O
id	O
of	O
the	O
video	O
,	O
we	O
can	O
treat	O
xv	O
as	O
a	O
|v|-	O
of	O
the	O
video	O
.	O
dimensional	O
bit	O
vector	O
with	O
just	O
one	O
bit	O
turned	O
on	O
.	O
)	O
we	O
may	O
also	O
know	O
features	B
about	O
the	O
user	O
,	O
which	O
we	O
can	O
denote	O
by	O
xu	O
.	O
in	O
some	O
cases	O
,	O
we	O
only	O
know	O
if	O
the	O
user	O
clicked	O
on	O
the	O
video	O
or	O
not	O
,	O
that	O
is	O
,	O
we	O
may	O
not	O
have	O
a	O
numerical	O
rating	O
.	O
we	O
can	O
then	O
modify	O
the	O
model	O
as	O
follows	O
:	O
p	O
(	O
r	O
(	O
u	O
,	O
v	O
)	O
|xu	O
,	O
xv	O
,	O
θ	O
)	O
=	O
ber	O
(	O
r	O
(	O
u	O
,	O
v	O
)	O
|	O
(	O
uxu	O
)	O
t	O
(	O
vxv	O
)	O
)	O
(	O
27.94	O
)	O
where	O
u	O
is	O
a	O
|u|	O
×	O
k	O
matrix	O
,	O
and	O
v	O
is	O
a	O
|v|	O
×	O
k	O
matrix	O
(	O
we	O
can	O
incorporate	O
an	O
offset	O
term	O
by	O
appending	O
a	O
1	O
to	O
xu	O
and	O
xv	O
in	O
the	O
usual	O
way	O
)	O
.	O
a	O
method	O
for	O
computing	O
the	O
approximate	O
posterior	O
p	O
(	O
u	O
,	O
v|d	O
)	O
in	O
an	O
online	O
fashion	O
,	O
using	O
adf	O
and	O
ep	O
,	O
was	O
described	O
in	O
(	O
stern	O
et	O
al	O
.	O
2009	O
)	O
.	O
this	O
was	O
implemented	O
by	O
microsoft	O
and	O
has	O
been	O
deployed	O
to	O
predict	O
click	O
through	O
rates	O
on	O
all	O
the	O
ads	O
used	O
by	O
bing	O
.	O
unfortunately	O
,	O
ﬁtting	O
this	O
model	O
just	O
from	O
positive	O
binary	O
data	O
can	O
result	O
in	O
an	O
over	O
prediction	O
of	O
links	O
,	O
since	O
no	O
negative	O
examples	O
are	O
included	O
.	O
better	O
performance	O
is	O
obtained	O
if	O
one	O
has	O
access	O
to	O
the	O
set	O
of	O
all	O
videos	O
shown	O
to	O
the	O
user	O
,	O
of	O
which	O
at	O
most	O
one	O
was	O
picked	O
;	O
data	O
of	O
this	O
form	O
is	O
known	O
as	O
an	O
impression	B
log	I
.	O
in	O
this	O
case	O
,	O
we	O
can	O
use	O
a	O
multinomial	B
model	O
instead	O
of	O
a	O
binary	O
model	O
;	O
in	O
(	O
yang	O
et	O
al	O
.	O
2011	O
)	O
,	O
this	O
was	O
shown	O
to	O
work	O
much	O
better	O
than	O
a	O
binary	O
model	O
.	O
to	O
understand	O
why	O
,	O
suppose	O
some	O
is	O
presented	O
with	O
a	O
choice	O
of	O
an	O
action	B
movie	O
starring	O
arnold	O
schwarzenegger	O
,	O
an	O
action	B
movie	O
starring	O
vin	O
diesel	O
,	O
and	O
a	O
comedy	O
starring	O
hugh	O
grant	O
.	O
if	O
the	O
user	O
picks	O
arnold	O
schwarzenegger	O
,	O
we	O
learn	O
not	O
only	O
that	O
they	O
like	O
prefer	O
action	B
movies	O
to	O
comedies	O
,	O
but	O
also	O
that	O
they	O
prefer	O
schwarzenegger	O
to	O
diesel	O
.	O
this	O
is	O
more	O
informative	O
than	O
just	O
knowing	O
that	O
they	O
like	O
schwarzenegger	O
and	O
action	B
movies	O
.	O
27.7	O
restricted	O
boltzmann	O
machines	O
(	O
rbms	O
)	O
so	O
far	O
,	O
all	O
the	O
models	O
we	O
have	O
proposed	O
in	O
this	O
chapter	O
have	O
been	O
representable	O
by	O
directed	O
graphical	O
models	O
.	O
but	O
some	O
models	O
are	O
better	O
represented	O
using	O
undirected	B
graphs	O
.	O
for	O
example	O
,	O
the	O
boltzmann	O
machine	O
(	O
ackley	O
et	O
al	O
.	O
1985	O
)	O
is	O
a	O
pairwise	O
mrf	O
with	O
hidden	B
nodes	I
h	O
and	O
visible	B
nodes	I
v	O
,	O
as	O
shown	O
in	O
figure	O
27.30	O
(	O
a	O
)	O
.	O
the	O
main	O
problem	O
with	O
the	O
boltzmann	O
machine	O
is	O
that	O
exact	O
inference	B
is	O
intractable	O
,	O
and	O
even	O
approximate	B
inference	I
,	O
using	O
e.g.	O
,	O
gibbs	O
sampling	O
,	O
can	O
be	O
slow	O
.	O
however	O
,	O
suppose	O
we	O
restrict	O
the	O
architecture	O
so	O
that	O
the	O
nodes	B
are	O
arranged	O
in	O
layers	O
,	O
and	O
so	O
that	O
there	O
are	O
no	O
connections	O
between	O
nodes	B
within	O
the	O
same	O
layer	O
(	O
see	O
figure	O
27.30	O
(	O
b	O
)	O
)	O
.	O
then	O
the	O
model	O
has	O
the	O
form	O
r	O
(	O
cid:20	O
)	O
k	O
(	O
cid:20	O
)	O
p	O
(	O
h	O
,	O
v|θ	O
)	O
=	O
1	O
z	O
(	O
θ	O
)	O
r=1	O
k=1	O
ψrk	O
(	O
vr	O
,	O
hk	O
)	O
(	O
27.95	O
)	O
where	O
r	O
is	O
the	O
number	O
of	O
visible	B
(	O
response	O
)	O
variables	O
,	O
k	O
is	O
the	O
number	O
of	O
hidden	B
variables	I
,	O
and	O
v	O
plays	O
the	O
role	O
of	O
y	O
earlier	O
in	O
this	O
chapter	O
.	O
this	O
model	O
is	O
known	O
as	O
a	O
restricted	O
boltzmann	O
machine	O
(	O
rbm	O
)	O
(	O
hinton	O
2002	O
)	O
,	O
or	O
a	O
harmonium	B
(	O
smolensky	O
1986	O
)	O
.	O
an	O
rbm	O
is	O
a	O
special	O
case	O
of	O
a	O
product	B
of	I
experts	I
(	O
poe	O
)	O
(	O
hinton	O
1999	O
)	O
,	O
which	O
is	O
so-called	O
because	O
we	O
are	O
multiplying	O
together	O
a	O
set	O
of	O
“	O
experts	O
”	O
(	O
here	O
,	O
potential	O
functions	O
on	O
each	O
edge	O
)	O
984	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
h	O
v	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
27.30	O
(	O
a	O
)	O
a	O
general	O
boltzmann	O
machine	O
,	O
with	O
an	O
arbitrary	O
graph	B
structure	O
.	O
the	O
shaded	O
(	O
visible	B
)	O
nodes	B
are	O
partitioned	O
into	O
input	O
and	O
output	O
,	O
although	O
the	O
model	O
is	O
actually	O
symmetric	B
and	O
deﬁnes	O
a	O
joint	O
density	O
on	O
all	O
the	O
nodes	B
.	O
(	O
b	O
)	O
a	O
restricted	O
boltzmann	O
machine	O
with	O
a	O
bipartite	O
structure	O
.	O
note	O
the	O
lack	O
of	O
intra-layer	O
connections	O
.	O
and	O
then	O
normalizing	O
,	O
whereas	O
in	O
a	O
mixture	B
of	I
experts	I
,	O
we	O
take	O
a	O
convex	B
combination	I
of	O
normalized	O
distributions	O
.	O
the	O
intuitive	O
reason	O
why	O
poe	O
models	O
might	O
work	O
better	O
than	O
a	O
mixture	B
is	O
that	O
each	O
expert	O
can	O
enforce	O
a	O
constraint	O
(	O
if	O
the	O
expert	O
has	O
a	O
value	O
which	O
is	O
(	O
cid:30	O
)	O
1	O
or	O
(	O
cid:22	O
)	O
1	O
)	O
or	O
a	O
“	O
don	O
’	O
t	O
care	O
”	O
condition	O
(	O
if	O
the	O
expert	O
has	O
value	O
1	O
)	O
.	O
by	O
multiplying	O
these	O
experts	O
together	O
in	O
different	O
ways	O
we	O
can	O
create	O
“	O
sharp	O
”	O
distributions	O
which	O
predict	O
data	O
which	O
satisﬁes	O
the	O
speciﬁed	O
constraints	O
(	O
hinton	O
and	O
teh	O
2001	O
)	O
.	O
for	O
example	O
,	O
consider	O
a	O
distributed	O
model	O
of	O
text	O
.	O
a	O
given	O
document	O
might	O
have	O
the	O
topics	O
“	O
government	O
”	O
,	O
“	O
maﬁa	O
”	O
and	O
“	O
playboy	O
”	O
.	O
if	O
we	O
“	O
multiply	O
”	O
the	O
predictions	O
of	O
each	O
topic	B
together	O
,	O
the	O
model	O
may	O
give	O
very	O
high	O
probability	O
to	O
the	O
word	O
“	O
berlusconi	O
”	O
9	O
(	O
salakhutdinov	O
and	O
hinton	O
2010	O
)	O
.	O
by	O
contrast	O
,	O
adding	O
together	O
experts	O
can	O
only	O
make	O
the	O
distribution	O
broader	O
(	O
see	O
figure	O
14.17	O
)	O
.	O
typically	O
the	O
hidden	B
nodes	I
in	O
an	O
rbm	O
are	O
binary	O
,	O
so	O
h	O
speciﬁes	O
which	O
constraints	O
are	O
active	O
.	O
it	O
is	O
worth	O
comparing	O
this	O
with	O
the	O
directed	B
models	O
we	O
have	O
discussed	O
.	O
in	O
a	O
mixture	B
model	I
,	O
we	O
have	O
one	O
hidden	B
variable	I
q	O
∈	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
}	O
.	O
we	O
can	O
represent	O
this	O
using	O
a	O
set	O
of	O
k	O
bits	B
,	O
with	O
the	O
restriction	O
that	O
exactly	O
one	O
bit	O
is	O
on	O
at	O
a	O
time	O
.	O
this	O
is	O
called	O
a	O
localist	B
encoding	I
,	O
since	O
only	O
one	O
hidden	B
unit	O
is	O
used	O
to	O
generate	O
the	O
response	O
vector	O
.	O
this	O
is	O
analogous	O
to	O
the	O
hypothetical	O
notion	O
of	O
grandmother	B
cells	I
in	O
the	O
brain	O
,	O
that	O
are	O
able	O
to	O
recognize	O
only	O
one	O
kind	O
of	O
object	O
.	O
by	O
contrast	O
,	O
an	O
rbm	O
uses	O
a	O
distributed	B
encoding	I
,	O
where	O
many	O
units	O
are	O
involved	O
in	O
generating	O
each	O
output	O
.	O
models	O
that	O
used	O
vector-valued	O
hidden	B
variables	I
,	O
such	O
as	O
π	O
∈	O
sk	O
,	O
as	O
in	O
mpca/	O
lda	O
,	O
or	O
z	O
∈	O
r	O
k	O
,	O
as	O
in	O
epca	O
also	O
use	O
distributed	O
encodings	O
.	O
the	O
main	O
difference	O
between	O
an	O
rbm	O
and	O
directed	B
two-layer	O
models	O
is	O
that	O
the	O
hidden	B
(	O
cid:20	O
)	O
variables	O
are	O
conditionally	B
independent	I
given	O
the	O
visible	B
variables	I
,	O
so	O
the	O
posterior	O
factorizes	O
:	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
=	O
p	O
(	O
hk|v	O
,	O
θ	O
)	O
(	O
27.96	O
)	O
k	O
this	O
makes	O
inference	B
much	O
simpler	O
than	O
in	O
a	O
directed	B
model	O
,	O
since	O
we	O
can	O
estimate	O
each	O
hk	O
9.	O
silvio	O
berlusconi	O
is	O
the	O
current	O
(	O
2011	O
)	O
prime	O
minister	O
of	O
italy	O
.	O
27.7.	O
restricted	O
boltzmann	O
machines	O
(	O
rbms	O
)	O
985	O
visible	B
binary	O
gaussian	O
categorical	B
multiple	O
categorical	B
gaussian	O
binary	O
hidden	O
binary	O
binary	O
binary	O
binary	O
gaussian	O
gaussian	O
name	O
binary	O
rbm	O
gaussian	O
rbm	O
categorical	B
rbm	O
replicated	O
softmax/	O
undirected	B
lda	O
undirected	B
pca	O
undirected	B
binary	O
pca	O
reference	O
(	O
hinton	O
2002	O
)	O
(	O
welling	O
and	O
sutton	O
2005	O
)	O
(	O
salakhutdinov	O
et	O
al	O
.	O
2007	O
)	O
(	O
salakhutdinov	O
and	O
hinton	O
2010	O
)	O
(	O
marks	O
and	O
movellan	O
2001	O
)	O
(	O
welling	O
and	O
sutton	O
2005	O
)	O
table	O
27.2	O
summary	O
of	O
different	O
kinds	O
of	O
rbm	O
.	O
independently	O
and	O
in	O
parallel	O
,	O
as	O
in	O
a	O
feedforward	B
neural	I
network	I
.	O
the	O
disadvantage	O
is	O
that	O
training	O
undirected	O
models	O
is	O
much	O
harder	O
,	O
as	O
we	O
discuss	O
below	O
.	O
27.7.1	O
varieties	O
of	O
rbms	O
in	O
this	O
section	O
,	O
we	O
describe	O
various	O
forms	O
of	O
rbms	O
,	O
by	O
deﬁning	O
different	O
pairwise	O
potential	O
functions	O
.	O
see	O
table	O
27.2	O
for	O
a	O
summary	O
.	O
all	O
of	O
these	O
are	O
special	O
cases	O
of	O
the	O
exponential	B
family	I
harmonium	I
(	O
welling	O
et	O
al	O
.	O
2004	O
)	O
.	O
27.7.1.1	O
binary	O
rbms	O
the	O
most	O
common	O
form	O
of	O
rbm	O
has	O
binary	O
hidden	O
nodes	B
and	O
binary	O
visible	O
nodes	B
.	O
the	O
joint	B
distribution	I
then	O
has	O
the	O
following	O
form	O
:	O
1	O
z	O
(	O
θ	O
)	O
p	O
(	O
v	O
,	O
h|θ	O
)	O
=	O
exp	O
(	O
−e	O
(	O
v	O
,	O
h	O
;	O
θ	O
)	O
)	O
k	O
(	O
cid:4	O
)	O
e	O
(	O
v	O
,	O
h	O
;	O
θ	O
)	O
(	O
cid:2	O
)	O
−	O
r	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
vrhkwrk	O
−	O
r	O
(	O
cid:4	O
)	O
=	O
−	O
(	O
vt	O
wh	O
+	O
vt	O
b	O
+	O
ht	O
c	O
)	O
exp	O
(	O
−e	O
(	O
v	O
,	O
h	O
;	O
θ	O
)	O
)	O
z	O
(	O
θ	O
)	O
=	O
r=1	O
k=1	O
r=1	O
vrbr	O
−	O
k	O
(	O
cid:4	O
)	O
k=1	O
hkck	O
(	O
27.97	O
)	O
(	O
27.98	O
)	O
(	O
27.99	O
)	O
(	O
27.100	O
)	O
v	O
h	O
where	O
e	O
is	O
the	O
energy	B
function	I
,	O
w	O
is	O
a	O
r×	O
k	O
weight	O
matrix	O
,	O
b	O
are	O
the	O
visible	B
bias	O
terms	O
,	O
c	O
are	O
the	O
hidden	B
bias	O
terms	O
,	O
and	O
θ	O
=	O
(	O
w	O
,	O
b	O
,	O
c	O
)	O
are	O
all	O
the	O
parameters	O
.	O
for	O
notational	O
simplicity	O
,	O
we	O
will	O
absorb	O
the	O
bias	B
terms	O
into	O
the	O
weight	O
matrix	O
by	O
clamping	B
dummy	O
units	O
v0	O
=	O
1	O
and	O
h0	O
=	O
1	O
and	O
setting	O
w0	O
,	O
:	O
=	O
c	O
and	O
w	O
:	O
,0	O
=	O
b.	O
note	O
that	O
naively	O
computing	O
z	O
(	O
θ	O
)	O
takes	O
o	O
(	O
2r2k	O
)	O
time	O
but	O
we	O
can	O
reduce	O
this	O
to	O
o	O
(	O
min	O
{	O
r2k	O
,	O
k2r	O
}	O
)	O
time	O
(	O
exercise	O
27.1	O
)	O
.	O
when	O
using	O
a	O
binary	O
rbm	O
,	O
the	O
posterior	O
can	O
be	O
computed	O
as	O
follows	O
:	O
ber	O
(	O
hk|sigm	O
(	O
wt	O
:	O
,kv	O
)	O
)	O
(	O
27.101	O
)	O
k	O
(	O
cid:20	O
)	O
k=1	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
=	O
(	O
cid:20	O
)	O
p	O
(	O
v|h	O
,	O
θ	O
)	O
=	O
p	O
(	O
hk|v	O
,	O
θ	O
)	O
=	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
k	O
by	O
symmetry	O
,	O
one	O
can	O
show	O
that	O
we	O
can	O
generate	O
data	O
given	O
the	O
hidden	B
variables	I
as	O
follows	O
:	O
p	O
(	O
vr|h	O
,	O
θ	O
)	O
=	O
ber	O
(	O
vr|sigm	O
(	O
wt	O
r	O
,	O
:h	O
)	O
)	O
r	O
r	O
(	O
27.102	O
)	O
986	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
we	O
can	O
write	O
this	O
in	O
matrix-vetor	O
notation	O
as	O
follows	O
:	O
e	O
[	O
h|vθ	O
]	O
=	O
sigm	O
(	O
wt	O
v	O
)	O
e	O
[	O
v|h	O
,	O
θ	O
]	O
=	O
sigm	O
(	O
wh	O
)	O
(	O
27.103	O
)	O
(	O
27.104	O
)	O
the	O
weights	O
in	O
w	O
are	O
called	O
the	O
generative	B
weights	I
,	O
since	O
they	O
are	O
used	O
to	O
generate	O
the	O
observations	O
,	O
and	O
the	O
weights	O
in	O
wt	O
are	O
called	O
the	O
recognition	B
weights	I
,	O
since	O
they	O
are	O
used	O
to	O
recognize	O
the	O
input	O
.	O
from	O
equation	O
27.101	O
,	O
we	O
see	O
that	O
we	O
activate	O
hidden	B
node	O
k	O
in	O
proportion	O
to	O
how	O
much	O
the	O
input	O
vector	O
v	O
“	O
looks	O
like	O
”	O
the	O
weight	B
vector	I
w	O
:	O
,k	O
(	O
up	O
to	O
scaling	O
factors	B
)	O
.	O
thus	O
each	O
hidden	B
node	O
captures	O
certain	O
features	B
of	O
the	O
input	O
,	O
as	O
encoded	O
in	O
its	O
weight	B
vector	I
,	O
similar	B
to	O
a	O
feedforward	B
neural	I
network	I
.	O
27.7.1.2	O
categorical	B
rbm	O
we	O
can	O
extend	O
the	O
binary	O
rbm	O
to	O
categorical	B
visible	O
variables	O
by	O
using	O
a	O
1-of-c	O
encoding	O
,	O
where	O
c	O
is	O
the	O
number	O
of	O
states	O
for	O
each	O
vir	O
.	O
we	O
deﬁne	O
a	O
new	O
energy	B
function	I
as	O
follows	O
(	O
salakhutdinov	O
et	O
al	O
.	O
2007	O
;	O
salakhutdinov	O
and	O
hinton	O
2010	O
)	O
:	O
e	O
(	O
v	O
,	O
h	O
;	O
θ	O
)	O
(	O
cid:2	O
)	O
−	O
r	O
(	O
cid:4	O
)	O
k	O
(	O
cid:4	O
)	O
c	O
(	O
cid:4	O
)	O
r=1	O
k=1	O
c=1	O
the	O
full	B
conditionals	O
are	O
given	O
by	O
p	O
(	O
vr|h	O
,	O
θ	O
)	O
=	O
cat	O
(	O
s	O
(	O
{	O
bc	O
p	O
(	O
hk	O
=	O
1|c	O
,	O
θ	O
)	O
=	O
sigm	O
(	O
ck	O
+	O
vc	O
rhkw	O
c	O
rk	O
−	O
r	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
hkw	O
c	O
r=1	O
k	O
r	O
+	O
vc	O
rw	O
c	O
rk	O
)	O
rk	O
}	O
c	O
c=1	O
)	O
)	O
c	O
(	O
cid:4	O
)	O
r	O
−	O
k	O
(	O
cid:4	O
)	O
vc	O
rbc	O
c=1	O
k=1	O
hkck	O
(	O
27.105	O
)	O
(	O
27.106	O
)	O
(	O
27.107	O
)	O
27.7.1.3	O
gaussian	O
rbm	O
r	O
c	O
we	O
can	O
generalize	B
the	O
model	O
to	O
handle	O
real-valued	O
data	O
.	O
in	O
particular	O
,	O
a	O
gaussian	O
rbm	O
has	O
the	O
following	O
energy	B
function	I
:	O
e	O
(	O
v	O
,	O
h|θ	O
)	O
=	O
−	O
r	O
(	O
cid:4	O
)	O
k	O
(	O
cid:4	O
)	O
r=1	O
k=1	O
wrkhkvr	O
−	O
1	O
2	O
r	O
(	O
cid:4	O
)	O
(	O
vr	O
−	O
br	O
)	O
2	O
−	O
k	O
(	O
cid:4	O
)	O
r=1	O
k=1	O
akhk	O
(	O
27.108	O
)	O
the	O
parameters	O
of	O
the	O
model	O
are	O
θ	O
=	O
(	O
wrk	O
,	O
ak	O
,	O
br	O
)	O
.	O
(	O
we	O
have	O
assumed	O
the	O
data	O
is	O
standardized	B
,	O
so	O
we	O
ﬁx	O
the	O
variance	B
to	O
σ2	O
=	O
1	O
.	O
)	O
compare	O
this	O
to	O
a	O
gaussian	O
in	O
information	B
form	I
:	O
nc	O
(	O
v|η	O
,	O
λ	O
)	O
∝	O
exp	O
(	O
ηt	O
v	O
−	O
1	O
2	O
vt	O
λv	O
)	O
(	O
cid:7	O
)	O
where	O
η	O
=	O
λμ	O
.	O
we	O
see	O
that	O
we	O
have	O
set	O
λ	O
=	O
i	O
,	O
and	O
η	O
=	O
given	O
by	O
μ	O
=	O
λ−1η	O
=	O
k	O
hkw	O
:	O
,k.	O
thus	O
the	O
mean	B
is	O
k	O
hkw	O
:	O
,k.	O
the	O
full	B
conditionals	O
,	O
which	O
are	O
needed	O
for	O
inference	B
and	O
(	O
cid:7	O
)	O
(	O
27.109	O
)	O
27.7.	O
restricted	O
boltzmann	O
machines	O
(	O
rbms	O
)	O
learning	B
,	O
are	O
given	O
by	O
p	O
(	O
vr|h	O
,	O
θ	O
)	O
=n	O
(	O
vr|br	O
+	O
(	O
cid:10	O
)	O
wrkhk	O
,	O
1	O
)	O
(	O
cid:11	O
)	O
p	O
(	O
hk	O
=	O
1|v	O
,	O
θ	O
)	O
=	O
sigm	O
ck	O
+	O
wrkvr	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
k	O
987	O
(	O
27.110	O
)	O
(	O
27.111	O
)	O
r	O
we	O
see	O
that	O
each	O
visible	B
unit	O
has	O
a	O
gaussian	O
distribution	O
whose	O
mean	B
is	O
a	O
function	O
of	O
the	O
hidden	B
bit	O
vector	O
.	O
more	O
powerful	O
models	O
,	O
which	O
make	O
the	O
(	O
co	O
)	O
variance	B
depend	O
on	O
the	O
hidden	B
state	O
,	O
can	O
also	O
be	O
developed	O
(	O
ranzato	O
and	O
hinton	O
2010	O
)	O
.	O
27.7.1.4	O
rbms	O
with	O
gaussian	O
hidden	B
units	I
if	O
we	O
use	O
gaussian	O
latent	B
variables	O
and	O
gaussian	O
visible	B
variables	I
,	O
we	O
get	O
an	O
undirected	B
version	O
of	O
factor	B
analysis	I
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
it	O
is	O
identical	O
to	O
the	O
standard	O
directed	O
version	O
(	O
marks	O
and	O
movellan	O
2001	O
)	O
.	O
if	O
we	O
use	O
gaussian	O
latent	B
variables	O
and	O
categorical	B
observed	O
variables	O
,	O
we	O
get	O
an	O
undirected	B
version	O
of	O
categorical	B
pca	O
(	O
section	O
27.2.2	O
)	O
.	O
in	O
(	O
salakhutdinov	O
et	O
al	O
.	O
2007	O
)	O
,	O
this	O
was	O
applied	O
to	O
the	O
netﬂix	O
collaborative	B
ﬁltering	I
problem	O
,	O
but	O
was	O
found	O
to	O
be	O
signiﬁcantly	O
inferior	O
to	O
using	O
binary	O
latent	O
variables	O
,	O
which	O
have	O
more	O
expressive	O
power	O
.	O
27.7.2	O
learning	B
rbms	O
27.7.2.1	O
in	O
this	O
section	O
,	O
we	O
discuss	O
some	O
ways	O
to	O
compute	O
ml	O
parameter	B
estimates	O
of	O
rbms	O
,	O
using	O
gradient-based	O
optimizers	O
.	O
it	O
is	O
common	O
to	O
use	O
stochastic	B
gradient	I
descent	I
,	O
since	O
rbms	O
often	O
have	O
many	O
parameters	O
and	O
therefore	O
need	O
to	O
be	O
trained	O
on	O
very	O
large	O
datasets	O
.	O
in	O
addition	O
,	O
it	O
is	O
standard	O
to	O
use	O
(	O
cid:2	O
)	O
2	O
regularization	B
,	O
a	O
technique	O
that	O
is	O
often	O
called	O
weight	B
decay	I
in	O
this	O
context	O
.	O
this	O
requires	O
a	O
very	O
small	O
change	O
to	O
the	O
objective	O
and	O
gradient	O
,	O
as	O
discussed	O
in	O
section	O
8.3.6.	O
deriving	O
the	O
gradient	O
using	O
p	O
(	O
h	O
,	O
v|θ	O
)	O
to	O
compute	O
the	O
gradient	O
,	O
we	O
can	O
modify	O
the	O
equations	O
from	O
section	O
19.5.2	O
,	O
which	O
show	O
how	O
to	O
ﬁt	O
a	O
generic	O
latent	O
variable	O
maxent	O
model	O
.	O
in	O
the	O
context	O
of	O
the	O
boltzmann	O
machine	O
,	O
we	O
have	O
one	O
feature	O
per	O
edge	O
,	O
so	O
the	O
gradient	O
is	O
given	O
by	O
e	O
[	O
vrhk|vi	O
,	O
θ	O
]	O
−	O
e	O
[	O
vrhk|θ	O
]	O
(	O
cid:14	O
)	O
(	O
cid:13	O
)	O
we	O
can	O
write	O
this	O
in	O
matrix-vector	O
form	O
as	O
follows	O
:	O
∂	O
(	O
cid:2	O
)	O
∂wrk	O
n	O
(	O
cid:4	O
)	O
(	O
27.112	O
)	O
1	O
n	O
i=1	O
=	O
(	O
cid:14	O
)	O
−	O
ep	O
(	O
·|θ	O
)	O
(	O
cid:13	O
)	O
vht	O
∇w	O
(	O
cid:2	O
)	O
=	O
epemp	O
(	O
·|θ	O
)	O
vht	O
(	O
27.113	O
)	O
where	O
pemp	O
(	O
v	O
,	O
h|θ	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
h|v	O
,	O
θ	O
)	O
pemp	O
(	O
v	O
)	O
,	O
and	O
pemp	O
(	O
v	O
)	O
=	O
1	O
distribution	O
.	O
hk	O
=	O
1	O
.	O
)	O
i=1	O
δvi	O
(	O
v	O
)	O
is	O
the	O
empirical	O
(	O
we	O
can	O
derive	O
a	O
similar	B
expression	O
for	O
the	O
bias	B
terms	O
by	O
setting	O
vr	O
=	O
1	O
or	O
n	O
the	O
ﬁrst	O
term	O
on	O
the	O
gradient	O
,	O
when	O
v	O
is	O
ﬁxed	O
to	O
a	O
data	O
case	O
,	O
is	O
sometimes	O
called	O
the	O
clamped	B
phase	I
,	O
and	O
the	O
second	O
term	O
,	O
when	O
v	O
is	O
free	O
,	O
is	O
sometimes	O
called	O
the	O
unclamped	O
(	O
cid:7	O
)	O
n	O
988	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
27.7.2.2	O
phase	B
.	O
when	O
the	O
model	O
expectations	O
match	O
the	O
empirical	O
expectations	O
,	O
the	O
two	O
terms	O
cancel	O
out	O
,	O
the	O
gradient	O
becomes	O
zero	O
and	O
learning	B
stops	O
.	O
this	O
algorithm	O
was	O
ﬁrst	O
proposed	O
in	O
(	O
ackley	O
et	O
al	O
.	O
1985	O
)	O
.	O
the	O
main	O
problem	O
is	O
efficiently	O
computing	O
the	O
expectations	O
.	O
we	O
discuss	O
some	O
ways	O
to	O
do	O
this	O
below	O
.	O
deriving	O
the	O
gradient	O
using	O
p	O
(	O
v|θ	O
)	O
we	O
now	O
present	O
an	O
alternative	O
way	O
to	O
derive	O
equation	O
27.112	O
,	O
which	O
also	O
applies	O
to	O
other	O
energy	B
based	I
models	I
.	O
first	O
we	O
marginalize	O
out	O
the	O
hidden	B
variables	I
and	O
write	O
the	O
rbm	O
in	O
the	O
form	O
p	O
(	O
v|θ	O
)	O
=	O
1	O
z	O
(	O
θ	O
)	O
exp	O
(	O
−f	O
(	O
v	O
;	O
θ	O
)	O
)	O
,	O
wheref	O
(	O
v	O
;	O
θ	O
)	O
is	O
the	O
free	B
energy	I
:	O
(	O
cid:11	O
)	O
(	O
27.114	O
)	O
(	O
27.115	O
)	O
(	O
27.116	O
)	O
(	O
27.117	O
)	O
(	O
27.118	O
)	O
(	O
27.119	O
)	O
(	O
27.120	O
)	O
(	O
27.121	O
)	O
(	O
27.122	O
)	O
(	O
27.123	O
)	O
e	O
(	O
v	O
,	O
h	O
)	O
=	O
exp	O
vrhkwrk	O
(	O
cid:10	O
)	O
r	O
(	O
cid:4	O
)	O
k	O
(	O
cid:4	O
)	O
(	O
cid:11	O
)	O
k=1	O
r=1	O
(	O
cid:11	O
)	O
h	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
k	O
(	O
cid:20	O
)	O
k	O
(	O
cid:20	O
)	O
k=1	O
h	O
k	O
(	O
cid:20	O
)	O
(	O
cid:4	O
)	O
k=1	O
(	O
cid:10	O
)	O
h	O
(	O
cid:4	O
)	O
r	O
(	O
cid:4	O
)	O
(	O
cid:10	O
)	O
r	O
(	O
cid:4	O
)	O
r=1	O
r	O
(	O
cid:4	O
)	O
r=1	O
exp	O
(	O
cid:10	O
)	O
hr∈	O
{	O
0,1	O
}	O
vrhrwrk	O
(	O
cid:11	O
)	O
1	O
+	O
exp	O
(	O
r=1	O
vrwrk	O
)	O
exp	O
vrhkwrk	O
f	O
(	O
v	O
)	O
(	O
cid:2	O
)	O
=	O
=	O
=	O
using	O
the	O
fact	O
that	O
z	O
(	O
θ	O
)	O
=	O
∇	O
(	O
cid:2	O
)	O
(	O
θ	O
)	O
=−	O
1	O
n	O
next	O
we	O
write	O
the	O
(	O
scaled	O
)	O
log-likelihood	O
in	O
the	O
following	O
form	O
:	O
f	O
(	O
vi|θ	O
)	O
−	O
log	O
z	O
(	O
θ	O
)	O
(	O
cid:2	O
)	O
(	O
θ	O
)	O
=	O
n	O
(	O
cid:4	O
)	O
k=1	O
n	O
(	O
cid:4	O
)	O
i=1	O
1	O
n	O
i=1	O
(	O
cid:7	O
)	O
v	O
exp	O
(	O
−f	O
(	O
v	O
;	O
θ	O
)	O
)	O
we	O
have	O
∇f	O
(	O
vi	O
)	O
−	O
∇z	O
(	O
cid:4	O
)	O
log	O
p	O
(	O
vi|θ	O
)	O
=	O
−	O
1	O
n	O
n	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
n	O
(	O
cid:4	O
)	O
∇f	O
(	O
vi	O
)	O
+	O
∇f	O
(	O
v	O
)	O
i=1	O
i=1	O
z	O
z	O
v	O
∇f	O
(	O
vi	O
)	O
+e	O
[	O
∇f	O
(	O
v	O
)	O
]	O
exp	O
(	O
−f	O
(	O
v	O
)	O
)	O
=	O
−	O
1	O
n	O
=	O
−	O
1	O
n	O
plugging	O
in	O
the	O
free	B
energy	I
(	O
equation	O
27.117	O
)	O
,	O
one	O
can	O
show	O
that	O
f	O
(	O
v	O
)	O
=	O
−vr	O
e	O
[	O
hk|v	O
,	O
θ	O
]	O
=	O
−e	O
[	O
vrhk|v	O
,	O
θ	O
]	O
(	O
cid:2	O
)	O
(	O
θ	O
)	O
=	O
1	O
n	O
e	O
[	O
vrhk|v	O
,	O
θ	O
]	O
−	O
e	O
[	O
vrhk|θ	O
]	O
which	O
matches	O
equation	O
27.112	O
.	O
∂	O
∂wrk	O
hence	O
∂	O
∂wrk	O
i=1	O
n	O
(	O
cid:4	O
)	O
i=1	O
27.7.	O
restricted	O
boltzmann	O
machines	O
(	O
rbms	O
)	O
989	O
(	O
cid:43	O
)	O
(	O
cid:77	O
)	O
(	O
cid:31	O
)	O
(	O
cid:59	O
)	O
(	O
cid:76	O
)	O
(	O
cid:43	O
)	O
(	O
cid:77	O
)	O
(	O
cid:33	O
)	O
(	O
cid:19	O
)	O
(	O
cid:31	O
)	O
(	O
cid:59	O
)	O
(	O
cid:76	O
)	O
(	O
cid:43	O
)	O
(	O
cid:77	O
)	O
(	O
cid:33	O
)	O
(	O
cid:20	O
)	O
(	O
cid:31	O
)	O
(	O
cid:59	O
)	O
(	O
cid:76	O
)	O
(	O
cid:43	O
)	O
(	O
cid:77	O
)	O
(	O
cid:33	O
)	O
(	O
cid:76	O
)	O
(	O
cid:81	O
)	O
(	O
cid:73	O
)	O
(	O
cid:76	O
)	O
(	O
cid:81	O
)	O
(	O
cid:76	O
)	O
(	O
cid:87	O
)	O
(	O
cid:92	O
)	O
(	O
cid:59	O
)	O
(	O
cid:76	O
)	O
(	O
cid:55	O
)	O
(	O
cid:3	O
)	O
(	O
cid:32	O
)	O
(	O
cid:3	O
)	O
(	O
cid:19	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:71	O
)	O
(	O
cid:68	O
)	O
(	O
cid:87	O
)	O
(	O
cid:68	O
)	O
(	O
cid:55	O
)	O
(	O
cid:3	O
)	O
(	O
cid:32	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:20	O
)	O
(	O
cid:16	O
)	O
(	O
cid:86	O
)	O
(	O
cid:87	O
)	O
(	O
cid:72	O
)	O
(	O
cid:83	O
)	O
(	O
cid:3	O
)	O
(	O
cid:85	O
)	O
(	O
cid:72	O
)	O
(	O
cid:70	O
)	O
(	O
cid:82	O
)	O
(	O
cid:81	O
)	O
(	O
cid:86	O
)	O
(	O
cid:87	O
)	O
(	O
cid:85	O
)	O
(	O
cid:88	O
)	O
(	O
cid:70	O
)	O
(	O
cid:87	O
)	O
(	O
cid:76	O
)	O
(	O
cid:82	O
)	O
(	O
cid:81	O
)	O
(	O
cid:86	O
)	O
(	O
cid:55	O
)	O
(	O
cid:3	O
)	O
(	O
cid:32	O
)	O
(	O
cid:3	O
)	O
(	O
cid:76	O
)	O
(	O
cid:81	O
)	O
(	O
cid:73	O
)	O
(	O
cid:76	O
)	O
(	O
cid:81	O
)	O
(	O
cid:76	O
)	O
(	O
cid:87	O
)	O
(	O
cid:92	O
)	O
(	O
cid:29	O
)	O
(	O
cid:3	O
)	O
(	O
cid:72	O
)	O
(	O
cid:84	O
)	O
(	O
cid:88	O
)	O
(	O
cid:76	O
)	O
(	O
cid:79	O
)	O
(	O
cid:76	O
)	O
(	O
cid:69	O
)	O
(	O
cid:85	O
)	O
(	O
cid:76	O
)	O
(	O
cid:88	O
)	O
(	O
cid:80	O
)	O
(	O
cid:3	O
)	O
(	O
cid:86	O
)	O
(	O
cid:68	O
)	O
(	O
cid:80	O
)	O
(	O
cid:83	O
)	O
(	O
cid:79	O
)	O
(	O
cid:72	O
)	O
(	O
cid:86	O
)	O
figure	O
27.31	O
illustration	O
of	O
gibbs	O
sampling	O
in	O
an	O
rbm	O
.	O
the	O
visible	B
nodes	I
are	O
initialized	O
at	O
a	O
datavector	O
,	O
then	O
we	O
sample	O
a	O
hidden	B
vector	O
,	O
then	O
another	O
visible	B
vector	O
,	O
etc	O
.	O
eventually	O
(	O
at	O
“	O
inﬁnity	O
”	O
)	O
we	O
will	O
be	O
producing	O
samples	B
from	O
the	O
joint	B
distribution	I
p	O
(	O
v	O
,	O
h|θ	O
)	O
.	O
27.7.2.3	O
approximating	O
the	O
expectations	O
we	O
can	O
approximate	O
the	O
expectations	O
needed	O
to	O
evaluate	O
the	O
gradient	O
by	O
performing	O
block	O
gibbs	O
sampling	O
,	O
using	O
equations	O
27.101	O
and	O
27.102.	O
in	O
more	O
detail	O
,	O
we	O
can	O
sample	O
from	O
the	O
joint	B
distribution	I
p	O
(	O
v	O
,	O
h|θ	O
)	O
as	O
follows	O
:	O
initialize	O
the	O
chain	O
at	O
vv1	O
(	O
e.g	O
.	O
by	O
setting	O
v1	O
=	O
vi	O
for	O
some	O
data	O
vector	O
)	O
,	O
and	O
then	O
sample	O
from	O
h1	O
∼	O
p	O
(	O
h|v1	O
)	O
,	O
then	O
from	O
v2	O
∼	O
p	O
(	O
v|h1	O
)	O
,	O
then	O
from	O
h2	O
∼	O
p	O
(	O
h|v2	O
)	O
,	O
etc	O
.	O
see	O
figure	O
27.31	O
for	O
an	O
illustration	O
.	O
note	O
,	O
however	O
,	O
that	O
we	O
have	O
to	O
wait	O
until	O
the	O
markov	O
chain	O
reaches	O
equilibrium	O
(	O
i.e.	O
,	O
until	O
it	O
has	O
“	O
burned	B
in	I
”	O
)	O
before	O
we	O
can	O
interpret	O
the	O
samples	B
as	O
coming	O
from	O
the	O
joint	B
distribution	I
of	O
interest	O
,	O
and	O
this	O
might	O
take	O
a	O
long	O
time	O
.	O
a	O
faster	O
alternative	O
is	O
to	O
use	O
mean	B
ﬁeld	I
,	O
where	O
we	O
make	O
the	O
approximation	O
e	O
[	O
vrhk	O
]	O
≈	O
e	O
[	O
vr	O
]	O
e	O
[	O
hk	O
]	O
.	O
however	O
,	O
since	O
p	O
(	O
v	O
,	O
h	O
)	O
is	O
typically	O
multimodal	O
,	O
this	O
is	O
usually	O
a	O
very	O
poor	O
approx-	O
imation	O
,	O
since	O
it	O
will	O
average	O
over	O
different	O
modes	O
(	O
see	O
section	O
21.2.2	O
)	O
.	O
furthermore	O
,	O
there	O
is	O
a	O
more	O
subtle	O
reason	O
not	O
to	O
use	O
mean	B
ﬁeld	I
:	O
since	O
the	O
gradient	O
has	O
the	O
form	O
e	O
[	O
vrhk|v	O
]	O
−	O
e	O
[	O
vrhk	O
]	O
,	O
we	O
see	O
that	O
the	O
negative	O
sign	O
in	O
front	O
means	O
that	O
the	O
method	O
will	O
try	O
to	O
make	O
the	O
variational	O
bound	O
as	O
loose	O
as	O
possible	O
(	O
salakhutdinov	O
and	O
hinton	O
2009	O
)	O
.	O
this	O
explains	O
why	O
earlier	O
attempts	O
to	O
use	O
mean	B
ﬁeld	I
to	O
learn	O
boltzmann	O
machines	O
(	O
e.g.	O
,	O
(	O
kappen	O
and	O
rodriguez	O
1998	O
)	O
)	O
did	O
not	O
work	O
.	O
27.7.2.4	O
contrastive	B
divergence	I
the	O
problem	O
with	O
using	O
gibbs	O
sampling	O
to	O
compute	O
the	O
gradient	O
is	O
that	O
it	O
is	O
slow	O
.	O
we	O
now	O
present	O
a	O
faster	O
method	O
known	O
as	O
contrastive	B
divergence	I
or	O
cd	O
(	O
hinton	O
2002	O
)	O
.	O
cd	O
was	O
originally	O
derived	O
by	O
approximating	O
an	O
objective	O
function	O
deﬁned	O
as	O
the	O
difference	O
of	O
two	O
kl	O
divergences	O
,	O
rather	O
than	O
trying	O
to	O
maximize	O
the	O
likelihood	B
itself	O
.	O
however	O
,	O
from	O
an	O
algorithmic	O
point	O
of	O
view	O
,	O
it	O
can	O
be	O
thought	O
of	O
as	O
similar	B
to	O
stochastic	B
gradient	I
descent	I
,	O
except	O
it	O
approxi-	O
mates	O
the	O
“	O
unclamped	O
”	O
expectations	O
with	O
“	O
brief	O
”	O
gibbs	O
sampling	O
where	O
we	O
initialize	O
each	O
markov	O
chain	O
at	O
the	O
data	O
vectors	O
.	O
that	O
is	O
,	O
we	O
approximate	O
the	O
gradient	O
for	O
one	O
datavector	O
as	O
follows	O
:	O
(	O
cid:13	O
)	O
(	O
cid:14	O
)	O
−	O
eq	O
(	O
cid:13	O
)	O
∇w	O
(	O
cid:2	O
)	O
≈	O
e	O
vht|vi	O
(	O
cid:14	O
)	O
vht	O
where	O
q	O
corresponds	O
to	O
the	O
distribution	O
generated	O
by	O
k	O
up-down	B
gibbs	O
sweeps	O
,	O
started	O
at	O
vi	O
,	O
as	O
in	O
figure	O
27.31.	O
this	O
is	O
known	O
as	O
cd-k.	O
in	O
more	O
detail	O
,	O
the	O
procedure	O
(	O
for	O
k	O
=	O
1	O
)	O
is	O
as	O
(	O
27.124	O
)	O
990	O
follows	O
:	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
hi	O
∼	O
p	O
(	O
h|vi	O
,	O
θ	O
)	O
i	O
∼	O
p	O
(	O
v|hi	O
,	O
θ	O
)	O
v	O
(	O
cid:2	O
)	O
i	O
∼	O
p	O
(	O
h|v	O
(	O
cid:2	O
)	O
h	O
(	O
cid:2	O
)	O
i	O
,	O
θ	O
)	O
(	O
cid:13	O
)	O
(	O
cid:14	O
)	O
≈	O
vi	O
(	O
h	O
(	O
cid:2	O
)	O
vht	O
eq	O
i	O
)	O
t	O
we	O
then	O
make	O
the	O
approximation	O
(	O
27.125	O
)	O
(	O
27.126	O
)	O
(	O
27.127	O
)	O
(	O
27.128	O
)	O
such	O
samples	B
are	O
sometimes	O
called	O
fantasy	B
data	I
.	O
we	O
can	O
think	O
of	O
v	O
(	O
cid:2	O
)	O
i	O
as	O
the	O
model	O
’	O
s	O
best	O
attempt	O
at	O
reconstructing	O
vi	O
after	O
being	O
coded	O
and	O
then	O
decoded	O
by	O
the	O
model	O
.	O
this	O
is	O
similar	B
to	O
the	O
way	O
we	O
train	O
auto-encoders	B
,	O
which	O
are	O
models	O
which	O
try	O
to	O
“	O
squeeze	O
”	O
the	O
data	O
through	O
a	O
restricted	O
parametric	O
“	O
bottleneck	B
”	O
(	O
see	O
section	O
28.3.2	O
)	O
.	O
i	O
in	O
the	O
ﬁnal	O
upwards	O
pass	O
,	O
since	O
this	O
reduces	O
the	O
variance	B
.	O
however	O
,	O
it	O
is	O
not	O
valid	O
to	O
use	O
e	O
[	O
h|vi	O
]	O
instead	O
of	O
sampling	O
hi	O
∼	O
p	O
(	O
h|vi	O
)	O
in	O
the	O
earlier	O
upwards	O
passes	O
,	O
because	O
then	O
each	O
hidden	B
unit	O
would	O
be	O
able	O
to	O
pass	O
more	O
than	O
1	O
bit	O
of	O
information	B
,	O
so	O
it	O
would	O
not	O
act	O
as	O
much	O
of	O
a	O
bottleneck	B
.	O
in	O
practice	O
,	O
it	O
is	O
common	O
to	O
use	O
e	O
[	O
h|v	O
(	O
cid:2	O
)	O
i	O
]	O
instead	O
of	O
a	O
sampled	O
value	O
h	O
(	O
cid:2	O
)	O
the	O
whole	O
procedure	O
is	O
summarized	O
in	O
algorithm	O
3	O
.	O
(	O
note	O
that	O
we	O
follow	O
the	O
positive	O
gradient	O
since	O
we	O
are	O
maximizing	O
likelihood	B
.	O
)	O
various	O
tricks	O
can	O
be	O
used	O
to	O
speed	O
this	O
algorithm	O
up	O
,	O
such	O
as	O
using	O
a	O
momentum	B
term	O
(	O
section	O
8.3.2	O
)	O
,	O
using	O
mini-batches	O
,	O
averaging	O
the	O
updates	O
,	O
etc	O
.	O
such	O
details	O
can	O
be	O
found	O
in	O
(	O
hinton	O
2010	O
;	O
swersky	O
et	O
al	O
.	O
2010	O
)	O
.	O
algorithm	O
27.3	O
:	O
cd-1	O
training	O
for	O
an	O
rbm	O
with	O
binary	O
hidden	O
and	O
visible	B
units	O
1	O
initialize	O
weights	O
w	O
∈	O
r	O
2	O
t	O
:	O
=	O
0	O
;	O
3	O
for	O
each	O
epoch	B
do	O
4	O
r×k	O
randomly	O
;	O
t	O
:	O
=	O
t	O
+	O
1	O
;	O
for	O
each	O
minibatch	O
of	O
size	O
b	O
do	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
13	O
14	O
set	O
minibatch	O
gradient	O
to	O
zero	O
,	O
g	O
:	O
=	O
0	O
;	O
for	O
each	O
case	O
vi	O
in	O
the	O
minibatch	O
do	O
compute	O
μi	O
=	O
e	O
[	O
h|vi	O
,	O
w	O
]	O
;	O
sample	O
hi	O
∼	O
p	O
(	O
h|vi	O
,	O
w	O
)	O
;	O
i	O
∼	O
p	O
(	O
v|hi	O
,	O
w	O
)	O
;	O
sample	O
v	O
(	O
cid:2	O
)	O
i	O
=	O
e	O
[	O
h|v	O
(	O
cid:2	O
)	O
compute	O
μ	O
(	O
cid:2	O
)	O
i	O
,	O
w	O
]	O
;	O
compute	O
gradient	O
∇w	O
=	O
(	O
vi	O
)	O
(	O
μi	O
)	O
t	O
−	O
(	O
v	O
(	O
cid:2	O
)	O
accumulate	O
g	O
:	O
=	O
g	O
+	O
∇w	O
;	O
update	O
parameters	O
w	O
:	O
=	O
w	O
+	O
(	O
αt/b	O
)	O
g	O
i	O
)	O
(	O
μ	O
(	O
cid:2	O
)	O
i	O
)	O
t	O
;	O
27.7.2.5	O
persistent	O
cd	O
in	O
section	O
19.5.5	O
,	O
we	O
presented	O
a	O
technique	O
called	O
stochastic	B
maximum	I
likelihood	I
(	O
sml	O
)	O
for	O
ﬁtting	O
maxent	B
models	O
.	O
this	O
avoids	O
the	O
need	O
to	O
run	O
mcmc	O
to	O
convergence	O
at	O
each	O
iteration	O
,	O
27.7.	O
restricted	O
boltzmann	O
machines	O
(	O
rbms	O
)	O
991	O
by	O
exploiting	O
the	O
fact	O
that	O
the	O
parameters	O
are	O
changing	O
slowly	O
,	O
so	O
the	O
markov	O
chains	O
will	O
not	O
be	O
pushed	O
too	O
far	O
from	O
equilibrium	O
after	O
each	O
update	O
(	O
younes	O
1989	O
)	O
.	O
in	O
other	O
words	O
,	O
there	O
are	O
two	O
dynamical	O
processes	O
running	O
at	O
different	O
time	O
scales	O
:	O
the	O
states	O
change	O
quickly	O
,	O
and	O
the	O
parameters	O
change	O
slowly	O
.	O
this	O
algorithm	O
was	O
independently	O
rediscovered	O
in	O
(	O
tieleman	O
2008	O
)	O
,	O
who	O
called	O
it	O
persistent	O
cd	O
.	O
see	O
algorithm	O
3	O
for	O
the	O
pseudocode	O
.	O
pcd	O
often	O
works	O
better	O
than	O
cd	O
(	O
see	O
e.g.	O
,	O
(	O
tieleman	O
2008	O
;	O
marlin	O
et	O
al	O
.	O
2010	O
;	O
swersky	O
et	O
al	O
.	O
2010	O
)	O
)	O
,	O
although	O
cd	O
can	O
be	O
faster	O
in	O
the	O
early	O
stages	O
of	O
learning	B
.	O
algorithm	O
27.4	O
:	O
persistent	O
cd	O
for	O
training	O
an	O
rbm	O
with	O
binary	O
hidden	O
and	O
visible	B
units	O
1	O
initialize	O
weights	O
w	O
∈	O
r	O
2	O
initialize	O
chains	O
(	O
vs	O
,	O
hs	O
)	O
s	O
3	O
for	O
t	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
do	O
4	O
d×l	O
randomly	O
;	O
s=1	O
randomly	O
;	O
5	O
6	O
7	O
8	O
9	O
10	O
//	O
mean	B
ﬁeld	I
updates	O
;	O
for	O
each	O
case	O
i	O
=	O
1	O
:	O
n	O
do	O
i	O
w	O
:	O
,k	O
)	O
μik	O
=	O
sigm	O
(	O
vt	O
//	O
mcmc	O
updates	O
;	O
for	O
each	O
sample	O
s	O
=	O
1	O
:	O
s	O
do	O
(	O
cid:7	O
)	O
n	O
i=1	O
vi	O
(	O
μi	O
)	O
t	O
−	O
1	O
//	O
parameter	B
updates	O
;	O
g	O
=	O
1	O
n	O
s	O
(	O
cid:7	O
)	O
s	O
11	O
12	O
w	O
:	O
=	O
w	O
+	O
αtg	O
;	O
13	O
decrease	O
αt	O
s=1	O
vs	O
(	O
hs	O
)	O
t	O
;	O
generate	O
(	O
vs	O
,	O
hs	O
)	O
by	O
brief	O
gibbs	O
sampling	O
from	O
old	O
(	O
vs	O
,	O
hs	O
)	O
27.7.3	O
applications	O
of	O
rbms	O
the	O
main	O
application	O
of	O
rbms	O
is	O
as	O
a	O
building	O
block	O
for	O
deep	B
generative	O
models	O
,	O
which	O
we	O
discuss	O
in	O
section	O
28.2.	O
but	O
they	O
can	O
also	O
be	O
used	O
as	O
substitutes	O
for	O
directed	B
two-layer	O
models	O
.	O
they	O
are	O
particularly	O
useful	O
in	O
cases	O
where	O
inference	B
of	O
the	O
hidden	B
states	O
at	O
test	O
time	O
must	O
be	O
fast	O
.	O
we	O
give	O
some	O
examples	O
below	O
.	O
27.7.3.1	O
language	B
modeling	I
and	O
document	O
retrieval	O
we	O
can	O
use	O
a	O
categorical	B
rbm	O
to	O
deﬁne	O
a	O
generative	O
model	O
for	O
bag-of-words	B
,	O
as	O
an	O
alternative	O
to	O
lda	O
.	O
one	O
subtlety	O
is	O
that	O
the	O
partition	B
function	I
in	O
an	O
undirected	B
models	O
depends	O
on	O
how	O
big	O
the	O
graph	B
is	O
,	O
and	O
therefore	O
on	O
how	O
long	O
the	O
document	O
is	O
.	O
a	O
solution	O
to	O
this	O
was	O
proposed	O
in	O
(	O
salakhutdinov	O
and	O
hinton	O
2010	O
)	O
:	O
use	O
a	O
categorical	B
rbm	O
with	O
tied	B
weights	O
,	O
but	O
multiply	O
the	O
hidden	B
activation	O
bias	B
terms	O
ck	O
by	O
the	O
document	O
length	O
l	O
to	O
compensate	O
form	O
the	O
fact	O
that	O
the	O
observed	O
word-count	O
vector	O
v	O
is	O
larger	O
in	O
magnitude	O
:	O
e	O
(	O
v	O
,	O
h	O
;	O
θ	O
)	O
(	O
cid:2	O
)	O
−	O
k	O
(	O
cid:4	O
)	O
c	O
(	O
cid:4	O
)	O
k	O
−	O
c	O
(	O
cid:4	O
)	O
vchkw	O
c	O
vcbc	O
r	O
−	O
l	O
k	O
(	O
cid:4	O
)	O
k=1	O
c=1	O
c=1	O
k=1	O
hkck	O
(	O
27.129	O
)	O
992	O
chapter	O
27.	O
latent	B
variable	I
models	I
for	O
discrete	B
data	O
data	O
set	O
nips	O
20-news	O
reuters	O
number	O
of	O
docs	O
train	O
test	O
1,690	O
50	O
11,314	O
794,414	O
7,531	O
10,000	O
k	O
¯d	O
st.	O
dev	O
.	O
avg	O
.	O
test	O
perplexity	O
per	O
word	O
(	O
in	O
nats	B
)	O
lda-50	O
lda-200	O
r.	O
soft-50	O
unigram	O
13,649	O
2,000	O
10,000	O
98.0	O
51.8	O
94.6	O
245.3	O
70.8	O
69.3	O
3576	O
1091	O
1437	O
3391	O
1058	O
1142	O
3405	O
953	O
988	O
4385	O
1335	O
2208	O
figure	O
27.32	O
comparison	O
of	O
rbm	O
(	O
replicated	O
softmax	O
)	O
and	O
lda	O
on	O
three	O
corpora	O
.	O
k	O
is	O
the	O
number	O
of	O
words	O
in	O
the	O
vocabulary	O
,	O
d	O
is	O
the	O
average	O
document	O
length	O
,	O
and	O
st.	O
dev	O
.	O
is	O
the	O
standard	B
deviation	I
of	O
the	O
document	O
length	O
.	O
source	O
:	O
(	O
salakhutdinov	O
and	O
hinton	O
2010	O
)	O
.	O
20-newsgroups	O
replicated	O
softmax	O
50−d	O
lda	O
50−d	O
60	O
50	O
40	O
30	O
20	O
10	O
)	O
%	O
(	O
n	O
o	O
i	O
s	O
i	O
c	O
e	O
r	O
p	O
reuters	O
replicated	O
softmax	O
50−d	O
lda	O
50−d	O
50	O
40	O
30	O
20	O
10	O
)	O
%	O
i	O
(	O
n	O
o	O
s	O
c	O
e	O
r	O
p	O
i	O
0.02	O
0.1	O
0.4	O
1.6	O
6.4	O
25.6	O
100	O
recall	B
(	O
%	O
)	O
0.001	O
0.006	O
0.051	O
0.4	O
1.6	O
6.4	O
25.6	O
100	O
recall	B
(	O
%	O
)	O
figure	O
27.33	O
precision-recall	O
curves	O
for	O
rbm	O
(	O
replicated	O
softmax	O
)	O
and	O
lda	O
on	O
two	O
corpora	O
.	O
from	O
figure	O
3	O
of	O
(	O
salakhutdinov	O
and	O
hinton	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
ruslan	O
salakhutdinov	O
.	O
(	O
cid:7	O
)	O
l	O
where	O
vc	O
=	O
l=1	O
i	O
(	O
yil	O
=	O
c	O
)	O
.	O
this	O
is	O
like	O
having	O
a	O
single	O
multinomial	O
node	O
(	O
so	O
we	O
have	O
dropped	O
the	O
r	O
subscript	O
)	O
with	O
c	O
states	O
,	O
where	O
c	O
is	O
the	O
number	O
of	O
words	O
in	O
the	O
vocabulary	O
.	O
this	O
is	O
called	O
the	O
replicated	B
softmax	I
model	I
(	O
salakhutdinov	O
and	O
hinton	O
2010	O
)	O
,	O
and	O
is	O
an	O
undirected	B
alternative	O
to	O
mpca/	O
lda	O
.	O
we	O
can	O
compare	O
the	O
modeling	O
power	O
of	O
rbms	O
vs	O
lda	O
by	O
measuring	O
the	O
perplexity	B
on	O
a	O
test	O
set	O
.	O
this	O
can	O
be	O
approximated	O
using	O
annealing	B
importance	I
sampling	I
(	O
section	O
24.6.2	O
)	O
.	O
the	O
results	O
are	O
shown	O
in	O
figure	O
27.32.	O
we	O
see	O
that	O
the	O
lda	O
is	O
signiﬁcantly	O
better	O
than	O
a	O
unigram	O
model	O
,	O
but	O
that	O
an	O
rbm	O
is	O
signiﬁcantly	O
better	O
than	O
lda	O
.	O
just	O
a	O
single	O
matrix-vector	O
another	O
advantage	O
of	O
the	O
lda	O
is	O
that	O
inference	B
is	O
fast	O
and	O
exact	O
:	O
multiply	O
followed	O
by	O
a	O
sigmoid	B
nonlinearity	O
,	O
as	O
in	O
equation	O
27.107.	O
in	O
addition	O
to	O
being	O
faster	O
,	O
the	O
rbm	O
is	O
more	O
accurate	O
.	O
this	O
is	O
illustrated	O
in	O
figure	O
27.33	O
,	O
which	O
shows	O
precision-recall	O
curves	O
for	O
rbms	O
and	O
lda	O
on	O
two	O
different	O
corpora	O
.	O
these	O
curves	O
were	O
generated	O
as	O
follows	O
:	O
a	O
query	O
document	O
from	O
the	O
test	O
set	O
is	O
taken	O
,	O
its	O
similarity	O
to	O
all	O
the	O
training	O
documents	O
is	O
computed	O
,	O
where	O
the	O
similarity	O
is	O
deﬁned	O
as	O
the	O
cosine	O
of	O
the	O
angle	O
between	O
the	O
two	O
topic	B
vectors	O
,	O
and	O
then	O
the	O
top	O
m	O
documents	O
are	O
returned	O
for	O
varying	O
m	O
.	O
a	O
retrieved	O
document	O
is	O
considered	O
relevant	O
if	O
it	O
has	O
the	O
same	O
class	O
label	O
as	O
that	O
of	O
the	O
query	O
’	O
s	O
(	O
this	O
is	O
the	O
only	O
place	O
where	O
labels	O
are	O
used	O
)	O
.	O
27.7.	O
restricted	O
boltzmann	O
machines	O
(	O
rbms	O
)	O
993	O
27.7.3.2	O
rbms	O
for	O
collaborative	B
ﬁltering	I
rbms	O
have	O
been	O
applied	O
to	O
the	O
netﬂix	O
collaborative	B
ﬁltering	I
competition	O
(	O
salakhutdinov	O
et	O
al	O
.	O
in	O
fact	O
,	O
an	O
rbm	O
with	O
binary	O
hidden	O
nodes	B
and	O
categorical	B
visible	O
nodes	B
can	O
slightly	O
2007	O
)	O
.	O
outperform	O
svd	O
.	O
by	O
combining	O
the	O
two	O
methods	O
,	O
performance	O
can	O
be	O
further	O
improved	O
.	O
(	O
the	O
winning	O
entry	O
in	O
the	O
challenge	O
was	O
an	O
ensemble	B
of	O
many	O
different	O
types	O
of	O
model	O
(	O
koren	O
2009a	O
)	O
.	O
)	O
exercises	O
exercise	O
27.1	O
partition	B
function	I
for	O
an	O
rbm	O
show	O
how	O
to	O
compute	O
z	O
(	O
θ	O
)	O
for	O
an	O
rbm	O
with	O
k	O
binary	O
hidden	O
nodes	B
and	O
r	O
binary	O
observed	O
nodes	B
in	O
o	O
(	O
r2k	O
)	O
time	O
,	O
assuming	O
k	O
<	O
r.	O
28	O
deep	B
learning	I
28.1	O
introduction	O
many	O
of	O
the	O
models	O
we	O
have	O
looked	O
at	O
in	O
this	O
book	O
have	O
a	O
simple	O
two-layer	O
architecture	O
of	O
the	O
form	O
z	O
→	O
y	O
for	O
unsupervised	O
latent	O
variable	O
models	O
,	O
or	O
x	O
→	O
y	O
for	O
supervised	O
models	O
.	O
however	O
,	O
when	O
we	O
look	O
at	O
the	O
brain	O
,	O
we	O
seem	O
many	O
levels	O
of	O
processing	O
.	O
it	O
is	O
believed	O
that	O
each	O
level	O
is	O
learning	B
features	O
or	O
representations	O
at	O
increasing	O
levels	O
of	O
abstraction	O
.	O
for	O
example	O
,	O
the	O
standard	B
model	I
of	O
the	O
visual	O
cortex	O
(	O
hubel	O
and	O
wiesel	O
1962	O
;	O
serre	O
et	O
al	O
.	O
2005	O
;	O
ranzato	O
et	O
al	O
.	O
2007	O
)	O
suggests	O
that	O
(	O
roughly	O
speaking	O
)	O
the	O
brain	O
ﬁrst	O
extracts	O
edges	B
,	O
then	O
patches	O
,	O
then	O
surfaces	O
,	O
then	O
objects	O
,	O
etc	O
.	O
(	O
see	O
e.g.	O
,	O
(	O
palmer	O
1999	O
;	O
kandel	O
et	O
al	O
.	O
2000	O
)	O
for	O
more	O
information	B
about	O
how	O
the	O
brain	O
might	O
perform	O
vision	O
.	O
)	O
this	O
observation	B
has	O
inspired	O
a	O
recent	O
trend	O
in	O
machine	B
learning	I
known	O
as	O
deep	B
learning	I
(	O
bengio	O
2009	O
)	O
,	O
deeplearning.net	O
,	O
and	O
the	O
references	O
therein	O
)	O
,	O
which	O
attempts	O
to	O
(	O
note	O
the	O
idea	O
can	O
be	O
applied	O
to	O
non-vision	O
(	O
see	O
e.g.	O
,	O
replicate	O
this	O
kind	O
of	O
architecture	O
in	O
a	O
computer	O
.	O
problems	O
as	O
well	O
,	O
such	O
as	O
speech	O
and	O
language	O
.	O
)	O
in	O
this	O
chapter	O
,	O
we	O
give	O
a	O
brief	O
overview	O
of	O
this	O
new	O
ﬁeld	O
.	O
however	O
,	O
we	O
caution	O
the	O
reader	O
that	O
the	O
topic	B
of	O
deep	B
learning	I
is	O
currently	O
evolving	O
very	O
quickly	O
,	O
so	O
the	O
material	O
in	O
this	O
chapter	O
may	O
soon	O
be	O
outdated	O
.	O
28.2	O
deep	B
generative	O
models	O
deep	B
models	O
often	O
have	O
millions	O
of	O
parameters	O
.	O
acquiring	O
enough	O
labeled	O
data	O
to	O
train	O
such	O
models	O
is	O
diffcult	O
,	O
despite	O
crowd	B
sourcing	I
sites	O
such	O
as	O
mechanical	O
turk	O
.	O
in	O
simple	O
settings	O
,	O
such	O
as	O
hand-written	O
character	O
recognition	O
,	O
it	O
is	O
possible	O
to	O
generate	O
lots	O
of	O
labeled	O
data	O
by	O
making	O
modiﬁed	O
copies	O
of	O
a	O
small	O
manually	O
labeled	O
training	O
set	O
(	O
see	O
e.g.	O
,	O
figure	O
16.13	O
)	O
,	O
but	O
it	O
seems	O
unlikely	O
that	O
this	O
approach	O
will	O
scale	O
to	O
complex	O
scenes.1	O
to	O
overcome	O
the	O
problem	O
of	O
needing	O
labeled	O
training	O
data	O
,	O
we	O
will	O
focus	O
on	O
unsupervised	B
learning	I
.	O
the	O
most	O
natural	O
way	O
to	O
perform	O
this	O
is	O
to	O
use	O
generative	O
models	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
three	O
different	O
kinds	O
of	O
deep	B
generative	O
models	O
:	O
directed	B
,	O
undirected	B
,	O
and	O
mixed	O
.	O
1.	O
there	O
have	O
been	O
some	O
attempts	O
to	O
use	O
computer	O
graphics	O
and	O
video	O
games	O
to	O
generate	O
realistic-looking	O
images	O
of	O
complex	O
scenes	O
,	O
and	O
then	O
to	O
use	O
this	O
as	O
training	O
data	O
for	O
computer	O
vision	O
systems	O
.	O
however	O
,	O
often	O
graphics	O
programs	O
cut	O
corners	O
in	O
order	O
to	O
make	O
perceptually	O
appealing	O
images	O
which	O
are	O
not	O
reﬂective	O
of	O
the	O
natural	O
statistics	O
of	O
real-world	O
images	O
.	O
996	O
chapter	O
28.	O
deep	B
learning	I
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
28.1	O
some	O
deep	B
multi-layer	O
graphical	O
models	O
.	O
observed	O
variables	O
are	O
at	O
the	O
bottom	O
.	O
(	O
a	O
)	O
a	O
directed	B
model	O
.	O
(	O
b	O
)	O
an	O
undirected	B
model	O
(	O
deep	B
boltzmann	O
machine	O
)	O
.	O
(	O
c	O
)	O
a	O
mixed	O
directed-undirected	O
model	O
(	O
deep	O
belief	O
net	O
)	O
.	O
28.2.1	O
deep	B
directed	I
networks	I
perhaps	O
the	O
most	O
natural	O
way	O
to	O
build	O
a	O
deep	B
generative	O
model	O
is	O
to	O
construct	O
a	O
deep	O
directed	O
graphical	O
model	O
,	O
as	O
shown	O
in	O
figure	O
28.1	O
(	O
a	O
)	O
.	O
the	O
bottom	O
level	O
contains	O
the	O
observed	O
pixels	O
(	O
or	O
whatever	O
the	O
data	O
is	O
)	O
,	O
and	O
the	O
remaining	O
layers	O
are	O
hidden	B
.	O
we	O
have	O
assumed	O
just	O
3	O
layers	O
for	O
notational	O
simplicity	O
.	O
the	O
number	O
and	O
size	O
of	O
layers	O
is	O
usually	O
chosen	O
by	O
hand	O
,	O
although	O
one	O
can	O
also	O
use	O
non-parametric	O
bayesian	O
methods	O
(	O
adams	O
et	O
al	O
.	O
2010	O
)	O
or	O
boosting	B
(	O
chen	O
et	O
al	O
.	O
2010	O
)	O
to	O
infer	O
the	O
model	O
structure	O
.	O
we	O
shall	O
call	O
models	O
of	O
this	O
form	O
deep	B
directed	I
networks	I
or	O
ddns	O
.	O
binary	O
,	O
and	O
all	O
cpds	O
are	O
logistic	B
functions	O
,	O
this	O
is	O
called	O
a	O
sigmoid	B
belief	I
net	I
(	O
neal	O
1992	O
)	O
.	O
this	O
case	O
,	O
the	O
model	O
deﬁnes	O
the	O
following	O
joint	B
distribution	I
:	O
if	O
all	O
the	O
nodes	B
are	O
in	O
(	O
cid:20	O
)	O
(	O
cid:20	O
)	O
i	O
1	O
w0i	O
)	O
)	O
ber	O
(	O
vi|sigm	O
(	O
ht	O
ber	O
(	O
h2k|sigm	O
(	O
ht	O
3	O
w2k	O
)	O
)	O
(	O
cid:20	O
)	O
ber	O
(	O
h1j|sigm	O
(	O
ht	O
(	O
cid:20	O
)	O
ber	O
(	O
h3l|w3l	O
)	O
j	O
2	O
w1j	O
)	O
)	O
(	O
28.1	O
)	O
(	O
28.2	O
)	O
p	O
(	O
h1	O
,	O
h2	O
,	O
h3	O
,	O
v|θ	O
)	O
=	O
k	O
l	O
unfortunately	O
,	O
inference	B
in	O
directed	B
models	O
such	O
as	O
these	O
is	O
intractable	O
because	O
the	O
posterior	O
on	O
the	O
hidden	B
nodes	I
is	O
correlated	O
due	O
to	O
explaining	B
away	I
.	O
one	O
can	O
use	O
fast	O
mean	O
ﬁeld	O
approxi-	O
mations	O
(	O
jaakkola	O
and	O
jordan	O
1996a	O
;	O
saul	O
and	O
jordan	O
2000	O
)	O
,	O
but	O
these	O
may	O
not	O
be	O
very	O
accurate	O
,	O
since	O
they	O
approximate	O
the	O
correlated	O
posterior	O
with	O
a	O
factorial	O
posterior	O
.	O
one	O
can	O
also	O
use	O
mcmc	O
inference	B
(	O
neal	O
1992	O
;	O
adams	O
et	O
al	O
.	O
2010	O
)	O
,	O
but	O
this	O
can	O
be	O
quite	O
slow	O
because	O
the	O
variables	O
are	O
highly	O
correlated	O
.	O
slow	O
inference	B
also	O
results	O
in	O
slow	O
learning	B
.	O
28.2.2	O
deep	B
boltzmann	O
machines	O
a	O
natural	O
alternative	O
to	O
a	O
directed	B
model	O
is	O
to	O
construct	O
a	O
deep	B
undirected	O
model	O
.	O
for	O
example	O
,	O
we	O
can	O
stack	O
a	O
series	O
of	O
rbms	O
on	O
top	O
of	O
each	O
other	O
,	O
as	O
shown	O
in	O
figure	O
28.1	O
(	O
b	O
)	O
.	O
this	O
is	O
known	O
as	O
a	O
deep	B
boltzmann	O
machine	O
or	O
dbm	O
(	O
salakhutdinov	O
and	O
hinton	O
2009	O
)	O
.	O
if	O
we	O
have	O
3	O
hidden	B
layers	O
,	O
the	O
model	O
is	O
deﬁned	O
as	O
follows	O
:	O
(	O
cid:4	O
)	O
(	O
cid:4	O
)	O
p	O
(	O
h1	O
,	O
h2	O
,	O
h3	O
,	O
v|θ	O
)	O
=	O
1	O
z	O
(	O
θ	O
)	O
exp	O
vih1jw1ij	O
+	O
h1jh2jw2jk	O
+	O
h2kh3lw3kl	O
ij	O
jk	O
kl	O
⎛	O
⎝	O
(	O
cid:4	O
)	O
⎞	O
⎠	O
(	O
28.3	O
)	O
28.2.	O
deep	B
generative	O
models	O
997	O
where	O
we	O
are	O
ignoring	O
constant	O
offset	O
or	O
bias	B
terms	O
.	O
the	O
main	O
advantage	O
over	O
the	O
directed	B
model	O
is	O
that	O
one	O
can	O
perform	O
efficient	O
block	O
(	O
layer-	O
wise	O
)	O
gibbs	O
sampling	O
,	O
or	O
block	O
mean	B
ﬁeld	I
,	O
since	O
all	O
the	O
nodes	B
in	O
each	O
layer	O
are	O
conditionally	B
independent	I
of	O
each	O
other	O
given	O
the	O
layers	O
above	O
and	O
below	O
(	O
salakhutdinov	O
and	O
larochelle	O
2010	O
)	O
.	O
the	O
main	O
disadvantage	O
is	O
that	O
training	O
undirected	O
models	O
is	O
more	O
difficult	O
,	O
because	O
of	O
the	O
partition	B
function	I
.	O
however	O
,	O
below	O
we	O
will	O
see	O
a	O
greedy	O
layer-wise	O
strategy	O
for	O
learning	B
deep	O
undirected	B
models	O
.	O
28.2.3	O
deep	O
belief	O
networks	O
an	O
interesting	O
compromise	O
is	O
to	O
use	O
a	O
model	O
that	O
is	O
partially	O
directed	O
and	O
partially	O
undirected	O
.	O
in	O
particular	O
,	O
suppose	O
we	O
construct	O
a	O
layered	O
model	O
which	O
has	O
directed	B
arrows	O
,	O
except	O
at	O
the	O
top	O
,	O
where	O
there	O
is	O
an	O
undirected	B
bipartite	O
graph	B
,	O
as	O
shown	O
in	O
figure	O
28.1	O
(	O
c	O
)	O
.	O
this	O
model	O
is	O
known	O
as	O
a	O
deep	B
belief	I
network	I
(	O
hinton	O
et	O
al	O
.	O
2006	O
)	O
or	O
dbn.2	O
if	O
we	O
have	O
3	O
hidden	B
layers	O
,	O
the	O
model	O
is	O
deﬁned	O
as	O
follows	O
:	O
p	O
(	O
h1	O
,	O
h2	O
,	O
h3	O
,	O
v|θ	O
)	O
=	O
ber	O
(	O
vi|sigm	O
(	O
ht	O
(	O
cid:20	O
)	O
(	O
28.4	O
)	O
2	O
w2j	O
)	O
1	O
w1i	O
)	O
(	O
cid:20	O
)	O
ber	O
(	O
h1j|sigm	O
(	O
ht	O
(	O
cid:11	O
)	O
j	O
(	O
cid:10	O
)	O
(	O
cid:4	O
)	O
exp	O
kl	O
i	O
1	O
z	O
(	O
θ	O
)	O
h2kh3lw3kl	O
(	O
28.5	O
)	O
essentially	O
the	O
top	O
two	O
layers	O
act	O
as	O
an	O
associative	B
memory	I
,	O
and	O
the	O
remaining	O
layers	O
then	O
generate	O
the	O
output	O
.	O
h2	O
(	O
cid:7	O
)	O
p	O
(	O
h1	O
,	O
h2	O
,	O
v|w1	O
)	O
has	O
the	O
form	O
p	O
(	O
h1	O
,	O
v|w1	O
)	O
=	O
the	O
advantage	O
of	O
this	O
peculiar	O
architecture	O
is	O
that	O
we	O
can	O
infer	O
the	O
hidden	B
states	O
in	O
a	O
fast	O
,	O
bottom-up	O
fashion	O
.	O
to	O
see	O
why	O
,	O
suppose	O
we	O
only	O
have	O
two	O
hidden	B
layers	O
,	O
and	O
that	O
w2	O
=	O
wt	O
1	O
,	O
so	O
the	O
second	O
level	O
weights	O
are	O
tied	B
to	O
the	O
ﬁrst	O
level	O
weights	O
(	O
see	O
figure	O
28.2	O
(	O
a	O
)	O
)	O
.	O
this	O
deﬁnes	O
a	O
model	O
of	O
the	O
form	O
p	O
(	O
h1	O
,	O
h2	O
,	O
v|w1	O
)	O
.	O
one	O
can	O
show	O
that	O
the	O
distribution	O
p	O
(	O
h1	O
,	O
v|w1	O
)	O
=	O
z	O
(	O
w1	O
)	O
exp	O
(	O
vt	O
w1h1	O
)	O
,	O
which	O
is	O
equivalent	O
to	O
an	O
rbm	O
.	O
since	O
the	O
dbn	O
is	O
equivalent	O
to	O
the	O
rbm	O
as	O
far	O
as	O
p	O
(	O
h1	O
,	O
v|w1	O
)	O
is	O
concerned	O
,	O
we	O
can	O
infer	O
the	O
posterior	O
p	O
(	O
h1|v	O
,	O
w1	O
)	O
in	O
the	O
dbn	O
exactly	O
as	O
in	O
the	O
rbm	O
.	O
this	O
posterior	O
is	O
exact	O
,	O
even	O
though	O
it	O
is	O
fully	O
factorized	O
.	O
now	O
the	O
only	O
way	O
to	O
get	O
a	O
factored	O
posterior	O
is	O
if	O
the	O
prior	O
p	O
(	O
h1|w1	O
)	O
is	O
a	O
complementary	B
prior	I
.	O
this	O
is	O
a	O
prior	O
which	O
,	O
when	O
multiplied	O
by	O
the	O
likelihood	B
p	O
(	O
v|h1	O
)	O
,	O
results	O
in	O
a	O
perfectly	O
factored	O
posterior	O
.	O
thus	O
we	O
see	O
that	O
the	O
top	O
level	O
rbm	O
in	O
a	O
dbn	O
acts	O
as	O
a	O
complementary	B
prior	I
for	O
the	O
bottom	O
level	O
directed	O
sigmoidal	O
likelihood	B
function	O
.	O
1	O
if	O
we	O
have	O
multiple	O
hidden	O
levels	O
,	O
and/or	O
if	O
the	O
weights	O
are	O
not	O
tied	O
,	O
the	O
correspondence	B
between	O
the	O
dbn	O
and	O
the	O
rbm	O
does	O
not	O
hold	O
exactly	O
any	O
more	O
,	O
but	O
we	O
can	O
still	O
use	O
the	O
factored	O
inference	B
rule	O
as	O
a	O
form	O
of	O
approximate	O
bottom-up	O
inference	B
.	O
below	O
we	O
show	O
that	O
this	O
is	O
a	O
valid	O
variational	O
lower	O
bound	O
.	O
this	O
bound	O
also	O
suggests	O
a	O
layer-wise	O
training	O
strategy	O
,	O
that	O
we	O
will	O
explain	O
in	O
more	O
detail	O
later	O
.	O
note	O
,	O
however	O
,	O
that	O
top-down	O
inference	B
in	O
a	O
dbn	O
is	O
not	O
tractable	O
,	O
so	O
dbns	O
are	O
usually	O
only	O
used	O
in	O
a	O
feedforward	O
manner	O
.	O
2.	O
unforuntately	O
the	O
acronym	O
“	O
dbn	O
”	O
also	O
stands	O
for	O
“	O
dynamic	O
bayes	O
net	O
”	O
(	O
section	O
17.6.7	O
)	O
.	O
geoff	O
hinton	O
,	O
who	O
invented	O
deep	O
belief	O
networks	O
,	O
has	O
suggested	O
the	O
acronyms	O
deebns	O
and	O
dybns	O
for	O
these	O
two	O
different	O
meanings	O
.	O
however	O
,	O
this	O
terminology	O
is	O
non-standard	O
.	O
998	O
chapter	O
28.	O
deep	B
learning	I
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
28.2	O
(	O
a	O
)	O
a	O
dbn	O
with	O
two	O
hidden	B
layers	O
and	O
tied	B
weights	O
that	O
is	O
equivalent	O
to	O
an	O
rbm	O
.	O
source	O
:	O
figure	O
2.2	O
of	O
(	O
salakhutdinov	O
2009	O
)	O
.	O
(	O
c	O
)	O
the	O
corresponding	O
dbn	O
.	O
source	O
:	O
figure	O
2.3	O
of	O
(	O
salakhutdinov	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
ruslan	O
salakhutdinov	O
.	O
(	O
b	O
)	O
a	O
stack	O
of	O
rbms	O
trained	O
greedily	O
.	O
28.2.4	O
greedy	O
layer-wise	O
learning	B
of	O
dbns	O
the	O
equivalence	O
between	O
dbns	O
and	O
rbms	O
suggests	O
the	O
following	O
strategy	O
for	O
learning	B
a	O
dbn	O
.	O
•	O
fit	O
an	O
rbm	O
to	O
learn	O
w1	O
using	O
methods	O
described	O
in	O
section	O
27.7.2	O
.	O
•	O
unroll	O
the	O
rbm	O
into	O
a	O
dbn	O
with	O
2	O
hidden	B
layers	O
,	O
as	O
in	O
figure	O
28.2	O
(	O
a	O
)	O
.	O
now	O
“	O
freeze	O
”	O
the	O
directed	B
weights	O
w1	O
and	O
let	O
w2	O
be	O
“	O
untied	O
”	O
so	O
it	O
is	O
no	O
longer	O
forced	O
to	O
be	O
equal	O
to	O
wt	O
1	O
.	O
we	O
will	O
now	O
learn	O
a	O
better	O
prior	O
for	O
p	O
(	O
h1|w2	O
)	O
by	O
ﬁtting	O
a	O
second	O
rbm	O
.	O
the	O
input	O
data	O
to	O
this	O
new	O
rbm	O
is	O
the	O
activation	B
of	O
the	O
hidden	B
units	I
e	O
[	O
h1|v	O
,	O
w1	O
]	O
which	O
can	O
be	O
computed	O
using	O
a	O
factorial	O
approximation	O
.	O
•	O
continue	O
to	O
add	O
more	O
hidden	B
layers	O
until	O
some	O
stopping	O
criterion	O
is	O
satisiﬁed	O
,	O
e.g.	O
,	O
you	O
run	O
out	O
of	O
time	O
or	O
memory	O
,	O
or	O
you	O
start	O
to	O
overﬁt	B
the	O
validation	B
set	I
.	O
construct	O
the	O
dbn	O
from	O
these	O
rbms	O
,	O
as	O
illustrated	O
in	O
figure	O
28.2	O
(	O
c	O
)	O
.	O
one	O
can	O
show	O
(	O
hinton	O
et	O
al	O
.	O
2006	O
)	O
that	O
this	O
procedure	O
always	O
increases	O
a	O
lower	O
bound	O
the	O
observed	O
data	O
likelihood	O
.	O
of	O
course	O
this	O
procedure	O
might	O
result	O
in	O
overﬁtting	B
,	O
but	O
that	O
is	O
a	O
different	O
matter	O
.	O
in	O
practice	O
,	O
we	O
want	O
to	O
be	O
able	O
to	O
use	O
any	O
number	O
of	O
hidden	B
units	I
in	O
each	O
level	O
.	O
this	O
means	O
we	O
will	O
not	O
be	O
able	O
to	O
initialize	O
the	O
weights	O
so	O
that	O
w	O
(	O
cid:8	O
)	O
=	O
wt	O
(	O
cid:8	O
)	O
−1	O
.	O
this	O
voids	O
the	O
theoretical	O
guarantee	O
.	O
nevertheless	O
the	O
method	O
works	O
well	O
in	O
practice	O
,	O
as	O
we	O
will	O
see	O
.	O
the	O
method	O
can	O
also	O
be	O
extended	O
to	O
train	O
dbms	O
in	O
a	O
greedy	O
way	O
(	O
salakhutdinov	O
and	O
larochelle	O
2010	O
)	O
.	O
after	O
using	O
the	O
greedy	O
layer-wise	O
training	O
strategy	O
,	O
it	O
is	O
standard	O
to	O
“	O
ﬁne	O
tune	O
”	O
the	O
weights	O
,	O
using	O
a	O
technique	O
called	O
backﬁtting	B
.	O
this	O
works	O
as	O
follows	O
.	O
perform	O
an	O
upwards	O
sampling	O
pass	O
to	O
the	O
top	O
.	O
then	O
perform	O
brief	O
gibbs	O
sampling	O
in	O
the	O
top	O
level	O
rbm	O
,	O
and	O
perform	O
a	O
cd	O
update	O
of	O
the	O
rbm	O
parameters	O
.	O
finally	O
,	O
perform	O
a	O
downwards	O
ancestral	B
sampling	I
pass	O
(	O
which	O
is	O
an	O
approximate	O
sample	O
from	O
the	O
posterior	O
)	O
,	O
and	O
update	O
the	O
logistic	B
cpd	O
parameters	O
using	O
a	O
small	O
gradient	O
step	O
.	O
this	O
is	O
called	O
the	O
up-down	B
procedure	O
(	O
hinton	O
et	O
al	O
.	O
2006	O
)	O
.	O
unfortunately	O
this	O
procedure	O
is	O
very	O
slow	O
.	O
28.3.	O
deep	B
neural	O
networks	O
999	O
28.3	O
deep	B
neural	O
networks	O
given	O
that	O
dbns	O
are	O
often	O
only	O
used	O
in	O
a	O
feed-forward	O
,	O
or	O
bottom-up	O
,	O
mode	B
,	O
they	O
are	O
effectively	O
acting	O
like	O
neural	B
networks	I
.	O
in	O
view	O
of	O
this	O
,	O
it	O
is	O
natural	O
to	O
dispense	O
with	O
the	O
generative	O
story	O
and	O
try	O
to	O
ﬁt	O
deep	B
neural	O
networks	O
directly	O
,	O
as	O
we	O
discuss	O
below	O
.	O
the	O
resulting	O
training	O
methods	O
are	O
often	O
simpler	O
to	O
implement	O
,	O
and	O
can	O
be	O
faster	O
.	O
note	O
,	O
however	O
,	O
that	O
performance	O
with	O
deep	B
neural	O
nets	O
is	O
sometimes	O
not	O
as	O
good	O
as	O
with	O
probabilistic	O
models	O
(	O
bengio	O
et	O
al	O
.	O
2007	O
)	O
.	O
one	O
reason	O
for	O
this	O
is	O
that	O
probabilistic	O
models	O
support	B
top-down	O
inference	B
as	O
well	O
as	O
bottom-up	O
inference	B
.	O
(	O
dbns	O
do	O
not	O
support	B
efficient	O
top-down	O
inference	B
,	O
but	O
dbms	O
do	O
,	O
and	O
this	O
has	O
been	O
shown	O
to	O
help	O
(	O
salakhutdinov	O
and	O
larochelle	O
2010	O
)	O
.	O
)	O
top-down	O
inference	B
is	O
useful	O
when	O
there	O
is	O
a	O
lot	O
of	O
ambiguity	O
about	O
the	O
correct	O
interpretation	O
of	O
the	O
signal	O
.	O
it	O
is	O
interesting	O
to	O
note	O
that	O
in	O
the	O
mammalian	O
visual	O
cortex	O
,	O
there	O
are	O
many	O
more	O
feedback	O
connections	O
than	O
there	O
are	O
feedforward	O
connections	O
(	O
see	O
e.g.	O
,	O
(	O
palmer	O
1999	O
;	O
kandel	O
et	O
al	O
.	O
2000	O
)	O
)	O
.	O
the	O
role	O
of	O
these	O
feedback	O
connections	O
is	O
not	O
precisely	O
understood	O
,	O
but	O
they	O
presumably	O
provide	O
contextual	O
prior	O
information	B
(	O
e.g.	O
,	O
coming	O
from	O
the	O
previous	O
“	O
frame	O
”	O
or	O
retinal	O
glance	O
)	O
which	O
can	O
be	O
used	O
to	O
disambiguate	O
the	O
current	O
bottom-up	O
signals	O
(	O
lee	O
and	O
mumford	O
2003	O
)	O
.	O
of	O
course	O
,	O
we	O
can	O
simulate	O
the	O
effect	O
of	O
top-down	O
inference	B
using	O
a	O
neural	B
network	I
.	O
however	O
the	O
models	O
we	O
discuss	O
below	O
do	O
not	O
do	O
this	O
.	O
28.3.1	O
deep	B
multi-layer	O
perceptrons	O
many	O
decision	B
problems	O
can	O
be	O
reduced	O
to	O
classiﬁcation	B
,	O
e.g.	O
,	O
predict	O
which	O
object	O
(	O
if	O
any	O
)	O
is	O
present	O
in	O
an	O
image	O
patch	O
,	O
or	O
predict	O
which	O
phoneme	O
is	O
present	O
in	O
a	O
given	O
acoustic	O
feature	O
vector	O
.	O
we	O
can	O
solve	O
such	O
problems	O
by	O
creating	O
a	O
deep	B
feedforward	O
neural	B
network	I
or	O
multi-	O
layer	O
perceptron	B
(	O
mlp	O
)	O
,	O
as	O
in	O
section	O
16.5	O
,	O
and	O
then	O
ﬁtting	O
the	O
parameters	O
using	O
gradient	B
descent	I
(	O
aka	O
back-propagation	B
)	O
.	O
unfortunately	O
,	O
this	O
method	O
does	O
not	O
work	O
very	O
well	O
.	O
one	O
problem	O
is	O
that	O
the	O
gradient	O
becomes	O
weaker	O
the	O
further	O
we	O
move	O
away	O
from	O
the	O
data	O
;	O
this	O
is	O
known	O
as	O
the	O
“	O
vanishing	B
gradient	I
”	O
problem	O
(	O
bengio	O
and	O
frasconi	O
1995	O
)	O
.	O
a	O
related	O
problem	O
is	O
that	O
there	O
can	O
be	O
large	O
plateaus	O
in	O
the	O
error	O
surface	O
,	O
which	O
cause	O
simple	O
ﬁrst-order	O
gadient-based	O
methods	O
to	O
get	O
stuck	O
(	O
glorot	O
and	O
bengio	O
2010	O
)	O
.	O
consequently	O
early	O
attempts	O
to	O
learn	O
deep	B
neural	O
networks	O
proved	O
unsuccesful	O
.	O
recently	O
there	O
has	O
been	O
some	O
progress	O
,	O
due	O
to	O
the	O
adoption	O
of	O
gpus	O
(	O
ciresan	O
et	O
al	O
.	O
2010	O
)	O
and	O
second-order	O
optimization	O
algorithms	O
(	O
martens	O
2010	O
)	O
.	O
nevertheless	O
,	O
such	O
models	O
remain	O
difficult	O
to	O
train	O
.	O
below	O
we	O
discuss	O
a	O
way	O
to	O
initialize	O
the	O
parameters	O
using	O
unsupervised	B
learning	I
;	O
this	O
is	O
called	O
generative	B
pre-training	I
.	O
the	O
advantage	O
of	O
performing	O
unsupervised	B
learning	I
ﬁrst	O
is	O
that	O
the	O
model	O
is	O
forced	O
to	O
model	O
a	O
high-dimensional	O
response	O
,	O
namely	O
the	O
input	O
feature	O
vector	O
,	O
rather	O
than	O
just	O
predicting	O
a	O
scalar	O
response	O
.	O
this	O
acts	O
like	O
a	O
data-induced	O
regularizer	O
,	O
and	O
helps	O
backpropagation	B
ﬁnd	O
local	O
minima	O
with	O
good	O
generalization	B
properties	O
(	O
erhan	O
et	O
al	O
.	O
2010	O
;	O
glorot	O
and	O
bengio	O
2010	O
)	O
.	O
1000	O
chapter	O
28.	O
deep	B
learning	I
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
#	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
(	O
cid:1	O
)	O
	O
	O
	O
	O
	O
(	O
cid:1	O
)	O
	O
	O
	O
	O
	O
(	O
cid:1	O
)	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
(	O
cid:1	O
)	O
	O
	O
	O
(	O
cid:1	O
)	O
	O
	O
	O
	O
	O
(	O
cid:1	O
)	O
	O
	O
	O
	O
	O
(	O
cid:1	O
)	O
	O
	O
	O
	O
	O
(	O
cid:1	O
)	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
!	O
	O
	O
	O
	O
	O
	O
	O
	O
(	O
cid:1	O
)	O
!	O
''	O
	O
	O
figure	O
28.3	O
training	O
a	O
deep	B
autoencoder	O
.	O
(	O
a	O
)	O
first	O
we	O
greedily	O
train	O
some	O
rbms	O
.	O
(	O
b	O
)	O
then	O
we	O
construct	O
the	O
auto-encoder	B
by	O
replicating	O
the	O
weights	O
.	O
(	O
c	O
)	O
finally	O
we	O
ﬁne-tune	O
the	O
weights	O
using	O
back-propagation	B
.	O
from	O
figure	O
1	O
of	O
(	O
hinton	O
and	O
salakhutdinov	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
ruslan	O
salakhutdinov	O
.	O
28.3.2	O
deep	B
auto-encoders	I
an	O
auto-encoder	B
is	O
a	O
kind	O
of	O
unsupervised	O
neural	O
network	O
that	O
is	O
used	O
for	O
dimensionality	B
reduction	I
and	O
feature	O
discovery	O
.	O
more	O
precisely	O
,	O
an	O
auto-encoder	B
is	O
a	O
feedforward	B
neural	I
network	I
that	O
is	O
trained	O
to	O
predict	O
the	O
input	O
itself	O
.	O
to	O
prevent	O
the	O
system	O
from	O
learning	B
the	O
trivial	O
identity	O
mapping	O
,	O
the	O
hidden	B
layer	I
in	O
the	O
middle	O
is	O
usually	O
constrained	O
to	O
be	O
a	O
narrow	O
bottleneck	B
.	O
the	O
system	O
can	O
minimize	O
the	O
reconstruction	B
error	I
by	O
ensuring	O
the	O
hidden	B
units	I
capture	O
the	O
most	O
relevant	O
aspects	O
of	O
the	O
data	O
.	O
suppose	O
the	O
system	O
has	O
one	O
hidden	B
layer	I
,	O
so	O
the	O
model	O
has	O
the	O
form	O
v	O
→	O
h	O
→	O
v.	O
further	O
,	O
in	O
this	O
case	O
,	O
one	O
can	O
show	O
that	O
the	O
weights	O
to	O
the	O
k	O
suppose	O
all	O
the	O
functions	O
are	O
linear	O
.	O
hidden	B
units	I
will	O
span	O
the	O
same	O
subspace	O
as	O
the	O
ﬁrst	O
k	O
principal	B
components	I
of	O
the	O
data	O
(	O
karhunen	O
and	O
joutsensalo	O
1995	O
;	O
japkowicz	O
et	O
al	O
.	O
2000	O
)	O
.	O
in	O
other	O
words	O
,	O
linear	O
auto-encoders	O
are	O
equivalent	O
to	O
pca	O
.	O
however	O
,	O
by	O
using	O
nonlinear	O
activation	B
functions	O
,	O
one	O
can	O
discover	O
nonlinear	O
representations	O
of	O
the	O
data	O
.	O
more	O
powerful	O
representations	O
can	O
be	O
learned	O
by	O
using	O
deep	B
auto-encoders	I
.	O
unfortunately	O
training	O
such	O
models	O
using	O
back-propagation	B
does	O
not	O
work	O
well	O
,	O
because	O
the	O
gradient	O
signal	O
becomes	O
too	O
small	O
as	O
it	O
passes	O
back	O
through	O
multiple	O
layers	O
,	O
and	O
the	O
learning	B
algorithm	O
often	O
gets	O
stuck	O
in	O
poor	O
local	O
minima	O
.	O
one	O
solution	O
to	O
this	O
problem	O
is	O
to	O
greedily	O
train	O
a	O
series	O
of	O
rbms	O
and	O
to	O
use	O
these	O
to	O
initialize	O
an	O
auto-encoder	B
,	O
as	O
illustrated	O
in	O
figure	O
28.3.	O
the	O
whole	O
system	O
can	O
then	O
be	O
ﬁne-tuned	O
using	O
backprop	O
in	O
the	O
usual	O
fashion	O
.	O
this	O
approach	O
,	O
ﬁrst	O
suggested	O
in	O
(	O
hinton	O
and	O
salakhutdinov	O
28.4.	O
applications	O
of	O
deep	B
networks	I
1001	O
2000	O
top-level	O
units	O
10	O
label	B
units	O
500	O
units	O
this	O
could	O
be	O
the	O
top	O
level	O
of	O
another	O
sensory	O
pathway	O
500	O
units	O
28	O
x	O
28	O
pixel	O
image	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
28.4	O
used	O
with	O
kind	O
permission	O
of	O
geoff	O
hinton	O
.	O
test	O
cases	O
of	O
mnist	O
.	O
above	O
each	O
image	O
is	O
the	O
estimated	O
label	O
.	O
used	O
with	O
kind	O
permission	O
of	O
geoff	O
hinton	O
.	O
compare	O
to	O
figure	O
16.15	O
.	O
(	O
a	O
)	O
a	O
dbn	O
architecture	O
for	O
classifying	O
mnist	O
digits	O
.	O
source	O
:	O
figure	O
1	O
of	O
(	O
hinton	O
et	O
al	O
.	O
2006	O
)	O
.	O
(	O
b	O
)	O
these	O
are	O
the	O
125	O
errors	O
made	O
by	O
the	O
dbn	O
on	O
the	O
10,000	O
source	O
:	O
figure	O
6	O
of	O
(	O
hinton	O
et	O
al	O
.	O
2006	O
)	O
.	O
2006	O
)	O
,	O
works	O
much	O
better	O
than	O
trying	O
to	O
ﬁt	O
the	O
deep	B
auto-encoder	O
directly	O
starting	O
with	O
random	O
weights	O
.	O
28.3.3	O
stacked	O
denoising	O
auto-encoders	O
a	O
standard	O
way	O
to	O
train	O
an	O
auto-encoder	B
is	O
to	O
ensure	O
that	O
the	O
hidden	B
layer	I
is	O
narrower	O
than	O
the	O
visible	B
layer	O
.	O
this	O
prevents	O
the	O
model	O
from	O
learning	B
the	O
identity	O
function	O
.	O
but	O
there	O
are	O
other	O
ways	O
to	O
prevent	O
this	O
trivial	O
solution	O
,	O
which	O
allow	O
for	O
the	O
use	O
of	O
an	O
over-complete	B
representation	O
.	O
one	O
approach	O
is	O
to	O
impose	O
sparsity	B
constraints	O
on	O
the	O
activation	B
of	O
the	O
hidden	B
units	I
(	O
ranzato	O
et	O
al	O
.	O
2006	O
)	O
.	O
another	O
approach	O
is	O
to	O
add	O
noise	O
to	O
the	O
inputs	O
;	O
this	O
is	O
called	O
a	O
denoising	O
auto-	O
encoder	O
(	O
vincent	O
et	O
al	O
.	O
2010	O
)	O
.	O
for	O
example	O
,	O
we	O
can	O
corrupt	O
some	O
of	O
the	O
inputs	O
,	O
for	O
example	O
by	O
setting	O
them	O
to	O
zero	O
,	O
so	O
the	O
model	O
has	O
to	O
learn	O
to	O
predict	O
the	O
missing	B
entries	O
.	O
this	O
can	O
be	O
shown	O
to	O
be	O
equivalent	O
to	O
a	O
certain	O
approximate	O
form	O
of	O
maximum	O
likelihood	O
training	O
(	O
known	O
as	O
score	B
matching	I
)	O
applied	O
to	O
an	O
rbm	O
(	O
vincent	O
2011	O
)	O
.	O
of	O
course	O
,	O
we	O
can	O
stack	O
these	O
models	O
on	O
top	O
of	O
each	O
other	O
to	O
learn	O
a	O
deep	B
stacked	O
denoising	B
auto-encoder	I
,	O
which	O
can	O
be	O
discriminatively	O
ﬁne-tuned	O
just	O
like	O
a	O
feedforward	B
neural	I
network	I
,	O
if	O
desired	O
.	O
28.4	O
applications	O
of	O
deep	B
networks	I
in	O
this	O
section	O
,	O
we	O
mention	O
a	O
few	O
applications	O
of	O
the	O
models	O
we	O
have	O
been	O
discussing	O
.	O
28.4.1	O
handwritten	O
digit	O
classiﬁcation	B
using	O
dbns	O
figure	O
28.4	O
(	O
a	O
)	O
shows	O
a	O
dbn	O
(	O
from	O
(	O
hinton	O
et	O
al	O
.	O
2006	O
)	O
)	O
consisting	O
of	O
3	O
hidden	B
layers	O
.	O
the	O
visible	B
layer	O
corresponds	O
to	O
binary	O
images	O
of	O
handwritten	O
digits	O
from	O
the	O
mnist	O
data	O
set	O
.	O
in	O
addition	O
,	O
the	O
top	O
rbm	O
is	O
connected	O
to	O
a	O
softmax	B
layer	O
with	O
10	O
units	O
,	O
representing	O
the	O
class	O
label	O
.	O
1002	O
chapter	O
28.	O
deep	B
learning	I
interbank	O
markets	O
european	O
community	O
monetary/economic	O
energy	O
markets	O
leading	O
economic	O
indicators	O
disasters	O
and	O
accidents	O
legal/judicial	O
(	O
a	O
)	O
accounts/	O
earnings	O
government	O
borrowings	O
(	O
b	O
)	O
figure	O
28.5	O
2d	O
visualization	O
of	O
some	O
bag	B
of	I
words	I
data	O
from	O
the	O
reuters	O
rcv1-v2	O
corpus	B
.	O
(	O
a	O
)	O
results	O
of	O
using	O
lsa	O
.	O
(	O
b	O
)	O
results	O
of	O
using	O
a	O
deep	B
auto-encoder	O
.	O
source	O
:	O
figure	O
4	O
of	O
(	O
hinton	O
and	O
salakhutdinov	O
2006	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
ruslan	O
salakhutdinov	O
.	O
the	O
ﬁrst	O
2	O
hidden	B
layers	O
were	O
trained	O
in	O
a	O
greedy	O
unsupervised	O
fashion	O
from	O
50,000	O
mnist	O
digits	O
,	O
using	O
30	O
epochs	O
(	O
passes	O
over	O
the	O
data	O
)	O
and	O
stochastic	B
gradient	I
descent	I
,	O
with	O
the	O
cd	O
heuristic	O
.	O
this	O
process	O
took	O
“	O
a	O
few	O
hours	O
per	O
layer	O
”	O
(	O
hinton	O
et	O
al	O
.	O
2006	O
,	O
p1540	O
)	O
.	O
then	O
the	O
top	O
layer	O
was	O
trained	O
using	O
as	O
input	O
the	O
activations	O
of	O
the	O
lower	O
hidden	B
layer	I
,	O
as	O
well	O
as	O
the	O
class	O
labels	O
.	O
the	O
corresponding	O
generative	O
model	O
had	O
a	O
test	O
error	O
of	O
about	O
2.5	O
%	O
.	O
the	O
network	O
weights	O
were	O
then	O
carefully	O
ﬁne-tuned	O
on	O
all	O
60,000	O
training	O
images	O
using	O
the	O
up-down	B
procedure	O
.	O
this	O
process	O
took	O
“	O
about	O
a	O
week	O
”	O
(	O
hinton	O
et	O
al	O
.	O
2006	O
,	O
p1540	O
)	O
.	O
the	O
model	O
can	O
be	O
used	O
to	O
classify	O
by	O
performing	O
a	O
deterministic	O
bottom-up	O
pass	O
,	O
and	O
then	O
computing	O
the	O
free	B
energy	I
for	O
the	O
top-level	O
rbm	O
for	O
each	O
possible	O
class	O
label	O
.	O
the	O
ﬁnal	O
error	O
on	O
the	O
test	O
set	O
was	O
about	O
1.25	O
%	O
.	O
the	O
misclassiﬁed	O
examples	O
are	O
shown	O
in	O
figure	O
28.4	O
(	O
b	O
)	O
.	O
this	O
was	O
the	O
best	O
error	O
rate	O
of	O
any	O
method	O
on	O
the	O
permutation-invariant	O
version	O
of	O
mnist	O
at	O
that	O
time	O
.	O
(	O
by	O
permutation-invariant	O
,	O
we	O
mean	B
a	O
method	O
that	O
does	O
not	O
exploit	O
the	O
fact	O
that	O
the	O
input	O
is	O
an	O
image	O
.	O
generic	O
methods	O
work	O
just	O
as	O
well	O
on	O
permuted	O
versions	O
of	O
the	O
input	O
(	O
see	O
figure	O
1.5	O
)	O
,	O
and	O
can	O
therefore	O
be	O
applied	O
to	O
other	O
kinds	O
of	O
datasets	O
.	O
)	O
the	O
only	O
other	O
method	O
that	O
comes	O
close	O
is	O
an	O
svm	O
with	O
a	O
degree	B
9	O
polynomial	B
kernel	I
,	O
which	O
has	O
achieved	O
an	O
error	O
rate	O
of	O
1.4	O
%	O
(	O
decoste	O
and	O
schoelkopf	O
2002	O
)	O
.	O
by	O
way	O
of	O
comparison	O
,	O
1-nearest	O
neighbor	O
(	O
using	O
all	O
60,000	O
examples	O
)	O
achieves	O
3.1	O
%	O
(	O
see	O
mnist1nndemo	O
)	O
.	O
this	O
is	O
not	O
as	O
good	O
,	O
although	O
1-nn	O
is	O
much	O
simpler.3	O
28.4.2	O
data	O
visualization	O
and	O
feature	O
discovery	O
using	O
deep	B
auto-encoders	I
deep	O
autoencoders	O
can	O
learn	O
informative	O
features	B
from	O
raw	O
data	O
.	O
such	O
features	B
are	O
often	O
used	O
as	O
input	O
to	O
standard	O
supervised	O
learning	B
methods	O
.	O
to	O
illustrate	O
this	O
,	O
consider	O
ﬁtting	O
a	O
deep	B
auto-encoder	O
with	O
a	O
2d	O
hidden	B
bottleneck	O
to	O
some	O
3.	O
one	O
can	O
get	O
much	O
improved	O
performance	O
on	O
this	O
task	O
by	O
exploiting	O
the	O
fact	O
that	O
the	O
input	O
is	O
an	O
image	O
.	O
one	O
way	O
to	O
do	O
this	O
is	O
to	O
create	O
distorted	B
versions	O
of	O
the	O
input	O
,	O
adding	O
small	O
shifts	O
and	O
translations	O
(	O
see	O
figure	O
16.13	O
for	O
some	O
examples	O
)	O
.	O
applying	O
this	O
trick	O
reduced	O
the	O
svm	O
error	O
rate	O
to	O
0.56	O
%	O
.	O
similar	B
error	O
rates	O
can	O
be	O
achieved	O
using	O
convolutional	O
neural	O
networks	O
(	O
section	O
16.5.1	O
)	O
trained	O
on	O
distorted	B
images	O
(	O
(	O
simard	O
et	O
al	O
.	O
2003	O
)	O
got	O
0.4	O
%	O
)	O
.	O
however	O
,	O
the	O
point	O
of	O
dbns	O
is	O
that	O
they	O
offer	O
a	O
way	O
to	O
learn	O
such	O
prior	O
knowledge	O
,	O
without	O
it	O
having	O
to	O
be	O
hand-crafted	O
.	O
28.4.	O
applications	O
of	O
deep	B
networks	I
1003	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
''	O
!	O
	O
	O
	O
	O
	O
	O
	O
	O
''	O
!	O
	O
	O
	O
	O
	O
	O
!	O
	O
	O
(	O
cid:1	O
)	O
!	O
''	O
	O
	O
	O
	O
	O
	O
	O
figure	O
28.6	O
precision-recall	O
curves	O
for	O
document	O
retrieval	O
in	O
the	O
reuters	O
rcv1-v2	O
corpus	B
.	O
source	O
:	O
figure	O
3.9	O
of	O
(	O
salakhutdinov	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
ruslan	O
salakhutdinov	O
.	O
text	O
data	O
.	O
the	O
results	O
are	O
shown	O
in	O
figure	O
28.5.	O
on	O
the	O
left	O
we	O
show	O
the	O
2d	O
embedding	B
produced	O
by	O
lsa	O
(	O
section	O
27.2.2	O
)	O
,	O
and	O
on	O
the	O
right	O
,	O
the	O
2d	O
embedding	B
produced	O
by	O
the	O
auto-encoder	B
.	O
it	O
is	O
clear	O
that	O
the	O
low-dimensional	O
representation	O
created	O
by	O
the	O
auto-encoder	B
has	O
captured	O
a	O
lot	O
of	O
the	O
meaning	O
of	O
the	O
documents	O
,	O
even	O
though	O
class	O
labels	O
were	O
not	O
used.4	O
note	O
that	O
various	O
other	O
ways	O
of	O
learning	B
low-dimensional	O
continuous	O
embeddings	O
of	O
words	O
have	O
been	O
proposed	O
.	O
see	O
e.g.	O
,	O
(	O
turian	O
et	O
al	O
.	O
2010	O
)	O
for	O
details	O
.	O
28.4.3	O
information	B
retrieval	I
using	O
deep	B
auto-encoders	I
(	O
semantic	B
hashing	I
)	O
in	O
view	O
of	O
the	O
sucess	O
of	O
rbms	O
for	O
information	B
retrieval	I
discussed	O
in	O
section	O
27.7.3.1	O
,	O
it	O
is	O
natural	O
to	O
wonder	O
if	O
deep	B
models	O
can	O
do	O
even	O
better	O
.	O
in	O
fact	O
they	O
can	O
,	O
as	O
is	O
shown	O
in	O
figure	O
28.6.	O
more	O
interestingly	O
,	O
we	O
can	O
use	O
a	O
binary	O
low-dimensional	O
representation	O
in	O
the	O
middle	O
layer	O
of	O
the	O
deep	B
auto-encoder	O
,	O
rather	O
than	O
a	O
continuous	O
representation	O
as	O
we	O
used	O
above	O
.	O
this	O
enables	O
very	O
fast	O
retrieval	O
of	O
related	O
documents	O
.	O
for	O
example	O
,	O
if	O
we	O
use	O
a	O
20-bit	O
code	O
,	O
we	O
can	O
precompute	O
the	O
binary	O
representation	O
for	O
all	O
the	O
documents	O
,	O
and	O
then	O
create	O
a	O
hash-table	O
mapping	O
codewords	O
to	O
documents	O
.	O
this	O
approach	O
is	O
known	O
as	O
semantic	B
hashing	I
,	O
since	O
the	O
binary	O
representation	O
of	O
semantically	O
similar	B
documents	O
will	O
be	O
close	O
in	O
hamming	B
distance	I
.	O
for	O
the	O
402,207	O
test	O
documents	O
in	O
reuters	O
rcv1-v2	O
,	O
this	O
results	O
in	O
about	O
0.4	O
documents	O
per	O
entry	O
in	O
the	O
table	O
.	O
at	O
test	O
time	O
,	O
we	O
compute	O
the	O
codeword	O
for	O
the	O
query	O
,	O
and	O
then	O
simply	O
retrieve	O
the	O
relevant	O
documents	O
in	O
constant	O
time	O
by	O
looking	O
up	O
the	O
contents	O
of	O
the	O
relevant	O
address	O
in	O
memory	O
.	O
to	O
ﬁnd	O
other	O
other	O
related	O
documents	O
,	O
we	O
can	O
compute	O
all	O
the	O
codewords	O
within	O
a	O
4.	O
some	O
details	O
.	O
salakhutdinov	O
and	O
hinton	O
used	O
the	O
reuters	O
rcv1-v2	O
data	O
set	O
,	O
which	O
consists	O
of	O
804,414	O
newswire	O
articles	O
,	O
manually	O
classiﬁed	O
into	O
103	O
topics	O
.	O
they	O
represent	O
each	O
document	O
by	O
counting	O
how	O
many	O
times	O
each	O
of	O
the	O
top	O
2000	O
most	O
frequent	O
words	O
occurs	O
.	O
they	O
trained	O
a	O
deep	B
auto-encoder	O
with	O
2000-500-250-125-2	O
layers	O
on	O
half	O
of	O
the	O
data	O
.	O
the	O
2000	O
visible	B
units	O
use	O
a	O
replicated	O
softmax	O
distribution	O
,	O
the	O
2	O
hidden	B
units	I
in	O
the	O
middle	O
layer	O
have	O
a	O
gaussian	O
distribution	O
,	O
and	O
the	O
remaining	O
units	O
have	O
the	O
usual	O
bernoulli-logistic	O
distribution	O
.	O
when	O
ﬁne	O
tuning	O
the	O
auto-encoder	B
,	O
a	O
cross-entropy	B
loss	O
function	O
(	O
equivalent	O
to	O
maximum	O
likelihood	O
under	O
a	O
multinoulli	B
distribution	I
)	O
was	O
used	O
.	O
see	O
(	O
hinton	O
and	O
salakhutdinov	O
2006	O
)	O
for	O
further	O
details	O
.	O
1004	O
chapter	O
28.	O
deep	B
learning	I
h1	O
1	O
h1	O
2	O
w1	O
h1	O
3	O
w1	O
w1	O
h2	O
1	O
w2	O
h2	O
2	O
w2	O
x1	O
x2	O
x3	O
h2	O
3	O
w2	O
x4	O
figure	O
28.7	O
a	O
small	O
1d	O
convolutional	O
rbm	O
with	O
two	O
groups	O
of	O
hidden	B
units	I
,	O
each	O
associated	O
with	O
a	O
ﬁlter	O
of	O
size	O
2.	O
h1	O
1	O
are	O
two	O
different	O
“	O
views	B
”	O
of	O
the	O
data	O
in	O
the	O
ﬁrst	O
window	O
,	O
(	O
x1	O
,	O
x2	O
)	O
.	O
the	O
ﬁrst	O
view	O
is	O
computed	O
using	O
the	O
ﬁlter	O
w1	O
,	O
the	O
second	O
view	O
using	O
ﬁlter	O
w2	O
.	O
similarly	O
,	O
h1	O
2	O
are	O
the	O
views	B
of	O
the	O
data	O
in	O
the	O
second	O
window	O
,	O
(	O
x2	O
,	O
x3	O
)	O
,	O
computed	O
using	O
w1	O
and	O
w2	O
respectively	O
.	O
1	O
and	O
h2	O
2	O
and	O
h2	O
hamming	B
distance	I
of	O
,	O
say	O
,	O
4.	O
this	O
results	O
in	O
retrieving	O
about	O
6196	O
×	O
0.4	O
≈	O
2500	O
documents5	O
.	O
the	O
key	O
point	O
is	O
that	O
the	O
total	O
time	O
is	O
independent	O
of	O
the	O
size	O
of	O
the	O
corpus	B
.	O
of	O
course	O
,	O
there	O
are	O
other	O
techniques	O
for	O
fast	O
document	O
retrieval	O
,	O
such	O
as	O
inverted	B
indices	I
.	O
these	O
rely	O
on	O
the	O
fact	O
that	O
individual	O
words	O
are	O
quite	O
informative	O
,	O
so	O
we	O
can	O
simply	O
intersect	O
all	O
the	O
documents	O
that	O
contain	O
each	O
word	O
.	O
however	O
,	O
when	O
performing	O
image	O
retrieval	O
,	O
it	O
is	O
clear	O
that	O
we	O
do	O
not	O
want	O
to	O
work	O
at	O
the	O
pixel	O
level	O
.	O
recently	O
(	O
krizhevsky	O
and	O
hinton	O
2010	O
)	O
showed	O
that	O
a	O
deep	B
autoencoder	O
could	O
learn	O
a	O
good	O
semantic	B
hashing	I
function	O
that	O
outperformed	O
previous	O
techniques	O
(	O
torralba	O
et	O
al	O
.	O
2008	O
;	O
weiss	O
et	O
al	O
.	O
2008	O
)	O
on	O
the	O
80	O
million	O
tiny	O
images	O
dataset	O
.	O
it	O
is	O
hard	O
to	O
apply	O
inverted	O
indexing	O
techniques	O
to	O
real-valued	O
data	O
(	O
although	O
one	O
could	O
imagine	O
vector	O
quantizing	O
image	O
patches	O
)	O
.	O
28.4.4	O
learning	B
audio	O
features	B
using	O
1d	O
convolutional	O
dbns	O
to	O
apply	O
dbns	O
to	O
time	O
series	O
of	O
unbounded	O
length	O
,	O
it	O
is	O
necessary	O
to	O
use	O
some	O
form	O
of	O
parameter	B
tying	I
.	O
one	O
way	O
to	O
do	O
this	O
is	O
to	O
use	O
convolutional	O
dbns	O
(	O
lee	O
et	O
al	O
.	O
2009	O
;	O
desjardins	O
and	O
bengio	O
2008	O
)	O
,	O
which	O
use	O
convolutional	O
rbms	O
as	O
their	O
basic	O
unit	O
.	O
these	O
models	O
are	O
a	O
generative	O
version	O
of	O
convolutional	B
neural	I
nets	I
discussed	O
in	O
section	O
16.5.1.	O
the	O
basic	O
idea	O
is	O
illustrated	O
in	O
figure	O
28.7.	O
the	O
hidden	B
activation	O
vector	O
for	O
each	O
group	O
is	O
computed	O
by	O
convolving	O
the	O
input	O
vector	O
with	O
that	O
group	O
’	O
s	O
ﬁlter	O
(	O
weight	B
vector	I
or	O
matrix	O
)	O
.	O
in	O
other	O
words	O
,	O
each	O
node	O
within	O
a	O
hidden	B
group	O
is	O
a	O
weighted	O
combination	O
of	O
a	O
subset	O
of	O
the	O
inputs	O
.	O
we	O
compute	O
the	O
activaton	O
of	O
all	O
the	O
hidden	B
nodes	I
by	O
“	O
sliding	O
”	O
this	O
weight	B
vector	I
over	O
the	O
input	O
.	O
this	O
allows	O
us	O
to	O
model	O
translation	O
invariance	O
,	O
since	O
we	O
use	O
the	O
same	O
weights	O
no	O
matter	O
where	O
in	O
the	O
input	O
vector	O
the	O
pattern	B
occurs.6	O
each	O
group	O
has	O
its	O
own	O
ﬁlter	O
,	O
corresponding	O
to	O
its	O
own	O
pattern	B
detector	O
.	O
k	O
k=0	O
is	O
the	O
number	O
of	O
bit	O
vectors	O
that	O
are	O
up	O
to	O
a	O
hamming	B
distance	I
of	O
4	O
away	O
.	O
5.	O
note	O
that	O
6196	O
=	O
6.	O
it	O
is	O
often	O
said	O
that	O
the	O
goal	O
of	O
deep	B
learnng	O
is	O
to	O
discover	O
invariant	B
features	I
,	O
e.g.	O
,	O
a	O
representation	O
of	O
an	O
object	O
that	O
does	O
not	O
change	O
even	O
as	O
nuisance	B
variables	I
,	O
such	O
as	O
the	O
lighting	O
,	O
do	O
change	O
.	O
however	O
,	O
sometimes	O
these	O
so-called	O
“	O
nuisance	B
variables	I
”	O
may	O
be	O
the	O
variables	O
of	O
interest	O
.	O
for	O
example	O
if	O
the	O
task	O
is	O
to	O
determine	O
if	O
a	O
photograph	O
was	O
taken	O
in	O
the	O
morning	O
or	O
the	O
evening	O
,	O
then	O
lighting	O
is	O
one	O
of	O
the	O
more	O
salient	O
features	B
,	O
and	O
object	O
identity	O
may	O
be	O
less	O
relevant	O
.	O
as	O
always	O
,	O
one	O
task	O
’	O
s	O
“	O
signal	O
”	O
is	O
another	O
task	O
’	O
s	O
“	O
noise	O
”	O
,	O
so	O
it	O
unwise	O
to	O
“	O
throw	O
away	O
”	O
apparently	O
irrelevant	O
information	B
(	O
cid:2	O
)	O
4	O
(	O
cid:12	O
)	O
20	O
(	O
cid:13	O
)	O
28.5.	O
discussion	O
1005	O
more	O
formally	O
,	O
for	O
binary	O
1d	O
signals	O
,	O
we	O
can	O
deﬁne	O
the	O
full	B
conditionals	O
in	O
a	O
convolutional	O
rbm	O
as	O
follows	O
(	O
lee	O
et	O
al	O
.	O
2009	O
)	O
:	O
t	O
=	O
1|v	O
)	O
=	O
sigm	O
(	O
(	O
wk	O
⊗	O
v	O
)	O
t	O
+	O
bt	O
)	O
p	O
(	O
hk	O
p	O
(	O
vs	O
=	O
1|h	O
)	O
=	O
sigm	O
(	O
(	O
cid:4	O
)	O
(	O
wk	O
⊗	O
hk	O
)	O
s	O
+	O
cs	O
)	O
(	O
28.6	O
)	O
(	O
28.7	O
)	O
k	O
where	O
wk	O
is	O
the	O
weight	B
vector	I
for	O
group	O
k	O
,	O
bt	O
and	O
cs	O
are	O
bias	B
terms	O
,	O
and	O
a	O
⊗	O
b	O
represents	O
the	O
convolution	O
of	O
vectors	O
a	O
and	O
b.	O
it	O
is	O
common	O
to	O
add	O
a	O
max	B
pooling	I
layer	O
as	O
well	O
as	O
a	O
convolutional	O
layer	O
,	O
which	O
computes	O
a	O
local	O
maximum	O
over	O
the	O
ﬁltered	O
response	O
.	O
this	O
allows	O
for	O
a	O
small	O
amount	O
of	O
translation	B
invariance	I
.	O
it	O
also	O
reduces	O
the	O
size	O
of	O
the	O
higher	O
levels	O
,	O
which	O
speeds	O
up	O
computation	O
consider-	O
ably	O
.	O
deﬁning	O
this	O
for	O
a	O
neural	B
network	I
is	O
simple	O
,	O
but	O
deﬁning	O
this	O
in	O
a	O
way	O
which	O
allows	O
for	O
information	B
ﬂow	O
backwards	O
as	O
well	O
as	O
forwards	O
is	O
a	O
bit	O
more	O
involved	O
.	O
the	O
basic	O
idea	O
is	O
similar	B
to	O
a	O
noisy-or	O
cpd	O
(	O
section	O
10.2.3	O
)	O
,	O
where	O
we	O
deﬁne	O
a	O
probabilistic	O
relationship	O
between	O
the	O
max	O
node	O
and	O
the	O
parts	O
it	O
is	O
maxing	O
over	O
.	O
see	O
(	O
lee	O
et	O
al	O
.	O
2009	O
)	O
for	O
details	O
.	O
note	O
,	O
however	O
,	O
that	O
the	O
top-down	O
generative	O
process	O
will	O
be	O
difficult	O
,	O
since	O
the	O
max	B
pooling	I
operation	O
throws	O
away	O
so	O
much	O
information	B
.	O
(	O
lee	O
et	O
al	O
.	O
2009	O
)	O
applies	O
1d	O
convolutional	O
dbns	O
of	O
depth	O
2	O
to	O
auditory	O
data	O
.	O
when	O
the	O
input	O
consists	O
of	O
speech	O
signals	O
,	O
the	O
method	O
recovers	O
a	O
representation	O
that	O
is	O
similar	B
to	O
phonemes	B
.	O
when	O
applied	O
to	O
music	O
classiﬁcation	B
and	O
speaker	O
identiﬁcation	O
,	O
their	O
method	O
outperforms	O
tech-	O
niques	O
using	O
standard	O
features	O
such	O
as	O
mfcc	O
.	O
(	O
all	O
features	O
were	O
fed	O
into	O
the	O
same	O
discriminative	B
classiﬁer	I
.	O
)	O
in	O
(	O
seide	O
et	O
al	O
.	O
2011	O
)	O
,	O
a	O
deep	B
neural	O
net	O
was	O
used	O
in	O
place	O
of	O
a	O
gmm	O
inside	O
a	O
conventional	O
hmm	O
.	O
the	O
use	O
of	O
dnns	O
signiﬁcantly	O
improved	O
performance	O
on	O
conversational	O
speech	O
recogni-	O
tion	O
.	O
in	O
an	O
interview	O
,	O
the	O
tech	O
lead	O
of	O
this	O
project	O
said	O
“	O
historically	O
,	O
there	O
have	O
been	O
very	O
few	O
individual	O
technologies	O
in	O
speech	B
recognition	I
that	O
have	O
led	O
to	O
improvements	O
of	O
this	O
magnitude	O
”	O
.7	O
28.4.5	O
learning	B
image	O
features	B
using	O
2d	O
convolutional	O
dbns	O
we	O
can	O
extend	O
a	O
convolutional	O
dbn	O
from	O
1d	O
to	O
2d	O
in	O
a	O
straightforward	O
way	O
(	O
lee	O
et	O
al	O
.	O
2009	O
)	O
,	O
as	O
illustrated	O
in	O
figure	O
28.8.	O
the	O
results	O
of	O
a	O
3	O
layer	O
system	O
trained	O
on	O
four	O
classes	O
of	O
visual	O
objects	O
(	O
cars	O
,	O
motorbikes	O
,	O
faces	O
and	O
airplanes	O
)	O
from	O
the	O
caltech	O
101	O
dataset	O
are	O
shown	O
in	O
figure	O
28.9.	O
we	O
only	O
show	O
the	O
results	O
for	O
layers	O
2	O
and	O
3	O
,	O
because	O
layer	O
1	O
learns	O
gabor-like	O
ﬁlters	O
that	O
are	O
very	O
similar	B
to	O
those	O
learned	O
by	O
sparse	B
coding	I
,	O
shown	O
in	O
figure	O
13.21	O
(	O
b	O
)	O
.	O
we	O
see	O
that	O
layer	O
2	O
has	O
learned	O
some	O
generic	O
visual	O
parts	O
that	O
are	O
shared	B
amongst	O
object	O
classes	O
,	O
and	O
layer	O
3	O
seems	O
to	O
have	O
learned	O
ﬁlters	O
that	O
look	O
like	O
grandmother	B
cells	I
,	O
that	O
are	O
speciﬁc	O
to	O
individual	O
object	O
classes	O
,	O
and	O
in	O
some	O
cases	O
,	O
to	O
individual	O
objects	O
.	O
28.5	O
discussion	O
so	O
far	O
,	O
we	O
have	O
been	O
discussing	O
models	O
inspired	O
by	O
low-level	O
processing	O
in	O
the	O
brain	O
.	O
these	O
models	O
have	O
produced	O
useful	O
features	B
for	O
simple	O
classiﬁcation	O
tasks	O
.	O
but	O
can	O
this	O
pure	B
bottom-up	O
too	O
early	O
.	O
7.	O
source	O
:	O
http	O
:	O
//research.microsoft.com/en-us/news/features/speechrecognition-082911.aspx	O
.	O
1006	O
chapter	O
28.	O
deep	B
learning	I
figure	O
28.8	O
a	O
2d	O
convolutional	O
rbm	O
with	O
max-pooling	O
layers	O
.	O
the	O
input	O
signal	O
is	O
a	O
stack	O
of	O
2d	O
images	O
(	O
e.g.	O
,	O
color	O
planes	O
)	O
.	O
each	O
input	O
layer	O
is	O
passed	O
through	O
a	O
different	O
set	O
of	O
ﬁlters	O
.	O
each	O
hidden	B
unit	O
is	O
obtained	O
by	O
convolving	O
with	O
the	O
appropriate	O
ﬁlter	O
,	O
and	O
then	O
summing	O
over	O
the	O
input	O
planes	O
.	O
the	O
ﬁnal	O
layer	O
is	O
obtained	O
by	O
computing	O
the	O
local	O
maximum	O
within	O
a	O
small	O
window	O
.	O
source	O
:	O
figure	O
1	O
of	O
(	O
chen	O
et	O
al	O
.	O
2010	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
bo	O
chen	O
.	O
faces	O
,	O
cars	O
,	O
airplanes	O
,	O
motorbikes	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
28.9	O
visualization	O
of	O
the	O
ﬁlters	O
learned	O
by	O
a	O
convolutional	O
dbn	O
in	O
layers	O
two	O
and	O
three	O
.	O
source	O
:	O
figure	O
3	O
of	O
(	O
lee	O
et	O
al	O
.	O
2009	O
)	O
.	O
used	O
with	O
kind	O
permission	O
of	O
honglak	O
lee	O
.	O
approach	O
scale	O
to	O
more	O
challenging	O
problems	O
,	O
such	O
as	O
scene	O
interpretation	O
or	O
natural	O
language	O
understanding	O
?	O
to	O
put	O
the	O
problem	O
in	O
perspective	O
,	O
consider	O
the	O
dbn	O
for	O
handwritten	O
digit	O
classiﬁcation	B
in	O
figure	O
28.4	O
(	O
a	O
)	O
.	O
this	O
has	O
about	O
1.6m	O
free	O
parameters	O
(	O
28×	O
28×	O
500	O
+	O
500×	O
500	O
+	O
510×	O
2000	O
=	O
1	O
,	O
662	O
,	O
000	O
)	O
.	O
although	O
this	O
is	O
a	O
lot	O
,	O
it	O
is	O
tiny	O
compared	O
to	O
the	O
number	O
of	O
neurons	O
in	O
the	O
brain	O
.	O
as	O
hinton	O
says	O
,	O
this	O
is	O
about	O
as	O
many	O
parameters	O
as	O
0.002	O
cubic	O
millimetres	O
of	O
mouse	O
cortex	O
,	O
and	O
several	O
hundred	O
networks	O
of	O
this	O
complexity	O
could	O
ﬁt	O
within	O
a	O
single	O
voxel	O
of	O
a	O
high-resolution	O
fmri	O
scan	O
.	O
this	O
suggests	O
that	O
much	O
bigger	O
networks	O
may	O
be	O
required	O
to	O
compete	O
with	O
human	O
shape	O
recognition	O
abilities	O
.	O
—	O
(	O
hinton	O
et	O
al	O
.	O
2006	O
,	O
p1547	O
)	O
.	O
to	O
scale	O
up	O
to	O
more	O
challenging	O
problems	O
,	O
various	O
groups	O
are	O
using	O
gpus	O
(	O
see	O
e.g.	O
,	O
(	O
raina	O
et	O
al	O
.	O
2009	O
)	O
)	O
and/or	O
parallel	O
computing	O
.	O
but	O
perhaps	O
a	O
more	O
efficient	O
approach	O
is	O
to	O
work	O
at	O
a	O
higher	O
level	O
of	O
abstraction	O
,	O
where	O
inference	B
is	O
done	O
in	O
the	O
space	O
of	O
objects	O
or	O
their	O
parts	O
,	O
rather	O
28.5.	O
discussion	O
1007	O
than	O
in	O
the	O
space	O
of	O
bits	B
and	O
pixels	O
.	O
that	O
is	O
,	O
we	O
want	O
to	O
bridge	O
the	O
signal-to-symbol	B
divide	O
,	O
where	O
by	O
“	O
symbol	O
”	O
we	O
mean	B
something	O
atomic	O
,	O
that	O
can	O
be	O
combined	O
with	O
other	O
symbols	O
in	O
a	O
compositional	O
way	O
.	O
the	O
question	O
of	O
how	O
to	O
convert	O
low	O
level	O
signals	O
into	O
a	O
more	O
structured/	O
“	O
semantic	O
”	O
represen-	O
tation	O
is	O
known	O
as	O
the	O
symbol	B
grounding	I
problem	O
(	O
harnard	O
1990	O
)	O
.	O
traditionally	O
such	O
symbols	O
are	O
associated	O
with	O
words	O
in	O
natural	O
language	O
,	O
but	O
it	O
seems	O
unlikely	O
we	O
can	O
jump	O
directly	O
from	O
low-level	O
signals	O
to	O
high-level	O
semantic	O
concepts	O
.	O
instead	O
,	O
what	O
we	O
need	O
is	O
an	O
intermediate	O
level	O
of	O
symbolic	O
or	O
atomic	O
parts	O
.	O
a	O
very	O
simple	O
way	O
to	O
create	O
such	O
parts	O
from	O
real-valued	O
signals	O
,	O
such	O
as	O
images	O
,	O
is	O
to	O
apply	O
vector	B
quantization	I
.	O
this	O
generates	O
a	O
set	O
of	O
visual	B
words	I
.	O
these	O
can	O
then	O
be	O
modelled	O
using	O
some	O
of	O
the	O
techniques	O
from	O
chapter	O
27	O
for	O
modeling	O
bags	O
of	O
words	O
.	O
such	O
models	O
,	O
however	O
,	O
are	O
still	O
quite	O
“	O
shallow	O
”	O
.	O
it	O
is	O
possible	O
to	O
deﬁne	O
,	O
and	O
learn	O
,	O
deep	B
models	O
which	O
use	O
discrete	B
latent	O
parts	O
.	O
here	O
we	O
just	O
mention	O
a	O
few	O
recent	O
approaches	O
,	O
to	O
give	O
a	O
ﬂavor	O
of	O
the	O
possibilites	O
.	O
(	O
salakhutdinov	O
et	O
al	O
.	O
2011	O
)	O
combine	O
rbms	O
with	O
hierarchical	O
latent	O
dirichlet	O
allocation	O
methods	O
,	O
trained	O
in	O
an	O
unsupervised	O
way	O
.	O
(	O
zhu	O
et	O
al	O
.	O
2010	O
)	O
use	O
latent	B
and-or	O
graphs	O
,	O
trained	O
in	O
a	O
manner	O
similar	B
to	O
a	O
latent	B
structural	O
svm	O
.	O
a	O
similar	B
approach	O
,	O
based	O
on	O
grammars	B
,	O
is	O
described	O
in	O
(	O
girshick	O
et	O
al	O
.	O
2011	O
)	O
.	O
what	O
is	O
interesting	O
about	O
these	O
techniques	O
is	O
that	O
they	O
apply	O
data-driven	O
machine	O
learning	B
methods	O
to	O
rich	O
structured/symbolic	O
“	O
ai-style	O
”	O
models	O
.	O
this	O
seems	O
like	O
a	O
promising	O
future	O
direction	O
for	O
machine	B
learning	I
.	O
notation	O
introduction	O
it	O
is	O
very	O
difficult	O
to	O
come	O
up	O
with	O
a	O
single	O
,	O
consistent	B
notation	O
to	O
cover	O
the	O
wide	O
variety	O
of	O
data	O
,	O
models	O
and	O
algorithms	O
that	O
we	O
discuss	O
.	O
furthermore	O
,	O
conventions	O
differ	O
between	O
machine	B
learning	I
and	O
statistics	O
,	O
and	O
between	O
different	O
books	O
and	O
papers	O
.	O
nevertheless	O
,	O
we	O
have	O
tried	O
to	O
be	O
as	O
consistent	B
as	O
possible	O
.	O
below	O
we	O
summarize	O
most	O
of	O
the	O
notation	O
used	O
in	O
this	O
book	O
,	O
although	O
individual	O
sections	O
may	O
introduce	O
new	O
notation	O
.	O
note	O
also	O
that	O
the	O
same	O
symbol	O
may	O
have	O
different	O
meanings	O
depending	O
on	O
the	O
context	O
,	O
although	O
we	O
try	O
to	O
avoid	O
this	O
where	O
possible	O
.	O
general	O
math	O
notation	O
symbol	O
(	O
cid:28	O
)	O
x	O
(	O
cid:29	O
)	O
%	O
x	O
&	O
x	O
⊗	O
y	O
x	O
$	O
y	O
a	O
∧	O
b	O
a	O
∨	O
b	O
¬a	O
i	O
(	O
x	O
)	O
∞	O
→	O
∝	O
|x|	O
|s|	O
n	O
!	O
∇	O
∇2	O
(	O
cid:2	O
)	O
o	O
(	O
·	O
)	O
r	O
1	O
:	O
n	O
≈	O
argmaxx	O
f	O
(	O
x	O
)	O
meaning	O
floor	O
of	O
x	O
,	O
i.e.	O
,	O
round	O
down	O
to	O
nearest	O
integer	O
ceiling	O
of	O
x	O
,	O
i.e.	O
,	O
round	O
up	O
to	O
nearest	O
integer	O
convolution	O
of	O
x	O
and	O
y	O
hadamard	O
(	O
elementwise	O
)	O
product	O
of	O
x	O
and	O
y	O
logical	O
and	O
logical	O
or	O
logical	O
not	O
indicator	B
function	I
,	O
i	O
(	O
x	O
)	O
=	O
1	O
if	O
x	O
is	O
true	O
,	O
else	O
i	O
(	O
x	O
)	O
=	O
0	O
inﬁnity	O
tends	O
towards	O
,	O
e.g.	O
,	O
n	O
→	O
∞	O
proportional	O
to	O
,	O
so	O
y	O
=	O
ax	O
can	O
be	O
written	O
as	O
y	O
∝	O
x	O
absolute	O
value	O
size	O
(	O
cardinality	O
)	O
of	O
a	O
set	O
factorial	O
function	O
vector	O
of	O
ﬁrst	O
derivatives	O
hessian	O
matrix	O
of	O
second	O
derivatives	O
deﬁned	O
as	O
big-o	O
:	O
roughly	O
means	O
order	O
of	O
magnitude	O
the	O
real	O
numbers	O
range	O
(	O
matlab	O
convention	O
)	O
:	O
1	O
:	O
n	O
=	O
{	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
approximately	O
equal	O
to	O
argmax	O
:	O
the	O
value	O
x	O
that	O
maximizes	O
f	O
1010	O
notation	O
(	O
cid:23	O
)	O
(	O
cid:22	O
)	O
b	O
(	O
a	O
,	O
b	O
)	O
b	O
(	O
α	O
)	O
n	O
k	O
δ	O
(	O
x	O
)	O
δij	O
δx	O
(	O
y	O
)	O
exp	O
(	O
x	O
)	O
γ	O
(	O
x	O
)	O
ψ	O
(	O
x	O
)	O
x	O
γ	O
(	O
k	O
γ	O
(	O
αk	O
)	O
(	O
cid:2	O
)	O
k	O
αk	O
)	O
beta	B
function	I
,	O
b	O
(	O
a	O
,	O
b	O
)	O
=	O
multivariate	O
beta	O
function	O
,	O
n	O
choose	O
k	O
,	O
equal	O
to	O
n	O
!	O
/	O
(	O
k	O
!	O
(	O
n	O
−	O
k	O
)	O
!	O
)	O
dirac	O
delta	O
function	O
,	O
δ	O
(	O
x	O
)	O
=∞	O
if	O
x	O
=	O
0	O
,	O
else	O
δ	O
(	O
x	O
)	O
=	O
0	O
kronecker	O
delta	O
,	O
equals	O
1	O
if	O
i	O
=	O
j	O
,	O
otherwise	O
equals	O
0	O
kronecker	O
delta	O
,	O
equals	O
1	O
if	O
x	O
=	O
y	O
,	O
otherwise	O
equals	O
0	O
exponential	O
function	O
ex	O
gamma	B
function	I
,	O
γ	O
(	O
x	O
)	O
=	O
digamma	B
function	O
,	O
ψ	O
(	O
x	O
)	O
=	O
d	O
a	O
set	O
from	O
which	O
values	O
are	O
drawn	O
(	O
e.g.	O
,	O
x	O
=	O
r	O
d	O
)	O
ux−1e−udu	O
dx	O
log	O
γ	O
(	O
x	O
)	O
(	O
cid:21	O
)	O
∞	O
γ	O
(	O
a	O
)	O
γ	O
(	O
b	O
)	O
γ	O
(	O
a+b	O
)	O
(	O
cid:3	O
)	O
0	O
linear	O
algebra	O
notation	O
we	O
use	O
boldface	O
lowercase	O
to	O
denote	O
vectors	O
,	O
such	O
as	O
a	O
,	O
and	O
boldface	O
uppercase	O
to	O
denote	O
matrices	O
,	O
such	O
as	O
a.	O
vectors	O
are	O
assumed	O
to	O
be	O
column	O
vectors	O
,	O
unless	O
noted	O
otherwise	O
.	O
symbol	O
a	O
’	O
0	O
tr	O
(	O
a	O
)	O
det	O
(	O
a	O
)	O
|a|	O
a−1	O
a†	O
at	O
at	O
diag	O
(	O
a	O
)	O
diag	O
(	O
a	O
)	O
i	O
or	O
id	O
1	O
or	O
1d	O
0	O
or	O
0d	O
||x||	O
=	O
||x||2	O
||x||1	O
a	O
:	O
,j	O
ai	O
,	O
:	O
aij	O
x	O
⊗	O
y	O
meaning	O
a	O
is	O
a	O
positive	B
deﬁnite	I
matrix	O
trace	B
of	O
a	O
matrix	O
determinant	O
of	O
matrix	O
a	O
determinant	O
of	O
matrix	O
a	O
inverse	O
of	O
a	O
matrix	O
pseudo-inverse	O
of	O
a	O
matrix	O
transpose	O
of	O
a	O
matrix	O
transpose	O
of	O
a	O
vector	O
diagonal	O
matrix	O
made	O
from	O
vector	O
a	O
diagonal	B
vector	O
extracted	O
from	O
matrix	O
a	O
identity	O
matrix	O
of	O
size	O
d	O
×	O
d	O
(	O
ones	O
on	O
diagonal	B
,	O
zeros	O
off	O
)	O
vector	O
of	O
ones	O
(	O
of	O
length	O
d	O
)	O
vector	O
of	O
zeros	O
(	O
of	O
length	O
d	O
)	O
(	O
cid:7	O
)	O
d	O
euclidean	O
or	O
(	O
cid:2	O
)	O
2	O
norm	O
j=1	O
|xj|	O
(	O
cid:2	O
)	O
1	O
norm	O
j	O
’	O
th	O
column	O
of	O
matrix	O
transpose	O
of	O
i	O
’	O
th	O
row	O
of	O
matrix	O
(	O
a	O
column	O
vector	O
)	O
element	O
(	O
i	O
,	O
j	O
)	O
of	O
matrix	O
a	O
tensor	B
product	I
of	O
x	O
and	O
y	O
$	O
(	O
cid:7	O
)	O
d	O
j=1	O
x2	O
j	O
probability	O
notation	O
we	O
denote	O
random	O
and	O
ﬁxed	O
scalars	O
by	O
lower	O
case	O
,	O
random	O
and	O
ﬁxed	O
vectors	O
by	O
bold	O
lower	O
case	O
,	O
and	O
random	O
and	O
ﬁxed	O
matrices	O
by	O
bold	O
upper	O
case	O
.	O
occastionally	O
we	O
use	O
non-bold	O
upper	O
case	O
to	O
denote	O
scalar	O
random	O
variables	O
.	O
also	O
,	O
we	O
use	O
p	O
(	O
)	O
for	O
both	O
discrete	B
and	O
continuous	O
random	O
variables	O
.	O
notation	O
1011	O
symbol	O
x	O
⊥	O
y	O
x	O
(	O
cid:8	O
)	O
⊥	O
y	O
x	O
⊥	O
y	O
|z	O
x	O
(	O
cid:8	O
)	O
⊥	O
y	O
|z	O
x	O
∼	O
p	O
α	O
cov	O
[	O
x	O
]	O
e	O
[	O
x	O
]	O
eq	O
[	O
x	O
]	O
h	O
(	O
x	O
)	O
or	O
h	O
(	O
p	O
)	O
i	O
(	O
x	O
;	O
y	O
)	O
kl	O
(	O
p||q	O
)	O
(	O
cid:2	O
)	O
(	O
θ	O
)	O
l	O
(	O
θ	O
,	O
a	O
)	O
λ	O
λ	O
mode	B
[	O
x	O
]	O
μ	O
μ	O
p	O
(	O
x	O
)	O
p	O
(	O
x|y	O
)	O
φ	O
φ	O
π	O
ρ	O
sigm	O
(	O
x	O
)	O
σ2	O
σ	O
var	O
[	O
x	O
]	O
ν	O
z	O
meaning	O
x	O
is	O
independent	O
of	O
y	O
x	O
is	O
not	O
independent	O
of	O
y	O
x	O
is	O
conditionally	B
independent	I
of	O
y	O
given	O
z	O
x	O
is	O
not	O
conditionally	O
independent	O
of	O
y	O
given	O
z	O
x	O
is	O
distributed	O
according	O
to	O
distribution	O
p	O
parameters	O
of	O
a	O
beta	O
or	O
dirichlet	O
distribution	O
covariance	B
of	O
x	O
expected	B
value	I
of	O
x	O
expected	B
value	I
of	O
x	O
wrt	O
distribution	O
q	O
entropy	B
of	O
distribution	O
p	O
(	O
x	O
)	O
mutual	B
information	I
between	O
x	O
and	O
y	O
kl	O
divergence	O
from	O
distribution	O
p	O
to	O
q	O
log-likelihood	O
function	O
loss	O
function	O
for	O
taking	O
action	B
a	O
when	O
true	O
state	O
of	O
nature	O
is	O
θ	O
precision	B
(	O
inverse	O
variance	O
)	O
λ	O
=	O
1/σ2	O
precision	B
matrix	I
λ	O
=	O
σ−1	O
most	O
probable	O
value	O
of	O
x	O
mean	B
of	O
a	O
scalar	O
distribution	O
mean	B
of	O
a	O
multivariate	O
distribution	O
probability	O
density	O
or	O
mass	O
function	O
conditional	O
probability	O
density	O
of	O
x	O
given	O
y	O
cdf	B
of	O
standard	B
normal	I
pdf	O
of	O
standard	B
normal	I
multinomial	O
parameter	B
vector	O
,	O
stationary	B
distribution	I
of	O
markov	O
chain	O
correlation	O
coefficient	O
sigmoid	B
(	O
logistic	B
)	O
function	O
,	O
variance	B
covariance	O
matrix	O
variance	O
of	O
x	O
degrees	B
of	I
freedom	I
parameter	O
normalization	O
constant	O
of	O
a	O
probability	O
distribution	O
1+e−x	O
1	O
machine	O
learning/statistics	O
notation	O
in	O
general	O
,	O
we	O
use	O
upper	O
case	O
letters	O
to	O
denote	O
constants	O
,	O
such	O
as	O
c	O
,	O
d	O
,	O
k	O
,	O
n	O
,	O
s	O
,	O
t	O
,	O
etc	O
.	O
we	O
use	O
lower	O
case	O
letters	O
as	O
dummy	O
indexes	O
of	O
the	O
appropriate	O
range	O
,	O
such	O
as	O
c	O
=	O
1	O
:	O
c	O
to	O
index	O
classes	O
,	O
j	O
=	O
1	O
:	O
d	O
to	O
index	O
input	O
features	B
,	O
k	O
=	O
1	O
:	O
k	O
to	O
index	O
states	O
or	O
clusters	B
,	O
s	O
=	O
1	O
:	O
s	O
to	O
index	O
samples	B
,	O
t	O
=	O
1	O
:	O
t	O
to	O
index	O
time	O
,	O
etc	O
.	O
to	O
index	O
data	O
cases	O
,	O
we	O
use	O
the	O
notation	O
i	O
=	O
1	O
:	O
n	O
,	O
although	O
the	O
notation	O
n	O
=	O
1	O
:	O
n	O
is	O
also	O
widely	O
used	O
.	O
we	O
use	O
x	O
to	O
represent	O
an	O
observed	O
data	O
vector	O
.	O
in	O
a	O
supervised	O
problem	O
,	O
we	O
use	O
y	O
or	O
y	O
to	O
represent	O
the	O
desired	O
output	O
label	B
.	O
we	O
use	O
z	O
to	O
represent	O
a	O
hidden	B
variable	I
.	O
sometimes	O
we	O
also	O
use	O
q	O
to	O
represent	O
a	O
hidden	B
discrete	O
variable	O
.	O
1012	O
notation	O
symbol	O
c	O
d	O
r	O
d	O
dtest	O
j	O
(	O
θ	O
)	O
k	O
κ	O
(	O
x	O
,	O
y	O
)	O
k	O
λ	O
n	O
nc	O
φ	O
(	O
x	O
)	O
φ	O
q	O
(	O
)	O
q	O
(	O
θ	O
,	O
θold	O
)	O
s	O
t	O
t	O
(	O
d	O
)	O
t	O
θ	O
θ	O
(	O
s	O
)	O
ˆθ	O
ˆθm	O
l	O
ˆθm	O
ap	O
θ	O
w	O
w	O
xij	O
xi	O
x	O
x	O
˜x	O
x∗	O
y	O
zij	O
n=1	O
i	O
(	O
yn	O
=	O
c	O
)	O
meaning	O
number	O
of	O
classes	O
dimensionality	O
of	O
data	O
vector	O
(	O
number	O
of	O
features	B
)	O
number	O
of	O
outputs	O
(	O
response	O
variables	O
)	O
training	O
data	O
d	O
=	O
{	O
xi|i	O
=	O
1	O
:	O
n	O
}	O
or	O
d	O
=	O
{	O
(	O
xi	O
,	O
yi	O
)	O
|i	O
=	O
1	O
:	O
n	O
}	O
test	O
data	O
cost	O
function	O
number	O
of	O
states	O
or	O
dimensions	O
of	O
a	O
variable	O
(	O
often	O
latent	B
)	O
kernel	B
function	I
kernel	O
matrix	O
strength	O
of	O
(	O
cid:2	O
)	O
2	O
or	O
(	O
cid:2	O
)	O
1	O
regularizer	O
number	O
of	O
data	O
cases	O
number	O
of	O
examples	O
of	O
class	O
c	O
,	O
nc	O
=	O
basis	B
function	I
expansion	I
of	O
feature	O
vector	O
x	O
basis	B
function	I
expansion	I
of	O
design	B
matrix	I
x	O
approximate	O
or	O
proposal	B
distribution	I
auxiliary	O
function	O
in	O
em	O
number	O
of	O
samples	B
length	O
of	O
a	O
sequence	O
test	O
statistic	O
for	O
data	O
transition	O
matrix	O
of	O
markov	O
chain	O
parameter	O
vector	O
s	O
’	O
th	O
sample	O
of	O
parameter	B
vector	O
estimate	O
(	O
usually	O
mle	O
or	O
map	O
)	O
of	O
θ	O
maximum	B
likelihood	I
estimate	I
of	O
θ	O
map	O
estimate	O
of	O
θ	O
estimate	O
(	O
usually	O
posterior	B
mean	I
)	O
of	O
θ	O
vector	O
of	O
regression	B
weights	O
(	O
called	O
β	O
in	O
statistics	O
)	O
matrix	O
of	O
regression	B
weights	O
component	O
(	O
i.e.	O
,	O
feature	O
)	O
j	O
of	O
data	O
case	O
i	O
,	O
fori	O
=	O
1	O
:	O
n	O
,	O
j	O
=	O
1	O
:	O
d	O
(	O
cid:7	O
)	O
n	O
training	O
case	O
,	O
i	O
=	O
1	O
:	O
n	O
design	B
matrix	I
of	O
size	O
n	O
×	O
d	O
empirical	O
mean	O
x	O
=	O
1	O
n	O
future	O
test	O
case	O
future	O
test	O
case	O
vector	O
of	O
all	O
training	O
labels	O
y	O
=	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
)	O
latent	B
component	O
j	O
for	O
case	O
i	O
(	O
cid:7	O
)	O
n	O
i=1	O
xi	O
graphical	B
model	I
notation	O
in	O
graphical	O
models	O
,	O
we	O
index	O
nodes	B
by	O
s	O
,	O
t	O
,	O
u	O
∈	O
v	O
,	O
and	O
states	O
by	O
i	O
,	O
j	O
,	O
k	O
∈	O
x	O
.	O
notation	O
1013	O
symbol	O
meaning	O
s	O
∼	O
t	O
bel	O
c	O
chj	O
descj	O
g	O
e	O
mbt	O
nbdt	O
pat	O
predt	O
ψc	O
(	O
xc	O
)	O
s	O
θsjk	O
v	O
node	O
s	O
is	O
connected	O
to	O
node	O
t	O
belief	O
function	O
cliques	O
of	O
a	O
graph	B
child	O
of	O
node	O
j	O
in	O
a	O
dag	O
descendants	B
of	O
node	O
j	O
in	O
a	O
dag	O
a	O
graph	B
edges	O
of	O
a	O
graph	B
markov	O
blanket	O
of	O
node	O
t	O
neighborhood	O
of	O
node	O
t	O
parents	O
of	O
node	O
t	O
in	O
a	O
dag	O
predecessors	O
of	O
node	O
t	O
in	O
a	O
dag	O
wrt	O
some	O
ordering	O
potential	B
function	I
for	O
clique	B
c	O
separators	O
of	O
a	O
graph	B
prob	O
.	O
node	O
s	O
is	O
in	O
state	B
k	O
given	O
its	O
parents	B
are	O
in	O
states	O
j	O
nodes	B
of	O
a	O
graph	B
1014	O
notation	O
list	O
of	O
commonly	O
used	O
abbreviations	O
abbreviation	O
meaning	O
cdf	B
cpd	O
cpt	O
crf	O
dag	O
dgm	O
eb	O
em	O
ep	O
glm	O
gmm	O
hmm	O
iid	B
iff	O
kl	O
lds	O
lhs	O
map	O
mcmc	O
mh	O
mle	O
mpm	O
mrf	O
mse	O
nll	O
ols	O
pd	O
pdf	B
pmf	O
rbpf	O
rhs	O
rjmcmc	O
rss	O
slds	O
sse	O
ugm	O
vb	O
wrt	O
cumulative	B
distribution	I
function	I
conditional	O
probability	O
distribution	O
conditional	B
probability	I
table	O
conditional	B
random	I
ﬁeld	I
directed	O
acyclic	O
graphic	O
directed	B
graphical	I
model	I
empirical	O
bayes	O
expectation	B
maximization	I
algorithm	O
expectation	B
propagation	I
generalized	O
linear	O
model	O
gaussian	O
mixture	B
model	I
hidden	O
markov	O
model	O
independent	O
and	O
identically	O
distributed	O
if	O
and	O
only	O
if	O
kullback	O
leibler	O
divergence	O
linear	B
dynamical	I
system	I
left	O
hand	O
side	O
(	O
of	O
an	O
equation	O
)	O
maximum	O
a	O
posterior	O
estimate	O
markov	O
chain	O
monte	O
carlo	O
metropolis	O
hastings	O
maximum	B
likelihood	I
estimate	I
maximum	O
of	O
posterior	O
marginals	O
markov	O
random	O
ﬁeld	O
mean	B
squared	I
error	I
negative	O
log	O
likelihood	O
ordinary	B
least	I
squares	I
positive	O
deﬁnite	O
(	O
matrix	O
)	O
probability	B
density	I
function	I
probability	O
mass	O
function	O
rao-blackwellised	O
particle	O
ﬁlter	O
right	O
hand	O
side	O
(	O
of	O
an	O
equation	O
)	O
reversible	O
jump	O
mcmc	O
residual	B
sum	I
of	I
squares	I
switching	O
linear	B
dynamical	I
system	I
sum	O
of	O
squared	O
errors	O
undirected	B
graphical	I
model	I
variational	O
bayes	O
with	O
respect	O
to	O
aji	O
,	O
s.	O
m.	O
and	O
r.	O
j.	O
mceliece	O
(	O
2000	O
,	O
march	O
)	O
.	O
the	O
generalized	O
distribu-	O
tive	O
law	O
.	O
info	O
.	O
the-	O
ory	O
46	O
(	O
2	O
)	O
,	O
325–343	O
.	O
ieee	O
trans	O
.	O
alag	O
,	O
s.	O
and	O
a.	O
agogino	O
(	O
1996	O
)	O
.	O
in-	O
ference	O
using	O
message	O
propoga-	O
tion	O
and	O
topology	O
transformation	O
in	O
vector	O
gaussian	O
continuous	O
net-	O
works	O
.	O
in	O
uai	O
.	O
albers	O
,	O
c.	O
,	O
m.	O
leisink	O
,	O
and	O
h.	O
kap-	O
pen	O
(	O
2006	O
)	O
.	O
the	O
cluster	O
variation	O
method	O
for	O
efficient	O
linkage	O
anal-	O
ysis	O
on	O
extended	O
pedigrees	O
.	O
bmc	O
bioinformatics	O
7.	O
albert	O
,	O
j.	O
and	O
s.	O
chib	O
(	O
1993	O
)	O
.	O
bayesian	O
analysis	O
of	O
binary	O
and	O
polychoto-	O
mous	O
response	O
data	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
88	O
(	O
422	O
)	O
,	O
669–679	O
.	O
allwein	O
,	O
e.	O
,	O
r.	O
schapire	O
,	O
and	O
y.	O
singer	O
(	O
2000	O
)	O
.	O
reducing	O
multiclass	O
to	O
bi-	O
nary	O
:	O
a	O
unifying	O
approach	O
for	O
mar-	O
gin	O
classiﬁers	O
.	O
j.	O
of	O
machine	O
learn-	O
ing	O
research	O
,	O
113–141	O
.	O
aloise	O
,	O
d.	O
,	O
a.	O
deshpande	O
,	O
p.	O
hansen	O
,	O
and	O
p.	O
popat	O
(	O
2009	O
)	O
.	O
np-hardness	O
of	O
euclidean	O
sum-of-squares	O
clus-	O
tering	O
.	O
machine	B
learning	I
75	O
,	O
245–	O
249.	O
alpaydin	O
,	O
e.	O
(	O
2004	O
)	O
.	O
machine	B
learning	I
.	O
mit	O
press	O
.	O
introduction	O
to	O
altun	O
,	O
y.	O
,	O
t.	O
hofmann	O
,	O
and	O
i.	O
tsochan-	O
taridis	O
(	O
2006	O
)	O
.	O
large	O
margin	O
meth-	O
ods	O
for	O
structured	O
and	O
interde-	O
pendent	O
output	O
variables	O
.	O
in	O
g.	O
bakir	O
,	O
t.	O
hofmann	O
,	O
b.	O
scholkopf	O
,	O
a.	O
smola	O
,	O
b.	O
taskar	O
,	O
and	O
s.	O
vish-	O
wanathan	O
(	O
eds	O
.	O
)	O
,	O
machine	B
learning	I
with	O
structured	O
outputs	O
.	O
mit	O
press	O
.	O
amir	O
,	O
e.	O
(	O
2010	O
)	O
.	O
approximation	O
al-	O
gorithms	O
for	O
treewidth	B
.	O
algorith-	O
mica	O
56	O
(	O
4	O
)	O
,	O
448.	O
amir	O
,	O
e.	O
and	O
s.	O
mcilraith	O
(	O
2005	O
)	O
.	O
partition-based	O
reason-	O
ing	O
for	O
ﬁrst-order	O
and	O
propo-	O
sitional	O
artiﬁcial	O
intelligence	O
162	O
(	O
1	O
)	O
,	O
49–88	O
.	O
theories	O
.	O
logical	O
bibliography	O
abend	O
,	O
k.	O
,	O
t.	O
j.	O
harley	O
,	O
and	O
l.	O
n.	O
kanal	O
(	O
1965	O
)	O
.	O
classiﬁcation	B
of	O
binary	O
ran-	O
dom	O
patterns	O
.	O
ieee	O
transactions	O
on	O
information	B
theory	I
11	O
(	O
4	O
)	O
,	O
538–544	O
.	O
ackley	O
,	O
d.	O
,	O
g.	O
hinton	O
,	O
and	O
t.	O
sejnowski	O
(	O
1985	O
)	O
.	O
a	O
learning	B
algorithm	O
for	O
boltzmann	O
machines	O
.	O
cognitive	O
science	O
9	O
,	O
147–169	O
.	O
adams	O
,	O
r.	O
p.	O
,	O
h.	O
wallach	O
,	O
and	O
z.	O
ghahramani	O
(	O
2010	O
)	O
.	O
learning	B
the	O
structure	O
of	O
deep	B
sparse	O
graphical	O
models	O
.	O
in	O
ai/statistics	O
.	O
aggarwal	O
,	O
d.	O
and	O
s.	O
merugu	O
(	O
2007	O
)	O
.	O
predictive	B
discrete	O
latent	B
factor	O
models	O
for	O
large	O
scale	O
dyadic	B
data	O
.	O
in	O
proc	O
.	O
of	O
the	O
int	O
’	O
l	O
conf	O
.	O
on	O
knowl-	O
edge	O
discovery	O
and	O
data	O
mining	O
.	O
ahmed	O
,	O
a.	O
and	O
e.	O
xing	O
(	O
2007	O
)	O
.	O
on	O
tight	O
approximate	B
inference	I
of	O
the	O
logistic-normal	O
topic	B
admixture	O
model	O
.	O
in	O
ai/statistics	O
.	O
ahn	O
,	O
j.-h.	O
and	O
j.-h.	O
oh	O
(	O
2003	O
)	O
.	O
a	O
con-	O
strained	O
em	O
algorithm	O
for	O
princi-	O
pal	O
component	O
analysis	O
.	O
neural	O
computation	O
15	O
,	O
57–65	O
.	O
ahn	O
,	O
s.	O
,	O
a.	O
korattikara	O
,	O
and	O
m.	O
welling	O
(	O
2012	O
)	O
.	O
bayesian	O
posterior	O
sam-	O
pling	O
via	O
stochastic	O
gradient	O
fisher	O
scoring	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
airoldi	O
,	O
e.	O
,	O
d.	O
blei	O
,	O
s.	O
fienberg	O
,	O
and	O
e.	O
xing	O
(	O
2008	O
)	O
.	O
mixed-membership	O
stochastic	O
blockmodels	O
.	O
j.	O
of	O
ma-	O
chine	O
learning	B
research	O
9	O
,	O
1981–	O
2014.	O
ando	O
,	O
r.	O
and	O
t.	O
zhang	O
(	O
2005	O
)	O
.	O
a	O
framework	O
for	O
learning	B
predictive	O
structures	O
from	O
multiple	O
tasks	O
and	O
unlabeled	O
data	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
6	O
,	O
1817–1853	O
.	O
aitchison	O
,	O
j	O
.	O
(	O
1982	O
)	O
.	O
the	O
statistical	O
analysis	O
of	O
compositional	O
data	O
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
44	O
(	O
2	O
)	O
,	O
139–	O
177.	O
andrews	O
,	O
d.	O
and	O
c.	O
mallows	O
(	O
1974	O
)	O
.	O
scale	O
mixtures	O
of	O
normal	B
distribu-	O
tions	O
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
36	O
,	O
99–102	O
.	O
(	O
2000	O
)	O
.	O
andrieu	O
,	O
c.	O
,	O
n.	O
de	O
freitas	O
,	O
and	O
a.	O
doucet	O
sequential	B
bayesian	O
estimation	O
and	O
model	O
se-	O
lection	O
for	O
dynamic	O
kernel	O
ma-	O
chines	O
.	O
technical	O
report	O
,	O
cam-	O
bridge	O
univ	O
.	O
andrieu	O
,	O
c.	O
,	O
n.	O
de	O
freitas	O
,	O
and	O
a.	O
doucet	O
robust	B
full	O
bayesian	O
learning	B
for	O
radial	O
ba-	O
sis	O
networks	O
.	O
neural	O
computa-	O
tion	O
13	O
(	O
10	O
)	O
,	O
2359–2407	O
.	O
(	O
2001	O
)	O
.	O
andrieu	O
,	O
c.	O
,	O
n.	O
de	O
freitas	O
,	O
a.	O
doucet	O
,	O
and	O
m.	O
jordan	O
(	O
2003	O
)	O
.	O
an	O
introduc-	O
tion	O
to	O
mcmc	O
for	O
machine	O
learn-	O
ing	O
.	O
machine	B
learning	I
50	O
,	O
5–43	O
.	O
andrieu	O
,	O
c.	O
,	O
a.	O
doucet	O
,	O
and	O
v.	O
tadic	O
(	O
2005	O
)	O
.	O
online	O
em	O
for	O
parameter	B
estimation	O
in	O
nonlinear-non	O
gaus-	O
sian	O
state-space	O
models	O
.	O
in	O
proc	O
.	O
ieee	O
cdc	O
.	O
andrieu	O
,	O
c.	O
and	O
j.	O
thoms	O
(	O
2008	O
)	O
.	O
a	O
tutorial	O
on	O
adaptive	O
mcmc	O
.	O
statis-	O
tical	O
computing	O
18	O
,	O
343–373	O
.	O
aoki	O
,	O
m.	O
(	O
1987	O
)	O
.	O
state	B
space	I
modeling	O
of	O
time	O
series	O
.	O
springer	O
.	O
archambeau	O
,	O
c.	O
and	O
f.	O
bach	O
(	O
2008	O
)	O
.	O
sparse	B
probabilistic	O
projections	O
.	O
in	O
nips	O
.	O
argyriou	O
,	O
a.	O
,	O
t.	O
evgeniou	O
,	O
and	O
m.	O
pon-	O
til	O
(	O
2008	O
)	O
.	O
convex	B
multi-task	O
fea-	O
ture	O
learning	B
.	O
machine	O
learn-	O
ing	O
73	O
(	O
3	O
)	O
,	O
243–272	O
.	O
armagan	O
,	O
a.	O
,	O
d.	O
dunson	O
,	O
and	O
j.	O
lee	O
(	O
2011	O
)	O
.	O
generalized	O
double	O
pareto	O
shrinkage	B
.	O
technical	O
report	O
,	O
duke	O
.	O
armstrong	O
,	O
h.	O
(	O
2005	O
)	O
.	O
bayesian	O
esti-	O
mation	O
of	O
decomposable	B
gaussian	O
graphical	O
models	O
.	O
thesis	O
,	O
unsw	O
.	O
ph.d.	O
armstrong	O
,	O
h.	O
,	O
c.	O
carter	O
,	O
k.	O
wong	O
,	O
and	O
r.	O
kohn	O
(	O
2008	O
)	O
.	O
bayesian	O
co-	O
variance	B
matrix	O
estimation	O
using	O
a	O
mixture	O
of	O
decomposable	O
graphi-	O
cal	O
models	O
.	O
statistics	O
and	O
comput-	O
ing	O
,	O
1573–1375	O
.	O
arnborg	O
,	O
s.	O
,	O
d.	O
g.	O
corneil	O
,	O
and	O
a.	O
proskurowski	O
(	O
1987	O
)	O
.	O
complex-	O
ity	O
of	O
ﬁnding	O
embeddings	O
in	O
a	O
k-	O
tree	B
.	O
siam	O
j.	O
on	O
algebraic	O
and	O
dis-	O
crete	O
methods	O
8	O
,	O
277–284	O
.	O
arora	O
,	O
s.	O
and	O
b.	O
barak	O
(	O
2009	O
)	O
.	O
com-	O
plexity	O
theory	O
:	O
a	O
modern	O
approach	O
.	O
cambridge	O
.	O
arthur	O
,	O
d.	O
and	O
s.	O
vassilvitskii	O
(	O
2007	O
)	O
.	O
k-	O
means++	O
:	O
the	O
advantages	O
of	O
careful	O
seeding	O
.	O
in	O
proc	O
.	O
18th	O
acm-siam	O
symp	O
.	O
on	O
discrete	B
algorithms	O
,	O
pp	O
.	O
1027â	O
˘a	O
¸s1035	O
.	O
1016	O
bibliography	O
arulampalam	O
,	O
m.	O
,	O
s.	O
maskell	O
,	O
n.	O
gor-	O
don	O
,	O
and	O
t.	O
clapp	O
(	O
2002	O
,	O
febru-	O
ary	O
)	O
.	O
a	O
tutorial	O
on	O
particle	O
fil-	O
ters	O
for	O
online	O
nonlinear/non-	O
gaussian	O
bayesian	O
tracking	B
.	O
ieee	O
trans	O
.	O
on	O
signal	B
processing	I
50	O
(	O
2	O
)	O
,	O
174–189	O
.	O
asavathiratham	O
,	O
c.	O
(	O
2000	O
)	O
.	O
the	O
inﬂu-	O
ence	O
model	O
:	O
a	O
tractable	O
representa-	O
tion	O
for	O
the	O
dynamics	O
of	O
networked	O
markov	O
chains	O
.	O
ph.d.	O
thesis	O
,	O
mit	O
,	O
dept	O
.	O
eecs	O
.	O
atay-kayis	O
,	O
a.	O
and	O
h.	O
massam	O
(	O
2005	O
)	O
.	O
a	O
monte	O
carlo	O
method	O
for	O
comput-	O
ing	O
the	O
marginal	B
likelihood	I
in	O
non-	O
decomposable	B
gaussian	O
graphical	O
models	O
.	O
biometrika	O
92	O
,	O
317–335	O
.	O
attenberg	O
,	O
j.	O
,	O
k.	O
weinberger	O
,	O
a.	O
smola	O
,	O
a.	O
dasgupta	O
,	O
and	O
m.	O
zinkevich	O
(	O
2009	O
)	O
.	O
collaborative	O
spam	O
ﬁlter-	O
ing	O
with	O
the	O
hashing	O
trick	O
.	O
in	O
virus	O
bulletin	O
.	O
attias	O
,	O
h.	O
(	O
1999	O
)	O
.	O
independent	O
factor	O
analysis	O
.	O
neural	O
computation	O
11	O
,	O
803–851	O
.	O
attias	O
,	O
h.	O
(	O
2000	O
)	O
.	O
a	O
variational	O
bayesian	O
framework	O
for	O
graphical	O
models	O
.	O
in	O
nips-12	O
.	O
bach	O
,	O
f.	O
(	O
2008	O
)	O
.	O
bolasso	B
:	O
model	O
con-	O
sistent	O
lasso	B
estimation	O
through	O
the	O
bootstrap	B
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
ma-	O
chine	O
learning	B
.	O
bach	O
,	O
f.	O
and	O
m.	O
jordan	O
(	O
2001	O
)	O
.	O
thin	B
junction	I
trees	I
.	O
in	O
nips	O
.	O
bach	O
,	O
f.	O
and	O
m.	O
jordan	O
(	O
2005	O
)	O
.	O
a	O
prob-	O
abilistic	O
interpretation	O
of	O
canonical	O
correlation	O
analysis	O
.	O
technical	O
re-	O
port	O
688	O
,	O
u.	O
c.	O
berkeley	O
.	O
bach	O
,	O
f.	O
and	O
e.	O
moulines	O
(	O
2011	O
)	O
.	O
non-	O
asymptotic	O
analysis	O
of	O
stochastic	B
approximation	I
algorithms	O
for	O
ma-	O
chine	O
learning	B
.	O
in	O
nips	O
.	O
bahmani	O
,	O
b.	O
,	O
b.	O
moseley	O
,	O
a.	O
vattani	O
,	O
r.	O
kumar	O
,	O
and	O
s.	O
vassilvitskii	O
(	O
2012	O
)	O
.	O
scalable	O
k-means++	B
.	O
in	O
vldb	O
.	O
bakker	O
,	O
b.	O
and	O
t.	O
heskes	O
(	O
2003	O
)	O
.	O
task	O
clustering	B
and	O
gating	O
for	O
bayesian	O
multitask	O
learning	B
.	O
j.	O
of	O
machine	B
learning	I
research	O
4	O
,	O
83–99	O
.	O
baldi	O
,	O
p.	O
and	O
y.	O
chauvin	O
(	O
1994	O
)	O
.	O
smooth	O
online	B
learning	I
algorithms	O
for	O
hidden	B
markov	O
models	O
.	O
neural	O
computation	O
6	O
,	O
305–316	O
.	O
balding	O
,	O
d.	O
(	O
2006	O
)	O
.	O
a	O
tutorial	O
on	O
sta-	O
tistical	O
methods	O
for	O
population	O
as-	O
sociation	O
studies	O
.	O
nature	O
reviews	O
genetics	O
7	O
,	O
81–91	O
.	O
technique	O
occuring	O
in	O
the	O
statisti-	O
cal	O
analysis	O
of	O
probabalistic	O
func-	O
tions	O
in	O
markov	O
chains	O
.	O
the	O
annals	O
of	O
mathematical	O
statistics	O
41	O
,	O
164–	O
171.	O
beal	O
,	O
m.	O
(	O
2003	O
)	O
.	O
variational	O
algorithms	O
for	O
approximate	O
bayesian	O
inference	B
.	O
ph.d.	O
thesis	O
,	O
gatsby	O
unit	O
.	O
beal	O
,	O
m.	O
and	O
z.	O
ghahramani	O
(	O
2006	O
)	O
.	O
variational	O
bayesian	O
learning	B
of	O
directed	O
graphical	O
models	O
with	O
hidden	B
variables	I
.	O
bayesian	O
anal-	O
ysis	O
1	O
(	O
4	O
)	O
.	O
beal	O
,	O
m.	O
j.	O
,	O
z.	O
ghahramani	O
,	O
and	O
c.	O
e.	O
rasmussen	O
(	O
2002	O
)	O
.	O
the	O
inﬁnite	O
hid-	O
den	O
markov	O
model	O
.	O
in	O
nips-14	O
.	O
beck	O
,	O
a.	O
and	O
m.	O
teboulle	O
(	O
2009	O
)	O
.	O
a	O
fast	O
iterative	O
shrinkage-thresholding	O
al-	O
gorothm	O
for	O
linear	O
inverse	O
prob-	O
lems	O
.	O
siam	O
j.	O
on	O
imaging	O
sci-	O
ences	O
2	O
(	O
1	O
)	O
,	O
183–202	O
.	O
beinlich	O
,	O
i.	O
,	O
h.	O
suermondt	O
,	O
r.	O
chavez	O
,	O
and	O
g.	O
cooper	O
(	O
1989	O
)	O
.	O
the	O
alarm	O
monitoring	O
system	O
:	O
a	O
case	O
study	O
with	O
two	O
probabilistic	B
inference	I
techniques	O
for	O
belief	B
networks	I
.	O
in	O
proc	O
.	O
of	O
the	O
second	O
european	O
conf	O
.	O
on	O
ai	O
in	O
medicine	O
,	O
pp	O
.	O
247–256	O
.	O
bekkerman	O
,	O
r.	O
,	O
m.	O
bilenko	O
,	O
and	O
j.	O
langford	O
(	O
eds	O
.	O
)	O
(	O
2011	O
)	O
.	O
scaling	O
up	O
machine	B
learning	I
.	O
cambridge	O
.	O
bell	O
,	O
a.	O
j.	O
and	O
t.	O
j.	O
sejnowski	O
(	O
1995	O
)	O
.	O
an	O
information	B
maximisation	O
ap-	O
proach	O
to	O
blind	O
separation	O
and	O
blind	O
deconvolution	O
.	O
neural	O
com-	O
putation	O
7	O
(	O
6	O
)	O
,	O
1129–1159	O
.	O
bengio	O
,	O
y	O
.	O
(	O
2009	O
)	O
.	O
learning	B
deep	O
ar-	O
chitectures	O
for	O
ai	O
.	O
foundations	O
and	O
trends	O
in	O
machine	B
learning	I
2	O
(	O
1	O
)	O
,	O
1–	O
127.	O
bengio	O
,	O
y.	O
and	O
s.	O
bengio	O
(	O
2000	O
)	O
.	O
modeling	O
high-dimensional	O
dis-	O
crete	O
data	O
with	O
multi-layer	O
neural	O
networks	O
.	O
in	O
nips	O
.	O
banerjee	O
,	O
o.	O
,	O
l.	O
e.	O
ghaoui	O
,	O
and	O
a.	O
d	O
’	O
aspremont	O
(	O
2008	O
)	O
.	O
model	O
se-	O
lection	O
through	O
sparse	B
maximum	O
likelihood	B
estimation	O
for	O
multivari-	O
ate	O
gaussian	O
or	O
binary	O
data	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
9	O
,	O
485–	O
516.	O
bar-shalom	O
,	O
y.	O
and	O
t.	O
fortmann	O
(	O
1988	O
)	O
.	O
tracking	B
and	O
data	O
associa-	O
tion	O
.	O
academic	O
press	O
.	O
bar-shalom	O
,	O
y.	O
and	O
x.	O
li	O
(	O
1993	O
)	O
.	O
es-	O
timation	O
and	O
tracking	B
:	O
principles	O
,	O
techniques	O
and	O
software	O
.	O
artech	O
house	O
.	O
barash	O
,	O
y.	O
and	O
n.	O
friedman	O
(	O
2002	O
)	O
.	O
context-speciﬁc	O
bayesian	O
cluster-	O
ing	O
for	O
gene	O
expression	O
data	O
.	O
j.	O
comp	O
.	O
bio	O
.	O
9	O
,	O
169–191	O
.	O
barber	O
,	O
d.	O
(	O
2006	O
)	O
.	O
expectation	O
cor-	O
rection	O
for	O
smoothed	O
inference	B
in	O
switching	O
linear	O
dynamical	O
sys-	O
tems	O
.	O
j.	O
of	O
machine	B
learning	I
re-	O
search	O
7	O
,	O
2515–2540	O
.	O
barber	O
,	O
d.	O
and	O
c.	O
bishop	O
(	O
1998	O
)	O
.	O
ensemble	B
learning	I
in	O
bayesian	O
neural	B
networks	I
.	O
in	O
c.	O
bishop	O
(	O
ed	O
.	O
)	O
,	O
neural	B
networks	I
and	O
machine	B
learning	I
,	O
pp	O
.	O
215–237	O
.	O
springer	O
.	O
barber	O
,	O
d.	O
and	O
s.	O
chiappa	O
(	O
2007	O
)	O
.	O
uniﬁed	O
inference	B
for	O
variational	O
bayesian	O
linear	O
gaussian	O
state	B
space	I
models	O
.	O
in	O
nips	O
.	O
barbieri	O
,	O
m.	O
and	O
j.	O
berger	O
(	O
2004	O
)	O
.	O
op-	O
timal	O
predictive	B
model	O
selection	O
.	O
annals	O
of	O
statistics	O
32	O
,	O
870–897	O
.	O
bartlett	O
,	O
p.	O
,	O
m.	O
jordan	O
,	O
and	O
j.	O
mcauliffe	O
(	O
2006	O
)	O
.	O
convexity	O
,	O
classiﬁcation	B
,	O
and	O
risk	B
bounds	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
101	O
(	O
473	O
)	O
,	O
138–156	O
.	O
baruniak	O
,	O
r.	O
(	O
2007	O
)	O
.	O
compressive	O
sens-	O
ieee	O
signal	B
processing	I
maga-	O
ing	O
.	O
zine	O
.	O
barzilai	O
,	O
j.	O
and	O
j.	O
borwein	O
(	O
1988	O
)	O
.	O
two	O
point	O
step	O
size	O
gradient	O
methods	O
.	O
ima	O
j.	O
of	O
numerical	O
analysis	O
8	O
,	O
141–	O
148.	O
p.	O
vincent	O
,	O
bengio	O
,	O
y.	O
,	O
o.	O
delalleau	O
,	O
n.	O
roux	O
,	O
and	O
j.	O
paiement	O
,	O
m.	O
ouimet	O
(	O
2004	O
)	O
.	O
learning	B
eigen-	O
functions	O
links	O
spectral	B
embedding	O
and	O
kernel	B
pca	O
.	O
neural	O
computa-	O
tion	O
16	O
,	O
2197–2219	O
.	O
bengio	O
,	O
y.	O
and	O
p.	O
frasconi	O
(	O
1995	O
)	O
.	O
dif-	O
fusion	O
of	O
context	O
and	O
credit	O
infor-	O
mation	O
in	O
markovian	O
models	O
.	O
j.	O
of	O
ai	O
research	O
3	O
,	O
249–270	O
.	O
bengio	O
,	O
y.	O
and	O
p.	O
frasconi	O
(	O
1996	O
)	O
.	O
input/output	O
hmms	O
for	O
sequence	O
processing	O
.	O
ieee	O
trans	O
.	O
on	O
neural	B
networks	I
7	O
(	O
5	O
)	O
,	O
1231–1249	O
.	O
human	O
basu	O
,	O
s.	O
,	O
t.	O
choudhury	O
,	O
b.	O
clarkson	O
,	O
learn-	O
and	O
a.	O
pentland	O
(	O
2001	O
)	O
.	O
interactions	O
with	O
ing	O
the	O
inﬂuence	B
model	I
.	O
techni-	O
cal	O
report	O
539	O
,	O
mit	O
media	O
lab	O
.	O
ftp	O
:	O
//whitechapel.media.mit.edu/pub/tech-	O
reports/tr-539-abstract.html	O
.	O
baum	O
,	O
l.	O
e.	O
,	O
t.	O
petrie	O
,	O
g.	O
soules	O
,	O
and	O
n.	O
weiss	O
(	O
1970	O
)	O
.	O
a	O
maximization	O
bibliography	O
1017	O
bengio	O
,	O
y.	O
,	O
p.	O
lamblin	O
,	O
d.	O
popovici	O
,	O
and	O
h.	O
larochelle	O
(	O
2007	O
)	O
.	O
greedy	O
layer-wise	O
training	O
of	O
deep	B
net-	O
works	O
.	O
in	O
nips	O
.	O
berchtold	O
,	O
a	O
.	O
(	O
1999	O
)	O
.	O
the	O
double	O
chain	O
markov	O
model	O
.	O
comm	O
.	O
stat	O
.	O
theor	O
.	O
methods	O
28	O
,	O
2569–2589	O
.	O
berger	O
,	O
j	O
.	O
(	O
1985	O
)	O
.	O
bayesian	O
salesman-	O
ship	O
.	O
in	O
p.	O
k.	O
goel	O
and	O
a.	O
zellner	O
(	O
eds	O
.	O
)	O
,	O
bayesian	O
inference	B
and	O
de-	O
cision	O
techniques	O
with	O
applications	O
:	O
essays	O
in	O
honor	O
of	O
bruno	O
definetti	O
.	O
north-holland	O
.	O
berger	O
,	O
j.	O
and	O
r.	O
wolpert	O
(	O
1988	O
)	O
.	O
the	O
likelihood	B
principle	I
.	O
the	O
institute	O
of	O
mathematical	O
statistics	O
.	O
2nd	O
edi-	O
tion	O
.	O
berkhin	O
,	O
p.	O
(	O
2006	O
)	O
.	O
a	O
survey	O
of	O
clustering	B
datamining	O
techniques	O
.	O
in	O
j.	O
kogan	O
,	O
c.	O
nicholas	O
,	O
and	O
m.	O
teboulle	O
(	O
eds	O
.	O
)	O
,	O
grouping	O
multi-	O
dimensional	O
data	O
:	O
recent	O
advances	O
in	O
clustering	B
,	O
pp	O
.	O
25–71	O
.	O
springer	O
.	O
bernardo	O
,	O
j.	O
and	O
a.	O
smith	O
(	O
1994	O
)	O
.	O
bayesian	O
theory	O
.	O
john	O
wiley	O
.	O
berrou	O
,	O
c.	O
,	O
a.	O
glavieux	O
,	O
and	O
p.	O
thiti-	O
majashima	O
(	O
1993	O
)	O
.	O
near	O
shannon	O
limit	O
error-correcting	O
coding	O
and	O
decoding	B
:	O
turbo	B
codes	I
.	O
proc	O
.	O
ieee	O
intl	O
.	O
comm	O
.	O
conf..	O
berry	O
,	O
d.	O
and	O
y.	O
hochberg	O
(	O
1999	O
)	O
.	O
bayesian	O
perspectives	O
on	O
multiple	O
comparisons	O
.	O
j.	O
statist	O
.	O
planning	O
and	O
inference	B
82	O
,	O
215–227	O
.	O
bertele	O
,	O
u.	O
and	O
f.	O
brioschi	O
(	O
1972	O
)	O
.	O
non-	O
serial	O
dynamic	B
programming	I
.	O
aca-	O
demic	O
press	O
.	O
bertsekas	O
,	O
d.	O
(	O
1997	O
)	O
.	O
parallel	O
and	O
dis-	O
tribution	O
computation	O
:	O
numerical	O
methods	O
.	O
athena	O
scientiﬁc	O
.	O
bertsekas	O
,	O
d.	O
(	O
1999	O
)	O
.	O
nonlinear	O
pro-	O
athena	O
gramming	O
(	O
second	O
ed.	O
)	O
.	O
scientiﬁc	O
.	O
bertsekas	O
,	O
d.	O
and	O
j.	O
tsitsiklis	O
(	O
2008	O
)	O
.	O
introduction	O
to	O
probability	O
.	O
athena	O
scientiﬁc	O
.	O
2nd	O
edition	O
.	O
besag	O
,	O
j	O
.	O
(	O
1975	O
)	O
.	O
statistical	O
analysis	O
of	O
non-lattice	O
data	O
.	O
the	O
statistician	O
24	O
,	O
179–196	O
.	O
bhattacharya	O
,	O
a.	O
and	O
d.	O
b.	O
dunson	O
(	O
2011	O
)	O
.	O
simplex	O
factor	O
models	O
for	O
multivariate	O
unordered	O
categorical	B
data	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc..	O
to	O
appear	O
.	O
bickel	O
,	O
p.	O
and	O
e.	O
levina	O
(	O
2004	O
)	O
.	O
some	O
theory	O
for	O
fisher	O
’	O
s	O
linear	O
discrimi-	O
nant	O
function	O
,	O
``	O
naive	O
bayes	O
''	O
,	O
and	O
some	O
alternatives	O
when	O
there	O
are	O
many	O
more	O
variables	O
than	O
obser-	O
vations	O
.	O
bernoulli	O
10	O
,	O
989–1010	O
.	O
bickson	O
,	O
d.	O
(	O
2009	O
)	O
.	O
gaussian	O
belief	B
propagation	I
:	O
theory	O
and	O
applica-	O
tion	O
.	O
ph.d.	O
thesis	O
,	O
hebrew	O
univer-	O
sity	O
of	O
jerusalem	O
.	O
bilmes	O
,	O
j	O
.	O
(	O
2000	O
)	O
.	O
dynamic	O
bayesian	O
multinets	O
.	O
in	O
uai	O
.	O
bilmes	O
,	O
j.	O
a	O
.	O
(	O
2001	O
)	O
.	O
graphical	O
models	O
and	O
automatic	B
speech	I
recognition	I
.	O
technical	O
report	O
uweetr-2001-	O
0005	O
,	O
univ	O
.	O
washington	O
,	O
dept	O
.	O
of	O
elec	O
.	O
eng	O
.	O
binder	O
,	O
j.	O
,	O
d.	O
koller	O
,	O
s.	O
j.	O
russell	O
,	O
and	O
k.	O
kanazawa	O
(	O
1997	O
)	O
.	O
adaptive	O
prob-	O
abilistic	O
networks	O
with	O
hidden	B
vari-	O
ables	O
.	O
machine	B
learning	I
29	O
,	O
213–	O
244.	O
binder	O
,	O
j.	O
,	O
k.	O
murphy	O
,	O
and	O
s.	O
russell	O
(	O
1997	O
)	O
.	O
space-efficient	O
inference	B
in	O
dynamic	O
probabilistic	O
networks	O
.	O
in	O
intl	O
.	O
joint	O
conf	O
.	O
on	O
ai	O
.	O
birnbaum	O
,	O
a	O
.	O
(	O
1962	O
)	O
.	O
on	O
the	O
founda-	O
j.	O
of	O
tions	O
of	O
statistical	O
infernece	O
.	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
57	O
,	O
269–326	O
.	O
bishop	O
,	O
c.	O
(	O
1999	O
)	O
.	O
bayesian	O
pca	O
.	O
nips	O
.	O
in	O
bishop	O
,	O
c.	O
(	O
2006a	O
)	O
.	O
pattern	B
recognition	I
and	O
machine	B
learning	I
.	O
springer	O
.	O
bishop	O
,	O
c.	O
(	O
2006b	O
)	O
.	O
pattern	B
recognition	I
and	O
machine	B
learning	I
.	O
springer	O
.	O
bishop	O
,	O
c.	O
and	O
g.	O
james	O
(	O
1993	O
)	O
.	O
analy-	O
sis	O
of	O
multiphase	O
ﬂows	O
using	O
dual-	O
energy	O
densitometry	O
and	O
neural	B
networks	I
.	O
nuclear	O
instruments	O
and	O
methods	O
in	O
physics	O
research	O
a327	O
,	O
580–593	O
.	O
bishop	O
,	O
c.	O
and	O
m.	O
svensén	O
(	O
2003	O
)	O
.	O
bayesian	O
hierarchical	O
mixtures	O
of	O
experts	O
.	O
in	O
uai	O
.	O
bishop	O
,	O
c.	O
and	O
m.	O
tipping	O
(	O
2000	O
)	O
.	O
variational	O
relevance	O
vector	O
ma-	O
chines	O
.	O
in	O
uai	O
.	O
bishop	O
,	O
c.	O
m.	O
(	O
1995	O
)	O
.	O
neural	B
networks	I
for	O
pattern	B
recognition	I
.	O
clarendon	O
press	O
.	O
bishop	O
,	O
y.	O
,	O
s.	O
fienberg	O
,	O
and	O
p.	O
holland	O
(	O
1975	O
)	O
.	O
discrete	B
multivariate	O
analy-	O
sis	O
:	O
theory	O
and	O
practice	O
.	O
mit	O
press	O
.	O
bistarelli	O
,	O
(	O
1997	O
)	O
.	O
s.	O
,	O
u.	O
montanari	O
,	O
and	O
semiring-based	O
f.	O
rossi	O
constraint	O
satisfaction	O
and	O
opti-	O
mization	O
.	O
j.	O
of	O
the	O
acm	O
44	O
(	O
2	O
)	O
,	O
201–	O
236.	O
blake	O
,	O
a.	O
,	O
p.	O
kohli	O
,	O
and	O
c.	O
rother	O
(	O
eds	O
.	O
)	O
(	O
2011	O
)	O
.	O
advances	O
in	O
markov	O
random	O
fields	O
for	O
vision	O
and	O
image	O
process-	O
ing	O
.	O
mit	O
press	O
.	O
blei	O
,	O
d.	O
and	O
j.	O
lafferty	O
(	O
2006a	O
)	O
.	O
corre-	O
lated	O
topic	B
models	O
.	O
in	O
nips	O
.	O
blei	O
,	O
d.	O
and	O
j.	O
lafferty	O
(	O
2006b	O
)	O
.	O
dy-	O
namic	O
topic	B
models	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
,	O
pp	O
.	O
113–120	O
.	O
blei	O
,	O
d.	O
and	O
j.	O
lafferty	O
(	O
2007	O
)	O
.	O
a	O
corre-	O
lated	O
topic	B
model	I
of	O
``	O
science	O
''	O
.	O
an-	O
nals	O
of	O
applied	O
stat	O
.	O
1	O
(	O
1	O
)	O
,	O
17–35	O
.	O
blei	O
,	O
d.	O
and	O
j.	O
mcauliffe	O
(	O
2010	O
,	O
march	O
)	O
.	O
supervised	O
topic	O
models	O
.	O
technical	O
report	O
,	O
princeton	O
.	O
blei	O
,	O
d.	O
,	O
a.	O
ng	O
,	O
and	O
m.	O
jordan	O
(	O
2003	O
)	O
.	O
latent	B
dirichlet	O
allocation	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
3	O
,	O
993–	O
1022.	O
blumensath	O
,	O
t.	O
and	O
m.	O
davies	O
(	O
2007	O
)	O
.	O
on	O
the	O
difference	O
between	O
orthog-	O
onal	O
matching	B
pursuit	I
and	O
orthog-	O
onal	O
least	B
squares	I
.	O
technical	O
re-	O
port	O
,	O
u.	O
edinburgh	O
.	O
bo	O
,	O
l.	O
,	O
c.	O
sminchisescu	O
,	O
a.	O
kanaujia	O
,	O
and	O
d.	O
metaxas	O
(	O
2008	O
)	O
.	O
fast	O
algo-	O
rithms	O
for	O
large	O
scale	O
conditional	O
3d	O
prediction	O
.	O
in	O
cvpr	O
.	O
bohning	O
,	O
d.	O
(	O
1992	O
)	O
.	O
multinomial	B
logis-	O
tic	O
regression	B
algorithm	O
.	O
annals	O
of	O
the	O
inst	O
.	O
of	O
statistical	O
math	O
.	O
44	O
,	O
197–	O
200.	O
bollen	O
,	O
k.	O
(	O
1989	O
)	O
.	O
structural	O
equation	O
john	O
models	O
with	O
latent	B
variables	O
.	O
wiley	O
&	O
sons	O
.	O
july	O
)	O
.	O
(	O
2009	O
,	O
bordes	O
,	O
a.	O
,	O
l.	O
bottou	O
,	O
and	O
p.	O
galli-	O
nari	O
sgd-qn	O
:	O
care-	O
ful	O
quasi-newton	O
stochastic	O
gradi-	O
ent	O
descent	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
10	O
,	O
1737–1754	O
.	O
bhatnagar	O
,	O
n.	O
,	O
c.	O
bogdanov	O
,	O
and	O
e.	O
mossel	O
the	O
compu-	O
tational	O
complexity	O
of	O
estimating	O
convergence	O
time	O
.	O
technical	O
re-	O
port	O
,	O
.	O
(	O
2010	O
)	O
.	O
bishop	O
,	O
c.	O
m.	O
(	O
1994	O
)	O
.	O
mixture	O
density	O
networks	O
.	O
technical	O
report	O
ncrg	O
4288	O
,	O
neural	O
computing	O
research	O
group	O
,	O
department	O
of	O
computer	O
science	O
,	O
aston	O
university	O
.	O
bordes	O
,	O
a.	O
,	O
l.	O
bottou	O
,	O
p.	O
gallinari	O
,	O
j.	O
chang	O
,	O
and	O
s.	O
a.	O
smith	O
(	O
2010	O
)	O
.	O
er-	O
ratum	O
:	O
sgdqn	O
is	O
less	O
careful	O
than	O
expected	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
11	O
,	O
2229–2240	O
.	O
1018	O
bibliography	O
boser	O
,	O
b.	O
e.	O
,	O
i.	O
m.	O
guyon	O
,	O
and	O
v.	O
n.	O
vapnik	O
(	O
1992	O
)	O
.	O
a	O
training	O
algorithm	O
for	O
optimal	O
margin	O
classiﬁers	O
.	O
in	O
proc	O
.	O
of	O
the	O
workshop	O
on	O
computa-	O
tional	O
learning	B
theory	O
.	O
brand	O
,	O
m.	O
(	O
1999	O
)	O
.	O
structure	B
learning	I
in	O
conditional	B
probability	I
models	O
via	O
an	O
entropic	O
prior	O
and	O
param-	O
eter	O
extinction	O
.	O
neural	O
computa-	O
tion	O
11	O
,	O
1155–1182	O
.	O
brown	O
,	O
p.	O
,	O
m.	O
vannucci	O
,	O
and	O
t.	O
fearn	O
(	O
1998	O
)	O
.	O
multivariate	O
bayesian	O
vari-	O
able	O
selection	O
and	O
prediction	O
.	O
j.	O
of	O
the	O
royal	O
statistical	O
society	O
b	O
60	O
(	O
3	O
)	O
,	O
627–641	O
.	O
bottcher	O
,	O
s.	O
g.	O
and	O
c.	O
dethlefsen	O
(	O
2003	O
)	O
.	O
deal	O
:	O
a	O
package	O
for	O
learning	B
bayesian	O
networks	O
.	O
j.	O
of	O
statistical	O
software	O
8	O
(	O
20	O
)	O
.	O
braun	O
,	O
m.	O
and	O
j.	O
mcauliffe	O
(	O
2010	O
)	O
.	O
vari-	O
ational	O
inference	B
for	O
large-scale	O
models	O
of	O
discrete	O
choice	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
105	O
(	O
489	O
)	O
,	O
324–335	O
.	O
bottolo	O
,	O
l.	O
and	O
s.	O
richardson	O
(	O
2010	O
)	O
.	O
search	O
.	O
evolutionary	O
bayesian	O
analysis	O
5	O
(	O
3	O
)	O
,	O
583–618	O
.	O
stochastic	O
bottou	O
,	O
l.	O
(	O
1998	O
)	O
.	O
online	O
algorithms	O
and	O
stochastic	O
approximations	O
.	O
in	O
d.	O
saad	O
(	O
ed	O
.	O
)	O
,	O
online	B
learning	I
and	O
neural	B
networks	I
.	O
cambridge	O
.	O
bottou	O
,	O
l.	O
(	O
2007	O
)	O
.	O
learning	B
with	O
large	O
datasets	O
(	O
nips	O
tutorial	O
)	O
.	O
bottou	O
,	O
l.	O
,	O
o.	O
chapelle	O
,	O
d.	O
decoste	O
,	O
and	O
j.	O
weston	O
(	O
eds	O
.	O
)	O
(	O
2007	O
)	O
.	O
large	O
scale	O
kernel	B
machines	O
.	O
mit	O
press	O
.	O
bouchard	O
,	O
g.	O
(	O
2007	O
)	O
.	O
efficient	O
bounds	O
for	O
the	O
softmax	B
and	O
applications	O
to	O
approximate	B
inference	I
in	O
hybrid	O
models	O
.	O
in	O
nips	O
2007	O
workshop	O
on	O
approximate	B
inference	I
in	O
hybrid	O
models	O
.	O
bouchard-cote	O
,	O
a.	O
and	O
m.	O
jordan	O
(	O
2009	O
)	O
.	O
optimization	B
of	O
structured	B
mean	I
ﬁeld	I
objectives	O
.	O
in	O
uai	O
.	O
bowman	O
,	O
a.	O
and	O
a.	O
azzalini	O
(	O
1997	O
)	O
.	O
ap-	O
plied	O
smoothing	O
techniques	O
for	O
data	O
analysis	O
.	O
oxford	O
.	O
box	O
,	O
g.	O
and	O
n.	O
draper	O
(	O
1987	O
)	O
.	O
empir-	O
ical	O
model-building	O
and	O
response	O
surfaces	O
.	O
wiley	O
.	O
box	O
,	O
g.	O
and	O
g.	O
tiao	O
(	O
1973	O
)	O
.	O
bayesian	O
in	O
statistical	O
analysis	O
.	O
inference	B
addison-wesley	O
.	O
boyd	O
,	O
s.	O
and	O
l.	O
vandenberghe	O
(	O
2004	O
)	O
.	O
convex	B
optimization	O
.	O
cambridge	O
.	O
boyen	O
,	O
x.	O
and	O
d.	O
koller	O
(	O
1998	O
)	O
.	O
tractable	O
inference	O
for	O
complex	O
stochastic	B
processes	I
.	O
in	O
uai	O
.	O
boykov	O
,	O
y.	O
,	O
o.	O
veksler	O
,	O
and	O
r.	O
zabih	O
fast	O
approximate	O
energy	O
(	O
2001	O
)	O
.	O
minimization	O
via	O
graph	B
cuts	I
.	O
ieee	O
trans	O
.	O
on	O
pattern	B
analysis	O
and	O
ma-	O
chine	O
intelligence	O
23	O
(	O
11	O
)	O
.	O
brand	O
,	O
m.	O
(	O
1996	O
)	O
.	O
coupled	O
hidden	O
markov	O
models	O
for	O
modeling	O
inter-	O
acting	O
processes	O
.	O
technical	O
report	O
405	O
,	O
mit	O
lab	O
for	O
perceptual	O
com-	O
puting	O
.	O
breiman	O
,	O
l.	O
(	O
1996	O
)	O
.	O
bagging	B
predictors	O
.	O
machine	B
learning	I
24	O
,	O
123–140	O
.	O
breiman	O
,	O
l.	O
(	O
1998	O
)	O
.	O
arcing	O
classiﬁers	O
.	O
annals	O
of	O
statistics	O
26	O
,	O
801–849	O
.	O
breiman	O
,	O
l.	O
(	O
2001a	O
)	O
.	O
random	B
forests	I
.	O
machine	B
learning	I
45	O
(	O
1	O
)	O
,	O
5–32	O
.	O
breiman	O
,	O
l.	O
(	O
2001b	O
)	O
.	O
statistical	O
mod-	O
eling	O
:	O
the	O
two	O
cultures	O
.	O
statistical	O
science	O
16	O
(	O
3	O
)	O
,	O
199–231	O
.	O
breiman	O
,	O
l.	O
,	O
j.	O
friedman	O
,	O
and	O
r.	O
ol-	O
shen	O
(	O
1984	O
)	O
.	O
classiﬁcation	B
and	O
re-	O
gression	O
trees	O
.	O
wadsworth	O
.	O
breslow	O
,	O
n.	O
e.	O
and	O
d.	O
g.	O
clayton	O
(	O
1993	O
)	O
.	O
approximate	B
inference	I
in	O
general-	O
ized	O
linear	O
mixed	O
models	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
88	O
(	O
421	O
)	O
,	O
9–25	O
.	O
briers	O
,	O
m.	O
,	O
a.	O
doucet	O
,	O
and	O
s.	O
maskel	O
smoothing	O
algorithms	O
for	O
(	O
2010	O
)	O
.	O
state-space	O
models	O
.	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathemat-	O
ics	O
62	O
(	O
1	O
)	O
,	O
61–89	O
.	O
brochu	O
,	O
e.	O
,	O
m.	O
cora	O
,	O
and	O
n.	O
de	O
fre-	O
itas	O
(	O
2009	O
,	O
november	O
)	O
.	O
a	O
tutorial	O
on	O
bayesian	O
optimization	B
of	O
expen-	O
sive	O
cost	O
functions	O
,	O
with	O
applica-	O
tion	O
to	O
active	O
user	O
modeling	O
and	O
hierarchical	O
learn-	O
ing	O
.	O
technical	O
report	O
tr-2009-23	O
,	O
department	O
of	O
computer	O
science	O
,	O
university	O
of	O
british	O
columbia	O
.	O
reinforcement	O
brooks	O
,	O
s.	O
and	O
g.	O
roberts	O
(	O
1998	O
)	O
.	O
assessing	O
convergence	O
of	O
markov	O
chain	O
monte	O
carlo	O
algorithms	O
.	O
statistics	O
and	O
computing	O
8	O
,	O
319–	O
335.	O
brown	O
,	O
l.	O
,	O
t.	O
cai	O
,	O
and	O
a.	O
dasgupta	O
(	O
2001	O
)	O
.	O
interval	O
estimation	O
for	O
a	O
bi-	O
nomial	O
proportion	O
.	O
statistical	O
sci-	O
ence	O
16	O
(	O
2	O
)	O
,	O
101–133	O
.	O
brown	O
,	O
m.	O
p.	O
,	O
r.	O
hughey	O
,	O
a.	O
krogh	O
,	O
i.	O
s.	O
mian	O
,	O
k.	O
sjölander	O
,	O
and	O
d.	O
haus-	O
sler	O
(	O
1993	O
)	O
.	O
using	O
dirichlet	O
mixtures	O
priors	O
to	O
derive	O
hidden	B
markov	O
models	O
for	O
protein	O
families	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
intelligent	O
systems	O
for	O
molecular	O
biology	O
,	O
pp	O
.	O
47–55	O
.	O
bruckstein	O
,	O
a.	O
,	O
d.	O
donoho	O
,	O
and	O
m.	O
elad	O
(	O
2009	O
)	O
.	O
from	O
sparse	B
so-	O
lutions	O
of	O
systems	O
of	O
equations	O
to	O
sparse	B
modeling	O
of	O
signals	O
and	O
im-	O
ages	O
.	O
siam	O
review	O
51	O
(	O
1	O
)	O
,	O
34–81	O
.	O
bryson	O
,	O
a.	O
and	O
y.-c.	O
ho	O
(	O
1969	O
)	O
.	O
applied	O
optimal	O
control	O
:	O
optimization	B
,	O
esti-	O
mation	O
,	O
and	O
control	O
.	O
blaisdell	O
pub-	O
lishing	O
company	O
.	O
buhlmann	O
,	O
p.	O
and	O
t.	O
hothorn	O
(	O
2007	O
)	O
.	O
boosting	B
algorithms	O
:	O
regulariza-	O
tion	O
,	O
prediction	O
and	O
model	O
fitting	O
.	O
statistical	O
science	O
22	O
(	O
4	O
)	O
,	O
477–505	O
.	O
buhlmann	O
,	O
s.	O
p.	O
(	O
2011	O
)	O
.	O
de	O
and	O
geer	O
statistics	O
for	O
high-	O
dimensional	O
data	O
:	O
methodology	O
,	O
theory	O
and	O
applications	O
.	O
springer	O
.	O
van	O
buhlmann	O
,	O
p.	O
and	O
b.	O
yu	O
(	O
2003	O
)	O
.	O
boost-	O
ing	O
with	O
the	O
l2	O
loss	B
:	O
regression	B
and	O
classiﬁcation	B
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
98	O
(	O
462	O
)	O
,	O
324–339	O
.	O
buhlmann	O
,	O
p.	O
and	O
b.	O
yu	O
(	O
2006	O
)	O
.	O
sparse	B
j.	O
of	O
machine	B
learning	I
boosting	O
.	O
research	O
7	O
,	O
1001–1024	O
.	O
bui	O
,	O
h.	O
,	O
s.	O
venkatesh	O
,	O
and	O
g.	O
west	O
(	O
2002	O
)	O
.	O
policy	B
recognition	O
in	O
the	O
abstract	O
hidden	B
markov	O
model	O
.	O
j.	O
of	O
ai	O
research	O
17	O
,	O
451–499	O
.	O
buntine	O
,	O
w.	O
(	O
2002	O
)	O
.	O
variational	O
exten-	O
sions	O
to	O
em	O
and	O
multinomial	B
pca	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
buntine	O
,	O
w.	O
and	O
a.	O
jakulin	O
(	O
2004	O
)	O
.	O
ap-	O
plying	O
discrete	B
pca	O
in	O
data	O
analy-	O
sis	O
.	O
in	O
uai	O
.	O
buntine	O
,	O
w.	O
and	O
a.	O
jakulin	O
(	O
2006	O
)	O
.	O
dis-	O
crete	O
component	O
analysis	O
.	O
in	O
sub-	O
space	O
,	O
latent	B
structure	O
and	O
feature	B
selection	I
:	O
statistical	O
and	O
optimiza-	O
tion	O
perspectives	O
workshop	O
.	O
buntine	O
,	O
w.	O
and	O
a.	O
weigend	O
(	O
1991	O
)	O
.	O
bayesian	O
backpropagation	B
.	O
com-	O
plex	O
systems	O
5	O
,	O
603–643	O
.	O
burges	O
,	O
c.	O
j.	O
,	O
t.	O
shaked	O
,	O
e.	O
renshaw	O
,	O
a.	O
lazier	O
,	O
m.	O
deeds	O
,	O
n.	O
hamilton	O
,	O
and	O
g.	O
hullender	O
(	O
2005	O
)	O
.	O
learning	B
to	I
rank	I
using	O
gradient	B
descent	I
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
,	O
pp	O
.	O
89–96	O
.	O
burkard	O
,	O
r.	O
,	O
m.	O
dell	O
’	O
amico	O
,	O
and	O
assignment	O
s.	O
martello	O
(	O
2009	O
)	O
.	O
problems	O
.	O
siam	O
.	O
bibliography	O
1019	O
byran	O
,	O
k.	O
and	O
t.	O
leise	O
(	O
2006	O
)	O
.	O
the	O
25,000,000,000	O
eigenvector	O
:	O
the	O
linear	O
algebra	O
behind	O
google	O
.	O
siam	O
review	O
48	O
(	O
3	O
)	O
.	O
calvetti	O
,	O
d.	O
and	O
e.	O
somersalo	O
(	O
2007	O
)	O
.	O
introduction	O
to	O
bayesian	O
scientiﬁc	O
computing	O
.	O
springer	O
.	O
candes	O
,	O
e.	O
,	O
j.	O
romberg	O
,	O
and	O
t.	O
tao	O
(	O
2006	O
)	O
.	O
robust	B
uncertainty	O
prin-	O
ciples	O
:	O
exact	O
signal	O
reconstruction	O
from	O
highly	O
incomplete	O
frequency	O
information	B
.	O
ieee	O
.	O
trans	O
.	O
inform	O
.	O
theory	O
52	O
(	O
2	O
)	O
,	O
489–509	O
.	O
candes	O
,	O
e.	O
and	O
m.	O
wakin	O
(	O
2008	O
,	O
march	O
)	O
.	O
an	O
introduction	O
to	O
com-	O
pressive	O
sampling	O
.	O
ieee	O
signal	O
pro-	O
cessing	O
magazine	O
21.	O
candes	O
,	O
e.	O
,	O
m.	O
wakin	O
,	O
and	O
s.	O
boyd	O
enhancing	O
sparsity	B
by	O
(	O
2008	O
)	O
.	O
reweighted	O
l1	O
minimization	O
.	O
j.	O
of	O
fourier	O
analysis	O
and	O
applications	O
1	O
,	O
877–905	O
.	O
cannings	O
,	O
c.	O
,	O
e.	O
a.	O
thompson	O
,	O
and	O
m.	O
h.	O
skolnick	O
(	O
1978	O
)	O
.	O
probabil-	O
ity	O
functions	O
in	O
complex	O
pedigrees	O
.	O
advances	O
in	O
applied	O
probability	O
10	O
,	O
26–61	O
.	O
canny	O
,	O
j	O
.	O
(	O
2004	O
)	O
.	O
gap	O
:	O
a	O
factor	B
model	O
in	O
proc	O
.	O
an-	O
intl	O
.	O
acm	O
sigir	O
conference	O
,	O
for	O
discrete	B
data	O
.	O
nual	O
pp	O
.	O
122–129	O
.	O
cao	O
,	O
z.	O
,	O
t.	O
qin	O
,	O
t.-y	O
.	O
liu	O
,	O
m.-f.	O
tsai	O
,	O
and	O
h.	O
li	O
(	O
2007	O
)	O
.	O
learning	B
to	I
rank	I
:	O
from	O
pairwise	O
approach	O
to	O
listwise	O
approach	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
,	O
pp	O
.	O
129â	O
˘a	O
¸s136	O
.	O
cappe	O
,	O
o	O
.	O
(	O
2010	O
)	O
.	O
online	O
expectation	O
maximisation	O
.	O
in	O
k.	O
mengersen	O
,	O
m.	O
titterington	O
,	O
and	O
c.	O
robert	O
(	O
eds	O
.	O
)	O
,	O
mixtures	O
.	O
cappe	O
,	O
o.	O
and	O
e.	O
mouline	O
(	O
2009	O
,	O
june	O
)	O
.	O
online	O
em	O
algorithm	O
for	O
latent	B
data	O
models	O
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
71	O
(	O
3	O
)	O
,	O
593–613	O
.	O
cappe	O
,	O
o.	O
,	O
e.	O
moulines	O
,	O
and	O
t.	O
ryden	O
(	O
2005	O
)	O
.	O
inference	B
in	O
hidden	B
markov	O
models	O
.	O
springer	O
.	O
carbonetto	O
,	O
p.	O
(	O
2003	O
)	O
.	O
unsupervised	O
statistical	O
models	O
for	O
general	O
object	O
recognition	O
.	O
master	B
’	O
s	O
thesis	O
,	O
uni-	O
versity	O
of	O
british	O
columbia	O
.	O
caron	O
,	O
f.	O
and	O
a.	O
doucet	O
(	O
2008	O
)	O
.	O
sparse	B
bayesian	O
nonparametric	O
re-	O
gression	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
carreira-perpinan	O
,	O
m.	O
and	O
c.	O
williams	O
(	O
2003	O
)	O
.	O
an	O
isotropic	B
gaussian	O
mix-	O
ture	O
can	O
have	O
more	O
modes	O
than	O
components	O
.	O
technical	O
report	O
edi-inf-rr-0185	O
,	O
school	O
of	O
infor-	O
matics	O
,	O
u.	O
edinburgh	O
.	O
carter	O
,	O
c.	O
and	O
r.	O
kohn	O
(	O
1994	O
)	O
.	O
on	O
gibbs	O
sampling	O
for	O
state	B
space	I
models	O
.	O
biometrika	O
81	O
(	O
3	O
)	O
,	O
541–553	O
.	O
carterette	O
,	O
b.	O
,	O
p.	O
bennett	O
,	O
d.	O
chicker-	O
ing	O
,	O
and	O
s.	O
dumais	O
(	O
2008	O
)	O
.	O
here	O
or	O
there	O
:	O
preference	O
judgments	O
for	O
relevance	O
.	O
in	O
proc	O
.	O
ecir	O
.	O
caruana	O
,	O
r.	O
(	O
1998	O
)	O
.	O
a	O
dozen	O
tricks	O
with	O
in	O
g.	O
orr	O
and	O
multitask	O
learning	B
.	O
k.-r.	O
mueller	O
(	O
eds	O
.	O
)	O
,	O
neural	O
net-	O
works	O
:	O
tricks	O
of	O
the	O
trade	O
.	O
springer-	O
verlag	O
.	O
caruana	O
,	O
r.	O
and	O
a.	O
niculescu-mizil	O
(	O
2006	O
)	O
.	O
an	O
empirical	O
comparison	O
of	O
supervised	B
learning	I
algorithms	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
carvahlo	O
,	O
c.	O
,	O
n.	O
polson	O
,	O
and	O
j.	O
scott	O
(	O
2010	O
)	O
.	O
the	O
horseshoe	O
estimator	B
for	O
sparse	B
signals	O
.	O
biometrika	O
97	O
(	O
2	O
)	O
,	O
465.	O
carvahlo	O
,	O
l.	O
and	O
c.	O
lawrence	O
(	O
2007	O
)	O
.	O
centroid	B
estimation	O
in	O
discrete	B
high-dimensional	O
spaces	O
with	O
ap-	O
plications	O
in	O
biology	O
.	O
proc	O
.	O
of	O
the	O
national	O
academy	O
of	O
science	O
,	O
usa	O
105	O
(	O
4	O
)	O
.	O
carvalho	O
,	O
c.	O
m.	O
and	O
m.	O
west	O
(	O
2007	O
)	O
.	O
dynamic	O
matrix-variate	O
graphical	O
models	O
.	O
bayesian	O
analysis	O
2	O
(	O
1	O
)	O
,	O
69–	O
98.	O
casella	O
,	O
g.	O
and	O
r.	O
berger	O
(	O
2002	O
)	O
.	O
statis-	O
tical	O
inference	B
.	O
duxbury	O
.	O
2nd	O
edi-	O
tion	O
.	O
castro	O
,	O
m.	O
,	O
m.	O
coates	O
,	O
and	O
r.	O
d.	O
nowak	O
(	O
2004	O
)	O
.	O
likelihood	B
based	O
hi-	O
erarchical	O
clustering	B
.	O
ieee	O
trans	O
.	O
in	O
signal	B
processing	I
52	O
(	O
8	O
)	O
,	O
230.	O
celeux	O
,	O
g.	O
and	O
j.	O
diebolt	O
(	O
1985	O
)	O
.	O
the	O
sem	O
algorithm	O
:	O
a	O
probabilis-	O
tic	O
teacher	O
derive	O
from	O
the	O
em	O
algorithm	O
for	O
the	O
mixture	B
prob-	O
lem	O
.	O
computational	O
statistics	O
quar-	O
terly	O
2	O
,	O
73–82	O
.	O
carlin	O
,	O
b.	O
p.	O
and	O
t.	O
a.	O
louis	O
(	O
1996	O
)	O
.	O
bayes	O
and	O
empirical	O
bayes	O
methods	O
for	O
data	O
analysis	O
.	O
chapman	O
and	O
hall	O
.	O
cemgil	O
,	O
a.	O
t.	O
(	O
2001	O
)	O
.	O
a	O
technique	O
for	O
painless	O
derivation	O
of	O
kalman	O
ﬁlter-	O
ing	O
recursions	O
.	O
technical	O
report	O
,	O
u.	O
nijmegen	O
.	O
cesa-bianchi	O
,	O
n.	O
and	O
g.	O
lugosi	O
(	O
2006	O
)	O
.	O
learning	B
,	O
and	O
games	O
.	O
prediction	O
,	O
cambridge	O
university	O
press	O
.	O
cevher	O
,	O
v.	O
(	O
2009	O
)	O
.	O
learning	B
with	O
com-	O
pressible	O
priors	O
.	O
in	O
nips	O
.	O
chai	O
,	O
k.	O
m.	O
a	O
.	O
(	O
2010	O
)	O
.	O
multi-task	O
learn-	O
ing	O
with	O
gaussian	O
processes	O
.	O
ph.d.	O
thesis	O
,	O
u.	O
edinburgh	O
.	O
chang	O
,	O
h.	O
,	O
y.	O
weiss	O
,	O
and	O
w.	O
freeman	O
(	O
2009	O
)	O
.	O
informative	O
sensing	O
.	O
tech-	O
nical	O
report	O
,	O
hebrew	O
u.	O
submitted	O
to	O
ieee	O
transactions	O
on	O
info	O
.	O
the-	O
ory	O
.	O
chang	O
,	O
j.	O
and	O
d.	O
blei	O
(	O
2010	O
)	O
.	O
hierar-	O
chical	O
relational	O
models	O
for	O
docu-	O
ment	O
networks	O
.	O
the	O
annals	O
of	O
ap-	O
plied	O
statistics	O
4	O
(	O
1	O
)	O
,	O
124–150	O
.	O
chang	O
,	O
j.	O
,	O
j.	O
boyd-graber	O
,	O
s.	O
gerrish	O
,	O
c.	O
wang	O
,	O
and	O
d.	O
blei	O
(	O
2009	O
)	O
.	O
read-	O
ing	O
tea	O
leaves	B
:	O
how	O
humans	O
inter-	O
pret	O
topic	B
models	O
.	O
in	O
nips	O
.	O
chapelle	O
,	O
o.	O
and	O
l.	O
li	O
(	O
2011	O
)	O
.	O
an	O
empir-	O
ical	O
evaluation	O
of	O
thompson	O
sam-	O
pling	O
.	O
in	O
nips	O
.	O
chartrand	O
,	O
r.	O
and	O
w.	O
yin	O
(	O
2008	O
)	O
.	O
it-	O
eratively	O
reweighted	O
algorithms	O
for	O
compressive	B
sensing	I
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
acoustics	O
,	O
speech	O
and	O
signal	O
proc	O
.	O
chechik	O
,	O
g.	O
,	O
a.	O
g.	O
n.	O
tishby	O
,	O
and	O
information	B
bot-	O
y.	O
weiss	O
(	O
2005	O
)	O
.	O
tleneck	O
for	O
gaussian	O
variables	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
6	O
,	O
165â	O
˘a	O
¸s188	O
.	O
cheeseman	O
,	O
p.	O
,	O
j.	O
kelly	O
,	O
m.	O
self	O
,	O
j.	O
stutz	O
,	O
w.	O
taylor	O
,	O
and	O
d.	O
freeman	O
(	O
1988	O
)	O
.	O
autoclass	B
:	O
a	O
bayesian	O
classiﬁcation	B
system	O
.	O
in	O
proc	O
.	O
of	O
the	O
fifth	O
intl	O
.	O
workshop	O
on	O
machine	B
learning	I
.	O
cheeseman	O
,	O
p.	O
and	O
j.	O
stutz	O
(	O
1996	O
)	O
.	O
bayesian	O
classiﬁcation	B
(	O
autoclass	B
)	O
:	O
in	O
fayyad	O
,	O
theory	O
and	O
results	O
.	O
and	O
pratetsky-shapiro	O
,	O
uthurasamy	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
knowledge	B
discovery	I
and	O
data	O
min-	O
ing	O
.	O
mit	O
press	O
.	O
smyth	O
,	O
chen	O
,	O
b.	O
,	O
k.	O
swersky	O
,	O
b.	O
marlin	O
,	O
and	O
n.	O
de	O
freitas	O
(	O
2010	O
)	O
.	O
sparsity	B
priors	O
and	O
boosting	B
for	O
learning	B
localized	O
distributed	O
feature	O
representations	O
.	O
technical	O
report	O
,	O
ubc	O
.	O
chen	O
,	O
b.	O
,	O
j.-a	O
.	O
ting	O
,	O
b.	O
marlin	O
,	O
and	O
n.	O
de	O
freitas	O
(	O
2010	O
)	O
.	O
deep	B
learning	I
of	O
fea-	O
tures	O
from	O
video	O
.	O
in	O
nips	O
workshop	O
on	O
deep	B
learning	I
.	O
invariant	B
spatio-temporal	O
1020	O
bibliography	O
chen	O
,	O
m.	O
,	O
d.	O
carlson	O
,	O
a.	O
zaas	O
,	O
c.	O
woods	O
,	O
g.	O
ginsburg	O
,	O
a.	O
hero	O
,	O
j.	O
lucas	O
,	O
and	O
l.	O
carin	O
(	O
2011	O
,	O
march	O
)	O
.	O
the	O
bayesian	O
elastic	B
net	I
:	O
classifying	O
multi-task	O
gene-	O
expression	O
data	O
.	O
ieee	O
trans	O
.	O
biomed	O
.	O
eng	O
.	O
58	O
(	O
3	O
)	O
,	O
468–79	O
.	O
chen	O
,	O
r.	O
and	O
s.	O
liu	O
(	O
2000	O
)	O
.	O
mixture	B
kalman	O
ﬁlters	O
.	O
j.	O
royal	O
stat	O
.	O
soc	O
.	O
b.	O
chen	O
,	O
s.	O
and	O
j.	O
goodman	O
(	O
1996	O
)	O
.	O
an	O
empirical	O
study	O
of	O
smoothing	O
tech-	O
niques	O
for	O
language	B
modeling	I
.	O
in	O
proc	O
.	O
34th	O
acl	O
,	O
pp	O
.	O
310–318	O
.	O
chen	O
,	O
s.	O
and	O
j.	O
goodman	O
(	O
1998	O
)	O
.	O
an	O
empirical	O
study	O
of	O
smoothing	O
techniques	O
for	O
language	B
modeling	I
.	O
technical	O
report	O
tr-10-98	O
,	O
dept	O
.	O
comp	O
.	O
sci.	O
,	O
harvard	O
.	O
chen	O
,	O
s.	O
and	O
j.	O
wigger	O
(	O
1995	O
,	O
july	O
)	O
.	O
fast	O
orthogonal	O
least	B
squares	I
algorithm	O
for	O
efficient	O
subset	O
model	B
selection	I
.	O
ieee	O
trans	O
.	O
signal	B
processing	I
3	O
(	O
7	O
)	O
,	O
1713–1715	O
.	O
chen	O
,	O
s.	O
s.	O
,	O
d.	O
l.	O
donoho	O
,	O
and	O
m.	O
a.	O
saunders	O
(	O
1998	O
)	O
.	O
atomic	O
decompo-	O
sition	O
by	O
basis	O
pursuit	O
.	O
siam	O
jour-	O
nal	O
on	O
scientiﬁc	O
computing	O
20	O
(	O
1	O
)	O
,	O
33–61	O
.	O
chen	O
,	O
x.	O
,	O
s.	O
kim	O
,	O
q.	O
lin	O
,	O
j.	O
g.	O
carbonell	O
,	O
and	O
e.	O
p.	O
xing	O
(	O
2010	O
)	O
.	O
graph-structured	O
multi-task	O
re-	O
gression	O
and	O
an	O
efficient	O
optimiza-	O
tion	O
method	O
for	O
general	O
fused	B
lasso	I
.	O
technical	O
report	O
,	O
cmu	O
.	O
chib	O
,	O
s.	O
(	O
1995	O
)	O
.	O
marginal	O
from	O
the	O
gibbs	O
output	O
.	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
90	O
,	O
1313–1321	O
.	O
likelihood	B
j.	O
of	O
the	O
chickering	O
,	O
d.	O
learning	B
bayesian	O
networks	O
is	O
np-complete	O
.	O
in	O
ai/stats	O
v.	O
(	O
1996	O
)	O
.	O
chickering	O
,	O
d.	O
and	O
d.	O
heckerman	O
(	O
1997	O
)	O
.	O
efficient	O
approximations	O
for	O
the	O
marginal	B
likelihood	I
of	O
incom-	O
plete	O
data	O
given	O
a	O
bayesian	O
net-	O
work	O
.	O
machine	B
learning	I
29	O
,	O
181–	O
212.	O
chickering	O
,	O
d.	O
m.	O
(	O
2002	O
)	O
.	O
optimal	O
structure	O
identiﬁcation	O
with	O
greedy	O
search	O
.	O
journal	O
of	O
machine	O
learn-	O
ing	O
research	O
3	O
,	O
507–554	O
.	O
chipman	O
,	O
h.	O
,	O
e.	O
george	O
,	O
and	O
r.	O
mc-	O
bayesian	O
cart	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
culloch	O
(	O
1998	O
)	O
.	O
model	O
search	O
.	O
assoc	O
.	O
93	O
,	O
935–960	O
.	O
chipman	O
,	O
h.	O
,	O
e.	O
george	O
,	O
and	O
r.	O
mc-	O
culloch	O
(	O
2001	O
)	O
.	O
the	O
practical	O
imple-	O
mentation	O
of	O
bayesian	O
model	O
se-	O
lection	O
.	O
model	B
selection	I
.	O
ims	O
lec-	O
ture	O
notes	O
.	O
chipman	O
,	O
h.	O
,	O
e.	O
george	O
,	O
and	O
r.	O
mc-	O
culloch	O
(	O
2006	O
)	O
.	O
bayesian	O
ensemble	B
learning	I
.	O
in	O
nips	O
.	O
chipman	O
,	O
h.	O
,	O
e.	O
george	O
,	O
and	O
r.	O
mc-	O
culloch	O
(	O
2010	O
)	O
.	O
bart	O
:	O
bayesian	O
ad-	O
ditive	O
regression	B
trees	O
.	O
ann	O
.	O
appl	O
.	O
stat	O
.	O
4	O
(	O
1	O
)	O
,	O
266–298	O
.	O
choi	O
,	O
m.	O
,	O
v.	O
tan	O
,	O
a.	O
anandkumar	O
,	O
and	O
a.	O
willsky	O
(	O
2011	O
)	O
.	O
learning	B
latent	O
tree	B
graphical	O
models	O
.	O
j.	O
of	O
ma-	O
chine	O
learning	B
research	O
.	O
(	O
2011	O
)	O
.	O
j.	O
trees	O
and	O
be-	O
choi	O
,	O
m.	O
exploiting	O
and	O
improving	O
yond	O
:	O
tree-structured	O
graphical	O
models	O
.	O
ph.d.	O
thesis	O
,	O
mit	O
.	O
choset	O
,	O
h.	O
and	O
k.	O
nagatani	O
(	O
2001	O
)	O
.	O
topological	O
simultaneous	O
localiza-	O
tion	O
and	O
mapping	O
(	O
slam	O
)	O
:	O
toward	O
exact	O
localization	O
without	O
explicit	O
localization	O
.	O
ieee	O
trans	O
.	O
robotics	O
and	O
automation	O
17	O
(	O
2	O
)	O
.	O
chow	O
,	O
c.	O
k.	O
and	O
c.	O
n.	O
liu	O
(	O
1968	O
)	O
.	O
approximating	O
discrete	B
probabil-	O
ity	O
distributions	O
with	O
dependence	O
trees	O
.	O
ieee	O
trans	O
.	O
on	O
info	O
.	O
theory	O
14	O
,	O
462–67	O
.	O
christensen	O
,	O
o.	O
,	O
g.	O
roberts	O
,	O
and	O
m.	O
skã˝uld	O
(	O
2006	O
)	O
.	O
robust	B
markov	O
chain	O
monte	O
carlo	O
methods	O
for	O
spatial	O
generalized	O
linear	O
mixed	O
models	O
.	O
j.	O
of	O
computational	O
and	O
graphical	O
statistics	O
15	O
,	O
1–17	O
.	O
chung	O
,	O
f.	O
(	O
1997	O
)	O
.	O
spectral	O
graph	O
the-	O
ory	O
.	O
ams	O
.	O
cimiano	O
,	O
p.	O
,	O
a.	O
schultz	O
,	O
s.	O
sizov	O
,	O
p.	O
sorg	O
,	O
and	O
s.	O
staab	O
(	O
2009	O
)	O
.	O
ex-	O
plicit	O
versus	O
latent	B
concept	O
models	O
for	O
cross-language	O
information	O
re-	O
trieval	O
.	O
in	O
intl	O
.	O
joint	O
conf	O
.	O
on	O
ai	O
.	O
cipra	O
,	O
b	O
.	O
(	O
2000	O
)	O
.	O
the	O
ising	O
model	O
is	O
np-complete	O
.	O
siam	O
news	O
33	O
(	O
6	O
)	O
.	O
ciresan	O
,	O
d.	O
c.	O
,	O
u.	O
meier	O
,	O
l.	O
m.	O
gambardella	O
,	O
and	O
j.	O
schmidhuber	O
(	O
2010	O
)	O
.	O
deep	B
big	O
simple	O
neural	O
nets	O
for	O
handwritten	O
digit	O
recognition	O
.	O
neural	O
computation	O
22	O
(	O
12	O
)	O
,	O
3207–	O
3220.	O
clarke	O
,	O
b	O
.	O
(	O
2003	O
)	O
.	O
bayes	O
model	O
av-	O
eraging	O
and	O
stacking	B
when	O
model	O
approximation	O
error	O
can	O
not	O
be	O
ig-	O
nored	O
.	O
j.	O
of	O
machine	B
learning	I
re-	O
search	O
,	O
683–712	O
.	O
cleveland	O
,	O
w.	O
and	O
s.	O
devlin	O
(	O
1988	O
)	O
.	O
locally-weighted	O
regression	B
:	O
an	O
approach	O
to	O
regression	B
analysis	O
by	O
local	O
ﬁtting	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
as-	O
soc	O
.	O
83	O
(	O
403	O
)	O
,	O
596–610	O
.	O
collins	O
,	O
m.	O
(	O
2002	O
)	O
.	O
discrimina-	O
tive	O
training	O
methods	O
for	O
hidden	B
markov	O
models	O
:	O
theory	O
and	O
exper-	O
iments	O
with	O
perceptron	B
algorithms	O
.	O
in	O
emnlp	O
.	O
collins	O
,	O
m.	O
,	O
s.	O
dasgupta	O
,	O
and	O
r.	O
e.	O
schapire	O
(	O
2002	O
)	O
.	O
a	O
generalization	B
of	O
principal	B
components	I
analysis	I
to	O
the	O
exponential	B
family	I
.	O
in	O
nips-	O
14.	O
collins	O
,	O
m.	O
and	O
n.	O
duffy	O
(	O
2002	O
)	O
.	O
con-	O
lan-	O
volution	O
kernels	O
for	O
natural	O
guage	O
.	O
in	O
nips	O
.	O
collobert	O
,	O
r.	O
and	O
j.	O
weston	O
(	O
2008	O
)	O
.	O
a	O
uniﬁed	O
architecture	O
for	O
natural	O
language	O
processing	O
:	O
deep	B
neural	O
networks	O
with	O
multitask	O
learning	B
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
combettes	O
,	O
p.	O
and	O
v.	O
wajs	O
(	O
2005	O
)	O
.	O
sig-	O
nal	O
recovery	O
by	O
proximal	O
forward-	O
backward	O
splitting	O
.	O
siam	O
j.	O
multi-	O
scale	O
model	O
.	O
simul	O
.	O
4	O
(	O
4	O
)	O
,	O
1168–1200	O
.	O
cook	O
,	O
j	O
.	O
(	O
2005	O
)	O
.	O
exact	O
calculation	O
of	O
beta	O
inequalities	O
.	O
technical	O
re-	O
port	O
,	O
m.	O
d.	O
anderson	O
cancer	O
cen-	O
ter	O
,	O
dept	O
.	O
biostatistics	O
.	O
cooper	O
,	O
g.	O
and	O
e.	O
herskovits	O
(	O
1992	O
)	O
.	O
a	O
bayesian	O
method	O
for	O
the	O
induc-	O
tion	O
of	O
probabilistic	O
networks	O
from	O
data	O
.	O
machine	B
learning	I
9	O
,	O
309–347	O
.	O
cooper	O
,	O
g.	O
and	O
c.	O
yoo	O
(	O
1999	O
)	O
.	O
causal	O
discovery	O
from	O
a	O
mixture	O
of	O
exper-	O
imental	O
and	O
observational	O
data	O
.	O
in	O
uai	O
.	O
cover	O
,	O
t.	O
and	O
p.	O
hart	O
(	O
1967	O
)	O
.	O
near-	O
est	O
neighbor	O
pattern	B
classiﬁcation	O
.	O
ieee	O
trans	O
.	O
inform	O
.	O
theory	O
13	O
(	O
1	O
)	O
,	O
21–	O
27.	O
cover	O
,	O
t.	O
m.	O
and	O
j.	O
a.	O
thomas	O
(	O
1991	O
)	O
.	O
elements	O
of	O
information	B
the-	O
ory	O
.	O
john	O
wiley	O
.	O
cover	O
,	O
t.	O
m.	O
and	O
j.	O
a.	O
thomas	O
information	B
(	O
2006	O
)	O
.	O
theory	O
.	O
john	O
wiley	O
.	O
2nd	O
edition	O
.	O
elements	O
of	O
clarke	O
,	O
b.	O
,	O
e.	O
fokoue	O
,	O
and	O
h.	O
h.	O
zhang	O
(	O
2009	O
)	O
.	O
principles	O
and	O
theory	O
for	O
data	O
mining	O
and	O
machine	O
learn-	O
ing	O
.	O
springer	O
.	O
cowles	O
,	O
m.	O
and	O
b.	O
carlin	O
(	O
1996	O
)	O
.	O
markov	O
chain	O
monte	O
carlo	O
conver-	O
gence	O
diagnostics	O
:	O
a	O
comparative	O
review	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
91	O
,	O
883–904	O
.	O
bibliography	O
1021	O
crisan	O
,	O
d.	O
,	O
p.	O
d.	O
moral	O
,	O
and	O
t.	O
lyons	O
(	O
1999	O
)	O
.	O
discrete	B
ﬁltering	O
using	O
branching	O
and	O
interacting	O
particle	O
systems	O
.	O
markov	O
processes	O
and	O
re-	O
lated	O
fields	O
5	O
(	O
3	O
)	O
,	O
293–318	O
.	O
dawid	O
,	O
a.	O
p.	O
and	O
s.	O
l.	O
lauritzen	O
(	O
1993	O
)	O
.	O
hyper-markov	O
laws	O
in	O
the	O
statistical	O
analysis	O
of	O
decompos-	O
able	O
graphical	O
models	O
.	O
the	O
annals	O
of	O
statistics	O
3	O
,	O
1272–1317	O
.	O
dempster	O
,	O
a.	O
p.	O
,	O
n.	O
m.	O
laird	O
,	O
and	O
d.	O
b.	O
rubin	O
(	O
1977	O
)	O
.	O
maximum	O
likelihood	O
from	O
incomplete	O
data	O
via	O
the	O
em	O
algorithm	O
.	O
j.	O
of	O
the	O
royal	O
statistical	O
society	O
,	O
series	O
b	O
34	O
,	O
1–38	O
.	O
cui	O
,	O
y.	O
,	O
x.	O
z.	O
fern	O
,	O
and	O
j.	O
g.	O
dy	O
(	O
2010	O
)	O
.	O
learning	B
multiple	O
nonre-	O
dundant	O
clusterings	O
.	O
acm	O
transac-	O
tions	O
on	O
knowledge	B
discovery	I
from	O
data	O
4	O
(	O
3	O
)	O
.	O
de	O
freitas	O
,	O
n.	O
,	O
r.	O
dearden	O
,	O
f.	O
hut-	O
ter	O
,	O
r.	O
morales-menendez	O
,	O
j.	O
mutch	O
,	O
and	O
d.	O
poole	O
(	O
2004	O
)	O
.	O
diagnosis	O
by	O
a	O
waiter	O
and	O
a	O
mars	O
explorer	O
.	O
proc	O
.	O
ieee	O
92	O
(	O
3	O
)	O
.	O
cukier	O
,	O
k.	O
(	O
2010	O
,	O
february	O
)	O
.	O
data	O
,	O
data	O
everywhere	O
.	O
dagum	O
,	O
p.	O
and	O
m.	O
luby	O
(	O
1993	O
)	O
.	O
ap-	O
proximating	O
probabilistic	B
inference	I
in	O
bayesian	O
belief	B
networks	I
is	O
np-	O
hard	O
.	O
artiﬁcial	O
intelligence	O
60	O
,	O
141–	O
153.	O
dahl	O
,	O
j.	O
,	O
l.	O
vandenberghe	O
,	O
and	O
v.	O
roy-	O
chowdhury	O
(	O
2008	O
,	O
august	O
)	O
.	O
co-	O
variance	B
selection	O
for	O
non-chordal	O
graphs	O
via	O
chordal	B
embedding	O
.	O
optimization	B
methods	O
and	O
soft-	O
ware	O
23	O
(	O
4	O
)	O
,	O
501–502	O
.	O
dahlhaus	O
,	O
r.	O
and	O
m.	O
eichler	O
(	O
2000	O
)	O
.	O
causality	B
and	O
graphical	O
models	O
for	O
time	O
series	O
.	O
in	O
p.	O
green	O
,	O
n.	O
hjort	O
,	O
and	O
s.	O
richardson	O
(	O
eds	O
.	O
)	O
,	O
highly	O
structured	O
stochastic	O
systems	O
.	O
ox-	O
ford	O
university	O
press	O
.	O
dallal	O
,	O
s.	O
and	O
w.	O
hall	O
(	O
1983	O
)	O
.	O
approxi-	O
mating	O
priors	O
by	O
mixtures	O
of	O
natu-	O
ral	O
conjugate	B
priors	I
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
45	O
,	O
278–286	O
.	O
darwiche	O
,	O
a	O
.	O
(	O
2009	O
)	O
.	O
modeling	O
and	O
reasoning	O
with	O
bayesian	O
networks	O
.	O
cambridge	O
.	O
de	O
freitas	O
,	O
n.	O
,	O
m.	O
niranjan	O
,	O
and	O
a.	O
gee	O
(	O
2000	O
)	O
.	O
hierarchical	O
bayesian	O
mod-	O
els	O
for	O
regularisation	O
in	O
sequential	B
learning	O
.	O
neural	O
computation	O
12	O
(	O
4	O
)	O
,	O
955–993	O
.	O
dechter	O
,	O
r.	O
(	O
1996	O
)	O
.	O
bucket	B
elimination	I
:	O
a	O
unifying	O
framework	O
for	O
proba-	O
bilistic	O
inference	B
.	O
in	O
uai	O
.	O
dechter	O
,	O
r.	O
(	O
2003	O
)	O
.	O
constraint	O
process-	O
ing	O
.	O
morgan	O
kaufmann	O
.	O
decoste	O
,	O
d.	O
and	O
b.	O
schoelkopf	O
(	O
2002	O
)	O
.	O
training	O
invariant	O
support	B
vector	I
machines	I
.	O
machine	O
learnng	O
41	O
,	O
161–	O
190.	O
deerwester	O
,	O
s.	O
,	O
s.	O
dumais	O
,	O
g.	O
fur-	O
nas	O
,	O
t.	O
landauer	O
,	O
and	O
r.	O
harshman	O
(	O
1990	O
)	O
.	O
indexing	O
by	O
latent	B
semantic	I
analysis	I
.	O
j.	O
of	O
the	O
american	O
society	O
for	O
information	B
science	O
41	O
(	O
6	O
)	O
,	O
391–	O
407.	O
degroot	O
,	O
m.	O
(	O
1970	O
)	O
.	O
optimal	O
statistical	O
decisions	O
.	O
mcgraw-hill	O
.	O
deisenroth	O
,	O
m.	O
,	O
c.	O
rasmussen	O
,	O
and	O
j.	O
peters	O
(	O
2009	O
)	O
.	O
gaussian	O
process	O
dynamic	B
programming	I
.	O
neurocom-	O
puting	O
72	O
(	O
7	O
)	O
,	O
1508–1524	O
.	O
daume	O
,	O
h.	O
(	O
2007a	O
)	O
.	O
fast	O
search	O
for	O
dirichlet	O
process	O
mixture	B
models	O
.	O
in	O
ai/statistics	O
.	O
daume	O
,	O
h.	O
(	O
2007b	O
)	O
.	O
frustratingly	O
easy	O
domain	B
adaptation	I
.	O
in	O
proc	O
.	O
the	O
as-	O
soc	O
.	O
for	O
comp	O
.	O
ling	O
.	O
dawid	O
,	O
a.	O
p.	O
(	O
1992	O
)	O
.	O
applications	O
of	O
a	O
general	O
propagation	O
algorithm	O
for	O
probabilistic	O
expert	O
systems	O
.	O
statis-	O
tics	O
and	O
computing	O
2	O
,	O
25–36	O
.	O
dawid	O
,	O
a.	O
p.	O
(	O
2002	O
)	O
.	O
inﬂuence	O
dia-	O
grams	O
for	O
causal	O
modelling	O
and	O
in-	O
ference	O
.	O
intl	O
.	O
stat	O
.	O
review	O
70	O
,	O
161–	O
189.	O
corrections	O
p437	O
.	O
dellaportas	O
,	O
p.	O
,	O
p.	O
giudici	O
,	O
and	O
g.	O
roberts	O
(	O
2003	O
)	O
.	O
bayesian	O
infer-	O
ence	O
for	O
nondecomposable	O
graphi-	O
cal	O
gaussian	O
models	O
.	O
sankhya	O
,	O
ser	O
.	O
a	O
65	O
,	O
43–55	O
.	O
dellaportas	O
,	O
p.	O
and	O
a.	O
f.	O
m.	O
smith	O
(	O
1993	O
)	O
.	O
bayesian	O
inference	B
for	O
gen-	O
eralized	O
linear	O
and	O
proportional	O
hazards	O
models	O
via	O
gibbs	O
sam-	O
pling	O
.	O
the	O
royal	O
statisti-	O
cal	O
society	O
.	O
series	O
c	O
(	O
applied	O
statis-	O
tics	O
)	O
42	O
(	O
3	O
)	O
,	O
443–459	O
.	O
j.	O
of	O
delyon	O
,	O
lavielle	O
,	O
b.	O
,	O
m.	O
and	O
e.	O
moulines	O
(	O
1999	O
)	O
.	O
convergence	O
of	O
a	O
stochastic	B
approximation	I
version	O
of	O
the	O
em	O
algorithm	O
.	O
annals	O
of	O
statistics	O
27	O
(	O
1	O
)	O
,	O
94–128	O
.	O
dawid	O
,	O
a.	O
p.	O
(	O
2010	O
)	O
.	O
beware	O
of	O
the	O
dag	O
!	O
j.	O
of	O
machine	B
learning	I
research	O
6	O
,	O
59–86	O
.	O
dempster	O
,	O
a	O
.	O
(	O
1972	O
)	O
.	O
covariance	B
selec-	O
tion	O
.	O
biometrics	O
28	O
(	O
1	O
)	O
.	O
denison	O
,	O
d.	O
,	O
c.	O
holmes	O
,	O
b.	O
mallick	O
,	O
and	O
a.	O
smith	O
(	O
2002	O
)	O
.	O
bayesian	O
methods	O
for	O
nonlinear	O
classiﬁcation	B
and	O
regression	B
.	O
wiley	O
.	O
denison	O
,	O
d.	O
,	O
b.	O
mallick	O
,	O
and	O
a.	O
smith	O
(	O
1998	O
)	O
.	O
a	O
bayesian	O
cart	O
algorithm	O
.	O
biometrika	O
85	O
,	O
363–377	O
.	O
desjardins	O
,	O
g.	O
and	O
y.	O
bengio	O
(	O
2008	O
)	O
.	O
empirical	O
evaluation	O
of	O
convolu-	O
tional	O
rbms	O
for	O
vision	O
.	O
technical	O
report	O
1327	O
,	O
u.	O
montreal	O
.	O
dey	O
,	O
d.	O
,	O
s.	O
ghosh	O
,	O
and	O
b.	O
mallick	O
(	O
eds	O
.	O
)	O
(	O
2000	O
)	O
.	O
generalized	B
linear	I
models	I
:	O
a	O
bayesian	O
perspective	O
.	O
chapman	O
&	O
hall/crc	O
biostatistics	O
series	O
.	O
diaconis	O
,	O
p.	O
,	O
s.	O
holmes	O
,	O
and	O
r.	O
mont-	O
gomery	O
(	O
2007	O
)	O
.	O
dynamical	O
bias	B
in	O
the	O
coin	O
toss	O
.	O
siam	O
review	O
49	O
(	O
2	O
)	O
,	O
211–235	O
.	O
diaconis	O
,	O
p.	O
and	O
d.	O
ylvisaker	O
(	O
1985	O
)	O
.	O
in	O
quantifying	O
prior	O
opinion	O
.	O
bayesian	O
statistics	O
2.	O
dietterich	O
,	O
t.	O
g.	O
and	O
g.	O
bakiri	O
(	O
1995	O
)	O
.	O
solving	O
multiclass	O
learning	O
prob-	O
lems	O
via	O
ecocs	O
.	O
j.	O
of	O
ai	O
research	O
2	O
,	O
263–286	O
.	O
diggle	O
,	O
p.	O
and	O
p.	O
ribeiro	O
(	O
2007	O
)	O
.	O
model-	O
based	O
geostatistics	O
.	O
springer	O
.	O
ding	O
,	O
y.	O
and	O
r.	O
harrison	O
(	O
2010	O
)	O
.	O
a	O
sparse	B
multinomial	O
probit	B
model	O
for	O
classiﬁcation	B
.	O
pattern	B
analysis	O
and	O
applications	O
,	O
1–9	O
.	O
dobra	O
,	O
a	O
.	O
(	O
2009	O
)	O
.	O
dependency	O
net-	O
works	O
for	O
genome-wide	O
data	O
.	O
tech-	O
nical	O
report	O
,	O
u.	O
washington	O
.	O
dobra	O
,	O
a.	O
and	O
h.	O
massam	O
(	O
2010	O
)	O
.	O
the	O
mode	B
oriented	O
stochastic	B
search	I
(	O
moss	O
)	O
algorithm	O
for	O
log-linear	B
models	O
with	O
conjugate	B
priors	I
.	O
sta-	O
tistical	O
methodology	O
7	O
,	O
240–253	O
.	O
domingos	O
,	O
p.	O
and	O
d.	O
lowd	O
(	O
2009	O
)	O
.	O
markov	O
logic	O
:	O
an	O
interface	O
layer	O
for	O
ai	O
.	O
morgan	O
&	O
claypool	O
.	O
domingos	O
,	O
p.	O
and	O
m.	O
pazzani	O
(	O
1997	O
)	O
.	O
on	O
the	O
optimality	O
of	O
the	O
simple	O
bayesian	O
classiﬁer	O
under	O
zero-one	O
loss	B
.	O
machine	B
learning	I
29	O
,	O
103–	O
130	O
.	O
1022	O
bibliography	O
domke	O
,	O
j.	O
,	O
a.	O
karapurkar	O
,	O
and	O
y.	O
aloi-	O
monos	O
(	O
2008	O
)	O
.	O
who	O
killed	O
the	O
di-	O
rected	O
model	O
?	O
in	O
cvpr	O
.	O
duda	O
,	O
r.	O
o.	O
,	O
p.	O
e.	O
hart	O
,	O
and	O
d.	O
g.	O
stork	O
(	O
2001	O
)	O
.	O
pattern	B
classiﬁcation	O
.	O
wiley	O
interscience	O
.	O
2nd	O
edition	O
.	O
doucet	O
,	O
a.	O
,	O
n.	O
de	O
freitas	O
,	O
and	O
n.	O
j.	O
gor-	O
don	O
(	O
2001	O
)	O
.	O
sequential	B
monte	O
carlo	O
methods	O
in	O
practice	O
.	O
springer	O
ver-	O
lag	B
.	O
doucet	O
,	O
a.	O
,	O
n.	O
gordon	O
,	O
and	O
v.	O
krish-	O
namurthy	O
(	O
2001	O
)	O
.	O
particle	O
filters	O
for	O
state	B
estimation	I
of	O
jump	O
markov	O
linear	O
systems	O
.	O
ieee	O
trans	O
.	O
on	O
sig-	O
nal	O
processing	O
49	O
(	O
3	O
)	O
,	O
613–624	O
.	O
dow	O
,	O
j.	O
and	O
j.	O
endersby	O
(	O
2004	O
)	O
.	O
multi-	O
nomial	O
probit	B
and	O
multinomial	B
logit	O
:	O
a	O
comparison	O
of	O
choice	O
mod-	O
els	O
for	O
voting	O
research	O
.	O
electoral	O
studies	O
23	O
(	O
1	O
)	O
,	O
107–122	O
.	O
drineas	O
,	O
p.	O
,	O
a.	O
frieze	O
,	O
r.	O
kannan	O
,	O
s.	O
vempala	O
,	O
and	O
v.	O
vinay	O
(	O
2004	O
)	O
.	O
clustering	B
large	O
graphs	O
via	O
the	O
sin-	O
gular	O
value	O
decomposition	O
.	O
ma-	O
chine	O
learning	B
56	O
,	O
9–33	O
.	O
drugowitsch	O
,	O
j	O
.	O
(	O
2008	O
)	O
.	O
bayesian	O
lin-	O
ear	O
regression	B
.	O
technical	O
report	O
,	O
u.	O
rochester	O
.	O
druilhet	O
,	O
p.	O
and	O
j.-m.	O
marin	O
(	O
2007	O
)	O
.	O
in-	O
variant	O
hpd	O
credible	O
sets	O
and	O
map	O
estimators	O
.	O
bayesian	O
analysis	O
2	O
(	O
4	O
)	O
,	O
681–692	O
.	O
duane	O
,	O
s.	O
,	O
a.	O
kennedy	O
,	O
b.	O
pendle-	O
ton	O
,	O
and	O
d.	O
roweth	O
(	O
1987	O
)	O
.	O
hy-	O
brid	O
monte	O
carlo	O
.	O
physics	O
letters	O
b	O
195	O
(	O
2	O
)	O
,	O
216–222	O
.	O
duchi	O
,	O
j.	O
,	O
s.	O
gould	O
,	O
and	O
d.	O
koller	O
(	O
2008	O
)	O
.	O
projected	O
subgradient	O
methods	O
for	O
learning	B
sparse	O
gaus-	O
sians	O
.	O
in	O
uai	O
.	O
duchi	O
,	O
for	O
online	B
learning	I
j.	O
,	O
e.	O
hazan	O
,	O
and	O
y.	O
singer	O
(	O
2010	O
)	O
.	O
adaptive	O
subgradient	O
meth-	O
and	O
ods	O
stochastic	B
optimization	I
.	O
in	O
proc	O
.	O
of	O
the	O
workshop	O
on	O
computational	B
learning	I
theory	I
.	O
duchi	O
,	O
j.	O
,	O
s.	O
shalev-shwartz	O
,	O
y.	O
singer	O
,	O
and	O
t.	O
chandra	O
(	O
2008	O
)	O
.	O
efficient	O
for	O
projections	O
onto	O
the	O
l1-ball	O
learning	B
in	O
high	O
dimensions	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
duchi	O
,	O
j.	O
and	O
y.	O
singer	O
(	O
2009	O
)	O
.	O
boost-	O
ing	O
with	O
structural	O
sparsity	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
duchi	O
,	O
j.	O
,	O
d.	O
tarlow	O
,	O
g.	O
elidan	O
,	O
and	O
d.	O
koller	O
(	O
2007	O
)	O
.	O
using	O
combi-	O
natorial	O
optimization	B
within	O
max-	O
product	O
belief	O
propagation	O
.	O
in	O
nips	O
.	O
dumais	O
,	O
s.	O
and	O
t.	O
landauer	O
(	O
1997	O
)	O
.	O
a	O
solution	O
to	O
plato	O
’	O
s	O
problem	O
:	O
the	O
latent	B
semantic	I
analysis	I
theory	O
of	O
acquisition	O
,	O
induction	B
and	O
repre-	O
sentation	O
of	O
knowledge	O
.	O
psycholog-	O
ical	O
review	O
104	O
,	O
211–240	O
.	O
elad	O
,	O
m.	O
and	O
i.	O
yavnch	O
(	O
2009	O
)	O
.	O
a	O
plu-	O
rality	O
of	O
sparse	B
representations	O
is	O
better	O
than	O
the	O
sparsest	O
one	O
alone	O
.	O
ieee	O
trans	O
.	O
on	O
info	O
.	O
theory	O
55	O
(	O
10	O
)	O
,	O
4701–4714	O
.	O
elidan	O
,	O
g.	O
and	O
s.	O
gould	O
(	O
2008	O
)	O
.	O
learn-	O
ing	O
bounded	O
treewidth	B
bayesian	O
networks	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
,	O
2699–2731	O
.	O
dunson	O
,	O
d.	O
,	O
j.	O
palomo	O
,	O
and	O
k.	O
bollen	O
(	O
2005	O
)	O
.	O
bayesian	O
structural	O
equa-	O
tion	O
modeling	O
.	O
technical	O
report	O
2005-5	O
,	O
samsi	O
.	O
elidan	O
,	O
g.	O
,	O
n.	O
lotner	O
,	O
n.	O
friedman	O
,	O
and	O
d.	O
koller	O
(	O
2000	O
)	O
.	O
discovering	O
hid-	O
den	O
variables	O
:	O
a	O
structure-based	O
approach	O
.	O
in	O
nips	O
.	O
durbin	O
,	O
j.	O
and	O
s.	O
j.	O
koopman	O
(	O
2001	O
)	O
.	O
time	O
series	O
analysis	O
by	O
state	B
space	I
methods	O
.	O
oxford	O
university	O
press	O
.	O
durbin	O
,	O
r.	O
,	O
s.	O
eddy	O
,	O
a.	O
krogh	O
,	O
and	O
g.	O
mitchison	O
(	O
1998	O
)	O
.	O
biological	O
se-	O
quence	O
analysis	O
:	O
probabilistic	O
mod-	O
els	O
of	O
proteins	O
and	O
nucleic	O
acids	O
.	O
cambridge	O
:	O
cambridge	O
university	O
press	O
.	O
earl	O
,	O
d.	O
and	O
m.	O
deem	O
(	O
2005	O
)	O
.	O
paral-	O
lel	O
tempering	O
:	O
theory	O
,	O
applications	O
,	O
and	O
new	O
perspectives	O
.	O
phys	O
.	O
chem	O
.	O
chem	O
.	O
phys	O
.	O
7	O
,	O
3910.	O
eaton	O
,	O
d.	O
and	O
k.	O
murphy	O
(	O
2007	O
)	O
.	O
exact	O
bayesian	O
structure	B
learning	I
from	O
uncertain	O
interventions	B
.	O
in	O
ai/s-	O
tatistics	O
.	O
elidan	O
,	O
g.	O
,	O
i.	O
mcgraw	O
,	O
and	O
d.	O
koller	O
residual	O
belief	O
propa-	O
(	O
2006	O
)	O
.	O
gation	O
:	O
informed	O
scheduling	O
for	O
asynchronous	O
message	O
passing	O
.	O
in	O
uai	O
.	O
elkan	O
,	O
c.	O
(	O
2003	O
)	O
.	O
using	O
the	O
triangle	O
in-	O
in	O
equality	O
to	O
accelerate	O
k-means	O
.	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
elkan	O
,	O
c.	O
(	O
2005	O
)	O
.	O
deriving	O
tf-idf	O
as	O
a	O
fisher	O
kernel	B
.	O
in	O
proc	O
.	O
intl	O
.	O
symp	O
.	O
on	O
string	O
processing	O
and	O
informa-	O
tion	O
retrieval	O
(	O
spire	O
)	O
,	O
pp	O
.	O
296–301	O
.	O
elkan	O
,	O
c.	O
(	O
2006	O
)	O
.	O
clustering	B
docu-	O
ments	O
with	O
an	O
exponential	O
fmaily	O
the	O
dirichlet	O
approximation	O
of	O
compoind	O
multinomial	B
model	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
edakunni	O
,	O
n.	O
,	O
s.	O
schaal	O
,	O
and	O
s.	O
vi-	O
jayakumar	O
(	O
2010	O
)	O
.	O
probabilistic	O
in-	O
cremental	O
locally	O
weighted	O
learn-	O
ing	O
using	O
randomly	O
varying	O
coeffi-	O
cient	O
model	O
.	O
technical	O
report	O
,	O
usc	O
.	O
ellis	O
,	O
b.	O
and	O
w.	O
h.	O
wong	O
(	O
2008	O
)	O
.	O
learn-	O
ing	O
causal	O
bayesian	O
network	O
struc-	O
tures	O
from	O
experimental	O
data	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
103	O
(	O
482	O
)	O
,	O
778–	O
789.	O
edwards	O
,	O
d.	O
,	O
g.	O
de	O
abreu	O
,	O
and	O
r.	O
labouriau	O
(	O
2010	O
)	O
.	O
selecting	O
high-	O
dimensional	O
mixed	O
graphical	O
mod-	O
els	O
using	O
minimal	B
aic	O
or	O
bic	O
forests	O
.	O
bmc	O
bioinformatics	O
11	O
(	O
18	O
)	O
.	O
efron	O
,	O
b	O
.	O
(	O
1986	O
)	O
.	O
why	O
isn	O
’	O
t	O
everyone	O
a	O
bayesian	O
?	O
the	O
american	O
statisti-	O
cian	O
40	O
(	O
1	O
)	O
.	O
efron	O
,	O
b	O
.	O
(	O
2010	O
)	O
.	O
large-scale	O
infer-	O
ence	O
:	O
empirical	O
bayes	O
methods	O
for	O
estimation	O
,	O
testing	O
,	O
and	O
prediction	O
.	O
cambridge	O
.	O
efron	O
,	O
b.	O
,	O
i.	O
johnstone	O
,	O
t.	O
hastie	O
,	O
and	O
r.	O
tibshirani	O
(	O
2004	O
)	O
.	O
least	O
angle	O
re-	O
gression	O
.	O
annals	O
of	O
statistics	O
32	O
(	O
2	O
)	O
,	O
407–499	O
.	O
efron	O
,	O
b.	O
and	O
c.	O
morris	O
(	O
1975	O
)	O
.	O
data	O
analysis	O
using	O
stein	O
’	O
s	O
estimator	B
and	O
its	O
generalizations	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
70	O
(	O
350	O
)	O
,	O
311–319	O
.	O
engel	O
,	O
y.	O
,	O
s.	O
mannor	O
,	O
and	O
r.	O
meir	O
reinforcement	B
learning	I
in	O
intl	O
.	O
(	O
2005	O
)	O
.	O
with	O
gaussian	O
processes	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
erhan	O
,	O
d.	O
,	O
y.	O
bengio	O
,	O
a.	O
courville	O
,	O
p.-a	O
.	O
manzagol	O
,	O
p.	O
vincent	O
,	O
and	O
s.	O
ben-	O
gio	O
(	O
2010	O
)	O
.	O
why	O
does	O
unsupervised	O
pre-training	O
help	O
deep	B
learning	I
?	O
j.	O
of	O
machine	B
learning	I
research	O
11	O
,	O
625–660	O
.	O
erosheva	O
,	O
s.	O
e.	O
,	O
fienberg	O
,	O
joutard	O
(	O
2007	O
)	O
.	O
and	O
c.	O
describing	O
disability	O
through	O
individual-level	O
mixture	B
models	O
for	O
multivariate	O
bi-	O
nary	O
data	O
.	O
annals	O
of	O
applied	O
statis-	O
tics	O
.	O
erosheva	O
,	O
e.	O
,	O
s.	O
fienberg	O
,	O
and	O
j.	O
laf-	O
ferty	O
(	O
2004	O
)	O
.	O
mixed-membership	O
models	O
of	O
scientiﬁc	O
publications	O
.	O
proc	O
.	O
of	O
the	O
national	O
academy	O
of	O
science	O
,	O
usa	O
101	O
,	O
5220–2227	O
.	O
bibliography	O
1023	O
escobar	O
,	O
m.	O
d.	O
and	O
m.	O
west	O
(	O
1995	O
)	O
.	O
bayesian	O
density	B
estimation	I
and	O
inference	B
using	O
mixtures	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
90	O
(	O
430	O
)	O
,	O
577–588	O
.	O
ewens	O
,	O
w.	O
(	O
1990	O
)	O
.	O
population	O
genet-	O
ics	O
theory	O
-	O
the	O
past	O
and	O
the	O
fu-	O
ture	O
.	O
in	O
s.lessard	O
(	O
ed	O
.	O
)	O
,	O
mathemeti-	O
cal	O
and	O
statistica	O
developments	O
of	O
evolutionary	O
theory	O
,	O
pp	O
.	O
177–227	O
.	O
reidel	O
.	O
fan	O
,	O
j.	O
and	O
r.	O
z.	O
li	O
(	O
2001	O
)	O
.	O
variable	O
se-	O
lection	O
via	O
non-concave	O
penalized	O
likelihood	O
and	O
its	O
oracle	O
properties	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
96	O
(	O
456	O
)	O
,	O
1348–1360	O
.	O
fearnhead	O
,	O
p.	O
(	O
2004	O
)	O
.	O
exact	O
bayesian	O
curve	O
ﬁtting	O
and	O
signal	O
segmen-	O
tation	O
.	O
ieee	O
trans	O
.	O
signal	O
process-	O
ing	O
53	O
,	O
2160–2166	O
.	O
felzenszwalb	O
,	O
p.	O
and	O
d.	O
huttenlocher	O
(	O
2006	O
)	O
.	O
efficient	O
belief	O
propagation	O
for	O
early	O
vision	O
.	O
intl	O
.	O
j.	O
computer	O
vision	O
70	O
(	O
1	O
)	O
,	O
41–54	O
.	O
ferrucci	O
,	O
d.	O
,	O
e.	O
brown	O
,	O
j.	O
chu-carroll	O
,	O
j.	O
fan	O
,	O
d.	O
gondek	O
,	O
a.	O
kalyanpur	O
,	O
a.	O
lally	O
,	O
j.	O
w.	O
murdock	O
,	O
e.	O
n.	O
amd	O
j.	O
prager	O
,	O
n.	O
schlaefter	O
,	O
and	O
c.	O
welty	O
(	O
2010	O
)	O
.	O
building	O
wat-	O
son	O
:	O
an	O
overview	O
of	O
the	O
deepqa	O
project	O
.	O
ai	O
magazine	O
,	O
59–79	O
.	O
fienberg	O
,	O
s.	O
(	O
1970	O
)	O
.	O
an	O
iterative	O
pro-	O
cedure	O
for	O
estimation	O
in	O
contin-	O
gency	O
tables	O
.	O
annals	O
of	O
mathemat-	O
ical	O
statistics	O
41	O
(	O
3	O
)	O
,	O
907â	O
˘a	O
¸s917	O
.	O
figueiredo	O
,	O
m.	O
(	O
2003	O
)	O
.	O
adaptive	O
sparseness	O
for	O
supervised	O
learn-	O
ing	O
.	O
ieee	O
trans	O
.	O
on	O
pattern	B
anal-	O
ysis	O
and	O
machine	O
intelligence	O
25	O
(	O
9	O
)	O
,	O
1150–1159	O
.	O
figueiredo	O
,	O
m.	O
,	O
r.	O
nowak	O
,	O
and	O
s.	O
wright	O
(	O
2007	O
)	O
.	O
gradient	O
pro-	O
jection	O
for	O
sparse	B
reconstruction	O
:	O
application	O
to	O
compressed	B
sensing	I
and	O
other	O
inverse	B
problems	I
.	O
ieee	O
.	O
j.	O
on	O
selected	O
topics	O
in	O
signal	O
pro-	O
cessing	O
.	O
figueiredo	O
,	O
m.	O
a.	O
t.	O
and	O
a.	O
k.	O
jain	O
(	O
2002	O
)	O
.	O
unsupervised	B
learning	I
of	O
ﬁ-	O
nite	O
mixture	B
models	O
.	O
ieee	O
trans	O
.	O
on	O
pattern	B
analysis	O
and	O
machine	O
intel-	O
ligence	O
24	O
(	O
3	O
)	O
,	O
381–396	O
.	O
matlab	O
code	O
at	O
http	O
:	O
//www.lx.it.pt/	O
mtf/mixture-	O
code.zip	O
.	O
fine	O
,	O
s.	O
,	O
y.	O
singer	O
,	O
and	O
n.	O
tishby	O
(	O
1998	O
)	O
.	O
the	O
hierarchical	O
hidden	O
markov	O
model	O
:	O
analysis	O
and	O
appli-	O
cations	O
.	O
machine	B
learning	I
32	O
,	O
41.	O
finkel	O
,	O
j.	O
and	O
c.	O
manning	O
(	O
2009	O
)	O
.	O
hier-	O
archical	O
bayesian	O
domain	O
adapta-	O
tion	O
.	O
in	O
proc	O
.	O
naacl	O
,	O
pp	O
.	O
602–610	O
.	O
fischer	O
,	O
b.	O
and	O
j.	O
schumann	O
(	O
2003	O
)	O
.	O
autobayes	O
:	O
a	O
system	O
for	O
generating	O
data	O
analysis	O
programs	O
from	O
sta-	O
tistical	O
models	O
.	O
j.	O
functional	O
pro-	O
gramming	O
13	O
(	O
3	O
)	O
,	O
483–508	O
.	O
fishelson	O
,	O
m.	O
and	O
d.	O
geiger	O
(	O
2002	O
)	O
.	O
exact	O
genetic	O
linkage	O
computations	O
for	O
general	O
pedigrees	O
.	O
bmc	O
bioin-	O
formatics	O
18.	O
fletcher	O
,	O
r.	O
(	O
2005	O
)	O
.	O
on	O
the	O
barzilai-	O
applied	O
opti-	O
borwein	O
method	O
.	O
mization	O
96	O
,	O
235–256	O
.	O
fokoue	O
,	O
e.	O
(	O
2005	O
)	O
.	O
mixtures	O
of	O
factor	B
analyzers	O
:	O
an	O
extension	B
with	O
co-	O
variates	O
.	O
j.	O
multivariate	O
analysis	O
95	O
,	O
370–384	O
.	O
forbes	O
,	O
j.	O
,	O
t.	O
huang	O
,	O
k.	O
kanazawa	O
,	O
and	O
s.	O
russell	O
(	O
1995	O
)	O
.	O
the	O
batmobile	O
:	O
towards	O
a	O
bayesian	O
automated	O
taxi	O
.	O
in	O
intl	O
.	O
joint	O
conf	O
.	O
on	O
ai	O
.	O
forsyth	O
,	O
d.	O
and	O
j.	O
ponce	O
(	O
2002	O
)	O
.	O
com-	O
puter	O
vision	O
:	O
a	O
modern	O
approach	O
.	O
prentice	O
hall	O
.	O
fraley	O
,	O
c.	O
and	O
a.	O
raftery	O
(	O
2002	O
)	O
.	O
model-based	B
clustering	I
,	O
discrimi-	O
nant	O
analysis	O
,	O
and	O
density	O
estima-	O
tion	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
(	O
97	O
)	O
,	O
611–631	O
.	O
fraley	O
,	O
c.	O
and	O
a.	O
raftery	O
(	O
2007	O
)	O
.	O
bayesian	O
regularization	B
for	O
normal	B
mixture	O
estimation	O
and	O
model-	O
based	O
clustering	B
.	O
j.	O
of	O
classiﬁca-	O
tion	O
24	O
,	O
155–181	O
.	O
franc	O
,	O
v.	O
,	O
a.	O
zien	O
,	O
and	O
b.	O
schoelkopf	O
(	O
2011	O
)	O
.	O
support	B
vector	I
machines	I
as	O
probabilistic	O
models	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
frank	O
,	O
i.	O
and	O
j.	O
friedman	O
(	O
1993	O
)	O
.	O
a	O
statistical	O
view	O
of	O
some	O
chemomet-	O
rics	O
regression	B
tools	O
.	O
technomet-	O
rics	O
35	O
(	O
2	O
)	O
,	O
109–135	O
.	O
fraser	O
,	O
a	O
.	O
(	O
2008	O
)	O
.	O
hidden	B
markov	O
mod-	O
els	O
and	O
dynamical	O
systems	O
.	O
siam	O
press	O
.	O
freund	O
,	O
y.	O
and	O
r.	O
r.	O
schapire	O
(	O
1996	O
)	O
.	O
experiments	O
with	O
a	O
new	O
boosting	B
algorithm	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
frey	O
,	O
b	O
.	O
(	O
1998	O
)	O
.	O
graphical	O
models	O
for	O
machine	B
learning	I
and	O
digital	O
com-	O
munication	O
.	O
mit	O
press	O
.	O
frey	O
,	O
b	O
.	O
(	O
2003	O
)	O
.	O
extending	O
factor	B
graphs	O
so	O
as	O
to	O
unify	O
directed	B
and	O
undirected	O
graphical	O
models	O
.	O
in	O
uai	O
.	O
frey	O
,	O
b.	O
and	O
d.	O
dueck	O
(	O
2007	O
,	O
febru-	O
ary	O
)	O
.	O
clustering	B
by	O
passing	O
mes-	O
sages	O
between	O
data	O
points	O
.	O
sci-	O
ence	O
315	O
,	O
972â	O
˘a	O
¸s976	O
.	O
friedman	O
,	O
j	O
.	O
(	O
1991	O
)	O
.	O
multivariate	B
adaptive	I
regression	I
splines	I
.	O
ann	O
.	O
statist	O
.	O
19	O
,	O
1–67	O
.	O
friedman	O
,	O
j	O
.	O
(	O
1997a	O
)	O
.	O
on	O
bias	B
,	O
variance	B
,	O
0-1	O
loss	B
and	O
the	O
curse	O
of	O
dimen-	O
sionality	O
.	O
j.	O
data	O
mining	O
and	O
knowl-	O
edge	O
discovery	O
1	O
,	O
55–77	O
.	O
friedman	O
,	O
j	O
.	O
(	O
2001	O
)	O
.	O
greedy	O
function	O
approximation	O
:	O
a	O
gradient	O
boost-	O
ing	O
machine	O
.	O
annals	O
of	O
statistics	O
29	O
,	O
1189–1232	O
.	O
friedman	O
,	O
j.	O
,	O
t.	O
hastie	O
,	O
and	O
r.	O
tibshi-	O
rani	O
(	O
2000	O
)	O
.	O
additive	O
logistic	B
regres-	O
sion	O
:	O
a	O
statistical	O
view	O
of	O
boosting	B
.	O
annals	O
of	O
statistics	O
28	O
(	O
2	O
)	O
,	O
337–374	O
.	O
friedman	O
,	O
j.	O
,	O
t.	O
hastie	O
,	O
and	O
r.	O
tib-	O
shirani	O
(	O
2008	O
)	O
.	O
sparse	B
inverse	O
co-	O
variance	B
estimation	O
the	O
graphical	B
lasso	I
.	O
biostatistics	O
9	O
(	O
3	O
)	O
,	O
432–441	O
.	O
friedman	O
,	O
j.	O
,	O
t.	O
hastie	O
,	O
and	O
r.	O
tibshi-	O
rani	O
(	O
2010	O
,	O
februrary	O
)	O
.	O
regulariza-	O
tion	O
paths	O
for	O
generalized	B
linear	I
models	I
via	O
coordinate	O
descent	O
.	O
j.	O
of	O
statistical	O
software	O
33	O
(	O
1	O
)	O
.	O
friedman	O
,	O
n.	O
(	O
1997b	O
)	O
.	O
learning	B
bayesian	O
networks	O
in	O
the	O
presence	O
of	O
missing	B
values	O
and	O
hidden	B
vari-	O
ables	O
.	O
in	O
uai	O
.	O
friedman	O
,	O
n.	O
,	O
d.	O
geiger	O
,	O
and	O
m.	O
gold-	O
szmidt	O
(	O
1997	O
)	O
.	O
bayesian	O
network	O
classiﬁers	O
.	O
machine	B
learning	I
j	O
.	O
29	O
,	O
131–163	O
.	O
friedman	O
,	O
n.	O
,	O
d.	O
geiger	O
,	O
and	O
n.	O
lot-	O
ner	O
(	O
2000	O
)	O
.	O
likelihood	B
computation	O
with	O
value	O
abstraction	O
.	O
in	O
uai	O
.	O
friedman	O
,	O
n.	O
and	O
d.	O
koller	O
(	O
2003	O
)	O
.	O
be-	O
ing	O
bayesian	O
about	O
network	O
struc-	O
ture	O
:	O
a	O
bayesian	O
approach	O
to	O
structure	O
discovery	O
in	O
bayesian	O
networks	O
.	O
machine	B
learning	I
50	O
,	O
95–126	O
.	O
friedman	O
,	O
n.	O
,	O
m.	O
ninion	O
,	O
i.	O
pe	O
’	O
er	O
,	O
and	O
t.	O
pupko	O
(	O
2002	O
)	O
.	O
a	O
structural	O
em	O
algorithm	O
for	O
phylogenetic	O
infer-	O
ence	O
.	O
j.	O
comp	O
.	O
bio	O
.	O
9	O
,	O
331–353	O
.	O
friedman	O
,	O
n.	O
and	O
y.	O
singer	O
(	O
1999	O
)	O
.	O
ef-	O
ﬁcient	O
bayesian	O
parameter	B
estima-	O
tion	O
in	O
large	O
discrete	O
domains	O
.	O
in	O
nips-11	O
.	O
1024	O
bibliography	O
fruhwirth-schnatter	O
,	O
s.	O
fi-	O
nite	O
mixture	B
and	O
markov	O
switching	O
models	O
.	O
springer	O
.	O
(	O
2007	O
)	O
.	O
fruhwirth-schnatter	O
,	O
s.	O
and	O
r.	O
fruh-	O
wirth	O
(	O
2010	O
)	O
.	O
data	B
augmentation	I
and	O
mcmc	O
for	O
binary	O
and	O
multi-	O
nomial	O
logit	B
models	O
.	O
in	O
t.	O
kneib	O
and	O
g.	O
tutz	O
(	O
eds	O
.	O
)	O
,	O
statistical	O
mod-	O
elling	O
and	O
regression	B
structures	O
,	O
pp	O
.	O
111–132	O
.	O
springer	O
.	O
fu	O
,	O
w.	O
(	O
1998	O
)	O
.	O
penalized	O
regressions	O
:	O
the	O
bridge	O
verus	O
the	O
lasso	B
.	O
j.	O
com-	O
putational	O
and	O
graphical	O
statistics	O
.	O
gelman	O
,	O
a.	O
,	O
j.	O
carlin	O
,	O
h.	O
stern	O
,	O
and	O
d.	O
rubin	O
(	O
2004	O
)	O
.	O
bayesian	O
data	O
analysis	O
.	O
chapman	O
and	O
hall	O
.	O
2nd	O
edition	O
.	O
gelman	O
,	O
a.	O
and	O
j.	O
hill	O
(	O
2007	O
)	O
.	O
data	O
analysis	O
using	O
regression	B
and	O
mul-	O
tilevel/	O
hierarchical	O
models	O
.	O
cam-	O
bridge	O
.	O
gelman	O
,	O
a.	O
and	O
x.-l.	O
meng	O
(	O
1998	O
)	O
.	O
simulating	O
normalizing	O
constants	O
:	O
from	O
importance	O
to	O
bridge	O
sampling	O
to	O
path	B
sampling	O
.	O
statisical	O
science	O
13	O
,	O
163–185	O
.	O
sampling	O
fukushima	O
,	O
k.	O
(	O
1975	O
)	O
.	O
cognitron	O
:	O
a	O
self-organizing	O
multilayered	O
neu-	O
ral	O
network	O
.	O
biological	O
cybernet-	O
ics	O
20	O
(	O
6	O
)	O
,	O
121–136	O
.	O
gelman	O
,	O
a.	O
and	O
t.	O
raghunathan	O
(	O
2001	O
)	O
.	O
using	O
conditional	O
distributions	O
for	O
missing-data	O
imputation	B
.	O
statistical	O
science	O
.	O
fung	O
,	O
r.	O
and	O
k.	O
chang	O
(	O
1989	O
)	O
.	O
weight-	O
ing	O
and	O
integrating	O
evidence	B
for	O
stochastic	O
simulation	O
in	O
bayesian	O
networks	O
.	O
in	O
uai	O
.	O
gelman	O
,	O
a.	O
and	O
d.	O
rubin	O
(	O
1992	O
)	O
.	O
infer-	O
ence	O
from	O
iterative	O
simulation	O
us-	O
ing	O
multiple	O
sequences	O
.	O
statistical	O
science	O
7	O
,	O
457–511	O
.	O
efficient	O
gabow	O
,	O
h.	O
,	O
z.	O
galil	O
,	O
and	O
t.	O
spencer	O
(	O
1984	O
)	O
.	O
implementation	O
of	O
graph	B
algorithms	O
using	O
contrac-	O
tion	O
.	O
in	O
ieee	O
symposium	O
on	O
the	O
foundations	O
of	O
computer	O
science	O
.	O
gales	O
,	O
m.	O
(	O
2002	O
)	O
.	O
maximum	O
like-	O
lihood	O
multiple	O
subspace	O
projec-	O
tions	O
for	O
hidden	B
markov	O
models	O
.	O
ieee	O
.	O
trans	O
.	O
on	O
speech	O
and	O
audio	O
processing	O
10	O
(	O
2	O
)	O
,	O
37–47	O
.	O
gales	O
,	O
m.	O
j.	O
f.	O
(	O
1999	O
)	O
.	O
semi-tied	O
covari-	O
ance	O
matrices	O
for	O
hidden	B
markov	O
models	O
.	O
ieee	O
trans	O
.	O
on	O
speech	O
and	O
audio	O
processing	O
7	O
(	O
3	O
)	O
,	O
272–281	O
.	O
gamerman	O
,	O
d.	O
(	O
1997	O
)	O
.	O
efficient	O
sam-	O
pling	O
from	O
the	O
posterior	O
distribu-	O
tion	O
in	O
generalized	O
linear	O
mixed	O
models	O
.	O
statistics	O
and	O
computing	O
7	O
,	O
57–68	O
.	O
geiger	O
,	O
d.	O
and	O
d.	O
heckerman	O
(	O
1994	O
)	O
.	O
in	O
learning	B
gaussian	O
networks	O
.	O
uai	O
,	O
volume	O
10	O
,	O
pp	O
.	O
235–243	O
.	O
geiger	O
,	O
d.	O
and	O
d.	O
heckerman	O
(	O
1997	O
)	O
.	O
a	O
characterization	O
of	O
dirchlet	O
dis-	O
tributions	O
through	O
local	O
and	O
global	O
independence	O
.	O
annals	O
of	O
statis-	O
tics	O
25	O
,	O
1344–1368	O
.	O
gelfand	O
,	O
a	O
.	O
(	O
1996	O
)	O
.	O
model	O
determina-	O
tion	O
using	O
sampling-based	O
meth-	O
ods	O
.	O
in	O
gilks	O
,	O
richardson	O
,	O
and	O
spiegelhalter	O
(	O
eds	O
.	O
)	O
,	O
markov	O
chain	O
monte	O
carlo	O
in	O
practice	O
.	O
chapman	O
&	O
hall	O
.	O
geman	O
,	O
s.	O
,	O
e.	O
bienenstock	O
,	O
and	O
r.	O
doursat	O
(	O
1992	O
)	O
.	O
neural	B
networks	I
and	O
the	O
bias-variance	O
dilemma	O
.	O
neural	O
computing	O
4	O
,	O
1–58	O
.	O
geman	O
,	O
s.	O
and	O
d.	O
geman	O
(	O
1984	O
)	O
.	O
stochastic	O
relaxation	O
,	O
gibbs	O
distri-	O
butions	O
,	O
and	O
the	O
bayesian	O
restora-	O
tion	O
of	O
images	O
.	O
ieee	O
trans	O
.	O
on	O
pat-	O
tern	O
analysis	O
and	O
machine	O
intelli-	O
gence	O
6	O
(	O
6	O
)	O
.	O
geoffrion	O
,	O
a	O
.	O
(	O
1974	O
)	O
.	O
lagrangian	O
relaxation	O
for	O
integer	O
program-	O
ming	O
.	O
mathematical	O
programming	O
study	O
2	O
,	O
82–114	O
.	O
george	O
,	O
e.	O
and	O
d.	O
foster	O
(	O
2000	O
)	O
.	O
cal-	O
ibration	O
and	O
empirical	O
bayes	O
vari-	O
able	O
selection	O
.	O
biometrika	O
87	O
(	O
4	O
)	O
,	O
731–747	O
.	O
getoor	O
,	O
l.	O
and	O
b.	O
taskar	O
(	O
eds	O
.	O
)	O
(	O
2007	O
)	O
.	O
introduction	O
to	O
relational	O
statistical	O
learning	B
.	O
mit	O
press	O
.	O
geyer	O
,	O
c.	O
(	O
1992	O
)	O
.	O
practical	O
markov	O
chain	O
monte	O
carlo	O
.	O
statistical	O
sci-	O
ence	O
7	O
,	O
473–483	O
.	O
ghahramani	O
,	O
z.	O
and	O
m.	O
beal	O
(	O
2000	O
)	O
.	O
inference	B
for	O
bayesian	O
in	O
variational	O
mixtures	O
of	O
factor	B
analysers	O
.	O
nips-12	O
.	O
ghahramani	O
,	O
z.	O
and	O
m.	O
beal	O
(	O
2001	O
)	O
.	O
propagation	O
algorithms	O
for	O
varia-	O
tional	O
bayesian	O
learning	B
.	O
in	O
nips-	O
13.	O
gelfand	O
,	O
a.	O
and	O
a.	O
smith	O
(	O
1990	O
)	O
.	O
sampling-based	O
approaches	O
to	O
cal-	O
culating	O
marginal	O
densities	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
85	O
,	O
385–409	O
.	O
ghahramani	O
,	O
z.	O
and	O
g.	O
hinton	O
(	O
1996a	O
)	O
.	O
the	O
em	O
algorithm	O
for	O
mixtures	O
of	O
factor	B
analyzers	O
.	O
technical	O
report	O
,	O
dept	O
.	O
of	O
comp	O
.	O
sci.	O
,	O
uni	O
.	O
toronto	O
.	O
ghahramani	O
,	O
z.	O
and	O
g.	O
hinton	O
(	O
1996b	O
)	O
.	O
parameter	B
estimation	O
for	O
linear	O
dy-	O
namical	O
systems	O
.	O
technical	O
re-	O
port	O
crg-tr-96-2	O
,	O
dept	O
.	O
comp	O
.	O
sci.	O
,	O
univ	O
.	O
toronto	O
.	O
ghahramani	O
,	O
z.	O
and	O
m.	O
jordan	O
(	O
1997	O
)	O
.	O
factorial	O
hidden	O
markov	O
models	O
.	O
machine	B
learning	I
29	O
,	O
245–273	O
.	O
a	O
moving	O
gilks	O
,	O
w.	O
and	O
c.	O
berzuini	O
(	O
2001	O
)	O
.	O
following	O
target	O
–	O
monte	O
carlo	O
infernece	O
for	O
dynamic	O
bayesian	O
models	O
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
63	O
,	O
127–146	O
.	O
gilks	O
,	O
w.	O
,	O
n.	O
best	O
,	O
and	O
k.	O
tan	O
(	O
1995	O
)	O
.	O
adaptive	O
rejection	O
metropolis	O
sam-	O
pling	O
.	O
applied	O
statistics	O
44	O
,	O
455–472	O
.	O
gilks	O
,	O
w.	O
and	O
p.	O
wild	O
(	O
1992	O
)	O
.	O
adaptive	B
rejection	I
sampling	I
for	O
gibbs	O
sam-	O
pling	O
.	O
applied	O
statistics	O
41	O
,	O
337–348	O
.	O
girolami	O
,	O
m.	O
,	O
b.	O
calderhead	O
,	O
and	O
s.	O
chin	O
(	O
2010	O
)	O
.	O
riemannian	O
man-	O
ifold	O
hamiltonian	O
monte	O
carlo	O
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b.	O
to	O
ap-	O
pear	O
.	O
girolami	O
,	O
m.	O
and	O
s.	O
rogers	O
(	O
2005	O
)	O
.	O
hi-	O
erarchic	O
bayesian	O
models	O
for	O
kernel	B
learning	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
,	O
pp	O
.	O
241–248	O
.	O
girolami	O
,	O
m.	O
and	O
s.	O
rogers	O
(	O
2006	O
)	O
.	O
variational	O
bayesian	O
multinomial	B
probit	I
regression	O
with	O
gaussian	O
process	O
priors	O
.	O
neural	O
comptua-	O
tion	O
18	O
(	O
8	O
)	O
,	O
1790	O
–	O
1817.	O
girshick	O
,	O
r.	O
,	O
p.	O
felzenszwalb	O
,	O
and	O
d.	O
mcallester	O
(	O
2011	O
)	O
.	O
object	O
de-	O
tection	O
with	O
grammar	O
models	O
.	O
in	O
nips	O
.	O
gittins	O
,	O
j	O
.	O
(	O
1989	O
)	O
.	O
multi-armed	B
bandit	I
allocation	O
indices	O
.	O
wiley	O
.	O
giudici	O
,	O
p.	O
and	O
p.	O
green	O
(	O
1999	O
)	O
.	O
gaus-	O
determination	O
.	O
decomposable	B
sian	O
model	O
biometrika	O
86	O
(	O
4	O
)	O
,	O
785–801	O
.	O
graphical	O
givoni	O
,	O
i.	O
e.	O
and	O
b.	O
j.	O
frey	O
(	O
2009	O
,	O
june	O
)	O
.	O
a	O
binary	O
variable	O
model	O
for	O
affin-	O
ity	O
propagation	O
.	O
neural	O
computa-	O
tion	O
21	O
(	O
6	O
)	O
,	O
1589–1600	O
.	O
globerson	O
,	O
a.	O
and	O
t.	O
jaakkola	O
(	O
2008	O
)	O
.	O
fixing	O
max-product	B
:	O
convergent	O
message	B
passing	I
algorithms	O
for	O
map	O
lp-relaxations	O
.	O
in	O
nips	O
.	O
glorot	O
,	O
x.	O
and	O
y.	O
bengio	O
(	O
2010	O
,	O
may	O
)	O
.	O
understanding	O
the	O
difficulty	O
of	O
training	O
deep	O
feedforward	O
neural	O
networks	O
.	O
in	O
ai/statistics	O
,	O
volume	O
9	O
,	O
pp	O
.	O
249–256	O
.	O
bibliography	O
1025	O
gogate	O
,	O
v.	O
,	O
w.	O
a.	O
webb	O
,	O
and	O
p.	O
domin-	O
learning	B
efficient	O
gos	O
markov	O
networks	O
.	O
in	O
nips	O
.	O
(	O
2010	O
)	O
.	O
goldenberg	O
,	O
a.	O
,	O
a.	O
x.	O
zheng	O
,	O
s.	O
e.	O
fien-	O
berg	O
,	O
and	O
e.	O
m.	O
airoldi	O
(	O
2009	O
)	O
.	O
a	O
survey	O
of	O
statistical	O
network	O
mod-	O
els	O
.	O
foundations	O
and	O
trends	O
in	O
ma-	O
chine	O
learning	B
,	O
129–233	O
.	O
golub	O
,	O
g.	O
and	O
c.	O
f.	O
van	O
loan	O
(	O
1996	O
)	O
.	O
johns	O
hop-	O
matrix	O
computations	O
.	O
kins	O
university	O
press	O
.	O
gonen	O
,	O
m.	O
,	O
w.	O
johnson	O
,	O
y.	O
lu	O
,	O
and	O
p.	O
westfall	O
(	O
2005	O
,	O
august	O
)	O
.	O
the	O
bayesian	O
two-sample	O
t	O
test	O
.	O
the	O
american	O
statistician	O
59	O
(	O
3	O
)	O
,	O
252–	O
257.	O
gonzales	O
,	O
t.	O
(	O
1985	O
)	O
.	O
clustering	B
to	O
minimize	O
the	O
maximum	O
interclus-	O
ter	O
distance	O
.	O
theor	O
.	O
comp	O
.	O
sci	O
.	O
38	O
,	O
293–306	O
.	O
gorder	O
,	O
p.	O
f.	O
(	O
2006	O
,	O
nov/dec	O
)	O
.	O
neu-	O
ral	O
networks	O
show	O
new	O
promise	O
for	O
machine	O
vision	O
.	O
computing	O
in	O
sci-	O
ence	O
&	O
engineering	O
8	O
(	O
6	O
)	O
,	O
4–8	O
.	O
gordon	O
,	O
n.	O
(	O
1993	O
)	O
.	O
ap-	O
proach	O
to	O
nonlinear/non-gaussian	O
bayesian	O
state	B
estimation	I
.	O
iee	O
pro-	O
ceedings	O
(	O
f	O
)	O
140	O
(	O
2	O
)	O
,	O
107–113	O
.	O
novel	O
graepel	O
,	O
t.	O
,	O
j.	O
quinonero-candela	O
,	O
t.	O
borchert	O
,	O
and	O
r.	O
herbrich	O
(	O
2010	O
)	O
.	O
web-scale	O
bayesian	O
click-	O
through	O
rate	B
prediction	O
for	O
spon-	O
sored	O
search	O
advertising	O
in	O
mi-	O
crosoftâ	O
˘a	O
´zs	O
bing	O
search	O
engine	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
grauman	O
,	O
k.	O
and	O
t.	O
darrell	O
(	O
2007	O
,	O
april	O
)	O
.	O
the	O
pyramid	B
match	I
kernel	I
:	O
efficient	O
learning	O
with	O
sets	O
of	O
fea-	O
tures	O
.	O
j.	O
of	O
machine	B
learning	I
re-	O
search	O
8	O
,	O
725–760	O
.	O
green	O
,	O
p.	O
(	O
1998	O
)	O
.	O
reversible	O
jump	O
markov	O
chain	O
monte	O
carlo	O
compu-	O
tation	O
and	O
bayesian	O
model	O
deter-	O
mination	O
.	O
biometrika	O
82	O
,	O
711–732	O
.	O
green	O
,	O
p.	O
(	O
2003	O
)	O
.	O
tutorial	O
on	O
trans-	O
dimensional	O
mcmc	O
.	O
in	O
p.	O
green	O
,	O
n.	O
hjort	O
,	O
and	O
s.	O
richardson	O
(	O
eds	O
.	O
)	O
,	O
highly	O
structured	O
stochastic	O
systems	O
.	O
oup	O
.	O
greenshtein	O
,	O
e.	O
and	O
j.	O
park	O
(	O
2009	O
)	O
.	O
ap-	O
plication	O
of	O
non	O
parametric	O
empir-	O
ical	O
bayes	O
estimation	O
to	O
high	O
di-	O
mensional	O
classiﬁcation	B
.	O
j.	O
of	O
ma-	O
chine	O
learning	B
research	O
10	O
,	O
1687–	O
1704.	O
greig	O
,	O
d.	O
,	O
b.	O
porteous	O
,	O
and	O
a.	O
seheult	O
(	O
1989	O
)	O
.	O
exact	O
maximum	B
a	I
posteriori	I
estimation	O
for	O
binary	O
images	O
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
51	O
(	O
2	O
)	O
,	O
271–	O
279.	O
griffin	O
,	O
j.	O
and	O
p.	O
brown	O
(	O
2007	O
)	O
.	O
bayesian	O
adaptive	O
lassos	O
with	O
non-	O
convex	B
penalization	O
.	O
technical	O
re-	O
port	O
,	O
u.	O
kent	O
.	O
griffin	O
,	O
j.	O
and	O
p.	O
brown	O
(	O
2010	O
)	O
.	O
in-	O
ference	O
with	O
normal-gamma	O
prior	O
distributions	O
in	O
regression	B
prob-	O
lems	O
.	O
bayesian	O
analysis	O
5	O
(	O
1	O
)	O
,	O
171–	O
188.	O
griffiths	O
,	O
t.	O
.	O
and	O
j.	O
tenenbaum	O
(	O
2009	O
)	O
.	O
theory-based	O
causal	O
induction	O
.	O
psychological	O
review	O
116	O
(	O
4	O
)	O
,	O
661–	O
716.	O
griffiths	O
,	O
t.	O
and	O
m.	O
steyvers	O
(	O
2004	O
)	O
.	O
finding	O
scientiﬁc	O
topics	O
.	O
proc	O
.	O
of	O
the	O
national	O
academy	O
of	O
science	O
,	O
usa	O
101	O
,	O
5228–5235	O
.	O
griffiths	O
,	O
t.	O
,	O
m.	O
steyvers	O
,	O
d.	O
blei	O
,	O
and	O
integrating	O
j.	O
tenenbaum	O
(	O
2004	O
)	O
.	O
topics	O
and	O
syntax	O
.	O
in	O
nips	O
.	O
griffiths	O
,	O
t.	O
and	O
j.	O
tenenbaum	O
(	O
2001	O
)	O
.	O
using	O
vocabulary	O
knowledge	O
in	O
bayesian	O
multinomial	B
estimation	O
.	O
in	O
nips	O
,	O
pp	O
.	O
1385–1392	O
.	O
griffiths	O
,	O
t.	O
and	O
j.	O
tenenbaum	O
(	O
2005	O
)	O
.	O
structure	O
and	O
strength	O
in	O
causal	O
induction	O
.	O
cognitive	O
psychology	O
51	O
,	O
334–384	O
.	O
grimmett	O
,	O
g.	O
and	O
d.	O
stirzaker	O
(	O
1992	O
)	O
.	O
probability	O
and	O
random	O
processes	O
.	O
oxford	O
.	O
guan	O
,	O
y.	O
,	O
j.	O
dy	O
,	O
d.	O
niu	O
,	O
and	O
z.	O
ghahra-	O
mani	O
(	O
2010	O
)	O
.	O
variational	B
inference	I
for	O
nonparametric	O
multiple	O
clus-	O
tering	O
.	O
in	O
1st	O
intl	O
.	O
workshop	O
on	O
discovering	O
,	O
summarizing	O
and	O
us-	O
ing	O
multiple	O
clustering	O
(	O
multiclust	O
)	O
.	O
guedon	O
,	O
y	O
.	O
(	O
2003	O
)	O
.	O
estimating	O
hidden	B
semi-markov	O
chains	O
from	O
discrete	B
sequences	O
.	O
j.	O
of	O
computational	O
and	O
graphical	O
statistics	O
12	O
,	O
604–639	O
.	O
gustafsson	O
,	O
m.	O
(	O
2001	O
)	O
.	O
bilistic	O
derivation	O
of	O
least-squares	O
algorithm	O
.	O
chemical	O
ing	O
41	O
,	O
288–294	O
.	O
a	O
proba-	O
the	O
partial	O
journal	O
of	O
information	B
and	O
model-	O
guyon	O
,	O
i.	O
,	O
s.	O
gunn	O
,	O
m.	O
nikravesh	O
,	O
and	O
l.	O
zadeh	O
(	O
eds	O
.	O
)	O
(	O
2006	O
)	O
.	O
feature	O
ex-	O
traction	O
:	O
foundations	O
and	O
applica-	O
tions	O
.	O
springer	O
.	O
hacker	O
,	O
j.	O
and	O
p.	O
pierson	O
(	O
2010	O
)	O
.	O
winner-take-all	O
how	O
washington	O
made	O
the	O
rich	O
richer–	O
and	O
turned	O
its	O
back	O
on	O
the	O
middle	O
class	O
.	O
simon	O
&	O
schuster	O
.	O
politics	O
:	O
halevy	O
,	O
a.	O
,	O
p.	O
norvig	O
,	O
and	O
f.	O
pereira	O
(	O
2009	O
)	O
.	O
the	O
unreasonable	O
effective-	O
ness	O
of	O
data	O
.	O
ieee	O
intelligent	O
sys-	O
tems	O
24	O
(	O
2	O
)	O
,	O
8–12	O
.	O
hall	O
,	O
p.	O
,	O
j.	O
t.	O
ormerod	O
,	O
and	O
m.	O
p.	O
wand	O
(	O
2011	O
)	O
.	O
theory	O
of	O
gaussian	O
varia-	O
tional	O
approximation	O
for	O
a	O
gener-	O
alised	O
linear	O
mixed	O
model	O
.	O
statis-	O
tica	O
sinica	O
21	O
,	O
269–389	O
.	O
hamilton	O
,	O
j	O
.	O
(	O
1990	O
)	O
.	O
analysis	O
of	O
time	O
series	O
subject	O
to	O
changes	O
in	O
regime	O
.	O
j.	O
econometrics	O
45	O
,	O
39–70	O
.	O
hans	O
,	O
c.	O
(	O
2009	O
)	O
.	O
bayesian	O
lasso	B
re-	O
gression	O
.	O
biometrika	O
96	O
(	O
4	O
)	O
,	O
835–	O
845.	O
hansen	O
,	O
m.	O
and	O
b.	O
yu	O
(	O
2001	O
)	O
.	O
model	B
selection	I
and	O
the	O
principle	O
of	O
min-	O
imum	O
description	O
length	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc..	O
hara	O
,	O
h.	O
and	O
a.	O
takimura	O
(	O
2008	O
)	O
.	O
a	O
localization	O
approach	O
to	O
im-	O
prove	O
iterative	O
proportional	O
scal-	O
ing	O
in	O
gaussian	O
graphical	O
models	O
.	O
communications	O
in	O
statistics	O
-	O
the-	O
ory	O
and	O
method	O
.	O
to	O
appear	O
.	O
hardin	O
,	O
j.	O
and	O
j.	O
hilbe	O
(	O
2003	O
)	O
.	O
gener-	O
alized	O
estimating	O
equations	O
.	O
chap-	O
man	O
and	O
hall/crc	O
.	O
harmeling	O
,	O
s.	O
and	O
c.	O
k.	O
i.	O
williams	O
(	O
2011	O
)	O
.	O
greedy	O
learning	O
of	O
binary	O
latent	O
trees	O
.	O
ieee	O
trans	O
.	O
on	O
pat-	O
tern	O
analysis	O
and	O
machine	O
intelli-	O
gence	O
33	O
(	O
6	O
)	O
,	O
1087–1097	O
.	O
harnard	O
,	O
s.	O
(	O
1990	O
)	O
.	O
the	O
symbol	B
grounding	I
problem	O
.	O
physica	O
d	O
42	O
,	O
335–346	O
.	O
green	O
,	O
p.	O
and	O
b.	O
silverman	O
(	O
1994	O
)	O
.	O
non-	O
parametric	O
regression	O
and	O
general-	O
ized	O
linear	O
models	O
.	O
chapman	O
and	O
hall	O
.	O
guo	O
,	O
y	O
.	O
(	O
2009	O
)	O
.	O
supervised	O
exponential	O
family	B
principal	O
component	O
anal-	O
ysis	O
via	O
convex	B
optimization	O
.	O
in	O
nips	O
.	O
harvey	O
,	O
a.	O
c.	O
(	O
1990	O
)	O
.	O
forecasting	O
,	O
struc-	O
tural	O
time	O
series	O
models	O
,	O
and	O
the	O
kalman	O
filter	O
.	O
cambridge	O
univer-	O
ity	O
press	O
.	O
1026	O
bibliography	O
hastie	O
,	O
t.	O
,	O
s.	O
rosset	O
,	O
r.	O
tibshirani	O
,	O
and	O
j.	O
zhu	O
(	O
2004	O
)	O
.	O
the	O
entire	O
regular-	O
ization	O
path	B
for	O
the	O
support	B
vector	I
machine	I
.	O
j.	O
of	O
machine	B
learning	I
research	O
5	O
,	O
1391–1415	O
.	O
hastie	O
,	O
t.	O
and	O
r.	O
tibshirani	O
(	O
1990	O
)	O
.	O
generalized	O
additive	O
models	O
.	O
chap-	O
man	O
and	O
hall	O
.	O
hastie	O
,	O
t.	O
,	O
r.	O
tibshirani	O
,	O
and	O
j.	O
fried-	O
man	O
(	O
2001	O
)	O
.	O
the	O
elements	O
of	O
statis-	O
tical	O
learning	B
.	O
springer	O
.	O
hastie	O
,	O
t.	O
,	O
r.	O
tibshirani	O
,	O
and	O
j.	O
fried-	O
man	O
(	O
2009	O
)	O
.	O
the	O
elements	O
of	O
statisti-	O
cal	O
learning	B
.	O
springer	O
.	O
2nd	O
edition	O
.	O
hastings	O
,	O
w.	O
(	O
1970	O
)	O
.	O
monte	O
carlo	O
sampling	O
methods	O
using	O
markov	O
chains	O
applications	O
.	O
biometrika	O
57	O
(	O
1	O
)	O
,	O
97–109	O
.	O
their	O
and	O
haykin	O
,	O
s.	O
(	O
1998	O
)	O
.	O
neural	B
networks	I
:	O
a	O
comprehensive	O
foundation	O
.	O
pren-	O
tice	O
hall	O
.	O
2nd	O
edition	O
.	O
haykin	O
,	O
s	O
.	O
(	O
ed	O
.	O
)	O
(	O
2001	O
)	O
.	O
kalman	O
filter-	O
ing	O
and	O
neural	B
networks	I
.	O
wiley	O
.	O
hazan	O
,	O
t.	O
and	O
a.	O
shashua	O
(	O
2008	O
)	O
.	O
convergent	O
message-passing	O
algo-	O
rithms	O
for	O
inference	B
over	O
general	O
graphs	O
with	O
convex	B
free	O
energy	O
.	O
in	O
uai	O
.	O
hazan	O
,	O
t.	O
and	O
a.	O
shashua	O
(	O
2010	O
)	O
.	O
norm-product	O
belief	B
propagation	I
:	O
primal-dual	O
message	B
passing	I
for	O
approximate	B
inference	I
.	O
ieee	O
trans	O
.	O
on	O
info	O
.	O
theory	O
56	O
(	O
12	O
)	O
,	O
6294–6316	O
.	O
he	O
,	O
y.-b	O
.	O
and	O
z.	O
geng	O
(	O
2009	O
)	O
.	O
active	B
learning	I
of	O
causal	B
networks	I
with	O
intervention	O
experiments	O
and	O
opti-	O
mal	O
designs	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
10	O
,	O
2523–2547	O
.	O
heaton	O
,	O
m.	O
and	O
j	O
.	O
(	O
2009	O
)	O
.	O
bayesian	O
computation	O
and	O
the	O
lin-	O
ear	O
model	O
.	O
technical	O
report	O
,	O
duke	O
.	O
scott	O
heckerman	O
,	O
d.	O
d.	O
,	O
chickering	O
,	O
and	O
c.	O
meek	O
,	O
r.	O
rounthwaite	O
,	O
c.	O
kadie	O
(	O
2000	O
)	O
.	O
dependency	B
networks	I
for	O
density	B
estimation	I
,	O
collaborative	B
ﬁltering	I
,	O
and	O
data	O
vi-	O
sualization	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
1	O
,	O
49–75	O
.	O
heckerman	O
,	O
d.	O
,	O
d.	O
geiger	O
,	O
and	O
learning	B
m.	O
chickering	O
(	O
1995	O
)	O
.	O
bayesian	O
networks	O
:	O
the	O
combina-	O
tion	O
of	O
knowledge	O
and	O
statistical	O
data	O
.	O
machine	B
learning	I
20	O
(	O
3	O
)	O
,	O
197–	O
243.	O
heckerman	O
,	O
d.	O
,	O
c.	O
meek	O
,	O
(	O
1997	O
,	O
february	O
)	O
.	O
and	O
g.	O
cooper	O
a	O
bayesian	O
approach	O
to	O
causal	O
dis-	O
covery	O
.	O
technical	O
report	O
msr-tr-	O
97-05	O
,	O
microsoft	O
research	O
.	O
heckerman	O
,	O
d.	O
,	O
c.	O
meek	O
,	O
and	O
d.	O
koller	O
probabilistic	O
models	O
for	O
(	O
2004	O
)	O
.	O
relational	O
data	O
.	O
technical	O
re-	O
port	O
msr-tr-2004-30	O
,	O
microsoft	O
research	O
.	O
heller	O
,	O
k.	O
and	O
z.	O
ghahramani	O
(	O
2005	O
)	O
.	O
bayesian	O
hierarchical	B
clustering	I
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
henrion	O
,	O
m.	O
(	O
1988	O
)	O
.	O
propagation	O
of	O
uncertainty	B
by	O
logic	B
sampling	I
in	O
bayes	O
’	O
networks	O
.	O
in	O
uai	O
,	O
pp	O
.	O
149–	O
164.	O
herbrich	O
,	O
r.	O
,	O
t.	O
minka	O
,	O
and	O
t.	O
graepel	O
(	O
2007	O
)	O
.	O
trueskill	O
:	O
a	O
bayesian	O
skill	O
rating	O
system	O
.	O
in	O
nips	O
.	O
hertz	O
,	O
j.	O
,	O
a.	O
krogh	O
,	O
and	O
r.	O
g.	O
palmer	O
(	O
1991	O
)	O
.	O
an	O
introduction	O
to	O
the	O
theory	O
of	O
neural	O
comptuation	O
.	O
addison-	O
wesley	O
.	O
hillar	O
,	O
c.	O
,	O
j.	O
sohl-dickstein	O
,	O
and	O
k.	O
koepsell	O
(	O
2012	O
,	O
april	O
)	O
.	O
efficient	O
and	O
optimal	O
binary	O
hopﬁeld	O
asso-	O
ciative	O
memory	O
storage	O
using	O
min-	O
imum	O
probability	O
ﬂow	O
.	O
technical	O
report	O
.	O
hinton	O
,	O
g.	O
(	O
1999	O
)	O
.	O
products	O
of	O
experts	O
.	O
in	O
proc	O
.	O
9th	O
intl	O
.	O
conf	O
.	O
on	O
artif	O
.	O
neu-	O
ral	O
networks	O
(	O
icann	O
)	O
,	O
volume	O
1	O
,	O
pp	O
.	O
1–6	O
.	O
hinton	O
,	O
g.	O
(	O
2002	O
)	O
.	O
training	O
products	O
of	O
experts	O
by	O
minimizing	O
contrastive	B
divergence	I
.	O
neural	O
computation	O
14	O
,	O
1771–1800	O
.	O
hinton	O
,	O
g.	O
(	O
2010	O
)	O
.	O
a	O
practical	O
guide	O
to	O
training	O
restricted	O
boltzmann	O
machines	O
.	O
technical	O
report	O
,	O
u.	O
toronto	O
.	O
hinton	O
,	O
g.	O
and	O
d.	O
v.	O
camp	O
(	O
1993	O
)	O
.	O
keeping	O
neural	B
networks	I
simple	O
by	O
minimizing	O
the	O
description	O
length	O
of	O
the	O
weights	O
.	O
in	O
in	O
proc	O
.	O
of	O
the	O
6th	O
ann	O
.	O
acm	O
conf	O
.	O
on	O
computa-	O
tional	O
learning	B
theory	O
,	O
pp	O
.	O
5–13	O
.	O
acm	O
press	O
.	O
hinton	O
,	O
g.	O
,	O
s.	O
osindero	O
,	O
and	O
y.	O
teh	O
(	O
2006	O
)	O
.	O
a	O
fast	O
learning	O
algorithm	O
for	O
deep	O
belief	O
nets	O
.	O
neural	O
com-	O
putation	O
18	O
,	O
1527–1554	O
.	O
hinton	O
,	O
g.	O
and	O
r.	O
salakhutdinov	O
(	O
2006	O
,	O
july	O
)	O
.	O
reducing	O
the	O
dimensionality	O
of	O
data	O
with	O
neural	B
networks	I
.	O
sci-	O
ence	O
313	O
(	O
5786	O
)	O
,	O
504–507	O
.	O
hinton	O
,	O
g.	O
e.	O
,	O
p.	O
dayan	O
,	O
and	O
m.	O
revow	O
(	O
1997	O
)	O
.	O
modeling	O
the	O
manifolds	O
of	O
images	O
of	O
handwritten	O
digits	O
.	O
ieee	O
trans	O
.	O
on	O
neural	B
networks	I
8	O
,	O
65–74	O
.	O
hinton	O
,	O
g.	O
e.	O
and	O
y.	O
teh	O
(	O
2001	O
)	O
.	O
discovering	O
multiple	O
constraints	O
that	O
are	O
frequently	O
approximately	O
satisïˇn	O
˛aed	O
.	O
in	O
uai	O
.	O
hjort	O
,	O
n.	O
,	O
c.	O
holmes	O
,	O
p.	O
muller	O
,	O
and	O
bayesian	O
s.	O
walker	O
(	O
eds	O
.	O
)	O
nonparametrics	O
.	O
cambridge	O
.	O
(	O
2010	O
)	O
.	O
hoeﬂing	O
,	O
h.	O
(	O
2010	O
)	O
.	O
a	O
path	B
algorithm	O
for	O
the	O
fused	B
lasso	I
signal	O
approx-	O
imator	O
.	O
technical	O
report	O
,	O
stanford	O
.	O
hoeﬂing	O
,	O
h.	O
r.	O
and	O
tibshirani	O
(	O
2009	O
)	O
.	O
estimation	O
of	O
sparse	B
bi-	O
nary	O
pairwise	O
markov	O
networks	O
us-	O
ing	O
pseudo-likelihoods	O
.	O
j.	O
of	O
ma-	O
chine	O
learning	B
research	O
10.	O
hoeting	O
,	O
j.	O
,	O
d.	O
madigan	O
,	O
a.	O
raftery	O
,	O
and	O
c.	O
volinsky	O
(	O
1999	O
)	O
.	O
bayesian	O
model	O
averaging	O
:	O
a	O
tutorial	O
.	O
statis-	O
tical	O
science	O
4	O
(	O
4	O
)	O
.	O
hoff	O
,	O
p.	O
d.	O
(	O
2009	O
,	O
a	O
first	O
course	O
in	O
bayesian	O
statistical	O
meth-	O
ods	O
.	O
springer	O
.	O
july	O
)	O
.	O
hoffman	O
,	O
m.	O
,	O
d.	O
blei	O
,	O
and	O
f.	O
bach	O
(	O
2010	O
)	O
.	O
online	B
learning	I
for	O
latent	B
dirichlet	O
allocation	O
.	O
in	O
nips	O
.	O
hoffman	O
,	O
m.	O
and	O
a.	O
gelman	O
(	O
2011	O
)	O
.	O
the	O
no-u-turn	O
sampler	O
:	O
adaptively	O
setting	O
path	B
lengths	O
in	O
hamilto-	O
nian	O
monte	O
carlo	O
.	O
technical	O
report	O
,	O
columbia	O
u.	O
hofmann	O
,	O
t.	O
(	O
1999	O
)	O
.	O
probabilistic	O
la-	O
tent	O
semantic	O
indexing	O
.	O
research	O
and	O
development	O
in	O
information	B
retrieval	I
,	O
50–57	O
.	O
holmes	O
,	O
c.	O
and	O
l.	O
held	O
(	O
2006	O
)	O
.	O
bayesian	O
auxiliary	O
variable	O
models	O
for	O
binary	O
and	O
multinomial	B
regres-	O
sion	O
.	O
bayesian	O
analysis	O
1	O
(	O
1	O
)	O
,	O
145–	O
168.	O
honkela	O
,	O
a.	O
and	O
h.	O
valpola	O
(	O
2004	O
)	O
.	O
variational	O
learning	O
and	O
bits-back	B
coding	O
:	O
an	O
information-theoretic	O
view	O
to	O
bayesian	O
learning	B
.	O
ieee	O
.	O
trans	O
.	O
on	O
neural	B
networks	I
15	O
(	O
4	O
)	O
.	O
honkela	O
,	O
a.	O
,	O
h.	O
valpola	O
,	O
and	O
j.	O
karhunen	O
(	O
2003	O
)	O
.	O
accelerat-	O
ing	O
cyclic	O
update	O
algorithms	O
for	O
parameter	B
estimation	O
by	O
pattern	B
searches	O
.	O
neural	O
processing	O
let-	O
ters	O
17	O
,	O
191–203	O
.	O
bibliography	O
1027	O
hopﬁeld	O
,	O
j.	O
j	O
.	O
(	O
1982	O
,	O
april	O
)	O
.	O
neu-	O
ral	O
networks	O
and	O
physical	O
systems	O
with	O
emergent	O
collective	O
computa-	O
tional	O
abilities	O
.	O
proc	O
.	O
of	O
the	O
national	O
academy	O
of	O
science	O
,	O
usa	O
79	O
(	O
8	O
)	O
,	O
2554â	O
˘a	O
¸s2558	O
.	O
hornik	O
,	O
k.	O
(	O
1991	O
)	O
.	O
approximation	O
ca-	O
pabilities	O
of	O
multilayer	O
feedforward	O
networks	O
.	O
neural	B
networks	I
4	O
(	O
2	O
)	O
,	O
251â	O
˘a	O
¸s257	O
.	O
horvitz	O
,	O
e.	O
,	O
j.	O
apacible	O
,	O
r.	O
sarin	O
,	O
and	O
l.	O
liao	O
(	O
2005	O
)	O
.	O
prediction	O
,	O
expecta-	O
tion	O
,	O
and	O
surprise	O
:	O
methods	O
,	O
de-	O
signs	O
,	O
and	O
study	O
of	O
a	O
deployed	O
traffic	O
forecasting	O
service	O
.	O
in	O
uai	O
.	O
howard	O
,	O
r.	O
and	O
j.	O
matheson	O
(	O
1981	O
)	O
.	O
in-	O
ﬂuence	O
diagrams	O
.	O
in	O
r.	O
howard	O
and	O
j.	O
matheson	O
(	O
eds	O
.	O
)	O
,	O
readings	O
on	O
the	O
principles	O
and	O
applications	O
of	O
decision	B
analysis	O
,	O
volume	O
ii	O
.	O
strate-	O
gic	O
decisions	O
group	O
.	O
hoyer	O
,	O
p.	O
(	O
2004	O
)	O
.	O
non-negative	O
matrix	O
factorizaton	O
with	O
sparseness	O
con-	O
straints	O
.	O
j.	O
of	O
machine	B
learning	I
re-	O
search	O
5	O
,	O
1457–1469	O
.	O
hsu	O
,	O
c.-w.	O
,	O
c.-c.	O
chang	O
,	O
and	O
c.-j	O
.	O
lin	O
(	O
2009	O
)	O
.	O
a	O
practical	O
guide	O
to	O
sup-	O
port	O
vector	O
classiﬁcation	O
.	O
technical	O
report	O
,	O
dept	O
.	O
comp	O
.	O
sci.	O
,	O
national	O
taiwan	O
university	O
.	O
hu	O
,	O
d.	O
,	O
l.	O
van	O
der	O
maaten	O
,	O
y.	O
cho	O
,	O
l.	O
saul	O
,	O
and	O
s.	O
lerner	O
(	O
2010	O
)	O
.	O
latent	B
variable	I
models	I
for	O
predicting	O
file	O
dependencies	O
in	O
large-scale	O
soft-	O
ware	O
development	O
.	O
in	O
nips	O
.	O
hu	O
,	O
m.	O
,	O
c.	O
ingram	O
,	O
m.sirski	O
,	O
c.	O
pal	O
,	O
s.	O
swamy	O
,	O
and	O
c.	O
patten	O
(	O
2000	O
)	O
.	O
a	O
hierarchical	O
hmm	O
implementa-	O
tion	O
for	O
vertebrate	O
gene	O
splice	O
site	O
prediction	O
.	O
technical	O
report	O
,	O
dept	O
.	O
computer	O
science	O
,	O
univ	O
.	O
waterloo	O
.	O
huang	O
,	O
j.	O
,	O
q.	O
morris	O
,	O
and	O
b.	O
frey	O
(	O
2007	O
)	O
.	O
bayesian	O
inference	B
of	O
mi-	O
crorna	O
targets	O
from	O
sequence	O
and	O
expression	O
data	O
.	O
j.	O
comp	O
.	O
bio..	O
hubel	O
,	O
d.	O
and	O
t.	O
wiesel	O
(	O
1962	O
)	O
.	O
recep-	O
tive	O
ﬁelds	O
,	O
binocular	O
itneraction	O
,	O
and	O
functional	O
architecture	O
in	O
the	O
cat	O
’	O
s	O
visual	O
cortex	O
.	O
j.	O
physiology	O
160	O
,	O
106–154	O
.	O
huber	O
,	O
p.	O
(	O
1964	O
)	O
.	O
robust	B
estimation	O
of	O
a	O
location	O
parameter	B
.	O
annals	O
of	O
statistics	O
53	O
,	O
73â	O
˘a	O
¸s101	O
.	O
hubert	O
,	O
l.	O
and	O
p.	O
arabie	O
(	O
1985	O
)	O
.	O
com-	O
j.	O
of	O
classiﬁca-	O
paring	O
partitions	O
.	O
tion	O
2	O
,	O
193–218	O
.	O
hunter	O
,	O
d.	O
and	O
r.	O
li	O
(	O
2005	O
)	O
.	O
variable	O
selection	O
using	O
mm	O
algorithms	O
.	O
annals	O
of	O
statistics	O
33	O
,	O
1617–1642	O
.	O
jacob	O
,	O
l.	O
,	O
f.	O
bach	O
,	O
and	O
j.-p.	O
vert	O
(	O
2008	O
)	O
.	O
a	O
clustered	O
multi-task	B
learning	I
:	O
convex	B
formulation	O
.	O
in	O
nips	O
.	O
hunter	O
,	O
d.	O
r.	O
and	O
k.	O
lange	O
(	O
2004	O
)	O
.	O
a	O
tutorial	O
on	O
mm	O
algorithms	O
.	O
the	O
american	O
statistician	O
58	O
,	O
30–37	O
.	O
jain	O
,	O
a.	O
and	O
r.	O
dubes	O
(	O
1988	O
)	O
.	O
algo-	O
rithms	O
for	O
clustering	B
data	O
.	O
prentice	O
hall	O
.	O
hyaﬁl	O
,	O
l.	O
and	O
r.	O
rivest	O
(	O
1976	O
)	O
.	O
con-	O
structing	O
optimal	O
binary	O
decision	B
trees	I
is	O
np-complete	O
.	O
information	B
processing	O
letters	O
5	O
(	O
1	O
)	O
,	O
15–17	O
.	O
james	O
,	O
g.	O
and	O
t.	O
hastie	O
(	O
1998	O
)	O
.	O
the	O
error	O
coding	O
method	O
and	O
picts	O
.	O
j.	O
of	O
computational	O
and	O
graphical	O
statistics	O
7	O
(	O
3	O
)	O
,	O
377–387	O
.	O
hyvarinen	O
,	O
a.	O
,	O
j.	O
hurri	O
,	O
and	O
p.	O
hoyer	O
(	O
2009	O
)	O
.	O
natural	O
image	O
statistics	O
:	O
a	O
probabilistic	O
approach	O
to	O
early	O
com-	O
putational	O
vision	O
.	O
springer	O
.	O
hyvarinen	O
,	O
a.	O
and	O
e.	O
oja	O
(	O
2000	O
)	O
.	O
in-	O
dependent	O
component	O
analysis	O
:	O
al-	O
gorithms	O
and	O
applications	O
.	O
neural	B
networks	I
13	O
,	O
411–430	O
.	O
ilin	O
,	O
a.	O
and	O
t.	O
raiko	O
(	O
2010	O
)	O
.	O
practi-	O
cal	O
approaches	O
to	O
principal	O
com-	O
ponent	O
analysis	O
in	O
the	O
presence	O
of	O
missing	B
values	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
11	O
,	O
1957–2000	O
.	O
insua	O
,	O
d.	O
r.	O
and	O
f.	O
ruggeri	O
(	O
eds	O
.	O
)	O
(	O
2000	O
)	O
.	O
robust	B
bayesian	O
analysis	O
.	O
springer	O
.	O
isard	O
,	O
m.	O
(	O
2003	O
)	O
.	O
pampas	O
:	O
real-valued	O
graphical	O
models	O
for	O
computer	O
vi-	O
sion	O
.	O
in	O
cvpr	O
,	O
volume	O
1	O
,	O
pp	O
.	O
613.	O
isard	O
,	O
m.	O
and	O
a.	O
blake	O
(	O
1998	O
)	O
.	O
con-	O
densation	O
-	O
conditional	O
density	O
propagation	O
for	O
visual	O
tracking	O
.	O
intl	O
.	O
j.	O
of	O
computer	O
vision	O
29	O
(	O
1	O
)	O
,	O
5–	O
18.	O
jaakkola	O
,	O
t.	O
(	O
2001	O
)	O
.	O
tutorial	O
on	O
varia-	O
tional	O
approximation	O
methods	O
.	O
in	O
m.	O
opper	O
and	O
d.	O
saad	O
(	O
eds	O
.	O
)	O
,	O
ad-	O
vanced	O
mean	B
ﬁeld	I
methods	O
.	O
mit	O
press	O
.	O
jaakkola	O
,	O
t.	O
and	O
d.	O
haussler	O
(	O
1998	O
)	O
.	O
ex-	O
ploiting	O
generative	O
models	O
in	O
dis-	O
criminative	O
classiﬁers	O
.	O
in	O
nips	O
,	O
pp	O
.	O
487–493	O
.	O
jaakkola	O
,	O
t.	O
and	O
m.	O
upper	O
computing	O
and	O
bounds	O
on	O
likelihoods	O
tractable	O
networks	O
.	O
in	O
uai	O
.	O
jordan	O
(	O
1996a	O
)	O
.	O
lower	O
in	O
in-	O
jaakkola	O
,	O
t.	O
and	O
m.	O
jordan	O
(	O
1996b	O
)	O
.	O
a	O
variational	O
approach	O
to	O
bayesian	O
logistic	B
regression	I
problems	O
and	O
their	O
extensions	O
.	O
in	O
ai	O
+	O
statistics	O
.	O
japkowicz	O
,	O
n.	O
,	O
s.	O
hanson	O
,	O
and	O
m.	O
gluck	O
(	O
2000	O
)	O
.	O
nonlinear	O
autoas-	O
sociation	O
is	O
not	O
equivalent	O
to	O
pca	O
.	O
neural	O
computation	O
12	O
,	O
531–545	O
.	O
jaynes	O
,	O
e.	O
t.	O
(	O
2003	O
)	O
.	O
probability	O
the-	O
ory	O
:	O
the	O
logic	O
of	O
science	O
.	O
cambridge	O
university	O
press	O
.	O
jebara	O
,	O
t.	O
,	O
r.	O
kondor	O
,	O
and	O
a.	O
howard	O
(	O
2004	O
)	O
.	O
probability	O
product	O
kernels	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
5	O
,	O
819–844	O
.	O
jeffreys	O
,	O
h.	O
(	O
1961	O
)	O
.	O
theory	O
of	O
probability	O
.	O
oxford	O
.	O
jelinek	O
,	O
f.	O
(	O
1997	O
)	O
.	O
statistical	O
methods	O
for	O
speech	B
recognition	I
.	O
mit	O
press	O
.	O
jensen	O
,	O
c.	O
s.	O
,	O
a.	O
kong	O
,	O
and	O
u.	O
kjaerulff	O
(	O
1995	O
)	O
.	O
blocking-gibbs	O
sampling	O
in	O
very	O
large	O
probabilistic	O
expert	O
systems	O
.	O
intl	O
.	O
j.	O
human-computer	O
studies	O
,	O
647–666	O
.	O
jermyn	O
,	O
i	O
.	O
(	O
2005	O
)	O
.	O
invariant	B
bayesian	O
estimation	O
on	O
manifolds	O
.	O
annals	O
of	O
statistics	O
33	O
(	O
2	O
)	O
,	O
583–605	O
.	O
jerrum	O
,	O
m.	O
and	O
a.	O
sinclair	O
(	O
1993	O
)	O
.	O
polynomial-time	O
approximation	O
al-	O
gorithms	O
for	O
the	O
ising	O
model	O
.	O
siam	O
j.	O
on	O
computing	O
22	O
,	O
1087–1116	O
.	O
jerrum	O
,	O
m.	O
and	O
a.	O
sinclair	O
(	O
1996	O
)	O
.	O
the	O
markov	O
chain	O
monte	O
carlo	O
method	O
:	O
an	O
approach	O
to	O
approxi-	O
mate	O
counting	O
and	O
integration	O
.	O
in	O
d.	O
s.	O
hochbaum	O
(	O
ed	O
.	O
)	O
,	O
approxima-	O
tion	O
algorithms	O
for	O
np-hard	O
prob-	O
lems	O
.	O
pws	O
publishing	O
.	O
jerrum	O
,	O
m.	O
,	O
a.	O
sinclair	O
,	O
and	O
e.	O
vigoda	O
(	O
2004	O
)	O
.	O
a	O
polynomial-time	O
approx-	O
imation	O
algorithm	O
for	O
the	O
perma-	O
nent	O
of	O
a	O
matrix	O
with	O
non-negative	O
entries	O
.	O
journal	O
of	O
the	O
acm	O
,	O
671–	O
697.	O
jaakkola	O
,	O
t.	O
s.	O
and	O
m.	O
i.	O
jordan	O
(	O
2000	O
)	O
.	O
bayesian	O
parameter	B
estimation	O
via	O
variational	O
methods	O
.	O
statistics	O
and	O
computing	O
10	O
,	O
25–37	O
.	O
ji	O
,	O
and	O
l.	O
carin	O
s.	O
,	O
d.	O
dunson	O
,	O
(	O
2009	O
)	O
.	O
multi-task	O
compressive	O
sensing	O
.	O
ieee	O
trans	O
.	O
signal	O
process-	O
ing	O
57	O
(	O
1	O
)	O
.	O
1028	O
bibliography	O
ji	O
,	O
s.	O
,	O
l.	O
tang	O
,	O
s.	O
yu	O
,	O
and	O
j.	O
ye	O
(	O
2010	O
)	O
.	O
a	O
shared-subspace	O
learning	B
frame-	O
work	O
for	O
multi-label	B
classiﬁcation	I
.	O
acm	O
trans	O
.	O
on	O
knowledge	B
discovery	I
from	O
data	O
4	O
(	O
2	O
)	O
.	O
jirousek	O
,	O
r.	O
and	O
s.	O
preucil	O
(	O
1995	O
)	O
.	O
on	O
the	O
effective	O
implementation	O
of	O
the	O
iterative	B
proportional	I
ﬁtting	I
proce-	O
dure	O
.	O
computational	O
statistics	O
&	O
data	O
analysis	O
19	O
,	O
177–189	O
.	O
joachims	O
,	O
t.	O
(	O
2006	O
)	O
.	O
training	O
linear	O
svms	O
in	O
linear	O
time	O
.	O
in	O
proc	O
.	O
of	O
the	O
int	O
’	O
l	O
conf	O
.	O
on	O
knowledge	O
discov-	O
ery	O
and	O
data	O
mining	O
.	O
joachims	O
,	O
t.	O
,	O
t.	O
finley	O
,	O
and	O
c.-n.	O
yu	O
(	O
2009	O
)	O
.	O
cutting-plane	O
training	O
of	O
structural	O
svms	O
.	O
machine	O
learn-	O
ing	O
77	O
(	O
1	O
)	O
,	O
27–59	O
.	O
johnson	O
,	O
j.	O
k.	O
,	O
d.	O
m.	O
malioutov	O
,	O
and	O
a.	O
s.	O
willsky	O
(	O
2006	O
)	O
.	O
walk-sum	O
in-	O
terpretation	O
and	O
analysis	O
of	O
gaus-	O
sian	O
belief	B
propagation	I
.	O
in	O
nips	O
,	O
pp	O
.	O
579–586	O
.	O
johnson	O
,	O
m.	O
(	O
2005	O
)	O
.	O
capacity	O
and	O
complexity	O
of	O
hmm	O
duration	O
mod-	O
eling	O
techniques	O
.	O
signal	B
processing	I
letters	O
12	O
(	O
5	O
)	O
,	O
407–410	O
.	O
johnson	O
,	O
n.	O
(	O
2009	O
)	O
.	O
a	O
study	O
of	O
the	O
nips	O
feature	B
selection	I
challenge	O
.	O
technical	O
report	O
,	O
stanford	O
.	O
johnson	O
,	O
v.	O
and	O
j.	O
albert	O
(	O
1999	O
)	O
.	O
ordi-	O
nal	O
data	O
modeling	O
.	O
springer	O
.	O
jones	O
,	O
b.	O
,	O
a.	O
dobra	O
,	O
c.	O
carvalho	O
,	O
c.	O
hans	O
,	O
c.	O
carter	O
,	O
and	O
m.	O
west	O
(	O
2005	O
)	O
.	O
experiments	O
in	O
stochastic	O
computation	O
for	O
high-dimensional	O
graphical	O
models	O
.	O
statistical	O
sci-	O
ence	O
20	O
,	O
388–400	O
.	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
2007	O
)	O
.	O
an	O
introduction	O
to	O
in	O
probabilistic	O
graphical	O
models	O
.	O
preparation	O
.	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
2011	O
)	O
.	O
the	O
era	O
of	O
big	O
in	O
isba	O
bulletin	O
,	O
volume	O
18	O
,	O
data	O
.	O
pp	O
.	O
1–3	O
.	O
jordan	O
,	O
m.	O
i.	O
,	O
z.	O
ghahramani	O
,	O
t.	O
s.	O
jaakkola	O
,	O
and	O
l.	O
k.	O
saul	O
(	O
1998	O
)	O
.	O
an	O
introduction	O
to	O
variational	O
meth-	O
ods	O
for	O
graphical	O
models	O
.	O
in	O
m.	O
jor-	O
dan	O
(	O
ed	O
.	O
)	O
,	O
learning	B
in	O
graphical	O
models	O
.	O
mit	O
press	O
.	O
journee	O
,	O
m.	O
,	O
y.	O
nesterov	O
,	O
p.	O
richtarik	O
,	O
and	O
r.	O
sepulchre	O
(	O
2010	O
)	O
.	O
general-	O
ized	O
power	B
method	I
for	O
sparse	B
prin-	O
cipal	O
components	O
analysis	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
11	O
,	O
517–	O
553.	O
julier	O
,	O
s.	O
and	O
j.	O
uhlmann	O
(	O
1997	O
)	O
.	O
a	O
new	O
extension	B
of	O
the	O
kalman	O
ﬁl-	O
ter	O
to	O
nonlinear	O
systems	O
.	O
in	O
proc	O
.	O
of	O
aerosense	O
:	O
the	O
11th	O
intl	O
.	O
symp	O
.	O
on	O
aerospace/defence	O
sensing	O
,	O
simula-	O
tion	O
and	O
controls	O
.	O
jurafsky	O
,	O
d.	O
and	O
j.	O
h.	O
martin	O
(	O
2000	O
)	O
.	O
speech	O
and	O
language	O
processing	O
:	O
an	O
introduction	O
to	O
natural	O
lan-	O
guage	O
processing	O
,	O
computational	O
linguistics	O
,	O
and	O
speech	B
recognition	I
.	O
prentice-hall	O
.	O
jurafsky	O
,	O
d.	O
and	O
j.	O
h.	O
martin	O
(	O
2008	O
)	O
.	O
speech	O
and	O
language	O
processing	O
:	O
an	O
introduction	O
to	O
natural	O
lan-	O
guage	O
processing	O
,	O
computational	O
linguistics	O
,	O
and	O
speech	B
recognition	I
.	O
prentice-hall	O
.	O
2nd	O
edition	O
.	O
kaariainen	O
,	O
m.	O
and	O
j.	O
langford	O
(	O
2005	O
)	O
.	O
a	O
comparison	O
of	O
tight	O
generaliza-	O
tion	O
bounds	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
ma-	O
chine	O
learning	B
.	O
kaelbling	O
,	O
l.	O
,	O
m.	O
a.	O
moore	O
(	O
1996	O
)	O
.	O
learning	B
:	O
a	O
survey	O
.	O
search	O
4	O
,	O
237–285	O
.	O
and	O
littman	O
,	O
reinforcement	O
j.	O
of	O
ai	O
re-	O
kaelbling	O
,	O
l.	O
p.	O
,	O
m.	O
littman	O
,	O
and	O
planning	O
a.	O
cassandra	O
(	O
1998	O
)	O
.	O
and	O
acting	O
in	O
partially	O
observable	O
stochastic	O
domains	O
.	O
artiﬁcial	O
intel-	O
ligence	O
101.	O
kaiser	O
,	O
h.	O
(	O
1958	O
)	O
.	O
the	O
varimax	B
crite-	O
rion	O
for	O
analytic	O
rotation	O
in	O
factor	B
analysis	I
.	O
psychometrika	O
23	O
(	O
3	O
)	O
.	O
kakade	O
,	O
s.	O
,	O
y.	O
w.	O
teh	O
,	O
and	O
s.	O
roweis	O
(	O
2002	O
)	O
.	O
an	O
alternate	O
objective	O
func-	O
tion	O
for	O
markovian	O
ﬁelds	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
kanazawa	O
,	O
k.	O
,	O
d.	O
koller	O
,	O
and	O
s.	O
rus-	O
sell	O
(	O
1995	O
)	O
.	O
stochastic	O
simulation	O
al-	O
gorithms	O
for	O
dynamic	O
probabilistic	O
networks	O
.	O
in	O
uai	O
.	O
kandel	O
,	O
e.	O
,	O
j.	O
schwarts	O
,	O
and	O
t.	O
jessell	O
(	O
2000	O
)	O
.	O
principles	O
of	O
neural	O
science	O
.	O
mcgraw-hill	O
.	O
jordan	O
,	O
m.	O
i.	O
and	O
r.	O
a.	O
jacobs	O
(	O
1994	O
)	O
.	O
hierarchical	O
mixtures	O
of	O
experts	O
and	O
the	O
em	O
algorithm	O
.	O
neural	O
computation	O
6	O
,	O
181–214	O
.	O
kappen	O
,	O
h.	O
and	O
f.	O
rodriguez	O
(	O
1998	O
)	O
.	O
boltzmann	O
machine	B
learning	I
using	O
mean	B
ﬁeld	I
theory	O
and	O
linear	O
re-	O
sponse	O
correction	O
.	O
in	O
nips	O
.	O
karhunen	O
,	O
j.	O
and	O
j.	O
joutsensalo	O
(	O
1995	O
)	O
.	O
generalizations	O
of	O
princi-	O
pal	O
component	O
analysis	O
,	O
optimiza-	O
tion	O
problems	O
,	O
and	O
neural	O
net-	O
works	O
.	O
neural	B
networks	I
8	O
(	O
4	O
)	O
,	O
549–	O
562.	O
kass	O
,	O
r.	O
and	O
l.	O
wasserman	O
(	O
1995	O
)	O
.	O
a	O
reference	O
bayesian	O
test	O
for	O
nested	O
hypotheses	O
and	O
its	O
relationship	O
to	O
the	O
schwarz	O
criterio	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
90	O
(	O
431	O
)	O
,	O
928–934	O
.	O
katayama	O
,	O
t.	O
(	O
2005	O
)	O
.	O
subspace	O
methods	O
for	O
systems	B
identiﬁcation	I
.	O
springer	O
verlag	O
.	O
kaufman	O
,	O
l.	O
and	O
p.	O
rousseeuw	O
(	O
1990	O
)	O
.	O
finding	O
groups	O
in	O
data	O
:	O
an	O
intro-	O
duction	O
to	O
cluster	O
analysis	O
.	O
wiley	O
.	O
kawakatsu	O
,	O
h.	O
and	O
a.	O
largey	O
(	O
2009	O
)	O
.	O
em	O
algorithms	O
for	O
ordered	O
probit	O
models	O
with	O
endogenous	O
regres-	O
sors	O
.	O
the	O
econometrics	O
journal	O
12	O
(	O
1	O
)	O
,	O
164–186	O
.	O
kearns	O
,	O
m.	O
j.	O
and	O
u.	O
v.	O
vazirani	O
(	O
1994	O
)	O
.	O
an	O
introduction	O
to	O
computational	B
learning	I
theory	I
.	O
mit	O
press	O
.	O
kelley	O
,	O
j.	O
e.	O
(	O
1960	O
)	O
.	O
the	O
cutting-plane	O
method	O
for	O
solving	O
convex	B
pro-	O
grams	O
.	O
j.	O
of	O
the	O
soc	O
.	O
for	O
industrial	O
and	O
applied	O
math	O
.	O
8	O
,	O
703–712	O
.	O
kemp	O
,	O
c.	O
,	O
j.	O
tenenbaum	O
,	O
s.	O
niyogi	O
,	O
and	O
t.	O
griffiths	O
(	O
2010	O
)	O
.	O
a	O
probabilistic	O
model	O
of	O
theory	O
formation	O
.	O
cogni-	O
tion	O
114	O
,	O
165–196	O
.	O
kemp	O
,	O
c.	O
,	O
j.	O
tenenbaum	O
,	O
t.	O
y.	O
t.	O
grif-	O
ﬁths	O
and	O
,	O
and	O
n.	O
ueda	O
(	O
2006	O
)	O
.	O
learning	B
systems	O
of	O
concepts	O
with	O
an	O
inﬁnite	B
relational	I
model	I
.	O
in	O
aaai	O
.	O
kersting	O
,	O
k.	O
,	O
s.	O
natarajan	O
,	O
and	O
d.	O
poole	O
(	O
2011	O
)	O
.	O
statistical	O
relational	O
ai	O
:	O
logic	O
,	O
probability	O
and	O
computa-	O
tion	O
.	O
technical	O
report	O
,	O
ubc	O
.	O
khan	O
,	O
m.	O
e.	O
,	O
b.	O
marlin	O
,	O
g.	O
bouchard	O
,	O
and	O
k.	O
p.	O
murphy	O
(	O
2010	O
)	O
.	O
varia-	O
tional	O
bounds	O
for	O
mixed-data	O
fac-	O
tor	O
analysis	O
.	O
in	O
nips	O
.	O
khan	O
,	O
z.	O
,	O
t.	O
balch	O
,	O
and	O
f.	O
dellaert	O
(	O
2006	O
)	O
.	O
mcmc	O
data	B
association	I
and	O
sparse	B
factorization	O
updating	O
for	O
real	O
time	O
multitarget	O
tracking	B
with	O
merged	O
and	O
multiple	O
mea-	O
surements	O
.	O
ieee	O
trans	O
.	O
on	O
pat-	O
tern	O
analysis	O
and	O
machine	O
intelli-	O
gence	O
28	O
(	O
12	O
)	O
.	O
kirkpatrick	O
,	O
s.	O
,	O
c.	O
g.	O
jr.	O
,	O
and	O
m.	O
vecchi	O
(	O
1983	O
)	O
.	O
optimization	B
by	O
simulated	B
annealing	I
.	O
science	O
220	O
,	O
671–680	O
.	O
bibliography	O
1029	O
kitagawa	O
,	O
g.	O
(	O
2004	O
)	O
.	O
the	O
two-ﬁlter	O
for-	O
mula	O
for	O
smoothing	O
and	O
an	O
im-	O
plementation	O
of	O
the	O
gaussian-sum	O
smoother	O
.	O
annals	O
of	O
the	O
institute	O
of	O
statistical	O
mathematics	O
46	O
(	O
4	O
)	O
,	O
605–	O
623.	O
koller	O
,	O
d.	O
and	O
u.	O
lerner	O
(	O
2001	O
)	O
.	O
sam-	O
pling	O
in	O
factored	O
dynamic	O
sys-	O
tems	O
.	O
in	O
a.	O
doucet	O
,	O
n.	O
de	O
fre-	O
itas	O
,	O
and	O
n.	O
gordon	O
(	O
eds	O
.	O
)	O
,	O
sequen-	O
tial	O
monte	O
carlo	O
methods	O
in	O
prac-	O
tice	O
.	O
springer	O
.	O
kjaerulff	O
,	O
u	O
.	O
(	O
1990	O
)	O
.	O
triangulation	O
of	O
graphs	O
–	O
algorithms	O
giving	O
small	O
total	O
state	B
space	I
.	O
technical	O
report	O
r-90-09	O
,	O
dept	O
.	O
of	O
math	O
.	O
and	O
comp	O
.	O
sci.	O
,	O
aalborg	O
univ.	O
,	O
denmark	O
.	O
kjaerulff	O
,	O
u.	O
and	O
a.	O
madsen	O
(	O
2008	O
)	O
.	O
bayesian	O
networks	O
and	O
inﬂuence	O
diagrams	O
:	O
a	O
guide	O
to	O
construction	O
and	O
analysis	O
.	O
springer	O
.	O
klaassen	O
,	O
c.	O
and	O
j.	O
a.	O
wellner	O
(	O
1997	O
)	O
.	O
efficient	O
estimation	O
in	O
the	O
bivari-	O
ate	O
noramal	O
copula	O
model	O
:	O
nor-	O
mal	O
margins	O
are	O
least	O
favorable	O
.	O
bernoulli	O
3	O
(	O
1	O
)	O
,	O
55–77	O
.	O
klami	O
,	O
a.	O
and	O
s.	O
kaski	O
(	O
2008	O
)	O
.	O
proba-	O
bilistic	O
approach	O
to	O
detecting	O
de-	O
pendencies	O
between	O
data	O
sets	O
.	O
neurocomputing	O
72	O
,	O
39–46	O
.	O
klami	O
,	O
a.	O
,	O
s.	O
virtanen	O
,	O
and	O
s.	O
kaski	O
(	O
2010	O
)	O
.	O
bayesian	O
exponential	O
fam-	O
ily	O
projections	O
for	O
coupled	O
data	O
sources	O
.	O
in	O
uai	O
.	O
kleiner	O
,	O
a.	O
,	O
a.	O
talwalkar	O
,	O
p.	O
sarkar	O
,	O
and	O
m.	O
i.	O
jordan	O
(	O
2011	O
)	O
.	O
a	O
scalable	O
boot-	O
strap	O
for	O
massive	O
data	O
.	O
technical	O
report	O
,	O
uc	O
berkeley	O
.	O
kneser	O
,	O
r.	O
and	O
h.	O
ney	O
(	O
1995	O
)	O
.	O
im-	O
proved	O
backing-off	O
for	O
n-gram	B
lan-	O
guage	O
modeling	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
acoustics	O
,	O
speech	O
and	O
signal	O
proc.	O
,	O
volume	O
1	O
,	O
pp	O
.	O
181–184	O
.	O
ko	O
,	O
j.	O
and	O
d.	O
fox	O
(	O
2009	O
)	O
.	O
gp-	O
bayesfilters	O
:	O
bayesian	O
filtering	O
us-	O
ing	O
gaussian	O
process	O
prediction	O
and	O
observation	B
models	O
.	O
au-	O
tonomous	O
robots	O
journal	O
.	O
kohn	O
,	O
r.	O
,	O
m.	O
smith	O
,	O
and	O
d.	O
chan	O
(	O
2001	O
)	O
.	O
nonparametric	O
regression	B
using	O
linear	O
combinations	O
of	O
basis	B
functions	I
.	O
statistical	O
computing	O
11	O
,	O
313–322	O
.	O
koivisto	O
,	O
m.	O
(	O
2006	O
)	O
.	O
advances	O
in	O
ex-	O
act	O
bayesian	O
structure	O
discovery	O
in	O
bayesian	O
networks	O
.	O
in	O
uai	O
.	O
koivisto	O
,	O
m.	O
and	O
k.	O
sood	O
(	O
2004	O
)	O
.	O
ex-	O
act	O
bayesian	O
structure	O
discovery	O
in	O
bayesian	O
networks	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
5	O
,	O
549–573	O
.	O
kolmogorov	O
,	O
v.	O
(	O
2006	O
,	O
october	O
)	O
.	O
con-	O
vergent	O
tree-reweighted	O
message	B
passing	I
for	O
energy	O
minimization	O
.	O
ieee	O
trans	O
.	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
28	O
(	O
10	O
)	O
,	O
1568–	O
1583.	O
kolmogorov	O
,	O
v.	O
and	O
m.	O
wainwright	O
(	O
2005	O
)	O
.	O
on	O
optimality	O
properties	O
of	O
tree-reweighted	O
message	O
pass-	O
ing	O
.	O
in	O
uai	O
,	O
pp	O
.	O
316–322	O
.	O
kolmogorov	O
,	O
v.	O
and	O
r.	O
zabin	O
(	O
2004	O
)	O
.	O
what	O
energy	O
functions	O
can	O
be	O
min-	O
imized	O
via	O
graph	B
cuts	I
?	O
ieee	O
trans	O
.	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
26	O
(	O
2	O
)	O
,	O
147–159	O
.	O
komodakis	O
,	O
n.	O
,	O
n.	O
paragios	O
,	O
and	O
g.	O
tziritas	O
(	O
2011	O
)	O
.	O
mrf	O
energy	O
mini-	O
mization	O
and	O
beyond	O
via	O
dual	O
de-	O
composition	O
.	O
ieee	O
trans	O
.	O
on	O
pat-	O
tern	O
analysis	O
and	O
machine	O
intelli-	O
gence	O
33	O
(	O
3	O
)	O
,	O
531–552	O
.	O
koo	O
,	O
t.	O
,	O
a.	O
m.	O
rush	O
,	O
m.	O
collins	O
,	O
t.	O
jaakkola	O
,	O
and	O
d.	O
sontag	O
(	O
2010	O
)	O
.	O
dual	B
decomposition	I
for	O
parsing	O
with	O
non-projective	O
head	O
au-	O
tomata	O
.	O
in	O
proc	O
.	O
emnlp	O
,	O
pp	O
.	O
1288â	O
˘a	O
¸s1298	O
.	O
koren	O
,	O
y	O
.	O
(	O
2009a	O
)	O
.	O
the	O
bellkor	O
solution	O
to	O
the	O
netﬂix	O
grand	O
prize	O
.	O
techni-	O
cal	O
report	O
,	O
yahoo	O
!	O
research	O
.	O
koren	O
,	O
y	O
.	O
(	O
2009b	O
)	O
.	O
collaborative	O
ﬁl-	O
tering	O
with	O
temporal	O
dynamics	O
.	O
in	O
proc	O
.	O
of	O
the	O
int	O
’	O
l	O
conf	O
.	O
on	O
knowledge	B
discovery	I
and	O
data	O
mining	O
.	O
koren	O
,	O
y.	O
,	O
r.	O
bell	O
,	O
and	O
c.	O
volinsky	O
(	O
2009	O
)	O
.	O
matrix	B
factorization	I
tech-	O
niques	O
for	O
recommender	O
systems	O
.	O
ieee	O
computer	O
42	O
(	O
8	O
)	O
,	O
30–37	O
.	O
krishnapuram	O
,	O
l.	O
b.	O
,	O
carin	O
,	O
m.	O
figueiredo	O
,	O
and	O
a.	O
hartemink	O
(	O
2005	O
)	O
.	O
learning	B
sparse	O
bayesian	O
classiﬁers	O
:	O
multi-class	O
formulation	O
,	O
fast	O
algorithms	O
,	O
and	O
generalization	B
bounds	O
.	O
ieee	O
transaction	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
.	O
kschischang	O
,	O
f.	O
,	O
b.	O
frey	O
,	O
and	O
h.-a	O
.	O
loeliger	O
(	O
2001	O
,	O
february	O
)	O
.	O
factor	B
graphs	O
and	O
the	O
sum-product	B
algo-	O
rithm	O
.	O
ieee	O
trans	O
info	O
.	O
theory	O
.	O
kuan	O
,	O
p.	O
,	O
g.	O
pan	O
,	O
j.	O
a.	O
thomson	O
,	O
r.	O
stewart	O
,	O
and	O
s.	O
keles	O
(	O
2009	O
)	O
.	O
a	O
hierarchical	O
semi-markov	O
model	O
for	O
detecting	O
enrichment	O
with	O
ap-	O
plication	O
to	O
chip-seq	O
experiments	O
.	O
technical	O
report	O
,	O
u.	O
wisconsin	O
.	O
kulesza	O
,	O
a.	O
and	O
b.	O
taskar	O
(	O
2011	O
)	O
.	O
learn-	O
ing	O
determinantal	O
point	O
processes	O
.	O
in	O
uai	O
.	O
kumar	O
,	O
n.	O
and	O
a.	O
andreo	O
(	O
1998	O
)	O
.	O
het-	O
eroscedastic	O
discriminant	B
analysis	I
and	O
reduced	O
rank	O
hmms	O
for	O
im-	O
proved	O
speech	B
recognition	I
.	O
speech	O
communication	O
26	O
,	O
283–297	O
.	O
kumar	O
,	O
s.	O
and	O
m.	O
hebert	O
(	O
2003	O
)	O
.	O
dis-	O
criminative	O
random	O
ﬁelds	O
:	O
a	O
dis-	O
criminative	O
framework	O
for	O
contex-	O
tual	O
interaction	O
in	O
classiﬁcation	B
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
computer	O
vision	O
.	O
kuo	O
,	O
l.	O
and	O
b.	O
mallick	O
(	O
1998	O
)	O
.	O
vari-	O
able	O
selection	O
for	O
regression	B
mod-	O
els	O
.	O
sankhya	O
series	O
b	O
60	O
,	O
65–81	O
.	O
kurihara	O
,	O
k.	O
,	O
m.	O
welling	O
,	O
and	O
n.	O
vlas-	O
sis	O
(	O
2006	O
)	O
.	O
accelerated	O
variational	O
dp	O
mixture	B
models	O
.	O
in	O
nips	O
.	O
kushner	O
,	O
h.	O
and	O
g.	O
yin	O
(	O
2003	O
)	O
.	O
stochastic	B
approximation	I
and	O
recur-	O
sive	O
algorithms	O
and	O
applications	O
.	O
springer	O
.	O
kuss	O
and	O
c.	O
rasmussen	O
(	O
2005	O
)	O
.	O
as-	O
sessing	O
approximate	B
inference	I
for	O
binary	O
gaussian	O
process	O
classiﬁca-	O
tion	O
.	O
j.	O
of	O
machine	B
learning	I
re-	O
search	O
6	O
,	O
1679–1704	O
.	O
kwon	O
,	O
j.	O
and	O
k.	O
murphy	O
(	O
2000	O
)	O
.	O
mod-	O
eling	O
freeway	O
traffic	O
with	O
coupled	O
hmms	O
.	O
technical	O
report	O
,	O
univ	O
.	O
cal-	O
ifornia	O
,	O
berkeley	O
.	O
kyung	O
,	O
m.	O
,	O
j.	O
gill	O
,	O
m.	O
ghosh	O
,	O
and	O
g.	O
casella	O
(	O
2010	O
)	O
.	O
penalized	O
regres-	O
sion	O
,	O
standard	B
errors	I
and	O
bayesian	O
lassos	O
.	O
bayesian	O
analysis	O
5	O
(	O
2	O
)	O
,	O
369–	O
412.	O
lacoste-julien	O
,	O
s.	O
,	O
f.	O
huszar	O
,	O
and	O
z.	O
ghahramani	O
(	O
2011	O
)	O
.	O
approximate	B
inference	I
for	O
the	O
loss-calibrated	O
bayesian	O
.	O
in	O
ai/statistics	O
.	O
koller	O
,	O
d.	O
and	O
n.	O
friedman	O
(	O
2009	O
)	O
.	O
probabilistic	O
graphical	O
models	O
:	O
principles	O
and	O
techniques	O
.	O
mit	O
press	O
.	O
krizhevsky	O
,	O
a.	O
and	O
g.	O
hinton	O
(	O
2010	O
)	O
.	O
using	O
very	O
deep	B
autoencoders	O
for	O
content-based	O
image	O
retrieval	O
.	O
submitted	O
.	O
lacoste-julien	O
,	O
s.	O
,	O
f.	O
sha	O
,	O
and	O
m.	O
i.	O
jor-	O
dan	O
(	O
2009	O
)	O
.	O
disclda	O
:	O
discrimina-	O
tive	O
learning	B
for	O
dimensionality	O
re-	O
duction	O
and	O
classiﬁcation	B
.	O
in	O
nips	O
.	O
1030	O
bibliography	O
lafferty	O
,	O
j.	O
,	O
a.	O
mccallum	O
,	O
and	O
f.	O
pereira	O
(	O
2001	O
)	O
.	O
conditional	O
ran-	O
dom	O
ﬁelds	O
:	O
probabilistic	O
models	O
for	O
segmenting	O
and	O
labeling	O
se-	O
quence	O
data	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
ma-	O
chine	O
learning	B
.	O
lange	O
,	O
k.	O
,	O
r.	O
little	O
,	O
and	O
j.	O
taylor	O
(	O
1989	O
)	O
.	O
robust	B
statistical	O
modeling	O
using	O
the	O
t	O
disribution	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
84	O
(	O
408	O
)	O
,	O
881–896	O
.	O
langville	O
,	O
a.	O
and	O
c.	O
meyer	O
(	O
2006	O
)	O
.	O
updating	O
markov	O
chains	O
with	O
an	O
eye	O
on	O
google	O
’	O
s	O
pagerank	O
.	O
siam	O
j.	O
on	O
matrix	O
analysis	O
and	O
applica-	O
tions	O
27	O
(	O
4	O
)	O
,	O
968–987	O
.	O
larranaga	O
,	O
p.	O
,	O
c.	O
m.	O
h.	O
kuijpers	O
,	O
m.	O
poza	O
,	O
and	O
r.	O
h.	O
murga	O
(	O
1997	O
)	O
.	O
decomposing	O
bayesian	O
networks	O
:	O
triangulation	O
of	O
the	O
moral	O
graph	B
with	O
genetic	B
algorithms	I
.	O
statistics	O
and	O
computing	O
(	O
uk	O
)	O
7	O
(	O
1	O
)	O
,	O
19–34	O
.	O
lashkari	O
,	O
d.	O
and	O
p.	O
golland	O
(	O
2007	O
)	O
.	O
convex	B
clustering	O
with	O
examplar-	O
based	O
models	O
.	O
in	O
nips	O
.	O
lasserre	O
,	O
j.	O
,	O
c.	O
bishop	O
,	O
and	O
t.	O
minka	O
(	O
2006	O
)	O
.	O
principled	O
hybrids	O
of	O
gen-	O
erative	O
and	O
discriminative	B
models	O
.	O
in	O
cvpr	O
.	O
lau	O
,	O
j.	O
and	O
p.	O
green	O
(	O
2006	O
)	O
.	O
bayesian	O
model-based	B
clustering	I
procedures	O
.	O
journal	O
of	O
computa-	O
tional	O
and	O
graphical	O
statistics	O
12	O
,	O
351–357	O
.	O
lauritzen	O
,	O
s.	O
(	O
1996	O
)	O
.	O
graphical	O
models	O
.	O
oup	O
.	O
lauritzen	O
,	O
s.	O
causal	O
(	O
2000	O
)	O
.	O
infer-	O
ence	O
from	O
graphical	O
models	O
.	O
in	O
d.	O
r.	O
c.	O
o.	O
e.	O
barndoff-nielsen	O
and	O
c.	O
klueppelberg	O
(	O
eds	O
.	O
)	O
,	O
com-	O
plex	O
stochastic	O
systems	O
.	O
chapman	O
and	O
hall	O
.	O
lauritzen	O
,	O
s.	O
and	O
d.	O
nilsson	O
(	O
2001	O
)	O
.	O
representing	O
and	O
solving	O
decision	B
problems	O
with	O
limited	O
information	O
.	O
management	O
science	O
47	O
,	O
1238–1251	O
.	O
lauritzen	O
,	O
s.	O
l.	O
(	O
1992	O
,	O
december	O
)	O
.	O
propagation	O
of	O
probabilities	O
,	O
means	O
and	O
variances	O
in	O
mixed	O
graphical	O
association	O
models	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
87	O
(	O
420	O
)	O
,	O
1098–1108	O
.	O
lauritzen	O
,	O
s.	O
l.	O
and	O
d.	O
j.	O
spiegelhal-	O
ter	O
(	O
1988	O
)	O
.	O
local	O
computations	O
with	O
probabilities	O
on	O
graphical	O
struc-	O
tures	O
and	O
their	O
applications	O
to	O
ex-	O
pert	O
systems	O
.	O
j.	O
r.	O
stat	O
.	O
soc	O
.	O
b	O
b	O
(	O
50	O
)	O
,	O
127–224	O
.	O
law	O
,	O
e.	O
,	O
b.	O
settles	O
,	O
and	O
t.	O
mitchell	O
(	O
2010	O
)	O
.	O
learning	O
to	O
tag	O
from	O
open	O
vocabulary	O
labels	O
.	O
in	O
proc	O
.	O
euro-	O
pean	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
law	O
,	O
m.	O
,	O
m.	O
figueiredo	O
,	O
and	O
a.	O
jain	O
(	O
2004	O
)	O
.	O
simultaneous	O
feature	O
se-	O
lection	O
and	O
clustering	B
using	O
mix-	O
ture	O
models	O
.	O
ieee	O
trans	O
.	O
on	O
pat-	O
tern	O
analysis	O
and	O
machine	O
intelli-	O
gence	O
26	O
(	O
4	O
)	O
.	O
lawrence	O
,	O
n.	O
d.	O
(	O
2005	O
)	O
.	O
probabilis-	O
tic	O
non-linear	O
principal	B
component	I
analysis	O
with	O
gaussian	O
process	O
la-	O
tent	O
variable	O
models	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
6	O
,	O
1783–1816	O
.	O
lawrence	O
,	O
n.	O
d.	O
(	O
2012	O
)	O
.	O
a	O
unifying	O
probabilistic	O
perspective	O
for	O
spec-	O
in-	O
tral	O
dimensionality	B
reduction	I
:	O
sights	O
and	O
new	O
models	O
.	O
j.	O
of	O
ma-	O
chine	O
learning	B
research	O
13	O
,	O
1609–	O
1638.	O
learned-miller	O
,	O
e.	O
(	O
2004	O
)	O
.	O
hyperspac-	O
ings	O
and	O
the	O
estimation	O
of	O
infor-	O
mation	O
theoretic	O
quantities	O
.	O
tech-	O
nical	O
report	O
04-104	O
,	O
u.	O
mass	O
.	O
amherst	O
comp	O
.	O
sci	O
.	O
dept	O
.	O
lecun	O
,	O
y.	O
,	O
b.	O
boser	O
,	O
j.	O
s.	O
denker	O
,	O
d.	O
henderson	O
,	O
r.	O
e.	O
howard	O
,	O
w.	O
hubbard	O
,	O
and	O
l.	O
d.	O
jackel	O
(	O
1989	O
,	O
winter	O
)	O
.	O
backpropagation	B
applied	O
to	O
handwritten	O
zip	O
code	O
recogni-	O
tion	O
.	O
neural	O
computation	O
1	O
(	O
4	O
)	O
,	O
541–	O
551.	O
lecun	O
,	O
y.	O
,	O
l.	O
bottou	O
,	O
y.	O
bengio	O
,	O
and	O
p.	O
haffner	O
(	O
1998	O
,	O
november	O
)	O
.	O
gradient-based	O
learning	B
applied	O
to	O
document	O
recognition	O
.	O
proceedings	O
of	O
the	O
ieee	O
86	O
(	O
11	O
)	O
,	O
2278–2324	O
.	O
lecun	O
,	O
y.	O
,	O
s.	O
chopra	O
,	O
r.	O
hadsell	O
,	O
f.-j	O
.	O
huang	O
,	O
and	O
m.-a	O
.	O
ranzato	O
(	O
2006	O
)	O
.	O
a	O
tutorial	O
on	O
energy-based	O
learn-	O
ing	O
.	O
(	O
ed	O
.	O
)	O
,	O
predicting	O
structured	O
outputs	O
.	O
mit	O
press	O
.	O
in	O
b.	O
et	O
al	O
.	O
ledoit	O
,	O
o.	O
and	O
m.	O
wolf	O
(	O
2004a	O
)	O
.	O
honey	O
,	O
i	O
shrunk	O
the	O
sample	O
covariance	O
matrix	O
.	O
j.	O
of	O
portfolio	O
manage-	O
ment	O
31	O
(	O
1	O
)	O
.	O
lee	O
,	O
a.	O
,	O
f.	O
caron	O
,	O
a.	O
doucet	O
,	O
and	O
c.	O
holmes	O
(	O
2010	O
)	O
.	O
a	O
hierarchical	O
bayesian	O
framework	O
for	O
construct-	O
ing	O
sparsity-inducing	O
priors	O
.	O
tech-	O
nical	O
report	O
,	O
u.	O
oxford	O
.	O
lee	O
,	O
a.	O
,	O
f.	O
caron	O
,	O
a.	O
doucet	O
,	O
and	O
c.	O
holmes	O
(	O
2011	O
)	O
.	O
bayesian	O
sparsity-	O
path-analysis	O
of	O
genetic	O
associ-	O
ation	O
signal	O
using	O
generalized	O
t	O
prior	O
.	O
technical	O
report	O
,	O
u.	O
oxford	O
.	O
lee	O
,	O
d.	O
and	O
s.	O
seung	O
(	O
2001	O
)	O
.	O
algo-	O
rithms	O
for	O
non-negative	O
matrix	O
fac-	O
torization	O
.	O
in	O
nips	O
.	O
lee	O
,	O
h.	O
,	O
r.	O
grosse	O
,	O
r.	O
ranganath	O
,	O
and	O
a.	O
ng	O
(	O
2009	O
)	O
.	O
convolutional	O
deep	O
belief	B
networks	I
for	O
scalable	O
un-	O
supervised	B
learning	I
of	O
hierarchical	O
representations	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
lee	O
,	O
h.	O
,	O
y.	O
largman	O
,	O
p.	O
pham	O
,	O
and	O
a.	O
ng	O
(	O
2009	O
)	O
.	O
unsupervised	O
feature	O
learning	B
for	O
audio	O
classiﬁcation	B
us-	O
ing	O
convolutional	O
deep	O
belief	O
net-	O
works	O
.	O
in	O
nips	O
.	O
lee	O
,	O
s.-i.	O
,	O
v.	O
ganapathi	O
,	O
and	O
d.	O
koller	O
(	O
2006	O
)	O
.	O
efficient	O
structure	O
learn-	O
ing	O
of	O
markov	O
networks	O
using	O
l1-	O
regularization	B
.	O
in	O
nips	O
.	O
lee	O
,	O
t.	O
s.	O
and	O
d.	O
mumford	O
(	O
2003	O
)	O
.	O
hi-	O
erarchical	O
bayesian	O
inference	B
in	O
the	O
visual	O
cortex	O
.	O
j.	O
of	O
optical	O
society	O
of	O
america	O
a	O
20	O
(	O
7	O
)	O
,	O
1434–1448	O
.	O
lenk	O
,	O
p.	O
,	O
w.	O
s.	O
desarbo	O
,	O
p.	O
green	O
,	O
and	O
m.	O
young	O
(	O
1996	O
)	O
.	O
hierarchi-	O
cal	O
bayes	O
conjoint	B
analysis	I
:	O
re-	O
covery	O
of	O
partworth	O
heterogeneity	O
from	O
reduced	O
experimental	O
de-	O
signs	O
.	O
marketing	O
science	O
15	O
(	O
2	O
)	O
,	O
173–	O
191.	O
lenkoski	O
,	O
a.	O
and	O
a.	O
dobra	O
(	O
2008	O
)	O
.	O
bayesian	O
structural	O
learning	O
and	O
estimation	O
in	O
gaussian	O
graphical	O
models	O
.	O
technical	O
report	O
545	O
,	O
de-	O
partment	O
of	O
statistics	O
,	O
university	O
of	O
washington	O
.	O
of	O
comparison	O
lepar	O
,	O
v.	O
and	O
p.	O
p.	O
shenoy	O
(	O
1998	O
)	O
.	O
a	O
lauritzen-	O
spiegelhalter	O
,	O
hugin	O
and	O
shenoy-	O
shafer	O
architectures	O
for	O
computing	O
marginals	O
of	O
probability	O
distribu-	O
tions	O
.	O
in	O
g.	O
cooper	O
and	O
s.	O
moral	O
(	O
eds	O
.	O
)	O
,	O
uai	O
,	O
pp	O
.	O
328–337	O
.	O
morgan	O
kaufmann	O
.	O
lauritzen	O
,	O
s.	O
l.	O
(	O
1995	O
)	O
.	O
the	O
em	O
al-	O
gorithm	O
for	O
graphical	O
association	O
models	O
with	O
missing	B
data	I
.	O
com-	O
putational	O
statistics	O
and	O
data	O
anal-	O
ysis	O
19	O
,	O
191–201	O
.	O
ledoit	O
,	O
o.	O
and	O
m.	O
wolf	O
(	O
2004b	O
)	O
.	O
a	O
well-	O
conditioned	O
estimator	B
large-	O
dimensional	O
covariance	B
matrices	O
.	O
j.	O
of	O
multivariate	O
analysis	O
88	O
(	O
2	O
)	O
,	O
365–	O
411.	O
for	O
lerner	O
,	O
u.	O
and	O
r.	O
parr	O
(	O
2001	O
)	O
.	O
infer-	O
ence	O
in	O
hybrid	O
networks	O
:	O
theoreti-	O
cal	O
limits	O
and	O
practical	O
algorithms	O
.	O
in	O
uai	O
.	O
bibliography	O
1031	O
leslie	O
,	O
c.	O
,	O
e.	O
eskin	O
,	O
a.	O
cohen	O
,	O
j.	O
we-	O
ston	O
,	O
and	O
w.	O
noble	O
(	O
2003	O
)	O
.	O
mis-	O
match	O
string	O
kernels	O
for	O
discrimi-	O
native	O
protein	O
classiﬁcation	B
.	O
bioin-	O
formatics	O
1	O
,	O
1–10	O
.	O
levy	O
,	O
s.	O
(	O
2011	O
)	O
.	O
in	O
the	O
plex	O
:	O
how	O
google	O
thinks	O
,	O
works	O
,	O
and	O
shapes	O
our	O
lives	O
.	O
simon	O
&	O
schuster	O
.	O
li	O
,	O
l.	O
,	O
w.	O
chu	O
,	O
j.	O
langford	O
,	O
and	O
x.	O
wang	O
(	O
2011	O
)	O
.	O
unbiased	B
offline	O
evaluation	O
of	O
contextual-bandit-	O
based	O
news	O
article	O
recommenda-	O
tion	O
algorithms	O
.	O
in	O
wsdm	O
.	O
liang	O
,	O
f.	O
,	O
s.	O
mukherjee	O
,	O
and	O
m.	O
west	O
(	O
2007	O
)	O
.	O
understanding	O
the	O
use	O
of	O
unlabelled	O
data	O
in	O
predictive	B
mod-	O
elling	O
.	O
statistical	O
science	O
22	O
,	O
189–	O
205.	O
liang	O
,	O
f.	O
,	O
r.	O
paulo	O
,	O
g.	O
molina	O
,	O
m.	O
clyde	O
,	O
and	O
j.	O
berger	O
(	O
2008	O
)	O
.	O
mix-	O
tures	O
of	O
g-priors	O
for	O
bayesian	O
vari-	O
able	O
selection	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
103	O
(	O
481	O
)	O
,	O
410–423	O
.	O
i.	O
liang	O
,	O
p.	O
and	O
m.	O
jordan	O
(	O
2008	O
)	O
.	O
an	O
asymptotic	O
analysis	O
of	O
gen-	O
erative	O
,	O
discriminative	B
,	O
and	O
pseu-	O
dolikelihood	O
estimators	O
.	O
in	O
in-	O
ternational	O
conference	O
on	O
machine	B
learning	I
(	O
icml	O
)	O
.	O
liang	O
,	O
p.	O
and	O
d.	O
klein	O
.	O
online	O
em	O
in	O
proc	O
.	O
for	O
unsupervised	O
models	O
.	O
naacl	O
conference	O
.	O
liao	O
,	O
l.	O
,	O
d.	O
j.	O
patterson	O
,	O
d.	O
fox	O
,	O
and	O
h.	O
kautz	O
(	O
2007	O
)	O
.	O
learning	B
and	O
inferring	O
transportation	O
routines	O
.	O
artiﬁcial	O
intelligence	O
171	O
(	O
5	O
)	O
,	O
311–331	O
.	O
lindley	O
,	O
d.	O
(	O
1982	O
)	O
.	O
scoring	O
rules	B
and	O
isi	O
the	O
inevetability	O
of	O
probability	O
.	O
review	O
50	O
,	O
1–26	O
.	O
lindley	O
,	O
d.	O
v.	O
(	O
1972	O
)	O
.	O
bayesian	O
statistics	O
:	O
a	O
review	O
.	O
siam	O
.	O
lindley	O
,	O
d.	O
v.	O
and	O
l.	O
d.	O
phillips	O
(	O
1976	O
)	O
.	O
inference	B
for	O
a	O
bernoulli	O
process	O
(	O
a	O
bayesian	O
view	O
)	O
.	O
the	O
american	O
statistician	O
30	O
(	O
3	O
)	O
,	O
112–119	O
.	O
lindsay	O
,	O
b	O
.	O
(	O
1988	O
)	O
.	O
composite	O
like-	O
contemporary	O
lihood	O
methods	O
.	O
mathematics	O
80	O
(	O
1	O
)	O
,	O
221–239	O
.	O
lipton	O
,	O
r.	O
j.	O
and	O
r.	O
e.	O
tarjan	O
(	O
1979	O
)	O
.	O
theorem	O
for	O
planar	O
siam	O
journal	O
of	O
applied	O
a	O
separator	O
graphs	O
.	O
math	O
36	O
,	O
177–189	O
.	O
little.	O
,	O
r.	O
j.	O
and	O
d.	O
b.	O
rubin	O
(	O
1987	O
)	O
.	O
sta-	O
tistical	O
analysis	O
with	O
missing	B
data	I
.	O
new	O
york	O
:	O
wiley	O
and	O
son	O
.	O
liu	O
,	O
c.	O
and	O
d.	O
rubin	O
(	O
1995	O
)	O
.	O
ml	O
esti-	O
mation	O
of	O
the	O
t	O
distribution	O
using	O
em	O
and	O
its	O
extensions	O
,	O
ecm	O
and	O
ecme	O
.	O
statistica	O
sinica	O
5	O
,	O
19–39	O
.	O
liu	O
,	O
h.	O
,	O
j.	O
lafferty	O
,	O
and	O
l.	O
wasserman	O
(	O
2009	O
)	O
.	O
the	O
nonparanormal	B
:	O
semi-	O
parametric	O
estimation	O
of	O
high	O
di-	O
mensional	O
undirected	B
graphs	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
10	O
,	O
2295–2328	O
.	O
liu	O
,	O
j	O
.	O
(	O
2001	O
)	O
.	O
monte	O
carlo	O
strategies	O
in	O
scientiﬁc	O
computation	O
.	O
springer	O
.	O
liu	O
,	O
j.	O
s.	O
,	O
w.	O
h.	O
wong	O
,	O
and	O
a.	O
kong	O
(	O
1994	O
)	O
.	O
covariance	B
structure	O
of	O
the	O
gibbs	O
sampler	O
with	O
applications	O
to	O
the	O
comparisons	O
of	O
estima-	O
tors	O
and	O
augmentation	O
schemes	O
.	O
biometrika	O
81	O
(	O
1	O
)	O
,	O
27–40	O
.	O
liu	O
,	O
t.-y	O
.	O
(	O
2009	O
)	O
.	O
learning	B
to	I
rank	I
for	O
information	B
retrieval	I
.	O
founda-	O
tions	O
and	O
trends	O
in	O
information	B
re-	O
trieval	O
3	O
(	O
3	O
)	O
,	O
225–331	O
.	O
lizotte	O
,	O
d.	O
(	O
2008	O
)	O
.	O
practical	O
bayesian	O
optimization	B
.	O
ph.d.	O
thesis	O
,	O
u.	O
al-	O
berta	O
.	O
ljung	O
,	O
l.	O
(	O
1987	O
)	O
.	O
system	O
identiﬁciation	O
:	O
theory	O
for	O
the	O
user	O
.	O
prentice	O
hall	O
.	O
lo	O
,	O
c.	O
h.	O
(	O
2009	O
)	O
.	O
statistical	O
methods	O
for	O
high	B
throughput	I
genomics	O
.	O
ph.d.	O
thesis	O
,	O
ubc	O
.	O
lo	O
,	O
k.	O
,	O
f.	O
hahne	O
,	O
r.	O
brinkman	O
,	O
r.	O
ryan	O
,	O
and	O
r.	O
gottardo	O
(	O
2009	O
,	O
may	O
)	O
.	O
ﬂow-	O
clust	O
:	O
a	O
bioconductor	O
package	O
for	O
automated	O
gating	O
of	O
ﬂow	O
cytome-	O
try	O
data	O
.	O
bmc	O
bioinformatics	O
10	O
,	O
145+	O
.	O
lopes	O
,	O
h.	O
and	O
m.	O
west	O
(	O
2004	O
)	O
.	O
bayesian	O
model	O
assessment	O
in	O
fac-	O
tor	O
analysis	O
.	O
statisica	O
sinica	O
14	O
,	O
41–	O
67.	O
lowe	O
,	O
d.	O
g.	O
(	O
1999	O
)	O
.	O
object	O
recognition	O
from	O
local	O
scale-invariant	O
features	B
.	O
in	O
proc	O
.	O
of	O
the	O
international	O
con-	O
ference	O
on	O
computer	O
vision	O
iccv	O
,	O
corfu	O
,	O
pp	O
.	O
1150–1157	O
.	O
luce	O
,	O
r.	O
(	O
1959	O
)	O
.	O
individual	O
choice	O
be-	O
havior	O
:	O
a	O
theoretical	O
analysis	O
.	O
wi-	O
ley	O
.	O
lunn	O
,	O
d.	O
,	O
n.	O
best	O
,	O
and	O
j.	O
whit-	O
taker	O
(	O
2009	O
)	O
.	O
generic	O
reversible	O
jump	O
mcmc	O
using	O
graphical	O
mod-	O
els	O
.	O
statistics	O
and	O
computing	O
19	O
(	O
4	O
)	O
,	O
395–408	O
.	O
lunn	O
,	O
d.	O
,	O
a.	O
thomas	O
,	O
n.	O
best	O
,	O
and	O
d.	O
spiegelhalter	O
(	O
2000	O
)	O
.	O
winbugs	O
–	O
a	O
bayesian	O
modelling	O
framework	O
:	O
concepts	O
,	O
structure	O
,	O
and	O
extensibil-	O
ity	O
.	O
statistics	O
and	O
computing	O
10	O
,	O
325–337	O
.	O
ma	O
,	O
h.	O
,	O
h.	O
yang	O
,	O
m.	O
lyu	O
,	O
and	O
i.	O
king	O
(	O
2008	O
)	O
.	O
sorec	O
:	O
social	O
recommenda-	O
tion	O
using	O
probabilistic	O
matrix	O
fac-	O
torization	O
.	O
in	O
proc	O
.	O
of	O
17th	O
conf	O
.	O
on	O
information	B
and	O
knowledge	O
man-	O
agement	O
.	O
ma	O
,	O
s.	O
,	O
c.	O
ji	O
,	O
and	O
j.	O
farmer	O
(	O
1997	O
)	O
.	O
an	O
efficient	O
em-based	O
training	O
algo-	O
rithm	O
for	O
feedforward	O
neural	O
net-	O
works	O
.	O
neural	B
networks	I
10	O
(	O
2	O
)	O
,	O
243–	O
256.	O
maathuis	O
,	O
m.	O
,	O
d.	O
colombo	O
,	O
m.	O
kalisch	O
,	O
and	O
p.	O
bãijhlmann	O
(	O
2010	O
)	O
.	O
pre-	O
dicting	O
causal	O
effects	O
in	O
large-scale	O
systems	O
from	O
observational	O
data	O
.	O
nature	O
methods	O
7	O
,	O
247–248	O
.	O
maathuis	O
,	O
m.	O
,	O
m.	O
kalisch	O
,	O
and	O
p.	O
bãijhlmann	O
(	O
2009	O
)	O
.	O
estimating	O
high-dimensional	O
intervention	O
ef-	O
fects	O
from	O
observational	O
data	O
.	O
an-	O
nals	O
of	O
statistics	O
37	O
,	O
3133–3164	O
.	O
mackay	O
,	O
d.	O
(	O
1992	O
)	O
.	O
bayesian	O
interpo-	O
lation	O
.	O
neural	O
computation	O
4	O
,	O
415–	O
447.	O
mackay	O
,	O
d.	O
(	O
1995a	O
)	O
.	O
developments	O
in	O
probabilistic	O
modeling	O
with	O
neural	B
networks	I
—	O
ensemble	B
learning	I
.	O
in	O
proc	O
.	O
3rd	O
ann	O
.	O
symp	O
.	O
neural	O
net-	O
works	O
.	O
mackay	O
,	O
d.	O
(	O
1995b	O
)	O
.	O
probable	O
net-	O
works	O
and	O
plausible	O
predictions	O
—	O
a	O
review	O
of	O
practical	O
bayesian	O
methods	O
for	O
supervised	O
neural	O
net-	O
works	O
.	O
network	O
.	O
mackay	O
,	O
d.	O
(	O
1997	O
)	O
.	O
ensemble	B
learning	I
for	O
hidden	B
markov	O
models	O
.	O
tech-	O
nical	O
report	O
,	O
u.	O
cambridge	O
.	O
mackay	O
,	O
d.	O
(	O
1999	O
)	O
.	O
comparision	O
of	O
approximate	O
methods	O
for	O
handling	O
hyperparameters	O
.	O
neural	O
computa-	O
tion	O
11	O
(	O
5	O
)	O
,	O
1035–1068	O
.	O
mackay	O
,	O
d.	O
(	O
2003	O
)	O
.	O
information	B
theory	I
,	O
inference	B
,	O
and	O
learning	B
algorithms	O
.	O
cambridge	O
university	O
press	O
.	O
macnaughton-smith	O
,	O
p.	O
,	O
w.	O
t.	O
williams	O
,	O
m.	O
b.	O
dale	O
,	O
and	O
g.	O
mock-	O
ett	O
(	O
1964	O
)	O
.	O
dissimilarity	B
analysis	I
:	O
a	O
new	O
technique	O
of	O
hierarchical	O
sub-division	O
.	O
nature	O
202	O
,	O
1034	O
–	O
1035	O
.	O
1032	O
bibliography	O
madeira	O
,	O
s.	O
c.	O
and	O
a.	O
l.	O
oliveira	O
(	O
2004	O
)	O
.	O
biclustering	B
algorithms	O
for	O
biological	O
data	O
analysis	O
:	O
a	O
survey	O
.	O
ieee/acm	O
transactions	O
on	O
compu-	O
tational	O
biology	O
and	O
bioinformat-	O
ics	O
1	O
(	O
1	O
)	O
,	O
24–45	O
.	O
madigan	O
,	O
d.	O
and	O
a.	O
raftery	O
(	O
1994	O
)	O
.	O
model	B
selection	I
and	O
accounting	O
for	O
model	O
uncertainty	O
in	O
graphical	O
models	O
using	O
occam	O
’	O
s	O
window	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
89	O
,	O
1535–1546	O
.	O
madsen	O
,	O
r.	O
,	O
d.	O
kauchak	O
,	O
and	O
c.	O
elkan	O
(	O
2005	O
)	O
.	O
modeling	O
word	O
burstiness	B
using	O
the	O
dirichlet	O
distribution	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
mairal	O
,	O
j.	O
,	O
f.	O
bach	O
,	O
j.	O
ponce	O
,	O
and	O
g.	O
sapiro	O
(	O
2010	O
)	O
.	O
online	B
learning	I
for	O
matrix	B
factorization	I
and	O
sparse	B
coding	I
.	O
j.	O
of	O
machine	B
learning	I
re-	O
search	O
11	O
,	O
19–60	O
.	O
mairal	O
,	O
j.	O
,	O
m.	O
elad	O
,	O
and	O
g.	O
sapiro	O
(	O
2008	O
)	O
.	O
sparse	B
representation	I
for	O
color	O
image	O
restoration	O
.	O
ieee	O
trans	O
.	O
on	O
image	O
processing	O
17	O
(	O
1	O
)	O
,	O
53–69	O
.	O
malioutov	O
,	O
d.	O
,	O
j.	O
johnson	O
,	O
and	O
a.	O
will-	O
sky	O
(	O
2006	O
)	O
.	O
walk-sums	O
and	O
belief	B
propagation	I
in	O
gaussian	O
graphical	O
models	O
.	O
j.	O
of	O
machine	B
learning	I
re-	O
search	O
7	O
,	O
2003–2030	O
.	O
july	O
)	O
.	O
mallat	O
,	O
s.	O
,	O
g.	O
davis	O
,	O
and	O
z.	O
zhang	O
time-	O
(	O
1994	O
,	O
adaptive	O
frequency	O
decompositions	O
.	O
spie	O
journal	O
of	O
optical	O
engineering	O
33	O
,	O
2183–2919	O
.	O
mallat	O
,	O
s.	O
and	O
z.	O
zhang	O
(	O
1993	O
)	O
.	O
match-	O
ing	O
pursuits	O
with	O
time-frequency	O
dictionaries	O
.	O
ieee	O
transactions	O
on	O
signal	B
processing	I
41	O
(	O
12	O
)	O
,	O
3397–3415	O
.	O
malouf	O
,	O
r.	O
(	O
2002	O
)	O
.	O
a	O
comparison	O
of	O
algorithms	O
for	O
maximum	B
entropy	I
parameter	O
estimation	O
.	O
in	O
proc	O
.	O
sixth	O
conference	O
on	O
natural	O
lan-	O
guage	O
learning	B
(	O
conll-2002	O
)	O
,	O
pp	O
.	O
49–55	O
.	O
manning	O
,	O
c.	O
,	O
p.	O
raghavan	O
,	O
and	O
h.	O
schuetze	O
(	O
2008	O
)	O
.	O
introduction	O
to	O
information	B
retrieval	I
.	O
cambridge	O
university	O
press	O
.	O
manning	O
,	O
c.	O
and	O
h.	O
schuetze	O
(	O
1999	O
)	O
.	O
statistical	O
natural	O
foundations	O
of	O
language	O
processing	O
.	O
mit	O
press	O
.	O
mansinghka	O
,	O
v.	O
,	O
d.	O
roy	O
,	O
r.	O
rifkin	O
,	O
and	O
j.	O
tenenbaum	O
(	O
2007	O
)	O
.	O
aclass	O
:	O
an	O
online	O
algorithm	O
for	O
generative	O
classiﬁcation	O
.	O
in	O
ai/statistics	O
.	O
mansinghka	O
,	O
v.	O
,	O
p.	O
shafto	O
,	O
e.	O
cross-categorization	O
:	O
jonas	O
,	O
c.	O
petschulat	O
,	O
and	O
j.	O
tenenbaum	O
(	O
2011	O
)	O
.	O
a	O
nonparametric	O
bayesian	O
method	O
for	O
modeling	O
heterogeneous	O
,	O
high	O
dimensional	O
data	O
.	O
technical	O
re-	O
port	O
,	O
mit	O
.	O
margolin	O
,	O
a.	O
,	O
i.	O
nemenman	O
,	O
k.	O
basso	O
,	O
c.	O
wiggins	O
,	O
g.	O
stolovitzky	O
,	O
and	O
r.	O
f.	O
abd	O
a.	O
califano	O
(	O
2006	O
)	O
.	O
aracne	O
:	O
an	O
algorithm	O
for	O
the	O
reconstruc-	O
tion	O
of	O
gene	O
regulatory	O
networks	O
in	O
a	O
mammalian	O
cellular	O
context	O
.	O
bmc	O
bionformatics	O
7.	O
marin	O
,	O
j.-m.	O
and	O
c.	O
robert	O
(	O
2007	O
)	O
.	O
bayesian	O
core	O
:	O
a	O
practical	O
approach	O
to	O
computational	O
bayesian	O
statistics	O
.	O
springer	O
.	O
marks	O
,	O
t.	O
k.	O
and	O
j.	O
r.	O
movellan	O
(	O
2001	O
)	O
.	O
diffusion	O
networks	O
,	O
products	O
of	O
ex-	O
perts	O
,	O
and	O
factor	B
analysis	I
.	O
techni-	O
cal	O
report	O
,	O
university	O
of	O
california	O
san	O
diego	O
.	O
marlin	O
,	O
b	O
.	O
(	O
2003	O
)	O
.	O
modeling	O
user	O
rat-	O
ing	O
proﬁles	O
for	O
collaborative	O
ﬁlter-	O
ing	O
.	O
in	O
nips	O
.	O
marlin	O
,	O
b	O
.	O
(	O
2008	O
)	O
.	O
missing	B
data	I
prob-	O
lems	O
in	O
machine	B
learning	I
.	O
ph.d.	O
thesis	O
,	O
u.	O
toronto	O
.	O
marlin	O
,	O
b.	O
,	O
e.	O
khan	O
,	O
and	O
k.	O
murphy	O
(	O
2011	O
)	O
.	O
piecewise	O
bounds	O
for	O
es-	O
timating	O
bernoulli-logistic	O
latent	B
gaussian	O
models	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
marlin	O
,	O
b.	O
and	O
r.	O
zemel	O
(	O
2009	O
)	O
.	O
collab-	O
orative	O
prediction	O
and	O
ranking	B
with	O
non-random	O
missing	B
data	I
.	O
in	O
proc	O
.	O
of	O
the	O
3rd	O
acm	O
conference	O
on	O
rec-	O
ommender	O
systems	O
.	O
marlin	O
,	O
b.	O
m.	O
,	O
k.	O
swersky	O
,	O
b.	O
chen	O
,	O
and	O
n.	O
de	O
freitas	O
(	O
2010	O
)	O
.	O
inductive	O
prin-	O
ciples	O
for	O
restricted	O
boltzmann	O
ma-	O
chine	O
learning	B
.	O
in	O
ai/statistics	O
.	O
marroquin	O
,	O
j.	O
,	O
s.	O
mitter	O
,	O
and	O
t.	O
pog-	O
gio	O
(	O
1987	O
)	O
.	O
probabilistic	O
solution	O
of	O
ill-posed	O
problems	O
in	O
computa-	O
tional	O
vision	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
as-	O
soc	O
.	O
82	O
(	O
297	O
)	O
,	O
76–89	O
.	O
martens	O
,	O
j	O
.	O
(	O
2010	O
)	O
.	O
deep	B
learning	I
via	O
in	O
intl	O
.	O
hessian-free	O
optimization	B
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
maruyama	O
,	O
y.	O
and	O
e.	O
george	O
(	O
2008	O
)	O
.	O
a	O
g-prior	B
extension	O
for	O
p	O
>	O
n.	O
tech-	O
nical	O
report	O
,	O
u.	O
tokyo	O
.	O
mason	O
,	O
l.	O
,	O
j.	O
baxter	O
,	O
p.	O
bartlett	O
,	O
and	O
boosting	B
algo-	O
in	O
m.	O
frean	O
(	O
2000	O
)	O
.	O
rithms	O
as	O
gradient	B
descent	I
.	O
nips	O
,	O
volume	O
12	O
,	O
pp	O
.	O
512–518	O
.	O
matthews	O
,	O
r.	O
(	O
1998	O
)	O
.	O
bayesian	O
critique	O
of	O
statistics	O
in	O
health	O
:	O
the	O
great	O
health	O
hoax	O
.	O
maybeck	O
,	O
p.	O
(	O
1979	O
)	O
.	O
stochastic	O
models	O
,	O
estimation	O
,	O
and	O
control	O
.	O
academic	O
press	O
.	O
mazumder	O
,	O
r.	O
and	O
t.	O
hastie	O
(	O
2012	O
)	O
.	O
the	O
graphical	B
lasso	I
:	O
new	O
insights	O
and	O
alternatives	O
.	O
technical	O
report	O
.	O
mcauliffe	O
,	O
j.	O
,	O
d.	O
blei	O
,	O
and	O
m.	O
jordan	O
(	O
2006	O
)	O
.	O
nonparametric	O
empirical	O
bayes	O
for	O
the	O
dirichlet	O
process	O
mix-	O
ture	O
model	O
.	O
statistics	O
and	O
comput-	O
ing	O
16	O
(	O
1	O
)	O
,	O
5–14	O
.	O
mccallum	O
,	O
a	O
.	O
(	O
2003	O
)	O
.	O
efficiently	O
induc-	O
ing	O
features	B
of	O
conditional	B
random	I
ﬁelds	I
.	O
in	O
uai	O
.	O
mccallum	O
,	O
freitag	O
,	O
a.	O
,	O
d.	O
and	O
f.	O
pereira	O
(	O
2000	O
)	O
.	O
maximum	O
en-	O
tropy	O
markov	O
models	O
for	O
informa-	O
tion	O
extraction	O
and	O
segmentation	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
mccallum	O
,	O
a.	O
and	O
k.	O
nigam	O
(	O
1998	O
)	O
.	O
a	O
comparison	O
of	O
event	O
models	O
for	O
naive	O
bayes	O
text	O
classiﬁcation	B
.	O
in	O
aaai/icml	O
workshop	O
on	O
learning	B
for	O
text	O
categorization	O
.	O
mccray	O
,	O
a	O
.	O
(	O
2003	O
)	O
.	O
an	O
upper	O
level	O
ontology	O
for	O
the	O
biomedical	O
do-	O
main	O
.	O
comparative	O
and	O
functional	O
genomics	O
4	O
,	O
80–84	O
.	O
mccullagh	O
,	O
p.	O
and	O
j.	O
nelder	O
(	O
1989	O
)	O
.	O
generalized	B
linear	I
models	I
.	O
chap-	O
man	O
and	O
hall	O
.	O
2nd	O
edition	O
.	O
mccullich	O
,	O
w.	O
and	O
w.	O
pitts	O
(	O
1943	O
)	O
.	O
a	O
logical	O
calculus	O
of	O
the	O
ideas	O
imma-	O
nent	O
in	O
nervous	O
activity	O
.	O
bulletin	O
of	O
mathematical	O
biophysics	O
5	O
,	O
115–137	O
.	O
mcdonald	O
,	O
j.	O
and	O
w.	O
newey	O
(	O
1988	O
)	O
.	O
partially	O
adaptive	O
estimation	O
of	O
re-	O
gression	O
models	O
via	O
the	O
general-	O
ized	O
t	O
distribution	O
.	O
econometric	O
theory	O
4	O
(	O
3	O
)	O
,	O
428–445	O
.	O
mceliece	O
,	O
r.	O
j.	O
,	O
d.	O
j.	O
c.	O
mackay	O
,	O
and	O
j.	O
f.	O
cheng	O
(	O
1998	O
)	O
.	O
turbo	O
decod-	O
ing	O
as	O
an	O
instance	O
of	O
pearl	O
’	O
s	O
’	O
belief	B
propagation	I
’	O
algorithm	O
.	O
ieee	O
j.	O
on	O
selected	O
areas	O
in	O
comm	O
.	O
16	O
(	O
2	O
)	O
,	O
140–	O
152.	O
mcfadden	O
,	O
d.	O
(	O
1974	O
)	O
.	O
conditional	O
logit	O
analysis	O
of	O
qualitative	O
choice	O
be-	O
havior	O
.	O
in	O
p.	O
zarembka	O
(	O
ed	O
.	O
)	O
,	O
fron-	O
tiers	O
in	O
econometrics	O
,	O
pp	O
.	O
105–142	O
.	O
academic	O
press	O
.	O
bibliography	O
1033	O
mcgrayne	O
,	O
s.	O
b	O
.	O
(	O
2011	O
)	O
.	O
the	O
the-	O
ory	O
that	O
would	O
not	O
die	O
:	O
how	O
bayes	O
’	O
rule	O
cracked	O
the	O
enigma	O
code	O
,	O
hunted	O
down	O
russian	O
submarines	O
,	O
and	O
emerged	O
triumphant	O
from	O
two	O
centuries	O
of	O
controversy	O
.	O
yale	O
uni-	O
versity	O
press	O
.	O
mckay	O
,	O
b.	O
d.	O
,	O
f.	O
e.	O
oggier	O
,	O
g.	O
f.	O
royle	O
,	O
n.	O
j.	O
a.	O
sloane	O
,	O
i.	O
m.	O
wanless	O
,	O
and	O
h.	O
s.	O
wilf	O
(	O
2004	O
)	O
.	O
acyclic	O
digraphs	O
and	O
eigenvalues	O
of	O
(	O
0,1	O
)	O
-matrices	O
.	O
j.	O
integer	O
sequences	O
7	O
(	O
04.3.3	O
)	O
.	O
mckay	O
,	O
d.	O
and	O
l.	O
c.	O
b.	O
peto	O
(	O
1995	O
)	O
.	O
language	O
a	O
hierarchical	O
dirichlet	O
model	O
.	O
natural	O
language	O
engineer-	O
ing	O
1	O
(	O
3	O
)	O
,	O
289–307	O
.	O
mclachlan	O
,	O
g.	O
j.	O
and	O
t.	O
krishnan	O
(	O
1997	O
)	O
.	O
the	O
em	O
algorithm	O
and	O
ex-	O
tensions	O
.	O
wiley	O
.	O
meek	O
,	O
c.	O
and	O
d.	O
heckerman	O
(	O
1997	O
)	O
.	O
learn-	O
independence	O
and	O
in	O
uai	O
,	O
structure	O
and	O
parameter	B
ing	O
for	O
causal	O
causal	O
interaction	O
models	O
.	O
pp	O
.	O
366–375	O
.	O
meek	O
,	O
c.	O
,	O
b.	O
thiesson	O
,	O
and	O
d.	O
hecker-	O
man	O
(	O
2002	O
)	O
.	O
staged	O
mixture	B
mod-	O
elling	O
and	O
boosting	B
.	O
in	O
uai	O
,	O
san	O
francisco	O
,	O
ca	O
,	O
pp	O
.	O
335–343	O
.	O
mor-	O
gan	O
kaufmann	O
.	O
meila	O
,	O
m.	O
(	O
2001	O
)	O
.	O
a	O
random	O
walks	O
view	O
in	O
ai/s-	O
of	O
spectral	B
segmentation	O
.	O
tatistics	O
.	O
meila	O
,	O
m.	O
(	O
2005	O
)	O
.	O
comparing	O
cluster-	O
in	O
intl	O
.	O
ings	O
:	O
an	O
axiomatic	O
view	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
meila	O
,	O
m.	O
and	O
t.	O
jaakkola	O
(	O
2006	O
)	O
.	O
tractable	O
bayesian	O
learning	B
of	O
tree	B
belief	O
networks	O
.	O
statistics	O
and	O
com-	O
puting	O
16	O
,	O
77–92	O
.	O
i.	O
meila	O
,	O
m.	O
and	O
m.	O
jordan	O
(	O
2000	O
)	O
.	O
learning	B
with	O
mixtures	O
of	O
trees	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
1	O
,	O
1–	O
48.	O
meinshausen	O
,	O
n.	O
(	O
2005	O
)	O
.	O
a	O
note	O
on	O
the	O
lasso	B
for	O
gaussian	O
graphical	B
model	I
selection	O
.	O
technical	O
report	O
,	O
eth	O
seminar	O
fur	O
statistik	O
.	O
meinshausen	O
,	O
n.	O
and	O
p.	O
buhlmann	O
(	O
2006	O
)	O
.	O
high	O
dimensional	O
graphs	O
and	O
variable	O
selection	O
with	O
the	O
lasso	B
.	O
the	O
annals	O
of	O
statistics	O
34	O
,	O
1436–1462	O
.	O
globally	O
optimal	O
meltzer	O
,	O
t.	O
,	O
c.	O
yanover	O
,	O
and	O
y.	O
weiss	O
(	O
2005	O
)	O
.	O
solu-	O
tions	O
for	O
energy	O
minimization	O
in	O
stereo	O
vision	O
using	O
reweighted	O
be-	O
lief	O
propagation	O
.	O
in	O
iccv	O
,	O
pp	O
.	O
428–	O
435.	O
meng	O
,	O
x.	O
l.	O
and	O
d.	O
van	O
dyk	O
(	O
1997	O
)	O
.	O
the	O
em	O
algorithm	O
—	O
an	O
old	O
folk	O
song	O
sung	O
to	O
a	O
fast	O
new	O
tune	O
(	O
with	O
discussion	O
)	O
.	O
j.	O
royal	O
stat	O
.	O
soc	O
.	O
b	O
59	O
,	O
511–567	O
.	O
mesot	O
,	O
b.	O
and	O
d.	O
barber	O
(	O
2009	O
)	O
.	O
a	O
sim-	O
ple	O
alternative	O
derivation	O
of	O
the	O
expectation	B
correction	I
algorithm	O
.	O
ieee	O
signal	B
processing	I
letters	O
16	O
(	O
1	O
)	O
,	O
121–124	O
.	O
metropolis	O
,	O
a.	O
n.	O
,	O
rosenbluth	O
,	O
m.	O
rosenbluth	O
,	O
a.	O
teller	O
,	O
and	O
e.	O
teller	O
(	O
1953	O
)	O
.	O
equation	O
of	O
state	B
calculations	O
by	O
fast	O
computing	O
machines	O
.	O
j.	O
of	O
chemical	O
physics	O
21	O
,	O
1087–1092	O
.	O
metz	O
,	O
c.	O
(	O
2010	O
)	O
.	O
google	O
behavioral	O
ad	O
targeter	O
is	O
a	O
smart	O
ass	O
.	O
the	O
regis-	O
ter	O
.	O
miller	O
,	O
a	O
.	O
(	O
2002	O
)	O
.	O
subset	O
selection	O
in	O
re-	O
gression	O
.	O
chapman	O
and	O
hall	O
.	O
2nd	O
edition	O
.	O
mimno	O
,	O
d.	O
and	O
a.	O
mccallum	O
(	O
2008	O
)	O
.	O
topic	B
models	O
conditioned	O
on	O
ar-	O
bitrary	O
features	B
with	O
dirichlet-	O
multinomial	O
regression	O
.	O
in	O
uai	O
.	O
minka	O
,	O
t.	O
(	O
1999	O
)	O
.	O
pathologies	B
of	O
ortho-	O
dox	O
statisics	O
.	O
technical	O
report	O
,	O
mit	O
media	O
lab	O
.	O
minka	O
,	O
t.	O
(	O
2000a	O
)	O
.	O
automatical	O
choice	O
of	O
dimensionality	O
for	O
pca	O
.	O
techni-	O
cal	O
report	O
,	O
mit	O
.	O
minka	O
,	O
t.	O
(	O
2000b	O
)	O
.	O
bayesian	O
linear	O
re-	O
gression	O
.	O
technical	O
report	O
,	O
mit	O
.	O
minka	O
,	O
t.	O
(	O
2000c	O
)	O
.	O
bayesian	O
model	O
av-	O
eraging	O
is	O
not	O
model	O
combination	O
.	O
technical	O
report	O
,	O
mit	O
media	O
lab	O
.	O
minka	O
,	O
t.	O
(	O
2000d	O
)	O
.	O
empirical	B
risk	I
min-	O
imization	O
is	O
an	O
incomplete	O
induc-	O
tive	O
principle	O
.	O
technical	O
report	O
,	O
mit	O
.	O
minka	O
,	O
t.	O
(	O
2000e	O
)	O
.	O
estimating	O
a	O
dirich-	O
let	O
distribution	O
.	O
technical	O
report	O
,	O
mit	O
.	O
minka	O
,	O
t.	O
(	O
2000f	O
)	O
.	O
inferring	O
a	O
gaussian	O
distribution	O
.	O
technical	O
report	O
,	O
mit	O
.	O
meinshausen	O
,	O
n.	O
and	O
p.	O
bãijhlmann	O
(	O
2010	O
)	O
.	O
stability	B
selection	I
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
72	O
,	O
417–473	O
.	O
minka	O
,	O
t.	O
(	O
2001a	O
)	O
.	O
bayesian	O
inference	B
of	O
a	O
uniform	B
distribution	I
.	O
techni-	O
cal	O
report	O
,	O
mit	O
.	O
minka	O
,	O
t.	O
(	O
2001b	O
)	O
.	O
empirical	B
risk	I
min-	O
imization	O
is	O
an	O
incomplete	O
induc-	O
tive	O
principle	O
.	O
technical	O
report	O
,	O
mit	O
.	O
minka	O
,	O
t.	O
(	O
2001c	O
)	O
.	O
expectation	O
propa-	O
gation	O
for	O
approximate	O
bayesian	O
in-	O
ference	O
.	O
in	O
uai	O
.	O
minka	O
,	O
t.	O
(	O
2001d	O
)	O
.	O
a	O
family	B
of	O
algo-	O
rithms	O
for	O
approximate	O
bayesian	O
in-	O
ference	O
.	O
ph.d.	O
thesis	O
,	O
mit	O
.	O
minka	O
,	O
t.	O
(	O
2001e	O
)	O
.	O
statistical	O
ap-	O
proaches	O
to	O
learning	B
and	O
discovery	O
10-602	O
:	O
homework	O
assignment	O
2	O
,	O
question	O
5.	O
technical	O
report	O
,	O
cmu	O
.	O
minka	O
,	O
t.	O
(	O
2003	O
)	O
.	O
a	O
comparison	O
of	O
nu-	O
merical	O
optimizers	O
for	O
logistic	B
re-	O
gression	O
.	O
technical	O
report	O
,	O
msr	O
.	O
minka	O
,	O
t.	O
(	O
2005	O
)	O
.	O
divergence	O
measures	O
and	O
message	B
passing	I
.	O
technical	O
re-	O
port	O
,	O
msr	O
cambridge	O
.	O
minka	O
,	O
t.	O
and	O
y.	O
qi	O
tree-	O
structured	O
approximations	O
by	O
ex-	O
pectation	O
propagation	O
.	O
in	O
nips	O
.	O
(	O
2003	O
)	O
.	O
minka	O
,	O
t.	O
,	O
j.	O
winn	O
,	O
d.	O
knowles	O
(	O
2010	O
)	O
.	O
microsoft	O
http	O
:	O
//research.microsoft.com/infernet	O
.	O
j.	O
guiver	O
,	O
and	O
infer.net	B
2.4.	O
research	O
cambridge	O
.	O
minsky	O
,	O
m.	O
and	O
s.	O
papert	O
(	O
1969	O
)	O
.	O
per-	O
ceptrons	O
.	O
mit	O
press	O
.	O
mitchell	O
,	O
t.	O
(	O
1997	O
)	O
.	O
machine	B
learning	I
.	O
mcgraw	O
hill	O
.	O
mitchell	O
,	O
t.	O
and	O
j.	O
beauchamp	O
(	O
1988	O
)	O
.	O
bayesian	O
variable	O
selection	O
in	O
lin-	O
ear	O
regression	B
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
83	O
,	O
1023–1036	O
.	O
mobahi	O
,	O
h.	O
,	O
r.	O
collobert	O
,	O
and	O
j.	O
we-	O
ston	O
(	O
2009	O
)	O
.	O
deep	B
learning	I
from	O
temporal	O
coherence	O
in	O
video	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
mockus	O
,	O
j.	O
,	O
w.	O
eddy	O
,	O
a.	O
mockus	O
,	O
l.	O
mockus	O
,	O
and	O
g.	O
reklaitis	O
(	O
1996	O
)	O
.	O
bayesian	O
heuristic	O
approach	O
to	O
dis-	O
crete	O
and	O
global	O
optimization	O
:	O
al-	O
gorithms	O
,	O
visualization	O
,	O
software	O
,	O
and	O
applications	O
.	O
kluwer	O
.	O
moghaddam	O
,	O
b.	O
,	O
a.	O
gruber	O
,	O
y.	O
weiss	O
,	O
and	O
s.	O
avidan	O
(	O
2008	O
)	O
.	O
sparse	B
re-	O
gression	O
as	O
a	O
sparse	B
eigenvalue	O
problem	O
.	O
in	O
information	B
theory	I
&	O
applications	O
workshop	O
(	O
ita	O
’	O
08	O
)	O
.	O
moghaddam	O
,	O
b.	O
,	O
b.	O
marlin	O
,	O
e.	O
khan	O
,	O
and	O
k.	O
murphy	O
(	O
2009	O
)	O
.	O
accel-	O
erating	O
bayesian	O
structural	O
infer-	O
ence	O
for	O
non-decomposable	O
gaus-	O
sian	O
graphical	O
models	O
.	O
in	O
nips	O
.	O
1034	O
bibliography	O
moghaddam	O
,	O
b.	O
and	O
a.	O
pentland	O
(	O
1995	O
)	O
.	O
probabilistic	O
visual	O
learning	B
for	O
object	B
detection	I
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
computer	O
vision	O
.	O
mohamed	O
,	O
s.	O
,	O
and	O
z.	O
ghahramani	O
bayesian	O
exponential	B
family	I
pca	O
.	O
in	O
nips	O
.	O
k.	O
heller	O
,	O
(	O
2008	O
)	O
.	O
moler	O
,	O
c.	O
(	O
2004	O
)	O
.	O
numerical	O
computing	O
with	O
matlab	O
.	O
siam	O
.	O
morris	O
,	O
r.	O
d.	O
,	O
x.	O
descombes	O
,	O
and	O
j.	O
zerubia	O
(	O
1996	O
)	O
.	O
the	O
ising/potts	O
model	O
is	O
not	O
well	O
suited	O
to	O
seg-	O
mentation	O
tasks	O
.	O
in	O
ieee	O
dsp	O
work-	O
shop	O
.	O
mosterman	O
,	O
p.	O
j.	O
and	O
g.	O
biswas	O
(	O
1999	O
)	O
.	O
diagnosis	O
of	O
continuous	O
valued	O
systems	O
in	O
transient	B
operating	O
re-	O
gions	O
.	O
ieee	O
trans	O
.	O
on	O
systems	O
,	O
man	O
,	O
and	O
cybernetics	O
,	O
part	O
a	O
29	O
(	O
6	O
)	O
,	O
554–	O
565.	O
moulines	O
,	O
e.	O
,	O
j.-f.	O
cardoso	O
,	O
and	O
e.	O
gas-	O
siat	O
(	O
1997	O
)	O
.	O
maximum	O
likelihood	O
for	O
blind	O
separation	O
and	O
deconvo-	O
lution	O
of	O
noisy	O
signals	O
using	O
mix-	O
ture	O
models	O
.	O
ieee	O
int	O
.	O
conf	O
.	O
on	O
acoustics	O
,	O
speech	O
and	O
sig-	O
nal	O
processing	O
(	O
icassp	O
’	O
97	O
)	O
,	O
munich	O
,	O
germany	O
,	O
pp	O
.	O
3617–3620	O
.	O
in	O
proc	O
.	O
muller	O
,	O
p.	O
,	O
g.	O
parmigiani	O
,	O
c.	O
robert	O
,	O
and	O
j.	O
rousseau	O
(	O
2004	O
)	O
.	O
optimal	O
sample	O
size	O
for	O
multiple	O
testing	O
:	O
the	O
case	O
of	O
gene	O
expression	O
mi-	O
croarrays	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
as-	O
soc	O
.	O
99	O
,	O
990–1001	O
.	O
mumford	O
,	O
d.	O
(	O
1994	O
)	O
.	O
neuronal	O
archi-	O
tectures	O
for	O
pattern-theoretic	O
prob-	O
lems	O
.	O
in	O
c.	O
koch	O
and	O
j.	O
davis	O
(	O
eds	O
.	O
)	O
,	O
large	O
scale	O
neuronal	O
theories	O
of	O
the	O
brain	O
.	O
mit	O
press	O
.	O
murphy	O
,	O
k.	O
(	O
2000	O
)	O
.	O
bayesian	O
map	O
learning	B
in	O
dynamic	O
environments	O
.	O
in	O
nips	O
,	O
volume	O
12.	O
murphy	O
,	O
k.	O
and	O
m.	O
paskin	O
(	O
2001	O
)	O
.	O
lin-	O
ear	O
time	O
inference	O
in	O
hierarchical	O
hmms	O
.	O
in	O
nips	O
.	O
murphy	O
,	O
k.	O
,	O
y.	O
weiss	O
,	O
and	O
m.	O
jordan	O
(	O
1999	O
)	O
.	O
loopy	B
belief	I
propagation	I
for	O
approximate	B
inference	I
:	O
an	O
empiri-	O
cal	O
study	O
.	O
in	O
uai	O
.	O
murphy	O
,	O
k.	O
p.	O
(	O
1998	O
)	O
.	O
filtering	O
and	O
smoothing	O
in	O
linear	O
dynamical	O
sys-	O
tems	O
using	O
the	O
junction	B
tree	I
algo-	O
rithm	O
.	O
technical	O
report	O
,	O
u.c	O
.	O
berke-	O
ley	O
,	O
dept	O
.	O
comp	O
.	O
sci	O
.	O
murray	O
,	O
i.	O
and	O
z.	O
ghahramani	O
(	O
2005	O
)	O
.	O
a	O
note	O
on	O
the	O
evidence	B
and	O
bayesian	O
occam	O
’	O
s	O
razor	O
.	O
technical	O
report	O
,	O
gatsby	O
.	O
musso	O
,	O
c.	O
,	O
n.	O
oudjane	O
,	O
and	O
f.	O
legland	O
improving	O
regularized	O
par-	O
(	O
2001	O
)	O
.	O
ticle	O
ﬁlters	O
.	O
in	O
a.	O
doucet	O
,	O
j.	O
f.	O
g.	O
de	O
freitas	O
,	O
and	O
n.	O
gordon	O
(	O
eds	O
.	O
)	O
,	O
sequential	B
monte	O
carlo	O
methods	O
in	O
practice	O
.	O
springer	O
.	O
nabney	O
,	O
i	O
.	O
(	O
2001	O
)	O
.	O
netlab	O
:	O
algorithms	O
for	O
pattern	B
recognition	I
.	O
springer	O
.	O
neal	O
,	O
r.	O
(	O
1992	O
)	O
.	O
connectionist	O
learning	B
of	O
belief	B
networks	I
.	O
artiﬁcial	O
intelli-	O
gence	O
56	O
,	O
71–113	O
.	O
neal	O
,	O
r.	O
(	O
1993	O
)	O
.	O
probabilistic	B
inference	I
using	O
markov	O
chain	O
monte	O
carlo	O
methods	O
.	O
technical	O
report	O
,	O
univ	O
.	O
toronto	O
.	O
neal	O
,	O
r.	O
(	O
1996	O
)	O
.	O
bayesian	O
learning	B
for	O
neural	B
networks	I
.	O
springer	O
.	O
neal	O
,	O
r.	O
(	O
1997	O
)	O
.	O
monte	O
carlo	O
im-	O
plementation	O
of	O
gaussian	O
process	O
models	O
for	O
bayesian	O
regression	B
and	O
classiﬁcation	B
.	O
technical	O
re-	O
port	O
9702	O
,	O
u.	O
toronto	O
.	O
neal	O
,	O
r.	O
(	O
1998	O
)	O
.	O
erroneous	O
results	O
in	O
’	O
marginal	B
likelihood	I
from	O
the	O
gibbs	O
output	O
’	O
.	O
technical	O
report	O
,	O
u.	O
toronto	O
.	O
neal	O
,	O
r.	O
(	O
2000	O
)	O
.	O
markov	O
chain	O
sam-	O
pling	O
methods	O
for	O
dirichlet	O
process	O
mixture	B
models	O
.	O
j.	O
of	O
computa-	O
tional	O
and	O
graphical	O
statistics	O
9	O
(	O
2	O
)	O
,	O
249–265	O
.	O
neal	O
,	O
r.	O
(	O
2003a	O
)	O
.	O
slice	B
sampling	I
.	O
an-	O
nals	O
of	O
statistics	O
31	O
(	O
3	O
)	O
,	O
7–5–767	O
.	O
neal	O
,	O
r.	O
(	O
2010	O
)	O
.	O
mcmc	O
using	O
hamil-	O
in	O
s.	O
brooks	O
,	O
tonian	O
dynamics	O
.	O
a.	O
gelman	O
,	O
g.	O
jones	O
,	O
and	O
x.-l.	O
meng	O
(	O
eds	O
.	O
)	O
,	O
handbook	O
of	O
markov	O
chain	O
monte	O
carlo	O
.	O
chapman	O
&	O
hall	O
.	O
neal	O
,	O
r.	O
and	O
d.	O
mackay	O
(	O
1998	O
)	O
.	O
likelihood-based	O
boosting	B
.	O
techni-	O
cal	O
report	O
,	O
u.	O
toronto	O
.	O
neal	O
,	O
r.	O
and	O
j.	O
zhang	O
(	O
2006	O
)	O
.	O
high	O
dimensional	O
classiﬁcation	B
bayesian	O
neural	B
networks	I
and	O
dirichlet	O
dif-	O
fusion	O
trees	O
.	O
in	O
i.	O
guyon	O
,	O
s.	O
gunn	O
,	O
m.	O
nikravesh	O
,	O
and	O
l.	O
zadeh	O
(	O
eds	O
.	O
)	O
,	O
feature	B
extraction	I
.	O
springer	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
2001	O
)	O
.	O
annealed	O
impor-	O
tance	O
sampling	O
.	O
statistics	O
and	O
com-	O
puting	O
11	O
,	O
125–139	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
2003b	O
)	O
.	O
density	O
model-	O
ing	O
and	O
clustering	B
using	O
dirichlet	O
diffusion	O
trees	O
.	O
in	O
j.	O
m.	O
bernardo	O
et	O
al	O
.	O
(	O
eds	O
.	O
)	O
,	O
bayesian	O
statistics	O
7	O
,	O
pp	O
.	O
619–629	O
.	O
oxford	O
university	O
press	O
.	O
neal	O
,	O
r.	O
m.	O
and	O
g.	O
e.	O
hinton	O
(	O
1998	O
)	O
.	O
a	O
new	O
view	O
of	O
the	O
em	O
algorithm	O
that	O
justiﬁes	O
incremental	O
and	O
other	O
variants	O
.	O
in	O
m.	O
jordan	O
(	O
ed	O
.	O
)	O
,	O
learn-	O
ing	O
in	O
graphical	O
models	O
.	O
mit	O
press	O
.	O
neapolitan	O
,	O
r.	O
(	O
2003	O
)	O
.	O
bayesian	O
networks	O
.	O
prentice	O
hall	O
.	O
learning	B
neﬁan	O
,	O
a.	O
,	O
l.	O
liang	O
,	O
x.	O
pi	O
,	O
x.	O
liu	O
,	O
and	O
k.	O
murphy	O
(	O
2002	O
)	O
.	O
dynamic	O
for	O
audio-	O
bayesian	O
networks	O
visual	O
speech	O
recognition	O
.	O
j.	O
ap-	O
plied	O
signal	B
processing	I
.	O
nemirovski	O
,	O
a.	O
and	O
d.	O
yudin	O
(	O
1978	O
)	O
.	O
on	O
cezari	O
’	O
s	O
convergence	O
of	O
the	O
steep-	O
est	O
descent	O
method	O
for	O
approxi-	O
mating	O
saddle	O
points	O
of	O
convex-	O
concave	B
functions	O
.	O
soviet	O
math	O
.	O
dokl	O
.	O
19.	O
nesterov	O
,	O
y	O
.	O
(	O
2004	O
)	O
.	O
introductory	O
lec-	O
tures	O
on	O
convex	B
optimization	O
.	O
a	O
basic	O
course	O
.	O
kluwer	O
.	O
newton	O
,	O
m.	O
,	O
d.	O
noueiry	O
,	O
d.	O
sarkar	O
,	O
and	O
p.	O
ahlquist	O
(	O
2004	O
)	O
.	O
detecting	O
differential	O
gene	O
expression	O
with	O
a	O
semiparametric	O
hierarchical	O
mix-	O
ture	O
method	O
.	O
biostatistics	O
5	O
,	O
155–	O
176.	O
newton	O
,	O
m.	O
and	O
a.	O
raftery	O
(	O
1994	O
)	O
.	O
ap-	O
proximate	O
bayesian	O
inference	B
with	O
the	O
weighted	O
likelihood	O
bootstrap	B
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
56	O
(	O
1	O
)	O
,	O
3–48	O
.	O
ng	O
,	O
a.	O
,	O
m.	O
jordan	O
,	O
and	O
y.	O
weiss	O
(	O
2001	O
)	O
.	O
on	O
spectral	B
clustering	I
:	O
analysis	O
and	O
an	O
algorithm	O
.	O
in	O
nips	O
.	O
ng	O
,	O
a.	O
y.	O
and	O
m.	O
i.	O
jordan	O
(	O
2002	O
)	O
.	O
on	O
discriminative	B
vs.	O
generative	O
classi-	O
ﬁers	O
:	O
a	O
comparison	O
of	O
logistic	B
re-	O
gression	O
and	O
naive	O
bayes	O
.	O
in	O
nips-	O
14.	O
nickisch	O
,	O
h.	O
and	O
c.	O
rasmussen	O
(	O
2008	O
)	O
.	O
approximations	O
for	O
binary	O
gaus-	O
sian	O
process	O
classiﬁcation	B
.	O
j.	O
of	O
ma-	O
chine	O
learning	B
research	O
9	O
,	O
2035–	O
2078.	O
nilsson	O
,	O
d.	O
(	O
1998	O
)	O
.	O
an	O
efficient	O
algo-	O
rithm	O
for	O
ﬁnding	O
the	O
m	O
most	O
prob-	O
able	O
conﬁgurations	O
in	O
a	O
probabilis-	O
tic	O
expert	O
system	O
.	O
statistics	O
and	O
computing	O
8	O
,	O
159–173	O
.	O
nilsson	O
,	O
d.	O
and	O
j.	O
goldberger	O
(	O
2001	O
)	O
.	O
sequentially	O
ﬁnding	O
the	O
n-best	O
list	O
in	O
hidden	B
markov	O
models	O
.	O
in	O
intl	O
.	O
joint	O
conf	O
.	O
on	O
ai	O
,	O
pp	O
.	O
1280–1285	O
.	O
nocedal	O
,	O
j.	O
and	O
s.	O
wright	O
(	O
2006	O
)	O
.	O
nu-	O
merical	O
optimization	B
.	O
springer	O
.	O
bibliography	O
1035	O
nowicki	O
,	O
k.	O
and	O
t.	O
a.	O
b.	O
snijders	O
(	O
2001	O
)	O
.	O
estimation	O
and	O
prediction	O
for	O
stochastic	O
blockstructures	O
.	O
jour-	O
nal	O
of	O
the	O
american	O
statistical	O
asso-	O
ciation	O
96	O
(	O
455	O
)	O
,	O
1077–	O
?	O
?	O
nowlan	O
,	O
s.	O
and	O
g.	O
hinton	O
(	O
1992	O
)	O
.	O
sim-	O
plifying	O
neural	B
networks	I
by	O
soft	B
weight	I
sharing	I
.	O
neural	O
computa-	O
tion	O
4	O
(	O
4	O
)	O
,	O
473–493	O
.	O
nummiaro	O
,	O
k.	O
,	O
e.	O
koller-meier	O
,	O
and	O
an	O
adaptive	O
l.	O
v.	O
gool	O
color-based	O
particle	O
ﬁlter	O
.	O
image	O
and	O
vision	O
computing	O
21	O
(	O
1	O
)	O
,	O
99–110	O
.	O
(	O
2003	O
)	O
.	O
obozinski	O
,	O
g.	O
,	O
b.	O
taskar	O
,	O
and	O
m.	O
i.	O
jor-	O
dan	O
(	O
2007	O
)	O
.	O
joint	O
covariate	O
selection	O
for	O
grouped	O
classiﬁcation	B
.	O
techni-	O
cal	O
report	O
,	O
uc	O
berkeley	O
.	O
oh	O
,	O
m.-s.	O
and	O
j.	O
berger	O
(	O
1992	O
)	O
.	O
adap-	O
tive	O
importance	B
sampling	I
in	O
monte	O
carlo	O
integration	O
.	O
j.	O
of	O
statistical	O
computation	O
and	O
simulation	O
41	O
(	O
3	O
)	O
,	O
143	O
–	O
168.	O
oh	O
,	O
s.	O
,	O
s.	O
russell	O
,	O
and	O
s.	O
sastry	O
(	O
2009	O
)	O
.	O
markov	O
chain	O
monte	O
carlo	O
data	B
association	I
for	O
multi-target	O
track-	O
ing	O
.	O
ieee	O
trans	O
.	O
on	O
automatic	O
con-	O
trol	O
54	O
(	O
3	O
)	O
,	O
481–497	O
.	O
o	O
’	O
hagan	O
,	O
a	O
.	O
(	O
1978	O
)	O
.	O
curve	O
ﬁtting	O
and	O
optimal	O
design	O
for	O
prediction	O
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
40	O
,	O
1–42	O
.	O
o	O
’	O
hara	O
,	O
r.	O
and	O
m.	O
sillanpaa	O
(	O
2009	O
)	O
.	O
a	O
review	O
of	O
bayesian	O
variable	O
se-	O
lection	O
methods	O
:	O
what	O
,	O
how	O
and	O
which	O
.	O
bayesian	O
analysis	O
4	O
(	O
1	O
)	O
,	O
85–	O
118.	O
olshausen	O
,	O
b.	O
a.	O
and	O
d.	O
j.	O
field	O
(	O
1996	O
)	O
.	O
emergence	O
of	O
simple	O
cell	O
recep-	O
tive	O
ﬁeld	O
properties	O
by	O
learning	B
a	O
sparse	B
code	O
for	O
natural	O
images	O
.	O
na-	O
ture	O
381	O
,	O
607–609	O
.	O
opper	O
,	O
m.	O
(	O
1998	O
)	O
.	O
a	O
bayesian	O
approach	O
to	O
online	B
learning	I
.	O
in	O
d.	O
saad	O
(	O
ed	O
.	O
)	O
,	O
on-line	O
learning	B
in	O
neural	B
networks	I
.	O
cambridge	O
.	O
opper	O
,	O
m.	O
and	O
c.	O
archambeau	O
(	O
2009	O
)	O
.	O
the	O
variational	O
gaussian	O
approxi-	O
mation	O
revisited	O
.	O
neural	O
computa-	O
tion	O
21	O
(	O
3	O
)	O
,	O
786–792	O
.	O
opper	O
,	O
m.	O
and	O
d.	O
saad	O
(	O
eds	O
.	O
)	O
(	O
2001	O
)	O
.	O
advanced	O
mean	B
ﬁeld	I
methods	O
:	O
the-	O
ory	O
and	O
practice	O
.	O
mit	O
press	O
.	O
osborne	O
,	O
m.	O
r.	O
,	O
b.	O
presnell	O
,	O
and	O
b.	O
a.	O
turlach	O
(	O
2000a	O
)	O
.	O
a	O
new	O
approach	O
to	O
variable	O
selection	O
in	O
least	B
squares	I
problems	O
.	O
ima	O
journal	O
of	O
numeri-	O
cal	O
analysis	O
20	O
(	O
3	O
)	O
,	O
389–403	O
.	O
osborne	O
,	O
m.	O
r.	O
,	O
b.	O
presnell	O
,	O
and	O
b.	O
a.	O
turlach	O
(	O
2000b	O
)	O
.	O
on	O
the	O
lasso	B
and	O
its	O
dual	O
.	O
j.	O
computational	O
and	O
graphical	O
statistics	O
9	O
,	O
319–337	O
.	O
ostendorf	O
,	O
m.	O
,	O
v.	O
digalakis	O
,	O
and	O
o.	O
kimball	O
(	O
1996	O
)	O
.	O
from	O
hmms	O
to	O
segment	O
models	O
:	O
a	O
uniﬁed	O
view	O
of	O
stochastic	O
modeling	O
for	O
speech	B
recognition	I
.	O
ieee	O
trans	O
.	O
on	O
speech	O
and	O
audio	O
processing	O
4	O
(	O
5	O
)	O
,	O
360–	O
378.	O
overschee	O
,	O
p.	O
v.	O
and	O
b.	O
d.	O
moor	O
(	O
1996	O
)	O
.	O
subspace	O
identiﬁcation	O
for	O
linear	O
systems	O
:	O
theory	O
,	O
implemen-	O
tation	O
,	O
applications	O
.	O
kluwer	O
aca-	O
demic	O
publishers	O
.	O
paatero	O
,	O
p.	O
and	O
u.	O
tapper	O
(	O
1994	O
)	O
.	O
pos-	O
itive	O
matrix	B
factorization	I
:	O
a	O
non-	O
negative	O
factor	O
model	O
with	O
opti-	O
mal	O
utilization	O
of	O
error	O
estimates	O
of	O
data	O
values	O
.	O
environmetrics	O
5	O
,	O
111–	O
126.	O
padadimitriou	O
,	O
c.	O
and	O
k.	O
steiglitz	O
(	O
1982	O
)	O
.	O
combinatorial	O
optimization	B
:	O
algorithms	O
and	O
complexity	O
.	O
pren-	O
tice	O
hall	O
.	O
paisley	O
,	O
j.	O
and	O
l.	O
carin	O
(	O
2009	O
)	O
.	O
non-	O
parametric	O
factor	O
analysis	O
with	O
beta	B
process	I
priors	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
palmer	O
,	O
s.	O
(	O
1999	O
)	O
.	O
vision	O
science	O
:	O
pho-	O
tons	O
to	O
phenomenology	O
.	O
mit	O
press	O
.	O
parise	O
,	O
s.	O
and	O
m.	O
welling	O
(	O
2005	O
)	O
.	O
random	O
learning	O
fields	O
:	O
an	O
empirical	O
study	O
.	O
in	O
joint	O
statistical	O
meeting	O
.	O
in	O
markov	O
park	O
,	O
t.	O
and	O
g.	O
casella	O
(	O
2008	O
)	O
.	O
the	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
bayesian	O
lasso	B
.	O
assoc	O
.	O
103	O
(	O
482	O
)	O
,	O
681–686	O
.	O
parviainen	O
,	O
p.	O
and	O
m.	O
koivisto	O
(	O
2011	O
)	O
.	O
ancestor	O
relations	O
in	O
the	O
presence	O
of	O
unobserved	O
variables	O
.	O
in	O
proc	O
.	O
european	O
conf	O
.	O
on	O
machine	O
learn-	O
ing	O
.	O
paskin	O
,	O
m.	O
(	O
2003	O
)	O
.	O
thin	O
junction	O
tree	O
ﬁlters	O
for	O
simultaneous	B
localization	I
and	I
mapping	I
.	O
in	O
intl	O
.	O
joint	O
conf	O
.	O
on	O
ai	O
.	O
pearl	O
,	O
j	O
.	O
(	O
1988	O
)	O
.	O
probabilistic	O
reasoning	O
in	O
intelligent	O
systems	O
:	O
networks	O
of	O
plausible	O
inference	B
.	O
morgan	O
kauf-	O
mann	O
.	O
pearl	O
,	O
j.	O
and	O
t.	O
verma	O
(	O
1991	O
)	O
.	O
a	O
theory	O
of	O
inferred	O
causation	O
.	O
in	O
knowledge	O
representation	O
,	O
pp	O
.	O
441–452	O
.	O
pe	O
’	O
er	O
,	O
d.	O
(	O
2005	O
,	O
april	O
)	O
.	O
bayesian	O
net-	O
work	O
analysis	O
of	O
signaling	O
net-	O
works	O
:	O
a	O
primer	O
.	O
science	O
stke	O
281	O
,	O
14.	O
peng	O
,	O
f.	O
,	O
r.	O
(	O
1996	O
)	O
.	O
jacobs	O
,	O
and	O
m.	O
tan-	O
ner	O
bayesian	O
inference	B
in	O
mixtures-of-experts	O
and	O
hier-	O
archical	O
mixtures-of-experts	O
mod-	O
els	O
with	O
an	O
application	O
to	O
speech	B
recognition	I
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
as-	O
soc	O
.	O
91	O
(	O
435	O
)	O
,	O
953–960	O
.	O
petris	O
,	O
g.	O
,	O
s.	O
petrone	O
,	O
and	O
p.	O
campag-	O
noli	O
(	O
2009	O
)	O
.	O
dynamic	O
linear	O
models	O
with	O
r.	O
springer	O
.	O
pham	O
,	O
d.-t.	O
and	O
p.	O
garrat	O
(	O
1997	O
)	O
.	O
blind	O
separation	O
of	O
mixture	O
of	O
inde-	O
pendent	O
sources	O
through	O
a	O
quasi-	O
approach	O
.	O
maximum	O
likelihood	O
ieee	O
trans	O
.	O
on	O
signal	O
process-	O
ing	O
45	O
(	O
7	O
)	O
,	O
1712–1725	O
.	O
pietra	O
,	O
s.	O
d.	O
,	O
v.	O
d.	O
pietra	O
,	O
and	O
j.	O
laf-	O
ferty	O
(	O
1997	O
)	O
.	O
inducing	O
features	B
of	O
random	O
ﬁelds	O
.	O
ieee	O
trans	O
.	O
on	O
pat-	O
tern	O
analysis	O
and	O
machine	O
intelli-	O
gence	O
19	O
(	O
4	O
)	O
.	O
plackett	O
,	O
r.	O
(	O
1975	O
)	O
.	O
the	O
analysis	O
of	O
per-	O
mutations	O
.	O
applied	O
stat	O
.	O
24	O
,	O
193–	O
202.	O
platt	O
,	O
j	O
.	O
(	O
1998	O
)	O
.	O
using	O
analytic	O
qp	O
and	O
sparseness	O
to	O
speed	O
training	O
of	O
support	B
vector	I
machines	I
.	O
in	O
nips	O
.	O
platt	O
,	O
j	O
.	O
(	O
2000	O
)	O
.	O
probabilities	O
for	O
sv	O
ma-	O
in	O
a.	O
smola	O
,	O
p.	O
bartlett	O
,	O
chines	O
.	O
b.	O
schoelkopf	O
,	O
and	O
d.	O
schuurmans	O
(	O
eds	O
.	O
)	O
,	O
advances	O
in	O
large	O
margin	O
classiﬁers	O
.	O
mit	O
press	O
.	O
platt	O
,	O
j.	O
,	O
n.	O
cristianini	O
,	O
and	O
j.	O
shawe-	O
taylor	O
(	O
2000	O
)	O
.	O
large	O
margin	O
dags	O
for	O
multiclass	B
classiﬁcation	I
.	O
in	O
nips	O
,	O
volume	O
12	O
,	O
pp	O
.	O
547–553	O
.	O
plummer	O
,	O
m.	O
(	O
2003	O
)	O
.	O
jags	O
:	O
a	O
program	O
for	O
analysis	O
of	O
bayesian	O
graphi-	O
cal	O
models	O
using	O
gibbs	O
sampling	O
.	O
in	O
proc	O
.	O
3rd	O
intl	O
.	O
workshop	O
on	O
dis-	O
tributed	O
statistical	O
computing	O
.	O
polson	O
,	O
n.	O
and	O
s.	O
scott	O
(	O
2011	O
)	O
.	O
data	B
augmentation	I
for	O
support	B
vector	I
machines	I
.	O
bayesian	O
analysis	O
6	O
(	O
1	O
)	O
,	O
1–124	O
.	O
pontil	O
,	O
m.	O
,	O
s.	O
mukherjee	O
,	O
and	O
f.	O
girosi	O
(	O
1998	O
)	O
.	O
on	O
the	O
noise	O
model	O
of	O
sup-	O
port	O
vector	O
machine	O
regression	B
.	O
technical	O
report	O
,	O
mit	O
ai	O
lab	O
.	O
pearl	O
,	O
j	O
.	O
(	O
2000	O
)	O
.	O
causality	B
:	O
models	O
,	O
rea-	O
soning	O
and	O
inference	B
.	O
cambridge	O
univ	O
.	O
press	O
.	O
poon	O
,	O
h.	O
and	O
p.	O
domingos	O
(	O
2011	O
)	O
.	O
sum-	O
product	O
networks	O
:	O
a	O
new	O
deep	B
ar-	O
chitecture	O
.	O
in	O
uai	O
.	O
1036	O
bibliography	O
pourahmadi	O
,	O
m.	O
(	O
2004	O
)	O
.	O
simultaneous	O
modelling	O
of	O
covariance	B
matrices	O
:	O
glm	O
,	O
bayesian	O
and	O
nonparamet-	O
ric	O
perspectives	O
.	O
technical	O
report	O
,	O
northern	O
illinois	O
university	O
.	O
prado	O
,	O
r.	O
and	O
m.	O
west	O
(	O
2010	O
)	O
.	O
time	O
series	O
:	O
modelling	O
,	O
computation	O
and	O
inference	B
.	O
crc	O
press	O
.	O
press	O
,	O
s.	O
j	O
.	O
(	O
2005	O
)	O
.	O
applied	O
mul-	O
tivariate	O
analysis	O
,	O
using	O
bayesian	O
and	O
frequentist	B
methods	O
of	O
infer-	O
ence	O
.	O
dover	O
.	O
second	O
edition	O
.	O
press	O
,	O
w.	O
,	O
w.	O
vetterling	O
,	O
s.	O
teukolosky	O
,	O
and	O
b.	O
flannery	O
(	O
1988	O
)	O
.	O
numeri-	O
cal	O
recipes	O
in	O
c	O
:	O
the	O
art	O
of	O
scien-	O
tiﬁc	O
computing	O
(	O
second	O
ed.	O
)	O
.	O
cam-	O
bridge	O
university	O
press	O
.	O
prince	O
,	O
s.	O
(	O
2012	O
)	O
.	O
computer	O
vision	O
:	O
models	O
,	O
learning	B
and	O
inference	B
.	O
cambridge	O
.	O
pritchard	O
,	O
j.	O
,	O
m.	O
m.	O
stephens	O
,	O
and	O
p.	O
donnelly	O
(	O
2000	O
)	O
.	O
inference	B
of	O
population	O
structure	O
using	O
multi-	O
locus	O
genotype	B
data	O
.	O
genetics	O
155	O
,	O
945–959	O
.	O
qi	O
,	O
y.	O
and	O
t.	O
jaakkola	O
(	O
2008	O
)	O
.	O
param-	O
eter	O
expanded	O
variational	O
bayesian	O
methods	O
.	O
in	O
nips	O
.	O
qi	O
,	O
y.	O
,	O
m.	O
szummer	O
,	O
and	O
t.	O
minka	O
(	O
2005	O
)	O
.	O
bayesian	O
conditional	O
ran-	O
dom	O
fields	O
.	O
in	O
10th	O
intl	O
.	O
workshop	O
on	O
ai/statistics	O
.	O
quinlan	O
,	O
j	O
.	O
(	O
1990	O
)	O
.	O
learning	B
logical	O
def-	O
initions	O
from	O
relations	O
.	O
machine	B
learning	I
5	O
,	O
239–266	O
.	O
quinlan	O
,	O
j.	O
r.	O
(	O
1986	O
)	O
.	O
induction	B
of	O
de-	O
cision	O
trees	O
.	O
machine	B
learning	I
1	O
,	O
81–106	O
.	O
quinlan	O
,	O
j.	O
r.	O
(	O
1993	O
)	O
.	O
c4.5	O
programs	O
for	O
machine	B
learning	I
.	O
morgan	O
kauff-	O
man	O
.	O
quinonero-candela	O
,	O
j.	O
,	O
c.	O
rasmussen	O
,	O
and	O
c.	O
williams	O
(	O
2007	O
)	O
.	O
approxi-	O
mation	O
methods	O
for	O
gaussian	O
pro-	O
cess	O
regression	B
.	O
in	O
l.	O
bottou	O
,	O
o.	O
chapelle	O
,	O
d.	O
decoste	O
,	O
and	O
j.	O
we-	O
ston	O
(	O
eds	O
.	O
)	O
,	O
large	O
scale	O
kernel	B
ma-	O
chines	O
,	O
pp	O
.	O
203–223	O
.	O
mit	O
press	O
.	O
rabiner	O
,	O
l.	O
r.	O
(	O
1989	O
)	O
.	O
a	O
tutorial	O
on	O
hid-	O
den	O
markov	O
models	O
and	O
selected	O
applications	O
in	O
speech	B
recognition	I
.	O
proc	O
.	O
of	O
the	O
ieee	O
77	O
(	O
2	O
)	O
,	O
257–286	O
.	O
rai	O
,	O
p.	O
and	O
h.	O
daume	O
(	O
2009	O
)	O
.	O
multi-	O
label	B
prediction	O
via	O
sparse	B
inﬁnite	O
cca	O
.	O
in	O
nips	O
.	O
raiffa	O
,	O
h.	O
(	O
1968	O
)	O
.	O
decision	B
analysis	O
.	O
ad-	O
dison	O
wesley	O
.	O
raina	O
,	O
r.	O
,	O
a.	O
madhavan	O
,	O
and	O
a.	O
ng	O
(	O
2009	O
)	O
.	O
large-scale	O
deep	B
unsuper-	O
vised	O
learning	B
using	O
graphics	O
pro-	O
cessors	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
raina	O
,	O
r.	O
,	O
a.	O
ng	O
,	O
and	O
d.	O
koller	O
(	O
2005	O
)	O
.	O
transfer	B
learning	I
by	O
constructing	O
informative	O
priors	O
.	O
in	O
nips	O
.	O
rajaraman	O
,	O
a.	O
and	O
j.	O
ullman	O
(	O
2010	O
)	O
.	O
self-	O
mining	O
of	O
massive	O
datasets	O
.	O
published	O
.	O
rajaraman	O
,	O
a.	O
and	O
j.	O
ullman	O
(	O
2011	O
)	O
.	O
mining	O
of	O
massive	O
datasets	O
.	O
cam-	O
bridge	O
.	O
rakotomamonjy	O
,	O
a.	O
,	O
f.	O
bach	O
,	O
s.	O
canu	O
,	O
and	O
y.	O
grandvalet	O
sim-	O
plemkl	O
.	O
j.	O
of	O
machine	B
learning	I
re-	O
search	O
9	O
,	O
2491–2521	O
.	O
(	O
2008	O
)	O
.	O
ramage	O
,	O
d.	O
,	O
d.	O
hall	O
,	O
r.	O
nallapati	O
,	O
and	O
c.	O
manning	O
(	O
2009	O
)	O
.	O
labeled	O
lda	O
:	O
a	O
supervised	O
topic	O
model	O
for	O
credit	O
attribution	O
in	O
multi-labeled	O
corpora	O
.	O
in	O
emnlp	O
.	O
ramage	O
,	O
d.	O
,	O
c.	O
manning	O
,	O
and	O
s.	O
du-	O
mais	O
(	O
2011	O
)	O
.	O
partially	O
labeled	O
topic	O
models	O
for	O
interpretable	O
text	O
min-	O
ing	O
.	O
in	O
proc	O
.	O
of	O
the	O
int	O
’	O
l	O
conf	O
.	O
on	O
knowledge	B
discovery	I
and	O
data	O
min-	O
ing	O
.	O
ramaswamy	O
,	O
s.	O
,	O
p.	O
tamayo	O
,	O
r.	O
rifkin	O
,	O
s.	O
mukherjee	O
,	O
c.	O
yeang	O
,	O
m.	O
angelo	O
,	O
c.	O
ladd	O
,	O
m.	O
reich	O
,	O
e.	O
latulippe	O
,	O
j.	O
mesirov	O
,	O
t.	O
poggio	O
,	O
w.	O
gerald	O
,	O
m.	O
loda	O
,	O
e.	O
lander	O
,	O
and	O
t.	O
golub	O
(	O
2001	O
)	O
.	O
multiclass	O
cancer	O
diagno-	O
sis	O
using	O
tumor	O
gene	O
expression	O
signature	O
.	O
proc	O
.	O
of	O
the	O
national	O
academy	O
of	O
science	O
,	O
usa	O
98	O
,	O
15149–	O
15154.	O
ranzato	O
,	O
m.	O
and	O
g.	O
hinton	O
(	O
2010	O
)	O
.	O
modeling	O
pixel	O
means	O
and	O
covari-	O
ances	O
using	O
factored	O
third-order	O
boltzmann	O
machines	O
.	O
in	O
cvpr	O
.	O
ranzato	O
,	O
m.	O
,	O
f.-j	O
.	O
huang	O
,	O
y.-l.	O
boureau	O
,	O
and	O
y.	O
lecun	O
(	O
2007	O
)	O
.	O
un-	O
supervised	B
learning	I
of	O
invariant	B
feature	O
hierarchies	O
with	O
applica-	O
tions	O
to	O
object	O
recognition	O
.	O
in	O
cvpr	O
.	O
ranzato	O
,	O
m.	O
,	O
c.	O
poultney	O
,	O
s.	O
chopra	O
,	O
and	O
y.lecun	O
(	O
2006	O
)	O
.	O
efficient	O
learning	O
of	O
sparse	B
representations	O
with	O
an	O
energy-based	O
model	O
.	O
in	O
nips	O
.	O
rao	O
,	O
a.	O
and	O
k.	O
rose	O
(	O
2001	O
,	O
february	O
)	O
.	O
deterministically	O
annealed	O
design	O
of	O
hidden	B
markov	O
model	O
speech	O
recognizers	O
.	O
ieee	O
trans	O
.	O
on	O
speech	O
and	O
audio	O
proc	O
.	O
9	O
(	O
2	O
)	O
,	O
111–126	O
.	O
rasmussen	O
,	O
c.	O
(	O
2000	O
)	O
.	O
the	O
inﬁnite	O
gaussian	O
mixture	B
model	I
.	O
in	O
nips	O
.	O
rasmussen	O
,	O
c.	O
e.	O
and	O
j.	O
quiñonero-	O
candela	O
(	O
2005	O
)	O
.	O
healing	O
the	O
rele-	O
vance	O
vector	O
machine	O
by	O
augmen-	O
tation	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
,	O
pp	O
.	O
689–696	O
.	O
rasmussen	O
,	O
c.	O
e.	O
and	O
c.	O
k.	O
i.	O
williams	O
(	O
2006	O
)	O
.	O
gaussian	O
processes	O
for	O
ma-	O
chine	O
learning	B
.	O
mit	O
press	O
.	O
ratsch	O
,	O
g.	O
,	O
t.	O
onoda	O
,	O
and	O
k.	O
muller	O
(	O
2001	O
)	O
.	O
soft	O
margins	O
for	O
adaboost	O
.	O
machine	B
learning	I
42	O
,	O
287–320	O
.	O
rattray	O
,	O
m.	O
,	O
o.	O
stegle	O
,	O
k.	O
sharp	O
,	O
and	O
j.	O
winn	O
(	O
2009	O
)	O
.	O
inference	B
algo-	O
rithms	O
and	O
learning	B
theory	O
for	O
bayesian	O
sparse	B
factor	O
analysis	O
.	O
in	O
proc	O
.	O
intl	O
.	O
workshop	O
on	O
statistical-	O
mechanical	O
informatics	O
.	O
rauch	O
,	O
h.	O
e.	O
,	O
f.	O
tung	O
,	O
and	O
c.	O
t.	O
striebel	O
(	O
1965	O
)	O
.	O
maximum	O
likelihood	O
esti-	O
mates	O
of	O
linear	O
dynamic	O
systems	O
.	O
aiaa	O
journal	O
3	O
(	O
8	O
)	O
,	O
1445–1450	O
.	O
ravikumar	O
,	O
p.	O
,	O
j.	O
lafferty	O
,	O
h.	O
liu	O
,	O
and	O
l.	O
wasserman	O
(	O
2009	O
)	O
.	O
sparse	B
ad-	O
ditive	O
models	O
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
71	O
(	O
5	O
)	O
,	O
1009–1030	O
.	O
raydan	O
,	O
m.	O
(	O
1997	O
)	O
.	O
the	O
barzilai	O
and	O
borwein	O
gradient	O
method	O
for	O
the	O
large	O
scale	O
unconstrained	O
mini-	O
mization	O
problem	O
.	O
siam	O
j.	O
on	O
opti-	O
mization	O
7	O
(	O
1	O
)	O
,	O
26–33	O
.	O
rennie	O
,	O
j	O
.	O
(	O
2004	O
)	O
.	O
why	O
sums	O
are	O
bad	O
.	O
technical	O
report	O
,	O
mit	O
.	O
rennie	O
,	O
j.	O
,	O
l.	O
shih	O
,	O
j.	O
teevan	O
,	O
and	O
d.	O
karger	O
(	O
2003	O
)	O
.	O
tackling	O
the	O
poor	O
assumptions	O
of	O
naive	O
bayes	O
text	O
classiﬁers	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
reshed	O
,	O
d.	O
,	O
y.	O
reshef	O
,	O
h.	O
finucane	O
,	O
s.	O
grossman	O
,	O
g.	O
mcvean	O
,	O
p.	O
turn-	O
baugh	O
,	O
e.	O
lander	O
,	O
m.	O
mitzen-	O
macher	O
,	O
and	O
p.	O
sabeti	O
(	O
2011	O
,	O
de-	O
cember	O
)	O
.	O
detecting	O
novel	O
associa-	O
tions	O
in	O
large	O
data	O
sets	O
.	O
science	O
334	O
,	O
1518–1524	O
.	O
resnick	O
,	O
s.	O
i	O
.	O
(	O
1992	O
)	O
.	O
adventures	O
in	O
stochastic	B
processes	I
.	O
birkhauser	O
.	O
rice	O
,	O
j	O
.	O
(	O
1995	O
)	O
.	O
mathematical	O
statistics	O
and	O
data	O
analysis	O
.	O
duxbury	O
.	O
2nd	O
edition	O
.	O
bibliography	O
1037	O
richardson	O
,	O
s.	O
and	O
p.	O
green	O
(	O
1997	O
)	O
.	O
on	O
bayesian	O
analysis	O
of	O
mixtures	O
with	O
an	O
unknown	B
number	O
of	O
compo-	O
nents	O
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
59	O
,	O
731–758	O
.	O
riesenhuber	O
,	O
m.	O
and	O
t.	O
poggio	O
(	O
1999	O
)	O
.	O
hierarchical	O
models	O
of	O
ob-	O
ject	O
recognition	O
in	O
cortex	O
.	O
nature	O
neuroscience	O
2	O
,	O
1019–1025	O
.	O
rish	O
,	O
i.	O
,	O
g.	O
grabarnik	O
,	O
g.	O
cec-	O
chi	O
,	O
f.	O
pereira	O
,	O
and	O
g.	O
gordon	O
(	O
2008	O
)	O
.	O
closed-form	O
supervised	O
di-	O
mensionality	O
reduction	O
with	O
gener-	O
alized	O
linear	O
models	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
ristic	O
,	O
b.	O
,	O
s.	O
arulampalam	O
,	O
and	O
n.	O
gor-	O
don	O
(	O
2004	O
)	O
.	O
beyond	O
the	O
kalman	O
fil-	O
ter	O
:	O
particle	O
filters	O
for	O
tracking	B
ap-	O
plications	O
.	O
artech	O
house	O
radar	B
li-	O
brary	O
.	O
robert	O
,	O
c.	O
(	O
1995	O
)	O
.	O
simulation	O
of	O
trun-	O
cated	O
normal	B
distributions	O
.	O
statis-	O
tics	O
and	O
computing	O
5	O
,	O
121–125	O
.	O
robert	O
,	O
c.	O
and	O
g.	O
casella	O
(	O
2004	O
)	O
.	O
monte	O
carlo	O
statisical	O
methods	O
.	O
springer	O
.	O
2nd	O
edition	O
.	O
roberts	O
,	O
g.	O
and	O
j.	O
rosenthal	O
scaling	O
optimal	O
metropolis-hastings	O
statistical	O
science	O
16	O
,	O
351–367	O
.	O
for	O
(	O
2001	O
)	O
.	O
various	O
algorithms	O
.	O
roberts	O
,	O
g.	O
o.	O
and	O
s.	O
k.	O
sahu	O
(	O
1997	O
)	O
.	O
updating	O
schemes	O
,	O
corre-	O
lation	O
structure	O
,	O
blocking	O
and	O
pa-	O
rameterization	O
for	O
the	O
gibbs	O
sam-	O
pler	O
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
59	O
(	O
2	O
)	O
,	O
291–317	O
.	O
robinson	O
,	O
r.	O
w.	O
(	O
1973	O
)	O
.	O
counting	O
la-	O
beled	O
acyclic	O
digraphs	O
.	O
in	O
f.	O
harary	O
(	O
ed	O
.	O
)	O
,	O
new	O
directions	O
in	O
the	O
theory	O
of	O
graphs	O
,	O
pp	O
.	O
239–273	O
.	O
academic	O
press	O
.	O
roch	O
,	O
s.	O
(	O
2006	O
)	O
.	O
a	O
short	O
proof	O
that	O
phylogenetic	B
tree	I
reconstru-	O
tion	O
by	O
maximum	O
likelihood	O
is	O
hard	O
.	O
ieee/acm	O
trans	O
.	O
comp	O
.	O
bio	O
.	O
bioinformatics	O
31	O
(	O
1	O
)	O
.	O
rodriguez	O
,	O
a.	O
and	O
k.	O
modeling	O
through	O
nested	O
(	O
2011	O
)	O
.	O
data	O
models	O
.	O
biometrika	O
.	O
to	O
appear	O
.	O
ghosh	O
relational	O
partition	O
rose	O
,	O
k.	O
(	O
1998	O
,	O
november	O
)	O
.	O
determin-	O
istic	O
annealing	B
for	O
clustering	B
,	O
com-	O
pression	O
,	O
classiﬁcation	B
,	O
regression	B
,	O
and	O
related	O
optimization	B
problems	O
.	O
proc	O
.	O
ieee	O
80	O
,	O
2210–2239	O
.	O
rosenblatt	O
,	O
f.	O
(	O
1958	O
)	O
.	O
the	O
percep-	O
tron	O
:	O
a	O
probabilistic	O
model	O
for	O
information	B
storage	O
and	O
organiza-	O
tion	O
in	O
the	O
brain	O
.	O
psychological	O
re-	O
view	O
65	O
(	O
6	O
)	O
,	O
386â	O
˘a	O
¸s408	O
.	O
ross	O
,	O
s.	O
(	O
1989	O
)	O
.	O
introduction	O
to	O
proba-	O
bility	O
models	O
.	O
academic	O
press	O
.	O
rosset	O
,	O
s.	O
,	O
j.	O
zhu	O
,	O
and	O
t.	O
hastie	O
(	O
2004	O
)	O
.	O
boosting	B
as	O
a	O
regularized	O
path	O
to	O
a	O
maximum	O
margin	O
classiﬁer	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
5	O
,	O
941–	O
973.	O
rossi	O
,	O
p.	O
,	O
g.	O
allenby	O
,	O
and	O
r.	O
mcculloch	O
(	O
2006	O
)	O
.	O
bayesian	O
statistics	O
and	O
mar-	O
keting	O
.	O
wiley	O
.	O
roth	O
,	O
d.	O
(	O
1996	O
,	O
apr	O
)	O
.	O
on	O
the	O
hardness	O
of	O
approximate	O
reasoning	O
.	O
artiﬁcial	O
intelligence	O
82	O
(	O
1-2	O
)	O
,	O
273–302	O
.	O
rother	O
,	O
c.	O
,	O
p.	O
kohli	O
,	O
w.	O
feng	O
,	O
and	O
j.	O
jia	O
(	O
2009	O
)	O
.	O
minimizing	O
sparse	B
higher	O
order	O
energy	O
functions	O
of	O
discrete	B
variables	O
.	O
in	O
cvpr	O
,	O
pp	O
.	O
1382–1389	O
.	O
rouder	O
,	O
j.	O
,	O
p.	O
speckman	O
,	O
d.	O
sun	O
,	O
and	O
r.	O
morey	O
(	O
2009	O
)	O
.	O
bayesian	O
t	O
tests	O
for	O
accepting	O
and	O
rejecting	O
the	O
null	B
hypothesis	I
.	O
pyschonomic	O
bulletin	O
&	O
review	O
16	O
(	O
2	O
)	O
,	O
225–237	O
.	O
roverato	O
,	O
a.	O
hyper	O
(	O
2002	O
)	O
.	O
in-	O
verse	O
wishart	O
distribution	O
for	O
non-	O
decomposable	B
graphs	I
and	O
its	O
ap-	O
plication	O
to	O
bayesian	O
inference	B
for	O
gaussian	O
graphical	O
models	O
.	O
scand	O
.	O
j.	O
statistics	O
29	O
,	O
391–411	O
.	O
roweis	O
,	O
s.	O
(	O
1997	O
)	O
.	O
em	O
algorithms	O
for	O
pca	O
and	O
spca	O
.	O
in	O
nips	O
.	O
rubin	O
,	O
d.	O
(	O
1998	O
)	O
.	O
using	O
the	O
sir	O
algo-	O
rithm	O
to	O
simulate	O
posterior	O
distri-	O
butions	O
.	O
in	O
bayesian	O
statistics	O
3.	O
rue	O
,	O
h.	O
and	O
l.	O
held	O
(	O
2005	O
)	O
.	O
gaus-	O
sian	O
markov	O
random	O
fields	O
:	O
the-	O
ory	O
and	O
applications	O
,	O
volume	O
104	O
of	O
monographs	O
on	O
statistics	O
and	O
ap-	O
plied	O
probability	O
.	O
london	O
:	O
chap-	O
man	O
&	O
hall	O
.	O
rue	O
,	O
h.	O
,	O
s.	O
martino	O
,	O
and	O
n.	O
chopin	O
(	O
2009	O
)	O
.	O
approximate	O
bayesian	O
in-	O
ference	O
for	O
latent	B
gaussian	O
models	O
using	O
integrated	O
nested	O
laplace	O
approximations	O
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
71	O
,	O
319–392	O
.	O
rumelhart	O
,	O
d.	O
,	O
g.	O
hinton	O
,	O
and	O
r.	O
williams	O
(	O
1986	O
)	O
.	O
learning	B
inter-	O
nal	O
representations	O
by	O
error	O
propa-	O
gation	O
.	O
in	O
d.	O
rumelhart	O
,	O
j.	O
mcclel-	O
land	O
,	O
and	O
the	O
pdd	O
research	O
group	O
(	O
eds	O
.	O
)	O
,	O
parallel	O
distributed	O
process-	O
ing	O
:	O
explorations	O
in	O
the	O
microstruc-	O
ture	O
of	O
cognition	O
.	O
mit	O
press	O
.	O
ruppert	O
,	O
d.	O
,	O
m.	O
wand	O
,	O
and	O
r.	O
carroll	O
(	O
2003	O
)	O
.	O
semiparametric	O
regression	B
.	O
cambridge	O
university	O
press	O
.	O
rush	O
,	O
a.	O
m.	O
and	O
m.	O
collins	O
(	O
2012	O
)	O
.	O
a	O
tutorial	O
on	O
lagrangian	O
relaxation	O
and	O
dual	B
decomposition	I
for	O
nlp	O
.	O
technical	O
report	O
,	O
columbia	O
u.	O
russell	O
,	O
s.	O
,	O
j.	O
binder	O
,	O
d.	O
koller	O
,	O
and	O
k.	O
kanazawa	O
(	O
1995	O
)	O
.	O
local	O
learning	O
in	O
probabilistic	O
networks	O
with	O
hid-	O
den	O
variables	O
.	O
in	O
intl	O
.	O
joint	O
conf	O
.	O
on	O
ai	O
.	O
russell	O
,	O
s.	O
and	O
p.	O
norvig	O
(	O
1995	O
)	O
.	O
ar-	O
tiﬁcial	O
intelligence	O
:	O
a	O
modern	O
ap-	O
proach	O
.	O
englewood	O
cliffs	O
,	O
nj	O
:	O
pren-	O
tice	O
hall	O
.	O
russell	O
,	O
s.	O
and	O
p.	O
norvig	O
(	O
2002	O
)	O
.	O
ar-	O
tiﬁcial	O
intelligence	O
:	O
a	O
modern	O
ap-	O
proach	O
.	O
prentice	O
hall	O
.	O
2nd	O
edition	O
.	O
russell	O
,	O
s.	O
and	O
p.	O
norvig	O
(	O
2010	O
)	O
.	O
ar-	O
tiﬁcial	O
intelligence	O
:	O
a	O
modern	O
ap-	O
proach	O
.	O
prentice	O
hall	O
.	O
3rd	O
edition	O
.	O
s.	O
and	O
m.	O
black	O
(	O
2009	O
,	O
april	O
)	O
.	O
fields	O
j.	O
computer	O
vi-	O
intl	O
.	O
of	O
experts	O
.	O
sion	O
82	O
(	O
2	O
)	O
,	O
205–229	O
.	O
sachs	O
,	O
k.	O
,	O
o.	O
perez	O
,	O
d.	O
pe	O
’	O
er	O
,	O
d.	O
lauffenburger	O
,	O
and	O
g.	O
nolan	O
(	O
2005	O
)	O
.	O
causal	O
protein-signaling	O
networks	O
derived	O
from	O
multipa-	O
rameter	O
sci-	O
ence	O
308.	O
single-cell	O
data	O
.	O
sahami	O
,	O
m.	O
and	O
t.	O
heilman	O
(	O
2006	O
)	O
.	O
a	O
web-based	O
kernel	B
function	I
for	O
measuring	O
the	O
similarity	O
of	O
short	O
text	O
snippets	O
.	O
in	O
www	O
conferenec	O
.	O
salakhutdinov	O
,	O
r.	O
erative	O
models	O
.	O
toronto	O
.	O
(	O
2009	O
)	O
.	O
deep	B
gen-	O
thesis	O
,	O
u.	O
ph.d.	O
salakhutdinov	O
,	O
r.	O
and	O
g.	O
hinton	O
(	O
2009	O
)	O
.	O
deep	B
boltzmann	O
machines	O
.	O
in	O
ai/statistics	O
,	O
volume	O
5	O
,	O
pp	O
.	O
448–	O
455.	O
salakhutdinov	O
,	O
r.	O
and	O
g.	O
hinton	O
an	O
(	O
2010	O
)	O
.	O
undirected	B
topic	O
model	O
.	O
in	O
nips	O
.	O
replicated	O
softmax	O
:	O
salakhutdinov	O
,	O
r.	O
and	O
h.	O
larochelle	O
(	O
2010	O
)	O
.	O
efficient	O
learning	O
of	O
deep	B
boltzmann	O
machines	O
.	O
in	O
ai/statis-	O
tics	O
.	O
salakhutdinov	O
,	O
r.	O
and	O
a.	O
mnih	O
(	O
2008	O
)	O
.	O
probabilistic	B
matrix	I
factorization	I
.	O
in	O
nips	O
,	O
volume	O
20	O
.	O
1038	O
bibliography	O
salakhutdinov	O
,	O
r.	O
and	O
s.	O
roweis	O
(	O
2003	O
)	O
.	O
adaptive	O
overrelaxed	O
bound	B
optimization	I
methods	O
.	O
in	O
proceed-	O
ings	O
of	O
the	O
international	O
conference	O
on	O
machine	B
learning	I
,	O
volume	O
20	O
,	O
pp	O
.	O
664–671	O
.	O
schaefer	O
,	O
j.	O
and	O
k.	O
strimmer	O
(	O
2005	O
)	O
.	O
a	O
shrinkage	B
approach	O
to	O
large-	O
scale	O
covariance	O
matrix	O
estimation	O
and	O
implications	O
for	O
functional	O
ge-	O
nomics	O
.	O
statist	O
.	O
appl	O
.	O
genet	O
.	O
mol	O
.	O
biol	O
4	O
(	O
32	O
)	O
.	O
salakhutdinov	O
,	O
r.	O
,	O
j.	O
tenenbaum	O
,	O
and	O
a.	O
torralba	O
(	O
2011	O
)	O
.	O
learning	B
to	I
learn	I
with	O
compound	O
hd	O
models	O
.	O
in	O
nips	O
.	O
salakhutdinov	O
,	O
r.	O
r.	O
,	O
a.	O
mnih	O
,	O
and	O
g.	O
e.	O
hinton	O
(	O
2007	O
)	O
.	O
restricted	O
boltzmann	O
machines	O
for	O
collabo-	O
rative	O
ﬁltering	B
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
,	O
volume	O
24	O
,	O
pp	O
.	O
791–798	O
.	O
salojarvi	O
,	O
j.	O
,	O
k.	O
puolamaki	O
,	O
and	O
s.	O
klaski	O
(	O
2005	O
)	O
.	O
on	O
discriminative	B
joint	O
density	O
modeling	O
.	O
in	O
proc	O
.	O
eu-	O
ropean	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
sampson	O
,	O
f.	O
(	O
1968	O
)	O
.	O
a	O
novitiate	O
in	O
a	O
period	B
of	O
change	O
:	O
an	O
experimental	O
and	O
case	O
study	O
of	O
social	O
relation-	O
ships	O
.	O
ph.d.	O
thesis	O
,	O
cornell	O
.	O
santner	O
,	O
t.	O
,	O
b.	O
williams	O
,	O
and	O
w.	O
notz	O
(	O
2003	O
)	O
.	O
the	O
design	O
and	O
analysis	O
of	O
computer	O
experiments	O
.	O
springer	O
.	O
sarkar	O
,	O
j	O
.	O
(	O
1991	O
)	O
.	O
one-armed	B
bandit	I
problems	O
with	O
covariates	B
.	O
the	O
an-	O
nals	O
of	O
statistics	O
19	O
(	O
4	O
)	O
,	O
1978–2002	O
.	O
sato	O
,	O
m.	O
and	O
s.	O
ishii	O
(	O
2000	O
)	O
.	O
on-line	O
em	O
algorithm	O
for	O
the	O
normalized	O
gaussian	O
network	O
.	O
neural	O
compu-	O
tation	O
12	O
,	O
407–432	O
.	O
saul	O
,	O
l.	O
,	O
t.	O
jaakkola	O
,	O
and	O
m.	O
jordan	O
(	O
1996	O
)	O
.	O
mean	B
field	O
theory	O
for	O
sig-	O
moid	O
belief	B
networks	I
.	O
j.	O
of	O
ai	O
re-	O
search	O
4	O
,	O
61–76	O
.	O
saul	O
,	O
l.	O
and	O
m.	O
jordan	O
(	O
1995	O
)	O
.	O
exploit-	O
ing	O
tractable	O
substructures	O
in	O
in-	O
tractable	O
networks	O
.	O
in	O
nips	O
,	O
vol-	O
ume	O
8.	O
saul	O
,	O
l.	O
and	O
m.	O
jordan	O
(	O
2000	O
)	O
.	O
attrac-	O
tor	O
dynamics	O
in	O
feedforward	O
neural	O
networks	O
.	O
neural	O
computation	O
12	O
,	O
1313–1335	O
.	O
saunders	O
,	O
c.	O
,	O
j.	O
shawe-taylor	O
,	O
and	O
a.	O
vinokourov	O
(	O
2003	O
)	O
.	O
string	O
ker-	O
nels	O
,	O
fisher	O
kernels	O
and	O
finite	O
state	B
automata	O
.	O
in	O
nips	O
.	O
savage	O
,	O
r.	O
,	O
k.	O
heller	O
,	O
y.	O
xi	O
,	O
z.	O
ghahra-	O
mani	O
,	O
w.	O
truman	O
,	O
m.	O
grant	O
,	O
k.	O
denby	O
,	O
and	O
d.	O
wild	O
(	O
2009	O
)	O
.	O
r/bhc	O
:	O
fast	O
bayesian	O
hierarchi-	O
cal	O
clustering	B
for	O
microarray	O
data	O
.	O
bmc	O
bioinformatics	O
10	O
(	O
242	O
)	O
.	O
schapire	O
,	O
r.	O
(	O
1990	O
)	O
.	O
the	O
strength	O
of	O
weak	O
learnability	O
.	O
machine	O
learn-	O
ing	O
5	O
,	O
197–227	O
.	O
schapire	O
,	O
r.	O
and	O
y.	O
freund	O
(	O
2012	O
)	O
.	O
foundations	O
and	O
algo-	O
boosting	B
:	O
rithms	O
.	O
mit	O
press	O
.	O
schapire	O
,	O
r.	O
,	O
y.	O
freund	O
,	O
p.	O
bartlett	O
,	O
and	O
w.	O
lee	O
(	O
1998	O
)	O
.	O
boosting	B
the	O
mar-	O
gin	O
:	O
a	O
new	O
explanation	O
for	O
the	O
ef-	O
fectiveness	O
of	O
voting	O
methods	O
.	O
an-	O
nals	O
of	O
statistics	O
5	O
,	O
1651–1686	O
.	O
scharstein	O
,	O
d.	O
and	O
r.	O
szeliski	O
(	O
2002	O
)	O
.	O
a	O
taxonomy	O
and	O
evaluation	O
of	O
dense	O
two-frame	O
stereo	O
correspondence	B
algorithms	O
.	O
j.	O
computer	O
vi-	O
sion	O
47	O
(	O
1	O
)	O
,	O
7–42	O
.	O
intl	O
.	O
schaul	O
,	O
t.	O
,	O
s.	O
zhang	O
,	O
and	O
y.	O
lecun	O
(	O
2012	O
)	O
.	O
no	O
more	O
pesky	O
learning	B
rates	O
.	O
technical	O
report	O
,	O
courant	O
in-	O
stite	O
of	O
mathematical	O
sciences	O
.	O
schmee	O
,	O
j.	O
and	O
g.	O
hahn	O
(	O
1979	O
)	O
.	O
a	O
sim-	O
ple	O
method	O
for	O
regresssion	O
analy-	O
sis	O
with	O
censored	O
data	O
.	O
technomet-	O
rics	O
21	O
,	O
417–432	O
.	O
schmidt	O
,	O
m.	O
(	O
2010	O
)	O
.	O
graphical	B
model	I
structure	O
learning	B
with	O
l1	O
regular-	O
ization	O
.	O
ph.d.	O
thesis	O
,	O
ubc	O
.	O
schmidt	O
,	O
m.	O
,	O
g.	O
fung	O
,	O
and	O
r.	O
rosales	O
(	O
2009	O
)	O
.	O
optimization	B
methods	O
for	O
(	O
cid:7	O
)	O
−	O
1	O
regularization	B
.	O
technical	O
re-	O
port	O
,	O
u.	O
british	O
columbia	O
.	O
schmidt	O
,	O
m.	O
and	O
k.	O
murphy	O
(	O
2009	O
)	O
.	O
modeling	O
discrete	B
interventional	O
data	O
using	O
directed	B
cyclic	O
graphi-	O
cal	O
models	O
.	O
in	O
uai	O
.	O
schmidt	O
,	O
m.	O
,	O
k.	O
murphy	O
,	O
g.	O
fung	O
,	O
and	O
r.	O
rosales	O
(	O
2008	O
)	O
.	O
structure	O
learn-	O
ing	O
in	O
random	O
fields	O
for	O
heart	O
motion	O
abnormality	O
detection	O
.	O
in	O
cvpr	O
.	O
schmidt	O
,	O
m.	O
,	O
a.	O
niculescu-mizil	O
,	O
and	O
k.	O
murphy	O
(	O
2007	O
)	O
.	O
learning	B
graph-	O
ical	O
model	O
structure	O
using	O
l1-	O
regularization	B
paths	O
.	O
in	O
aaai	O
.	O
schmidt	O
,	O
m.	O
,	O
e.	O
van	O
den	O
berg	O
,	O
m.	O
friedlander	O
,	O
and	O
k.	O
murphy	O
(	O
2009	O
)	O
.	O
optimizing	O
costly	O
func-	O
tions	O
with	O
simple	O
constraints	O
:	O
a	O
limited-memory	O
projected	O
quasi-	O
newton	O
algorithm	O
.	O
in	O
ai	O
&	O
statis-	O
tics	O
.	O
schniter	O
,	O
p.	O
,	O
l.	O
c.	O
potter	O
,	O
and	O
j.	O
ziniel	O
(	O
2008	O
)	O
.	O
fast	O
bayesian	O
matching	B
pursuit	I
:	O
model	O
uncertainty	O
and	O
parameter	B
estimation	O
for	O
sparse	B
linear	O
models	O
.	O
technical	O
report	O
,	O
u.	O
ohio	O
.	O
submitted	O
to	O
ieee	O
trans	O
.	O
on	O
signal	B
processing	I
.	O
schnitzspan	O
,	O
p.	O
,	O
s.	O
roth	O
,	O
and	O
b.	O
schiele	O
(	O
2010	O
)	O
.	O
automatic	O
discovery	O
of	O
meaningful	O
object	O
parts	O
with	O
latent	B
crfs	O
.	O
in	O
cvpr	O
.	O
schoelkopf	O
,	O
b.	O
and	O
a.	O
smola	O
(	O
2002	O
)	O
.	O
learning	B
with	O
kernels	O
:	O
support	B
vec-	O
tor	O
machines	O
,	O
regularization	B
,	O
opti-	O
mization	O
,	O
and	O
beyond	O
.	O
mit	O
press	O
.	O
schoelkopf	O
,	O
b.	O
,	O
a.	O
smola	O
,	O
and	O
k.-r.	O
mueller	O
(	O
1998	O
)	O
.	O
nonlinear	O
compo-	O
nent	O
analysis	O
as	O
a	O
kernel	B
eigen-	O
value	O
problem	O
.	O
neural	O
computa-	O
tion	O
10	O
,	O
1299	O
–	O
1319.	O
schraudolph	O
,	O
n.	O
n.	O
,	O
j.	O
yu	O
,	O
and	O
s.	O
gün-	O
ter	O
(	O
2007	O
)	O
.	O
a	O
stochastic	O
quasi-	O
newton	O
method	O
for	O
online	O
convex	O
optimization	B
.	O
in	O
ai/statistics	O
,	O
pp	O
.	O
436–443	O
.	O
schwarz	O
,	O
g.	O
(	O
1978	O
)	O
.	O
estimating	O
the	O
di-	O
mension	O
of	O
a	O
model	O
.	O
annals	O
of	O
statistics	O
6	O
(	O
2	O
)	O
,	O
461â	O
˘a	O
¸s464	O
.	O
schwarz	O
,	O
r.	O
and	O
y.	O
chow	O
(	O
1990	O
)	O
.	O
the	O
n-best	O
algorithm	O
:	O
an	O
efficient	O
and	O
exact	O
procedure	O
for	O
ﬁnding	O
the	O
n	O
most	O
in	O
intl	O
.	O
conf	O
.	O
on	O
acoustics	O
,	O
speech	O
and	O
sig-	O
nal	O
proc	O
.	O
likely	O
hypotheses	O
.	O
schweikerta	O
,	O
g.	O
,	O
a.	O
zien	O
,	O
g.	O
zeller	O
,	O
j.	O
behr	O
,	O
c.	O
dieterich	O
,	O
c.	O
ong	O
,	O
p.	O
philips	O
,	O
f.	O
d.	O
bona	O
,	O
l.	O
hartmann	O
,	O
a.	O
bohlen	O
,	O
n.	O
krãijger	O
,	O
s.	O
son-	O
nenburg	O
,	O
and	O
g.	O
rãd	O
’	O
tsch	O
(	O
2009	O
)	O
.	O
mgene	O
:	O
accurate	O
svm-based	O
gene	O
finding	O
with	O
an	O
application	O
to	O
ne-	O
matode	O
genomes	O
.	O
genome	B
re-	O
search	O
,	O
19	O
,	O
2133–2143	O
.	O
scott	O
,	O
d.	O
(	O
1979	O
)	O
.	O
and	O
biometrika	O
66	O
(	O
3	O
)	O
,	O
605–610	O
.	O
data-based	O
on	O
optimal	O
histograms	O
.	O
scott	O
,	O
j.	O
g.	O
and	O
c.	O
m.	O
carvalho	O
(	O
2008	O
)	O
.	O
feature-inclusion	O
stochastic	B
search	I
for	O
gaussian	O
graphical	O
models	O
.	O
j.	O
of	O
computational	O
and	O
graphical	O
statistics	O
17	O
(	O
4	O
)	O
,	O
790–808	O
.	O
scott	O
,	O
s.	O
(	O
2009	O
)	O
.	O
data	O
augmenta-	O
tion	O
,	O
frequentist	B
estimation	O
,	O
and	O
the	O
bayesian	O
analysis	O
of	O
multino-	O
mial	O
logit	B
models	O
.	O
statistical	O
papers	O
.	O
scott	O
,	O
s.	O
(	O
2010	O
)	O
.	O
a	O
modern	O
bayesian	O
look	O
at	O
the	O
multi-armed	B
bandit	I
.	O
applied	O
stochastic	O
models	O
in	O
busi-	O
ness	O
and	O
industry	O
26	O
,	O
639–658	O
.	O
bibliography	O
1039	O
sedgewick	O
,	O
r.	O
and	O
k.	O
wayne	O
(	O
2011	O
)	O
.	O
al-	O
gorithms	O
.	O
addison	O
wesley	O
.	O
seeger	O
,	O
m.	O
(	O
2008	O
)	O
.	O
bayesian	O
inference	B
and	O
optimal	O
design	O
in	O
the	O
sparse	B
linear	O
model	O
.	O
j.	O
of	O
machine	O
learn-	O
ing	O
research	O
9	O
,	O
759–813	O
.	O
seeger	O
,	O
m.	O
and	O
h.	O
nickish	O
(	O
2008	O
)	O
.	O
compressed	B
sensing	I
and	O
bayesian	O
experimental	O
design	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
segal	O
,	O
d.	O
(	O
2011	O
,	O
12	O
february	O
)	O
.	O
the	O
dirty	O
little	O
secrets	O
of	O
search	O
.	O
new	O
york	O
times	O
.	O
seide	O
,	O
f.	O
,	O
g.	O
li	O
,	O
and	O
d.	O
yu	O
(	O
2011	O
)	O
.	O
con-	O
versational	O
speech	O
transcription	O
using	O
context-dependent	O
deep	B
neural	O
networks	O
.	O
in	O
interspeech	O
.	O
sejnowski	O
,	O
t.	O
and	O
c.	O
rosenberg	O
(	O
1987	O
)	O
.	O
parallel	O
networks	O
that	O
learn	O
to	O
pro-	O
nounce	O
english	O
text	O
.	O
complex	O
sys-	O
tems	O
1	O
,	O
145–168	O
.	O
sellke	O
,	O
t.	O
,	O
m.	O
j.	O
bayarri	O
,	O
and	O
j.	O
berger	O
(	O
2001	O
)	O
.	O
calibration	B
of	O
p	O
values	O
for	O
testing	O
precise	O
null	O
hypothe-	O
ses	O
.	O
the	O
american	O
statistician	O
55	O
(	O
1	O
)	O
,	O
62–71	O
.	O
serre	O
,	O
t.	O
,	O
l.	O
wolf	O
,	O
and	O
t.	O
poggio	O
(	O
2005	O
)	O
.	O
recognition	O
with	O
features	B
object	O
inspired	O
by	O
visual	O
cortex	O
.	O
in	O
cvpr	O
,	O
pp	O
.	O
994–1000	O
.	O
shachter	O
,	O
r.	O
(	O
1998	O
)	O
.	O
bayes-ball	O
:	O
the	O
rational	O
pastime	O
(	O
for	O
determining	O
irrelevance	O
and	O
requisite	O
informa-	O
tion	O
in	O
belief	B
networks	I
and	O
inﬂu-	O
ence	O
diagrams	O
)	O
.	O
in	O
uai	O
.	O
r.	O
and	O
c.	O
r.	O
kenley	O
shachter	O
,	O
(	O
1989	O
)	O
.	O
gaussian	O
inﬂuence	O
dia-	O
grams	O
.	O
managment	O
science	O
35	O
(	O
5	O
)	O
,	O
527–550	O
.	O
shachter	O
,	O
r.	O
d.	O
and	O
m.	O
a.	O
peot	O
(	O
1989	O
)	O
.	O
simulation	O
approaches	O
to	O
general	O
probabilistic	B
inference	I
on	O
belief	B
networks	I
.	O
in	O
uai	O
,	O
volume	O
5.	O
shafer	O
,	O
g.	O
r.	O
and	O
p.	O
p.	O
shenoy	O
(	O
1990	O
)	O
.	O
probability	O
propagation	O
.	O
annals	O
of	O
mathematics	O
and	O
ai	O
2	O
,	O
327–352	O
.	O
shafto	O
,	O
p.	O
,	O
c.	O
kemp	O
,	O
v.	O
mansinghka	O
,	O
m.	O
gordon	O
,	O
and	O
j.	O
b.	O
tenenbaum	O
(	O
2006	O
)	O
.	O
learning	B
cross-cutting	O
sys-	O
tems	O
of	O
categories	O
.	O
in	O
cognitive	O
sci-	O
ence	O
conference	O
.	O
shahaf	O
,	O
d.	O
,	O
a.	O
chechetka	O
,	O
and	O
c.	O
guestrin	O
(	O
2009	O
)	O
.	O
learning	B
thin	O
junction	B
trees	I
via	O
graph	B
cuts	I
.	O
in	O
aistats	O
.	O
shalev-shwartz	O
,	O
s.	O
,	O
y.	O
singer	O
,	O
and	O
n.	O
srebro	O
(	O
2007	O
)	O
.	O
pegasos	O
:	O
pri-	O
mal	O
estimated	O
sub-gradient	O
solver	O
for	O
svm	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
shalizi	O
,	O
c.	O
(	O
2009	O
)	O
.	O
cs	O
36-350	O
lecture	O
10	O
:	O
principal	B
components	I
:	O
mathe-	O
matics	O
,	O
example	O
,	O
interpretation	O
.	O
shan	O
,	O
h.	O
and	O
a.	O
banerjee	O
(	O
2010	O
)	O
.	O
resid-	O
ual	O
bayesian	O
co-clustering	B
for	O
ma-	O
trix	O
approximation	O
.	O
in	O
siam	O
intl	O
.	O
conf	O
.	O
on	O
data	O
mining	O
.	O
shawe-taylor	O
,	O
j.	O
and	O
n.	O
cristianini	O
(	O
2004	O
)	O
.	O
kernel	B
methods	O
for	O
pattern	B
analysis	O
.	O
cambridge	O
.	O
sheng	O
,	O
q.	O
,	O
y.	O
moreau	O
,	O
and	O
b.	O
d.	O
moor	O
(	O
2003	O
)	O
.	O
biclustering	B
microarray	O
data	O
by	O
gibbs	O
sampling	O
.	O
bioinformat-	O
ics	O
19	O
,	O
ii196–ii205	O
.	O
shi	O
,	O
j.	O
and	O
j.	O
malik	O
(	O
2000	O
)	O
.	O
normal-	O
ized	O
cuts	O
and	O
image	B
segmentation	I
.	O
ieee	O
trans	O
.	O
on	O
pattern	B
analysis	O
and	O
machine	O
intelligence	O
.	O
shoham	O
,	O
y.	O
and	O
k.	O
leyton-brown	O
(	O
2009	O
)	O
.	O
multiagent	O
systems	O
:	O
algo-	O
rithmic	O
,	O
game-	O
theoretic	O
,	O
and	O
log-	O
ical	O
foundations	O
.	O
cambridge	O
uni-	O
versity	O
press	O
.	O
shotton	O
,	O
j.	O
,	O
a.	O
fitzgibbon	O
,	O
m.	O
cook	O
,	O
t.	O
sharp	O
,	O
m.	O
finocchio	O
,	O
r.	O
moore	O
,	O
a.	O
kipman	O
,	O
and	O
a.	O
blake	O
(	O
2011	O
)	O
.	O
real-time	O
human	O
pose	O
recognition	O
in	O
parts	O
from	O
a	O
single	O
depth	O
image	O
.	O
in	O
cvpr	O
.	O
shwe	O
,	O
m.	O
,	O
b.	O
middleton	O
,	O
d.	O
heck-	O
erman	O
,	O
m.	O
henrion	O
,	O
e.	O
horvitz	O
,	O
h.	O
lehmann	O
,	O
and	O
g.	O
cooper	O
(	O
1991	O
)	O
.	O
probabilistic	O
diagnosis	O
using	O
a	O
re-	O
formulation	O
of	O
the	O
internist-1/qmr	O
knowledge	B
base	I
.	O
inf	O
.	O
med	O
30	O
(	O
4	O
)	O
,	O
241–255	O
.	O
methods	O
.	O
siddiqi	O
,	O
s.	O
,	O
b.	O
boots	O
,	O
and	O
g.	O
gordon	O
(	O
2007	O
)	O
.	O
a	O
constraint	O
generation	O
ap-	O
proach	O
to	O
learning	B
stable	O
linear	O
dy-	O
namical	O
systems	O
.	O
in	O
nips	O
.	O
siepel	O
,	O
a.	O
and	O
d.	O
haussler	O
(	O
2003	O
)	O
.	O
combining	O
phylogenetic	O
and	O
hid-	O
den	O
markov	O
models	O
in	O
biosequence	B
analysis	I
.	O
in	O
proc	O
.	O
7th	O
intl	O
.	O
conf	O
.	O
on	O
computational	O
molecular	O
biol-	O
ogy	O
(	O
recomb	O
)	O
.	O
silander	O
,	O
t.	O
,	O
p.	O
kontkanen	O
,	O
and	O
p.	O
myl-	O
lymãd	O
’	O
ki	O
(	O
2007	O
)	O
.	O
on	O
sensitivity	B
of	O
the	O
map	O
bayesian	O
network	O
struc-	O
ture	O
to	O
the	O
equivalent	B
sample	I
size	I
parameter	O
.	O
in	O
uai	O
,	O
pp	O
.	O
360–367	O
.	O
silander	O
,	O
t.	O
and	O
p.	O
myllmaki	O
(	O
2006	O
)	O
.	O
a	O
simple	O
approach	O
for	O
ﬁnding	O
the	O
globally	O
optimal	O
bayesian	O
network	O
structure	O
.	O
in	O
uai	O
.	O
sill	O
,	O
j.	O
,	O
g.	O
takacs	O
,	O
l.	O
mackey	O
,	O
and	O
d.	O
lin	O
(	O
2009	O
)	O
.	O
feature-weighted	O
lin-	O
ear	O
stacking	B
.	O
technical	O
report	O
,	O
.	O
(	O
1984	O
)	O
.	O
silverman	O
,	O
b.	O
w.	O
spline	B
smoothing	O
:	O
the	O
equivalent	O
variable	O
kernel	B
method	O
.	O
annals	O
of	O
statis-	O
tics	O
12	O
(	O
3	O
)	O
,	O
898–916	O
.	O
simard	O
,	O
p.	O
,	O
d.	O
steinkraus	O
,	O
and	O
j.	O
platt	O
(	O
2003	O
)	O
.	O
best	O
practices	O
for	O
convolu-	O
tional	O
neural	B
networks	I
applied	O
to	O
visual	O
document	O
analysis	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
document	O
analysis	O
and	O
recognition	O
(	O
icdar	O
)	O
.	O
simon	O
,	O
d.	O
(	O
2006	O
)	O
.	O
optimal	O
state	O
es-	O
timation	O
:	O
kalman	O
,	O
h	O
inﬁnity	O
,	O
and	O
nonlinear	O
approaches	O
.	O
wiley	O
.	O
singliar	O
,	O
t.	O
and	O
m.	O
hauskrecht	O
(	O
2006	O
)	O
.	O
noisy-or	O
component	O
analysis	O
and	O
its	O
application	O
to	O
link	O
analysis	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
7.	O
smidl	O
,	O
v.	O
and	O
a.	O
quinn	O
(	O
2005	O
)	O
.	O
the	O
variational	O
bayes	O
method	O
in	O
signal	B
processing	I
.	O
springer	O
.	O
smith	O
,	O
a.	O
f.	O
m.	O
and	O
a.	O
e.	O
gelfand	O
(	O
1992	O
)	O
.	O
bayesian	O
statistics	O
with-	O
out	O
tears	O
:	O
a	O
sampling-resampling	O
perspective	O
.	O
the	O
american	O
statisti-	O
cian	O
46	O
(	O
2	O
)	O
,	O
84–88	O
.	O
smith	O
,	O
r.	O
and	O
p.	O
cheeseman	O
(	O
1986	O
)	O
.	O
on	O
the	O
representation	O
and	O
estima-	O
tion	O
of	O
spatial	O
uncertainty	B
.	O
intl	O
.	O
j.	O
robotics	O
research	O
5	O
(	O
4	O
)	O
,	O
56–68	O
.	O
smith	O
,	O
t.	O
v.	O
,	O
j.	O
yu	O
,	O
smulders	O
,	O
a.	O
hartemink	O
,	O
and	O
e.	O
jarvis	O
(	O
2006	O
)	O
.	O
computational	O
inference	O
of	O
neural	O
information	O
flow	O
networks	O
.	O
plos	O
computational	O
biology	O
2	O
,	O
1436–	O
1439.	O
smolensky	O
,	O
p.	O
(	O
1986	O
)	O
.	O
information	B
processing	O
in	O
dynamical	O
systems	O
:	O
foundations	O
of	O
harmony	O
theory	O
.	O
in	O
d.	O
rumehart	O
and	O
j.	O
mcclel-	O
land	O
(	O
eds	O
.	O
)	O
,	O
parallel	O
distributed	O
pro-	O
cessing	O
:	O
explorations	O
in	O
the	O
mi-	O
crostructure	O
of	O
cognition	O
.	O
volume	O
1.	O
mcgraw-hill	O
.	O
smyth	O
,	O
p.	O
,	O
d.	O
heckerman	O
,	O
and	O
m.	O
i.	O
jor-	O
dan	O
(	O
1997	O
)	O
.	O
probabilistic	O
indepen-	O
dence	O
networks	O
for	O
hidden	B
markov	O
probability	O
models	O
.	O
neural	O
compu-	O
tation	O
9	O
(	O
2	O
)	O
,	O
227–269	O
.	O
sohl-dickstein	O
,	O
m.	O
deweese	O
(	O
2011	O
)	O
.	O
on	O
machine	B
learning	I
.	O
j.	O
,	O
p.	O
battaglino	O
,	O
and	O
in	O
intl	O
.	O
conf	O
.	O
1040	O
bibliography	O
sollich	O
,	O
p.	O
(	O
2002	O
)	O
.	O
bayesian	O
methods	O
for	O
support	B
vector	I
machines	I
:	O
evi-	O
dence	O
and	O
predictive	B
class	O
proba-	O
bilities	O
.	O
machine	B
learning	I
46	O
,	O
21–	O
52.	O
sontag	O
,	O
d.	O
,	O
a.	O
globerson	O
,	O
and	O
t.	O
jaakkola	O
(	O
2011	O
)	O
.	O
introduction	O
to	O
dual	B
decomposition	I
for	O
inference	B
.	O
in	O
s.	O
sra	O
,	O
s.	O
nowozin	O
,	O
and	O
s.	O
j.	O
wright	O
(	O
eds	O
.	O
)	O
,	O
optimization	B
for	O
ma-	O
chine	O
learning	B
.	O
mit	O
press	O
.	O
sorenson	O
,	O
h.	O
and	O
d.	O
alspach	O
(	O
1971	O
)	O
.	O
recursive	B
bayesian	O
estimation	O
us-	O
ing	O
gaussian	O
sums	O
.	O
automatica	O
7	O
,	O
465â	O
˘a	O
¸s	O
479.	O
soussen	O
,	O
c.	O
,	O
j.	O
iier	O
,	O
d.	O
brie	O
,	O
and	O
j.	O
duan	O
(	O
2010	O
)	O
.	O
from	O
bernoulli-	O
gaussian	O
deconvolution	O
to	O
sparse	B
signal	O
restoration	O
.	O
technical	O
report	O
,	O
centre	O
de	O
recherche	O
en	O
automa-	O
tique	O
de	O
nancy	O
.	O
spaan	O
,	O
m.	O
and	O
n.	O
vlassis	O
(	O
2005	O
)	O
.	O
perseus	O
:	O
randomized	O
point-based	O
value	O
iteration	O
for	O
pomdps	O
.	O
j.	O
of	O
ai	O
research	O
24	O
,	O
195–220	O
.	O
spall	O
,	O
j	O
.	O
(	O
2003	O
)	O
.	O
introduction	O
to	O
stochas-	O
tic	O
search	O
and	O
optimization	B
:	O
es-	O
timation	O
,	O
simulation	O
,	O
and	O
control	O
.	O
wiley	O
.	O
speed	O
,	O
t.	O
(	O
2011	O
,	O
december	O
)	O
.	O
a	O
cor-	O
relation	B
for	O
the	O
21st	O
century	O
.	O
sci-	O
ence	O
334	O
,	O
152–1503	O
.	O
speed	O
,	O
t.	O
and	O
h.	O
kiiveri	O
(	O
1986	O
)	O
.	O
gaus-	O
sian	O
markov	O
distributions	O
over	O
ﬁ-	O
nite	O
graphs	O
.	O
annals	O
of	O
statis-	O
tics	O
14	O
(	O
1	O
)	O
,	O
138–150	O
.	O
spiegelhalter	O
,	O
d.	O
j.	O
and	O
s.	O
l.	O
lauritzen	O
(	O
1990	O
)	O
.	O
sequential	B
updating	O
of	O
con-	O
ditional	O
probabilities	O
on	O
directed	O
graphical	O
structures	O
.	O
networks	O
20.	O
spirtes	O
,	O
p.	O
,	O
scheines	O
c.	O
glymour	O
,	O
and	O
r.	O
causa-	O
tion	O
,	O
prediction	O
,	O
and	O
search	O
.	O
mit	O
press	O
.	O
2nd	O
edition	O
.	O
(	O
2000	O
)	O
.	O
srebro	O
,	O
n.	O
(	O
2001	O
)	O
.	O
maximum	O
likelihood	O
bounded	O
tree-width	O
markov	O
net-	O
works	O
.	O
in	O
uai	O
.	O
srebro	O
,	O
n.	O
and	O
t.	O
jaakkola	O
(	O
2003	O
)	O
.	O
approxima-	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	O
low-rank	O
weighted	O
tions	O
.	O
learning	B
.	O
steinbach	O
,	O
m.	O
,	O
g.	O
karypis	O
,	O
and	O
v.	O
ku-	O
mar	O
(	O
2000	O
)	O
.	O
a	O
comparison	O
of	O
doc-	O
ument	O
clustering	B
techniques	O
.	O
in	O
kdd	O
workshop	O
on	O
text	O
mining	O
.	O
stephens	O
,	O
m.	O
(	O
2000	O
)	O
.	O
dealing	O
with	O
label-switching	O
in	O
mixture	B
mod-	O
els	O
.	O
j.	O
royal	O
statistical	O
society	O
,	O
series	O
b	O
62	O
,	O
795–809	O
.	O
stern	O
,	O
d.	O
,	O
r.	O
herbrich	O
,	O
and	O
t.	O
grae-	O
pel	O
(	O
2009	O
)	O
.	O
matchbox	O
:	O
large	O
scale	O
bayesian	O
recommendations	O
.	O
in	O
proc	O
.	O
18th	O
.	O
intl	O
.	O
world	O
wide	O
web	O
conference	O
.	O
steyvers	O
,	O
m.	O
and	O
t.	O
griffiths	O
(	O
2007	O
)	O
.	O
probabilistic	O
topic	O
models	O
.	O
in	O
t.	O
landauer	O
,	O
d.	O
mcnamara	O
,	O
s.	O
den-	O
nis	O
,	O
and	O
w.	O
kintsch	O
(	O
eds	O
.	O
)	O
,	O
latent	B
semantic	I
analysis	I
:	O
a	O
road	O
to	O
mean-	O
ing	O
.	O
laurence	O
erlbaum	O
.	O
stigler	O
,	O
s.	O
(	O
1986	O
)	O
.	O
the	O
history	O
of	O
statis-	O
tics	O
.	O
harvard	O
university	O
press	O
.	O
stolcke	O
,	O
a.	O
and	O
s.	O
m.	O
omohundro	O
(	O
1992	O
)	O
.	O
hidden	B
markov	O
model	O
in-	O
duction	O
by	O
bayesian	O
model	O
merg-	O
ing	O
.	O
in	O
nips-5	O
.	O
stoyanov	O
,	O
v.	O
,	O
a.	O
ropson	O
,	O
and	O
j.	O
eis-	O
ner	O
(	O
2011	O
)	O
.	O
empirical	B
risk	I
minimiza-	O
tion	O
of	O
graphical	B
model	I
parameters	O
given	O
approximate	B
inference	I
,	O
de-	O
coding	O
,	O
and	O
model	O
structure	O
.	O
in	O
ai/statistics	O
.	O
sudderth	O
,	O
e.	O
(	O
2006	O
)	O
.	O
graphical	O
models	O
for	O
visual	O
object	O
recognition	O
and	O
tracking	B
.	O
ph.d.	O
thesis	O
,	O
mit	O
.	O
sudderth	O
,	O
e.	O
and	O
w.	O
freeman	O
(	O
2008	O
,	O
march	O
)	O
.	O
signal	O
and	O
image	O
process-	O
ing	O
with	O
belief	B
propagation	I
.	O
ieee	O
signal	B
processing	I
magazine	O
.	O
sudderth	O
,	O
e.	O
,	O
a.	O
ihler	O
,	O
w.	O
freeman	O
,	O
and	O
a.	O
willsky	O
(	O
2003	O
)	O
.	O
nonparametric	O
belief	B
propagation	I
.	O
in	O
cvpr	O
.	O
sudderth	O
,	O
e.	O
,	O
a.	O
ihler	O
,	O
m.	O
isard	O
,	O
w.	O
freeman	O
,	O
and	O
a.	O
willsky	O
(	O
2010	O
)	O
.	O
nonparametric	O
belief	B
propagation	I
.	O
comm	O
.	O
of	O
the	O
acm	O
53	O
(	O
10	O
)	O
.	O
sudderth	O
,	O
e.	O
and	O
m.	O
jordan	O
(	O
2008	O
)	O
.	O
shared	B
segmentation	O
of	O
natural	O
scenes	O
using	O
dependent	O
pitman-	O
yor	O
processes	O
.	O
in	O
nips	O
.	O
sudderth	O
,	O
e.	O
,	O
m.	O
wainwright	O
,	O
and	O
a.	O
willsky	O
(	O
2008	O
)	O
.	O
loop	B
series	O
and	O
bethe	O
variational	O
bounds	O
for	O
attrac-	O
tive	O
graphical	O
models	O
.	O
in	O
nips	O
.	O
sun	O
,	O
l.	O
,	O
s.	O
ji	O
,	O
s.	O
yu	O
,	O
and	O
j.	O
ye	O
(	O
2009	O
)	O
.	O
on	O
the	O
equivalence	O
between	O
canonical	O
correlation	O
analysis	O
and	O
orthonor-	O
malized	O
partial	O
in	O
intl	O
.	O
joint	O
conf	O
.	O
on	O
ai	O
.	O
least	B
squares	I
.	O
sunehag	O
,	O
p.	O
,	O
j.	O
trumpf	O
,	O
s.	O
v.	O
n.	O
vish-	O
wanathan	O
,	O
and	O
n.	O
n.	O
schraudolph	O
(	O
2009	O
)	O
.	O
variable	O
metric	O
stochastic	B
approximation	I
theory	O
.	O
in	O
ai/statis-	O
tics	O
,	O
pp	O
.	O
560–566	O
.	O
sutton	O
,	O
c.	O
and	O
a.	O
mccallum	O
(	O
2007	O
)	O
.	O
improved	O
dynamic	O
schedules	O
for	O
belief	B
propagation	I
.	O
in	O
uai	O
.	O
sutton	O
,	O
r.	O
and	O
a.	O
barto	O
(	O
1998	O
)	O
.	O
rein-	O
forcment	O
learning	B
:	O
an	O
introduction	O
.	O
mit	O
press	O
.	O
swendsen	O
,	O
r.	O
and	O
j.-s.	O
wang	O
(	O
1987	O
)	O
.	O
nonuniversal	O
critical	O
dynamics	O
in	O
monte	O
carlo	O
simulations	O
.	O
physical	O
review	O
letters	O
58	O
,	O
86–88	O
.	O
swersky	O
,	O
k.	O
,	O
b.	O
chen	O
,	O
b.	O
marlin	O
,	O
and	O
n.	O
de	O
freitas	O
(	O
2010	O
)	O
.	O
a	O
tuto-	O
rial	O
on	O
stochastic	B
approximation	I
algorithms	O
for	O
training	O
restricted	O
boltzmann	O
machines	O
and	O
deep	B
be-	O
lief	O
nets	O
.	O
in	O
information	B
theory	I
and	O
applications	O
(	O
ita	O
)	O
workshop	O
.	O
szeliski	O
,	O
r.	O
(	O
2010	O
)	O
.	O
computer	O
vi-	O
sion	O
:	O
algorithms	O
and	O
applications	O
.	O
springer	O
.	O
szeliski	O
,	O
r.	O
,	O
r.	O
zabih	O
,	O
d.	O
scharstein	O
,	O
o.	O
veksler	O
,	O
v.	O
kolmogorov	O
,	O
a.	O
agar-	O
wala	O
,	O
m.	O
tappen	O
,	O
and	O
c.	O
rother	O
(	O
2008	O
)	O
.	O
a	O
comparative	O
study	O
of	O
energy	O
minimization	O
methods	O
for	O
markov	O
random	O
fields	O
with	O
smoothness-based	O
priors	O
.	O
ieee	O
trans	O
.	O
on	O
pattern	B
analysis	O
and	O
ma-	O
chine	O
intelligence	O
30	O
(	O
6	O
)	O
,	O
1068–1080	O
.	O
szepesvari	O
,	O
c.	O
(	O
2010	O
)	O
.	O
algorithms	O
for	O
reinforcement	B
learning	I
.	O
morgan	O
claypool	O
.	O
taleb	O
,	O
n.	O
(	O
2007	O
)	O
.	O
the	O
black	O
swan	O
:	O
the	O
impact	O
of	O
the	O
highly	O
improba-	O
ble	O
.	O
random	O
house	O
.	O
talhouk	O
,	O
a.	O
,	O
k.	O
murphy	O
,	O
and	O
a.	O
doucet	O
(	O
2011	O
)	O
.	O
efficient	O
bayesian	O
inference	B
for	O
multivariate	B
probit	I
models	O
with	O
sparse	B
inverse	O
correlation	O
matri-	O
ces	O
.	O
j.	O
comp	O
.	O
graph	B
.	O
statist..	O
tanner	O
,	O
m.	O
(	O
1996	O
)	O
.	O
tools	O
for	O
statistical	O
inference	O
.	O
springer	O
.	O
sun	O
,	O
j.	O
,	O
n.	O
zheng	O
,	O
and	O
h.	O
shum	O
(	O
2003	O
)	O
.	O
stereo	O
matching	O
using	O
be-	O
lief	O
propagation	O
.	O
ieee	O
trans	O
.	O
on	O
pat-	O
tern	O
analysis	O
and	O
machine	O
intelli-	O
gence	O
25	O
(	O
7	O
)	O
,	O
787–800	O
.	O
tanner	O
,	O
m.	O
and	O
w.	O
wong	O
(	O
1987	O
)	O
.	O
the	O
calculation	O
of	O
posterior	O
distribu-	O
tions	O
by	O
data	B
augmentation	I
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
82	O
(	O
398	O
)	O
,	O
528–	O
540.	O
bibliography	O
1041	O
tarlow	O
,	O
d.	O
,	O
i.	O
givoni	O
,	O
and	O
r.	O
zemel	O
(	O
2010	O
)	O
.	O
hop-map	O
:	O
efficient	O
message	O
passing	O
with	O
high	O
order	O
potentials	O
.	O
in	O
ai/statistics	O
.	O
taskar	O
,	O
b.	O
,	O
c.	O
guestrin	O
,	O
and	O
d.	O
koller	O
(	O
2003	O
)	O
.	O
max-margin	O
markov	O
net-	O
works	O
.	O
in	O
nips	O
.	O
taskar	O
,	O
b.	O
,	O
d.	O
klein	O
,	O
m.	O
collins	O
,	O
d.	O
koller	O
,	O
and	O
c.	O
manning	O
(	O
2004	O
)	O
.	O
max-margin	O
parsing	O
.	O
in	O
proc	O
.	O
em-	O
pirical	O
methods	O
in	O
natural	O
lan-	O
guage	O
processing	O
.	O
teh	O
,	O
y.	O
w.	O
(	O
2006	O
)	O
.	O
a	O
hierarchical	O
bayesian	O
language	B
model	I
based	O
on	O
pitman-yor	O
processes	O
.	O
in	O
proc	O
.	O
of	O
the	O
assoc	O
.	O
for	O
computational	O
lin-	O
guistics	O
,	O
pp	O
.	O
985=992	O
.	O
teh	O
,	O
y.-w.	O
,	O
m.	O
jordan	O
,	O
m.	O
beal	O
,	O
and	O
d.	O
blei	O
(	O
2006	O
)	O
.	O
hierarchical	O
dirich-	O
let	O
processes	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
as-	O
soc	O
.	O
101	O
(	O
476	O
)	O
,	O
1566–1581	O
.	O
tenenbaum	O
,	O
j	O
.	O
(	O
1999	O
)	O
.	O
framework	O
for	O
ph.d.	O
thesis	O
,	O
mit	O
.	O
a	O
bayesian	O
learning	B
.	O
concept	B
tenenbaum	O
,	O
j.	O
b.	O
and	O
f.	O
xu	O
(	O
2000	O
)	O
.	O
word	O
learning	B
as	O
bayesian	O
infer-	O
ence	O
.	O
in	O
proc	O
.	O
22nd	O
annual	O
conf.of	O
the	O
cognitive	O
science	O
society	O
.	O
theocharous	O
,	O
g.	O
,	O
k.	O
murphy	O
,	O
and	O
l.	O
kaelbling	O
(	O
2004	O
)	O
.	O
representing	O
hierarchical	O
pomdps	O
as	O
dbns	O
for	O
multi-scale	O
robot	O
localization	O
.	O
in	O
ieee	O
intl	O
.	O
conf	O
.	O
on	O
robotics	O
and	O
au-	O
tomation	O
.	O
thiesson	O
,	O
b.	O
,	O
c.	O
meek	O
,	O
d.	O
chickering	O
,	O
and	O
d.	O
heckerman	O
(	O
1998	O
)	O
.	O
learning	B
mixtures	O
of	O
dag	O
models	O
.	O
in	O
uai	O
.	O
thomas	O
,	O
a.	O
and	O
p.	O
green	O
(	O
2009	O
)	O
.	O
enumerating	O
the	O
decomposable	B
neighbours	O
of	O
a	O
decomposable	B
graph	O
under	O
a	O
simple	O
perturbation	O
scheme	O
.	O
comp	O
.	O
statistics	O
and	O
data	O
analysis	O
53	O
,	O
1232–1238	O
.	O
thrun	O
,	O
s.	O
,	O
w.	O
burgard	O
,	O
and	O
d.	O
fox	O
(	O
2006	O
)	O
.	O
probabilistic	O
robotics	O
.	O
mit	O
press	O
.	O
thrun	O
,	O
s.	O
,	O
m.	O
montemerlo	O
,	O
d.	O
koller	O
,	O
b.	O
wegbreit	O
,	O
j.	O
nieto	O
,	O
and	O
e.	O
nebot	O
(	O
2004	O
)	O
.	O
fastslam	O
:	O
an	O
efficient	O
so-	O
lution	O
to	O
the	O
simultaneous	O
local-	O
ization	O
and	O
mapping	O
problem	O
with	O
unknown	B
data	O
association	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
2004.	O
thrun	O
,	O
s.	O
and	O
l.	O
pratt	O
(	O
eds	O
.	O
)	O
learning	B
to	I
learn	I
.	O
kluwer	O
.	O
(	O
1997	O
)	O
.	O
tibshirani	O
,	O
r.	O
(	O
1996	O
)	O
.	O
regression	B
shrinkage	O
and	O
selection	O
via	O
the	O
lasso	B
.	O
j.	O
royal	O
.	O
statist	O
.	O
soc	O
b	O
58	O
(	O
1	O
)	O
,	O
267–288	O
.	O
tibshirani	O
,	O
r.	O
,	O
g.	O
walther	O
,	O
and	O
t.	O
hastie	O
(	O
2001	O
)	O
.	O
estimating	O
the	O
number	O
of	O
clusters	B
in	O
a	O
dataset	O
via	O
the	O
gap	B
statistic	I
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
32	O
(	O
2	O
)	O
,	O
411–423	O
.	O
tieleman	O
,	O
t.	O
(	O
2008	O
)	O
.	O
training	O
re-	O
stricted	O
boltzmann	O
machines	O
us-	O
ing	O
approximations	O
to	O
the	O
likeli-	O
hood	O
gradient	O
.	O
in	O
proceedings	O
of	O
the	O
25th	O
international	O
conference	O
on	O
machine	B
learning	I
,	O
pp	O
.	O
1064–1071	O
.	O
acm	O
new	O
york	O
,	O
ny	O
,	O
usa	O
.	O
ting	O
,	O
j.	O
,	O
a.	O
d	O
’	O
souza	O
,	O
s.	O
vijayakumar	O
,	O
and	O
s.	O
schaal	O
efficient	O
learning	O
and	O
feature	B
selection	I
in	O
high-dimensional	O
regression	B
.	O
neu-	O
ral	O
computation	O
22	O
(	O
4	O
)	O
,	O
831–886	O
.	O
(	O
2010	O
)	O
.	O
tipping	O
,	O
m.	O
(	O
1998	O
)	O
.	O
probabilistic	O
visual-	O
ization	O
of	O
high-dimensional	O
binary	O
data	O
.	O
in	O
nips	O
.	O
tipping	O
,	O
m.	O
(	O
2001	O
)	O
.	O
sparse	B
bayesian	O
learning	B
and	O
the	O
relevance	B
vector	I
machine	I
.	O
j.	O
of	O
machine	B
learning	I
research	O
1	O
,	O
211–244	O
.	O
tipping	O
,	O
m.	O
and	O
c.	O
bishop	O
(	O
1999	O
)	O
.	O
probabilistic	O
principal	O
component	O
analysis	O
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
21	O
(	O
3	O
)	O
,	O
611–622	O
.	O
tipping	O
,	O
m.	O
and	O
a.	O
faul	O
(	O
2003	O
)	O
.	O
fast	O
likelihood	O
maximisation	O
marginal	O
for	O
sparse	B
bayesian	O
models	O
.	O
in	O
ai/s-	O
tats	O
.	O
tishby	O
,	O
n.	O
,	O
f.	O
pereira	O
,	O
and	O
w.	O
biale	O
the	O
information	B
bottle-	O
(	O
1999	O
)	O
.	O
neck	O
method	O
.	O
in	O
the	O
37th	O
an-	O
nual	O
allerton	O
conf	O
.	O
on	O
communica-	O
tion	O
,	O
control	O
,	O
and	O
computing	O
,	O
pp	O
.	O
368â	O
˘a	O
¸s377	O
.	O
tomas	O
,	O
m.	O
,	O
d.	O
anoop	O
,	O
k.	O
stefan	O
,	O
b.	O
lukas	O
,	O
and	O
c.	O
jan	O
(	O
2011	O
)	O
.	O
empir-	O
ical	O
evaluation	O
and	O
combination	O
of	O
advanced	O
language	B
modeling	I
tech-	O
niques	O
.	O
in	O
proc	O
.	O
12th	O
annual	O
conf	O
.	O
of	O
the	O
intl	O
.	O
speech	O
communication	O
association	O
(	O
interspeech	O
)	O
.	O
torralba	O
,	O
a.	O
,	O
r.	O
fergus	O
,	O
and	O
y.	O
weiss	O
(	O
2008	O
)	O
.	O
small	O
codes	O
and	O
large	O
im-	O
age	O
databases	O
for	O
recognition	O
.	O
in	O
cvpr	O
.	O
train	O
,	O
k.	O
(	O
2009	O
)	O
.	O
discrete	O
choice	O
meth-	O
cambridge	O
ods	O
with	O
simulation	O
.	O
university	O
press	O
.	O
second	O
edition	O
.	O
tseng	O
,	O
p.	O
(	O
2008	O
)	O
.	O
on	O
accelerated	O
proxi-	O
mal	O
gradient	O
methods	O
for	O
convex-	O
concave	B
optimization	O
.	O
technical	O
report	O
,	O
u.	O
washington	O
.	O
tsochantaridis	O
,	O
i.	O
,	O
t.	O
joachims	O
,	O
t.	O
hof-	O
mann	O
,	O
and	O
y.	O
altun	O
(	O
2005	O
,	O
septem-	O
ber	O
)	O
.	O
large	O
margin	O
methods	O
for	O
structured	O
and	O
interdependent	O
output	O
variables	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
6	O
,	O
1453–1484	O
.	O
tu	O
,	O
z.	O
and	O
s.	O
zhu	O
(	O
2002	O
)	O
.	O
image	O
seg-	O
mentation	O
by	O
data-driven	O
markov	O
chain	O
monte	O
carlo	O
.	O
ieee	O
trans	O
.	O
on	O
pattern	B
analysis	O
and	O
machine	O
intel-	O
ligence	O
24	O
(	O
5	O
)	O
,	O
657–673	O
.	O
turian	O
,	O
j.	O
,	O
l.	O
ratinov	O
,	O
and	O
y.	O
ben-	O
gio	O
(	O
2010	O
)	O
.	O
word	O
representations	O
:	O
a	O
simple	O
and	O
general	O
method	O
for	O
semi-supervised	B
learning	I
.	O
in	O
proc	O
.	O
acl	O
.	O
turlach	O
,	O
b.	O
,	O
w.	O
venables	O
,	O
and	O
s.	O
wright	O
(	O
2005	O
)	O
.	O
simultaneous	O
variable	O
se-	O
lection	O
.	O
technometrics	O
47	O
(	O
3	O
)	O
,	O
349–	O
363.	O
turner	O
,	O
r.	O
,	O
p.	O
berkes	O
,	O
m.	O
sahani	O
,	O
and	O
d.	O
mackay	O
(	O
2008	O
)	O
.	O
counterexam-	O
ples	O
to	O
variational	B
free	I
energy	I
com-	O
pactness	O
folk	O
theorems	O
.	O
technical	O
report	O
,	O
u.	O
cambridge	O
.	O
ueda	O
,	O
n.	O
and	O
r.	O
nakano	O
(	O
1998	O
)	O
.	O
deter-	O
ministic	O
annealing	B
em	O
algorithm	O
.	O
neural	B
networks	I
11	O
,	O
271–282	O
.	O
usunier	O
,	O
n.	O
,	O
d.	O
buffoni	O
,	O
and	O
p.	O
galli-	O
nari	O
(	O
2009	O
)	O
.	O
ranking	B
with	O
ordered	O
weighted	O
pairwise	O
classiﬁcation	O
.	O
vaithyanathan	O
,	O
s.	O
and	O
b.	O
dom	O
(	O
1999	O
)	O
.	O
model	B
selection	I
in	O
unsupervised	B
learning	I
with	O
applications	O
to	O
doc-	O
ument	O
clustering	B
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
van	O
der	O
merwe	O
,	O
r.	O
,	O
a.	O
doucet	O
,	O
n.	O
de	O
freitas	O
,	O
and	O
e.	O
wan	O
(	O
2000	O
)	O
.	O
the	O
unscented	B
particle	I
ﬁlter	I
.	O
in	O
nips-13	O
.	O
van	O
dyk	O
,	O
d.	O
and	O
x.-l.	O
meng	O
(	O
2001	O
)	O
.	O
the	O
art	O
of	O
data	B
augmentation	I
.	O
j.	O
computational	O
and	O
graphical	O
statistics	O
10	O
(	O
1	O
)	O
,	O
1–50	O
.	O
vandenberghe	O
,	O
l.	O
(	O
2006	O
)	O
.	O
applied	O
nu-	O
merical	O
computing	O
:	O
lecture	O
notes	O
.	O
vandenberghe	O
,	O
l.	O
(	O
2011	O
)	O
.	O
ee236c	O
-	O
op-	O
timization	O
methods	O
for	O
large-scale	O
systems	O
.	O
vanhatalo	O
,	O
j	O
.	O
(	O
2010	O
)	O
.	O
speeding	O
up	O
the	O
inference	B
in	O
gaussian	O
process	O
mod-	O
els	O
.	O
ph.d.	O
thesis	O
,	O
helsinki	O
univ	O
.	O
technology	O
.	O
1042	O
bibliography	O
vanhatalo	O
,	O
j.	O
,	O
v.	O
pietilãd	O
’	O
inen	O
,	O
and	O
a.	O
vehtari	O
(	O
2010	O
)	O
.	O
approximate	O
in-	O
ference	O
for	O
disease	B
mapping	I
with	O
sparse	B
gaussian	O
processes	O
.	O
statis-	O
tics	O
in	O
medicine	O
29	O
(	O
15	O
)	O
,	O
1580–1607	O
.	O
vapnik	O
,	O
v.	O
(	O
1998	O
)	O
.	O
statistical	B
learning	I
theory	I
.	O
wiley	O
.	O
vapnik	O
,	O
v.	O
,	O
s.	O
golowich	O
,	O
and	O
a.	O
smola	O
(	O
1997	O
)	O
.	O
support	O
vector	O
method	O
for	O
function	B
approximation	I
,	O
regression	B
estimation	O
,	O
and	O
signal	B
processing	I
.	O
in	O
nips	O
.	O
varian	O
,	O
h.	O
(	O
2011	O
)	O
.	O
structural	B
time	I
series	I
in	O
r	O
:	O
a	O
tutorial	O
.	O
technical	O
report	O
,	O
google	O
.	O
verma	O
,	O
t.	O
and	O
j.	O
pearl	O
(	O
1990	O
)	O
.	O
equiva-	O
lence	O
and	O
synthesis	O
of	O
causal	O
mod-	O
els	O
.	O
in	O
uai	O
.	O
viinikanoja	O
,	O
j.	O
,	O
a.	O
klami	O
,	O
and	O
s.	O
kaski	O
(	O
2010	O
)	O
.	O
variational	O
bayesian	O
mixture	O
of	O
robust	O
cca	O
models	O
.	O
in	O
proc	O
.	O
eu-	O
ropean	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
vincent	O
,	O
p.	O
(	O
2011	O
)	O
.	O
a	O
connection	O
be-	O
tween	O
score	B
matching	I
and	O
denois-	O
ing	O
autoencoders	O
.	O
neural	O
compu-	O
tation	O
23	O
(	O
7	O
)	O
,	O
1661–1674	O
.	O
vincent	O
,	O
p.	O
,	O
h.	O
larochelle	O
,	O
i.	O
la-	O
joie	O
,	O
y.	O
bengio	O
,	O
and	O
p.-a	O
.	O
manzagol	O
(	O
2010	O
)	O
.	O
stacked	O
denoising	O
autoen-	O
coders	O
:	O
learning	B
useful	O
represen-	O
tations	O
in	O
a	O
deep	B
network	O
with	O
a	O
local	O
denoising	O
criterion	O
.	O
j.	O
of	O
ma-	O
chine	O
learning	B
research	O
11	O
,	O
3371–	O
3408.	O
vinh	O
,	O
n.	O
,	O
j.	O
epps	O
,	O
and	O
j.	O
bailey	O
(	O
2009	O
)	O
.	O
information	B
theoretic	O
measures	O
for	O
is	O
a	O
cor-	O
clusterings	O
comparison	O
:	O
rection	O
for	O
chance	O
necessary	O
?	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
vinyals	O
,	O
m.	O
,	O
j.	O
cerquides	O
,	O
j.	O
rodriguez-	O
aguilar	O
,	O
and	O
a.	O
farinelli	O
(	O
2010	O
)	O
.	O
worst-case	O
bounds	O
on	O
the	O
quality	O
of	O
max-product	B
ﬁxed-points	O
.	O
in	O
nips	O
.	O
viola	O
,	O
p.	O
and	O
m.	O
jones	O
(	O
2001	O
)	O
.	O
rapid	O
object	B
detection	I
using	O
a	O
boosted	O
cascade	B
of	O
simple	O
classiﬁers	O
.	O
in	O
cvpr	O
.	O
virtanen	O
,	O
s.	O
(	O
2010	O
)	O
.	O
bayesian	O
expo-	O
nential	O
family	B
projections	O
.	O
master	B
’	O
s	O
thesis	O
,	O
aalto	O
university	O
.	O
vishwanathan	O
,	O
s.	O
v.	O
n.	O
and	O
a.	O
smola	O
(	O
2003	O
)	O
.	O
fast	O
kernels	O
for	O
string	O
and	O
tree	B
matching	O
.	O
in	O
nips	O
.	O
viterbi	O
,	O
a	O
.	O
(	O
1967	O
)	O
.	O
error	O
bounds	O
for	O
convolutional	O
codes	O
and	O
an	O
asymptotically	O
optimum	O
decoding	B
algorithm	O
.	O
ieee	O
trans	O
.	O
on	O
informa-	O
tion	O
theory	O
13	O
(	O
2	O
)	O
,	O
260â	O
˘a	O
¸s269	O
.	O
von	O
luxburg	O
,	O
u	O
.	O
(	O
2007	O
)	O
.	O
a	O
tutorial	O
on	O
statistics	O
and	O
spectral	B
clustering	I
.	O
computing	O
17	O
(	O
4	O
)	O
,	O
395–416	O
.	O
wagenmakers	O
,	O
e.-j.	O
,	O
r.	O
wetzels	O
,	O
d.	O
borsboom	O
,	O
and	O
h.	O
van	O
der	O
maas	O
(	O
2011	O
)	O
.	O
why	O
psychologists	O
must	O
change	O
the	O
way	O
they	O
ana-	O
lyze	O
their	O
data	O
:	O
the	O
case	O
of	O
psi	O
.	O
journal	O
of	O
personality	O
and	O
social	O
psychology	O
.	O
wagner	O
,	O
d.	O
and	O
f.	O
wagner	O
(	O
1993	O
)	O
.	O
be-	O
tween	O
min	O
cut	O
and	O
graph	B
bisec-	O
tion	O
.	O
in	O
proc	O
.	O
18th	O
intl	O
.	O
symp	O
.	O
on	O
math	O
.	O
found	O
.	O
of	O
comp	O
.	O
sci.	O
,	O
pp	O
.	O
744–	O
750.	O
wainwright	O
,	O
m.	O
,	O
t.	O
jaakkola	O
,	O
and	O
a.	O
willsky	O
(	O
2001	O
)	O
.	O
tree-based	O
repa-	O
rameterization	O
for	O
approximate	O
es-	O
timation	O
on	O
loopy	O
graphs	O
.	O
in	O
nips-	O
14.	O
jaakkola	O
,	O
and	O
wainwright	O
,	O
m.	O
,	O
t.	O
a.	O
willsky	O
(	O
2005	O
)	O
.	O
a	O
new	O
class	O
of	O
upper	O
bounds	O
on	O
the	O
log	O
parti-	O
tion	O
function	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
the-	O
ory	O
51	O
(	O
7	O
)	O
,	O
2313–2335	O
.	O
wainwright	O
,	O
m.	O
,	O
p.	O
ravikumar	O
,	O
and	O
inferring	O
graph-	O
j.	O
lafferty	O
(	O
2006	O
)	O
.	O
ical	O
model	O
structure	O
using	O
(	O
cid:7	O
)	O
−	O
1-	O
regularized	O
pseudo-likelihood	O
.	O
in	O
nips	O
.	O
wainwright	O
,	O
m.	O
j.	O
,	O
t.	O
s.	O
jaakkola	O
,	O
and	O
a.	O
s.	O
willsky	O
(	O
2003	O
)	O
.	O
tree-based	O
reparameterization	O
framework	O
for	O
analysis	O
of	O
sum-product	B
and	O
re-	O
lated	O
algorithms	O
.	O
ieee	O
trans	O
.	O
on	O
in-	O
formation	O
theory	O
49	O
(	O
5	O
)	O
,	O
1120–1146	O
.	O
wainwright	O
,	O
m.	O
j.	O
and	O
m.	O
jordan	O
(	O
2008a	O
)	O
.	O
graphical	O
models	O
,	O
expo-	O
nential	O
families	O
,	O
and	O
variational	O
in-	O
ference	O
.	O
foundations	O
and	O
trends	O
in	O
machine	B
learning	I
1–2	O
,	O
1–305	O
.	O
i.	O
wainwright	O
,	O
m.	O
j.	O
and	O
m.	O
jordan	O
(	O
2008b	O
)	O
.	O
graphical	O
models	O
,	O
expo-	O
nential	O
families	O
,	O
and	O
variational	O
in-	O
ference	O
.	O
foundations	O
and	O
trends	O
in	O
machine	B
learning	I
1–2	O
,	O
1–305	O
.	O
i.	O
wallach	O
,	O
h.	O
,	O
i.	O
murray	O
,	O
r.	O
salakhutdi-	O
nov	O
,	O
and	O
d.	O
mimno	O
(	O
2009	O
)	O
.	O
evalua-	O
tion	O
methods	O
for	O
topic	B
models	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
wan	O
,	O
e.	O
a.	O
and	O
r.	O
v.	O
der	O
merwe	O
(	O
2001	O
)	O
.	O
the	O
unscented	O
kalman	O
filter	O
.	O
in	O
s.	O
haykin	O
(	O
ed	O
.	O
)	O
,	O
kalman	O
filtering	O
and	O
neural	B
networks	I
.	O
wiley	O
.	O
wand	O
,	O
m.	O
(	O
2009	O
)	O
.	O
semiparametric	O
regression	B
and	O
graphical	O
models	O
.	O
aust	O
.	O
n.	O
z.	O
j.	O
stat	O
.	O
51	O
(	O
1	O
)	O
,	O
9–41	O
.	O
wand	O
,	O
m.	O
p.	O
,	O
j.	O
t.	O
ormerod	O
,	O
s.	O
a.	O
padoan	O
,	O
and	O
r.	O
fruhrwirth	O
(	O
2011	O
)	O
.	O
mean	B
field	O
variational	O
bayes	O
for	O
elaborate	O
distributions	O
.	O
bayesian	O
analysis	O
6	O
(	O
4	O
)	O
,	O
847	O
–	O
900.	O
wang	O
,	O
c.	O
(	O
2007	O
)	O
.	O
variational	O
bayesian	O
approach	O
to	O
canonical	O
correlation	O
analysis	O
.	O
ieee	O
trans	O
.	O
on	O
neural	O
net-	O
works	O
18	O
(	O
3	O
)	O
,	O
905–910	O
.	O
wasserman	O
,	O
l.	O
(	O
2004	O
)	O
.	O
all	O
of	O
statistics	O
.	O
a	O
concise	O
course	O
in	O
statistical	O
infer-	O
ence	O
.	O
springer	O
.	O
wei	O
,	O
g.	O
and	O
m.	O
tanner	O
(	O
1990	O
)	O
.	O
a	O
monte	O
carlo	O
implementation	O
of	O
the	O
em	O
al-	O
gorithm	O
and	O
the	O
poor	O
man	O
’	O
s	O
data	B
augmentation	I
algorithms	O
.	O
j.	O
of	O
the	O
am	O
.	O
stat	O
.	O
assoc	O
.	O
85	O
(	O
411	O
)	O
,	O
699–704	O
.	O
weinberger	O
,	O
k.	O
,	O
a.	O
dasgupta	O
,	O
j.	O
atten-	O
berg	O
,	O
j.	O
langford	O
,	O
and	O
a.	O
smola	O
(	O
2009	O
)	O
.	O
feature	O
hashing	O
for	O
large	O
scale	O
multitask	O
learning	B
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
weiss	O
,	O
d.	O
,	O
b.	O
sapp	O
,	O
and	O
b.	O
taskar	O
(	O
2010	O
)	O
.	O
sidestepping	O
intractable	O
in-	O
ference	O
with	O
structured	O
ensemble	O
cascades	O
.	O
in	O
nips	O
.	O
weiss	O
,	O
y	O
.	O
(	O
2000	O
)	O
.	O
correctness	O
of	O
local	O
probability	O
propagation	O
in	O
graph-	O
ical	O
models	O
with	O
loops	O
.	O
neural	O
computation	O
12	O
,	O
1–41	O
.	O
weiss	O
,	O
y	O
.	O
(	O
2001	O
)	O
.	O
comparing	O
the	O
mean	B
ﬁeld	I
method	O
and	O
belief	O
propaga-	O
tion	O
for	O
approximate	B
inference	I
in	O
mrfs	O
.	O
in	O
saad	O
and	O
opper	O
(	O
eds	O
.	O
)	O
,	O
advanced	O
mean	B
field	O
methods	O
.	O
mit	O
press	O
.	O
weiss	O
,	O
y.	O
and	O
w.	O
t.	O
freeman	O
(	O
1999	O
)	O
.	O
correctness	O
of	O
belief	B
propagation	I
in	O
gaussian	O
graphical	O
models	O
of	O
ar-	O
bitrary	O
topology	O
.	O
in	O
nips-12	O
.	O
weiss	O
,	O
y.	O
and	O
w.	O
t.	O
freeman	O
(	O
2001a	O
)	O
.	O
correctness	O
of	O
belief	B
propagation	I
in	O
gaussian	O
graphical	O
models	O
of	O
ar-	O
bitrary	O
topology	O
.	O
neural	O
computa-	O
tion	O
13	O
(	O
10	O
)	O
,	O
2173–2200	O
.	O
weiss	O
,	O
y.	O
and	O
w.	O
t.	O
freeman	O
(	O
2001b	O
)	O
.	O
on	O
the	O
optimality	O
of	O
solutions	O
of	O
the	O
max-product	O
belief	O
propaga-	O
tion	O
algorithm	O
in	O
arbitrary	O
graphs	O
.	O
ieee	O
trans	O
.	O
information	B
theory	I
,	O
special	O
issue	O
on	O
codes	O
on	O
graphs	O
and	O
iterative	O
algorithms	O
47	O
(	O
2	O
)	O
,	O
723–	O
735.	O
weiss	O
,	O
y.	O
,	O
a.	O
torralba	O
,	O
and	O
r.	O
fergus	O
(	O
2008	O
)	O
.	O
spectral	B
hashing	O
.	O
in	O
nips	O
.	O
bibliography	O
1043	O
welling	O
,	O
m.	O
,	O
c.	O
chemudugunta	O
,	O
and	O
n.	O
sutter	O
(	O
2008	O
)	O
.	O
deterministic	O
la-	O
tent	O
variable	O
models	O
and	O
their	O
pit-	O
falls	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
data	O
mining	O
.	O
welling	O
,	O
m.	O
,	O
t.	O
minka	O
,	O
and	O
y.	O
w.	O
teh	O
(	O
2005	O
)	O
.	O
structured	O
region	O
graphs	O
:	O
morphing	O
ep	O
into	O
gbp	O
.	O
in	O
uai	O
.	O
welling	O
,	O
m.	O
,	O
m.	O
rosen-zvi	O
,	O
and	O
g.	O
hin-	O
ton	O
(	O
2004	O
)	O
.	O
exponential	B
family	I
har-	O
moniums	O
with	O
an	O
application	O
to	O
information	B
retrieval	I
.	O
in	O
nips-14	O
.	O
welling	O
,	O
m.	O
and	O
c.	O
sutton	O
(	O
2005	O
)	O
.	O
learning	B
in	O
markov	O
random	O
ﬁelds	O
with	O
contrastive	O
free	O
energies	O
.	O
in	O
tenth	O
international	O
workshop	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
(	O
aistats	O
)	O
.	O
welling	O
,	O
m.	O
and	O
y.-w.	O
teh	O
(	O
2001	O
)	O
.	O
belief	O
optimization	O
for	O
binary	O
networks	O
:	O
a	O
stable	B
alternative	O
to	O
loopy	B
belief	I
propagation	I
.	O
in	O
uai	O
.	O
werbos	O
,	O
p.	O
(	O
1974	O
)	O
.	O
beyond	O
regression	B
:	O
new	O
tools	O
for	O
prediction	O
and	O
analy-	O
sis	O
in	O
the	O
behavioral	O
sciences	O
.	O
ph.d.	O
thesis	O
,	O
harvard	O
.	O
west	O
,	O
m.	O
(	O
1987	O
)	O
.	O
of	O
tures	O
biometrika	O
74	O
,	O
646–648	O
.	O
normal	B
on	O
scale	O
mix-	O
distributions	O
.	O
west	O
,	O
m.	O
(	O
2003	O
)	O
.	O
bayesian	O
factor	B
re-	O
gression	O
models	O
in	O
the	O
``	O
large	O
p	O
,	O
small	O
n	O
''	O
paradigm	O
.	O
bayesian	O
statis-	O
tics	O
7	O
.	O
west	O
,	O
m.	O
and	O
j.	O
harrison	O
(	O
1997	O
)	O
.	O
bayesian	O
forecasting	O
and	O
dynamic	O
models	O
.	O
springer	O
.	O
weston	O
,	O
j.	O
,	O
s.	O
bengio	O
,	O
and	O
n.	O
usunier	O
(	O
2010	O
)	O
.	O
large	O
scale	O
image	O
annota-	O
tion	O
:	O
learning	B
to	I
rank	I
with	O
joint	O
word-image	O
embeddings	O
.	O
in	O
proc	O
.	O
european	O
conf	O
.	O
on	O
machine	O
learn-	O
ing	O
.	O
weston	O
,	O
j.	O
,	O
f.	O
ratle	O
,	O
and	O
r.	O
collobert	O
(	O
2008	O
)	O
.	O
deep	B
learning	I
via	O
semi-	O
supervised	O
embedding	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
weston	O
,	O
j.	O
and	O
c.	O
watkins	O
(	O
1999	O
)	O
.	O
support	O
vector	O
ma-	O
multi-lcass	O
chines	O
.	O
in	O
esann	O
.	O
wiering	O
,	O
m.	O
and	O
m.	O
van	O
otterlo	O
(	O
eds	O
.	O
)	O
(	O
2012	O
)	O
.	O
reinforcement	O
learn-	O
ing	O
:	O
state-of-the-art	O
.	O
springer	O
.	O
wilkinson	O
,	O
d.	O
and	O
s.	O
yeung	O
(	O
2002	O
)	O
.	O
conditional	O
simulation	O
from	O
highly	O
structured	O
gaussian	O
systems	O
with	O
application	O
to	O
blocking-mcmc	O
for	O
the	O
bayesian	O
analysis	O
of	O
very	O
large	O
linear	O
models	O
.	O
statistics	O
and	O
com-	O
puting	O
12	O
,	O
287–300	O
.	O
williams	O
,	O
c.	O
(	O
1998	O
)	O
.	O
computation	O
with	O
inﬁnite	O
networks	O
.	O
neural	O
computa-	O
tion	O
10	O
(	O
5	O
)	O
,	O
1203–1216	O
.	O
williams	O
,	O
c.	O
(	O
2000	O
)	O
.	O
a	O
mcmc	O
ap-	O
proach	O
to	O
hierarchical	O
mixture	O
modelling	O
.	O
in	O
s.	O
a.	O
solla	O
,	O
t.	O
k.	O
leen	O
,	O
and	O
k.-r.	O
müller	O
(	O
eds	O
.	O
)	O
,	O
nips	O
.	O
mit	O
press	O
.	O
williams	O
,	O
c.	O
(	O
2002	O
)	O
.	O
on	O
a	O
connec-	O
tion	O
between	O
kernel	B
pca	O
and	O
met-	O
ric	O
multidimensional	B
scaling	I
.	O
ma-	O
chine	O
learning	B
j	O
.	O
46	O
(	O
1	O
)	O
.	O
williams	O
,	O
o.	O
and	O
a.	O
fitzgibbon	O
(	O
2006	O
)	O
.	O
gaussian	O
process	O
implicit	O
surfaces	O
.	O
in	O
gaussian	O
processes	O
in	O
practice	O
.	O
williamson	O
,	O
s.	O
and	O
z.	O
ghahramani	O
(	O
2008	O
)	O
.	O
probabilistic	O
models	O
for	O
data	O
combination	O
in	O
recommender	O
systems	O
.	O
in	O
nips	O
workshop	O
on	O
learning	B
from	O
multiple	O
sources	O
.	O
winn	O
,	O
j.	O
and	O
c.	O
bishop	O
(	O
2005	O
)	O
.	O
varia-	O
tional	O
message	B
passing	I
.	O
j.	O
of	O
ma-	O
chine	O
learning	B
research	O
6	O
,	O
661–	O
694.	O
wipf	O
,	O
d.	O
and	O
s.	O
nagarajan	O
(	O
2007	O
)	O
.	O
a	O
new	O
view	O
of	O
automatic	B
relevancy	I
determination	I
.	O
in	O
nips	O
.	O
wipf	O
,	O
d.	O
and	O
s.	O
nagarajan	O
(	O
2010	O
,	O
april	O
)	O
.	O
iterative	O
reweighted	O
(	O
cid:7	O
)	O
−1	O
and	O
(	O
cid:7	O
)	O
−2	O
methods	O
for	O
finding	O
sparse	B
solu-	O
tions	O
.	O
j.	O
of	O
selected	O
topics	O
in	O
signal	B
processing	I
(	O
special	O
issue	O
on	O
com-	O
pressive	O
sensing	O
)	O
4	O
(	O
2	O
)	O
.	O
wipf	O
,	O
d.	O
,	O
b.	O
rao	O
,	O
and	O
s.	O
nagarajan	O
latent	O
variable	O
bayesian	O
(	O
2010	O
)	O
.	O
models	O
for	O
promoting	O
sparsity	B
.	O
ieee	O
transactions	O
on	O
information	B
theory	I
.	O
witten	O
,	O
d.	O
,	O
r.	O
tibshirani	O
,	O
and	O
t.	O
hastie	O
(	O
2009	O
)	O
.	O
a	O
penalized	O
matrix	O
de-	O
composition	O
,	O
with	O
applications	O
to	O
sparse	B
principal	O
components	O
and	O
canonical	O
correlation	O
analysis	O
.	O
bio-	O
statistics	O
10	O
(	O
3	O
)	O
,	O
515–534	O
.	O
wolpert	O
,	O
d.	O
(	O
1992	O
)	O
.	O
stacked	O
generaliza-	O
tion	O
.	O
neural	B
networks	I
5	O
(	O
2	O
)	O
,	O
241–259	O
.	O
wolpert	O
,	O
d.	O
(	O
1996	O
)	O
.	O
the	O
lack	O
of	O
a	O
priori	O
distinctions	O
between	O
learning	B
algo-	O
rithms	O
.	O
neural	O
computation	O
8	O
(	O
7	O
)	O
,	O
1341–1390	O
.	O
wong	O
,	O
f.	O
,	O
c.	O
carter	O
,	O
and	O
r.	O
kohn	O
efficient	O
estimation	O
of	O
models	O
.	O
(	O
2003	O
)	O
.	O
covariance	B
biometrika	O
90	O
(	O
4	O
)	O
,	O
809–830	O
.	O
selection	O
wood	O
,	O
f.	O
,	O
c.	O
archambeau	O
,	O
j.	O
gasthaus	O
,	O
l.	O
james	O
,	O
and	O
y.	O
w.	O
teh	O
(	O
2009	O
)	O
.	O
a	O
stochastic	O
memoizer	O
for	O
sequence	O
data	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
wright	O
,	O
r.	O
s.	O
,	O
nowak	O
,	O
and	O
m.	O
figueiredo	O
(	O
2009	O
)	O
.	O
sparse	B
reconstruction	O
by	O
separable	O
ap-	O
proximation	O
.	O
ieee	O
trans	O
.	O
on	O
signal	B
processing	I
57	O
(	O
7	O
)	O
,	O
2479–2493	O
.	O
wu	O
,	O
t.	O
t.	O
and	O
k.	O
lange	O
(	O
2008	O
)	O
.	O
coordi-	O
nate	O
descent	O
algorithms	O
for	O
lasso	B
penalized	O
regression	B
.	O
ann	O
.	O
appl	O
.	O
stat	O
2	O
(	O
1	O
)	O
,	O
224–244	O
.	O
wu	O
,	O
y.	O
,	O
h.	O
tjelmeland	O
,	O
and	O
m.	O
west	O
(	O
2007	O
)	O
.	O
bayesian	O
cart	O
:	O
prior	O
struc-	O
ture	O
and	O
mcmc	O
computations	O
.	O
j.	O
of	O
computational	O
and	O
graphical	O
statistics	O
16	O
(	O
1	O
)	O
,	O
44–66	O
.	O
xu	O
,	O
f.	O
and	O
j.	O
tenenbaum	O
(	O
2007	O
)	O
.	O
word	O
learning	B
as	O
bayesian	O
inference	B
.	O
psy-	O
chological	O
review	O
114	O
(	O
2	O
)	O
.	O
xu	O
,	O
z.	O
,	O
v.	O
tresp	O
,	O
a.	O
rettinger	O
,	O
and	O
k.	O
kersting	O
(	O
2008	O
)	O
.	O
social	O
net-	O
work	O
mining	O
with	O
nonparametric	O
relational	O
models	O
.	O
in	O
acm	O
work-	O
shop	O
on	O
social	O
network	O
mining	O
and	O
analysis	O
(	O
sna-kdd	O
2008	O
)	O
.	O
xu	O
,	O
z.	O
,	O
v.	O
tresp	O
,	O
k.	O
yu	O
,	O
and	O
h.-p.	O
kriegel	O
(	O
2006	O
)	O
.	O
inﬁnite	O
hidden	O
rela-	O
tional	O
models	O
.	O
in	O
uai	O
.	O
xu	O
,	O
z.	O
,	O
v.	O
tresp	O
,	O
s.	O
yu	O
,	O
k.	O
yu	O
,	O
and	O
h.-p.	O
kriegel	O
(	O
2007	O
)	O
.	O
fast	O
inference	O
in	O
in-	O
ﬁnite	O
hidden	O
relational	O
models	O
.	O
in	O
workshop	O
on	O
mining	O
and	O
learning	B
with	O
graphs	O
.	O
xue	O
,	O
y.	O
,	O
x.	O
liao	O
,	O
l.	O
carin	O
,	O
and	O
b.	O
krish-	O
napuram	O
(	O
2007	O
)	O
.	O
multi-task	O
learn-	O
ing	O
for	O
classiﬁcation	B
with	O
dirichlet	O
process	O
priors	O
.	O
j.	O
of	O
machine	O
learn-	O
ing	O
research	O
8	O
,	O
2007.	O
yadollahpour	O
,	O
p.	O
,	O
d.	O
batra	O
,	O
and	O
g.	O
shakhnarovich	O
(	O
2011	O
)	O
.	O
diverse	O
m-	O
best	O
solutions	O
in	O
mrfs	O
.	O
in	O
nips	O
workshop	O
on	O
disrete	O
optimization	B
in	O
machine	B
learning	I
.	O
yan	O
,	O
d.	O
,	O
l.	O
huang	O
,	O
and	O
m.	O
i.	O
jordan	O
(	O
2009	O
)	O
.	O
fast	O
approximate	O
spectral	B
clustering	I
.	O
in	O
15th	O
acm	O
conf	O
.	O
on	O
knowledge	B
discovery	I
and	O
data	O
min-	O
ing	O
.	O
fast	O
feb	O
)	O
.	O
yang	O
,	O
a.	O
,	O
a.	O
ganesh	O
,	O
s.	O
sastry	O
,	O
and	O
y.	O
ma	O
(	O
2010	O
,	O
l1-	O
minimization	O
algorithms	O
and	O
an	O
application	O
in	O
robust	B
face	O
recog-	O
nition	O
:	O
a	O
review	O
.	O
technical	O
re-	O
port	O
ucb/eecs-2010-13	O
,	O
eecs	O
de-	O
partment	O
,	O
university	O
of	O
california	O
,	O
berkeley	O
.	O
1044	O
bibliography	O
yang	O
,	O
c.	O
,	O
r.	O
duraiswami	O
,	O
and	O
l.	O
david	O
(	O
2005	O
)	O
.	O
efficient	O
kernel	O
machines	O
using	O
the	O
improved	O
fast	O
gauss	O
transform	O
.	O
in	O
nips	O
.	O
yang	O
,	O
s.	O
,	O
b.	O
long	O
,	O
a.	O
smola	O
,	O
h.	O
zha	O
,	O
and	O
z.	O
zheng	O
(	O
2011	O
)	O
.	O
collaborative	O
competitive	O
ﬁltering	B
:	O
learning	B
rec-	O
ommender	O
using	O
context	O
of	O
user	O
choice	O
.	O
in	O
proc	O
.	O
annual	O
intl	O
.	O
acm	O
sigir	O
conference	O
.	O
yanover	O
,	O
c.	O
,	O
o.	O
schueler-furman	O
,	O
and	O
y.	O
weiss	O
(	O
2007	O
)	O
.	O
minimizing	O
and	O
learning	B
energy	O
functions	O
for	O
side-chain	O
prediction	O
.	O
in	O
recomb	O
.	O
yaun	O
,	O
g.-x.	O
,	O
k.-w.	O
chang	O
,	O
c.-j	O
.	O
hsieh	O
,	O
and	O
c.-j	O
.	O
lin	O
(	O
2010	O
)	O
.	O
a	O
comparison	O
of	O
optimization	B
methods	O
and	O
soft-	O
ware	O
for	O
large-scale	O
l1-regularized	O
linear	O
classiﬁcation	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
11	O
,	O
3183–3234	O
.	O
yedidia	O
,	O
j.	O
,	O
w.	O
t.	O
freeman	O
,	O
and	O
y.	O
weiss	O
(	O
2001	O
)	O
.	O
understanding	O
be-	O
lief	O
propagation	O
and	O
its	O
generaliza-	O
tions	O
.	O
in	O
intl	O
.	O
joint	O
conf	O
.	O
on	O
ai	O
.	O
yoshida	O
,	O
r.	O
and	O
m.	O
west	O
(	O
2010	O
)	O
.	O
bayesian	O
learning	B
in	O
sparse	B
graphi-	O
cal	O
factor	B
models	O
via	O
annealed	O
en-	O
tropy	O
.	O
j.	O
of	O
machine	B
learning	I
re-	O
search	O
11	O
,	O
1771–1798	O
.	O
younes	O
,	O
l.	O
(	O
1989	O
)	O
.	O
parameter	B
estima-	O
tion	O
for	O
imperfectly	O
observed	O
gibb-	O
sian	O
ﬁelds	O
.	O
probab	O
.	O
theory	O
and	O
re-	O
lated	O
fields	O
82	O
,	O
625–645	O
.	O
yu	O
,	O
c.	O
and	O
t.	O
joachims	O
(	O
2009	O
)	O
.	O
learn-	O
ing	O
structural	O
svms	O
with	O
latent	B
variables	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
yu	O
,	O
s.	O
,	O
k.	O
yu	O
,	O
v.	O
tresp	O
,	O
k.	O
h-p.	O
,	O
and	O
m.	O
wu	O
(	O
2006	O
)	O
.	O
supervised	O
proba-	O
bilistic	O
principal	B
component	I
anal-	O
ysis	O
.	O
in	O
proc	O
.	O
of	O
the	O
int	O
’	O
l	O
conf	O
.	O
on	O
knowledge	B
discovery	I
and	O
data	O
min-	O
ing	O
.	O
yu	O
,	O
s.-z	O
.	O
and	O
h.	O
kobayashi	O
(	O
2006	O
)	O
.	O
practical	O
implementation	O
of	O
an	O
ef-	O
ﬁcient	O
forward-backward	O
algorithm	O
for	O
an	O
explicit-duration	O
hidden	B
markov	O
model	O
.	O
ieee	O
trans	O
.	O
on	O
sig-	O
nal	O
processing	O
54	O
(	O
5	O
)	O
,	O
1947–	O
1951.	O
yuan	O
,	O
m.	O
and	O
y.	O
lin	O
(	O
2006	O
)	O
.	O
model	B
selection	I
and	O
estimation	O
in	O
re-	O
gression	O
with	O
grouped	O
variables	O
.	O
j.	O
royal	O
statistical	O
society	O
,	O
series	O
b	O
68	O
(	O
1	O
)	O
,	O
49–67	O
.	O
y.	O
lin	O
yuan	O
,	O
m.	O
and	O
(	O
2007	O
)	O
.	O
model	B
selection	I
and	O
estimation	O
in	O
the	O
gaussian	O
graphical	B
model	I
.	O
biometrika	O
94	O
(	O
1	O
)	O
,	O
19–35	O
.	O
yuille	O
,	O
a	O
.	O
(	O
2001	O
)	O
.	O
cccp	O
algorithms	O
to	O
minimze	O
the	O
bethe	O
and	O
kikuchi	O
free	O
energies	O
:	O
convergent	O
alterna-	O
tives	O
to	O
belief	B
propagation	I
.	O
neural	O
computation	O
14	O
,	O
1691–1722	O
.	O
yuille	O
,	O
a.	O
and	O
a.	O
rangarajan	O
(	O
2003	O
)	O
.	O
concave-convex	B
procedure	I
.	O
the	O
neural	O
computation	O
15	O
,	O
915.	O
yuille	O
,	O
a.	O
and	O
s.	O
zheng	O
(	O
2009	O
)	O
.	O
com-	O
positional	O
noisy-logical	O
learning	B
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
.	O
yuille	O
,	O
a.	O
l.	O
and	O
x.	O
he	O
(	O
2011	O
)	O
.	O
proba-	O
bilistic	O
models	O
of	O
vision	O
and	O
max-	O
margin	B
methods	O
.	O
frontiers	O
of	O
electrical	O
and	O
electronic	O
engineer-	O
ing	O
7	O
(	O
1	O
)	O
.	O
zellner	O
,	O
a	O
.	O
(	O
1986	O
)	O
.	O
on	O
assessing	O
prior	O
distributions	O
and	O
bayesian	O
regres-	O
sion	O
analysis	O
with	O
g-prior	B
distri-	O
butions	O
.	O
in	O
bayesian	O
inference	B
and	O
decision	B
techniques	O
,	O
studies	O
of	O
bayesian	O
and	O
econometrics	O
and	O
statistics	O
volume	O
6.	O
north	O
holland	O
.	O
zhai	O
,	O
c.	O
and	O
j.	O
lafferty	O
(	O
2004	O
)	O
.	O
a	O
study	O
of	O
smoothing	O
methods	O
for	O
language	B
models	I
applied	O
to	O
infor-	O
mation	O
retrieval	O
.	O
acm	O
trans	O
.	O
on	O
in-	O
formation	O
systems	O
22	O
(	O
2	O
)	O
,	O
179–214	O
.	O
zhang	O
,	O
n.	O
(	O
2004	O
)	O
.	O
hierarchical	O
latnet	O
class	O
models	O
for	O
cluster	O
analysis	O
.	O
j.	O
of	O
machine	B
learning	I
research	O
,	O
301–	O
308.	O
zhang	O
,	O
n.	O
and	O
d.	O
poole	O
(	O
1996	O
)	O
.	O
ex-	O
independence	O
in	O
j.	O
of	O
ploiting	O
causal	O
bayesian	O
network	O
inference	B
.	O
ai	O
research	O
,	O
301–328	O
.	O
zhang	O
,	O
t.	O
(	O
2008	O
)	O
.	O
adaptive	O
forward-	O
backward	O
greedy	O
algorithm	O
for	O
sparse	B
learning	O
with	O
linear	O
mod-	O
els	O
.	O
in	O
nips	O
.	O
zhang	O
,	O
x.	O
,	O
t.	O
graepel	O
,	O
and	O
r.	O
herbrich	O
(	O
2010	O
)	O
.	O
bayesian	O
online	B
learning	I
for	O
multi-label	O
and	O
multi-variate	O
performance	O
measures	O
.	O
in	O
ai/statis-	O
tics	O
.	O
zhao	O
,	O
j.-h.	O
and	O
p.	O
l.	O
h.	O
yu	O
(	O
2008	O
,	O
november	O
)	O
.	O
fast	O
ml	O
estimation	O
for	O
the	O
mixture	O
of	O
factor	O
analyzers	O
via	O
an	O
ecm	O
algorithm	O
.	O
ieee	O
.	O
trans	O
.	O
on	O
neural	B
networks	I
19	O
(	O
11	O
)	O
.	O
zhao	O
,	O
p.	O
and	O
b.	O
yu	O
(	O
2007	O
)	O
.	O
stagewise	O
j.	O
of	O
machine	B
learning	I
re-	O
lasso	B
.	O
search	O
8	O
,	O
2701–2726	O
.	O
zhou	O
,	O
h.	O
,	O
d.	O
karakos	O
,	O
s.	O
khudanpur	O
,	O
a.	O
andreou	O
,	O
and	O
c.	O
priebe	O
(	O
2009	O
)	O
.	O
on	O
projections	O
of	O
gaussian	O
dis-	O
tributions	O
using	O
maximum	O
likeli-	O
hood	O
criteria	O
.	O
in	O
proc	O
.	O
of	O
the	O
work-	O
shop	O
on	O
information	B
theory	I
and	O
its	O
applications	O
.	O
zhou	O
,	O
m.	O
,	O
h.	O
chen	O
,	O
j.	O
paisley	O
,	O
l.	O
ren	O
,	O
g.	O
sapiro	O
,	O
and	O
l.	O
carin	O
(	O
2009	O
)	O
.	O
non-parametric	O
bayesian	O
dictio-	O
nary	O
learning	B
for	O
sparse	B
image	O
representations	O
.	O
in	O
nips	O
.	O
zhou	O
,	O
x.	O
and	O
x.	O
liu	O
(	O
2008	O
)	O
.	O
the	O
em	O
algorithm	O
for	O
the	O
extended	O
ﬁ-	O
nite	O
mixture	O
of	O
the	O
factor	B
analyz-	O
ers	O
model	O
.	O
computational	O
statistics	O
and	O
data	O
analysis	O
52	O
,	O
3939–3953	O
.	O
zhu	O
,	O
c.	O
s.	O
,	O
n.	O
y.	O
wu	O
,	O
and	O
d.	O
mum-	O
ford	O
(	O
1997	O
,	O
november	O
)	O
.	O
minimax	O
entropy	O
principle	O
and	O
its	O
applica-	O
tion	O
to	O
texture	O
modeling	O
.	O
neural	O
computation	O
9	O
(	O
8	O
)	O
.	O
zhu	O
,	O
j.	O
and	O
e.	O
xing	O
(	O
2010	O
)	O
.	O
conditional	O
in	O
intl	O
.	O
conf	O
.	O
topic	B
random	O
ﬁelds	O
.	O
on	O
machine	B
learning	I
.	O
zhu	O
,	O
l.	O
,	O
y.	O
chen	O
,	O
a.yuille	O
,	O
and	O
w.	O
free-	O
man	O
(	O
2010	O
)	O
.	O
latent	B
hierarchical	O
structure	B
learning	I
for	O
object	O
detec-	O
tion	O
.	O
in	O
cvpr	O
.	O
zhu	O
,	O
m.	O
and	O
a.	O
ghodsi	O
(	O
2006	O
)	O
.	O
au-	O
tomatic	O
dimensionality	O
selection	O
from	O
the	O
scree	B
plot	I
via	O
the	O
use	O
of	O
proﬁle	O
likelihood	O
.	O
computational	O
statistics	O
&	O
data	O
analysis	O
51	O
,	O
918–	O
930.	O
zhu	O
,	O
m.	O
and	O
a.	O
lu	O
(	O
2004	O
)	O
.	O
the	O
counter-	O
intuitive	O
non-informative	B
prior	O
for	O
the	O
bernoulli	O
family	B
.	O
j.	O
statistics	O
ed-	O
ucation	O
.	O
zinkevich	O
,	O
m.	O
(	O
2003	O
)	O
.	O
online	O
con-	O
vex	O
programming	O
and	O
generalized	O
inﬁnitesimal	O
gradient	O
ascent	O
.	O
in	O
intl	O
.	O
conf	O
.	O
on	O
machine	B
learning	I
,	O
pp	O
.	O
928â	O
˘a	O
¸s936	O
.	O
zobay	O
,	O
o	O
.	O
(	O
2009	O
)	O
.	O
mean	B
ﬁeld	I
inference	O
for	O
the	O
dirichlet	O
process	O
mixture	B
model	I
.	O
electronic	O
j.	O
of	O
statistics	O
3	O
,	O
507–545	O
.	O
zhao	O
,	O
p.	O
,	O
g.	O
rocha	O
,	O
and	O
b.	O
yu	O
(	O
2005	O
)	O
.	O
grouped	O
and	O
hierarchical	O
model	O
selection	O
through	O
composite	O
abso-	O
lute	O
penalties	O
.	O
technical	O
report	O
,	O
uc	O
berkeley	O
.	O
zoeter	O
,	O
o	O
.	O
(	O
2007	O
)	O
.	O
bayesian	O
generalized	B
linear	I
models	I
in	O
a	O
terabyte	O
world	O
.	O
in	O
proc	O
.	O
5th	O
international	O
sympo-	O
sium	O
on	O
image	O
and	O
signal	O
process-	O
ing	O
and	O
analysis	O
.	O
bibliography	O
1045	O
zou	O
,	O
h.	O
(	O
2006	O
)	O
.	O
the	O
adaptive	B
lasso	I
j.	O
of	O
the	O
and	O
its	O
oracle	O
properties	O
.	O
am	O
.	O
stat	O
.	O
assoc.	O
,	O
1418–1429	O
.	O
nent	O
analysis	O
.	O
j.	O
of	O
computational	O
and	O
graphical	O
statistics	O
15	O
(	O
2	O
)	O
,	O
262–	O
286.	O
cave	O
penalized	O
likelihood	O
models	O
.	O
annals	O
of	O
statistics	O
36	O
(	O
4	O
)	O
,	O
1509–1533	O
.	O
zou	O
,	O
h.	O
and	O
t.	O
hastie	O
(	O
2005	O
)	O
.	O
regular-	O
ization	O
and	O
variable	O
selection	O
via	O
the	O
elastic	B
net	I
.	O
j.	O
of	O
royal	O
stat	O
.	O
soc	O
.	O
series	O
b	O
67	O
(	O
2	O
)	O
,	O
301–320	O
.	O
zou	O
,	O
h.	O
,	O
t.	O
hastie	O
,	O
and	O
r.	O
tibshirani	O
(	O
2007	O
)	O
.	O
on	O
the	O
``	O
degrees	O
of	O
free-	O
dom	O
''	O
of	O
the	O
lasso	B
.	O
annals	O
of	O
statis-	O
tics	O
35	O
(	O
5	O
)	O
,	O
2173–2192	O
.	O
zou	O
,	O
h.	O
,	O
t.	O
hastie	O
,	O
and	O
r.	O
tibshirani	O
sparse	B
principal	O
compo-	O
(	O
2006	O
)	O
.	O
zou	O
,	O
h.	O
and	O
r.	O
li	O
one-	O
step	O
sparse	O
estimates	O
in	O
noncon-	O
(	O
2008	O
)	O
.	O
zweig	O
,	O
g.	O
and	O
m.	O
padmanabhan	O
(	O
2000	O
)	O
.	O
exact	O
alpha-beta	O
computa-	O
tion	O
in	O
logarithmic	O
space	O
with	O
ap-	O
plication	O
to	O
map	O
word	O
graph	B
con-	O
struction	O
.	O
in	O
proc	O
.	O
intl	O
.	O
conf	O
.	O
spoken	O
lang	O
.	O