copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
information	B
theory	I
,	O
inference	B
,	O
and	B
learning	I
algorithms	O
david	O
j.c.	O
mackay	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
information	B
theory	I
,	O
inference	B
,	O
and	B
learning	I
algorithms	O
david	O
j.c.	O
mackay	O
mackay	O
@	O
mrao.cam.ac.uk	O
c	O
(	O
cid:13	O
)	O
1995	O
,	O
1996	O
,	O
1997	O
,	O
1998	O
,	O
1999	O
,	O
2000	O
,	O
2001	O
,	O
2002	O
,	O
2003	O
,	O
2004	O
,	O
2005	O
c	O
(	O
cid:13	O
)	O
cambridge	O
university	O
press	O
2003	O
version	O
7.2	O
(	O
fourth	O
printing	O
)	O
march	O
28	O
,	O
2005	O
please	O
send	O
feedback	B
on	O
this	O
book	O
via	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
version	O
6.0	O
of	O
this	O
book	O
was	O
published	O
by	O
c.u.p	O
.	O
in	O
september	O
2003.	O
it	O
will	O
remain	O
viewable	O
on-screen	O
on	O
the	O
above	O
website	O
,	O
in	O
postscript	O
,	O
djvu	O
,	O
and	O
pdf	O
formats	O
.	O
in	O
the	O
second	O
printing	O
(	O
version	O
6.6	O
)	O
minor	O
typos	O
were	O
corrected	O
,	O
and	O
the	O
book	O
design	O
was	O
slightly	O
altered	O
to	O
modify	O
the	O
placement	O
of	O
section	O
numbers	O
.	O
in	O
the	O
third	O
printing	O
(	O
version	O
7.0	O
)	O
minor	O
typos	O
were	O
corrected	O
,	O
and	O
chapter	O
8	O
was	O
renamed	O
‘	O
dependent	O
random	O
variables	O
’	O
(	O
instead	O
of	O
‘	O
correlated	O
’	O
)	O
.	O
in	O
the	O
fourth	O
printing	O
(	O
version	O
7.2	O
)	O
minor	O
typos	O
were	O
corrected	O
.	O
(	O
c.u.p	O
.	O
replace	O
this	O
page	O
with	O
their	O
own	O
page	O
ii	O
.	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
contents	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
preface	O
introduction	O
to	O
information	B
theory	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
1	O
2	O
3	O
more	O
about	O
inference	B
i	O
data	B
compression	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4	O
5	O
6	O
7	O
the	O
source	B
coding	I
theorem	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
symbol	O
codes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
stream	B
codes	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
codes	O
for	B
integers	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
v	O
3	O
22	O
48	O
65	O
67	O
91	O
110	O
132	O
ii	O
noisy-channel	B
coding	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
137	O
dependent	O
random	O
variables	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
8	O
communication	B
over	O
a	O
noisy	B
channel	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
9	O
10	O
the	O
noisy-channel	B
coding	I
theorem	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
11	O
error-correcting	B
codes	I
and	O
real	O
channels	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
138	O
146	O
162	O
177	O
iii	O
further	O
topics	O
in	O
information	O
theory	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
191	O
12	O
hash	O
codes	O
:	O
codes	O
for	O
e	O
(	O
cid:14	O
)	O
cient	O
information	B
retrieval	I
13	O
binary	O
codes	O
14	O
very	B
good	I
linear	O
codes	O
exist	O
15	O
further	O
exercises	O
on	O
information	O
theory	B
16	O
message	B
passing	I
17	O
communication	B
over	O
constrained	B
noiseless	O
channels	O
18	O
crosswords	O
and	O
codebreaking	O
19	O
why	O
have	O
sex	O
?	O
information	B
acquisition	O
and	O
evolution	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
193	O
206	O
229	O
233	O
241	O
248	O
260	O
269	O
iv	O
probabilities	O
and	O
inference	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
281	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
20	O
an	O
example	O
inference	B
task	O
:	O
clustering	B
21	O
exact	O
inference	O
by	O
complete	O
enumeration	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
22	O
maximum	B
likelihood	I
and	O
clustering	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
23	O
useful	O
probability	B
distributions	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
24	O
exact	O
marginalization	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
25	O
exact	O
marginalization	O
in	O
trellises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
26	O
exact	O
marginalization	O
in	O
graphs	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
27	O
laplace	O
’	O
s	O
method	B
284	O
293	O
300	O
311	O
319	O
324	O
334	O
341	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
ising	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
28	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
29	O
monte	O
carlo	O
methods	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
30	O
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
31	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
32	O
exact	O
monte	O
carlo	O
sampling	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
33	O
variational	B
methods	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
34	O
independent	B
component	I
analysis	I
and	O
latent	B
variable	I
mod-	O
elling	O
343	O
357	O
387	O
400	O
413	O
422	O
437	O
445	O
451	O
457	O
35	O
random	B
inference	O
topics	O
36	O
decision	B
theory	I
37	O
bayesian	O
inference	B
and	O
sampling	B
theory	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
v	O
neural	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
467	O
introduction	O
to	O
neural	O
networks	O
38	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
39	O
the	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
40	O
capacity	B
of	O
a	O
single	B
neuron	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
41	O
learning	B
as	I
inference	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
42	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
43	O
boltzmann	O
machines	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
44	O
supervised	O
learning	O
in	O
multilayer	O
networks	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
45	O
gaussian	O
processes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
46	O
deconvolution	B
468	O
471	O
483	O
492	O
505	O
522	O
527	O
535	O
549	O
vi	O
sparse	B
graph	I
codes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
555	O
47	O
low-density	B
parity-check	I
codes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
48	O
convolutional	B
codes	O
and	O
turbo	O
codes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
49	O
repeat	O
{	O
accumulate	O
codes	O
50	O
digital	B
fountain	I
codes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
557	O
574	O
582	O
589	O
vii	O
appendices	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
597	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
a	O
notation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
b	O
some	O
physics	B
c	O
some	O
mathematics	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
bibliography	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
index	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
598	O
601	O
605	O
613	O
620	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
preface	O
this	O
book	O
is	O
aimed	O
at	O
senior	O
undergraduates	O
and	O
graduate	O
students	O
in	O
engi-	O
neering	O
,	O
science	O
,	O
mathematics	O
,	O
and	O
computing	O
.	O
it	O
expects	O
familiarity	O
with	O
calculus	O
,	O
probability	B
theory	O
,	O
and	O
linear	O
algebra	O
as	O
taught	O
in	O
a	O
(	O
cid:12	O
)	O
rst-	O
or	O
second-	O
year	O
undergraduate	O
course	O
on	O
mathematics	O
for	O
scientists	O
and	O
engineers	O
.	O
conventional	O
courses	O
on	O
information	O
theory	B
cover	O
not	O
only	O
the	O
beauti-	O
ful	O
theoretical	O
ideas	O
of	O
shannon	O
,	O
but	O
also	O
practical	B
solutions	O
to	O
communica-	O
tion	O
problems	O
.	O
this	O
book	O
goes	O
further	O
,	O
bringing	O
in	O
bayesian	O
data	B
modelling	I
,	O
monte	O
carlo	O
methods	B
,	O
variational	B
methods	I
,	O
clustering	B
algorithms	O
,	O
and	O
neural	O
networks	O
.	O
why	O
unify	O
information	B
theory	I
and	O
machine	B
learning	I
?	O
because	O
they	O
are	O
two	O
sides	O
of	O
the	O
same	O
coin	B
.	O
in	O
the	O
1960s	O
,	O
a	O
single	O
(	O
cid:12	O
)	O
eld	O
,	O
cybernetics	O
,	O
was	O
populated	O
by	O
information	O
theorists	O
,	O
computer	B
scientists	O
,	O
and	O
neuroscientists	O
,	O
all	O
studying	O
common	O
problems	O
.	O
information	B
theory	I
and	O
machine	B
learning	I
still	O
belong	O
together	O
.	O
brains	O
are	O
the	O
ultimate	O
compression	B
and	O
communication	B
systems	O
.	O
and	O
the	O
state-of-the-art	O
algorithms	B
for	O
both	O
data	B
compression	I
and	O
error-correcting	B
codes	I
use	O
the	O
same	O
tools	O
as	O
machine	O
learning	B
.	O
how	O
to	O
use	O
this	O
book	O
the	O
essential	O
dependencies	O
between	O
chapters	O
are	O
indicated	O
in	O
the	O
(	O
cid:12	O
)	O
gure	O
on	O
the	O
next	O
page	O
.	O
an	O
arrow	O
from	O
one	O
chapter	O
to	O
another	O
indicates	O
that	O
the	O
second	O
chapter	O
requires	O
some	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
.	O
within	O
parts	O
i	O
,	O
ii	O
,	O
iv	O
,	O
and	O
v	O
of	O
this	O
book	O
,	O
chapters	O
on	O
advanced	O
or	O
optional	O
topics	O
are	O
towards	O
the	O
end	O
.	O
all	O
chapters	O
of	O
part	O
iii	O
are	O
optional	O
on	O
a	O
(	O
cid:12	O
)	O
rst	O
reading	O
,	O
except	O
perhaps	O
for	O
chapter	O
16	O
(	O
message	B
passing	I
)	O
.	O
the	O
same	O
system	O
sometimes	O
applies	O
within	O
a	O
chapter	O
:	O
the	O
(	O
cid:12	O
)	O
nal	O
sections	O
of-	O
ten	O
deal	O
with	O
advanced	O
topics	O
that	O
can	O
be	O
skipped	O
on	O
a	O
(	O
cid:12	O
)	O
rst	O
reading	O
.	O
for	O
exam-	O
ple	O
in	O
two	O
key	O
chapters	O
{	O
chapter	O
4	O
(	O
the	O
source	B
coding	I
theorem	I
)	O
and	O
chap-	O
ter	O
10	O
(	O
the	O
noisy-channel	B
coding	I
theorem	I
)	O
{	O
the	O
(	O
cid:12	O
)	O
rst-time	O
reader	O
should	O
detour	O
at	O
section	B
4.5	O
and	O
section	O
10.4	O
respectively	O
.	O
pages	O
vii	O
{	O
x	O
show	O
a	O
few	O
ways	O
to	O
use	O
this	O
book	O
.	O
first	O
,	O
i	O
give	O
the	O
roadmap	O
for	O
a	O
course	O
that	O
i	O
teach	O
in	O
cambridge	O
:	O
‘	O
information	B
theory	I
,	O
pattern	B
recognition	I
,	O
and	O
neural	O
networks	O
’	O
.	O
the	O
book	O
is	O
also	O
intended	O
as	O
a	O
textbook	O
for	O
traditional	O
courses	O
in	O
information	O
theory	B
.	O
the	O
second	O
roadmap	O
shows	O
the	O
chapters	O
for	O
an	O
introductory	O
information	B
theory	I
course	O
and	O
the	O
third	O
for	O
a	O
course	O
aimed	O
at	O
an	O
understanding	O
of	O
state-of-the-art	O
error-correcting	B
codes	I
.	O
the	O
fourth	O
roadmap	O
shows	O
how	O
to	O
use	O
the	O
text	O
in	O
a	O
conventional	O
course	O
on	O
machine	O
learning	B
.	O
v	O
vi	O
1	O
2	O
3	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
more	O
about	O
inference	B
i	O
data	B
compression	I
4	O
5	O
6	O
7	O
the	O
source	B
coding	I
theorem	I
symbol	O
codes	O
stream	B
codes	I
codes	O
for	B
integers	I
8	O
9	O
10	O
11	O
dependent	O
random	O
variables	O
communication	B
over	O
a	O
noisy	B
channel	I
the	O
noisy-channel	B
coding	I
theorem	I
error-correcting	O
codes	O
and	O
real	O
channels	O
iii	O
further	O
topics	O
in	O
information	O
theory	B
hash	O
codes	O
binary	O
codes	O
12	O
13	O
14	O
15	O
20	O
21	O
an	O
example	O
inference	B
task	O
:	O
clustering	B
exact	O
inference	B
by	O
complete	O
enumeration	O
22	O
maximum	B
likelihood	I
and	O
clustering	B
23	O
24	O
25	O
26	O
27	O
useful	O
probability	B
distributions	I
exact	O
marginalization	B
exact	O
marginalization	B
in	O
trellises	O
exact	O
marginalization	O
in	O
graphs	O
laplace	O
’	O
s	O
method	B
28	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
30	O
31	O
32	O
33	O
34	O
35	O
36	O
37	O
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
ising	O
models	O
exact	O
monte	O
carlo	O
sampling	O
variational	O
methods	B
independent	O
component	O
analysis	B
random	O
inference	B
topics	O
decision	B
theory	I
bayesian	O
inference	B
and	O
sampling	B
theory	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
introduction	O
to	O
information	B
theory	I
iv	O
probabilities	O
and	O
inference	O
preface	O
ii	O
noisy-channel	B
coding	I
29	O
monte	O
carlo	O
methods	B
very	O
good	B
linear	O
codes	O
exist	O
further	O
exercises	O
on	O
information	O
theory	B
v	O
neural	O
networks	O
16	O
message	B
passing	I
17	O
18	O
constrained	B
noiseless	O
channels	O
crosswords	O
and	O
codebreaking	O
19	O
why	O
have	O
sex	O
?	O
dependencies	O
38	O
39	O
40	O
41	O
42	O
43	O
44	O
45	O
46	O
introduction	O
to	O
neural	O
networks	O
the	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
capacity	B
of	O
a	O
single	B
neuron	I
learning	O
as	B
inference	I
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
boltzmann	O
machines	O
supervised	O
learning	O
in	O
multilayer	O
networks	O
gaussian	O
processes	O
deconvolution	B
vi	O
sparse	B
graph	I
codes	O
47	O
48	O
49	O
50	O
low-density	B
parity-check	I
codes	O
convolutional	B
codes	O
and	O
turbo	O
codes	O
repeat	O
{	O
accumulate	O
codes	O
digital	B
fountain	I
codes	O
preface	O
1	O
1	O
2	O
2	O
3	O
3	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
more	O
about	O
inference	B
more	O
about	O
inference	B
i	O
data	B
compression	I
4	O
4	O
5	O
5	O
6	O
6	O
7	O
the	O
source	B
coding	I
theorem	I
the	O
source	B
coding	I
theorem	I
symbol	O
codes	O
symbol	O
codes	O
stream	B
codes	I
stream	O
codes	O
codes	O
for	B
integers	I
8	O
8	O
9	O
9	O
10	O
10	O
11	O
11	O
dependent	O
random	O
variables	O
dependent	O
random	O
variables	O
communication	B
over	O
a	O
noisy	B
channel	I
communication	O
over	O
a	O
noisy	B
channel	I
the	O
noisy-channel	B
coding	I
theorem	I
the	O
noisy-channel	B
coding	I
theorem	I
error-correcting	O
codes	O
and	O
real	O
channels	O
error-correcting	B
codes	I
and	O
real	O
channels	O
iii	O
further	O
topics	O
in	O
information	O
theory	B
hash	O
codes	O
binary	O
codes	O
12	O
13	O
14	O
15	O
20	O
20	O
21	O
21	O
an	O
example	O
inference	B
task	O
:	O
clustering	B
an	O
example	O
inference	B
task	O
:	O
clustering	B
exact	O
inference	B
by	O
complete	O
enumeration	O
exact	O
inference	O
by	O
complete	O
enumeration	O
22	O
maximum	B
likelihood	I
and	O
clustering	B
22	O
maximum	B
likelihood	I
and	O
clustering	B
23	O
24	O
24	O
25	O
26	O
27	O
27	O
useful	O
probability	B
distributions	I
exact	O
marginalization	B
exact	O
marginalization	B
exact	O
marginalization	B
in	O
trellises	O
exact	O
marginalization	O
in	O
graphs	O
laplace	O
’	O
s	O
method	B
laplace	O
’	O
s	O
method	B
28	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
30	O
30	O
31	O
31	O
32	O
32	O
33	O
33	O
34	O
35	O
36	O
37	O
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
ising	O
models	O
ising	O
models	O
exact	O
monte	O
carlo	O
sampling	O
exact	O
monte	O
carlo	O
sampling	O
variational	O
methods	B
variational	O
methods	B
independent	O
component	O
analysis	B
random	O
inference	B
topics	O
decision	B
theory	I
bayesian	O
inference	B
and	O
sampling	B
theory	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
introduction	O
to	O
information	B
theory	I
introduction	O
to	O
information	B
theory	I
iv	O
probabilities	O
and	O
inference	O
vii	O
ii	O
noisy-channel	B
coding	I
29	O
monte	O
carlo	O
methods	B
29	O
monte	O
carlo	O
methods	B
very	O
good	B
linear	O
codes	O
exist	O
further	O
exercises	O
on	O
information	O
theory	B
v	O
neural	O
networks	O
16	O
message	B
passing	I
17	O
18	O
constrained	B
noiseless	O
channels	O
crosswords	O
and	O
codebreaking	O
19	O
why	O
have	O
sex	O
?	O
my	O
cambridge	O
course	O
on	O
,	O
information	B
theory	I
,	O
pattern	B
recognition	I
,	O
and	O
neural	O
networks	O
38	O
38	O
39	O
39	O
40	O
40	O
41	O
41	O
42	O
42	O
43	O
44	O
45	O
46	O
introduction	O
to	O
neural	O
networks	O
introduction	O
to	O
neural	O
networks	O
the	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
the	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
capacity	B
of	O
a	O
single	B
neuron	I
capacity	O
of	O
a	O
single	B
neuron	I
learning	O
as	B
inference	I
learning	O
as	B
inference	I
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
boltzmann	O
machines	O
supervised	O
learning	O
in	O
multilayer	O
networks	O
gaussian	O
processes	O
deconvolution	B
vi	O
sparse	B
graph	I
codes	O
47	O
47	O
48	O
49	O
50	O
low-density	B
parity-check	I
codes	O
low-density	B
parity-check	I
codes	O
convolutional	B
codes	O
and	O
turbo	O
codes	O
repeat	O
{	O
accumulate	O
codes	O
digital	B
fountain	I
codes	O
viii	O
1	O
1	O
2	O
2	O
3	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
more	O
about	O
inference	B
i	O
data	B
compression	I
4	O
4	O
5	O
5	O
6	O
6	O
7	O
the	O
source	B
coding	I
theorem	I
the	O
source	B
coding	I
theorem	I
symbol	O
codes	O
symbol	O
codes	O
stream	B
codes	I
stream	O
codes	O
codes	O
for	B
integers	I
8	O
8	O
9	O
9	O
10	O
10	O
11	O
dependent	O
random	O
variables	O
dependent	O
random	O
variables	O
communication	B
over	O
a	O
noisy	B
channel	I
communication	O
over	O
a	O
noisy	B
channel	I
the	O
noisy-channel	B
coding	I
theorem	I
the	O
noisy-channel	B
coding	I
theorem	I
error-correcting	O
codes	O
and	O
real	O
channels	O
iii	O
further	O
topics	O
in	O
information	O
theory	B
hash	O
codes	O
binary	O
codes	O
12	O
13	O
14	O
15	O
20	O
21	O
an	O
example	O
inference	B
task	O
:	O
clustering	B
exact	O
inference	B
by	O
complete	O
enumeration	O
22	O
maximum	B
likelihood	I
and	O
clustering	B
23	O
24	O
25	O
26	O
27	O
useful	O
probability	B
distributions	I
exact	O
marginalization	B
exact	O
marginalization	B
in	O
trellises	O
exact	O
marginalization	O
in	O
graphs	O
laplace	O
’	O
s	O
method	B
28	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
30	O
31	O
32	O
33	O
34	O
35	O
36	O
37	O
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
ising	O
models	O
exact	O
monte	O
carlo	O
sampling	O
variational	O
methods	B
independent	O
component	O
analysis	B
random	O
inference	B
topics	O
decision	B
theory	I
bayesian	O
inference	B
and	O
sampling	B
theory	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
introduction	O
to	O
information	B
theory	I
introduction	O
to	O
information	B
theory	I
iv	O
probabilities	O
and	O
inference	O
preface	O
ii	O
noisy-channel	B
coding	I
29	O
monte	O
carlo	O
methods	B
very	O
good	B
linear	O
codes	O
exist	O
further	O
exercises	O
on	O
information	O
theory	B
v	O
neural	O
networks	O
16	O
message	B
passing	I
17	O
18	O
constrained	B
noiseless	O
channels	O
crosswords	O
and	O
codebreaking	O
19	O
why	O
have	O
sex	O
?	O
short	O
course	O
on	O
information	O
theory	B
38	O
39	O
40	O
41	O
42	O
43	O
44	O
45	O
46	O
introduction	O
to	O
neural	O
networks	O
the	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
capacity	B
of	O
a	O
single	B
neuron	I
learning	O
as	B
inference	I
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
boltzmann	O
machines	O
supervised	O
learning	O
in	O
multilayer	O
networks	O
gaussian	O
processes	O
deconvolution	B
vi	O
sparse	B
graph	I
codes	O
47	O
48	O
49	O
50	O
low-density	B
parity-check	I
codes	O
convolutional	B
codes	O
and	O
turbo	O
codes	O
repeat	O
{	O
accumulate	O
codes	O
digital	B
fountain	I
codes	O
preface	O
1	O
2	O
3	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
more	O
about	O
inference	B
i	O
data	B
compression	I
4	O
5	O
6	O
7	O
the	O
source	B
coding	I
theorem	I
symbol	O
codes	O
stream	B
codes	I
codes	O
for	B
integers	I
8	O
9	O
10	O
11	O
11	O
dependent	O
random	O
variables	O
communication	B
over	O
a	O
noisy	B
channel	I
the	O
noisy-channel	B
coding	I
theorem	I
error-correcting	O
codes	O
and	O
real	O
channels	O
error-correcting	B
codes	I
and	O
real	O
channels	O
iii	O
further	O
topics	O
in	O
information	O
theory	B
hash	O
codes	O
hash	O
codes	O
binary	O
codes	O
binary	O
codes	O
12	O
12	O
13	O
13	O
14	O
14	O
15	O
15	O
20	O
21	O
an	O
example	O
inference	B
task	O
:	O
clustering	B
exact	O
inference	B
by	O
complete	O
enumeration	O
22	O
maximum	B
likelihood	I
and	O
clustering	B
23	O
24	O
24	O
25	O
25	O
26	O
26	O
27	O
useful	O
probability	B
distributions	I
exact	O
marginalization	B
exact	O
marginalization	B
exact	O
marginalization	B
in	O
trellises	O
exact	O
marginalization	O
in	O
trellises	O
exact	O
marginalization	O
in	O
graphs	O
exact	O
marginalization	B
in	O
graphs	O
laplace	O
’	O
s	O
method	B
28	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
30	O
31	O
32	O
33	O
34	O
35	O
36	O
37	O
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
ising	O
models	O
exact	O
monte	O
carlo	O
sampling	O
variational	O
methods	B
independent	O
component	O
analysis	B
random	O
inference	B
topics	O
decision	B
theory	I
bayesian	O
inference	B
and	O
sampling	B
theory	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
introduction	O
to	O
information	B
theory	I
iv	O
probabilities	O
and	O
inference	O
ix	O
ii	O
noisy-channel	B
coding	I
29	O
monte	O
carlo	O
methods	B
very	O
good	B
linear	O
codes	O
exist	O
very	B
good	I
linear	O
codes	O
exist	O
further	O
exercises	O
on	O
information	O
theory	B
further	O
exercises	O
on	O
information	O
theory	B
v	O
neural	O
networks	O
16	O
message	B
passing	I
16	O
message	B
passing	I
17	O
17	O
18	O
constrained	B
noiseless	O
channels	O
constrained	B
noiseless	O
channels	O
crosswords	O
and	O
codebreaking	O
19	O
why	O
have	O
sex	O
?	O
advanced	O
course	O
on	O
information	O
theory	B
and	O
coding	O
38	O
39	O
40	O
41	O
42	O
43	O
44	O
45	O
46	O
introduction	O
to	O
neural	O
networks	O
the	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
capacity	B
of	O
a	O
single	B
neuron	I
learning	O
as	B
inference	I
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
boltzmann	O
machines	O
supervised	O
learning	O
in	O
multilayer	O
networks	O
gaussian	O
processes	O
deconvolution	B
vi	O
sparse	B
graph	I
codes	O
47	O
47	O
48	O
48	O
49	O
49	O
50	O
50	O
low-density	B
parity-check	I
codes	O
low-density	B
parity-check	I
codes	O
convolutional	B
codes	O
and	O
turbo	O
codes	O
convolutional	B
codes	O
and	O
turbo	O
codes	O
repeat	O
{	O
accumulate	O
codes	O
repeat	O
{	O
accumulate	O
codes	O
digital	B
fountain	I
codes	O
digital	B
fountain	I
codes	O
x	O
1	O
2	O
2	O
3	O
3	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
more	O
about	O
inference	B
more	O
about	O
inference	B
i	O
data	B
compression	I
4	O
5	O
6	O
7	O
the	O
source	B
coding	I
theorem	I
symbol	O
codes	O
stream	B
codes	I
codes	O
for	B
integers	I
8	O
9	O
10	O
11	O
dependent	O
random	O
variables	O
communication	B
over	O
a	O
noisy	B
channel	I
the	O
noisy-channel	B
coding	I
theorem	I
error-correcting	O
codes	O
and	O
real	O
channels	O
iii	O
further	O
topics	O
in	O
information	O
theory	B
hash	O
codes	O
binary	O
codes	O
12	O
13	O
14	O
15	O
20	O
20	O
21	O
21	O
an	O
example	O
inference	B
task	O
:	O
clustering	B
an	O
example	O
inference	B
task	O
:	O
clustering	B
exact	O
inference	B
by	O
complete	O
enumeration	O
exact	O
inference	O
by	O
complete	O
enumeration	O
22	O
maximum	B
likelihood	I
and	O
clustering	B
22	O
maximum	B
likelihood	I
and	O
clustering	B
23	O
24	O
24	O
25	O
26	O
27	O
27	O
useful	O
probability	B
distributions	I
exact	O
marginalization	B
exact	O
marginalization	B
exact	O
marginalization	B
in	O
trellises	O
exact	O
marginalization	O
in	O
graphs	O
laplace	O
’	O
s	O
method	B
laplace	O
’	O
s	O
method	B
28	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
28	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
30	O
30	O
31	O
31	O
32	O
32	O
33	O
33	O
34	O
34	O
35	O
36	O
37	O
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
ising	O
models	O
ising	O
models	O
exact	O
monte	O
carlo	O
sampling	O
exact	O
monte	O
carlo	O
sampling	O
variational	O
methods	B
variational	O
methods	B
independent	O
component	O
analysis	B
independent	O
component	O
analysis	B
random	O
inference	B
topics	O
decision	B
theory	I
bayesian	O
inference	B
and	O
sampling	B
theory	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
introduction	O
to	O
information	B
theory	I
iv	O
probabilities	O
and	O
inference	O
preface	O
ii	O
noisy-channel	B
coding	I
29	O
monte	O
carlo	O
methods	B
29	O
monte	O
carlo	O
methods	B
very	O
good	B
linear	O
codes	O
exist	O
further	O
exercises	O
on	O
information	O
theory	B
v	O
neural	O
networks	O
16	O
message	B
passing	I
17	O
18	O
constrained	B
noiseless	O
channels	O
crosswords	O
and	O
codebreaking	O
19	O
why	O
have	O
sex	O
?	O
a	O
course	O
on	O
bayesian	O
inference	B
and	O
machine	B
learning	I
38	O
38	O
39	O
39	O
40	O
40	O
41	O
41	O
42	O
42	O
43	O
43	O
44	O
44	O
45	O
45	O
46	O
introduction	O
to	O
neural	O
networks	O
introduction	O
to	O
neural	O
networks	O
the	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
the	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
capacity	B
of	O
a	O
single	B
neuron	I
capacity	O
of	O
a	O
single	B
neuron	I
learning	O
as	B
inference	I
learning	O
as	B
inference	I
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
boltzmann	O
machines	O
boltzmann	O
machines	O
supervised	O
learning	O
in	O
multilayer	O
networks	O
supervised	O
learning	O
in	O
multilayer	O
networks	O
gaussian	O
processes	O
gaussian	O
processes	O
deconvolution	B
vi	O
sparse	B
graph	I
codes	O
47	O
48	O
49	O
50	O
low-density	B
parity-check	I
codes	O
convolutional	B
codes	O
and	O
turbo	O
codes	O
repeat	O
{	O
accumulate	O
codes	O
digital	B
fountain	I
codes	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
xi	O
preface	O
about	O
the	O
exercises	O
you	O
can	O
understand	O
a	O
subject	O
only	O
by	O
creating	O
it	O
for	O
yourself	O
.	O
the	O
exercises	O
play	O
an	O
essential	O
role	O
in	O
this	O
book	O
.	O
for	O
guidance	O
,	O
each	O
has	O
a	O
rating	O
(	O
similar	O
to	O
that	O
used	O
by	O
knuth	O
(	O
1968	O
)	O
)	O
from	O
1	O
to	O
5	O
to	O
indicate	O
its	O
di	O
(	O
cid:14	O
)	O
culty	O
.	O
in	O
addition	O
,	O
exercises	O
that	O
are	O
especially	O
recommended	O
are	O
marked	O
by	O
a	O
marginal	B
encouraging	O
rat	O
.	O
some	O
exercises	O
that	O
require	O
the	O
use	O
of	O
a	O
computer	B
are	O
marked	O
with	O
a	O
c.	O
answers	O
to	O
many	O
exercises	O
are	O
provided	O
.	O
use	O
them	O
wisely	O
.	O
where	O
a	O
solu-	O
tion	O
is	O
provided	O
,	O
this	O
is	O
indicated	O
by	O
including	O
its	O
page	O
number	O
alongside	O
the	O
di	O
(	O
cid:14	O
)	O
culty	O
rating	O
.	O
solutions	O
to	O
many	O
of	O
the	O
other	O
exercises	O
will	O
be	O
supplied	O
to	O
instructors	O
using	O
this	O
book	O
in	O
their	O
teaching	O
;	O
please	O
email	B
solutions	O
@	O
cambridge.org	O
.	O
summary	B
of	O
codes	O
for	O
exercises	O
especially	O
recommended	O
.	O
c	O
recommended	O
parts	O
require	O
a	O
computer	B
[	O
p.	O
42	O
]	O
solution	O
provided	O
on	O
page	O
42	O
[	O
1	O
]	O
simple	O
(	O
one	O
minute	O
)	O
[	O
2	O
]	O
medium	O
(	O
quarter	O
hour	O
)	O
[	O
3	O
]	O
moderately	O
hard	O
[	O
4	O
]	O
hard	O
[	O
5	O
]	O
research	O
project	O
internet	B
resources	O
the	O
website	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila	O
contains	O
several	O
resources	O
:	O
1.	O
software	B
.	O
teaching	O
software	B
that	O
i	O
use	O
in	O
lectures	O
,	O
interactive	O
software	B
,	O
and	O
research	O
software	B
,	O
written	O
in	O
perl	O
,	O
octave	B
,	O
tcl	O
,	O
c	O
,	O
and	O
gnuplot	O
.	O
also	O
some	O
animations	O
.	O
2.	O
corrections	O
to	O
the	O
book	O
.	O
thank	O
you	O
in	O
advance	O
for	O
emailing	O
these	O
!	O
3.	O
this	O
book	O
.	O
the	O
book	O
is	O
provided	O
in	O
postscript	O
,	O
pdf	O
,	O
and	O
djvu	O
formats	O
for	O
on-screen	O
viewing	O
.	O
the	O
same	O
copyright	O
restrictions	O
apply	O
as	O
to	O
a	O
normal	B
book	O
.	O
about	O
this	O
edition	O
this	O
is	O
the	O
fourth	O
printing	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
edition	O
.	O
in	O
the	O
second	O
printing	O
,	O
the	O
design	O
of	O
the	O
book	O
was	O
altered	O
slightly	O
.	O
page-numbering	O
generally	O
remained	O
unchanged	O
,	O
except	O
in	O
chapters	O
1	O
,	O
6	O
,	O
and	O
28	O
,	O
where	O
a	O
few	O
paragraphs	O
,	O
(	O
cid:12	O
)	O
gures	O
,	O
and	O
equations	O
moved	O
around	O
.	O
all	O
equation	O
,	O
section	B
,	O
and	O
exercise	O
numbers	O
were	O
unchanged	O
.	O
in	O
the	O
third	O
printing	O
,	O
chapter	O
8	O
was	O
renamed	O
‘	O
dependent	O
random	O
variables	O
’	O
,	O
instead	O
of	O
‘	O
correlated	O
’	O
,	O
which	O
was	O
sloppy	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
preface	O
xii	O
acknowledgments	O
i	O
am	O
most	O
grateful	O
to	O
the	O
organizations	O
who	O
have	O
supported	O
me	O
while	O
this	O
book	O
gestated	O
:	O
the	O
royal	O
society	O
and	O
darwin	O
college	O
who	O
gave	O
me	O
a	O
fantas-	O
tic	O
research	O
fellowship	O
in	O
the	O
early	O
years	O
;	O
the	O
university	O
of	O
cambridge	O
;	O
the	O
keck	O
centre	O
at	O
the	O
university	O
of	O
california	O
in	O
san	O
francisco	O
,	O
where	O
i	O
spent	O
a	O
productive	O
sabbatical	O
;	O
and	O
the	O
gatsby	O
charitable	O
foundation	O
,	O
whose	O
support	O
gave	O
me	O
the	O
freedom	O
to	O
break	O
out	O
of	O
the	O
escher	O
staircase	B
that	O
book-writing	O
had	O
become	O
.	O
my	O
work	O
has	O
depended	O
on	O
the	O
generosity	O
of	O
free	O
software	B
authors	O
.	O
i	O
wrote	O
the	O
book	O
in	O
latex	O
2	O
''	O
.	O
three	O
cheers	O
for	O
donald	O
knuth	O
and	O
leslie	O
lamport	O
!	O
our	O
computers	O
run	O
the	O
gnu/linux	O
operating	O
system	O
.	O
i	O
use	O
emacs	O
,	O
perl	O
,	O
and	O
gnuplot	O
every	O
day	O
.	O
thank	O
you	O
richard	O
stallman	O
,	O
thank	O
you	O
linus	O
torvalds	O
,	O
thank	O
you	O
everyone	O
.	O
many	O
readers	O
,	O
too	O
numerous	O
to	O
name	O
here	O
,	O
have	O
given	O
feedback	B
on	O
the	O
book	O
,	O
and	O
to	O
them	O
all	O
i	O
extend	O
my	O
sincere	O
acknowledgments	O
.	O
i	O
especially	O
wish	O
to	O
thank	O
all	O
the	O
students	O
and	O
colleagues	O
at	O
cambridge	O
university	O
who	O
have	O
attended	O
my	O
lectures	O
on	O
information	O
theory	B
and	O
machine	B
learning	I
over	O
the	O
last	O
nine	O
years	O
.	O
the	O
members	O
of	O
the	O
inference	O
research	O
group	O
have	O
given	O
immense	O
support	O
,	O
and	O
i	O
thank	O
them	O
all	O
for	O
their	O
generosity	O
and	O
patience	O
over	O
the	O
last	O
ten	O
years	O
:	O
mark	O
gibbs	O
,	O
michelle	O
povinelli	O
,	O
simon	O
wilson	O
,	O
coryn	O
bailer-jones	O
,	O
matthew	O
davey	O
,	O
katriona	O
macphee	O
,	O
james	O
miskin	O
,	O
david	O
ward	O
,	O
edward	O
ratzer	O
,	O
seb	O
wills	O
,	O
john	O
barry	O
,	O
john	O
winn	O
,	O
phil	O
cowans	O
,	O
hanna	O
wallach	O
,	O
matthew	O
gar-	O
rett	O
,	O
and	O
especially	O
sanjoy	O
mahajan	O
.	O
thank	O
you	O
too	O
to	O
graeme	O
mitchison	O
,	O
mike	O
cates	O
,	O
and	O
davin	O
yap	O
.	O
finally	O
i	O
would	O
like	O
to	O
express	O
my	O
debt	O
to	O
my	O
personal	O
heroes	O
,	O
the	O
mentors	O
from	O
whom	O
i	O
have	O
learned	O
so	O
much	O
:	O
yaser	O
abu-mostafa	O
,	O
andrew	O
blake	O
,	O
john	O
bridle	O
,	O
peter	O
cheeseman	O
,	O
steve	O
gull	O
,	O
geo	O
(	O
cid:11	O
)	O
hinton	O
,	O
john	O
hop	O
(	O
cid:12	O
)	O
eld	O
,	O
steve	O
lut-	O
trell	O
,	O
robert	O
mackay	O
,	O
bob	O
mceliece	O
,	O
radford	O
neal	O
,	O
roger	O
sewell	O
,	O
and	O
john	O
skilling	O
.	O
dedication	O
this	O
book	O
is	O
dedicated	O
to	O
the	O
campaign	O
against	O
the	O
arms	O
trade	O
.	O
www.caat.org.uk	O
peace	O
can	O
not	O
be	O
kept	O
by	O
force	O
.	O
it	O
can	O
only	O
be	O
achieved	O
through	O
understanding	O
.	O
{	O
albert	O
einstein	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
chapter	O
1	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
chapter	O
,	O
you	O
will	O
need	O
to	O
be	O
familiar	O
with	O
the	O
binomial	B
distribution	I
.	O
and	O
to	O
solve	O
the	O
exercises	O
in	O
the	O
text	O
{	O
which	O
i	O
urge	O
you	O
to	O
do	O
{	O
you	O
will	O
need	O
to	O
know	O
stirling	O
’	O
s	O
approximation	B
for	O
the	O
factorial	B
function	O
,	O
x	O
!	O
’	O
xx	O
e	O
(	O
cid:0	O
)	O
x	O
,	O
and	O
be	O
able	O
to	O
apply	O
it	O
to	O
(	O
cid:0	O
)	O
n	O
(	O
n	O
(	O
cid:0	O
)	O
r	O
)	O
!	O
r	O
!	O
.	O
these	O
topics	O
are	O
reviewed	O
below	O
.	O
r	O
(	O
cid:1	O
)	O
=	O
n	O
!	O
unfamiliar	O
notation	B
?	O
see	O
appendix	O
a	O
,	O
p.598	O
.	O
the	O
binomial	B
distribution	I
example	O
1.1.	O
a	O
bent	B
coin	I
has	O
probability	B
f	O
of	O
coming	O
up	O
heads	O
.	O
the	O
coin	B
is	O
tossed	O
n	O
times	O
.	O
what	O
is	O
the	O
probability	B
distribution	O
of	O
the	O
number	O
of	O
heads	O
,	O
r	O
?	O
what	O
are	O
the	O
mean	B
and	O
variance	B
of	O
r	O
?	O
solution	O
.	O
the	O
number	O
of	O
heads	O
has	O
a	O
binomial	B
distribution	I
.	O
p	O
(	O
r	O
j	O
f	O
;	O
n	O
)	O
=	O
(	O
cid:18	O
)	O
n	O
r	O
(	O
cid:19	O
)	O
f	O
r	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
n	O
(	O
cid:0	O
)	O
r	O
:	O
(	O
1.1	O
)	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
the	O
mean	B
,	O
e	O
[	O
r	O
]	O
,	O
and	O
variance	O
,	O
var	O
[	O
r	O
]	O
,	O
of	O
this	O
distribution	B
are	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
r	O
n	O
p	O
(	O
r	O
j	O
f	O
;	O
n	O
)	O
r	O
(	O
1.2	O
)	O
figure	O
1.1.	O
the	O
binomial	B
distribution	I
p	O
(	O
r	O
j	O
f	O
=	O
0:3	O
;	O
n	O
=	O
10	O
)	O
.	O
e	O
[	O
r	O
]	O
(	O
cid:17	O
)	O
xr=0	O
var	O
[	O
r	O
]	O
(	O
cid:17	O
)	O
eh	O
(	O
r	O
(	O
cid:0	O
)	O
e	O
[	O
r	O
]	O
)	O
2i	O
=	O
e	O
[	O
r2	O
]	O
(	O
cid:0	O
)	O
(	O
e	O
[	O
r	O
]	O
)	O
2	O
=	O
n	O
xr=0	O
p	O
(	O
r	O
j	O
f	O
;	O
n	O
)	O
r2	O
(	O
cid:0	O
)	O
(	O
e	O
[	O
r	O
]	O
)	O
2	O
:	O
(	O
1.3	O
)	O
(	O
1.4	O
)	O
rather	O
than	O
evaluating	O
the	O
sums	O
over	O
r	O
in	O
(	O
1.2	O
)	O
and	O
(	O
1.4	O
)	O
directly	O
,	O
it	O
is	O
easiest	O
to	O
obtain	O
the	O
mean	B
and	O
variance	B
by	O
noting	O
that	O
r	O
is	O
the	O
sum	O
of	O
n	O
independent	O
random	O
variables	O
,	O
namely	O
,	O
the	O
number	O
of	O
heads	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
toss	O
(	O
which	O
is	O
either	O
zero	O
or	O
one	O
)	O
,	O
the	O
number	O
of	O
heads	O
in	O
the	O
second	O
toss	O
,	O
and	O
so	O
forth	O
.	O
in	O
general	O
,	O
e	O
[	O
x	O
+	O
y	O
]	O
=	O
e	O
[	O
x	O
]	O
+	O
e	O
[	O
y	O
]	O
var	O
[	O
x	O
+	O
y	O
]	O
=	O
var	O
[	O
x	O
]	O
+	O
var	O
[	O
y	O
]	O
for	O
any	O
random	B
variables	O
x	O
and	O
y	O
;	O
if	O
x	O
and	O
y	O
are	O
independent	O
:	O
(	O
1.5	O
)	O
so	O
the	O
mean	B
of	O
r	O
is	O
the	O
sum	O
of	O
the	O
means	O
of	O
those	O
random	B
variables	O
,	O
and	O
the	O
variance	B
of	O
r	O
is	O
the	O
sum	O
of	O
their	O
variances	O
.	O
the	O
mean	B
number	O
of	O
heads	O
in	O
a	O
single	O
toss	O
is	O
f	O
(	O
cid:2	O
)	O
1	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
(	O
cid:2	O
)	O
0	O
=	O
f	O
,	O
and	O
the	O
variance	B
of	O
the	O
number	O
of	O
heads	O
in	O
a	O
single	O
toss	O
is	O
(	O
cid:2	O
)	O
f	O
(	O
cid:2	O
)	O
12	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
(	O
cid:2	O
)	O
02	O
(	O
cid:3	O
)	O
(	O
cid:0	O
)	O
f	O
2	O
=	O
f	O
(	O
cid:0	O
)	O
f	O
2	O
=	O
f	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
;	O
so	O
the	O
mean	B
and	O
variance	B
of	O
r	O
are	O
:	O
(	O
1.6	O
)	O
e	O
[	O
r	O
]	O
=	O
n	O
f	O
and	O
var	O
[	O
r	O
]	O
=	O
n	O
f	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
:	O
2	O
(	O
1.7	O
)	O
1	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
2	O
about	O
chapter	O
1	O
approximating	O
x	O
!	O
and	O
(	O
cid:0	O
)	O
n	O
r	O
(	O
cid:1	O
)	O
let	O
’	O
s	O
derive	O
stirling	O
’	O
s	O
approximation	B
by	O
an	O
unconventional	O
route	O
.	O
we	O
start	O
from	O
the	O
poisson	O
distribution	B
with	O
mean	B
(	O
cid:21	O
)	O
,	O
p	O
(	O
r	O
j	O
(	O
cid:21	O
)	O
)	O
=	O
e	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
r	O
r	O
!	O
r	O
2	O
f0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
g	O
:	O
(	O
1.8	O
)	O
for	O
large	O
(	O
cid:21	O
)	O
,	O
this	O
distribution	B
is	O
well	O
approximated	O
{	O
at	O
least	O
in	O
the	O
vicinity	O
of	O
r	O
’	O
(	O
cid:21	O
)	O
{	O
by	O
a	O
gaussian	O
distribution	B
with	O
mean	B
(	O
cid:21	O
)	O
and	O
variance	O
(	O
cid:21	O
)	O
:	O
e	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
r	O
r	O
!	O
’	O
1	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:21	O
)	O
(	O
r	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
)	O
2	O
2	O
(	O
cid:21	O
)	O
:	O
e	O
(	O
cid:0	O
)	O
let	O
’	O
s	O
plug	O
r	O
=	O
(	O
cid:21	O
)	O
into	O
this	O
formula	O
,	O
then	O
rearrange	O
it	O
.	O
1	O
(	O
cid:21	O
)	O
!	O
’	O
e	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
)	O
(	O
cid:21	O
)	O
!	O
’	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
e	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:21	O
)	O
:	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:21	O
)	O
this	O
is	O
stirling	O
’	O
s	O
approximation	B
for	O
the	O
factorial	B
function	O
.	O
x	O
!	O
’	O
xx	O
e	O
(	O
cid:0	O
)	O
xp2	O
(	O
cid:25	O
)	O
x	O
,	O
ln	O
x	O
!	O
’	O
x	O
ln	O
x	O
(	O
cid:0	O
)	O
x	O
+	O
1	O
2	O
ln	O
2	O
(	O
cid:25	O
)	O
x	O
:	O
(	O
1.9	O
)	O
(	O
1.10	O
)	O
(	O
1.11	O
)	O
(	O
1.12	O
)	O
0.12	O
0.1	O
0.08	O
0.06	O
0.04	O
0.02	O
0	O
15	O
20	O
25	O
0	O
5	O
10	O
r	O
figure	O
1.2.	O
the	O
poisson	O
distribution	B
p	O
(	O
r	O
j	O
(	O
cid:21	O
)	O
=	O
15	O
)	O
.	O
we	O
have	O
derived	O
not	O
only	O
the	O
leading	O
order	O
behaviour	O
,	O
x	O
!	O
’	O
xx	O
e	O
(	O
cid:0	O
)	O
x	O
,	O
but	O
also	O
,	O
at	O
no	O
cost	O
,	O
the	O
next-order	O
correction	O
term	O
p2	O
(	O
cid:25	O
)	O
x.	O
we	O
now	O
apply	O
stirling	O
’	O
s	O
approximation	B
to	O
ln	O
(	O
cid:0	O
)	O
n	O
r	O
(	O
cid:1	O
)	O
:	O
r	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
ln	O
(	O
n	O
(	O
cid:0	O
)	O
r	O
)	O
!	O
r	O
!	O
’	O
(	O
n	O
(	O
cid:0	O
)	O
r	O
)	O
ln	O
ln	O
(	O
cid:18	O
)	O
n	O
n	O
n	O
(	O
cid:0	O
)	O
r	O
(	O
1.13	O
)	O
+	O
r	O
ln	O
n	O
!	O
n	O
r	O
:	O
since	O
all	O
the	O
terms	O
in	O
this	O
equation	O
are	O
logarithms	B
,	O
this	O
result	O
can	O
be	O
rewritten	O
in	O
any	O
base	O
.	O
we	O
will	O
denote	O
natural	B
logarithms	O
(	O
log	O
e	O
)	O
by	O
‘	O
ln	O
’	O
,	O
and	O
logarithms	O
to	O
base	O
2	O
(	O
log2	O
)	O
by	O
‘	O
log	O
’	O
.	O
if	O
we	O
introduce	O
the	O
binary	B
entropy	I
function	I
,	O
recall	O
that	O
log2	O
x	O
=	O
.	O
loge	O
x	O
loge	O
2	O
1	O
loge	O
2	O
note	O
that	O
@	O
log2	O
x	O
@	O
x	O
=	O
1	O
x	O
.	O
h2	O
(	O
x	O
)	O
(	O
cid:17	O
)	O
x	O
log	O
1	O
x	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
x	O
)	O
log	O
1	O
(	O
1	O
(	O
cid:0	O
)	O
x	O
)	O
;	O
then	O
we	O
can	O
rewrite	O
the	O
approximation	B
(	O
1.13	O
)	O
as	O
or	O
,	O
equivalently	O
,	O
log	O
(	O
cid:18	O
)	O
n	O
r	O
(	O
cid:19	O
)	O
’	O
n	O
h2	O
(	O
r=n	O
)	O
;	O
(	O
cid:18	O
)	O
n	O
r	O
(	O
cid:19	O
)	O
’	O
2n	O
h2	O
(	O
r=n	O
)	O
:	O
(	O
1.14	O
)	O
(	O
1.15	O
)	O
(	O
1.16	O
)	O
1	O
h2	O
(	O
x	O
)	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
x	O
if	O
we	O
need	O
a	O
more	O
accurate	O
approximation	B
,	O
we	O
can	O
include	O
terms	O
of	O
the	O
next	O
order	O
from	O
stirling	O
’	O
s	O
approximation	B
(	O
1.12	O
)	O
:	O
figure	O
1.3.	O
the	O
binary	B
entropy	I
function	I
.	O
log	O
(	O
cid:18	O
)	O
n	O
r	O
(	O
cid:19	O
)	O
’	O
n	O
h2	O
(	O
r=n	O
)	O
(	O
cid:0	O
)	O
1	O
2	O
log	O
(	O
cid:20	O
)	O
2	O
(	O
cid:25	O
)	O
n	O
n	O
(	O
cid:0	O
)	O
r	O
n	O
r	O
n	O
(	O
cid:21	O
)	O
:	O
(	O
1.17	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
1	O
introduction	O
to	O
information	B
theory	I
the	O
fundamental	O
problem	O
of	O
communication	O
is	O
that	O
of	O
reproducing	O
at	O
one	O
point	O
either	O
exactly	O
or	O
approximately	O
a	O
message	O
selected	O
at	O
another	O
point	O
.	O
(	O
claude	O
shannon	O
,	O
1948	O
)	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
half	O
of	O
this	O
book	O
we	O
study	O
how	B
to	I
measure	I
information	O
content	B
;	O
we	O
learn	O
how	O
to	O
compress	O
data	O
;	O
and	O
we	O
learn	O
how	O
to	O
communicate	O
perfectly	O
over	O
imperfect	O
communication	B
channels	O
.	O
we	O
start	O
by	O
getting	O
a	O
feeling	O
for	O
this	O
last	O
problem	O
.	O
1.1	O
how	O
can	O
we	O
achieve	O
perfect	B
communication	O
over	O
an	O
imperfect	O
,	O
noisy	B
communication	O
channel	B
?	O
some	O
examples	O
of	O
noisy	O
communication	B
channels	O
are	O
:	O
(	O
cid:15	O
)	O
an	O
analogue	O
telephone	B
line	O
,	O
over	O
which	O
two	O
modems	O
communicate	O
digital	O
information	O
;	O
(	O
cid:15	O
)	O
the	O
radio	B
communication	O
link	O
from	O
galileo	O
,	O
the	O
jupiter-orbiting	O
space-	O
craft	O
,	O
to	O
earth	O
;	O
(	O
cid:15	O
)	O
reproducing	O
cells	O
,	O
in	O
which	O
the	O
daughter	O
cells	O
’	O
dna	O
contains	O
information	B
from	O
the	O
parent	B
cells	O
;	O
(	O
cid:15	O
)	O
a	O
disk	B
drive	I
.	O
the	O
last	O
example	O
shows	O
that	O
communication	B
doesn	O
’	O
t	O
have	O
to	O
involve	O
informa-	O
tion	O
going	O
from	O
one	O
place	O
to	O
another	O
.	O
when	O
we	O
write	O
a	O
(	O
cid:12	O
)	O
le	O
on	O
a	O
disk	B
drive	I
,	O
we	O
’	O
ll	O
read	O
it	O
o	O
(	O
cid:11	O
)	O
in	O
the	O
same	O
location	O
{	O
but	O
at	O
a	O
later	O
time	O
.	O
these	O
channels	O
are	O
noisy	B
.	O
a	O
telephone	B
line	O
su	O
(	O
cid:11	O
)	O
ers	O
from	O
cross-talk	O
with	O
other	O
lines	O
;	O
the	O
hardware	O
in	O
the	O
line	O
distorts	O
and	O
adds	O
noise	B
to	O
the	O
transmitted	O
signal	O
.	O
the	O
deep	O
space	O
network	B
that	O
listens	O
to	O
galileo	O
’	O
s	O
puny	O
transmitter	O
receives	O
background	O
radiation	O
from	O
terrestrial	O
and	O
cosmic	O
sources	O
.	O
dna	O
is	O
subject	O
to	O
mutations	O
and	O
damage	O
.	O
a	O
disk	B
drive	I
,	O
which	O
writes	O
a	O
binary	O
digit	O
(	O
a	O
one	O
or	O
zero	O
,	O
also	O
known	O
as	O
a	O
bit	B
)	O
by	O
aligning	O
a	O
patch	O
of	O
magnetic	O
material	O
in	O
one	O
of	O
two	O
orientations	O
,	O
may	O
later	O
fail	O
to	O
read	O
out	O
the	O
stored	O
binary	O
digit	O
:	O
the	O
patch	O
of	O
material	O
might	O
spontaneously	O
(	O
cid:13	O
)	O
ip	O
magnetization	O
,	O
or	O
a	O
glitch	O
of	O
background	O
noise	B
might	O
cause	O
the	O
reading	O
circuit	O
to	O
report	O
the	O
wrong	O
value	O
for	O
the	O
binary	O
digit	O
,	O
or	O
the	O
writing	B
head	O
might	O
not	O
induce	O
the	O
magnetization	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
place	O
because	O
of	O
interference	O
from	O
neighbouring	O
bits	O
.	O
in	O
all	O
these	O
cases	O
,	O
if	O
we	O
transmit	O
data	O
,	O
e.g.	O
,	O
a	O
string	O
of	O
bits	O
,	O
over	O
the	O
channel	B
,	O
there	O
is	O
some	O
probability	B
that	O
the	O
received	O
message	O
will	O
not	O
be	O
identical	O
to	O
the	O
3	O
modem	O
-	O
phone	B
-	O
line	O
modem	O
galileo	O
-	O
-	O
radio	B
waves	O
earth	O
parent	B
cell	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
@	O
r	O
daughter	O
cell	O
daughter	O
cell	O
computer	B
memory	O
-	O
disk	B
drive	I
-	O
computer	B
memory	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
4	O
1	O
|	O
introduction	O
to	O
information	B
theory	I
transmitted	O
message	O
.	O
we	O
would	O
prefer	O
to	O
have	O
a	O
communication	B
channel	O
for	O
which	O
this	O
probability	B
was	O
zero	O
{	O
or	O
so	O
close	O
to	O
zero	O
that	O
for	O
practical	O
purposes	O
it	O
is	O
indistinguishable	O
from	O
zero	O
.	O
let	O
’	O
s	O
consider	O
a	O
noisy	B
disk	O
drive	O
that	O
transmits	O
each	O
bit	B
correctly	O
with	O
probability	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
and	O
incorrectly	O
with	O
probability	O
f	O
.	O
this	O
model	B
communi-	O
cation	O
channel	B
is	O
known	O
as	O
the	O
binary	B
symmetric	I
channel	I
(	O
(	O
cid:12	O
)	O
gure	O
1.4	O
)	O
.	O
0	O
-	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
@	O
r	O
x	O
-	O
1	O
0	O
1	O
y	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
0	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
f	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
0	O
)	O
=	O
f	O
;	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
1	O
)	O
=	O
f	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
1	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
f	O
:	O
0	O
1	O
@	O
(	O
cid:0	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
-	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
f	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
0	O
1	O
figure	O
1.4.	O
the	O
binary	B
symmetric	I
channel	I
.	O
the	O
transmitted	O
symbol	O
is	O
x	O
and	O
the	O
received	O
symbol	O
y.	O
the	O
noise	B
level	O
,	O
the	O
probability	B
that	O
a	O
bit	B
is	O
(	O
cid:13	O
)	O
ipped	O
,	O
is	O
f	O
.	O
figure	O
1.5.	O
a	O
binary	O
data	O
sequence	B
of	O
length	B
10	O
000	O
transmitted	O
over	O
a	O
binary	B
symmetric	I
channel	I
with	O
noise	B
level	O
f	O
=	O
0:1	O
.	O
[	O
dilbert	O
image	B
copyright	O
c	O
(	O
cid:13	O
)	O
1997	O
united	O
feature	O
syndicate	O
,	O
inc.	O
,	O
used	O
with	O
permission	O
.	O
]	O
as	O
an	O
example	O
,	O
let	O
’	O
s	O
imagine	O
that	O
f	O
=	O
0:1	O
,	O
that	O
is	O
,	O
ten	O
per	O
cent	O
of	O
the	O
bits	O
are	O
(	O
cid:13	O
)	O
ipped	O
(	O
(	O
cid:12	O
)	O
gure	O
1.5	O
)	O
.	O
a	O
useful	O
disk	B
drive	I
would	O
(	O
cid:13	O
)	O
ip	O
no	O
bits	O
at	O
all	O
in	O
its	O
entire	O
lifetime	O
.	O
if	O
we	O
expect	O
to	O
read	O
and	O
write	O
a	O
gigabyte	O
per	O
day	O
for	O
ten	O
years	O
,	O
we	O
require	O
a	O
bit	B
error	O
probability	O
of	O
the	O
order	O
of	O
10	O
(	O
cid:0	O
)	O
15	O
,	O
or	O
smaller	O
.	O
there	O
are	O
two	O
approaches	O
to	O
this	O
goal	O
.	O
the	O
physical	O
solution	O
the	O
physical	O
solution	O
is	O
to	O
improve	O
the	O
physical	O
characteristics	O
of	O
the	O
commu-	O
nication	O
channel	B
to	O
reduce	O
its	O
error	B
probability	I
.	O
we	O
could	O
improve	O
our	O
disk	B
drive	I
by	O
1.	O
using	O
more	O
reliable	O
components	O
in	O
its	O
circuitry	O
;	O
2.	O
evacuating	O
the	O
air	O
from	O
the	O
disk	O
enclosure	O
so	O
as	O
to	O
eliminate	O
the	O
turbu-	O
lence	O
that	O
perturbs	O
the	O
reading	O
head	O
from	O
the	O
track	O
;	O
3.	O
using	O
a	O
larger	O
magnetic	O
patch	O
to	O
represent	O
each	O
bit	B
;	O
or	O
4.	O
using	O
higher-power	O
signals	O
or	O
cooling	O
the	O
circuitry	O
in	O
order	O
to	O
reduce	O
thermal	O
noise	O
.	O
these	O
physical	O
modi	O
(	O
cid:12	O
)	O
cations	O
typically	O
increase	O
the	O
cost	O
of	O
the	O
communication	B
channel	O
.	O
the	O
‘	O
system	O
’	O
solution	O
information	B
theory	I
and	O
coding	B
theory	I
o	O
(	O
cid:11	O
)	O
er	O
an	O
alternative	O
(	O
and	O
much	O
more	O
ex-	O
citing	O
)	O
approach	O
:	O
we	O
accept	O
the	O
given	O
noisy	B
channel	I
as	O
it	O
is	O
and	O
add	O
communi-	O
cation	O
systems	O
to	O
it	O
so	O
that	O
we	O
can	O
detect	O
and	O
correct	O
the	O
errors	B
introduced	O
by	O
the	O
channel	B
.	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
1.6	O
,	O
we	O
add	O
an	O
encoder	B
before	O
the	O
channel	B
and	O
a	O
decoder	B
after	O
it	O
.	O
the	O
encoder	B
encodes	O
the	O
source	O
message	O
s	O
into	O
a	O
transmit-	O
ted	O
message	O
t	O
,	O
adding	O
redundancy	B
to	O
the	O
original	O
message	O
in	O
some	O
way	O
.	O
the	O
channel	B
adds	O
noise	B
to	O
the	O
transmitted	O
message	O
,	O
yielding	O
a	O
received	O
message	O
r.	O
the	O
decoder	B
uses	O
the	O
known	O
redundancy	B
introduced	O
by	O
the	O
encoding	O
system	O
to	O
infer	O
both	O
the	O
original	O
signal	O
s	O
and	O
the	O
added	O
noise	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
1.2	O
:	O
error-correcting	B
codes	I
for	O
the	O
binary	B
symmetric	I
channel	I
5	O
source	O
s	O
?	O
encoder	B
t	O
-	O
6	O
^s	O
decoder	B
6	O
r	O
noisy	B
channel	I
figure	O
1.6.	O
the	O
‘	O
system	O
’	O
solution	O
for	O
achieving	O
reliable	O
communication	B
over	O
a	O
noisy	B
channel	I
.	O
the	O
encoding	O
system	O
introduces	O
systematic	B
redundancy	O
into	O
the	O
transmitted	O
vector	O
t.	O
the	O
decoding	B
system	O
uses	O
this	O
known	O
redundancy	B
to	O
deduce	O
from	O
the	O
received	O
vector	O
r	O
both	O
the	O
original	O
source	O
vector	O
and	O
the	O
noise	B
introduced	O
by	O
the	O
channel	B
.	O
whereas	O
physical	O
solutions	O
give	O
incremental	O
channel	B
improvements	O
only	O
at	O
an	O
ever-increasing	O
cost	O
,	O
system	O
solutions	O
can	O
turn	O
noisy	B
channels	O
into	O
reliable	O
communication	B
channels	O
with	O
the	O
only	O
cost	O
being	O
a	O
computational	O
requirement	O
at	O
the	O
encoder	B
and	O
decoder	B
.	O
information	B
theory	I
is	O
concerned	O
with	O
the	O
theoretical	O
limitations	O
and	O
po-	O
‘	O
what	O
is	O
the	O
best	O
error-correcting	O
performance	O
we	O
tentials	O
of	O
such	O
systems	O
.	O
could	O
achieve	O
?	O
’	O
coding	B
theory	I
is	O
concerned	O
with	O
the	O
creation	O
of	O
practical	O
encoding	O
and	O
decoding	O
systems	O
.	O
1.2	O
error-correcting	B
codes	I
for	O
the	O
binary	B
symmetric	I
channel	I
we	O
now	O
consider	O
examples	O
of	O
encoding	O
and	O
decoding	O
systems	O
.	O
what	O
is	O
the	O
simplest	O
way	O
to	O
add	O
useful	O
redundancy	B
to	O
a	O
transmission	O
?	O
[	O
to	O
make	O
the	O
rules	B
of	O
the	O
game	B
clear	O
:	O
we	O
want	O
to	O
be	O
able	O
to	O
detect	O
and	O
correct	O
errors	B
;	O
and	O
re-	O
transmission	O
is	O
not	O
an	O
option	O
.	O
we	O
get	O
only	O
one	O
chance	O
to	O
encode	O
,	O
transmit	O
,	O
and	O
decode	O
.	O
]	O
repetition	B
codes	O
a	O
straightforward	O
idea	O
is	O
to	O
repeat	O
every	O
bit	B
of	O
the	O
message	O
a	O
prearranged	O
number	O
of	O
times	O
{	O
for	O
example	O
,	O
three	O
times	O
,	O
as	O
shown	O
in	O
table	O
1.7.	O
we	O
call	O
this	O
repetition	B
code	I
‘	O
r3	O
’	O
.	O
imagine	O
that	O
we	O
transmit	O
the	O
source	O
message	O
s	O
=	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
over	O
a	O
binary	B
symmetric	I
channel	I
with	O
noise	B
level	O
f	O
=	O
0:1	O
using	O
this	O
repetition	B
code	I
.	O
we	O
can	O
describe	O
the	O
channel	B
as	O
‘	O
adding	O
’	O
a	O
sparse	O
noise	O
vector	O
n	O
to	O
the	O
transmitted	O
vector	O
{	O
adding	O
in	O
modulo	O
2	O
arithmetic	O
,	O
i.e.	O
,	O
the	O
binary	O
algebra	O
in	O
which	O
1+1=0	O
.	O
a	O
possible	O
noise	B
vector	O
n	O
and	O
received	O
vector	O
r	O
=	O
t	O
+	O
n	O
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
1.8.	O
source	O
sequence	O
transmitted	O
sequence	B
s	O
0	O
1	O
t	O
000	O
111	O
table	O
1.7.	O
the	O
repetition	B
code	I
r3	O
.	O
s	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
z	O
}	O
|	O
{	O
0	O
0	O
0	O
z	O
}	O
|	O
{	O
0	O
0	O
0	O
z	O
}	O
|	O
{	O
1	O
1	O
1	O
z	O
}	O
|	O
{	O
0	O
0	O
0	O
z	O
}	O
|	O
{	O
1	O
1	O
1	O
z	O
}	O
|	O
{	O
1	O
1	O
1	O
z	O
}	O
|	O
{	O
0	O
0	O
0	O
t	O
n	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
r	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
1	O
0	O
0	O
0	O
figure	O
1.8.	O
an	O
example	O
transmission	O
using	O
r3	O
.	O
how	O
should	O
we	O
decode	O
this	O
received	O
vector	O
?	O
the	O
optimal	B
algorithm	O
looks	O
at	O
the	O
received	O
bits	O
three	O
at	O
a	O
time	O
and	O
takes	O
a	O
majority	B
vote	I
(	O
algorithm	B
1.9	O
)	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
6	O
1	O
|	O
introduction	O
to	O
information	B
theory	I
received	O
sequence	B
r	O
likelihood	B
ratio	O
p	O
(	O
rj	O
s	O
=	O
1	O
)	O
p	O
(	O
rj	O
s	O
=	O
0	O
)	O
decoded	O
sequence	B
^s	O
000	O
001	O
010	O
100	O
101	O
110	O
011	O
111	O
(	O
cid:13	O
)	O
(	O
cid:0	O
)	O
3	O
(	O
cid:13	O
)	O
(	O
cid:0	O
)	O
1	O
(	O
cid:13	O
)	O
(	O
cid:0	O
)	O
1	O
(	O
cid:13	O
)	O
(	O
cid:0	O
)	O
1	O
(	O
cid:13	O
)	O
1	O
(	O
cid:13	O
)	O
1	O
(	O
cid:13	O
)	O
1	O
(	O
cid:13	O
)	O
3	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
algorithm	B
1.9.	O
majority-vote	O
decoding	B
algorithm	O
for	O
r3	O
.	O
also	O
shown	O
are	O
the	O
likelihood	B
ratios	O
(	O
1.23	O
)	O
,	O
assuming	O
the	O
channel	B
is	O
a	O
binary	B
symmetric	I
channel	I
;	O
(	O
cid:13	O
)	O
(	O
cid:17	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
=f	O
.	O
at	O
the	O
risk	O
of	O
explaining	O
the	O
obvious	O
,	O
let	O
’	O
s	O
prove	O
this	O
result	O
.	O
the	O
optimal	B
decoding	O
decision	O
(	O
optimal	B
in	O
the	O
sense	O
of	O
having	O
the	O
smallest	O
probability	O
of	O
being	O
wrong	O
)	O
is	O
to	O
(	O
cid:12	O
)	O
nd	O
which	O
value	O
of	O
s	O
is	O
most	O
probable	O
,	O
given	O
r.	O
consider	O
the	O
decoding	B
of	O
a	O
single	O
bit	O
s	O
,	O
which	O
was	O
encoded	O
as	O
t	O
(	O
s	O
)	O
and	O
gave	O
rise	O
to	O
three	O
received	O
bits	O
r	O
=	O
r1r2r3	O
.	O
by	O
bayes	O
’	O
theorem	B
,	O
the	O
posterior	B
probability	I
of	O
s	O
is	O
p	O
(	O
sj	O
r1r2r3	O
)	O
=	O
p	O
(	O
r1r2r3	O
j	O
s	O
)	O
p	O
(	O
s	O
)	O
p	O
(	O
r1r2r3	O
)	O
:	O
we	O
can	O
spell	B
out	O
the	O
posterior	B
probability	I
of	O
the	O
two	O
alternatives	O
thus	O
:	O
p	O
(	O
s	O
=	O
1j	O
r1r2r3	O
)	O
=	O
p	O
(	O
r1r2r3	O
j	O
s	O
=	O
1	O
)	O
p	O
(	O
s	O
=	O
1	O
)	O
p	O
(	O
r1r2r3	O
)	O
p	O
(	O
s	O
=	O
0j	O
r1r2r3	O
)	O
=	O
p	O
(	O
r1r2r3	O
j	O
s	O
=	O
0	O
)	O
p	O
(	O
s	O
=	O
0	O
)	O
p	O
(	O
r1r2r3	O
)	O
;	O
:	O
(	O
1.18	O
)	O
(	O
1.19	O
)	O
(	O
1.20	O
)	O
this	O
posterior	B
probability	I
is	O
determined	O
by	O
two	O
factors	O
:	O
the	O
prior	B
probability	O
p	O
(	O
s	O
)	O
,	O
and	O
the	O
data-dependent	O
term	O
p	O
(	O
r1r2r3	O
j	O
s	O
)	O
,	O
which	O
is	O
called	O
the	O
likelihood	B
of	O
s.	O
the	O
normalizing	B
constant	I
p	O
(	O
r1r2r3	O
)	O
needn	O
’	O
t	O
be	O
computed	O
when	O
(	O
cid:12	O
)	O
nding	O
the	O
optimal	B
decoding	O
decision	O
,	O
which	O
is	O
to	O
guess	O
^s	O
=	O
0	O
if	O
p	O
(	O
s	O
=	O
0j	O
r	O
)	O
>	O
p	O
(	O
s	O
=	O
1j	O
r	O
)	O
,	O
and	O
^s	O
=	O
1	O
otherwise	O
.	O
to	O
(	O
cid:12	O
)	O
nd	O
p	O
(	O
s	O
=	O
0j	O
r	O
)	O
and	O
p	O
(	O
s	O
=	O
1j	O
r	O
)	O
,	O
we	O
must	O
make	O
an	O
assumption	O
about	O
the	O
prior	B
probabilities	O
of	O
the	O
two	O
hypotheses	O
s	O
=	O
0	O
and	O
s	O
=	O
1	O
,	O
and	O
we	O
must	O
make	O
an	O
assumption	O
about	O
the	O
probability	O
of	O
r	O
given	O
s.	O
we	O
assume	O
that	O
the	O
prior	B
prob-	O
abilities	O
are	O
equal	O
:	O
p	O
(	O
s	O
=	O
0	O
)	O
=	O
p	O
(	O
s	O
=	O
1	O
)	O
=	O
0:5	O
;	O
then	O
maximizing	O
the	O
posterior	B
probability	I
p	O
(	O
sj	O
r	O
)	O
is	O
equivalent	O
to	O
maximizing	O
the	O
likelihood	B
p	O
(	O
rj	O
s	O
)	O
.	O
and	O
we	O
assume	O
that	O
the	O
channel	B
is	O
a	O
binary	B
symmetric	I
channel	I
with	O
noise	B
level	O
f	O
<	O
0:5	O
,	O
so	O
that	O
the	O
likelihood	B
is	O
p	O
(	O
rj	O
s	O
)	O
=	O
p	O
(	O
rj	O
t	O
(	O
s	O
)	O
)	O
=	O
n	O
yn=1	O
p	O
(	O
rn	O
j	O
tn	O
(	O
s	O
)	O
)	O
;	O
(	O
1.21	O
)	O
where	O
n	O
=	O
3	O
is	O
the	O
number	O
of	O
transmitted	O
bits	O
in	O
the	O
block	B
we	O
are	O
considering	O
,	O
and	O
p	O
(	O
rn	O
j	O
tn	O
)	O
=	O
(	O
cid:26	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
f	O
if	O
if	O
rn	O
=	O
tn	O
rn	O
6=	O
tn	O
:	O
(	O
1.22	O
)	O
thus	O
the	O
likelihood	B
ratio	O
for	O
the	O
two	O
hypotheses	O
is	O
n	O
yn=1	O
=	O
p	O
(	O
rj	O
s	O
=	O
1	O
)	O
p	O
(	O
rj	O
s	O
=	O
0	O
)	O
p	O
(	O
rnjtn	O
(	O
0	O
)	O
)	O
equals	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
each	O
factor	O
p	O
(	O
rnjtn	O
(	O
1	O
)	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
if	O
rn	O
=	O
0.	O
the	O
ratio	O
(	O
cid:13	O
)	O
(	O
cid:17	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
is	O
greater	O
than	O
1	O
,	O
since	O
f	O
<	O
0:5	O
,	O
so	O
the	O
winning	O
hypothesis	O
is	O
the	O
one	O
with	O
the	O
most	O
‘	O
votes	O
’	O
,	O
each	O
vote	O
counting	B
for	O
a	O
factor	O
of	O
(	O
cid:13	O
)	O
in	O
the	O
likelihood	B
ratio	O
.	O
p	O
(	O
rn	O
j	O
tn	O
(	O
1	O
)	O
)	O
p	O
(	O
rn	O
j	O
tn	O
(	O
0	O
)	O
)	O
if	O
rn	O
=	O
1	O
and	O
(	O
1.23	O
)	O
;	O
f	O
f	O
f	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
1.2	O
:	O
error-correcting	B
codes	I
for	O
the	O
binary	B
symmetric	I
channel	I
7	O
thus	O
the	O
majority-vote	O
decoder	B
shown	O
in	O
algorithm	O
1.9	O
is	O
the	O
optimal	B
decoder	I
if	O
we	O
assume	O
that	O
the	O
channel	B
is	O
a	O
binary	B
symmetric	I
channel	I
and	O
that	O
the	O
two	O
possible	O
source	O
messages	O
0	O
and	O
1	O
have	O
equal	O
prior	B
probability	O
.	O
we	O
now	O
apply	O
the	O
majority	B
vote	I
decoder	O
to	O
the	O
received	O
vector	O
of	O
(	O
cid:12	O
)	O
gure	O
1.8.	O
the	O
(	O
cid:12	O
)	O
rst	O
three	O
received	O
bits	O
are	O
all	O
0	O
,	O
so	O
we	O
decode	O
this	O
triplet	O
as	O
a	O
0.	O
in	O
the	O
second	O
triplet	O
of	O
(	O
cid:12	O
)	O
gure	O
1.8	O
,	O
there	O
are	O
two	O
0s	O
and	O
one	O
1	O
,	O
so	O
we	O
decode	O
this	O
triplet	O
as	O
a	O
0	O
{	O
which	O
in	O
this	O
case	O
corrects	O
the	O
error	O
.	O
not	O
all	O
errors	B
are	O
corrected	O
,	O
however	O
.	O
if	O
we	O
are	O
unlucky	O
and	O
two	O
errors	B
fall	O
in	O
a	O
single	O
block	O
,	O
as	O
in	O
the	O
(	O
cid:12	O
)	O
fth	O
triplet	O
of	O
(	O
cid:12	O
)	O
gure	O
1.8	O
,	O
then	O
the	O
decoding	B
rule	O
gets	O
the	O
wrong	O
answer	O
,	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
1.10.	O
s	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
figure	O
1.10.	O
decoding	B
the	O
received	O
vector	O
from	O
(	O
cid:12	O
)	O
gure	O
1.8.	O
t	O
n	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
r	O
0	O
0	O
0	O
0	O
0	O
0	O
z	O
}	O
|	O
{	O
0	O
0	O
0	O
z	O
}	O
|	O
{	O
0	O
0	O
0	O
z	O
}	O
|	O
{	O
1	O
1	O
1	O
z	O
}	O
|	O
{	O
0	O
0	O
0	O
z	O
}	O
|	O
{	O
1	O
1	O
1	O
z	O
}	O
|	O
{	O
1	O
1	O
1	O
z	O
}	O
|	O
{	O
0	O
0	O
0	O
|	O
{	O
z	O
}	O
|	O
{	O
z	O
}	O
|	O
{	O
z	O
}	O
|	O
{	O
z	O
}	O
|	O
{	O
z	O
}	O
0	O
?	O
1	O
|	O
{	O
z	O
}	O
1	O
1	O
1	O
1	O
|	O
{	O
z	O
}	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
1	O
0	O
^s	O
corrected	O
errors	B
undetected	O
errors	B
0	O
0	O
0	O
?	O
exercise	O
1.2	O
.	O
[	O
2	O
,	O
p.16	O
]	O
show	O
that	O
the	O
error	B
probability	I
is	O
reduced	O
by	O
the	O
use	O
of	O
r3	O
by	O
computing	O
the	O
error	B
probability	I
of	O
this	O
code	B
for	O
a	O
binary	B
symmetric	I
channel	I
with	O
noise	B
level	O
f	O
.	O
the	O
error	B
probability	I
is	O
dominated	O
by	O
the	O
probability	B
that	O
two	O
bits	O
in	O
a	O
block	B
of	O
three	O
are	O
(	O
cid:13	O
)	O
ipped	O
,	O
which	O
scales	O
as	O
f	O
2.	O
in	O
the	O
case	O
of	O
the	O
binary	O
symmetric	B
channel	I
with	O
f	O
=	O
0:1	O
,	O
the	O
r3	O
code	B
has	O
a	O
probability	B
of	I
error	I
,	O
after	O
decoding	B
,	O
of	O
pb	O
’	O
0:03	O
per	O
bit	B
.	O
figure	O
1.11	O
shows	O
the	O
result	O
of	O
transmitting	O
a	O
binary	O
image	O
over	O
a	O
binary	B
symmetric	I
channel	I
using	O
the	O
repetition	B
code	I
.	O
the	O
exercise	O
’	O
s	O
rating	O
,	O
e.g.	O
‘	O
[	O
2	O
]	O
’	O
,	O
indicates	O
its	O
di	O
(	O
cid:14	O
)	O
culty	O
:	O
‘	O
1	O
’	O
exercises	O
are	O
the	O
easiest	O
.	O
exercises	O
that	O
are	O
accompanied	O
by	O
a	O
marginal	B
rat	O
are	O
especially	O
recommended	O
.	O
if	O
a	O
solution	O
or	O
partial	B
solution	O
is	O
provided	O
,	O
the	O
page	O
is	O
indicated	O
after	O
the	O
di	O
(	O
cid:14	O
)	O
culty	O
rating	O
;	O
for	O
example	O
,	O
this	O
exercise	O
’	O
s	O
solution	O
is	O
on	O
page	O
16.	O
s	O
encoder	B
t	O
-	O
channel	B
f	O
=	O
10	O
%	O
-	O
r	O
decoder	B
^s	O
-	O
figure	O
1.11.	O
transmitting	O
10	O
000	O
source	O
bits	O
over	O
a	O
binary	B
symmetric	I
channel	I
with	O
f	O
=	O
10	O
%	O
using	O
a	O
repetition	B
code	I
and	O
the	O
majority	B
vote	I
decoding	O
algorithm	B
.	O
the	O
probability	O
of	O
decoded	O
bit	B
error	O
has	O
fallen	O
to	O
about	O
3	O
%	O
;	O
the	O
rate	B
has	O
fallen	O
to	O
1/3	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
8	O
0.1	O
0.08	O
0.06	O
0.04	O
0.02	O
0	O
1	O
|	O
introduction	O
to	O
information	B
theory	I
r1	O
0.1	O
0.01	O
r5	O
r3	O
r1	O
more	O
useful	O
codes	O
1e-05	O
pb	O
1e-10	O
figure	O
1.12.	O
error	B
probability	I
pb	O
versus	O
rate	B
for	O
repetition	B
codes	O
over	O
a	O
binary	B
symmetric	I
channel	I
with	O
f	O
=	O
0:1.	O
the	O
right-hand	O
(	O
cid:12	O
)	O
gure	O
shows	O
pb	O
on	O
a	O
logarithmic	O
scale	O
.	O
we	O
would	O
like	O
the	O
rate	B
to	O
be	O
large	O
and	O
pb	O
to	O
be	O
small	O
.	O
r3	O
r5	O
r61	O
more	O
useful	O
codes	O
r61	O
1e-15	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
rate	B
rate	O
the	O
repetition	B
code	I
r3	O
has	O
therefore	O
reduced	O
the	O
probability	B
of	I
error	I
,	O
as	O
desired	O
.	O
yet	O
we	O
have	O
lost	O
something	O
:	O
our	O
rate	B
of	O
information	B
transfer	O
has	O
fallen	O
by	O
a	O
factor	O
of	O
three	O
.	O
so	O
if	O
we	O
use	O
a	O
repetition	B
code	I
to	O
communicate	O
data	O
over	O
a	O
telephone	B
line	O
,	O
it	O
will	O
reduce	O
the	O
error	O
frequency	O
,	O
but	O
it	O
will	O
also	O
reduce	O
our	O
communication	B
rate	O
.	O
we	O
will	O
have	O
to	O
pay	O
three	O
times	O
as	O
much	O
for	O
each	O
phone	B
call	O
.	O
similarly	O
,	O
we	O
would	O
need	O
three	O
of	O
the	O
original	O
noisy	B
gigabyte	O
disk	O
drives	O
in	O
order	O
to	O
create	O
a	O
one-gigabyte	O
disk	B
drive	I
with	O
pb	O
=	O
0:03.	O
can	O
we	O
push	O
the	O
error	B
probability	I
lower	O
,	O
to	O
the	O
values	O
required	O
for	O
a	O
sell-	O
able	O
disk	B
drive	I
{	O
10	O
(	O
cid:0	O
)	O
15	O
?	O
we	O
could	O
achieve	O
lower	O
error	O
probabilities	O
by	O
using	O
repetition	B
codes	O
with	O
more	O
repetitions	O
.	O
exercise	O
1.3	O
.	O
[	O
3	O
,	O
p.16	O
]	O
(	O
a	O
)	O
show	O
that	O
the	O
probability	B
of	I
error	I
of	O
rn	O
,	O
the	O
repe-	O
tition	O
code	B
with	O
n	O
repetitions	O
,	O
is	O
pb	O
=	O
for	O
odd	O
n	O
.	O
n	O
xn=	O
(	O
n	O
+1	O
)	O
=2	O
(	O
cid:18	O
)	O
n	O
n	O
(	O
cid:19	O
)	O
f	O
n	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
n	O
(	O
cid:0	O
)	O
n	O
;	O
(	O
1.24	O
)	O
(	O
b	O
)	O
assuming	O
f	O
=	O
0:1	O
,	O
which	O
of	O
the	O
terms	O
in	O
this	O
sum	O
is	O
the	O
biggest	O
?	O
how	O
much	O
bigger	O
is	O
it	O
than	O
the	O
second-biggest	O
term	O
?	O
(	O
c	O
)	O
use	O
stirling	O
’	O
s	O
approximation	B
(	O
p.2	O
)	O
to	O
approximate	O
the	O
(	O
cid:0	O
)	O
n	O
largest	O
term	O
,	O
and	O
(	O
cid:12	O
)	O
nd	O
,	O
approximately	O
,	O
the	O
probability	B
of	I
error	I
of	O
the	O
repetition	B
code	I
with	O
n	O
repetitions	O
.	O
n	O
(	O
cid:1	O
)	O
in	O
the	O
(	O
d	O
)	O
assuming	O
f	O
=	O
0:1	O
,	O
(	O
cid:12	O
)	O
nd	O
how	O
many	O
repetitions	O
are	O
required	O
to	O
get	O
the	O
probability	B
of	I
error	I
down	O
to	O
10	O
(	O
cid:0	O
)	O
15	O
.	O
[	O
answer	O
:	O
about	O
60	O
.	O
]	O
so	O
to	O
build	O
a	O
single	O
gigabyte	O
disk	B
drive	I
with	O
the	O
required	O
reliability	O
from	O
noisy	B
gigabyte	O
drives	O
with	O
f	O
=	O
0:1	O
,	O
we	O
would	O
need	O
sixty	O
of	O
the	O
noisy	O
disk	O
drives	O
.	O
the	O
tradeo	O
(	O
cid:11	O
)	O
between	O
error	B
probability	I
and	O
rate	B
for	O
repetition	B
codes	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
1.12.	O
block	B
codes	O
{	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
we	O
would	O
like	O
to	O
communicate	O
with	O
tiny	O
probability	B
of	I
error	I
and	O
at	O
a	O
substan-	O
tial	O
rate	B
.	O
can	O
we	O
improve	O
on	O
repetition	O
codes	O
?	O
what	O
if	O
we	O
add	O
redundancy	B
to	O
blocks	O
of	O
data	O
instead	O
of	O
encoding	O
one	O
bit	B
at	O
a	O
time	O
?	O
we	O
now	O
study	O
a	O
simple	O
block	O
code	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
1.2	O
:	O
error-correcting	B
codes	I
for	O
the	O
binary	B
symmetric	I
channel	I
9	O
a	O
block	B
code	I
is	O
a	O
rule	O
for	O
converting	O
a	O
sequence	B
of	O
source	O
bits	O
s	O
,	O
of	O
length	O
k	O
,	O
say	O
,	O
into	O
a	O
transmitted	O
sequence	B
t	O
of	O
length	O
n	O
bits	O
.	O
to	O
add	O
redundancy	B
,	O
we	O
make	O
n	O
greater	O
than	O
k.	O
in	O
a	O
linear	B
block	I
code	I
,	O
the	O
extra	O
n	O
(	O
cid:0	O
)	O
k	O
bits	O
are	O
linear	B
functions	O
of	O
the	O
original	O
k	O
bits	O
;	O
these	O
extra	O
bits	O
are	O
called	O
parity-check	B
bits	I
.	O
an	O
example	O
of	O
a	O
linear	B
block	I
code	I
is	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
,	O
which	O
transmits	O
n	O
=	O
7	O
bits	O
for	O
every	O
k	O
=	O
4	O
source	O
bits	O
.	O
t	O
5	O
s	O
3	O
4s	O
s	O
2	O
t	O
6	O
s	O
1	O
t	O
7	O
1	O
0	O
0	O
1	O
1	O
0	O
0	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
1.13.	O
pictorial	O
representation	O
of	O
encoding	O
for	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
.	O
the	O
encoding	O
operation	O
for	O
the	O
code	B
is	O
shown	O
pictorially	O
in	O
(	O
cid:12	O
)	O
gure	O
1.13.	O
we	O
arrange	O
the	O
seven	O
transmitted	O
bits	O
in	O
three	O
intersecting	O
circles	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
four	O
transmitted	O
bits	O
,	O
t1t2t3t4	O
,	O
are	O
set	B
equal	O
to	O
the	O
four	O
source	O
bits	O
,	O
s1s2s3s4	O
.	O
the	O
parity-check	B
bits	I
t5t6t7	O
are	O
set	B
so	O
that	O
the	O
parity	B
within	O
each	O
circle	B
is	O
even	O
:	O
the	O
(	O
cid:12	O
)	O
rst	O
parity-check	O
bit	O
is	O
the	O
parity	B
of	O
the	O
(	O
cid:12	O
)	O
rst	O
three	O
source	O
bits	O
(	O
that	O
is	O
,	O
it	O
is	O
0	O
if	O
the	O
sum	O
of	O
those	O
bits	O
is	O
even	O
,	O
and	O
1	O
if	O
the	O
sum	O
is	O
odd	O
)	O
;	O
the	O
second	O
is	O
the	O
parity	B
of	O
the	O
last	O
three	O
;	O
and	O
the	O
third	O
parity	O
bit	B
is	O
the	O
parity	B
of	O
source	O
bits	O
one	O
,	O
three	O
and	O
four	O
.	O
as	O
an	O
example	O
,	O
(	O
cid:12	O
)	O
gure	O
1.13b	O
shows	O
the	O
transmitted	O
codeword	B
for	O
the	O
case	O
s	O
=	O
1000.	O
table	O
1.14	O
shows	O
the	O
codewords	O
generated	O
by	O
each	O
of	O
the	O
24	O
=	O
sixteen	O
settings	O
of	O
the	O
four	O
source	O
bits	O
.	O
these	O
codewords	O
have	O
the	O
special	O
property	O
that	O
any	O
pair	O
di	O
(	O
cid:11	O
)	O
er	O
from	O
each	O
other	O
in	O
at	O
least	O
three	O
bits	O
.	O
s	O
0000	O
0001	O
0010	O
0011	O
t	O
0000000	O
0001011	O
0010111	O
0011100	O
s	O
0100	O
0101	O
0110	O
0111	O
t	O
0100110	O
0101101	O
0110001	O
0111010	O
s	O
1000	O
1001	O
1010	O
1011	O
t	O
1000101	O
1001110	O
1010010	O
1011001	O
s	O
1100	O
1101	O
1110	O
1111	O
t	O
1100011	O
1101000	O
1110100	O
1111111	O
table	O
1.14.	O
the	O
sixteen	O
codewords	O
ftg	O
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
.	O
any	O
pair	O
of	O
codewords	O
di	O
(	O
cid:11	O
)	O
er	O
from	O
each	O
other	O
in	O
at	O
least	O
three	O
bits	O
.	O
because	O
the	O
hamming	O
code	B
is	O
a	O
linear	B
code	O
,	O
it	O
can	O
be	O
written	O
compactly	O
in	O
terms	O
of	O
matrices	O
as	O
follows	O
.	O
the	O
transmitted	O
codeword	B
t	O
is	O
obtained	O
from	O
the	O
source	O
sequence	O
s	O
by	O
a	O
linear	B
operation	O
,	O
t	O
=	O
gts	O
;	O
(	O
1.25	O
)	O
where	O
g	O
is	O
the	O
generator	B
matrix	I
of	O
the	O
code	B
,	O
2	O
666666664	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
1	O
1	O
3	O
777777775	O
gt	O
=	O
;	O
(	O
1.26	O
)	O
and	O
the	O
encoding	O
operation	O
(	O
1.25	O
)	O
uses	O
modulo-2	O
arithmetic	O
(	O
1	O
+	O
1	O
=	O
0	O
,	O
0	O
+	O
1	O
=	O
1	O
,	O
etc.	O
)	O
.	O
in	O
the	O
encoding	O
operation	O
(	O
1.25	O
)	O
i	O
have	O
assumed	O
that	O
s	O
and	O
t	O
are	O
column	O
vectors	B
.	O
if	O
instead	O
they	O
are	O
row	O
vectors	B
,	O
then	O
this	O
equation	O
is	O
replaced	O
by	O
t	O
=	O
sg	O
;	O
(	O
1.27	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
10	O
where	O
1	O
|	O
introduction	O
to	O
information	B
theory	I
:	O
(	O
1.28	O
)	O
g	O
=2	O
664	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
3	O
775	O
i	O
(	O
cid:12	O
)	O
nd	O
it	O
easier	O
to	O
relate	O
to	O
the	O
right-multiplication	O
(	O
1.25	O
)	O
than	O
the	O
left-multiplica-	O
tion	O
(	O
1.27	O
)	O
.	O
many	O
coding	B
theory	I
texts	O
use	O
the	O
left-multiplying	O
conventions	B
(	O
1.27	O
{	O
1.28	O
)	O
,	O
however	O
.	O
the	O
rows	O
of	O
the	O
generator	O
matrix	B
(	O
1.28	O
)	O
can	O
be	O
viewed	O
as	O
de	O
(	O
cid:12	O
)	O
ning	O
four	O
basis	O
vectors	O
lying	O
in	O
a	O
seven-dimensional	O
binary	O
space	O
.	O
the	O
sixteen	O
codewords	O
are	O
obtained	O
by	O
making	O
all	O
possible	O
linear	B
combinations	O
of	O
these	O
vectors	B
.	O
decoding	B
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
when	O
we	O
invent	O
a	O
more	O
complex	B
encoder	O
s	O
!	O
t	O
,	O
the	O
task	O
of	O
decoding	O
the	O
received	O
vector	O
r	O
becomes	O
less	O
straightforward	O
.	O
remember	O
that	O
any	O
of	O
the	O
bits	O
may	O
have	O
been	O
(	O
cid:13	O
)	O
ipped	O
,	O
including	O
the	O
parity	B
bits	O
.	O
if	O
we	O
assume	O
that	O
the	O
channel	B
is	O
a	O
binary	B
symmetric	I
channel	I
and	O
that	O
all	O
source	O
vectors	O
are	O
equiprobable	O
,	O
then	O
the	O
optimal	B
decoder	I
identi	O
(	O
cid:12	O
)	O
es	O
the	O
source	O
vector	O
s	O
whose	O
encoding	O
t	O
(	O
s	O
)	O
di	O
(	O
cid:11	O
)	O
ers	O
from	O
the	O
received	O
vector	O
r	O
in	O
the	O
fewest	O
bits	O
.	O
[	O
refer	O
to	O
the	O
likelihood	B
function	O
(	O
1.23	O
)	O
to	O
see	O
why	O
this	O
is	O
so	O
.	O
]	O
we	O
could	O
solve	O
the	O
decoding	B
problem	O
by	O
measuring	O
how	O
far	O
r	O
is	O
from	O
each	O
of	O
the	O
sixteen	O
codewords	O
in	O
table	O
1.14	O
,	O
then	O
picking	O
the	O
closest	O
.	O
is	O
there	O
a	O
more	O
e	O
(	O
cid:14	O
)	O
cient	O
way	O
of	O
(	O
cid:12	O
)	O
nding	O
the	O
most	O
probable	O
source	O
vector	O
?	O
syndrome	B
decoding	I
for	O
the	O
hamming	O
code	B
for	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
there	O
is	O
a	O
pictorial	O
solution	O
to	O
the	O
decoding	B
problem	O
,	O
based	O
on	O
the	O
encoding	O
picture	O
,	O
(	O
cid:12	O
)	O
gure	O
1.13.	O
as	O
a	O
(	O
cid:12	O
)	O
rst	O
example	O
,	O
let	O
’	O
s	O
assume	O
the	O
transmission	O
was	O
t	O
=	O
1000101	O
and	O
the	O
noise	B
(	O
cid:13	O
)	O
ips	O
the	O
second	O
bit	B
,	O
so	O
the	O
received	O
vector	O
is	O
r	O
=	O
1000101	O
(	O
cid:8	O
)	O
0100000	O
=	O
1100101.	O
we	O
write	O
the	O
received	O
vector	O
into	O
the	O
three	O
circles	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
1.15a	O
,	O
and	O
look	O
at	O
each	O
of	O
the	O
three	O
circles	O
to	O
see	O
whether	O
its	O
parity	B
is	O
even	O
.	O
the	O
circles	O
whose	O
parity	B
is	O
not	O
even	O
are	O
shown	O
by	O
dashed	O
lines	O
in	O
(	O
cid:12	O
)	O
gure	O
1.15b	O
.	O
the	O
decoding	B
task	O
is	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
smallest	O
set	B
of	O
(	O
cid:13	O
)	O
ipped	O
bits	O
that	O
can	O
account	O
for	O
these	O
violations	O
of	O
the	O
parity	O
rules	B
.	O
[	O
the	O
pattern	O
of	O
violations	O
of	O
the	O
parity	O
checks	O
is	O
called	O
the	O
syndrome	B
,	O
and	O
can	O
be	O
written	O
as	O
a	O
binary	O
vector	O
{	O
for	O
example	O
,	O
in	O
(	O
cid:12	O
)	O
gure	O
1.15b	O
,	O
the	O
syndrome	B
is	O
z	O
=	O
(	O
1	O
;	O
1	O
;	O
0	O
)	O
,	O
because	O
the	O
(	O
cid:12	O
)	O
rst	O
two	O
circles	O
are	O
‘	O
unhappy	O
’	O
(	O
parity	B
1	O
)	O
and	O
the	O
third	O
circle	O
is	O
‘	O
happy	O
’	O
(	O
parity	B
0	O
)	O
.	O
]	O
to	O
solve	O
the	O
decoding	B
task	O
,	O
we	O
ask	O
the	O
question	O
:	O
can	O
we	O
(	O
cid:12	O
)	O
nd	O
a	O
unique	O
bit	B
that	O
lies	O
inside	O
all	O
the	O
‘	O
unhappy	O
’	O
circles	O
and	O
outside	O
all	O
the	O
‘	O
happy	O
’	O
circles	O
?	O
if	O
so	O
,	O
the	O
(	O
cid:13	O
)	O
ipping	O
of	O
that	O
bit	B
would	O
account	O
for	O
the	O
observed	O
syndrome	B
.	O
in	O
the	O
case	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
1.15b	O
,	O
the	O
bit	B
r2	O
lies	O
inside	O
the	O
two	O
unhappy	O
circles	O
and	O
outside	O
the	O
happy	O
circle	B
;	O
no	O
other	O
single	O
bit	O
has	O
this	O
property	O
,	O
so	O
r2	O
is	O
the	O
only	O
single	O
bit	O
capable	O
of	O
explaining	O
the	O
syndrome	B
.	O
let	O
’	O
s	O
work	O
through	O
a	O
couple	O
more	O
examples	O
.	O
figure	O
1.15c	O
shows	O
what	O
happens	O
if	O
one	O
of	O
the	O
parity	O
bits	O
,	O
t5	O
,	O
is	O
(	O
cid:13	O
)	O
ipped	O
by	O
the	O
noise	B
.	O
just	O
one	O
of	O
the	O
checks	O
is	O
violated	O
.	O
only	O
r5	O
lies	O
inside	O
this	O
unhappy	O
circle	B
and	O
outside	O
the	O
other	O
two	O
happy	O
circles	O
,	O
so	O
r5	O
is	O
identi	O
(	O
cid:12	O
)	O
ed	O
as	O
the	O
only	O
single	O
bit	O
capable	O
of	O
explaining	O
the	O
syndrome	B
.	O
if	O
the	O
central	O
bit	B
r3	O
is	O
received	O
(	O
cid:13	O
)	O
ipped	O
,	O
(	O
cid:12	O
)	O
gure	O
1.15d	O
shows	O
that	O
all	O
three	O
checks	O
are	O
violated	O
;	O
only	O
r3	O
lies	O
inside	O
all	O
three	O
circles	O
,	O
so	O
r3	O
is	O
identi	O
(	O
cid:12	O
)	O
ed	O
as	O
the	O
suspect	O
bit	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
1.2	O
:	O
error-correcting	B
codes	I
for	O
the	O
binary	B
symmetric	I
channel	I
11	O
figure	O
1.15.	O
pictorial	O
representation	O
of	O
decoding	O
of	O
the	O
hamming	O
(	O
7	O
;	O
4	O
)	O
code	B
.	O
the	O
received	O
vector	O
is	O
written	O
into	O
the	O
diagram	O
as	O
shown	O
in	O
(	O
a	O
)	O
.	O
in	O
(	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
)	O
,	O
the	O
received	O
vector	O
is	O
shown	O
,	O
assuming	O
that	O
the	O
transmitted	O
vector	O
was	O
as	O
in	O
(	O
cid:12	O
)	O
gure	O
1.13b	O
and	O
the	O
bits	O
labelled	O
by	O
?	O
were	O
(	O
cid:13	O
)	O
ipped	O
.	O
the	O
violated	O
parity	B
checks	O
are	O
highlighted	O
by	O
dashed	O
circles	O
.	O
one	O
of	O
the	O
seven	O
bits	O
is	O
the	O
most	O
probable	O
suspect	O
to	O
account	O
for	O
each	O
‘	O
syndrome	B
’	O
,	O
i.e.	O
,	O
each	O
pattern	O
of	O
violated	O
and	O
satis	O
(	O
cid:12	O
)	O
ed	O
parity	B
checks	O
.	O
in	O
examples	O
(	O
b	O
)	O
,	O
(	O
c	O
)	O
,	O
and	O
(	O
d	O
)	O
,	O
the	O
most	O
probable	O
suspect	O
is	O
the	O
one	O
bit	B
that	O
was	O
(	O
cid:13	O
)	O
ipped	O
.	O
in	O
example	O
(	O
e	O
)	O
,	O
two	O
bits	O
have	O
been	O
(	O
cid:13	O
)	O
ipped	O
,	O
s3	O
and	O
t7	O
.	O
the	O
most	O
probable	O
suspect	O
is	O
r2	O
,	O
marked	O
by	O
a	O
circle	B
in	O
(	O
e0	O
)	O
,	O
which	O
shows	O
the	O
output	O
of	O
the	O
decoding	O
algorithm	B
.	O
algorithm	B
1.16.	O
actions	O
taken	O
by	O
the	O
optimal	B
decoder	I
for	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
,	O
assuming	O
a	O
binary	B
symmetric	I
channel	I
with	O
small	O
noise	B
level	O
f	O
.	O
the	O
syndrome	B
vector	O
z	O
lists	O
whether	O
each	O
parity	B
check	O
is	O
violated	O
(	O
1	O
)	O
or	O
satis	O
(	O
cid:12	O
)	O
ed	O
(	O
0	O
)	O
,	O
going	O
through	O
the	O
checks	O
in	O
the	O
order	O
of	O
the	O
bits	O
r5	O
,	O
r6	O
,	O
and	O
r7	O
.	O
(	O
a	O
)	O
r	O
5	O
r	O
3	O
4r	O
1	O
0	O
0	O
r	O
2	O
r	O
6	O
1*	O
0	O
r	O
1	O
r	O
7	O
1	O
1	O
*0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1*	O
0	O
0	O
0	O
1	O
1	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
1	O
1*	O
0	O
0	O
0	O
1	O
0*	O
(	O
e	O
)	O
1	O
1*	O
0	O
1	O
0	O
-	O
1	O
0*	O
(	O
e0	O
)	O
syndrome	B
z	O
000	O
001	O
010	O
011	O
100	O
101	O
110	O
111	O
un	O
(	O
cid:13	O
)	O
ip	O
this	O
bit	B
none	O
r7	O
r6	O
r4	O
r5	O
r1	O
r2	O
r3	O
if	O
you	O
try	O
(	O
cid:13	O
)	O
ipping	O
any	O
one	O
of	O
the	O
seven	O
bits	O
,	O
you	O
’	O
ll	O
(	O
cid:12	O
)	O
nd	O
that	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
syndrome	B
is	O
obtained	O
in	O
each	O
case	O
{	O
seven	O
non-zero	O
syndromes	O
,	O
one	O
for	O
each	O
bit	B
.	O
there	O
is	O
only	O
one	O
other	O
syndrome	B
,	O
the	O
all-zero	O
syndrome	B
.	O
so	O
if	O
the	O
channel	B
is	O
a	O
binary	B
symmetric	I
channel	I
with	O
a	O
small	O
noise	B
level	O
f	O
,	O
the	O
optimal	B
decoder	I
un	O
(	O
cid:13	O
)	O
ips	O
at	O
most	O
one	O
bit	B
,	O
depending	O
on	O
the	O
syndrome	B
,	O
as	O
shown	O
in	O
algorithm	O
1.16.	O
each	O
syndrome	B
could	O
have	O
been	O
caused	O
by	O
other	O
noise	B
patterns	O
too	O
,	O
but	O
any	O
other	O
noise	B
pattern	O
that	O
has	O
the	O
same	O
syndrome	B
must	O
be	O
less	O
probable	O
because	O
it	O
involves	O
a	O
larger	O
number	O
of	O
noise	O
events	O
.	O
what	O
happens	O
if	O
the	O
noise	B
actually	O
(	O
cid:13	O
)	O
ips	O
more	O
than	O
one	O
bit	B
?	O
figure	O
1.15e	O
shows	O
the	O
situation	O
when	O
two	O
bits	O
,	O
r3	O
and	O
r7	O
,	O
are	O
received	O
(	O
cid:13	O
)	O
ipped	O
.	O
the	O
syn-	O
drome	O
,	O
110	O
,	O
makes	O
us	O
suspect	O
the	O
single	O
bit	O
r2	O
;	O
so	O
our	O
optimal	B
decoding	O
al-	O
gorithm	O
(	O
cid:13	O
)	O
ips	O
this	O
bit	B
,	O
giving	O
a	O
decoded	O
pattern	O
with	O
three	O
errors	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
1.15e0	O
.	O
if	O
we	O
use	O
the	O
optimal	B
decoding	O
algorithm	B
,	O
any	O
two-bit	O
error	O
pattern	O
will	O
lead	O
to	O
a	O
decoded	O
seven-bit	O
vector	O
that	O
contains	O
three	O
errors	O
.	O
general	O
view	O
of	O
decoding	O
for	O
linear	O
codes	O
:	O
syndrome	B
decoding	I
we	O
can	O
also	O
describe	O
the	O
decoding	B
problem	O
for	O
a	O
linear	B
code	O
in	O
terms	O
of	O
matrices	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
four	O
received	O
bits	O
,	O
r1r2r3r4	O
,	O
purport	O
to	O
be	O
the	O
four	O
source	O
bits	O
;	O
and	O
the	O
received	O
bits	O
r5r6r7	O
purport	O
to	O
be	O
the	O
parities	O
of	O
the	O
source	O
bits	O
,	O
as	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
the	O
generator	B
matrix	I
g.	O
we	O
evaluate	O
the	O
three	O
parity-check	O
bits	O
for	O
the	O
received	O
bits	O
,	O
r1r2r3r4	O
,	O
and	O
see	O
whether	O
they	O
match	O
the	O
three	O
received	O
bits	O
,	O
r5r6r7	O
.	O
the	O
di	O
(	O
cid:11	O
)	O
erences	O
(	O
modulo	O
2	O
)	O
between	O
these	O
two	O
triplets	O
are	O
called	O
the	O
syndrome	B
of	O
the	O
received	O
vector	O
.	O
if	O
the	O
syndrome	B
is	O
zero	O
{	O
if	O
all	O
three	O
parity	O
checks	O
are	O
happy	O
{	O
then	O
the	O
received	O
vector	O
is	O
a	O
codeword	B
,	O
and	O
the	O
most	O
probable	O
decoding	O
is	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
12	O
1	O
|	O
introduction	O
to	O
information	B
theory	I
s	O
encoder	B
t	O
-	O
channel	B
f	O
=	O
10	O
%	O
-	O
r	O
decoder	B
^s	O
-	O
figure	O
1.17.	O
transmitting	O
10	O
000	O
source	O
bits	O
over	O
a	O
binary	B
symmetric	I
channel	I
with	O
f	O
=	O
10	O
%	O
using	O
a	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
.	O
the	O
probability	O
of	O
decoded	O
bit	B
error	O
is	O
about	O
7	O
%	O
.	O
parity	B
bits	O
8	O
>	O
>	O
<	O
>	O
>	O
:	O
given	O
by	O
reading	O
out	O
its	O
(	O
cid:12	O
)	O
rst	O
four	O
bits	O
.	O
if	O
the	O
syndrome	B
is	O
non-zero	O
,	O
then	O
the	O
noise	B
sequence	O
for	O
this	O
block	B
was	O
non-zero	O
,	O
and	O
the	O
syndrome	B
is	O
our	O
pointer	B
to	O
the	O
most	O
probable	O
error	O
pattern	O
.	O
the	O
computation	O
of	O
the	O
syndrome	O
vector	O
is	O
a	O
linear	B
operation	O
.	O
if	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
3	O
(	O
cid:2	O
)	O
4	O
matrix	B
p	O
such	O
that	O
the	O
matrix	B
of	O
equation	O
(	O
1.26	O
)	O
is	O
p	O
(	O
cid:21	O
)	O
;	O
gt	O
=	O
(	O
cid:20	O
)	O
i4	O
(	O
1.29	O
)	O
where	O
i4	O
is	O
the	O
4	O
(	O
cid:2	O
)	O
4	O
identity	B
matrix	I
,	O
then	O
the	O
syndrome	B
vector	O
is	O
z	O
=	O
hr	O
,	O
where	O
the	O
parity-check	B
matrix	I
h	O
is	O
given	O
by	O
h	O
=	O
(	O
cid:2	O
)	O
(	O
cid:0	O
)	O
p	O
i3	O
(	O
cid:3	O
)	O
;	O
in	O
modulo	O
2	O
arithmetic	O
,	O
(	O
cid:0	O
)	O
1	O
(	O
cid:17	O
)	O
1	O
,	O
so	O
h	O
=	O
(	O
cid:2	O
)	O
p	O
i3	O
(	O
cid:3	O
)	O
=2	O
4	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
3	O
5	O
:	O
all	O
the	O
codewords	O
t	O
=	O
gts	O
of	O
the	O
code	O
satisfy	O
ht	O
=2	O
4	O
0	O
0	O
0	O
3	O
5	O
:	O
(	O
1.30	O
)	O
(	O
1.31	O
)	O
.	O
exercise	O
1.4	O
.	O
[	O
1	O
]	O
prove	O
that	O
this	O
is	O
so	O
by	O
evaluating	O
the	O
3	O
(	O
cid:2	O
)	O
4	O
matrix	B
hgt	O
.	O
since	O
the	O
received	O
vector	O
r	O
is	O
given	O
by	O
r	O
=	O
gts	O
+	O
n	O
,	O
the	O
syndrome-decoding	O
problem	O
is	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
most	O
probable	O
noise	O
vector	O
n	O
satisfying	O
the	O
equation	O
hn	O
=	O
z	O
:	O
(	O
1.32	O
)	O
a	O
decoding	B
algorithm	O
that	O
solves	O
this	O
problem	O
is	O
called	O
a	O
maximum-likelihood	O
decoder	B
.	O
we	O
will	O
discuss	O
decoding	B
problems	O
like	O
this	O
in	O
later	O
chapters	O
.	O
summary	B
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
’	O
s	O
properties	O
every	O
possible	O
received	O
vector	O
of	O
length	B
7	O
bits	O
is	O
either	O
a	O
codeword	B
,	O
or	O
it	O
’	O
s	O
one	O
(	O
cid:13	O
)	O
ip	O
away	O
from	O
a	O
codeword	B
.	O
since	O
there	O
are	O
three	O
parity	O
constraints	O
,	O
each	O
of	O
which	O
might	O
or	O
might	O
not	O
be	O
violated	O
,	O
there	O
are	O
2	O
(	O
cid:2	O
)	O
2	O
(	O
cid:2	O
)	O
2	O
=	O
8	O
distinct	O
syndromes	O
.	O
they	O
can	O
be	O
divided	O
into	O
seven	O
non-zero	O
syndromes	O
{	O
one	O
for	O
each	O
of	O
the	O
one-bit	O
error	O
patterns	O
{	O
and	O
the	O
all-zero	O
syndrome	B
,	O
corresponding	O
to	O
the	O
zero-noise	O
case	O
.	O
the	O
optimal	B
decoder	I
takes	O
no	O
action	O
if	O
the	O
syndrome	B
is	O
zero	O
,	O
otherwise	O
it	O
uses	O
this	O
mapping	B
of	O
non-zero	O
syndromes	O
onto	O
one-bit	O
error	O
patterns	O
to	O
un	O
(	O
cid:13	O
)	O
ip	O
the	O
suspect	O
bit	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
1.2	O
:	O
error-correcting	B
codes	I
for	O
the	O
binary	B
symmetric	I
channel	I
13	O
there	O
is	O
a	O
decoding	B
error	O
if	O
the	O
four	O
decoded	O
bits	O
^s1	O
;	O
^s2	O
;	O
^s3	O
;	O
^s4	O
do	O
not	O
all	O
match	O
the	O
source	O
bits	O
s1	O
;	O
s2	O
;	O
s3	O
;	O
s4	O
.	O
the	O
probability	B
of	I
block	I
error	I
pb	O
is	O
the	O
probability	B
that	O
one	O
or	O
more	O
of	O
the	O
decoded	O
bits	O
in	O
one	O
block	B
fail	O
to	O
match	O
the	O
corresponding	O
source	O
bits	O
,	O
the	O
probability	O
of	O
bit	O
error	O
pb	O
is	O
the	O
average	B
probability	O
that	O
a	O
decoded	O
bit	B
fails	O
to	O
match	O
the	O
corresponding	O
source	O
bit	O
,	O
pb	O
=	O
p	O
(	O
^s	O
6=	O
s	O
)	O
:	O
(	O
1.33	O
)	O
pb	O
=	O
1	O
k	O
k	O
xk=1	O
p	O
(	O
^sk	O
6=	O
sk	O
)	O
:	O
(	O
1.34	O
)	O
in	O
the	O
case	O
of	O
the	O
hamming	O
code	B
,	O
a	O
decoding	B
error	O
will	O
occur	O
whenever	O
the	O
noise	B
has	O
(	O
cid:13	O
)	O
ipped	O
more	O
than	O
one	O
bit	B
in	O
a	O
block	B
of	O
seven	O
.	O
the	O
probability	B
of	I
block	I
error	I
is	O
thus	O
the	O
probability	B
that	O
two	O
or	O
more	O
bits	O
are	O
(	O
cid:13	O
)	O
ipped	O
in	O
a	O
block	B
.	O
this	O
probability	B
scales	O
as	O
o	O
(	O
f	O
2	O
)	O
,	O
as	O
did	O
the	O
probability	B
of	I
error	I
for	O
the	O
repetition	B
code	I
r3	O
.	O
but	O
notice	O
that	O
the	O
hamming	O
code	B
communicates	O
at	O
a	O
greater	O
rate	B
,	O
r	O
=	O
4=7	O
.	O
figure	O
1.17	O
shows	O
a	O
binary	O
image	O
transmitted	O
over	O
a	O
binary	B
symmetric	I
channel	I
using	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
.	O
about	O
7	O
%	O
of	O
the	O
decoded	O
bits	O
are	O
in	O
error	O
.	O
notice	O
that	O
the	O
errors	B
are	O
correlated	O
:	O
often	O
two	O
or	O
three	O
successive	O
decoded	O
bits	O
are	O
(	O
cid:13	O
)	O
ipped	O
.	O
exercise	O
1.5	O
.	O
[	O
1	O
]	O
this	O
exercise	O
and	O
the	O
next	O
three	O
refer	O
to	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
.	O
decode	O
the	O
received	O
strings	O
:	O
(	O
a	O
)	O
r	O
=	O
1101011	O
(	O
b	O
)	O
r	O
=	O
0110110	O
(	O
c	O
)	O
r	O
=	O
0100111	O
(	O
d	O
)	O
r	O
=	O
1111111.	O
exercise	O
1.6	O
.	O
[	O
2	O
,	O
p.17	O
]	O
(	O
a	O
)	O
calculate	O
the	O
probability	B
of	I
block	I
error	I
pb	O
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
as	O
a	O
function	B
of	O
the	O
noise	B
level	O
f	O
and	O
show	O
that	O
to	O
leading	O
order	O
it	O
goes	O
as	O
21f	O
2	O
.	O
(	O
b	O
)	O
[	O
3	O
]	O
show	O
that	O
to	O
leading	O
order	O
the	O
probability	O
of	O
bit	O
error	O
pb	O
goes	O
as	O
9f	O
2.	O
exercise	O
1.7	O
.	O
[	O
2	O
,	O
p.19	O
]	O
find	O
some	O
noise	B
vectors	O
that	O
give	O
the	O
all-zero	O
syndrome	B
(	O
that	O
is	O
,	O
noise	B
vectors	O
that	O
leave	O
all	O
the	O
parity	B
checks	O
unviolated	O
)	O
.	O
how	O
many	O
such	O
noise	B
vectors	O
are	O
there	O
?	O
.	O
exercise	O
1.8	O
.	O
[	O
2	O
]	O
i	O
asserted	O
above	O
that	O
a	O
block	B
decoding	O
error	O
will	O
result	O
when-	O
ever	O
two	O
or	O
more	O
bits	O
are	O
(	O
cid:13	O
)	O
ipped	O
in	O
a	O
single	O
block	O
.	O
show	O
that	O
this	O
is	O
indeed	O
so	O
.	O
[	O
in	O
principle	O
,	O
there	O
might	O
be	O
error	O
patterns	O
that	O
,	O
after	O
de-	O
coding	O
,	O
led	O
only	O
to	O
the	O
corruption	O
of	O
the	O
parity	O
bits	O
,	O
with	O
no	O
source	O
bits	O
incorrectly	O
decoded	O
.	O
]	O
summary	B
of	O
codes	O
’	O
performances	O
figure	O
1.18	O
shows	O
the	O
performance	O
of	O
repetition	O
codes	O
and	O
the	O
hamming	O
code	B
.	O
it	O
also	O
shows	O
the	O
performance	O
of	O
a	O
family	O
of	O
linear	O
block	B
codes	O
that	O
are	O
gen-	O
eralizations	O
of	O
hamming	O
codes	O
,	O
called	O
bch	O
codes	O
.	O
this	O
(	O
cid:12	O
)	O
gure	O
shows	O
that	O
we	O
can	O
,	O
using	O
linear	O
block	O
codes	O
,	O
achieve	O
better	O
performance	O
than	O
repetition	B
codes	O
;	O
but	O
the	O
asymptotic	O
situation	O
still	O
looks	O
grim	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
14	O
0.1	O
0.08	O
0.06	O
0.04	O
0.02	O
0	O
1	O
|	O
introduction	O
to	O
information	B
theory	I
r1	O
0.1	O
0.01	O
r5	O
h	O
(	O
7,4	O
)	O
r1	O
h	O
(	O
7,4	O
)	O
1e-05	O
pb	O
more	O
useful	O
codes	O
bch	O
(	O
511,76	O
)	O
figure	O
1.18.	O
error	B
probability	I
pb	O
versus	O
rate	B
r	O
for	O
repetition	O
codes	O
,	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
and	O
bch	O
codes	O
with	O
blocklengths	O
up	O
to	O
1023	O
over	O
a	O
binary	B
symmetric	I
channel	I
with	O
f	O
=	O
0:1.	O
the	O
righthand	O
(	O
cid:12	O
)	O
gure	O
shows	O
pb	O
on	O
a	O
logarithmic	O
scale	O
.	O
bch	O
(	O
31,16	O
)	O
1e-10	O
r3	O
bch	O
(	O
15,7	O
)	O
r5	O
more	O
useful	O
codes	O
bch	O
(	O
1023,101	O
)	O
1e-15	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
rate	B
rate	O
exercise	O
1.9	O
.	O
[	O
4	O
,	O
p.19	O
]	O
design	O
an	O
error-correcting	B
code	I
and	O
a	O
decoding	B
algorithm	O
for	O
it	O
,	O
estimate	O
its	O
probability	B
of	I
error	I
,	O
and	O
add	O
it	O
to	O
(	O
cid:12	O
)	O
gure	O
1.18	O
.	O
[	O
don	O
’	O
t	O
worry	O
if	O
you	O
(	O
cid:12	O
)	O
nd	O
it	O
di	O
(	O
cid:14	O
)	O
cult	O
to	O
make	O
a	O
code	B
better	O
than	O
the	O
hamming	O
code	B
,	O
or	O
if	O
you	O
(	O
cid:12	O
)	O
nd	O
it	O
di	O
(	O
cid:14	O
)	O
cult	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
good	B
decoder	O
for	O
your	O
code	B
;	O
that	O
’	O
s	O
the	O
point	O
of	O
this	O
exercise	O
.	O
]	O
exercise	O
1.10	O
.	O
[	O
3	O
,	O
p.20	O
]	O
a	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
can	O
correct	O
any	O
one	O
error	O
;	O
might	O
there	O
be	O
a	O
(	O
14	O
;	O
8	O
)	O
code	B
that	O
can	O
correct	O
any	O
two	O
errors	B
?	O
optional	O
extra	O
:	O
does	O
the	O
answer	O
to	O
this	O
question	O
depend	O
on	O
whether	O
the	O
code	B
is	O
linear	B
or	O
nonlinear	B
?	O
exercise	O
1.11	O
.	O
[	O
4	O
,	O
p.21	O
]	O
design	O
an	O
error-correcting	B
code	I
,	O
other	O
than	O
a	O
repetition	B
code	I
,	O
that	O
can	O
correct	O
any	O
two	O
errors	B
in	O
a	O
block	B
of	O
size	O
n	O
.	O
1.3	O
what	O
performance	O
can	O
the	O
best	O
codes	O
achieve	O
?	O
there	O
seems	O
to	O
be	O
a	O
trade-o	O
(	O
cid:11	O
)	O
between	O
the	O
decoded	O
bit-error	O
probability	B
pb	O
(	O
which	O
we	O
would	O
like	O
to	O
reduce	O
)	O
and	O
the	O
rate	B
r	O
(	O
which	O
we	O
would	O
like	O
to	O
keep	O
large	O
)	O
.	O
how	O
can	O
this	O
trade-o	O
(	O
cid:11	O
)	O
be	O
characterized	O
?	O
what	O
points	O
in	O
the	O
(	O
r	O
;	O
pb	O
)	O
plane	O
are	O
achievable	O
?	O
this	O
question	O
was	O
addressed	O
by	O
claude	O
shannon	O
in	O
his	O
pioneering	O
paper	O
of	O
1948	O
,	O
in	O
which	O
he	O
both	O
created	O
the	O
(	O
cid:12	O
)	O
eld	O
of	O
information	O
theory	B
and	O
solved	O
most	O
of	O
its	O
fundamental	O
problems	O
.	O
at	O
that	O
time	O
there	O
was	O
a	O
widespread	O
belief	B
that	O
the	O
boundary	O
between	O
achievable	O
and	O
nonachievable	O
points	O
in	O
the	O
(	O
r	O
;	O
pb	O
)	O
plane	O
was	O
a	O
curve	O
passing	O
through	O
the	O
origin	O
(	O
r	O
;	O
pb	O
)	O
=	O
(	O
0	O
;	O
0	O
)	O
;	O
if	O
this	O
were	O
so	O
,	O
then	O
,	O
in	O
order	O
to	O
achieve	O
a	O
vanishingly	O
small	O
error	B
probability	I
pb	O
,	O
one	O
would	O
have	O
to	O
reduce	O
the	O
rate	B
correspondingly	O
close	O
to	O
zero	O
.	O
‘	O
no	O
pain	O
,	O
no	O
gain.	O
’	O
however	O
,	O
shannon	O
proved	O
the	O
remarkable	O
result	O
that	O
the	O
boundary	O
be-	O
(	O
cid:3	O
)	O
tween	O
achievable	O
and	O
nonachievable	O
points	O
meets	O
the	O
r	O
axis	O
at	O
a	O
non-zero	O
value	O
r	O
=	O
c	O
,	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
1.19.	O
for	O
any	O
channel	B
,	O
there	O
exist	O
codes	O
that	O
make	O
it	O
possible	O
to	O
communicate	O
with	O
arbitrarily	O
small	O
probability	B
of	I
error	I
pb	O
at	O
non-zero	O
rates	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
half	O
of	O
this	O
book	O
(	O
parts	O
i	O
{	O
iii	O
)	O
will	O
be	O
devoted	O
to	O
understanding	O
this	O
remarkable	O
result	O
,	O
which	O
is	O
called	O
the	O
noisy-channel	B
coding	I
theorem	I
.	O
example	O
:	O
f	O
=	O
0:1	O
the	O
maximum	O
rate	O
at	O
which	O
communication	B
is	O
possible	O
with	O
arbitrarily	O
small	O
pb	O
is	O
called	O
the	O
capacity	B
of	O
the	O
channel	B
.	O
the	O
formula	O
for	O
the	O
capacity	B
of	O
a	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
1.4	O
:	O
summary	B
0.1	O
0.08	O
0.06	O
0.04	O
0.02	O
0	O
r1	O
0.1	O
0.01	O
r5	O
r1	O
h	O
(	O
7,4	O
)	O
1e-05	O
pb	O
r3	O
r5	O
achievable	O
not	O
achievable	O
0	O
0.2	O
0.4	O
c	O
0.6	O
rate	B
0.8	O
1	O
0	O
0.2	O
0.4	O
c	O
0.6	O
rate	B
0.8	O
1	O
achievable	O
not	O
achievable	O
1e-10	O
1e-15	O
15	O
figure	O
1.19.	O
shannon	O
’	O
s	O
noisy-channel	B
coding	I
theorem	I
.	O
the	O
solid	O
curve	O
shows	O
the	O
shannon	O
limit	O
on	O
achievable	O
values	O
of	O
(	O
r	O
;	O
pb	O
)	O
for	O
the	O
binary	B
symmetric	I
channel	I
with	O
f	O
=	O
0:1.	O
rates	O
up	O
to	O
r	O
=	O
c	O
are	O
achievable	O
with	O
arbitrarily	O
small	O
pb	O
.	O
the	O
points	O
show	O
the	O
performance	O
of	O
some	O
textbook	O
codes	O
,	O
as	O
in	O
(	O
cid:12	O
)	O
gure	O
1.18.	O
the	O
equation	O
de	O
(	O
cid:12	O
)	O
ning	O
the	O
shannon	O
limit	O
(	O
the	O
solid	O
curve	O
)	O
is	O
r	O
=	O
c=	O
(	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
pb	O
)	O
)	O
;	O
where	O
c	O
and	O
h2	O
are	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
equation	O
(	O
1.35	O
)	O
.	O
binary	B
symmetric	I
channel	I
with	O
noise	B
level	O
f	O
is	O
c	O
(	O
f	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
f	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
(	O
cid:20	O
)	O
f	O
log2	O
1	O
f	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
log2	O
1	O
1	O
(	O
cid:0	O
)	O
f	O
(	O
cid:21	O
)	O
;	O
(	O
1.35	O
)	O
the	O
channel	B
we	O
were	O
discussing	O
earlier	O
with	O
noise	O
level	O
f	O
=	O
0:1	O
has	O
capacity	B
c	O
’	O
0:53.	O
let	O
us	O
consider	O
what	O
this	O
means	O
in	O
terms	O
of	O
noisy	O
disk	O
drives	O
.	O
the	O
repetition	B
code	I
r3	O
could	O
communicate	O
over	O
this	O
channel	O
with	O
pb	O
=	O
0:03	O
at	O
a	O
rate	B
r	O
=	O
1=3	O
.	O
thus	O
we	O
know	O
how	O
to	O
build	O
a	O
single	O
gigabyte	O
disk	B
drive	I
with	O
pb	O
=	O
0:03	O
from	O
three	O
noisy	B
gigabyte	O
disk	O
drives	O
.	O
we	O
also	O
know	O
how	O
to	O
make	O
a	O
single	O
gigabyte	O
disk	B
drive	I
with	O
pb	O
’	O
10	O
(	O
cid:0	O
)	O
15	O
from	O
sixty	O
noisy	B
one-gigabyte	O
drives	O
(	O
exercise	O
1.3	O
,	O
p.8	O
)	O
.	O
and	O
now	O
shannon	O
passes	O
by	O
,	O
notices	O
us	O
juggling	B
with	O
disk	O
drives	O
and	O
codes	O
and	O
says	O
:	O
‘	O
what	O
performance	O
are	O
you	O
trying	O
to	O
achieve	O
?	O
10	O
(	O
cid:0	O
)	O
15	O
?	O
you	O
don	O
’	O
t	O
need	O
sixty	O
disk	O
drives	O
{	O
you	O
can	O
get	O
that	O
performance	O
with	O
just	O
two	O
disk	O
drives	O
(	O
since	O
1/2	O
is	O
less	O
than	O
0:53	O
)	O
.	O
and	O
if	O
you	O
want	O
pb	O
=	O
10	O
(	O
cid:0	O
)	O
18	O
or	O
10	O
(	O
cid:0	O
)	O
24	O
or	O
anything	O
,	O
you	O
can	O
get	O
there	O
with	O
two	O
disk	O
drives	O
too	O
!	O
’	O
[	O
strictly	O
,	O
the	O
above	O
statements	O
might	O
not	O
be	O
quite	O
right	O
,	O
since	O
,	O
as	O
we	O
shall	O
see	O
,	O
shannon	O
proved	O
his	O
noisy-channel	B
coding	I
theorem	I
by	O
studying	O
sequences	O
of	O
block	O
codes	O
with	O
ever-increasing	O
blocklengths	O
,	O
and	O
the	O
required	O
blocklength	O
might	O
be	O
bigger	O
than	O
a	O
gigabyte	O
(	O
the	O
size	O
of	O
our	O
disk	B
drive	I
)	O
,	O
in	O
which	O
case	O
,	O
shannon	O
might	O
say	O
‘	O
well	O
,	O
you	O
can	O
’	O
t	O
do	O
it	O
with	O
those	O
tiny	O
disk	O
drives	O
,	O
but	O
if	O
you	O
had	O
two	O
noisy	B
terabyte	O
drives	O
,	O
you	O
could	O
make	O
a	O
single	O
high-quality	O
terabyte	O
drive	O
from	O
them	O
’	O
.	O
]	O
1.4	O
summary	B
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
by	O
including	O
three	O
parity-check	O
bits	O
in	O
a	O
block	B
of	O
7	O
bits	O
it	O
is	O
possible	O
to	O
detect	O
and	O
correct	O
any	O
single	O
bit	O
error	O
in	O
each	O
block	B
.	O
shannon	O
’	O
s	O
noisy-channel	B
coding	I
theorem	I
information	O
can	O
be	O
communicated	O
over	O
a	O
noisy	B
channel	I
at	O
a	O
non-zero	O
rate	B
with	O
arbitrarily	O
small	O
error	B
probability	I
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
16	O
1	O
|	O
introduction	O
to	O
information	B
theory	I
information	O
theory	B
addresses	O
both	O
the	O
limitations	O
and	O
the	O
possibilities	O
of	O
communication	O
.	O
the	O
noisy-channel	B
coding	I
theorem	I
,	O
which	O
we	O
will	O
prove	O
in	O
chapter	O
10	O
,	O
asserts	O
both	O
that	O
reliable	O
communication	B
at	O
any	O
rate	B
beyond	O
the	O
capacity	B
is	O
impossible	O
,	O
and	O
that	O
reliable	O
communication	B
at	O
all	O
rates	O
up	O
to	O
capacity	B
is	O
possible	O
.	O
the	O
next	O
few	O
chapters	O
lay	O
the	O
foundations	O
for	O
this	O
result	O
by	O
discussing	O
how	B
to	I
measure	I
information	O
content	B
and	O
the	O
intimately	O
related	O
topic	O
of	O
data	O
compression	B
.	O
1.5	O
further	O
exercises	O
.	O
exercise	O
1.12	O
.	O
[	O
2	O
,	O
p.21	O
]	O
consider	O
the	O
repetition	B
code	I
r9	O
.	O
one	O
way	O
of	O
viewing	O
this	O
code	B
is	O
as	O
a	O
concatenation	B
of	O
r3	O
with	O
r3	O
.	O
we	O
(	O
cid:12	O
)	O
rst	O
encode	O
the	O
source	O
stream	O
with	O
r3	O
,	O
then	O
encode	O
the	O
resulting	O
output	O
with	O
r3	O
.	O
we	O
could	O
call	O
this	O
code	B
‘	O
r2	O
3	O
’	O
.	O
this	O
idea	O
motivates	O
an	O
alternative	O
decoding	B
algorithm	O
,	O
in	O
which	O
we	O
decode	O
the	O
bits	O
three	O
at	O
a	O
time	O
using	O
the	O
decoder	B
for	O
r3	O
;	O
then	O
decode	O
the	O
decoded	O
bits	O
from	O
that	O
(	O
cid:12	O
)	O
rst	O
decoder	B
using	O
the	O
decoder	B
for	O
r3	O
.	O
evaluate	O
the	O
probability	B
of	I
error	I
for	O
this	O
decoder	B
and	O
compare	O
it	O
with	O
the	O
probability	B
of	I
error	I
for	O
the	O
optimal	B
decoder	I
for	O
r9	O
.	O
do	O
the	O
concatenated	O
encoder	B
and	O
decoder	B
for	O
r2	O
those	O
for	O
r9	O
?	O
3	O
have	O
advantages	O
over	O
1.6	O
solutions	O
solution	O
to	O
exercise	O
1.2	O
(	O
p.7	O
)	O
.	O
an	O
error	O
is	O
made	O
by	O
r3	O
if	O
two	O
or	O
more	O
bits	O
are	O
(	O
cid:13	O
)	O
ipped	O
in	O
a	O
block	B
of	O
three	O
.	O
so	O
the	O
error	B
probability	I
of	O
r3	O
is	O
a	O
sum	O
of	O
two	O
terms	O
:	O
the	O
probability	B
that	O
all	O
three	O
bits	O
are	O
(	O
cid:13	O
)	O
ipped	O
,	O
f	O
3	O
;	O
and	O
the	O
probability	B
that	O
exactly	O
two	O
bits	O
are	O
(	O
cid:13	O
)	O
ipped	O
,	O
3f	O
2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
.	O
[	O
if	O
these	O
expressions	O
are	O
not	O
obvious	O
,	O
see	O
example	O
1.1	O
(	O
p.1	O
)	O
:	O
the	O
expressions	O
are	O
p	O
(	O
r	O
=	O
3j	O
f	O
;	O
n	O
=	O
3	O
)	O
and	O
p	O
(	O
r	O
=	O
2j	O
f	O
;	O
n	O
=	O
3	O
)	O
.	O
]	O
pb	O
=	O
pb	O
=	O
3f	O
2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
+	O
f	O
3	O
=	O
3f	O
2	O
(	O
cid:0	O
)	O
2f	O
3	O
:	O
this	O
probability	B
is	O
dominated	O
for	O
small	O
f	O
by	O
the	O
term	O
3f	O
2.	O
see	O
exercise	O
2.38	O
(	O
p.39	O
)	O
for	O
further	O
discussion	O
of	O
this	O
problem	O
.	O
(	O
1.36	O
)	O
solution	O
to	O
exercise	O
1.3	O
(	O
p.8	O
)	O
.	O
the	O
probability	B
of	I
error	I
for	O
the	O
repetition	B
code	I
rn	O
is	O
dominated	O
by	O
the	O
probability	B
that	O
dn=2e	O
bits	O
are	O
(	O
cid:13	O
)	O
ipped	O
,	O
which	O
goes	O
(	O
for	O
odd	O
n	O
)	O
as	O
dn=2e	O
(	O
cid:19	O
)	O
f	O
(	O
n	O
+1	O
)	O
=2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
=2	O
:	O
(	O
cid:18	O
)	O
n	O
k	O
(	O
cid:19	O
)	O
(	O
cid:20	O
)	O
2n	O
h2	O
(	O
k=n	O
)	O
)	O
(	O
cid:18	O
)	O
n	O
k	O
(	O
cid:1	O
)	O
can	O
be	O
approximated	O
using	O
the	O
binary	B
entropy	I
function	I
:	O
k	O
(	O
cid:19	O
)	O
’	O
2n	O
h2	O
(	O
k=n	O
)	O
;	O
2n	O
h2	O
(	O
k=n	O
)	O
(	O
cid:20	O
)	O
(	O
cid:18	O
)	O
n	O
the	O
term	O
(	O
cid:0	O
)	O
n	O
(	O
1.38	O
)	O
where	O
this	O
approximation	B
introduces	O
an	O
error	O
of	O
order	O
pn	O
{	O
as	O
shown	O
in	O
equation	O
(	O
1.17	O
)	O
.	O
so	O
notation	B
:	O
(	O
cid:6	O
)	O
n=2	O
(	O
cid:7	O
)	O
denotes	O
the	O
smallest	O
integer	O
greater	O
than	O
or	O
equal	O
to	O
n=2	O
.	O
(	O
1.37	O
)	O
1	O
n	O
+	O
1	O
pb	O
=	O
pb	O
’	O
2n	O
(	O
f	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
)	O
n=2	O
=	O
(	O
4f	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
)	O
n=2	O
:	O
(	O
1.39	O
)	O
log	O
4f	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
=	O
68.	O
this	O
answer	O
is	O
a	O
little	O
out	O
because	O
the	O
approximation	B
we	O
used	O
overestimated	O
setting	O
this	O
equal	O
to	O
the	O
required	O
value	O
of	O
10	O
(	O
cid:0	O
)	O
15	O
we	O
(	O
cid:12	O
)	O
nd	O
n	O
’	O
2	O
log	O
10	O
(	O
cid:0	O
)	O
15	O
k	O
(	O
cid:1	O
)	O
and	O
we	O
did	O
not	O
distinguish	O
between	O
dn=2e	O
and	O
n=2	O
.	O
(	O
cid:0	O
)	O
n	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
1.6	O
:	O
solutions	O
17	O
a	O
slightly	O
more	O
careful	O
answer	O
(	O
short	O
of	O
explicit	O
computation	O
)	O
goes	O
as	O
follows	O
.	O
k	O
(	O
cid:1	O
)	O
to	O
the	O
next	O
order	O
,	O
we	O
(	O
cid:12	O
)	O
nd	O
:	O
taking	O
the	O
approximation	B
for	O
(	O
cid:0	O
)	O
n	O
n=2	O
(	O
cid:19	O
)	O
’	O
2n	O
(	O
cid:18	O
)	O
n	O
1	O
:	O
p2	O
(	O
cid:25	O
)	O
n=4	O
this	O
approximation	B
can	O
be	O
proved	O
from	O
an	O
accurate	O
version	O
of	O
stirling	O
’	O
s	O
ap-	O
proximation	O
(	O
1.12	O
)	O
,	O
or	O
by	O
considering	O
the	O
binomial	B
distribution	I
with	O
p	O
=	O
1=2	O
and	O
noting	O
(	O
1.40	O
)	O
n=2	O
(	O
cid:19	O
)	O
n=2	O
xr=	O
(	O
cid:0	O
)	O
n=2	O
k	O
(	O
cid:19	O
)	O
2	O
(	O
cid:0	O
)	O
n	O
’	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
cid:18	O
)	O
n	O
1	O
=xk	O
(	O
cid:18	O
)	O
n	O
n=2	O
(	O
cid:19	O
)	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
;	O
(	O
1.41	O
)	O
where	O
(	O
cid:27	O
)	O
=pn=4	O
,	O
from	O
which	O
equation	O
(	O
1.40	O
)	O
follows	O
.	O
the	O
distinction	O
between	O
dn=2e	O
and	O
n=2	O
is	O
not	O
important	O
in	O
this	O
term	O
since	O
(	O
cid:0	O
)	O
n	O
k	O
(	O
cid:1	O
)	O
has	O
a	O
maximum	O
at	O
’	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
cid:18	O
)	O
n	O
e	O
(	O
cid:0	O
)	O
r2=2	O
(	O
cid:27	O
)	O
2	O
k	O
=	O
n=2	O
.	O
then	O
the	O
probability	B
of	I
error	I
(	O
for	O
odd	O
n	O
)	O
is	O
to	O
leading	O
order	O
pb	O
’	O
(	O
cid:18	O
)	O
n	O
(	O
n	O
+1	O
)	O
=2	O
(	O
cid:19	O
)	O
f	O
(	O
n	O
+1	O
)	O
=2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
=2	O
f	O
[	O
f	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
]	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
=2	O
’	O
p	O
(	O
cid:25	O
)	O
n=2	O
1	O
the	O
equation	O
pb	O
=	O
10	O
(	O
cid:0	O
)	O
15	O
can	O
be	O
written	O
’	O
2n	O
1	O
p	O
(	O
cid:25	O
)	O
n=8	O
(	O
1.42	O
)	O
f	O
[	O
4f	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
]	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
=2	O
:	O
(	O
1.43	O
)	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
=2	O
’	O
log	O
10	O
(	O
cid:0	O
)	O
15	O
+	O
log	O
p	O
(	O
cid:25	O
)	O
n=8	O
f	O
log	O
4f	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
in	O
equation	O
(	O
1.44	O
)	O
,	O
the	O
logarithms	B
can	O
be	O
taken	O
to	O
any	O
base	O
,	O
as	O
long	O
as	O
it	O
’	O
s	O
the	O
same	O
base	O
throughout	O
.	O
in	O
equation	O
(	O
1.45	O
)	O
,	O
i	O
use	O
base	O
10	O
.	O
(	O
1.44	O
)	O
which	O
may	O
be	O
solved	O
for	O
n	O
iteratively	O
,	O
the	O
(	O
cid:12	O
)	O
rst	O
iteration	O
starting	O
from	O
^n1	O
=	O
68	O
:	O
(	O
^n2	O
(	O
cid:0	O
)	O
1	O
)	O
=2	O
’	O
(	O
cid:0	O
)	O
15	O
+	O
1:7	O
(	O
cid:0	O
)	O
0:44	O
=	O
29:9	O
)	O
^n2	O
’	O
60:9	O
:	O
(	O
1.45	O
)	O
this	O
answer	O
is	O
found	O
to	O
be	O
stable	O
,	O
so	O
n	O
’	O
61	O
is	O
the	O
blocklength	O
at	O
which	O
pb	O
’	O
10	O
(	O
cid:0	O
)	O
15.	O
solution	O
to	O
exercise	O
1.6	O
(	O
p.13	O
)	O
.	O
(	O
a	O
)	O
the	O
probability	B
of	I
block	I
error	I
of	O
the	O
hamming	O
code	B
is	O
a	O
sum	O
of	O
six	B
terms	O
{	O
the	O
probabilities	O
that	O
2	O
,	O
3	O
,	O
4	O
,	O
5	O
,	O
6	O
,	O
or	O
7	O
errors	B
occur	O
in	O
one	O
block	B
.	O
pb	O
=	O
7	O
xr=2	O
(	O
cid:18	O
)	O
7	O
r	O
(	O
cid:19	O
)	O
f	O
r	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
7	O
(	O
cid:0	O
)	O
r	O
:	O
to	O
leading	O
order	O
,	O
this	O
goes	O
as	O
pb	O
’	O
(	O
cid:18	O
)	O
7	O
2	O
(	O
cid:19	O
)	O
f	O
2	O
=	O
21f	O
2	O
:	O
(	O
1.46	O
)	O
(	O
1.47	O
)	O
(	O
b	O
)	O
the	O
probability	O
of	O
bit	O
error	O
of	O
the	O
hamming	O
code	B
is	O
smaller	O
than	O
the	O
probability	B
of	I
block	I
error	I
because	O
a	O
block	B
error	O
rarely	O
corrupts	O
all	O
bits	O
in	O
the	O
decoded	O
block	B
.	O
the	O
leading-order	O
behaviour	O
is	O
found	O
by	O
considering	O
the	O
outcome	O
in	O
the	O
most	O
probable	O
case	O
where	O
the	O
noise	B
vector	O
has	O
weight	B
two	O
.	O
the	O
decoder	B
will	O
erroneously	O
(	O
cid:13	O
)	O
ip	O
a	O
third	O
bit	O
,	O
so	O
that	O
the	O
modi	O
(	O
cid:12	O
)	O
ed	O
received	O
vector	O
(	O
of	O
length	O
7	O
)	O
di	O
(	O
cid:11	O
)	O
ers	O
in	O
three	O
bits	O
from	O
the	O
transmitted	O
vector	O
.	O
that	O
means	O
,	O
if	O
we	O
average	B
over	O
all	O
seven	O
bits	O
,	O
the	O
probability	B
that	O
a	O
randomly	O
chosen	O
bit	B
is	O
(	O
cid:13	O
)	O
ipped	O
is	O
3=7	O
times	O
the	O
block	B
error	O
probability	B
,	O
to	O
leading	O
order	O
.	O
now	O
,	O
what	O
we	O
really	O
care	O
about	O
is	O
the	O
probability	B
that	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
18	O
1	O
|	O
introduction	O
to	O
information	B
theory	I
a	O
source	O
bit	O
is	O
(	O
cid:13	O
)	O
ipped	O
.	O
are	O
parity	B
bits	O
or	O
source	O
bits	O
more	O
likely	O
to	O
be	O
among	O
these	O
three	O
(	O
cid:13	O
)	O
ipped	O
bits	O
,	O
or	O
are	O
all	O
seven	O
bits	O
equally	O
likely	O
to	O
be	O
corrupted	O
when	O
the	O
noise	B
vector	O
has	O
weight	B
two	O
?	O
the	O
hamming	O
code	B
is	O
in	O
fact	O
completely	O
symmetric	B
in	O
the	O
protection	O
it	O
a	O
(	O
cid:11	O
)	O
ords	O
to	O
the	O
seven	O
bits	O
(	O
assuming	O
a	O
binary	B
symmetric	I
channel	I
)	O
.	O
[	O
this	O
symmetry	O
can	O
be	O
proved	O
by	O
showing	O
that	O
the	O
role	O
of	O
a	O
parity	B
bit	O
can	O
be	O
exchanged	O
with	O
a	O
source	O
bit	O
and	O
the	O
resulting	O
code	B
is	O
still	O
a	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
;	O
see	O
below	O
.	O
]	O
the	O
probability	B
that	O
any	O
one	O
bit	B
ends	O
up	O
corrupted	O
is	O
the	O
same	O
for	O
all	O
seven	O
bits	O
.	O
so	O
the	O
probability	O
of	O
bit	O
error	O
(	O
for	O
the	O
source	O
bits	O
)	O
is	O
simply	O
three	O
sevenths	O
of	O
the	O
probability	O
of	O
block	O
error	O
.	O
pb	O
’	O
3	O
7	O
pb	O
’	O
9f	O
2	O
:	O
(	O
1.48	O
)	O
symmetry	O
of	O
the	O
hamming	O
(	O
7	O
;	O
4	O
)	O
code	B
to	O
prove	O
that	O
the	O
(	O
7	O
;	O
4	O
)	O
code	B
protects	O
all	O
bits	O
equally	O
,	O
we	O
start	O
from	O
the	O
parity-	O
check	O
matrix	B
h	O
=2	O
4	O
h	O
=2	O
4	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
(	O
1.49	O
)	O
(	O
1.50	O
)	O
the	O
symmetry	O
among	O
the	O
seven	O
transmitted	O
bits	O
will	O
be	O
easiest	O
to	O
see	O
if	O
we	O
reorder	O
the	O
seven	O
bits	O
using	O
the	O
permutation	B
(	O
t1t2t3t4t5t6t7	O
)	O
!	O
(	O
t5t2t3t4t1t6t7	O
)	O
.	O
then	O
we	O
can	O
rewrite	O
h	O
thus	O
:	O
now	O
,	O
if	O
we	O
take	O
any	O
two	O
parity	B
constraints	O
that	O
t	O
satis	O
(	O
cid:12	O
)	O
es	O
and	O
add	O
them	O
together	O
,	O
we	O
get	O
another	O
parity	B
constraint	O
.	O
for	O
example	O
,	O
row	O
1	O
asserts	O
t5	O
+	O
t2	O
+	O
t3	O
+	O
t1	O
=	O
even	O
,	O
and	O
row	O
2	O
asserts	O
t2	O
+	O
t3	O
+	O
t4	O
+	O
t6	O
=	O
even	O
,	O
and	O
the	O
sum	O
of	O
these	O
two	O
constraints	O
is	O
t5	O
+	O
2t2	O
+	O
2t3	O
+	O
t1	O
+	O
t4	O
+	O
t6	O
=	O
even	O
;	O
(	O
1.51	O
)	O
we	O
can	O
drop	O
the	O
terms	O
2t2	O
and	O
2t3	O
,	O
since	O
they	O
are	O
even	O
whatever	O
t2	O
and	O
t3	O
are	O
;	O
thus	O
we	O
have	O
derived	O
the	O
parity	B
constraint	O
t5	O
+	O
t1	O
+	O
t4	O
+	O
t6	O
=	O
even	O
,	O
which	O
we	O
can	O
if	O
we	O
wish	O
add	O
into	O
the	O
parity-check	B
matrix	I
as	O
a	O
fourth	O
row	O
.	O
[	O
the	O
set	B
of	O
vectors	B
satisfying	O
ht	O
=	O
0	O
will	O
not	O
be	O
changed	O
.	O
]	O
we	O
thus	O
de	O
(	O
cid:12	O
)	O
ne	O
h0	O
=2	O
664	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
1	O
0	O
:	O
(	O
1.52	O
)	O
the	O
fourth	O
row	O
is	O
the	O
sum	O
(	O
modulo	O
two	O
)	O
of	O
the	O
top	O
two	O
rows	O
.	O
notice	O
that	O
the	O
second	O
,	O
third	O
,	O
and	O
fourth	O
rows	O
are	O
all	O
cyclic	B
shifts	O
of	O
the	O
top	O
row	O
.	O
if	O
,	O
having	O
added	O
the	O
fourth	O
redundant	O
constraint	O
,	O
we	O
drop	O
the	O
(	O
cid:12	O
)	O
rst	O
constraint	O
,	O
we	O
obtain	O
a	O
new	O
parity-check	B
matrix	I
h00	O
,	O
h00	O
=2	O
4	O
0	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
1	O
0	O
(	O
1.53	O
)	O
which	O
still	O
satis	O
(	O
cid:12	O
)	O
es	O
h00t	O
=	O
0	O
for	O
all	O
codewords	O
,	O
and	O
which	O
looks	O
just	O
like	O
the	O
starting	O
h	O
in	O
(	O
1.50	O
)	O
,	O
except	O
that	O
all	O
the	O
columns	O
have	O
shifted	O
along	O
one	O
3	O
5	O
:	O
3	O
5	O
:	O
3	O
775	O
3	O
5	O
;	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
1.6	O
:	O
solutions	O
19	O
to	O
the	O
right	O
,	O
and	O
the	O
rightmost	O
column	O
has	O
reappeared	O
at	O
the	O
left	O
(	O
a	O
cyclic	B
permutation	O
of	O
the	O
columns	O
)	O
.	O
this	O
establishes	O
the	O
symmetry	O
among	O
the	O
seven	O
bits	O
.	O
iterating	O
the	O
above	O
procedure	O
(	O
cid:12	O
)	O
ve	O
more	O
times	O
,	O
we	O
can	O
make	O
a	O
total	O
of	O
seven	O
di	O
(	O
cid:11	O
)	O
erent	O
h	O
matrices	B
for	O
the	O
same	O
original	O
code	B
,	O
each	O
of	O
which	O
assigns	O
each	O
bit	B
to	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
role	O
.	O
we	O
may	O
also	O
construct	O
the	O
super-redundant	O
seven-row	O
parity-check	B
matrix	I
for	O
the	O
code	B
,	O
h000	O
=	O
:	O
(	O
1.54	O
)	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
1	O
0	O
0	O
1	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
1	O
2	O
666666664	O
3	O
777777775	O
this	O
matrix	B
is	O
‘	O
redundant	O
’	O
in	O
the	O
sense	O
that	O
the	O
space	O
spanned	O
by	O
its	O
rows	O
is	O
only	O
three-dimensional	O
,	O
not	O
seven	O
.	O
this	O
matrix	B
is	O
also	O
a	O
cyclic	B
matrix	O
.	O
every	O
row	O
is	O
a	O
cyclic	B
permutation	O
of	O
the	O
top	O
row	O
.	O
cyclic	B
codes	O
:	O
if	O
there	O
is	O
an	O
ordering	O
of	O
the	O
bits	O
t1	O
:	O
:	O
:	O
tn	O
such	O
that	O
a	O
linear	B
code	O
has	O
a	O
cyclic	B
parity-check	O
matrix	B
,	O
then	O
the	O
code	B
is	O
called	O
a	O
cyclic	B
code	O
.	O
the	O
codewords	O
of	O
such	O
a	O
code	B
also	O
have	O
cyclic	B
properties	O
:	O
any	O
cyclic	B
permutation	O
of	O
a	O
codeword	B
is	O
a	O
codeword	B
.	O
for	O
example	O
,	O
the	O
hamming	O
(	O
7	O
;	O
4	O
)	O
code	B
,	O
with	O
its	O
bits	O
ordered	O
as	O
above	O
,	O
consists	O
of	O
all	O
seven	O
cyclic	B
shifts	O
of	O
the	O
codewords	O
1110100	O
and	O
1011000	O
,	O
and	O
the	O
codewords	O
0000000	O
and	O
1111111.	O
cyclic	B
codes	O
are	O
a	O
cornerstone	O
of	O
the	O
algebraic	O
approach	O
to	O
error-correcting	B
codes	I
.	O
we	O
won	O
’	O
t	O
use	O
them	O
again	O
in	O
this	O
book	O
,	O
however	O
,	O
as	O
they	O
have	O
been	O
superceded	O
by	O
sparse-graph	O
codes	O
(	O
part	O
vi	O
)	O
.	O
solution	O
to	O
exercise	O
1.7	O
(	O
p.13	O
)	O
.	O
there	O
are	O
(	O
cid:12	O
)	O
fteen	O
non-zero	O
noise	B
vectors	O
which	O
give	O
the	O
all-zero	O
syndrome	B
;	O
these	O
are	O
precisely	O
the	O
(	O
cid:12	O
)	O
fteen	O
non-zero	O
codewords	O
of	O
the	O
hamming	O
code	B
.	O
notice	O
that	O
because	O
the	O
hamming	O
code	B
is	O
linear	B
,	O
the	O
sum	O
of	O
any	O
two	O
codewords	O
is	O
a	O
codeword	B
.	O
graphs	O
corresponding	O
to	O
codes	O
solution	O
to	O
exercise	O
1.9	O
(	O
p.14	O
)	O
.	O
when	O
answering	O
this	O
question	O
,	O
you	O
will	O
prob-	O
ably	O
(	O
cid:12	O
)	O
nd	O
that	O
it	O
is	O
easier	O
to	O
invent	O
new	O
codes	O
than	O
to	O
(	O
cid:12	O
)	O
nd	O
optimal	B
decoders	O
for	O
them	O
.	O
there	O
are	O
many	O
ways	O
to	O
design	O
codes	O
,	O
and	O
what	O
follows	O
is	O
just	O
one	O
possible	O
train	O
of	O
thought	O
.	O
we	O
make	O
a	O
linear	B
block	I
code	I
that	O
is	O
similar	O
to	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
,	O
but	O
bigger	O
.	O
many	O
codes	O
can	O
be	O
conveniently	O
expressed	O
in	O
terms	O
of	O
graphs	O
.	O
in	O
(	O
cid:12	O
)	O
g-	O
ure	O
1.13	O
,	O
we	O
introduced	O
a	O
pictorial	O
representation	O
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
.	O
if	O
we	O
replace	O
that	O
(	O
cid:12	O
)	O
gure	O
’	O
s	O
big	O
circles	O
,	O
each	O
of	O
which	O
shows	O
that	O
the	O
parity	B
of	O
four	O
particular	O
bits	O
is	O
even	O
,	O
by	O
a	O
‘	O
parity-check	O
node	O
’	O
that	O
is	O
connected	O
to	O
the	O
four	O
bits	O
,	O
then	O
we	O
obtain	O
the	O
representation	O
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
by	O
a	O
bipartite	B
graph	I
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
1.20.	O
the	O
7	O
circles	O
are	O
the	O
7	O
transmitted	O
bits	O
.	O
the	O
3	O
squares	O
are	O
the	O
parity-check	B
nodes	I
(	O
not	O
to	O
be	O
confused	O
with	O
the	O
3	O
parity-check	B
bits	I
,	O
which	O
are	O
the	O
three	O
most	O
peripheral	O
circles	O
)	O
.	O
the	O
graph	B
is	O
a	O
‘	O
bipartite	O
’	O
graph	B
because	O
its	O
nodes	O
fall	O
into	O
two	O
classes	O
{	O
bits	O
and	O
checks	O
figure	O
1.20.	O
the	O
graph	B
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
.	O
the	O
7	O
circles	O
are	O
the	O
bit	B
nodes	O
and	O
the	O
3	O
squares	O
are	O
the	O
parity-check	B
nodes	I
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
20	O
1	O
|	O
introduction	O
to	O
information	B
theory	I
{	O
and	O
there	O
are	O
edges	O
only	O
between	O
nodes	O
in	O
di	O
(	O
cid:11	O
)	O
erent	O
classes	O
.	O
the	O
graph	B
and	O
the	O
code	B
’	O
s	O
parity-check	B
matrix	I
(	O
1.30	O
)	O
are	O
simply	O
related	O
to	O
each	O
other	O
:	O
each	O
parity-check	O
node	O
corresponds	O
to	O
a	O
row	O
of	O
h	O
and	O
each	O
bit	B
node	O
corresponds	O
to	O
a	O
column	O
of	O
h	O
;	O
for	O
every	O
1	O
in	O
h	O
,	O
there	O
is	O
an	O
edge	B
between	O
the	O
corresponding	O
pair	O
of	O
nodes	O
.	O
having	O
noticed	O
this	O
connection	B
between	I
linear	O
codes	O
and	O
graphs	O
,	O
one	O
way	O
to	O
invent	O
linear	B
codes	I
is	O
simply	O
to	O
think	O
of	O
a	O
bipartite	B
graph	I
.	O
for	O
example	O
,	O
a	O
pretty	O
bipartite	B
graph	I
can	O
be	O
obtained	O
from	O
a	O
dodecahedron	B
by	O
calling	O
the	O
vertices	O
of	O
the	O
dodecahedron	O
the	O
parity-check	B
nodes	I
,	O
and	O
putting	O
a	O
transmitted	O
bit	B
on	O
each	O
edge	B
in	O
the	O
dodecahedron	B
.	O
this	O
construction	B
de	O
(	O
cid:12	O
)	O
nes	O
a	O
parity-	O
check	O
matrix	B
in	O
which	O
every	O
column	O
has	O
weight	B
2	O
and	O
every	O
row	O
has	O
weight	B
3	O
.	O
[	O
the	O
weight	B
of	O
a	O
binary	O
vector	O
is	O
the	O
number	O
of	O
1s	O
it	O
contains	O
.	O
]	O
this	O
code	B
has	O
n	O
=	O
30	O
bits	O
,	O
and	O
it	O
appears	O
to	O
have	O
mapparent	O
=	O
20	O
parity-	O
check	O
constraints	O
.	O
actually	O
,	O
there	O
are	O
only	O
m	O
=	O
19	O
independent	O
constraints	O
;	O
the	O
20th	O
constraint	O
is	O
redundant	O
(	O
that	O
is	O
,	O
if	O
19	O
constraints	O
are	O
satis	O
(	O
cid:12	O
)	O
ed	O
,	O
then	O
the	O
20th	O
is	O
automatically	O
satis	O
(	O
cid:12	O
)	O
ed	O
)	O
;	O
so	O
the	O
number	O
of	O
source	O
bits	O
is	O
k	O
=	O
n	O
(	O
cid:0	O
)	O
m	O
=	O
11.	O
the	O
code	B
is	O
a	O
(	O
30	O
;	O
11	O
)	O
code	B
.	O
it	O
is	O
hard	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
decoding	B
algorithm	O
for	O
this	O
code	B
,	O
but	O
we	O
can	O
estimate	O
its	O
probability	B
of	I
error	I
by	O
(	O
cid:12	O
)	O
nding	O
its	O
lowest-weight	O
codewords	O
.	O
if	O
we	O
(	O
cid:13	O
)	O
ip	O
all	O
the	O
bits	O
surrounding	O
one	O
face	O
of	O
the	O
original	O
dodecahedron	B
,	O
then	O
all	O
the	O
parity	B
checks	O
will	O
be	O
satis	O
(	O
cid:12	O
)	O
ed	O
;	O
so	O
the	O
code	B
has	O
12	O
codewords	O
of	O
weight	O
5	O
,	O
one	O
for	O
each	O
face	O
.	O
since	O
the	O
lowest-weight	O
codewords	O
have	O
weight	B
5	O
,	O
we	O
say	O
that	O
the	O
code	B
has	O
distance	B
d	O
=	O
5	O
;	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
had	O
distance	B
3	O
and	O
could	O
correct	O
all	O
single	O
bit-	O
(	O
cid:13	O
)	O
ip	O
errors	B
.	O
a	O
code	B
with	O
distance	B
5	O
can	O
correct	O
all	O
double	O
bit-	O
(	O
cid:13	O
)	O
ip	O
errors	B
,	O
but	O
there	O
are	O
some	O
triple	O
bit-	O
(	O
cid:13	O
)	O
ip	O
errors	B
that	O
it	O
can	O
not	O
correct	O
.	O
so	O
the	O
error	B
probability	I
of	O
this	O
code	B
,	O
assuming	O
a	O
binary	B
symmetric	I
channel	I
,	O
will	O
be	O
dominated	O
,	O
at	O
least	O
for	O
low	O
noise	B
levels	O
f	O
,	O
by	O
a	O
term	O
of	O
order	O
f	O
3	O
,	O
perhaps	O
something	O
like	O
12	O
(	O
cid:18	O
)	O
5	O
3	O
(	O
cid:19	O
)	O
f	O
3	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
27	O
:	O
(	O
1.55	O
)	O
of	O
course	O
,	O
there	O
is	O
no	O
obligation	O
to	O
make	O
codes	O
whose	O
graphs	O
can	O
be	O
rep-	O
resented	O
on	O
a	O
plane	O
,	O
as	O
this	O
one	O
can	O
;	O
the	O
best	O
linear	B
codes	I
,	O
which	O
have	O
simple	O
graphical	O
descriptions	O
,	O
have	O
graphs	O
that	O
are	O
more	O
tangled	O
,	O
as	O
illustrated	O
by	O
the	O
tiny	O
(	O
16	O
;	O
4	O
)	O
code	B
of	O
(	O
cid:12	O
)	O
gure	O
1.22.	O
furthermore	O
,	O
there	O
is	O
no	O
reason	O
for	O
sticking	O
to	O
linear	B
codes	I
;	O
indeed	O
some	O
nonlinear	B
codes	O
{	O
codes	O
whose	O
codewords	O
can	O
not	O
be	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
a	O
linear	B
equa-	O
tion	O
like	O
ht	O
=	O
0	O
{	O
have	O
very	B
good	I
properties	O
.	O
but	O
the	O
encoding	O
and	O
decoding	O
of	O
a	O
nonlinear	B
code	I
are	O
even	O
trickier	O
tasks	O
.	O
solution	O
to	O
exercise	O
1.10	O
(	O
p.14	O
)	O
.	O
code	B
and	O
decoding	B
it	O
with	O
syndrome	O
decoding	B
.	O
bits	O
,	O
then	O
the	O
number	O
of	O
possible	O
error	O
patterns	O
of	O
weight	O
up	O
to	O
two	O
is	O
first	O
let	O
’	O
s	O
assume	O
we	O
are	O
making	O
a	O
linear	B
if	O
there	O
are	O
n	O
transmitted	O
2	O
(	O
cid:19	O
)	O
+	O
(	O
cid:18	O
)	O
n	O
(	O
cid:18	O
)	O
n	O
0	O
(	O
cid:19	O
)	O
:	O
1	O
(	O
cid:19	O
)	O
+	O
(	O
cid:18	O
)	O
n	O
(	O
1.56	O
)	O
for	O
n	O
=	O
14	O
,	O
that	O
’	O
s	O
91	O
+	O
14	O
+	O
1	O
=	O
106	O
patterns	O
.	O
now	O
,	O
every	O
distinguishable	O
error	O
pattern	O
must	O
give	O
rise	O
to	O
a	O
distinct	O
syndrome	B
;	O
and	O
the	O
syndrome	B
is	O
a	O
list	O
of	O
m	O
bits	O
,	O
so	O
the	O
maximum	O
possible	O
number	O
of	O
syndromes	O
is	O
2m	O
.	O
for	O
a	O
(	O
14	O
;	O
8	O
)	O
code	B
,	O
m	O
=	O
6	O
,	O
so	O
there	O
are	O
at	O
most	O
26	O
=	O
64	O
syndromes	O
.	O
the	O
number	O
of	O
possible	O
error	O
patterns	O
of	O
weight	O
up	O
to	O
two	O
,	O
106	O
,	O
is	O
bigger	O
than	O
the	O
number	O
of	O
syndromes	O
,	O
64	O
,	O
so	O
we	O
can	O
immediately	O
rule	O
out	O
the	O
possibility	O
that	O
there	O
is	O
a	O
(	O
14	O
;	O
8	O
)	O
code	B
that	O
is	O
2-error-correcting	O
.	O
figure	O
1.21.	O
the	O
graph	B
de	O
(	O
cid:12	O
)	O
ning	O
the	O
(	O
30	O
;	O
11	O
)	O
dodecahedron	B
code	I
.	O
the	O
circles	O
are	O
the	O
30	O
transmitted	O
bits	O
and	O
the	O
triangles	O
are	O
the	O
20	O
parity	B
checks	O
.	O
one	O
parity	B
check	O
is	O
redundant	O
.	O
figure	O
1.22.	O
graph	B
of	O
a	O
rate-1/4	O
low-density	B
parity-check	I
code	I
(	O
gallager	O
code	B
)	O
with	O
blocklength	O
n	O
=	O
16	O
,	O
and	O
m	O
=	O
12	O
parity-check	B
constraints	I
.	O
each	O
white	B
circle	O
represents	O
a	O
transmitted	O
bit	B
.	O
each	O
bit	B
participates	O
in	O
j	O
=	O
3	O
constraints	O
,	O
represented	O
by	O
squares	O
.	O
the	O
edges	O
between	O
nodes	O
were	O
placed	O
at	O
random	B
.	O
(	O
see	O
chapter	O
47	O
for	O
more	O
.	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
1.6	O
:	O
solutions	O
21	O
the	O
same	O
counting	B
argument	I
works	O
(	O
cid:12	O
)	O
ne	O
for	O
nonlinear	O
codes	O
too	O
.	O
when	O
the	O
decoder	B
receives	O
r	O
=	O
t	O
+	O
n	O
,	O
his	O
aim	O
is	O
to	O
deduce	O
both	O
t	O
and	O
n	O
from	O
r.	O
if	O
it	O
is	O
the	O
case	O
that	O
the	O
sender	O
can	O
select	O
any	O
transmission	O
t	O
from	O
a	O
code	B
of	O
size	O
st	O
,	O
and	O
the	O
channel	B
can	O
select	O
any	O
noise	B
vector	O
from	O
a	O
set	B
of	O
size	O
sn	O
,	O
and	O
those	O
two	O
selections	O
can	O
be	O
recovered	O
from	O
the	O
received	O
bit	B
string	O
r	O
,	O
which	O
is	O
one	O
of	O
at	O
most	O
2n	O
possible	O
strings	O
,	O
then	O
it	O
must	O
be	O
the	O
case	O
that	O
so	O
,	O
for	O
a	O
(	O
n	O
;	O
k	O
)	O
two-error-correcting	O
code	B
,	O
whether	O
linear	B
or	O
nonlinear	B
,	O
stsn	O
(	O
cid:20	O
)	O
2n	O
:	O
2k	O
(	O
cid:20	O
)	O
(	O
cid:18	O
)	O
n	O
2	O
(	O
cid:19	O
)	O
+	O
(	O
cid:18	O
)	O
n	O
1	O
(	O
cid:19	O
)	O
+	O
(	O
cid:18	O
)	O
n	O
0	O
(	O
cid:19	O
)	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
2n	O
:	O
(	O
1.57	O
)	O
(	O
1.58	O
)	O
solution	O
to	O
exercise	O
1.11	O
(	O
p.14	O
)	O
.	O
there	O
are	O
various	O
strategies	O
for	O
making	O
codes	O
that	O
can	O
correct	O
multiple	O
errors	O
,	O
and	O
i	O
strongly	O
recommend	O
you	O
think	O
out	O
one	O
or	O
two	O
of	O
them	O
for	O
yourself	O
.	O
if	O
your	O
approach	O
uses	O
a	O
linear	B
code	O
,	O
e.g.	O
,	O
one	O
with	O
a	O
collection	O
of	O
m	O
parity	B
checks	O
,	O
it	O
is	O
helpful	O
to	O
bear	O
in	O
mind	O
the	O
counting	B
argument	I
given	O
in	O
the	O
previous	O
exercise	O
,	O
in	O
order	O
to	O
anticipate	O
how	O
many	O
parity	B
checks	O
,	O
m	O
,	O
you	O
might	O
need	O
.	O
examples	O
of	O
codes	O
that	O
can	O
correct	O
any	O
two	O
errors	B
are	O
the	O
(	O
30	O
;	O
11	O
)	O
dodeca-	O
hedron	O
code	B
on	O
page	O
20	O
,	O
and	O
the	O
(	O
15	O
;	O
6	O
)	O
pentagonful	B
code	I
to	O
be	O
introduced	O
on	O
p.221	O
.	O
further	O
simple	O
ideas	O
for	O
making	O
codes	O
that	O
can	O
correct	O
multiple	O
errors	O
from	O
codes	O
that	O
can	O
correct	O
only	O
one	O
error	O
are	O
discussed	O
in	O
section	O
13.7.	O
solution	O
to	O
exercise	O
1.12	O
(	O
p.16	O
)	O
.	O
the	O
probability	B
of	I
error	I
of	O
r2	O
order	O
,	O
3	O
is	O
,	O
to	O
leading	O
pb	O
(	O
r2	O
3	O
)	O
’	O
3	O
[	O
pb	O
(	O
r3	O
)	O
]	O
2	O
=	O
3	O
(	O
3f	O
2	O
)	O
2	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
=	O
27f	O
4	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
;	O
(	O
1.59	O
)	O
whereas	O
the	O
probability	B
of	I
error	I
of	O
r9	O
is	O
dominated	O
by	O
the	O
probability	O
of	O
(	O
cid:12	O
)	O
ve	O
(	O
cid:13	O
)	O
ips	O
,	O
pb	O
(	O
r9	O
)	O
’	O
(	O
cid:18	O
)	O
9	O
5	O
(	O
cid:19	O
)	O
f	O
5	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
4	O
’	O
126f	O
5	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
:	O
(	O
1.60	O
)	O
the	O
r2	O
tors	O
of	O
weight	O
four	O
that	O
cause	O
it	O
to	O
make	O
a	O
decoding	B
error	O
.	O
3	O
decoding	B
procedure	O
is	O
therefore	O
suboptimal	O
,	O
since	O
there	O
are	O
noise	B
vec-	O
it	O
has	O
the	O
advantage	O
,	O
however	O
,	O
of	O
requiring	O
smaller	O
computational	O
re-	O
sources	O
:	O
only	O
memorization	O
of	O
three	O
bits	O
,	O
and	O
counting	O
up	O
to	O
three	O
,	O
rather	O
than	O
counting	B
up	O
to	O
nine	O
.	O
this	O
simple	O
code	O
illustrates	O
an	O
important	O
concept	O
.	O
concatenated	B
codes	O
are	O
widely	O
used	O
in	O
practice	O
because	O
concatenation	B
allows	O
large	O
codes	O
to	O
be	O
implemented	O
using	O
simple	O
encoding	O
and	O
decoding	O
hardware	O
.	O
some	O
of	O
the	O
best	O
known	O
practical	B
codes	O
are	O
concatenated	B
codes	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
2	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
this	O
chapter	O
,	O
and	O
its	O
sibling	O
,	O
chapter	O
8	O
,	O
devote	O
some	O
time	O
to	O
notation	B
.	O
just	O
as	O
the	O
white	B
knight	O
distinguished	O
between	O
the	O
song	O
,	O
the	O
name	O
of	O
the	O
song	O
,	O
and	O
what	O
the	O
name	O
of	O
the	O
song	O
was	O
called	O
(	O
carroll	O
,	O
1998	O
)	O
,	O
we	O
will	O
sometimes	O
need	O
to	O
be	O
careful	O
to	O
distinguish	O
between	O
a	O
random	B
variable	I
,	O
the	O
value	O
of	O
the	O
random	O
variable	O
,	O
and	O
the	O
proposition	O
that	O
asserts	O
that	O
the	O
random	B
variable	I
has	O
a	O
particular	O
value	O
.	O
in	O
any	O
particular	O
chapter	O
,	O
however	O
,	O
i	O
will	O
use	O
the	O
most	O
simple	O
and	O
friendly	O
notation	B
possible	O
,	O
at	O
the	O
risk	O
of	O
upsetting	O
pure-minded	O
readers	O
.	O
for	O
example	O
,	O
if	O
something	O
is	O
‘	O
true	O
with	O
probability	O
1	O
’	O
,	O
i	O
will	O
usually	O
simply	O
say	O
that	O
it	O
is	O
‘	O
true	O
’	O
.	O
2.1	O
probabilities	O
and	O
ensembles	O
an	O
ensemble	B
x	O
is	O
a	O
triple	O
(	O
x	O
;	O
ax	O
;	O
px	O
)	O
,	O
where	O
the	O
outcome	O
x	O
is	O
the	O
value	O
of	O
a	O
random	B
variable	I
,	O
which	O
takes	O
on	O
one	O
of	O
a	O
set	B
of	O
possible	O
values	O
,	O
ax	O
=	O
fa1	O
;	O
a2	O
;	O
:	O
:	O
:	O
;	O
ai	O
;	O
:	O
:	O
:	O
;	O
aig	O
,	O
having	O
probabilities	O
px	O
=	O
fp1	O
;	O
p2	O
;	O
:	O
:	O
:	O
;	O
pig	O
,	O
with	O
p	O
(	O
x	O
=	O
ai	O
)	O
=	O
pi	O
,	O
pi	O
(	O
cid:21	O
)	O
0	O
and	O
pai2ax	O
the	O
name	O
a	O
is	O
mnemonic	O
for	O
‘	O
alphabet	O
’	O
.	O
one	O
example	O
of	O
an	O
ensemble	B
is	O
a	O
letter	O
that	O
is	O
randomly	O
selected	O
from	O
an	O
english	O
document	O
.	O
this	O
ensemble	B
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
2.1.	O
there	O
are	O
twenty-seven	O
possible	O
letters	O
:	O
a	O
{	O
z	O
,	O
and	O
a	O
space	O
character	O
‘	O
-	O
’	O
.	O
p	O
(	O
x	O
=	O
ai	O
)	O
=	O
1.	O
abbreviations	O
.	O
briefer	O
notation	B
will	O
sometimes	O
be	O
used	O
.	O
for	O
example	O
,	O
p	O
(	O
x	O
=	O
ai	O
)	O
may	O
be	O
written	O
as	O
p	O
(	O
ai	O
)	O
or	O
p	O
(	O
x	O
)	O
.	O
probability	O
of	O
a	O
subset	B
.	O
if	O
t	O
is	O
a	O
subset	B
of	O
ax	O
then	O
:	O
p	O
(	O
t	O
)	O
=	O
p	O
(	O
x2	O
t	O
)	O
=	O
xai2t	O
p	O
(	O
x	O
=	O
ai	O
)	O
:	O
(	O
2.1	O
)	O
for	O
example	O
,	O
fa	O
;	O
e	O
;	O
i	O
;	O
o	O
;	O
ug	O
,	O
then	O
if	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
v	O
to	O
be	O
vowels	O
from	O
(	O
cid:12	O
)	O
gure	O
2.1	O
,	O
v	O
=	O
p	O
(	O
v	O
)	O
=	O
0:06	O
+	O
0:09	O
+	O
0:06	O
+	O
0:07	O
+	O
0:03	O
=	O
0:31	O
:	O
(	O
2.2	O
)	O
a	O
joint	B
ensemble	I
xy	O
is	O
an	O
ensemble	B
in	O
which	O
each	O
outcome	O
is	O
an	O
ordered	O
pair	O
x	O
;	O
y	O
with	O
x	O
2	O
ax	O
=	O
fa1	O
;	O
:	O
:	O
:	O
;	O
aig	O
and	O
y	O
2	O
ay	O
=	O
fb1	O
;	O
:	O
:	O
:	O
;	O
bjg	O
.	O
we	O
call	O
p	O
(	O
x	O
;	O
y	O
)	O
the	O
joint	B
probability	O
of	O
x	O
and	O
y.	O
commas	O
are	O
optional	O
when	O
writing	B
ordered	O
pairs	O
,	O
so	O
xy	O
,	O
x	O
;	O
y.	O
n.b	O
.	O
in	O
a	O
joint	B
ensemble	I
xy	O
the	O
two	O
variables	O
are	O
not	O
necessarily	O
inde-	O
pendent	O
.	O
22	O
i	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
13	O
14	O
15	O
16	O
17	O
18	O
19	O
20	O
21	O
22	O
23	O
24	O
25	O
26	O
27	O
ai	O
pi	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
{	O
0.0575	O
0.0128	O
0.0263	O
0.0285	O
0.0913	O
0.0173	O
0.0133	O
0.0313	O
0.0599	O
0.0006	O
0.0084	O
0.0335	O
0.0235	O
0.0596	O
0.0689	O
0.0192	O
0.0008	O
0.0508	O
0.0567	O
0.0706	O
0.0334	O
0.0069	O
0.0119	O
0.0073	O
0.0164	O
0.0007	O
0.1928	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
{	O
figure	O
2.1.	O
probability	B
distribution	O
over	O
the	O
27	O
outcomes	O
for	O
a	O
randomly	O
selected	O
letter	O
in	O
an	O
english	O
language	O
document	O
(	O
estimated	O
from	O
the	O
frequently	O
asked	O
questions	O
manual	O
for	O
linux	O
)	O
.	O
the	O
picture	O
shows	O
the	O
probabilities	O
by	O
the	O
areas	O
of	O
white	O
squares	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
2.1	O
:	O
probabilities	O
and	O
ensembles	O
23	O
figure	O
2.2.	O
the	O
probability	B
distribution	O
over	O
the	O
27	O
(	O
cid:2	O
)	O
27	O
possible	O
bigrams	O
xy	O
in	O
an	O
english	O
language	O
document	O
,	O
the	O
frequently	O
asked	O
questions	O
manual	O
for	O
linux	O
.	O
x	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
{	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
{	O
y	O
marginal	B
probability	I
.	O
we	O
can	O
obtain	O
the	O
marginal	B
probability	I
p	O
(	O
x	O
)	O
from	O
the	O
joint	B
probability	O
p	O
(	O
x	O
;	O
y	O
)	O
by	O
summation	O
:	O
p	O
(	O
x	O
=	O
ai	O
)	O
(	O
cid:17	O
)	O
xy2ay	O
p	O
(	O
x	O
=	O
ai	O
;	O
y	O
)	O
:	O
(	O
2.3	O
)	O
similarly	O
,	O
using	O
briefer	O
notation	B
,	O
the	O
marginal	B
probability	I
of	O
y	O
is	O
:	O
p	O
(	O
y	O
)	O
(	O
cid:17	O
)	O
xx2ax	O
p	O
(	O
x	O
;	O
y	O
)	O
:	O
(	O
2.4	O
)	O
conditional	B
probability	O
p	O
(	O
x	O
=	O
ai	O
j	O
y	O
=	O
bj	O
)	O
(	O
cid:17	O
)	O
p	O
(	O
x	O
=	O
ai	O
;	O
y	O
=	O
bj	O
)	O
p	O
(	O
y	O
=	O
bj	O
)	O
if	O
p	O
(	O
y	O
=	O
bj	O
)	O
6=	O
0	O
.	O
(	O
2.5	O
)	O
[	O
if	O
p	O
(	O
y	O
=	O
bj	O
)	O
=	O
0	O
then	O
p	O
(	O
x	O
=	O
ai	O
j	O
y	O
=	O
bj	O
)	O
is	O
unde	O
(	O
cid:12	O
)	O
ned	O
.	O
]	O
we	O
pronounce	O
p	O
(	O
x	O
=	O
ai	O
j	O
y	O
=	O
bj	O
)	O
‘	O
the	O
probability	B
that	O
x	O
equals	O
ai	O
,	O
given	O
y	O
equals	O
bj	O
’	O
.	O
example	O
2.1.	O
an	O
example	O
of	O
a	O
joint	B
ensemble	I
is	O
the	O
ordered	O
pair	O
xy	O
consisting	O
of	O
two	O
successive	O
letters	O
in	O
an	O
english	O
document	O
.	O
the	O
possible	O
outcomes	O
are	O
ordered	O
pairs	O
such	O
as	O
aa	O
,	O
ab	O
,	O
ac	O
,	O
and	O
zz	O
;	O
of	O
these	O
,	O
we	O
might	O
expect	O
ab	O
and	O
ac	O
to	O
be	O
more	O
probable	O
than	O
aa	O
and	O
zz	O
.	O
an	O
estimate	O
of	O
the	O
joint	O
probability	B
distribution	O
for	O
two	O
neighbouring	O
characters	O
is	O
shown	O
graphically	O
in	O
(	O
cid:12	O
)	O
gure	O
2.2.	O
this	O
joint	B
ensemble	I
has	O
the	O
special	O
property	O
that	O
its	O
two	O
marginal	B
dis-	O
tributions	O
,	O
p	O
(	O
x	O
)	O
and	O
p	O
(	O
y	O
)	O
,	O
are	O
identical	O
.	O
they	O
are	O
both	O
equal	O
to	O
the	O
monogram	O
distribution	B
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
2.1.	O
from	O
this	O
joint	B
ensemble	I
p	O
(	O
x	O
;	O
y	O
)	O
we	O
can	O
obtain	O
conditional	B
distributions	O
,	O
p	O
(	O
y	O
j	O
x	O
)	O
and	O
p	O
(	O
xj	O
y	O
)	O
,	O
by	O
normalizing	O
the	O
rows	O
and	O
columns	O
,	O
respectively	O
(	O
(	O
cid:12	O
)	O
gure	O
2.3	O
)	O
.	O
the	O
probability	B
p	O
(	O
y	O
j	O
x	O
=	O
q	O
)	O
is	O
the	O
probability	B
distribution	O
of	O
the	O
second	O
letter	O
given	O
that	O
the	O
(	O
cid:12	O
)	O
rst	O
letter	O
is	O
a	O
q.	O
as	O
you	O
can	O
see	O
in	O
(	O
cid:12	O
)	O
gure	O
2.3a	O
,	O
the	O
two	O
most	O
probable	O
values	O
for	O
the	O
second	O
letter	O
y	O
given	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
24	O
x	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
{	O
2	O
|	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
figure	O
2.3.	O
conditional	B
probability	O
distributions	O
.	O
(	O
a	O
)	O
p	O
(	O
y	O
j	O
x	O
)	O
:	O
each	O
row	O
shows	O
the	O
conditional	B
distribution	O
of	O
the	O
second	O
letter	O
,	O
y	O
,	O
given	O
the	O
(	O
cid:12	O
)	O
rst	O
letter	O
,	O
x	O
,	O
in	O
a	O
bigram	O
xy	O
.	O
(	O
b	O
)	O
p	O
(	O
xj	O
y	O
)	O
:	O
each	O
column	O
shows	O
the	O
conditional	B
distribution	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
letter	O
,	O
x	O
,	O
given	O
the	O
second	O
letter	O
,	O
y.	O
x	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
{	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
{	O
y	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
{	O
y	O
(	O
a	O
)	O
p	O
(	O
y	O
j	O
x	O
)	O
(	O
b	O
)	O
p	O
(	O
xj	O
y	O
)	O
that	O
the	O
(	O
cid:12	O
)	O
rst	O
letter	O
x	O
is	O
q	O
are	O
u	O
and	O
-	O
.	O
(	O
the	O
space	O
is	O
common	O
after	O
q	O
because	O
the	O
source	O
document	O
makes	O
heavy	O
use	O
of	O
the	O
word	O
faq	O
.	O
)	O
the	O
probability	B
p	O
(	O
xj	O
y	O
=	O
u	O
)	O
is	O
the	O
probability	B
distribution	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
letter	O
x	O
given	O
that	O
the	O
second	O
letter	O
y	O
is	O
a	O
u.	O
as	O
you	O
can	O
see	O
in	O
(	O
cid:12	O
)	O
gure	O
2.3b	O
the	O
two	O
most	O
probable	O
values	O
for	O
x	O
given	O
y	O
=	O
u	O
are	O
n	O
and	O
o.	O
rather	O
than	O
writing	B
down	O
the	O
joint	B
probability	O
directly	O
,	O
we	O
often	O
de	O
(	O
cid:12	O
)	O
ne	O
an	O
ensemble	B
in	O
terms	O
of	O
a	O
collection	O
of	O
conditional	O
probabilities	O
.	O
the	O
following	O
rules	B
of	O
probability	B
theory	O
will	O
be	O
useful	O
.	O
(	O
h	O
denotes	O
assumptions	B
on	O
which	O
the	O
probabilities	O
are	O
based	O
.	O
)	O
product	O
rule	O
{	O
obtained	O
from	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
conditional	O
probability	B
:	O
p	O
(	O
x	O
;	O
y	O
jh	O
)	O
=	O
p	O
(	O
xj	O
y	O
;	O
h	O
)	O
p	O
(	O
y	O
jh	O
)	O
=	O
p	O
(	O
y	O
j	O
x	O
;	O
h	O
)	O
p	O
(	O
xjh	O
)	O
:	O
(	O
2.6	O
)	O
this	O
rule	O
is	O
also	O
known	O
as	O
the	O
chain	B
rule	I
.	O
sum	B
rule	I
{	O
a	O
rewriting	O
of	O
the	O
marginal	O
probability	B
de	O
(	O
cid:12	O
)	O
nition	O
:	O
(	O
2.7	O
)	O
(	O
2.8	O
)	O
(	O
2.9	O
)	O
:	O
(	O
2.10	O
)	O
p	O
(	O
xj	O
y	O
;	O
h	O
)	O
p	O
(	O
y	O
jh	O
)	O
p	O
(	O
xjh	O
)	O
p	O
(	O
xj	O
y	O
;	O
h	O
)	O
p	O
(	O
y	O
jh	O
)	O
py0	O
p	O
(	O
xj	O
y0	O
;	O
h	O
)	O
p	O
(	O
y0	O
jh	O
)	O
p	O
(	O
xjh	O
)	O
=	O
xy	O
=	O
xy	O
p	O
(	O
x	O
;	O
y	O
jh	O
)	O
p	O
(	O
xj	O
y	O
;	O
h	O
)	O
p	O
(	O
y	O
jh	O
)	O
:	O
bayes	O
’	O
theorem	B
{	O
obtained	O
from	O
the	O
product	O
rule	O
:	O
p	O
(	O
y	O
j	O
x	O
;	O
h	O
)	O
=	O
=	O
independence	B
.	O
two	O
random	B
variables	O
x	O
and	O
y	O
are	O
independent	O
(	O
sometimes	O
written	O
x	O
?	O
y	O
)	O
if	O
and	O
only	O
if	O
p	O
(	O
x	O
;	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
:	O
(	O
2.11	O
)	O
exercise	O
2.2	O
.	O
[	O
1	O
,	O
p.40	O
]	O
are	O
the	O
random	B
variables	O
x	O
and	O
y	O
in	O
the	O
joint	B
ensemble	I
of	O
(	O
cid:12	O
)	O
gure	O
2.2	O
independent	O
?	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
2.2	O
:	O
the	O
meaning	O
of	O
probability	O
25	O
i	O
said	O
that	O
we	O
often	O
de	O
(	O
cid:12	O
)	O
ne	O
an	O
ensemble	B
in	O
terms	O
of	O
a	O
collection	O
of	O
condi-	O
tional	O
probabilities	O
.	O
the	O
following	O
example	O
illustrates	O
this	O
idea	O
.	O
example	O
2.3.	O
jo	O
has	O
a	O
test	B
for	O
a	O
nasty	O
disease	B
.	O
we	O
denote	O
jo	O
’	O
s	O
state	O
of	O
health	O
by	O
the	O
variable	O
a	O
and	O
the	O
test	B
result	O
by	O
b.	O
a	O
=	O
1	O
a	O
=	O
0	O
jo	O
has	O
the	O
disease	B
jo	O
does	O
not	O
have	O
the	O
disease	B
.	O
(	O
2.12	O
)	O
the	O
result	O
of	O
the	O
test	O
is	O
either	O
‘	O
positive	O
’	O
(	O
b	O
=	O
1	O
)	O
or	O
‘	O
negative	O
’	O
(	O
b	O
=	O
0	O
)	O
;	O
the	O
test	B
is	O
95	O
%	O
reliable	O
:	O
in	O
95	O
%	O
of	O
cases	O
of	O
people	O
who	O
really	O
have	O
the	O
disease	B
,	O
a	O
positive	O
result	O
is	O
returned	O
,	O
and	O
in	O
95	O
%	O
of	O
cases	O
of	O
people	O
who	O
do	O
not	O
have	O
the	O
disease	B
,	O
a	O
negative	O
result	O
is	O
obtained	O
.	O
the	O
(	O
cid:12	O
)	O
nal	O
piece	O
of	O
background	O
information	B
is	O
that	O
1	O
%	O
of	O
people	O
of	O
jo	O
’	O
s	O
age	O
and	O
background	O
have	O
the	O
disease	B
.	O
ok	O
{	O
jo	O
has	O
the	O
test	B
,	O
and	O
the	O
result	O
is	O
positive	O
.	O
what	O
is	O
the	O
probability	B
that	O
jo	O
has	O
the	O
disease	B
?	O
solution	O
.	O
we	O
write	O
down	O
all	O
the	O
provided	O
probabilities	O
.	O
the	O
test	B
reliability	O
speci	O
(	O
cid:12	O
)	O
es	O
the	O
conditional	B
probability	O
of	O
b	O
given	O
a	O
:	O
p	O
(	O
b	O
=	O
1j	O
a	O
=	O
1	O
)	O
=	O
0:95	O
p	O
(	O
b	O
=	O
0j	O
a	O
=	O
1	O
)	O
=	O
0:05	O
p	O
(	O
b	O
=	O
1j	O
a	O
=	O
0	O
)	O
=	O
0:05	O
p	O
(	O
b	O
=	O
0j	O
a	O
=	O
0	O
)	O
=	O
0:95	O
;	O
(	O
2.13	O
)	O
and	O
the	O
disease	B
prevalence	O
tells	O
us	O
about	O
the	O
marginal	B
probability	I
of	O
a	O
:	O
p	O
(	O
a	O
=	O
1	O
)	O
=	O
0:01	O
p	O
(	O
a	O
=	O
0	O
)	O
=	O
0:99	O
:	O
(	O
2.14	O
)	O
from	O
the	O
marginal	B
p	O
(	O
a	O
)	O
and	O
the	O
conditional	B
probability	O
p	O
(	O
bj	O
a	O
)	O
we	O
can	O
deduce	O
the	O
joint	B
probability	O
p	O
(	O
a	O
;	O
b	O
)	O
=	O
p	O
(	O
a	O
)	O
p	O
(	O
bj	O
a	O
)	O
and	O
any	O
other	O
probabilities	O
we	O
are	O
interested	O
in	O
.	O
for	O
example	O
,	O
by	O
the	O
sum	B
rule	I
,	O
the	O
marginal	B
probability	I
of	O
b	O
=	O
1	O
{	O
the	O
probability	O
of	O
getting	O
a	O
positive	O
result	O
{	O
is	O
p	O
(	O
b	O
=	O
1	O
)	O
=	O
p	O
(	O
b	O
=	O
1j	O
a	O
=	O
1	O
)	O
p	O
(	O
a	O
=	O
1	O
)	O
+	O
p	O
(	O
b	O
=	O
1j	O
a	O
=	O
0	O
)	O
p	O
(	O
a	O
=	O
0	O
)	O
:	O
(	O
2.15	O
)	O
jo	O
has	O
received	O
a	O
positive	O
result	O
b	O
=	O
1	O
and	O
is	O
interested	O
in	O
how	O
plausible	O
it	O
is	O
that	O
she	O
has	O
the	O
disease	B
(	O
i.e.	O
,	O
that	O
a	O
=	O
1	O
)	O
.	O
the	O
man	O
in	O
the	O
street	O
might	O
be	O
duped	O
by	O
the	O
statement	O
‘	O
the	O
test	B
is	O
95	O
%	O
reliable	O
,	O
so	O
jo	O
’	O
s	O
positive	O
result	O
implies	O
that	O
there	O
is	O
a	O
95	O
%	O
chance	O
that	O
jo	O
has	O
the	O
disease	B
’	O
,	O
but	O
this	O
is	O
incorrect	O
.	O
the	O
correct	O
solution	O
to	O
an	O
inference	B
problem	O
is	O
found	O
using	O
bayes	O
’	O
theorem	B
.	O
p	O
(	O
a	O
=	O
1j	O
b	O
=	O
1	O
)	O
=	O
=	O
p	O
(	O
b	O
=	O
1j	O
a	O
=	O
1	O
)	O
p	O
(	O
a	O
=	O
1	O
)	O
p	O
(	O
b	O
=	O
1j	O
a	O
=	O
1	O
)	O
p	O
(	O
a	O
=	O
1	O
)	O
+	O
p	O
(	O
b	O
=	O
1j	O
a	O
=	O
0	O
)	O
p	O
(	O
a	O
=	O
0	O
)	O
0:95	O
(	O
cid:2	O
)	O
0:01	O
0:95	O
(	O
cid:2	O
)	O
0:01	O
+	O
0:05	O
(	O
cid:2	O
)	O
0:99	O
=	O
0:16	O
:	O
(	O
2.16	O
)	O
(	O
2.17	O
)	O
(	O
2.18	O
)	O
so	O
in	O
spite	O
of	O
the	O
positive	O
result	O
,	O
the	O
probability	B
that	O
jo	O
has	O
the	O
disease	B
is	O
only	O
16	O
%	O
.	O
2	O
2.2	O
the	O
meaning	O
of	O
probability	O
probabilities	O
can	O
be	O
used	O
in	O
two	O
ways	O
.	O
probabilities	O
can	O
describe	O
frequencies	O
of	O
outcomes	O
in	O
random	O
experiments	O
,	O
but	O
giving	O
noncircular	O
de	O
(	O
cid:12	O
)	O
nitions	O
of	O
the	O
terms	O
‘	O
frequency	B
’	O
and	O
‘	O
random	B
’	O
is	O
a	O
challenge	O
{	O
what	O
does	O
it	O
mean	B
to	O
say	O
that	O
the	O
frequency	B
of	O
a	O
tossed	O
coin	B
’	O
s	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
26	O
2	O
|	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
box	B
2.4.	O
the	O
cox	O
axioms	O
.	O
if	O
a	O
set	B
of	O
beliefs	O
satisfy	O
these	O
axioms	O
then	O
they	O
can	O
be	O
mapped	O
onto	O
probabilities	O
satisfying	O
p	O
(	O
false	O
)	O
=	O
0	O
,	O
p	O
(	O
true	O
)	O
=	O
1	O
,	O
0	O
(	O
cid:20	O
)	O
p	O
(	O
x	O
)	O
(	O
cid:20	O
)	O
1	O
,	O
and	O
the	O
rules	B
of	O
probability	B
:	O
and	O
p	O
(	O
x	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
p	O
(	O
x	O
)	O
,	O
p	O
(	O
x	O
;	O
y	O
)	O
=	O
p	O
(	O
xj	O
y	O
)	O
p	O
(	O
y	O
)	O
:	O
notation	B
.	O
let	O
‘	O
the	O
degree	B
of	O
belief	B
in	O
proposition	O
x	O
’	O
be	O
denoted	O
by	O
b	O
(	O
x	O
)	O
.	O
the	O
negation	O
of	O
x	O
(	O
not-x	O
)	O
is	O
written	O
x.	O
the	O
degree	B
of	O
belief	B
in	O
a	O
condi-	O
tional	O
proposition	O
,	O
‘	O
x	O
,	O
assuming	O
proposition	O
y	O
to	O
be	O
true	O
’	O
,	O
is	O
represented	O
by	O
b	O
(	O
xj	O
y	O
)	O
.	O
axiom	O
1.	O
degrees	B
of	I
belief	I
can	O
be	O
ordered	O
;	O
if	O
b	O
(	O
x	O
)	O
is	O
‘	O
greater	O
’	O
than	O
b	O
(	O
y	O
)	O
,	O
and	O
b	O
(	O
y	O
)	O
is	O
‘	O
greater	O
’	O
than	O
b	O
(	O
z	O
)	O
,	O
then	O
b	O
(	O
x	O
)	O
is	O
‘	O
greater	O
’	O
than	O
b	O
(	O
z	O
)	O
.	O
[	O
consequence	O
:	O
beliefs	O
can	O
be	O
mapped	O
onto	O
real	O
numbers	O
.	O
]	O
axiom	O
2.	O
the	O
degree	B
of	O
belief	B
in	O
a	O
proposition	O
x	O
and	O
its	O
negation	O
x	O
are	O
related	O
.	O
there	O
is	O
a	O
function	B
f	O
such	O
that	O
b	O
(	O
x	O
)	O
=	O
f	O
[	O
b	O
(	O
x	O
)	O
]	O
:	O
axiom	O
3.	O
the	O
degree	B
of	O
belief	B
in	O
a	O
conjunction	O
of	O
propositions	O
x	O
;	O
y	O
(	O
x	O
and	O
y	O
)	O
is	O
related	O
to	O
the	O
degree	B
of	O
belief	B
in	O
the	O
conditional	B
proposition	O
xj	O
y	O
and	O
the	O
degree	B
of	O
belief	B
in	O
the	O
proposition	O
y.	O
there	O
is	O
a	O
function	B
g	O
such	O
that	O
b	O
(	O
x	O
;	O
y	O
)	O
=	O
g	O
[	O
b	O
(	O
xj	O
y	O
)	O
;	O
b	O
(	O
y	O
)	O
]	O
:	O
coming	O
up	O
heads	O
is	O
1/2	O
?	O
if	O
we	O
say	O
that	O
this	O
frequency	B
is	O
the	O
average	B
fraction	O
of	O
heads	O
in	O
long	O
sequences	O
,	O
we	O
have	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
‘	O
average	B
’	O
;	O
and	O
it	O
is	O
hard	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
‘	O
average	B
’	O
without	O
using	O
a	O
word	O
synonymous	O
to	O
probability	B
!	O
i	O
will	O
not	O
attempt	O
to	O
cut	O
this	O
philosophical	O
knot	O
.	O
probabilities	O
can	O
also	O
be	O
used	O
,	O
more	O
generally	O
,	O
to	O
describe	O
degrees	O
of	O
be-	O
lief	O
in	O
propositions	O
that	O
do	O
not	O
involve	O
random	B
variables	O
{	O
for	O
example	O
‘	O
the	O
probability	B
that	O
mr.	O
s.	O
was	O
the	O
murderer	O
of	O
mrs.	O
s.	O
,	O
given	O
the	O
evidence	B
’	O
(	O
he	O
either	O
was	O
or	O
wasn	O
’	O
t	O
,	O
and	O
it	O
’	O
s	O
the	O
jury	B
’	O
s	O
job	O
to	O
assess	O
how	O
probable	O
it	O
is	O
that	O
he	O
was	O
)	O
;	O
‘	O
the	O
probability	B
that	O
thomas	O
je	O
(	O
cid:11	O
)	O
erson	O
had	O
a	O
child	O
by	O
one	O
of	O
his	O
slaves	O
’	O
;	O
‘	O
the	O
probability	B
that	O
shakespeare	O
’	O
s	O
plays	O
were	O
written	O
by	O
francis	O
bacon	O
’	O
;	O
or	O
,	O
to	O
pick	O
a	O
modern-day	O
example	O
,	O
‘	O
the	O
probability	B
that	O
a	O
particular	O
signature	O
on	O
a	O
particular	O
cheque	O
is	O
genuine	O
’	O
.	O
the	O
man	O
in	O
the	O
street	O
is	O
happy	O
to	O
use	O
probabilities	O
in	O
both	O
these	O
ways	O
,	O
but	O
some	O
books	O
on	O
probability	O
restrict	O
probabilities	O
to	O
refer	O
only	O
to	O
frequencies	O
of	O
outcomes	O
in	O
repeatable	O
random	B
experiments	O
.	O
nevertheless	O
,	O
degrees	B
of	I
belief	I
can	O
be	O
mapped	O
onto	O
probabilities	O
if	O
they	O
sat-	O
isfy	O
simple	O
consistency	O
rules	B
known	O
as	O
the	O
cox	O
axioms	O
(	O
cox	O
,	O
1946	O
)	O
(	O
(	O
cid:12	O
)	O
gure	O
2.4	O
)	O
.	O
thus	O
probabilities	O
can	O
be	O
used	O
to	O
describe	O
assumptions	B
,	O
and	O
to	O
describe	O
in-	O
ferences	O
given	O
those	O
assumptions	B
.	O
the	O
rules	B
of	O
probability	B
ensure	O
that	O
if	O
two	O
people	O
make	O
the	O
same	O
assumptions	B
and	O
receive	O
the	O
same	O
data	O
then	O
they	O
will	O
draw	O
identical	O
conclusions	O
.	O
this	O
more	O
general	O
use	O
of	O
probability	O
to	O
quantify	O
beliefs	O
is	O
known	O
as	O
the	O
bayesian	O
viewpoint	O
.	O
it	O
is	O
also	O
known	O
as	O
the	O
subjective	O
interpretation	O
of	O
probability	O
,	O
since	O
the	O
probabilities	O
depend	O
on	O
assumptions	O
.	O
advocates	O
of	O
a	O
bayesian	O
approach	O
to	O
data	B
modelling	I
and	O
pattern	B
recognition	I
do	O
not	O
view	O
this	O
subjectivity	B
as	O
a	O
defect	O
,	O
since	O
in	O
their	O
view	O
,	O
you	O
can	O
not	O
do	O
inference	O
without	O
making	O
assumptions	B
.	O
in	O
this	O
book	O
it	O
will	O
from	O
time	O
to	O
time	O
be	O
taken	O
for	O
granted	O
that	O
a	O
bayesian	O
approach	O
makes	O
sense	O
,	O
but	O
the	O
reader	O
is	O
warned	O
that	O
this	O
is	O
not	O
yet	O
a	O
globally	O
held	O
view	O
{	O
the	O
(	O
cid:12	O
)	O
eld	O
of	O
statistics	O
was	O
dominated	O
for	O
most	O
of	O
the	O
20th	O
century	O
by	O
non-bayesian	O
methods	B
in	O
which	O
probabilities	O
are	O
allowed	O
to	O
describe	O
only	O
random	B
variables	O
.	O
the	O
big	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
the	O
two	O
approaches	O
is	O
that	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
2.3	O
:	O
forward	O
probabilities	O
and	O
inverse	O
probabilities	O
27	O
bayesians	O
also	O
use	O
probabilities	O
to	O
describe	O
inferences	O
.	O
2.3	O
forward	O
probabilities	O
and	O
inverse	O
probabilities	O
probability	B
calculations	O
often	O
fall	O
into	O
one	O
of	O
two	O
categories	O
:	O
forward	O
prob-	O
ability	O
and	O
inverse	O
probability	B
.	O
here	O
is	O
an	O
example	O
of	O
a	O
forward	B
probability	I
problem	O
:	O
exercise	O
2.4	O
.	O
[	O
2	O
,	O
p.40	O
]	O
an	O
urn	B
contains	O
k	O
balls	O
,	O
of	O
which	O
b	O
are	O
black	B
and	O
w	O
=	O
k	O
(	O
cid:0	O
)	O
b	O
are	O
white	B
.	O
fred	O
draws	O
a	O
ball	O
at	O
random	B
from	O
the	O
urn	B
and	O
replaces	O
it	O
,	O
n	O
times	O
.	O
(	O
a	O
)	O
what	O
is	O
the	O
probability	B
distribution	O
of	O
the	O
number	O
of	O
times	O
a	O
black	B
ball	O
is	O
drawn	O
,	O
nb	O
?	O
(	O
b	O
)	O
what	O
is	O
the	O
expectation	B
of	O
nb	O
?	O
what	O
is	O
the	O
variance	B
of	O
nb	O
?	O
what	O
is	O
the	O
standard	B
deviation	I
of	O
nb	O
?	O
give	O
numerical	O
answers	O
for	O
the	O
cases	O
n	O
=	O
5	O
and	O
n	O
=	O
400	O
,	O
when	O
b	O
=	O
2	O
and	O
k	O
=	O
10.	O
forward	B
probability	I
problems	O
involve	O
a	O
generative	B
model	I
that	O
describes	O
a	O
pro-	O
cess	O
that	O
is	O
assumed	O
to	O
give	O
rise	O
to	O
some	O
data	O
;	O
the	O
task	O
is	O
to	O
compute	O
the	O
probability	B
distribution	O
or	O
expectation	B
of	O
some	O
quantity	O
that	O
depends	O
on	O
the	O
data	O
.	O
here	O
is	O
another	O
example	O
of	O
a	O
forward	B
probability	I
problem	O
:	O
exercise	O
2.5	O
.	O
[	O
2	O
,	O
p.40	O
]	O
an	O
urn	B
contains	O
k	O
balls	O
,	O
of	O
which	O
b	O
are	O
black	B
and	O
w	O
=	O
k	O
(	O
cid:0	O
)	O
b	O
are	O
white	B
.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
fraction	O
fb	O
(	O
cid:17	O
)	O
b=k	O
.	O
fred	O
draws	O
n	O
times	O
from	O
the	O
urn	B
,	O
exactly	O
as	O
in	O
exercise	O
2.4	O
,	O
obtaining	O
nb	O
blacks	O
,	O
and	O
computes	O
the	O
quantity	O
z	O
=	O
(	O
nb	O
(	O
cid:0	O
)	O
fbn	O
)	O
2	O
n	O
fb	O
(	O
1	O
(	O
cid:0	O
)	O
fb	O
)	O
:	O
(	O
2.19	O
)	O
what	O
is	O
the	O
expectation	B
of	O
z	O
?	O
in	O
the	O
case	O
n	O
=	O
5	O
and	O
fb	O
=	O
1=5	O
,	O
what	O
is	O
the	O
probability	B
distribution	O
of	O
z	O
?	O
what	O
is	O
the	O
probability	B
that	O
z	O
<	O
1	O
?	O
[	O
hint	O
:	O
compare	O
z	O
with	O
the	O
quantities	O
computed	O
in	O
the	O
previous	O
exercise	O
.	O
]	O
like	O
forward	B
probability	I
problems	O
,	O
inverse	B
probability	I
problems	O
involve	O
a	O
generative	B
model	I
of	O
a	O
process	O
,	O
but	O
instead	O
of	O
computing	O
the	O
probability	B
distri-	O
bution	O
of	O
some	O
quantity	O
produced	O
by	O
the	O
process	O
,	O
we	O
compute	O
the	O
conditional	B
probability	O
of	O
one	O
or	O
more	O
of	O
the	O
unobserved	O
variables	O
in	O
the	O
process	O
,	O
given	O
the	O
observed	O
variables	O
.	O
this	O
invariably	O
requires	O
the	O
use	O
of	O
bayes	O
’	O
theorem	B
.	O
example	O
2.6.	O
there	O
are	O
eleven	O
urns	O
labelled	O
by	O
u	O
2	O
f0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
10g	O
,	O
each	O
con-	O
taining	O
ten	O
balls	O
.	O
urn	B
u	O
contains	O
u	O
black	B
balls	O
and	O
10	O
(	O
cid:0	O
)	O
u	O
white	B
balls	O
.	O
fred	O
selects	O
an	O
urn	B
u	O
at	O
random	B
and	O
draws	O
n	O
times	O
with	O
replacement	O
from	O
that	O
urn	B
,	O
obtaining	O
nb	O
blacks	O
and	O
n	O
(	O
cid:0	O
)	O
nb	O
whites	O
.	O
fred	O
’	O
s	O
friend	O
,	O
bill	O
,	O
looks	O
on	O
.	O
if	O
after	O
n	O
=	O
10	O
draws	O
nb	O
=	O
3	O
blacks	O
have	O
been	O
drawn	O
,	O
what	O
is	O
the	O
probability	B
that	O
the	O
urn	B
fred	O
is	O
using	O
is	O
urn	B
u	O
,	O
from	O
bill	O
’	O
s	O
point	O
of	O
view	O
?	O
(	O
bill	O
doesn	O
’	O
t	O
know	O
the	O
value	O
of	O
u	O
.	O
)	O
solution	O
.	O
the	O
joint	B
probability	O
distribution	B
of	O
the	O
random	B
variables	O
u	O
and	O
nb	O
can	O
be	O
written	O
p	O
(	O
u	O
;	O
nb	O
j	O
n	O
)	O
=	O
p	O
(	O
nb	O
j	O
u	O
;	O
n	O
)	O
p	O
(	O
u	O
)	O
:	O
(	O
2.20	O
)	O
from	O
the	O
joint	B
probability	O
of	O
u	O
and	O
nb	O
,	O
we	O
can	O
obtain	O
the	O
conditional	B
distribution	O
of	O
u	O
given	O
nb	O
:	O
p	O
(	O
uj	O
nb	O
;	O
n	O
)	O
=	O
=	O
p	O
(	O
u	O
;	O
nb	O
j	O
n	O
)	O
p	O
(	O
nb	O
j	O
n	O
)	O
p	O
(	O
nb	O
j	O
u	O
;	O
n	O
)	O
p	O
(	O
u	O
)	O
p	O
(	O
nb	O
j	O
n	O
)	O
:	O
(	O
2.21	O
)	O
(	O
2.22	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
28	O
2	O
|	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
figure	O
2.5.	O
joint	B
probability	O
of	O
u	O
and	O
nb	O
for	O
bill	O
and	O
fred	O
’	O
s	O
urn	B
problem	O
,	O
after	O
n	O
=	O
10	O
draws	O
.	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
0	O
1	O
2	O
3	O
4	O
5	O
u	O
6	O
7	O
8	O
9	O
10	O
u	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
p	O
(	O
uj	O
nb	O
=	O
3	O
;	O
n	O
)	O
0	O
0.063	O
0.22	O
0.29	O
0.24	O
0.13	O
0.047	O
0.0099	O
0.00086	O
0.0000096	O
0	O
figure	O
2.6.	O
conditional	B
probability	O
of	O
u	O
given	O
nb	O
=	O
3	O
and	O
n	O
=	O
10.	O
u	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
nb	O
the	O
marginal	B
probability	I
of	O
u	O
is	O
p	O
(	O
u	O
)	O
=	O
1	O
11	O
for	O
all	O
u.	O
you	O
wrote	O
down	O
the	O
probability	O
of	O
nb	O
given	O
u	O
and	O
n	O
,	O
p	O
(	O
nb	O
j	O
u	O
;	O
n	O
)	O
,	O
when	O
you	O
solved	O
exercise	O
2.4	O
(	O
p.27	O
)	O
.	O
[	O
you	O
are	O
doing	O
the	O
highly	O
recommended	O
exercises	O
,	O
aren	O
’	O
t	O
you	O
?	O
]	O
if	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
fu	O
(	O
cid:17	O
)	O
u=10	O
then	O
p	O
(	O
nb	O
j	O
u	O
;	O
n	O
)	O
=	O
(	O
cid:18	O
)	O
n	O
nb	O
(	O
cid:19	O
)	O
f	O
nb	O
u	O
(	O
1	O
(	O
cid:0	O
)	O
fu	O
)	O
n	O
(	O
cid:0	O
)	O
nb	O
:	O
(	O
2.23	O
)	O
what	O
about	O
the	O
denominator	O
,	O
p	O
(	O
nb	O
j	O
n	O
)	O
?	O
this	O
is	O
the	O
marginal	B
probability	I
of	O
nb	O
,	O
which	O
we	O
can	O
obtain	O
using	O
the	O
sum	B
rule	I
:	O
p	O
(	O
nb	O
j	O
n	O
)	O
=xu	O
p	O
(	O
u	O
;	O
nb	O
j	O
n	O
)	O
=xu	O
p	O
(	O
u	O
)	O
p	O
(	O
nb	O
j	O
u	O
;	O
n	O
)	O
:	O
(	O
2.24	O
)	O
so	O
the	O
conditional	B
probability	O
of	O
u	O
given	O
nb	O
is	O
p	O
(	O
uj	O
nb	O
;	O
n	O
)	O
=	O
=	O
p	O
(	O
u	O
)	O
p	O
(	O
nb	O
j	O
u	O
;	O
n	O
)	O
p	O
(	O
nb	O
j	O
n	O
)	O
11	O
(	O
cid:18	O
)	O
n	O
1	O
1	O
p	O
(	O
nb	O
j	O
n	O
)	O
nb	O
(	O
cid:19	O
)	O
f	O
nb	O
u	O
(	O
1	O
(	O
cid:0	O
)	O
fu	O
)	O
n	O
(	O
cid:0	O
)	O
nb	O
:	O
(	O
2.25	O
)	O
(	O
2.26	O
)	O
this	O
conditional	B
distribution	O
can	O
be	O
found	O
by	O
normalizing	O
column	O
3	O
of	O
(	O
cid:12	O
)	O
gure	O
2.5	O
and	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
2.6.	O
the	O
normalizing	B
constant	I
,	O
the	O
marginal	B
probability	I
of	O
nb	O
,	O
is	O
p	O
(	O
nb	O
=	O
3j	O
n	O
=	O
10	O
)	O
=	O
0:083.	O
the	O
posterior	B
probability	I
(	O
2.26	O
)	O
is	O
correct	O
for	O
all	O
u	O
,	O
including	O
the	O
end-points	O
u	O
=	O
0	O
and	O
u	O
=	O
10	O
,	O
where	O
fu	O
=	O
0	O
and	O
fu	O
=	O
1	O
respectively	O
.	O
the	O
posterior	B
probability	I
that	O
u	O
=	O
0	O
given	O
nb	O
=	O
3	O
is	O
equal	O
to	O
zero	O
,	O
because	O
if	O
fred	O
were	O
drawing	O
from	O
urn	O
0	O
it	O
would	O
be	O
impossible	O
for	O
any	O
black	B
balls	O
to	O
be	O
drawn	O
.	O
the	O
posterior	B
probability	I
that	O
u	O
=	O
10	O
is	O
also	O
zero	O
,	O
because	O
there	O
are	O
no	O
white	B
balls	O
in	O
that	O
urn	B
.	O
the	O
other	O
hypotheses	O
u	O
=	O
1	O
;	O
u	O
=	O
2	O
,	O
:	O
:	O
:	O
u	O
=	O
9	O
all	O
have	O
non-zero	O
posterior	B
probability	I
.	O
2	O
terminology	B
of	O
inverse	B
probability	I
in	O
inverse	B
probability	I
problems	O
it	O
is	O
convenient	O
to	O
give	O
names	O
to	O
the	O
proba-	O
bilities	O
appearing	O
in	O
bayes	O
’	O
theorem	B
.	O
in	O
equation	O
(	O
2.25	O
)	O
,	O
we	O
call	O
the	O
marginal	B
probability	I
p	O
(	O
u	O
)	O
the	O
prior	B
probability	O
of	O
u	O
,	O
and	O
p	O
(	O
nb	O
j	O
u	O
;	O
n	O
)	O
is	O
called	O
the	O
like-	O
lihood	O
of	O
u.	O
it	O
is	O
important	O
to	O
note	O
that	O
the	O
terms	O
likelihood	B
and	O
probability	B
are	O
not	O
synonyms	O
.	O
the	O
quantity	O
p	O
(	O
nb	O
j	O
u	O
;	O
n	O
)	O
is	O
a	O
function	B
of	O
both	O
nb	O
and	O
u.	O
for	O
(	O
cid:12	O
)	O
xed	O
u	O
,	O
p	O
(	O
nb	O
j	O
u	O
;	O
n	O
)	O
de	O
(	O
cid:12	O
)	O
nes	O
a	O
probability	B
over	O
nb	O
.	O
for	O
(	O
cid:12	O
)	O
xed	O
nb	O
,	O
p	O
(	O
nb	O
j	O
u	O
;	O
n	O
)	O
de	O
(	O
cid:12	O
)	O
nes	O
the	O
likelihood	B
of	O
u.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
2.3	O
:	O
forward	O
probabilities	O
and	O
inverse	O
probabilities	O
29	O
never	O
say	O
‘	O
the	O
likelihood	B
of	O
the	O
data	O
’	O
.	O
always	O
say	O
‘	O
the	O
likelihood	B
of	O
the	O
parameters	B
’	O
.	O
the	O
likelihood	B
function	O
is	O
not	O
a	O
probability	B
distribution	O
.	O
(	O
if	O
you	O
want	O
to	O
mention	O
the	O
data	O
that	O
a	O
likelihood	B
function	O
is	O
associated	O
with	O
,	O
you	O
may	O
say	O
‘	O
the	O
likelihood	B
of	O
the	O
parameters	B
given	O
the	O
data	O
’	O
.	O
)	O
the	O
conditional	B
probability	O
p	O
(	O
uj	O
nb	O
;	O
n	O
)	O
is	O
called	O
the	O
posterior	B
probability	I
of	O
u	O
given	O
nb	O
.	O
the	O
normalizing	B
constant	I
p	O
(	O
nb	O
j	O
n	O
)	O
has	O
no	O
u-dependence	O
so	O
its	O
value	O
is	O
not	O
important	O
if	O
we	O
simply	O
wish	O
to	O
evaluate	O
the	O
relative	B
probabilities	O
of	O
the	O
alternative	O
hypotheses	O
u.	O
however	O
,	O
in	O
most	O
data-modelling	O
problems	O
of	O
any	O
complexity	O
,	O
this	O
quantity	O
becomes	O
important	O
,	O
and	O
it	O
is	O
given	O
various	O
names	O
:	O
p	O
(	O
nb	O
j	O
n	O
)	O
is	O
known	O
as	O
the	O
evidence	B
or	O
the	O
marginal	B
likelihood	I
.	O
if	O
(	O
cid:18	O
)	O
denotes	O
the	O
unknown	O
parameters	O
,	O
d	O
denotes	O
the	O
data	O
,	O
and	O
h	O
denotes	O
the	O
overall	O
hypothesis	O
space	O
,	O
the	O
general	O
equation	O
:	O
is	O
written	O
:	O
p	O
(	O
(	O
cid:18	O
)	O
j	O
d	O
;	O
h	O
)	O
=	O
p	O
(	O
d	O
j	O
(	O
cid:18	O
)	O
;	O
h	O
)	O
p	O
(	O
(	O
cid:18	O
)	O
jh	O
)	O
p	O
(	O
d	O
jh	O
)	O
posterior	O
=	O
likelihood	B
(	O
cid:2	O
)	O
prior	B
evidence	O
:	O
(	O
2.27	O
)	O
(	O
2.28	O
)	O
inverse	B
probability	I
and	O
prediction	B
example	O
2.6	O
(	O
continued	O
)	O
.	O
assuming	O
again	O
that	O
bill	O
has	O
observed	O
nb	O
=	O
3	O
blacks	O
in	O
n	O
=	O
10	O
draws	O
,	O
let	O
fred	O
draw	O
another	O
ball	O
from	O
the	O
same	O
urn	B
.	O
what	O
is	O
the	O
probability	B
that	O
the	O
next	O
drawn	O
ball	O
is	O
a	O
black	B
?	O
[	O
you	O
should	O
make	O
use	O
of	O
the	O
posterior	O
probabilities	O
in	O
(	O
cid:12	O
)	O
gure	O
2.6	O
.	O
]	O
solution	O
.	O
by	O
the	O
sum	B
rule	I
,	O
p	O
(	O
balln+1	O
is	O
black	B
j	O
nb	O
;	O
n	O
)	O
=xu	O
p	O
(	O
balln+1	O
is	O
black	B
j	O
u	O
;	O
nb	O
;	O
n	O
)	O
p	O
(	O
uj	O
nb	O
;	O
n	O
)	O
:	O
(	O
2.29	O
)	O
since	O
the	O
balls	O
are	O
drawn	O
with	O
replacement	O
from	O
the	O
chosen	O
urn	B
,	O
the	O
proba-	O
bility	O
p	O
(	O
balln+1	O
is	O
black	B
j	O
u	O
;	O
nb	O
;	O
n	O
)	O
is	O
just	O
fu	O
=	O
u=10	O
,	O
whatever	O
nb	O
and	O
n	O
are	O
.	O
so	O
p	O
(	O
balln+1	O
is	O
black	B
j	O
nb	O
;	O
n	O
)	O
=xu	O
fup	O
(	O
uj	O
nb	O
;	O
n	O
)	O
:	O
(	O
2.30	O
)	O
using	O
the	O
values	O
of	O
p	O
(	O
uj	O
nb	O
;	O
n	O
)	O
given	O
in	O
(	O
cid:12	O
)	O
gure	O
2.6	O
we	O
obtain	O
p	O
(	O
balln+1	O
is	O
black	B
j	O
nb	O
=	O
3	O
;	O
n	O
=	O
10	O
)	O
=	O
0:333	O
:	O
2	O
(	O
2.31	O
)	O
comment	O
.	O
notice	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
this	O
prediction	B
obtained	O
using	O
prob-	O
ability	O
theory	B
,	O
and	O
the	O
widespread	O
practice	O
in	B
statistics	I
of	O
making	O
predictions	O
by	O
(	O
cid:12	O
)	O
rst	O
selecting	O
the	O
most	O
plausible	O
hypothesis	O
(	O
which	O
here	O
would	O
be	O
that	O
the	O
urn	B
is	O
urn	B
u	O
=	O
3	O
)	O
and	O
then	O
making	O
the	O
predictions	O
assuming	O
that	O
hypothesis	O
to	O
be	O
true	O
(	O
which	O
would	O
give	O
a	O
probability	O
of	O
0.3	O
that	O
the	O
next	O
ball	O
is	O
black	B
)	O
.	O
the	O
correct	O
prediction	B
is	O
the	O
one	O
that	O
takes	O
into	O
account	O
the	O
uncertainty	O
by	O
marginalizing	O
over	O
the	O
possible	O
values	O
of	O
the	O
hypothesis	O
u.	O
marginalization	B
here	O
leads	O
to	O
slightly	O
more	O
moderate	O
,	O
less	O
extreme	O
predictions	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
30	O
2	O
|	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
inference	B
as	O
inverse	B
probability	I
now	O
consider	O
the	O
following	O
exercise	O
,	O
which	O
has	O
the	O
character	O
of	O
a	O
simple	O
sci-	O
enti	O
(	O
cid:12	O
)	O
c	O
investigation	O
.	O
example	O
2.7.	O
bill	O
tosses	O
a	O
bent	B
coin	I
n	O
times	O
,	O
obtaining	O
a	O
sequence	B
of	O
heads	O
and	O
tails	O
.	O
we	O
assume	O
that	O
the	O
coin	B
has	O
a	O
probability	B
fh	O
of	O
coming	O
up	O
heads	O
;	O
we	O
do	O
not	O
know	O
fh	O
.	O
if	O
nh	O
heads	O
have	O
occurred	O
in	O
n	O
tosses	O
,	O
what	O
is	O
the	O
probability	B
distribution	O
of	O
fh	O
?	O
(	O
for	O
example	O
,	O
n	O
might	O
be	O
10	O
,	O
and	O
nh	O
might	O
be	O
3	O
;	O
or	O
,	O
after	O
a	O
lot	O
more	O
tossing	O
,	O
we	O
might	O
have	O
n	O
=	O
300	O
and	O
nh	O
=	O
29	O
.	O
)	O
what	O
is	O
the	O
probability	B
that	O
the	O
n	O
+1th	O
outcome	O
will	O
be	O
a	O
head	O
,	O
given	O
nh	O
heads	O
in	O
n	O
tosses	O
?	O
unlike	O
example	O
2.6	O
(	O
p.27	O
)	O
,	O
this	O
problem	O
has	O
a	O
subjective	O
element	O
.	O
given	O
a	O
restricted	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
probability	O
that	O
says	O
‘	O
probabilities	O
are	O
the	O
frequencies	O
of	O
random	O
variables	O
’	O
,	O
this	O
example	O
is	O
di	O
(	O
cid:11	O
)	O
erent	O
from	O
the	O
eleven-urns	O
example	O
.	O
whereas	O
the	O
urn	B
u	O
was	O
a	O
random	B
variable	I
,	O
the	O
bias	B
fh	O
of	O
the	O
coin	O
would	O
not	O
normally	O
be	O
called	O
a	O
random	B
variable	I
.	O
it	O
is	O
just	O
a	O
(	O
cid:12	O
)	O
xed	O
but	O
unknown	O
parameter	O
that	O
we	O
are	O
interested	O
in	O
.	O
yet	O
don	O
’	O
t	O
the	O
two	O
examples	O
2.6	O
and	O
2.7	O
seem	O
to	O
have	O
an	O
essential	O
similarity	O
?	O
[	O
especially	O
when	O
n	O
=	O
10	O
and	O
nh	O
=	O
3	O
!	O
]	O
to	O
solve	O
example	O
2.7	O
,	O
we	O
have	O
to	O
make	O
an	O
assumption	O
about	O
what	O
the	O
bias	B
of	O
the	O
coin	B
fh	O
might	O
be	O
.	O
this	O
prior	B
probability	O
distribution	B
over	O
fh	O
,	O
p	O
(	O
fh	O
)	O
,	O
corresponds	O
to	O
the	O
prior	B
over	O
u	O
in	O
the	O
eleven-urns	O
problem	O
.	O
in	O
that	O
example	O
,	O
the	O
helpful	O
problem	O
de	O
(	O
cid:12	O
)	O
nition	O
speci	O
(	O
cid:12	O
)	O
ed	O
p	O
(	O
u	O
)	O
.	O
in	O
real	O
life	B
,	O
we	O
have	O
to	O
make	O
assumptions	B
in	O
order	O
to	O
assign	O
priors	O
;	O
these	O
assumptions	B
will	O
be	O
subjective	O
,	O
and	O
our	O
answers	O
will	O
depend	O
on	O
them	O
.	O
exactly	O
the	O
same	O
can	O
be	O
said	O
for	O
the	O
other	O
probabilities	O
in	O
our	O
generative	B
model	I
too	O
.	O
we	O
are	O
assuming	O
,	O
for	O
example	O
,	O
that	O
the	O
balls	O
are	O
drawn	O
from	O
an	O
urn	B
independently	O
;	O
but	O
could	O
there	O
not	O
be	O
correlations	B
in	O
the	O
sequence	B
because	O
fred	O
’	O
s	O
ball-drawing	O
action	O
is	O
not	O
perfectly	O
random	B
?	O
indeed	O
there	O
could	O
be	O
,	O
so	O
the	O
likelihood	B
function	O
that	O
we	O
use	O
depends	O
on	O
assumptions	O
too	O
.	O
in	O
real	O
data	B
modelling	I
problems	O
,	O
priors	O
are	O
subjective	O
and	O
so	O
are	O
likelihoods	O
.	O
here	O
p	O
(	O
f	O
)	O
denotes	O
a	O
probability	B
density	O
,	O
rather	O
than	O
a	O
probability	B
distribution	O
.	O
we	O
are	O
now	O
using	O
p	O
(	O
)	O
to	O
denote	O
probability	B
densities	O
over	O
continuous	O
vari-	O
ables	O
as	O
well	O
as	O
probabilities	O
over	O
discrete	O
variables	O
and	O
probabilities	O
of	O
logical	O
propositions	O
.	O
the	O
probability	B
that	O
a	O
continuous	B
variable	O
v	O
lies	O
between	O
values	O
a	O
dv	O
p	O
(	O
v	O
)	O
.	O
p	O
(	O
v	O
)	O
dv	O
is	O
dimensionless	O
.	O
the	O
density	B
p	O
(	O
v	O
)	O
is	O
a	O
dimensional	O
quantity	O
,	O
having	O
dimensions	B
inverse	O
to	O
the	O
dimensions	B
of	O
v	O
{	O
in	O
contrast	O
to	O
discrete	O
probabilities	O
,	O
which	O
are	O
dimensionless	O
.	O
don	O
’	O
t	O
be	O
surprised	O
to	O
see	O
probability	O
densities	O
greater	O
than	O
1.	O
this	O
is	O
normal	B
,	O
a	O
and	O
b	O
(	O
where	O
b	O
>	O
a	O
)	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
r	O
b	O
and	O
nothing	O
is	O
wrong	O
,	O
as	O
long	O
as	O
r	O
b	O
conditional	B
and	O
joint	B
probability	O
densities	O
are	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
just	O
the	O
same	O
way	O
as	O
conditional	O
and	O
joint	O
probabilities	O
.	O
a	O
dv	O
p	O
(	O
v	O
)	O
(	O
cid:20	O
)	O
1	O
for	O
any	O
interval	O
(	O
a	O
;	O
b	O
)	O
.	O
.	O
exercise	O
2.8	O
.	O
[	O
2	O
]	O
assuming	O
a	O
uniform	O
prior	B
on	O
fh	O
,	O
p	O
(	O
fh	O
)	O
=	O
1	O
,	O
solve	O
the	O
problem	O
posed	O
in	O
example	O
2.7	O
(	O
p.30	O
)	O
.	O
sketch	O
the	O
posterior	O
distribution	O
of	O
fh	O
and	O
compute	O
the	O
probability	B
that	O
the	O
n	O
+1th	O
outcome	O
will	O
be	O
a	O
head	O
,	O
for	O
(	O
a	O
)	O
n	O
=	O
3	O
and	O
nh	O
=	O
0	O
;	O
(	O
b	O
)	O
n	O
=	O
3	O
and	O
nh	O
=	O
2	O
;	O
(	O
c	O
)	O
n	O
=	O
10	O
and	O
nh	O
=	O
3	O
;	O
(	O
d	O
)	O
n	O
=	O
300	O
and	O
nh	O
=	O
29.	O
you	O
will	O
(	O
cid:12	O
)	O
nd	O
the	O
beta	B
integral	I
useful	O
:	O
z	O
1	O
0	O
dpa	O
pfa	O
a	O
(	O
1	O
(	O
cid:0	O
)	O
pa	O
)	O
fb	O
=	O
(	O
cid:0	O
)	O
(	O
fa	O
+	O
1	O
)	O
(	O
cid:0	O
)	O
(	O
fb	O
+	O
1	O
)	O
(	O
cid:0	O
)	O
(	O
fa	O
+	O
fb	O
+	O
2	O
)	O
=	O
fa	O
!	O
fb	O
!	O
(	O
fa	O
+	O
fb	O
+	O
1	O
)	O
!	O
:	O
(	O
2.32	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
2.3	O
:	O
forward	O
probabilities	O
and	O
inverse	O
probabilities	O
31	O
you	O
may	O
also	O
(	O
cid:12	O
)	O
nd	O
it	O
instructive	O
to	O
look	O
back	O
at	O
example	O
2.6	O
(	O
p.27	O
)	O
and	O
equation	O
(	O
2.31	O
)	O
.	O
people	O
sometimes	O
confuse	O
assigning	B
a	O
prior	B
distribution	O
to	O
an	O
unknown	O
pa-	O
rameter	O
such	O
as	O
fh	O
with	O
making	O
an	O
initial	O
guess	O
of	O
the	O
value	O
of	O
the	O
parameter	O
.	O
but	O
the	O
prior	B
over	O
fh	O
,	O
p	O
(	O
fh	O
)	O
,	O
is	O
not	O
a	O
simple	O
statement	O
like	O
‘	O
initially	O
,	O
i	O
would	O
guess	O
fh	O
=	O
1/2	O
’	O
.	O
the	O
prior	B
is	O
a	O
probability	B
density	O
over	O
fh	O
which	O
speci	O
(	O
cid:12	O
)	O
es	O
the	O
prior	B
degree	O
of	O
belief	O
that	O
fh	O
lies	O
in	O
any	O
interval	O
(	O
f	O
;	O
f	O
+	O
(	O
cid:14	O
)	O
f	O
)	O
.	O
it	O
may	O
well	O
be	O
the	O
case	O
that	O
our	O
prior	B
for	O
fh	O
is	O
symmetric	B
about	O
1/2	O
,	O
so	O
that	O
the	O
mean	B
of	O
fh	O
under	O
the	O
prior	B
is	O
1/2	O
.	O
in	O
this	O
case	O
,	O
the	O
predictive	B
distribution	I
for	O
the	O
(	O
cid:12	O
)	O
rst	O
toss	O
x1	O
would	O
indeed	O
be	O
p	O
(	O
x1	O
=	O
head	O
)	O
=z	O
dfh	O
p	O
(	O
fh	O
)	O
p	O
(	O
x1	O
=	O
headj	O
fh	O
)	O
=z	O
dfh	O
p	O
(	O
fh	O
)	O
fh	O
=	O
1/2	O
:	O
(	O
2.33	O
)	O
but	O
the	O
prediction	B
for	O
subsequent	O
tosses	O
will	O
depend	O
on	O
the	O
whole	O
prior	B
dis-	O
tribution	O
,	O
not	O
just	O
its	O
mean	B
.	O
data	B
compression	I
and	O
inverse	B
probability	I
consider	O
the	O
following	O
task	O
.	O
example	O
2.9.	O
write	O
a	O
computer	B
program	O
capable	O
of	O
compressing	O
binary	O
(	O
cid:12	O
)	O
les	O
like	O
this	O
one	O
:	O
0000000000000000000010010001000000100000010000000000000000000000000000000000001010000000000000110000	O
1000000000010000100000000010000000000000000000000100000000000000000100000000011000001000000011000100	O
0000000001001000000000010001000000000000000011000000000000000000000000000010000000000000000100000000	O
the	O
string	O
shown	O
contains	O
n1	O
=	O
29	O
1s	O
and	O
n0	O
=	O
271	O
0s	O
.	O
intuitively	O
,	O
compression	B
works	O
by	O
taking	O
advantage	O
of	O
the	O
predictability	O
of	O
a	O
(	O
cid:12	O
)	O
le	O
.	O
in	O
this	O
case	O
,	O
the	O
source	O
of	O
the	O
(	O
cid:12	O
)	O
le	O
appears	O
more	O
likely	O
to	O
emit	O
0s	O
than	O
1s	O
.	O
a	O
data	B
compression	I
program	O
that	O
compresses	O
this	O
(	O
cid:12	O
)	O
le	O
must	O
,	O
implicitly	O
or	O
explicitly	O
,	O
be	O
addressing	O
the	O
question	O
‘	O
what	O
is	O
the	O
probability	B
that	O
the	O
next	O
character	O
in	O
this	O
(	O
cid:12	O
)	O
le	O
is	O
a	O
1	O
?	O
’	O
do	O
you	O
think	O
this	O
problem	O
is	O
similar	O
in	O
character	O
to	O
example	O
2.7	O
(	O
p.30	O
)	O
?	O
i	O
do	O
.	O
one	O
of	O
the	O
themes	O
of	O
this	O
book	O
is	O
that	O
data	B
compression	I
and	O
data	B
modelling	I
are	O
one	O
and	O
the	O
same	O
,	O
and	O
that	O
they	O
should	O
both	O
be	O
addressed	O
,	O
like	O
the	O
urn	B
of	O
example	O
2.6	O
,	O
using	O
inverse	B
probability	I
.	O
example	O
2.9	O
is	O
solved	O
in	O
chapter	O
6.	O
the	O
likelihood	B
principle	I
please	O
solve	O
the	O
following	O
two	O
exercises	O
.	O
a	O
b	O
example	O
2.10.	O
urn	B
a	O
contains	O
three	O
balls	O
:	O
one	O
black	B
,	O
and	O
two	O
white	B
;	O
urn	B
b	O
contains	O
three	O
balls	O
:	O
two	O
black	B
,	O
and	O
one	O
white	B
.	O
one	O
of	O
the	O
urns	O
is	O
selected	O
at	O
random	B
and	O
one	O
ball	O
is	O
drawn	O
.	O
the	O
ball	O
is	O
black	B
.	O
what	O
is	O
the	O
probability	B
that	O
the	O
selected	O
urn	B
is	O
urn	B
a	O
?	O
figure	O
2.7.	O
urns	O
for	O
example	O
2.10.	O
example	O
2.11.	O
urn	B
a	O
contains	O
(	O
cid:12	O
)	O
ve	O
balls	O
:	O
one	O
black	B
,	O
two	O
white	B
,	O
one	O
green	O
and	O
one	O
pink	O
;	O
urn	B
b	O
contains	O
(	O
cid:12	O
)	O
ve	O
hundred	O
balls	O
:	O
two	O
hundred	O
black	B
,	O
one	O
hundred	O
white	B
,	O
50	O
yellow	O
,	O
40	O
cyan	O
,	O
30	O
sienna	O
,	O
25	O
green	O
,	O
25	O
silver	O
,	O
20	O
gold	O
,	O
and	O
10	O
purple	O
.	O
[	O
one	O
(	O
cid:12	O
)	O
fth	O
of	O
a	O
’	O
s	O
balls	O
are	O
black	B
;	O
two-	O
(	O
cid:12	O
)	O
fths	O
of	O
b	O
’	O
s	O
are	O
black	B
.	O
]	O
one	O
of	O
the	O
urns	O
is	O
selected	O
at	O
random	B
and	O
one	O
ball	O
is	O
drawn	O
.	O
the	O
ball	O
is	O
black	B
.	O
what	O
is	O
the	O
probability	B
that	O
the	O
urn	B
is	O
urn	B
a	O
?	O
p	O
g	O
s	O
...	O
...	O
...	O
c	O
g	O
p	O
y	O
figure	O
2.8.	O
urns	O
for	O
example	O
2.11.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
32	O
2	O
|	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
ai	O
pi	O
h	O
(	O
pi	O
)	O
i	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
13	O
14	O
15	O
16	O
17	O
18	O
19	O
20	O
21	O
22	O
23	O
24	O
25	O
26	O
27	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
-	O
.0575	O
.0128	O
.0263	O
.0285	O
.0913	O
.0173	O
.0133	O
.0313	O
.0599	O
.0006	O
.0084	O
.0335	O
.0235	O
.0596	O
.0689	O
.0192	O
.0008	O
.0508	O
.0567	O
.0706	O
.0334	O
.0069	O
.0119	O
.0073	O
.0164	O
.0007	O
.1928	O
4.1	O
6.3	O
5.2	O
5.1	O
3.5	O
5.9	O
6.2	O
5.0	O
4.1	O
10.7	O
6.9	O
4.9	O
5.4	O
4.1	O
3.9	O
5.7	O
10.3	O
4.3	O
4.1	O
3.8	O
4.9	O
7.2	O
6.4	O
7.1	O
5.9	O
10.4	O
2.4	O
4.1	O
what	O
do	O
you	O
notice	O
about	O
your	O
solutions	O
?	O
does	O
each	O
answer	O
depend	O
on	O
the	O
detailed	O
contents	O
of	O
each	O
urn	B
?	O
the	O
details	O
of	O
the	O
other	O
possible	O
outcomes	O
and	O
their	O
probabilities	O
are	O
ir-	O
relevant	O
.	O
all	O
that	O
matters	O
is	O
the	O
probability	O
of	O
the	O
outcome	O
that	O
actually	O
happened	O
(	O
here	O
,	O
that	O
the	O
ball	O
drawn	O
was	O
black	B
)	O
given	O
the	O
di	O
(	O
cid:11	O
)	O
erent	O
hypothe-	O
ses	O
.	O
we	O
need	O
only	O
to	O
know	O
the	O
likelihood	B
,	O
i.e.	O
,	O
how	O
the	O
probability	O
of	O
the	O
data	O
that	O
happened	O
varies	O
with	O
the	O
hypothesis	O
.	O
this	O
simple	O
rule	O
about	O
inference	B
is	O
known	O
as	O
the	O
likelihood	B
principle	I
.	O
the	O
likelihood	B
principle	I
:	O
given	O
a	O
generative	B
model	I
for	O
data	O
d	O
given	O
parameters	B
(	O
cid:18	O
)	O
,	O
p	O
(	O
dj	O
(	O
cid:18	O
)	O
)	O
,	O
and	O
having	O
observed	O
a	O
particular	O
outcome	O
d1	O
,	O
all	O
inferences	O
and	O
predictions	O
should	O
depend	O
only	O
on	O
the	O
function	B
p	O
(	O
d1	O
j	O
(	O
cid:18	O
)	O
)	O
.	O
in	O
spite	O
of	O
the	O
simplicity	O
of	O
this	O
principle	O
,	O
many	O
classical	O
statistical	O
methods	B
violate	O
it	O
.	O
2.4	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
entropy	O
and	O
related	O
functions	B
the	O
shannon	O
information	O
content	B
of	O
an	O
outcome	O
x	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
h	O
(	O
x	O
)	O
=	O
log2	O
1	O
p	O
(	O
x	O
)	O
:	O
(	O
2.34	O
)	O
it	O
is	O
measured	O
in	O
bits	O
.	O
[	O
the	O
word	O
‘	O
bit	B
’	O
is	O
also	O
used	O
to	O
denote	O
a	O
variable	O
whose	O
value	O
is	O
0	O
or	O
1	O
;	O
i	O
hope	O
context	O
will	O
always	O
make	O
clear	O
which	O
of	O
the	O
two	O
meanings	O
is	O
intended	O
.	O
]	O
in	O
the	O
next	O
few	O
chapters	O
,	O
we	O
will	O
establish	O
that	O
the	O
shannon	O
information	O
content	B
h	O
(	O
ai	O
)	O
is	O
indeed	O
a	O
natural	B
measure	O
of	O
the	O
information	O
content	B
of	O
the	O
event	O
x	O
=	O
ai	O
.	O
at	O
that	O
point	O
,	O
we	O
will	O
shorten	O
the	O
name	O
of	O
this	O
quantity	O
to	O
‘	O
the	O
information	B
content	I
’	O
.	O
the	O
fourth	O
column	O
in	O
table	O
2.9	O
shows	O
the	O
shannon	O
information	O
content	B
of	O
the	O
27	O
possible	O
outcomes	O
when	O
a	O
random	B
character	O
is	O
picked	O
from	O
an	O
english	O
document	O
.	O
the	O
outcome	O
x	O
=	O
z	O
has	O
a	O
shannon	O
information	O
content	B
of	O
10.4	O
bits	O
,	O
and	O
x	O
=	O
e	O
has	O
an	O
information	B
content	I
of	O
3.5	O
bits	O
.	O
the	O
entropy	B
of	O
an	O
ensemble	B
x	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
the	O
average	B
shannon	O
in-	O
formation	O
content	B
of	O
an	O
outcome	O
:	O
h	O
(	O
x	O
)	O
(	O
cid:17	O
)	O
xx2ax	O
p	O
(	O
x	O
)	O
log	O
1	O
p	O
(	O
x	O
)	O
;	O
convention	O
for	O
p	O
(	O
x	O
)	O
=	O
0	O
with	O
the	O
lim	O
(	O
cid:18	O
)	O
!	O
0+	O
(	O
cid:18	O
)	O
log	O
1=	O
(	O
cid:18	O
)	O
=	O
0.	O
like	O
the	O
information	B
content	I
,	O
entropy	B
is	O
measured	O
in	O
bits	O
.	O
that	O
0	O
(	O
cid:2	O
)	O
log	O
1=0	O
(	O
cid:17	O
)	O
0	O
,	O
(	O
2.35	O
)	O
since	O
when	O
it	O
is	O
convenient	O
,	O
we	O
may	O
also	O
write	O
h	O
(	O
x	O
)	O
as	O
h	O
(	O
p	O
)	O
,	O
where	O
p	O
is	O
the	O
vector	O
(	O
p1	O
;	O
p2	O
;	O
:	O
:	O
:	O
;	O
pi	O
)	O
.	O
another	O
name	O
for	O
the	O
entropy	B
of	O
x	O
is	O
the	O
uncertainty	O
of	O
x.	O
example	O
2.12.	O
the	O
entropy	B
of	O
a	O
randomly	O
selected	O
letter	O
in	O
an	O
english	O
docu-	O
ment	O
is	O
about	O
4.11	O
bits	O
,	O
assuming	O
its	O
probability	B
is	O
as	O
given	O
in	O
table	O
2.9.	O
we	O
obtain	O
this	O
number	O
by	O
averaging	O
log	O
1=pi	O
(	O
shown	O
in	O
the	O
fourth	O
col-	O
umn	O
)	O
under	O
the	O
probability	B
distribution	O
pi	O
(	O
shown	O
in	O
the	O
third	O
column	O
)	O
.	O
pi	O
log2	O
1	O
pi	O
xi	O
table	O
2.9.	O
shannon	O
information	O
contents	O
of	O
the	O
outcomes	O
a	O
{	O
z	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
2.5	O
:	O
decomposability	O
of	O
the	O
entropy	O
33	O
we	O
now	O
note	O
some	O
properties	O
of	O
the	O
entropy	O
function	B
.	O
(	O
cid:15	O
)	O
h	O
(	O
x	O
)	O
(	O
cid:21	O
)	O
0	O
with	O
equality	O
i	O
(	O
cid:11	O
)	O
pi	O
=	O
1	O
for	O
one	O
i	O
.	O
[	O
‘	O
i	O
(	O
cid:11	O
)	O
’	O
means	O
‘	O
if	O
and	O
only	O
if	O
’	O
.	O
]	O
(	O
cid:15	O
)	O
entropy	B
is	O
maximized	O
if	O
p	O
is	O
uniform	O
:	O
h	O
(	O
x	O
)	O
(	O
cid:20	O
)	O
log	O
(	O
jaxj	O
)	O
with	O
equality	O
i	O
(	O
cid:11	O
)	O
pi	O
=	O
1=jaxj	O
for	O
all	O
i	O
.	O
(	O
2.36	O
)	O
notation	B
:	O
the	O
vertical	O
bars	O
‘	O
j	O
(	O
cid:1	O
)	O
j	O
’	O
have	O
two	O
meanings	O
.	O
if	O
ax	O
is	O
a	O
set	B
,	O
jaxj	O
denotes	O
the	O
number	O
of	O
elements	O
in	O
ax	O
;	O
if	O
x	O
is	O
a	O
number	O
,	O
then	O
jxj	O
is	O
the	O
absolute	B
value	I
of	O
x.	O
the	O
redundancy	B
measures	O
the	O
fractional	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
h	O
(	O
x	O
)	O
and	O
its	O
max-	O
imum	O
possible	O
value	O
,	O
log	O
(	O
jaxj	O
)	O
.	O
the	O
redundancy	B
of	O
x	O
is	O
:	O
1	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
)	O
log	O
jaxj	O
:	O
(	O
2.37	O
)	O
we	O
won	O
’	O
t	O
make	O
use	O
of	O
‘	O
redundancy	B
’	O
in	O
this	O
book	O
,	O
so	O
i	O
have	O
not	O
assigned	O
a	O
symbol	O
to	O
it	O
.	O
the	O
joint	B
entropy	I
of	O
x	O
;	O
y	O
is	O
:	O
h	O
(	O
x	O
;	O
y	O
)	O
=	O
xxy2axay	O
p	O
(	O
x	O
;	O
y	O
)	O
log	O
1	O
p	O
(	O
x	O
;	O
y	O
)	O
:	O
(	O
2.38	O
)	O
entropy	B
is	O
additive	O
for	O
independent	O
random	B
variables	O
:	O
h	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
+	O
h	O
(	O
y	O
)	O
i	O
(	O
cid:11	O
)	O
p	O
(	O
x	O
;	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
:	O
(	O
2.39	O
)	O
our	O
de	O
(	O
cid:12	O
)	O
nitions	O
for	O
information	O
content	B
so	O
far	O
apply	O
only	O
to	O
discrete	O
probability	O
distributions	O
over	O
(	O
cid:12	O
)	O
nite	O
sets	O
ax	O
.	O
the	O
de	O
(	O
cid:12	O
)	O
nitions	O
can	O
be	O
extended	B
to	O
in	O
(	O
cid:12	O
)	O
nite	O
sets	O
,	O
though	O
the	O
entropy	B
may	O
then	O
be	O
in	O
(	O
cid:12	O
)	O
nite	O
.	O
the	O
case	O
of	O
a	O
probability	B
density	O
over	O
a	O
continuous	B
set	O
is	O
addressed	O
in	O
section	O
11.3.	O
further	O
important	O
de	O
(	O
cid:12	O
)	O
nitions	O
and	O
exercises	O
to	O
do	O
with	O
entropy	B
will	O
come	O
along	O
in	O
section	O
8.1	O
.	O
2.5	O
decomposability	O
of	O
the	O
entropy	O
the	O
entropy	B
function	O
satis	O
(	O
cid:12	O
)	O
es	O
a	O
recursive	O
property	O
that	O
can	O
be	O
very	O
useful	O
when	O
computing	O
entropies	O
.	O
for	O
convenience	O
,	O
we	O
’	O
ll	O
stretch	O
our	O
notation	B
so	O
that	O
we	O
can	O
write	O
h	O
(	O
x	O
)	O
as	O
h	O
(	O
p	O
)	O
,	O
where	O
p	O
is	O
the	O
probability	B
vector	O
associated	O
with	O
the	O
ensemble	B
x.	O
let	O
’	O
s	O
illustrate	O
the	O
property	O
by	O
an	O
example	O
(	O
cid:12	O
)	O
rst	O
.	O
imagine	O
that	O
a	O
random	B
variable	I
x	O
2	O
f0	O
;	O
1	O
;	O
2g	O
is	O
created	O
by	O
(	O
cid:12	O
)	O
rst	O
(	O
cid:13	O
)	O
ipping	O
a	O
fair	O
coin	B
to	O
determine	O
whether	O
x	O
=	O
0	O
;	O
then	O
,	O
if	O
x	O
is	O
not	O
0	O
,	O
(	O
cid:13	O
)	O
ipping	O
a	O
fair	O
coin	B
a	O
second	O
time	O
to	O
determine	O
whether	O
x	O
is	O
1	O
or	O
2.	O
the	O
probability	B
distribution	O
of	O
x	O
is	O
p	O
(	O
x	O
=	O
0	O
)	O
=	O
1	O
2	O
;	O
p	O
(	O
x	O
=	O
1	O
)	O
=	O
1	O
4	O
;	O
p	O
(	O
x	O
=	O
2	O
)	O
=	O
1	O
4	O
:	O
(	O
2.40	O
)	O
what	O
is	O
the	O
entropy	B
of	O
x	O
?	O
we	O
can	O
either	O
compute	O
it	O
by	O
brute	O
force	O
:	O
h	O
(	O
x	O
)	O
=	O
1/2	O
log	O
2	O
+	O
1/4	O
log	O
4	O
+	O
1/4	O
log	O
4	O
=	O
1:5	O
;	O
(	O
2.41	O
)	O
or	O
we	O
can	O
use	O
the	O
following	O
decomposition	O
,	O
in	O
which	O
the	O
value	O
of	O
x	O
is	O
revealed	O
gradually	O
.	O
imagine	O
(	O
cid:12	O
)	O
rst	O
learning	B
whether	O
x	O
=	O
0	O
,	O
and	O
then	O
,	O
if	O
x	O
is	O
not	O
0	O
,	O
learning	B
which	O
non-zero	O
value	O
is	O
the	O
case	O
.	O
the	O
revelation	O
of	O
whether	O
x	O
=	O
0	O
or	O
not	O
entails	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
34	O
2	O
|	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
revealing	O
a	O
binary	O
variable	O
whose	O
probability	B
distribution	O
is	O
f1/2	O
;	O
1/2g	O
.	O
this	O
revelation	O
has	O
an	O
entropy	B
h	O
(	O
1/2	O
;	O
1/2	O
)	O
=	O
1	O
2	O
log	O
2	O
=	O
1	O
bit	B
.	O
if	O
x	O
is	O
not	O
0	O
,	O
we	O
learn	O
the	O
value	O
of	O
the	O
second	O
coin	B
(	O
cid:13	O
)	O
ip	O
.	O
this	O
too	O
is	O
a	O
binary	O
variable	O
whose	O
probability	B
distribution	O
is	O
f1/2	O
;	O
1/2g	O
,	O
and	O
whose	O
entropy	B
is	O
1	O
bit	B
.	O
we	O
only	O
get	O
to	O
experience	O
the	O
second	O
revelation	O
half	O
the	O
time	O
,	O
however	O
,	O
so	O
the	O
entropy	B
can	O
be	O
written	O
:	O
2	O
log	O
2	O
+	O
1	O
h	O
(	O
x	O
)	O
=	O
h	O
(	O
1/2	O
;	O
1/2	O
)	O
+	O
1/2	O
h	O
(	O
1/2	O
;	O
1/2	O
)	O
:	O
(	O
2.42	O
)	O
generalizing	O
,	O
the	O
observation	O
we	O
are	O
making	O
about	O
the	O
entropy	B
of	O
any	O
probability	B
distribution	O
p	O
=	O
fp1	O
;	O
p2	O
;	O
:	O
:	O
:	O
;	O
pig	O
is	O
that	O
p3	O
1	O
(	O
cid:0	O
)	O
p1	O
h	O
(	O
p	O
)	O
=	O
h	O
(	O
p1	O
;	O
1	O
(	O
cid:0	O
)	O
p1	O
)	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
p1	O
)	O
h	O
(	O
cid:18	O
)	O
p2	O
1	O
(	O
cid:0	O
)	O
p1	O
;	O
;	O
:	O
:	O
:	O
;	O
pi	O
1	O
(	O
cid:0	O
)	O
p1	O
(	O
cid:19	O
)	O
:	O
(	O
2.43	O
)	O
when	O
it	O
’	O
s	O
written	O
as	O
a	O
formula	O
,	O
this	O
property	O
looks	O
regrettably	O
ugly	O
;	O
nev-	O
ertheless	O
it	O
is	O
a	O
simple	O
property	O
and	O
one	O
that	O
you	O
should	O
make	O
use	O
of	O
.	O
generalizing	O
further	O
,	O
the	O
entropy	B
has	O
the	O
property	O
for	O
any	O
m	O
that	O
h	O
(	O
p	O
)	O
=	O
h	O
[	O
(	O
p1	O
+	O
p2	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
pm	O
)	O
;	O
(	O
pm+1	O
+	O
pm+2	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
pi	O
)	O
]	O
pm	O
p1	O
;	O
:	O
:	O
:	O
;	O
(	O
p1	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
pm	O
)	O
(	O
cid:19	O
)	O
;	O
:	O
:	O
:	O
;	O
pi	O
(	O
pm+1	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
pi	O
)	O
(	O
cid:19	O
)	O
:	O
(	O
2.44	O
)	O
+	O
(	O
p1	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
pm	O
)	O
h	O
(	O
cid:18	O
)	O
+	O
(	O
pm+1	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
pi	O
)	O
h	O
(	O
cid:18	O
)	O
(	O
p1	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
pm	O
)	O
pm+1	O
(	O
pm+1	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
pi	O
)	O
example	O
2.13.	O
a	O
source	O
produces	O
a	O
character	O
x	O
from	O
the	O
alphabet	O
a	O
=	O
f0	O
;	O
1	O
;	O
:	O
:	O
:	O
;	O
9	O
;	O
a	O
;	O
b	O
;	O
:	O
:	O
:	O
;	O
zg	O
;	O
with	O
probability	O
1/3	O
,	O
x	O
is	O
a	O
numeral	O
(	O
0	O
;	O
:	O
:	O
:	O
;	O
9	O
)	O
;	O
with	O
probability	O
1/3	O
,	O
x	O
is	O
a	O
vowel	O
(	O
a	O
;	O
e	O
;	O
i	O
;	O
o	O
;	O
u	O
)	O
;	O
and	O
with	O
probability	B
1/3	O
it	O
’	O
s	O
one	O
of	O
the	O
21	O
consonants	O
.	O
all	O
numerals	O
are	O
equiprobable	O
,	O
and	O
the	O
same	O
goes	O
for	O
vowels	O
and	O
consonants	O
.	O
estimate	O
the	O
entropy	B
of	O
x.	O
solution	O
.	O
log	O
3	O
+	O
1	O
3	O
(	O
log	O
10	O
+	O
log	O
5	O
+	O
log	O
21	O
)	O
=	O
log	O
3	O
+	O
1	O
3	O
log	O
1050	O
’	O
log	O
30	O
bits	O
.	O
2	O
the	O
‘	O
ei	O
’	O
in	O
leibler	O
is	O
pronounced	O
the	O
same	O
as	O
in	O
heist	O
.	O
2.6	O
gibbs	O
’	O
inequality	B
the	O
relative	B
entropy	I
or	O
kullback	O
{	O
leibler	O
divergence	B
between	O
two	O
probability	B
distributions	I
p	O
(	O
x	O
)	O
and	O
q	O
(	O
x	O
)	O
that	O
are	O
de	O
(	O
cid:12	O
)	O
ned	O
over	O
the	O
same	O
alphabet	O
ax	O
is	O
dkl	O
(	O
pjjq	O
)	O
=xx	O
p	O
(	O
x	O
)	O
log	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
:	O
the	O
relative	B
entropy	I
satis	O
(	O
cid:12	O
)	O
es	O
gibbs	O
’	O
inequality	B
dkl	O
(	O
pjjq	O
)	O
(	O
cid:21	O
)	O
0	O
(	O
2.45	O
)	O
(	O
2.46	O
)	O
with	O
equality	O
only	O
if	O
p	O
=	O
q.	O
note	O
that	O
in	O
general	O
the	O
relative	B
entropy	I
is	O
not	O
symmetric	B
under	O
interchange	O
of	O
the	O
distributions	O
p	O
and	O
q	O
:	O
in	O
general	O
dkl	O
(	O
pjjq	O
)	O
6=	O
dkl	O
(	O
qjjp	O
)	O
,	O
so	O
dkl	O
,	O
although	O
it	O
is	O
sometimes	O
called	O
the	O
‘	O
kl	O
distance	B
’	O
,	O
is	O
not	O
strictly	O
a	O
distance	B
.	O
the	O
relative	B
entropy	I
is	O
important	O
in	O
pattern	O
recognition	B
and	O
neural	O
networks	O
,	O
as	O
well	O
as	O
in	O
information	B
theory	I
.	O
gibbs	O
’	O
inequality	B
is	O
probably	O
the	O
most	O
important	O
inequality	B
in	O
this	O
book	O
.	O
it	O
,	O
and	O
many	O
other	O
inequalities	O
,	O
can	O
be	O
proved	O
using	O
the	O
concept	O
of	O
convexity	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
2.7	O
:	O
jensen	O
’	O
s	O
inequality	B
for	O
convex	B
functions	O
35	O
2.7	O
jensen	O
’	O
s	O
inequality	B
for	O
convex	B
functions	O
the	O
words	O
‘	O
convex	B
^	O
’	O
and	O
‘	O
concave	B
_	I
’	O
may	O
be	O
pronounced	O
‘	O
convex-smile	O
’	O
and	O
‘	O
concave-frown	O
’	O
.	O
this	O
terminology	B
has	O
useful	O
redundancy	B
:	O
while	O
one	O
may	O
forget	O
which	O
way	O
up	O
‘	O
convex	B
’	O
and	O
‘	O
concave	O
’	O
are	O
,	O
it	O
is	O
harder	O
to	O
confuse	O
a	O
smile	O
with	O
a	O
frown	O
.	O
convex	B
^	O
functions	B
.	O
a	O
function	B
f	O
(	O
x	O
)	O
is	O
convex	B
^	O
over	O
(	O
a	O
;	O
b	O
)	O
if	O
every	O
chord	O
of	O
the	O
function	O
lies	O
above	O
the	O
function	B
,	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
2.10	O
;	O
that	O
is	O
,	O
for	O
all	O
x1	O
;	O
x2	O
2	O
(	O
a	O
;	O
b	O
)	O
and	O
0	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
1	O
,	O
f	O
(	O
(	O
cid:21	O
)	O
x1	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
)	O
x2	O
)	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
f	O
(	O
x1	O
)	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
)	O
f	O
(	O
x2	O
)	O
:	O
(	O
2.47	O
)	O
a	O
function	B
f	O
is	O
strictly	O
convex	B
^	O
if	O
,	O
for	O
all	O
x1	O
;	O
x2	O
2	O
(	O
a	O
;	O
b	O
)	O
,	O
the	O
equality	O
holds	O
only	O
for	O
(	O
cid:21	O
)	O
=	O
0	O
and	O
(	O
cid:21	O
)	O
=	O
1.	O
similar	O
de	O
(	O
cid:12	O
)	O
nitions	O
apply	O
to	O
concave	B
_	I
and	O
strictly	O
concave	B
_	I
functions	O
.	O
(	O
cid:21	O
)	O
f	O
(	O
x1	O
)	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
)	O
f	O
(	O
x2	O
)	O
f	O
(	O
x	O
(	O
cid:3	O
)	O
)	O
x1	O
x2	O
x	O
(	O
cid:3	O
)	O
=	O
(	O
cid:21	O
)	O
x1	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
)	O
x2	O
figure	O
2.10.	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
convexity	O
.	O
some	O
strictly	O
convex	B
^	O
functions	B
are	O
(	O
cid:15	O
)	O
x2	O
,	O
ex	O
and	O
e	O
(	O
cid:0	O
)	O
x	O
for	O
all	O
x	O
;	O
(	O
cid:15	O
)	O
log	O
(	O
1=x	O
)	O
and	O
x	O
log	O
x	O
for	O
x	O
>	O
0.	O
x2	O
e	O
(	O
cid:0	O
)	O
x	O
log	O
1	O
x	O
x	O
log	O
x	O
figure	O
2.11.	O
convex	B
^	O
functions	B
.	O
centre	B
of	I
gravity	I
-1	O
0	O
1	O
2	O
3	O
-1	O
0	O
1	O
2	O
3	O
0	O
1	O
2	O
3	O
0	O
1	O
2	O
3	O
jensen	O
’	O
s	O
inequality	B
.	O
if	O
f	O
is	O
a	O
convex	B
^	O
function	B
and	O
x	O
is	O
a	O
random	B
variable	I
then	O
:	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
(	O
cid:21	O
)	O
f	O
(	O
e	O
[	O
x	O
]	O
)	O
;	O
(	O
2.48	O
)	O
where	O
e	O
denotes	O
expectation	B
.	O
if	O
f	O
is	O
strictly	O
convex	B
^	O
and	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
=	O
f	O
(	O
e	O
[	O
x	O
]	O
)	O
,	O
then	O
the	O
random	B
variable	I
x	O
is	O
a	O
constant	O
.	O
jensen	O
’	O
s	O
inequality	B
can	O
also	O
be	O
rewritten	O
for	O
a	O
concave	B
_	I
function	O
,	O
with	O
the	O
direction	O
of	O
the	O
inequality	O
reversed	O
.	O
a	O
physical	O
version	O
of	O
jensen	O
’	O
s	O
inequality	B
runs	O
as	O
follows	O
.	O
if	O
a	O
collection	O
of	O
masses	O
pi	O
are	O
placed	O
on	O
a	O
convex	B
^	O
curve	O
f	O
(	O
x	O
)	O
at	O
locations	O
(	O
xi	O
;	O
f	O
(	O
xi	O
)	O
)	O
,	O
then	O
the	O
centre	B
of	I
gravity	I
of	O
those	O
masses	O
,	O
which	O
is	O
at	O
(	O
e	O
[	O
x	O
]	O
;	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
)	O
,	O
lies	O
above	O
the	O
curve	O
.	O
if	O
this	O
fails	O
to	O
convince	O
you	O
,	O
then	O
feel	O
free	O
to	O
do	O
the	O
following	O
exercise	O
.	O
exercise	O
2.14	O
.	O
[	O
2	O
,	O
p.41	O
]	O
prove	O
jensen	O
’	O
s	O
inequality	B
.	O
example	O
2.15.	O
three	O
squares	O
have	O
average	B
area	O
(	O
cid:22	O
)	O
a	O
=	O
100	O
m2	O
.	O
the	O
average	B
of	O
the	O
lengths	O
of	O
their	O
sides	O
is	O
(	O
cid:22	O
)	O
l	O
=	O
10	O
m.	O
what	O
can	O
be	O
said	O
about	O
the	O
size	O
of	O
the	O
largest	O
of	O
the	O
three	O
squares	O
?	O
[	O
use	O
jensen	O
’	O
s	O
inequality	B
.	O
]	O
solution	O
.	O
let	O
x	O
be	O
the	O
length	B
of	O
the	O
side	O
of	O
a	O
square	B
,	O
and	O
let	O
the	O
probability	O
of	O
x	O
be	O
1/3	O
;	O
1/3	O
;	O
1/3	O
over	O
the	O
three	O
lengths	O
l1	O
;	O
l2	O
;	O
l3	O
.	O
then	O
the	O
information	B
that	O
we	O
have	O
is	O
that	O
e	O
[	O
x	O
]	O
=	O
10	O
and	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
=	O
100	O
,	O
where	O
f	O
(	O
x	O
)	O
=	O
x2	O
is	O
the	O
function	B
mapping	O
lengths	O
to	O
areas	O
.	O
this	O
is	O
a	O
strictly	O
convex	B
^	O
function	B
.	O
we	O
notice	O
that	O
the	O
equality	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
=	O
f	O
(	O
e	O
[	O
x	O
]	O
)	O
holds	O
,	O
therefore	O
x	O
is	O
a	O
constant	O
,	O
and	O
the	O
three	O
lengths	O
must	O
all	O
be	O
equal	O
.	O
the	O
area	O
of	O
the	O
largest	O
square	B
is	O
100	O
m2	O
.	O
2	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
36	O
2	O
|	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
convexity	B
and	O
concavity	O
also	O
relate	O
to	O
maximization	O
if	O
f	O
(	O
x	O
)	O
is	O
concave	B
_	I
and	O
there	O
exists	O
a	O
point	O
at	O
which	O
@	O
f	O
@	O
xk	O
=	O
0	O
for	O
all	O
k	O
;	O
(	O
2.49	O
)	O
then	O
f	O
(	O
x	O
)	O
has	O
its	O
maximum	O
value	O
at	O
that	O
point	O
.	O
the	O
converse	O
does	O
not	O
hold	O
:	O
if	O
a	O
concave	B
_	I
f	O
(	O
x	O
)	O
is	O
maximized	O
at	O
some	O
x	O
it	O
is	O
not	O
necessarily	O
true	O
that	O
the	O
gradient	O
rf	O
(	O
x	O
)	O
is	O
equal	O
to	O
zero	O
there	O
.	O
for	O
example	O
,	O
f	O
(	O
x	O
)	O
=	O
(	O
cid:0	O
)	O
jxj	O
is	O
maximized	O
at	O
x	O
=	O
0	O
where	O
its	O
derivative	O
is	O
unde	O
(	O
cid:12	O
)	O
ned	O
;	O
and	O
f	O
(	O
p	O
)	O
=	O
log	O
(	O
p	O
)	O
;	O
for	O
a	O
probability	B
p	O
2	O
(	O
0	O
;	O
1	O
)	O
,	O
is	O
maximized	O
on	O
the	O
boundary	O
of	O
the	O
range	O
,	O
at	O
p	O
=	O
1	O
,	O
where	O
the	O
gradient	O
df	O
(	O
p	O
)	O
=dp	O
=	O
1	O
.	O
2.8	O
exercises	O
sums	O
of	O
random	O
variables	O
exercise	O
2.16	O
.	O
[	O
3	O
,	O
p.41	O
]	O
(	O
a	O
)	O
two	O
ordinary	O
dice	O
with	O
faces	O
labelled	O
1	O
;	O
:	O
:	O
:	O
;	O
6	O
are	O
thrown	O
.	O
what	O
is	O
the	O
probability	B
distribution	O
of	O
the	O
sum	O
of	O
the	O
val-	O
ues	O
?	O
what	O
is	O
the	O
probability	B
distribution	O
of	O
the	O
absolute	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
the	O
values	O
?	O
(	O
b	O
)	O
one	O
hundred	O
ordinary	O
dice	O
are	O
thrown	O
.	O
what	O
,	O
roughly	O
,	O
is	O
the	O
prob-	O
ability	O
distribution	B
of	O
the	O
sum	O
of	O
the	O
values	O
?	O
sketch	O
the	O
probability	B
distribution	O
and	O
estimate	O
its	O
mean	B
and	O
standard	B
deviation	I
.	O
(	O
c	O
)	O
how	O
can	O
two	O
cubical	O
dice	O
be	O
labelled	O
using	O
the	O
numbers	O
f0	O
;	O
1	O
;	O
2	O
;	O
3	O
;	O
4	O
;	O
5	O
;	O
6g	O
so	O
that	O
when	O
the	O
two	O
dice	O
are	O
thrown	O
the	O
sum	O
has	O
a	O
uniform	O
probability	B
distribution	O
over	O
the	O
integers	O
1	O
{	O
12	O
?	O
(	O
d	O
)	O
is	O
there	O
any	O
way	O
that	O
one	O
hundred	O
dice	O
could	O
be	O
labelled	O
with	O
inte-	O
gers	O
such	O
that	O
the	O
probability	B
distribution	O
of	O
the	O
sum	O
is	O
uniform	O
?	O
inference	B
problems	O
exercise	O
2.17	O
.	O
[	O
2	O
,	O
p.41	O
]	O
if	O
q	O
=	O
1	O
(	O
cid:0	O
)	O
p	O
and	O
a	O
=	O
ln	O
p=q	O
,	O
show	O
that	O
p	O
=	O
1	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
a	O
)	O
:	O
(	O
2.50	O
)	O
sketch	O
this	O
function	B
and	O
(	O
cid:12	O
)	O
nd	O
its	O
relationship	O
to	O
the	O
hyperbolic	O
tangent	O
function	B
tanh	O
(	O
u	O
)	O
=	O
eu	O
(	O
cid:0	O
)	O
e	O
(	O
cid:0	O
)	O
u	O
eu+e	O
(	O
cid:0	O
)	O
u	O
.	O
it	O
will	O
be	O
useful	O
to	O
be	O
(	O
cid:13	O
)	O
uent	O
in	O
base-2	O
logarithms	B
also	O
.	O
if	O
b	O
=	O
log	O
2	O
p=q	O
,	O
what	O
is	O
p	O
as	O
a	O
function	B
of	O
b	O
?	O
.	O
exercise	O
2.18	O
.	O
[	O
2	O
,	O
p.42	O
]	O
let	O
x	O
and	O
y	O
be	O
dependent	O
random	O
variables	O
with	O
x	O
a	O
binary	O
variable	O
taking	O
values	O
in	O
ax	O
=	O
f0	O
;	O
1g	O
.	O
use	O
bayes	O
’	O
theorem	B
to	O
show	O
that	O
the	O
log	O
posterior	B
probability	I
ratio	O
for	O
x	O
given	O
y	O
is	O
log	O
p	O
(	O
x	O
=	O
1j	O
y	O
)	O
p	O
(	O
x	O
=	O
0j	O
y	O
)	O
=	O
log	O
p	O
(	O
y	O
j	O
x	O
=	O
1	O
)	O
p	O
(	O
y	O
j	O
x	O
=	O
0	O
)	O
+	O
log	O
p	O
(	O
x	O
=	O
1	O
)	O
p	O
(	O
x	O
=	O
0	O
)	O
:	O
(	O
2.51	O
)	O
.	O
exercise	O
2.19	O
.	O
[	O
2	O
,	O
p.42	O
]	O
let	O
x	O
,	O
d1	O
and	O
d2	O
be	O
random	B
variables	O
such	O
that	O
d1	O
and	O
d2	O
are	O
conditionally	O
independent	O
given	O
a	O
binary	O
variable	O
x.	O
use	O
bayes	O
’	O
theorem	B
to	O
show	O
that	O
the	O
posterior	B
probability	I
ratio	O
for	O
x	O
given	O
fdig	O
is	O
p	O
(	O
x	O
=	O
1jfdig	O
)	O
p	O
(	O
x	O
=	O
0jfdig	O
)	O
=	O
p	O
(	O
d1	O
j	O
x	O
=	O
1	O
)	O
p	O
(	O
d1	O
j	O
x	O
=	O
0	O
)	O
p	O
(	O
d2	O
j	O
x	O
=	O
1	O
)	O
p	O
(	O
d2	O
j	O
x	O
=	O
0	O
)	O
p	O
(	O
x	O
=	O
1	O
)	O
p	O
(	O
x	O
=	O
0	O
)	O
:	O
(	O
2.52	O
)	O
this	O
exercise	O
is	O
intended	O
to	O
help	O
you	O
think	O
about	O
the	O
central-limit	B
theorem	I
,	O
which	O
says	O
that	O
if	O
independent	O
random	O
variables	O
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
have	O
means	O
(	O
cid:22	O
)	O
n	O
and	O
(	O
cid:12	O
)	O
nite	O
variances	O
(	O
cid:27	O
)	O
2	O
n	O
,	O
then	O
,	O
in	O
the	O
has	O
a	O
distribution	B
that	O
tends	O
to	O
a	O
normal	B
(	O
gaussian	O
)	O
distribution	B
limit	O
of	O
large	O
n	O
,	O
the	O
sum	O
pn	O
xn	O
with	O
meanpn	O
(	O
cid:22	O
)	O
n	O
and	O
variance	O
pn	O
(	O
cid:27	O
)	O
2	O
n.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
2.8	O
:	O
exercises	O
life	O
in	O
high-dimensional	O
spaces	O
probability	B
distributions	I
and	O
volumes	O
have	O
some	O
unexpected	O
properties	O
in	O
high-dimensional	O
spaces	O
.	O
37	O
exercise	O
2.20	O
.	O
[	O
2	O
,	O
p.42	O
]	O
consider	O
a	O
sphere	O
of	O
radius	O
r	O
in	O
an	O
n	O
-dimensional	O
real	O
space	O
.	O
show	O
that	O
the	O
fraction	O
of	O
the	O
volume	O
of	O
the	O
sphere	O
that	O
is	O
in	O
the	O
surface	O
shell	O
lying	O
at	O
values	O
of	O
the	O
radius	O
between	O
r	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
and	O
r	O
,	O
where	O
0	O
<	O
(	O
cid:15	O
)	O
<	O
r	O
,	O
is	O
:	O
f	O
=	O
1	O
(	O
cid:0	O
)	O
(	O
cid:16	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
r	O
(	O
cid:17	O
)	O
n	O
:	O
(	O
2.53	O
)	O
evaluate	O
f	O
for	O
the	O
cases	O
n	O
=	O
2	O
,	O
n	O
=	O
10	O
and	O
n	O
=	O
1000	O
,	O
with	O
(	O
a	O
)	O
(	O
cid:15	O
)	O
=r	O
=	O
0:01	O
;	O
(	O
b	O
)	O
(	O
cid:15	O
)	O
=r	O
=	O
0:5.	O
implication	O
:	O
points	O
that	O
are	O
uniformly	O
distributed	O
in	O
a	O
sphere	O
in	O
n	O
di-	O
mensions	O
,	O
where	O
n	O
is	O
large	O
,	O
are	O
very	O
likely	O
to	O
be	O
in	O
a	O
thin	B
shell	I
near	O
the	O
surface	O
.	O
expectations	O
and	O
entropies	O
you	O
are	O
probably	O
familiar	O
with	O
the	O
idea	O
of	O
computing	O
the	O
expectation	B
of	O
a	O
function	B
of	O
x	O
,	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
=	O
hf	O
(	O
x	O
)	O
i	O
=xx	O
p	O
(	O
x	O
)	O
f	O
(	O
x	O
)	O
:	O
(	O
2.54	O
)	O
maybe	O
you	O
are	O
not	O
so	O
comfortable	O
with	O
computing	O
this	O
expectation	B
in	O
cases	O
where	O
the	O
function	B
f	O
(	O
x	O
)	O
depends	O
on	O
the	O
probability	B
p	O
(	O
x	O
)	O
.	O
the	O
next	O
few	O
ex-	O
amples	O
address	B
this	O
concern	O
.	O
exercise	O
2.21	O
.	O
[	O
1	O
,	O
p.43	O
]	O
let	O
pa	O
=	O
0:1	O
,	O
pb	O
=	O
0:2	O
,	O
and	O
pc	O
=	O
0:7.	O
let	O
f	O
(	O
a	O
)	O
=	O
10	O
,	O
f	O
(	O
b	O
)	O
=	O
5	O
,	O
and	O
f	O
(	O
c	O
)	O
=	O
10=7	O
.	O
what	O
is	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
?	O
what	O
is	O
e	O
[	O
1=p	O
(	O
x	O
)	O
]	O
?	O
exercise	O
2.22	O
.	O
[	O
2	O
,	O
p.43	O
]	O
for	O
an	O
arbitrary	O
ensemble	B
,	O
what	O
is	O
e	O
[	O
1=p	O
(	O
x	O
)	O
]	O
?	O
.	O
exercise	O
2.23	O
.	O
[	O
1	O
,	O
p.43	O
]	O
let	O
pa	O
=	O
0:1	O
,	O
pb	O
=	O
0:2	O
,	O
and	O
pc	O
=	O
0:7.	O
let	O
g	O
(	O
a	O
)	O
=	O
0	O
,	O
g	O
(	O
b	O
)	O
=	O
1	O
,	O
and	O
g	O
(	O
c	O
)	O
=	O
0.	O
what	O
is	O
e	O
[	O
g	O
(	O
x	O
)	O
]	O
?	O
.	O
exercise	O
2.24	O
.	O
[	O
1	O
,	O
p.43	O
]	O
let	O
pa	O
=	O
0:1	O
,	O
pb	O
=	O
0:2	O
,	O
and	O
pc	O
=	O
0:7.	O
what	O
is	O
the	O
proba-	O
bility	O
that	O
p	O
(	O
x	O
)	O
2	O
[	O
0:15	O
;	O
0:5	O
]	O
?	O
what	O
is	O
p	O
(	O
x	O
)	O
log	O
p	O
(	O
cid:18	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
0:2	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
>	O
0:05	O
(	O
cid:19	O
)	O
?	O
exercise	O
2.25	O
.	O
[	O
3	O
,	O
p.43	O
]	O
prove	O
the	O
assertion	O
that	O
h	O
(	O
x	O
)	O
(	O
cid:20	O
)	O
log	O
(	O
jaxj	O
)	O
with	O
equal-	O
ity	O
i	O
(	O
cid:11	O
)	O
pi	O
=	O
1=jaxj	O
for	O
all	O
i	O
.	O
(	O
jaxj	O
denotes	O
the	O
number	O
of	O
elements	O
in	O
the	O
set	B
ax	O
.	O
)	O
[	O
hint	O
:	O
use	O
jensen	O
’	O
s	O
inequality	B
(	O
2.48	O
)	O
;	O
if	O
your	O
(	O
cid:12	O
)	O
rst	O
attempt	O
to	O
use	O
jensen	O
does	O
not	O
succeed	O
,	O
remember	O
that	O
jensen	O
involves	O
both	O
a	O
random	B
variable	I
and	O
a	O
function	B
,	O
and	O
you	O
have	O
quite	O
a	O
lot	O
of	O
freedom	O
in	O
choosing	O
these	O
;	O
think	O
about	O
whether	O
your	O
chosen	O
function	B
f	O
should	O
be	O
convex	B
or	O
concave	O
.	O
]	O
.	O
exercise	O
2.26	O
.	O
[	O
3	O
,	O
p.44	O
]	O
prove	O
that	O
the	O
relative	B
entropy	I
(	O
equation	O
(	O
2.45	O
)	O
)	O
satis	O
(	O
cid:12	O
)	O
es	O
dkl	O
(	O
pjjq	O
)	O
(	O
cid:21	O
)	O
0	O
(	O
gibbs	O
’	O
inequality	B
)	O
with	O
equality	O
only	O
if	O
p	O
=	O
q.	O
.	O
exercise	O
2.27	O
.	O
[	O
2	O
]	O
prove	O
that	O
the	O
entropy	B
is	O
indeed	O
decomposable	O
as	O
described	O
in	O
equations	O
(	O
2.43	O
{	O
2.44	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
38	O
2	O
|	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
g	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
r	O
1	O
(	O
cid:0	O
)	O
g	O
(	O
cid:0	O
)	O
@	O
@	O
r	O
h	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
@	O
@	O
r	O
1	O
(	O
cid:0	O
)	O
h	O
0	O
1	O
2	O
3	O
f	O
(	O
cid:0	O
)	O
@	O
1	O
(	O
cid:0	O
)	O
f	O
.	O
exercise	O
2.28	O
.	O
[	O
2	O
,	O
p.45	O
]	O
a	O
random	B
variable	I
x	O
2	O
f0	O
;	O
1	O
;	O
2	O
;	O
3g	O
is	O
selected	O
by	O
(	O
cid:13	O
)	O
ipping	O
a	O
bent	B
coin	I
with	O
bias	B
f	O
to	O
determine	O
whether	O
the	O
outcome	O
is	O
in	O
f0	O
;	O
1g	O
or	O
f2	O
;	O
3g	O
;	O
then	O
either	O
(	O
cid:13	O
)	O
ipping	O
a	O
second	O
bent	B
coin	I
with	O
bias	B
g	O
or	O
a	O
third	O
bent	O
coin	B
with	O
bias	B
h	O
respectively	O
.	O
write	O
down	O
the	O
probability	B
distribution	O
of	O
x.	O
use	O
the	O
decomposability	O
of	O
the	O
entropy	O
(	O
2.44	O
)	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
entropy	B
of	O
x	O
.	O
[	O
notice	O
how	O
compact	O
an	O
expression	O
is	O
obtained	O
if	O
you	O
make	O
use	O
of	O
the	O
binary	O
entropy	B
function	O
h2	O
(	O
x	O
)	O
,	O
compared	O
with	O
writing	O
out	O
the	O
four-term	O
entropy	B
explicitly	O
.	O
]	O
find	O
the	O
derivative	O
of	O
h	O
(	O
x	O
)	O
with	O
respect	O
to	O
f	O
.	O
[	O
hint	O
:	O
dh2	O
(	O
x	O
)	O
=dx	O
=	O
log	O
(	O
(	O
1	O
(	O
cid:0	O
)	O
x	O
)	O
=x	O
)	O
.	O
]	O
.	O
exercise	O
2.29	O
.	O
[	O
2	O
,	O
p.45	O
]	O
an	O
unbiased	O
coin	O
is	O
(	O
cid:13	O
)	O
ipped	O
until	O
one	O
head	O
is	O
thrown	O
.	O
what	O
is	O
the	O
entropy	B
of	O
the	O
random	B
variable	I
x	O
2	O
f1	O
;	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
g	O
,	O
the	O
num-	O
ber	O
of	O
(	O
cid:13	O
)	O
ips	O
?	O
repeat	O
the	O
calculation	O
for	O
the	O
case	O
of	O
a	O
biased	O
coin	B
with	O
probability	B
f	O
of	O
coming	O
up	O
heads	O
.	O
[	O
hint	O
:	O
solve	O
the	O
problem	O
both	O
directly	O
and	O
by	O
using	O
the	O
decomposability	O
of	O
the	O
entropy	O
(	O
2.43	O
)	O
.	O
]	O
2.9	O
further	O
exercises	O
forward	B
probability	I
.	O
exercise	O
2.30	O
.	O
[	O
1	O
]	O
an	O
urn	B
contains	O
w	O
white	B
balls	O
and	O
b	O
black	B
balls	O
.	O
two	O
balls	O
are	O
drawn	O
,	O
one	O
after	O
the	O
other	O
,	O
without	O
replacement	O
.	O
prove	O
that	O
the	O
probability	B
that	O
the	O
(	O
cid:12	O
)	O
rst	O
ball	O
is	O
white	B
is	O
equal	O
to	O
the	O
probability	B
that	O
the	O
second	O
is	O
white	B
.	O
.	O
exercise	O
2.31	O
.	O
[	O
2	O
]	O
a	O
circular	O
coin	B
of	O
diameter	O
a	O
is	O
thrown	O
onto	O
a	O
square	B
grid	O
whose	O
squares	O
are	O
b	O
(	O
cid:2	O
)	O
b	O
.	O
(	O
a	O
<	O
b	O
)	O
what	O
is	O
the	O
probability	B
that	O
the	O
coin	B
will	O
lie	O
entirely	O
within	O
one	O
square	B
?	O
[	O
ans	O
:	O
(	O
1	O
(	O
cid:0	O
)	O
a=b	O
)	O
2	O
]	O
.	O
exercise	O
2.32	O
.	O
[	O
3	O
]	O
bu	O
(	O
cid:11	O
)	O
on	O
’	O
s	O
needle	B
.	O
a	O
needle	B
of	O
length	B
a	O
is	O
thrown	O
onto	O
a	O
plane	O
covered	O
with	O
equally	O
spaced	O
parallel	O
lines	O
with	O
separation	O
b.	O
what	O
is	O
the	O
probability	B
that	O
the	O
needle	B
will	O
cross	O
a	O
line	O
?	O
[	O
ans	O
,	O
if	O
a	O
<	O
b	O
:	O
2a/	O
(	O
cid:25	O
)	O
b	O
]	O
[	O
generalization	B
{	O
bu	O
(	O
cid:11	O
)	O
on	O
’	O
s	O
noodle	B
:	O
on	O
average	O
,	O
a	O
random	B
curve	O
of	O
length	O
a	O
is	O
expected	O
to	O
intersect	O
the	O
lines	O
2a/	O
(	O
cid:25	O
)	O
b	O
times	O
.	O
]	O
exercise	O
2.33	O
.	O
[	O
2	O
]	O
two	O
points	O
are	O
selected	O
at	O
random	B
on	O
a	O
straight	O
line	O
segment	O
of	O
length	O
1.	O
what	O
is	O
the	O
probability	B
that	O
a	O
triangle	B
can	O
be	O
constructed	O
out	O
of	O
the	O
three	O
resulting	O
segments	O
?	O
exercise	O
2.34	O
.	O
[	O
2	O
,	O
p.45	O
]	O
an	O
unbiased	O
coin	O
is	O
(	O
cid:13	O
)	O
ipped	O
until	O
one	O
head	O
is	O
thrown	O
.	O
what	O
is	O
the	O
expected	O
number	O
of	O
tails	O
and	O
the	O
expected	O
number	O
of	O
heads	O
?	O
fred	O
,	O
who	O
doesn	O
’	O
t	O
know	O
that	O
the	O
coin	B
is	O
unbiased	O
,	O
estimates	O
the	O
bias	B
using	O
^f	O
(	O
cid:17	O
)	O
h=	O
(	O
h	O
+	O
t	O
)	O
,	O
where	O
h	O
and	O
t	O
are	O
the	O
numbers	O
of	O
heads	O
and	O
tails	O
tossed	O
.	O
compute	O
and	O
sketch	O
the	O
probability	B
distribution	O
of	O
^f	O
.	O
n.b.	O
,	O
this	O
is	O
a	O
forward	B
probability	I
problem	O
,	O
a	O
sampling	B
theory	I
problem	O
,	O
not	O
an	O
inference	B
problem	O
.	O
don	O
’	O
t	O
use	O
bayes	O
’	O
theorem	B
.	O
exercise	O
2.35	O
.	O
[	O
2	O
,	O
p.45	O
]	O
fred	O
rolls	O
an	O
unbiased	O
six-sided	O
die	B
once	O
per	O
second	O
,	O
not-	O
ing	O
the	O
occasions	O
when	O
the	O
outcome	O
is	O
a	O
six	B
.	O
(	O
a	O
)	O
what	O
is	O
the	O
mean	B
number	O
of	O
rolls	O
from	O
one	O
six	B
to	O
the	O
next	O
six	B
?	O
(	O
b	O
)	O
between	O
two	O
rolls	O
,	O
the	O
clock	O
strikes	O
one	O
.	O
what	O
is	O
the	O
mean	B
number	O
of	O
rolls	O
until	O
the	O
next	O
six	B
?	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
2.9	O
:	O
further	O
exercises	O
39	O
(	O
c	O
)	O
now	O
think	O
back	O
before	O
the	O
clock	O
struck	O
.	O
what	O
is	O
the	O
mean	B
number	O
of	O
rolls	O
,	O
going	O
back	O
in	O
time	O
,	O
until	O
the	O
most	O
recent	O
six	B
?	O
(	O
d	O
)	O
what	O
is	O
the	O
mean	B
number	O
of	O
rolls	O
from	O
the	O
six	B
before	O
the	O
clock	O
struck	O
to	O
the	O
next	O
six	B
?	O
(	O
e	O
)	O
is	O
your	O
answer	O
to	O
(	O
d	O
)	O
di	O
(	O
cid:11	O
)	O
erent	O
from	O
your	O
answer	O
to	O
(	O
a	O
)	O
?	O
explain	O
.	O
another	O
version	O
of	O
this	O
exercise	O
refers	O
to	O
fred	O
waiting	B
for	I
a	I
bus	I
at	O
a	O
bus-stop	B
in	O
poissonville	O
where	O
buses	O
arrive	O
independently	O
at	O
random	B
(	O
a	O
poisson	O
process	O
)	O
,	O
with	O
,	O
on	O
average	O
,	O
one	O
bus	O
every	O
six	B
minutes	O
.	O
what	O
is	O
the	O
average	B
wait	O
for	O
a	O
bus	O
,	O
after	O
fred	O
arrives	O
at	O
the	O
stop	O
?	O
[	O
6	O
minutes	O
.	O
]	O
so	O
what	O
is	O
the	O
time	O
between	O
the	O
two	O
buses	O
,	O
the	O
one	O
that	O
fred	O
just	O
missed	O
,	O
and	O
the	O
one	O
that	O
he	O
catches	O
?	O
[	O
12	O
minutes	O
.	O
]	O
explain	O
the	O
apparent	O
para-	O
dox	O
.	O
note	O
the	O
contrast	O
with	O
the	O
situation	O
in	O
clockville	O
,	O
where	O
the	O
buses	O
are	O
spaced	O
exactly	O
6	O
minutes	O
apart	O
.	O
there	O
,	O
as	O
you	O
can	O
con	O
(	O
cid:12	O
)	O
rm	O
,	O
the	O
mean	B
wait	O
at	O
a	O
bus-stop	B
is	O
3	O
minutes	O
,	O
and	O
the	O
time	O
between	O
the	O
missed	O
bus	O
and	O
the	O
next	O
one	O
is	O
6	O
minutes	O
.	O
conditional	B
probability	O
.	O
exercise	O
2.36	O
.	O
[	O
2	O
]	O
you	O
meet	O
fred	O
.	O
fred	O
tells	O
you	O
he	O
has	O
two	O
brothers	O
,	O
alf	O
and	O
bob	O
.	O
what	O
is	O
the	O
probability	B
that	O
fred	O
is	O
older	O
than	O
bob	O
?	O
fred	O
tells	O
you	O
that	O
he	O
is	O
older	O
than	O
alf	O
.	O
now	O
,	O
what	O
is	O
the	O
probability	B
that	O
fred	O
is	O
older	O
than	O
bob	O
?	O
(	O
that	O
is	O
,	O
what	O
is	O
the	O
conditional	B
probability	O
that	O
f	O
>	O
b	O
given	O
that	O
f	O
>	O
a	O
?	O
)	O
.	O
exercise	O
2.37	O
.	O
[	O
2	O
]	O
the	O
inhabitants	O
of	O
an	O
island	O
tell	O
the	O
truth	O
one	O
third	O
of	O
the	O
time	O
.	O
they	O
lie	O
with	O
probability	O
2/3	O
.	O
on	O
an	O
occasion	O
,	O
after	O
one	O
of	O
them	O
made	O
a	O
statement	O
,	O
you	O
ask	O
another	O
‘	O
was	O
that	O
statement	O
true	O
?	O
’	O
and	O
he	O
says	O
‘	O
yes	O
’	O
.	O
what	O
is	O
the	O
probability	B
that	O
the	O
statement	O
was	O
indeed	O
true	O
?	O
.	O
exercise	O
2.38	O
.	O
[	O
2	O
,	O
p.46	O
]	O
compare	O
two	O
ways	O
of	O
computing	O
the	O
probability	B
of	I
error	I
of	O
the	O
repetition	B
code	I
r3	O
,	O
assuming	O
a	O
binary	B
symmetric	I
channel	I
(	O
you	O
did	O
this	O
once	O
for	O
exercise	O
1.2	O
(	O
p.7	O
)	O
)	O
and	O
con	O
(	O
cid:12	O
)	O
rm	O
that	O
they	O
give	O
the	O
same	O
answer	O
.	O
binomial	B
distribution	I
method	O
.	O
add	O
the	O
probability	B
that	O
all	O
three	O
bits	O
are	O
(	O
cid:13	O
)	O
ipped	O
to	O
the	O
probability	B
that	O
exactly	O
two	O
bits	O
are	O
(	O
cid:13	O
)	O
ipped	O
.	O
sum	B
rule	I
method	O
.	O
using	O
the	O
sum	B
rule	I
,	O
compute	O
the	O
marginal	B
prob-	O
ability	O
that	O
r	O
takes	O
on	O
each	O
of	O
the	O
eight	O
possible	O
values	O
,	O
p	O
(	O
r	O
)	O
.	O
[	O
p	O
(	O
r	O
)	O
=	O
ps	O
p	O
(	O
s	O
)	O
p	O
(	O
rj	O
s	O
)	O
.	O
]	O
then	O
compute	O
the	O
posterior	O
probabil-	O
ity	O
of	O
s	O
for	O
each	O
of	O
the	O
eight	O
values	O
of	O
r.	O
[	O
in	O
fact	O
,	O
by	O
symmetry	O
,	O
only	O
two	O
example	O
cases	O
r	O
=	O
(	O
000	O
)	O
and	O
r	O
=	O
(	O
001	O
)	O
need	O
be	O
consid-	O
ered	O
.	O
]	O
notice	O
that	O
some	O
of	O
the	O
inferred	O
bits	O
are	O
better	O
determined	O
than	O
others	O
.	O
from	O
the	O
posterior	B
probability	I
p	O
(	O
sj	O
r	O
)	O
you	O
can	O
read	O
out	O
the	O
case-by-case	O
error	B
probability	I
,	O
the	O
probability	B
that	O
the	O
more	O
probable	O
hypothesis	O
is	O
not	O
correct	O
,	O
p	O
(	O
errorj	O
r	O
)	O
.	O
find	O
the	O
average	B
error	O
probability	B
using	O
the	O
sum	B
rule	I
,	O
p	O
(	O
error	O
)	O
=xr	O
p	O
(	O
r	O
)	O
p	O
(	O
error	O
j	O
r	O
)	O
:	O
(	O
2.55	O
)	O
equation	O
(	O
1.18	O
)	O
gives	O
the	O
posterior	B
probability	I
of	O
the	O
input	O
s	O
,	O
given	O
the	O
received	O
vector	O
r.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
40	O
2	O
|	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
.	O
exercise	O
2.39	O
.	O
[	O
3c	O
,	O
p.46	O
]	O
the	O
frequency	B
pn	O
of	O
the	O
nth	O
most	O
frequent	O
word	O
in	O
english	O
is	O
roughly	O
approximated	O
by	O
pn	O
’	O
(	O
cid:26	O
)	O
0:1	O
n	O
0	O
for	O
n	O
2	O
1	O
;	O
:	O
:	O
:	O
;	O
12	O
367	O
n	O
>	O
12	O
367	O
:	O
(	O
2.56	O
)	O
[	O
this	O
remarkable	O
1=n	O
law	O
is	O
known	O
as	O
zipf	O
’	O
s	O
law	O
,	O
and	O
applies	O
to	O
the	O
word	O
frequencies	O
of	O
many	O
languages	O
(	O
zipf	O
,	O
1949	O
)	O
.	O
]	O
if	O
we	O
assume	O
that	O
english	O
is	O
generated	O
by	O
picking	O
words	O
at	O
random	B
according	O
to	O
this	O
distribution	B
,	O
what	O
is	O
the	O
entropy	B
of	O
english	O
(	O
per	O
word	O
)	O
?	O
[	O
this	O
calculation	O
can	O
be	O
found	O
in	O
‘	O
prediction	B
and	O
entropy	B
of	O
printed	O
english	O
’	O
,	O
c.e	O
.	O
shannon	O
,	O
bell	O
syst	O
.	O
tech	O
.	O
j	O
.	O
30	O
,	O
pp.50	O
{	O
64	O
(	O
1950	O
)	O
,	O
but	O
,	O
inexplicably	O
,	O
the	O
great	O
man	O
made	O
numerical	O
errors	B
in	O
it	O
.	O
]	O
2.10	O
solutions	O
solution	O
to	O
exercise	O
2.2	O
(	O
p.24	O
)	O
.	O
no	O
,	O
they	O
are	O
not	O
independent	O
.	O
if	O
they	O
were	O
then	O
all	O
the	O
conditional	B
distributions	O
p	O
(	O
y	O
j	O
x	O
)	O
would	O
be	O
identical	O
functions	O
of	O
y	O
,	O
regardless	O
of	O
x	O
(	O
cf	O
.	O
(	O
cid:12	O
)	O
gure	O
2.3	O
)	O
.	O
solution	O
to	O
exercise	O
2.4	O
(	O
p.27	O
)	O
.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
fraction	O
fb	O
(	O
cid:17	O
)	O
b=k	O
.	O
(	O
a	O
)	O
the	O
number	O
of	O
black	O
balls	O
has	O
a	O
binomial	B
distribution	I
.	O
p	O
(	O
nb	O
j	O
fb	O
;	O
n	O
)	O
=	O
(	O
cid:18	O
)	O
n	O
nb	O
(	O
cid:19	O
)	O
f	O
nb	O
b	O
(	O
1	O
(	O
cid:0	O
)	O
fb	O
)	O
n	O
(	O
cid:0	O
)	O
nb	O
:	O
(	O
b	O
)	O
the	O
mean	B
and	O
variance	B
of	O
this	O
distribution	B
are	O
:	O
e	O
[	O
nb	O
]	O
=	O
n	O
fb	O
var	O
[	O
nb	O
]	O
=	O
n	O
fb	O
(	O
1	O
(	O
cid:0	O
)	O
fb	O
)	O
:	O
(	O
2.57	O
)	O
(	O
2.58	O
)	O
(	O
2.59	O
)	O
these	O
results	O
were	O
derived	O
in	O
example	O
1.1	O
(	O
p.1	O
)	O
.	O
the	O
standard	B
deviation	I
of	O
nb	O
is	O
pvar	O
[	O
nb	O
]	O
=pn	O
fb	O
(	O
1	O
(	O
cid:0	O
)	O
fb	O
)	O
.	O
when	O
b=k	O
=	O
1=5	O
and	O
n	O
=	O
5	O
,	O
the	O
expectation	B
and	O
variance	B
of	O
nb	O
are	O
1	O
and	O
4/5	O
.	O
the	O
standard	B
deviation	I
is	O
0.89.	O
when	O
b=k	O
=	O
1=5	O
and	O
n	O
=	O
400	O
,	O
the	O
expectation	B
and	O
variance	B
of	O
nb	O
are	O
80	O
and	O
64.	O
the	O
standard	B
deviation	I
is	O
8.	O
solution	O
to	O
exercise	O
2.5	O
(	O
p.27	O
)	O
.	O
the	O
numerator	O
of	O
the	O
quantity	O
z	O
=	O
(	O
nb	O
(	O
cid:0	O
)	O
fbn	O
)	O
2	O
n	O
fb	O
(	O
1	O
(	O
cid:0	O
)	O
fb	O
)	O
can	O
be	O
recognized	O
as	O
(	O
nb	O
(	O
cid:0	O
)	O
e	O
[	O
nb	O
]	O
)	O
2	O
;	O
the	O
denominator	O
is	O
equal	O
to	O
the	O
variance	B
of	O
nb	O
(	O
2.59	O
)	O
,	O
which	O
is	O
by	O
de	O
(	O
cid:12	O
)	O
nition	O
the	O
expectation	B
of	O
the	O
numerator	O
.	O
so	O
the	O
expectation	B
of	O
z	O
is	O
1	O
.	O
[	O
a	O
random	B
variable	I
like	O
z	O
,	O
which	O
measures	O
the	O
deviation	O
of	O
data	O
from	O
the	O
expected	O
value	O
,	O
is	O
sometimes	O
called	O
(	O
cid:31	O
)	O
2	O
(	O
chi-squared	B
)	O
.	O
]	O
in	O
the	O
case	O
n	O
=	O
5	O
and	O
fb	O
=	O
1=5	O
,	O
n	O
fb	O
is	O
1	O
,	O
and	O
var	O
[	O
nb	O
]	O
is	O
4/5	O
.	O
the	O
numerator	O
has	O
(	O
cid:12	O
)	O
ve	O
possible	O
values	O
,	O
only	O
one	O
of	O
which	O
is	O
smaller	O
than	O
1	O
:	O
(	O
nb	O
(	O
cid:0	O
)	O
fbn	O
)	O
2	O
=	O
0	O
has	O
probability	B
p	O
(	O
nb	O
=	O
1	O
)	O
=	O
0:4096	O
;	O
so	O
the	O
probability	B
that	O
z	O
<	O
1	O
is	O
0.4096	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
2.10	O
:	O
solutions	O
41	O
solution	O
to	O
exercise	O
2.14	O
(	O
p.35	O
)	O
.	O
we	O
wish	O
to	O
prove	O
,	O
given	O
the	O
property	O
f	O
(	O
(	O
cid:21	O
)	O
x1	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
)	O
x2	O
)	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
f	O
(	O
x1	O
)	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
)	O
f	O
(	O
x2	O
)	O
;	O
(	O
2.60	O
)	O
that	O
,	O
if	O
p	O
pi	O
=	O
1	O
and	O
pi	O
(	O
cid:21	O
)	O
0	O
,	O
xi=1	O
i	O
pif	O
(	O
xi	O
)	O
(	O
cid:21	O
)	O
f	O
i	O
xi=1	O
pixi	O
!	O
:	O
(	O
2.61	O
)	O
we	O
proceed	O
by	O
recursion	O
,	O
working	O
from	O
the	O
right-hand	O
side	O
.	O
(	O
this	O
proof	O
does	O
not	O
handle	O
cases	O
where	O
some	O
pi	O
=	O
0	O
;	O
such	O
details	O
are	O
left	O
to	O
the	O
pedantic	O
reader	O
.	O
)	O
at	O
the	O
(	O
cid:12	O
)	O
rst	O
line	O
we	O
use	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
convexity	O
(	O
2.60	O
)	O
with	O
(	O
cid:21	O
)	O
=	O
p1	O
i	O
i=1	O
pi	O
=	O
p1	O
;	O
at	O
the	O
second	O
line	O
,	O
(	O
cid:21	O
)	O
=	O
p2	O
.	O
i	O
i=2	O
pi	O
i	O
pixi	O
!	O
=	O
f	O
p1x1	O
+	O
f	O
i	O
xi=1	O
(	O
cid:20	O
)	O
p1f	O
(	O
x1	O
)	O
+	O
''	O
i	O
xi=2	O
(	O
cid:20	O
)	O
p1f	O
(	O
x1	O
)	O
+	O
''	O
i	O
xi=2	O
pixi	O
!	O
xi=2	O
pi	O
!	O
#	O
pixi	O
,	O
i	O
pi	O
#	O
''	O
f	O
i	O
xi=2	O
xi=2	O
pi	O
#	O
''	O
f	O
(	O
x2	O
)	O
+pi	O
pi	O
p2	O
i=2	O
pi	O
pi	O
i=3	O
pi	O
i=2	O
pi	O
and	O
so	O
forth	O
.	O
solution	O
to	O
exercise	O
2.16	O
(	O
p.36	O
)	O
.	O
(	O
2.62	O
)	O
f	O
i	O
xi=3	O
pixi	O
,	O
i	O
xi=3	O
pi	O
!	O
#	O
;	O
2	O
(	O
a	O
)	O
for	O
the	O
outcomes	O
f2	O
;	O
3	O
;	O
4	O
;	O
5	O
;	O
6	O
;	O
7	O
;	O
8	O
;	O
9	O
;	O
10	O
;	O
11	O
;	O
12g	O
,	O
the	O
probabilities	O
are	O
p	O
=	O
36	O
;	O
2	O
f	O
1	O
36	O
;	O
3	O
36	O
;	O
4	O
36	O
;	O
5	O
36	O
;	O
6	O
36	O
;	O
5	O
36	O
;	O
4	O
36	O
;	O
3	O
36	O
;	O
2	O
36	O
;	O
1	O
36g	O
.	O
(	O
b	O
)	O
the	O
value	O
of	O
one	O
die	B
has	O
mean	B
3:5	O
and	O
variance	O
35=12	O
.	O
so	O
the	O
sum	O
of	O
one	O
hundred	O
has	O
mean	B
350	O
and	O
variance	O
3500=12	O
’	O
292	O
,	O
and	O
by	O
the	O
central-limit	B
theorem	I
the	O
probability	B
distribution	O
is	O
roughly	O
gaussian	O
(	O
but	O
con	O
(	O
cid:12	O
)	O
ned	O
to	O
the	O
integers	O
)	O
,	O
with	O
this	O
mean	B
and	O
variance	B
.	O
(	O
c	O
)	O
in	O
order	O
to	O
obtain	O
a	O
sum	O
that	O
has	O
a	O
uniform	O
distribution	B
we	O
have	O
to	O
start	O
from	O
random	O
variables	O
some	O
of	O
which	O
have	O
a	O
spiky	O
distribution	B
with	O
the	O
probability	B
mass	O
concentrated	O
at	O
the	O
extremes	O
.	O
the	O
unique	O
solution	O
is	O
to	O
have	O
one	O
ordinary	O
die	B
and	O
one	O
with	O
faces	O
6	O
,	O
6	O
,	O
6	O
,	O
0	O
,	O
0	O
,	O
0	O
.	O
(	O
d	O
)	O
yes	O
,	O
a	O
uniform	O
distribution	B
can	O
be	O
created	O
in	O
several	O
ways	O
,	O
for	O
example	O
by	O
labelling	O
the	O
rth	O
die	B
with	O
the	O
numbers	O
f0	O
;	O
1	O
;	O
2	O
;	O
3	O
;	O
4	O
;	O
5g	O
(	O
cid:2	O
)	O
6r	O
.	O
to	O
think	O
about	O
:	O
does	O
this	O
uniform	O
distribution	B
contradict	O
the	O
central-limit	B
theorem	I
?	O
solution	O
to	O
exercise	O
2.17	O
(	O
p.36	O
)	O
.	O
a	O
=	O
ln	O
p	O
q	O
)	O
p	O
q	O
=	O
ea	O
and	O
q	O
=	O
1	O
(	O
cid:0	O
)	O
p	O
gives	O
p	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
p	O
=	O
the	O
hyperbolic	O
tangent	O
is	O
=	O
ea	O
ea	O
ea	O
+	O
1	O
=	O
1	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
a	O
)	O
:	O
tanh	O
(	O
a	O
)	O
=	O
ea	O
(	O
cid:0	O
)	O
e	O
(	O
cid:0	O
)	O
a	O
ea	O
+	O
e	O
(	O
cid:0	O
)	O
a	O
(	O
2.63	O
)	O
(	O
2.64	O
)	O
(	O
2.65	O
)	O
(	O
2.66	O
)	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
42	O
so	O
f	O
(	O
a	O
)	O
(	O
cid:17	O
)	O
=	O
1	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
a	O
)	O
2	O
ea=2	O
(	O
cid:0	O
)	O
e	O
(	O
cid:0	O
)	O
a=2	O
1	O
ea=2	O
+	O
e	O
(	O
cid:0	O
)	O
a=2	O
1	O
=	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
a	O
+	O
1	O
(	O
cid:19	O
)	O
2	O
(	O
cid:18	O
)	O
1	O
(	O
cid:0	O
)	O
e	O
(	O
cid:0	O
)	O
a	O
+	O
1	O
!	O
=	O
1	O
2	O
(	O
tanh	O
(	O
a=2	O
)	O
+	O
1	O
)	O
:	O
2	O
|	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
(	O
2.67	O
)	O
in	O
the	O
case	O
b	O
=	O
log2	O
p=q	O
,	O
we	O
can	O
repeat	O
steps	O
(	O
2.63	O
{	O
2.65	O
)	O
,	O
replacing	O
e	O
by	O
2	O
,	O
to	O
obtain	O
p	O
=	O
1	O
1	O
+	O
2	O
(	O
cid:0	O
)	O
b	O
:	O
solution	O
to	O
exercise	O
2.18	O
(	O
p.36	O
)	O
.	O
p	O
(	O
xj	O
y	O
)	O
=	O
p	O
(	O
y	O
j	O
x	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
)	O
)	O
log	O
p	O
(	O
x	O
=	O
1j	O
y	O
)	O
p	O
(	O
x	O
=	O
0j	O
y	O
)	O
p	O
(	O
x	O
=	O
1j	O
y	O
)	O
p	O
(	O
x	O
=	O
0j	O
y	O
)	O
=	O
p	O
(	O
y	O
j	O
x	O
=	O
1	O
)	O
p	O
(	O
y	O
j	O
x	O
=	O
0	O
)	O
p	O
(	O
x	O
=	O
1	O
)	O
p	O
(	O
x	O
=	O
0	O
)	O
=	O
log	O
p	O
(	O
y	O
j	O
x	O
=	O
1	O
)	O
p	O
(	O
y	O
j	O
x	O
=	O
0	O
)	O
+	O
log	O
p	O
(	O
x	O
=	O
1	O
)	O
p	O
(	O
x	O
=	O
0	O
)	O
:	O
(	O
2.68	O
)	O
(	O
2.69	O
)	O
(	O
2.70	O
)	O
(	O
2.71	O
)	O
solution	O
to	O
exercise	O
2.19	O
(	O
p.36	O
)	O
.	O
the	O
conditional	B
independence	O
of	O
d1	O
and	O
d2	O
given	O
x	O
means	O
p	O
(	O
x	O
;	O
d1	O
;	O
d2	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
d1	O
j	O
x	O
)	O
p	O
(	O
d2	O
j	O
x	O
)	O
:	O
(	O
2.72	O
)	O
this	O
gives	O
a	O
separation	B
of	O
the	O
posterior	B
probability	I
ratio	O
into	O
a	O
series	O
of	O
factors	O
,	O
one	O
for	O
each	O
data	O
point	O
,	O
times	O
the	O
prior	B
probability	O
ratio	O
.	O
p	O
(	O
x	O
=	O
1jfdig	O
)	O
p	O
(	O
x	O
=	O
0jfdig	O
)	O
=	O
=	O
p	O
(	O
fdigj	O
x	O
=	O
1	O
)	O
p	O
(	O
fdigj	O
x	O
=	O
0	O
)	O
p	O
(	O
d1	O
j	O
x	O
=	O
1	O
)	O
p	O
(	O
d1	O
j	O
x	O
=	O
0	O
)	O
p	O
(	O
x	O
=	O
1	O
)	O
p	O
(	O
x	O
=	O
0	O
)	O
p	O
(	O
d2	O
j	O
x	O
=	O
1	O
)	O
p	O
(	O
d2	O
j	O
x	O
=	O
0	O
)	O
p	O
(	O
x	O
=	O
1	O
)	O
p	O
(	O
x	O
=	O
0	O
)	O
:	O
(	O
2.73	O
)	O
(	O
2.74	O
)	O
life	O
in	O
high-dimensional	O
spaces	O
solution	O
to	O
exercise	O
2.20	O
(	O
p.37	O
)	O
.	O
the	O
volume	B
of	O
a	O
hypersphere	B
of	O
radius	O
r	O
in	O
n	O
dimensions	B
is	O
in	O
fact	O
v	O
(	O
r	O
;	O
n	O
)	O
=	O
(	O
cid:25	O
)	O
n=2	O
(	O
n=2	O
)	O
!	O
rn	O
;	O
(	O
2.75	O
)	O
but	O
you	O
don	O
’	O
t	O
need	O
to	O
know	O
this	O
.	O
for	O
this	O
question	O
all	O
that	O
we	O
need	O
is	O
the	O
r-dependence	O
,	O
v	O
(	O
r	O
;	O
n	O
)	O
/	O
rn	O
:	O
so	O
the	O
fractional	O
volume	B
in	O
(	O
r	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
;	O
r	O
)	O
is	O
rn	O
(	O
cid:0	O
)	O
(	O
r	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
)	O
n	O
rn	O
=	O
1	O
(	O
cid:0	O
)	O
(	O
cid:16	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
r	O
(	O
cid:17	O
)	O
n	O
:	O
(	O
2.76	O
)	O
the	O
fractional	O
volumes	O
in	O
the	O
shells	O
for	O
the	O
required	O
cases	O
are	O
:	O
n	O
2	O
10	O
1000	O
(	O
cid:15	O
)	O
=r	O
=	O
0:01	O
(	O
cid:15	O
)	O
=r	O
=	O
0:5	O
0.02	O
0.75	O
0.096	O
0.999	O
0.99996	O
1	O
(	O
cid:0	O
)	O
2	O
(	O
cid:0	O
)	O
1000	O
notice	O
that	O
no	O
matter	O
how	O
small	O
(	O
cid:15	O
)	O
is	O
,	O
for	O
large	O
enough	O
n	O
essentially	O
all	O
the	O
probability	B
mass	O
is	O
in	O
the	O
surface	O
shell	O
of	O
thickness	O
(	O
cid:15	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
2.10	O
:	O
solutions	O
43	O
solution	O
to	O
exercise	O
2.21	O
(	O
p.37	O
)	O
.	O
f	O
(	O
b	O
)	O
=	O
5	O
,	O
and	O
f	O
(	O
c	O
)	O
=	O
10=7	O
.	O
pa	O
=	O
0:1	O
,	O
pb	O
=	O
0:2	O
,	O
pc	O
=	O
0:7.	O
f	O
(	O
a	O
)	O
=	O
10	O
,	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
=	O
0:1	O
(	O
cid:2	O
)	O
10	O
+	O
0:2	O
(	O
cid:2	O
)	O
5	O
+	O
0:7	O
(	O
cid:2	O
)	O
10=7	O
=	O
3	O
:	O
for	O
each	O
x	O
,	O
f	O
(	O
x	O
)	O
=	O
1=p	O
(	O
x	O
)	O
,	O
so	O
solution	O
to	O
exercise	O
2.22	O
(	O
p.37	O
)	O
.	O
for	O
general	O
x	O
,	O
e	O
[	O
1=p	O
(	O
x	O
)	O
]	O
=	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
=	O
3	O
:	O
p	O
(	O
x	O
)	O
1=p	O
(	O
x	O
)	O
=	O
xx2ax	O
e	O
[	O
1=p	O
(	O
x	O
)	O
]	O
=	O
xx2ax	O
1	O
=	O
jaxj	O
:	O
(	O
2.77	O
)	O
(	O
2.78	O
)	O
(	O
2.79	O
)	O
solution	O
to	O
exercise	O
2.23	O
(	O
p.37	O
)	O
.	O
pa	O
=	O
0:1	O
,	O
pb	O
=	O
0:2	O
,	O
pc	O
=	O
0:7.	O
g	O
(	O
a	O
)	O
=	O
0	O
,	O
g	O
(	O
b	O
)	O
=	O
1	O
,	O
and	O
g	O
(	O
c	O
)	O
=	O
0.	O
solution	O
to	O
exercise	O
2.24	O
(	O
p.37	O
)	O
.	O
e	O
[	O
g	O
(	O
x	O
)	O
]	O
=	O
pb	O
=	O
0:2	O
:	O
p	O
(	O
p	O
(	O
x	O
)	O
2	O
[	O
0:15	O
;	O
0:5	O
]	O
)	O
=	O
pb	O
=	O
0:2	O
:	O
log	O
>	O
0:05	O
(	O
cid:19	O
)	O
=	O
pa	O
+	O
pc	O
=	O
0:8	O
:	O
p	O
(	O
x	O
)	O
p	O
(	O
cid:18	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
0:2	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
solution	O
to	O
exercise	O
2.25	O
(	O
p.37	O
)	O
.	O
this	O
type	O
of	O
question	O
can	O
be	O
approached	O
in	O
two	O
ways	O
:	O
either	O
by	O
di	O
(	O
cid:11	O
)	O
erentiating	O
the	O
function	B
to	O
be	O
maximized	O
,	O
(	O
cid:12	O
)	O
nding	O
the	O
maximum	O
,	O
and	O
proving	O
it	O
is	O
a	O
global	O
maximum	O
;	O
this	O
strategy	O
is	O
somewhat	O
risky	O
since	O
it	O
is	O
possible	O
for	O
the	O
maximum	O
of	O
a	O
function	B
to	O
be	O
at	O
the	O
boundary	O
of	O
the	O
space	O
,	O
at	O
a	O
place	O
where	O
the	O
derivative	O
is	O
not	O
zero	O
.	O
alternatively	O
,	O
a	O
carefully	O
chosen	O
inequality	B
can	O
establish	O
the	O
answer	O
.	O
the	O
second	O
method	B
is	O
much	O
neater	O
.	O
proof	O
by	O
di	O
(	O
cid:11	O
)	O
erentiation	O
(	O
not	O
the	O
recommended	O
method	B
)	O
.	O
since	O
it	O
is	O
slightly	O
easier	O
to	O
di	O
(	O
cid:11	O
)	O
erentiate	O
ln	O
1=p	O
than	O
log2	O
1=p	O
,	O
we	O
temporarily	O
de	O
(	O
cid:12	O
)	O
ne	O
h	O
(	O
x	O
)	O
to	O
be	O
measured	O
using	O
natural	B
logarithms	O
,	O
thus	O
scaling	B
it	O
down	O
by	O
a	O
factor	O
of	O
log	O
2	O
e.	O
h	O
(	O
x	O
)	O
=	O
xi	O
pi	O
ln	O
1	O
pi	O
@	O
h	O
(	O
x	O
)	O
@	O
pi	O
=	O
ln	O
1	O
pi	O
(	O
cid:0	O
)	O
1	O
(	O
2.83	O
)	O
(	O
2.84	O
)	O
we	O
maximize	O
subject	O
to	O
the	O
constraint	O
pi	O
pi	O
=	O
1	O
which	O
can	O
be	O
enforced	O
with	O
a	O
lagrange	O
multiplier	O
:	O
(	O
2.80	O
)	O
(	O
2.81	O
)	O
(	O
2.82	O
)	O
(	O
2.85	O
)	O
(	O
2.86	O
)	O
(	O
2.87	O
)	O
(	O
2.88	O
)	O
at	O
a	O
maximum	O
,	O
@	O
pi	O
g	O
(	O
p	O
)	O
(	O
cid:17	O
)	O
h	O
(	O
x	O
)	O
+	O
(	O
cid:21	O
)	O
xi	O
pi	O
(	O
cid:0	O
)	O
1	O
!	O
@	O
g	O
(	O
p	O
)	O
=	O
ln	O
1	O
pi	O
(	O
cid:0	O
)	O
1	O
+	O
(	O
cid:21	O
)	O
:	O
ln	O
1	O
pi	O
(	O
cid:0	O
)	O
1	O
+	O
(	O
cid:21	O
)	O
=	O
0	O
)	O
ln	O
1	O
pi	O
=	O
1	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
;	O
so	O
all	O
the	O
pi	O
are	O
equal	O
.	O
that	O
this	O
extremum	O
is	O
indeed	O
a	O
maximum	O
is	O
established	O
by	O
(	O
cid:12	O
)	O
nding	O
the	O
curvature	O
:	O
which	O
is	O
negative	O
de	O
(	O
cid:12	O
)	O
nite	O
.	O
@	O
2g	O
(	O
p	O
)	O
@	O
pi	O
@	O
pj	O
=	O
(	O
cid:0	O
)	O
1	O
pi	O
(	O
cid:14	O
)	O
ij	O
;	O
(	O
2.89	O
)	O
2	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
44	O
2	O
|	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
proof	O
using	O
jensen	O
’	O
s	O
inequality	B
(	O
recommended	O
method	B
)	O
.	O
the	O
inequality	B
.	O
first	O
a	O
reminder	O
of	O
if	O
f	O
is	O
a	O
convex	B
^	O
function	B
and	O
x	O
is	O
a	O
random	B
variable	I
then	O
:	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
(	O
cid:21	O
)	O
f	O
(	O
e	O
[	O
x	O
]	O
)	O
:	O
if	O
f	O
is	O
strictly	O
convex	B
^	O
and	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
=	O
f	O
(	O
e	O
[	O
x	O
]	O
)	O
,	O
then	O
the	O
random	B
variable	I
x	O
is	O
a	O
constant	O
(	O
with	O
probability	O
1	O
)	O
.	O
the	O
secret	B
of	O
a	O
proof	O
using	O
jensen	O
’	O
s	O
inequality	B
is	O
to	O
choose	O
the	O
right	O
func-	O
tion	O
and	O
the	O
right	O
random	B
variable	I
.	O
we	O
could	O
de	O
(	O
cid:12	O
)	O
ne	O
f	O
(	O
u	O
)	O
=	O
log	O
1	O
u	O
=	O
(	O
cid:0	O
)	O
log	O
u	O
(	O
2.90	O
)	O
(	O
which	O
is	O
a	O
convex	B
function	O
)	O
and	O
think	O
of	O
h	O
(	O
x	O
)	O
=p	O
pi	O
log	O
1	O
as	O
the	O
mean	B
of	O
f	O
(	O
u	O
)	O
where	O
u	O
=	O
p	O
(	O
x	O
)	O
,	O
but	O
this	O
would	O
not	O
get	O
us	O
there	O
{	O
it	O
would	O
give	O
us	O
an	O
inequality	B
in	O
the	O
wrong	O
direction	O
.	O
if	O
instead	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
pi	O
then	O
we	O
(	O
cid:12	O
)	O
nd	O
:	O
u	O
=	O
1=p	O
(	O
x	O
)	O
h	O
(	O
x	O
)	O
=	O
(	O
cid:0	O
)	O
e	O
[	O
f	O
(	O
1=p	O
(	O
x	O
)	O
)	O
]	O
(	O
cid:20	O
)	O
(	O
cid:0	O
)	O
f	O
(	O
e	O
[	O
1=p	O
(	O
x	O
)	O
]	O
)	O
;	O
now	O
we	O
know	O
from	O
exercise	O
2.22	O
(	O
p.37	O
)	O
that	O
e	O
[	O
1=p	O
(	O
x	O
)	O
]	O
=	O
jaxj	O
,	O
so	O
h	O
(	O
x	O
)	O
(	O
cid:20	O
)	O
(	O
cid:0	O
)	O
f	O
(	O
jaxj	O
)	O
=	O
log	O
jaxj	O
:	O
(	O
2.91	O
)	O
(	O
2.92	O
)	O
(	O
2.93	O
)	O
equality	O
holds	O
only	O
if	O
the	O
random	B
variable	I
u	O
=	O
1=p	O
(	O
x	O
)	O
is	O
a	O
constant	O
,	O
which	O
means	O
p	O
(	O
x	O
)	O
is	O
a	O
constant	O
for	O
all	O
x	O
.	O
2	O
solution	O
to	O
exercise	O
2.26	O
(	O
p.37	O
)	O
.	O
dkl	O
(	O
pjjq	O
)	O
=xx	O
p	O
(	O
x	O
)	O
log	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
:	O
(	O
2.94	O
)	O
we	O
prove	O
gibbs	O
’	O
inequality	B
using	O
jensen	O
’	O
s	O
inequality	B
.	O
let	O
f	O
(	O
u	O
)	O
=	O
log	O
1=u	O
and	O
u	O
=	O
q	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
.	O
then	O
dkl	O
(	O
pjjq	O
)	O
=	O
e	O
[	O
f	O
(	O
q	O
(	O
x	O
)	O
=p	O
(	O
x	O
)	O
)	O
]	O
q	O
(	O
x	O
)	O
(	O
cid:21	O
)	O
f	O
xx	O
p	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
!	O
=	O
log	O
(	O
cid:18	O
)	O
with	O
equality	O
only	O
if	O
u	O
=	O
q	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
is	O
a	O
constant	O
,	O
that	O
is	O
,	O
if	O
q	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
)	O
.	O
1	O
px	O
q	O
(	O
x	O
)	O
(	O
cid:19	O
)	O
=	O
0	O
;	O
(	O
2.95	O
)	O
(	O
2.96	O
)	O
2	O
in	O
the	O
above	O
proof	O
the	O
expectations	O
were	O
with	O
respect	O
to	O
second	O
solution	O
.	O
the	O
probability	B
distribution	O
p	O
(	O
x	O
)	O
.	O
a	O
second	O
solution	O
method	B
uses	O
jensen	O
’	O
s	O
inequality	B
with	O
q	O
(	O
x	O
)	O
instead	O
.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
f	O
(	O
u	O
)	O
=	O
u	O
log	O
u	O
and	O
let	O
u	O
=	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
.	O
then	O
q	O
(	O
x	O
)	O
dkl	O
(	O
pjjq	O
)	O
=	O
xx	O
(	O
cid:21	O
)	O
f	O
xx	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
log	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
=xx	O
q	O
(	O
x	O
)	O
!	O
=	O
f	O
(	O
1	O
)	O
=	O
0	O
;	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
f	O
(	O
cid:18	O
)	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
(	O
cid:19	O
)	O
(	O
2.97	O
)	O
(	O
2.98	O
)	O
with	O
equality	O
only	O
if	O
u	O
=	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
is	O
a	O
constant	O
,	O
that	O
is	O
,	O
if	O
q	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
)	O
.	O
2	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
45	O
2.10	O
:	O
solutions	O
solution	O
to	O
exercise	O
2.28	O
(	O
p.38	O
)	O
.	O
h	O
(	O
x	O
)	O
=	O
h2	O
(	O
f	O
)	O
+	O
f	O
h2	O
(	O
g	O
)	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
h2	O
(	O
h	O
)	O
:	O
(	O
2.99	O
)	O
solution	O
to	O
exercise	O
2.29	O
(	O
p.38	O
)	O
.	O
the	O
probability	B
that	O
there	O
are	O
x	O
(	O
cid:0	O
)	O
1	O
tails	O
and	O
then	O
one	O
head	O
(	O
so	O
we	O
get	O
the	O
(	O
cid:12	O
)	O
rst	O
head	O
on	O
the	O
xth	O
toss	O
)	O
is	O
p	O
(	O
x	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
x	O
(	O
cid:0	O
)	O
1f	O
:	O
(	O
2.100	O
)	O
if	O
the	O
(	O
cid:12	O
)	O
rst	O
toss	O
is	O
a	O
tail	B
,	O
the	O
probability	B
distribution	O
for	O
the	O
future	O
looks	O
just	O
like	O
it	O
did	O
before	O
we	O
made	O
the	O
(	O
cid:12	O
)	O
rst	O
toss	O
.	O
thus	O
we	O
have	O
a	O
recursive	O
expression	O
for	O
the	O
entropy	B
:	O
rearranging	O
,	O
h	O
(	O
x	O
)	O
=	O
h2	O
(	O
f	O
)	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
h	O
(	O
x	O
)	O
:	O
h	O
(	O
x	O
)	O
=	O
h2	O
(	O
f	O
)	O
=f	O
:	O
(	O
2.101	O
)	O
(	O
2.102	O
)	O
solution	O
to	O
exercise	O
2.34	O
(	O
p.38	O
)	O
.	O
the	O
probability	O
of	O
the	O
number	O
of	O
tails	O
t	O
is	O
p	O
(	O
t	O
)	O
=	O
(	O
cid:18	O
)	O
1	O
2	O
(	O
cid:19	O
)	O
t	O
1	O
2	O
for	O
t	O
(	O
cid:21	O
)	O
0	O
:	O
(	O
2.103	O
)	O
the	O
expected	O
number	O
of	O
heads	O
is	O
1	O
,	O
by	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
the	O
problem	O
.	O
the	O
expected	O
number	O
of	O
tails	O
is	O
which	O
may	O
be	O
shown	O
to	O
be	O
1	O
in	O
a	O
variety	O
of	O
ways	O
.	O
for	O
example	O
,	O
since	O
the	O
situation	O
after	O
one	O
tail	B
is	O
thrown	O
is	O
equivalent	O
to	O
the	O
opening	O
situation	O
,	O
we	O
can	O
write	O
down	O
the	O
recurrence	O
relation	O
e	O
[	O
t	O
]	O
=	O
1	O
2	O
(	O
1	O
+	O
e	O
[	O
t	O
]	O
)	O
+	O
1	O
2	O
0	O
)	O
e	O
[	O
t	O
]	O
=	O
1	O
:	O
(	O
2.105	O
)	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
^f	O
the	O
probability	B
distribution	O
of	O
the	O
‘	O
estimator	B
’	O
^f	O
=	O
1=	O
(	O
1	O
+	O
t	O
)	O
,	O
given	O
that	O
f	O
=	O
1=2	O
,	O
is	O
plotted	O
in	O
(	O
cid:12	O
)	O
gure	O
2.12.	O
the	O
probability	O
of	O
^f	O
is	O
simply	O
the	O
probability	O
of	O
the	O
corresponding	O
value	O
of	O
t.	O
figure	O
2.12.	O
the	O
probability	B
distribution	O
of	O
the	O
estimator	O
^f	O
=	O
1=	O
(	O
1	O
+	O
t	O
)	O
,	O
given	O
that	O
f	O
=	O
1=2	O
.	O
solution	O
to	O
exercise	O
2.35	O
(	O
p.38	O
)	O
.	O
(	O
a	O
)	O
the	O
mean	B
number	O
of	O
rolls	O
from	O
one	O
six	B
to	O
the	O
next	O
six	B
is	O
six	B
(	O
assuming	O
we	O
start	O
counting	B
rolls	O
after	O
the	O
(	O
cid:12	O
)	O
rst	O
of	O
the	O
two	O
sixes	O
)	O
.	O
the	O
probability	B
that	O
the	O
next	O
six	B
occurs	O
on	O
the	O
rth	O
roll	O
is	O
the	O
probability	O
of	O
not	O
getting	O
a	O
six	B
for	O
r	O
(	O
cid:0	O
)	O
1	O
rolls	O
multiplied	O
by	O
the	O
probability	O
of	O
then	O
getting	O
a	O
six	B
:	O
p	O
(	O
r1	O
=	O
r	O
)	O
=	O
(	O
cid:18	O
)	O
5	O
6	O
(	O
cid:19	O
)	O
r	O
(	O
cid:0	O
)	O
1	O
1	O
6	O
;	O
for	O
r	O
2	O
f1	O
;	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
g.	O
(	O
2.106	O
)	O
this	O
probability	B
distribution	O
of	O
the	O
number	O
of	O
rolls	O
,	O
r	O
,	O
may	O
be	O
called	O
an	O
exponential	B
distribution	I
,	O
since	O
p	O
(	O
r1	O
=	O
r	O
)	O
=	O
e	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
r=z	O
;	O
(	O
2.107	O
)	O
where	O
(	O
cid:11	O
)	O
=	O
ln	O
(	O
6=5	O
)	O
,	O
and	O
z	O
is	O
a	O
normalizing	B
constant	I
.	O
(	O
b	O
)	O
the	O
mean	B
number	O
of	O
rolls	O
from	O
the	O
clock	O
until	O
the	O
next	O
six	B
is	O
six	B
.	O
(	O
c	O
)	O
the	O
mean	B
number	O
of	O
rolls	O
,	O
going	O
back	O
in	O
time	O
,	O
until	O
the	O
most	O
recent	O
six	B
is	O
six	B
.	O
e	O
[	O
t	O
]	O
=	O
1xt=0	O
t	O
(	O
cid:18	O
)	O
1	O
2	O
(	O
cid:19	O
)	O
t	O
1	O
2	O
;	O
(	O
2.104	O
)	O
p	O
(	O
^f	O
)	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
46	O
2	O
|	O
probability	B
,	O
entropy	B
,	O
and	O
inference	O
(	O
d	O
)	O
the	O
mean	B
number	O
of	O
rolls	O
from	O
the	O
six	B
before	O
the	O
clock	O
struck	O
to	O
the	O
six	B
after	O
the	O
clock	O
struck	O
is	O
the	O
sum	O
of	O
the	O
answers	O
to	O
(	O
b	O
)	O
and	O
(	O
c	O
)	O
,	O
less	O
one	O
,	O
that	O
is	O
,	O
eleven	O
.	O
(	O
e	O
)	O
rather	O
than	O
explaining	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
(	O
a	O
)	O
and	O
(	O
d	O
)	O
,	O
let	O
me	O
give	O
another	O
hint	O
.	O
imagine	O
that	O
the	O
buses	O
in	O
poissonville	O
arrive	O
indepen-	O
dently	O
at	O
random	B
(	O
a	O
poisson	O
process	O
)	O
,	O
with	O
,	O
on	O
average	O
,	O
one	O
bus	O
every	O
six	B
minutes	O
.	O
imagine	O
that	O
passengers	O
turn	O
up	O
at	O
bus-stops	O
at	O
a	O
uniform	O
rate	B
,	O
and	O
are	O
scooped	O
up	O
by	O
the	O
bus	O
without	O
delay	O
,	O
so	O
the	O
interval	O
be-	O
tween	O
two	O
buses	O
remains	O
constant	O
.	O
buses	O
that	O
follow	O
gaps	O
bigger	O
than	O
six	B
minutes	O
become	O
overcrowded	O
.	O
the	O
passengers	O
’	O
representative	O
com-	O
plains	O
that	O
two-thirds	O
of	O
all	O
passengers	O
found	O
themselves	O
on	O
overcrowded	O
buses	O
.	O
the	O
bus	O
operator	O
claims	O
,	O
‘	O
no	O
,	O
no	O
{	O
only	O
one	O
third	O
of	O
our	O
buses	O
are	O
overcrowded	O
’	O
.	O
can	O
both	O
these	O
claims	O
be	O
true	O
?	O
solution	O
to	O
exercise	O
2.38	O
(	O
p.39	O
)	O
.	O
binomial	B
distribution	I
method	O
.	O
from	O
the	O
solution	O
to	O
exercise	O
1.2	O
,	O
pb	O
=	O
3f	O
2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
+	O
f	O
3.	O
sum	B
rule	I
method	O
.	O
the	O
marginal	B
probabilities	O
of	O
the	O
eight	O
values	O
of	O
r	O
are	O
illustrated	O
by	O
:	O
p	O
(	O
r	O
=	O
000	O
)	O
=	O
1/2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
3	O
+	O
1/2f	O
3	O
;	O
p	O
(	O
r	O
=	O
001	O
)	O
=	O
1/2f	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
2	O
+	O
1/2f	O
2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
=	O
1/2f	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
:	O
the	O
posterior	O
probabilities	O
are	O
represented	O
by	O
p	O
(	O
s	O
=	O
1j	O
r	O
=	O
000	O
)	O
=	O
f	O
3	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
3	O
+	O
f	O
3	O
(	O
2.108	O
)	O
(	O
2.109	O
)	O
(	O
2.110	O
)	O
0.15	O
0.1	O
0.05	O
0	O
0	O
5	O
10	O
15	O
20	O
figure	O
2.13.	O
the	O
probability	B
distribution	O
of	O
the	O
number	O
of	O
rolls	O
r1	O
from	O
one	O
6	O
to	O
the	O
next	O
(	O
falling	O
solid	O
line	O
)	O
,	O
p	O
(	O
r1	O
=	O
r	O
)	O
=	O
(	O
cid:18	O
)	O
5	O
6	O
(	O
cid:19	O
)	O
r	O
(	O
cid:0	O
)	O
1	O
1	O
6	O
;	O
and	O
the	O
probability	B
distribution	O
(	O
dashed	O
line	O
)	O
of	O
the	O
number	O
of	O
rolls	O
from	O
the	O
6	O
before	O
1pm	O
to	O
the	O
next	O
6	O
,	O
rtot	O
,	O
p	O
(	O
rtot	O
=	O
r	O
)	O
=	O
r	O
(	O
cid:18	O
)	O
5	O
6	O
(	O
cid:19	O
)	O
r	O
(	O
cid:0	O
)	O
1	O
(	O
cid:18	O
)	O
1	O
6	O
(	O
cid:19	O
)	O
2	O
:	O
the	O
probability	B
p	O
(	O
r1	O
>	O
6	O
)	O
is	O
about	O
1/3	O
;	O
the	O
probability	B
p	O
(	O
rtot	O
>	O
6	O
)	O
is	O
about	O
2/3	O
.	O
the	O
mean	B
of	O
r1	O
is	O
6	O
,	O
and	O
the	O
mean	B
of	O
rtot	O
is	O
11.	O
and	O
p	O
(	O
s	O
=	O
1j	O
r	O
=	O
001	O
)	O
=	O
=	O
f	O
:	O
(	O
2.111	O
)	O
the	O
probabilities	O
of	O
error	O
in	O
these	O
representative	O
cases	O
are	O
thus	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
f	O
2	O
f	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
2	O
+	O
f	O
2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
p	O
(	O
errorj	O
r	O
=	O
000	O
)	O
=	O
f	O
3	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
3	O
+	O
f	O
3	O
(	O
2.112	O
)	O
and	O
(	O
2.113	O
)	O
notice	O
that	O
while	O
the	O
average	B
probability	O
of	O
error	O
of	O
r3	O
is	O
about	O
3f	O
2	O
,	O
the	O
probability	B
(	O
given	O
r	O
)	O
that	O
any	O
particular	O
bit	B
is	O
wrong	O
is	O
either	O
about	O
f	O
3	O
or	O
f	O
.	O
p	O
(	O
errorj	O
r	O
=	O
001	O
)	O
=	O
f	O
:	O
the	O
average	B
error	O
probability	B
,	O
using	O
the	O
sum	B
rule	I
,	O
is	O
p	O
(	O
error	O
)	O
=	O
xr	O
p	O
(	O
r	O
)	O
p	O
(	O
error	O
j	O
r	O
)	O
=	O
2	O
[	O
1/2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
3	O
+	O
1/2f	O
3	O
]	O
so	O
f	O
3	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
3	O
+	O
f	O
3	O
+	O
6	O
[	O
1/2f	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
]	O
f	O
:	O
p	O
(	O
error	O
)	O
=	O
f	O
3	O
+	O
3f	O
2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
:	O
solution	O
to	O
exercise	O
2.39	O
(	O
p.40	O
)	O
.	O
the	O
entropy	B
is	O
9.7	O
bits	O
per	O
word	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
two	O
terms	O
are	O
for	O
the	O
cases	O
r	O
=	O
000	O
and	O
111	O
;	O
the	O
remaining	O
6	O
are	O
for	O
the	O
other	O
outcomes	O
,	O
which	O
share	O
the	O
same	O
probability	O
of	O
occurring	O
and	O
identical	O
error	B
probability	I
,	O
f	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
chapter	O
3	O
if	O
you	O
are	O
eager	O
to	O
get	O
on	O
to	O
information	B
theory	I
,	O
data	B
compression	I
,	O
and	O
noisy	O
channels	O
,	O
you	O
can	O
skip	O
to	O
chapter	O
4.	O
data	B
compression	I
and	O
data	B
modelling	I
are	O
intimately	O
connected	O
,	O
however	O
,	O
so	O
you	O
’	O
ll	O
probably	O
want	O
to	O
come	O
back	O
to	O
this	O
chapter	O
by	O
the	O
time	O
you	O
get	O
to	O
chapter	O
6.	O
before	O
reading	O
chapter	O
3	O
,	O
it	O
might	O
be	O
good	B
to	O
look	O
at	O
the	O
following	O
exercises	O
.	O
.	O
exercise	O
3.1	O
.	O
[	O
2	O
,	O
p.59	O
]	O
a	O
die	B
is	O
selected	O
at	O
random	B
from	O
two	O
twenty-faced	O
dice	O
on	O
which	O
the	O
symbols	O
1	O
{	O
10	O
are	O
written	O
with	O
nonuniform	O
frequency	B
as	O
follows	O
.	O
symbol	O
1	O
number	O
of	O
faces	O
of	O
die	O
a	O
6	O
number	O
of	O
faces	O
of	O
die	O
b	O
3	O
2	O
4	O
3	O
3	O
3	O
2	O
4	O
2	O
2	O
5	O
1	O
2	O
6	O
1	O
2	O
7	O
1	O
2	O
8	O
1	O
2	O
9	O
1	O
1	O
10	O
0	O
1	O
the	O
randomly	O
chosen	O
die	B
is	O
rolled	O
7	O
times	O
,	O
with	O
the	O
following	O
outcomes	O
:	O
what	O
is	O
the	O
probability	B
that	O
the	O
die	B
is	O
die	B
a	O
?	O
5	O
,	O
3	O
,	O
9	O
,	O
3	O
,	O
8	O
,	O
4	O
,	O
7.	O
.	O
exercise	O
3.2	O
.	O
[	O
2	O
,	O
p.59	O
]	O
assume	O
that	O
there	O
is	O
a	O
third	O
twenty-faced	O
die	B
,	O
die	B
c	O
,	O
on	O
which	O
the	O
symbols	O
1	O
{	O
20	O
are	O
written	O
once	O
each	O
.	O
as	O
above	O
,	O
one	O
of	O
the	O
three	O
dice	O
is	O
selected	O
at	O
random	B
and	O
rolled	O
7	O
times	O
,	O
giving	O
the	O
outcomes	O
:	O
3	O
,	O
5	O
,	O
4	O
,	O
8	O
,	O
3	O
,	O
9	O
,	O
7.	O
what	O
is	O
the	O
probability	B
that	O
the	O
die	B
is	O
(	O
a	O
)	O
die	B
a	O
,	O
(	O
b	O
)	O
die	B
b	O
,	O
(	O
c	O
)	O
die	B
c	O
?	O
exercise	O
3.3	O
.	O
[	O
3	O
,	O
p.48	O
]	O
inferring	O
a	O
decay	O
constant	O
unstable	O
particles	O
are	O
emitted	O
from	O
a	O
source	O
and	O
decay	O
at	O
a	O
distance	B
x	O
,	O
a	O
real	O
number	O
that	O
has	O
an	O
exponential	B
probability	O
distribution	B
with	O
characteristic	O
length	B
(	O
cid:21	O
)	O
.	O
decay	O
events	O
can	O
be	O
observed	O
only	O
if	O
they	O
occur	O
in	O
a	O
window	B
extending	O
from	O
x	O
=	O
1	O
cm	O
to	O
x	O
=	O
20	O
cm	O
.	O
n	O
decays	O
are	O
observed	O
at	O
locations	O
fx1	O
;	O
:	O
:	O
:	O
;	O
xng	O
.	O
what	O
is	O
(	O
cid:21	O
)	O
?	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
x	O
.	O
exercise	O
3.4	O
.	O
[	O
3	O
,	O
p.55	O
]	O
forensic	B
evidence	O
two	O
people	O
have	O
left	O
traces	O
of	O
their	O
own	O
blood	O
at	O
the	O
scene	O
of	O
a	O
crime	O
.	O
a	O
suspect	O
,	O
oliver	O
,	O
is	O
tested	O
and	O
found	O
to	O
have	O
type	O
‘	O
o	O
’	O
blood	O
.	O
the	O
blood	O
groups	O
of	O
the	O
two	O
traces	O
are	O
found	O
to	O
be	O
of	O
type	O
‘	O
o	O
’	O
(	O
a	O
common	O
type	O
in	O
the	O
local	O
population	O
,	O
having	O
frequency	B
60	O
%	O
)	O
and	O
of	O
type	O
‘	O
ab	O
’	O
(	O
a	O
rare	O
type	O
,	O
with	O
frequency	O
1	O
%	O
)	O
.	O
do	O
these	O
data	O
(	O
type	O
‘	O
o	O
’	O
and	O
‘	O
ab	O
’	O
blood	O
were	O
found	O
at	O
scene	O
)	O
give	O
evidence	B
in	O
favour	O
of	O
the	O
proposition	O
that	O
oliver	O
was	O
one	O
of	O
the	O
two	O
people	O
present	O
at	O
the	O
crime	O
?	O
47	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
3	O
more	O
about	O
inference	B
it	O
is	O
not	O
a	O
controversial	O
statement	O
that	O
bayes	O
’	O
theorem	B
provides	O
the	O
correct	O
language	O
for	O
describing	O
the	O
inference	B
of	O
a	O
message	O
communicated	O
over	O
a	O
noisy	B
channel	I
,	O
as	O
we	O
used	O
it	O
in	O
chapter	O
1	O
(	O
p.6	O
)	O
.	O
but	O
strangely	O
,	O
when	O
it	O
comes	O
to	O
other	O
inference	B
problems	O
,	O
the	O
use	O
of	O
bayes	O
’	O
theorem	B
is	O
not	O
so	O
widespread	O
.	O
3.1	O
a	O
(	O
cid:12	O
)	O
rst	O
inference	B
problem	O
when	O
i	O
was	O
an	O
undergraduate	O
in	O
cambridge	O
,	O
i	O
was	O
privileged	O
to	O
receive	O
su-	O
pervisions	O
from	O
steve	O
gull	O
.	O
sitting	O
at	O
his	O
desk	O
in	O
a	O
dishevelled	O
o	O
(	O
cid:14	O
)	O
ce	O
in	O
st.	O
john	O
’	O
s	O
college	O
,	O
i	O
asked	O
him	O
how	O
one	O
ought	O
to	O
answer	O
an	O
old	O
tripos	O
question	O
(	O
exercise	O
3.3	O
)	O
:	O
unstable	O
particles	O
are	O
emitted	O
from	O
a	O
source	O
and	O
decay	O
at	O
a	O
distance	B
x	O
,	O
a	O
real	O
number	O
that	O
has	O
an	O
exponential	B
probability	O
dis-	O
tribution	O
with	O
characteristic	O
length	B
(	O
cid:21	O
)	O
.	O
decay	O
events	O
can	O
be	O
ob-	O
served	O
only	O
if	O
they	O
occur	O
in	O
a	O
window	B
extending	O
from	O
x	O
=	O
1	O
cm	O
to	O
x	O
=	O
20	O
cm	O
.	O
n	O
decays	O
are	O
observed	O
at	O
locations	O
fx1	O
;	O
:	O
:	O
:	O
;	O
xng	O
.	O
what	O
is	O
(	O
cid:21	O
)	O
?	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
x	O
i	O
had	O
scratched	O
my	O
head	O
over	O
this	O
for	O
some	O
time	O
.	O
my	O
education	O
had	O
provided	O
me	O
with	O
a	O
couple	O
of	O
approaches	O
to	O
solving	O
such	O
inference	B
problems	O
:	O
construct-	O
ing	O
‘	O
estimators	O
’	O
of	O
the	O
unknown	O
parameters	B
;	O
or	O
‘	O
(	O
cid:12	O
)	O
tting	O
’	O
the	O
model	B
to	O
the	O
data	O
,	O
or	O
to	O
a	O
processed	O
version	O
of	O
the	O
data	O
.	O
since	O
the	O
mean	B
of	O
an	O
unconstrained	O
exponential	B
distribution	I
is	O
(	O
cid:21	O
)	O
,	O
it	O
seemed	O
reasonable	O
to	O
examine	O
the	O
sample	B
mean	O
(	O
cid:22	O
)	O
x	O
=pn	O
xn=n	O
and	O
see	O
if	O
an	O
estimator	B
^	O
(	O
cid:21	O
)	O
could	O
be	O
obtained	O
from	O
it	O
.	O
it	O
was	O
evident	O
that	O
the	O
estimator	B
^	O
(	O
cid:21	O
)	O
=	O
(	O
cid:22	O
)	O
x	O
(	O
cid:0	O
)	O
1	O
would	O
be	O
appropriate	O
for	O
(	O
cid:21	O
)	O
(	O
cid:28	O
)	O
20	O
cm	O
,	O
but	O
not	O
for	O
cases	O
where	O
the	O
truncation	O
of	O
the	O
distribution	O
at	O
the	O
right-hand	O
side	O
is	O
signi	O
(	O
cid:12	O
)	O
cant	O
;	O
with	O
a	O
little	O
ingenuity	O
and	O
the	O
introduction	O
of	O
ad	O
hoc	O
bins	O
,	O
promising	O
estimators	O
for	O
(	O
cid:21	O
)	O
(	O
cid:29	O
)	O
20	O
cm	O
could	O
be	O
constructed	O
.	O
but	O
there	O
was	O
no	O
obvious	O
estimator	B
that	O
would	O
work	O
under	O
all	O
conditions	O
.	O
to	O
a	O
histogram	O
derived	O
from	O
the	O
data	O
.	O
i	O
was	O
stuck	O
.	O
nor	O
could	O
i	O
(	O
cid:12	O
)	O
nd	O
a	O
satisfactory	O
approach	O
based	O
on	O
(	O
cid:12	O
)	O
tting	O
the	O
density	B
p	O
(	O
xj	O
(	O
cid:21	O
)	O
)	O
what	O
is	O
the	O
general	O
solution	O
to	O
this	O
problem	O
and	O
others	O
like	O
it	O
?	O
is	O
it	O
always	O
necessary	O
,	O
when	O
confronted	O
by	O
a	O
new	O
inference	B
problem	O
,	O
to	O
grope	O
in	O
the	O
dark	O
for	O
appropriate	O
‘	O
estimators	O
’	O
and	O
worry	O
about	O
(	O
cid:12	O
)	O
nding	O
the	O
‘	O
best	O
’	O
estimator	B
(	O
whatever	O
that	O
means	O
)	O
?	O
48	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
3.1	O
:	O
a	O
(	O
cid:12	O
)	O
rst	O
inference	B
problem	O
49	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
p	O
(	O
x|lambda=2	O
)	O
p	O
(	O
x|lambda=5	O
)	O
p	O
(	O
x|lambda=10	O
)	O
figure	O
3.1.	O
the	O
probability	B
density	O
p	O
(	O
xj	O
(	O
cid:21	O
)	O
)	O
as	O
a	O
function	B
of	O
x	O
.	O
2	O
4	O
6	O
8	O
10	O
12	O
14	O
16	O
18	O
20	O
x	O
p	O
(	O
x=3|lambda	O
)	O
p	O
(	O
x=5|lambda	O
)	O
p	O
(	O
x=12|lambda	O
)	O
1	O
10	O
100	O
(	O
cid:21	O
)	O
figure	O
3.2.	O
the	O
probability	B
density	O
p	O
(	O
xj	O
(	O
cid:21	O
)	O
)	O
as	O
a	O
function	B
of	O
(	O
cid:21	O
)	O
,	O
for	O
three	O
di	O
(	O
cid:11	O
)	O
erent	O
values	O
of	O
x.	O
when	O
plotted	O
this	O
way	O
round	O
,	O
the	O
function	B
is	O
known	O
as	O
the	O
likelihood	B
of	O
(	O
cid:21	O
)	O
.	O
the	O
marks	O
indicate	O
the	O
three	O
values	O
of	O
(	O
cid:21	O
)	O
,	O
(	O
cid:21	O
)	O
=	O
2	O
;	O
5	O
;	O
10	O
,	O
that	O
were	O
used	O
in	O
the	O
preceding	O
(	O
cid:12	O
)	O
gure	O
.	O
steve	O
wrote	O
down	O
the	O
probability	O
of	O
one	O
data	O
point	O
,	O
given	O
(	O
cid:21	O
)	O
:	O
where	O
p	O
(	O
xj	O
(	O
cid:21	O
)	O
)	O
=	O
(	O
cid:26	O
)	O
1	O
z	O
(	O
(	O
cid:21	O
)	O
)	O
=z	O
20	O
dx	O
1	O
1	O
(	O
cid:21	O
)	O
e	O
(	O
cid:0	O
)	O
x=	O
(	O
cid:21	O
)	O
=z	O
(	O
(	O
cid:21	O
)	O
)	O
1	O
<	O
x	O
<	O
20	O
otherwise	O
0	O
(	O
cid:21	O
)	O
e	O
(	O
cid:0	O
)	O
x=	O
(	O
cid:21	O
)	O
=	O
(	O
cid:16	O
)	O
e	O
(	O
cid:0	O
)	O
1=	O
(	O
cid:21	O
)	O
(	O
cid:0	O
)	O
e	O
(	O
cid:0	O
)	O
20=	O
(	O
cid:21	O
)	O
(	O
cid:17	O
)	O
:	O
this	O
seemed	O
obvious	O
enough	O
.	O
then	O
he	O
wrote	O
bayes	O
’	O
theorem	B
:	O
(	O
3.1	O
)	O
(	O
3.2	O
)	O
(	O
3.3	O
)	O
(	O
3.4	O
)	O
3	O
2	O
1	O
p	O
(	O
(	O
cid:21	O
)	O
jfx1	O
;	O
:	O
:	O
:	O
;	O
xng	O
)	O
=	O
/	O
p	O
(	O
fxgj	O
(	O
cid:21	O
)	O
)	O
p	O
(	O
(	O
cid:21	O
)	O
)	O
p	O
(	O
fxg	O
)	O
1	O
(	O
(	O
cid:21	O
)	O
z	O
(	O
(	O
cid:21	O
)	O
)	O
)	O
n	O
exp	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
pn	O
1	O
xn=	O
(	O
cid:21	O
)	O
(	O
cid:17	O
)	O
p	O
(	O
(	O
cid:21	O
)	O
)	O
:	O
suddenly	O
,	O
the	O
straightforward	O
distribution	B
p	O
(	O
fx1	O
;	O
:	O
:	O
:	O
;	O
xngj	O
(	O
cid:21	O
)	O
)	O
,	O
de	O
(	O
cid:12	O
)	O
ning	O
the	O
probability	O
of	O
the	O
data	O
given	O
the	O
hypothesis	O
(	O
cid:21	O
)	O
,	O
was	O
being	O
turned	O
on	O
its	O
head	O
so	O
as	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
probability	O
of	O
a	O
hypothesis	O
given	O
the	O
data	O
.	O
a	O
simple	O
(	O
cid:12	O
)	O
gure	O
showed	O
the	O
probability	O
of	O
a	O
single	O
data	O
point	O
p	O
(	O
xj	O
(	O
cid:21	O
)	O
)	O
as	O
a	O
familiar	O
function	B
of	O
x	O
,	O
for	O
di	O
(	O
cid:11	O
)	O
erent	O
values	O
of	O
(	O
cid:21	O
)	O
(	O
(	O
cid:12	O
)	O
gure	O
3.1	O
)	O
.	O
each	O
curve	O
was	O
an	O
innocent	O
exponential	B
,	O
normalized	O
to	O
have	O
area	O
1.	O
plotting	O
the	O
same	O
function	B
as	O
a	O
function	B
of	O
(	O
cid:21	O
)	O
for	O
a	O
(	O
cid:12	O
)	O
xed	O
value	O
of	O
x	O
,	O
something	O
remarkable	O
happens	O
:	O
a	O
peak	O
emerges	O
(	O
(	O
cid:12	O
)	O
gure	O
3.2	O
)	O
.	O
to	O
help	O
understand	O
these	O
two	O
points	O
of	O
view	O
of	O
the	O
one	O
function	B
,	O
(	O
cid:12	O
)	O
gure	O
3.3	O
shows	O
a	O
surface	O
plot	O
of	O
p	O
(	O
xj	O
(	O
cid:21	O
)	O
)	O
as	O
a	O
function	B
of	O
x	O
and	O
(	O
cid:21	O
)	O
.	O
n=1	O
=	O
f1:5	O
;	O
2	O
;	O
3	O
;	O
4	O
;	O
5	O
;	O
12g	O
,	O
the	O
likelihood	B
function	O
p	O
(	O
fxgj	O
(	O
cid:21	O
)	O
)	O
is	O
the	O
product	O
of	O
the	O
n	O
functions	B
of	O
(	O
cid:21	O
)	O
,	O
p	O
(	O
xn	O
j	O
(	O
cid:21	O
)	O
)	O
(	O
(	O
cid:12	O
)	O
gure	O
3.4	O
)	O
.	O
for	O
a	O
dataset	O
consisting	O
of	O
several	O
points	O
,	O
e.g.	O
,	O
the	O
six	B
points	O
fxgn	O
1.4e-06	O
1.2e-06	O
1e-06	O
8e-07	O
6e-07	O
4e-07	O
2e-07	O
0	O
1	O
10	O
100	O
1	O
1.5	O
x	O
2	O
2.5	O
100	O
10	O
(	O
cid:21	O
)	O
1	O
figure	O
3.3.	O
the	O
probability	B
density	O
p	O
(	O
xj	O
(	O
cid:21	O
)	O
)	O
as	O
a	O
function	B
of	O
x	O
and	O
(	O
cid:21	O
)	O
.	O
figures	O
3.1	O
and	O
3.2	O
are	O
vertical	O
sections	O
through	O
this	O
surface	O
.	O
figure	O
3.4.	O
the	O
likelihood	B
function	O
in	O
the	O
case	O
of	O
a	O
six-point	O
dataset	O
,	O
p	O
(	O
fxg	O
=	O
f1:5	O
;	O
2	O
;	O
3	O
;	O
4	O
;	O
5	O
;	O
12gj	O
(	O
cid:21	O
)	O
)	O
,	O
as	O
a	O
function	B
of	O
(	O
cid:21	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
50	O
3	O
|	O
more	O
about	O
inference	B
if	O
you	O
have	O
any	O
di	O
(	O
cid:14	O
)	O
culty	O
understanding	O
this	O
chapter	O
i	O
recommend	O
ensuring	O
you	O
are	O
happy	O
with	O
exercises	O
3.1	O
and	O
3.2	O
(	O
p.47	O
)	O
then	O
noting	O
their	O
similarity	O
to	O
exercise	O
3.3.	O
steve	O
summarized	O
bayes	O
’	O
theorem	B
as	O
embodying	O
the	O
fact	O
that	O
what	O
you	O
know	O
about	O
(	O
cid:21	O
)	O
after	O
the	O
data	O
arrive	O
is	O
what	O
you	O
knew	O
before	O
[	O
p	O
(	O
(	O
cid:21	O
)	O
)	O
]	O
,	O
and	O
what	O
the	O
data	O
told	O
you	O
[	O
p	O
(	O
fxgj	O
(	O
cid:21	O
)	O
)	O
]	O
.	O
probabilities	O
are	O
used	O
here	O
to	O
quantify	O
degrees	B
of	I
belief	I
.	O
to	O
nip	O
possible	O
confusion	O
in	O
the	O
bud	O
,	O
it	O
must	O
be	O
emphasized	O
that	O
the	O
hypothesis	O
(	O
cid:21	O
)	O
that	O
cor-	O
rectly	O
describes	O
the	O
situation	O
is	O
not	O
a	O
stochastic	B
variable	O
,	O
and	O
the	O
fact	O
that	O
the	O
bayesian	O
uses	O
a	O
probability	B
distribution	O
p	O
does	O
not	O
mean	B
that	O
he	O
thinks	O
of	O
the	O
world	O
as	O
stochastically	O
changing	O
its	O
nature	O
between	O
the	O
states	O
described	O
by	O
the	O
di	O
(	O
cid:11	O
)	O
erent	O
hypotheses	O
.	O
he	O
uses	O
the	O
notation	B
of	O
probabilities	O
to	O
represent	O
his	O
beliefs	O
about	O
the	O
mutually	O
exclusive	O
micro-hypotheses	O
(	O
here	O
,	O
values	O
of	O
(	O
cid:21	O
)	O
)	O
,	O
of	O
which	O
only	O
one	O
is	O
actually	O
true	O
.	O
that	O
probabilities	O
can	O
denote	O
degrees	B
of	I
belief	I
,	O
given	O
assumptions	B
,	O
seemed	O
reasonable	O
to	O
me	O
.	O
the	O
posterior	B
probability	I
distribution	O
(	O
3.4	O
)	O
represents	O
the	O
unique	O
and	O
com-	O
plete	O
solution	O
to	O
the	O
problem	O
.	O
there	O
is	O
no	O
need	O
to	O
invent	O
‘	O
estimators	O
’	O
;	O
nor	O
do	O
we	O
need	O
to	O
invent	O
criteria	O
for	O
comparing	O
alternative	O
estimators	O
with	O
each	O
other	O
.	O
whereas	O
orthodox	O
statisticians	O
o	O
(	O
cid:11	O
)	O
er	O
twenty	O
ways	O
of	O
solving	O
a	O
problem	O
,	O
and	O
an-	O
other	O
twenty	O
di	O
(	O
cid:11	O
)	O
erent	O
criteria	O
for	O
deciding	O
which	O
of	O
these	O
solutions	O
is	O
the	O
best	O
,	O
bayesian	O
statistics	O
only	O
o	O
(	O
cid:11	O
)	O
ers	O
one	O
answer	O
to	O
a	O
well-posed	O
problem	O
.	O
assumptions	B
in	O
inference	B
our	O
inference	B
is	O
conditional	B
on	O
our	O
assumptions	B
[	O
for	O
example	O
,	O
the	O
prior	B
p	O
(	O
(	O
cid:21	O
)	O
)	O
]	O
.	O
critics	O
view	O
such	O
priors	O
as	O
a	O
di	O
(	O
cid:14	O
)	O
culty	O
because	O
they	O
are	O
‘	O
subjective	O
’	O
,	O
but	O
i	O
don	O
’	O
t	O
see	O
how	O
it	O
could	O
be	O
otherwise	O
.	O
how	O
can	O
one	O
perform	O
inference	B
without	O
making	O
assumptions	B
?	O
i	O
believe	O
that	O
it	O
is	O
of	O
great	O
value	O
that	O
bayesian	O
methods	B
force	O
one	O
to	O
make	O
these	O
tacit	O
assumptions	B
explicit	O
.	O
first	O
,	O
once	O
assumptions	B
are	O
made	O
,	O
the	O
inferences	O
are	O
objective	O
and	O
unique	O
,	O
reproducible	O
with	O
complete	O
agreement	O
by	O
anyone	O
who	O
has	O
the	O
same	O
informa-	O
tion	O
and	O
makes	O
the	O
same	O
assumptions	B
.	O
for	O
example	O
,	O
given	O
the	O
assumptions	B
listed	O
above	O
,	O
h	O
,	O
and	O
the	O
data	O
d	O
,	O
everyone	O
will	O
agree	O
about	O
the	O
posterior	O
prob-	O
ability	O
of	O
the	O
decay	O
length	B
(	O
cid:21	O
)	O
:	O
p	O
(	O
(	O
cid:21	O
)	O
j	O
d	O
;	O
h	O
)	O
=	O
p	O
(	O
d	O
j	O
(	O
cid:21	O
)	O
;	O
h	O
)	O
p	O
(	O
(	O
cid:21	O
)	O
jh	O
)	O
p	O
(	O
d	O
jh	O
)	O
:	O
(	O
3.5	O
)	O
second	O
,	O
when	O
the	O
assumptions	B
are	O
explicit	O
,	O
they	O
are	O
easier	O
to	O
criticize	O
,	O
and	O
easier	O
to	O
modify	O
{	O
indeed	O
,	O
we	O
can	O
quantify	O
the	O
sensitivity	O
of	O
our	O
inferences	O
to	O
the	O
details	O
of	O
the	O
assumptions	O
.	O
for	O
example	O
,	O
we	O
can	O
note	O
from	O
the	O
likelihood	B
curves	O
in	O
(	O
cid:12	O
)	O
gure	O
3.2	O
that	O
in	O
the	O
case	O
of	O
a	O
single	O
data	O
point	O
at	O
x	O
=	O
5	O
,	O
the	O
likelihood	B
function	O
is	O
less	O
strongly	O
peaked	O
than	O
in	O
the	O
case	O
x	O
=	O
3	O
;	O
the	O
details	O
of	O
the	O
prior	O
p	O
(	O
(	O
cid:21	O
)	O
)	O
become	O
increasingly	O
important	O
as	O
the	O
sample	B
mean	O
(	O
cid:22	O
)	O
x	O
gets	O
closer	O
to	O
the	O
middle	O
of	O
the	O
window	O
,	O
10.5.	O
in	O
the	O
case	O
x	O
=	O
12	O
,	O
the	O
likelihood	B
function	O
doesn	O
’	O
t	O
have	O
a	O
peak	O
at	O
all	O
{	O
such	O
data	O
merely	O
rule	O
out	O
small	O
values	O
of	O
(	O
cid:21	O
)	O
,	O
and	O
don	O
’	O
t	O
give	O
any	O
information	B
about	O
the	O
relative	B
probabilities	O
of	O
large	O
values	O
of	O
(	O
cid:21	O
)	O
.	O
so	O
in	O
this	O
case	O
,	O
the	O
details	O
of	O
the	O
prior	O
at	O
the	O
small	O
{	O
(	O
cid:21	O
)	O
end	O
of	O
things	O
are	O
not	O
important	O
,	O
but	O
at	O
the	O
large	O
{	O
(	O
cid:21	O
)	O
end	O
,	O
the	O
prior	B
is	O
important	O
.	O
third	O
,	O
when	O
we	O
are	O
not	O
sure	O
which	O
of	O
various	O
alternative	O
assumptions	B
is	O
the	O
most	O
appropriate	O
for	O
a	O
problem	O
,	O
we	O
can	O
treat	O
this	O
question	O
as	O
another	O
inference	B
task	O
.	O
thus	O
,	O
given	O
data	O
d	O
,	O
we	O
can	O
compare	O
alternative	O
assumptions	B
h	O
using	O
bayes	O
’	O
theorem	B
:	O
p	O
(	O
h	O
j	O
d	O
;	O
i	O
)	O
=	O
p	O
(	O
d	O
jh	O
;	O
i	O
)	O
p	O
(	O
h	O
j	O
i	O
)	O
p	O
(	O
d	O
j	O
i	O
)	O
;	O
(	O
3.6	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
3.2	O
:	O
the	O
bent	B
coin	I
51	O
where	O
i	O
denotes	O
the	O
highest	O
assumptions	B
,	O
which	O
we	O
are	O
not	O
questioning	O
.	O
fourth	O
,	O
we	O
can	O
take	O
into	O
account	O
our	O
uncertainty	O
regarding	O
such	O
assump-	O
tions	O
when	O
we	O
make	O
subsequent	O
predictions	O
.	O
rather	O
than	O
choosing	O
one	O
partic-	O
ular	O
assumption	O
h	O
(	O
cid:3	O
)	O
,	O
and	O
working	O
out	O
our	O
predictions	O
about	O
some	O
quantity	O
t	O
,	O
p	O
(	O
tj	O
d	O
;	O
h	O
(	O
cid:3	O
)	O
;	O
i	O
)	O
,	O
we	O
obtain	O
predictions	O
that	O
take	O
into	O
account	O
our	O
uncertainty	O
about	O
h	O
by	O
using	O
the	O
sum	B
rule	I
:	O
p	O
(	O
tj	O
d	O
;	O
i	O
)	O
=xh	O
p	O
(	O
tj	O
d	O
;	O
h	O
;	O
i	O
)	O
p	O
(	O
h	O
j	O
d	O
;	O
i	O
)	O
:	O
(	O
3.7	O
)	O
this	O
is	O
another	O
contrast	O
with	O
orthodox	O
statistics	O
,	O
in	O
which	O
it	O
is	O
conventional	O
to	O
‘	O
test	B
’	O
a	O
default	O
model	B
,	O
and	O
then	O
,	O
if	O
the	O
test	B
‘	O
accepts	O
the	O
model	B
’	O
at	O
some	O
‘	O
signi	O
(	O
cid:12	O
)	O
cance	O
level	O
’	O
,	O
to	O
use	O
exclusively	O
that	O
model	B
to	O
make	O
predictions	O
.	O
steve	O
thus	O
persuaded	O
me	O
that	O
probability	B
theory	O
reaches	O
parts	O
that	O
ad	O
hoc	O
methods	B
can	O
not	O
reach	O
.	O
let	O
’	O
s	O
look	O
at	O
a	O
few	O
more	O
examples	O
of	O
simple	O
inference	B
problems	O
.	O
3.2	O
the	O
bent	B
coin	I
a	O
bent	B
coin	I
is	O
tossed	O
f	O
times	O
;	O
we	O
observe	O
a	O
sequence	B
s	O
of	O
heads	O
and	O
tails	O
(	O
which	O
we	O
’	O
ll	O
denote	O
by	O
the	O
symbols	O
a	O
and	O
b	O
)	O
.	O
we	O
wish	O
to	O
know	O
the	O
bias	B
of	O
the	O
coin	B
,	O
and	O
predict	O
the	O
probability	B
that	O
the	O
next	O
toss	O
will	O
result	O
in	O
a	O
head	O
.	O
we	O
(	O
cid:12	O
)	O
rst	O
encountered	O
this	O
task	O
in	O
example	O
2.7	O
(	O
p.30	O
)	O
,	O
and	O
we	O
will	O
encounter	O
it	O
again	O
in	O
chapter	O
6	O
,	O
when	O
we	O
discuss	O
adaptive	B
data	O
compression	B
.	O
it	O
is	O
also	O
the	O
original	O
inference	B
problem	O
studied	O
by	O
thomas	O
bayes	O
in	O
his	O
essay	O
published	O
in	O
1763.	O
as	O
in	O
exercise	O
2.8	O
(	O
p.30	O
)	O
,	O
we	O
will	O
assume	O
a	O
uniform	O
prior	B
distribution	O
and	O
obtain	O
a	O
posterior	O
distribution	O
by	O
multiplying	O
by	O
the	O
likelihood	B
.	O
a	O
critic	O
might	O
object	O
,	O
‘	O
where	O
did	O
this	O
prior	B
come	O
from	O
?	O
’	O
i	O
will	O
not	O
claim	O
that	O
the	O
uniform	O
prior	B
is	O
in	O
any	O
way	O
fundamental	O
;	O
indeed	O
we	O
’	O
ll	O
give	O
examples	O
of	O
nonuniform	O
priors	O
later	O
.	O
the	O
prior	B
is	O
a	O
subjective	O
assumption	O
.	O
one	O
of	O
the	O
themes	O
of	O
this	O
book	O
is	O
:	O
you	O
can	O
’	O
t	O
do	O
inference	O
{	O
or	O
data	B
compression	I
{	O
without	O
making	O
assumptions	B
.	O
we	O
give	O
the	O
name	O
h1	O
to	O
our	O
assumptions	B
.	O
[	O
we	O
’	O
ll	O
be	O
introducing	O
an	O
al-	O
ternative	O
set	B
of	O
assumptions	B
in	O
a	O
moment	O
.	O
]	O
the	O
probability	B
,	O
given	O
p	O
a	O
,	O
that	O
f	O
tosses	O
result	O
in	O
a	O
sequence	B
s	O
that	O
contains	O
ffa	O
;	O
fbg	O
counts	O
of	O
the	O
two	O
outcomes	O
is	O
(	O
3.8	O
)	O
[	O
for	O
example	O
,	O
p	O
(	O
s	O
=	O
aabaj	O
pa	O
;	O
f	O
=	O
4	O
;	O
h1	O
)	O
=	O
papa	O
(	O
1	O
(	O
cid:0	O
)	O
pa	O
)	O
pa	O
:	O
]	O
our	O
(	O
cid:12	O
)	O
rst	O
model	B
assumes	O
a	O
uniform	O
prior	B
distribution	O
for	O
pa	O
,	O
p	O
(	O
sj	O
pa	O
;	O
f	O
;	O
h1	O
)	O
=	O
pfa	O
a	O
(	O
1	O
(	O
cid:0	O
)	O
pa	O
)	O
fb	O
:	O
p	O
(	O
pa	O
jh1	O
)	O
=	O
1	O
;	O
pa	O
2	O
[	O
0	O
;	O
1	O
]	O
(	O
3.9	O
)	O
and	O
pb	O
(	O
cid:17	O
)	O
1	O
(	O
cid:0	O
)	O
pa.	O
inferring	O
unknown	O
parameters	O
given	O
a	O
string	O
of	O
length	O
f	O
of	O
which	O
fa	O
are	O
as	O
and	O
fb	O
are	O
bs	O
,	O
we	O
are	O
interested	O
in	O
(	O
a	O
)	O
inferring	O
what	O
pa	O
might	O
be	O
;	O
(	O
b	O
)	O
predicting	O
whether	O
the	O
next	O
character	O
is	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
52	O
3	O
|	O
more	O
about	O
inference	B
an	O
a	O
or	O
a	O
b	O
.	O
[	O
predictions	O
are	O
always	O
expressed	O
as	O
probabilities	O
.	O
so	O
‘	O
predicting	O
whether	O
the	O
next	O
character	O
is	O
an	O
a	O
’	O
is	O
the	O
same	O
as	O
computing	O
the	O
probability	B
that	O
the	O
next	O
character	O
is	O
an	O
a	O
.	O
]	O
assuming	O
h1	O
to	O
be	O
true	O
,	O
the	O
posterior	B
probability	I
of	O
pa	O
,	O
given	O
a	O
string	O
s	O
of	O
length	O
f	O
that	O
has	O
counts	O
ffa	O
;	O
fbg	O
,	O
is	O
,	O
by	O
bayes	O
’	O
theorem	B
,	O
p	O
(	O
sj	O
pa	O
;	O
f	O
;	O
h1	O
)	O
p	O
(	O
pa	O
jh1	O
)	O
p	O
(	O
pa	O
j	O
s	O
;	O
f	O
;	O
h1	O
)	O
=	O
p	O
(	O
sj	O
f	O
;	O
h1	O
)	O
:	O
(	O
3.10	O
)	O
the	O
factor	O
p	O
(	O
sj	O
pa	O
;	O
f	O
;	O
h1	O
)	O
,	O
which	O
,	O
as	O
a	O
function	B
of	O
pa	O
,	O
is	O
known	O
as	O
the	O
likeli-	O
hood	O
function	B
,	O
was	O
given	O
in	O
equation	O
(	O
3.8	O
)	O
;	O
the	O
prior	B
p	O
(	O
p	O
a	O
jh1	O
)	O
was	O
given	O
in	O
equation	O
(	O
3.9	O
)	O
.	O
our	O
inference	B
of	O
pa	O
is	O
thus	O
:	O
pfa	O
a	O
(	O
1	O
(	O
cid:0	O
)	O
pa	O
)	O
fb	O
p	O
(	O
sj	O
f	O
;	O
h1	O
)	O
the	O
normalizing	B
constant	I
is	O
given	O
by	O
the	O
beta	B
integral	I
p	O
(	O
pa	O
j	O
s	O
;	O
f	O
;	O
h1	O
)	O
=	O
:	O
(	O
3.11	O
)	O
0	O
dpa	O
pfa	O
p	O
(	O
sj	O
f	O
;	O
h1	O
)	O
=z	O
1	O
exercise	O
3.5	O
.	O
[	O
2	O
,	O
p.59	O
]	O
sketch	O
the	O
posterior	B
probability	I
p	O
(	O
pa	O
j	O
s	O
=	O
aba	O
;	O
f	O
=	O
3	O
)	O
.	O
a	O
(	O
1	O
(	O
cid:0	O
)	O
pa	O
)	O
fb	O
=	O
(	O
cid:0	O
)	O
(	O
fa	O
+	O
1	O
)	O
(	O
cid:0	O
)	O
(	O
fb	O
+	O
1	O
)	O
what	O
is	O
the	O
most	O
probable	O
value	O
of	O
pa	O
(	O
i.e.	O
,	O
the	O
value	O
that	O
maximizes	O
the	O
posterior	B
probability	I
density	O
)	O
?	O
what	O
is	O
the	O
mean	B
value	O
of	O
p	O
a	O
under	O
this	O
distribution	B
?	O
(	O
cid:0	O
)	O
(	O
fa	O
+	O
fb	O
+	O
2	O
)	O
(	O
fa	O
+	O
fb	O
+	O
1	O
)	O
!	O
:	O
(	O
3.12	O
)	O
fa	O
!	O
fb	O
!	O
=	O
the	O
answer	O
same	O
p	O
(	O
pa	O
j	O
s	O
=	O
bbb	O
;	O
f	O
=	O
3	O
)	O
.	O
from	O
inferences	O
to	O
predictions	O
questions	O
for	O
the	O
posterior	B
probability	I
our	O
prediction	B
about	O
the	O
next	O
toss	O
,	O
the	O
probability	B
that	O
the	O
next	O
toss	O
is	O
an	O
a	O
,	O
is	O
obtained	O
by	O
integrating	O
over	O
pa.	O
this	O
has	O
the	O
e	O
(	O
cid:11	O
)	O
ect	O
of	O
taking	O
into	O
account	O
our	O
uncertainty	O
about	O
pa	O
when	O
making	O
predictions	O
.	O
by	O
the	O
sum	B
rule	I
,	O
p	O
(	O
aj	O
s	O
;	O
f	O
)	O
=	O
z	O
dpa	O
p	O
(	O
aj	O
pa	O
)	O
p	O
(	O
pa	O
j	O
s	O
;	O
f	O
)	O
:	O
the	O
probability	O
of	O
an	O
a	O
given	O
pa	O
is	O
simply	O
pa	O
,	O
so	O
a	O
(	O
1	O
(	O
cid:0	O
)	O
pa	O
)	O
fb	O
pfa	O
p	O
(	O
sj	O
f	O
)	O
p	O
(	O
aj	O
s	O
;	O
f	O
)	O
=z	O
dpa	O
pa	O
=	O
z	O
dpa	O
(	O
1	O
(	O
cid:0	O
)	O
pa	O
)	O
fb	O
p	O
(	O
sj	O
f	O
)	O
(	O
fa	O
+	O
fb	O
+	O
2	O
)	O
!	O
(	O
cid:21	O
)	O
(	O
cid:30	O
)	O
(	O
cid:20	O
)	O
=	O
(	O
cid:20	O
)	O
(	O
fa	O
+	O
1	O
)	O
!	O
fb	O
!	O
pfa+1	O
a	O
which	O
is	O
known	O
as	O
laplace	O
’	O
s	O
rule	O
.	O
(	O
3.13	O
)	O
(	O
3.14	O
)	O
(	O
3.15	O
)	O
fa	O
!	O
fb	O
!	O
(	O
fa	O
+	O
fb	O
+	O
1	O
)	O
!	O
(	O
cid:21	O
)	O
=	O
fa	O
+	O
1	O
fa	O
+	O
fb	O
+	O
2	O
;	O
(	O
3.16	O
)	O
3.3	O
the	O
bent	B
coin	I
and	O
model	B
comparison	I
imagine	O
that	O
a	O
scientist	O
introduces	O
another	O
theory	B
for	O
our	O
data	O
.	O
he	O
asserts	O
that	O
the	O
source	O
is	O
not	O
really	O
a	O
bent	B
coin	I
but	O
is	O
really	O
a	O
perfectly	O
formed	O
die	B
with	O
one	O
face	O
painted	O
heads	O
(	O
‘	O
a	O
’	O
)	O
and	O
the	O
other	O
(	O
cid:12	O
)	O
ve	O
painted	O
tails	O
(	O
‘	O
b	O
’	O
)	O
.	O
thus	O
the	O
parameter	O
pa	O
,	O
which	O
in	O
the	O
original	O
model	B
,	O
h1	O
,	O
could	O
take	O
any	O
value	O
between	O
0	O
and	O
1	O
,	O
is	O
according	O
to	O
the	O
new	O
hypothesis	O
,	O
h0	O
,	O
not	O
a	O
free	O
parameter	O
at	O
all	O
;	O
rather	O
,	O
it	O
is	O
equal	O
to	O
1=6	O
.	O
[	O
this	O
hypothesis	O
is	O
termed	O
h0	O
so	O
that	O
the	O
su	O
(	O
cid:14	O
)	O
x	O
of	O
each	O
model	B
indicates	O
its	O
number	O
of	O
free	O
parameters	B
.	O
]	O
how	O
can	O
we	O
compare	O
these	O
two	O
models	O
in	O
the	O
light	O
of	O
data	O
?	O
we	O
wish	O
to	O
infer	O
how	O
probable	O
h1	O
is	O
relative	B
to	O
h0	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
3.3	O
:	O
the	O
bent	B
coin	I
and	O
model	B
comparison	I
53	O
model	B
comparison	I
as	O
inference	B
in	O
order	O
to	O
perform	O
model	B
comparison	I
,	O
we	O
write	O
down	O
bayes	O
’	O
theorem	B
again	O
,	O
but	O
this	O
time	O
with	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
argument	O
on	O
the	O
left-hand	O
side	O
.	O
we	O
wish	O
to	O
know	O
how	O
probable	O
h1	O
is	O
given	O
the	O
data	O
.	O
by	O
bayes	O
’	O
theorem	B
,	O
p	O
(	O
h1	O
j	O
s	O
;	O
f	O
)	O
=	O
p	O
(	O
sj	O
f	O
;	O
h1	O
)	O
p	O
(	O
h1	O
)	O
p	O
(	O
sj	O
f	O
)	O
similarly	O
,	O
the	O
posterior	B
probability	I
of	O
h0	O
is	O
p	O
(	O
h0	O
j	O
s	O
;	O
f	O
)	O
=	O
p	O
(	O
sj	O
f	O
;	O
h0	O
)	O
p	O
(	O
h0	O
)	O
p	O
(	O
sj	O
f	O
)	O
:	O
:	O
(	O
3.17	O
)	O
(	O
3.18	O
)	O
the	O
normalizing	B
constant	I
in	O
both	O
cases	O
is	O
p	O
(	O
sj	O
f	O
)	O
,	O
which	O
is	O
the	O
total	O
proba-	O
bility	O
of	O
getting	O
the	O
observed	O
data	O
.	O
if	O
h1	O
and	O
h0	O
are	O
the	O
only	O
models	O
under	O
consideration	O
,	O
this	O
probability	B
is	O
given	O
by	O
the	O
sum	B
rule	I
:	O
p	O
(	O
sj	O
f	O
)	O
=	O
p	O
(	O
sj	O
f	O
;	O
h1	O
)	O
p	O
(	O
h1	O
)	O
+	O
p	O
(	O
sj	O
f	O
;	O
h0	O
)	O
p	O
(	O
h0	O
)	O
:	O
(	O
3.19	O
)	O
to	O
evaluate	O
the	O
posterior	O
probabilities	O
of	O
the	O
hypotheses	O
we	O
need	O
to	O
assign	O
values	O
to	O
the	O
prior	B
probabilities	O
p	O
(	O
h1	O
)	O
and	O
p	O
(	O
h0	O
)	O
;	O
in	O
this	O
case	O
,	O
we	O
might	O
set	B
these	O
to	O
1/2	O
each	O
.	O
and	O
we	O
need	O
to	O
evaluate	O
the	O
data-dependent	O
terms	O
p	O
(	O
sj	O
f	O
;	O
h1	O
)	O
and	O
p	O
(	O
sj	O
f	O
;	O
h0	O
)	O
.	O
we	O
can	O
give	O
names	O
to	O
these	O
quantities	O
.	O
the	O
quantity	O
p	O
(	O
sj	O
f	O
;	O
h1	O
)	O
is	O
a	O
measure	O
of	O
how	O
much	O
the	O
data	O
favour	O
h1	O
,	O
and	O
we	O
call	O
it	O
the	O
evidence	B
for	O
model	B
h1	O
.	O
we	O
already	O
encountered	O
this	O
quantity	O
in	O
equation	O
(	O
3.10	O
)	O
where	O
it	O
appeared	O
as	O
the	O
normalizing	B
constant	I
of	O
the	O
(	O
cid:12	O
)	O
rst	O
inference	B
we	O
made	O
{	O
the	O
inference	B
of	O
pa	O
given	O
the	O
data	O
.	O
how	O
model	O
comparison	O
works	O
:	O
the	O
evidence	B
for	O
a	O
model	B
is	O
usually	O
the	O
normalizing	B
constant	I
of	O
an	O
earlier	O
bayesian	O
inference	B
.	O
we	O
evaluated	O
the	O
normalizing	B
constant	I
for	O
model	B
h1	O
in	O
(	O
3.12	O
)	O
.	O
the	O
evi-	O
dence	O
for	O
model	O
h0	O
is	O
very	O
simple	O
because	O
this	O
model	B
has	O
no	O
parameters	B
to	O
infer	O
.	O
de	O
(	O
cid:12	O
)	O
ning	O
p0	O
to	O
be	O
1=6	O
,	O
we	O
have	O
p	O
(	O
sj	O
f	O
;	O
h0	O
)	O
=	O
pfa	O
0	O
(	O
1	O
(	O
cid:0	O
)	O
p0	O
)	O
fb	O
:	O
thus	O
the	O
posterior	B
probability	I
ratio	O
of	O
model	O
h1	O
to	O
model	B
h0	O
is	O
p	O
(	O
h1	O
j	O
s	O
;	O
f	O
)	O
p	O
(	O
h0	O
j	O
s	O
;	O
f	O
)	O
=	O
=	O
p	O
(	O
sj	O
f	O
;	O
h1	O
)	O
p	O
(	O
h1	O
)	O
p	O
(	O
sj	O
f	O
;	O
h0	O
)	O
p	O
(	O
h0	O
)	O
(	O
fa	O
+	O
fb	O
+	O
1	O
)	O
!	O
(	O
cid:30	O
)	O
pfa	O
fa	O
!	O
fb	O
!	O
0	O
(	O
1	O
(	O
cid:0	O
)	O
p0	O
)	O
fb	O
:	O
(	O
3.20	O
)	O
(	O
3.21	O
)	O
(	O
3.22	O
)	O
some	O
values	O
of	O
this	O
posterior	B
probability	I
ratio	O
are	O
illustrated	O
in	O
table	O
3.5.	O
the	O
(	O
cid:12	O
)	O
rst	O
(	O
cid:12	O
)	O
ve	O
lines	O
illustrate	O
that	O
some	O
outcomes	O
favour	O
one	O
model	B
,	O
and	O
some	O
favour	O
the	O
other	O
.	O
no	O
outcome	O
is	O
completely	O
incompatible	O
with	O
either	O
model	B
.	O
with	O
small	O
amounts	O
of	O
data	O
(	O
six	B
tosses	O
,	O
say	O
)	O
it	O
is	O
typically	O
not	O
the	O
case	O
that	O
one	O
of	O
the	O
two	O
models	O
is	O
overwhelmingly	O
more	O
probable	O
than	O
the	O
other	O
.	O
but	O
with	O
more	O
data	O
,	O
the	O
evidence	B
against	O
h0	O
given	O
by	O
any	O
data	B
set	I
with	O
the	O
ratio	O
fa	O
:	O
fb	O
di	O
(	O
cid:11	O
)	O
ering	O
from	O
1	O
:	O
5	O
mounts	O
up	O
.	O
you	O
can	O
’	O
t	O
predict	O
in	O
advance	O
how	O
much	O
data	O
are	O
needed	O
to	O
be	O
pretty	O
sure	O
which	O
theory	B
is	O
true	O
.	O
it	O
depends	O
what	O
p	O
a	O
is	O
.	O
the	O
simpler	O
model	B
,	O
h0	O
,	O
since	O
it	O
has	O
no	O
adjustable	O
parameters	B
,	O
is	O
able	O
to	O
lose	O
out	O
by	O
the	O
biggest	O
margin	O
.	O
the	O
odds	B
may	O
be	O
hundreds	O
to	O
one	O
against	O
it	O
.	O
the	O
more	O
complex	B
model	O
can	O
never	O
lose	O
out	O
by	O
a	O
large	O
margin	O
;	O
there	O
’	O
s	O
no	O
data	B
set	I
that	O
is	O
actually	O
unlikely	O
given	O
model	B
h1	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
3	O
|	O
more	O
about	O
inference	B
table	O
3.5.	O
outcome	O
of	O
model	O
comparison	O
between	O
models	O
h1	O
and	O
h0	O
for	O
the	O
‘	O
bent	B
coin	I
’	O
.	O
model	B
h0	O
states	O
that	O
pa	O
=	O
1=6	O
,	O
pb	O
=	O
5=6	O
.	O
figure	O
3.6.	O
typical	B
behaviour	I
of	I
the	O
evidence	B
in	O
favour	O
of	O
h1	O
as	O
bent	O
coin	B
tosses	O
accumulate	O
under	O
three	O
di	O
(	O
cid:11	O
)	O
erent	O
conditions	O
(	O
columns	O
1	O
,	O
2	O
,	O
3	O
)	O
.	O
horizontal	O
axis	O
is	O
the	O
number	O
of	O
tosses	O
,	O
f	O
.	O
the	O
vertical	O
axis	O
on	O
the	O
left	O
is	O
ln	O
p	O
(	O
s	O
j	O
f	O
;	O
h1	O
)	O
p	O
(	O
s	O
j	O
f	O
;	O
h0	O
)	O
vertical	O
axis	O
shows	O
the	O
values	O
of	O
p	O
(	O
s	O
j	O
f	O
;	O
h1	O
)	O
p	O
(	O
s	O
j	O
f	O
;	O
h0	O
)	O
the	O
three	O
rows	O
show	O
independent	O
simulated	O
experiments	O
.	O
(	O
see	O
also	O
(	O
cid:12	O
)	O
gure	O
3.8	O
,	O
p.60	O
.	O
)	O
;	O
the	O
right-hand	O
.	O
54	O
f	O
data	O
(	O
fa	O
;	O
fb	O
)	O
6	O
6	O
6	O
6	O
6	O
20	O
20	O
20	O
(	O
5	O
;	O
1	O
)	O
(	O
3	O
;	O
3	O
)	O
(	O
2	O
;	O
4	O
)	O
(	O
1	O
;	O
5	O
)	O
(	O
0	O
;	O
6	O
)	O
(	O
10	O
;	O
10	O
)	O
(	O
3	O
;	O
17	O
)	O
(	O
0	O
;	O
20	O
)	O
p	O
(	O
h1	O
j	O
s	O
;	O
f	O
)	O
p	O
(	O
h0	O
j	O
s	O
;	O
f	O
)	O
222.2	O
2.67	O
0.71	O
0.356	O
0.427	O
96.5	O
0.2	O
1.83	O
=	O
1/1.4	O
=	O
1/2.8	O
=	O
1/2.3	O
=	O
1/5	O
h0	O
is	O
true	O
pa	O
=	O
1=6	O
1000/1	O
100/1	O
10/1	O
1/1	O
1/10	O
1/100	O
100	O
150	O
200	O
1000/1	O
100/1	O
10/1	O
1/1	O
1/10	O
1/100	O
100	O
150	O
200	O
1000/1	O
100/1	O
10/1	O
1/1	O
1/10	O
1/100	O
100	O
150	O
200	O
8	O
6	O
4	O
2	O
0	O
-2	O
-4	O
8	O
6	O
4	O
2	O
0	O
-2	O
-4	O
8	O
6	O
4	O
2	O
0	O
-2	O
-4	O
0	O
50	O
0	O
50	O
0	O
50	O
h1	O
is	O
true	O
8	O
6	O
4	O
2	O
0	O
-2	O
-4	O
8	O
6	O
4	O
2	O
0	O
-2	O
-4	O
8	O
6	O
4	O
2	O
0	O
-2	O
-4	O
0	O
50	O
0	O
50	O
0	O
50	O
pa	O
=	O
0:25	O
1000/1	O
100/1	O
10/1	O
1/1	O
1/10	O
1/100	O
100	O
150	O
200	O
1000/1	O
100/1	O
10/1	O
1/1	O
1/10	O
1/100	O
100	O
150	O
200	O
1000/1	O
100/1	O
10/1	O
1/1	O
1/10	O
1/100	O
100	O
150	O
200	O
8	O
6	O
4	O
2	O
0	O
-2	O
-4	O
8	O
6	O
4	O
2	O
0	O
-2	O
-4	O
8	O
6	O
4	O
2	O
0	O
-2	O
-4	O
0	O
50	O
0	O
50	O
0	O
50	O
pa	O
=	O
0:5	O
1000/1	O
100/1	O
10/1	O
1/1	O
1/10	O
1/100	O
100	O
150	O
200	O
1000/1	O
100/1	O
10/1	O
1/1	O
1/10	O
1/100	O
100	O
150	O
200	O
1000/1	O
100/1	O
10/1	O
1/1	O
1/10	O
1/100	O
100	O
150	O
200	O
.	O
exercise	O
3.6	O
.	O
[	O
2	O
]	O
show	O
that	O
after	O
f	O
tosses	O
have	O
taken	O
place	O
,	O
the	O
biggest	O
value	O
that	O
the	O
log	O
evidence	B
ratio	O
log	O
p	O
(	O
sj	O
f	O
;	O
h1	O
)	O
p	O
(	O
sj	O
f	O
;	O
h0	O
)	O
(	O
3.23	O
)	O
can	O
have	O
scales	O
linearly	O
with	O
f	O
if	O
h1	O
is	O
more	O
probable	O
,	O
but	O
the	O
log	O
evidence	B
in	O
favour	O
of	O
h0	O
can	O
grow	O
at	O
most	O
as	O
log	O
f	O
.	O
.	O
exercise	O
3.7	O
.	O
[	O
3	O
,	O
p.60	O
]	O
putting	O
your	O
sampling	B
theory	I
hat	O
on	O
,	O
assuming	O
fa	O
has	O
not	O
yet	O
been	O
measured	O
,	O
compute	O
a	O
plausible	O
range	O
that	O
the	O
log	O
evidence	B
ratio	O
might	O
lie	O
in	O
,	O
as	O
a	O
function	B
of	O
f	O
and	O
the	O
true	O
value	O
of	O
p	O
a	O
,	O
and	O
sketch	O
it	O
as	O
a	O
function	B
of	O
f	O
for	O
pa	O
=	O
p0	O
=	O
1=6	O
,	O
pa	O
=	O
0:25	O
,	O
and	O
pa	O
=	O
1=2	O
.	O
[	O
hint	O
:	O
sketch	O
the	O
log	O
evidence	B
as	O
a	O
function	B
of	O
the	O
random	B
variable	I
f	O
a	O
and	O
work	O
out	O
the	O
mean	B
and	O
standard	B
deviation	I
of	O
fa	O
.	O
]	O
typical	B
behaviour	I
of	I
the	O
evidence	B
figure	O
3.6	O
shows	O
the	O
log	O
evidence	B
ratio	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
tosses	O
,	O
f	O
,	O
in	O
a	O
number	O
of	O
simulated	O
experiments	O
.	O
in	O
the	O
left-hand	O
experiments	O
,	O
h0	O
was	O
true	O
.	O
in	O
the	O
right-hand	O
ones	O
,	O
h1	O
was	O
true	O
,	O
and	O
the	O
value	O
of	O
pa	O
was	O
either	O
0.25	O
or	O
0.5.	O
we	O
will	O
discuss	O
model	B
comparison	I
more	O
in	O
a	O
later	O
chapter	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
55	O
3.4	O
:	O
an	O
example	O
of	O
legal	O
evidence	B
3.4	O
an	O
example	O
of	O
legal	O
evidence	B
the	O
following	O
example	O
illustrates	O
that	O
there	O
is	O
more	O
to	O
bayesian	O
inference	B
than	O
the	O
priors	O
.	O
two	O
people	O
have	O
left	O
traces	O
of	O
their	O
own	O
blood	O
at	O
the	O
scene	O
of	O
a	O
crime	O
.	O
a	O
suspect	O
,	O
oliver	O
,	O
is	O
tested	O
and	O
found	O
to	O
have	O
type	O
‘	O
o	O
’	O
blood	O
.	O
the	O
blood	O
groups	O
of	O
the	O
two	O
traces	O
are	O
found	O
to	O
be	O
of	O
type	O
‘	O
o	O
’	O
(	O
a	O
common	O
type	O
in	O
the	O
local	O
population	O
,	O
having	O
frequency	B
60	O
%	O
)	O
and	O
of	O
type	O
‘	O
ab	O
’	O
(	O
a	O
rare	O
type	O
,	O
with	O
frequency	O
1	O
%	O
)	O
.	O
do	O
these	O
data	O
(	O
type	O
‘	O
o	O
’	O
and	O
‘	O
ab	O
’	O
blood	O
were	O
found	O
at	O
scene	O
)	O
give	O
evidence	B
in	O
favour	O
of	O
the	O
proposition	O
that	O
oliver	O
was	O
one	O
of	O
the	O
two	O
people	O
present	O
at	O
the	O
crime	O
?	O
a	O
careless	O
lawyer	B
might	O
claim	O
that	O
the	O
fact	O
that	O
the	O
suspect	O
’	O
s	O
blood	O
type	O
was	O
found	O
at	O
the	O
scene	O
is	O
positive	O
evidence	O
for	O
the	O
theory	B
that	O
he	O
was	O
present	O
.	O
but	O
this	O
is	O
not	O
so	O
.	O
denote	O
the	O
proposition	O
‘	O
the	O
suspect	O
and	O
one	O
unknown	O
person	O
were	O
present	O
’	O
by	O
s.	O
the	O
alternative	O
,	O
(	O
cid:22	O
)	O
s	O
,	O
states	O
‘	O
two	O
unknown	O
people	O
from	O
the	O
population	O
were	O
present	O
’	O
.	O
the	O
prior	B
in	O
this	O
problem	O
is	O
the	O
prior	B
probability	O
ratio	O
between	O
the	O
propositions	O
s	O
and	O
(	O
cid:22	O
)	O
s.	O
this	O
quantity	O
is	O
important	O
to	O
the	O
(	O
cid:12	O
)	O
nal	O
verdict	O
and	O
would	O
be	O
based	O
on	O
all	O
other	O
available	O
information	B
in	O
the	O
case	O
.	O
our	O
task	O
here	O
is	O
just	O
to	O
evaluate	O
the	O
contribution	O
made	O
by	O
the	O
data	O
d	O
,	O
that	O
is	O
,	O
the	O
likelihood	B
ratio	O
,	O
p	O
(	O
d	O
j	O
s	O
;	O
h	O
)	O
=p	O
(	O
d	O
j	O
(	O
cid:22	O
)	O
s	O
;	O
h	O
)	O
.	O
in	O
my	O
view	O
,	O
a	O
jury	B
’	O
s	O
task	O
should	O
generally	O
be	O
to	O
multiply	O
together	O
carefully	O
evaluated	O
likelihood	B
ratios	O
from	O
each	O
independent	O
piece	O
of	O
admissible	O
evidence	B
with	O
an	O
equally	O
carefully	O
reasoned	O
prior	B
proba-	O
bility	O
.	O
[	O
this	O
view	O
is	O
shared	O
by	O
many	O
statisticians	O
but	O
learned	O
british	O
appeal	O
judges	O
recently	O
disagreed	O
and	O
actually	O
overturned	O
the	O
verdict	O
of	O
a	O
trial	O
because	O
the	O
jurors	O
had	O
been	O
taught	O
to	O
use	O
bayes	O
’	O
theorem	B
to	O
handle	O
complicated	O
dna	O
evidence	B
.	O
]	O
the	O
probability	O
of	O
the	O
data	O
given	O
s	O
is	O
the	O
probability	B
that	O
one	O
unknown	O
person	O
drawn	O
from	O
the	O
population	O
has	O
blood	O
type	O
ab	O
:	O
p	O
(	O
d	O
j	O
s	O
;	O
h	O
)	O
=	O
pab	O
(	O
3.24	O
)	O
(	O
since	O
given	O
s	O
,	O
we	O
already	O
know	O
that	O
one	O
trace	O
will	O
be	O
of	O
type	O
o	O
)	O
.	O
the	O
prob-	O
ability	O
of	O
the	O
data	O
given	O
(	O
cid:22	O
)	O
s	O
is	O
the	O
probability	B
that	O
two	O
unknown	O
people	O
drawn	O
from	O
the	O
population	O
have	O
types	O
o	O
and	O
ab	O
:	O
p	O
(	O
d	O
j	O
(	O
cid:22	O
)	O
s	O
;	O
h	O
)	O
=	O
2	O
po	O
pab	O
:	O
(	O
3.25	O
)	O
in	O
these	O
equations	O
h	O
denotes	O
the	O
assumptions	B
that	O
two	O
people	O
were	O
present	O
and	O
left	O
blood	O
there	O
,	O
and	O
that	O
the	O
probability	B
distribution	O
of	O
the	O
blood	O
groups	O
of	O
unknown	O
people	O
in	O
an	O
explanation	O
is	O
the	O
same	O
as	O
the	O
population	O
frequencies	O
.	O
dividing	O
,	O
we	O
obtain	O
the	O
likelihood	B
ratio	O
:	O
p	O
(	O
d	O
j	O
s	O
;	O
h	O
)	O
p	O
(	O
d	O
j	O
(	O
cid:22	O
)	O
s	O
;	O
h	O
)	O
=	O
1	O
2po	O
=	O
1	O
2	O
(	O
cid:2	O
)	O
0:6	O
=	O
0:83	O
:	O
(	O
3.26	O
)	O
thus	O
the	O
data	O
in	O
fact	O
provide	O
weak	O
evidence	B
against	O
the	O
supposition	O
that	O
oliver	O
was	O
present	O
.	O
this	O
result	O
may	O
be	O
found	O
surprising	O
,	O
so	O
let	O
us	O
examine	O
it	O
from	O
various	O
points	O
of	O
view	O
.	O
first	O
consider	O
the	O
case	O
of	O
another	O
suspect	O
,	O
alberto	O
,	O
who	O
has	O
type	O
ab	O
.	O
intuitively	O
,	O
the	O
data	O
do	O
provide	O
evidence	B
in	O
favour	O
of	O
the	O
theory	O
s0	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
56	O
3	O
|	O
more	O
about	O
inference	B
that	O
this	O
suspect	O
was	O
present	O
,	O
relative	B
to	O
the	O
null	O
hypothesis	O
(	O
cid:22	O
)	O
s.	O
and	O
indeed	O
the	O
likelihood	B
ratio	O
in	O
this	O
case	O
is	O
:	O
p	O
(	O
d	O
j	O
s0	O
;	O
h	O
)	O
p	O
(	O
d	O
j	O
(	O
cid:22	O
)	O
s	O
;	O
h	O
)	O
=	O
1	O
2	O
pab	O
=	O
50	O
:	O
(	O
3.27	O
)	O
now	O
let	O
us	O
change	O
the	O
situation	O
slightly	O
;	O
imagine	O
that	O
99	O
%	O
of	O
people	O
are	O
of	O
blood	O
type	O
o	O
,	O
and	O
the	O
rest	O
are	O
of	O
type	O
ab	O
.	O
only	O
these	O
two	O
blood	O
types	O
exist	O
in	O
the	O
population	O
.	O
the	O
data	O
at	O
the	O
scene	O
are	O
the	O
same	O
as	O
before	O
.	O
consider	O
again	O
how	O
these	O
data	O
in	O
(	O
cid:13	O
)	O
uence	O
our	O
beliefs	O
about	O
oliver	O
,	O
a	O
suspect	O
of	O
type	O
o	O
,	O
and	O
alberto	O
,	O
a	O
suspect	O
of	O
type	O
ab	O
.	O
intuitively	O
,	O
we	O
still	O
believe	O
that	O
the	O
presence	O
of	O
the	O
rare	O
ab	O
blood	O
provides	O
positive	O
evidence	O
that	O
alberto	O
was	O
there	O
.	O
but	O
does	O
the	O
fact	O
that	O
type	O
o	O
blood	O
was	O
detected	O
at	O
the	O
scene	O
favour	O
the	O
hypothesis	O
that	O
oliver	O
was	O
present	O
?	O
if	O
this	O
were	O
the	O
case	O
,	O
that	O
would	O
mean	B
that	O
regardless	O
of	O
who	O
the	O
suspect	O
is	O
,	O
the	O
data	O
make	O
it	O
more	O
probable	O
they	O
were	O
present	O
;	O
everyone	O
in	O
the	O
population	O
would	O
be	O
under	O
greater	O
suspicion	O
,	O
which	O
would	O
be	O
absurd	O
.	O
the	O
data	O
may	O
be	O
compatible	O
with	O
any	O
suspect	O
of	O
either	O
blood	O
type	O
being	O
present	O
,	O
but	O
if	O
they	O
provide	O
evidence	B
for	O
some	O
theories	O
,	O
they	O
must	O
also	O
provide	O
evidence	B
against	O
other	O
theories	O
.	O
here	O
is	O
another	O
way	O
of	O
thinking	O
about	O
this	O
:	O
imagine	O
that	O
instead	O
of	O
two	O
people	O
’	O
s	O
blood	O
stains	O
there	O
are	O
ten	O
,	O
and	O
that	O
in	O
the	O
entire	O
local	O
population	O
of	O
one	O
hundred	O
,	O
there	O
are	O
ninety	O
type	O
o	O
suspects	O
and	O
ten	O
type	O
ab	O
suspects	O
.	O
consider	O
a	O
particular	O
type	O
o	O
suspect	O
,	O
oliver	O
:	O
without	O
any	O
other	O
information	B
,	O
and	O
before	O
the	O
blood	O
test	O
results	O
come	O
in	O
,	O
there	O
is	O
a	O
one	O
in	O
10	O
chance	O
that	O
he	O
was	O
at	O
the	O
scene	O
,	O
since	O
we	O
know	O
that	O
10	O
out	O
of	O
the	O
100	O
suspects	O
were	O
present	O
.	O
we	O
now	O
get	O
the	O
results	O
of	O
blood	O
tests	O
,	O
and	O
(	O
cid:12	O
)	O
nd	O
that	O
nine	O
of	O
the	O
ten	O
stains	O
are	O
of	O
type	O
ab	O
,	O
and	O
one	O
of	O
the	O
stains	O
is	O
of	O
type	O
o.	O
does	O
this	O
make	O
it	O
more	O
likely	O
that	O
oliver	O
was	O
there	O
?	O
no	O
,	O
there	O
is	O
now	O
only	O
a	O
one	O
in	O
ninety	O
chance	O
that	O
he	O
was	O
there	O
,	O
since	O
we	O
know	O
that	O
only	O
one	O
person	O
present	O
was	O
of	O
type	O
o.	O
maybe	O
the	O
intuition	O
is	O
aided	O
(	O
cid:12	O
)	O
nally	O
by	O
writing	O
down	O
the	O
formulae	O
for	O
the	O
general	O
case	O
where	O
no	O
blood	O
stains	O
of	O
individuals	O
of	O
type	O
o	O
are	O
found	O
,	O
and	O
nab	O
of	O
type	O
ab	O
,	O
a	O
total	O
of	O
n	O
individuals	O
in	O
all	O
,	O
and	O
unknown	O
people	O
come	O
from	O
a	O
large	O
population	O
with	O
fractions	O
po	O
;	O
pab	O
.	O
(	O
there	O
may	O
be	O
other	O
blood	O
types	O
too	O
.	O
)	O
the	O
task	O
is	O
to	O
evaluate	O
the	O
likelihood	B
ratio	O
for	O
the	O
two	O
hypotheses	O
:	O
s	O
,	O
‘	O
the	O
type	O
o	O
suspect	O
(	O
oliver	O
)	O
and	O
n	O
(	O
cid:0	O
)	O
1	O
unknown	O
others	O
left	O
n	O
stains	O
’	O
;	O
and	O
(	O
cid:22	O
)	O
s	O
,	O
‘	O
n	O
unknowns	O
left	O
n	O
stains	O
’	O
.	O
the	O
probability	O
of	O
the	O
data	O
under	O
hypothesis	O
(	O
cid:22	O
)	O
s	O
is	O
just	O
the	O
probability	O
of	O
getting	O
no	O
;	O
nab	O
individuals	O
of	O
the	O
two	O
types	O
when	O
n	O
individuals	O
are	O
drawn	O
at	O
random	B
from	O
the	O
population	O
:	O
p	O
(	O
no	O
;	O
nab	O
j	O
(	O
cid:22	O
)	O
s	O
)	O
=	O
n	O
!	O
no	O
!	O
nab	O
!	O
o	O
pnab	O
pno	O
ab	O
:	O
(	O
3.28	O
)	O
in	O
the	O
case	O
of	O
hypothesis	O
s	O
,	O
we	O
need	O
the	O
distribution	B
of	O
the	O
n	O
(	O
cid:0	O
)	O
1	O
other	O
indi-	O
viduals	O
:	O
pno	O
(	O
cid:0	O
)	O
1	O
o	O
pnab	O
ab	O
:	O
(	O
3.29	O
)	O
p	O
(	O
no	O
;	O
nab	O
j	O
s	O
)	O
=	O
the	O
likelihood	B
ratio	O
is	O
:	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
!	O
(	O
no	O
(	O
cid:0	O
)	O
1	O
)	O
!	O
nab	O
!	O
p	O
(	O
no	O
;	O
nab	O
j	O
s	O
)	O
p	O
(	O
no	O
;	O
nab	O
j	O
(	O
cid:22	O
)	O
s	O
)	O
=	O
no=n	O
po	O
:	O
(	O
3.30	O
)	O
this	O
is	O
an	O
instructive	O
result	O
.	O
the	O
likelihood	B
ratio	O
,	O
i.e	O
.	O
the	O
contribution	O
of	O
these	O
data	O
to	O
the	O
question	O
of	O
whether	O
oliver	O
was	O
present	O
,	O
depends	O
simply	O
on	O
a	O
comparison	O
of	O
the	O
frequency	O
of	O
his	O
blood	O
type	O
in	O
the	O
observed	O
data	O
with	O
the	O
background	O
frequency	O
in	O
the	O
population	O
.	O
there	O
is	O
no	O
dependence	O
on	O
the	O
counts	O
of	O
the	O
other	O
types	O
found	O
at	O
the	O
scene	O
,	O
or	O
their	O
frequencies	O
in	O
the	O
population	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
3.5	O
:	O
exercises	O
57	O
if	O
there	O
are	O
more	O
type	O
o	O
stains	O
than	O
the	O
average	B
number	O
expected	O
under	O
hypothesis	O
(	O
cid:22	O
)	O
s	O
,	O
then	O
the	O
data	O
give	O
evidence	B
in	O
favour	O
of	O
the	O
presence	O
of	O
oliver	O
.	O
conversely	O
,	O
if	O
there	O
are	O
fewer	O
type	O
o	O
stains	O
than	O
the	O
expected	O
number	O
under	O
(	O
cid:22	O
)	O
s	O
,	O
then	O
the	O
data	O
reduce	O
the	O
probability	O
of	O
the	O
hypothesis	O
that	O
he	O
was	O
there	O
.	O
in	O
the	O
special	O
case	O
no=n	O
=	O
po	O
,	O
the	O
data	O
contribute	O
no	O
evidence	B
either	O
way	O
,	O
regardless	O
of	O
the	O
fact	O
that	O
the	O
data	O
are	O
compatible	O
with	O
the	O
hypothesis	O
s.	O
3.5	O
exercises	O
exercise	O
3.8	O
.	O
[	O
2	O
,	O
p.60	O
]	O
the	O
three	B
doors	I
,	O
normal	B
rules	O
.	O
on	O
a	O
game	B
show	I
,	O
a	O
contestant	O
is	O
told	O
the	O
rules	B
as	O
follows	O
:	O
there	O
are	O
three	B
doors	I
,	O
labelled	O
1	O
,	O
2	O
,	O
3.	O
a	O
single	O
prize	O
has	O
been	O
hidden	O
behind	O
one	O
of	O
them	O
.	O
you	O
get	O
to	O
select	O
one	O
door	O
.	O
initially	O
your	O
chosen	O
door	O
will	O
not	O
be	O
opened	O
.	O
instead	O
,	O
the	O
gameshow	O
host	O
will	O
open	O
one	O
of	O
the	O
other	O
two	O
doors	B
,	O
and	O
he	O
will	O
do	O
so	O
in	O
such	O
a	O
way	O
as	O
not	O
to	O
reveal	O
the	O
prize	B
.	O
for	O
example	O
,	O
if	O
you	O
(	O
cid:12	O
)	O
rst	O
choose	O
door	O
1	O
,	O
he	O
will	O
then	O
open	O
one	O
of	O
doors	O
2	O
and	O
3	O
,	O
and	O
it	O
is	O
guaranteed	O
that	O
he	O
will	O
choose	O
which	O
one	O
to	O
open	O
so	O
that	O
the	O
prize	B
will	O
not	O
be	O
revealed	O
.	O
at	O
this	O
point	O
,	O
you	O
will	O
be	O
given	O
a	O
fresh	O
choice	O
of	O
door	O
:	O
you	O
can	O
either	O
stick	O
with	O
your	O
(	O
cid:12	O
)	O
rst	O
choice	O
,	O
or	O
you	O
can	O
switch	O
to	O
the	O
other	O
closed	O
door	O
.	O
all	O
the	O
doors	B
will	O
then	O
be	O
opened	O
and	O
you	O
will	O
receive	O
whatever	O
is	O
behind	O
your	O
(	O
cid:12	O
)	O
nal	O
choice	O
of	O
door	O
.	O
imagine	O
that	O
the	O
contestant	O
chooses	O
door	O
1	O
(	O
cid:12	O
)	O
rst	O
;	O
then	O
the	O
gameshow	O
host	O
opens	O
door	O
3	O
,	O
revealing	O
nothing	O
behind	O
the	O
door	O
,	O
as	O
promised	O
.	O
should	O
the	O
contestant	O
(	O
a	O
)	O
stick	O
with	O
door	O
1	O
,	O
or	O
(	O
b	O
)	O
switch	O
to	O
door	O
2	O
,	O
or	O
(	O
c	O
)	O
does	O
it	O
make	O
no	O
di	O
(	O
cid:11	O
)	O
erence	O
?	O
exercise	O
3.9	O
.	O
[	O
2	O
,	O
p.61	O
]	O
the	O
three	B
doors	I
,	O
earthquake	B
scenario	O
.	O
imagine	O
that	O
the	O
game	B
happens	O
again	O
and	O
just	O
as	O
the	O
gameshow	O
host	O
is	O
about	O
to	O
open	O
one	O
of	O
the	O
doors	O
a	O
violent	O
earthquake	B
rattles	O
the	O
building	O
and	O
one	O
of	O
the	O
three	O
doors	B
(	O
cid:13	O
)	O
ies	O
open	O
.	O
it	O
happens	O
to	O
be	O
door	O
3	O
,	O
and	O
it	O
happens	O
not	O
to	O
have	O
the	O
prize	B
behind	O
it	O
.	O
the	O
contestant	O
had	O
initially	O
chosen	O
door	O
1.	O
repositioning	O
his	O
toup	O
(	O
cid:19	O
)	O
ee	O
,	O
the	O
host	O
suggests	O
,	O
‘	O
ok	O
,	O
since	O
you	O
chose	O
door	O
1	O
initially	O
,	O
door	O
3	O
is	O
a	O
valid	O
door	O
for	O
me	O
to	O
open	O
,	O
according	O
to	O
the	O
rules	B
of	O
the	O
game	B
;	O
i	O
’	O
ll	O
let	O
door	O
3	O
stay	O
open	O
.	O
let	O
’	O
s	O
carry	O
on	O
as	O
if	O
nothing	O
happened.	O
’	O
should	O
the	O
contestant	O
stick	O
with	O
door	O
1	O
,	O
or	O
switch	O
to	O
door	O
2	O
,	O
or	O
does	O
it	O
make	O
no	O
di	O
(	O
cid:11	O
)	O
erence	O
?	O
assume	O
that	O
the	O
prize	B
was	O
placed	O
randomly	O
,	O
that	O
the	O
gameshow	O
host	O
does	O
not	O
know	O
where	O
it	O
is	O
,	O
and	O
that	O
the	O
door	O
(	O
cid:13	O
)	O
ew	O
open	O
because	O
its	O
latch	O
was	O
broken	O
by	O
the	O
earthquake	B
.	O
[	O
a	O
similar	O
alternative	O
scenario	O
is	O
a	O
gameshow	O
whose	O
confused	O
host	O
for-	O
gets	O
the	O
rules	B
,	O
and	O
where	O
the	O
prize	B
is	O
,	O
and	O
opens	O
one	O
of	O
the	O
unchosen	O
doors	B
at	O
random	B
.	O
he	O
opens	O
door	O
3	O
,	O
and	O
the	O
prize	B
is	O
not	O
revealed	O
.	O
should	O
the	O
contestant	O
choose	O
what	O
’	O
s	O
behind	O
door	O
1	O
or	O
door	O
2	O
?	O
does	O
the	O
opti-	O
mal	O
decision	O
for	O
the	O
contestant	O
depend	O
on	O
the	O
contestant	O
’	O
s	O
beliefs	O
about	O
whether	O
the	O
gameshow	O
host	O
is	O
confused	O
or	O
not	O
?	O
]	O
.	O
exercise	O
3.10	O
.	O
[	O
2	O
]	O
another	O
example	O
in	O
which	O
the	O
emphasis	O
is	O
not	O
on	O
priors	O
.	O
you	O
visit	O
a	O
family	O
whose	O
three	O
children	O
are	O
all	O
at	O
the	O
local	O
school	O
.	O
you	O
don	O
’	O
t	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
58	O
3	O
|	O
more	O
about	O
inference	B
know	O
anything	O
about	O
the	O
sexes	O
of	O
the	O
children	O
.	O
while	O
walking	O
clum-	O
sily	O
round	O
the	O
home	O
,	O
you	O
stumble	O
through	O
one	O
of	O
the	O
three	O
unlabelled	O
bedroom	O
doors	B
that	O
you	O
know	O
belong	O
,	O
one	O
each	O
,	O
to	O
the	O
three	O
children	O
,	O
and	O
(	O
cid:12	O
)	O
nd	O
that	O
the	O
bedroom	O
contains	O
girlie	O
stu	O
(	O
cid:11	O
)	O
in	O
su	O
(	O
cid:14	O
)	O
cient	O
quantities	O
to	O
convince	O
you	O
that	O
the	O
child	O
who	O
lives	O
in	O
that	O
bedroom	O
is	O
a	O
girl	O
.	O
later	O
,	O
you	O
sneak	O
a	O
look	O
at	O
a	O
letter	O
addressed	O
to	O
the	O
parents	O
,	O
which	O
reads	O
‘	O
from	O
the	O
headmaster	O
:	O
we	O
are	O
sending	O
this	O
letter	O
to	O
all	O
parents	O
who	O
have	O
male	B
children	O
at	O
the	O
school	O
to	O
inform	O
them	O
about	O
the	O
following	O
boyish	O
mat-	O
ters	O
.	O
.	O
.	O
’	O
.	O
these	O
two	O
sources	O
of	O
evidence	O
establish	O
that	O
at	O
least	O
one	O
of	O
the	O
three	O
children	O
is	O
a	O
girl	O
,	O
and	O
that	O
at	O
least	O
one	O
of	O
the	O
children	O
is	O
a	O
boy	O
.	O
what	O
are	O
the	O
probabilities	O
that	O
there	O
are	O
(	O
a	O
)	O
two	O
girls	O
and	O
one	O
boy	O
;	O
(	O
b	O
)	O
two	O
boys	O
and	O
one	O
girl	O
?	O
.	O
exercise	O
3.11	O
.	O
[	O
2	O
,	O
p.61	O
]	O
mrs	O
s	O
is	O
found	O
stabbed	O
in	O
her	O
family	O
garden	O
.	O
mr	O
s	O
behaves	O
strangely	O
after	O
her	O
death	O
and	O
is	O
considered	O
as	O
a	O
suspect	O
.	O
on	O
investigation	O
of	O
police	O
and	O
social	O
records	O
it	O
is	O
found	O
that	O
mr	O
s	O
had	O
beaten	O
up	O
his	O
wife	O
on	O
at	O
least	O
nine	O
previous	O
occasions	O
.	O
the	O
prosecution	O
advances	O
this	O
data	O
as	O
evidence	B
in	O
favour	O
of	O
the	O
hypothesis	O
that	O
mr	O
s	O
is	O
guilty	O
of	O
the	O
murder	O
.	O
‘	O
ah	O
no	O
,	O
’	O
says	O
mr	O
s	O
’	O
s	O
highly	O
paid	O
lawyer	B
,	O
‘	O
statistically	O
,	O
only	O
one	O
in	O
a	O
thousand	O
wife-beaters	O
actually	O
goes	O
on	O
to	O
murder	B
his	O
wife.1	O
so	O
the	O
wife-beating	O
is	O
not	O
strong	O
evidence	B
at	O
all	O
.	O
in	O
fact	O
,	O
given	O
the	O
wife-beating	O
evidence	B
alone	O
,	O
it	O
’	O
s	O
extremely	O
unlikely	O
that	O
he	O
would	O
be	O
the	O
murderer	O
of	O
his	O
wife	O
{	O
only	O
a	O
1=1000	O
chance	O
.	O
you	O
should	O
therefore	O
(	O
cid:12	O
)	O
nd	O
him	O
innocent.	O
’	O
is	O
the	O
lawyer	B
right	O
to	O
imply	O
that	O
the	O
history	O
of	O
wife-beating	O
does	O
not	O
point	O
to	O
mr	O
s	O
’	O
s	O
being	O
the	O
murderer	O
?	O
or	O
is	O
the	O
lawyer	B
a	O
slimy	O
trickster	O
?	O
if	O
the	O
latter	O
,	O
what	O
is	O
wrong	O
with	O
his	O
argument	O
?	O
[	O
having	O
received	O
an	O
indignant	O
letter	O
from	O
a	O
lawyer	B
about	O
the	O
preceding	O
paragraph	O
,	O
i	O
’	O
d	O
like	O
to	O
add	O
an	O
extra	O
inference	O
exercise	O
at	O
this	O
point	O
:	O
does	O
my	O
suggestion	O
that	O
mr.	O
s.	O
’	O
s	O
lawyer	B
may	O
have	O
been	O
a	O
slimy	O
trickster	O
imply	O
that	O
i	O
believe	O
all	O
lawyers	O
are	O
slimy	O
tricksters	O
?	O
(	O
answer	O
:	O
no	O
.	O
)	O
]	O
.	O
exercise	O
3.12	O
.	O
[	O
2	O
]	O
a	O
bag	O
contains	O
one	O
counter	O
,	O
known	O
to	O
be	O
either	O
white	B
or	O
black	B
.	O
a	O
white	B
counter	O
is	O
put	O
in	O
,	O
the	O
bag	O
is	O
shaken	O
,	O
and	O
a	O
counter	O
is	O
drawn	O
out	O
,	O
which	O
proves	O
to	O
be	O
white	B
.	O
what	O
is	O
now	O
the	O
chance	O
of	O
drawing	O
a	O
white	B
counter	O
?	O
[	O
notice	O
that	O
the	O
state	O
of	O
the	O
bag	O
,	O
after	O
the	O
operations	O
,	O
is	O
exactly	O
identical	O
to	O
its	O
state	O
before	O
.	O
]	O
.	O
exercise	O
3.13	O
.	O
[	O
2	O
,	O
p.62	O
]	O
you	O
move	O
into	O
a	O
new	O
house	O
;	O
the	O
phone	B
is	O
connected	O
,	O
and	O
you	O
’	O
re	O
pretty	O
sure	O
that	O
the	O
phone	B
number	I
is	O
740511	O
,	O
but	O
not	O
as	O
sure	O
as	O
you	O
would	O
like	O
to	O
be	O
.	O
as	O
an	O
experiment	O
,	O
you	O
pick	O
up	O
the	O
phone	B
and	O
dial	O
740511	O
;	O
you	O
obtain	O
a	O
‘	O
busy	O
’	O
signal	O
.	O
are	O
you	O
now	O
more	O
sure	O
of	O
your	O
phone	B
number	I
?	O
if	O
so	O
,	O
how	O
much	O
?	O
.	O
exercise	O
3.14	O
.	O
[	O
1	O
]	O
in	O
a	O
game	B
,	O
two	O
coins	O
are	O
tossed	O
.	O
if	O
either	O
of	O
the	O
coins	O
comes	O
up	O
heads	O
,	O
you	O
have	O
won	O
a	O
prize	B
.	O
to	O
claim	O
the	O
prize	B
,	O
you	O
must	O
point	O
to	O
one	O
of	O
your	O
coins	O
that	O
is	O
a	O
head	O
and	O
say	O
‘	O
look	O
,	O
that	O
coin	B
’	O
s	O
a	O
head	O
,	O
i	O
’	O
ve	O
won	O
’	O
.	O
you	O
watch	O
fred	O
play	O
the	O
game	B
.	O
he	O
tosses	O
the	O
two	O
coins	O
,	O
and	O
he	O
1in	O
the	O
u.s.a.	O
,	O
it	O
is	O
estimated	O
that	O
2	O
million	O
women	O
are	O
abused	O
each	O
year	O
by	O
their	O
partners	O
.	O
in	O
1994	O
,	O
4739	O
women	O
were	O
victims	O
of	O
homicide	O
;	O
of	O
those	O
,	O
1326	O
women	O
(	O
28	O
%	O
)	O
were	O
slain	O
by	O
husbands	O
and	O
boyfriends	O
.	O
(	O
sources	O
:	O
http	O
:	O
//www.umn.edu/mincava/papers/factoid.htm	O
,	O
http	O
:	O
//www.gunfree.inter.net/vpc/womenfs.htm	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
3.6	O
:	O
solutions	O
59	O
points	O
to	O
a	O
coin	B
and	O
says	O
‘	O
look	O
,	O
that	O
coin	B
’	O
s	O
a	O
head	O
,	O
i	O
’	O
ve	O
won	O
’	O
.	O
what	O
is	O
the	O
probability	B
that	O
the	O
other	O
coin	B
is	O
a	O
head	O
?	O
.	O
exercise	O
3.15	O
.	O
[	O
2	O
,	O
p.63	O
]	O
a	O
statistical	B
statement	O
appeared	O
in	O
the	O
guardian	O
on	O
friday	O
january	O
4	O
,	O
2002	O
:	O
when	O
spun	O
on	O
edge	O
250	O
times	O
,	O
a	O
belgian	O
one-euro	O
coin	B
came	O
up	O
heads	O
140	O
times	O
and	O
tails	O
110	O
.	O
‘	O
it	O
looks	O
very	O
suspicious	O
to	O
me	O
’	O
,	O
said	O
barry	O
blight	O
,	O
a	O
statistics	O
lecturer	O
at	O
the	O
london	O
school	O
of	O
economics	O
.	O
‘	O
if	O
the	O
coin	B
were	O
unbiased	O
the	O
chance	O
of	O
getting	O
a	O
result	O
as	O
extreme	O
as	O
that	O
would	O
be	O
less	O
than	O
7	O
%	O
’	O
.	O
but	O
do	O
these	O
data	O
give	O
evidence	B
that	O
the	O
coin	B
is	O
biased	O
rather	O
than	O
fair	O
?	O
[	O
hint	O
:	O
see	O
equation	O
(	O
3.22	O
)	O
.	O
]	O
3.6	O
solutions	O
solution	O
to	O
exercise	O
3.1	O
(	O
p.47	O
)	O
.	O
probabilities	O
,	O
let	O
the	O
data	O
be	O
d.	O
assuming	O
equal	O
prior	B
p	O
(	O
aj	O
d	O
)	O
p	O
(	O
b	O
j	O
d	O
)	O
=	O
1	O
2	O
3	O
2	O
1	O
1	O
3	O
2	O
1	O
2	O
2	O
2	O
1	O
2	O
=	O
9	O
32	O
(	O
3.31	O
)	O
and	O
p	O
(	O
aj	O
d	O
)	O
=	O
9=41	O
:	O
solution	O
to	O
exercise	O
3.2	O
(	O
p.47	O
)	O
.	O
the	O
probability	O
of	O
the	O
data	O
given	O
each	O
hy-	O
pothesis	O
is	O
:	O
p	O
(	O
d	O
j	O
a	O
)	O
=	O
p	O
(	O
d	O
j	O
b	O
)	O
=	O
p	O
(	O
d	O
j	O
c	O
)	O
=	O
=	O
=	O
=	O
18	O
207	O
;	O
64	O
207	O
;	O
1	O
207	O
:	O
3	O
20	O
2	O
20	O
1	O
20	O
1	O
20	O
2	O
20	O
1	O
20	O
2	O
20	O
2	O
20	O
1	O
20	O
1	O
20	O
2	O
20	O
1	O
20	O
3	O
20	O
2	O
20	O
1	O
20	O
1	O
20	O
1	O
20	O
1	O
20	O
1	O
20	O
2	O
20	O
1	O
20	O
(	O
3.32	O
)	O
(	O
3.33	O
)	O
(	O
3.34	O
)	O
:	O
1	O
83	O
(	O
3.35	O
)	O
figure	O
3.7.	O
posterior	B
probability	I
for	O
the	O
bias	B
pa	O
of	O
a	O
bent	B
coin	I
given	O
two	O
di	O
(	O
cid:11	O
)	O
erent	O
data	O
sets	O
.	O
so	O
p	O
(	O
aj	O
d	O
)	O
=	O
18	O
18	O
+	O
64	O
+	O
1	O
=	O
18	O
83	O
;	O
p	O
(	O
b	O
j	O
d	O
)	O
=	O
64	O
83	O
;	O
p	O
(	O
c	O
j	O
d	O
)	O
=	O
(	O
a	O
)	O
0	O
0.2	O
0.4	O
0.6	O
p	O
(	O
pa	O
j	O
s	O
=	O
aba	O
;	O
f	O
=	O
3	O
)	O
/	O
p2	O
0.8	O
1	O
a	O
(	O
1	O
(	O
cid:0	O
)	O
pa	O
)	O
(	O
b	O
)	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
p	O
(	O
pa	O
j	O
s	O
=	O
bbb	O
;	O
f	O
=	O
3	O
)	O
/	O
(	O
1	O
(	O
cid:0	O
)	O
pa	O
)	O
3	O
solution	O
to	O
exercise	O
3.5	O
(	O
p.52	O
)	O
.	O
(	O
a	O
)	O
p	O
(	O
pa	O
j	O
s	O
=	O
aba	O
;	O
f	O
=	O
3	O
)	O
/	O
p2	O
a	O
(	O
1	O
(	O
cid:0	O
)	O
pa	O
)	O
.	O
the	O
most	O
probable	O
value	O
of	O
pa	O
(	O
i.e.	O
,	O
the	O
value	O
that	O
maximizes	O
the	O
posterior	B
probability	I
density	O
)	O
is	O
2=3	O
.	O
the	O
mean	B
value	O
of	O
pa	O
is	O
3=5	O
.	O
see	O
(	O
cid:12	O
)	O
gure	O
3.7a	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
60	O
3	O
|	O
more	O
about	O
inference	B
.	O
;	O
the	O
right-hand	O
figure	O
3.8.	O
range	O
of	O
plausible	O
values	O
of	O
the	O
log	O
evidence	B
in	O
favour	O
of	O
h1	O
as	O
a	O
function	B
of	O
f	O
.	O
the	O
vertical	O
axis	O
on	O
the	O
left	O
is	O
log	O
p	O
(	O
s	O
j	O
f	O
;	O
h1	O
)	O
p	O
(	O
s	O
j	O
f	O
;	O
h0	O
)	O
vertical	O
axis	O
shows	O
the	O
values	O
of	O
p	O
(	O
s	O
j	O
f	O
;	O
h1	O
)	O
p	O
(	O
s	O
j	O
f	O
;	O
h0	O
)	O
the	O
solid	O
line	O
shows	O
the	O
log	O
evidence	B
if	O
the	O
random	B
variable	I
fa	O
takes	O
on	O
its	O
mean	B
value	O
,	O
fa	O
=	O
paf	O
.	O
the	O
dotted	O
lines	O
show	O
(	O
approximately	O
)	O
the	O
log	O
evidence	B
if	O
fa	O
is	O
at	O
its	O
2.5th	O
or	O
97.5th	O
percentile	O
.	O
(	O
see	O
also	O
(	O
cid:12	O
)	O
gure	O
3.6	O
,	O
p.54	O
.	O
)	O
(	O
b	O
)	O
p	O
(	O
pa	O
j	O
s	O
=	O
bbb	O
;	O
f	O
=	O
3	O
)	O
/	O
(	O
1	O
(	O
cid:0	O
)	O
pa	O
)	O
3.	O
the	O
most	O
probable	O
value	O
of	O
pa	O
(	O
i.e.	O
,	O
the	O
value	O
that	O
maximizes	O
the	O
posterior	B
probability	I
density	O
)	O
is	O
0.	O
the	O
mean	B
value	O
of	O
pa	O
is	O
1=5	O
.	O
see	O
(	O
cid:12	O
)	O
gure	O
3.7b	O
.	O
h0	O
is	O
true	O
pa	O
=	O
1=6	O
1000/1	O
100/1	O
10/1	O
1/1	O
1/10	O
1/100	O
100	O
150	O
200	O
8	O
6	O
4	O
2	O
0	O
-2	O
-4	O
0	O
50	O
h1	O
is	O
true	O
8	O
6	O
4	O
2	O
0	O
-2	O
-4	O
0	O
50	O
pa	O
=	O
0:25	O
1000/1	O
100/1	O
10/1	O
1/1	O
1/10	O
1/100	O
100	O
150	O
200	O
8	O
6	O
4	O
2	O
0	O
-2	O
-4	O
0	O
50	O
pa	O
=	O
0:5	O
1000/1	O
100/1	O
10/1	O
1/1	O
1/10	O
1/100	O
100	O
150	O
200	O
solution	O
to	O
exercise	O
3.7	O
(	O
p.54	O
)	O
.	O
the	O
curves	O
in	O
(	O
cid:12	O
)	O
gure	O
3.8	O
were	O
found	O
by	O
(	O
cid:12	O
)	O
nding	O
the	O
mean	B
and	O
standard	B
deviation	I
of	O
fa	O
,	O
then	O
setting	O
fa	O
to	O
the	O
mean	B
(	O
cid:6	O
)	O
two	O
standard	O
deviations	O
to	O
get	O
a	O
95	O
%	O
plausible	O
range	O
for	O
fa	O
,	O
and	O
computing	O
the	O
three	O
corresponding	O
values	O
of	O
the	O
log	O
evidence	B
ratio	O
.	O
solution	O
to	O
exercise	O
3.8	O
(	O
p.57	O
)	O
.	O
let	O
hi	O
denote	O
the	O
hypothesis	O
that	O
the	O
prize	B
is	O
behind	O
door	O
i.	O
we	O
make	O
the	O
following	O
assumptions	B
:	O
the	O
three	O
hypotheses	O
h1	O
,	O
h2	O
and	O
h3	O
are	O
equiprobable	O
a	O
priori	O
,	O
i.e.	O
,	O
p	O
(	O
h1	O
)	O
=	O
p	O
(	O
h2	O
)	O
=	O
p	O
(	O
h3	O
)	O
=	O
1	O
3	O
:	O
(	O
3.36	O
)	O
the	O
datum	O
we	O
receive	O
,	O
after	O
choosing	O
door	O
1	O
,	O
is	O
one	O
of	O
d	O
=	O
3	O
and	O
d	O
=	O
2	O
(	O
mean-	O
ing	O
door	O
3	O
or	O
2	O
is	O
opened	O
,	O
respectively	O
)	O
.	O
we	O
assume	O
that	O
these	O
two	O
possible	O
outcomes	O
have	O
the	O
following	O
probabilities	O
.	O
if	O
the	O
prize	B
is	O
behind	O
door	O
1	O
then	O
the	O
host	O
has	O
a	O
free	O
choice	O
;	O
in	O
this	O
case	O
we	O
assume	O
that	O
the	O
host	O
selects	O
at	O
random	B
between	O
d	O
=	O
2	O
and	O
d	O
=	O
3.	O
otherwise	O
the	O
choice	O
of	O
the	O
host	O
is	O
forced	O
and	O
the	O
probabilities	O
are	O
0	O
and	O
1.	O
p	O
(	O
d	O
=	O
2jh1	O
)	O
=	O
1/2	O
p	O
(	O
d	O
=	O
2jh2	O
)	O
=	O
0	O
p	O
(	O
d	O
=	O
2jh3	O
)	O
=	O
1	O
p	O
(	O
d	O
=	O
3jh1	O
)	O
=	O
1/2	O
p	O
(	O
d	O
=	O
3jh2	O
)	O
=	O
1	O
p	O
(	O
d	O
=	O
3jh3	O
)	O
=	O
0	O
(	O
3.37	O
)	O
now	O
,	O
using	O
bayes	O
’	O
theorem	B
,	O
we	O
evaluate	O
the	O
posterior	O
probabilities	O
of	O
the	O
hypotheses	O
:	O
p	O
(	O
hi	O
j	O
d	O
=	O
3	O
)	O
=	O
p	O
(	O
d	O
=	O
3jhi	O
)	O
p	O
(	O
hi	O
)	O
p	O
(	O
d	O
=	O
3	O
)	O
(	O
3.38	O
)	O
p	O
(	O
h1	O
j	O
d	O
=	O
3	O
)	O
=	O
(	O
1=2	O
)	O
(	O
1=3	O
)	O
p	O
(	O
d=3	O
)	O
(	O
3.39	O
)	O
the	O
denominator	O
p	O
(	O
d	O
=	O
3	O
)	O
is	O
(	O
1=2	O
)	O
because	O
it	O
is	O
the	O
normalizing	B
constant	I
for	O
this	O
posterior	O
distribution	O
.	O
so	O
p	O
(	O
h2	O
j	O
d	O
=	O
3	O
)	O
=	O
(	O
1	O
)	O
(	O
1=3	O
)	O
p	O
(	O
d=3	O
)	O
p	O
(	O
h3	O
j	O
d	O
=	O
3	O
)	O
=	O
(	O
0	O
)	O
(	O
1=3	O
)	O
p	O
(	O
d=3	O
)	O
p	O
(	O
h1	O
j	O
d	O
=	O
3	O
)	O
=	O
1/3	O
p	O
(	O
h2	O
j	O
d	O
=	O
3	O
)	O
=	O
2/3	O
p	O
(	O
h3	O
j	O
d	O
=	O
3	O
)	O
=	O
0	O
:	O
(	O
3.40	O
)	O
so	O
the	O
contestant	O
should	O
switch	O
to	O
door	O
2	O
in	O
order	O
to	O
have	O
the	O
biggest	O
chance	O
of	O
getting	O
the	O
prize	B
.	O
many	O
people	O
(	O
cid:12	O
)	O
nd	O
this	O
outcome	O
surprising	O
.	O
there	O
are	O
two	O
ways	O
to	O
make	O
it	O
more	O
intuitive	O
.	O
one	O
is	O
to	O
play	O
the	O
game	B
thirty	O
times	O
with	O
a	O
friend	O
and	O
keep	O
track	O
of	O
the	O
frequency	O
with	O
which	O
switching	O
gets	O
the	O
prize	B
.	O
alternatively	O
,	O
you	O
can	O
perform	O
a	O
thought	O
experiment	O
in	O
which	O
the	O
game	B
is	O
played	O
with	O
a	O
million	O
doors	B
.	O
the	O
rules	B
are	O
now	O
that	O
the	O
contestant	O
chooses	O
one	O
door	O
,	O
then	O
the	O
game	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
3.6	O
:	O
solutions	O
61	O
show	O
host	O
opens	O
999,998	O
doors	B
in	O
such	O
a	O
way	O
as	O
not	O
to	O
reveal	O
the	O
prize	B
,	O
leaving	O
the	O
contestant	O
’	O
s	O
selected	O
door	O
and	O
one	O
other	O
door	O
closed	O
.	O
the	O
contestant	O
may	O
now	O
stick	O
or	O
switch	O
.	O
imagine	O
the	O
contestant	O
confronted	O
by	O
a	O
million	O
doors	B
,	O
of	O
which	O
doors	B
1	O
and	O
234,598	O
have	O
not	O
been	O
opened	O
,	O
door	O
1	O
having	O
been	O
the	O
contestant	O
’	O
s	O
initial	O
guess	O
.	O
where	O
do	O
you	O
think	O
the	O
prize	B
is	O
?	O
solution	O
to	O
exercise	O
3.9	O
(	O
p.57	O
)	O
.	O
if	O
door	O
3	O
is	O
opened	O
by	O
an	O
earthquake	B
,	O
the	O
inference	B
comes	O
out	O
di	O
(	O
cid:11	O
)	O
erently	O
{	O
even	O
though	O
visually	O
the	O
scene	O
looks	O
the	O
same	O
.	O
the	O
nature	O
of	O
the	O
data	O
,	O
and	O
the	O
probability	O
of	O
the	O
data	O
,	O
are	O
both	O
now	O
di	O
(	O
cid:11	O
)	O
erent	O
.	O
the	O
possible	O
data	O
outcomes	O
are	O
,	O
(	O
cid:12	O
)	O
rstly	O
,	O
that	O
any	O
number	O
of	O
the	O
doors	O
might	O
have	O
opened	O
.	O
we	O
could	O
label	O
the	O
eight	O
possible	O
outcomes	O
d	O
=	O
(	O
0	O
;	O
0	O
;	O
0	O
)	O
;	O
(	O
0	O
;	O
0	O
;	O
1	O
)	O
;	O
(	O
0	O
;	O
1	O
;	O
0	O
)	O
;	O
(	O
1	O
;	O
0	O
;	O
0	O
)	O
;	O
(	O
0	O
;	O
1	O
;	O
1	O
)	O
;	O
:	O
:	O
:	O
;	O
(	O
1	O
;	O
1	O
;	O
1	O
)	O
.	O
secondly	O
,	O
it	O
might	O
be	O
that	O
the	O
prize	B
is	O
visible	O
after	O
the	O
earthquake	B
has	O
opened	O
one	O
or	O
more	O
doors	B
.	O
so	O
the	O
data	O
d	O
consists	O
of	O
the	O
value	O
of	O
d	O
,	O
and	O
a	O
statement	O
of	O
whether	O
the	O
prize	B
was	O
revealed	O
.	O
it	O
is	O
hard	O
to	O
say	O
what	O
the	O
probabilities	O
of	O
these	O
outcomes	O
are	O
,	O
since	O
they	O
depend	O
on	O
our	O
beliefs	O
about	O
the	O
reliability	O
of	O
the	O
door	O
latches	O
and	O
the	O
properties	O
of	O
earthquakes	O
,	O
but	O
it	O
is	O
possible	O
to	O
extract	O
the	O
desired	O
posterior	B
probability	I
without	O
naming	O
the	O
values	O
of	O
p	O
(	O
djhi	O
)	O
for	O
each	O
d.	O
all	O
that	O
matters	O
are	O
the	O
relative	B
values	O
of	O
the	O
quantities	O
p	O
(	O
d	O
jh1	O
)	O
,	O
p	O
(	O
d	O
jh2	O
)	O
,	O
p	O
(	O
d	O
jh3	O
)	O
,	O
for	O
the	O
value	O
of	O
d	O
that	O
actually	O
occurred	O
.	O
[	O
this	O
is	O
the	O
likelihood	B
principle	I
,	O
which	O
we	O
met	O
in	O
section	O
2.3	O
.	O
]	O
the	O
value	O
of	O
d	O
that	O
actually	O
occurred	O
is	O
‘	O
d	O
=	O
(	O
0	O
;	O
0	O
;	O
1	O
)	O
,	O
and	O
no	O
prize	B
visible	O
’	O
.	O
first	O
,	O
it	O
is	O
clear	O
that	O
p	O
(	O
d	O
jh3	O
)	O
=	O
0	O
,	O
since	O
the	O
datum	O
that	O
no	O
prize	B
is	O
visible	O
is	O
incompatible	O
with	O
h3	O
.	O
now	O
,	O
assuming	O
that	O
the	O
contestant	O
selected	O
door	O
1	O
,	O
how	O
does	O
the	O
probability	B
p	O
(	O
d	O
jh1	O
)	O
compare	O
with	O
p	O
(	O
d	O
jh2	O
)	O
?	O
assuming	O
that	O
earthquakes	O
are	O
not	O
sensitive	O
to	O
decisions	O
of	O
game	O
show	O
contestants	O
,	O
these	O
two	O
quantities	O
have	O
to	O
be	O
equal	O
,	O
by	O
symmetry	O
.	O
we	O
don	O
’	O
t	O
know	O
how	O
likely	O
it	O
is	O
that	O
door	O
3	O
falls	O
o	O
(	O
cid:11	O
)	O
its	O
hinges	O
,	O
but	O
however	O
likely	O
it	O
is	O
,	O
it	O
’	O
s	O
just	O
as	O
likely	O
to	O
do	O
so	O
whether	O
the	O
prize	B
is	O
behind	O
door	O
1	O
or	O
door	O
2.	O
so	O
,	O
if	O
p	O
(	O
d	O
jh1	O
)	O
and	O
p	O
(	O
d	O
jh2	O
)	O
are	O
equal	O
,	O
we	O
obtain	O
:	O
p	O
(	O
h2jd	O
)	O
=	O
p	O
(	O
djh2	O
)	O
(	O
1/3	O
)	O
p	O
(	O
h1jd	O
)	O
=	O
p	O
(	O
djh1	O
)	O
(	O
1/3	O
)	O
p	O
(	O
h3jd	O
)	O
=	O
p	O
(	O
djh3	O
)	O
(	O
1/3	O
)	O
p	O
(	O
d	O
)	O
p	O
(	O
d	O
)	O
p	O
(	O
d	O
)	O
=	O
0	O
:	O
=	O
1/2	O
=	O
1/2	O
(	O
3.41	O
)	O
the	O
two	O
possible	O
hypotheses	O
are	O
now	O
equally	O
likely	O
.	O
if	O
we	O
assume	O
that	O
the	O
host	O
knows	O
where	O
the	O
prize	B
is	O
and	O
might	O
be	O
acting	O
deceptively	O
,	O
then	O
the	O
answer	O
might	O
be	O
further	O
modi	O
(	O
cid:12	O
)	O
ed	O
,	O
because	O
we	O
have	O
to	O
view	O
the	O
host	O
’	O
s	O
words	O
as	O
part	O
of	O
the	O
data	O
.	O
confused	O
?	O
it	O
’	O
s	O
well	O
worth	O
making	O
sure	O
you	O
understand	O
these	O
two	O
gameshow	O
problems	O
.	O
don	O
’	O
t	O
worry	O
,	O
i	O
slipped	O
up	O
on	O
the	O
second	O
problem	O
,	O
the	O
(	O
cid:12	O
)	O
rst	O
time	O
i	O
met	O
it	O
.	O
there	O
is	O
a	O
general	O
rule	O
which	O
helps	O
immensely	O
when	O
you	O
have	O
a	O
confusing	O
probability	B
problem	O
:	O
always	O
write	O
down	O
the	O
probability	O
of	O
everything	O
.	O
(	O
steve	O
gull	O
)	O
from	O
this	O
joint	B
probability	O
,	O
any	O
desired	O
inference	B
can	O
be	O
mechanically	O
ob-	O
tained	O
(	O
(	O
cid:12	O
)	O
gure	O
3.9	O
)	O
.	O
solution	O
to	O
exercise	O
3.11	O
(	O
p.58	O
)	O
.	O
the	O
statistic	B
quoted	O
by	O
the	O
lawyer	B
indicates	O
the	O
probability	B
that	O
a	O
randomly	O
selected	O
wife-beater	B
will	O
also	O
murder	B
his	O
wife	O
.	O
the	O
probability	B
that	O
the	O
husband	O
was	O
the	O
murderer	O
,	O
given	O
that	O
the	O
wife	O
has	O
been	O
murdered	O
,	O
is	O
a	O
completely	O
di	O
(	O
cid:11	O
)	O
erent	O
quantity	O
.	O
where	O
the	O
prize	B
is	O
door	O
door	O
door	O
1	O
2	O
3	O
none	O
pnone	O
pnone	O
pnone	O
3	O
3	O
3	O
p3	O
3	O
p3	O
3	O
p3	O
3	O
e	O
k	O
a	O
u	O
q	O
h	O
t	O
r	O
a	O
e	O
y	O
b	O
d	O
e	O
n	O
e	O
p	O
o	O
s	O
r	O
o	O
o	O
d	O
h	O
c	O
i	O
h	O
w	O
1	O
2	O
3	O
1,2	O
1,3	O
2,3	O
1,2,3	O
p1	O
;	O
2	O
;	O
3	O
p1	O
;	O
2	O
;	O
3	O
p1	O
;	O
2	O
;	O
3	O
3	O
3	O
3	O
figure	O
3.9.	O
the	O
probability	O
of	O
everything	O
,	O
for	O
the	O
second	O
three-door	O
problem	O
,	O
assuming	O
an	O
earthquake	B
has	O
just	O
occurred	O
.	O
here	O
,	O
p3	O
is	O
the	O
probability	B
that	O
door	O
3	O
alone	O
is	O
opened	O
by	O
an	O
earthquake	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
62	O
3	O
|	O
more	O
about	O
inference	B
to	O
deduce	O
the	O
latter	O
,	O
we	O
need	O
to	O
make	O
further	O
assumptions	B
about	O
the	O
probability	B
that	O
the	O
wife	O
is	O
murdered	O
by	O
someone	O
else	O
.	O
if	O
she	O
lives	O
in	O
a	O
neigh-	O
bourhood	O
with	O
frequent	O
random	B
murders	O
,	O
then	O
this	O
probability	B
is	O
large	O
and	O
the	O
posterior	B
probability	I
that	O
the	O
husband	O
did	O
it	O
(	O
in	O
the	O
absence	O
of	O
other	O
ev-	O
idence	O
)	O
may	O
not	O
be	O
very	O
large	O
.	O
but	O
in	O
more	O
peaceful	O
regions	O
,	O
it	O
may	O
well	O
be	O
that	O
the	O
most	O
likely	O
person	O
to	O
have	O
murdered	O
you	O
,	O
if	O
you	O
are	O
found	O
murdered	O
,	O
is	O
one	O
of	O
your	O
closest	O
relatives	O
.	O
let	O
’	O
s	O
work	O
out	O
some	O
illustrative	O
numbers	O
with	O
the	O
help	O
of	O
the	O
statistics	O
on	O
page	O
58.	O
let	O
m	O
=	O
1	O
denote	O
the	O
proposition	O
that	O
a	O
woman	O
has	O
been	O
mur-	O
dered	O
;	O
h	O
=	O
1	O
,	O
the	O
proposition	O
that	O
the	O
husband	O
did	O
it	O
;	O
and	O
b	O
=	O
1	O
,	O
the	O
propo-	O
sition	O
that	O
he	O
beat	O
her	O
in	O
the	O
year	O
preceding	O
the	O
murder	B
.	O
the	O
statement	O
‘	O
someone	O
else	O
did	O
it	O
’	O
is	O
denoted	O
by	O
h	O
=	O
0.	O
we	O
need	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
p	O
(	O
hj	O
m	O
=	O
1	O
)	O
,	O
p	O
(	O
bj	O
h	O
=	O
1	O
;	O
m	O
=	O
1	O
)	O
,	O
and	O
p	O
(	O
b	O
=	O
1j	O
h	O
=	O
0	O
;	O
m	O
=	O
1	O
)	O
in	O
order	O
to	O
compute	O
the	O
pos-	O
terior	O
probability	B
p	O
(	O
h	O
=	O
1j	O
b	O
=	O
1	O
;	O
m	O
=	O
1	O
)	O
.	O
from	O
the	O
statistics	O
,	O
we	O
can	O
read	O
out	O
p	O
(	O
h	O
=	O
1j	O
m	O
=	O
1	O
)	O
=	O
0:28.	O
and	O
if	O
two	O
million	O
women	O
out	O
of	O
100	O
million	O
are	O
beaten	O
,	O
then	O
p	O
(	O
b	O
=	O
1j	O
h	O
=	O
0	O
;	O
m	O
=	O
1	O
)	O
=	O
0:02.	O
finally	O
,	O
we	O
need	O
a	O
value	O
for	O
p	O
(	O
bj	O
h	O
=	O
1	O
;	O
m	O
=	O
1	O
)	O
:	O
if	O
a	O
man	O
murders	O
his	O
wife	O
,	O
how	O
likely	O
is	O
it	O
that	O
this	O
is	O
the	O
(	O
cid:12	O
)	O
rst	O
time	O
he	O
laid	O
a	O
(	O
cid:12	O
)	O
nger	O
on	O
her	O
?	O
i	O
expect	O
it	O
’	O
s	O
pretty	O
unlikely	O
;	O
so	O
maybe	O
p	O
(	O
b	O
=	O
1j	O
h	O
=	O
1	O
;	O
m	O
=	O
1	O
)	O
is	O
0.9	O
or	O
larger	O
.	O
by	O
bayes	O
’	O
theorem	B
,	O
then	O
,	O
p	O
(	O
h	O
=	O
1j	O
b	O
=	O
1	O
;	O
m	O
=	O
1	O
)	O
=	O
:9	O
(	O
cid:2	O
)	O
:28	O
:9	O
(	O
cid:2	O
)	O
:28	O
+	O
:02	O
(	O
cid:2	O
)	O
:72	O
’	O
95	O
%	O
:	O
(	O
3.42	O
)	O
one	O
way	O
to	O
make	O
obvious	O
the	O
sliminess	O
of	O
the	O
lawyer	O
on	O
p.58	O
is	O
to	O
construct	O
arguments	O
,	O
with	O
the	O
same	O
logical	O
structure	O
as	O
his	O
,	O
that	O
are	O
clearly	O
wrong	O
.	O
for	O
example	O
,	O
the	O
lawyer	B
could	O
say	O
‘	O
not	O
only	O
was	O
mrs.	O
s	O
murdered	O
,	O
she	O
was	O
murdered	O
between	O
4.02pm	O
and	O
4.03pm	O
.	O
statistically	O
,	O
only	O
one	O
in	O
a	O
million	O
wife-beaters	O
actually	O
goes	O
on	O
to	O
murder	B
his	O
wife	O
between	O
4.02pm	O
and	O
4.03pm	O
.	O
so	O
the	O
wife-beating	O
is	O
not	O
strong	O
evidence	B
at	O
all	O
.	O
in	O
fact	O
,	O
given	O
the	O
wife-beating	O
evidence	B
alone	O
,	O
it	O
’	O
s	O
extremely	O
unlikely	O
that	O
he	O
would	O
murder	B
his	O
wife	O
in	O
this	O
way	O
{	O
only	O
a	O
1/1,000,000	O
chance.	O
’	O
solution	O
to	O
exercise	O
3.13	O
(	O
p.58	O
)	O
.	O
there	O
are	O
two	O
hypotheses	O
.	O
h0	O
:	O
your	O
number	O
is	O
740511	O
;	O
h1	O
:	O
it	O
is	O
another	O
number	O
.	O
the	O
data	O
,	O
d	O
,	O
are	O
‘	O
when	O
i	O
dialed	O
740511	O
,	O
i	O
got	O
a	O
busy	O
signal	O
’	O
.	O
what	O
is	O
the	O
probability	O
of	O
d	O
,	O
given	O
each	O
hypothesis	O
?	O
if	O
your	O
number	O
is	O
740511	O
,	O
then	O
we	O
expect	O
a	O
busy	O
signal	O
with	O
certainty	O
:	O
p	O
(	O
d	O
jh0	O
)	O
=	O
1	O
:	O
on	O
the	O
other	O
hand	O
,	O
if	O
h1	O
is	O
true	O
,	O
then	O
the	O
probability	B
that	O
the	O
number	O
dialled	O
returns	O
a	O
busy	O
signal	O
is	O
smaller	O
than	O
1	O
,	O
since	O
various	O
other	O
outcomes	O
were	O
also	O
possible	O
(	O
a	O
ringing	O
tone	O
,	O
or	O
a	O
number-unobtainable	O
signal	O
,	O
for	O
example	O
)	O
.	O
the	O
value	O
of	O
this	O
probability	B
p	O
(	O
d	O
jh1	O
)	O
will	O
depend	O
on	O
the	O
probability	B
(	O
cid:11	O
)	O
that	O
a	O
random	B
phone	O
number	O
similar	O
to	O
your	O
own	O
phone	B
number	I
would	O
be	O
a	O
valid	O
phone	B
number	I
,	O
and	O
on	O
the	O
probability	B
(	O
cid:12	O
)	O
that	O
you	O
get	O
a	O
busy	O
signal	O
when	O
you	O
dial	O
a	O
valid	O
phone	B
number	I
.	O
i	O
estimate	O
from	O
the	O
size	O
of	O
my	O
phone	B
book	O
that	O
cambridge	O
has	O
about	O
75	O
000	O
valid	O
phone	B
numbers	O
,	O
all	O
of	O
length	O
six	B
digits	O
.	O
the	O
probability	B
that	O
a	O
random	B
six-digit	O
number	O
is	O
valid	O
is	O
therefore	O
about	O
75	O
000=106	O
=	O
0:075.	O
if	O
we	O
exclude	O
numbers	O
beginning	O
with	O
0	O
,	O
1	O
,	O
and	O
9	O
from	O
the	O
random	B
choice	O
,	O
the	O
probability	B
(	O
cid:11	O
)	O
is	O
about	O
75	O
000=700	O
000	O
’	O
0:1.	O
if	O
we	O
assume	O
that	O
telephone	B
numbers	O
are	O
clustered	O
then	O
a	O
misremembered	O
number	O
might	O
be	O
more	O
likely	O
to	O
be	O
valid	O
than	O
a	O
randomly	O
chosen	O
number	O
;	O
so	O
the	O
probability	B
,	O
(	O
cid:11	O
)	O
,	O
that	O
our	O
guessed	O
number	O
would	O
be	O
valid	O
,	O
assuming	O
h1	O
is	O
true	O
,	O
might	O
be	O
bigger	O
than	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
3.6	O
:	O
solutions	O
63	O
0.1.	O
anyway	O
,	O
(	O
cid:11	O
)	O
must	O
be	O
somewhere	O
between	O
0.1	O
and	O
1.	O
we	O
can	O
carry	O
forward	O
this	O
uncertainty	O
in	O
the	O
probability	B
and	O
see	O
how	O
much	O
it	O
matters	O
at	O
the	O
end	O
.	O
the	O
probability	B
(	O
cid:12	O
)	O
that	O
you	O
get	O
a	O
busy	O
signal	O
when	O
you	O
dial	O
a	O
valid	O
phone	B
number	I
is	O
equal	O
to	O
the	O
fraction	O
of	O
phones	O
you	O
think	O
are	O
in	O
use	O
or	O
o	O
(	O
cid:11	O
)	O
-the-hook	O
when	O
you	O
make	O
your	O
tentative	O
call	O
.	O
this	O
fraction	O
varies	O
from	O
town	O
to	O
town	O
and	O
with	O
the	O
time	O
of	O
day	O
.	O
in	O
cambridge	O
,	O
during	O
the	O
day	O
,	O
i	O
would	O
guess	O
that	O
about	O
1	O
%	O
of	O
phones	O
are	O
in	O
use	O
.	O
at	O
4am	O
,	O
maybe	O
0.1	O
%	O
,	O
or	O
fewer	O
.	O
the	O
probability	B
p	O
(	O
d	O
jh1	O
)	O
is	O
the	O
product	O
of	O
(	O
cid:11	O
)	O
and	O
(	O
cid:12	O
)	O
,	O
that	O
is	O
,	O
about	O
0:1	O
(	O
cid:2	O
)	O
0:01	O
=	O
10	O
(	O
cid:0	O
)	O
3.	O
according	O
to	O
our	O
estimates	O
,	O
there	O
’	O
s	O
about	O
a	O
one-in-a-thousand	O
chance	O
of	O
getting	O
a	O
busy	O
signal	O
when	O
you	O
dial	O
a	O
random	O
number	O
;	O
or	O
one-in-a-	O
hundred	O
,	O
if	O
valid	O
numbers	O
are	O
strongly	O
clustered	O
;	O
or	O
one-in-104	O
,	O
if	O
you	O
dial	O
in	O
the	O
wee	O
hours	O
.	O
how	O
do	O
the	O
data	O
a	O
(	O
cid:11	O
)	O
ect	O
your	O
beliefs	O
about	O
your	O
phone	B
number	I
?	O
the	O
pos-	O
terior	O
probability	B
ratio	O
is	O
the	O
likelihood	B
ratio	O
times	O
the	O
prior	B
probability	O
ratio	O
:	O
p	O
(	O
h0	O
j	O
d	O
)	O
p	O
(	O
h1	O
j	O
d	O
)	O
=	O
p	O
(	O
d	O
jh0	O
)	O
p	O
(	O
d	O
jh1	O
)	O
p	O
(	O
h0	O
)	O
p	O
(	O
h1	O
)	O
:	O
(	O
3.43	O
)	O
the	O
likelihood	B
ratio	O
is	O
about	O
100-to-1	O
or	O
1000-to-1	O
,	O
so	O
the	O
posterior	B
probability	I
ratio	O
is	O
swung	O
by	O
a	O
factor	O
of	O
100	O
or	O
1000	O
in	O
favour	O
of	O
h0	O
.	O
if	O
the	O
prior	B
probability	O
of	O
h0	O
was	O
0.5	O
then	O
the	O
posterior	B
probability	I
is	O
p	O
(	O
h0	O
j	O
d	O
)	O
=	O
1	O
1	O
+	O
p	O
(	O
h1	O
j	O
d	O
)	O
p	O
(	O
h0	O
j	O
d	O
)	O
’	O
0:99	O
or	O
0:999	O
:	O
(	O
3.44	O
)	O
solution	O
to	O
exercise	O
3.15	O
(	O
p.59	O
)	O
.	O
we	O
compare	O
the	O
models	O
h0	O
{	O
the	O
coin	B
is	O
fair	O
{	O
and	O
h1	O
{	O
the	O
coin	B
is	O
biased	O
,	O
with	O
the	O
prior	B
on	O
its	O
bias	B
set	O
to	O
the	O
uniform	O
distribution	B
p	O
(	O
pjh1	O
)	O
=	O
1	O
.	O
[	O
the	O
use	O
of	O
a	O
uniform	O
prior	B
seems	O
reasonable	O
to	O
me	O
,	O
since	O
i	O
know	O
that	O
some	O
coins	O
,	O
such	O
as	O
american	O
pennies	O
,	O
have	O
severe	O
biases	O
when	O
spun	O
on	O
edge	O
;	O
so	O
the	O
situations	O
p	O
=	O
0:01	O
or	O
p	O
=	O
0:1	O
or	O
p	O
=	O
0:95	O
would	O
not	O
surprise	O
me	O
.	O
]	O
when	O
i	O
mention	O
h0	O
{	O
the	O
coin	B
is	O
fair	O
{	O
a	O
pedant	O
would	O
say	O
,	O
‘	O
how	O
absurd	O
to	O
even	O
consider	O
that	O
the	O
coin	B
is	O
fair	O
{	O
any	O
coin	B
is	O
surely	O
biased	O
to	O
some	O
extent	O
’	O
.	O
and	O
of	O
course	O
i	O
would	O
agree	O
.	O
so	O
will	O
pedants	O
kindly	O
understand	O
h0	O
as	O
meaning	O
‘	O
the	O
coin	B
is	O
fair	O
to	O
within	O
one	O
part	O
in	O
a	O
thousand	O
,	O
i.e.	O
,	O
p	O
2	O
0:5	O
(	O
cid:6	O
)	O
0:001	O
’	O
.	O
the	O
likelihood	B
ratio	O
is	O
:	O
p	O
(	O
djh1	O
)	O
p	O
(	O
djh0	O
)	O
140	O
!	O
110	O
!	O
=	O
251	O
!	O
1=2250	O
=	O
0:48	O
:	O
(	O
3.45	O
)	O
thus	O
the	O
data	O
give	O
scarcely	O
any	O
evidence	B
either	O
way	O
;	O
in	O
fact	O
they	O
give	O
weak	O
evidence	B
(	O
two	O
to	O
one	O
)	O
in	O
favour	O
of	O
h0	O
!	O
‘	O
no	O
,	O
no	O
’	O
,	O
objects	O
the	O
believer	O
in	O
bias	O
,	O
‘	O
your	O
silly	O
uniform	O
prior	B
doesn	O
’	O
t	O
represent	O
my	O
prior	B
beliefs	O
about	O
the	O
bias	B
of	O
biased	O
coins	O
{	O
i	O
was	O
expecting	O
only	O
a	O
small	O
bias	B
’	O
.	O
to	O
be	O
as	O
generous	O
as	O
possible	O
to	O
the	O
h1	O
,	O
let	O
’	O
s	O
see	O
how	O
well	O
it	O
could	O
fare	O
if	O
the	O
prior	B
were	O
presciently	O
set	B
.	O
let	O
us	O
allow	O
a	O
prior	B
of	O
the	O
form	O
p	O
(	O
pjh1	O
;	O
(	O
cid:11	O
)	O
)	O
=	O
1	O
z	O
(	O
(	O
cid:11	O
)	O
)	O
p	O
(	O
cid:11	O
)	O
(	O
cid:0	O
)	O
1	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
(	O
cid:11	O
)	O
(	O
cid:0	O
)	O
1	O
;	O
where	O
z	O
(	O
(	O
cid:11	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
(	O
(	O
cid:11	O
)	O
)	O
2=	O
(	O
cid:0	O
)	O
(	O
2	O
(	O
cid:11	O
)	O
)	O
(	O
3.46	O
)	O
0.05	O
0.04	O
0.03	O
0.02	O
0.01	O
0	O
0	O
h0	O
h1	O
140	O
50	O
100	O
150	O
200	O
250	O
figure	O
3.10.	O
the	O
probability	B
distribution	O
of	O
the	O
number	O
of	O
heads	O
given	O
the	O
two	O
hypotheses	O
,	O
that	O
the	O
coin	B
is	O
fair	O
,	O
and	O
that	O
it	O
is	O
biased	O
,	O
with	O
the	O
prior	B
distribution	O
of	O
the	O
bias	O
being	O
uniform	O
.	O
the	O
outcome	O
(	O
d	O
=	O
140	O
heads	O
)	O
gives	O
weak	O
evidence	B
in	O
favour	O
of	O
h0	O
,	O
the	O
hypothesis	O
that	O
the	O
coin	B
is	O
fair	O
.	O
(	O
a	O
beta	B
distribution	I
,	O
with	O
the	O
original	O
uniform	O
prior	B
reproduced	O
by	O
setting	O
(	O
cid:11	O
)	O
=	O
1	O
)	O
.	O
by	O
tweaking	O
(	O
cid:11	O
)	O
,	O
the	O
likelihood	B
ratio	O
for	O
h1	O
over	O
h0	O
,	O
(	O
cid:0	O
)	O
(	O
140+	O
(	O
cid:11	O
)	O
)	O
(	O
cid:0	O
)	O
(	O
110+	O
(	O
cid:11	O
)	O
)	O
(	O
cid:0	O
)	O
(	O
2	O
(	O
cid:11	O
)	O
)	O
2250	O
;	O
(	O
3.47	O
)	O
p	O
(	O
djh1	O
;	O
(	O
cid:11	O
)	O
)	O
p	O
(	O
djh0	O
)	O
=	O
(	O
cid:0	O
)	O
(	O
250+2	O
(	O
cid:11	O
)	O
)	O
(	O
cid:0	O
)	O
(	O
(	O
cid:11	O
)	O
)	O
2	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
64	O
3	O
|	O
more	O
about	O
inference	B
can	O
be	O
increased	O
a	O
little	O
.	O
it	O
is	O
shown	O
for	O
several	O
values	O
of	O
(	O
cid:11	O
)	O
in	O
(	O
cid:12	O
)	O
gure	O
3.11.	O
even	O
the	O
most	O
favourable	O
choice	O
of	O
(	O
cid:11	O
)	O
(	O
(	O
cid:11	O
)	O
’	O
50	O
)	O
can	O
yield	O
a	O
likelihood	B
ratio	O
of	O
only	O
two	O
to	O
one	O
in	O
favour	O
of	O
h1	O
.	O
in	O
conclusion	O
,	O
the	O
data	O
are	O
not	O
‘	O
very	O
suspicious	O
’	O
.	O
they	O
can	O
be	O
construed	O
as	O
giving	O
at	O
most	O
two-to-one	O
evidence	B
in	O
favour	O
of	O
one	O
or	O
other	O
of	O
the	O
two	O
hypotheses	O
.	O
are	O
these	O
wimpy	O
likelihood	B
ratios	O
the	O
fault	O
of	O
over-restrictive	O
priors	O
?	O
is	O
there	O
any	O
way	O
of	O
producing	O
a	O
‘	O
very	O
suspicious	O
’	O
conclusion	O
?	O
the	O
prior	B
that	O
is	O
best-	O
matched	O
to	O
the	O
data	O
,	O
in	O
terms	O
of	O
likelihood	O
,	O
is	O
the	O
prior	B
that	O
sets	O
p	O
to	O
f	O
(	O
cid:17	O
)	O
140=250	O
with	O
probability	O
one	O
.	O
let	O
’	O
s	O
call	O
this	O
model	B
h	O
(	O
cid:3	O
)	O
.	O
the	O
likelihood	B
ratio	O
is	O
p	O
(	O
djh	O
(	O
cid:3	O
)	O
)	O
=p	O
(	O
djh0	O
)	O
=	O
2250f	O
140	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
110	O
=	O
6:1.	O
so	O
the	O
strongest	O
evidence	B
that	O
these	O
data	O
can	O
possibly	O
muster	O
against	O
the	O
hypothesis	O
that	O
there	O
is	O
no	O
bias	B
is	O
six-to-one	O
.	O
while	O
we	O
are	O
noticing	O
the	O
absurdly	O
misleading	O
answers	O
that	O
‘	O
sampling	O
the-	O
ory	O
’	O
statistics	O
produces	O
,	O
such	O
as	O
the	O
p-value	B
of	O
7	O
%	O
in	O
the	O
exercise	O
we	O
just	O
solved	O
,	O
let	O
’	O
s	O
stick	O
the	O
boot	O
in	O
.	O
if	O
we	O
make	O
a	O
tiny	O
change	O
to	O
the	O
data	B
set	I
,	O
increasing	O
the	O
number	O
of	O
heads	O
in	O
250	O
tosses	O
from	O
140	O
to	O
141	O
,	O
we	O
(	O
cid:12	O
)	O
nd	O
that	O
the	O
p-value	B
goes	O
below	O
the	O
mystical	O
value	O
of	O
0.05	O
(	O
the	O
p-value	B
is	O
0.0497	O
)	O
.	O
the	O
sampling	B
theory	I
statistician	O
would	O
happily	O
squeak	O
‘	O
the	O
probability	O
of	O
getting	O
a	O
result	O
as	O
extreme	O
as	O
141	O
heads	O
is	O
smaller	O
than	O
0.05	O
{	O
we	O
thus	O
reject	O
the	O
null	O
hypothesis	O
at	O
a	O
signi	O
(	O
cid:12	O
)	O
cance	O
level	O
of	O
5	O
%	O
’	O
.	O
the	O
correct	O
answer	O
is	O
shown	O
for	O
several	O
values	O
of	O
(	O
cid:11	O
)	O
in	O
(	O
cid:12	O
)	O
gure	O
3.12.	O
the	O
values	O
worth	O
highlighting	O
from	O
this	O
table	O
are	O
,	O
(	O
cid:12	O
)	O
rst	O
,	O
the	O
likelihood	B
ratio	O
when	O
h1	O
uses	O
the	O
standard	O
uniform	O
prior	B
,	O
which	O
is	O
1:0.61	O
in	O
favour	O
of	O
the	O
null	O
hypothesis	O
h0	O
.	O
second	O
,	O
the	O
most	O
favourable	O
choice	O
of	O
(	O
cid:11	O
)	O
,	O
from	O
the	O
point	O
of	O
view	O
of	O
h1	O
,	O
can	O
only	O
yield	O
a	O
likelihood	B
ratio	O
of	O
about	O
2.3:1	O
in	O
favour	O
of	O
h1	O
.	O
be	O
warned	O
!	O
a	O
p-value	B
of	O
0.05	O
is	O
often	O
interpreted	O
as	O
implying	O
that	O
the	O
odds	B
are	O
stacked	O
about	O
twenty-to-one	O
against	O
the	O
null	O
hypothesis	O
.	O
but	O
the	O
truth	O
in	O
this	O
case	O
is	O
that	O
the	O
evidence	B
either	O
slightly	O
favours	O
the	O
null	O
hypothesis	O
,	O
or	O
disfavours	O
it	O
by	O
at	O
most	O
2.3	O
to	O
one	O
,	O
depending	O
on	O
the	O
choice	O
of	O
prior	O
.	O
the	O
p-values	O
and	O
‘	O
signi	O
(	O
cid:12	O
)	O
cance	O
levels	O
’	O
of	O
classical	O
statistics	O
should	O
be	O
treated	O
with	O
extreme	O
caution	B
.	O
shun	O
them	O
!	O
here	O
ends	O
the	O
sermon	B
.	O
(	O
cid:11	O
)	O
.37	O
1.0	O
2.7	O
7.4	O
20	O
55	O
148	O
403	O
1096	O
p	O
(	O
djh1	O
;	O
(	O
cid:11	O
)	O
)	O
p	O
(	O
djh0	O
)	O
.25	O
.48	O
.82	O
1.3	O
1.8	O
1.9	O
1.7	O
1.3	O
1.1	O
figure	O
3.11.	O
likelihood	B
ratio	O
for	O
various	O
choices	O
of	O
the	O
prior	O
distribution	B
’	O
s	O
hyperparameter	B
(	O
cid:11	O
)	O
.	O
(	O
cid:11	O
)	O
.37	O
1.0	O
2.7	O
7.4	O
20	O
55	O
148	O
403	O
1096	O
p	O
(	O
d0jh1	O
;	O
(	O
cid:11	O
)	O
)	O
p	O
(	O
d0jh0	O
)	O
.32	O
.61	O
1.0	O
1.6	O
2.2	O
2.3	O
1.9	O
1.4	O
1.2	O
figure	O
3.12.	O
likelihood	B
ratio	O
for	O
various	O
choices	O
of	O
the	O
prior	O
distribution	B
’	O
s	O
hyperparameter	B
(	O
cid:11	O
)	O
,	O
when	O
the	O
data	O
are	O
d0	O
=	O
141	O
heads	O
in	O
250	O
trials	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
part	O
i	O
data	B
compression	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
notation	B
x	O
is	O
a	O
member	O
of	O
the	O
x	O
2	O
a	O
set	B
a	O
s	O
is	O
a	O
subset	B
of	O
the	O
s	O
(	O
cid:26	O
)	O
a	O
set	B
a	O
s	O
(	O
cid:18	O
)	O
a	O
s	O
is	O
a	O
subset	B
of	O
,	O
or	O
equal	O
to	O
,	O
the	O
set	B
a	O
v	O
=	O
b	O
[	O
a	O
v	O
is	O
the	O
union	B
of	O
the	O
sets	O
b	O
and	O
a	O
v	O
=	O
b	O
\	O
a	O
v	O
is	O
the	O
intersection	B
of	O
the	O
sets	O
b	O
and	O
a	O
number	O
of	O
elements	O
jaj	O
in	O
set	O
a	O
about	O
chapter	O
4	O
in	O
this	O
chapter	O
we	O
discuss	O
how	B
to	I
measure	I
the	O
information	B
content	I
of	O
the	O
outcome	O
of	O
a	O
random	B
experiment	O
.	O
this	O
chapter	O
has	O
some	O
tough	O
bits	O
.	O
if	O
you	O
(	O
cid:12	O
)	O
nd	O
the	O
mathematical	O
details	O
hard	O
,	O
skim	O
through	O
them	O
and	O
keep	O
going	O
{	O
you	O
’	O
ll	O
be	O
able	O
to	O
enjoy	O
chapters	O
5	O
and	O
6	O
without	O
this	O
chapter	O
’	O
s	O
tools	O
.	O
before	O
reading	O
chapter	O
4	O
,	O
you	O
should	O
have	O
read	O
chapter	O
2	O
and	O
worked	O
on	O
exercises	O
2.21	O
{	O
2.25	O
and	O
2.16	O
(	O
pp.36	O
{	O
37	O
)	O
,	O
and	O
exercise	O
4.1	O
below	O
.	O
the	O
following	O
exercise	O
is	O
intended	O
to	O
help	O
you	O
think	O
about	O
how	B
to	I
measure	I
information	O
content	B
.	O
exercise	O
4.1	O
.	O
[	O
2	O
,	O
p.69	O
]	O
{	O
please	O
work	O
on	O
this	O
problem	O
before	O
reading	O
chapter	O
4.	O
you	O
are	O
given	O
12	O
balls	O
,	O
all	O
equal	O
in	O
weight	O
except	O
for	O
one	O
that	O
is	O
either	O
heavier	O
or	O
lighter	O
.	O
you	O
are	O
also	O
given	O
a	O
two-pan	O
balance	B
to	O
use	O
.	O
in	O
each	O
use	O
of	O
the	O
balance	O
you	O
may	O
put	O
any	O
number	O
of	O
the	O
12	O
balls	O
on	O
the	O
left	O
pan	O
,	O
and	O
the	O
same	O
number	O
on	O
the	O
right	O
pan	O
,	O
and	O
push	O
a	O
button	O
to	O
initiate	O
the	O
weighing	O
;	O
there	O
are	O
three	O
possible	O
outcomes	O
:	O
either	O
the	O
weights	O
are	O
equal	O
,	O
or	O
the	O
balls	O
on	O
the	O
left	O
are	O
heavier	O
,	O
or	O
the	O
balls	O
on	O
the	O
left	O
are	O
lighter	O
.	O
your	O
task	O
is	O
to	O
design	O
a	O
strategy	O
to	O
determine	O
which	O
is	O
the	O
odd	O
ball	O
and	O
whether	O
it	O
is	O
heavier	O
or	O
lighter	O
than	O
the	O
others	O
in	O
as	O
few	O
uses	O
of	O
the	O
balance	B
as	O
possible	O
.	O
while	O
thinking	O
about	O
this	O
problem	O
,	O
you	O
may	O
(	O
cid:12	O
)	O
nd	O
it	O
helpful	O
to	O
consider	O
the	O
following	O
questions	O
:	O
(	O
a	O
)	O
how	O
can	O
one	O
measure	O
information	B
?	O
(	O
b	O
)	O
when	O
you	O
have	O
identi	O
(	O
cid:12	O
)	O
ed	O
the	O
odd	O
ball	O
and	O
whether	O
it	O
is	O
heavy	O
or	O
light	O
,	O
how	O
much	O
information	O
have	O
you	O
gained	O
?	O
(	O
c	O
)	O
once	O
you	O
have	O
designed	O
a	O
strategy	O
,	O
draw	O
a	O
tree	B
showing	O
,	O
for	O
each	O
of	O
the	O
possible	O
outcomes	O
of	O
a	O
weighing	O
,	O
what	O
weighing	O
you	O
perform	O
next	O
.	O
at	O
each	O
node	O
in	O
the	O
tree	B
,	O
how	O
much	O
information	O
have	O
the	O
outcomes	O
so	O
far	O
given	O
you	O
,	O
and	O
how	O
much	O
information	B
remains	O
to	O
be	O
gained	O
?	O
(	O
d	O
)	O
how	O
much	O
information	O
is	O
gained	O
when	O
you	O
learn	O
(	O
i	O
)	O
the	O
state	O
of	O
a	O
(	O
cid:13	O
)	O
ipped	O
coin	B
;	O
(	O
ii	O
)	O
the	O
states	O
of	O
two	O
(	O
cid:13	O
)	O
ipped	O
coins	O
;	O
(	O
iii	O
)	O
the	O
outcome	O
when	O
a	O
four-sided	O
die	B
is	O
rolled	O
?	O
(	O
e	O
)	O
how	O
much	O
information	O
is	O
gained	O
on	O
the	O
(	O
cid:12	O
)	O
rst	O
step	O
of	O
the	O
weighing	O
problem	O
if	O
6	O
balls	O
are	O
weighed	O
against	O
the	O
other	O
6	O
?	O
how	O
much	O
is	O
gained	O
if	O
4	O
are	O
weighed	O
against	O
4	O
on	O
the	O
(	O
cid:12	O
)	O
rst	O
step	O
,	O
leaving	O
out	O
4	O
balls	O
?	O
66	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
4	O
the	O
source	B
coding	I
theorem	I
4.1	O
how	B
to	I
measure	I
the	O
information	B
content	I
of	O
a	O
random	B
variable	I
?	O
in	O
the	O
next	O
few	O
chapters	O
,	O
we	O
’	O
ll	O
be	O
talking	O
about	O
probability	B
distributions	I
and	O
random	B
variables	O
.	O
most	O
of	O
the	O
time	O
we	O
can	O
get	O
by	O
with	O
sloppy	O
notation	B
,	O
but	O
occasionally	O
,	O
we	O
will	O
need	O
precise	O
notation	B
.	O
here	O
is	O
the	O
notation	B
that	O
we	O
established	O
in	O
chapter	O
2.	O
an	O
ensemble	B
x	O
is	O
a	O
triple	O
(	O
x	O
;	O
ax	O
;	O
px	O
)	O
,	O
where	O
the	O
outcome	O
x	O
is	O
the	O
value	O
of	O
a	O
random	B
variable	I
,	O
which	O
takes	O
on	O
one	O
of	O
a	O
set	B
of	O
possible	O
values	O
,	O
ax	O
=	O
fa1	O
;	O
a2	O
;	O
:	O
:	O
:	O
;	O
ai	O
;	O
:	O
:	O
:	O
;	O
aig	O
,	O
having	O
probabilities	O
px	O
=	O
fp1	O
;	O
p2	O
;	O
:	O
:	O
:	O
;	O
pig	O
,	O
with	O
p	O
(	O
x	O
=	O
ai	O
)	O
=	O
pi	O
,	O
pi	O
(	O
cid:21	O
)	O
0	O
and	O
pai2ax	O
how	O
can	O
we	O
measure	O
the	O
information	B
content	I
of	O
an	O
outcome	O
x	O
=	O
ai	O
from	O
such	O
an	O
ensemble	B
?	O
in	O
this	O
chapter	O
we	O
examine	O
the	O
assertions	O
p	O
(	O
x	O
=	O
ai	O
)	O
=	O
1	O
.	O
1.	O
that	O
the	O
shannon	O
information	O
content	B
,	O
h	O
(	O
x	O
=	O
ai	O
)	O
(	O
cid:17	O
)	O
log2	O
1	O
pi	O
;	O
(	O
4.1	O
)	O
is	O
a	O
sensible	O
measure	O
of	O
the	O
information	O
content	B
of	O
the	O
outcome	O
x	O
=	O
ai	O
,	O
and	O
2.	O
that	O
the	O
entropy	B
of	O
the	O
ensemble	B
,	O
h	O
(	O
x	O
)	O
=xi	O
pi	O
log2	O
1	O
pi	O
;	O
(	O
4.2	O
)	O
is	O
a	O
sensible	O
measure	O
of	O
the	O
ensemble	O
’	O
s	O
average	B
information	O
content	B
.	O
10	O
h	O
(	O
p	O
)	O
=	O
log2	O
1	O
p	O
8	O
6	O
4	O
2	O
0	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
p	O
p	O
h	O
(	O
p	O
)	O
h2	O
(	O
p	O
)	O
0.001	O
0.01	O
0.1	O
0.2	O
0.5	O
10.0	O
6.6	O
3.3	O
2.3	O
1.0	O
0.011	O
0.081	O
0.47	O
0.72	O
1.0	O
h2	O
(	O
p	O
)	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
p	O
figure	O
4.1.	O
the	O
shannon	O
information	O
content	B
h	O
(	O
p	O
)	O
=	O
log2	O
and	O
the	O
binary	B
entropy	I
function	I
h2	O
(	O
p	O
)	O
=	O
h	O
(	O
p	O
;	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
=	O
p	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
log2	O
p	O
log2	O
function	B
of	O
p.	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
as	O
a	O
1	O
1	O
1	O
p	O
figure	O
4.1	O
shows	O
the	O
shannon	O
information	O
content	B
of	O
an	O
outcome	O
with	O
prob-	O
ability	O
p	O
,	O
as	O
a	O
function	B
of	O
p.	O
the	O
less	O
probable	O
an	O
outcome	O
is	O
,	O
the	O
greater	O
its	O
shannon	O
information	O
content	B
.	O
figure	O
4.1	O
also	O
shows	O
the	O
binary	B
entropy	I
function	I
,	O
h2	O
(	O
p	O
)	O
=	O
h	O
(	O
p	O
;	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
=	O
p	O
log2	O
1	O
p	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
log2	O
1	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
;	O
(	O
4.3	O
)	O
which	O
is	O
the	O
entropy	B
of	O
the	O
ensemble	B
x	O
whose	O
alphabet	O
and	O
probability	O
dis-	O
tribution	O
are	O
ax	O
=	O
fa	O
;	O
bg	O
;	O
px	O
=	O
fp	O
;	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
g.	O
67	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
68	O
4	O
|	O
the	O
source	B
coding	I
theorem	I
information	O
content	B
of	O
independent	O
random	O
variables	O
why	O
should	O
log	O
1=pi	O
have	O
anything	O
to	O
do	O
with	O
the	O
information	B
content	I
?	O
why	O
not	O
some	O
other	O
function	B
of	O
pi	O
?	O
we	O
’	O
ll	O
explore	B
this	O
question	O
in	O
detail	O
shortly	O
,	O
but	O
(	O
cid:12	O
)	O
rst	O
,	O
notice	O
a	O
nice	O
property	O
of	O
this	O
particular	O
function	B
h	O
(	O
x	O
)	O
=	O
log	O
1=p	O
(	O
x	O
)	O
.	O
imagine	O
learning	B
the	O
value	O
of	O
two	O
independent	O
random	O
variables	O
,	O
x	O
and	O
y.	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
independence	O
is	O
that	O
the	O
probability	B
distribution	O
is	O
separable	O
into	O
a	O
product	O
:	O
p	O
(	O
x	O
;	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
:	O
(	O
4.4	O
)	O
intuitively	O
,	O
we	O
might	O
want	O
any	O
measure	O
of	O
the	O
‘	O
amount	O
of	O
information	O
gained	O
’	O
to	O
have	O
the	O
property	O
of	O
additivity	O
{	O
that	O
is	O
,	O
for	O
independent	O
random	B
variables	O
x	O
and	O
y	O
,	O
the	O
information	B
gained	O
when	O
we	O
learn	O
x	O
and	O
y	O
should	O
equal	O
the	O
sum	O
of	O
the	O
information	B
gained	O
if	O
x	O
alone	O
were	O
learned	O
and	O
the	O
information	B
gained	O
if	O
y	O
alone	O
were	O
learned	O
.	O
the	O
shannon	O
information	O
content	B
of	O
the	O
outcome	O
x	O
;	O
y	O
is	O
h	O
(	O
x	O
;	O
y	O
)	O
=	O
log	O
1	O
p	O
(	O
x	O
;	O
y	O
)	O
=	O
log	O
1	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
=	O
log	O
1	O
p	O
(	O
x	O
)	O
+	O
log	O
1	O
p	O
(	O
y	O
)	O
(	O
4.5	O
)	O
so	O
it	O
does	O
indeed	O
satisfy	O
h	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
+	O
h	O
(	O
y	O
)	O
;	O
if	O
x	O
and	O
y	O
are	O
independent	O
.	O
(	O
4.6	O
)	O
exercise	O
4.2	O
.	O
[	O
1	O
,	O
p.86	O
]	O
show	O
that	O
,	O
if	O
x	O
and	O
y	O
are	O
independent	O
,	O
the	O
entropy	B
of	O
the	O
outcome	O
x	O
;	O
y	O
satis	O
(	O
cid:12	O
)	O
es	O
h	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
+	O
h	O
(	O
y	O
)	O
:	O
(	O
4.7	O
)	O
in	O
words	O
,	O
entropy	B
is	O
additive	O
for	O
independent	O
variables	O
.	O
we	O
now	O
explore	B
these	O
ideas	O
with	O
some	O
examples	O
;	O
then	O
,	O
in	O
section	O
4.4	O
and	O
in	O
chapters	O
5	O
and	O
6	O
,	O
we	O
prove	O
that	O
the	O
shannon	O
information	O
content	B
and	O
the	O
entropy	B
are	O
related	O
to	O
the	O
number	O
of	O
bits	O
needed	O
to	O
describe	O
the	O
outcome	O
of	O
an	O
experiment	O
.	O
the	O
weighing	B
problem	I
:	O
designing	O
informative	O
experiments	O
have	O
you	O
solved	O
the	O
weighing	B
problem	I
(	O
exercise	O
4.1	O
,	O
p.66	O
)	O
yet	O
?	O
are	O
you	O
sure	O
?	O
notice	O
that	O
in	O
three	O
uses	O
of	O
the	O
balance	B
{	O
which	O
reads	O
either	O
‘	O
left	O
heavier	O
’	O
,	O
‘	O
right	O
heavier	O
’	O
,	O
or	O
‘	O
balanced	O
’	O
{	O
the	O
number	O
of	O
conceivable	O
outcomes	O
is	O
33	O
=	O
27	O
,	O
whereas	O
the	O
number	O
of	O
possible	O
states	O
of	O
the	O
world	O
is	O
24	O
:	O
the	O
odd	O
ball	O
could	O
be	O
any	O
of	O
twelve	O
balls	O
,	O
and	O
it	O
could	O
be	O
heavy	O
or	O
light	O
.	O
so	O
in	O
principle	O
,	O
the	O
problem	O
might	O
be	O
solvable	O
in	O
three	O
weighings	O
{	O
but	O
not	O
in	O
two	O
,	O
since	O
32	O
<	O
24.	O
if	O
you	O
know	O
how	O
you	O
can	O
determine	O
the	O
odd	O
weight	B
and	O
whether	O
it	O
is	O
heavy	O
or	O
light	O
in	O
three	O
weighings	O
,	O
then	O
you	O
may	O
read	O
on	O
.	O
if	O
you	O
haven	O
’	O
t	O
found	O
a	O
strategy	O
that	O
always	O
gets	O
there	O
in	O
three	O
weighings	O
,	O
i	O
encourage	O
you	O
to	O
think	O
about	O
exercise	O
4.1	O
some	O
more	O
.	O
why	O
is	O
your	O
strategy	O
optimal	B
?	O
what	O
is	O
it	O
about	O
your	O
series	O
of	O
weighings	O
that	O
allows	O
useful	O
information	B
to	O
be	O
gained	O
as	O
quickly	O
as	O
possible	O
?	O
the	O
answer	O
is	O
that	O
at	O
each	O
step	O
of	O
an	O
optimal	B
procedure	O
,	O
the	O
three	O
outcomes	O
(	O
‘	O
left	O
heavier	O
’	O
,	O
‘	O
right	O
heavier	O
’	O
,	O
and	O
‘	O
balance	B
’	O
)	O
are	O
as	O
close	O
as	O
possible	O
to	O
equiprobable	O
.	O
an	O
optimal	B
solution	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
4.2.	O
suboptimal	O
strategies	O
,	O
such	O
as	O
weighing	O
balls	O
1	O
{	O
6	O
against	O
7	O
{	O
12	O
on	O
the	O
(	O
cid:12	O
)	O
rst	O
step	O
,	O
do	O
not	O
achieve	O
all	O
outcomes	O
with	O
equal	O
probability	B
:	O
these	O
two	O
sets	O
of	O
balls	O
can	O
never	O
balance	B
,	O
so	O
the	O
only	O
possible	O
outcomes	O
are	O
‘	O
left	O
heavy	O
’	O
and	O
‘	O
right	O
heavy	O
’	O
.	O
such	O
a	O
binary	O
outcome	O
rules	B
out	O
only	O
half	O
of	O
the	O
possible	O
hypotheses	O
,	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
4.1	O
:	O
how	B
to	I
measure	I
the	O
information	B
content	I
of	O
a	O
random	B
variable	I
?	O
69	O
1+	O
2+	O
3+	O
4+	O
5+	O
6+	O
7+	O
8+	O
9+	O
10+	O
11+	O
12+	O
1	O
(	O
cid:0	O
)	O
2	O
(	O
cid:0	O
)	O
3	O
(	O
cid:0	O
)	O
4	O
(	O
cid:0	O
)	O
5	O
(	O
cid:0	O
)	O
6	O
(	O
cid:0	O
)	O
7	O
(	O
cid:0	O
)	O
8	O
(	O
cid:0	O
)	O
9	O
(	O
cid:0	O
)	O
10	O
(	O
cid:0	O
)	O
11	O
(	O
cid:0	O
)	O
12	O
(	O
cid:0	O
)	O
weigh	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
(	O
cid:2	O
)	O
b	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
b	O
b	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:14	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
-	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
b	O
b	O
b	O
b	O
b	O
b	O
b	O
b	O
b	O
b	O
bbn	O
1+	O
2+	O
3+	O
4+	O
5	O
(	O
cid:0	O
)	O
6	O
(	O
cid:0	O
)	O
7	O
(	O
cid:0	O
)	O
8	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
2	O
(	O
cid:0	O
)	O
3	O
(	O
cid:0	O
)	O
4	O
(	O
cid:0	O
)	O
5+	O
6+	O
7+	O
8+	O
9+	O
10+	O
11+	O
12+	O
9	O
(	O
cid:0	O
)	O
10	O
(	O
cid:0	O
)	O
11	O
(	O
cid:0	O
)	O
12	O
(	O
cid:0	O
)	O
weigh	O
1	O
2	O
6	O
3	O
4	O
5	O
weigh	O
1	O
2	O
6	O
3	O
4	O
5	O
weigh	O
9	O
10	O
11	O
1	O
2	O
3	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
a	O
a	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
-	O
a	O
a	O
au	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
a	O
a	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
-	O
a	O
a	O
au	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
a	O
a	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
-	O
a	O
a	O
au	O
1+2+5	O
(	O
cid:0	O
)	O
3+4+6	O
(	O
cid:0	O
)	O
7	O
(	O
cid:0	O
)	O
8	O
(	O
cid:0	O
)	O
6+3	O
(	O
cid:0	O
)	O
4	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
2	O
(	O
cid:0	O
)	O
5+	O
7+8+	O
9+10+11+	O
9	O
(	O
cid:0	O
)	O
10	O
(	O
cid:0	O
)	O
11	O
(	O
cid:0	O
)	O
12+12	O
(	O
cid:0	O
)	O
1	O
2	O
3	O
4	O
1	O
7	O
3	O
4	O
1	O
2	O
7	O
1	O
9	O
10	O
9	O
10	O
12	O
1	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
-	O
@	O
r	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
-	O
@	O
r	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
-	O
@	O
r	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
-	O
@	O
r	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
-	O
@	O
r	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
-	O
@	O
r	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
-	O
@	O
r	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
-	O
@	O
r	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
-	O
@	O
r	O
1+	O
2+	O
5	O
(	O
cid:0	O
)	O
3+	O
4+	O
6	O
(	O
cid:0	O
)	O
7	O
(	O
cid:0	O
)	O
8	O
(	O
cid:0	O
)	O
?	O
4	O
(	O
cid:0	O
)	O
3	O
(	O
cid:0	O
)	O
6+	O
2	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
5+	O
7+	O
8+	O
?	O
9+	O
10+	O
11+	O
10	O
(	O
cid:0	O
)	O
9	O
(	O
cid:0	O
)	O
11	O
(	O
cid:0	O
)	O
12+	O
12	O
(	O
cid:0	O
)	O
?	O
figure	O
4.2.	O
an	O
optimal	B
solution	O
to	O
the	O
weighing	B
problem	I
.	O
at	O
each	O
step	O
there	O
are	O
two	O
boxes	O
:	O
the	O
left	O
box	B
shows	O
which	O
hypotheses	O
are	O
still	O
possible	O
;	O
the	O
right	O
box	B
shows	O
the	O
balls	O
involved	O
in	O
the	O
next	O
weighing	O
.	O
the	O
24	O
hypotheses	O
are	O
written	O
1+	O
;	O
:	O
:	O
:	O
;	O
12	O
(	O
cid:0	O
)	O
,	O
with	O
,	O
e.g.	O
,	O
1+	O
denoting	O
that	O
1	O
is	O
the	O
odd	O
ball	O
and	O
it	O
is	O
heavy	O
.	O
weighings	O
are	O
written	O
by	O
listing	O
the	O
names	O
of	O
the	O
balls	O
on	O
the	O
two	O
pans	O
,	O
separated	O
by	O
a	O
line	O
;	O
for	O
example	O
,	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
weighing	O
,	O
balls	O
1	O
,	O
2	O
,	O
3	O
,	O
and	O
4	O
are	O
put	O
on	O
the	O
left-hand	O
side	O
and	O
5	O
,	O
6	O
,	O
7	O
,	O
and	O
8	O
on	O
the	O
right	O
.	O
in	O
each	O
triplet	O
of	O
arrows	O
the	O
upper	O
arrow	O
leads	O
to	O
the	O
situation	O
when	O
the	O
left	O
side	O
is	O
heavier	O
,	O
the	O
middle	O
arrow	O
to	O
the	O
situation	O
when	O
the	O
right	O
side	O
is	O
heavier	O
,	O
and	O
the	O
lower	O
arrow	O
to	O
the	O
situation	O
when	O
the	O
outcome	O
is	O
balanced	O
.	O
the	O
three	O
points	O
labelled	O
?	O
correspond	O
to	O
impossible	O
outcomes	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
70	O
4	O
|	O
the	O
source	B
coding	I
theorem	I
so	O
a	O
strategy	O
that	O
uses	O
such	O
outcomes	O
must	O
sometimes	O
take	O
longer	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
right	O
answer	O
.	O
the	O
insight	O
that	O
the	O
outcomes	O
should	O
be	O
as	O
near	O
as	O
possible	O
to	O
equiprobable	O
makes	O
it	O
easier	O
to	O
search	O
for	O
an	O
optimal	B
strategy	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
weighing	O
must	O
divide	O
the	O
24	O
possible	O
hypotheses	O
into	O
three	O
groups	O
of	O
eight	O
.	O
then	O
the	O
second	O
weighing	O
must	O
be	O
chosen	O
so	O
that	O
there	O
is	O
a	O
3:3:2	O
split	O
of	O
the	O
hypotheses	O
.	O
thus	O
we	O
might	O
conclude	O
:	O
the	O
outcome	O
of	O
a	O
random	B
experiment	O
is	O
guaranteed	O
to	O
be	O
most	O
in-	O
formative	O
if	O
the	O
probability	B
distribution	O
over	O
outcomes	O
is	O
uniform	O
.	O
this	O
conclusion	O
agrees	O
with	O
the	O
property	O
of	O
the	O
entropy	O
that	O
you	O
proved	O
when	O
you	O
solved	O
exercise	O
2.25	O
(	O
p.37	O
)	O
:	O
the	O
entropy	B
of	O
an	O
ensemble	B
x	O
is	O
biggest	O
if	O
all	O
the	O
outcomes	O
have	O
equal	O
probability	B
pi	O
=	O
1=jaxj	O
.	O
guessing	B
games	O
in	O
the	O
game	B
of	O
twenty	B
questions	I
,	O
one	O
player	O
thinks	O
of	O
an	O
object	O
,	O
and	O
the	O
other	O
player	O
attempts	O
to	O
guess	O
what	O
the	O
object	O
is	O
by	O
asking	O
questions	O
that	O
have	O
yes/no	O
answers	O
,	O
for	O
example	O
,	O
‘	O
is	O
it	O
alive	O
?	O
’	O
,	O
or	O
‘	O
is	O
it	O
human	B
?	O
’	O
the	O
aim	O
is	O
to	O
identify	O
the	O
object	O
with	O
as	O
few	O
questions	O
as	O
possible	O
.	O
what	O
is	O
the	O
best	O
strategy	O
for	O
playing	O
this	O
game	B
?	O
for	O
simplicity	O
,	O
imagine	O
that	O
we	O
are	O
playing	O
the	O
rather	O
dull	O
version	O
of	O
twenty	O
questions	O
called	O
‘	O
sixty-three	B
’	O
.	O
example	O
4.3.	O
the	O
game	B
‘	O
sixty-three	B
’	O
.	O
what	O
’	O
s	O
the	O
smallest	O
number	O
of	O
yes/no	O
questions	O
needed	O
to	O
identify	O
an	O
integer	O
x	O
between	O
0	O
and	O
63	O
?	O
intuitively	O
,	O
the	O
best	O
questions	O
successively	O
divide	O
the	O
64	O
possibilities	O
into	O
equal	O
sized	O
sets	O
.	O
six	B
questions	O
su	O
(	O
cid:14	O
)	O
ce	O
.	O
one	O
reasonable	O
strategy	O
asks	O
the	O
following	O
questions	O
:	O
1	O
:	O
is	O
x	O
(	O
cid:21	O
)	O
32	O
?	O
2	O
:	O
is	O
x	O
mod	O
32	O
(	O
cid:21	O
)	O
16	O
?	O
3	O
:	O
is	O
x	O
mod	O
16	O
(	O
cid:21	O
)	O
8	O
?	O
4	O
:	O
is	O
x	O
mod	O
8	O
(	O
cid:21	O
)	O
4	O
?	O
5	O
:	O
is	O
x	O
mod	O
4	O
(	O
cid:21	O
)	O
2	O
?	O
6	O
:	O
is	O
x	O
mod	O
2	O
=	O
1	O
?	O
[	O
the	O
notation	B
x	O
mod	O
32	O
,	O
pronounced	O
‘	O
x	O
modulo	O
32	O
’	O
,	O
denotes	O
the	O
remainder	O
when	O
x	O
is	O
divided	O
by	O
32	O
;	O
for	O
example	O
,	O
35	O
mod	O
32	O
=	O
3	O
and	O
32	O
mod	O
32	O
=	O
0	O
.	O
]	O
the	O
answers	O
to	O
these	O
questions	O
,	O
if	O
translated	O
from	O
fyes	O
;	O
nog	O
to	O
f1	O
;	O
0g	O
,	O
give	O
the	O
binary	O
expansion	O
of	O
x	O
,	O
for	O
example	O
35	O
)	O
100011	O
.	O
2	O
what	O
are	O
the	O
shannon	O
information	O
contents	O
of	O
the	O
outcomes	O
in	O
this	O
ex-	O
ample	O
?	O
if	O
we	O
assume	O
that	O
all	O
values	O
of	O
x	O
are	O
equally	O
likely	O
,	O
then	O
the	O
answers	O
to	O
the	O
questions	O
are	O
independent	O
and	O
each	O
has	O
shannon	O
information	O
content	B
log2	O
(	O
1=0:5	O
)	O
=	O
1	O
bit	B
;	O
the	O
total	O
shannon	O
information	O
gained	O
is	O
always	O
six	B
bits	O
.	O
furthermore	O
,	O
the	O
number	O
x	O
that	O
we	O
learn	O
from	O
these	O
questions	O
is	O
a	O
six-bit	O
bi-	O
nary	O
number	O
.	O
our	O
questioning	O
strategy	O
de	O
(	O
cid:12	O
)	O
nes	O
a	O
way	O
of	O
encoding	O
the	O
random	B
variable	I
x	O
as	O
a	O
binary	O
(	O
cid:12	O
)	O
le	O
.	O
so	O
far	O
,	O
the	O
shannon	O
information	O
content	B
makes	O
sense	O
:	O
it	O
measures	O
the	O
length	B
of	O
a	O
binary	O
(	O
cid:12	O
)	O
le	O
that	O
encodes	O
x.	O
however	O
,	O
we	O
have	O
not	O
yet	O
studied	O
ensembles	O
where	O
the	O
outcomes	O
have	O
unequal	O
probabilities	O
.	O
does	O
the	O
shannon	O
information	O
content	B
make	O
sense	O
there	O
too	O
?	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
4.1	O
:	O
how	B
to	I
measure	I
the	O
information	B
content	I
of	O
a	O
random	B
variable	I
?	O
71	O
32	O
e5	O
x	O
=	O
n	O
32	O
33	O
0.0443	O
1.0	O
48	O
f3	O
x	O
=	O
n	O
16	O
17	O
0.0874	O
2.0	O
x	O
=	O
y	O
1	O
16	O
4.0	O
6.0	O
figure	O
4.3.	O
a	O
game	B
of	O
submarine	B
.	O
the	O
submarine	B
is	O
hit	O
on	O
the	O
49th	O
attempt	O
.	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
(	O
cid:2	O
)	O
j	O
87654321	O
1	O
g3	O
x	O
=	O
n	O
63	O
64	O
0.0227	O
0.0227	O
(	O
cid:2	O
)	O
j	O
(	O
cid:2	O
)	O
2	O
b1	O
x	O
=	O
n	O
62	O
63	O
0.0230	O
0.0458	O
move	O
#	O
question	O
outcome	O
p	O
(	O
x	O
)	O
h	O
(	O
x	O
)	O
total	O
info	O
.	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
j	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
j	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
js	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
49	O
h3	O
the	O
game	B
of	O
submarine	B
:	O
how	O
many	O
bits	O
can	O
one	O
bit	B
convey	O
?	O
in	O
the	O
game	B
of	O
battleships	B
,	O
each	O
player	O
hides	O
a	O
(	O
cid:13	O
)	O
eet	O
of	O
ships	O
in	O
a	O
sea	O
represented	O
by	O
a	O
square	B
grid	O
.	O
on	O
each	O
turn	O
,	O
one	O
player	O
attempts	O
to	O
hit	O
the	O
other	O
’	O
s	O
ships	O
by	O
(	O
cid:12	O
)	O
ring	O
at	O
one	O
square	B
in	O
the	O
opponent	O
’	O
s	O
sea	O
.	O
the	O
response	O
to	O
a	O
selected	O
square	B
such	O
as	O
‘	O
g3	O
’	O
is	O
either	O
‘	O
miss	O
’	O
,	O
‘	O
hit	O
’	O
,	O
or	O
‘	O
hit	O
and	O
destroyed	O
’	O
.	O
in	O
a	O
boring	O
version	O
of	O
battleships	O
called	O
submarine	B
,	O
each	O
player	O
hides	O
just	O
one	O
submarine	B
in	O
one	O
square	B
of	O
an	O
eight-by-eight	O
grid	O
.	O
figure	O
4.3	O
shows	O
a	O
few	O
pictures	O
of	O
this	O
game	B
in	O
progress	O
:	O
the	O
circle	B
represents	O
the	O
square	B
that	O
is	O
being	O
(	O
cid:12	O
)	O
red	O
at	O
,	O
and	O
the	O
(	O
cid:2	O
)	O
s	O
show	O
squares	O
in	O
which	O
the	O
outcome	O
was	O
a	O
miss	O
,	O
x	O
=	O
n	O
;	O
the	O
submarine	B
is	O
hit	O
(	O
outcome	O
x	O
=	O
y	O
shown	O
by	O
the	O
symbol	O
s	O
)	O
on	O
the	O
49th	O
attempt	O
.	O
each	O
shot	O
made	O
by	O
a	O
player	O
de	O
(	O
cid:12	O
)	O
nes	O
an	O
ensemble	B
.	O
the	O
two	O
possible	O
out-	O
comes	O
are	O
fy	O
;	O
ng	O
,	O
corresponding	O
to	O
a	O
hit	O
and	O
a	O
miss	O
,	O
and	O
their	O
probabili-	O
ties	O
depend	O
on	O
the	O
state	O
of	O
the	O
board	O
.	O
at	O
the	O
beginning	O
,	O
p	O
(	O
y	O
)	O
=	O
1=64	O
and	O
p	O
(	O
n	O
)	O
=	O
63=64	O
.	O
at	O
the	O
second	O
shot	O
,	O
if	O
the	O
(	O
cid:12	O
)	O
rst	O
shot	O
missed	O
,	O
p	O
(	O
y	O
)	O
=	O
1=63	O
and	O
p	O
(	O
n	O
)	O
=	O
62=63	O
.	O
at	O
the	O
third	O
shot	O
,	O
if	O
the	O
(	O
cid:12	O
)	O
rst	O
two	O
shots	O
missed	O
,	O
p	O
(	O
y	O
)	O
=	O
1=62	O
and	O
p	O
(	O
n	O
)	O
=	O
61=62	O
.	O
the	O
shannon	O
information	O
gained	O
from	O
an	O
outcome	O
x	O
is	O
h	O
(	O
x	O
)	O
=	O
log	O
(	O
1=p	O
(	O
x	O
)	O
)	O
.	O
if	O
we	O
are	O
lucky	O
,	O
and	O
hit	O
the	O
submarine	B
on	O
the	O
(	O
cid:12	O
)	O
rst	O
shot	O
,	O
then	O
h	O
(	O
x	O
)	O
=	O
h	O
(	O
1	O
)	O
(	O
y	O
)	O
=	O
log2	O
64	O
=	O
6	O
bits	O
:	O
(	O
4.8	O
)	O
now	O
,	O
it	O
might	O
seem	O
a	O
little	O
strange	O
that	O
one	O
binary	O
outcome	O
can	O
convey	O
six	B
bits	O
.	O
but	O
we	O
have	O
learnt	O
the	O
hiding	O
place	O
,	O
which	O
could	O
have	O
been	O
any	O
of	O
64	O
squares	O
;	O
so	O
we	O
have	O
,	O
by	O
one	O
lucky	O
binary	O
question	O
,	O
indeed	O
learnt	O
six	B
bits	O
.	O
what	O
if	O
the	O
(	O
cid:12	O
)	O
rst	O
shot	O
misses	O
?	O
the	O
shannon	O
information	O
that	O
we	O
gain	B
from	O
this	O
outcome	O
is	O
h	O
(	O
x	O
)	O
=	O
h	O
(	O
1	O
)	O
(	O
n	O
)	O
=	O
log2	O
64	O
63	O
=	O
0:0227	O
bits	O
:	O
(	O
4.9	O
)	O
does	O
this	O
make	O
sense	O
?	O
it	O
is	O
not	O
so	O
obvious	O
.	O
let	O
’	O
s	O
keep	O
going	O
.	O
if	O
our	O
second	O
shot	O
also	O
misses	O
,	O
the	O
shannon	O
information	O
content	B
of	O
the	O
second	O
outcome	O
is	O
h	O
(	O
2	O
)	O
(	O
n	O
)	O
=	O
log2	O
63	O
62	O
=	O
0:0230	O
bits	O
:	O
(	O
4.10	O
)	O
if	O
we	O
miss	O
thirty-two	O
times	O
(	O
(	O
cid:12	O
)	O
ring	O
at	O
a	O
new	O
square	B
each	O
time	O
)	O
,	O
the	O
total	O
shan-	O
non	O
information	B
gained	O
is	O
64	O
63	O
63	O
62	O
+	O
log2	O
log2	O
=	O
0:0227	O
+	O
0:0230	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
0:0430	O
=	O
1:0	O
bits	O
:	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
log2	O
33	O
32	O
(	O
4.11	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
72	O
4	O
|	O
the	O
source	B
coding	I
theorem	I
why	O
this	O
round	O
number	O
?	O
well	O
,	O
what	O
have	O
we	O
learnt	O
?	O
we	O
now	O
know	O
that	O
the	O
submarine	B
is	O
not	O
in	O
any	O
of	O
the	O
32	O
squares	O
we	O
(	O
cid:12	O
)	O
red	O
at	O
;	O
learning	B
that	O
fact	O
is	O
just	O
like	O
playing	O
a	O
game	B
of	O
sixty-three	B
(	O
p.70	O
)	O
,	O
asking	O
as	O
our	O
(	O
cid:12	O
)	O
rst	O
question	O
‘	O
is	O
x	O
one	O
of	O
the	O
thirty-two	O
numbers	O
corresponding	O
to	O
these	O
squares	O
i	O
(	O
cid:12	O
)	O
red	O
at	O
?	O
’	O
,	O
and	O
receiving	O
the	O
answer	O
‘	O
no	O
’	O
.	O
this	O
answer	O
rules	B
out	O
half	O
of	O
the	O
hypotheses	O
,	O
so	O
it	O
gives	O
us	O
one	O
bit	B
.	O
after	O
48	O
unsuccessful	O
shots	O
,	O
the	O
information	B
gained	O
is	O
2	O
bits	O
:	O
the	O
unknown	O
location	O
has	O
been	O
narrowed	O
down	O
to	O
one	O
quarter	O
of	O
the	O
original	O
hypothesis	O
space	O
.	O
what	O
if	O
we	O
hit	O
the	O
submarine	B
on	O
the	O
49th	O
shot	O
,	O
when	O
there	O
were	O
16	O
squares	O
left	O
?	O
the	O
shannon	O
information	O
content	B
of	O
this	O
outcome	O
is	O
h	O
(	O
49	O
)	O
(	O
y	O
)	O
=	O
log2	O
16	O
=	O
4:0	O
bits	O
:	O
(	O
4.12	O
)	O
the	O
total	O
shannon	O
information	O
content	B
of	O
all	O
the	O
outcomes	O
is	O
log2	O
64	O
63	O
+	O
log2	O
63	O
62	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
log2	O
17	O
16	O
+	O
log2	O
16	O
1	O
=	O
0:0227	O
+	O
0:0230	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
0:0874	O
+	O
4:0	O
=	O
6:0	O
bits	O
:	O
(	O
4.13	O
)	O
so	O
once	O
we	O
know	O
where	O
the	O
submarine	B
is	O
,	O
the	O
total	O
shannon	O
information	O
con-	O
tent	O
gained	O
is	O
6	O
bits	O
.	O
this	O
result	O
holds	O
regardless	O
of	O
when	O
we	O
hit	O
the	O
submarine	B
.	O
if	O
we	O
hit	O
it	O
when	O
there	O
are	O
n	O
squares	O
left	O
to	O
choose	O
from	O
{	O
n	O
was	O
16	O
in	O
equation	O
(	O
4.13	O
)	O
{	O
then	O
the	O
total	O
information	B
gained	O
is	O
:	O
log2	O
64	O
63	O
+	O
log2	O
63	O
62	O
=	O
log2	O
(	O
cid:20	O
)	O
64	O
63	O
(	O
cid:2	O
)	O
n	O
+	O
1	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
log2	O
63	O
62	O
(	O
cid:2	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:2	O
)	O
n	O
n	O
+	O
1	O
n	O
(	O
cid:2	O
)	O
+	O
log2	O
n	O
1	O
n	O
1	O
(	O
cid:21	O
)	O
=	O
log2	O
64	O
1	O
=	O
6	O
bits	O
:	O
(	O
4.14	O
)	O
what	O
have	O
we	O
learned	O
from	O
the	O
examples	O
so	O
far	O
?	O
i	O
think	O
the	O
submarine	B
example	O
makes	O
quite	O
a	O
convincing	O
case	O
for	O
the	O
claim	O
that	O
the	O
shannon	O
infor-	O
mation	O
content	B
is	O
a	O
sensible	O
measure	O
of	O
information	O
content	B
.	O
and	O
the	O
game	B
of	O
sixty-three	B
shows	O
that	O
the	O
shannon	O
information	O
content	B
can	O
be	O
intimately	O
connected	O
to	O
the	O
size	O
of	O
a	O
(	O
cid:12	O
)	O
le	O
that	O
encodes	O
the	O
outcomes	O
of	O
a	O
random	B
experi-	O
ment	O
,	O
thus	O
suggesting	O
a	O
possible	O
connection	O
to	O
data	B
compression	I
.	O
in	O
case	O
you	O
’	O
re	O
not	O
convinced	O
,	O
let	O
’	O
s	O
look	O
at	O
one	O
more	O
example	O
.	O
the	O
wenglish	O
language	O
wenglish	O
is	O
a	O
language	O
similar	O
to	O
english	O
.	O
wenglish	O
sentences	O
consist	O
of	O
words	O
drawn	O
at	O
random	B
from	O
the	O
wenglish	O
dictionary	B
,	O
which	O
contains	O
215	O
=	O
32,768	O
words	O
,	O
all	O
of	O
length	O
5	O
characters	O
.	O
each	O
word	O
in	O
the	O
wenglish	O
dictionary	B
was	O
constructed	O
at	O
random	B
by	O
picking	O
(	O
cid:12	O
)	O
ve	O
letters	O
from	O
the	O
probability	B
distribution	O
over	O
a	O
:	O
:	O
:	O
z	O
depicted	O
in	O
(	O
cid:12	O
)	O
gure	O
2.1.	O
some	O
entries	O
from	O
the	O
dictionary	B
are	O
shown	O
in	O
alphabetical	O
order	O
in	O
(	O
cid:12	O
)	O
g-	O
ure	O
4.4.	O
notice	O
that	O
the	O
number	O
of	O
words	O
in	O
the	O
dictionary	B
(	O
32,768	O
)	O
is	O
much	O
smaller	O
than	O
the	O
total	O
number	O
of	O
possible	O
words	O
of	O
length	O
5	O
letters	O
,	O
265	O
’	O
12,000,000.	O
because	O
the	O
probability	O
of	O
the	O
letter	O
z	O
is	O
about	O
1=1000	O
,	O
only	O
32	O
of	O
the	O
words	O
in	O
the	O
dictionary	B
begin	O
with	O
the	O
letter	O
z.	O
in	O
contrast	O
,	O
the	O
probability	O
of	O
the	O
letter	O
a	O
is	O
about	O
0:0625	O
,	O
and	O
2048	O
of	O
the	O
words	O
begin	O
with	O
the	O
letter	O
a.	O
of	O
those	O
2048	O
words	O
,	O
two	O
start	O
az	O
,	O
and	O
128	O
start	O
aa	O
.	O
let	O
’	O
s	O
imagine	O
that	O
we	O
are	O
reading	O
a	O
wenglish	O
document	O
,	O
and	O
let	O
’	O
s	O
discuss	O
the	O
shannon	O
information	O
content	B
of	O
the	O
characters	O
as	O
we	O
acquire	O
them	O
.	O
if	O
we	O
1	O
2	O
3	O
aaail	O
aaaiu	O
aaald	O
...	O
129	O
abati	O
...	O
2047	O
2048	O
azpan	O
aztdn	O
...	O
...	O
16	O
384	O
odrcr	O
...	O
...	O
32	O
737	O
zatnt	O
...	O
32	O
768	O
zxast	O
figure	O
4.4.	O
the	O
wenglish	O
dictionary	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
4.2	O
:	O
data	B
compression	I
73	O
are	O
given	O
the	O
text	O
one	O
word	O
at	O
a	O
time	O
,	O
the	O
shannon	O
information	O
content	B
of	O
each	O
(	O
cid:12	O
)	O
ve-character	O
word	O
is	O
log	O
32,768	O
=	O
15	O
bits	O
,	O
since	O
wenglish	O
uses	O
all	O
its	O
words	O
with	O
equal	O
probability	B
.	O
the	O
average	B
information	O
content	B
per	O
character	O
is	O
therefore	O
3	O
bits	O
.	O
now	O
let	O
’	O
s	O
look	O
at	O
the	O
information	B
content	I
if	O
we	O
read	O
the	O
document	O
one	O
character	O
at	O
a	O
time	O
.	O
if	O
,	O
say	O
,	O
the	O
(	O
cid:12	O
)	O
rst	O
letter	O
of	O
a	O
word	O
is	O
a	O
,	O
the	O
shannon	O
information	O
content	B
is	O
log	O
1=0:0625	O
’	O
4	O
bits	O
.	O
if	O
the	O
(	O
cid:12	O
)	O
rst	O
letter	O
is	O
z	O
,	O
the	O
shannon	O
information	O
content	B
is	O
log	O
1=0:001	O
’	O
10	O
bits	O
.	O
the	O
information	B
content	I
is	O
thus	O
highly	O
variable	O
at	O
the	O
(	O
cid:12	O
)	O
rst	O
character	O
.	O
the	O
total	O
information	B
content	I
of	O
the	O
5	O
characters	O
in	O
a	O
word	O
,	O
however	O
,	O
is	O
exactly	O
15	O
bits	O
;	O
so	O
the	O
letters	O
that	O
follow	O
an	O
initial	O
z	O
have	O
lower	O
average	B
information	O
content	B
per	O
character	O
than	O
the	O
letters	O
that	O
follow	O
an	O
initial	O
a.	O
a	O
rare	O
initial	O
letter	O
such	O
as	O
z	O
indeed	O
conveys	O
more	O
information	B
about	O
what	O
the	O
word	O
is	O
than	O
a	O
common	O
initial	O
letter	O
.	O
similarly	O
,	O
in	O
english	O
,	O
if	O
rare	O
characters	O
occur	O
at	O
the	O
start	O
of	O
the	O
word	O
(	O
e.g	O
.	O
xyl	O
...	O
)	O
,	O
then	O
often	O
we	O
can	O
identify	O
the	O
whole	O
word	O
immediately	O
;	O
whereas	O
words	O
that	O
start	O
with	O
common	O
characters	O
(	O
e.g	O
.	O
pro	O
...	O
)	O
require	O
more	O
charac-	O
ters	O
before	O
we	O
can	O
identify	O
them	O
.	O
4.2	O
data	B
compression	I
the	O
preceding	O
examples	O
justify	O
the	O
idea	O
that	O
the	O
shannon	O
information	O
content	B
of	O
an	O
outcome	O
is	O
a	O
natural	B
measure	O
of	O
its	O
information	B
content	I
.	O
improbable	O
out-	O
comes	O
do	O
convey	O
more	O
information	B
than	O
probable	O
outcomes	O
.	O
we	O
now	O
discuss	O
the	O
information	B
content	I
of	O
a	O
source	O
by	O
considering	O
how	O
many	O
bits	O
are	O
needed	O
to	O
describe	O
the	O
outcome	O
of	O
an	O
experiment	O
.	O
if	O
we	O
can	O
show	O
that	O
we	O
can	O
compress	B
data	O
from	O
a	O
particular	O
source	O
into	O
a	O
(	O
cid:12	O
)	O
le	O
of	O
l	O
bits	O
per	O
source	O
symbol	O
and	O
recover	O
the	O
data	O
reliably	O
,	O
then	O
we	O
will	O
say	O
that	O
the	O
average	B
information	O
content	B
of	O
that	O
source	O
is	O
at	O
most	O
l	O
bits	O
per	O
symbol	O
.	O
example	O
:	O
compression	B
of	O
text	O
(	O
cid:12	O
)	O
les	O
a	O
(	O
cid:12	O
)	O
le	O
is	O
composed	O
of	O
a	O
sequence	B
of	O
bytes	O
.	O
a	O
byte	B
is	O
composed	O
of	O
8	O
bits	O
and	O
can	O
have	O
a	O
decimal	O
value	O
between	O
0	O
and	O
255.	O
a	O
typical	B
text	O
(	O
cid:12	O
)	O
le	O
is	O
composed	O
of	O
the	O
ascii	O
character	O
set	B
(	O
decimal	O
values	O
0	O
to	O
127	O
)	O
.	O
this	O
character	O
set	B
uses	O
only	O
seven	O
of	O
the	O
eight	O
bits	O
in	O
a	O
byte	B
.	O
here	O
we	O
use	O
the	O
word	O
‘	O
bit	B
’	O
with	O
its	O
meaning	O
,	O
‘	O
a	O
symbol	O
with	O
two	O
values	O
’	O
,	O
not	O
to	O
be	O
confused	O
with	O
the	O
unit	O
of	O
information	O
content	B
.	O
.	O
exercise	O
4.4	O
.	O
[	O
1	O
,	O
p.86	O
]	O
by	O
how	O
much	O
could	O
the	O
size	O
of	O
a	O
(	O
cid:12	O
)	O
le	O
be	O
reduced	O
given	O
that	O
it	O
is	O
an	O
ascii	O
(	O
cid:12	O
)	O
le	O
?	O
how	O
would	O
you	O
achieve	O
this	O
reduction	O
?	O
intuitively	O
,	O
it	O
seems	O
reasonable	O
to	O
assert	O
that	O
an	O
ascii	O
(	O
cid:12	O
)	O
le	O
contains	O
7=8	O
as	O
much	O
information	B
as	O
an	O
arbitrary	O
(	O
cid:12	O
)	O
le	O
of	O
the	O
same	O
size	O
,	O
since	O
we	O
already	O
know	O
one	O
out	O
of	O
every	O
eight	O
bits	O
before	O
we	O
even	O
look	O
at	O
the	O
(	O
cid:12	O
)	O
le	O
.	O
this	O
is	O
a	O
simple	O
ex-	O
ample	O
of	O
redundancy	O
.	O
most	O
sources	O
of	O
data	O
have	O
further	O
redundancy	B
:	O
english	O
text	O
(	O
cid:12	O
)	O
les	O
use	O
the	O
ascii	O
characters	O
with	O
non-equal	O
frequency	B
;	O
certain	O
pairs	O
of	O
letters	O
are	O
more	O
probable	O
than	O
others	O
;	O
and	O
entire	O
words	O
can	O
be	O
predicted	O
given	O
the	O
context	O
and	O
a	O
semantic	O
understanding	O
of	O
the	O
text	O
.	O
some	O
simple	O
data	O
compression	B
methods	O
that	O
de	O
(	O
cid:12	O
)	O
ne	O
measures	O
of	O
informa-	O
tion	O
content	B
one	O
way	O
of	O
measuring	O
the	O
information	B
content	I
of	O
a	O
random	B
variable	I
is	O
simply	O
to	O
count	O
the	O
number	O
of	O
possible	O
outcomes	O
,	O
jaxj	O
.	O
(	O
the	O
number	O
of	O
elements	O
in	O
a	O
set	B
a	O
is	O
denoted	O
by	O
jaj	O
.	O
)	O
if	O
we	O
gave	O
a	O
binary	O
name	O
to	O
each	O
outcome	O
,	O
the	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
74	O
4	O
|	O
the	O
source	B
coding	I
theorem	I
length	O
of	O
each	O
name	O
would	O
be	O
log2	O
jaxj	O
bits	O
,	O
if	O
jaxj	O
happened	O
to	O
be	O
a	O
power	O
of	O
2.	O
we	O
thus	O
make	O
the	O
following	O
de	O
(	O
cid:12	O
)	O
nition	O
.	O
the	O
raw	O
bit	B
content	O
of	O
x	O
is	O
h0	O
(	O
x	O
)	O
=	O
log2	O
jaxj	O
:	O
(	O
4.15	O
)	O
h0	O
(	O
x	O
)	O
is	O
a	O
lower	O
bound	B
for	O
the	O
number	O
of	O
binary	O
questions	O
that	O
are	O
always	O
guaranteed	O
to	O
identify	O
an	O
outcome	O
from	O
the	O
ensemble	B
x.	O
it	O
is	O
an	O
additive	O
quantity	O
:	O
the	O
raw	O
bit	B
content	O
of	O
an	O
ordered	O
pair	O
x	O
;	O
y	O
,	O
having	O
jaxjjay	O
j	O
possible	O
outcomes	O
,	O
satis	O
(	O
cid:12	O
)	O
es	O
(	O
4.16	O
)	O
h0	O
(	O
x	O
;	O
y	O
)	O
=	O
h0	O
(	O
x	O
)	O
+	O
h0	O
(	O
y	O
)	O
:	O
this	O
measure	O
of	O
information	O
content	B
does	O
not	O
include	O
any	O
probabilistic	O
element	O
,	O
and	O
the	O
encoding	O
rule	O
it	O
corresponds	O
to	O
does	O
not	O
‘	O
compress	B
’	O
the	O
source	O
data	O
,	O
it	O
simply	O
maps	O
each	O
outcome	O
to	O
a	O
constant-length	O
binary	O
string	O
.	O
exercise	O
4.5	O
.	O
[	O
2	O
,	O
p.86	O
]	O
could	O
there	O
be	O
a	O
compressor	O
that	O
maps	O
an	O
outcome	O
x	O
to	O
a	O
binary	O
code	O
c	O
(	O
x	O
)	O
,	O
and	O
a	O
decompressor	O
that	O
maps	O
c	O
back	O
to	O
x	O
,	O
such	O
that	O
every	O
possible	O
outcome	O
is	O
compressed	O
into	O
a	O
binary	O
code	O
of	O
length	O
shorter	O
than	O
h0	O
(	O
x	O
)	O
bits	O
?	O
even	O
though	O
a	O
simple	O
counting	O
argument	O
shows	O
that	O
it	O
is	O
impossible	O
to	O
make	O
a	O
reversible	B
compression	O
program	O
that	O
reduces	O
the	O
size	O
of	O
all	O
(	O
cid:12	O
)	O
les	O
,	O
ama-	O
teur	O
compression	B
enthusiasts	O
frequently	O
announce	O
that	O
they	O
have	O
invented	O
a	O
program	O
that	O
can	O
do	O
this	O
{	O
indeed	O
that	O
they	O
can	O
further	O
compress	B
com-	O
pressed	O
(	O
cid:12	O
)	O
les	O
by	O
putting	O
them	O
through	O
their	O
compressor	O
several	O
times	O
.	O
stranger	O
yet	O
,	O
patents	O
have	O
been	O
granted	O
to	O
these	O
modern-day	O
alchemists	B
.	O
see	O
the	O
comp.compression	O
frequently	O
asked	O
questions	O
for	O
further	O
reading.1	O
there	O
are	O
only	O
two	O
ways	O
in	O
which	O
a	O
‘	O
compressor	O
’	O
can	O
actually	O
compress	B
(	O
cid:12	O
)	O
les	O
:	O
1.	O
a	O
lossy	B
compressor	O
compresses	O
some	O
(	O
cid:12	O
)	O
les	O
,	O
but	O
maps	O
some	O
(	O
cid:12	O
)	O
les	O
to	O
the	O
same	O
encoding	O
.	O
we	O
’	O
ll	O
assume	O
that	O
the	O
user	O
requires	O
perfect	B
recovery	O
of	O
the	O
source	O
(	O
cid:12	O
)	O
le	O
,	O
so	O
the	O
occurrence	O
of	O
one	O
of	O
these	O
confusable	O
(	O
cid:12	O
)	O
les	O
leads	O
to	O
a	O
failure	O
(	O
though	O
in	O
applications	O
such	O
as	O
image	O
compression	B
,	O
lossy	B
compression	I
is	O
viewed	O
as	O
satisfactory	O
)	O
.	O
we	O
’	O
ll	O
denote	O
by	O
(	O
cid:14	O
)	O
the	O
probability	B
that	O
the	O
source	O
string	O
is	O
one	O
of	O
the	O
confusable	O
(	O
cid:12	O
)	O
les	O
,	O
so	O
a	O
lossy	B
compressor	O
has	O
a	O
probability	B
(	O
cid:14	O
)	O
of	O
failure	O
.	O
if	O
(	O
cid:14	O
)	O
can	O
be	O
made	O
very	O
small	O
then	O
a	O
lossy	B
compressor	O
may	O
be	O
practically	O
useful	O
.	O
2.	O
a	O
lossless	B
compressor	O
maps	O
all	O
(	O
cid:12	O
)	O
les	O
to	O
di	O
(	O
cid:11	O
)	O
erent	O
encodings	O
;	O
if	O
it	O
shortens	O
some	O
(	O
cid:12	O
)	O
les	O
,	O
it	O
necessarily	O
makes	O
others	O
longer	O
.	O
we	O
try	O
to	O
design	O
the	O
compressor	O
so	O
that	O
the	O
probability	B
that	O
a	O
(	O
cid:12	O
)	O
le	O
is	O
lengthened	O
is	O
very	O
small	O
,	O
and	O
the	O
probability	B
that	O
it	O
is	O
shortened	O
is	O
large	O
.	O
in	O
this	O
chapter	O
we	O
discuss	O
a	O
simple	O
lossy	O
compressor	O
.	O
in	O
subsequent	O
chapters	O
we	O
discuss	O
lossless	B
compression	O
methods	B
.	O
4.3	O
information	B
content	I
de	O
(	O
cid:12	O
)	O
ned	O
in	O
terms	O
of	O
lossy	O
compression	B
whichever	O
type	O
of	O
compressor	O
we	O
construct	O
,	O
we	O
need	O
somehow	O
to	O
take	O
into	O
account	O
the	O
probabilities	O
of	O
the	O
di	O
(	O
cid:11	O
)	O
erent	O
outcomes	O
.	O
imagine	O
comparing	O
the	O
information	B
contents	O
of	O
two	O
text	O
(	O
cid:12	O
)	O
les	O
{	O
one	O
in	O
which	O
all	O
128	O
ascii	O
characters	O
1http	O
:	O
//sunsite.org.uk/public/usenet/news-faqs/comp.compression/	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
4.3	O
:	O
information	B
content	I
de	O
(	O
cid:12	O
)	O
ned	O
in	O
terms	O
of	O
lossy	O
compression	B
75	O
are	O
used	O
with	O
equal	O
probability	B
,	O
and	O
one	O
in	O
which	O
the	O
characters	O
are	O
used	O
with	O
their	O
frequencies	O
in	O
english	O
text	O
.	O
can	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
measure	O
of	O
information	O
content	B
that	O
distinguishes	O
between	O
these	O
two	O
(	O
cid:12	O
)	O
les	O
?	O
intuitively	O
,	O
the	O
latter	O
(	O
cid:12	O
)	O
le	O
contains	O
less	O
information	B
per	O
character	O
because	O
it	O
is	O
more	O
predictable	O
.	O
one	O
simple	O
way	O
to	O
use	O
our	O
knowledge	O
that	O
some	O
symbols	O
have	O
a	O
smaller	O
probability	B
is	O
to	O
imagine	O
recoding	O
the	O
observations	O
into	O
a	O
smaller	O
alphabet	O
{	O
thus	O
losing	O
the	O
ability	O
to	O
encode	O
some	O
of	O
the	O
more	O
improbable	O
symbols	O
{	O
and	O
then	O
measuring	O
the	O
raw	O
bit	B
content	O
of	O
the	O
new	O
alphabet	O
.	O
for	O
example	O
,	O
we	O
might	O
take	O
a	O
risk	O
when	O
compressing	O
english	O
text	O
,	O
guessing	B
that	O
the	O
most	O
infrequent	O
characters	O
won	O
’	O
t	O
occur	O
,	O
and	O
make	O
a	O
reduced	O
ascii	O
code	B
that	O
omits	O
the	O
characters	O
f	O
!	O
,	O
@	O
,	O
#	O
,	O
%	O
,	O
^	O
,	O
*	O
,	O
~	O
,	O
<	O
,	O
>	O
,	O
/	O
,	O
\	O
,	O
_	O
,	O
{	O
,	O
}	O
,	O
[	O
,	O
]	O
,	O
|	O
g	O
,	O
thereby	O
reducing	O
the	O
size	O
of	O
the	O
alphabet	O
by	O
seventeen	O
.	O
the	O
larger	O
the	O
risk	O
we	O
are	O
willing	O
to	O
take	O
,	O
the	O
smaller	O
our	O
(	O
cid:12	O
)	O
nal	O
alphabet	O
becomes	O
.	O
we	O
introduce	O
a	O
parameter	O
(	O
cid:14	O
)	O
that	O
describes	O
the	O
risk	O
we	O
are	O
taking	O
when	O
using	O
this	O
compression	B
method	O
:	O
(	O
cid:14	O
)	O
is	O
the	O
probability	B
that	O
there	O
will	O
be	O
no	O
name	O
for	O
an	O
outcome	O
x.	O
example	O
4.6.	O
let	O
4	O
;	O
1	O
4	O
;	O
1	O
(	O
4.17	O
)	O
ax	O
=f	O
a	O
;	O
b	O
;	O
c	O
;	O
d	O
;	O
e	O
;	O
f	O
;	O
g	O
;	O
h	O
g	O
;	O
64	O
g	O
:	O
and	O
px	O
=f	O
1	O
64	O
;	O
1	O
64	O
;	O
1	O
64	O
;	O
1	O
4	O
;	O
3	O
16	O
;	O
1	O
the	O
raw	O
bit	B
content	O
of	O
this	O
ensemble	B
is	O
3	O
bits	O
,	O
corresponding	O
to	O
8	O
binary	O
names	O
.	O
but	O
notice	O
that	O
p	O
(	O
x	O
2	O
fa	O
;	O
b	O
;	O
c	O
;	O
dg	O
)	O
=	O
15=16	O
.	O
so	O
if	O
we	O
are	O
willing	O
to	O
run	O
a	O
risk	O
of	O
(	O
cid:14	O
)	O
=	O
1=16	O
of	O
not	O
having	O
a	O
name	O
for	O
x	O
,	O
then	O
we	O
can	O
get	O
by	O
with	O
four	O
names	O
{	O
half	O
as	O
many	O
names	O
as	O
are	O
needed	O
if	O
every	O
x	O
2	O
ax	O
has	O
a	O
name	O
.	O
(	O
cid:14	O
)	O
=	O
0	O
(	O
cid:14	O
)	O
=	O
1=16	O
x	O
c	O
(	O
x	O
)	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
000	O
001	O
010	O
011	O
100	O
101	O
110	O
111	O
x	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
c	O
(	O
x	O
)	O
00	O
01	O
10	O
11	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
table	O
4.5	O
shows	O
binary	O
names	O
that	O
could	O
be	O
given	O
to	O
the	O
di	O
(	O
cid:11	O
)	O
erent	O
out-	O
comes	O
in	O
the	O
cases	O
(	O
cid:14	O
)	O
=	O
0	O
and	O
(	O
cid:14	O
)	O
=	O
1=16	O
.	O
when	O
(	O
cid:14	O
)	O
=	O
0	O
we	O
need	O
3	O
bits	O
to	O
encode	O
the	O
outcome	O
;	O
when	O
(	O
cid:14	O
)	O
=	O
1=16	O
we	O
need	O
only	O
2	O
bits	O
.	O
table	O
4.5.	O
binary	O
names	O
for	O
the	O
outcomes	O
,	O
for	O
two	O
failure	O
probabilities	O
(	O
cid:14	O
)	O
.	O
let	O
us	O
now	O
formalize	O
this	O
idea	O
.	O
to	O
make	O
a	O
compression	B
strategy	O
with	O
risk	O
(	O
cid:14	O
)	O
,	O
we	O
make	O
the	O
smallest	O
possible	O
subset	B
s	O
(	O
cid:14	O
)	O
such	O
that	O
the	O
probability	B
that	O
x	O
is	O
not	O
in	O
s	O
(	O
cid:14	O
)	O
is	O
less	O
than	O
or	O
equal	O
to	O
(	O
cid:14	O
)	O
,	O
i.e.	O
,	O
p	O
(	O
x	O
62	O
s	O
(	O
cid:14	O
)	O
)	O
(	O
cid:20	O
)	O
(	O
cid:14	O
)	O
.	O
for	O
each	O
value	O
of	O
(	O
cid:14	O
)	O
we	O
can	O
then	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
new	O
measure	O
of	O
information	O
content	B
{	O
the	O
log	O
of	O
the	O
size	O
of	O
this	O
smallest	O
subset	B
s	O
(	O
cid:14	O
)	O
.	O
[	O
in	O
ensembles	O
in	O
which	O
several	O
elements	O
have	O
the	O
same	O
probability	B
,	O
there	O
may	O
be	O
several	O
smallest	O
subsets	O
that	O
contain	O
di	O
(	O
cid:11	O
)	O
erent	O
elements	O
,	O
but	O
all	O
that	O
matters	O
is	O
their	O
sizes	O
(	O
which	O
are	O
equal	O
)	O
,	O
so	O
we	O
will	O
not	O
dwell	O
on	O
this	O
ambiguity	O
.	O
]	O
the	O
smallest	O
(	O
cid:14	O
)	O
-su	O
(	O
cid:14	O
)	O
cient	O
subset	B
s	O
(	O
cid:14	O
)	O
is	O
the	O
smallest	O
subset	B
of	O
ax	O
satisfying	O
(	O
4.18	O
)	O
p	O
(	O
x	O
2	O
s	O
(	O
cid:14	O
)	O
)	O
(	O
cid:21	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:14	O
)	O
:	O
the	O
subset	B
s	O
(	O
cid:14	O
)	O
can	O
be	O
constructed	O
by	O
ranking	O
the	O
elements	O
of	O
ax	O
in	O
order	O
of	O
decreasing	O
probability	B
and	O
adding	O
successive	O
elements	O
starting	O
from	O
the	O
most	O
probable	O
elements	O
until	O
the	O
total	O
probability	B
is	O
(	O
cid:21	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:14	O
)	O
)	O
.	O
we	O
can	O
make	O
a	O
data	B
compression	I
code	O
by	O
assigning	O
a	O
binary	O
name	O
to	O
each	O
element	O
of	O
the	O
smallest	O
su	O
(	O
cid:14	O
)	O
cient	O
subset	B
.	O
this	O
compression	B
scheme	O
motivates	O
the	O
following	O
measure	O
of	O
information	O
content	B
:	O
the	O
essential	O
bit	B
content	O
of	O
x	O
is	O
:	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
)	O
=	O
log2	O
js	O
(	O
cid:14	O
)	O
j	O
:	O
(	O
4.19	O
)	O
note	O
that	O
h0	O
(	O
x	O
)	O
is	O
the	O
special	O
case	O
of	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
)	O
with	O
(	O
cid:14	O
)	O
=	O
0	O
(	O
if	O
p	O
(	O
x	O
)	O
>	O
0	O
for	O
all	O
x	O
2	O
ax	O
)	O
.	O
[	O
caution	B
:	O
do	O
not	O
confuse	O
h0	O
(	O
x	O
)	O
and	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
)	O
with	O
the	O
function	B
h2	O
(	O
p	O
)	O
displayed	O
in	O
(	O
cid:12	O
)	O
gure	O
4.1	O
.	O
]	O
figure	O
4.6	O
shows	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
)	O
for	O
the	O
ensemble	B
of	O
example	O
4.6	O
as	O
a	O
function	B
of	O
(	O
cid:14	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
s0	O
(	O
cid:0	O
)	O
6	O
6	O
e	O
,	O
f	O
,	O
g	O
,	O
h	O
76	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
cid:0	O
)	O
4	O
(	O
cid:0	O
)	O
2:4	O
(	O
cid:0	O
)	O
2	O
log2	O
p	O
(	O
x	O
)	O
-	O
s	O
1	O
16	O
66	O
d	O
a	O
,	O
b	O
,	O
c	O
4	O
|	O
the	O
source	B
coding	I
theorem	I
figure	O
4.6	O
.	O
(	O
a	O
)	O
the	O
outcomes	O
of	O
x	O
(	O
from	O
example	O
4.6	O
(	O
p.75	O
)	O
)	O
,	O
ranked	O
by	O
their	O
probability	B
.	O
(	O
b	O
)	O
the	O
essential	O
bit	B
content	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
)	O
.	O
the	O
labels	O
on	O
the	O
graph	B
show	O
the	O
smallest	O
su	O
(	O
cid:14	O
)	O
cient	O
set	B
as	O
a	O
function	B
of	O
(	O
cid:14	O
)	O
.	O
note	O
h0	O
(	O
x	O
)	O
=	O
3	O
bits	O
and	O
h1=16	O
(	O
x	O
)	O
=	O
2	O
bits	O
.	O
3	O
2.5	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
)	O
2	O
1.5	O
1	O
0.5	O
0	O
0	O
{	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
,	O
g	O
,	O
h	O
}	O
{	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
,	O
g	O
}	O
{	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
}	O
{	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
}	O
{	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
}	O
{	O
a	O
,	O
b	O
,	O
c	O
}	O
{	O
a	O
,	O
b	O
}	O
{	O
a	O
}	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
(	O
cid:14	O
)	O
extended	B
ensembles	O
is	O
this	O
compression	B
method	O
any	O
more	O
useful	O
if	O
we	O
compress	B
blocks	O
of	O
symbols	O
from	O
a	O
source	O
?	O
we	O
now	O
turn	O
to	O
examples	O
where	O
the	O
outcome	O
x	O
=	O
(	O
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
)	O
is	O
a	O
string	O
of	O
n	O
independent	O
identically	O
distributed	O
random	B
variables	O
from	O
a	O
single	O
ensemble	O
x.	O
we	O
will	O
denote	O
by	O
x	O
n	O
the	O
ensemble	B
(	O
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
)	O
.	O
remem-	O
ber	O
that	O
entropy	B
is	O
additive	O
for	O
independent	O
variables	O
(	O
exercise	O
4.2	O
(	O
p.68	O
)	O
)	O
,	O
so	O
h	O
(	O
x	O
n	O
)	O
=	O
n	O
h	O
(	O
x	O
)	O
.	O
example	O
4.7.	O
consider	O
a	O
string	O
of	O
n	O
(	O
cid:13	O
)	O
ips	O
of	O
a	O
bent	B
coin	I
,	O
x	O
=	O
(	O
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
)	O
,	O
where	O
xn	O
2	O
f0	O
;	O
1g	O
,	O
with	O
probabilities	O
p0	O
=	O
0:9	O
;	O
p1	O
=	O
0:1.	O
the	O
most	O
prob-	O
able	O
strings	O
x	O
are	O
those	O
with	O
most	O
0s	O
.	O
if	O
r	O
(	O
x	O
)	O
is	O
the	O
number	O
of	O
1s	O
in	O
x	O
then	O
pr	O
(	O
x	O
)	O
1	O
:	O
p	O
(	O
x	O
)	O
=	O
pn	O
(	O
cid:0	O
)	O
r	O
(	O
x	O
)	O
0	O
(	O
4.20	O
)	O
to	O
evaluate	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
we	O
must	O
(	O
cid:12	O
)	O
nd	O
the	O
smallest	O
su	O
(	O
cid:14	O
)	O
cient	O
subset	B
s	O
(	O
cid:14	O
)	O
.	O
this	O
subset	B
will	O
contain	O
all	O
x	O
with	O
r	O
(	O
x	O
)	O
=	O
0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
up	O
to	O
some	O
rmax	O
(	O
(	O
cid:14	O
)	O
)	O
(	O
cid:0	O
)	O
1	O
,	O
and	O
some	O
of	O
the	O
x	O
with	O
r	O
(	O
x	O
)	O
=	O
rmax	O
(	O
(	O
cid:14	O
)	O
)	O
.	O
figures	O
4.7	O
and	O
4.8	O
show	O
graphs	O
of	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
against	O
(	O
cid:14	O
)	O
for	O
the	O
cases	O
n	O
=	O
4	O
and	O
n	O
=	O
10.	O
the	O
steps	O
are	O
the	O
values	O
of	O
(	O
cid:14	O
)	O
at	O
which	O
js	O
(	O
cid:14	O
)	O
j	O
changes	O
by	O
1	O
,	O
and	O
the	O
cusps	O
where	O
the	O
slope	O
of	O
the	O
staircase	O
changes	O
are	O
the	O
points	O
where	O
rmax	O
changes	O
by	O
1.	O
exercise	O
4.8	O
.	O
[	O
2	O
,	O
p.86	O
]	O
what	O
are	O
the	O
mathematical	O
shapes	O
of	O
the	O
curves	O
between	O
the	O
cusps	O
?	O
for	O
the	O
examples	O
shown	O
in	O
(	O
cid:12	O
)	O
gures	O
4.6	O
{	O
4.8	O
,	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
depends	O
strongly	O
on	O
the	O
value	O
of	O
(	O
cid:14	O
)	O
,	O
so	O
it	O
might	O
not	O
seem	O
a	O
fundamental	O
or	O
useful	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
information	O
content	B
.	O
but	O
we	O
will	O
consider	O
what	O
happens	O
as	O
n	O
,	O
the	O
number	O
of	O
independent	O
variables	O
in	O
x	O
n	O
,	O
increases	O
.	O
we	O
will	O
(	O
cid:12	O
)	O
nd	O
the	O
remarkable	O
result	O
that	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
becomes	O
almost	O
independent	O
of	O
(	O
cid:14	O
)	O
{	O
and	O
for	O
all	O
(	O
cid:14	O
)	O
it	O
is	O
very	O
close	O
to	O
n	O
h	O
(	O
x	O
)	O
,	O
where	O
h	O
(	O
x	O
)	O
is	O
the	O
entropy	B
of	O
one	O
of	O
the	O
random	O
variables	O
.	O
figure	O
4.9	O
illustrates	O
this	O
asymptotic	O
tendency	O
for	O
the	O
binary	O
ensemble	O
of	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
becomes	O
an	O
increasingly	O
(	O
cid:13	O
)	O
at	O
function	B
,	O
example	O
4.7.	O
as	O
n	O
increases	O
,	O
1	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
4.3	O
:	O
information	B
content	I
de	O
(	O
cid:12	O
)	O
ned	O
in	O
terms	O
of	O
lossy	O
compression	B
77	O
(	O
cid:0	O
)	O
14	O
(	O
cid:0	O
)	O
12	O
(	O
cid:0	O
)	O
10	O
(	O
cid:0	O
)	O
8	O
s0:01	O
(	O
cid:0	O
)	O
6	O
s0:1	O
(	O
cid:0	O
)	O
4	O
(	O
cid:0	O
)	O
2	O
log2	O
p	O
(	O
x	O
)	O
0	O
-	O
6	O
1111	O
(	O
a	O
)	O
6	O
6	O
6	O
1101	O
;	O
1011	O
;	O
:	O
:	O
:	O
0110	O
;	O
1010	O
;	O
:	O
:	O
:	O
0010	O
;	O
0001	O
;	O
:	O
:	O
:	O
6	O
0000	O
figure	O
4.7	O
.	O
(	O
a	O
)	O
the	O
sixteen	O
outcomes	O
of	O
the	O
ensemble	O
x	O
4	O
with	O
p1	O
=	O
0:1	O
,	O
ranked	O
by	O
probability	O
.	O
(	O
b	O
)	O
the	O
essential	O
bit	B
content	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
4	O
)	O
.	O
the	O
upper	O
schematic	O
diagram	O
indicates	O
the	O
strings	O
’	O
probabilities	O
by	O
the	O
vertical	O
lines	O
’	O
lengths	O
(	O
not	O
to	O
scale	O
)	O
.	O
(	O
b	O
)	O
4	O
3.5	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
4	O
)	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
0	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
10	O
)	O
10	O
8	O
6	O
4	O
2	O
0	O
n=4	O
0.05	O
0.1	O
0.15	O
0.2	O
0.25	O
0.3	O
0.35	O
0.4	O
(	O
cid:14	O
)	O
n=10	O
figure	O
4.8.	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
for	O
n	O
=	O
10	O
binary	O
variables	O
with	O
p1	O
=	O
0:1	O
.	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
(	O
cid:14	O
)	O
1	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0	O
n=10	O
n=210	O
n=410	O
n=610	O
n=810	O
n=1010	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
for	O
figure	O
4.9	O
.	O
1	O
n	O
=	O
10	O
;	O
210	O
;	O
:	O
:	O
:	O
;	O
1010	O
binary	O
variables	O
with	O
p1	O
=	O
0:1	O
.	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
(	O
cid:14	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
78	O
4	O
|	O
the	O
source	B
coding	I
theorem	I
figure	O
4.10.	O
the	O
top	O
15	O
strings	O
are	O
samples	O
from	O
x	O
100	O
,	O
where	O
p1	O
=	O
0:1	O
and	O
p0	O
=	O
0:9.	O
the	O
bottom	O
two	O
are	O
the	O
most	O
and	O
least	O
probable	O
strings	O
in	O
this	O
ensemble	B
.	O
the	O
(	O
cid:12	O
)	O
nal	O
column	O
shows	O
the	O
log-probabilities	O
of	O
the	O
random	O
strings	O
,	O
which	O
may	O
be	O
compared	O
with	O
the	O
entropy	B
h	O
(	O
x	O
100	O
)	O
=	O
46:9	O
bits	O
.	O
x	O
...	O
1	O
...	O
...	O
...	O
...	O
...	O
...	O
.1	O
...	O
..1	O
...	O
.1.1	O
...	O
...	O
.1	O
...	O
...	O
..1	O
...	O
...	O
...	O
..1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
1	O
...	O
...	O
.11	O
...	O
...	O
..1..1..1	O
...	O
...	O
...	O
...	O
...	O
111	O
...	O
...	O
...	O
...	O
...	O
...	O
.1	O
...	O
...	O
...	O
...	O
...	O
1	O
...	O
...	O
...	O
1.1	O
...	O
1	O
...	O
1	O
...	O
...	O
...	O
...	O
.1	O
...	O
...	O
...	O
1	O
...	O
...	O
...	O
.1	O
...	O
..1	O
...	O
...	O
1	O
...	O
...	O
...	O
.1	O
...	O
.1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
.1	O
...	O
...	O
11	O
...	O
...	O
...	O
..1	O
...	O
1	O
...	O
..1.1	O
...	O
...	O
1	O
...	O
...	O
...	O
.1	O
...	O
.1	O
...	O
1	O
...	O
..1	O
...	O
...	O
...	O
...	O
1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
.	O
...	O
...	O
...	O
...	O
..1	O
...	O
...	O
1	O
...	O
...	O
...	O
1.1	O
...	O
...	O
.1	O
...	O
...	O
...	O
.1	O
...	O
...	O
...	O
...	O
1	O
...	O
1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
.1	O
...	O
...	O
.	O
...	O
..1	O
...	O
...	O
..1	O
...	O
...	O
.1	O
...	O
1	O
...	O
...	O
...	O
...	O
1	O
...	O
...	O
...	O
...	O
1	O
...	O
...	O
...	O
..1	O
...	O
...	O
1..11	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
1.1	O
...	O
1	O
...	O
...	O
...	O
...	O
...	O
.1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
..11.1..1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
.1	O
...	O
..1..1.11	O
...	O
..	O
...	O
...	O
..1	O
...	O
.1..1	O
...	O
1	O
...	O
.11..1.1	O
...	O
...	O
...	O
11	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
.1	O
...	O
1.1..1	O
...	O
1	O
...	O
...	O
...	O
...	O
...	O
.1	O
.	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
.1	O
...	O
..1	O
...	O
..1	O
...	O
...	O
.1	O
...	O
.1	O
...	O
...	O
...	O
1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
.1	O
...	O
.	O
log2	O
(	O
p	O
(	O
x	O
)	O
)	O
(	O
cid:0	O
)	O
50.1	O
(	O
cid:0	O
)	O
37.3	O
(	O
cid:0	O
)	O
65.9	O
(	O
cid:0	O
)	O
56.4	O
(	O
cid:0	O
)	O
53.2	O
(	O
cid:0	O
)	O
43.7	O
(	O
cid:0	O
)	O
46.8	O
(	O
cid:0	O
)	O
56.4	O
(	O
cid:0	O
)	O
37.3	O
(	O
cid:0	O
)	O
43.7	O
(	O
cid:0	O
)	O
56.4	O
(	O
cid:0	O
)	O
37.3	O
(	O
cid:0	O
)	O
56.4	O
(	O
cid:0	O
)	O
59.5	O
(	O
cid:0	O
)	O
46.8	O
(	O
cid:0	O
)	O
15.2	O
1111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111111	O
(	O
cid:0	O
)	O
332.1	O
...	O
...	O
1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
1	O
...	O
...	O
...	O
...	O
..1	O
...	O
..1..1.1.1..1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
..1	O
.	O
1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
..1	O
...	O
...	O
...	O
.1	O
...	O
1	O
...	O
...	O
...	O
...	O
...	O
...	O
.1	O
...	O
.1	O
...	O
.1	O
...	O
...	O
..1..11..1.1	O
...	O
1	O
...	O
...	O
..	O
...	O
...	O
...	O
..11.1	O
...	O
...	O
...	O
1	O
...	O
...	O
...	O
...	O
...	O
.1	O
...	O
...	O
1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
..	O
.1	O
...	O
...	O
...	O
.1	O
...	O
1.1	O
...	O
...	O
...	O
...	O
.1	O
...	O
...	O
.11	O
...	O
...	O
...	O
..1.1	O
...	O
1	O
...	O
...	O
...	O
...	O
..1	O
...	O
...	O
...	O
...	O
.11	O
...	O
...	O
...	O
.	O
...	O
...	O
1	O
...	O
1..1	O
...	O
..1..11.1.1.1	O
...	O
1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
1	O
...	O
...	O
...	O
...	O
1	O
...	O
...	O
...	O
...	O
.1..1	O
...	O
...	O
...	O
...	O
..	O
...	O
...	O
...	O
...	O
11.1	O
...	O
...	O
1	O
...	O
.1..1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
.1	O
...	O
...	O
.1	O
...	O
...	O
...	O
...	O
..1	O
...	O
...	O
.1	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
...	O
.	O
except	O
for	O
tails	O
close	O
to	O
(	O
cid:14	O
)	O
=	O
0	O
and	O
1.	O
as	O
long	O
as	O
we	O
are	O
allowed	O
a	O
tiny	O
probability	B
of	I
error	I
(	O
cid:14	O
)	O
,	O
compression	B
down	O
to	O
n	O
h	O
bits	O
is	O
possible	O
.	O
even	O
if	O
we	O
are	O
allowed	O
a	O
large	O
probability	B
of	I
error	I
,	O
we	O
still	O
can	O
compress	B
only	O
down	O
to	O
n	O
h	O
bits	O
.	O
this	O
is	O
the	O
source	B
coding	I
theorem	I
.	O
theorem	B
4.1	O
shannon	O
’	O
s	O
source	B
coding	I
theorem	I
.	O
let	O
x	O
be	O
an	O
ensemble	B
with	O
entropy	B
h	O
(	O
x	O
)	O
=	O
h	O
bits	O
.	O
given	O
(	O
cid:15	O
)	O
>	O
0	O
and	O
0	O
<	O
(	O
cid:14	O
)	O
<	O
1	O
,	O
there	O
exists	O
a	O
positive	O
integer	O
n0	O
such	O
that	O
for	O
n	O
>	O
n0	O
,	O
1	O
n	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
<	O
(	O
cid:15	O
)	O
:	O
(	O
4.21	O
)	O
4.4	O
typicality	B
why	O
does	O
increasing	O
n	O
help	O
?	O
let	O
’	O
s	O
examine	O
long	O
strings	O
from	O
x	O
n	O
.	O
table	O
4.10	O
shows	O
(	O
cid:12	O
)	O
fteen	O
samples	O
from	O
x	O
n	O
for	O
n	O
=	O
100	O
and	O
p1	O
=	O
0:1.	O
the	O
probability	O
of	O
a	O
string	O
x	O
that	O
contains	O
r	O
1s	O
and	O
n	O
(	O
cid:0	O
)	O
r	O
0s	O
is	O
p	O
(	O
x	O
)	O
=	O
pr	O
1	O
(	O
1	O
(	O
cid:0	O
)	O
p1	O
)	O
n	O
(	O
cid:0	O
)	O
r	O
:	O
the	O
number	O
of	O
strings	O
that	O
contain	O
r	O
1s	O
is	O
n	O
(	O
r	O
)	O
=	O
(	O
cid:18	O
)	O
n	O
r	O
(	O
cid:19	O
)	O
:	O
so	O
the	O
number	O
of	O
1s	O
,	O
r	O
,	O
has	O
a	O
binomial	B
distribution	I
:	O
p	O
(	O
r	O
)	O
=	O
(	O
cid:18	O
)	O
n	O
r	O
(	O
cid:19	O
)	O
pr	O
1	O
(	O
1	O
(	O
cid:0	O
)	O
p1	O
)	O
n	O
(	O
cid:0	O
)	O
r	O
:	O
(	O
4.22	O
)	O
(	O
4.23	O
)	O
(	O
4.24	O
)	O
these	O
functions	B
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
4.11.	O
the	O
mean	B
of	O
r	O
is	O
n	O
p1	O
,	O
and	O
its	O
standard	B
deviation	I
is	O
pn	O
p1	O
(	O
1	O
(	O
cid:0	O
)	O
p1	O
)	O
(	O
p.1	O
)	O
.	O
if	O
n	O
is	O
100	O
then	O
r	O
(	O
cid:24	O
)	O
n	O
p1	O
(	O
cid:6	O
)	O
pn	O
p1	O
(	O
1	O
(	O
cid:0	O
)	O
p1	O
)	O
’	O
10	O
(	O
cid:6	O
)	O
3	O
:	O
(	O
4.25	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
4.4	O
:	O
typicality	B
79	O
r	O
(	O
cid:1	O
)	O
n	O
(	O
r	O
)	O
=	O
(	O
cid:0	O
)	O
n	O
p	O
(	O
x	O
)	O
=	O
pr	O
1	O
(	O
1	O
(	O
cid:0	O
)	O
p1	O
)	O
n	O
(	O
cid:0	O
)	O
r	O
log2	O
p	O
(	O
x	O
)	O
r	O
(	O
cid:1	O
)	O
pr	O
n	O
(	O
r	O
)	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:0	O
)	O
n	O
1	O
(	O
1	O
(	O
cid:0	O
)	O
p1	O
)	O
n	O
(	O
cid:0	O
)	O
r	O
n	O
=	O
100	O
n	O
=	O
1000	O
3e+299	O
2.5e+299	O
2e+299	O
1.5e+299	O
0	O
10	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
90	O
100	O
2e-05	O
1e-05	O
0	O
0	O
1	O
2	O
3	O
4	O
5	O
0	O
10	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
90	O
100	O
t	O
0	O
10	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
90	O
100	O
0	O
10	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
90	O
100	O
1e+299	O
5e+298	O
0	O
0	O
-500	O
-1000	O
-1500	O
-2000	O
-2500	O
-3000	O
-3500	O
0.045	O
0.04	O
0.035	O
0.03	O
0.025	O
0.02	O
0.015	O
0.01	O
0.005	O
0	O
0	O
100	O
200	O
300	O
400	O
500	O
600	O
700	O
800	O
9001000	O
t	O
0	O
100	O
200	O
300	O
400	O
500	O
600	O
700	O
800	O
9001000	O
0	O
100	O
200	O
300	O
400	O
500	O
600	O
700	O
800	O
9001000	O
1.2e+29	O
1e+29	O
8e+28	O
6e+28	O
4e+28	O
2e+28	O
0	O
2e-05	O
1e-05	O
0	O
0	O
-50	O
-100	O
-150	O
-200	O
-250	O
-300	O
-350	O
0.14	O
0.12	O
0.1	O
0.08	O
0.06	O
0.04	O
0.02	O
0	O
r	O
r	O
figure	O
4.11.	O
anatomy	O
of	O
the	O
typical	O
set	B
t	O
.	O
for	O
p1	O
=	O
0:1	O
and	O
n	O
=	O
100	O
and	O
n	O
=	O
1000	O
,	O
these	O
graphs	O
show	O
n	O
(	O
r	O
)	O
,	O
the	O
number	O
of	O
strings	O
containing	O
r	O
1s	O
;	O
the	O
probability	B
p	O
(	O
x	O
)	O
of	O
a	O
single	O
string	O
that	O
contains	O
r	O
1s	O
;	O
the	O
same	O
probability	B
on	O
a	O
log	O
scale	O
;	O
and	O
the	O
total	O
probability	B
n	O
(	O
r	O
)	O
p	O
(	O
x	O
)	O
of	O
all	O
strings	O
that	O
contain	O
r	O
1s	O
.	O
the	O
number	O
r	O
is	O
on	O
the	O
horizontal	O
axis	O
.	O
the	O
plot	O
of	O
log2	O
p	O
(	O
x	O
)	O
also	O
shows	O
by	O
a	O
dotted	O
line	O
the	O
mean	B
value	O
of	O
log2	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:0	O
)	O
n	O
h2	O
(	O
p1	O
)	O
,	O
which	O
equals	O
(	O
cid:0	O
)	O
46:9	O
when	O
n	O
=	O
100	O
and	O
(	O
cid:0	O
)	O
469	O
when	O
n	O
=	O
1000.	O
the	O
typical	B
set	I
includes	O
only	O
the	O
strings	O
that	O
have	O
log2	O
p	O
(	O
x	O
)	O
close	O
to	O
this	O
value	O
.	O
the	O
range	O
marked	O
t	O
shows	O
the	O
set	B
tn	O
(	O
cid:12	O
)	O
(	O
as	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
section	O
4.4	O
)	O
for	O
n	O
=	O
100	O
and	O
(	O
cid:12	O
)	O
=	O
0:29	O
(	O
left	O
)	O
and	O
n	O
=	O
1000	O
,	O
(	O
cid:12	O
)	O
=	O
0:09	O
(	O
right	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
80	O
if	O
n	O
=	O
1000	O
then	O
4	O
|	O
the	O
source	B
coding	I
theorem	I
(	O
4.26	O
)	O
r	O
(	O
cid:24	O
)	O
100	O
(	O
cid:6	O
)	O
10	O
:	O
notice	O
that	O
as	O
n	O
gets	O
bigger	O
,	O
the	O
probability	B
distribution	O
of	O
r	O
becomes	O
more	O
concentrated	O
,	O
in	O
the	O
sense	O
that	O
while	O
the	O
range	O
of	O
possible	O
values	O
of	O
r	O
grows	O
as	O
n	O
,	O
the	O
standard	B
deviation	I
of	O
r	O
grows	O
only	O
as	O
pn	O
.	O
that	O
r	O
is	O
most	O
likely	O
to	O
fall	O
in	O
a	O
small	O
range	O
of	O
values	O
implies	O
that	O
the	O
outcome	O
x	O
is	O
also	O
most	O
likely	O
to	O
fall	O
in	O
a	O
corresponding	O
small	O
subset	B
of	O
outcomes	O
that	O
we	O
will	O
call	O
the	O
typical	B
set	I
.	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
the	O
typical	O
set	B
let	O
us	O
de	O
(	O
cid:12	O
)	O
ne	O
typicality	B
for	O
an	O
arbitrary	O
ensemble	B
x	O
with	O
alphabet	O
ax	O
.	O
our	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
a	O
typical	B
string	O
will	O
involve	O
the	O
string	O
’	O
s	O
probability	B
.	O
a	O
long	O
string	O
of	O
n	O
symbols	O
will	O
usually	O
contain	O
about	O
p1n	O
occurrences	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
symbol	O
,	O
p2n	O
occurrences	O
of	O
the	O
second	O
,	O
etc	O
.	O
hence	O
the	O
probability	O
of	O
this	O
string	O
is	O
roughly	O
p	O
(	O
x	O
)	O
typ	O
=	O
p	O
(	O
x1	O
)	O
p	O
(	O
x2	O
)	O
p	O
(	O
x3	O
)	O
:	O
:	O
:	O
p	O
(	O
xn	O
)	O
’	O
p	O
(	O
p1n	O
)	O
1	O
p	O
(	O
p2n	O
)	O
2	O
:	O
:	O
:	O
p	O
(	O
pi	O
n	O
)	O
i	O
(	O
4.27	O
)	O
so	O
that	O
the	O
information	B
content	I
of	O
a	O
typical	B
string	O
is	O
log2	O
1	O
p	O
(	O
x	O
)	O
’	O
nxi	O
pi	O
log2	O
1	O
pi	O
=	O
n	O
h	O
:	O
(	O
4.28	O
)	O
1/p	O
(	O
x	O
)	O
,	O
which	O
is	O
the	O
information	B
content	I
of	O
x	O
,	O
is	O
so	O
the	O
random	B
variable	I
log2	O
very	O
likely	O
to	O
be	O
close	O
in	O
value	O
to	O
n	O
h.	O
we	O
build	O
our	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
typicality	O
on	O
this	O
observation	O
.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
typical	B
elements	O
of	O
an	O
x	O
to	O
be	O
those	O
elements	O
that	O
have	O
prob-	O
ability	O
close	O
to	O
2	O
(	O
cid:0	O
)	O
n	O
h	O
.	O
(	O
note	O
that	O
the	O
typical	B
set	I
,	O
unlike	O
the	O
smallest	O
su	O
(	O
cid:14	O
)	O
cient	O
subset	B
,	O
does	O
not	O
include	O
the	O
most	O
probable	O
elements	O
of	O
an	O
x	O
,	O
but	O
we	O
will	O
show	O
that	O
these	O
most	O
probable	O
elements	O
contribute	O
negligible	O
probability	B
.	O
)	O
we	O
introduce	O
a	O
parameter	O
(	O
cid:12	O
)	O
that	O
de	O
(	O
cid:12	O
)	O
nes	O
how	O
close	O
the	O
probability	B
has	O
to	O
be	O
to	O
2	O
(	O
cid:0	O
)	O
n	O
h	O
for	O
an	O
element	O
to	O
be	O
‘	O
typical	B
’	O
.	O
we	O
call	O
the	O
set	B
of	O
typical	B
elements	O
the	O
typical	B
set	I
,	O
tn	O
(	O
cid:12	O
)	O
:	O
x	O
:	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
tn	O
(	O
cid:12	O
)	O
(	O
cid:17	O
)	O
(	O
cid:26	O
)	O
x	O
2	O
an	O
1	O
n	O
log2	O
1	O
p	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
<	O
(	O
cid:12	O
)	O
(	O
cid:27	O
)	O
:	O
(	O
4.29	O
)	O
we	O
will	O
show	O
that	O
whatever	O
value	O
of	O
(	O
cid:12	O
)	O
we	O
choose	O
,	O
the	O
typical	B
set	I
contains	O
almost	O
all	O
the	O
probability	B
as	O
n	O
increases	O
.	O
this	O
important	O
result	O
is	O
sometimes	O
called	O
the	O
‘	O
asymptotic	B
equipartition	I
’	O
principle	O
.	O
‘	O
asymptotic	B
equipartition	I
’	O
principle	O
.	O
for	O
an	O
ensemble	B
of	O
n	O
independent	O
identically	O
distributed	O
(	O
i.i.d	O
.	O
)	O
random	B
variables	O
x	O
n	O
(	O
cid:17	O
)	O
(	O
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
)	O
,	O
with	O
n	O
su	O
(	O
cid:14	O
)	O
ciently	O
large	O
,	O
the	O
outcome	O
x	O
=	O
(	O
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
)	O
is	O
almost	O
certain	O
to	O
belong	O
to	O
a	O
subset	B
of	O
an	O
x	O
having	O
only	O
2n	O
h	O
(	O
x	O
)	O
members	O
,	O
each	O
having	O
probability	B
‘	O
close	O
to	O
’	O
2	O
(	O
cid:0	O
)	O
n	O
h	O
(	O
x	O
)	O
.	O
notice	O
that	O
if	O
h	O
(	O
x	O
)	O
<	O
h0	O
(	O
x	O
)	O
then	O
2n	O
h	O
(	O
x	O
)	O
is	O
a	O
tiny	O
fraction	O
of	O
the	O
number	O
of	O
possible	O
outcomes	O
jan	O
xj	O
=	O
jaxjn	O
=	O
2n	O
h0	O
(	O
x	O
)	O
:	O
the	O
term	O
equipartition	B
is	O
chosen	O
to	O
describe	O
the	O
idea	O
that	O
the	O
members	O
of	O
the	O
typical	O
set	B
have	O
roughly	O
equal	O
probability	B
.	O
[	O
this	O
should	O
not	O
be	O
taken	O
too	O
literally	O
,	O
hence	O
my	O
use	O
of	O
quotes	O
around	O
‘	O
asymptotic	B
equipartition	I
’	O
;	O
see	O
page	O
83	O
.	O
]	O
a	O
second	O
meaning	O
for	O
equipartition	O
,	O
in	O
thermal	O
physics	B
,	O
is	O
the	O
idea	O
that	O
each	O
degree	B
of	O
freedom	O
of	O
a	O
classical	O
system	O
has	O
equal	O
average	B
energy	O
,	O
1	O
2	O
kt	O
.	O
this	O
second	O
meaning	O
is	O
not	O
intended	O
here	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
4.5	O
:	O
proofs	O
81	O
log2	O
p	O
(	O
x	O
)	O
-	O
(	O
cid:0	O
)	O
n	O
h	O
(	O
x	O
)	O
tn	O
(	O
cid:12	O
)	O
6	O
1111111111110.	O
.	O
.	O
11111110111	O
66	O
6	O
6	O
0000100000010.	O
.	O
.	O
00001000010	O
0100000001000.	O
.	O
.	O
00010000000	O
0001000000000.	O
.	O
.	O
00000000000	O
0000000000000.	O
.	O
.	O
00000000000	O
figure	O
4.12.	O
schematic	O
diagram	O
showing	O
all	O
strings	O
in	O
the	O
ensemble	B
x	O
n	O
ranked	O
by	O
their	O
probability	B
,	O
and	O
the	O
typical	B
set	I
tn	O
(	O
cid:12	O
)	O
.	O
the	O
‘	O
asymptotic	B
equipartition	I
’	O
principle	O
is	O
equivalent	O
to	O
:	O
shannon	O
’	O
s	O
source	B
coding	I
theorem	I
(	O
verbal	O
statement	O
)	O
.	O
n	O
i.i.d	O
.	O
ran-	O
dom	O
variables	O
each	O
with	O
entropy	O
h	O
(	O
x	O
)	O
can	O
be	O
compressed	O
into	O
more	O
than	O
n	O
h	O
(	O
x	O
)	O
bits	O
with	O
negligible	O
risk	O
of	O
information	O
loss	O
,	O
as	O
n	O
!	O
1	O
;	O
conversely	O
if	O
they	O
are	O
compressed	O
into	O
fewer	O
than	O
n	O
h	O
(	O
x	O
)	O
bits	O
it	O
is	O
vir-	O
tually	O
certain	O
that	O
information	B
will	O
be	O
lost	O
.	O
these	O
two	O
theorems	O
are	O
equivalent	O
because	O
we	O
can	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
compression	B
algo-	O
rithm	O
that	O
gives	O
a	O
distinct	O
name	O
of	O
length	O
n	O
h	O
(	O
x	O
)	O
bits	O
to	O
each	O
x	O
in	O
the	O
typical	B
set	I
.	O
4.5	O
proofs	O
this	O
section	B
may	O
be	O
skipped	O
if	O
found	O
tough	O
going	O
.	O
the	O
law	B
of	I
large	I
numbers	I
our	O
proof	O
of	O
the	O
source	O
coding	B
theorem	I
uses	O
the	O
law	B
of	I
large	I
numbers	I
.	O
mean	B
and	O
variance	B
of	O
a	O
real	O
random	B
variable	I
are	O
e	O
[	O
u	O
]	O
=	O
(	O
cid:22	O
)	O
u	O
=	O
pu	O
p	O
(	O
u	O
)	O
u	O
u	O
=	O
e	O
[	O
(	O
u	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
u	O
)	O
2	O
]	O
=pu	O
p	O
(	O
u	O
)	O
(	O
u	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
u	O
)	O
2	O
:	O
and	O
var	O
(	O
u	O
)	O
=	O
(	O
cid:27	O
)	O
2	O
technical	O
note	O
:	O
strictly	O
i	O
am	O
assuming	O
here	O
that	O
u	O
is	O
a	O
function	B
u	O
(	O
x	O
)	O
of	O
a	O
sample	B
x	O
from	O
a	O
(	O
cid:12	O
)	O
nite	O
discrete	O
ensemble	O
x.	O
then	O
the	O
summations	O
pu	O
p	O
(	O
u	O
)	O
f	O
(	O
u	O
)	O
should	O
be	O
written	O
px	O
p	O
(	O
x	O
)	O
f	O
(	O
u	O
(	O
x	O
)	O
)	O
.	O
this	O
means	O
that	O
p	O
(	O
u	O
)	O
is	O
a	O
(	O
cid:12	O
)	O
nite	O
sum	O
of	O
delta	O
functions	O
.	O
this	O
restriction	O
guarantees	O
that	O
the	O
mean	B
and	O
variance	B
of	O
u	O
do	O
exist	O
,	O
which	O
is	O
not	O
necessarily	O
the	O
case	O
for	O
general	O
p	O
(	O
u	O
)	O
.	O
chebyshev	O
’	O
s	O
inequality	B
1.	O
let	O
t	O
be	O
a	O
non-negative	O
real	O
random	B
variable	I
,	O
and	O
let	O
(	O
cid:11	O
)	O
be	O
a	O
positive	O
real	O
number	O
.	O
then	O
p	O
(	O
t	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
t	O
(	O
cid:11	O
)	O
:	O
(	O
4.30	O
)	O
proof	O
:	O
p	O
(	O
t	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
)	O
=	O
pt	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
p	O
(	O
t	O
)	O
.	O
we	O
multiply	O
each	O
term	O
by	O
t=	O
(	O
cid:11	O
)	O
(	O
cid:21	O
)	O
1	O
and	O
obtain	O
:	O
p	O
(	O
t	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
)	O
(	O
cid:20	O
)	O
pt	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
p	O
(	O
t	O
)	O
t=	O
(	O
cid:11	O
)	O
:	O
we	O
add	O
the	O
(	O
non-negative	O
)	O
missing	O
terms	O
and	O
obtain	O
:	O
p	O
(	O
t	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
)	O
(	O
cid:20	O
)	O
pt	O
p	O
(	O
t	O
)	O
t=	O
(	O
cid:11	O
)	O
=	O
(	O
cid:22	O
)	O
t=	O
(	O
cid:11	O
)	O
.	O
2	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
82	O
4	O
|	O
the	O
source	B
coding	I
theorem	I
chebyshev	O
’	O
s	O
inequality	B
2.	O
let	O
x	O
be	O
a	O
random	B
variable	I
,	O
and	O
let	O
(	O
cid:11	O
)	O
be	O
a	O
positive	O
real	O
number	O
.	O
then	O
proof	O
:	O
take	O
t	O
=	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
and	O
apply	O
the	O
previous	O
proposition	O
.	O
p	O
(	O
cid:0	O
)	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
(	O
cid:1	O
)	O
(	O
cid:20	O
)	O
(	O
cid:27	O
)	O
2	O
x=	O
(	O
cid:11	O
)	O
:	O
(	O
4.31	O
)	O
2	O
weak	O
law	B
of	I
large	I
numbers	I
.	O
take	O
x	O
to	O
be	O
the	O
average	B
of	O
n	O
independent	O
random	O
variables	O
h1	O
;	O
:	O
:	O
:	O
;	O
hn	O
,	O
having	O
common	O
mean	B
(	O
cid:22	O
)	O
h	O
and	O
common	O
vari-	O
ance	O
(	O
cid:27	O
)	O
2	O
n=1	O
hn	O
.	O
then	O
h	O
:	O
x	O
=	O
1	O
n	O
pn	O
p	O
(	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
h	O
)	O
2	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
)	O
(	O
cid:20	O
)	O
(	O
cid:27	O
)	O
2	O
h=	O
(	O
cid:11	O
)	O
n	O
:	O
(	O
4.32	O
)	O
2	O
proof	O
:	O
obtained	O
by	O
showing	O
that	O
(	O
cid:22	O
)	O
x	O
=	O
(	O
cid:22	O
)	O
h	O
and	O
that	O
(	O
cid:27	O
)	O
2	O
x	O
=	O
(	O
cid:27	O
)	O
2	O
h=n	O
.	O
we	O
are	O
interested	O
in	O
x	O
being	O
very	O
close	O
to	O
the	O
mean	B
(	O
(	O
cid:11	O
)	O
very	O
small	O
)	O
.	O
no	O
matter	O
how	O
large	O
(	O
cid:27	O
)	O
2	O
h	O
is	O
,	O
and	O
no	O
matter	O
how	O
small	O
the	O
required	O
(	O
cid:11	O
)	O
is	O
,	O
and	O
no	O
matter	O
how	O
small	O
the	O
desired	O
probability	B
that	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
h	O
)	O
2	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
,	O
we	O
can	O
always	O
achieve	O
it	O
by	O
taking	O
n	O
large	O
enough	O
.	O
proof	O
of	O
theorem	O
4.1	O
(	O
p.78	O
)	O
we	O
apply	O
the	O
law	B
of	I
large	I
numbers	I
to	O
the	O
random	B
variable	I
1	O
1	O
p	O
(	O
x	O
)	O
de	O
(	O
cid:12	O
)	O
ned	O
for	O
x	O
drawn	O
from	O
the	O
ensemble	B
x	O
n	O
.	O
this	O
random	B
variable	I
can	O
be	O
written	O
as	O
the	O
average	B
of	O
n	O
information	B
contents	O
hn	O
=	O
log2	O
(	O
1=p	O
(	O
xn	O
)	O
)	O
,	O
each	O
of	O
which	O
is	O
a	O
random	B
variable	I
with	O
mean	B
h	O
=	O
h	O
(	O
x	O
)	O
and	O
variance	O
(	O
cid:27	O
)	O
2	O
(	O
cid:17	O
)	O
var	O
[	O
log2	O
(	O
1=p	O
(	O
xn	O
)	O
)	O
]	O
.	O
(	O
each	O
term	O
hn	O
is	O
the	O
shannon	O
information	O
content	B
of	O
the	O
nth	O
outcome	O
.	O
)	O
n	O
log2	O
we	O
again	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
typical	B
set	I
with	O
parameters	B
n	O
and	O
(	O
cid:12	O
)	O
thus	O
:	O
tn	O
(	O
cid:12	O
)	O
=	O
(	O
x	O
2	O
an	O
x	O
:	O
(	O
cid:20	O
)	O
1	O
n	O
log2	O
1	O
p	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
cid:21	O
)	O
2	O
<	O
(	O
cid:12	O
)	O
2	O
)	O
:	O
for	O
all	O
x	O
2	O
tn	O
(	O
cid:12	O
)	O
,	O
the	O
probability	O
of	O
x	O
satis	O
(	O
cid:12	O
)	O
es	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
h+	O
(	O
cid:12	O
)	O
)	O
<	O
p	O
(	O
x	O
)	O
<	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
h	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
)	O
:	O
and	O
by	O
the	O
law	B
of	I
large	I
numbers	I
,	O
p	O
(	O
x	O
2	O
tn	O
(	O
cid:12	O
)	O
)	O
(	O
cid:21	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:27	O
)	O
2	O
(	O
cid:12	O
)	O
2n	O
:	O
(	O
4.33	O
)	O
(	O
4.34	O
)	O
(	O
4.35	O
)	O
we	O
have	O
thus	O
proved	O
the	O
‘	O
asymptotic	B
equipartition	I
’	O
principle	O
.	O
as	O
n	O
increases	O
,	O
the	O
probability	B
that	O
x	O
falls	O
in	O
tn	O
(	O
cid:12	O
)	O
approaches	O
1	O
,	O
for	O
any	O
(	O
cid:12	O
)	O
.	O
how	O
does	O
this	O
result	O
relate	O
to	O
source	O
coding	O
?	O
we	O
must	O
relate	O
tn	O
(	O
cid:12	O
)	O
to	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
.	O
we	O
will	O
show	O
that	O
for	O
any	O
given	O
(	O
cid:14	O
)	O
there	O
is	O
a	O
su	O
(	O
cid:14	O
)	O
ciently	O
big	O
n	O
such	O
that	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
’	O
n	O
h.	O
part	O
1	O
:	O
1	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
<	O
h	O
+	O
(	O
cid:15	O
)	O
.	O
the	O
set	B
tn	O
(	O
cid:12	O
)	O
is	O
not	O
the	O
best	O
subset	B
for	O
compression	B
.	O
so	O
the	O
size	O
of	O
tn	O
(	O
cid:12	O
)	O
gives	O
an	O
upper	O
bound	B
on	O
h	O
(	O
cid:14	O
)	O
.	O
we	O
show	O
how	O
small	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
must	O
be	O
by	O
calculating	O
how	O
big	O
tn	O
(	O
cid:12	O
)	O
could	O
possibly	O
be	O
.	O
we	O
are	O
free	O
to	O
set	B
(	O
cid:12	O
)	O
to	O
any	O
convenient	O
value	O
.	O
the	O
smallest	O
possible	O
probability	B
that	O
a	O
member	O
of	O
tn	O
(	O
cid:12	O
)	O
can	O
have	O
is	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
h+	O
(	O
cid:12	O
)	O
)	O
,	O
and	O
the	O
total	O
probability	B
contained	O
by	O
tn	O
(	O
cid:12	O
)	O
can	O
’	O
t	O
be	O
any	O
bigger	O
than	O
1.	O
so	O
jtn	O
(	O
cid:12	O
)	O
j	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
h+	O
(	O
cid:12	O
)	O
)	O
<	O
1	O
;	O
that	O
is	O
,	O
the	O
size	O
of	O
the	O
typical	O
set	B
is	O
bounded	O
by	O
jtn	O
(	O
cid:12	O
)	O
j	O
<	O
2n	O
(	O
h+	O
(	O
cid:12	O
)	O
)	O
:	O
if	O
we	O
set	B
(	O
cid:12	O
)	O
=	O
(	O
cid:15	O
)	O
and	O
n0	O
such	O
that	O
tn	O
(	O
cid:12	O
)	O
becomes	O
a	O
witness	O
to	O
the	O
fact	O
that	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
(	O
cid:20	O
)	O
log2	O
jtn	O
(	O
cid:12	O
)	O
j	O
<	O
n	O
(	O
h	O
+	O
(	O
cid:15	O
)	O
)	O
.	O
(	O
4.37	O
)	O
(	O
cid:15	O
)	O
2n0	O
(	O
cid:20	O
)	O
(	O
cid:14	O
)	O
,	O
then	O
p	O
(	O
tn	O
(	O
cid:12	O
)	O
)	O
(	O
cid:21	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:14	O
)	O
,	O
and	O
the	O
set	B
(	O
cid:27	O
)	O
2	O
(	O
4.36	O
)	O
1	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
h0	O
(	O
x	O
)	O
h	O
+	O
(	O
cid:15	O
)	O
h	O
h	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
0	O
1	O
(	O
cid:14	O
)	O
figure	O
4.13.	O
schematic	O
illustration	O
of	O
the	O
two	O
parts	O
of	O
the	O
theorem	O
.	O
given	O
any	O
(	O
cid:14	O
)	O
and	O
(	O
cid:15	O
)	O
,	O
we	O
show	O
that	O
for	O
large	O
enough	O
n	O
,	O
1	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
lies	O
(	O
1	O
)	O
below	O
the	O
line	O
h	O
+	O
(	O
cid:15	O
)	O
and	O
(	O
2	O
)	O
above	O
the	O
line	O
h	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
83	O
’	O
$	O
&	O
%	O
’	O
$	O
&	O
%	O
cco	O
c	O
s0	O
\	O
tn	O
(	O
cid:12	O
)	O
s0	O
\	O
tn	O
(	O
cid:12	O
)	O
s0	O
@	O
@	O
i	O
4.6	O
:	O
comments	O
part	O
2	O
:	O
1	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
>	O
h	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
.	O
imagine	O
that	O
someone	O
claims	O
this	O
second	O
part	O
is	O
not	O
so	O
{	O
that	O
,	O
for	O
any	O
n	O
,	O
the	O
smallest	O
(	O
cid:14	O
)	O
-su	O
(	O
cid:14	O
)	O
cient	O
subset	B
s	O
(	O
cid:14	O
)	O
is	O
smaller	O
than	O
the	O
above	O
inequality	B
would	O
allow	O
.	O
we	O
can	O
make	O
use	O
of	O
our	O
typical	B
set	I
to	O
show	O
that	O
they	O
must	O
be	O
mistaken	O
.	O
remember	O
that	O
we	O
are	O
free	O
to	O
set	B
(	O
cid:12	O
)	O
to	O
any	O
value	O
we	O
choose	O
.	O
we	O
will	O
set	B
(	O
cid:12	O
)	O
=	O
(	O
cid:15	O
)	O
=2	O
,	O
so	O
that	O
our	O
task	O
is	O
to	O
prove	O
that	O
a	O
subset	B
s0	O
having	O
js0j	O
(	O
cid:20	O
)	O
2n	O
(	O
h	O
(	O
cid:0	O
)	O
2	O
(	O
cid:12	O
)	O
)	O
and	O
achieving	O
p	O
(	O
x	O
2	O
s0	O
)	O
(	O
cid:21	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:14	O
)	O
can	O
not	O
exist	O
(	O
for	O
n	O
greater	O
than	O
an	O
n0	O
that	O
we	O
will	O
specify	O
)	O
.	O
so	O
,	O
let	O
us	O
consider	O
the	O
probability	O
of	O
falling	O
in	O
this	O
rival	O
smaller	O
subset	B
s0	O
.	O
the	O
probability	O
of	O
the	O
subset	B
s0	O
is	O
p	O
(	O
x	O
2	O
s0	O
)	O
=	O
p	O
(	O
x	O
2	O
s0\tn	O
(	O
cid:12	O
)	O
)	O
+	O
p	O
(	O
x	O
2	O
s0\tn	O
(	O
cid:12	O
)	O
)	O
;	O
(	O
4.38	O
)	O
where	O
tn	O
(	O
cid:12	O
)	O
denotes	O
the	O
complement	O
fx	O
62	O
tn	O
(	O
cid:12	O
)	O
g.	O
the	O
maximum	O
value	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
term	O
is	O
found	O
if	O
s0	O
\	O
tn	O
(	O
cid:12	O
)	O
contains	O
2n	O
(	O
h	O
(	O
cid:0	O
)	O
2	O
(	O
cid:12	O
)	O
)	O
outcomes	O
all	O
with	O
the	O
maximum	O
probability	O
,	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
h	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
)	O
.	O
the	O
maximum	O
value	O
the	O
second	O
term	O
can	O
have	O
is	O
p	O
(	O
x	O
62	O
tn	O
(	O
cid:12	O
)	O
)	O
.	O
so	O
:	O
p	O
(	O
x	O
2	O
s0	O
)	O
(	O
cid:20	O
)	O
2n	O
(	O
h	O
(	O
cid:0	O
)	O
2	O
(	O
cid:12	O
)	O
)	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
h	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
)	O
+	O
(	O
cid:27	O
)	O
2	O
(	O
cid:12	O
)	O
2n	O
=	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
cid:12	O
)	O
+	O
(	O
cid:27	O
)	O
2	O
(	O
cid:12	O
)	O
2n	O
:	O
(	O
4.39	O
)	O
tn	O
(	O
cid:12	O
)	O
we	O
can	O
now	O
set	B
(	O
cid:12	O
)	O
=	O
(	O
cid:15	O
)	O
=2	O
and	O
n0	O
such	O
that	O
p	O
(	O
x	O
2	O
s0	O
)	O
<	O
1	O
(	O
cid:0	O
)	O
(	O
cid:14	O
)	O
,	O
which	O
shows	O
that	O
s0	O
can	O
not	O
satisfy	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
a	O
su	O
(	O
cid:14	O
)	O
cient	O
subset	B
s	O
(	O
cid:14	O
)	O
.	O
thus	O
any	O
subset	B
s0	O
with	O
size	O
js0j	O
(	O
cid:20	O
)	O
2n	O
(	O
h	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
)	O
has	O
probability	B
less	O
than	O
1	O
(	O
cid:0	O
)	O
(	O
cid:14	O
)	O
,	O
so	O
by	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
h	O
(	O
cid:14	O
)	O
,	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
>	O
n	O
(	O
h	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
)	O
.	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
is	O
essentially	O
a	O
constant	O
function	B
of	O
(	O
cid:14	O
)	O
,	O
for	O
0	O
<	O
(	O
cid:14	O
)	O
<	O
1	O
,	O
as	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gures	O
4.9	O
and	O
4.13.	O
thus	O
for	O
large	O
enough	O
n	O
,	O
the	O
function	B
1	O
2	O
4.6	O
comments	O
the	O
source	B
coding	I
theorem	I
(	O
p.78	O
)	O
has	O
two	O
parts	O
,	O
1	O
1	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
>	O
h	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
.	O
both	O
results	O
are	O
interesting	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
part	O
tells	O
us	O
that	O
even	O
if	O
the	O
probability	B
of	I
error	I
(	O
cid:14	O
)	O
is	O
extremely	O
small	O
,	O
the	O
number	O
of	O
bits	O
per	O
symbol	O
1	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
needed	O
to	O
specify	O
a	O
long	O
n	O
-symbol	O
string	O
x	O
with	O
vanishingly	O
small	O
error	B
probability	I
does	O
not	O
have	O
to	O
exceed	O
h	O
+	O
(	O
cid:15	O
)	O
bits	O
.	O
we	O
need	O
to	O
have	O
only	O
a	O
tiny	O
tolerance	O
for	O
error	O
,	O
and	O
the	O
number	O
of	O
bits	O
required	O
drops	O
signi	O
(	O
cid:12	O
)	O
cantly	O
from	O
h0	O
(	O
x	O
)	O
to	O
(	O
h	O
+	O
(	O
cid:15	O
)	O
)	O
.	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
<	O
h	O
+	O
(	O
cid:15	O
)	O
,	O
and	O
what	O
happens	O
if	O
we	O
are	O
yet	O
more	O
tolerant	O
to	O
compression	B
errors	O
?	O
part	O
2	O
tells	O
us	O
that	O
even	O
if	O
(	O
cid:14	O
)	O
is	O
very	O
close	O
to	O
1	O
,	O
so	O
that	O
errors	B
are	O
made	O
most	O
of	O
the	O
time	O
,	O
the	O
average	B
number	O
of	O
bits	O
per	O
symbol	O
needed	O
to	O
specify	O
x	O
must	O
still	O
be	O
at	O
least	O
h	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
bits	O
.	O
these	O
two	O
extremes	O
tell	O
us	O
that	O
regardless	O
of	O
our	O
speci	O
(	O
cid:12	O
)	O
c	O
allowance	O
for	O
error	O
,	O
the	O
number	O
of	O
bits	O
per	O
symbol	O
needed	O
to	O
specify	O
x	O
is	O
h	O
bits	O
;	O
no	O
more	O
and	O
no	O
less	O
.	O
caveat	B
regarding	O
‘	O
asymptotic	B
equipartition	I
’	O
i	O
put	O
the	O
words	O
‘	O
asymptotic	B
equipartition	I
’	O
in	O
quotes	O
because	O
it	O
is	O
important	O
not	O
to	O
think	O
that	O
the	O
elements	O
of	O
the	O
typical	O
set	B
tn	O
(	O
cid:12	O
)	O
really	O
do	O
have	O
roughly	O
the	O
same	O
probability	B
as	O
each	O
other	O
.	O
they	O
are	O
similar	O
in	O
probability	O
only	O
in	O
1	O
p	O
(	O
x	O
)	O
are	O
within	O
2n	O
(	O
cid:12	O
)	O
of	O
each	O
other	O
.	O
now	O
,	O
as	O
the	O
sense	O
that	O
their	O
values	O
of	O
log2	O
(	O
cid:12	O
)	O
is	O
decreased	O
,	O
how	O
does	O
n	O
have	O
to	O
increase	O
,	O
if	O
we	O
are	O
to	O
keep	O
our	O
bound	B
on	O
the	O
mass	O
of	O
the	O
typical	O
set	B
,	O
p	O
(	O
x	O
2	O
tn	O
(	O
cid:12	O
)	O
)	O
(	O
cid:21	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:27	O
)	O
2	O
(	O
cid:12	O
)	O
2n	O
,	O
constant	O
?	O
n	O
must	O
grow	O
as	O
1=	O
(	O
cid:12	O
)	O
2	O
,	O
so	O
,	O
if	O
we	O
write	O
(	O
cid:12	O
)	O
in	O
terms	O
of	O
n	O
as	O
(	O
cid:11	O
)	O
=pn	O
,	O
for	O
some	O
constant	O
(	O
cid:11	O
)	O
,	O
then	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
84	O
4	O
|	O
the	O
source	B
coding	I
theorem	I
the	O
most	O
probable	O
string	O
in	O
the	O
typical	B
set	I
will	O
be	O
of	O
order	O
2	O
(	O
cid:11	O
)	O
pn	O
times	O
greater	O
than	O
the	O
least	O
probable	O
string	O
in	O
the	O
typical	B
set	I
.	O
as	O
(	O
cid:12	O
)	O
decreases	O
,	O
n	O
increases	O
,	O
and	O
this	O
ratio	O
2	O
(	O
cid:11	O
)	O
pn	O
grows	O
exponentially	O
.	O
thus	O
we	O
have	O
‘	O
equipartition	B
’	O
only	O
in	O
a	O
weak	O
sense	O
!	O
why	O
did	O
we	O
introduce	O
the	O
typical	B
set	I
?	O
the	O
best	O
choice	O
of	O
subset	O
for	O
block	O
compression	B
is	O
(	O
by	O
de	O
(	O
cid:12	O
)	O
nition	O
)	O
s	O
(	O
cid:14	O
)	O
,	O
not	O
a	O
typical	B
set	I
.	O
so	O
why	O
did	O
we	O
bother	O
introducing	O
the	O
typical	B
set	I
?	O
the	O
answer	O
is	O
,	O
we	O
can	O
count	O
the	O
typical	B
set	I
.	O
we	O
know	O
that	O
all	O
its	O
elements	O
have	O
‘	O
almost	O
iden-	O
tical	O
’	O
probability	B
(	O
2	O
(	O
cid:0	O
)	O
n	O
h	O
)	O
,	O
and	O
we	O
know	O
the	O
whole	O
set	B
has	O
probability	B
almost	O
1	O
,	O
so	O
the	O
typical	B
set	I
must	O
have	O
roughly	O
2n	O
h	O
elements	O
.	O
without	O
the	O
help	O
of	O
the	O
typical	O
set	B
(	O
which	O
is	O
very	O
similar	O
to	O
s	O
(	O
cid:14	O
)	O
)	O
it	O
would	O
have	O
been	O
hard	O
to	O
count	O
how	O
many	O
elements	O
there	O
are	O
in	O
s	O
(	O
cid:14	O
)	O
.	O
4.7	O
exercises	O
weighing	O
problems	O
.	O
exercise	O
4.9	O
.	O
[	O
1	O
]	O
while	O
some	O
people	O
,	O
when	O
they	O
(	O
cid:12	O
)	O
rst	O
encounter	O
the	O
weighing	B
problem	I
with	O
12	O
balls	O
and	O
the	O
three-outcome	O
balance	B
(	O
exercise	O
4.1	O
(	O
p.66	O
)	O
)	O
,	O
think	O
that	O
weighing	O
six	O
balls	O
against	O
six	B
balls	O
is	O
a	O
good	B
(	O
cid:12	O
)	O
rst	O
weighing	O
,	O
others	O
say	O
‘	O
no	O
,	O
weighing	O
six	O
against	O
six	B
conveys	O
no	O
informa-	O
tion	O
at	O
all	O
’	O
.	O
explain	O
to	O
the	O
second	O
group	O
why	O
they	O
are	O
both	O
right	O
and	O
wrong	O
.	O
compute	O
the	O
information	B
gained	O
about	O
which	O
is	O
the	O
odd	O
ball	O
,	O
and	O
the	O
information	B
gained	O
about	O
which	O
is	O
the	O
odd	O
ball	O
and	O
whether	O
it	O
is	O
heavy	O
or	O
light	O
.	O
.	O
exercise	O
4.10	O
.	O
[	O
2	O
]	O
solve	O
the	O
weighing	B
problem	I
for	O
the	O
case	O
where	O
there	O
are	O
39	O
balls	O
of	O
which	O
one	O
is	O
known	O
to	O
be	O
odd	O
.	O
.	O
exercise	O
4.11	O
.	O
[	O
2	O
]	O
you	O
are	O
given	O
16	O
balls	O
,	O
all	O
of	O
which	O
are	O
equal	O
in	O
weight	O
except	O
for	O
one	O
that	O
is	O
either	O
heavier	O
or	O
lighter	O
.	O
you	O
are	O
also	O
given	O
a	O
bizarre	O
two-	O
pan	O
balance	B
that	O
can	O
report	O
only	O
two	O
outcomes	O
:	O
‘	O
the	O
two	O
sides	O
balance	B
’	O
or	O
‘	O
the	O
two	O
sides	O
do	O
not	O
balance	B
’	O
.	O
design	O
a	O
strategy	O
to	O
determine	O
which	O
is	O
the	O
odd	O
ball	O
in	O
as	O
few	O
uses	O
of	O
the	O
balance	B
as	O
possible	O
.	O
.	O
exercise	O
4.12	O
.	O
[	O
2	O
]	O
you	O
have	O
a	O
two-pan	O
balance	B
;	O
your	O
job	O
is	O
to	O
weigh	O
out	O
bags	O
of	O
(	O
cid:13	O
)	O
our	O
with	O
integer	O
weights	O
1	O
to	O
40	O
pounds	O
inclusive	O
.	O
how	O
many	O
weights	O
do	O
you	O
need	O
?	O
[	O
you	O
are	O
allowed	O
to	O
put	O
weights	O
on	O
either	O
pan	O
.	O
you	O
’	O
re	O
only	O
allowed	O
to	O
put	O
one	O
(	O
cid:13	O
)	O
our	O
bag	O
on	O
the	O
balance	B
at	O
a	O
time	O
.	O
]	O
exercise	O
4.13	O
.	O
[	O
4	O
,	O
p.86	O
]	O
(	O
a	O
)	O
is	O
it	O
possible	O
to	O
solve	O
exercise	O
4.1	O
(	O
p.66	O
)	O
(	O
the	O
weigh-	O
ing	O
problem	O
with	O
12	O
balls	O
and	O
the	O
three-outcome	O
balance	B
)	O
using	O
a	O
sequence	B
of	O
three	O
(	O
cid:12	O
)	O
xed	O
weighings	O
,	O
such	O
that	O
the	O
balls	O
chosen	O
for	O
the	O
second	O
weighing	O
do	O
not	O
depend	O
on	O
the	O
outcome	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
,	O
and	O
the	O
third	O
weighing	O
does	O
not	O
depend	O
on	O
the	O
(	O
cid:12	O
)	O
rst	O
or	O
second	O
?	O
(	O
b	O
)	O
find	O
a	O
solution	O
to	O
the	O
general	O
n	O
-ball	O
weighing	B
problem	I
in	O
which	O
exactly	O
one	O
of	O
n	O
balls	O
is	O
odd	O
.	O
show	O
that	O
in	O
w	O
weighings	O
,	O
an	O
odd	O
ball	O
can	O
be	O
identi	O
(	O
cid:12	O
)	O
ed	O
from	O
among	O
n	O
=	O
(	O
3w	O
(	O
cid:0	O
)	O
3	O
)	O
=2	O
balls	O
.	O
exercise	O
4.14	O
.	O
[	O
3	O
]	O
you	O
are	O
given	O
12	O
balls	O
and	O
the	O
three-outcome	O
balance	B
of	O
exer-	O
cise	O
4.1	O
;	O
this	O
time	O
,	O
two	O
of	O
the	O
balls	O
are	O
odd	O
;	O
each	O
odd	O
ball	O
may	O
be	O
heavy	O
or	O
light	O
,	O
and	O
we	O
don	O
’	O
t	O
know	O
which	O
.	O
we	O
want	O
to	O
identify	O
the	O
odd	O
balls	O
and	O
in	O
which	O
direction	O
they	O
are	O
odd	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
4.7	O
:	O
exercises	O
85	O
(	O
a	O
)	O
estimate	O
how	O
many	O
weighings	O
are	O
required	O
by	O
the	O
optimal	B
strategy	O
.	O
and	O
what	O
if	O
there	O
are	O
three	O
odd	O
balls	O
?	O
(	O
b	O
)	O
how	O
do	O
your	O
answers	O
change	O
if	O
it	O
is	O
known	O
that	O
all	O
the	O
regular	B
balls	O
weigh	O
100	O
g	O
,	O
that	O
light	O
balls	O
weigh	O
99	O
g	O
,	O
and	O
heavy	O
ones	O
weigh	O
110	O
g	O
?	O
source	O
coding	O
with	O
a	O
lossy	B
compressor	O
,	O
with	O
loss	O
(	O
cid:14	O
)	O
.	O
exercise	O
4.15	O
.	O
[	O
2	O
,	O
p.87	O
]	O
let	O
px	O
=	O
f0:2	O
;	O
0:8g	O
.	O
sketch	O
1	O
(	O
cid:14	O
)	O
for	O
n	O
=	O
1	O
;	O
2	O
and	O
1000.	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
as	O
a	O
function	B
of	O
.	O
exercise	O
4.16	O
.	O
[	O
2	O
]	O
let	O
py	O
=	O
f0:5	O
;	O
0:5g	O
.	O
sketch	O
1	O
n	O
=	O
1	O
;	O
2	O
;	O
3	O
and	O
100.	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
y	O
n	O
)	O
as	O
a	O
function	B
of	O
(	O
cid:14	O
)	O
for	O
.	O
exercise	O
4.17	O
.	O
[	O
2	O
,	O
p.87	O
]	O
(	O
for	O
physics	O
students	O
.	O
)	O
discuss	O
the	O
relationship	O
between	O
the	O
proof	O
of	O
the	O
‘	O
asymptotic	B
equipartition	I
’	O
principle	O
and	O
the	O
equivalence	B
(	O
for	O
large	O
systems	O
)	O
of	O
the	O
boltzmann	O
entropy	B
and	O
the	O
gibbs	O
entropy	B
.	O
distributions	O
that	O
don	O
’	O
t	O
obey	O
the	O
law	B
of	I
large	I
numbers	I
the	O
law	B
of	I
large	I
numbers	I
,	O
which	O
we	O
used	O
in	O
this	O
chapter	O
,	O
shows	O
that	O
the	O
mean	B
of	O
a	O
set	B
of	O
n	O
i.i.d	O
.	O
random	B
variables	O
has	O
a	O
probability	B
distribution	O
that	O
becomes	O
narrower	O
,	O
with	O
width	O
/	O
1=pn	O
,	O
as	O
n	O
increases	O
.	O
however	O
,	O
we	O
have	O
proved	O
this	O
property	O
only	O
for	O
discrete	O
random	B
variables	O
,	O
that	O
is	O
,	O
for	O
real	O
numbers	O
taking	O
on	O
a	O
(	O
cid:12	O
)	O
nite	O
set	B
of	O
possible	O
values	O
.	O
while	O
many	O
random	B
variables	O
with	O
continuous	O
probability	B
distributions	I
also	O
satisfy	O
the	O
law	B
of	I
large	I
numbers	I
,	O
there	O
are	O
important	O
distributions	O
that	O
do	O
not	O
.	O
some	O
continuous	B
distributions	O
do	O
not	O
have	O
a	O
mean	B
or	O
variance	B
.	O
.	O
exercise	O
4.18	O
.	O
[	O
3	O
,	O
p.88	O
]	O
sketch	O
the	O
cauchy	O
distribution	B
p	O
(	O
x	O
)	O
=	O
1	O
z	O
1	O
x2	O
+	O
1	O
;	O
x	O
2	O
(	O
(	O
cid:0	O
)	O
1	O
;	O
1	O
)	O
:	O
(	O
4.40	O
)	O
what	O
is	O
its	O
normalizing	B
constant	I
z	O
?	O
can	O
you	O
evaluate	O
its	O
mean	B
or	O
variance	B
?	O
consider	O
the	O
sum	O
z	O
=	O
x1	O
+	O
x2	O
,	O
where	O
x1	O
and	O
x2	O
are	O
independent	O
random	O
variables	O
from	O
a	O
cauchy	O
distribution	B
.	O
what	O
is	O
p	O
(	O
z	O
)	O
?	O
what	O
is	O
the	O
prob-	O
ability	O
distribution	B
of	O
the	O
mean	B
of	O
x1	O
and	O
x2	O
,	O
(	O
cid:22	O
)	O
x	O
=	O
(	O
x1	O
+	O
x2	O
)	O
=2	O
?	O
what	O
is	O
the	O
probability	B
distribution	O
of	O
the	O
mean	O
of	O
n	O
samples	O
from	O
this	O
cauchy	O
distribution	B
?	O
other	O
asymptotic	O
properties	O
exercise	O
4.19	O
.	O
[	O
3	O
]	O
cherno	O
(	O
cid:11	O
)	O
bound	B
.	O
we	O
derived	O
the	O
weak	O
law	B
of	I
large	I
numbers	I
from	O
chebyshev	O
’	O
s	O
inequality	B
(	O
4.30	O
)	O
by	O
letting	O
the	O
random	B
variable	I
t	O
in	O
the	O
inequality	B
p	O
(	O
t	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
t=	O
(	O
cid:11	O
)	O
be	O
a	O
function	B
,	O
t	O
=	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
,	O
of	O
the	O
random	O
variable	O
x	O
we	O
were	O
interested	O
in	O
.	O
other	O
useful	O
inequalities	O
can	O
be	O
obtained	O
by	O
using	O
other	O
functions	B
.	O
the	O
cherno	O
(	O
cid:11	O
)	O
bound	B
,	O
which	O
is	O
useful	O
for	O
bounding	O
the	O
tails	O
of	O
a	O
distribution	B
,	O
is	O
obtained	O
by	O
letting	O
t	O
=	O
exp	O
(	O
sx	O
)	O
.	O
show	O
that	O
and	O
p	O
(	O
x	O
(	O
cid:21	O
)	O
a	O
)	O
(	O
cid:20	O
)	O
e	O
(	O
cid:0	O
)	O
sag	O
(	O
s	O
)	O
;	O
for	O
any	O
s	O
>	O
0	O
p	O
(	O
x	O
(	O
cid:20	O
)	O
a	O
)	O
(	O
cid:20	O
)	O
e	O
(	O
cid:0	O
)	O
sag	O
(	O
s	O
)	O
;	O
for	O
any	O
s	O
<	O
0	O
(	O
4.41	O
)	O
(	O
4.42	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
86	O
4	O
|	O
the	O
source	B
coding	I
theorem	I
where	O
g	O
(	O
s	O
)	O
is	O
the	O
moment-generating	O
function	B
of	O
x	O
,	O
g	O
(	O
s	O
)	O
=xx	O
p	O
(	O
x	O
)	O
esx	O
:	O
(	O
4.43	O
)	O
curious	O
functions	B
related	O
to	O
p	O
log	O
1=p	O
exercise	O
4.20	O
.	O
[	O
4	O
,	O
p.89	O
]	O
this	O
exercise	O
has	O
no	O
purpose	O
at	O
all	O
;	O
it	O
’	O
s	O
included	O
for	O
the	O
enjoyment	O
of	O
those	O
who	O
like	O
mathematical	O
curiosities	O
.	O
sketch	O
the	O
function	B
f	O
(	O
x	O
)	O
=	O
xxxxx	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
4.44	O
)	O
for	O
x	O
(	O
cid:21	O
)	O
0.	O
hint	O
:	O
work	O
out	O
the	O
inverse	O
function	O
to	O
f	O
{	O
that	O
is	O
,	O
the	O
function	B
g	O
(	O
y	O
)	O
such	O
that	O
if	O
x	O
=	O
g	O
(	O
y	O
)	O
then	O
y	O
=	O
f	O
(	O
x	O
)	O
{	O
it	O
’	O
s	O
closely	O
related	O
to	O
p	O
log	O
1=p	O
.	O
4.8	O
solutions	O
solution	O
to	O
exercise	O
4.2	O
(	O
p.68	O
)	O
.	O
let	O
p	O
(	O
x	O
;	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
.	O
then	O
h	O
(	O
x	O
;	O
y	O
)	O
=	O
xxy	O
=	O
xxy	O
=	O
xx	O
=	O
h	O
(	O
x	O
)	O
+	O
h	O
(	O
y	O
)	O
:	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
log	O
1	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
log	O
1	O
p	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
log	O
1	O
p	O
(	O
y	O
)	O
+xxy	O
p	O
(	O
x	O
)	O
log	O
1	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
log	O
1	O
p	O
(	O
y	O
)	O
+xy	O
(	O
4.45	O
)	O
(	O
4.46	O
)	O
(	O
4.47	O
)	O
(	O
4.48	O
)	O
solution	O
to	O
exercise	O
4.4	O
(	O
p.73	O
)	O
.	O
an	O
ascii	O
(	O
cid:12	O
)	O
le	O
can	O
be	O
reduced	O
in	O
size	O
by	O
a	O
factor	O
of	O
7/8	O
.	O
this	O
reduction	O
could	O
be	O
achieved	O
by	O
a	O
block	B
code	I
that	O
maps	O
8-byte	O
blocks	O
into	O
7-byte	O
blocks	O
by	O
copying	O
the	O
56	O
information-carrying	O
bits	O
into	O
7	O
bytes	O
,	O
and	O
ignoring	O
the	O
last	O
bit	B
of	O
every	O
character	O
.	O
solution	O
to	O
exercise	O
4.5	O
(	O
p.74	O
)	O
.	O
the	O
pigeon-hole	B
principle	I
states	O
:	O
you	O
can	O
’	O
t	O
put	O
16	O
pigeons	O
into	O
15	O
holes	O
without	O
using	O
one	O
of	O
the	O
holes	O
twice	O
.	O
similarly	O
,	O
you	O
can	O
’	O
t	O
give	O
ax	O
outcomes	O
unique	O
binary	O
names	O
of	O
some	O
length	B
l	O
shorter	O
than	O
log2	O
jaxj	O
bits	O
,	O
because	O
there	O
are	O
only	O
2l	O
such	O
binary	O
names	O
,	O
and	O
l	O
<	O
log2	O
jaxj	O
implies	O
2l	O
<	O
jaxj	O
,	O
so	O
at	O
least	O
two	O
di	O
(	O
cid:11	O
)	O
erent	O
inputs	O
to	O
the	O
compressor	O
would	O
compress	B
to	O
the	O
same	O
output	O
(	O
cid:12	O
)	O
le	O
.	O
solution	O
to	O
exercise	O
4.8	O
(	O
p.76	O
)	O
.	O
between	O
the	O
cusps	O
,	O
all	O
the	O
changes	O
in	O
proba-	O
bility	O
are	O
equal	O
,	O
and	O
the	O
number	O
of	O
elements	O
in	O
t	O
changes	O
by	O
one	O
at	O
each	O
step	O
.	O
so	O
h	O
(	O
cid:14	O
)	O
varies	O
logarithmically	O
with	O
(	O
(	O
cid:0	O
)	O
(	O
cid:14	O
)	O
)	O
.	O
solution	O
to	O
exercise	O
4.13	O
(	O
p.84	O
)	O
.	O
this	O
solution	O
was	O
found	O
by	O
dyson	O
and	O
lyness	O
in	O
1946	O
and	O
presented	O
in	O
the	O
following	O
elegant	O
form	O
by	O
john	O
conway	O
in	O
1999.	O
be	O
warned	O
:	O
the	O
symbols	O
a	O
,	O
b	O
,	O
and	O
c	O
are	O
used	O
to	O
name	O
the	O
balls	O
,	O
to	O
name	O
the	O
pans	O
of	O
the	O
balance	O
,	O
to	O
name	O
the	O
outcomes	O
,	O
and	O
to	O
name	O
the	O
possible	O
states	O
of	O
the	O
odd	O
ball	O
!	O
(	O
a	O
)	O
label	O
the	O
12	O
balls	O
by	O
the	O
sequences	O
aab	O
aba	O
abb	O
abc	O
bbc	O
bca	O
bcb	O
bcc	O
caa	O
cab	O
cac	O
cca	O
and	O
in	O
the	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
4.8	O
:	O
solutions	O
87	O
1st	O
2nd	O
weighings	O
put	O
aab	O
caa	O
cab	O
cac	O
in	O
pan	O
a	O
,	O
aba	O
abb	O
abc	O
bbc	O
in	O
pan	O
b	O
.	O
3rd	O
aab	O
aba	O
abb	O
abc	O
bbc	O
bca	O
bcb	O
bcc	O
aba	O
bca	O
caa	O
cca	O
aab	O
abb	O
bcb	O
cab	O
now	O
in	O
a	O
given	O
weighing	O
,	O
a	O
pan	O
will	O
either	O
end	O
up	O
in	O
the	O
(	O
cid:15	O
)	O
canonical	B
position	O
(	O
c	O
)	O
that	O
it	O
assumes	O
when	O
the	O
pans	O
are	O
balanced	O
,	O
or	O
(	O
cid:15	O
)	O
above	O
that	O
position	O
(	O
a	O
)	O
,	O
or	O
(	O
cid:15	O
)	O
below	O
it	O
(	O
b	O
)	O
,	O
so	O
the	O
three	O
weighings	O
determine	O
for	O
each	O
pan	O
a	O
sequence	B
of	O
three	O
of	O
these	O
letters	O
.	O
if	O
both	O
sequences	O
are	O
ccc	O
,	O
then	O
there	O
’	O
s	O
no	O
odd	O
ball	O
.	O
otherwise	O
,	O
for	O
just	O
one	O
of	O
the	O
two	O
pans	O
,	O
the	O
sequence	B
is	O
among	O
the	O
12	O
above	O
,	O
and	O
names	O
the	O
odd	O
ball	O
,	O
whose	O
weight	B
is	O
above	O
or	O
below	O
the	O
proper	B
one	O
according	O
as	O
the	O
pan	O
is	O
a	O
or	O
b	O
.	O
(	O
b	O
)	O
in	O
w	O
weighings	O
the	O
odd	O
ball	O
can	O
be	O
identi	O
(	O
cid:12	O
)	O
ed	O
from	O
among	O
n	O
=	O
(	O
3w	O
(	O
cid:0	O
)	O
3	O
)	O
=2	O
(	O
4.49	O
)	O
balls	O
in	O
the	O
same	O
way	O
,	O
by	O
labelling	O
them	O
with	O
all	O
the	O
non-constant	O
se-	O
quences	O
of	O
w	O
letters	O
from	O
a	O
,	O
b	O
,	O
c	O
whose	O
(	O
cid:12	O
)	O
rst	O
change	O
is	O
a-to-b	O
or	O
b-to-c	O
or	O
c-to-a	O
,	O
and	O
at	O
the	O
wth	O
weighing	O
putting	O
those	O
whose	O
wth	O
letter	O
is	O
a	O
in	O
pan	O
a	O
and	O
those	O
whose	O
wth	O
letter	O
is	O
b	O
in	O
pan	O
b.	O
solution	O
to	O
exercise	O
4.15	O
(	O
p.85	O
)	O
.	O
the	O
curves	O
1	O
n	O
=	O
1	O
;	O
2	O
and	O
1000	O
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
4.14.	O
note	O
that	O
h2	O
(	O
0:2	O
)	O
=	O
0:72	O
bits	O
.	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
n	O
)	O
as	O
a	O
function	B
of	O
(	O
cid:14	O
)	O
for	O
n=1	O
n=2	O
n=1000	O
n	O
=	O
1	O
n	O
=	O
2	O
(	O
cid:14	O
)	O
1	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
)	O
2h	O
(	O
cid:14	O
)	O
(	O
x	O
)	O
(	O
cid:14	O
)	O
1	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
)	O
2h	O
(	O
cid:14	O
)	O
(	O
x	O
)	O
0	O
{	O
0.2	O
0.2	O
{	O
1	O
1	O
0	O
2	O
1	O
0	O
{	O
0.04	O
0.04	O
{	O
0.2	O
0.2	O
{	O
0.36	O
0.36	O
{	O
1	O
1	O
0.79	O
0.5	O
0	O
4	O
3	O
2	O
1	O
figure	O
4.14	O
.	O
1	O
n	O
h	O
(	O
cid:14	O
)	O
(	O
x	O
)	O
(	O
vertical	O
axis	O
)	O
against	O
(	O
cid:14	O
)	O
(	O
horizontal	O
)	O
,	O
for	O
n	O
=	O
1	O
;	O
2	O
;	O
100	O
binary	O
variables	O
with	O
p1	O
=	O
0:4	O
.	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
solution	O
to	O
exercise	O
4.17	O
(	O
p.85	O
)	O
.	O
the	O
gibbs	O
entropy	B
is	O
kbpi	O
pi	O
ln	O
1	O
,	O
where	O
i	O
runs	O
over	O
all	O
states	O
of	O
the	O
system	O
.	O
this	O
entropy	B
is	O
equivalent	O
(	O
apart	O
from	O
the	O
factor	O
of	O
kb	O
)	O
to	O
the	O
shannon	O
entropy	O
of	O
the	O
ensemble	O
.	O
pi	O
whereas	O
the	O
gibbs	O
entropy	B
can	O
be	O
de	O
(	O
cid:12	O
)	O
ned	O
for	O
any	O
ensemble	B
,	O
the	O
boltz-	O
mann	O
entropy	B
is	O
only	O
de	O
(	O
cid:12	O
)	O
ned	O
for	O
microcanonical	O
ensembles	O
,	O
which	O
have	O
a	O
probability	B
distribution	O
that	O
is	O
uniform	O
over	O
a	O
set	B
of	O
accessible	O
states	O
.	O
the	O
boltzmann	O
entropy	B
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
sb	O
=	O
kb	O
ln	O
(	O
cid:10	O
)	O
where	O
(	O
cid:10	O
)	O
is	O
the	O
number	O
of	O
ac-	O
cessible	O
states	O
of	O
the	O
microcanonical	O
ensemble	B
.	O
this	O
is	O
equivalent	O
(	O
apart	O
from	O
the	O
factor	O
of	O
kb	O
)	O
to	O
the	O
perfect	B
information	O
content	B
h0	O
of	O
that	O
constrained	B
ensemble	O
.	O
the	O
gibbs	O
entropy	B
of	O
a	O
microcanonical	B
ensemble	O
is	O
trivially	O
equal	O
to	O
the	O
boltzmann	O
entropy	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
88	O
4	O
|	O
the	O
source	B
coding	I
theorem	I
we	O
now	O
consider	O
a	O
thermal	B
distribution	I
(	O
the	O
canonical	B
ensemble	O
)	O
,	O
where	O
the	O
probability	O
of	O
a	O
state	O
x	O
is	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
e	O
(	O
x	O
)	O
kbt	O
(	O
cid:19	O
)	O
:	O
(	O
4.50	O
)	O
with	O
this	O
canonical	B
ensemble	O
we	O
can	O
associate	O
a	O
corresponding	O
microcanonical	B
ensemble	O
,	O
an	O
ensemble	B
with	O
total	O
energy	B
(	O
cid:12	O
)	O
xed	O
to	O
the	O
mean	B
energy	O
of	O
the	O
canonical	O
ensemble	B
(	O
(	O
cid:12	O
)	O
xed	O
to	O
within	O
some	O
precision	B
(	O
cid:15	O
)	O
)	O
.	O
now	O
,	O
(	O
cid:12	O
)	O
xing	O
the	O
total	O
energy	B
to	O
a	O
precision	B
(	O
cid:15	O
)	O
is	O
equivalent	O
to	O
(	O
cid:12	O
)	O
xing	O
the	O
value	O
of	O
ln	O
1/p	O
(	O
x	O
)	O
to	O
within	O
(	O
cid:15	O
)	O
kbt	O
.	O
our	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
the	O
typical	O
set	B
tn	O
(	O
cid:12	O
)	O
was	O
precisely	O
that	O
it	O
consisted	O
of	O
all	O
elements	O
that	O
have	O
a	O
value	O
of	O
log	O
p	O
(	O
x	O
)	O
very	O
close	O
to	O
the	O
mean	B
value	O
of	O
log	O
p	O
(	O
x	O
)	O
under	O
the	O
canonical	B
ensemble	O
,	O
(	O
cid:0	O
)	O
n	O
h	O
(	O
x	O
)	O
.	O
thus	O
the	O
microcanonical	B
ensemble	O
is	O
equivalent	O
to	O
a	O
uniform	O
distribution	B
over	O
the	O
typical	B
set	I
of	O
the	O
canonical	B
ensemble	O
.	O
our	O
proof	O
of	O
the	O
‘	O
asymptotic	B
equipartition	I
’	O
principle	O
thus	O
proves	O
{	O
for	O
the	O
case	O
of	O
a	O
system	O
whose	O
energy	B
is	O
separable	O
into	O
a	O
sum	O
of	O
independent	O
terms	O
{	O
that	O
the	O
boltzmann	O
entropy	B
of	O
the	O
microcanonical	B
ensemble	O
is	O
very	O
close	O
(	O
for	O
large	O
n	O
)	O
to	O
the	O
gibbs	O
entropy	B
of	O
the	O
canonical	B
ensemble	O
,	O
if	O
the	O
energy	B
of	O
the	O
microcanonical	B
ensemble	O
is	O
constrained	B
to	O
equal	O
the	O
mean	B
energy	O
of	O
the	O
canonical	O
ensemble	B
.	O
solution	O
to	O
exercise	O
4.18	O
(	O
p.85	O
)	O
.	O
the	O
normalizing	B
constant	I
of	O
the	O
cauchy	O
dis-	O
tribution	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
1	O
x2	O
+	O
1	O
is	O
z	O
=z	O
1	O
(	O
cid:0	O
)	O
1	O
dx	O
1	O
x2	O
+	O
1	O
=	O
(	O
cid:2	O
)	O
tan	O
(	O
cid:0	O
)	O
1x	O
(	O
cid:3	O
)	O
1	O
(	O
cid:0	O
)	O
1	O
=	O
(	O
cid:25	O
)	O
2	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
2	O
=	O
(	O
cid:25	O
)	O
:	O
(	O
4.51	O
)	O
the	O
mean	B
and	O
variance	B
of	O
this	O
distribution	B
are	O
both	O
unde	O
(	O
cid:12	O
)	O
ned	O
.	O
(	O
the	O
distribu-	O
tion	O
is	O
symmetrical	O
about	O
zero	O
,	O
but	O
this	O
does	O
not	O
imply	O
that	O
its	O
mean	B
is	O
zero	O
.	O
the	O
mean	B
is	O
the	O
value	O
of	O
a	O
divergent	O
integral	B
.	O
)	O
the	O
sum	O
z	O
=	O
x1	O
+	O
x2	O
,	O
where	O
x1	O
and	O
x2	O
both	O
have	O
cauchy	O
distributions	O
,	O
has	O
probability	B
density	O
given	O
by	O
the	O
convolution	B
p	O
(	O
z	O
)	O
=	O
1	O
(	O
cid:25	O
)	O
2	O
z	O
1	O
(	O
cid:0	O
)	O
1	O
dx1	O
1	O
x2	O
1	O
+	O
1	O
1	O
(	O
z	O
(	O
cid:0	O
)	O
x1	O
)	O
2	O
+	O
1	O
;	O
(	O
4.52	O
)	O
which	O
after	O
a	O
considerable	O
labour	O
using	O
standard	O
methods	O
gives	O
p	O
(	O
z	O
)	O
=	O
1	O
(	O
cid:25	O
)	O
2	O
2	O
(	O
cid:25	O
)	O
z2	O
+	O
4	O
=	O
2	O
(	O
cid:25	O
)	O
1	O
z2	O
+	O
22	O
;	O
(	O
4.53	O
)	O
which	O
we	O
recognize	O
as	O
a	O
cauchy	O
distribution	B
with	O
width	O
parameter	O
2	O
(	O
where	O
the	O
original	O
distribution	B
has	O
width	O
parameter	O
1	O
)	O
.	O
this	O
implies	O
that	O
the	O
mean	B
of	O
the	O
two	O
points	O
,	O
(	O
cid:22	O
)	O
x	O
=	O
(	O
x1	O
+	O
x2	O
)	O
=2	O
=	O
z=2	O
,	O
has	O
a	O
cauchy	O
distribution	B
with	O
width	O
parameter	O
1.	O
generalizing	O
,	O
the	O
mean	B
of	O
n	O
samples	O
from	O
a	O
cauchy	O
distribution	B
is	O
cauchy-distributed	O
with	O
the	O
same	O
parameters	B
as	O
the	O
individual	O
samples	O
.	O
the	O
probability	B
distribution	O
of	O
the	O
mean	O
does	O
not	O
become	O
narrower	O
as	O
1=pn	O
.	O
the	O
central-limit	B
theorem	I
does	O
not	O
apply	O
to	O
the	O
cauchy	O
distribution	B
,	O
be-	O
cause	O
it	O
does	O
not	O
have	O
a	O
(	O
cid:12	O
)	O
nite	O
variance	B
.	O
an	O
alternative	O
neat	O
method	B
for	O
getting	O
to	O
equation	O
(	O
4.53	O
)	O
makes	O
use	O
of	O
the	O
fourier	O
transform	O
of	O
the	O
cauchy	O
distribution	B
,	O
which	O
is	O
a	O
biexponential	B
e	O
(	O
cid:0	O
)	O
j	O
!	O
j	O
.	O
convolution	B
in	O
real	O
space	O
corresponds	O
to	O
multiplication	O
in	O
fourier	O
space	O
,	O
so	O
the	O
fourier	O
transform	O
of	O
z	O
is	O
simply	O
e	O
(	O
cid:0	O
)	O
j2	O
!	O
j	O
.	O
reversing	O
the	O
transform	O
,	O
we	O
obtain	O
equation	O
(	O
4.53	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
4.8	O
:	O
solutions	O
89	O
solution	O
to	O
exercise	O
4.20	O
(	O
p.86	O
)	O
.	O
the	O
function	B
f	O
(	O
x	O
)	O
has	O
inverse	O
function	O
note	O
g	O
(	O
y	O
)	O
=	O
y1=y	O
:	O
log	O
g	O
(	O
y	O
)	O
=	O
1=y	O
log	O
y	O
:	O
(	O
4.54	O
)	O
(	O
4.55	O
)	O
i	O
obtained	O
a	O
tentative	O
graph	B
of	O
f	O
(	O
x	O
)	O
by	O
plotting	O
g	O
(	O
y	O
)	O
with	O
y	O
along	O
the	O
vertical	O
axis	O
and	O
g	O
(	O
y	O
)	O
along	O
the	O
horizontal	O
axis	O
.	O
the	O
resulting	O
graph	B
suggests	O
that	O
f	O
(	O
x	O
)	O
is	O
single	O
valued	O
for	O
x	O
2	O
(	O
0	O
;	O
1	O
)	O
,	O
and	O
looks	O
surprisingly	O
well-behaved	O
and	O
ordinary	O
;	O
for	O
x	O
2	O
(	O
1	O
;	O
e1=e	O
)	O
,	O
f	O
(	O
x	O
)	O
is	O
two-valued	O
.	O
f	O
(	O
p2	O
)	O
is	O
equal	O
both	O
to	O
2	O
and	O
4.	O
for	O
x	O
>	O
e1=e	O
(	O
which	O
is	O
about	O
1.44	O
)	O
,	O
f	O
(	O
x	O
)	O
is	O
in	O
(	O
cid:12	O
)	O
nite	O
.	O
however	O
,	O
it	O
might	O
be	O
argued	O
that	O
this	O
approach	O
to	O
sketching	O
f	O
(	O
x	O
)	O
is	O
only	O
partly	O
valid	O
,	O
if	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
f	O
as	O
the	O
limit	O
of	O
the	O
sequence	B
of	O
functions	B
x	O
,	O
xx	O
,	O
xxx	O
;	O
:	O
:	O
:	O
;	O
this	O
sequence	B
does	O
not	O
have	O
a	O
limit	O
for	O
0	O
(	O
cid:20	O
)	O
x	O
(	O
cid:20	O
)	O
(	O
1=e	O
)	O
e	O
’	O
0:07	O
on	O
account	O
of	O
a	O
pitchfork	B
bifurcation	I
at	O
x	O
=	O
(	O
1=e	O
)	O
e	O
;	O
and	O
for	O
x	O
2	O
(	O
1	O
;	O
e1=e	O
)	O
,	O
the	O
sequence	B
’	O
s	O
limit	O
is	O
single-valued	O
{	O
the	O
lower	O
of	O
the	O
two	O
values	O
sketched	O
in	O
the	O
(	O
cid:12	O
)	O
gure	O
.	O
50	O
40	O
30	O
20	O
10	O
0	O
5	O
4	O
3	O
2	O
1	O
0	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
1.2	O
1.4	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
1.2	O
1.4	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
figure	O
4.15.	O
f	O
(	O
x	O
)	O
=	O
xxx	O
;	O
at	O
three	O
di	O
(	O
cid:11	O
)	O
erent	O
scales	O
.	O
x	O
0.2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
x	O
shown	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
chapter	O
5	O
in	O
the	O
last	O
chapter	O
,	O
we	O
saw	O
a	O
proof	O
of	O
the	O
fundamental	O
status	O
of	O
the	O
entropy	O
as	O
a	O
measure	O
of	O
average	O
information	B
content	I
.	O
we	O
de	O
(	O
cid:12	O
)	O
ned	O
a	O
data	B
compression	I
scheme	O
using	O
(	O
cid:12	O
)	O
xed	O
length	B
block	O
codes	O
,	O
and	O
proved	O
that	O
as	O
n	O
increases	O
,	O
it	O
is	O
possible	O
to	O
encode	O
n	O
i.i.d	O
.	O
variables	O
x	O
=	O
(	O
x1	O
;	O
:	O
:	O
:	O
;	O
xn	O
)	O
into	O
a	O
block	B
of	O
n	O
(	O
h	O
(	O
x	O
)	O
+	O
(	O
cid:15	O
)	O
)	O
bits	O
with	O
vanishing	O
probability	B
of	I
error	I
,	O
whereas	O
if	O
we	O
attempt	O
to	O
encode	O
x	O
n	O
into	O
n	O
(	O
h	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
)	O
bits	O
,	O
the	O
probability	B
of	I
error	I
is	O
virtually	O
1.	O
we	O
thus	O
veri	O
(	O
cid:12	O
)	O
ed	O
the	O
possibility	O
of	O
data	O
compression	B
,	O
but	O
the	O
block	B
coding	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
the	O
proof	O
did	O
not	O
give	O
a	O
practical	B
algorithm	O
.	O
in	O
this	O
chapter	O
and	O
the	O
next	O
,	O
we	O
study	O
practical	B
data	O
compression	B
algorithms	O
.	O
whereas	O
the	O
last	O
chapter	O
’	O
s	O
compression	B
scheme	O
used	O
large	O
blocks	O
of	O
(	O
cid:12	O
)	O
xed	O
size	O
and	O
was	O
lossy	B
,	O
in	O
the	O
next	O
chapter	O
we	O
discuss	O
variable-length	B
compression	O
schemes	O
that	O
are	O
practical	B
for	O
small	O
block	B
sizes	O
and	O
that	O
are	O
not	O
lossy	B
.	O
imagine	O
a	O
rubber	O
glove	O
(	O
cid:12	O
)	O
lled	O
with	O
water	O
.	O
if	O
we	O
compress	B
two	O
(	O
cid:12	O
)	O
ngers	O
of	O
the	O
glove	O
,	O
some	O
other	O
part	O
of	O
the	O
glove	O
has	O
to	O
expand	O
,	O
because	O
the	O
total	O
volume	B
of	O
water	O
is	O
constant	O
.	O
(	O
water	O
is	O
essentially	O
incompressible	O
.	O
)	O
similarly	O
,	O
when	O
we	O
shorten	O
the	O
codewords	O
for	O
some	O
outcomes	O
,	O
there	O
must	O
be	O
other	O
codewords	O
that	O
get	O
longer	O
,	O
if	O
the	O
scheme	O
is	O
not	O
lossy	B
.	O
in	O
this	O
chapter	O
we	O
will	O
discover	O
the	O
information-theoretic	O
equivalent	O
of	O
water	O
volume	B
.	O
before	O
reading	O
chapter	O
5	O
,	O
you	O
should	O
have	O
worked	O
on	O
exercise	O
2.26	O
(	O
p.37	O
)	O
.	O
we	O
will	O
use	O
the	O
following	O
notation	B
for	O
intervals	B
:	O
x	O
2	O
[	O
1	O
;	O
2	O
)	O
means	O
that	O
x	O
(	O
cid:21	O
)	O
1	O
and	O
x	O
<	O
2	O
;	O
x	O
2	O
(	O
1	O
;	O
2	O
]	O
means	O
that	O
x	O
>	O
1	O
and	O
x	O
(	O
cid:20	O
)	O
2	O
.	O
90	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
5	O
symbol	O
codes	O
in	O
this	O
chapter	O
,	O
we	O
discuss	O
variable-length	B
symbol	O
codes	O
,	O
which	O
encode	O
one	O
source	O
symbol	O
at	O
a	O
time	O
,	O
instead	O
of	O
encoding	O
huge	O
strings	O
of	O
n	O
source	O
sym-	O
bols	O
.	O
these	O
codes	O
are	O
lossless	B
:	O
unlike	O
the	O
last	O
chapter	O
’	O
s	O
block	B
codes	O
,	O
they	O
are	O
guaranteed	O
to	O
compress	B
and	O
decompress	O
without	O
any	O
errors	B
;	O
but	O
there	O
is	O
a	O
chance	O
that	O
the	O
codes	O
may	O
sometimes	O
produce	O
encoded	O
strings	O
longer	O
than	O
the	O
original	O
source	O
string	O
.	O
the	O
idea	O
is	O
that	O
we	O
can	O
achieve	O
compression	B
,	O
on	O
average	O
,	O
by	O
assigning	O
shorter	O
encodings	O
to	O
the	O
more	O
probable	O
outcomes	O
and	O
longer	O
encodings	O
to	O
the	O
less	O
probable	O
.	O
the	O
key	O
issues	O
are	O
:	O
what	O
are	O
the	O
implications	O
if	O
a	O
symbol	B
code	I
is	O
lossless	B
?	O
if	O
some	O
code-	O
words	O
are	O
shortened	O
,	O
by	O
how	O
much	O
do	O
other	O
codewords	O
have	O
to	O
be	O
length-	O
ened	O
?	O
making	O
compression	B
practical	O
.	O
how	O
can	O
we	O
ensure	O
that	O
a	O
symbol	B
code	I
is	O
easy	O
to	O
decode	O
?	O
optimal	B
symbol	O
codes	O
.	O
how	O
should	O
we	O
assign	O
codelengths	O
to	O
achieve	O
the	O
best	O
compression	B
,	O
and	O
what	O
is	O
the	O
best	O
achievable	O
compression	B
?	O
we	O
again	O
verify	O
the	O
fundamental	O
status	O
of	O
the	O
shannon	O
information	B
content	I
and	O
the	O
entropy	B
,	O
proving	O
:	O
source	B
coding	I
theorem	I
(	O
symbol	O
codes	O
)	O
.	O
there	O
exists	O
a	O
variable-length	B
encoding	O
c	O
of	O
an	O
ensemble	B
x	O
such	O
that	O
the	O
average	B
length	O
of	O
an	O
en-	O
coded	O
symbol	O
,	O
l	O
(	O
c	O
;	O
x	O
)	O
,	O
satis	O
(	O
cid:12	O
)	O
es	O
l	O
(	O
c	O
;	O
x	O
)	O
2	O
[	O
h	O
(	O
x	O
)	O
;	O
h	O
(	O
x	O
)	O
+	O
1	O
)	O
.	O
the	O
average	B
length	O
is	O
equal	O
to	O
the	O
entropy	B
h	O
(	O
x	O
)	O
only	O
if	O
the	O
codelength	O
for	O
each	O
outcome	O
is	O
equal	O
to	O
its	O
shannon	O
information	O
content	B
.	O
we	O
will	O
also	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
constructive	O
procedure	O
,	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
coding	O
algorithm	O
,	O
that	O
produces	O
optimal	B
symbol	O
codes	O
.	O
notation	B
for	O
alphabets	O
.	O
an	O
denotes	O
the	O
set	B
of	O
ordered	O
n	O
-tuples	O
of	O
ele-	O
ments	O
from	O
the	O
set	B
a	O
,	O
i.e.	O
,	O
all	O
strings	O
of	O
length	O
n	O
.	O
the	O
symbol	O
a+	O
will	O
denote	O
the	O
set	B
of	O
all	O
strings	O
of	O
(	O
cid:12	O
)	O
nite	O
length	B
composed	O
of	O
elements	O
from	O
the	O
set	B
a.	O
example	O
5.1.	O
f0	O
;	O
1g3	O
=	O
f000	O
;	O
001	O
;	O
010	O
;	O
011	O
;	O
100	O
;	O
101	O
;	O
110	O
;	O
111g	O
.	O
example	O
5.2.	O
f0	O
;	O
1g+	O
=	O
f0	O
;	O
1	O
;	O
00	O
;	O
01	O
;	O
10	O
;	O
11	O
;	O
000	O
;	O
001	O
;	O
:	O
:	O
:	O
g.	O
91	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
5	O
|	O
symbol	O
codes	O
92	O
5.1	O
symbol	O
codes	O
a	O
(	O
binary	O
)	O
symbol	B
code	I
c	O
for	O
an	O
ensemble	B
x	O
is	O
a	O
mapping	B
from	O
the	O
range	O
of	O
x	O
,	O
ax	O
=fa1	O
;	O
:	O
:	O
:	O
;	O
aig	O
,	O
to	O
f0	O
;	O
1g+	O
.	O
c	O
(	O
x	O
)	O
will	O
denote	O
the	O
codeword	B
cor-	O
responding	O
to	O
x	O
,	O
and	O
l	O
(	O
x	O
)	O
will	O
denote	O
its	O
length	B
,	O
with	O
li	O
=	O
l	O
(	O
ai	O
)	O
.	O
the	O
extended	B
code	I
c	O
+	O
is	O
a	O
mapping	B
from	O
a+	O
concatenation	B
,	O
without	O
punctuation	O
,	O
of	O
the	O
corresponding	O
codewords	O
:	O
x	O
to	O
f0	O
;	O
1g+	O
obtained	O
by	O
c+	O
(	O
x1x2	O
:	O
:	O
:	O
xn	O
)	O
=	O
c	O
(	O
x1	O
)	O
c	O
(	O
x2	O
)	O
:	O
:	O
:	O
c	O
(	O
xn	O
)	O
:	O
(	O
5.1	O
)	O
[	O
the	O
term	O
‘	O
mapping	B
’	O
here	O
is	O
a	O
synonym	O
for	O
‘	O
function	B
’	O
.	O
]	O
example	O
5.3.	O
a	O
symbol	B
code	I
for	O
the	O
ensemble	B
x	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
ax	O
=	O
f	O
a	O
;	O
b	O
;	O
c	O
;	O
d	O
g	O
;	O
px	O
=	O
f	O
1/2	O
;	O
1/4	O
;	O
1/8	O
;	O
1/8	O
g	O
;	O
(	O
5.2	O
)	O
is	O
c0	O
,	O
shown	O
in	O
the	O
margin	O
.	O
using	O
the	O
extended	B
code	I
,	O
we	O
may	O
encode	O
acdbac	O
as	O
c+	O
(	O
acdbac	O
)	O
=	O
100000100001010010000010	O
:	O
(	O
5.3	O
)	O
c0	O
:	O
ai	O
c	O
(	O
ai	O
)	O
a	O
b	O
c	O
d	O
1000	O
0100	O
0010	O
0001	O
li	O
4	O
4	O
4	O
4	O
there	O
are	O
basic	O
requirements	O
for	O
a	O
useful	O
symbol	B
code	I
.	O
first	O
,	O
any	O
encoded	O
string	O
must	O
have	O
a	O
unique	O
decoding	B
.	O
second	O
,	O
the	O
symbol	B
code	I
must	O
be	O
easy	O
to	O
decode	O
.	O
and	O
third	O
,	O
the	O
code	B
should	O
achieve	O
as	O
much	O
compression	B
as	O
possible	O
.	O
any	O
encoded	O
string	O
must	O
have	O
a	O
unique	O
decoding	B
a	O
code	B
c	O
(	O
x	O
)	O
is	O
uniquely	B
decodeable	I
if	O
,	O
under	O
the	O
extended	B
code	I
c	O
+	O
,	O
no	O
two	O
distinct	O
strings	O
have	O
the	O
same	O
encoding	O
,	O
i.e.	O
,	O
8	O
x	O
;	O
y	O
2	O
a+	O
x	O
;	O
x	O
6=	O
y	O
)	O
c+	O
(	O
x	O
)	O
6=	O
c+	O
(	O
y	O
)	O
:	O
(	O
5.4	O
)	O
the	O
code	B
c0	O
de	O
(	O
cid:12	O
)	O
ned	O
above	O
is	O
an	O
example	O
of	O
a	O
uniquely	B
decodeable	I
code	O
.	O
the	O
symbol	B
code	I
must	O
be	O
easy	O
to	O
decode	O
a	O
symbol	B
code	I
is	O
easiest	O
to	O
decode	O
if	O
it	O
is	O
possible	O
to	O
identify	O
the	O
end	O
of	O
a	O
codeword	B
as	O
soon	O
as	O
it	O
arrives	O
,	O
which	O
means	O
that	O
no	O
codeword	B
can	O
be	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
of	O
another	O
codeword	B
.	O
[	O
a	O
word	O
c	O
is	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
of	O
another	O
word	O
d	O
if	O
there	O
exists	O
a	O
tail	B
string	O
t	O
such	O
that	O
the	O
concatenation	B
ct	O
is	O
identical	O
to	O
d.	O
for	O
example	O
,	O
1	O
is	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
of	O
101	O
,	O
and	O
so	O
is	O
10	O
.	O
]	O
we	O
will	O
show	O
later	O
that	O
we	O
don	O
’	O
t	O
lose	O
any	O
performance	O
if	O
we	O
constrain	O
our	O
symbol	B
code	I
to	O
be	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
.	O
a	O
symbol	B
code	I
is	O
called	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
if	O
no	O
codeword	B
is	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
of	O
any	O
other	O
codeword	B
.	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
is	O
also	O
known	O
as	O
an	O
instantaneous	B
or	O
self-punctuating	B
code	O
,	O
because	O
an	O
encoded	O
string	O
can	O
be	O
decoded	O
from	O
left	O
to	O
right	O
without	O
looking	O
ahead	O
to	O
subsequent	O
codewords	O
.	O
the	O
end	O
of	O
a	O
codeword	B
is	O
im-	O
mediately	O
recognizable	O
.	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
is	O
uniquely	B
decodeable	I
.	O
pre	O
(	O
cid:12	O
)	O
x	O
codes	O
are	O
also	O
known	O
as	O
‘	O
pre	O
(	O
cid:12	O
)	O
x-free	O
codes	O
’	O
or	O
‘	O
pre	O
(	O
cid:12	O
)	O
x	O
condition	O
codes	O
’	O
.	O
pre	O
(	O
cid:12	O
)	O
x	O
codes	O
correspond	O
to	O
trees	O
,	O
as	O
illustrated	O
in	O
the	O
margin	O
of	O
the	O
next	O
page	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
5.1	O
:	O
symbol	O
codes	O
93	O
example	O
5.4.	O
the	O
code	B
c1	O
=	O
f0	O
;	O
101g	O
is	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
because	O
0	O
is	O
not	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
of	O
101	O
,	O
nor	O
is	O
101	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
of	O
0.	O
example	O
5.5.	O
let	O
c2	O
=	O
f1	O
;	O
101g	O
.	O
this	O
code	B
is	O
not	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
because	O
1	O
is	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
of	O
101.	O
example	O
5.6.	O
the	O
code	B
c3	O
=	O
f0	O
;	O
10	O
;	O
110	O
;	O
111g	O
is	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
.	O
example	O
5.7.	O
the	O
code	B
c4	O
=	O
f00	O
;	O
01	O
;	O
10	O
;	O
11g	O
is	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
.	O
exercise	O
5.8	O
.	O
[	O
1	O
,	O
p.104	O
]	O
is	O
c2	O
uniquely	B
decodeable	I
?	O
example	O
5.9.	O
consider	O
exercise	O
4.1	O
(	O
p.66	O
)	O
and	O
(	O
cid:12	O
)	O
gure	O
4.2	O
(	O
p.69	O
)	O
.	O
any	O
weighing	O
strategy	O
that	O
identi	O
(	O
cid:12	O
)	O
es	O
the	O
odd	O
ball	O
and	O
whether	O
it	O
is	O
heavy	O
or	O
light	O
can	O
be	O
viewed	O
as	O
assigning	O
a	O
ternary	O
code	B
to	O
each	O
of	O
the	O
24	O
possible	O
states	O
.	O
this	O
code	B
is	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
.	O
the	O
code	B
should	O
achieve	O
as	O
much	O
compression	B
as	O
possible	O
the	O
expected	O
length	B
l	O
(	O
c	O
;	O
x	O
)	O
of	O
a	O
symbol	B
code	I
c	O
for	O
ensemble	O
x	O
is	O
l	O
(	O
c	O
;	O
x	O
)	O
=	O
xx2ax	O
p	O
(	O
x	O
)	O
l	O
(	O
x	O
)	O
:	O
(	O
5.5	O
)	O
we	O
may	O
also	O
write	O
this	O
quantity	O
as	O
where	O
i	O
=	O
jaxj	O
.	O
example	O
5.10.	O
let	O
l	O
(	O
c	O
;	O
x	O
)	O
=	O
pili	O
i	O
xi=1	O
and	O
ax	O
=	O
f	O
a	O
;	O
b	O
;	O
c	O
;	O
d	O
g	O
;	O
px	O
=	O
f	O
1/2	O
;	O
1/4	O
;	O
1/8	O
;	O
1/8g	O
;	O
(	O
5.6	O
)	O
(	O
5.7	O
)	O
and	O
consider	O
the	O
code	B
c3	O
.	O
the	O
entropy	B
of	O
x	O
is	O
1.75	O
bits	O
,	O
and	O
the	O
expected	O
length	B
l	O
(	O
c3	O
;	O
x	O
)	O
of	O
this	O
code	B
is	O
also	O
1.75	O
bits	O
.	O
the	O
sequence	B
of	O
symbols	O
x	O
=	O
(	O
acdbac	O
)	O
is	O
encoded	O
as	O
c+	O
(	O
x	O
)	O
=	O
0110111100110.	O
c3	O
is	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
and	O
is	O
therefore	O
uniquely	B
decodeable	I
.	O
notice	O
that	O
the	O
codeword	B
lengths	O
satisfy	O
li	O
=	O
log2	O
(	O
1=pi	O
)	O
,	O
or	O
equivalently	O
,	O
pi	O
=	O
2	O
(	O
cid:0	O
)	O
li	O
.	O
example	O
5.11.	O
consider	O
the	O
(	O
cid:12	O
)	O
xed	O
length	B
code	O
for	O
the	O
same	O
ensemble	B
x	O
,	O
c4	O
.	O
the	O
expected	O
length	B
l	O
(	O
c4	O
;	O
x	O
)	O
is	O
2	O
bits	O
.	O
example	O
5.12.	O
consider	O
c5	O
.	O
the	O
expected	O
length	B
l	O
(	O
c5	O
;	O
x	O
)	O
is	O
1.25	O
bits	O
,	O
which	O
is	O
less	O
than	O
h	O
(	O
x	O
)	O
.	O
but	O
the	O
code	B
is	O
not	O
uniquely	B
decodeable	I
.	O
the	O
se-	O
quence	O
x	O
=	O
(	O
acdbac	O
)	O
encodes	O
as	O
000111000	O
,	O
which	O
can	O
also	O
be	O
decoded	O
as	O
(	O
cabdca	O
)	O
.	O
example	O
5.13.	O
consider	O
the	O
code	B
c6	O
.	O
the	O
expected	O
length	B
l	O
(	O
c6	O
;	O
x	O
)	O
of	O
this	O
code	B
is	O
1.75	O
bits	O
.	O
the	O
sequence	B
of	O
symbols	O
x	O
=	O
(	O
acdbac	O
)	O
is	O
encoded	O
as	O
c+	O
(	O
x	O
)	O
=	O
0011111010011.	O
is	O
c6	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
?	O
it	O
is	O
not	O
,	O
because	O
c	O
(	O
a	O
)	O
=	O
0	O
is	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
of	O
both	O
c	O
(	O
b	O
)	O
and	O
c	O
(	O
c	O
)	O
.	O
0	O
0	O
0	O
1	O
c1	O
1	O
101	O
0	O
1	O
0	O
1	O
c3	O
c4	O
0	O
0	O
1	O
0	O
1	O
0	O
1	O
10	O
0	O
110	O
1	O
111	O
00	O
01	O
10	O
11	O
pre	O
(	O
cid:12	O
)	O
x	O
codes	O
can	O
be	O
represented	O
on	O
binary	O
trees	O
.	O
complete	O
pre	O
(	O
cid:12	O
)	O
x	O
codes	O
correspond	O
to	O
binary	O
trees	O
with	O
no	O
unused	O
branches	O
.	O
c1	O
is	O
an	O
incomplete	O
code	B
.	O
c3	O
:	O
pi	O
1/2	O
1/4	O
1/8	O
1/8	O
ai	O
c	O
(	O
ai	O
)	O
a	O
b	O
c	O
d	O
0	O
10	O
110	O
111	O
h	O
(	O
pi	O
)	O
1.0	O
2.0	O
3.0	O
3.0	O
li	O
1	O
2	O
3	O
3	O
c4	O
c5	O
a	O
b	O
c	O
d	O
00	O
01	O
10	O
11	O
0	O
1	O
00	O
11	O
c6	O
:	O
pi	O
1/2	O
1/4	O
1/8	O
1/8	O
ai	O
c	O
(	O
ai	O
)	O
a	O
b	O
c	O
d	O
0	O
01	O
011	O
111	O
h	O
(	O
pi	O
)	O
1.0	O
2.0	O
3.0	O
3.0	O
li	O
1	O
2	O
3	O
3	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
94	O
5	O
|	O
symbol	O
codes	O
is	O
c6	O
uniquely	B
decodeable	I
?	O
this	O
is	O
not	O
so	O
obvious	O
.	O
if	O
you	O
think	O
that	O
it	O
might	O
not	O
be	O
uniquely	B
decodeable	I
,	O
try	O
to	O
prove	O
it	O
so	O
by	O
(	O
cid:12	O
)	O
nding	O
a	O
pair	O
of	O
strings	O
x	O
and	O
y	O
that	O
have	O
the	O
same	O
encoding	O
.	O
[	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
unique	O
decodeability	O
is	O
given	O
in	O
equation	O
(	O
5.4	O
)	O
.	O
]	O
c6	O
certainly	O
isn	O
’	O
t	O
easy	O
to	O
decode	O
.	O
when	O
we	O
receive	O
‘	O
00	O
’	O
,	O
it	O
is	O
possible	O
that	O
x	O
could	O
start	O
‘	O
aa	O
’	O
,	O
‘	O
ab	O
’	O
or	O
‘	O
ac	O
’	O
.	O
once	O
we	O
have	O
received	O
‘	O
001111	O
’	O
,	O
the	O
second	O
symbol	O
is	O
still	O
ambiguous	O
,	O
as	O
x	O
could	O
be	O
‘	O
abd	O
.	O
.	O
.	O
’	O
or	O
‘	O
acd	O
.	O
.	O
.	O
’	O
.	O
but	O
eventually	O
a	O
unique	O
decoding	B
crystallizes	O
,	O
once	O
the	O
next	O
0	O
appears	O
in	O
the	O
encoded	O
stream	O
.	O
c6	O
is	O
in	O
fact	O
uniquely	B
decodeable	I
.	O
comparing	O
with	O
the	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
c3	O
,	O
we	O
see	O
that	O
the	O
codewords	O
of	O
c6	O
are	O
the	O
reverse	B
of	O
c3	O
’	O
s	O
.	O
that	O
c3	O
is	O
uniquely	B
decodeable	I
proves	O
that	O
c6	O
is	O
too	O
,	O
since	O
any	O
string	O
from	O
c6	O
is	O
identical	O
to	O
a	O
string	O
from	O
c3	O
read	O
backwards	O
.	O
5.2	O
what	O
limit	O
is	O
imposed	O
by	O
unique	O
decodeability	O
?	O
we	O
now	O
ask	O
,	O
given	O
a	O
list	O
of	O
positive	O
integers	O
flig	O
,	O
does	O
there	O
exist	O
a	O
uniquely	B
decodeable	I
code	O
with	O
those	O
integers	O
as	O
its	O
codeword	B
lengths	O
?	O
at	O
this	O
stage	O
,	O
we	O
ignore	O
the	O
probabilities	O
of	O
the	O
di	O
(	O
cid:11	O
)	O
erent	O
symbols	O
;	O
once	O
we	O
understand	O
unique	O
decodeability	O
better	O
,	O
we	O
’	O
ll	O
reintroduce	O
the	O
probabilities	O
and	O
discuss	O
how	O
to	O
make	O
an	O
optimal	B
uniquely	O
decodeable	O
symbol	B
code	I
.	O
in	O
the	O
examples	O
above	O
,	O
we	O
have	O
observed	O
that	O
if	O
we	O
take	O
a	O
code	B
such	O
as	O
f00	O
;	O
01	O
;	O
10	O
;	O
11g	O
,	O
and	O
shorten	O
one	O
of	O
its	O
codewords	O
,	O
for	O
example	O
00	O
!	O
0	O
,	O
then	O
we	O
can	O
retain	O
unique	O
decodeability	O
only	O
if	O
we	O
lengthen	O
other	O
codewords	O
.	O
thus	O
there	O
seems	O
to	O
be	O
a	O
constrained	B
budget	O
that	O
we	O
can	O
spend	O
on	O
codewords	O
,	O
with	O
shorter	O
codewords	O
being	O
more	O
expensive	O
.	O
let	O
us	O
explore	B
the	O
nature	O
of	O
this	O
budget	B
.	O
if	O
we	O
build	O
a	O
code	B
purely	O
from	O
codewords	O
of	O
length	O
l	O
equal	O
to	O
three	O
,	O
how	O
many	O
codewords	O
can	O
we	O
have	O
and	O
retain	O
unique	O
decodeability	O
?	O
the	O
answer	O
is	O
2l	O
=	O
8.	O
once	O
we	O
have	O
chosen	O
all	O
eight	O
of	O
these	O
codewords	O
,	O
is	O
there	O
any	O
way	O
we	O
could	O
add	O
to	O
the	O
code	B
another	O
codeword	B
of	O
some	O
other	O
length	B
and	O
retain	O
unique	O
decodeability	O
?	O
it	O
would	O
seem	O
not	O
.	O
what	O
if	O
we	O
make	O
a	O
code	B
that	O
includes	O
a	O
length-one	O
codeword	B
,	O
‘	O
0	O
’	O
,	O
with	O
the	O
other	O
codewords	O
being	O
of	O
length	O
three	O
?	O
how	O
many	O
length-three	O
codewords	O
can	O
we	O
have	O
?	O
if	O
we	O
restrict	O
attention	O
to	O
pre	O
(	O
cid:12	O
)	O
x	O
codes	O
,	O
then	O
we	O
can	O
have	O
only	O
four	O
codewords	O
of	O
length	O
three	O
,	O
namely	O
f100	O
;	O
101	O
;	O
110	O
;	O
111g	O
.	O
what	O
about	O
other	O
codes	O
?	O
is	O
there	O
any	O
other	O
way	O
of	O
choosing	O
codewords	O
of	O
length	O
3	O
that	O
can	O
give	O
more	O
codewords	O
?	O
intuitively	O
,	O
we	O
think	O
this	O
unlikely	O
.	O
a	O
codeword	B
of	O
length	B
3	O
appears	O
to	O
have	O
a	O
cost	O
that	O
is	O
22	O
times	O
smaller	O
than	O
a	O
codeword	B
of	O
length	B
1.	O
let	O
’	O
s	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
total	O
budget	B
of	O
size	O
1	O
,	O
which	O
we	O
can	O
spend	O
on	O
codewords	O
.	O
if	O
we	O
set	B
the	O
cost	O
of	O
a	O
codeword	B
whose	O
length	B
is	O
l	O
to	O
2	O
(	O
cid:0	O
)	O
l	O
,	O
then	O
we	O
have	O
a	O
pricing	O
system	O
that	O
(	O
cid:12	O
)	O
ts	O
the	O
examples	O
discussed	O
above	O
.	O
codewords	O
of	O
length	O
3	O
cost	O
1/8	O
each	O
;	O
codewords	O
of	O
length	O
1	O
cost	O
1=2	O
each	O
.	O
we	O
can	O
spend	O
our	O
budget	B
on	O
any	O
codewords	O
.	O
if	O
we	O
go	O
over	O
our	O
budget	B
then	O
the	O
code	B
will	O
certainly	O
not	O
be	O
uniquely	B
decodeable	I
.	O
if	O
,	O
on	O
the	O
other	O
hand	O
,	O
2	O
(	O
cid:0	O
)	O
li	O
(	O
cid:20	O
)	O
1	O
;	O
xi	O
(	O
5.8	O
)	O
then	O
the	O
code	B
may	O
be	O
uniquely	B
decodeable	I
.	O
this	O
inequality	B
is	O
the	O
kraft	O
in-	O
equality	O
.	O
kraft	O
inequality	B
.	O
for	O
any	O
uniquely	B
decodeable	I
code	O
c	O
(	O
x	O
)	O
over	O
the	O
binary	O
 	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
5.2	O
:	O
what	O
limit	O
is	O
imposed	O
by	O
unique	O
decodeability	O
?	O
95	O
alphabet	O
f0	O
;	O
1g	O
,	O
the	O
codeword	B
lengths	O
must	O
satisfy	O
:	O
i	O
xi=1	O
2	O
(	O
cid:0	O
)	O
li	O
(	O
cid:20	O
)	O
1	O
;	O
(	O
5.9	O
)	O
where	O
i	O
=	O
jaxj	O
.	O
completeness	O
.	O
if	O
a	O
uniquely	B
decodeable	I
code	O
satis	O
(	O
cid:12	O
)	O
es	O
the	O
kraft	O
inequality	B
with	O
equality	O
then	O
it	O
is	O
called	O
a	O
complete	O
code	B
.	O
we	O
want	O
codes	O
that	O
are	O
uniquely	B
decodeable	I
;	O
pre	O
(	O
cid:12	O
)	O
x	O
codes	O
are	O
uniquely	O
de-	O
codeable	O
,	O
and	O
are	O
easy	O
to	O
decode	O
.	O
so	O
life	B
would	O
be	O
simpler	O
for	O
us	O
if	O
we	O
could	O
restrict	O
attention	O
to	O
pre	O
(	O
cid:12	O
)	O
x	O
codes	O
.	O
fortunately	O
,	O
for	O
any	O
source	O
there	O
is	O
an	O
op-	O
timal	O
symbol	B
code	I
that	O
is	O
also	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
.	O
kraft	O
inequality	B
and	O
pre	O
(	O
cid:12	O
)	O
x	O
codes	O
.	O
given	O
a	O
set	B
of	O
codeword	B
lengths	O
that	O
satisfy	O
the	O
kraft	O
inequality	B
,	O
there	O
exists	O
a	O
uniquely	B
decodeable	I
pre	O
(	O
cid:12	O
)	O
x	O
code	B
with	O
these	O
codeword	B
lengths	O
.	O
the	O
kraft	O
inequality	B
might	O
be	O
more	O
accurately	O
referred	O
to	O
as	O
the	O
kraft	O
{	O
mcmillan	O
inequality	B
:	O
kraft	O
proved	O
that	O
if	O
the	O
inequality	B
is	O
satis	O
(	O
cid:12	O
)	O
ed	O
,	O
then	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
exists	O
with	O
the	O
given	O
lengths	O
.	O
mcmillan	O
(	O
1956	O
)	O
proved	O
the	O
con-	O
verse	O
,	O
that	O
unique	O
decodeability	O
implies	O
that	O
the	O
inequality	B
holds	O
.	O
proof	O
of	O
the	O
kraft	O
inequality	B
.	O
de	O
(	O
cid:12	O
)	O
ne	O
s	O
=pi	O
2	O
(	O
cid:0	O
)	O
li	O
.	O
consider	O
the	O
quantity	O
2	O
(	O
cid:0	O
)	O
(	O
li1	O
+	O
li2	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
lin	O
)	O
:	O
(	O
5.10	O
)	O
sn	O
=	O
''	O
xi	O
2	O
(	O
cid:0	O
)	O
li	O
#	O
n	O
=	O
i	O
i	O
xi1=1	O
xi2=1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
i	O
xin	O
=1	O
the	O
quantity	O
in	O
the	O
exponent	O
,	O
(	O
li1	O
+	O
li2	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
lin	O
)	O
,	O
is	O
the	O
length	B
of	O
the	O
encoding	O
of	O
the	O
string	O
x	O
=	O
ai1ai2	O
:	O
:	O
:	O
ain	O
.	O
for	O
every	O
string	O
x	O
of	O
length	O
n	O
,	O
there	O
is	O
one	O
term	O
in	O
the	O
above	O
sum	O
.	O
introduce	O
an	O
array	O
al	O
that	O
counts	O
how	O
many	O
strings	O
x	O
have	O
encoded	O
length	B
l.	O
then	O
,	O
de	O
(	O
cid:12	O
)	O
ning	O
lmin	O
=	O
mini	O
li	O
and	O
lmax	O
=	O
maxi	O
li	O
:	O
sn	O
=	O
n	O
lmax	O
xl=n	O
lmin	O
2	O
(	O
cid:0	O
)	O
lal	O
:	O
(	O
5.11	O
)	O
now	O
assume	O
c	O
is	O
uniquely	B
decodeable	I
,	O
so	O
that	O
for	O
all	O
x	O
6=	O
y	O
,	O
c+	O
(	O
x	O
)	O
6=	O
c+	O
(	O
y	O
)	O
.	O
concentrate	O
on	O
the	O
x	O
that	O
have	O
encoded	O
length	B
l.	O
there	O
are	O
a	O
total	O
of	O
2l	O
distinct	O
bit	B
strings	O
of	O
length	O
l	O
,	O
so	O
it	O
must	O
be	O
the	O
case	O
that	O
al	O
(	O
cid:20	O
)	O
2l	O
.	O
so	O
sn	O
=	O
n	O
lmax	O
xl=n	O
lmin	O
2	O
(	O
cid:0	O
)	O
lal	O
(	O
cid:20	O
)	O
n	O
lmax	O
xl=n	O
lmin	O
1	O
(	O
cid:20	O
)	O
n	O
lmax	O
:	O
(	O
5.12	O
)	O
thus	O
sn	O
(	O
cid:20	O
)	O
lmaxn	O
for	O
all	O
n	O
.	O
now	O
if	O
s	O
were	O
greater	O
than	O
1	O
,	O
then	O
as	O
n	O
increases	O
,	O
sn	O
would	O
be	O
an	O
exponentially	O
growing	O
function	B
,	O
and	O
for	O
large	O
enough	O
n	O
,	O
an	O
exponential	B
always	O
exceeds	O
a	O
polynomial	O
such	O
as	O
lmaxn	O
.	O
but	O
our	O
result	O
(	O
sn	O
(	O
cid:20	O
)	O
lmaxn	O
)	O
is	O
true	O
for	O
any	O
n	O
.	O
therefore	O
s	O
(	O
cid:20	O
)	O
1	O
.	O
2	O
.	O
exercise	O
5.14	O
.	O
[	O
3	O
,	O
p.104	O
]	O
prove	O
the	O
result	O
stated	O
above	O
,	O
that	O
for	O
any	O
set	B
of	O
code-	O
word	O
lengths	O
flig	O
satisfying	O
the	O
kraft	O
inequality	B
,	O
there	O
is	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
having	O
those	O
lengths	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
96	O
5	O
|	O
symbol	O
codes	O
0	O
1	O
00	O
01	O
10	O
11	O
000	O
001	O
010	O
011	O
100	O
101	O
110	O
111	O
0000	O
0001	O
0010	O
0011	O
0100	O
0101	O
0110	O
0111	O
1000	O
1001	O
1010	O
1011	O
1100	O
1101	O
1110	O
1111	O
t	O
e	O
g	O
d	O
u	O
b	O
e	O
d	O
o	O
c	O
l	O
o	O
b	O
m	O
y	O
s	O
l	O
a	O
t	O
o	O
t	O
e	O
h	O
t	O
figure	O
5.1.	O
the	O
symbol	O
coding	O
budget	B
.	O
the	O
‘	O
cost	O
’	O
2	O
(	O
cid:0	O
)	O
l	O
of	O
each	O
codeword	B
(	O
with	O
length	O
l	O
)	O
is	O
indicated	O
by	O
the	O
size	O
of	O
the	O
box	O
it	O
is	O
written	O
in	O
.	O
the	O
total	O
budget	B
available	O
when	O
making	O
a	O
uniquely	B
decodeable	I
code	O
is	O
1.	O
you	O
can	O
think	O
of	O
this	O
diagram	O
as	O
showing	O
a	O
codeword	B
supermarket	O
,	O
with	O
the	O
codewords	O
arranged	O
in	O
aisles	O
by	O
their	O
length	B
,	O
and	O
the	O
cost	O
of	O
each	O
codeword	B
indicated	O
by	O
the	O
size	O
of	O
its	O
box	B
on	O
the	O
shelf	O
.	O
if	O
the	O
cost	O
of	O
the	O
codewords	O
that	O
you	O
take	O
exceeds	O
the	O
budget	B
then	O
your	O
code	B
will	O
not	O
be	O
uniquely	B
decodeable	I
.	O
c0	O
00	O
01	O
10	O
11	O
000	O
001	O
010	O
011	O
100	O
101	O
110	O
111	O
0	O
1	O
0000	O
0001	O
0010	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
0011	O
	O
	O
	O
	O
	O
	O
	O
	O
0100	O
0101	O
0110	O
          	O
	O
          	O
	O
          	O
	O
          	O
	O
0111	O
1000	O
1001	O
1010	O
1011	O
1100	O
1101	O
1110	O
1111	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
0	O
1	O
c3	O
00	O
01	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
10	O
000	O
001	O
010	O
011	O
100	O
101	O
11	O
110	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
111	O
0000	O
0001	O
0010	O
0011	O
0100	O
0101	O
0110	O
0111	O
1000	O
1001	O
1010	O
1011	O
1100	O
1101	O
1110	O
1111	O
0	O
1	O
c4	O
c6	O
00	O
01	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
11	O
10	O
000	O
001	O
010	O
011	O
100	O
101	O
110	O
111	O
0000	O
0001	O
0010	O
0011	O
0100	O
0101	O
0110	O
0111	O
1000	O
1001	O
1010	O
1011	O
1100	O
1101	O
1110	O
1111	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
0	O
00	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
01	O
1	O
10	O
11	O
000	O
001	O
010	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
011	O
100	O
101	O
110	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
111	O
0000	O
0001	O
0010	O
0011	O
0100	O
0101	O
0110	O
0111	O
1000	O
1001	O
1010	O
1011	O
1100	O
1101	O
1110	O
1111	O
figure	O
5.2.	O
selections	O
of	O
codewords	O
made	O
by	O
codes	O
c0	O
;	O
c3	O
;	O
c4	O
and	O
c6	O
from	O
section	O
5.1.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
5.3	O
:	O
what	O
’	O
s	O
the	O
most	O
compression	O
that	O
we	O
can	O
hope	O
for	O
?	O
97	O
a	O
pictorial	O
view	O
of	O
the	O
kraft	O
inequality	B
may	O
help	O
you	O
solve	O
this	O
exercise	O
.	O
imagine	O
that	O
we	O
are	O
choosing	O
the	O
codewords	O
to	O
make	O
a	O
symbol	B
code	I
.	O
we	O
can	O
draw	O
the	O
set	B
of	O
all	O
candidate	O
codewords	O
in	O
a	O
supermarket	B
that	O
displays	O
the	O
‘	O
cost	O
’	O
of	O
the	O
codeword	O
by	O
the	O
area	O
of	O
a	O
box	B
(	O
(	O
cid:12	O
)	O
gure	O
5.1	O
)	O
.	O
the	O
total	O
budget	B
available	O
{	O
the	O
‘	O
1	O
’	O
on	O
the	O
right-hand	O
side	O
of	O
the	O
kraft	O
inequality	B
{	O
is	O
shown	O
at	O
one	O
side	O
.	O
some	O
of	O
the	O
codes	O
discussed	O
in	O
section	O
5.1	O
are	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gure	O
5.2.	O
notice	O
that	O
the	O
codes	O
that	O
are	O
pre	O
(	O
cid:12	O
)	O
x	O
codes	O
,	O
c0	O
,	O
c3	O
,	O
and	O
c4	O
,	O
have	O
the	O
property	O
that	O
to	O
the	O
right	O
of	O
any	O
selected	O
codeword	B
,	O
there	O
are	O
no	O
other	O
selected	O
codewords	O
{	O
because	O
pre	O
(	O
cid:12	O
)	O
x	O
codes	O
correspond	O
to	O
trees	O
.	O
notice	O
that	O
a	O
complete	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
corresponds	O
to	O
a	O
complete	O
tree	B
having	O
no	O
unused	O
branches	O
.	O
we	O
are	O
now	O
ready	O
to	O
put	O
back	O
the	O
symbols	O
’	O
probabilities	O
fpig	O
.	O
given	O
a	O
set	B
of	O
symbol	O
probabilities	O
(	O
the	O
english	O
language	O
probabilities	O
of	O
(	O
cid:12	O
)	O
gure	O
2.1	O
,	O
for	O
example	O
)	O
,	O
how	O
do	O
we	O
make	O
the	O
best	O
symbol	B
code	I
{	O
one	O
with	O
the	O
smallest	O
possible	O
expected	O
length	B
l	O
(	O
c	O
;	O
x	O
)	O
?	O
and	O
what	O
is	O
that	O
smallest	O
possible	O
expected	O
length	B
?	O
it	O
’	O
s	O
not	O
obvious	O
how	O
to	O
assign	O
the	O
codeword	B
lengths	O
.	O
if	O
we	O
give	O
short	O
codewords	O
to	O
the	O
more	O
probable	O
symbols	O
then	O
the	O
expected	O
length	B
might	O
be	O
reduced	O
;	O
on	O
the	O
other	O
hand	O
,	O
shortening	B
some	O
codewords	O
necessarily	O
causes	O
others	O
to	O
lengthen	O
,	O
by	O
the	O
kraft	O
inequality	B
.	O
5.3	O
what	O
’	O
s	O
the	O
most	O
compression	O
that	O
we	O
can	O
hope	O
for	O
?	O
we	O
wish	O
to	O
minimize	O
the	O
expected	O
length	B
of	O
a	O
code	B
,	O
l	O
(	O
c	O
;	O
x	O
)	O
=	O
xi	O
(	O
cid:21	O
)	O
xi	O
(	O
cid:21	O
)	O
h	O
(	O
x	O
)	O
:	O
pi	O
log	O
1=qi	O
(	O
cid:0	O
)	O
log	O
z	O
pili	O
=xi	O
pi	O
log	O
1=pi	O
(	O
cid:0	O
)	O
log	O
z	O
(	O
5.15	O
)	O
(	O
5.16	O
)	O
l	O
(	O
c	O
;	O
x	O
)	O
=	O
xi	O
pili	O
:	O
(	O
5.13	O
)	O
as	O
you	O
might	O
have	O
guessed	O
,	O
the	O
entropy	B
appears	O
as	O
the	O
lower	O
bound	B
on	O
the	O
expected	O
length	B
of	O
a	O
code	B
.	O
lower	O
bound	B
on	O
expected	O
length	B
.	O
the	O
expected	O
length	B
l	O
(	O
c	O
;	O
x	O
)	O
of	O
a	O
uniquely	B
decodeable	I
code	O
is	O
bounded	O
below	O
by	O
h	O
(	O
x	O
)	O
.	O
proof	O
.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
implicit	B
probabilities	I
qi	O
(	O
cid:17	O
)	O
2	O
(	O
cid:0	O
)	O
li=z	O
,	O
where	O
z	O
=pi0	O
2	O
(	O
cid:0	O
)	O
li0	O
,	O
so	O
that	O
li	O
=	O
log	O
1=qi	O
(	O
cid:0	O
)	O
log	O
z.	O
we	O
then	O
use	O
gibbs	O
’	O
inequality	B
,	O
pi	O
pi	O
log	O
1=qi	O
(	O
cid:21	O
)	O
pi	O
pi	O
log	O
1=pi	O
,	O
with	O
equality	O
if	O
qi	O
=	O
pi	O
,	O
and	O
the	O
kraft	O
inequality	B
z	O
(	O
cid:20	O
)	O
1	O
:	O
(	O
5.14	O
)	O
the	O
equality	O
l	O
(	O
c	O
;	O
x	O
)	O
=	O
h	O
(	O
x	O
)	O
is	O
achieved	O
only	O
if	O
the	O
kraft	O
equality	O
z	O
=	O
1	O
is	O
satis	O
(	O
cid:12	O
)	O
ed	O
,	O
and	O
if	O
the	O
codelengths	O
satisfy	O
li	O
=	O
log	O
(	O
1=pi	O
)	O
.	O
2	O
this	O
is	O
an	O
important	O
result	O
so	O
let	O
’	O
s	O
say	O
it	O
again	O
:	O
optimal	B
source	O
codelengths	O
.	O
the	O
expected	O
length	B
is	O
minimized	O
and	O
is	O
equal	O
to	O
h	O
(	O
x	O
)	O
only	O
if	O
the	O
codelengths	O
are	O
equal	O
to	O
the	O
shannon	O
in-	O
formation	O
contents	O
:	O
li	O
=	O
log2	O
(	O
1=pi	O
)	O
:	O
(	O
5.17	O
)	O
implicit	B
probabilities	I
de	O
(	O
cid:12	O
)	O
ned	O
by	O
codelengths	O
.	O
conversely	O
,	O
any	O
choice	O
of	O
codelengths	O
flig	O
implicitly	O
de	O
(	O
cid:12	O
)	O
nes	O
a	O
probability	B
distribution	O
fqig	O
,	O
qi	O
(	O
cid:17	O
)	O
2	O
(	O
cid:0	O
)	O
li=z	O
;	O
(	O
5.18	O
)	O
for	O
which	O
those	O
codelengths	O
would	O
be	O
the	O
optimal	B
codelengths	O
.	O
if	O
the	O
code	B
is	O
complete	O
then	O
z	O
=	O
1	O
and	O
the	O
implicit	B
probabilities	I
are	O
given	O
by	O
qi	O
=	O
2	O
(	O
cid:0	O
)	O
li	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
98	O
5	O
|	O
symbol	O
codes	O
5.4	O
how	O
much	O
can	O
we	O
compress	B
?	O
so	O
,	O
we	O
can	O
’	O
t	O
compress	B
below	O
the	O
entropy	B
.	O
how	O
close	O
can	O
we	O
expect	O
to	O
get	O
to	O
the	O
entropy	B
?	O
theorem	B
5.1	O
source	B
coding	I
theorem	I
for	O
symbol	O
codes	O
.	O
for	O
an	O
ensemble	B
x	O
there	O
exists	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
c	O
with	O
expected	O
length	B
satisfying	O
h	O
(	O
x	O
)	O
(	O
cid:20	O
)	O
l	O
(	O
c	O
;	O
x	O
)	O
<	O
h	O
(	O
x	O
)	O
+	O
1	O
:	O
(	O
5.19	O
)	O
proof	O
.	O
we	O
set	B
the	O
codelengths	O
to	O
integers	O
slightly	O
larger	O
than	O
the	O
optimum	O
lengths	O
:	O
li	O
=	O
dlog2	O
(	O
1=pi	O
)	O
e	O
(	O
5.20	O
)	O
where	O
dl	O
(	O
cid:3	O
)	O
e	O
denotes	O
the	O
smallest	O
integer	O
greater	O
than	O
or	O
equal	O
to	O
l	O
(	O
cid:3	O
)	O
.	O
[	O
we	O
are	O
not	O
asserting	O
that	O
the	O
optimal	B
code	O
necessarily	O
uses	O
these	O
lengths	O
,	O
we	O
are	O
simply	O
choosing	O
these	O
lengths	O
because	O
we	O
can	O
use	O
them	O
to	O
prove	O
the	O
theorem	B
.	O
]	O
we	O
check	O
that	O
there	O
is	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
with	O
these	O
lengths	O
by	O
con	O
(	O
cid:12	O
)	O
rming	O
that	O
the	O
kraft	O
inequality	B
is	O
satis	O
(	O
cid:12	O
)	O
ed	O
.	O
xi	O
2	O
(	O
cid:0	O
)	O
li	O
=xi	O
2	O
(	O
cid:0	O
)	O
dlog2	O
(	O
1=pi	O
)	O
e	O
(	O
cid:20	O
)	O
xi	O
2	O
(	O
cid:0	O
)	O
log2	O
(	O
1=pi	O
)	O
=xi	O
pi	O
=	O
1	O
:	O
(	O
5.21	O
)	O
then	O
we	O
con	O
(	O
cid:12	O
)	O
rm	O
l	O
(	O
c	O
;	O
x	O
)	O
=xi	O
pidlog	O
(	O
1=pi	O
)	O
e	O
<	O
xi	O
pi	O
(	O
log	O
(	O
1=pi	O
)	O
+	O
1	O
)	O
=	O
h	O
(	O
x	O
)	O
+	O
1	O
:	O
(	O
5.22	O
)	O
2	O
the	O
cost	O
of	O
using	O
the	O
wrong	O
codelengths	O
if	O
we	O
use	O
a	O
code	B
whose	O
lengths	O
are	O
not	O
equal	O
to	O
the	O
optimal	B
codelengths	O
,	O
the	O
average	B
message	O
length	B
will	O
be	O
larger	O
than	O
the	O
entropy	B
.	O
if	O
the	O
true	O
probabilities	O
are	O
fpig	O
and	O
we	O
use	O
a	O
complete	O
code	B
with	O
lengths	O
li	O
,	O
we	O
can	O
view	O
those	O
lengths	O
as	O
de	O
(	O
cid:12	O
)	O
ning	O
implicit	B
probabilities	I
qi	O
=	O
2	O
(	O
cid:0	O
)	O
li	O
.	O
con-	O
tinuing	O
from	O
equation	O
(	O
5.14	O
)	O
,	O
the	O
average	B
length	O
is	O
l	O
(	O
c	O
;	O
x	O
)	O
=	O
h	O
(	O
x	O
)	O
+xi	O
pi	O
log	O
pi=qi	O
;	O
(	O
5.23	O
)	O
i.e.	O
,	O
it	O
exceeds	O
the	O
entropy	B
by	O
the	O
relative	B
entropy	I
dkl	O
(	O
pjjq	O
)	O
(	O
as	O
de	O
(	O
cid:12	O
)	O
ned	O
on	O
p.34	O
)	O
.	O
5.5	O
optimal	B
source	O
coding	O
with	O
symbol	O
codes	O
:	O
hu	O
(	O
cid:11	O
)	O
man	O
coding	O
given	O
a	O
set	B
of	O
probabilities	O
p	O
,	O
how	O
can	O
we	O
design	O
an	O
optimal	B
pre	O
(	O
cid:12	O
)	O
x	O
code	B
?	O
for	O
example	O
,	O
what	O
is	O
the	O
best	O
symbol	B
code	I
for	O
the	O
english	O
language	O
ensemble	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
5.3	O
?	O
when	O
we	O
say	O
‘	O
optimal	B
’	O
,	O
let	O
’	O
s	O
assume	O
our	O
aim	O
is	O
to	O
minimize	O
the	O
expected	O
length	B
l	O
(	O
c	O
;	O
x	O
)	O
.	O
how	O
not	O
to	O
do	O
it	O
one	O
might	O
try	O
to	O
roughly	O
split	O
the	O
set	B
ax	O
in	O
two	O
,	O
and	O
continue	O
bisecting	O
the	O
subsets	O
so	O
as	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
binary	O
tree	O
from	O
the	O
root	O
.	O
this	O
construction	B
has	O
the	O
right	O
spirit	O
,	O
as	O
in	O
the	O
weighing	B
problem	I
,	O
but	O
it	O
is	O
not	O
necessarily	O
optimal	B
;	O
it	O
achieves	O
l	O
(	O
c	O
;	O
x	O
)	O
(	O
cid:20	O
)	O
h	O
(	O
x	O
)	O
+	O
2.	O
x	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
(	O
cid:0	O
)	O
p	O
(	O
x	O
)	O
0.0575	O
0.0128	O
0.0263	O
0.0285	O
0.0913	O
0.0173	O
0.0133	O
0.0313	O
0.0599	O
0.0006	O
0.0084	O
0.0335	O
0.0235	O
0.0596	O
0.0689	O
0.0192	O
0.0008	O
0.0508	O
0.0567	O
0.0706	O
0.0334	O
0.0069	O
0.0119	O
0.0073	O
0.0164	O
0.0007	O
0.1928	O
figure	O
5.3.	O
an	O
ensemble	B
in	O
need	O
of	O
a	O
symbol	B
code	I
.	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
5.5	O
:	O
optimal	B
source	O
coding	O
with	O
symbol	O
codes	O
:	O
hu	O
(	O
cid:11	O
)	O
man	O
coding	O
99	O
algorithm	B
5.4.	O
hu	O
(	O
cid:11	O
)	O
man	O
coding	O
algorithm	O
.	O
ai	O
pi	O
h	O
(	O
pi	O
)	O
a	O
b	O
c	O
d	O
e	O
0.25	O
0.25	O
0.2	O
0.15	O
0.15	O
2.0	O
2.0	O
2.3	O
2.7	O
2.7	O
li	O
2	O
2	O
2	O
3	O
3	O
c	O
(	O
ai	O
)	O
00	O
10	O
11	O
010	O
011	O
table	O
5.5.	O
code	B
created	O
by	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
algorithm	B
.	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
coding	O
algorithm	O
we	O
now	O
present	O
a	O
beautifully	O
simple	O
algorithm	O
for	O
(	O
cid:12	O
)	O
nding	O
an	O
optimal	B
pre	O
(	O
cid:12	O
)	O
x	O
code	B
.	O
the	O
trick	O
is	O
to	O
construct	O
the	O
code	B
backwards	O
starting	O
from	O
the	O
tails	O
of	O
the	O
codewords	O
;	O
we	O
build	O
the	O
binary	O
tree	O
from	O
its	O
leaves	O
.	O
1.	O
take	O
the	O
two	O
least	O
probable	O
symbols	O
in	O
the	O
alphabet	O
.	O
these	O
two	O
symbols	O
will	O
be	O
given	O
the	O
longest	O
codewords	O
,	O
which	O
will	O
have	O
equal	O
length	B
,	O
and	O
di	O
(	O
cid:11	O
)	O
er	O
only	O
in	O
the	O
last	O
digit	O
.	O
2.	O
combine	O
these	O
two	O
symbols	O
into	O
a	O
single	O
symbol	O
,	O
and	O
repeat	O
.	O
since	O
each	O
step	O
reduces	O
the	O
size	O
of	O
the	O
alphabet	O
by	O
one	O
,	O
this	O
algorithm	B
will	O
have	O
assigned	O
strings	O
to	O
all	O
the	O
symbols	O
after	O
jaxj	O
(	O
cid:0	O
)	O
1	O
steps	O
.	O
example	O
5.15.	O
let	O
ax	O
=f	O
a	O
,	O
g	O
and	O
px	O
=f	O
0.25	O
,	O
0.25	O
,	O
0.2	O
,	O
0.15	O
,	O
0.15	O
g.	O
c	O
,	O
d	O
,	O
b	O
,	O
e	O
x	O
a	O
b	O
c	O
d	O
e	O
step	O
1	O
step	O
2	O
step	O
3	O
step	O
4	O
0.55	O
0.45	O
0	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
1	O
1.0	O
0.25	O
0.25	O
0.2	O
0.3	O
0	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
1	O
0.25	O
0.45	O
0.3	O
0.25	O
0.25	O
0.2	O
0.15	O
0.15	O
0	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
1	O
0	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
1	O
the	O
codewords	O
are	O
then	O
obtained	O
by	O
concatenating	O
the	O
binary	O
digits	O
in	O
reverse	O
order	O
:	O
c	O
=	O
f00	O
;	O
10	O
;	O
11	O
;	O
010	O
;	O
011g	O
.	O
the	O
codelengths	O
selected	O
by	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
algorithm	B
(	O
column	O
4	O
of	O
table	O
5.5	O
)	O
are	O
in	O
some	O
cases	O
longer	O
and	O
in	O
some	O
cases	O
shorter	O
than	O
the	O
ideal	O
codelengths	O
,	O
the	O
shannon	O
1/pi	O
(	O
column	O
3	O
)	O
.	O
the	O
expected	O
length	B
of	O
the	O
information	B
contents	O
log2	O
code	B
is	O
l	O
=	O
2:30	O
bits	O
,	O
whereas	O
the	O
entropy	B
is	O
h	O
=	O
2:2855	O
bits	O
.	O
2	O
if	O
at	O
any	O
point	O
there	O
is	O
more	O
than	O
one	O
way	O
of	O
selecting	O
the	O
two	O
least	O
probable	O
symbols	O
then	O
the	O
choice	O
may	O
be	O
made	O
in	O
any	O
manner	O
{	O
the	O
expected	O
length	B
of	O
the	O
code	B
will	O
not	O
depend	O
on	O
the	O
choice	O
.	O
exercise	O
5.16	O
.	O
[	O
3	O
,	O
p.105	O
]	O
prove	O
that	O
there	O
is	O
no	O
better	O
symbol	B
code	I
for	O
a	O
source	O
than	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
.	O
example	O
5.17.	O
we	O
can	O
make	O
a	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
for	O
the	O
probability	B
distribution	O
over	O
the	O
alphabet	O
introduced	O
in	O
(	O
cid:12	O
)	O
gure	O
2.1.	O
the	O
result	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
g-	O
ure	O
5.6.	O
this	O
code	B
has	O
an	O
expected	O
length	B
of	O
4.15	O
bits	O
;	O
the	O
entropy	B
of	O
the	O
ensemble	B
is	O
4.11	O
bits	O
.	O
observe	O
the	O
disparities	O
between	O
the	O
assigned	O
codelengths	O
and	O
the	O
ideal	O
codelengths	O
log2	O
1/pi	O
.	O
constructing	O
a	O
binary	O
tree	O
top-down	O
is	O
suboptimal	O
in	O
previous	O
chapters	O
we	O
studied	O
weighing	O
problems	O
in	O
which	O
we	O
built	O
ternary	O
or	O
binary	O
trees	O
.	O
we	O
noticed	O
that	O
balanced	O
trees	O
{	O
ones	O
in	O
which	O
,	O
at	O
every	O
step	O
,	O
the	O
two	O
possible	O
outcomes	O
were	O
as	O
close	O
as	O
possible	O
to	O
equiprobable	O
{	O
appeared	O
to	O
describe	O
the	O
most	O
e	O
(	O
cid:14	O
)	O
cient	O
experiments	O
.	O
this	O
gave	O
an	O
intuitive	O
motivation	O
for	O
entropy	O
as	O
a	O
measure	O
of	O
information	O
content	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
5	O
|	O
symbol	O
codes	O
figure	O
5.6.	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
for	O
the	O
english	O
language	O
ensemble	O
(	O
monogram	O
statistics	O
)	O
.	O
ai	O
pi	O
greedy	O
hu	O
(	O
cid:11	O
)	O
man	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
.01	O
.24	O
.05	O
.20	O
.47	O
.01	O
.02	O
000	O
001	O
010	O
011	O
10	O
110	O
111	O
000000	O
01	O
0001	O
001	O
1	O
000001	O
00001	O
table	O
5.7.	O
a	O
greedily-constructed	O
code	B
compared	O
with	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
.	O
100	O
ai	O
pi	O
log2	O
1	O
pi	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
{	O
0.0575	O
0.0128	O
0.0263	O
0.0285	O
0.0913	O
0.0173	O
0.0133	O
0.0313	O
0.0599	O
0.0006	O
0.0084	O
0.0335	O
0.0235	O
0.0596	O
0.0689	O
0.0192	O
0.0008	O
0.0508	O
0.0567	O
0.0706	O
0.0334	O
0.0069	O
0.0119	O
0.0073	O
0.0164	O
0.0007	O
0.1928	O
4.1	O
6.3	O
5.2	O
5.1	O
3.5	O
5.9	O
6.2	O
5.0	O
4.1	O
10.7	O
6.9	O
4.9	O
5.4	O
4.1	O
3.9	O
5.7	O
10.3	O
4.3	O
4.1	O
3.8	O
4.9	O
7.2	O
6.4	O
7.1	O
5.9	O
10.4	O
2.4	O
li	O
4	O
6	O
5	O
5	O
4	O
6	O
6	O
5	O
4	O
10	O
7	O
5	O
6	O
4	O
4	O
6	O
9	O
5	O
4	O
4	O
5	O
8	O
7	O
7	O
6	O
10	O
2	O
c	O
(	O
ai	O
)	O
0000	O
001000	O
00101	O
10000	O
1100	O
111000	O
001001	O
10001	O
1001	O
1101000000	O
1010000	O
11101	O
110101	O
0001	O
1011	O
111001	O
110100001	O
11011	O
0011	O
1111	O
10101	O
11010001	O
1101001	O
1010001	O
101001	O
1101000001	O
01	O
−	O
a	O
n	O
s	O
i	O
o	O
e	O
t	O
b	O
g	O
c	O
d	O
h	O
k	O
x	O
y	O
u	O
j	O
z	O
q	O
v	O
w	O
m	O
f	O
p	O
r	O
l	O
it	O
is	O
not	O
the	O
case	O
,	O
however	O
,	O
that	O
optimal	B
codes	O
can	O
always	O
be	O
constructed	O
by	O
a	O
greedy	O
top-down	O
method	B
in	O
which	O
the	O
alphabet	O
is	O
successively	O
divided	O
into	O
subsets	O
that	O
are	O
as	O
near	O
as	O
possible	O
to	O
equiprobable	O
.	O
example	O
5.18.	O
find	O
the	O
optimal	B
binary	O
symbol	B
code	I
for	O
the	O
ensemble	B
:	O
g	O
g	O
ax	O
=	O
f	O
a	O
;	O
px	O
=	O
f	O
0:01	O
;	O
0:24	O
;	O
0:05	O
;	O
0:20	O
;	O
0:47	O
;	O
0:01	O
;	O
0:02	O
g	O
d	O
;	O
b	O
;	O
c	O
;	O
e	O
;	O
f	O
;	O
:	O
(	O
5.24	O
)	O
notice	O
that	O
a	O
greedy	O
top-down	O
method	B
can	O
split	O
this	O
set	B
into	O
two	O
sub-	O
sets	O
fa	O
;	O
b	O
;	O
c	O
;	O
dg	O
and	O
fe	O
;	O
f	O
;	O
gg	O
which	O
both	O
have	O
probability	B
1=2	O
,	O
and	O
that	O
fa	O
;	O
b	O
;	O
c	O
;	O
dg	O
can	O
be	O
divided	O
into	O
subsets	O
fa	O
;	O
bg	O
and	O
fc	O
;	O
dg	O
,	O
which	O
have	O
prob-	O
ability	O
1=4	O
;	O
so	O
a	O
greedy	O
top-down	O
method	B
gives	O
the	O
code	B
shown	O
in	O
the	O
third	O
column	O
of	O
table	O
5.7	O
,	O
which	O
has	O
expected	O
length	B
2.53.	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
coding	O
algorithm	O
yields	O
the	O
code	B
shown	O
in	O
the	O
fourth	O
column	O
,	O
which	O
has	O
expected	O
length	B
1.97	O
.	O
2	O
5.6	O
disadvantages	B
of	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
the	O
hu	O
(	O
cid:11	O
)	O
man	O
algorithm	B
produces	O
an	O
optimal	B
symbol	O
code	B
for	O
an	O
ensemble	B
,	O
but	O
this	O
is	O
not	O
the	O
end	O
of	O
the	O
story	O
.	O
both	O
the	O
word	O
‘	O
ensemble	B
’	O
and	O
the	O
phrase	O
‘	O
symbol	B
code	I
’	O
need	O
careful	O
attention	O
.	O
changing	O
ensemble	B
if	O
we	O
wish	O
to	O
communicate	O
a	O
sequence	B
of	O
outcomes	O
from	O
one	O
unchanging	O
en-	O
semble	O
,	O
then	O
a	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
may	O
be	O
convenient	O
.	O
but	O
often	O
the	O
appropriate	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
5.6	O
:	O
disadvantages	B
of	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
101	O
ensemble	B
changes	O
.	O
if	O
for	O
example	O
we	O
are	O
compressing	O
text	O
,	O
then	O
the	O
symbol	O
frequencies	O
will	O
vary	O
with	O
context	O
:	O
in	O
english	O
the	O
letter	O
u	O
is	O
much	O
more	O
prob-	O
able	O
after	O
a	O
q	O
than	O
after	O
an	O
e	O
(	O
(	O
cid:12	O
)	O
gure	O
2.3	O
)	O
.	O
and	O
furthermore	O
,	O
our	O
knowledge	O
of	O
these	O
context-dependent	O
symbol	O
frequencies	O
will	O
also	O
change	O
as	O
we	O
learn	O
the	O
statistical	B
properties	O
of	O
the	O
text	O
source	O
.	O
hu	O
(	O
cid:11	O
)	O
man	O
codes	O
do	O
not	O
handle	O
changing	O
ensemble	B
probabilities	O
with	O
any	O
elegance	O
.	O
one	O
brute-force	O
approach	O
would	O
be	O
to	O
recompute	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
every	O
time	O
the	O
probability	B
over	O
symbols	O
changes	O
.	O
another	O
attitude	O
is	O
to	O
deny	O
the	O
option	O
of	O
adaptation	O
,	O
and	O
instead	O
run	O
through	O
the	O
entire	O
(	O
cid:12	O
)	O
le	O
in	O
advance	O
and	O
compute	O
a	O
good	B
probability	O
distribution	B
,	O
which	O
will	O
then	O
remain	O
(	O
cid:12	O
)	O
xed	O
throughout	O
transmission	O
.	O
the	O
code	B
itself	O
must	O
also	O
be	O
communicated	O
in	O
this	O
scenario	O
.	O
such	O
a	O
technique	O
is	O
not	O
only	O
cumbersome	O
and	O
restrictive	O
,	O
it	O
is	O
also	O
suboptimal	O
,	O
since	O
the	O
initial	O
message	O
specifying	O
the	O
code	B
and	O
the	O
document	O
itself	O
are	O
partially	O
redundant	O
.	O
this	O
technique	O
therefore	O
wastes	O
bits	O
.	O
the	O
extra	B
bit	I
an	O
equally	O
serious	O
problem	O
with	O
hu	O
(	O
cid:11	O
)	O
man	O
codes	O
is	O
the	O
innocuous-looking	O
‘	O
ex-	O
tra	O
bit	B
’	O
relative	B
to	O
the	O
ideal	O
average	B
length	O
of	O
h	O
(	O
x	O
)	O
{	O
a	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
achieves	O
a	O
length	B
that	O
satis	O
(	O
cid:12	O
)	O
es	O
h	O
(	O
x	O
)	O
(	O
cid:20	O
)	O
l	O
(	O
c	O
;	O
x	O
)	O
<	O
h	O
(	O
x	O
)	O
+1	O
;	O
as	O
proved	O
in	O
theorem	O
5.1.	O
a	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
thus	O
incurs	O
an	O
overhead	O
of	O
between	O
0	O
and	O
1	O
bits	O
per	O
symbol	O
.	O
if	O
h	O
(	O
x	O
)	O
were	O
large	O
,	O
then	O
this	O
overhead	O
would	O
be	O
an	O
unimportant	O
fractional	O
increase	O
.	O
but	O
for	O
many	O
applications	O
,	O
the	O
entropy	B
may	O
be	O
as	O
low	O
as	O
one	O
bit	B
per	O
symbol	O
,	O
or	O
even	O
smaller	O
,	O
so	O
the	O
overhead	O
l	O
(	O
c	O
;	O
x	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
)	O
may	O
domi-	O
in	O
some	O
contexts	O
,	O
long	O
nate	O
the	O
encoded	O
(	O
cid:12	O
)	O
le	O
length	B
.	O
consider	O
english	O
text	O
:	O
strings	O
of	O
characters	O
may	O
be	O
highly	O
predictable	O
.	O
for	O
example	O
,	O
in	O
the	O
context	O
‘	O
strings_of_ch	O
’	O
,	O
one	O
might	O
predict	O
the	O
next	O
nine	O
symbols	O
to	O
be	O
‘	O
aracters_	O
’	O
with	O
a	O
probability	O
of	O
0.99	O
each	O
.	O
a	O
traditional	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
would	O
be	O
obliged	O
to	O
use	O
at	O
least	O
one	O
bit	B
per	O
character	O
,	O
making	O
a	O
total	O
cost	O
of	O
nine	O
bits	O
where	O
virtually	O
no	O
information	B
is	O
being	O
conveyed	O
(	O
0.13	O
bits	O
in	O
total	O
,	O
to	O
be	O
precise	O
)	O
.	O
the	O
entropy	B
of	O
english	O
,	O
given	O
a	O
good	B
model	O
,	O
is	O
about	O
one	O
bit	B
per	O
character	O
(	O
shannon	O
,	O
1948	O
)	O
,	O
so	O
a	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
is	O
likely	O
to	O
be	O
highly	O
ine	O
(	O
cid:14	O
)	O
cient	O
.	O
a	O
traditional	O
patch-up	O
of	O
hu	O
(	O
cid:11	O
)	O
man	O
codes	O
uses	O
them	O
to	O
compress	B
blocks	O
of	O
symbols	O
,	O
for	O
example	O
the	O
‘	O
extended	B
sources	O
’	O
x	O
n	O
we	O
discussed	O
in	O
chapter	O
4.	O
the	O
overhead	O
per	O
block	B
is	O
at	O
most	O
1	O
bit	B
so	O
the	O
overhead	O
per	O
symbol	O
is	O
at	O
most	O
1=n	O
bits	O
.	O
for	O
su	O
(	O
cid:14	O
)	O
ciently	O
large	O
blocks	O
,	O
the	O
problem	O
of	O
the	O
extra	O
bit	B
may	O
be	O
removed	O
{	O
but	O
only	O
at	O
the	O
expenses	O
of	O
(	O
a	O
)	O
losing	O
the	O
elegant	O
instantaneous	B
decodeability	O
of	O
simple	O
hu	O
(	O
cid:11	O
)	O
man	O
coding	O
;	O
and	O
(	O
b	O
)	O
having	O
to	O
compute	O
the	O
prob-	O
abilities	O
of	O
all	O
relevant	O
strings	O
and	O
build	O
the	O
associated	O
hu	O
(	O
cid:11	O
)	O
man	O
tree	B
.	O
one	O
will	O
end	O
up	O
explicitly	O
computing	O
the	O
probabilities	O
and	O
codes	O
for	O
a	O
huge	O
number	O
of	O
strings	O
,	O
most	O
of	O
which	O
will	O
never	O
actually	O
occur	O
.	O
(	O
see	O
exercise	O
5.29	O
(	O
p.103	O
)	O
.	O
)	O
beyond	O
symbol	O
codes	O
hu	O
(	O
cid:11	O
)	O
man	O
codes	O
,	O
therefore	O
,	O
although	O
widely	O
trumpeted	O
as	O
‘	O
optimal	B
’	O
,	O
have	O
many	O
defects	O
for	O
practical	O
purposes	O
.	O
they	O
are	O
optimal	B
symbol	O
codes	O
,	O
but	O
for	O
practi-	O
cal	O
purposes	O
we	O
don	O
’	O
t	O
want	O
a	O
symbol	B
code	I
.	O
the	O
defects	O
of	O
hu	O
(	O
cid:11	O
)	O
man	O
codes	O
are	O
recti	O
(	O
cid:12	O
)	O
ed	O
by	O
arithmetic	O
coding	O
,	O
which	O
dispenses	O
with	O
the	O
restriction	O
that	O
each	O
symbol	O
must	O
translate	O
into	O
an	O
integer	O
number	O
of	O
bits	O
.	O
arithmetic	B
coding	I
is	O
the	O
main	O
topic	O
of	O
the	O
next	O
chapter	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
5	O
|	O
symbol	O
codes	O
102	O
5.7	O
summary	B
kraft	O
inequality	B
.	O
if	O
a	O
code	B
is	O
uniquely	B
decodeable	I
its	O
lengths	O
must	O
satisfy	O
2	O
(	O
cid:0	O
)	O
li	O
(	O
cid:20	O
)	O
1	O
:	O
xi	O
(	O
5.25	O
)	O
for	O
any	O
lengths	O
satisfying	O
the	O
kraft	O
inequality	B
,	O
there	O
exists	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
with	O
those	O
lengths	O
.	O
optimal	B
source	O
codelengths	O
for	O
an	O
ensemble	B
are	O
equal	O
to	O
the	O
shannon	O
information	O
contents	O
li	O
=	O
log2	O
1	O
pi	O
;	O
(	O
5.26	O
)	O
and	O
conversely	O
,	O
any	O
choice	O
of	O
codelengths	O
de	O
(	O
cid:12	O
)	O
nes	O
implicit	B
probabilities	I
qi	O
=	O
2	O
(	O
cid:0	O
)	O
li	O
z	O
:	O
(	O
5.27	O
)	O
the	O
relative	B
entropy	I
dkl	O
(	O
pjjq	O
)	O
measures	O
how	O
many	O
bits	O
per	O
symbol	O
are	O
wasted	O
by	O
using	O
a	O
code	B
whose	O
implicit	B
probabilities	I
are	O
q	O
,	O
when	O
the	O
ensemble	B
’	O
s	O
true	O
probability	B
distribution	O
is	O
p.	O
source	B
coding	I
theorem	I
for	O
symbol	O
codes	O
.	O
for	O
an	O
ensemble	B
x	O
,	O
there	O
ex-	O
ists	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
whose	O
expected	O
length	B
satis	O
(	O
cid:12	O
)	O
es	O
h	O
(	O
x	O
)	O
(	O
cid:20	O
)	O
l	O
(	O
c	O
;	O
x	O
)	O
<	O
h	O
(	O
x	O
)	O
+	O
1	O
:	O
(	O
5.28	O
)	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
coding	O
algorithm	O
generates	O
an	O
optimal	B
symbol	O
code	B
itera-	O
tively	O
.	O
at	O
each	O
iteration	O
,	O
the	O
two	O
least	O
probable	O
symbols	O
are	O
combined	O
.	O
5.8	O
exercises	O
.	O
exercise	O
5.19	O
.	O
[	O
2	O
]	O
is	O
the	O
code	B
f00	O
;	O
11	O
;	O
0101	O
;	O
111	O
;	O
1010	O
;	O
100100	O
;	O
0110g	O
uniquely	B
decodeable	I
?	O
.	O
exercise	O
5.20	O
.	O
[	O
2	O
]	O
is	O
the	O
ternary	O
code	B
f00	O
;	O
012	O
;	O
0110	O
;	O
0112	O
;	O
100	O
;	O
201	O
;	O
212	O
;	O
22g	O
uniquely	B
decodeable	I
?	O
exercise	O
5.21	O
.	O
[	O
3	O
,	O
p.106	O
]	O
make	O
hu	O
(	O
cid:11	O
)	O
man	O
codes	O
for	O
x	O
2	O
,	O
x	O
3	O
and	O
x	O
4	O
where	O
ax	O
=	O
f0	O
;	O
1g	O
and	O
px	O
=	O
f0:9	O
;	O
0:1g	O
.	O
compute	O
their	O
expected	O
lengths	O
and	O
com-	O
pare	O
them	O
with	O
the	O
entropies	O
h	O
(	O
x	O
2	O
)	O
,	O
h	O
(	O
x	O
3	O
)	O
and	O
h	O
(	O
x	O
4	O
)	O
.	O
repeat	O
this	O
exercise	O
for	O
x	O
2	O
and	O
x	O
4	O
where	O
px	O
=	O
f0:6	O
;	O
0:4g	O
.	O
exercise	O
5.22	O
.	O
[	O
2	O
,	O
p.106	O
]	O
find	O
a	O
probability	B
distribution	O
fp1	O
;	O
p2	O
;	O
p3	O
;	O
p4g	O
such	O
that	O
there	O
are	O
two	O
optimal	B
codes	O
that	O
assign	O
di	O
(	O
cid:11	O
)	O
erent	O
lengths	O
flig	O
to	O
the	O
four	O
symbols	O
.	O
exercise	O
5.23	O
.	O
[	O
3	O
]	O
(	O
continuation	O
of	O
exercise	O
5.22	O
.	O
)	O
assume	O
that	O
the	O
four	O
proba-	O
bilities	O
fp1	O
;	O
p2	O
;	O
p3	O
;	O
p4g	O
are	O
ordered	O
such	O
that	O
p1	O
(	O
cid:21	O
)	O
p2	O
(	O
cid:21	O
)	O
p3	O
(	O
cid:21	O
)	O
p4	O
(	O
cid:21	O
)	O
0.	O
let	O
q	O
be	O
the	O
set	B
of	O
all	O
probability	B
vectors	O
p	O
such	O
that	O
there	O
are	O
two	O
optimal	B
codes	O
with	O
di	O
(	O
cid:11	O
)	O
erent	O
lengths	O
.	O
give	O
a	O
complete	O
description	O
of	O
q.	O
find	O
three	O
probability	O
vectors	B
q	O
(	O
1	O
)	O
,	O
q	O
(	O
2	O
)	O
,	O
q	O
(	O
3	O
)	O
,	O
which	O
are	O
the	O
convex	B
hull	I
of	O
q	O
,	O
i.e.	O
,	O
such	O
that	O
any	O
p	O
2	O
q	O
can	O
be	O
written	O
as	O
p	O
=	O
(	O
cid:22	O
)	O
1q	O
(	O
1	O
)	O
+	O
(	O
cid:22	O
)	O
2q	O
(	O
2	O
)	O
+	O
(	O
cid:22	O
)	O
3q	O
(	O
3	O
)	O
;	O
(	O
5.29	O
)	O
where	O
f	O
(	O
cid:22	O
)	O
ig	O
are	O
positive	O
.	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
5.8	O
:	O
exercises	O
103	O
.	O
exercise	O
5.24	O
.	O
[	O
1	O
]	O
write	O
a	O
short	O
essay	O
discussing	O
how	O
to	O
play	O
the	O
game	B
of	O
twenty	B
questions	I
optimally	O
.	O
[	O
in	O
twenty	O
questions	O
,	O
one	O
player	O
thinks	O
of	O
an	O
object	O
,	O
and	O
the	O
other	O
player	O
has	O
to	O
guess	O
the	O
object	O
using	O
as	O
few	O
binary	O
questions	O
as	O
possible	O
,	O
preferably	O
fewer	O
than	O
twenty	O
.	O
]	O
.	O
exercise	O
5.25	O
.	O
[	O
2	O
]	O
show	O
that	O
,	O
if	O
each	O
probability	B
pi	O
is	O
equal	O
to	O
an	O
integer	O
power	O
of	O
2	O
then	O
there	O
exists	O
a	O
source	B
code	I
whose	O
expected	O
length	B
equals	O
the	O
entropy	B
.	O
.	O
exercise	O
5.26	O
.	O
[	O
2	O
,	O
p.106	O
]	O
make	O
ensembles	O
for	O
which	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
the	O
entropy	B
and	O
the	O
expected	O
length	B
of	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
is	O
as	O
big	O
as	O
possible	O
.	O
.	O
exercise	O
5.27	O
.	O
[	O
2	O
,	O
p.106	O
]	O
a	O
source	O
x	O
has	O
an	O
alphabet	O
of	O
eleven	O
characters	O
fa	O
;	O
b	O
;	O
c	O
;	O
d	O
;	O
e	O
;	O
f	O
;	O
g	O
;	O
h	O
;	O
i	O
;	O
j	O
;	O
kg	O
;	O
all	O
of	O
which	O
have	O
equal	O
probability	B
,	O
1=11	O
.	O
find	O
an	O
optimal	B
uniquely	O
decodeable	O
symbol	B
code	I
for	O
this	O
source	O
.	O
how	O
much	O
greater	O
is	O
the	O
expected	O
length	B
of	O
this	O
optimal	B
code	O
than	O
the	O
entropy	B
of	O
x	O
?	O
.	O
exercise	O
5.28	O
.	O
[	O
2	O
]	O
consider	O
the	O
optimal	B
symbol	O
code	B
for	O
an	O
ensemble	B
x	O
with	O
alphabet	O
size	O
i	O
from	O
which	O
all	O
symbols	O
have	O
identical	O
probability	O
p	O
=	O
1=i	O
.	O
i	O
is	O
not	O
a	O
power	O
of	O
2.	O
show	O
that	O
the	O
fraction	O
f	O
+	O
of	O
the	O
i	O
symbols	O
that	O
are	O
assigned	O
codelengths	O
equal	O
to	O
satis	O
(	O
cid:12	O
)	O
es	O
l+	O
(	O
cid:17	O
)	O
dlog2	O
ie	O
2l+	O
i	O
f	O
+	O
=	O
2	O
(	O
cid:0	O
)	O
and	O
that	O
the	O
expected	O
length	B
of	O
the	O
optimal	B
symbol	O
code	B
is	O
l	O
=	O
l+	O
(	O
cid:0	O
)	O
1	O
+	O
f	O
+	O
:	O
(	O
5.30	O
)	O
(	O
5.31	O
)	O
(	O
5.32	O
)	O
by	O
di	O
(	O
cid:11	O
)	O
erentiating	O
the	O
excess	O
length	B
(	O
cid:1	O
)	O
l	O
(	O
cid:17	O
)	O
l	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
)	O
with	O
respect	O
to	O
i	O
,	O
show	O
that	O
the	O
excess	O
length	B
is	O
bounded	O
by	O
(	O
cid:1	O
)	O
l	O
(	O
cid:20	O
)	O
1	O
(	O
cid:0	O
)	O
ln	O
(	O
ln	O
2	O
)	O
ln	O
2	O
(	O
cid:0	O
)	O
1	O
ln	O
2	O
=	O
0:086	O
:	O
(	O
5.33	O
)	O
exercise	O
5.29	O
.	O
[	O
2	O
]	O
consider	O
a	O
sparse	O
binary	O
source	O
with	O
px	O
=	O
f0:99	O
;	O
0:01g	O
.	O
dis-	O
cuss	O
how	O
hu	O
(	O
cid:11	O
)	O
man	O
codes	O
could	O
be	O
used	O
to	O
compress	B
this	O
source	O
e	O
(	O
cid:14	O
)	O
ciently	O
.	O
estimate	O
how	O
many	O
codewords	O
your	O
proposed	O
solutions	O
require	O
.	O
.	O
exercise	O
5.30	O
.	O
[	O
2	O
]	O
scienti	O
(	O
cid:12	O
)	O
c	O
american	O
carried	O
the	O
following	O
puzzle	B
in	O
1975.	O
the	O
poisoned	B
glass	I
.	O
‘	O
mathematicians	O
are	O
curious	O
birds	O
’	O
,	O
the	O
police	O
commissioner	O
said	O
to	O
his	O
wife	O
.	O
‘	O
you	O
see	O
,	O
we	O
had	O
all	O
those	O
partly	O
(	O
cid:12	O
)	O
lled	O
glasses	O
lined	O
up	O
in	O
rows	O
on	O
a	O
table	O
in	O
the	O
hotel	O
kitchen	O
.	O
only	O
one	O
contained	O
poison	O
,	O
and	O
we	O
wanted	O
to	O
know	O
which	O
one	O
before	O
searching	O
that	O
glass	O
for	O
(	O
cid:12	O
)	O
ngerprints	O
.	O
our	O
lab	O
could	O
test	B
the	O
liquid	O
in	O
each	O
glass	O
,	O
but	O
the	O
tests	O
take	O
time	O
and	O
money	O
,	O
so	O
we	O
wanted	O
to	O
make	O
as	O
few	O
of	O
them	O
as	O
possible	O
by	O
simultaneously	O
testing	O
mixtures	O
of	O
small	O
samples	O
from	O
groups	O
of	O
glasses	O
.	O
the	O
university	O
sent	O
over	O
a	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
104	O
5	O
|	O
symbol	O
codes	O
mathematics	O
professor	O
to	O
help	O
us	O
.	O
he	O
counted	O
the	O
glasses	O
,	O
smiled	O
and	O
said	O
:	O
‘	O
\pick	O
any	O
glass	O
you	O
want	O
,	O
commissioner	O
.	O
we	O
’	O
ll	O
test	B
it	O
(	O
cid:12	O
)	O
rst	O
.	O
''	O
‘	O
\but	O
won	O
’	O
t	O
that	O
waste	O
a	O
test	B
?	O
''	O
i	O
asked	O
.	O
‘	O
\no	O
,	O
''	O
he	O
said	O
,	O
\it	O
’	O
s	O
part	O
of	O
the	O
best	O
procedure	O
.	O
we	O
can	O
test	B
one	O
glass	O
(	O
cid:12	O
)	O
rst	O
.	O
it	O
doesn	O
’	O
t	O
matter	O
which	O
one	O
.	O
''	O
’	O
‘	O
how	O
many	O
glasses	O
were	O
there	O
to	O
start	O
with	O
?	O
’	O
the	O
commissioner	O
’	O
s	O
wife	O
asked	O
.	O
‘	O
i	O
don	O
’	O
t	O
remember	O
.	O
somewhere	O
between	O
100	O
and	O
200.	O
’	O
what	O
was	O
the	O
exact	O
number	O
of	O
glasses	O
?	O
solve	O
this	O
puzzle	B
and	O
then	O
explain	O
why	O
the	O
professor	O
was	O
in	O
fact	O
wrong	O
and	O
the	O
commissioner	O
was	O
right	O
.	O
what	O
is	O
in	O
fact	O
the	O
optimal	B
procedure	O
for	O
identifying	O
the	O
one	O
poisoned	B
glass	I
?	O
what	O
is	O
the	O
expected	O
waste	O
relative	B
to	O
this	O
optimum	O
if	O
one	O
followed	O
the	O
professor	O
’	O
s	O
strategy	O
?	O
explain	O
the	O
relationship	O
to	O
symbol	O
coding	O
.	O
exercise	O
5.31	O
.	O
[	O
2	O
,	O
p.106	O
]	O
assume	O
that	O
a	O
sequence	B
of	O
symbols	O
from	O
the	O
ensemble	B
x	O
introduced	O
at	O
the	O
beginning	O
of	O
this	O
chapter	O
is	O
compressed	O
using	O
the	O
code	B
c3	O
.	O
imagine	O
picking	O
one	O
bit	B
at	O
random	B
from	O
the	O
binary	O
encoded	O
sequence	B
c	O
=	O
c	O
(	O
x1	O
)	O
c	O
(	O
x2	O
)	O
c	O
(	O
x3	O
)	O
:	O
:	O
:	O
.	O
what	O
is	O
the	O
probability	B
that	O
this	O
bit	B
is	O
a	O
1	O
?	O
.	O
exercise	O
5.32	O
.	O
[	O
2	O
,	O
p.107	O
]	O
how	O
should	O
the	O
binary	O
hu	O
(	O
cid:11	O
)	O
man	O
encoding	O
scheme	O
be	O
modi	O
(	O
cid:12	O
)	O
ed	O
to	O
make	O
optimal	B
symbol	O
codes	O
in	O
an	O
encoding	O
alphabet	O
with	O
q	O
symbols	O
?	O
(	O
also	O
known	O
as	O
‘	O
radix	B
q	O
’	O
.	O
)	O
mixture	O
codes	O
it	O
is	O
a	O
tempting	O
idea	O
to	O
construct	O
a	O
‘	O
metacode	B
’	O
from	O
several	O
symbol	O
codes	O
that	O
assign	O
di	O
(	O
cid:11	O
)	O
erent-length	O
codewords	O
to	O
the	O
alternative	O
symbols	O
,	O
then	O
switch	O
from	O
one	O
code	B
to	O
another	O
,	O
choosing	O
whichever	O
assigns	O
the	O
shortest	O
codeword	B
to	O
the	O
current	O
symbol	O
.	O
clearly	O
we	O
can	O
not	O
do	O
this	O
for	O
free	O
.	O
if	O
one	O
wishes	O
to	O
choose	O
between	O
two	O
codes	O
,	O
then	O
it	O
is	O
necessary	O
to	O
lengthen	O
the	O
message	O
in	O
a	O
way	O
that	O
indicates	O
which	O
of	O
the	O
two	O
codes	O
is	O
being	O
used	O
.	O
if	O
we	O
indicate	O
this	O
choice	O
by	O
a	O
single	O
leading	O
bit	B
,	O
it	O
will	O
be	O
found	O
that	O
the	O
resulting	O
code	B
is	O
suboptimal	O
because	O
it	O
is	O
incomplete	O
(	O
that	O
is	O
,	O
it	O
fails	O
the	O
kraft	O
equality	O
)	O
.	O
exercise	O
5.33	O
.	O
[	O
3	O
,	O
p.108	O
]	O
prove	O
that	O
this	O
metacode	B
is	O
incomplete	O
,	O
and	O
explain	O
why	O
this	O
combined	O
code	B
is	O
suboptimal	O
.	O
5.9	O
solutions	O
solution	O
to	O
exercise	O
5.8	O
(	O
p.93	O
)	O
.	O
yes	O
,	O
c2	O
=	O
f1	O
;	O
101g	O
is	O
uniquely	B
decodeable	I
,	O
even	O
though	O
it	O
is	O
not	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
,	O
because	O
no	O
two	O
di	O
(	O
cid:11	O
)	O
erent	O
strings	O
can	O
map	O
onto	O
the	O
same	O
string	O
;	O
only	O
the	O
codeword	B
c	O
(	O
a2	O
)	O
=	O
101	O
contains	O
the	O
symbol	O
0.	O
solution	O
to	O
exercise	O
5.14	O
(	O
p.95	O
)	O
.	O
we	O
wish	O
to	O
prove	O
that	O
for	O
any	O
set	B
of	O
codeword	B
lengths	O
flig	O
satisfying	O
the	O
kraft	O
inequality	B
,	O
there	O
is	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
having	O
those	O
lengths	O
.	O
this	O
is	O
readily	O
proved	O
by	O
thinking	O
of	O
the	O
codewords	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gure	O
5.8	O
as	O
being	O
in	O
a	O
‘	O
codeword	B
supermarket	O
’	O
,	O
with	O
size	O
indicating	O
cost	O
.	O
we	O
imagine	O
purchasing	O
codewords	O
one	O
at	O
a	O
time	O
,	O
starting	O
from	O
the	O
shortest	O
codewords	O
(	O
i.e.	O
,	O
the	O
biggest	O
purchases	O
)	O
,	O
using	O
the	O
budget	B
shown	O
at	O
the	O
right	O
of	O
(	O
cid:12	O
)	O
gure	O
5.8.	O
we	O
start	O
at	O
one	O
side	O
of	O
the	O
codeword	O
supermarket	B
,	O
say	O
the	O
c3	O
:	O
pi	O
1/2	O
1/4	O
1/8	O
1/8	O
ai	O
c	O
(	O
ai	O
)	O
a	O
b	O
c	O
d	O
0	O
10	O
110	O
111	O
h	O
(	O
pi	O
)	O
1.0	O
2.0	O
3.0	O
3.0	O
li	O
1	O
2	O
3	O
3	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
105	O
figure	O
5.8.	O
the	O
codeword	B
supermarket	O
and	O
the	O
symbol	O
coding	O
budget	B
.	O
the	O
‘	O
cost	O
’	O
2	O
(	O
cid:0	O
)	O
l	O
of	O
each	O
codeword	B
(	O
with	O
length	O
l	O
)	O
is	O
indicated	O
by	O
the	O
size	O
of	O
the	O
box	O
it	O
is	O
written	O
in	O
.	O
the	O
total	O
budget	B
available	O
when	O
making	O
a	O
uniquely	B
decodeable	I
code	O
is	O
1.	O
figure	O
5.9.	O
proof	O
that	O
hu	O
(	O
cid:11	O
)	O
man	O
coding	O
makes	O
an	O
optimal	B
symbol	O
code	B
.	O
we	O
assume	O
that	O
the	O
rival	O
code	B
,	O
which	O
is	O
said	O
to	O
be	O
optimal	B
,	O
assigns	O
unequal	O
length	B
codewords	O
to	O
the	O
two	O
symbols	O
with	O
smallest	O
probability	B
,	O
a	O
and	O
b.	O
by	O
interchanging	O
codewords	O
a	O
and	O
c	O
of	O
the	O
rival	O
code	B
,	O
where	O
c	O
is	O
a	O
symbol	O
with	O
rival	O
codelength	O
as	O
long	O
as	O
b	O
’	O
s	O
,	O
we	O
can	O
make	O
a	O
code	B
better	O
than	O
the	O
rival	O
code	B
.	O
this	O
shows	O
that	O
the	O
rival	O
code	B
was	O
not	O
optimal	B
.	O
5.9	O
:	O
solutions	O
0	O
1	O
00	O
01	O
10	O
11	O
000	O
001	O
010	O
011	O
100	O
101	O
110	O
111	O
0000	O
0001	O
0010	O
0011	O
0100	O
0101	O
0110	O
0111	O
1000	O
1001	O
1010	O
1011	O
1100	O
1101	O
1110	O
1111	O
t	O
e	O
g	O
d	O
u	O
b	O
e	O
d	O
o	O
c	O
l	O
o	O
b	O
m	O
y	O
s	O
l	O
a	O
t	O
o	O
t	O
e	O
h	O
t	O
symbol	O
probability	O
hu	O
(	O
cid:11	O
)	O
man	O
a	O
b	O
c	O
pa	O
pb	O
pc	O
codewords	O
ch	O
(	O
a	O
)	O
ch	O
(	O
b	O
)	O
ch	O
(	O
c	O
)	O
rival	O
code	B
’	O
s	O
modi	O
(	O
cid:12	O
)	O
ed	O
rival	O
codewords	O
code	B
cr	O
(	O
a	O
)	O
cr	O
(	O
b	O
)	O
cr	O
(	O
c	O
)	O
cr	O
(	O
c	O
)	O
cr	O
(	O
b	O
)	O
cr	O
(	O
a	O
)	O
top	O
,	O
and	O
purchase	O
the	O
(	O
cid:12	O
)	O
rst	O
codeword	B
of	O
the	O
required	O
length	B
.	O
we	O
advance	O
down	O
the	O
supermarket	B
a	O
distance	B
2	O
(	O
cid:0	O
)	O
l	O
,	O
and	O
purchase	O
the	O
next	O
codeword	B
of	O
the	O
next	O
required	O
length	B
,	O
and	O
so	O
forth	O
.	O
because	O
the	O
codeword	B
lengths	O
are	O
getting	O
longer	O
,	O
and	O
the	O
corresponding	O
intervals	B
are	O
getting	O
shorter	O
,	O
we	O
can	O
always	O
buy	O
an	O
adjacent	O
codeword	B
to	O
the	O
latest	O
purchase	O
,	O
so	O
there	O
is	O
no	O
wasting	O
of	O
i=1	O
2	O
(	O
cid:0	O
)	O
li	O
the	O
budget	B
.	O
thus	O
at	O
the	O
ith	O
codeword	B
we	O
have	O
advanced	O
a	O
distance	B
pi	O
down	O
the	O
supermarket	B
;	O
ifp	O
2	O
(	O
cid:0	O
)	O
li	O
(	O
cid:20	O
)	O
1	O
,	O
we	O
will	O
have	O
purchased	O
all	O
the	O
codewords	O
without	O
running	O
out	O
of	O
budget	O
.	O
solution	O
to	O
exercise	O
5.16	O
(	O
p.99	O
)	O
.	O
the	O
proof	O
that	O
hu	O
(	O
cid:11	O
)	O
man	O
coding	O
is	O
optimal	B
depends	O
on	O
proving	O
that	O
the	O
key	O
step	O
in	O
the	O
algorithm	B
{	O
the	O
decision	O
to	O
give	O
the	O
two	O
symbols	O
with	O
smallest	O
probability	B
equal	O
encoded	O
lengths	O
{	O
can	O
not	O
lead	O
to	O
a	O
larger	O
expected	O
length	B
than	O
any	O
other	O
code	B
.	O
we	O
can	O
prove	O
this	O
by	O
contradiction	O
.	O
assume	O
that	O
the	O
two	O
symbols	O
with	O
smallest	O
probability	B
,	O
called	O
a	O
and	O
b	O
,	O
to	O
which	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
algorithm	B
would	O
assign	O
equal	O
length	B
codewords	O
,	O
do	O
not	O
have	O
equal	O
lengths	O
in	O
any	O
optimal	B
symbol	O
code	B
.	O
the	O
optimal	B
symbol	O
code	B
is	O
some	O
other	O
rival	O
code	B
in	O
which	O
these	O
two	O
codewords	O
have	O
unequal	O
lengths	O
la	O
and	O
lb	O
with	O
la	O
<	O
lb	O
.	O
without	O
loss	O
of	O
generality	O
we	O
can	O
assume	O
that	O
this	O
other	O
code	B
is	O
a	O
complete	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
,	O
because	O
any	O
codelengths	O
of	O
a	O
uniquely	B
decodeable	I
code	O
can	O
be	O
realized	O
by	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
.	O
in	O
this	O
rival	O
code	B
,	O
there	O
must	O
be	O
some	O
other	O
symbol	O
c	O
whose	O
probability	B
pc	O
is	O
greater	O
than	O
pa	O
and	O
whose	O
length	B
in	O
the	O
rival	O
code	B
is	O
greater	O
than	O
or	O
equal	O
to	O
lb	O
,	O
because	O
the	O
code	B
for	O
b	O
must	O
have	O
an	O
adjacent	O
codeword	B
of	O
equal	O
or	O
greater	O
length	B
{	O
a	O
complete	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
never	O
has	O
a	O
solo	O
codeword	B
of	O
the	O
maximum	O
length	O
.	O
consider	O
exchanging	O
the	O
codewords	O
of	O
a	O
and	O
c	O
(	O
(	O
cid:12	O
)	O
gure	O
5.9	O
)	O
,	O
so	O
that	O
a	O
is	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
106	O
5	O
|	O
symbol	O
codes	O
encoded	O
with	O
the	O
longer	O
codeword	B
that	O
was	O
c	O
’	O
s	O
,	O
and	O
c	O
,	O
which	O
is	O
more	O
probable	O
than	O
a	O
,	O
gets	O
the	O
shorter	O
codeword	B
.	O
clearly	O
this	O
reduces	O
the	O
expected	O
length	B
of	O
the	O
code	B
.	O
the	O
change	O
in	O
expected	O
length	B
is	O
(	O
pa	O
(	O
cid:0	O
)	O
pc	O
)	O
(	O
lc	O
(	O
cid:0	O
)	O
la	O
)	O
.	O
thus	O
we	O
have	O
contradicted	O
the	O
assumption	O
that	O
the	O
rival	O
code	B
is	O
optimal	B
.	O
therefore	O
it	O
is	O
valid	O
to	O
give	O
the	O
two	O
symbols	O
with	O
smallest	O
probability	B
equal	O
encoded	O
lengths	O
.	O
hu	O
(	O
cid:11	O
)	O
man	O
coding	O
produces	O
optimal	B
symbol	O
codes	O
.	O
2	O
solution	O
to	O
exercise	O
5.21	O
(	O
p.102	O
)	O
.	O
a	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
for	O
x	O
2	O
where	O
ax	O
=	O
f0	O
;	O
1g	O
and	O
px	O
=	O
f0:9	O
;	O
0:1g	O
is	O
f00	O
;	O
01	O
;	O
10	O
;	O
11g	O
!	O
f1	O
;	O
01	O
;	O
000	O
;	O
001g	O
.	O
this	O
code	B
has	O
l	O
(	O
c	O
;	O
x	O
2	O
)	O
=	O
1:29	O
,	O
whereas	O
the	O
entropy	B
h	O
(	O
x	O
2	O
)	O
is	O
0.938.	O
a	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
for	O
x	O
3	O
is	O
f000	O
;	O
100	O
;	O
010	O
;	O
001	O
;	O
101	O
;	O
011	O
;	O
110	O
;	O
111g	O
!	O
f1	O
;	O
011	O
;	O
010	O
;	O
001	O
;	O
00000	O
;	O
00001	O
;	O
00010	O
;	O
00011g	O
:	O
this	O
has	O
expected	O
length	B
l	O
(	O
c	O
;	O
x	O
3	O
)	O
=	O
1:598	O
whereas	O
the	O
entropy	B
h	O
(	O
x	O
3	O
)	O
is	O
1.4069.	O
a	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
for	O
x	O
4	O
maps	O
the	O
sixteen	O
source	O
strings	O
to	O
the	O
following	O
codelengths	O
:	O
f0000	O
;	O
1000	O
;	O
0100	O
;	O
0010	O
;	O
0001	O
;	O
1100	O
;	O
0110	O
;	O
0011	O
;	O
0101	O
;	O
1010	O
;	O
1001	O
;	O
1110	O
;	O
1101	O
;	O
1011	O
;	O
0111	O
;	O
1111g	O
!	O
f1	O
;	O
3	O
;	O
3	O
;	O
3	O
;	O
4	O
;	O
6	O
;	O
7	O
;	O
7	O
;	O
7	O
;	O
7	O
;	O
7	O
;	O
9	O
;	O
9	O
;	O
9	O
;	O
10	O
;	O
10g	O
:	O
this	O
has	O
expected	O
length	B
l	O
(	O
c	O
;	O
x	O
4	O
)	O
=	O
1:9702	O
whereas	O
the	O
entropy	B
h	O
(	O
x	O
4	O
)	O
is	O
1.876.	O
when	O
px	O
=	O
f0:6	O
;	O
0:4g	O
,	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
for	O
x	O
2	O
has	O
lengths	O
f2	O
;	O
2	O
;	O
2	O
;	O
2g	O
;	O
the	O
expected	O
length	B
is	O
2	O
bits	O
,	O
and	O
the	O
entropy	B
is	O
1.94	O
bits	O
.	O
a	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
for	O
x	O
4	O
is	O
shown	O
in	O
table	O
5.10.	O
the	O
expected	O
length	B
is	O
3.92	O
bits	O
,	O
and	O
the	O
entropy	B
is	O
3.88	O
bits	O
.	O
solution	O
to	O
exercise	O
5.22	O
(	O
p.102	O
)	O
.	O
the	O
set	B
of	O
probabilities	O
fp1	O
;	O
p2	O
;	O
p3	O
;	O
p4g	O
=	O
f1/6	O
;	O
1/6	O
;	O
1/3	O
;	O
1/3g	O
gives	O
rise	O
to	O
two	O
di	O
(	O
cid:11	O
)	O
erent	O
optimal	B
sets	O
of	O
codelengths	O
,	O
because	O
at	O
the	O
second	O
step	O
of	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
coding	O
algorithm	O
we	O
can	O
choose	O
any	O
of	O
the	O
three	O
possible	O
pairings	O
.	O
we	O
may	O
either	O
put	O
them	O
in	O
a	O
constant	O
length	B
code	O
f00	O
;	O
01	O
;	O
10	O
;	O
11g	O
or	O
the	O
code	B
f000	O
;	O
001	O
;	O
01	O
;	O
1g	O
.	O
both	O
codes	O
have	O
expected	O
length	B
2.	O
another	O
solution	O
is	O
fp1	O
;	O
p2	O
;	O
p3	O
;	O
p4g	O
=	O
f1/5	O
;	O
1/5	O
;	O
1/5	O
;	O
2/5g	O
.	O
and	O
a	O
third	O
is	O
fp1	O
;	O
p2	O
;	O
p3	O
;	O
p4g	O
=	O
f1/3	O
;	O
1/3	O
;	O
1/3	O
;	O
0g	O
.	O
solution	O
to	O
exercise	O
5.26	O
(	O
p.103	O
)	O
.	O
let	O
pmax	O
be	O
the	O
largest	O
probability	B
in	O
p1	O
;	O
p2	O
;	O
:	O
:	O
:	O
;	O
pi	O
.	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
the	O
expected	O
length	B
l	O
and	O
the	O
entropy	B
h	O
can	O
be	O
no	O
bigger	O
than	O
max	O
(	O
pmax	O
;	O
0:086	O
)	O
(	O
gallager	O
,	O
1978	O
)	O
.	O
see	O
exercises	O
5.27	O
{	O
5.28	O
to	O
understand	O
where	O
the	O
curious	O
0.086	O
comes	O
from	O
.	O
solution	O
to	O
exercise	O
5.27	O
(	O
p.103	O
)	O
.	O
length	B
(	O
cid:0	O
)	O
entropy	B
=	O
0.086.	O
solution	O
to	O
exercise	O
5.31	O
(	O
p.104	O
)	O
.	O
there	O
are	O
two	O
ways	O
to	O
answer	O
this	O
problem	O
correctly	O
,	O
and	O
one	O
popular	O
way	O
to	O
answer	O
it	O
incorrectly	O
.	O
let	O
’	O
s	O
give	O
the	O
incorrect	O
answer	O
(	O
cid:12	O
)	O
rst	O
:	O
erroneous	O
answer	O
.	O
\we	O
can	O
pick	O
a	O
random	B
bit	O
by	O
(	O
cid:12	O
)	O
rst	O
picking	O
a	O
random	B
source	O
symbol	O
xi	O
with	O
probability	O
pi	O
,	O
then	O
picking	O
a	O
random	B
bit	O
from	O
c	O
(	O
xi	O
)	O
.	O
if	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
fi	O
to	O
be	O
the	O
fraction	O
of	O
the	O
bits	O
of	O
c	O
(	O
xi	O
)	O
that	O
are	O
1s	O
,	O
we	O
(	O
cid:12	O
)	O
nd	O
p	O
(	O
bit	B
is	O
1	O
)	O
=	O
xi	O
pifi	O
(	O
5.34	O
)	O
=	O
1/2	O
(	O
cid:2	O
)	O
0	O
+	O
1/4	O
(	O
cid:2	O
)	O
1/2	O
+	O
1/8	O
(	O
cid:2	O
)	O
2/3	O
+	O
1/8	O
(	O
cid:2	O
)	O
1	O
=	O
1/3	O
.	O
''	O
(	O
5.35	O
)	O
ai	O
pi	O
li	O
c	O
(	O
ai	O
)	O
0000	O
0001	O
0010	O
0100	O
1000	O
1100	O
1010	O
1001	O
0110	O
0101	O
0011	O
1110	O
1101	O
1011	O
0111	O
1111	O
0.1296	O
0.0864	O
0.0864	O
0.0864	O
0.0864	O
0.0576	O
0.0576	O
0.0576	O
0.0576	O
0.0576	O
0.0576	O
0.0384	O
0.0384	O
0.0384	O
0.0384	O
0.0256	O
3	O
4	O
4	O
4	O
3	O
4	O
4	O
4	O
4	O
4	O
4	O
5	O
5	O
5	O
4	O
5	O
000	O
0100	O
0110	O
0111	O
100	O
1010	O
1100	O
1101	O
1110	O
1111	O
0010	O
00110	O
01010	O
01011	O
1011	O
00111	O
table	O
5.10.	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
for	O
x	O
4	O
when	O
p0	O
=	O
0:6.	O
column	O
3	O
shows	O
the	O
assigned	O
codelengths	O
and	O
column	O
4	O
the	O
codewords	O
.	O
some	O
strings	O
whose	O
probabilities	O
are	O
identical	O
,	O
e.g.	O
,	O
the	O
fourth	O
and	O
(	O
cid:12	O
)	O
fth	O
,	O
receive	O
di	O
(	O
cid:11	O
)	O
erent	O
codelengths	O
.	O
c3	O
:	O
ai	O
c	O
(	O
ai	O
)	O
a	O
b	O
c	O
d	O
0	O
10	O
110	O
111	O
pi	O
1/2	O
1/4	O
1/8	O
1/8	O
li	O
1	O
2	O
3	O
3	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
5.9	O
:	O
solutions	O
107	O
this	O
answer	O
is	O
wrong	O
because	O
it	O
falls	O
for	O
the	O
bus-stop	B
fallacy	O
,	O
which	O
was	O
intro-	O
duced	O
in	O
exercise	O
2.35	O
(	O
p.38	O
)	O
:	O
if	O
buses	O
arrive	O
at	O
random	B
,	O
and	O
we	O
are	O
interested	O
in	O
‘	O
the	O
average	B
time	O
from	O
one	O
bus	O
until	O
the	O
next	O
’	O
,	O
we	O
must	O
distinguish	O
two	O
possible	O
averages	O
:	O
(	O
a	O
)	O
the	O
average	B
time	O
from	O
a	O
randomly	O
chosen	O
bus	O
until	O
the	O
next	O
;	O
(	O
b	O
)	O
the	O
average	B
time	O
between	O
the	O
bus	O
you	O
just	O
missed	O
and	O
the	O
next	O
bus	O
.	O
the	O
second	O
‘	O
average	B
’	O
is	O
twice	O
as	O
big	O
as	O
the	O
(	O
cid:12	O
)	O
rst	O
because	O
,	O
by	O
waiting	O
for	O
a	O
bus	O
at	O
a	O
random	B
time	O
,	O
you	O
bias	B
your	O
selection	O
of	O
a	O
bus	O
in	O
favour	O
of	O
buses	O
that	O
follow	O
a	O
large	O
gap	O
.	O
you	O
’	O
re	O
unlikely	O
to	O
catch	O
a	O
bus	O
that	O
comes	O
10	O
seconds	O
after	O
a	O
preceding	O
bus	O
!	O
similarly	O
,	O
the	O
symbols	O
c	O
and	O
d	O
get	O
encoded	O
into	O
longer-length	O
binary	O
strings	O
than	O
a	O
,	O
so	O
when	O
we	O
pick	O
a	O
bit	B
from	O
the	O
compressed	O
string	O
at	O
random	B
,	O
we	O
are	O
more	O
likely	O
to	O
land	O
in	O
a	O
bit	B
belonging	O
to	O
a	O
c	O
or	O
a	O
d	O
than	O
would	O
be	O
given	O
by	O
the	O
probabilities	O
pi	O
in	O
the	O
expectation	B
(	O
5.34	O
)	O
.	O
all	O
the	O
probabilities	O
need	O
to	O
be	O
scaled	O
up	O
by	O
li	O
,	O
and	O
renormalized	O
.	O
correct	O
answer	O
in	O
the	O
same	O
style	O
.	O
every	O
time	O
symbol	O
xi	O
is	O
encoded	O
,	O
li	O
bits	O
are	O
added	O
to	O
the	O
binary	O
string	O
,	O
of	O
which	O
fili	O
are	O
1s	O
.	O
the	O
expected	O
number	O
of	O
1s	O
added	O
per	O
symbol	O
is	O
pifili	O
;	O
xi	O
and	O
the	O
expected	O
total	O
number	O
of	O
bits	O
added	O
per	O
symbol	O
is	O
pili	O
:	O
xi	O
so	O
the	O
fraction	O
of	O
1s	O
in	O
the	O
transmitted	O
string	O
is	O
p	O
(	O
bit	B
is	O
1	O
)	O
=	O
pi	O
pifili	O
pi	O
pili	O
1/2	O
(	O
cid:2	O
)	O
0	O
+	O
1/4	O
(	O
cid:2	O
)	O
1	O
+	O
1/8	O
(	O
cid:2	O
)	O
2	O
+	O
1/8	O
(	O
cid:2	O
)	O
3	O
=	O
7/4	O
(	O
5.36	O
)	O
(	O
5.37	O
)	O
(	O
5.38	O
)	O
=	O
7/8	O
7/4	O
=	O
1=2	O
:	O
for	O
a	O
general	O
symbol	O
code	B
and	O
a	O
general	O
ensemble	O
,	O
the	O
expectation	B
(	O
5.38	O
)	O
is	O
the	O
correct	O
answer	O
.	O
but	O
in	O
this	O
case	O
,	O
we	O
can	O
use	O
a	O
more	O
powerful	O
argument	O
.	O
information-theoretic	O
answer	O
.	O
the	O
encoded	O
string	O
c	O
is	O
the	O
output	O
of	O
an	O
optimal	B
compressor	O
that	O
compresses	O
samples	O
from	O
x	O
down	O
to	O
an	O
ex-	O
pected	O
length	B
of	O
h	O
(	O
x	O
)	O
bits	O
.	O
we	O
can	O
’	O
t	O
expect	O
to	O
compress	B
this	O
data	O
any	O
further	O
.	O
but	O
if	O
the	O
probability	B
p	O
(	O
bit	B
is	O
1	O
)	O
were	O
not	O
equal	O
to	O
1/2	O
then	O
it	O
would	O
be	O
possible	O
to	O
compress	B
the	O
binary	O
string	O
further	O
(	O
using	O
a	O
block	B
compression	O
code	B
,	O
say	O
)	O
.	O
therefore	O
p	O
(	O
bit	B
is	O
1	O
)	O
must	O
be	O
equal	O
to	O
1/2	O
;	O
in-	O
deed	O
the	O
probability	O
of	O
any	O
sequence	B
of	O
l	O
bits	O
in	O
the	O
compressed	O
stream	O
taking	O
on	O
any	O
particular	O
value	O
must	O
be	O
2	O
(	O
cid:0	O
)	O
l.	O
the	O
output	O
of	O
a	O
perfect	B
compressor	O
is	O
always	O
perfectly	O
random	B
bits	O
.	O
to	O
put	O
it	O
another	O
way	O
,	O
if	O
the	O
probability	B
p	O
(	O
bit	B
is	O
1	O
)	O
were	O
not	O
equal	O
to	O
1/2	O
,	O
then	O
the	O
information	B
content	I
per	O
bit	B
of	O
the	O
compressed	O
string	O
would	O
be	O
at	O
most	O
h2	O
(	O
p	O
(	O
1	O
)	O
)	O
,	O
which	O
would	O
be	O
less	O
than	O
1	O
;	O
but	O
this	O
contradicts	O
the	O
fact	O
that	O
we	O
can	O
recover	O
the	O
original	O
data	O
from	O
c	O
,	O
so	O
the	O
information	B
content	I
per	O
bit	B
of	O
the	O
compressed	O
string	O
must	O
be	O
h	O
(	O
x	O
)	O
=l	O
(	O
c	O
;	O
x	O
)	O
=	O
1.	O
solution	O
to	O
exercise	O
5.32	O
(	O
p.104	O
)	O
.	O
the	O
general	O
hu	O
(	O
cid:11	O
)	O
man	O
coding	O
algorithm	O
for	O
an	O
encoding	O
alphabet	O
with	O
q	O
symbols	O
has	O
one	O
di	O
(	O
cid:11	O
)	O
erence	O
from	O
the	O
binary	O
case	O
.	O
the	O
process	O
of	O
combining	O
q	O
symbols	O
into	O
1	O
symbol	O
reduces	O
the	O
number	O
of	O
symbols	O
by	O
q	O
(	O
cid:0	O
)	O
1.	O
so	O
if	O
we	O
start	O
with	O
a	O
symbols	O
,	O
we	O
’	O
ll	O
only	O
end	O
up	O
with	O
a	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
108	O
5	O
|	O
symbol	O
codes	O
complete	O
q-ary	O
tree	B
if	O
a	O
mod	O
(	O
q	O
(	O
cid:0	O
)	O
1	O
)	O
is	O
equal	O
to	O
1.	O
otherwise	O
,	O
we	O
know	O
that	O
whatever	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
we	O
make	O
,	O
it	O
must	O
be	O
an	O
incomplete	O
tree	B
with	O
a	O
number	O
of	O
missing	O
leaves	O
equal	O
,	O
modulo	O
(	O
q	O
(	O
cid:0	O
)	O
1	O
)	O
,	O
to	O
a	O
mod	O
(	O
q	O
(	O
cid:0	O
)	O
1	O
)	O
(	O
cid:0	O
)	O
1.	O
for	O
example	O
,	O
if	O
a	O
ternary	O
tree	B
is	O
built	O
for	O
eight	O
symbols	O
,	O
then	O
there	O
will	O
unavoidably	O
be	O
one	O
missing	O
leaf	B
in	O
the	O
tree	B
.	O
the	O
optimal	B
q-ary	O
code	B
is	O
made	O
by	O
putting	O
these	O
extra	O
leaves	O
in	O
the	O
longest	O
branch	O
of	O
the	O
tree	O
.	O
this	O
can	O
be	O
achieved	O
by	O
adding	O
the	O
appropriate	O
number	O
of	O
symbols	O
to	O
the	O
original	O
source	O
symbol	O
set	B
,	O
all	O
of	O
these	O
extra	O
symbols	O
having	O
probability	B
zero	O
.	O
the	O
total	O
number	O
of	O
leaves	O
is	O
then	O
equal	O
to	O
r	O
(	O
q	O
(	O
cid:0	O
)	O
1	O
)	O
+	O
1	O
,	O
for	O
some	O
integer	O
r.	O
the	O
symbols	O
are	O
then	O
repeatedly	O
combined	O
by	O
taking	O
the	O
q	O
symbols	O
with	O
smallest	O
probability	B
and	O
replacing	O
them	O
by	O
a	O
single	O
symbol	O
,	O
as	O
in	O
the	O
binary	O
hu	O
(	O
cid:11	O
)	O
man	O
coding	O
algorithm	O
.	O
solution	O
to	O
exercise	O
5.33	O
(	O
p.104	O
)	O
.	O
we	O
wish	O
to	O
show	O
that	O
a	O
greedy	O
metacode	B
,	O
which	O
picks	O
the	O
code	B
which	O
gives	O
the	O
shortest	O
encoding	O
,	O
is	O
actually	O
suboptimal	O
,	O
because	O
it	O
violates	O
the	O
kraft	O
inequality	B
.	O
we	O
’	O
ll	O
assume	O
that	O
each	O
symbol	O
x	O
is	O
assigned	O
lengths	O
lk	O
(	O
x	O
)	O
by	O
each	O
of	O
the	O
candidate	O
codes	O
ck	O
.	O
let	O
us	O
assume	O
there	O
are	O
k	O
alternative	O
codes	O
and	O
that	O
we	O
can	O
encode	O
which	O
code	B
is	O
being	O
used	O
with	O
a	O
header	O
of	O
length	O
log	O
k	O
bits	O
.	O
then	O
the	O
metacode	B
assigns	O
lengths	O
l0	O
(	O
x	O
)	O
that	O
are	O
given	O
by	O
l0	O
(	O
x	O
)	O
=	O
log2	O
k	O
+	O
min	O
k	O
lk	O
(	O
x	O
)	O
:	O
we	O
compute	O
the	O
kraft	O
sum	O
:	O
2	O
(	O
cid:0	O
)	O
l0	O
(	O
x	O
)	O
=	O
2	O
(	O
cid:0	O
)	O
mink	O
lk	O
(	O
x	O
)	O
:	O
s	O
=xx	O
1	O
k	O
xx	O
(	O
5.39	O
)	O
(	O
5.40	O
)	O
let	O
’	O
s	O
divide	O
the	O
set	B
ax	O
into	O
non-overlapping	O
subsets	O
fakgk	O
ak	O
contains	O
all	O
the	O
symbols	O
x	O
that	O
the	O
metacode	B
sends	O
via	O
code	B
k.	O
then	O
k=1	O
such	O
that	O
subset	B
2	O
(	O
cid:0	O
)	O
lk	O
(	O
x	O
)	O
:	O
(	O
5.41	O
)	O
now	O
if	O
one	O
sub-code	O
k	O
satis	O
(	O
cid:12	O
)	O
es	O
the	O
kraft	O
equality	O
px2ax	O
must	O
be	O
the	O
case	O
that	O
2	O
(	O
cid:0	O
)	O
lk	O
(	O
x	O
)	O
=	O
1	O
,	O
then	O
it	O
(	O
5.42	O
)	O
with	O
equality	O
only	O
if	O
all	O
the	O
symbols	O
x	O
are	O
in	O
ak	O
,	O
which	O
would	O
mean	B
that	O
we	O
are	O
only	O
using	O
one	O
of	O
the	O
k	O
codes	O
.	O
so	O
s	O
=	O
1	O
k	O
xk	O
xx2ak	O
2	O
(	O
cid:0	O
)	O
lk	O
(	O
x	O
)	O
(	O
cid:20	O
)	O
1	O
;	O
xx2ak	O
s	O
(	O
cid:20	O
)	O
1	O
k	O
k	O
xk=1	O
1	O
=	O
1	O
;	O
(	O
5.43	O
)	O
with	O
equality	O
only	O
if	O
equation	O
(	O
5.42	O
)	O
is	O
an	O
equality	O
for	O
all	O
codes	O
k.	O
but	O
it	O
’	O
s	O
impossible	O
for	O
all	O
the	O
symbols	O
to	O
be	O
in	O
all	O
the	O
non-overlapping	O
subsets	O
fakgk	O
k=1	O
,	O
so	O
we	O
can	O
’	O
t	O
have	O
equality	O
(	O
5.42	O
)	O
holding	O
for	O
all	O
k.	O
so	O
s	O
<	O
1.	O
another	O
way	O
of	O
seeing	O
that	O
a	O
mixture	O
code	O
is	O
suboptimal	O
is	O
to	O
consider	O
the	O
binary	O
tree	O
that	O
it	O
de	O
(	O
cid:12	O
)	O
nes	O
.	O
think	O
of	O
the	O
special	O
case	O
of	O
two	O
codes	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
bit	B
we	O
send	O
identi	O
(	O
cid:12	O
)	O
es	O
which	O
code	B
we	O
are	O
using	O
.	O
now	O
,	O
in	O
a	O
complete	O
code	B
,	O
any	O
subsequent	O
binary	O
string	O
is	O
a	O
valid	O
string	O
.	O
but	O
once	O
we	O
know	O
that	O
we	O
are	O
using	O
,	O
say	O
,	O
code	B
a	O
,	O
we	O
know	O
that	O
what	O
follows	O
can	O
only	O
be	O
a	O
codeword	B
corresponding	O
to	O
a	O
symbol	O
x	O
whose	O
encoding	O
is	O
shorter	O
under	O
code	B
a	O
than	O
code	B
b.	O
so	O
some	O
strings	O
are	O
invalid	O
continuations	O
,	O
and	O
the	O
mixture	O
code	O
is	O
incomplete	O
and	O
suboptimal	O
.	O
for	O
further	O
discussion	O
of	O
this	O
issue	O
and	O
its	O
relationship	O
to	O
probabilistic	O
modelling	O
read	O
about	O
‘	O
bits	B
back	I
coding	O
’	O
in	O
section	O
28.3	O
and	O
in	O
frey	O
(	O
1998	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
chapter	O
6	O
before	O
reading	O
chapter	O
6	O
,	O
you	O
should	O
have	O
read	O
the	O
previous	O
chapter	O
and	O
worked	O
on	O
most	O
of	O
the	O
exercises	O
in	O
it	O
.	O
we	O
’	O
ll	O
also	O
make	O
use	O
of	O
some	O
bayesian	O
modelling	B
ideas	O
that	O
arrived	O
in	O
the	O
vicinity	O
of	O
exercise	O
2.8	O
(	O
p.30	O
)	O
.	O
109	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
6	O
stream	B
codes	I
in	O
this	O
chapter	O
we	O
discuss	O
two	O
data	B
compression	I
schemes	O
.	O
arithmetic	B
coding	I
is	O
a	O
beautiful	O
method	B
that	O
goes	O
hand	O
in	O
hand	O
with	O
the	O
philosophy	B
that	O
compression	B
of	O
data	O
from	O
a	O
source	O
entails	O
probabilistic	O
mod-	O
elling	O
of	O
that	O
source	O
.	O
as	O
of	O
1999	O
,	O
the	O
best	O
compression	B
methods	O
for	O
text	O
(	O
cid:12	O
)	O
les	O
use	O
arithmetic	B
coding	I
,	O
and	O
several	O
state-of-the-art	O
image	B
compression	I
systems	O
use	O
it	O
too	O
.	O
lempel	O
{	O
ziv	O
coding	O
is	O
a	O
‘	O
universal	B
’	O
method	B
,	O
designed	O
under	O
the	O
philosophy	B
that	O
we	O
would	O
like	O
a	O
single	O
compression	O
algorithm	B
that	O
will	O
do	O
a	O
reasonable	O
job	O
for	O
any	O
source	O
.	O
in	O
fact	O
,	O
for	O
many	O
real	O
life	B
sources	O
,	O
this	O
algorithm	B
’	O
s	O
universal	B
properties	O
hold	O
only	O
in	O
the	O
limit	O
of	O
unfeasibly	O
large	O
amounts	O
of	O
data	O
,	O
but	O
,	O
all	O
the	O
same	O
,	O
lempel	O
{	O
ziv	O
compression	B
is	O
widely	O
used	O
and	O
often	O
e	O
(	O
cid:11	O
)	O
ective	O
.	O
6.1	O
the	O
guessing	B
game	I
as	O
a	O
motivation	O
for	O
these	O
two	O
compression	B
methods	O
,	O
consider	O
the	O
redundancy	B
in	O
a	O
typical	B
english	O
text	O
(	O
cid:12	O
)	O
le	O
.	O
such	O
(	O
cid:12	O
)	O
les	O
have	O
redundancy	B
at	O
several	O
levels	O
:	O
for	O
example	O
,	O
they	O
contain	O
the	O
ascii	O
characters	O
with	O
non-equal	O
frequency	B
;	O
certain	O
consecutive	O
pairs	O
of	O
letters	O
are	O
more	O
probable	O
than	O
others	O
;	O
and	O
entire	O
words	O
can	O
be	O
predicted	O
given	O
the	O
context	O
and	O
a	O
semantic	O
understanding	O
of	O
the	O
text	O
.	O
to	O
illustrate	O
the	O
redundancy	B
of	O
english	O
,	O
and	O
a	O
curious	O
way	O
in	O
which	O
it	O
could	O
be	O
compressed	O
,	O
we	O
can	O
imagine	O
a	O
guessing	B
game	I
in	O
which	O
an	O
english	O
speaker	O
repeatedly	O
attempts	O
to	O
predict	O
the	O
next	O
character	O
in	O
a	O
text	O
(	O
cid:12	O
)	O
le	O
.	O
for	O
simplicity	O
,	O
let	O
us	O
assume	O
that	O
the	O
allowed	O
alphabet	O
consists	O
of	O
the	O
26	O
upper	O
case	O
letters	O
a	O
,	O
b	O
,	O
c	O
,	O
...	O
,	O
z	O
and	O
a	O
space	O
‘	O
-	O
’	O
.	O
the	O
game	B
involves	O
asking	O
the	O
subject	O
to	O
guess	O
the	O
next	O
character	O
repeatedly	O
,	O
the	O
only	O
feedback	B
being	O
whether	O
the	O
guess	O
is	O
correct	O
or	O
not	O
,	O
until	O
the	O
character	O
is	O
correctly	O
guessed	O
.	O
after	O
a	O
correct	O
guess	O
,	O
we	O
note	O
the	O
number	O
of	O
guesses	O
that	O
were	O
made	O
when	O
the	O
character	O
was	O
identi	O
(	O
cid:12	O
)	O
ed	O
,	O
and	O
ask	O
the	O
subject	O
to	O
guess	O
the	O
next	O
character	O
in	O
the	O
same	O
way	O
.	O
one	O
sentence	O
gave	O
the	O
following	O
result	O
when	O
a	O
human	B
was	O
asked	O
to	O
guess	O
a	O
sentence	O
.	O
the	O
numbers	O
of	O
guesses	O
are	O
listed	O
below	O
each	O
character	O
.	O
t	O
h	O
e	O
r	O
e	O
-	O
i	O
s	O
-	O
n	O
o	O
-	O
r	O
e	O
v	O
e	O
r	O
s	O
e	O
-	O
o	O
n	O
-	O
a	O
-	O
m	O
o	O
t	O
o	O
r	O
c	O
y	O
c	O
l	O
e	O
-	O
1	O
1	O
1	O
5	O
1	O
1	O
2	O
1	O
1	O
2	O
1	O
1	O
15	O
1	O
17	O
1	O
1	O
1	O
2	O
1	O
3	O
2	O
1	O
2	O
2	O
7	O
1	O
1	O
1	O
1	O
4	O
1	O
1	O
1	O
1	O
1	O
notice	O
that	O
in	O
many	O
cases	O
,	O
the	O
next	O
letter	O
is	O
guessed	O
immediately	O
,	O
in	O
one	O
guess	O
.	O
in	O
other	O
cases	O
,	O
particularly	O
at	O
the	O
start	O
of	O
syllables	O
,	O
more	O
guesses	O
are	O
needed	O
.	O
what	O
do	O
this	O
game	B
and	O
these	O
results	O
o	O
(	O
cid:11	O
)	O
er	O
us	O
?	O
first	O
,	O
they	O
demonstrate	O
the	O
redundancy	B
of	O
english	O
from	O
the	O
point	O
of	O
view	O
of	O
an	O
english	O
speaker	O
.	O
second	O
,	O
this	O
game	B
might	O
be	O
used	O
in	O
a	O
data	B
compression	I
scheme	O
,	O
as	O
follows	O
.	O
110	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
6.2	O
:	O
arithmetic	O
codes	O
111	O
the	O
string	O
of	O
numbers	O
‘	O
1	O
,	O
1	O
,	O
1	O
,	O
5	O
,	O
1	O
,	O
.	O
.	O
.	O
’	O
,	O
listed	O
above	O
,	O
was	O
obtained	O
by	O
presenting	O
the	O
text	O
to	O
the	O
subject	O
.	O
the	O
maximum	O
number	O
of	O
guesses	O
that	O
the	O
subject	O
will	O
make	O
for	O
a	O
given	O
letter	O
is	O
twenty-seven	O
,	O
so	O
what	O
the	O
subject	O
is	O
doing	O
for	O
us	O
is	O
performing	O
a	O
time-varying	O
mapping	B
of	O
the	O
twenty-seven	O
letters	O
fa	O
;	O
b	O
;	O
c	O
;	O
:	O
:	O
:	O
;	O
z	O
;	O
(	O
cid:0	O
)	O
g	O
onto	O
the	O
twenty-seven	O
numbers	O
f1	O
;	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
;	O
27g	O
,	O
which	O
we	O
can	O
view	O
as	O
symbols	O
in	O
a	O
new	O
alphabet	O
.	O
the	O
total	O
number	O
of	O
symbols	O
has	O
not	O
been	O
reduced	O
,	O
but	O
since	O
he	O
uses	O
some	O
of	O
these	O
symbols	O
much	O
more	O
frequently	O
than	O
others	O
{	O
for	O
example	O
,	O
1	O
and	O
2	O
{	O
it	O
should	O
be	O
easy	O
to	O
compress	B
this	O
new	O
string	O
of	O
symbols	O
.	O
how	O
would	O
the	O
uncompression	B
of	O
the	O
sequence	B
of	O
numbers	O
‘	O
1	O
,	O
1	O
,	O
1	O
,	O
5	O
,	O
1	O
,	O
.	O
.	O
.	O
’	O
work	O
?	O
at	O
uncompression	B
time	O
,	O
we	O
do	O
not	O
have	O
the	O
original	O
string	O
‘	O
there	O
.	O
.	O
.	O
’	O
,	O
we	O
have	O
only	O
the	O
encoded	O
sequence	B
.	O
imagine	O
that	O
our	O
subject	O
has	O
an	O
absolutely	O
identical	B
twin	I
who	O
also	O
plays	O
the	O
guessing	B
game	I
with	O
us	O
,	O
as	O
if	O
we	O
knew	O
the	O
source	O
text	O
.	O
if	O
we	O
stop	O
him	O
whenever	O
he	O
has	O
made	O
a	O
number	O
of	O
guesses	O
equal	O
to	O
the	O
given	O
number	O
,	O
then	O
he	O
will	O
have	O
just	O
guessed	O
the	O
correct	O
letter	O
,	O
and	O
we	O
can	O
then	O
say	O
‘	O
yes	O
,	O
that	O
’	O
s	O
right	O
’	O
,	O
and	O
move	O
to	O
the	O
next	O
character	O
.	O
alternatively	O
,	O
if	O
the	O
identical	B
twin	I
is	O
not	O
available	O
,	O
we	O
could	O
design	O
a	O
compression	B
system	O
with	O
the	O
help	O
of	O
just	O
one	O
human	B
as	O
follows	O
.	O
we	O
choose	O
a	O
window	B
length	O
l	O
,	O
that	O
is	O
,	O
a	O
number	O
of	O
characters	O
of	O
context	O
to	O
show	O
the	O
human	B
.	O
for	O
every	O
one	O
of	O
the	O
27l	O
possible	O
strings	O
of	O
length	O
l	O
,	O
we	O
ask	O
them	O
,	O
‘	O
what	O
would	O
you	O
predict	O
is	O
the	O
next	O
character	O
?	O
’	O
,	O
and	O
‘	O
if	O
that	O
prediction	B
were	O
wrong	O
,	O
what	O
would	O
your	O
next	O
guesses	O
be	O
?	O
’	O
.	O
after	O
tabulating	O
their	O
answers	O
to	O
these	O
26	O
(	O
cid:2	O
)	O
27l	O
questions	O
,	O
we	O
could	O
use	O
two	O
copies	O
of	O
these	O
enormous	O
tables	O
at	O
the	O
encoder	B
and	O
the	O
decoder	B
in	O
place	O
of	O
the	O
two	O
human	B
twins	O
.	O
such	O
a	O
language	B
model	I
is	O
called	O
an	O
lth	O
order	O
markov	O
model	B
.	O
these	O
systems	O
are	O
clearly	O
unrealistic	O
for	O
practical	O
compression	B
,	O
but	O
they	O
illustrate	O
several	O
principles	O
that	O
we	O
will	O
make	O
use	O
of	O
now	O
.	O
6.2	O
arithmetic	O
codes	O
when	O
we	O
discussed	O
variable-length	B
symbol	O
codes	O
,	O
and	O
the	O
optimal	B
hu	O
(	O
cid:11	O
)	O
man	O
algorithm	B
for	O
constructing	O
them	O
,	O
we	O
concluded	O
by	O
pointing	O
out	O
two	O
practical	B
and	O
theoretical	O
problems	O
with	O
hu	O
(	O
cid:11	O
)	O
man	O
codes	O
(	O
section	B
5.6	O
)	O
.	O
these	O
defects	O
are	O
recti	O
(	O
cid:12	O
)	O
ed	O
by	O
arithmetic	O
codes	O
,	O
which	O
were	O
invented	O
by	O
elias	O
,	O
by	O
rissanen	O
and	O
by	O
pasco	O
,	O
and	O
subsequently	O
made	O
practical	B
by	O
witten	O
et	O
al	O
.	O
(	O
1987	O
)	O
.	O
in	O
an	O
arithmetic	O
code	O
,	O
the	O
probabilistic	O
modelling	O
is	O
clearly	O
separated	O
from	O
the	O
encoding	O
operation	O
.	O
the	O
system	O
is	O
rather	O
similar	O
to	O
the	O
guessing	B
game	I
.	O
the	O
human	B
predictor	O
is	O
replaced	O
by	O
a	O
probabilistic	B
model	I
of	O
the	O
source	O
.	O
as	O
each	O
symbol	O
is	O
produced	O
by	O
the	O
source	O
,	O
the	O
probabilistic	B
model	I
supplies	O
a	O
predictive	B
distribution	I
over	O
all	O
possible	O
values	O
of	O
the	O
next	O
symbol	O
,	O
that	O
is	O
,	O
a	O
list	O
of	O
positive	O
numbers	O
fpig	O
that	O
sum	O
to	O
one	O
.	O
if	O
we	O
choose	O
to	O
model	B
the	O
source	O
as	O
producing	O
i.i.d	O
.	O
symbols	O
with	O
some	O
known	O
distribution	B
,	O
then	O
the	O
predictive	B
distribution	I
is	O
the	O
same	O
every	O
time	O
;	O
but	O
arithmetic	B
coding	I
can	O
with	O
equal	O
ease	O
handle	O
complex	B
adaptive	O
models	O
that	O
produce	O
context-dependent	O
predictive	O
distributions	O
.	O
the	O
predictive	O
model	O
is	O
usually	O
implemented	O
in	O
a	O
computer	B
program	O
.	O
the	O
encoder	B
makes	O
use	O
of	O
the	O
model	O
’	O
s	O
predictions	O
to	O
create	O
a	O
binary	O
string	O
.	O
the	O
decoder	B
makes	O
use	O
of	O
an	O
identical	B
twin	I
of	O
the	O
model	B
(	O
just	O
as	O
in	O
the	O
guessing	B
game	I
)	O
to	O
interpret	O
the	O
binary	O
string	O
.	O
let	O
the	O
source	O
alphabet	O
be	O
ax	O
=	O
fa1	O
;	O
:	O
:	O
:	O
;	O
aig	O
,	O
and	O
let	O
the	O
ith	O
symbol	O
ai	O
have	O
the	O
special	O
meaning	O
‘	O
end	O
of	O
transmission	O
’	O
.	O
the	O
source	O
spits	O
out	O
a	O
sequence	B
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
xn	O
;	O
:	O
:	O
:	O
:	O
the	O
source	O
does	O
not	O
necessarily	O
produce	O
i.i.d	O
.	O
symbols	O
.	O
we	O
will	O
assume	O
that	O
a	O
computer	B
program	O
is	O
provided	O
to	O
the	O
encoder	B
that	O
assigns	O
a	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
112	O
6	O
|	O
stream	B
codes	I
predictive	O
probability	B
distribution	O
over	O
ai	O
given	O
the	O
sequence	B
that	O
has	O
occurred	O
thus	O
far	O
,	O
p	O
(	O
xn	O
=	O
ai	O
j	O
x1	O
;	O
:	O
:	O
:	O
;	O
xn	O
(	O
cid:0	O
)	O
1	O
)	O
.	O
the	O
receiver	O
has	O
an	O
identical	O
program	O
that	O
produces	O
the	O
same	O
predictive	O
probability	O
distribution	B
p	O
(	O
xn	O
=	O
ai	O
j	O
x1	O
;	O
:	O
:	O
:	O
;	O
xn	O
(	O
cid:0	O
)	O
1	O
)	O
.	O
0.00	O
0.25	O
0.50	O
0.75	O
1.00	O
(	O
cid:27	O
)	O
6	O
01	O
?	O
6	O
0	O
?	O
6	O
1	O
?	O
01101	O
figure	O
6.1.	O
binary	O
strings	O
de	O
(	O
cid:12	O
)	O
ne	O
real	O
intervals	B
within	O
the	O
real	O
line	O
[	O
0,1	O
)	O
.	O
we	O
(	O
cid:12	O
)	O
rst	O
encountered	O
a	O
picture	O
like	O
this	O
when	O
we	O
discussed	O
the	O
symbol-code	O
supermarket	B
in	O
chapter	O
5.	O
concepts	O
for	O
understanding	O
arithmetic	B
coding	I
notation	O
for	O
intervals	O
.	O
the	O
interval	O
[	O
0:01	O
;	O
0:10	O
)	O
is	O
all	O
numbers	O
between	O
0:01	O
and	O
0:10	O
,	O
including	O
0:01	O
_0	O
(	O
cid:17	O
)	O
0:01000	O
:	O
:	O
:	O
but	O
not	O
0:10	O
_0	O
(	O
cid:17	O
)	O
0:10000	O
:	O
:	O
:	O
:	O
a	O
binary	O
transmission	O
de	O
(	O
cid:12	O
)	O
nes	O
an	O
interval	O
within	O
the	O
real	O
line	O
from	O
0	O
to	O
1.	O
for	O
example	O
,	O
the	O
string	O
01	O
is	O
interpreted	O
as	O
a	O
binary	O
real	O
number	O
0.01.	O
.	O
.	O
,	O
which	O
corresponds	O
to	O
the	O
interval	O
[	O
0:01	O
;	O
0:10	O
)	O
in	O
binary	O
,	O
i.e.	O
,	O
the	O
interval	O
[	O
0:25	O
;	O
0:50	O
)	O
in	O
base	O
ten	O
.	O
the	O
longer	O
string	O
01101	O
corresponds	O
to	O
a	O
smaller	O
interval	O
[	O
0:01101	O
;	O
0:01110	O
)	O
.	O
because	O
01101	O
has	O
the	O
(	O
cid:12	O
)	O
rst	O
string	O
,	O
01	O
,	O
as	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
,	O
the	O
new	O
in-	O
terval	O
is	O
a	O
sub-interval	O
of	O
the	O
interval	O
[	O
0:01	O
;	O
0:10	O
)	O
.	O
a	O
one-megabyte	O
binary	O
(	O
cid:12	O
)	O
le	O
(	O
223	O
bits	O
)	O
is	O
thus	O
viewed	O
as	O
specifying	O
a	O
number	O
between	O
0	O
and	O
1	O
to	O
a	O
precision	B
of	O
about	O
two	O
million	O
decimal	O
places	O
{	O
two	O
million	O
decimal	O
digits	O
,	O
because	O
each	O
byte	B
translates	O
into	O
a	O
little	O
more	O
than	O
two	O
decimal	O
digits	O
.	O
now	O
,	O
we	O
can	O
also	O
divide	O
the	O
real	O
line	O
[	O
0,1	O
)	O
into	O
i	O
intervals	B
of	O
lengths	O
equal	O
to	O
the	O
probabilities	O
p	O
(	O
x1	O
=	O
ai	O
)	O
,	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
6.2	O
.	O
0.00	O
p	O
(	O
x1	O
=	O
a1	O
)	O
p	O
(	O
x1	O
=	O
a1	O
)	O
+	O
p	O
(	O
x1	O
=	O
a2	O
)	O
p	O
(	O
x1	O
=	O
a1	O
)	O
+	O
:	O
:	O
:	O
+	O
p	O
(	O
x1	O
=	O
ai	O
(	O
cid:0	O
)	O
1	O
)	O
1.0	O
(	O
cid:27	O
)	O
(	O
cid:27	O
)	O
...	O
6	O
?	O
a1	O
6	O
a2	O
?	O
...	O
6	O
?	O
ai	O
figure	O
6.2.	O
a	O
probabilistic	B
model	I
de	O
(	O
cid:12	O
)	O
nes	O
real	O
intervals	B
within	O
the	O
real	O
line	O
[	O
0,1	O
)	O
.	O
a2a1	O
a2a5	O
we	O
may	O
then	O
take	O
each	O
interval	O
ai	O
and	O
subdivide	O
it	O
into	O
intervals	B
de-	O
is	O
proportional	O
to	O
indeed	O
the	O
length	B
of	O
the	O
interval	O
aiaj	O
will	O
be	O
precisely	O
noted	O
aia1	O
;	O
aia2	O
;	O
:	O
:	O
:	O
;	O
aiai	O
,	O
such	O
that	O
the	O
length	B
of	O
aiaj	O
p	O
(	O
x2	O
=	O
aj	O
j	O
x1	O
=	O
ai	O
)	O
.	O
the	O
joint	B
probability	O
p	O
(	O
x1	O
=	O
ai	O
;	O
x2	O
=	O
aj	O
)	O
=	O
p	O
(	O
x1	O
=	O
ai	O
)	O
p	O
(	O
x2	O
=	O
aj	O
j	O
x1	O
=	O
ai	O
)	O
:	O
(	O
6.1	O
)	O
iterating	O
this	O
procedure	O
,	O
the	O
interval	O
[	O
0	O
;	O
1	O
)	O
can	O
be	O
divided	O
into	O
a	O
sequence	B
of	O
intervals	B
corresponding	O
to	O
all	O
possible	O
(	O
cid:12	O
)	O
nite	O
length	B
strings	O
x1x2	O
:	O
:	O
:	O
xn	O
,	O
such	O
that	O
the	O
length	B
of	O
an	O
interval	O
is	O
equal	O
to	O
the	O
probability	O
of	O
the	O
string	O
given	O
our	O
model	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
113	O
algorithm	B
6.3.	O
arithmetic	B
coding	I
.	O
iterative	O
procedure	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
interval	O
[	O
u	O
;	O
v	O
)	O
for	O
the	O
string	O
x1x2	O
:	O
:	O
:	O
xn	O
.	O
6.2	O
:	O
arithmetic	O
codes	O
u	O
:	O
=	O
0.0	O
v	O
:	O
=	O
1.0	O
p	O
:	O
=	O
v	O
(	O
cid:0	O
)	O
u	O
for	O
n	O
=	O
1	O
to	O
n	O
{	O
compute	O
the	O
cumulative	O
probabilities	O
qn	O
and	O
rn	O
(	O
6.2	O
,	O
6.3	O
)	O
v	O
:	O
=	O
u	O
+	O
prn	O
(	O
xn	O
j	O
x1	O
;	O
:	O
:	O
:	O
;	O
xn	O
(	O
cid:0	O
)	O
1	O
)	O
u	O
:	O
=	O
u	O
+	O
pqn	O
(	O
xn	O
j	O
x1	O
;	O
:	O
:	O
:	O
;	O
xn	O
(	O
cid:0	O
)	O
1	O
)	O
p	O
:	O
=	O
v	O
(	O
cid:0	O
)	O
u	O
}	O
formulae	O
describing	O
arithmetic	B
coding	I
the	O
process	O
depicted	O
in	O
(	O
cid:12	O
)	O
gure	O
6.2	O
can	O
be	O
written	O
explicitly	O
as	O
follows	O
.	O
the	O
intervals	B
are	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
terms	O
of	O
the	O
lower	O
and	O
upper	O
cumulative	O
probabilities	O
qn	O
(	O
ai	O
j	O
x1	O
;	O
:	O
:	O
:	O
;	O
xn	O
(	O
cid:0	O
)	O
1	O
)	O
(	O
cid:17	O
)	O
rn	O
(	O
ai	O
j	O
x1	O
;	O
:	O
:	O
:	O
;	O
xn	O
(	O
cid:0	O
)	O
1	O
)	O
(	O
cid:17	O
)	O
i	O
(	O
cid:0	O
)	O
1	O
xi0	O
=	O
1	O
xi0	O
=	O
1	O
i	O
p	O
(	O
xn	O
=	O
ai0	O
j	O
x1	O
;	O
:	O
:	O
:	O
;	O
xn	O
(	O
cid:0	O
)	O
1	O
)	O
;	O
p	O
(	O
xn	O
=	O
ai0	O
j	O
x1	O
;	O
:	O
:	O
:	O
;	O
xn	O
(	O
cid:0	O
)	O
1	O
)	O
:	O
(	O
6.2	O
)	O
(	O
6.3	O
)	O
as	O
the	O
nth	O
symbol	O
arrives	O
,	O
we	O
subdivide	O
the	O
n	O
(	O
cid:0	O
)	O
1th	O
interval	O
at	O
the	O
points	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
qn	O
and	O
rn	O
.	O
for	O
example	O
,	O
starting	O
with	O
the	O
(	O
cid:12	O
)	O
rst	O
symbol	O
,	O
the	O
intervals	B
‘	O
a1	O
’	O
,	O
‘	O
a2	O
’	O
,	O
and	O
‘	O
ai	O
’	O
are	O
a1	O
$	O
[	O
q1	O
(	O
a1	O
)	O
;	O
r1	O
(	O
a1	O
)	O
)	O
=	O
[	O
0	O
;	O
p	O
(	O
x1	O
=	O
a1	O
)	O
)	O
;	O
a2	O
$	O
[	O
q1	O
(	O
a2	O
)	O
;	O
r1	O
(	O
a2	O
)	O
)	O
=	O
[	O
p	O
(	O
x	O
=	O
a1	O
)	O
;	O
p	O
(	O
x	O
=	O
a1	O
)	O
+	O
p	O
(	O
x	O
=	O
a2	O
)	O
)	O
;	O
(	O
6.4	O
)	O
(	O
6.5	O
)	O
and	O
ai	O
$	O
[	O
q1	O
(	O
ai	O
)	O
;	O
r1	O
(	O
ai	O
)	O
)	O
=	O
[	O
p	O
(	O
x1	O
=	O
a1	O
)	O
+	O
:	O
:	O
:	O
+	O
p	O
(	O
x1	O
=	O
ai	O
(	O
cid:0	O
)	O
1	O
)	O
;	O
1:0	O
)	O
:	O
(	O
6.6	O
)	O
algorithm	B
6.3	O
describes	O
the	O
general	O
procedure	O
.	O
to	O
encode	O
a	O
string	O
x1x2	O
:	O
:	O
:	O
xn	O
,	O
we	O
locate	O
the	O
interval	O
corresponding	O
to	O
x1x2	O
:	O
:	O
:	O
xn	O
,	O
and	O
send	O
a	O
binary	O
string	O
whose	O
interval	O
lies	O
within	O
that	O
interval	O
.	O
this	O
encoding	O
can	O
be	O
performed	O
on	O
the	O
(	O
cid:13	O
)	O
y	O
,	O
as	O
we	O
now	O
illustrate	O
.	O
example	O
:	O
compressing	O
the	O
tosses	O
of	O
a	O
bent	B
coin	I
imagine	O
that	O
we	O
watch	O
as	O
a	O
bent	B
coin	I
is	O
tossed	O
some	O
number	O
of	O
times	O
(	O
cf	O
.	O
example	O
2.7	O
(	O
p.30	O
)	O
and	O
section	O
3.2	O
(	O
p.51	O
)	O
)	O
.	O
the	O
two	O
outcomes	O
when	O
the	O
coin	B
is	O
tossed	O
are	O
denoted	O
a	O
and	O
b.	O
a	O
third	O
possibility	O
is	O
that	O
the	O
experiment	O
is	O
halted	O
,	O
an	O
event	O
denoted	O
by	O
the	O
‘	O
end	O
of	O
(	O
cid:12	O
)	O
le	O
’	O
symbol	O
,	O
‘	O
2	O
’	O
.	O
because	O
the	O
coin	B
is	O
bent	O
,	O
we	O
expect	O
that	O
the	O
probabilities	O
of	O
the	O
outcomes	O
a	O
and	O
b	O
are	O
not	O
equal	O
,	O
though	O
beforehand	O
we	O
don	O
’	O
t	O
know	O
which	O
is	O
the	O
more	O
probable	O
outcome	O
.	O
encoding	O
let	O
the	O
source	O
string	O
be	O
‘	O
bbba2	O
’	O
.	O
we	O
pass	O
along	O
the	O
string	O
one	O
symbol	O
at	O
a	O
time	O
and	O
use	O
our	O
model	B
to	O
compute	O
the	O
probability	B
distribution	O
of	O
the	O
next	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
114	O
6	O
|	O
stream	B
codes	I
symbol	O
given	O
the	O
string	O
thus	O
far	O
.	O
let	O
these	O
probabilities	O
be	O
:	O
context	O
(	O
sequence	B
thus	O
far	O
)	O
probability	O
of	O
next	O
symbol	O
b	O
bb	O
bbb	O
bbba	O
p	O
(	O
a	O
)	O
=	O
0:425	O
p	O
(	O
aj	O
b	O
)	O
=	O
0:28	O
p	O
(	O
aj	O
bb	O
)	O
=	O
0:21	O
p	O
(	O
aj	O
bbb	O
)	O
=	O
0:17	O
p	O
(	O
aj	O
bbba	O
)	O
=	O
0:28	O
p	O
(	O
b	O
)	O
=	O
0:425	O
p	O
(	O
bj	O
b	O
)	O
=	O
0:57	O
p	O
(	O
bj	O
bb	O
)	O
=	O
0:64	O
p	O
(	O
bj	O
bbb	O
)	O
=	O
0:68	O
p	O
(	O
bj	O
bbba	O
)	O
=	O
0:57	O
p	O
(	O
2	O
)	O
=	O
0:15	O
p	O
(	O
2j	O
b	O
)	O
=	O
0:15	O
p	O
(	O
2j	O
bb	O
)	O
=	O
0:15	O
p	O
(	O
2j	O
bbb	O
)	O
=	O
0:15	O
p	O
(	O
2j	O
bbba	O
)	O
=	O
0:15	O
figure	O
6.4	O
shows	O
the	O
corresponding	O
intervals	B
.	O
the	O
interval	O
b	O
is	O
the	O
middle	O
0.425	O
of	O
[	O
0	O
;	O
1	O
)	O
.	O
the	O
interval	O
bb	O
is	O
the	O
middle	O
0.567	O
of	O
b	O
,	O
and	O
so	O
forth	O
.	O
00000	O
00001	O
00010	O
00011	O
00100	O
00101	O
00110	O
00111	O
01000	O
01001	O
01010	O
01011	O
01100	O
01101	O
01110	O
01111	O
10000	O
10001	O
10010	O
10011	O
10100	O
10101	O
10110	O
10111	O
11000	O
11001	O
11010	O
11011	O
11100	O
11101	O
11110	O
11111	O
0000	O
000	O
0001	O
0010	O
001	O
0011	O
0100	O
010	O
0101	O
0110	O
011	O
0111	O
1000	O
100	O
1001	O
1010	O
101	O
1011	O
1100	O
110	O
1101	O
1110	O
111	O
1111	O
00	O
01	O
10	O
11	O
0	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
b	O
b	O
(	O
cid:2	O
)	O
(	O
cid:2	O
)	O
b	O
b	O
1	O
a	O
b	O
2	O
ba	O
bba	O
bbba	O
bb	O
bbb	O
bbbb	O
bbb2	O
bb2	O
b2	O
figure	O
6.4.	O
illustration	O
of	O
the	O
arithmetic	O
coding	O
process	O
as	O
the	O
sequence	B
bbba2	O
is	O
transmitted	O
.	O
bbbaa	O
bbba	O
bbbab	O
bbba2	O
10010111	O
10011000	O
10011001	O
10011010	O
10011011	O
10011100	O
10011101	O
10011110	O
10011111	O
c	O
10100000	O
c	O
100111101	O
cco	O
10011	O
when	O
the	O
(	O
cid:12	O
)	O
rst	O
symbol	O
‘	O
b	O
’	O
is	O
observed	O
,	O
the	O
encoder	B
knows	O
that	O
the	O
encoded	O
string	O
will	O
start	O
‘	O
01	O
’	O
,	O
‘	O
10	O
’	O
,	O
or	O
‘	O
11	O
’	O
,	O
but	O
does	O
not	O
know	O
which	O
.	O
the	O
encoder	B
writes	O
nothing	O
for	O
the	O
time	O
being	O
,	O
and	O
examines	O
the	O
next	O
symbol	O
,	O
which	O
is	O
‘	O
b	O
’	O
.	O
the	O
interval	O
‘	O
bb	O
’	O
lies	O
wholly	O
within	O
interval	O
‘	O
1	O
’	O
,	O
so	O
the	O
encoder	B
can	O
write	O
the	O
(	O
cid:12	O
)	O
rst	O
bit	B
:	O
‘	O
1	O
’	O
.	O
the	O
third	O
symbol	O
‘	O
b	O
’	O
narrows	O
down	O
the	O
interval	O
a	O
little	O
,	O
but	O
not	O
quite	O
enough	O
for	O
it	O
to	O
lie	O
wholly	O
within	O
interval	O
‘	O
10	O
’	O
.	O
only	O
when	O
the	O
next	O
‘	O
a	O
’	O
is	O
read	O
from	O
the	O
source	O
can	O
we	O
transmit	O
some	O
more	O
bits	O
.	O
interval	O
‘	O
bbba	O
’	O
lies	O
wholly	O
within	O
the	O
interval	O
‘	O
1001	O
’	O
,	O
so	O
the	O
encoder	B
adds	O
‘	O
001	O
’	O
to	O
the	O
‘	O
1	O
’	O
it	O
has	O
written	O
.	O
finally	O
when	O
the	O
‘	O
2	O
’	O
arrives	O
,	O
we	O
need	O
a	O
procedure	O
for	O
terminating	O
the	O
encoding	O
.	O
magnifying	O
the	O
interval	O
‘	O
bbba2	O
’	O
(	O
(	O
cid:12	O
)	O
gure	O
6.4	O
,	O
right	O
)	O
we	O
note	O
that	O
the	O
marked	O
interval	O
‘	O
100111101	O
’	O
is	O
wholly	O
contained	O
by	O
bbba2	O
,	O
so	O
the	O
encoding	O
can	O
be	O
completed	O
by	O
appending	O
‘	O
11101	O
’	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
6.2	O
:	O
arithmetic	O
codes	O
115	O
exercise	O
6.1	O
.	O
[	O
2	O
,	O
p.127	O
]	O
show	O
that	O
the	O
overhead	O
required	O
to	O
terminate	O
a	O
message	O
is	O
never	O
more	O
than	O
2	O
bits	O
,	O
relative	B
to	O
the	O
ideal	O
message	O
length	O
given	O
the	O
probabilistic	B
model	I
h	O
,	O
h	O
(	O
xjh	O
)	O
=	O
log	O
[	O
1=p	O
(	O
xjh	O
)	O
]	O
.	O
this	O
is	O
an	O
important	O
result	O
.	O
arithmetic	B
coding	I
is	O
very	O
nearly	O
optimal	B
.	O
the	O
message	O
length	O
is	O
always	O
within	O
two	O
bits	O
of	O
the	O
shannon	O
information	O
content	B
of	O
the	O
entire	O
source	O
string	O
,	O
so	O
the	O
expected	O
message	O
length	O
is	O
within	O
two	O
bits	O
of	O
the	O
entropy	B
of	O
the	O
entire	O
message	O
.	O
decoding	B
the	O
decoder	B
receives	O
the	O
string	O
‘	O
100111101	O
’	O
and	O
passes	O
along	O
it	O
one	O
symbol	O
at	O
a	O
time	O
.	O
first	O
,	O
the	O
probabilities	O
p	O
(	O
a	O
)	O
;	O
p	O
(	O
b	O
)	O
;	O
p	O
(	O
2	O
)	O
are	O
computed	O
using	O
the	O
identical	O
program	O
that	O
the	O
encoder	B
used	O
and	O
the	O
intervals	B
‘	O
a	O
’	O
,	O
‘	O
b	O
’	O
and	O
‘	O
2	O
’	O
are	O
deduced	O
.	O
once	O
the	O
(	O
cid:12	O
)	O
rst	O
two	O
bits	O
‘	O
10	O
’	O
have	O
been	O
examined	O
,	O
it	O
is	O
certain	O
that	O
the	O
original	O
string	O
must	O
have	O
been	O
started	O
with	O
a	O
‘	O
b	O
’	O
,	O
since	O
the	O
interval	O
‘	O
10	O
’	O
lies	O
wholly	O
within	O
interval	O
‘	O
b	O
’	O
.	O
the	O
decoder	B
can	O
then	O
use	O
the	O
model	B
to	O
compute	O
p	O
(	O
aj	O
b	O
)	O
;	O
p	O
(	O
bj	O
b	O
)	O
;	O
p	O
(	O
2j	O
b	O
)	O
and	O
deduce	O
the	O
boundaries	O
of	O
the	O
intervals	O
‘	O
ba	O
’	O
,	O
‘	O
bb	O
’	O
and	O
‘	O
b2	O
’	O
.	O
continuing	O
,	O
we	O
decode	O
the	O
second	O
b	O
once	O
we	O
reach	O
‘	O
1001	O
’	O
,	O
the	O
third	O
b	O
once	O
we	O
reach	O
‘	O
100111	O
’	O
,	O
and	O
so	O
forth	O
,	O
with	O
the	O
unambiguous	O
identi	O
(	O
cid:12	O
)	O
cation	O
of	O
‘	O
bbba2	O
’	O
once	O
the	O
whole	O
binary	O
string	O
has	O
been	O
read	O
.	O
with	O
the	O
convention	O
that	O
‘	O
2	O
’	O
denotes	O
the	O
end	O
of	O
the	O
message	O
,	O
the	O
decoder	B
knows	O
to	O
stop	O
decoding	B
.	O
transmission	O
of	O
multiple	O
(	O
cid:12	O
)	O
les	O
how	O
might	O
one	O
use	O
arithmetic	B
coding	I
to	O
communicate	O
several	O
distinct	O
(	O
cid:12	O
)	O
les	O
over	O
the	O
binary	O
channel	O
?	O
once	O
the	O
2	O
character	O
has	O
been	O
transmitted	O
,	O
we	O
imagine	O
that	O
the	O
decoder	B
is	O
reset	O
into	O
its	O
initial	O
state	O
.	O
there	O
is	O
no	O
transfer	O
of	O
the	O
learnt	O
statistics	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
(	O
cid:12	O
)	O
le	O
to	O
the	O
second	O
(	O
cid:12	O
)	O
le	O
.	O
if	O
,	O
however	O
,	O
we	O
did	O
believe	O
that	O
there	O
is	O
a	O
relationship	O
among	O
the	O
(	O
cid:12	O
)	O
les	O
that	O
we	O
are	O
going	O
to	O
compress	B
,	O
we	O
could	O
de	O
(	O
cid:12	O
)	O
ne	O
our	O
alphabet	O
di	O
(	O
cid:11	O
)	O
erently	O
,	O
introducing	O
a	O
second	O
end-of-	O
(	O
cid:12	O
)	O
le	O
character	O
that	O
marks	O
the	O
end	O
of	O
the	O
(	O
cid:12	O
)	O
le	O
but	O
instructs	O
the	O
encoder	B
and	O
decoder	B
to	O
continue	O
using	O
the	O
same	O
probabilistic	B
model	I
.	O
the	O
big	O
picture	O
notice	O
that	O
to	O
communicate	O
a	O
string	O
of	O
n	O
letters	O
both	O
the	O
encoder	B
and	O
the	O
decoder	B
needed	O
to	O
compute	O
only	O
njaj	O
conditional	B
probabilities	O
{	O
the	O
proba-	O
bilities	O
of	O
each	O
possible	O
letter	O
in	O
each	O
context	O
actually	O
encountered	O
{	O
just	O
as	O
in	O
the	O
guessing	B
game	I
.	O
this	O
cost	O
can	O
be	O
contrasted	O
with	O
the	O
alternative	O
of	O
using	O
a	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
with	O
a	O
large	O
block	B
size	O
(	O
in	O
order	O
to	O
reduce	O
the	O
possible	O
one-	O
bit-per-symbol	O
overhead	O
discussed	O
in	O
section	O
5.6	O
)	O
,	O
where	O
all	O
block	B
sequences	O
that	O
could	O
occur	O
must	O
be	O
considered	O
and	O
their	O
probabilities	O
evaluated	O
.	O
notice	O
how	O
(	O
cid:13	O
)	O
exible	O
arithmetic	B
coding	I
is	O
:	O
it	O
can	O
be	O
used	O
with	O
any	O
source	O
alphabet	O
and	O
any	O
encoded	O
alphabet	O
.	O
the	O
size	O
of	O
the	O
source	O
alphabet	O
and	O
the	O
encoded	O
alphabet	O
can	O
change	O
with	O
time	O
.	O
arithmetic	B
coding	I
can	O
be	O
used	O
with	O
any	O
probability	B
distribution	O
,	O
which	O
can	O
change	O
utterly	O
from	O
context	O
to	O
context	O
.	O
furthermore	O
,	O
if	O
we	O
would	O
like	O
the	O
symbols	O
of	O
the	O
encoding	O
alphabet	O
(	O
say	O
,	O
0	O
and	O
1	O
)	O
to	O
be	O
used	O
with	O
unequal	O
frequency	B
,	O
that	O
can	O
easily	O
be	O
arranged	O
by	O
subdividing	O
the	O
right-hand	O
interval	O
in	O
proportion	O
to	O
the	O
required	O
frequencies	O
.	O
how	O
the	O
probabilistic	B
model	I
might	O
make	O
its	O
predictions	O
the	O
technique	O
of	O
arithmetic	O
coding	O
does	O
not	O
force	O
one	O
to	O
produce	O
the	O
predic-	O
tive	O
probability	B
in	O
any	O
particular	O
way	O
,	O
but	O
the	O
predictive	O
distributions	O
might	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
6	O
|	O
stream	B
codes	I
figure	O
6.5.	O
illustration	O
of	O
the	O
intervals	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
a	O
simple	O
bayesian	O
probabilistic	B
model	I
.	O
the	O
size	O
of	O
an	O
intervals	B
is	O
proportional	O
to	O
the	O
probability	O
of	O
the	O
string	O
.	O
this	O
model	B
anticipates	O
that	O
the	O
source	O
is	O
likely	O
to	O
be	O
biased	O
towards	O
one	O
of	O
a	O
and	O
b	O
,	O
so	O
sequences	O
having	O
lots	O
of	O
as	O
or	O
lots	O
of	O
bs	O
have	O
larger	O
intervals	B
than	O
sequences	O
of	O
the	O
same	O
length	B
that	O
are	O
50:50	O
as	O
and	O
bs	O
.	O
116	O
aa	O
ab	O
a2	O
ba	O
aaa	O
aab	O
aa2	O
aba	O
abb	O
ab2	O
baa	O
bab	O
ba2	O
bba	O
aaaa	O
aaab	O
aaba	O
aabb	O
abaa	O
abab	O
abba	O
abbb	O
baaa	O
baab	O
baba	O
babb	O
bbaa	O
bbab	O
bbba	O
bb	O
bbb	O
bbbb	O
bb2	O
b2	O
a	O
b	O
2	O
00000	O
00001	O
00010	O
00011	O
00100	O
00101	O
00110	O
00111	O
01000	O
01001	O
01010	O
01011	O
01100	O
01101	O
01110	O
01111	O
10000	O
10001	O
10010	O
10011	O
10100	O
10101	O
10110	O
10111	O
11000	O
11001	O
11010	O
11011	O
11100	O
11101	O
11110	O
11111	O
0000	O
0001	O
0010	O
0011	O
0100	O
0101	O
0110	O
0111	O
1000	O
1001	O
1010	O
1011	O
1100	O
1101	O
1110	O
1111	O
000	O
001	O
010	O
011	O
100	O
101	O
110	O
111	O
00	O
01	O
10	O
11	O
0	O
1	O
naturally	O
be	O
produced	O
by	O
a	O
bayesian	O
model	B
.	O
figure	O
6.4	O
was	O
generated	O
using	O
a	O
simple	O
model	O
that	O
always	O
assigns	O
a	O
prob-	O
ability	O
of	O
0.15	O
to	O
2	O
,	O
and	O
assigns	O
the	O
remaining	O
0.85	O
to	O
a	O
and	O
b	O
,	O
divided	O
in	O
proportion	O
to	O
probabilities	O
given	O
by	O
laplace	O
’	O
s	O
rule	O
,	O
pl	O
(	O
aj	O
x1	O
;	O
:	O
:	O
:	O
;	O
xn	O
(	O
cid:0	O
)	O
1	O
)	O
=	O
fa	O
+	O
1	O
fa	O
+	O
fb	O
+	O
2	O
;	O
(	O
6.7	O
)	O
where	O
fa	O
(	O
x1	O
;	O
:	O
:	O
:	O
;	O
xn	O
(	O
cid:0	O
)	O
1	O
)	O
is	O
the	O
number	O
of	O
times	O
that	O
a	O
has	O
occurred	O
so	O
far	O
,	O
and	O
fb	O
is	O
the	O
count	O
of	O
bs	O
.	O
these	O
predictions	O
correspond	O
to	O
a	O
simple	O
bayesian	O
model	B
that	O
expects	O
and	O
adapts	O
to	O
a	O
non-equal	O
frequency	B
of	O
use	O
of	O
the	O
source	O
symbols	O
a	O
and	O
b	O
within	O
a	O
(	O
cid:12	O
)	O
le	O
.	O
figure	O
6.5	O
displays	O
the	O
intervals	B
corresponding	O
to	O
a	O
number	O
of	O
strings	O
of	O
length	O
up	O
to	O
(	O
cid:12	O
)	O
ve	O
.	O
note	O
that	O
if	O
the	O
string	O
so	O
far	O
has	O
contained	O
a	O
large	O
number	O
of	O
bs	O
then	O
the	O
probability	O
of	O
b	O
relative	B
to	O
a	O
is	O
increased	O
,	O
and	O
conversely	O
if	O
many	O
as	O
occur	O
then	O
as	O
are	O
made	O
more	O
probable	O
.	O
larger	O
intervals	B
,	O
remember	O
,	O
require	O
fewer	O
bits	O
to	O
encode	O
.	O
details	O
of	O
the	O
bayesian	O
model	B
having	O
emphasized	O
that	O
any	O
model	B
could	O
be	O
used	O
{	O
arithmetic	B
coding	I
is	O
not	O
wedded	O
to	O
any	O
particular	O
set	B
of	O
probabilities	O
{	O
let	O
me	O
explain	O
the	O
simple	O
adaptive	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
6.2	O
:	O
arithmetic	O
codes	O
117	O
probabilistic	B
model	I
used	O
in	O
the	O
preceding	O
example	O
;	O
we	O
(	O
cid:12	O
)	O
rst	O
encountered	O
this	O
model	B
in	O
exercise	O
2.8	O
(	O
p.30	O
)	O
.	O
assumptions	B
the	O
model	B
will	O
be	O
described	O
using	O
parameters	B
p2	O
,	O
pa	O
and	O
pb	O
,	O
de	O
(	O
cid:12	O
)	O
ned	O
below	O
,	O
which	O
should	O
not	O
be	O
confused	O
with	O
the	O
predictive	O
probabilities	O
in	O
a	O
particular	O
context	O
,	O
for	O
example	O
,	O
p	O
(	O
aj	O
s	O
=	O
baa	O
)	O
.	O
a	O
bent	B
coin	I
labelled	O
a	O
and	O
b	O
is	O
tossed	O
some	O
number	O
of	O
times	O
l	O
,	O
which	O
we	O
don	O
’	O
t	O
know	O
beforehand	O
.	O
the	O
coin	B
’	O
s	O
probability	O
of	O
coming	O
up	O
a	O
when	O
tossed	O
is	O
pa	O
,	O
and	O
pb	O
=	O
1	O
(	O
cid:0	O
)	O
pa	O
;	O
the	O
parameters	B
pa	O
;	O
pb	O
are	O
not	O
known	O
beforehand	O
.	O
the	O
source	O
string	O
s	O
=	O
baaba2	O
indicates	O
that	O
l	O
was	O
5	O
and	O
the	O
sequence	B
of	O
outcomes	O
was	O
baaba	O
.	O
1.	O
it	O
is	O
assumed	O
that	O
the	O
length	B
of	O
the	O
string	O
l	O
has	O
an	O
exponential	B
probability	O
distribution	B
p	O
(	O
l	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
p2	O
)	O
lp2	O
:	O
(	O
6.8	O
)	O
this	O
distribution	B
corresponds	O
to	O
assuming	O
a	O
constant	O
probability	B
p2	O
for	O
the	O
termination	B
symbol	O
‘	O
2	O
’	O
at	O
each	O
character	O
.	O
2.	O
it	O
is	O
assumed	O
that	O
the	O
non-terminal	O
characters	O
in	O
the	O
string	O
are	O
selected	O
in-	O
dependently	O
at	O
random	B
from	O
an	O
ensemble	B
with	O
probabilities	O
p	O
=	O
fpa	O
;	O
pbg	O
;	O
the	O
probability	B
pa	O
is	O
(	O
cid:12	O
)	O
xed	O
throughout	O
the	O
string	O
to	O
some	O
unknown	O
value	O
that	O
could	O
be	O
anywhere	O
between	O
0	O
and	O
1.	O
the	O
probability	O
of	O
an	O
a	O
occur-	O
ring	O
as	O
the	O
next	O
symbol	O
,	O
given	O
pa	O
(	O
if	O
only	O
we	O
knew	O
it	O
)	O
,	O
is	O
(	O
1	O
(	O
cid:0	O
)	O
p2	O
)	O
pa.	O
the	O
probability	B
,	O
given	O
pa	O
,	O
that	O
an	O
unterminated	O
string	O
of	O
length	O
f	O
is	O
a	O
given	O
string	O
s	O
that	O
contains	O
ffa	O
;	O
fbg	O
counts	O
of	O
the	O
two	O
outcomes	O
is	O
the	O
bernoulli	O
distribution	B
(	O
6.9	O
)	O
p	O
(	O
sj	O
pa	O
;	O
f	O
)	O
=	O
pfa	O
a	O
(	O
1	O
(	O
cid:0	O
)	O
pa	O
)	O
fb	O
:	O
3.	O
we	O
assume	O
a	O
uniform	O
prior	B
distribution	O
for	O
pa	O
,	O
p	O
(	O
pa	O
)	O
=	O
1	O
;	O
pa	O
2	O
[	O
0	O
;	O
1	O
]	O
;	O
(	O
6.10	O
)	O
and	O
de	O
(	O
cid:12	O
)	O
ne	O
pb	O
(	O
cid:17	O
)	O
1	O
(	O
cid:0	O
)	O
pa.	O
it	O
would	O
be	O
easy	O
to	O
assume	O
other	O
priors	O
on	O
pa	O
,	O
with	O
beta	O
distributions	O
being	O
the	O
most	O
convenient	O
to	O
handle	O
.	O
this	O
model	B
was	O
studied	O
in	O
section	O
3.2.	O
the	O
key	O
result	O
we	O
require	O
is	O
the	O
predictive	B
distribution	I
for	O
the	O
next	O
symbol	O
,	O
given	O
the	O
string	O
so	O
far	O
,	O
s.	O
this	O
probability	B
that	O
the	O
next	O
character	O
is	O
a	O
or	O
b	O
(	O
assuming	O
that	O
it	O
is	O
not	O
‘	O
2	O
’	O
)	O
was	O
derived	O
in	O
equation	O
(	O
3.16	O
)	O
and	O
is	O
precisely	O
laplace	O
’	O
s	O
rule	O
(	O
6.7	O
)	O
.	O
.	O
exercise	O
6.2	O
.	O
[	O
3	O
]	O
compare	O
the	O
expected	O
message	O
length	O
when	O
an	O
ascii	O
(	O
cid:12	O
)	O
le	O
is	O
compressed	O
by	O
the	O
following	O
three	O
methods	O
.	O
hu	O
(	O
cid:11	O
)	O
man-with-header	O
.	O
read	O
the	O
whole	O
(	O
cid:12	O
)	O
le	O
,	O
(	O
cid:12	O
)	O
nd	O
the	O
empirical	O
fre-	O
quency	O
of	O
each	O
symbol	O
,	O
construct	O
a	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
for	O
those	O
frequen-	O
cies	O
,	O
transmit	O
the	O
code	B
by	O
transmitting	O
the	O
lengths	O
of	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
codewords	O
,	O
then	O
transmit	O
the	O
(	O
cid:12	O
)	O
le	O
using	O
the	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
.	O
(	O
the	O
actual	O
codewords	O
don	O
’	O
t	O
need	O
to	O
be	O
transmitted	O
,	O
since	O
we	O
can	O
use	O
a	O
deterministic	B
method	O
for	O
building	O
the	O
tree	B
given	O
the	O
codelengths	O
.	O
)	O
arithmetic	O
code	O
using	O
the	O
laplace	O
model	B
.	O
pl	O
(	O
aj	O
x1	O
;	O
:	O
:	O
:	O
;	O
xn	O
(	O
cid:0	O
)	O
1	O
)	O
=	O
:	O
(	O
6.11	O
)	O
fa	O
+	O
1	O
pa0	O
(	O
fa0	O
+	O
1	O
)	O
arithmetic	O
code	O
using	O
a	O
dirichlet	O
model	B
.	O
this	O
model	B
’	O
s	O
predic-	O
tions	O
are	O
:	O
pd	O
(	O
aj	O
x1	O
;	O
:	O
:	O
:	O
;	O
xn	O
(	O
cid:0	O
)	O
1	O
)	O
=	O
;	O
fa	O
+	O
(	O
cid:11	O
)	O
pa0	O
(	O
fa0	O
+	O
(	O
cid:11	O
)	O
)	O
(	O
6.12	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
118	O
6	O
|	O
stream	B
codes	I
where	O
(	O
cid:11	O
)	O
is	O
(	O
cid:12	O
)	O
xed	O
to	O
a	O
number	O
such	O
as	O
0.01.	O
a	O
small	O
value	O
of	O
(	O
cid:11	O
)	O
corresponds	O
to	O
a	O
more	O
responsive	O
version	O
of	O
the	O
laplace	O
model	B
;	O
the	O
probability	B
over	O
characters	O
is	O
expected	O
to	O
be	O
more	O
nonuniform	O
;	O
(	O
cid:11	O
)	O
=	O
1	O
reproduces	O
the	O
laplace	O
model	B
.	O
take	O
care	O
that	O
the	O
header	O
of	O
your	O
hu	O
(	O
cid:11	O
)	O
man	O
message	O
is	O
self-delimiting	B
.	O
special	O
cases	O
worth	O
considering	O
are	O
(	O
a	O
)	O
short	O
(	O
cid:12	O
)	O
les	O
with	O
just	O
a	O
few	O
hundred	O
characters	O
;	O
(	O
b	O
)	O
large	O
(	O
cid:12	O
)	O
les	O
in	O
which	O
some	O
characters	O
are	O
never	O
used	O
.	O
6.3	O
further	O
applications	O
of	O
arithmetic	O
coding	O
e	O
(	O
cid:14	O
)	O
cient	O
generation	O
of	O
random	O
samples	O
arithmetic	B
coding	I
not	O
only	O
o	O
(	O
cid:11	O
)	O
ers	O
a	O
way	O
to	O
compress	B
strings	O
believed	O
to	O
come	O
from	O
a	O
given	O
model	B
;	O
it	O
also	O
o	O
(	O
cid:11	O
)	O
ers	O
a	O
way	O
to	O
generate	O
random	B
strings	O
from	O
a	O
model	B
.	O
imagine	O
sticking	O
a	O
pin	O
into	O
the	O
unit	O
interval	O
at	O
random	B
,	O
that	O
line	O
having	O
been	O
divided	O
into	O
subintervals	O
in	O
proportion	O
to	O
probabilities	O
pi	O
;	O
the	O
probability	B
that	O
your	O
pin	O
will	O
lie	O
in	O
interval	O
i	O
is	O
pi	O
.	O
so	O
to	O
generate	O
a	O
sample	B
from	I
a	O
model	B
,	O
all	O
we	O
need	O
to	O
do	O
is	O
feed	O
ordinary	O
random	B
bits	O
into	O
an	O
arithmetic	O
decoder	O
for	O
that	O
model	B
.	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
random	B
bit	O
sequence	B
corresponds	O
to	O
the	O
selection	O
of	O
a	O
point	O
at	O
random	B
from	O
the	O
line	O
[	O
0	O
;	O
1	O
)	O
,	O
so	O
the	O
decoder	B
will	O
then	O
select	O
a	O
string	O
at	O
random	B
from	O
the	O
assumed	O
distribution	B
.	O
this	O
arithmetic	O
method	O
is	O
guaranteed	O
to	O
use	O
very	O
nearly	O
the	O
smallest	O
number	O
of	O
random	O
bits	O
possible	O
to	O
make	O
the	O
selection	O
{	O
an	O
important	O
point	O
in	O
communities	O
where	O
random	B
numbers	O
are	O
expensive	O
!	O
[	O
this	O
is	O
not	O
a	O
joke	O
.	O
large	O
amounts	O
of	O
money	O
are	O
spent	O
on	O
generating	O
random	B
bits	O
in	O
software	O
and	O
hardware	O
.	O
random	B
numbers	O
are	O
valuable	O
.	O
]	O
a	O
simple	O
example	O
of	O
the	O
use	O
of	O
this	O
technique	O
is	O
in	O
the	O
generation	O
of	O
random	O
bits	O
with	O
a	O
nonuniform	O
distribution	B
fp0	O
;	O
p1g	O
.	O
exercise	O
6.3	O
.	O
[	O
2	O
,	O
p.128	O
]	O
compare	O
the	O
following	O
two	O
techniques	O
for	O
generating	O
random	B
symbols	O
from	O
a	O
nonuniform	O
distribution	B
fp0	O
;	O
p1g	O
=	O
f0:99	O
;	O
0:01g	O
:	O
(	O
a	O
)	O
the	O
standard	O
method	O
:	O
use	O
a	O
standard	O
random	O
number	O
generator	O
to	O
generate	O
an	O
integer	O
between	O
1	O
and	O
232.	O
rescale	O
the	O
integer	O
to	O
(	O
0	O
;	O
1	O
)	O
.	O
test	B
whether	O
this	O
uniformly	O
distributed	O
random	B
variable	I
is	O
less	O
than	O
0:99	O
,	O
and	O
emit	O
a	O
0	O
or	O
1	O
accordingly	O
.	O
(	O
b	O
)	O
arithmetic	B
coding	I
using	O
the	O
correct	O
model	B
,	O
fed	O
with	O
standard	O
ran-	O
dom	O
bits	O
.	O
roughly	O
how	O
many	O
random	B
bits	O
will	O
each	O
method	B
use	O
to	O
generate	O
a	O
thousand	O
samples	O
from	O
this	O
sparse	O
distribution	O
?	O
e	O
(	O
cid:14	O
)	O
cient	O
data-entry	O
devices	O
when	O
we	O
enter	O
text	O
into	O
a	O
computer	B
,	O
we	O
make	O
gestures	O
of	O
some	O
sort	O
{	O
maybe	O
we	O
tap	B
a	O
keyboard	B
,	O
or	O
scribble	O
with	O
a	O
pointer	B
,	O
or	O
click	O
with	O
a	O
mouse	O
;	O
an	O
e	O
(	O
cid:14	O
)	O
cient	O
text	B
entry	I
system	O
is	O
one	O
where	O
the	O
number	O
of	O
gestures	O
required	O
to	O
enter	O
a	O
given	O
text	O
string	O
is	O
small	O
.	O
writing	B
can	O
be	O
viewed	O
as	O
an	O
inverse	O
process	O
to	O
data	B
compression	I
.	O
in	O
data	O
compression	B
,	O
the	O
aim	O
is	O
to	O
map	O
a	O
given	O
text	O
string	O
into	O
a	O
small	O
number	O
of	O
bits	O
.	O
in	O
text	O
entry	O
,	O
we	O
want	O
a	O
small	O
sequence	B
of	O
gestures	O
to	O
produce	O
our	O
intended	O
text	O
.	O
by	O
inverting	O
an	O
arithmetic	O
coder	O
,	O
we	O
can	O
obtain	O
an	O
information-e	O
(	O
cid:14	O
)	O
cient	O
text	B
entry	I
device	O
that	O
is	O
driven	O
by	O
continuous	O
pointing	O
gestures	O
(	O
ward	O
et	O
al.	O
,	O
compression	B
:	O
text	O
!	O
bits	O
writing	O
:	O
text	O
gestures	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
6.4	O
:	O
lempel	O
{	O
ziv	O
coding	O
119	O
2000	O
)	O
.	O
in	O
this	O
system	O
,	O
called	O
dasher	O
,	O
the	O
user	O
zooms	O
in	O
on	O
the	O
unit	O
interval	O
to	O
locate	O
the	O
interval	O
corresponding	O
to	O
their	O
intended	O
string	O
,	O
in	O
the	O
same	O
style	O
as	O
(	O
cid:12	O
)	O
gure	O
6.4.	O
a	O
language	B
model	I
(	O
exactly	O
as	O
used	O
in	O
text	O
compression	B
)	O
controls	O
the	O
sizes	O
of	O
the	O
intervals	O
such	O
that	O
probable	O
strings	O
are	O
quick	O
and	O
easy	O
to	O
identify	O
.	O
after	O
an	O
hour	O
’	O
s	O
practice	O
,	O
a	O
novice	O
user	O
can	O
write	O
with	O
one	O
(	O
cid:12	O
)	O
nger	O
driving	O
dasher	O
at	O
about	O
25	O
words	O
per	O
minute	O
{	O
that	O
’	O
s	O
about	O
half	O
their	O
normal	B
ten-	O
(	O
cid:12	O
)	O
nger	O
typing	O
speed	O
on	O
a	O
regular	B
keyboard	O
.	O
it	O
’	O
s	O
even	O
possible	O
to	O
write	O
at	O
25	O
words	O
per	O
minute	O
,	O
hands-free	O
,	O
using	O
gaze	O
direction	O
to	O
drive	O
dasher	O
(	O
ward	O
and	O
mackay	O
,	O
2002	O
)	O
.	O
dasher	O
is	O
available	O
as	O
free	O
software	B
for	O
various	O
platforms.1	O
6.4	O
lempel	O
{	O
ziv	O
coding	O
the	O
lempel	O
{	O
ziv	O
algorithms	B
,	O
which	O
are	O
widely	O
used	O
for	O
data	O
compression	B
(	O
e.g.	O
,	O
the	O
compress	B
and	O
gzip	B
commands	O
)	O
,	O
are	O
di	O
(	O
cid:11	O
)	O
erent	O
in	O
philosophy	O
to	O
arithmetic	B
coding	I
.	O
there	O
is	O
no	O
separation	B
between	O
modelling	B
and	O
coding	O
,	O
and	O
no	O
oppor-	O
tunity	O
for	O
explicit	O
modelling	B
.	O
basic	O
lempel	O
{	O
ziv	O
algorithm	B
the	O
method	B
of	O
compression	B
is	O
to	O
replace	O
a	O
substring	B
with	O
a	O
pointer	B
to	O
an	O
earlier	O
occurrence	O
of	O
the	O
same	O
substring	B
.	O
for	O
example	O
if	O
the	O
string	O
is	O
1011010100010.	O
.	O
.	O
,	O
we	O
parse	B
it	O
into	O
an	O
ordered	O
dictionary	O
of	O
substrings	O
that	O
have	O
not	O
appeared	O
before	O
as	O
follows	O
:	O
(	O
cid:21	O
)	O
,	O
1	O
,	O
0	O
,	O
11	O
,	O
01	O
,	O
010	O
,	O
00	O
,	O
10	O
,	O
.	O
.	O
.	O
.	O
we	O
in-	O
clude	O
the	O
empty	O
substring	O
(	O
cid:21	O
)	O
as	O
the	O
(	O
cid:12	O
)	O
rst	O
substring	B
in	O
the	O
dictionary	B
and	O
order	O
the	O
substrings	O
in	O
the	O
dictionary	B
by	O
the	O
order	O
in	O
which	O
they	O
emerged	O
from	O
the	O
source	O
.	O
after	O
every	O
comma	O
,	O
we	O
look	O
along	O
the	O
next	O
part	O
of	O
the	O
input	O
sequence	B
until	O
we	O
have	O
read	O
a	O
substring	B
that	O
has	O
not	O
been	O
marked	O
o	O
(	O
cid:11	O
)	O
before	O
.	O
a	O
mo-	O
ment	O
’	O
s	O
re	O
(	O
cid:13	O
)	O
ection	O
will	O
con	O
(	O
cid:12	O
)	O
rm	O
that	O
this	O
substring	B
is	O
longer	O
by	O
one	O
bit	B
than	O
a	O
substring	B
that	O
has	O
occurred	O
earlier	O
in	O
the	O
dictionary	B
.	O
this	O
means	O
that	O
we	O
can	O
encode	O
each	O
substring	B
by	O
giving	O
a	O
pointer	B
to	O
the	O
earlier	O
occurrence	O
of	O
that	O
pre-	O
(	O
cid:12	O
)	O
x	O
and	O
then	O
sending	O
the	O
extra	B
bit	I
by	O
which	O
the	O
new	O
substring	B
in	O
the	O
dictionary	B
di	O
(	O
cid:11	O
)	O
ers	O
from	O
the	O
earlier	O
substring	B
.	O
if	O
,	O
at	O
the	O
nth	O
bit	B
,	O
we	O
have	O
enumerated	O
s	O
(	O
n	O
)	O
substrings	O
,	O
then	O
we	O
can	O
give	O
the	O
value	O
of	O
the	O
pointer	O
in	O
dlog	O
2	O
s	O
(	O
n	O
)	O
e	O
bits	O
.	O
the	O
code	B
for	O
the	O
above	O
sequence	B
is	O
then	O
as	O
shown	O
in	O
the	O
fourth	O
line	O
of	O
the	O
following	O
table	O
(	O
with	O
punctuation	O
included	O
for	O
clarity	O
)	O
,	O
the	O
upper	O
lines	O
indicating	O
the	O
source	O
string	O
and	O
the	O
value	O
of	O
s	O
(	O
n	O
)	O
:	O
source	O
substrings	O
(	O
cid:21	O
)	O
s	O
(	O
n	O
)	O
0	O
s	O
(	O
n	O
)	O
binary	O
000	O
001	O
(	O
;	O
1	O
)	O
(	O
pointer	B
;	O
bit	B
)	O
1	O
1	O
0	O
2	O
010	O
(	O
0	O
;	O
0	O
)	O
11	O
3	O
011	O
(	O
01	O
;	O
1	O
)	O
01	O
4	O
100	O
(	O
10	O
;	O
1	O
)	O
010	O
5	O
101	O
(	O
100	O
;	O
0	O
)	O
00	O
6	O
110	O
(	O
010	O
;	O
0	O
)	O
10	O
7	O
111	O
(	O
001	O
;	O
0	O
)	O
notice	O
that	O
the	O
(	O
cid:12	O
)	O
rst	O
pointer	B
we	O
send	O
is	O
empty	O
,	O
because	O
,	O
given	O
that	O
there	O
is	O
only	O
one	O
substring	B
in	O
the	O
dictionary	B
{	O
the	O
string	O
(	O
cid:21	O
)	O
{	O
no	O
bits	O
are	O
needed	O
to	O
convey	O
the	O
‘	O
choice	O
’	O
of	O
that	O
substring	B
as	O
the	O
pre	O
(	O
cid:12	O
)	O
x.	O
the	O
encoded	O
string	O
is	O
100011101100001000010.	O
the	O
encoding	O
,	O
in	O
this	O
simple	O
case	O
,	O
is	O
actually	O
a	O
longer	O
string	O
than	O
the	O
source	O
string	O
,	O
because	O
there	O
was	O
no	O
obvious	O
redundancy	B
in	O
the	O
source	O
string	O
.	O
.	O
exercise	O
6.4	O
.	O
[	O
2	O
]	O
prove	O
that	O
any	O
uniquely	B
decodeable	I
code	O
from	O
f0	O
;	O
1g+	O
to	O
f0	O
;	O
1g+	O
necessarily	O
makes	O
some	O
strings	O
longer	O
if	O
it	O
makes	O
some	O
strings	O
shorter	O
.	O
1http	O
:	O
//www.inference.phy.cam.ac.uk/dasher/	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
120	O
6	O
|	O
stream	B
codes	I
one	O
reason	O
why	O
the	O
algorithm	B
described	O
above	O
lengthens	O
a	O
lot	O
of	O
strings	O
is	O
because	O
it	O
is	O
ine	O
(	O
cid:14	O
)	O
cient	O
{	O
it	O
transmits	O
unnecessary	O
bits	O
;	O
to	O
put	O
it	O
another	O
way	O
,	O
its	O
code	B
is	O
not	O
complete	O
.	O
once	O
a	O
substring	B
in	O
the	O
dictionary	B
has	O
been	O
joined	O
there	O
by	O
both	O
of	O
its	O
children	O
,	O
then	O
we	O
can	O
be	O
sure	O
that	O
it	O
will	O
not	O
be	O
needed	O
(	O
except	O
possibly	O
as	O
part	O
of	O
our	O
protocol	B
for	O
terminating	O
a	O
message	O
)	O
;	O
so	O
at	O
that	O
point	O
we	O
could	O
drop	O
it	O
from	O
our	O
dictionary	B
of	O
substrings	O
and	O
shu	O
(	O
cid:15	O
)	O
e	O
them	O
all	O
along	O
one	O
,	O
thereby	O
reducing	O
the	O
length	B
of	O
subsequent	O
pointer	B
messages	O
.	O
equivalently	O
,	O
we	O
could	O
write	O
the	O
second	O
pre	O
(	O
cid:12	O
)	O
x	O
into	O
the	O
dictionary	B
at	O
the	O
point	O
previously	O
occupied	O
by	O
the	O
parent	B
.	O
a	O
second	O
unnecessary	O
overhead	O
is	O
the	O
transmission	O
of	O
the	O
new	O
bit	B
in	O
these	O
cases	O
{	O
the	O
second	O
time	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
is	O
used	O
,	O
we	O
can	O
be	O
sure	O
of	O
the	O
identity	O
of	O
the	O
next	O
bit	B
.	O
decoding	B
the	O
decoder	B
again	O
involves	O
an	O
identical	B
twin	I
at	O
the	O
decoding	B
end	O
who	O
con-	O
structs	O
the	O
dictionary	B
of	O
substrings	O
as	O
the	O
data	O
are	O
decoded	O
.	O
.	O
exercise	O
6.5	O
.	O
[	O
2	O
,	O
p.128	O
]	O
encode	O
the	O
string	O
000000000000100000000000	O
using	O
the	O
basic	O
lempel	O
{	O
ziv	O
algorithm	B
described	O
above	O
.	O
.	O
exercise	O
6.6	O
.	O
[	O
2	O
,	O
p.128	O
]	O
decode	O
the	O
string	O
00101011101100100100011010101000011	O
that	O
was	O
encoded	O
using	O
the	O
basic	O
lempel	O
{	O
ziv	O
algorithm	B
.	O
practicalities	O
in	O
this	O
description	O
i	O
have	O
not	O
discussed	O
the	O
method	B
for	O
terminating	O
a	O
string	O
.	O
there	O
are	O
many	O
variations	O
on	O
the	O
lempel	O
{	O
ziv	O
algorithm	B
,	O
all	O
exploiting	O
the	O
same	O
idea	O
but	O
using	O
di	O
(	O
cid:11	O
)	O
erent	O
procedures	O
for	O
dictionary	O
management	O
,	O
etc	O
.	O
the	O
resulting	O
programs	O
are	O
fast	O
,	O
but	O
their	O
performance	O
on	O
compression	O
of	O
english	O
text	O
,	O
although	O
useful	O
,	O
does	O
not	O
match	O
the	O
standards	O
set	B
in	O
the	O
arithmetic	B
coding	I
literature	O
.	O
theoretical	O
properties	O
in	O
contrast	O
to	O
the	O
block	B
code	I
,	O
hu	O
(	O
cid:11	O
)	O
man	O
code	B
,	O
and	O
arithmetic	O
coding	O
methods	O
we	O
discussed	O
in	O
the	O
last	O
three	O
chapters	O
,	O
the	O
lempel	O
{	O
ziv	O
algorithm	B
is	O
de	O
(	O
cid:12	O
)	O
ned	O
without	O
making	O
any	O
mention	O
of	O
a	O
probabilistic	B
model	I
for	O
the	O
source	O
.	O
yet	O
,	O
given	O
any	O
ergodic	B
source	O
(	O
i.e.	O
,	O
one	O
that	O
is	O
memoryless	O
on	O
su	O
(	O
cid:14	O
)	O
ciently	O
long	O
timescales	O
)	O
,	O
the	O
lempel	O
{	O
ziv	O
algorithm	B
can	O
be	O
proven	O
asymptotically	O
to	O
compress	B
down	O
to	O
the	O
entropy	B
of	O
the	O
source	O
.	O
this	O
is	O
why	O
it	O
is	O
called	O
a	O
‘	O
universal	B
’	O
compression	B
algorithm	O
.	O
for	O
a	O
proof	O
of	O
this	O
property	O
,	O
see	O
cover	O
and	O
thomas	O
(	O
1991	O
)	O
.	O
it	O
achieves	O
its	O
compression	B
,	O
however	O
,	O
only	O
by	O
memorizing	O
substrings	O
that	O
have	O
happened	O
so	O
that	O
it	O
has	O
a	O
short	O
name	O
for	O
them	O
the	O
next	O
time	O
they	O
occur	O
.	O
the	O
asymptotic	O
timescale	O
on	O
which	O
this	O
universal	B
performance	O
is	O
achieved	O
may	O
,	O
for	O
many	O
sources	O
,	O
be	O
unfeasibly	O
long	O
,	O
because	O
the	O
number	O
of	O
typical	O
substrings	O
that	O
need	O
memorizing	O
may	O
be	O
enormous	O
.	O
the	O
useful	O
performance	O
of	O
the	O
al-	O
gorithm	O
in	O
practice	O
is	O
a	O
re	O
(	O
cid:13	O
)	O
ection	O
of	O
the	O
fact	O
that	O
many	O
(	O
cid:12	O
)	O
les	O
contain	O
multiple	O
repetitions	O
of	O
particular	O
short	O
sequences	O
of	O
characters	O
,	O
a	O
form	O
of	O
redundancy	O
to	O
which	O
the	O
algorithm	B
is	O
well	O
suited	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
121	O
6.5	O
:	O
demonstration	O
common	O
ground	O
i	O
have	O
emphasized	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
in	O
philosophy	O
behind	O
arithmetic	B
coding	I
and	O
lempel	O
{	O
ziv	O
coding	O
.	O
there	O
is	O
common	O
ground	O
between	O
them	O
,	O
though	O
:	O
in	O
prin-	O
ciple	O
,	O
one	O
can	O
design	O
adaptive	O
probabilistic	O
models	O
,	O
and	O
thence	O
arithmetic	O
codes	O
,	O
that	O
are	O
‘	O
universal	B
’	O
,	O
that	O
is	O
,	O
models	O
that	O
will	O
asymptotically	O
compress	B
any	O
source	O
in	O
some	O
class	O
to	O
within	O
some	O
factor	O
(	O
preferably	O
1	O
)	O
of	O
its	O
entropy	B
.	O
however	O
,	O
for	O
practical	O
purposes	O
,	O
i	O
think	O
such	O
universal	B
models	O
can	O
only	O
be	O
constructed	O
if	O
the	O
class	O
of	O
sources	O
is	O
severely	O
restricted	O
.	O
a	O
general	O
purpose	O
compressor	O
that	O
can	O
discover	O
the	O
probability	B
distribution	O
of	O
any	O
source	O
would	O
be	O
a	O
general	O
purpose	O
arti	O
(	O
cid:12	O
)	O
cial	O
intelligence	O
!	O
a	O
general	O
purpose	O
arti	O
(	O
cid:12	O
)	O
cial	O
intelli-	O
gence	O
does	O
not	O
yet	O
exist	O
.	O
6.5	O
demonstration	O
an	O
interactive	O
aid	O
for	O
exploring	O
arithmetic	B
coding	I
,	O
dasher.tcl	O
,	O
is	O
available.2	O
a	O
demonstration	O
arithmetic-coding	O
software	B
package	O
written	O
by	O
radford	O
neal3	O
consists	O
of	O
encoding	O
and	O
decoding	O
modules	O
to	O
which	O
the	O
user	O
adds	O
a	O
module	O
de	O
(	O
cid:12	O
)	O
ning	O
the	O
probabilistic	B
model	I
.	O
it	O
should	O
be	O
emphasized	O
that	O
there	O
is	O
no	O
single	O
general-purpose	O
arithmetic-coding	O
compressor	O
;	O
a	O
new	O
model	B
has	O
to	O
be	O
written	O
for	O
each	O
type	O
of	O
source	O
.	O
radford	O
neal	O
’	O
s	O
package	O
includes	O
a	O
simple	O
adaptive	O
model	B
similar	O
to	O
the	O
bayesian	O
model	B
demonstrated	O
in	O
section	O
6.2.	O
the	O
results	O
using	O
this	O
laplace	O
model	B
should	O
be	O
viewed	O
as	O
a	O
basic	O
benchmark	O
since	O
it	O
is	O
the	O
simplest	O
possible	O
probabilistic	B
model	I
{	O
it	O
simply	O
assumes	O
the	O
characters	O
in	O
the	O
(	O
cid:12	O
)	O
le	O
come	O
independently	O
from	O
a	O
(	O
cid:12	O
)	O
xed	O
ensemble	B
.	O
the	O
counts	O
ffig	O
of	O
the	O
symbols	O
faig	O
are	O
rescaled	O
and	O
rounded	O
as	O
the	O
(	O
cid:12	O
)	O
le	O
is	O
read	O
such	O
that	O
all	O
the	O
counts	O
lie	O
between	O
1	O
and	O
256.	O
a	O
state-of-the-art	O
compressor	O
for	O
documents	O
containing	O
text	O
and	O
images	B
,	O
djvu	O
,	O
uses	O
arithmetic	O
coding.4	O
it	O
uses	O
a	O
carefully	O
designed	O
approximate	O
arith-	O
metic	O
coder	O
for	O
binary	O
alphabets	O
called	O
the	O
z-coder	O
(	O
bottou	O
et	O
al.	O
,	O
1998	O
)	O
,	O
which	O
is	O
much	O
faster	O
than	O
the	O
arithmetic	B
coding	I
software	O
described	O
above	O
.	O
one	O
of	O
the	O
neat	O
tricks	O
the	O
z-coder	O
uses	O
is	O
this	O
:	O
the	O
adaptive	B
model	O
adapts	O
only	O
occa-	O
sionally	O
(	O
to	O
save	O
on	O
computer	O
time	O
)	O
,	O
with	O
the	O
decision	O
about	O
when	O
to	O
adapt	O
being	O
pseudo-randomly	O
controlled	O
by	O
whether	O
the	O
arithmetic	O
encoder	O
emitted	O
a	O
bit	B
.	O
the	O
jbig	O
image	B
compression	I
standard	O
for	O
binary	O
images	B
uses	O
arithmetic	B
coding	I
with	O
a	O
context-dependent	O
model	B
,	O
which	O
adapts	O
using	O
a	O
rule	O
similar	O
to	O
laplace	O
’	O
s	O
rule	O
.	O
ppm	O
(	O
teahan	O
,	O
1995	O
)	O
is	O
a	O
leading	O
method	B
for	O
text	O
compression	O
,	O
and	O
it	O
uses	O
arithmetic	O
coding	O
.	O
there	O
are	O
many	O
lempel	O
{	O
ziv-based	O
programs	O
.	O
gzip	B
is	O
based	O
on	O
a	O
version	O
of	O
lempel	O
{	O
ziv	O
called	O
‘	O
lz77	O
’	O
(	O
ziv	O
and	O
lempel	O
,	O
1977	O
)	O
.	O
compress	B
is	O
based	O
on	O
‘	O
lzw	O
’	O
(	O
welch	O
,	O
1984	O
)	O
.	O
in	O
my	O
experience	O
the	O
best	O
is	O
gzip	B
,	O
with	O
compress	O
being	O
inferior	O
on	O
most	O
(	O
cid:12	O
)	O
les	O
.	O
bzip	O
is	O
a	O
block-sorting	B
(	O
cid:12	O
)	O
le	O
compressor	O
,	O
which	O
makes	O
use	O
of	O
a	O
neat	O
hack	O
called	O
the	O
burrows	O
{	O
wheeler	O
transform	O
(	O
burrows	O
and	O
wheeler	O
,	O
1994	O
)	O
.	O
this	O
method	B
is	O
not	O
based	O
on	O
an	O
explicit	O
probabilistic	B
model	I
,	O
and	O
it	O
only	O
works	O
well	O
for	O
(	O
cid:12	O
)	O
les	O
larger	O
than	O
several	O
thousand	O
characters	O
;	O
but	O
in	O
practice	O
it	O
is	O
a	O
very	O
e	O
(	O
cid:11	O
)	O
ective	O
compressor	O
for	O
(	O
cid:12	O
)	O
les	O
in	O
which	O
the	O
context	O
of	O
a	O
character	O
is	O
a	O
good	B
predictor	O
for	O
that	O
character.5	O
2http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itprnn/softwarei.html	O
3ftp	O
:	O
//ftp.cs.toronto.edu/pub/radford/www/ac.software.html	O
4http	O
:	O
//www.djvuzone.org/	O
5there	O
is	O
a	O
lot	O
of	O
information	O
about	O
the	O
burrows	O
{	O
wheeler	O
transform	O
on	O
the	O
net	O
.	O
http	O
:	O
//dogma.net/datacompression/bwt.shtml	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
122	O
compression	B
of	O
a	O
text	O
(	O
cid:12	O
)	O
le	O
table	O
6.6	O
gives	O
the	O
computer	B
time	O
in	O
seconds	O
taken	O
and	O
the	O
compression	B
achieved	O
when	O
these	O
programs	O
are	O
applied	O
to	O
the	O
latex	O
(	O
cid:12	O
)	O
le	O
containing	O
the	O
text	O
of	O
this	O
chapter	O
,	O
of	O
size	O
20,942	O
bytes	O
.	O
6	O
|	O
stream	B
codes	I
method	O
compression	B
compressed	O
size	O
uncompression	B
time	O
=	O
sec	O
(	O
%	O
age	O
of	O
20,942	O
)	O
time	O
=	O
sec	O
table	O
6.6.	O
comparison	O
of	O
compression	O
algorithms	B
applied	O
to	O
a	O
text	O
(	O
cid:12	O
)	O
le	O
.	O
laplace	O
model	B
gzip	O
compress	B
0.28	O
0.10	O
0.05	O
bzip	O
bzip2	O
ppmz	O
12	O
974	O
(	O
61	O
%	O
)	O
8	O
177	O
(	O
39	O
%	O
)	O
10	O
816	O
(	O
51	O
%	O
)	O
7	O
495	O
(	O
36	O
%	O
)	O
7	O
640	O
(	O
36	O
%	O
)	O
6	O
800	O
(	O
32	O
%	O
)	O
0.32	O
0.01	O
0.05	O
compression	B
of	O
a	O
sparse	O
(	O
cid:12	O
)	O
le	O
interestingly	O
,	O
gzip	B
does	O
not	O
always	O
do	O
so	O
well	O
.	O
table	O
6.7	O
gives	O
the	O
compres-	O
sion	O
achieved	O
when	O
these	O
programs	O
are	O
applied	O
to	O
a	O
text	O
(	O
cid:12	O
)	O
le	O
containing	O
106	O
characters	O
,	O
each	O
of	O
which	O
is	O
either	O
0	O
and	O
1	O
with	O
probabilities	O
0.99	O
and	O
0.01.	O
the	O
laplace	O
model	B
is	O
quite	O
well	O
matched	O
to	O
this	O
source	O
,	O
and	O
the	O
benchmark	O
arithmetic	O
coder	O
gives	O
good	B
performance	O
,	O
followed	O
closely	O
by	O
compress	O
;	O
gzip	B
is	O
worst	O
.	O
an	O
ideal	O
model	B
for	O
this	O
source	O
would	O
compress	B
the	O
(	O
cid:12	O
)	O
le	O
into	O
about	O
106h2	O
(	O
0:01	O
)	O
=8	O
’	O
10	O
100	O
bytes	O
.	O
the	O
laplace-model	O
compressor	O
falls	O
short	O
of	O
this	O
performance	O
because	O
it	O
is	O
implemented	O
using	O
only	O
eight-bit	O
precision	B
.	O
the	O
ppmz	O
compressor	O
compresses	O
the	O
best	O
of	O
all	O
,	O
but	O
takes	O
much	O
more	O
computer	B
time	O
.	O
method	B
compression	O
compressed	O
size	O
uncompression	B
time	O
=	O
sec	O
=	O
bytes	O
time	O
=	O
sec	O
laplace	O
model	B
gzip	O
gzip	B
--	O
best+	O
compress	B
bzip	O
bzip2	O
ppmz	O
0.45	O
0.22	O
1.63	O
0.13	O
0.30	O
0.19	O
533	O
14	O
143	O
(	O
1.4	O
%	O
)	O
20	O
646	O
(	O
2.1	O
%	O
)	O
15	O
553	O
(	O
1.6	O
%	O
)	O
14	O
785	O
(	O
1.5	O
%	O
)	O
10	O
903	O
(	O
1.09	O
%	O
)	O
11	O
260	O
(	O
1.12	O
%	O
)	O
10	O
447	O
(	O
1.04	O
%	O
)	O
0.57	O
0.04	O
0.05	O
0.03	O
0.17	O
0.05	O
535	O
6.6	O
summary	B
in	O
the	O
last	O
three	O
chapters	O
we	O
have	O
studied	O
three	O
classes	O
of	O
data	O
compression	B
codes	O
.	O
fixed-length	O
block	B
codes	O
(	O
chapter	O
4	O
)	O
.	O
these	O
are	O
mappings	O
from	O
a	O
(	O
cid:12	O
)	O
xed	O
number	O
of	O
source	O
symbols	O
to	O
a	O
(	O
cid:12	O
)	O
xed-length	O
binary	O
message	O
.	O
only	O
a	O
tiny	O
fraction	O
of	O
the	O
source	O
strings	O
are	O
given	O
an	O
encoding	O
.	O
these	O
codes	O
were	O
fun	O
for	O
identifying	O
the	O
entropy	B
as	O
the	O
measure	O
of	O
compressibility	O
but	O
they	O
are	O
of	O
little	O
practical	B
use	O
.	O
table	O
6.7.	O
comparison	O
of	O
compression	O
algorithms	B
applied	O
to	O
a	O
random	B
(	O
cid:12	O
)	O
le	O
of	O
106	O
characters	O
,	O
99	O
%	O
0s	O
and	O
1	O
%	O
1s	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
6.7	O
:	O
exercises	O
on	O
stream	O
codes	O
123	O
symbol	O
codes	O
(	O
chapter	O
5	O
)	O
.	O
symbol	O
codes	O
employ	O
a	O
variable-length	B
code	I
for	O
each	O
symbol	O
in	O
the	O
source	O
alphabet	O
,	O
the	O
codelengths	O
being	O
integer	O
lengths	O
determined	O
by	O
the	O
probabilities	O
of	O
the	O
symbols	O
.	O
hu	O
(	O
cid:11	O
)	O
man	O
’	O
s	O
algorithm	B
constructs	O
an	O
optimal	B
symbol	O
code	B
for	O
a	O
given	O
set	B
of	O
symbol	O
probabilities	O
.	O
every	O
source	O
string	O
has	O
a	O
uniquely	B
decodeable	I
encoding	O
,	O
and	O
if	O
the	O
source	O
symbols	O
come	O
from	O
the	O
assumed	O
distribution	B
then	O
the	O
symbol	B
code	I
will	O
compress	B
to	O
an	O
expected	O
length	B
per	O
character	O
l	O
lying	O
in	O
the	O
interval	O
[	O
h	O
;	O
h	O
+	O
1	O
)	O
.	O
statistical	B
(	O
cid:13	O
)	O
uctuations	O
in	O
the	O
source	O
may	O
make	O
the	O
actual	O
length	B
longer	O
or	O
shorter	O
than	O
this	O
mean	B
length	O
.	O
if	O
the	O
source	O
is	O
not	O
well	O
matched	O
to	O
the	O
assumed	O
distribution	B
then	O
the	O
mean	B
length	O
is	O
increased	O
by	O
the	O
relative	B
entropy	I
dkl	O
between	O
the	O
source	O
distribution	O
and	O
the	O
code	B
’	O
s	O
implicit	O
distribution	O
.	O
for	O
sources	O
with	O
small	O
entropy	B
,	O
the	O
symbol	O
has	O
to	O
emit	O
at	O
least	O
one	O
bit	B
per	O
source	O
symbol	O
;	O
compression	B
below	O
one	O
bit	B
per	O
source	O
symbol	O
can	O
be	O
achieved	O
only	O
by	O
the	O
cumbersome	O
procedure	O
of	O
putting	O
the	O
source	O
data	O
into	O
blocks	O
.	O
stream	B
codes	I
.	O
the	O
distinctive	O
property	O
of	O
stream	O
codes	O
,	O
compared	O
with	O
symbol	O
codes	O
,	O
is	O
that	O
they	O
are	O
not	O
constrained	B
to	O
emit	O
at	O
least	O
one	O
bit	B
for	O
every	O
symbol	O
read	O
from	O
the	O
source	O
stream	O
.	O
so	O
large	O
numbers	O
of	O
source	O
symbols	O
may	O
be	O
coded	O
into	O
a	O
smaller	O
number	O
of	O
bits	O
.	O
this	O
property	O
could	O
be	O
obtained	O
using	O
a	O
symbol	B
code	I
only	O
if	O
the	O
source	O
stream	O
were	O
somehow	O
chopped	O
into	O
blocks	O
.	O
(	O
cid:15	O
)	O
arithmetic	O
codes	O
combine	O
a	O
probabilistic	B
model	I
with	O
an	O
encoding	O
algorithm	B
that	O
identi	O
(	O
cid:12	O
)	O
es	O
each	O
string	O
with	O
a	O
sub-interval	O
of	O
[	O
0	O
;	O
1	O
)	O
of	O
size	O
equal	O
to	O
the	O
probability	O
of	O
that	O
string	O
under	O
the	O
model	B
.	O
this	O
code	B
is	O
almost	O
optimal	B
in	O
the	O
sense	O
that	O
the	O
compressed	O
length	B
of	O
a	O
string	O
x	O
closely	O
matches	O
the	O
shannon	O
information	O
content	B
of	O
x	O
given	O
the	O
probabilistic	B
model	I
.	O
arithmetic	O
codes	O
(	O
cid:12	O
)	O
t	O
with	O
the	O
philosophy	B
that	O
good	B
compression	O
requires	O
data	B
modelling	I
,	O
in	O
the	O
form	O
of	O
an	O
adaptive	B
bayesian	O
model	B
.	O
(	O
cid:15	O
)	O
lempel	O
{	O
ziv	O
codes	O
are	O
adaptive	B
in	O
the	O
sense	O
that	O
they	O
memorize	O
strings	O
that	O
have	O
already	O
occurred	O
.	O
they	O
are	O
built	O
on	O
the	O
philoso-	O
phy	O
that	O
we	O
don	O
’	O
t	O
know	O
anything	O
at	O
all	O
about	O
what	O
the	O
probability	B
distribution	O
of	O
the	O
source	O
will	O
be	O
,	O
and	O
we	O
want	O
a	O
compression	B
algo-	O
rithm	O
that	O
will	O
perform	O
reasonably	O
well	O
whatever	O
that	O
distribution	B
is	O
.	O
both	O
arithmetic	O
codes	O
and	O
lempel	O
{	O
ziv	O
codes	O
will	O
fail	O
to	O
decode	O
correctly	O
if	O
any	O
of	O
the	O
bits	O
of	O
the	O
compressed	O
(	O
cid:12	O
)	O
le	O
are	O
altered	O
.	O
so	O
if	O
compressed	O
(	O
cid:12	O
)	O
les	O
are	O
to	O
be	O
stored	O
or	O
transmitted	O
over	O
noisy	O
media	O
,	O
error-correcting	B
codes	I
will	O
be	O
essential	O
.	O
reliable	O
communication	B
over	O
unreliable	O
channels	O
is	O
the	O
topic	O
of	O
part	O
ii	O
.	O
6.7	O
exercises	O
on	O
stream	O
codes	O
exercise	O
6.7	O
.	O
[	O
2	O
]	O
describe	O
an	O
arithmetic	B
coding	I
algorithm	O
to	O
encode	O
random	B
bit	O
strings	O
of	O
length	O
n	O
and	O
weight	O
k	O
(	O
i.e.	O
,	O
k	O
ones	O
and	O
n	O
(	O
cid:0	O
)	O
k	O
zeroes	O
)	O
where	O
n	O
and	O
k	O
are	O
given	O
.	O
for	O
the	O
case	O
n	O
=	O
5	O
,	O
k	O
=	O
2	O
,	O
show	O
in	O
detail	O
the	O
intervals	B
corresponding	O
to	O
all	O
source	O
substrings	O
of	O
lengths	O
1	O
{	O
5.	O
.	O
exercise	O
6.8	O
.	O
[	O
2	O
,	O
p.128	O
]	O
how	O
many	O
bits	O
are	O
needed	O
to	O
specify	O
a	O
selection	O
of	O
k	O
objects	O
from	O
n	O
objects	O
?	O
(	O
n	O
and	O
k	O
are	O
assumed	O
to	O
be	O
known	O
and	O
the	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
124	O
6	O
|	O
stream	B
codes	I
selection	O
of	O
k	O
objects	O
is	O
unordered	O
.	O
)	O
how	O
might	O
such	O
a	O
selection	O
be	O
made	O
at	O
random	B
without	O
being	O
wasteful	O
of	O
random	O
bits	O
?	O
.	O
exercise	O
6.9	O
.	O
[	O
2	O
]	O
a	O
binary	O
source	O
x	O
emits	O
independent	O
identically	O
distributed	O
symbols	O
with	O
probability	O
distribution	B
ff0	O
;	O
f1g	O
,	O
where	O
f1	O
=	O
0:01.	O
find	O
an	O
optimal	B
uniquely-decodeable	O
symbol	B
code	I
for	O
a	O
string	O
x	O
=	O
x1x2x3	O
of	O
three	O
successive	O
samples	O
from	O
this	O
source	O
.	O
estimate	O
(	O
to	O
one	O
decimal	O
place	O
)	O
the	O
factor	O
by	O
which	O
the	O
expected	O
length	B
of	O
this	O
optimal	B
code	O
is	O
greater	O
than	O
the	O
entropy	B
of	O
the	O
three-bit	O
string	O
x	O
.	O
[	O
h2	O
(	O
0:01	O
)	O
’	O
0:08	O
,	O
where	O
h2	O
(	O
x	O
)	O
=	O
x	O
log2	O
(	O
1=x	O
)	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
x	O
)	O
log2	O
(	O
1=	O
(	O
1	O
(	O
cid:0	O
)	O
x	O
)	O
)	O
.	O
]	O
an	O
arithmetic	O
code	O
is	O
used	O
to	O
compress	B
a	O
string	O
of	O
1000	O
samples	O
from	O
the	O
source	O
x.	O
estimate	O
the	O
mean	B
and	O
standard	B
deviation	I
of	O
the	O
length	B
of	O
the	O
compressed	O
(	O
cid:12	O
)	O
le	O
.	O
.	O
exercise	O
6.10	O
.	O
[	O
2	O
]	O
describe	O
an	O
arithmetic	B
coding	I
algorithm	O
to	O
generate	O
random	B
bit	O
strings	O
of	O
length	O
n	O
with	O
density	O
f	O
(	O
i.e.	O
,	O
each	O
bit	B
has	O
probability	B
f	O
of	O
being	O
a	O
one	O
)	O
where	O
n	O
is	O
given	O
.	O
exercise	O
6.11	O
.	O
[	O
2	O
]	O
use	O
a	O
modi	O
(	O
cid:12	O
)	O
ed	O
lempel	O
{	O
ziv	O
algorithm	B
in	O
which	O
,	O
as	O
discussed	O
on	O
p.120	O
,	O
the	O
dictionary	B
of	O
pre	O
(	O
cid:12	O
)	O
xes	O
is	O
pruned	O
by	O
writing	O
new	O
pre	O
(	O
cid:12	O
)	O
xes	O
into	O
the	O
space	O
occupied	O
by	O
pre	O
(	O
cid:12	O
)	O
xes	O
that	O
will	O
not	O
be	O
needed	O
again	O
.	O
such	O
pre	O
(	O
cid:12	O
)	O
xes	O
can	O
be	O
identi	O
(	O
cid:12	O
)	O
ed	O
when	O
both	O
their	O
children	O
have	O
been	O
added	O
to	O
the	O
dictionary	B
of	O
pre	O
(	O
cid:12	O
)	O
xes	O
.	O
(	O
you	O
may	O
neglect	O
the	O
issue	O
of	O
termination	O
of	O
encoding	O
.	O
)	O
use	O
this	O
algorithm	B
to	O
encode	O
the	O
string	O
0100001000100010101000001.	O
highlight	O
the	O
bits	O
that	O
follow	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
on	O
the	O
second	O
occasion	O
that	O
that	O
pre	O
(	O
cid:12	O
)	O
x	O
is	O
used	O
.	O
(	O
as	O
discussed	O
earlier	O
,	O
these	O
bits	O
could	O
be	O
omitted	O
.	O
)	O
exercise	O
6.12	O
.	O
[	O
2	O
,	O
p.128	O
]	O
show	O
that	O
this	O
modi	O
(	O
cid:12	O
)	O
ed	O
lempel	O
{	O
ziv	O
code	B
is	O
still	O
not	O
‘	O
complete	O
’	O
,	O
that	O
is	O
,	O
there	O
are	O
binary	O
strings	O
that	O
are	O
not	O
encodings	O
of	O
any	O
string	O
.	O
.	O
exercise	O
6.13	O
.	O
[	O
3	O
,	O
p.128	O
]	O
give	O
examples	O
of	O
simple	O
sources	O
that	O
have	O
low	O
entropy	B
but	O
would	O
not	O
be	O
compressed	O
well	O
by	O
the	O
lempel	O
{	O
ziv	O
algorithm	B
.	O
6.8	O
further	O
exercises	O
on	O
data	O
compression	B
the	O
following	O
exercises	O
may	O
be	O
skipped	O
by	O
the	O
reader	O
who	O
is	O
eager	O
to	O
learn	O
about	O
noisy	B
channels	O
.	O
exercise	O
6.14	O
.	O
[	O
3	O
,	O
p.130	O
]	O
consider	O
a	O
gaussian	O
distribution	B
in	O
n	O
dimensions	B
,	O
n	O
(	O
6.13	O
)	O
p	O
(	O
x	O
)	O
=	O
1	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
)	O
n=2	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
pn	O
x2	O
2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:19	O
)	O
:	O
n	O
(	O
cid:1	O
)	O
1=2	O
.	O
estimate	O
the	O
mean	B
de	O
(	O
cid:12	O
)	O
ne	O
the	O
radius	O
of	O
a	O
point	O
x	O
to	O
be	O
r	O
=	O
(	O
cid:0	O
)	O
pn	O
x2	O
and	O
variance	O
of	O
the	O
square	O
of	O
the	O
radius	O
,	O
r2	O
=	O
(	O
cid:0	O
)	O
pn	O
x2	O
n	O
(	O
cid:1	O
)	O
.	O
2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:19	O
)	O
=	O
3	O
(	O
cid:27	O
)	O
4	O
;	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
)	O
1=2	O
x4	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
you	O
may	O
(	O
cid:12	O
)	O
nd	O
helpful	O
the	O
integral	B
z	O
dx	O
1	O
x2	O
though	O
you	O
should	O
be	O
able	O
to	O
estimate	O
the	O
required	O
quantities	O
without	O
it	O
.	O
(	O
6.14	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
6.8	O
:	O
further	O
exercises	O
on	O
data	O
compression	B
assuming	O
that	O
n	O
is	O
large	O
,	O
show	O
that	O
nearly	O
all	O
the	O
probability	O
of	O
a	O
gaussian	O
is	O
contained	O
in	O
a	O
thin	B
shell	I
of	O
radius	O
pn	O
(	O
cid:27	O
)	O
.	O
find	O
the	O
thickness	O
of	O
the	O
shell	O
.	O
evaluate	O
the	O
probability	B
density	O
(	O
6.13	O
)	O
at	O
a	O
point	O
in	O
that	O
thin	B
shell	I
and	O
at	O
the	O
origin	O
x	O
=	O
0	O
and	O
compare	O
.	O
use	O
the	O
case	O
n	O
=	O
1000	O
as	O
an	O
example	O
.	O
notice	O
that	O
nearly	O
all	O
the	O
probability	B
mass	O
is	O
located	O
in	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
part	O
of	O
the	O
space	O
from	O
the	O
region	O
of	O
highest	O
probability	B
density	O
.	O
exercise	O
6.15	O
.	O
[	O
2	O
]	O
explain	O
what	O
is	O
meant	O
by	O
an	O
optimal	B
binary	O
symbol	B
code	I
.	O
find	O
an	O
optimal	B
binary	O
symbol	B
code	I
for	O
the	O
ensemble	B
:	O
125	O
probability	B
density	O
is	O
maximized	O
here	O
pn	O
(	O
cid:27	O
)	O
almost	O
all	O
probability	B
mass	O
is	O
here	O
figure	O
6.8.	O
schematic	O
representation	O
of	O
the	O
typical	O
set	B
of	O
an	O
n	O
-dimensional	O
gaussian	O
distribution	B
.	O
a	O
=	O
fa	O
;	O
b	O
;	O
c	O
;	O
d	O
;	O
e	O
;	O
f	O
;	O
g	O
;	O
h	O
;	O
i	O
;	O
jg	O
;	O
10	O
100	O
9	O
100	O
5	O
100	O
4	O
100	O
;	O
8	O
100	O
;	O
;	O
2	O
100	O
;	O
;	O
6	O
100	O
;	O
p	O
=	O
(	O
cid:26	O
)	O
1	O
100	O
;	O
;	O
25	O
100	O
;	O
30	O
100	O
(	O
cid:27	O
)	O
;	O
and	O
compute	O
the	O
expected	O
length	B
of	O
the	O
code	B
.	O
exercise	O
6.16	O
.	O
[	O
2	O
]	O
a	O
string	O
y	O
=	O
x1x2	O
consists	O
of	O
two	O
independent	O
samples	O
from	O
an	O
ensemble	B
x	O
:	O
ax	O
=	O
fa	O
;	O
b	O
;	O
cg	O
;	O
px	O
=	O
(	O
cid:26	O
)	O
1	O
10	O
;	O
3	O
10	O
;	O
6	O
10	O
(	O
cid:27	O
)	O
:	O
what	O
is	O
the	O
entropy	B
of	O
y	O
?	O
construct	O
an	O
optimal	B
binary	O
symbol	B
code	I
for	O
the	O
string	O
y	O
,	O
and	O
(	O
cid:12	O
)	O
nd	O
its	O
expected	O
length	B
.	O
exercise	O
6.17	O
.	O
[	O
2	O
]	O
strings	O
of	O
n	O
independent	O
samples	O
from	O
an	O
ensemble	B
with	O
p	O
=	O
f0:1	O
;	O
0:9g	O
are	O
compressed	O
using	O
an	O
arithmetic	O
code	O
that	O
is	O
matched	O
to	O
that	O
ensemble	B
.	O
estimate	O
the	O
mean	B
and	O
standard	B
deviation	I
of	O
the	O
compressed	O
strings	O
’	O
lengths	O
for	O
the	O
case	O
n	O
=	O
1000	O
.	O
[	O
h2	O
(	O
0:1	O
)	O
’	O
0:47	O
]	O
exercise	O
6.18	O
.	O
[	O
3	O
]	O
source	O
coding	O
with	O
variable-length	B
symbols	O
.	O
in	O
the	O
chapters	O
on	O
source	O
coding	O
,	O
we	O
assumed	O
that	O
we	O
were	O
encoding	O
into	O
a	O
binary	O
alphabet	O
f0	O
;	O
1g	O
in	O
which	O
both	O
symbols	O
should	O
be	O
used	O
with	O
equal	O
frequency	B
.	O
in	O
this	O
question	O
we	O
ex-	O
plore	O
how	O
the	O
encoding	O
alphabet	O
should	O
be	O
used	O
if	O
the	O
symbols	O
take	O
di	O
(	O
cid:11	O
)	O
erent	O
times	O
to	O
transmit	O
.	O
a	O
poverty-stricken	O
student	B
communicates	O
for	O
free	O
with	O
a	O
friend	O
using	O
a	O
telephone	B
by	O
selecting	O
an	O
integer	O
n	O
2	O
f1	O
;	O
2	O
;	O
3	O
:	O
:	O
:	O
g	O
,	O
making	O
the	O
friend	O
’	O
s	O
phone	B
ring	O
n	O
times	O
,	O
then	O
hanging	O
up	O
in	O
the	O
middle	O
of	O
the	O
nth	O
ring	O
.	O
this	O
process	O
is	O
repeated	O
so	O
that	O
a	O
string	O
of	O
symbols	O
n1n2n3	O
:	O
:	O
:	O
is	O
received	O
.	O
what	O
is	O
the	O
optimal	B
way	O
to	O
communicate	O
?	O
if	O
large	O
integers	O
n	O
are	O
selected	O
then	O
the	O
message	O
takes	O
longer	O
to	O
communicate	O
.	O
if	O
only	O
small	O
integers	O
n	O
are	O
used	O
then	O
the	O
information	B
content	I
per	O
symbol	O
is	O
small	O
.	O
we	O
aim	O
to	O
maximize	O
the	O
rate	B
of	O
information	B
transfer	O
,	O
per	O
unit	O
time	O
.	O
assume	O
that	O
the	O
time	O
taken	O
to	O
transmit	O
a	O
number	O
of	O
rings	O
n	O
and	O
to	O
redial	O
is	O
ln	O
seconds	O
.	O
consider	O
a	O
probability	B
distribution	O
over	O
n	O
,	O
fpng	O
.	O
de	O
(	O
cid:12	O
)	O
ning	O
the	O
average	B
duration	O
per	O
symbol	O
to	O
be	O
l	O
(	O
p	O
)	O
=xn	O
pnln	O
(	O
6.15	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
126	O
6	O
|	O
stream	B
codes	I
and	O
the	O
entropy	B
per	O
symbol	O
to	O
be	O
h	O
(	O
p	O
)	O
=xn	O
pn	O
log2	O
1	O
pn	O
;	O
(	O
6.16	O
)	O
show	O
that	O
for	O
the	O
average	B
information	O
rate	B
per	O
second	O
to	O
be	O
maximized	O
,	O
the	O
symbols	O
must	O
be	O
used	O
with	O
probabilities	O
of	O
the	O
form	O
pn	O
=	O
1	O
z	O
2	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
ln	O
where	O
z	O
=pn	O
2	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
ln	O
and	O
(	O
cid:12	O
)	O
satis	O
(	O
cid:12	O
)	O
es	O
the	O
implicit	O
equation	O
(	O
cid:12	O
)	O
=	O
h	O
(	O
p	O
)	O
l	O
(	O
p	O
)	O
;	O
(	O
6.17	O
)	O
(	O
6.18	O
)	O
that	O
is	O
,	O
(	O
cid:12	O
)	O
is	O
the	O
rate	B
of	O
communication	B
.	O
show	O
that	O
these	O
two	O
equations	O
(	O
6.17	O
,	O
6.18	O
)	O
imply	O
that	O
(	O
cid:12	O
)	O
must	O
be	O
set	B
such	O
that	O
log	O
z	O
=	O
0	O
:	O
assuming	O
that	O
the	O
channel	B
has	O
the	O
property	O
ln	O
=	O
n	O
seconds	O
;	O
(	O
6.19	O
)	O
(	O
6.20	O
)	O
(	O
cid:12	O
)	O
nd	O
the	O
optimal	B
distribution	O
p	O
and	O
show	O
that	O
the	O
maximal	O
information	B
rate	O
is	O
1	O
bit	B
per	O
second	O
.	O
how	O
does	O
this	O
compare	O
with	O
the	O
information	B
rate	O
per	O
second	O
achieved	O
if	O
p	O
is	O
set	B
to	O
(	O
1=2	O
;	O
1=2	O
;	O
0	O
;	O
0	O
;	O
0	O
;	O
0	O
;	O
:	O
:	O
:	O
)	O
|	O
that	O
is	O
,	O
only	O
the	O
symbols	O
n	O
=	O
1	O
and	O
n	O
=	O
2	O
are	O
selected	O
,	O
and	O
they	O
have	O
equal	O
probability	B
?	O
discuss	O
the	O
relationship	O
between	O
the	O
results	O
(	O
6.17	O
,	O
6.19	O
)	O
derived	O
above	O
,	O
and	O
the	O
kraft	O
inequality	B
from	O
source	O
coding	O
theory	O
.	O
how	O
might	O
a	O
random	B
binary	O
source	O
be	O
e	O
(	O
cid:14	O
)	O
ciently	O
encoded	O
into	O
a	O
se-	O
quence	O
of	O
symbols	O
n1n2n3	O
:	O
:	O
:	O
for	O
transmission	O
over	O
the	O
channel	B
de	O
(	O
cid:12	O
)	O
ned	O
in	O
equation	O
(	O
6.20	O
)	O
?	O
.	O
exercise	O
6.19	O
.	O
[	O
1	O
]	O
how	O
many	O
bits	O
does	O
it	O
take	O
to	O
shu	O
(	O
cid:15	O
)	O
e	O
a	O
pack	O
of	O
cards	O
?	O
.	O
exercise	O
6.20	O
.	O
[	O
2	O
]	O
in	O
the	O
card	B
game	O
bridge	O
,	O
the	O
four	O
players	O
receive	O
13	O
cards	O
each	O
from	O
the	O
deck	O
of	O
52	O
and	O
start	O
each	O
game	B
by	O
looking	O
at	O
their	O
own	O
hand	O
and	O
bidding	O
.	O
the	O
legal	O
bids	O
are	O
,	O
in	O
ascending	O
order	O
1|	O
;	O
1	O
}	O
;	O
1~	O
;	O
1	O
(	O
cid:127	O
)	O
;	O
1n	O
t	O
;	O
2|	O
;	O
2	O
}	O
;	O
:	O
:	O
:	O
7~	O
;	O
7	O
(	O
cid:127	O
)	O
;	O
7n	O
t	O
,	O
and	O
successive	O
bids	O
must	O
follow	O
this	O
order	O
;	O
a	O
bid	O
of	O
,	O
say	O
,	O
2~	O
may	O
only	O
be	O
followed	O
by	O
higher	O
bids	O
such	O
as	O
2	O
(	O
cid:127	O
)	O
or	O
3|	O
or	O
7n	O
t	O
.	O
(	O
let	O
us	O
neglect	O
the	O
‘	O
double	O
’	O
bid	O
.	O
)	O
the	O
players	O
have	O
several	O
aims	O
when	O
bidding	O
.	O
one	O
of	O
the	O
aims	O
is	O
for	O
two	O
partners	O
to	O
communicate	O
to	O
each	O
other	O
as	O
much	O
as	O
possible	O
about	O
what	O
cards	O
are	O
in	O
their	O
hands	O
.	O
let	O
us	O
concentrate	O
on	O
this	O
task	O
.	O
(	O
a	O
)	O
after	O
the	O
cards	O
have	O
been	O
dealt	O
,	O
how	O
many	O
bits	O
are	O
needed	O
for	O
north	O
to	O
convey	O
to	O
south	O
what	O
her	O
hand	O
is	O
?	O
(	O
b	O
)	O
assuming	O
that	O
e	O
and	O
w	O
do	O
not	O
bid	O
at	O
all	O
,	O
what	O
is	O
the	O
maximum	O
total	O
information	B
that	O
n	O
and	O
s	O
can	O
convey	O
to	O
each	O
other	O
while	O
bidding	O
?	O
assume	O
that	O
n	O
starts	O
the	O
bidding	O
,	O
and	O
that	O
once	O
either	O
n	O
or	O
s	O
stops	O
bidding	O
,	O
the	O
bidding	O
stops	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
6.9	O
:	O
solutions	O
127	O
.	O
exercise	O
6.21	O
.	O
[	O
2	O
]	O
my	O
old	O
‘	O
arabic	B
’	O
microwave	B
oven	I
had	O
11	O
buttons	O
for	O
entering	O
cooking	O
times	O
,	O
and	O
my	O
new	O
‘	O
roman	B
’	O
microwave	O
has	O
just	O
(	O
cid:12	O
)	O
ve	O
.	O
the	O
but-	O
tons	O
of	O
the	O
roman	O
microwave	O
are	O
labelled	O
‘	O
10	O
minutes	O
’	O
,	O
‘	O
1	O
minute	O
’	O
,	O
‘	O
10	O
seconds	O
’	O
,	O
‘	O
1	O
second	O
’	O
,	O
and	O
‘	O
start	O
’	O
;	O
i	O
’	O
ll	O
abbreviate	O
these	O
(	O
cid:12	O
)	O
ve	O
strings	O
to	O
the	O
symbols	O
m	O
,	O
c	O
,	O
x	O
,	O
i	O
,	O
2.	O
to	O
enter	O
one	O
minute	O
and	O
twenty-three	O
seconds	O
(	O
1:23	O
)	O
,	O
the	O
arabic	B
sequence	O
is	O
arabic	B
roman	O
m	O
c	O
x	O
i	O
2	O
1	O
4	O
7	O
3	O
2	O
6	O
5	O
8	O
9	O
0	O
2	O
1232	O
;	O
(	O
6.21	O
)	O
figure	O
6.9.	O
alternative	O
keypads	O
for	O
microwave	O
ovens	O
.	O
and	O
the	O
roman	B
sequence	O
is	O
cxxiii2	O
:	O
(	O
6.22	O
)	O
each	O
of	O
these	O
keypads	O
de	O
(	O
cid:12	O
)	O
nes	O
a	O
code	B
mapping	O
the	O
3599	O
cooking	O
times	O
from	O
0:01	O
to	O
59:59	O
into	O
a	O
string	O
of	O
symbols	O
.	O
(	O
a	O
)	O
which	O
times	O
can	O
be	O
produced	O
with	O
two	O
or	O
three	O
symbols	O
?	O
(	O
for	O
example	O
,	O
0:20	O
can	O
be	O
produced	O
by	O
three	O
symbols	O
in	O
either	O
code	B
:	O
xx2	O
and	O
202	O
.	O
)	O
(	O
b	O
)	O
are	O
the	O
two	O
codes	O
complete	O
?	O
give	O
a	O
detailed	O
answer	O
.	O
(	O
c	O
)	O
for	O
each	O
code	B
,	O
name	O
a	O
cooking	O
time	O
that	O
it	O
can	O
produce	O
in	O
four	O
symbols	O
that	O
the	O
other	O
code	B
can	O
not	O
.	O
(	O
d	O
)	O
discuss	O
the	O
implicit	O
probability	O
distributions	O
over	O
times	O
to	O
which	O
each	O
of	O
these	O
codes	O
is	O
best	O
matched	O
.	O
(	O
e	O
)	O
concoct	O
a	O
plausible	O
probability	B
distribution	O
over	O
times	O
that	O
a	O
real	O
user	O
might	O
use	O
,	O
and	O
evaluate	O
roughly	O
the	O
expected	O
number	O
of	O
sym-	O
bols	O
,	O
and	O
maximum	O
number	O
of	O
symbols	O
,	O
that	O
each	O
code	B
requires	O
.	O
discuss	O
the	O
ways	O
in	O
which	O
each	O
code	B
is	O
ine	O
(	O
cid:14	O
)	O
cient	O
or	O
e	O
(	O
cid:14	O
)	O
cient	O
.	O
(	O
f	O
)	O
invent	O
a	O
more	O
e	O
(	O
cid:14	O
)	O
cient	O
cooking-time-encoding	O
system	O
for	O
a	O
mi-	O
crowave	O
oven	O
.	O
exercise	O
6.22	O
.	O
[	O
2	O
,	O
p.132	O
]	O
is	O
the	O
standard	O
binary	O
representation	O
for	O
positive	O
inte-	O
gers	O
(	O
e.g	O
.	O
cb	O
(	O
5	O
)	O
=	O
101	O
)	O
a	O
uniquely	B
decodeable	I
code	O
?	O
design	O
a	O
binary	O
code	O
for	O
the	O
positive	O
integers	O
,	O
i.e.	O
,	O
a	O
mapping	B
from	O
n	O
2	O
f1	O
;	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
g	O
to	O
c	O
(	O
n	O
)	O
2	O
f0	O
;	O
1g+	O
,	O
that	O
is	O
uniquely	B
decodeable	I
.	O
try	O
to	O
design	O
codes	O
that	O
are	O
pre	O
(	O
cid:12	O
)	O
x	O
codes	O
and	O
that	O
satisfy	O
the	O
kraft	O
equality	O
pn	O
2	O
(	O
cid:0	O
)	O
ln	O
=	O
1.	O
motivations	O
:	O
any	O
data	O
(	O
cid:12	O
)	O
le	O
terminated	O
by	O
a	O
special	O
end	O
of	O
(	O
cid:12	O
)	O
le	O
character	O
can	O
be	O
mapped	O
onto	O
an	O
integer	O
,	O
so	O
a	O
pre	O
(	O
cid:12	O
)	O
x	O
code	B
for	O
integers	O
can	O
be	O
used	O
as	O
a	O
self-delimiting	B
encoding	O
of	O
(	O
cid:12	O
)	O
les	O
too	O
.	O
large	O
(	O
cid:12	O
)	O
les	O
correspond	O
to	O
large	O
integers	O
.	O
also	O
,	O
one	O
of	O
the	O
building	O
blocks	O
of	O
a	O
‘	O
universal	B
’	O
coding	O
scheme	O
{	O
that	O
is	O
,	O
a	O
coding	O
scheme	O
that	O
will	O
work	O
ok	O
for	O
a	O
large	O
variety	O
of	O
sources	O
{	O
is	O
the	O
ability	O
to	O
encode	O
integers	O
.	O
finally	O
,	O
in	O
microwave	O
ovens	O
,	O
cooking	O
times	O
are	O
positive	O
integers	O
!	O
discuss	O
criteria	O
by	O
which	O
one	O
might	O
compare	O
alternative	O
codes	O
for	O
inte-	O
gers	O
(	O
or	O
,	O
equivalently	O
,	O
alternative	O
self-delimiting	B
codes	O
for	O
(	O
cid:12	O
)	O
les	O
)	O
.	O
6.9	O
solutions	O
solution	O
to	O
exercise	O
6.1	O
(	O
p.115	O
)	O
.	O
the	O
worst-case	O
situation	O
is	O
when	O
the	O
interval	O
to	O
be	O
represented	O
lies	O
just	O
inside	O
a	O
binary	O
interval	O
.	O
in	O
this	O
case	O
,	O
we	O
may	O
choose	O
either	O
of	O
two	O
binary	O
intervals	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
6.10.	O
these	O
binary	O
intervals	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
128	O
6	O
|	O
stream	B
codes	I
source	O
string	O
’	O
s	O
interval	O
binary	O
intervals	O
6	O
p	O
(	O
xjh	O
)	O
?	O
6	O
?	O
6	O
?	O
figure	O
6.10.	O
termination	B
of	O
arithmetic	B
coding	I
in	O
the	O
worst	O
case	O
,	O
where	O
there	O
is	O
a	O
two	O
bit	B
overhead	O
.	O
either	O
of	O
the	O
two	O
binary	O
intervals	O
marked	O
on	O
the	O
right-hand	O
side	O
may	O
be	O
chosen	O
.	O
these	O
binary	O
intervals	O
are	O
no	O
smaller	O
than	O
p	O
(	O
xjh	O
)	O
=4	O
.	O
are	O
no	O
smaller	O
than	O
p	O
(	O
xjh	O
)	O
=4	O
,	O
so	O
the	O
binary	O
encoding	O
has	O
a	O
length	B
no	O
greater	O
than	O
log2	O
1=p	O
(	O
xjh	O
)	O
+	O
log2	O
4	O
,	O
which	O
is	O
two	O
bits	O
more	O
than	O
the	O
ideal	O
message	O
length	O
.	O
solution	O
to	O
exercise	O
6.3	O
(	O
p.118	O
)	O
.	O
the	O
standard	O
method	O
uses	O
32	O
random	B
bits	O
per	O
generated	O
symbol	O
and	O
so	O
requires	O
32	O
000	O
bits	O
to	O
generate	O
one	O
thousand	O
samples	O
.	O
arithmetic	B
coding	I
uses	O
on	O
average	O
about	O
h2	O
(	O
0:01	O
)	O
=	O
0:081	O
bits	O
per	O
gener-	O
ated	O
symbol	O
,	O
and	O
so	O
requires	O
about	O
83	O
bits	O
to	O
generate	O
one	O
thousand	O
samples	O
(	O
assuming	O
an	O
overhead	O
of	O
roughly	O
two	O
bits	O
associated	O
with	O
termination	O
)	O
.	O
fluctuations	O
in	O
the	O
number	O
of	O
1s	O
would	O
produce	O
variations	O
around	O
this	O
mean	B
with	O
standard	B
deviation	I
21.	O
solution	O
to	O
exercise	O
6.5	O
(	O
p.120	O
)	O
.	O
the	O
encoding	O
is	O
010100110010110001100	O
,	O
which	O
comes	O
from	O
the	O
parsing	O
0	O
;	O
00	O
;	O
000	O
;	O
0000	O
;	O
001	O
;	O
00000	O
;	O
000000	O
(	O
6.23	O
)	O
which	O
is	O
encoded	O
thus	O
:	O
(	O
;	O
0	O
)	O
;	O
(	O
1	O
;	O
0	O
)	O
;	O
(	O
10	O
;	O
0	O
)	O
;	O
(	O
11	O
;	O
0	O
)	O
;	O
(	O
010	O
;	O
1	O
)	O
;	O
(	O
100	O
;	O
0	O
)	O
;	O
(	O
110	O
;	O
0	O
)	O
:	O
(	O
6.24	O
)	O
solution	O
to	O
exercise	O
6.6	O
(	O
p.120	O
)	O
.	O
the	O
decoding	B
is	O
0100001000100010101000001.	O
solution	O
to	O
exercise	O
6.8	O
(	O
p.123	O
)	O
.	O
this	O
problem	O
is	O
equivalent	O
to	O
exercise	O
6.7	O
(	O
p.123	O
)	O
.	O
k	O
(	O
cid:1	O
)	O
e	O
bits	O
’	O
the	O
selection	O
of	O
k	O
objects	O
from	O
n	O
objects	O
requires	O
dlog	O
2	O
(	O
cid:0	O
)	O
n	O
n	O
h2	O
(	O
k=n	O
)	O
bits	O
.	O
this	O
selection	O
could	O
be	O
made	O
using	O
arithmetic	B
coding	I
.	O
the	O
selection	O
corresponds	O
to	O
a	O
binary	O
string	O
of	O
length	O
n	O
in	O
which	O
the	O
1	O
bits	O
rep-	O
resent	O
which	O
objects	O
are	O
selected	O
.	O
initially	O
the	O
probability	O
of	O
a	O
1	O
is	O
k=n	O
and	O
the	O
probability	O
of	O
a	O
0	O
is	O
(	O
n	O
(	O
cid:0	O
)	O
k	O
)	O
=n	O
.	O
thereafter	O
,	O
given	O
that	O
the	O
emitted	O
string	O
thus	O
far	O
,	O
of	O
length	O
n	O
,	O
contains	O
k	O
1s	O
,	O
the	O
probability	O
of	O
a	O
1	O
is	O
(	O
k	O
(	O
cid:0	O
)	O
k	O
)	O
=	O
(	O
n	O
(	O
cid:0	O
)	O
n	O
)	O
and	O
the	O
probability	O
of	O
a	O
0	O
is	O
1	O
(	O
cid:0	O
)	O
(	O
k	O
(	O
cid:0	O
)	O
k	O
)	O
=	O
(	O
n	O
(	O
cid:0	O
)	O
n	O
)	O
.	O
solution	O
to	O
exercise	O
6.12	O
(	O
p.124	O
)	O
.	O
this	O
modi	O
(	O
cid:12	O
)	O
ed	O
lempel	O
{	O
ziv	O
code	B
is	O
still	O
not	O
‘	O
complete	O
’	O
,	O
because	O
,	O
for	O
example	O
,	O
after	O
(	O
cid:12	O
)	O
ve	O
pre	O
(	O
cid:12	O
)	O
xes	O
have	O
been	O
collected	O
,	O
the	O
pointer	B
could	O
be	O
any	O
of	O
the	O
strings	O
000	O
,	O
001	O
,	O
010	O
,	O
011	O
,	O
100	O
,	O
but	O
it	O
can	O
not	O
be	O
101	O
,	O
110	O
or	O
111.	O
thus	O
there	O
are	O
some	O
binary	O
strings	O
that	O
can	O
not	O
be	O
produced	O
as	O
encodings	O
.	O
solution	O
to	O
exercise	O
6.13	O
(	O
p.124	O
)	O
.	O
sources	O
with	O
low	O
entropy	B
that	O
are	O
not	O
well	O
compressed	O
by	O
lempel	O
{	O
ziv	O
include	O
:	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
6.9	O
:	O
solutions	O
129	O
(	O
a	O
)	O
sources	O
with	O
some	O
symbols	O
that	O
have	O
long	O
range	O
correlations	B
and	O
inter-	O
vening	O
random	B
junk	O
.	O
an	O
ideal	O
model	B
should	O
capture	O
what	O
’	O
s	O
correlated	O
and	O
compress	B
it	O
.	O
lempel	O
{	O
ziv	O
can	O
compress	B
the	O
correlated	O
features	O
only	O
by	O
memorizing	O
all	O
cases	O
of	O
the	O
intervening	O
junk	O
.	O
as	O
a	O
simple	O
example	O
,	O
consider	O
a	O
telephone	B
book	O
in	O
which	O
every	O
line	O
contains	O
an	O
(	O
old	O
number	O
,	O
new	O
number	O
)	O
pair	O
:	O
285-3820:572-58922	O
258-8302:593-20102	O
the	O
number	O
of	O
characters	O
per	O
line	O
is	O
18	O
,	O
drawn	O
from	O
the	O
13-character	O
alphabet	O
f0	O
;	O
1	O
;	O
:	O
:	O
:	O
;	O
9	O
;	O
(	O
cid:0	O
)	O
;	O
:	O
;	O
2g	O
.	O
the	O
characters	O
‘	O
-	O
’	O
,	O
‘	O
:	O
’	O
and	O
‘	O
2	O
’	O
occur	O
in	O
a	O
predictable	O
sequence	B
,	O
so	O
the	O
true	O
information	B
content	I
per	O
line	O
,	O
assuming	O
all	O
the	O
phone	B
numbers	O
are	O
seven	O
digits	O
long	O
,	O
and	O
assuming	O
that	O
they	O
are	O
random	B
sequences	O
,	O
is	O
about	O
14	O
bans	O
.	O
(	O
a	O
ban	O
is	O
the	O
information	B
content	I
of	O
a	O
random	B
integer	O
between	O
0	O
and	O
9	O
.	O
)	O
a	O
(	O
cid:12	O
)	O
nite	O
state	O
language	O
model	B
could	O
easily	O
capture	O
the	O
regularities	O
in	O
these	O
data	O
.	O
a	O
lempel	O
{	O
ziv	O
algorithm	B
will	O
take	O
a	O
long	O
time	O
before	O
it	O
compresses	O
such	O
a	O
(	O
cid:12	O
)	O
le	O
down	O
to	O
14	O
bans	O
per	O
line	O
,	O
however	O
,	O
because	O
in	O
order	O
for	O
it	O
to	O
‘	O
learn	O
’	O
that	O
the	O
string	O
:	O
ddd	O
is	O
always	O
followed	O
by	O
-	O
,	O
for	O
any	O
three	O
digits	O
ddd	O
,	O
it	O
will	O
have	O
to	O
see	O
all	O
those	O
strings	O
.	O
so	O
near-optimal	O
compression	B
will	O
only	O
be	O
achieved	O
after	O
thousands	O
of	O
lines	O
of	O
the	O
(	O
cid:12	O
)	O
le	O
have	O
been	O
read	O
.	O
figure	O
6.11.	O
a	O
source	O
with	O
low	O
entropy	B
that	O
is	O
not	O
well	O
compressed	O
by	O
lempel	O
{	O
ziv	O
.	O
the	O
bit	B
sequence	O
is	O
read	O
from	O
left	O
to	O
right	O
.	O
each	O
line	O
di	O
(	O
cid:11	O
)	O
ers	O
from	O
the	O
line	O
above	O
in	O
f	O
=	O
5	O
%	O
of	O
its	O
bits	O
.	O
the	O
image	B
width	O
is	O
400	O
pixels	O
.	O
(	O
b	O
)	O
sources	O
with	O
long	O
range	O
correlations	B
,	O
for	O
example	O
two-dimensional	B
im-	O
ages	O
that	O
are	O
represented	O
by	O
a	O
sequence	B
of	O
pixels	O
,	O
row	O
by	O
row	O
,	O
so	O
that	O
vertically	O
adjacent	O
pixels	O
are	O
a	O
distance	B
w	O
apart	O
in	O
the	O
source	O
stream	O
,	O
where	O
w	O
is	O
the	O
image	B
width	O
.	O
consider	O
,	O
for	O
example	O
,	O
a	O
fax	O
transmission	O
in	O
which	O
each	O
line	O
is	O
very	O
similar	O
to	O
the	O
previous	O
line	O
(	O
(	O
cid:12	O
)	O
gure	O
6.11	O
)	O
.	O
the	O
true	O
entropy	B
is	O
only	O
h2	O
(	O
f	O
)	O
per	O
pixel	O
,	O
where	O
f	O
is	O
the	O
probability	B
that	O
a	O
pixel	O
di	O
(	O
cid:11	O
)	O
ers	O
from	O
its	O
parent	B
.	O
lempel	O
{	O
ziv	O
algorithms	B
will	O
only	O
compress	B
down	O
to	O
the	O
entropy	B
once	O
all	O
strings	O
of	O
length	O
2w	O
=	O
2400	O
have	O
occurred	O
and	O
their	O
successors	O
have	O
been	O
memorized	O
.	O
there	O
are	O
only	O
about	O
2300	O
par-	O
ticles	O
in	O
the	O
universe	O
,	O
so	O
we	O
can	O
con	O
(	O
cid:12	O
)	O
dently	O
say	O
that	O
lempel	O
{	O
ziv	O
codes	O
will	O
never	O
capture	O
the	O
redundancy	B
of	O
such	O
an	O
image	B
.	O
another	O
highly	O
redundant	O
texture	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
6.12.	O
the	O
image	B
was	O
made	O
by	O
dropping	O
horizontal	O
and	O
vertical	O
pins	O
randomly	O
on	O
the	O
plane	O
.	O
it	O
contains	O
both	O
long-range	O
vertical	O
correlations	B
and	O
long-range	O
horizontal	O
correlations	B
.	O
there	O
is	O
no	O
practical	B
way	O
that	O
lempel	O
{	O
ziv	O
,	O
fed	O
with	O
a	O
pixel-by-pixel	O
scan	O
of	O
this	O
image	B
,	O
could	O
capture	O
both	O
these	O
correlations	B
.	O
biological	O
computational	O
systems	O
can	O
readily	O
identify	O
the	O
redundancy	B
in	O
these	O
images	B
and	O
in	B
images	I
that	O
are	O
much	O
more	O
complex	B
;	O
thus	O
we	O
might	O
anticipate	O
that	O
the	O
best	O
data	B
compression	I
algorithms	O
will	O
result	O
from	O
the	O
development	O
of	O
arti	O
(	O
cid:12	O
)	O
cial	O
intelligence	O
methods	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
130	O
6	O
|	O
stream	B
codes	I
figure	O
6.12.	O
a	O
texture	O
consisting	O
of	O
horizontal	O
and	O
vertical	O
pins	O
dropped	O
at	O
random	B
on	O
the	O
plane	O
.	O
(	O
c	O
)	O
sources	O
with	O
intricate	O
redundancy	B
,	O
such	O
as	O
(	O
cid:12	O
)	O
les	O
generated	O
by	O
computers	O
.	O
for	O
example	O
,	O
a	O
latex	O
(	O
cid:12	O
)	O
le	O
followed	O
by	O
its	O
encoding	O
into	O
a	O
postscript	O
(	O
cid:12	O
)	O
le	O
.	O
the	O
information	B
content	I
of	O
this	O
pair	O
of	O
(	O
cid:12	O
)	O
les	O
is	O
roughly	O
equal	O
to	O
the	O
information	B
content	I
of	O
the	O
latex	O
(	O
cid:12	O
)	O
le	O
alone	O
.	O
(	O
d	O
)	O
a	O
picture	O
of	O
the	O
mandelbrot	O
set	B
.	O
the	O
picture	O
has	O
an	O
information	B
content	I
equal	O
to	O
the	O
number	O
of	O
bits	O
required	O
to	O
specify	O
the	O
range	O
of	O
the	O
complex	O
plane	O
studied	O
,	O
the	O
pixel	O
sizes	O
,	O
and	O
the	O
colouring	O
rule	O
used	O
.	O
(	O
e	O
)	O
a	O
picture	O
of	O
a	O
ground	O
state	O
of	O
a	O
frustrated	O
antiferromagnetic	B
ising	O
model	B
(	O
(	O
cid:12	O
)	O
gure	O
6.13	O
)	O
,	O
which	O
we	O
will	O
discuss	O
in	O
chapter	O
31.	O
like	O
(	O
cid:12	O
)	O
gure	O
6.12	O
,	O
this	O
binary	O
image	O
has	O
interesting	O
correlations	B
in	O
two	O
directions	O
.	O
figure	O
6.13.	O
frustrated	O
triangular	O
ising	O
model	B
in	O
one	O
of	O
its	O
ground	O
states	O
.	O
(	O
f	O
)	O
cellular	B
automata	O
{	O
(	O
cid:12	O
)	O
gure	O
6.14	O
shows	O
the	O
state	O
history	O
of	O
100	O
steps	O
of	O
a	O
cellular	B
automaton	I
with	O
400	O
cells	O
.	O
the	O
update	O
rule	O
,	O
in	O
which	O
each	O
cell	O
’	O
s	O
new	O
state	O
depends	O
on	O
the	O
state	O
of	O
(	O
cid:12	O
)	O
ve	O
preceding	O
cells	O
,	O
was	O
selected	O
at	O
random	B
.	O
the	O
information	B
content	I
is	O
equal	O
to	O
the	O
information	B
in	O
the	O
boundary	O
(	O
400	O
bits	O
)	O
,	O
and	O
the	O
propagation	O
rule	O
,	O
which	O
here	O
can	O
be	O
de-	O
scribed	O
in	O
32	O
bits	O
.	O
an	O
optimal	B
compressor	O
will	O
thus	O
give	O
a	O
compressed	O
(	O
cid:12	O
)	O
le	O
length	B
which	O
is	O
essentially	O
constant	O
,	O
independent	O
of	O
the	O
vertical	O
height	O
of	O
the	O
image	O
.	O
lempel	O
{	O
ziv	O
would	O
only	O
give	O
this	O
zero-cost	O
compression	B
once	O
the	O
cellular	B
automaton	I
has	O
entered	O
a	O
periodic	O
limit	O
cycle	O
,	O
which	O
could	O
easily	O
take	O
about	O
2100	O
iterations	O
.	O
in	O
contrast	O
,	O
the	O
jbig	O
compression	B
method	O
,	O
which	O
models	O
the	O
probability	O
of	O
a	O
pixel	O
given	O
its	O
local	O
context	O
and	O
uses	O
arithmetic	B
coding	I
,	O
would	O
do	O
a	O
good	B
job	O
on	O
these	O
images	B
.	O
solution	O
to	O
exercise	O
6.14	O
(	O
p.124	O
)	O
.	O
for	O
a	O
one-dimensional	O
gaussian	O
,	O
the	O
vari-	O
ance	O
of	O
x	O
,	O
e	O
[	O
x2	O
]	O
,	O
is	O
(	O
cid:27	O
)	O
2.	O
so	O
the	O
mean	B
value	O
of	O
r2	O
in	O
n	O
dimensions	B
,	O
since	O
the	O
components	O
of	O
x	O
are	O
independent	O
random	O
variables	O
,	O
is	O
e	O
[	O
r2	O
]	O
=	O
n	O
(	O
cid:27	O
)	O
2	O
:	O
(	O
6.25	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
6.9	O
:	O
solutions	O
131	O
figure	O
6.14.	O
the	O
100-step	O
time-history	O
of	O
a	O
cellular	B
automaton	I
with	O
400	O
cells	O
.	O
the	O
variance	B
of	O
r2	O
,	O
similarly	O
,	O
is	O
n	O
times	O
the	O
variance	B
of	O
x2	O
,	O
where	O
x	O
is	O
a	O
one-dimensional	O
gaussian	O
variable	O
.	O
var	O
(	O
x2	O
)	O
=z	O
dx	O
1	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
)	O
1=2	O
x4	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
x2	O
2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:19	O
)	O
(	O
cid:0	O
)	O
(	O
cid:27	O
)	O
4	O
:	O
(	O
6.26	O
)	O
the	O
integral	B
is	O
found	O
to	O
be	O
3	O
(	O
cid:27	O
)	O
4	O
(	O
equation	O
(	O
6.14	O
)	O
)	O
,	O
so	O
var	O
(	O
x2	O
)	O
=	O
2	O
(	O
cid:27	O
)	O
4.	O
thus	O
the	O
variance	B
of	O
r2	O
is	O
2n	O
(	O
cid:27	O
)	O
4.	O
for	O
large	O
n	O
,	O
the	O
central-limit	B
theorem	I
indicates	O
that	O
r	O
2	O
has	O
a	O
gaussian	O
distribution	B
with	O
mean	B
n	O
(	O
cid:27	O
)	O
2	O
and	O
standard	O
deviation	O
p2n	O
(	O
cid:27	O
)	O
2	O
,	O
so	O
the	O
probability	B
density	O
of	O
r	O
must	O
similarly	O
be	O
concentrated	O
about	O
r	O
’	O
pn	O
(	O
cid:27	O
)	O
.	O
the	O
thickness	O
of	O
this	O
shell	O
is	O
given	O
by	O
turning	O
the	O
standard	B
deviation	I
of	O
r2	O
into	O
a	O
standard	B
deviation	I
on	O
r	O
:	O
for	O
small	O
(	O
cid:14	O
)	O
r=r	O
,	O
(	O
cid:14	O
)	O
log	O
r	O
=	O
(	O
cid:14	O
)	O
r=r	O
=	O
(	O
1/2	O
)	O
(	O
cid:14	O
)	O
log	O
r2	O
=	O
(	O
1/2	O
)	O
(	O
cid:14	O
)	O
(	O
r2	O
)	O
=r2	O
,	O
so	O
setting	O
(	O
cid:14	O
)	O
(	O
r2	O
)	O
=	O
p2n	O
(	O
cid:27	O
)	O
2	O
,	O
r	O
has	O
standard	O
de-	O
viation	O
(	O
cid:14	O
)	O
r	O
=	O
(	O
1/2	O
)	O
r	O
(	O
cid:14	O
)	O
(	O
r2	O
)	O
=r2	O
=	O
(	O
cid:27	O
)	O
=p2	O
.	O
the	O
probability	B
density	O
of	O
the	O
gaussian	O
at	O
a	O
point	O
xshell	O
where	O
r	O
=	O
pn	O
(	O
cid:27	O
)	O
is	O
p	O
(	O
xshell	O
)	O
=	O
1	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
)	O
n=2	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
n	O
(	O
cid:27	O
)	O
2	O
2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:19	O
)	O
=	O
1	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
)	O
n=2	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
n	O
2	O
(	O
cid:19	O
)	O
:	O
whereas	O
the	O
probability	B
density	O
at	O
the	O
origin	O
is	O
p	O
(	O
x	O
=	O
0	O
)	O
=	O
1	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
)	O
n=2	O
:	O
(	O
6.27	O
)	O
(	O
6.28	O
)	O
thus	O
p	O
(	O
xshell	O
)	O
=p	O
(	O
x	O
=	O
0	O
)	O
=	O
exp	O
(	O
(	O
cid:0	O
)	O
n=2	O
)	O
:	O
the	O
probability	B
density	O
at	O
the	O
typical	B
radius	O
is	O
e	O
(	O
cid:0	O
)	O
n=2	O
times	O
smaller	O
than	O
the	O
density	B
at	O
the	O
origin	O
.	O
if	O
n	O
=	O
1000	O
,	O
then	O
the	O
probability	B
density	O
at	O
the	O
origin	O
is	O
e500	O
times	O
greater	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
7	O
codes	O
for	B
integers	I
this	O
chapter	O
is	O
an	O
aside	O
,	O
which	O
may	O
safely	O
be	O
skipped	O
.	O
solution	O
to	O
exercise	O
6.22	O
(	O
p.127	O
)	O
to	O
discuss	O
the	O
coding	O
of	O
integers	O
we	O
need	O
some	O
de	O
(	O
cid:12	O
)	O
nitions	O
.	O
the	O
standard	O
binary	O
representation	O
of	O
a	O
positive	O
integer	O
n	O
will	O
be	O
denoted	O
by	O
cb	O
(	O
n	O
)	O
,	O
e.g.	O
,	O
cb	O
(	O
5	O
)	O
=	O
101	O
,	O
cb	O
(	O
45	O
)	O
=	O
101101.	O
the	O
standard	O
binary	O
length	B
of	O
a	O
positive	O
integer	O
n	O
,	O
lb	O
(	O
n	O
)	O
,	O
length	B
of	O
the	O
string	O
cb	O
(	O
n	O
)	O
.	O
for	O
example	O
,	O
lb	O
(	O
5	O
)	O
=	O
3	O
,	O
lb	O
(	O
45	O
)	O
=	O
6.	O
is	O
the	O
the	O
standard	O
binary	O
representation	O
cb	O
(	O
n	O
)	O
is	O
not	O
a	O
uniquely	B
decodeable	I
code	O
for	B
integers	I
since	O
there	O
is	O
no	O
way	O
of	O
knowing	O
when	O
an	O
integer	O
has	O
ended	O
.	O
for	O
example	O
,	O
cb	O
(	O
5	O
)	O
cb	O
(	O
5	O
)	O
is	O
identical	O
to	O
cb	O
(	O
45	O
)	O
.	O
it	O
would	O
be	O
uniquely	B
decodeable	I
if	O
we	O
knew	O
the	O
standard	O
binary	O
length	B
of	O
each	O
integer	O
before	O
it	O
was	O
received	O
.	O
noticing	O
that	O
all	O
positive	O
integers	O
have	O
a	O
standard	O
binary	O
representation	O
that	O
starts	O
with	O
a	O
1	O
,	O
we	O
might	O
de	O
(	O
cid:12	O
)	O
ne	O
another	O
representation	O
:	O
the	O
headless	O
binary	O
representation	O
of	O
a	O
positive	O
integer	O
n	O
will	O
be	O
de-	O
noted	O
by	O
cb	O
(	O
n	O
)	O
,	O
e.g.	O
,	O
cb	O
(	O
5	O
)	O
=	O
01	O
,	O
cb	O
(	O
45	O
)	O
=	O
01101	O
and	O
cb	O
(	O
1	O
)	O
=	O
(	O
cid:21	O
)	O
(	O
where	O
(	O
cid:21	O
)	O
denotes	O
the	O
null	O
string	O
)	O
.	O
this	O
representation	O
would	O
be	O
uniquely	B
decodeable	I
if	O
we	O
knew	O
the	O
length	B
lb	O
(	O
n	O
)	O
of	O
the	O
integer	O
.	O
so	O
,	O
how	O
can	O
we	O
make	O
a	O
uniquely	B
decodeable	I
code	O
for	B
integers	I
?	O
two	O
strate-	O
gies	O
can	O
be	O
distinguished	O
.	O
1.	O
self-delimiting	B
codes	O
.	O
we	O
(	O
cid:12	O
)	O
rst	O
communicate	O
somehow	O
the	O
length	B
of	O
the	O
integer	O
,	O
lb	O
(	O
n	O
)	O
,	O
which	O
is	O
also	O
a	O
positive	O
integer	O
;	O
then	O
communicate	O
the	O
original	O
integer	O
n	O
itself	O
using	O
cb	O
(	O
n	O
)	O
.	O
2.	O
codes	O
with	O
‘	O
end	O
of	O
(	O
cid:12	O
)	O
le	O
’	O
characters	O
.	O
we	O
code	B
the	O
integer	O
into	O
blocks	O
of	O
length	O
b	O
bits	O
,	O
and	O
reserve	O
one	O
of	O
the	O
2b	O
symbols	O
to	O
have	O
the	O
special	O
meaning	O
‘	O
end	O
of	O
(	O
cid:12	O
)	O
le	O
’	O
.	O
the	O
coding	O
of	O
integers	O
into	O
blocks	O
is	O
arranged	O
so	O
that	O
this	O
reserved	O
symbol	O
is	O
not	O
needed	O
for	O
any	O
other	O
purpose	O
.	O
the	O
simplest	O
uniquely	B
decodeable	I
code	O
for	B
integers	I
is	O
the	O
unary	O
code	B
,	O
which	O
can	O
be	O
viewed	O
as	O
a	O
code	B
with	O
an	O
end	O
of	O
(	O
cid:12	O
)	O
le	O
character	O
.	O
132	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
7	O
|	O
codes	O
for	B
integers	I
133	O
unary	O
code	B
.	O
an	O
integer	O
n	O
is	O
encoded	O
by	O
sending	O
a	O
string	O
of	O
n	O
(	O
cid:0	O
)	O
1	O
0s	O
followed	O
by	O
a	O
1.	O
n	O
cu	O
(	O
n	O
)	O
1	O
01	O
001	O
0001	O
00001	O
1	O
2	O
3	O
4	O
5	O
...	O
45	O
000000000000000000000000000000000000000000001	O
the	O
unary	O
code	B
has	O
length	B
lu	O
(	O
n	O
)	O
=	O
n.	O
the	O
unary	O
code	B
is	O
the	O
optimal	B
code	O
for	B
integers	I
if	O
the	O
probability	B
distri-	O
bution	O
over	O
n	O
is	O
pu	O
(	O
n	O
)	O
=	O
2	O
(	O
cid:0	O
)	O
n.	O
self-delimiting	B
codes	O
we	O
can	O
use	O
the	O
unary	O
code	B
to	O
encode	O
the	O
length	B
of	O
the	O
binary	O
encoding	O
of	O
n	O
and	O
make	O
a	O
self-delimiting	B
code	O
:	O
code	B
c	O
(	O
cid:11	O
)	O
.	O
we	O
send	O
the	O
unary	O
code	B
for	O
lb	O
(	O
n	O
)	O
,	O
followed	O
by	O
the	O
headless	O
binary	O
representation	O
of	O
n.	O
c	O
(	O
cid:11	O
)	O
(	O
n	O
)	O
=	O
cu	O
[	O
lb	O
(	O
n	O
)	O
]	O
cb	O
(	O
n	O
)	O
:	O
(	O
7.1	O
)	O
table	O
7.1	O
shows	O
the	O
codes	O
for	O
some	O
integers	O
.	O
the	O
overlining	O
indicates	O
the	O
division	O
of	O
each	O
string	O
into	O
the	O
parts	O
cu	O
[	O
lb	O
(	O
n	O
)	O
]	O
and	O
cb	O
(	O
n	O
)	O
.	O
we	O
might	O
equivalently	O
view	O
c	O
(	O
cid:11	O
)	O
(	O
n	O
)	O
as	O
consisting	O
of	O
a	O
string	O
of	O
(	O
lb	O
(	O
n	O
)	O
(	O
cid:0	O
)	O
1	O
)	O
zeroes	O
followed	O
by	O
the	O
standard	O
binary	O
representation	O
of	O
n	O
,	O
cb	O
(	O
n	O
)	O
.	O
the	O
codeword	B
c	O
(	O
cid:11	O
)	O
(	O
n	O
)	O
has	O
length	B
l	O
(	O
cid:11	O
)	O
(	O
n	O
)	O
=	O
2lb	O
(	O
n	O
)	O
(	O
cid:0	O
)	O
1.	O
the	O
implicit	O
probability	O
distribution	B
over	O
n	O
for	O
the	O
code	B
c	O
(	O
cid:11	O
)	O
is	O
separable	O
into	O
the	O
product	O
of	O
a	O
probability	B
distribution	O
over	O
the	O
length	B
l	O
,	O
and	O
a	O
uniform	O
distribution	B
over	O
integers	O
having	O
that	O
length	B
,	O
p	O
(	O
l	O
)	O
=	O
2	O
(	O
cid:0	O
)	O
l	O
;	O
p	O
(	O
nj	O
l	O
)	O
=	O
(	O
cid:26	O
)	O
2	O
(	O
cid:0	O
)	O
l+1	O
0	O
lb	O
(	O
n	O
)	O
=	O
l	O
otherwise	O
:	O
(	O
7.2	O
)	O
(	O
7.3	O
)	O
now	O
,	O
for	O
the	O
above	O
code	B
,	O
the	O
header	O
that	O
communicates	O
the	O
length	B
always	O
occupies	O
the	O
same	O
number	O
of	O
bits	O
as	O
the	O
standard	O
binary	O
representation	O
of	O
the	O
integer	O
(	O
give	O
or	O
take	O
one	O
)	O
.	O
if	O
we	O
are	O
expecting	O
to	O
encounter	O
large	O
integers	O
(	O
large	O
(	O
cid:12	O
)	O
les	O
)	O
then	O
this	O
representation	O
seems	O
suboptimal	O
,	O
since	O
it	O
leads	O
to	O
all	O
(	O
cid:12	O
)	O
les	O
occupying	O
a	O
size	O
that	O
is	O
double	O
their	O
original	O
uncoded	O
size	O
.	O
instead	O
of	O
using	O
the	O
unary	O
code	B
to	O
encode	O
the	O
length	B
lb	O
(	O
n	O
)	O
,	O
we	O
could	O
use	O
c	O
(	O
cid:11	O
)	O
.	O
code	B
c	O
(	O
cid:12	O
)	O
.	O
we	O
send	O
the	O
length	B
lb	O
(	O
n	O
)	O
using	O
c	O
(	O
cid:11	O
)	O
,	O
followed	O
by	O
the	O
headless	O
binary	O
representation	O
of	O
n.	O
c	O
(	O
cid:12	O
)	O
(	O
n	O
)	O
=	O
c	O
(	O
cid:11	O
)	O
[	O
lb	O
(	O
n	O
)	O
]	O
cb	O
(	O
n	O
)	O
:	O
iterating	O
this	O
procedure	O
,	O
we	O
can	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
sequence	B
of	O
codes	O
.	O
code	B
c	O
(	O
cid:13	O
)	O
.	O
code	B
c	O
(	O
cid:14	O
)	O
.	O
c	O
(	O
cid:13	O
)	O
(	O
n	O
)	O
=	O
c	O
(	O
cid:12	O
)	O
[	O
lb	O
(	O
n	O
)	O
]	O
cb	O
(	O
n	O
)	O
:	O
c	O
(	O
cid:14	O
)	O
(	O
n	O
)	O
=	O
c	O
(	O
cid:13	O
)	O
[	O
lb	O
(	O
n	O
)	O
]	O
cb	O
(	O
n	O
)	O
:	O
(	O
7.4	O
)	O
(	O
7.5	O
)	O
(	O
7.6	O
)	O
n	O
1	O
2	O
3	O
4	O
5	O
6	O
...	O
45	O
cb	O
(	O
n	O
)	O
lb	O
(	O
n	O
)	O
c	O
(	O
cid:11	O
)	O
(	O
n	O
)	O
1	O
10	O
11	O
100	O
101	O
110	O
101101	O
1	O
2	O
2	O
3	O
3	O
3	O
6	O
1	O
010	O
011	O
00100	O
00101	O
00110	O
00000101101	O
table	O
7.1.	O
c	O
(	O
cid:11	O
)	O
.	O
n	O
1	O
2	O
3	O
4	O
5	O
6	O
...	O
45	O
c	O
(	O
cid:12	O
)	O
(	O
n	O
)	O
1	O
0100	O
0101	O
01100	O
01101	O
01110	O
c	O
(	O
cid:13	O
)	O
(	O
n	O
)	O
1	O
01000	O
01001	O
010100	O
010101	O
010110	O
0011001101	O
0111001101	O
table	O
7.2.	O
c	O
(	O
cid:12	O
)	O
and	O
c	O
(	O
cid:13	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
7	O
|	O
codes	O
for	B
integers	I
n	O
c3	O
(	O
n	O
)	O
c7	O
(	O
n	O
)	O
001	O
111	O
010	O
111	O
011	O
111	O
01	O
11	O
10	O
11	O
01	O
00	O
11	O
1	O
2	O
3	O
...	O
45	O
01	O
10	O
00	O
00	O
11	O
110	O
011	O
111	O
table	O
7.3.	O
two	O
codes	O
with	O
end-of-	O
(	O
cid:12	O
)	O
le	O
symbols	O
,	O
c3	O
and	O
c7	O
.	O
spaces	O
have	O
been	O
included	O
to	O
show	O
the	O
byte	B
boundaries	O
.	O
134	O
codes	O
with	O
end-of-	O
(	O
cid:12	O
)	O
le	O
symbols	O
we	O
can	O
also	O
make	O
byte-based	O
representations	O
.	O
(	O
let	O
’	O
s	O
use	O
the	O
term	O
byte	B
(	O
cid:13	O
)	O
exibly	O
here	O
,	O
to	O
denote	O
any	O
(	O
cid:12	O
)	O
xed-length	O
string	O
of	O
bits	O
,	O
not	O
just	O
a	O
string	O
of	O
length	O
8	O
bits	O
.	O
)	O
if	O
we	O
encode	O
the	O
number	O
in	O
some	O
base	O
,	O
for	O
example	O
decimal	O
,	O
then	O
we	O
can	O
represent	O
each	O
digit	O
in	O
a	O
byte	B
.	O
in	O
order	O
to	O
represent	O
a	O
digit	O
from	O
0	O
to	O
9	O
in	O
a	O
byte	B
we	O
need	O
four	O
bits	O
.	O
because	O
24	O
=	O
16	O
,	O
this	O
leaves	O
6	O
extra	O
four-bit	O
symbols	O
,	O
f1010	O
,	O
1011	O
,	O
1100	O
,	O
1101	O
,	O
1110	O
,	O
1111g	O
,	O
that	O
correspond	O
to	O
no	O
decimal	O
digit	O
.	O
we	O
can	O
use	O
these	O
as	O
end-of-	O
(	O
cid:12	O
)	O
le	O
symbols	O
to	O
indicate	O
the	O
end	O
of	O
our	O
positive	O
integer	O
.	O
clearly	O
it	O
is	O
redundant	O
to	O
have	O
more	O
than	O
one	O
end-of-	O
(	O
cid:12	O
)	O
le	O
symbol	O
,	O
so	O
a	O
more	O
e	O
(	O
cid:14	O
)	O
cient	O
code	B
would	O
encode	O
the	O
integer	O
into	O
base	O
15	O
,	O
and	O
use	O
just	O
the	O
sixteenth	O
symbol	O
,	O
1111	O
,	O
as	O
the	O
punctuation	O
character	O
.	O
generalizing	O
this	O
idea	O
,	O
we	O
can	O
make	O
similar	O
byte-based	O
codes	O
for	B
integers	I
in	O
bases	O
3	O
and	O
7	O
,	O
and	O
in	O
any	O
base	O
of	O
the	O
form	O
2n	O
(	O
cid:0	O
)	O
1.	O
these	O
codes	O
are	O
almost	O
complete	O
.	O
(	O
recall	O
that	O
a	O
code	B
is	O
‘	O
complete	O
’	O
if	O
it	O
satis	O
(	O
cid:12	O
)	O
es	O
the	O
kraft	O
inequality	B
with	O
equality	O
.	O
)	O
the	O
codes	O
’	O
remaining	O
ine	O
(	O
cid:14	O
)	O
ciency	O
is	O
that	O
they	O
provide	O
the	O
ability	O
to	O
encode	O
the	O
integer	O
zero	O
and	O
the	O
empty	B
string	I
,	O
neither	O
of	O
which	O
was	O
required	O
.	O
.	O
exercise	O
7.1	O
.	O
[	O
2	O
,	O
p.136	O
]	O
consider	O
the	O
implicit	O
probability	O
distribution	B
over	O
inte-	O
gers	O
corresponding	O
to	O
the	O
code	B
with	O
an	O
end-of-	O
(	O
cid:12	O
)	O
le	O
character	O
.	O
(	O
a	O
)	O
if	O
the	O
code	B
has	O
eight-bit	O
blocks	O
(	O
i.e.	O
,	O
the	O
integer	O
is	O
coded	O
in	O
base	O
255	O
)	O
,	O
what	O
is	O
the	O
mean	B
length	O
in	O
bits	O
of	O
the	O
integer	O
,	O
under	O
the	O
implicit	O
distribution	O
?	O
(	O
b	O
)	O
if	O
one	O
wishes	O
to	O
encode	O
binary	O
(	O
cid:12	O
)	O
les	O
of	O
expected	O
size	O
about	O
one	O
hun-	O
dred	O
kilobytes	O
using	O
a	O
code	B
with	O
an	O
end-of-	O
(	O
cid:12	O
)	O
le	O
character	O
,	O
what	O
is	O
the	O
optimal	B
block	O
size	O
?	O
encoding	O
a	O
tiny	O
(	O
cid:12	O
)	O
le	O
to	O
illustrate	O
the	O
codes	O
we	O
have	O
discussed	O
,	O
we	O
now	O
use	O
each	O
code	B
to	O
encode	O
a	O
small	O
(	O
cid:12	O
)	O
le	O
consisting	O
of	O
just	O
14	O
characters	O
,	O
claude	O
shannon	O
:	O
(	O
cid:15	O
)	O
if	O
we	O
map	O
the	O
ascii	O
characters	O
onto	O
seven-bit	O
symbols	O
(	O
e.g.	O
,	O
in	O
decimal	O
,	O
c	O
=	O
67	O
,	O
l	O
=	O
108	O
,	O
etc	O
.	O
)	O
,	O
this	O
14	O
character	O
(	O
cid:12	O
)	O
le	O
corresponds	O
to	O
the	O
integer	O
n	O
=	O
167	O
987	O
786	O
364	O
950	O
891	O
085	O
602	O
469	O
870	O
(	O
decimal	O
)	O
:	O
(	O
cid:15	O
)	O
the	O
unary	O
code	B
for	O
n	O
consists	O
of	O
this	O
many	O
(	O
less	O
one	O
)	O
zeroes	O
,	O
followed	O
by	O
a	O
one	O
.	O
if	O
all	O
the	O
oceans	O
were	O
turned	O
into	O
ink	O
,	O
and	O
if	O
we	O
wrote	O
a	O
hundred	O
bits	O
with	O
every	O
cubic	O
millimeter	O
,	O
there	O
might	O
be	O
enough	O
ink	O
to	O
write	O
cu	O
(	O
n	O
)	O
.	O
(	O
cid:15	O
)	O
the	O
standard	O
binary	O
representation	O
of	O
n	O
is	O
this	O
length-98	O
sequence	B
of	O
bits	O
:	O
cb	O
(	O
n	O
)	O
=	O
1000011110110011000011110101110010011001010100000	O
1010011110100011000011101110110111011011111101110	O
:	O
.	O
exercise	O
7.2	O
.	O
[	O
2	O
]	O
write	O
down	O
or	O
describe	O
the	O
following	O
self-delimiting	B
represen-	O
tations	O
of	O
the	O
above	O
number	O
n	O
:	O
c	O
(	O
cid:11	O
)	O
(	O
n	O
)	O
,	O
c	O
(	O
cid:12	O
)	O
(	O
n	O
)	O
,	O
c	O
(	O
cid:13	O
)	O
(	O
n	O
)	O
,	O
c	O
(	O
cid:14	O
)	O
(	O
n	O
)	O
,	O
c3	O
(	O
n	O
)	O
,	O
c7	O
(	O
n	O
)	O
,	O
and	O
c15	O
(	O
n	O
)	O
.	O
which	O
of	O
these	O
encodings	O
is	O
the	O
shortest	O
?	O
[	O
answer	O
:	O
c15	O
.	O
]	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
135	O
algorithm	B
7.4.	O
elias	O
’	O
s	O
encoder	B
for	O
an	O
integer	O
n.	O
7	O
|	O
codes	O
for	B
integers	I
comparing	O
the	O
codes	O
one	O
could	O
answer	O
the	O
question	O
‘	O
which	O
of	O
two	O
codes	O
is	O
superior	O
?	O
’	O
by	O
a	O
sentence	O
of	O
the	O
form	O
‘	O
for	O
n	O
>	O
k	O
,	O
code	B
1	O
is	O
superior	O
,	O
for	O
n	O
<	O
k	O
,	O
code	B
2	O
is	O
superior	O
’	O
but	O
i	O
contend	O
that	O
such	O
an	O
answer	O
misses	O
the	O
point	O
:	O
any	O
complete	O
code	B
corresponds	O
to	O
a	O
prior	B
for	O
which	O
it	O
is	O
optimal	B
;	O
you	O
should	O
not	O
say	O
that	O
any	O
other	O
code	B
is	O
superior	O
to	O
it	O
.	O
other	O
codes	O
are	O
optimal	B
for	O
other	O
priors	O
.	O
these	O
implicit	O
priors	O
should	O
be	O
thought	O
about	O
so	O
as	O
to	O
achieve	O
the	O
best	O
code	B
for	O
one	O
’	O
s	O
application	O
.	O
notice	O
that	O
one	O
can	O
not	O
,	O
for	O
free	O
,	O
switch	O
from	O
one	O
code	B
to	O
another	O
,	O
choosing	O
if	O
one	O
were	O
to	O
do	O
this	O
,	O
then	O
it	O
would	O
be	O
necessary	O
to	O
whichever	O
is	O
shorter	O
.	O
lengthen	O
the	O
message	O
in	O
some	O
way	O
that	O
indicates	O
which	O
of	O
the	O
two	O
codes	O
is	O
being	O
used	O
.	O
if	O
this	O
is	O
done	O
by	O
a	O
single	O
leading	O
bit	B
,	O
it	O
will	O
be	O
found	O
that	O
the	O
resulting	O
code	B
is	O
suboptimal	O
because	O
it	O
fails	O
the	O
kraft	O
equality	O
,	O
as	O
was	O
discussed	O
in	O
exercise	O
5.33	O
(	O
p.104	O
)	O
.	O
another	O
way	O
to	O
compare	O
codes	O
for	B
integers	I
is	O
to	O
consider	O
a	O
sequence	B
of	O
probability	B
distributions	I
,	O
such	O
as	O
monotonic	O
probability	B
distributions	I
over	O
n	O
(	O
cid:21	O
)	O
1	O
,	O
and	O
rank	O
the	O
codes	O
as	O
to	O
how	O
well	O
they	O
encode	O
any	O
of	O
these	O
distributions	O
.	O
a	O
code	B
is	O
called	O
a	O
‘	O
universal	B
’	O
code	B
if	O
for	O
any	O
distribution	B
in	O
a	O
given	O
class	O
,	O
it	O
encodes	O
into	O
an	O
average	B
length	O
that	O
is	O
within	O
some	O
factor	O
of	O
the	O
ideal	O
average	B
length	O
.	O
let	O
me	O
say	O
this	O
again	O
.	O
we	O
are	O
meeting	O
an	O
alternative	O
world	O
view	O
{	O
rather	O
than	O
(	O
cid:12	O
)	O
guring	O
out	O
a	O
good	B
prior	O
over	O
integers	O
,	O
as	O
advocated	O
above	O
,	O
many	O
the-	O
orists	O
have	O
studied	O
the	O
problem	O
of	O
creating	O
codes	O
that	O
are	O
reasonably	O
good	B
codes	O
for	O
any	O
priors	O
in	O
a	O
broad	O
class	O
.	O
here	O
the	O
class	O
of	O
priors	O
convention-	O
ally	O
considered	O
is	O
the	O
set	B
of	O
priors	O
that	O
(	O
a	O
)	O
assign	O
a	O
monotonically	O
decreasing	O
probability	B
over	O
integers	O
and	O
(	O
b	O
)	O
have	O
(	O
cid:12	O
)	O
nite	O
entropy	B
.	O
several	O
of	O
the	O
codes	O
we	O
have	O
discussed	O
above	O
are	O
universal	B
.	O
another	O
code	B
which	O
elegantly	O
transcends	O
the	O
sequence	B
of	O
self-delimiting	B
codes	O
is	O
elias	O
’	O
s	O
‘	O
uni-	O
versal	O
code	B
for	O
integers	O
’	O
(	O
elias	O
,	O
1975	O
)	O
,	O
which	O
e	O
(	O
cid:11	O
)	O
ectively	O
chooses	O
from	O
all	O
the	O
codes	O
c	O
(	O
cid:11	O
)	O
;	O
c	O
(	O
cid:12	O
)	O
;	O
:	O
:	O
:	O
:	O
it	O
works	O
by	O
sending	O
a	O
sequence	B
of	O
messages	O
each	O
of	O
which	O
encodes	O
the	O
length	B
of	O
the	O
next	O
message	O
,	O
and	O
indicates	O
by	O
a	O
single	O
bit	O
whether	O
or	O
not	O
that	O
message	O
is	O
the	O
(	O
cid:12	O
)	O
nal	O
integer	O
(	O
in	O
its	O
standard	O
binary	O
representation	O
)	O
.	O
because	O
a	O
length	B
is	O
a	O
positive	O
integer	O
and	O
all	O
positive	O
integers	O
begin	O
with	O
‘	O
1	O
’	O
,	O
all	O
the	O
leading	O
1s	O
can	O
be	O
omitted	O
.	O
write	O
‘	O
0	O
’	O
loop	O
f	O
g	O
if	O
blog	O
nc	O
=	O
0	O
halt	O
prepend	O
cb	O
(	O
n	O
)	O
to	O
the	O
written	O
string	O
n	O
:	O
=blog	O
nc	O
the	O
encoder	B
of	O
c	O
!	O
is	O
shown	O
in	O
algorithm	O
7.4.	O
the	O
encoding	O
is	O
generated	O
from	O
right	O
to	O
left	O
.	O
table	O
7.5	O
shows	O
the	O
resulting	O
codewords	O
.	O
.	O
exercise	O
7.3	O
.	O
[	O
2	O
]	O
show	O
that	O
the	O
elias	O
code	B
is	O
not	O
actually	O
the	O
best	O
code	B
for	O
a	O
prior	B
distribution	O
that	O
expects	O
very	O
large	O
integers	O
.	O
(	O
do	O
this	O
by	O
construct-	O
ing	O
another	O
code	B
and	O
specifying	O
how	O
large	O
n	O
must	O
be	O
for	O
your	O
code	B
to	O
give	O
a	O
shorter	O
length	B
than	O
elias	O
’	O
s	O
.	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
136	O
n	O
c	O
!	O
(	O
n	O
)	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
0	O
100	O
110	O
101000	O
101010	O
101100	O
101110	O
1110000	O
solutions	O
7	O
|	O
codes	O
for	B
integers	I
n	O
9	O
10	O
11	O
12	O
13	O
14	O
15	O
16	O
c	O
!	O
(	O
n	O
)	O
1110010	O
1110100	O
1110110	O
1111000	O
1111010	O
1111100	O
1111110	O
10100100000	O
n	O
31	O
32	O
45	O
63	O
64	O
127	O
128	O
255	O
c	O
!	O
(	O
n	O
)	O
n	O
c	O
!	O
(	O
n	O
)	O
10100111110	O
101011000000	O
101011011010	O
101011111110	O
1011010000000	O
1011011111110	O
10111100000000	O
10111111111110	O
256	O
365	O
511	O
512	O
719	O
1023	O
1024	O
1025	O
1110001000000000	O
1110001011011010	O
1110001111111110	O
11100110000000000	O
11100110110011110	O
11100111111111110	O
111010100000000000	O
111010100000000010	O
table	O
7.5.	O
elias	O
’	O
s	O
‘	O
universal	B
’	O
code	B
for	O
integers	O
.	O
examples	O
from	O
1	O
to	O
1025.	O
solution	O
to	O
exercise	O
7.1	O
(	O
p.134	O
)	O
.	O
the	O
use	O
of	O
the	O
end-of-	O
(	O
cid:12	O
)	O
le	O
symbol	O
in	O
a	O
code	B
that	O
represents	O
the	O
integer	O
in	O
some	O
base	O
q	O
corresponds	O
to	O
a	O
belief	B
that	O
there	O
is	O
a	O
probability	O
of	O
(	O
1=	O
(	O
q	O
+	O
1	O
)	O
)	O
that	O
the	O
current	O
character	O
is	O
the	O
last	O
character	O
of	O
the	O
number	O
.	O
thus	O
the	O
prior	B
to	O
which	O
this	O
code	B
is	O
matched	O
puts	O
an	O
exponential	B
prior	O
distribution	B
over	O
the	O
length	B
of	O
the	O
integer	O
.	O
(	O
a	O
)	O
the	O
expected	O
number	O
of	O
characters	O
is	O
q	O
+1	O
=	O
256	O
,	O
so	O
the	O
expected	O
length	B
of	O
the	O
integer	O
is	O
256	O
(	O
cid:2	O
)	O
8	O
’	O
2000	O
bits	O
.	O
(	O
b	O
)	O
we	O
wish	O
to	O
(	O
cid:12	O
)	O
nd	O
q	O
such	O
that	O
q	O
log	O
q	O
’	O
800	O
000	O
bits	O
.	O
a	O
value	O
of	O
q	O
between	O
215	O
and	O
216	O
satis	O
(	O
cid:12	O
)	O
es	O
this	O
constraint	O
,	O
so	O
16-bit	O
blocks	O
are	O
roughly	O
the	O
optimal	B
size	O
,	O
assuming	O
there	O
is	O
one	O
end-of-	O
(	O
cid:12	O
)	O
le	O
character	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
part	O
ii	O
noisy-channel	B
coding	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
8	O
dependent	O
random	O
variables	O
in	O
the	O
last	O
three	O
chapters	O
on	O
data	O
compression	B
we	O
concentrated	O
on	O
random	O
vectors	B
x	O
coming	O
from	O
an	O
extremely	O
simple	O
probability	O
distribution	B
,	O
namely	O
the	O
separable	O
distribution	B
in	O
which	O
each	O
component	O
xn	O
is	O
independent	O
of	O
the	O
others	O
.	O
in	O
this	O
chapter	O
,	O
we	O
consider	O
joint	B
ensembles	O
in	O
which	O
the	O
random	B
variables	O
are	O
dependent	O
.	O
this	O
material	O
has	O
two	O
motivations	O
.	O
first	O
,	O
data	O
from	O
the	O
real	O
world	O
have	O
interesting	O
correlations	B
,	O
so	O
to	O
do	O
data	O
compression	B
well	O
,	O
we	O
need	O
to	O
know	O
how	O
to	O
work	O
with	O
models	O
that	O
include	O
dependences	O
.	O
second	O
,	O
a	O
noisy	B
channel	I
with	O
input	O
x	O
and	O
output	O
y	O
de	O
(	O
cid:12	O
)	O
nes	O
a	O
joint	B
ensemble	I
in	O
which	O
x	O
and	O
y	O
are	O
dependent	O
{	O
if	O
they	O
were	O
independent	O
,	O
it	O
would	O
be	O
impossible	O
to	O
communicate	O
over	O
the	O
channel	B
{	O
so	O
communication	B
over	O
noisy	B
channels	O
(	O
the	O
topic	O
of	O
chapters	O
9	O
{	O
11	O
)	O
is	O
described	O
in	O
terms	O
of	O
the	O
entropy	O
of	O
joint	O
ensembles	O
.	O
8.1	O
more	O
about	O
entropy	B
this	O
section	B
gives	O
de	O
(	O
cid:12	O
)	O
nitions	O
and	O
exercises	O
to	O
do	O
with	O
entropy	B
,	O
carrying	O
on	O
from	O
section	B
2.4.	O
the	O
joint	B
entropy	I
of	O
x	O
;	O
y	O
is	O
:	O
h	O
(	O
x	O
;	O
y	O
)	O
=	O
xxy2axay	O
p	O
(	O
x	O
;	O
y	O
)	O
log	O
1	O
p	O
(	O
x	O
;	O
y	O
)	O
:	O
(	O
8.1	O
)	O
entropy	B
is	O
additive	O
for	O
independent	O
random	B
variables	O
:	O
h	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
+	O
h	O
(	O
y	O
)	O
i	O
(	O
cid:11	O
)	O
p	O
(	O
x	O
;	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
:	O
(	O
8.2	O
)	O
the	O
conditional	B
entropy	I
of	O
x	O
given	O
y	O
=	O
bk	O
is	O
the	O
entropy	B
of	O
the	O
proba-	O
bility	O
distribution	B
p	O
(	O
xj	O
y	O
=	O
bk	O
)	O
.	O
h	O
(	O
x	O
j	O
y	O
=	O
bk	O
)	O
(	O
cid:17	O
)	O
xx2ax	O
p	O
(	O
xj	O
y	O
=	O
bk	O
)	O
log	O
1	O
p	O
(	O
xj	O
y	O
=	O
bk	O
)	O
:	O
(	O
8.3	O
)	O
the	O
conditional	B
entropy	I
of	O
x	O
given	O
y	O
is	O
the	O
average	B
,	O
over	O
y	O
,	O
of	O
the	O
con-	O
ditional	O
entropy	B
of	O
x	O
given	O
y.	O
h	O
(	O
x	O
j	O
y	O
)	O
(	O
cid:17	O
)	O
xy2ay	O
p	O
(	O
y	O
)	O
2	O
=	O
xxy2axay	O
4	O
xx2ax	O
p	O
(	O
x	O
;	O
y	O
)	O
log	O
1	O
p	O
(	O
xj	O
y	O
)	O
3	O
5	O
(	O
8.4	O
)	O
p	O
(	O
xj	O
y	O
)	O
log	O
1	O
p	O
(	O
xj	O
y	O
)	O
:	O
this	O
measures	O
the	O
average	B
uncertainty	O
that	O
remains	O
about	O
x	O
when	O
y	O
is	O
known	O
.	O
138	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
8.1	O
:	O
more	O
about	O
entropy	B
139	O
the	O
marginal	B
entropy	I
of	O
x	O
is	O
another	O
name	O
for	O
the	O
entropy	B
of	O
x	O
,	O
h	O
(	O
x	O
)	O
,	O
used	O
to	O
contrast	O
it	O
with	O
the	O
conditional	B
entropies	O
listed	O
above	O
.	O
chain	B
rule	I
for	O
information	B
content	I
.	O
from	O
the	O
product	O
rule	O
for	O
probabil-	O
ities	O
,	O
equation	O
(	O
2.6	O
)	O
,	O
we	O
obtain	O
:	O
so	O
log	O
1	O
p	O
(	O
x	O
;	O
y	O
)	O
=	O
log	O
1	O
p	O
(	O
x	O
)	O
+	O
log	O
1	O
p	O
(	O
y	O
j	O
x	O
)	O
h	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
+	O
h	O
(	O
y	O
j	O
x	O
)	O
:	O
(	O
8.5	O
)	O
(	O
8.6	O
)	O
in	O
words	O
,	O
this	O
says	O
that	O
the	O
information	B
content	I
of	O
x	O
and	O
y	O
is	O
the	O
infor-	O
mation	O
content	B
of	O
x	O
plus	O
the	O
information	B
content	I
of	O
y	O
given	O
x.	O
chain	B
rule	I
for	O
entropy	B
.	O
the	O
joint	B
marginal	O
entropy	B
are	O
related	O
by	O
:	O
entropy	B
,	O
conditional	B
entropy	I
and	O
h	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
+	O
h	O
(	O
y	O
j	O
x	O
)	O
=	O
h	O
(	O
y	O
)	O
+	O
h	O
(	O
x	O
j	O
y	O
)	O
:	O
(	O
8.7	O
)	O
in	O
words	O
,	O
this	O
says	O
that	O
the	O
uncertainty	O
of	O
x	O
and	O
y	O
is	O
the	O
uncertainty	O
of	O
x	O
plus	O
the	O
uncertainty	O
of	O
y	O
given	O
x.	O
the	O
mutual	B
information	I
between	O
x	O
and	O
y	O
is	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:17	O
)	O
h	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
j	O
y	O
)	O
;	O
and	O
satis	O
(	O
cid:12	O
)	O
es	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
i	O
(	O
y	O
;	O
x	O
)	O
,	O
and	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:21	O
)	O
0.	O
it	O
measures	O
the	O
average	B
reduction	O
in	O
uncertainty	O
about	O
x	O
that	O
results	O
from	O
learning	O
the	O
value	O
of	O
y	O
;	O
or	O
vice	O
versa	O
,	O
the	O
average	B
amount	O
of	O
information	O
that	O
x	O
conveys	O
about	O
y	O
.	O
(	O
8.8	O
)	O
the	O
conditional	B
mutual	O
information	B
between	O
x	O
and	O
y	O
given	O
z	O
=	O
ck	O
is	O
the	O
mutual	B
information	I
between	O
the	O
random	B
variables	O
x	O
and	O
y	O
in	O
the	O
joint	B
ensemble	I
p	O
(	O
x	O
;	O
y	O
j	O
z	O
=	O
ck	O
)	O
,	O
i	O
(	O
x	O
;	O
y	O
j	O
z	O
=	O
ck	O
)	O
=	O
h	O
(	O
x	O
j	O
z	O
=	O
ck	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
j	O
y	O
;	O
z	O
=	O
ck	O
)	O
:	O
(	O
8.9	O
)	O
the	O
conditional	B
mutual	O
information	B
between	O
x	O
and	O
y	O
given	O
z	O
is	O
the	O
average	B
over	O
z	O
of	O
the	O
above	O
conditional	B
mutual	O
information	B
.	O
i	O
(	O
x	O
;	O
y	O
j	O
z	O
)	O
=	O
h	O
(	O
x	O
j	O
z	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
j	O
y	O
;	O
z	O
)	O
:	O
(	O
8.10	O
)	O
no	O
other	O
‘	O
three-term	O
entropies	O
’	O
will	O
be	O
de	O
(	O
cid:12	O
)	O
ned	O
.	O
for	O
example	O
,	O
expres-	O
sions	O
such	O
as	O
i	O
(	O
x	O
;	O
y	O
;	O
z	O
)	O
and	O
i	O
(	O
x	O
j	O
y	O
;	O
z	O
)	O
are	O
illegal	O
.	O
but	O
you	O
may	O
put	O
conjunctions	O
of	O
arbitrary	O
numbers	O
of	O
variables	O
in	O
each	O
of	O
the	O
three	O
spots	O
in	O
the	O
expression	O
i	O
(	O
x	O
;	O
y	O
j	O
z	O
)	O
{	O
for	O
example	O
,	O
i	O
(	O
a	O
;	O
b	O
;	O
c	O
;	O
d	O
j	O
e	O
;	O
f	O
)	O
is	O
(	O
cid:12	O
)	O
ne	O
:	O
it	O
measures	O
how	O
much	O
information	O
on	O
average	O
c	O
and	O
d	O
convey	O
about	O
a	O
and	O
b	O
,	O
assuming	O
e	O
and	O
f	O
are	O
known	O
.	O
figure	O
8.1	O
shows	O
how	O
the	O
total	O
entropy	B
h	O
(	O
x	O
;	O
y	O
)	O
of	O
a	O
joint	B
ensemble	I
can	O
be	O
broken	O
down	O
.	O
this	O
(	O
cid:12	O
)	O
gure	O
is	O
important	O
.	O
(	O
cid:3	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
140	O
8	O
|	O
dependent	O
random	O
variables	O
h	O
(	O
x	O
;	O
y	O
)	O
h	O
(	O
x	O
)	O
h	O
(	O
y	O
)	O
h	O
(	O
x	O
j	O
y	O
)	O
i	O
(	O
x	O
;	O
y	O
)	O
h	O
(	O
y	O
jx	O
)	O
figure	O
8.1.	O
the	O
relationship	O
between	O
joint	B
information	O
,	O
marginal	B
entropy	I
,	O
conditional	B
entropy	I
and	O
mutual	O
entropy	O
.	O
8.2	O
exercises	O
.	O
exercise	O
8.1	O
.	O
[	O
1	O
]	O
consider	O
three	O
independent	O
random	B
variables	O
u	O
;	O
v	O
;	O
w	O
with	O
en-	O
tropies	O
hu	O
;	O
hv	O
;	O
hw	O
.	O
let	O
x	O
(	O
cid:17	O
)	O
(	O
u	O
;	O
v	O
)	O
and	O
y	O
(	O
cid:17	O
)	O
(	O
v	O
;	O
w	O
)	O
.	O
what	O
is	O
h	O
(	O
x	O
;	O
y	O
)	O
?	O
what	O
is	O
h	O
(	O
x	O
j	O
y	O
)	O
?	O
what	O
is	O
i	O
(	O
x	O
;	O
y	O
)	O
?	O
.	O
exercise	O
8.2	O
.	O
[	O
3	O
,	O
p.142	O
]	O
referring	O
to	O
the	O
de	O
(	O
cid:12	O
)	O
nitions	O
of	O
conditional	O
entropy	B
(	O
8.3	O
{	O
8.4	O
)	O
,	O
con	O
(	O
cid:12	O
)	O
rm	O
(	O
with	O
an	O
example	O
)	O
that	O
it	O
is	O
possible	O
for	O
h	O
(	O
x	O
j	O
y	O
=	O
bk	O
)	O
to	O
exceed	O
h	O
(	O
x	O
)	O
,	O
but	O
that	O
the	O
average	B
,	O
h	O
(	O
x	O
j	O
y	O
)	O
,	O
is	O
less	O
than	O
h	O
(	O
x	O
)	O
.	O
so	O
data	O
are	O
helpful	O
{	O
they	O
do	O
not	O
increase	O
uncertainty	O
,	O
on	O
average	O
.	O
.	O
exercise	O
8.3	O
.	O
[	O
2	O
,	O
p.143	O
]	O
prove	O
the	O
chain	B
rule	I
for	O
entropy	B
,	O
equation	O
(	O
8.7	O
)	O
.	O
[	O
h	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
+	O
h	O
(	O
y	O
j	O
x	O
)	O
]	O
.	O
exercise	O
8.4	O
.	O
[	O
2	O
,	O
p.143	O
]	O
prove	O
that	O
the	O
mutual	B
information	I
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:17	O
)	O
h	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
j	O
y	O
)	O
satis	O
(	O
cid:12	O
)	O
es	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
i	O
(	O
y	O
;	O
x	O
)	O
and	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:21	O
)	O
0	O
.	O
[	O
hint	O
:	O
see	O
exercise	O
2.26	O
(	O
p.37	O
)	O
and	O
note	O
that	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
dkl	O
(	O
p	O
(	O
x	O
;	O
y	O
)	O
jjp	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
)	O
:	O
]	O
(	O
8.11	O
)	O
exercise	O
8.5	O
.	O
[	O
4	O
]	O
the	O
‘	O
entropy	B
distance	I
’	O
between	O
two	O
random	B
variables	O
can	O
be	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
their	O
joint	B
entropy	I
and	O
their	O
mutual	B
information	I
:	O
dh	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:17	O
)	O
h	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:0	O
)	O
i	O
(	O
x	O
;	O
y	O
)	O
:	O
(	O
8.12	O
)	O
prove	O
that	O
the	O
entropy	B
distance	I
satis	O
(	O
cid:12	O
)	O
es	O
the	O
axioms	O
for	O
a	O
distance	B
{	O
dh	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:21	O
)	O
0	O
,	O
dh	O
(	O
x	O
;	O
x	O
)	O
=	O
0	O
,	O
dh	O
(	O
x	O
;	O
y	O
)	O
=	O
dh	O
(	O
y	O
;	O
x	O
)	O
,	O
and	O
dh	O
(	O
x	O
;	O
z	O
)	O
(	O
cid:20	O
)	O
dh	O
(	O
x	O
;	O
y	O
)	O
+	O
dh	O
(	O
y	O
;	O
z	O
)	O
.	O
[	O
incidentally	O
,	O
we	O
are	O
unlikely	O
to	O
see	O
dh	O
(	O
x	O
;	O
y	O
)	O
again	O
but	O
it	O
is	O
a	O
good	B
function	O
on	O
which	O
to	O
practise	O
inequality-proving	O
.	O
]	O
exercise	O
8.6	O
.	O
[	O
2	O
,	O
p.147	O
]	O
a	O
joint	B
ensemble	I
xy	O
has	O
the	O
following	O
joint	B
distribution	O
.	O
1	O
2	O
3	O
4	O
p	O
(	O
x	O
;	O
y	O
)	O
x	O
1	O
2	O
3	O
4	O
1/8	O
1/16	O
1/16	O
1/4	O
1/16	O
1/8	O
1/16	O
1/32	O
1/32	O
1/16	O
1/32	O
1/32	O
1/16	O
0	O
0	O
0	O
y	O
1	O
2	O
3	O
4	O
1	O
2	O
3	O
4	O
what	O
is	O
the	O
joint	B
entropy	I
h	O
(	O
x	O
;	O
y	O
)	O
?	O
what	O
are	O
the	O
marginal	B
entropies	O
h	O
(	O
x	O
)	O
and	O
h	O
(	O
y	O
)	O
?	O
for	O
each	O
value	O
of	O
y	O
,	O
what	O
is	O
the	O
conditional	B
entropy	I
h	O
(	O
x	O
j	O
y	O
)	O
?	O
what	O
is	O
the	O
conditional	B
entropy	I
h	O
(	O
x	O
j	O
y	O
)	O
?	O
what	O
is	O
the	O
conditional	B
entropy	I
of	O
y	O
given	O
x	O
?	O
what	O
is	O
the	O
mutual	B
information	I
between	O
x	O
and	O
y	O
?	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
8.3	O
:	O
further	O
exercises	O
141	O
exercise	O
8.7	O
.	O
[	O
2	O
,	O
p.143	O
]	O
consider	O
the	O
ensemble	B
xy	O
z	O
in	O
which	O
ax	O
=	O
ay	O
=	O
az	O
=	O
f0	O
;	O
1g	O
,	O
x	O
and	O
y	O
are	O
independent	O
with	O
px	O
=	O
fp	O
;	O
1	O
(	O
cid:0	O
)	O
pg	O
and	O
py	O
=	O
fq	O
;	O
1	O
(	O
cid:0	O
)	O
qg	O
and	O
(	O
8.13	O
)	O
z	O
=	O
(	O
x	O
+	O
y	O
)	O
mod	O
2	O
:	O
(	O
a	O
)	O
if	O
q	O
=	O
1/2	O
,	O
what	O
is	O
pz	O
?	O
what	O
is	O
i	O
(	O
z	O
;	O
x	O
)	O
?	O
(	O
b	O
)	O
for	O
general	O
p	O
and	O
q	O
,	O
what	O
is	O
pz	O
?	O
what	O
is	O
i	O
(	O
z	O
;	O
x	O
)	O
?	O
notice	O
that	O
this	O
ensemble	B
is	O
related	O
to	O
the	O
binary	B
symmetric	I
channel	I
,	O
with	O
x	O
=	O
input	O
,	O
y	O
=	O
noise	B
,	O
and	O
z	O
=	O
output	O
.	O
h	O
(	O
y	O
)	O
figure	O
8.2.	O
a	O
misleading	O
representation	O
of	O
entropies	O
(	O
contrast	O
with	O
(	O
cid:12	O
)	O
gure	O
8.1	O
)	O
.	O
h	O
(	O
x|y	O
)	O
i	O
(	O
x	O
;	O
y	O
)	O
h	O
(	O
y|x	O
)	O
h	O
(	O
x	O
,	O
y	O
)	O
h	O
(	O
x	O
)	O
three	O
term	O
entropies	O
exercise	O
8.8	O
.	O
[	O
3	O
,	O
p.143	O
]	O
many	O
texts	O
draw	O
(	O
cid:12	O
)	O
gure	O
8.1	O
in	O
the	O
form	O
of	O
a	O
venn	O
diagram	O
(	O
(	O
cid:12	O
)	O
gure	O
8.2	O
)	O
.	O
discuss	O
why	O
this	O
diagram	O
is	O
a	O
misleading	O
representation	O
of	O
entropies	O
.	O
hint	O
:	O
consider	O
the	O
three-variable	O
ensemble	B
xy	O
z	O
in	O
which	O
x	O
2	O
f0	O
;	O
1g	O
and	O
y	O
2	O
f0	O
;	O
1g	O
are	O
independent	O
binary	O
variables	O
and	O
z	O
2	O
f0	O
;	O
1g	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
z	O
=	O
x	O
+	O
y	O
mod	O
2	O
.	O
8.3	O
further	O
exercises	O
the	O
data-processing	O
theorem	B
the	O
data	O
processing	O
theorem	B
states	O
that	O
data	O
processing	O
can	O
only	O
destroy	O
information	B
.	O
exercise	O
8.9	O
.	O
[	O
3	O
,	O
p.144	O
]	O
prove	O
this	O
theorem	B
by	O
considering	O
an	O
ensemble	B
w	O
dr	O
in	O
which	O
w	O
is	O
the	O
state	O
of	O
the	O
world	O
,	O
d	O
is	O
data	O
gathered	O
,	O
and	O
r	O
is	O
the	O
processed	O
data	O
,	O
so	O
that	O
these	O
three	O
variables	O
form	O
a	O
markov	O
chain	B
that	O
is	O
,	O
the	O
probability	B
p	O
(	O
w	O
;	O
d	O
;	O
r	O
)	O
can	O
be	O
written	O
as	O
w	O
!	O
d	O
!	O
r	O
;	O
p	O
(	O
w	O
;	O
d	O
;	O
r	O
)	O
=	O
p	O
(	O
w	O
)	O
p	O
(	O
dj	O
w	O
)	O
p	O
(	O
r	O
j	O
d	O
)	O
:	O
(	O
8.14	O
)	O
(	O
8.15	O
)	O
show	O
that	O
the	O
average	B
information	O
that	O
r	O
conveys	O
about	O
w	O
,	O
i	O
(	O
w	O
;	O
r	O
)	O
,	O
is	O
less	O
than	O
or	O
equal	O
to	O
the	O
average	B
information	O
that	O
d	O
conveys	O
about	O
w	O
,	O
i	O
(	O
w	O
;	O
d	O
)	O
.	O
this	O
theorem	B
is	O
as	O
much	O
a	O
caution	B
about	O
our	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
‘	O
information	B
’	O
as	O
it	O
is	O
a	O
caution	B
about	O
data	O
processing	O
!	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
142	O
8	O
|	O
dependent	O
random	O
variables	O
inference	B
and	O
information	B
measures	O
exercise	O
8.10	O
.	O
[	O
2	O
]	O
the	O
three	B
cards	I
.	O
(	O
a	O
)	O
one	O
card	B
is	O
white	B
on	O
both	O
faces	O
;	O
one	O
is	O
black	B
on	O
both	O
faces	O
;	O
and	O
one	O
is	O
white	B
on	O
one	O
side	O
and	O
black	O
on	O
the	O
other	O
.	O
the	O
three	B
cards	I
are	O
shu	O
(	O
cid:15	O
)	O
ed	O
and	O
their	O
orientations	O
randomized	O
.	O
one	O
card	B
is	O
drawn	O
and	O
placed	O
on	O
the	O
table	O
.	O
the	O
upper	O
face	O
is	O
black	B
.	O
what	O
is	O
the	O
colour	O
of	O
its	O
lower	O
face	O
?	O
(	O
solve	O
the	O
inference	B
problem	O
.	O
)	O
(	O
b	O
)	O
does	O
seeing	O
the	O
top	O
face	O
convey	O
information	B
about	O
the	O
colour	O
of	O
the	O
bottom	O
face	O
?	O
discuss	O
the	O
information	B
contents	O
and	O
entropies	O
in	O
this	O
situation	O
.	O
let	O
the	O
value	O
of	O
the	O
upper	O
face	O
’	O
s	O
colour	O
be	O
u	O
and	O
the	O
value	O
of	O
the	O
lower	O
face	O
’	O
s	O
colour	O
be	O
l.	O
imagine	O
that	O
we	O
draw	O
a	O
random	B
card	O
and	O
learn	O
both	O
u	O
and	O
l.	O
what	O
is	O
the	O
entropy	B
of	O
u	O
,	O
h	O
(	O
u	O
)	O
?	O
what	O
is	O
the	O
entropy	B
of	O
l	O
,	O
h	O
(	O
l	O
)	O
?	O
what	O
is	O
the	O
mutual	B
information	I
between	O
u	O
and	O
l	O
,	O
i	O
(	O
u	O
;	O
l	O
)	O
?	O
entropies	O
of	O
markov	O
processes	O
.	O
exercise	O
8.11	O
.	O
[	O
3	O
]	O
in	O
the	O
guessing	B
game	I
,	O
we	O
imagined	O
predicting	O
the	O
next	O
letter	O
in	O
a	O
document	O
starting	O
from	O
the	O
beginning	O
and	O
working	O
towards	O
the	O
end	O
.	O
consider	O
the	O
task	O
of	O
predicting	O
the	O
reversed	O
text	O
,	O
that	O
is	O
,	O
predicting	O
the	O
letter	O
that	O
precedes	O
those	O
already	O
known	O
.	O
most	O
people	O
(	O
cid:12	O
)	O
nd	O
this	O
a	O
harder	O
task	O
.	O
assuming	O
that	O
we	O
model	B
the	O
language	O
using	O
an	O
n	O
-gram	O
model	B
(	O
which	O
says	O
the	O
probability	O
of	O
the	O
next	O
character	O
depends	O
only	O
on	O
the	O
n	O
(	O
cid:0	O
)	O
1	O
preceding	O
characters	O
)	O
,	O
is	O
there	O
any	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
the	O
average	B
information	O
contents	O
of	O
the	O
reversed	O
language	O
and	O
the	O
forward	O
language	O
?	O
8.4	O
solutions	O
solution	O
to	O
exercise	O
8.2	O
(	O
p.140	O
)	O
.	O
see	O
exercise	O
8.6	O
(	O
p.140	O
)	O
for	O
an	O
example	O
where	O
h	O
(	O
x	O
j	O
y	O
)	O
exceeds	O
h	O
(	O
x	O
)	O
(	O
set	B
y	O
=	O
3	O
)	O
.	O
we	O
can	O
prove	O
the	O
inequality	B
h	O
(	O
x	O
j	O
y	O
)	O
(	O
cid:20	O
)	O
h	O
(	O
x	O
)	O
by	O
turning	O
the	O
expression	O
into	O
a	O
relative	B
entropy	I
(	O
using	O
bayes	O
’	O
theorem	B
)	O
and	O
invoking	O
gibbs	O
’	O
inequality	B
(	O
exercise	O
2.26	O
(	O
p.37	O
)	O
)	O
:	O
1	O
p	O
(	O
xj	O
y	O
)	O
3	O
5	O
4	O
xx2ax	O
p	O
(	O
x	O
;	O
y	O
)	O
log	O
p	O
(	O
xj	O
y	O
)	O
log	O
1	O
p	O
(	O
xj	O
y	O
)	O
p	O
(	O
y	O
)	O
h	O
(	O
x	O
j	O
y	O
)	O
(	O
cid:17	O
)	O
xy2ay	O
p	O
(	O
y	O
)	O
2	O
=	O
xxy2axay	O
=	O
xxy	O
=	O
xx	O
p	O
(	O
x	O
)	O
log	O
(	O
8.16	O
)	O
(	O
8.17	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
j	O
x	O
)	O
log	O
1	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
j	O
x	O
)	O
p	O
(	O
x	O
)	O
+xx	O
p	O
(	O
x	O
)	O
xy	O
p	O
(	O
y	O
j	O
x	O
)	O
log	O
p	O
(	O
y	O
)	O
p	O
(	O
y	O
j	O
x	O
)	O
:	O
(	O
8.18	O
)	O
the	O
last	O
expression	O
is	O
a	O
sum	O
of	O
relative	B
entropies	O
between	O
the	O
distributions	O
p	O
(	O
y	O
j	O
x	O
)	O
and	O
p	O
(	O
y	O
)	O
.	O
so	O
h	O
(	O
x	O
j	O
y	O
)	O
(	O
cid:20	O
)	O
h	O
(	O
x	O
)	O
+	O
0	O
;	O
(	O
8.19	O
)	O
with	O
equality	O
only	O
if	O
p	O
(	O
y	O
j	O
x	O
)	O
=	O
p	O
(	O
y	O
)	O
for	O
all	O
x	O
and	O
y	O
(	O
that	O
is	O
,	O
only	O
if	O
x	O
and	O
y	O
are	O
independent	O
)	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
8.4	O
:	O
solutions	O
143	O
solution	O
to	O
exercise	O
8.3	O
(	O
p.140	O
)	O
.	O
the	O
chain	B
rule	I
for	O
entropy	B
follows	O
from	O
the	O
decomposition	O
of	O
a	O
joint	B
probability	O
:	O
1	O
p	O
(	O
x	O
;	O
y	O
)	O
p	O
(	O
x	O
;	O
y	O
)	O
log	O
h	O
(	O
x	O
;	O
y	O
)	O
=	O
xxy	O
=	O
xxy	O
=	O
xx	O
p	O
(	O
x	O
)	O
=	O
h	O
(	O
x	O
)	O
+	O
h	O
(	O
y	O
j	O
x	O
)	O
:	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
j	O
x	O
)	O
(	O
cid:20	O
)	O
log	O
+xx	O
p	O
(	O
x	O
)	O
log	O
1	O
1	O
p	O
(	O
x	O
)	O
+	O
log	O
p	O
(	O
x	O
)	O
xy	O
1	O
p	O
(	O
y	O
j	O
x	O
)	O
(	O
cid:21	O
)	O
p	O
(	O
y	O
j	O
x	O
)	O
log	O
1	O
p	O
(	O
y	O
j	O
x	O
)	O
solution	O
to	O
exercise	O
8.4	O
(	O
p.140	O
)	O
.	O
symmetry	O
of	O
mutual	B
information	I
:	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
j	O
y	O
)	O
p	O
(	O
x	O
)	O
log	O
p	O
(	O
x	O
;	O
y	O
)	O
log	O
1	O
p	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
xxy	O
p	O
(	O
xj	O
y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
x	O
;	O
y	O
)	O
log	O
p	O
(	O
x	O
;	O
y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
:	O
=	O
xx	O
=	O
xxy	O
=	O
xxy	O
p	O
(	O
x	O
;	O
y	O
)	O
log	O
1	O
p	O
(	O
xj	O
y	O
)	O
(	O
8.20	O
)	O
(	O
8.21	O
)	O
(	O
8.22	O
)	O
(	O
8.23	O
)	O
(	O
8.24	O
)	O
(	O
8.25	O
)	O
(	O
8.26	O
)	O
(	O
8.27	O
)	O
(	O
8.29	O
)	O
this	O
expression	O
is	O
symmetric	B
in	O
x	O
and	O
y	O
so	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
j	O
y	O
)	O
=	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
j	O
x	O
)	O
:	O
(	O
8.28	O
)	O
we	O
can	O
prove	O
that	O
mutual	B
information	I
is	O
positive	O
in	O
two	O
ways	O
.	O
one	O
is	O
to	O
continue	O
from	O
i	O
(	O
x	O
;	O
y	O
)	O
=xx	O
;	O
y	O
p	O
(	O
x	O
;	O
y	O
)	O
log	O
p	O
(	O
x	O
;	O
y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
which	O
is	O
a	O
relative	B
entropy	I
and	O
use	O
gibbs	O
’	O
inequality	B
(	O
proved	O
on	O
p.44	O
)	O
,	O
which	O
asserts	O
that	O
this	O
relative	B
entropy	I
is	O
(	O
cid:21	O
)	O
0	O
,	O
with	O
equality	O
only	O
if	O
p	O
(	O
x	O
;	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
,	O
that	O
is	O
,	O
if	O
x	O
and	O
y	O
are	O
independent	O
.	O
the	O
other	O
is	O
to	O
use	O
jensen	O
’	O
s	O
inequality	B
on	O
p	O
(	O
x	O
;	O
y	O
)	O
log	O
(	O
cid:0	O
)	O
xx	O
;	O
y	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
p	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:21	O
)	O
(	O
cid:0	O
)	O
logxx	O
;	O
y	O
p	O
(	O
x	O
;	O
y	O
)	O
p	O
(	O
x	O
;	O
y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
=	O
log	O
1	O
=	O
0	O
:	O
(	O
8.30	O
)	O
solution	O
to	O
exercise	O
8.7	O
(	O
p.141	O
)	O
.	O
z	O
=	O
x	O
+	O
y	O
mod	O
2	O
:	O
(	O
a	O
)	O
if	O
q	O
=	O
1/2	O
,	O
pz	O
=	O
f1/2	O
;	O
1/2g	O
and	O
i	O
(	O
z	O
;	O
x	O
)	O
=	O
h	O
(	O
z	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
z	O
j	O
x	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
1	O
=	O
0	O
.	O
(	O
b	O
)	O
for	O
general	O
q	O
and	O
p	O
,	O
pz	O
=	O
fpq+	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
q	O
)	O
;	O
p	O
(	O
1	O
(	O
cid:0	O
)	O
q	O
)	O
+q	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
g.	O
the	O
mutual	B
information	I
is	O
i	O
(	O
z	O
;	O
x	O
)	O
=	O
h	O
(	O
z	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
z	O
j	O
x	O
)	O
=	O
h2	O
(	O
pq+	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
q	O
)	O
)	O
(	O
cid:0	O
)	O
h2	O
(	O
q	O
)	O
.	O
three	O
term	O
entropies	O
solution	O
to	O
exercise	O
8.8	O
(	O
p.141	O
)	O
.	O
the	O
depiction	O
of	O
entropies	O
in	O
terms	O
of	O
venn	O
diagrams	O
is	O
misleading	O
for	O
at	O
least	O
two	O
reasons	O
.	O
first	O
,	O
one	O
is	O
used	O
to	O
thinking	O
of	O
venn	O
diagrams	O
as	O
depicting	O
sets	O
;	O
but	O
what	O
are	O
the	O
‘	O
sets	O
’	O
h	O
(	O
x	O
)	O
and	O
h	O
(	O
y	O
)	O
depicted	O
in	O
(	O
cid:12	O
)	O
gure	O
8.2	O
,	O
and	O
what	O
are	O
the	O
objects	O
that	O
are	O
members	O
of	O
those	O
sets	O
?	O
i	O
think	O
this	O
diagram	O
encourages	O
the	O
novice	O
student	B
to	O
make	O
inappropriate	O
analogies	O
.	O
for	O
example	O
,	O
some	O
students	O
imagine	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
8	O
|	O
dependent	O
random	O
variables	O
figure	O
8.3.	O
a	O
misleading	O
representation	O
of	O
entropies	O
,	O
continued	O
.	O
144	O
i	O
(	O
x	O
;	O
y	O
)	O
h	O
(	O
x|y	O
,	O
z	O
)	O
h	O
(	O
x	O
)	O
     	O
	O
     	O
	O
     	O
	O
     	O
	O
     	O
	O
     	O
	O
     	O
	O
     	O
	O
     	O
	O
     	O
	O
     	O
	O
     	O
	O
     	O
	O
     	O
	O
     	O
	O
     	O
	O
     	O
	O
h	O
(	O
z|x	O
)	O
h	O
(	O
y	O
)	O
h	O
(	O
y|x	O
,	O
z	O
)	O
h	O
(	O
z	O
)	O
i	O
(	O
x	O
;	O
y|z	O
)	O
a	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
h	O
(	O
z|y	O
)	O
h	O
(	O
x	O
,	O
y|z	O
)	O
h	O
(	O
z|x	O
,	O
y	O
)	O
that	O
the	O
random	B
outcome	O
(	O
x	O
;	O
y	O
)	O
might	O
correspond	O
to	O
a	O
point	O
in	O
the	O
diagram	O
,	O
and	O
thus	O
confuse	O
entropies	O
with	O
probabilities	O
.	O
secondly	O
,	O
the	O
depiction	O
in	O
terms	O
of	O
venn	O
diagrams	O
encourages	O
one	O
to	O
be-	O
lieve	O
that	O
all	O
the	O
areas	O
correspond	O
to	O
positive	O
quantities	O
.	O
in	O
the	O
special	O
case	O
of	O
two	O
random	B
variables	O
it	O
is	O
indeed	O
true	O
that	O
h	O
(	O
x	O
j	O
y	O
)	O
,	O
i	O
(	O
x	O
;	O
y	O
)	O
and	O
h	O
(	O
y	O
j	O
x	O
)	O
are	O
positive	O
quantities	O
.	O
but	O
as	O
soon	O
as	O
we	O
progress	O
to	O
three-variable	O
ensembles	O
,	O
we	O
obtain	O
a	O
diagram	O
with	O
positive-looking	O
areas	O
that	O
may	O
actually	O
correspond	O
to	O
negative	O
quantities	O
.	O
figure	O
8.3	O
correctly	O
shows	O
relationships	O
such	O
as	O
h	O
(	O
x	O
)	O
+	O
h	O
(	O
z	O
j	O
x	O
)	O
+	O
h	O
(	O
y	O
j	O
x	O
;	O
z	O
)	O
=	O
h	O
(	O
x	O
;	O
y	O
;	O
z	O
)	O
:	O
(	O
8.31	O
)	O
but	O
it	O
gives	O
the	O
misleading	O
impression	O
that	O
the	O
conditional	B
mutual	O
information	B
i	O
(	O
x	O
;	O
y	O
j	O
z	O
)	O
is	O
less	O
than	O
the	O
mutual	B
information	I
i	O
(	O
x	O
;	O
y	O
)	O
.	O
in	O
fact	O
the	O
area	O
labelled	O
a	O
can	O
correspond	O
to	O
a	O
negative	O
quantity	O
.	O
consider	O
the	O
joint	B
ensemble	I
(	O
x	O
;	O
y	O
;	O
z	O
)	O
in	O
which	O
x	O
2	O
f0	O
;	O
1g	O
and	O
y	O
2	O
f0	O
;	O
1g	O
are	O
independent	O
binary	O
variables	O
and	O
z	O
2	O
f0	O
;	O
1g	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
z	O
=	O
x	O
+	O
y	O
mod	O
2.	O
then	O
clearly	O
h	O
(	O
x	O
)	O
=	O
h	O
(	O
y	O
)	O
=	O
1	O
bit	B
.	O
also	O
h	O
(	O
z	O
)	O
=	O
1	O
bit	B
.	O
and	O
h	O
(	O
y	O
j	O
x	O
)	O
=	O
h	O
(	O
y	O
)	O
=	O
1	O
since	O
the	O
two	O
variables	O
are	O
independent	O
.	O
so	O
the	O
mutual	B
information	I
between	O
x	O
and	O
y	O
is	O
zero	O
.	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
0.	O
however	O
,	O
if	O
z	O
is	O
observed	O
,	O
x	O
and	O
y	O
become	O
dependent	O
|	O
knowing	O
x	O
,	O
given	O
z	O
,	O
tells	O
you	O
what	O
y	O
is	O
:	O
y	O
=	O
z	O
(	O
cid:0	O
)	O
x	O
mod	O
2.	O
so	O
i	O
(	O
x	O
;	O
y	O
j	O
z	O
)	O
=	O
1	O
bit	B
.	O
thus	O
the	O
area	O
labelled	O
a	O
must	O
correspond	O
to	O
(	O
cid:0	O
)	O
1	O
bits	O
for	O
the	O
(	O
cid:12	O
)	O
gure	O
to	O
give	O
the	O
correct	O
answers	O
.	O
the	O
above	O
example	O
is	O
not	O
at	O
all	O
a	O
capricious	O
or	O
exceptional	O
illustration	O
.	O
the	O
binary	B
symmetric	I
channel	I
with	O
input	O
x	O
,	O
noise	B
y	O
,	O
and	O
output	O
z	O
is	O
a	O
situation	O
in	O
which	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
0	O
(	O
input	O
and	O
noise	B
are	O
independent	O
)	O
but	O
i	O
(	O
x	O
;	O
y	O
j	O
z	O
)	O
>	O
0	O
(	O
once	O
you	O
see	O
the	O
output	O
,	O
the	O
unknown	O
input	O
and	O
the	O
unknown	O
noise	O
are	O
intimately	O
related	O
!	O
)	O
.	O
the	O
venn	O
diagram	O
representation	O
is	O
therefore	O
valid	O
only	O
if	O
one	O
is	O
aware	O
that	O
positive	O
areas	O
may	O
represent	O
negative	O
quantities	O
.	O
with	O
this	O
proviso	O
kept	O
in	O
mind	O
,	O
the	O
interpretation	O
of	O
entropies	O
in	O
terms	O
of	O
sets	O
can	O
be	O
helpful	O
(	O
yeung	O
,	O
1991	O
)	O
.	O
solution	O
to	O
exercise	O
8.9	O
(	O
p.141	O
)	O
.	O
for	O
any	O
joint	B
ensemble	I
xy	O
z	O
,	O
the	O
following	O
chain	B
rule	I
for	O
mutual	B
information	I
holds	O
.	O
(	O
8.32	O
)	O
in	O
the	O
case	O
w	O
!	O
d	O
!	O
r	O
,	O
w	O
and	O
r	O
are	O
independent	O
given	O
d	O
,	O
so	O
i	O
(	O
x	O
;	O
y	O
;	O
z	O
)	O
=	O
i	O
(	O
x	O
;	O
y	O
)	O
+	O
i	O
(	O
x	O
;	O
z	O
j	O
y	O
)	O
:	O
now	O
,	O
i	O
(	O
w	O
;	O
r	O
j	O
d	O
)	O
=	O
0.	O
using	O
the	O
chain	B
rule	I
twice	O
,	O
we	O
have	O
:	O
and	O
so	O
i	O
(	O
w	O
;	O
d	O
;	O
r	O
)	O
=	O
i	O
(	O
w	O
;	O
d	O
)	O
i	O
(	O
w	O
;	O
d	O
;	O
r	O
)	O
=	O
i	O
(	O
w	O
;	O
r	O
)	O
+	O
i	O
(	O
w	O
;	O
d	O
j	O
r	O
)	O
;	O
i	O
(	O
w	O
;	O
r	O
)	O
(	O
cid:0	O
)	O
i	O
(	O
w	O
;	O
d	O
)	O
(	O
cid:20	O
)	O
0	O
:	O
(	O
8.33	O
)	O
(	O
8.34	O
)	O
(	O
8.35	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
chapter	O
9	O
before	O
reading	O
chapter	O
9	O
,	O
you	O
should	O
have	O
read	O
chapter	O
1	O
and	O
worked	O
on	O
exercise	O
2.26	O
(	O
p.37	O
)	O
,	O
and	O
exercises	O
8.2	O
{	O
8.7	O
(	O
pp.140	O
{	O
141	O
)	O
.	O
145	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
9	O
communication	B
over	O
a	O
noisy	B
channel	I
9.1	O
the	O
big	O
picture	O
source	O
coding	O
channel	O
coding	O
source	O
?	O
6	O
compressor	O
decompressor	O
?	O
encoder	B
-	O
noisy	B
channel	I
6	O
decoder	B
6	O
in	O
chapters	O
4	O
{	O
6	O
,	O
we	O
discussed	O
source	O
coding	O
with	O
block	B
codes	O
,	O
symbol	O
codes	O
and	O
stream	O
codes	O
.	O
we	O
implicitly	O
assumed	O
that	O
the	O
channel	B
from	O
the	O
compres-	O
sor	O
to	O
the	O
decompressor	O
was	O
noise-free	O
.	O
real	O
channels	O
are	O
noisy	B
.	O
we	O
will	O
now	O
spend	O
two	O
chapters	O
on	O
the	O
subject	O
of	O
noisy-channel	O
coding	O
{	O
the	O
fundamen-	O
tal	O
possibilities	O
and	O
limitations	O
of	O
error-free	O
communication	B
through	O
a	O
noisy	B
channel	I
.	O
the	O
aim	O
of	O
channel	O
coding	O
is	O
to	O
make	O
the	O
noisy	B
channel	I
behave	O
like	O
a	O
noiseless	B
channel	O
.	O
we	O
will	O
assume	O
that	O
the	O
data	O
to	O
be	O
transmitted	O
has	O
been	O
through	O
a	O
good	B
compressor	O
,	O
so	O
the	O
bit	B
stream	O
has	O
no	O
obvious	O
redundancy	B
.	O
the	O
channel	B
code	O
,	O
which	O
makes	O
the	O
transmission	O
,	O
will	O
put	O
back	O
redundancy	B
of	O
a	O
special	O
sort	O
,	O
designed	O
to	O
make	O
the	O
noisy	B
received	O
signal	O
decodeable	O
.	O
suppose	O
we	O
transmit	O
1000	O
bits	O
per	O
second	O
with	O
p0	O
=	O
p1	O
=	O
1/2	O
over	O
a	O
noisy	B
channel	I
that	O
(	O
cid:13	O
)	O
ips	O
bits	O
with	O
probability	B
f	O
=	O
0:1.	O
what	O
is	O
the	O
rate	B
of	O
transmission	O
of	O
information	O
?	O
we	O
might	O
guess	O
that	O
the	O
rate	B
is	O
900	O
bits	O
per	O
second	O
by	O
subtracting	O
the	O
expected	O
number	O
of	O
errors	O
per	O
second	O
.	O
but	O
this	O
is	O
not	O
correct	O
,	O
because	O
the	O
recipient	O
does	O
not	O
know	O
where	O
the	O
errors	B
occurred	O
.	O
consider	O
the	O
case	O
where	O
the	O
noise	B
is	O
so	O
great	O
that	O
the	O
received	O
symbols	O
are	O
independent	O
of	O
the	O
transmitted	O
symbols	O
.	O
this	O
corresponds	O
to	O
a	O
noise	B
level	O
of	O
f	O
=	O
0:5	O
,	O
since	O
half	O
of	O
the	O
received	O
symbols	O
are	O
correct	O
due	O
to	O
chance	O
alone	O
.	O
but	O
when	O
f	O
=	O
0:5	O
,	O
no	O
information	B
is	O
transmitted	O
at	O
all	O
.	O
given	O
what	O
we	O
have	O
learnt	O
about	O
entropy	B
,	O
it	O
seems	O
reasonable	O
that	O
a	O
mea-	O
sure	O
of	O
the	O
information	O
transmitted	O
is	O
given	O
by	O
the	O
mutual	B
information	I
between	O
the	O
source	O
and	O
the	O
received	O
signal	O
,	O
that	O
is	O
,	O
the	O
entropy	B
of	O
the	O
source	O
minus	O
the	O
conditional	B
entropy	I
of	O
the	O
source	O
given	O
the	O
received	O
signal	O
.	O
we	O
will	O
now	O
review	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
conditional	O
entropy	B
and	O
mutual	O
in-	O
formation	O
.	O
then	O
we	O
will	O
examine	O
whether	O
it	O
is	O
possible	O
to	O
use	O
such	O
a	O
noisy	B
channel	I
to	O
communicate	O
reliably	O
.	O
we	O
will	O
show	O
that	O
for	O
any	O
channel	B
q	O
there	O
is	O
a	O
non-zero	O
rate	B
,	O
the	O
capacity	B
c	O
(	O
q	O
)	O
,	O
up	O
to	O
which	O
information	B
can	O
be	O
sent	O
146	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
9.2	O
:	O
review	O
of	O
probability	O
and	O
information	O
147	O
with	O
arbitrarily	O
small	O
probability	B
of	I
error	I
.	O
9.2	O
review	O
of	O
probability	O
and	O
information	O
as	O
an	O
example	O
,	O
we	O
take	O
the	O
joint	B
distribution	O
xy	O
from	O
exercise	O
8.6	O
(	O
p.140	O
)	O
.	O
the	O
marginal	B
distributions	O
p	O
(	O
x	O
)	O
and	O
p	O
(	O
y	O
)	O
are	O
shown	O
in	O
the	O
margins	O
.	O
p	O
(	O
x	O
;	O
y	O
)	O
x	O
p	O
(	O
y	O
)	O
1	O
1/8	O
1/16	O
1/16	O
1/4	O
1/2	O
2	O
1/16	O
1/8	O
1/16	O
0	O
1/4	O
3	O
1/32	O
1/32	O
1/16	O
0	O
1/8	O
4	O
1/32	O
1/32	O
1/16	O
0	O
1/8	O
1/4	O
1/4	O
1/4	O
1/4	O
y	O
1	O
2	O
3	O
4	O
p	O
(	O
x	O
)	O
the	O
joint	B
entropy	I
is	O
h	O
(	O
x	O
;	O
y	O
)	O
=	O
27=8	O
bits	O
.	O
the	O
marginal	B
entropies	O
are	O
h	O
(	O
x	O
)	O
=	O
7=4	O
bits	O
and	O
h	O
(	O
y	O
)	O
=	O
2	O
bits	O
.	O
we	O
can	O
compute	O
the	O
conditional	B
distribution	O
of	O
x	O
for	O
each	O
value	O
of	O
y	O
,	O
and	O
the	O
entropy	B
of	O
each	O
of	O
those	O
conditional	B
distributions	O
:	O
p	O
(	O
xj	O
y	O
)	O
y	O
1	O
2	O
3	O
4	O
x	O
1	O
1/2	O
1/4	O
1/4	O
1	O
2	O
1/4	O
1/2	O
1/4	O
0	O
3	O
1/8	O
1/8	O
1/4	O
0	O
4	O
1/8	O
1/8	O
1/4	O
0	O
h	O
(	O
x	O
j	O
y	O
)	O
=bits	O
7/4	O
7/4	O
2	O
0	O
h	O
(	O
x	O
j	O
y	O
)	O
=	O
11/8	O
note	O
that	O
whereas	O
h	O
(	O
x	O
j	O
y	O
=	O
4	O
)	O
=	O
0	O
is	O
less	O
than	O
h	O
(	O
x	O
)	O
,	O
h	O
(	O
x	O
j	O
y	O
=	O
3	O
)	O
is	O
greater	O
than	O
h	O
(	O
x	O
)	O
.	O
so	O
in	O
some	O
cases	O
,	O
learning	B
y	O
can	O
increase	O
our	O
uncertainty	O
about	O
x.	O
note	O
also	O
that	O
although	O
p	O
(	O
xj	O
y	O
=	O
2	O
)	O
is	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
distribution	B
from	O
p	O
(	O
x	O
)	O
,	O
the	O
conditional	B
entropy	I
h	O
(	O
x	O
j	O
y	O
=	O
2	O
)	O
is	O
equal	O
to	O
h	O
(	O
x	O
)	O
.	O
so	O
learning	B
that	O
y	O
is	O
2	O
changes	O
our	O
knowledge	O
about	O
x	O
but	O
does	O
not	O
reduce	O
the	O
uncertainty	O
of	O
x	O
,	O
as	O
measured	O
by	O
the	O
entropy	B
.	O
on	O
average	O
though	O
,	O
learning	B
y	O
does	O
convey	O
information	B
about	O
x	O
,	O
since	O
h	O
(	O
x	O
j	O
y	O
)	O
<	O
h	O
(	O
x	O
)	O
.	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
j	O
y	O
)	O
=	O
3=8	O
bits	O
.	O
one	O
may	O
also	O
evaluate	O
h	O
(	O
y	O
jx	O
)	O
=	O
13=8	O
bits	O
.	O
the	O
mutual	B
information	I
is	O
9.3	O
noisy	B
channels	O
a	O
discrete	B
memoryless	I
channel	O
q	O
is	O
characterized	O
by	O
an	O
input	O
alphabet	O
ax	O
,	O
an	O
output	O
alphabet	O
ay	O
,	O
and	O
a	O
set	B
of	O
conditional	B
probability	O
distri-	O
butions	O
p	O
(	O
y	O
j	O
x	O
)	O
,	O
one	O
for	O
each	O
x	O
2	O
ax	O
.	O
these	O
transition	B
probabilities	O
may	O
be	O
written	O
in	O
a	O
matrix	B
qjji	O
=	O
p	O
(	O
y	O
=	O
bj	O
j	O
x	O
=	O
ai	O
)	O
:	O
(	O
9.1	O
)	O
i	O
usually	O
orient	O
this	O
matrix	B
with	O
the	O
output	O
variable	O
j	O
indexing	O
the	O
rows	O
and	O
the	O
input	O
variable	O
i	O
indexing	O
the	O
columns	O
,	O
so	O
that	O
each	O
column	O
of	O
q	O
is	O
a	O
probability	B
vector	O
.	O
with	O
this	O
convention	O
,	O
we	O
can	O
obtain	O
the	O
probability	O
of	O
the	O
output	O
,	O
py	O
,	O
from	O
a	O
probability	B
distribution	O
over	O
the	O
input	O
,	O
px	O
,	O
by	O
right-multiplication	O
:	O
py	O
=	O
qpx	O
:	O
(	O
9.2	O
)	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
148	O
9	O
|	O
communication	B
over	O
a	O
noisy	B
channel	I
some	O
useful	O
model	B
channels	O
are	O
:	O
binary	B
symmetric	I
channel	I
.	O
ax	O
=f0	O
;	O
1g	O
.	O
ay	O
=f0	O
;	O
1g	O
.	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
0	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
f	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
0	O
)	O
=	O
f	O
;	O
binary	B
erasure	I
channel	I
.	O
ax	O
=f0	O
;	O
1g	O
.	O
ay	O
=f0	O
;	O
?	O
;	O
1g	O
.	O
-	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
@	O
r	O
x	O
-	O
y	O
1	O
0	O
1	O
0	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
1	O
)	O
=	O
f	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
1	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
f	O
:	O
-	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
-	O
x	O
0	O
1	O
0	O
?	O
y	O
1	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
0	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
f	O
;	O
p	O
(	O
y	O
=	O
?	O
j	O
x	O
=	O
0	O
)	O
=	O
f	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
0	O
)	O
=	O
0	O
;	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
1	O
)	O
=	O
0	O
;	O
p	O
(	O
y	O
=	O
?	O
j	O
x	O
=	O
1	O
)	O
=	O
f	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
1	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
f	O
:	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
?	O
1	O
noisy	B
typewriter	I
.	O
ax	O
=	O
ay	O
=	O
the	O
27	O
letters	O
fa	O
,	O
b	O
,	O
.	O
.	O
.	O
,	O
z	O
,	O
-g.	O
the	O
letters	O
are	O
arranged	O
in	O
a	O
circle	B
,	O
and	O
when	O
the	O
typist	O
attempts	O
to	O
type	O
b	O
,	O
what	O
comes	O
out	O
is	O
either	O
a	O
,	O
b	O
or	O
c	O
,	O
with	O
probability	O
1/3	O
each	O
;	O
when	O
the	O
input	O
is	O
c	O
,	O
the	O
output	O
is	O
b	O
,	O
c	O
or	O
d	O
;	O
and	O
so	O
forth	O
,	O
with	O
the	O
(	O
cid:12	O
)	O
nal	O
letter	O
‘	O
-	O
’	O
adjacent	O
to	O
the	O
(	O
cid:12	O
)	O
rst	O
letter	O
a	O
.	O
-pppq	O
-	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1	O
c	O
(	O
cid:3	O
)	O
(	O
cid:23	O
)	O
-	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1	O
pppq	O
c	O
(	O
cid:3	O
)	O
pppq	O
-	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1	O
c	O
(	O
cid:3	O
)	O
-	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1	O
pppq	O
c	O
(	O
cid:3	O
)	O
-	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1	O
pppq	O
(	O
cid:3	O
)	O
c	O
-	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1	O
pppq	O
pppq	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1-	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1	O
pppq	O
c	O
(	O
cid:3	O
)	O
...	O
pppq	O
c	O
(	O
cid:3	O
)	O
-	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1	O
-	O
(	O
cid:3	O
)	O
c	O
pppq	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1	O
(	O
cid:3	O
)	O
-	O
c	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1	O
pppq	O
-	O
(	O
cid:3	O
)	O
cw	O
p	O
(	O
y	O
=	O
fj	O
x	O
=	O
g	O
)	O
=	O
1=3	O
;	O
p	O
(	O
y	O
=	O
gj	O
x	O
=	O
g	O
)	O
=	O
1=3	O
;	O
p	O
(	O
y	O
=	O
hj	O
x	O
=	O
g	O
)	O
=	O
1=3	O
;	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
...	O
...	O
c	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
c	O
y	O
z	O
-	O
y	O
z	O
-	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
-	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
-	O
z	O
channel	B
.	O
ax	O
=f0	O
;	O
1g	O
.	O
ay	O
=f0	O
;	O
1g	O
.	O
0	O
-	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
x	O
-	O
1	O
y	O
0	O
1	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
0	O
)	O
=	O
1	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
0	O
)	O
=	O
0	O
;	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
1	O
)	O
=	O
f	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
1	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
f	O
:	O
0	O
1	O
0	O
1	O
9.4	O
inferring	O
the	O
input	O
given	O
the	O
output	O
if	O
we	O
assume	O
that	O
the	O
input	O
x	O
to	O
a	O
channel	B
comes	O
from	O
an	O
ensemble	B
x	O
,	O
then	O
we	O
obtain	O
a	O
joint	B
ensemble	I
xy	O
in	O
which	O
the	O
random	B
variables	O
x	O
and	O
y	O
have	O
the	O
joint	B
distribution	O
:	O
p	O
(	O
x	O
;	O
y	O
)	O
=	O
p	O
(	O
y	O
j	O
x	O
)	O
p	O
(	O
x	O
)	O
:	O
(	O
9.3	O
)	O
now	O
if	O
we	O
receive	O
a	O
particular	O
symbol	O
y	O
,	O
what	O
was	O
the	O
input	O
symbol	O
x	O
?	O
we	O
typically	O
won	O
’	O
t	O
know	O
for	O
certain	O
.	O
we	O
can	O
write	O
down	O
the	O
posterior	O
distribution	O
of	O
the	O
input	O
using	O
bayes	O
’	O
theorem	B
:	O
p	O
(	O
xj	O
y	O
)	O
=	O
p	O
(	O
y	O
j	O
x	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
=	O
p	O
(	O
y	O
j	O
x	O
)	O
p	O
(	O
x	O
)	O
px0	O
p	O
(	O
y	O
j	O
x0	O
)	O
p	O
(	O
x0	O
)	O
example	O
9.1.	O
consider	O
a	O
binary	B
symmetric	I
channel	I
with	O
probability	B
of	I
error	I
f	O
=	O
0:15.	O
let	O
the	O
input	B
ensemble	I
be	O
px	O
:	O
fp0	O
=	O
0:9	O
;	O
p1	O
=	O
0:1g	O
.	O
assume	O
we	O
observe	O
y	O
=	O
1.	O
:	O
(	O
9.4	O
)	O
p	O
(	O
x	O
=	O
1j	O
y	O
=	O
1	O
)	O
=	O
=	O
=	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
1	O
)	O
p	O
(	O
x	O
=	O
1	O
)	O
px0	O
p	O
(	O
y	O
j	O
x0	O
)	O
p	O
(	O
x0	O
)	O
0:85	O
(	O
cid:2	O
)	O
0:1	O
0:85	O
(	O
cid:2	O
)	O
0:1	O
+	O
0:15	O
(	O
cid:2	O
)	O
0:9	O
0:085	O
0:22	O
=	O
0:39	O
:	O
(	O
9.5	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
9.5	O
:	O
information	B
conveyed	O
by	O
a	O
channel	B
149	O
thus	O
‘	O
x	O
=	O
1	O
’	O
is	O
still	O
less	O
probable	O
than	O
‘	O
x	O
=	O
0	O
’	O
,	O
although	O
it	O
is	O
not	O
as	O
im-	O
probable	O
as	O
it	O
was	O
before	O
.	O
exercise	O
9.2	O
.	O
[	O
1	O
,	O
p.157	O
]	O
now	O
assume	O
we	O
observe	O
y	O
=	O
0.	O
compute	O
the	O
probability	O
of	O
x	O
=	O
1	O
given	O
y	O
=	O
0.	O
example	O
9.3.	O
consider	O
a	O
z	O
channel	O
with	O
probability	O
of	O
error	O
f	O
=	O
0:15.	O
let	O
the	O
input	B
ensemble	I
be	O
px	O
:	O
fp0	O
=	O
0:9	O
;	O
p1	O
=	O
0:1g	O
.	O
assume	O
we	O
observe	O
y	O
=	O
1.	O
p	O
(	O
x	O
=	O
1j	O
y	O
=	O
1	O
)	O
=	O
=	O
0:85	O
(	O
cid:2	O
)	O
0:1	O
0:85	O
(	O
cid:2	O
)	O
0:1	O
+	O
0	O
(	O
cid:2	O
)	O
0:9	O
0:085	O
0:085	O
=	O
1:0	O
:	O
(	O
9.6	O
)	O
so	O
given	O
the	O
output	O
y	O
=	O
1	O
we	O
become	O
certain	O
of	O
the	O
input	O
.	O
exercise	O
9.4	O
.	O
[	O
1	O
,	O
p.157	O
]	O
alternatively	O
,	O
assume	O
we	O
observe	O
y	O
=	O
0.	O
compute	O
p	O
(	O
x	O
=	O
1j	O
y	O
=	O
0	O
)	O
.	O
9.5	O
information	B
conveyed	O
by	O
a	O
channel	B
we	O
now	O
consider	O
how	O
much	O
information	O
can	O
be	O
communicated	O
through	O
a	O
chan-	O
nel	O
.	O
in	O
operational	O
terms	O
,	O
we	O
are	O
interested	O
in	O
(	O
cid:12	O
)	O
nding	O
ways	O
of	O
using	O
the	O
chan-	O
nel	O
such	O
that	O
all	O
the	O
bits	O
that	O
are	O
communicated	O
are	O
recovered	O
with	O
negligible	O
probability	B
of	I
error	I
.	O
in	O
mathematical	O
terms	O
,	O
assuming	O
a	O
particular	O
input	O
en-	O
semble	O
x	O
,	O
we	O
can	O
measure	O
how	O
much	O
information	O
the	O
output	O
conveys	O
about	O
the	O
input	O
by	O
the	O
mutual	B
information	I
:	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:17	O
)	O
h	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
j	O
y	O
)	O
=	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
jx	O
)	O
:	O
(	O
9.7	O
)	O
our	O
aim	O
is	O
to	O
establish	O
the	O
connection	B
between	I
these	O
two	O
ideas	O
.	O
let	O
us	O
evaluate	O
i	O
(	O
x	O
;	O
y	O
)	O
for	O
some	O
of	O
the	O
channels	O
above	O
.	O
hint	B
for	I
computing	I
mutual	I
information	O
we	O
will	O
tend	O
to	O
think	O
of	O
i	O
(	O
x	O
;	O
y	O
)	O
as	O
h	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
j	O
y	O
)	O
,	O
i.e.	O
,	O
how	O
much	O
the	O
uncertainty	O
of	O
the	O
input	O
x	O
is	O
reduced	O
when	O
we	O
look	O
at	O
the	O
output	O
y	O
.	O
but	O
for	O
computational	O
purposes	O
it	O
is	O
often	O
handy	O
to	O
evaluate	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
jx	O
)	O
instead	O
.	O
h	O
(	O
x	O
;	O
y	O
)	O
h	O
(	O
x	O
)	O
h	O
(	O
y	O
)	O
h	O
(	O
x	O
j	O
y	O
)	O
i	O
(	O
x	O
;	O
y	O
)	O
h	O
(	O
y	O
jx	O
)	O
figure	O
9.1.	O
the	O
relationship	O
between	O
joint	B
information	O
,	O
marginal	B
entropy	I
,	O
conditional	B
entropy	I
and	O
mutual	O
entropy	O
.	O
this	O
(	O
cid:12	O
)	O
gure	O
is	O
important	O
,	O
so	O
i	O
’	O
m	O
showing	O
it	O
twice	O
.	O
example	O
9.5.	O
consider	O
the	O
binary	B
symmetric	I
channel	I
again	O
,	O
with	O
f	O
=	O
0:15	O
and	O
px	O
:	O
fp0	O
=	O
0:9	O
;	O
p1	O
=	O
0:1g	O
.	O
we	O
already	O
evaluated	O
the	O
marginal	B
probabil-	O
ities	O
p	O
(	O
y	O
)	O
implicitly	O
above	O
:	O
p	O
(	O
y	O
=	O
0	O
)	O
=	O
0:78	O
;	O
p	O
(	O
y	O
=	O
1	O
)	O
=	O
0:22.	O
the	O
mutual	B
information	I
is	O
:	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
jx	O
)	O
:	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
150	O
9	O
|	O
communication	B
over	O
a	O
noisy	B
channel	I
what	O
is	O
h	O
(	O
y	O
jx	O
)	O
?	O
it	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
the	O
weighted	O
sum	O
over	O
x	O
of	O
h	O
(	O
y	O
j	O
x	O
)	O
;	O
but	O
h	O
(	O
y	O
j	O
x	O
)	O
is	O
the	O
same	O
for	O
each	O
value	O
of	O
x	O
:	O
h	O
(	O
y	O
j	O
x	O
=	O
0	O
)	O
is	O
h2	O
(	O
0:15	O
)	O
,	O
and	O
h	O
(	O
y	O
j	O
x	O
=	O
1	O
)	O
is	O
h2	O
(	O
0:15	O
)	O
.	O
so	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
jx	O
)	O
=	O
h2	O
(	O
0:22	O
)	O
(	O
cid:0	O
)	O
h2	O
(	O
0:15	O
)	O
=	O
0:76	O
(	O
cid:0	O
)	O
0:61	O
=	O
0:15	O
bits	O
:	O
(	O
9.8	O
)	O
throughout	O
this	O
book	O
,	O
log	O
means	O
log2	O
.	O
this	O
may	O
be	O
contrasted	O
with	O
the	O
entropy	B
of	O
the	O
source	O
h	O
(	O
x	O
)	O
=	O
h2	O
(	O
0:1	O
)	O
=	O
0:47	O
bits	O
.	O
note	O
:	O
here	O
we	O
have	O
used	O
the	O
binary	B
entropy	I
function	I
h2	O
(	O
p	O
)	O
(	O
cid:17	O
)	O
h	O
(	O
p	O
;	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
=	O
p	O
log	O
1	O
1	O
p	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
log	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
.	O
example	O
9.6.	O
and	O
now	O
the	O
z	O
channel	B
,	O
with	O
px	O
as	O
above	O
.	O
p	O
(	O
y	O
=	O
1	O
)	O
=	O
0:085.	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
jx	O
)	O
=	O
h2	O
(	O
0:085	O
)	O
(	O
cid:0	O
)	O
[	O
0:9h2	O
(	O
0	O
)	O
+	O
0:1h2	O
(	O
0:15	O
)	O
]	O
=	O
0:42	O
(	O
cid:0	O
)	O
(	O
0:1	O
(	O
cid:2	O
)	O
0:61	O
)	O
=	O
0:36	O
bits	O
:	O
(	O
9.9	O
)	O
the	O
entropy	B
of	O
the	O
source	O
,	O
as	O
above	O
,	O
is	O
h	O
(	O
x	O
)	O
=	O
0:47	O
bits	O
.	O
notice	O
that	O
the	O
mutual	B
information	I
i	O
(	O
x	O
;	O
y	O
)	O
for	O
the	O
z	O
channel	B
is	O
bigger	O
than	O
the	O
mutual	B
information	I
for	O
the	O
binary	B
symmetric	I
channel	I
with	O
the	O
same	O
f	O
.	O
the	O
z	O
channel	B
is	O
a	O
more	O
reliable	O
channel	B
.	O
exercise	O
9.7	O
.	O
[	O
1	O
,	O
p.157	O
]	O
compute	O
the	O
mutual	B
information	I
between	O
x	O
and	O
y	O
for	O
the	O
binary	B
symmetric	I
channel	I
with	O
f	O
=	O
0:15	O
when	O
the	O
input	O
distribution	O
is	O
px	O
=	O
fp0	O
=	O
0:5	O
;	O
p1	O
=	O
0:5g	O
.	O
exercise	O
9.8	O
.	O
[	O
2	O
,	O
p.157	O
]	O
compute	O
the	O
mutual	B
information	I
between	O
x	O
and	O
y	O
for	O
the	O
z	O
channel	O
with	O
f	O
=	O
0:15	O
when	O
the	O
input	O
distribution	O
is	O
px	O
:	O
fp0	O
=	O
0:5	O
;	O
p1	O
=	O
0:5g	O
.	O
maximizing	O
the	O
mutual	B
information	I
we	O
have	O
observed	O
in	O
the	O
above	O
examples	O
that	O
the	O
mutual	B
information	I
between	O
the	O
input	O
and	O
the	O
output	O
depends	O
on	O
the	O
chosen	O
input	B
ensemble	I
.	O
let	O
us	O
assume	O
that	O
we	O
wish	O
to	O
maximize	O
the	O
mutual	B
information	I
conveyed	O
by	O
the	O
channel	B
by	O
choosing	O
the	O
best	O
possible	O
input	B
ensemble	I
.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
capacity	B
of	O
the	O
channel	B
to	O
be	O
its	O
maximum	O
mutual	O
information	B
.	O
the	O
capacity	B
of	O
a	O
channel	B
q	O
is	O
:	O
c	O
(	O
q	O
)	O
=	O
max	O
px	O
i	O
(	O
x	O
;	O
y	O
)	O
:	O
(	O
9.10	O
)	O
the	O
distribution	B
px	O
that	O
achieves	O
the	O
maximum	O
is	O
called	O
the	O
optimal	B
input	I
distribution	I
,	O
denoted	O
by	O
p	O
(	O
cid:3	O
)	O
x	O
.	O
[	O
there	O
may	O
be	O
multiple	O
optimal	O
input	O
distributions	O
achieving	O
the	O
same	O
value	O
of	O
i	O
(	O
x	O
;	O
y	O
)	O
.	O
]	O
in	O
chapter	O
10	O
we	O
will	O
show	O
that	O
the	O
capacity	B
does	O
indeed	O
measure	O
the	O
maxi-	O
mum	O
amount	O
of	O
error-free	O
information	B
that	O
can	O
be	O
transmitted	O
over	O
the	O
chan-	O
nel	O
per	O
unit	O
time	O
.	O
example	O
9.9.	O
consider	O
the	O
binary	B
symmetric	I
channel	I
with	O
f	O
=	O
0:15.	O
above	O
,	O
we	O
considered	O
px	O
=	O
fp0	O
=	O
0:9	O
;	O
p1	O
=	O
0:1g	O
,	O
and	O
found	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
0:15	O
bits	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
9.6	O
:	O
the	O
noisy-channel	B
coding	I
theorem	I
151	O
how	O
much	O
better	O
can	O
we	O
do	O
?	O
by	O
symmetry	O
,	O
the	O
optimal	O
input	O
distribu-	O
tion	O
is	O
f0:5	O
;	O
0:5g	O
and	O
the	O
capacity	B
is	O
i	O
(	O
x	O
;	O
y	O
)	O
c	O
(	O
qbsc	O
)	O
=	O
h2	O
(	O
0:5	O
)	O
(	O
cid:0	O
)	O
h2	O
(	O
0:15	O
)	O
=	O
1:0	O
(	O
cid:0	O
)	O
0:61	O
=	O
0:39	O
bits	O
:	O
(	O
9.11	O
)	O
we	O
’	O
ll	O
justify	O
the	O
symmetry	B
argument	I
later	O
.	O
if	O
there	O
’	O
s	O
any	O
doubt	O
about	O
the	O
symmetry	B
argument	I
,	O
we	O
can	O
always	O
resort	O
to	O
explicit	O
maximization	O
of	O
the	O
mutual	O
information	B
i	O
(	O
x	O
;	O
y	O
)	O
,	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h2	O
(	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
p1	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
p1	O
)	O
f	O
)	O
(	O
cid:0	O
)	O
h2	O
(	O
f	O
)	O
(	O
(	O
cid:12	O
)	O
gure	O
9.2	O
)	O
.	O
(	O
9.12	O
)	O
example	O
9.10.	O
the	O
noisy	B
typewriter	I
.	O
the	O
optimal	B
input	I
distribution	I
is	O
a	O
uni-	O
form	O
distribution	B
over	O
x	O
,	O
and	O
gives	O
c	O
=	O
log2	O
9	O
bits	O
.	O
example	O
9.11.	O
consider	O
the	O
z	O
channel	O
with	O
f	O
=	O
0:15.	O
identifying	O
the	O
optimal	B
input	I
distribution	I
is	O
not	O
so	O
straightforward	O
.	O
we	O
evaluate	O
i	O
(	O
x	O
;	O
y	O
)	O
explic-	O
itly	O
for	O
px	O
=	O
fp0	O
;	O
p1g	O
.	O
first	O
,	O
we	O
need	O
to	O
compute	O
p	O
(	O
y	O
)	O
.	O
the	O
probability	O
of	O
y	O
=	O
1	O
is	O
easiest	O
to	O
write	O
down	O
:	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
0.25	O
0.5	O
0.75	O
1	O
p1	O
figure	O
9.2.	O
the	O
mutual	B
information	I
i	O
(	O
x	O
;	O
y	O
)	O
for	O
a	O
binary	B
symmetric	I
channel	I
with	O
f	O
=	O
0:15	O
as	O
a	O
function	B
of	O
the	O
input	O
distribution	O
.	O
p	O
(	O
y	O
=	O
1	O
)	O
=	O
p1	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
:	O
(	O
9.13	O
)	O
then	O
the	O
mutual	B
information	I
is	O
:	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
jx	O
)	O
=	O
h2	O
(	O
p1	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
)	O
(	O
cid:0	O
)	O
(	O
p0h2	O
(	O
0	O
)	O
+	O
p1h2	O
(	O
f	O
)	O
)	O
=	O
h2	O
(	O
p1	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
)	O
(	O
cid:0	O
)	O
p1h2	O
(	O
f	O
)	O
:	O
(	O
9.14	O
)	O
this	O
is	O
a	O
non-trivial	O
function	B
of	O
p1	O
,	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
9.3.	O
it	O
is	O
maximized	O
for	O
f	O
=	O
0:15	O
by	O
p	O
(	O
cid:3	O
)	O
1	O
=	O
0:445.	O
we	O
(	O
cid:12	O
)	O
nd	O
c	O
(	O
qz	O
)	O
=	O
0:685.	O
notice	O
the	O
optimal	B
input	I
distribution	I
is	O
not	O
f0:5	O
;	O
0:5g	O
.	O
we	O
can	O
communicate	O
slightly	O
more	O
information	B
by	O
using	O
input	O
symbol	O
0	O
more	O
frequently	O
than	O
1.	O
i	O
(	O
x	O
;	O
y	O
)	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
0.25	O
0.5	O
0.75	O
1	O
p1	O
figure	O
9.3.	O
the	O
mutual	B
information	I
i	O
(	O
x	O
;	O
y	O
)	O
for	O
a	O
z	O
channel	O
with	O
f	O
=	O
0:15	O
as	O
a	O
function	B
of	O
the	O
input	O
distribution	O
.	O
exercise	O
9.12	O
.	O
[	O
1	O
,	O
p.158	O
]	O
what	O
is	O
the	O
capacity	B
of	O
the	O
binary	B
symmetric	I
channel	I
for	O
general	O
f	O
?	O
exercise	O
9.13	O
.	O
[	O
2	O
,	O
p.158	O
]	O
show	O
that	O
the	O
capacity	B
of	O
the	O
binary	B
erasure	I
channel	I
with	O
f	O
=	O
0:15	O
is	O
cbec	O
=	O
0:85.	O
what	O
is	O
its	O
capacity	B
for	O
general	O
f	O
?	O
comment	O
.	O
9.6	O
the	O
noisy-channel	B
coding	I
theorem	I
it	O
seems	O
plausible	O
that	O
the	O
‘	O
capacity	B
’	O
we	O
have	O
de	O
(	O
cid:12	O
)	O
ned	O
may	O
be	O
a	O
measure	O
of	O
information	O
conveyed	O
by	O
a	O
channel	B
;	O
what	O
is	O
not	O
obvious	O
,	O
and	O
what	O
we	O
will	O
prove	O
in	O
the	O
next	O
chapter	O
,	O
is	O
that	O
the	O
capacity	B
indeed	O
measures	O
the	O
rate	B
at	O
which	O
blocks	O
of	O
data	O
can	O
be	O
communicated	O
over	O
the	O
channel	O
with	O
arbitrarily	O
small	O
probability	B
of	I
error	I
.	O
we	O
make	O
the	O
following	O
de	O
(	O
cid:12	O
)	O
nitions	O
.	O
an	O
(	O
n	O
;	O
k	O
)	O
block	B
code	I
for	O
a	O
channel	B
q	O
is	O
a	O
list	O
of	O
s	O
=	O
2k	O
codewords	O
fx	O
(	O
1	O
)	O
;	O
x	O
(	O
2	O
)	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
2k	O
)	O
g	O
;	O
x	O
(	O
s	O
)	O
2	O
an	O
x	O
;	O
length	B
n	O
.	O
using	O
this	O
code	B
we	O
can	O
encode	O
a	O
signal	O
s	O
2	O
each	O
of	O
f1	O
;	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
;	O
2kg	O
as	O
x	O
(	O
s	O
)	O
.	O
[	O
the	O
number	O
of	O
codewords	O
s	O
is	O
an	O
integer	O
,	O
but	O
the	O
number	O
of	O
bits	O
speci	O
(	O
cid:12	O
)	O
ed	O
by	O
choosing	O
a	O
codeword	B
,	O
k	O
(	O
cid:17	O
)	O
log	O
2	O
s	O
,	O
is	O
not	O
necessarily	O
an	O
integer	O
.	O
]	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
152	O
9	O
|	O
communication	B
over	O
a	O
noisy	B
channel	I
the	O
rate	B
of	O
the	O
code	B
is	O
r	O
=	O
k=n	O
bits	O
per	O
channel	B
use	O
.	O
[	O
we	O
will	O
use	O
this	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
the	O
rate	O
for	O
any	O
channel	B
,	O
not	O
only	O
chan-	O
nels	O
with	O
binary	O
inputs	O
;	O
note	O
however	O
that	O
it	O
is	O
sometimes	O
conventional	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
rate	B
of	O
a	O
code	B
for	O
a	O
channel	O
with	O
q	O
input	O
symbols	O
to	O
be	O
k=	O
(	O
n	O
log	O
q	O
)	O
.	O
]	O
a	O
decoder	B
for	O
an	O
(	O
n	O
;	O
k	O
)	O
block	B
code	I
is	O
a	O
mapping	B
from	O
the	O
set	B
of	O
length-n	O
y	O
,	O
to	O
a	O
codeword	B
label	O
^s	O
2	O
f0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
2kg	O
.	O
strings	O
of	O
channel	O
outputs	O
,	O
an	O
the	O
extra	O
symbol	O
^s	O
=	O
0	O
can	O
be	O
used	O
to	O
indicate	O
a	O
‘	O
failure	O
’	O
.	O
the	O
probability	B
of	I
block	I
error	I
of	O
a	O
code	B
and	O
decoder	B
,	O
for	O
a	O
given	O
channel	B
,	O
and	O
for	O
a	O
given	O
probability	B
distribution	O
over	O
the	O
encoded	O
signal	O
p	O
(	O
sin	O
)	O
,	O
is	O
:	O
pb	O
=xsin	O
p	O
(	O
sin	O
)	O
p	O
(	O
sout6=	O
sin	O
j	O
sin	O
)	O
:	O
the	O
maximal	O
probability	B
of	I
block	I
error	I
is	O
pbm	O
=	O
max	O
sin	O
p	O
(	O
sout6=	O
sin	O
j	O
sin	O
)	O
:	O
(	O
9.15	O
)	O
(	O
9.16	O
)	O
the	O
optimal	B
decoder	I
for	O
a	O
channel	B
code	O
is	O
the	O
one	O
that	O
minimizes	O
the	O
prob-	O
ability	O
of	O
block	O
error	O
.	O
it	O
decodes	O
an	O
output	O
y	O
as	O
the	O
input	O
s	O
that	O
has	O
maximum	O
posterior	O
probability	B
p	O
(	O
sj	O
y	O
)	O
.	O
p	O
(	O
sj	O
y	O
)	O
=	O
p	O
(	O
y	O
j	O
s	O
)	O
p	O
(	O
s	O
)	O
ps0	O
p	O
(	O
y	O
j	O
s0	O
)	O
p	O
(	O
s0	O
)	O
^soptimal	O
=	O
argmax	O
p	O
(	O
sj	O
y	O
)	O
:	O
(	O
9.17	O
)	O
(	O
9.18	O
)	O
a	O
uniform	O
prior	B
distribution	O
on	O
s	O
is	O
usually	O
assumed	O
,	O
in	O
which	O
case	O
the	O
optimal	B
decoder	I
is	O
also	O
the	O
maximum	B
likelihood	I
decoder	O
,	O
i.e.	O
,	O
the	O
decoder	B
that	O
maps	O
an	O
output	O
y	O
to	O
the	O
input	O
s	O
that	O
has	O
maximum	B
likelihood	I
p	O
(	O
y	O
j	O
s	O
)	O
.	O
the	O
probability	O
of	O
bit	O
error	O
pb	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
assuming	O
that	O
the	O
codeword	B
number	O
s	O
is	O
represented	O
by	O
a	O
binary	O
vector	O
s	O
of	O
length	O
k	O
bits	O
;	O
it	O
is	O
the	O
average	B
probability	O
that	O
a	O
bit	B
of	O
sout	O
is	O
not	O
equal	O
to	O
the	O
corresponding	O
bit	B
of	O
sin	O
(	O
averaging	O
over	O
all	O
k	O
bits	O
)	O
.	O
shannon	O
’	O
s	O
noisy-channel	B
coding	I
theorem	I
(	O
part	O
one	O
)	O
.	O
associated	O
with	O
each	O
discrete	B
memoryless	I
channel	O
,	O
there	O
is	O
a	O
non-negative	O
number	O
c	O
(	O
called	O
the	O
channel	O
capacity	O
)	O
with	O
the	O
following	O
property	O
.	O
for	O
any	O
(	O
cid:15	O
)	O
>	O
0	O
and	O
r	O
<	O
c	O
,	O
for	O
large	O
enough	O
n	O
,	O
there	O
exists	O
a	O
block	B
code	I
of	O
length	B
n	O
and	O
rate	O
(	O
cid:21	O
)	O
r	O
and	O
a	O
decoding	B
algorithm	O
,	O
such	O
that	O
the	O
maximal	O
probability	B
of	I
block	I
error	I
is	O
<	O
(	O
cid:15	O
)	O
.	O
con	O
(	O
cid:12	O
)	O
rmation	O
of	O
the	O
theorem	O
for	O
the	O
noisy	B
typewriter	I
channel	O
in	O
the	O
case	O
of	O
the	O
noisy	O
typewriter	O
,	O
we	O
can	O
easily	O
con	O
(	O
cid:12	O
)	O
rm	O
the	O
theorem	B
,	O
because	O
we	O
can	O
create	O
a	O
completely	O
error-free	O
communication	B
strategy	O
using	O
a	O
block	B
code	I
of	O
length	B
n	O
=	O
1	O
:	O
we	O
use	O
only	O
the	O
letters	O
b	O
,	O
e	O
,	O
h	O
,	O
.	O
.	O
.	O
,	O
z	O
,	O
i.e.	O
,	O
every	O
third	O
letter	O
.	O
these	O
letters	O
form	O
a	O
non-confusable	O
subset	O
of	O
the	O
input	O
alphabet	O
(	O
see	O
(	O
cid:12	O
)	O
gure	O
9.5	O
)	O
.	O
any	O
output	O
can	O
be	O
uniquely	O
decoded	O
.	O
the	O
number	O
of	O
inputs	O
in	O
the	O
non-confusable	O
subset	O
is	O
9	O
,	O
so	O
the	O
error-free	O
information	B
rate	O
of	O
this	O
system	O
is	O
log2	O
9	O
bits	O
,	O
which	O
is	O
equal	O
to	O
the	O
capacity	B
c	O
,	O
which	O
we	O
evaluated	O
in	O
example	O
9.10	O
(	O
p.151	O
)	O
.	O
pbm	O
6	O
achievable	O
-	O
r	O
c	O
figure	O
9.4.	O
portion	O
of	O
the	O
r	O
;	O
pbm	O
plane	O
asserted	O
to	O
be	O
achievable	O
by	O
the	O
(	O
cid:12	O
)	O
rst	O
part	O
of	O
shannon	O
’	O
s	O
noisy	B
channel	I
coding	O
theorem	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
9.7	O
:	O
intuitive	O
preview	O
of	O
proof	O
153	O
-	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1	O
pppq	O
-	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1	O
pppq	O
-	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1	O
pppq	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
1	O
-	O
pppq	O
...	O
b	O
e	O
h	O
z	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
-	O
figure	O
9.5.	O
a	O
non-confusable	O
subset	O
of	O
inputs	O
for	O
the	O
noisy	B
typewriter	I
.	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
y	O
z	O
-	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
h	O
i	O
j	O
k	O
l	O
m	O
n	O
o	O
p	O
q	O
r	O
s	O
t	O
u	O
v	O
w	O
x	O
y	O
z	O
-	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
1	O
1	O
0	O
1	O
1	O
1	O
1	O
figure	O
9.6.	O
extended	B
channels	O
obtained	O
from	O
a	O
binary	B
symmetric	I
channel	I
with	O
transition	B
probability	I
0.15	O
.	O
0000	O
1000	O
0100	O
1100	O
0010	O
1010	O
0110	O
1110	O
0001	O
1001	O
0101	O
1101	O
0011	O
1011	O
0111	O
1111	O
0	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
0	O
1	O
00	O
10	O
01	O
11	O
0	O
1	O
n	O
=	O
1	O
n	O
=	O
2	O
n	O
=	O
4	O
how	O
does	O
this	O
translate	O
into	O
the	O
terms	O
of	O
the	O
theorem	O
?	O
the	O
following	O
table	O
explains	O
.	O
the	O
theorem	B
associated	O
with	O
each	O
discrete	B
memoryless	I
channel	O
,	O
there	O
is	O
a	O
non-negative	O
number	O
c.	O
for	O
any	O
(	O
cid:15	O
)	O
>	O
0	O
and	O
r	O
<	O
c	O
,	O
for	O
large	O
enough	O
n	O
,	O
there	O
exists	O
a	O
block	B
code	I
of	O
length	B
n	O
and	O
rate	O
(	O
cid:21	O
)	O
r	O
and	O
a	O
decoding	B
algorithm	O
,	O
how	O
it	O
applies	O
to	O
the	O
noisy	B
typewriter	I
the	O
capacity	B
c	O
is	O
log2	O
9.	O
no	O
matter	O
what	O
(	O
cid:15	O
)	O
and	O
r	O
are	O
,	O
we	O
set	B
the	O
blocklength	O
n	O
to	O
1.	O
the	O
block	B
code	I
is	O
fb	O
;	O
e	O
;	O
:	O
:	O
:	O
;	O
zg	O
.	O
the	O
value	O
of	O
k	O
is	O
given	O
by	O
2k	O
=	O
9	O
,	O
so	O
k	O
=	O
log2	O
9	O
,	O
and	O
this	O
code	B
has	O
rate	B
log2	O
9	O
,	O
which	O
is	O
greater	O
than	O
the	O
requested	O
value	O
of	O
r.	O
the	O
decoding	B
algorithm	O
maps	O
the	O
received	O
letter	O
to	O
the	O
nearest	O
letter	O
in	O
the	O
code	B
;	O
such	O
that	O
the	O
maximal	O
probability	B
of	I
block	I
error	I
is	O
<	O
(	O
cid:15	O
)	O
.	O
the	O
maximal	O
probability	B
of	I
block	I
error	I
is	O
zero	O
,	O
which	O
is	O
less	O
than	O
the	O
given	O
(	O
cid:15	O
)	O
.	O
9.7	O
intuitive	O
preview	O
of	O
proof	O
extended	B
channels	O
to	O
prove	O
the	O
theorem	B
for	O
any	O
given	O
channel	B
,	O
we	O
consider	O
the	O
extended	B
channel	I
corresponding	O
to	O
n	O
uses	O
of	O
the	O
channel	B
.	O
the	O
extended	B
channel	I
has	O
jaxjn	O
possible	O
inputs	O
x	O
and	O
jay	O
jn	O
possible	O
outputs	O
.	O
extended	B
channels	O
obtained	O
from	O
a	O
binary	B
symmetric	I
channel	I
and	O
from	O
a	O
z	O
channel	B
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gures	O
9.6	O
and	O
9.7	O
,	O
with	O
n	O
=	O
2	O
and	O
n	O
=	O
4	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
154	O
9	O
|	O
communication	B
over	O
a	O
noisy	B
channel	I
figure	O
9.7.	O
extended	B
channels	O
obtained	O
from	O
a	O
z	O
channel	O
with	O
transition	O
probability	B
0.15.	O
each	O
column	O
corresponds	O
to	O
an	O
input	O
,	O
and	O
each	O
row	O
is	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
output	O
.	O
figure	O
9.8	O
.	O
(	O
a	O
)	O
some	O
typical	B
outputs	O
in	O
an	O
y	O
corresponding	O
to	O
typical	B
inputs	O
x	O
.	O
(	O
b	O
)	O
a	O
subset	B
of	O
the	O
typical	B
sets	O
shown	O
in	O
(	O
a	O
)	O
that	O
do	O
not	O
overlap	O
each	O
other	O
.	O
this	O
picture	O
can	O
be	O
compared	O
with	O
the	O
solution	O
to	O
the	O
noisy	B
typewriter	I
in	O
(	O
cid:12	O
)	O
gure	O
9.5	O
.	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
1	O
1	O
0	O
1	O
1	O
1	O
1	O
0000	O
1000	O
0100	O
1100	O
0010	O
1010	O
0110	O
1110	O
0001	O
1001	O
0101	O
1101	O
0011	O
1011	O
0111	O
1111	O
0	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
0	O
1	O
00	O
10	O
01	O
11	O
0	O
1	O
n	O
=	O
1	O
n	O
=	O
2	O
n	O
=	O
4	O
an	O
typical	B
y	O
y	O
’	O
$	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
&	O
%	O
typical	B
y	O
for	O
a	O
given	O
typical	B
x	O
6	O
an	O
$	O
typical	B
y	O
y	O
’	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
&	O
%	O
(	O
a	O
)	O
(	O
b	O
)	O
exercise	O
9.14	O
.	O
[	O
2	O
,	O
p.159	O
]	O
find	O
the	O
transition	B
probability	I
matrices	O
q	O
for	O
the	O
ex-	O
tended	O
channel	B
,	O
with	O
n	O
=	O
2	O
,	O
derived	O
from	O
the	O
binary	B
erasure	I
channel	I
having	O
erasure	B
probability	O
0.15.	O
by	O
selecting	O
two	O
columns	O
of	O
this	O
transition	B
probability	I
matrix	O
,	O
we	O
can	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
rate-1/2	O
code	B
for	O
this	O
channel	O
with	O
blocklength	O
n	O
=	O
2.	O
what	O
is	O
the	O
best	O
choice	O
of	O
two	O
columns	O
?	O
what	O
is	O
the	O
decoding	B
algorithm	O
?	O
to	O
prove	O
the	O
noisy-channel	B
coding	I
theorem	I
,	O
we	O
make	O
use	O
of	O
large	O
block-	O
lengths	O
n	O
.	O
the	O
intuitive	O
idea	O
is	O
that	O
,	O
if	O
n	O
is	O
large	O
,	O
an	O
extended	B
channel	I
looks	O
a	O
lot	O
like	O
the	O
noisy	B
typewriter	I
.	O
any	O
particular	O
input	O
x	O
is	O
very	O
likely	O
to	O
produce	O
an	O
output	O
in	O
a	O
small	O
subspace	O
of	O
the	O
output	O
alphabet	O
{	O
the	O
typical	B
output	O
set	B
,	O
given	O
that	O
input	O
.	O
so	O
we	O
can	O
(	O
cid:12	O
)	O
nd	O
a	O
non-confusable	O
subset	O
of	O
the	O
inputs	O
that	O
produce	O
essentially	O
disjoint	O
output	O
sequences	O
.	O
for	O
a	O
given	O
n	O
,	O
let	O
us	O
consider	O
a	O
way	O
of	O
generating	O
such	O
a	O
non-confusable	O
subset	O
of	O
the	O
inputs	O
,	O
and	O
count	O
up	O
how	O
many	O
distinct	O
inputs	O
it	O
contains	O
.	O
imagine	O
making	O
an	O
input	O
sequence	O
x	O
for	O
the	O
extended	B
channel	I
by	O
drawing	O
it	O
from	O
an	O
ensemble	B
x	O
n	O
,	O
where	O
x	O
is	O
an	O
arbitrary	O
ensemble	B
over	O
the	O
input	O
alphabet	O
.	O
recall	O
the	O
source	B
coding	I
theorem	I
of	O
chapter	O
4	O
,	O
and	O
consider	O
the	O
number	O
of	O
probable	O
output	O
sequences	O
y.	O
the	O
total	O
number	O
of	O
typical	O
output	O
sequences	O
y	O
is	O
2n	O
h	O
(	O
y	O
)	O
,	O
all	O
having	O
similar	O
probability	B
.	O
for	O
any	O
particular	O
typical	B
input	O
sequence	B
x	O
,	O
there	O
are	O
about	O
2n	O
h	O
(	O
y	O
jx	O
)	O
probable	O
sequences	O
.	O
some	O
of	O
these	O
subsets	O
of	O
an	O
we	O
now	O
imagine	O
restricting	O
ourselves	O
to	O
a	O
subset	B
of	O
the	O
typical	B
inputs	O
x	O
such	O
that	O
the	O
corresponding	O
typical	B
output	O
sets	O
do	O
not	O
overlap	O
,	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
9.8b	O
.	O
we	O
can	O
then	O
bound	B
the	O
number	O
of	O
non-confusable	O
inputs	O
by	O
dividing	O
the	O
size	O
of	O
the	O
typical	O
y	O
set	B
,	O
2n	O
h	O
(	O
y	O
)	O
,	O
by	O
the	O
size	O
of	O
each	O
typical-y-	O
y	O
are	O
depicted	O
by	O
circles	O
in	O
(	O
cid:12	O
)	O
gure	O
9.8a	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
9.8	O
:	O
further	O
exercises	O
155	O
given-typical-x	O
set	B
,	O
2n	O
h	O
(	O
y	O
jx	O
)	O
.	O
so	O
the	O
number	O
of	O
non-confusable	O
inputs	O
,	O
if	O
they	O
are	O
selected	O
from	O
the	O
set	B
of	O
typical	B
inputs	O
x	O
(	O
cid:24	O
)	O
x	O
n	O
,	O
is	O
(	O
cid:20	O
)	O
2n	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
n	O
h	O
(	O
y	O
jx	O
)	O
=	O
2n	O
i	O
(	O
x	O
;	O
y	O
)	O
.	O
the	O
maximum	O
value	O
of	O
this	O
bound	B
is	O
achieved	O
if	O
x	O
is	O
the	O
ensemble	B
that	O
maximizes	O
i	O
(	O
x	O
;	O
y	O
)	O
,	O
in	O
which	O
case	O
the	O
number	O
of	O
non-confusable	O
inputs	O
is	O
(	O
cid:20	O
)	O
2n	O
c	O
.	O
thus	O
asymptotically	O
up	O
to	O
c	O
bits	O
per	O
cycle	O
,	O
and	O
no	O
more	O
,	O
can	O
be	O
communicated	O
with	O
vanishing	O
error	B
probability	I
.	O
2	O
this	O
sketch	O
has	O
not	O
rigorously	O
proved	O
that	O
reliable	O
communication	B
really	O
is	O
possible	O
{	O
that	O
’	O
s	O
our	O
task	O
for	O
the	O
next	O
chapter	O
.	O
9.8	O
further	O
exercises	O
exercise	O
9.15	O
.	O
[	O
3	O
,	O
p.159	O
]	O
refer	O
back	O
to	O
the	O
computation	O
of	O
the	O
capacity	O
of	O
the	O
z	O
channel	O
with	O
f	O
=	O
0:15	O
.	O
(	O
a	O
)	O
why	O
is	O
p	O
(	O
cid:3	O
)	O
1	O
less	O
than	O
0.5	O
?	O
one	O
could	O
argue	O
that	O
it	O
is	O
good	B
to	O
favour	O
the	O
0	O
input	O
,	O
since	O
it	O
is	O
transmitted	O
without	O
error	O
{	O
and	O
also	O
argue	O
that	O
it	O
is	O
good	B
to	O
favour	O
the	O
1	O
input	O
,	O
since	O
it	O
often	O
gives	O
rise	O
to	O
the	O
highly	O
prized	O
1	O
output	O
,	O
which	O
allows	O
certain	O
identi	O
(	O
cid:12	O
)	O
cation	O
of	O
the	O
input	O
!	O
try	O
to	O
make	O
a	O
convincing	O
argument	O
.	O
(	O
b	O
)	O
in	O
the	O
case	O
of	O
general	O
f	O
,	O
show	O
that	O
the	O
optimal	B
input	I
distribution	I
is	O
p	O
(	O
cid:3	O
)	O
1	O
=	O
1=	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
1	O
+	O
2	O
(	O
h2	O
(	O
f	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
)	O
:	O
(	O
9.19	O
)	O
(	O
c	O
)	O
what	O
happens	O
to	O
p	O
(	O
cid:3	O
)	O
1	O
if	O
the	O
noise	B
level	O
f	O
is	O
very	O
close	O
to	O
1	O
?	O
exercise	O
9.16	O
.	O
[	O
2	O
,	O
p.159	O
]	O
sketch	O
graphs	O
of	O
the	O
capacity	B
of	O
the	O
z	O
channel	B
,	O
the	O
binary	B
symmetric	I
channel	I
and	O
the	O
binary	B
erasure	I
channel	I
as	O
a	O
function	B
of	O
f	O
.	O
.	O
exercise	O
9.17	O
.	O
[	O
2	O
]	O
what	O
is	O
the	O
capacity	B
of	O
the	O
(	O
cid:12	O
)	O
ve-input	O
,	O
ten-output	O
channel	B
whose	O
transition	B
probability	I
matrix	O
is	O
0	O
0	O
0:25	O
0:25	O
0:25	O
0:25	O
0:25	O
0:25	O
0	O
0	O
0	O
0	O
0:25	O
0:25	O
0:25	O
0:25	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0:25	O
0:25	O
0:25	O
0:25	O
0	O
0	O
0:25	O
0:25	O
0:25	O
0:25	O
2	O
666666666666664	O
0	O
0	O
0	O
0	O
0	O
0	O
0:25	O
0:25	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
2	O
3	O
4	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
?	O
(	O
9.20	O
)	O
3	O
777777777777775	O
exercise	O
9.18	O
.	O
[	O
2	O
,	O
p.159	O
]	O
consider	O
a	O
gaussian	O
channel	O
with	O
binary	O
input	O
x	O
2	O
f	O
(	O
cid:0	O
)	O
1	O
;	O
+1g	O
and	O
real	O
output	O
alphabet	O
ay	O
,	O
with	O
transition	O
probability	B
den-	O
sity	O
q	O
(	O
y	O
j	O
x	O
;	O
(	O
cid:11	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
where	O
(	O
cid:11	O
)	O
is	O
the	O
signal	O
amplitude	O
.	O
1	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
e	O
(	O
cid:0	O
)	O
(	O
y	O
(	O
cid:0	O
)	O
x	O
(	O
cid:11	O
)	O
)	O
2	O
2	O
(	O
cid:27	O
)	O
2	O
;	O
(	O
9.21	O
)	O
(	O
a	O
)	O
compute	O
the	O
posterior	B
probability	I
of	O
x	O
given	O
y	O
,	O
assuming	O
that	O
the	O
two	O
inputs	O
are	O
equiprobable	O
.	O
put	O
your	O
answer	O
in	O
the	O
form	O
p	O
(	O
x	O
=	O
1j	O
y	O
;	O
(	O
cid:11	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
1	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
a	O
(	O
y	O
)	O
:	O
(	O
9.22	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
156	O
9	O
|	O
communication	B
over	O
a	O
noisy	B
channel	I
sketch	O
the	O
value	O
of	O
p	O
(	O
x	O
=	O
1j	O
y	O
;	O
(	O
cid:11	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
as	O
a	O
function	B
of	O
y	O
.	O
(	O
b	O
)	O
assume	O
that	O
a	O
single	O
bit	O
is	O
to	O
be	O
transmitted	O
.	O
what	O
is	O
the	O
optimal	B
decoder	I
,	O
and	O
what	O
is	O
its	O
probability	B
of	I
error	I
?	O
express	O
your	O
answer	O
in	O
terms	O
of	O
the	O
signal-to-noise	O
ratio	O
(	O
cid:11	O
)	O
2=	O
(	O
cid:27	O
)	O
2	O
and	O
the	O
error	B
function	I
(	O
the	O
cumulative	B
probability	I
function	I
of	O
the	O
gaussian	O
distribution	B
)	O
,	O
(	O
cid:8	O
)	O
(	O
z	O
)	O
(	O
cid:17	O
)	O
z	O
z	O
(	O
cid:0	O
)	O
1	O
1	O
p2	O
(	O
cid:25	O
)	O
z2	O
2	O
dz	O
:	O
e	O
(	O
cid:0	O
)	O
(	O
9.23	O
)	O
[	O
note	O
that	O
this	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
the	O
error	O
function	B
(	O
cid:8	O
)	O
(	O
z	O
)	O
may	O
not	O
corre-	O
spond	O
to	O
other	O
people	O
’	O
s	O
.	O
]	O
pattern	B
recognition	I
as	O
a	O
noisy	B
channel	I
we	O
may	O
think	O
of	O
many	O
pattern	B
recognition	I
problems	O
in	O
terms	O
of	O
communi-	O
cation	O
channels	O
.	O
consider	O
the	O
case	O
of	O
recognizing	O
handwritten	B
digits	I
(	O
such	O
as	O
postcodes	O
on	O
envelopes	O
)	O
.	O
the	O
author	O
of	O
the	O
digit	O
wishes	O
to	O
communicate	O
a	O
message	O
from	O
the	O
set	B
ax	O
=	O
f0	O
;	O
1	O
;	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
;	O
9g	O
;	O
this	O
selected	O
message	O
is	O
the	O
input	O
to	O
the	O
channel	B
.	O
what	O
comes	O
out	O
of	O
the	O
channel	O
is	O
a	O
pattern	O
of	O
ink	O
on	O
paper	O
.	O
if	O
the	O
ink	O
pattern	O
is	O
represented	O
using	O
256	O
binary	O
pixels	O
,	O
the	O
channel	B
q	O
has	O
as	O
its	O
output	O
a	O
random	B
variable	I
y	O
2	O
ay	O
=	O
f0	O
;	O
1g256	O
.	O
an	O
example	O
of	O
an	O
element	O
from	O
this	O
alphabet	O
is	O
shown	O
in	O
the	O
margin	O
.	O
exercise	O
9.19	O
.	O
[	O
2	O
]	O
estimate	O
how	O
many	O
patterns	O
in	O
ay	O
are	O
recognizable	O
as	O
the	O
[	O
the	O
aim	O
of	O
this	O
problem	O
is	O
to	O
try	O
to	O
demonstrate	O
the	O
character	O
‘	O
2	O
’	O
.	O
existence	O
of	O
as	O
many	O
patterns	O
as	O
possible	O
that	O
are	O
recognizable	O
as	O
2s	O
.	O
]	O
discuss	O
how	O
one	O
might	O
model	B
the	O
channel	B
p	O
(	O
y	O
j	O
x	O
=	O
2	O
)	O
.	O
estimate	O
the	O
entropy	B
of	O
the	O
probability	B
distribution	O
p	O
(	O
y	O
j	O
x	O
=	O
2	O
)	O
.	O
one	O
strategy	O
for	O
doing	O
pattern	B
recognition	I
is	O
to	O
create	O
a	O
model	B
for	O
p	O
(	O
y	O
j	O
x	O
)	O
for	O
each	O
value	O
of	O
the	O
input	O
x	O
=	O
f0	O
;	O
1	O
;	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
;	O
9g	O
,	O
then	O
use	O
bayes	O
’	O
theorem	B
to	O
infer	O
x	O
given	O
y.	O
p	O
(	O
xj	O
y	O
)	O
=	O
p	O
(	O
y	O
j	O
x	O
)	O
p	O
(	O
x	O
)	O
px0	O
p	O
(	O
y	O
j	O
x0	O
)	O
p	O
(	O
x0	O
)	O
:	O
(	O
9.24	O
)	O
this	O
strategy	O
is	O
known	O
as	O
full	O
probabilistic	O
modelling	O
or	O
generative	O
modelling	O
.	O
this	O
is	O
essentially	O
how	O
current	O
speech	O
recognition	B
systems	O
work	O
.	O
in	O
addition	O
to	O
the	O
channel	B
model	O
,	O
p	O
(	O
y	O
j	O
x	O
)	O
,	O
one	O
uses	O
a	O
prior	B
proba-	O
bility	O
distribution	B
p	O
(	O
x	O
)	O
,	O
which	O
in	O
the	O
case	O
of	O
both	O
character	O
recognition	B
and	O
speech	O
recognition	B
is	O
a	O
language	B
model	I
that	O
speci	O
(	O
cid:12	O
)	O
es	O
the	O
probability	O
of	O
the	O
next	O
character/word	O
given	O
the	O
context	O
and	O
the	O
known	O
grammar	O
and	O
statistics	O
of	O
the	O
language	O
.	O
random	B
coding	O
exercise	O
9.20	O
.	O
[	O
2	O
,	O
p.160	O
]	O
given	O
twenty-four	O
people	O
in	O
a	O
room	O
,	O
what	O
is	O
the	O
prob-	O
ability	O
that	O
there	O
are	O
at	O
least	O
two	O
people	O
present	O
who	O
have	O
the	O
same	O
birthday	B
(	O
i.e.	O
,	O
day	O
and	O
month	O
of	O
birth	O
)	O
?	O
what	O
is	O
the	O
expected	O
number	O
of	O
pairs	O
of	O
people	O
with	O
the	O
same	O
birthday	B
?	O
which	O
of	O
these	O
two	O
questions	O
is	O
easiest	O
to	O
solve	O
?	O
which	O
answer	O
gives	O
most	O
insight	O
?	O
you	O
may	O
(	O
cid:12	O
)	O
nd	O
it	O
helpful	O
to	O
solve	O
these	O
problems	O
and	O
those	O
that	O
follow	O
using	O
notation	B
such	O
as	O
a	O
=	O
number	O
of	O
days	O
in	O
year	O
=	O
365	O
and	O
s	O
=	O
number	O
of	O
people	O
=	O
24.	O
.	O
exercise	O
9.21	O
.	O
[	O
2	O
]	O
the	O
birthday	B
problem	O
may	O
be	O
related	O
to	O
a	O
coding	O
scheme	O
.	O
assume	O
we	O
wish	O
to	O
convey	O
a	O
message	O
to	O
an	O
outsider	O
identifying	O
one	O
of	O
figure	O
9.9.	O
some	O
more	O
2s	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
9.9	O
:	O
solutions	O
157	O
the	O
twenty-four	O
people	O
.	O
we	O
could	O
simply	O
communicate	O
a	O
number	O
s	O
from	O
as	O
=	O
f1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
24g	O
,	O
having	O
agreed	O
a	O
mapping	B
of	O
people	O
onto	O
numbers	O
;	O
alternatively	O
,	O
we	O
could	O
convey	O
a	O
number	O
from	O
ax	O
=	O
f1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
365g	O
,	O
identifying	O
the	O
day	O
of	O
the	O
year	O
that	O
is	O
the	O
selected	O
person	O
’	O
s	O
birthday	B
(	O
with	O
apologies	O
to	O
leapyearians	O
)	O
.	O
[	O
the	O
receiver	O
is	O
assumed	O
to	O
know	O
all	O
the	O
people	O
’	O
s	O
birthdays	O
.	O
]	O
what	O
,	O
roughly	O
,	O
is	O
the	O
probability	B
of	I
error	I
of	O
this	O
communication	B
scheme	O
,	O
assuming	O
it	O
is	O
used	O
for	O
a	O
single	O
transmission	O
?	O
what	O
is	O
the	O
capacity	B
of	O
the	O
communication	B
channel	O
,	O
and	O
what	O
is	O
the	O
rate	B
of	O
communication	B
attempted	O
by	O
this	O
scheme	O
?	O
.	O
exercise	O
9.22	O
.	O
[	O
2	O
]	O
now	O
imagine	O
that	O
there	O
are	O
k	O
rooms	O
in	O
a	O
building	O
,	O
each	O
containing	O
q	O
people	O
.	O
(	O
you	O
might	O
think	O
of	O
k	O
=	O
2	O
and	O
q	O
=	O
24	O
as	O
an	O
example	O
.	O
)	O
the	O
aim	O
is	O
to	O
communicate	O
a	O
selection	O
of	O
one	O
person	O
from	O
each	O
room	O
by	O
transmitting	O
an	O
ordered	O
list	O
of	O
k	O
days	O
(	O
from	O
ax	O
)	O
.	O
compare	O
the	O
probability	B
of	I
error	I
of	O
the	O
following	O
two	O
schemes	O
.	O
(	O
a	O
)	O
as	O
before	O
,	O
where	O
each	O
room	O
transmits	O
the	O
birthday	B
of	O
the	O
selected	O
person	O
.	O
(	O
b	O
)	O
to	O
each	O
k-tuple	O
of	O
people	O
,	O
one	O
drawn	O
from	O
each	O
room	O
,	O
an	O
ordered	O
k-tuple	O
of	O
randomly	O
selected	O
days	O
from	O
ax	O
is	O
assigned	O
(	O
this	O
k-	O
tuple	O
has	O
nothing	O
to	O
do	O
with	O
their	O
birthdays	O
)	O
.	O
this	O
enormous	O
list	O
of	O
s	O
=	O
qk	O
strings	O
is	O
known	O
to	O
the	O
receiver	O
.	O
when	O
the	O
building	O
has	O
selected	O
a	O
particular	O
person	O
from	O
each	O
room	O
,	O
the	O
ordered	O
string	O
of	O
days	O
corresponding	O
to	O
that	O
k-tuple	O
of	O
people	O
is	O
transmitted	O
.	O
what	O
is	O
the	O
probability	B
of	I
error	I
when	O
q	O
=	O
364	O
and	O
k	O
=	O
1	O
?	O
what	O
is	O
the	O
probability	B
of	I
error	I
when	O
q	O
=	O
364	O
and	O
k	O
is	O
large	O
,	O
e.g	O
.	O
k	O
=	O
6000	O
?	O
9.9	O
solutions	O
solution	O
to	O
exercise	O
9.2	O
(	O
p.149	O
)	O
.	O
if	O
we	O
assume	O
we	O
observe	O
y	O
=	O
0	O
,	O
p	O
(	O
x	O
=	O
1j	O
y	O
=	O
0	O
)	O
=	O
=	O
=	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
1	O
)	O
p	O
(	O
x	O
=	O
1	O
)	O
px0	O
p	O
(	O
y	O
j	O
x0	O
)	O
p	O
(	O
x0	O
)	O
0:15	O
(	O
cid:2	O
)	O
0:1	O
0:15	O
(	O
cid:2	O
)	O
0:1	O
+	O
0:85	O
(	O
cid:2	O
)	O
0:9	O
0:015	O
0:78	O
=	O
0:019	O
:	O
solution	O
to	O
exercise	O
9.4	O
(	O
p.149	O
)	O
.	O
if	O
we	O
observe	O
y	O
=	O
0	O
,	O
p	O
(	O
x	O
=	O
1j	O
y	O
=	O
0	O
)	O
=	O
=	O
0:15	O
(	O
cid:2	O
)	O
0:1	O
0:15	O
(	O
cid:2	O
)	O
0:1	O
+	O
1:0	O
(	O
cid:2	O
)	O
0:9	O
0:015	O
0:915	O
=	O
0:016	O
:	O
(	O
9.25	O
)	O
(	O
9.26	O
)	O
(	O
9.27	O
)	O
(	O
9.28	O
)	O
(	O
9.29	O
)	O
solution	O
to	O
exercise	O
9.7	O
(	O
p.150	O
)	O
.	O
the	O
probability	B
that	O
y	O
=	O
1	O
is	O
0:5	O
,	O
so	O
the	O
mutual	B
information	I
is	O
:	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
j	O
x	O
)	O
=	O
h2	O
(	O
0:5	O
)	O
(	O
cid:0	O
)	O
h2	O
(	O
0:15	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
0:61	O
=	O
0:39	O
bits	O
:	O
(	O
9.30	O
)	O
(	O
9.31	O
)	O
(	O
9.32	O
)	O
solution	O
to	O
exercise	O
9.8	O
(	O
p.150	O
)	O
.	O
we	O
again	O
compute	O
the	O
mutual	B
information	I
using	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
j	O
x	O
)	O
.	O
the	O
probability	B
that	O
y	O
=	O
0	O
is	O
0:575	O
,	O
and	O
 	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
158	O
9	O
|	O
communication	B
over	O
a	O
noisy	B
channel	I
h	O
(	O
y	O
j	O
x	O
)	O
=px	O
p	O
(	O
x	O
)	O
h	O
(	O
y	O
j	O
x	O
)	O
=	O
p	O
(	O
x	O
=	O
1	O
)	O
h	O
(	O
y	O
j	O
x	O
=	O
1	O
)	O
+	O
p	O
(	O
x	O
=	O
0	O
)	O
h	O
(	O
y	O
j	O
x	O
=	O
0	O
)	O
so	O
the	O
mutual	B
information	I
is	O
:	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
j	O
x	O
)	O
=	O
h2	O
(	O
0:575	O
)	O
(	O
cid:0	O
)	O
[	O
0:5	O
(	O
cid:2	O
)	O
h2	O
(	O
0:15	O
)	O
+	O
0:5	O
(	O
cid:2	O
)	O
0	O
]	O
=	O
0:98	O
(	O
cid:0	O
)	O
0:30	O
=	O
0:679	O
bits	O
:	O
(	O
9.33	O
)	O
(	O
9.34	O
)	O
(	O
9.35	O
)	O
solution	O
to	O
exercise	O
9.12	O
(	O
p.151	O
)	O
.	O
by	O
symmetry	O
,	O
the	O
optimal	B
input	I
distribution	I
is	O
f0:5	O
;	O
0:5g	O
.	O
then	O
the	O
capacity	B
is	O
c	O
=	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
j	O
x	O
)	O
=	O
h2	O
(	O
0:5	O
)	O
(	O
cid:0	O
)	O
h2	O
(	O
f	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
f	O
)	O
:	O
(	O
9.36	O
)	O
(	O
9.37	O
)	O
(	O
9.38	O
)	O
would	O
you	O
like	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
optimal	B
input	I
distribution	I
without	O
invoking	O
sym-	O
metry	O
?	O
we	O
can	O
do	O
this	O
by	O
computing	O
the	O
mutual	B
information	I
in	O
the	O
general	O
case	O
where	O
the	O
input	B
ensemble	I
is	O
fp0	O
;	O
p1g	O
:	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
j	O
x	O
)	O
=	O
h2	O
(	O
p0f	O
+	O
p1	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
)	O
(	O
cid:0	O
)	O
h2	O
(	O
f	O
)	O
:	O
(	O
9.39	O
)	O
(	O
9.40	O
)	O
the	O
only	O
p-dependence	O
is	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
term	O
h2	O
(	O
p0f	O
+	O
p1	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
)	O
,	O
which	O
is	O
maximized	O
by	O
setting	O
the	O
argument	O
to	O
0.5.	O
this	O
value	O
is	O
given	O
by	O
setting	O
p0	O
=	O
1=2	O
.	O
solution	O
to	O
exercise	O
9.13	O
(	O
p.151	O
)	O
.	O
answer	O
1.	O
by	O
symmetry	O
,	O
the	O
optimal	B
input	I
distribution	I
is	O
f0:5	O
;	O
0:5g	O
.	O
the	O
capacity	B
is	O
most	O
easily	O
evaluated	O
by	O
writing	O
the	O
mutual	B
information	I
as	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
j	O
y	O
)	O
.	O
the	O
conditional	B
entropy	I
h	O
(	O
x	O
j	O
y	O
)	O
is	O
py	O
p	O
(	O
y	O
)	O
h	O
(	O
x	O
j	O
y	O
)	O
;	O
when	O
y	O
is	O
known	O
,	O
x	O
is	O
uncertain	O
only	O
if	O
y	O
=	O
?	O
,	O
which	O
occurs	O
with	O
probability	O
f	O
=2	O
+	O
f	O
=2	O
,	O
so	O
the	O
conditional	B
entropy	I
h	O
(	O
x	O
j	O
y	O
)	O
is	O
f	O
h2	O
(	O
0:5	O
)	O
.	O
c	O
=	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
j	O
y	O
)	O
=	O
h2	O
(	O
0:5	O
)	O
(	O
cid:0	O
)	O
f	O
h2	O
(	O
0:5	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
f	O
:	O
(	O
9.41	O
)	O
(	O
9.42	O
)	O
(	O
9.43	O
)	O
the	O
binary	B
erasure	I
channel	I
fails	O
a	O
fraction	O
f	O
of	O
the	O
time	O
.	O
its	O
capacity	B
is	O
precisely	O
1	O
(	O
cid:0	O
)	O
f	O
,	O
which	O
is	O
the	O
fraction	O
of	O
the	O
time	O
that	O
the	O
channel	B
is	O
reliable	O
.	O
this	O
result	O
seems	O
very	O
reasonable	O
,	O
but	O
it	O
is	O
far	O
from	O
obvious	O
how	O
to	O
encode	O
information	B
so	O
as	O
to	O
communicate	O
reliably	O
over	O
this	O
channel	B
.	O
answer	O
2.	O
alternatively	O
,	O
without	O
invoking	O
the	O
symmetry	O
assumed	O
above	O
,	O
we	O
can	O
start	O
from	O
the	O
input	B
ensemble	I
fp0	O
;	O
p1g	O
.	O
the	O
probability	B
that	O
y	O
=	O
?	O
is	O
p0f	O
+	O
p1f	O
=	O
f	O
,	O
and	O
when	O
we	O
receive	O
y	O
=	O
?	O
,	O
the	O
posterior	B
probability	I
of	O
x	O
is	O
the	O
same	O
as	O
the	O
prior	B
probability	O
,	O
so	O
:	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
j	O
y	O
)	O
=	O
h2	O
(	O
p1	O
)	O
(	O
cid:0	O
)	O
f	O
h2	O
(	O
p1	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
h2	O
(	O
p1	O
)	O
:	O
(	O
9.44	O
)	O
(	O
9.45	O
)	O
(	O
9.46	O
)	O
this	O
mutual	B
information	I
achieves	O
its	O
maximum	O
value	O
of	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
when	O
p1	O
=	O
1=2	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
9.9	O
:	O
solutions	O
0	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
x	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
0	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
x	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
0	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
00	O
?	O
0	O
10	O
0	O
?	O
?	O
?	O
1	O
?	O
01	O
?	O
1	O
11	O
(	O
a	O
)	O
0	O
1	O
0	O
?	O
q	O
1	O
n	O
=	O
1	O
n	O
=	O
2	O
00	O
?	O
0	O
10	O
0	O
?	O
?	O
?	O
1	O
?	O
01	O
?	O
1	O
11	O
(	O
b	O
)	O
00	O
?	O
0	O
10	O
0	O
?	O
?	O
?	O
1	O
?	O
01	O
?	O
1	O
11	O
(	O
c	O
)	O
-	O
-	O
-	O
-	O
-	O
-	O
-	O
^m	O
=	O
1	O
^m	O
=	O
1	O
^m	O
=	O
1	O
^m	O
=	O
0	O
^m	O
=	O
2	O
^m	O
=	O
2	O
^m	O
=	O
2	O
the	O
extended	B
channel	I
solution	O
to	O
exercise	O
9.14	O
(	O
p.153	O
)	O
.	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
g-	O
ure	O
9.10.	O
the	O
best	O
code	B
for	O
this	O
channel	O
with	O
n	O
=	O
2	O
is	O
obtained	O
by	O
choosing	O
two	O
columns	O
that	O
have	O
minimal	O
overlap	O
,	O
for	O
example	O
,	O
columns	O
00	O
and	O
11.	O
the	O
decoding	B
algorithm	O
returns	O
‘	O
00	O
’	O
if	O
the	O
extended	B
channel	I
output	O
is	O
among	O
the	O
top	O
four	O
and	O
‘	O
11	O
’	O
if	O
it	O
’	O
s	O
among	O
the	O
bottom	O
four	O
,	O
and	O
gives	O
up	O
if	O
the	O
output	O
is	O
‘	O
?	O
?	O
’	O
.	O
solution	O
to	O
exercise	O
9.15	O
(	O
p.155	O
)	O
.	O
mutual	B
information	I
between	O
input	O
and	O
output	O
of	O
the	O
z	O
channel	B
is	O
in	O
example	O
9.11	O
(	O
p.151	O
)	O
we	O
showed	O
that	O
the	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
j	O
x	O
)	O
=	O
h2	O
(	O
p1	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
)	O
(	O
cid:0	O
)	O
p1h2	O
(	O
f	O
)	O
:	O
(	O
9.47	O
)	O
we	O
di	O
(	O
cid:11	O
)	O
erentiate	O
this	O
expression	O
with	O
respect	O
to	O
p1	O
,	O
taking	O
care	O
not	O
to	O
confuse	O
log2	O
with	O
loge	O
:	O
d	O
dp1	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
log2	O
1	O
(	O
cid:0	O
)	O
p1	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
p1	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
(	O
cid:0	O
)	O
h2	O
(	O
f	O
)	O
:	O
(	O
9.48	O
)	O
setting	O
this	O
derivative	O
to	O
zero	O
and	O
rearranging	O
using	O
skills	O
developed	O
in	O
exer-	O
cise	O
2.17	O
(	O
p.36	O
)	O
,	O
we	O
obtain	O
:	O
p	O
(	O
cid:3	O
)	O
1	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
=	O
1	O
1	O
+	O
2h2	O
(	O
f	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
;	O
so	O
the	O
optimal	B
input	I
distribution	I
is	O
p	O
(	O
cid:3	O
)	O
1	O
=	O
1=	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
1	O
+	O
2	O
(	O
h2	O
(	O
f	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
)	O
:	O
(	O
9.49	O
)	O
(	O
9.50	O
)	O
as	O
the	O
noise	B
level	O
f	O
tends	O
to	O
1	O
,	O
this	O
expression	O
tends	O
to	O
1=e	O
(	O
as	O
you	O
can	O
prove	O
using	O
l	O
’	O
h^opital	O
’	O
s	O
rule	O
)	O
.	O
for	O
all	O
values	O
of	O
f	O
,	O
p	O
(	O
cid:3	O
)	O
1	O
is	O
smaller	O
than	O
1=2	O
.	O
a	O
rough	O
intuition	O
for	O
why	O
input	O
1	O
is	O
used	O
less	O
than	O
input	O
0	O
is	O
that	O
when	O
input	O
1	O
is	O
used	O
,	O
the	O
noisy	B
channel	I
injects	O
entropy	B
into	O
the	O
received	O
string	O
;	O
whereas	O
when	O
input	O
0	O
is	O
used	O
,	O
the	O
noise	B
has	O
zero	O
entropy	B
.	O
solution	O
to	O
exercise	O
9.16	O
(	O
p.155	O
)	O
.	O
the	O
capacities	O
of	O
the	O
three	O
channels	O
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
9.11.	O
for	O
any	O
f	O
<	O
0:5	O
,	O
the	O
bec	O
is	O
the	O
channel	O
with	O
highest	O
capacity	B
and	O
the	O
bsc	O
the	O
lowest	O
.	O
solution	O
to	O
exercise	O
9.18	O
(	O
p.155	O
)	O
.	O
the	O
logarithm	O
of	O
the	O
posterior	O
probability	B
ratio	O
,	O
given	O
y	O
,	O
is	O
a	O
(	O
y	O
)	O
=	O
ln	O
p	O
(	O
x	O
=	O
1j	O
y	O
;	O
(	O
cid:11	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
p	O
(	O
x	O
=	O
(	O
cid:0	O
)	O
1j	O
y	O
;	O
(	O
cid:11	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
ln	O
q	O
(	O
y	O
j	O
x	O
=	O
1	O
;	O
(	O
cid:11	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
q	O
(	O
y	O
j	O
x	O
=	O
(	O
cid:0	O
)	O
1	O
;	O
(	O
cid:11	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
2	O
(	O
cid:11	O
)	O
y	O
(	O
cid:27	O
)	O
2	O
:	O
(	O
9.51	O
)	O
159	O
figure	O
9.10	O
.	O
(	O
a	O
)	O
the	O
extended	B
channel	I
(	O
n	O
=	O
2	O
)	O
obtained	O
from	O
a	O
binary	B
erasure	I
channel	I
with	O
erasure	B
probability	O
0.15	O
.	O
(	O
b	O
)	O
a	O
block	B
code	I
consisting	O
of	O
the	O
two	O
codewords	O
00	O
and	O
11	O
.	O
(	O
c	O
)	O
the	O
optimal	B
decoder	I
for	O
this	O
code	B
.	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
z	O
bsc	O
bec	O
0	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1	O
figure	O
9.11.	O
capacities	O
of	O
the	O
z	O
channel	B
,	O
binary	B
symmetric	I
channel	I
,	O
and	O
binary	O
erasure	B
channel	I
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
160	O
9	O
|	O
communication	B
over	O
a	O
noisy	B
channel	I
using	O
our	O
skills	O
picked	O
up	O
from	O
exercise	O
2.17	O
(	O
p.36	O
)	O
,	O
we	O
rewrite	O
this	O
in	O
the	O
form	O
p	O
(	O
x	O
=	O
1j	O
y	O
;	O
(	O
cid:11	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
a	O
(	O
y	O
)	O
1	O
:	O
(	O
9.52	O
)	O
the	O
optimal	B
decoder	I
selects	O
the	O
most	O
probable	O
hypothesis	O
;	O
this	O
can	O
be	O
done	O
simply	O
by	O
looking	O
at	O
the	O
sign	O
of	O
a	O
(	O
y	O
)	O
.	O
if	O
a	O
(	O
y	O
)	O
>	O
0	O
then	O
decode	O
as	O
^x	O
=	O
1.	O
the	O
probability	B
of	I
error	I
is	O
pb	O
=z	O
0	O
(	O
cid:0	O
)	O
1	O
dy	O
q	O
(	O
y	O
j	O
x	O
=	O
1	O
;	O
(	O
cid:11	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=z	O
(	O
cid:0	O
)	O
x	O
(	O
cid:11	O
)	O
(	O
cid:0	O
)	O
1	O
dy	O
1	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
e	O
(	O
cid:0	O
)	O
y2	O
2	O
(	O
cid:27	O
)	O
2	O
=	O
(	O
cid:8	O
)	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
x	O
(	O
cid:11	O
)	O
(	O
cid:27	O
)	O
(	O
cid:17	O
)	O
:	O
(	O
9.53	O
)	O
random	B
coding	O
solution	O
to	O
exercise	O
9.20	O
(	O
p.156	O
)	O
.	O
the	O
probability	B
that	O
s	O
=	O
24	O
people	O
whose	O
birthdays	O
are	O
drawn	O
at	O
random	B
from	O
a	O
=	O
365	O
days	O
all	O
have	O
distinct	O
birthdays	O
is	O
a	O
(	O
a	O
(	O
cid:0	O
)	O
1	O
)	O
(	O
a	O
(	O
cid:0	O
)	O
2	O
)	O
:	O
:	O
:	O
(	O
a	O
(	O
cid:0	O
)	O
s	O
+	O
1	O
)	O
:	O
(	O
9.54	O
)	O
as	O
the	O
probability	B
that	O
two	O
(	O
or	O
more	O
)	O
people	O
share	O
a	O
birthday	B
is	O
one	O
minus	O
this	O
quantity	O
,	O
which	O
,	O
for	O
s	O
=	O
24	O
and	O
a	O
=	O
365	O
,	O
is	O
about	O
0.5.	O
this	O
exact	O
way	O
of	O
answering	O
the	O
question	O
is	O
not	O
very	O
informative	O
since	O
it	O
is	O
not	O
clear	O
for	O
what	O
value	O
of	O
s	O
the	O
probability	B
changes	O
from	O
being	O
close	O
to	O
0	O
to	O
being	O
close	O
to	O
1.	O
the	O
number	O
of	O
pairs	O
is	O
s	O
(	O
s	O
(	O
cid:0	O
)	O
1	O
)	O
=2	O
,	O
and	O
the	O
probability	B
that	O
a	O
particular	O
pair	O
shares	O
a	O
birthday	B
is	O
1=a	O
,	O
so	O
the	O
expected	O
number	O
of	O
collisions	O
is	O
s	O
(	O
s	O
(	O
cid:0	O
)	O
1	O
)	O
2	O
1	O
a	O
:	O
(	O
9.55	O
)	O
this	O
answer	O
is	O
more	O
instructive	O
.	O
the	O
expected	O
number	O
of	O
collisions	O
is	O
tiny	O
if	O
s	O
(	O
cid:28	O
)	O
pa	O
and	O
big	O
if	O
s	O
(	O
cid:29	O
)	O
pa.	O
we	O
can	O
also	O
approximate	O
the	O
probability	B
that	O
all	O
birthdays	O
are	O
distinct	O
,	O
for	O
small	O
s	O
,	O
thus	O
:	O
a	O
(	O
a	O
(	O
cid:0	O
)	O
1	O
)	O
(	O
a	O
(	O
cid:0	O
)	O
2	O
)	O
:	O
:	O
:	O
(	O
a	O
(	O
cid:0	O
)	O
s	O
+	O
1	O
)	O
as	O
’	O
exp	O
(	O
0	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
1=a	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
2=a	O
)	O
:	O
:	O
:	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
s	O
(	O
cid:0	O
)	O
1	O
)	O
=a	O
)	O
’	O
exp	O
(	O
cid:0	O
)	O
=	O
(	O
1	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
1/a	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
2/a	O
)	O
:	O
:	O
:	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
s	O
(	O
cid:0	O
)	O
1	O
)	O
/a	O
)	O
(	O
9.56	O
)	O
i	O
!	O
=	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
s	O
(	O
s	O
(	O
cid:0	O
)	O
1	O
)	O
=2	O
(	O
cid:19	O
)	O
:	O
(	O
9.57	O
)	O
a	O
1	O
a	O
s	O
(	O
cid:0	O
)	O
1	O
xi=1	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
chapter	O
10	O
before	O
reading	O
chapter	O
10	O
,	O
you	O
should	O
have	O
read	O
chapters	O
4	O
and	O
9.	O
exer-	O
cise	O
9.14	O
(	O
p.153	O
)	O
is	O
especially	O
recommended	O
.	O
cast	O
of	O
characters	O
q	O
c	O
x	O
n	O
c	O
n	O
x	O
(	O
s	O
)	O
s	O
the	O
noisy	B
channel	I
the	O
capacity	B
of	O
the	O
channel	B
an	O
ensemble	B
used	O
to	O
create	O
a	O
random	B
code	I
a	O
random	B
code	I
the	O
length	B
of	O
the	O
codewords	O
a	O
codeword	B
,	O
the	O
sth	O
in	O
the	O
code	B
the	O
number	O
of	O
a	O
chosen	O
codeword	B
(	O
mnemonic	O
:	O
selects	O
s	O
)	O
the	O
total	O
number	O
of	O
codewords	O
in	O
the	O
code	B
the	O
source	O
s	O
=	O
2k	O
k	O
=	O
log2	O
s	O
the	O
number	O
of	O
bits	O
conveyed	O
by	O
the	O
choice	O
of	O
one	O
codeword	B
s	O
r	O
=	O
k=n	O
^s	O
from	O
s	O
,	O
assuming	O
it	O
is	O
chosen	O
with	O
uniform	O
probability	B
a	O
binary	O
representation	O
of	O
the	O
number	O
s	O
the	O
rate	B
of	O
the	O
code	B
,	O
in	O
bits	O
per	O
channel	B
use	O
(	O
sometimes	O
called	O
r0	O
instead	O
)	O
the	O
decoder	B
’	O
s	O
guess	O
of	O
s	O
161	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
r	O
(	O
pb	O
)	O
3	O
-	O
r	O
(	O
cid:1	O
)	O
2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
c	O
figure	O
10.1.	O
portion	O
of	O
the	O
r	O
;	O
pb	O
plane	O
to	O
be	O
proved	O
achievable	O
(	O
1	O
,	O
2	O
)	O
and	O
not	O
achievable	O
(	O
3	O
)	O
.	O
10	O
the	O
noisy-channel	B
coding	I
theorem	I
10.1	O
the	O
theorem	B
the	O
theorem	B
has	O
three	O
parts	O
,	O
two	O
positive	O
and	O
one	O
negative	O
.	O
the	O
main	O
positive	O
result	O
is	O
the	O
(	O
cid:12	O
)	O
rst	O
.	O
1.	O
for	O
every	O
discrete	B
memoryless	I
channel	O
,	O
the	O
channel	O
capacity	O
6	O
pb	O
c	O
=	O
max	O
px	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
10.1	O
)	O
1	O
has	O
the	O
following	O
property	O
.	O
for	O
any	O
(	O
cid:15	O
)	O
>	O
0	O
and	O
r	O
<	O
c	O
,	O
for	O
large	O
enough	O
n	O
,	O
there	O
exists	O
a	O
code	B
of	O
length	B
n	O
and	O
rate	O
(	O
cid:21	O
)	O
r	O
and	O
a	O
decoding	B
algorithm	O
,	O
such	O
that	O
the	O
maximal	O
probability	B
of	I
block	I
error	I
is	O
<	O
(	O
cid:15	O
)	O
.	O
2.	O
if	O
a	O
probability	O
of	O
bit	O
error	O
pb	O
is	O
acceptable	O
,	O
rates	O
up	O
to	O
r	O
(	O
pb	O
)	O
are	O
achiev-	O
able	O
,	O
where	O
r	O
(	O
pb	O
)	O
=	O
c	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
pb	O
)	O
:	O
(	O
10.2	O
)	O
3.	O
for	O
any	O
pb	O
,	O
rates	O
greater	O
than	O
r	O
(	O
pb	O
)	O
are	O
not	O
achievable	O
.	O
10.2	O
jointly-typical	O
sequences	O
we	O
formalize	O
the	O
intuitive	O
preview	O
of	O
the	O
last	O
chapter	O
.	O
we	O
will	O
de	O
(	O
cid:12	O
)	O
ne	O
codewords	O
x	O
(	O
s	O
)	O
as	O
coming	O
from	O
an	O
ensemble	B
x	O
n	O
,	O
and	O
con-	O
sider	O
the	O
random	B
selection	O
of	O
one	O
codeword	B
and	O
a	O
corresponding	O
channel	B
out-	O
put	O
y	O
,	O
thus	O
de	O
(	O
cid:12	O
)	O
ning	O
a	O
joint	B
ensemble	I
(	O
xy	O
)	O
n	O
.	O
we	O
will	O
use	O
a	O
typical-set	B
decoder	I
,	O
which	O
decodes	O
a	O
received	O
signal	O
y	O
as	O
s	O
if	O
x	O
(	O
s	O
)	O
and	O
y	O
are	O
jointly	O
typical	B
,	O
a	O
term	O
to	O
be	O
de	O
(	O
cid:12	O
)	O
ned	O
shortly	O
.	O
the	O
proof	O
will	O
then	O
centre	O
on	O
determining	O
the	O
probabilities	O
(	O
a	O
)	O
that	O
the	O
true	O
input	O
codeword	O
is	O
not	O
jointly	O
typical	B
with	O
the	O
output	O
sequence	B
;	O
and	O
(	O
b	O
)	O
that	O
a	O
false	O
input	O
codeword	O
is	O
jointly	O
typical	B
with	O
the	O
output	O
.	O
we	O
will	O
show	O
that	O
,	O
for	O
large	O
n	O
,	O
both	O
probabilities	O
go	O
to	O
zero	O
as	O
long	O
as	O
there	O
are	O
fewer	O
than	O
2n	O
c	O
codewords	O
,	O
and	O
the	O
ensemble	B
x	O
is	O
the	O
optimal	B
input	I
distribution	I
.	O
joint	B
typicality	I
.	O
a	O
pair	O
of	O
sequences	O
x	O
;	O
y	O
of	O
length	O
n	O
are	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
jointly	O
typical	B
(	O
to	O
tolerance	O
(	O
cid:12	O
)	O
)	O
with	O
respect	O
to	O
the	O
distribution	B
p	O
(	O
x	O
;	O
y	O
)	O
if	O
x	O
is	O
typical	B
of	O
p	O
(	O
x	O
)	O
,	O
i.e.	O
,	O
y	O
is	O
typical	B
of	O
p	O
(	O
y	O
)	O
,	O
i.e.	O
,	O
and	O
x	O
;	O
y	O
is	O
typical	B
of	O
p	O
(	O
x	O
;	O
y	O
)	O
,	O
i.e.	O
,	O
162	O
log	O
log	O
log	O
1	O
n	O
1	O
n	O
1	O
n	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
1	O
1	O
1	O
<	O
(	O
cid:12	O
)	O
;	O
p	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
p	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
p	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
<	O
(	O
cid:12	O
)	O
;	O
<	O
(	O
cid:12	O
)	O
:	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
10.2	O
:	O
jointly-typical	O
sequences	O
163	O
the	O
jointly-typical	O
set	B
jn	O
(	O
cid:12	O
)	O
is	O
the	O
set	B
of	O
all	O
jointly-typical	O
sequence	B
pairs	O
of	O
length	O
n	O
.	O
example	O
.	O
here	O
is	O
a	O
jointly-typical	O
pair	O
of	O
length	O
n	O
=	O
100	O
for	O
the	O
ensemble	B
p	O
(	O
x	O
;	O
y	O
)	O
in	O
which	O
p	O
(	O
x	O
)	O
has	O
(	O
p0	O
;	O
p1	O
)	O
=	O
(	O
0:9	O
;	O
0:1	O
)	O
and	O
p	O
(	O
y	O
j	O
x	O
)	O
corresponds	O
to	O
a	O
binary	B
symmetric	I
channel	I
with	O
noise	B
level	O
0:2.	O
x	O
1111111111000000000000000000000000000000000000000000000000000000000000000000000000000000000000000000	O
y	O
0011111111000000000000000000000000000000000000000000000000000000000000000000000000111111111111111111	O
notice	O
that	O
x	O
has	O
10	O
1s	O
,	O
and	O
so	O
is	O
typical	B
of	O
the	O
probability	B
p	O
(	O
x	O
)	O
(	O
at	O
any	O
tolerance	O
(	O
cid:12	O
)	O
)	O
;	O
and	O
y	O
has	O
26	O
1s	O
,	O
so	O
it	O
is	O
typical	B
of	O
p	O
(	O
y	O
)	O
(	O
because	O
p	O
(	O
y	O
=	O
1	O
)	O
=	O
0:26	O
)	O
;	O
and	O
x	O
and	O
y	O
di	O
(	O
cid:11	O
)	O
er	O
in	O
20	O
bits	O
,	O
which	O
is	O
the	O
typical	B
number	O
of	O
(	O
cid:13	O
)	O
ips	O
for	O
this	O
channel	B
.	O
joint	B
typicality	I
theorem	I
.	O
let	O
x	O
;	O
y	O
be	O
drawn	O
from	O
the	O
ensemble	B
(	O
xy	O
)	O
n	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
then	O
p	O
(	O
x	O
;	O
y	O
)	O
=	O
p	O
(	O
xn	O
;	O
yn	O
)	O
:	O
n	O
yn=1	O
1.	O
the	O
probability	B
that	O
x	O
;	O
y	O
are	O
jointly	O
typical	B
(	O
to	O
tolerance	O
(	O
cid:12	O
)	O
)	O
tends	O
to	O
1	O
as	O
n	O
!	O
1	O
;	O
to	O
be	O
precise	O
,	O
2.	O
the	O
number	O
of	O
jointly-typical	O
sequences	O
jjn	O
(	O
cid:12	O
)	O
j	O
is	O
close	O
to	O
2n	O
h	O
(	O
x	O
;	O
y	O
)	O
.	O
(	O
10.3	O
)	O
3.	O
if	O
x0	O
(	O
cid:24	O
)	O
x	O
n	O
and	O
y0	O
(	O
cid:24	O
)	O
y	O
n	O
,	O
i.e.	O
,	O
x0	O
and	O
y0	O
are	O
independent	O
samples	O
with	O
the	O
same	O
marginal	B
distribution	O
as	O
p	O
(	O
x	O
;	O
y	O
)	O
,	O
then	O
the	O
probability	B
that	O
(	O
x0	O
;	O
y0	O
)	O
lands	O
in	O
the	O
jointly-typical	O
set	B
is	O
about	O
2	O
(	O
cid:0	O
)	O
n	O
i	O
(	O
x	O
;	O
y	O
)	O
.	O
to	O
be	O
precise	O
,	O
jjn	O
(	O
cid:12	O
)	O
j	O
(	O
cid:20	O
)	O
2n	O
(	O
h	O
(	O
x	O
;	O
y	O
)	O
+	O
(	O
cid:12	O
)	O
)	O
;	O
p	O
(	O
(	O
x0	O
;	O
y0	O
)	O
2	O
jn	O
(	O
cid:12	O
)	O
)	O
(	O
cid:20	O
)	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:0	O
)	O
3	O
(	O
cid:12	O
)	O
)	O
:	O
(	O
10.4	O
)	O
proof	O
.	O
the	O
proof	O
of	O
parts	O
1	O
and	O
2	O
by	O
the	O
law	B
of	I
large	I
numbers	I
follows	O
that	O
of	O
the	O
source	O
coding	B
theorem	I
in	O
chapter	O
4.	O
for	O
part	O
2	O
,	O
let	O
the	O
pair	O
x	O
;	O
y	O
play	O
the	O
role	O
of	O
x	O
in	O
the	O
source	B
coding	I
theorem	I
,	O
replacing	O
p	O
(	O
x	O
)	O
there	O
by	O
the	O
probability	B
distribution	O
p	O
(	O
x	O
;	O
y	O
)	O
.	O
for	O
the	O
third	O
part	O
,	O
p	O
(	O
(	O
x0	O
;	O
y0	O
)	O
2	O
jn	O
(	O
cid:12	O
)	O
)	O
=	O
x	O
(	O
x	O
;	O
y	O
)	O
2jn	O
(	O
cid:12	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
(	O
cid:20	O
)	O
jjn	O
(	O
cid:12	O
)	O
j	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
h	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
)	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
)	O
(	O
cid:20	O
)	O
2n	O
(	O
h	O
(	O
x	O
;	O
y	O
)	O
+	O
(	O
cid:12	O
)	O
)	O
(	O
cid:0	O
)	O
n	O
(	O
h	O
(	O
x	O
)	O
+h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
2	O
(	O
cid:12	O
)	O
)	O
=	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:0	O
)	O
3	O
(	O
cid:12	O
)	O
)	O
:	O
(	O
10.5	O
)	O
(	O
10.6	O
)	O
(	O
10.7	O
)	O
2	O
(	O
10.8	O
)	O
a	O
cartoon	O
of	O
the	O
jointly-typical	O
set	B
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
10.2.	O
two	O
independent	O
typical	O
vectors	B
are	O
jointly	O
typical	B
with	O
probability	B
p	O
(	O
(	O
x0	O
;	O
y0	O
)	O
2	O
jn	O
(	O
cid:12	O
)	O
)	O
’	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
i	O
(	O
x	O
;	O
y	O
)	O
)	O
(	O
10.9	O
)	O
because	O
the	O
total	O
number	O
of	O
independent	O
typical	B
pairs	O
is	O
the	O
area	O
of	O
the	O
dashed	O
rectangle	O
,	O
2n	O
h	O
(	O
x	O
)	O
2n	O
h	O
(	O
y	O
)	O
,	O
and	O
the	O
number	O
of	O
jointly-typical	O
pairs	O
is	O
roughly	O
2n	O
h	O
(	O
x	O
;	O
y	O
)	O
,	O
so	O
the	O
probability	O
of	O
hitting	O
a	O
jointly-typical	O
pair	O
is	O
roughly	O
2n	O
h	O
(	O
x	O
;	O
y	O
)	O
=2n	O
h	O
(	O
x	O
)	O
+n	O
h	O
(	O
y	O
)	O
=	O
2	O
(	O
cid:0	O
)	O
n	O
i	O
(	O
x	O
;	O
y	O
)	O
:	O
(	O
10.10	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
164	O
10	O
|	O
the	O
noisy-channel	B
coding	I
theorem	I
-	O
y	O
,	O
the	O
set	B
of	O
x	O
,	O
the	O
set	B
of	O
all	O
input	O
figure	O
10.2.	O
the	O
jointly-typical	O
set	B
.	O
the	O
horizontal	O
direction	O
represents	O
an	O
strings	O
of	O
length	O
n	O
.	O
the	O
vertical	O
direction	O
represents	O
an	O
all	O
output	O
strings	O
of	O
length	O
n	O
.	O
the	O
outer	O
box	O
contains	O
all	O
conceivable	O
input	O
{	O
output	O
pairs	O
.	O
each	O
dot	O
represents	O
a	O
jointly-typical	O
pair	O
of	O
sequences	O
(	O
x	O
;	O
y	O
)	O
.	O
the	O
total	O
number	O
of	O
jointly-typical	O
sequences	O
is	O
about	O
2n	O
h	O
(	O
x	O
;	O
y	O
)	O
.	O
an	O
x	O
(	O
cid:27	O
)	O
6	O
-	O
2n	O
h	O
(	O
x	O
)	O
2n	O
h	O
(	O
x	O
;	O
y	O
)	O
dots	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
6	O
2n	O
h	O
(	O
y	O
jx	O
)	O
?	O
2n	O
h	O
(	O
xjy	O
)	O
2n	O
h	O
(	O
y	O
)	O
6	O
?	O
-	O
(	O
cid:27	O
)	O
-	O
(	O
cid:27	O
)	O
an	O
y	O
(	O
cid:27	O
)	O
6	O
?	O
?	O
10.3	O
proof	O
of	O
the	O
noisy-channel	O
coding	B
theorem	I
analogy	O
imagine	O
that	O
we	O
wish	O
to	O
prove	O
that	O
there	O
is	O
a	O
baby	O
in	O
a	O
class	O
of	O
one	O
hundred	O
babies	O
who	O
weighs	O
less	O
than	O
10	O
kg	O
.	O
individual	O
babies	O
are	O
di	O
(	O
cid:14	O
)	O
cult	O
to	O
catch	O
and	O
weigh	O
.	O
shannon	O
’	O
s	O
method	B
of	O
solving	O
the	O
task	O
is	O
to	O
scoop	O
up	O
all	O
the	O
babies	O
and	O
weigh	O
them	O
all	O
at	O
once	O
on	O
a	O
big	O
weighing	O
machine	O
.	O
if	O
we	O
(	O
cid:12	O
)	O
nd	O
that	O
their	O
average	B
weight	O
is	O
smaller	O
than	O
10	O
kg	O
,	O
there	O
must	O
exist	O
at	O
least	O
one	O
baby	O
who	O
weighs	O
less	O
than	O
10	O
kg	O
{	O
indeed	O
there	O
must	O
be	O
many	O
!	O
shannon	O
’	O
s	O
method	B
isn	O
’	O
t	O
guaranteed	O
to	O
reveal	O
the	O
existence	O
of	O
an	O
underweight	O
child	O
,	O
since	O
it	O
relies	O
on	O
there	O
being	O
a	O
tiny	O
number	O
of	O
elephants	O
in	O
the	O
class	O
.	O
but	O
if	O
we	O
use	O
his	O
method	B
and	O
get	O
a	O
total	O
weight	B
smaller	O
than	O
1000	O
kg	O
then	O
our	O
task	O
is	O
solved	O
.	O
from	O
skinny	O
children	O
to	O
fantastic	O
codes	O
we	O
wish	O
to	O
show	O
that	O
there	O
exists	O
a	O
code	B
and	O
a	O
decoder	B
having	O
small	O
prob-	O
ability	O
of	O
error	O
.	O
evaluating	O
the	O
probability	B
of	I
error	I
of	O
any	O
particular	O
coding	O
and	O
decoding	B
system	O
is	O
not	O
easy	O
.	O
shannon	O
’	O
s	O
innovation	O
was	O
this	O
:	O
instead	O
of	O
constructing	O
a	O
good	B
coding	O
and	O
decoding	O
system	O
and	O
evaluating	O
its	O
error	O
prob-	O
ability	O
,	O
shannon	O
calculated	O
the	O
average	B
probability	O
of	O
block	O
error	O
of	O
all	O
codes	O
,	O
and	O
proved	O
that	O
this	O
average	B
is	O
small	O
.	O
there	O
must	O
then	O
exist	O
individual	O
codes	O
that	O
have	O
small	O
probability	B
of	I
block	I
error	I
.	O
random	B
coding	O
and	O
typical-set	O
decoding	B
consider	O
the	O
following	O
encoding	O
{	O
decoding	B
system	O
,	O
whose	O
rate	B
is	O
r0	O
.	O
1.	O
we	O
(	O
cid:12	O
)	O
x	O
p	O
(	O
x	O
)	O
and	O
generate	O
the	O
s	O
=	O
2n	O
r0	O
codewords	O
of	O
a	O
(	O
n	O
;	O
n	O
r0	O
)	O
=	O
figure	O
10.3.	O
shannon	O
’	O
s	O
method	B
for	O
proving	O
one	O
baby	O
weighs	O
less	O
than	O
10	O
kg	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
10.3	O
:	O
proof	O
of	O
the	O
noisy-channel	O
coding	B
theorem	I
165	O
x	O
(	O
2	O
)	O
x	O
(	O
4	O
)	O
x	O
(	O
3	O
)	O
x	O
(	O
1	O
)	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
ya	O
yb	O
yd	O
yc	O
x	O
(	O
2	O
)	O
x	O
(	O
4	O
)	O
x	O
(	O
3	O
)	O
x	O
(	O
1	O
)	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
qqqqqqqqqqqqqqqqqqqqq	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
n	O
;	O
k	O
)	O
code	B
c	O
at	O
random	B
according	O
to	O
yn=1	O
p	O
(	O
x	O
)	O
=	O
n	O
p	O
(	O
xn	O
)	O
:	O
figure	O
10.4	O
.	O
(	O
a	O
)	O
a	O
random	B
code	I
.	O
(	O
b	O
)	O
example	O
decodings	O
by	O
the	O
typical	B
set	I
decoder	O
.	O
a	O
sequence	B
that	O
is	O
not	O
jointly	O
typical	B
with	O
any	O
of	O
the	O
codewords	O
,	O
such	O
as	O
ya	O
,	O
is	O
decoded	O
as	O
^s	O
=	O
0.	O
a	O
sequence	B
that	O
is	O
jointly	O
typical	B
with	O
codeword	B
x	O
(	O
3	O
)	O
alone	O
,	O
yb	O
,	O
is	O
decoded	O
as	O
^s	O
=	O
3.	O
similarly	O
,	O
yc	O
is	O
decoded	O
as	O
^s	O
=	O
4.	O
a	O
sequence	B
that	O
is	O
jointly	O
typical	B
with	O
more	O
than	O
one	O
codeword	B
,	O
such	O
as	O
yd	O
,	O
is	O
decoded	O
as	O
^s	O
=	O
0	O
.	O
-	O
-	O
^s	O
(	O
ya	O
)	O
=	O
0	O
^s	O
(	O
yb	O
)	O
=	O
3	O
-	O
-	O
^s	O
(	O
yd	O
)	O
=	O
0	O
^s	O
(	O
yc	O
)	O
=	O
4	O
(	O
10.11	O
)	O
a	O
random	B
code	I
is	O
shown	O
schematically	O
in	O
(	O
cid:12	O
)	O
gure	O
10.4a	O
.	O
2.	O
the	O
code	B
is	O
known	O
to	O
both	O
sender	O
and	O
receiver	O
.	O
3.	O
a	O
message	O
s	O
is	O
chosen	O
from	O
f1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
2n	O
r0g	O
,	O
and	O
x	O
(	O
s	O
)	O
is	O
transmitted	O
.	O
the	O
received	O
signal	O
is	O
y	O
,	O
with	O
p	O
(	O
y	O
j	O
x	O
(	O
s	O
)	O
)	O
=	O
n	O
yn=1	O
p	O
(	O
yn	O
j	O
x	O
(	O
s	O
)	O
n	O
)	O
:	O
(	O
10.12	O
)	O
4.	O
the	O
signal	O
is	O
decoded	O
by	O
typical-set	O
decoding	B
.	O
typical-set	O
decoding	O
.	O
decode	O
y	O
as	O
^s	O
if	O
(	O
x	O
(	O
^s	O
)	O
;	O
y	O
)	O
are	O
jointly	O
typical	B
and	O
there	O
is	O
no	O
other	O
s0	O
such	O
that	O
(	O
x	O
(	O
s0	O
)	O
;	O
y	O
)	O
are	O
jointly	O
typical	B
;	O
otherwise	O
declare	O
a	O
failure	O
(	O
^s	O
=	O
0	O
)	O
.	O
this	O
is	O
not	O
the	O
optimal	B
decoding	O
algorithm	B
,	O
but	O
it	O
will	O
be	O
good	B
enough	O
,	O
and	O
easier	O
to	O
analyze	O
.	O
the	O
typical-set	B
decoder	I
is	O
illustrated	O
in	O
(	O
cid:12	O
)	O
g-	O
ure	O
10.4b	O
.	O
5.	O
a	O
decoding	B
error	O
occurs	O
if	O
^s	O
6=	O
s.	O
there	O
are	O
three	O
probabilities	O
of	O
error	O
that	O
we	O
can	O
distinguish	O
.	O
first	O
,	O
there	O
is	O
the	O
probability	B
of	I
block	I
error	I
for	O
a	O
particular	O
code	B
c	O
,	O
that	O
is	O
,	O
pb	O
(	O
c	O
)	O
(	O
cid:17	O
)	O
p	O
(	O
^s	O
6=	O
sjc	O
)	O
:	O
(	O
10.13	O
)	O
this	O
is	O
a	O
di	O
(	O
cid:14	O
)	O
cult	O
quantity	O
to	O
evaluate	O
for	O
any	O
given	O
code	B
.	O
second	O
,	O
there	O
is	O
the	O
average	B
over	O
all	O
codes	O
of	O
this	O
block	B
error	O
probability	B
,	O
hpbi	O
(	O
cid:17	O
)	O
xc	O
p	O
(	O
^s	O
6=	O
sjc	O
)	O
p	O
(	O
c	O
)	O
:	O
(	O
10.14	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
166	O
10	O
|	O
the	O
noisy-channel	B
coding	I
theorem	I
hpbi	O
is	O
just	O
the	O
probability	B
that	O
there	O
is	O
a	O
decoding	B
error	O
at	O
step	O
5	O
of	O
the	O
(	O
cid:12	O
)	O
ve-step	O
process	O
on	O
the	O
previous	O
page	O
.	O
fortunately	O
,	O
this	O
quantity	O
is	O
much	O
easier	O
to	O
evaluate	O
than	O
the	O
(	O
cid:12	O
)	O
rst	O
quantity	O
p	O
(	O
^s	O
6=	O
sjc	O
)	O
.	O
third	O
,	O
the	O
maximal	O
block	B
error	O
probability	O
of	O
a	O
code	B
c	O
,	O
pbm	O
(	O
c	O
)	O
(	O
cid:17	O
)	O
max	O
s	O
p	O
(	O
^s	O
6=	O
sj	O
s	O
;	O
c	O
)	O
;	O
(	O
10.15	O
)	O
is	O
the	O
quantity	O
we	O
are	O
most	O
interested	O
in	O
:	O
we	O
wish	O
to	O
show	O
that	O
there	O
exists	O
a	O
code	B
c	O
with	O
the	O
required	O
rate	B
whose	O
maximal	O
block	B
error	O
probability	B
is	O
small	O
.	O
we	O
will	O
get	O
to	O
this	O
result	O
by	O
(	O
cid:12	O
)	O
rst	O
(	O
cid:12	O
)	O
nding	O
the	O
average	B
block	O
error	B
probability	I
,	O
hpbi	O
.	O
once	O
we	O
have	O
shown	O
that	O
this	O
can	O
be	O
made	O
smaller	O
than	O
a	O
desired	O
small	O
number	O
,	O
we	O
immediately	O
deduce	O
that	O
there	O
must	O
exist	O
at	O
least	O
one	O
code	B
c	O
whose	O
block	B
error	O
probability	B
is	O
also	O
less	O
than	O
this	O
small	O
number	O
.	O
finally	O
,	O
we	O
show	O
that	O
this	O
code	B
,	O
whose	O
block	B
error	O
probability	B
is	O
satisfactorily	O
small	O
but	O
whose	O
maximal	O
block	B
error	O
probability	B
is	O
unknown	O
(	O
and	O
could	O
conceivably	O
be	O
enormous	O
)	O
,	O
can	O
be	O
modi	O
(	O
cid:12	O
)	O
ed	O
to	O
make	O
a	O
code	B
of	O
slightly	O
smaller	O
rate	B
whose	O
maximal	O
block	B
error	O
probability	B
is	O
also	O
guaranteed	O
to	O
be	O
small	O
.	O
we	O
modify	O
the	O
code	B
by	O
throwing	O
away	O
the	O
worst	O
50	O
%	O
of	O
its	O
codewords	O
.	O
we	O
therefore	O
now	O
embark	O
on	O
(	O
cid:12	O
)	O
nding	O
the	O
average	B
probability	O
of	O
block	O
error	O
.	O
probability	B
of	I
error	I
of	O
typical-set	B
decoder	I
there	O
are	O
two	O
sources	O
of	O
error	O
when	O
we	O
use	O
typical-set	O
decoding	O
.	O
either	O
(	O
a	O
)	O
the	O
output	O
y	O
is	O
not	O
jointly	O
typical	B
with	O
the	O
transmitted	O
codeword	B
x	O
(	O
s	O
)	O
,	O
or	O
(	O
b	O
)	O
there	O
is	O
some	O
other	O
codeword	B
in	O
c	O
that	O
is	O
jointly	O
typical	B
with	O
y.	O
by	O
the	O
symmetry	O
of	O
the	O
code	B
construction	O
,	O
the	O
average	B
probability	O
of	O
error	O
averaged	O
over	O
all	O
codes	O
does	O
not	O
depend	O
on	O
the	O
selected	O
value	O
of	O
s	O
;	O
we	O
can	O
assume	O
without	O
loss	O
of	O
generality	O
that	O
s	O
=	O
1	O
.	O
(	O
a	O
)	O
the	O
probability	B
that	O
the	O
input	O
x	O
(	O
1	O
)	O
and	O
the	O
output	O
y	O
are	O
not	O
jointly	O
typical	B
vanishes	O
,	O
by	O
the	O
joint	B
typicality	I
theorem	I
’	O
s	O
(	O
cid:12	O
)	O
rst	O
part	O
(	O
p.163	O
)	O
.	O
we	O
give	O
a	O
name	O
,	O
(	O
cid:14	O
)	O
,	O
to	O
the	O
upper	O
bound	B
on	O
this	O
probability	B
,	O
satisfying	O
(	O
cid:14	O
)	O
!	O
0	O
as	O
n	O
!	O
1	O
;	O
for	O
any	O
desired	O
(	O
cid:14	O
)	O
,	O
we	O
can	O
(	O
cid:12	O
)	O
nd	O
a	O
blocklength	O
n	O
(	O
(	O
cid:14	O
)	O
)	O
such	O
that	O
the	O
p	O
(	O
(	O
x	O
(	O
1	O
)	O
;	O
y	O
)	O
62	O
jn	O
(	O
cid:12	O
)	O
)	O
(	O
cid:20	O
)	O
(	O
cid:14	O
)	O
.	O
(	O
b	O
)	O
the	O
probability	B
that	O
x	O
(	O
s0	O
)	O
and	O
y	O
are	O
jointly	O
typical	B
,	O
for	O
a	O
given	O
s0	O
6=	O
1	O
is	O
(	O
cid:20	O
)	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:0	O
)	O
3	O
(	O
cid:12	O
)	O
)	O
,	O
by	O
part	O
3.	O
and	O
there	O
are	O
(	O
2n	O
r0	O
(	O
cid:0	O
)	O
1	O
)	O
rival	O
values	O
of	O
s0	O
to	O
worry	O
about	O
.	O
thus	O
the	O
average	B
probability	O
of	O
error	O
hpbi	O
satis	O
(	O
cid:12	O
)	O
es	O
:	O
hpbi	O
(	O
cid:20	O
)	O
(	O
cid:14	O
)	O
+	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:0	O
)	O
3	O
(	O
cid:12	O
)	O
)	O
2n	O
r0	O
xs0=2	O
(	O
cid:20	O
)	O
(	O
cid:14	O
)	O
+	O
2	O
(	O
cid:0	O
)	O
n	O
(	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:0	O
)	O
r0	O
(	O
cid:0	O
)	O
3	O
(	O
cid:12	O
)	O
)	O
:	O
(	O
10.16	O
)	O
(	O
10.17	O
)	O
the	O
inequality	B
(	O
10.16	O
)	O
that	O
bounds	O
a	O
total	O
probability	B
of	I
error	I
ptot	O
by	O
the	O
sum	O
of	O
the	O
probabilities	O
ps0	O
of	O
all	O
sorts	O
of	O
events	O
s0	O
each	O
of	O
which	O
is	O
su	O
(	O
cid:14	O
)	O
cient	O
to	O
cause	O
error	O
,	O
is	O
called	O
a	O
union	B
bound	I
.	O
it	O
is	O
only	O
an	O
equality	O
if	O
the	O
di	O
(	O
cid:11	O
)	O
erent	O
events	O
that	O
cause	O
error	O
never	O
occur	O
at	O
the	O
same	O
time	O
as	O
each	O
other	O
.	O
ptot	O
(	O
cid:20	O
)	O
p1	O
+	O
p2	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
;	O
the	O
average	B
probability	O
of	O
error	O
(	O
10.17	O
)	O
can	O
be	O
made	O
<	O
2	O
(	O
cid:14	O
)	O
by	O
increasing	O
n	O
if	O
r0	O
<	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:0	O
)	O
3	O
(	O
cid:12	O
)	O
:	O
(	O
10.18	O
)	O
we	O
are	O
almost	O
there	O
.	O
we	O
make	O
three	O
modi	O
(	O
cid:12	O
)	O
cations	O
:	O
1.	O
we	O
choose	O
p	O
(	O
x	O
)	O
in	O
the	O
proof	O
to	O
be	O
the	O
optimal	B
input	I
distribution	I
of	O
the	O
channel	B
.	O
then	O
the	O
condition	O
r0	O
<	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:0	O
)	O
3	O
(	O
cid:12	O
)	O
becomes	O
r0	O
<	O
c	O
(	O
cid:0	O
)	O
3	O
(	O
cid:12	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
10.4	O
:	O
communication	B
(	O
with	O
errors	O
)	O
above	O
capacity	B
167	O
figure	O
10.5.	O
how	O
expurgation	O
works	O
.	O
(	O
a	O
)	O
in	O
a	O
typical	B
random	O
code	B
,	O
a	O
small	O
fraction	O
of	O
the	O
codewords	O
are	O
involved	O
in	O
collisions	O
{	O
pairs	O
of	O
codewords	O
are	O
su	O
(	O
cid:14	O
)	O
ciently	O
close	O
to	O
each	O
other	O
that	O
the	O
probability	B
of	I
error	I
when	O
either	O
codeword	B
is	O
transmitted	O
is	O
not	O
tiny	O
.	O
we	O
obtain	O
a	O
new	O
code	B
from	O
a	O
random	B
code	I
by	O
deleting	O
all	O
these	O
confusable	O
codewords	O
.	O
(	O
b	O
)	O
the	O
resulting	O
code	B
has	O
slightly	O
fewer	O
codewords	O
,	O
so	O
has	O
a	O
slightly	O
lower	O
rate	B
,	O
and	O
its	O
maximal	O
probability	B
of	I
error	I
is	O
greatly	O
reduced	O
.	O
6	O
pb	O
achievable	O
-	O
r	O
c	O
figure	O
10.6.	O
portion	O
of	O
the	O
r	O
;	O
pb	O
plane	O
proved	O
achievable	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
part	O
of	O
the	O
theorem	O
.	O
[	O
we	O
’	O
ve	O
proved	O
that	O
the	O
maximal	O
probability	B
of	I
block	I
error	I
pbm	O
can	O
be	O
made	O
arbitrarily	O
small	O
,	O
so	O
the	O
same	O
goes	O
for	O
the	O
bit	B
error	O
probability	B
pb	O
,	O
which	O
must	O
be	O
smaller	O
than	O
pbm	O
.	O
]	O
(	O
a	O
)	O
a	O
random	B
code	I
:	O
:	O
:	O
)	O
(	O
b	O
)	O
expurgated	O
2.	O
since	O
the	O
average	B
probability	O
of	O
error	O
over	O
all	O
codes	O
is	O
<	O
2	O
(	O
cid:14	O
)	O
,	O
there	O
must	O
exist	O
a	O
code	B
with	O
mean	B
probability	O
of	O
block	O
error	O
pb	O
(	O
c	O
)	O
<	O
2	O
(	O
cid:14	O
)	O
.	O
3.	O
to	O
show	O
that	O
not	O
only	O
the	O
average	B
but	O
also	O
the	O
maximal	O
probability	B
of	I
error	I
,	O
pbm	O
,	O
can	O
be	O
made	O
small	O
,	O
we	O
modify	O
this	O
code	B
by	O
throwing	O
away	O
the	O
worst	O
half	O
of	O
the	O
codewords	O
{	O
the	O
ones	O
most	O
likely	O
to	O
produce	O
errors	B
.	O
those	O
that	O
remain	O
must	O
all	O
have	O
conditional	B
probability	O
of	O
error	O
less	O
than	O
4	O
(	O
cid:14	O
)	O
.	O
we	O
use	O
these	O
remaining	O
codewords	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
new	O
code	B
.	O
this	O
new	O
code	B
has	O
2n	O
r0	O
(	O
cid:0	O
)	O
1	O
codewords	O
,	O
i.e.	O
,	O
we	O
have	O
reduced	O
the	O
rate	B
from	O
r0	O
to	O
r0	O
(	O
cid:0	O
)	O
1/n	O
(	O
a	O
negligible	O
reduction	O
,	O
if	O
n	O
is	O
large	O
)	O
,	O
and	O
achieved	O
pbm	O
<	O
4	O
(	O
cid:14	O
)	O
.	O
this	O
trick	O
is	O
called	O
expurgation	B
(	O
(	O
cid:12	O
)	O
gure	O
10.5	O
)	O
.	O
the	O
resulting	O
code	B
may	O
not	O
be	O
the	O
best	O
code	B
of	O
its	O
rate	B
and	O
length	B
,	O
but	O
it	O
is	O
still	O
good	B
enough	O
to	O
prove	O
the	O
noisy-channel	B
coding	I
theorem	I
,	O
which	O
is	O
what	O
we	O
are	O
trying	O
to	O
do	O
here	O
.	O
in	O
conclusion	O
,	O
we	O
can	O
‘	O
construct	O
’	O
a	O
code	B
of	O
rate	B
r0	O
(	O
cid:0	O
)	O
1/n	O
,	O
where	O
r0	O
<	O
c	O
(	O
cid:0	O
)	O
3	O
(	O
cid:12	O
)	O
,	O
with	O
maximal	O
probability	B
of	I
error	I
<	O
4	O
(	O
cid:14	O
)	O
.	O
we	O
obtain	O
the	O
theorem	B
as	O
stated	O
by	O
setting	O
r0	O
=	O
(	O
r	O
+	O
c	O
)	O
=2	O
,	O
(	O
cid:14	O
)	O
=	O
(	O
cid:15	O
)	O
=4	O
,	O
(	O
cid:12	O
)	O
<	O
(	O
c	O
(	O
cid:0	O
)	O
r0	O
)	O
=3	O
,	O
and	O
n	O
su	O
(	O
cid:14	O
)	O
ciently	O
large	O
for	O
the	O
remaining	O
conditions	O
to	O
hold	O
.	O
the	O
theorem	B
’	O
s	O
(	O
cid:12	O
)	O
rst	O
part	O
is	O
thus	O
proved	O
.	O
2	O
10.4	O
communication	B
(	O
with	O
errors	O
)	O
above	O
capacity	B
we	O
have	O
proved	O
,	O
for	O
any	O
discrete	B
memoryless	I
channel	O
,	O
the	O
achievability	O
of	O
a	O
portion	O
of	O
the	O
r	O
;	O
pb	O
plane	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
10.6.	O
we	O
have	O
shown	O
that	O
we	O
can	O
turn	O
any	O
noisy	B
channel	I
into	O
an	O
essentially	O
noiseless	B
binary	O
channel	O
with	O
rate	O
up	O
to	O
c	O
bits	O
per	O
cycle	O
.	O
we	O
now	O
extend	O
the	O
right-hand	O
boundary	O
of	O
the	O
region	O
of	O
achievability	O
at	O
non-zero	O
error	O
probabilities	O
.	O
[	O
this	O
is	O
called	O
rate-distortion	B
theory	I
.	O
]	O
we	O
do	O
this	O
with	O
a	O
new	O
trick	O
.	O
since	O
we	O
know	O
we	O
can	O
make	O
the	O
noisy	B
channel	I
into	O
a	O
perfect	B
channel	O
with	O
a	O
smaller	O
rate	B
,	O
it	O
is	O
su	O
(	O
cid:14	O
)	O
cient	O
to	O
consider	O
commu-	O
nication	O
with	O
errors	O
over	O
a	O
noiseless	B
channel	O
.	O
how	O
fast	O
can	O
we	O
communicate	O
over	O
a	O
noiseless	B
channel	O
,	O
if	O
we	O
are	O
allowed	O
to	O
make	O
errors	B
?	O
consider	O
a	O
noiseless	B
binary	O
channel	B
,	O
and	O
assume	O
that	O
we	O
force	O
communi-	O
cation	O
at	O
a	O
rate	B
greater	O
than	O
its	O
capacity	B
of	O
1	O
bit	B
.	O
for	O
example	O
,	O
if	O
we	O
require	O
the	O
sender	O
to	O
attempt	O
to	O
communicate	O
at	O
r	O
=	O
2	O
bits	O
per	O
cycle	O
then	O
he	O
must	O
e	O
(	O
cid:11	O
)	O
ectively	O
throw	O
away	O
half	O
of	O
the	O
information	O
.	O
what	O
is	O
the	O
best	O
way	O
to	O
do	O
this	O
if	O
the	O
aim	O
is	O
to	O
achieve	O
the	O
smallest	O
possible	O
probability	O
of	O
bit	O
error	O
?	O
one	O
simple	O
strategy	O
is	O
to	O
communicate	O
a	O
fraction	O
1=r	O
of	O
the	O
source	O
bits	O
,	O
and	O
ignore	O
the	O
rest	O
.	O
the	O
receiver	O
guesses	O
the	O
missing	O
fraction	O
1	O
(	O
cid:0	O
)	O
1=r	O
at	O
random	B
,	O
and	O
 	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
168	O
10	O
|	O
the	O
noisy-channel	B
coding	I
theorem	I
optimum	O
simple	O
0.3	O
0.25	O
0.2	O
pb	O
0.15	O
0.1	O
0.05	O
0	O
0	O
0.5	O
1	O
1.5	O
2	O
2.5	O
r	O
figure	O
10.7.	O
a	O
simple	O
bound	O
on	O
achievable	O
points	O
(	O
r	O
;	O
pb	O
)	O
,	O
and	O
shannon	O
’	O
s	O
bound	B
.	O
the	O
average	B
probability	O
of	O
bit	O
error	O
is	O
pb	O
=	O
1	O
2	O
(	O
1	O
(	O
cid:0	O
)	O
1=r	O
)	O
:	O
(	O
10.19	O
)	O
the	O
curve	O
corresponding	O
to	O
this	O
strategy	O
is	O
shown	O
by	O
the	O
dashed	O
line	O
in	O
(	O
cid:12	O
)	O
g-	O
ure	O
10.7.	O
the	O
risk	O
of	O
corruption	O
evenly	O
among	O
all	O
the	O
bits	O
.	O
pb	O
=	O
h	O
(	O
cid:0	O
)	O
1	O
can	O
this	O
optimum	O
be	O
achieved	O
?	O
we	O
can	O
do	O
better	O
than	O
this	O
(	O
in	O
terms	O
of	O
minimizing	O
pb	O
)	O
by	O
spreading	O
out	O
in	O
fact	O
,	O
we	O
can	O
achieve	O
2	O
(	O
1	O
(	O
cid:0	O
)	O
1=r	O
)	O
,	O
which	O
is	O
shown	O
by	O
the	O
solid	O
curve	O
in	O
(	O
cid:12	O
)	O
gure	O
10.7.	O
so	O
,	O
how	O
we	O
reuse	O
a	O
tool	O
that	O
we	O
just	O
developed	O
,	O
namely	O
the	O
(	O
n	O
;	O
k	O
)	O
code	B
for	O
a	O
noisy	B
channel	I
,	O
and	O
we	O
turn	O
it	O
on	O
its	O
head	O
,	O
using	O
the	O
decoder	B
to	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
lossy	B
compressor	O
.	O
speci	O
(	O
cid:12	O
)	O
cally	O
,	O
we	O
take	O
an	O
excellent	O
(	O
n	O
;	O
k	O
)	O
code	B
for	O
the	O
binary	B
symmetric	I
channel	I
.	O
assume	O
that	O
such	O
a	O
code	B
has	O
a	O
rate	B
r0	O
=	O
k=n	O
,	O
and	O
that	O
it	O
is	O
capable	O
of	O
correcting	O
errors	B
introduced	O
by	O
a	O
binary	B
symmetric	I
channel	I
whose	O
transition	B
probability	I
is	O
q.	O
asymptotically	O
,	O
rate-r0	O
codes	O
exist	O
that	O
have	O
r0	O
’	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
q	O
)	O
.	O
recall	O
that	O
,	O
if	O
we	O
attach	O
one	O
of	O
these	O
capacity-achieving	O
codes	O
of	O
length	O
n	O
to	O
a	O
binary	B
symmetric	I
channel	I
then	O
(	O
a	O
)	O
the	O
probability	B
distribution	O
over	O
the	O
outputs	O
is	O
close	O
to	O
uniform	O
,	O
since	O
the	O
entropy	B
of	O
the	O
output	O
is	O
equal	O
to	O
the	O
entropy	B
of	O
the	O
source	O
(	O
n	O
r0	O
)	O
plus	O
the	O
entropy	B
of	O
the	O
noise	B
(	O
n	O
h2	O
(	O
q	O
)	O
)	O
,	O
and	O
(	O
b	O
)	O
the	O
optimal	B
decoder	I
of	O
the	O
code	B
,	O
in	O
this	O
situation	O
,	O
typically	O
maps	O
a	O
received	O
vector	O
of	O
length	B
n	O
to	O
a	O
transmitted	O
vector	O
di	O
(	O
cid:11	O
)	O
ering	O
in	O
qn	O
bits	O
from	O
the	O
received	O
vector	O
.	O
we	O
take	O
the	O
signal	O
that	O
we	O
wish	O
to	O
send	O
,	O
and	O
chop	O
it	O
into	O
blocks	O
of	O
length	O
n	O
(	O
yes	O
,	O
n	O
,	O
not	O
k	O
)	O
.	O
we	O
pass	O
each	O
block	B
through	O
the	O
decoder	B
,	O
and	O
obtain	O
a	O
shorter	O
signal	O
of	O
length	O
k	O
bits	O
,	O
which	O
we	O
communicate	O
over	O
the	O
noiseless	B
channel	O
.	O
to	O
decode	O
the	O
transmission	O
,	O
we	O
pass	O
the	O
k	O
bit	B
message	O
to	O
the	O
encoder	B
of	O
the	O
original	O
code	B
.	O
the	O
reconstituted	O
message	O
will	O
now	O
di	O
(	O
cid:11	O
)	O
er	O
from	O
the	O
original	O
message	O
in	O
some	O
of	O
its	O
bits	O
{	O
typically	O
qn	O
of	O
them	O
.	O
so	O
the	O
probability	O
of	O
bit	O
error	O
will	O
be	O
pb	O
=	O
q.	O
the	O
rate	B
of	O
this	O
lossy	B
compressor	O
is	O
r	O
=	O
n=k	O
=	O
1=r0	O
=	O
1=	O
(	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
pb	O
)	O
)	O
.	O
now	O
,	O
attaching	O
this	O
lossy	B
compressor	O
to	O
our	O
capacity-c	O
error-free	O
commu-	O
nicator	O
,	O
we	O
have	O
proved	O
the	O
achievability	O
of	O
communication	O
up	O
to	O
the	O
curve	O
(	O
pb	O
;	O
r	O
)	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
:	O
r	O
=	O
:	O
2	O
(	O
10.20	O
)	O
c	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
pb	O
)	O
for	O
further	O
reading	O
about	O
rate-distortion	B
theory	I
,	O
see	O
gallager	O
(	O
1968	O
)	O
,	O
p.	O
451	O
,	O
or	O
mceliece	O
(	O
2002	O
)	O
,	O
p.	O
75	O
.	O
10.5	O
the	O
non-achievable	O
region	O
(	O
part	O
3	O
of	O
the	O
theorem	O
)	O
the	O
source	O
,	O
encoder	B
,	O
noisy	B
channel	I
and	O
decoder	B
de	O
(	O
cid:12	O
)	O
ne	O
a	O
markov	O
chain	B
:	O
s	O
!	O
x	O
!	O
y	O
!	O
^s	O
p	O
(	O
s	O
;	O
x	O
;	O
y	O
;	O
^s	O
)	O
=	O
p	O
(	O
s	O
)	O
p	O
(	O
xj	O
s	O
)	O
p	O
(	O
y	O
j	O
x	O
)	O
p	O
(	O
^s	O
j	O
y	O
)	O
:	O
(	O
10.21	O
)	O
the	O
data	O
processing	O
inequality	B
(	O
exercise	O
8.9	O
,	O
p.141	O
)	O
must	O
apply	O
to	O
this	O
chain	B
:	O
i	O
(	O
s	O
;	O
^s	O
)	O
(	O
cid:20	O
)	O
i	O
(	O
x	O
;	O
y	O
)	O
:	O
furthermore	O
,	O
by	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
channel	O
capacity	B
,	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:20	O
)	O
n	O
c	O
,	O
so	O
i	O
(	O
s	O
;	O
^s	O
)	O
(	O
cid:20	O
)	O
n	O
c.	O
assume	O
that	O
a	O
system	O
achieves	O
a	O
rate	B
r	O
and	O
a	O
bit	B
error	O
probability	B
pb	O
;	O
then	O
the	O
mutual	B
information	I
i	O
(	O
s	O
;	O
^s	O
)	O
is	O
(	O
cid:21	O
)	O
n	O
r	O
(	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
pb	O
)	O
)	O
.	O
but	O
i	O
(	O
s	O
;	O
^s	O
)	O
>	O
n	O
c	O
is	O
not	O
achievable	O
,	O
so	O
r	O
>	O
is	O
not	O
achievable	O
.	O
2	O
c	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
pb	O
)	O
exercise	O
10.1	O
.	O
[	O
3	O
]	O
fill	O
in	O
the	O
details	O
in	O
the	O
preceding	O
argument	O
.	O
if	O
the	O
bit	B
errors	O
between	O
^s	O
and	O
s	O
are	O
independent	O
then	O
we	O
have	O
i	O
(	O
s	O
;	O
^s	O
)	O
=	O
n	O
r	O
(	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
pb	O
)	O
)	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
10.6	O
:	O
computing	O
capacity	B
169	O
sections	O
10.6	O
{	O
10.8	O
contain	O
advanced	O
material	O
.	O
the	O
(	O
cid:12	O
)	O
rst-time	O
reader	O
is	O
encouraged	O
to	O
skip	O
to	O
section	B
10.9	O
(	O
p.172	O
)	O
.	O
what	O
if	O
we	O
have	O
complex	B
correlations	O
among	O
those	O
bit	B
errors	O
?	O
why	O
does	O
the	O
inequality	B
i	O
(	O
s	O
;	O
^s	O
)	O
(	O
cid:21	O
)	O
n	O
r	O
(	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
pb	O
)	O
)	O
hold	O
?	O
10.6	O
computing	O
capacity	B
we	O
have	O
proved	O
that	O
the	O
capacity	B
of	O
a	O
channel	B
is	O
the	O
maximum	O
rate	O
at	O
which	O
reliable	O
communication	B
can	O
be	O
achieved	O
.	O
how	O
can	O
we	O
compute	O
the	O
capacity	B
of	O
a	O
given	O
discrete	B
memoryless	I
channel	O
?	O
we	O
need	O
to	O
(	O
cid:12	O
)	O
nd	O
its	O
optimal	O
input	O
distri-	O
bution	O
.	O
in	O
general	O
we	O
can	O
(	O
cid:12	O
)	O
nd	O
the	O
optimal	B
input	I
distribution	I
by	O
a	O
computer	B
search	O
,	O
making	O
use	O
of	O
the	O
derivative	O
of	O
the	O
mutual	O
information	B
with	O
respect	O
to	O
the	O
input	O
probabilities	O
.	O
.	O
exercise	O
10.2	O
.	O
[	O
2	O
]	O
find	O
the	O
derivative	O
of	O
i	O
(	O
x	O
;	O
y	O
)	O
with	O
respect	O
to	O
the	O
input	O
prob-	O
ability	O
pi	O
,	O
@	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
@	O
pi	O
,	O
for	O
a	O
channel	O
with	O
conditional	O
probabilities	O
qjji	O
.	O
exercise	O
10.3	O
.	O
[	O
2	O
]	O
show	O
that	O
i	O
(	O
x	O
;	O
y	O
)	O
is	O
a	O
concave	B
_	I
function	O
of	O
the	O
input	O
prob-	O
ability	O
vector	O
p.	O
since	O
i	O
(	O
x	O
;	O
y	O
)	O
is	O
concave	B
_	I
in	O
the	O
input	O
distribution	O
p	O
,	O
any	O
probability	B
distri-	O
bution	O
p	O
at	O
which	O
i	O
(	O
x	O
;	O
y	O
)	O
is	O
stationary	O
must	O
be	O
a	O
global	O
maximum	O
of	O
i	O
(	O
x	O
;	O
y	O
)	O
.	O
so	O
it	O
is	O
tempting	O
to	O
put	O
the	O
derivative	O
of	O
i	O
(	O
x	O
;	O
y	O
)	O
into	O
a	O
routine	O
that	O
(	O
cid:12	O
)	O
nds	O
a	O
local	O
maximum	O
of	O
i	O
(	O
x	O
;	O
y	O
)	O
,	O
that	O
is	O
,	O
an	O
input	O
distribution	O
p	O
(	O
x	O
)	O
such	O
that	O
@	O
i	O
(	O
x	O
;	O
y	O
)	O
@	O
pi	O
=	O
(	O
cid:21	O
)	O
for	O
all	O
i	O
;	O
(	O
10.22	O
)	O
where	O
(	O
cid:21	O
)	O
is	O
a	O
lagrange	O
multiplier	O
associated	O
with	O
the	O
constraint	O
pi	O
pi	O
=	O
1.	O
however	O
,	O
this	O
approach	O
may	O
fail	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
right	O
answer	O
,	O
because	O
i	O
(	O
x	O
;	O
y	O
)	O
might	O
be	O
maximized	O
by	O
a	O
distribution	B
that	O
has	O
pi	O
=	O
0	O
for	O
some	O
inputs	O
.	O
a	O
simple	O
example	O
is	O
given	O
by	O
the	O
ternary	O
confusion	O
channel	B
.	O
ternary	O
confusion	O
channel	B
.	O
ax	O
=f0	O
;	O
?	O
;	O
1g	O
.	O
ay	O
=f0	O
;	O
1g	O
.	O
-	O
0	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
?	O
@	O
@	O
r1	O
-	O
0	O
1	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
0	O
)	O
=	O
1	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
0	O
)	O
=	O
0	O
;	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
?	O
)	O
=	O
1=2	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
?	O
)	O
=	O
1=2	O
;	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
1	O
)	O
=	O
0	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
1	O
)	O
=	O
1	O
:	O
whenever	O
the	O
input	O
?	O
is	O
used	O
,	O
the	O
output	O
is	O
random	B
;	O
the	O
other	O
inputs	O
are	O
reliable	O
inputs	O
.	O
the	O
maximum	O
information	O
rate	B
of	O
1	O
bit	B
is	O
achieved	O
by	O
making	O
no	O
use	O
of	O
the	O
input	O
?	O
.	O
.	O
exercise	O
10.4	O
.	O
[	O
2	O
,	O
p.173	O
]	O
sketch	O
the	O
mutual	B
information	I
for	O
this	O
channel	B
as	O
a	O
function	B
of	O
the	O
input	O
distribution	O
p.	O
pick	O
a	O
convenient	O
two-dimensional	B
representation	O
of	O
p.	O
the	O
optimization	B
routine	O
must	O
therefore	O
take	O
account	O
of	O
the	O
possibility	O
that	O
,	O
as	O
we	O
go	O
up	O
hill	O
on	O
i	O
(	O
x	O
;	O
y	O
)	O
,	O
we	O
may	O
run	O
into	O
the	O
inequality	B
constraints	O
pi	O
(	O
cid:21	O
)	O
0.	O
.	O
exercise	O
10.5	O
.	O
[	O
2	O
,	O
p.174	O
]	O
describe	O
the	O
condition	O
,	O
similar	O
to	O
equation	O
(	O
10.22	O
)	O
,	O
that	O
is	O
satis	O
(	O
cid:12	O
)	O
ed	O
at	O
a	O
point	O
where	O
i	O
(	O
x	O
;	O
y	O
)	O
is	O
maximized	O
,	O
and	O
describe	O
a	O
com-	O
puter	O
program	O
for	O
(	O
cid:12	O
)	O
nding	O
the	O
capacity	B
of	O
a	O
channel	B
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
170	O
10	O
|	O
the	O
noisy-channel	B
coding	I
theorem	I
results	O
that	O
may	O
help	O
in	O
(	O
cid:12	O
)	O
nding	O
the	O
optimal	B
input	I
distribution	I
1.	O
all	O
outputs	O
must	O
be	O
used	O
.	O
2.	O
i	O
(	O
x	O
;	O
y	O
)	O
is	O
a	O
convex	B
^	O
function	B
of	O
the	O
channel	B
parameters	O
.	O
3.	O
there	O
may	O
be	O
several	O
optimal	O
input	O
distributions	O
,	O
but	O
they	O
all	O
look	O
the	O
same	O
at	O
the	O
output	O
.	O
.	O
exercise	O
10.6	O
.	O
[	O
2	O
]	O
prove	O
that	O
no	O
output	O
y	O
is	O
unused	O
by	O
an	O
optimal	O
input	O
distri-	O
reminder	O
:	O
the	O
term	O
‘	O
convex	B
^	O
’	O
means	O
‘	O
convex	B
’	O
,	O
and	O
the	O
term	O
‘	O
concave	B
_	I
’	O
means	O
‘	O
concave	O
’	O
;	O
the	O
little	O
smile	O
and	O
frown	O
symbols	O
are	O
included	O
simply	O
to	O
remind	O
you	O
what	O
convex	O
and	O
concave	O
mean	B
.	O
bution	O
,	O
unless	O
it	O
is	O
unreachable	O
,	O
that	O
is	O
,	O
has	O
q	O
(	O
y	O
j	O
x	O
)	O
=	O
0	O
for	O
all	O
x.	O
exercise	O
10.7	O
.	O
[	O
2	O
]	O
prove	O
that	O
i	O
(	O
x	O
;	O
y	O
)	O
is	O
a	O
convex	B
^	O
function	B
of	O
q	O
(	O
y	O
j	O
x	O
)	O
.	O
exercise	O
10.8	O
.	O
[	O
2	O
]	O
prove	O
that	O
all	O
optimal	O
input	O
distributions	O
of	O
a	O
channel	B
have	O
the	O
same	O
output	O
probability	B
distribution	O
p	O
(	O
y	O
)	O
=px	O
p	O
(	O
x	O
)	O
q	O
(	O
y	O
j	O
x	O
)	O
.	O
these	O
results	O
,	O
along	O
with	O
the	O
fact	O
that	O
i	O
(	O
x	O
;	O
y	O
)	O
is	O
a	O
concave	B
_	I
function	O
of	O
the	O
input	O
probability	B
vector	O
p	O
,	O
prove	O
the	O
validity	O
of	O
the	O
symmetry	O
argument	O
that	O
we	O
have	O
used	O
when	O
(	O
cid:12	O
)	O
nding	O
the	O
capacity	B
of	O
symmetric	B
channels	O
.	O
if	O
a	O
channel	B
is	O
invariant	O
under	O
a	O
group	O
of	O
symmetry	O
operations	O
{	O
for	O
example	O
,	O
interchanging	O
the	O
input	O
symbols	O
and	O
interchanging	O
the	O
output	O
symbols	O
{	O
then	O
,	O
given	O
any	O
optimal	B
input	I
distribution	I
that	O
is	O
not	O
symmetric	B
,	O
i.e.	O
,	O
is	O
not	O
invariant	O
under	O
these	O
operations	O
,	O
we	O
can	O
create	O
another	O
input	O
distribution	O
by	O
averaging	O
together	O
this	O
optimal	B
input	I
distribution	I
and	O
all	O
its	O
permuted	O
forms	O
that	O
we	O
can	O
make	O
by	O
applying	O
the	O
symmetry	O
operations	O
to	O
the	O
original	O
optimal	B
input	I
distribution	I
.	O
the	O
permuted	O
distributions	O
must	O
have	O
the	O
same	O
i	O
(	O
x	O
;	O
y	O
)	O
as	O
the	O
original	O
,	O
by	O
symmetry	O
,	O
so	O
the	O
new	O
input	O
distribution	O
created	O
by	O
averaging	O
must	O
have	O
i	O
(	O
x	O
;	O
y	O
)	O
bigger	O
than	O
or	O
equal	O
to	O
that	O
of	O
the	O
original	O
distribution	B
,	O
because	O
of	O
the	O
concavity	O
of	O
i.	O
symmetric	B
channels	O
in	O
order	O
to	O
use	O
symmetry	O
arguments	O
,	O
it	O
will	O
help	O
to	O
have	O
a	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
a	O
symmetric	B
channel	I
.	O
i	O
like	O
gallager	O
’	O
s	O
(	O
1968	O
)	O
de	O
(	O
cid:12	O
)	O
nition	O
.	O
a	O
discrete	B
memoryless	I
channel	O
is	O
a	O
symmetric	B
channel	I
if	O
the	O
set	B
of	O
outputs	O
can	O
be	O
partitioned	O
into	O
subsets	O
in	O
such	O
a	O
way	O
that	O
for	O
each	O
subset	B
the	O
matrix	B
of	O
transition	B
probabilities	O
has	O
the	O
property	O
that	O
each	O
row	O
(	O
if	O
more	O
than	O
1	O
)	O
is	O
a	O
permutation	B
of	O
each	O
other	O
row	O
and	O
each	O
column	O
is	O
a	O
permutation	B
of	O
each	O
other	O
column	O
.	O
example	O
10.9.	O
this	O
channel	B
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
0	O
)	O
=	O
0:7	O
;	O
p	O
(	O
y	O
=	O
?	O
j	O
x	O
=	O
0	O
)	O
=	O
0:2	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
0	O
)	O
=	O
0:1	O
;	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
1	O
)	O
=	O
0:1	O
;	O
p	O
(	O
y	O
=	O
?	O
j	O
x	O
=	O
1	O
)	O
=	O
0:2	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
1	O
)	O
=	O
0:7	O
:	O
(	O
10.23	O
)	O
is	O
a	O
symmetric	B
channel	I
because	O
its	O
outputs	O
can	O
be	O
partitioned	O
into	O
(	O
0	O
;	O
1	O
)	O
and	O
?	O
,	O
so	O
that	O
the	O
matrix	B
can	O
be	O
rewritten	O
:	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
0	O
)	O
=	O
0:7	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
0	O
)	O
=	O
0:1	O
;	O
p	O
(	O
y	O
=	O
?	O
j	O
x	O
=	O
0	O
)	O
=	O
0:2	O
;	O
p	O
(	O
y	O
=	O
0j	O
x	O
=	O
1	O
)	O
=	O
0:1	O
;	O
p	O
(	O
y	O
=	O
1j	O
x	O
=	O
1	O
)	O
=	O
0:7	O
;	O
p	O
(	O
y	O
=	O
?	O
j	O
x	O
=	O
1	O
)	O
=	O
0:2	O
:	O
(	O
10.24	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
10.7	O
:	O
other	O
coding	O
theorems	O
171	O
symmetry	O
is	O
a	O
useful	O
property	O
because	O
,	O
as	O
we	O
will	O
see	O
in	O
a	O
later	O
chapter	O
,	O
communication	B
at	O
capacity	B
can	O
be	O
achieved	O
over	O
symmetric	O
channels	O
by	O
linear	O
codes	O
.	O
exercise	O
10.10	O
.	O
[	O
2	O
]	O
prove	O
that	O
for	O
a	O
symmetric	B
channel	I
with	O
any	O
number	O
of	O
inputs	O
,	O
the	O
uniform	O
distribution	B
over	O
the	O
inputs	O
is	O
an	O
optimal	B
input	I
distribution	I
.	O
.	O
exercise	O
10.11	O
.	O
[	O
2	O
,	O
p.174	O
]	O
are	O
there	O
channels	O
that	O
are	O
not	O
symmetric	B
whose	O
op-	O
timal	O
input	O
distributions	O
are	O
uniform	O
?	O
find	O
one	O
,	O
or	O
prove	O
there	O
are	O
none	O
.	O
10.7	O
other	O
coding	O
theorems	O
the	O
noisy-channel	B
coding	I
theorem	I
that	O
we	O
proved	O
in	O
this	O
chapter	O
is	O
quite	O
gen-	O
eral	O
,	O
applying	O
to	O
any	O
discrete	B
memoryless	I
channel	O
;	O
but	O
it	O
is	O
not	O
very	O
speci	O
(	O
cid:12	O
)	O
c.	O
the	O
theorem	B
only	O
says	O
that	O
reliable	O
communication	B
with	O
error	B
probability	I
(	O
cid:15	O
)	O
and	O
rate	O
r	O
can	O
be	O
achieved	O
by	O
using	O
codes	O
with	O
su	O
(	O
cid:14	O
)	O
ciently	O
large	O
blocklength	O
n	O
.	O
the	O
theorem	B
does	O
not	O
say	O
how	O
large	O
n	O
needs	O
to	O
be	O
to	O
achieve	O
given	O
values	O
of	O
r	O
and	O
(	O
cid:15	O
)	O
.	O
presumably	O
,	O
the	O
smaller	O
(	O
cid:15	O
)	O
is	O
and	O
the	O
closer	O
r	O
is	O
to	O
c	O
,	O
the	O
larger	O
n	O
has	O
to	O
be	O
.	O
er	O
(	O
r	O
)	O
c	O
r	O
figure	O
10.8.	O
a	O
typical	B
random-coding	O
exponent	O
.	O
noisy-channel	B
coding	I
theorem	I
{	O
version	O
with	O
explicit	O
n	O
-dependence	O
for	O
a	O
discrete	B
memoryless	I
channel	O
,	O
a	O
blocklength	O
n	O
and	O
a	O
rate	B
r	O
,	O
there	O
exist	O
block	B
codes	O
of	O
length	O
n	O
whose	O
average	B
probability	O
of	O
error	O
satis	O
(	O
cid:12	O
)	O
es	O
:	O
pb	O
(	O
cid:20	O
)	O
exp	O
[	O
(	O
cid:0	O
)	O
n	O
er	O
(	O
r	O
)	O
]	O
(	O
10.25	O
)	O
where	O
er	O
(	O
r	O
)	O
is	O
the	O
random-coding	B
exponent	I
of	O
the	O
channel	B
,	O
a	O
convex	B
^	O
,	O
decreasing	O
,	O
positive	O
function	O
of	O
r	O
for	O
0	O
(	O
cid:20	O
)	O
r	O
<	O
c.	O
the	O
random-coding	B
exponent	I
is	O
also	O
known	O
as	O
the	O
reliability	B
function	I
.	O
[	O
by	O
an	O
expurgation	B
argument	O
it	O
can	O
also	O
be	O
shown	O
that	O
there	O
exist	O
block	B
codes	O
for	O
which	O
the	O
maximal	O
probability	B
of	I
error	I
pbm	O
is	O
also	O
exponentially	O
small	O
in	O
n	O
.	O
]	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
er	O
(	O
r	O
)	O
is	O
given	O
in	O
gallager	O
(	O
1968	O
)	O
,	O
p.	O
139.	O
er	O
(	O
r	O
)	O
approaches	O
zero	O
as	O
r	O
!	O
c	O
;	O
the	O
typical	B
behaviour	I
of	I
this	O
function	B
is	O
illustrated	O
in	O
(	O
cid:12	O
)	O
g-	O
ure	O
10.8.	O
the	O
computation	O
of	O
the	O
random-coding	O
exponent	O
for	O
interesting	O
channels	O
is	O
a	O
challenging	O
task	O
on	O
which	O
much	O
e	O
(	O
cid:11	O
)	O
ort	O
has	O
been	O
expended	O
.	O
even	O
for	O
simple	O
channels	O
like	O
the	O
binary	B
symmetric	I
channel	I
,	O
there	O
is	O
no	O
simple	O
ex-	O
pression	O
for	O
er	O
(	O
r	O
)	O
.	O
lower	O
bounds	O
on	O
the	O
error	B
probability	I
as	O
a	O
function	B
of	O
blocklength	O
the	O
theorem	B
stated	O
above	O
asserts	O
that	O
there	O
are	O
codes	O
with	O
pb	O
smaller	O
than	O
exp	O
[	O
(	O
cid:0	O
)	O
n	O
er	O
(	O
r	O
)	O
]	O
.	O
but	O
how	O
small	O
can	O
the	O
error	B
probability	I
be	O
?	O
could	O
it	O
be	O
much	O
smaller	O
?	O
for	O
any	O
code	B
with	O
blocklength	O
n	O
on	O
a	O
discrete	B
memoryless	I
channel	O
,	O
the	O
probability	B
of	I
error	I
assuming	O
all	O
source	O
messages	O
are	O
used	O
with	O
equal	O
probability	B
satis	O
(	O
cid:12	O
)	O
es	O
pb	O
 	B
exp	O
[	O
(	O
cid:0	O
)	O
n	O
esp	O
(	O
r	O
)	O
]	O
;	O
(	O
10.26	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
172	O
10	O
|	O
the	O
noisy-channel	B
coding	I
theorem	I
where	O
the	O
function	B
esp	O
(	O
r	O
)	O
,	O
the	O
sphere-packing	B
exponent	I
of	O
the	O
channel	B
,	O
is	O
a	O
convex	B
^	O
,	O
decreasing	O
,	O
positive	O
function	O
of	O
r	O
for	O
0	O
(	O
cid:20	O
)	O
r	O
<	O
c.	O
for	O
a	O
precise	O
statement	O
of	O
this	O
result	O
and	O
further	O
references	O
,	O
see	O
gallager	O
(	O
1968	O
)	O
,	O
p.	O
157	O
.	O
10.8	O
noisy-channel	B
coding	I
theorems	O
and	O
coding	O
practice	O
imagine	O
a	O
customer	O
who	O
wants	O
to	O
buy	O
an	O
error-correcting	B
code	I
and	O
decoder	B
for	O
a	O
noisy	B
channel	I
.	O
the	O
results	O
described	O
above	O
allow	O
us	O
to	O
o	O
(	O
cid:11	O
)	O
er	O
the	O
following	O
service	O
:	O
if	O
he	O
tells	O
us	O
the	O
properties	O
of	O
his	O
channel	B
,	O
the	O
desired	O
rate	B
r	O
and	O
the	O
desired	O
error	B
probability	I
pb	O
,	O
we	O
can	O
,	O
after	O
working	O
out	O
the	O
relevant	O
functions	B
c	O
,	O
er	O
(	O
r	O
)	O
,	O
and	O
esp	O
(	O
r	O
)	O
,	O
advise	O
him	O
that	O
there	O
exists	O
a	O
solution	O
to	O
his	O
problem	O
using	O
a	O
particular	O
blocklength	O
n	O
;	O
indeed	O
that	O
almost	O
any	O
randomly	O
chosen	O
code	B
with	O
that	O
blocklength	O
should	O
do	O
the	O
job	O
.	O
unfortunately	O
we	O
have	O
not	O
found	O
out	O
how	O
to	O
implement	O
these	O
encoders	O
and	O
decoders	O
in	O
practice	O
;	O
the	O
cost	O
of	O
implementing	O
the	O
encoder	B
and	O
decoder	B
for	O
a	O
random	B
code	I
with	O
large	O
n	O
would	O
be	O
exponentially	O
large	O
in	O
n	O
.	O
furthermore	O
,	O
for	O
practical	O
purposes	O
,	O
the	O
customer	O
is	O
unlikely	O
to	O
know	O
ex-	O
actly	O
what	O
channel	O
he	O
is	O
dealing	O
with	O
.	O
so	O
berlekamp	O
(	O
1980	O
)	O
suggests	O
that	O
the	O
sensible	O
way	O
to	O
approach	O
error-correction	B
is	O
to	O
design	O
encoding-decoding	O
systems	O
and	O
plot	O
their	O
performance	O
on	O
a	O
variety	O
of	O
idealized	O
channels	O
as	O
a	O
function	B
of	O
the	O
channel	B
’	O
s	O
noise	B
level	O
.	O
these	O
charts	O
(	O
one	O
of	O
which	O
is	O
illustrated	O
on	O
page	O
568	O
)	O
can	O
then	O
be	O
shown	O
to	O
the	O
customer	O
,	O
who	O
can	O
choose	O
among	O
the	O
systems	O
on	O
o	O
(	O
cid:11	O
)	O
er	O
without	O
having	O
to	O
specify	O
what	O
he	O
really	O
thinks	O
his	O
channel	B
is	O
like	O
.	O
with	O
this	O
attitude	O
to	O
the	O
practical	B
problem	O
,	O
the	O
importance	O
of	O
the	O
functions	B
er	O
(	O
r	O
)	O
and	O
esp	O
(	O
r	O
)	O
is	O
diminished	O
.	O
10.9	O
further	O
exercises	O
exercise	O
10.12	O
.	O
[	O
2	O
]	O
a	O
binary	B
erasure	I
channel	I
with	O
input	O
x	O
and	O
output	O
y	O
has	O
transition	B
probability	I
matrix	O
:	O
q	O
=2	O
4	O
1	O
(	O
cid:0	O
)	O
q	O
q	O
0	O
0	O
q	O
1	O
(	O
cid:0	O
)	O
q	O
3	O
5	O
-	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
-	O
0	O
1	O
0	O
?	O
1	O
find	O
the	O
mutual	B
information	I
i	O
(	O
x	O
;	O
y	O
)	O
between	O
the	O
input	O
and	O
output	O
for	O
general	O
input	O
distribution	O
fp0	O
;	O
p1g	O
,	O
and	O
show	O
that	O
the	O
capacity	B
of	O
this	O
channel	B
is	O
c	O
=	O
1	O
(	O
cid:0	O
)	O
q	O
bits	O
.	O
a	O
z	O
channel	B
has	O
transition	B
probability	I
matrix	O
:	O
q	O
=	O
(	O
cid:20	O
)	O
1	O
0	O
1	O
(	O
cid:0	O
)	O
q	O
(	O
cid:21	O
)	O
q	O
-	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
-	O
(	O
cid:1	O
)	O
0	O
1	O
0	O
1	O
show	O
that	O
,	O
using	O
a	O
(	O
2	O
;	O
1	O
)	O
code	B
,	O
two	O
uses	O
of	O
a	O
z	O
channel	B
can	O
be	O
made	O
to	O
emulate	O
one	O
use	O
of	O
an	O
erasure	B
channel	I
,	O
and	O
state	O
the	O
erasure	B
probability	O
of	O
that	O
erasure	B
channel	I
.	O
hence	O
show	O
that	O
the	O
capacity	B
of	O
the	O
z	O
channel	B
,	O
cz	O
,	O
satis	O
(	O
cid:12	O
)	O
es	O
cz	O
(	O
cid:21	O
)	O
1	O
2	O
(	O
1	O
(	O
cid:0	O
)	O
q	O
)	O
bits	O
.	O
explain	O
why	O
the	O
result	O
cz	O
(	O
cid:21	O
)	O
1	O
equality	O
.	O
2	O
(	O
1	O
(	O
cid:0	O
)	O
q	O
)	O
is	O
an	O
inequality	B
rather	O
than	O
an	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
10.10	O
:	O
solutions	O
173	O
exercise	O
10.13	O
.	O
[	O
3	O
,	O
p.174	O
]	O
a	O
transatlantic	B
cable	I
contains	O
n	O
=	O
20	O
indistinguish-	O
able	O
electrical	O
wires	O
.	O
you	O
have	O
the	O
job	O
of	O
(	O
cid:12	O
)	O
guring	O
out	O
which	O
wire	O
is	O
which	O
,	O
that	O
is	O
,	O
to	O
create	O
a	O
consistent	O
labelling	O
of	O
the	O
wires	O
at	O
each	O
end	O
.	O
your	O
only	O
tools	O
are	O
the	O
ability	O
to	O
connect	O
wires	O
to	O
each	O
other	O
in	O
groups	O
of	O
two	O
or	O
more	O
,	O
and	O
to	O
test	B
for	O
connectedness	O
with	O
a	O
continuity	O
tester	O
.	O
what	O
is	O
the	O
smallest	O
number	O
of	O
transatlantic	O
trips	O
you	O
need	O
to	O
make	O
,	O
and	O
how	O
do	O
you	O
do	O
it	O
?	O
how	O
would	O
you	O
solve	O
the	O
problem	O
for	O
larger	O
n	O
such	O
as	O
n	O
=	O
1000	O
?	O
as	O
an	O
illustration	O
,	O
if	O
n	O
were	O
3	O
then	O
the	O
task	O
can	O
be	O
solved	O
in	O
two	O
steps	O
by	O
labelling	O
one	O
wire	O
at	O
one	O
end	O
a	O
,	O
connecting	O
the	O
other	O
two	O
together	O
,	O
crossing	O
the	O
atlantic	O
,	O
measuring	O
which	O
two	O
wires	O
are	O
connected	O
,	O
labelling	O
them	O
b	O
and	O
c	O
and	O
the	O
unconnected	O
one	O
a	O
,	O
then	O
connecting	O
b	O
to	O
a	O
and	O
returning	O
across	O
the	O
atlantic	O
,	O
whereupon	O
on	O
disconnecting	O
b	O
from	O
c	O
,	O
the	O
identities	O
of	O
b	O
and	O
c	O
can	O
be	O
deduced	O
.	O
this	O
problem	O
can	O
be	O
solved	O
by	O
persistent	O
search	O
,	O
but	O
the	O
reason	O
it	O
is	O
posed	O
in	O
this	O
chapter	O
is	O
that	O
it	O
can	O
also	O
be	O
solved	O
by	O
a	O
greedy	O
approach	O
based	O
on	O
maximizing	O
the	O
acquired	O
information	B
.	O
let	O
the	O
unknown	O
per-	O
mutation	O
of	O
wires	O
be	O
x.	O
having	O
chosen	O
a	O
set	B
of	O
connections	O
of	O
wires	O
c	O
at	O
one	O
end	O
,	O
you	O
can	O
then	O
make	O
measurements	O
at	O
the	O
other	O
end	O
,	O
and	O
these	O
measurements	O
y	O
convey	O
information	B
about	O
x.	O
how	O
much	O
?	O
and	O
for	O
what	O
set	O
of	O
connections	O
is	O
the	O
information	B
that	O
y	O
conveys	O
about	O
x	O
maximized	O
?	O
10.10	O
solutions	O
solution	O
to	O
exercise	O
10.4	O
(	O
p.169	O
)	O
.	O
the	O
mutual	B
information	I
is	O
if	O
the	O
input	O
distribution	O
is	O
p	O
=	O
(	O
p0	O
;	O
p	O
?	O
;	O
p1	O
)	O
,	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
y	O
jx	O
)	O
=	O
h2	O
(	O
p0	O
+	O
p	O
?	O
=2	O
)	O
(	O
cid:0	O
)	O
p	O
?	O
:	O
(	O
10.27	O
)	O
we	O
can	O
build	O
a	O
good	B
sketch	O
of	O
this	O
function	B
in	O
two	O
ways	O
:	O
by	O
careful	O
inspection	O
of	O
the	O
function	O
,	O
or	O
by	O
looking	O
at	O
special	O
cases	O
.	O
for	O
the	O
plots	O
,	O
the	O
two-dimensional	B
representation	O
of	O
p	O
i	O
will	O
use	O
has	O
p0	O
and	O
p1	O
as	O
the	O
independent	O
variables	O
,	O
so	O
that	O
p	O
=	O
(	O
p0	O
;	O
p	O
?	O
;	O
p1	O
)	O
=	O
(	O
p0	O
;	O
(	O
1	O
(	O
cid:0	O
)	O
p0	O
(	O
cid:0	O
)	O
p1	O
)	O
;	O
p1	O
)	O
.	O
if	O
we	O
use	O
the	O
quantities	O
p	O
(	O
cid:3	O
)	O
(	O
cid:17	O
)	O
p0	O
+	O
p	O
?	O
=2	O
and	O
p	O
?	O
as	O
our	O
two	O
by	O
inspection	O
.	O
degrees	B
of	I
freedom	I
,	O
the	O
mutual	B
information	I
becomes	O
very	O
simple	O
:	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h2	O
(	O
p	O
(	O
cid:3	O
)	O
)	O
(	O
cid:0	O
)	O
p	O
?	O
.	O
converting	O
back	O
to	O
p0	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
cid:0	O
)	O
p	O
?	O
=2	O
and	O
p1	O
=	O
1	O
(	O
cid:0	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
cid:0	O
)	O
p	O
?	O
=2	O
,	O
we	O
obtain	O
the	O
sketch	O
shown	O
at	O
the	O
left	O
below	O
.	O
this	O
function	B
is	O
like	O
a	O
tunnel	O
rising	O
up	O
the	O
direction	O
of	O
increasing	O
p0	O
and	O
p1	O
.	O
to	O
obtain	O
the	O
required	O
plot	O
of	O
i	O
(	O
x	O
;	O
y	O
)	O
we	O
have	O
to	O
strip	O
away	O
the	O
parts	O
of	O
this	O
tunnel	O
that	O
live	O
outside	O
the	O
feasible	O
simplex	B
of	O
probabilities	O
;	O
we	O
do	O
this	O
by	O
redrawing	O
the	O
surface	O
,	O
showing	O
only	O
the	O
parts	O
where	O
p0	O
>	O
0	O
and	O
p1	O
>	O
0.	O
a	O
full	O
plot	O
of	O
the	O
function	O
is	O
shown	O
at	O
the	O
right	O
.	O
-	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
1/2	O
1/2	O
@	O
@	O
r	O
-	O
(	O
cid:0	O
)	O
@	O
0	O
?	O
1	O
0	O
1	O
1	O
0.5	O
0	O
-0.5	O
-1	O
-0.5	O
0	O
0.5	O
p0	O
1	O
1	O
1	O
0.5	O
0.5	O
0	O
0	O
0	O
0	O
p1	O
1	O
0.5	O
0	O
-0.5	O
1	O
1	O
0.5	O
0.5	O
p1	O
0	O
0	O
0.5	O
0.5	O
1	O
1	O
p0	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
174	O
10	O
|	O
the	O
noisy-channel	B
coding	I
theorem	I
1	O
0.5	O
0	O
0	O
p1	O
1	O
0.5	O
0	O
0.5	O
p0	O
1	O
figure	O
10.9.	O
skeleton	O
of	O
the	O
mutual	O
information	B
for	O
the	O
ternary	O
confusion	O
channel	B
.	O
special	O
cases	O
.	O
channel	B
,	O
and	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
h2	O
(	O
p0	O
)	O
.	O
in	O
the	O
special	O
case	O
p	O
?	O
=	O
0	O
,	O
the	O
channel	B
is	O
a	O
noiseless	B
binary	O
in	O
the	O
special	O
case	O
p0	O
=	O
p1	O
,	O
the	O
term	O
h2	O
(	O
p0	O
+	O
p	O
?	O
=2	O
)	O
is	O
equal	O
to	O
1	O
,	O
so	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
p	O
?	O
.	O
0.5.	O
we	O
know	O
how	O
to	O
sketch	O
that	O
,	O
from	O
the	O
previous	O
chapter	O
(	O
(	O
cid:12	O
)	O
gure	O
9.3	O
)	O
.	O
in	O
the	O
special	O
case	O
p0	O
=	O
0	O
,	O
the	O
channel	B
is	O
a	O
z	O
channel	O
with	O
error	O
probability	B
these	O
special	O
cases	O
allow	O
us	O
to	O
construct	O
the	O
skeleton	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
10.9.	O
solution	O
to	O
exercise	O
10.5	O
(	O
p.169	O
)	O
.	O
necessary	O
and	O
su	O
(	O
cid:14	O
)	O
cient	O
conditions	O
for	O
p	O
to	O
maximize	O
i	O
(	O
x	O
;	O
y	O
)	O
are	O
@	O
i	O
(	O
x	O
;	O
y	O
)	O
@	O
pi	O
@	O
i	O
(	O
x	O
;	O
y	O
)	O
@	O
pi	O
=	O
(	O
cid:21	O
)	O
and	O
pi	O
>	O
0	O
(	O
cid:20	O
)	O
(	O
cid:21	O
)	O
and	O
pi	O
=	O
0	O
)	O
for	O
all	O
i	O
;	O
(	O
10.28	O
)	O
where	O
(	O
cid:21	O
)	O
is	O
a	O
constant	O
related	O
to	O
the	O
capacity	B
by	O
c	O
=	O
(	O
cid:21	O
)	O
+	O
log2	O
e.	O
this	O
result	O
can	O
be	O
used	O
in	O
a	O
computer	B
program	O
that	O
evaluates	O
the	O
deriva-	O
tives	O
,	O
and	O
increments	O
and	O
decrements	O
the	O
probabilities	O
pi	O
in	O
proportion	O
to	O
the	O
di	O
(	O
cid:11	O
)	O
erences	O
between	O
those	O
derivatives	O
.	O
this	O
result	O
is	O
also	O
useful	O
for	O
lazy	O
human	B
capacity-	O
(	O
cid:12	O
)	O
nders	O
who	O
are	O
good	B
guessers	O
.	O
having	O
guessed	O
the	O
optimal	B
input	I
distribution	I
,	O
one	O
can	O
simply	O
con-	O
(	O
cid:12	O
)	O
rm	O
that	O
equation	O
(	O
10.28	O
)	O
holds	O
.	O
solution	O
to	O
exercise	O
10.11	O
(	O
p.171	O
)	O
.	O
we	O
certainly	O
expect	O
nonsymmetric	O
chan-	O
nels	O
with	O
uniform	O
optimal	O
input	O
distributions	O
to	O
exist	O
,	O
since	O
when	O
inventing	O
a	O
channel	B
we	O
have	O
i	O
(	O
j	O
(	O
cid:0	O
)	O
1	O
)	O
degrees	B
of	I
freedom	I
whereas	O
the	O
optimal	O
input	O
dis-	O
tribution	O
is	O
just	O
(	O
i	O
(	O
cid:0	O
)	O
1	O
)	O
-dimensional	O
;	O
so	O
in	O
the	O
i	O
(	O
j	O
(	O
cid:0	O
)	O
1	O
)	O
-dimensional	O
space	O
of	O
perturbations	O
around	O
a	O
symmetric	B
channel	I
,	O
we	O
expect	O
there	O
to	O
be	O
a	O
subspace	O
of	O
perturbations	O
of	O
dimension	O
i	O
(	O
j	O
(	O
cid:0	O
)	O
1	O
)	O
(	O
cid:0	O
)	O
(	O
i	O
(	O
cid:0	O
)	O
1	O
)	O
=	O
i	O
(	O
j	O
(	O
cid:0	O
)	O
2	O
)	O
+	O
1	O
that	O
leave	O
the	O
optimal	B
input	I
distribution	I
unchanged	O
.	O
here	O
is	O
an	O
explicit	O
example	O
,	O
a	O
bit	B
like	O
a	O
z	O
channel	B
.	O
q	O
=2	O
664	O
0:9585	O
0:0415	O
0:35	O
0:0415	O
0:9585	O
0:0	O
0:65	O
0:0	O
0:35	O
0	O
0	O
0	O
0	O
0	O
0	O
0:65	O
3	O
775	O
(	O
10.29	O
)	O
solution	O
to	O
exercise	O
10.13	O
(	O
p.173	O
)	O
.	O
the	O
labelling	O
problem	O
can	O
be	O
solved	O
for	O
any	O
n	O
>	O
2	O
with	O
just	O
two	O
trips	O
,	O
one	O
each	O
way	O
across	O
the	O
atlantic	O
.	O
the	O
key	O
step	O
in	O
the	O
information-theoretic	O
approach	O
to	O
this	O
problem	O
is	O
to	O
write	O
down	O
the	O
information	B
content	I
of	O
one	O
partition	B
,	O
the	O
combinatorial	O
object	O
that	O
is	O
the	O
connecting	O
together	O
of	O
subsets	O
of	O
wires	O
.	O
if	O
n	O
wires	O
are	O
grouped	O
together	O
into	O
g1	O
subsets	O
of	O
size	O
1	O
,	O
g2	O
subsets	O
of	O
size	O
2	O
,	O
:	O
:	O
:	O
;	O
then	O
the	O
number	O
of	O
such	O
partitions	O
is	O
(	O
cid:10	O
)	O
=	O
;	O
(	O
10.30	O
)	O
n	O
!	O
(	O
r	O
!	O
)	O
gr	O
gr	O
!	O
yr	O
and	O
the	O
information	B
content	I
of	O
one	O
such	O
partition	B
is	O
the	O
log	O
of	O
this	O
quantity	O
.	O
in	O
a	O
greedy	O
strategy	O
we	O
choose	O
the	O
(	O
cid:12	O
)	O
rst	O
partition	B
to	O
maximize	O
this	O
information	B
content	I
.	O
one	O
game	B
we	O
can	O
play	O
is	O
to	O
maximize	O
this	O
information	B
content	I
with	O
re-	O
spect	O
to	O
the	O
quantities	O
gr	O
,	O
treated	O
as	O
real	O
numbers	O
,	O
subject	O
to	O
the	O
constraint	O
introducing	O
a	O
lagrange	O
multiplier	O
(	O
cid:21	O
)	O
for	O
the	O
constraint	O
,	O
the	O
pr	O
grr	O
=	O
n	O
.	O
derivative	O
is	O
@	O
@	O
gr	O
log	O
(	O
cid:10	O
)	O
+	O
(	O
cid:21	O
)	O
xr	O
grr	O
!	O
=	O
(	O
cid:0	O
)	O
log	O
r	O
!	O
(	O
cid:0	O
)	O
log	O
gr	O
+	O
(	O
cid:21	O
)	O
r	O
;	O
(	O
10.31	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
10.10	O
:	O
solutions	O
175	O
which	O
,	O
when	O
set	B
to	O
zero	O
,	O
leads	O
to	O
the	O
rather	O
nice	O
expression	O
gr	O
=	O
e	O
(	O
cid:21	O
)	O
r	O
r	O
!	O
;	O
(	O
10.32	O
)	O
the	O
optimal	B
gr	O
is	O
proportional	O
to	O
a	O
poisson	O
distribution	B
!	O
we	O
can	O
solve	O
for	O
the	O
n	O
=	O
(	O
cid:22	O
)	O
e	O
(	O
cid:22	O
)	O
;	O
gives	O
the	O
implicit	O
equation	O
lagrange	O
multiplier	O
by	O
plugging	O
gr	O
into	O
the	O
constraint	O
pr	O
grr	O
=	O
n	O
,	O
which	O
(	O
10.33	O
)	O
where	O
(	O
cid:22	O
)	O
(	O
cid:17	O
)	O
e	O
(	O
cid:21	O
)	O
is	O
a	O
convenient	O
reparameterization	O
of	O
the	O
lagrange	O
multiplier	O
.	O
figure	O
10.10a	O
shows	O
a	O
graph	B
of	O
(	O
cid:22	O
)	O
(	O
n	O
)	O
;	O
(	O
cid:12	O
)	O
gure	O
10.10b	O
shows	O
the	O
deduced	O
non-	O
integer	O
assignments	O
gr	O
when	O
(	O
cid:22	O
)	O
=	O
2:2	O
,	O
and	O
nearby	O
integers	O
gr	O
=	O
f1	O
;	O
2	O
;	O
2	O
;	O
1	O
;	O
1g	O
that	O
motivate	O
setting	O
the	O
(	O
cid:12	O
)	O
rst	O
partition	B
to	O
(	O
a	O
)	O
(	O
bc	O
)	O
(	O
de	O
)	O
(	O
fgh	O
)	O
(	O
ijk	O
)	O
(	O
lmno	O
)	O
(	O
pqrst	O
)	O
.	O
this	O
partition	B
produces	O
a	O
random	B
partition	O
at	O
the	O
other	O
end	O
,	O
which	O
has	O
an	O
information	B
content	I
of	O
log	O
(	O
cid:10	O
)	O
=	O
40:4	O
bits	O
,	O
which	O
is	O
a	O
lot	O
more	O
than	O
half	O
the	O
total	O
information	B
content	I
we	O
need	O
to	O
acquire	O
to	O
infer	O
the	O
transatlantic	B
permutation	O
,	O
log	O
20	O
!	O
’	O
61	O
bits	O
.	O
[	O
in	O
contrast	O
,	O
if	O
all	O
the	O
wires	O
are	O
joined	O
together	O
in	O
pairs	O
,	O
the	O
information	B
content	I
generated	O
is	O
only	O
about	O
29	O
bits	O
.	O
]	O
how	O
to	O
choose	O
the	O
second	O
partition	B
is	O
left	O
to	O
the	O
reader	O
.	O
a	O
shannonesque	O
approach	O
is	O
appropriate	O
,	O
picking	O
a	O
random	B
partition	O
at	O
the	O
other	O
end	O
,	O
using	O
the	O
same	O
fgrg	O
;	O
you	O
need	O
to	O
ensure	O
the	O
two	O
partitions	O
are	O
as	O
unlike	O
each	O
other	O
as	O
possible	O
.	O
if	O
n	O
6=	O
2	O
,	O
5	O
or	O
9	O
,	O
then	O
the	O
labelling	O
problem	O
has	O
solutions	O
that	O
are	O
particularly	O
simple	O
to	O
implement	O
,	O
called	O
knowlton	O
{	O
graham	O
partitions	O
:	O
par-	O
tition	O
f1	O
;	O
:	O
:	O
:	O
;	O
ng	O
into	O
disjoint	O
sets	O
in	O
two	O
ways	O
a	O
and	O
b	O
,	O
subject	O
to	O
the	O
condition	O
that	O
at	O
most	O
one	O
element	O
appears	O
both	O
in	O
an	O
a	O
set	B
of	O
cardinal-	O
ity	O
j	O
and	O
in	O
a	O
b	O
set	B
of	O
cardinality	O
k	O
,	O
for	O
each	O
j	O
and	O
k	O
(	O
graham	O
,	O
1966	O
;	O
graham	O
and	O
knowlton	O
,	O
1968	O
)	O
.	O
5.5	O
5	O
4.5	O
4	O
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
(	O
a	O
)	O
0.5	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
(	O
b	O
)	O
1	O
10	O
100	O
1000	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
figure	O
10.10.	O
approximate	O
solution	O
of	O
the	O
cable-labelling	O
problem	O
using	O
lagrange	O
multipliers	O
.	O
(	O
a	O
)	O
the	O
parameter	O
(	O
cid:22	O
)	O
as	O
a	O
function	B
of	O
n	O
;	O
the	O
value	O
(	O
cid:22	O
)	O
(	O
20	O
)	O
=	O
2:2	O
is	O
highlighted	O
.	O
(	O
b	O
)	O
non-integer	O
values	O
of	O
the	O
function	O
gr	O
=	O
(	O
cid:22	O
)	O
r/r	O
!	O
are	O
shown	O
by	O
lines	O
and	O
integer	O
values	O
of	O
gr	O
motivated	O
by	O
those	O
non-integer	O
values	O
are	O
shown	O
by	O
crosses	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
chapter	O
11	O
before	O
reading	O
chapter	O
11	O
,	O
you	O
should	O
have	O
read	O
chapters	O
9	O
and	O
10.	O
you	O
will	O
also	O
need	O
to	O
be	O
familiar	O
with	O
the	O
gaussian	O
distribution	B
.	O
one-dimensional	O
gaussian	O
distribution	B
.	O
if	O
a	O
random	B
variable	I
y	O
is	O
gaus-	O
sian	O
and	O
has	O
mean	B
(	O
cid:22	O
)	O
and	O
variance	O
(	O
cid:27	O
)	O
2	O
,	O
which	O
we	O
write	O
:	O
y	O
(	O
cid:24	O
)	O
normal	B
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
)	O
;	O
or	O
p	O
(	O
y	O
)	O
=	O
normal	B
(	O
y	O
;	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
)	O
;	O
(	O
11.1	O
)	O
then	O
the	O
distribution	B
of	O
y	O
is	O
:	O
p	O
(	O
y	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
)	O
=	O
1	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
exp	O
(	O
cid:2	O
)	O
(	O
cid:0	O
)	O
(	O
y	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
)	O
2=2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:3	O
)	O
:	O
(	O
11.2	O
)	O
[	O
i	O
use	O
the	O
symbol	O
p	O
for	O
both	O
probability	B
densities	O
and	O
probabilities	O
.	O
]	O
the	O
inverse-variance	O
(	O
cid:28	O
)	O
(	O
cid:17	O
)	O
1/	O
(	O
cid:27	O
)	O
2	O
is	O
sometimes	O
called	O
the	O
precision	B
of	O
the	O
gaussian	O
distribution	B
.	O
multi-dimensional	B
gaussian	O
distribution	B
.	O
if	O
y	O
=	O
(	O
y1	O
;	O
y2	O
;	O
:	O
:	O
:	O
;	O
yn	O
)	O
has	O
a	O
multivariate	O
gaussian	O
distribution	B
,	O
then	O
p	O
(	O
y	O
j	O
x	O
;	O
a	O
)	O
=	O
1	O
z	O
(	O
a	O
)	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
y	O
(	O
cid:0	O
)	O
x	O
)	O
ta	O
(	O
y	O
(	O
cid:0	O
)	O
x	O
)	O
(	O
cid:19	O
)	O
;	O
(	O
11.3	O
)	O
where	O
x	O
is	O
the	O
mean	B
of	O
the	O
distribution	B
,	O
a	O
is	O
the	O
inverse	O
of	O
the	O
variance	B
{	O
covariance	B
matrix	I
,	O
and	O
the	O
normalizing	B
constant	I
is	O
z	O
(	O
a	O
)	O
=	O
(	O
det	O
(	O
a=2	O
(	O
cid:25	O
)	O
)	O
)	O
(	O
cid:0	O
)	O
1=2	O
.	O
this	O
distribution	B
has	O
the	O
property	O
that	O
the	O
variance	B
(	O
cid:6	O
)	O
ii	O
of	O
yi	O
,	O
and	O
the	O
covariance	B
(	O
cid:6	O
)	O
ij	O
of	O
yi	O
and	O
yj	O
are	O
given	O
by	O
(	O
cid:6	O
)	O
ij	O
(	O
cid:17	O
)	O
e	O
[	O
(	O
yi	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
yi	O
)	O
(	O
yj	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
yj	O
)	O
]	O
=	O
a	O
(	O
cid:0	O
)	O
1	O
ij	O
;	O
(	O
11.4	O
)	O
where	O
a	O
(	O
cid:0	O
)	O
1	O
is	O
the	O
inverse	O
of	O
the	O
matrix	B
a.	O
the	O
marginal	B
distribution	O
p	O
(	O
yi	O
)	O
of	O
one	O
component	O
yi	O
is	O
gaussian	O
;	O
the	O
joint	B
marginal	O
distribution	B
of	O
any	O
subset	B
of	O
the	O
components	O
is	O
multivariate-gaussian	O
;	O
and	O
the	O
conditional	B
density	O
of	O
any	O
subset	O
,	O
given	O
the	O
values	O
of	O
another	O
subset	B
,	O
for	O
example	O
,	O
p	O
(	O
yi	O
j	O
yj	O
)	O
,	O
is	O
also	O
gaussian	O
.	O
176	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
11	O
error-correcting	B
codes	I
&	O
real	O
channels	O
the	O
noisy-channel	B
coding	I
theorem	I
that	O
we	O
have	O
proved	O
shows	O
that	O
there	O
exist	O
reliable	O
error-correcting	B
codes	I
for	O
any	O
noisy	B
channel	I
.	O
in	O
this	O
chapter	O
we	O
address	B
two	O
questions	O
.	O
first	O
,	O
many	O
practical	B
channels	O
have	O
real	O
,	O
rather	O
than	O
discrete	O
,	O
inputs	O
and	O
outputs	O
.	O
what	O
can	O
shannon	O
tell	O
us	O
about	O
these	O
continuous	B
channels	O
?	O
and	O
how	O
should	O
digital	O
signals	O
be	O
mapped	O
into	O
analogue	O
waveforms	O
,	O
and	O
vice	O
versa	O
?	O
second	O
,	O
how	O
are	O
practical	B
error-correcting	O
codes	O
made	O
,	O
and	O
what	O
is	O
achieved	O
in	O
practice	O
,	O
relative	B
to	O
the	O
possibilities	O
proved	O
by	O
shannon	O
?	O
11.1	O
the	O
gaussian	O
channel	B
the	O
most	O
popular	O
model	B
of	O
a	O
real-input	O
,	O
real-output	O
channel	B
is	O
the	O
gaussian	O
channel	B
.	O
the	O
gaussian	O
channel	B
has	O
a	O
real	O
input	O
x	O
and	O
a	O
real	O
output	O
y.	O
the	O
condi-	O
tional	O
distribution	B
of	O
y	O
given	O
x	O
is	O
a	O
gaussian	O
distribution	B
:	O
p	O
(	O
y	O
j	O
x	O
)	O
=	O
1	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
exp	O
(	O
cid:2	O
)	O
(	O
cid:0	O
)	O
(	O
y	O
(	O
cid:0	O
)	O
x	O
)	O
2=2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:3	O
)	O
:	O
(	O
11.5	O
)	O
this	O
channel	B
has	O
a	O
continuous	B
input	O
and	O
output	O
but	O
is	O
discrete	O
in	O
time	O
.	O
we	O
will	O
show	O
below	O
that	O
certain	O
continuous-time	O
channels	O
are	O
equivalent	O
to	O
the	O
discrete-time	O
gaussian	O
channel	B
.	O
this	O
channel	B
(	O
awgn	O
)	O
channel	B
.	O
is	O
sometimes	O
called	O
the	O
additive	O
white	B
gaussian	O
noise	B
as	O
with	O
discrete	O
channels	O
,	O
we	O
will	O
discuss	O
what	O
rate	O
of	O
error-free	O
information	B
communication	O
can	O
be	O
achieved	O
over	O
this	O
channel	B
.	O
motivation	O
in	O
terms	O
of	O
a	O
continuous-time	O
channel	B
consider	O
a	O
physical	O
(	O
electrical	O
,	O
say	O
)	O
channel	O
with	O
inputs	O
and	O
outputs	O
that	O
are	O
continuous	B
in	O
time	O
.	O
we	O
put	O
in	O
x	O
(	O
t	O
)	O
,	O
and	O
out	O
comes	O
y	O
(	O
t	O
)	O
=	O
x	O
(	O
t	O
)	O
+	O
n	O
(	O
t	O
)	O
.	O
our	O
transmission	O
has	O
a	O
power	B
cost	I
.	O
the	O
average	B
power	O
of	O
a	O
transmission	O
of	O
length	O
t	O
may	O
be	O
constrained	B
thus	O
:	O
z	O
t	O
0	O
dt	O
[	O
x	O
(	O
t	O
)	O
]	O
2=t	O
(	O
cid:20	O
)	O
p	O
:	O
(	O
11.6	O
)	O
the	O
received	O
signal	O
is	O
assumed	O
to	O
di	O
(	O
cid:11	O
)	O
er	O
from	O
x	O
(	O
t	O
)	O
by	O
additive	O
noise	B
n	O
(	O
t	O
)	O
(	O
for	O
example	O
johnson	O
noise	B
)	O
,	O
which	O
we	O
will	O
model	B
as	O
white	B
gaussian	O
noise	B
.	O
the	O
magnitude	O
of	O
this	O
noise	B
is	O
quanti	O
(	O
cid:12	O
)	O
ed	O
by	O
the	O
noise	B
spectral	O
density	B
,	O
n0	O
.	O
177	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
178	O
11	O
|	O
error-correcting	B
codes	I
and	O
real	O
channels	O
how	O
could	O
such	O
a	O
channel	B
be	O
used	O
to	O
communicate	O
information	B
?	O
consider	O
n=1	O
in	O
a	O
signal	O
of	O
duration	O
t	O
made	O
transmitting	O
a	O
set	B
of	O
n	O
real	O
numbers	O
fxngn	O
up	O
of	O
a	O
weighted	O
combination	B
of	O
orthonormal	O
basis	O
functions	O
(	O
cid:30	O
)	O
n	O
(	O
t	O
)	O
,	O
(	O
cid:30	O
)	O
1	O
(	O
t	O
)	O
x	O
(	O
t	O
)	O
=	O
n	O
xn=1	O
xn	O
(	O
cid:30	O
)	O
n	O
(	O
t	O
)	O
;	O
(	O
11.7	O
)	O
(	O
cid:30	O
)	O
2	O
(	O
t	O
)	O
where	O
r	O
t	O
0	O
dt	O
(	O
cid:30	O
)	O
n	O
(	O
t	O
)	O
(	O
cid:30	O
)	O
m	O
(	O
t	O
)	O
=	O
(	O
cid:14	O
)	O
nm	O
.	O
the	O
receiver	O
can	O
then	O
compute	O
the	O
scalars	O
:	O
yn	O
(	O
cid:17	O
)	O
z	O
t	O
0	O
dt	O
(	O
cid:30	O
)	O
n	O
(	O
t	O
)	O
y	O
(	O
t	O
)	O
=	O
xn	O
+z	O
t	O
(	O
cid:17	O
)	O
xn	O
+	O
nn	O
0	O
dt	O
(	O
cid:30	O
)	O
n	O
(	O
t	O
)	O
n	O
(	O
t	O
)	O
(	O
11.8	O
)	O
(	O
11.9	O
)	O
for	O
n	O
=	O
1	O
:	O
:	O
:	O
n	O
.	O
if	O
there	O
were	O
no	O
noise	B
,	O
then	O
yn	O
would	O
equal	O
xn	O
.	O
the	O
white	B
gaussian	O
noise	B
n	O
(	O
t	O
)	O
adds	O
scalar	O
noise	B
nn	O
to	O
the	O
estimate	O
yn	O
.	O
this	O
noise	B
is	O
gaussian	O
:	O
nn	O
(	O
cid:24	O
)	O
normal	B
(	O
0	O
;	O
n0=2	O
)	O
;	O
(	O
11.10	O
)	O
where	O
n0	O
is	O
the	O
spectral	B
density	I
introduced	O
above	O
.	O
thus	O
a	O
continuous	B
chan-	O
nel	O
used	O
in	O
this	O
way	O
is	O
equivalent	O
to	O
the	O
gaussian	O
channel	B
de	O
(	O
cid:12	O
)	O
ned	O
at	O
equa-	O
0	O
dt	O
[	O
x	O
(	O
t	O
)	O
]	O
2	O
(	O
cid:20	O
)	O
p	O
t	O
de	O
(	O
cid:12	O
)	O
nes	O
a	O
constraint	O
on	O
tion	O
(	O
11.5	O
)	O
.	O
the	O
power	O
constraint	O
r	O
t	O
the	O
signal	O
amplitudes	O
xn	O
,	O
(	O
cid:30	O
)	O
3	O
(	O
t	O
)	O
x	O
(	O
t	O
)	O
figure	O
11.1.	O
three	O
basis	O
functions	B
,	O
and	O
a	O
weighted	O
combination	B
of	O
them	O
,	O
x	O
(	O
t	O
)	O
=pn	O
n=1	O
xn	O
(	O
cid:30	O
)	O
n	O
(	O
t	O
)	O
;	O
with	O
x1	O
=	O
0:4	O
,	O
x2	O
=	O
(	O
cid:0	O
)	O
0:2	O
,	O
and	O
x3	O
=	O
0:1.	O
x2	O
n	O
(	O
cid:20	O
)	O
p	O
t	O
)	O
xn	O
x2	O
n	O
(	O
cid:20	O
)	O
p	O
t	O
n	O
:	O
(	O
11.11	O
)	O
before	O
returning	O
to	O
the	O
gaussian	O
channel	B
,	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
bandwidth	B
(	O
mea-	O
sured	O
in	O
hertz	O
)	O
of	O
the	O
continuous	O
channel	B
to	O
be	O
:	O
w	O
=	O
n	O
max	O
2t	O
;	O
(	O
11.12	O
)	O
where	O
n	O
max	O
is	O
the	O
maximum	O
number	O
of	O
orthonormal	O
functions	B
that	O
can	O
be	O
produced	O
in	O
an	O
interval	O
of	O
length	O
t	O
.	O
this	O
de	O
(	O
cid:12	O
)	O
nition	O
can	O
be	O
motivated	O
by	O
imagining	O
creating	O
a	O
band-limited	B
signal	I
of	O
duration	O
t	O
from	O
orthonormal	O
co-	O
sine	O
and	O
sine	O
curves	O
of	O
maximum	O
frequency	B
w	O
.	O
the	O
number	O
of	O
orthonormal	O
functions	B
is	O
n	O
max	O
=	O
2w	O
t	O
.	O
this	O
de	O
(	O
cid:12	O
)	O
nition	O
relates	O
to	O
the	O
nyquist	O
sampling	O
theorem	O
:	O
if	O
the	O
highest	O
frequency	B
present	O
in	O
a	O
signal	O
is	O
w	O
,	O
then	O
the	O
signal	O
can	O
be	O
fully	O
determined	O
from	O
its	O
values	O
at	O
a	O
series	O
of	O
discrete	O
sample	B
points	O
separated	O
by	O
the	O
nyquist	O
interval	O
(	O
cid:1	O
)	O
t	O
=	O
1/2w	O
seconds	O
.	O
so	O
the	O
use	O
of	O
a	O
real	O
continuous	B
channel	I
with	O
bandwidth	B
w	O
,	O
noise	B
spectral	O
density	B
n0	O
,	O
and	O
power	O
p	O
is	O
equivalent	O
to	O
n=t	O
=	O
2w	O
uses	O
per	O
second	O
of	O
a	O
gaussian	O
channel	O
with	O
noise	O
level	O
(	O
cid:27	O
)	O
2	O
=	O
n0=2	O
and	O
subject	O
to	O
the	O
signal	O
power	O
constraint	O
x2	O
n	O
(	O
cid:20	O
)	O
p/2w	O
.	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
eb=n0	O
imagine	O
that	O
the	O
gaussian	O
channel	B
yn	O
=	O
xn	O
+	O
nn	O
is	O
used	O
with	O
an	O
encoding	O
system	O
to	O
transmit	O
binary	O
source	O
bits	O
at	O
a	O
rate	B
of	O
r	O
bits	O
per	O
channel	B
use	O
.	O
how	O
can	O
we	O
compare	O
two	O
encoding	O
systems	O
that	O
have	O
di	O
(	O
cid:11	O
)	O
erent	O
rates	O
of	O
communi-	O
cation	O
r	O
and	O
that	O
use	O
di	O
(	O
cid:11	O
)	O
erent	O
powers	O
x2	O
n	O
?	O
transmitting	O
at	O
a	O
large	O
rate	B
r	O
is	O
good	B
;	O
using	O
small	O
power	O
is	O
good	B
too	O
.	O
it	O
is	O
conventional	O
to	O
measure	O
the	O
rate-compensated	O
signal-to-noise	B
ratio	I
by	O
n=r	O
to	O
the	O
noise	B
spectral	O
density	B
the	O
ratio	O
of	O
the	O
power	O
per	O
source	O
bit	O
eb	O
=	O
x2	O
n0	O
:	O
eb=n0	O
=	O
x2	O
n	O
2	O
(	O
cid:27	O
)	O
2r	O
:	O
(	O
11.13	O
)	O
eb=n0	O
is	O
one	O
of	O
the	O
measures	O
used	O
to	O
compare	O
coding	O
schemes	O
for	O
gaussian	O
channels	O
.	O
eb=n0	O
is	O
dimensionless	O
,	O
but	O
it	O
is	O
usually	O
reported	O
in	O
the	O
units	B
of	O
decibels	O
;	O
the	O
value	O
given	O
is	O
10	O
log10	O
eb=n0	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
11.2	O
:	O
inferring	O
the	O
input	O
to	O
a	O
real	O
channel	B
179	O
11.2	O
inferring	O
the	O
input	O
to	O
a	O
real	O
channel	B
‘	O
the	O
best	O
detection	O
of	O
pulses	O
’	O
in	O
1944	O
shannon	O
wrote	O
a	O
memorandum	O
(	O
shannon	O
,	O
1993	O
)	O
on	O
the	O
problem	O
of	O
best	O
di	O
(	O
cid:11	O
)	O
erentiating	O
between	O
two	O
types	O
of	O
pulses	O
of	O
known	O
shape	O
,	O
represented	O
by	O
vectors	O
x0	O
and	O
x1	O
,	O
given	O
that	O
one	O
of	O
them	O
has	O
been	O
transmitted	O
over	O
a	O
noisy	B
channel	I
.	O
this	O
is	O
a	O
pattern	B
recognition	I
problem	O
.	O
it	O
is	O
assumed	O
that	O
the	O
noise	B
is	O
gaussian	O
with	O
probability	O
density	B
p	O
(	O
n	O
)	O
=	O
(	O
cid:20	O
)	O
det	O
(	O
cid:18	O
)	O
a	O
2	O
(	O
cid:25	O
)	O
(	O
cid:19	O
)	O
(	O
cid:21	O
)	O
1=2	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
1	O
2	O
ntan	O
(	O
cid:19	O
)	O
;	O
(	O
11.14	O
)	O
where	O
a	O
is	O
the	O
inverse	O
of	O
the	O
variance	B
{	O
covariance	B
matrix	I
of	O
the	O
noise	B
,	O
a	O
sym-	O
metric	B
and	O
positive-de	O
(	O
cid:12	O
)	O
nite	O
matrix	B
.	O
(	O
if	O
a	O
is	O
a	O
multiple	O
of	O
the	O
identity	B
matrix	I
,	O
i=	O
(	O
cid:27	O
)	O
2	O
,	O
then	O
the	O
noise	B
is	O
‘	O
white	B
’	O
.	O
for	O
more	O
general	O
a	O
,	O
the	O
noise	B
is	O
‘	O
coloured	B
’	O
.	O
)	O
the	O
probability	O
of	O
the	O
received	O
vector	O
y	O
given	O
that	O
the	O
source	O
signal	O
was	O
s	O
(	O
either	O
zero	O
or	O
one	O
)	O
is	O
then	O
p	O
(	O
y	O
j	O
s	O
)	O
=	O
(	O
cid:20	O
)	O
det	O
(	O
cid:18	O
)	O
a	O
2	O
(	O
cid:25	O
)	O
(	O
cid:19	O
)	O
(	O
cid:21	O
)	O
1=2	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
y	O
(	O
cid:0	O
)	O
xs	O
)	O
ta	O
(	O
y	O
(	O
cid:0	O
)	O
xs	O
)	O
(	O
cid:19	O
)	O
:	O
the	O
optimal	B
detector	O
is	O
based	O
on	O
the	O
posterior	B
probability	I
ratio	O
:	O
p	O
(	O
s	O
=	O
1	O
)	O
p	O
(	O
s	O
=	O
0	O
)	O
p	O
(	O
s	O
=	O
1j	O
y	O
)	O
p	O
(	O
s	O
=	O
0j	O
y	O
)	O
=	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
=	O
exp	O
(	O
yta	O
(	O
x1	O
(	O
cid:0	O
)	O
x0	O
)	O
+	O
(	O
cid:18	O
)	O
)	O
;	O
p	O
(	O
y	O
j	O
s	O
=	O
1	O
)	O
p	O
(	O
y	O
j	O
s	O
=	O
0	O
)	O
(	O
y	O
(	O
cid:0	O
)	O
x1	O
)	O
ta	O
(	O
y	O
(	O
cid:0	O
)	O
x1	O
)	O
+	O
=	O
1	O
2	O
1	O
2	O
(	O
y	O
(	O
cid:0	O
)	O
x0	O
)	O
ta	O
(	O
y	O
(	O
cid:0	O
)	O
x0	O
)	O
+	O
ln	O
(	O
11.15	O
)	O
(	O
11.16	O
)	O
p	O
(	O
s	O
=	O
1	O
)	O
p	O
(	O
s	O
=	O
0	O
)	O
(	O
cid:19	O
)	O
(	O
11.17	O
)	O
where	O
(	O
cid:18	O
)	O
is	O
a	O
constant	O
independent	O
of	O
the	O
received	O
vector	O
y	O
,	O
(	O
cid:18	O
)	O
=	O
(	O
cid:0	O
)	O
1	O
2	O
xt	O
1ax1	O
+	O
1	O
2	O
xt	O
0ax0	O
+	O
ln	O
p	O
(	O
s	O
=	O
1	O
)	O
p	O
(	O
s	O
=	O
0	O
)	O
:	O
(	O
11.18	O
)	O
if	O
the	O
detector	O
is	O
forced	O
to	O
make	O
a	O
decision	O
(	O
i.e.	O
,	O
guess	O
either	O
s	O
=	O
1	O
or	O
s	O
=	O
0	O
)	O
then	O
the	O
decision	O
that	O
minimizes	O
the	O
probability	B
of	I
error	I
is	O
to	O
guess	O
the	O
most	O
prob-	O
able	O
hypothesis	O
.	O
we	O
can	O
write	O
the	O
optimal	B
decision	O
in	O
terms	O
of	O
a	O
discriminant	B
function	I
:	O
with	O
the	O
decisions	O
a	O
(	O
y	O
)	O
(	O
cid:17	O
)	O
yta	O
(	O
x1	O
(	O
cid:0	O
)	O
x0	O
)	O
+	O
(	O
cid:18	O
)	O
a	O
(	O
y	O
)	O
>	O
0	O
!	O
guess	O
s	O
=	O
1	O
a	O
(	O
y	O
)	O
<	O
0	O
!	O
guess	O
s	O
=	O
0	O
a	O
(	O
y	O
)	O
=	O
0	O
!	O
guess	O
either	O
.	O
(	O
11.19	O
)	O
(	O
11.20	O
)	O
notice	O
that	O
a	O
(	O
y	O
)	O
is	O
a	O
linear	B
function	O
of	O
the	O
received	O
vector	O
,	O
a	O
(	O
y	O
)	O
=	O
wty	O
+	O
(	O
cid:18	O
)	O
;	O
(	O
11.21	O
)	O
where	O
w	O
(	O
cid:17	O
)	O
a	O
(	O
x1	O
(	O
cid:0	O
)	O
x0	O
)	O
.	O
11.3	O
capacity	B
of	O
gaussian	O
channel	B
until	O
now	O
we	O
have	O
measured	O
the	O
joint	B
,	O
marginal	B
,	O
and	O
conditional	O
entropy	B
of	O
discrete	O
variables	O
only	O
.	O
in	O
order	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
information	B
conveyed	O
by	O
continuous	O
variables	O
,	O
there	O
are	O
two	O
issues	O
we	O
must	O
address	B
{	O
the	O
in	O
(	O
cid:12	O
)	O
nite	O
length	B
of	O
the	O
real	O
line	O
,	O
and	O
the	O
in	O
(	O
cid:12	O
)	O
nite	O
precision	B
of	O
real	O
numbers	O
.	O
x0	O
x1	O
y	O
figure	O
11.2.	O
two	O
pulses	O
x0	O
and	O
x1	O
,	O
represented	O
as	O
31-dimensional	O
vectors	B
,	O
and	O
a	O
noisy	B
version	O
of	O
one	O
of	O
them	O
,	O
y.	O
w	O
figure	O
11.3.	O
the	O
weight	B
vector	O
w	O
/	O
x1	O
(	O
cid:0	O
)	O
x0	O
that	O
is	O
used	O
to	O
discriminate	O
between	O
x0	O
and	O
x1	O
.	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
180	O
in	O
(	O
cid:12	O
)	O
nite	O
inputs	O
11	O
|	O
error-correcting	B
codes	I
and	O
real	O
channels	O
(	O
a	O
)	O
(	O
b	O
)	O
-	O
(	O
cid:27	O
)	O
g	O
...	O
figure	O
11.4	O
.	O
(	O
a	O
)	O
a	O
probability	B
density	O
p	O
(	O
x	O
)	O
.	O
question	O
:	O
can	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
‘	O
entropy	B
’	O
of	O
this	O
density	B
?	O
(	O
b	O
)	O
we	O
could	O
evaluate	O
the	O
entropies	O
of	O
a	O
sequence	B
of	O
probability	B
distributions	I
with	O
decreasing	O
grain-size	O
g	O
,	O
but	O
these	O
entropies	O
tend	O
to	O
z	O
p	O
(	O
x	O
)	O
log	O
1	O
p	O
(	O
x	O
)	O
g	O
independent	O
of	O
g	O
:	O
the	O
entropy	B
goes	O
up	O
by	O
one	O
bit	B
for	O
every	O
halving	O
of	O
g.	O
dx	O
,	O
which	O
is	O
not	O
z	O
p	O
(	O
x	O
)	O
log	O
integral	B
.	O
1	O
p	O
(	O
x	O
)	O
dx	O
is	O
an	O
illegal	O
how	O
much	O
information	B
can	O
we	O
convey	O
in	O
one	O
use	O
of	O
a	O
gaussian	O
channel	B
?	O
if	O
we	O
are	O
allowed	O
to	O
put	O
any	O
real	O
number	O
x	O
into	O
the	O
gaussian	O
channel	B
,	O
we	O
could	O
communicate	O
an	O
enormous	O
string	O
of	O
n	O
digits	O
d1d2d3	O
:	O
:	O
:	O
dn	O
by	O
setting	O
x	O
=	O
d1d2d3	O
:	O
:	O
:	O
dn	O
000	O
:	O
:	O
:	O
000.	O
the	O
amount	O
of	O
error-free	O
information	B
conveyed	O
in	O
just	O
a	O
single	O
transmission	O
could	O
be	O
made	O
arbitrarily	O
large	O
by	O
increasing	O
n	O
,	O
and	O
the	O
communication	B
could	O
be	O
made	O
arbitrarily	O
reliable	O
by	O
increasing	O
the	O
number	O
of	O
zeroes	O
at	O
the	O
end	O
of	O
x.	O
there	O
is	O
usually	O
some	O
power	B
cost	I
associated	O
with	O
large	O
inputs	O
,	O
however	O
,	O
not	O
to	O
mention	O
practical	B
limits	O
in	O
the	O
dynamic	O
range	O
acceptable	O
to	O
a	O
receiver	O
.	O
it	O
is	O
therefore	O
conventional	O
to	O
introduce	O
a	O
cost	B
function	I
v	O
(	O
x	O
)	O
for	O
every	O
input	O
x	O
,	O
and	O
constrain	O
codes	O
to	O
have	O
an	O
average	B
cost	O
(	O
cid:22	O
)	O
v	O
less	O
than	O
or	O
equal	O
to	O
some	O
maximum	O
value	O
.	O
a	O
generalized	B
channel	O
coding	B
theorem	I
,	O
including	O
a	O
cost	B
function	I
for	O
the	O
inputs	O
,	O
can	O
be	O
proved	O
{	O
see	O
mceliece	O
(	O
1977	O
)	O
.	O
the	O
result	O
is	O
a	O
channel	O
capacity	O
c	O
(	O
(	O
cid:22	O
)	O
v	O
)	O
that	O
is	O
a	O
function	B
of	O
the	O
permitted	O
cost	O
.	O
for	O
the	O
gaussian	O
channel	B
we	O
will	O
assume	O
a	O
cost	O
v	O
(	O
x	O
)	O
=	O
x2	O
(	O
11.22	O
)	O
such	O
that	O
the	O
‘	O
average	B
power	O
’	O
x2	O
of	O
the	O
input	O
is	O
constrained	B
.	O
we	O
motivated	O
this	O
cost	B
function	I
above	O
in	O
the	O
case	O
of	O
real	O
electrical	O
channels	O
in	O
which	O
the	O
physical	O
power	O
consumption	O
is	O
indeed	O
quadratic	O
in	O
x.	O
the	O
constraint	O
x2	O
=	O
(	O
cid:22	O
)	O
v	O
makes	O
it	O
impossible	O
to	O
communicate	O
in	O
(	O
cid:12	O
)	O
nite	O
information	B
in	O
one	O
use	O
of	O
the	O
gaussian	O
channel	B
.	O
in	O
(	O
cid:12	O
)	O
nite	O
precision	B
it	O
is	O
tempting	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
joint	B
,	O
marginal	B
,	O
and	O
conditional	O
entropies	O
for	O
real	O
variables	O
simply	O
by	O
replacing	O
summations	O
by	O
integrals	O
,	O
but	O
this	O
is	O
not	O
a	O
well	O
de	O
(	O
cid:12	O
)	O
ned	O
operation	O
.	O
as	O
we	O
discretize	O
an	O
interval	O
into	O
smaller	O
and	O
smaller	O
divi-	O
sions	O
,	O
the	O
entropy	B
of	O
the	O
discrete	O
distribution	O
diverges	O
(	O
as	O
the	O
logarithm	O
of	O
the	O
granularity	O
)	O
(	O
(	O
cid:12	O
)	O
gure	O
11.4	O
)	O
.	O
also	O
,	O
it	O
is	O
not	O
permissible	O
to	O
take	O
the	O
logarithm	O
of	O
a	O
dimensional	O
quantity	O
such	O
as	O
a	O
probability	B
density	O
p	O
(	O
x	O
)	O
(	O
whose	O
dimensions	B
are	O
[	O
x	O
]	O
(	O
cid:0	O
)	O
1	O
)	O
.	O
there	O
is	O
one	O
information	B
measure	O
,	O
however	O
,	O
that	O
has	O
a	O
well-behaved	O
limit	O
,	O
namely	O
the	O
mutual	B
information	I
{	O
and	O
this	O
is	O
the	O
one	O
that	O
really	O
matters	O
,	O
since	O
it	O
measures	O
how	O
much	O
information	O
one	O
variable	O
conveys	O
about	O
another	O
.	O
in	O
the	O
discrete	O
case	O
,	O
i	O
(	O
x	O
;	O
y	O
)	O
=xx	O
;	O
y	O
p	O
(	O
x	O
;	O
y	O
)	O
log	O
p	O
(	O
x	O
;	O
y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
:	O
(	O
11.23	O
)	O
now	O
because	O
the	O
argument	O
of	O
the	O
log	O
is	O
a	O
ratio	O
of	O
two	O
probabilities	O
over	O
the	O
same	O
space	O
,	O
it	O
is	O
ok	O
to	O
have	O
p	O
(	O
x	O
;	O
y	O
)	O
,	O
p	O
(	O
x	O
)	O
and	O
p	O
(	O
y	O
)	O
be	O
probability	B
densities	O
and	O
replace	O
the	O
sum	O
by	O
an	O
integral	B
:	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
z	O
dx	O
dy	O
p	O
(	O
x	O
;	O
y	O
)	O
log	O
p	O
(	O
x	O
;	O
y	O
)	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
=	O
z	O
dx	O
dy	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
j	O
x	O
)	O
log	O
p	O
(	O
y	O
j	O
x	O
)	O
p	O
(	O
y	O
)	O
:	O
(	O
11.24	O
)	O
(	O
11.25	O
)	O
we	O
can	O
now	O
ask	O
these	O
questions	O
for	O
the	O
gaussian	O
channel	B
:	O
(	O
a	O
)	O
what	O
probability	O
distribution	B
p	O
(	O
x	O
)	O
maximizes	O
the	O
mutual	B
information	I
(	O
subject	O
to	O
the	O
constraint	O
x2	O
=	O
v	O
)	O
?	O
and	O
(	O
b	O
)	O
does	O
the	O
maximal	O
mutual	B
information	I
still	O
measure	O
the	O
maximum	O
error-free	O
communication	B
rate	O
of	O
this	O
real	O
channel	B
,	O
as	O
it	O
did	O
for	O
the	O
discrete	O
channel	O
?	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
11.3	O
:	O
capacity	B
of	O
gaussian	O
channel	B
181	O
exercise	O
11.1	O
.	O
[	O
3	O
,	O
p.189	O
]	O
prove	O
that	O
the	O
probability	B
distribution	O
p	O
(	O
x	O
)	O
that	O
max-	O
imizes	O
the	O
mutual	B
information	I
(	O
subject	O
to	O
the	O
constraint	O
x2	O
=	O
v	O
)	O
is	O
a	O
gaussian	O
distribution	B
of	O
mean	B
zero	O
and	O
variance	O
v.	O
.	O
exercise	O
11.2	O
.	O
[	O
2	O
,	O
p.189	O
]	O
show	O
that	O
the	O
mutual	B
information	I
i	O
(	O
x	O
;	O
y	O
)	O
,	O
in	O
the	O
case	O
of	O
this	O
optimized	O
distribution	B
,	O
is	O
c	O
=	O
1	O
2	O
log	O
(	O
cid:16	O
)	O
1	O
+	O
v	O
(	O
cid:27	O
)	O
2	O
(	O
cid:17	O
)	O
:	O
(	O
11.26	O
)	O
this	O
is	O
an	O
important	O
result	O
.	O
we	O
see	O
that	O
the	O
capacity	B
of	O
the	O
gaussian	O
channel	B
is	O
a	O
function	B
of	O
the	O
signal-to-noise	B
ratio	I
v=	O
(	O
cid:27	O
)	O
2.	O
inferences	O
given	O
a	O
gaussian	O
input	O
distribution	O
if	O
p	O
(	O
x	O
)	O
=	O
normal	B
(	O
x	O
;	O
0	O
;	O
v	O
)	O
and	O
p	O
(	O
y	O
j	O
x	O
)	O
=	O
normal	B
(	O
y	O
;	O
x	O
;	O
(	O
cid:27	O
)	O
2	O
)	O
then	O
the	O
marginal	B
distribution	O
of	O
y	O
is	O
p	O
(	O
y	O
)	O
=	O
normal	B
(	O
y	O
;	O
0	O
;	O
v+	O
(	O
cid:27	O
)	O
2	O
)	O
and	O
the	O
posterior	O
distribution	O
of	O
the	O
input	O
,	O
given	O
that	O
the	O
output	O
is	O
y	O
,	O
is	O
:	O
p	O
(	O
xj	O
y	O
)	O
/	O
p	O
(	O
y	O
j	O
x	O
)	O
p	O
(	O
x	O
)	O
/	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
y	O
(	O
cid:0	O
)	O
x	O
)	O
2=2	O
(	O
cid:27	O
)	O
2	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
x2=2v	O
)	O
=	O
normal	B
x	O
;	O
v	O
+	O
(	O
cid:27	O
)	O
2	O
y	O
;	O
(	O
cid:18	O
)	O
1	O
+	O
v	O
1	O
(	O
cid:27	O
)	O
2	O
(	O
cid:19	O
)	O
(	O
cid:0	O
)	O
1	O
!	O
:	O
v	O
(	O
11.27	O
)	O
(	O
11.28	O
)	O
(	O
11.29	O
)	O
[	O
the	O
step	O
from	O
(	O
11.28	O
)	O
to	O
(	O
11.29	O
)	O
is	O
made	O
by	O
completing	O
the	O
square	B
in	O
the	O
exponent	O
.	O
]	O
this	O
formula	O
deserves	O
careful	O
study	O
.	O
the	O
mean	B
of	O
the	O
posterior	O
distribution	O
,	O
v+	O
(	O
cid:27	O
)	O
2	O
y	O
,	O
can	O
be	O
viewed	O
as	O
a	O
weighted	O
combination	B
of	O
the	O
value	O
that	O
best	O
(	O
cid:12	O
)	O
ts	O
the	O
output	O
,	O
x	O
=	O
y	O
,	O
and	O
the	O
value	O
that	O
best	O
(	O
cid:12	O
)	O
ts	O
the	O
prior	B
,	O
x	O
=	O
0	O
:	O
v	O
v	O
v	O
+	O
(	O
cid:27	O
)	O
2	O
y	O
=	O
1=	O
(	O
cid:27	O
)	O
2	O
1=v	O
+	O
1=	O
(	O
cid:27	O
)	O
2	O
y	O
+	O
1=v	O
1=v	O
+	O
1=	O
(	O
cid:27	O
)	O
2	O
0	O
:	O
(	O
11.30	O
)	O
the	O
weights	O
1=	O
(	O
cid:27	O
)	O
2	O
and	O
1=v	O
are	O
the	O
precisions	O
of	O
the	O
two	O
gaussians	O
that	O
we	O
multiplied	O
together	O
in	O
equation	O
(	O
11.28	O
)	O
:	O
the	O
prior	B
and	O
the	O
likelihood	B
.	O
the	O
precision	B
of	O
the	O
posterior	O
distribution	O
is	O
the	O
sum	O
of	O
these	O
two	O
pre-	O
cisions	O
.	O
this	O
is	O
a	O
general	O
property	O
:	O
whenever	O
two	O
independent	O
sources	O
con-	O
tribute	O
information	B
,	O
via	O
gaussian	O
distributions	O
,	O
about	O
an	O
unknown	O
variable	O
,	O
the	O
precisions	B
add	I
.	O
[	O
this	O
is	O
the	O
dual	B
to	O
the	O
better-known	O
relationship	O
‘	O
when	O
independent	O
variables	O
are	O
added	O
,	O
their	O
variances	B
add	I
’	O
.	O
]	O
noisy-channel	B
coding	I
theorem	I
for	O
the	O
gaussian	O
channel	B
we	O
have	O
evaluated	O
a	O
maximal	O
mutual	B
information	I
.	O
does	O
it	O
correspond	O
to	O
a	O
maximum	O
possible	O
rate	B
of	O
error-free	O
information	B
transmission	O
?	O
one	O
way	O
of	O
proving	O
that	O
this	O
is	O
so	O
is	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
sequence	B
of	O
discrete	O
channels	O
,	O
all	O
derived	O
from	O
the	O
gaussian	O
channel	B
,	O
with	O
increasing	O
numbers	O
of	O
inputs	O
and	O
outputs	O
,	O
and	O
prove	O
that	O
the	O
maximum	O
mutual	O
information	B
of	O
these	O
channels	O
tends	O
to	O
the	O
asserted	O
c.	O
the	O
noisy-channel	B
coding	I
theorem	I
for	O
discrete	O
channels	O
applies	O
to	O
each	O
of	O
these	O
derived	O
channels	O
,	O
thus	O
we	O
obtain	O
a	O
coding	B
theorem	I
for	O
the	O
continuous	B
channel	I
.	O
alternatively	O
,	O
we	O
can	O
make	O
an	O
intuitive	O
argument	O
for	O
the	O
coding	B
theorem	I
speci	O
(	O
cid:12	O
)	O
c	O
for	O
the	O
gaussian	O
channel	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
182	O
11	O
|	O
error-correcting	B
codes	I
and	O
real	O
channels	O
geometrical	O
view	O
of	O
the	O
noisy-channel	O
coding	B
theorem	I
:	O
sphere	B
packing	I
consider	O
a	O
sequence	B
x	O
=	O
(	O
x1	O
;	O
:	O
:	O
:	O
;	O
xn	O
)	O
of	O
inputs	O
,	O
and	O
the	O
corresponding	O
output	O
y	O
,	O
as	O
de	O
(	O
cid:12	O
)	O
ning	O
two	O
points	O
in	O
an	O
n	O
dimensional	O
space	O
.	O
for	O
large	O
n	O
,	O
the	O
noise	B
power	O
is	O
very	O
likely	O
to	O
be	O
close	O
(	O
fractionally	O
)	O
to	O
n	O
(	O
cid:27	O
)	O
2.	O
the	O
output	O
y	O
is	O
therefore	O
very	O
likely	O
to	O
be	O
close	O
to	O
the	O
surface	O
of	O
a	O
sphere	O
of	O
radius	O
pn	O
(	O
cid:27	O
)	O
2	O
centred	O
on	O
x.	O
similarly	O
,	O
if	O
the	O
original	O
signal	O
x	O
is	O
generated	O
at	O
random	B
subject	O
to	O
an	O
average	B
power	O
constraint	O
x2	O
=	O
v	O
,	O
then	O
x	O
is	O
likely	O
to	O
lie	O
close	O
to	O
a	O
sphere	O
,	O
centred	O
on	O
the	O
origin	O
,	O
of	O
radius	O
pn	O
v	O
;	O
and	O
because	O
the	O
total	O
average	B
power	O
of	O
y	O
is	O
v	O
+	O
(	O
cid:27	O
)	O
2	O
,	O
the	O
received	O
signal	O
y	O
is	O
likely	O
to	O
lie	O
on	O
the	O
surface	O
of	O
a	O
sphere	O
of	O
radius	O
pn	O
(	O
v	O
+	O
(	O
cid:27	O
)	O
2	O
)	O
,	O
centred	O
on	O
the	O
origin	O
.	O
the	O
volume	B
of	O
an	O
n	O
-dimensional	O
sphere	O
of	O
radius	O
r	O
is	O
v	O
(	O
r	O
;	O
n	O
)	O
=	O
(	O
cid:25	O
)	O
n=2	O
(	O
cid:0	O
)	O
(	O
n=2+1	O
)	O
rn	O
:	O
(	O
11.31	O
)	O
now	O
consider	O
making	O
a	O
communication	B
system	O
based	O
on	O
non-confusable	O
inputs	O
x	O
,	O
that	O
is	O
,	O
inputs	O
whose	O
spheres	O
do	O
not	O
overlap	O
signi	O
(	O
cid:12	O
)	O
cantly	O
.	O
the	O
max-	O
imum	O
number	O
s	O
of	O
non-confusable	O
inputs	O
is	O
given	O
by	O
dividing	O
the	O
volume	B
of	O
the	O
sphere	O
of	O
probable	O
ys	O
by	O
the	O
volume	B
of	O
the	O
sphere	O
for	O
y	O
given	O
x	O
:	O
s	O
(	O
cid:20	O
)	O
pn	O
(	O
v	O
+	O
(	O
cid:27	O
)	O
2	O
)	O
pn	O
(	O
cid:27	O
)	O
2	O
!	O
n	O
thus	O
the	O
capacity	B
is	O
bounded	O
by	O
:	O
c	O
=	O
1	O
n	O
log	O
m	O
(	O
cid:20	O
)	O
1	O
2	O
log	O
(	O
cid:16	O
)	O
1	O
+	O
(	O
11.32	O
)	O
(	O
11.33	O
)	O
v	O
(	O
cid:27	O
)	O
2	O
(	O
cid:17	O
)	O
:	O
a	O
more	O
detailed	O
argument	O
like	O
the	O
one	O
used	O
in	O
the	O
previous	O
chapter	O
can	O
es-	O
tablish	O
equality	O
.	O
back	O
to	O
the	O
continuous	B
channel	I
recall	O
that	O
the	O
use	O
of	O
a	O
real	O
continuous	B
channel	I
with	O
bandwidth	B
w	O
,	O
noise	B
spectral	O
density	B
n0	O
and	O
power	O
p	O
is	O
equivalent	O
to	O
n=t	O
=	O
2w	O
uses	O
per	O
second	O
of	O
a	O
gaussian	O
channel	O
with	O
(	O
cid:27	O
)	O
2	O
=	O
n0=2	O
and	O
subject	O
to	O
the	O
constraint	O
x2	O
n	O
(	O
cid:20	O
)	O
p=2w	O
.	O
substituting	O
the	O
result	O
for	O
the	O
capacity	B
of	O
the	O
gaussian	O
channel	B
,	O
we	O
(	O
cid:12	O
)	O
nd	O
the	O
capacity	B
of	O
the	O
continuous	B
channel	I
to	O
be	O
:	O
c	O
=	O
w	O
log	O
(	O
cid:18	O
)	O
1	O
+	O
p	O
n0w	O
(	O
cid:19	O
)	O
bits	O
per	O
second	O
.	O
(	O
11.34	O
)	O
this	O
formula	O
gives	O
insight	O
into	O
the	O
tradeo	O
(	O
cid:11	O
)	O
s	O
of	O
practical	O
communication	B
.	O
imag-	O
ine	O
that	O
we	O
have	O
a	O
(	O
cid:12	O
)	O
xed	O
power	O
constraint	O
.	O
what	O
is	O
the	O
best	O
bandwidth	B
to	O
make	O
use	O
of	O
that	O
power	O
?	O
introducing	O
w0	O
=	O
p=n0	O
,	O
i.e.	O
,	O
the	O
bandwidth	B
for	O
which	O
the	O
signal-to-noise	B
ratio	I
is	O
1	O
,	O
(	O
cid:12	O
)	O
gure	O
11.5	O
shows	O
c=w0	O
=	O
w=w0	O
log	O
(	O
1	O
+	O
w0=w	O
)	O
as	O
a	O
function	B
of	O
w=w0	O
.	O
the	O
capacity	B
increases	O
to	O
an	O
asymptote	O
of	O
w0	O
log	O
e.	O
it	O
is	O
dramatically	O
better	O
(	O
in	O
terms	O
of	O
capacity	O
for	O
(	O
cid:12	O
)	O
xed	O
power	O
)	O
to	O
transmit	O
at	O
a	O
low	O
signal-to-noise	B
ratio	I
over	O
a	O
large	O
bandwidth	B
,	O
than	O
with	O
high	O
signal-to-noise	O
in	O
a	O
narrow	O
bandwidth	B
;	O
this	O
is	O
one	O
motivation	O
for	O
wideband	O
communication	B
methods	O
such	O
as	O
the	O
‘	O
direct	O
sequence	B
spread-spectrum	O
’	O
approach	O
used	O
in	O
3g	O
mobile	O
phones	O
.	O
of	O
course	O
,	O
you	O
are	O
not	O
alone	O
,	O
and	O
your	O
electromagnetic	O
neigh-	O
bours	O
may	O
not	O
be	O
pleased	O
if	O
you	O
use	O
a	O
large	O
bandwidth	B
,	O
so	O
for	O
social	O
reasons	O
,	O
engineers	O
often	O
have	O
to	O
make	O
do	O
with	O
higher-power	O
,	O
narrow-bandwidth	O
trans-	O
mitters	O
.	O
y	O
t	O
i	O
c	O
a	O
p	O
a	O
c	O
1.4	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0	O
1	O
3	O
2	O
4	O
bandwidth	B
5	O
6	O
figure	O
11.5.	O
capacity	B
versus	O
bandwidth	B
for	O
a	O
real	O
channel	B
:	O
c=w0	O
=	O
w=w0	O
log	O
(	O
1	O
+	O
w0=w	O
)	O
as	O
a	O
function	B
of	O
w=w0	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
11.4	O
:	O
what	O
are	O
the	O
capabilities	O
of	O
practical	O
error-correcting	B
codes	I
?	O
183	O
11.4	O
what	O
are	O
the	O
capabilities	O
of	O
practical	O
error-correcting	B
codes	I
?	O
nearly	O
all	O
codes	O
are	O
good	B
,	O
but	O
nearly	O
all	O
codes	O
require	O
exponential	B
look-up	O
tables	O
for	O
practical	O
implementation	O
of	O
the	O
encoder	O
and	O
decoder	O
{	O
exponential	B
in	O
the	O
blocklength	O
n	O
.	O
and	O
the	O
coding	B
theorem	I
required	O
n	O
to	O
be	O
large	O
.	O
by	O
a	O
practical	B
error-correcting	O
code	B
,	O
we	O
mean	B
one	O
that	O
can	O
be	O
encoded	O
and	O
decoded	O
in	O
a	O
reasonable	O
amount	O
of	O
time	O
,	O
for	O
example	O
,	O
a	O
time	O
that	O
scales	O
as	O
a	O
polynomial	O
function	B
of	O
the	O
blocklength	O
n	O
{	O
preferably	O
linearly	O
.	O
the	O
shannon	O
limit	O
is	O
not	O
achieved	O
in	O
practice	O
the	O
non-constructive	O
proof	O
of	O
the	O
noisy-channel	O
coding	B
theorem	I
showed	O
that	O
good	B
block	O
codes	O
exist	O
for	O
any	O
noisy	B
channel	I
,	O
and	O
indeed	O
that	O
nearly	O
all	O
block	B
codes	O
are	O
good	B
.	O
but	O
writing	B
down	O
an	O
explicit	O
and	O
practical	O
encoder	B
and	O
de-	O
coder	O
that	O
are	O
as	O
good	O
as	O
promised	O
by	O
shannon	O
is	O
still	O
an	O
unsolved	O
problem	O
.	O
very	B
good	I
codes	O
.	O
given	O
a	O
channel	B
,	O
a	O
family	O
of	O
block	O
codes	O
that	O
achieve	O
arbitrarily	O
small	O
probability	B
of	I
error	I
at	O
any	O
communication	B
rate	O
up	O
to	O
the	O
capacity	B
of	O
the	O
channel	B
are	O
called	O
‘	O
very	B
good	I
’	O
codes	O
for	O
that	O
channel	B
.	O
good	B
codes	O
are	O
code	B
families	O
that	O
achieve	O
arbitrarily	O
small	O
probability	B
of	I
error	I
at	O
non-zero	O
communication	B
rates	O
up	O
to	O
some	O
maximum	O
rate	O
that	O
may	O
be	O
less	O
than	O
the	O
capacity	B
of	O
the	O
given	O
channel	B
.	O
bad	B
codes	O
are	O
code	B
families	O
that	O
can	O
not	O
achieve	O
arbitrarily	O
small	O
probability	B
of	I
error	I
,	O
or	O
that	O
can	O
achieve	O
arbitrarily	O
small	O
probability	B
of	I
error	I
only	O
by	O
decreasing	O
the	O
information	B
rate	O
to	O
zero	O
.	O
repetition	B
codes	O
are	O
an	O
example	O
of	O
a	O
bad	B
code	O
family	O
.	O
(	O
bad	B
codes	O
are	O
not	O
necessarily	O
useless	O
for	O
practical	O
purposes	O
.	O
)	O
practical	B
codes	O
are	O
code	B
families	O
that	O
can	O
be	O
encoded	O
and	O
decoded	O
in	O
time	O
and	O
space	O
polynomial	O
in	O
the	O
blocklength	O
.	O
most	O
established	O
codes	O
are	O
linear	B
codes	I
let	O
us	O
review	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
a	O
block	B
code	I
,	O
and	O
then	O
add	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
a	O
linear	B
block	I
code	I
.	O
an	O
(	O
n	O
;	O
k	O
)	O
block	B
code	I
for	O
a	O
channel	B
q	O
is	O
a	O
list	O
of	O
s	O
=	O
2k	O
codewords	O
fx	O
(	O
1	O
)	O
;	O
x	O
(	O
2	O
)	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
2k	O
)	O
g	O
,	O
each	O
of	O
length	O
n	O
:	O
x	O
(	O
s	O
)	O
2	O
an	O
x.	O
the	O
signal	O
to	O
be	O
encoded	O
,	O
s	O
,	O
which	O
comes	O
from	O
an	O
alphabet	O
of	O
size	O
2k	O
,	O
is	O
encoded	O
as	O
x	O
(	O
s	O
)	O
.	O
a	O
linear	B
(	O
n	O
;	O
k	O
)	O
block	B
code	I
is	O
a	O
block	B
code	I
in	O
which	O
the	O
codewords	O
fx	O
(	O
s	O
)	O
g	O
make	O
up	O
a	O
k-dimensional	O
subspace	O
of	O
an	O
x.	O
the	O
encoding	O
operation	O
can	O
be	O
represented	O
by	O
an	O
n	O
(	O
cid:2	O
)	O
k	O
binary	O
matrix	O
gt	O
such	O
that	O
if	O
the	O
signal	O
to	O
be	O
encoded	O
,	O
in	O
binary	O
notation	B
,	O
is	O
s	O
(	O
a	O
vector	O
of	O
length	B
k	O
bits	O
)	O
,	O
then	O
the	O
encoded	O
signal	O
is	O
t	O
=	O
gts	O
modulo	O
2.	O
the	O
codewords	O
ftg	O
can	O
be	O
de	O
(	O
cid:12	O
)	O
ned	O
as	O
the	O
set	B
of	O
vectors	B
satisfying	O
ht	O
=	O
0	O
mod	O
2	O
,	O
where	O
h	O
is	O
the	O
parity-check	B
matrix	I
of	O
the	O
code	B
.	O
for	O
example	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
of	O
section	B
1.2	O
takes	O
k	O
=	O
4	O
signal	O
bits	O
,	O
s	O
,	O
and	O
transmits	O
them	O
followed	O
by	O
three	O
parity-check	B
bits	I
.	O
the	O
n	O
=	O
7	O
transmitted	O
symbols	O
are	O
given	O
by	O
gts	O
mod	O
2.	O
coding	B
theory	I
was	O
born	O
with	O
the	O
work	O
of	O
hamming	O
,	O
who	O
invented	O
a	O
fam-	O
ily	O
of	O
practical	O
error-correcting	B
codes	I
,	O
each	O
able	O
to	O
correct	O
one	O
error	O
in	O
a	O
block	B
of	O
length	B
n	O
,	O
of	O
which	O
the	O
repetition	B
code	I
r3	O
and	O
the	O
(	O
7	O
;	O
4	O
)	O
code	B
are	O
gt	O
=2	O
6664	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
1	O
1	O
(	O
cid:1	O
)	O
1	O
1	O
3	O
7775	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
184	O
11	O
|	O
error-correcting	B
codes	I
and	O
real	O
channels	O
the	O
simplest	O
.	O
since	O
then	O
most	O
established	O
codes	O
have	O
been	O
generalizations	O
of	O
hamming	O
’	O
s	O
codes	O
:	O
bose	O
{	O
chaudhury	O
{	O
hocquenhem	O
codes	O
,	O
reed	O
{	O
m	O
(	O
cid:127	O
)	O
uller	O
codes	O
,	O
reed	O
{	O
solomon	O
codes	O
,	O
and	O
goppa	O
codes	O
,	O
to	O
name	O
a	O
few	O
.	O
convolutional	B
codes	O
another	O
family	O
of	O
linear	O
codes	O
are	O
convolutional	B
codes	O
,	O
which	O
do	O
not	O
divide	O
the	O
source	O
stream	O
into	O
blocks	O
,	O
but	O
instead	O
read	O
and	O
transmit	O
bits	O
continuously	O
.	O
the	O
transmitted	O
bits	O
are	O
a	O
linear	B
function	O
of	O
the	O
past	O
source	O
bits	O
.	O
usually	O
the	O
rule	O
for	O
generating	O
the	O
transmitted	O
bits	O
involves	O
feeding	O
the	O
present	O
source	O
bit	O
into	O
a	O
linear-feedback	B
shift-register	I
of	O
length	B
k	O
,	O
and	O
transmitting	O
one	O
or	O
more	O
linear	B
functions	O
of	O
the	O
state	O
of	O
the	O
shift	O
register	O
at	O
each	O
iteration	O
.	O
the	O
resulting	O
transmitted	O
bit	B
stream	O
is	O
the	O
convolution	B
of	O
the	O
source	O
stream	O
with	O
a	O
linear	B
(	O
cid:12	O
)	O
lter	O
.	O
the	O
impulse-response	O
function	B
of	O
this	O
(	O
cid:12	O
)	O
lter	O
may	O
have	O
(	O
cid:12	O
)	O
nite	O
or	O
in	O
(	O
cid:12	O
)	O
nite	O
duration	O
,	O
depending	O
on	O
the	O
choice	O
of	O
feedback	O
shift-register	O
.	O
we	O
will	O
discuss	O
convolutional	B
codes	O
in	O
chapter	O
48.	O
are	O
linear	B
codes	I
‘	O
good	B
’	O
?	O
one	O
might	O
ask	O
,	O
is	O
the	O
reason	O
that	O
the	O
shannon	O
limit	O
is	O
not	O
achieved	O
in	O
practice	O
because	O
linear	B
codes	I
are	O
inherently	O
not	O
as	O
good	O
as	O
random	O
codes	O
?	O
the	O
answer	O
is	O
no	O
,	O
the	O
noisy-channel	B
coding	I
theorem	I
can	O
still	O
be	O
proved	O
for	O
linear	O
codes	O
,	O
at	O
least	O
for	O
some	O
channels	O
(	O
see	O
chapter	O
14	O
)	O
,	O
though	O
the	O
proofs	O
,	O
like	O
shannon	O
’	O
s	O
proof	O
for	O
random	O
codes	O
,	O
are	O
non-constructive	O
.	O
linear	B
codes	I
are	O
easy	O
to	O
implement	O
at	O
the	O
encoding	O
end	O
.	O
is	O
decoding	B
a	O
linear	B
code	O
also	O
easy	O
?	O
not	O
necessarily	O
.	O
the	O
general	O
decoding	O
problem	O
(	O
(	O
cid:12	O
)	O
nd	O
the	O
maximum	B
likelihood	I
s	O
in	O
the	O
equation	O
gts	O
+	O
n	O
=	O
r	O
)	O
is	O
in	O
fact	O
np-complete	O
(	O
berlekamp	O
et	O
al.	O
,	O
1978	O
)	O
.	O
[	O
np-complete	O
problems	O
are	O
computational	O
problems	O
that	O
are	O
all	O
equally	O
di	O
(	O
cid:14	O
)	O
cult	O
and	O
which	O
are	O
widely	O
believed	O
to	O
require	O
expo-	O
nential	O
computer	B
time	O
to	O
solve	O
in	O
general	O
.	O
]	O
so	O
attention	O
focuses	O
on	O
families	O
of	O
codes	O
for	O
which	O
there	O
is	O
a	O
fast	O
decoding	O
algorithm	B
.	O
concatenation	B
one	O
trick	O
for	O
building	O
codes	O
with	O
practical	O
decoders	O
is	O
the	O
idea	O
of	O
concatena-	O
tion	O
.	O
an	O
encoder	B
{	O
channel	B
{	O
decoder	B
system	O
c	O
!	O
q	O
!	O
d	O
can	O
be	O
viewed	O
as	O
de	O
(	O
cid:12	O
)	O
ning	O
a	O
super-channel	B
q0	O
with	O
a	O
smaller	O
probability	B
of	I
error	I
,	O
and	O
with	O
complex	B
correlations	O
among	O
its	O
errors	B
.	O
we	O
can	O
create	O
an	O
encoder	B
c0	O
and	O
decoder	O
d0	O
for	O
this	O
super-channel	B
q0	O
.	O
the	O
code	B
consisting	O
of	O
the	O
outer	O
code	B
c0	O
followed	O
by	O
the	O
inner	B
code	I
c	O
is	O
known	O
as	O
a	O
concatenated	B
code	O
.	O
some	O
concatenated	B
codes	O
make	O
use	O
of	O
the	O
idea	O
of	O
interleaving	O
.	O
we	O
read	O
the	O
data	O
in	O
blocks	O
,	O
the	O
size	O
of	O
each	O
block	B
being	O
larger	O
than	O
the	O
blocklengths	O
of	O
the	O
constituent	O
codes	O
c	O
and	O
c0	O
.	O
after	O
encoding	O
the	O
data	O
of	O
one	O
block	B
using	O
code	B
c0	O
,	O
the	O
bits	O
are	O
reordered	O
within	O
the	O
block	B
in	O
such	O
a	O
way	O
that	O
nearby	O
bits	O
are	O
separated	O
from	O
each	O
other	O
once	O
the	O
block	B
is	O
fed	O
to	O
the	O
second	O
code	B
c.	O
a	O
simple	O
example	O
of	O
an	O
interleaver	O
is	O
a	O
rectangular	B
code	I
or	O
product	B
code	I
in	O
which	O
the	O
data	O
are	O
arranged	O
in	O
a	O
k2	O
(	O
cid:2	O
)	O
k1	O
block	B
,	O
and	O
encoded	O
horizontally	O
using	O
an	O
(	O
n1	O
;	O
k1	O
)	O
linear	B
code	O
,	O
then	O
vertically	O
using	O
a	O
(	O
n2	O
;	O
k2	O
)	O
linear	B
code	O
.	O
.	O
exercise	O
11.3	O
.	O
[	O
3	O
]	O
show	O
that	O
either	O
of	O
the	O
two	O
codes	O
can	O
be	O
viewed	O
as	O
the	O
inner	B
code	I
or	O
the	O
outer	B
code	I
.	O
as	O
an	O
example	O
,	O
(	O
cid:12	O
)	O
gure	O
11.6	O
shows	O
a	O
product	B
code	I
in	O
which	O
we	O
encode	O
(	O
cid:12	O
)	O
rst	O
with	O
the	O
repetition	B
code	I
r3	O
(	O
also	O
known	O
as	O
the	O
hamming	O
code	B
h	O
(	O
3	O
;	O
1	O
)	O
)	O
c0	O
!	O
c	O
!	O
q	O
!	O
d	O
}	O
{	O
z	O
q0	O
|	O
!	O
d0	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
11.4	O
:	O
what	O
are	O
the	O
capabilities	O
of	O
practical	O
error-correcting	B
codes	I
?	O
185	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
(	O
a	O
)	O
?	O
?	O
?	O
?	O
?	O
(	O
b	O
)	O
(	O
c	O
)	O
1	O
1	O
1	O
1	O
0	O
1	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
1	O
0	O
1	O
1	O
1	O
1	O
1	O
0	O
0	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
(	O
d	O
)	O
(	O
d0	O
)	O
(	O
e	O
)	O
(	O
e0	O
)	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
(	O
1	O
)	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
(	O
1	O
)	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
(	O
1	O
)	O
1	O
1	O
0	O
0	O
1	O
figure	O
11.6.	O
a	O
product	B
code	I
.	O
(	O
a	O
)	O
a	O
string	O
1011	O
encoded	O
using	O
a	O
concatenated	B
code	O
consisting	O
of	O
two	O
hamming	O
codes	O
,	O
h	O
(	O
3	O
;	O
1	O
)	O
and	O
h	O
(	O
7	O
;	O
4	O
)	O
.	O
(	O
b	O
)	O
a	O
noise	B
pattern	O
that	O
(	O
cid:13	O
)	O
ips	O
5	O
bits	O
.	O
(	O
c	O
)	O
the	O
received	O
vector	O
.	O
(	O
d	O
)	O
after	O
decoding	B
using	O
the	O
horizontal	O
(	O
3	O
;	O
1	O
)	O
decoder	B
,	O
and	O
(	O
e	O
)	O
after	O
subsequently	O
using	O
the	O
vertical	O
(	O
7	O
;	O
4	O
)	O
decoder	B
.	O
the	O
decoded	O
vector	O
matches	O
the	O
original	O
.	O
(	O
d0	O
,	O
e0	O
)	O
after	O
decoding	B
in	O
the	O
other	O
order	O
,	O
three	O
errors	O
still	O
remain	O
.	O
horizontally	O
then	O
with	O
h	O
(	O
7	O
;	O
4	O
)	O
vertically	O
.	O
the	O
blocklength	O
of	O
the	O
concatenated	O
code	B
is	O
27.	O
the	O
number	O
of	O
source	O
bits	O
per	O
codeword	B
is	O
four	O
,	O
shown	O
by	O
the	O
small	O
rectangle	O
.	O
we	O
can	O
decode	O
conveniently	O
(	O
though	O
not	O
optimally	O
)	O
by	O
using	O
the	O
individual	O
decoders	O
for	O
each	O
of	O
the	O
subcodes	O
in	O
some	O
sequence	B
.	O
it	O
makes	O
most	O
sense	O
to	O
(	O
cid:12	O
)	O
rst	O
decode	O
the	O
code	B
which	O
has	O
the	O
lowest	O
rate	B
and	O
hence	O
the	O
greatest	O
error-	O
correcting	O
ability	O
.	O
figure	O
11.6	O
(	O
c	O
{	O
e	O
)	O
shows	O
what	O
happens	O
if	O
we	O
receive	O
the	O
codeword	B
of	O
(	O
cid:12	O
)	O
g-	O
ure	O
11.6a	O
with	O
some	O
errors	B
(	O
(	O
cid:12	O
)	O
ve	O
bits	O
(	O
cid:13	O
)	O
ipped	O
,	O
as	O
shown	O
)	O
and	O
apply	O
the	O
decoder	B
for	O
h	O
(	O
3	O
;	O
1	O
)	O
(	O
cid:12	O
)	O
rst	O
,	O
and	O
then	O
the	O
decoder	B
for	O
h	O
(	O
7	O
;	O
4	O
)	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
decoder	B
corrects	O
three	O
of	O
the	O
errors	B
,	O
but	O
erroneously	O
modi	O
(	O
cid:12	O
)	O
es	O
the	O
third	O
bit	O
in	O
the	O
second	O
row	O
where	O
there	O
are	O
two	O
bit	B
errors	O
.	O
the	O
(	O
7	O
;	O
4	O
)	O
decoder	B
can	O
then	O
correct	O
all	O
three	O
of	O
these	O
errors	B
.	O
figure	O
11.6	O
(	O
d0	O
{	O
e0	O
)	O
shows	O
what	O
happens	O
if	O
we	O
decode	O
the	O
two	O
codes	O
in	O
the	O
other	O
order	O
.	O
in	O
columns	O
one	O
and	O
two	O
there	O
are	O
two	O
errors	B
,	O
so	O
the	O
(	O
7	O
;	O
4	O
)	O
decoder	B
introduces	O
two	O
extra	O
errors	O
.	O
it	O
corrects	O
the	O
one	O
error	O
in	O
column	O
3.	O
the	O
(	O
3	O
;	O
1	O
)	O
decoder	B
then	O
cleans	O
up	O
four	O
of	O
the	O
errors	O
,	O
but	O
erroneously	O
infers	O
the	O
second	O
bit	B
.	O
interleaving	B
the	O
motivation	O
for	O
interleaving	O
is	O
that	O
by	O
spreading	O
out	O
bits	O
that	O
are	O
nearby	O
in	O
one	O
code	B
,	O
we	O
make	O
it	O
possible	O
to	O
ignore	O
the	O
complex	B
correlations	O
among	O
the	O
errors	B
that	O
are	O
produced	O
by	O
the	O
inner	B
code	I
.	O
maybe	O
the	O
inner	B
code	I
will	O
mess	O
up	O
an	O
entire	O
codeword	B
;	O
but	O
that	O
codeword	B
is	O
spread	O
out	O
one	O
bit	B
at	O
a	O
time	O
over	O
several	O
codewords	O
of	O
the	O
outer	O
code	B
.	O
so	O
we	O
can	O
treat	O
the	O
errors	B
introduced	O
by	O
the	O
inner	B
code	I
as	O
if	O
they	O
are	O
independent	O
.	O
other	O
channel	B
models	O
in	O
addition	O
to	O
the	O
binary	B
symmetric	I
channel	I
and	O
the	O
gaussian	O
channel	B
,	O
coding	O
theorists	O
keep	O
more	O
complex	B
channels	O
in	O
mind	O
also	O
.	O
burst-error	O
channels	O
are	O
important	O
models	O
in	O
practice	O
.	O
reed	O
{	O
solomon	O
codes	O
use	O
galois	O
(	O
cid:12	O
)	O
elds	O
(	O
see	O
appendix	O
c.1	O
)	O
with	O
large	O
numbers	O
of	O
elements	O
(	O
e.g	O
.	O
216	O
)	O
as	O
their	O
input	O
alphabets	O
,	O
and	O
thereby	O
automatically	O
achieve	O
a	O
degree	B
of	O
burst-error	O
tolerance	O
in	O
that	O
even	O
if	O
17	O
successive	O
bits	O
are	O
corrupted	O
,	O
only	O
2	O
successive	O
symbols	O
in	O
the	O
galois	O
(	O
cid:12	O
)	O
eld	O
representation	O
are	O
corrupted	O
.	O
concate-	O
nation	O
and	O
interleaving	O
can	O
give	O
further	O
protection	O
against	O
burst	B
errors	I
.	O
the	O
concatenated	B
reed	O
{	O
solomon	O
codes	O
used	O
on	O
digital	O
compact	O
discs	O
are	O
able	O
to	O
correct	O
bursts	O
of	O
errors	O
of	O
length	O
4000	O
bits	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
186	O
11	O
|	O
error-correcting	B
codes	I
and	O
real	O
channels	O
.	O
exercise	O
11.4	O
.	O
[	O
2	O
,	O
p.189	O
]	O
the	O
technique	O
of	O
interleaving	O
,	O
which	O
allows	O
bursts	O
of	O
errors	O
to	O
be	O
treated	O
as	O
independent	O
,	O
is	O
widely	O
used	O
,	O
but	O
is	O
theoretically	O
a	O
poor	O
way	O
to	O
protect	O
data	O
against	O
burst	B
errors	I
,	O
in	O
terms	O
of	O
the	O
amount	O
of	O
redundancy	O
required	O
.	O
explain	O
why	O
interleaving	B
is	O
a	O
poor	O
method	O
,	O
using	O
the	O
following	O
burst-error	O
channel	B
as	O
an	O
example	O
.	O
time	O
is	O
divided	O
into	O
chunks	O
of	O
length	O
n	O
=	O
100	O
clock	O
cycles	O
;	O
during	O
each	O
chunk	O
,	O
there	O
is	O
a	O
burst	O
with	O
probability	B
b	O
=	O
0:2	O
;	O
during	O
a	O
burst	O
,	O
the	O
channel	B
is	O
a	O
bi-	O
nary	O
symmetric	B
channel	I
with	O
f	O
=	O
0:5.	O
if	O
there	O
is	O
no	O
burst	O
,	O
the	O
channel	B
is	O
an	O
error-free	O
binary	O
channel	O
.	O
compute	O
the	O
capacity	B
of	O
this	O
channel	B
and	O
compare	O
it	O
with	O
the	O
maximum	O
communication	O
rate	B
that	O
could	O
con-	O
ceivably	O
be	O
achieved	O
if	O
one	O
used	O
interleaving	B
and	O
treated	O
the	O
errors	B
as	O
independent	O
.	O
fading	B
channels	O
are	O
real	O
channels	O
like	O
gaussian	O
channels	O
except	O
that	O
the	O
received	O
power	O
is	O
assumed	O
to	O
vary	O
with	O
time	O
.	O
a	O
moving	O
mobile	B
phone	I
is	O
an	O
important	O
example	O
.	O
the	O
incoming	O
radio	B
signal	O
is	O
re	O
(	O
cid:13	O
)	O
ected	O
o	O
(	O
cid:11	O
)	O
nearby	O
objects	O
so	O
that	O
there	O
are	O
interference	O
patterns	O
and	O
the	O
intensity	O
of	O
the	O
signal	O
received	O
by	O
the	O
phone	B
varies	O
with	O
its	O
location	O
.	O
the	O
received	O
power	O
can	O
easily	O
vary	O
by	O
10	O
decibels	O
(	O
a	O
factor	O
of	O
ten	O
)	O
as	O
the	O
phone	B
’	O
s	O
antenna	O
moves	O
through	O
a	O
distance	B
similar	O
to	O
the	O
wavelength	O
of	O
the	O
radio	O
signal	O
(	O
a	O
few	O
centimetres	O
)	O
.	O
11.5	O
the	O
state	O
of	O
the	O
art	O
what	O
are	O
the	O
best	O
known	O
codes	O
for	O
communicating	O
over	O
gaussian	O
channels	O
?	O
all	O
the	O
practical	B
codes	O
are	O
linear	B
codes	I
,	O
and	O
are	O
either	O
based	O
on	O
convolutional	O
codes	O
or	O
block	B
codes	O
.	O
convolutional	B
codes	O
,	O
and	O
codes	O
based	O
on	O
them	O
textbook	O
convolutional	B
codes	O
.	O
the	O
‘	O
de	O
facto	O
standard	O
’	O
error-correcting	B
code	I
for	O
satellite	B
communications	I
is	O
a	O
convolutional	B
code	I
with	O
constraint	O
length	O
7.	O
convolutional	B
codes	O
are	O
discussed	O
in	O
chapter	O
48.	O
concatenated	B
convolutional	O
codes	O
.	O
the	O
above	O
convolutional	B
code	I
can	O
be	O
used	O
as	O
the	O
inner	B
code	I
of	O
a	O
concatenated	B
code	O
whose	O
outer	B
code	I
is	O
a	O
reed	O
{	O
solomon	O
code	B
with	O
eight-bit	O
symbols	O
.	O
this	O
code	B
was	O
used	O
in	O
deep	O
space	O
communication	B
systems	O
such	O
as	O
the	O
voyager	O
spacecraft	O
.	O
for	O
further	O
reading	O
about	O
reed	O
{	O
solomon	O
codes	O
,	O
see	O
lin	O
and	O
costello	O
(	O
1983	O
)	O
.	O
the	O
code	B
for	O
galileo	O
.	O
a	O
code	B
using	O
the	O
same	O
format	O
but	O
using	O
a	O
longer	O
constraint	O
length	O
{	O
15	O
{	O
for	O
its	O
convolutional	B
code	I
and	O
a	O
larger	O
reed	O
{	O
solomon	O
code	B
was	O
developed	O
by	O
the	O
jet	O
propulsion	O
laboratory	O
(	O
swan-	O
son	O
,	O
1988	O
)	O
.	O
the	O
details	O
of	O
this	O
code	B
are	O
unpublished	O
outside	O
jpl	O
,	O
and	O
the	O
decoding	B
is	O
only	O
possible	O
using	O
a	O
room	O
full	O
of	O
special-purpose	O
hardware	O
.	O
in	O
1992	O
,	O
this	O
was	O
the	O
best	O
code	B
known	O
of	O
rate	O
1/4	O
.	O
turbo	B
codes	I
.	O
in	O
1993	O
,	O
berrou	O
,	O
glavieux	O
and	O
thitimajshima	O
reported	O
work	O
on	O
turbo	O
codes	O
.	O
the	O
encoder	B
of	O
a	O
turbo	B
code	I
is	O
based	O
on	O
the	O
encoders	O
of	O
two	O
convolutional	B
codes	O
.	O
the	O
source	O
bits	O
are	O
fed	O
into	O
each	O
encoder	B
,	O
the	O
order	O
of	O
the	O
source	O
bits	O
being	O
permuted	O
in	O
a	O
random	B
way	O
,	O
and	O
the	O
resulting	O
parity	B
bits	O
from	O
each	O
constituent	O
code	B
are	O
transmitted	O
.	O
the	O
decoding	B
algorithm	O
involves	O
iteratively	O
decoding	B
each	O
constituent	O
code	B
using	O
its	O
standard	O
decoding	O
algorithm	B
,	O
then	O
using	O
the	O
output	O
of	O
the	O
decoder	O
as	O
the	O
input	O
to	O
the	O
other	O
decoder	B
.	O
this	O
decoding	B
algorithm	O
-	O
c1	O
-	O
-	O
-	O
-	O
c2	O
(	O
cid:14	O
)	O
(	O
cid:13	O
)	O
(	O
cid:15	O
)	O
(	O
cid:12	O
)	O
(	O
cid:25	O
)	O
figure	O
11.7.	O
the	O
encoder	B
of	O
a	O
turbo	B
code	I
.	O
each	O
box	B
c1	O
,	O
c2	O
,	O
contains	O
a	O
convolutional	B
code	I
.	O
the	O
source	O
bits	O
are	O
reordered	O
using	O
a	O
permutation	B
(	O
cid:25	O
)	O
before	O
they	O
are	O
fed	O
to	O
c2	O
.	O
the	O
transmitted	O
codeword	B
is	O
obtained	O
by	O
concatenating	O
or	O
interleaving	B
the	O
outputs	O
of	O
the	O
two	O
convolutional	B
codes	O
.	O
the	O
random	B
permutation	O
is	O
chosen	O
when	O
the	O
code	B
is	O
designed	O
,	O
and	O
(	O
cid:12	O
)	O
xed	O
thereafter	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
11.6	O
:	O
summary	B
187	O
is	O
an	O
instance	O
of	O
a	O
message-passing	B
algorithm	O
called	O
the	O
sum	O
{	O
product	O
algorithm	O
.	O
turbo	B
codes	I
are	O
discussed	O
in	O
chapter	O
48	O
,	O
and	O
message	O
passing	O
in	O
chap-	O
ters	O
16	O
,	O
17	O
,	O
25	O
,	O
and	O
26.	O
h	O
=	O
block	B
codes	O
gallager	O
’	O
s	O
low-density	B
parity-check	I
codes	O
.	O
the	O
best	O
block	B
codes	O
known	O
for	O
gaussian	O
channels	O
were	O
invented	O
by	O
gallager	O
in	O
1962	O
but	O
were	O
promptly	O
forgotten	O
by	O
most	O
of	O
the	O
coding	O
theory	B
community	O
.	O
they	O
were	O
rediscovered	O
in	O
1995	O
and	O
shown	O
to	O
have	O
outstanding	O
theoretical	O
and	O
prac-	O
tical	O
properties	O
.	O
like	O
turbo	B
codes	I
,	O
they	O
are	O
decoded	O
by	O
message-passing	O
algorithms	B
.	O
we	O
will	O
discuss	O
these	O
beautifully	O
simple	O
codes	O
in	O
chapter	O
47.	O
the	O
performances	O
of	O
the	O
above	O
codes	O
are	O
compared	O
for	O
gaussian	O
channels	O
in	O
(	O
cid:12	O
)	O
gure	O
47.17	O
,	O
p.568	O
.	O
11.6	O
summary	B
random	O
codes	O
are	O
good	B
,	O
but	O
they	O
require	O
exponential	B
resources	O
to	O
encode	O
and	O
decode	O
them	O
.	O
non-random	O
codes	O
tend	O
for	O
the	O
most	O
part	O
not	O
to	O
be	O
as	O
good	O
as	O
random	O
codes	O
.	O
for	O
a	O
non-random	O
code	B
,	O
encoding	O
may	O
be	O
easy	O
,	O
but	O
even	O
for	O
simply-de	O
(	O
cid:12	O
)	O
ned	O
linear	B
codes	I
,	O
the	O
decoding	B
problem	O
remains	O
very	O
di	O
(	O
cid:14	O
)	O
cult	O
.	O
the	O
best	O
practical	B
codes	O
(	O
a	O
)	O
employ	O
very	O
large	O
block	B
sizes	O
;	O
(	O
b	O
)	O
are	O
based	O
on	O
semi-random	O
code	B
constructions	O
;	O
and	O
(	O
c	O
)	O
make	O
use	O
of	O
probability-	O
based	O
decoding	B
algorithms	O
.	O
11.7	O
nonlinear	B
codes	O
figure	O
11.8.	O
a	O
low-density	B
parity-check	I
matrix	O
and	O
the	O
corresponding	O
graph	B
of	O
a	O
rate-1/4	O
low-density	B
parity-check	I
code	I
with	O
blocklength	O
n	O
=	O
16	O
,	O
and	O
m	O
=	O
12	O
constraints	O
.	O
each	O
white	B
circle	O
represents	O
a	O
transmitted	O
bit	B
.	O
each	O
bit	B
participates	O
in	O
j	O
=	O
3	O
constraints	O
,	O
represented	O
by	O
squares	O
.	O
each	O
constraint	O
forces	O
the	O
sum	O
of	O
the	O
k	O
=	O
4	O
bits	O
to	O
which	O
it	O
is	O
connected	O
to	O
be	O
even	O
.	O
this	O
code	B
is	O
a	O
(	O
16	O
;	O
4	O
)	O
code	B
.	O
outstanding	O
performance	O
is	O
obtained	O
when	O
the	O
blocklength	O
is	O
increased	O
to	O
n	O
’	O
10	O
000.	O
most	O
practically	O
used	O
codes	O
are	O
linear	B
,	O
but	O
not	O
all	O
.	O
digital	O
soundtracks	O
are	O
encoded	O
onto	O
cinema	B
(	O
cid:12	O
)	O
lm	O
as	O
a	O
binary	O
pattern	O
.	O
the	O
likely	O
errors	B
a	O
(	O
cid:11	O
)	O
ecting	O
the	O
(	O
cid:12	O
)	O
lm	O
involve	O
dirt	O
and	O
scratches	O
,	O
which	O
produce	O
large	O
numbers	O
of	O
1s	O
and	O
0s	O
respectively	O
.	O
we	O
want	O
none	O
of	O
the	O
codewords	O
to	O
look	O
like	O
all-1s	O
or	O
all-0s	O
,	O
so	O
that	O
it	O
will	O
be	O
easy	O
to	O
detect	O
errors	B
caused	O
by	O
dirt	O
and	O
scratches	O
.	O
one	O
of	O
the	O
codes	O
used	O
in	O
digital	O
cinema	B
sound	O
systems	O
is	O
a	O
nonlinear	B
(	O
8	O
;	O
6	O
)	O
code	B
consisting	O
of	O
64	O
of	O
the	O
(	O
cid:0	O
)	O
8	O
4	O
(	O
cid:1	O
)	O
binary	O
patterns	O
of	O
weight	O
4	O
.	O
11.8	O
errors	B
other	O
than	O
noise	B
another	O
source	O
of	O
uncertainty	O
for	O
the	O
receiver	O
is	O
uncertainty	O
about	O
the	O
tim-	O
ing	O
of	O
the	O
transmitted	O
signal	O
x	O
(	O
t	O
)	O
.	O
in	O
ordinary	O
coding	B
theory	I
and	O
infor-	O
mation	O
theory	B
,	O
the	O
transmitter	O
’	O
s	O
time	O
t	O
and	O
the	O
receiver	O
’	O
s	O
time	O
u	O
are	O
as-	O
sumed	O
to	O
be	O
perfectly	O
synchronized	O
.	O
but	O
if	O
the	O
receiver	O
receives	O
a	O
signal	O
y	O
(	O
u	O
)	O
,	O
where	O
the	O
receiver	O
’	O
s	O
time	O
,	O
u	O
,	O
is	O
an	O
imperfectly	O
known	O
function	B
u	O
(	O
t	O
)	O
of	O
the	O
transmitter	O
’	O
s	O
time	O
t	O
,	O
then	O
the	O
capacity	B
of	O
this	O
channel	B
for	O
commu-	O
nication	O
is	O
reduced	O
.	O
the	O
theory	B
of	O
such	O
channels	O
is	O
incomplete	O
,	O
compared	O
with	O
the	O
synchronized	O
channels	O
we	O
have	O
discussed	O
thus	O
far	O
.	O
not	O
even	O
the	O
ca-	O
pacity	O
of	O
channels	O
with	O
synchronization	O
errors	B
is	O
known	O
(	O
levenshtein	O
,	O
1966	O
;	O
ferreira	O
et	O
al.	O
,	O
1997	O
)	O
;	O
codes	O
for	O
reliable	O
communication	B
over	O
channels	O
with	O
synchronization	O
errors	B
remain	O
an	O
active	O
research	O
area	O
(	O
davey	O
and	O
mackay	O
,	O
2001	O
)	O
.	O
 	B
 	I
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
188	O
further	O
reading	O
11	O
|	O
error-correcting	B
codes	I
and	O
real	O
channels	O
for	O
a	O
review	O
of	O
the	O
history	O
of	O
spread-spectrum	O
methods	B
,	O
see	O
scholtz	O
(	O
1982	O
)	O
.	O
11.9	O
exercises	O
the	O
gaussian	O
channel	B
.	O
exercise	O
11.5	O
.	O
[	O
2	O
,	O
p.190	O
]	O
consider	O
a	O
gaussian	O
channel	O
with	O
a	O
real	O
input	O
x	O
,	O
and	O
signal	O
to	O
noise	B
ratio	O
v=	O
(	O
cid:27	O
)	O
2	O
.	O
(	O
a	O
)	O
what	O
is	O
its	O
capacity	B
c	O
?	O
(	O
b	O
)	O
if	O
the	O
input	O
is	O
constrained	B
to	O
be	O
binary	O
,	O
x	O
2	O
f	O
(	O
cid:6	O
)	O
pvg	O
,	O
what	O
is	O
the	O
capacity	B
c0	O
of	O
this	O
constrained	B
channel	I
?	O
(	O
c	O
)	O
if	O
in	O
addition	O
the	O
output	O
of	O
the	O
channel	O
is	O
thresholded	O
using	O
the	O
mapping	B
y	O
!	O
y0	O
=	O
(	O
cid:26	O
)	O
1	O
y	O
>	O
0	O
0	O
y	O
(	O
cid:20	O
)	O
0	O
;	O
(	O
11.35	O
)	O
what	O
is	O
the	O
capacity	B
c00	O
of	O
the	O
resulting	O
channel	B
?	O
(	O
d	O
)	O
plot	O
the	O
three	O
capacities	O
above	O
as	O
a	O
function	B
of	O
v=	O
(	O
cid:27	O
)	O
2	O
from	O
0.1	O
to	O
2	O
.	O
[	O
you	O
’	O
ll	O
need	O
to	O
do	O
a	O
numerical	O
integral	B
to	O
evaluate	O
c0	O
.	O
]	O
.	O
exercise	O
11.6	O
.	O
[	O
3	O
]	O
for	O
large	O
integers	O
k	O
and	O
n	O
,	O
what	O
fraction	O
of	O
all	O
binary	O
error-	O
correcting	O
codes	O
of	O
length	O
n	O
and	O
rate	O
r	O
=	O
k=n	O
are	O
linear	B
codes	I
?	O
[	O
the	O
answer	O
will	O
depend	O
on	O
whether	O
you	O
choose	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
code	B
to	O
be	O
an	O
ordered	O
list	O
of	O
2k	O
codewords	O
,	O
that	O
is	O
,	O
a	O
mapping	B
from	O
s	O
2	O
f1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
2kg	O
to	O
x	O
(	O
s	O
)	O
,	O
or	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
code	B
to	O
be	O
an	O
unordered	O
list	O
,	O
so	O
that	O
two	O
codes	O
consisting	O
of	O
the	O
same	O
codewords	O
are	O
identical	O
.	O
use	O
the	O
latter	O
de	O
(	O
cid:12	O
)	O
nition	O
:	O
a	O
code	B
is	O
a	O
set	B
of	O
codewords	O
;	O
how	O
the	O
encoder	B
operates	O
is	O
not	O
part	O
of	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
the	O
code	O
.	O
]	O
erasure	B
channels	O
.	O
exercise	O
11.7	O
.	O
[	O
4	O
]	O
design	O
a	O
code	B
for	O
the	O
binary	B
erasure	I
channel	I
,	O
and	O
a	O
decoding	B
algorithm	O
,	O
and	O
evaluate	O
their	O
probability	B
of	I
error	I
.	O
[	O
the	O
design	O
of	O
good	B
codes	O
for	O
erasure	O
channels	O
is	O
an	O
active	O
research	O
area	O
(	O
spielman	O
,	O
1996	O
;	O
byers	O
et	O
al.	O
,	O
1998	O
)	O
;	O
see	O
also	O
chapter	O
50	O
.	O
]	O
.	O
exercise	O
11.8	O
.	O
[	O
5	O
]	O
design	O
a	O
code	B
for	O
the	O
q-ary	O
erasure	B
channel	I
,	O
whose	O
input	O
x	O
is	O
drawn	O
from	O
0	O
;	O
1	O
;	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
;	O
(	O
q	O
(	O
cid:0	O
)	O
1	O
)	O
,	O
and	O
whose	O
output	O
y	O
is	O
equal	O
to	O
x	O
with	O
probability	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
and	O
equal	O
to	O
?	O
otherwise	O
.	O
[	O
this	O
erasure	B
channel	I
is	O
a	O
good	B
model	O
for	O
packets	O
transmitted	O
over	O
the	O
internet	B
,	O
which	O
are	O
either	O
received	O
reliably	O
or	O
are	O
lost	O
.	O
]	O
exercise	O
11.9	O
.	O
[	O
3	O
,	O
p.190	O
]	O
how	O
do	O
redundant	O
arrays	O
of	O
independent	O
disks	O
(	O
raid	O
)	O
work	O
?	O
these	O
are	O
information	B
storage	O
systems	O
consisting	O
of	O
about	O
ten	O
disk	O
drives	O
,	O
of	O
which	O
any	O
two	O
or	O
three	O
can	O
be	O
disabled	O
and	O
the	O
others	O
are	O
able	O
to	O
still	O
able	O
to	O
reconstruct	O
any	O
requested	O
(	O
cid:12	O
)	O
le	O
.	O
what	O
codes	O
are	O
used	O
,	O
and	O
how	O
far	O
are	O
these	O
systems	O
from	O
the	O
shannon	O
limit	O
for	O
the	O
problem	O
they	O
are	O
solving	O
?	O
how	O
would	O
you	O
design	O
a	O
better	O
raid	O
system	O
?	O
some	O
information	B
is	O
provided	O
in	O
the	O
solution	O
section	B
.	O
see	O
http	O
:	O
//www.acnc	O
.	O
com/raid2.html	O
;	O
see	O
also	O
chapter	O
50	O
.	O
[	O
some	O
people	O
say	O
raid	O
stands	O
for	O
‘	O
redundant	O
array	O
of	O
inexpensive	O
disks	O
’	O
,	O
but	O
i	O
think	O
that	O
’	O
s	O
silly	O
{	O
raid	O
would	O
still	O
be	O
a	O
good	B
idea	O
even	O
if	O
the	O
disks	O
were	O
expensive	O
!	O
]	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
11.10	O
:	O
solutions	O
11.10	O
solutions	O
solution	O
to	O
exercise	O
11.1	O
(	O
p.181	O
)	O
.	O
introduce	O
a	O
lagrange	O
multiplier	O
(	O
cid:21	O
)	O
for	O
the	O
power	O
constraint	O
and	O
another	O
,	O
(	O
cid:22	O
)	O
,	O
for	O
the	O
constraint	O
of	O
normalization	O
of	O
p	O
(	O
x	O
)	O
.	O
189	O
f	O
=	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
r	O
dx	O
p	O
(	O
x	O
)	O
x2	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
r	O
dx	O
p	O
(	O
x	O
)	O
=	O
z	O
dx	O
p	O
(	O
x	O
)	O
(	O
cid:20	O
)	O
z	O
dy	O
p	O
(	O
y	O
j	O
x	O
)	O
ln	O
p	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
x2	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
cid:21	O
)	O
:	O
p	O
(	O
y	O
j	O
x	O
)	O
(	O
11.36	O
)	O
(	O
11.37	O
)	O
make	O
the	O
functional	O
derivative	O
with	O
respect	O
to	O
p	O
(	O
x	O
(	O
cid:3	O
)	O
)	O
.	O
(	O
cid:14	O
)	O
f	O
(	O
cid:14	O
)	O
p	O
(	O
x	O
(	O
cid:3	O
)	O
)	O
=	O
z	O
dy	O
p	O
(	O
y	O
j	O
x	O
(	O
cid:3	O
)	O
)	O
ln	O
p	O
(	O
y	O
j	O
x	O
(	O
cid:3	O
)	O
)	O
p	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
x	O
(	O
cid:3	O
)	O
2	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
z	O
dx	O
p	O
(	O
x	O
)	O
z	O
dy	O
p	O
(	O
y	O
j	O
x	O
)	O
(	O
cid:14	O
)	O
p	O
(	O
y	O
)	O
(	O
cid:14	O
)	O
p	O
(	O
x	O
(	O
cid:3	O
)	O
)	O
p	O
(	O
y	O
)	O
1	O
:	O
(	O
11.38	O
)	O
the	O
(	O
cid:12	O
)	O
nal	O
factor	O
(	O
cid:14	O
)	O
p	O
(	O
y	O
)	O
=	O
(	O
cid:14	O
)	O
p	O
(	O
x	O
(	O
cid:3	O
)	O
)	O
is	O
found	O
,	O
using	O
p	O
(	O
y	O
)	O
=r	O
dx	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
j	O
x	O
)	O
,	O
to	O
be	O
p	O
(	O
y	O
j	O
x	O
(	O
cid:3	O
)	O
)	O
,	O
and	O
the	O
whole	O
of	O
the	O
last	O
term	O
collapses	O
in	O
a	O
pu	O
(	O
cid:11	O
)	O
of	O
smoke	O
to	O
1	O
,	O
which	O
can	O
be	O
absorbed	O
into	O
the	O
(	O
cid:22	O
)	O
term	O
.	O
substitute	O
p	O
(	O
y	O
j	O
x	O
)	O
=	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
y	O
(	O
cid:0	O
)	O
x	O
)	O
2=2	O
(	O
cid:27	O
)	O
2	O
)	O
=p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
and	O
set	O
the	O
derivative	O
to	O
zero	O
:	O
p	O
(	O
y	O
j	O
x	O
)	O
p	O
(	O
y	O
)	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
x2	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
0	O
=	O
0	O
(	O
11.39	O
)	O
ln	O
[	O
p	O
(	O
y	O
)	O
(	O
cid:27	O
)	O
]	O
=	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
x2	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
0	O
(	O
cid:0	O
)	O
this	O
condition	O
must	O
be	O
satis	O
(	O
cid:12	O
)	O
ed	O
by	O
ln	O
[	O
p	O
(	O
y	O
)	O
(	O
cid:27	O
)	O
]	O
for	O
all	O
x	O
.	O
)	O
z	O
dy	O
writing	B
a	O
taylor	O
expansion	O
of	O
ln	O
[	O
p	O
(	O
y	O
)	O
(	O
cid:27	O
)	O
]	O
=	O
a+by+cy	O
2+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
,	O
only	O
a	O
quadratic	O
function	B
ln	O
[	O
p	O
(	O
y	O
)	O
(	O
cid:27	O
)	O
]	O
=	O
a	O
+	O
cy2	O
would	O
satisfy	O
the	O
constraint	O
(	O
11.40	O
)	O
.	O
(	O
any	O
higher	O
order	O
terms	O
yp	O
,	O
p	O
>	O
2	O
,	O
would	O
produce	O
terms	O
in	O
xp	O
that	O
are	O
not	O
present	O
on	O
the	O
right-hand	O
side	O
.	O
)	O
therefore	O
p	O
(	O
y	O
)	O
is	O
gaussian	O
.	O
we	O
can	O
obtain	O
this	O
optimal	B
output	O
distribution	B
by	O
using	O
a	O
gaussian	O
input	O
distribution	O
p	O
(	O
x	O
)	O
.	O
(	O
11.40	O
)	O
1	O
2	O
:	O
z	O
dy	O
p	O
(	O
y	O
j	O
x	O
)	O
ln	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
y	O
(	O
cid:0	O
)	O
x	O
)	O
2=2	O
(	O
cid:27	O
)	O
2	O
)	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
solution	O
to	O
exercise	O
11.2	O
(	O
p.181	O
)	O
.	O
given	O
a	O
gaussian	O
input	O
distribution	O
of	O
vari-	O
ance	O
v	O
,	O
the	O
output	O
distribution	B
is	O
normal	B
(	O
0	O
;	O
v	O
+	O
(	O
cid:27	O
)	O
2	O
)	O
,	O
since	O
x	O
and	O
the	O
noise	B
are	O
independent	O
random	O
variables	O
,	O
and	O
variances	O
add	O
for	O
independent	O
random	B
variables	O
.	O
the	O
mutual	B
information	I
is	O
:	O
i	O
(	O
x	O
;	O
y	O
)	O
=	O
z	O
dx	O
dy	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
j	O
x	O
)	O
log	O
p	O
(	O
y	O
j	O
x	O
)	O
(	O
cid:0	O
)	O
z	O
dy	O
p	O
(	O
y	O
)	O
log	O
p	O
(	O
y	O
)	O
(	O
11.41	O
)	O
=	O
=	O
1	O
2	O
1	O
2	O
log	O
1	O
(	O
cid:27	O
)	O
2	O
(	O
cid:0	O
)	O
log	O
(	O
cid:16	O
)	O
1	O
+	O
1	O
v	O
+	O
(	O
cid:27	O
)	O
2	O
1	O
2	O
v	O
log	O
(	O
cid:27	O
)	O
2	O
(	O
cid:17	O
)	O
:	O
(	O
11.42	O
)	O
(	O
11.43	O
)	O
solution	O
to	O
exercise	O
11.4	O
(	O
p.186	O
)	O
.	O
the	O
capacity	B
of	O
the	O
channel	B
is	O
one	O
minus	O
the	O
information	B
content	I
of	O
the	O
noise	B
that	O
it	O
adds	O
.	O
that	O
information	B
content	I
is	O
,	O
per	O
chunk	O
,	O
the	O
entropy	B
of	O
the	O
selection	O
of	O
whether	O
the	O
chunk	O
is	O
bursty	B
,	O
h2	O
(	O
b	O
)	O
,	O
plus	O
,	O
with	O
probability	O
b	O
,	O
the	O
entropy	B
of	O
the	O
(	O
cid:13	O
)	O
ipped	O
bits	O
,	O
n	O
,	O
which	O
adds	O
up	O
to	O
h2	O
(	O
b	O
)	O
+	O
n	O
b	O
per	O
chunk	O
(	O
roughly	O
;	O
accurate	O
if	O
n	O
is	O
large	O
)	O
.	O
so	O
,	O
per	O
bit	B
,	O
the	O
capacity	B
is	O
,	O
for	O
n	O
=	O
100	O
,	O
c	O
=	O
1	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
1	O
n	O
h2	O
(	O
b	O
)	O
+	O
b	O
(	O
cid:19	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
0:207	O
=	O
0:793	O
:	O
(	O
11.44	O
)	O
in	O
contrast	O
,	O
interleaving	B
,	O
which	O
treats	O
bursts	O
of	O
errors	O
as	O
independent	O
,	O
causes	O
the	O
channel	B
to	O
be	O
treated	O
as	O
a	O
binary	B
symmetric	I
channel	I
with	O
f	O
=	O
0:2	O
(	O
cid:2	O
)	O
0:5	O
=	O
0:1	O
,	O
whose	O
capacity	B
is	O
about	O
0.53	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
190	O
11	O
|	O
error-correcting	B
codes	I
and	O
real	O
channels	O
interleaving	B
throws	O
away	O
the	O
useful	O
information	B
about	O
the	O
correlated-	O
ness	O
of	O
the	O
errors	O
.	O
theoretically	O
,	O
we	O
should	O
be	O
able	O
to	O
communicate	O
about	O
(	O
0:79=0:53	O
)	O
’	O
1:6	O
times	O
faster	O
using	O
a	O
code	B
and	O
decoder	B
that	O
explicitly	O
treat	O
bursts	O
as	O
bursts	O
.	O
solution	O
to	O
exercise	O
11.5	O
(	O
p.188	O
)	O
.	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0	O
0.5	O
1	O
1.5	O
2	O
2.5	O
1	O
0.1	O
0.01	O
0.1	O
1	O
figure	O
11.9.	O
capacities	O
(	O
from	O
top	O
to	O
bottom	O
in	O
each	O
graph	B
)	O
c	O
,	O
c	O
0	O
,	O
and	O
c	O
00	O
,	O
versus	O
the	O
signal-to-noise	B
ratio	I
(	O
pv=	O
(	O
cid:27	O
)	O
)	O
.	O
the	O
lower	O
graph	B
is	O
a	O
log	O
{	O
log	O
plot	O
.	O
c	O
=	O
1	O
2	O
log	O
(	O
cid:16	O
)	O
1	O
+	O
v	O
(	O
cid:27	O
)	O
2	O
(	O
cid:17	O
)	O
:	O
(	O
11.45	O
)	O
(	O
a	O
)	O
putting	O
together	O
the	O
results	O
of	O
exercises	O
11.1	O
and	O
11.2	O
,	O
we	O
deduce	O
that	O
a	O
gaussian	O
channel	O
with	O
real	O
input	O
x	O
,	O
and	O
signal	O
to	O
noise	B
ratio	O
v=	O
(	O
cid:27	O
)	O
2	O
has	O
capacity	B
(	O
b	O
)	O
if	O
the	O
input	O
is	O
constrained	B
to	O
be	O
binary	O
,	O
x	O
2	O
f	O
(	O
cid:6	O
)	O
pvg	O
,	O
the	O
capacity	B
is	O
achieved	O
by	O
using	O
these	O
two	O
inputs	O
with	O
equal	O
probability	B
.	O
the	O
capacity	B
is	O
reduced	O
to	O
a	O
somewhat	O
messy	O
integral	B
,	O
c00	O
=z	O
1	O
(	O
cid:0	O
)	O
1	O
dy	O
n	O
(	O
y	O
;	O
0	O
)	O
log	O
n	O
(	O
y	O
;	O
0	O
)	O
(	O
cid:0	O
)	O
z	O
1	O
(	O
cid:0	O
)	O
1	O
dy	O
p	O
(	O
y	O
)	O
log	O
p	O
(	O
y	O
)	O
;	O
(	O
11.46	O
)	O
where	O
n	O
(	O
y	O
;	O
x	O
)	O
(	O
cid:17	O
)	O
(	O
1=p2	O
(	O
cid:25	O
)	O
)	O
exp	O
[	O
(	O
y	O
(	O
cid:0	O
)	O
x	O
)	O
2=2	O
]	O
,	O
x	O
(	O
cid:17	O
)	O
pv=	O
(	O
cid:27	O
)	O
,	O
and	O
p	O
(	O
y	O
)	O
(	O
cid:17	O
)	O
[	O
n	O
(	O
y	O
;	O
x	O
)	O
+	O
n	O
(	O
y	O
;	O
(	O
cid:0	O
)	O
x	O
)	O
]	O
=2	O
.	O
this	O
capacity	B
is	O
smaller	O
than	O
the	O
unconstrained	O
capacity	B
(	O
11.45	O
)	O
,	O
but	O
for	O
small	O
signal-to-noise	B
ratio	I
,	O
the	O
two	O
capacities	O
are	O
close	O
in	O
value	O
.	O
(	O
c	O
)	O
if	O
the	O
output	O
is	O
thresholded	O
,	O
then	O
the	O
gaussian	O
channel	B
is	O
turned	O
into	O
a	O
binary	B
symmetric	I
channel	I
whose	O
transition	B
probability	I
is	O
given	O
by	O
the	O
error	B
function	I
(	O
cid:8	O
)	O
de	O
(	O
cid:12	O
)	O
ned	O
on	O
page	O
156.	O
the	O
capacity	B
is	O
c00	O
=	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
f	O
)	O
;	O
where	O
f	O
=	O
(	O
cid:8	O
)	O
(	O
pv=	O
(	O
cid:27	O
)	O
)	O
:	O
(	O
11.47	O
)	O
solution	O
to	O
exercise	O
11.9	O
(	O
p.188	O
)	O
.	O
there	O
are	O
several	O
raid	O
systems	O
.	O
one	O
of	O
the	O
easiest	O
to	O
understand	O
consists	O
of	O
7	O
disk	O
drives	O
which	O
store	O
data	O
at	O
rate	B
4=7	O
using	O
a	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
:	O
each	O
successive	O
four	O
bits	O
are	O
encoded	O
with	O
the	O
code	B
and	O
the	O
seven	O
codeword	B
bits	O
are	O
written	O
one	O
to	O
each	O
disk	O
.	O
two	O
or	O
perhaps	O
three	O
disk	O
drives	O
can	O
go	O
down	O
and	O
the	O
others	O
can	O
recover	O
the	O
data	O
.	O
the	O
e	O
(	O
cid:11	O
)	O
ective	O
channel	B
model	O
here	O
is	O
a	O
binary	B
erasure	I
channel	I
,	O
because	O
it	O
is	O
assumed	O
that	O
we	O
can	O
tell	O
when	O
a	O
disk	O
is	O
dead	O
.	O
it	O
is	O
not	O
possible	O
to	O
recover	O
the	O
data	O
for	O
some	O
choices	O
of	O
the	O
three	O
dead	O
disk	O
drives	O
;	O
can	O
you	O
see	O
why	O
?	O
.	O
exercise	O
11.10	O
.	O
[	O
2	O
,	O
p.190	O
]	O
give	O
an	O
example	O
of	O
three	O
disk	O
drives	O
that	O
,	O
if	O
lost	O
,	O
lead	O
to	O
failure	O
of	O
the	O
above	O
raid	O
system	O
,	O
and	O
three	O
that	O
can	O
be	O
lost	O
without	O
failure	O
.	O
solution	O
to	O
exercise	O
11.10	O
(	O
p.190	O
)	O
.	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
has	O
codewords	O
of	O
weight	O
3.	O
if	O
any	O
set	B
of	O
three	O
disk	O
drives	O
corresponding	O
to	O
one	O
of	O
those	O
code-	O
words	O
is	O
lost	O
,	O
then	O
the	O
other	O
four	O
disks	O
can	O
recover	O
only	O
3	O
bits	O
of	O
information	B
about	O
the	O
four	O
source	O
bits	O
;	O
a	O
fourth	O
bit	B
is	O
lost	O
.	O
[	O
cf	O
.	O
exercise	O
13.13	O
(	O
p.220	O
)	O
with	O
q	O
=	O
2	O
:	O
there	O
are	O
no	O
binary	O
mds	O
codes	O
.	O
this	O
de	O
(	O
cid:12	O
)	O
cit	O
is	O
discussed	O
further	O
in	O
section	O
13.11	O
.	O
]	O
any	O
other	O
set	B
of	O
three	O
disk	O
drives	O
can	O
be	O
lost	O
without	O
problems	O
because	O
the	O
corresponding	O
four	O
by	O
four	O
submatrix	O
of	O
the	O
generator	O
matrix	B
is	O
invertible	O
.	O
a	O
better	O
code	B
would	O
be	O
a	O
digital	B
fountain	I
{	O
see	O
chapter	O
50.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
part	O
iii	O
further	O
topics	O
in	O
information	O
theory	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
chapter	O
12	O
in	O
chapters	O
1	O
{	O
11	O
,	O
we	O
concentrated	O
on	O
two	O
aspects	O
of	O
information	O
theory	B
and	O
coding	B
theory	I
:	O
source	O
coding	O
{	O
the	O
compression	B
of	O
information	B
so	O
as	O
to	O
make	O
e	O
(	O
cid:14	O
)	O
cient	O
use	O
of	O
data	O
transmission	O
and	O
storage	O
channels	O
;	O
and	O
channel	O
coding	O
{	O
the	O
redundant	O
encoding	O
of	O
information	O
so	O
as	O
to	O
be	O
able	O
to	O
detect	O
and	O
correct	O
communication	B
errors	O
.	O
in	O
both	O
these	O
areas	O
we	O
started	O
by	O
ignoring	O
practical	B
considerations	O
,	O
concen-	O
trating	O
on	O
the	O
question	O
of	O
the	O
theoretical	O
limitations	O
and	O
possibilities	O
of	O
coding	O
.	O
we	O
then	O
discussed	O
practical	B
source-coding	O
and	O
channel-coding	O
schemes	O
,	O
shift-	O
ing	O
the	O
emphasis	O
towards	O
computational	O
feasibility	O
.	O
but	O
the	O
prime	O
criterion	O
for	O
comparing	O
encoding	O
schemes	O
remained	O
the	O
e	O
(	O
cid:14	O
)	O
ciency	O
of	O
the	O
code	O
in	O
terms	O
of	O
the	O
channel	O
resources	O
it	O
required	O
:	O
the	O
best	O
source	O
codes	O
were	O
those	O
that	O
achieved	O
the	O
greatest	O
compression	B
;	O
the	O
best	O
channel	B
codes	O
were	O
those	O
that	O
communicated	O
at	O
the	O
highest	O
rate	B
with	O
a	O
given	O
probability	B
of	I
error	I
.	O
in	O
this	O
chapter	O
we	O
now	O
shift	O
our	O
viewpoint	O
a	O
little	O
,	O
thinking	O
of	O
ease	O
of	O
information	O
retrieval	O
as	O
a	O
primary	O
goal	O
.	O
it	O
turns	O
out	O
that	O
the	O
random	B
codes	O
which	O
were	O
theoretically	O
useful	O
in	O
our	O
study	O
of	O
channel	O
coding	O
are	O
also	O
useful	O
for	O
rapid	O
information	B
retrieval	I
.	O
e	O
(	O
cid:14	O
)	O
cient	O
information	B
retrieval	I
is	O
one	O
of	O
the	O
problems	O
that	O
brains	O
seem	O
to	O
solve	O
e	O
(	O
cid:11	O
)	O
ortlessly	O
,	O
and	O
content-addressable	O
memory	B
is	O
one	O
of	O
the	O
topics	O
we	O
will	O
study	O
when	O
we	O
look	O
at	O
neural	O
networks	O
.	O
192	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
string	O
length	B
number	O
of	O
strings	O
number	O
of	O
possible	O
strings	O
n	O
’	O
200	O
s	O
’	O
223	O
2n	O
’	O
2200	O
figure	O
12.1.	O
cast	O
of	O
characters	O
.	O
12	O
hash	O
codes	O
:	O
codes	O
for	O
e	O
(	O
cid:14	O
)	O
cient	O
information	B
retrieval	I
12.1	O
the	O
information-retrieval	O
problem	O
a	O
simple	O
example	O
of	O
an	O
information-retrieval	O
problem	O
is	O
the	O
task	O
of	O
imple-	O
menting	O
a	O
phone	B
directory	I
service	O
,	O
which	O
,	O
in	O
response	O
to	O
a	O
person	O
’	O
s	O
name	O
,	O
returns	O
(	O
a	O
)	O
a	O
con	O
(	O
cid:12	O
)	O
rmation	O
that	O
that	O
person	O
is	O
listed	O
in	O
the	O
directory	B
;	O
and	O
(	O
b	O
)	O
the	O
person	O
’	O
s	O
phone	B
number	I
and	O
other	O
details	O
.	O
we	O
could	O
formalize	O
this	O
prob-	O
lem	O
as	O
follows	O
,	O
with	O
s	O
being	O
the	O
number	O
of	O
names	O
that	O
must	O
be	O
stored	O
in	O
the	O
directory	B
.	O
you	O
are	O
given	O
a	O
list	O
of	O
s	O
binary	O
strings	O
of	O
length	O
n	O
bits	O
,	O
fx	O
(	O
1	O
)	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
s	O
)	O
g	O
,	O
where	O
s	O
is	O
considerably	O
smaller	O
than	O
the	O
total	O
number	O
of	O
possible	O
strings	O
,	O
2n	O
.	O
we	O
will	O
call	O
the	O
superscript	O
‘	O
s	O
’	O
in	O
x	O
(	O
s	O
)	O
the	O
record	O
number	O
of	O
the	O
string	O
.	O
the	O
idea	O
is	O
that	O
s	O
runs	O
over	O
customers	O
in	O
the	O
order	O
in	O
which	O
they	O
are	O
added	O
to	O
the	O
directory	B
and	O
x	O
(	O
s	O
)	O
is	O
the	O
name	O
of	O
customer	O
s.	O
we	O
assume	O
for	O
simplicity	O
that	O
all	O
people	O
have	O
names	O
of	O
the	O
same	O
length	B
.	O
the	O
name	O
length	B
might	O
be	O
,	O
say	O
,	O
n	O
=	O
200	O
bits	O
,	O
and	O
we	O
might	O
want	O
to	O
store	O
the	O
details	O
of	O
ten	O
million	O
customers	O
,	O
so	O
s	O
’	O
107	O
’	O
223.	O
we	O
will	O
ignore	O
the	O
possibility	O
that	O
two	O
customers	O
have	O
identical	O
names	O
.	O
the	O
task	O
is	O
to	O
construct	O
the	O
inverse	O
of	O
the	O
mapping	B
from	O
s	O
to	O
x	O
(	O
s	O
)	O
,	O
i.e.	O
,	O
to	O
make	O
a	O
system	O
that	O
,	O
given	O
a	O
string	O
x	O
,	O
returns	O
the	O
value	O
of	O
s	O
such	O
that	O
x	O
=	O
x	O
(	O
s	O
)	O
if	O
one	O
exists	O
,	O
and	O
otherwise	O
reports	O
that	O
no	O
such	O
s	O
exists	O
.	O
(	O
once	O
we	O
have	O
the	O
record	O
number	O
,	O
we	O
can	O
go	O
and	O
look	O
in	O
memory	O
location	O
s	O
in	O
a	O
separate	O
memory	B
full	O
of	O
phone	O
numbers	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
required	O
number	O
.	O
)	O
the	O
aim	O
,	O
when	O
solving	O
this	O
task	O
,	O
is	O
to	O
use	O
minimal	O
computational	O
resources	O
in	O
terms	O
of	O
the	O
amount	O
of	O
memory	O
used	O
to	O
store	O
the	O
inverse	O
mapping	O
from	O
x	O
to	O
s	O
and	O
the	O
amount	O
of	O
time	O
to	O
compute	O
the	O
inverse	O
mapping	O
.	O
and	O
,	O
preferably	O
,	O
the	O
inverse	O
mapping	O
should	O
be	O
implemented	O
in	O
such	O
a	O
way	O
that	O
further	O
new	O
strings	O
can	O
be	O
added	O
to	O
the	O
directory	B
in	O
a	O
small	O
amount	O
of	O
computer	O
time	O
too	O
.	O
some	O
standard	O
solutions	O
the	O
simplest	O
and	O
dumbest	O
solutions	O
to	O
the	O
information-retrieval	O
problem	O
are	O
a	O
look-up	O
table	O
and	O
a	O
raw	O
list	O
.	O
the	O
look-up	O
table	O
is	O
a	O
piece	O
of	O
memory	O
of	O
size	O
2n	O
log2	O
s	O
,	O
log2	O
s	O
being	O
the	O
amount	O
of	O
memory	O
required	O
to	O
store	O
an	O
integer	O
between	O
1	O
and	O
s.	O
in	O
each	O
of	O
the	O
2n	O
locations	O
,	O
we	O
put	O
a	O
zero	O
,	O
except	O
for	O
the	O
locations	O
x	O
that	O
correspond	O
to	O
strings	O
x	O
(	O
s	O
)	O
,	O
into	O
which	O
we	O
write	O
the	O
value	O
of	O
s.	O
the	O
look-up	O
table	O
is	O
a	O
simple	O
and	O
quick	O
solution	O
,	O
but	O
only	O
if	O
there	O
is	O
su	O
(	O
cid:14	O
)	O
cient	O
memory	B
for	O
the	O
table	O
,	O
and	O
if	O
the	O
cost	O
of	O
looking	O
up	O
entries	O
in	O
193	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
194	O
12	O
|	O
hash	O
codes	O
:	O
codes	O
for	O
e	O
(	O
cid:14	O
)	O
cient	O
information	B
retrieval	I
memory	O
is	O
independent	O
of	O
the	O
memory	B
size	O
.	O
but	O
in	O
our	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
the	O
task	O
,	O
we	O
assumed	O
that	O
n	O
is	O
about	O
200	O
bits	O
or	O
more	O
,	O
so	O
the	O
amount	O
of	O
memory	O
required	O
would	O
be	O
of	O
size	O
2200	O
;	O
this	O
solution	O
is	O
completely	O
out	O
of	O
the	O
question	O
.	O
bear	O
in	O
mind	O
that	O
the	O
number	O
of	O
particles	O
in	O
the	O
solar	B
system	I
is	O
only	O
about	O
2190.	O
the	O
raw	O
list	O
is	O
a	O
simple	O
list	O
of	O
ordered	O
pairs	O
(	O
s	O
;	O
x	O
(	O
s	O
)	O
)	O
ordered	O
by	O
the	O
value	O
of	O
s.	O
the	O
mapping	B
from	O
x	O
to	O
s	O
is	O
achieved	O
by	O
searching	O
through	O
the	O
list	O
of	O
strings	O
,	O
starting	O
from	O
the	O
top	O
,	O
and	O
comparing	O
the	O
incoming	O
string	O
x	O
with	O
each	O
record	O
x	O
(	O
s	O
)	O
until	O
a	O
match	O
is	O
found	O
.	O
this	O
system	O
is	O
very	O
easy	O
to	O
maintain	O
,	O
and	O
uses	O
a	O
small	O
amount	O
of	O
memory	O
,	O
about	O
sn	O
bits	O
,	O
but	O
is	O
rather	O
slow	O
to	O
use	O
,	O
since	O
on	O
average	O
(	O
cid:12	O
)	O
ve	O
million	O
pairwise	O
comparisons	O
will	O
be	O
made	O
.	O
.	O
exercise	O
12.1	O
.	O
[	O
2	O
,	O
p.202	O
]	O
show	O
that	O
the	O
average	B
time	O
taken	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
required	O
string	O
in	O
a	O
raw	O
list	O
,	O
assuming	O
that	O
the	O
original	O
names	O
were	O
chosen	O
at	O
random	B
,	O
is	O
about	O
s	O
+	O
n	O
binary	O
comparisons	O
.	O
(	O
note	O
that	O
you	O
don	O
’	O
t	O
have	O
to	O
compare	O
the	O
whole	O
string	O
of	O
length	O
n	O
,	O
since	O
a	O
comparison	O
can	O
be	O
terminated	O
as	O
soon	O
as	O
a	O
mismatch	O
occurs	O
;	O
show	O
that	O
you	O
need	O
on	O
average	O
two	O
binary	O
comparisons	O
per	O
incorrect	O
string	O
match	O
.	O
)	O
compare	O
this	O
with	O
the	O
worst-case	O
search	O
time	O
{	O
assuming	O
that	O
the	O
devil	O
chooses	O
the	O
set	B
of	O
strings	O
and	O
the	O
search	O
key	O
.	O
the	O
standard	O
way	O
in	O
which	O
phone	B
directories	O
are	O
made	O
improves	O
on	O
the	O
look-up	O
table	O
and	O
the	O
raw	O
list	O
by	O
using	O
an	O
alphabetically-ordered	O
list	O
.	O
alphabetical	O
list	O
.	O
the	O
strings	O
fx	O
(	O
s	O
)	O
g	O
are	O
sorted	O
into	O
alphabetical	O
order	O
.	O
searching	O
for	O
an	O
entry	O
now	O
usually	O
takes	O
less	O
time	O
than	O
was	O
needed	O
for	O
the	O
raw	O
list	O
because	O
we	O
can	O
take	O
advantage	O
of	O
the	O
sortedness	O
;	O
for	O
example	O
,	O
we	O
can	O
open	O
the	O
phonebook	O
at	O
its	O
middle	O
page	O
,	O
and	O
compare	O
the	O
name	O
we	O
(	O
cid:12	O
)	O
nd	O
there	O
with	O
the	O
target	O
string	O
;	O
if	O
the	O
target	O
is	O
‘	O
greater	O
’	O
than	O
the	O
middle	O
string	O
then	O
we	O
know	O
that	O
the	O
required	O
string	O
,	O
if	O
it	O
exists	O
,	O
will	O
be	O
found	O
in	O
the	O
second	O
half	O
of	O
the	O
alphabetical	O
directory	B
.	O
otherwise	O
,	O
we	O
look	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
half	O
.	O
by	O
iterating	O
this	O
splitting-in-the-middle	O
proce-	O
dure	O
,	O
we	O
can	O
identify	O
the	O
target	O
string	O
,	O
or	O
establish	O
that	O
the	O
string	O
is	O
not	O
listed	O
,	O
in	O
dlog2	O
se	O
string	O
comparisons	O
.	O
the	O
expected	O
number	O
of	O
binary	O
comparisons	O
per	O
string	O
comparison	O
will	O
tend	O
to	O
increase	O
as	O
the	O
search	O
progresses	O
,	O
but	O
the	O
total	O
number	O
of	O
binary	O
comparisons	O
required	O
will	O
be	O
no	O
greater	O
than	O
dlog2	O
sen	O
.	O
the	O
amount	O
of	O
memory	O
required	O
is	O
the	O
same	O
as	O
that	O
required	O
for	O
the	O
raw	O
list	O
.	O
adding	O
new	O
strings	O
to	O
the	O
database	O
requires	O
that	O
we	O
insert	O
them	O
in	O
the	O
correct	O
location	O
in	O
the	O
list	O
.	O
to	O
(	O
cid:12	O
)	O
nd	O
that	O
location	O
takes	O
about	O
dlog	O
2	O
se	O
binary	O
comparisons	O
.	O
can	O
we	O
improve	O
on	O
the	O
well-established	O
alphabetized	O
list	O
?	O
let	O
us	O
consider	O
our	O
task	O
from	O
some	O
new	O
viewpoints	O
.	O
the	O
task	O
is	O
to	O
construct	O
a	O
mapping	B
x	O
!	O
s	O
from	O
n	O
bits	O
to	O
log	O
2	O
s	O
bits	O
.	O
this	O
is	O
a	O
pseudo-invertible	O
mapping	B
,	O
since	O
for	O
any	O
x	O
that	O
maps	O
to	O
a	O
non-zero	O
s	O
,	O
the	O
customer	O
database	O
contains	O
the	O
pair	O
(	O
s	O
;	O
x	O
(	O
s	O
)	O
)	O
that	O
takes	O
us	O
back	O
.	O
where	O
have	O
we	O
come	O
across	O
the	O
idea	O
of	O
mapping	O
from	O
n	O
bits	O
to	O
m	O
bits	O
before	O
?	O
we	O
encountered	O
this	O
idea	O
twice	O
:	O
(	O
cid:12	O
)	O
rst	O
,	O
in	O
source	O
coding	O
,	O
we	O
studied	O
block	B
codes	O
which	O
were	O
mappings	O
from	O
strings	O
of	O
n	O
symbols	O
to	O
a	O
selection	O
of	O
one	O
label	O
in	O
a	O
list	O
.	O
the	O
task	O
of	O
information	O
retrieval	O
is	O
similar	O
to	O
the	O
task	O
(	O
which	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
12.2	O
:	O
hash	O
codes	O
195	O
string	O
length	B
number	O
of	O
strings	O
size	O
of	O
hash	O
function	B
m	O
’	O
30	O
bits	O
size	O
of	O
hash	O
table	O
n	O
’	O
200	O
s	O
’	O
223	O
t	O
=	O
2m	O
’	O
230	O
figure	O
12.2.	O
revised	O
cast	O
of	O
characters	O
.	O
we	O
never	O
actually	O
solved	O
)	O
of	O
making	O
an	O
encoder	B
for	O
a	O
typical-set	O
compression	O
code	B
.	O
the	O
second	O
time	O
that	O
we	O
mapped	O
bit	B
strings	O
to	O
bit	B
strings	O
of	O
another	O
dimensionality	O
was	O
when	O
we	O
studied	O
channel	B
codes	O
.	O
there	O
,	O
we	O
considered	O
codes	O
that	O
mapped	O
from	O
k	O
bits	O
to	O
n	O
bits	O
,	O
with	O
n	O
greater	O
than	O
k	O
,	O
and	O
we	O
made	O
theoretical	O
progress	O
using	O
random	B
codes	O
.	O
in	O
hash	O
codes	O
,	O
we	O
put	O
together	O
these	O
two	O
notions	O
.	O
we	O
will	O
study	O
random	B
codes	O
that	O
map	O
from	O
n	O
bits	O
to	O
m	O
bits	O
where	O
m	O
is	O
smaller	O
than	O
n	O
.	O
the	O
idea	O
is	O
that	O
we	O
will	O
map	O
the	O
original	O
high-dimensional	O
space	O
down	O
into	O
a	O
lower-dimensional	O
space	O
,	O
one	O
in	O
which	O
it	O
is	O
feasible	O
to	O
implement	O
the	O
dumb	O
look-up	O
table	O
method	B
which	O
we	O
rejected	O
a	O
moment	O
ago	O
.	O
12.2	O
hash	O
codes	O
first	O
we	O
will	O
describe	O
how	O
a	O
hash	B
code	I
works	O
,	O
then	O
we	O
will	O
study	O
the	O
properties	O
of	O
idealized	O
hash	O
codes	O
.	O
a	O
hash	B
code	I
implements	O
a	O
solution	O
to	O
the	O
information-	O
retrieval	O
problem	O
,	O
that	O
is	O
,	O
a	O
mapping	B
from	O
x	O
to	O
s	O
,	O
with	O
the	O
help	O
of	O
a	O
pseudo-	O
random	B
function	O
called	O
a	O
hash	B
function	I
,	O
which	O
maps	O
the	O
n	O
-bit	O
string	O
x	O
to	O
an	O
m	O
-bit	O
string	O
h	O
(	O
x	O
)	O
,	O
where	O
m	O
is	O
smaller	O
than	O
n	O
.	O
m	O
is	O
typically	O
chosen	O
such	O
that	O
the	O
‘	O
table	O
size	O
’	O
t	O
’	O
2m	O
is	O
a	O
little	O
bigger	O
than	O
s	O
{	O
say	O
,	O
ten	O
times	O
bigger	O
.	O
for	O
example	O
,	O
if	O
we	O
were	O
expecting	O
s	O
to	O
be	O
about	O
a	O
million	O
,	O
we	O
might	O
map	O
x	O
into	O
a	O
30-bit	O
hash	O
h	O
(	O
regardless	O
of	O
the	O
size	O
n	O
of	O
each	O
item	O
x	O
)	O
.	O
the	O
hash	B
function	I
is	O
some	O
(	O
cid:12	O
)	O
xed	O
deterministic	B
function	O
which	O
should	O
ideally	O
be	O
indistinguishable	O
from	O
a	O
(	O
cid:12	O
)	O
xed	O
random	B
code	I
.	O
for	O
practical	O
purposes	O
,	O
the	O
hash	B
function	I
must	O
be	O
quick	O
to	O
compute	O
.	O
two	O
simple	O
examples	O
of	O
hash	O
functions	B
are	O
:	O
division	O
method	B
.	O
the	O
table	O
size	O
t	O
is	O
a	O
prime	O
number	O
,	O
preferably	O
one	O
that	O
is	O
not	O
close	O
to	O
a	O
power	O
of	O
2.	O
the	O
hash	O
value	O
is	O
the	O
remainder	O
when	O
the	O
integer	O
x	O
is	O
divided	O
by	O
t	O
.	O
variable	O
string	O
addition	O
method	B
.	O
this	O
method	B
assumes	O
that	O
x	O
is	O
a	O
string	O
of	O
bytes	O
and	O
that	O
the	O
table	O
size	O
t	O
is	O
256.	O
the	O
characters	O
of	O
x	O
are	O
added	O
,	O
modulo	O
256.	O
this	O
hash	B
function	I
has	O
the	O
defect	O
that	O
it	O
maps	O
strings	O
that	O
are	O
anagrams	O
of	O
each	O
other	O
onto	O
the	O
same	O
hash	O
.	O
it	O
may	O
be	O
improved	O
by	O
putting	O
the	O
running	O
total	O
through	O
a	O
(	O
cid:12	O
)	O
xed	O
pseu-	O
dorandom	O
permutation	B
after	O
each	O
character	O
is	O
added	O
.	O
in	O
the	O
variable	O
string	O
exclusive-or	O
method	B
with	O
table	O
size	O
(	O
cid:20	O
)	O
65	O
536	O
,	O
the	O
string	O
is	O
hashed	O
twice	O
in	O
this	O
way	O
,	O
with	O
the	O
initial	O
running	O
total	O
being	O
set	B
to	O
0	O
and	O
1	O
respectively	O
(	O
algorithm	B
12.3	O
)	O
.	O
the	O
result	O
is	O
a	O
16-bit	O
hash	O
.	O
having	O
picked	O
a	O
hash	B
function	I
h	O
(	O
x	O
)	O
,	O
we	O
implement	O
an	O
information	B
retriever	O
as	O
follows	O
.	O
(	O
see	O
(	O
cid:12	O
)	O
gure	O
12.4	O
.	O
)	O
encoding	O
.	O
a	O
piece	O
of	O
memory	O
called	O
the	O
hash	O
table	O
is	O
created	O
of	O
size	O
2m	O
b	O
memory	B
units	O
,	O
where	O
b	O
is	O
the	O
amount	O
of	O
memory	O
needed	O
to	O
represent	O
an	O
integer	O
between	O
0	O
and	O
s.	O
this	O
table	O
is	O
initially	O
set	B
to	O
zero	O
throughout	O
.	O
each	O
memory	B
x	O
(	O
s	O
)	O
is	O
put	O
through	O
the	O
hash	B
function	I
,	O
and	O
at	O
the	O
location	O
in	O
the	O
hash	O
table	O
corresponding	O
to	O
the	O
resulting	O
vector	O
h	O
(	O
s	O
)	O
=	O
h	O
(	O
x	O
(	O
s	O
)	O
)	O
,	O
the	O
integer	O
s	O
is	O
written	O
{	O
unless	O
that	O
entry	O
in	O
the	O
hash	O
table	O
is	O
already	O
occupied	O
,	O
in	O
which	O
case	O
we	O
have	O
a	O
collision	B
between	O
x	O
(	O
s	O
)	O
and	O
some	O
earlier	O
x	O
(	O
s0	O
)	O
which	O
both	O
happen	O
to	O
have	O
the	O
same	O
hash	B
code	I
.	O
collisions	O
can	O
be	O
handled	O
in	O
various	O
ways	O
{	O
we	O
will	O
discuss	O
some	O
in	O
a	O
moment	O
{	O
but	O
(	O
cid:12	O
)	O
rst	O
let	O
us	O
complete	O
the	O
basic	O
picture	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
196	O
12	O
|	O
hash	O
codes	O
:	O
codes	O
for	O
e	O
(	O
cid:14	O
)	O
cient	O
information	B
retrieval	I
algorithm	O
12.3.	O
c	O
code	B
implementing	O
the	O
variable	O
string	O
exclusive-or	O
method	B
to	O
create	O
a	O
hash	O
h	O
in	O
the	O
range	O
0	O
:	O
:	O
:	O
65	O
535	O
from	O
a	O
string	O
x.	O
author	O
:	O
thomas	O
niemann	O
.	O
unsigned	O
char	O
rand8	O
[	O
256	O
]	O
;	O
//	O
this	O
array	O
contains	O
a	O
random	B
int	O
hash	O
(	O
char	O
*x	O
)	O
{	O
int	O
h	O
;	O
unsigned	O
char	O
h1	O
,	O
h2	O
;	O
permutation	B
from	O
0..255	O
to	O
0..255	O
//	O
x	O
is	O
a	O
pointer	B
to	O
the	O
first	O
char	O
;	O
//	O
*x	O
is	O
the	O
first	O
character	O
if	O
(	O
*x	O
==	O
0	O
)	O
return	O
0	O
;	O
h1	O
=	O
*x	O
;	O
h2	O
=	O
*x	O
+	O
1	O
;	O
x++	O
;	O
while	O
(	O
*x	O
)	O
{	O
//	O
special	O
handling	O
of	O
empty	O
string	O
//	O
initialize	O
two	O
hashes	O
//	O
proceed	O
to	O
the	O
next	O
character	O
h1	O
=	O
rand8	O
[	O
h1	O
^	O
*x	O
]	O
;	O
//	O
exclusive-or	O
with	O
the	O
two	O
hashes	O
h2	O
=	O
rand8	O
[	O
h2	O
^	O
*x	O
]	O
;	O
//	O
x++	O
;	O
and	O
put	O
through	O
the	O
randomizer	O
}	O
h	O
=	O
(	O
(	O
int	O
)	O
(	O
h1	O
)	O
<	O
<	O
8	O
)	O
|	O
//	O
end	O
of	O
string	O
is	O
reached	O
when	O
*x=0	O
//	O
shift	O
h1	O
left	O
8	O
bits	O
and	O
add	O
h2	O
(	O
int	O
)	O
h2	O
;	O
return	O
h	O
;	O
}	O
//	O
hash	O
is	O
concatenation	B
of	O
h1	O
and	O
h2	O
figure	O
12.4.	O
use	O
of	O
hash	O
functions	B
for	O
information	B
retrieval	I
.	O
for	O
each	O
string	O
x	O
(	O
s	O
)	O
,	O
the	O
hash	O
h	O
=	O
h	O
(	O
x	O
(	O
s	O
)	O
)	O
is	O
computed	O
,	O
and	O
the	O
value	O
of	O
s	O
is	O
written	O
into	O
the	O
hth	O
row	O
of	O
the	O
hash	O
table	O
.	O
blank	O
rows	O
in	O
the	O
hash	O
table	O
contain	O
the	O
value	O
zero	O
.	O
the	O
table	O
size	O
is	O
t	O
=	O
2m	O
.	O
strings	O
hash	O
function-	O
hashes	O
hash	O
table	O
(	O
cid:27	O
)	O
-m	O
bits	O
(	O
cid:27	O
)	O
n	O
bits	O
-	O
x	O
(	O
1	O
)	O
x	O
(	O
2	O
)	O
x	O
(	O
3	O
)	O
...	O
x	O
(	O
s	O
)	O
...	O
s	O
6	O
?	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
@	O
a	O
a	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
@	O
@	O
r	O
@	O
@	O
r	O
a	O
a	O
a	O
au	O
h	O
(	O
x	O
(	O
2	O
)	O
)	O
!	O
h	O
(	O
x	O
(	O
1	O
)	O
)	O
!	O
h	O
(	O
x	O
(	O
3	O
)	O
)	O
!	O
h	O
(	O
x	O
(	O
s	O
)	O
)	O
!	O
2	O
1	O
3	O
s	O
6	O
2m	O
?	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
12.3	O
:	O
collision	B
resolution	O
197	O
decoding	B
.	O
to	O
retrieve	O
a	O
piece	O
of	O
information	O
corresponding	O
to	O
a	O
target	O
vector	O
x	O
,	O
we	O
compute	O
the	O
hash	O
h	O
of	O
x	O
and	O
look	O
at	O
the	O
corresponding	O
location	O
in	O
the	O
hash	O
table	O
.	O
if	O
there	O
is	O
a	O
zero	O
,	O
then	O
we	O
know	O
immediately	O
that	O
the	O
string	O
x	O
is	O
not	O
in	O
the	O
database	O
.	O
the	O
cost	O
of	O
this	O
answer	O
is	O
the	O
cost	O
of	O
one	O
hash-function	O
evaluation	O
and	O
one	O
look-up	O
in	O
the	O
table	O
of	O
size	O
2m	O
.	O
if	O
,	O
on	O
the	O
other	O
hand	O
,	O
there	O
is	O
a	O
non-zero	O
entry	O
s	O
in	O
the	O
table	O
,	O
there	O
are	O
two	O
possibilities	O
:	O
either	O
the	O
vector	O
x	O
is	O
indeed	O
equal	O
to	O
x	O
(	O
s	O
)	O
;	O
or	O
the	O
vector	O
x	O
(	O
s	O
)	O
is	O
another	O
vector	O
that	O
happens	O
to	O
have	O
the	O
same	O
hash	B
code	I
as	O
the	O
target	O
x	O
.	O
(	O
a	O
third	O
possibility	O
is	O
that	O
this	O
non-zero	O
entry	O
might	O
have	O
something	O
to	O
do	O
with	O
our	O
yet-to-be-discussed	O
collision-resolution	O
system	O
.	O
)	O
to	O
check	O
whether	O
x	O
is	O
indeed	O
equal	O
to	O
x	O
(	O
s	O
)	O
,	O
we	O
take	O
the	O
tentative	O
answer	O
s	O
,	O
look	O
up	O
x	O
(	O
s	O
)	O
in	O
the	O
original	O
forward	O
database	O
,	O
and	O
compare	O
it	O
bit	B
by	O
bit	B
with	O
x	O
;	O
if	O
it	O
matches	O
then	O
we	O
report	O
s	O
as	O
the	O
desired	O
answer	O
.	O
this	O
successful	O
retrieval	O
has	O
an	O
overall	O
cost	O
of	O
one	O
hash-function	O
evaluation	O
,	O
one	O
look-up	O
in	O
the	O
table	O
of	O
size	O
2m	O
,	O
another	O
look-up	O
in	O
a	O
table	O
of	O
size	O
s	O
,	O
and	O
n	O
binary	O
comparisons	O
{	O
which	O
may	O
be	O
much	O
cheaper	O
than	O
the	O
simple	O
solutions	O
presented	O
in	O
section	O
12.1.	O
.	O
exercise	O
12.2	O
.	O
[	O
2	O
,	O
p.202	O
]	O
if	O
we	O
have	O
checked	O
the	O
(	O
cid:12	O
)	O
rst	O
few	O
bits	O
of	O
x	O
(	O
s	O
)	O
with	O
x	O
and	O
found	O
them	O
to	O
be	O
equal	O
,	O
what	O
is	O
the	O
probability	B
that	O
the	O
correct	O
entry	O
has	O
been	O
retrieved	O
,	O
if	O
the	O
alternative	O
hypothesis	O
is	O
that	O
x	O
is	O
actually	O
not	O
in	O
the	O
database	O
?	O
assume	O
that	O
the	O
original	O
source	O
strings	O
are	O
random	B
,	O
and	O
the	O
hash	B
function	I
is	O
a	O
random	B
hash	O
function	B
.	O
how	O
many	O
binary	O
evaluations	O
are	O
needed	O
to	O
be	O
sure	O
with	O
odds	O
of	O
a	O
billion	O
to	O
one	O
that	O
the	O
correct	O
entry	O
has	O
been	O
retrieved	O
?	O
the	O
hashing	O
method	B
of	O
information	B
retrieval	I
can	O
be	O
used	O
for	O
strings	O
x	O
of	O
arbitrary	O
length	B
,	O
if	O
the	O
hash	B
function	I
h	O
(	O
x	O
)	O
can	O
be	O
applied	O
to	O
strings	O
of	O
any	O
length	O
.	O
12.3	O
collision	B
resolution	O
we	O
will	O
study	O
two	O
ways	O
of	O
resolving	O
collisions	O
:	O
appending	O
in	O
the	O
table	O
,	O
and	O
storing	O
elsewhere	O
.	O
appending	O
in	O
table	O
when	O
encoding	O
,	O
if	O
a	O
collision	B
occurs	O
,	O
we	O
continue	O
down	O
the	O
hash	O
table	O
and	O
write	O
the	O
value	O
of	O
s	O
into	O
the	O
next	O
available	O
location	O
in	O
memory	O
that	O
currently	O
contains	O
a	O
zero	O
.	O
if	O
we	O
reach	O
the	O
bottom	O
of	O
the	O
table	O
before	O
encountering	O
a	O
zero	O
,	O
we	O
continue	O
from	O
the	O
top	O
.	O
when	O
decoding	B
,	O
if	O
we	O
compute	O
the	O
hash	B
code	I
for	O
x	O
and	O
(	O
cid:12	O
)	O
nd	O
that	O
the	O
s	O
contained	O
in	O
the	O
table	O
doesn	O
’	O
t	O
point	O
to	O
an	O
x	O
(	O
s	O
)	O
that	O
matches	O
the	O
cue	O
x	O
,	O
we	O
continue	O
down	O
the	O
hash	O
table	O
until	O
we	O
either	O
(	O
cid:12	O
)	O
nd	O
an	O
s	O
whose	O
x	O
(	O
s	O
)	O
does	O
match	O
the	O
cue	O
x	O
,	O
in	O
which	O
case	O
we	O
are	O
done	O
,	O
or	O
else	O
encounter	O
a	O
zero	O
,	O
in	O
which	O
case	O
we	O
know	O
that	O
the	O
cue	O
x	O
is	O
not	O
in	O
the	O
database	O
.	O
for	O
this	O
method	B
,	O
it	O
is	O
essential	O
that	O
the	O
table	O
be	O
substantially	O
bigger	O
in	O
size	O
than	O
s.	O
if	O
2m	O
<	O
s	O
then	O
the	O
encoding	O
rule	O
will	O
become	O
stuck	O
with	O
nowhere	O
to	O
put	O
the	O
last	O
strings	O
.	O
storing	O
elsewhere	O
a	O
more	O
robust	O
and	O
(	O
cid:13	O
)	O
exible	O
method	B
is	O
to	O
use	O
pointers	O
to	O
additional	O
pieces	O
of	O
memory	O
in	O
which	O
collided	O
strings	O
are	O
stored	O
.	O
there	O
are	O
many	O
ways	O
of	O
doing	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
198	O
12	O
|	O
hash	O
codes	O
:	O
codes	O
for	O
e	O
(	O
cid:14	O
)	O
cient	O
information	B
retrieval	I
this	O
.	O
as	O
an	O
example	O
,	O
we	O
could	O
store	O
in	O
location	O
h	O
in	O
the	O
hash	O
table	O
a	O
pointer	B
(	O
which	O
must	O
be	O
distinguishable	O
from	O
a	O
valid	O
record	O
number	O
s	O
)	O
to	O
a	O
‘	O
bucket	O
’	O
where	O
all	O
the	O
strings	O
that	O
have	O
hash	B
code	I
h	O
are	O
stored	O
in	O
a	O
sorted	O
list	O
.	O
the	O
encoder	B
sorts	O
the	O
strings	O
in	O
each	O
bucket	O
alphabetically	O
as	O
the	O
hash	O
table	O
and	O
buckets	O
are	O
created	O
.	O
the	O
decoder	B
simply	O
has	O
to	O
go	O
and	O
look	O
in	O
the	O
relevant	O
bucket	O
and	O
then	O
check	O
the	O
short	O
list	O
of	O
strings	O
that	O
are	O
there	O
by	O
a	O
brief	O
alphabetical	O
search	O
.	O
this	O
method	B
of	O
storing	O
the	O
strings	O
in	O
buckets	O
allows	O
the	O
option	O
of	O
making	O
the	O
hash	O
table	O
quite	O
small	O
,	O
which	O
may	O
have	O
practical	B
bene	O
(	O
cid:12	O
)	O
ts	O
.	O
we	O
may	O
make	O
it	O
so	O
small	O
that	O
almost	O
all	O
strings	O
are	O
involved	O
in	O
collisions	O
,	O
so	O
all	O
buckets	O
contain	O
a	O
small	O
number	O
of	O
strings	O
.	O
it	O
only	O
takes	O
a	O
small	O
number	O
of	O
binary	O
comparisons	O
to	O
identify	O
which	O
of	O
the	O
strings	O
in	O
the	O
bucket	O
matches	O
the	O
cue	O
x	O
.	O
12.4	O
planning	O
for	O
collisions	O
:	O
a	O
birthday	B
problem	O
exercise	O
12.3	O
.	O
[	O
2	O
,	O
p.202	O
]	O
if	O
we	O
wish	O
to	O
store	O
s	O
entries	O
using	O
a	O
hash	B
function	I
whose	O
output	O
has	O
m	O
bits	O
,	O
how	O
many	O
collisions	O
should	O
we	O
expect	O
to	O
happen	O
,	O
assuming	O
that	O
our	O
hash	B
function	I
is	O
an	O
ideal	O
random	B
function	O
?	O
what	O
size	O
m	O
of	O
hash	O
table	O
is	O
needed	O
if	O
we	O
would	O
like	O
the	O
expected	O
number	O
of	O
collisions	O
to	O
be	O
smaller	O
than	O
1	O
?	O
what	O
size	O
m	O
of	O
hash	O
table	O
is	O
needed	O
if	O
we	O
would	O
like	O
the	O
expected	O
number	O
of	O
collisions	O
to	O
be	O
a	O
small	O
fraction	O
,	O
say	O
1	O
%	O
,	O
of	O
s	O
?	O
[	O
notice	O
the	O
similarity	O
of	O
this	O
problem	O
to	O
exercise	O
9.20	O
(	O
p.156	O
)	O
.	O
]	O
12.5	O
other	O
roles	O
for	O
hash	O
codes	O
checking	O
arithmetic	O
if	O
you	O
wish	O
to	O
check	O
an	O
addition	O
that	O
was	O
done	O
by	O
hand	O
,	O
you	O
may	O
(	O
cid:12	O
)	O
nd	O
useful	O
the	O
method	B
of	O
casting	B
out	I
nines	I
.	O
in	O
casting	O
out	O
nines	B
,	O
one	O
(	O
cid:12	O
)	O
nds	O
the	O
sum	O
,	O
modulo	O
nine	O
,	O
of	O
all	O
the	O
digits	O
of	O
the	O
numbers	O
to	O
be	O
summed	O
and	O
compares	O
it	O
with	O
the	O
sum	O
,	O
modulo	O
nine	O
,	O
of	O
the	O
digits	O
of	O
the	O
putative	O
answer	O
.	O
[	O
with	O
a	O
little	O
practice	O
,	O
these	O
sums	O
can	O
be	O
computed	O
much	O
more	O
rapidly	O
than	O
the	O
full	O
original	O
addition	O
.	O
]	O
example	O
12.4.	O
in	O
the	O
calculation	O
shown	O
in	O
the	O
margin	O
the	O
sum	O
,	O
modulo	O
nine	O
,	O
of	O
the	O
digits	O
in	O
189+1254+238	O
is	O
7	O
,	O
and	O
the	O
sum	O
,	O
modulo	O
nine	O
,	O
of	O
1+6+8+1	O
is	O
7.	O
the	O
calculation	O
thus	O
passes	O
the	O
casting-out-nines	O
test	B
.	O
casting	B
out	I
nines	I
gives	O
a	O
simple	O
example	O
of	O
a	O
hash	B
function	I
.	O
for	O
any	O
addition	O
expression	O
of	O
the	O
form	O
a	O
+	O
b	O
+	O
c	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
,	O
where	O
a	O
;	O
b	O
;	O
c	O
;	O
:	O
:	O
:	O
are	O
decimal	O
numbers	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
h	O
2	O
f0	O
;	O
1	O
;	O
2	O
;	O
3	O
;	O
4	O
;	O
5	O
;	O
6	O
;	O
7	O
;	O
8g	O
by	O
h	O
(	O
a	O
+	O
b	O
+	O
c	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
=	O
sum	O
modulo	O
nine	O
of	O
all	O
digits	O
in	O
a	O
;	O
b	O
;	O
c	O
;	O
(	O
12.1	O
)	O
then	O
it	O
is	O
nice	O
property	O
of	O
decimal	O
arithmetic	O
that	O
if	O
a	O
+	O
b	O
+	O
c	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
=	O
m	O
+	O
n	O
+	O
o	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
12.2	O
)	O
then	O
the	O
hashes	O
h	O
(	O
a	O
+	O
b	O
+	O
c	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
and	O
h	O
(	O
m	O
+	O
n	O
+	O
o	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
are	O
equal	O
.	O
.	O
exercise	O
12.5	O
.	O
[	O
1	O
,	O
p.203	O
]	O
what	O
evidence	O
does	O
a	O
correct	O
casting-out-nines	O
match	O
give	O
in	O
favour	O
of	O
the	O
hypothesis	O
that	O
the	O
addition	O
has	O
been	O
done	O
cor-	O
rectly	O
?	O
189	O
+1254	O
+	O
238	O
1681	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
199	O
12.5	O
:	O
other	O
roles	O
for	O
hash	O
codes	O
error	B
detection	I
among	O
friends	O
are	O
two	O
(	O
cid:12	O
)	O
les	O
the	O
same	O
?	O
if	O
the	O
(	O
cid:12	O
)	O
les	O
are	O
on	O
the	O
same	O
computer	B
,	O
we	O
could	O
just	O
compare	O
them	O
bit	B
by	O
bit	B
.	O
but	O
if	O
the	O
two	O
(	O
cid:12	O
)	O
les	O
are	O
on	O
separate	O
machines	O
,	O
it	O
would	O
be	O
nice	O
to	O
have	O
a	O
way	O
of	O
con	O
(	O
cid:12	O
)	O
rming	O
that	O
two	O
(	O
cid:12	O
)	O
les	O
are	O
identical	O
without	O
having	O
to	O
transfer	O
one	O
of	O
the	O
(	O
cid:12	O
)	O
les	O
from	O
a	O
to	O
b	O
.	O
[	O
and	O
even	O
if	O
we	O
did	O
transfer	O
one	O
of	O
the	O
(	O
cid:12	O
)	O
les	O
,	O
we	O
would	O
still	O
like	O
a	O
way	O
to	O
con	O
(	O
cid:12	O
)	O
rm	O
whether	O
it	O
has	O
been	O
received	O
without	O
modi	O
(	O
cid:12	O
)	O
cations	O
!	O
]	O
this	O
problem	O
can	O
be	O
solved	O
using	O
hash	O
codes	O
.	O
let	O
alice	O
and	O
bob	O
be	O
the	O
holders	O
of	O
the	O
two	O
(	O
cid:12	O
)	O
les	O
;	O
alice	O
sent	O
the	O
(	O
cid:12	O
)	O
le	O
to	O
bob	O
,	O
and	O
they	O
wish	O
to	O
con	O
(	O
cid:12	O
)	O
rm	O
it	O
has	O
been	O
received	O
without	O
error	O
.	O
if	O
alice	O
computes	O
the	O
hash	O
of	O
her	O
(	O
cid:12	O
)	O
le	O
and	O
sends	O
it	O
to	O
bob	O
,	O
and	O
bob	O
computes	O
the	O
hash	O
of	O
his	O
(	O
cid:12	O
)	O
le	O
,	O
using	O
the	O
same	O
m	O
-bit	O
hash	B
function	I
,	O
and	O
the	O
two	O
hashes	O
match	O
,	O
then	O
bob	O
can	O
deduce	O
that	O
the	O
two	O
(	O
cid:12	O
)	O
les	O
are	O
almost	O
surely	O
the	O
same	O
.	O
example	O
12.6.	O
what	O
is	O
the	O
probability	O
of	O
a	O
false	O
negative	O
,	O
i.e.	O
,	O
the	O
probability	B
,	O
given	O
that	O
the	O
two	O
(	O
cid:12	O
)	O
les	O
do	O
di	O
(	O
cid:11	O
)	O
er	O
,	O
that	O
the	O
two	O
hashes	O
are	O
nevertheless	O
identical	O
?	O
if	O
we	O
assume	O
that	O
the	O
hash	B
function	I
is	O
random	B
and	O
that	O
the	O
process	O
that	O
causes	O
the	O
(	O
cid:12	O
)	O
les	O
to	O
di	O
(	O
cid:11	O
)	O
er	O
knows	O
nothing	O
about	O
the	O
hash	B
function	I
,	O
then	O
the	O
probability	O
of	O
a	O
false	O
negative	O
is	O
2	O
(	O
cid:0	O
)	O
m	O
.	O
2	O
a	O
32-bit	O
hash	O
gives	O
a	O
probability	O
of	O
false	O
negative	O
of	O
about	O
10	O
(	O
cid:0	O
)	O
10.	O
it	O
is	O
common	O
practice	O
to	O
use	O
a	O
linear	B
hash	O
function	B
called	O
a	O
32-bit	O
cyclic	B
redundancy	O
check	O
to	O
detect	O
errors	B
in	O
(	O
cid:12	O
)	O
les	O
.	O
(	O
a	O
cyclic	B
redundancy	O
check	O
is	O
a	O
set	B
of	O
32	O
parity-	O
check	O
bits	O
similar	O
to	O
the	O
3	O
parity-check	B
bits	I
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
.	O
)	O
to	O
have	O
a	O
false-negative	O
rate	B
smaller	O
than	O
one	O
in	O
a	O
billion	O
,	O
m	O
=	O
32	O
bits	O
is	O
plenty	O
,	O
if	O
the	O
errors	B
are	O
produced	O
by	O
noise	O
.	O
.	O
exercise	O
12.7	O
.	O
[	O
2	O
,	O
p.203	O
]	O
such	O
a	O
simple	O
parity-check	O
code	B
only	O
detects	O
errors	B
;	O
it	O
doesn	O
’	O
t	O
help	O
correct	O
them	O
.	O
since	O
error-correcting	B
codes	I
exist	O
,	O
why	O
not	O
use	O
one	O
of	O
them	O
to	O
get	O
some	O
error-correcting	O
capability	O
too	O
?	O
tamper	B
detection	I
what	O
if	O
the	O
di	O
(	O
cid:11	O
)	O
erences	O
between	O
the	O
two	O
(	O
cid:12	O
)	O
les	O
are	O
not	O
simply	O
‘	O
noise	B
’	O
,	O
but	O
are	O
introduced	O
by	O
an	O
adversary	O
,	O
a	O
clever	O
forger	O
called	O
fiona	O
,	O
who	O
modi	O
(	O
cid:12	O
)	O
es	O
the	O
original	O
(	O
cid:12	O
)	O
le	O
to	O
make	O
a	O
forgery	B
that	O
purports	O
to	O
be	O
alice	O
’	O
s	O
(	O
cid:12	O
)	O
le	O
?	O
how	O
can	O
alice	O
make	O
a	O
digital	B
signature	I
for	O
the	O
(	O
cid:12	O
)	O
le	O
so	O
that	O
bob	O
can	O
con	O
(	O
cid:12	O
)	O
rm	O
that	O
no-one	O
has	O
tampered	O
with	O
the	O
(	O
cid:12	O
)	O
le	O
?	O
and	O
how	O
can	O
we	O
prevent	O
fiona	O
from	O
listening	O
in	O
on	O
alice	O
’	O
s	O
signature	O
and	O
attaching	O
it	O
to	O
other	O
(	O
cid:12	O
)	O
les	O
?	O
let	O
’	O
s	O
assume	O
that	O
alice	O
computes	O
a	O
hash	B
function	I
for	O
the	O
(	O
cid:12	O
)	O
le	O
and	O
sends	O
it	O
securely	O
to	O
bob	O
.	O
if	O
alice	O
computes	O
a	O
simple	O
hash	O
function	B
for	O
the	O
(	O
cid:12	O
)	O
le	O
like	O
the	O
linear	B
cyclic	O
redundancy	B
check	O
,	O
and	O
fiona	O
knows	O
that	O
this	O
is	O
the	O
method	B
of	O
verifying	O
the	O
(	O
cid:12	O
)	O
le	O
’	O
s	O
integrity	O
,	O
fiona	O
can	O
make	O
her	O
chosen	O
modi	O
(	O
cid:12	O
)	O
cations	O
to	O
the	O
(	O
cid:12	O
)	O
le	O
and	O
then	O
easily	O
identify	O
(	O
by	O
linear	O
algebra	O
)	O
a	O
further	O
32-or-so	O
single	O
bits	O
that	O
,	O
when	O
(	O
cid:13	O
)	O
ipped	O
,	O
restore	O
the	O
hash	B
function	I
of	O
the	O
(	O
cid:12	O
)	O
le	O
to	O
its	O
original	O
value	O
.	O
linear	B
hash	O
functions	B
give	O
no	O
security	B
against	O
forgers	O
.	O
we	O
must	O
therefore	O
require	O
that	O
the	O
hash	B
function	I
be	O
hard	O
to	O
invert	O
so	O
that	O
no-one	O
can	O
construct	O
a	O
tampering	O
that	O
leaves	O
the	O
hash	B
function	I
una	O
(	O
cid:11	O
)	O
ected	O
.	O
we	O
would	O
still	O
like	O
the	O
hash	B
function	I
to	O
be	O
easy	O
to	O
compute	O
,	O
however	O
,	O
so	O
that	O
bob	O
doesn	O
’	O
t	O
have	O
to	O
do	O
hours	O
of	O
work	O
to	O
verify	O
every	O
(	O
cid:12	O
)	O
le	O
he	O
received	O
.	O
such	O
a	O
hash	B
function	I
{	O
easy	O
to	O
compute	O
,	O
but	O
hard	O
to	O
invert	O
{	O
is	O
called	O
a	O
one-way	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
200	O
12	O
|	O
hash	O
codes	O
:	O
codes	O
for	O
e	O
(	O
cid:14	O
)	O
cient	O
information	B
retrieval	I
hash	O
function	B
.	O
finding	O
such	O
functions	B
is	O
one	O
of	O
the	O
active	O
research	O
areas	O
of	O
cryptography	O
.	O
a	O
hash	B
function	I
that	O
is	O
widely	O
used	O
in	O
the	O
free	O
software	O
community	O
to	O
con	O
(	O
cid:12	O
)	O
rm	O
that	O
two	O
(	O
cid:12	O
)	O
les	O
do	O
not	O
di	O
(	O
cid:11	O
)	O
er	O
is	O
md5	O
,	O
which	O
produces	O
a	O
128-bit	O
hash	O
.	O
the	O
details	O
of	O
how	O
it	O
works	O
are	O
quite	O
complicated	O
,	O
involving	O
convoluted	O
exclusive-	O
or-ing	O
and	O
if-ing	O
and	O
and-ing.1	O
even	O
with	O
a	O
good	B
one-way	O
hash	B
function	I
,	O
the	O
digital	B
signatures	I
described	O
above	O
are	O
still	O
vulnerable	O
to	O
attack	O
,	O
if	O
fiona	O
has	O
access	O
to	O
the	O
hash	B
function	I
.	O
fiona	O
could	O
take	O
the	O
tampered	O
(	O
cid:12	O
)	O
le	O
and	O
hunt	O
for	O
a	O
further	O
tiny	O
modi	O
(	O
cid:12	O
)	O
cation	O
to	O
it	O
such	O
that	O
its	O
hash	O
matches	O
the	O
original	O
hash	O
of	O
alice	O
’	O
s	O
(	O
cid:12	O
)	O
le	O
.	O
this	O
would	O
take	O
some	O
time	O
{	O
on	O
average	O
,	O
about	O
232	O
attempts	O
,	O
if	O
the	O
hash	B
function	I
has	O
32	O
bits	O
{	O
but	O
eventually	O
fiona	O
would	O
(	O
cid:12	O
)	O
nd	O
a	O
tampered	O
(	O
cid:12	O
)	O
le	O
that	O
matches	O
the	O
given	O
hash	O
.	O
to	O
be	O
secure	O
against	O
forgery	B
,	O
digital	B
signatures	I
must	O
either	O
have	O
enough	O
bits	O
for	O
such	O
a	O
random	B
search	O
to	O
take	O
too	O
long	O
,	O
or	O
the	O
hash	B
function	I
itself	O
must	O
be	O
kept	O
secret	B
.	O
fiona	O
has	O
to	O
hash	O
2m	O
(	O
cid:12	O
)	O
les	O
to	O
cheat	B
.	O
232	O
(	O
cid:12	O
)	O
le	O
modi	O
(	O
cid:12	O
)	O
cations	O
is	O
not	O
very	O
many	O
,	O
so	O
a	O
32-bit	O
hash	B
function	I
is	O
not	O
large	O
enough	O
for	O
forgery	O
prevention	O
.	O
another	O
person	O
who	O
might	O
have	O
a	O
motivation	O
for	O
forgery	O
is	O
alice	O
herself	O
.	O
for	O
example	O
,	O
she	O
might	O
be	O
making	O
a	O
bet	B
on	O
the	O
outcome	O
of	O
a	O
race	B
,	O
without	O
wishing	O
to	O
broadcast	B
her	O
prediction	B
publicly	O
;	O
a	O
method	B
for	O
placing	O
bets	O
would	O
be	O
for	O
her	O
to	O
send	O
to	O
bob	O
the	O
bookie	O
the	O
hash	O
of	O
her	O
bet	B
.	O
later	O
on	O
,	O
she	O
could	O
send	O
bob	O
the	O
details	O
of	O
her	O
bet	B
.	O
everyone	O
can	O
con	O
(	O
cid:12	O
)	O
rm	O
that	O
her	O
bet	B
is	O
consis-	O
tent	O
with	O
the	O
previously	O
publicized	O
hash	O
.	O
[	O
this	O
method	B
of	O
secret	B
publication	O
was	O
used	O
by	O
isaac	O
newton	O
and	O
robert	O
hooke	O
when	O
they	O
wished	O
to	O
establish	O
priority	O
for	O
scienti	O
(	O
cid:12	O
)	O
c	O
ideas	O
without	O
revealing	O
them	O
.	O
hooke	O
’	O
s	O
hash	B
function	I
was	O
alphabetization	O
as	O
illustrated	O
by	O
the	O
conversion	O
of	O
ut	O
tensio	O
,	O
sic	O
vis	O
into	O
the	O
anagram	B
ceiiinosssttuv	O
.	O
]	O
such	O
a	O
protocol	B
relies	O
on	O
the	O
assumption	O
that	O
alice	O
can	O
not	O
change	O
her	O
bet	B
after	O
the	O
event	O
without	O
the	O
hash	O
coming	O
out	O
wrong	O
.	O
how	O
big	O
a	O
hash	B
function	I
do	O
we	O
need	O
to	O
use	O
to	O
ensure	O
that	O
alice	O
can	O
not	O
cheat	B
?	O
the	O
answer	O
is	O
di	O
(	O
cid:11	O
)	O
erent	O
from	O
the	O
size	O
of	O
the	O
hash	O
we	O
needed	O
in	O
order	O
to	O
defeat	O
fiona	O
above	O
,	O
because	O
alice	O
is	O
the	O
author	O
of	O
both	O
(	O
cid:12	O
)	O
les	O
.	O
alice	O
could	O
cheat	B
by	O
searching	O
for	O
two	O
(	O
cid:12	O
)	O
les	O
that	O
have	O
identical	O
hashes	O
to	O
each	O
other	O
.	O
for	O
example	O
,	O
if	O
she	O
’	O
d	O
like	O
to	O
cheat	B
by	O
placing	O
two	O
bets	O
for	O
the	O
price	O
of	O
one	O
,	O
she	O
could	O
make	O
a	O
large	O
number	O
n1	O
of	O
versions	O
of	O
bet	O
one	O
(	O
di	O
(	O
cid:11	O
)	O
ering	O
from	O
each	O
other	O
in	O
minor	O
details	O
only	O
)	O
,	O
and	O
a	O
large	O
number	O
n2	O
of	O
versions	O
of	O
bet	O
two	O
,	O
and	O
hash	O
them	O
all	O
.	O
if	O
there	O
’	O
s	O
a	O
collision	B
between	O
the	O
hashes	O
of	O
two	O
bets	O
of	O
di	O
(	O
cid:11	O
)	O
erent	O
types	O
,	O
then	O
she	O
can	O
submit	O
the	O
common	O
hash	O
and	O
thus	O
buy	O
herself	O
the	O
option	O
of	O
placing	O
either	O
bet	B
.	O
example	O
12.8.	O
if	O
the	O
hash	O
has	O
m	O
bits	O
,	O
how	O
big	O
do	O
n1	O
and	O
n2	O
need	O
to	O
be	O
for	O
alice	O
to	O
have	O
a	O
good	B
chance	O
of	O
(	O
cid:12	O
)	O
nding	O
two	O
di	O
(	O
cid:11	O
)	O
erent	O
bets	O
with	O
the	O
same	O
hash	O
?	O
this	O
is	O
a	O
birthday	B
problem	O
like	O
exercise	O
9.20	O
(	O
p.156	O
)	O
.	O
if	O
there	O
are	O
n1	O
montagues	O
and	O
n2	O
capulets	O
at	O
a	O
party	O
,	O
and	O
each	O
is	O
assigned	O
a	O
‘	O
birthday	B
’	O
of	O
m	O
bits	O
,	O
the	O
expected	O
number	O
of	O
collisions	O
between	O
a	O
montague	O
and	O
a	O
capulet	O
is	O
n1n22	O
(	O
cid:0	O
)	O
m	O
;	O
(	O
12.3	O
)	O
1http	O
:	O
//www.freesoft.org/cie/rfc/1321/3.htm	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
12.6	O
:	O
further	O
exercises	O
201	O
so	O
to	O
minimize	O
the	O
number	O
of	O
(	O
cid:12	O
)	O
les	O
hashed	O
,	O
n1	O
+	O
n2	O
,	O
alice	O
should	O
make	O
n1	O
and	O
n2	O
equal	O
,	O
and	O
will	O
need	O
to	O
hash	O
about	O
2m=2	O
(	O
cid:12	O
)	O
les	O
until	O
she	O
(	O
cid:12	O
)	O
nds	O
two	O
that	O
match	O
.	O
2	O
alice	O
has	O
to	O
hash	O
2m=2	O
(	O
cid:12	O
)	O
les	O
to	O
cheat	B
.	O
[	O
this	O
is	O
the	O
square	B
root	O
of	O
the	O
number	O
of	O
hashes	O
fiona	O
had	O
to	O
make	O
.	O
]	O
if	O
alice	O
has	O
the	O
use	O
of	O
c	O
=	O
106	O
computers	O
for	O
t	O
=	O
10	O
years	O
,	O
each	O
computer	B
taking	O
t	O
=	O
1	O
ns	O
to	O
evaluate	O
a	O
hash	O
,	O
the	O
bet-communication	O
system	O
is	O
secure	O
against	O
alice	O
’	O
s	O
dishonesty	O
only	O
if	O
m	O
(	O
cid:29	O
)	O
2	O
log2	O
ct	O
=t	O
’	O
160	O
bits	O
.	O
further	O
reading	O
the	O
bible	O
for	O
hash	O
codes	O
is	O
volume	B
3	O
of	O
knuth	O
(	O
1968	O
)	O
.	O
i	O
highly	O
recommend	O
the	O
story	O
of	O
doug	O
mcilroy	O
’	O
s	O
spell	B
program	O
,	O
as	O
told	O
in	O
section	O
13.8	O
of	O
programming	O
pearls	O
(	O
bentley	O
,	O
2000	O
)	O
.	O
this	O
astonishing	O
piece	O
of	O
software	O
makes	O
use	O
of	O
a	O
64-	O
kilobyte	O
data	O
structure	O
to	O
store	O
the	O
spellings	O
of	O
all	O
the	O
words	O
of	O
75	O
000-word	O
dictionary	B
.	O
12.6	O
further	O
exercises	O
exercise	O
12.9	O
.	O
[	O
1	O
]	O
what	O
is	O
the	O
shortest	O
the	O
address	B
on	O
a	O
typical	B
international	O
letter	O
could	O
be	O
,	O
if	O
it	O
is	O
to	O
get	O
to	O
a	O
unique	O
human	B
recipient	O
?	O
(	O
assume	O
the	O
permitted	O
characters	O
are	O
[	O
a-z,0-9	O
]	O
.	O
)	O
how	O
long	O
are	O
typical	B
email	O
addresses	O
?	O
exercise	O
12.10	O
.	O
[	O
2	O
,	O
p.203	O
]	O
how	O
long	O
does	O
a	O
piece	O
of	O
text	O
need	O
to	O
be	O
for	O
you	O
to	O
be	O
pretty	O
sure	O
that	O
no	O
human	B
has	O
written	O
that	O
string	O
of	O
characters	O
before	O
?	O
how	O
many	O
notes	O
are	O
there	O
in	O
a	O
new	O
melody	B
that	O
has	O
not	O
been	O
composed	O
before	O
?	O
.	O
exercise	O
12.11	O
.	O
[	O
3	O
,	O
p.204	O
]	O
pattern	B
recognition	I
by	O
molecules	B
.	O
some	O
proteins	O
produced	O
in	O
a	O
cell	O
have	O
a	O
regulatory	B
role	O
.	O
a	O
regulatory	B
protein	O
controls	O
the	O
transcription	O
of	O
speci	O
(	O
cid:12	O
)	O
c	O
genes	B
in	O
the	O
genome	B
.	O
this	O
control	O
often	O
involves	O
the	O
protein	B
’	O
s	O
binding	O
to	O
a	O
particular	O
dna	O
sequence	B
in	O
the	O
vicinity	O
of	O
the	O
regulated	O
gene	O
.	O
the	O
presence	O
of	O
the	O
bound	O
protein	B
either	O
promotes	O
or	O
inhibits	O
transcription	O
of	O
the	O
gene	O
.	O
(	O
a	O
)	O
use	O
information-theoretic	O
arguments	O
to	O
obtain	O
a	O
lower	O
bound	B
on	O
the	O
size	O
of	O
a	O
typical	B
protein	O
that	O
acts	O
as	O
a	O
regulator	O
speci	O
(	O
cid:12	O
)	O
c	O
to	O
one	O
gene	O
in	O
the	O
whole	O
human	B
genome	O
.	O
assume	O
that	O
the	O
genome	B
is	O
a	O
sequence	B
of	O
3	O
(	O
cid:2	O
)	O
109	O
nucleotides	O
drawn	O
from	O
a	O
four	O
letter	O
alphabet	O
fa	O
;	O
c	O
;	O
g	O
;	O
tg	O
;	O
a	O
protein	B
is	O
a	O
sequence	B
of	O
amino	O
acids	O
drawn	O
from	O
a	O
twenty	O
letter	O
alphabet	O
.	O
[	O
hint	O
:	O
establish	O
how	O
long	O
the	O
recognized	O
dna	O
sequence	B
has	O
to	O
be	O
in	O
order	O
for	O
that	O
sequence	B
to	O
be	O
unique	O
to	O
the	O
vicinity	O
of	O
one	O
gene	O
,	O
treating	O
the	O
rest	O
of	O
the	O
genome	O
as	O
a	O
random	B
sequence	O
.	O
then	O
discuss	O
how	O
big	O
the	O
protein	B
must	O
be	O
to	O
recognize	O
a	O
sequence	B
of	O
that	O
length	B
uniquely	O
.	O
]	O
(	O
b	O
)	O
some	O
of	O
the	O
sequences	O
recognized	O
by	O
dna-binding	O
regulatory	B
pro-	O
teins	O
consist	O
of	O
a	O
subsequence	O
that	O
is	O
repeated	O
twice	O
or	O
more	O
,	O
for	O
example	O
the	O
sequence	B
gccccccacccctgccccc	O
(	O
12.4	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
202	O
12	O
|	O
hash	O
codes	O
:	O
codes	O
for	O
e	O
(	O
cid:14	O
)	O
cient	O
information	B
retrieval	I
is	O
a	O
binding	O
site	O
found	O
upstream	O
of	O
the	O
alpha-actin	O
gene	O
in	O
humans	O
.	O
does	O
the	O
fact	O
that	O
some	O
binding	O
sites	O
consist	O
of	O
a	O
repeated	O
subse-	O
quence	O
in	O
(	O
cid:13	O
)	O
uence	O
your	O
answer	O
to	O
part	O
(	O
a	O
)	O
?	O
12.7	O
solutions	O
solution	O
to	O
exercise	O
12.1	O
(	O
p.194	O
)	O
.	O
first	O
imagine	O
comparing	O
the	O
string	O
x	O
with	O
another	O
random	B
string	O
x	O
(	O
s	O
)	O
.	O
the	O
probability	B
that	O
the	O
(	O
cid:12	O
)	O
rst	O
bits	O
of	O
the	O
two	O
strings	O
match	O
is	O
1=2	O
.	O
the	O
probability	B
that	O
the	O
second	O
bits	O
match	O
is	O
1=2	O
.	O
as-	O
suming	O
we	O
stop	O
comparing	O
once	O
we	O
hit	O
the	O
(	O
cid:12	O
)	O
rst	O
mismatch	O
,	O
the	O
expected	O
number	O
of	O
matches	O
is	O
1	O
,	O
so	O
the	O
expected	O
number	O
of	O
comparisons	O
is	O
2	O
(	O
exercise	O
2.34	O
,	O
p.38	O
)	O
.	O
assuming	O
the	O
correct	O
string	O
is	O
located	O
at	O
random	B
in	O
the	O
raw	O
list	O
,	O
we	O
will	O
have	O
to	O
compare	O
with	O
an	O
average	B
of	O
s=2	O
strings	O
before	O
we	O
(	O
cid:12	O
)	O
nd	O
it	O
,	O
which	O
costs	O
2s=2	O
binary	O
comparisons	O
;	O
and	O
comparing	O
the	O
correct	O
strings	O
takes	O
n	O
binary	O
comparisons	O
,	O
giving	O
a	O
total	O
expectation	B
of	O
s	O
+	O
n	O
binary	O
comparisons	O
,	O
if	O
the	O
strings	O
are	O
chosen	O
at	O
random	B
.	O
in	O
the	O
worst	O
case	O
(	O
which	O
may	O
indeed	O
happen	O
in	O
practice	O
)	O
,	O
the	O
other	O
strings	O
are	O
very	O
similar	O
to	O
the	O
search	O
key	O
,	O
so	O
that	O
a	O
lengthy	O
sequence	B
of	O
comparisons	O
is	O
needed	O
to	O
(	O
cid:12	O
)	O
nd	O
each	O
mismatch	O
.	O
the	O
worst	O
case	O
is	O
when	O
the	O
correct	O
string	O
is	O
last	O
in	O
the	O
list	O
,	O
and	O
all	O
the	O
other	O
strings	O
di	O
(	O
cid:11	O
)	O
er	O
in	O
the	O
last	O
bit	B
only	O
,	O
giving	O
a	O
requirement	O
of	O
sn	O
binary	O
comparisons	O
.	O
solution	O
to	O
exercise	O
12.2	O
(	O
p.197	O
)	O
.	O
the	O
likelihood	B
ratio	O
for	O
the	O
two	O
hypotheses	O
,	O
h0	O
:	O
x	O
(	O
s	O
)	O
=	O
x	O
,	O
and	O
h1	O
:	O
x	O
(	O
s	O
)	O
6=	O
x	O
,	O
contributed	O
by	O
the	O
datum	O
‘	O
the	O
(	O
cid:12	O
)	O
rst	O
bits	O
of	O
x	O
(	O
s	O
)	O
and	O
x	O
are	O
equal	O
’	O
is	O
p	O
(	O
datum	O
jh0	O
)	O
p	O
(	O
datum	O
jh1	O
)	O
=	O
1	O
1=2	O
=	O
2	O
:	O
(	O
12.5	O
)	O
if	O
the	O
(	O
cid:12	O
)	O
rst	O
r	O
bits	O
all	O
match	O
,	O
the	O
likelihood	B
ratio	O
is	O
2r	O
to	O
one	O
.	O
on	O
(	O
cid:12	O
)	O
nding	O
that	O
30	O
bits	O
match	O
,	O
the	O
odds	B
are	O
a	O
billion	O
to	O
one	O
in	O
favour	O
of	O
h0	O
,	O
assuming	O
we	O
start	O
from	O
even	O
odds	B
.	O
[	O
for	O
a	O
complete	O
answer	O
,	O
we	O
should	O
compute	O
the	O
evidence	B
given	O
by	O
the	O
prior	B
information	O
that	O
the	O
hash	O
entry	O
s	O
has	O
been	O
found	O
in	O
the	O
table	O
at	O
h	O
(	O
x	O
)	O
.	O
this	O
fact	O
gives	O
further	O
evidence	B
in	O
favour	O
of	O
h0	O
.	O
]	O
solution	O
to	O
exercise	O
12.3	O
(	O
p.198	O
)	O
.	O
let	O
the	O
hash	B
function	I
have	O
an	O
output	O
al-	O
phabet	O
of	O
size	O
t	O
=	O
2m	O
.	O
if	O
m	O
were	O
equal	O
to	O
log2	O
s	O
then	O
we	O
would	O
have	O
exactly	O
enough	O
bits	O
for	O
each	O
entry	O
to	O
have	O
its	O
own	O
unique	O
hash	O
.	O
the	O
probability	B
that	O
one	O
particular	O
pair	O
of	O
entries	O
collide	O
under	O
a	O
random	B
hash	O
function	B
is	O
1=t	O
.	O
the	O
number	O
of	O
pairs	O
is	O
s	O
(	O
s	O
(	O
cid:0	O
)	O
1	O
)	O
=2	O
.	O
so	O
the	O
expected	O
number	O
of	O
collisions	O
between	O
pairs	O
is	O
exactly	O
(	O
12.6	O
)	O
s	O
(	O
s	O
(	O
cid:0	O
)	O
1	O
)	O
=	O
(	O
2t	O
)	O
:	O
if	O
we	O
would	O
like	O
this	O
to	O
be	O
smaller	O
than	O
1	O
,	O
then	O
we	O
need	O
t	O
>	O
s	O
(	O
s	O
(	O
cid:0	O
)	O
1	O
)	O
=2	O
so	O
m	O
>	O
2	O
log2	O
s	O
:	O
(	O
12.7	O
)	O
we	O
need	O
twice	O
as	O
many	O
bits	O
as	O
the	O
number	O
of	O
bits	O
,	O
log	O
2	O
s	O
,	O
that	O
would	O
be	O
su	O
(	O
cid:14	O
)	O
cient	O
to	O
give	O
each	O
entry	O
a	O
unique	O
name	O
.	O
if	O
we	O
are	O
happy	O
to	O
have	O
occasional	O
collisions	O
,	O
involving	O
a	O
fraction	O
f	O
of	O
the	O
names	O
s	O
,	O
then	O
we	O
need	O
t	O
>	O
s=f	O
(	O
since	O
the	O
probability	B
that	O
one	O
particular	O
name	O
is	O
collided-with	O
is	O
f	O
’	O
s=t	O
)	O
so	O
m	O
>	O
log2	O
s	O
+	O
log2	O
[	O
1=f	O
]	O
;	O
(	O
12.8	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
12.7	O
:	O
solutions	O
203	O
which	O
means	O
for	O
f	O
’	O
0:01	O
that	O
we	O
need	O
an	O
extra	O
7	O
bits	O
above	O
log2	O
s.	O
the	O
important	O
point	O
to	O
note	O
is	O
the	O
scaling	B
of	O
t	O
with	O
s	O
in	O
the	O
two	O
cases	O
(	O
12.7	O
,	O
12.8	O
)	O
.	O
if	O
we	O
want	O
the	O
hash	B
function	I
to	O
be	O
collision-free	O
,	O
then	O
we	O
must	O
have	O
t	O
greater	O
than	O
(	O
cid:24	O
)	O
s2	O
.	O
if	O
we	O
are	O
happy	O
to	O
have	O
a	O
small	O
frequency	B
of	O
collisions	O
,	O
then	O
t	O
needs	O
to	O
be	O
of	O
order	O
s	O
only	O
.	O
solution	O
to	O
exercise	O
12.5	O
(	O
p.198	O
)	O
.	O
the	O
posterior	B
probability	I
ratio	O
for	O
the	O
two	O
hypotheses	O
,	O
h+	O
=	O
‘	O
calculation	O
correct	O
’	O
and	O
h	O
(	O
cid:0	O
)	O
=	O
‘	O
calculation	O
incorrect	O
’	O
is	O
the	O
product	O
of	O
the	O
prior	B
probability	O
ratio	O
p	O
(	O
h+	O
)	O
=p	O
(	O
h	O
(	O
cid:0	O
)	O
)	O
and	O
the	O
likelihood	B
ratio	O
,	O
p	O
(	O
match	O
jh+	O
)	O
=p	O
(	O
match	O
jh	O
(	O
cid:0	O
)	O
)	O
.	O
this	O
second	O
factor	O
is	O
the	O
answer	O
to	O
the	O
question	O
.	O
the	O
numerator	O
p	O
(	O
match	O
jh+	O
)	O
is	O
equal	O
to	O
1.	O
the	O
denominator	O
’	O
s	O
value	O
depends	O
on	O
our	O
model	B
of	O
errors	B
.	O
if	O
we	O
know	O
that	O
the	O
human	B
calculator	O
is	O
prone	O
to	O
errors	B
involving	O
multiplication	O
of	O
the	O
answer	O
by	O
10	O
,	O
or	O
to	O
transposition	O
of	O
adjacent	O
digits	O
,	O
neither	O
of	O
which	O
a	O
(	O
cid:11	O
)	O
ects	O
the	O
hash	O
value	O
,	O
then	O
p	O
(	O
match	O
jh	O
(	O
cid:0	O
)	O
)	O
could	O
be	O
equal	O
to	O
1	O
also	O
,	O
so	O
that	O
the	O
correct	O
match	O
gives	O
no	O
evidence	B
in	O
favour	O
of	O
h+	O
.	O
but	O
if	O
we	O
assume	O
that	O
errors	B
are	O
‘	O
random	B
from	O
the	O
point	O
of	O
view	O
of	O
the	O
hash	O
function	B
’	O
then	O
the	O
probability	O
of	O
a	O
false	O
positive	O
is	O
p	O
(	O
match	O
jh	O
(	O
cid:0	O
)	O
)	O
=	O
1=9	O
,	O
and	O
the	O
correct	O
match	O
gives	O
evidence	B
9:1	O
in	O
favour	O
of	O
h+	O
.	O
solution	O
to	O
exercise	O
12.7	O
(	O
p.199	O
)	O
.	O
if	O
you	O
add	O
a	O
tiny	O
m	O
=	O
32	O
extra	O
bits	O
of	O
hash	O
to	O
a	O
huge	O
n	O
-bit	O
(	O
cid:12	O
)	O
le	O
you	O
get	O
pretty	O
good	B
error	O
detection	O
{	O
the	O
probability	B
that	O
an	O
error	O
is	O
undetected	O
is	O
2	O
(	O
cid:0	O
)	O
m	O
,	O
less	O
than	O
one	O
in	O
a	O
billion	O
.	O
to	O
do	O
error	O
correction	O
requires	O
far	O
more	O
check	O
bits	O
,	O
the	O
number	O
depending	O
on	O
the	O
expected	O
types	O
of	O
corruption	O
,	O
and	O
on	O
the	O
(	O
cid:12	O
)	O
le	O
size	O
.	O
for	O
example	O
,	O
if	O
just	O
eight	O
random	B
bits	O
in	O
a	O
8	O
(	O
cid:1	O
)	O
’	O
23	O
(	O
cid:2	O
)	O
8	O
’	O
180	O
megabyte	O
(	O
cid:12	O
)	O
le	O
are	O
corrupted	O
,	O
it	O
would	O
take	O
about	O
log2	O
(	O
cid:0	O
)	O
223	O
bits	O
to	O
specify	O
which	O
are	O
the	O
corrupted	O
bits	O
,	O
and	O
the	O
number	O
of	O
parity-check	O
bits	O
used	O
by	O
a	O
successful	O
error-correcting	B
code	I
would	O
have	O
to	O
be	O
at	O
least	O
this	O
number	O
,	O
by	O
the	O
counting	B
argument	I
of	O
exercise	O
1.10	O
(	O
solution	O
,	O
p.20	O
)	O
.	O
solution	O
to	O
exercise	O
12.10	O
(	O
p.201	O
)	O
.	O
we	O
want	O
to	O
know	O
the	O
length	B
l	O
of	O
a	O
string	O
such	O
that	O
it	O
is	O
very	O
improbable	O
that	O
that	O
string	O
matches	O
any	O
part	O
of	O
the	O
entire	O
writings	O
of	O
humanity	O
.	O
let	O
’	O
s	O
estimate	O
that	O
these	O
writings	O
total	O
about	O
one	O
book	O
for	O
each	O
person	O
living	O
,	O
and	O
that	O
each	O
book	O
contains	O
two	O
million	O
characters	O
(	O
200	O
pages	O
with	O
10	O
000	O
characters	O
per	O
page	O
)	O
{	O
that	O
’	O
s	O
1016	O
characters	O
,	O
drawn	O
from	O
an	O
alphabet	O
of	O
,	O
say	O
,	O
37	O
characters	O
.	O
the	O
probability	B
that	O
a	O
randomly	O
chosen	O
string	O
of	O
length	O
l	O
matches	O
at	O
one	O
point	O
in	O
the	O
collected	O
works	O
of	O
humanity	O
is	O
1=37l	O
.	O
so	O
the	O
expected	O
number	O
of	O
matches	O
is	O
1016=37l	O
,	O
which	O
is	O
vanishingly	O
small	O
if	O
l	O
(	O
cid:21	O
)	O
16=	O
log	O
10	O
37	O
’	O
10.	O
because	O
of	O
the	O
redundancy	O
and	O
repetition	O
of	O
humanity	O
’	O
s	O
writings	O
,	O
it	O
is	O
possible	O
that	O
l	O
’	O
10	O
is	O
an	O
overestimate	O
.	O
so	O
,	O
if	O
you	O
want	O
to	O
write	O
something	O
unique	O
,	O
sit	O
down	O
and	O
compose	O
a	O
string	O
of	O
ten	O
characters	O
.	O
but	O
don	O
’	O
t	O
write	O
gidnebinzz	O
,	O
because	O
i	O
already	O
thought	O
of	O
that	O
string	O
.	O
as	O
for	O
a	O
new	O
melody	B
,	O
if	O
we	O
focus	B
on	O
the	O
sequence	B
of	O
notes	O
,	O
ignoring	O
duration	O
and	O
stress	O
,	O
and	O
allow	O
leaps	O
of	O
up	O
to	O
an	O
octave	B
at	O
each	O
note	O
,	O
then	O
the	O
number	O
of	O
choices	O
per	O
note	O
is	O
23.	O
the	O
pitch	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
note	O
is	O
arbitrary	O
.	O
the	O
number	O
of	O
melodies	O
of	O
length	O
r	O
notes	O
in	O
this	O
rather	O
ugly	O
ensemble	B
of	O
sch	O
(	O
cid:127	O
)	O
onbergian	O
tunes	O
is	O
23r	O
(	O
cid:0	O
)	O
1	O
;	O
for	O
example	O
,	O
there	O
are	O
250	O
000	O
of	O
length	O
r	O
=	O
5.	O
restricting	O
the	O
permitted	O
intervals	B
will	O
reduce	O
this	O
(	O
cid:12	O
)	O
gure	O
;	O
including	O
duration	O
and	O
stress	O
will	O
increase	O
it	O
again	O
.	O
[	O
if	O
we	O
restrict	O
the	O
permitted	O
intervals	B
to	O
repetitions	O
and	O
tones	O
or	O
semitones	O
,	O
the	O
reduction	O
is	O
particularly	O
severe	O
;	O
is	O
this	O
why	O
the	O
melody	B
of	O
‘	O
ode	O
to	O
joy	O
’	O
sounds	O
so	O
boring	O
?	O
]	O
the	O
number	O
of	O
recorded	O
compositions	O
is	O
probably	O
less	O
than	O
a	O
million	O
.	O
if	O
you	O
learn	O
100	O
new	O
melodies	O
per	O
week	O
for	O
every	O
week	O
of	O
your	O
life	B
then	O
you	O
will	O
have	O
learned	O
250	O
000	O
melodies	O
at	O
age	O
50.	O
based	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
204	O
12	O
|	O
hash	O
codes	O
:	O
codes	O
for	O
e	O
(	O
cid:14	O
)	O
cient	O
information	B
retrieval	I
in	O
guess	B
that	I
tune	I
,	O
one	O
player	O
chooses	O
a	O
melody	B
,	O
and	O
sings	O
a	O
gradually-increasing	O
number	O
of	O
its	O
notes	O
,	O
while	O
the	O
other	O
participants	O
try	O
to	O
guess	O
the	O
whole	O
melody	B
.	O
the	O
parsons	O
code	B
is	O
a	O
related	O
hash	B
function	I
for	O
melodies	O
:	O
each	O
pair	O
of	O
consecutive	O
notes	O
is	O
coded	O
as	O
u	O
(	O
‘	O
up	O
’	O
)	O
if	O
the	O
second	O
note	O
is	O
higher	O
than	O
the	O
(	O
cid:12	O
)	O
rst	O
,	O
r	O
(	O
‘	O
repeat	O
’	O
)	O
if	O
the	O
pitches	O
are	O
equal	O
,	O
and	O
d	O
(	O
‘	O
down	O
’	O
)	O
otherwise	O
.	O
you	O
can	O
(	O
cid:12	O
)	O
nd	O
out	O
how	O
well	O
this	O
hash	B
function	I
works	O
at	O
http	O
:	O
//musipedia.org/	O
.	O
on	O
empirical	O
experience	O
of	O
playing	O
the	O
game	B
‘	O
guess	B
that	I
tune	I
’	O
,	O
it	O
seems	O
to	O
me	O
that	O
whereas	O
many	O
four-note	O
sequences	O
are	O
shared	O
in	O
common	O
between	O
melodies	O
,	O
the	O
number	O
of	O
collisions	O
between	O
(	O
cid:12	O
)	O
ve-note	O
sequences	O
is	O
rather	O
smaller	O
{	O
most	O
famous	O
(	O
cid:12	O
)	O
ve-note	O
sequences	O
are	O
unique	O
.	O
solution	O
to	O
exercise	O
12.11	O
(	O
p.201	O
)	O
.	O
(	O
a	O
)	O
let	O
the	O
dna-binding	O
protein	B
recognize	O
a	O
sequence	B
of	O
length	B
l	O
nucleotides	O
.	O
that	O
is	O
,	O
it	O
binds	O
preferentially	O
to	O
that	O
dna	O
sequence	B
,	O
and	O
not	O
to	O
any	O
other	O
pieces	O
of	O
dna	O
in	O
the	O
whole	O
genome	B
.	O
(	O
in	O
reality	O
,	O
the	O
recognized	O
sequence	B
may	O
contain	O
some	O
wildcard	O
characters	O
,	O
e.g.	O
,	O
the	O
*	O
in	O
tataa*a	O
,	O
which	O
denotes	O
‘	O
any	O
of	O
a	O
,	O
c	O
,	O
g	O
and	O
t	O
’	O
;	O
so	O
,	O
to	O
be	O
precise	O
,	O
we	O
are	O
assuming	O
that	O
the	O
recognized	O
sequence	B
contains	O
l	O
non-wildcard	O
characters	O
.	O
)	O
assuming	O
the	O
rest	O
of	O
the	O
genome	O
is	O
‘	O
random	B
’	O
,	O
i.e.	O
,	O
that	O
the	O
sequence	B
con-	O
sists	O
of	O
random	O
nucleotides	O
a	O
,	O
c	O
,	O
g	O
and	O
t	O
with	O
equal	O
probability	B
{	O
which	O
is	O
obviously	O
untrue	O
,	O
but	O
it	O
shouldn	O
’	O
t	O
make	O
too	O
much	O
di	O
(	O
cid:11	O
)	O
erence	O
to	O
our	O
calculation	O
{	O
the	O
chance	O
that	O
there	O
is	O
no	O
other	O
occurrence	O
of	O
the	O
target	O
sequence	B
in	O
the	O
whole	O
genome	B
,	O
of	O
length	O
n	O
nucleotides	O
,	O
is	O
roughly	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
1=4	O
)	O
l	O
)	O
n	O
’	O
exp	O
(	O
(	O
cid:0	O
)	O
n	O
(	O
1=4	O
)	O
l	O
)	O
;	O
which	O
is	O
close	O
to	O
one	O
only	O
if	O
that	O
is	O
,	O
n	O
4	O
(	O
cid:0	O
)	O
l	O
(	O
cid:28	O
)	O
1	O
;	O
(	O
12.9	O
)	O
(	O
12.10	O
)	O
(	O
12.11	O
)	O
using	O
n	O
=	O
3	O
(	O
cid:2	O
)	O
109	O
,	O
we	O
require	O
the	O
recognized	O
sequence	B
to	O
be	O
longer	O
than	O
lmin	O
=	O
16	O
nucleotides	O
.	O
l	O
>	O
log	O
n=	O
log	O
4	O
:	O
what	O
size	O
of	O
protein	O
does	O
this	O
imply	O
?	O
(	O
cid:15	O
)	O
a	O
weak	O
lower	O
bound	B
can	O
be	O
obtained	O
by	O
assuming	O
that	O
the	O
information	B
content	I
of	O
the	O
protein	B
sequence	O
itself	O
is	O
greater	O
than	O
the	O
information	B
content	I
of	O
the	O
nucleotide	B
sequence	O
the	O
protein	B
prefers	O
to	O
bind	O
to	O
(	O
which	O
we	O
have	O
argued	O
above	O
must	O
be	O
at	O
least	O
32	O
bits	O
)	O
.	O
this	O
gives	O
a	O
minimum	O
protein	O
length	B
of	O
32=	O
log2	O
(	O
20	O
)	O
’	O
7	O
amino	O
acids	O
.	O
(	O
cid:15	O
)	O
thinking	O
realistically	O
,	O
the	O
recognition	B
of	O
the	O
dna	O
sequence	B
by	O
the	O
pro-	O
tein	O
presumably	O
involves	O
the	O
protein	B
coming	O
into	O
contact	O
with	O
all	O
sixteen	O
nucleotides	O
in	O
the	O
target	O
sequence	B
.	O
if	O
the	O
protein	B
is	O
a	O
monomer	O
,	O
it	O
must	O
be	O
big	O
enough	O
that	O
it	O
can	O
simultaneously	O
make	O
contact	O
with	O
sixteen	O
nu-	O
cleotides	O
of	O
dna	O
.	O
one	O
helical	O
turn	O
of	O
dna	O
containing	O
ten	O
nucleotides	O
has	O
a	O
length	B
of	O
3.4	O
nm	O
,	O
so	O
a	O
contiguous	O
sequence	B
of	O
sixteen	O
nucleotides	O
has	O
a	O
length	B
of	O
5.4	O
nm	O
.	O
the	O
diameter	O
of	O
the	O
protein	O
must	O
therefore	O
be	O
about	O
5.4	O
nm	O
or	O
greater	O
.	O
egg-white	O
lysozyme	O
is	O
a	O
small	O
globular	O
protein	B
with	O
a	O
length	B
of	O
129	O
amino	O
acids	O
and	O
a	O
diameter	O
of	O
about	O
4	O
nm	O
.	O
as-	O
suming	O
that	O
volume	B
is	O
proportional	O
to	O
sequence	B
length	O
and	O
that	O
volume	B
scales	O
as	O
the	O
cube	O
of	O
the	O
diameter	O
,	O
a	O
protein	B
of	O
diameter	O
5.4	O
nm	O
must	O
have	O
a	O
sequence	B
of	O
length	B
2:5	O
(	O
cid:2	O
)	O
129	O
’	O
324	O
amino	O
acids	O
.	O
(	O
b	O
)	O
if	O
,	O
however	O
,	O
a	O
target	O
sequence	B
consists	O
of	O
a	O
twice-repeated	O
sub-sequence	O
,	O
we	O
can	O
get	O
by	O
with	O
a	O
much	O
smaller	O
protein	B
that	O
recognizes	O
only	O
the	O
sub-sequence	O
,	O
and	O
that	O
binds	O
to	O
the	O
dna	O
strongly	O
only	O
if	O
it	O
can	O
form	O
a	O
dimer	B
,	O
both	O
halves	O
of	O
which	O
are	O
bound	B
to	O
the	O
recognized	O
sequence	B
.	O
halving	O
the	O
diameter	O
of	O
the	O
protein	O
,	O
we	O
now	O
only	O
need	O
a	O
protein	B
whose	O
length	B
is	O
greater	O
than	O
324/8	O
=	O
40	O
amino	O
acids	O
.	O
a	O
protein	B
of	O
length	B
smaller	O
than	O
this	O
can	O
not	O
by	O
itself	O
serve	O
as	O
a	O
regulatory	B
protein	O
speci	O
(	O
cid:12	O
)	O
c	O
to	O
one	O
gene	O
,	O
because	O
it	O
’	O
s	O
simply	O
too	O
small	O
to	O
be	O
able	O
to	O
make	O
a	O
su	O
(	O
cid:14	O
)	O
ciently	O
speci	O
(	O
cid:12	O
)	O
c	O
match	O
{	O
its	O
available	O
surface	O
does	O
not	O
have	O
enough	O
information	B
content	I
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
chapter	O
13	O
in	O
chapters	O
8	O
{	O
11	O
,	O
we	O
established	O
shannon	O
’	O
s	O
noisy-channel	B
coding	I
theorem	I
for	O
a	O
general	O
channel	O
with	O
any	O
input	O
and	O
output	O
alphabets	O
.	O
a	O
great	O
deal	O
of	O
attention	O
in	O
coding	O
theory	B
focuses	O
on	O
the	O
special	O
case	O
of	O
channels	O
with	O
binary	O
inputs	O
.	O
constraining	O
ourselves	O
to	O
these	O
channels	O
simpli	O
(	O
cid:12	O
)	O
es	O
matters	O
,	O
and	O
leads	O
us	O
into	O
an	O
exceptionally	O
rich	O
world	O
,	O
which	O
we	O
will	O
only	O
taste	O
in	O
this	O
book	O
.	O
one	O
of	O
the	O
aims	O
of	O
this	O
chapter	O
is	O
to	O
point	O
out	O
a	O
contrast	O
between	O
shannon	O
’	O
s	O
aim	O
of	O
achieving	O
reliable	O
communication	B
over	O
a	O
noisy	B
channel	I
and	O
the	O
apparent	O
aim	O
of	O
many	O
in	O
the	O
world	O
of	O
coding	B
theory	I
.	O
many	O
coding	O
theorists	O
take	O
as	O
their	O
fundamental	O
problem	O
the	O
task	O
of	O
packing	O
as	O
many	O
spheres	O
as	O
possible	O
,	O
with	O
radius	O
as	O
large	O
as	O
possible	O
,	O
into	O
an	O
n	O
-dimensional	O
space	O
,	O
with	O
no	O
spheres	O
overlapping	O
.	O
prizes	O
are	O
awarded	O
to	O
people	O
who	O
(	O
cid:12	O
)	O
nd	O
packings	O
that	O
squeeze	O
in	O
an	O
extra	O
few	O
spheres	O
.	O
while	O
this	O
is	O
a	O
fascinating	O
mathematical	O
topic	O
,	O
we	O
shall	O
see	O
that	O
the	O
aim	O
of	O
maximizing	O
the	O
distance	B
between	O
codewords	O
in	O
a	O
code	B
has	O
only	O
a	O
tenuous	O
relationship	O
to	O
shannon	O
’	O
s	O
aim	O
of	O
reliable	O
communication	B
.	O
205	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
13	O
binary	O
codes	O
we	O
’	O
ve	O
established	O
shannon	O
’	O
s	O
noisy-channel	B
coding	I
theorem	I
for	O
a	O
general	O
chan-	O
nel	O
with	O
any	O
input	O
and	O
output	O
alphabets	O
.	O
a	O
great	O
deal	O
of	O
attention	O
in	O
coding	O
theory	B
focuses	O
on	O
the	O
special	O
case	O
of	O
channels	O
with	O
binary	O
inputs	O
,	O
the	O
(	O
cid:12	O
)	O
rst	O
implicit	O
choice	O
being	O
the	O
binary	B
symmetric	I
channel	I
.	O
the	O
optimal	B
decoder	I
for	O
a	O
code	B
,	O
given	O
a	O
binary	B
symmetric	I
channel	I
,	O
(	O
cid:12	O
)	O
nds	O
the	O
codeword	B
that	O
is	O
closest	O
to	O
the	O
received	O
vector	O
,	O
closest	O
in	O
hamming	O
dis-	O
tance	O
.	O
the	O
hamming	O
distance	B
between	O
two	O
binary	O
vectors	O
is	O
the	O
number	O
of	O
coordinates	O
in	O
which	O
the	O
two	O
vectors	B
di	O
(	O
cid:11	O
)	O
er	O
.	O
decoding	B
errors	O
will	O
occur	O
if	O
the	O
noise	B
takes	O
us	O
from	O
the	O
transmitted	O
codeword	B
t	O
to	O
a	O
received	O
vector	O
r	O
that	O
is	O
closer	O
to	O
some	O
other	O
codeword	B
.	O
the	O
distances	O
between	O
codewords	O
are	O
thus	O
relevant	O
to	O
the	O
probability	O
of	O
a	O
decoding	B
error	O
.	O
13.1	O
distance	B
properties	O
of	O
a	O
code	B
the	O
distance	B
of	O
a	O
code	B
is	O
the	O
smallest	O
separation	B
between	O
two	O
of	O
its	O
codewords	O
.	O
example	O
13.1.	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
(	O
p.8	O
)	O
has	O
distance	B
d	O
=	O
3.	O
all	O
pairs	O
of	O
its	O
codewords	O
di	O
(	O
cid:11	O
)	O
er	O
in	O
at	O
least	O
3	O
bits	O
.	O
the	O
maximum	O
number	O
of	O
errors	O
it	O
can	O
correct	O
is	O
t	O
=	O
1	O
;	O
in	O
general	O
a	O
code	B
with	O
distance	B
d	O
is	O
b	O
(	O
d	O
(	O
cid:0	O
)	O
1	O
)	O
=2c-	O
error-correcting	O
.	O
a	O
more	O
precise	O
term	O
for	O
distance	O
is	O
the	O
minimum	B
distance	I
of	O
the	O
code	B
.	O
the	O
distance	B
of	O
a	O
code	B
is	O
often	O
denoted	O
by	O
d	O
or	O
dmin	O
.	O
we	O
’	O
ll	O
now	O
constrain	O
our	O
attention	O
to	O
linear	B
codes	I
.	O
in	O
a	O
linear	B
code	O
,	O
all	O
codewords	O
have	O
identical	O
distance	O
properties	O
,	O
so	O
we	O
can	O
summarize	O
all	O
the	O
distances	O
between	O
the	O
code	B
’	O
s	O
codewords	O
by	O
counting	O
the	O
distances	O
from	O
the	O
all-zero	O
codeword	B
.	O
the	O
weight	B
enumerator	I
function	O
of	O
a	O
code	B
,	O
a	O
(	O
w	O
)	O
,	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
the	O
number	O
of	O
codewords	O
in	O
the	O
code	B
that	O
have	O
weight	B
w.	O
the	O
weight	B
enumerator	I
function	O
is	O
also	O
known	O
as	O
the	O
distance	B
distribution	I
of	O
the	O
code	B
.	O
example	O
13.2.	O
the	O
weight	B
enumerator	I
functions	O
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
and	O
the	O
dodecahedron	B
code	I
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gures	O
13.1	O
and	O
13.2	O
.	O
13.2	O
obsession	O
with	O
distance	O
since	O
the	O
maximum	O
number	O
of	O
errors	O
that	O
a	O
code	B
can	O
guarantee	O
to	O
correct	O
,	O
t	O
,	O
is	O
related	O
to	O
its	O
distance	B
d	O
by	O
t	O
=	O
b	O
(	O
d	O
(	O
cid:0	O
)	O
1	O
)	O
=2c	O
,	O
many	O
coding	O
theorists	O
focus	B
on	O
the	O
distance	B
of	O
a	O
code	B
,	O
searching	O
for	O
codes	O
of	O
a	O
given	O
size	O
that	O
have	O
the	O
biggest	O
possible	O
distance	B
.	O
much	O
of	O
practical	O
coding	B
theory	I
has	O
focused	O
on	O
decoders	O
that	O
give	O
the	O
optimal	B
decoding	O
for	O
all	O
error	O
patterns	O
of	O
weight	O
up	O
to	O
the	O
half-distance	O
t	O
of	O
their	O
codes	O
.	O
206	O
example	O
:	O
the	O
hamming	O
distance	B
between	O
and	O
00001111	O
11001101	O
is	O
3.	O
w	O
a	O
(	O
w	O
)	O
0	O
3	O
4	O
7	O
1	O
7	O
7	O
1	O
total	O
16	O
8	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
0	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
figure	O
13.1.	O
the	O
graph	B
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
,	O
and	O
its	O
weight	B
enumerator	I
function	O
.	O
d	O
=	O
2t	O
+	O
1	O
if	O
d	O
is	O
odd	O
,	O
and	O
d	O
=	O
2t	O
+	O
2	O
if	O
d	O
is	O
even	O
.	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
13.2	O
:	O
obsession	O
with	O
distance	O
207	O
w	O
a	O
(	O
w	O
)	O
0	O
5	O
8	O
9	O
10	O
11	O
12	O
13	O
14	O
15	O
16	O
17	O
18	O
19	O
20	O
1	O
12	O
30	O
20	O
72	O
120	O
100	O
180	O
240	O
272	O
345	O
300	O
200	O
120	O
36	O
total	O
2048	O
350	O
300	O
250	O
200	O
150	O
100	O
50	O
0	O
100	O
10	O
1	O
0	O
5	O
8	O
10	O
15	O
20	O
25	O
30	O
0	O
5	O
8	O
10	O
15	O
20	O
25	O
30	O
figure	O
13.2.	O
the	O
graph	B
de	O
(	O
cid:12	O
)	O
ning	O
the	O
(	O
30	O
;	O
11	O
)	O
dodecahedron	B
code	I
(	O
the	O
circles	O
are	O
the	O
30	O
transmitted	O
bits	O
and	O
the	O
triangles	O
are	O
the	O
20	O
parity	B
checks	O
,	O
one	O
of	O
which	O
is	O
redundant	O
)	O
and	O
the	O
weight	B
enumerator	I
function	O
(	O
solid	O
lines	O
)	O
.	O
the	O
dotted	O
lines	O
show	O
the	O
average	B
weight	O
enumerator	O
function	B
of	O
all	O
random	B
linear	I
codes	O
with	O
the	O
same	O
size	O
of	O
generator	O
matrix	B
,	O
which	O
will	O
be	O
computed	O
shortly	O
.	O
the	O
lower	O
(	O
cid:12	O
)	O
gure	O
shows	O
the	O
same	O
functions	B
on	O
a	O
log	O
scale	O
.	O
a	O
bounded-distance	B
decoder	I
is	O
a	O
decoder	B
that	O
returns	O
the	O
closest	O
code-	O
word	O
to	O
a	O
received	O
binary	O
vector	O
r	O
if	O
the	O
distance	B
from	O
r	O
to	O
that	O
codeword	B
is	O
less	O
than	O
or	O
equal	O
to	O
t	O
;	O
otherwise	O
it	O
returns	O
a	O
failure	O
message	O
.	O
the	O
rationale	O
for	O
not	O
trying	O
to	O
decode	O
when	O
more	O
than	O
t	O
errors	B
have	O
occurred	O
might	O
be	O
‘	O
we	O
can	O
’	O
t	O
guarantee	O
that	O
we	O
can	O
correct	O
more	O
than	O
t	O
errors	B
,	O
so	O
we	O
won	O
’	O
t	O
bother	O
trying	O
{	O
who	O
would	O
be	O
interested	O
in	O
a	O
decoder	B
that	O
corrects	O
some	O
error	O
patterns	O
of	O
weight	O
greater	O
than	O
t	O
,	O
but	O
not	O
others	O
?	O
’	O
this	O
defeatist	O
attitude	O
is	O
an	O
example	O
of	O
worst-case-ism	O
,	O
a	O
widespread	O
mental	O
ailment	O
which	O
this	O
book	O
is	O
intended	O
to	O
cure	O
.	O
the	O
fact	O
is	O
that	O
bounded-distance	B
decoders	O
can	O
not	O
reach	O
the	O
shannon	O
limit	O
(	O
cid:3	O
)	O
of	O
the	O
binary	O
symmetric	B
channel	I
;	O
only	O
a	O
decoder	B
that	O
often	O
corrects	O
more	O
than	O
t	O
errors	B
can	O
do	O
this	O
.	O
the	O
state	O
of	O
the	O
art	O
in	O
error-correcting	O
codes	O
have	O
decoders	O
that	O
work	O
way	O
beyond	O
the	O
minimum	B
distance	I
of	O
the	O
code	B
.	O
de	O
(	O
cid:12	O
)	O
nitions	O
of	O
good	O
and	O
bad	O
distance	B
properties	O
given	O
a	O
family	O
of	O
codes	O
of	O
increasing	O
blocklength	O
n	O
,	O
and	O
with	O
rates	O
approach-	O
ing	O
a	O
limit	O
r	O
>	O
0	O
,	O
we	O
may	O
be	O
able	O
to	O
put	O
that	O
family	O
in	O
one	O
of	O
the	O
following	O
categories	O
,	O
which	O
have	O
some	O
similarities	O
to	O
the	O
categories	O
of	O
‘	O
good	B
’	O
and	O
‘	O
bad	B
’	O
codes	O
de	O
(	O
cid:12	O
)	O
ned	O
earlier	O
(	O
p.183	O
)	O
:	O
a	O
sequence	B
of	O
codes	O
has	O
‘	O
good	B
’	O
distance	B
if	O
d=n	O
tends	O
to	O
a	O
constant	O
greater	O
than	O
zero	O
.	O
a	O
sequence	B
of	O
codes	O
has	O
‘	O
bad	B
’	O
distance	B
if	O
d=n	O
tends	O
to	O
zero	O
.	O
a	O
sequence	B
of	O
codes	O
has	O
‘	O
very	B
bad	I
’	O
distance	B
if	O
d	O
tends	O
to	O
a	O
constant	O
.	O
example	O
13.3.	O
a	O
low-density	B
generator-matrix	I
code	I
is	O
a	O
linear	B
code	O
whose	O
k	O
(	O
cid:2	O
)	O
n	O
generator	B
matrix	I
g	O
has	O
a	O
small	O
number	O
d0	O
of	O
1s	O
per	O
row	O
,	O
regardless	O
of	O
how	O
big	O
n	O
is	O
.	O
the	O
minimum	B
distance	I
of	O
such	O
a	O
code	B
is	O
at	O
most	O
d0	O
,	O
so	O
low-density	B
generator-matrix	I
codes	O
have	O
‘	O
very	B
bad	I
’	O
distance	B
.	O
while	O
having	O
large	O
distance	B
is	O
no	O
bad	B
thing	O
,	O
we	O
’	O
ll	O
see	O
,	O
later	O
on	O
,	O
why	O
an	O
emphasis	O
on	O
distance	O
can	O
be	O
unhealthy	O
.	O
figure	O
13.3.	O
the	O
graph	B
of	O
a	O
rate-1/2	O
low-density	B
generator-matrix	I
code	I
.	O
the	O
rightmost	O
m	O
of	O
the	O
transmitted	O
bits	O
are	O
each	O
connected	O
to	O
a	O
single	O
distinct	O
parity	B
constraint	O
.	O
the	O
leftmost	O
k	O
transmitted	O
bits	O
are	O
each	O
connected	O
to	O
a	O
small	O
number	O
of	O
parity	O
constraints	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
208	O
13	O
|	O
binary	O
codes	O
figure	O
13.4.	O
schematic	O
picture	O
of	O
part	O
of	O
hamming	O
space	O
perfectly	O
(	O
cid:12	O
)	O
lled	O
by	O
t-spheres	O
centred	O
on	O
the	O
codewords	O
of	O
a	O
perfect	B
code	I
.	O
t	O
t	O
t	O
.	O
.	O
.	O
1	O
2	O
13.3	O
perfect	B
codes	O
a	O
t-sphere	O
(	O
or	O
a	O
sphere	O
of	O
radius	O
t	O
)	O
in	O
hamming	O
space	O
,	O
centred	O
on	O
a	O
point	O
x	O
,	O
is	O
the	O
set	B
of	O
points	O
whose	O
hamming	O
distance	B
from	O
x	O
is	O
less	O
than	O
or	O
equal	O
to	O
t.	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
has	O
the	O
beautiful	O
property	O
that	O
if	O
we	O
place	O
1-	O
spheres	O
about	O
each	O
of	O
its	O
16	O
codewords	O
,	O
those	O
spheres	O
perfectly	O
(	O
cid:12	O
)	O
ll	O
hamming	O
space	O
without	O
overlapping	O
.	O
as	O
we	O
saw	O
in	O
chapter	O
1	O
,	O
every	O
binary	O
vector	O
of	O
length	O
7	O
is	O
within	O
a	O
distance	B
of	O
t	O
=	O
1	O
of	O
exactly	O
one	O
codeword	B
of	O
the	O
hamming	O
code	B
.	O
a	O
code	B
is	O
a	O
perfect	B
t-error-correcting	O
code	B
if	O
the	O
set	B
of	O
t-spheres	O
cen-	O
tred	O
on	O
the	O
codewords	O
of	O
the	O
code	O
(	O
cid:12	O
)	O
ll	O
the	O
hamming	O
space	O
without	O
over-	O
lapping	O
.	O
(	O
see	O
(	O
cid:12	O
)	O
gure	O
13.4	O
.	O
)	O
let	O
’	O
s	O
recap	O
our	O
cast	O
of	O
characters	O
.	O
the	O
number	O
of	O
codewords	O
is	O
s	O
=	O
2k	O
.	O
the	O
number	O
of	O
points	O
in	O
the	O
entire	O
hamming	O
space	O
is	O
2n	O
.	O
the	O
number	O
of	O
points	O
in	O
a	O
hamming	O
sphere	O
of	O
radius	O
t	O
is	O
t	O
xw=0	O
(	O
cid:18	O
)	O
n	O
w	O
(	O
cid:19	O
)	O
:	O
(	O
13.1	O
)	O
for	O
a	O
code	B
to	O
be	O
perfect	B
with	O
these	O
parameters	B
,	O
we	O
require	O
s	O
times	O
the	O
number	O
of	O
points	O
in	O
the	O
t-sphere	O
to	O
equal	O
2n	O
:	O
for	O
a	O
perfect	B
code	I
,	O
2k	O
or	O
,	O
equivalently	O
,	O
t	O
t	O
xw=0	O
(	O
cid:18	O
)	O
n	O
xw=0	O
(	O
cid:18	O
)	O
n	O
w	O
(	O
cid:19	O
)	O
=	O
2n	O
w	O
(	O
cid:19	O
)	O
=	O
2n	O
(	O
cid:0	O
)	O
k	O
:	O
(	O
13.2	O
)	O
(	O
13.3	O
)	O
for	O
a	O
perfect	B
code	I
,	O
the	O
number	O
of	O
noise	O
vectors	B
in	O
one	O
sphere	O
must	O
equal	O
the	O
number	O
of	O
possible	O
syndromes	O
.	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
satis	O
(	O
cid:12	O
)	O
es	O
this	O
numerological	O
condition	O
because	O
1	O
+	O
(	O
cid:18	O
)	O
7	O
1	O
(	O
cid:19	O
)	O
=	O
23	O
:	O
(	O
13.4	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
13.3	O
:	O
perfect	B
codes	O
t	O
.	O
.	O
.	O
1	O
2	O
t	O
.	O
.	O
.	O
1	O
2	O
t	O
.	O
.	O
.	O
1	O
2	O
t	O
.	O
.	O
.	O
1	O
2	O
209	O
figure	O
13.5.	O
schematic	O
picture	O
of	O
hamming	O
space	O
not	O
perfectly	O
(	O
cid:12	O
)	O
lled	O
by	O
t-spheres	O
centred	O
on	O
the	O
codewords	O
of	O
a	O
code	B
.	O
the	O
grey	O
regions	O
show	O
points	O
that	O
are	O
at	O
a	O
hamming	O
distance	B
of	O
more	O
than	O
t	O
from	O
any	O
codeword	B
.	O
this	O
is	O
a	O
misleading	O
picture	O
,	O
as	O
,	O
for	O
any	O
code	B
with	O
large	O
t	O
in	O
high	O
dimensions	B
,	O
the	O
grey	O
space	O
between	O
the	O
spheres	O
takes	O
up	O
almost	O
all	O
of	O
hamming	O
space	O
.	O
how	O
happy	O
we	O
would	O
be	O
to	O
use	O
perfect	B
codes	O
if	O
there	O
were	O
large	O
numbers	O
of	O
perfect	O
codes	O
to	O
choose	O
from	O
,	O
with	O
a	O
wide	O
range	O
of	O
blocklengths	O
and	O
rates	O
,	O
then	O
these	O
would	O
be	O
the	O
perfect	B
solution	O
to	O
shannon	O
’	O
s	O
problem	O
.	O
we	O
could	O
communicate	O
over	O
a	O
binary	B
symmetric	I
channel	I
with	O
noise	B
level	O
f	O
,	O
for	O
example	O
,	O
by	O
picking	O
a	O
perfect	B
t-error-correcting	O
code	B
with	O
blocklength	O
n	O
and	O
t	O
=	O
f	O
(	O
cid:3	O
)	O
n	O
,	O
where	O
f	O
(	O
cid:3	O
)	O
=	O
f	O
+	O
(	O
cid:14	O
)	O
and	O
n	O
and	O
(	O
cid:14	O
)	O
are	O
chosen	O
such	O
that	O
the	O
probability	B
that	O
the	O
noise	B
(	O
cid:13	O
)	O
ips	O
more	O
than	O
t	O
bits	O
is	O
satisfactorily	O
small	O
.	O
however	O
,	O
there	O
are	O
almost	O
no	O
perfect	B
codes	O
.	O
the	O
only	O
nontrivial	O
perfect	B
(	O
cid:3	O
)	O
binary	O
codes	O
are	O
1.	O
the	O
hamming	O
codes	O
,	O
which	O
are	O
perfect	B
codes	O
with	O
t	O
=	O
1	O
and	O
blocklength	O
n	O
=	O
2m	O
(	O
cid:0	O
)	O
1	O
,	O
de	O
(	O
cid:12	O
)	O
ned	O
below	O
;	O
the	O
rate	B
of	O
a	O
hamming	O
code	B
approaches	O
1	O
as	O
its	O
blocklength	O
n	O
increases	O
;	O
2.	O
the	O
repetition	B
codes	O
of	O
odd	O
blocklength	O
n	O
,	O
which	O
are	O
perfect	B
codes	O
with	O
t	O
=	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
=2	O
;	O
the	O
rate	B
of	O
repetition	B
codes	O
goes	O
to	O
zero	O
as	O
1=n	O
;	O
and	O
3.	O
one	O
remarkable	O
3-error-correcting	O
code	B
with	O
212	O
codewords	O
of	O
block-	O
length	B
n	O
=	O
23	O
known	O
as	O
the	O
binary	O
golay	O
code	B
.	O
[	O
a	O
second	O
2-error-	O
correcting	O
golay	O
code	B
of	O
length	B
n	O
=	O
11	O
over	O
a	O
ternary	O
alphabet	O
was	O
dis-	O
covered	O
by	O
a	O
finnish	O
football-pool	O
enthusiast	O
called	O
juhani	O
virtakallio	O
in	O
1947	O
.	O
]	O
there	O
are	O
no	O
other	O
binary	O
perfect	O
codes	O
.	O
why	O
this	O
shortage	O
of	O
perfect	O
codes	O
?	O
is	O
it	O
because	O
precise	O
numerological	O
coincidences	O
like	O
those	O
satis	O
(	O
cid:12	O
)	O
ed	O
by	O
the	O
parameters	B
of	O
the	O
hamming	O
code	B
(	O
13.4	O
)	O
and	O
the	O
golay	O
code	B
,	O
1	O
+	O
(	O
cid:18	O
)	O
23	O
1	O
(	O
cid:19	O
)	O
+	O
(	O
cid:18	O
)	O
23	O
2	O
(	O
cid:19	O
)	O
+	O
(	O
cid:18	O
)	O
23	O
3	O
(	O
cid:19	O
)	O
=	O
211	O
;	O
(	O
13.5	O
)	O
are	O
rare	O
?	O
are	O
there	O
plenty	O
of	O
‘	O
almost-perfect	O
’	O
codes	O
for	O
which	O
the	O
t-spheres	O
(	O
cid:12	O
)	O
ll	O
almost	O
the	O
whole	O
space	O
?	O
no	O
.	O
in	O
fact	O
,	O
the	O
picture	O
of	O
hamming	O
spheres	O
centred	O
on	O
the	O
codewords	O
almost	O
(	O
cid:12	O
)	O
lling	O
hamming	O
space	O
(	O
(	O
cid:12	O
)	O
gure	O
13.5	O
)	O
is	O
a	O
misleading	O
one	O
:	O
for	O
most	O
codes	O
,	O
whether	O
they	O
are	O
good	B
codes	O
or	O
bad	B
codes	O
,	O
almost	O
all	O
the	O
hamming	O
space	O
is	O
taken	O
up	O
by	O
the	O
space	O
between	O
t-spheres	O
(	O
which	O
is	O
shown	O
in	O
grey	O
in	O
(	O
cid:12	O
)	O
gure	O
13.5	O
)	O
.	O
having	O
established	O
this	O
gloomy	O
picture	O
,	O
we	O
spend	O
a	O
moment	O
(	O
cid:12	O
)	O
lling	O
in	O
the	O
properties	O
of	O
the	O
perfect	O
codes	O
mentioned	O
above	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
210	O
13	O
|	O
binary	O
codes	O
figure	O
13.6.	O
three	O
codewords	O
.	O
00000	O
0	O
00000	O
100000	O
0000	O
0	O
1	O
1	O
1	O
1	O
0000	O
1	O
1	O
1	O
wn	O
xn	O
0	O
00000	O
11	O
00000	O
1	O
0	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
00000	O
00000	O
1	O
1	O
1	O
1	O
0	O
0	O
1	O
1	O
1	O
1	O
11	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
un	O
vn	O
n	O
the	O
hamming	O
codes	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
can	O
be	O
de	O
(	O
cid:12	O
)	O
ned	O
as	O
the	O
linear	B
code	O
whose	O
3	O
(	O
cid:2	O
)	O
7	O
parity-	O
check	O
matrix	B
contains	O
,	O
as	O
its	O
columns	O
,	O
all	O
the	O
7	O
(	O
=	O
23	O
(	O
cid:0	O
)	O
1	O
)	O
non-zero	O
vectors	B
of	O
length	B
3.	O
since	O
these	O
7	O
vectors	B
are	O
all	O
di	O
(	O
cid:11	O
)	O
erent	O
,	O
any	O
single	O
bit-	O
(	O
cid:13	O
)	O
ip	O
produces	O
a	O
distinct	O
syndrome	B
,	O
so	O
all	O
single-bit	O
errors	B
can	O
be	O
detected	O
and	O
corrected	O
.	O
we	O
can	O
generalize	O
this	O
code	B
,	O
with	O
m	O
=	O
3	O
parity	B
constraints	O
,	O
as	O
follows	O
.	O
the	O
hamming	O
codes	O
are	O
single-error-correcting	O
codes	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
picking	O
a	O
number	O
of	O
parity-check	O
constraints	O
,	O
m	O
;	O
the	O
blocklength	O
n	O
is	O
n	O
=	O
2m	O
(	O
cid:0	O
)	O
1	O
;	O
the	O
parity-	O
check	O
matrix	B
contains	O
,	O
as	O
its	O
columns	O
,	O
all	O
the	O
n	O
non-zero	O
vectors	B
of	O
length	B
m	O
bits	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
few	O
hamming	O
codes	O
have	O
the	O
following	O
rates	O
:	O
checks	O
,	O
m	O
(	O
n	O
;	O
k	O
)	O
r	O
=	O
k=n	O
2	O
3	O
4	O
5	O
6	O
(	O
3	O
,	O
1	O
)	O
(	O
7	O
,	O
4	O
)	O
(	O
15	O
,	O
11	O
)	O
(	O
31	O
,	O
26	O
)	O
(	O
63	O
,	O
57	O
)	O
1/3	O
4/7	O
11/15	O
26/31	O
57/63	O
repetition	B
code	I
r3	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
exercise	O
13.4	O
.	O
[	O
2	O
,	O
p.223	O
]	O
what	O
is	O
the	O
probability	B
of	I
block	I
error	I
of	O
the	O
(	O
n	O
;	O
k	O
)	O
hamming	O
code	B
to	O
leading	O
order	O
,	O
when	O
the	O
code	B
is	O
used	O
for	O
a	O
binary	B
symmetric	I
channel	I
with	O
noise	B
density	O
f	O
?	O
13.4	O
perfectness	O
is	O
unattainable	O
{	O
(	O
cid:12	O
)	O
rst	O
proof	O
we	O
will	O
show	O
in	O
several	O
ways	O
that	O
useful	O
perfect	B
codes	O
do	O
not	O
exist	O
(	O
here	O
,	O
‘	O
useful	O
’	O
means	O
‘	O
having	O
large	O
blocklength	O
n	O
,	O
and	O
rate	O
close	O
neither	O
to	O
0	O
nor	O
1	O
’	O
)	O
.	O
shannon	O
proved	O
that	O
,	O
given	O
a	O
binary	B
symmetric	I
channel	I
with	O
any	O
noise	B
level	O
f	O
,	O
there	O
exist	O
codes	O
with	O
large	O
blocklength	O
n	O
and	O
rate	O
as	O
close	O
as	O
you	O
like	O
to	O
c	O
(	O
f	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
f	O
)	O
that	O
enable	O
communication	B
with	O
arbitrarily	O
small	O
error	B
probability	I
.	O
for	O
large	O
n	O
,	O
the	O
number	O
of	O
errors	O
per	O
block	B
will	O
typically	O
be	O
about	O
fn	O
,	O
so	O
these	O
codes	O
of	O
shannon	O
are	O
‘	O
almost-certainly-fn	O
-error-correcting	O
’	O
codes	O
.	O
let	O
’	O
s	O
pick	O
the	O
special	O
case	O
of	O
a	O
noisy	B
channel	I
with	O
f	O
2	O
(	O
1=3	O
;	O
1=2	O
)	O
.	O
can	O
we	O
(	O
cid:12	O
)	O
nd	O
a	O
large	O
perfect	B
code	I
that	O
is	O
fn	O
-error-correcting	O
?	O
well	O
,	O
let	O
’	O
s	O
suppose	O
that	O
such	O
a	O
code	B
has	O
been	O
found	O
,	O
and	O
examine	O
just	O
three	O
of	O
its	O
codewords	O
.	O
(	O
remember	O
that	O
the	O
code	B
ought	O
to	O
have	O
rate	B
r	O
’	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
f	O
)	O
,	O
so	O
it	O
should	O
have	O
an	O
enormous	O
number	O
(	O
2n	O
r	O
)	O
of	O
codewords	O
.	O
)	O
without	O
loss	O
of	O
generality	O
,	O
we	O
choose	O
one	O
of	O
the	O
codewords	O
to	O
be	O
the	O
all-zero	O
codeword	B
and	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
other	O
two	O
to	O
have	O
overlaps	O
with	O
it	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
13.6.	O
the	O
second	O
codeword	B
di	O
(	O
cid:11	O
)	O
ers	O
from	O
the	O
(	O
cid:12	O
)	O
rst	O
in	O
a	O
fraction	O
u	O
+	O
v	O
of	O
its	O
coordinates	O
.	O
the	O
third	O
codeword	O
di	O
(	O
cid:11	O
)	O
ers	O
from	O
the	O
(	O
cid:12	O
)	O
rst	O
in	O
a	O
fraction	O
v	O
+	O
w	O
,	O
and	O
from	O
the	O
second	O
in	O
a	O
fraction	O
u	O
+	O
w.	O
a	O
fraction	O
x	O
of	O
the	O
coordinates	O
have	O
value	O
zero	O
in	O
all	O
three	O
codewords	O
.	O
now	O
,	O
if	O
the	O
code	B
is	O
fn	O
-error-correcting	O
,	O
its	O
minimum	B
distance	I
must	O
be	O
greater	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
13.5	O
:	O
weight	B
enumerator	I
function	O
of	O
random	O
linear	B
codes	I
211	O
than	O
2fn	O
,	O
so	O
u	O
+	O
v	O
>	O
2f	O
;	O
v	O
+	O
w	O
>	O
2f	O
;	O
and	O
u	O
+	O
w	O
>	O
2f	O
:	O
(	O
13.6	O
)	O
summing	O
these	O
three	O
inequalities	O
and	O
dividing	O
by	O
two	O
,	O
we	O
have	O
u	O
+	O
v	O
+	O
w	O
>	O
3f	O
:	O
(	O
13.7	O
)	O
so	O
if	O
f	O
>	O
1=3	O
,	O
we	O
can	O
deduce	O
u	O
+	O
v	O
+	O
w	O
>	O
1	O
,	O
so	O
that	O
x	O
<	O
0	O
,	O
which	O
is	O
impossible	O
.	O
such	O
a	O
code	B
can	O
not	O
exist	O
.	O
so	O
the	O
code	B
can	O
not	O
have	O
three	O
codewords	O
,	O
let	O
alone	O
2n	O
r.	O
we	O
conclude	O
that	O
,	O
whereas	O
shannon	O
proved	O
there	O
are	O
plenty	O
of	O
codes	O
for	O
communicating	O
over	O
a	O
binary	B
symmetric	I
channel	I
with	O
f	O
>	O
1=3	O
,	O
there	O
are	O
no	O
perfect	B
codes	O
that	O
can	O
do	O
this	O
.	O
we	O
now	O
study	O
a	O
more	O
general	O
argument	O
that	O
indicates	O
that	O
there	O
are	O
no	O
large	O
perfect	B
linear	O
codes	O
for	O
general	O
rates	O
(	O
other	O
than	O
0	O
and	O
1	O
)	O
.	O
we	O
do	O
this	O
by	O
(	O
cid:12	O
)	O
nding	O
the	O
typical	B
distance	O
of	O
a	O
random	B
linear	I
code	O
.	O
13.5	O
weight	B
enumerator	I
function	O
of	O
random	O
linear	B
codes	I
imagine	O
making	O
a	O
code	B
by	O
picking	O
the	O
binary	O
entries	O
in	O
the	O
m	O
(	O
cid:2	O
)	O
n	O
parity-check	B
matrix	I
h	O
at	O
random	B
.	O
what	O
weight	O
enumerator	O
function	B
should	O
we	O
expect	O
?	O
the	O
weight	B
enumerator	I
of	O
one	O
particular	O
code	B
with	O
parity-check	B
matrix	I
h	O
,	O
a	O
(	O
w	O
)	O
h	O
,	O
is	O
the	O
number	O
of	O
codewords	O
of	O
weight	O
w	O
,	O
which	O
can	O
be	O
written	O
a	O
(	O
w	O
)	O
h	O
=	O
xx	O
:	O
jxj=w	O
	O
[	O
hx	O
=	O
0	O
]	O
;	O
(	O
13.8	O
)	O
where	O
the	O
sum	O
is	O
over	O
all	O
vectors	B
x	O
whose	O
weight	B
is	O
w	O
and	O
the	O
truth	B
function	I
	O
[	O
hx	O
=	O
0	O
]	O
equals	O
one	O
if	O
hx	O
=	O
0	O
and	O
zero	O
otherwise	O
.	O
we	O
can	O
(	O
cid:12	O
)	O
nd	O
the	O
expected	O
value	O
of	O
a	O
(	O
w	O
)	O
,	O
n	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
1	O
0	O
1	O
1	O
1	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
1	O
0	O
1	O
0	O
1	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
1	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
1	O
0	O
1	O
1	O
1	O
1	O
1	O
0	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
0	O
1	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
1	O
m	O
figure	O
13.7.	O
a	O
random	B
binary	O
parity-check	B
matrix	I
.	O
ha	O
(	O
w	O
)	O
i	O
=	O
xh	O
p	O
(	O
h	O
)	O
a	O
(	O
w	O
)	O
h	O
=	O
xx	O
:	O
jxj=wxh	O
p	O
(	O
h	O
)	O
	O
[	O
hx	O
=	O
0	O
]	O
;	O
(	O
13.9	O
)	O
(	O
13.10	O
)	O
by	O
evaluating	O
the	O
probability	B
that	O
a	O
particular	O
word	O
of	O
weight	O
w	O
>	O
0	O
is	O
a	O
codeword	B
of	O
the	O
code	B
(	O
averaging	O
over	O
all	O
binary	O
linear	O
codes	O
in	O
our	O
ensemble	B
)	O
.	O
by	O
symmetry	O
,	O
this	O
probability	B
depends	O
only	O
on	O
the	O
weight	B
w	O
of	O
the	O
word	O
,	O
not	O
on	O
the	O
details	O
of	O
the	O
word	O
.	O
the	O
probability	B
that	O
the	O
entire	O
syndrome	B
hx	O
is	O
zero	O
can	O
be	O
found	O
by	O
multiplying	O
together	O
the	O
probabilities	O
that	O
each	O
of	O
the	O
m	O
bits	O
in	O
the	O
syndrome	B
is	O
zero	O
.	O
each	O
bit	B
zm	O
of	O
the	O
syndrome	O
is	O
a	O
sum	O
(	O
mod	O
2	O
)	O
of	O
w	O
random	B
bits	O
,	O
so	O
the	O
probability	B
that	O
zm	O
=	O
0	O
is	O
1/2	O
.	O
the	O
probability	B
that	O
hx	O
=	O
0	O
is	O
thus	O
p	O
(	O
h	O
)	O
	O
[	O
hx	O
=	O
0	O
]	O
=	O
(	O
1/2	O
)	O
m	O
=	O
2	O
(	O
cid:0	O
)	O
m	O
;	O
(	O
13.11	O
)	O
xh	O
independent	O
of	O
w.	O
the	O
expected	O
number	O
of	O
words	O
of	O
weight	O
w	O
(	O
13.10	O
)	O
is	O
given	O
by	O
summing	O
,	O
over	O
all	O
words	O
of	O
weight	O
w	O
,	O
the	O
probability	B
that	O
each	O
word	O
is	O
a	O
codeword	B
.	O
the	O
number	O
of	O
words	O
of	O
weight	O
w	O
is	O
(	O
cid:0	O
)	O
n	O
ha	O
(	O
w	O
)	O
i	O
=	O
(	O
cid:18	O
)	O
n	O
w	O
(	O
cid:1	O
)	O
,	O
so	O
w	O
(	O
cid:19	O
)	O
2	O
(	O
cid:0	O
)	O
m	O
for	O
any	O
w	O
>	O
0	O
:	O
(	O
13.12	O
)	O
 	B
 	I
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
212	O
13	O
|	O
binary	O
codes	O
6e+52	O
5e+52	O
4e+52	O
3e+52	O
2e+52	O
1e+52	O
0	O
0	O
100	O
200	O
300	O
400	O
500	O
1e+60	O
1e+40	O
1e+20	O
1	O
1e-20	O
1e-40	O
1e-60	O
1e-80	O
1e-100	O
1e-120	O
0	O
100	O
200	O
300	O
400	O
500	O
figure	O
13.8.	O
the	O
expected	O
weight	B
enumerator	I
function	O
ha	O
(	O
w	O
)	O
i	O
of	O
a	O
random	B
linear	I
code	O
with	O
n	O
=	O
540	O
and	O
m	O
=	O
360.	O
lower	O
(	O
cid:12	O
)	O
gure	O
shows	O
ha	O
(	O
w	O
)	O
i	O
on	O
a	O
logarithmic	O
scale	O
.	O
1	O
0.5	O
0	O
0	O
capacity	B
r_gv	O
0.25	O
0.5	O
f	O
figure	O
13.9.	O
contrast	O
between	O
shannon	O
’	O
s	O
channel	O
capacity	O
c	O
and	O
the	O
gilbert	O
rate	B
rgv	O
{	O
the	O
maximum	O
communication	O
rate	B
achievable	O
using	O
a	O
bounded-distance	B
decoder	I
,	O
as	O
a	O
function	B
of	O
noise	B
level	O
f	O
.	O
for	O
any	O
given	O
rate	B
,	O
r	O
,	O
the	O
maximum	O
tolerable	O
noise	B
level	O
for	O
shannon	O
is	O
twice	O
as	O
big	O
as	O
the	O
maximum	O
tolerable	O
noise	B
level	O
for	O
a	O
‘	O
worst-case-ist	O
’	O
who	O
uses	O
a	O
bounded-distance	B
decoder	I
.	O
for	O
large	O
n	O
,	O
we	O
can	O
use	O
log	O
(	O
cid:0	O
)	O
n	O
w	O
(	O
cid:1	O
)	O
’	O
n	O
h2	O
(	O
w=n	O
)	O
and	O
r	O
’	O
1	O
(	O
cid:0	O
)	O
m=n	O
to	O
write	O
log2ha	O
(	O
w	O
)	O
i	O
’	O
n	O
h2	O
(	O
w=n	O
)	O
(	O
cid:0	O
)	O
m	O
’	O
n	O
[	O
h2	O
(	O
w=n	O
)	O
(	O
cid:0	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
r	O
)	O
]	O
for	O
any	O
w	O
>	O
0	O
:	O
(	O
13.13	O
)	O
(	O
13.14	O
)	O
as	O
a	O
concrete	O
example	O
,	O
(	O
cid:12	O
)	O
gure	O
13.8	O
shows	O
the	O
expected	O
weight	B
enumerator	I
function	O
of	O
a	O
rate-1=3	O
random	B
linear	I
code	O
with	O
n	O
=	O
540	O
and	O
m	O
=	O
360.	O
gilbert	O
{	O
varshamov	O
distance	B
for	O
weights	O
w	O
such	O
that	O
h2	O
(	O
w=n	O
)	O
<	O
(	O
1	O
(	O
cid:0	O
)	O
r	O
)	O
,	O
the	O
expectation	B
of	O
a	O
(	O
w	O
)	O
is	O
smaller	O
than	O
1	O
;	O
for	O
weights	O
such	O
that	O
h2	O
(	O
w=n	O
)	O
>	O
(	O
1	O
(	O
cid:0	O
)	O
r	O
)	O
,	O
the	O
expectation	B
is	O
greater	O
than	O
1.	O
we	O
thus	O
expect	O
,	O
for	O
large	O
n	O
,	O
that	O
the	O
minimum	B
distance	I
of	O
a	O
random	B
linear	I
code	O
will	O
be	O
close	O
to	O
the	O
distance	B
dgv	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
h2	O
(	O
dgv=n	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
r	O
)	O
:	O
(	O
13.15	O
)	O
de	O
(	O
cid:12	O
)	O
nition	O
.	O
this	O
distance	B
,	O
dgv	O
(	O
cid:17	O
)	O
n	O
h	O
(	O
cid:0	O
)	O
1	O
distance	B
for	O
rate	B
r	O
and	O
blocklength	O
n	O
.	O
2	O
(	O
1	O
(	O
cid:0	O
)	O
r	O
)	O
,	O
is	O
the	O
gilbert	O
{	O
varshamov	O
the	O
gilbert	O
{	O
varshamov	O
conjecture	O
,	O
widely	O
believed	O
,	O
asserts	O
that	O
(	O
for	O
large	O
n	O
)	O
it	O
is	O
not	O
possible	O
to	O
create	O
binary	O
codes	O
with	O
minimum	O
distance	B
signi	O
(	O
cid:12	O
)	O
cantly	O
greater	O
than	O
dgv	O
.	O
de	O
(	O
cid:12	O
)	O
nition	O
.	O
the	O
gilbert	O
{	O
varshamov	O
rate	B
rgv	O
is	O
the	O
maximum	O
rate	O
at	O
which	O
you	O
can	O
reliably	O
communicate	O
with	O
a	O
bounded-distance	B
decoder	I
(	O
as	O
de	O
(	O
cid:12	O
)	O
ned	O
on	O
p.207	O
)	O
,	O
assuming	O
that	O
the	O
gilbert	O
{	O
varshamov	O
conjecture	O
is	O
true	O
.	O
why	O
sphere-packing	B
is	O
a	O
bad	B
perspective	O
,	O
and	O
an	O
obsession	O
with	O
distance	O
is	O
inappropriate	O
if	O
one	O
uses	O
a	O
bounded-distance	B
decoder	I
,	O
the	O
maximum	O
tolerable	O
noise	B
level	O
will	O
(	O
cid:13	O
)	O
ip	O
a	O
fraction	O
fbd	O
=	O
1	O
2	O
dmin=n	O
of	O
the	O
bits	O
.	O
so	O
,	O
assuming	O
dmin	O
is	O
equal	O
to	O
the	O
gilbert	O
distance	B
dgv	O
(	O
13.15	O
)	O
,	O
we	O
have	O
:	O
h2	O
(	O
2fbd	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
rgv	O
)	O
:	O
rgv	O
=	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
2fbd	O
)	O
:	O
(	O
13.16	O
)	O
(	O
13.17	O
)	O
now	O
,	O
here	O
’	O
s	O
the	O
crunch	O
:	O
what	O
did	O
shannon	O
say	O
is	O
achievable	O
?	O
he	O
said	O
the	O
maximum	O
possible	O
rate	B
of	O
communication	B
is	O
the	O
capacity	B
,	O
c	O
=	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
f	O
)	O
:	O
(	O
13.18	O
)	O
so	O
for	O
a	O
given	O
rate	B
r	O
,	O
the	O
maximum	O
tolerable	O
noise	B
level	O
,	O
according	O
to	O
shannon	O
,	O
is	O
given	O
by	O
h2	O
(	O
f	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
r	O
)	O
:	O
(	O
13.19	O
)	O
our	O
conclusion	O
:	O
imagine	O
a	O
good	B
code	O
of	O
rate	O
r	O
has	O
been	O
chosen	O
;	O
equations	O
(	O
13.16	O
)	O
and	O
(	O
13.19	O
)	O
respectively	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
maximum	O
noise	O
levels	O
tolerable	O
by	O
a	O
bounded-distance	B
decoder	I
,	O
fbd	O
,	O
and	O
by	O
shannon	O
’	O
s	O
decoder	B
,	O
f	O
.	O
fbd	O
=	O
f	O
=2	O
:	O
(	O
13.20	O
)	O
bounded-distance	B
decoders	O
can	O
only	O
ever	O
cope	O
with	O
half	O
the	O
noise-level	O
that	O
shannon	O
proved	O
is	O
tolerable	O
!	O
how	O
does	O
this	O
relate	O
to	O
perfect	B
codes	O
?	O
a	O
code	B
is	O
perfect	B
if	O
there	O
are	O
t-	O
spheres	O
around	O
its	O
codewords	O
that	O
(	O
cid:12	O
)	O
ll	O
hamming	O
space	O
without	O
overlapping	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
13.6	O
:	O
berlekamp	O
’	O
s	O
bats	O
213	O
figure	O
13.10.	O
two	O
overlapping	O
spheres	O
whose	O
radius	O
is	O
almost	O
as	O
big	O
as	O
the	O
distance	B
between	O
their	O
centres	O
.	O
but	O
when	O
a	O
typical	B
random	O
linear	B
code	O
is	O
used	O
to	O
communicate	O
over	O
a	O
bi-	O
nary	O
symmetric	B
channel	I
near	O
to	O
the	O
shannon	O
limit	O
,	O
the	O
typical	B
number	O
of	O
bits	O
(	O
cid:13	O
)	O
ipped	O
is	O
fn	O
,	O
and	O
the	O
minimum	B
distance	I
between	O
codewords	O
is	O
also	O
fn	O
,	O
or	O
a	O
little	O
bigger	O
,	O
if	O
we	O
are	O
a	O
little	O
below	O
the	O
shannon	O
limit	O
.	O
so	O
the	O
fn	O
-spheres	O
around	O
the	O
codewords	O
overlap	O
with	O
each	O
other	O
su	O
(	O
cid:14	O
)	O
ciently	O
that	O
each	O
sphere	O
almost	O
contains	O
the	O
centre	O
of	O
its	O
nearest	O
neighbour	O
!	O
the	O
reason	O
why	O
this	O
overlap	O
is	O
not	O
disastrous	O
is	O
because	O
,	O
in	O
high	O
dimensions	B
,	O
the	O
volume	B
associated	O
with	O
the	O
overlap	O
,	O
shown	O
shaded	O
in	O
(	O
cid:12	O
)	O
gure	O
13.10	O
,	O
is	O
a	O
tiny	O
fraction	O
of	O
either	O
sphere	O
,	O
so	O
the	O
probability	O
of	O
landing	O
in	O
it	O
is	O
extremely	O
small	O
.	O
the	O
moral	O
of	O
the	O
story	O
is	O
that	O
worst-case-ism	B
can	O
be	O
bad	B
for	O
you	O
,	O
halving	O
your	O
ability	O
to	O
tolerate	O
noise	B
.	O
you	O
have	O
to	O
be	O
able	O
to	O
decode	O
way	O
beyond	O
the	O
minimum	B
distance	I
of	O
a	O
code	B
to	O
get	O
to	O
the	O
shannon	O
limit	O
!	O
nevertheless	O
,	O
the	O
minimum	B
distance	I
of	O
a	O
code	B
is	O
of	O
interest	O
in	O
practice	O
,	O
because	O
,	O
under	O
some	O
conditions	O
,	O
the	O
minimum	B
distance	I
dominates	O
the	O
errors	B
made	O
by	O
a	O
code	B
.	O
13.6	O
berlekamp	O
’	O
s	O
bats	O
a	O
blind	O
bat	B
lives	O
in	O
a	O
cave	B
.	O
it	O
(	O
cid:13	O
)	O
ies	O
about	O
the	O
centre	O
of	O
the	O
cave	B
,	O
which	O
corre-	O
sponds	O
to	O
one	O
codeword	B
,	O
with	O
its	O
typical	B
distance	O
from	O
the	O
centre	O
controlled	O
by	O
a	O
friskiness	O
parameter	O
f	O
.	O
(	O
the	O
displacement	O
of	O
the	O
bat	O
from	O
the	O
centre	O
corresponds	O
to	O
the	O
noise	B
vector	O
.	O
)	O
the	O
boundaries	O
of	O
the	O
cave	O
are	O
made	O
up	O
of	O
stalactites	O
that	O
point	O
in	O
towards	O
the	O
centre	O
of	O
the	O
cave	B
(	O
(	O
cid:12	O
)	O
gure	O
13.11	O
)	O
.	O
each	O
stalactite	B
is	O
analogous	O
to	O
the	O
boundary	O
between	O
the	O
home	O
codeword	B
and	O
an-	O
other	O
codeword	B
.	O
the	O
stalactite	B
is	O
like	O
the	O
shaded	O
region	O
in	O
(	O
cid:12	O
)	O
gure	O
13.10	O
,	O
but	O
reshaped	O
to	O
convey	O
the	O
idea	O
that	O
it	O
is	O
a	O
region	O
of	O
very	O
small	O
volume	B
.	O
decoding	B
errors	O
correspond	O
to	O
the	O
bat	B
’	O
s	O
intended	O
trajectory	O
passing	O
inside	O
a	O
stalactite	B
.	O
collisions	O
with	O
stalactites	O
at	O
various	O
distances	O
from	O
the	O
centre	O
are	O
possible	O
.	O
if	O
the	O
friskiness	O
is	O
very	O
small	O
,	O
the	O
bat	B
is	O
usually	O
very	O
close	O
to	O
the	O
centre	O
of	O
the	O
cave	B
;	O
collisions	O
will	O
be	O
rare	O
,	O
and	O
when	O
they	O
do	O
occur	O
,	O
they	O
will	O
usually	O
involve	O
the	O
stalactites	O
whose	O
tips	O
are	O
closest	O
to	O
the	O
centre	O
point	O
.	O
similarly	O
,	O
under	O
low-noise	O
conditions	O
,	O
decoding	B
errors	O
will	O
be	O
rare	O
,	O
and	O
they	O
will	O
typi-	O
cally	O
involve	O
low-weight	O
codewords	O
.	O
under	O
low-noise	O
conditions	O
,	O
the	O
minimum	B
distance	I
of	O
a	O
code	B
is	O
relevant	O
to	O
the	O
(	O
very	O
small	O
)	O
probability	B
of	I
error	I
.	O
figure	O
13.11.	O
berlekamp	O
’	O
s	O
schematic	O
picture	O
of	O
hamming	O
space	O
in	O
the	O
vicinity	O
of	O
a	O
codeword	B
.	O
the	O
jagged	O
solid	O
line	O
encloses	O
all	O
points	O
to	O
which	O
this	O
codeword	B
is	O
the	O
closest	O
.	O
the	O
t-sphere	O
around	O
the	O
codeword	B
takes	O
up	O
a	O
small	O
fraction	O
of	O
this	O
space	O
.	O
t	O
.	O
.	O
.	O
1	O
2	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
214	O
13	O
|	O
binary	O
codes	O
if	O
the	O
friskiness	O
is	O
higher	O
,	O
the	O
bat	B
may	O
often	O
make	O
excursions	O
beyond	O
the	O
safe	O
distance	B
t	O
where	O
the	O
longest	O
stalactites	O
start	O
,	O
but	O
it	O
will	O
collide	O
most	O
fre-	O
quently	O
with	O
more	O
distant	O
stalactites	O
,	O
owing	O
to	O
their	O
greater	O
number	O
.	O
there	O
’	O
s	O
only	O
a	O
tiny	O
number	O
of	O
stalactites	O
at	O
the	O
minimum	B
distance	I
,	O
so	O
they	O
are	O
rela-	O
tively	O
unlikely	O
to	O
cause	O
the	O
errors	B
.	O
similarly	O
,	O
errors	B
in	O
a	O
real	O
error-correcting	B
code	I
depend	O
on	O
the	O
properties	O
of	O
the	O
weight	O
enumerator	O
function	B
.	O
at	O
very	O
high	O
friskiness	O
,	O
the	O
bat	B
is	O
always	O
a	O
long	O
way	O
from	O
the	O
centre	O
of	O
the	O
cave	B
,	O
and	O
almost	O
all	O
its	O
collisions	O
involve	O
contact	O
with	O
distant	O
stalactites	O
.	O
under	O
these	O
conditions	O
,	O
the	O
bat	B
’	O
s	O
collision	B
frequency	O
has	O
nothing	O
to	O
do	O
with	O
the	O
distance	B
from	O
the	O
centre	O
to	O
the	O
closest	O
stalactite	B
.	O
13.7	O
concatenation	B
of	O
hamming	O
codes	O
it	O
is	O
instructive	O
to	O
play	O
some	O
more	O
with	O
the	O
concatenation	B
of	O
hamming	O
codes	O
,	O
a	O
concept	O
we	O
(	O
cid:12	O
)	O
rst	O
visited	O
in	O
(	O
cid:12	O
)	O
gure	O
11.6	O
,	O
because	O
we	O
will	O
get	O
insights	O
into	O
the	O
notion	O
of	O
good	O
codes	O
and	O
the	O
relevance	O
or	O
otherwise	O
of	O
the	O
minimum	O
distance	B
of	O
a	O
code	B
.	O
we	O
can	O
create	O
a	O
concatenated	B
code	O
for	O
a	O
binary	B
symmetric	I
channel	I
with	O
noise	B
density	O
f	O
by	O
encoding	O
with	O
several	O
hamming	O
codes	O
in	O
succession	O
.	O
the	O
table	O
recaps	O
the	O
key	O
properties	O
of	O
the	O
hamming	O
codes	O
,	O
indexed	O
by	O
number	O
of	O
constraints	O
,	O
m	O
.	O
all	O
the	O
hamming	O
codes	O
have	O
minimum	B
distance	I
d	O
=	O
3	O
and	O
can	O
correct	O
one	O
error	O
in	O
n	O
.	O
blocklength	O
n	O
=	O
2m	O
(	O
cid:0	O
)	O
1	O
k	O
=	O
n	O
(	O
cid:0	O
)	O
m	O
number	O
of	O
source	O
bits	O
2	O
(	O
cid:1	O
)	O
f	O
2	O
n	O
(	O
cid:0	O
)	O
n	O
pb	O
=	O
3	O
probability	B
of	I
block	I
error	I
to	O
leading	O
order	O
if	O
we	O
make	O
a	O
product	B
code	I
by	O
concatenating	O
a	O
sequence	B
of	O
c	O
hamming	O
c=1	O
in	O
such	O
a	O
codes	O
with	O
increasing	O
m	O
,	O
we	O
can	O
choose	O
those	O
parameters	B
fmcgc	O
way	O
that	O
the	O
rate	B
of	O
the	O
product	B
code	I
rc	O
=	O
nc	O
(	O
cid:0	O
)	O
mc	O
nc	O
c	O
yc=1	O
(	O
13.21	O
)	O
tends	O
to	O
a	O
non-zero	O
limit	O
as	O
c	O
increases	O
.	O
for	O
example	O
,	O
if	O
we	O
set	B
m1	O
=	O
2	O
,	O
m2	O
=	O
3	O
,	O
m3	O
=	O
4	O
,	O
etc.	O
,	O
then	O
the	O
asymptotic	O
rate	O
is	O
0.093	O
(	O
(	O
cid:12	O
)	O
gure	O
13.12	O
)	O
.	O
the	O
blocklength	O
n	O
is	O
a	O
rapidly-growing	O
function	B
of	O
c	O
,	O
so	O
these	O
codes	O
are	O
somewhat	O
impractical	O
.	O
a	O
further	O
weakness	B
of	I
these	O
codes	O
is	O
that	O
their	O
min-	O
imum	O
distance	B
is	O
not	O
very	B
good	I
(	O
(	O
cid:12	O
)	O
gure	O
13.13	O
)	O
.	O
every	O
one	O
of	O
the	O
constituent	O
hamming	O
codes	O
has	O
minimum	B
distance	I
3	O
,	O
so	O
the	O
minimum	B
distance	I
of	O
the	O
cth	O
product	O
is	O
3c	O
.	O
the	O
blocklength	O
n	O
grows	O
faster	O
than	O
3c	O
,	O
so	O
the	O
ratio	O
d=n	O
tends	O
to	O
zero	O
as	O
c	O
increases	O
.	O
in	O
contrast	O
,	O
for	O
typical	O
random	B
codes	O
,	O
the	O
ratio	O
d=n	O
tends	O
to	O
a	O
constant	O
such	O
that	O
h2	O
(	O
d=n	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
r.	O
concatenated	B
hamming	O
codes	O
thus	O
have	O
‘	O
bad	B
’	O
distance	B
.	O
nevertheless	O
,	O
it	O
turns	O
out	O
that	O
this	O
simple	O
sequence	O
of	O
codes	O
yields	O
good	B
codes	O
for	O
some	O
channels	O
{	O
but	O
not	O
very	B
good	I
codes	O
(	O
see	O
section	O
11.4	O
to	O
recall	O
the	O
de	O
(	O
cid:12	O
)	O
nitions	O
of	O
the	O
terms	O
‘	O
good	B
’	O
and	O
‘	O
very	B
good	I
’	O
)	O
.	O
rather	O
than	O
prove	O
this	O
result	O
,	O
we	O
will	O
simply	O
explore	B
it	O
numerically	O
.	O
figure	O
13.14	O
shows	O
the	O
bit	B
error	O
probability	B
pb	O
of	O
the	O
concatenated	O
codes	O
assuming	O
that	O
the	O
constituent	O
codes	O
are	O
decoded	O
in	O
sequence	O
,	O
as	O
described	O
in	O
section	O
11.4	O
.	O
[	O
this	O
one-code-at-a-time	O
decoding	B
is	O
suboptimal	O
,	O
as	O
we	O
saw	O
there	O
.	O
]	O
the	O
horizontal	O
axis	O
shows	O
the	O
rates	O
of	O
the	O
codes	O
.	O
as	O
the	O
number	O
of	O
concatenations	O
increases	O
,	O
the	O
rate	B
drops	O
to	O
0.093	O
and	O
the	O
error	B
probability	I
drops	O
towards	O
zero	O
.	O
the	O
channel	B
assumed	O
in	O
the	O
(	O
cid:12	O
)	O
gure	O
is	O
the	O
binary	B
symmetric	I
r	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0	O
2	O
4	O
6	O
8	O
10	O
12	O
c	O
figure	O
13.12.	O
the	O
rate	B
r	O
of	O
the	O
concatenated	O
hamming	O
code	B
as	O
a	O
function	B
of	O
the	O
number	O
of	O
concatenations	O
,	O
c.	O
1e+25	O
1e+20	O
1e+15	O
1e+10	O
100000	O
1	O
0	O
2	O
4	O
6	O
8	O
10	O
12	O
c	O
figure	O
13.13.	O
the	O
blocklength	O
nc	O
(	O
upper	O
curve	O
)	O
and	O
minimum	O
distance	B
dc	O
(	O
lower	O
curve	O
)	O
of	O
the	O
concatenated	O
hamming	O
code	B
as	O
a	O
function	B
of	O
the	O
number	O
of	O
concatenations	O
c.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
13.8	O
:	O
distance	B
isn	O
’	O
t	O
everything	O
215	O
channel	O
with	O
f	O
=	O
0:0588.	O
this	O
is	O
the	O
highest	O
noise	B
level	O
that	O
can	O
be	O
tolerated	O
using	O
this	O
concatenated	B
code	O
.	O
the	O
take-home	O
message	O
from	O
this	O
story	O
is	O
distance	B
isn	O
’	O
t	O
everything	O
.	O
the	O
minimum	B
distance	I
of	O
a	O
code	B
,	O
although	O
widely	O
worshipped	O
by	O
coding	O
theorists	O
,	O
is	O
not	O
of	O
fundamental	O
importance	O
to	O
shannon	O
’	O
s	O
mission	O
of	O
achieving	O
reliable	O
communication	B
over	O
noisy	B
channels	O
.	O
pb	O
1	O
0.01	O
0.0001	O
1e-06	O
1e-08	O
1e-10	O
1e-12	O
1e-14	O
n=3	O
21	O
315	O
61525	O
10^13	O
0	O
0.2	O
0.4	O
0.6	O
r	O
0.8	O
1	O
figure	O
13.14.	O
the	O
bit	B
error	O
probabilities	O
versus	O
the	O
rates	O
r	O
of	O
the	O
concatenated	O
hamming	O
codes	O
,	O
for	O
the	O
binary	B
symmetric	I
channel	I
with	O
f	O
=	O
0:0588.	O
labels	O
alongside	O
the	O
points	O
show	O
the	O
blocklengths	O
,	O
n	O
.	O
the	O
solid	O
line	O
shows	O
the	O
shannon	O
limit	O
for	O
this	O
channel	B
.	O
the	O
bit	B
error	O
probability	B
drops	O
to	O
zero	O
while	O
the	O
rate	B
tends	O
to	O
0.093	O
,	O
so	O
the	O
concatenated	B
hamming	O
codes	O
are	O
a	O
‘	O
good	B
’	O
code	B
family	O
.	O
d=10	O
d=20	O
d=30	O
d=40	O
d=50	O
d=60	O
1	O
1e-05	O
1e-10	O
1e-15	O
1e-20	O
0.0001	O
0.001	O
0.01	O
0.1	O
figure	O
13.15.	O
the	O
error	B
probability	I
associated	O
with	O
a	O
single	O
codeword	O
of	O
weight	O
d	O
,	O
(	O
cid:0	O
)	O
d	O
d=2	O
(	O
cid:1	O
)	O
f	O
d=2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
d=2	O
,	O
as	O
a	O
function	B
of	O
f	O
.	O
.	O
exercise	O
13.5	O
.	O
[	O
3	O
]	O
prove	O
that	O
there	O
exist	O
families	O
of	O
codes	O
with	O
‘	O
bad	B
’	O
distance	B
that	O
are	O
‘	O
very	B
good	I
’	O
codes	O
.	O
13.8	O
distance	B
isn	O
’	O
t	O
everything	O
let	O
’	O
s	O
get	O
a	O
quantitative	O
feeling	O
for	O
the	O
e	O
(	O
cid:11	O
)	O
ect	O
of	O
the	O
minimum	O
distance	B
of	O
a	O
code	B
,	O
for	O
the	O
special	O
case	O
of	O
a	O
binary	B
symmetric	I
channel	I
.	O
the	O
error	B
probability	I
associated	O
with	O
one	O
low-weight	O
codeword	B
let	O
a	O
binary	O
code	O
have	O
blocklength	O
n	O
and	O
just	O
two	O
codewords	O
,	O
which	O
di	O
(	O
cid:11	O
)	O
er	O
in	O
d	O
places	O
.	O
for	O
simplicity	O
,	O
let	O
’	O
s	O
assume	O
d	O
is	O
even	O
.	O
what	O
is	O
the	O
error	B
probability	I
if	O
this	O
code	B
is	O
used	O
on	O
a	O
binary	B
symmetric	I
channel	I
with	O
noise	B
level	O
f	O
?	O
bit	B
(	O
cid:13	O
)	O
ips	O
matter	O
only	O
in	O
places	O
where	O
the	O
two	O
codewords	O
di	O
(	O
cid:11	O
)	O
er	O
.	O
the	O
error	B
probability	I
is	O
dominated	O
by	O
the	O
probability	B
that	O
d=2	O
of	O
these	O
bits	O
are	O
(	O
cid:13	O
)	O
ipped	O
.	O
what	O
happens	O
to	O
the	O
other	O
bits	O
is	O
irrelevant	O
,	O
since	O
the	O
optimal	B
decoder	I
ignores	O
them	O
.	O
p	O
(	O
block	B
error	O
)	O
’	O
(	O
cid:18	O
)	O
d	O
d=2	O
(	O
cid:19	O
)	O
f	O
d=2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
d=2	O
:	O
(	O
13.22	O
)	O
this	O
error	B
probability	I
associated	O
with	O
a	O
single	O
codeword	O
of	O
weight	O
d	O
is	O
plotted	O
in	O
(	O
cid:12	O
)	O
gure	O
13.15.	O
using	O
the	O
approximation	B
for	O
the	O
binomial	B
coe	O
(	O
cid:14	O
)	O
cient	O
(	O
1.16	O
)	O
,	O
we	O
can	O
further	O
approximate	O
p	O
(	O
block	B
error	O
)	O
’	O
h2f	O
1=2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
1=2id	O
(	O
cid:17	O
)	O
[	O
(	O
cid:12	O
)	O
(	O
f	O
)	O
]	O
d	O
;	O
(	O
13.23	O
)	O
(	O
13.24	O
)	O
where	O
(	O
cid:12	O
)	O
(	O
f	O
)	O
=	O
2f	O
1=2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
1=2	O
is	O
called	O
the	O
bhattacharyya	O
parameter	O
of	O
the	O
channel	O
.	O
now	O
,	O
consider	O
a	O
general	O
linear	O
code	B
with	O
distance	B
d.	O
its	O
block	B
error	O
prob-	O
ability	O
must	O
be	O
at	O
least	O
(	O
cid:0	O
)	O
d	O
d=2	O
(	O
cid:1	O
)	O
f	O
d=2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
d=2	O
,	O
independent	O
of	O
the	O
blocklength	O
n	O
of	O
the	O
code	O
.	O
for	O
this	O
reason	O
,	O
a	O
sequence	B
of	O
codes	O
of	O
increasing	O
blocklength	O
n	O
and	O
constant	O
distance	B
d	O
(	O
i.e.	O
,	O
‘	O
very	B
bad	I
’	O
distance	B
)	O
can	O
not	O
have	O
a	O
block	B
er-	O
ror	O
probability	B
that	O
tends	O
to	O
zero	O
,	O
on	O
any	O
binary	B
symmetric	I
channel	I
.	O
if	O
we	O
are	O
interested	O
in	O
making	O
superb	O
error-correcting	B
codes	I
with	O
tiny	O
,	O
tiny	O
error	B
probability	I
,	O
we	O
might	O
therefore	O
shun	O
codes	O
with	O
bad	O
distance	B
.	O
however	O
,	O
being	O
pragmatic	O
,	O
we	O
should	O
look	O
more	O
carefully	O
at	O
(	O
cid:12	O
)	O
gure	O
13.15.	O
in	O
chapter	O
1	O
we	O
argued	O
that	O
codes	O
for	O
disk	O
drives	O
need	O
an	O
error	B
probability	I
smaller	O
than	O
about	O
10	O
(	O
cid:0	O
)	O
18.	O
if	O
the	O
raw	O
error	B
probability	I
in	O
the	O
disk	B
drive	I
is	O
about	O
0:001	O
,	O
the	O
error	B
probability	I
associated	O
with	O
one	O
codeword	B
at	O
distance	B
d	O
=	O
20	O
is	O
smaller	O
than	O
10	O
(	O
cid:0	O
)	O
24.	O
if	O
the	O
raw	O
error	B
probability	I
in	O
the	O
disk	B
drive	I
is	O
about	O
0:01	O
,	O
the	O
error	B
probability	I
associated	O
with	O
one	O
codeword	B
at	O
distance	B
d	O
=	O
30	O
is	O
smaller	O
than	O
10	O
(	O
cid:0	O
)	O
20.	O
for	O
practical	O
purposes	O
,	O
therefore	O
,	O
it	O
is	O
not	O
essential	O
for	O
a	O
code	B
to	O
have	O
good	B
distance	O
.	O
for	O
example	O
,	O
codes	O
of	O
blocklength	O
10	O
000	O
,	O
known	O
to	O
have	O
many	O
codewords	O
of	O
weight	O
32	O
,	O
can	O
nevertheless	O
correct	O
errors	B
of	O
weight	B
320	O
with	O
tiny	O
error	B
probability	I
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
216	O
13	O
|	O
binary	O
codes	O
i	O
wouldn	O
’	O
t	O
want	O
you	O
to	O
think	O
i	O
am	O
recommending	O
the	O
use	O
of	O
codes	O
with	O
bad	O
distance	B
;	O
in	O
chapter	O
47	O
we	O
will	O
discuss	O
low-density	B
parity-check	I
codes	O
,	O
my	O
favourite	O
codes	O
,	O
which	O
have	O
both	O
excellent	O
performance	O
and	O
good	O
distance	B
.	O
13.9	O
the	O
union	B
bound	I
the	O
error	B
probability	I
of	O
a	O
code	B
on	O
the	O
binary	B
symmetric	I
channel	I
can	O
be	O
bounded	O
in	O
terms	O
of	O
its	O
weight	B
enumerator	I
function	O
by	O
adding	O
up	O
appropriate	O
multiples	O
of	O
the	O
error	O
probability	B
associated	O
with	O
a	O
single	O
codeword	O
(	O
13.24	O
)	O
:	O
p	O
(	O
block	B
error	O
)	O
(	O
cid:20	O
)	O
xw	O
>	O
0	O
a	O
(	O
w	O
)	O
[	O
(	O
cid:12	O
)	O
(	O
f	O
)	O
]	O
w	O
:	O
(	O
13.25	O
)	O
this	O
inequality	B
,	O
which	O
is	O
an	O
example	O
of	O
a	O
union	B
bound	I
,	O
is	O
accurate	O
for	O
low	O
noise	B
levels	O
f	O
,	O
but	O
inaccurate	O
for	O
high	O
noise	B
levels	O
,	O
because	O
it	O
overcounts	O
the	O
contribution	O
of	O
errors	O
that	O
cause	O
confusion	O
with	O
more	O
than	O
one	O
codeword	B
at	O
a	O
time	O
.	O
.	O
exercise	O
13.6	O
.	O
[	O
3	O
]	O
poor	O
man	O
’	O
s	O
noisy-channel	B
coding	I
theorem	I
.	O
pretending	O
that	O
the	O
union	B
bound	I
(	O
13.25	O
)	O
is	O
accurate	O
,	O
and	O
using	O
the	O
aver-	O
age	O
weight	B
enumerator	I
function	O
of	O
a	O
random	B
linear	I
code	O
(	O
13.14	O
)	O
(	O
section	B
13.5	O
)	O
as	O
a	O
(	O
w	O
)	O
,	O
estimate	O
the	O
maximum	O
rate	O
rub	O
(	O
f	O
)	O
at	O
which	O
one	O
can	O
communicate	O
over	O
a	O
binary	B
symmetric	I
channel	I
.	O
or	O
,	O
to	O
look	O
at	O
it	O
more	O
positively	O
,	O
using	O
the	O
union	B
bound	I
(	O
13.25	O
)	O
as	O
an	O
inequality	B
,	O
show	O
that	O
communication	B
at	O
rates	O
up	O
to	O
rub	O
(	O
f	O
)	O
is	O
possible	O
over	O
the	O
binary	B
symmetric	I
channel	I
.	O
in	O
the	O
following	O
chapter	O
,	O
by	O
analysing	O
the	O
probability	B
of	I
error	I
of	O
syndrome	B
decoding	I
for	O
a	O
binary	O
linear	O
code	B
,	O
and	O
using	O
a	O
union	B
bound	I
,	O
we	O
will	O
prove	O
shannon	O
’	O
s	O
noisy-channel	B
coding	I
theorem	I
(	O
for	O
symmetric	O
binary	O
channels	O
)	O
,	O
and	O
thus	O
show	O
that	O
very	B
good	I
linear	O
codes	O
exist	O
.	O
13.10	O
dual	B
codes	O
a	O
concept	O
that	O
has	O
some	O
importance	O
in	O
coding	B
theory	I
,	O
though	O
we	O
will	O
have	O
no	O
immediate	O
use	O
for	O
it	O
in	O
this	O
book	O
,	O
is	O
the	O
idea	O
of	O
the	O
dual	O
of	O
a	O
linear	B
error-	O
correcting	O
code	B
.	O
an	O
(	O
n	O
;	O
k	O
)	O
linear	B
error-correcting	O
code	B
can	O
be	O
thought	O
of	O
as	O
a	O
set	B
of	O
2k	O
codewords	O
generated	O
by	O
adding	O
together	O
all	O
combinations	O
of	O
k	O
independent	O
basis	O
codewords	O
.	O
the	O
generator	B
matrix	I
of	O
the	O
code	B
consists	O
of	O
those	O
k	O
basis	O
codewords	O
,	O
conventionally	O
written	O
as	O
row	O
vectors	B
.	O
for	O
example	O
,	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
’	O
s	O
generator	B
matrix	I
(	O
from	O
p.10	O
)	O
is	O
3	O
775	O
(	O
13.26	O
)	O
g	O
=2	O
664	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
and	O
its	O
sixteen	O
codewords	O
were	O
displayed	O
in	O
table	O
1.14	O
(	O
p.9	O
)	O
.	O
the	O
code-	O
words	O
of	O
this	O
code	B
are	O
linear	B
combinations	O
of	O
the	O
four	O
vectors	B
[	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
]	O
,	O
[	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
]	O
,	O
[	O
0	O
0	O
1	O
0	O
1	O
1	O
1	O
]	O
,	O
and	O
[	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
]	O
.	O
an	O
(	O
n	O
;	O
k	O
)	O
code	B
may	O
also	O
be	O
described	O
in	O
terms	O
of	O
an	O
m	O
(	O
cid:2	O
)	O
n	O
parity-check	B
matrix	I
(	O
where	O
m	O
=	O
n	O
(	O
cid:0	O
)	O
k	O
)	O
as	O
the	O
set	B
of	O
vectors	B
ftg	O
that	O
satisfy	O
ht	O
=	O
0	O
:	O
(	O
13.27	O
)	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
13.10	O
:	O
dual	B
codes	O
217	O
one	O
way	O
of	O
thinking	O
of	O
this	O
equation	O
is	O
that	O
each	O
row	O
of	O
h	O
speci	O
(	O
cid:12	O
)	O
es	O
a	O
vector	O
to	O
which	O
t	O
must	O
be	O
orthogonal	O
if	O
it	O
is	O
a	O
codeword	B
.	O
the	O
generator	B
matrix	I
speci	O
(	O
cid:12	O
)	O
es	O
k	O
vectors	B
from	O
which	O
all	O
codewords	O
can	O
be	O
built	O
,	O
and	O
the	O
parity-check	B
matrix	I
speci	O
(	O
cid:12	O
)	O
es	O
a	O
set	B
of	O
m	O
vectors	B
to	O
which	O
all	O
codewords	O
are	O
orthogonal	O
.	O
the	O
dual	B
of	O
a	O
code	B
is	O
obtained	O
by	O
exchanging	O
the	O
generator	B
matrix	I
and	O
the	O
parity-check	B
matrix	I
.	O
de	O
(	O
cid:12	O
)	O
nition	O
.	O
the	O
set	B
of	O
all	O
vectors	B
of	O
length	B
n	O
that	O
are	O
orthogonal	O
to	O
all	O
code-	O
words	O
in	O
a	O
code	B
,	O
c	O
,	O
is	O
called	O
the	O
dual	B
of	O
the	O
code	B
,	O
c	O
?	O
.	O
if	O
t	O
is	O
orthogonal	O
to	O
h1	O
and	O
h2	O
,	O
then	O
it	O
is	O
also	O
orthogonal	O
to	O
h3	O
(	O
cid:17	O
)	O
h1	O
+	O
h2	O
;	O
so	O
all	O
codewords	O
are	O
orthogonal	O
to	O
any	O
linear	B
combination	O
of	O
the	O
m	O
rows	O
of	O
h.	O
so	O
the	O
set	B
of	O
all	O
linear	B
combinations	O
of	O
the	O
rows	O
of	O
the	O
parity-check	O
matrix	B
is	O
the	O
dual	B
code	O
.	O
for	O
our	O
hamming	O
(	O
7	O
;	O
4	O
)	O
code	B
,	O
the	O
parity-check	B
matrix	I
is	O
(	O
from	O
p.12	O
)	O
:	O
h	O
=	O
(	O
cid:2	O
)	O
p	O
i3	O
(	O
cid:3	O
)	O
=2	O
4	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
3	O
5	O
:	O
(	O
13.28	O
)	O
the	O
dual	B
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
h	O
(	O
7	O
;	O
4	O
)	O
is	O
the	O
code	B
shown	O
in	O
table	O
13.16	O
.	O
0000000	O
0010111	O
0101101	O
0111010	O
1001110	O
1011001	O
1100011	O
1110100	O
a	O
possibly	O
unexpected	O
property	O
of	O
this	O
pair	O
of	O
codes	O
is	O
that	O
the	O
dual	B
,	O
h	O
?	O
(	O
7	O
;	O
4	O
)	O
,	O
is	O
contained	O
within	O
the	O
code	B
h	O
(	O
7	O
;	O
4	O
)	O
itself	O
:	O
every	O
word	O
in	O
the	O
dual	B
code	O
is	O
a	O
codeword	B
of	O
the	O
original	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
.	O
this	O
relationship	O
can	O
be	O
written	O
using	O
set	B
notation	O
:	O
h	O
?	O
(	O
7	O
;	O
4	O
)	O
(	O
cid:26	O
)	O
h	O
(	O
7	O
;	O
4	O
)	O
:	O
(	O
13.29	O
)	O
the	O
possibility	O
that	O
the	O
set	B
of	O
dual	B
vectors	O
can	O
overlap	O
the	O
set	B
of	O
codeword	B
vectors	O
is	O
counterintuitive	O
if	O
we	O
think	O
of	O
the	O
vectors	O
as	O
real	O
vectors	B
{	O
how	O
can	O
a	O
vector	O
be	O
orthogonal	O
to	O
itself	O
?	O
but	O
when	O
we	O
work	O
in	O
modulo-two	O
arithmetic	O
,	O
many	O
non-zero	O
vectors	B
are	O
indeed	O
orthogonal	O
to	O
themselves	O
!	O
.	O
exercise	O
13.7	O
.	O
[	O
1	O
,	O
p.223	O
]	O
give	O
a	O
simple	O
rule	O
that	O
distinguishes	O
whether	O
a	O
binary	O
vector	O
is	O
orthogonal	O
to	O
itself	O
,	O
as	O
is	O
each	O
of	O
the	O
three	O
vectors	B
[	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
]	O
,	O
[	O
0	O
1	O
1	O
1	O
0	O
1	O
0	O
]	O
,	O
and	O
[	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
]	O
.	O
some	O
more	O
duals	O
in	O
general	O
,	O
if	O
a	O
code	B
has	O
a	O
systematic	B
generator	O
matrix	B
,	O
g	O
=	O
[	O
ikjpt	O
]	O
;	O
where	O
p	O
is	O
a	O
k	O
(	O
cid:2	O
)	O
m	O
matrix	B
,	O
then	O
its	O
parity-check	B
matrix	I
is	O
h	O
=	O
[	O
pjim	O
]	O
:	O
(	O
13.30	O
)	O
(	O
13.31	O
)	O
table	O
13.16.	O
the	O
eight	O
codewords	O
of	O
the	O
dual	O
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
.	O
[	O
compare	O
with	O
table	O
1.14	O
,	O
p.9	O
.	O
]	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
218	O
13	O
|	O
binary	O
codes	O
example	O
13.8.	O
the	O
repetition	B
code	I
r3	O
has	O
generator	B
matrix	I
its	O
parity-check	B
matrix	I
is	O
g	O
=	O
(	O
cid:2	O
)	O
1	O
1	O
1	O
(	O
cid:3	O
)	O
;	O
1	O
0	O
1	O
(	O
cid:21	O
)	O
:	O
h	O
=	O
(	O
cid:20	O
)	O
1	O
1	O
0	O
the	O
two	O
codewords	O
are	O
[	O
1	O
1	O
1	O
]	O
and	O
[	O
0	O
0	O
0	O
]	O
.	O
the	O
dual	B
code	O
has	O
generator	B
matrix	I
g	O
?	O
=	O
h	O
=	O
(	O
cid:20	O
)	O
1	O
1	O
0	O
1	O
0	O
1	O
(	O
cid:21	O
)	O
(	O
13.32	O
)	O
(	O
13.33	O
)	O
(	O
13.34	O
)	O
or	O
equivalently	O
,	O
modifying	O
g	O
?	O
into	O
systematic	B
form	O
by	O
row	O
additions	O
,	O
g	O
?	O
=	O
(	O
cid:20	O
)	O
1	O
0	O
1	O
0	O
1	O
1	O
(	O
cid:21	O
)	O
:	O
(	O
13.35	O
)	O
we	O
call	O
this	O
dual	B
code	O
the	O
simple	B
parity	I
code	O
p3	O
;	O
it	O
is	O
the	O
code	B
with	O
one	O
parity-check	O
bit	O
,	O
which	O
is	O
equal	O
to	O
the	O
sum	O
of	O
the	O
two	O
source	O
bits	O
.	O
the	O
dual	B
code	O
’	O
s	O
four	O
codewords	O
are	O
[	O
1	O
1	O
0	O
]	O
,	O
[	O
1	O
0	O
1	O
]	O
,	O
[	O
0	O
0	O
0	O
]	O
,	O
and	O
[	O
0	O
1	O
1	O
]	O
.	O
in	O
this	O
case	O
,	O
the	O
only	O
vector	O
common	O
to	O
the	O
code	B
and	O
the	O
dual	B
is	O
the	O
all-zero	O
codeword	B
.	O
goodness	O
of	O
duals	O
if	O
a	O
sequence	B
of	O
codes	O
is	O
‘	O
good	B
’	O
,	O
are	O
their	O
duals	O
good	B
too	O
?	O
examples	O
can	O
be	O
constructed	O
of	O
all	O
cases	O
:	O
good	B
codes	O
with	O
good	O
duals	O
(	O
random	B
linear	I
codes	O
)	O
;	O
bad	B
codes	O
with	O
bad	O
duals	O
;	O
and	O
good	O
codes	O
with	O
bad	O
duals	O
.	O
the	O
last	O
category	O
is	O
especially	O
important	O
:	O
many	O
state-of-the-art	O
codes	O
have	O
the	O
property	O
that	O
their	O
duals	O
are	O
bad	B
.	O
the	O
classic	O
example	O
is	O
the	O
low-density	B
parity-check	I
code	I
,	O
whose	O
dual	B
is	O
a	O
low-density	B
generator-matrix	I
code	I
.	O
.	O
exercise	O
13.9	O
.	O
[	O
3	O
]	O
show	O
that	O
low-density	B
generator-matrix	I
codes	O
are	O
bad	B
.	O
a	O
family	O
of	O
low-density	O
generator-matrix	O
codes	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
two	O
param-	O
eters	O
j	O
;	O
k	O
,	O
which	O
are	O
the	O
column	O
weight	B
and	O
row	O
weight	B
of	O
all	O
rows	O
and	O
columns	O
respectively	O
of	O
g.	O
these	O
weights	O
are	O
(	O
cid:12	O
)	O
xed	O
,	O
independent	O
of	O
n	O
;	O
for	O
example	O
,	O
(	O
j	O
;	O
k	O
)	O
=	O
(	O
3	O
;	O
6	O
)	O
.	O
[	O
hint	O
:	O
show	O
that	O
the	O
code	B
has	O
low-weight	O
codewords	O
,	O
then	O
use	O
the	O
argument	O
from	O
p.215	O
.	O
]	O
exercise	O
13.10	O
.	O
[	O
5	O
]	O
show	O
that	O
low-density	B
parity-check	I
codes	O
are	O
good	B
,	O
and	O
have	O
good	B
distance	O
.	O
(	O
for	O
solutions	O
,	O
see	O
gallager	O
(	O
1963	O
)	O
and	O
mackay	O
(	O
1999b	O
)	O
.	O
)	O
self-dual	B
codes	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
had	O
the	O
property	O
that	O
the	O
dual	B
was	O
contained	O
in	O
the	O
code	B
itself	O
.	O
a	O
code	B
is	O
self-orthogonal	B
if	O
it	O
is	O
contained	O
in	O
its	O
dual	B
.	O
for	O
example	O
,	O
the	O
dual	B
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
is	O
a	O
self-orthogonal	B
code	O
.	O
one	O
way	O
of	O
seeing	O
this	O
is	O
that	O
the	O
overlap	O
between	O
any	O
pair	O
of	O
rows	O
of	O
h	O
is	O
even	O
.	O
codes	O
that	O
contain	O
their	O
duals	O
are	O
important	O
in	O
quantum	O
error-correction	B
(	O
calderbank	O
and	O
shor	O
,	O
1996	O
)	O
.	O
it	O
is	O
intriguing	O
,	O
though	O
not	O
necessarily	O
useful	O
,	O
to	O
look	O
at	O
codes	O
that	O
are	O
self-dual	B
.	O
a	O
code	B
c	O
is	O
self-dual	B
if	O
the	O
dual	B
of	O
the	O
code	B
is	O
identical	O
to	O
the	O
code	B
.	O
(	O
13.36	O
)	O
c	O
?	O
=	O
c	O
:	O
some	O
properties	O
of	O
self-dual	O
codes	O
can	O
be	O
deduced	O
:	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
13.11	O
:	O
generalizing	O
perfectness	O
to	O
other	O
channels	O
219	O
1.	O
if	O
a	O
code	B
is	O
self-dual	B
,	O
then	O
its	O
generator	B
matrix	I
is	O
also	O
a	O
parity-check	B
matrix	I
for	O
the	O
code	B
.	O
2.	O
self-dual	B
codes	O
have	O
rate	B
1=2	O
,	O
i.e.	O
,	O
m	O
=	O
k	O
=	O
n=2	O
.	O
3.	O
all	O
codewords	O
have	O
even	O
weight	B
.	O
.	O
exercise	O
13.11	O
.	O
[	O
2	O
,	O
p.223	O
]	O
what	O
property	O
must	O
the	O
matrix	B
p	O
satisfy	O
,	O
if	O
the	O
code	B
with	O
generator	B
matrix	I
g	O
=	O
[	O
ikjpt	O
]	O
is	O
self-dual	B
?	O
examples	O
of	O
self-dual	O
codes	O
1.	O
the	O
repetition	B
code	I
r2	O
is	O
a	O
simple	O
example	O
of	O
a	O
self-dual	B
code	O
.	O
2.	O
the	O
smallest	O
non-trivial	O
self-dual	B
code	O
is	O
the	O
following	O
(	O
8	O
;	O
4	O
)	O
code	B
.	O
g	O
=	O
h	O
=	O
(	O
cid:2	O
)	O
1	O
1	O
(	O
cid:3	O
)	O
:	O
(	O
13.37	O
)	O
g	O
=	O
(	O
cid:2	O
)	O
i4	O
pt	O
(	O
cid:3	O
)	O
=2	O
664	O
1	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
:	O
3	O
775	O
(	O
13.38	O
)	O
.	O
exercise	O
13.12	O
.	O
[	O
2	O
,	O
p.223	O
]	O
find	O
the	O
relationship	O
of	O
the	O
above	O
(	O
8	O
;	O
4	O
)	O
code	B
to	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
.	O
duals	O
and	O
graphs	O
let	O
a	O
code	B
be	O
represented	O
by	O
a	O
graph	B
in	O
which	O
there	O
are	O
nodes	O
of	O
two	O
types	O
,	O
parity-check	B
constraints	I
and	O
equality	O
constraints	O
,	O
joined	O
by	O
edges	O
which	O
rep-	O
resent	O
the	O
bits	O
of	O
the	O
code	B
(	O
not	O
all	O
of	O
which	O
need	O
be	O
transmitted	O
)	O
.	O
the	O
dual	B
code	O
’	O
s	O
graph	B
is	O
obtained	O
by	O
replacing	O
all	O
parity-check	B
nodes	I
by	O
equality	O
nodes	O
and	O
vice	O
versa	O
.	O
this	O
type	O
of	O
graph	O
is	O
called	O
a	O
normal	B
graph	I
by	O
forney	O
(	O
2001	O
)	O
.	O
further	O
reading	O
duals	O
are	O
important	O
in	O
coding	O
theory	B
because	O
functions	B
involving	O
a	O
code	B
(	O
such	O
as	O
the	O
posterior	O
distribution	O
over	O
codewords	O
)	O
can	O
be	O
transformed	O
by	O
a	O
fourier	O
transform	O
into	O
functions	B
over	O
the	O
dual	B
code	O
.	O
for	O
an	O
accessible	O
introduction	O
to	O
fourier	O
analysis	B
on	O
(	O
cid:12	O
)	O
nite	O
groups	O
,	O
see	O
terras	O
(	O
1999	O
)	O
.	O
see	O
also	O
macwilliams	O
and	O
sloane	O
(	O
1977	O
)	O
.	O
13.11	O
generalizing	O
perfectness	O
to	O
other	O
channels	O
having	O
given	O
up	O
on	O
the	O
search	O
for	O
perfect	O
codes	O
for	O
the	O
binary	B
symmetric	I
channel	I
,	O
we	O
could	O
console	O
ourselves	O
by	O
changing	O
channel	B
.	O
we	O
could	O
call	O
a	O
code	B
‘	O
a	O
perfect	B
u-error-correcting	O
code	B
for	O
the	O
binary	B
erasure	I
channel	I
’	O
if	O
it	O
can	O
restore	O
any	O
u	O
erased	O
bits	O
,	O
and	O
never	O
more	O
than	O
u.	O
rather	O
than	O
using	O
the	O
word	O
perfect	B
,	O
however	O
,	O
the	O
conventional	O
term	O
for	O
such	O
a	O
code	B
is	O
a	O
‘	O
maximum	B
distance	I
separable	I
code	O
’	O
,	O
or	O
mds	O
code	B
.	O
as	O
we	O
already	O
noted	O
in	O
exercise	O
11.10	O
(	O
p.190	O
)	O
,	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
is	O
not	O
an	O
mds	O
code	B
.	O
it	O
can	O
recover	O
some	O
sets	O
of	O
3	O
erased	O
bits	O
,	O
but	O
not	O
all	O
.	O
if	O
any	O
3	O
bits	O
corresponding	O
to	O
a	O
codeword	B
of	O
weight	B
3	O
are	O
erased	O
,	O
then	O
one	O
bit	B
of	O
information	B
is	O
unrecoverable	O
.	O
this	O
is	O
why	O
the	O
(	O
7	O
;	O
4	O
)	O
code	B
is	O
a	O
poor	O
choice	O
for	O
a	O
raid	O
system	O
.	O
in	O
a	O
perfect	B
u-error-correcting	O
code	B
for	O
the	O
binary	B
erasure	I
channel	I
,	O
the	O
number	O
of	O
redundant	O
bits	O
must	O
be	O
n	O
(	O
cid:0	O
)	O
k	O
=	O
u	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
220	O
13	O
|	O
binary	O
codes	O
a	O
tiny	O
example	O
of	O
a	O
maximum	B
distance	I
separable	I
code	O
is	O
the	O
simple	O
parity-	O
check	O
code	B
p3	O
whose	O
parity-check	B
matrix	I
is	O
h	O
=	O
[	O
1	O
1	O
1	O
]	O
.	O
this	O
code	B
has	O
4	O
codewords	O
,	O
all	O
of	O
which	O
have	O
even	O
parity	B
.	O
all	O
codewords	O
are	O
separated	O
by	O
a	O
distance	B
of	O
2.	O
any	O
single	O
erased	O
bit	B
can	O
be	O
restored	O
by	O
setting	O
it	O
to	O
the	O
parity	B
of	O
the	O
other	O
two	O
bits	O
.	O
the	O
repetition	B
codes	O
are	O
also	O
maximum	B
distance	I
separable	I
codes	O
.	O
.	O
exercise	O
13.13	O
.	O
[	O
5	O
,	O
p.224	O
]	O
can	O
you	O
make	O
an	O
(	O
n	O
;	O
k	O
)	O
code	B
,	O
with	O
m	O
=	O
n	O
(	O
cid:0	O
)	O
k	O
parity	B
symbols	O
,	O
for	O
a	O
q-ary	O
erasure	B
channel	I
,	O
such	O
that	O
the	O
decoder	B
can	O
recover	O
the	O
codeword	B
when	O
any	O
m	O
symbols	O
are	O
erased	O
in	O
a	O
block	B
of	O
n	O
?	O
[	O
example	O
:	O
for	O
the	O
channel	O
with	O
q	O
=	O
4	O
symbols	O
there	O
is	O
an	O
(	O
n	O
;	O
k	O
)	O
=	O
(	O
5	O
;	O
2	O
)	O
code	B
which	O
can	O
correct	O
any	O
m	O
=	O
3	O
erasures	O
.	O
]	O
for	O
the	O
q-ary	O
erasure	B
channel	I
with	O
q	O
>	O
2	O
,	O
there	O
are	O
large	O
numbers	O
of	O
mds	O
codes	O
,	O
of	O
which	O
the	O
reed	O
{	O
solomon	O
codes	O
are	O
the	O
most	O
famous	O
and	O
most	O
widely	O
used	O
.	O
as	O
long	O
as	O
the	O
(	O
cid:12	O
)	O
eld	O
size	O
q	O
is	O
bigger	O
than	O
the	O
blocklength	O
n	O
,	O
mds	O
block	B
codes	O
of	O
any	O
rate	O
can	O
be	O
found	O
.	O
(	O
for	O
further	O
reading	O
,	O
see	O
lin	O
and	O
costello	O
(	O
1983	O
)	O
.	O
)	O
13.12	O
summary	B
shannon	O
’	O
s	O
codes	O
for	O
the	O
binary	B
symmetric	I
channel	I
can	O
almost	O
always	O
correct	O
fn	O
errors	B
,	O
but	O
they	O
are	O
not	O
fn	O
-error-correcting	O
codes	O
.	O
reasons	O
why	O
the	O
distance	B
of	O
a	O
code	B
has	O
little	O
relevance	O
1.	O
the	O
shannon	O
limit	O
shows	O
that	O
the	O
best	O
codes	O
must	O
be	O
able	O
to	O
cope	O
with	O
a	O
noise	B
level	O
twice	O
as	O
big	O
as	O
the	O
maximum	O
noise	O
level	O
for	O
a	O
bounded-	O
distance	B
decoder	O
.	O
2.	O
when	O
the	O
binary	B
symmetric	I
channel	I
has	O
f	O
>	O
1=4	O
,	O
no	O
code	B
with	O
a	O
bounded-distance	B
decoder	I
can	O
communicate	O
at	O
all	O
;	O
but	O
shannon	O
says	O
good	B
codes	O
exist	O
for	O
such	O
channels	O
.	O
3.	O
concatenation	B
shows	O
that	O
we	O
can	O
get	O
good	B
performance	O
even	O
if	O
the	O
dis-	O
tance	O
is	O
bad	B
.	O
the	O
whole	O
weight	B
enumerator	I
function	O
is	O
relevant	O
to	O
the	O
question	O
of	O
whether	O
a	O
code	B
is	O
a	O
good	B
code	O
.	O
the	O
relationship	O
between	O
good	B
codes	O
and	B
distance	I
properties	O
is	O
discussed	O
further	O
in	O
exercise	O
13.14	O
(	O
p.220	O
)	O
.	O
13.13	O
further	O
exercises	O
exercise	O
13.14	O
.	O
[	O
3	O
,	O
p.224	O
]	O
a	O
codeword	B
t	O
is	O
selected	O
from	O
a	O
linear	B
(	O
n	O
;	O
k	O
)	O
code	B
c	O
,	O
and	O
it	O
is	O
transmitted	O
over	O
a	O
noisy	B
channel	I
;	O
the	O
received	O
signal	O
is	O
y.	O
we	O
assume	O
that	O
the	O
channel	B
is	O
a	O
memoryless	O
channel	B
such	O
as	O
a	O
gaus-	O
sian	O
channel	B
.	O
given	O
an	O
assumed	O
channel	B
model	O
p	O
(	O
y	O
j	O
t	O
)	O
,	O
there	O
are	O
two	O
decoding	B
problems	O
.	O
the	O
codeword	B
decoding	O
problem	O
is	O
the	O
task	O
of	O
inferring	O
which	O
codeword	B
t	O
was	O
transmitted	O
given	O
the	O
received	O
signal	O
.	O
the	O
bitwise	B
decoding	O
problem	O
is	O
the	O
task	O
of	O
inferring	O
for	O
each	O
transmitted	O
bit	B
tn	O
how	O
likely	O
it	O
is	O
that	O
that	O
bit	B
was	O
a	O
one	O
rather	O
than	O
a	O
zero	O
.	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
13.13	O
:	O
further	O
exercises	O
221	O
consider	O
optimal	B
decoders	O
for	O
these	O
two	O
decoding	B
problems	O
.	O
prove	O
that	O
the	O
probability	B
of	I
error	I
of	O
the	O
optimal	B
bitwise-decoder	O
is	O
closely	O
related	O
to	O
the	O
probability	B
of	I
error	I
of	O
the	O
optimal	B
codeword-decoder	O
,	O
by	O
proving	O
the	O
following	O
theorem	B
.	O
theorem	B
13.1	O
if	O
a	O
binary	O
linear	O
code	B
has	O
minimum	B
distance	I
dmin	O
,	O
then	O
,	O
for	O
any	O
given	O
channel	B
,	O
the	O
codeword	B
bit	O
error	B
probability	I
of	O
the	O
optimal	B
bitwise	O
decoder	B
,	O
pb	O
,	O
and	O
the	O
block	B
error	O
probability	O
of	O
the	O
maxi-	O
mum	O
likelihood	B
decoder	O
,	O
pb	O
,	O
are	O
related	O
by	O
:	O
pb	O
(	O
cid:21	O
)	O
pb	O
(	O
cid:21	O
)	O
1	O
2	O
dmin	O
n	O
pb	O
:	O
(	O
13.39	O
)	O
exercise	O
13.15	O
.	O
[	O
1	O
]	O
what	O
are	O
the	O
minimum	O
distances	O
of	O
the	O
(	O
15	O
;	O
11	O
)	O
hamming	O
code	B
and	O
the	O
(	O
31	O
;	O
26	O
)	O
hamming	O
code	B
?	O
.	O
exercise	O
13.16	O
.	O
[	O
2	O
]	O
let	O
a	O
(	O
w	O
)	O
be	O
the	O
average	B
weight	O
enumerator	O
function	B
of	O
a	O
rate-1=3	O
random	B
linear	I
code	O
with	O
n	O
=	O
540	O
and	O
m	O
=	O
360.	O
estimate	O
,	O
from	O
(	O
cid:12	O
)	O
rst	O
principles	O
,	O
the	O
value	O
of	O
a	O
(	O
w	O
)	O
at	O
w	O
=	O
1.	O
exercise	O
13.17	O
.	O
[	O
3c	O
]	O
a	O
code	B
with	O
minimum	B
distance	I
greater	O
than	O
dgv	O
.	O
a	O
rather	O
nice	O
(	O
15	O
;	O
5	O
)	O
code	B
is	O
generated	O
by	O
this	O
generator	B
matrix	I
,	O
which	O
is	O
based	O
on	O
measuring	O
the	O
parities	O
of	O
all	O
the	O
(	O
cid:0	O
)	O
5	O
g	O
=2	O
66664	O
(	O
cid:1	O
)	O
1	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
1	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
3	O
(	O
cid:1	O
)	O
=	O
10	O
triplets	O
of	O
source	O
bits	O
:	O
(	O
cid:1	O
)	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
(	O
cid:1	O
)	O
1	O
1	O
1	O
1	O
1	O
1	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
1	O
1	O
(	O
cid:1	O
)	O
1	O
1	O
(	O
cid:1	O
)	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
3	O
77775	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
find	O
the	O
minimum	B
distance	I
and	O
weight	B
enumerator	I
function	O
of	O
this	O
code	B
.	O
:	O
(	O
13.40	O
)	O
exercise	O
13.18	O
.	O
[	O
3c	O
]	O
find	O
the	O
minimum	B
distance	I
of	O
the	O
‘	O
pentagonful	B
’	O
low-	O
density	B
parity-check	O
code	B
whose	O
parity-check	B
matrix	I
is	O
h	O
=	O
:	O
(	O
13.41	O
)	O
2	O
666666666666664	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
3	O
777777777777775	O
show	O
that	O
nine	O
of	O
the	O
ten	O
rows	O
are	O
independent	O
,	O
so	O
the	O
code	B
has	O
param-	O
eters	O
n	O
=	O
15	O
,	O
k	O
=	O
6.	O
using	O
a	O
computer	B
,	O
(	O
cid:12	O
)	O
nd	O
its	O
weight	B
enumerator	I
function	O
.	O
.	O
exercise	O
13.19	O
.	O
[	O
3c	O
]	O
replicate	O
the	O
calculations	O
used	O
to	O
produce	O
(	O
cid:12	O
)	O
gure	O
13.12.	O
check	O
the	O
assertion	O
that	O
the	O
highest	O
noise	B
level	O
that	O
’	O
s	O
correctable	O
is	O
0.0588.	O
explore	B
alternative	O
concatenated	B
sequences	O
of	O
codes	O
.	O
can	O
you	O
(	O
cid:12	O
)	O
nd	O
a	O
better	O
sequence	B
of	O
concatenated	B
codes	O
{	O
better	O
in	O
the	O
sense	O
that	O
it	O
has	O
either	O
higher	O
asymptotic	O
rate	O
r	O
or	O
can	O
tolerate	O
a	O
higher	O
noise	B
level	O
f	O
?	O
figure	O
13.17.	O
the	O
graph	B
of	O
the	O
pentagonful	B
low-density	O
parity-check	B
code	I
with	O
15	O
bit	B
nodes	O
(	O
circles	O
)	O
and	O
10	O
parity-check	B
nodes	I
(	O
triangles	O
)	O
.	O
[	O
this	O
graph	B
is	O
known	O
as	O
the	O
petersen	O
graph	B
.	O
]	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
222	O
13	O
|	O
binary	O
codes	O
exercise	O
13.20	O
.	O
[	O
3	O
,	O
p.226	O
]	O
investigate	O
the	O
possibility	O
of	O
achieving	O
the	O
shannon	O
limit	O
with	O
linear	O
block	B
codes	O
,	O
using	O
the	O
following	O
counting	B
argument	I
.	O
assume	O
a	O
linear	B
code	O
of	O
large	O
blocklength	O
n	O
and	O
rate	O
r	O
=	O
k=n	O
.	O
the	O
code	B
’	O
s	O
parity-check	B
matrix	I
h	O
has	O
m	O
=	O
n	O
(	O
cid:0	O
)	O
k	O
rows	O
.	O
assume	O
that	O
the	O
code	B
’	O
s	O
optimal	B
decoder	I
,	O
which	O
solves	O
the	O
syndrome	B
decoding	I
problem	O
hn	O
=	O
z	O
,	O
allows	O
reliable	O
communication	B
over	O
a	O
binary	B
symmetric	I
channel	I
with	O
(	O
cid:13	O
)	O
ip	O
probability	B
f	O
.	O
how	O
many	O
‘	O
typical	B
’	O
noise	B
vectors	O
n	O
are	O
there	O
?	O
roughly	O
how	O
many	O
distinct	O
syndromes	O
z	O
are	O
there	O
?	O
since	O
n	O
is	O
reliably	O
deduced	O
from	O
z	O
by	O
the	O
optimal	B
decoder	I
,	O
the	O
number	O
of	O
syndromes	O
must	O
be	O
greater	O
than	O
or	O
equal	O
to	O
the	O
number	O
of	O
typical	O
noise	B
vectors	O
.	O
what	O
does	O
this	O
tell	O
you	O
about	O
the	O
largest	O
possible	O
value	O
of	O
rate	O
r	O
for	O
a	O
given	O
f	O
?	O
.	O
exercise	O
13.21	O
.	O
[	O
2	O
]	O
linear	B
binary	O
codes	O
use	O
the	O
input	O
symbols	O
0	O
and	O
1	O
with	O
equal	O
probability	B
,	O
implicitly	O
treating	O
the	O
channel	B
as	O
a	O
symmetric	B
chan-	O
nel	O
.	O
investigate	O
how	O
much	O
loss	O
in	O
communication	O
rate	B
is	O
caused	O
by	O
this	O
assumption	O
,	O
if	O
in	O
fact	O
the	O
channel	B
is	O
a	O
highly	O
asymmetric	O
channel	B
.	O
take	O
as	O
an	O
example	O
a	O
z-channel	O
.	O
how	O
much	O
smaller	O
is	O
the	O
maximum	O
possible	O
rate	B
of	O
communication	B
using	O
symmetric	B
inputs	O
than	O
the	O
capacity	B
of	O
the	O
channel	B
?	O
[	O
answer	O
:	O
about	O
6	O
%	O
.	O
]	O
exercise	O
13.22	O
.	O
[	O
2	O
]	O
show	O
that	O
codes	O
with	O
‘	O
very	B
bad	I
’	O
distance	B
are	O
‘	O
bad	B
’	O
codes	O
,	O
as	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
section	O
11.4	O
(	O
p.183	O
)	O
.	O
exercise	O
13.23	O
.	O
[	O
3	O
]	O
one	O
linear	B
code	O
can	O
be	O
obtained	O
from	O
another	O
by	O
punctur-	O
ing	O
.	O
puncturing	B
means	O
taking	O
each	O
codeword	B
and	O
deleting	O
a	O
de	O
(	O
cid:12	O
)	O
ned	O
set	B
of	O
bits	O
.	O
puncturing	B
turns	O
an	O
(	O
n	O
;	O
k	O
)	O
code	B
into	O
an	O
(	O
n	O
0	O
;	O
k	O
)	O
code	B
,	O
where	O
n0	O
<	O
n	O
.	O
another	O
way	O
to	O
make	O
new	O
linear	B
codes	I
from	O
old	O
is	O
shortening	B
.	O
shortening	B
means	O
constraining	O
a	O
de	O
(	O
cid:12	O
)	O
ned	O
set	B
of	O
bits	O
to	O
be	O
zero	O
,	O
and	O
then	O
deleting	O
them	O
from	O
the	O
codewords	O
.	O
typically	O
if	O
we	O
shorten	O
by	O
one	O
bit	B
,	O
half	O
of	O
the	O
code	O
’	O
s	O
codewords	O
are	O
lost	O
.	O
shortening	B
typically	O
turns	O
an	O
(	O
n	O
;	O
k	O
)	O
code	B
into	O
an	O
(	O
n0	O
;	O
k0	O
)	O
code	B
,	O
where	O
n	O
(	O
cid:0	O
)	O
n0	O
=	O
k	O
(	O
cid:0	O
)	O
k0	O
.	O
another	O
way	O
to	O
make	O
a	O
new	O
linear	B
code	O
from	O
two	O
old	O
ones	O
is	O
to	O
make	O
the	O
intersection	B
of	O
the	O
two	O
codes	O
:	O
a	O
codeword	B
is	O
only	O
retained	O
in	O
the	O
new	O
code	B
if	O
it	O
is	O
present	O
in	O
both	O
of	O
the	O
two	O
old	O
codes	O
.	O
discuss	O
the	O
e	O
(	O
cid:11	O
)	O
ect	O
on	O
a	O
code	B
’	O
s	O
distance-properties	O
of	O
puncturing	O
,	O
short-	O
ening	O
,	O
and	O
intersection	O
.	O
is	O
it	O
possible	O
to	O
turn	O
a	O
code	B
family	O
with	O
bad	O
distance	B
into	O
a	O
code	B
family	O
with	O
good	O
distance	B
,	O
or	O
vice	O
versa	O
,	O
by	O
each	O
of	O
these	O
three	O
manipulations	O
?	O
exercise	O
13.24	O
.	O
[	O
3	O
,	O
p.226	O
]	O
todd	O
ebert	O
’	O
s	O
‘	O
hat	B
puzzle	I
’	O
.	O
three	O
players	O
enter	O
a	O
room	O
and	O
a	O
red	O
or	O
blue	O
hat	B
is	O
placed	O
on	O
each	O
person	O
’	O
s	O
head	O
.	O
the	O
colour	O
of	O
each	O
hat	B
is	O
determined	O
by	O
a	O
coin	B
toss	O
,	O
with	O
the	O
outcome	O
of	O
one	O
coin	B
toss	O
having	O
no	O
e	O
(	O
cid:11	O
)	O
ect	O
on	O
the	O
others	O
.	O
each	O
person	O
can	O
see	O
the	O
other	O
players	O
’	O
hats	O
but	O
not	O
his	O
own	O
.	O
no	O
communication	B
of	O
any	O
sort	O
is	O
allowed	O
,	O
except	O
for	O
an	O
initial	O
strategy	O
session	O
before	O
the	O
group	O
enters	O
the	O
room	O
.	O
once	O
they	O
have	O
had	O
a	O
chance	O
to	O
look	O
at	O
the	O
other	O
hats	O
,	O
the	O
players	O
must	O
simultaneously	O
guess	O
their	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
13.14	O
:	O
solutions	O
223	O
if	O
you	O
already	O
know	O
the	O
hat	B
puzzle	I
,	O
you	O
could	O
try	O
the	O
‘	O
scottish	O
version	O
’	O
of	O
the	O
rules	O
in	O
which	O
the	O
prize	B
is	O
only	O
awarded	O
to	O
the	O
group	O
if	O
they	O
all	O
guess	O
correctly	O
.	O
in	O
the	O
‘	O
reformed	O
scottish	O
version	O
’	O
,	O
all	O
the	O
players	O
must	O
guess	O
correctly	O
,	O
and	O
there	O
are	O
two	O
rounds	O
of	O
guessing	O
.	O
those	O
players	O
who	O
guess	O
during	O
round	O
one	O
leave	O
the	O
room	O
.	O
the	O
remaining	O
players	O
must	O
guess	O
in	O
round	O
two	O
.	O
what	O
strategy	O
should	O
the	O
team	O
adopt	O
to	O
maximize	O
their	O
chance	O
of	O
winning	O
?	O
own	O
hat	B
’	O
s	O
colour	O
or	O
pass	O
.	O
the	O
group	O
shares	O
a	O
$	O
3	O
million	O
prize	B
if	O
at	O
least	O
one	O
player	O
guesses	O
correctly	O
and	O
no	O
players	O
guess	O
incorrectly	O
.	O
the	O
same	O
game	B
can	O
be	O
played	O
with	O
any	O
number	O
of	O
players	O
.	O
the	O
general	O
problem	O
is	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
strategy	O
for	O
the	O
group	O
that	O
maximizes	O
its	O
chances	O
of	O
winning	O
the	O
prize	B
.	O
find	O
the	O
best	O
strategies	O
for	O
groups	O
of	O
size	O
three	O
and	O
seven	O
.	O
[	O
hint	O
:	O
when	O
you	O
’	O
ve	O
done	O
three	O
and	O
seven	O
,	O
you	O
might	O
be	O
able	O
to	O
solve	O
(	O
cid:12	O
)	O
fteen	O
.	O
]	O
exercise	O
13.25	O
.	O
[	O
5	O
]	O
estimate	O
how	O
many	O
binary	O
low-density	O
parity-check	O
codes	O
have	O
self-orthogonal	B
duals	O
.	O
[	O
note	O
that	O
we	O
don	O
’	O
t	O
expect	O
a	O
huge	O
number	O
,	O
since	O
almost	O
all	O
low-density	B
parity-check	I
codes	O
are	O
‘	O
good	B
’	O
,	O
but	O
a	O
low-	O
density	B
parity-check	O
code	B
that	O
contains	O
its	O
dual	B
must	O
be	O
‘	O
bad	B
’	O
.	O
]	O
exercise	O
13.26	O
.	O
[	O
2c	O
]	O
in	O
(	O
cid:12	O
)	O
gure	O
13.15	O
we	O
plotted	O
the	O
error	B
probability	I
associated	O
with	O
a	O
single	O
codeword	O
of	O
weight	O
d	O
as	O
a	O
function	B
of	O
the	O
noise	B
level	O
f	O
of	O
a	O
binary	B
symmetric	I
channel	I
.	O
make	O
an	O
equivalent	O
plot	O
for	O
the	O
case	O
of	O
the	O
gaussian	O
channel	B
,	O
showing	O
the	O
error	B
probability	I
associated	O
with	O
a	O
single	O
codeword	O
of	O
weight	O
d	O
as	O
a	O
function	B
of	O
the	O
rate-compensated	O
signal-to-	O
noise	B
ratio	O
eb=n0	O
.	O
because	O
eb=n0	O
depends	O
on	O
the	O
rate	B
,	O
you	O
have	O
to	O
choose	O
a	O
code	B
rate	O
.	O
choose	O
r	O
=	O
1=2	O
,	O
2=3	O
,	O
3=4	O
,	O
or	O
5=6	O
.	O
13.14	O
solutions	O
solution	O
to	O
exercise	O
13.4	O
(	O
p.210	O
)	O
.	O
the	O
probability	B
of	I
block	I
error	I
to	O
leading	O
order	O
is	O
pb	O
=	O
3	O
solution	O
to	O
exercise	O
13.7	O
(	O
p.217	O
)	O
.	O
a	O
binary	O
vector	O
is	O
perpendicular	O
to	O
itself	O
if	O
it	O
has	O
even	O
weight	B
,	O
i.e.	O
,	O
an	O
even	O
number	O
of	O
1s	O
.	O
n	O
(	O
cid:0	O
)	O
n	O
2	O
(	O
cid:1	O
)	O
f	O
2.	O
solution	O
to	O
exercise	O
13.11	O
(	O
p.219	O
)	O
.	O
the	O
self-dual	B
code	O
has	O
two	O
equivalent	O
parity-check	O
matrices	O
,	O
h1	O
=	O
g	O
=	O
[	O
ikjpt	O
]	O
and	O
h2	O
=	O
[	O
pjik	O
]	O
;	O
these	O
must	O
be	O
equivalent	O
to	O
each	O
other	O
through	O
row	O
additions	O
,	O
that	O
is	O
,	O
there	O
is	O
a	O
matrix	B
u	O
such	O
that	O
uh2	O
=	O
h1	O
,	O
so	O
[	O
upjuik	O
]	O
=	O
[	O
ikjpt	O
]	O
:	O
(	O
13.42	O
)	O
from	O
the	O
right-hand	O
sides	O
of	O
this	O
equation	O
,	O
we	O
have	O
u	O
=	O
pt	O
,	O
so	O
the	O
left-hand	O
sides	O
become	O
:	O
(	O
13.43	O
)	O
thus	O
if	O
a	O
code	B
with	O
generator	B
matrix	I
g	O
=	O
[	O
ikjpt	O
]	O
is	O
self-dual	B
then	O
p	O
is	O
an	O
orthogonal	O
matrix	B
,	O
modulo	O
2	O
,	O
and	O
vice	O
versa	O
.	O
ptp	O
=	O
ik	O
:	O
solution	O
to	O
exercise	O
13.12	O
(	O
p.219	O
)	O
.	O
the	O
(	O
8	O
;	O
4	O
)	O
and	O
(	O
7	O
;	O
4	O
)	O
codes	O
are	O
intimately	O
related	O
.	O
the	O
(	O
8	O
;	O
4	O
)	O
code	B
,	O
whose	O
parity-check	B
matrix	I
is	O
h	O
=	O
(	O
cid:2	O
)	O
p	O
i4	O
(	O
cid:3	O
)	O
=2	O
664	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
0	O
0	O
1	O
0	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
1	O
;	O
(	O
13.44	O
)	O
3	O
775	O
is	O
obtained	O
by	O
(	O
a	O
)	O
appending	O
an	O
extra	O
parity-check	O
bit	B
which	O
can	O
be	O
thought	O
of	O
as	O
the	O
parity	B
of	O
all	O
seven	O
bits	O
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
;	O
and	O
(	O
b	O
)	O
reordering	O
the	O
(	O
cid:12	O
)	O
rst	O
four	O
bits	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
224	O
13	O
|	O
binary	O
codes	O
if	O
an	O
(	O
n	O
;	O
k	O
)	O
code	B
,	O
with	O
m	O
=	O
n	O
(	O
cid:0	O
)	O
k	O
parity	B
solution	O
to	O
exercise	O
13.13	O
(	O
p.220	O
)	O
.	O
symbols	O
,	O
has	O
the	O
property	O
that	O
the	O
decoder	B
can	O
recover	O
the	O
codeword	B
when	O
any	O
m	O
symbols	O
are	O
erased	O
in	O
a	O
block	B
of	O
n	O
,	O
then	O
the	O
code	B
is	O
said	O
to	O
be	O
maximum	B
distance	I
separable	I
(	O
mds	O
)	O
.	O
no	O
mds	O
binary	O
codes	O
exist	O
,	O
apart	O
from	O
the	O
repetition	B
codes	O
and	O
simple	O
parity	B
codes	O
.	O
for	O
q	O
>	O
2	O
,	O
some	O
mds	O
codes	O
can	O
be	O
found	O
.	O
as	O
a	O
simple	O
example	O
,	O
here	O
is	O
a	O
(	O
9	O
;	O
2	O
)	O
code	B
for	O
the	O
8-ary	O
erasure	B
channel	I
.	O
the	O
code	B
is	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
terms	O
of	O
the	O
multiplication	O
and	O
addition	O
rules	B
of	O
gf	O
(	O
8	O
)	O
,	O
which	O
are	O
given	O
in	O
appendix	O
c.1	O
.	O
the	O
elements	O
of	O
the	O
input	O
alphabet	O
are	O
f0	O
;	O
1	O
;	O
a	O
;	O
b	O
;	O
c	O
;	O
d	O
;	O
e	O
;	O
fg	O
and	O
the	O
generator	B
matrix	I
of	O
the	O
code	B
is	O
1	O
(	O
cid:21	O
)	O
:	O
g	O
=	O
(	O
cid:20	O
)	O
1	O
0	O
1	O
a	O
b	O
c	O
d	O
e	O
f	O
0	O
1	O
1	O
(	O
13.45	O
)	O
1	O
1	O
1	O
1	O
1	O
the	O
resulting	O
64	O
codewords	O
are	O
:	O
000000000	O
101abcdef	O
a0aceb1fd	O
b0bedfc1a	O
c0cbfead1	O
d0d1cafbe	O
e0ef1dbac	O
f0fda1ecb	O
011111111	O
110badcfe	O
a1bdfa0ec	O
b1afced0b	O
c1daefbc0	O
d1c0dbeaf	O
e1fe0cabd	O
f1ecb0fda	O
0aaaaaaaa	O
1ab01efcd	O
aa0ec1bdf	O
ba1cfdeb0	O
cae1dc0fb	O
dafbe0d1c	O
eacdbf10e	O
fadf0bce1	O
0bbbbbbbb	O
1ba10fedc	O
ab1fd0ace	O
bb0decfa1	O
cbf0cd1ea	O
dbeaf1c0d	O
ebdcae01f	O
fbce1adf0	O
0cccccccc	O
1cdef01ab	O
ace0afdb1	O
bcfa1b0de	O
cc0fbae1d	O
dc1d0ebfa	O
ecabd1fe0	O
fcb1eda0f	O
0dddddddd	O
1dcfe10ba	O
adf1beca0	O
bdeb0a1cf	O
cd1eabf0c	O
dd0c1faeb	O
edbac0ef1	O
fda0fcb1e	O
0eeeeeeee	O
1efcdab01	O
aeca0df1b	O
bed0b1afc	O
cead10cbf	O
debfac1d0	O
ee01fbdca	O
fe1bcf0ad	O
0ffffffff	O
1fedcba10	O
afdb1ce0a	O
bfc1a0bed	O
cfbc01dae	O
dfaebd0c1	O
ef10eacdb	O
ff0ade1bc	O
solution	O
to	O
exercise	O
13.14	O
(	O
p.220	O
)	O
.	O
quick	O
,	O
rough	O
proof	O
of	O
the	O
theorem	O
.	O
let	O
x	O
denote	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
the	O
reconstructed	O
codeword	B
and	O
the	O
transmitted	O
codeword	B
.	O
for	O
any	O
given	O
channel	B
output	O
r	O
,	O
there	O
is	O
a	O
posterior	O
distribution	O
over	O
x.	O
this	O
posterior	O
distribution	O
is	O
positive	O
only	O
on	O
vectors	O
x	O
belonging	O
to	O
the	O
code	B
;	O
the	O
sums	O
that	O
follow	O
are	O
over	O
codewords	O
x.	O
the	O
block	B
error	O
probability	B
is	O
:	O
pb	O
=xx6=0	O
p	O
(	O
xj	O
r	O
)	O
:	O
(	O
13.46	O
)	O
the	O
average	B
bit	O
error	B
probability	I
,	O
averaging	O
over	O
all	O
bits	O
in	O
the	O
codeword	B
,	O
is	O
:	O
pb	O
=xx6=0	O
p	O
(	O
xj	O
r	O
)	O
w	O
(	O
x	O
)	O
n	O
;	O
(	O
13.47	O
)	O
where	O
w	O
(	O
x	O
)	O
is	O
the	O
weight	B
of	O
codeword	B
x.	O
now	O
the	O
weights	O
of	O
the	O
non-zero	O
codewords	O
satisfy	O
w	O
(	O
x	O
)	O
n	O
(	O
cid:21	O
)	O
dmin	O
n	O
:	O
1	O
(	O
cid:21	O
)	O
(	O
13.48	O
)	O
substituting	O
the	O
inequalities	O
(	O
13.48	O
)	O
into	O
the	O
de	O
(	O
cid:12	O
)	O
nitions	O
(	O
13.46	O
,	O
13.47	O
)	O
,	O
we	O
ob-	O
tain	O
:	O
pb	O
(	O
cid:21	O
)	O
pb	O
(	O
cid:21	O
)	O
dmin	O
n	O
pb	O
;	O
(	O
13.49	O
)	O
which	O
is	O
a	O
factor	O
of	O
two	O
stronger	O
,	O
on	O
the	O
right	O
,	O
than	O
the	O
stated	O
result	O
(	O
13.39	O
)	O
.	O
in	O
making	O
the	O
proof	O
watertight	O
,	O
i	O
have	O
weakened	O
the	O
result	O
a	O
little	O
.	O
careful	O
proof	O
.	O
the	O
theorem	B
relates	O
the	O
performance	O
of	O
the	O
optimal	O
block	B
de-	O
coding	O
algorithm	O
and	O
the	O
optimal	B
bitwise	O
decoding	B
algorithm	O
.	O
we	O
introduce	O
another	O
pair	O
of	O
decoding	O
algorithms	B
,	O
called	O
the	O
block-	O
guessing	B
decoder	I
and	O
the	O
bit-guessing	O
decoder	B
.	O
the	O
idea	O
is	O
that	O
these	O
two	O
algorithms	B
are	O
similar	O
to	O
the	O
optimal	B
block	O
decoder	B
and	O
the	O
optimal	B
bitwise	O
decoder	B
,	O
but	O
lend	O
themselves	O
more	O
easily	O
to	O
analysis	B
.	O
we	O
now	O
de	O
(	O
cid:12	O
)	O
ne	O
these	O
decoders	O
.	O
let	O
x	O
denote	O
the	O
inferred	O
codeword	B
.	O
for	O
any	O
given	O
code	B
:	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
13.14	O
:	O
solutions	O
225	O
the	O
optimal	B
block	O
decoder	B
returns	O
the	O
codeword	B
x	O
that	O
maximizes	O
the	O
posterior	B
probability	I
p	O
(	O
xj	O
r	O
)	O
,	O
which	O
is	O
proportional	O
to	O
the	O
likelihood	B
p	O
(	O
rj	O
x	O
)	O
.	O
the	O
probability	B
of	I
error	I
of	O
this	O
decoder	B
is	O
called	O
pb	O
.	O
the	O
optimal	B
bit	O
decoder	B
returns	O
the	O
value	O
of	O
a	O
that	O
maximizes	O
the	O
posterior	B
probability	I
p	O
(	O
xn	O
=	O
aj	O
r	O
)	O
=	O
the	O
n	O
bits	O
,	O
xn	O
,	O
for	O
each	O
of	O
px	O
p	O
(	O
xj	O
r	O
)	O
	O
[	O
xn	O
=	O
a	O
]	O
.	O
the	O
probability	B
of	I
error	I
of	O
this	O
decoder	B
is	O
called	O
pb	O
.	O
the	O
block-guessing	O
decoder	B
returns	O
a	O
random	B
codeword	O
x	O
with	O
probabil-	O
ity	O
distribution	B
given	O
by	O
the	O
posterior	B
probability	I
p	O
(	O
xj	O
r	O
)	O
.	O
the	O
probability	B
of	I
error	I
of	O
this	O
decoder	B
is	O
called	O
pg	O
b.	O
the	O
bit-guessing	O
decoder	B
returns	O
for	O
each	O
of	O
the	O
n	O
bits	O
,	O
xn	O
,	O
a	O
random	B
bit	O
from	O
the	O
probability	B
distribution	O
p	O
(	O
xn	O
=	O
aj	O
r	O
)	O
.	O
the	O
probability	B
of	I
error	I
of	O
this	O
decoder	B
is	O
called	O
pg	O
b	O
.	O
the	O
theorem	B
states	O
that	O
the	O
optimal	B
bit	O
error	B
probability	I
pb	O
is	O
bounded	O
above	O
by	O
pb	O
and	O
below	O
by	O
a	O
given	O
multiple	O
of	O
pb	O
(	O
13.39	O
)	O
.	O
the	O
left-hand	O
inequality	B
in	O
(	O
13.39	O
)	O
is	O
trivially	O
true	O
{	O
if	O
a	O
block	B
is	O
correct	O
,	O
all	O
its	O
constituent	O
bits	O
are	O
correct	O
;	O
so	O
if	O
the	O
optimal	B
block	O
decoder	B
outperformed	O
the	O
optimal	B
bit	O
decoder	B
,	O
we	O
could	O
make	O
a	O
better	O
bit	B
decoder	O
from	O
the	O
block	B
decoder	O
.	O
we	O
prove	O
the	O
right-hand	O
inequality	B
by	O
establishing	O
that	O
:	O
(	O
a	O
)	O
the	O
bit-guessing	O
decoder	B
is	O
nearly	O
as	O
good	O
as	O
the	O
optimal	B
bit	O
decoder	B
:	O
pg	O
b	O
(	O
cid:20	O
)	O
2pb	O
:	O
(	O
13.50	O
)	O
(	O
b	O
)	O
the	O
bit-guessing	O
decoder	B
’	O
s	O
error	B
probability	I
is	O
related	O
to	O
the	O
block-	O
guessing	B
decoder	I
’	O
s	O
by	O
then	O
since	O
pg	O
b	O
(	O
cid:21	O
)	O
pb	O
,	O
we	O
have	O
1	O
2	O
pb	O
>	O
pg	O
b	O
(	O
cid:21	O
)	O
pg	O
b	O
(	O
cid:21	O
)	O
dmin	O
n	O
pg	O
b	O
:	O
1	O
2	O
dmin	O
n	O
pg	O
b	O
(	O
cid:21	O
)	O
1	O
2	O
dmin	O
n	O
pb	O
:	O
(	O
13.51	O
)	O
(	O
13.52	O
)	O
we	O
now	O
prove	O
the	O
two	O
lemmas	O
.	O
near-optimality	O
of	O
guessing	O
:	O
consider	O
(	O
cid:12	O
)	O
rst	O
the	O
case	O
of	O
a	O
single	O
bit	O
,	O
with	O
posterior	O
probability	B
fp0	O
;	O
p1g	O
.	O
the	O
optimal	B
bit	O
decoder	B
has	O
probability	B
of	I
error	I
p	O
optimal	B
=	O
min	O
(	O
p0	O
;	O
p1	O
)	O
:	O
(	O
13.53	O
)	O
the	O
guessing	B
decoder	I
picks	O
from	O
0	O
and	O
1.	O
the	O
truth	O
is	O
also	O
distributed	O
with	O
the	O
same	O
probability	B
.	O
the	O
probability	B
that	O
the	O
guesser	O
and	O
the	O
truth	O
match	O
is	O
p2	O
0	O
+	O
p2	O
1	O
;	O
the	O
probability	B
that	O
they	O
mismatch	O
is	O
the	O
guessing	B
error	O
probability	B
,	O
p	O
guess	O
=	O
2p0p1	O
(	O
cid:20	O
)	O
2	O
min	O
(	O
p0	O
;	O
p1	O
)	O
=	O
2p	O
optimal	B
:	O
(	O
13.54	O
)	O
since	O
pg	O
b	O
is	O
the	O
average	B
of	O
many	O
such	O
error	O
probabilities	O
,	O
p	O
guess	O
,	O
and	O
pb	O
is	O
the	O
average	B
of	O
the	O
corresponding	O
optimal	B
error	O
probabilities	O
,	O
p	O
optimal	B
,	O
we	O
obtain	O
the	O
desired	O
relationship	O
(	O
13.50	O
)	O
between	O
pg	O
2	O
b	O
and	O
pb	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
226	O
13	O
|	O
binary	O
codes	O
relationship	O
between	O
bit	B
error	O
probability	B
and	O
block	B
error	O
probability	B
:	O
the	O
bit-	O
guessing	B
and	O
block-guessing	O
decoders	O
can	O
be	O
combined	O
in	O
a	O
single	O
system	O
:	O
we	O
can	O
draw	O
a	O
sample	B
xn	O
from	O
the	O
marginal	B
distribution	O
p	O
(	O
xn	O
j	O
r	O
)	O
by	O
drawing	O
a	O
sample	B
(	O
xn	O
;	O
x	O
)	O
from	O
the	O
joint	B
distribution	O
p	O
(	O
xn	O
;	O
xj	O
r	O
)	O
,	O
then	O
discarding	O
the	O
value	O
of	O
x.	O
we	O
can	O
distinguish	O
between	O
two	O
cases	O
:	O
the	O
discarded	O
value	O
of	O
x	O
is	O
the	O
correct	O
codeword	B
,	O
or	O
not	O
.	O
the	O
probability	O
of	O
bit	O
error	O
for	O
the	O
bit-guessing	O
decoder	B
can	O
then	O
be	O
written	O
as	O
a	O
sum	O
of	O
two	O
terms	O
:	O
pg	O
b	O
=	O
p	O
(	O
x	O
correct	O
)	O
p	O
(	O
bit	B
errorj	O
x	O
correct	O
)	O
+	O
p	O
(	O
x	O
incorrect	O
)	O
p	O
(	O
bit	B
errorj	O
x	O
incorrect	O
)	O
=	O
0	O
+	O
pg	O
bp	O
(	O
bit	B
errorj	O
x	O
incorrect	O
)	O
:	O
now	O
,	O
whenever	O
the	O
guessed	O
x	O
is	O
incorrect	O
,	O
the	O
true	O
x	O
must	O
di	O
(	O
cid:11	O
)	O
er	O
from	O
it	O
in	O
at	O
least	O
d	O
bits	O
,	O
so	O
the	O
probability	O
of	O
bit	O
error	O
in	O
these	O
cases	O
is	O
at	O
least	O
d=n	O
.	O
so	O
pg	O
b	O
(	O
cid:21	O
)	O
d	O
n	O
pg	O
b	O
:	O
qed	O
.	O
2	O
solution	O
to	O
exercise	O
13.20	O
(	O
p.222	O
)	O
.	O
the	O
number	O
of	O
‘	O
typical	B
’	O
noise	B
vectors	O
n	O
is	O
roughly	O
2n	O
h2	O
(	O
f	O
)	O
.	O
the	O
number	O
of	O
distinct	O
syndromes	O
z	O
is	O
2m	O
.	O
so	O
reliable	O
communication	B
implies	O
or	O
,	O
in	O
terms	O
of	O
the	O
rate	O
r	O
=	O
1	O
(	O
cid:0	O
)	O
m=n	O
,	O
m	O
(	O
cid:21	O
)	O
n	O
h2	O
(	O
f	O
)	O
;	O
r	O
(	O
cid:20	O
)	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
f	O
)	O
;	O
(	O
13.55	O
)	O
(	O
13.56	O
)	O
a	O
bound	B
which	O
agrees	O
precisely	O
with	O
the	O
capacity	B
of	O
the	O
channel	B
.	O
this	O
argument	O
is	O
turned	O
into	O
a	O
proof	O
in	O
the	O
following	O
chapter	O
.	O
solution	O
to	O
exercise	O
13.24	O
(	O
p.222	O
)	O
.	O
the	O
group	O
to	O
win	O
three-quarters	O
of	O
the	O
time	O
.	O
in	O
the	O
three-player	O
case	O
,	O
it	O
is	O
possible	O
for	O
three-quarters	O
of	O
the	O
time	O
,	O
two	O
of	O
the	O
players	O
will	O
have	O
hats	O
of	O
the	O
same	O
colour	O
and	O
the	O
third	O
player	O
’	O
s	O
hat	B
will	O
be	O
the	O
opposite	O
colour	O
.	O
the	O
group	O
can	O
win	O
every	O
time	O
this	O
happens	O
by	O
using	O
the	O
following	O
strategy	O
.	O
each	O
player	O
looks	O
at	O
the	O
other	O
two	O
players	O
’	O
hats	O
.	O
if	O
the	O
two	O
hats	O
are	O
di	O
(	O
cid:11	O
)	O
erent	O
colours	O
,	O
he	O
passes	O
.	O
if	O
they	O
are	O
the	O
same	O
colour	O
,	O
the	O
player	O
guesses	O
his	O
own	O
hat	B
is	O
the	O
opposite	O
colour	O
.	O
this	O
way	O
,	O
every	O
time	O
the	O
hat	B
colours	O
are	O
distributed	O
two	O
and	O
one	O
,	O
one	O
player	O
will	O
guess	O
correctly	O
and	O
the	O
others	O
will	O
pass	O
,	O
and	O
the	O
group	O
will	O
win	O
the	O
game	B
.	O
when	O
all	O
the	O
hats	O
are	O
the	O
same	O
colour	O
,	O
however	O
,	O
all	O
three	O
players	O
will	O
guess	O
incorrectly	O
and	O
the	O
group	O
will	O
lose	O
.	O
when	O
any	O
particular	O
player	O
guesses	O
a	O
colour	O
,	O
it	O
is	O
true	O
that	O
there	O
is	O
only	O
a	O
50:50	O
chance	O
that	O
their	O
guess	O
is	O
right	O
.	O
the	O
reason	O
that	O
the	O
group	O
wins	O
75	O
%	O
of	O
the	O
time	O
is	O
that	O
their	O
strategy	O
ensures	O
that	O
when	O
players	O
are	O
guessing	B
wrong	O
,	O
a	O
great	O
many	O
are	O
guessing	B
wrong	O
.	O
for	O
larger	O
numbers	O
of	O
players	O
,	O
the	O
aim	O
is	O
to	O
ensure	O
that	O
most	O
of	O
the	O
time	O
no	O
one	O
is	O
wrong	O
and	O
occasionally	O
everyone	O
is	O
wrong	O
at	O
once	O
.	O
in	O
the	O
game	B
with	O
7	O
players	O
,	O
there	O
is	O
a	O
strategy	O
for	O
which	O
the	O
group	O
wins	O
7	O
out	O
of	O
every	O
8	O
times	O
they	O
play	O
.	O
in	O
the	O
game	B
with	O
15	O
players	O
,	O
the	O
group	O
can	O
win	O
15	O
out	O
of	O
16	O
times	O
.	O
if	O
you	O
have	O
not	O
(	O
cid:12	O
)	O
gured	O
out	O
these	O
winning	O
strategies	O
for	O
teams	O
of	O
7	O
and	O
15	O
,	O
i	O
recommend	O
thinking	O
about	O
the	O
solution	O
to	O
the	O
three-player	O
game	B
in	O
terms	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
13.14	O
:	O
solutions	O
227	O
of	O
the	O
locations	O
of	O
the	O
winning	O
and	O
losing	O
states	O
on	O
the	O
three-dimensional	O
hypercube	O
,	O
then	O
thinking	O
laterally	O
.	O
if	O
the	O
number	O
of	O
players	O
,	O
n	O
,	O
is	O
2r	O
(	O
cid:0	O
)	O
1	O
,	O
the	O
optimal	B
strategy	O
can	O
be	O
de	O
(	O
cid:12	O
)	O
ned	O
using	O
a	O
hamming	O
code	B
of	O
length	B
n	O
,	O
and	O
the	O
probability	O
of	O
winning	O
the	O
prize	B
is	O
n=	O
(	O
n	O
+	O
1	O
)	O
.	O
each	O
player	O
is	O
identi	O
(	O
cid:12	O
)	O
ed	O
with	O
a	O
number	O
n	O
2	O
1	O
:	O
:	O
:	O
n	O
.	O
the	O
two	O
colours	O
are	O
mapped	O
onto	O
0	O
and	O
1.	O
any	O
state	O
of	O
their	O
hats	O
can	O
be	O
viewed	O
as	O
a	O
received	O
vector	O
out	O
of	O
a	O
binary	O
channel	O
.	O
a	O
random	B
binary	O
vector	O
of	O
length	B
n	O
is	O
either	O
a	O
codeword	B
of	O
the	O
hamming	O
code	B
,	O
with	O
probability	O
1=	O
(	O
n	O
+	O
1	O
)	O
,	O
or	O
it	O
di	O
(	O
cid:11	O
)	O
ers	O
in	O
exactly	O
one	O
bit	B
from	O
a	O
codeword	B
.	O
each	O
player	O
looks	O
at	O
all	O
the	O
other	O
bits	O
and	O
considers	O
whether	O
his	O
bit	B
can	O
be	O
set	B
to	O
a	O
colour	O
such	O
that	O
the	O
state	O
is	O
a	O
codeword	B
(	O
which	O
can	O
be	O
deduced	O
using	O
the	O
decoder	B
of	O
the	O
hamming	O
code	B
)	O
.	O
if	O
it	O
can	O
,	O
then	O
the	O
player	O
guesses	O
that	O
his	O
hat	B
is	O
the	O
other	O
colour	O
.	O
if	O
the	O
state	O
is	O
actually	O
a	O
codeword	B
,	O
all	O
players	O
will	O
guess	O
and	O
will	O
guess	O
wrong	O
.	O
if	O
the	O
state	O
is	O
a	O
non-codeword	O
,	O
only	O
one	O
player	O
will	O
guess	O
,	O
and	O
his	O
guess	O
will	O
be	O
correct	O
.	O
it	O
’	O
s	O
quite	O
easy	O
to	O
train	O
seven	O
players	O
to	O
follow	O
the	O
optimal	B
strategy	O
if	O
the	O
cyclic	B
representation	O
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
is	O
used	O
(	O
p.19	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
chapter	O
14	O
in	O
this	O
chapter	O
we	O
will	O
draw	O
together	O
several	O
ideas	O
that	O
we	O
’	O
ve	O
encountered	O
so	O
far	O
in	O
one	O
nice	O
short	O
proof	O
.	O
we	O
will	O
simultaneously	O
prove	O
both	O
shannon	O
’	O
s	O
noisy-channel	B
coding	I
theorem	I
(	O
for	O
symmetric	O
binary	O
channels	O
)	O
and	O
his	O
source	B
coding	I
theorem	I
(	O
for	O
binary	O
sources	O
)	O
.	O
while	O
this	O
proof	O
has	O
connections	O
to	O
many	O
preceding	O
chapters	O
in	O
the	O
book	O
,	O
it	O
’	O
s	O
not	O
essential	O
to	O
have	O
read	O
them	O
all	O
.	O
on	O
the	O
noisy-channel	B
coding	I
side	O
,	O
our	O
proof	O
will	O
be	O
more	O
constructive	O
than	O
the	O
proof	O
given	O
in	O
chapter	O
10	O
;	O
there	O
,	O
we	O
proved	O
that	O
almost	O
any	O
random	B
code	I
is	O
‘	O
very	B
good	I
’	O
.	O
here	O
we	O
will	O
show	O
that	O
almost	O
any	O
linear	B
code	O
is	O
very	B
good	I
.	O
we	O
will	O
make	O
use	O
of	O
the	O
idea	O
of	O
typical	O
sets	O
(	O
chapters	O
4	O
and	O
10	O
)	O
,	O
and	O
we	O
’	O
ll	O
borrow	O
from	O
the	O
previous	O
chapter	O
’	O
s	O
calculation	O
of	O
the	O
weight	O
enumerator	O
function	B
of	O
random	B
linear	I
codes	O
(	O
section	B
13.5	O
)	O
.	O
on	O
the	O
source	O
coding	O
side	O
,	O
our	O
proof	O
will	O
show	O
that	O
random	B
linear	I
hash	O
functions	B
can	O
be	O
used	O
for	B
compression	I
of	O
compressible	O
binary	O
sources	O
,	O
thus	O
giving	O
a	O
link	O
to	O
chapter	O
12	O
.	O
228	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
14	O
very	B
good	I
linear	O
codes	O
exist	O
in	O
this	O
chapter	O
we	O
’	O
ll	O
use	O
a	O
single	O
calculation	O
to	O
prove	O
simultaneously	O
the	O
source	B
coding	I
theorem	I
and	O
the	O
noisy-channel	B
coding	I
theorem	I
for	O
the	O
binary	O
symmet-	O
ric	O
channel	B
.	O
incidentally	O
,	O
this	O
proof	O
works	O
for	O
much	O
more	O
general	O
channel	O
models	O
,	O
not	O
only	O
the	O
binary	B
symmetric	I
channel	I
.	O
for	O
example	O
,	O
the	O
proof	O
can	O
be	O
reworked	O
for	O
channels	O
with	O
non-binary	O
outputs	O
,	O
for	O
time-varying	O
channels	O
and	O
for	O
chan-	O
nels	O
with	B
memory	I
,	O
as	O
long	O
as	O
they	O
have	O
binary	O
inputs	O
satisfying	O
a	O
symmetry	O
property	O
,	O
cf	O
.	O
section	B
10.6	O
.	O
14.1	O
a	O
simultaneous	O
proof	O
of	O
the	O
source	O
coding	O
and	O
noisy-channel	B
coding	I
theorems	O
we	O
consider	O
a	O
linear	B
error-correcting	O
code	B
with	O
binary	O
parity-check	O
matrix	B
h.	O
the	O
matrix	B
has	O
m	O
rows	O
and	O
n	O
columns	O
.	O
later	O
in	O
the	O
proof	O
we	O
will	O
increase	O
n	O
and	O
m	O
,	O
keeping	O
m	O
/	O
n	O
.	O
the	O
rate	B
of	O
the	O
code	B
satis	O
(	O
cid:12	O
)	O
es	O
r	O
(	O
cid:21	O
)	O
1	O
(	O
cid:0	O
)	O
m	O
n	O
:	O
(	O
14.1	O
)	O
if	O
all	O
the	O
rows	O
of	O
h	O
are	O
independent	O
then	O
this	O
is	O
an	O
equality	O
,	O
r	O
=	O
1	O
(	O
cid:0	O
)	O
m=n	O
.	O
in	O
what	O
follows	O
,	O
we	O
’	O
ll	O
assume	O
the	O
equality	O
holds	O
.	O
eager	O
readers	O
may	O
work	O
out	O
the	O
expected	O
rank	O
of	O
a	O
random	B
binary	O
matrix	B
h	O
(	O
it	O
’	O
s	O
very	O
close	O
to	O
m	O
)	O
and	O
pursue	O
the	O
e	O
(	O
cid:11	O
)	O
ect	O
that	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
(	O
m	O
(	O
cid:0	O
)	O
rank	O
)	O
has	O
on	O
the	O
rest	O
of	O
this	O
proof	O
(	O
it	O
’	O
s	O
negligible	O
)	O
.	O
a	O
codeword	B
t	O
is	O
selected	O
,	O
satisfying	O
ht	O
=	O
0	O
mod	O
2	O
;	O
(	O
14.2	O
)	O
and	O
a	O
binary	B
symmetric	I
channel	I
adds	O
noise	B
x	O
,	O
giving	O
the	O
received	O
signal	O
r	O
=	O
t	O
+	O
x	O
mod	O
2	O
:	O
(	O
14.3	O
)	O
the	O
receiver	O
aims	O
to	O
infer	O
both	O
t	O
and	O
x	O
from	O
r	O
using	O
a	O
syndrome-decoding	O
approach	O
.	O
syndrome	B
decoding	I
was	O
(	O
cid:12	O
)	O
rst	O
introduced	O
in	O
section	O
1.2	O
(	O
p.10	O
and	O
11	O
)	O
.	O
the	O
receiver	O
computes	O
the	O
syndrome	B
z	O
=	O
hr	O
mod	O
2	O
=	O
ht	O
+	O
hx	O
mod	O
2	O
=	O
hx	O
mod	O
2	O
:	O
(	O
14.4	O
)	O
the	O
syndrome	B
only	O
depends	O
on	O
the	O
noise	B
x	O
,	O
and	O
the	O
decoding	B
problem	O
is	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
most	O
probable	O
x	O
that	O
satis	O
(	O
cid:12	O
)	O
es	O
hx	O
=	O
z	O
mod	O
2	O
:	O
(	O
14.5	O
)	O
229	O
in	O
this	O
chapter	O
x	O
denotes	O
the	O
noise	B
added	O
by	O
the	O
channel	B
,	O
not	O
the	O
input	O
to	O
the	O
channel	B
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
230	O
14	O
|	O
very	B
good	I
linear	O
codes	O
exist	O
this	O
best	O
estimate	O
for	O
the	O
noise	B
vector	O
,	O
^x	O
,	O
is	O
then	O
subtracted	O
from	O
r	O
to	O
give	O
the	O
best	O
guess	O
for	O
t.	O
our	O
aim	O
is	O
to	O
show	O
that	O
,	O
as	O
long	O
as	O
r	O
<	O
1	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
f	O
)	O
,	O
where	O
f	O
is	O
the	O
(	O
cid:13	O
)	O
ip	O
probability	O
of	O
the	O
binary	B
symmetric	I
channel	I
,	O
the	O
optimal	B
decoder	I
for	O
this	O
syndrome-decoding	O
problem	O
has	O
vanishing	O
probability	B
of	I
error	I
,	O
as	O
n	O
increases	O
,	O
for	O
random	O
h.	O
we	O
prove	O
this	O
result	O
by	O
studying	O
a	O
sub-optimal	O
strategy	O
for	O
solving	O
the	O
decoding	B
problem	O
.	O
neither	O
the	O
optimal	B
decoder	I
nor	O
this	O
typical-set	B
decoder	I
would	O
be	O
easy	O
to	O
implement	O
,	O
but	O
the	O
typical-set	B
decoder	I
is	O
easier	O
to	O
analyze	O
.	O
the	O
typical-set	B
decoder	I
examines	O
the	O
typical	B
set	I
t	O
of	O
noise	O
vectors	B
,	O
the	O
set	B
of	O
noise	B
vectors	O
x0	O
that	O
satisfy	O
log	O
1/p	O
(	O
x0	O
)	O
’	O
n	O
h	O
(	O
x	O
)	O
,	O
checking	O
to	O
see	O
if	O
any	O
of	O
we	O
’	O
ll	O
leave	O
out	O
the	O
(	O
cid:15	O
)	O
s	O
and	O
(	O
cid:12	O
)	O
s	O
that	O
those	O
typical	B
vectors	O
x0	O
satis	O
(	O
cid:12	O
)	O
es	O
the	O
observed	O
syndrome	B
,	O
make	O
a	O
typical-set	O
de	O
(	O
cid:12	O
)	O
nition	O
rigorous	O
.	O
enthusiasts	O
are	O
encouraged	O
to	O
revisit	O
section	B
4.4	O
and	O
put	O
these	O
details	O
into	O
this	O
proof	O
.	O
hx0	O
=	O
z	O
:	O
(	O
14.6	O
)	O
if	O
exactly	O
one	O
typical	B
vector	O
x0	O
does	O
so	O
,	O
the	O
typical	B
set	I
decoder	O
reports	O
that	O
vector	O
as	O
the	O
hypothesized	O
noise	B
vector	O
.	O
if	O
no	O
typical	B
vector	O
matches	O
the	O
observed	O
syndrome	B
,	O
or	O
more	O
than	O
one	O
does	O
,	O
then	O
the	O
typical	B
set	I
decoder	O
reports	O
an	O
error	O
.	O
the	O
probability	B
of	I
error	I
of	O
the	O
typical-set	B
decoder	I
,	O
for	O
a	O
given	O
matrix	B
h	O
,	O
can	O
be	O
written	O
as	O
a	O
sum	O
of	O
two	O
terms	O
,	O
ptsjh	O
=	O
p	O
(	O
i	O
)	O
+	O
p	O
(	O
ii	O
)	O
tsjh	O
;	O
(	O
14.7	O
)	O
where	O
p	O
(	O
i	O
)	O
is	O
the	O
probability	B
that	O
the	O
true	O
noise	B
vector	O
x	O
is	O
itself	O
not	O
typical	B
,	O
and	O
p	O
(	O
ii	O
)	O
tsjh	O
is	O
the	O
probability	B
that	O
the	O
true	O
x	O
is	O
typical	B
and	O
at	O
least	O
one	O
other	O
typical	B
vector	O
clashes	O
with	O
it	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
probability	B
vanishes	O
as	O
n	O
increases	O
,	O
as	O
we	O
proved	O
when	O
we	O
(	O
cid:12	O
)	O
rst	O
studied	O
typical	B
sets	O
(	O
chapter	O
4	O
)	O
.	O
we	O
concentrate	O
on	O
the	O
second	O
probability	B
.	O
to	O
recap	O
,	O
we	O
’	O
re	O
imagining	O
a	O
true	O
noise	B
vector	O
,	O
x	O
;	O
and	O
if	O
any	O
of	O
the	O
typical	O
noise	B
vectors	O
x0	O
,	O
di	O
(	O
cid:11	O
)	O
erent	O
from	O
x	O
,	O
satis	O
(	O
cid:12	O
)	O
es	O
h	O
(	O
x0	O
(	O
cid:0	O
)	O
x	O
)	O
=	O
0	O
,	O
then	O
we	O
have	O
an	O
error	O
.	O
we	O
use	O
the	O
truth	B
function	I
	O
(	O
cid:2	O
)	O
h	O
(	O
x0	O
(	O
cid:0	O
)	O
x	O
)	O
=	O
0	O
(	O
cid:3	O
)	O
;	O
(	O
14.8	O
)	O
whose	O
value	O
is	O
one	O
if	O
the	O
statement	O
h	O
(	O
x0	O
(	O
cid:0	O
)	O
x	O
)	O
=	O
0	O
is	O
true	O
and	O
zero	O
otherwise	O
.	O
we	O
can	O
bound	B
the	O
number	O
of	O
type	O
ii	O
errors	B
made	O
when	O
the	O
noise	B
is	O
x	O
thus	O
:	O
[	O
number	O
of	O
errors	O
given	O
x	O
and	O
h	O
]	O
(	O
cid:20	O
)	O
xx0	O
:	O
x0	O
2	O
t	O
x0	O
6=	O
x	O
	O
(	O
cid:2	O
)	O
h	O
(	O
x0	O
(	O
cid:0	O
)	O
x	O
)	O
=	O
0	O
(	O
cid:3	O
)	O
:	O
(	O
14.9	O
)	O
the	O
number	O
of	O
errors	O
is	O
either	O
zero	O
or	O
one	O
;	O
the	O
sum	O
on	O
the	O
right-hand	O
side	O
may	O
exceed	O
one	O
,	O
in	O
cases	O
where	O
several	O
typical	B
noise	O
vectors	B
have	O
the	O
same	O
syndrome	B
.	O
we	O
can	O
now	O
write	O
down	O
the	O
probability	O
of	O
a	O
type-ii	O
error	O
by	O
averaging	O
over	O
equation	O
(	O
14.9	O
)	O
is	O
a	O
union	B
bound	I
.	O
x	O
:	O
p	O
(	O
ii	O
)	O
tsjh	O
(	O
cid:20	O
)	O
xx2t	O
p	O
(	O
x	O
)	O
xx0	O
:	O
x0	O
2	O
t	O
x0	O
6=	O
x	O
	O
(	O
cid:2	O
)	O
h	O
(	O
x0	O
(	O
cid:0	O
)	O
x	O
)	O
=	O
0	O
(	O
cid:3	O
)	O
:	O
(	O
14.10	O
)	O
now	O
,	O
we	O
will	O
(	O
cid:12	O
)	O
nd	O
the	O
average	B
of	O
this	O
probability	O
of	O
type-ii	O
error	O
over	O
all	O
linear	B
codes	I
by	O
averaging	O
over	O
h.	O
by	O
showing	O
that	O
the	O
average	B
probability	O
of	O
type-ii	O
error	O
vanishes	O
,	O
we	O
will	O
thus	O
show	O
that	O
there	O
exist	O
linear	B
codes	I
with	O
vanishing	O
error	B
probability	I
,	O
indeed	O
,	O
that	O
almost	O
all	O
linear	B
codes	I
are	O
very	B
good	I
.	O
we	O
denote	O
averaging	O
over	O
all	O
binary	O
matrices	O
h	O
by	O
h	O
:	O
:	O
:	O
ih	O
.	O
the	O
average	B
probability	O
of	O
type-ii	O
error	O
is	O
(	O
cid:22	O
)	O
p	O
(	O
ii	O
)	O
ts	O
=	O
xh	O
p	O
(	O
h	O
)	O
p	O
(	O
ii	O
)	O
tsjh	O
=	O
dp	O
(	O
ii	O
)	O
tsjheh	O
(	O
14.11	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
14.2	O
:	O
data	B
compression	I
by	O
linear	B
hash	O
codes	O
231	O
=	O
*xx2t	O
=	O
xx2t	O
p	O
(	O
x	O
)	O
xx0	O
:	O
x0	O
2	O
t	O
p	O
(	O
x	O
)	O
xx0	O
:	O
x0	O
2	O
t	O
	O
(	O
cid:2	O
)	O
h	O
(	O
x0	O
(	O
cid:0	O
)	O
x	O
)	O
=	O
0	O
(	O
cid:3	O
)	O
+	O
x0	O
6=	O
x	O
(	O
cid:10	O
)	O
	O
(	O
cid:2	O
)	O
h	O
(	O
x0	O
(	O
cid:0	O
)	O
x	O
)	O
=	O
0	O
(	O
cid:3	O
)	O
(	O
cid:11	O
)	O
h	O
:	O
x0	O
6=	O
x	O
h	O
(	O
14.12	O
)	O
(	O
14.13	O
)	O
now	O
,	O
the	O
quantity	O
h	O
[	O
h	O
(	O
x0	O
(	O
cid:0	O
)	O
x	O
)	O
=	O
0	O
]	O
ih	O
already	O
cropped	O
up	O
when	O
we	O
were	O
calculating	O
the	O
expected	O
weight	B
enumerator	I
function	O
of	O
random	O
linear	B
codes	I
(	O
section	B
13.5	O
)	O
:	O
for	O
any	O
non-zero	O
binary	O
vector	O
v	O
,	O
the	O
probability	B
that	O
hv	O
=	O
0	O
,	O
averaging	O
over	O
all	O
matrices	B
h	O
,	O
is	O
2	O
(	O
cid:0	O
)	O
m	O
.	O
so	O
(	O
cid:22	O
)	O
p	O
(	O
ii	O
)	O
ts	O
=	O
xx2t	O
p	O
(	O
x	O
)	O
!	O
(	O
jtj	O
(	O
cid:0	O
)	O
1	O
)	O
2	O
(	O
cid:0	O
)	O
m	O
(	O
14.14	O
)	O
(	O
14.15	O
)	O
where	O
jtj	O
denotes	O
the	O
size	O
of	O
the	O
typical	O
set	B
.	O
as	O
you	O
will	O
recall	O
from	O
chapter	O
4	O
,	O
there	O
are	O
roughly	O
2n	O
h	O
(	O
x	O
)	O
noise	B
vectors	O
in	O
the	O
typical	B
set	I
.	O
so	O
(	O
cid:20	O
)	O
jtj	O
2	O
(	O
cid:0	O
)	O
m	O
;	O
(	O
cid:22	O
)	O
p	O
(	O
ii	O
)	O
ts	O
(	O
cid:20	O
)	O
2n	O
h	O
(	O
x	O
)	O
2	O
(	O
cid:0	O
)	O
m	O
:	O
(	O
14.16	O
)	O
this	O
bound	B
on	O
the	O
probability	B
of	I
error	I
either	O
vanishes	O
or	O
grows	O
exponentially	O
as	O
n	O
increases	O
(	O
remembering	O
that	O
we	O
are	O
keeping	O
m	O
proportional	O
to	O
n	O
as	O
n	O
increases	O
)	O
.	O
it	O
vanishes	O
if	O
(	O
14.17	O
)	O
substituting	O
r	O
=	O
1	O
(	O
cid:0	O
)	O
m=n	O
,	O
we	O
have	O
thus	O
established	O
the	O
noisy-channel	B
coding	I
theorem	I
for	O
the	O
binary	B
symmetric	I
channel	I
:	O
very	B
good	I
linear	O
codes	O
exist	O
for	O
any	O
rate	B
r	O
satisfying	O
h	O
(	O
x	O
)	O
<	O
m=n	O
:	O
where	O
h	O
(	O
x	O
)	O
is	O
the	O
entropy	B
of	O
the	O
channel	B
noise	O
,	O
per	O
bit	B
.	O
r	O
<	O
1	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
)	O
;	O
(	O
14.18	O
)	O
2	O
exercise	O
14.1	O
.	O
[	O
3	O
]	O
redo	O
the	O
proof	O
for	O
a	O
more	O
general	O
channel	O
.	O
14.2	O
data	B
compression	I
by	O
linear	B
hash	O
codes	O
the	O
decoding	B
game	O
we	O
have	O
just	O
played	O
can	O
also	O
be	O
viewed	O
as	O
an	O
uncompres-	O
sion	O
game	B
.	O
the	O
world	O
produces	O
a	O
binary	O
noise	O
vector	O
x	O
from	O
a	O
source	O
p	O
(	O
x	O
)	O
.	O
the	O
noise	B
has	O
redundancy	B
(	O
if	O
the	O
(	O
cid:13	O
)	O
ip	O
probability	B
is	O
not	O
0.5	O
)	O
.	O
we	O
compress	B
it	O
with	O
a	O
linear	B
compressor	O
that	O
maps	O
the	O
n	O
-bit	O
input	O
x	O
(	O
the	O
noise	B
)	O
to	O
the	O
m	O
-bit	O
output	O
z	O
(	O
the	O
syndrome	B
)	O
.	O
our	O
uncompression	B
task	O
is	O
to	O
recover	O
the	O
input	O
x	O
from	O
the	O
output	O
z.	O
the	O
rate	B
of	O
the	O
compressor	O
is	O
rcompressor	O
(	O
cid:17	O
)	O
m=n	O
:	O
(	O
14.19	O
)	O
[	O
we	O
don	O
’	O
t	O
care	O
about	O
the	O
possibility	O
of	O
linear	O
redundancies	O
in	O
our	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
the	O
rate	O
,	O
here	O
.	O
]	O
the	O
result	O
that	O
we	O
just	O
found	O
,	O
that	O
the	O
decoding	B
problem	O
can	O
be	O
solved	O
,	O
for	O
almost	O
any	O
h	O
,	O
with	O
vanishing	O
error	B
probability	I
,	O
as	O
long	O
as	O
h	O
(	O
x	O
)	O
<	O
m=n	O
,	O
thus	O
instantly	O
proves	O
a	O
source	B
coding	I
theorem	I
:	O
given	O
a	O
binary	O
source	O
x	O
of	O
entropy	O
h	O
(	O
x	O
)	O
,	O
and	O
a	O
required	O
com-	O
pressed	O
rate	B
r	O
>	O
h	O
(	O
x	O
)	O
,	O
there	O
exists	O
a	O
linear	B
compressor	O
x	O
!	O
z	O
=	O
hx	O
mod	O
2	O
having	O
rate	B
m=n	O
equal	O
to	O
that	O
required	O
rate	B
r	O
,	O
and	O
an	O
associated	O
uncompressor	O
,	O
that	O
is	O
virtually	O
lossless	B
.	O
this	O
theorem	B
is	O
true	O
not	O
only	O
for	O
a	O
source	O
of	O
independent	O
identically	O
dis-	O
tributed	O
symbols	O
but	O
also	O
for	O
any	O
source	O
for	O
which	O
a	O
typical	B
set	I
can	O
be	O
de-	O
(	O
cid:12	O
)	O
ned	O
:	O
sources	O
with	B
memory	I
,	O
and	O
time-varying	O
sources	O
,	O
for	O
example	O
;	O
all	O
that	O
’	O
s	O
required	O
is	O
that	O
the	O
source	O
be	O
ergodic	B
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
232	O
notes	O
14	O
|	O
very	B
good	I
linear	O
codes	O
exist	O
this	O
method	B
for	O
proving	O
that	O
codes	O
are	O
good	B
can	O
be	O
applied	O
to	O
other	O
linear	B
codes	I
,	O
such	O
as	O
low-density	O
parity-check	O
codes	O
(	O
mackay	O
,	O
1999b	O
;	O
aji	O
et	O
al.	O
,	O
2000	O
)	O
.	O
for	O
each	O
code	B
we	O
need	O
an	O
approximation	B
of	O
its	O
expected	O
weight	B
enumerator	I
function	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
15	O
further	O
exercises	O
on	O
information	O
theory	B
the	O
most	O
exciting	O
exercises	O
,	O
which	O
will	O
introduce	O
you	O
to	O
further	O
ideas	O
in	O
in-	O
formation	O
theory	B
,	O
are	O
towards	O
the	O
end	O
of	O
this	O
chapter	O
.	O
refresher	O
exercises	O
on	O
source	O
coding	O
and	O
noisy	B
channels	O
.	O
exercise	O
15.1	O
.	O
[	O
2	O
]	O
let	O
x	O
be	O
an	O
ensemble	B
with	O
ax	O
=	O
f0	O
;	O
1g	O
and	O
px	O
=	O
f0:995	O
;	O
0:005g	O
.	O
consider	O
source	O
coding	O
using	O
the	O
block	B
coding	O
of	O
x	O
100	O
where	O
every	O
x	O
2	O
x	O
100	O
containing	O
3	O
or	O
fewer	O
1s	O
is	O
assigned	O
a	O
distinct	O
codeword	B
,	O
while	O
the	O
other	O
xs	O
are	O
ignored	O
.	O
(	O
a	O
)	O
if	O
the	O
assigned	O
codewords	O
are	O
all	O
of	O
the	O
same	O
length	B
,	O
(	O
cid:12	O
)	O
nd	O
the	O
min-	O
imum	O
length	B
required	O
to	O
provide	O
the	O
above	O
set	B
with	O
distinct	O
code-	O
words	O
.	O
(	O
b	O
)	O
calculate	O
the	O
probability	O
of	O
getting	O
an	O
x	O
that	O
will	O
be	O
ignored	O
.	O
.	O
exercise	O
15.2	O
.	O
[	O
2	O
]	O
let	O
x	O
be	O
an	O
ensemble	B
with	O
px	O
=	O
f0:1	O
;	O
0:2	O
;	O
0:3	O
;	O
0:4g	O
.	O
the	O
en-	O
semble	O
is	O
encoded	O
using	O
the	O
symbol	B
code	I
c	O
=	O
f0001	O
;	O
001	O
;	O
01	O
;	O
1g	O
.	O
consider	O
the	O
codeword	B
corresponding	O
to	O
x	O
2	O
x	O
n	O
,	O
where	O
n	O
is	O
large	O
.	O
(	O
a	O
)	O
compute	O
the	O
entropy	B
of	O
the	O
fourth	O
bit	B
of	O
transmission	O
.	O
(	O
b	O
)	O
compute	O
the	O
conditional	B
entropy	I
of	O
the	O
fourth	O
bit	B
given	O
the	O
third	O
bit	O
.	O
(	O
c	O
)	O
estimate	O
the	O
entropy	B
of	O
the	O
hundredth	O
bit	B
.	O
(	O
d	O
)	O
estimate	O
the	O
conditional	B
entropy	I
of	O
the	O
hundredth	O
bit	B
given	O
the	O
ninety-ninth	O
bit	B
.	O
exercise	O
15.3	O
.	O
[	O
2	O
]	O
two	O
fair	O
dice	O
are	O
rolled	O
by	O
alice	O
and	O
the	O
sum	O
is	O
recorded	O
.	O
bob	O
’	O
s	O
task	O
is	O
to	O
ask	O
a	O
sequence	B
of	O
questions	O
with	O
yes/no	O
answers	O
to	O
(	O
cid:12	O
)	O
nd	O
out	O
this	O
number	O
.	O
devise	O
in	O
detail	O
a	O
strategy	O
that	O
achieves	O
the	O
minimum	O
possible	O
average	B
number	O
of	O
questions	O
.	O
.	O
exercise	O
15.4	O
.	O
[	O
2	O
]	O
how	O
can	O
you	O
use	O
a	O
coin	B
to	O
draw	B
straws	I
among	O
3	O
people	O
?	O
.	O
exercise	O
15.5	O
.	O
[	O
2	O
]	O
in	O
a	O
magic	B
trick	I
,	O
there	O
are	O
three	O
participants	O
:	O
the	O
magician	B
,	O
an	O
assistant	O
,	O
and	O
a	O
volunteer	O
.	O
the	O
assistant	O
,	O
who	O
claims	O
to	O
have	O
paranor-	O
mal	O
abilities	O
,	O
is	O
in	O
a	O
soundproof	O
room	O
.	O
the	O
magician	B
gives	O
the	O
volunteer	O
six	B
blank	O
cards	O
,	O
(	O
cid:12	O
)	O
ve	O
white	B
and	O
one	O
blue	O
.	O
the	O
volunteer	O
writes	O
a	O
dif-	O
ferent	O
integer	O
from	O
1	O
to	O
100	O
on	O
each	O
card	B
,	O
as	O
the	O
magician	B
is	O
watching	O
.	O
the	O
volunteer	O
keeps	O
the	O
blue	O
card	B
.	O
the	O
magician	B
arranges	O
the	O
(	O
cid:12	O
)	O
ve	O
white	B
cards	O
in	O
some	O
order	O
and	O
passes	O
them	O
to	O
the	O
assistant	O
.	O
the	O
assistant	O
then	O
announces	O
the	O
number	O
on	O
the	O
blue	O
card	B
.	O
how	O
does	O
the	O
trick	O
work	O
?	O
233	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
234	O
15	O
|	O
further	O
exercises	O
on	O
information	O
theory	B
.	O
exercise	O
15.6	O
.	O
[	O
3	O
]	O
how	O
does	O
this	O
trick	O
work	O
?	O
‘	O
here	O
’	O
s	O
an	O
ordinary	O
pack	O
of	O
cards	O
,	O
shu	O
(	O
cid:15	O
)	O
ed	O
into	O
random	B
order	O
.	O
please	O
choose	O
(	O
cid:12	O
)	O
ve	O
cards	O
from	O
the	O
pack	O
,	O
any	O
that	O
you	O
wish	O
.	O
don	O
’	O
t	O
let	O
me	O
see	O
their	O
faces	O
.	O
no	O
,	O
don	O
’	O
t	O
give	O
them	O
to	O
me	O
:	O
pass	O
them	O
to	O
my	O
assistant	O
esmerelda	O
.	O
she	O
can	O
look	O
at	O
them	O
.	O
‘	O
now	O
,	O
esmerelda	O
,	O
show	O
me	O
four	O
of	O
the	O
cards	O
.	O
hmm	O
:	O
:	O
:	O
nine	O
of	O
spades	O
,	O
six	B
of	O
clubs	O
,	O
four	O
of	O
hearts	O
,	O
ten	O
of	O
diamonds	O
.	O
the	O
hidden	O
card	O
,	O
then	O
,	O
must	O
be	O
the	O
queen	O
of	O
spades	O
!	O
’	O
the	O
trick	O
can	O
be	O
performed	O
as	O
described	O
above	O
for	O
a	O
pack	O
of	O
52	O
cards	O
.	O
use	O
information	B
theory	I
to	O
give	O
an	O
upper	O
bound	B
on	O
the	O
number	O
of	O
cards	O
for	O
which	O
the	O
trick	O
can	O
be	O
performed	O
.	O
.	O
exercise	O
15.7	O
.	O
[	O
2	O
]	O
find	O
a	O
probability	B
sequence	O
p	O
=	O
(	O
p1	O
;	O
p2	O
;	O
:	O
:	O
:	O
)	O
such	O
that	O
h	O
(	O
p	O
)	O
=	O
1.	O
.	O
exercise	O
15.8	O
.	O
[	O
2	O
]	O
consider	O
a	O
discrete	B
memoryless	I
source	O
with	O
ax	O
=	O
fa	O
;	O
b	O
;	O
c	O
;	O
dg	O
and	O
px	O
=	O
f1=2	O
;	O
1=4	O
;	O
1=8	O
;	O
1=8g	O
.	O
there	O
are	O
48	O
=	O
65	O
536	O
eight-letter	O
words	O
that	O
can	O
be	O
formed	O
from	O
the	O
four	O
letters	O
.	O
find	O
the	O
total	O
number	O
of	O
such	O
words	O
that	O
are	O
in	O
the	O
typical	B
set	I
tn	O
(	O
cid:12	O
)	O
(	O
equation	O
4.29	O
)	O
where	O
n	O
=	O
8	O
and	O
(	O
cid:12	O
)	O
=	O
0:1.	O
.	O
exercise	O
15.9	O
.	O
[	O
2	O
]	O
consider	O
=	O
f1/3	O
;	O
1/3	O
;	O
1/9	O
;	O
1/9	O
;	O
1/9g	O
and	O
the	O
channel	B
whose	O
transition	B
probability	I
matrix	O
is	O
the	O
source	O
as	O
q	O
=2	O
664	O
0	O
1	O
0	O
0	O
0	O
2/3	O
0	O
0	O
1	O
1	O
0	O
0	O
1/3	O
0	O
=	O
fa	O
;	O
b	O
;	O
c	O
;	O
d	O
;	O
eg	O
,	O
ps	O
3	O
775	O
:	O
0	O
0	O
(	O
15.1	O
)	O
note	O
that	O
the	O
source	O
alphabet	O
has	O
(	O
cid:12	O
)	O
ve	O
symbols	O
,	O
but	O
the	O
channel	B
alphabet	O
ax	O
=	O
ay	O
=	O
f0	O
;	O
1	O
;	O
2	O
;	O
3g	O
has	O
only	O
four	O
.	O
assume	O
that	O
the	O
source	O
produces	O
symbols	O
at	O
exactly	O
3/4	O
the	O
rate	B
that	O
the	O
channel	B
accepts	O
channel	B
sym-	O
bols	O
.	O
for	O
a	O
given	O
(	O
tiny	O
)	O
(	O
cid:15	O
)	O
>	O
0	O
,	O
explain	O
how	O
you	O
would	O
design	O
a	O
system	O
for	O
communicating	O
the	O
source	O
’	O
s	O
output	O
over	O
the	O
channel	O
with	O
an	O
aver-	O
age	O
error	B
probability	I
per	O
source	O
symbol	O
less	O
than	O
(	O
cid:15	O
)	O
.	O
be	O
as	O
explicit	O
as	O
possible	O
.	O
in	O
particular	O
,	O
do	O
not	O
invoke	O
shannon	O
’	O
s	O
noisy-channel	B
coding	I
theorem	I
.	O
.	O
exercise	O
15.10	O
.	O
[	O
2	O
]	O
consider	O
a	O
binary	B
symmetric	I
channel	I
and	O
a	O
code	B
c	O
=	O
f0000	O
;	O
0011	O
;	O
1100	O
;	O
1111g	O
;	O
assume	O
that	O
the	O
four	O
codewords	O
are	O
used	O
with	O
probabilities	O
f1=2	O
;	O
1=8	O
;	O
1=8	O
;	O
1=4g	O
.	O
what	O
is	O
the	O
decoding	B
rule	O
that	O
minimizes	O
the	O
probability	O
of	O
decoding	O
error	O
?	O
[	O
the	O
optimal	B
decoding	O
rule	O
depends	O
on	O
the	O
noise	B
level	O
f	O
of	O
the	O
binary	O
symmetric	B
channel	I
.	O
give	O
the	O
decoding	B
rule	O
for	O
each	O
range	O
of	O
values	O
of	O
f	O
,	O
for	O
f	O
between	O
0	O
and	O
1=2	O
.	O
]	O
exercise	O
15.11	O
.	O
[	O
2	O
]	O
find	O
the	O
capacity	B
and	O
optimal	B
input	I
distribution	I
for	O
the	O
three-input	O
,	O
three-output	O
channel	B
whose	O
transition	B
probabilities	O
are	O
:	O
q	O
=2	O
4	O
0	O
1	O
0	O
2/3	O
0	O
1/3	O
0	O
1/3	O
2/3	O
3	O
5	O
:	O
(	O
15.2	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
15	O
|	O
further	O
exercises	O
on	O
information	O
theory	B
235	O
exercise	O
15.12	O
.	O
[	O
3	O
,	O
p.239	O
]	O
the	O
input	O
to	O
a	O
channel	B
q	O
is	O
a	O
word	O
of	O
8	O
bits	O
.	O
the	O
output	O
is	O
also	O
a	O
word	O
of	O
8	O
bits	O
.	O
each	O
time	O
it	O
is	O
used	O
,	O
the	O
channel	B
(	O
cid:13	O
)	O
ips	O
exactly	O
one	O
of	O
the	O
transmitted	O
bits	O
,	O
but	O
the	O
receiver	O
does	O
not	O
know	O
which	O
one	O
.	O
the	O
other	O
seven	O
bits	O
are	O
received	O
without	O
error	O
.	O
all	O
8	O
bits	O
are	O
equally	O
likely	O
to	O
be	O
the	O
one	O
that	O
is	O
(	O
cid:13	O
)	O
ipped	O
.	O
derive	O
the	O
capacity	B
of	O
this	O
channel	B
.	O
show	O
,	O
by	O
describing	O
an	O
explicit	O
encoder	B
and	O
decoder	B
that	O
it	O
is	O
possible	O
reliably	O
(	O
that	O
is	O
,	O
with	O
zero	O
error	B
probability	I
)	O
to	O
communicate	O
5	O
bits	O
per	O
cycle	O
over	O
this	O
channel	B
.	O
.	O
exercise	O
15.13	O
.	O
[	O
2	O
]	O
a	O
channel	O
with	O
input	O
x	O
2	O
fa	O
;	O
b	O
;	O
cg	O
and	O
output	O
y	O
2	O
fr	O
;	O
s	O
;	O
t	O
;	O
ug	O
has	O
conditional	B
probability	O
matrix	B
:	O
q	O
=2	O
664	O
what	O
is	O
its	O
capacity	B
?	O
1/2	O
1/2	O
0	O
0	O
0	O
1/2	O
1/2	O
0	O
0	O
0	O
1/2	O
1/2	O
:	O
3	O
775	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhj	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhj	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhj	O
a	O
b	O
c	O
r	O
s	O
t	O
u	O
0-521-64298-1	O
1-010-00000-4	O
table	O
15.1.	O
some	O
valid	O
isbns	O
.	O
[	O
the	O
hyphens	O
are	O
included	O
for	O
legibility	O
.	O
]	O
.	O
exercise	O
15.14	O
.	O
[	O
3	O
]	O
the	O
ten-digit	O
number	O
on	O
the	O
cover	O
of	O
a	O
book	O
known	O
as	O
the	O
isbn	O
incorporates	O
an	O
error-detecting	O
code	B
.	O
the	O
number	O
consists	O
of	O
nine	O
source	O
digits	O
x1	O
;	O
x2	O
;	O
:	O
:	O
:	O
;	O
x9	O
,	O
satisfying	O
xn	O
2	O
f0	O
;	O
1	O
;	O
:	O
:	O
:	O
;	O
9g	O
,	O
and	O
a	O
tenth	O
check	O
digit	O
whose	O
value	O
is	O
given	O
by	O
x10	O
=	O
9	O
xn=1	O
nxn	O
!	O
mod	O
11	O
:	O
here	O
x10	O
2	O
f0	O
;	O
1	O
;	O
:	O
:	O
:	O
;	O
9	O
;	O
10g	O
:	O
if	O
x10	O
=	O
10	O
then	O
the	O
tenth	O
digit	O
is	O
shown	O
using	O
the	O
roman	B
numeral	O
x.	O
show	O
that	O
a	O
valid	O
isbn	O
satis	O
(	O
cid:12	O
)	O
es	O
:	O
10	O
xn=1	O
nxn	O
!	O
mod	O
11	O
=	O
0	O
:	O
imagine	O
that	O
an	O
isbn	O
is	O
communicated	O
over	O
an	O
unreliable	O
human	B
chan-	O
nel	O
which	O
sometimes	O
modi	O
(	O
cid:12	O
)	O
es	O
digits	O
and	O
sometimes	O
reorders	O
digits	O
.	O
show	O
that	O
this	O
code	B
can	O
be	O
used	O
to	O
detect	O
(	O
but	O
not	O
correct	O
)	O
all	O
errors	B
in	O
which	O
any	O
one	O
of	O
the	O
ten	O
digits	O
is	O
modi	O
(	O
cid:12	O
)	O
ed	O
(	O
for	O
example	O
,	O
1-010-00000-4	O
!	O
1-010-00080-4	O
)	O
.	O
show	O
that	O
this	O
code	B
can	O
be	O
used	O
to	O
detect	O
all	O
errors	B
in	O
which	O
any	O
two	O
ad-	O
jacent	O
digits	O
are	O
transposed	O
(	O
for	O
example	O
,	O
1-010-00000-4	O
!	O
1-100-00000-	O
4	O
)	O
.	O
what	O
other	O
transpositions	O
of	O
pairs	O
of	O
non-adjacent	O
digits	O
can	O
be	O
de-	O
tected	O
?	O
if	O
the	O
tenth	O
digit	O
were	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
x10	O
=	O
9	O
xn=1	O
nxn	O
!	O
mod	O
10	O
;	O
why	O
would	O
the	O
code	B
not	O
work	O
so	O
well	O
?	O
(	O
discuss	O
the	O
detection	O
of	O
both	O
modi	O
(	O
cid:12	O
)	O
cations	O
of	O
single	O
digits	O
and	O
transpositions	O
of	O
digits	O
.	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
236	O
15	O
|	O
further	O
exercises	O
on	O
information	O
theory	B
-	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
r	O
-	O
@	O
(	O
cid:0	O
)	O
-	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
@	O
r	O
-	O
(	O
cid:0	O
)	O
a	O
b	O
c	O
d	O
a	O
b	O
c	O
d	O
remember	O
d	O
dp	O
h2	O
(	O
p	O
)	O
=	O
log2	O
1	O
(	O
cid:0	O
)	O
p	O
p	O
.	O
exercise	O
15.15	O
.	O
[	O
3	O
]	O
a	O
channel	O
with	O
input	O
x	O
and	O
output	O
y	O
has	O
transition	B
proba-	O
bility	O
matrix	B
:	O
f	O
1	O
(	O
cid:0	O
)	O
f	O
0	O
0	O
0	O
0	O
1	O
(	O
cid:0	O
)	O
g	O
g	O
0	O
0	O
g	O
1	O
(	O
cid:0	O
)	O
g	O
:	O
3	O
775	O
1	O
(	O
cid:0	O
)	O
f	O
f	O
0	O
0	O
q	O
=2	O
664	O
px	O
=	O
(	O
cid:26	O
)	O
p	O
2	O
assuming	O
an	O
input	O
distribution	O
of	O
the	O
form	O
;	O
p	O
2	O
;	O
1	O
(	O
cid:0	O
)	O
p	O
2	O
;	O
2	O
(	O
cid:27	O
)	O
;	O
1	O
(	O
cid:0	O
)	O
p	O
write	O
down	O
the	O
entropy	B
of	O
the	O
output	O
,	O
h	O
(	O
y	O
)	O
,	O
and	O
the	O
conditional	B
entropy	I
of	O
the	O
output	O
given	O
the	O
input	O
,	O
h	O
(	O
y	O
jx	O
)	O
.	O
show	O
that	O
the	O
optimal	B
input	I
distribution	I
is	O
given	O
by	O
p	O
=	O
1	O
1	O
+	O
2	O
(	O
cid:0	O
)	O
h2	O
(	O
g	O
)	O
+h2	O
(	O
f	O
)	O
;	O
1	O
where	O
h2	O
(	O
f	O
)	O
=	O
f	O
log2	O
write	O
down	O
the	O
optimal	B
input	I
distribution	I
and	O
the	O
capacity	B
of	O
the	O
chan-	O
nel	O
in	O
the	O
case	O
f	O
=	O
1=2	O
,	O
g	O
=	O
0	O
,	O
and	O
comment	O
on	O
your	O
answer	O
.	O
f	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
log2	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
.	O
1	O
.	O
exercise	O
15.16	O
.	O
[	O
2	O
]	O
what	O
are	O
the	O
di	O
(	O
cid:11	O
)	O
erences	O
in	O
the	O
redundancies	O
needed	O
in	O
an	O
error-detecting	O
code	B
(	O
which	O
can	O
reliably	O
detect	O
that	O
a	O
block	B
of	O
data	O
has	O
been	O
corrupted	O
)	O
and	O
an	O
error-correcting	B
code	I
(	O
which	O
can	O
detect	O
and	O
cor-	O
rect	O
errors	B
)	O
?	O
further	O
tales	O
from	O
information	O
theory	B
the	O
following	O
exercises	O
give	O
you	O
the	O
chance	O
to	O
discover	O
for	O
yourself	O
the	O
answers	O
to	O
some	O
more	O
surprising	O
results	O
of	O
information	O
theory	B
.	O
exercise	O
15.17	O
.	O
[	O
3	O
]	O
communication	B
of	O
information	B
from	O
correlated	B
sources	I
.	O
imag-	O
ine	O
that	O
we	O
want	O
to	O
communicate	O
data	O
from	O
two	O
data	O
sources	O
x	O
(	O
a	O
)	O
and	O
x	O
(	O
b	O
)	O
to	O
a	O
central	O
location	O
c	O
via	O
noise-free	O
one-way	B
communication	O
channels	O
(	O
(	O
cid:12	O
)	O
g-	O
ure	O
15.2a	O
)	O
.	O
the	O
signals	O
x	O
(	O
a	O
)	O
and	O
x	O
(	O
b	O
)	O
are	O
strongly	O
dependent	O
,	O
so	O
their	O
joint	B
information	O
content	B
is	O
only	O
a	O
little	O
greater	O
than	O
the	O
marginal	B
information	O
con-	O
tent	O
of	O
either	O
of	O
them	O
.	O
for	O
example	O
,	O
c	O
is	O
a	O
weather	B
collator	I
who	O
wishes	O
to	O
receive	O
a	O
string	O
of	O
reports	O
saying	O
whether	O
it	O
is	O
raining	O
in	O
allerton	O
(	O
x	O
(	O
a	O
)	O
)	O
and	O
whether	O
it	O
is	O
raining	O
in	O
bognor	O
(	O
x	O
(	O
b	O
)	O
)	O
.	O
the	O
joint	B
probability	O
of	O
x	O
(	O
a	O
)	O
and	O
x	O
(	O
b	O
)	O
might	O
be	O
p	O
(	O
x	O
(	O
a	O
)	O
;	O
x	O
(	O
b	O
)	O
)	O
:	O
x	O
(	O
a	O
)	O
0	O
1	O
x	O
(	O
b	O
)	O
0	O
1	O
0.49	O
0.01	O
0.01	O
0.49	O
(	O
15.3	O
)	O
the	O
weather	B
collator	I
would	O
like	O
to	O
know	O
n	O
successive	O
values	O
of	O
x	O
(	O
a	O
)	O
and	O
x	O
(	O
b	O
)	O
exactly	O
,	O
but	O
,	O
since	O
he	O
has	O
to	O
pay	O
for	O
every	O
bit	B
of	O
information	B
he	O
receives	O
,	O
he	O
is	O
interested	O
in	O
the	O
possibility	O
of	O
avoiding	O
buying	O
n	O
bits	O
from	O
source	O
a	O
and	O
n	O
bits	O
from	O
source	O
b.	O
assuming	O
that	O
variables	O
x	O
(	O
a	O
)	O
and	O
x	O
(	O
b	O
)	O
are	O
generated	O
repeatedly	O
from	O
this	O
distribution	B
,	O
can	O
they	O
be	O
encoded	O
at	O
rates	O
ra	O
and	O
rb	O
in	O
such	O
a	O
way	O
that	O
c	O
can	O
reconstruct	O
all	O
the	O
variables	O
,	O
with	O
the	O
sum	O
of	O
information	B
transmission	O
rates	O
on	O
the	O
two	O
lines	O
being	O
less	O
than	O
two	O
bits	O
per	O
cycle	O
?	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
15	O
|	O
further	O
exercises	O
on	O
information	O
theory	B
237	O
x	O
(	O
a	O
)	O
encode	O
-	O
ra	O
t	O
(	O
a	O
)	O
(	O
a	O
)	O
x	O
(	O
b	O
)	O
encode	O
-	O
rb	O
t	O
(	O
b	O
)	O
hhhj	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
c	O
rb	O
h	O
(	O
x	O
(	O
a	O
)	O
;	O
x	O
(	O
b	O
)	O
)	O
h	O
(	O
x	O
(	O
b	O
)	O
)	O
h	O
(	O
x	O
(	O
b	O
)	O
j	O
x	O
(	O
a	O
)	O
)	O
                    	O
	O
                    	O
	O
                    	O
	O
                    	O
	O
                    	O
	O
                    	O
	O
                    	O
	O
                    	O
	O
                    	O
	O
                    	O
	O
                    	O
	O
                    	O
	O
                    	O
	O
                    	O
	O
                    	O
	O
                    	O
	O
                    	O
	O
achievable	O
(	O
b	O
)	O
h	O
(	O
x	O
(	O
a	O
)	O
j	O
x	O
(	O
b	O
)	O
)	O
h	O
(	O
x	O
(	O
a	O
)	O
)	O
ra	O
the	O
answer	O
,	O
which	O
you	O
should	O
demonstrate	O
,	O
is	O
indicated	O
in	O
(	O
cid:12	O
)	O
gure	O
15.2.	O
in	O
the	O
general	O
case	O
of	O
two	O
dependent	B
sources	I
x	O
(	O
a	O
)	O
and	O
x	O
(	O
b	O
)	O
,	O
there	O
exist	O
codes	O
for	O
the	O
two	O
transmitters	O
that	O
can	O
achieve	O
reliable	O
communication	B
of	O
both	O
x	O
(	O
a	O
)	O
and	O
x	O
(	O
b	O
)	O
to	O
c	O
,	O
as	O
long	O
as	O
:	O
the	O
information	B
rate	O
from	O
x	O
(	O
a	O
)	O
,	O
ra	O
,	O
exceeds	O
h	O
(	O
x	O
(	O
a	O
)	O
j	O
x	O
(	O
b	O
)	O
)	O
;	O
the	O
information	B
rate	O
from	O
x	O
(	O
b	O
)	O
,	O
rb	O
,	O
exceeds	O
h	O
(	O
x	O
(	O
b	O
)	O
j	O
x	O
(	O
a	O
)	O
)	O
;	O
and	O
the	O
total	O
information	B
rate	O
ra	O
+	O
rb	O
exceeds	O
the	O
joint	B
entropy	I
h	O
(	O
x	O
(	O
a	O
)	O
;	O
x	O
(	O
b	O
)	O
)	O
(	O
slepian	O
and	O
wolf	O
,	O
1973	O
)	O
.	O
so	O
in	O
the	O
case	O
of	O
x	O
(	O
a	O
)	O
and	O
x	O
(	O
b	O
)	O
above	O
,	O
each	O
transmitter	O
must	O
transmit	O
at	O
a	O
rate	B
greater	O
than	O
h2	O
(	O
0:02	O
)	O
=	O
0:14	O
bits	O
,	O
and	O
the	O
total	O
rate	B
ra	O
+	O
rb	O
must	O
be	O
greater	O
than	O
1.14	O
bits	O
,	O
for	O
example	O
ra	O
=	O
0:6	O
,	O
rb	O
=	O
0:6.	O
there	O
exist	O
codes	O
that	O
can	O
achieve	O
these	O
rates	O
.	O
your	O
task	O
is	O
to	O
(	O
cid:12	O
)	O
gure	O
out	O
why	O
this	O
is	O
so	O
.	O
try	O
to	O
(	O
cid:12	O
)	O
nd	O
an	O
explicit	O
solution	O
in	O
which	O
one	O
of	O
the	O
sources	O
is	O
sent	O
as	O
plain	O
text	O
,	O
t	O
(	O
b	O
)	O
=	O
x	O
(	O
b	O
)	O
,	O
and	O
the	O
other	O
is	O
encoded	O
.	O
exercise	O
15.18	O
.	O
[	O
3	O
]	O
multiple	B
access	I
channels	O
.	O
consider	O
a	O
channel	O
with	O
two	O
sets	O
of	O
inputs	O
and	O
one	O
output	O
{	O
for	O
example	O
,	O
a	O
shared	O
telephone	B
line	O
(	O
(	O
cid:12	O
)	O
gure	O
15.3a	O
)	O
.	O
a	O
simple	O
model	O
system	O
has	O
two	O
binary	O
inputs	O
x	O
(	O
a	O
)	O
and	O
x	O
(	O
b	O
)	O
and	O
a	O
ternary	O
output	O
y	O
equal	O
to	O
the	O
arithmetic	O
sum	O
of	O
the	O
two	O
inputs	O
,	O
that	O
’	O
s	O
0	O
,	O
1	O
or	O
2.	O
there	O
is	O
no	O
noise	B
.	O
users	O
a	O
and	O
b	O
can	O
not	O
communicate	O
with	O
each	O
other	O
,	O
and	O
they	O
can	O
not	O
hear	O
the	O
output	O
of	O
the	O
channel	O
.	O
if	O
the	O
output	O
is	O
a	O
0	O
,	O
the	O
receiver	O
can	O
be	O
certain	O
that	O
both	O
inputs	O
were	O
set	B
to	O
0	O
;	O
and	O
if	O
the	O
output	O
is	O
a	O
2	O
,	O
the	O
receiver	O
can	O
be	O
certain	O
that	O
both	O
inputs	O
were	O
set	B
to	O
1.	O
but	O
if	O
the	O
output	O
is	O
1	O
,	O
then	O
it	O
could	O
be	O
that	O
the	O
input	O
state	O
was	O
(	O
0	O
;	O
1	O
)	O
or	O
(	O
1	O
;	O
0	O
)	O
.	O
how	O
should	O
users	O
a	O
and	O
b	O
use	O
this	O
channel	B
so	O
that	O
their	O
messages	O
can	O
be	O
deduced	O
from	O
the	O
received	O
signals	O
?	O
how	O
fast	O
can	O
a	O
and	O
b	O
communicate	O
?	O
clearly	O
the	O
total	O
information	B
rate	O
from	O
a	O
and	O
b	O
to	O
the	O
receiver	O
can	O
not	O
be	O
two	O
bits	O
.	O
on	O
the	O
other	O
hand	O
it	O
is	O
easy	O
to	O
achieve	O
a	O
total	O
information	B
rate	O
ra+rb	O
of	O
one	O
bit	B
.	O
can	O
reliable	O
communication	B
be	O
achieved	O
at	O
rates	O
(	O
ra	O
;	O
rb	O
)	O
such	O
that	O
ra	O
+	O
rb	O
>	O
1	O
?	O
the	O
answer	O
is	O
indicated	O
in	O
(	O
cid:12	O
)	O
gure	O
15.3.	O
some	O
practical	B
codes	O
for	O
multi-user	O
channels	O
are	O
presented	O
in	O
ratzer	O
and	O
mackay	O
(	O
2003	O
)	O
.	O
exercise	O
15.19	O
.	O
[	O
3	O
]	O
broadcast	B
channels	O
.	O
a	O
broadcast	B
channel	I
consists	O
of	O
a	O
single	O
transmitter	O
and	O
two	O
or	O
more	O
receivers	O
.	O
the	O
properties	O
of	O
the	O
channel	O
are	O
de-	O
(	O
cid:12	O
)	O
ned	O
by	O
a	O
conditional	B
distribution	O
q	O
(	O
y	O
(	O
a	O
)	O
;	O
y	O
(	O
b	O
)	O
j	O
x	O
)	O
.	O
(	O
we	O
’	O
ll	O
assume	O
the	O
channel	B
is	O
memoryless	O
.	O
)	O
the	O
task	O
is	O
to	O
add	O
an	O
encoder	B
and	O
two	O
decoders	O
to	O
enable	O
reliable	O
communication	B
of	O
a	O
common	O
message	O
at	O
rate	B
r0	O
to	O
both	O
receivers	O
,	O
an	O
individual	O
message	O
at	O
rate	B
ra	O
to	O
receiver	O
a	O
,	O
and	O
an	O
individual	O
message	O
at	O
rate	B
rb	O
to	O
receiver	O
b.	O
the	O
capacity	B
region	O
of	O
the	O
broadcast	O
channel	B
is	O
the	O
convex	B
hull	I
of	O
the	O
set	B
of	O
achievable	O
rate	B
triplets	O
(	O
r0	O
;	O
ra	O
;	O
rb	O
)	O
.	O
a	O
simple	O
benchmark	O
for	O
such	O
a	O
channel	B
is	O
given	O
by	O
time-sharing	O
(	O
time-	O
division	O
signaling	O
)	O
.	O
if	O
the	O
capacities	O
of	O
the	O
two	O
channels	O
,	O
considered	O
separately	O
,	O
figure	O
15.2.	O
communication	B
of	O
information	B
from	O
dependent	B
sources	I
.	O
(	O
a	O
)	O
x	O
(	O
a	O
)	O
and	O
x	O
(	O
b	O
)	O
are	O
dependent	B
sources	I
(	O
the	O
dependence	O
is	O
represented	O
by	O
the	O
dotted	O
arrow	O
)	O
.	O
strings	O
of	O
values	O
of	O
each	O
variable	O
are	O
encoded	O
using	O
codes	O
of	O
rate	O
ra	O
and	O
rb	O
into	O
transmissions	O
t	O
(	O
a	O
)	O
and	O
t	O
(	O
b	O
)	O
,	O
which	O
are	O
communicated	O
over	O
noise-free	O
channels	O
to	O
a	O
receiver	O
c.	O
(	O
b	O
)	O
the	O
achievable	O
rate	B
region	O
.	O
both	O
strings	O
can	O
be	O
conveyed	O
without	O
error	O
even	O
though	O
ra	O
<	O
h	O
(	O
x	O
(	O
a	O
)	O
)	O
and	O
rb	O
<	O
h	O
(	O
x	O
(	O
b	O
)	O
)	O
.	O
x	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhj	O
y	O
(	O
a	O
)	O
y	O
(	O
b	O
)	O
figure	O
15.4.	O
the	O
broadcast	B
channel	I
.	O
x	O
is	O
the	O
channel	B
input	O
;	O
y	O
(	O
a	O
)	O
and	O
y	O
(	O
b	O
)	O
are	O
the	O
outputs	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
238	O
15	O
|	O
further	O
exercises	O
on	O
information	O
theory	B
x	O
(	O
a	O
)	O
x	O
(	O
b	O
)	O
-	O
-	O
(	O
a	O
)	O
p	O
(	O
yjx	O
(	O
a	O
)	O
;	O
x	O
(	O
b	O
)	O
)	O
y-	O
rb	O
1	O
y	O
:	O
x	O
(	O
b	O
)	O
0	O
1	O
(	O
b	O
)	O
x	O
(	O
a	O
)	O
0	O
1	O
0	O
1	O
1	O
2	O
1=2	O
achievable	O
(	O
c	O
)	O
1=2	O
1	O
ra	O
are	O
c	O
(	O
a	O
)	O
and	O
c	O
(	O
b	O
)	O
,	O
then	O
by	O
devoting	O
a	O
fraction	O
(	O
cid:30	O
)	O
a	O
of	O
the	O
transmission	O
time	O
to	O
channel	B
a	O
and	O
(	O
cid:30	O
)	O
b	O
=	O
1	O
(	O
cid:0	O
)	O
(	O
cid:30	O
)	O
a	O
to	O
channel	B
b	O
,	O
we	O
can	O
achieve	O
(	O
r0	O
;	O
ra	O
;	O
rb	O
)	O
=	O
(	O
0	O
;	O
(	O
cid:30	O
)	O
ac	O
(	O
a	O
)	O
;	O
(	O
cid:30	O
)	O
bc	O
(	O
b	O
)	O
)	O
.	O
we	O
can	O
do	O
better	O
than	O
this	O
,	O
however	O
.	O
as	O
an	O
analogy	O
,	O
imagine	O
speaking	O
simultaneously	O
to	O
an	O
american	O
and	O
a	O
belarusian	O
;	O
you	O
are	O
(	O
cid:13	O
)	O
uent	O
in	O
american	O
and	O
in	O
belarusian	O
,	O
but	O
neither	O
of	O
your	O
two	O
receivers	O
understands	O
the	O
other	O
’	O
s	O
language	O
.	O
if	O
each	O
receiver	O
can	O
distinguish	O
whether	O
a	O
word	O
is	O
in	O
their	O
own	O
language	O
or	O
not	O
,	O
then	O
an	O
extra	O
binary	O
(	O
cid:12	O
)	O
le	O
can	O
be	O
conveyed	O
to	O
both	O
recipients	O
by	O
using	O
its	O
bits	O
to	O
decide	O
whether	O
the	O
next	O
transmitted	O
word	O
should	O
be	O
from	O
the	O
american	O
source	O
text	O
or	O
from	O
the	O
belarusian	O
source	O
text	O
.	O
each	O
recipient	O
can	O
concatenate	O
the	O
words	O
that	O
they	O
understand	O
in	O
order	O
to	O
receive	O
their	O
personal	O
message	O
,	O
and	O
can	O
also	O
recover	O
the	O
binary	O
string	O
.	O
an	O
example	O
of	O
a	O
broadcast	B
channel	I
consists	O
of	O
two	O
binary	B
symmetric	I
chan-	O
nels	O
with	O
a	O
common	O
input	O
.	O
the	O
two	O
halves	O
of	O
the	O
channel	O
have	O
(	O
cid:13	O
)	O
ip	O
prob-	O
abilities	O
fa	O
and	O
fb	O
.	O
we	O
’	O
ll	O
assume	O
that	O
a	O
has	O
the	O
better	O
half-channel	O
,	O
i.e.	O
,	O
fa	O
<	O
fb	O
<	O
1/2	O
.	O
[	O
a	O
closely	O
related	O
channel	B
is	O
a	O
‘	O
degraded	O
’	O
broadcast	B
channel	I
,	O
in	O
which	O
the	O
conditional	B
probabilities	O
are	O
such	O
that	O
the	O
random	B
variables	O
have	O
the	O
structure	O
of	O
a	O
markov	O
chain	B
,	O
x	O
!	O
y	O
(	O
a	O
)	O
!	O
y	O
(	O
b	O
)	O
;	O
(	O
15.4	O
)	O
i.e.	O
,	O
y	O
(	O
b	O
)	O
is	O
a	O
further	O
degraded	O
version	O
of	O
y	O
(	O
a	O
)	O
.	O
]	O
in	O
this	O
special	O
case	O
,	O
it	O
turns	O
out	O
that	O
whatever	O
information	B
is	O
getting	O
through	O
to	O
receiver	O
b	O
can	O
also	O
be	O
recovered	O
by	O
receiver	O
a.	O
so	O
there	O
is	O
no	O
point	O
distinguishing	O
between	O
r0	O
and	O
rb	O
:	O
the	O
task	O
is	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
capacity	B
region	O
for	O
the	O
rate	B
pair	O
(	O
r0	O
;	O
ra	O
)	O
,	O
where	O
r0	O
is	O
the	O
rate	B
of	O
information	B
reaching	O
both	O
a	O
and	O
b	O
,	O
and	O
ra	O
is	O
the	O
rate	B
of	O
the	O
extra	O
information	O
reaching	O
a.	O
the	O
following	O
exercise	O
is	O
equivalent	O
to	O
this	O
one	O
,	O
and	O
a	O
solution	O
to	O
it	O
is	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gure	O
15.8.	O
exercise	O
15.20	O
.	O
[	O
3	O
]	O
variable-rate	B
error-correcting	I
codes	I
for	O
channels	O
with	O
unknown	O
noise	B
level	O
.	O
in	O
real	O
life	B
,	O
channels	O
may	O
sometimes	O
not	O
be	O
well	O
characterized	O
before	O
the	O
encoder	B
is	O
installed	O
.	O
as	O
a	O
model	B
of	O
this	O
situation	O
,	O
imagine	O
that	O
a	O
channel	B
is	O
known	O
to	O
be	O
a	O
binary	B
symmetric	I
channel	I
with	O
noise	B
level	O
either	O
fa	O
or	O
fb	O
.	O
let	O
fb	O
>	O
fa	O
,	O
and	O
let	O
the	O
two	O
capacities	O
be	O
ca	O
and	O
cb	O
.	O
those	O
who	O
like	O
to	O
live	O
dangerously	O
might	O
install	O
a	O
system	O
designed	O
for	O
noise	O
level	O
fa	O
with	O
rate	O
ra	O
’	O
ca	O
;	O
in	O
the	O
event	O
that	O
the	O
noise	B
level	O
turns	O
out	O
to	O
be	O
fb	O
,	O
our	O
experience	O
of	O
shannon	O
’	O
s	O
theories	O
would	O
lead	O
us	O
to	O
expect	O
that	O
there	O
figure	O
15.3.	O
multiple	B
access	I
channels	O
.	O
(	O
a	O
)	O
a	O
general	O
multiple	O
access	O
channel	O
with	O
two	O
transmitters	O
and	O
one	O
receiver	O
.	O
(	O
b	O
)	O
a	O
binary	O
multiple	O
access	O
channel	O
with	O
output	O
equal	O
to	O
the	O
sum	O
of	O
two	O
inputs	O
.	O
(	O
c	O
)	O
the	O
achievable	O
region	O
.	O
rb	O
c	O
(	O
b	O
)	O
6	O
@	O
@	O
@	O
-	O
@	O
@	O
c	O
(	O
a	O
)	O
ra	O
figure	O
15.5.	O
rates	O
achievable	O
by	O
simple	O
timesharing	O
.	O
r	O
c	O
a	O
bc	O
f	O
a	O
f	O
b	O
f	O
figure	O
15.6.	O
rate	B
of	O
reliable	O
communication	B
r	O
,	O
as	O
a	O
function	B
of	O
noise	B
level	O
f	O
,	O
for	O
shannonesque	O
codes	O
designed	O
to	O
operate	O
at	O
noise	B
levels	O
fa	O
(	O
solid	O
line	O
)	O
and	O
fb	O
(	O
dashed	O
line	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
15	O
|	O
further	O
exercises	O
on	O
information	O
theory	B
239	O
r	O
c	O
a	O
bc	O
f	O
a	O
f	O
b	O
f	O
figure	O
15.7.	O
rate	B
of	O
reliable	O
communication	B
r	O
,	O
as	O
a	O
function	B
of	O
noise	B
level	O
f	O
,	O
for	O
a	O
desired	O
variable-rate	O
code	O
.	O
0.6	O
0.4	O
0.2	O
0	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
figure	O
15.8.	O
an	O
achievable	O
region	O
for	O
the	O
channel	O
with	O
unknown	O
noise	B
level	O
.	O
assuming	O
the	O
two	O
possible	O
noise	B
levels	O
are	O
fa	O
=	O
0:01	O
and	O
fb	O
=	O
0:1	O
,	O
the	O
dashed	O
lines	O
show	O
the	O
rates	O
ra	O
;	O
rb	O
that	O
are	O
achievable	O
using	O
a	O
simple	O
time-sharing	O
approach	O
,	O
and	O
the	O
solid	O
line	O
shows	O
rates	O
achievable	O
using	O
a	O
more	O
cunning	O
approach	O
.	O
would	O
be	O
a	O
catastrophic	O
failure	O
to	O
communicate	O
information	B
reliably	O
(	O
solid	O
line	O
in	O
(	O
cid:12	O
)	O
gure	O
15.6	O
)	O
.	O
a	O
conservative	O
approach	O
would	O
design	O
the	O
encoding	O
system	O
for	O
the	O
worst-	O
case	O
scenario	O
,	O
installing	O
a	O
code	B
with	O
rate	B
rb	O
’	O
cb	O
(	O
dashed	O
line	O
in	O
(	O
cid:12	O
)	O
gure	O
15.6	O
)	O
.	O
in	O
the	O
event	O
that	O
the	O
lower	O
noise	B
level	O
,	O
fa	O
,	O
holds	O
true	O
,	O
the	O
managers	O
would	O
have	O
a	O
feeling	O
of	O
regret	O
because	O
of	O
the	O
wasted	O
capacity	B
di	O
(	O
cid:11	O
)	O
erence	O
ca	O
(	O
cid:0	O
)	O
rb	O
.	O
is	O
it	O
possible	O
to	O
create	O
a	O
system	O
that	O
not	O
only	O
transmits	O
reliably	O
at	O
some	O
rate	B
r0	O
whatever	O
the	O
noise	B
level	O
,	O
but	O
also	O
communicates	O
some	O
extra	O
,	O
‘	O
lower-	O
priority	O
’	O
bits	O
if	O
the	O
noise	B
level	O
is	O
low	O
,	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
15.7	O
?	O
this	O
code	B
communicates	O
the	O
high-priority	O
bits	O
reliably	O
at	O
all	O
noise	B
levels	O
between	O
fa	O
and	O
fb	O
,	O
and	O
communicates	O
the	O
low-priority	O
bits	O
also	O
if	O
the	O
noise	B
level	O
is	O
fa	O
or	O
below	O
.	O
this	O
problem	O
is	O
mathematically	O
equivalent	O
to	O
the	O
previous	O
problem	O
,	O
the	O
degraded	O
broadcast	B
channel	I
.	O
the	O
lower	O
rate	B
of	O
communication	B
was	O
there	O
called	O
r0	O
,	O
and	O
the	O
rate	B
at	O
which	O
the	O
low-priority	O
bits	O
are	O
communicated	O
if	O
the	O
noise	B
level	O
is	O
low	O
was	O
called	O
ra	O
.	O
an	O
illustrative	O
answer	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
15.8	O
,	O
for	O
the	O
case	O
fa	O
=	O
0:01	O
and	O
fb	O
=	O
0:1	O
.	O
(	O
this	O
(	O
cid:12	O
)	O
gure	O
also	O
shows	O
the	O
achievable	O
region	O
for	O
a	O
broadcast	B
channel	I
whose	O
two	O
half-channels	O
have	O
noise	B
levels	O
fa	O
=	O
0:01	O
and	O
fb	O
=	O
0:1	O
.	O
)	O
i	O
admit	O
i	O
(	O
cid:12	O
)	O
nd	O
the	O
gap	O
between	O
the	O
simple	O
time-sharing	O
solution	O
and	O
the	O
cunning	O
solution	O
disappointingly	O
small	O
.	O
in	O
chapter	O
50	O
we	O
will	O
discuss	O
codes	O
for	O
a	O
special	O
class	O
of	O
broadcast	O
channels	O
,	O
namely	O
erasure	B
channels	O
,	O
where	O
every	O
symbol	O
is	O
either	O
received	O
without	O
error	O
or	O
erased	O
.	O
these	O
codes	O
have	O
the	O
nice	O
property	O
that	O
they	O
are	O
rateless	B
{	O
the	O
number	O
of	O
symbols	O
transmitted	O
is	O
determined	O
on	O
the	O
(	O
cid:13	O
)	O
y	O
such	O
that	O
reliable	O
comunication	O
is	O
achieved	O
,	O
whatever	O
the	O
erasure	B
statistics	O
of	O
the	O
channel	O
.	O
exercise	O
15.21	O
.	O
[	O
3	O
]	O
multiterminal	B
information	O
networks	O
are	O
both	O
important	O
practi-	O
cally	O
and	O
intriguing	O
theoretically	O
.	O
consider	O
the	O
following	O
example	O
of	O
a	O
two-way	O
binary	O
channel	O
(	O
(	O
cid:12	O
)	O
gure	O
15.9a	O
,	O
b	O
)	O
:	O
two	O
people	O
both	O
wish	O
to	O
talk	O
over	O
the	O
channel	B
,	O
and	O
they	O
both	O
want	O
to	O
hear	O
what	O
the	O
other	O
person	O
is	O
saying	O
;	O
but	O
you	O
can	O
hear	O
the	O
signal	O
transmitted	O
by	O
the	O
other	O
person	O
only	O
if	O
you	O
are	O
transmitting	O
a	O
zero	O
.	O
what	O
simultaneous	O
information	B
rates	O
from	O
a	O
to	O
b	O
and	O
from	O
b	O
to	O
a	O
can	O
be	O
achieved	O
,	O
and	O
how	O
?	O
everyday	O
examples	O
of	O
such	O
networks	O
include	O
the	O
vhf	O
channels	O
used	O
by	O
ships	O
,	O
and	O
computer	O
ethernet	O
networks	O
(	O
in	O
which	O
all	O
the	O
devices	O
are	O
unable	O
to	O
hear	O
anything	O
if	O
two	O
or	O
more	O
devices	O
are	O
broadcasting	O
simultaneously	O
)	O
.	O
obviously	O
,	O
we	O
can	O
achieve	O
rates	O
of	O
1/2	O
in	O
both	O
directions	O
by	O
simple	O
time-	O
sharing	O
.	O
but	O
can	O
the	O
two	O
information	B
rates	O
be	O
made	O
larger	O
?	O
finding	O
the	O
capacity	B
of	O
a	O
general	O
two-way	O
channel	B
is	O
still	O
an	O
open	O
problem	O
.	O
however	O
,	O
we	O
can	O
obtain	O
interesting	O
results	O
concerning	O
achievable	O
points	O
for	O
the	O
simple	O
binary	O
channel	B
discussed	O
above	O
,	O
as	O
indicated	O
in	O
(	O
cid:12	O
)	O
gure	O
15.9c	O
.	O
there	O
exist	O
codes	O
that	O
can	O
achieve	O
rates	O
up	O
to	O
the	O
boundary	O
shown	O
.	O
there	O
may	O
exist	O
better	O
codes	O
too	O
.	O
solutions	O
solution	O
to	O
exercise	O
15.12	O
(	O
p.235	O
)	O
.	O
c	O
(	O
q	O
)	O
=	O
5	O
bits	O
.	O
hint	O
for	O
the	O
last	O
part	O
:	O
a	O
solution	O
exists	O
that	O
involves	O
a	O
simple	O
(	O
8	O
;	O
5	O
)	O
code	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
240	O
15	O
|	O
further	O
exercises	O
on	O
information	O
theory	B
x	O
(	O
b	O
)	O
x	O
(	O
a	O
)	O
0	O
1	O
0	O
1	O
0	O
0	O
figure	O
15.9	O
.	O
(	O
a	O
)	O
a	O
general	O
two-way	O
channel	B
.	O
(	O
b	O
)	O
the	O
rules	B
for	O
a	O
binary	O
two-way	O
channel	B
.	O
the	O
two	O
tables	O
show	O
the	O
outputs	O
y	O
(	O
a	O
)	O
and	O
y	O
(	O
b	O
)	O
that	O
result	O
for	O
each	O
state	O
of	O
the	O
inputs	O
.	O
(	O
c	O
)	O
achievable	O
region	O
for	O
the	O
two-way	O
binary	O
channel	O
.	O
rates	O
below	O
the	O
solid	O
line	O
are	O
achievable	O
.	O
the	O
dotted	O
line	O
shows	O
the	O
‘	O
obviously	O
achievable	O
’	O
region	O
which	O
can	O
be	O
attained	O
by	O
simple	O
time-sharing	O
.	O
-	O
x	O
(	O
a	O
)	O
y	O
(	O
a	O
)	O
(	O
cid:27	O
)	O
(	O
a	O
)	O
p	O
(	O
y	O
(	O
a	O
)	O
;	O
y	O
(	O
b	O
)	O
jx	O
(	O
a	O
)	O
;	O
x	O
(	O
b	O
)	O
)	O
(	O
cid:27	O
)	O
-	O
y	O
(	O
b	O
)	O
y	O
(	O
a	O
)	O
:	O
x	O
(	O
b	O
)	O
0	O
1	O
(	O
b	O
)	O
x	O
(	O
a	O
)	O
0	O
1	O
0	O
0	O
1	O
0	O
y	O
(	O
b	O
)	O
:	O
x	O
(	O
b	O
)	O
0	O
1	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
)	O
(	O
b	O
r	O
achievable	O
(	O
c	O
)	O
0	O
0	O
0.2	O
0.4	O
r	O
(	O
a	O
)	O
0.6	O
0.8	O
1	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
16	O
message	B
passing	I
one	O
of	O
the	O
themes	O
of	O
this	O
book	O
is	O
the	O
idea	O
of	O
doing	O
complicated	O
calculations	O
using	O
simple	O
distributed	O
hardware	O
.	O
it	O
turns	O
out	O
that	O
quite	O
a	O
few	O
interesting	O
problems	O
can	O
be	O
solved	O
by	O
message-passing	O
algorithms	B
,	O
in	O
which	O
simple	O
mes-	O
sages	O
are	O
passed	O
locally	O
among	O
simple	O
processors	O
whose	O
operations	O
lead	O
,	O
after	O
some	O
time	O
,	O
to	O
the	O
solution	O
of	O
a	O
global	O
problem	O
.	O
16.1	O
counting	B
as	O
an	O
example	O
,	O
consider	O
a	O
line	O
of	O
soldiers	O
walking	O
in	O
the	O
mist	O
.	O
the	O
commander	B
wishes	O
to	O
perform	O
the	O
complex	B
calculation	O
of	O
counting	O
the	O
number	O
of	O
soldiers	O
in	O
the	O
line	O
.	O
this	O
problem	O
could	O
be	O
solved	O
in	O
two	O
ways	O
.	O
first	O
there	O
is	O
a	O
solution	O
that	O
uses	O
expensive	O
hardware	O
:	O
the	O
loud	O
booming	O
voices	O
of	O
the	O
commander	O
and	O
his	O
men	O
.	O
the	O
commander	B
could	O
shout	O
‘	O
all	O
soldiers	O
report	O
back	O
to	O
me	O
within	O
one	O
minute	O
!	O
’	O
,	O
then	O
he	O
could	O
listen	O
carefully	O
as	O
the	O
men	O
respond	O
‘	O
molesworth	O
here	O
sir	O
!	O
’	O
,	O
‘	O
fotherington	O
{	O
thomas	O
here	O
sir	O
!	O
’	O
,	O
and	O
so	O
on	O
.	O
this	O
solution	O
relies	O
on	O
several	O
expensive	O
pieces	O
of	O
hardware	O
:	O
there	O
must	O
be	O
a	O
reliable	O
communication	B
channel	O
to	O
and	O
from	O
every	O
soldier	B
;	O
the	O
commander	B
must	O
be	O
able	O
to	O
listen	O
to	O
all	O
the	O
incoming	O
messages	O
{	O
even	O
when	O
there	O
are	O
hundreds	O
of	O
soldiers	O
{	O
and	O
must	O
be	O
able	O
to	O
count	O
;	O
and	O
all	O
the	O
soldiers	O
must	O
be	O
well-fed	O
if	O
they	O
are	O
to	O
be	O
able	O
to	O
shout	O
back	O
across	O
the	O
possibly-large	O
distance	B
separating	O
them	O
from	O
the	O
commander	B
.	O
the	O
second	O
way	O
of	O
(	O
cid:12	O
)	O
nding	O
this	O
global	O
function	B
,	O
the	O
number	O
of	O
soldiers	O
,	O
does	O
not	O
require	O
global	O
communication	B
hardware	O
,	O
high	O
iq	O
,	O
or	O
good	B
food	O
;	O
we	O
simply	O
require	O
that	O
each	O
soldier	B
can	O
communicate	O
single	O
integers	O
with	O
the	O
two	O
adjacent	O
soldiers	O
in	O
the	O
line	O
,	O
and	O
that	O
the	O
soldiers	O
are	O
capable	O
of	O
adding	O
one	O
to	O
a	O
number	O
.	O
each	O
soldier	B
follows	O
these	O
rules	B
:	O
1.	O
if	O
you	O
are	O
the	O
front	O
soldier	B
in	O
the	O
line	O
,	O
say	O
the	O
number	O
‘	O
one	O
’	O
to	O
the	O
soldier	B
behind	O
you	O
.	O
algorithm	B
16.1.	O
message-passing	B
rule-set	O
a	O
.	O
2.	O
if	O
you	O
are	O
the	O
rearmost	O
soldier	B
in	O
the	O
line	O
,	O
say	O
the	O
number	O
‘	O
one	O
’	O
to	O
the	O
soldier	B
in	O
front	O
of	O
you	O
.	O
3.	O
if	O
a	O
soldier	B
ahead	O
of	O
or	O
behind	O
you	O
says	O
a	O
number	O
to	O
you	O
,	O
add	O
one	O
to	O
it	O
,	O
and	O
say	O
the	O
new	O
number	O
to	O
the	O
soldier	B
on	O
the	O
other	O
side	O
.	O
if	O
the	O
clever	O
commander	B
can	O
not	O
only	O
add	O
one	O
to	O
a	O
number	O
,	O
but	O
also	O
add	O
two	O
numbers	O
together	O
,	O
then	O
he	O
can	O
(	O
cid:12	O
)	O
nd	O
the	O
global	O
number	O
of	O
soldiers	O
by	O
simply	O
adding	O
together	O
:	O
241	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
242	O
16	O
|	O
message	B
passing	I
figure	O
16.2.	O
a	O
line	O
of	O
soldiers	O
counting	B
themselves	O
using	O
message-passing	B
rule-set	O
a.	O
the	O
commander	B
can	O
add	O
‘	O
3	O
’	O
from	O
the	O
soldier	B
in	O
front	O
,	O
‘	O
1	O
’	O
from	O
the	O
soldier	B
behind	O
,	O
and	O
‘	O
1	O
’	O
for	O
himself	O
,	O
and	O
deduce	O
that	O
there	O
are	O
5	O
soldiers	O
in	O
total	O
.	O
figure	O
16.3.	O
a	O
swarm	O
of	O
guerillas	O
.	O
the	O
number	O
said	O
to	O
him	O
by	O
the	O
soldier	B
in	O
front	O
of	O
him	O
,	O
+	O
the	O
number	O
said	O
to	O
the	O
com-	O
mander	O
by	O
the	O
soldier	B
behind	O
him	O
,	O
+	O
one	O
(	O
which	O
equals	O
the	O
total	O
number	O
of	O
soldiers	O
in	O
front	O
)	O
(	O
which	O
is	O
the	O
number	O
behind	O
)	O
(	O
to	O
count	O
the	O
commander	B
himself	O
)	O
.	O
this	O
solution	O
requires	O
only	O
local	O
communication	B
hardware	O
and	O
simple	O
compu-	O
tations	O
(	O
storage	O
and	O
addition	O
of	O
integers	O
)	O
.	O
1	O
4	O
2	O
3	O
3	O
2	O
4	O
1	O
commander	B
separation	O
this	O
clever	O
trick	O
makes	O
use	O
of	O
a	O
profound	O
property	O
of	O
the	O
total	O
number	O
of	O
soldiers	O
:	O
that	O
it	O
can	O
be	O
written	O
as	O
the	O
sum	O
of	O
the	O
number	O
of	O
soldiers	O
in	O
front	O
of	O
a	O
point	O
and	O
the	O
number	O
behind	O
that	O
point	O
,	O
two	O
quantities	O
which	O
can	O
be	O
computed	O
separately	O
,	O
because	O
the	O
two	O
groups	O
are	O
separated	O
by	O
the	O
commander	B
.	O
if	O
the	O
soldiers	O
were	O
not	O
arranged	O
in	O
a	O
line	O
but	O
were	O
travelling	O
in	O
a	O
swarm	O
,	O
then	O
it	O
would	O
not	O
be	O
easy	O
to	O
separate	O
them	O
into	O
two	O
groups	O
in	O
this	O
way	O
.	O
the	O
commander	B
jim	O
guerillas	O
in	O
(	O
cid:12	O
)	O
gure	O
16.3	O
could	O
not	O
be	O
counted	O
using	O
the	O
above	O
message-passing	B
rule-set	O
a	O
,	O
because	O
,	O
while	O
the	O
guerillas	O
do	O
have	O
neighbours	O
(	O
shown	O
by	O
lines	O
)	O
,	O
it	O
is	O
not	O
clear	O
who	O
is	O
‘	O
in	O
front	O
’	O
and	O
who	O
is	O
‘	O
behind	O
’	O
;	O
furthermore	O
,	O
since	O
the	O
graph	B
of	O
connections	O
between	O
the	O
guerillas	O
contains	O
cycles	O
,	O
it	O
is	O
not	O
possible	O
for	O
a	O
guerilla	B
in	O
a	O
cycle	O
(	O
such	O
as	O
‘	O
jim	O
’	O
)	O
to	O
separate	O
the	O
group	O
into	O
two	O
groups	O
,	O
‘	O
those	O
in	O
front	O
’	O
,	O
and	O
‘	O
those	O
behind	O
’	O
.	O
a	O
swarm	O
of	O
guerillas	O
can	O
be	O
counted	O
by	O
a	O
modi	O
(	O
cid:12	O
)	O
ed	O
message-passing	B
algo-	O
rithm	O
if	O
they	O
are	O
arranged	O
in	O
a	O
graph	B
that	O
contains	O
no	O
cycles	O
.	O
rule-set	O
b	O
is	O
a	O
message-passing	B
algorithm	O
for	O
counting	O
a	O
swarm	O
of	O
guerillas	O
whose	O
connections	O
form	O
a	O
cycle-free	O
graph	B
,	O
also	O
known	O
as	O
a	O
tree	B
,	O
as	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gure	O
16.4.	O
any	O
guerilla	B
can	O
deduce	O
the	O
total	O
in	O
the	O
tree	B
from	O
the	O
messages	O
that	O
they	O
receive	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
16.1	O
:	O
counting	B
243	O
figure	O
16.4.	O
a	O
swarm	O
of	O
guerillas	O
whose	O
connections	O
form	O
a	O
tree	B
.	O
commander	B
jim	O
algorithm	B
16.5.	O
message-passing	B
rule-set	O
b	O
.	O
1.	O
count	O
your	O
number	O
of	O
neighbours	O
,	O
n	O
.	O
2.	O
keep	O
count	O
of	O
the	O
number	O
of	O
messages	O
you	O
have	O
received	O
from	O
your	O
neighbours	O
,	O
m	O
,	O
and	O
of	O
the	O
values	O
v1	O
,	O
v2	O
,	O
:	O
:	O
:	O
;	O
vn	O
of	O
each	O
of	O
those	O
messages	O
.	O
let	O
v	O
be	O
the	O
running	O
total	O
of	O
the	O
messages	O
you	O
have	O
received	O
.	O
3.	O
if	O
the	O
number	O
of	O
messages	O
you	O
have	O
received	O
,	O
m	O
,	O
is	O
equal	O
to	O
n	O
(	O
cid:0	O
)	O
1	O
,	O
then	O
identify	O
the	O
neighbour	O
who	O
has	O
not	O
sent	O
you	O
a	O
message	O
and	O
tell	O
them	O
the	O
number	O
v	O
+	O
1	O
.	O
4.	O
if	O
the	O
number	O
of	O
messages	O
you	O
have	O
received	O
is	O
equal	O
to	O
n	O
,	O
then	O
:	O
(	O
a	O
)	O
the	O
number	O
v	O
+	O
1	O
is	O
the	O
required	O
total	O
.	O
(	O
b	O
)	O
for	O
each	O
neighbour	O
n	O
f	O
say	O
to	O
neighbour	O
n	O
the	O
number	O
v	O
+	O
1	O
(	O
cid:0	O
)	O
vn	O
.	O
g	O
a	O
figure	O
16.6.	O
a	O
triangular	O
41	O
(	O
cid:2	O
)	O
41	O
grid	O
.	O
how	O
many	O
paths	O
are	O
there	O
from	O
a	O
to	O
b	O
?	O
one	O
path	O
is	O
shown	O
.	O
b	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
244	O
16.2	O
path-counting	B
a	O
more	O
profound	O
task	O
than	O
counting	B
squaddies	O
is	O
the	O
task	O
of	O
counting	O
the	O
number	O
of	O
paths	O
through	O
a	O
grid	O
,	O
and	O
(	O
cid:12	O
)	O
nding	O
how	O
many	O
paths	O
pass	O
through	O
any	O
given	O
point	O
in	O
the	O
grid	O
.	O
figure	O
16.6	O
shows	O
a	O
rectangular	B
grid	O
,	O
and	O
a	O
path	O
through	O
the	O
grid	O
,	O
con-	O
necting	O
points	O
a	O
and	O
b.	O
a	O
valid	O
path	O
is	O
one	O
that	O
starts	O
from	O
a	O
and	O
proceeds	O
to	O
b	O
by	O
rightward	O
and	O
downward	O
moves	O
.	O
our	O
questions	O
are	O
:	O
1.	O
how	O
many	O
such	O
paths	O
are	O
there	O
from	O
a	O
to	O
b	O
?	O
2.	O
if	O
a	O
random	B
path	O
from	O
a	O
to	O
b	O
is	O
selected	O
,	O
what	O
is	O
the	O
probability	B
that	O
it	O
passes	O
through	O
a	O
particular	O
node	O
in	O
the	O
grid	O
?	O
[	O
when	O
we	O
say	O
‘	O
random	B
’	O
,	O
we	O
mean	B
that	O
all	O
paths	O
have	O
exactly	O
the	O
same	O
probability	O
of	O
being	O
selected	O
.	O
]	O
3.	O
how	O
can	O
a	O
random	B
path	O
from	O
a	O
to	O
b	O
be	O
selected	O
?	O
counting	B
all	O
the	O
paths	O
from	O
a	O
to	O
b	O
doesn	O
’	O
t	O
seem	O
straightforward	O
.	O
the	O
number	O
of	O
paths	O
is	O
expected	O
to	O
be	O
pretty	O
big	O
{	O
even	O
if	O
the	O
permitted	O
grid	O
were	O
a	O
diagonal	O
strip	O
only	O
three	O
nodes	O
wide	O
,	O
there	O
would	O
still	O
be	O
about	O
2n=2	O
possible	O
paths	O
.	O
the	O
computational	O
breakthrough	O
is	O
to	O
realize	O
that	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
number	O
of	O
paths	O
,	O
we	O
do	O
not	O
have	O
to	O
enumerate	O
all	O
the	O
paths	O
explicitly	O
.	O
pick	O
a	O
point	O
p	O
in	O
the	O
grid	O
and	O
consider	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
p.	O
every	O
path	O
from	O
a	O
to	O
p	O
must	O
come	O
in	O
to	O
p	O
through	O
one	O
of	O
its	O
upstream	O
neighbours	O
(	O
‘	O
upstream	O
’	O
meaning	O
above	O
or	O
to	O
the	O
left	O
)	O
.	O
so	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
p	O
can	O
be	O
found	O
by	O
adding	O
up	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
each	O
of	O
those	O
neighbours	O
.	O
this	O
message-passing	B
algorithm	O
is	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gure	O
16.8	O
for	O
a	O
simple	O
grid	O
with	O
ten	O
vertices	O
connected	O
by	O
twelve	O
directed	O
edges	O
.	O
we	O
start	O
by	O
send-	O
ing	O
the	O
‘	O
1	O
’	O
message	O
from	O
a.	O
when	O
any	O
node	O
has	O
received	O
messages	O
from	O
all	O
its	O
upstream	O
neighbours	O
,	O
it	O
sends	O
the	O
sum	O
of	O
them	O
on	O
to	O
its	O
downstream	O
neigh-	O
bours	O
.	O
at	O
b	O
,	O
the	O
number	O
5	O
emerges	O
:	O
we	O
have	O
counted	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
b	O
without	O
enumerating	O
them	O
all	O
.	O
as	O
a	O
sanity-check	O
,	O
(	O
cid:12	O
)	O
gure	O
16.9	O
shows	O
the	O
(	O
cid:12	O
)	O
ve	O
distinct	O
paths	O
from	O
a	O
to	O
b.	O
having	O
counted	O
all	O
paths	O
,	O
we	O
can	O
now	O
move	O
on	O
to	O
more	O
challenging	O
prob-	O
lems	O
:	O
computing	O
the	O
probability	B
that	O
a	O
random	B
path	O
goes	O
through	O
a	O
given	O
vertex	O
,	O
and	O
creating	O
a	O
random	B
path	O
.	O
probability	O
of	O
passing	O
through	O
a	O
node	O
by	O
making	O
a	O
backward	B
pass	I
as	O
well	O
as	O
the	O
forward	B
pass	I
,	O
we	O
can	O
deduce	O
how	O
many	O
of	O
the	O
paths	O
go	O
through	O
each	O
node	O
;	O
and	O
if	O
we	O
divide	O
that	O
by	O
the	O
total	O
number	O
of	O
paths	O
,	O
we	O
obtain	O
the	O
probability	B
that	O
a	O
randomly	O
selected	O
path	O
passes	O
through	O
that	O
node	O
.	O
figure	O
16.10	O
shows	O
the	O
backward-passing	O
messages	O
in	O
the	O
lower-right	O
corners	O
of	O
the	O
tables	O
,	O
and	O
the	O
original	O
forward-passing	O
mes-	O
sages	O
in	O
the	O
upper-left	O
corners	O
.	O
by	O
multiplying	O
these	O
two	O
numbers	O
at	O
a	O
given	O
vertex	O
,	O
we	O
(	O
cid:12	O
)	O
nd	O
the	O
total	O
number	O
of	O
paths	O
passing	O
through	O
that	O
vertex	O
.	O
for	O
example	O
,	O
four	O
paths	O
pass	O
through	O
the	O
central	O
vertex	O
.	O
figure	O
16.11	O
shows	O
the	O
result	O
of	O
this	O
computation	O
for	O
the	O
triangular	O
41	O
(	O
cid:2	O
)	O
41	O
grid	O
.	O
the	O
area	O
of	O
each	O
blob	O
is	O
proportional	O
to	O
the	O
probability	O
of	O
passing	O
through	O
the	O
corresponding	O
node	O
.	O
random	B
path	O
sampling	O
exercise	O
16.1	O
.	O
[	O
1	O
,	O
p.247	O
]	O
if	O
one	O
creates	O
a	O
‘	O
random	B
’	O
path	O
from	O
a	O
to	O
b	O
by	O
(	O
cid:13	O
)	O
ipping	O
a	O
fair	O
coin	B
at	O
every	O
junction	O
where	O
there	O
is	O
a	O
choice	O
of	O
two	O
directions	O
,	O
is	O
16	O
|	O
message	B
passing	I
a	O
n	O
p	O
m	O
b	O
figure	O
16.7.	O
every	O
path	O
from	O
a	O
to	O
p	O
enters	O
p	O
through	O
an	O
upstream	O
neighbour	O
of	O
p	O
,	O
either	O
m	O
or	O
n	O
;	O
so	O
we	O
can	O
(	O
cid:12	O
)	O
nd	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
p	O
by	O
adding	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
m	O
to	O
the	O
number	O
from	O
a	O
to	O
n.	O
1	O
a	O
1	O
1	O
1	O
2	O
2	O
1	O
3	O
5	O
5	O
b	O
figure	O
16.8.	O
messages	O
sent	O
in	O
the	O
forward	B
pass	I
.	O
a	O
b	O
figure	O
16.9.	O
the	O
(	O
cid:12	O
)	O
ve	O
paths	O
.	O
1	O
5	O
a	O
1	O
1	O
5	O
2	O
1	O
2	O
2	O
3	O
2	O
1	O
b	O
1	O
1	O
3	O
5	O
5	O
1	O
1	O
1	O
figure	O
16.10.	O
messages	O
sent	O
in	O
the	O
forward	O
and	O
backward	O
passes	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
16.3	O
:	O
finding	O
the	O
lowest-cost	O
path	O
245	O
the	O
resulting	O
path	O
a	O
uniform	O
random	B
sample	O
from	O
the	O
set	B
of	O
all	O
paths	O
?	O
[	O
hint	O
:	O
imagine	O
trying	O
it	O
for	O
the	O
grid	O
of	O
(	O
cid:12	O
)	O
gure	O
16.8	O
.	O
]	O
there	O
is	O
a	O
neat	O
insight	O
to	O
be	O
had	O
here	O
,	O
and	O
i	O
’	O
d	O
like	O
you	O
to	O
have	O
the	O
satisfaction	O
of	O
(	O
cid:12	O
)	O
guring	O
it	O
out	O
.	O
exercise	O
16.2	O
.	O
[	O
2	O
,	O
p.247	O
]	O
having	O
run	O
the	O
forward	O
and	O
backward	O
algorithms	O
be-	O
tween	O
points	O
a	O
and	O
b	O
on	O
a	O
grid	O
,	O
how	O
can	O
one	O
draw	O
one	O
path	O
from	O
a	O
to	O
b	O
uniformly	O
at	O
random	B
?	O
(	O
figure	O
16.11	O
.	O
)	O
a	O
figure	O
16.11	O
.	O
(	O
a	O
)	O
the	O
probability	O
of	O
passing	O
through	O
each	O
node	O
,	O
and	O
(	O
b	O
)	O
a	O
randomly	O
chosen	O
path	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
b	O
the	O
message-passing	B
algorithm	O
we	O
used	O
to	O
count	O
the	O
paths	O
to	O
b	O
is	O
an	O
example	O
of	O
the	O
sum	O
{	O
product	O
algorithm	O
.	O
the	O
‘	O
sum	O
’	O
takes	O
place	O
at	O
each	O
node	O
when	O
it	O
adds	O
together	O
the	O
messages	O
coming	O
from	O
its	O
predecessors	O
;	O
the	O
‘	O
product	O
’	O
was	O
not	O
mentioned	O
,	O
but	O
you	O
can	O
think	O
of	O
the	O
sum	O
as	O
a	O
weighted	O
sum	O
in	O
which	O
all	O
the	O
summed	O
terms	O
happened	O
to	O
have	O
weight	B
1	O
.	O
16.3	O
finding	O
the	O
lowest-cost	O
path	O
imagine	O
you	O
wish	O
to	O
travel	O
as	O
quickly	O
as	O
possible	O
from	O
ambridge	O
(	O
a	O
)	O
to	O
bognor	O
(	O
b	O
)	O
.	O
the	O
various	O
possible	O
routes	O
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
16.12	O
,	O
along	O
with	O
the	O
cost	O
in	O
hours	O
of	O
traversing	O
each	O
edge	B
in	O
the	O
graph	B
.	O
for	O
example	O
,	O
the	O
route	O
a	O
{	O
i	O
{	O
l	O
{	O
n	O
{	O
b	O
has	O
a	O
cost	O
of	O
8	O
hours	O
.	O
we	O
would	O
like	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
lowest-cost	O
path	O
without	O
explicitly	O
evaluating	O
the	O
cost	O
of	O
all	O
paths	O
.	O
we	O
can	O
do	O
this	O
e	O
(	O
cid:14	O
)	O
ciently	O
by	O
(	O
cid:12	O
)	O
nding	O
for	O
each	O
node	O
what	O
the	O
cost	O
of	O
the	O
lowest-cost	O
path	O
to	O
that	O
node	O
from	O
a	O
is	O
.	O
these	O
quantities	O
can	O
be	O
computed	O
by	O
message-passing	O
,	O
starting	O
from	O
node	O
a.	O
the	O
message-passing	B
algorithm	O
is	O
called	O
the	O
min	O
{	O
sum	O
algorithm	O
or	O
viterbi	O
algorithm	B
.	O
for	O
brevity	O
,	O
we	O
’	O
ll	O
call	O
the	O
cost	O
of	O
the	O
lowest-cost	O
path	O
from	O
node	O
a	O
to	O
node	O
x	O
‘	O
the	O
cost	O
of	O
x	O
’	O
.	O
each	O
node	O
can	O
broadcast	B
its	O
cost	O
to	O
its	O
descendants	O
once	O
it	O
knows	O
the	O
costs	O
of	O
all	O
its	O
possible	O
predecessors	O
.	O
let	O
’	O
s	O
step	O
through	O
the	O
algorithm	B
by	O
hand	O
.	O
the	O
cost	O
of	O
a	O
is	O
zero	O
.	O
we	O
pass	O
this	O
news	O
on	O
to	O
h	O
and	O
i.	O
as	O
the	O
message	O
passes	O
along	O
each	O
edge	B
in	O
the	O
graph	B
,	O
the	O
cost	O
of	O
that	O
edge	B
is	O
added	O
.	O
we	O
(	O
cid:12	O
)	O
nd	O
the	O
costs	O
of	O
h	O
and	O
i	O
are	O
4	O
and	O
1	O
respectively	O
(	O
(	O
cid:12	O
)	O
gure	O
16.13a	O
)	O
.	O
similarly	O
then	O
,	O
the	O
costs	O
of	O
j	O
and	O
l	O
are	O
found	O
to	O
be	O
6	O
and	O
2	O
respectively	O
,	O
but	O
what	O
about	O
k	O
?	O
out	O
of	O
the	O
edge	O
h	O
{	O
k	O
comes	O
the	O
message	O
that	O
a	O
path	O
of	O
cost	O
5	O
exists	O
from	O
a	O
to	O
k	O
via	O
h	O
;	O
and	O
from	O
edge	B
i	O
{	O
k	O
we	O
learn	O
of	O
an	O
alternative	O
path	O
of	O
cost	O
3	O
(	O
(	O
cid:12	O
)	O
gure	O
16.13b	O
)	O
.	O
the	O
min	O
{	O
sum	O
algorithm	O
sets	O
the	O
cost	O
of	O
k	O
equal	O
to	O
the	O
minimum	O
of	O
these	O
(	O
the	O
‘	O
min	O
’	O
)	O
,	O
and	O
records	O
which	O
was	O
the	O
smallest-cost	O
route	O
into	O
k	O
by	O
retaining	O
only	O
the	O
edge	B
i	O
{	O
k	O
and	O
pruning	O
away	O
the	O
other	O
edges	O
leading	O
to	O
k	O
(	O
(	O
cid:12	O
)	O
gure	O
16.13c	O
)	O
.	O
figures	O
16.13d	O
and	O
e	O
show	O
the	O
remaining	O
two	O
iterations	O
of	O
the	O
algorithm	O
which	O
reveal	O
that	O
there	O
is	O
a	O
path	O
from	O
a	O
to	O
b	O
with	O
cost	O
6	O
.	O
[	O
if	O
the	O
min	O
{	O
sum	O
algorithm	O
encounters	O
a	O
tie	O
,	O
where	O
the	O
minimum-cost	O
a	O
4	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
h	O
i	O
2	O
1	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
2	O
1	O
j	O
k	O
l	O
2	O
2	O
hhhj	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
1	O
3	O
m	O
n	O
1	O
hhhj	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
3	O
b	O
figure	O
16.12.	O
route	O
diagram	O
from	O
ambridge	O
to	O
bognor	O
,	O
showing	O
the	O
costs	O
associated	O
with	O
the	O
edges	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
246	O
16	O
|	O
message	B
passing	I
path	O
to	O
a	O
node	O
is	O
achieved	O
by	O
more	O
than	O
one	O
route	O
to	O
it	O
,	O
then	O
the	O
algorithm	B
can	O
pick	O
any	O
of	O
those	O
routes	O
at	O
random	B
.	O
]	O
we	O
can	O
recover	O
this	O
lowest-cost	O
path	O
by	O
backtracking	O
from	O
b	O
,	O
following	O
the	O
trail	O
of	O
surviving	O
edges	O
back	O
to	O
a.	O
we	O
deduce	O
that	O
the	O
lowest-cost	O
path	O
is	O
a	O
{	O
i	O
{	O
k	O
{	O
m	O
{	O
b.	O
other	O
applications	O
of	O
the	O
min	O
{	O
sum	O
algorithm	O
imagine	O
that	O
you	O
manage	O
the	O
production	O
of	O
a	O
product	O
from	O
raw	O
materials	O
via	O
a	O
large	O
set	B
of	O
operations	O
.	O
you	O
wish	O
to	O
identify	O
the	O
critical	B
path	I
in	O
your	O
process	O
,	O
that	O
is	O
,	O
the	O
subset	B
of	O
operations	O
that	O
are	O
holding	O
up	O
production	O
.	O
if	O
any	O
operations	O
on	O
the	O
critical	B
path	I
were	O
carried	O
out	O
a	O
little	O
faster	O
then	O
the	O
time	O
to	O
get	O
from	O
raw	O
materials	O
to	O
product	O
would	O
be	O
reduced	O
.	O
the	O
critical	B
path	I
of	O
a	O
set	B
of	O
operations	O
can	O
be	O
found	O
using	O
the	O
min	O
{	O
sum	O
algorithm	O
.	O
in	O
chapter	O
25	O
the	O
min	O
{	O
sum	O
algorithm	O
will	O
be	O
used	O
in	O
the	O
decoding	B
of	O
error-correcting	B
codes	I
.	O
16.4	O
summary	B
and	O
related	O
ideas	O
some	O
global	O
functions	B
have	O
a	O
separability	O
property	O
.	O
for	O
example	O
,	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
p	O
separates	O
into	O
the	O
sum	O
of	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
m	O
(	O
the	O
point	O
to	O
p	O
’	O
s	O
left	O
)	O
and	O
the	O
number	O
of	O
paths	O
from	O
a	O
to	O
n	O
(	O
the	O
point	O
above	O
p	O
)	O
.	O
such	O
functions	B
can	O
be	O
computed	O
e	O
(	O
cid:14	O
)	O
ciently	O
by	O
message-passing	O
.	O
other	O
functions	B
do	O
not	O
have	O
such	O
separability	O
properties	O
,	O
for	O
example	O
1.	O
the	O
number	O
of	O
pairs	O
of	O
soldiers	O
in	O
a	O
troop	O
who	O
share	O
the	O
same	O
birthday	B
;	O
2.	O
the	O
size	O
of	O
the	O
largest	O
group	O
of	O
soldiers	O
who	O
share	O
a	O
common	O
height	O
(	O
rounded	O
to	O
the	O
nearest	O
centimetre	O
)	O
;	O
3.	O
the	O
length	B
of	O
the	O
shortest	O
tour	O
that	O
a	O
travelling	O
salesman	O
could	O
take	O
that	O
visits	O
every	O
soldier	B
in	O
a	O
troop	O
.	O
one	O
of	O
the	O
challenges	O
of	O
machine	O
learning	B
is	O
to	O
(	O
cid:12	O
)	O
nd	O
low-cost	O
solutions	O
to	O
prob-	O
lems	O
like	O
these	O
.	O
the	O
problem	O
of	O
(	O
cid:12	O
)	O
nding	O
a	O
large	O
subset	B
of	O
variables	O
that	O
are	O
approximately	O
equal	O
can	O
be	O
solved	O
with	O
a	O
neural	B
network	I
approach	O
(	O
hop	O
(	O
cid:12	O
)	O
eld	O
and	O
brody	O
,	O
2000	O
;	O
hop	O
(	O
cid:12	O
)	O
eld	O
and	O
brody	O
,	O
2001	O
)	O
.	O
a	O
neural	O
approach	O
to	O
the	O
trav-	O
elling	O
salesman	O
problem	O
will	O
be	O
discussed	O
in	O
section	O
42.9	O
.	O
(	O
a	O
)	O
0	O
a	O
(	O
b	O
)	O
0	O
a	O
(	O
c	O
)	O
0	O
a	O
(	O
d	O
)	O
0	O
a	O
(	O
e	O
)	O
0	O
a	O
4	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
4	O
h	O
1	O
i	O
2	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
2	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
4	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
4	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
4	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
4	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
4	O
h	O
1	O
i	O
4	O
h	O
1	O
i	O
4	O
h	O
1	O
i	O
4	O
h	O
1	O
i	O
2	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhj	O
1	O
2	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
2	O
1	O
2	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
2	O
1	O
2	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
2	O
1	O
2	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
j	O
k	O
l	O
6	O
j	O
5	O
k	O
3	O
2	O
l	O
6	O
j	O
3	O
k	O
2	O
l	O
6	O
j	O
3	O
k	O
2	O
l	O
6	O
j	O
3	O
k	O
2	O
l	O
m	O
n	O
hhhj	O
1	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
3	O
b	O
m	O
n	O
hhhj	O
1	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
3	O
b	O
m	O
n	O
hhhj	O
1	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
3	O
b	O
hhhj	O
2	O
2	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
3	O
hhhj	O
2	O
2	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
3	O
hhhj	O
2	O
2	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
3	O
2	O
5	O
m	O
2	O
hhhj	O
1	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
1	O
3	O
4	O
n	O
b	O
3	O
2	O
2	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
*	O
hhhj	O
1	O
3	O
5	O
m	O
1	O
hhhj	O
4	O
n	O
3	O
6	O
b	O
16.5	O
further	O
exercises	O
.	O
exercise	O
16.3	O
.	O
[	O
2	O
]	O
describe	O
the	O
asymptotic	O
properties	O
of	O
the	O
probabilities	O
de-	O
picted	O
in	O
(	O
cid:12	O
)	O
gure	O
16.11a	O
,	O
for	O
a	O
grid	O
in	O
a	O
triangle	B
of	O
width	O
and	O
height	O
n	O
.	O
.	O
exercise	O
16.4	O
.	O
[	O
2	O
]	O
in	O
image	O
processing	O
,	O
the	O
integral	B
image	I
i	O
(	O
x	O
;	O
y	O
)	O
obtained	O
from	O
an	O
image	B
f	O
(	O
x	O
;	O
y	O
)	O
(	O
where	O
x	O
and	O
y	O
are	O
pixel	O
coordinates	O
)	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
figure	O
16.13.	O
min	O
{	O
sum	O
message-passing	O
algorithm	B
to	O
(	O
cid:12	O
)	O
nd	O
the	O
cost	O
of	O
getting	O
to	O
each	O
node	O
,	O
and	O
thence	O
the	O
lowest	O
cost	O
route	O
from	O
a	O
to	O
b.	O
i	O
(	O
x	O
;	O
y	O
)	O
(	O
cid:17	O
)	O
x	O
xu=0	O
y	O
xv=0	O
f	O
(	O
u	O
;	O
v	O
)	O
:	O
(	O
16.1	O
)	O
show	O
that	O
the	O
integral	B
image	I
i	O
(	O
x	O
;	O
y	O
)	O
can	O
be	O
e	O
(	O
cid:14	O
)	O
ciently	O
computed	O
by	O
mes-	O
sage	O
passing	O
.	O
show	O
that	O
,	O
from	O
the	O
integral	B
image	I
,	O
some	O
simple	O
functions	O
of	O
the	O
image	O
can	O
be	O
obtained	O
.	O
for	O
example	O
,	O
give	O
an	O
expression	O
for	O
the	O
sum	O
of	O
the	O
image	B
intensities	O
f	O
(	O
x	O
;	O
y	O
)	O
for	O
all	O
(	O
x	O
;	O
y	O
)	O
in	O
a	O
rectangular	B
region	O
extending	O
from	O
(	O
x1	O
;	O
y1	O
)	O
to	O
(	O
x2	O
;	O
y2	O
)	O
.	O
y2	O
y1	O
x1	O
x2	O
(	O
0	O
;	O
0	O
)	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
247	O
16.6	O
:	O
solutions	O
16.6	O
solutions	O
solution	O
to	O
exercise	O
16.1	O
(	O
p.244	O
)	O
.	O
since	O
there	O
are	O
(	O
cid:12	O
)	O
ve	O
paths	O
through	O
the	O
grid	O
of	O
(	O
cid:12	O
)	O
gure	O
16.8	O
,	O
they	O
must	O
all	O
have	O
probability	B
1=5	O
.	O
but	O
a	O
strategy	O
based	O
on	O
fair	O
coin-	O
(	O
cid:13	O
)	O
ips	O
will	O
produce	O
paths	O
whose	O
probabilities	O
are	O
powers	O
of	O
1=2	O
.	O
solution	O
to	O
exercise	O
16.2	O
(	O
p.245	O
)	O
.	O
to	O
make	O
a	O
uniform	O
random	B
walk	I
,	O
each	O
for-	O
ward	O
step	O
of	O
the	O
walk	O
should	O
be	O
chosen	O
using	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
biased	O
coin	B
at	O
each	O
junction	O
,	O
with	O
the	O
biases	O
chosen	O
in	O
proportion	O
to	O
the	O
backward	O
messages	O
ema-	O
nating	O
from	O
the	O
two	O
options	O
.	O
for	O
example	O
,	O
at	O
the	O
(	O
cid:12	O
)	O
rst	O
choice	O
after	O
leaving	O
a	O
,	O
there	O
is	O
a	O
‘	O
3	O
’	O
message	O
coming	O
from	O
the	O
east	O
,	O
and	O
a	O
‘	O
2	O
’	O
coming	O
from	O
south	O
,	O
so	O
one	O
should	O
go	O
east	O
with	O
probability	O
3=5	O
and	O
south	O
with	O
probability	O
2=5	O
.	O
this	O
is	O
how	O
the	O
path	O
in	O
(	O
cid:12	O
)	O
gure	O
16.11b	O
was	O
generated	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
17	O
communication	B
over	O
constrained	B
noiseless	O
channels	O
in	O
this	O
chapter	O
we	O
study	O
the	O
task	O
of	O
communicating	O
e	O
(	O
cid:14	O
)	O
ciently	O
over	O
a	O
con-	O
strained	O
noiseless	B
channel	O
{	O
a	O
constrained	B
channel	I
over	O
which	O
not	O
all	O
strings	O
from	O
the	O
input	O
alphabet	O
may	O
be	O
transmitted	O
.	O
we	O
make	O
use	O
of	O
the	O
idea	O
introduced	O
in	O
chapter	O
16	O
,	O
that	O
global	O
properties	O
of	O
graphs	O
can	O
be	O
computed	O
by	O
a	O
local	O
message-passing	B
algorithm	O
.	O
17.1	O
three	O
examples	O
of	O
constrained	O
binary	O
channels	O
a	O
constrained	B
channel	I
can	O
be	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
rules	O
that	O
de	O
(	O
cid:12	O
)	O
ne	O
which	O
strings	O
are	O
permitted	O
.	O
example	O
17.1.	O
in	O
channel	O
a	O
every	O
1	O
must	O
be	O
followed	O
by	O
at	O
least	O
one	O
0.	O
a	O
valid	O
string	O
for	O
this	O
channel	B
is	O
channel	B
a	O
:	O
the	O
substring	B
11	O
is	O
forbidden	O
.	O
00100101001010100010	O
:	O
(	O
17.1	O
)	O
as	O
a	O
motivation	O
for	O
this	O
model	B
,	O
consider	O
a	O
channel	B
in	O
which	O
1s	O
are	O
repre-	O
sented	O
by	O
pulses	O
of	O
electromagnetic	O
energy	B
,	O
and	O
the	O
device	O
that	O
produces	O
those	O
pulses	O
requires	O
a	O
recovery	O
time	O
of	O
one	O
clock	O
cycle	O
after	O
generating	O
a	O
pulse	O
before	O
it	O
can	O
generate	O
another	O
.	O
example	O
17.2.	O
channel	B
b	O
has	O
the	O
rule	O
that	O
all	O
1s	O
must	O
come	O
in	O
groups	O
of	O
two	O
or	O
more	O
,	O
and	O
all	O
0s	O
must	O
come	O
in	O
groups	O
of	O
two	O
or	O
more	O
.	O
a	O
valid	O
string	O
for	O
this	O
channel	B
is	O
channel	B
b	O
:	O
101	O
and	O
010	O
are	O
forbidden	O
.	O
00111001110011000011	O
:	O
(	O
17.2	O
)	O
as	O
a	O
motivation	O
for	O
this	O
model	B
,	O
consider	O
a	O
disk	B
drive	I
in	O
which	O
succes-	O
sive	O
bits	O
are	O
written	O
onto	O
neighbouring	O
points	O
in	O
a	O
track	O
along	O
the	O
disk	O
surface	O
;	O
the	O
values	O
0	O
and	O
1	O
are	O
represented	O
by	O
two	O
opposite	O
magnetic	O
orientations	O
.	O
the	O
strings	O
101	O
and	O
010	O
are	O
forbidden	O
because	O
a	O
single	O
isolated	O
magnetic	O
domain	O
surrounded	O
by	O
domains	O
having	O
the	O
opposite	O
orientation	O
is	O
unstable	O
,	O
so	O
that	O
101	O
might	O
turn	O
into	O
111	O
,	O
for	O
example	O
.	O
example	O
17.3.	O
channel	B
c	O
has	O
the	O
rule	O
that	O
the	O
largest	O
permitted	O
runlength	B
is	O
two	O
,	O
that	O
is	O
,	O
each	O
symbol	O
can	O
be	O
repeated	O
at	O
most	O
once	O
.	O
a	O
valid	O
string	O
for	O
this	O
channel	B
is	O
channel	B
c	O
:	O
111	O
and	O
000	O
are	O
forbidden	O
.	O
10010011011001101001	O
:	O
(	O
17.3	O
)	O
248	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
17.1	O
:	O
three	O
examples	O
of	O
constrained	O
binary	O
channels	O
249	O
a	O
physical	O
motivation	O
for	O
this	O
model	B
is	O
a	O
disk	B
drive	I
in	O
which	O
the	O
rate	B
of	O
rotation	O
of	O
the	O
disk	O
is	O
not	O
known	O
accurately	O
,	O
so	O
it	O
is	O
di	O
(	O
cid:14	O
)	O
cult	O
to	O
distinguish	O
between	O
a	O
string	O
of	O
two	O
1s	O
and	O
a	O
string	O
of	O
three	O
1s	O
,	O
which	O
are	O
represented	O
by	O
oriented	O
magnetizations	O
of	O
duration	O
2	O
(	O
cid:28	O
)	O
and	O
3	O
(	O
cid:28	O
)	O
respectively	O
,	O
where	O
(	O
cid:28	O
)	O
is	O
the	O
(	O
poorly	O
known	O
)	O
time	O
taken	O
for	O
one	O
bit	B
to	O
pass	O
by	O
;	O
to	O
avoid	O
the	O
possibility	O
of	O
confusion	O
,	O
and	O
the	O
resulting	O
loss	O
of	O
synchronization	B
of	O
sender	O
and	O
receiver	O
,	O
we	O
forbid	O
the	O
string	O
of	O
three	O
1s	O
and	O
the	O
string	O
of	O
three	O
0s	O
.	O
all	O
three	O
of	O
these	O
channels	O
are	O
examples	O
of	O
runlength-limited	O
channels	O
.	O
the	O
rules	B
constrain	O
the	O
minimum	O
and	O
maximum	O
numbers	O
of	O
successive	O
1s	O
and	O
0s	O
.	O
channel	B
runlength	O
of	O
1s	O
runlength	B
of	O
0s	O
minimum	O
maximum	O
minimum	O
maximum	O
unconstrained	O
a	O
b	O
c	O
1	O
1	O
2	O
1	O
1	O
1	O
1	O
2	O
1	O
1	O
2	O
1	O
1	O
1	O
1	O
2	O
in	O
channel	O
a	O
,	O
runs	O
of	O
0s	O
may	O
be	O
of	O
any	O
length	O
but	O
runs	O
of	O
1s	O
are	O
restricted	O
to	O
length	B
one	O
.	O
in	O
channel	O
b	O
all	O
runs	O
must	O
be	O
of	O
length	O
two	O
or	O
more	O
.	O
in	O
channel	O
c	O
,	O
all	O
runs	O
must	O
be	O
of	O
length	O
one	O
or	O
two	O
.	O
the	O
capacity	B
of	O
the	O
unconstrained	O
binary	O
channel	O
is	O
one	O
bit	B
per	O
channel	B
use	O
.	O
what	O
are	O
the	O
capacities	O
of	O
the	O
three	O
constrained	B
channels	O
?	O
[	O
to	O
be	O
fair	O
,	O
we	O
haven	O
’	O
t	O
de	O
(	O
cid:12	O
)	O
ned	O
the	O
‘	O
capacity	B
’	O
of	O
such	O
channels	O
yet	O
;	O
please	O
understand	O
‘	O
ca-	O
pacity	O
’	O
as	O
meaning	O
how	O
many	O
bits	O
can	O
be	O
conveyed	O
reliably	O
per	O
channel-use	O
.	O
]	O
some	O
codes	O
for	O
a	O
constrained	B
channel	I
let	O
us	O
concentrate	O
for	O
a	O
moment	O
on	O
channel	O
a	O
,	O
in	O
which	O
runs	O
of	O
0s	O
may	O
be	O
of	O
any	O
length	O
but	O
runs	O
of	O
1s	O
are	O
restricted	O
to	O
length	B
one	O
.	O
we	O
would	O
like	O
to	O
communicate	O
a	O
random	B
binary	O
(	O
cid:12	O
)	O
le	O
over	O
this	O
channel	B
as	O
e	O
(	O
cid:14	O
)	O
ciently	O
as	O
possible	O
.	O
a	O
simple	O
starting	O
point	O
is	O
a	O
(	O
2	O
;	O
1	O
)	O
code	B
that	O
maps	O
each	O
source	O
bit	O
into	O
two	O
transmitted	O
bits	O
,	O
c1	O
.	O
this	O
is	O
a	O
rate-1/2	O
code	B
,	O
and	O
it	O
respects	O
the	O
constraints	O
of	O
channel	O
a	O
,	O
so	O
the	O
capacity	B
of	O
channel	B
a	O
is	O
at	O
least	O
0.5.	O
can	O
we	O
do	O
better	O
?	O
c1	O
is	O
redundant	O
because	O
if	O
the	O
(	O
cid:12	O
)	O
rst	O
of	O
two	O
received	O
bits	O
is	O
a	O
zero	O
,	O
we	O
know	O
that	O
the	O
second	O
bit	B
will	O
also	O
be	O
a	O
zero	O
.	O
we	O
can	O
achieve	O
a	O
smaller	O
average	B
transmitted	O
length	B
using	O
a	O
code	B
that	O
omits	O
the	O
redundant	O
zeroes	O
in	O
c1	O
.	O
c2	O
is	O
such	O
a	O
variable-length	B
code	I
.	O
if	O
the	O
source	O
symbols	O
are	O
used	O
with	O
equal	O
frequency	B
then	O
the	O
average	B
transmitted	O
length	B
per	O
source	O
bit	O
is	O
l	O
=	O
1	O
2	O
1	O
+	O
1	O
2	O
2	O
=	O
3	O
2	O
;	O
so	O
the	O
average	B
communication	O
rate	B
is	O
r	O
=	O
2/3	O
;	O
(	O
17.4	O
)	O
(	O
17.5	O
)	O
and	O
the	O
capacity	B
of	O
channel	B
a	O
must	O
be	O
at	O
least	O
2/3	O
.	O
can	O
we	O
do	O
better	O
than	O
c2	O
?	O
there	O
are	O
two	O
ways	O
to	O
argue	O
that	O
the	O
infor-	O
mation	O
rate	B
could	O
be	O
increased	O
above	O
r	O
=	O
2/3	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
argument	O
assumes	O
we	O
are	O
comfortable	O
with	O
the	O
entropy	B
as	O
a	O
measure	O
of	O
information	O
content	B
.	O
the	O
idea	O
is	O
that	O
,	O
starting	O
from	O
code	O
c2	O
,	O
we	O
can	O
reduce	O
the	O
average	B
message	O
length	B
,	O
without	O
greatly	O
reducing	O
the	O
entropy	B
code	O
c1	O
s	O
t	O
0	O
00	O
1	O
10	O
code	B
c2	O
s	O
t	O
0	O
0	O
1	O
10	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
250	O
17	O
|	O
communication	B
over	O
constrained	B
noiseless	O
channels	O
2	O
1	O
0	O
0	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
1+f	O
h_2	O
(	O
f	O
)	O
0.25	O
0.5	O
0.75	O
1	O
r	O
(	O
f	O
)	O
=	O
h_2	O
(	O
f	O
)	O
/	O
(	O
1+f	O
)	O
0.25	O
0.5	O
0.75	O
1	O
figure	O
17.1.	O
top	O
:	O
the	O
information	B
content	I
per	O
source	O
symbol	O
and	O
mean	O
transmitted	O
length	B
per	O
source	O
symbol	O
as	O
a	O
function	B
of	O
the	O
source	O
density	O
.	O
bottom	O
:	O
the	O
information	B
content	I
per	O
transmitted	O
symbol	O
,	O
in	O
bits	O
,	O
as	O
a	O
function	B
of	O
f	O
.	O
of	O
the	O
message	O
we	O
send	O
,	O
by	O
decreasing	O
the	O
fraction	O
of	O
1s	O
that	O
we	O
transmit	O
.	O
imagine	O
feeding	O
into	O
c2	O
a	O
stream	O
of	O
bits	O
in	O
which	O
the	O
frequency	B
of	O
1s	O
is	O
f	O
.	O
[	O
such	O
a	O
stream	O
could	O
be	O
obtained	O
from	O
an	O
arbitrary	O
binary	O
(	O
cid:12	O
)	O
le	O
by	O
passing	O
the	O
source	O
(	O
cid:12	O
)	O
le	O
into	O
the	O
decoder	B
of	O
an	O
arithmetic	O
code	O
that	O
is	O
optimal	B
for	O
compressing	O
binary	O
strings	O
of	O
density	O
f	O
.	O
]	O
the	O
information	B
rate	O
r	O
achieved	O
is	O
the	O
entropy	B
of	O
the	O
source	O
,	O
h2	O
(	O
f	O
)	O
,	O
divided	O
by	O
the	O
mean	B
transmitted	O
length	B
,	O
thus	O
l	O
(	O
f	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
+	O
2f	O
=	O
1	O
+	O
f	O
:	O
r	O
(	O
f	O
)	O
=	O
h2	O
(	O
f	O
)	O
l	O
(	O
f	O
)	O
=	O
h2	O
(	O
f	O
)	O
1	O
+	O
f	O
:	O
(	O
17.6	O
)	O
(	O
17.7	O
)	O
the	O
original	O
code	B
c2	O
,	O
without	O
preprocessor	O
,	O
corresponds	O
to	O
f	O
=	O
1/2	O
.	O
what	O
happens	O
if	O
we	O
perturb	O
f	O
a	O
little	O
towards	O
smaller	O
f	O
,	O
setting	O
f	O
=	O
1	O
2	O
+	O
(	O
cid:14	O
)	O
;	O
(	O
17.8	O
)	O
for	O
small	O
negative	O
(	O
cid:14	O
)	O
?	O
in	O
the	O
vicinity	O
of	O
f	O
=	O
1/2	O
,	O
the	O
denominator	O
l	O
(	O
f	O
)	O
varies	O
linearly	O
with	O
(	O
cid:14	O
)	O
.	O
in	O
contrast	O
,	O
the	O
numerator	O
h2	O
(	O
f	O
)	O
only	O
has	O
a	O
second-order	O
dependence	O
on	O
(	O
cid:14	O
)	O
.	O
.	O
exercise	O
17.4	O
.	O
[	O
1	O
]	O
find	O
,	O
to	O
order	O
(	O
cid:14	O
)	O
2	O
,	O
the	O
taylor	O
expansion	O
of	O
h2	O
(	O
f	O
)	O
as	O
a	O
function	B
of	O
(	O
cid:14	O
)	O
.	O
to	O
(	O
cid:12	O
)	O
rst	O
order	O
,	O
r	O
(	O
f	O
)	O
increases	O
linearly	O
with	O
decreasing	O
(	O
cid:14	O
)	O
.	O
it	O
must	O
be	O
possible	O
to	O
increase	O
r	O
by	O
decreasing	O
f	O
.	O
figure	O
17.1	O
shows	O
these	O
functions	B
;	O
r	O
(	O
f	O
)	O
does	O
indeed	O
increase	O
as	O
f	O
decreases	O
and	O
has	O
a	O
maximum	O
of	O
about	O
0.69	O
bits	O
per	O
channel	B
use	O
at	O
f	O
’	O
0:38.	O
maxf	O
r	O
(	O
f	O
)	O
=	O
0:69.	O
by	O
this	O
argument	O
we	O
have	O
shown	O
that	O
the	O
capacity	B
of	O
channel	B
a	O
is	O
at	O
least	O
.	O
exercise	O
17.5	O
.	O
[	O
2	O
,	O
p.257	O
]	O
if	O
a	O
(	O
cid:12	O
)	O
le	O
containing	O
a	O
fraction	O
f	O
=	O
0:5	O
1s	O
is	O
transmitted	O
by	O
c2	O
,	O
what	O
fraction	O
of	O
the	O
transmitted	O
stream	O
is	O
1s	O
?	O
what	O
fraction	O
of	O
the	O
transmitted	O
bits	O
is	O
1s	O
if	O
we	O
drive	O
code	B
c2	O
with	O
a	O
sparse	O
source	O
of	O
density	O
f	O
=	O
0:38	O
?	O
a	O
second	O
,	O
more	O
fundamental	O
approach	O
counts	O
how	O
many	O
valid	O
sequences	O
of	O
length	O
n	O
there	O
are	O
,	O
sn	O
.	O
we	O
can	O
communicate	O
log	O
sn	O
bits	O
in	O
n	O
channel	B
cycles	O
by	O
giving	O
one	O
name	O
to	O
each	O
of	O
these	O
valid	O
sequences	O
.	O
17.2	O
the	O
capacity	B
of	O
a	O
constrained	B
noiseless	O
channel	B
we	O
de	O
(	O
cid:12	O
)	O
ned	O
the	O
capacity	B
of	O
a	O
noisy	B
channel	I
in	O
terms	O
of	O
the	O
mutual	O
information	B
between	O
its	O
input	O
and	O
its	O
output	O
,	O
then	O
we	O
proved	O
that	O
this	O
number	O
,	O
the	O
capac-	O
ity	O
,	O
was	O
related	O
to	O
the	O
number	O
of	O
distinguishable	O
messages	O
s	O
(	O
n	O
)	O
that	O
could	O
be	O
reliably	O
conveyed	O
over	O
the	O
channel	B
in	O
n	O
uses	O
of	O
the	O
channel	B
by	O
c	O
=	O
lim	O
n	O
!	O
1	O
1	O
n	O
log	O
s	O
(	O
n	O
)	O
:	O
(	O
17.9	O
)	O
in	O
the	O
case	O
of	O
the	O
constrained	O
noiseless	B
channel	O
,	O
we	O
can	O
adopt	O
this	O
identity	O
as	O
our	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
the	O
channel	O
’	O
s	O
capacity	B
.	O
however	O
,	O
the	O
name	O
s	O
,	O
which	O
,	O
when	O
we	O
were	O
making	O
codes	O
for	O
noisy	O
channels	O
(	O
section	B
9.6	O
)	O
,	O
ran	O
over	O
messages	O
s	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
s	O
,	O
is	O
about	O
to	O
take	O
on	O
a	O
new	O
role	O
:	O
labelling	O
the	O
states	O
of	O
our	O
channel	B
;	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
17.3	O
:	O
counting	B
the	O
number	O
of	O
possible	O
messages	O
251	O
1	O
0	O
0	O
1	O
0	O
(	O
a	O
)	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
1	O
(	O
c	O
)	O
f0	O
-	O
(	O
cid:0	O
)	O
0	O
s1	O
1	O
f	O
f	O
0	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
1	O
0	O
@	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
0	O
s2	O
1	O
f	O
f	O
0	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
1	O
0	O
@	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
0	O
s3	O
1	O
f	O
f	O
0	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
1	O
0	O
@	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
0	O
s4	O
1	O
f	O
f	O
0	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
1	O
0	O
@	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
0	O
s5	O
1	O
f	O
f	O
0	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
1	O
0	O
@	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
0	O
s6	O
1	O
f	O
f	O
0	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
1	O
0	O
@	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
0	O
s7	O
1	O
f	O
f	O
0	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
1	O
0	O
@	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
0	O
s8	O
1	O
f	O
f	O
0	O
(	O
b	O
)	O
sn	O
1	O
j	O
j	O
0	O
@	O
0	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
1	O
(	O
cid:0	O
)	O
@	O
@	O
r	O
-	O
0	O
1	O
11	O
1	O
0	O
1	O
0	O
1	O
0	O
00	O
0	O
b	O
1	O
1	O
0	O
11	O
sn	O
sn+1	O
j	O
j	O
m	O
m	O
m	O
m	O
a	O
=2	O
664	O
00	O
0	O
sn+1	O
11	O
-1	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
1	O
(	O
cid:0	O
)	O
a	O
0	O
a	O
(	O
cid:0	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
1	O
(	O
cid:1	O
)	O
@	O
0	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
a	O
a	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
aau	O
@	O
@	O
r	O
-	O
0	O
00	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
m	O
m	O
m	O
m	O
3	O
775	O
figure	O
17.2	O
.	O
(	O
a	O
)	O
state	B
diagram	I
for	O
channel	B
a	O
.	O
(	O
b	O
)	O
trellis	B
section	O
.	O
(	O
c	O
)	O
trellis	B
.	O
(	O
d	O
)	O
connection	B
matrix	I
.	O
figure	O
17.3.	O
state	O
diagrams	O
,	O
trellis	B
sections	O
and	O
connection	O
matrices	B
for	O
channels	O
b	O
and	O
c.	O
(	O
d	O
)	O
a	O
=	O
(	O
to	O
)	O
1	O
0	O
(	O
from	O
)	O
1	O
0	O
1	O
(	O
cid:20	O
)	O
0	O
1	O
1	O
(	O
cid:21	O
)	O
11	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
0	O
00	O
c	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
1	O
(	O
cid:0	O
)	O
0	O
a	O
(	O
cid:0	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
a	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
1	O
@	O
0	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
(	O
cid:0	O
)	O
a	O
@	O
(	O
cid:0	O
)	O
(	O
cid:1	O
)	O
aau	O
@	O
1	O
(	O
cid:0	O
)	O
@	O
r	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
@	O
0	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
@	O
@	O
r	O
sn	O
1	O
0	O
a	O
11	O
n	O
n	O
n	O
n	O
a	O
=2	O
664	O
00	O
(	O
cid:1	O
)	O
sn+1	O
11	O
1	O
0	O
n	O
n	O
n	O
n	O
3	O
775	O
00	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
so	O
in	O
this	O
chapter	O
we	O
will	O
denote	O
the	O
number	O
of	O
distinguishable	O
messages	O
of	O
length	O
n	O
by	O
mn	O
,	O
and	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
capacity	B
to	O
be	O
:	O
c	O
=	O
lim	O
n	O
!	O
1	O
1	O
n	O
log	O
mn	O
:	O
(	O
17.10	O
)	O
once	O
we	O
have	O
(	O
cid:12	O
)	O
gured	O
out	O
the	O
capacity	B
of	O
a	O
channel	B
we	O
will	O
return	O
to	O
the	O
task	O
of	O
making	O
a	O
practical	B
code	O
for	O
that	O
channel	B
.	O
17.3	O
counting	B
the	O
number	O
of	O
possible	O
messages	O
first	O
let	O
us	O
introduce	O
some	O
representations	O
of	O
constrained	O
channels	O
.	O
in	O
a	O
state	B
diagram	I
,	O
states	O
of	O
the	O
transmitter	O
are	O
represented	O
by	O
circles	O
labelled	O
with	O
the	O
name	O
of	O
the	O
state	O
.	O
directed	O
edges	O
from	O
one	O
state	O
to	O
another	O
indicate	O
that	O
the	O
transmitter	O
is	O
permitted	O
to	O
move	O
from	O
the	O
(	O
cid:12	O
)	O
rst	O
state	O
to	O
the	O
second	O
,	O
and	O
a	O
label	O
on	O
that	O
edge	B
indicates	O
the	O
symbol	O
emitted	O
when	O
that	O
transition	B
is	O
made	O
.	O
figure	O
17.2a	O
shows	O
the	O
state	B
diagram	I
for	O
channel	B
a.	O
it	O
has	O
two	O
states	O
,	O
0	O
and	O
1.	O
when	O
transitions	O
to	O
state	O
0	O
are	O
made	O
,	O
a	O
0	O
is	O
transmitted	O
;	O
when	O
transitions	O
to	O
state	O
1	O
are	O
made	O
,	O
a	O
1	O
is	O
transmitted	O
;	O
transitions	O
from	O
state	O
1	O
to	O
state	O
1	O
are	O
not	O
possible	O
.	O
we	O
can	O
also	O
represent	O
the	O
state	B
diagram	I
by	O
a	O
trellis	B
section	O
,	O
which	O
shows	O
two	O
successive	O
states	O
in	O
time	O
at	O
two	O
successive	O
horizontal	O
locations	O
(	O
(	O
cid:12	O
)	O
g-	O
ure	O
17.2b	O
)	O
.	O
the	O
state	O
of	O
the	O
transmitter	O
at	O
time	O
n	O
is	O
called	O
sn	O
.	O
the	O
set	B
of	O
possible	O
state	O
sequences	O
can	O
be	O
represented	O
by	O
a	O
trellis	B
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
17.2c	O
.	O
a	O
valid	O
sequence	B
corresponds	O
to	O
a	O
path	O
through	O
the	O
trellis	B
,	O
and	O
the	O
number	O
of	O
 	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
252	O
17	O
|	O
communication	B
over	O
constrained	B
noiseless	O
channels	O
m1	O
=	O
2	O
m1	O
=	O
2	O
m2	O
=	O
3	O
m1	O
=	O
2	O
m2	O
=	O
3	O
m3	O
=	O
5	O
1	O
1	O
h	O
h	O
0	O
1	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
-	O
(	O
cid:0	O
)	O
h0	O
1	O
1	O
h	O
h	O
0	O
1	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
-	O
(	O
cid:0	O
)	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
h0	O
1	O
1	O
h	O
h	O
0	O
2	O
h0	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
-	O
(	O
cid:0	O
)	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
1	O
1	O
h	O
h	O
0	O
2	O
2	O
1	O
h	O
h	O
0	O
3	O
1	O
1	O
h	O
h	O
0	O
1	O
figure	O
17.4.	O
counting	B
the	O
number	O
of	O
paths	O
in	O
the	O
trellis	B
of	O
channel	B
a.	O
the	O
counts	O
next	O
to	O
the	O
nodes	O
are	O
accumulated	O
by	O
passing	O
from	O
left	O
to	O
right	O
across	O
the	O
trellises	O
.	O
1	O
1	O
h	O
h	O
0	O
1	O
11	O
1	O
1	O
h	O
h	O
h	O
h	O
0	O
00	O
1	O
1	O
1	O
h	O
h	O
0	O
2	O
1	O
11	O
1	O
1	O
h	O
h	O
h	O
h	O
0	O
00	O
1	O
(	O
a	O
)	O
channel	B
a	O
(	O
b	O
)	O
channel	B
b	O
(	O
c	O
)	O
channel	B
c	O
m1	O
=	O
2	O
m2	O
=	O
3	O
m3	O
=	O
5	O
m4	O
=	O
8	O
m5	O
=	O
13	O
m6	O
=	O
21	O
m7	O
=	O
34	O
m8	O
=	O
55	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
-	O
(	O
cid:0	O
)	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
-	O
3	O
1	O
h	O
h	O
0	O
5	O
5	O
1	O
h	O
h	O
0	O
8	O
8	O
1	O
h	O
h	O
0	O
13	O
13	O
1	O
h	O
h	O
0	O
21	O
21	O
1	O
h	O
h	O
0	O
34	O
2	O
1	O
h	O
h	O
0	O
3	O
h0	O
m1	O
=	O
2	O
m2	O
=	O
3	O
m3	O
=	O
5	O
m4	O
=	O
8	O
m5	O
=	O
13	O
m6	O
=	O
21	O
m7	O
=	O
34	O
m8	O
=	O
55	O
a	O
(	O
cid:0	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
-	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
a	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
a	O
(	O
cid:1	O
)	O
aau	O
(	O
cid:1	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
-	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
a	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
a	O
(	O
cid:1	O
)	O
aau	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
@	O
@	O
r	O
-	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
@	O
@	O
r	O
-	O
-	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
a	O
(	O
cid:1	O
)	O
aau	O
@	O
@	O
r	O
-	O
(	O
cid:0	O
)	O
a	O
a	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
@	O
2	O
11	O
1	O
1	O
1	O
0	O
h	O
h	O
h	O
h	O
00	O
1	O
a	O
a	O
(	O
cid:0	O
)	O
@	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
3	O
11	O
1	O
1	O
2	O
0	O
h	O
h	O
h	O
h	O
00	O
2	O
a	O
(	O
cid:0	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
-	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
a	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
a	O
(	O
cid:1	O
)	O
aau	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
@	O
@	O
r	O
-	O
4	O
11	O
2	O
1	O
3	O
0	O
h	O
h	O
h	O
h	O
00	O
4	O
a	O
a	O
(	O
cid:0	O
)	O
@	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
-	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
a	O
(	O
cid:1	O
)	O
aau	O
@	O
@	O
r	O
-	O
(	O
cid:0	O
)	O
a	O
a	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
@	O
-	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
a	O
(	O
cid:1	O
)	O
aau	O
@	O
@	O
r	O
-	O
(	O
cid:0	O
)	O
a	O
a	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
@	O
6	O
11	O
4	O
1	O
4	O
0	O
h	O
h	O
h	O
h	O
00	O
7	O
a	O
a	O
(	O
cid:0	O
)	O
@	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
10	O
11	O
7	O
1	O
6	O
0	O
h	O
h	O
h	O
h	O
00	O
11	O
a	O
(	O
cid:0	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
-	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
a	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
a	O
(	O
cid:1	O
)	O
aau	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
@	O
@	O
r	O
-	O
17	O
11	O
11	O
1	O
10	O
0	O
h	O
h	O
h	O
h	O
00	O
17	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
-	O
(	O
cid:1	O
)	O
h00	O
m1	O
=	O
1	O
m2	O
=	O
2	O
m3	O
=	O
3	O
m4	O
=	O
5	O
m5	O
=	O
8	O
m6	O
=	O
13	O
m7	O
=	O
21	O
m8	O
=	O
34	O
11	O
1	O
1	O
h	O
h	O
h	O
h	O
0	O
00	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
@	O
(	O
cid:0	O
)	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
a	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
a	O
(	O
cid:0	O
)	O
@	O
(	O
cid:1	O
)	O
aau	O
@	O
@	O
r	O
(	O
cid:1	O
)	O
@	O
@	O
r	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
h00	O
1	O
11	O
h	O
h	O
h	O
h	O
1	O
1	O
0	O
00	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
@	O
(	O
cid:0	O
)	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
a	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
a	O
(	O
cid:0	O
)	O
@	O
(	O
cid:1	O
)	O
aau	O
@	O
@	O
r	O
(	O
cid:1	O
)	O
@	O
@	O
r	O
11	O
1	O
1	O
1	O
0	O
h	O
h	O
h	O
h	O
a	O
a	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
@	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
00	O
1	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
a	O
a	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
a	O
(	O
cid:0	O
)	O
@	O
(	O
cid:1	O
)	O
aau	O
@	O
@	O
r	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
@	O
@	O
@	O
r	O
1	O
11	O
2	O
1	O
1	O
0	O
h	O
h	O
h	O
h	O
00	O
1	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
@	O
(	O
cid:0	O
)	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
a	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
a	O
(	O
cid:0	O
)	O
@	O
(	O
cid:1	O
)	O
aau	O
@	O
@	O
r	O
(	O
cid:1	O
)	O
@	O
@	O
r	O
2	O
11	O
2	O
1	O
3	O
0	O
h	O
h	O
h	O
h	O
a	O
a	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
@	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
a	O
a	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
a	O
(	O
cid:0	O
)	O
@	O
(	O
cid:1	O
)	O
aau	O
@	O
@	O
r	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
@	O
@	O
@	O
r	O
2	O
11	O
4	O
1	O
4	O
0	O
h	O
h	O
h	O
h	O
a	O
a	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
@	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
00	O
3	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
a	O
a	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
a	O
(	O
cid:0	O
)	O
@	O
(	O
cid:1	O
)	O
aau	O
@	O
@	O
r	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
@	O
@	O
@	O
r	O
00	O
1	O
4	O
11	O
7	O
1	O
6	O
0	O
h	O
h	O
h	O
h	O
00	O
4	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
(	O
cid:0	O
)	O
a	O
@	O
(	O
cid:0	O
)	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
@	O
(	O
cid:1	O
)	O
a	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
a	O
(	O
cid:0	O
)	O
@	O
(	O
cid:1	O
)	O
aau	O
@	O
@	O
r	O
(	O
cid:1	O
)	O
@	O
@	O
r	O
7	O
11	O
10	O
1	O
11	O
0	O
h	O
h	O
h	O
h	O
00	O
6	O
figure	O
17.5.	O
counting	B
the	O
number	O
of	O
paths	O
in	O
the	O
trellises	O
of	O
channels	O
a	O
,	O
b	O
,	O
and	O
c.	O
we	O
assume	O
that	O
at	O
the	O
start	O
the	O
(	O
cid:12	O
)	O
rst	O
bit	B
is	O
preceded	O
by	O
00	O
,	O
so	O
that	O
for	O
channels	O
a	O
and	O
b	O
,	O
any	O
initial	O
character	O
is	O
permitted	O
,	O
but	O
for	O
channel	O
c	O
,	O
the	O
(	O
cid:12	O
)	O
rst	O
character	O
must	O
be	O
a	O
1.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
17.3	O
:	O
counting	B
the	O
number	O
of	O
possible	O
messages	O
253	O
n	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
100	O
200	O
300	O
400	O
mn	O
mn=mn	O
(	O
cid:0	O
)	O
1	O
log2	O
mn	O
1	O
n	O
log2	O
mn	O
figure	O
17.6.	O
counting	B
the	O
number	O
of	O
paths	O
in	O
the	O
trellis	B
of	O
channel	B
a	O
.	O
2	O
3	O
5	O
8	O
13	O
21	O
34	O
55	O
89	O
144	O
233	O
377	O
9	O
(	O
cid:2	O
)	O
1020	O
7	O
(	O
cid:2	O
)	O
1041	O
6	O
(	O
cid:2	O
)	O
1062	O
5	O
(	O
cid:2	O
)	O
1083	O
1.500	O
1.667	O
1.600	O
1.625	O
1.615	O
1.619	O
1.618	O
1.618	O
1.618	O
1.618	O
1.618	O
1.618	O
1.618	O
1.618	O
1.618	O
1.0	O
1.6	O
2.3	O
3.0	O
3.7	O
4.4	O
5.1	O
5.8	O
6.5	O
7.2	O
7.9	O
8.6	O
69.7	O
139.1	O
208.5	O
277.9	O
1.00	O
0.79	O
0.77	O
0.75	O
0.74	O
0.73	O
0.73	O
0.72	O
0.72	O
0.72	O
0.71	O
0.71	O
0.70	O
0.70	O
0.70	O
0.69	O
valid	O
sequences	O
is	O
the	O
number	O
of	O
paths	O
.	O
for	O
the	O
purpose	O
of	O
counting	O
how	O
many	O
paths	O
there	O
are	O
through	O
the	O
trellis	B
,	O
we	O
can	O
ignore	O
the	O
labels	O
on	O
the	O
edges	O
and	O
summarize	O
the	O
trellis	B
section	O
by	O
the	O
connection	B
matrix	I
a	O
,	O
in	O
which	O
ass0	O
=	O
1	O
if	O
there	O
is	O
an	O
edge	B
from	O
state	O
s	O
to	O
s0	O
,	O
and	O
ass0	O
=	O
0	O
otherwise	O
(	O
(	O
cid:12	O
)	O
gure	O
17.2d	O
)	O
.	O
figure	O
17.3	O
shows	O
the	O
state	O
diagrams	O
,	O
trellis	B
sections	O
and	O
connection	O
matrices	B
for	O
channels	O
b	O
and	O
c.	O
let	O
’	O
s	O
count	O
the	O
number	O
of	O
paths	O
for	O
channel	O
a	O
by	O
message-passing	O
in	O
its	O
trellis	B
.	O
figure	O
17.4	O
shows	O
the	O
(	O
cid:12	O
)	O
rst	O
few	O
steps	O
of	O
this	O
counting	B
process	O
,	O
and	O
(	O
cid:12	O
)	O
gure	O
17.5a	O
shows	O
the	O
number	O
of	O
paths	O
ending	O
in	O
each	O
state	O
after	O
n	O
steps	O
for	O
n	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
8.	O
the	O
total	O
number	O
of	O
paths	O
of	O
length	O
n	O
,	O
mn	O
,	O
is	O
shown	O
along	O
the	O
top	O
.	O
we	O
recognize	O
mn	O
as	O
the	O
fibonacci	O
series	O
.	O
.	O
exercise	O
17.6	O
.	O
[	O
1	O
]	O
show	O
that	O
the	O
ratio	O
of	O
successive	O
terms	O
in	O
the	O
fibonacci	O
series	O
tends	O
to	O
the	O
golden	B
ratio	I
,	O
1	O
+	O
p5	O
2	O
(	O
cid:13	O
)	O
(	O
cid:17	O
)	O
=	O
1:618	O
:	O
(	O
17.11	O
)	O
thus	O
,	O
to	O
within	O
a	O
constant	O
factor	O
,	O
mn	O
scales	O
as	O
mn	O
(	O
cid:24	O
)	O
(	O
cid:13	O
)	O
n	O
as	O
n	O
!	O
1	O
,	O
so	O
the	O
capacity	B
of	O
channel	B
a	O
is	O
c	O
=	O
lim	O
1	O
n	O
log2	O
(	O
cid:2	O
)	O
constant	O
(	O
cid:1	O
)	O
(	O
cid:13	O
)	O
n	O
(	O
cid:3	O
)	O
=	O
log2	O
(	O
cid:13	O
)	O
=	O
log2	O
1:618	O
=	O
0:694	O
:	O
(	O
17.12	O
)	O
how	O
can	O
we	O
describe	O
what	O
we	O
just	O
did	O
?	O
the	O
count	O
of	O
the	O
number	O
of	O
paths	O
is	O
a	O
vector	O
c	O
(	O
n	O
)	O
;	O
we	O
can	O
obtain	O
c	O
(	O
n+1	O
)	O
from	O
c	O
(	O
n	O
)	O
using	O
:	O
c	O
(	O
n+1	O
)	O
=	O
ac	O
(	O
n	O
)	O
:	O
(	O
17.13	O
)	O
so	O
c	O
(	O
n	O
)	O
=	O
an	O
c	O
(	O
0	O
)	O
;	O
(	O
17.14	O
)	O
where	O
c	O
(	O
0	O
)	O
is	O
the	O
state	O
count	O
before	O
any	O
symbols	O
are	O
transmitted	O
.	O
in	O
(	O
cid:12	O
)	O
gure	O
17.5	O
we	O
assumed	O
c	O
(	O
0	O
)	O
=	O
[	O
0	O
;	O
1	O
]	O
t	O
,	O
i.e.	O
,	O
that	O
either	O
of	O
the	O
two	O
symbols	O
is	O
permitted	O
at	O
s	O
=	O
c	O
(	O
n	O
)	O
(	O
cid:1	O
)	O
n.	O
in	O
the	O
limit	O
,	O
the	O
outset	O
.	O
the	O
total	O
number	O
of	O
paths	O
is	O
mn	O
=ps	O
c	O
(	O
n	O
)	O
c	O
(	O
n	O
)	O
becomes	O
dominated	O
by	O
the	O
principal	O
right-eigenvector	O
of	O
a.	O
c	O
(	O
n	O
)	O
!	O
constant	O
(	O
cid:1	O
)	O
(	O
cid:21	O
)	O
n	O
1	O
e	O
(	O
0	O
)	O
r	O
:	O
(	O
17.15	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
254	O
17	O
|	O
communication	B
over	O
constrained	B
noiseless	O
channels	O
here	O
,	O
(	O
cid:21	O
)	O
1	O
is	O
the	O
principal	O
eigenvalue	B
of	O
a.	O
so	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
capacity	B
of	O
any	O
constrained	B
channel	I
,	O
all	O
we	O
need	O
to	O
do	O
is	O
(	O
cid:12	O
)	O
nd	O
the	O
principal	O
eigenvalue	B
,	O
(	O
cid:21	O
)	O
1	O
,	O
of	O
its	O
connection	B
matrix	I
.	O
then	O
c	O
=	O
log2	O
(	O
cid:21	O
)	O
1	O
:	O
(	O
17.16	O
)	O
17.4	O
back	O
to	O
our	O
model	B
channels	O
comparing	O
(	O
cid:12	O
)	O
gure	O
17.5a	O
and	O
(	O
cid:12	O
)	O
gures	O
17.5b	O
and	O
c	O
it	O
looks	O
as	O
if	O
channels	O
b	O
and	O
c	O
have	O
the	O
same	O
capacity	B
as	O
channel	B
a.	O
the	O
principal	O
eigenvalues	O
of	O
the	O
three	O
trellises	O
are	O
the	O
same	O
(	O
the	O
eigenvectors	O
for	O
channels	O
a	O
and	O
b	O
are	O
given	O
at	O
the	O
bottom	O
of	O
table	O
c.4	O
,	O
p.608	O
)	O
.	O
and	O
indeed	O
the	O
channels	O
are	O
intimately	O
related	O
.	O
-	O
t	O
z1	O
hd	O
-	O
z0	O
6	O
(	O
cid:27	O
)	O
(	O
cid:8	O
)	O
s	O
z1	O
hd	O
(	O
cid:27	O
)	O
z0	O
t	O
?	O
-	O
s	O
(	O
cid:8	O
)	O
-	O
figure	O
17.7.	O
an	O
accumulator	B
and	O
a	O
di	O
(	O
cid:11	O
)	O
erentiator	O
.	O
equivalence	B
of	O
channels	O
a	O
and	O
b	O
if	O
we	O
take	O
any	O
valid	O
string	O
s	O
for	O
channel	O
a	O
and	O
pass	O
it	O
through	O
an	O
accumulator	B
,	O
obtaining	O
t	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
:	O
t1	O
=	O
s1	O
tn	O
=	O
tn	O
(	O
cid:0	O
)	O
1	O
+	O
sn	O
mod	O
2	O
for	O
n	O
(	O
cid:21	O
)	O
2	O
,	O
(	O
17.17	O
)	O
then	O
the	O
resulting	O
string	O
is	O
a	O
valid	O
string	O
for	O
channel	O
b	O
,	O
because	O
there	O
are	O
no	O
11s	O
in	O
s	O
,	O
so	O
there	O
are	O
no	O
isolated	O
digits	O
in	O
t.	O
the	O
accumulator	B
is	O
an	O
invertible	O
operator	O
,	O
so	O
,	O
similarly	O
,	O
any	O
valid	O
string	O
t	O
for	O
channel	O
b	O
can	O
be	O
mapped	O
onto	O
a	O
valid	O
string	O
s	O
for	O
channel	O
a	O
through	O
the	O
binary	O
di	O
(	O
cid:11	O
)	O
erentiator	O
,	O
s1	O
=	O
t1	O
sn	O
=	O
tn	O
(	O
cid:0	O
)	O
tn	O
(	O
cid:0	O
)	O
1	O
mod	O
2	O
for	O
n	O
(	O
cid:21	O
)	O
2	O
.	O
(	O
17.18	O
)	O
because	O
+	O
and	O
(	O
cid:0	O
)	O
are	O
equivalent	O
in	O
modulo	O
2	O
arithmetic	O
,	O
the	O
di	O
(	O
cid:11	O
)	O
erentiator	O
is	O
also	O
a	O
blurrer	O
,	O
convolving	O
the	O
source	O
stream	O
with	O
the	O
(	O
cid:12	O
)	O
lter	O
(	O
1	O
;	O
1	O
)	O
.	O
channel	B
c	O
is	O
also	O
intimately	O
related	O
to	O
channels	O
a	O
and	O
b.	O
.	O
exercise	O
17.7	O
.	O
[	O
1	O
,	O
p.257	O
]	O
what	O
is	O
the	O
relationship	O
of	O
channel	O
c	O
to	O
channels	O
a	O
and	O
b	O
?	O
17.5	O
practical	B
communication	O
over	O
constrained	O
channels	O
ok	O
,	O
how	O
to	O
do	O
it	O
in	O
practice	O
?	O
since	O
all	O
three	O
channels	O
are	O
equivalent	O
,	O
we	O
can	O
concentrate	O
on	O
channel	O
a.	O
fixed-length	O
solutions	O
we	O
start	O
with	O
explicitly-enumerated	O
codes	O
.	O
the	O
code	B
in	O
the	O
table	O
17.8	O
achieves	O
a	O
rate	B
of	O
3/5	O
=	O
0:6.	O
.	O
exercise	O
17.8	O
.	O
[	O
1	O
,	O
p.257	O
]	O
similarly	O
,	O
enumerate	O
all	O
strings	O
of	O
length	O
8	O
that	O
end	O
in	O
the	O
zero	O
state	O
.	O
(	O
there	O
are	O
34	O
of	O
them	O
.	O
)	O
hence	O
show	O
that	O
we	O
can	O
map	O
5	O
bits	O
(	O
32	O
source	O
strings	O
)	O
to	O
8	O
transmitted	O
bits	O
and	O
achieve	O
rate	B
5/8	O
=	O
0:625.	O
what	O
rate	O
can	O
be	O
achieved	O
by	O
mapping	O
an	O
integer	O
number	O
of	O
source	O
bits	O
to	O
n	O
=	O
16	O
transmitted	O
bits	O
?	O
s	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
c	O
(	O
s	O
)	O
00000	O
10000	O
01000	O
00100	O
00010	O
10100	O
01010	O
10010	O
table	O
17.8.	O
a	O
runlength-limited	O
code	O
for	O
channel	O
a	O
.	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
17.5	O
:	O
practical	B
communication	O
over	O
constrained	O
channels	O
255	O
optimal	B
variable-length	O
solution	O
the	O
optimal	B
way	O
to	O
convey	O
information	B
over	O
the	O
constrained	B
channel	I
is	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
optimal	B
transition	O
probabilities	O
for	O
all	O
points	O
in	O
the	O
trellis	B
,	O
qs0js	O
,	O
and	O
make	O
transitions	O
with	O
these	O
probabilities	O
.	O
when	O
discussing	O
channel	B
a	O
,	O
we	O
showed	O
that	O
a	O
sparse	O
source	O
with	O
density	O
f	O
=	O
0:38	O
,	O
driving	O
code	B
c2	O
,	O
would	O
achieve	O
capacity	B
.	O
and	O
we	O
know	O
how	O
to	O
make	O
sparsi	O
(	O
cid:12	O
)	O
ers	O
(	O
chapter	O
6	O
)	O
:	O
we	O
design	O
an	O
arithmetic	O
code	O
that	O
is	O
optimal	B
for	O
compressing	O
a	O
sparse	O
source	O
;	O
then	O
its	O
associated	O
decoder	B
gives	O
an	O
optimal	B
mapping	O
from	O
dense	O
(	O
i.e.	O
,	O
random	B
binary	O
)	O
strings	O
to	O
sparse	O
strings	O
.	O
the	O
task	O
of	O
(	O
cid:12	O
)	O
nding	O
the	O
optimal	B
probabilities	O
is	O
given	O
as	O
an	O
exercise	O
.	O
exercise	O
17.9	O
.	O
[	O
3	O
]	O
show	O
that	O
the	O
optimal	B
transition	O
probabilities	O
q	O
can	O
be	O
found	O
as	O
follows	O
.	O
find	O
the	O
principal	O
right-	O
and	O
left-eigenvectors	O
of	O
a	O
,	O
that	O
is	O
the	O
solutions	O
of	O
ae	O
(	O
r	O
)	O
=	O
(	O
cid:21	O
)	O
e	O
(	O
r	O
)	O
and	O
e	O
(	O
l	O
)	O
t	O
with	O
largest	O
eigenvalue	B
(	O
cid:21	O
)	O
.	O
then	O
construct	O
a	O
matrix	B
q	O
whose	O
invariant	B
distribution	I
is	O
proportional	O
to	O
e	O
(	O
r	O
)	O
i	O
a	O
=	O
(	O
cid:21	O
)	O
e	O
(	O
l	O
)	O
t	O
,	O
namely	O
e	O
(	O
l	O
)	O
i	O
qs0js	O
=	O
e	O
(	O
l	O
)	O
s0	O
as0s	O
(	O
cid:21	O
)	O
e	O
(	O
l	O
)	O
s	O
:	O
(	O
17.19	O
)	O
[	O
hint	O
:	O
exercise	O
16.2	O
(	O
p.245	O
)	O
might	O
give	O
helpful	O
cross-fertilization	O
here	O
.	O
]	O
.	O
exercise	O
17.10	O
.	O
[	O
3	O
,	O
p.258	O
]	O
show	O
that	O
when	O
sequences	O
are	O
generated	O
using	O
the	O
op-	O
timal	O
transition	B
probability	I
matrix	O
(	O
17.19	O
)	O
,	O
the	O
entropy	B
of	O
the	O
resulting	O
sequence	B
is	O
asymptotically	O
log2	O
(	O
cid:21	O
)	O
per	O
symbol	O
.	O
[	O
hint	O
:	O
consider	O
the	O
condi-	O
tional	O
entropy	B
of	O
just	O
one	O
symbol	O
given	O
the	O
previous	O
one	O
,	O
assuming	O
the	O
previous	O
one	O
’	O
s	O
distribution	B
is	O
the	O
invariant	B
distribution	I
.	O
]	O
in	O
practice	O
,	O
we	O
would	O
probably	O
use	O
(	O
cid:12	O
)	O
nite-precision	O
approximations	O
to	O
the	O
optimal	B
variable-length	O
solution	O
.	O
one	O
might	O
dislike	O
variable-length	B
solutions	O
because	O
of	O
the	O
resulting	O
unpredictability	O
of	O
the	O
actual	O
encoded	O
length	B
in	O
any	O
particular	O
case	O
.	O
perhaps	O
in	O
some	O
applications	O
we	O
would	O
like	O
a	O
guarantee	O
that	O
the	O
encoded	O
length	B
of	O
a	O
source	O
(	O
cid:12	O
)	O
le	O
of	O
size	O
n	O
bits	O
will	O
be	O
less	O
than	O
a	O
given	O
length	B
such	O
as	O
n=	O
(	O
c	O
+	O
(	O
cid:15	O
)	O
)	O
.	O
for	O
example	O
,	O
a	O
disk	B
drive	I
is	O
easier	O
to	O
control	O
if	O
all	O
blocks	O
of	O
512	O
bytes	O
are	O
known	O
to	O
take	O
exactly	O
the	O
same	O
amount	O
of	O
disk	O
real-estate	O
.	O
for	O
some	O
constrained	B
channels	O
we	O
can	O
make	O
a	O
simple	O
modi	O
(	O
cid:12	O
)	O
cation	O
to	O
our	O
variable-length	B
encoding	O
and	O
o	O
(	O
cid:11	O
)	O
er	O
such	O
a	O
guarantee	O
,	O
as	O
follows	O
.	O
we	O
(	O
cid:12	O
)	O
nd	O
two	O
codes	O
,	O
two	O
mappings	O
of	O
binary	O
strings	O
to	O
variable-length	B
encodings	O
,	O
having	O
the	O
property	O
that	O
for	O
any	O
source	O
string	O
x	O
,	O
if	O
the	O
encoding	O
of	O
x	O
under	O
the	O
(	O
cid:12	O
)	O
rst	O
code	B
is	O
shorter	O
than	O
average	B
,	O
then	O
the	O
encoding	O
of	O
x	O
under	O
the	O
second	O
code	B
is	O
longer	O
than	O
average	B
,	O
and	O
vice	O
versa	O
.	O
then	O
to	O
transmit	O
a	O
string	O
x	O
we	O
encode	O
the	O
whole	O
string	O
with	O
both	O
codes	O
and	O
send	O
whichever	O
encoding	O
has	O
the	O
shortest	O
length	B
,	O
prepended	O
by	O
a	O
suitably	O
encoded	O
single	O
bit	O
to	O
convey	O
which	O
of	O
the	O
two	O
codes	O
is	O
being	O
used	O
.	O
.	O
exercise	O
17.11	O
.	O
[	O
3c	O
,	O
p.258	O
]	O
how	O
many	O
valid	O
sequences	O
of	O
length	O
8	O
starting	O
with	O
a	O
0	O
are	O
there	O
for	O
the	O
run-length-limited	O
channels	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
17.9	O
?	O
what	O
are	O
the	O
capacities	O
of	O
these	O
channels	O
?	O
using	O
a	O
computer	B
,	O
(	O
cid:12	O
)	O
nd	O
the	O
matrices	B
q	O
for	O
generating	O
a	O
random	B
path	O
through	O
the	O
trellises	O
of	O
the	O
channel	O
a	O
,	O
and	O
the	O
two	O
run-length-limited	O
channels	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
17.9	O
.	O
0	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
3	O
5	O
1	O
1	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
1	O
2	O
4	O
2	O
664	O
2	O
0	O
1	O
0	O
0	O
0	O
0	O
3	O
2	O
1	O
0	O
0	O
1	O
1	O
3	O
775	O
1	O
0	O
figure	O
17.9.	O
state	O
diagrams	O
and	O
connection	O
matrices	B
for	O
channels	O
with	O
maximum	O
runlengths	O
for	O
1s	O
equal	O
to	O
2	O
and	O
3.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
256	O
17	O
|	O
communication	B
over	O
constrained	B
noiseless	O
channels	O
.	O
exercise	O
17.12	O
.	O
[	O
3	O
,	O
p.258	O
]	O
consider	O
the	O
run-length-limited	O
channel	B
in	O
which	O
any	O
length	B
of	O
run	O
of	O
0s	O
is	O
permitted	O
,	O
and	O
the	O
maximum	O
run	O
length	B
of	O
1s	O
is	O
a	O
large	O
number	O
l	O
such	O
as	O
nine	O
or	O
ninety	O
.	O
estimate	O
the	O
capacity	B
of	O
this	O
channel	B
.	O
(	O
give	O
the	O
(	O
cid:12	O
)	O
rst	O
two	O
terms	O
in	O
a	O
series	O
expansion	O
involving	O
l.	O
)	O
what	O
,	O
roughly	O
,	O
is	O
the	O
form	O
of	O
the	O
optimal	O
matrix	B
q	O
for	O
generating	O
a	O
random	B
path	O
through	O
the	O
trellis	B
of	O
this	O
channel	B
?	O
focus	B
on	O
the	O
values	O
of	O
the	O
elements	O
q1j0	O
,	O
the	O
probability	O
of	O
generating	O
a	O
1	O
given	O
a	O
preceding	O
0	O
,	O
and	O
qljl	O
(	O
cid:0	O
)	O
1	O
,	O
the	O
probability	O
of	O
generating	O
a	O
1	O
given	O
a	O
preceding	O
run	O
of	O
l	O
(	O
cid:0	O
)	O
1	O
1s	O
.	O
check	O
your	O
answer	O
by	O
explicit	O
computation	O
for	O
the	O
channel	B
in	O
which	O
the	O
maximum	O
runlength	O
of	O
1s	O
is	O
nine	O
.	O
17.6	O
variable	B
symbol	I
durations	I
we	O
can	O
add	O
a	O
further	O
frill	O
to	O
the	O
task	O
of	O
communicating	O
over	O
constrained	O
channels	O
by	O
assuming	O
that	O
the	O
symbols	O
we	O
send	O
have	O
di	O
(	O
cid:11	O
)	O
erent	O
durations	O
,	O
and	O
that	O
our	O
aim	O
is	O
to	O
communicate	O
at	O
the	O
maximum	O
possible	O
rate	B
per	O
unit	O
time	O
.	O
such	O
channels	O
can	O
come	O
in	O
two	O
(	O
cid:13	O
)	O
avours	O
:	O
unconstrained	O
,	O
and	O
constrained	O
.	O
unconstrained	O
channels	O
with	O
variable	O
symbol	O
durations	O
we	O
encountered	O
an	O
unconstrained	O
noiseless	B
channel	O
with	O
variable	O
symbol	O
du-	O
rations	O
in	O
exercise	O
6.18	O
(	O
p.125	O
)	O
.	O
solve	O
that	O
problem	O
,	O
and	O
you	O
’	O
ve	O
done	O
this	O
topic	O
.	O
the	O
task	O
is	O
to	O
determine	O
the	O
optimal	B
frequencies	O
with	O
which	O
the	O
sym-	O
bols	O
should	O
be	O
used	O
,	O
given	O
their	O
durations	O
.	O
there	O
is	O
a	O
nice	O
analogy	O
between	O
this	O
task	O
and	O
the	O
task	O
of	O
designing	O
an	O
optimal	B
symbol	O
code	B
(	O
chapter	O
4	O
)	O
.	O
when	O
we	O
make	O
an	O
binary	O
symbol	O
code	B
for	O
a	O
source	O
with	O
unequal	O
probabilities	O
pi	O
,	O
the	O
optimal	B
message	O
lengths	O
are	O
l	O
(	O
cid:3	O
)	O
i	O
=	O
log2	O
1/pi	O
,	O
so	O
pi	O
=	O
2	O
(	O
cid:0	O
)	O
l	O
(	O
cid:3	O
)	O
i	O
:	O
(	O
17.20	O
)	O
similarly	O
,	O
when	O
we	O
have	O
a	O
channel	B
whose	O
symbols	O
have	O
durations	O
li	O
(	O
in	O
some	O
units	B
of	O
time	O
)	O
,	O
the	O
optimal	B
probability	O
with	O
which	O
those	O
symbols	O
should	O
be	O
used	O
is	O
p	O
(	O
cid:3	O
)	O
i	O
=	O
2	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
li	O
;	O
(	O
17.21	O
)	O
where	O
(	O
cid:12	O
)	O
is	O
the	O
capacity	B
of	O
the	O
channel	B
in	O
bits	O
per	O
unit	O
time	O
.	O
constrained	B
channels	O
with	O
variable	O
symbol	O
durations	O
once	O
you	O
have	O
grasped	O
the	O
preceding	O
topics	O
in	O
this	O
chapter	O
,	O
you	O
should	O
be	O
able	O
to	O
(	O
cid:12	O
)	O
gure	O
out	O
how	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
and	O
(	O
cid:12	O
)	O
nd	O
the	O
capacity	B
of	O
these	O
,	O
the	O
trickiest	O
constrained	B
channels	O
.	O
exercise	O
17.13	O
.	O
[	O
3	O
]	O
a	O
classic	O
example	O
of	O
a	O
constrained	B
channel	I
with	O
variable	B
symbol	I
durations	I
is	O
the	O
‘	O
morse	O
’	O
channel	B
,	O
whose	O
symbols	O
are	O
the	O
dot	O
the	O
dash	O
the	O
short	O
space	O
(	O
used	O
between	O
letters	O
in	O
morse	O
code	B
)	O
the	O
long	O
space	O
(	O
used	O
between	O
words	O
)	O
d	O
,	O
d	O
,	O
s	O
,	O
and	O
s	O
;	O
the	O
constraints	O
are	O
that	O
spaces	O
may	O
only	O
be	O
followed	O
by	O
dots	O
and	O
dashes	O
.	O
find	O
the	O
capacity	B
of	O
this	O
channel	B
in	O
bits	O
per	O
unit	O
time	O
assuming	O
(	O
a	O
)	O
that	O
all	O
four	O
symbols	O
have	O
equal	O
durations	O
;	O
or	O
(	O
b	O
)	O
that	O
the	O
symbol	O
durations	O
are	O
2	O
,	O
4	O
,	O
3	O
and	O
6	O
time	O
units	B
respectively	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
17.7	O
:	O
solutions	O
257	O
exercise	O
17.14	O
.	O
[	O
4	O
]	O
how	O
well-designed	O
is	O
morse	O
code	B
for	O
english	O
(	O
with	O
,	O
say	O
,	O
the	O
probability	B
distribution	O
of	O
(	O
cid:12	O
)	O
gure	O
2.1	O
)	O
?	O
exercise	O
17.15	O
.	O
[	O
3c	O
]	O
how	O
di	O
(	O
cid:14	O
)	O
cult	O
is	O
it	O
to	O
get	O
dna	O
into	O
a	O
narrow	O
tube	B
?	O
to	O
an	O
information	B
theorist	O
,	O
the	O
entropy	B
associated	O
with	O
a	O
constrained	B
channel	I
reveals	O
how	O
much	O
information	O
can	O
be	O
conveyed	O
over	O
it	O
.	O
in	O
sta-	O
tistical	O
physics	B
,	O
the	O
same	O
calculations	O
are	O
done	O
for	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
reason	O
:	O
to	O
predict	O
the	O
thermodynamics	B
of	O
polymers	O
,	O
for	O
example	O
.	O
as	O
a	O
toy	O
example	O
,	O
consider	O
a	O
polymer	B
of	O
length	B
n	O
that	O
can	O
either	O
sit	O
in	O
a	O
constraining	O
tube	B
,	O
of	O
width	O
l	O
,	O
or	O
in	O
the	O
open	O
where	O
there	O
are	O
no	O
constraints	O
.	O
in	O
the	O
open	O
,	O
the	O
polymer	B
adopts	O
a	O
state	O
drawn	O
at	O
random	B
from	O
the	O
set	B
of	O
one	O
dimensional	O
random	B
walks	O
,	O
with	O
,	O
say	O
,	O
3	O
possible	O
directions	O
per	O
step	O
.	O
the	O
entropy	B
of	O
this	O
walk	O
is	O
log	O
3	O
per	O
step	O
,	O
i.e.	O
,	O
a	O
[	O
the	O
free	B
energy	I
of	O
the	O
polymer	B
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
(	O
cid:0	O
)	O
kt	O
total	O
of	O
n	O
log	O
3.	O
times	O
this	O
,	O
where	O
t	O
is	O
the	O
temperature	B
.	O
]	O
in	O
the	O
tube	B
,	O
the	O
polymer	B
’	O
s	O
one-	O
dimensional	O
walk	O
can	O
go	O
in	O
3	O
directions	O
unless	O
the	O
wall	O
is	O
in	O
the	O
way	O
,	O
so	O
the	O
connection	B
matrix	I
is	O
,	O
for	O
example	O
(	O
if	O
l	O
=	O
10	O
)	O
,	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
:	O
3	O
777777777775	O
2	O
666666666664	O
now	O
,	O
what	O
is	O
the	O
entropy	B
of	O
the	O
polymer	B
?	O
what	O
is	O
the	O
change	O
in	O
entropy	O
associated	O
with	O
the	O
polymer	B
entering	O
the	O
tube	B
?	O
if	O
possible	O
,	O
obtain	O
an	O
expression	O
as	O
a	O
function	B
of	O
l.	O
use	O
a	O
computer	B
to	O
(	O
cid:12	O
)	O
nd	O
the	O
entropy	B
of	O
the	O
walk	O
for	O
a	O
particular	O
value	O
of	O
l	O
,	O
e.g	O
.	O
20	O
,	O
and	O
plot	O
the	O
probability	B
density	O
of	O
the	O
polymer	O
’	O
s	O
transverse	O
location	O
in	O
the	O
tube	B
.	O
notice	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
in	O
capacity	O
between	O
two	O
channels	O
,	O
one	O
constrained	B
and	O
one	O
unconstrained	O
,	O
is	O
directly	O
proportional	O
to	O
the	O
force	O
required	O
to	O
pull	O
the	O
dna	O
into	O
the	O
tube	B
.	O
17.7	O
solutions	O
solution	O
to	O
exercise	O
17.5	O
(	O
p.250	O
)	O
.	O
a	O
(	O
cid:12	O
)	O
le	O
transmitted	O
by	O
c2	O
contains	O
,	O
on	O
aver-	O
age	O
,	O
one-third	O
1s	O
and	O
two-thirds	O
0s	O
.	O
if	O
f	O
=	O
0:38	O
,	O
the	O
fraction	O
of	O
1s	O
is	O
f	O
=	O
(	O
1	O
+	O
f	O
)	O
=	O
(	O
(	O
cid:13	O
)	O
(	O
cid:0	O
)	O
1:0	O
)	O
=	O
(	O
2	O
(	O
cid:13	O
)	O
(	O
cid:0	O
)	O
1:0	O
)	O
=	O
0:2764.	O
solution	O
to	O
exercise	O
17.7	O
(	O
p.254	O
)	O
.	O
a	O
valid	O
string	O
for	O
channel	O
c	O
can	O
be	O
obtained	O
from	O
a	O
valid	O
string	O
for	O
channel	O
a	O
by	O
(	O
cid:12	O
)	O
rst	O
inverting	O
it	O
[	O
1	O
!	O
0	O
;	O
0	O
!	O
1	O
]	O
,	O
then	O
passing	O
it	O
through	O
an	O
accumulator	B
.	O
these	O
operations	O
are	O
invertible	O
,	O
so	O
any	O
valid	O
string	O
for	O
c	O
can	O
also	O
be	O
mapped	O
onto	O
a	O
valid	O
string	O
for	O
a.	O
the	O
only	O
proviso	O
here	O
comes	O
from	O
the	O
edge	B
e	O
(	O
cid:11	O
)	O
ects	O
.	O
if	O
we	O
assume	O
that	O
the	O
(	O
cid:12	O
)	O
rst	O
character	O
transmitted	O
over	O
channel	O
c	O
is	O
preceded	O
by	O
a	O
string	O
of	O
zeroes	O
,	O
so	O
that	O
the	O
(	O
cid:12	O
)	O
rst	O
character	O
is	O
forced	O
to	O
be	O
a	O
1	O
(	O
(	O
cid:12	O
)	O
gure	O
17.5c	O
)	O
then	O
the	O
two	O
channels	O
are	O
exactly	O
equivalent	O
only	O
if	O
we	O
assume	O
that	O
channel	B
a	O
’	O
s	O
(	O
cid:12	O
)	O
rst	O
character	O
must	O
be	O
a	O
zero	O
.	O
solution	O
to	O
exercise	O
17.8	O
(	O
p.254	O
)	O
.	O
with	O
n	O
=	O
16	O
transmitted	O
bits	O
,	O
the	O
largest	O
integer	O
number	O
of	O
source	O
bits	O
that	O
can	O
be	O
encoded	O
is	O
10	O
,	O
so	O
the	O
maximum	O
rate	O
of	O
a	O
(	O
cid:12	O
)	O
xed	O
length	B
code	O
with	O
n	O
=	O
16	O
is	O
0.625.	O
figure	O
17.10.	O
model	B
of	O
dna	O
squashed	O
in	O
a	O
narrow	O
tube	B
.	O
the	O
dna	O
will	O
have	O
a	O
tendency	O
to	O
pop	O
out	O
of	O
the	O
tube	O
,	O
because	O
,	O
outside	O
the	O
tube	B
,	O
its	O
random	B
walk	I
has	O
greater	O
entropy	B
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
258	O
17	O
|	O
communication	B
over	O
constrained	B
noiseless	O
channels	O
solution	O
to	O
exercise	O
17.10	O
(	O
p.255	O
)	O
.	O
let	O
the	O
invariant	B
distribution	I
be	O
p	O
(	O
s	O
)	O
=	O
(	O
cid:11	O
)	O
e	O
(	O
l	O
)	O
s	O
e	O
(	O
r	O
)	O
s	O
;	O
(	O
17.22	O
)	O
where	O
(	O
cid:11	O
)	O
is	O
a	O
normalization	O
constant	O
.	O
the	O
entropy	B
of	O
st	O
given	O
st	O
(	O
cid:0	O
)	O
1	O
,	O
assuming	O
st	O
(	O
cid:0	O
)	O
1	O
comes	O
from	O
the	O
invariant	B
distribution	I
,	O
is	O
h	O
(	O
stjst	O
(	O
cid:0	O
)	O
1	O
)	O
=	O
(	O
cid:0	O
)	O
xs	O
;	O
s0	O
=	O
(	O
cid:0	O
)	O
xs	O
;	O
s0	O
hlog	O
e	O
(	O
l	O
)	O
e	O
(	O
l	O
)	O
s0	O
as0s	O
(	O
cid:21	O
)	O
(	O
cid:11	O
)	O
e	O
(	O
r	O
)	O
s	O
=	O
(	O
cid:0	O
)	O
xs	O
;	O
s0	O
p	O
(	O
s	O
)	O
p	O
(	O
s0js	O
)	O
log	O
p	O
(	O
s0js	O
)	O
(	O
cid:11	O
)	O
e	O
(	O
l	O
)	O
s	O
e	O
(	O
r	O
)	O
s	O
e	O
(	O
l	O
)	O
s0	O
as0s	O
(	O
cid:21	O
)	O
e	O
(	O
l	O
)	O
s	O
log	O
e	O
(	O
l	O
)	O
s0	O
as0s	O
(	O
cid:21	O
)	O
e	O
(	O
l	O
)	O
s	O
s	O
i	O
:	O
s0	O
+	O
log	O
as0s	O
(	O
cid:0	O
)	O
log	O
(	O
cid:21	O
)	O
(	O
cid:0	O
)	O
log	O
e	O
(	O
l	O
)	O
(	O
17.23	O
)	O
(	O
17.24	O
)	O
(	O
17.25	O
)	O
now	O
,	O
as0s	O
is	O
either	O
0	O
or	O
1	O
,	O
so	O
the	O
contributions	O
from	O
the	O
terms	O
proportional	O
to	O
as0s	O
log	O
as0s	O
are	O
all	O
zero	O
.	O
so	O
here	O
,	O
as	O
in	O
chapter	O
4	O
,	O
st	O
denotes	O
the	O
ensemble	B
whose	O
random	B
variable	I
is	O
the	O
state	O
st.	O
h	O
(	O
stjst	O
(	O
cid:0	O
)	O
1	O
)	O
=	O
log	O
(	O
cid:21	O
)	O
+	O
(	O
cid:0	O
)	O
log	O
e	O
(	O
l	O
)	O
s0	O
+	O
as0se	O
(	O
r	O
)	O
s	O
!	O
e	O
(	O
l	O
)	O
s0	O
(	O
cid:11	O
)	O
(	O
cid:21	O
)	O
xs0	O
xs	O
s0	O
as0s	O
!	O
e	O
(	O
r	O
)	O
e	O
(	O
l	O
)	O
s	O
log	O
e	O
(	O
l	O
)	O
s	O
(	O
17.26	O
)	O
(	O
cid:11	O
)	O
(	O
cid:21	O
)	O
xs	O
xs0	O
(	O
cid:21	O
)	O
xs0	O
(	O
cid:21	O
)	O
e	O
(	O
r	O
)	O
s0	O
e	O
(	O
l	O
)	O
s0	O
(	O
cid:11	O
)	O
=	O
log	O
(	O
cid:21	O
)	O
(	O
cid:0	O
)	O
=	O
log	O
(	O
cid:21	O
)	O
:	O
log	O
e	O
(	O
l	O
)	O
s0	O
+	O
(	O
cid:11	O
)	O
(	O
cid:21	O
)	O
xs	O
(	O
cid:21	O
)	O
e	O
(	O
l	O
)	O
s	O
e	O
(	O
r	O
)	O
s	O
log	O
e	O
(	O
l	O
)	O
s	O
(	O
17.27	O
)	O
(	O
17.28	O
)	O
solution	O
to	O
exercise	O
17.11	O
(	O
p.255	O
)	O
.	O
the	O
principal	O
eigenvalues	O
of	O
the	O
connection	O
matrices	B
of	O
the	O
two	O
channels	O
are	O
1.839	O
and	O
1.928.	O
the	O
capacities	O
(	O
log	O
(	O
cid:21	O
)	O
)	O
are	O
0.879	O
and	O
0.947	O
bits	O
.	O
solution	O
to	O
exercise	O
17.12	O
(	O
p.256	O
)	O
.	O
the	O
channel	B
is	O
similar	O
to	O
the	O
unconstrained	O
binary	O
channel	O
;	O
runs	O
of	O
length	O
greater	O
than	O
l	O
are	O
rare	O
if	O
l	O
is	O
large	O
,	O
so	O
we	O
only	O
expect	O
weak	O
di	O
(	O
cid:11	O
)	O
erences	O
from	O
this	O
channel	B
;	O
these	O
di	O
(	O
cid:11	O
)	O
erences	O
will	O
show	O
up	O
in	O
contexts	O
where	O
the	O
run	O
length	B
is	O
close	O
to	O
l.	O
the	O
capacity	B
of	O
the	O
channel	B
is	O
very	O
close	O
to	O
one	O
bit	B
.	O
a	O
lower	O
bound	B
on	O
the	O
capacity	B
is	O
obtained	O
by	O
considering	O
the	O
simple	O
variable-length	O
code	B
for	O
this	O
channel	B
which	O
replaces	O
occurrences	O
of	O
the	O
maxi-	O
mum	O
runlength	B
string	O
111	O
:	O
:	O
:1	O
by	O
111	O
:	O
:	O
:10	O
,	O
and	O
otherwise	O
leaves	O
the	O
source	O
(	O
cid:12	O
)	O
le	O
unchanged	O
.	O
the	O
average	B
rate	O
of	O
this	O
code	B
is	O
1=	O
(	O
1	O
+	O
2	O
(	O
cid:0	O
)	O
l	O
)	O
because	O
the	O
invariant	B
distribution	I
will	O
hit	O
the	O
‘	O
add	O
an	O
extra	O
zero	O
’	O
state	O
a	O
fraction	O
2	O
(	O
cid:0	O
)	O
l	O
of	O
the	O
time	O
.	O
we	O
can	O
reuse	O
the	O
solution	O
for	O
the	O
variable-length	B
channel	O
in	O
exercise	O
6.18	O
(	O
p.125	O
)	O
.	O
the	O
capacity	B
is	O
the	O
value	O
of	O
(	O
cid:12	O
)	O
such	O
that	O
the	O
equation	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
=	O
2	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
l	O
=	O
1	O
l+1	O
xl=1	O
(	O
17.29	O
)	O
is	O
satis	O
(	O
cid:12	O
)	O
ed	O
.	O
the	O
l+1	O
terms	O
in	O
the	O
sum	O
correspond	O
to	O
the	O
l+1	O
possible	O
strings	O
that	O
can	O
be	O
emitted	O
,	O
0	O
,	O
10	O
,	O
110	O
,	O
:	O
:	O
:	O
,	O
11	O
:	O
:	O
:10.	O
the	O
sum	O
is	O
exactly	O
given	O
by	O
:	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
=	O
2	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
(	O
cid:0	O
)	O
2	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
(	O
cid:1	O
)	O
l+1	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
(	O
cid:0	O
)	O
1	O
:	O
(	O
17.30	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
17.7	O
:	O
solutions	O
259	O
l	O
2	O
3	O
4	O
5	O
6	O
9	O
(	O
cid:12	O
)	O
true	O
capacity	B
0.910	O
0.955	O
0.977	O
0.9887	O
0.9944	O
0.9993	O
0.879	O
0.947	O
0.975	O
0.9881	O
0.9942	O
0.9993	O
n	O
arn	O
=	O
''	O
here	O
we	O
used	O
we	O
anticipate	O
that	O
(	O
cid:12	O
)	O
should	O
be	O
a	O
little	O
less	O
than	O
1	O
in	O
order	O
for	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
to	O
equal	O
1.	O
rearranging	O
and	O
solving	O
approximately	O
for	O
(	O
cid:12	O
)	O
,	O
using	O
ln	O
(	O
1	O
+	O
x	O
)	O
’	O
x	O
,	O
a	O
(	O
rn	O
+1	O
(	O
cid:0	O
)	O
1	O
)	O
xn=0	O
r	O
(	O
cid:0	O
)	O
1	O
:	O
#	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
=	O
1	O
)	O
(	O
cid:12	O
)	O
’	O
1	O
(	O
cid:0	O
)	O
2	O
(	O
cid:0	O
)	O
(	O
l+2	O
)	O
=	O
ln	O
2	O
:	O
(	O
17.31	O
)	O
(	O
17.32	O
)	O
we	O
evaluated	O
the	O
true	O
capacities	O
for	O
l	O
=	O
2	O
and	O
l	O
=	O
3	O
in	O
an	O
earlier	O
exercise	O
.	O
the	O
table	O
compares	O
the	O
approximate	O
capacity	B
(	O
cid:12	O
)	O
with	O
the	O
true	O
capacity	B
for	O
a	O
selection	O
of	O
values	O
of	O
l.	O
the	O
element	O
q1j0	O
will	O
be	O
close	O
to	O
1=2	O
(	O
just	O
a	O
tiny	O
bit	B
larger	O
)	O
,	O
since	O
in	O
the	O
unconstrained	O
binary	O
channel	O
q1j0	O
=	O
1=2	O
.	O
when	O
a	O
run	O
of	O
length	O
l	O
(	O
cid:0	O
)	O
1	O
has	O
occurred	O
,	O
we	O
e	O
(	O
cid:11	O
)	O
ectively	O
have	O
a	O
choice	O
of	O
printing	O
10	O
or	O
0.	O
let	O
the	O
probability	O
of	O
selecting	O
10	O
be	O
f	O
.	O
let	O
us	O
estimate	O
the	O
entropy	B
of	O
the	O
remaining	O
n	O
characters	O
in	O
the	O
stream	O
as	O
a	O
function	B
of	O
f	O
,	O
assuming	O
the	O
rest	O
of	O
the	O
matrix	O
q	O
to	O
have	O
been	O
set	B
to	O
its	O
optimal	B
value	O
.	O
the	O
entropy	B
of	O
the	O
next	O
n	O
characters	O
in	O
the	O
stream	O
is	O
the	O
entropy	B
of	O
the	O
(	O
cid:12	O
)	O
rst	O
bit	B
,	O
h2	O
(	O
f	O
)	O
,	O
plus	O
the	O
entropy	B
of	O
the	O
remaining	O
characters	O
,	O
which	O
is	O
roughly	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
bits	O
if	O
we	O
select	O
0	O
as	O
the	O
(	O
cid:12	O
)	O
rst	O
bit	B
and	O
(	O
n	O
(	O
cid:0	O
)	O
2	O
)	O
bits	O
if	O
1	O
is	O
selected	O
.	O
more	O
precisely	O
,	O
if	O
c	O
is	O
the	O
capacity	B
of	O
the	O
channel	B
(	O
which	O
is	O
roughly	O
1	O
)	O
,	O
h	O
(	O
the	O
next	O
n	O
chars	O
)	O
’	O
h2	O
(	O
f	O
)	O
+	O
[	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
+	O
(	O
n	O
(	O
cid:0	O
)	O
2	O
)	O
f	O
]	O
c	O
=	O
h2	O
(	O
f	O
)	O
+	O
n	O
c	O
(	O
cid:0	O
)	O
f	O
c	O
’	O
h2	O
(	O
f	O
)	O
+	O
n	O
(	O
cid:0	O
)	O
f	O
:	O
(	O
17.33	O
)	O
di	O
(	O
cid:11	O
)	O
erentiating	O
and	O
setting	O
to	O
zero	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
optimal	B
f	O
,	O
we	O
obtain	O
:	O
log2	O
1	O
(	O
cid:0	O
)	O
f	O
f	O
’	O
1	O
)	O
1	O
(	O
cid:0	O
)	O
f	O
f	O
’	O
2	O
)	O
f	O
’	O
1=3	O
:	O
(	O
17.34	O
)	O
the	O
probability	O
of	O
emitting	O
a	O
1	O
thus	O
decreases	O
from	O
about	O
0.5	O
to	O
about	O
1=3	O
as	O
the	O
number	O
of	O
emitted	O
1s	O
increases	O
.	O
here	O
is	O
the	O
optimal	B
matrix	O
:	O
0	O
0	O
:4669	O
0	O
0	O
0	O
0	O
:4287	O
0	O
:3334	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
:4998	O
1	O
:6666	O
:5713	O
:5331	O
:5159	O
:5077	O
:5037	O
:5017	O
:5007	O
:5002	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
:4993	O
0	O
:4923	O
:4841	O
:4963	O
:4983	O
2	O
666666666666664	O
:	O
(	O
17.35	O
)	O
3	O
777777777777775	O
our	O
rough	O
theory	B
works	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
18	O
crosswords	O
and	O
codebreaking	O
in	O
this	O
chapter	O
we	O
make	O
a	O
random	B
walk	I
through	O
a	O
few	O
topics	O
related	O
to	O
lan-	O
guage	O
modelling	B
.	O
18.1	O
crosswords	O
the	O
rules	B
of	O
crossword-making	O
may	O
be	O
thought	O
of	O
as	O
de	O
(	O
cid:12	O
)	O
ning	O
a	O
constrained	B
channel	I
.	O
the	O
fact	O
that	O
many	O
valid	O
crosswords	O
can	O
be	O
made	O
demonstrates	O
that	O
this	O
constrained	B
channel	I
has	O
a	O
capacity	B
greater	O
than	O
zero	O
.	O
there	O
are	O
two	O
archetypal	O
crossword	B
formats	O
.	O
in	O
a	O
‘	O
type	O
a	O
’	O
(	O
or	O
american	O
)	O
crossword	B
,	O
every	O
row	O
and	O
column	O
consists	O
of	O
a	O
succession	O
of	O
words	O
of	O
length	O
2	O
or	O
more	O
separated	O
by	O
one	O
or	O
more	O
spaces	O
.	O
in	O
a	O
‘	O
type	O
b	O
’	O
(	O
or	O
british	O
)	O
crossword	B
,	O
each	O
row	O
and	O
column	O
consists	O
of	O
a	O
mixture	O
of	O
words	O
and	O
single	O
characters	O
,	O
separated	O
by	O
one	O
or	O
more	O
spaces	O
,	O
and	O
every	O
character	O
lies	O
in	O
at	O
least	O
one	O
word	O
(	O
horizontal	O
or	O
vertical	O
)	O
.	O
whereas	O
in	O
a	O
type	O
a	O
crossword	B
every	O
letter	O
lies	O
in	O
a	O
horizontal	O
word	O
and	O
a	O
vertical	O
word	O
,	O
in	O
a	O
typical	B
type	O
b	O
crossword	B
only	O
about	O
half	O
of	O
the	O
letters	O
do	O
so	O
;	O
the	O
other	O
half	O
lie	O
in	O
one	O
word	O
only	O
.	O
type	O
a	O
crosswords	O
are	O
harder	O
to	O
create	O
than	O
type	O
b	O
because	O
of	O
the	O
con-	O
straint	O
that	O
no	O
single	O
characters	O
are	O
permitted	O
.	O
type	O
b	O
crosswords	O
are	O
gener-	O
ally	O
harder	O
to	O
solve	O
because	O
there	O
are	O
fewer	O
constraints	O
per	O
character	O
.	O
why	O
are	O
crosswords	O
possible	O
?	O
if	O
a	O
language	O
has	O
no	O
redundancy	B
,	O
then	O
any	O
letters	O
written	O
on	O
a	O
grid	O
form	O
a	O
valid	O
crossword	B
.	O
in	O
a	O
language	O
with	O
high	O
redundancy	O
,	O
on	O
the	O
other	O
hand	O
,	O
it	O
is	O
hard	O
to	O
make	O
crosswords	O
(	O
except	O
perhaps	O
a	O
small	O
number	O
of	O
trivial	O
ones	O
)	O
.	O
the	O
possibility	O
of	O
making	O
crosswords	O
in	O
a	O
language	O
thus	O
demonstrates	O
a	O
bound	B
on	O
the	O
redundancy	B
of	O
that	O
language	O
.	O
crosswords	O
are	O
not	O
normally	O
written	O
in	O
genuine	O
english	O
.	O
they	O
are	O
written	O
in	O
‘	O
word-english	O
’	O
,	O
the	O
language	O
consisting	O
of	O
strings	O
of	O
words	O
from	O
a	O
dictionary	B
,	O
separated	O
by	O
spaces	O
.	O
d	O
u	O
f	O
f	O
s	O
t	O
u	O
d	O
g	O
i	O
l	O
d	O
s	O
b	O
p	O
v	O
j	O
d	O
p	O
b	O
a	O
f	O
a	O
r	O
t	O
i	O
t	O
o	O
a	O
d	O
i	O
e	O
u	O
a	O
v	O
a	O
l	O
a	O
n	O
c	O
h	O
e	O
u	O
s	O
h	O
e	O
r	O
t	O
o	O
t	O
o	O
o	O
l	O
a	O
v	O
r	O
i	O
d	O
e	O
r	O
n	O
r	O
l	O
a	O
n	O
e	O
i	O
i	O
a	O
s	O
h	O
m	O
o	O
t	O
h	O
e	O
r	O
g	O
o	O
o	O
s	O
e	O
g	O
a	O
l	O
l	O
e	O
o	O
n	O
n	O
e	O
t	O
t	O
l	O
e	O
s	O
e	O
v	O
i	O
l	O
s	O
c	O
u	O
l	O
t	O
e	O
i	O
n	O
o	O
i	O
w	O
t	O
s	O
t	O
r	O
e	O
s	O
s	O
s	O
o	O
l	O
e	O
b	O
a	O
s	O
r	O
o	O
a	O
s	O
t	O
b	O
e	O
e	O
f	O
n	O
o	O
b	O
e	O
l	O
c	O
i	O
t	O
e	O
s	O
u	O
t	O
t	O
e	O
r	O
r	O
o	O
t	O
m	O
i	O
e	O
u	O
a	O
e	O
h	O
e	O
i	O
r	O
s	O
n	O
e	O
e	O
r	O
c	O
o	O
r	O
e	O
b	O
r	O
e	O
m	O
n	O
e	O
r	O
r	O
o	O
t	O
a	O
t	O
e	O
s	O
m	O
u	O
m	O
a	O
t	O
l	O
a	O
s	O
m	O
a	O
t	O
t	O
e	O
a	O
n	O
e	O
h	O
c	O
t	O
o	O
p	O
e	O
p	O
a	O
u	O
l	O
m	O
i	O
s	O
h	O
a	O
p	O
k	O
i	O
t	O
e	O
s	O
a	O
u	O
s	O
t	O
r	O
a	O
l	O
i	O
a	O
e	O
p	O
i	O
c	O
c	O
a	O
r	O
t	O
e	O
e	O
l	O
p	O
t	O
a	O
e	O
u	O
s	O
i	O
s	O
t	O
e	O
r	O
k	O
e	O
n	O
n	O
y	O
r	O
a	O
h	O
r	O
o	O
c	O
k	O
e	O
t	O
s	O
e	O
x	O
c	O
u	O
s	O
e	O
s	O
a	O
l	O
o	O
h	O
a	O
i	O
r	O
o	O
n	O
t	O
r	O
e	O
e	O
i	O
a	O
t	O
o	O
p	O
k	O
t	O
t	O
s	O
i	O
r	O
e	O
s	O
l	O
a	O
t	O
e	O
e	O
a	O
r	O
l	O
e	O
l	O
t	O
o	O
n	O
d	O
e	O
s	O
p	O
e	O
r	O
a	O
t	O
e	O
s	O
a	O
b	O
r	O
e	O
y	O
s	O
e	O
r	O
a	O
t	O
o	O
m	O
s	O
s	O
a	O
y	O
r	O
r	O
n	O
.	O
exercise	O
18.1	O
.	O
[	O
2	O
]	O
estimate	O
the	O
capacity	B
of	O
word-english	O
,	O
in	O
bits	O
per	O
character	O
.	O
[	O
hint	O
:	O
think	O
of	O
word-english	O
as	O
de	O
(	O
cid:12	O
)	O
ning	O
a	O
constrained	B
channel	I
(	O
chapter	O
17	O
)	O
and	O
see	O
exercise	O
6.18	O
(	O
p.125	O
)	O
.	O
]	O
figure	O
18.1.	O
crosswords	O
of	O
types	O
a	O
(	O
american	O
)	O
and	O
b	O
(	O
british	O
)	O
.	O
the	O
fact	O
that	O
many	O
crosswords	O
can	O
be	O
made	O
leads	O
to	O
a	O
lower	O
bound	B
on	O
the	O
entropy	B
of	O
word-english	O
.	O
for	O
simplicity	O
,	O
we	O
now	O
model	B
word-english	O
by	O
wenglish	O
,	O
the	O
language	O
in-	O
troduced	O
in	O
section	O
4.1	O
which	O
consists	O
of	O
w	O
words	O
all	O
of	O
length	O
l.	O
the	O
entropy	B
of	O
such	O
a	O
language	O
,	O
per	O
character	O
,	O
including	O
inter-word	O
spaces	O
,	O
is	O
:	O
hw	O
(	O
cid:17	O
)	O
log2	O
w	O
l	O
+	O
1	O
:	O
260	O
(	O
18.1	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
18.1	O
:	O
crosswords	O
261	O
we	O
’	O
ll	O
(	O
cid:12	O
)	O
nd	O
that	O
the	O
conclusions	O
we	O
come	O
to	O
depend	O
on	O
the	O
value	O
of	O
hw	O
and	O
are	O
not	O
terribly	O
sensitive	O
to	O
the	O
value	O
of	O
l.	O
consider	O
a	O
large	O
crossword	B
of	O
size	O
s	O
squares	O
in	O
area	O
.	O
let	O
the	O
number	O
of	O
words	O
be	O
fws	O
and	O
let	O
the	O
number	O
of	O
letter-occupied	O
squares	O
be	O
f1s	O
.	O
for	O
typical	O
crosswords	O
of	O
types	O
a	O
and	O
b	O
made	O
of	O
words	O
of	O
length	O
l	O
,	O
the	O
two	O
fractions	O
fw	O
and	O
f1	O
have	O
roughly	O
the	O
values	O
in	O
table	O
18.2.	O
we	O
now	O
estimate	O
how	O
many	O
crosswords	O
there	O
are	O
of	O
size	O
s	O
using	O
our	O
simple	O
model	O
of	O
wenglish	O
.	O
we	O
assume	O
that	O
wenglish	O
is	O
created	O
at	O
random	B
by	O
gener-	O
ating	O
w	O
strings	O
from	O
a	O
monogram	O
(	O
i.e.	O
,	O
memoryless	O
)	O
source	O
with	O
entropy	B
h0	O
.	O
if	O
,	O
for	O
example	O
,	O
the	O
source	O
used	O
all	O
a	O
=	O
26	O
characters	O
with	O
equal	O
probability	B
then	O
h0	O
=	O
log2	O
a	O
=	O
4:7	O
bits	O
.	O
if	O
instead	O
we	O
use	O
chapter	O
2	O
’	O
s	O
distribution	B
then	O
the	O
entropy	B
is	O
4.2.	O
the	O
redundancy	B
of	O
wenglish	O
stems	O
from	O
two	O
sources	O
:	O
it	O
tends	O
to	O
use	O
some	O
letters	O
more	O
than	O
others	O
;	O
and	O
there	O
are	O
only	O
w	O
words	O
in	O
the	O
dictionary	B
.	O
let	O
’	O
s	O
now	O
count	O
how	O
many	O
crosswords	O
there	O
are	O
by	O
imagining	O
(	O
cid:12	O
)	O
lling	O
in	O
the	O
squares	O
of	O
a	O
crossword	B
at	O
random	B
using	O
the	O
same	O
distribution	B
that	O
pro-	O
duced	O
the	O
wenglish	O
dictionary	B
and	O
evaluating	O
the	O
probability	B
that	O
this	O
random	B
scribbling	O
produces	O
valid	O
words	O
in	O
all	O
rows	O
and	O
columns	O
.	O
the	O
total	O
number	O
of	O
typical	O
(	O
cid:12	O
)	O
llings-in	O
of	O
the	O
f1s	O
squares	O
in	O
the	O
crossword	B
that	O
can	O
be	O
made	O
is	O
the	O
probability	B
that	O
one	O
word	O
of	O
length	O
l	O
is	O
validly	O
(	O
cid:12	O
)	O
lled-in	O
is	O
jtj	O
=	O
2f1sh0	O
:	O
(	O
cid:12	O
)	O
=	O
w	O
2lh0	O
;	O
(	O
18.2	O
)	O
(	O
18.3	O
)	O
and	O
the	O
probability	B
that	O
the	O
whole	O
crossword	B
,	O
made	O
of	O
fws	O
words	O
,	O
is	O
validly	O
(	O
cid:12	O
)	O
lled-in	O
by	O
a	O
single	O
typical	O
in-	O
(	O
cid:12	O
)	O
lling	O
is	O
approximately	O
(	O
cid:12	O
)	O
fws	O
:	O
(	O
18.4	O
)	O
so	O
the	O
log	O
of	O
the	O
number	O
of	O
valid	O
crosswords	O
of	O
size	O
s	O
is	O
estimated	O
to	O
be	O
log	O
(	O
cid:12	O
)	O
fwsjtj	O
=	O
s	O
[	O
(	O
f1	O
(	O
cid:0	O
)	O
fwl	O
)	O
h0	O
+	O
fw	O
log	O
w	O
]	O
=	O
s	O
[	O
(	O
f1	O
(	O
cid:0	O
)	O
fwl	O
)	O
h0	O
+	O
fw	O
(	O
l	O
+	O
1	O
)	O
hw	O
]	O
;	O
which	O
is	O
an	O
increasing	O
function	B
of	O
s	O
only	O
if	O
(	O
f1	O
(	O
cid:0	O
)	O
fwl	O
)	O
h0	O
+	O
fw	O
(	O
l	O
+	O
1	O
)	O
hw	O
>	O
0	O
:	O
(	O
18.5	O
)	O
(	O
18.6	O
)	O
(	O
18.7	O
)	O
so	O
arbitrarily	O
many	O
crosswords	O
can	O
be	O
made	O
only	O
if	O
there	O
’	O
s	O
enough	O
words	O
in	O
the	O
wenglish	O
dictionary	B
that	O
hw	O
>	O
(	O
fwl	O
(	O
cid:0	O
)	O
f1	O
)	O
fw	O
(	O
l	O
+	O
1	O
)	O
h0	O
:	O
(	O
18.8	O
)	O
plugging	O
in	O
the	O
values	O
of	O
f1	O
and	O
fw	O
from	O
table	O
18.2	O
,	O
we	O
(	O
cid:12	O
)	O
nd	O
the	O
following	O
.	O
a	O
2	O
b	O
1	O
l	O
+	O
1	O
l	O
l	O
+	O
1	O
l	O
+	O
1	O
3	O
l	O
4	O
l	O
+	O
1	O
fw	O
f1	O
table	O
18.2.	O
factors	O
fw	O
and	O
f1	O
by	O
which	O
the	O
number	O
of	O
words	O
and	O
number	O
of	O
letter-squares	O
respectively	O
are	O
smaller	O
than	O
the	O
total	O
number	O
of	O
squares	O
.	O
this	O
calculation	O
underestimates	O
the	O
number	O
of	O
valid	O
wenglish	O
crosswords	O
by	O
counting	O
only	O
crosswords	O
(	O
cid:12	O
)	O
lled	O
with	O
‘	O
typical	B
’	O
strings	O
.	O
if	O
the	O
monogram	O
distribution	B
is	O
non-uniform	O
then	O
the	O
true	O
count	O
is	O
dominated	O
by	O
‘	O
atypical	O
’	O
(	O
cid:12	O
)	O
llings-in	O
,	O
in	O
which	O
crossword-friendly	O
words	O
appear	O
more	O
often	O
.	O
crossword	B
type	O
a	O
condition	O
for	O
crosswords	O
hw	O
>	O
1	O
2	O
b	O
l+1	O
h0	O
hw	O
>	O
1	O
l	O
4	O
l	O
l+1	O
h0	O
if	O
we	O
set	B
h0	O
=	O
4:2	O
bits	O
and	O
assume	O
there	O
are	O
w	O
=	O
4000	O
words	O
in	O
a	O
normal	B
english-speaker	O
’	O
s	O
dictionary	B
,	O
all	O
with	O
length	O
l	O
=	O
5	O
,	O
then	O
we	O
(	O
cid:12	O
)	O
nd	O
that	O
the	O
condition	O
for	O
crosswords	O
of	O
type	O
b	O
is	O
satis	O
(	O
cid:12	O
)	O
ed	O
,	O
but	O
the	O
condition	O
for	O
crosswords	O
of	O
type	O
a	O
is	O
only	O
just	O
satis	O
(	O
cid:12	O
)	O
ed	O
.	O
this	O
(	O
cid:12	O
)	O
ts	O
with	O
my	O
experience	O
that	O
crosswords	O
of	O
type	O
a	O
usually	O
contain	O
more	O
obscure	O
words	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
262	O
further	O
reading	O
18	O
|	O
crosswords	O
and	O
codebreaking	O
figure	O
18.3.	O
a	O
binary	O
pattern	O
in	O
which	O
every	O
pixel	O
is	O
adjacent	O
to	O
four	O
black	B
and	O
four	O
white	B
pixels	O
.	O
these	O
observations	O
about	O
crosswords	O
were	O
(	O
cid:12	O
)	O
rst	O
made	O
by	O
shannon	O
(	O
1948	O
)	O
;	O
i	O
learned	O
about	O
them	O
from	O
wolf	O
and	O
siegel	O
(	O
1998	O
)	O
.	O
the	O
topic	O
is	O
closely	O
related	O
to	O
the	O
capacity	B
of	O
two-dimensional	B
constrained	O
channels	O
.	O
an	O
example	O
of	O
a	O
two-dimensional	B
constrained	O
channel	B
is	O
a	O
two-dimensional	B
bar-code	O
,	O
as	O
seen	O
on	O
parcels	O
.	O
exercise	O
18.2	O
.	O
[	O
3	O
]	O
a	O
two-dimensional	B
channel	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
the	O
constraint	O
that	O
,	O
of	O
the	O
eight	O
neighbours	O
of	O
every	O
interior	O
pixel	O
in	O
an	O
n	O
(	O
cid:2	O
)	O
n	O
rectangular	B
grid	O
,	O
four	O
must	O
be	O
black	B
and	O
four	O
white	B
.	O
(	O
the	O
counts	O
of	O
black	O
and	O
white	O
pixels	O
around	O
boundary	O
pixels	O
are	O
not	O
constrained	B
.	O
)	O
a	O
binary	O
pattern	O
satisfying	O
this	O
constraint	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
18.3.	O
what	O
is	O
the	O
capacity	B
of	O
this	O
channel	B
,	O
in	O
bits	O
per	O
pixel	O
,	O
for	O
large	O
n	O
?	O
18.2	O
simple	O
language	O
models	O
the	O
zipf	O
{	O
mandelbrot	O
distribution	B
the	O
crudest	O
model	B
for	O
a	O
language	O
is	O
the	O
monogram	O
model	B
,	O
which	O
asserts	O
that	O
each	O
successive	O
word	O
is	O
drawn	O
independently	O
from	O
a	O
distribution	B
over	O
words	O
.	O
what	O
is	O
the	O
nature	O
of	O
this	O
distribution	B
over	O
words	O
?	O
zipf	O
’	O
s	O
law	O
(	O
zipf	O
,	O
1949	O
)	O
asserts	O
that	O
the	O
probability	O
of	O
the	O
rth	O
most	O
probable	O
word	O
in	O
a	O
language	O
is	O
approximately	O
p	O
(	O
r	O
)	O
=	O
(	O
cid:20	O
)	O
r	O
(	O
cid:11	O
)	O
;	O
(	O
18.9	O
)	O
where	O
the	O
exponent	O
(	O
cid:11	O
)	O
has	O
a	O
value	O
close	O
to	O
1	O
,	O
and	O
(	O
cid:20	O
)	O
is	O
a	O
constant	O
.	O
according	O
to	O
zipf	O
,	O
a	O
log	O
{	O
log	O
plot	O
of	O
frequency	O
versus	O
word-rank	O
should	O
show	O
a	O
straight	O
line	O
with	O
slope	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
.	O
eter	O
v	O
,	O
asserting	O
that	O
the	O
probabilities	O
are	O
given	O
by	O
mandelbrot	O
’	O
s	O
(	O
1982	O
)	O
modi	O
(	O
cid:12	O
)	O
cation	O
of	O
zipf	O
’	O
s	O
law	O
introduces	O
a	O
third	O
param-	O
p	O
(	O
r	O
)	O
=	O
(	O
cid:20	O
)	O
(	O
r	O
+	O
v	O
)	O
(	O
cid:11	O
)	O
:	O
(	O
18.10	O
)	O
for	O
some	O
documents	O
,	O
such	O
as	O
jane	O
austen	O
’	O
s	O
emma	O
,	O
the	O
zipf	O
{	O
mandelbrot	O
dis-	O
tribution	O
(	O
cid:12	O
)	O
ts	O
well	O
{	O
(	O
cid:12	O
)	O
gure	O
18.4.	O
other	O
documents	O
give	O
distributions	O
that	O
are	O
not	O
so	O
well	O
(	O
cid:12	O
)	O
tted	O
by	O
a	O
zipf	O
{	O
mandelbrot	O
distribution	B
.	O
figure	O
18.5	O
shows	O
a	O
plot	O
of	O
frequency	O
versus	O
rank	O
for	O
the	O
latex	O
source	O
of	O
this	O
book	O
.	O
qualitatively	O
,	O
the	O
graph	B
is	O
similar	O
to	O
a	O
straight	O
line	O
,	O
but	O
a	O
curve	O
is	O
noticeable	O
.	O
to	O
be	O
fair	O
,	O
this	O
source	O
(	O
cid:12	O
)	O
le	O
is	O
not	O
written	O
in	O
pure	O
english	O
{	O
it	O
is	O
a	O
mix	O
of	O
english	O
,	O
maths	O
symbols	O
such	O
as	O
‘	O
x	O
’	O
,	O
and	O
latex	O
commands	O
.	O
0.1	O
0.01	O
to	O
theand	O
of	O
i	O
is	O
harriet	O
0.001	O
0.0001	O
1e-05	O
1	O
information	B
probability	O
10	O
100	O
1000	O
10000	O
figure	O
18.4.	O
fit	O
of	O
the	O
zipf	O
{	O
mandelbrot	O
distribution	B
(	O
18.10	O
)	O
(	O
curve	O
)	O
to	O
the	O
empirical	O
frequencies	O
of	O
words	O
in	O
jane	O
austen	O
’	O
s	O
emma	O
(	O
dots	O
)	O
.	O
the	O
(	O
cid:12	O
)	O
tted	O
parameters	B
are	O
(	O
cid:20	O
)	O
=	O
0:56	O
;	O
v	O
=	O
8:0	O
;	O
(	O
cid:11	O
)	O
=	O
1:26	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
18.2	O
:	O
simple	O
language	O
models	O
263	O
0.1	O
0.01	O
the	O
of	O
a	O
is	O
x	O
0.001	O
0.0001	O
0.00001	O
0.1	O
0.01	O
0.001	O
0.0001	O
0.00001	O
probability	B
information	O
shannon	O
bayes	O
1	O
10	O
100	O
1000	O
alpha=1	O
alpha=10	O
alpha=100	O
alpha=1000	O
book	O
1	O
10	O
100	O
1000	O
10000	O
figure	O
18.5.	O
log	O
{	O
log	O
plot	O
of	O
frequency	O
versus	O
rank	O
for	O
the	O
words	O
in	O
the	O
latex	O
(	O
cid:12	O
)	O
le	O
of	O
this	O
book	O
.	O
figure	O
18.6.	O
zipf	O
plots	O
for	O
four	O
‘	O
languages	O
’	O
randomly	O
generated	O
from	O
dirichlet	O
processes	O
with	O
parameter	O
(	O
cid:11	O
)	O
ranging	O
from	O
1	O
to	O
1000.	O
also	O
shown	O
is	O
the	O
zipf	O
plot	O
for	O
this	O
book	O
.	O
the	O
dirichlet	O
process	O
assuming	O
we	O
are	O
interested	O
in	O
monogram	O
models	O
for	O
languages	O
,	O
what	O
model	O
should	O
we	O
use	O
?	O
one	O
di	O
(	O
cid:14	O
)	O
culty	O
in	O
modelling	O
a	O
language	O
is	O
the	O
unboundedness	O
of	O
vocabulary	O
.	O
the	O
greater	O
the	O
sample	B
of	O
language	O
,	O
the	O
greater	O
the	O
number	O
of	O
words	O
encountered	O
.	O
a	O
generative	B
model	I
for	O
a	O
language	O
should	O
emulate	O
this	O
property	O
.	O
if	O
asked	O
‘	O
what	O
is	O
the	O
next	O
word	O
in	O
a	O
newly-discovered	O
work	O
of	O
shakespeare	O
?	O
’	O
our	O
probability	B
distribution	O
over	O
words	O
must	O
surely	O
include	O
some	O
non-zero	O
probability	B
for	O
words	O
that	O
shakespeare	O
never	O
used	O
before	O
.	O
our	O
generative	O
monogram	O
model	B
for	O
language	O
should	O
also	O
satisfy	O
a	O
consistency	O
rule	O
called	O
exchangeability	B
.	O
if	O
we	O
imagine	O
generating	O
a	O
new	O
language	O
from	O
our	O
generative	B
model	I
,	O
producing	O
an	O
ever-growing	O
corpus	O
of	O
text	O
,	O
all	O
statistical	B
properties	O
of	O
the	O
text	O
should	O
be	O
homogeneous	B
:	O
the	O
probability	O
of	O
(	O
cid:12	O
)	O
nding	O
a	O
particular	O
word	O
at	O
a	O
given	O
location	O
in	O
the	O
stream	O
of	O
text	O
should	O
be	O
the	O
same	O
everywhere	O
in	O
the	O
stream	O
.	O
the	O
dirichlet	O
process	O
model	B
is	O
a	O
model	B
for	O
a	O
stream	O
of	O
symbols	O
(	O
which	O
we	O
think	O
of	O
as	O
‘	O
words	O
’	O
)	O
that	O
satis	O
(	O
cid:12	O
)	O
es	O
the	O
exchangeability	B
rule	O
and	O
that	O
allows	O
the	O
vocabulary	O
of	O
symbols	O
to	O
grow	O
without	O
limit	O
.	O
the	O
model	B
has	O
one	O
parameter	O
(	O
cid:11	O
)	O
.	O
as	O
the	O
stream	O
of	O
symbols	O
is	O
produced	O
,	O
we	O
identify	O
each	O
new	O
symbol	O
by	O
a	O
unique	O
integer	O
w.	O
when	O
we	O
have	O
seen	O
a	O
stream	O
of	O
length	B
f	O
symbols	O
,	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
probability	O
of	O
the	O
next	O
symbol	O
in	O
terms	O
of	O
the	O
counts	O
ffwg	O
of	O
the	O
symbols	O
seen	O
so	O
far	O
thus	O
:	O
the	O
probability	B
that	O
the	O
next	O
symbol	O
is	O
a	O
new	O
symbol	O
,	O
never	O
seen	O
before	O
,	O
is	O
(	O
cid:11	O
)	O
(	O
18.11	O
)	O
(	O
18.12	O
)	O
:	O
f	O
+	O
(	O
cid:11	O
)	O
the	O
probability	B
that	O
the	O
next	O
symbol	O
is	O
symbol	O
w	O
is	O
fw	O
f	O
+	O
(	O
cid:11	O
)	O
:	O
figure	O
18.6	O
shows	O
zipf	O
plots	O
(	O
i.e.	O
,	O
plots	O
of	O
symbol	O
frequency	B
versus	O
rank	O
)	O
for	O
million-symbol	O
‘	O
documents	O
’	O
generated	O
by	O
dirichlet	O
process	O
priors	O
with	O
values	O
of	O
(	O
cid:11	O
)	O
ranging	O
from	O
1	O
to	O
1000.	O
it	O
is	O
evident	O
that	O
a	O
dirichlet	O
process	O
is	O
not	O
an	O
adequate	O
model	B
for	O
observed	O
distributions	O
that	O
roughly	O
obey	O
zipf	O
’	O
s	O
law	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
264	O
18	O
|	O
crosswords	O
and	O
codebreaking	O
0.1	O
0.01	O
0.001	O
0.0001	O
0.00001	O
1	O
10	O
100	O
1000	O
10000	O
figure	O
18.7.	O
zipf	O
plots	O
for	O
the	O
words	O
of	O
two	O
‘	O
languages	O
’	O
generated	O
by	O
creating	O
successive	O
characters	O
from	O
a	O
dirichlet	O
process	O
with	O
(	O
cid:11	O
)	O
=	O
2	O
,	O
and	O
declaring	O
one	O
character	O
to	O
be	O
the	O
space	O
character	O
.	O
the	O
two	O
curves	O
result	O
from	O
two	O
di	O
(	O
cid:11	O
)	O
erent	O
choices	O
of	O
the	O
space	O
character	O
.	O
with	O
a	O
small	O
tweak	O
,	O
however	O
,	O
dirichlet	O
processes	O
can	O
produce	O
rather	O
nice	O
zipf	O
plots	O
.	O
imagine	O
generating	O
a	O
language	O
composed	O
of	O
elementary	O
symbols	O
using	O
a	O
dirichlet	O
process	O
with	O
a	O
rather	O
small	O
value	O
of	O
the	O
parameter	O
(	O
cid:11	O
)	O
,	O
so	O
that	O
the	O
number	O
of	O
reasonably	O
frequent	O
symbols	O
is	O
about	O
27.	O
if	O
we	O
then	O
declare	O
one	O
of	O
those	O
symbols	O
(	O
now	O
called	O
‘	O
characters	O
’	O
rather	O
than	O
words	O
)	O
to	O
be	O
a	O
space	O
character	O
,	O
then	O
we	O
can	O
identify	O
the	O
strings	O
between	O
the	O
space	O
characters	O
as	O
‘	O
words	O
’	O
.	O
if	O
we	O
generate	O
a	O
language	O
in	O
this	O
way	O
then	O
the	O
frequencies	O
of	O
words	O
often	O
come	O
out	O
as	O
very	O
nice	O
zipf	O
plots	O
,	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
18.7.	O
which	O
character	O
is	O
selected	O
as	O
the	O
space	O
character	O
determines	O
the	O
slope	O
of	O
the	O
zipf	O
plot	O
{	O
a	O
less	O
probable	O
space	O
character	O
gives	O
rise	O
to	O
a	O
richer	O
language	O
with	O
a	O
shallower	O
slope	O
.	O
18.3	O
units	B
of	O
information	B
content	I
the	O
information	B
content	I
of	O
an	O
outcome	O
,	O
x	O
,	O
whose	O
probability	B
is	O
p	O
(	O
x	O
)	O
,	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
h	O
(	O
x	O
)	O
=	O
log	O
:	O
(	O
18.13	O
)	O
1	O
p	O
(	O
x	O
)	O
the	O
entropy	B
of	O
an	O
ensemble	B
is	O
an	O
average	B
information	O
content	B
,	O
h	O
(	O
x	O
)	O
=xx	O
p	O
(	O
x	O
)	O
log	O
1	O
p	O
(	O
x	O
)	O
:	O
(	O
18.14	O
)	O
when	O
we	O
compare	O
hypotheses	O
with	O
each	O
other	O
in	O
the	O
light	O
of	O
data	O
,	O
it	O
is	O
of-	O
ten	O
convenient	O
to	O
compare	O
the	O
log	O
of	O
the	O
probability	O
of	O
the	O
data	O
under	O
the	O
alternative	O
hypotheses	O
,	O
‘	O
log	O
evidence	B
for	O
hi	O
’	O
=	O
log	O
p	O
(	O
d	O
jhi	O
)	O
;	O
(	O
18.15	O
)	O
or	O
,	O
in	O
the	O
case	O
where	O
just	O
two	O
hypotheses	O
are	O
being	O
compared	O
,	O
we	O
evaluate	O
the	O
‘	O
log	O
odds	B
’	O
,	O
log	O
p	O
(	O
d	O
jh1	O
)	O
p	O
(	O
d	O
jh2	O
)	O
;	O
(	O
18.16	O
)	O
which	O
has	O
also	O
been	O
called	O
the	O
‘	O
weight	B
of	O
evidence	B
in	O
favour	O
of	O
h1	O
’	O
.	O
the	O
log	O
evidence	B
for	O
a	O
hypothesis	O
,	O
log	O
p	O
(	O
d	O
jhi	O
)	O
is	O
the	O
negative	O
of	O
the	O
information	O
content	B
of	O
the	O
data	O
d	O
:	O
if	O
the	O
data	O
have	O
large	O
information	B
content	I
,	O
given	O
a	O
hy-	O
pothesis	O
,	O
then	O
they	O
are	O
surprising	O
to	O
that	O
hypothesis	O
;	O
if	O
some	O
other	O
hypothesis	O
is	O
not	O
so	O
surprised	O
by	O
the	O
data	O
,	O
then	O
that	O
hypothesis	O
becomes	O
more	O
probable	O
.	O
‘	O
information	B
content	I
’	O
,	O
‘	O
surprise	B
value	I
’	O
,	O
and	O
log	O
likelihood	B
or	O
log	O
evidence	B
are	O
the	O
same	O
thing	O
.	O
all	O
these	O
quantities	O
are	O
logarithms	B
of	O
probabilities	O
,	O
or	O
weighted	O
sums	O
of	O
logarithms	O
of	O
probabilities	O
,	O
so	O
they	O
can	O
all	O
be	O
measured	O
in	O
the	O
same	O
units	B
.	O
the	O
units	B
depend	O
on	O
the	O
choice	O
of	O
the	O
base	O
of	O
the	O
logarithm	O
.	O
the	O
names	O
that	O
have	O
been	O
given	O
to	O
these	O
units	B
are	O
shown	O
in	O
table	O
18.8	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
18.4	O
:	O
a	O
taste	O
of	O
banburismus	O
265	O
unit	O
bit	B
nat	O
ban	O
deciban	O
(	O
db	O
)	O
expression	O
that	O
has	O
those	O
units	B
table	O
18.8.	O
units	B
of	O
measurement	O
of	O
information	O
content	B
.	O
log2	O
p	O
loge	O
p	O
log10	O
p	O
10	O
log10	O
p	O
the	O
bit	B
is	O
the	O
unit	O
that	O
we	O
use	O
most	O
in	O
this	O
book	O
.	O
because	O
the	O
word	O
‘	O
bit	B
’	O
has	O
other	O
meanings	O
,	O
a	O
backup	O
name	O
for	O
this	O
unit	O
is	O
the	O
shannon	O
.	O
a	O
byte	B
is	O
8	O
bits	O
.	O
a	O
megabyte	O
is	O
220	O
’	O
106	O
bytes	O
.	O
if	O
one	O
works	O
in	O
natural	O
logarithms	B
,	O
information	B
contents	O
and	O
weights	O
of	O
evidence	O
are	O
measured	O
in	O
nats	O
.	O
the	O
most	O
interesting	O
units	B
are	O
the	O
ban	O
and	O
the	O
deciban	O
.	O
the	O
history	O
of	O
the	O
ban	O
let	O
me	O
tell	O
you	O
why	O
a	O
factor	O
of	O
ten	O
in	O
probability	O
is	O
called	O
a	O
ban	O
.	O
when	O
alan	O
turing	O
and	O
the	O
other	O
codebreakers	B
at	O
bletchley	O
park	O
were	O
breaking	O
each	O
new	O
day	O
’	O
s	O
enigma	O
code	B
,	O
their	O
task	O
was	O
a	O
huge	O
inference	B
problem	O
:	O
to	O
infer	O
,	O
given	O
the	O
day	O
’	O
s	O
cyphertext	O
,	O
which	O
three	O
wheels	O
were	O
in	O
the	O
enigma	O
machines	O
that	O
day	O
;	O
what	O
their	O
starting	O
positions	O
were	O
;	O
what	O
further	O
letter	O
substitutions	O
were	O
in	O
use	O
on	O
the	O
steckerboard	O
;	O
and	O
,	O
not	O
least	O
,	O
what	O
the	O
original	O
german	O
messages	O
were	O
.	O
these	O
inferences	O
were	O
conducted	O
using	O
bayesian	O
methods	B
(	O
of	O
course	O
!	O
)	O
,	O
and	O
the	O
chosen	O
units	B
were	O
decibans	O
or	O
half-decibans	O
,	O
the	O
deciban	O
being	O
judged	O
the	O
smallest	O
weight	B
of	O
evidence	B
discernible	O
to	O
a	O
human	B
.	O
the	O
evidence	B
in	O
favour	O
of	O
particular	O
hypotheses	O
was	O
tallied	O
using	O
sheets	O
of	O
paper	O
that	O
were	O
specially	O
printed	O
in	O
banbury	O
,	O
a	O
town	O
about	O
30	O
miles	O
from	O
bletchley	O
.	O
the	O
inference	B
task	O
was	O
known	O
as	O
banburismus	O
,	O
and	O
the	O
units	B
in	O
which	O
banburismus	O
was	O
played	O
were	O
called	O
bans	O
,	O
after	O
that	O
town	O
.	O
18.4	O
a	O
taste	O
of	O
banburismus	O
the	O
details	O
of	O
the	O
code-breaking	O
methods	B
of	O
bletchley	O
park	O
were	O
kept	O
secret	B
for	O
a	O
long	O
time	O
,	O
but	O
some	O
aspects	O
of	O
banburismus	O
can	O
be	O
pieced	O
together	O
.	O
i	O
hope	O
the	O
following	O
description	O
of	O
a	O
small	O
part	O
of	O
banburismus	O
is	O
not	O
too	O
inaccurate.1	O
how	O
much	O
information	O
was	O
needed	O
?	O
the	O
number	O
of	O
possible	O
settings	O
of	O
the	O
enigma	O
machine	O
was	O
about	O
8	O
(	O
cid:2	O
)	O
1012.	O
to	O
deduce	O
the	O
state	O
of	O
the	O
machine	O
,	O
‘	O
it	O
was	O
therefore	O
necessary	O
to	O
(	O
cid:12	O
)	O
nd	O
about	O
129	O
decibans	O
from	O
somewhere	O
’	O
,	O
as	O
good	O
puts	O
it	O
.	O
banburismus	O
was	O
aimed	O
not	O
at	O
deducing	O
the	O
entire	O
state	O
of	O
the	O
machine	O
,	O
but	O
only	O
at	O
(	O
cid:12	O
)	O
guring	O
out	O
which	O
wheels	O
were	O
in	O
use	O
;	O
the	O
logic-based	O
bombes	B
,	O
fed	O
with	O
guesses	O
of	O
the	O
plaintext	O
(	O
cribs	O
)	O
,	O
were	O
then	O
used	O
to	O
crack	O
what	O
the	O
settings	O
of	O
the	O
wheels	O
were	O
.	O
the	O
enigma	O
machine	O
,	O
once	O
its	O
wheels	O
and	O
plugs	O
were	O
put	O
in	O
place	O
,	O
im-	O
plemented	O
a	O
continually-changing	O
permutation	B
cypher	O
that	O
wandered	O
deter-	O
ministically	O
through	O
a	O
state	O
space	O
of	O
263	O
permutations	O
.	O
because	O
an	O
enormous	O
number	O
of	O
messages	O
were	O
sent	O
each	O
day	O
,	O
there	O
was	O
a	O
good	B
chance	O
that	O
what-	O
ever	O
state	O
one	O
machine	O
was	O
in	O
when	O
sending	O
one	O
character	O
of	O
a	O
message	O
,	O
there	O
would	O
be	O
another	O
machine	O
in	O
the	O
same	O
state	O
while	O
sending	O
a	O
particular	O
char-	O
acter	O
in	O
another	O
message	O
.	O
because	O
the	O
evolution	B
of	O
the	O
machine	O
’	O
s	O
state	O
was	O
deterministic	B
,	O
the	O
two	O
machines	O
would	O
remain	O
in	O
the	O
same	O
state	O
as	O
each	O
other	O
1i	O
’	O
ve	O
been	O
most	O
helped	O
by	O
descriptions	O
(	O
http	O
:	O
//www	O
.	O
codesandciphers.org.uk/lectures/	O
)	O
and	O
by	O
jack	O
good	B
(	O
1979	O
)	O
,	O
who	O
worked	O
with	O
turing	O
at	O
bletchley	O
.	O
given	O
by	O
tony	O
sale	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
266	O
18	O
|	O
crosswords	O
and	O
codebreaking	O
for	O
the	O
rest	O
of	O
the	O
transmission	O
.	O
the	O
resulting	O
correlations	B
between	O
the	O
out-	O
puts	O
of	O
such	O
pairs	O
of	O
machines	O
provided	O
a	O
dribble	O
of	O
information-content	O
from	O
which	O
turing	O
and	O
his	O
co-workers	O
extracted	O
their	O
daily	O
129	O
decibans	O
.	O
how	O
to	O
detect	O
that	O
two	O
messages	O
came	O
from	O
machines	O
with	O
a	O
common	O
state	O
sequence	O
the	O
hypotheses	O
are	O
the	O
null	O
hypothesis	O
,	O
h0	O
,	O
which	O
states	O
that	O
the	O
machines	O
are	O
in	O
di	O
(	O
cid:11	O
)	O
erent	O
states	O
,	O
and	O
that	O
the	O
two	O
plain	O
messages	O
are	O
unrelated	O
;	O
and	O
the	O
‘	O
match	O
’	O
hypothesis	O
,	O
h1	O
,	O
which	O
says	O
that	O
the	O
machines	O
are	O
in	O
the	O
same	O
state	O
,	O
and	O
that	O
the	O
two	O
plain	O
messages	O
are	O
unrelated	O
.	O
no	O
attempt	O
is	O
being	O
made	O
here	O
to	O
infer	O
what	O
the	O
state	O
of	O
either	O
machine	O
is	O
.	O
the	O
data	O
provided	O
are	O
the	O
two	O
cyphertexts	O
x	O
and	O
y	O
;	O
let	O
’	O
s	O
assume	O
they	O
both	O
have	O
length	B
t	O
and	O
that	O
the	O
alphabet	O
size	O
is	O
a	O
(	O
26	O
in	O
enigma	O
)	O
.	O
what	O
is	O
the	O
probability	O
of	O
the	O
data	O
,	O
given	O
the	O
two	O
hypotheses	O
?	O
first	O
,	O
the	O
null	O
hypothesis	O
.	O
this	O
hypothesis	O
asserts	O
that	O
the	O
two	O
cyphertexts	O
are	O
given	O
by	O
and	O
x	O
=	O
x1x2x3	O
:	O
:	O
:	O
=	O
c1	O
(	O
u1	O
)	O
c2	O
(	O
u2	O
)	O
c3	O
(	O
u3	O
)	O
:	O
:	O
:	O
(	O
18.17	O
)	O
y	O
=	O
y1y2y3	O
:	O
:	O
:	O
=	O
c01	O
(	O
v1	O
)	O
c02	O
(	O
v2	O
)	O
c03	O
(	O
v3	O
)	O
:	O
:	O
:	O
;	O
(	O
18.18	O
)	O
where	O
the	O
codes	O
ct	O
and	O
c0t	O
are	O
two	O
unrelated	O
time-varying	O
permutations	O
of	O
the	O
alphabet	O
,	O
and	O
u1u2u3	O
:	O
:	O
:	O
and	O
v1v2v3	O
:	O
:	O
:	O
are	O
the	O
plaintext	B
messages	O
.	O
an	O
exact	O
computation	O
of	O
the	O
probability	O
of	O
the	O
data	O
(	O
x	O
;	O
y	O
)	O
would	O
depend	O
on	O
a	O
language	B
model	I
of	O
the	O
plain	O
text	O
,	O
and	O
a	O
model	B
of	O
the	O
enigma	O
machine	O
’	O
s	O
guts	O
,	O
but	O
if	O
we	O
assume	O
that	O
each	O
enigma	O
machine	O
is	O
an	O
ideal	O
random	B
time-varying	O
permuta-	O
tion	O
,	O
then	O
the	O
probability	B
distribution	O
of	O
the	O
two	O
cyphertexts	O
is	O
uniform	O
.	O
all	O
cyphertexts	O
are	O
equally	O
likely	O
.	O
p	O
(	O
x	O
;	O
y	O
jh0	O
)	O
=	O
(	O
cid:18	O
)	O
1	O
a	O
(	O
cid:19	O
)	O
2t	O
for	O
all	O
x	O
;	O
y	O
of	O
length	O
t	O
:	O
(	O
18.19	O
)	O
what	O
about	O
h1	O
?	O
this	O
hypothesis	O
asserts	O
that	O
a	O
single	O
time-varying	O
permuta-	O
tion	O
ct	O
underlies	O
both	O
x	O
=	O
x1x2x3	O
:	O
:	O
:	O
=	O
c1	O
(	O
u1	O
)	O
c2	O
(	O
u2	O
)	O
c3	O
(	O
u3	O
)	O
:	O
:	O
:	O
(	O
18.20	O
)	O
and	O
y	O
=	O
y1y2y3	O
:	O
:	O
:	O
=	O
c1	O
(	O
v1	O
)	O
c2	O
(	O
v2	O
)	O
c3	O
(	O
v3	O
)	O
:	O
:	O
:	O
:	O
(	O
18.21	O
)	O
what	O
is	O
the	O
probability	O
of	O
the	O
data	O
(	O
x	O
;	O
y	O
)	O
?	O
we	O
have	O
to	O
make	O
some	O
assumptions	B
about	O
the	O
plaintext	B
language	O
.	O
if	O
it	O
were	O
the	O
case	O
that	O
the	O
plaintext	B
language	O
was	O
completely	O
random	B
,	O
then	O
the	O
probability	O
of	O
u1u2u3	O
:	O
:	O
:	O
and	O
v1v2v3	O
:	O
:	O
:	O
would	O
be	O
uniform	O
,	O
and	O
so	O
would	O
that	O
of	O
x	O
and	O
y	O
,	O
so	O
the	O
probability	B
p	O
(	O
x	O
;	O
y	O
jh1	O
)	O
would	O
be	O
equal	O
to	O
p	O
(	O
x	O
;	O
y	O
jh0	O
)	O
,	O
and	O
the	O
two	O
hypotheses	O
h0	O
and	O
h1	O
would	O
be	O
indistinguishable	O
.	O
we	O
make	O
progress	O
by	O
assuming	O
that	O
the	O
plaintext	B
is	O
not	O
completely	O
ran-	O
dom	O
.	O
both	O
plaintexts	O
are	O
written	O
in	O
a	O
language	O
,	O
and	O
that	O
language	O
has	O
redun-	O
dancies	O
.	O
assume	O
for	O
example	O
that	O
particular	O
plaintext	B
letters	O
are	O
used	O
more	O
often	O
than	O
others	O
.	O
so	O
,	O
even	O
though	O
the	O
two	O
plaintext	B
messages	O
are	O
unrelated	O
,	O
they	O
are	O
slightly	O
more	O
likely	O
to	O
use	O
the	O
same	O
letters	O
as	O
each	O
other	O
;	O
if	O
h1	O
is	O
true	O
,	O
two	O
synchronized	O
letters	O
from	O
the	O
two	O
cyphertexts	O
are	O
slightly	O
more	O
likely	O
to	O
be	O
identical	O
.	O
similarly	O
,	O
if	O
a	O
language	O
uses	O
particular	O
bigrams	O
and	O
trigrams	O
frequently	O
,	O
then	O
the	O
two	O
plaintext	B
messages	O
will	O
occasionally	O
contain	O
the	O
same	O
bigrams	O
and	O
trigrams	O
at	O
the	O
same	O
time	O
as	O
each	O
other	O
,	O
giving	O
rise	O
,	O
if	O
h1	O
is	O
true	O
,	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
18.4	O
:	O
a	O
taste	O
of	O
banburismus	O
267	O
u	O
little-jack-horner-sat-in-the-corner-eating-a-christmas-pie	O
--	O
he-put-in-h	O
v	O
ride-a-cock-horse-to-banbury-cross-to-see-a-fine-lady-upon-a-white-horse	O
matches	O
:	O
.*	O
...	O
.*..******.*	O
...	O
...	O
...	O
...	O
..*	O
...	O
...	O
...	O
..*	O
...	O
...	O
...	O
...	O
...	O
.*	O
...	O
...	O
...	O
..	O
table	O
18.9.	O
two	O
aligned	O
pieces	O
of	O
english	O
plaintext	B
,	O
u	O
and	O
v	O
,	O
with	O
matches	O
marked	O
by	O
*	O
.	O
notice	O
that	O
there	O
are	O
twelve	O
matches	O
,	O
including	O
a	O
run	O
of	O
six	O
,	O
whereas	O
the	O
expected	O
number	O
of	O
matches	O
in	O
two	O
completely	O
random	B
strings	O
of	O
length	O
t	O
=	O
74	O
would	O
be	O
about	O
3.	O
the	O
two	O
corresponding	O
cyphertexts	O
from	O
two	O
machines	O
in	O
identical	O
states	O
would	O
also	O
have	O
twelve	O
matches	O
.	O
to	O
a	O
little	O
burst	O
of	O
2	O
or	O
3	O
identical	O
letters	O
.	O
table	O
18.9	O
shows	O
such	O
a	O
coinci-	O
dence	O
in	O
two	O
plaintext	B
messages	O
that	O
are	O
unrelated	O
,	O
except	O
that	O
they	O
are	O
both	O
written	O
in	O
english	O
.	O
the	O
codebreakers	B
hunted	O
among	O
pairs	O
of	O
messages	O
for	O
pairs	O
that	O
were	O
sus-	O
piciously	O
similar	O
to	O
each	O
other	O
,	O
counting	B
up	O
the	O
numbers	O
of	O
matching	O
mono-	O
grams	O
,	O
bigrams	O
,	O
trigrams	O
,	O
etc	O
.	O
this	O
method	B
was	O
(	O
cid:12	O
)	O
rst	O
used	O
by	O
the	O
polish	O
codebreaker	O
rejewski	O
.	O
let	O
’	O
s	O
look	O
at	O
the	O
simple	O
case	O
of	O
a	O
monogram	O
language	B
model	I
and	O
estimate	O
how	O
long	O
a	O
message	O
is	O
needed	O
to	O
be	O
able	O
to	O
decide	O
whether	O
two	O
machines	O
are	O
in	O
the	O
same	O
state	O
.	O
i	O
’	O
ll	O
assume	O
the	O
source	O
language	O
is	O
monogram-english	O
,	O
the	O
language	O
in	O
which	O
successive	O
letters	O
are	O
drawn	O
i.i.d	O
.	O
from	O
the	O
probability	B
distribution	O
fpig	O
of	O
(	O
cid:12	O
)	O
gure	O
2.1.	O
the	O
probability	O
of	O
x	O
and	O
y	O
is	O
nonuniform	O
:	O
consider	O
two	O
single	O
characters	O
,	O
xt	O
=	O
ct	O
(	O
ut	O
)	O
and	O
yt	O
=	O
ct	O
(	O
vt	O
)	O
;	O
the	O
probability	B
that	O
they	O
are	O
identical	O
is	O
xut	O
;	O
vt	O
p	O
(	O
ut	O
)	O
p	O
(	O
vt	O
)	O
	O
[	O
ut	O
=	O
vt	O
]	O
=	O
xi	O
p2	O
i	O
(	O
cid:17	O
)	O
m	O
:	O
(	O
18.22	O
)	O
we	O
give	O
this	O
quantity	O
the	O
name	O
m	O
,	O
for	O
‘	O
match	O
probability	B
’	O
;	O
for	O
both	O
english	O
and	O
german	O
,	O
m	O
is	O
about	O
2=26	O
rather	O
than	O
1=26	O
(	O
the	O
value	O
that	O
would	O
hold	O
for	O
a	O
completely	O
random	B
language	O
)	O
.	O
assuming	O
that	O
ct	O
is	O
an	O
ideal	O
random	B
permutation	O
,	O
the	O
probability	O
of	O
xt	O
and	O
yt	O
is	O
,	O
by	O
symmetry	O
,	O
p	O
(	O
xt	O
;	O
yt	O
jh1	O
)	O
=	O
(	O
m	O
a	O
(	O
1	O
(	O
cid:0	O
)	O
m	O
)	O
a	O
(	O
a	O
(	O
cid:0	O
)	O
1	O
)	O
if	O
xt	O
=	O
yt	O
for	O
xt	O
6=	O
yt	O
.	O
(	O
18.23	O
)	O
given	O
a	O
pair	O
of	O
cyphertexts	O
x	O
and	O
y	O
of	O
length	O
t	O
that	O
match	O
in	O
m	O
places	O
and	O
do	O
not	O
match	O
in	O
n	O
places	O
,	O
the	O
log	O
evidence	B
in	O
favour	O
of	O
h1	O
is	O
then	O
log	O
p	O
(	O
x	O
;	O
y	O
jh1	O
)	O
p	O
(	O
x	O
;	O
y	O
jh0	O
)	O
=	O
m	O
log	O
m=a	O
1=a2	O
+	O
n	O
log	O
(	O
1	O
(	O
cid:0	O
)	O
m	O
)	O
a	O
(	O
a	O
(	O
cid:0	O
)	O
1	O
)	O
1=a2	O
=	O
m	O
log	O
ma	O
+	O
n	O
log	O
(	O
1	O
(	O
cid:0	O
)	O
m	O
)	O
a	O
a	O
(	O
cid:0	O
)	O
1	O
:	O
(	O
18.24	O
)	O
(	O
18.25	O
)	O
every	O
match	O
contributes	O
log	O
ma	O
in	O
favour	O
of	O
h1	O
;	O
every	O
non-match	O
contributes	O
log	O
a	O
(	O
cid:0	O
)	O
1	O
0.076	O
0.037	O
3.1	O
db	O
10	O
log10	O
ma	O
(	O
1	O
(	O
cid:0	O
)	O
m	O
)	O
a	O
(	O
a	O
(	O
cid:0	O
)	O
1	O
)	O
(	O
cid:0	O
)	O
0:18	O
db	O
(	O
1	O
(	O
cid:0	O
)	O
m	O
)	O
a	O
in	O
favour	O
of	O
h0	O
.	O
match	O
probability	B
for	O
monogram-english	O
coincidental	O
match	O
probability	B
log-evidence	O
for	O
h1	O
per	O
match	O
log-evidence	O
for	O
h1	O
per	O
non-match	O
if	O
there	O
were	O
m	O
=	O
4	O
matches	O
and	O
n	O
=	O
47	O
non-matches	O
in	O
a	O
pair	O
of	O
length	O
t	O
=	O
51	O
,	O
for	O
example	O
,	O
the	O
weight	B
of	O
evidence	B
in	O
favour	O
of	O
h1	O
would	O
be	O
+4	O
decibans	O
,	O
or	O
a	O
likelihood	B
ratio	O
of	O
2.5	O
to	O
1	O
in	O
favour	O
.	O
the	O
expected	O
weight	B
of	O
evidence	B
from	O
a	O
line	O
of	O
text	O
of	O
length	O
t	O
=	O
20	O
characters	O
is	O
the	O
expectation	B
of	O
(	O
18.25	O
)	O
,	O
which	O
depends	O
on	O
whether	O
h1	O
or	O
h0	O
is	O
true	O
.	O
if	O
h1	O
is	O
true	O
then	O
matches	O
are	O
expected	O
to	O
turn	O
up	O
at	O
rate	B
m	O
,	O
and	O
the	O
expected	O
weight	B
of	O
evidence	B
is	O
1.4	O
decibans	O
per	O
20	O
characters	O
.	O
if	O
h0	O
is	O
true	O
m	O
1=a	O
10	O
log10	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
268	O
18	O
|	O
crosswords	O
and	O
codebreaking	O
then	O
spurious	O
matches	O
are	O
expected	O
to	O
turn	O
up	O
at	O
rate	B
1=a	O
,	O
and	O
the	O
expected	O
weight	B
of	O
evidence	B
is	O
(	O
cid:0	O
)	O
1:1	O
decibans	O
per	O
20	O
characters	O
.	O
typically	O
,	O
roughly	O
400	O
characters	O
need	O
to	O
be	O
inspected	O
in	O
order	O
to	O
have	O
a	O
weight	B
of	O
evidence	B
greater	O
than	O
a	O
hundred	O
to	O
one	O
(	O
20	O
decibans	O
)	O
in	O
favour	O
of	O
one	O
hypothesis	O
or	O
the	O
other	O
.	O
so	O
,	O
two	O
english	O
plaintexts	O
have	O
more	O
matches	O
than	O
two	O
random	B
strings	O
.	O
furthermore	O
,	O
because	O
consecutive	O
characters	O
in	O
english	O
are	O
not	O
independent	O
,	O
the	O
bigram	O
and	O
trigram	O
statistics	O
of	O
english	O
are	O
nonuniform	O
and	O
the	O
matches	O
tend	O
to	O
occur	O
in	O
bursts	O
of	O
consecutive	O
matches	O
.	O
[	O
the	O
same	O
observations	O
also	O
apply	O
to	O
german	O
.	O
]	O
using	O
better	O
language	O
models	O
,	O
the	O
evidence	B
contributed	O
by	O
runs	O
of	O
matches	O
was	O
more	O
accurately	O
computed	O
.	O
such	O
a	O
scoring	O
system	O
was	O
worked	O
out	O
by	O
turing	O
and	O
re	O
(	O
cid:12	O
)	O
ned	O
by	O
good	O
.	O
positive	O
results	O
were	O
passed	O
on	O
to	O
automated	O
and	O
human-powered	O
codebreakers	B
.	O
according	O
to	O
good	B
,	O
the	O
longest	O
false-positive	O
that	O
arose	O
in	O
this	O
work	O
was	O
a	O
string	O
of	O
8	O
consecutive	O
matches	O
between	O
two	O
machines	O
that	O
were	O
actually	O
in	O
unrelated	O
states	O
.	O
further	O
reading	O
for	O
further	O
reading	O
about	O
turing	O
and	O
bletchley	O
park	O
,	O
see	O
hodges	O
(	O
1983	O
)	O
and	O
good	O
(	O
1979	O
)	O
.	O
for	O
an	O
in-depth	O
read	O
about	O
cryptography	B
,	O
schneier	O
’	O
s	O
(	O
1996	O
)	O
book	O
is	O
highly	O
recommended	O
.	O
it	O
is	O
readable	O
,	O
clear	O
,	O
and	O
entertaining	O
.	O
18.5	O
exercises	O
.	O
exercise	O
18.3	O
.	O
[	O
2	O
]	O
another	O
weakness	O
in	O
the	O
design	O
of	O
the	O
enigma	O
machine	O
,	O
which	O
was	O
intended	O
to	O
emulate	O
a	O
perfectly	O
random	B
time-varying	O
permu-	O
tation	O
,	O
is	O
that	O
it	O
never	O
mapped	O
a	O
letter	O
to	O
itself	O
.	O
when	O
you	O
press	O
q	O
,	O
what	O
comes	O
out	O
is	O
always	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
letter	O
from	O
q.	O
how	O
much	O
information	O
per	O
character	O
is	O
leaked	O
by	O
this	O
design	O
(	O
cid:13	O
)	O
aw	O
?	O
how	O
long	O
a	O
crib	B
would	O
be	O
needed	O
to	O
be	O
con	O
(	O
cid:12	O
)	O
dent	O
that	O
the	O
crib	B
is	O
correctly	O
aligned	O
with	O
the	O
cyphertext	O
?	O
and	O
how	O
long	O
a	O
crib	B
would	O
be	O
needed	O
to	O
be	O
able	O
con	O
(	O
cid:12	O
)	O
dently	O
to	O
identify	O
the	O
correct	O
key	O
?	O
[	O
a	O
crib	B
is	O
a	O
guess	O
for	O
what	O
the	O
plaintext	B
was	O
.	O
imagine	O
that	O
the	O
brits	O
know	O
that	O
a	O
very	O
important	O
german	O
is	O
travelling	O
from	O
berlin	O
to	O
aachen	O
,	O
and	O
they	O
intercept	O
enigma-encoded	O
messages	O
sent	O
to	O
aachen	O
.	O
it	O
is	O
a	O
good	B
bet	O
that	O
one	O
or	O
more	O
of	O
the	O
original	O
plaintext	B
messages	O
contains	O
the	O
string	O
obersturmbannfuehrerxgrafxheinrichxvonxweizsaecker	O
,	O
the	O
name	O
of	O
the	O
important	O
chap	O
.	O
a	O
crib	B
could	O
be	O
used	O
in	O
a	O
brute-force	O
approach	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
correct	O
enigma	O
key	O
(	O
feed	O
the	O
received	O
messages	O
through	O
all	O
possible	O
engima	O
machines	O
and	O
see	O
if	O
any	O
of	O
the	O
putative	O
decoded	O
texts	O
match	O
the	O
above	O
plaintext	B
)	O
.	O
this	O
question	O
centres	O
on	O
the	O
idea	O
that	O
the	O
crib	B
can	O
also	O
be	O
used	O
in	O
a	O
much	O
less	O
expensive	O
manner	O
:	O
slide	O
the	O
plaintext	B
crib	O
along	O
all	O
the	O
encoded	O
messages	O
until	O
a	O
perfect	B
mismatch	O
of	O
the	O
crib	O
and	O
the	O
encoded	O
message	O
is	O
found	O
;	O
if	O
correct	O
,	O
this	O
alignment	O
then	O
tells	O
you	O
a	O
lot	O
about	O
the	O
key	O
.	O
]	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
19	O
why	O
have	O
sex	O
?	O
information	B
acquisition	O
and	O
evolution	O
evolution	B
has	O
been	O
happening	O
on	O
earth	O
for	O
about	O
the	O
last	O
109	O
years	O
.	O
un-	O
deniably	O
,	O
information	B
has	O
been	O
acquired	O
during	O
this	O
process	O
.	O
thanks	O
to	O
the	O
tireless	O
work	O
of	O
the	O
blind	O
watchmaker	O
,	O
some	O
cells	O
now	O
carry	O
within	O
them	O
all	O
the	O
information	B
required	O
to	O
be	O
outstanding	O
spiders	O
;	O
other	O
cells	O
carry	O
all	O
the	O
information	B
required	O
to	O
make	O
excellent	O
octopuses	O
.	O
where	O
did	O
this	O
information	B
come	O
from	O
?	O
the	O
entire	O
blueprint	O
of	O
all	O
organisms	O
on	O
the	O
planet	O
has	O
emerged	O
in	O
a	O
teach-	O
ing	O
process	O
in	O
which	O
the	O
teacher	O
is	O
natural	B
selection	I
:	O
(	O
cid:12	O
)	O
tter	O
individuals	O
have	O
more	O
progeny	O
,	O
the	O
(	O
cid:12	O
)	O
tness	O
being	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
the	O
local	O
environment	O
(	O
including	O
the	O
other	O
organisms	O
)	O
.	O
the	O
teaching	O
signal	O
is	O
only	O
a	O
few	O
bits	O
per	O
individual	O
:	O
an	O
individual	O
simply	O
has	O
a	O
smaller	O
or	O
larger	O
number	O
of	O
grandchildren	O
,	O
depending	O
on	O
the	O
individual	O
’	O
s	O
(	O
cid:12	O
)	O
tness	O
.	O
‘	O
fitness	O
’	O
is	O
a	O
broad	O
term	O
that	O
could	O
cover	O
(	O
cid:15	O
)	O
the	O
ability	O
of	O
an	O
antelope	O
to	O
run	O
faster	O
than	O
other	O
antelopes	O
and	O
hence	O
avoid	O
being	O
eaten	O
by	O
a	O
lion	O
;	O
(	O
cid:15	O
)	O
the	O
ability	O
of	O
a	O
lion	O
to	O
be	O
well-enough	O
camou	O
(	O
cid:13	O
)	O
aged	O
and	O
run	O
fast	O
enough	O
to	O
catch	O
one	O
antelope	O
per	O
day	O
;	O
(	O
cid:15	O
)	O
the	O
ability	O
of	O
a	O
peacock	O
to	O
attract	O
a	O
peahen	O
to	O
mate	O
with	O
it	O
;	O
(	O
cid:15	O
)	O
the	O
ability	O
of	O
a	O
peahen	O
to	O
rear	O
many	O
young	O
simultaneously	O
.	O
the	O
(	O
cid:12	O
)	O
tness	O
of	O
an	O
organism	O
is	O
largely	O
determined	O
by	O
its	O
dna	O
{	O
both	O
the	O
coding	O
regions	O
,	O
or	O
genes	B
,	O
and	O
the	O
non-coding	O
regions	O
(	O
which	O
play	O
an	O
important	O
role	O
in	O
regulating	O
the	O
transcription	O
of	O
genes	O
)	O
.	O
we	O
’	O
ll	O
think	O
of	O
(	O
cid:12	O
)	O
tness	O
as	O
a	O
function	B
of	O
the	O
dna	O
sequence	B
and	O
the	O
environment	O
.	O
how	O
does	O
the	O
dna	O
determine	O
(	O
cid:12	O
)	O
tness	O
,	O
and	O
how	O
does	O
information	B
get	O
from	O
natural	O
selection	O
into	O
the	O
genome	B
?	O
well	O
,	O
if	O
the	O
gene	O
that	O
codes	O
for	O
one	O
of	O
an	O
antelope	O
’	O
s	O
proteins	O
is	O
defective	O
,	O
that	O
antelope	O
might	O
get	O
eaten	O
by	O
a	O
lion	O
early	O
in	O
life	O
and	O
have	O
only	O
two	O
grandchildren	O
rather	O
than	O
forty	O
.	O
the	O
information	B
content	I
of	O
natural	B
selection	I
is	O
fully	O
contained	O
in	O
a	O
speci	O
(	O
cid:12	O
)	O
cation	O
of	O
which	O
o	O
(	O
cid:11	O
)	O
-	O
spring	B
survived	O
to	O
have	O
children	O
{	O
an	O
information	B
content	I
of	O
at	O
most	O
one	O
bit	B
per	O
o	O
(	O
cid:11	O
)	O
spring	B
.	O
the	O
teaching	O
signal	O
does	O
not	O
communicate	O
to	O
the	O
ecosystem	O
any	O
description	O
of	O
the	O
imperfections	O
in	O
the	O
organism	O
that	O
caused	O
it	O
to	O
have	O
fewer	O
children	O
.	O
the	O
bits	O
of	O
the	O
teaching	O
signal	O
are	O
highly	O
redundant	O
,	O
because	O
,	O
throughout	O
a	O
species	B
,	O
un	O
(	O
cid:12	O
)	O
t	O
individuals	O
who	O
are	O
similar	O
to	O
each	O
other	O
will	O
be	O
failing	O
to	O
have	O
o	O
(	O
cid:11	O
)	O
spring	B
for	O
similar	O
reasons	O
.	O
so	O
,	O
how	O
many	O
bits	O
per	O
generation	O
are	O
acquired	O
by	O
the	O
species	B
as	O
a	O
whole	O
by	O
natural	O
selection	O
?	O
how	O
many	O
bits	O
has	O
natural	B
selection	I
succeeded	O
in	O
con-	O
veying	O
to	O
the	O
human	B
branch	O
of	O
the	O
tree	O
of	O
life	O
,	O
since	O
the	O
divergence	B
between	O
269	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
270	O
19	O
|	O
why	O
have	O
sex	O
?	O
information	B
acquisition	O
and	O
evolution	O
australopithecines	O
and	O
apes	O
4	O
000	O
000	O
years	O
ago	O
?	O
assuming	O
a	O
generation	O
time	O
of	O
10	O
years	O
for	O
reproduction	O
,	O
there	O
have	O
been	O
about	O
400	O
000	O
generations	O
of	O
human	O
precursors	O
since	O
the	O
divergence	B
from	O
apes	O
.	O
assuming	O
a	O
population	O
of	O
109	O
individuals	O
,	O
each	O
receiving	O
a	O
couple	O
of	O
bits	O
of	O
information	O
from	O
natural	O
selection	O
,	O
the	O
total	O
number	O
of	O
bits	O
of	O
information	O
responsible	O
for	O
modifying	O
the	O
genomes	O
of	O
4	O
million	O
b.c	O
.	O
into	O
today	O
’	O
s	O
human	B
genome	O
is	O
about	O
8	O
(	O
cid:2	O
)	O
1014	O
bits	O
.	O
however	O
,	O
as	O
we	O
noted	O
,	O
natural	B
selection	I
is	O
not	O
smart	O
at	O
collating	O
the	O
information	B
that	O
it	O
dishes	O
out	O
to	O
the	O
population	O
,	O
and	O
there	O
is	O
a	O
great	O
deal	O
of	O
redundancy	O
in	O
that	O
information	B
.	O
if	O
the	O
population	O
size	O
were	O
twice	O
as	O
great	O
,	O
would	O
it	O
evolve	O
twice	O
as	O
fast	O
?	O
no	O
,	O
because	O
natural	B
selection	I
will	O
simply	O
be	O
correcting	O
the	O
same	O
defects	O
twice	O
as	O
often	O
.	O
john	O
maynard	O
smith	O
has	O
suggested	O
that	O
the	O
rate	B
of	O
information	B
acquisition	O
by	O
a	O
species	B
is	O
independent	O
of	O
the	O
population	O
size	O
,	O
and	O
is	O
of	O
order	O
1	O
bit	B
per	O
generation	O
.	O
this	O
(	O
cid:12	O
)	O
gure	O
would	O
allow	O
for	O
only	O
400	O
000	O
bits	O
of	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
apes	O
and	O
humans	O
,	O
a	O
number	O
that	O
is	O
much	O
smaller	O
than	O
the	O
total	O
size	O
of	O
the	O
human	O
genome	B
{	O
6	O
(	O
cid:2	O
)	O
109	O
bits	O
.	O
[	O
one	O
human	B
genome	O
contains	O
about	O
3	O
(	O
cid:2	O
)	O
109	O
nucleotides	O
.	O
]	O
it	O
is	O
certainly	O
the	O
case	O
that	O
the	O
genomic	O
overlap	O
between	O
apes	O
and	O
humans	O
is	O
huge	O
,	O
but	O
is	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
that	O
small	O
?	O
in	O
this	O
chapter	O
,	O
we	O
’	O
ll	O
develop	O
a	O
crude	O
model	B
of	O
the	O
process	O
of	O
information	O
acquisition	O
through	O
evolution	B
,	O
based	O
on	O
the	O
assumption	O
that	O
a	O
gene	O
with	O
two	O
defects	O
is	O
typically	O
likely	O
to	O
be	O
more	O
defective	O
than	O
a	O
gene	O
with	O
one	O
defect	O
,	O
and	O
an	O
organism	O
with	O
two	O
defective	O
genes	B
is	O
likely	O
to	O
be	O
less	O
(	O
cid:12	O
)	O
t	O
than	O
an	O
organism	O
with	O
one	O
defective	O
gene	O
.	O
undeniably	O
,	O
this	O
is	O
a	O
crude	O
model	B
,	O
since	O
real	O
biological	O
systems	O
are	O
baroque	O
constructions	O
with	O
complex	O
interactions	O
.	O
nevertheless	O
,	O
we	O
persist	O
with	O
a	O
simple	O
model	O
because	O
it	O
readily	O
yields	O
striking	O
results	O
.	O
what	O
we	O
(	O
cid:12	O
)	O
nd	O
from	O
this	O
simple	O
model	O
is	O
that	O
1.	O
john	O
maynard	O
smith	O
’	O
s	O
(	O
cid:12	O
)	O
gure	O
of	O
1	O
bit	B
per	O
generation	O
is	O
correct	O
for	O
an	O
asexually-reproducing	O
population	O
;	O
2.	O
in	O
contrast	O
,	O
if	O
the	O
species	B
reproduces	O
sexually	O
,	O
the	O
rate	B
of	O
information	B
acquisition	O
can	O
be	O
as	O
large	O
as	O
pg	O
bits	O
per	O
generation	O
,	O
where	O
g	O
is	O
the	O
size	O
of	O
the	O
genome	O
.	O
we	O
’	O
ll	O
also	O
(	O
cid:12	O
)	O
nd	O
interesting	O
results	O
concerning	O
the	O
maximum	O
mutation	O
rate	B
that	O
a	O
species	B
can	O
withstand	O
.	O
19.1	O
the	O
model	B
we	O
study	O
a	O
simple	O
model	O
of	O
a	O
reproducing	O
population	O
of	O
n	O
individuals	O
with	O
a	O
genome	B
of	O
size	O
g	O
bits	O
:	O
variation	O
is	O
produced	O
by	O
mutation	O
or	O
by	O
recombina-	O
tion	O
(	O
i.e.	O
,	O
sex	O
)	O
and	O
truncation	O
selection	O
selects	O
the	O
n	O
(	O
cid:12	O
)	O
ttest	O
children	O
at	O
each	O
generation	O
to	O
be	O
the	O
parents	O
of	O
the	O
next	O
.	O
we	O
(	O
cid:12	O
)	O
nd	O
striking	O
di	O
(	O
cid:11	O
)	O
erences	O
between	O
populations	O
that	O
have	O
recombination	O
and	O
populations	O
that	O
do	O
not	O
.	O
the	O
genotype	O
of	O
each	O
individual	O
is	O
a	O
vector	O
x	O
of	O
g	O
bits	O
,	O
each	O
having	O
a	O
good	B
state	O
xg	O
=	O
1	O
and	O
a	O
bad	B
state	O
xg	O
=	O
0.	O
the	O
(	O
cid:12	O
)	O
tness	O
f	O
(	O
x	O
)	O
of	O
an	O
individual	O
is	O
simply	O
the	O
sum	O
of	O
her	O
bits	O
:	O
the	O
bits	O
in	O
the	O
genome	B
could	O
be	O
considered	O
to	O
correspond	O
either	O
to	O
genes	B
that	O
have	O
good	B
alleles	O
(	O
xg	O
=	O
1	O
)	O
and	O
bad	O
alleles	O
(	O
xg	O
=	O
0	O
)	O
,	O
or	O
to	O
the	O
nucleotides	O
of	O
a	O
genome	B
.	O
we	O
will	O
concentrate	O
on	O
the	O
latter	O
interpretation	O
.	O
the	O
essential	O
property	O
of	O
(	O
cid:12	O
)	O
tness	O
that	O
we	O
are	O
assuming	O
is	O
that	O
it	O
is	O
locally	O
a	O
roughly	O
linear	B
function	O
of	O
the	O
genome	O
,	O
that	O
is	O
,	O
that	O
there	O
are	O
many	O
possible	O
changes	O
one	O
f	O
(	O
x	O
)	O
=	O
xg	O
:	O
(	O
19.1	O
)	O
g	O
xg=1	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
19.2	O
:	O
rate	B
of	O
increase	O
of	O
(	O
cid:12	O
)	O
tness	O
271	O
could	O
make	O
to	O
the	O
genome	B
,	O
each	O
of	O
which	O
has	O
a	O
small	O
e	O
(	O
cid:11	O
)	O
ect	O
on	O
(	O
cid:12	O
)	O
tness	O
,	O
and	O
that	O
these	O
e	O
(	O
cid:11	O
)	O
ects	O
combine	O
approximately	O
linearly	O
.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
normalized	O
(	O
cid:12	O
)	O
tness	O
f	O
(	O
x	O
)	O
(	O
cid:17	O
)	O
f	O
(	O
x	O
)	O
=g	O
.	O
we	O
consider	O
evolution	B
by	O
natural	B
selection	I
under	O
two	O
models	O
of	O
variation	O
.	O
variation	O
by	O
mutation	O
.	O
the	O
model	B
assumes	O
discrete	O
generations	O
.	O
at	O
each	O
generation	O
,	O
t	O
,	O
every	O
individual	O
produces	O
two	O
children	O
.	O
the	O
children	O
’	O
s	O
genotypes	O
di	O
(	O
cid:11	O
)	O
er	O
from	O
the	O
parent	B
’	O
s	O
by	O
random	O
mutations	O
.	O
natural	B
selec-	O
tion	O
selects	O
the	O
(	O
cid:12	O
)	O
ttest	O
n	O
progeny	O
in	O
the	O
child	O
population	O
to	O
reproduce	O
,	O
and	O
a	O
new	O
generation	O
starts	O
.	O
[	O
the	O
selection	O
of	O
the	O
(	O
cid:12	O
)	O
ttest	O
n	O
individuals	O
at	O
each	O
generation	O
is	O
known	O
as	O
truncation	O
selection	O
.	O
]	O
the	O
simplest	O
model	B
of	O
mutations	O
is	O
that	O
the	O
child	O
’	O
s	O
bits	O
fxgg	O
are	O
in-	O
dependent	O
.	O
each	O
bit	B
has	O
a	O
small	O
probability	O
of	O
being	O
(	O
cid:13	O
)	O
ipped	O
,	O
which	O
,	O
thinking	O
of	O
the	O
bits	O
as	O
corresponding	O
roughly	O
to	O
nucleotides	O
,	O
is	O
taken	O
to	O
be	O
a	O
constant	O
m	O
,	O
independent	O
of	O
xg	O
.	O
[	O
if	O
alternatively	O
we	O
thought	O
of	O
the	O
bits	O
as	O
corresponding	O
to	O
genes	B
,	O
then	O
we	O
would	O
model	B
the	O
probability	O
of	O
the	O
discovery	O
of	O
a	O
good	B
gene	O
,	O
p	O
(	O
xg	O
=	O
0	O
!	O
xg	O
=	O
1	O
)	O
,	O
as	O
being	O
a	O
smaller	O
number	O
than	O
the	O
probability	O
of	O
a	O
deleterious	O
mutation	O
in	O
a	O
good	B
gene	O
,	O
p	O
(	O
xg	O
=	O
1	O
!	O
xg	O
=	O
0	O
)	O
.	O
]	O
variation	O
by	O
recombination	O
(	O
or	O
crossover	B
,	O
or	O
sex	O
)	O
.	O
our	O
organisms	O
are	O
haploid	O
,	O
not	O
diploid	O
.	O
they	O
enjoy	O
sex	O
by	O
recombination	O
.	O
the	O
n	O
individ-	O
uals	O
in	O
the	O
population	O
are	O
married	O
into	O
m	O
=	O
n=2	O
couples	O
,	O
at	O
random	B
,	O
and	O
each	O
couple	O
has	O
c	O
children	O
{	O
with	O
c	O
=	O
4	O
children	O
being	O
our	O
stan-	O
dard	O
assumption	O
,	O
so	O
as	O
to	O
have	O
the	O
population	O
double	O
and	O
halve	O
every	O
generation	O
,	O
as	O
before	O
.	O
the	O
c	O
children	O
’	O
s	O
genotypes	O
are	O
independent	O
given	O
the	O
parents	O
’	O
.	O
each	O
child	O
obtains	O
its	O
genotype	O
z	O
by	O
random	O
crossover	B
of	O
its	O
parents	O
’	O
genotypes	O
,	O
x	O
and	O
y.	O
the	O
simplest	O
model	B
of	O
recombination	O
has	O
no	O
linkage	O
,	O
so	O
that	O
:	O
zg	O
=	O
(	O
cid:26	O
)	O
xg	O
with	O
probability	O
1=2	O
yg	O
with	O
probability	O
1=2	O
.	O
(	O
19.2	O
)	O
once	O
the	O
m	O
c	O
progeny	O
have	O
been	O
born	O
,	O
the	O
parents	O
pass	O
away	O
,	O
the	O
(	O
cid:12	O
)	O
ttest	O
n	O
progeny	O
are	O
selected	O
by	O
natural	O
selection	O
,	O
and	O
a	O
new	O
generation	O
starts	O
.	O
we	O
now	O
study	O
these	O
two	O
models	O
of	O
variation	O
in	O
detail	O
.	O
19.2	O
rate	B
of	O
increase	O
of	O
(	O
cid:12	O
)	O
tness	O
theory	B
of	O
mutations	O
we	O
assume	O
that	O
the	O
genotype	O
of	O
an	O
individual	O
with	O
normalized	O
(	O
cid:12	O
)	O
tness	O
f	O
=	O
f=g	O
is	O
subjected	O
to	O
mutations	O
that	O
(	O
cid:13	O
)	O
ip	O
bits	O
with	O
probability	B
m.	O
we	O
(	O
cid:12	O
)	O
rst	O
show	O
that	O
if	O
the	O
average	B
normalized	O
(	O
cid:12	O
)	O
tness	O
f	O
of	O
the	O
population	O
is	O
greater	O
than	O
1=2	O
,	O
then	O
the	O
optimal	B
mutation	O
rate	B
is	O
small	O
,	O
and	O
the	O
rate	B
of	O
acquisition	O
of	O
information	O
is	O
at	O
most	O
of	O
order	O
one	O
bit	B
per	O
generation	O
.	O
since	O
it	O
is	O
easy	O
to	O
achieve	O
a	O
normalized	O
(	O
cid:12	O
)	O
tness	O
of	O
f	O
=	O
1=2	O
by	O
simple	O
muta-	O
tion	O
,	O
we	O
’	O
ll	O
assume	O
f	O
>	O
1=2	O
and	O
work	O
in	O
terms	O
of	O
the	O
excess	O
normalized	O
(	O
cid:12	O
)	O
tness	O
(	O
cid:14	O
)	O
f	O
(	O
cid:17	O
)	O
f	O
(	O
cid:0	O
)	O
1=2	O
.	O
if	O
an	O
individual	O
with	O
excess	O
normalized	O
(	O
cid:12	O
)	O
tness	O
(	O
cid:14	O
)	O
f	O
has	O
a	O
child	O
and	O
the	O
mutation	B
rate	I
m	O
is	O
small	O
,	O
the	O
probability	B
distribution	O
of	O
the	O
excess	O
normalized	O
(	O
cid:12	O
)	O
tness	O
of	O
the	O
child	O
has	O
mean	B
(	O
cid:14	O
)	O
f	O
child	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
2m	O
)	O
(	O
cid:14	O
)	O
f	O
(	O
19.3	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
272	O
and	O
variance	O
19	O
|	O
why	O
have	O
sex	O
?	O
information	B
acquisition	O
and	O
evolution	O
m	O
(	O
1	O
(	O
cid:0	O
)	O
m	O
)	O
g	O
m	O
g	O
:	O
’	O
(	O
19.4	O
)	O
if	O
the	O
population	O
of	O
parents	O
has	O
mean	B
(	O
cid:14	O
)	O
f	O
(	O
t	O
)	O
and	O
variance	O
(	O
cid:27	O
)	O
2	O
(	O
t	O
)	O
(	O
cid:17	O
)	O
(	O
cid:12	O
)	O
m=g	O
,	O
then	O
the	O
child	O
population	O
,	O
before	O
selection	O
,	O
will	O
have	O
mean	B
(	O
1	O
(	O
cid:0	O
)	O
2m	O
)	O
(	O
cid:14	O
)	O
f	O
(	O
t	O
)	O
and	O
vari-	O
ance	O
(	O
1+	O
(	O
cid:12	O
)	O
)	O
m=g	O
.	O
natural	B
selection	I
chooses	O
the	O
upper	O
half	O
of	O
this	O
distribution	B
,	O
so	O
the	O
mean	B
(	O
cid:12	O
)	O
tness	O
and	O
variance	O
of	O
(	O
cid:12	O
)	O
tness	O
at	O
the	O
next	O
generation	O
are	O
given	O
by	O
(	O
cid:14	O
)	O
f	O
(	O
t+1	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
2m	O
)	O
(	O
cid:14	O
)	O
f	O
(	O
t	O
)	O
+	O
(	O
cid:11	O
)	O
p	O
(	O
1	O
+	O
(	O
cid:12	O
)	O
)	O
r	O
m	O
g	O
;	O
(	O
19.5	O
)	O
(	O
cid:27	O
)	O
2	O
(	O
t+1	O
)	O
=	O
(	O
cid:13	O
)	O
(	O
1	O
+	O
(	O
cid:12	O
)	O
)	O
;	O
(	O
19.6	O
)	O
m	O
g	O
where	O
(	O
cid:11	O
)	O
is	O
the	O
mean	B
deviation	O
from	O
the	O
mean	B
,	O
measured	O
in	O
standard	O
devia-	O
tions	O
,	O
and	O
(	O
cid:13	O
)	O
is	O
the	O
factor	O
by	O
which	O
the	O
child	O
distribution	B
’	O
s	O
variance	B
is	O
reduced	O
by	O
selection	O
.	O
the	O
numbers	O
(	O
cid:11	O
)	O
and	O
(	O
cid:13	O
)	O
are	O
of	O
order	O
1.	O
for	O
the	O
case	O
of	O
a	O
gaussian	O
distribution	B
,	O
(	O
cid:11	O
)	O
=	O
p2=	O
(	O
cid:25	O
)	O
’	O
0:8	O
and	O
(	O
cid:13	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
2=	O
(	O
cid:25	O
)	O
)	O
’	O
0:36.	O
if	O
we	O
assume	O
that	O
the	O
variance	B
is	O
in	O
dynamic	O
equilibrium	O
,	O
i.e.	O
,	O
(	O
cid:27	O
)	O
2	O
(	O
t+1	O
)	O
’	O
(	O
cid:27	O
)	O
2	O
(	O
t	O
)	O
,	O
then	O
(	O
cid:13	O
)	O
(	O
1	O
+	O
(	O
cid:12	O
)	O
)	O
=	O
(	O
cid:12	O
)	O
;	O
so	O
(	O
1	O
+	O
(	O
cid:12	O
)	O
)	O
=	O
1	O
1	O
(	O
cid:0	O
)	O
(	O
cid:13	O
)	O
;	O
(	O
19.7	O
)	O
and	O
the	O
factor	O
(	O
cid:11	O
)	O
p	O
(	O
1	O
+	O
(	O
cid:12	O
)	O
)	O
in	O
equation	O
(	O
19.5	O
)	O
is	O
equal	O
to	O
1	O
,	O
if	O
we	O
take	O
the	O
results	O
for	O
the	O
gaussian	O
distribution	B
,	O
an	O
approximation	B
that	O
becomes	O
poorest	O
when	O
the	O
discreteness	O
of	O
(	O
cid:12	O
)	O
tness	O
becomes	O
important	O
,	O
i.e.	O
,	O
for	O
small	O
m.	O
the	O
rate	B
of	O
increase	O
of	O
normalized	O
(	O
cid:12	O
)	O
tness	O
is	O
thus	O
:	O
dt	O
’	O
(	O
cid:0	O
)	O
2m	O
(	O
cid:14	O
)	O
f	O
+r	O
m	O
which	O
,	O
assuming	O
g	O
(	O
(	O
cid:14	O
)	O
f	O
)	O
2	O
(	O
cid:29	O
)	O
1	O
,	O
is	O
maximized	O
for	O
df	O
g	O
;	O
(	O
19.8	O
)	O
at	O
which	O
point	O
,	O
mopt	O
=	O
1	O
16g	O
(	O
(	O
cid:14	O
)	O
f	O
)	O
2	O
;	O
dt	O
(	O
cid:19	O
)	O
opt	O
(	O
cid:18	O
)	O
df	O
=	O
1	O
8g	O
(	O
(	O
cid:14	O
)	O
f	O
)	O
:	O
(	O
19.9	O
)	O
(	O
19.10	O
)	O
so	O
the	O
rate	B
of	O
increase	O
of	O
(	O
cid:12	O
)	O
tness	O
f	O
=	O
f	O
g	O
is	O
at	O
most	O
df	O
dt	O
=	O
1	O
8	O
(	O
(	O
cid:14	O
)	O
f	O
)	O
per	O
generation	O
:	O
(	O
19.11	O
)	O
for	O
a	O
population	O
with	O
low	O
(	O
cid:12	O
)	O
tness	O
(	O
(	O
cid:14	O
)	O
f	O
<	O
0:125	O
)	O
,	O
the	O
rate	B
of	O
increase	O
of	O
(	O
cid:12	O
)	O
tness	O
1=pg	O
,	O
the	O
rate	B
of	O
increase	O
,	O
if	O
may	O
exceed	O
1	O
unit	O
per	O
generation	O
.	O
indeed	O
,	O
if	O
(	O
cid:14	O
)	O
f	O
 	B
m	O
=	O
1/2	O
,	O
is	O
of	O
order	O
pg	O
;	O
this	O
initial	O
spurt	O
can	O
last	O
only	O
of	O
order	O
pg	O
generations	O
.	O
for	O
(	O
cid:14	O
)	O
f	O
>	O
0:125	O
,	O
the	O
rate	B
of	O
increase	O
of	O
(	O
cid:12	O
)	O
tness	O
is	O
smaller	O
than	O
one	O
per	O
generation	O
.	O
as	O
the	O
(	O
cid:12	O
)	O
tness	O
approaches	O
g	O
,	O
the	O
optimal	B
mutation	O
rate	B
tends	O
to	O
m	O
=	O
1=	O
(	O
4g	O
)	O
,	O
so	O
that	O
an	O
average	B
of	O
1=4	O
bits	O
are	O
(	O
cid:13	O
)	O
ipped	O
per	O
genotype	O
,	O
and	O
the	O
rate	B
of	O
increase	O
of	O
(	O
cid:12	O
)	O
tness	O
is	O
also	O
equal	O
to	O
1=4	O
;	O
information	B
is	O
gained	O
at	O
a	O
rate	B
of	O
about	O
0:5	O
bits	O
per	O
generation	O
.	O
it	O
takes	O
about	O
2g	O
generations	O
for	O
the	O
genotypes	O
of	O
all	O
individuals	O
in	O
the	O
population	O
to	O
attain	O
perfection	O
.	O
for	O
(	O
cid:12	O
)	O
xed	O
m	O
,	O
the	O
(	O
cid:12	O
)	O
tness	O
is	O
given	O
by	O
(	O
cid:14	O
)	O
f	O
(	O
t	O
)	O
=	O
1	O
2pmg	O
(	O
1	O
(	O
cid:0	O
)	O
c	O
e	O
(	O
cid:0	O
)	O
2mt	O
)	O
;	O
(	O
19.12	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
19.2	O
:	O
rate	B
of	O
increase	O
of	O
(	O
cid:12	O
)	O
tness	O
273	O
no	O
sex	O
sex	O
histogram	O
of	O
parents	O
’	O
(	O
cid:12	O
)	O
tness	O
histogram	O
of	O
children	O
’	O
s	O
(	O
cid:12	O
)	O
tness	O
selected	O
children	O
’	O
s	O
(	O
cid:12	O
)	O
tness	O
figure	O
19.1.	O
why	O
sex	O
is	O
better	O
than	O
sex-free	O
reproduction	O
.	O
if	O
mutations	O
are	O
used	O
to	O
create	O
variation	O
among	O
children	O
,	O
then	O
it	O
is	O
unavoidable	O
that	O
the	O
average	B
(	O
cid:12	O
)	O
tness	O
of	O
the	O
children	O
is	O
lower	O
than	O
the	O
parents	O
’	O
(	O
cid:12	O
)	O
tness	O
;	O
the	O
greater	O
the	O
variation	O
,	O
the	O
greater	O
the	O
average	B
de	O
(	O
cid:12	O
)	O
cit	O
.	O
selection	O
bumps	O
up	O
the	O
mean	B
(	O
cid:12	O
)	O
tness	O
again	O
.	O
in	O
contrast	O
,	O
recombination	O
produces	O
variation	O
without	O
a	O
decrease	O
in	O
average	O
(	O
cid:12	O
)	O
tness	O
.	O
the	O
typical	B
amount	O
of	O
variation	O
scales	O
as	O
pg	O
,	O
where	O
g	O
is	O
the	O
genome	B
size	O
,	O
so	O
after	O
selection	O
,	O
the	O
average	B
(	O
cid:12	O
)	O
tness	O
rises	O
by	O
o	O
(	O
pg	O
)	O
.	O
subject	O
to	O
the	O
constraint	O
(	O
cid:14	O
)	O
f	O
(	O
t	O
)	O
(	O
cid:20	O
)	O
1=2	O
,	O
where	O
c	O
is	O
a	O
constant	O
of	O
integration	O
,	O
equal	O
to	O
1	O
if	O
f	O
(	O
0	O
)	O
=	O
1=2	O
.	O
if	O
the	O
mean	B
number	O
of	O
bits	O
(	O
cid:13	O
)	O
ipped	O
per	O
genotype	O
,	O
mg	O
,	O
exceeds	O
1	O
,	O
then	O
the	O
(	O
cid:12	O
)	O
tness	O
f	O
approaches	O
an	O
equilibrium	O
value	O
feqm	O
=	O
(	O
1=2	O
+	O
1=	O
(	O
2pmg	O
)	O
)	O
g.	O
this	O
theory	B
is	O
somewhat	O
inaccurate	O
in	O
that	O
the	O
true	O
probability	B
distribu-	O
tion	O
of	O
(	O
cid:12	O
)	O
tness	O
is	O
non-gaussian	O
,	O
asymmetrical	O
,	O
and	O
quantized	O
to	O
integer	O
values	O
.	O
all	O
the	O
same	O
,	O
the	O
predictions	O
of	O
the	O
theory	O
are	O
not	O
grossly	O
at	O
variance	B
with	O
the	O
results	O
of	O
simulations	O
described	O
below	O
.	O
theory	B
of	O
sex	O
the	O
analysis	B
of	O
the	O
sexual	O
population	O
becomes	O
tractable	O
with	O
two	O
approxi-	O
mations	O
:	O
(	O
cid:12	O
)	O
rst	O
,	O
we	O
assume	O
that	O
the	O
gene-pool	O
mixes	O
su	O
(	O
cid:14	O
)	O
ciently	O
rapidly	O
that	O
correlations	B
between	O
genes	B
can	O
be	O
neglected	O
;	O
second	O
,	O
we	O
assume	O
homogeneity	O
,	O
i.e.	O
,	O
that	O
the	O
fraction	O
fg	O
of	O
bits	O
g	O
that	O
are	O
in	O
the	O
good	B
state	O
is	O
the	O
same	O
,	O
f	O
(	O
t	O
)	O
,	O
for	O
all	O
g.	O
given	O
these	O
assumptions	B
,	O
if	O
two	O
parents	O
of	O
(	O
cid:12	O
)	O
tness	O
f	O
=	O
f	O
g	O
mate	O
,	O
the	O
prob-	O
ability	O
distribution	B
of	O
their	O
children	O
’	O
s	O
(	O
cid:12	O
)	O
tness	O
has	O
mean	B
equal	O
to	O
the	O
parents	O
’	O
(	O
cid:12	O
)	O
tness	O
,	O
f	O
;	O
the	O
variation	O
produced	O
by	O
sex	O
does	O
not	O
reduce	O
the	O
average	B
(	O
cid:12	O
)	O
tness	O
.	O
the	O
standard	B
deviation	I
of	O
the	O
(	O
cid:12	O
)	O
tness	O
of	O
the	O
children	O
scales	O
as	O
pgf	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
.	O
since	O
,	O
after	O
selection	O
,	O
the	O
increase	O
in	O
(	O
cid:12	O
)	O
tness	O
is	O
proportional	O
to	O
this	O
standard	B
deviation	I
,	O
the	O
(	O
cid:12	O
)	O
tness	O
increase	O
per	O
generation	O
scales	O
as	O
the	O
square	B
root	O
of	O
the	O
size	O
of	O
the	O
genome	O
,	O
pg	O
.	O
as	O
shown	O
in	O
box	O
19.2	O
,	O
the	O
mean	B
(	O
cid:12	O
)	O
tness	O
(	O
cid:22	O
)	O
f	O
=	O
f	O
g	O
evolves	O
in	O
accordance	O
with	O
the	O
di	O
(	O
cid:11	O
)	O
erential	O
equation	O
:	O
d	O
(	O
cid:22	O
)	O
f	O
dt	O
’	O
(	O
cid:17	O
)	O
pf	O
(	O
t	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
(	O
t	O
)	O
)	O
g	O
;	O
where	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
p2=	O
(	O
(	O
cid:25	O
)	O
+	O
2	O
)	O
.	O
the	O
solution	O
of	O
this	O
equation	O
is	O
(	O
t	O
+	O
c	O
)	O
(	O
cid:19	O
)	O
(	O
cid:21	O
)	O
;	O
for	O
t	O
+	O
c	O
2	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
(	O
cid:25	O
)	O
pg=	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
,	O
(	O
19.14	O
)	O
where	O
c	O
is	O
a	O
constant	O
of	O
integration	O
,	O
c	O
=	O
sin	O
(	O
cid:0	O
)	O
1	O
(	O
2f	O
(	O
0	O
)	O
(	O
cid:0	O
)	O
1	O
)	O
.	O
so	O
this	O
idealized	O
system	O
reaches	O
a	O
state	O
of	O
eugenic	O
perfection	O
(	O
f	O
=	O
1	O
)	O
within	O
a	O
(	O
cid:12	O
)	O
nite	O
time	O
:	O
(	O
(	O
cid:25	O
)	O
=	O
(	O
cid:17	O
)	O
)	O
pg	O
generations	O
.	O
pg=	O
(	O
cid:17	O
)	O
;	O
(	O
cid:25	O
)	O
1	O
2	O
(	O
cid:20	O
)	O
1	O
+	O
sin	O
(	O
cid:18	O
)	O
(	O
cid:17	O
)	O
pg	O
(	O
19.13	O
)	O
f	O
(	O
t	O
)	O
=	O
2	O
2	O
simulations	O
figure	O
19.3a	O
shows	O
the	O
(	O
cid:12	O
)	O
tness	O
of	O
a	O
sexual	O
population	O
of	O
n	O
=	O
1000	O
individ-	O
uals	O
with	O
a	O
genome	B
size	O
of	O
g	O
=	O
1000	O
starting	O
from	O
a	O
random	B
initial	O
state	O
with	O
normalized	O
(	O
cid:12	O
)	O
tness	O
0:5.	O
it	O
also	O
shows	O
the	O
theoretical	O
curve	O
f	O
(	O
t	O
)	O
g	O
from	O
equation	O
(	O
19.14	O
)	O
,	O
which	O
(	O
cid:12	O
)	O
ts	O
remarkably	O
well	O
.	O
in	O
contrast	O
,	O
(	O
cid:12	O
)	O
gures	O
19.3	O
(	O
b	O
)	O
and	O
(	O
c	O
)	O
show	O
the	O
evolving	O
(	O
cid:12	O
)	O
tness	O
when	O
variation	O
is	O
produced	O
by	O
mutation	O
at	O
rates	O
m	O
=	O
0:25=g	O
and	O
m	O
=	O
6=g	O
respectively	O
.	O
note	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
in	O
the	O
horizontal	O
scales	O
from	O
panel	O
(	O
a	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
274	O
19	O
|	O
why	O
have	O
sex	O
?	O
information	B
acquisition	O
and	O
evolution	O
how	O
does	O
f	O
(	O
t+1	O
)	O
depend	O
on	O
f	O
(	O
t	O
)	O
?	O
let	O
’	O
s	O
(	O
cid:12	O
)	O
rst	O
assume	O
the	O
two	O
parents	O
of	O
a	O
child	O
both	O
have	O
exactly	O
f	O
(	O
t	O
)	O
g	O
good	B
bits	O
,	O
and	O
,	O
by	O
our	O
homogeneity	O
assumption	O
,	O
that	O
those	O
bits	O
are	O
independent	O
random	O
subsets	O
of	O
the	O
g	O
bits	O
.	O
the	O
number	O
of	O
bits	O
that	O
are	O
good	B
in	O
both	O
parents	O
is	O
roughly	O
f	O
(	O
t	O
)	O
2g	O
,	O
and	O
the	O
number	O
that	O
are	O
good	B
in	O
one	O
parent	B
only	O
is	O
roughly	O
2f	O
(	O
t	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
(	O
t	O
)	O
)	O
g	O
,	O
so	O
the	O
(	O
cid:12	O
)	O
tness	O
of	O
the	O
child	O
will	O
be	O
f	O
(	O
t	O
)	O
2g	O
plus	O
the	O
sum	O
of	O
2f	O
(	O
t	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
(	O
t	O
)	O
)	O
g	O
fair	O
coin	B
(	O
cid:13	O
)	O
ips	O
,	O
which	O
has	O
a	O
binomial	B
distribution	I
of	O
mean	B
f	O
(	O
t	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
(	O
t	O
)	O
)	O
g	O
and	O
variance	O
2	O
f	O
(	O
t	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
(	O
t	O
)	O
)	O
g.	O
the	O
(	O
cid:12	O
)	O
tness	O
of	O
a	O
child	O
is	O
thus	O
roughly	O
distributed	O
as	O
1	O
fchild	O
(	O
cid:24	O
)	O
normal	B
mean	O
=	O
f	O
(	O
t	O
)	O
g	O
;	O
variance	B
=	O
1	O
2	O
f	O
(	O
t	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
(	O
t	O
)	O
)	O
g	O
:	O
box	B
19.2.	O
details	O
of	O
the	O
theory	O
of	O
sex	O
.	O
the	O
important	O
property	O
of	O
this	O
distribution	B
,	O
contrasted	O
with	O
the	O
distribution	B
under	O
mutation	O
,	O
is	O
that	O
the	O
mean	B
(	O
cid:12	O
)	O
tness	O
is	O
equal	O
to	O
the	O
parents	O
’	O
(	O
cid:12	O
)	O
tness	O
;	O
the	O
variation	O
produced	O
by	O
sex	O
does	O
not	O
reduce	O
the	O
average	B
(	O
cid:12	O
)	O
tness	O
.	O
if	O
we	O
include	O
the	O
parental	O
population	O
’	O
s	O
variance	B
,	O
which	O
we	O
will	O
write	O
as	O
(	O
cid:27	O
)	O
2	O
(	O
t	O
)	O
=	O
(	O
cid:12	O
)	O
(	O
t	O
)	O
1	O
2	O
f	O
(	O
t	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
(	O
t	O
)	O
)	O
g	O
,	O
the	O
children	O
’	O
s	O
(	O
cid:12	O
)	O
tnesses	O
are	O
distributed	O
as	O
1	O
2	O
fchild	O
(	O
cid:24	O
)	O
normal	B
mean	O
=	O
f	O
(	O
t	O
)	O
g	O
;	O
variance	B
=	O
1	O
+	O
f	O
(	O
t	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
(	O
t	O
)	O
)	O
g	O
:	O
natural	B
selection	I
selects	O
the	O
children	O
on	O
the	O
upper	O
side	O
of	O
this	O
distribution	B
.	O
the	O
mean	B
increase	O
in	O
(	O
cid:12	O
)	O
tness	O
will	O
be	O
(	O
cid:12	O
)	O
2	O
(	O
cid:22	O
)	O
f	O
(	O
t+1	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
f	O
(	O
t	O
)	O
=	O
[	O
(	O
cid:11	O
)	O
(	O
1	O
+	O
(	O
cid:12	O
)	O
=2	O
)	O
1=2=p2	O
]	O
and	O
the	O
variance	B
of	O
the	O
surviving	O
children	O
will	O
be	O
f	O
(	O
t	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
(	O
t	O
)	O
)	O
g	O
;	O
(	O
cid:27	O
)	O
2	O
(	O
t	O
+	O
1	O
)	O
=	O
(	O
cid:13	O
)	O
(	O
1	O
+	O
(	O
cid:12	O
)	O
=2	O
)	O
1	O
2	O
f	O
(	O
t	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
(	O
t	O
)	O
)	O
g	O
;	O
where	O
(	O
cid:11	O
)	O
=	O
then	O
the	O
factor	O
in	O
(	O
19.2	O
)	O
is	O
2=	O
(	O
cid:25	O
)	O
and	O
(	O
cid:13	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
2=	O
(	O
cid:25	O
)	O
)	O
.	O
if	O
there	O
is	O
dynamic	O
equilibrium	O
[	O
(	O
cid:27	O
)	O
2	O
(	O
t	O
+	O
1	O
)	O
=	O
(	O
cid:27	O
)	O
2	O
(	O
t	O
)	O
]	O
(	O
cid:11	O
)	O
(	O
1	O
+	O
(	O
cid:12	O
)	O
=2	O
)	O
1=2=p2	O
=	O
2	O
(	O
(	O
cid:25	O
)	O
+	O
2	O
)	O
’	O
0:62	O
:	O
2=	O
(	O
(	O
cid:25	O
)	O
+	O
2	O
)	O
,	O
we	O
conclude	O
that	O
,	O
under	O
sex	O
and	O
natural	O
selection	O
,	O
the	O
mean	B
(	O
cid:12	O
)	O
tness	O
of	O
the	O
population	O
increases	O
at	O
a	O
rate	B
proportional	O
to	O
the	O
square	B
root	O
of	O
the	O
size	O
of	O
the	O
genome	O
,	O
de	O
(	O
cid:12	O
)	O
ning	O
this	O
constant	O
to	O
be	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
	O
d	O
(	O
cid:22	O
)	O
f	O
dt	O
’	O
(	O
cid:17	O
)	O
f	O
(	O
t	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
(	O
t	O
)	O
)	O
g	O
bits	O
per	O
generation	O
:	O
 	B
	O
 	B
 	I
	O
	O
	O
	O
	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
19.3	O
:	O
the	O
maximal	O
tolerable	O
mutation	B
rate	I
275	O
figure	O
19.3.	O
fitness	O
as	O
a	O
function	B
of	O
time	O
.	O
the	O
genome	B
size	O
is	O
g	O
=	O
1000.	O
the	O
dots	O
show	O
the	O
(	O
cid:12	O
)	O
tness	O
of	O
six	O
randomly	O
selected	O
individuals	O
from	O
the	O
birth	O
population	O
at	O
each	O
generation	O
.	O
the	O
initial	O
population	O
of	O
n	O
=	O
1000	O
had	O
randomly	O
generated	O
genomes	O
with	O
f	O
(	O
0	O
)	O
=	O
0:5	O
(	O
exactly	O
)	O
.	O
(	O
a	O
)	O
variation	O
produced	O
by	O
sex	O
alone	O
.	O
line	O
shows	O
theoretical	O
curve	O
(	O
19.14	O
)	O
for	O
in	O
(	O
cid:12	O
)	O
nite	O
homogeneous	B
population	O
.	O
(	O
b	O
,	O
c	O
)	O
variation	O
produced	O
by	O
mutation	O
,	O
with	O
and	O
without	O
sex	O
,	O
when	O
the	O
mutation	B
rate	I
is	O
mg	O
=	O
0:25	O
(	O
b	O
)	O
or	O
6	O
(	O
c	O
)	O
bits	O
per	O
genome	B
.	O
the	O
dashed	O
line	O
shows	O
the	O
curve	O
(	O
19.12	O
)	O
.	O
figure	O
19.4.	O
maximal	O
tolerable	O
mutation	B
rate	I
,	O
shown	O
as	O
number	O
of	O
errors	O
per	O
genome	B
(	O
mg	O
)	O
,	O
versus	O
normalized	O
(	O
cid:12	O
)	O
tness	O
f	O
=	O
f=g	O
.	O
left	O
panel	O
:	O
genome	B
size	O
g	O
=	O
1000	O
;	O
right	O
:	O
g	O
=	O
100	O
000.	O
independent	O
of	O
genome	B
size	O
,	O
a	O
parthenogenetic	O
species	B
(	O
no	O
sex	O
)	O
can	O
tolerate	O
only	O
of	O
order	O
1	O
error	O
per	O
genome	B
per	O
generation	O
;	O
a	O
species	B
that	O
uses	O
recombination	O
(	O
sex	O
)	O
can	O
tolerate	O
far	O
greater	O
mutation	O
rates	O
.	O
1000	O
900	O
800	O
700	O
600	O
500	O
(	O
a	O
)	O
0	O
10	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
1000	O
900	O
800	O
700	O
600	O
500	O
(	O
b	O
)	O
sex	O
no	O
sex	O
0	O
200	O
400	O
600	O
800	O
1000	O
1200	O
1400	O
1600	O
1000	O
900	O
800	O
700	O
600	O
500	O
0	O
(	O
c	O
)	O
sex	O
no	O
sex	O
50	O
100	O
150	O
200	O
250	O
300	O
350	O
g	O
=	O
1000	O
g	O
=	O
100	O
000	O
mg	O
20	O
15	O
10	O
5	O
0	O
with	O
sex	O
without	O
sex	O
50	O
45	O
40	O
35	O
30	O
25	O
20	O
15	O
10	O
5	O
0	O
with	O
sex	O
without	O
sex	O
0.65	O
0.7	O
0.75	O
0.8	O
0.85	O
0.9	O
0.95	O
1	O
0.65	O
0.7	O
0.75	O
0.8	O
0.85	O
0.9	O
0.95	O
1	O
f	O
f	O
exercise	O
19.1	O
.	O
[	O
3	O
,	O
p.280	O
]	O
dependence	O
on	O
population	O
size	O
.	O
how	O
do	O
the	O
results	O
for	O
a	O
sexual	O
population	O
depend	O
on	O
the	O
population	O
size	O
?	O
we	O
anticipate	O
that	O
there	O
is	O
a	O
minimum	O
population	O
size	O
above	O
which	O
the	O
theory	B
of	O
sex	O
is	O
accurate	O
.	O
how	O
is	O
that	O
minimum	O
population	O
size	O
related	O
to	O
g	O
?	O
exercise	O
19.2	O
.	O
[	O
3	O
]	O
dependence	O
on	O
crossover	O
mechanism	O
.	O
in	O
the	O
simple	O
model	O
of	O
sex	O
,	O
each	O
bit	B
is	O
taken	O
at	O
random	B
from	O
one	O
of	O
the	O
two	O
parents	O
,	O
that	O
is	O
,	O
we	O
allow	O
crossovers	O
to	O
occur	O
with	O
probability	O
50	O
%	O
between	O
any	O
two	O
adjacent	O
nucleotides	O
.	O
how	O
is	O
the	O
model	B
a	O
(	O
cid:11	O
)	O
ected	O
(	O
a	O
)	O
if	O
the	O
crossover	B
probability	O
is	O
smaller	O
?	O
(	O
b	O
)	O
if	O
crossovers	O
occur	O
exclusively	O
at	O
hot-spots	O
located	O
every	O
d	O
bits	O
along	O
the	O
genome	B
?	O
19.3	O
the	O
maximal	O
tolerable	O
mutation	B
rate	I
what	O
if	O
we	O
combine	O
the	O
two	O
models	O
of	O
variation	O
?	O
what	O
is	O
the	O
maximum	O
mutation	O
rate	B
that	O
can	O
be	O
tolerated	O
by	O
a	O
species	B
that	O
has	O
sex	O
?	O
the	O
rate	B
of	O
increase	O
of	O
(	O
cid:12	O
)	O
tness	O
is	O
given	O
by	O
df	O
dt	O
’	O
(	O
cid:0	O
)	O
2m	O
(	O
cid:14	O
)	O
f	O
+	O
(	O
cid:17	O
)	O
p2r	O
m	O
+	O
f	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
=2	O
g	O
;	O
(	O
19.15	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
276	O
19	O
|	O
why	O
have	O
sex	O
?	O
information	B
acquisition	O
and	O
evolution	O
which	O
is	O
positive	O
if	O
the	O
mutation	B
rate	I
satis	O
(	O
cid:12	O
)	O
es	O
m	O
<	O
(	O
cid:17	O
)	O
r	O
f	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
g	O
:	O
(	O
19.16	O
)	O
let	O
us	O
compare	O
this	O
rate	B
with	O
the	O
result	O
in	O
the	O
absence	O
of	O
sex	O
,	O
which	O
,	O
from	O
equation	O
(	O
19.8	O
)	O
,	O
is	O
that	O
the	O
maximum	O
tolerable	O
mutation	B
rate	I
is	O
(	O
19.17	O
)	O
the	O
tolerable	O
mutation	B
rate	I
with	O
sex	O
is	O
of	O
order	O
pg	O
times	O
greater	O
than	O
that	O
without	O
sex	O
!	O
(	O
2	O
(	O
cid:14	O
)	O
f	O
)	O
2	O
:	O
m	O
<	O
1	O
1	O
g	O
a	O
parthenogenetic	O
(	O
non-sexual	O
)	O
species	B
could	O
try	O
to	O
wriggle	O
out	O
of	O
this	O
bound	B
on	O
its	O
mutation	B
rate	I
by	O
increasing	O
its	O
litter	O
sizes	O
.	O
but	O
if	O
mutation	O
(	O
cid:13	O
)	O
ips	O
on	O
average	O
mg	O
bits	O
,	O
the	O
probability	B
that	O
no	O
bits	O
are	O
(	O
cid:13	O
)	O
ipped	O
in	O
one	O
genome	B
is	O
roughly	O
e	O
(	O
cid:0	O
)	O
mg	O
,	O
so	O
a	O
mother	O
needs	O
to	O
have	O
roughly	O
emg	O
o	O
(	O
cid:11	O
)	O
spring	B
in	O
order	O
to	O
have	O
a	O
good	B
chance	O
of	O
having	O
one	O
child	O
with	O
the	O
same	O
(	O
cid:12	O
)	O
tness	O
as	O
her	O
.	O
the	O
litter	O
size	O
of	O
a	O
non-sexual	O
species	B
thus	O
has	O
to	O
be	O
exponential	B
in	O
mg	O
(	O
if	O
mg	O
is	O
bigger	O
than	O
1	O
)	O
,	O
if	O
the	O
species	B
is	O
to	O
persist	O
.	O
so	O
the	O
maximum	O
tolerable	O
mutation	B
rate	I
is	O
pinned	O
close	O
to	O
1=g	O
,	O
for	O
a	O
non-	O
sexual	O
species	B
,	O
whereas	O
it	O
is	O
a	O
larger	O
number	O
of	O
order	O
1=pg	O
,	O
for	O
a	O
species	B
with	O
recombination	O
.	O
turning	O
these	O
results	O
around	O
,	O
we	O
can	O
predict	O
the	O
largest	O
possible	O
genome	B
size	O
for	O
a	O
given	O
(	O
cid:12	O
)	O
xed	O
mutation	B
rate	I
,	O
m.	O
for	O
a	O
parthenogenetic	O
species	B
,	O
the	O
largest	O
genome	B
size	O
is	O
of	O
order	O
1=m	O
,	O
and	O
for	O
a	O
sexual	O
species	B
,	O
1=m2	O
.	O
taking	O
the	O
(	O
cid:12	O
)	O
gure	O
m	O
=	O
10	O
(	O
cid:0	O
)	O
8	O
as	O
the	O
mutation	B
rate	I
per	O
nucleotide	B
per	O
generation	O
(	O
eyre-	O
walker	O
and	O
keightley	O
,	O
1999	O
)	O
,	O
and	O
allowing	O
for	O
a	O
maximum	O
brood	O
size	O
of	O
20	O
000	O
(	O
that	O
is	O
,	O
mg	O
’	O
10	O
)	O
,	O
we	O
predict	O
that	O
all	O
species	B
with	O
more	O
than	O
g	O
=	O
109	O
coding	O
nucleotides	O
make	O
at	O
least	O
occasional	O
use	O
of	O
recombination	O
.	O
if	O
the	O
brood	O
size	O
is	O
12	O
,	O
then	O
this	O
number	O
falls	O
to	O
g	O
=	O
2:5	O
(	O
cid:2	O
)	O
108	O
.	O
19.4	O
fitness	O
increase	O
and	O
information	O
acquisition	O
for	O
this	O
simple	O
model	O
it	O
is	O
possible	O
to	O
relate	O
increasing	O
(	O
cid:12	O
)	O
tness	O
to	O
information	B
acquisition	O
.	O
if	O
the	O
bits	O
are	O
set	B
at	O
random	B
,	O
the	O
(	O
cid:12	O
)	O
tness	O
is	O
roughly	O
f	O
=	O
g=2	O
.	O
if	O
evolution	B
leads	O
to	O
a	O
population	O
in	O
which	O
all	O
individuals	O
have	O
the	O
maximum	O
(	O
cid:12	O
)	O
tness	O
f	O
=	O
g	O
,	O
then	O
g	O
bits	O
of	O
information	B
have	O
been	O
acquired	O
by	O
the	O
species	B
,	O
namely	O
for	O
each	O
bit	B
xg	O
,	O
the	O
species	B
has	O
(	O
cid:12	O
)	O
gured	O
out	O
which	O
of	O
the	O
two	O
states	O
is	O
the	O
better	O
.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
information	B
acquired	O
at	O
an	O
intermediate	O
(	O
cid:12	O
)	O
tness	O
to	O
be	O
the	O
amount	O
of	O
selection	O
(	O
measured	O
in	O
bits	O
)	O
required	O
to	O
select	O
the	O
perfect	B
state	O
from	O
the	O
gene	O
pool	O
.	O
let	O
a	O
fraction	O
fg	O
of	O
the	O
population	O
have	O
xg	O
=	O
1.	O
because	O
log2	O
(	O
1=f	O
)	O
is	O
the	O
information	B
required	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
black	B
ball	O
in	O
an	O
urn	B
containing	O
black	B
and	O
white	B
balls	O
in	O
the	O
ratio	O
f	O
:	O
1	O
(	O
cid:0	O
)	O
f	O
,	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
information	B
acquired	O
to	O
be	O
bits	O
:	O
(	O
19.18	O
)	O
log2	O
fg	O
1=2	O
i	O
=xg	O
if	O
all	O
the	O
fractions	O
fg	O
are	O
equal	O
to	O
f=g	O
,	O
then	O
2f	O
g	O
i	O
=	O
g	O
log2	O
;	O
which	O
is	O
well	O
approximated	O
by	O
~i	O
(	O
cid:17	O
)	O
2	O
(	O
f	O
(	O
cid:0	O
)	O
g=2	O
)	O
:	O
(	O
19.19	O
)	O
(	O
19.20	O
)	O
the	O
rate	B
of	O
information	B
acquisition	O
is	O
thus	O
roughly	O
two	O
times	O
the	O
rate	B
of	O
in-	O
crease	O
of	O
(	O
cid:12	O
)	O
tness	O
in	O
the	O
population	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
277	O
19.5	O
:	O
discussion	O
19.5	O
discussion	O
these	O
results	O
quantify	O
the	O
well	O
known	O
argument	O
for	O
why	O
species	B
reproduce	O
by	O
sex	O
with	O
recombination	O
,	O
namely	O
that	O
recombination	O
allows	O
useful	O
muta-	O
tions	O
to	O
spread	O
more	O
rapidly	O
through	O
the	O
species	B
and	O
allows	O
deleterious	O
muta-	O
tions	O
to	O
be	O
more	O
rapidly	O
cleared	O
from	O
the	O
population	O
(	O
maynard	O
smith	O
,	O
1978	O
;	O
felsenstein	O
,	O
1985	O
;	O
maynard	O
smith	O
,	O
1988	O
;	O
maynard	O
smith	O
and	O
sz	O
(	O
cid:19	O
)	O
athmary	O
,	O
1995	O
)	O
.	O
a	O
population	O
that	O
reproduces	O
by	O
recombination	O
can	O
acquire	O
informa-	O
tion	O
from	O
natural	O
selection	O
at	O
a	O
rate	B
of	O
order	O
pg	O
times	O
faster	O
than	O
a	O
partheno-	O
genetic	B
population	O
,	O
and	O
it	O
can	O
tolerate	O
a	O
mutation	B
rate	I
that	O
is	O
of	O
order	O
pg	O
times	O
greater	O
.	O
for	O
genomes	O
of	O
size	O
g	O
’	O
108	O
coding	O
nucleotides	O
,	O
this	O
factor	O
of	O
pg	O
is	O
substantial	O
.	O
this	O
enormous	O
advantage	O
conferred	O
by	O
sex	O
has	O
been	O
noted	O
before	O
by	O
kon-	O
drashov	O
(	O
1988	O
)	O
,	O
but	O
this	O
meme	O
,	O
which	O
kondrashov	O
calls	O
‘	O
the	O
deterministic	B
mutation	O
hypothesis	O
’	O
,	O
does	O
not	O
seem	O
to	O
have	O
di	O
(	O
cid:11	O
)	O
used	O
throughout	O
the	O
evolu-	O
tionary	O
research	O
community	O
,	O
as	O
there	O
are	O
still	O
numerous	O
papers	O
in	O
which	O
the	O
prevalence	O
of	O
sex	O
is	O
viewed	O
as	O
a	O
mystery	O
to	O
be	O
explained	O
by	O
elaborate	O
mecha-	O
nisms	O
.	O
‘	O
the	O
cost	B
of	I
males	I
’	O
{	O
stability	O
of	O
a	O
gene	O
for	O
sex	O
or	O
parthenogenesis	B
why	O
do	O
people	O
declare	O
sex	O
to	O
be	O
a	O
mystery	O
?	O
the	O
main	O
motivation	O
for	O
being	O
mysti	O
(	O
cid:12	O
)	O
ed	O
is	O
an	O
idea	O
called	O
the	O
‘	O
cost	B
of	I
males	I
’	O
.	O
sexual	O
reproduction	O
is	O
disad-	O
vantageous	O
compared	O
with	O
asexual	O
reproduction	O
,	O
it	O
’	O
s	O
argued	O
,	O
because	O
of	O
every	O
two	O
o	O
(	O
cid:11	O
)	O
spring	B
produced	O
by	O
sex	O
,	O
one	O
(	O
on	O
average	O
)	O
is	O
a	O
useless	O
male	B
,	O
incapable	O
of	O
child-bearing	O
,	O
and	O
only	O
one	O
is	O
a	O
productive	O
female	B
.	O
in	O
the	O
same	O
time	O
,	O
a	O
parthenogenetic	O
mother	O
could	O
give	O
birth	O
to	O
two	O
female	B
clones	O
.	O
to	O
put	O
it	O
an-	O
other	O
way	O
,	O
the	O
big	O
advantage	O
of	O
parthenogenesis	O
,	O
from	O
the	O
point	O
of	O
view	O
of	O
the	O
individual	O
,	O
is	O
that	O
one	O
is	O
able	O
to	O
pass	O
on	O
100	O
%	O
of	O
one	O
’	O
s	O
genome	B
to	O
one	O
’	O
s	O
children	O
,	O
instead	O
of	O
only	O
50	O
%	O
.	O
thus	O
if	O
there	O
were	O
two	O
versions	O
of	O
a	O
species	B
,	O
one	O
reproducing	O
with	O
and	O
one	O
without	O
sex	O
,	O
the	O
single	O
mothers	O
would	O
be	O
expected	O
to	O
outstrip	O
their	O
sexual	O
cousins	O
.	O
the	O
simple	O
model	O
presented	O
thus	O
far	O
did	O
not	O
include	O
either	O
genders	O
or	O
the	O
ability	O
to	O
convert	O
from	O
sexual	O
reproduction	O
to	O
asexual	O
,	O
but	O
we	O
can	O
easily	O
modify	O
the	O
model	B
.	O
we	O
modify	O
the	O
model	B
so	O
that	O
one	O
of	O
the	O
g	O
bits	O
in	O
the	O
genome	B
determines	O
whether	O
an	O
individual	O
prefers	O
to	O
reproduce	O
parthenogenetically	O
(	O
x	O
=	O
1	O
)	O
or	O
sex-	O
ually	O
(	O
x	O
=	O
0	O
)	O
.	O
the	O
results	O
depend	O
on	O
the	O
number	O
of	O
children	O
had	O
by	O
a	O
single	O
parthenogenetic	O
mother	O
,	O
kp	O
and	O
the	O
number	O
of	O
children	O
born	O
by	O
a	O
sexual	O
couple	O
,	O
ks	O
.	O
both	O
(	O
kp	O
=	O
2	O
,	O
ks	O
=	O
4	O
)	O
and	O
(	O
kp	O
=	O
4	O
,	O
ks	O
=	O
4	O
)	O
are	O
reasonable	O
mod-	O
els	O
.	O
the	O
former	O
(	O
kp	O
=	O
2	O
,	O
ks	O
=	O
4	O
)	O
would	O
seem	O
most	O
appropriate	O
in	O
the	O
case	O
of	O
unicellular	O
organisms	O
,	O
where	O
the	O
cytoplasm	O
of	O
both	O
parents	O
goes	O
into	O
the	O
children	O
.	O
the	O
latter	O
(	O
kp	O
=	O
4	O
,	O
ks	O
=	O
4	O
)	O
is	O
appropriate	O
if	O
the	O
children	O
are	O
solely	O
nurtured	O
by	O
one	O
of	O
the	O
parents	O
,	O
so	O
single	O
mothers	O
have	O
just	O
as	O
many	O
o	O
(	O
cid:11	O
)	O
spring	B
as	O
a	O
sexual	O
pair	O
.	O
i	O
concentrate	O
on	O
the	O
latter	O
model	B
,	O
since	O
it	O
gives	O
the	O
greatest	O
advantage	O
to	O
the	O
parthenogens	O
,	O
who	O
are	O
supposedly	O
expected	O
to	O
outbreed	O
the	O
sexual	O
community	O
.	O
because	O
parthenogens	O
have	O
four	O
children	O
per	O
generation	O
,	O
the	O
maximum	O
tolerable	O
mutation	B
rate	I
for	O
them	O
is	O
twice	O
the	O
expression	O
(	O
19.17	O
)	O
derived	O
before	O
for	O
kp	O
=	O
2.	O
if	O
the	O
(	O
cid:12	O
)	O
tness	O
is	O
large	O
,	O
the	O
maximum	O
tolerable	O
rate	B
is	O
mg	O
’	O
2.	O
initially	O
the	O
genomes	O
are	O
set	B
randomly	O
with	O
f	O
=	O
g=2	O
,	O
with	O
half	O
of	O
the	O
pop-	O
ulation	O
having	O
the	O
gene	O
for	O
parthenogenesis	O
.	O
figure	O
19.5	O
shows	O
the	O
outcome	O
.	O
during	O
the	O
‘	O
learning	B
’	O
phase	O
of	O
evolution	B
,	O
in	O
which	O
the	O
(	O
cid:12	O
)	O
tness	O
is	O
increasing	O
rapidly	O
,	O
pockets	O
of	O
parthenogens	O
appear	O
brie	O
(	O
cid:13	O
)	O
y	O
,	O
but	O
then	O
disappear	O
within	O
a	O
couple	O
of	O
generations	O
as	O
their	O
sexual	O
cousins	O
overtake	O
them	O
in	O
(	O
cid:12	O
)	O
tness	O
and	O
 	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
278	O
19	O
|	O
why	O
have	O
sex	O
?	O
information	B
acquisition	O
and	O
evolution	O
figure	O
19.5.	O
results	O
when	O
there	O
is	O
a	O
gene	O
for	O
parthenogenesis	O
,	O
and	O
no	O
interbreeding	O
,	O
and	O
single	O
mothers	O
produce	O
as	O
many	O
children	O
as	O
sexual	O
couples	O
.	O
g	O
=	O
1000	O
,	O
n	O
=	O
1000	O
.	O
(	O
a	O
)	O
mg	O
=	O
4	O
;	O
(	O
b	O
)	O
mg	O
=	O
1.	O
vertical	O
axes	O
show	O
the	O
(	O
cid:12	O
)	O
tnesses	O
of	O
the	O
two	O
sub-populations	O
,	O
and	O
the	O
percentage	O
of	O
the	O
population	O
that	O
is	O
parthenogenetic	O
.	O
(	O
a	O
)	O
mg	O
=	O
4	O
(	O
b	O
)	O
mg	O
=	O
1	O
s	O
e	O
s	O
s	O
e	O
n	O
t	O
i	O
f	O
e	O
g	O
a	O
t	O
n	O
e	O
c	O
r	O
e	O
p	O
1000	O
900	O
800	O
700	O
600	O
500	O
0	O
100	O
80	O
60	O
40	O
20	O
0	O
0	O
1000	O
900	O
800	O
700	O
600	O
500	O
0	O
100	O
80	O
60	O
40	O
20	O
0	O
0	O
sexual	O
fitness	O
parthen	O
fitness	O
50	O
100	O
150	O
200	O
250	O
50	O
100	O
150	O
200	O
250	O
sexual	O
fitness	O
parthen	O
fitness	O
50	O
100	O
150	O
200	O
250	O
50	O
100	O
150	O
200	O
250	O
in	O
the	O
presence	O
of	O
a	O
higher	O
mutation	B
rate	I
(	O
mg	O
=	O
4	O
)	O
,	O
however	O
,	O
leave	O
them	O
behind	O
.	O
once	O
the	O
population	O
reaches	O
its	O
top	O
(	O
cid:12	O
)	O
tness	O
,	O
however	O
,	O
the	O
parthenogens	O
can	O
take	O
over	O
,	O
if	O
the	O
mutation	B
rate	I
is	O
su	O
(	O
cid:14	O
)	O
ciently	O
low	O
(	O
mg	O
=	O
1	O
)	O
.	O
the	O
parthenogens	O
never	O
take	O
over	O
.	O
the	O
breadth	O
of	O
the	O
sexual	O
population	O
’	O
s	O
(	O
cid:12	O
)	O
t-	O
ness	O
is	O
of	O
order	O
pg	O
,	O
so	O
a	O
mutant	O
parthenogenetic	O
colony	O
arising	O
with	O
slightly	O
above-average	O
(	O
cid:12	O
)	O
tness	O
will	O
last	O
for	O
about	O
pg=	O
(	O
mg	O
)	O
=	O
1=	O
(	O
mpg	O
)	O
generations	O
before	O
its	O
(	O
cid:12	O
)	O
tness	O
falls	O
below	O
that	O
of	O
its	O
sexual	O
cousins	O
.	O
as	O
long	O
as	O
the	O
popu-	O
lation	O
size	O
is	O
su	O
(	O
cid:14	O
)	O
ciently	O
large	O
for	O
some	O
sexual	O
individuals	O
to	O
survive	O
for	O
this	O
time	O
,	O
sex	O
will	O
not	O
die	B
out	O
.	O
in	O
a	O
su	O
(	O
cid:14	O
)	O
ciently	O
unstable	O
environment	O
,	O
where	O
the	O
(	O
cid:12	O
)	O
tness	O
function	B
is	O
con-	O
tinually	O
changing	O
,	O
the	O
parthenogens	O
will	O
always	O
lag	O
behind	O
the	O
sexual	O
commu-	O
nity	O
.	O
these	O
results	O
are	O
consistent	O
with	O
the	O
argument	O
of	O
haldane	O
and	O
hamilton	O
(	O
2002	O
)	O
that	O
sex	O
is	O
helpful	O
in	O
an	O
arms	B
race	I
with	O
parasites	O
.	O
the	O
parasites	O
de	O
(	O
cid:12	O
)	O
ne	O
an	O
e	O
(	O
cid:11	O
)	O
ective	O
(	O
cid:12	O
)	O
tness	O
function	B
which	O
changes	O
with	O
time	O
,	O
and	O
a	O
sexual	O
population	O
will	O
always	O
ascend	O
the	O
current	O
(	O
cid:12	O
)	O
tness	O
function	B
more	O
rapidly	O
.	O
additive	O
(	O
cid:12	O
)	O
tness	O
function	B
of	O
course	O
,	O
our	O
results	O
depend	O
on	O
the	O
(	O
cid:12	O
)	O
tness	O
function	B
that	O
we	O
assume	O
,	O
and	O
on	O
our	O
model	B
of	O
selection	O
.	O
is	O
it	O
reasonable	O
to	O
model	B
(	O
cid:12	O
)	O
tness	O
,	O
to	O
(	O
cid:12	O
)	O
rst	O
order	O
,	O
as	O
a	O
sum	O
of	O
independent	O
terms	O
?	O
maynard	O
smith	O
(	O
1968	O
)	O
argues	O
that	O
it	O
is	O
:	O
the	O
more	O
good	B
genes	O
you	O
have	O
,	O
the	O
higher	O
you	O
come	O
in	O
the	O
pecking	O
order	O
,	O
for	O
example	O
.	O
the	O
directional	O
selection	O
model	B
has	O
been	O
used	O
extensively	O
in	O
theoretical	O
popula-	O
tion	O
genetic	B
studies	O
(	O
bulmer	O
,	O
1985	O
)	O
.	O
we	O
might	O
expect	O
real	O
(	O
cid:12	O
)	O
tness	O
functions	B
to	O
involve	O
interactions	O
,	O
in	O
which	O
case	O
crossover	B
might	O
reduce	O
the	O
average	B
(	O
cid:12	O
)	O
tness	O
.	O
however	O
,	O
since	O
recombination	O
gives	O
the	O
biggest	O
advantage	O
to	O
species	B
whose	O
(	O
cid:12	O
)	O
t-	O
ness	O
functions	B
are	O
additive	O
,	O
we	O
might	O
predict	O
that	O
evolution	B
will	O
have	O
favoured	O
species	B
that	O
used	O
a	O
representation	O
of	O
the	O
genome	O
that	O
corresponds	O
to	O
a	O
(	O
cid:12	O
)	O
tness	O
function	B
that	O
has	O
only	O
weak	O
interactions	O
.	O
and	O
even	O
if	O
there	O
are	O
interactions	O
,	O
it	O
seems	O
plausible	O
that	O
the	O
(	O
cid:12	O
)	O
tness	O
would	O
still	O
involve	O
a	O
sum	O
of	O
such	O
interacting	O
terms	O
,	O
with	O
the	O
number	O
of	O
terms	O
being	O
some	O
fraction	O
of	O
the	O
genome	O
size	O
g.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
19.6	O
:	O
further	O
exercises	O
279	O
exercise	O
19.3	O
.	O
[	O
3c	O
]	O
investigate	O
how	O
fast	O
sexual	O
and	O
asexual	O
species	B
evolve	O
if	O
they	O
have	O
a	O
(	O
cid:12	O
)	O
tness	O
function	B
with	O
interactions	O
.	O
for	O
example	O
,	O
let	O
the	O
(	O
cid:12	O
)	O
tness	O
be	O
a	O
sum	O
of	O
exclusive-ors	O
of	O
pairs	O
of	O
bits	O
;	O
compare	O
the	O
evolving	O
(	O
cid:12	O
)	O
tnesses	O
with	O
those	O
of	O
the	O
sexual	O
and	O
asexual	O
species	B
with	O
a	O
simple	O
additive	O
(	O
cid:12	O
)	O
tness	O
function	B
.	O
furthermore	O
,	O
if	O
the	O
(	O
cid:12	O
)	O
tness	O
function	B
were	O
a	O
highly	O
nonlinear	B
function	O
of	O
the	O
genotype	O
,	O
it	O
could	O
be	O
made	O
more	O
smooth	O
and	O
locally	O
linear	B
by	O
the	O
baldwin	O
e	O
(	O
cid:11	O
)	O
ect	O
.	O
the	O
baldwin	O
e	O
(	O
cid:11	O
)	O
ect	O
(	O
baldwin	O
,	O
1896	O
;	O
hinton	O
and	O
nowlan	O
,	O
1987	O
)	O
has	O
been	O
widely	O
studied	O
as	O
a	O
mechanism	O
whereby	O
learning	B
guides	O
evolution	B
,	O
and	O
it	O
could	O
also	O
act	O
at	O
the	O
level	O
of	O
transcription	O
and	O
translation	O
.	O
consider	O
the	O
evolution	B
of	O
a	O
peptide	O
sequence	B
for	O
a	O
new	O
purpose	O
.	O
assume	O
the	O
e	O
(	O
cid:11	O
)	O
ectiveness	O
of	O
the	O
peptide	O
is	O
a	O
highly	O
nonlinear	B
function	O
of	O
the	O
sequence	O
,	O
perhaps	O
having	O
a	O
small	O
island	O
of	O
good	O
sequences	O
surrounded	O
by	O
an	O
ocean	O
of	O
equally	O
bad	B
sequences	O
.	O
in	O
an	O
organism	O
whose	O
transcription	O
and	O
translation	O
machinery	O
is	O
(	O
cid:13	O
)	O
awless	O
,	O
the	O
(	O
cid:12	O
)	O
tness	O
will	O
be	O
an	O
equally	O
nonlinear	B
function	O
of	O
the	O
dna	O
sequence	B
,	O
and	O
evolution	O
will	O
wander	O
around	O
the	O
ocean	O
making	O
progress	O
towards	O
the	O
island	O
only	O
by	O
a	O
random	B
walk	I
.	O
in	O
contrast	O
,	O
an	O
organism	O
having	O
the	O
same	O
dna	O
sequence	B
,	O
but	O
whose	O
dna-to-rna	O
transcription	O
or	O
rna-to-protein	O
translation	O
is	O
‘	O
faulty	O
’	O
,	O
will	O
occasionally	O
,	O
by	O
mistranslation	O
or	O
mistranscription	O
,	O
accidentally	O
produce	O
a	O
working	O
enzyme	O
;	O
and	O
it	O
will	O
do	O
so	O
with	O
greater	O
probability	B
if	O
its	O
dna	O
sequence	B
is	O
close	O
to	O
a	O
good	B
sequence	O
.	O
one	O
cell	O
might	O
produce	O
1000	O
proteins	O
from	O
the	O
one	O
mrna	O
sequence	B
,	O
of	O
which	O
999	O
have	O
no	O
enzymatic	O
e	O
(	O
cid:11	O
)	O
ect	O
,	O
and	O
one	O
does	O
.	O
the	O
one	O
working	O
catalyst	O
will	O
be	O
enough	O
for	O
that	O
cell	O
to	O
have	O
an	O
increased	O
(	O
cid:12	O
)	O
tness	O
relative	B
to	O
rivals	O
whose	O
dna	O
sequence	B
is	O
further	O
from	O
the	O
island	O
of	O
good	O
sequences	O
.	O
for	O
this	O
reason	O
i	O
conjecture	O
that	O
,	O
at	O
least	O
early	O
in	B
evolution	I
,	O
and	O
perhaps	O
still	O
now	O
,	O
the	O
genetic	B
code	I
was	O
not	O
implemented	O
perfectly	O
but	O
was	O
implemented	O
noisily	O
,	O
with	O
some	O
codons	O
coding	O
for	O
a	O
distribution	B
of	O
possible	O
amino	O
acids	O
.	O
this	O
noisy	B
code	O
could	O
even	O
be	O
switched	O
on	O
and	O
o	O
(	O
cid:11	O
)	O
from	O
cell	O
to	O
cell	O
in	O
an	O
organism	O
by	O
having	O
multiple	O
aminoacyl-trna	O
synthetases	O
,	O
some	O
more	O
reliable	O
than	O
others	O
.	O
whilst	O
our	O
model	B
assumed	O
that	O
the	O
bits	O
of	O
the	O
genome	B
do	O
not	O
interact	O
,	O
ignored	O
the	O
fact	O
that	O
the	O
information	B
is	O
represented	O
redundantly	O
,	O
assumed	O
that	O
there	O
is	O
a	O
direct	O
relationship	O
between	O
phenotypic	O
(	O
cid:12	O
)	O
tness	O
and	O
the	O
genotype	O
,	O
and	O
assumed	O
that	O
the	O
crossover	B
probability	O
in	O
recombination	O
is	O
high	O
,	O
i	O
believe	O
these	O
qualitative	O
results	O
would	O
still	O
hold	O
if	O
more	O
complex	B
models	O
of	O
(	O
cid:12	O
)	O
tness	O
and	O
crossover	O
were	O
used	O
:	O
the	O
relative	B
bene	O
(	O
cid:12	O
)	O
t	O
of	O
sex	O
will	O
still	O
scale	O
as	O
pg	O
.	O
only	O
in	O
small	O
,	O
in-bred	O
populations	O
are	O
the	O
bene	O
(	O
cid:12	O
)	O
ts	O
of	O
sex	O
expected	O
to	O
be	O
diminished	O
.	O
in	O
summary	O
:	O
why	O
have	O
sex	O
?	O
because	O
sex	O
is	O
good	B
for	O
your	O
bits	O
!	O
further	O
reading	O
how	O
did	O
a	O
high-information-content	O
self-replicating	O
system	O
ever	O
emerge	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
place	O
?	O
in	O
the	O
general	O
area	O
of	O
the	O
origins	O
of	O
life	O
and	O
other	O
tricky	O
ques-	O
tions	O
about	O
evolution	B
,	O
i	O
highly	O
recommend	O
maynard	O
smith	O
and	O
sz	O
(	O
cid:19	O
)	O
athmary	O
(	O
1995	O
)	O
,	O
maynard	O
smith	O
and	O
sz	O
(	O
cid:19	O
)	O
athmary	O
(	O
1999	O
)	O
,	O
kondrashov	O
(	O
1988	O
)	O
,	O
may-	O
nard	O
smith	O
(	O
1988	O
)	O
,	O
ridley	O
(	O
2000	O
)	O
,	O
dyson	O
(	O
1985	O
)	O
,	O
cairns-smith	O
(	O
1985	O
)	O
,	O
and	O
hop	O
(	O
cid:12	O
)	O
eld	O
(	O
1978	O
)	O
.	O
19.6	O
further	O
exercises	O
exercise	O
19.4	O
.	O
[	O
3	O
]	O
how	O
good	O
must	O
the	O
error-correcting	O
machinery	O
in	O
dna	O
repli-	O
cation	O
be	O
,	O
given	O
that	O
mammals	O
have	O
not	O
all	O
died	O
out	O
long	O
ago	O
?	O
estimate	O
the	O
probability	O
of	O
nucleotide	O
substitution	O
,	O
per	O
cell	O
division	O
.	O
[	O
see	O
appendix	O
c.4	O
.	O
]	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
280	O
19	O
|	O
why	O
have	O
sex	O
?	O
information	B
acquisition	O
and	O
evolution	O
exercise	O
19.5	O
.	O
[	O
4	O
]	O
given	O
that	O
dna	O
replication	B
is	O
achieved	O
by	O
bumbling	O
brow-	O
nian	O
motion	O
and	O
ordinary	O
thermodynamics	B
in	O
a	O
biochemical	O
porridge	B
at	O
a	O
temperature	B
of	O
35	O
c	O
,	O
it	O
’	O
s	O
astonishing	O
that	O
the	O
error-rate	O
of	O
dna	O
replication	B
is	O
about	O
10	O
(	O
cid:0	O
)	O
9	O
per	O
replicated	O
nucleotide	B
.	O
how	O
can	O
this	O
reliability	O
be	O
achieved	O
,	O
given	O
that	O
the	O
energetic	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
a	O
correct	O
base-pairing	B
and	O
an	O
incor-	O
rect	O
one	O
is	O
only	O
one	O
or	O
two	O
hydrogen	O
bonds	O
and	O
the	O
thermal	O
energy	O
kt	O
is	O
only	O
about	O
a	O
factor	O
of	O
four	O
smaller	O
than	O
the	O
free	B
energy	I
associated	O
with	O
a	O
hydro-	O
gen	O
bond	O
?	O
if	O
ordinary	O
thermodynamics	B
is	O
what	O
favours	O
correct	O
base-pairing	B
,	O
surely	O
the	O
frequency	B
of	O
incorrect	O
base-pairing	B
should	O
be	O
about	O
f	O
=	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:1	O
)	O
e/kt	O
)	O
;	O
(	O
19.21	O
)	O
where	O
(	O
cid:1	O
)	O
e	O
is	O
the	O
free	B
energy	I
di	O
(	O
cid:11	O
)	O
erence	O
,	O
i.e.	O
,	O
an	O
error	O
frequency	O
of	O
f	O
’	O
10	O
(	O
cid:0	O
)	O
4	O
?	O
how	O
has	O
dna	O
replication	B
cheated	O
thermodynamics	B
?	O
the	O
situation	O
is	O
equally	O
perplexing	O
in	O
the	O
case	O
of	O
protein	O
synthesis	B
,	O
which	O
translates	O
an	O
mrna	O
sequence	B
into	O
a	O
polypeptide	O
in	O
accordance	O
with	O
the	O
ge-	O
netic	O
code	B
.	O
two	O
speci	O
(	O
cid:12	O
)	O
c	O
chemical	O
reactions	O
are	O
protected	O
against	O
errors	B
:	O
the	O
binding	O
of	O
trna	O
molecules	B
to	O
amino	O
acids	O
,	O
and	O
the	O
production	O
of	O
the	O
polypep-	O
tide	O
in	O
the	O
ribosome	O
,	O
which	O
,	O
involves	O
base-pairing	B
.	O
again	O
,	O
the	O
(	O
cid:12	O
)	O
delity	O
is	O
high	O
(	O
an	O
error	O
rate	O
of	O
about	O
10	O
(	O
cid:0	O
)	O
4	O
)	O
,	O
and	O
this	O
(	O
cid:12	O
)	O
delity	O
can	O
’	O
t	O
be	O
caused	O
by	O
the	O
energy	B
of	O
the	O
‘	O
correct	O
’	O
(	O
cid:12	O
)	O
nal	O
state	O
being	O
especially	O
low	O
{	O
the	O
correct	O
polypeptide	O
sequence	B
is	O
not	O
expected	O
to	O
be	O
signi	O
(	O
cid:12	O
)	O
cantly	O
lower	O
in	O
energy	O
than	O
any	O
other	O
sequence	B
.	O
how	O
do	O
cells	O
perform	O
error	B
correction	I
?	O
(	O
see	O
hop	O
(	O
cid:12	O
)	O
eld	O
(	O
1974	O
)	O
,	O
hop	O
(	O
cid:12	O
)	O
eld	O
(	O
1980	O
)	O
)	O
.	O
like	O
dna	O
replication	B
,	O
exercise	O
19.6	O
.	O
[	O
2	O
]	O
while	O
the	O
genome	B
acquires	O
information	B
through	O
natural	B
se-	O
lection	O
at	O
a	O
rate	B
of	O
a	O
few	O
bits	O
per	O
generation	O
,	O
your	O
brain	B
acquires	O
information	B
at	O
a	O
greater	O
rate	B
.	O
estimate	O
at	O
what	O
rate	O
new	O
information	B
can	O
be	O
stored	O
in	O
long	O
term	O
memory	B
by	O
your	O
brain	B
.	O
think	O
of	O
learning	O
the	O
words	O
of	O
a	O
new	O
language	O
,	O
for	O
example	O
.	O
19.7	O
solutions	O
solution	O
to	O
exercise	O
19.1	O
(	O
p.275	O
)	O
.	O
for	O
small	O
enough	O
n	O
,	O
whilst	O
the	O
average	B
(	O
cid:12	O
)	O
t-	O
ness	O
of	O
the	O
population	O
increases	O
,	O
some	O
unlucky	O
bits	O
become	O
frozen	O
into	O
the	O
bad	B
state	O
.	O
(	O
these	O
bad	B
genes	O
are	O
sometimes	O
known	O
as	O
hitchhikers	O
.	O
)	O
the	O
ho-	O
mogeneity	O
assumption	O
breaks	O
down	O
.	O
eventually	O
,	O
all	O
individuals	O
have	O
identical	O
genotypes	O
that	O
are	O
mainly	O
1-bits	O
,	O
but	O
contain	O
some	O
0-bits	O
too	O
.	O
the	O
smaller	O
the	O
population	O
,	O
the	O
greater	O
the	O
number	O
of	O
frozen	O
0-bits	O
is	O
expected	O
to	O
be	O
.	O
how	O
small	O
can	O
the	O
population	O
size	O
n	O
be	O
if	O
the	O
theory	B
of	O
sex	O
is	O
accurate	O
?	O
we	O
(	O
cid:12	O
)	O
nd	O
experimentally	O
that	O
the	O
theory	B
based	O
on	O
assuming	O
homogeneity	O
(	O
cid:12	O
)	O
ts	O
poorly	O
only	O
if	O
the	O
population	O
size	O
n	O
is	O
smaller	O
than	O
(	O
cid:24	O
)	O
pg	O
.	O
if	O
n	O
is	O
signi	O
(	O
cid:12	O
)	O
cantly	O
smaller	O
than	O
pg	O
,	O
information	B
can	O
not	O
possibly	O
be	O
acquired	O
at	O
a	O
rate	B
as	O
big	O
as	O
pg	O
,	O
since	O
the	O
information	B
content	I
of	O
the	O
blind	O
watchmaker	O
’	O
s	O
decisions	O
can	O
not	O
be	O
any	O
greater	O
than	O
2n	O
bits	O
per	O
generation	O
,	O
this	O
being	O
the	O
number	O
of	O
bits	O
required	O
to	O
specify	O
which	O
of	O
the	O
2n	O
children	O
get	O
to	O
reproduce	O
.	O
baum	O
et	O
al	O
.	O
(	O
1995	O
)	O
,	O
analyzing	O
a	O
similar	O
model	B
,	O
show	O
that	O
the	O
population	O
size	O
n	O
should	O
be	O
about	O
pg	O
(	O
log	O
g	O
)	O
2	O
to	O
make	O
hitchhikers	O
unlikely	O
to	O
arise	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
part	O
iv	O
probabilities	O
and	O
inference	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
part	O
iv	O
the	O
number	O
of	O
inference	O
problems	O
that	O
can	O
(	O
and	O
perhaps	O
should	O
)	O
be	O
tackled	O
by	O
bayesian	O
inference	B
methods	O
is	O
enormous	O
.	O
in	O
this	O
book	O
,	O
for	O
example	O
,	O
we	O
discuss	O
the	O
decoding	B
problem	O
for	O
error-correcting	O
codes	O
,	O
the	O
task	O
of	O
inferring	O
clusters	O
from	O
data	O
,	O
the	O
task	O
of	O
interpolation	O
through	O
noisy	B
data	O
,	O
and	O
the	O
task	O
of	O
classifying	O
patterns	O
given	O
labelled	O
examples	O
.	O
most	O
techniques	O
for	O
solving	O
these	O
problems	O
can	O
be	O
categorized	O
as	O
follows	O
.	O
exact	O
methods	O
compute	O
the	O
required	O
quantities	O
directly	O
.	O
only	O
a	O
few	O
inter-	O
esting	O
problems	O
have	O
a	O
direct	O
solution	O
,	O
but	O
exact	O
methods	O
are	O
important	O
as	O
tools	O
for	O
solving	O
subtasks	O
within	O
larger	O
problems	O
.	O
methods	B
for	O
the	O
exact	O
solution	O
of	O
inference	O
problems	O
are	O
the	O
subject	O
of	O
chapters	O
21	O
,	O
24	O
,	O
25	O
,	O
and	O
26.	O
approximate	O
methods	B
can	O
be	O
subdivided	O
into	O
1.	O
deterministic	B
approximations	O
,	O
which	O
include	O
maximum	O
likeli-	O
hood	O
(	O
chapter	O
22	O
)	O
,	O
laplace	O
’	O
s	O
method	B
(	O
chapters	O
27	O
and	O
28	O
)	O
and	O
variational	O
methods	B
(	O
chapter	O
33	O
)	O
;	O
and	O
2.	O
monte	O
carlo	O
methods	B
{	O
techniques	O
in	O
which	O
random	B
numbers	O
play	O
an	O
integral	B
part	O
{	O
which	O
will	O
be	O
discussed	O
in	O
chapters	O
29	O
,	O
30	O
,	O
and	O
32.	O
this	O
part	O
of	O
the	O
book	O
does	O
not	O
form	O
a	O
one-dimensional	O
story	O
.	O
rather	O
,	O
the	O
ideas	O
make	O
up	O
a	O
web	O
of	O
interrelated	O
threads	O
which	O
will	O
recombine	O
in	O
subsequent	O
chapters	O
.	O
chapter	O
3	O
,	O
which	O
is	O
an	O
honorary	O
member	O
of	O
this	O
part	O
,	O
discussed	O
a	O
range	O
of	O
simple	O
examples	O
of	O
inference	O
problems	O
and	O
their	O
bayesian	O
solutions	O
.	O
to	O
give	O
further	O
motivation	O
for	O
the	O
toolbox	O
of	O
inference	O
methods	B
discussed	O
in	O
this	O
part	O
,	O
chapter	O
20	O
discusses	O
the	O
problem	O
of	O
clustering	O
;	O
subsequent	O
chapters	O
discuss	O
the	O
probabilistic	O
interpretation	O
of	O
clustering	O
as	O
mixture	O
modelling	B
.	O
chapter	O
21	O
discusses	O
the	O
option	O
of	O
dealing	O
with	O
probability	O
distributions	O
by	O
completely	O
enumerating	O
all	O
hypotheses	O
.	O
chapter	O
22	O
introduces	O
the	O
idea	O
of	O
maximization	O
methods	B
as	O
a	O
way	O
of	O
avoiding	O
the	O
large	O
cost	O
associated	O
with	O
complete	O
enumeration	O
,	O
and	O
points	O
out	O
reasons	O
why	O
maximum	B
likelihood	I
is	O
not	O
good	B
enough	O
.	O
chapter	O
23	O
reviews	O
the	O
probability	B
distributions	I
that	O
arise	O
most	O
often	O
in	O
bayesian	O
inference	B
.	O
chapters	O
24	O
,	O
25	O
,	O
and	O
26	O
discuss	O
another	O
way	O
of	O
avoiding	O
the	O
cost	O
of	O
complete	O
enumeration	O
:	O
marginalization	B
.	O
chapter	O
25	O
discusses	O
message-passing	B
methods	O
appropriate	O
for	O
graphical	O
models	O
,	O
using	O
the	O
decoding	B
of	O
error-correcting	B
codes	I
as	O
an	O
example	O
.	O
chapter	O
26	O
combines	O
these	O
ideas	O
with	O
message-passing	O
concepts	O
from	O
chapters	O
16	O
and	O
17.	O
these	O
chapters	O
are	O
a	O
prerequisite	O
for	O
the	O
understanding	O
of	O
advanced	O
error-correcting	B
codes	I
.	O
chapter	O
27	O
discusses	O
deterministic	B
approximations	O
including	O
laplace	O
’	O
s	O
method	B
.	O
this	O
chapter	O
is	O
a	O
prerequisite	O
for	O
understanding	O
the	O
topic	O
of	O
complex-	O
ity	O
control	O
in	O
learning	B
algorithms	I
,	O
an	O
idea	O
that	O
is	O
discussed	O
in	O
general	O
terms	O
in	O
chapter	O
28	O
.	O
282	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
part	O
iv	O
283	O
chapter	O
29	O
discusses	O
monte	O
carlo	O
methods	B
.	O
chapter	O
30	O
gives	O
details	O
of	O
state-of-the-art	O
monte	O
carlo	O
techniques	O
.	O
chapter	O
31	O
introduces	O
the	O
ising	O
model	B
as	O
a	O
test-bed	O
for	O
probabilistic	O
meth-	O
ods	O
.	O
an	O
exact	O
message-passing	O
method	B
and	O
a	O
monte	O
carlo	O
method	B
are	O
demon-	O
strated	O
.	O
a	O
motivation	O
for	O
studying	O
the	O
ising	O
model	B
is	O
that	O
it	O
is	O
intimately	O
related	O
to	O
several	O
neural	B
network	I
models	O
.	O
chapter	O
32	O
describes	O
‘	O
exact	O
’	O
monte	O
carlo	O
methods	B
and	O
demonstrates	O
their	O
application	O
to	O
the	O
ising	O
model	B
.	O
chapter	O
33	O
discusses	O
variational	B
methods	I
and	O
their	O
application	O
to	O
ising	O
models	O
and	O
to	O
simple	O
statistical	O
inference	B
problems	O
including	O
clustering	B
.	O
this	O
chapter	O
will	O
help	O
the	O
reader	O
understand	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
(	O
chapter	O
42	O
)	O
and	O
the	O
em	O
algorithm	B
,	O
which	O
is	O
an	O
important	O
method	B
in	O
latent-variable	O
mod-	O
elling	O
.	O
chapter	O
34	O
discusses	O
a	O
particularly	O
simple	O
latent	O
variable	B
model	I
called	O
independent	B
component	I
analysis	I
.	O
chapter	O
35	O
discusses	O
a	O
ragbag	O
of	O
assorted	O
inference	B
topics	O
.	O
chapter	O
36	O
discusses	O
a	O
simple	O
example	O
of	O
decision	O
theory	B
.	O
chapter	O
37	O
discusses	O
di	O
(	O
cid:11	O
)	O
erences	O
between	O
sampling	B
theory	I
and	O
bayesian	O
methods	B
.	O
a	O
theme	O
:	O
what	O
inference	O
is	O
about	O
a	O
widespread	O
misconception	O
is	O
that	O
the	O
aim	O
of	O
inference	O
is	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
most	O
probable	O
explanation	O
for	O
some	O
data	O
.	O
while	O
this	O
most	O
probable	O
hypothesis	O
may	O
be	O
of	O
interest	O
,	O
and	O
some	O
inference	B
methods	O
do	O
locate	O
it	O
,	O
this	O
hypothesis	O
is	O
just	O
the	O
peak	O
of	O
a	O
probability	B
distribution	O
,	O
and	O
it	O
is	O
the	O
whole	O
distribution	B
that	O
is	O
of	O
interest	O
.	O
as	O
we	O
saw	O
in	O
chapter	O
4	O
,	O
the	O
most	O
probable	O
outcome	O
from	O
a	O
source	O
is	O
often	O
not	O
a	O
typical	B
outcome	O
from	O
that	O
source	O
.	O
similarly	O
,	O
the	O
most	O
probable	O
hypothesis	O
given	O
some	O
data	O
may	O
be	O
atypical	O
of	O
the	O
whole	O
set	B
of	O
reasonably-	O
plausible	O
hypotheses	O
.	O
about	O
chapter	O
20	O
before	O
reading	O
the	O
next	O
chapter	O
,	O
exercise	O
2.17	O
(	O
p.36	O
)	O
and	O
section	O
11.2	O
(	O
inferring	O
the	O
input	O
to	O
a	O
gaussian	O
channel	B
)	O
are	O
recommended	O
reading	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
20	O
an	O
example	O
inference	B
task	O
:	O
clustering	B
human	O
brains	O
are	O
good	B
at	O
(	O
cid:12	O
)	O
nding	O
regularities	O
in	O
data	O
.	O
one	O
way	O
of	O
expressing	O
regularity	O
is	O
to	O
put	O
a	O
set	B
of	O
objects	O
into	O
groups	O
that	O
are	O
similar	O
to	O
each	O
other	O
.	O
for	O
example	O
,	O
biologists	O
have	O
found	O
that	O
most	O
objects	O
in	O
the	O
natural	B
world	O
fall	O
into	O
one	O
of	O
two	O
categories	O
:	O
things	O
that	O
are	O
brown	O
and	O
run	O
away	O
,	O
and	O
things	O
that	O
are	O
green	O
and	O
don	O
’	O
t	O
run	O
away	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
group	O
they	O
call	O
animals	O
,	O
and	O
the	O
second	O
,	O
plants	O
.	O
we	O
’	O
ll	O
call	O
this	O
operation	O
of	O
grouping	O
things	O
together	O
clustering	B
.	O
if	O
the	O
biologist	O
further	O
sub-divides	O
the	O
cluster	O
of	O
plants	O
into	O
sub-	O
clusters	O
,	O
we	O
would	O
call	O
this	O
‘	O
hierarchical	B
clustering	I
’	O
;	O
but	O
we	O
won	O
’	O
t	O
be	O
talking	O
about	O
hierarchical	B
clustering	I
yet	O
.	O
in	O
this	O
chapter	O
we	O
’	O
ll	O
just	O
discuss	O
ways	O
to	O
take	O
a	O
set	B
of	O
n	O
objects	O
and	O
group	O
them	O
into	O
k	O
clusters	O
.	O
there	O
are	O
several	O
motivations	O
for	O
clustering	O
.	O
first	O
,	O
a	O
good	B
clustering	O
has	O
predictive	O
power	O
.	O
when	O
an	O
early	O
biologist	O
encounters	O
a	O
new	O
green	O
thing	O
he	O
has	O
not	O
seen	O
before	O
,	O
his	O
internal	O
model	B
of	O
plants	O
and	O
animals	O
(	O
cid:12	O
)	O
lls	O
in	O
predictions	O
for	O
attributes	O
of	O
the	O
green	O
thing	O
:	O
it	O
’	O
s	O
unlikely	O
to	O
jump	O
on	O
him	O
and	O
eat	O
him	O
;	O
if	O
he	O
touches	O
it	O
,	O
he	O
might	O
get	O
grazed	O
or	O
stung	O
;	O
if	O
he	O
eats	O
it	O
,	O
he	O
might	O
feel	O
sick	O
.	O
all	O
of	O
these	O
predictions	O
,	O
while	O
uncertain	O
,	O
are	O
useful	O
,	O
because	O
they	O
help	O
the	O
biologist	O
invest	O
his	O
resources	O
(	O
for	O
example	O
,	O
the	O
time	O
spent	O
watching	O
for	O
predators	O
)	O
well	O
.	O
thus	O
,	O
we	O
perform	O
clustering	B
because	O
we	O
believe	O
the	O
underlying	O
cluster	O
labels	O
are	O
meaningful	O
,	O
will	O
lead	O
to	O
a	O
more	O
e	O
(	O
cid:14	O
)	O
cient	O
description	O
of	O
our	O
data	O
,	O
and	O
will	O
help	O
us	O
choose	O
better	O
actions	O
.	O
this	O
type	O
of	O
clustering	O
is	O
sometimes	O
called	O
‘	O
mixture	O
density	O
modelling	B
’	O
,	O
and	O
the	O
objective	B
function	I
that	O
measures	O
how	O
well	O
the	O
predictive	O
model	O
is	O
working	O
is	O
the	O
information	B
content	I
of	O
the	O
data	O
,	O
log	O
1=p	O
(	O
fxg	O
)	O
.	O
second	O
,	O
clusters	O
can	O
be	O
a	O
useful	O
aid	O
to	O
communication	B
because	O
they	O
allow	O
lossy	B
compression	I
.	O
the	O
biologist	O
can	O
give	O
directions	O
to	O
a	O
friend	O
such	O
as	O
‘	O
go	O
to	O
the	O
third	O
tree	O
on	O
the	O
right	O
then	O
take	O
a	O
right	O
turn	O
’	O
(	O
rather	O
than	O
‘	O
go	O
past	O
the	O
large	O
green	O
thing	O
with	O
red	O
berries	O
,	O
then	O
past	O
the	O
large	O
green	O
thing	O
with	O
thorns	O
,	O
then	O
:	O
:	O
:	O
’	O
)	O
.	O
the	O
brief	O
category	O
name	O
‘	O
tree	B
’	O
is	O
helpful	O
because	O
it	O
is	O
su	O
(	O
cid:14	O
)	O
cient	O
to	O
identify	O
an	O
object	O
.	O
similarly	O
,	O
in	O
lossy	O
image	B
compression	I
,	O
the	O
aim	O
is	O
to	O
convey	O
in	O
as	O
few	O
bits	O
as	O
possible	O
a	O
reasonable	O
reproduction	O
of	O
a	O
picture	O
;	O
one	O
way	O
to	O
do	O
this	O
is	O
to	O
divide	O
the	O
image	B
into	O
n	O
small	O
patches	O
,	O
and	O
(	O
cid:12	O
)	O
nd	O
a	O
close	O
match	O
to	O
each	O
patch	O
in	O
an	O
alphabet	O
of	O
k	O
image-templates	O
;	O
then	O
we	O
send	O
a	O
close	O
(	O
cid:12	O
)	O
t	O
to	O
the	O
image	B
by	O
sending	O
the	O
list	O
of	O
labels	O
k1	O
;	O
k2	O
;	O
:	O
:	O
:	O
;	O
kn	O
of	O
the	O
matching	O
templates	O
.	O
the	O
task	O
of	O
creating	O
a	O
good	B
library	O
of	O
image-templates	O
is	O
equivalent	O
to	O
(	O
cid:12	O
)	O
nding	O
a	O
set	B
of	O
cluster	O
centres	O
.	O
this	O
type	O
of	O
clustering	O
is	O
sometimes	O
called	O
‘	O
vector	B
quantization	I
’	O
.	O
we	O
can	O
formalize	O
a	O
vector	O
quantizer	O
in	O
terms	O
of	O
an	O
assignment	O
rule	O
x	O
!	O
k	O
(	O
x	O
)	O
for	O
assigning	O
datapoints	O
x	O
to	O
one	O
of	O
k	O
codenames	O
,	O
and	O
a	O
reconstruction	O
rule	O
k	O
!	O
m	O
(	O
k	O
)	O
,	O
the	O
aim	O
being	O
to	O
choose	O
the	O
functions	B
k	O
(	O
x	O
)	O
and	O
m	O
(	O
k	O
)	O
so	O
as	O
to	O
284	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
20.1	O
:	O
k-means	O
clustering	B
285	O
minimize	O
the	O
expected	O
distortion	O
,	O
which	O
might	O
be	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
d	O
=xx	O
p	O
(	O
x	O
)	O
1	O
2hm	O
(	O
k	O
(	O
x	O
)	O
)	O
(	O
cid:0	O
)	O
xi2	O
:	O
(	O
20.1	O
)	O
[	O
the	O
ideal	O
objective	B
function	I
would	O
be	O
to	O
minimize	O
the	O
psychologically	O
per-	O
ceived	O
distortion	O
of	O
the	O
image	O
.	O
since	O
it	O
is	O
hard	O
to	O
quantify	O
the	O
distortion	O
perceived	O
by	O
a	O
human	B
,	O
vector	B
quantization	I
and	I
lossy	O
compression	B
are	O
not	O
so	O
crisply	O
de	O
(	O
cid:12	O
)	O
ned	O
problems	O
as	O
data	O
modelling	B
and	O
lossless	B
compression	O
.	O
]	O
in	O
vec-	O
tor	O
quantization	O
,	O
we	O
don	O
’	O
t	O
necessarily	O
believe	O
that	O
the	O
templates	O
fm	O
(	O
k	O
)	O
g	O
have	O
any	O
natural	B
meaning	O
;	O
they	O
are	O
simply	O
tools	O
to	O
do	O
a	O
job	O
.	O
we	O
note	O
in	O
passing	O
the	O
similarity	O
of	O
the	O
assignment	O
rule	O
(	O
i.e.	O
,	O
the	O
encoder	B
)	O
of	O
vector	O
quantization	O
to	O
the	O
decoding	B
problem	O
when	O
decoding	B
an	O
error-correcting	B
code	I
.	O
a	O
third	O
reason	O
for	O
making	O
a	O
cluster	O
model	B
is	O
that	O
failures	O
of	O
the	O
cluster	O
model	B
may	O
highlight	O
interesting	O
objects	O
that	O
deserve	O
special	O
attention	O
.	O
if	O
we	O
have	O
trained	O
a	O
vector	O
quantizer	O
to	O
do	O
a	O
good	B
job	O
of	O
compressing	O
satellite	O
pictures	O
of	O
ocean	O
surfaces	O
,	O
then	O
maybe	O
patches	O
of	O
image	O
that	O
are	O
not	O
well	O
compressed	O
by	O
the	O
vector	O
quantizer	O
are	O
the	O
patches	O
that	O
contain	O
ships	O
!	O
if	O
the	O
biologist	O
encounters	O
a	O
green	O
thing	O
and	O
sees	O
it	O
run	O
(	O
or	O
slither	O
)	O
away	O
,	O
this	O
mis	O
(	O
cid:12	O
)	O
t	O
with	O
his	O
cluster	O
model	B
(	O
which	O
says	O
green	O
things	O
don	O
’	O
t	O
run	O
away	O
)	O
cues	O
him	O
to	O
pay	O
special	O
attention	O
.	O
one	O
can	O
’	O
t	O
spend	O
all	O
one	O
’	O
s	O
time	O
being	O
fascinated	O
by	O
things	O
;	O
the	O
cluster	O
model	B
can	O
help	O
sift	O
out	O
from	O
the	O
multitude	O
of	O
objects	O
in	O
one	O
’	O
s	O
world	O
the	O
ones	O
that	O
really	O
deserve	O
attention	O
.	O
a	O
fourth	O
reason	O
for	O
liking	O
clustering	B
algorithms	O
is	O
that	O
they	O
may	O
serve	O
as	O
models	O
of	O
learning	O
processes	O
in	O
neural	O
systems	O
.	O
the	O
clustering	B
algorithm	O
that	O
we	O
now	O
discuss	O
,	O
the	O
k-means	O
algorithm	B
,	O
is	O
an	O
example	O
of	O
a	O
competitive	B
learning	I
algorithm	O
.	O
the	O
algorithm	B
works	O
by	O
having	O
the	O
k	O
clusters	O
compete	O
with	O
each	O
other	O
for	O
the	O
right	O
to	O
own	O
the	O
data	O
points	O
.	O
20.1	O
k-means	O
clustering	B
figure	O
20.1.	O
n	O
=	O
40	O
data	O
points	O
.	O
the	O
k-means	O
algorithm	B
is	O
an	O
algorithm	B
for	O
putting	O
n	O
data	O
points	O
in	O
an	O
i-	O
dimensional	O
space	O
into	O
k	O
clusters	O
.	O
each	O
cluster	O
is	O
parameterized	O
by	O
a	O
vector	O
m	O
(	O
k	O
)	O
called	O
its	O
mean	B
.	O
the	O
data	O
points	O
will	O
be	O
denoted	O
by	O
fx	O
(	O
n	O
)	O
g	O
where	O
the	O
superscript	O
n	O
runs	O
from	O
1	O
to	O
the	O
number	O
of	O
data	O
points	O
n	O
.	O
each	O
vector	O
x	O
has	O
i	O
components	O
xi	O
.	O
we	O
will	O
assume	O
that	O
the	O
space	O
that	O
x	O
lives	O
in	O
is	O
a	O
real	O
space	O
and	O
that	O
we	O
have	O
a	O
metric	B
that	O
de	O
(	O
cid:12	O
)	O
nes	O
distances	O
between	O
points	O
,	O
for	O
example	O
,	O
about	O
the	O
name	O
...	O
as	O
far	O
as	O
i	O
know	O
,	O
the	O
‘	O
k	O
’	O
in	O
k-means	O
clustering	B
simply	O
refers	O
to	O
the	O
chosen	O
number	O
of	O
clusters	O
.	O
if	O
newton	O
had	O
followed	O
the	O
same	O
naming	O
policy	O
,	O
maybe	O
we	O
would	O
learn	O
at	O
school	O
about	O
‘	O
calculus	O
for	O
the	O
variable	O
x	O
’	O
.	O
it	O
’	O
s	O
a	O
silly	O
name	O
,	O
but	O
we	O
are	O
stuck	O
with	O
it	O
.	O
d	O
(	O
x	O
;	O
y	O
)	O
=	O
1	O
2xi	O
(	O
xi	O
(	O
cid:0	O
)	O
yi	O
)	O
2	O
:	O
(	O
20.2	O
)	O
to	O
start	O
the	O
k-means	O
algorithm	B
(	O
algorithm	B
20.2	O
)	O
,	O
the	O
k	O
means	O
fm	O
(	O
k	O
)	O
g	O
are	O
initialized	O
in	O
some	O
way	O
,	O
for	O
example	O
to	O
random	B
values	O
.	O
k-means	O
is	O
then	O
an	O
iterative	O
two-step	O
algorithm	B
.	O
in	O
the	O
assignment	O
step	O
,	O
each	O
data	O
point	O
n	O
is	O
assigned	O
to	O
the	O
nearest	O
mean	B
.	O
in	O
the	O
update	O
step	O
,	O
the	O
means	O
are	O
adjusted	O
to	O
match	O
the	O
sample	B
means	O
of	O
the	O
data	O
points	O
that	O
they	O
are	O
responsible	O
for	O
.	O
the	O
k-means	O
algorithm	B
is	O
demonstrated	O
for	O
a	O
toy	O
two-dimensional	B
data	O
set	B
in	O
(	O
cid:12	O
)	O
gure	O
20.3	O
,	O
where	O
2	O
means	O
are	O
used	O
.	O
the	O
assignments	O
of	O
the	O
points	O
to	O
the	O
two	O
clusters	O
are	O
indicated	O
by	O
two	O
point	O
styles	O
,	O
and	O
the	O
two	O
means	O
are	O
shown	O
by	O
the	O
circles	O
.	O
the	O
algorithm	B
converges	O
after	O
three	O
iterations	O
,	O
at	O
which	O
point	O
the	O
assignments	O
are	O
unchanged	O
so	O
the	O
means	O
remain	O
unmoved	O
when	O
updated	O
.	O
the	O
k-means	O
algorithm	B
always	O
converges	O
to	O
a	O
(	O
cid:12	O
)	O
xed	O
point	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
286	O
20	O
|	O
an	O
example	O
inference	B
task	O
:	O
clustering	B
algorithm	O
20.2.	O
the	O
k-means	O
clustering	B
algorithm	O
.	O
initialization	O
.	O
set	B
k	O
means	O
fm	O
(	O
k	O
)	O
g	O
to	O
random	B
values	O
.	O
assignment	O
step	O
.	O
each	O
data	O
point	O
n	O
is	O
assigned	O
to	O
the	O
nearest	O
mean	B
.	O
we	O
denote	O
our	O
guess	O
for	O
the	O
cluster	O
k	O
(	O
n	O
)	O
that	O
the	O
point	O
x	O
(	O
n	O
)	O
belongs	O
to	O
by	O
^k	O
(	O
n	O
)	O
.	O
fd	O
(	O
m	O
(	O
k	O
)	O
;	O
x	O
(	O
n	O
)	O
)	O
g	O
:	O
(	O
20.3	O
)	O
^k	O
(	O
n	O
)	O
=	O
argmin	O
k	O
an	O
alternative	O
,	O
equivalent	O
representation	O
of	O
this	O
assignment	O
of	O
points	O
to	O
clusters	O
is	O
given	O
by	O
‘	O
responsibilities	O
’	O
,	O
which	O
are	O
indicator	O
variables	O
r	O
(	O
n	O
)	O
to	O
one	O
if	O
mean	B
k	O
is	O
the	O
closest	O
mean	B
to	O
datapoint	O
x	O
(	O
n	O
)	O
;	O
otherwise	O
r	O
(	O
n	O
)	O
k	O
.	O
in	O
the	O
assignment	O
step	O
,	O
we	O
set	B
r	O
(	O
n	O
)	O
is	O
zero	O
.	O
k	O
k	O
r	O
(	O
n	O
)	O
k	O
=	O
(	O
cid:26	O
)	O
1	O
0	O
if	O
if	O
^k	O
(	O
n	O
)	O
=	O
k	O
^k	O
(	O
n	O
)	O
6=	O
k	O
:	O
(	O
20.4	O
)	O
what	O
about	O
ties	O
?	O
{	O
we	O
don	O
’	O
t	O
expect	O
two	O
means	O
to	O
be	O
exactly	O
the	O
same	O
distance	B
from	O
a	O
data	O
point	O
,	O
but	O
if	O
a	O
tie	O
does	O
happen	O
,	O
^k	O
(	O
n	O
)	O
is	O
set	B
to	O
the	O
smallest	O
of	O
the	O
winning	O
fkg	O
.	O
update	O
step	O
.	O
the	O
model	B
parameters	O
,	O
the	O
means	O
,	O
are	O
adjusted	O
to	O
match	O
the	O
sample	B
means	O
of	O
the	O
data	O
points	O
that	O
they	O
are	O
responsible	O
for	O
.	O
m	O
(	O
k	O
)	O
=	O
xn	O
r	O
(	O
n	O
)	O
k	O
x	O
(	O
n	O
)	O
r	O
(	O
k	O
)	O
where	O
r	O
(	O
k	O
)	O
is	O
the	O
total	O
responsibility	B
of	O
mean	B
k	O
,	O
r	O
(	O
k	O
)	O
=xn	O
r	O
(	O
n	O
)	O
k	O
:	O
(	O
20.5	O
)	O
(	O
20.6	O
)	O
what	O
about	O
means	O
with	O
no	O
responsibilities	O
?	O
{	O
if	O
r	O
(	O
k	O
)	O
=	O
0	O
,	O
then	O
we	O
leave	O
the	O
mean	B
m	O
(	O
k	O
)	O
where	O
it	O
is	O
.	O
repeat	O
the	O
assignment	O
step	O
and	O
update	O
step	O
until	O
the	O
assign-	O
ments	O
do	O
not	O
change	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
20.1	O
:	O
k-means	O
clustering	B
data	O
:	O
287	O
figure	O
20.3.	O
k-means	O
algorithm	B
applied	O
to	O
a	O
data	B
set	I
of	O
40	O
points	O
.	O
k	O
=	O
2	O
means	O
evolve	O
to	O
stable	O
locations	O
after	O
three	O
iterations	O
.	O
assignment	O
update	O
assignment	O
update	O
assignment	O
update	O
run	O
1	O
run	O
2	O
figure	O
20.4.	O
k-means	O
algorithm	B
applied	O
to	O
a	O
data	B
set	I
of	O
40	O
points	O
.	O
two	O
separate	O
runs	O
,	O
both	O
with	O
k	O
=	O
4	O
means	O
,	O
reach	O
di	O
(	O
cid:11	O
)	O
erent	O
solutions	O
.	O
each	O
frame	O
shows	O
a	O
successive	O
assignment	O
step	O
.	O
exercise	O
20.1	O
.	O
[	O
4	O
,	O
p.291	O
]	O
see	O
if	O
you	O
can	O
prove	O
that	O
k-means	O
always	O
converges	O
.	O
[	O
hint	O
:	O
(	O
cid:12	O
)	O
nd	O
a	O
physical	O
analogy	O
and	O
an	O
associated	O
lyapunov	O
function	B
.	O
]	O
[	O
a	O
lyapunov	O
function	B
is	O
a	O
function	B
of	O
the	O
state	O
of	O
the	O
algorithm	B
that	O
decreases	O
whenever	O
the	O
state	O
changes	O
and	O
that	O
is	O
bounded	O
below	O
.	O
if	O
a	O
system	O
has	O
a	O
lyapunov	O
function	B
then	O
its	O
dynamics	O
converge	O
.	O
]	O
the	O
k-means	O
algorithm	B
with	O
a	O
larger	O
number	O
of	O
means	O
,	O
4	O
,	O
is	O
demonstrated	O
in	O
(	O
cid:12	O
)	O
gure	O
20.4.	O
the	O
outcome	O
of	O
the	O
algorithm	O
depends	O
on	O
the	O
initial	O
condition	O
.	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
case	O
,	O
after	O
(	O
cid:12	O
)	O
ve	O
iterations	O
,	O
a	O
steady	O
state	O
is	O
found	O
in	O
which	O
the	O
data	O
points	O
are	O
fairly	O
evenly	O
split	O
between	O
the	O
four	O
clusters	O
.	O
in	O
the	O
second	O
case	O
,	O
after	O
six	B
iterations	O
,	O
half	O
the	O
data	O
points	O
are	O
in	O
one	O
cluster	O
,	O
and	O
the	O
others	O
are	O
shared	O
among	O
the	O
other	O
three	O
clusters	O
.	O
questions	O
about	O
this	O
algorithm	B
the	O
k-means	O
algorithm	B
has	O
several	O
ad	O
hoc	O
features	O
.	O
why	O
does	O
the	O
update	O
step	O
set	B
the	O
‘	O
mean	B
’	O
to	O
the	O
mean	B
of	O
the	O
assigned	O
points	O
?	O
where	O
did	O
the	O
distance	B
d	O
come	O
from	O
?	O
what	O
if	O
we	O
used	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
measure	O
of	O
distance	O
between	O
x	O
and	O
m	O
?	O
how	O
can	O
we	O
choose	O
the	O
‘	O
best	O
’	O
distance	B
?	O
[	O
in	O
vector	O
quantization	O
,	O
the	O
distance	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
288	O
(	O
a	O
)	O
10	O
8	O
6	O
4	O
2	O
0	O
0	O
2	O
4	O
6	O
8	O
10	O
20	O
|	O
an	O
example	O
inference	B
task	O
:	O
clustering	B
(	O
b	O
)	O
10	O
8	O
6	O
4	O
2	O
0	O
0	O
2	O
4	O
6	O
8	O
10	O
figure	O
20.5.	O
k-means	O
algorithm	B
for	O
a	O
case	O
with	O
two	O
dissimilar	O
clusters	O
.	O
(	O
a	O
)	O
the	O
\little	O
’	O
n	O
’	O
large	O
''	O
data	O
.	O
(	O
b	O
)	O
a	O
stable	O
set	B
of	O
assignments	O
and	O
means	O
.	O
note	O
that	O
four	O
points	O
belonging	O
to	O
the	O
broad	O
cluster	O
have	O
been	O
incorrectly	O
assigned	O
to	O
the	O
narrower	O
cluster	O
.	O
(	O
points	O
assigned	O
to	O
the	O
right-hand	O
cluster	O
are	O
shown	O
by	O
plus	O
signs	O
.	O
)	O
figure	O
20.6.	O
two	O
elongated	O
clusters	O
,	O
and	O
the	O
stable	O
solution	O
found	O
by	O
the	O
k-means	O
algorithm	B
.	O
(	O
a	O
)	O
(	O
b	O
)	O
function	B
is	O
provided	O
as	O
part	O
of	O
the	O
problem	O
de	O
(	O
cid:12	O
)	O
nition	O
;	O
but	O
i	O
’	O
m	O
assuming	O
we	O
are	O
interested	O
in	O
data-modelling	O
rather	O
than	O
vector	B
quantization	I
.	O
]	O
how	O
do	O
we	O
choose	O
k	O
?	O
having	O
found	O
multiple	O
alternative	O
clusterings	O
for	O
a	O
given	O
k	O
,	O
how	O
can	O
we	O
choose	O
among	O
them	O
?	O
cases	O
where	O
k-means	O
might	O
be	O
viewed	O
as	O
failing	O
.	O
further	O
questions	O
arise	O
when	O
we	O
look	O
for	O
cases	O
where	O
the	O
algorithm	B
behaves	O
badly	O
(	O
compared	O
with	O
what	O
the	O
man	O
in	O
the	O
street	O
would	O
call	O
‘	O
clustering	B
’	O
)	O
.	O
figure	O
20.5a	O
shows	O
a	O
set	B
of	O
75	O
data	O
points	O
generated	O
from	O
a	O
mixture	O
of	O
two	O
gaussians	O
.	O
the	O
right-hand	O
gaussian	O
has	O
less	O
weight	B
(	O
only	O
one	O
(	O
cid:12	O
)	O
fth	O
of	O
the	O
data	O
points	O
)	O
,	O
and	O
it	O
is	O
a	O
less	O
broad	O
cluster	O
.	O
figure	O
20.5b	O
shows	O
the	O
outcome	O
of	O
using	O
k-means	O
clustering	B
with	O
k	O
=	O
2	O
means	O
.	O
four	O
of	O
the	O
big	O
cluster	O
’	O
s	O
data	O
points	O
have	O
been	O
assigned	O
to	O
the	O
small	O
cluster	O
,	O
and	O
both	O
means	O
end	O
up	O
displaced	O
to	O
the	O
left	O
of	O
the	O
true	O
centres	O
of	O
the	O
clusters	O
.	O
the	O
k-means	O
algorithm	B
takes	O
account	O
only	O
of	O
the	O
distance	O
between	O
the	O
means	O
and	O
the	O
data	O
points	O
;	O
it	O
has	O
no	O
representation	O
of	O
the	O
weight	O
or	O
breadth	O
of	O
each	O
cluster	O
.	O
consequently	O
,	O
data	O
points	O
that	O
actually	O
belong	O
to	O
the	O
broad	O
cluster	O
are	O
incorrectly	O
assigned	O
to	O
the	O
narrow	O
cluster	O
.	O
figure	O
20.6	O
shows	O
another	O
case	O
of	O
k-means	O
behaving	O
badly	O
.	O
the	O
data	O
evidently	O
fall	O
into	O
two	O
elongated	O
clusters	O
.	O
but	O
the	O
only	O
stable	O
state	O
of	O
the	O
k-means	O
algorithm	B
is	O
that	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
20.6b	O
:	O
the	O
two	O
clusters	O
have	O
been	O
sliced	O
in	O
half	O
!	O
these	O
two	O
examples	O
show	O
that	O
there	O
is	O
something	O
wrong	O
with	O
the	O
distance	B
d	O
in	O
the	O
k-means	O
algorithm	B
.	O
the	O
k-means	O
algorithm	B
has	O
no	O
way	O
of	O
representing	O
the	O
size	O
or	O
shape	O
of	O
a	O
cluster	O
.	O
a	O
(	O
cid:12	O
)	O
nal	O
criticism	O
of	O
k-means	O
is	O
that	O
it	O
is	O
a	O
‘	O
hard	O
’	O
rather	O
than	O
a	O
‘	O
soft	B
’	O
algorithm	B
:	O
points	O
are	O
assigned	O
to	O
exactly	O
one	O
cluster	O
and	O
all	O
points	O
assigned	O
to	O
a	O
cluster	O
are	O
equals	O
in	O
that	O
cluster	O
.	O
points	O
located	O
near	O
the	O
border	O
between	O
two	O
or	O
more	O
clusters	O
should	O
,	O
arguably	O
,	O
play	O
a	O
partial	B
role	O
in	O
determining	O
the	O
locations	O
of	O
all	O
the	O
clusters	O
that	O
they	O
could	O
plausibly	O
be	O
assigned	O
to	O
.	O
but	O
in	O
the	O
k-means	O
algorithm	B
,	O
each	O
borderline	O
point	O
is	O
dumped	O
in	O
one	O
cluster	O
,	O
and	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
20.2	O
:	O
soft	B
k-means	O
clustering	B
289	O
has	O
an	O
equal	O
vote	O
with	O
all	O
the	O
other	O
points	O
in	O
that	O
cluster	O
,	O
and	O
no	O
vote	O
in	O
any	O
other	O
clusters	O
.	O
20.2	O
soft	B
k-means	O
clustering	B
these	O
criticisms	B
of	O
k-means	O
motivate	O
the	O
‘	O
soft	B
k-means	O
algorithm	B
’	O
,	O
algo-	O
rithm	O
20.7.	O
the	O
algorithm	B
has	O
one	O
parameter	O
,	O
(	O
cid:12	O
)	O
,	O
which	O
we	O
could	O
term	O
the	O
sti	O
(	O
cid:11	O
)	O
ness	O
.	O
assignment	O
step	O
.	O
each	O
data	O
point	O
x	O
(	O
n	O
)	O
is	O
given	O
a	O
soft	B
‘	O
degree	B
of	O
as-	O
signment	O
’	O
to	O
each	O
of	O
the	O
means	O
.	O
we	O
call	O
the	O
degree	B
to	O
which	O
x	O
(	O
n	O
)	O
is	O
assigned	O
to	O
cluster	O
k	O
the	O
responsibility	B
r	O
(	O
n	O
)	O
(	O
the	O
responsibility	B
of	O
cluster	O
k	O
for	O
point	O
n	O
)	O
.	O
k	O
algorithm	B
20.7.	O
soft	B
k-means	O
algorithm	B
,	O
version	O
1.	O
r	O
(	O
n	O
)	O
k	O
=	O
exp	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
d	O
(	O
m	O
(	O
k	O
)	O
;	O
x	O
(	O
n	O
)	O
)	O
(	O
cid:1	O
)	O
pk0	O
exp	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
d	O
(	O
m	O
(	O
k0	O
)	O
;	O
x	O
(	O
n	O
)	O
)	O
(	O
cid:1	O
)	O
:	O
the	O
sum	O
of	O
the	O
k	O
responsibilities	O
for	O
the	O
nth	O
point	O
is	O
1	O
.	O
(	O
20.7	O
)	O
update	O
step	O
.	O
the	O
model	B
parameters	O
,	O
the	O
means	O
,	O
are	O
adjusted	O
to	O
match	O
the	O
sample	B
means	O
of	O
the	O
data	O
points	O
that	O
they	O
are	O
responsible	O
for	O
.	O
m	O
(	O
k	O
)	O
=	O
xn	O
r	O
(	O
n	O
)	O
k	O
x	O
(	O
n	O
)	O
r	O
(	O
k	O
)	O
where	O
r	O
(	O
k	O
)	O
is	O
the	O
total	O
responsibility	B
of	O
mean	B
k	O
,	O
r	O
(	O
k	O
)	O
=xn	O
r	O
(	O
n	O
)	O
k	O
:	O
(	O
20.8	O
)	O
(	O
20.9	O
)	O
notice	O
the	O
similarity	O
of	O
this	O
soft	B
k-means	O
algorithm	B
to	O
the	O
hard	O
k-means	O
algorithm	B
20.2.	O
the	O
update	O
step	O
is	O
identical	O
;	O
the	O
only	O
di	O
(	O
cid:11	O
)	O
erence	O
is	O
that	O
the	O
responsibilities	O
r	O
(	O
n	O
)	O
can	O
take	O
on	O
values	O
between	O
0	O
and	O
1.	O
whereas	O
the	O
assign-	O
ment	O
^k	O
(	O
n	O
)	O
in	O
the	O
k-means	O
algorithm	B
involved	O
a	O
‘	O
min	O
’	O
over	O
the	O
distances	O
,	O
the	O
rule	O
for	O
assigning	B
the	O
responsibilities	O
is	O
a	O
‘	O
soft-min	O
’	O
(	O
20.7	O
)	O
.	O
k	O
.	O
exercise	O
20.2	O
.	O
[	O
2	O
]	O
show	O
that	O
as	O
the	O
sti	O
(	O
cid:11	O
)	O
ness	O
(	O
cid:12	O
)	O
goes	O
to	O
1	O
,	O
the	O
soft	B
k-means	O
algo-	O
rithm	O
becomes	O
identical	O
to	O
the	O
original	O
hard	O
k-means	O
algorithm	B
,	O
except	O
for	O
the	O
way	O
in	O
which	O
means	O
with	O
no	O
assigned	O
points	O
behave	O
.	O
describe	O
what	O
those	O
means	O
do	O
instead	O
of	O
sitting	O
still	O
.	O
dimensionally	O
,	O
the	O
sti	O
(	O
cid:11	O
)	O
ness	O
(	O
cid:12	O
)	O
is	O
an	O
inverse-length-squared	O
,	O
so	O
we	O
can	O
as-	O
sociate	O
a	O
lengthscale	O
,	O
(	O
cid:27	O
)	O
(	O
cid:17	O
)	O
1=p	O
(	O
cid:12	O
)	O
,	O
with	O
it	O
.	O
the	O
soft	B
k-means	O
algorithm	B
is	O
demonstrated	O
in	O
(	O
cid:12	O
)	O
gure	O
20.8.	O
the	O
lengthscale	O
is	O
shown	O
by	O
the	O
radius	O
of	O
the	O
circles	O
surrounding	O
the	O
four	O
means	O
.	O
each	O
panel	O
shows	O
the	O
(	O
cid:12	O
)	O
nal	O
(	O
cid:12	O
)	O
xed	O
point	O
reached	O
for	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
value	O
of	O
the	O
lengthscale	O
(	O
cid:27	O
)	O
.	O
20.3	O
conclusion	O
at	O
this	O
point	O
,	O
we	O
may	O
have	O
(	O
cid:12	O
)	O
xed	O
some	O
of	O
the	O
problems	O
with	O
the	O
original	O
k-	O
means	O
algorithm	B
by	O
introducing	O
an	O
extra	O
complexity-control	O
parameter	O
(	O
cid:12	O
)	O
.	O
but	O
how	O
should	O
we	O
set	B
(	O
cid:12	O
)	O
?	O
and	O
what	O
about	O
the	O
problem	O
of	O
the	O
elongated	O
clusters	O
,	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
290	O
large	O
(	O
cid:27	O
)	O
:	O
:	O
:	O
:	O
:	O
:	O
20	O
|	O
an	O
example	O
inference	B
task	O
:	O
clustering	B
figure	O
20.8.	O
soft	B
k-means	O
algorithm	B
,	O
version	O
1	O
,	O
applied	O
to	O
a	O
data	B
set	I
of	O
40	O
points	O
.	O
k	O
=	O
4.	O
implicit	O
lengthscale	O
parameter	O
(	O
cid:27	O
)	O
=	O
1=	O
(	O
cid:12	O
)	O
1=2	O
varied	O
from	O
a	O
large	O
to	O
a	O
small	O
value	O
.	O
each	O
picture	O
shows	O
the	O
state	O
of	O
all	O
four	O
means	O
,	O
with	O
the	O
implicit	O
lengthscale	O
shown	O
by	O
the	O
radius	O
of	O
the	O
four	O
circles	O
,	O
after	O
running	O
the	O
algorithm	B
for	O
several	O
tens	O
of	O
iterations	O
.	O
at	O
the	O
largest	O
lengthscale	O
,	O
all	O
four	O
means	O
converge	O
exactly	O
to	O
the	O
data	O
mean	O
.	O
then	O
the	O
four	O
means	O
separate	O
into	O
two	O
groups	O
of	O
two	O
.	O
at	O
shorter	O
lengthscales	O
,	O
each	O
of	O
these	O
pairs	O
itself	O
bifurcates	O
into	O
subgroups	O
.	O
:	O
:	O
:	O
small	O
(	O
cid:27	O
)	O
and	O
the	O
clusters	O
of	O
unequal	O
weight	B
and	O
width	O
?	O
adding	O
one	O
sti	O
(	O
cid:11	O
)	O
ness	O
parameter	O
(	O
cid:12	O
)	O
is	O
not	O
going	O
to	O
make	O
all	O
these	O
problems	O
go	O
away	O
.	O
we	O
’	O
ll	O
come	O
back	O
to	O
these	O
questions	O
in	O
a	O
later	O
chapter	O
,	O
as	O
we	O
develop	O
the	O
mixture-density-modelling	O
view	O
of	O
clustering	O
.	O
further	O
reading	O
for	O
a	O
vector-quantization	O
approach	O
to	O
clustering	B
see	O
(	O
luttrell	O
,	O
1989	O
;	O
luttrell	O
,	O
1990	O
)	O
.	O
20.4	O
exercises	O
.	O
exercise	O
20.3	O
.	O
[	O
3	O
,	O
p.291	O
]	O
explore	B
the	O
properties	O
of	O
the	O
soft	O
k-means	O
algorithm	B
,	O
version	O
1	O
,	O
assuming	O
that	O
the	O
datapoints	O
fxg	O
come	O
from	O
a	O
single	O
separable	O
two-dimensional	B
gaussian	O
distribution	B
with	O
mean	B
zero	O
and	O
variances	O
(	O
var	O
(	O
x1	O
)	O
;	O
var	O
(	O
x2	O
)	O
)	O
=	O
(	O
(	O
cid:27	O
)	O
2	O
2.	O
set	B
k	O
=	O
2	O
,	O
assume	O
n	O
is	O
large	O
,	O
and	O
investigate	O
the	O
(	O
cid:12	O
)	O
xed	O
points	O
of	O
the	O
algorithm	O
as	O
(	O
cid:12	O
)	O
is	O
varied	O
.	O
[	O
hint	O
:	O
assume	O
that	O
m	O
(	O
1	O
)	O
=	O
(	O
m	O
;	O
0	O
)	O
and	O
m	O
(	O
2	O
)	O
=	O
(	O
(	O
cid:0	O
)	O
m	O
;	O
0	O
)	O
.	O
]	O
2	O
)	O
,	O
with	O
(	O
cid:27	O
)	O
2	O
1	O
;	O
(	O
cid:27	O
)	O
2	O
1	O
>	O
(	O
cid:27	O
)	O
2	O
.	O
exercise	O
20.4	O
.	O
[	O
3	O
]	O
consider	O
the	O
soft	B
k-means	O
algorithm	B
applied	O
to	O
a	O
large	O
amount	O
of	O
one-dimensional	O
data	O
that	O
comes	O
from	O
a	O
mixture	O
of	O
two	O
equal-	O
weight	B
gaussians	O
with	O
true	O
means	O
(	O
cid:22	O
)	O
=	O
(	O
cid:6	O
)	O
1	O
and	O
standard	O
deviation	O
(	O
cid:27	O
)	O
p	O
,	O
for	O
example	O
(	O
cid:27	O
)	O
p	O
=	O
1.	O
show	O
that	O
the	O
hard	O
k-means	O
algorithm	B
with	O
k	O
=	O
2	O
leads	O
to	O
a	O
solution	O
in	O
which	O
the	O
two	O
means	O
are	O
further	O
apart	O
than	O
the	O
two	O
true	O
means	O
.	O
discuss	O
what	O
happens	O
for	O
other	O
values	O
of	O
(	O
cid:12	O
)	O
,	O
and	O
(	O
cid:12	O
)	O
nd	O
the	O
value	O
of	O
(	O
cid:12	O
)	O
such	O
that	O
the	O
soft	B
algorithm	O
puts	O
the	O
two	O
means	O
in	O
the	O
correct	O
places	O
.	O
-1	O
1	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
20.5	O
:	O
solutions	O
20.5	O
solutions	O
solution	O
to	O
exercise	O
20.1	O
(	O
p.287	O
)	O
.	O
we	O
can	O
associate	O
an	O
‘	O
energy	B
’	O
with	O
the	O
state	O
of	O
the	O
k-means	O
algorithm	B
by	O
connecting	O
a	O
spring	B
between	O
each	O
point	O
x	O
(	O
n	O
)	O
and	O
the	O
mean	B
that	O
is	O
responsible	O
for	O
it	O
.	O
the	O
energy	B
of	O
one	O
spring	B
is	O
proportional	O
to	O
its	O
squared	O
length	B
,	O
namely	O
(	O
cid:12	O
)	O
d	O
(	O
x	O
(	O
n	O
)	O
;	O
m	O
(	O
k	O
)	O
)	O
where	O
(	O
cid:12	O
)	O
is	O
the	O
sti	O
(	O
cid:11	O
)	O
ness	O
of	O
the	O
spring	O
.	O
the	O
total	O
energy	B
of	O
all	O
the	O
springs	O
is	O
a	O
lyapunov	O
function	B
for	O
the	O
algorithm	B
,	O
because	O
(	O
a	O
)	O
the	O
assignment	O
step	O
can	O
only	O
decrease	O
the	O
energy	B
{	O
a	O
point	O
only	O
changes	O
its	O
allegiance	O
if	O
the	O
length	B
of	O
its	O
spring	B
would	O
be	O
reduced	O
;	O
(	O
b	O
)	O
the	O
update	O
step	O
can	O
only	O
decrease	O
the	O
energy	B
{	O
moving	O
m	O
(	O
k	O
)	O
to	O
the	O
mean	B
is	O
the	O
way	O
to	O
minimize	O
the	O
energy	B
of	O
its	O
springs	O
;	O
and	O
(	O
c	O
)	O
the	O
energy	B
is	O
bounded	O
below	O
{	O
which	O
is	O
the	O
second	O
condition	O
for	O
a	O
lyapunov	O
function	B
.	O
since	O
the	O
algorithm	B
has	O
a	O
lyapunov	O
function	B
,	O
it	O
converges	O
.	O
291	O
m1	O
m2	O
if	O
the	O
means	O
are	O
initialized	O
to	O
m	O
(	O
1	O
)	O
=	O
(	O
m	O
;	O
0	O
)	O
solution	O
to	O
exercise	O
20.3	O
(	O
p.290	O
)	O
.	O
and	O
m	O
(	O
1	O
)	O
=	O
(	O
(	O
cid:0	O
)	O
m	O
;	O
0	O
)	O
,	O
the	O
assignment	O
step	O
for	O
a	O
point	O
at	O
location	O
x1	O
;	O
x2	O
gives	O
m2	O
m1	O
r1	O
(	O
x	O
)	O
=	O
=	O
and	O
the	O
updated	O
m	O
is	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
(	O
x1	O
(	O
cid:0	O
)	O
m	O
)	O
2=2	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
(	O
x1	O
(	O
cid:0	O
)	O
m	O
)	O
2=2	O
)	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
(	O
x1	O
+	O
m	O
)	O
2=2	O
)	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
2	O
(	O
cid:12	O
)	O
mx1	O
)	O
1	O
;	O
m0	O
=	O
r	O
dx1	O
p	O
(	O
x1	O
)	O
x1	O
r1	O
(	O
x	O
)	O
r	O
dx1	O
p	O
(	O
x1	O
)	O
r1	O
(	O
x	O
)	O
=	O
2z	O
dx1	O
p	O
(	O
x1	O
)	O
x1	O
1	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
2	O
(	O
cid:12	O
)	O
mx1	O
)	O
(	O
20.10	O
)	O
(	O
20.11	O
)	O
(	O
20.12	O
)	O
figure	O
20.9.	O
schematic	O
diagram	O
of	O
the	O
bifurcation	O
as	O
the	O
largest	O
data	O
variance	O
(	O
cid:27	O
)	O
1	O
increases	O
from	O
below	O
1=	O
(	O
cid:12	O
)	O
1=2	O
to	O
above	O
1=	O
(	O
cid:12	O
)	O
1=2	O
.	O
the	O
data	O
variance	O
is	O
indicated	O
by	O
the	O
ellipse	O
.	O
:	O
(	O
20.13	O
)	O
now	O
,	O
m	O
=	O
0	O
is	O
a	O
(	O
cid:12	O
)	O
xed	O
point	O
,	O
but	O
the	O
question	O
is	O
,	O
is	O
it	O
stable	O
or	O
unstable	O
?	O
for	O
tiny	O
m	O
(	O
that	O
is	O
,	O
(	O
cid:12	O
)	O
(	O
cid:27	O
)	O
1m	O
(	O
cid:28	O
)	O
1	O
)	O
,	O
we	O
can	O
taylor-expand	O
1	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
2	O
(	O
cid:12	O
)	O
mx1	O
)	O
’	O
1	O
2	O
(	O
1	O
+	O
(	O
cid:12	O
)	O
mx1	O
)	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
so	O
m0	O
’	O
z	O
dx1	O
p	O
(	O
x1	O
)	O
x1	O
(	O
1	O
+	O
(	O
cid:12	O
)	O
mx1	O
)	O
=	O
(	O
cid:27	O
)	O
2	O
1	O
(	O
cid:12	O
)	O
m	O
:	O
(	O
20.14	O
)	O
(	O
20.15	O
)	O
(	O
20.16	O
)	O
for	O
small	O
m	O
,	O
m	O
either	O
grows	O
or	O
decays	O
exponentially	O
under	O
this	O
mapping	B
,	O
depending	O
on	O
whether	O
(	O
cid:27	O
)	O
2	O
1	O
(	O
cid:12	O
)	O
is	O
greater	O
than	O
or	O
less	O
than	O
1.	O
the	O
(	O
cid:12	O
)	O
xed	O
point	O
m	O
=	O
0	O
is	O
stable	O
if	O
(	O
cid:27	O
)	O
2	O
1	O
(	O
cid:20	O
)	O
1=	O
(	O
cid:12	O
)	O
(	O
20.17	O
)	O
if	O
(	O
cid:27	O
)	O
2	O
and	O
unstable	O
otherwise	O
.	O
[	O
incidentally	O
,	O
this	O
derivation	B
shows	O
that	O
this	O
result	O
is	O
general	O
,	O
holding	O
for	O
any	O
true	O
probability	B
distribution	O
p	O
(	O
x1	O
)	O
having	O
variance	B
(	O
cid:27	O
)	O
2	O
1	O
,	O
not	O
just	O
the	O
gaussian	O
.	O
]	O
1	O
>	O
1=	O
(	O
cid:12	O
)	O
then	O
there	O
is	O
a	O
bifurcation	B
and	O
there	O
are	O
two	O
stable	O
(	O
cid:12	O
)	O
xed	O
points	O
surrounding	O
the	O
unstable	O
(	O
cid:12	O
)	O
xed	O
point	O
at	O
m	O
=	O
0.	O
to	O
illustrate	O
this	O
bifurcation	B
,	O
(	O
cid:12	O
)	O
gure	O
20.10	O
shows	O
the	O
outcome	O
of	O
running	O
the	O
soft	B
k-means	O
algorithm	B
with	O
(	O
cid:12	O
)	O
=	O
1	O
on	O
one-dimensional	O
data	O
with	O
standard	B
deviation	I
(	O
cid:27	O
)	O
1	O
for	O
various	O
values	O
of	O
(	O
cid:27	O
)	O
1.	O
figure	O
20.11	O
shows	O
this	O
pitchfork	B
bifurcation	I
from	O
the	O
other	O
point	O
of	O
view	O
,	O
where	O
the	O
data	O
’	O
s	O
standard	B
deviation	I
(	O
cid:27	O
)	O
1	O
is	O
(	O
cid:12	O
)	O
xed	O
and	O
the	O
algorithm	B
’	O
s	O
lengthscale	O
(	O
cid:27	O
)	O
=	O
1=	O
(	O
cid:12	O
)	O
1=2	O
is	O
varied	O
on	O
the	O
horizontal	O
axis	O
.	O
4	O
3	O
2	O
1	O
0	O
-1	O
-2	O
-3	O
-4	O
data	O
density	O
mean	B
locations	O
-2-1	O
0	O
1	O
2	O
-2-1	O
0	O
1	O
2	O
0	O
0.5	O
1	O
1.5	O
2	O
2.5	O
3	O
3.5	O
4	O
figure	O
20.10.	O
the	O
stable	O
mean	B
locations	O
as	O
a	O
function	B
of	O
(	O
cid:27	O
)	O
1	O
,	O
for	O
constant	O
(	O
cid:12	O
)	O
,	O
found	O
numerically	O
(	O
thick	O
lines	O
)	O
,	O
and	O
the	O
approximation	B
(	O
20.22	O
)	O
(	O
thin	O
lines	O
)	O
.	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
-0.2	O
-0.4	O
-0.6	O
-0.8	O
data	O
density	O
mean	B
locns	O
.	O
-2	O
-1	O
0	O
1	O
2	O
-2	O
-1	O
0	O
1	O
2	O
0	O
0.5	O
1	O
1.5	O
2	O
figure	O
20.11.	O
the	O
stable	O
mean	B
locations	O
as	O
a	O
function	B
of	O
1=	O
(	O
cid:12	O
)	O
1=2	O
,	O
for	O
constant	O
(	O
cid:27	O
)	O
1	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
292	O
20	O
|	O
an	O
example	O
inference	B
task	O
:	O
clustering	B
here	O
is	O
a	O
cheap	O
theory	B
to	O
model	B
how	O
the	O
(	O
cid:12	O
)	O
tted	O
parameters	B
(	O
cid:6	O
)	O
m	O
behave	O
beyond	O
the	O
bifurcation	B
,	O
based	O
on	O
continuing	O
the	O
series	O
expansion	O
.	O
this	O
continuation	O
of	O
the	O
series	O
is	O
rather	O
suspect	O
,	O
since	O
the	O
series	O
isn	O
’	O
t	O
necessarily	O
expected	O
to	O
converge	O
beyond	O
the	O
bifurcation	B
point	O
,	O
but	O
the	O
theory	B
(	O
cid:12	O
)	O
ts	O
well	O
anyway	O
.	O
we	O
take	O
our	O
analytic	O
approach	O
one	O
term	O
further	O
in	O
the	O
expansion	O
1	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
2	O
(	O
cid:12	O
)	O
mx1	O
)	O
’	O
1	O
2	O
(	O
1	O
+	O
(	O
cid:12	O
)	O
mx1	O
(	O
cid:0	O
)	O
1	O
3	O
(	O
(	O
cid:12	O
)	O
mx1	O
)	O
3	O
)	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
20.18	O
)	O
then	O
we	O
can	O
solve	O
for	O
the	O
shape	O
of	O
the	O
bifurcation	O
to	O
leading	O
order	O
,	O
which	O
depends	O
on	O
the	O
fourth	O
moment	O
of	O
the	O
distribution	O
:	O
m0	O
’	O
z	O
dx1	O
p	O
(	O
x1	O
)	O
x1	O
(	O
1	O
+	O
(	O
cid:12	O
)	O
mx1	O
(	O
cid:0	O
)	O
(	O
(	O
cid:12	O
)	O
m	O
)	O
33	O
(	O
cid:27	O
)	O
4	O
1	O
:	O
=	O
(	O
cid:27	O
)	O
2	O
1	O
(	O
cid:12	O
)	O
m	O
(	O
cid:0	O
)	O
1	O
3	O
1	O
3	O
(	O
(	O
cid:12	O
)	O
mx1	O
)	O
3	O
)	O
(	O
20.19	O
)	O
(	O
20.20	O
)	O
[	O
at	O
(	O
20.20	O
)	O
we	O
use	O
the	O
fact	O
that	O
p	O
(	O
x1	O
)	O
is	O
gaussian	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
fourth	O
moment	O
.	O
]	O
this	O
map	O
has	O
a	O
(	O
cid:12	O
)	O
xed	O
point	O
at	O
m	O
such	O
that	O
(	O
cid:27	O
)	O
2	O
1	O
(	O
cid:12	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
(	O
cid:12	O
)	O
m	O
)	O
2	O
(	O
cid:27	O
)	O
2	O
1	O
)	O
=	O
1	O
;	O
(	O
20.21	O
)	O
i.e.	O
,	O
m	O
=	O
(	O
cid:6	O
)	O
(	O
cid:12	O
)	O
(	O
cid:0	O
)	O
1=2	O
(	O
(	O
cid:27	O
)	O
2	O
1	O
(	O
cid:12	O
)	O
(	O
cid:0	O
)	O
1	O
)	O
1=2	O
(	O
cid:27	O
)	O
2	O
1	O
(	O
cid:12	O
)	O
:	O
(	O
20.22	O
)	O
the	O
thin	O
line	O
in	O
(	O
cid:12	O
)	O
gure	O
20.10	O
shows	O
this	O
theoretical	O
approximation	B
.	O
figure	O
20.10	O
shows	O
the	O
bifurcation	B
as	O
a	O
function	B
of	O
(	O
cid:27	O
)	O
1	O
for	O
(	O
cid:12	O
)	O
xed	O
(	O
cid:12	O
)	O
;	O
(	O
cid:12	O
)	O
gure	O
20.11	O
shows	O
the	O
bifurcation	B
as	O
a	O
function	B
of	O
1=	O
(	O
cid:12	O
)	O
1=2	O
for	O
(	O
cid:12	O
)	O
xed	O
(	O
cid:27	O
)	O
1.	O
.	O
exercise	O
20.5	O
.	O
[	O
2	O
,	O
p.292	O
]	O
why	O
does	O
the	O
pitchfork	O
in	O
(	O
cid:12	O
)	O
gure	O
20.11	O
tend	O
to	O
the	O
val-	O
ues	O
(	O
cid:24	O
)	O
(	O
cid:6	O
)	O
0:8	O
as	O
1=	O
(	O
cid:12	O
)	O
1=2	O
!	O
0	O
?	O
give	O
an	O
analytic	O
expression	O
for	O
this	O
asymp-	O
tote	O
.	O
solution	O
to	O
exercise	O
20.5	O
(	O
p.292	O
)	O
.	O
the	O
asymptote	O
is	O
the	O
mean	B
of	O
the	O
recti	O
(	O
cid:12	O
)	O
ed	O
gaussian	O
,	O
r	O
10	O
normal	B
(	O
x	O
;	O
1	O
)	O
x	O
dx	O
1=2	O
=p2=	O
(	O
cid:25	O
)	O
’	O
0:798	O
:	O
(	O
20.23	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
21	O
exact	O
inference	O
by	O
complete	O
enumeration	O
we	O
open	O
our	O
toolbox	O
of	O
methods	O
for	O
handling	O
probabilities	O
by	O
discussing	O
a	O
brute-force	O
inference	B
method	O
:	O
complete	O
enumeration	O
of	O
all	O
hypotheses	O
,	O
and	O
evaluation	O
of	O
their	O
probabilities	O
.	O
this	O
approach	O
is	O
an	O
exact	O
method	O
,	O
and	O
the	O
di	O
(	O
cid:14	O
)	O
culty	O
of	O
carrying	O
it	O
out	O
will	O
motivate	O
the	O
smarter	O
exact	O
and	O
approximate	O
methods	B
introduced	O
in	O
the	O
following	O
chapters	O
.	O
21.1	O
the	O
burglar	O
alarm	O
bayesian	O
probability	B
theory	O
is	O
sometimes	O
called	O
‘	O
common	O
sense	O
,	O
ampli	O
(	O
cid:12	O
)	O
ed	O
’	O
.	O
when	O
thinking	O
about	O
the	O
following	O
questions	O
,	O
please	O
ask	O
your	O
common	O
sense	O
what	O
it	O
thinks	O
the	O
answers	O
are	O
;	O
we	O
will	O
then	O
see	O
how	O
bayesian	O
methods	B
con	O
(	O
cid:12	O
)	O
rm	O
your	O
everyday	O
intuition	O
.	O
example	O
21.1.	O
fred	O
lives	O
in	O
los	O
angeles	O
and	O
commutes	O
60	O
miles	O
to	O
work	O
.	O
whilst	O
at	O
work	O
,	O
he	O
receives	O
a	O
phone-call	O
from	O
his	O
neighbour	O
saying	O
that	O
fred	O
’	O
s	O
burglar	O
alarm	O
is	O
ringing	O
.	O
what	O
is	O
the	O
probability	B
that	O
there	O
was	O
a	O
burglar	O
in	O
his	O
house	O
today	O
?	O
while	O
driving	O
home	O
to	O
investigate	O
,	O
fred	O
hears	O
on	O
the	O
radio	B
that	O
there	O
was	O
a	O
small	O
earthquake	B
that	O
day	O
near	O
his	O
home	O
.	O
‘	O
oh	O
’	O
,	O
he	O
says	O
,	O
feeling	O
relieved	O
,	O
‘	O
it	O
was	O
probably	O
the	O
earthquake	B
that	O
set	B
o	O
(	O
cid:11	O
)	O
the	O
alarm	O
’	O
.	O
what	O
is	O
the	O
probability	B
that	O
there	O
was	O
a	O
burglar	O
in	O
his	O
house	O
?	O
(	O
after	O
pearl	O
,	O
1988	O
)	O
.	O
let	O
’	O
s	O
introduce	O
variables	O
b	O
(	O
a	O
burglar	O
was	O
present	O
in	O
fred	O
’	O
s	O
house	O
today	O
)	O
,	O
a	O
(	O
the	O
alarm	O
is	O
ringing	O
)	O
,	O
p	O
(	O
fred	O
receives	O
a	O
phonecall	O
from	O
the	O
neighbour	O
re-	O
porting	O
the	O
alarm	O
)	O
,	O
e	O
(	O
a	O
small	O
earthquake	B
took	O
place	O
today	O
near	O
fred	O
’	O
s	O
house	O
)	O
,	O
and	O
r	O
(	O
the	O
radio	B
report	O
of	O
earthquake	O
is	O
heard	O
by	O
fred	O
)	O
.	O
the	O
probability	O
of	O
all	O
these	O
variables	O
might	O
factorize	O
as	O
follows	O
:	O
p	O
(	O
b	O
;	O
e	O
;	O
a	O
;	O
p	O
;	O
r	O
)	O
=	O
p	O
(	O
b	O
)	O
p	O
(	O
e	O
)	O
p	O
(	O
aj	O
b	O
;	O
e	O
)	O
p	O
(	O
pj	O
a	O
)	O
p	O
(	O
r	O
j	O
e	O
)	O
;	O
(	O
21.1	O
)	O
and	O
plausible	O
values	O
for	O
the	O
probabilities	O
are	O
:	O
1.	O
burglar	O
probability	O
:	O
p	O
(	O
b	O
=	O
1	O
)	O
=	O
(	O
cid:12	O
)	O
;	O
p	O
(	O
b	O
=	O
0	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
;	O
(	O
21.2	O
)	O
e.g.	O
,	O
(	O
cid:12	O
)	O
=	O
0:001	O
gives	O
a	O
mean	B
burglary	O
rate	B
of	O
once	O
every	O
three	O
years	O
.	O
2.	O
earthquake	B
probability	O
:	O
p	O
(	O
e	O
=	O
1	O
)	O
=	O
(	O
cid:15	O
)	O
;	O
p	O
(	O
e	O
=	O
0	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
;	O
(	O
21.3	O
)	O
293	O
jearthquake	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:9	O
)	O
jradio	O
@	O
@	O
r	O
jburglar	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:9	O
)	O
jalarm	O
@	O
@	O
r	O
jphonecall	O
figure	O
21.1.	O
belief	B
network	O
for	O
the	O
burglar	O
alarm	O
problem	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
294	O
21	O
|	O
exact	O
inference	O
by	O
complete	O
enumeration	O
with	O
,	O
e.g.	O
,	O
(	O
cid:15	O
)	O
=	O
0:001	O
;	O
our	O
assertion	O
that	O
the	O
earthquakes	O
are	O
independent	O
of	O
burglars	O
,	O
i.e.	O
,	O
the	O
prior	B
probability	O
of	O
b	O
and	O
e	O
is	O
p	O
(	O
b	O
;	O
e	O
)	O
=	O
p	O
(	O
b	O
)	O
p	O
(	O
e	O
)	O
,	O
seems	O
reasonable	O
unless	O
we	O
take	O
into	O
account	O
opportunistic	O
burglars	O
who	O
strike	O
immediately	O
after	O
earthquakes	O
.	O
3.	O
alarm	O
ringing	O
probability	B
:	O
we	O
assume	O
the	O
alarm	O
will	O
ring	O
if	O
any	O
of	O
the	O
following	O
three	O
events	O
happens	O
:	O
(	O
a	O
)	O
a	O
burglar	O
enters	O
the	O
house	O
,	O
and	O
trig-	O
gers	O
the	O
alarm	O
(	O
let	O
’	O
s	O
assume	O
the	O
alarm	O
has	O
a	O
reliability	O
of	O
(	O
cid:11	O
)	O
b	O
=	O
0:99	O
,	O
i.e.	O
,	O
99	O
%	O
of	O
burglars	O
trigger	O
the	O
alarm	O
)	O
;	O
(	O
b	O
)	O
an	O
earthquake	B
takes	O
place	O
,	O
and	O
triggers	O
the	O
alarm	O
(	O
perhaps	O
(	O
cid:11	O
)	O
e	O
=	O
1	O
%	O
of	O
alarms	O
are	O
triggered	O
by	O
earth-	O
quakes	O
?	O
)	O
;	O
or	O
(	O
c	O
)	O
some	O
other	O
event	O
causes	O
a	O
false	O
alarm	O
;	O
let	O
’	O
s	O
assume	O
the	O
false	O
alarm	O
rate	B
f	O
is	O
0.001	O
,	O
so	O
fred	O
has	O
false	O
alarms	O
from	O
non-earthquake	O
causes	O
once	O
every	O
three	O
years	O
.	O
[	O
this	O
type	O
of	O
dependence	O
of	O
a	O
on	O
b	O
and	O
e	O
is	O
known	O
as	O
a	O
‘	O
noisy-or	B
’	O
.	O
]	O
the	O
probabilities	O
of	O
a	O
given	O
b	O
and	O
e	O
are	O
then	O
:	O
p	O
(	O
a	O
=	O
1j	O
b	O
=	O
0	O
;	O
e	O
=	O
0	O
)	O
=	O
f	O
p	O
(	O
a	O
=	O
1j	O
b	O
=	O
1	O
;	O
e	O
=	O
0	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
b	O
)	O
p	O
(	O
a	O
=	O
1j	O
b	O
=	O
0	O
;	O
e	O
=	O
1	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
e	O
)	O
p	O
(	O
a	O
=	O
0j	O
b	O
=	O
0	O
;	O
e	O
=	O
0	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
;	O
p	O
(	O
a	O
=	O
0j	O
b	O
=	O
1	O
;	O
e	O
=	O
0	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
b	O
)	O
;	O
p	O
(	O
a	O
=	O
0j	O
b	O
=	O
0	O
;	O
e	O
=	O
1	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
e	O
)	O
;	O
p	O
(	O
a	O
=	O
0j	O
b	O
=	O
1	O
;	O
e	O
=	O
1	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
b	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
e	O
)	O
;	O
p	O
(	O
a	O
=	O
1j	O
b	O
=	O
1	O
;	O
e	O
=	O
1	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
b	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
e	O
)	O
or	O
,	O
in	O
numbers	O
,	O
p	O
(	O
a	O
=	O
0j	O
b	O
=	O
0	O
;	O
e	O
=	O
0	O
)	O
=	O
0:999	O
;	O
p	O
(	O
a	O
=	O
0j	O
b	O
=	O
1	O
;	O
e	O
=	O
0	O
)	O
=	O
0:009	O
99	O
;	O
p	O
(	O
a	O
=	O
0j	O
b	O
=	O
0	O
;	O
e	O
=	O
1	O
)	O
=	O
0:989	O
01	O
;	O
p	O
(	O
a	O
=	O
0j	O
b	O
=	O
1	O
;	O
e	O
=	O
1	O
)	O
=	O
0:009	O
890	O
1	O
;	O
p	O
(	O
a	O
=	O
1j	O
b	O
=	O
1	O
;	O
e	O
=	O
1	O
)	O
=	O
0:990	O
109	O
9	O
:	O
p	O
(	O
a	O
=	O
1j	O
b	O
=	O
0	O
;	O
e	O
=	O
0	O
)	O
=	O
0:001	O
p	O
(	O
a	O
=	O
1j	O
b	O
=	O
1	O
;	O
e	O
=	O
0	O
)	O
=	O
0:990	O
01	O
p	O
(	O
a	O
=	O
1j	O
b	O
=	O
0	O
;	O
e	O
=	O
1	O
)	O
=	O
0:010	O
99	O
we	O
assume	O
the	O
neighbour	O
would	O
never	O
phone	B
if	O
the	O
alarm	O
is	O
not	O
ringing	O
[	O
p	O
(	O
p	O
=	O
1j	O
a	O
=	O
0	O
)	O
=	O
0	O
]	O
;	O
and	O
that	O
the	O
radio	B
is	O
a	O
trustworthy	O
reporter	O
too	O
[	O
p	O
(	O
r	O
=	O
1j	O
e	O
=	O
0	O
)	O
=	O
0	O
]	O
;	O
we	O
won	O
’	O
t	O
need	O
to	O
specify	O
the	O
probabilities	O
p	O
(	O
p	O
=	O
1j	O
a	O
=	O
1	O
)	O
or	O
p	O
(	O
r	O
=	O
1j	O
e	O
=	O
1	O
)	O
in	O
order	O
to	O
answer	O
the	O
questions	O
above	O
,	O
since	O
the	O
outcomes	O
p	O
=	O
1	O
and	O
r	O
=	O
1	O
give	O
us	O
certainty	O
respectively	O
that	O
a	O
=	O
1	O
and	O
e	O
=	O
1.	O
we	O
can	O
answer	O
the	O
two	O
questions	O
about	O
the	O
burglar	O
by	O
computing	O
the	O
posterior	O
probabilities	O
of	O
all	O
hypotheses	O
given	O
the	O
available	O
information	B
.	O
let	O
’	O
s	O
start	O
by	O
reminding	O
ourselves	O
that	O
the	O
probability	B
that	O
there	O
is	O
a	O
burglar	O
,	O
before	O
either	O
p	O
or	O
r	O
is	O
observed	O
,	O
is	O
p	O
(	O
b	O
=	O
1	O
)	O
=	O
(	O
cid:12	O
)	O
=	O
0:001	O
,	O
and	O
the	O
probability	B
that	O
an	O
earthquake	B
took	O
place	O
is	O
p	O
(	O
e	O
=	O
1	O
)	O
=	O
(	O
cid:15	O
)	O
=	O
0:001	O
,	O
and	O
these	O
two	O
propositions	O
are	O
independent	O
.	O
first	O
,	O
when	O
p	O
=	O
1	O
,	O
we	O
know	O
that	O
the	O
alarm	O
is	O
ringing	O
:	O
a	O
=	O
1.	O
the	O
posterior	B
probability	I
of	O
b	O
and	O
e	O
becomes	O
:	O
p	O
(	O
b	O
;	O
ej	O
a	O
=	O
1	O
)	O
=	O
p	O
(	O
a	O
=	O
1j	O
b	O
;	O
e	O
)	O
p	O
(	O
b	O
)	O
p	O
(	O
e	O
)	O
p	O
(	O
a	O
=	O
1	O
)	O
:	O
(	O
21.4	O
)	O
the	O
numerator	O
’	O
s	O
four	O
possible	O
values	O
are	O
p	O
(	O
a	O
=	O
1j	O
b	O
=	O
0	O
;	O
e	O
=	O
0	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
b	O
=	O
0	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
e	O
=	O
0	O
)	O
=	O
0:001	O
(	O
cid:2	O
)	O
0:999	O
(	O
cid:2	O
)	O
0:999	O
=	O
0:000	O
998	O
p	O
(	O
a	O
=	O
1j	O
b	O
=	O
1	O
;	O
e	O
=	O
0	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
b	O
=	O
1	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
e	O
=	O
0	O
)	O
=	O
0:990	O
01	O
(	O
cid:2	O
)	O
0:001	O
(	O
cid:2	O
)	O
0:999	O
=	O
0:000	O
989	O
p	O
(	O
a	O
=	O
1j	O
b	O
=	O
0	O
;	O
e	O
=	O
1	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
b	O
=	O
0	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
e	O
=	O
1	O
)	O
=	O
0:010	O
99	O
(	O
cid:2	O
)	O
0:999	O
(	O
cid:2	O
)	O
0:001	O
=	O
0:000	O
010	O
979	O
p	O
(	O
a	O
=	O
1j	O
b	O
=	O
1	O
;	O
e	O
=	O
1	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
b	O
=	O
1	O
)	O
(	O
cid:2	O
)	O
p	O
(	O
e	O
=	O
1	O
)	O
=	O
0:990	O
109	O
9	O
(	O
cid:2	O
)	O
0:001	O
(	O
cid:2	O
)	O
0:001	O
=	O
9:9	O
(	O
cid:2	O
)	O
10	O
(	O
cid:0	O
)	O
7	O
:	O
the	O
normalizing	B
constant	I
is	O
the	O
sum	O
of	O
these	O
four	O
numbers	O
,	O
p	O
(	O
a	O
=	O
1	O
)	O
=	O
0:002	O
,	O
and	O
the	O
posterior	O
probabilities	O
are	O
p	O
(	O
b	O
=	O
0	O
;	O
e	O
=	O
0j	O
a	O
=	O
1	O
)	O
=	O
0:4993	O
p	O
(	O
b	O
=	O
1	O
;	O
e	O
=	O
0j	O
a	O
=	O
1	O
)	O
=	O
0:4947	O
p	O
(	O
b	O
=	O
0	O
;	O
e	O
=	O
1j	O
a	O
=	O
1	O
)	O
=	O
0:0055	O
p	O
(	O
b	O
=	O
1	O
;	O
e	O
=	O
1j	O
a	O
=	O
1	O
)	O
=	O
0:0005	O
:	O
(	O
21.5	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
21.2	O
:	O
exact	O
inference	O
for	O
continuous	O
hypothesis	O
spaces	O
295	O
to	O
answer	O
the	O
question	O
,	O
‘	O
what	O
’	O
s	O
the	O
probability	B
a	O
burglar	O
was	O
there	O
?	O
’	O
we	O
marginalize	O
over	O
the	O
earthquake	B
variable	O
e	O
:	O
p	O
(	O
b	O
=	O
0j	O
a	O
=	O
1	O
)	O
=	O
p	O
(	O
b	O
=	O
0	O
;	O
e	O
=	O
0j	O
a	O
=	O
1	O
)	O
+	O
p	O
(	O
b	O
=	O
0	O
;	O
e	O
=	O
1j	O
a	O
=	O
1	O
)	O
=	O
0:505	O
p	O
(	O
b	O
=	O
1j	O
a	O
=	O
1	O
)	O
=	O
p	O
(	O
b	O
=	O
1	O
;	O
e	O
=	O
0j	O
a	O
=	O
1	O
)	O
+	O
p	O
(	O
b	O
=	O
1	O
;	O
e	O
=	O
1j	O
a	O
=	O
1	O
)	O
=	O
0:495	O
:	O
(	O
21.6	O
)	O
so	O
there	O
is	O
nearly	O
a	O
50	O
%	O
chance	O
that	O
there	O
was	O
a	O
burglar	O
present	O
.	O
it	O
is	O
impor-	O
tant	O
to	O
note	O
that	O
the	O
variables	O
b	O
and	O
e	O
,	O
which	O
were	O
independent	O
a	O
priori	O
,	O
are	O
now	O
dependent	O
.	O
the	O
posterior	O
distribution	O
(	O
21.5	O
)	O
is	O
not	O
a	O
separable	O
function	B
of	O
b	O
and	O
e.	O
this	O
fact	O
is	O
illustrated	O
most	O
simply	O
by	O
studying	O
the	O
e	O
(	O
cid:11	O
)	O
ect	O
of	O
learning	O
that	O
e	O
=	O
1.	O
when	O
we	O
learn	O
e	O
=	O
1	O
,	O
the	O
posterior	B
probability	I
of	O
b	O
is	O
given	O
by	O
p	O
(	O
bj	O
e	O
=	O
1	O
;	O
a	O
=	O
1	O
)	O
=	O
p	O
(	O
b	O
;	O
e	O
=	O
1j	O
a	O
=	O
1	O
)	O
=p	O
(	O
e	O
=	O
1j	O
a	O
=	O
1	O
)	O
,	O
i.e.	O
,	O
by	O
dividing	O
the	O
bot-	O
tom	O
two	O
rows	O
of	O
(	O
21.5	O
)	O
,	O
by	O
their	O
sum	O
p	O
(	O
e	O
=	O
1j	O
a	O
=	O
1	O
)	O
=	O
0:0060.	O
the	O
posterior	B
probability	I
of	O
b	O
is	O
:	O
p	O
(	O
b	O
=	O
0j	O
e	O
=	O
1	O
;	O
a	O
=	O
1	O
)	O
=	O
0:92	O
p	O
(	O
b	O
=	O
1j	O
e	O
=	O
1	O
;	O
a	O
=	O
1	O
)	O
=	O
0:08	O
:	O
(	O
21.7	O
)	O
there	O
is	O
thus	O
now	O
an	O
8	O
%	O
chance	O
that	O
a	O
burglar	O
was	O
in	O
fred	O
’	O
s	O
house	O
.	O
it	O
is	O
in	O
accordance	O
with	O
everyday	O
intuition	O
that	O
the	O
probability	B
that	O
b	O
=	O
1	O
(	O
a	O
pos-	O
sible	O
cause	O
of	O
the	O
alarm	O
)	O
reduces	O
when	O
fred	O
learns	O
that	O
an	O
earthquake	B
,	O
an	O
alternative	O
explanation	O
of	O
the	O
alarm	O
,	O
has	O
happened	O
.	O
explaining	B
away	I
this	O
phenomenon	O
,	O
that	O
one	O
of	O
the	O
possible	O
causes	O
(	O
b	O
=	O
1	O
)	O
of	O
some	O
data	O
(	O
the	O
data	O
in	O
this	O
case	O
being	O
a	O
=	O
1	O
)	O
becomes	O
less	O
probable	O
when	O
another	O
of	O
the	O
causes	O
(	O
e	O
=	O
1	O
)	O
becomes	O
more	O
probable	O
,	O
even	O
though	O
those	O
two	O
causes	O
were	O
indepen-	O
dent	O
variables	O
a	O
priori	O
,	O
is	O
known	O
as	O
explaining	O
away	O
.	O
explaining	B
away	I
is	O
an	O
important	O
feature	O
of	O
correct	O
inferences	O
,	O
and	O
one	O
that	O
any	O
arti	O
(	O
cid:12	O
)	O
cial	O
intelligence	O
should	O
replicate	O
.	O
if	O
we	O
believe	O
that	O
the	O
neighbour	O
and	O
the	O
radio	B
service	O
are	O
unreliable	O
or	O
capricious	O
,	O
so	O
that	O
we	O
are	O
not	O
certain	O
that	O
the	O
alarm	O
really	O
is	O
ringing	O
or	O
that	O
an	O
earthquake	B
really	O
has	O
happened	O
,	O
the	O
calculations	O
become	O
more	O
complex	B
,	O
but	O
the	O
explaining-away	O
e	O
(	O
cid:11	O
)	O
ect	O
persists	O
;	O
the	O
arrival	O
of	O
the	O
earthquake	O
report	O
r	O
simultaneously	O
makes	O
it	O
more	O
probable	O
that	O
the	O
alarm	O
truly	O
is	O
ringing	O
,	O
and	O
less	O
probable	O
that	O
the	O
burglar	O
was	O
present	O
.	O
in	O
summary	O
,	O
we	O
solved	O
the	O
inference	B
questions	O
about	O
the	O
burglar	O
by	O
enu-	O
merating	O
all	O
four	O
hypotheses	O
about	O
the	O
variables	O
(	O
b	O
;	O
e	O
)	O
,	O
(	O
cid:12	O
)	O
nding	O
their	O
posterior	O
probabilities	O
,	O
and	O
marginalizing	O
to	O
obtain	O
the	O
required	O
inferences	O
about	O
b.	O
.	O
exercise	O
21.2	O
.	O
[	O
2	O
]	O
after	O
fred	O
receives	O
the	O
phone-call	O
about	O
the	O
burglar	O
alarm	O
,	O
but	O
before	O
he	O
hears	O
the	O
radio	B
report	O
,	O
what	O
,	O
from	O
his	O
point	O
of	O
view	O
,	O
is	O
the	O
probability	B
that	O
there	O
was	O
a	O
small	O
earthquake	B
today	O
?	O
21.2	O
exact	O
inference	O
for	O
continuous	O
hypothesis	O
spaces	O
many	O
of	O
the	O
hypothesis	O
spaces	O
we	O
will	O
consider	O
are	O
naturally	O
thought	O
of	O
as	O
continuous	B
.	O
for	O
example	O
,	O
the	O
unknown	O
decay	O
length	B
(	O
cid:21	O
)	O
of	O
section	O
3.1	O
(	O
p.48	O
)	O
lives	O
in	O
a	O
continuous	B
one-dimensional	O
space	O
;	O
and	O
the	O
unknown	O
mean	O
and	O
stan-	O
dard	O
deviation	O
of	O
a	O
gaussian	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
live	O
in	O
a	O
continuous	B
two-dimensional	O
space	O
.	O
in	O
any	O
practical	B
computer	O
implementation	O
,	O
such	O
continuous	B
spaces	O
will	O
neces-	O
sarily	O
be	O
discretized	O
,	O
however	O
,	O
and	O
so	O
can	O
,	O
in	O
principle	O
,	O
be	O
enumerated	O
{	O
at	O
a	O
grid	O
of	O
parameter	O
values	O
,	O
for	O
example	O
.	O
in	O
(	O
cid:12	O
)	O
gure	O
3.2	O
we	O
plotted	O
the	O
likelihood	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
296	O
21	O
|	O
exact	O
inference	O
by	O
complete	O
enumeration	O
figure	O
21.2.	O
enumeration	O
of	O
an	O
entire	O
(	O
discretized	O
)	O
hypothesis	O
space	O
for	O
one	O
gaussian	O
with	O
parameters	O
(	O
cid:22	O
)	O
(	O
horizontal	O
axis	O
)	O
and	O
(	O
cid:27	O
)	O
(	O
vertical	O
)	O
.	O
function	B
for	O
the	O
decay	O
length	B
as	O
a	O
function	B
of	O
(	O
cid:21	O
)	O
by	O
evaluating	O
the	O
likelihood	B
at	O
a	O
(	O
cid:12	O
)	O
nely-spaced	O
series	O
of	O
points	O
.	O
a	O
two-parameter	O
model	B
let	O
’	O
s	O
look	O
at	O
the	O
gaussian	O
distribution	B
as	O
an	O
example	O
of	O
a	O
model	B
with	O
a	O
two-	O
dimensional	O
hypothesis	O
space	O
.	O
the	O
one-dimensional	O
gaussian	O
distribution	B
is	O
parameterized	O
by	O
a	O
mean	B
(	O
cid:22	O
)	O
and	O
a	O
standard	B
deviation	I
(	O
cid:27	O
)	O
:	O
p	O
(	O
xj	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
1	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
normal	B
(	O
x	O
;	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
)	O
:	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
)	O
2	O
(	O
21.8	O
)	O
figure	O
21.2	O
shows	O
an	O
enumeration	O
of	O
one	O
hundred	O
hypotheses	O
about	O
the	O
mean	B
and	O
standard	B
deviation	I
of	O
a	O
one-dimensional	O
gaussian	O
distribution	B
.	O
these	O
hypotheses	O
are	O
evenly	O
spaced	O
in	O
a	O
ten	O
by	O
ten	O
square	B
grid	O
covering	O
ten	O
values	O
of	O
(	O
cid:22	O
)	O
and	O
ten	O
values	O
of	O
(	O
cid:27	O
)	O
.	O
each	O
hypothesis	O
is	O
represented	O
by	O
a	O
picture	O
showing	O
the	O
probability	B
density	O
that	O
it	O
puts	O
on	O
x.	O
we	O
now	O
examine	O
the	O
inference	B
of	O
(	O
cid:22	O
)	O
and	O
(	O
cid:27	O
)	O
given	O
data	O
points	O
xn	O
,	O
n	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
n	O
,	O
assumed	O
to	O
be	O
drawn	O
independently	O
from	O
this	O
density	B
.	O
imagine	O
that	O
we	O
acquire	O
data	O
,	O
for	O
example	O
the	O
(	O
cid:12	O
)	O
ve	O
points	O
shown	O
in	O
(	O
cid:12	O
)	O
g-	O
ure	O
21.3.	O
we	O
can	O
now	O
evaluate	O
the	O
posterior	B
probability	I
of	O
each	O
of	O
the	O
one	O
hundred	O
subhypotheses	O
by	O
evaluating	O
the	O
likelihood	B
of	O
each	O
,	O
that	O
is	O
,	O
the	O
value	O
of	O
p	O
(	O
fxng5	O
n=1	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
.	O
the	O
likelihood	B
values	O
are	O
shown	O
diagrammatically	O
in	O
(	O
cid:12	O
)	O
gure	O
21.4	O
using	O
the	O
line	O
thickness	O
to	O
encode	O
the	O
value	O
of	O
the	O
likelihood	O
.	O
sub-	O
hypotheses	O
with	O
likelihood	O
smaller	O
than	O
e	O
(	O
cid:0	O
)	O
8	O
times	O
the	O
maximum	B
likelihood	I
have	O
been	O
deleted	O
.	O
using	O
a	O
(	O
cid:12	O
)	O
ner	O
grid	O
,	O
we	O
can	O
represent	O
the	O
same	O
information	B
by	O
plotting	O
the	O
likelihood	B
as	O
a	O
surface	O
plot	O
or	O
contour	O
plot	O
as	O
a	O
function	B
of	O
(	O
cid:22	O
)	O
and	O
(	O
cid:27	O
)	O
(	O
(	O
cid:12	O
)	O
gure	O
21.5	O
)	O
.	O
a	O
(	O
cid:12	O
)	O
ve-parameter	O
mixture	O
model	O
eyeballing	O
the	O
data	O
(	O
(	O
cid:12	O
)	O
gure	O
21.3	O
)	O
,	O
you	O
might	O
agree	O
that	O
it	O
seems	O
more	O
plau-	O
sible	O
that	O
they	O
come	O
not	O
from	O
a	O
single	O
gaussian	O
but	O
from	O
a	O
mixture	O
of	O
two	O
gaussians	O
,	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
two	O
means	O
,	O
two	O
standard	O
deviations	O
,	O
and	O
two	O
mixing	O
-0.5	O
0	O
0.5	O
1	O
1.5	O
2	O
2.5	O
n=1	O
.	O
the	O
horizontal	O
figure	O
21.3.	O
five	O
datapoints	O
fxng5	O
coordinate	O
is	O
the	O
value	O
of	O
the	O
datum	O
,	O
xn	O
;	O
the	O
vertical	O
coordinate	O
has	O
no	O
meaning	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
21.2	O
:	O
exact	O
inference	O
for	O
continuous	O
hypothesis	O
spaces	O
297	O
figure	O
21.4.	O
likelihood	B
function	O
,	O
given	O
the	O
data	O
of	O
(	O
cid:12	O
)	O
gure	O
21.3	O
,	O
represented	O
by	O
line	O
thickness	O
.	O
subhypotheses	O
having	O
likelihood	B
smaller	O
than	O
e	O
(	O
cid:0	O
)	O
8	O
times	O
the	O
maximum	B
likelihood	I
are	O
not	O
shown	O
.	O
0.06	O
0.05	O
0.04	O
0.03	O
0.02	O
0.01	O
1	O
0	O
0.8	O
0.6	O
sigma	O
0.4	O
0.2	O
0.5	O
1	O
mean	B
0	O
1.5	O
2	O
0	O
0.5	O
1	O
1.5	O
2	O
mean	B
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
sigma	O
figure	O
21.5.	O
the	O
likelihood	B
function	O
for	O
the	O
parameters	B
of	O
a	O
gaussian	O
distribution	B
.	O
surface	O
plot	O
and	O
contour	O
plot	O
of	O
the	O
log	O
likelihood	B
as	O
a	O
function	B
of	O
(	O
cid:22	O
)	O
and	O
(	O
cid:27	O
)	O
.	O
the	O
data	B
set	I
of	O
n	O
=	O
5	O
points	O
had	O
mean	B
(	O
cid:22	O
)	O
x	O
=	O
1:0	O
and	O
s	O
=p	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
=	O
1:0.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
298	O
21	O
|	O
exact	O
inference	O
by	O
complete	O
enumeration	O
figure	O
21.6.	O
enumeration	O
of	O
the	O
entire	O
(	O
discretized	O
)	O
hypothesis	O
space	O
for	O
a	O
mixture	O
of	O
two	O
gaussians	O
.	O
weight	B
of	O
the	O
mixture	O
components	O
is	O
(	O
cid:25	O
)	O
1	O
;	O
(	O
cid:25	O
)	O
2	O
=	O
0:6	O
;	O
0:4	O
in	O
the	O
top	O
half	O
and	O
0:8	O
;	O
0:2	O
in	O
the	O
bottom	O
half	O
.	O
means	O
(	O
cid:22	O
)	O
1	O
and	O
(	O
cid:22	O
)	O
2	O
vary	O
horizontally	O
,	O
and	O
standard	O
deviations	O
(	O
cid:27	O
)	O
1	O
and	O
(	O
cid:27	O
)	O
2	O
vary	O
vertically	O
.	O
coe	O
(	O
cid:14	O
)	O
cients	O
(	O
cid:25	O
)	O
1	O
and	O
(	O
cid:25	O
)	O
2	O
,	O
satisfying	O
(	O
cid:25	O
)	O
1	O
+	O
(	O
cid:25	O
)	O
2	O
=	O
1	O
,	O
(	O
cid:25	O
)	O
i	O
(	O
cid:21	O
)	O
0	O
.	O
1	O
(	O
cid:17	O
)	O
+	O
exp	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
1	O
)	O
2	O
p	O
(	O
xj	O
(	O
cid:22	O
)	O
1	O
;	O
(	O
cid:27	O
)	O
1	O
;	O
(	O
cid:25	O
)	O
1	O
;	O
(	O
cid:22	O
)	O
2	O
;	O
(	O
cid:27	O
)	O
2	O
;	O
(	O
cid:25	O
)	O
2	O
)	O
=	O
(	O
cid:25	O
)	O
1p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
1	O
2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:25	O
)	O
2p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
2	O
(	O
cid:17	O
)	O
exp	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
2	O
)	O
2	O
2	O
(	O
cid:27	O
)	O
2	O
let	O
’	O
s	O
enumerate	O
the	O
subhypotheses	O
for	O
this	O
alternative	O
model	B
.	O
the	O
parameter	O
space	O
is	O
(	O
cid:12	O
)	O
ve-dimensional	O
,	O
so	O
it	O
becomes	O
challenging	O
to	O
represent	O
it	O
on	O
a	O
single	O
page	O
.	O
figure	O
21.6	O
enumerates	O
800	O
subhypotheses	O
with	O
di	O
(	O
cid:11	O
)	O
erent	O
values	O
of	O
the	O
(	O
cid:12	O
)	O
ve	O
parameters	B
(	O
cid:22	O
)	O
1	O
;	O
(	O
cid:22	O
)	O
2	O
;	O
(	O
cid:27	O
)	O
1	O
;	O
(	O
cid:27	O
)	O
2	O
;	O
(	O
cid:25	O
)	O
1.	O
the	O
means	O
are	O
varied	O
between	O
(	O
cid:12	O
)	O
ve	O
values	O
each	O
in	O
the	O
horizontal	O
directions	O
.	O
the	O
standard	O
deviations	O
take	O
on	O
four	O
values	O
each	O
vertically	O
.	O
and	O
(	O
cid:25	O
)	O
1	O
takes	O
on	O
two	O
values	O
vertically	O
.	O
we	O
can	O
represent	O
the	O
inference	B
about	O
these	O
(	O
cid:12	O
)	O
ve	O
parameters	B
in	O
the	O
light	O
of	O
the	O
(	O
cid:12	O
)	O
ve	O
datapoints	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
21.7.	O
if	O
we	O
wish	O
to	O
compare	O
the	O
one-gaussian	O
model	B
with	O
the	O
mixture-of-two	O
model	B
,	O
we	O
can	O
(	O
cid:12	O
)	O
nd	O
the	O
models	O
’	O
posterior	O
probabilities	O
by	O
evaluating	O
the	O
marginal	B
likelihood	I
or	O
evidence	B
for	O
each	O
model	B
h	O
,	O
p	O
(	O
fxgjh	O
)	O
.	O
the	O
evidence	B
is	O
given	O
by	O
integrating	O
over	O
the	O
parameters	B
,	O
(	O
cid:18	O
)	O
;	O
the	O
integration	O
can	O
be	O
imple-	O
mented	O
numerically	O
by	O
summing	O
over	O
the	O
alternative	O
enumerated	O
values	O
of	O
(	O
cid:18	O
)	O
,	O
p	O
(	O
fxgjh	O
)	O
=x	O
p	O
(	O
(	O
cid:18	O
)	O
)	O
p	O
(	O
fxgj	O
(	O
cid:18	O
)	O
;	O
h	O
)	O
;	O
(	O
21.9	O
)	O
where	O
p	O
(	O
(	O
cid:18	O
)	O
)	O
is	O
the	O
prior	B
distribution	O
over	O
the	O
grid	O
of	O
parameter	O
values	O
,	O
which	O
i	O
take	O
to	O
be	O
uniform	O
.	O
for	O
the	O
mixture	O
of	O
two	O
gaussians	O
this	O
integral	B
is	O
a	O
(	O
cid:12	O
)	O
ve-dimensional	O
integral	B
;	O
if	O
it	O
is	O
to	O
be	O
performed	O
at	O
all	O
accurately	O
,	O
the	O
grid	O
of	O
points	O
will	O
need	O
to	O
be	O
much	O
(	O
cid:12	O
)	O
ner	O
than	O
the	O
grids	O
shown	O
in	O
the	O
(	O
cid:12	O
)	O
gures	O
.	O
if	O
the	O
uncertainty	O
about	O
each	O
of	O
k	O
parameters	B
has	O
been	O
reduced	O
by	O
,	O
say	O
,	O
a	O
factor	O
of	O
ten	O
by	O
observing	O
the	O
data	O
,	O
then	O
brute-force	O
integration	O
requires	O
a	O
grid	O
of	O
at	O
least	O
10k	O
points	O
.	O
this	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
21.2	O
:	O
exact	O
inference	O
for	O
continuous	O
hypothesis	O
spaces	O
299	O
figure	O
21.7.	O
inferring	O
a	O
mixture	O
of	O
two	O
gaussians	O
.	O
likelihood	B
function	O
,	O
given	O
the	O
data	O
of	O
(	O
cid:12	O
)	O
gure	O
21.3	O
,	O
represented	O
by	O
line	O
thickness	O
.	O
the	O
hypothesis	O
space	O
is	O
identical	O
to	O
that	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
21.6.	O
subhypotheses	O
having	O
likelihood	B
smaller	O
than	O
e	O
(	O
cid:0	O
)	O
8	O
times	O
the	O
maximum	B
likelihood	I
are	O
not	O
shown	O
,	O
hence	O
the	O
blank	O
regions	O
,	O
which	O
correspond	O
to	O
hypotheses	O
that	O
the	O
data	O
have	O
ruled	O
out	O
.	O
-0.5	O
0	O
0.5	O
1	O
1.5	O
2	O
2.5	O
exponential	B
growth	O
of	O
computation	O
with	O
model	O
size	O
is	O
the	O
reason	O
why	O
complete	O
enumeration	O
is	O
rarely	O
a	O
feasible	O
computational	O
strategy	O
.	O
exercise	O
21.3	O
.	O
[	O
1	O
]	O
imagine	O
(	O
cid:12	O
)	O
tting	O
a	O
mixture	O
of	O
ten	O
gaussians	O
to	O
data	O
in	O
a	O
twenty-dimensional	O
space	O
.	O
estimate	O
the	O
computational	O
cost	O
of	O
imple-	O
menting	O
inferences	O
for	O
this	O
model	B
by	O
enumeration	O
of	O
a	O
grid	O
of	O
parameter	O
values	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
22	O
maximum	B
likelihood	I
and	O
clustering	B
rather	O
than	O
enumerate	O
all	O
hypotheses	O
{	O
which	O
may	O
be	O
exponential	B
in	O
number	O
{	O
we	O
can	O
save	O
a	O
lot	O
of	O
time	O
by	O
homing	O
in	O
on	O
one	O
good	B
hypothesis	O
that	O
(	O
cid:12	O
)	O
ts	O
the	O
data	O
well	O
.	O
this	O
is	O
the	O
philosophy	B
behind	O
the	O
maximum	B
likelihood	I
method	O
,	O
which	O
identi	O
(	O
cid:12	O
)	O
es	O
the	O
setting	O
of	O
the	O
parameter	O
vector	O
(	O
cid:18	O
)	O
that	O
maximizes	O
the	O
likelihood	B
,	O
p	O
(	O
data	O
j	O
(	O
cid:18	O
)	O
;	O
h	O
)	O
.	O
for	O
some	O
models	O
the	O
maximum	B
likelihood	I
parameters	O
can	O
be	O
identi	O
(	O
cid:12	O
)	O
ed	O
instantly	O
from	O
the	O
data	O
;	O
for	O
more	O
complex	B
models	O
,	O
(	O
cid:12	O
)	O
nding	O
the	O
maximum	O
like-	O
lihood	O
parameters	B
may	O
require	O
an	O
iterative	O
algorithm	O
.	O
for	O
any	O
model	B
,	O
it	O
is	O
usually	O
easiest	O
to	O
work	O
with	O
the	O
logarithm	O
of	O
the	O
likelihood	O
rather	O
than	O
the	O
likelihood	B
,	O
since	O
likelihoods	O
,	O
being	O
products	O
of	O
the	O
probabilities	O
of	O
many	O
data	O
points	O
,	O
tend	O
to	O
be	O
very	O
small	O
.	O
likelihoods	O
multiply	O
;	O
log	O
likelihoods	O
add	O
.	O
22.1	O
maximum	B
likelihood	I
for	O
one	O
gaussian	O
we	O
return	O
to	O
the	O
gaussian	O
for	O
our	O
(	O
cid:12	O
)	O
rst	O
examples	O
.	O
assume	O
we	O
have	O
data	O
fxngn	O
n=1	O
.	O
the	O
log	O
likelihood	B
is	O
:	O
ln	O
p	O
(	O
fxngn	O
n=1	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
n	O
ln	O
(	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
)	O
(	O
cid:0	O
)	O
xn	O
(	O
xn	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
)	O
2=	O
(	O
2	O
(	O
cid:27	O
)	O
2	O
)	O
:	O
(	O
22.1	O
)	O
the	O
likelihood	B
can	O
be	O
expressed	O
in	O
terms	O
of	O
two	O
functions	B
of	O
the	O
data	O
,	O
the	O
sample	B
mean	O
and	O
the	O
sum	O
of	O
square	B
deviations	O
n	O
(	O
cid:22	O
)	O
x	O
(	O
cid:17	O
)	O
xn=n	O
;	O
xn=1	O
s	O
(	O
cid:17	O
)	O
xn	O
(	O
xn	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
:	O
(	O
22.2	O
)	O
(	O
22.3	O
)	O
(	O
22.4	O
)	O
ln	O
p	O
(	O
fxngn	O
n=1	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
n	O
ln	O
(	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
)	O
(	O
cid:0	O
)	O
[	O
n	O
(	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
+	O
s	O
]	O
=	O
(	O
2	O
(	O
cid:27	O
)	O
2	O
)	O
:	O
because	O
the	O
likelihood	B
depends	O
on	O
the	O
data	O
only	O
through	O
(	O
cid:22	O
)	O
x	O
and	O
s	O
,	O
these	O
two	O
quantities	O
are	O
known	O
as	O
su	O
(	O
cid:14	O
)	O
cient	O
statistics	O
.	O
example	O
22.1.	O
di	O
(	O
cid:11	O
)	O
erentiate	O
the	O
log	O
likelihood	B
with	O
respect	O
to	O
(	O
cid:22	O
)	O
and	O
show	O
that	O
,	O
if	O
the	O
standard	B
deviation	I
is	O
known	O
to	O
be	O
(	O
cid:27	O
)	O
,	O
the	O
maximum	B
likelihood	I
mean	O
(	O
cid:22	O
)	O
of	O
a	O
gaussian	O
is	O
equal	O
to	O
the	O
sample	B
mean	O
(	O
cid:22	O
)	O
x	O
,	O
for	O
any	O
value	O
of	O
(	O
cid:27	O
)	O
.	O
solution	O
.	O
@	O
@	O
(	O
cid:22	O
)	O
ln	O
p	O
=	O
(	O
cid:0	O
)	O
n	O
(	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
(	O
cid:27	O
)	O
2	O
=	O
0	O
when	O
(	O
cid:22	O
)	O
=	O
(	O
cid:22	O
)	O
x	O
.	O
300	O
(	O
22.5	O
)	O
2	O
(	O
22.6	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
22.1	O
:	O
maximum	B
likelihood	I
for	O
one	O
gaussian	O
301	O
figure	O
22.1.	O
the	O
likelihood	B
function	O
for	O
the	O
parameters	B
of	O
a	O
gaussian	O
distribution	B
.	O
(	O
a1	O
,	O
a2	O
)	O
surface	O
plot	O
and	O
contour	O
plot	O
of	O
the	O
log	O
likelihood	B
as	O
a	O
function	B
of	O
(	O
cid:22	O
)	O
and	O
(	O
cid:27	O
)	O
.	O
the	O
data	B
set	I
of	O
n	O
=	O
5	O
points	O
had	O
mean	B
(	O
cid:22	O
)	O
x	O
=	O
1:0	O
and	O
s	O
=p	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
=	O
1:0	O
.	O
(	O
b	O
)	O
the	O
posterior	B
probability	I
of	O
(	O
cid:22	O
)	O
for	O
various	O
values	O
of	O
(	O
cid:27	O
)	O
.	O
(	O
c	O
)	O
the	O
posterior	B
probability	I
of	O
(	O
cid:27	O
)	O
for	O
various	O
(	O
cid:12	O
)	O
xed	O
values	O
of	O
(	O
cid:22	O
)	O
(	O
shown	O
as	O
a	O
density	B
over	O
ln	O
(	O
cid:27	O
)	O
)	O
.	O
0.06	O
0.05	O
0.04	O
0.03	O
0.02	O
0.01	O
1	O
0	O
0.8	O
0.6	O
sigma	O
(	O
a1	O
)	O
0.4	O
0.2	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
sigma	O
0.5	O
1	O
mean	B
0	O
1.5	O
2	O
0	O
(	O
a2	O
)	O
0.5	O
1	O
1.5	O
2	O
mean	B
4.5	O
4	O
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
0	O
r	O
o	O
i	O
r	O
e	O
t	O
s	O
o	O
p	O
(	O
b	O
)	O
mu=1	O
mu=1.25	O
mu=1.5	O
sigma=0.2	O
sigma=0.4	O
sigma=0.6	O
0.09	O
0.08	O
0.07	O
0.06	O
0.05	O
0.04	O
0.03	O
0.02	O
0.01	O
(	O
c	O
)	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
mean	B
1.2	O
1.4	O
1.6	O
1.8	O
2	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
1.2	O
1.4	O
1.6	O
1.8	O
2	O
if	O
we	O
taylor-expand	O
the	O
log	O
likelihood	B
about	O
the	O
maximum	O
,	O
we	O
can	O
de-	O
(	O
cid:12	O
)	O
ne	O
approximate	O
error	B
bars	I
on	O
the	O
maximum	B
likelihood	I
parameter	O
:	O
we	O
use	O
a	O
quadratic	O
approximation	B
to	O
estimate	O
how	O
far	O
from	O
the	O
maximum-likelihood	O
parameter	O
setting	O
we	O
can	O
go	O
before	O
the	O
likelihood	B
falls	O
by	O
some	O
standard	O
fac-	O
tor	O
,	O
for	O
example	O
e1=2	O
,	O
or	O
e4=2	O
.	O
in	O
the	O
special	O
case	O
of	O
a	O
likelihood	B
that	O
is	O
a	O
gaussian	O
function	B
of	O
the	O
parameters	B
,	O
the	O
quadratic	O
approximation	B
is	O
exact	O
.	O
example	O
22.2.	O
find	O
the	O
second	O
derivative	O
of	O
the	O
log	O
likelihood	B
with	O
respect	O
to	O
(	O
cid:22	O
)	O
,	O
and	O
(	O
cid:12	O
)	O
nd	O
the	O
error	B
bars	I
on	O
(	O
cid:22	O
)	O
,	O
given	O
the	O
data	O
and	O
(	O
cid:27	O
)	O
.	O
solution	O
.	O
@	O
2	O
@	O
(	O
cid:22	O
)	O
2	O
ln	O
p	O
=	O
(	O
cid:0	O
)	O
n	O
(	O
cid:27	O
)	O
2	O
:	O
2	O
(	O
22.7	O
)	O
comparing	O
this	O
curvature	O
with	O
the	O
curvature	O
of	O
the	O
log	O
of	O
a	O
gaussian	O
distri-	O
bution	O
over	O
(	O
cid:22	O
)	O
of	O
standard	O
deviation	O
(	O
cid:27	O
)	O
(	O
cid:22	O
)	O
,	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
2=	O
(	O
2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:22	O
)	O
,	O
we	O
can	O
deduce	O
that	O
the	O
error	B
bars	I
on	O
(	O
cid:22	O
)	O
(	O
derived	O
from	O
the	O
likelihood	B
function	O
)	O
are	O
(	O
cid:22	O
)	O
)	O
)	O
,	O
which	O
is	O
(	O
cid:0	O
)	O
1=	O
(	O
cid:27	O
)	O
2	O
(	O
cid:27	O
)	O
(	O
cid:22	O
)	O
=	O
(	O
cid:27	O
)	O
pn	O
:	O
(	O
22.8	O
)	O
the	O
error	B
bars	I
have	O
this	O
property	O
:	O
at	O
the	O
two	O
points	O
(	O
cid:22	O
)	O
=	O
(	O
cid:22	O
)	O
x	O
(	O
cid:6	O
)	O
(	O
cid:27	O
)	O
(	O
cid:22	O
)	O
,	O
the	O
likelihood	B
is	O
smaller	O
than	O
its	O
maximum	O
value	O
by	O
a	O
factor	O
of	O
e1=2	O
.	O
example	O
22.3.	O
find	O
the	O
maximum	B
likelihood	I
standard	O
deviation	O
(	O
cid:27	O
)	O
of	O
a	O
gaus-	O
sian	O
,	O
whose	O
mean	B
is	O
known	O
to	O
be	O
(	O
cid:22	O
)	O
,	O
in	O
the	O
light	O
of	O
data	O
fxngn	O
n=1	O
.	O
find	O
the	O
second	O
derivative	O
of	O
the	O
log	O
likelihood	B
with	O
respect	O
to	O
ln	O
(	O
cid:27	O
)	O
,	O
and	O
error	O
bars	O
on	O
ln	O
(	O
cid:27	O
)	O
.	O
solution	O
.	O
the	O
likelihood	B
’	O
s	O
dependence	O
on	O
(	O
cid:27	O
)	O
is	O
ln	O
p	O
(	O
fxngn	O
n=1	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
n	O
ln	O
(	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
)	O
(	O
cid:0	O
)	O
stot	O
(	O
2	O
(	O
cid:27	O
)	O
2	O
)	O
;	O
(	O
22.9	O
)	O
where	O
stot	O
=	O
pn	O
(	O
xn	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
)	O
2.	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
maximum	O
of	O
the	O
likelihood	B
,	O
we	O
can	O
di	O
(	O
cid:11	O
)	O
erentiate	O
with	O
respect	O
to	O
ln	O
(	O
cid:27	O
)	O
.	O
[	O
it	O
’	O
s	O
often	O
most	O
hygienic	O
to	O
di	O
(	O
cid:11	O
)	O
erentiate	O
with	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
302	O
22	O
|	O
maximum	B
likelihood	I
and	O
clustering	B
respect	O
to	O
ln	O
u	O
rather	O
than	O
u	O
,	O
when	O
u	O
is	O
a	O
scale	O
variable	O
;	O
we	O
use	O
dun=d	O
(	O
ln	O
u	O
)	O
=	O
nun	O
.	O
]	O
@	O
ln	O
p	O
(	O
fxngn	O
@	O
ln	O
(	O
cid:27	O
)	O
n=1	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
n	O
+	O
stot	O
(	O
cid:27	O
)	O
2	O
this	O
derivative	O
is	O
zero	O
when	O
(	O
cid:27	O
)	O
2	O
=	O
stot	O
n	O
;	O
(	O
22.10	O
)	O
(	O
22.11	O
)	O
i.e.	O
,	O
the	O
second	O
derivative	O
is	O
(	O
cid:27	O
)	O
=spn	O
n=1	O
(	O
xn	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
)	O
2	O
n	O
:	O
(	O
22.12	O
)	O
@	O
2	O
ln	O
p	O
(	O
fxngn	O
@	O
(	O
ln	O
(	O
cid:27	O
)	O
)	O
2	O
n=1	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
2	O
stot	O
(	O
cid:27	O
)	O
2	O
;	O
(	O
22.13	O
)	O
and	O
at	O
the	O
maximum-likelihood	O
value	O
of	O
(	O
cid:27	O
)	O
2	O
,	O
this	O
equals	O
(	O
cid:0	O
)	O
2n	O
.	O
so	O
error	B
bars	I
on	O
ln	O
(	O
cid:27	O
)	O
are	O
(	O
cid:27	O
)	O
ln	O
(	O
cid:27	O
)	O
=	O
:	O
2	O
(	O
22.14	O
)	O
1	O
p2n	O
.	O
exercise	O
22.4	O
.	O
[	O
1	O
]	O
show	O
that	O
the	O
values	O
of	O
(	O
cid:22	O
)	O
and	O
ln	O
(	O
cid:27	O
)	O
that	O
jointly	O
maximize	O
the	O
likelihood	B
are	O
:	O
f	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
gml	O
=n	O
(	O
cid:22	O
)	O
x	O
;	O
(	O
cid:27	O
)	O
n	O
=ps=no	O
;	O
where	O
(	O
cid:27	O
)	O
n	O
(	O
cid:17	O
)	O
spn	O
n=1	O
(	O
xn	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
n	O
:	O
(	O
22.15	O
)	O
22.2	O
maximum	B
likelihood	I
for	O
a	O
mixture	O
of	O
gaussians	O
we	O
now	O
derive	O
an	O
algorithm	B
for	O
(	O
cid:12	O
)	O
tting	O
a	O
mixture	O
of	O
gaussians	O
to	O
one-	O
dimensional	O
data	O
.	O
in	O
fact	O
,	O
this	O
algorithm	B
is	O
so	O
important	O
to	O
understand	O
that	O
,	O
you	O
,	O
gentle	O
reader	O
,	O
get	O
to	O
derive	O
the	O
algorithm	B
.	O
please	O
work	O
through	O
the	O
fol-	O
lowing	O
exercise	O
.	O
exercise	O
22.5	O
.	O
[	O
2	O
,	O
p.310	O
]	O
a	O
random	B
variable	I
x	O
is	O
assumed	O
to	O
have	O
a	O
probability	B
distribution	O
that	O
is	O
a	O
mixture	O
of	O
two	O
gaussians	O
,	O
p	O
(	O
xj	O
(	O
cid:22	O
)	O
1	O
;	O
(	O
cid:22	O
)	O
2	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
''	O
2	O
xk=1	O
pk	O
1	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:19	O
)	O
#	O
;	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
k	O
)	O
2	O
(	O
22.16	O
)	O
where	O
the	O
two	O
gaussians	O
are	O
given	O
the	O
labels	O
k	O
=	O
1	O
and	O
k	O
=	O
2	O
;	O
the	O
prior	B
probability	O
of	O
the	O
class	O
label	O
k	O
is	O
fp1	O
=	O
1=2	O
;	O
p2	O
=	O
1=2g	O
;	O
f	O
(	O
cid:22	O
)	O
kg	O
are	O
the	O
means	O
of	O
the	O
two	O
gaussians	O
;	O
and	O
both	O
have	O
standard	B
deviation	I
(	O
cid:27	O
)	O
.	O
for	O
brevity	O
,	O
we	O
denote	O
these	O
parameters	B
by	O
(	O
cid:18	O
)	O
(	O
cid:17	O
)	O
ff	O
(	O
cid:22	O
)	O
kg	O
;	O
(	O
cid:27	O
)	O
g.	O
a	O
data	B
set	I
consists	O
of	O
n	O
points	O
fxngn	O
n=1	O
which	O
are	O
assumed	O
to	O
be	O
indepen-	O
dent	O
samples	O
from	O
this	O
distribution	B
.	O
let	O
kn	O
denote	O
the	O
unknown	O
class	O
label	O
of	O
the	O
nth	O
point	O
.	O
assuming	O
that	O
f	O
(	O
cid:22	O
)	O
kg	O
and	O
(	O
cid:27	O
)	O
are	O
known	O
,	O
show	O
that	O
the	O
posterior	B
probability	I
of	O
the	O
class	O
label	O
kn	O
of	O
the	O
nth	O
point	O
can	O
be	O
written	O
as	O
p	O
(	O
kn	O
=	O
1j	O
xn	O
;	O
(	O
cid:18	O
)	O
)	O
=	O
p	O
(	O
kn	O
=	O
2j	O
xn	O
;	O
(	O
cid:18	O
)	O
)	O
=	O
1	O
1	O
+	O
exp	O
[	O
(	O
cid:0	O
)	O
(	O
w1xn	O
+	O
w0	O
)	O
]	O
1	O
1	O
+	O
exp	O
[	O
+	O
(	O
w1xn	O
+	O
w0	O
)	O
]	O
;	O
(	O
22.17	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
22.3	O
:	O
enhancements	O
to	O
soft	B
k-means	O
and	O
give	O
expressions	O
for	O
w1	O
and	O
w0	O
.	O
303	O
assume	O
now	O
that	O
the	O
means	O
f	O
(	O
cid:22	O
)	O
kg	O
are	O
not	O
known	O
,	O
and	O
that	O
we	O
wish	O
to	O
infer	O
them	O
from	O
the	O
data	O
fxngn	O
n=1	O
.	O
(	O
the	O
standard	B
deviation	I
(	O
cid:27	O
)	O
is	O
known	O
.	O
)	O
in	O
the	O
remainder	O
of	O
this	O
question	O
we	O
will	O
derive	O
an	O
iterative	O
algorithm	O
for	O
(	O
cid:12	O
)	O
nding	O
values	O
for	O
f	O
(	O
cid:22	O
)	O
kg	O
that	O
maximize	O
the	O
likelihood	B
,	O
n=1	O
jf	O
(	O
cid:22	O
)	O
kg	O
;	O
(	O
cid:27	O
)	O
)	O
=yn	O
p	O
(	O
xn	O
jf	O
(	O
cid:22	O
)	O
kg	O
;	O
(	O
cid:27	O
)	O
)	O
:	O
p	O
(	O
fxngn	O
(	O
22.18	O
)	O
let	O
l	O
denote	O
the	O
natural	B
log	O
of	O
the	O
likelihood	O
.	O
show	O
that	O
the	O
derivative	O
of	O
the	O
log	O
likelihood	B
with	O
respect	O
to	O
(	O
cid:22	O
)	O
k	O
is	O
given	O
by	O
@	O
@	O
(	O
cid:22	O
)	O
k	O
l	O
=xn	O
(	O
xn	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
k	O
)	O
(	O
cid:27	O
)	O
2	O
;	O
pkjn	O
(	O
22.19	O
)	O
where	O
pkjn	O
(	O
cid:17	O
)	O
p	O
(	O
kn	O
=	O
k	O
j	O
xn	O
;	O
(	O
cid:18	O
)	O
)	O
appeared	O
above	O
at	O
equation	O
(	O
22.17	O
)	O
.	O
show	O
,	O
neglecting	O
terms	O
in	O
@	O
@	O
(	O
cid:22	O
)	O
k	O
is	O
approximately	O
given	O
by	O
p	O
(	O
kn	O
=	O
k	O
j	O
xn	O
;	O
(	O
cid:18	O
)	O
)	O
,	O
that	O
the	O
second	O
derivative	O
@	O
2	O
@	O
(	O
cid:22	O
)	O
2	O
k	O
l	O
=	O
(	O
cid:0	O
)	O
xn	O
1	O
(	O
cid:27	O
)	O
2	O
:	O
pkjn	O
(	O
22.20	O
)	O
hence	O
show	O
that	O
from	O
an	O
initial	O
state	O
(	O
cid:22	O
)	O
1	O
;	O
(	O
cid:22	O
)	O
2	O
,	O
an	O
approximate	O
newton	O
{	O
raphson	O
step	O
updates	O
these	O
parameters	B
to	O
(	O
cid:22	O
)	O
01	O
;	O
(	O
cid:22	O
)	O
02	O
,	O
where	O
(	O
cid:22	O
)	O
0k	O
=	O
pn	O
pkjnxn	O
pn	O
pkjn	O
:	O
(	O
22.21	O
)	O
[	O
the	O
newton	O
{	O
raphson	O
method	B
for	O
maximizing	O
l	O
(	O
(	O
cid:22	O
)	O
)	O
updates	O
(	O
cid:22	O
)	O
to	O
(	O
cid:22	O
)	O
0	O
=	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
h	O
@	O
l	O
@	O
(	O
cid:22	O
)	O
.	O
@	O
2l	O
@	O
(	O
cid:22	O
)	O
2i	O
.	O
]	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
assuming	O
that	O
(	O
cid:27	O
)	O
=	O
1	O
,	O
sketch	O
a	O
contour	O
plot	O
of	O
the	O
likelihood	O
function	B
as	O
a	O
function	B
of	O
(	O
cid:22	O
)	O
1	O
and	O
(	O
cid:22	O
)	O
2	O
for	O
the	O
data	B
set	I
shown	O
above	O
.	O
the	O
data	B
set	I
consists	O
of	O
32	O
points	O
.	O
describe	O
the	O
peaks	O
in	O
your	O
sketch	O
and	O
indicate	O
their	O
widths	O
.	O
notice	O
that	O
the	O
algorithm	B
you	O
have	O
derived	O
for	O
maximizing	O
the	O
likelihood	B
is	O
identical	O
to	O
the	O
soft	B
k-means	O
algorithm	B
of	O
section	B
20.4.	O
now	O
that	O
it	O
is	O
clear	O
that	O
clustering	B
can	O
be	O
viewed	O
as	O
mixture-density-modelling	O
,	O
we	O
are	O
able	O
to	O
derive	O
enhancements	O
to	O
the	O
k-means	O
algorithm	B
,	O
which	O
rectify	O
the	O
problems	O
we	O
noted	O
earlier	O
.	O
22.3	O
enhancements	O
to	O
soft	B
k-means	O
algorithm	B
22.2	O
shows	O
a	O
version	O
of	O
the	O
soft-k-means	O
algorithm	B
corresponding	O
to	O
a	O
modelling	B
assumption	O
that	O
each	O
cluster	O
is	O
a	O
spherical	O
gaussian	O
having	O
its	O
own	O
width	O
(	O
each	O
cluster	O
has	O
its	O
own	O
(	O
cid:12	O
)	O
(	O
k	O
)	O
=	O
1/	O
(	O
cid:27	O
)	O
2	O
k	O
)	O
.	O
the	O
algorithm	B
updates	O
the	O
lengthscales	O
(	O
cid:27	O
)	O
k	O
for	O
itself	O
.	O
the	O
algorithm	B
also	O
includes	O
cluster	O
weight	B
parame-	O
ters	O
(	O
cid:25	O
)	O
1	O
;	O
(	O
cid:25	O
)	O
2	O
;	O
:	O
:	O
:	O
;	O
(	O
cid:25	O
)	O
k	O
which	O
also	O
update	O
themselves	O
,	O
allowing	O
accurate	O
modelling	B
of	O
data	O
from	O
clusters	O
of	O
unequal	O
weights	O
.	O
this	O
algorithm	B
is	O
demonstrated	O
in	O
(	O
cid:12	O
)	O
gure	O
22.3	O
for	O
two	O
data	O
sets	O
that	O
we	O
’	O
ve	O
seen	O
before	O
.	O
the	O
second	O
example	O
shows	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
304	O
22	O
|	O
maximum	B
likelihood	I
and	O
clustering	B
assignment	O
step	O
.	O
the	O
responsibilities	O
are	O
algorithm	B
22.2.	O
the	O
soft	B
k-means	O
algorithm	B
,	O
version	O
2.	O
r	O
(	O
n	O
)	O
k	O
=	O
(	O
cid:25	O
)	O
k	O
1	O
1	O
(	O
cid:27	O
)	O
2	O
k	O
(	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
k	O
)	O
i	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
(	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
k0	O
)	O
i	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
d	O
(	O
m	O
(	O
k	O
)	O
;	O
x	O
(	O
n	O
)	O
)	O
(	O
cid:19	O
)	O
d	O
(	O
m	O
(	O
k0	O
)	O
;	O
x	O
(	O
n	O
)	O
)	O
(	O
cid:19	O
)	O
1	O
(	O
cid:27	O
)	O
2	O
k0	O
1	O
pk0	O
(	O
cid:25	O
)	O
k	O
where	O
i	O
is	O
the	O
dimensionality	O
of	O
x	O
.	O
(	O
22.22	O
)	O
update	O
step	O
.	O
each	O
cluster	O
’	O
s	O
parameters	B
,	O
m	O
(	O
k	O
)	O
,	O
(	O
cid:25	O
)	O
k	O
,	O
and	O
(	O
cid:27	O
)	O
2	O
k	O
,	O
are	O
adjusted	O
to	O
match	O
the	O
data	O
points	O
that	O
it	O
is	O
responsible	O
for	O
.	O
r	O
(	O
n	O
)	O
k	O
x	O
(	O
n	O
)	O
r	O
(	O
k	O
)	O
m	O
(	O
k	O
)	O
=	O
xn	O
k	O
=	O
xn	O
(	O
cid:27	O
)	O
2	O
r	O
(	O
n	O
)	O
k	O
(	O
x	O
(	O
n	O
)	O
(	O
cid:0	O
)	O
m	O
(	O
k	O
)	O
)	O
2	O
where	O
r	O
(	O
k	O
)	O
is	O
the	O
total	O
responsibility	B
of	O
mean	B
k	O
,	O
ir	O
(	O
k	O
)	O
r	O
(	O
k	O
)	O
(	O
cid:25	O
)	O
k	O
=	O
pk	O
r	O
(	O
k	O
)	O
r	O
(	O
k	O
)	O
=xn	O
r	O
(	O
n	O
)	O
k	O
:	O
(	O
22.23	O
)	O
(	O
22.24	O
)	O
(	O
22.25	O
)	O
(	O
22.26	O
)	O
t	O
=	O
0	O
t	O
=	O
1	O
t	O
=	O
2	O
t	O
=	O
3	O
t	O
=	O
9	O
figure	O
22.3.	O
soft	B
k-means	O
algorithm	B
,	O
with	O
k	O
=	O
2	O
,	O
applied	O
(	O
a	O
)	O
to	O
the	O
40-point	O
data	B
set	I
of	O
(	O
cid:12	O
)	O
gure	O
20.3	O
;	O
(	O
b	O
)	O
to	O
the	O
little	O
’	O
n	O
’	O
large	O
data	B
set	I
of	O
(	O
cid:12	O
)	O
gure	O
20.5.	O
t	O
=	O
0	O
t	O
=	O
1	O
t	O
=	O
10	O
t	O
=	O
20	O
t	O
=	O
30	O
t	O
=	O
35	O
(	O
cid:25	O
)	O
k	O
r	O
(	O
n	O
)	O
k	O
=	O
i	O
exp	O
(	O
cid:0	O
)	O
1	O
(	O
m	O
(	O
k	O
)	O
i	O
(	O
cid:0	O
)	O
x	O
(	O
n	O
)	O
i=1	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
(	O
k	O
)	O
qi	O
pk0	O
(	O
numerator	O
,	O
with	O
k0	O
in	O
place	O
of	O
k	O
)	O
xi=1	O
i	O
i	O
)	O
2	O
.	O
2	O
(	O
(	O
cid:27	O
)	O
(	O
k	O
)	O
i	O
=	O
xn	O
r	O
(	O
n	O
)	O
k	O
(	O
x	O
(	O
n	O
)	O
i	O
i	O
(	O
cid:0	O
)	O
m	O
(	O
k	O
)	O
r	O
(	O
k	O
)	O
)	O
2	O
(	O
k	O
)	O
(	O
cid:27	O
)	O
2	O
i	O
algorithm	B
22.4.	O
the	O
soft	B
k-means	O
algorithm	B
,	O
version	O
3	O
,	O
which	O
corresponds	O
to	O
a	O
model	B
of	O
axis-aligned	O
gaussians	O
.	O
)	O
2	O
!	O
(	O
22.27	O
)	O
(	O
22.28	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
22.4	O
:	O
a	O
fatal	O
(	O
cid:13	O
)	O
aw	O
of	O
maximum	O
likelihood	B
305	O
t	O
=	O
0	O
t	O
=	O
10	O
t	O
=	O
20	O
t	O
=	O
30	O
figure	O
22.5.	O
soft	B
k-means	O
algorithm	B
,	O
version	O
3	O
,	O
applied	O
to	O
the	O
data	O
consisting	O
of	O
two	O
cigar-shaped	O
clusters	O
.	O
k	O
=	O
2	O
(	O
cf	O
.	O
(	O
cid:12	O
)	O
gure	O
20.6	O
)	O
.	O
t	O
=	O
0	O
t	O
=	O
10	O
t	O
=	O
20	O
t	O
=	O
26	O
t	O
=	O
32	O
figure	O
22.6.	O
soft	B
k-means	O
algorithm	B
,	O
version	O
3	O
,	O
applied	O
to	O
the	O
little	O
’	O
n	O
’	O
large	O
data	B
set	I
.	O
k	O
=	O
2.	O
that	O
convergence	O
can	O
take	O
a	O
long	O
time	O
,	O
but	O
eventually	O
the	O
algorithm	B
identi	O
(	O
cid:12	O
)	O
es	O
the	O
small	O
cluster	O
and	O
the	O
large	O
cluster	O
.	O
soft	B
k-means	O
,	O
version	O
2	O
,	O
is	O
a	O
maximum-likelihood	O
algorithm	B
for	O
(	O
cid:12	O
)	O
tting	O
a	O
mixture	O
of	O
spherical	O
gaussians	O
to	O
data	O
{	O
‘	O
spherical	O
’	O
meaning	O
that	O
the	O
variance	B
of	O
the	O
gaussian	O
is	O
the	O
same	O
in	O
all	O
directions	O
.	O
this	O
algorithm	B
is	O
still	O
no	O
good	B
at	O
modelling	B
the	O
cigar-shaped	O
clusters	O
of	O
(	O
cid:12	O
)	O
gure	O
20.6.	O
if	O
we	O
wish	O
to	O
model	B
the	O
clusters	O
by	O
axis-aligned	O
gaussians	O
with	O
possibly-unequal	O
variances	O
,	O
we	O
replace	O
the	O
assignment	O
rule	O
(	O
22.22	O
)	O
and	O
the	O
variance	B
update	O
rule	O
(	O
22.24	O
)	O
by	O
the	O
rules	B
(	O
22.27	O
)	O
and	O
(	O
22.28	O
)	O
displayed	O
in	O
algorithm	O
22.4.	O
this	O
third	O
version	O
of	O
soft	O
k-means	O
is	O
demonstrated	O
in	O
(	O
cid:12	O
)	O
gure	O
22.5	O
on	O
the	O
‘	O
two	O
cigars	O
’	O
data	B
set	I
of	O
(	O
cid:12	O
)	O
gure	O
20.6.	O
after	O
30	O
iterations	O
,	O
the	O
algorithm	B
correctly	O
locates	O
the	O
two	O
clusters	O
.	O
figure	O
22.6	O
shows	O
the	O
same	O
algorithm	B
applied	O
to	O
the	O
little	O
’	O
n	O
’	O
large	O
data	B
set	I
;	O
again	O
,	O
the	O
correct	O
cluster	O
locations	O
are	O
found	O
.	O
22.4	O
a	O
fatal	O
(	O
cid:13	O
)	O
aw	O
of	O
maximum	O
likelihood	B
finally	O
,	O
(	O
cid:12	O
)	O
gure	O
22.7	O
sounds	O
a	O
cautionary	O
note	O
:	O
when	O
we	O
(	O
cid:12	O
)	O
t	O
k	O
=	O
4	O
means	O
to	O
our	O
(	O
cid:12	O
)	O
rst	O
toy	O
data	B
set	I
,	O
we	O
sometimes	O
(	O
cid:12	O
)	O
nd	O
that	O
very	O
small	O
clusters	O
form	O
,	O
covering	O
just	O
one	O
or	O
two	O
data	O
points	O
.	O
this	O
is	O
a	O
pathological	O
property	O
of	O
soft	O
k-means	O
clustering	B
,	O
versions	O
2	O
and	O
3.	O
.	O
exercise	O
22.6	O
.	O
[	O
2	O
]	O
investigate	O
what	O
happens	O
if	O
one	O
mean	B
m	O
(	O
k	O
)	O
sits	O
exactly	O
on	O
k	O
is	O
su	O
(	O
cid:14	O
)	O
ciently	O
small	O
,	O
top	O
of	O
one	O
data	O
point	O
;	O
show	O
that	O
if	O
the	O
variance	B
(	O
cid:27	O
)	O
2	O
then	O
no	O
return	O
is	O
possible	O
:	O
(	O
cid:27	O
)	O
2	O
k	O
becomes	O
ever	O
smaller	O
.	O
t	O
=	O
0	O
t	O
=	O
5	O
t	O
=	O
10	O
t	O
=	O
20	O
a	O
proof	O
that	O
the	O
algorithm	B
does	O
indeed	O
maximize	O
the	O
likelihood	B
is	O
deferred	O
to	O
section	B
33.7.	O
figure	O
22.7.	O
soft	B
k-means	O
algorithm	B
applied	O
to	O
a	O
data	B
set	I
of	O
40	O
points	O
.	O
k	O
=	O
4.	O
notice	O
that	O
at	O
convergence	O
,	O
one	O
very	O
small	O
cluster	O
has	O
formed	O
between	O
two	O
data	O
points	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
306	O
kaboom	B
!	O
22	O
|	O
maximum	B
likelihood	I
and	O
clustering	B
soft	O
k-means	O
can	O
blow	B
up	I
.	O
put	O
one	O
cluster	O
exactly	O
on	O
one	O
data	O
point	O
and	O
let	O
its	O
variance	B
go	O
to	O
zero	O
{	O
you	O
can	O
obtain	O
an	O
arbitrarily	O
large	O
likelihood	B
!	O
maximum	B
likelihood	I
methods	O
can	O
break	O
down	O
by	O
(	O
cid:12	O
)	O
nding	O
highly	O
tuned	O
models	O
that	O
(	O
cid:12	O
)	O
t	O
part	O
of	O
the	O
data	O
perfectly	O
.	O
this	O
phenomenon	O
is	O
known	O
as	O
over	O
(	O
cid:12	O
)	O
tting	O
.	O
the	O
reason	O
we	O
are	O
not	O
interested	O
in	O
these	O
solutions	O
with	O
enormous	O
likelihood	B
is	O
this	O
:	O
sure	O
,	O
these	O
parameter-settings	O
may	O
have	O
enormous	O
posterior	B
probability	I
density	O
,	O
but	O
the	O
density	B
is	O
large	O
over	O
only	O
a	O
very	O
small	O
volume	B
of	O
parameter	O
space	O
.	O
so	O
the	O
probability	B
mass	O
associated	O
with	O
these	O
likelihood	B
spikes	O
is	O
usually	O
tiny	O
.	O
we	O
conclude	O
that	O
maximum	B
likelihood	I
methods	O
are	O
not	O
a	O
satisfactory	O
gen-	O
eral	O
solution	O
to	O
data-modelling	O
problems	O
:	O
the	O
likelihood	B
may	O
be	O
in	O
(	O
cid:12	O
)	O
nitely	O
large	O
at	O
certain	O
parameter	O
settings	O
.	O
even	O
if	O
the	O
likelihood	B
does	O
not	O
have	O
in	O
(	O
cid:12	O
)	O
nitely-	O
large	O
spikes	O
,	O
the	O
maximum	O
of	O
the	O
likelihood	B
is	O
often	O
unrepresentative	O
,	O
in	O
high-	O
dimensional	O
problems	O
.	O
even	O
in	O
low-dimensional	O
problems	O
,	O
maximum	B
likelihood	I
solutions	O
can	O
be	O
unrepresentative	O
.	O
as	O
you	O
may	O
know	O
from	O
basic	O
statistics	O
,	O
the	O
maximum	O
like-	O
lihood	O
estimator	B
(	O
22.15	O
)	O
for	O
a	O
gaussian	O
’	O
s	O
standard	B
deviation	I
,	O
(	O
cid:27	O
)	O
n	O
,	O
is	O
a	O
biased	O
estimator	B
,	O
a	O
topic	O
that	O
we	O
’	O
ll	O
take	O
up	O
in	O
chapter	O
24.	O
the	O
maximum	B
a	I
posteriori	I
(	O
map	O
)	O
method	B
a	O
popular	O
replacement	O
for	O
maximizing	O
the	O
likelihood	B
is	O
maximizing	O
the	O
bayesian	O
posterior	B
probability	I
density	O
of	O
the	O
parameters	O
instead	O
.	O
however	O
,	O
multiplying	O
the	O
likelihood	B
by	O
a	O
prior	B
and	O
maximizing	O
the	O
posterior	O
does	O
not	O
make	O
the	O
above	O
problems	O
go	O
away	O
;	O
the	O
posterior	O
density	O
often	O
also	O
has	O
in	O
(	O
cid:12	O
)	O
nitely-large	O
spikes	O
,	O
and	O
the	O
maximum	O
of	O
the	O
posterior	B
probability	I
density	O
is	O
often	O
unrepresentative	O
of	O
the	O
whole	O
posterior	O
distribution	O
.	O
think	O
back	O
to	O
the	O
concept	O
of	O
typicality	O
,	O
which	O
we	O
encountered	O
in	O
chapter	O
4	O
:	O
in	O
high	O
dimen-	O
sions	O
,	O
most	O
of	O
the	O
probability	B
mass	O
is	O
in	O
a	O
typical	B
set	I
whose	O
properties	O
are	O
quite	O
di	O
(	O
cid:11	O
)	O
erent	O
from	O
the	O
points	O
that	O
have	O
the	O
maximum	O
probability	O
density	B
.	O
maxima	O
are	O
atypical	O
.	O
a	O
further	O
reason	O
for	O
disliking	O
the	O
maximum	B
a	I
posteriori	I
is	O
that	O
it	O
is	O
basis-	O
dependent	O
.	O
if	O
we	O
make	O
a	O
nonlinear	B
change	O
of	O
basis	O
from	O
the	O
parameter	O
(	O
cid:18	O
)	O
to	O
the	O
parameter	O
u	O
=	O
f	O
(	O
(	O
cid:18	O
)	O
)	O
then	O
the	O
probability	B
density	O
of	O
(	O
cid:18	O
)	O
is	O
transformed	O
to	O
p	O
(	O
u	O
)	O
=	O
p	O
(	O
(	O
cid:18	O
)	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
@	O
(	O
cid:18	O
)	O
@	O
u	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
:	O
(	O
22.29	O
)	O
the	O
maximum	O
of	O
the	O
density	B
p	O
(	O
u	O
)	O
will	O
usually	O
not	O
coincide	O
with	O
the	O
maximum	O
of	O
the	O
density	B
p	O
(	O
(	O
cid:18	O
)	O
)	O
.	O
(	O
for	O
(	O
cid:12	O
)	O
gures	O
illustrating	O
such	O
nonlinear	B
changes	O
of	O
basis	O
,	O
see	O
the	O
next	O
chapter	O
.	O
)	O
it	O
seems	O
undesirable	O
to	O
use	O
a	O
method	B
whose	O
answers	O
change	O
when	O
we	O
change	O
representation	O
.	O
further	O
reading	O
the	O
soft	B
k-means	O
algorithm	B
is	O
at	O
the	O
heart	O
of	O
the	O
automatic	O
classi	O
(	O
cid:12	O
)	O
cation	O
package	O
,	O
autoclass	O
(	O
hanson	O
et	O
al.	O
,	O
1991b	O
;	O
hanson	O
et	O
al.	O
,	O
1991a	O
)	O
.	O
22.5	O
further	O
exercises	O
exercises	O
where	O
maximum	B
likelihood	I
may	O
be	O
useful	O
exercise	O
22.7	O
.	O
[	O
3	O
]	O
make	O
a	O
version	O
of	O
the	O
k-means	O
algorithm	B
that	O
models	O
the	O
data	O
as	O
a	O
mixture	O
of	O
k	O
arbitrary	O
gaussians	O
,	O
i.e.	O
,	O
gaussians	O
that	O
are	O
not	O
constrained	B
to	O
be	O
axis-aligned	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
22.5	O
:	O
further	O
exercises	O
307	O
.	O
exercise	O
22.8	O
.	O
[	O
2	O
]	O
(	O
a	O
)	O
a	O
photon	B
counter	I
is	O
pointed	O
at	O
a	O
remote	O
star	O
for	O
one	O
minute	O
,	O
in	O
order	O
to	O
infer	O
the	O
brightness	O
,	O
i.e.	O
,	O
the	O
rate	B
of	O
photons	O
arriving	O
at	O
the	O
counter	O
per	O
minute	O
,	O
(	O
cid:21	O
)	O
.	O
assuming	O
the	O
number	O
of	O
photons	O
collected	O
r	O
has	O
a	O
poisson	O
distribution	B
with	O
mean	B
(	O
cid:21	O
)	O
,	O
p	O
(	O
r	O
j	O
(	O
cid:21	O
)	O
)	O
=	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
)	O
(	O
cid:21	O
)	O
r	O
r	O
!	O
;	O
(	O
22.30	O
)	O
what	O
is	O
the	O
maximum	B
likelihood	I
estimate	O
for	O
(	O
cid:21	O
)	O
,	O
given	O
r	O
=	O
9	O
?	O
find	O
error	B
bars	I
on	O
ln	O
(	O
cid:21	O
)	O
.	O
(	O
b	O
)	O
same	O
situation	O
,	O
but	O
now	O
we	O
assume	O
that	O
the	O
counter	O
detects	O
not	O
only	O
photons	O
from	O
the	O
star	O
but	O
also	O
‘	O
background	O
’	O
photons	O
.	O
the	O
background	B
rate	I
of	O
photons	O
is	O
known	O
to	O
be	O
b	O
=	O
13	O
photons	O
per	O
minute	O
.	O
we	O
assume	O
the	O
number	O
of	O
photons	O
collected	O
,	O
r	O
,	O
has	O
a	O
pois-	O
son	O
distribution	B
with	O
mean	B
(	O
cid:21	O
)	O
+b	O
.	O
now	O
,	O
given	O
r	O
=	O
9	O
detected	O
photons	O
,	O
what	O
is	O
the	O
maximum	B
likelihood	I
estimate	O
for	O
(	O
cid:21	O
)	O
?	O
comment	O
on	O
this	O
answer	O
,	O
discussing	O
also	O
the	O
bayesian	O
posterior	O
distribution	O
,	O
and	O
the	O
‘	O
unbiased	B
estimator	I
’	O
of	O
sampling	O
theory	B
,	O
^	O
(	O
cid:21	O
)	O
(	O
cid:17	O
)	O
r	O
(	O
cid:0	O
)	O
b.	O
exercise	O
22.9	O
.	O
[	O
2	O
]	O
a	O
bent	B
coin	I
is	O
tossed	O
n	O
times	O
,	O
giving	O
na	O
heads	O
and	O
nb	O
tails	O
.	O
assume	O
a	O
beta	B
distribution	I
prior	O
for	O
the	O
probability	O
of	O
heads	O
,	O
p	O
,	O
for	O
example	O
the	O
uniform	O
distribution	B
.	O
find	O
the	O
maximum	B
likelihood	I
and	O
maximum	B
a	I
posteriori	I
values	O
of	O
p	O
,	O
then	O
(	O
cid:12	O
)	O
nd	O
the	O
maximum	B
likelihood	I
and	O
maximum	B
a	I
posteriori	I
values	O
of	O
the	O
logit	O
a	O
(	O
cid:17	O
)	O
ln	O
[	O
p=	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
]	O
.	O
compare	O
with	O
the	O
predictive	B
distribution	I
,	O
i.e.	O
,	O
the	O
probability	B
that	O
the	O
next	O
toss	O
will	O
come	O
up	O
heads	O
.	O
.	O
exercise	O
22.10	O
.	O
[	O
2	O
]	O
two	O
men	O
looked	O
through	O
prison	O
bars	O
;	O
one	O
saw	O
stars	B
,	O
the	O
other	O
tried	O
to	O
infer	O
where	O
the	O
window	B
frame	O
was	O
.	O
from	O
the	O
other	O
side	O
of	O
a	O
room	O
,	O
you	O
look	O
through	O
a	O
window	B
and	O
see	O
stars	O
at	O
locations	O
f	O
(	O
xn	O
;	O
yn	O
)	O
g.	O
you	O
can	O
’	O
t	O
see	O
the	O
window	B
edges	O
because	O
it	O
is	O
to-	O
tally	O
dark	O
apart	O
from	O
the	O
stars	B
.	O
assuming	O
the	O
window	B
is	O
rectangular	B
and	O
that	O
the	O
visible	O
stars	B
’	O
locations	O
are	O
independently	O
randomly	O
distributed	O
,	O
what	O
are	O
the	O
inferred	O
values	O
of	O
(	O
xmin	O
;	O
ymin	O
,	O
xmax	O
,	O
ymax	O
)	O
,	O
according	O
to	O
maximum	B
likelihood	I
?	O
sketch	O
the	O
likelihood	B
as	O
a	O
function	B
of	O
xmax	O
,	O
for	O
(	O
cid:12	O
)	O
xed	O
xmin	O
,	O
ymin	O
,	O
and	O
ymax	O
.	O
.	O
exercise	O
22.11	O
.	O
[	O
3	O
]	O
a	O
sailor	B
infers	O
his	O
location	O
(	O
x	O
;	O
y	O
)	O
by	O
measuring	O
the	O
bearings	O
of	O
three	O
buoys	O
whose	O
locations	O
(	O
xn	O
;	O
yn	O
)	O
are	O
given	O
on	O
his	O
chart	O
.	O
let	O
the	O
true	O
bearings	O
of	O
the	O
buoys	O
be	O
(	O
cid:18	O
)	O
n.	O
assuming	O
that	O
his	O
measurement	O
~	O
(	O
cid:18	O
)	O
n	O
of	O
each	O
bearing	B
is	O
subject	O
to	O
gaussian	O
noise	B
of	O
small	O
standard	B
deviation	I
(	O
cid:27	O
)	O
,	O
what	O
is	O
his	O
inferred	O
location	O
,	O
by	O
maximum	O
likelihood	B
?	O
the	O
sailor	B
’	O
s	O
rule	B
of	I
thumb	I
says	O
that	O
the	O
boat	O
’	O
s	O
position	O
can	O
be	O
taken	O
to	O
be	O
the	O
centre	O
of	O
the	O
cocked	B
hat	I
,	O
the	O
triangle	B
produced	O
by	O
the	O
intersection	B
of	O
the	O
three	O
measured	O
bearings	O
(	O
(	O
cid:12	O
)	O
gure	O
22.8	O
)	O
.	O
can	O
you	O
persuade	O
him	O
that	O
the	O
maximum	B
likelihood	I
answer	O
is	O
better	O
?	O
.	O
exercise	O
22.12	O
.	O
[	O
3	O
,	O
p.310	O
]	O
maximum	B
likelihood	I
(	O
cid:12	O
)	O
tting	O
of	O
an	O
exponential-family	B
model	O
.	O
assume	O
that	O
a	O
variable	O
x	O
comes	O
from	O
a	O
probability	B
distribution	O
of	O
the	O
form	O
p	O
(	O
xj	O
w	O
)	O
=	O
1	O
z	O
(	O
w	O
)	O
exp	O
xk	O
wkfk	O
(	O
x	O
)	O
!	O
;	O
(	O
22.31	O
)	O
(	O
xmax	O
;	O
ymax	O
)	O
?	O
?	O
?	O
?	O
?	O
?	O
(	O
xmin	O
;	O
ymin	O
)	O
(	O
x3	O
;	O
y3	O
)	O
b	O
q	O
q	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
b	O
(	O
x1	O
;	O
y1	O
)	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
a	O
q	O
a	O
(	O
cid:0	O
)	O
q	O
a	O
qq	O
(	O
cid:0	O
)	O
a	O
a	O
(	O
cid:0	O
)	O
a	O
a	O
a	O
a	O
(	O
x2	O
;	O
y2	O
)	O
b	O
figure	O
22.8.	O
the	O
standard	O
way	O
of	O
drawing	O
three	O
slightly	O
inconsistent	O
bearings	O
on	O
a	O
chart	O
produces	O
a	O
triangle	B
called	O
a	O
cocked	B
hat	I
.	O
where	O
is	O
the	O
sailor	B
?	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
308	O
22	O
|	O
maximum	B
likelihood	I
and	O
clustering	B
where	O
the	O
functions	B
fk	O
(	O
x	O
)	O
are	O
given	O
,	O
and	O
the	O
parameters	B
w	O
=	O
fwkg	O
are	O
not	O
known	O
.	O
a	O
data	B
set	I
fx	O
(	O
n	O
)	O
g	O
of	O
n	O
points	O
is	O
supplied	O
.	O
show	O
by	O
di	O
(	O
cid:11	O
)	O
erentiating	O
the	O
log	O
likelihood	B
that	O
the	O
maximum-likelihood	O
parameters	B
wml	O
satisfy	O
p	O
(	O
xj	O
wml	O
)	O
fk	O
(	O
x	O
)	O
=	O
xx	O
1	O
n	O
xn	O
fk	O
(	O
x	O
(	O
n	O
)	O
)	O
;	O
(	O
22.32	O
)	O
where	O
the	O
left-hand	O
sum	O
is	O
over	O
all	O
x	O
,	O
and	O
the	O
right-hand	O
sum	O
is	O
over	O
the	O
data	O
points	O
.	O
a	O
shorthand	O
for	O
this	O
result	O
is	O
that	O
each	O
function-average	O
under	O
the	O
(	O
cid:12	O
)	O
tted	O
model	B
must	O
equal	O
the	O
function-average	O
found	O
in	O
the	O
data	O
:	O
hfkip	O
(	O
x	O
j	O
wml	O
)	O
=	O
hfkidata	O
:	O
(	O
22.33	O
)	O
.	O
exercise	O
22.13	O
.	O
[	O
3	O
]	O
‘	O
maximum	B
entropy	I
’	O
(	O
cid:12	O
)	O
tting	O
of	O
models	O
to	O
constraints	O
.	O
when	O
confronted	O
by	O
a	O
probability	B
distribution	O
p	O
(	O
x	O
)	O
about	O
which	O
only	O
a	O
few	O
facts	O
are	O
known	O
,	O
the	O
maximum	B
entropy	I
principle	O
(	O
maxent	B
)	O
o	O
(	O
cid:11	O
)	O
ers	O
a	O
rule	O
for	O
choosing	O
a	O
distribution	B
that	O
satis	O
(	O
cid:12	O
)	O
es	O
those	O
constraints	O
.	O
accord-	O
ing	O
to	O
maxent	B
,	O
you	O
should	O
select	O
the	O
p	O
(	O
x	O
)	O
that	O
maximizes	O
the	O
entropy	B
h	O
=xx	O
p	O
(	O
x	O
)	O
log	O
1=p	O
(	O
x	O
)	O
;	O
(	O
22.34	O
)	O
subject	O
to	O
the	O
constraints	O
.	O
assuming	O
the	O
constraints	O
assert	O
that	O
the	O
averages	O
of	O
certain	O
functions	B
fk	O
(	O
x	O
)	O
are	O
known	O
,	O
i.e.	O
,	O
hfkip	O
(	O
x	O
)	O
=	O
fk	O
;	O
(	O
22.35	O
)	O
show	O
,	O
by	O
introducing	O
lagrange	O
multipliers	O
(	O
one	O
for	O
each	O
constraint	O
,	O
in-	O
cluding	O
normalization	O
)	O
,	O
that	O
the	O
maximum-entropy	O
distribution	B
has	O
the	O
form	O
exp	O
xk	O
wkfk	O
(	O
x	O
)	O
!	O
;	O
p	O
(	O
x	O
)	O
maxent	B
=	O
1	O
z	O
(	O
22.36	O
)	O
where	O
the	O
parameters	B
z	O
and	O
fwkg	O
are	O
set	B
such	O
that	O
the	O
constraints	O
(	O
22.35	O
)	O
are	O
satis	O
(	O
cid:12	O
)	O
ed	O
.	O
and	O
hence	O
the	O
maximum	B
entropy	I
method	O
gives	O
identical	O
results	O
to	O
max-	O
imum	O
likelihood	B
(	O
cid:12	O
)	O
tting	O
of	O
an	O
exponential-family	B
model	O
(	O
previous	O
exer-	O
cise	O
)	O
.	O
the	O
maximum	B
entropy	I
method	O
has	O
sometimes	O
been	O
recommended	O
as	O
a	O
method	B
for	O
assigning	B
prior	O
distributions	O
in	O
bayesian	O
modelling	B
.	O
while	O
the	O
outcomes	O
of	O
the	O
maximum	O
entropy	B
method	O
are	O
sometimes	O
interesting	O
and	O
thought-provoking	O
,	O
i	O
do	O
not	O
advocate	O
maxent	B
as	O
the	O
approach	O
to	O
assigning	B
priors	O
.	O
maximum	B
entropy	I
is	O
also	O
sometimes	O
proposed	O
as	O
a	O
method	B
for	O
solv-	O
ing	O
inference	B
problems	O
{	O
for	O
example	O
,	O
‘	O
given	O
that	O
the	O
mean	B
score	O
of	O
this	O
unfair	O
six-sided	O
die	B
is	O
2.5	O
,	O
what	O
is	O
its	O
probability	B
distribution	O
(	O
p1	O
;	O
p2	O
;	O
p3	O
;	O
p4	O
;	O
p5	O
;	O
p6	O
)	O
?	O
’	O
i	O
think	O
it	O
is	O
a	O
bad	B
idea	O
to	O
use	O
maximum	B
entropy	I
in	O
this	O
way	O
;	O
it	O
can	O
give	O
silly	O
answers	O
.	O
the	O
correct	O
way	O
to	O
solve	O
inference	B
problems	O
is	O
to	O
use	O
bayes	O
’	O
theorem	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
22.5	O
:	O
further	O
exercises	O
309	O
exercises	O
where	O
maximum	B
likelihood	I
and	O
map	O
have	O
di	O
(	O
cid:14	O
)	O
culties	O
a	O
b	O
c	O
d-g	O
-30	O
-20	O
-10	O
0	O
10	O
20	O
scientist	O
xn	O
a	O
b	O
c	O
d	O
e	O
f	O
g	O
(	O
cid:0	O
)	O
27.020	O
3.570	O
8.191	O
9.898	O
9.603	O
9.945	O
10.056	O
figure	O
22.9.	O
seven	O
measurements	O
fxng	O
of	O
a	O
parameter	O
(	O
cid:22	O
)	O
by	O
seven	O
scientists	B
each	O
having	O
his	O
own	O
noise-level	O
(	O
cid:27	O
)	O
n.	O
.	O
exercise	O
22.14	O
.	O
[	O
2	O
]	O
this	O
exercise	O
explores	O
the	O
idea	O
that	O
maximizing	O
a	O
proba-	O
bility	O
density	B
is	O
a	O
poor	O
way	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
point	O
that	O
is	O
representative	O
of	O
the	O
density	O
.	O
consider	O
a	O
gaussian	O
distribution	B
in	O
a	O
k-dimensional	O
space	O
,	O
p	O
(	O
w	O
)	O
=	O
(	O
1=p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
w	O
)	O
k	O
exp	O
(	O
(	O
cid:0	O
)	O
pk	O
w	O
)	O
.	O
show	O
that	O
nearly	O
all	O
of	O
the	O
probability	O
mass	O
of	O
a	O
gaussian	O
is	O
in	O
a	O
thin	B
shell	I
of	O
radius	O
r	O
=	O
pk	O
(	O
cid:27	O
)	O
w	O
and	O
of	O
thickness	O
proportional	O
to	O
r=pk	O
.	O
for	O
example	O
,	O
in	O
1000	O
dimen-	O
sions	O
,	O
90	O
%	O
of	O
the	O
mass	O
of	O
a	O
gaussian	O
with	O
(	O
cid:27	O
)	O
w	O
=	O
1	O
is	O
in	O
a	O
shell	O
of	O
radius	O
31.6	O
and	O
thickness	O
2.8.	O
however	O
,	O
the	O
probability	B
density	O
at	O
the	O
origin	O
is	O
ek=2	O
’	O
10217	O
times	O
bigger	O
than	O
the	O
density	B
at	O
this	O
shell	O
where	O
most	O
of	O
the	O
probability	B
mass	O
is	O
.	O
i	O
=2	O
(	O
cid:27	O
)	O
2	O
1	O
w2	O
now	O
consider	O
two	O
gaussian	O
densities	O
in	O
1000	O
dimensions	B
that	O
di	O
(	O
cid:11	O
)	O
er	O
in	O
radius	O
(	O
cid:27	O
)	O
w	O
by	O
just	O
1	O
%	O
,	O
and	O
that	O
contain	O
equal	O
total	O
probability	B
mass	O
.	O
show	O
that	O
the	O
maximum	O
probability	O
density	B
is	O
greater	O
at	O
the	O
centre	O
of	O
the	O
gaussian	O
with	O
smaller	O
(	O
cid:27	O
)	O
w	O
by	O
a	O
factor	O
of	O
(	O
cid:24	O
)	O
exp	O
(	O
0:01k	O
)	O
’	O
20	O
000.	O
in	O
ill-posed	O
problems	O
,	O
a	O
typical	B
posterior	O
distribution	B
is	O
often	O
a	O
weighted	O
superposition	O
of	O
gaussians	O
with	O
varying	O
means	O
and	O
standard	O
deviations	O
,	O
so	O
the	O
true	O
posterior	O
has	O
a	O
skew	O
peak	O
,	O
with	O
the	O
maximum	O
of	O
the	O
prob-	O
ability	O
density	B
located	O
near	O
the	O
mean	B
of	O
the	O
gaussian	O
distribution	B
that	O
has	O
the	O
smallest	O
standard	B
deviation	I
,	O
not	O
the	O
gaussian	O
with	O
the	O
greatest	O
weight	B
.	O
.	O
exercise	O
22.15	O
.	O
[	O
3	O
]	O
the	O
seven	O
scientists	B
.	O
n	O
datapoints	O
fxng	O
are	O
drawn	O
from	O
n	O
distributions	O
,	O
all	O
of	O
which	O
are	O
gaussian	O
with	O
a	O
common	O
mean	B
(	O
cid:22	O
)	O
but	O
with	O
di	O
(	O
cid:11	O
)	O
erent	O
unknown	O
standard	O
deviations	O
(	O
cid:27	O
)	O
n.	O
what	O
are	O
the	O
maximum	B
likelihood	I
parameters	O
(	O
cid:22	O
)	O
;	O
f	O
(	O
cid:27	O
)	O
ng	O
given	O
the	O
data	O
?	O
for	O
example	O
,	O
seven	O
scientists	B
(	O
a	O
,	O
b	O
,	O
c	O
,	O
d	O
,	O
e	O
,	O
f	O
,	O
g	O
)	O
with	O
wildly-di	O
(	O
cid:11	O
)	O
ering	O
experimental	O
skills	O
measure	O
(	O
cid:22	O
)	O
.	O
you	O
expect	O
some	O
of	O
them	O
to	O
do	O
accurate	O
work	O
(	O
i.e.	O
,	O
to	O
have	O
small	O
(	O
cid:27	O
)	O
n	O
)	O
,	O
and	O
some	O
of	O
them	O
to	O
turn	O
in	O
wildly	O
inaccurate	O
answers	O
(	O
i.e.	O
,	O
to	O
have	O
enormous	O
(	O
cid:27	O
)	O
n	O
)	O
.	O
figure	O
22.9	O
shows	O
their	O
seven	O
results	O
.	O
what	O
is	O
(	O
cid:22	O
)	O
,	O
and	O
how	O
reliable	O
is	O
each	O
scientist	O
?	O
i	O
hope	O
you	O
agree	O
that	O
,	O
intuitively	O
,	O
it	O
looks	O
pretty	O
certain	O
that	O
a	O
and	O
b	O
are	O
both	O
inept	O
measurers	O
,	O
that	O
d	O
{	O
g	O
are	O
better	O
,	O
and	O
that	O
the	O
true	O
value	O
of	O
(	O
cid:22	O
)	O
is	O
somewhere	O
close	O
to	O
10.	O
but	O
what	O
does	O
maximizing	O
the	O
likelihood	B
tell	O
you	O
?	O
exercise	O
22.16	O
.	O
[	O
3	O
]	O
problems	O
with	O
map	O
method	B
.	O
a	O
collection	O
of	O
widgets	O
i	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
k	O
have	O
a	O
property	O
called	O
‘	O
wodge	B
’	O
,	O
wi	O
,	O
which	O
we	O
measure	O
,	O
wid-	O
get	O
by	O
widget	O
,	O
in	O
noisy	O
experiments	O
with	O
a	O
known	O
noise	B
level	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
=	O
1:0.	O
our	O
model	B
for	O
these	O
quantities	O
is	O
that	O
they	O
come	O
from	O
a	O
gaussian	O
prior	B
p	O
(	O
wi	O
j	O
(	O
cid:11	O
)	O
)	O
=	O
normal	B
(	O
0	O
;	O
1/	O
(	O
cid:11	O
)	O
)	O
,	O
where	O
(	O
cid:11	O
)	O
=	O
1=	O
(	O
cid:27	O
)	O
2	O
w	O
is	O
not	O
known	O
.	O
our	O
prior	B
for	O
this	O
variance	B
is	O
(	O
cid:13	O
)	O
at	O
over	O
log	O
(	O
cid:27	O
)	O
w	O
from	O
(	O
cid:27	O
)	O
w	O
=	O
0:1	O
to	O
(	O
cid:27	O
)	O
w	O
=	O
10.	O
scenario	O
1.	O
suppose	O
four	O
widgets	O
have	O
been	O
measured	O
and	O
give	O
the	O
fol-	O
lowing	O
data	O
:	O
fd1	O
;	O
d2	O
;	O
d3	O
;	O
d4g	O
=	O
f2.2	O
,	O
(	O
cid:0	O
)	O
2:2	O
,	O
2.8	O
,	O
(	O
cid:0	O
)	O
2:8g	O
.	O
we	O
are	O
interested	O
in	O
inferring	O
the	O
wodges	O
of	O
these	O
four	O
widgets	O
.	O
(	O
a	O
)	O
find	O
the	O
values	O
of	O
w	O
and	O
(	O
cid:11	O
)	O
that	O
maximize	O
the	O
posterior	B
probability	I
p	O
(	O
w	O
;	O
log	O
(	O
cid:11	O
)	O
j	O
d	O
)	O
.	O
(	O
b	O
)	O
marginalize	O
over	O
(	O
cid:11	O
)	O
and	O
(	O
cid:12	O
)	O
nd	O
the	O
posterior	B
probability	I
density	O
of	O
w	O
given	O
the	O
data	O
.	O
[	O
integration	O
skills	O
required	O
.	O
see	O
mackay	O
(	O
1999a	O
)	O
for	O
solution	O
.	O
]	O
find	O
maxima	O
of	O
p	O
(	O
w	O
j	O
d	O
)	O
.	O
[	O
answer	O
:	O
two	O
maxima	O
{	O
one	O
at	O
wmp	O
=	O
f1:8	O
;	O
(	O
cid:0	O
)	O
1:8	O
;	O
2:2	O
;	O
(	O
cid:0	O
)	O
2:2g	O
;	O
with	O
error	O
bars	O
on	O
all	O
four	O
parameters	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
310	O
22	O
|	O
maximum	B
likelihood	I
and	O
clustering	B
(	O
obtained	O
from	O
gaussian	O
approximation	B
to	O
the	O
posterior	O
)	O
(	O
cid:6	O
)	O
0:9	O
;	O
and	O
one	O
at	O
w0mp	O
=	O
f0:03	O
;	O
(	O
cid:0	O
)	O
0:03	O
;	O
0:04	O
;	O
(	O
cid:0	O
)	O
0:04g	O
with	O
error	O
bars	O
(	O
cid:6	O
)	O
0:1	O
.	O
]	O
scenario	O
2.	O
suppose	O
in	O
addition	O
to	O
the	O
four	O
measurements	O
above	O
we	O
are	O
now	O
informed	O
that	O
there	O
are	O
four	O
more	O
widgets	O
that	O
have	O
been	O
measured	O
with	O
a	O
much	O
less	O
accurate	O
instrument	O
,	O
having	O
(	O
cid:27	O
)	O
0	O
(	O
cid:23	O
)	O
=	O
100:0.	O
thus	O
we	O
now	O
have	O
both	O
well-determined	O
and	O
ill-determined	O
parameters	B
,	O
as	O
in	O
a	O
typical	B
ill-posed	O
problem	O
.	O
the	O
data	O
from	O
these	O
measurements	O
were	O
a	O
string	O
of	O
uninformative	O
values	O
,	O
fd5	O
;	O
d6	O
;	O
d7	O
;	O
d8g	O
=	O
f100	O
,	O
(	O
cid:0	O
)	O
100	O
;	O
100	O
,	O
(	O
cid:0	O
)	O
100g	O
.	O
we	O
are	O
again	O
asked	O
to	O
infer	O
the	O
wodges	O
of	O
the	O
widgets	O
.	O
intuitively	O
,	O
our	O
inferences	O
about	O
the	O
well-measured	O
widgets	O
should	O
be	O
negligibly	O
a	O
(	O
cid:11	O
)	O
ected	O
by	O
this	O
vacuous	O
information	B
about	O
the	O
poorly-measured	O
widgets	O
.	O
but	O
what	O
happens	O
to	O
the	O
map	O
method	B
?	O
(	O
a	O
)	O
find	O
the	O
values	O
of	O
w	O
and	O
(	O
cid:11	O
)	O
that	O
maximize	O
the	O
posterior	B
probability	I
p	O
(	O
w	O
;	O
log	O
(	O
cid:11	O
)	O
j	O
d	O
)	O
.	O
(	O
b	O
)	O
find	O
maxima	O
of	O
p	O
(	O
w	O
j	O
d	O
)	O
.	O
[	O
answer	O
:	O
only	O
one	O
maximum	O
,	O
wmp	O
=	O
f0:03	O
,	O
(	O
cid:0	O
)	O
0:03	O
,	O
0:03	O
,	O
(	O
cid:0	O
)	O
0:03	O
,	O
0:0001	O
,	O
(	O
cid:0	O
)	O
0:0001	O
,	O
0:0001	O
,	O
(	O
cid:0	O
)	O
0:0001g	O
,	O
with	O
error	O
bars	O
on	O
all	O
eight	O
parameters	B
(	O
cid:6	O
)	O
0:11	O
.	O
]	O
22.6	O
solutions	O
solution	O
to	O
exercise	O
22.5	O
(	O
p.302	O
)	O
.	O
figure	O
22.10	O
shows	O
a	O
contour	O
plot	O
of	O
the	O
likelihood	O
function	B
for	O
the	O
32	O
data	O
points	O
.	O
the	O
peaks	O
are	O
pretty-near	O
centred	O
on	O
the	O
points	O
(	O
1	O
;	O
5	O
)	O
and	O
(	O
5	O
;	O
1	O
)	O
,	O
and	O
are	O
pretty-near	O
circular	O
in	O
their	O
contours	O
.	O
the	O
width	O
of	O
each	O
of	O
the	O
peaks	O
is	O
a	O
standard	B
deviation	I
of	O
(	O
cid:27	O
)	O
=p16	O
=	O
1/4	O
.	O
the	O
peaks	O
are	O
roughly	O
gaussian	O
in	O
shape	O
.	O
solution	O
to	O
exercise	O
22.12	O
(	O
p.307	O
)	O
.	O
the	O
log	O
likelihood	B
is	O
:	O
ln	O
p	O
(	O
fx	O
(	O
n	O
)	O
gj	O
w	O
)	O
=	O
(	O
cid:0	O
)	O
n	O
ln	O
z	O
(	O
w	O
)	O
+xn	O
xk	O
wkfk	O
(	O
x	O
(	O
n	O
)	O
)	O
:	O
(	O
22.37	O
)	O
5	O
4	O
3	O
2	O
1	O
0	O
0	O
1	O
2	O
3	O
4	O
5	O
figure	O
22.10.	O
the	O
likelihood	B
as	O
a	O
function	B
of	O
(	O
cid:22	O
)	O
1	O
and	O
(	O
cid:22	O
)	O
2	O
.	O
@	O
@	O
wk	O
ln	O
p	O
(	O
fx	O
(	O
n	O
)	O
gj	O
w	O
)	O
=	O
(	O
cid:0	O
)	O
n	O
@	O
@	O
wk	O
ln	O
z	O
(	O
w	O
)	O
+xn	O
fk	O
(	O
x	O
)	O
:	O
(	O
22.38	O
)	O
now	O
,	O
the	O
fun	O
part	O
is	O
what	O
happens	O
when	O
we	O
di	O
(	O
cid:11	O
)	O
erentiate	O
the	O
log	O
of	O
the	O
nor-	O
malizing	O
constant	O
:	O
wk0fk0	O
(	O
x	O
)	O
!	O
1	O
@	O
@	O
wk	O
exp	O
xk0	O
z	O
(	O
w	O
)	O
xx	O
wk0fk0	O
(	O
x	O
)	O
!	O
fk	O
(	O
x	O
)	O
=	O
xx	O
p	O
(	O
xj	O
w	O
)	O
fk	O
(	O
x	O
)	O
;	O
(	O
22.39	O
)	O
@	O
@	O
wk	O
ln	O
z	O
(	O
w	O
)	O
=	O
=	O
so	O
1	O
exp	O
xk0	O
z	O
(	O
w	O
)	O
xx	O
ln	O
p	O
(	O
fx	O
(	O
n	O
)	O
gj	O
w	O
)	O
=	O
(	O
cid:0	O
)	O
nxx	O
@	O
@	O
wk	O
p	O
(	O
xj	O
w	O
)	O
fk	O
(	O
x	O
)	O
+xn	O
fk	O
(	O
x	O
)	O
;	O
(	O
22.40	O
)	O
and	O
at	O
the	O
maximum	O
of	O
the	O
likelihood	B
,	O
xx	O
p	O
(	O
xj	O
wml	O
)	O
fk	O
(	O
x	O
)	O
=	O
1	O
n	O
xn	O
fk	O
(	O
x	O
(	O
n	O
)	O
)	O
:	O
(	O
22.41	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
23	O
useful	O
probability	B
distributions	I
in	O
bayesian	O
data	B
modelling	I
,	O
there	O
’	O
s	O
a	O
small	O
collection	O
of	O
probability	O
distribu-	O
tions	O
that	O
come	O
up	O
again	O
and	O
again	O
.	O
the	O
purpose	O
of	O
this	O
chapter	O
is	O
to	O
intro-	O
duce	O
these	O
distributions	O
so	O
that	O
they	O
won	O
’	O
t	O
be	O
intimidating	O
when	O
encountered	O
in	O
combat	O
situations	O
.	O
there	O
is	O
no	O
need	O
to	O
memorize	O
any	O
of	O
them	O
,	O
except	O
perhaps	O
the	O
gaussian	O
;	O
if	O
a	O
distribution	B
is	O
important	O
enough	O
,	O
it	O
will	O
memorize	O
itself	O
,	O
and	O
otherwise	O
,	O
it	O
can	O
easily	O
be	O
looked	O
up	O
.	O
23.1	O
distributions	O
over	O
integers	O
binomial	B
,	O
poisson	O
,	O
exponential	B
we	O
already	O
encountered	O
the	O
binomial	B
distribution	I
and	O
the	O
poisson	O
distribution	B
on	O
page	O
2.	O
the	O
binomial	B
distribution	I
for	O
an	O
integer	O
r	O
with	O
parameters	O
f	O
(	O
the	O
bias	B
,	O
f	O
2	O
[	O
0	O
;	O
1	O
]	O
)	O
and	O
n	O
(	O
the	O
number	O
of	O
trials	O
)	O
is	O
:	O
r	O
(	O
cid:19	O
)	O
f	O
r	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
n	O
(	O
cid:0	O
)	O
r	O
p	O
(	O
r	O
j	O
f	O
;	O
n	O
)	O
=	O
(	O
cid:18	O
)	O
n	O
r	O
2	O
f0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
ng	O
:	O
(	O
23.1	O
)	O
the	O
binomial	B
distribution	I
arises	O
,	O
for	O
example	O
,	O
when	O
we	O
(	O
cid:13	O
)	O
ip	O
a	O
bent	B
coin	I
,	O
with	O
bias	O
f	O
,	O
n	O
times	O
,	O
and	O
observe	O
the	O
number	O
of	O
heads	O
,	O
r.	O
the	O
poisson	O
distribution	B
with	O
parameter	O
(	O
cid:21	O
)	O
>	O
0	O
is	O
:	O
p	O
(	O
r	O
j	O
(	O
cid:21	O
)	O
)	O
=	O
e	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
r	O
r	O
!	O
r	O
2	O
f0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
g	O
:	O
(	O
23.2	O
)	O
the	O
poisson	O
distribution	B
arises	O
,	O
for	O
example	O
,	O
when	O
we	O
count	O
the	O
number	O
of	O
photons	O
r	O
that	O
arrive	O
in	O
a	O
pixel	O
during	O
a	O
(	O
cid:12	O
)	O
xed	O
interval	O
,	O
given	O
that	O
the	O
mean	B
intensity	O
on	O
the	O
pixel	O
corresponds	O
to	O
an	O
average	B
number	O
of	O
photons	O
(	O
cid:21	O
)	O
.	O
the	O
exponential	B
distribution	I
on	O
integers	O
,	O
,	O
p	O
(	O
r	O
j	O
f	O
)	O
=	O
f	O
r	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
r	O
2	O
(	O
0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
1	O
)	O
;	O
(	O
23.3	O
)	O
arises	O
in	O
waiting	O
problems	O
.	O
how	O
long	O
will	O
you	O
have	O
to	O
wait	O
until	O
a	O
six	B
is	O
rolled	O
,	O
if	O
a	O
fair	O
six-sided	O
dice	O
is	O
rolled	O
?	O
answer	O
:	O
the	O
probability	B
distribution	O
of	O
the	O
number	O
of	O
rolls	O
,	O
r	O
,	O
is	O
exponential	B
over	O
integers	O
with	O
parameter	O
f	O
=	O
5=6	O
.	O
the	O
distribution	B
may	O
also	O
be	O
written	O
p	O
(	O
r	O
j	O
f	O
)	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
e	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
r	O
r	O
2	O
(	O
0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
1	O
)	O
;	O
(	O
23.4	O
)	O
where	O
(	O
cid:21	O
)	O
=	O
ln	O
(	O
1=f	O
)	O
.	O
311	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
1	O
0.1	O
0.01	O
0.001	O
0.0001	O
1e-05	O
1e-06	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
r	O
figure	O
23.1.	O
the	O
binomial	B
distribution	I
p	O
(	O
r	O
j	O
f	O
=	O
0:3	O
;	O
n	O
=	O
10	O
)	O
,	O
on	O
a	O
linear	B
scale	O
(	O
top	O
)	O
and	O
a	O
logarithmic	O
scale	O
(	O
bottom	O
)	O
.	O
0	O
5	O
10	O
15	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
1	O
0.1	O
0.01	O
0.001	O
0.0001	O
1e-05	O
1e-06	O
1e-07	O
0	O
5	O
r	O
10	O
15	O
figure	O
23.2.	O
the	O
poisson	O
distribution	B
p	O
(	O
r	O
j	O
(	O
cid:21	O
)	O
=	O
2:7	O
)	O
,	O
on	O
a	O
linear	B
scale	O
(	O
top	O
)	O
and	O
a	O
logarithmic	O
scale	O
(	O
bottom	O
)	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
312	O
23	O
|	O
useful	O
probability	B
distributions	I
23.2	O
distributions	O
over	O
unbounded	O
real	O
numbers	O
gaussian	O
,	O
student	B
,	O
cauchy	O
,	O
biexponential	B
,	O
inverse-cosh	B
.	O
the	O
gaussian	O
distribution	B
or	O
normal	B
distribution	O
with	O
mean	O
(	O
cid:22	O
)	O
and	O
standard	O
deviation	O
(	O
cid:27	O
)	O
is	O
p	O
(	O
xj	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
1	O
z	O
where	O
2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:19	O
)	O
x	O
2	O
(	O
(	O
cid:0	O
)	O
1	O
;	O
1	O
)	O
;	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
)	O
2	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
z	O
=	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
:	O
(	O
23.6	O
)	O
it	O
is	O
sometimes	O
useful	O
to	O
work	O
with	O
the	O
quantity	O
(	O
cid:28	O
)	O
(	O
cid:17	O
)	O
1=	O
(	O
cid:27	O
)	O
2	O
,	O
which	O
is	O
called	O
the	O
precision	B
parameter	O
of	O
the	O
gaussian	O
.	O
a	O
sample	B
z	O
from	O
a	O
standard	O
univariate	O
gaussian	O
can	O
be	O
generated	O
by	O
computing	O
(	O
23.5	O
)	O
(	O
23.7	O
)	O
z	O
=	O
cos	O
(	O
2	O
(	O
cid:25	O
)	O
u1	O
)	O
p2	O
ln	O
(	O
1=u2	O
)	O
;	O
where	O
u1	O
and	O
u2	O
are	O
uniformly	O
distributed	O
in	O
(	O
0	O
;	O
1	O
)	O
.	O
a	O
second	O
sample	B
z2	O
=	O
sin	O
(	O
2	O
(	O
cid:25	O
)	O
u1	O
)	O
p2	O
ln	O
(	O
1=u2	O
)	O
,	O
independent	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
,	O
can	O
then	O
be	O
obtained	O
for	O
free	O
.	O
the	O
gaussian	O
distribution	B
is	O
widely	O
used	O
and	O
often	O
asserted	O
to	O
be	O
a	O
very	O
common	O
distribution	B
in	O
the	O
real	O
world	O
,	O
but	O
i	O
am	O
sceptical	O
about	O
this	O
asser-	O
tion	O
.	O
yes	O
,	O
unimodal	O
distributions	O
may	O
be	O
common	O
;	O
but	O
a	O
gaussian	O
is	O
a	O
spe-	O
cial	O
,	O
rather	O
extreme	O
,	O
unimodal	O
distribution	B
.	O
it	O
has	O
very	O
light	O
tails	O
:	O
the	O
log-	O
probability-density	O
decreases	O
quadratically	O
.	O
the	O
typical	B
deviation	O
of	O
x	O
from	O
(	O
cid:22	O
)	O
is	O
(	O
cid:27	O
)	O
,	O
but	O
the	O
respective	O
probabilities	O
that	O
x	O
deviates	O
from	O
(	O
cid:22	O
)	O
by	O
more	O
than	O
2	O
(	O
cid:27	O
)	O
,	O
3	O
(	O
cid:27	O
)	O
,	O
4	O
(	O
cid:27	O
)	O
,	O
and	O
5	O
(	O
cid:27	O
)	O
,	O
are	O
0:046	O
,	O
0.003	O
,	O
6	O
(	O
cid:2	O
)	O
10	O
(	O
cid:0	O
)	O
5	O
,	O
and	O
6	O
(	O
cid:2	O
)	O
10	O
(	O
cid:0	O
)	O
7.	O
in	O
my	O
experience	O
,	O
deviations	O
from	O
a	O
mean	B
four	O
or	O
(	O
cid:12	O
)	O
ve	O
times	O
greater	O
than	O
the	O
typical	B
deviation	O
may	O
be	O
rare	O
,	O
but	O
not	O
as	O
rare	O
as	O
6	O
(	O
cid:2	O
)	O
10	O
(	O
cid:0	O
)	O
5	O
!	O
i	O
therefore	O
urge	O
caution	B
in	O
the	O
use	O
of	O
gaussian	O
distributions	O
:	O
if	O
a	O
variable	O
that	O
is	O
modelled	O
with	O
a	O
gaussian	O
actually	O
has	O
a	O
heavier-tailed	O
distribution	B
,	O
the	O
rest	O
of	O
the	O
model	O
will	O
contort	O
itself	O
to	O
reduce	O
the	O
deviations	O
of	O
the	O
outliers	O
,	O
like	O
a	O
sheet	O
of	O
paper	O
being	O
crushed	O
by	O
a	O
rubber	O
band	O
.	O
.	O
exercise	O
23.1	O
.	O
[	O
1	O
]	O
pick	O
a	O
variable	O
that	O
is	O
supposedly	O
bell-shaped	O
in	O
probability	O
distribution	B
,	O
gather	O
data	O
,	O
and	O
make	O
a	O
plot	O
of	O
the	O
variable	O
’	O
s	O
empirical	O
distribution	B
.	O
show	O
the	O
distribution	B
as	O
a	O
histogram	O
on	O
a	O
log	O
scale	O
and	O
investigate	O
whether	O
the	O
tails	O
are	O
well-modelled	O
by	O
a	O
gaussian	O
distribu-	O
tion	O
.	O
[	O
one	O
example	O
of	O
a	O
variable	O
to	O
study	O
is	O
the	O
amplitude	O
of	O
an	O
audio	O
signal	O
.	O
]	O
one	O
distribution	B
with	O
heavier	O
tails	O
than	O
a	O
gaussian	O
is	O
a	O
mixture	O
of	O
gaus-	O
sians	O
.	O
a	O
mixture	O
of	O
two	O
gaussians	O
,	O
for	O
example	O
,	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
two	O
means	O
,	O
two	O
standard	O
deviations	O
,	O
and	O
two	O
mixing	O
coe	O
(	O
cid:14	O
)	O
cients	O
(	O
cid:25	O
)	O
1	O
and	O
(	O
cid:25	O
)	O
2	O
,	O
satisfying	O
(	O
cid:25	O
)	O
1	O
+	O
(	O
cid:25	O
)	O
2	O
=	O
1	O
,	O
(	O
cid:25	O
)	O
i	O
(	O
cid:21	O
)	O
0.	O
p	O
(	O
xj	O
(	O
cid:22	O
)	O
1	O
;	O
(	O
cid:27	O
)	O
1	O
;	O
(	O
cid:25	O
)	O
1	O
;	O
(	O
cid:22	O
)	O
2	O
;	O
(	O
cid:27	O
)	O
2	O
;	O
(	O
cid:25	O
)	O
2	O
)	O
=	O
(	O
cid:25	O
)	O
1p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
1	O
(	O
cid:25	O
)	O
2p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
2	O
(	O
cid:17	O
)	O
:	O
exp	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
2	O
)	O
2	O
2	O
(	O
cid:27	O
)	O
2	O
1	O
(	O
cid:17	O
)	O
+	O
exp	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
1	O
)	O
2	O
2	O
(	O
cid:27	O
)	O
2	O
if	O
we	O
take	O
an	O
appropriately	O
weighted	O
mixture	O
of	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
number	O
of	O
gaussians	O
,	O
all	O
having	O
mean	B
(	O
cid:22	O
)	O
,	O
we	O
obtain	O
a	O
student-t	O
distribution	B
,	O
where	O
p	O
(	O
xj	O
(	O
cid:22	O
)	O
;	O
s	O
;	O
n	O
)	O
=	O
1	O
z	O
1	O
(	O
1	O
+	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
)	O
2=	O
(	O
ns2	O
)	O
)	O
(	O
n+1	O
)	O
=2	O
;	O
(	O
23.8	O
)	O
z	O
=	O
p	O
(	O
cid:25	O
)	O
ns2	O
(	O
cid:0	O
)	O
(	O
n=2	O
)	O
(	O
cid:0	O
)	O
(	O
(	O
n	O
+	O
1	O
)	O
=2	O
)	O
(	O
23.9	O
)	O
-2	O
0	O
2	O
4	O
6	O
8	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0.1	O
0.01	O
0.001	O
0.0001	O
-2	O
0	O
2	O
4	O
6	O
8	O
figure	O
23.3.	O
three	O
unimodal	O
distributions	O
.	O
two	O
student	B
distributions	O
,	O
with	O
parameters	O
(	O
m	O
;	O
s	O
)	O
=	O
(	O
1	O
;	O
1	O
)	O
(	O
heavy	O
line	O
)	O
(	O
a	O
cauchy	O
distribution	B
)	O
and	O
(	O
2	O
;	O
4	O
)	O
(	O
light	O
line	O
)	O
,	O
and	O
a	O
gaussian	O
distribution	B
with	O
mean	B
(	O
cid:22	O
)	O
=	O
3	O
and	O
standard	O
deviation	O
(	O
cid:27	O
)	O
=	O
3	O
(	O
dashed	O
line	O
)	O
,	O
shown	O
on	O
linear	O
vertical	O
scales	O
(	O
top	O
)	O
and	O
logarithmic	O
vertical	O
scales	O
(	O
bottom	O
)	O
.	O
notice	O
that	O
the	O
heavy	O
tails	O
of	O
the	O
cauchy	O
distribution	B
are	O
scarcely	O
evident	O
in	O
the	O
upper	O
‘	O
bell-shaped	O
curve	O
’	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
23.3	O
:	O
distributions	O
over	O
positive	O
real	O
numbers	O
313	O
and	O
n	O
is	O
called	O
the	O
number	O
of	O
degrees	O
of	O
freedom	O
and	O
(	O
cid:0	O
)	O
is	O
the	O
gamma	B
function	I
.	O
if	O
n	O
>	O
1	O
then	O
the	O
student	B
distribution	O
(	O
23.8	O
)	O
has	O
a	O
mean	B
and	O
that	O
mean	B
is	O
if	O
n	O
>	O
2	O
the	O
distribution	B
also	O
has	O
a	O
(	O
cid:12	O
)	O
nite	O
variance	B
,	O
(	O
cid:27	O
)	O
2	O
=	O
ns2=	O
(	O
n	O
(	O
cid:0	O
)	O
2	O
)	O
.	O
(	O
cid:22	O
)	O
.	O
as	O
n	O
!	O
1	O
,	O
the	O
student	B
distribution	O
approaches	O
the	O
normal	B
distribution	O
with	O
mean	O
(	O
cid:22	O
)	O
and	O
standard	O
deviation	O
s.	O
the	O
student	B
distribution	O
arises	O
both	O
in	O
classical	O
statistics	O
(	O
as	O
the	O
sampling-theoretic	O
distribution	B
of	O
certain	O
statistics	O
)	O
and	O
in	O
bayesian	O
inference	B
(	O
as	O
the	O
probability	B
distribution	O
of	O
a	O
variable	O
coming	O
from	O
a	O
gaussian	O
distribution	B
whose	O
standard	B
deviation	I
we	O
aren	O
’	O
t	O
sure	O
of	O
)	O
.	O
in	O
the	O
special	O
case	O
n	O
=	O
1	O
,	O
the	O
student	B
distribution	O
is	O
called	O
the	O
cauchy	O
distribution	B
.	O
a	O
distribution	B
whose	O
tails	O
are	O
intermediate	O
in	O
heaviness	O
between	O
student	B
and	O
gaussian	O
is	O
the	O
biexponential	B
distribution	O
,	O
p	O
(	O
xj	O
(	O
cid:22	O
)	O
;	O
s	O
)	O
=	O
1	O
z	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
jx	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
j	O
s	O
(	O
cid:19	O
)	O
x	O
2	O
(	O
(	O
cid:0	O
)	O
1	O
;	O
1	O
)	O
where	O
z	O
=	O
2s	O
:	O
the	O
inverse-cosh	B
distribution	I
p	O
(	O
xj	O
(	O
cid:12	O
)	O
)	O
/	O
1	O
[	O
cosh	O
(	O
(	O
cid:12	O
)	O
x	O
)	O
]	O
1=	O
(	O
cid:12	O
)	O
(	O
23.10	O
)	O
(	O
23.11	O
)	O
(	O
23.12	O
)	O
is	O
a	O
popular	O
model	B
in	O
independent	B
component	I
analysis	I
.	O
in	O
the	O
limit	O
of	O
large	O
(	O
cid:12	O
)	O
,	O
the	O
probability	B
distribution	O
p	O
(	O
xj	O
(	O
cid:12	O
)	O
)	O
becomes	O
a	O
biexponential	B
distribution	O
.	O
in	O
the	O
limit	O
(	O
cid:12	O
)	O
!	O
0	O
p	O
(	O
xj	O
(	O
cid:12	O
)	O
)	O
approaches	O
a	O
gaussian	O
with	O
mean	O
zero	O
and	O
variance	O
1=	O
(	O
cid:12	O
)	O
.	O
23.3	O
distributions	O
over	O
positive	O
real	O
numbers	O
exponential	B
,	O
gamma	B
,	O
inverse-gamma	O
,	O
and	O
log-normal	O
.	O
the	O
exponential	B
distribution	I
,	O
where	O
p	O
(	O
xj	O
s	O
)	O
=	O
1	O
z	O
exp	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
x	O
s	O
(	O
cid:17	O
)	O
x	O
2	O
(	O
0	O
;	O
1	O
)	O
;	O
z	O
=	O
s	O
;	O
(	O
23.13	O
)	O
(	O
23.14	O
)	O
arises	O
in	O
waiting	O
problems	O
.	O
how	O
long	O
will	O
you	O
have	O
to	O
wait	O
for	O
a	O
bus	O
in	O
pois-	O
sonville	O
,	O
given	O
that	O
buses	O
arrive	O
independently	O
at	O
random	B
with	O
one	O
every	O
s	O
minutes	O
on	O
average	O
?	O
answer	O
:	O
the	O
probability	B
distribution	O
of	O
your	O
wait	O
,	O
x	O
,	O
is	O
exponential	B
with	O
mean	B
s.	O
the	O
gamma	B
distribution	I
is	O
like	O
a	O
gaussian	O
distribution	B
,	O
except	O
whereas	O
the	O
gaussian	O
goes	O
from	O
(	O
cid:0	O
)	O
1	O
to	O
1	O
,	O
gamma	B
distributions	O
go	O
from	O
0	O
to	O
1.	O
just	O
as	O
the	O
gaussian	O
distribution	B
has	O
two	O
parameters	B
(	O
cid:22	O
)	O
and	O
(	O
cid:27	O
)	O
which	O
control	O
the	O
mean	B
and	O
width	O
of	O
the	O
distribution	O
,	O
the	O
gamma	B
distribution	I
has	O
two	O
parameters	B
.	O
it	O
is	O
the	O
product	O
of	O
the	O
one-parameter	O
exponential	B
distribution	I
(	O
23.13	O
)	O
with	O
a	O
polynomial	O
,	O
xc	O
(	O
cid:0	O
)	O
1.	O
the	O
exponent	O
c	O
in	O
the	O
polynomial	O
is	O
the	O
second	O
parameter	O
.	O
p	O
(	O
xj	O
s	O
;	O
c	O
)	O
=	O
(	O
cid:0	O
)	O
(	O
x	O
;	O
s	O
;	O
c	O
)	O
=	O
where	O
1	O
z	O
(	O
cid:16	O
)	O
x	O
s	O
(	O
cid:17	O
)	O
c	O
(	O
cid:0	O
)	O
1	O
exp	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
x	O
s	O
(	O
cid:17	O
)	O
;	O
0	O
(	O
cid:20	O
)	O
x	O
<	O
1	O
(	O
23.15	O
)	O
z	O
=	O
(	O
cid:0	O
)	O
(	O
c	O
)	O
s	O
:	O
(	O
23.16	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
314	O
23	O
|	O
useful	O
probability	B
distributions	I
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
1	O
0.1	O
0.01	O
0.001	O
0.0001	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
2	O
4	O
6	O
8	O
10	O
-4	O
-2	O
0	O
2	O
4	O
0.1	O
0.01	O
0.001	O
0.0001	O
0	O
2	O
4	O
6	O
8	O
10	O
x	O
-4	O
-2	O
0	O
2	O
4	O
l	O
=	O
ln	O
x	O
figure	O
23.4.	O
two	O
gamma	B
distributions	O
,	O
with	O
parameters	O
(	O
s	O
;	O
c	O
)	O
=	O
(	O
1	O
;	O
3	O
)	O
(	O
heavy	O
lines	O
)	O
and	O
10	O
;	O
0:3	O
(	O
light	O
lines	O
)	O
,	O
shown	O
on	O
linear	O
vertical	O
scales	O
(	O
top	O
)	O
and	O
logarithmic	O
vertical	O
scales	O
(	O
bottom	O
)	O
;	O
and	O
shown	O
as	O
a	O
function	B
of	O
x	O
on	O
the	O
left	O
(	O
23.15	O
)	O
and	O
l	O
=	O
ln	O
x	O
on	O
the	O
right	O
(	O
23.18	O
)	O
.	O
this	O
is	O
a	O
simple	O
peaked	O
distribution	B
with	O
mean	B
sc	O
and	O
variance	O
s2c	O
.	O
it	O
is	O
often	O
natural	B
to	O
represent	O
a	O
positive	O
real	O
variable	O
x	O
in	O
terms	O
of	O
its	O
logarithm	O
l	O
=	O
ln	O
x.	O
the	O
probability	B
density	O
of	O
l	O
is	O
@	O
x	O
@	O
l	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
p	O
(	O
l	O
)	O
=	O
p	O
(	O
x	O
(	O
l	O
)	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
zl	O
(	O
cid:18	O
)	O
x	O
(	O
l	O
)	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
s	O
(	O
cid:19	O
)	O
c	O
x	O
(	O
l	O
)	O
s	O
(	O
cid:19	O
)	O
;	O
=	O
p	O
(	O
x	O
(	O
l	O
)	O
)	O
x	O
(	O
l	O
)	O
1	O
=	O
where	O
zl	O
=	O
(	O
cid:0	O
)	O
(	O
c	O
)	O
:	O
(	O
23.17	O
)	O
(	O
23.18	O
)	O
(	O
23.19	O
)	O
[	O
the	O
gamma	B
distribution	I
is	O
named	O
after	O
its	O
normalizing	B
constant	I
{	O
an	O
odd	O
convention	O
,	O
it	O
seems	O
to	O
me	O
!	O
]	O
figure	O
23.4	O
shows	O
a	O
couple	O
of	O
gamma	O
distributions	O
as	O
a	O
function	B
of	O
x	O
and	O
of	O
l.	O
notice	O
that	O
where	O
the	O
original	O
gamma	B
distribution	I
(	O
23.15	O
)	O
may	O
have	O
a	O
‘	O
spike	O
’	O
at	O
x	O
=	O
0	O
,	O
the	O
distribution	B
over	O
l	O
never	O
has	O
such	O
a	O
spike	O
.	O
the	O
spike	O
is	O
an	O
artefact	O
of	O
a	O
bad	B
choice	O
of	O
basis	O
.	O
in	O
the	O
limit	O
sc	O
=	O
1	O
;	O
c	O
!	O
0	O
,	O
we	O
obtain	O
the	O
noninformative	B
prior	O
for	O
a	O
scale	O
parameter	O
,	O
the	O
1=x	O
prior	B
.	O
this	O
improper	B
prior	O
is	O
called	O
noninformative	B
because	O
it	O
has	O
no	O
associated	O
length	B
scale	O
,	O
no	O
characteristic	O
value	O
of	O
x	O
,	O
so	O
it	O
prefers	O
all	O
values	O
of	O
x	O
equally	O
.	O
it	O
is	O
invariant	O
under	O
the	O
reparameterization	O
x	O
=	O
mx	O
.	O
if	O
we	O
transform	O
the	O
1=x	O
probability	B
density	O
into	O
a	O
density	B
over	O
l	O
=	O
ln	O
x	O
we	O
(	O
cid:12	O
)	O
nd	O
the	O
latter	O
density	B
is	O
uniform	O
.	O
.	O
exercise	O
23.2	O
.	O
[	O
1	O
]	O
imagine	O
that	O
we	O
reparameterize	O
a	O
positive	O
variable	O
x	O
in	O
terms	O
of	O
its	O
cube	O
root	O
,	O
u	O
=	O
x1=3	O
.	O
if	O
the	O
probability	B
density	O
of	O
x	O
is	O
the	O
improper	B
distribution	O
1=x	O
,	O
what	O
is	O
the	O
probability	B
density	O
of	O
u	O
?	O
the	O
gamma	B
distribution	I
is	O
always	O
a	O
unimodal	O
density	B
over	O
l	O
=	O
ln	O
x	O
,	O
and	O
,	O
as	O
can	O
be	O
seen	O
in	O
the	O
(	O
cid:12	O
)	O
gures	O
,	O
it	O
is	O
asymmetric	O
.	O
if	O
x	O
has	O
a	O
gamma	B
distribution	I
,	O
and	O
we	O
decide	O
to	O
work	O
in	O
terms	O
of	O
the	O
inverse	O
of	O
x	O
,	O
v	O
=	O
1=x	O
,	O
we	O
obtain	O
a	O
new	O
distribution	B
,	O
in	O
which	O
the	O
density	B
over	O
l	O
is	O
(	O
cid:13	O
)	O
ipped	O
left-for-right	O
:	O
the	O
probability	B
density	O
of	O
v	O
is	O
called	O
an	O
inverse-gamma	B
distribution	I
,	O
p	O
(	O
v	O
j	O
s	O
;	O
c	O
)	O
=	O
where	O
1	O
zv	O
(	O
cid:18	O
)	O
1	O
sv	O
(	O
cid:19	O
)	O
c+1	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
1	O
sv	O
(	O
cid:19	O
)	O
;	O
0	O
(	O
cid:20	O
)	O
v	O
<	O
1	O
(	O
23.20	O
)	O
zv	O
=	O
(	O
cid:0	O
)	O
(	O
c	O
)	O
=s	O
:	O
(	O
23.21	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
23.4	O
:	O
distributions	O
over	B
periodic	I
variables	I
315	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
1	O
0.1	O
0.01	O
0.001	O
0.0001	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
1	O
2	O
3	O
-4	O
-2	O
0	O
2	O
4	O
0.1	O
0.01	O
0.001	O
0.0001	O
0	O
1	O
2	O
3	O
v	O
-4	O
-2	O
0	O
2	O
4	O
ln	O
v	O
figure	O
23.5.	O
two	O
inverse	O
gamma	O
distributions	O
,	O
with	O
parameters	O
(	O
s	O
;	O
c	O
)	O
=	O
(	O
1	O
;	O
3	O
)	O
(	O
heavy	O
lines	O
)	O
and	O
10	O
;	O
0:3	O
(	O
light	O
lines	O
)	O
,	O
shown	O
on	O
linear	O
vertical	O
scales	O
(	O
top	O
)	O
and	O
logarithmic	O
vertical	O
scales	O
(	O
bottom	O
)	O
;	O
and	O
shown	O
as	O
a	O
function	B
of	O
x	O
on	O
the	O
left	O
and	O
l	O
=	O
ln	O
x	O
on	O
the	O
right	O
.	O
gamma	B
and	O
inverse	O
gamma	O
distributions	O
crop	O
up	O
in	O
many	O
inference	B
prob-	O
lems	O
in	O
which	O
a	O
positive	O
quantity	O
is	O
inferred	O
from	O
data	O
.	O
examples	O
include	O
inferring	O
the	O
variance	B
of	O
gaussian	O
noise	B
from	O
some	O
noise	B
samples	O
,	O
and	O
infer-	O
ring	O
the	O
rate	B
parameter	O
of	O
a	O
poisson	O
distribution	B
from	O
the	O
count	O
.	O
gamma	B
distributions	O
also	O
arise	O
naturally	O
in	O
the	O
distributions	O
of	O
waiting	O
times	O
between	O
poisson-distributed	O
events	O
.	O
given	O
a	O
poisson	O
process	O
with	O
rate	O
(	O
cid:21	O
)	O
,	O
the	O
probability	B
density	O
of	O
the	O
arrival	O
time	O
x	O
of	O
the	O
mth	O
event	O
is	O
(	O
cid:21	O
)	O
(	O
(	O
cid:21	O
)	O
x	O
)	O
m	O
(	O
cid:0	O
)	O
1	O
(	O
m	O
(	O
cid:0	O
)	O
1	O
)	O
!	O
e	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
x	O
:	O
(	O
23.22	O
)	O
log-normal	B
distribution	O
another	O
distribution	B
over	O
a	O
positive	O
real	O
number	O
x	O
is	O
the	O
log-normal	B
distribu-	O
tion	O
,	O
which	O
is	O
the	O
distribution	B
that	O
results	O
when	O
l	O
=	O
ln	O
x	O
has	O
a	O
normal	B
distri-	O
bution	O
.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
m	O
to	O
be	O
the	O
median	O
value	O
of	O
x	O
,	O
and	O
s	O
to	O
be	O
the	O
standard	B
deviation	I
of	O
ln	O
x	O
.	O
0	O
1	O
2	O
3	O
4	O
5	O
0.4	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
0.1	O
0.01	O
0.001	O
0.0001	O
p	O
(	O
l	O
j	O
m	O
;	O
s	O
)	O
=	O
1	O
z	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
(	O
l	O
(	O
cid:0	O
)	O
ln	O
m	O
)	O
2	O
2s2	O
(	O
cid:19	O
)	O
l	O
2	O
(	O
(	O
cid:0	O
)	O
1	O
;	O
1	O
)	O
;	O
z	O
=	O
p2	O
(	O
cid:25	O
)	O
s2	O
;	O
where	O
implies	O
p	O
(	O
xj	O
m	O
;	O
s	O
)	O
=	O
1	O
x	O
1	O
z	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
(	O
ln	O
x	O
(	O
cid:0	O
)	O
ln	O
m	O
)	O
2	O
2s2	O
(	O
cid:19	O
)	O
x	O
2	O
(	O
0	O
;	O
1	O
)	O
:	O
(	O
23.23	O
)	O
0	O
1	O
2	O
3	O
4	O
5	O
(	O
23.24	O
)	O
(	O
23.25	O
)	O
figure	O
23.6.	O
two	O
log-normal	B
distributions	O
,	O
with	O
parameters	O
(	O
m	O
;	O
s	O
)	O
=	O
(	O
3	O
;	O
1:8	O
)	O
(	O
heavy	O
line	O
)	O
and	O
(	O
3	O
;	O
0:7	O
)	O
(	O
light	O
line	O
)	O
,	O
shown	O
on	O
linear	O
vertical	O
scales	O
(	O
top	O
)	O
and	O
logarithmic	O
vertical	O
scales	O
(	O
bottom	O
)	O
.	O
[	O
yes	O
,	O
they	O
really	O
do	O
have	O
the	O
same	O
value	O
of	O
the	O
median	O
,	O
m	O
=	O
3	O
.	O
]	O
23.4	O
distributions	O
over	B
periodic	I
variables	I
a	O
periodic	B
variable	I
(	O
cid:18	O
)	O
is	O
a	O
real	O
number	O
2	O
[	O
0	O
;	O
2	O
(	O
cid:25	O
)	O
]	O
having	O
the	O
property	O
that	O
(	O
cid:18	O
)	O
=	O
0	O
and	O
(	O
cid:18	O
)	O
=	O
2	O
(	O
cid:25	O
)	O
are	O
equivalent	O
.	O
a	O
distribution	B
that	O
plays	O
for	O
periodic	O
variables	O
the	O
role	O
played	O
by	O
the	O
gaus-	O
sian	O
distribution	B
for	O
real	O
variables	O
is	O
the	O
von	O
mises	O
distribution	B
:	O
p	O
(	O
(	O
cid:18	O
)	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:12	O
)	O
)	O
=	O
1	O
z	O
exp	O
(	O
(	O
cid:12	O
)	O
cos	O
(	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
)	O
)	O
(	O
cid:18	O
)	O
2	O
(	O
0	O
;	O
2	O
(	O
cid:25	O
)	O
)	O
:	O
(	O
23.26	O
)	O
the	O
normalizing	B
constant	I
is	O
z	O
=	O
2	O
(	O
cid:25	O
)	O
i0	O
(	O
(	O
cid:12	O
)	O
)	O
,	O
where	O
i0	O
(	O
x	O
)	O
is	O
a	O
modi	O
(	O
cid:12	O
)	O
ed	O
bessel	O
function	B
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
316	O
23	O
|	O
useful	O
probability	B
distributions	I
a	O
distribution	B
that	O
arises	O
from	O
brownian	O
di	O
(	O
cid:11	O
)	O
usion	O
around	O
the	O
circle	B
is	O
the	O
wrapped	O
gaussian	O
distribution	B
,	O
p	O
(	O
(	O
cid:18	O
)	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
1xn=	O
(	O
cid:0	O
)	O
1	O
normal	B
(	O
(	O
cid:18	O
)	O
;	O
(	O
(	O
cid:22	O
)	O
+	O
2	O
(	O
cid:25	O
)	O
n	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
)	O
(	O
cid:18	O
)	O
2	O
(	O
0	O
;	O
2	O
(	O
cid:25	O
)	O
)	O
:	O
(	O
23.27	O
)	O
23.5	O
distributions	O
over	O
probabilities	O
beta	B
distribution	I
,	O
dirichlet	O
distribution	B
,	O
entropic	B
distribution	I
the	O
beta	B
distribution	I
is	O
a	O
probability	B
density	O
over	O
a	O
variable	O
p	O
that	O
is	O
a	O
prob-	O
ability	O
,	O
p	O
2	O
(	O
0	O
;	O
1	O
)	O
:	O
p	O
(	O
pj	O
u1	O
;	O
u2	O
)	O
=	O
1	O
z	O
(	O
u1	O
;	O
u2	O
)	O
pu1	O
(	O
cid:0	O
)	O
1	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
u2	O
(	O
cid:0	O
)	O
1	O
:	O
(	O
23.28	O
)	O
the	O
parameters	B
u1	O
;	O
u2	O
may	O
take	O
any	O
positive	O
value	O
.	O
the	O
normalizing	B
constant	I
is	O
the	O
beta	B
function	I
,	O
z	O
(	O
u1	O
;	O
u2	O
)	O
=	O
(	O
cid:0	O
)	O
(	O
u1	O
)	O
(	O
cid:0	O
)	O
(	O
u2	O
)	O
(	O
cid:0	O
)	O
(	O
u1	O
+	O
u2	O
)	O
:	O
(	O
23.29	O
)	O
special	O
cases	O
include	O
the	O
uniform	O
distribution	B
{	O
u1	O
=	O
1	O
;	O
u2	O
=	O
1	O
;	O
the	O
je	O
(	O
cid:11	O
)	O
reys	O
prior	B
{	O
u1	O
=	O
0:5	O
;	O
u2	O
=	O
0:5	O
;	O
and	O
the	O
improper	B
laplace	O
prior	B
{	O
u1	O
=	O
0	O
;	O
u2	O
=	O
0.	O
if	O
we	O
transform	O
the	O
beta	B
distribution	I
to	O
the	O
corresponding	O
density	B
over	O
the	O
logit	B
l	O
(	O
cid:17	O
)	O
ln	O
p/	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
,	O
we	O
(	O
cid:12	O
)	O
nd	O
it	O
is	O
always	O
a	O
pleasant	O
bell-shaped	O
density	B
over	O
l	O
,	O
while	O
the	O
density	B
over	O
p	O
may	O
have	O
singularities	O
at	O
p	O
=	O
0	O
and	O
p	O
=	O
1	O
(	O
(	O
cid:12	O
)	O
gure	O
23.7	O
)	O
.	O
more	O
dimensions	B
5	O
4	O
3	O
2	O
1	O
0	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
0.25	O
0.5	O
0.75	O
1	O
-6	O
-4	O
-2	O
0	O
2	O
4	O
6	O
figure	O
23.7.	O
three	O
beta	O
distributions	O
,	O
with	O
(	O
u1	O
;	O
u2	O
)	O
=	O
(	O
0:3	O
;	O
1	O
)	O
,	O
(	O
1:3	O
;	O
1	O
)	O
,	O
and	O
(	O
12	O
;	O
2	O
)	O
.	O
the	O
upper	O
(	O
cid:12	O
)	O
gure	O
shows	O
p	O
(	O
pj	O
u1	O
;	O
u2	O
)	O
as	O
a	O
function	B
of	O
p	O
;	O
the	O
lower	O
shows	O
the	O
corresponding	O
density	B
over	O
the	O
logit	B
,	O
ln	O
p	O
1	O
(	O
cid:0	O
)	O
p	O
:	O
notice	O
how	O
well-behaved	O
the	O
densities	O
are	O
as	O
a	O
function	B
of	O
the	O
logit	B
.	O
the	O
dirichlet	O
distribution	B
is	O
a	O
density	B
over	O
an	O
i-dimensional	O
vector	O
p	O
whose	O
i	O
components	O
are	O
positive	O
and	O
sum	O
to	O
1.	O
the	O
beta	B
distribution	I
is	O
a	O
special	O
case	O
of	O
a	O
dirichlet	O
distribution	B
with	O
i	O
=	O
2.	O
the	O
dirichlet	O
distribution	B
is	O
parameterized	O
by	O
a	O
measure	O
u	O
(	O
a	O
vector	O
with	O
all	O
coe	O
(	O
cid:14	O
)	O
cients	O
ui	O
>	O
0	O
)	O
which	O
i	O
will	O
write	O
here	O
as	O
u	O
=	O
(	O
cid:11	O
)	O
m	O
,	O
where	O
m	O
is	O
a	O
normalized	O
measure	O
over	O
the	O
i	O
components	O
(	O
p	O
mi	O
=	O
1	O
)	O
,	O
and	O
(	O
cid:11	O
)	O
is	O
positive	O
:	O
p	O
(	O
pj	O
(	O
cid:11	O
)	O
m	O
)	O
=	O
p	O
(	O
cid:11	O
)	O
mi	O
(	O
cid:0	O
)	O
1	O
i	O
z	O
(	O
(	O
cid:11	O
)	O
m	O
)	O
1	O
(	O
cid:14	O
)	O
(	O
pi	O
pi	O
(	O
cid:0	O
)	O
1	O
)	O
(	O
cid:17	O
)	O
dirichlet	O
(	O
i	O
)	O
(	O
pj	O
(	O
cid:11	O
)	O
m	O
)	O
:	O
(	O
23.30	O
)	O
to	O
the	O
simplex	B
such	O
that	O
p	O
is	O
normalized	O
,	O
i.e.	O
,	O
pi	O
pi	O
=	O
1.	O
the	O
normalizing	O
the	O
function	B
(	O
cid:14	O
)	O
(	O
x	O
)	O
is	O
the	O
dirac	O
delta	B
function	I
,	O
which	O
restricts	O
the	O
distribution	B
constant	O
of	O
the	O
dirichlet	O
distribution	B
is	O
:	O
i	O
yi=1	O
(	O
cid:0	O
)	O
(	O
(	O
cid:11	O
)	O
mi	O
)	O
/	O
(	O
cid:0	O
)	O
(	O
(	O
cid:11	O
)	O
)	O
:	O
(	O
23.31	O
)	O
z	O
(	O
(	O
cid:11	O
)	O
m	O
)	O
=yi	O
the	O
vector	O
m	O
is	O
the	O
mean	B
of	O
the	O
probability	B
distribution	O
:	O
z	O
dirichlet	O
(	O
i	O
)	O
(	O
pj	O
(	O
cid:11	O
)	O
m	O
)	O
p	O
dip	O
=	O
m	O
:	O
(	O
23.32	O
)	O
when	O
working	O
with	O
a	O
probability	B
vector	O
p	O
,	O
it	O
is	O
often	O
helpful	O
to	O
work	O
in	O
the	O
‘	O
softmax	B
basis	O
’	O
,	O
in	O
which	O
,	O
for	O
example	O
,	O
a	O
three-dimensional	O
probability	B
p	O
=	O
(	O
p1	O
;	O
p2	O
;	O
p3	O
)	O
is	O
represented	O
by	O
three	O
numbers	O
a1	O
;	O
a2	O
;	O
a3	O
satisfying	O
a1	O
+a2	O
+a3	O
=	O
0	O
and	O
pi	O
=	O
1	O
z	O
eai	O
;	O
where	O
z	O
=pi	O
eai	O
.	O
(	O
23.33	O
)	O
this	O
nonlinear	B
transformation	O
is	O
analogous	O
to	O
the	O
(	O
cid:27	O
)	O
!	O
ln	O
(	O
cid:27	O
)	O
transformation	O
for	O
a	O
scale	O
variable	O
and	O
the	O
logit	B
transformation	O
for	O
a	O
single	O
probability	O
,	O
p	O
!	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
23.5	O
:	O
distributions	O
over	O
probabilities	O
317	O
u	O
=	O
(	O
20	O
;	O
10	O
;	O
7	O
)	O
u	O
=	O
(	O
0:2	O
;	O
1	O
;	O
2	O
)	O
u	O
=	O
(	O
0:2	O
;	O
0:3	O
;	O
0:15	O
)	O
8	O
4	O
0	O
-4	O
8	O
4	O
0	O
-4	O
8	O
4	O
0	O
-4	O
-8	O
-8	O
-4	O
0	O
4	O
-8	O
-8	O
8	O
-4	O
0	O
4	O
-8	O
-8	O
8	O
-4	O
0	O
4	O
8	O
ln	O
p	O
1	O
(	O
cid:0	O
)	O
p	O
.	O
dirichlet	O
distribution	B
(	O
23.30	O
)	O
disappear	O
,	O
and	O
the	O
density	B
is	O
given	O
by	O
:	O
in	O
the	O
softmax	B
basis	O
,	O
the	O
ugly	O
minus-ones	O
in	O
the	O
exponents	O
in	O
the	O
p	O
(	O
aj	O
(	O
cid:11	O
)	O
m	O
)	O
/	O
1	O
z	O
(	O
(	O
cid:11	O
)	O
m	O
)	O
p	O
(	O
cid:11	O
)	O
mi	O
i	O
i	O
yi=1	O
(	O
cid:14	O
)	O
(	O
pi	O
ai	O
)	O
:	O
(	O
23.34	O
)	O
the	O
role	O
of	O
the	O
parameter	O
(	O
cid:11	O
)	O
can	O
be	O
characterized	O
in	O
two	O
ways	O
.	O
first	O
,	O
(	O
cid:11	O
)	O
mea-	O
sures	O
the	O
sharpness	O
of	O
the	O
distribution	O
(	O
(	O
cid:12	O
)	O
gure	O
23.8	O
)	O
;	O
it	O
measures	O
how	O
di	O
(	O
cid:11	O
)	O
erent	O
we	O
expect	O
typical	B
samples	O
p	O
from	O
the	O
distribution	B
to	O
be	O
from	O
the	O
mean	B
m	O
,	O
just	O
as	O
the	O
precision	B
(	O
cid:28	O
)	O
=	O
1/	O
(	O
cid:27	O
)	O
2	O
of	O
a	O
gaussian	O
measures	O
how	O
far	O
samples	O
stray	O
from	O
its	O
mean	B
.	O
a	O
large	O
value	O
of	O
(	O
cid:11	O
)	O
produces	O
a	O
distribution	B
over	O
p	O
that	O
is	O
sharply	O
peaked	O
around	O
m.	O
the	O
e	O
(	O
cid:11	O
)	O
ect	O
of	O
(	O
cid:11	O
)	O
in	O
higher-dimensional	O
situations	O
can	O
be	O
visualized	O
by	O
drawing	O
a	O
typical	B
sample	O
from	O
the	O
distribution	B
dirichlet	O
(	O
i	O
)	O
(	O
pj	O
(	O
cid:11	O
)	O
m	O
)	O
,	O
with	O
m	O
set	B
to	O
the	O
uniform	O
vector	O
mi	O
=	O
1/i	O
,	O
and	O
making	O
a	O
zipf	O
plot	O
,	O
that	O
is	O
,	O
a	O
ranked	O
plot	O
of	O
the	O
values	O
of	O
the	O
components	O
pi	O
.	O
it	O
is	O
traditional	O
to	O
plot	O
both	O
pi	O
(	O
ver-	O
tical	O
axis	O
)	O
and	O
the	O
rank	O
(	O
horizontal	O
axis	O
)	O
on	O
logarithmic	O
scales	O
so	O
that	O
power	B
law	I
relationships	O
appear	O
as	O
straight	O
lines	O
.	O
figure	O
23.9	O
shows	O
these	O
plots	O
for	O
a	O
single	O
sample	O
from	O
ensembles	O
with	O
i	O
=	O
100	O
and	O
i	O
=	O
1000	O
and	O
with	O
(	O
cid:11	O
)	O
from	O
0.1	O
to	O
1000.	O
for	O
large	O
(	O
cid:11	O
)	O
,	O
the	O
plot	O
is	O
shallow	O
with	O
many	O
components	O
having	O
simi-	O
lar	O
values	O
.	O
for	O
small	O
(	O
cid:11	O
)	O
,	O
typically	O
one	O
component	O
pi	O
receives	O
an	O
overwhelming	O
share	O
of	O
the	O
probability	O
,	O
and	O
of	O
the	O
small	O
probability	B
that	O
remains	O
to	O
be	O
shared	O
among	O
the	O
other	O
components	O
,	O
another	O
component	O
pi0	O
receives	O
a	O
similarly	O
large	O
share	O
.	O
in	O
the	O
limit	O
as	O
(	O
cid:11	O
)	O
goes	O
to	O
zero	O
,	O
the	O
plot	O
tends	O
to	O
an	O
increasingly	O
steep	O
power	B
law	I
.	O
second	O
,	O
we	O
can	O
characterize	O
the	O
role	O
of	O
(	O
cid:11	O
)	O
in	O
terms	O
of	O
the	O
predictive	O
dis-	O
tribution	O
that	O
results	O
when	O
we	O
observe	O
samples	O
from	O
p	O
and	O
obtain	O
counts	O
f	O
=	O
(	O
f1	O
;	O
f2	O
;	O
:	O
:	O
:	O
;	O
fi	O
)	O
of	O
the	O
possible	O
outcomes	O
.	O
the	O
value	O
of	O
(	O
cid:11	O
)	O
de	O
(	O
cid:12	O
)	O
nes	O
the	O
number	O
of	O
samples	O
from	O
p	O
that	O
are	O
required	O
in	O
order	O
that	O
the	O
data	O
dominate	O
over	O
the	O
prior	B
in	O
predictions	O
.	O
exercise	O
23.3	O
.	O
[	O
3	O
]	O
the	O
dirichlet	O
distribution	B
satis	O
(	O
cid:12	O
)	O
es	O
a	O
nice	O
additivity	O
property	O
.	O
imagine	O
that	O
a	O
biased	O
six-sided	O
die	B
has	O
two	O
red	O
faces	O
and	O
four	O
blue	O
faces	O
.	O
the	O
die	B
is	O
rolled	O
n	O
times	O
and	O
two	O
bayesians	O
examine	O
the	O
outcomes	O
in	O
order	O
to	O
infer	O
the	O
bias	B
of	O
the	O
die	B
and	O
make	O
predictions	O
.	O
one	O
bayesian	O
has	O
access	O
to	O
the	O
red/blue	O
colour	O
outcomes	O
only	O
,	O
and	O
he	O
infers	O
a	O
two-	O
component	O
probability	B
vector	O
(	O
pr	O
;	O
pb	O
)	O
.	O
the	O
other	O
bayesian	O
has	O
access	O
to	O
each	O
full	O
outcome	O
:	O
he	O
can	O
see	O
which	O
of	O
the	O
six	O
faces	O
came	O
up	O
,	O
and	O
he	O
infers	O
a	O
six-component	O
probability	B
vector	O
(	O
p1	O
;	O
p2	O
;	O
p3	O
;	O
p4	O
;	O
p5	O
;	O
p6	O
)	O
,	O
where	O
figure	O
23.8.	O
three	O
dirichlet	O
distributions	O
over	O
a	O
three-dimensional	O
probability	B
vector	O
(	O
p1	O
;	O
p2	O
;	O
p3	O
)	O
.	O
the	O
upper	O
(	O
cid:12	O
)	O
gures	O
show	O
1000	O
random	B
draws	O
from	O
each	O
distribution	B
,	O
showing	O
the	O
values	O
of	O
p1	O
and	O
p2	O
on	O
the	O
two	O
axes	O
.	O
p3	O
=	O
1	O
(	O
cid:0	O
)	O
(	O
p1	O
+	O
p2	O
)	O
.	O
the	O
triangle	B
in	O
the	O
(	O
cid:12	O
)	O
rst	O
(	O
cid:12	O
)	O
gure	O
is	O
the	O
simplex	B
of	O
legal	O
probability	B
distributions	I
.	O
the	O
lower	O
(	O
cid:12	O
)	O
gures	O
show	O
the	O
same	O
points	O
in	O
the	O
‘	O
softmax	B
’	O
basis	O
(	O
equation	O
(	O
23.33	O
)	O
)	O
.	O
the	O
two	O
axes	O
show	O
a1	O
and	O
a2	O
.	O
a3	O
=	O
(	O
cid:0	O
)	O
a1	O
(	O
cid:0	O
)	O
a2	O
.	O
1	O
0.1	O
0.01	O
0.001	O
0.0001	O
1	O
1	O
0.1	O
0.01	O
0.001	O
0.0001	O
1e-05	O
1	O
i	O
=	O
100	O
0.1	O
1	O
10	O
100	O
1000	O
10	O
100	O
i	O
=	O
1000	O
0.1	O
1	O
10	O
100	O
1000	O
10	O
100	O
1000	O
figure	O
23.9.	O
zipf	O
plots	O
for	O
random	O
samples	O
from	O
dirichlet	O
distributions	O
with	O
various	O
values	O
of	O
(	O
cid:11	O
)	O
=	O
0:1	O
:	O
:	O
:	O
1000.	O
for	O
each	O
value	O
of	O
i	O
=	O
100	O
or	O
1000	O
and	O
each	O
(	O
cid:11	O
)	O
,	O
one	O
sample	B
p	O
from	O
the	O
dirichlet	O
distribution	B
was	O
generated	O
.	O
the	O
zipf	O
plot	O
shows	O
the	O
probabilities	O
pi	O
,	O
ranked	O
by	O
magnitude	O
,	O
versus	O
their	O
rank	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
318	O
23	O
|	O
useful	O
probability	B
distributions	I
pr	O
=	O
p1	O
+	O
p2	O
and	O
pb	O
=	O
p3	O
+	O
p4	O
+	O
p5	O
+	O
p6	O
.	O
assuming	O
that	O
the	O
sec-	O
ond	O
bayesian	O
assigns	O
a	O
dirichlet	O
distribution	B
to	O
(	O
p1	O
;	O
p2	O
;	O
p3	O
;	O
p4	O
;	O
p5	O
;	O
p6	O
)	O
with	O
hyperparameters	O
(	O
u1	O
;	O
u2	O
;	O
u3	O
;	O
u4	O
;	O
u5	O
;	O
u6	O
)	O
,	O
show	O
that	O
,	O
in	O
order	O
for	O
the	O
(	O
cid:12	O
)	O
rst	O
bayesian	O
’	O
s	O
inferences	O
to	O
be	O
consistent	O
with	O
those	O
of	O
the	O
second	O
bayesian	O
,	O
the	O
(	O
cid:12	O
)	O
rst	O
bayesian	O
’	O
s	O
prior	B
should	O
be	O
a	O
dirichlet	O
distribution	B
with	O
hyper-	O
parameters	B
(	O
(	O
u1	O
+	O
u2	O
)	O
;	O
(	O
u3	O
+	O
u4	O
+	O
u5	O
+	O
u6	O
)	O
)	O
.	O
hint	O
:	O
a	O
brute-force	O
approach	O
is	O
to	O
compute	O
the	O
integral	B
p	O
(	O
pr	O
;	O
pb	O
)	O
=	O
r	O
d6p	O
p	O
(	O
pj	O
u	O
)	O
(	O
cid:14	O
)	O
(	O
pr	O
(	O
cid:0	O
)	O
(	O
p1	O
+	O
p2	O
)	O
)	O
(	O
cid:14	O
)	O
(	O
pb	O
(	O
cid:0	O
)	O
(	O
p3	O
+	O
p4	O
+	O
p5	O
+	O
p6	O
)	O
)	O
.	O
a	O
cheaper	O
approach	O
is	O
to	O
compute	O
the	O
predictive	O
distributions	O
,	O
given	O
arbitrary	O
data	O
(	O
f1	O
;	O
f2	O
;	O
f3	O
;	O
f4	O
;	O
f5	O
;	O
f6	O
)	O
,	O
and	O
(	O
cid:12	O
)	O
nd	O
the	O
condition	O
for	O
the	O
two	O
predictive	O
dis-	O
tributions	O
to	O
match	O
for	O
all	O
data	O
.	O
the	O
entropic	B
distribution	I
for	O
a	O
probability	B
vector	O
p	O
is	O
sometimes	O
used	O
in	O
the	O
‘	O
maximum	B
entropy	I
’	O
image	B
reconstruction	I
community	O
.	O
p	O
(	O
pj	O
(	O
cid:11	O
)	O
;	O
m	O
)	O
=	O
1	O
z	O
(	O
(	O
cid:11	O
)	O
;	O
m	O
)	O
exp	O
[	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
dkl	O
(	O
pjjm	O
)	O
]	O
(	O
cid:14	O
)	O
(	O
pi	O
pi	O
(	O
cid:0	O
)	O
1	O
)	O
;	O
(	O
23.35	O
)	O
where	O
m	O
,	O
the	O
measure	O
,	O
is	O
a	O
positive	O
vector	O
,	O
and	O
dkl	O
(	O
pjjm	O
)	O
=pi	O
pi	O
log	O
pi=mi	O
.	O
further	O
reading	O
see	O
(	O
mackay	O
and	O
peto	O
,	O
1995	O
)	O
for	O
fun	O
with	O
dirichlets	O
.	O
23.6	O
further	O
exercises	O
exercise	O
23.4	O
.	O
[	O
2	O
]	O
n	O
datapoints	O
fxng	O
are	O
drawn	O
from	O
a	O
gamma	B
distribution	I
p	O
(	O
xj	O
s	O
;	O
c	O
)	O
=	O
(	O
cid:0	O
)	O
(	O
x	O
;	O
s	O
;	O
c	O
)	O
with	O
unknown	O
parameters	B
s	O
and	O
c.	O
what	O
are	O
the	O
maximum	B
likelihood	I
parameters	O
s	O
and	O
c	O
?	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
24	O
exact	O
marginalization	O
how	O
can	O
we	O
avoid	O
the	O
exponentially	O
large	O
cost	O
of	O
complete	O
enumeration	O
of	O
all	O
hypotheses	O
?	O
before	O
we	O
stoop	O
to	O
approximate	O
methods	B
,	O
we	O
explore	B
two	O
approaches	O
to	O
exact	O
marginalization	O
:	O
(	O
cid:12	O
)	O
rst	O
,	O
marginalization	B
over	O
continuous	B
variables	O
(	O
sometimes	O
known	O
as	O
nuisance	O
parameters	B
)	O
by	O
doing	O
integrals	O
;	O
and	O
second	O
,	O
summation	O
over	O
discrete	O
variables	O
by	O
message-passing	O
.	O
exact	O
marginalization	O
over	O
continuous	O
parameters	B
is	O
a	O
macho	B
activity	O
en-	O
joyed	O
by	O
those	O
who	O
are	O
(	O
cid:13	O
)	O
uent	O
in	O
de	O
(	O
cid:12	O
)	O
nite	O
integration	O
.	O
this	O
chapter	O
uses	O
gamma	O
distributions	O
;	O
as	O
was	O
explained	O
in	O
the	O
previous	O
chapter	O
,	O
gamma	B
distributions	O
are	O
a	O
lot	O
like	O
gaussian	O
distributions	O
,	O
except	O
that	O
whereas	O
the	O
gaussian	O
goes	O
from	O
(	O
cid:0	O
)	O
1	O
to	O
1	O
,	O
gamma	B
distributions	O
go	O
from	O
0	O
to	O
1	O
.	O
24.1	O
inferring	O
the	O
mean	B
and	O
variance	B
of	O
a	O
gaussian	O
distribution	B
we	O
discuss	O
again	O
the	O
one-dimensional	O
gaussian	O
distribution	B
,	O
parameterized	O
by	O
a	O
mean	B
(	O
cid:22	O
)	O
and	O
a	O
standard	B
deviation	I
(	O
cid:27	O
)	O
:	O
p	O
(	O
xj	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
1	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
normal	B
(	O
x	O
;	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
)	O
:	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
)	O
2	O
(	O
24.1	O
)	O
when	O
inferring	O
these	O
parameters	B
,	O
we	O
must	O
specify	O
their	O
prior	B
distribution	O
.	O
the	O
prior	B
gives	O
us	O
the	O
opportunity	O
to	O
include	O
speci	O
(	O
cid:12	O
)	O
c	O
knowledge	O
that	O
we	O
have	O
about	O
(	O
cid:22	O
)	O
and	O
(	O
cid:27	O
)	O
(	O
from	O
independent	O
experiments	O
,	O
or	O
on	O
theoretical	O
grounds	O
,	O
for	O
example	O
)	O
.	O
if	O
we	O
have	O
no	O
such	O
knowledge	O
,	O
then	O
we	O
can	O
construct	O
an	O
appropriate	O
prior	B
that	O
embodies	O
our	O
supposed	O
ignorance	B
.	O
in	O
section	O
21.2	O
,	O
we	O
assumed	O
a	O
uniform	O
prior	B
over	O
the	O
range	O
of	O
parameters	O
plotted	O
.	O
if	O
we	O
wish	O
to	O
be	O
able	O
to	O
perform	O
exact	O
marginalizations	O
,	O
it	O
may	O
be	O
useful	O
to	O
consider	O
conjugate	O
priors	O
;	O
these	O
are	O
priors	O
whose	O
functional	O
form	O
combines	O
naturally	O
with	O
the	O
likelihood	B
such	O
that	O
the	O
inferences	O
have	O
a	O
convenient	O
form	O
.	O
conjugate	O
priors	O
for	O
(	O
cid:22	O
)	O
and	O
(	O
cid:27	O
)	O
the	O
conjugate	B
prior	I
for	O
a	O
mean	B
(	O
cid:22	O
)	O
is	O
a	O
gaussian	O
:	O
we	O
introduce	O
two	O
‘	O
hy-	O
perparameters	O
’	O
,	O
(	O
cid:22	O
)	O
0	O
and	O
(	O
cid:27	O
)	O
(	O
cid:22	O
)	O
,	O
which	O
parameterize	O
the	O
prior	B
on	O
(	O
cid:22	O
)	O
,	O
and	O
write	O
p	O
(	O
(	O
cid:22	O
)	O
j	O
(	O
cid:22	O
)	O
0	O
;	O
(	O
cid:27	O
)	O
(	O
cid:22	O
)	O
)	O
=	O
normal	B
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:22	O
)	O
0	O
;	O
(	O
cid:27	O
)	O
2	O
in	O
the	O
limit	O
(	O
cid:22	O
)	O
0	O
=	O
0	O
,	O
(	O
cid:27	O
)	O
(	O
cid:22	O
)	O
!	O
1	O
,	O
we	O
obtain	O
(	O
cid:22	O
)	O
)	O
.	O
the	O
noninformative	B
prior	O
for	O
a	O
location	O
parameter	O
,	O
the	O
(	O
cid:13	O
)	O
at	O
prior	B
.	O
this	O
is	O
noninformative	B
because	O
it	O
is	O
invariant	O
under	O
the	O
natural	B
reparameterization	O
(	O
cid:22	O
)	O
0	O
=	O
(	O
cid:22	O
)	O
+	O
c.	O
the	O
prior	B
p	O
(	O
(	O
cid:22	O
)	O
)	O
=	O
const	O
:	O
is	O
also	O
an	O
improper	B
prior	O
,	O
that	O
is	O
,	O
it	O
is	O
not	O
normalizable	O
.	O
the	O
conjugate	B
prior	I
for	O
a	O
standard	B
deviation	I
(	O
cid:27	O
)	O
is	O
a	O
gamma	B
distribution	I
,	O
which	O
has	O
two	O
parameters	B
b	O
(	O
cid:12	O
)	O
and	O
c	O
(	O
cid:12	O
)	O
.	O
it	O
is	O
most	O
convenient	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
prior	B
319	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
320	O
24	O
|	O
exact	O
marginalization	O
density	B
of	O
the	O
inverse	O
variance	O
(	O
the	O
precision	B
parameter	O
)	O
(	O
cid:12	O
)	O
=	O
1=	O
(	O
cid:27	O
)	O
2	O
:	O
p	O
(	O
(	O
cid:12	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
(	O
(	O
cid:12	O
)	O
;	O
b	O
(	O
cid:12	O
)	O
;	O
c	O
(	O
cid:12	O
)	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
(	O
c	O
(	O
cid:12	O
)	O
)	O
(	O
cid:12	O
)	O
c	O
(	O
cid:12	O
)	O
(	O
cid:0	O
)	O
1	O
b	O
c	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
b	O
(	O
cid:12	O
)	O
(	O
cid:19	O
)	O
;	O
0	O
(	O
cid:20	O
)	O
(	O
cid:12	O
)	O
<	O
1	O
:	O
(	O
24.2	O
)	O
this	O
is	O
a	O
simple	O
peaked	O
distribution	B
with	O
mean	B
b	O
(	O
cid:12	O
)	O
c	O
(	O
cid:12	O
)	O
and	O
variance	O
b2	O
(	O
cid:12	O
)	O
c	O
(	O
cid:12	O
)	O
.	O
in	O
the	O
limit	O
b	O
(	O
cid:12	O
)	O
c	O
(	O
cid:12	O
)	O
=	O
1	O
;	O
c	O
(	O
cid:12	O
)	O
!	O
0	O
,	O
we	O
obtain	O
the	O
noninformative	B
prior	O
for	O
a	O
scale	O
parameter	O
,	O
the	O
1=	O
(	O
cid:27	O
)	O
prior	B
.	O
this	O
is	O
‘	O
noninformative	B
’	O
because	O
it	O
is	O
invariant	O
under	O
the	O
reparameterization	O
(	O
cid:27	O
)	O
0	O
=	O
c	O
(	O
cid:27	O
)	O
.	O
the	O
1=	O
(	O
cid:27	O
)	O
prior	B
is	O
less	O
strange-looking	O
if	O
we	O
examine	O
the	O
resulting	O
density	B
over	O
ln	O
(	O
cid:27	O
)	O
,	O
or	O
ln	O
(	O
cid:12	O
)	O
,	O
which	O
is	O
(	O
cid:13	O
)	O
at	O
.	O
this	O
is	O
the	O
prior	B
that	O
expresses	O
ignorance	B
about	O
(	O
cid:27	O
)	O
by	O
saying	O
‘	O
well	O
,	O
it	O
could	O
be	O
10	O
,	O
or	O
it	O
could	O
be	O
1	O
,	O
or	O
it	O
could	O
be	O
0.1	O
,	O
.	O
.	O
.	O
’	O
scale	O
variables	O
such	O
as	O
(	O
cid:27	O
)	O
are	O
usually	O
best	O
represented	O
in	O
terms	O
of	O
their	O
logarithm	O
.	O
again	O
,	O
this	O
noninformative	B
1=	O
(	O
cid:27	O
)	O
prior	B
is	O
improper	B
.	O
in	O
the	O
following	O
examples	O
,	O
i	O
will	O
use	O
the	O
improper	B
noninformative	O
priors	O
for	O
(	O
cid:22	O
)	O
and	O
(	O
cid:27	O
)	O
.	O
using	O
improper	B
priors	O
is	O
viewed	O
as	O
distasteful	O
in	O
some	O
circles	O
,	O
so	O
let	O
me	O
excuse	O
myself	O
by	O
saying	O
it	O
’	O
s	O
for	O
the	O
sake	O
of	O
readability	O
;	O
if	O
i	O
included	O
proper	B
priors	O
,	O
the	O
calculations	O
could	O
still	O
be	O
done	O
but	O
the	O
key	B
points	I
would	O
be	O
obscured	O
by	O
the	O
(	O
cid:13	O
)	O
ood	O
of	O
extra	O
parameters	B
.	O
maximum	B
likelihood	I
and	O
marginalization	B
:	O
(	O
cid:27	O
)	O
n	O
and	O
(	O
cid:27	O
)	O
n	O
(	O
cid:0	O
)	O
1	O
the	O
task	O
of	O
inferring	O
the	O
mean	B
and	O
standard	B
deviation	I
of	O
a	O
gaussian	O
distribu-	O
tion	O
from	O
n	O
samples	O
is	O
a	O
familiar	O
one	O
,	O
though	O
maybe	O
not	O
everyone	O
understands	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
the	O
(	O
cid:27	O
)	O
n	O
and	O
(	O
cid:27	O
)	O
n	O
(	O
cid:0	O
)	O
1	O
buttons	O
on	O
their	O
calculator	B
.	O
let	O
us	O
recap	O
the	O
formulae	O
,	O
then	O
derive	O
them	O
.	O
given	O
data	O
d	O
=	O
fxngn	O
n=1	O
,	O
an	O
‘	O
estimator	B
’	O
of	O
(	O
cid:22	O
)	O
is	O
reminder	O
:	O
when	O
we	O
change	O
variables	O
from	O
(	O
cid:27	O
)	O
to	O
l	O
(	O
(	O
cid:27	O
)	O
)	O
,	O
a	O
one-to-one	O
function	B
of	O
(	O
cid:27	O
)	O
,	O
the	O
probability	B
density	O
transforms	O
from	O
p	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
to	O
here	O
,	O
the	O
jacobian	O
is	O
:	O
@	O
(	O
cid:27	O
)	O
@	O
l	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
pl	O
(	O
l	O
)	O
=	O
p	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
@	O
ln	O
(	O
cid:27	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
@	O
(	O
cid:27	O
)	O
=	O
(	O
cid:27	O
)	O
:	O
and	O
two	O
estimators	O
of	O
(	O
cid:27	O
)	O
are	O
:	O
(	O
cid:27	O
)	O
n	O
(	O
cid:17	O
)	O
spn	O
n=1	O
(	O
xn	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
n	O
n=1	O
xn=n	O
;	O
(	O
cid:22	O
)	O
x	O
(	O
cid:17	O
)	O
pn	O
and	O
(	O
cid:27	O
)	O
n	O
(	O
cid:0	O
)	O
1	O
(	O
cid:17	O
)	O
spn	O
n=1	O
(	O
xn	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
n	O
(	O
cid:0	O
)	O
1	O
(	O
24.3	O
)	O
:	O
(	O
24.4	O
)	O
there	O
are	O
two	O
principal	O
paradigms	O
for	O
statistics	O
:	O
sampling	B
theory	I
and	O
bayesian	O
inference	B
.	O
in	O
sampling	O
theory	B
(	O
also	O
known	O
as	O
‘	O
frequentist	B
’	O
or	O
orthodox	O
statis-	O
tics	O
)	O
,	O
one	O
invents	O
estimators	O
of	O
quantities	O
of	O
interest	O
and	O
then	O
chooses	O
between	O
those	O
estimators	O
using	O
some	O
criterion	O
measuring	O
their	O
sampling	O
properties	O
;	O
there	O
is	O
no	O
clear	O
principle	O
for	O
deciding	O
which	O
criterion	O
to	O
use	O
to	O
measure	O
the	O
performance	O
of	O
an	O
estimator	B
;	O
nor	O
,	O
for	O
most	O
criteria	O
,	O
is	O
there	O
any	O
systematic	B
procedure	O
for	O
the	O
construction	B
of	O
optimal	B
estimators	O
.	O
in	O
bayesian	O
inference	B
,	O
in	O
contrast	O
,	O
once	O
we	O
have	O
made	O
explicit	O
all	O
our	O
assumptions	B
about	O
the	O
model	B
and	O
the	O
data	O
,	O
our	O
inferences	O
are	O
mechanical	O
.	O
whatever	O
question	O
we	O
wish	O
to	O
pose	O
,	O
the	O
rules	B
of	O
probability	B
theory	O
give	O
a	O
unique	O
answer	O
which	O
consistently	O
takes	O
into	O
account	O
all	O
the	O
given	O
information	B
.	O
human-designed	O
estimators	O
and	O
con	O
(	O
cid:12	O
)	O
dence	O
intervals	B
have	O
no	O
role	O
in	O
bayesian	O
inference	B
;	O
human	B
input	O
only	O
en-	O
ters	O
into	O
the	O
important	O
tasks	O
of	O
designing	O
the	O
hypothesis	O
space	O
(	O
that	O
is	O
,	O
the	O
speci	O
(	O
cid:12	O
)	O
cation	O
of	O
the	O
model	O
and	O
all	O
its	O
probability	B
distributions	I
)	O
,	O
and	O
(	O
cid:12	O
)	O
guring	O
out	O
how	O
to	O
do	O
the	O
computations	O
that	O
implement	O
inference	B
in	O
that	O
space	O
.	O
the	O
answers	O
to	O
our	O
questions	O
are	O
probability	B
distributions	I
over	O
the	O
quantities	O
of	O
interest	O
.	O
we	O
often	O
(	O
cid:12	O
)	O
nd	O
that	O
the	O
estimators	O
of	O
sampling	O
theory	B
emerge	O
auto-	O
matically	O
as	O
modes	O
or	O
means	O
of	O
these	O
posterior	O
distributions	O
when	O
we	O
choose	O
a	O
simple	O
hypothesis	O
space	O
and	O
turn	O
the	O
handle	O
of	O
bayesian	O
inference	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
24.1	O
:	O
inferring	O
the	O
mean	B
and	O
variance	B
of	O
a	O
gaussian	O
distribution	B
321	O
figure	O
24.1.	O
the	O
likelihood	B
function	O
for	O
the	O
parameters	B
of	O
a	O
gaussian	O
distribution	B
,	O
repeated	O
from	O
(	O
cid:12	O
)	O
gure	O
21.5	O
.	O
(	O
a1	O
,	O
a2	O
)	O
surface	O
plot	O
and	O
contour	O
plot	O
of	O
the	O
log	O
likelihood	B
as	O
a	O
function	B
of	O
(	O
cid:22	O
)	O
and	O
(	O
cid:27	O
)	O
.	O
the	O
data	B
set	I
of	O
n	O
=	O
5	O
points	O
had	O
mean	B
(	O
cid:22	O
)	O
x	O
=	O
1:0	O
and	O
s	O
=p	O
(	O
x	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
=	O
1:0.	O
notice	O
that	O
the	O
maximum	O
is	O
skew	O
in	O
(	O
cid:27	O
)	O
.	O
the	O
two	O
estimators	O
of	O
standard	O
deviation	O
have	O
values	O
(	O
cid:27	O
)	O
n	O
=	O
0:45	O
and	O
(	O
cid:27	O
)	O
n	O
(	O
cid:0	O
)	O
1	O
=	O
0:50	O
.	O
(	O
c	O
)	O
the	O
posterior	B
probability	I
of	O
(	O
cid:27	O
)	O
for	O
various	O
(	O
cid:12	O
)	O
xed	O
values	O
of	O
(	O
cid:22	O
)	O
(	O
shown	O
as	O
a	O
density	B
over	O
ln	O
(	O
cid:27	O
)	O
)	O
.	O
(	O
d	O
)	O
the	O
posterior	B
probability	I
of	O
(	O
cid:27	O
)	O
,	O
p	O
(	O
(	O
cid:27	O
)	O
j	O
d	O
)	O
,	O
assuming	O
a	O
(	O
cid:13	O
)	O
at	O
prior	B
on	O
(	O
cid:22	O
)	O
,	O
obtained	O
by	O
projecting	O
the	O
probability	B
mass	O
in	O
(	O
a	O
)	O
onto	O
the	O
(	O
cid:27	O
)	O
axis	O
.	O
the	O
maximum	O
of	O
p	O
(	O
(	O
cid:27	O
)	O
j	O
d	O
)	O
is	O
at	O
(	O
cid:27	O
)	O
n	O
(	O
cid:0	O
)	O
1.	O
by	O
contrast	O
,	O
the	O
maximum	O
of	O
p	O
(	O
(	O
cid:27	O
)	O
j	O
d	O
;	O
(	O
cid:22	O
)	O
=	O
(	O
cid:22	O
)	O
x	O
)	O
is	O
at	O
(	O
cid:27	O
)	O
n	O
.	O
(	O
both	O
probabilities	O
are	O
shows	O
as	O
densities	O
over	O
ln	O
(	O
cid:27	O
)	O
.	O
)	O
0.06	O
0.05	O
0.04	O
0.03	O
0.02	O
0.01	O
1	O
0	O
0.8	O
0.6	O
sigma	O
(	O
a1	O
)	O
0.4	O
0.2	O
0.09	O
0.08	O
0.07	O
0.06	O
0.05	O
0.04	O
0.03	O
0.02	O
0.01	O
0	O
0.2	O
(	O
c	O
)	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
sigma	O
0.5	O
1	O
mean	B
0	O
1.5	O
2	O
0	O
(	O
a2	O
)	O
0.5	O
1	O
1.5	O
2	O
mean	B
mu=1	O
mu=1.25	O
mu=1.5	O
p	O
(	O
sigma|d	O
,	O
mu=1	O
)	O
0.09	O
0.08	O
0.07	O
0.06	O
0.05	O
0.04	O
0.03	O
0.02	O
0.01	O
(	O
d	O
)	O
p	O
(	O
sigma|d	O
)	O
0.4	O
0.6	O
0.8	O
1	O
1.2	O
1.4	O
1.6	O
1.8	O
2	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
1.2	O
1.4	O
1.6	O
1.8	O
2	O
in	O
sampling	O
theory	B
,	O
the	O
estimators	O
above	O
can	O
be	O
motivated	O
as	O
follows	O
.	O
(	O
cid:22	O
)	O
x	O
is	O
an	O
unbiased	B
estimator	I
of	O
(	O
cid:22	O
)	O
which	O
,	O
out	O
of	O
all	O
the	O
possible	O
unbiased	O
estimators	O
of	O
(	O
cid:22	O
)	O
,	O
has	O
smallest	O
variance	B
(	O
where	O
this	O
variance	B
is	O
computed	O
by	O
averaging	O
over	O
an	O
ensemble	B
of	O
imaginary	O
experiments	O
in	O
which	O
the	O
data	O
samples	O
are	O
assumed	O
to	O
come	O
from	O
an	O
unknown	O
gaussian	O
distribution	B
)	O
.	O
the	O
estimator	B
(	O
(	O
cid:22	O
)	O
x	O
;	O
(	O
cid:27	O
)	O
n	O
)	O
is	O
the	O
maximum	B
likelihood	I
estimator	O
for	O
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
.	O
the	O
estimator	B
(	O
cid:27	O
)	O
n	O
is	O
biased	O
,	O
however	O
:	O
the	O
expectation	B
of	O
(	O
cid:27	O
)	O
n	O
,	O
given	O
(	O
cid:27	O
)	O
,	O
averaging	O
over	O
many	O
imagined	O
experiments	O
,	O
is	O
not	O
(	O
cid:27	O
)	O
.	O
exercise	O
24.1	O
.	O
[	O
2	O
,	O
p.323	O
]	O
give	O
an	O
intuitive	O
explanation	O
why	O
the	O
estimator	B
(	O
cid:27	O
)	O
n	O
is	O
biased	O
.	O
this	O
bias	B
motivates	O
the	O
invention	O
,	O
in	O
sampling	O
theory	B
,	O
of	O
(	O
cid:27	O
)	O
n	O
(	O
cid:0	O
)	O
1	O
,	O
which	O
can	O
be	O
shown	O
to	O
be	O
an	O
unbiased	B
estimator	I
.	O
or	O
to	O
be	O
precise	O
,	O
it	O
is	O
(	O
cid:27	O
)	O
2	O
n	O
(	O
cid:0	O
)	O
1	O
that	O
is	O
an	O
unbiased	B
estimator	I
of	O
(	O
cid:27	O
)	O
2.	O
we	O
now	O
look	O
at	O
some	O
bayesian	O
inferences	O
for	O
this	O
problem	O
,	O
assuming	O
non-	O
informative	O
priors	O
for	O
(	O
cid:22	O
)	O
and	O
(	O
cid:27	O
)	O
.	O
the	O
emphasis	O
is	O
thus	O
not	O
on	O
the	O
priors	O
,	O
but	O
rather	O
on	O
(	O
a	O
)	O
the	O
likelihood	B
function	O
,	O
and	O
(	O
b	O
)	O
the	O
concept	O
of	O
marginalization	O
.	O
the	O
joint	B
posterior	O
probability	O
of	O
(	O
cid:22	O
)	O
and	O
(	O
cid:27	O
)	O
is	O
proportional	O
to	O
the	O
likelihood	B
function	O
illustrated	O
by	O
a	O
contour	O
plot	O
in	O
(	O
cid:12	O
)	O
gure	O
24.1a	O
.	O
the	O
log	O
likelihood	B
is	O
:	O
ln	O
p	O
(	O
fxngn	O
n=1	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
n	O
ln	O
(	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
)	O
(	O
cid:0	O
)	O
xn	O
(	O
xn	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
)	O
2=	O
(	O
2	O
(	O
cid:27	O
)	O
2	O
)	O
;	O
=	O
(	O
cid:0	O
)	O
n	O
ln	O
(	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
)	O
(	O
cid:0	O
)	O
[	O
n	O
(	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
+	O
s	O
]	O
=	O
(	O
2	O
(	O
cid:27	O
)	O
2	O
)	O
;	O
(	O
24.5	O
)	O
(	O
24.6	O
)	O
where	O
s	O
(	O
cid:17	O
)	O
pn	O
(	O
xn	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2.	O
given	O
the	O
gaussian	O
model	B
,	O
the	O
likelihood	B
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
two	O
functions	B
of	O
the	O
data	O
(	O
cid:22	O
)	O
x	O
and	O
s	O
,	O
so	O
these	O
two	O
quantities	O
are	O
known	O
as	O
‘	O
su	O
(	O
cid:14	O
)	O
cient	O
statistics	O
’	O
.	O
the	O
posterior	B
probability	I
of	O
(	O
cid:22	O
)	O
and	O
(	O
cid:27	O
)	O
is	O
,	O
using	O
the	O
improper	B
priors	O
:	O
p	O
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
jfxngn	O
n=1	O
)	O
=	O
=	O
p	O
(	O
fxngn	O
n=1	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
p	O
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
p	O
(	O
fxngn	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
)	O
n=2	O
exp	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
n	O
(	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2+s	O
n=1	O
)	O
1	O
2	O
(	O
cid:27	O
)	O
2	O
n=1	O
)	O
p	O
(	O
fxngn	O
(	O
24.7	O
)	O
(	O
24.8	O
)	O
(	O
cid:17	O
)	O
1	O
(	O
cid:27	O
)	O
(	O
cid:22	O
)	O
1	O
(	O
cid:27	O
)	O
:	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
322	O
24	O
|	O
exact	O
marginalization	O
this	O
function	B
describes	O
the	O
answer	O
to	O
the	O
question	O
,	O
‘	O
given	O
the	O
data	O
,	O
and	O
the	O
noninformative	B
priors	O
,	O
what	O
might	O
(	O
cid:22	O
)	O
and	O
(	O
cid:27	O
)	O
be	O
?	O
’	O
it	O
may	O
be	O
of	O
interest	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
parameter	O
values	O
that	O
maximize	O
the	O
posterior	B
probability	I
,	O
though	O
it	O
should	O
be	O
emphasized	O
that	O
posterior	B
probability	I
maxima	O
have	O
no	O
fundamental	O
status	O
in	O
bayesian	O
inference	B
,	O
since	O
their	O
location	O
depends	O
on	O
the	O
choice	O
of	O
basis	O
.	O
here	O
we	O
choose	O
the	O
basis	O
(	O
(	O
cid:22	O
)	O
;	O
ln	O
(	O
cid:27	O
)	O
)	O
,	O
in	O
which	O
our	O
prior	B
is	O
(	O
cid:13	O
)	O
at	O
,	O
so	O
that	O
the	O
posterior	B
probability	I
maximum	O
coincides	O
with	O
the	O
maximum	O
of	O
the	O
likelihood	B
.	O
as	O
we	O
saw	O
in	O
exercise	O
22.4	O
(	O
p.302	O
)	O
,	O
the	O
maximum	B
likelihood	I
solution	O
for	O
(	O
cid:22	O
)	O
and	O
ln	O
(	O
cid:27	O
)	O
is	O
f	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
gml	O
=n	O
(	O
cid:22	O
)	O
x	O
;	O
(	O
cid:27	O
)	O
n	O
=ps=no	O
:	O
there	O
is	O
more	O
to	O
the	O
posterior	O
distribution	O
than	O
just	O
its	O
mode	O
.	O
as	O
can	O
be	O
seen	O
in	O
(	O
cid:12	O
)	O
gure	O
24.1a	O
,	O
the	O
likelihood	B
has	O
a	O
skew	O
peak	O
.	O
as	O
we	O
increase	O
(	O
cid:27	O
)	O
,	O
the	O
width	O
of	O
the	O
conditional	O
distribution	B
of	O
(	O
cid:22	O
)	O
increases	O
(	O
(	O
cid:12	O
)	O
gure	O
22.1b	O
)	O
.	O
and	O
if	O
we	O
(	O
cid:12	O
)	O
x	O
(	O
cid:22	O
)	O
to	O
a	O
sequence	B
of	O
values	O
moving	O
away	O
from	O
the	O
sample	B
mean	O
(	O
cid:22	O
)	O
x	O
,	O
we	O
obtain	O
a	O
sequence	B
of	O
conditional	B
distributions	O
over	O
(	O
cid:27	O
)	O
whose	O
maxima	O
move	O
to	O
increasing	O
values	O
of	O
(	O
cid:27	O
)	O
(	O
(	O
cid:12	O
)	O
gure	O
24.1c	O
)	O
.	O
the	O
posterior	B
probability	I
of	O
(	O
cid:22	O
)	O
given	O
(	O
cid:27	O
)	O
is	O
p	O
(	O
(	O
cid:22	O
)	O
jfxngn	O
n=1	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
p	O
(	O
fxngn	O
n=1	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
p	O
(	O
(	O
cid:22	O
)	O
)	O
p	O
(	O
fxngn	O
n=1	O
j	O
(	O
cid:27	O
)	O
)	O
/	O
exp	O
(	O
(	O
cid:0	O
)	O
n	O
(	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2=	O
(	O
2	O
(	O
cid:27	O
)	O
2	O
)	O
)	O
=	O
normal	B
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:22	O
)	O
x	O
;	O
(	O
cid:27	O
)	O
2=n	O
)	O
:	O
we	O
note	O
the	O
familiar	O
(	O
cid:27	O
)	O
=pn	O
scaling	B
of	O
the	O
error	B
bars	I
on	O
(	O
cid:22	O
)	O
.	O
(	O
24.9	O
)	O
(	O
24.10	O
)	O
(	O
24.11	O
)	O
let	O
us	O
now	O
ask	O
the	O
question	O
‘	O
given	O
the	O
data	O
,	O
and	O
the	O
noninformative	B
priors	O
,	O
what	O
might	O
(	O
cid:27	O
)	O
be	O
?	O
’	O
this	O
question	O
di	O
(	O
cid:11	O
)	O
ers	O
from	O
the	O
(	O
cid:12	O
)	O
rst	O
one	O
we	O
asked	O
in	O
that	O
we	O
are	O
now	O
not	O
interested	O
in	O
(	O
cid:22	O
)	O
.	O
this	O
parameter	O
must	O
therefore	O
be	O
marginalized	O
over	O
.	O
the	O
posterior	B
probability	I
of	O
(	O
cid:27	O
)	O
is	O
:	O
p	O
(	O
(	O
cid:27	O
)	O
jfxngn	O
n=1	O
)	O
=	O
p	O
(	O
fxngn	O
n=1	O
j	O
(	O
cid:27	O
)	O
)	O
p	O
(	O
(	O
cid:27	O
)	O
)	O
p	O
(	O
fxngn	O
n=1	O
)	O
:	O
(	O
24.12	O
)	O
the	O
data-dependent	O
term	O
p	O
(	O
fxngn	O
n=1	O
j	O
(	O
cid:27	O
)	O
)	O
appeared	O
earlier	O
as	O
the	O
normalizing	B
constant	I
in	O
equation	O
(	O
24.9	O
)	O
;	O
one	O
name	O
for	O
this	O
quantity	O
is	O
the	O
‘	O
evidence	B
’	O
,	O
or	O
marginal	B
likelihood	I
,	O
for	O
(	O
cid:27	O
)	O
.	O
we	O
obtain	O
the	O
evidence	B
for	O
(	O
cid:27	O
)	O
by	O
integrating	O
out	O
(	O
cid:22	O
)	O
;	O
a	O
noninformative	B
prior	O
p	O
(	O
(	O
cid:22	O
)	O
)	O
=	O
constant	O
is	O
assumed	O
;	O
we	O
call	O
this	O
constant	O
1=	O
(	O
cid:27	O
)	O
(	O
cid:22	O
)	O
,	O
so	O
that	O
we	O
can	O
think	O
of	O
the	O
prior	O
as	O
a	O
top-hat	O
prior	B
of	O
width	O
(	O
cid:27	O
)	O
(	O
cid:22	O
)	O
.	O
the	O
gaussian	O
integral	B
,	O
p	O
(	O
fxngn	O
n=1	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
p	O
(	O
(	O
cid:22	O
)	O
)	O
d	O
(	O
cid:22	O
)	O
;	O
yields	O
:	O
n=1	O
j	O
(	O
cid:27	O
)	O
)	O
=r	O
p	O
(	O
fxngn	O
ln	O
p	O
(	O
fxngn	O
n=1	O
j	O
(	O
cid:27	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
n	O
ln	O
(	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
)	O
(	O
cid:0	O
)	O
s	O
2	O
(	O
cid:27	O
)	O
2	O
+	O
ln	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
=pn	O
(	O
cid:27	O
)	O
(	O
cid:22	O
)	O
:	O
(	O
24.13	O
)	O
the	O
(	O
cid:12	O
)	O
rst	O
two	O
terms	O
are	O
the	O
best-	O
(	O
cid:12	O
)	O
t	O
log	O
likelihood	B
(	O
i.e.	O
,	O
the	O
log	O
likelihood	B
with	O
(	O
cid:22	O
)	O
=	O
(	O
cid:22	O
)	O
x	O
)	O
.	O
the	O
last	O
term	O
is	O
the	O
log	O
of	O
the	O
occam	O
factor	O
which	O
penalizes	O
smaller	O
values	O
of	O
(	O
cid:27	O
)	O
.	O
(	O
we	O
will	O
discuss	O
occam	O
factors	O
more	O
in	O
chapter	O
28	O
.	O
)	O
when	O
we	O
di	O
(	O
cid:11	O
)	O
erentiate	O
the	O
log	O
evidence	B
with	O
respect	O
to	O
ln	O
(	O
cid:27	O
)	O
,	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
most	O
probable	O
(	O
cid:27	O
)	O
,	O
the	O
additional	O
volume	B
factor	O
(	O
(	O
cid:27	O
)	O
=pn	O
)	O
shifts	O
the	O
maximum	O
from	O
(	O
cid:27	O
)	O
n	O
to	O
(	O
cid:27	O
)	O
n	O
(	O
cid:0	O
)	O
1	O
=ps=	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
:	O
(	O
24.14	O
)	O
intuitively	O
,	O
the	O
denominator	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
counts	O
the	O
number	O
of	O
noise	O
measurements	O
contained	O
in	O
the	O
quantity	O
s	O
=	O
pn	O
(	O
xn	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2.	O
the	O
sum	O
contains	O
n	O
residuals	O
squared	O
,	O
but	O
there	O
are	O
only	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
e	O
(	O
cid:11	O
)	O
ective	O
noise	B
measurements	O
because	O
the	O
determination	O
of	O
one	O
parameter	O
(	O
cid:22	O
)	O
from	O
the	O
data	O
causes	O
one	O
dimension	O
of	O
noise	O
to	O
be	O
gobbled	O
up	O
in	O
unavoidable	O
over	O
(	O
cid:12	O
)	O
tting	O
.	O
in	O
the	O
terminology	B
of	O
classical	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
24.2	O
:	O
exercises	O
323	O
statistics	O
,	O
the	O
bayesian	O
’	O
s	O
best	O
guess	O
for	O
(	O
cid:27	O
)	O
sets	O
(	O
cid:31	O
)	O
2	O
(	O
the	O
measure	O
of	O
deviance	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
(	O
cid:31	O
)	O
2	O
(	O
cid:17	O
)	O
pn	O
(	O
xn	O
(	O
cid:0	O
)	O
^	O
(	O
cid:22	O
)	O
)	O
2=^	O
(	O
cid:27	O
)	O
2	O
)	O
equal	O
to	O
the	O
number	O
of	O
degrees	O
of	O
freedom	O
,	O
n	O
(	O
cid:0	O
)	O
1.	O
figure	O
24.1d	O
shows	O
the	O
posterior	B
probability	I
of	O
(	O
cid:27	O
)	O
,	O
which	O
is	O
proportional	O
to	O
the	O
marginal	B
likelihood	I
.	O
this	O
may	O
be	O
contrasted	O
with	O
the	O
posterior	O
prob-	O
ability	O
of	O
(	O
cid:27	O
)	O
with	O
(	O
cid:22	O
)	O
(	O
cid:12	O
)	O
xed	O
to	O
its	O
most	O
probable	O
value	O
,	O
(	O
cid:22	O
)	O
x	O
=	O
1	O
,	O
which	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
24.1c	O
and	O
d.	O
the	O
(	O
cid:12	O
)	O
nal	O
inference	B
we	O
might	O
wish	O
to	O
make	O
is	O
‘	O
given	O
the	O
data	O
,	O
what	O
is	O
(	O
cid:22	O
)	O
?	O
’	O
.	O
exercise	O
24.2	O
.	O
[	O
3	O
]	O
marginalize	O
over	O
(	O
cid:27	O
)	O
and	O
obtain	O
the	O
posterior	O
marginal	O
distri-	O
bution	O
of	O
(	O
cid:22	O
)	O
,	O
which	O
is	O
a	O
student-t	O
distribution	B
:	O
p	O
(	O
(	O
cid:22	O
)	O
j	O
d	O
)	O
/	O
1=	O
(	O
cid:0	O
)	O
n	O
(	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
+	O
s	O
(	O
cid:1	O
)	O
n=2	O
:	O
(	O
24.15	O
)	O
further	O
reading	O
a	O
bible	O
of	O
exact	O
marginalization	B
is	O
bretthorst	O
’	O
s	O
(	O
1988	O
)	O
book	O
on	O
bayesian	O
spec-	O
trum	O
analysis	B
and	O
parameter	O
estimation	O
.	O
24.2	O
exercises	O
.	O
exercise	O
24.3	O
.	O
[	O
3	O
]	O
[	O
this	O
exercise	O
requires	O
macho	B
integration	O
capabilities	O
.	O
]	O
give	O
a	O
bayesian	O
solution	O
to	O
exercise	O
22.15	O
(	O
p.309	O
)	O
,	O
where	O
seven	O
scientists	B
of	O
varying	O
capabilities	O
have	O
measured	O
(	O
cid:22	O
)	O
with	O
personal	O
noise	B
levels	O
(	O
cid:27	O
)	O
n	O
,	O
and	O
we	O
are	O
interested	O
in	O
inferring	O
(	O
cid:22	O
)	O
.	O
let	O
the	O
prior	B
on	O
each	O
(	O
cid:27	O
)	O
n	O
be	O
a	O
broad	O
prior	B
,	O
for	O
example	O
a	O
gamma	B
distribution	I
with	O
parameters	B
(	O
s	O
;	O
c	O
)	O
=	O
(	O
10	O
;	O
0:1	O
)	O
.	O
find	O
the	O
posterior	O
distribution	O
of	O
(	O
cid:22	O
)	O
.	O
plot	O
it	O
,	O
and	O
explore	O
its	O
properties	O
for	O
a	O
variety	O
of	O
data	O
sets	O
such	O
as	O
the	O
one	O
given	O
,	O
and	O
the	O
data	B
set	I
fxng	O
=	O
f13:01	O
;	O
7:39g	O
.	O
[	O
hint	O
:	O
(	O
cid:12	O
)	O
rst	O
(	O
cid:12	O
)	O
nd	O
the	O
posterior	O
distribution	O
of	O
(	O
cid:27	O
)	O
n	O
given	O
(	O
cid:22	O
)	O
and	O
xn	O
,	O
p	O
(	O
(	O
cid:27	O
)	O
n	O
j	O
xn	O
;	O
(	O
cid:22	O
)	O
)	O
.	O
note	O
that	O
the	O
normalizing	B
constant	I
for	O
this	O
inference	B
is	O
p	O
(	O
xn	O
j	O
(	O
cid:22	O
)	O
)	O
.	O
marginalize	O
over	O
(	O
cid:27	O
)	O
n	O
to	O
(	O
cid:12	O
)	O
nd	O
this	O
normalizing	B
constant	I
,	O
then	O
use	O
bayes	O
’	O
theorem	B
a	O
second	O
time	O
to	O
(	O
cid:12	O
)	O
nd	O
p	O
(	O
(	O
cid:22	O
)	O
jfxng	O
)	O
.	O
]	O
24.3	O
solutions	O
solution	O
to	O
exercise	O
24.1	O
(	O
p.321	O
)	O
.	O
1.	O
the	O
data	O
points	O
are	O
distributed	O
with	O
mean	O
squared	O
deviation	O
(	O
cid:27	O
)	O
2	O
about	O
the	O
true	O
mean	B
.	O
2.	O
the	O
sample	B
mean	O
is	O
unlikely	O
to	O
exactly	O
equal	O
the	O
true	O
mean	B
.	O
3.	O
the	O
sample	B
mean	O
is	O
the	O
value	O
of	O
(	O
cid:22	O
)	O
that	O
minimizes	O
the	O
sum	O
squared	O
deviation	O
of	O
the	O
data	O
points	O
from	O
(	O
cid:22	O
)	O
.	O
any	O
other	O
value	O
of	O
(	O
cid:22	O
)	O
(	O
in	O
particular	O
,	O
the	O
true	O
value	O
of	O
(	O
cid:22	O
)	O
)	O
will	O
have	O
a	O
larger	O
value	O
of	O
the	O
sum-squared	O
deviation	O
that	O
(	O
cid:22	O
)	O
=	O
(	O
cid:22	O
)	O
x.	O
so	O
the	O
expected	O
mean	B
squared	O
deviation	O
from	O
the	O
sample	B
mean	O
is	O
neces-	O
sarily	O
smaller	O
than	O
the	O
mean	B
squared	O
deviation	O
(	O
cid:27	O
)	O
2	O
about	O
the	O
true	O
mean	B
.	O
a	O
b	O
c	O
d-g	O
-30	O
-20	O
-10	O
0	O
10	O
20	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
25	O
exact	O
marginalization	O
in	O
trellises	O
in	O
this	O
chapter	O
we	O
will	O
discuss	O
a	O
few	O
exact	O
methods	O
that	O
are	O
used	O
in	O
proba-	O
bilistic	O
modelling	B
.	O
as	O
an	O
example	O
we	O
will	O
discuss	O
the	O
task	O
of	O
decoding	O
a	O
linear	B
error-correcting	O
code	B
.	O
we	O
will	O
see	O
that	O
inferences	O
can	O
be	O
conducted	O
most	O
e	O
(	O
cid:14	O
)	O
-	O
ciently	O
by	O
message-passing	O
algorithms	B
,	O
which	O
take	O
advantage	O
of	O
the	O
graphical	O
structure	O
of	O
the	O
problem	O
to	O
avoid	O
unnecessary	O
duplication	O
of	O
computations	O
(	O
see	O
chapter	O
16	O
)	O
.	O
25.1	O
decoding	B
problems	O
a	O
codeword	B
t	O
is	O
selected	O
from	O
a	O
linear	B
(	O
n	O
;	O
k	O
)	O
code	B
c	O
,	O
and	O
it	O
is	O
transmitted	O
over	O
a	O
noisy	B
channel	I
;	O
the	O
received	O
signal	O
is	O
y.	O
in	O
this	O
chapter	O
we	O
will	O
assume	O
that	O
the	O
channel	B
is	O
a	O
memoryless	O
channel	B
such	O
as	O
a	O
gaussian	O
channel	B
.	O
given	O
an	O
assumed	O
channel	B
model	O
p	O
(	O
y	O
j	O
t	O
)	O
,	O
there	O
are	O
two	O
decoding	B
problems	O
.	O
the	O
codeword	B
decoding	O
problem	O
is	O
the	O
task	O
of	O
inferring	O
which	O
codeword	B
t	O
was	O
transmitted	O
given	O
the	O
received	O
signal	O
.	O
the	O
bitwise	B
decoding	O
problem	O
is	O
the	O
task	O
of	O
inferring	O
for	O
each	O
transmit-	O
ted	O
bit	B
tn	O
how	O
likely	O
it	O
is	O
that	O
that	O
bit	B
was	O
a	O
one	O
rather	O
than	O
a	O
zero	O
.	O
as	O
a	O
concrete	O
example	O
,	O
take	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
.	O
in	O
chapter	O
1	O
,	O
we	O
discussed	O
the	O
codeword	B
decoding	O
problem	O
for	O
that	O
code	B
,	O
assuming	O
a	O
binary	B
symmetric	I
channel	I
.	O
we	O
didn	O
’	O
t	O
discuss	O
the	O
bitwise	B
decoding	O
problem	O
and	O
we	O
didn	O
’	O
t	O
discuss	O
how	O
to	O
handle	O
more	O
general	O
channel	O
models	O
such	O
as	O
a	O
gaussian	O
channel	B
.	O
solving	O
the	O
codeword	B
decoding	O
problem	O
by	O
bayes	O
’	O
theorem	B
,	O
the	O
posterior	B
probability	I
of	O
the	O
codeword	B
t	O
is	O
p	O
(	O
tj	O
y	O
)	O
=	O
p	O
(	O
y	O
j	O
t	O
)	O
p	O
(	O
t	O
)	O
p	O
(	O
y	O
)	O
:	O
(	O
25.1	O
)	O
likelihood	B
function	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
factor	O
in	O
the	O
numerator	O
,	O
p	O
(	O
y	O
j	O
t	O
)	O
,	O
is	O
the	O
likeli-	O
hood	O
of	O
the	O
codeword	O
,	O
which	O
,	O
for	O
any	O
memoryless	O
channel	B
,	O
is	O
a	O
separable	O
function	B
,	O
p	O
(	O
y	O
j	O
t	O
)	O
=	O
n	O
yn=1	O
p	O
(	O
yn	O
j	O
tn	O
)	O
:	O
(	O
25.2	O
)	O
for	O
example	O
,	O
if	O
the	O
channel	B
is	O
a	O
gaussian	O
channel	O
with	O
transmissions	O
(	O
cid:6	O
)	O
x	O
and	O
additive	O
noise	B
of	O
standard	B
deviation	I
(	O
cid:27	O
)	O
,	O
then	O
the	O
probability	B
density	O
324	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
25.1	O
:	O
decoding	B
problems	O
325	O
of	O
the	O
received	O
signal	O
yn	O
in	O
the	O
two	O
cases	O
tn	O
=	O
0	O
;	O
1	O
is	O
p	O
(	O
yn	O
j	O
tn	O
=	O
1	O
)	O
=	O
p	O
(	O
yn	O
j	O
tn	O
=	O
0	O
)	O
=	O
1	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
1	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:19	O
)	O
(	O
yn	O
(	O
cid:0	O
)	O
x	O
)	O
2	O
2	O
(	O
cid:27	O
)	O
2	O
(	O
cid:19	O
)	O
:	O
(	O
yn	O
+	O
x	O
)	O
2	O
(	O
25.3	O
)	O
(	O
25.4	O
)	O
from	O
the	O
point	O
of	O
view	O
of	O
decoding	O
,	O
all	O
that	O
matters	O
is	O
the	O
likelihood	B
ratio	O
,	O
which	O
for	O
the	O
case	O
of	O
the	O
gaussian	O
channel	B
is	O
p	O
(	O
yn	O
j	O
tn	O
=	O
1	O
)	O
p	O
(	O
yn	O
j	O
tn	O
=	O
0	O
)	O
=	O
exp	O
(	O
cid:18	O
)	O
2xyn	O
(	O
cid:27	O
)	O
2	O
(	O
cid:19	O
)	O
:	O
(	O
25.5	O
)	O
exercise	O
25.1	O
.	O
[	O
2	O
]	O
show	O
that	O
from	O
the	O
point	O
of	O
view	O
of	O
decoding	O
,	O
a	O
gaussian	O
channel	B
is	O
equivalent	O
to	O
a	O
time-varying	O
binary	B
symmetric	I
channel	I
with	O
a	O
known	O
noise	B
level	O
fn	O
which	O
depends	O
on	O
n.	O
prior	B
.	O
the	O
second	O
factor	O
in	O
the	O
numerator	O
is	O
the	O
prior	B
probability	O
of	O
the	O
codeword	O
,	O
p	O
(	O
t	O
)	O
,	O
which	O
is	O
usually	O
assumed	O
to	O
be	O
uniform	O
over	O
all	O
valid	O
codewords	O
.	O
the	O
denominator	O
in	O
(	O
25.1	O
)	O
is	O
the	O
normalizing	B
constant	I
p	O
(	O
y	O
)	O
=xt	O
p	O
(	O
y	O
j	O
t	O
)	O
p	O
(	O
t	O
)	O
:	O
(	O
25.6	O
)	O
the	O
complete	O
solution	O
to	O
the	O
codeword	B
decoding	O
problem	O
is	O
a	O
list	O
of	O
all	O
codewords	O
and	O
their	O
probabilities	O
as	O
given	O
by	O
equation	O
(	O
25.1	O
)	O
.	O
since	O
the	O
num-	O
ber	O
of	O
codewords	O
in	O
a	O
linear	B
code	O
,	O
2k	O
,	O
is	O
often	O
very	O
large	O
,	O
and	O
since	O
we	O
are	O
not	O
interested	O
in	O
knowing	O
the	O
detailed	O
probabilities	O
of	O
all	O
the	O
codewords	O
,	O
we	O
often	O
restrict	O
attention	O
to	O
a	O
simpli	O
(	O
cid:12	O
)	O
ed	O
version	O
of	O
the	O
codeword	O
decoding	B
problem	O
.	O
the	O
map	O
codeword	B
decoding	O
problem	O
is	O
the	O
task	O
of	O
identifying	O
the	O
most	O
probable	O
codeword	O
t	O
given	O
the	O
received	O
signal	O
.	O
if	O
the	O
prior	B
probability	O
over	O
codewords	O
is	O
uniform	O
then	O
this	O
task	O
is	O
iden-	O
tical	O
to	O
the	O
problem	O
of	O
maximum	O
likelihood	B
decoding	O
,	O
that	O
is	O
,	O
identifying	O
the	O
codeword	B
that	O
maximizes	O
p	O
(	O
y	O
j	O
t	O
)	O
.	O
example	O
:	O
in	O
chapter	O
1	O
,	O
for	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
and	O
a	O
binary	B
symmetric	I
channel	I
we	O
discussed	O
a	O
method	B
for	O
deducing	O
the	O
most	O
probable	O
codeword	O
from	O
the	O
syndrome	B
of	O
the	O
received	O
signal	O
,	O
thus	O
solving	O
the	O
map	O
codeword	B
decoding	O
problem	O
for	O
that	O
case	O
.	O
we	O
would	O
like	O
a	O
more	O
general	O
solution	O
.	O
the	O
map	O
codeword	B
decoding	O
problem	O
can	O
be	O
solved	O
in	O
exponential	O
time	O
(	O
of	O
order	O
2k	O
)	O
by	O
searching	O
through	O
all	O
codewords	O
for	O
the	O
one	O
that	O
maximizes	O
p	O
(	O
y	O
j	O
t	O
)	O
p	O
(	O
t	O
)	O
.	O
but	O
we	O
are	O
interested	O
in	O
methods	O
that	O
are	O
more	O
e	O
(	O
cid:14	O
)	O
cient	O
than	O
this	O
.	O
in	O
section	O
25.3	O
,	O
we	O
will	O
discuss	O
an	O
exact	O
method	O
known	O
as	O
the	O
min	O
{	O
sum	O
algorithm	O
which	O
may	O
be	O
able	O
to	O
solve	O
the	O
codeword	B
decoding	O
problem	O
more	O
e	O
(	O
cid:14	O
)	O
ciently	O
;	O
how	O
much	O
more	O
e	O
(	O
cid:14	O
)	O
ciently	O
depends	O
on	O
the	O
properties	O
of	O
the	O
code	O
.	O
it	O
is	O
worth	O
emphasizing	O
that	O
map	O
codeword	B
decoding	O
for	O
a	O
general	O
lin-	O
ear	O
code	B
is	O
known	O
to	O
be	O
np-complete	O
(	O
which	O
means	O
in	O
layman	O
’	O
s	O
terms	O
that	O
map	O
codeword	B
decoding	O
has	O
a	O
complexity	B
that	O
scales	O
exponentially	O
with	O
the	O
blocklength	O
,	O
unless	O
there	O
is	O
a	O
revolution	O
in	O
computer	O
science	O
)	O
.	O
so	O
restrict-	O
ing	O
attention	O
to	O
the	O
map	O
decoding	B
problem	O
hasn	O
’	O
t	O
necessarily	O
made	O
the	O
task	O
much	O
less	O
challenging	O
;	O
it	O
simply	O
makes	O
the	O
answer	O
briefer	O
to	O
report	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
326	O
25	O
|	O
exact	O
marginalization	O
in	O
trellises	O
solving	O
the	O
bitwise	B
decoding	O
problem	O
formally	O
,	O
the	O
exact	O
solution	O
of	O
the	O
bitwise	O
decoding	B
problem	O
is	O
obtained	O
from	O
equation	O
(	O
25.1	O
)	O
by	O
marginalizing	O
over	O
the	O
other	O
bits	O
.	O
p	O
(	O
tn	O
j	O
y	O
)	O
=	O
xftn0	O
:	O
n06=ng	O
p	O
(	O
tj	O
y	O
)	O
:	O
(	O
25.7	O
)	O
we	O
can	O
also	O
write	O
this	O
marginal	B
with	O
the	O
aid	O
of	O
a	O
truth	B
function	I
	O
[	O
s	O
]	O
that	O
is	O
one	O
if	O
the	O
proposition	O
s	O
is	O
true	O
and	O
zero	O
otherwise	O
.	O
p	O
(	O
tn	O
=	O
1j	O
y	O
)	O
=	O
xt	O
p	O
(	O
tn	O
=	O
0j	O
y	O
)	O
=	O
xt	O
p	O
(	O
tj	O
y	O
)	O
	O
[	O
tn	O
=	O
1	O
]	O
p	O
(	O
tj	O
y	O
)	O
	O
[	O
tn	O
=	O
0	O
]	O
:	O
(	O
25.8	O
)	O
(	O
25.9	O
)	O
computing	O
these	O
marginal	B
probabilities	O
by	O
an	O
explicit	O
sum	O
over	O
all	O
codewords	O
t	O
takes	O
exponential	B
time	O
.	O
but	O
,	O
for	O
certain	O
codes	O
,	O
the	O
bitwise	B
decoding	O
problem	O
can	O
be	O
solved	O
much	O
more	O
e	O
(	O
cid:14	O
)	O
ciently	O
using	O
the	O
forward	O
{	O
backward	O
algorithm	O
.	O
we	O
will	O
describe	O
this	O
algorithm	B
,	O
which	O
is	O
an	O
example	O
of	O
the	O
sum	O
{	O
product	O
algorithm	O
,	O
in	O
a	O
moment	O
.	O
both	O
the	O
min	O
{	O
sum	O
algorithm	O
and	O
the	O
sum	O
{	O
product	O
algorithm	O
have	O
widespread	O
importance	O
,	O
and	O
have	O
been	O
invented	O
many	O
times	O
in	O
many	O
(	O
cid:12	O
)	O
elds	O
.	O
25.2	O
codes	O
and	O
trellises	O
in	O
chapters	O
1	O
and	O
11	O
,	O
we	O
represented	O
linear	B
(	O
n	O
;	O
k	O
)	O
codes	O
in	O
terms	O
of	O
their	O
generator	O
matrices	O
and	O
their	O
parity-check	O
matrices	O
.	O
in	O
the	O
case	O
of	O
a	O
systematic	B
block	O
code	B
,	O
the	O
(	O
cid:12	O
)	O
rst	O
k	O
transmitted	O
bits	O
in	O
each	O
block	B
of	O
size	O
n	O
are	O
the	O
source	O
bits	O
,	O
and	O
the	O
remaining	O
m	O
=	O
n	O
(	O
cid:0	O
)	O
k	O
bits	O
are	O
the	O
parity-check	B
bits	I
.	O
this	O
means	O
that	O
the	O
generator	B
matrix	I
of	O
the	O
code	B
can	O
be	O
written	O
(	O
a	O
)	O
(	O
b	O
)	O
repetition	B
code	I
r3	O
simple	B
parity	I
code	O
p3	O
p	O
(	O
cid:21	O
)	O
;	O
gt	O
=	O
(	O
cid:20	O
)	O
ik	O
and	O
the	O
parity-check	B
matrix	I
can	O
be	O
written	O
h	O
=	O
(	O
cid:2	O
)	O
p	O
im	O
(	O
cid:3	O
)	O
;	O
(	O
25.10	O
)	O
(	O
c	O
)	O
(	O
25.11	O
)	O
where	O
p	O
is	O
an	O
m	O
(	O
cid:2	O
)	O
k	O
matrix	B
.	O
in	O
this	O
section	B
we	O
will	O
study	O
another	O
representation	O
of	O
a	O
linear	B
code	O
called	O
a	O
trellis	B
.	O
the	O
codes	O
that	O
these	O
trellises	O
represent	O
will	O
not	O
in	O
general	O
be	O
systematic	B
codes	O
,	O
but	O
they	O
can	O
be	O
mapped	O
onto	O
systematic	B
codes	O
if	O
desired	O
by	O
a	O
reordering	O
of	O
the	O
bits	O
in	O
a	O
block	B
.	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
figure	O
25.1.	O
examples	O
of	O
trellises	O
.	O
each	O
edge	B
in	O
a	O
trellis	B
is	O
labelled	O
by	O
a	O
zero	O
(	O
shown	O
by	O
a	O
square	B
)	O
or	O
a	O
one	O
(	O
shown	O
by	O
a	O
cross	O
)	O
.	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
a	O
trellis	B
our	O
de	O
(	O
cid:12	O
)	O
nition	O
will	O
be	O
quite	O
narrow	O
.	O
for	O
a	O
more	O
comprehensive	O
view	O
of	O
trellises	O
,	O
the	O
reader	O
should	O
consult	O
kschischang	O
and	O
sorokine	O
(	O
1995	O
)	O
.	O
a	O
trellis	B
is	O
a	O
graph	B
consisting	O
of	O
nodes	O
(	O
also	O
known	O
as	O
states	O
or	O
vertices	O
)	O
and	O
edges	O
.	O
the	O
nodes	O
are	O
grouped	O
into	O
vertical	O
slices	O
called	O
times	O
,	O
and	O
the	O
times	O
are	O
ordered	O
such	O
that	O
each	O
edge	B
connects	O
a	O
node	O
in	O
one	O
time	O
to	O
a	O
node	O
in	O
a	O
neighbouring	O
time	O
.	O
every	O
edge	B
is	O
labelled	O
with	O
a	O
symbol	O
.	O
the	O
leftmost	O
and	O
rightmost	O
states	O
contain	O
only	O
one	O
node	O
.	O
apart	O
from	O
these	O
two	O
extreme	O
nodes	O
,	O
all	O
nodes	O
in	O
the	O
trellis	B
have	O
at	O
least	O
one	O
edge	B
connecting	O
leftwards	O
and	O
at	O
least	O
one	O
connecting	O
rightwards	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
25.3	O
:	O
solving	O
the	O
decoding	B
problems	O
on	O
a	O
trellis	B
327	O
a	O
trellis	B
with	O
n	O
+	O
1	O
times	O
de	O
(	O
cid:12	O
)	O
nes	O
a	O
code	B
of	O
blocklength	O
n	O
as	O
follows	O
:	O
a	O
codeword	B
is	O
obtained	O
by	O
taking	O
a	O
path	O
that	O
crosses	O
the	O
trellis	B
from	O
left	O
to	O
right	O
and	O
reading	O
out	O
the	O
symbols	O
on	O
the	O
edges	O
that	O
are	O
traversed	O
.	O
each	O
valid	O
path	O
through	O
the	O
trellis	B
de	O
(	O
cid:12	O
)	O
nes	O
a	O
codeword	B
.	O
we	O
will	O
number	O
the	O
leftmost	O
time	O
‘	O
time	O
0	O
’	O
and	O
the	O
rightmost	O
‘	O
time	O
n	O
’	O
.	O
we	O
will	O
number	O
the	O
leftmost	O
state	O
‘	O
state	O
0	O
’	O
and	O
the	O
rightmost	O
‘	O
state	O
i	O
’	O
,	O
where	O
i	O
is	O
the	O
total	O
number	O
of	O
states	O
(	O
vertices	O
)	O
in	O
the	O
trellis	B
.	O
the	O
nth	O
bit	B
of	O
the	O
codeword	B
is	O
emitted	O
as	O
we	O
move	O
from	O
time	O
n	O
(	O
cid:0	O
)	O
1	O
to	O
time	O
n.	O
the	O
width	O
of	O
the	O
trellis	O
at	O
a	O
given	O
time	O
is	O
the	O
number	O
of	O
nodes	O
in	O
that	O
time	O
.	O
the	O
maximal	O
width	O
of	O
a	O
trellis	B
is	O
what	O
it	O
sounds	O
like	O
.	O
a	O
trellis	B
is	O
called	O
a	O
linear	B
trellis	O
if	O
the	O
code	B
it	O
de	O
(	O
cid:12	O
)	O
nes	O
is	O
a	O
linear	B
code	O
.	O
we	O
will	O
solely	O
be	O
concerned	O
with	O
linear	O
trellises	O
from	O
now	O
on	O
,	O
as	O
nonlinear	O
trellises	O
are	O
much	O
more	O
complex	B
beasts	O
.	O
for	O
brevity	O
,	O
we	O
will	O
only	O
discuss	O
binary	O
trellises	O
,	O
that	O
is	O
,	O
trellises	O
whose	O
edges	O
are	O
labelled	O
with	O
zeroes	O
and	O
ones	O
.	O
it	O
is	O
not	O
hard	O
to	O
generalize	O
the	O
methods	B
that	O
follow	O
to	O
q-ary	O
trellises	O
.	O
figures	O
25.1	O
(	O
a	O
{	O
c	O
)	O
show	O
the	O
trellises	O
corresponding	O
to	O
the	O
repetition	B
code	I
r3	O
which	O
has	O
(	O
n	O
;	O
k	O
)	O
=	O
(	O
3	O
;	O
1	O
)	O
;	O
the	O
parity	B
code	O
p3	O
with	O
(	O
n	O
;	O
k	O
)	O
=	O
(	O
3	O
;	O
2	O
)	O
;	O
and	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
.	O
.	O
exercise	O
25.2	O
.	O
[	O
2	O
]	O
con	O
(	O
cid:12	O
)	O
rm	O
that	O
the	O
sixteen	O
codewords	O
listed	O
in	O
table	O
1.14	O
are	O
generated	O
by	O
the	O
trellis	B
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
25.1c	O
.	O
observations	O
about	O
linear	B
trellises	O
for	O
any	O
linear	B
code	O
the	O
minimal	O
trellis	B
is	O
the	O
one	O
that	O
has	O
the	O
smallest	O
number	O
of	O
nodes	O
.	O
in	O
a	O
minimal	O
trellis	B
,	O
each	O
node	O
has	O
at	O
most	O
two	O
edges	O
entering	O
it	O
and	O
at	O
most	O
two	O
edges	O
leaving	O
it	O
.	O
all	O
nodes	O
in	O
a	O
time	O
have	O
the	O
same	O
left	O
degree	B
as	O
each	O
other	O
and	O
they	O
have	O
the	O
same	O
right	O
degree	B
as	O
each	O
other	O
.	O
the	O
width	O
is	O
always	O
a	O
power	O
of	O
two	O
.	O
a	O
minimal	O
trellis	B
for	O
a	O
linear	B
(	O
n	O
;	O
k	O
)	O
code	B
can	O
not	O
have	O
a	O
width	O
greater	O
than	O
2k	O
since	O
every	O
node	O
has	O
at	O
least	O
one	O
valid	O
codeword	B
through	O
it	O
,	O
and	O
there	O
are	O
only	O
2k	O
codewords	O
.	O
furthermore	O
,	O
if	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
m	O
=	O
n	O
(	O
cid:0	O
)	O
k	O
,	O
the	O
minimal	O
trellis	B
’	O
s	O
width	O
is	O
everywhere	O
less	O
than	O
2m	O
.	O
this	O
will	O
be	O
proved	O
in	O
section	O
25.4.	O
notice	O
that	O
for	O
the	O
linear	B
trellises	O
in	O
(	O
cid:12	O
)	O
gure	O
25.1	O
,	O
all	O
of	O
which	O
are	O
minimal	O
trellises	O
,	O
k	O
is	O
the	O
number	O
of	O
times	O
a	O
binary	O
branch	O
point	O
is	O
encountered	O
as	O
the	O
trellis	B
is	O
traversed	O
from	O
left	O
to	O
right	O
or	O
from	O
right	O
to	O
left	O
.	O
we	O
will	O
discuss	O
the	O
construction	B
of	O
trellises	O
more	O
in	O
section	O
25.4.	O
but	O
we	O
now	O
know	O
enough	O
to	O
discuss	O
the	O
decoding	B
problem	O
.	O
25.3	O
solving	O
the	O
decoding	B
problems	O
on	O
a	O
trellis	B
we	O
can	O
view	O
the	O
trellis	B
of	O
a	O
linear	B
code	O
as	O
giving	O
a	O
causal	O
description	O
of	O
the	O
probabilistic	O
process	O
that	O
gives	O
rise	O
to	O
a	O
codeword	B
,	O
with	O
time	O
(	O
cid:13	O
)	O
owing	O
from	O
left	O
to	O
right	O
.	O
each	O
time	O
a	O
divergence	B
is	O
encountered	O
,	O
a	O
random	B
source	O
(	O
the	O
source	O
of	O
information	B
bits	O
for	O
communication	O
)	O
determines	O
which	O
way	O
we	O
go	O
.	O
at	O
the	O
receiving	O
end	O
,	O
we	O
receive	O
a	O
noisy	B
version	O
of	O
the	O
sequence	O
of	O
edge-	O
labels	O
,	O
and	O
wish	O
to	O
infer	O
which	O
path	O
was	O
taken	O
,	O
or	O
to	O
be	O
precise	O
,	O
(	O
a	O
)	O
we	O
want	O
to	O
identify	O
the	O
most	O
probable	O
path	O
in	O
order	O
to	O
solve	O
the	O
codeword	B
decoding	O
problem	O
;	O
and	O
(	O
b	O
)	O
we	O
want	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
probability	B
that	O
the	O
transmitted	O
symbol	O
at	O
time	O
n	O
was	O
a	O
zero	O
or	O
a	O
one	O
,	O
to	O
solve	O
the	O
bitwise	B
decoding	O
problem	O
.	O
example	O
25.3.	O
consider	O
the	O
case	O
of	O
a	O
single	O
transmission	O
from	O
the	O
hamming	O
(	O
7	O
;	O
4	O
)	O
trellis	B
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
25.1c	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
328	O
25	O
|	O
exact	O
marginalization	O
in	O
trellises	O
figure	O
25.2.	O
posterior	O
probabilities	O
over	O
the	O
sixteen	O
codewords	O
when	O
the	O
received	O
vector	O
y	O
has	O
normalized	O
likelihoods	O
(	O
0:1	O
;	O
0:4	O
;	O
0:9	O
;	O
0:1	O
;	O
0:1	O
;	O
0:1	O
;	O
0:3	O
)	O
.	O
t	O
likelihood	B
posterior	O
probability	B
0000000	O
0001011	O
0010111	O
0011100	O
0100110	O
0101101	O
0110001	O
0111010	O
1000101	O
1001110	O
1010010	O
1011001	O
1100011	O
1101000	O
1110100	O
1111111	O
0.0275562	O
0.0001458	O
0.0013122	O
0.0030618	O
0.0002268	O
0.0000972	O
0.0708588	O
0.0020412	O
0.0001458	O
0.0000042	O
0.0030618	O
0.0013122	O
0.0000972	O
0.0002268	O
0.0020412	O
0.0000108	O
0.25	O
0.0013	O
0.012	O
0.027	O
0.0020	O
0.0009	O
0.63	O
0.018	O
0.0013	O
0.0000	O
0.027	O
0.012	O
0.0009	O
0.0020	O
0.018	O
0.0001	O
let	O
the	O
normalized	O
likelihoods	O
be	O
:	O
(	O
0:1	O
;	O
0:4	O
;	O
0:9	O
;	O
0:1	O
;	O
0:1	O
;	O
0:1	O
;	O
0:3	O
)	O
.	O
that	O
is	O
,	O
the	O
ratios	O
of	O
the	O
likelihoods	O
are	O
p	O
(	O
y1	O
j	O
x1	O
=	O
1	O
)	O
p	O
(	O
y1	O
j	O
x1	O
=	O
0	O
)	O
=	O
0:1	O
0:9	O
;	O
p	O
(	O
y2	O
j	O
x2	O
=	O
1	O
)	O
p	O
(	O
y2	O
j	O
x2	O
=	O
0	O
)	O
=	O
0:4	O
0:6	O
;	O
etc	O
.	O
(	O
25.12	O
)	O
how	O
should	O
this	O
received	O
signal	O
be	O
decoded	O
?	O
1.	O
if	O
we	O
threshold	B
the	O
likelihoods	O
at	O
0.5	O
to	O
turn	O
the	O
signal	O
into	O
a	O
bi-	O
nary	O
received	O
vector	O
,	O
we	O
have	O
r	O
=	O
(	O
0	O
;	O
0	O
;	O
1	O
;	O
0	O
;	O
0	O
;	O
0	O
;	O
0	O
)	O
,	O
which	O
decodes	O
,	O
using	O
the	O
decoder	B
for	O
the	O
binary	B
symmetric	I
channel	I
(	O
chapter	O
1	O
)	O
,	O
into	O
^t	O
=	O
(	O
0	O
;	O
0	O
;	O
0	O
;	O
0	O
;	O
0	O
;	O
0	O
;	O
0	O
)	O
.	O
this	O
is	O
not	O
the	O
optimal	B
decoding	O
procedure	O
.	O
optimal	B
inferences	O
are	O
always	O
obtained	O
by	O
using	O
bayes	O
’	O
theorem	B
.	O
2.	O
we	O
can	O
(	O
cid:12	O
)	O
nd	O
the	O
posterior	B
probability	I
over	O
codewords	O
by	O
explicit	O
enu-	O
meration	O
of	O
all	O
sixteen	O
codewords	O
.	O
this	O
posterior	O
distribution	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
25.2.	O
of	O
course	O
,	O
we	O
aren	O
’	O
t	O
really	O
interested	O
in	O
such	O
brute-force	O
solutions	O
,	O
and	O
the	O
aim	O
of	O
this	O
chapter	O
is	O
to	O
understand	O
algorithms	B
for	O
getting	O
the	O
same	O
information	B
out	O
in	O
less	O
than	O
2k	O
computer	B
time	O
.	O
examining	O
the	O
posterior	O
probabilities	O
,	O
we	O
notice	O
that	O
the	O
most	O
probable	O
codeword	O
is	O
actually	O
the	O
string	O
t	O
=	O
0110001.	O
this	O
is	O
more	O
than	O
twice	O
as	O
probable	O
as	O
the	O
answer	O
found	O
by	O
thresholding	O
,	O
0000000.	O
using	O
the	O
posterior	O
probabilities	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
25.2	O
,	O
we	O
can	O
also	O
com-	O
pute	O
the	O
posterior	O
marginal	O
distributions	O
of	O
each	O
of	O
the	O
bits	O
.	O
the	O
result	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
25.3.	O
notice	O
that	O
bits	O
1	O
,	O
4	O
,	O
5	O
and	O
6	O
are	O
all	O
quite	O
con-	O
(	O
cid:12	O
)	O
dently	O
inferred	O
to	O
be	O
zero	O
.	O
the	O
strengths	O
of	O
the	O
posterior	O
probabilities	O
for	O
bits	O
2	O
,	O
3	O
,	O
and	O
7	O
are	O
not	O
so	O
great	O
.	O
2	O
in	O
the	O
above	O
example	O
,	O
the	O
map	O
codeword	B
is	O
in	O
agreement	O
with	O
the	O
bitwise	B
decoding	O
that	O
is	O
obtained	O
by	O
selecting	O
the	O
most	O
probable	O
state	O
for	O
each	O
bit	B
using	O
the	O
posterior	O
marginal	O
distributions	O
.	O
but	O
this	O
is	O
not	O
always	O
the	O
case	O
,	O
as	O
the	O
following	O
exercise	O
shows	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
25.3	O
:	O
solving	O
the	O
decoding	B
problems	O
on	O
a	O
trellis	B
329	O
likelihood	B
posterior	O
marginals	O
p	O
(	O
yn	O
j	O
tn	O
=	O
1	O
)	O
p	O
(	O
yn	O
j	O
tn	O
=	O
0	O
)	O
p	O
(	O
tn	O
=	O
1j	O
y	O
)	O
p	O
(	O
tn	O
=	O
0j	O
y	O
)	O
figure	O
25.3.	O
marginal	B
posterior	O
probabilities	O
for	O
the	O
7	O
bits	O
under	O
the	O
posterior	O
distribution	O
of	O
(	O
cid:12	O
)	O
gure	O
25.2	O
.	O
0:1	O
0:4	O
0:9	O
0:1	O
0:1	O
0:1	O
0:3	O
0:9	O
0:6	O
0:1	O
0:9	O
0:9	O
0:9	O
0:7	O
0:061	O
0:674	O
0:746	O
0:061	O
0:061	O
0:061	O
0:659	O
0:939	O
0:326	O
0:254	O
0:939	O
0:939	O
0:939	O
0:341	O
n	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
exercise	O
25.4	O
.	O
[	O
2	O
,	O
p.333	O
]	O
find	O
the	O
most	O
probable	O
codeword	O
in	O
the	O
case	O
where	O
the	O
normalized	O
likelihood	B
is	O
(	O
0:2	O
;	O
0:2	O
;	O
0:9	O
;	O
0:2	O
;	O
0:2	O
;	O
0:2	O
;	O
0:2	O
)	O
.	O
also	O
(	O
cid:12	O
)	O
nd	O
or	O
estimate	O
the	O
marginal	B
posterior	O
probability	B
for	O
each	O
of	O
the	O
seven	O
bits	O
,	O
and	O
give	O
the	O
bit-by-bit	O
decoding	B
.	O
[	O
hint	O
:	O
concentrate	O
on	O
the	O
few	O
codewords	O
that	O
have	O
the	O
largest	O
probabil-	O
ity	O
.	O
]	O
we	O
now	O
discuss	O
how	O
to	O
use	O
message	B
passing	I
on	O
a	O
code	B
’	O
s	O
trellis	B
to	O
solve	O
the	O
decoding	B
problems	O
.	O
the	O
min	O
{	O
sum	O
algorithm	O
the	O
map	O
codeword	B
decoding	O
problem	O
can	O
be	O
solved	O
using	O
the	O
min	O
{	O
sum	O
al-	O
gorithm	O
that	O
was	O
introduced	O
in	O
section	O
16.3.	O
each	O
codeword	B
of	O
the	O
code	B
corresponds	O
to	O
a	O
path	O
across	O
the	O
trellis	B
.	O
just	O
as	O
the	O
cost	O
of	O
a	O
journey	O
is	O
the	O
sum	O
of	O
the	O
costs	O
of	O
its	O
constituent	O
steps	O
,	O
the	O
log	O
likelihood	B
of	O
a	O
codeword	B
is	O
the	O
sum	O
of	O
the	O
bitwise	B
log	O
likelihoods	O
.	O
by	O
convention	O
,	O
we	O
(	O
cid:13	O
)	O
ip	O
the	O
sign	O
of	O
the	O
log	O
likelihood	B
(	O
which	O
we	O
would	O
like	O
to	O
maximize	O
)	O
and	O
talk	O
in	O
terms	O
of	O
a	O
cost	O
,	O
which	O
we	O
would	O
like	O
to	O
minimize	O
.	O
we	O
associate	O
with	O
each	O
edge	B
a	O
cost	O
(	O
cid:0	O
)	O
log	O
p	O
(	O
yn	O
j	O
tn	O
)	O
,	O
where	O
tn	O
is	O
the	O
trans-	O
mitted	O
bit	B
associated	O
with	O
that	O
edge	B
,	O
and	O
yn	O
is	O
the	O
received	O
symbol	O
.	O
the	O
min	O
{	O
sum	O
algorithm	O
presented	O
in	O
section	O
16.3	O
can	O
then	O
identify	O
the	O
most	O
prob-	O
able	O
codeword	B
in	O
a	O
number	O
of	O
computer	O
operations	O
equal	O
to	O
the	O
number	O
of	O
edges	O
in	O
the	O
trellis	B
.	O
this	O
algorithm	B
is	O
also	O
known	O
as	O
the	O
viterbi	O
algorithm	B
(	O
viterbi	O
,	O
1967	O
)	O
.	O
the	O
sum	O
{	O
product	O
algorithm	O
to	O
solve	O
the	O
bitwise	B
decoding	O
problem	O
,	O
we	O
can	O
make	O
a	O
small	O
modi	O
(	O
cid:12	O
)	O
cation	O
to	O
the	O
min	O
{	O
sum	O
algorithm	O
,	O
so	O
that	O
the	O
messages	O
passed	O
through	O
the	O
trellis	B
de	O
(	O
cid:12	O
)	O
ne	O
‘	O
the	O
probability	O
of	O
the	O
data	O
up	O
to	O
the	O
current	O
point	O
’	O
instead	O
of	O
‘	O
the	O
cost	O
of	O
the	O
best	O
route	O
to	O
this	O
point	O
’	O
.	O
we	O
replace	O
the	O
costs	O
on	O
the	O
edges	O
,	O
(	O
cid:0	O
)	O
log	O
p	O
(	O
yn	O
j	O
tn	O
)	O
,	O
by	O
the	O
likelihoods	O
themselves	O
,	O
p	O
(	O
yn	O
j	O
tn	O
)	O
.	O
we	O
replace	O
the	O
min	O
and	O
sum	O
operations	O
of	O
the	O
min	O
{	O
sum	O
algorithm	O
by	O
a	O
sum	O
and	O
product	O
respectively	O
.	O
let	O
i	O
run	O
over	O
nodes/states	O
,	O
i	O
=	O
0	O
be	O
the	O
label	O
for	O
the	O
start	O
state	O
,	O
p	O
(	O
i	O
)	O
denote	O
the	O
set	B
of	O
states	O
that	O
are	O
parents	O
of	O
state	O
i	O
,	O
and	O
wij	O
be	O
the	O
likelihood	B
associated	O
with	O
the	O
edge	B
from	O
node	O
j	O
to	O
node	O
i.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
forward-pass	O
messages	O
(	O
cid:11	O
)	O
i	O
by	O
(	O
cid:11	O
)	O
0	O
=	O
1	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
330	O
25	O
|	O
exact	O
marginalization	O
in	O
trellises	O
(	O
cid:11	O
)	O
i	O
=	O
xj2p	O
(	O
i	O
)	O
wij	O
(	O
cid:11	O
)	O
j	O
:	O
(	O
25.13	O
)	O
these	O
messages	O
can	O
be	O
computed	O
sequentially	O
from	O
left	O
to	O
right	O
.	O
.	O
exercise	O
25.5	O
.	O
[	O
2	O
]	O
show	O
that	O
for	O
a	O
node	O
i	O
whose	O
time-coordinate	O
is	O
n	O
,	O
(	O
cid:11	O
)	O
i	O
is	O
proportional	O
to	O
the	O
joint	B
probability	O
that	O
the	O
codeword	B
’	O
s	O
path	O
passed	O
through	O
node	O
i	O
and	O
that	O
the	O
(	O
cid:12	O
)	O
rst	O
n	O
received	O
symbols	O
were	O
y1	O
;	O
:	O
:	O
:	O
;	O
yn	O
.	O
the	O
message	O
(	O
cid:11	O
)	O
i	O
computed	O
at	O
the	O
end	O
node	O
of	O
the	O
trellis	O
is	O
proportional	O
to	O
the	O
marginal	B
probability	I
of	O
the	O
data	O
.	O
.	O
exercise	O
25.6	O
.	O
[	O
2	O
]	O
what	O
is	O
the	O
constant	O
of	O
proportionality	O
?	O
[	O
answer	O
:	O
2k	O
]	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
second	O
set	B
of	O
backward-pass	O
messages	O
(	O
cid:12	O
)	O
i	O
in	O
a	O
similar	O
manner	O
.	O
let	O
node	O
i	O
be	O
the	O
end	O
node	O
.	O
(	O
cid:12	O
)	O
i	O
=	O
1	O
(	O
cid:12	O
)	O
j	O
=	O
xi	O
:	O
j2p	O
(	O
i	O
)	O
wij	O
(	O
cid:12	O
)	O
i	O
:	O
(	O
25.14	O
)	O
these	O
messages	O
can	O
be	O
computed	O
sequentially	O
in	O
a	O
backward	B
pass	I
from	O
right	O
to	O
left	O
.	O
.	O
exercise	O
25.7	O
.	O
[	O
2	O
]	O
show	O
that	O
for	O
a	O
node	O
i	O
whose	O
time-coordinate	O
is	O
n	O
,	O
(	O
cid:12	O
)	O
i	O
is	O
proportional	O
to	O
the	O
conditional	B
probability	O
,	O
given	O
that	O
the	O
codeword	B
’	O
s	O
path	O
passed	O
through	O
node	O
i	O
,	O
that	O
the	O
subsequent	O
received	O
symbols	O
were	O
yn+1	O
:	O
:	O
:	O
yn	O
.	O
finally	O
,	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
probability	B
that	O
the	O
nth	O
bit	B
was	O
a	O
1	O
or	O
0	O
,	O
we	O
do	O
two	O
summations	O
of	O
products	O
of	O
the	O
forward	O
and	O
backward	O
messages	O
.	O
let	O
i	O
run	O
over	O
nodes	O
at	O
time	O
n	O
and	O
j	O
run	O
over	O
nodes	O
at	O
time	O
n	O
(	O
cid:0	O
)	O
1	O
,	O
and	O
let	O
tij	O
be	O
the	O
value	O
of	O
tn	O
associated	O
with	O
the	O
trellis	B
edge	O
from	O
node	O
j	O
to	O
node	O
i.	O
for	O
each	O
value	O
of	O
t	O
=	O
0=1	O
,	O
we	O
compute	O
r	O
(	O
t	O
)	O
n	O
=	O
xi	O
;	O
j	O
:	O
j2p	O
(	O
i	O
)	O
;	O
tij	O
=t	O
(	O
cid:11	O
)	O
jwij	O
(	O
cid:12	O
)	O
i	O
:	O
(	O
25.15	O
)	O
then	O
the	O
posterior	B
probability	I
that	O
tn	O
was	O
t	O
=	O
0=1	O
is	O
p	O
(	O
tn	O
=	O
tj	O
y	O
)	O
=	O
1	O
z	O
n	O
+	O
r	O
(	O
1	O
)	O
where	O
the	O
normalizing	B
constant	I
z	O
=	O
r	O
(	O
0	O
)	O
forward	O
message	O
(	O
cid:11	O
)	O
i	O
that	O
was	O
computed	O
earlier	O
.	O
exercise	O
25.8	O
.	O
[	O
2	O
]	O
con	O
(	O
cid:12	O
)	O
rm	O
that	O
the	O
above	O
sum	O
{	O
product	O
algorithm	O
does	O
com-	O
n	O
should	O
be	O
identical	O
to	O
the	O
(	O
cid:12	O
)	O
nal	O
r	O
(	O
t	O
)	O
n	O
;	O
(	O
25.16	O
)	O
pute	O
p	O
(	O
tn	O
=	O
tj	O
y	O
)	O
.	O
other	O
names	O
for	O
the	O
sum	O
{	O
product	O
algorithm	O
presented	O
here	O
are	O
‘	O
the	O
forward	O
{	O
backward	O
algorithm	O
’	O
,	O
‘	O
the	O
bcjr	O
algorithm	B
’	O
,	O
and	O
‘	O
belief	B
propagation	I
’	O
.	O
.	O
exercise	O
25.9	O
.	O
[	O
2	O
,	O
p.333	O
]	O
a	O
codeword	B
of	O
the	O
simple	B
parity	I
code	O
p3	O
is	O
transmitted	O
,	O
and	O
the	O
received	O
signal	O
y	O
has	O
associated	O
likelihoods	O
shown	O
in	O
table	O
25.4.	O
use	O
the	O
min	O
{	O
sum	O
algorithm	O
and	O
the	O
sum	O
{	O
product	O
algorithm	O
in	O
the	O
trellis	B
(	O
(	O
cid:12	O
)	O
gure	O
25.1	O
)	O
to	O
solve	O
the	O
map	O
codeword	B
decoding	O
problem	O
and	O
the	O
bitwise	B
decoding	O
problem	O
.	O
con	O
(	O
cid:12	O
)	O
rm	O
your	O
answers	O
by	O
enumeration	O
of	O
all	O
codewords	O
(	O
000	O
,	O
011	O
,	O
110	O
,	O
101	O
)	O
.	O
[	O
hint	O
:	O
use	O
logs	O
to	O
base	O
2	O
and	O
do	O
the	O
min	O
{	O
sum	O
computations	O
by	O
hand	O
.	O
when	O
working	O
the	O
sum	O
{	O
product	O
algorithm	O
by	O
hand	O
,	O
you	O
may	O
(	O
cid:12	O
)	O
nd	O
it	O
helpful	O
to	O
use	O
three	O
colours	O
of	O
pen	O
,	O
one	O
for	O
the	O
(	O
cid:11	O
)	O
s	O
,	O
one	O
for	O
the	O
ws	O
,	O
and	O
one	O
for	O
the	O
(	O
cid:12	O
)	O
s.	O
]	O
n	O
1	O
2	O
3	O
p	O
(	O
yn	O
j	O
tn	O
)	O
tn	O
=	O
0	O
tn	O
=	O
1	O
1/4	O
1/2	O
1/8	O
1/2	O
1/4	O
1/2	O
table	O
25.4.	O
bitwise	B
likelihoods	O
for	O
a	O
codeword	B
of	O
p3	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
25.4	O
:	O
more	O
on	O
trellises	O
25.4	O
more	O
on	O
trellises	O
we	O
now	O
discuss	O
various	O
ways	O
of	O
making	O
the	O
trellis	B
of	O
a	O
code	B
.	O
you	O
may	O
safely	O
jump	O
over	O
this	O
section	B
.	O
the	O
span	B
of	O
a	O
codeword	B
is	O
the	O
set	B
of	O
bits	O
contained	O
between	O
the	O
(	O
cid:12	O
)	O
rst	O
bit	B
in	O
the	O
codeword	B
that	O
is	O
non-zero	O
,	O
and	O
the	O
last	O
bit	B
that	O
is	O
non-zero	O
,	O
inclusive	O
.	O
we	O
can	O
indicate	O
the	O
span	B
of	O
a	O
codeword	B
by	O
a	O
binary	O
vector	O
as	O
shown	O
in	O
table	O
25.5	O
.	O
331	O
codeword	B
0000000	O
span	B
0000000	O
0001011	O
0001111	O
0100110	O
0111110	O
1100011	O
1111111	O
0101101	O
0111111	O
table	O
25.5.	O
some	O
codewords	O
and	O
their	O
spans	O
.	O
a	O
generator	B
matrix	I
is	O
in	O
trellis-oriented	O
form	O
if	O
the	O
spans	O
of	O
the	O
rows	O
of	O
the	O
generator	O
matrix	B
all	O
start	O
in	O
di	O
(	O
cid:11	O
)	O
erent	O
columns	O
and	O
the	O
spans	O
all	O
end	O
in	O
di	O
(	O
cid:11	O
)	O
erent	O
columns	O
.	O
how	O
to	O
make	O
a	O
trellis	B
from	O
a	O
generator	B
matrix	I
first	O
,	O
put	O
the	O
generator	B
matrix	I
into	O
trellis-oriented	O
form	O
by	O
row-manipulations	O
similar	O
to	O
gaussian	O
elimination	O
.	O
for	O
example	O
,	O
our	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
can	O
be	O
generated	O
by	O
(	O
25.17	O
)	O
g	O
=2	O
664	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
3	O
775	O
but	O
this	O
matrix	B
is	O
not	O
in	O
trellis-oriented	O
form	O
{	O
for	O
example	O
,	O
rows	O
1	O
,	O
3	O
and	O
4	O
all	O
have	O
spans	O
that	O
end	O
in	O
the	O
same	O
column	O
.	O
by	O
subtracting	O
lower	O
rows	O
from	O
upper	O
rows	O
,	O
we	O
can	O
obtain	O
an	O
equivalent	O
generator	B
matrix	I
(	O
that	O
is	O
,	O
one	O
that	O
generates	O
the	O
same	O
set	B
of	O
codewords	O
)	O
as	O
follows	O
:	O
g	O
=2	O
664	O
1	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
:	O
3	O
775	O
(	O
25.18	O
)	O
now	O
,	O
each	O
row	O
of	O
the	O
generator	O
matrix	B
can	O
be	O
thought	O
of	O
as	O
de	O
(	O
cid:12	O
)	O
ning	O
an	O
(	O
n	O
;	O
1	O
)	O
subcode	O
of	O
the	O
(	O
n	O
;	O
k	O
)	O
code	B
,	O
that	O
is	O
,	O
in	O
this	O
case	O
,	O
a	O
code	B
with	O
two	O
codewords	O
of	O
length	O
n	O
=	O
7.	O
for	O
the	O
(	O
cid:12	O
)	O
rst	O
row	O
,	O
the	O
code	B
consists	O
of	O
the	O
two	O
codewords	O
1101000	O
and	O
0000000.	O
the	O
subcode	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
the	O
second	O
row	O
consists	O
of	O
0100110	O
and	O
0000000.	O
it	O
is	O
easy	O
to	O
construct	O
the	O
minimal	O
trellises	O
of	O
these	O
subcodes	O
;	O
they	O
are	O
shown	O
in	O
the	O
left	O
column	O
of	O
(	O
cid:12	O
)	O
gure	O
25.6.	O
we	O
build	O
the	O
trellis	B
incrementally	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
25.6.	O
we	O
start	O
with	O
the	O
trellis	B
corresponding	O
to	O
the	O
subcode	O
given	O
by	O
the	O
(	O
cid:12	O
)	O
rst	O
row	O
of	O
the	O
generator	O
matrix	B
.	O
then	O
we	O
add	O
in	O
one	O
subcode	O
at	O
a	O
time	O
.	O
the	O
vertices	O
within	O
the	O
span	B
of	O
the	O
new	O
subcode	O
are	O
all	O
duplicated	O
.	O
the	O
edge	B
symbols	O
in	O
the	O
original	O
trellis	B
are	O
left	O
unchanged	O
and	O
the	O
edge	B
symbols	O
in	O
the	O
second	O
part	O
of	O
the	O
trellis	O
are	O
(	O
cid:13	O
)	O
ipped	O
wherever	O
the	O
new	O
subcode	O
has	O
a	O
1	O
and	O
otherwise	O
left	O
alone	O
.	O
another	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
can	O
be	O
generated	O
by	O
g	O
=2	O
664	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
:	O
3	O
775	O
(	O
25.19	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
332	O
25	O
|	O
exact	O
marginalization	O
in	O
trellises	O
figure	O
25.6.	O
trellises	O
for	O
four	O
subcodes	O
of	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
(	O
left	O
column	O
)	O
,	O
and	O
the	O
sequence	B
of	O
trellises	O
that	O
are	O
made	O
when	O
constructing	O
the	O
trellis	B
for	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
(	O
right	O
column	O
)	O
.	O
each	O
edge	B
in	O
a	O
trellis	B
is	O
labelled	O
by	O
a	O
zero	O
(	O
shown	O
by	O
a	O
square	B
)	O
or	O
a	O
one	O
(	O
shown	O
by	O
a	O
cross	O
)	O
.	O
+	O
+	O
+	O
=	O
=	O
=	O
the	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
generated	O
by	O
this	O
matrix	B
di	O
(	O
cid:11	O
)	O
ers	O
by	O
a	O
permutation	B
of	O
its	O
bits	O
from	O
the	O
code	B
generated	O
by	O
the	O
systematic	B
matrix	O
used	O
in	O
chapter	O
1	O
and	O
above	O
.	O
the	O
parity-check	B
matrix	I
corresponding	O
to	O
this	O
permutation	B
is	O
:	O
h	O
=2	O
4	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
3	O
5	O
:	O
(	O
25.20	O
)	O
(	O
a	O
)	O
the	O
trellis	B
obtained	O
from	O
the	O
permuted	O
matrix	B
g	O
given	O
in	O
equation	O
(	O
25.19	O
)	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
25.7a	O
.	O
notice	O
that	O
the	O
number	O
of	O
nodes	O
in	O
this	O
trellis	B
is	O
smaller	O
than	O
the	O
number	O
of	O
nodes	O
in	O
the	O
previous	O
trellis	B
for	O
the	O
hamming	O
(	O
7	O
;	O
4	O
)	O
code	B
in	O
(	O
cid:12	O
)	O
gure	O
25.1c	O
.	O
we	O
thus	O
observe	O
that	O
rearranging	O
the	O
order	O
of	O
the	O
codeword	B
bits	O
can	O
sometimes	O
lead	O
to	O
smaller	O
,	O
simpler	O
trellises	O
.	O
trellises	O
from	O
parity-check	O
matrices	B
another	O
way	O
of	O
viewing	O
the	O
trellis	B
is	O
in	O
terms	O
of	O
the	O
syndrome	O
.	O
the	O
syndrome	B
of	O
a	O
vector	O
r	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
hr	O
,	O
where	O
h	O
is	O
the	O
parity-check	B
matrix	I
.	O
a	O
vector	O
is	O
only	O
a	O
codeword	B
if	O
its	O
syndrome	B
is	O
zero	O
.	O
as	O
we	O
generate	O
a	O
codeword	B
we	O
can	O
describe	O
the	O
current	O
state	O
by	O
the	O
partial	B
syndrome	O
,	O
that	O
is	O
,	O
the	O
product	O
of	O
h	O
with	O
the	O
codeword	B
bits	O
thus	O
far	O
generated	O
.	O
each	O
state	O
in	O
the	O
trellis	B
is	O
a	O
partial	B
syndrome	O
at	O
one	O
time	O
coordinate	O
.	O
the	O
starting	O
and	O
ending	O
states	O
are	O
both	O
constrained	B
to	O
be	O
the	O
zero	O
syndrome	B
.	O
each	O
node	O
in	O
a	O
state	O
represents	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
possible	O
value	O
for	O
the	O
partial	B
syndrome	O
.	O
since	O
h	O
is	O
an	O
m	O
(	O
cid:2	O
)	O
n	O
matrix	B
,	O
where	O
m	O
=	O
n	O
(	O
cid:0	O
)	O
k	O
,	O
the	O
syndrome	B
is	O
at	O
most	O
an	O
m	O
-bit	O
vector	O
.	O
so	O
we	O
need	O
at	O
most	O
2m	O
nodes	O
in	O
each	O
state	O
.	O
we	O
can	O
construct	O
the	O
trellis	B
of	O
a	O
code	B
from	O
its	O
parity-check	B
matrix	I
by	O
walking	O
from	O
each	O
end	O
,	O
generating	O
two	O
trees	O
of	O
possible	O
syndrome	B
sequences	O
.	O
the	O
intersection	B
of	O
these	O
two	O
trees	O
de	O
(	O
cid:12	O
)	O
nes	O
the	O
trellis	B
of	O
the	O
code	B
.	O
in	O
the	O
pictures	O
we	O
obtain	O
from	O
this	O
construction	B
,	O
we	O
can	O
let	O
the	O
vertical	O
coordinate	O
represent	O
the	O
syndrome	B
.	O
then	O
any	O
horizontal	O
edge	B
is	O
necessarily	O
associated	O
with	O
a	O
zero	O
bit	B
(	O
since	O
only	O
a	O
non-zero	O
bit	B
changes	O
the	O
syndrome	B
)	O
(	O
b	O
)	O
figure	O
25.7.	O
trellises	O
for	O
the	O
permuted	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
generated	O
from	O
(	O
a	O
)	O
the	O
generator	B
matrix	I
by	O
the	O
method	B
of	O
(	O
cid:12	O
)	O
gure	O
25.6	O
;	O
(	O
b	O
)	O
the	O
parity-check	B
matrix	I
by	O
the	O
method	B
on	O
page	O
332.	O
each	O
edge	B
in	O
a	O
trellis	B
is	O
labelled	O
by	O
a	O
zero	O
(	O
shown	O
by	O
a	O
square	B
)	O
or	O
a	O
one	O
(	O
shown	O
by	O
a	O
cross	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
25.5	O
:	O
solutions	O
333	O
and	O
any	O
non-horizontal	O
edge	B
is	O
associated	O
with	O
a	O
one	O
bit	B
.	O
(	O
thus	O
in	O
this	O
rep-	O
resentation	O
we	O
no	O
longer	O
need	O
to	O
label	O
the	O
edges	O
in	O
the	O
trellis	B
.	O
)	O
figure	O
25.7b	O
shows	O
the	O
trellis	B
corresponding	O
to	O
the	O
parity-check	B
matrix	I
of	O
equation	O
(	O
25.20	O
)	O
.	O
25.5	O
solutions	O
t	O
likelihood	B
posterior	O
probability	B
table	O
25.8.	O
the	O
posterior	B
probability	I
over	O
codewords	O
for	O
exercise	O
25.4	O
.	O
0000000	O
0001011	O
0010111	O
0011100	O
0100110	O
0101101	O
0110001	O
0111010	O
1000101	O
1001110	O
1010010	O
1011001	O
1100011	O
1101000	O
1110100	O
1111111	O
0.026	O
0.00041	O
0.0037	O
0.015	O
0.00041	O
0.00010	O
0.015	O
0.0037	O
0.00041	O
0.00010	O
0.015	O
0.0037	O
0.00010	O
0.00041	O
0.0037	O
0.000058	O
0.3006	O
0.0047	O
0.0423	O
0.1691	O
0.0047	O
0.0012	O
0.1691	O
0.0423	O
0.0047	O
0.0012	O
0.1691	O
0.0423	O
0.0012	O
0.0047	O
0.0423	O
0.0007	O
solution	O
to	O
exercise	O
25.4	O
(	O
p.329	O
)	O
.	O
the	O
posterior	B
probability	I
over	O
codewords	O
is	O
shown	O
in	O
table	O
25.8.	O
the	O
most	O
probable	O
codeword	O
is	O
0000000.	O
the	O
marginal	B
posterior	O
probabilities	O
of	O
all	O
seven	O
bits	O
are	O
:	O
n	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
likelihood	B
posterior	O
marginals	O
p	O
(	O
yn	O
j	O
tn	O
=	O
1	O
)	O
p	O
(	O
yn	O
j	O
tn	O
=	O
0	O
)	O
p	O
(	O
tn	O
=	O
1j	O
y	O
)	O
p	O
(	O
tn	O
=	O
0j	O
y	O
)	O
0:2	O
0:2	O
0:9	O
0:2	O
0:2	O
0:2	O
0:2	O
0:8	O
0:8	O
0:1	O
0:8	O
0:8	O
0:8	O
0:8	O
0:266	O
0:266	O
0:677	O
0:266	O
0:266	O
0:266	O
0:266	O
0:734	O
0:734	O
0:323	O
0:734	O
0:734	O
0:734	O
0:734	O
so	O
the	O
bitwise	B
decoding	O
is	O
0010000	O
,	O
which	O
is	O
not	O
actually	O
a	O
codeword	B
.	O
solution	O
to	O
exercise	O
25.9	O
(	O
p.330	O
)	O
.	O
the	O
map	O
codeword	B
is	O
101	O
,	O
and	O
its	O
like-	O
lihood	O
is	O
1=8	O
.	O
the	O
normalizing	B
constant	I
of	O
the	O
sum	O
{	O
product	O
algorithm	O
is	O
z	O
=	O
(	O
cid:11	O
)	O
i	O
=	O
3/16	O
.	O
the	O
intermediate	O
(	O
cid:11	O
)	O
i	O
are	O
(	O
from	O
left	O
to	O
right	O
)	O
1/2	O
,	O
1/4	O
,	O
5/16	O
,	O
4/16	O
;	O
the	O
intermediate	O
(	O
cid:12	O
)	O
i	O
are	O
(	O
from	O
right	O
to	O
left	O
)	O
,	O
1/2	O
,	O
1/8	O
,	O
9/32	O
,	O
3/16	O
.	O
the	O
bitwise	B
decoding	O
is	O
:	O
p	O
(	O
t1	O
=	O
1j	O
y	O
)	O
=	O
3=4	O
;	O
p	O
(	O
t1	O
=	O
1j	O
y	O
)	O
=	O
1=4	O
;	O
p	O
(	O
t1	O
=	O
1j	O
y	O
)	O
=	O
5=6	O
.	O
the	O
codewords	O
’	O
probabilities	O
are	O
1/12	O
,	O
2/12	O
,	O
1/12	O
,	O
8/12	O
for	O
000	O
,	O
011	O
,	O
110	O
,	O
101	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
26	O
exact	O
marginalization	O
in	O
graphs	O
we	O
now	O
take	O
a	O
more	O
general	O
view	O
of	O
the	O
tasks	O
of	O
inference	O
and	O
marginalization	O
.	O
before	O
reading	O
this	O
chapter	O
,	O
you	O
should	O
read	O
about	O
message	B
passing	I
in	O
chapter	O
16	O
.	O
26.1	O
the	O
general	O
problem	O
assume	O
that	O
a	O
function	B
p	O
(	O
cid:3	O
)	O
of	O
a	O
set	B
of	O
n	O
variables	O
x	O
(	O
cid:17	O
)	O
fxngn	O
a	O
product	O
of	O
m	O
factors	O
as	O
follows	O
:	O
n=1	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
as	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=	O
m	O
ym=1	O
fm	O
(	O
xm	O
)	O
:	O
(	O
26.1	O
)	O
each	O
of	O
the	O
factors	O
fm	O
(	O
xm	O
)	O
is	O
a	O
function	B
of	O
a	O
subset	B
xm	O
of	O
the	O
variables	O
that	O
make	O
up	O
x.	O
if	O
p	O
(	O
cid:3	O
)	O
is	O
a	O
positive	O
function	O
then	O
we	O
may	O
be	O
interested	O
in	O
a	O
second	O
normalized	O
function	B
,	O
p	O
(	O
x	O
)	O
(	O
cid:17	O
)	O
1	O
z	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=	O
1	O
z	O
m	O
ym=1	O
fm	O
(	O
xm	O
)	O
;	O
(	O
26.2	O
)	O
where	O
the	O
normalizing	B
constant	I
z	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
z	O
=xx	O
m	O
ym=1	O
fm	O
(	O
xm	O
)	O
:	O
(	O
26.3	O
)	O
as	O
an	O
example	O
of	O
the	O
notation	O
we	O
’	O
ve	O
just	O
introduced	O
,	O
here	O
’	O
s	O
a	O
function	B
of	O
three	O
binary	O
variables	O
x1	O
,	O
x2	O
,	O
x3	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
the	O
(	O
cid:12	O
)	O
ve	O
factors	O
:	O
0:9	O
x1	O
=	O
1	O
f1	O
(	O
x1	O
)	O
=	O
(	O
cid:26	O
)	O
0:1	O
x1	O
=	O
0	O
f2	O
(	O
x2	O
)	O
=	O
(	O
cid:26	O
)	O
0:1	O
x2	O
=	O
0	O
f3	O
(	O
x3	O
)	O
=	O
(	O
cid:26	O
)	O
0:9	O
x3	O
=	O
0	O
f4	O
(	O
x1	O
;	O
x2	O
)	O
=	O
(	O
cid:26	O
)	O
1	O
(	O
x1	O
;	O
x2	O
)	O
=	O
(	O
0	O
;	O
0	O
)	O
or	O
(	O
1	O
;	O
1	O
)	O
f5	O
(	O
x2	O
;	O
x3	O
)	O
=	O
(	O
cid:26	O
)	O
1	O
(	O
x2	O
;	O
x3	O
)	O
=	O
(	O
0	O
;	O
0	O
)	O
or	O
(	O
1	O
;	O
1	O
)	O
0	O
(	O
x1	O
;	O
x2	O
)	O
=	O
(	O
1	O
;	O
0	O
)	O
or	O
(	O
0	O
;	O
1	O
)	O
0	O
(	O
x2	O
;	O
x3	O
)	O
=	O
(	O
1	O
;	O
0	O
)	O
or	O
(	O
0	O
;	O
1	O
)	O
0:9	O
x2	O
=	O
1	O
0:1	O
x3	O
=	O
1	O
(	O
26.4	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=	O
f1	O
(	O
x1	O
)	O
f2	O
(	O
x2	O
)	O
f3	O
(	O
x3	O
)	O
f4	O
(	O
x1	O
;	O
x2	O
)	O
f5	O
(	O
x2	O
;	O
x3	O
)	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
f1	O
(	O
x1	O
)	O
f2	O
(	O
x2	O
)	O
f3	O
(	O
x3	O
)	O
f4	O
(	O
x1	O
;	O
x2	O
)	O
f5	O
(	O
x2	O
;	O
x3	O
)	O
:	O
the	O
(	O
cid:12	O
)	O
ve	O
subsets	O
of	O
fx1	O
;	O
x2	O
;	O
x3g	O
denoted	O
by	O
xm	O
in	O
the	O
general	O
function	O
(	O
26.1	O
)	O
are	O
here	O
x1	O
=	O
fx1g	O
,	O
x2	O
=	O
fx2g	O
,	O
x3	O
=	O
fx3g	O
,	O
x4	O
=	O
fx1	O
;	O
x2g	O
,	O
and	O
x5	O
=	O
fx2	O
;	O
x3g	O
.	O
334	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
26.1	O
:	O
the	O
general	O
problem	O
335	O
g	O
g	O
x2	O
x1	O
x3	O
@	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
@	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
f5	O
f4	O
f3	O
f2	O
f1	O
g	O
figure	O
26.1.	O
the	O
factor	B
graph	I
associated	O
with	O
the	O
function	B
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
(	O
26.4	O
)	O
.	O
the	O
function	B
p	O
(	O
x	O
)	O
,	O
by	O
the	O
way	O
,	O
may	O
be	O
recognized	O
as	O
the	O
posterior	O
prob-	O
ability	O
distribution	B
of	O
the	O
three	O
transmitted	O
bits	O
in	O
a	O
repetition	B
code	I
(	O
section	B
1.2	O
)	O
when	O
the	O
received	O
signal	O
is	O
r	O
=	O
(	O
1	O
;	O
1	O
;	O
0	O
)	O
and	O
the	O
channel	B
is	O
a	O
binary	O
sym-	O
metric	B
channel	O
with	O
(	O
cid:13	O
)	O
ip	O
probability	B
0.1.	O
the	O
factors	O
f4	O
and	O
f5	O
respectively	O
enforce	O
the	O
constraints	O
that	O
x1	O
and	O
x2	O
must	O
be	O
identical	O
and	O
that	O
x2	O
and	O
x3	O
must	O
be	O
identical	O
.	O
the	O
factors	O
f1	O
,	O
f2	O
,	O
f3	O
are	O
the	O
likelihood	B
functions	O
con-	O
tributed	O
by	O
each	O
component	O
of	O
r.	O
a	O
function	B
of	O
the	O
factored	O
form	O
(	O
26.1	O
)	O
can	O
be	O
depicted	O
by	O
a	O
factor	B
graph	I
,	O
in	O
which	O
the	O
variables	O
are	O
depicted	O
by	O
circular	O
nodes	O
and	O
the	O
factors	O
are	O
depicted	O
by	O
square	O
nodes	O
.	O
an	O
edge	B
is	O
put	O
between	O
variable	O
node	O
n	O
and	O
factor	O
node	O
m	O
if	O
the	O
function	B
fm	O
(	O
xm	O
)	O
has	O
any	O
dependence	O
on	O
variable	O
xn	O
.	O
the	O
factor	B
graph	I
for	O
the	O
example	O
function	B
(	O
26.4	O
)	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
26.1.	O
the	O
normalization	O
problem	O
the	O
(	O
cid:12	O
)	O
rst	O
task	O
to	O
be	O
solved	O
is	O
to	O
compute	O
the	O
normalizing	B
constant	I
z.	O
the	O
marginalization	B
problems	O
the	O
second	O
task	O
to	O
be	O
solved	O
is	O
to	O
compute	O
the	O
marginal	B
function	O
of	O
any	O
variable	O
xn	O
,	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
zn	O
(	O
xn	O
)	O
=	O
xfxn0g	O
;	O
n06=n	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
:	O
(	O
26.5	O
)	O
for	O
example	O
,	O
if	O
f	O
is	O
a	O
function	B
of	O
three	O
variables	O
then	O
the	O
marginal	B
for	O
n	O
=	O
1	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
z1	O
(	O
x1	O
)	O
=	O
xx2	O
;	O
x3	O
f	O
(	O
x1	O
;	O
x2	O
;	O
x3	O
)	O
:	O
(	O
26.6	O
)	O
this	O
type	O
of	O
summation	O
,	O
over	O
‘	O
all	O
the	O
xn0	O
except	O
for	O
xn	O
’	O
is	O
so	O
important	O
that	O
it	O
can	O
be	O
useful	O
to	O
have	O
a	O
special	O
notation	B
for	O
it	O
{	O
the	O
‘	O
not-sum	B
’	O
or	O
‘	O
summary	B
’	O
.	O
the	O
third	O
task	O
to	O
be	O
solved	O
is	O
to	O
compute	O
the	O
normalized	O
marginal	B
of	O
any	O
variable	O
xn	O
,	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
pn	O
(	O
xn	O
)	O
(	O
cid:17	O
)	O
xfxn0g	O
;	O
n06=n	O
p	O
(	O
x	O
)	O
:	O
(	O
26.7	O
)	O
[	O
we	O
include	O
the	O
su	O
(	O
cid:14	O
)	O
x	O
‘	O
n	O
’	O
in	O
pn	O
(	O
xn	O
)	O
,	O
departing	O
from	O
our	O
normal	B
practice	O
in	O
the	O
rest	O
of	O
the	O
book	O
,	O
where	O
we	O
would	O
omit	O
it	O
.	O
]	O
.	O
exercise	O
26.1	O
.	O
[	O
1	O
]	O
show	O
that	O
the	O
normalized	O
marginal	B
is	O
related	O
to	O
the	O
marginal	B
zn	O
(	O
xn	O
)	O
by	O
pn	O
(	O
xn	O
)	O
=	O
zn	O
(	O
xn	O
)	O
z	O
:	O
(	O
26.8	O
)	O
we	O
might	O
also	O
be	O
interested	O
in	O
marginals	O
over	O
a	O
subset	B
of	O
the	O
variables	O
,	O
such	O
as	O
z12	O
(	O
x1	O
;	O
x2	O
)	O
(	O
cid:17	O
)	O
xx3	O
p	O
(	O
cid:3	O
)	O
(	O
x1	O
;	O
x2	O
;	O
x3	O
)	O
:	O
(	O
26.9	O
)	O
all	O
these	O
tasks	O
are	O
intractable	O
in	O
general	O
.	O
even	O
if	O
every	O
factor	O
is	O
a	O
function	B
of	O
only	O
three	O
variables	O
,	O
the	O
cost	O
of	O
computing	O
exact	O
solutions	O
for	O
z	O
and	O
for	O
the	O
marginals	O
is	O
believed	O
in	O
general	O
to	O
grow	O
exponentially	O
with	O
the	O
number	O
of	O
variables	O
n	O
.	O
for	O
certain	O
functions	B
p	O
(	O
cid:3	O
)	O
,	O
however	O
,	O
the	O
marginals	O
can	O
be	O
computed	O
e	O
(	O
cid:14	O
)	O
-	O
ciently	O
by	O
exploiting	O
the	O
factorization	O
of	O
p	O
(	O
cid:3	O
)	O
.	O
the	O
idea	O
of	O
how	O
this	O
e	O
(	O
cid:14	O
)	O
ciency	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
336	O
26	O
|	O
exact	O
marginalization	O
in	O
graphs	O
arises	O
is	O
well	O
illustrated	O
by	O
the	O
message-passing	B
examples	O
of	O
chapter	O
16.	O
the	O
sum	O
{	O
product	O
algorithm	O
that	O
we	O
now	O
review	O
is	O
a	O
generalization	B
of	O
message-	O
passing	O
rule-set	O
b	O
(	O
p.242	O
)	O
.	O
as	O
was	O
the	O
case	O
there	O
,	O
the	O
sum	O
{	O
product	O
algorithm	O
is	O
only	O
valid	O
if	O
the	O
graph	B
is	O
tree-like	O
.	O
26.2	O
the	O
sum	O
{	O
product	O
algorithm	O
notation	B
we	O
identify	O
the	O
set	B
of	O
variables	O
that	O
the	O
mth	O
factor	O
depends	O
on	O
,	O
xm	O
,	O
by	O
the	O
set	B
of	O
their	O
indices	O
n	O
(	O
m	O
)	O
.	O
for	O
our	O
example	O
function	B
(	O
26.4	O
)	O
,	O
the	O
sets	O
are	O
n	O
(	O
1	O
)	O
=	O
f1g	O
(	O
since	O
f1	O
is	O
a	O
function	B
of	O
x1	O
alone	O
)	O
,	O
n	O
(	O
2	O
)	O
=	O
f2g	O
,	O
n	O
(	O
3	O
)	O
=	O
f3g	O
,	O
n	O
(	O
4	O
)	O
=	O
f1	O
;	O
2g	O
,	O
and	O
n	O
(	O
5	O
)	O
=	O
f2	O
;	O
3g	O
.	O
similarly	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
set	B
of	O
factors	O
in	O
which	O
variable	O
n	O
participates	O
,	O
by	O
m	O
(	O
n	O
)	O
.	O
we	O
denote	O
a	O
set	B
n	O
(	O
m	O
)	O
with	O
variable	O
n	O
excluded	O
by	O
n	O
(	O
m	O
)	O
nn	O
.	O
we	O
introduce	O
the	O
shorthand	O
xmnn	O
or	O
xmnn	O
to	O
denote	O
the	O
set	B
of	O
variables	O
in	O
xm	O
with	O
xn	O
excluded	O
,	O
i.e.	O
,	O
xmnn	O
(	O
cid:17	O
)	O
fxn0	O
:	O
n0	O
2	O
n	O
(	O
m	O
)	O
nng	O
:	O
(	O
26.10	O
)	O
the	O
sum	O
{	O
product	O
algorithm	O
will	O
involve	O
messages	O
of	O
two	O
types	O
passing	O
along	O
the	O
edges	O
in	O
the	O
factor	B
graph	I
:	O
messages	O
qn	O
!	O
m	O
from	O
variable	O
nodes	O
to	O
factor	O
nodes	O
,	O
and	O
messages	O
rm	O
!	O
n	O
from	O
factor	O
nodes	O
to	O
variable	O
nodes	O
.	O
a	O
message	O
(	O
of	O
either	O
type	O
,	O
q	O
or	O
r	O
)	O
that	O
is	O
sent	O
along	O
an	O
edge	B
connecting	O
factor	O
fm	O
to	O
variable	O
xn	O
is	O
always	O
a	O
function	B
of	O
the	O
variable	O
xn	O
.	O
here	O
are	O
the	O
two	O
rules	B
for	O
the	O
updating	O
of	O
the	O
two	O
sets	O
of	O
messages	O
.	O
from	O
variable	O
to	O
factor	O
:	O
qn	O
!	O
m	O
(	O
xn	O
)	O
=	O
ym02m	O
(	O
n	O
)	O
nm	O
rm0	O
!	O
n	O
(	O
xn	O
)	O
:	O
(	O
26.11	O
)	O
from	O
factor	O
to	O
variable	O
:	O
rm	O
!	O
n	O
(	O
xn	O
)	O
=	O
xxmnn	O
0	O
@	O
fm	O
(	O
xm	O
)	O
yn02n	O
(	O
m	O
)	O
nn	O
qn0	O
!	O
m	O
(	O
xn0	O
)	O
1	O
a	O
:	O
xn	O
rm	O
!	O
n	O
(	O
xn	O
)	O
=	O
fm	O
(	O
xn	O
)	O
(	O
26.12	O
)	O
fm	O
how	O
these	O
rules	B
apply	O
to	O
leaves	O
in	O
the	O
factor	B
graph	I
a	O
node	O
that	O
has	O
only	O
one	O
edge	B
connecting	O
it	O
to	O
another	O
node	O
is	O
called	O
a	O
leaf	B
node	O
.	O
some	O
factor	O
nodes	O
in	O
the	O
graph	B
may	O
be	O
connected	O
to	O
only	O
one	O
vari-	O
able	O
node	O
,	O
in	O
which	O
case	O
the	O
set	B
n	O
(	O
m	O
)	O
nn	O
of	O
variables	O
appearing	O
in	O
the	O
fac-	O
tor	O
message	O
update	O
(	O
26.12	O
)	O
is	O
an	O
empty	O
set	O
,	O
and	O
the	O
product	O
of	O
functions	B
qn02n	O
(	O
m	O
)	O
nn	O
qn0	O
!	O
m	O
(	O
xn0	O
)	O
is	O
the	O
empty	O
product	O
,	O
whose	O
value	O
is	O
1.	O
such	O
a	O
fac-	O
tor	O
node	O
therefore	O
always	O
broadcasts	O
to	O
its	O
one	O
neighbour	O
xn	O
the	O
message	O
rm	O
!	O
n	O
(	O
xn	O
)	O
=	O
fm	O
(	O
xn	O
)	O
.	O
similarly	O
,	O
there	O
may	O
be	O
variable	O
nodes	O
that	O
are	O
connected	O
to	O
only	O
one	O
factor	O
node	O
,	O
so	O
the	O
set	B
m	O
(	O
n	O
)	O
nm	O
in	O
(	O
26.11	O
)	O
is	O
empty	O
.	O
these	O
nodes	O
perpetually	O
broadcast	B
the	O
message	O
qn	O
!	O
m	O
(	O
xn	O
)	O
=	O
1.	O
starting	O
and	O
(	O
cid:12	O
)	O
nishing	O
,	O
method	B
1	O
the	O
algorithm	B
can	O
be	O
initialized	O
in	O
two	O
ways	O
.	O
if	O
the	O
graph	B
is	O
tree-like	O
then	O
it	O
must	O
have	O
nodes	O
that	O
are	O
leaves	O
.	O
these	O
leaf	B
nodes	O
can	O
broadcast	B
their	O
figure	O
26.2.	O
a	O
factor	O
node	O
that	O
is	O
a	O
leaf	B
node	O
perpetually	O
sends	O
the	O
message	O
rm	O
!	O
n	O
(	O
xn	O
)	O
=	O
fm	O
(	O
xn	O
)	O
to	O
its	O
one	O
neighbour	O
xn	O
.	O
xn	O
qn	O
!	O
m	O
(	O
xn	O
)	O
=	O
1	O
fm	O
figure	O
26.3.	O
a	O
variable	O
node	O
that	O
is	O
a	O
leaf	B
node	O
perpetually	O
sends	O
the	O
message	O
qn	O
!	O
m	O
(	O
xn	O
)	O
=	O
1	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
26.2	O
:	O
the	O
sum	O
{	O
product	O
algorithm	O
337	O
messages	O
to	O
their	O
respective	O
neighbours	O
from	O
the	O
start	O
.	O
for	O
all	O
leaf	B
variable	O
nodes	O
n	O
:	O
for	O
all	O
leaf	B
factor	O
nodes	O
m	O
:	O
qn	O
!	O
m	O
(	O
xn	O
)	O
=	O
1	O
rm	O
!	O
n	O
(	O
xn	O
)	O
=	O
fm	O
(	O
xn	O
)	O
:	O
(	O
26.13	O
)	O
(	O
26.14	O
)	O
we	O
can	O
then	O
adopt	O
the	O
procedure	O
used	O
in	O
chapter	O
16	O
’	O
s	O
message-passing	B
rule-	O
set	B
b	O
(	O
p.242	O
)	O
:	O
a	O
message	O
is	O
created	O
in	O
accordance	O
with	O
the	O
rules	B
(	O
26.11	O
,	O
26.12	O
)	O
only	O
if	O
all	O
the	O
messages	O
on	O
which	O
it	O
depends	O
are	O
present	O
.	O
for	O
example	O
,	O
in	O
(	O
cid:12	O
)	O
gure	O
26.4	O
,	O
the	O
message	O
from	O
x1	O
to	O
f1	O
will	O
be	O
sent	O
only	O
when	O
the	O
message	O
from	O
f4	O
to	O
x1	O
has	O
been	O
received	O
;	O
and	O
the	O
message	O
from	O
x2	O
to	O
f2	O
,	O
q2	O
!	O
2	O
,	O
can	O
be	O
sent	O
only	O
when	O
the	O
messages	O
r4	O
!	O
2	O
and	O
r5	O
!	O
2	O
have	O
both	O
been	O
received	O
.	O
messages	O
will	O
thus	O
(	O
cid:13	O
)	O
ow	O
through	O
the	O
tree	B
,	O
one	O
in	O
each	O
direction	O
along	O
every	O
edge	B
,	O
and	O
after	O
a	O
number	O
of	O
steps	O
equal	O
to	O
the	O
diameter	O
of	O
the	O
graph	O
,	O
every	O
message	O
will	O
have	O
been	O
created	O
.	O
the	O
answers	O
we	O
require	O
can	O
then	O
be	O
read	O
out	O
.	O
the	O
marginal	B
function	O
of	O
xn	O
is	O
obtained	O
by	O
multiplying	O
all	O
the	O
incoming	O
messages	O
at	O
that	O
node	O
.	O
zn	O
(	O
xn	O
)	O
=	O
ym2m	O
(	O
n	O
)	O
rm	O
!	O
n	O
(	O
xn	O
)	O
:	O
(	O
26.15	O
)	O
g	O
g	O
x1	O
x2	O
x3	O
@	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
@	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
f5	O
f4	O
f3	O
f2	O
f1	O
g	O
figure	O
26.4.	O
our	O
model	B
factor	O
graph	B
for	O
the	O
function	B
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
(	O
26.4	O
)	O
.	O
the	O
normalizing	B
constant	I
z	O
can	O
be	O
obtained	O
by	O
summing	O
any	O
marginal	B
function	O
,	O
z	O
=pxn	O
zn	O
(	O
xn	O
)	O
,	O
and	O
the	O
normalized	O
marginals	O
obtained	O
from	O
zn	O
(	O
xn	O
)	O
:	O
z	O
pn	O
(	O
xn	O
)	O
=	O
(	O
26.16	O
)	O
.	O
exercise	O
26.2	O
.	O
[	O
2	O
]	O
apply	O
the	O
sum	O
{	O
product	O
algorithm	O
to	O
the	O
function	B
de	O
(	O
cid:12	O
)	O
ned	O
in	O
equation	O
(	O
26.4	O
)	O
and	O
(	O
cid:12	O
)	O
gure	O
26.1.	O
check	O
that	O
the	O
normalized	O
marginals	O
are	O
consistent	O
with	O
what	O
you	O
know	O
about	O
the	O
repetition	B
code	I
r3	O
.	O
exercise	O
26.3	O
.	O
[	O
3	O
]	O
prove	O
that	O
the	O
sum	O
{	O
product	O
algorithm	O
correctly	O
computes	O
the	O
marginal	B
functions	O
zn	O
(	O
xn	O
)	O
if	O
the	O
graph	B
is	O
tree-like	O
.	O
exercise	O
26.4	O
.	O
[	O
3	O
]	O
describe	O
how	O
to	O
use	O
the	O
messages	O
computed	O
by	O
the	O
sum	O
{	O
product	O
algorithm	O
to	O
obtain	O
more	O
complicated	O
marginal	B
functions	O
in	O
a	O
tree-like	O
graph	B
,	O
for	O
example	O
z1	O
;	O
2	O
(	O
x1	O
;	O
x2	O
)	O
,	O
for	O
two	O
variables	O
x1	O
and	O
x2	O
that	O
are	O
connected	O
to	O
one	O
common	O
factor	O
node	O
.	O
starting	O
and	O
(	O
cid:12	O
)	O
nishing	O
,	O
method	B
2	O
alternatively	O
,	O
the	O
algorithm	B
can	O
be	O
initialized	O
by	O
setting	O
all	O
the	O
initial	O
mes-	O
sages	O
from	O
variables	O
to	O
1	O
:	O
for	O
all	O
n	O
,	O
m	O
:	O
qn	O
!	O
m	O
(	O
xn	O
)	O
=	O
1	O
;	O
(	O
26.17	O
)	O
then	O
proceeding	O
with	O
the	O
factor	O
message	O
update	O
rule	O
(	O
26.12	O
)	O
,	O
alternating	O
with	O
the	O
variable	O
message	O
update	O
rule	O
(	O
26.11	O
)	O
.	O
compared	O
with	O
method	O
1	O
,	O
this	O
lazy	O
initialization	O
method	B
leads	O
to	O
a	O
load	O
of	O
wasted	O
computations	O
,	O
whose	O
results	O
are	O
gradually	O
(	O
cid:13	O
)	O
ushed	O
out	O
by	O
the	O
correct	O
answers	O
computed	O
by	O
method	O
1.	O
after	O
a	O
number	O
of	O
iterations	O
equal	O
to	O
the	O
diameter	O
of	O
the	O
factor	O
graph	B
,	O
the	O
algorithm	B
will	O
converge	O
to	O
a	O
set	B
of	O
messages	O
satisfying	O
the	O
sum	O
{	O
product	O
relationships	O
(	O
26.11	O
,	O
26.12	O
)	O
.	O
exercise	O
26.5	O
.	O
[	O
2	O
]	O
apply	O
this	O
second	O
version	O
of	O
the	O
sum	O
{	O
product	O
algorithm	O
to	O
the	O
function	B
de	O
(	O
cid:12	O
)	O
ned	O
in	O
equation	O
(	O
26.4	O
)	O
and	O
(	O
cid:12	O
)	O
gure	O
26.1.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
338	O
26	O
|	O
exact	O
marginalization	O
in	O
graphs	O
the	O
reason	O
for	O
introducing	O
this	O
lazy	O
method	B
is	O
that	O
(	O
unlike	O
method	B
1	O
)	O
it	O
can	O
be	O
applied	O
to	O
graphs	O
that	O
are	O
not	O
tree-like	O
.	O
when	O
the	O
sum	O
{	O
product	O
algorithm	O
is	O
run	O
on	O
a	O
graph	B
with	O
cycles	O
,	O
the	O
algorithm	B
does	O
not	O
necessarily	O
converge	O
,	O
and	O
certainly	O
does	O
not	O
in	O
general	O
compute	O
the	O
correct	O
marginal	B
functions	O
;	O
but	O
it	O
is	O
nevertheless	O
an	O
algorithm	B
of	O
great	O
practical	B
importance	O
,	O
especially	O
in	O
the	O
decoding	B
of	O
sparse-graph	O
codes	O
.	O
sum	O
{	O
product	O
algorithm	O
with	O
on-the-	O
(	O
cid:13	O
)	O
y	O
normalization	O
if	O
we	O
are	O
interested	O
in	O
only	O
the	O
normalized	O
marginals	O
,	O
then	O
another	O
version	O
of	O
the	O
sum	O
{	O
product	O
algorithm	O
may	O
be	O
useful	O
.	O
the	O
factor-to-variable	O
messages	O
rm	O
!	O
n	O
are	O
computed	O
in	O
just	O
the	O
same	O
way	O
(	O
26.12	O
)	O
,	O
but	O
the	O
variable-to-factor	O
messages	O
are	O
normalized	O
thus	O
:	O
qn	O
!	O
m	O
(	O
xn	O
)	O
=	O
(	O
cid:11	O
)	O
nm	O
ym02m	O
(	O
n	O
)	O
nm	O
rm0	O
!	O
n	O
(	O
xn	O
)	O
where	O
(	O
cid:11	O
)	O
nm	O
is	O
a	O
scalar	O
chosen	O
such	O
that	O
qn	O
!	O
m	O
(	O
xn	O
)	O
=	O
1	O
:	O
xxn	O
(	O
26.18	O
)	O
(	O
26.19	O
)	O
exercise	O
26.6	O
.	O
[	O
2	O
]	O
apply	O
this	O
normalized	O
version	O
of	O
the	O
sum	O
{	O
product	O
algorithm	O
to	O
the	O
function	B
de	O
(	O
cid:12	O
)	O
ned	O
in	O
equation	O
(	O
26.4	O
)	O
and	O
(	O
cid:12	O
)	O
gure	O
26.1.	O
a	O
factorization	O
view	O
of	O
the	O
sum	O
{	O
product	O
algorithm	O
one	O
way	O
to	O
view	O
the	O
sum	O
{	O
product	O
algorithm	O
is	O
that	O
it	O
reexpresses	O
the	O
original	O
m=1	O
fm	O
(	O
xm	O
)	O
,	O
as	O
another	O
factored	O
function	B
,	O
the	O
product	O
of	O
m	O
factors	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=qm	O
factored	O
function	B
which	O
is	O
the	O
product	O
of	O
m	O
+	O
n	O
factors	O
,	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=	O
m	O
ym=1	O
(	O
cid:30	O
)	O
m	O
(	O
xm	O
)	O
n	O
yn=1	O
n	O
(	O
xn	O
)	O
:	O
(	O
26.20	O
)	O
each	O
factor	O
(	O
cid:30	O
)	O
m	O
is	O
associated	O
with	O
a	O
factor	O
node	O
m	O
,	O
and	O
each	O
factor	O
n	O
(	O
xn	O
)	O
is	O
associated	O
with	O
a	O
variable	O
node	O
.	O
initially	O
(	O
cid:30	O
)	O
m	O
(	O
xm	O
)	O
=	O
fm	O
(	O
xm	O
)	O
and	O
n	O
(	O
xn	O
)	O
=	O
1.	O
each	O
time	O
a	O
factor-to-variable	O
message	O
rm	O
!	O
n	O
(	O
xn	O
)	O
is	O
sent	O
,	O
the	O
factorization	O
is	O
updated	O
thus	O
:	O
n	O
(	O
xn	O
)	O
=	O
ym2m	O
(	O
n	O
)	O
rm	O
!	O
n	O
(	O
xn	O
)	O
(	O
26.21	O
)	O
(	O
cid:30	O
)	O
m	O
(	O
xm	O
)	O
=	O
:	O
(	O
26.22	O
)	O
and	O
each	O
message	O
can	O
be	O
computed	O
in	O
terms	O
of	O
(	O
cid:30	O
)	O
and	O
using	O
f	O
(	O
xm	O
)	O
qn2n	O
(	O
m	O
)	O
rm	O
!	O
n	O
(	O
xn	O
)	O
0	O
@	O
(	O
cid:30	O
)	O
m	O
(	O
xm	O
)	O
yn02n	O
(	O
m	O
)	O
n0	O
(	O
xn0	O
)	O
1	O
a	O
rm	O
!	O
n	O
(	O
xn	O
)	O
=	O
xxmnn	O
(	O
26.23	O
)	O
which	O
di	O
(	O
cid:11	O
)	O
ers	O
from	O
the	O
assignment	O
(	O
26.12	O
)	O
in	O
that	O
the	O
product	O
is	O
over	O
all	O
n0	O
2	O
n	O
(	O
m	O
)	O
.	O
exercise	O
26.7	O
.	O
[	O
2	O
]	O
con	O
(	O
cid:12	O
)	O
rm	O
that	O
the	O
update	O
rules	B
(	O
26.21	O
{	O
26.23	O
)	O
are	O
equivalent	O
to	O
the	O
sum	O
{	O
product	O
rules	O
(	O
26.11	O
{	O
26.12	O
)	O
.	O
so	O
n	O
(	O
xn	O
)	O
eventually	O
becomes	O
the	O
marginal	B
zn	O
(	O
xn	O
)	O
.	O
this	O
factorization	O
viewpoint	O
applies	O
whether	O
or	O
not	O
the	O
graph	B
is	O
tree-like	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
339	O
26.3	O
:	O
the	O
min	O
{	O
sum	O
algorithm	O
computational	O
tricks	O
on-the-	O
(	O
cid:13	O
)	O
y	O
normalization	O
is	O
a	O
good	B
idea	O
from	O
a	O
computational	O
point	O
of	O
view	O
because	O
if	O
p	O
(	O
cid:3	O
)	O
is	O
a	O
product	O
of	O
many	O
factors	O
,	O
its	O
values	O
are	O
likely	O
to	O
be	O
very	O
large	O
or	O
very	O
small	O
.	O
another	O
useful	O
computational	O
trick	O
involves	O
passing	O
the	O
logarithms	B
of	O
the	O
messages	O
q	O
and	O
r	O
instead	O
of	O
q	O
and	O
r	O
themselves	O
;	O
the	O
computations	O
of	O
the	O
products	O
in	O
the	O
algorithm	B
(	O
26.11	O
,	O
26.12	O
)	O
are	O
then	O
replaced	O
by	O
simpler	O
additions	O
.	O
the	O
summations	O
in	O
(	O
26.12	O
)	O
of	O
course	O
become	O
more	O
di	O
(	O
cid:14	O
)	O
cult	O
:	O
to	O
carry	O
them	O
out	O
and	O
return	O
the	O
logarithm	O
,	O
we	O
need	O
to	O
compute	O
softmax	B
functions	O
like	O
l	O
=	O
ln	O
(	O
el1	O
+	O
el2	O
+	O
el3	O
)	O
:	O
(	O
26.24	O
)	O
but	O
this	O
computation	O
can	O
be	O
done	O
e	O
(	O
cid:14	O
)	O
ciently	O
using	O
look-up	O
tables	O
along	O
with	O
the	O
observation	O
that	O
the	O
value	O
of	O
the	O
answer	O
l	O
is	O
typically	O
just	O
a	O
little	O
larger	O
than	O
maxi	O
li	O
.	O
if	O
we	O
store	O
in	O
look-up	O
tables	O
values	O
of	O
the	O
function	O
ln	O
(	O
1	O
+	O
e	O
(	O
cid:14	O
)	O
)	O
(	O
26.25	O
)	O
(	O
for	O
negative	O
(	O
cid:14	O
)	O
)	O
then	O
l	O
can	O
be	O
computed	O
exactly	O
in	O
a	O
number	O
of	O
look-ups	O
and	O
additions	O
scaling	B
as	O
the	O
number	O
of	O
terms	O
in	O
the	O
sum	O
.	O
if	O
look-ups	O
and	O
sorting	O
operations	O
are	O
cheaper	O
than	O
exp	O
(	O
)	O
then	O
this	O
approach	O
costs	O
less	O
than	O
the	O
direct	O
evaluation	O
(	O
26.24	O
)	O
.	O
the	O
number	O
of	O
operations	O
can	O
be	O
further	O
reduced	O
by	O
omitting	O
negligible	O
contributions	O
from	O
the	O
smallest	O
of	O
the	O
flig	O
.	O
a	O
third	O
computational	O
trick	O
applicable	O
to	O
certain	O
error-correcting	B
codes	I
is	O
to	O
pass	O
not	O
the	O
messages	O
but	O
the	O
fourier	O
transforms	O
of	O
the	O
messages	O
.	O
this	O
again	O
makes	O
the	O
computations	O
of	O
the	O
factor-to-variable	O
messages	O
quicker	O
.	O
a	O
simple	O
example	O
of	O
this	O
fourier	O
transform	O
trick	O
is	O
given	O
in	O
chapter	O
47	O
at	O
equa-	O
tion	O
(	O
47.9	O
)	O
.	O
26.3	O
the	O
min	O
{	O
sum	O
algorithm	O
the	O
sum	O
{	O
product	O
algorithm	O
solves	O
the	O
problem	O
of	O
(	O
cid:12	O
)	O
nding	O
the	O
marginal	B
func-	O
tion	O
of	O
a	O
given	O
product	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
this	O
is	O
analogous	O
to	O
solving	O
the	O
bitwise	B
decod-	O
ing	O
problem	O
of	O
section	O
25.1.	O
and	O
just	O
as	O
there	O
were	O
other	O
decoding	B
problems	O
(	O
for	O
example	O
,	O
the	O
codeword	B
decoding	O
problem	O
)	O
,	O
we	O
can	O
de	O
(	O
cid:12	O
)	O
ne	O
other	O
tasks	O
involving	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
that	O
can	O
be	O
solved	O
by	O
modi	O
(	O
cid:12	O
)	O
cations	O
of	O
the	O
sum	O
{	O
product	O
algo-	O
rithm	O
.	O
for	O
example	O
,	O
consider	O
this	O
task	O
,	O
analogous	O
to	O
the	O
codeword	B
decoding	O
problem	O
:	O
the	O
maximization	O
problem	O
.	O
find	O
the	O
setting	O
of	O
x	O
that	O
maximizes	O
the	O
product	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
this	O
problem	O
can	O
be	O
solved	O
by	O
replacing	O
the	O
two	O
operations	O
add	O
and	O
mul-	O
tiply	O
everywhere	O
they	O
appear	O
in	O
the	O
sum	O
{	O
product	O
algorithm	O
by	O
another	O
pair	O
of	O
operations	O
that	O
satisfy	O
the	O
distributive	O
law	O
,	O
namely	O
max	O
and	O
multiply	O
.	O
if	O
we	O
replace	O
summation	O
(	O
+	O
,	O
p	O
)	O
by	O
maximization	O
,	O
we	O
notice	O
that	O
the	O
quantity	O
formerly	O
known	O
as	O
the	O
normalizing	B
constant	I
,	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
;	O
(	O
26.26	O
)	O
z	O
=xx	O
becomes	O
maxx	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
thus	O
the	O
sum	O
{	O
product	O
algorithm	O
can	O
be	O
turned	O
into	O
a	O
max	O
{	O
product	O
algo-	O
rithm	O
that	O
computes	O
maxx	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
,	O
and	O
from	O
which	O
the	O
solution	O
of	O
the	O
max-	O
imization	O
problem	O
can	O
be	O
deduced	O
.	O
each	O
‘	O
marginal	B
’	O
zn	O
(	O
xn	O
)	O
then	O
lists	O
the	O
maximum	O
value	O
that	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
can	O
attain	O
for	O
each	O
value	O
of	O
xn	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
340	O
26	O
|	O
exact	O
marginalization	O
in	O
graphs	O
in	O
practice	O
,	O
the	O
max	O
{	O
product	O
algorithm	O
is	O
most	O
often	O
carried	O
out	O
in	O
the	O
negative	O
log	O
likelihood	B
domain	O
,	O
where	O
max	O
and	O
product	O
become	O
min	O
and	O
sum	O
.	O
the	O
min	O
{	O
sum	O
algorithm	O
is	O
also	O
known	O
as	O
the	O
viterbi	O
algorithm	B
.	O
26.4	O
the	O
junction	B
tree	I
algorithm	I
what	O
should	O
one	O
do	O
when	O
the	O
factor	B
graph	I
one	O
is	O
interested	O
in	O
is	O
not	O
a	O
tree	B
?	O
there	O
are	O
several	O
options	O
,	O
and	O
they	O
divide	O
into	O
exact	O
methods	O
and	O
approx-	O
imate	O
methods	B
.	O
the	O
most	O
widely	O
used	O
exact	O
method	O
for	O
handling	O
marginaliza-	O
tion	O
on	O
graphs	O
with	O
cycles	O
is	O
called	O
the	O
junction	B
tree	I
algorithm	I
.	O
this	O
algorithm	B
works	O
by	O
agglomerating	O
variables	O
together	O
until	O
the	O
agglomerated	O
graph	B
has	O
no	O
cycles	O
.	O
you	O
can	O
probably	O
(	O
cid:12	O
)	O
gure	O
out	O
the	O
details	O
for	O
yourself	O
;	O
the	O
complexity	B
of	O
the	O
marginalization	B
grows	O
exponentially	O
with	O
the	O
number	O
of	O
agglomerated	O
variables	O
.	O
read	O
more	O
about	O
the	O
junction	B
tree	I
algorithm	I
in	O
(	O
lauritzen	O
,	O
1996	O
;	O
jordan	O
,	O
1998	O
)	O
.	O
there	O
are	O
many	O
approximate	O
methods	B
,	O
and	O
we	O
’	O
ll	O
visit	O
some	O
of	O
them	O
over	O
the	O
next	O
few	O
chapters	O
{	O
monte	O
carlo	O
methods	B
and	O
variational	B
methods	I
,	O
to	O
name	O
a	O
couple	O
.	O
however	O
,	O
the	O
most	O
amusing	O
way	O
of	O
handling	O
factor	O
graphs	O
to	O
which	O
the	O
sum	O
{	O
product	O
algorithm	O
may	O
not	O
be	O
applied	O
is	O
,	O
as	O
we	O
already	O
mentioned	O
,	O
to	O
apply	O
the	O
sum	O
{	O
product	O
algorithm	O
!	O
we	O
simply	O
compute	O
the	O
messages	O
for	O
each	O
node	O
in	O
the	O
graph	B
,	O
as	O
if	O
the	O
graph	B
were	O
a	O
tree	B
,	O
iterate	O
,	O
and	O
cross	O
our	O
(	O
cid:12	O
)	O
ngers	O
.	O
this	O
so-called	O
‘	O
loopy	B
’	O
message	B
passing	I
has	O
great	O
importance	O
in	O
the	O
decoding	B
of	O
error-correcting	B
codes	I
,	O
and	O
we	O
’	O
ll	O
come	O
back	O
to	O
it	O
in	O
section	O
33.8	O
and	O
part	O
vi	O
.	O
further	O
reading	O
for	O
further	O
reading	O
about	O
factor	O
graphs	O
and	O
the	O
sum	O
{	O
product	O
algorithm	O
,	O
see	O
kschischang	O
et	O
al	O
.	O
(	O
2001	O
)	O
,	O
yedidia	O
et	O
al	O
.	O
(	O
2000	O
)	O
,	O
yedidia	O
et	O
al	O
.	O
(	O
2001a	O
)	O
,	O
yedidia	O
et	O
al	O
.	O
(	O
2002	O
)	O
,	O
wainwright	O
et	O
al	O
.	O
(	O
2003	O
)	O
,	O
and	O
forney	O
(	O
2001	O
)	O
.	O
see	O
also	O
pearl	O
(	O
1988	O
)	O
.	O
a	O
good	B
reference	O
for	O
the	O
fundamental	O
theory	B
of	O
graphical	O
models	O
is	O
lauritzen	O
(	O
1996	O
)	O
.	O
a	O
readable	O
introduction	O
to	O
bayesian	O
networks	O
is	O
given	O
by	O
jensen	O
(	O
1996	O
)	O
.	O
interesting	O
message-passing	B
algorithms	O
that	O
have	O
di	O
(	O
cid:11	O
)	O
erent	O
capabilities	O
from	O
the	O
sum	O
{	O
product	O
algorithm	O
include	O
expectation	B
propagation	I
(	O
minka	O
,	O
2001	O
)	O
and	O
survey	O
propagation	O
(	O
braunstein	O
et	O
al.	O
,	O
2003	O
)	O
.	O
see	O
also	O
section	B
33.8	O
.	O
26.5	O
exercises	O
.	O
exercise	O
26.8	O
.	O
[	O
2	O
]	O
express	O
the	O
joint	B
probability	O
distribution	B
from	O
the	O
burglar	B
alarm	I
and	I
earthquake	I
problem	O
(	O
example	O
21.1	O
(	O
p.293	O
)	O
)	O
as	O
a	O
factor	B
graph	I
,	O
and	O
(	O
cid:12	O
)	O
nd	O
the	O
marginal	B
probabilities	O
of	O
all	O
the	O
variables	O
as	O
each	O
piece	O
of	O
information	O
comes	O
to	O
fred	O
’	O
s	O
attention	O
,	O
using	O
the	O
sum	O
{	O
product	O
algorithm	O
with	O
on-the-	O
(	O
cid:13	O
)	O
y	O
normalization	O
.	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
27	O
laplace	O
’	O
s	O
method	B
the	O
idea	O
behind	O
the	O
laplace	O
approximation	B
is	O
simple	O
.	O
we	O
assume	O
that	O
an	O
unnormalized	O
probability	B
density	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
,	O
whose	O
normalizing	B
constant	I
zp	O
(	O
cid:17	O
)	O
z	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
dx	O
(	O
27.1	O
)	O
is	O
of	O
interest	O
,	O
has	O
a	O
peak	O
at	O
a	O
point	O
x0	O
.	O
we	O
taylor-expand	O
the	O
logarithm	O
of	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
around	O
this	O
peak	O
:	O
ln	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
’	O
ln	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
(	O
cid:0	O
)	O
c	O
2	O
(	O
x	O
(	O
cid:0	O
)	O
x0	O
)	O
2	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
;	O
where	O
c	O
=	O
(	O
cid:0	O
)	O
@	O
2	O
@	O
x2	O
ln	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
x=x0	O
:	O
we	O
then	O
approximate	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
by	O
an	O
unnormalized	O
gaussian	O
,	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
(	O
cid:17	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
exph	O
(	O
cid:0	O
)	O
c	O
2	O
(	O
x	O
(	O
cid:0	O
)	O
x0	O
)	O
2i	O
;	O
(	O
27.2	O
)	O
(	O
27.3	O
)	O
(	O
27.4	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
ln	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
ln	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
&	O
ln	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
&	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
and	O
we	O
approximate	O
the	O
normalizing	B
constant	I
zp	O
by	O
the	O
normalizing	B
constant	I
of	O
this	O
gaussian	O
,	O
zq	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
r	O
2	O
(	O
cid:25	O
)	O
c	O
:	O
(	O
27.5	O
)	O
we	O
can	O
generalize	O
this	O
integral	B
to	O
approximate	O
zp	O
for	O
a	O
density	B
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
over	O
a	O
k-dimensional	O
space	O
x.	O
if	O
the	O
matrix	B
of	O
second	O
derivatives	O
of	O
(	O
cid:0	O
)	O
ln	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
at	O
the	O
maximum	O
x0	O
is	O
a	O
,	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
:	O
aij	O
=	O
(	O
cid:0	O
)	O
@	O
2	O
@	O
xi	O
@	O
xj	O
ln	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
x=x0	O
so	O
that	O
the	O
expansion	O
(	O
27.2	O
)	O
is	O
generalized	B
to	O
;	O
(	O
27.6	O
)	O
ln	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
’	O
ln	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
x	O
(	O
cid:0	O
)	O
x0	O
)	O
ta	O
(	O
x	O
(	O
cid:0	O
)	O
x0	O
)	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
;	O
(	O
27.7	O
)	O
then	O
the	O
normalizing	B
constant	I
can	O
be	O
approximated	O
by	O
:	O
zp	O
’	O
zq	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
r	O
(	O
2	O
(	O
cid:25	O
)	O
)	O
k	O
det	O
a	O
:	O
(	O
27.8	O
)	O
predictions	O
can	O
be	O
made	O
using	O
the	O
approximation	B
q.	O
physicists	O
also	O
call	O
this	O
widely-used	O
approximation	B
the	O
saddle-point	B
approximation	I
.	O
1	O
qdet	O
1	O
2	O
(	O
cid:25	O
)	O
a	O
341	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
342	O
27	O
|	O
laplace	O
’	O
s	O
method	B
the	O
fact	O
that	O
the	O
normalizing	B
constant	I
of	O
a	O
gaussian	O
is	O
given	O
by	O
z	O
dkx	O
exp	O
(	O
cid:20	O
)	O
(	O
cid:0	O
)	O
1	O
2	O
xtax	O
(	O
cid:21	O
)	O
=r	O
(	O
2	O
(	O
cid:25	O
)	O
)	O
k	O
det	O
a	O
(	O
27.9	O
)	O
can	O
be	O
proved	O
by	O
making	O
an	O
orthogonal	O
transformation	O
into	O
the	O
basis	O
u	O
in	O
which	O
a	O
is	O
transformed	O
into	O
a	O
diagonal	O
matrix	B
.	O
the	O
integral	B
then	O
separates	O
into	O
a	O
product	O
of	O
one-dimensional	O
integrals	O
,	O
each	O
of	O
the	O
form	O
z	O
dui	O
exp	O
(	O
cid:20	O
)	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
cid:21	O
)	O
iu2	O
i	O
(	O
cid:21	O
)	O
=r	O
2	O
(	O
cid:25	O
)	O
(	O
cid:21	O
)	O
i	O
:	O
(	O
27.10	O
)	O
the	O
product	O
of	O
the	O
eigenvalues	O
(	O
cid:21	O
)	O
i	O
is	O
the	O
determinant	O
of	O
a.	O
the	O
laplace	O
approximation	B
is	O
basis-dependent	O
:	O
if	O
x	O
is	O
transformed	O
to	O
a	O
nonlinear	B
function	O
u	O
(	O
x	O
)	O
and	O
the	O
density	B
is	O
transformed	O
to	O
p	O
(	O
u	O
)	O
=	O
p	O
(	O
x	O
)	O
jdx=duj	O
then	O
in	O
general	O
the	O
approximate	O
normalizing	O
constants	O
zq	O
will	O
be	O
di	O
(	O
cid:11	O
)	O
erent	O
.	O
this	O
can	O
be	O
viewed	O
as	O
a	O
defect	O
{	O
since	O
the	O
true	O
value	O
zp	O
is	O
basis-independent	O
{	O
or	O
an	O
opportunity	O
{	O
because	O
we	O
can	O
hunt	O
for	O
a	O
choice	O
of	O
basis	O
in	O
which	O
the	O
laplace	O
approximation	B
is	O
most	O
accurate	O
.	O
27.1	O
exercises	O
exercise	O
27.1	O
.	O
[	O
2	O
]	O
(	O
see	O
also	O
exercise	O
22.8	O
(	O
p.307	O
)	O
.	O
)	O
a	O
photon	B
counter	I
is	O
pointed	O
at	O
a	O
remote	O
star	O
for	O
one	O
minute	O
,	O
in	O
order	O
to	O
infer	O
the	O
rate	B
of	O
photons	O
arriving	O
at	O
the	O
counter	O
per	O
minute	O
,	O
(	O
cid:21	O
)	O
.	O
assuming	O
the	O
number	O
of	O
photons	O
collected	O
r	O
has	O
a	O
poisson	O
distribution	B
with	O
mean	B
(	O
cid:21	O
)	O
,	O
p	O
(	O
r	O
j	O
(	O
cid:21	O
)	O
)	O
=	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
)	O
(	O
cid:21	O
)	O
r	O
r	O
!	O
;	O
(	O
27.11	O
)	O
and	O
assuming	O
the	O
improper	B
prior	O
p	O
(	O
(	O
cid:21	O
)	O
)	O
=	O
1=	O
(	O
cid:21	O
)	O
,	O
make	O
laplace	O
approxima-	O
tions	O
to	O
the	O
posterior	O
distribution	O
(	O
a	O
)	O
over	O
(	O
cid:21	O
)	O
(	O
b	O
)	O
over	O
log	O
(	O
cid:21	O
)	O
.	O
constant	O
.	O
]	O
[	O
note	O
the	O
improper	B
prior	O
transforms	O
to	O
p	O
(	O
log	O
(	O
cid:21	O
)	O
)	O
=	O
.	O
exercise	O
27.2	O
.	O
[	O
2	O
]	O
use	O
laplace	O
’	O
s	O
method	B
to	O
approximate	O
the	O
integral	B
z	O
(	O
u1	O
;	O
u2	O
)	O
=z	O
1	O
(	O
cid:0	O
)	O
1	O
da	O
f	O
(	O
a	O
)	O
u1	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
(	O
a	O
)	O
)	O
u2	O
;	O
(	O
27.12	O
)	O
where	O
f	O
(	O
a	O
)	O
=	O
1=	O
(	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
a	O
)	O
and	O
u1	O
;	O
u2	O
are	O
positive	O
.	O
check	O
the	O
accuracy	O
of	O
the	O
approximation	O
against	O
the	O
exact	O
answer	O
(	O
23.29	O
,	O
p.316	O
)	O
for	O
(	O
u1	O
;	O
u2	O
)	O
=	O
(	O
1/2	O
;	O
1/2	O
)	O
and	O
(	O
u1	O
;	O
u2	O
)	O
=	O
(	O
1	O
;	O
1	O
)	O
.	O
measure	O
the	O
error	O
(	O
log	O
zp	O
(	O
cid:0	O
)	O
log	O
zq	O
)	O
in	O
bits	O
.	O
.	O
exercise	O
27.3	O
.	O
[	O
3	O
]	O
linear	B
regression	I
.	O
n	O
datapoints	O
f	O
(	O
x	O
(	O
n	O
)	O
;	O
t	O
(	O
n	O
)	O
)	O
g	O
are	O
generated	O
by	O
the	O
experimenter	O
choosing	O
each	O
x	O
(	O
n	O
)	O
,	O
then	O
the	O
world	O
delivering	O
a	O
noisy	B
version	O
of	O
the	O
linear	O
function	B
y	O
(	O
x	O
)	O
=	O
w0	O
+	O
w1x	O
;	O
t	O
(	O
n	O
)	O
(	O
cid:24	O
)	O
normal	B
(	O
y	O
(	O
x	O
(	O
n	O
)	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
)	O
:	O
(	O
27.13	O
)	O
(	O
27.14	O
)	O
assuming	O
gaussian	O
priors	O
on	O
w0	O
and	O
w1	O
,	O
make	O
the	O
laplace	O
approxima-	O
tion	O
to	O
the	O
posterior	O
distribution	O
of	O
w0	O
and	O
w1	O
(	O
which	O
is	O
exact	O
,	O
in	O
fact	O
)	O
and	O
obtain	O
the	O
predictive	B
distribution	I
for	O
the	O
next	O
datapoint	O
t	O
(	O
n+1	O
)	O
,	O
given	O
x	O
(	O
n+1	O
)	O
.	O
(	O
see	O
mackay	O
(	O
1992a	O
)	O
for	O
further	O
reading	O
.	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
28	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
                                          	O
	O
28.1	O
occam	O
’	O
s	O
razor	O
how	O
many	O
boxes	O
are	O
in	O
the	O
picture	O
(	O
(	O
cid:12	O
)	O
gure	O
28.1	O
)	O
?	O
in	O
particular	O
,	O
how	O
many	O
boxes	O
are	O
in	O
the	O
vicinity	O
of	O
the	O
tree	O
?	O
if	O
we	O
looked	O
with	O
x-ray	O
spectacles	O
,	O
would	O
we	O
see	O
one	O
or	O
two	O
boxes	O
behind	O
the	O
trunk	O
(	O
(	O
cid:12	O
)	O
gure	O
28.2	O
)	O
?	O
(	O
or	O
even	O
more	O
?	O
)	O
occam	O
’	O
s	O
razor	O
is	O
the	O
principle	O
that	O
states	O
a	O
preference	O
for	O
simple	O
theories	O
.	O
‘	O
accept	O
the	O
simplest	O
explanation	O
that	O
(	O
cid:12	O
)	O
ts	O
the	O
data	O
’	O
.	O
thus	O
according	O
to	O
occam	O
’	O
s	O
razor	O
,	O
we	O
should	O
deduce	O
that	O
there	O
is	O
only	O
one	O
box	B
behind	O
the	O
tree	B
.	O
is	O
this	O
an	O
ad	O
hoc	O
rule	B
of	I
thumb	I
?	O
or	O
is	O
there	O
a	O
convincing	O
reason	O
for	O
believing	O
there	O
is	O
most	O
likely	O
one	O
box	B
?	O
perhaps	O
your	O
intuition	O
likes	O
the	O
argument	O
‘	O
well	O
,	O
it	O
would	O
be	O
a	O
remarkable	O
coincidence	B
for	O
the	O
two	O
boxes	O
to	O
be	O
just	O
the	O
same	O
height	O
and	O
colour	O
as	O
each	O
other	O
’	O
.	O
if	O
we	O
wish	O
to	O
make	O
arti	O
(	O
cid:12	O
)	O
cial	O
intelligences	O
that	O
interpret	O
data	O
correctly	O
,	O
we	O
must	O
translate	O
this	O
intuitive	O
feeling	O
into	O
a	O
concrete	O
theory	B
.	O
motivations	O
for	O
occam	O
’	O
s	O
razor	O
if	O
several	O
explanations	O
are	O
compatible	O
with	O
a	O
set	B
of	O
observations	O
,	O
occam	O
’	O
s	O
razor	O
advises	O
us	O
to	O
buy	O
the	O
simplest	O
.	O
this	O
principle	O
is	O
often	O
advocated	O
for	O
one	O
of	O
two	O
reasons	O
:	O
the	O
(	O
cid:12	O
)	O
rst	O
is	O
aesthetic	O
(	O
‘	O
a	O
theory	B
with	O
mathematical	O
beauty	O
is	O
more	O
likely	O
to	O
be	O
correct	O
than	O
an	O
ugly	O
one	O
that	O
(	O
cid:12	O
)	O
ts	O
some	O
experimental	O
data	O
’	O
343	O
figure	O
28.1.	O
a	O
picture	O
to	O
be	O
interpreted	O
.	O
it	O
contains	O
a	O
tree	B
and	O
some	O
boxes	O
.	O
1	O
?	O
or	O
2	O
?	O
figure	O
28.2.	O
how	O
many	O
boxes	O
are	O
behind	O
the	O
tree	B
?	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
344	O
28	O
|	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
figure	O
28.3.	O
why	O
bayesian	O
inference	B
embodies	O
occam	O
’	O
s	O
razor	O
.	O
this	O
(	O
cid:12	O
)	O
gure	O
gives	O
the	O
basic	O
intuition	O
for	O
why	O
complex	B
models	O
can	O
turn	O
out	O
to	O
be	O
less	O
probable	O
.	O
the	O
horizontal	O
axis	O
represents	O
the	O
space	O
of	O
possible	O
data	O
sets	O
d.	O
bayes	O
’	O
theorem	B
rewards	O
models	O
in	O
proportion	O
to	O
how	O
much	O
they	O
predicted	O
the	O
data	O
that	O
occurred	O
.	O
these	O
predictions	O
are	O
quanti	O
(	O
cid:12	O
)	O
ed	O
by	O
a	O
normalized	O
probability	B
distribution	O
on	O
d.	O
this	O
probability	O
of	O
the	O
data	O
given	O
model	B
hi	O
,	O
p	O
(	O
d	O
jhi	O
)	O
,	O
is	O
called	O
the	O
evidence	B
for	O
hi	O
.	O
a	O
simple	O
model	O
h1	O
makes	O
only	O
a	O
limited	O
range	O
of	O
predictions	O
,	O
shown	O
by	O
p	O
(	O
d	O
jh1	O
)	O
;	O
a	O
more	O
powerful	O
model	B
h2	O
,	O
that	O
has	O
,	O
for	O
example	O
,	O
more	O
free	O
parameters	O
than	O
h1	O
,	O
is	O
able	O
to	O
predict	O
a	O
greater	O
variety	O
of	O
data	O
sets	O
.	O
this	O
means	O
,	O
however	O
,	O
that	O
h2	O
does	O
not	O
predict	O
the	O
data	O
sets	O
in	O
region	O
c1	O
as	O
strongly	O
as	O
h1	O
.	O
suppose	O
that	O
equal	O
prior	B
probabilities	O
have	O
been	O
assigned	O
to	O
the	O
two	O
models	O
.	O
then	O
,	O
if	O
the	O
data	B
set	I
falls	O
in	O
region	O
c1	O
,	O
the	O
less	O
powerful	O
model	B
h1	O
will	O
be	O
the	O
more	O
probable	O
model	B
.	O
evidence	B
p	O
(	O
d|h	O
)	O
1	O
p	O
(	O
d|h	O
)	O
2	O
c	O
1	O
d	O
(	O
paul	O
dirac	O
)	O
)	O
;	O
the	O
second	O
reason	O
is	O
the	O
past	O
empirical	O
success	O
of	O
occam	O
’	O
s	O
razor	O
.	O
however	O
there	O
is	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
justi	O
(	O
cid:12	O
)	O
cation	O
for	O
occam	O
’	O
s	O
razor	O
,	O
namely	O
:	O
coherent	O
inference	B
(	O
as	O
embodied	O
by	O
bayesian	O
probability	B
)	O
auto-	O
matically	O
embodies	O
occam	O
’	O
s	O
razor	O
,	O
quantitatively	O
.	O
it	O
is	O
indeed	O
more	O
probable	O
that	O
there	O
’	O
s	O
one	O
box	B
behind	O
the	O
tree	B
,	O
and	O
we	O
can	O
compute	O
how	O
much	O
more	O
probable	O
one	O
is	O
than	O
two	O
.	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
we	O
evaluate	O
the	O
plausibility	O
of	O
two	O
alternative	O
theories	O
h1	O
and	O
h2	O
in	O
the	O
light	O
of	O
data	O
d	O
as	O
follows	O
:	O
using	O
bayes	O
’	O
theorem	B
,	O
we	O
relate	O
the	O
plausibility	O
of	O
model	O
h1	O
given	O
the	O
data	O
,	O
p	O
(	O
h1	O
j	O
d	O
)	O
,	O
to	O
the	O
predictions	O
made	O
by	O
the	O
model	B
about	O
the	O
data	O
,	O
p	O
(	O
d	O
jh1	O
)	O
,	O
and	O
the	O
prior	B
plausibility	O
of	O
h1	O
,	O
p	O
(	O
h1	O
)	O
.	O
this	O
gives	O
the	O
following	O
probability	B
ratio	O
between	O
theory	B
h1	O
and	O
theory	O
h2	O
:	O
:	O
(	O
28.1	O
)	O
p	O
(	O
h1	O
j	O
d	O
)	O
p	O
(	O
h2	O
j	O
d	O
)	O
=	O
p	O
(	O
h1	O
)	O
p	O
(	O
h2	O
)	O
p	O
(	O
d	O
jh1	O
)	O
p	O
(	O
d	O
jh2	O
)	O
the	O
(	O
cid:12	O
)	O
rst	O
ratio	O
(	O
p	O
(	O
h1	O
)	O
=p	O
(	O
h2	O
)	O
)	O
on	O
the	O
right-hand	O
side	O
measures	O
how	O
much	O
our	O
initial	O
beliefs	O
favoured	O
h1	O
over	O
h2	O
.	O
the	O
second	O
ratio	O
expresses	O
how	O
well	O
the	O
observed	O
data	O
were	O
predicted	O
by	O
h1	O
,	O
compared	O
to	O
h2	O
.	O
how	O
does	O
this	O
relate	O
to	O
occam	O
’	O
s	O
razor	O
,	O
when	O
h1	O
is	O
a	O
simpler	O
model	B
than	O
h2	O
?	O
the	O
(	O
cid:12	O
)	O
rst	O
ratio	O
(	O
p	O
(	O
h1	O
)	O
=p	O
(	O
h2	O
)	O
)	O
gives	O
us	O
the	O
opportunity	O
,	O
if	O
we	O
wish	O
,	O
to	O
insert	O
a	O
prior	B
bias	O
in	O
favour	O
of	O
h1	O
on	O
aesthetic	O
grounds	O
,	O
or	O
on	O
the	O
basis	O
of	O
experience	O
.	O
this	O
would	O
correspond	O
to	O
the	O
aesthetic	O
and	O
empirical	O
motivations	O
for	O
occam	O
’	O
s	O
razor	O
mentioned	O
earlier	O
.	O
but	O
such	O
a	O
prior	B
bias	O
is	O
not	O
necessary	O
:	O
the	O
second	O
ratio	O
,	O
the	O
data-dependent	O
factor	O
,	O
embodies	O
occam	O
’	O
s	O
razor	O
auto-	O
matically	O
.	O
simple	O
models	O
tend	O
to	O
make	O
precise	O
predictions	O
.	O
complex	B
models	O
,	O
by	O
their	O
nature	O
,	O
are	O
capable	O
of	O
making	O
a	O
greater	O
variety	O
of	O
predictions	O
(	O
(	O
cid:12	O
)	O
gure	O
28.3	O
)	O
.	O
so	O
if	O
h2	O
is	O
a	O
more	O
complex	B
model	O
,	O
it	O
must	O
spread	O
its	O
predictive	O
proba-	O
bility	O
p	O
(	O
d	O
jh2	O
)	O
more	O
thinly	O
over	O
the	O
data	O
space	O
than	O
h1	O
.	O
thus	O
,	O
in	O
the	O
case	O
where	O
the	O
data	O
are	O
compatible	O
with	O
both	O
theories	O
,	O
the	O
simpler	O
h1	O
will	O
turn	O
out	O
more	O
probable	O
than	O
h2	O
,	O
without	O
our	O
having	O
to	O
express	O
any	O
subjective	O
dislike	O
for	O
complex	O
models	O
.	O
our	O
subjective	O
prior	O
just	O
needs	O
to	O
assign	O
equal	O
prior	B
prob-	O
abilities	O
to	O
the	O
possibilities	O
of	O
simplicity	O
and	O
complexity	O
.	O
probability	B
theory	O
then	O
allows	O
the	O
observed	O
data	O
to	O
express	O
their	O
opinion	O
.	O
let	O
us	O
turn	O
to	O
a	O
simple	O
example	O
.	O
here	O
is	O
a	O
sequence	B
of	O
numbers	O
:	O
(	O
cid:0	O
)	O
1	O
;	O
3	O
;	O
7	O
;	O
11	O
:	O
the	O
task	O
is	O
to	O
predict	O
the	O
next	O
two	O
numbers	O
,	O
and	O
infer	O
the	O
underlying	O
process	O
that	O
gave	O
rise	O
to	O
this	O
sequence	B
.	O
a	O
popular	O
answer	O
to	O
this	O
question	O
is	O
the	O
prediction	B
‘	O
15	O
,	O
19	O
’	O
,	O
with	O
the	O
explanation	O
‘	O
add	O
4	O
to	O
the	O
previous	O
number	O
’	O
.	O
what	O
about	O
the	O
alternative	O
answer	O
‘	O
(	O
cid:0	O
)	O
19:9	O
;	O
1043:8	O
’	O
with	O
the	O
underlying	O
rule	O
being	O
:	O
‘	O
get	O
the	O
next	O
number	O
from	O
the	O
previous	O
number	O
,	O
x	O
,	O
by	O
evaluating	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
28.1	O
:	O
occam	O
’	O
s	O
razor	O
345	O
(	O
cid:0	O
)	O
x3=11	O
+	O
9=11x2	O
+	O
23=11	O
’	O
?	O
i	O
assume	O
that	O
this	O
prediction	B
seems	O
rather	O
less	O
plausible	O
.	O
but	O
the	O
second	O
rule	O
(	O
cid:12	O
)	O
ts	O
the	O
data	O
(	O
(	O
cid:0	O
)	O
1	O
,	O
3	O
,	O
7	O
,	O
11	O
)	O
just	O
as	O
well	O
as	O
the	O
rule	O
‘	O
add	O
4	O
’	O
.	O
so	O
why	O
should	O
we	O
(	O
cid:12	O
)	O
nd	O
it	O
less	O
plausible	O
?	O
let	O
us	O
give	O
labels	O
to	O
the	O
two	O
general	O
theories	O
:	O
ha	O
{	O
the	O
sequence	B
is	O
an	O
arithmetic	B
progression	I
,	O
‘	O
add	O
n	O
’	O
,	O
where	O
n	O
is	O
an	O
integer	O
.	O
hc	O
{	O
the	O
sequence	B
is	O
generated	O
by	O
a	O
cubic	O
function	B
of	O
the	O
form	O
x	O
!	O
cx3	O
+	O
dx2	O
+	O
e	O
,	O
where	O
c	O
,	O
d	O
and	O
e	O
are	O
fractions	O
.	O
one	O
reason	O
for	O
(	O
cid:12	O
)	O
nding	O
the	O
second	O
explanation	O
,	O
hc	O
,	O
less	O
plausible	O
,	O
might	O
be	O
that	O
arithmetic	O
progressions	O
are	O
more	O
frequently	O
encountered	O
than	O
cubic	O
func-	O
tions	O
.	O
this	O
would	O
put	O
a	O
bias	B
in	O
the	O
prior	B
probability	O
ratio	O
p	O
(	O
ha	O
)	O
=p	O
(	O
hc	O
)	O
in	O
equation	O
(	O
28.1	O
)	O
.	O
but	O
let	O
us	O
give	O
the	O
two	O
theories	O
equal	O
prior	B
probabilities	O
,	O
and	O
concentrate	O
on	O
what	O
the	O
data	O
have	O
to	O
say	O
.	O
how	O
well	O
did	O
each	O
theory	B
predict	O
the	O
data	O
?	O
to	O
obtain	O
p	O
(	O
d	O
jha	O
)	O
we	O
must	O
specify	O
the	O
probability	B
distribution	O
that	O
each	O
model	B
assigns	O
to	O
its	O
parameters	B
.	O
first	O
,	O
ha	O
depends	O
on	O
the	O
added	O
integer	O
n	O
,	O
and	O
the	O
(	O
cid:12	O
)	O
rst	O
number	O
in	O
the	O
sequence	B
.	O
let	O
us	O
say	O
that	O
these	O
numbers	O
could	O
each	O
have	O
been	O
anywhere	O
between	O
(	O
cid:0	O
)	O
50	O
and	O
50.	O
then	O
since	O
only	O
the	O
pair	O
of	O
values	O
fn	O
=	O
4	O
,	O
(	O
cid:12	O
)	O
rst	O
number	O
=	O
(	O
cid:0	O
)	O
1g	O
give	O
rise	O
to	O
the	O
observed	O
data	O
d	O
=	O
(	O
(	O
cid:0	O
)	O
1	O
,	O
3	O
,	O
7	O
,	O
11	O
)	O
,	O
the	O
probability	O
of	O
the	O
data	O
,	O
given	O
ha	O
,	O
is	O
:	O
p	O
(	O
d	O
jha	O
)	O
=	O
1	O
101	O
1	O
101	O
=	O
0:00010	O
:	O
(	O
28.2	O
)	O
to	O
evaluate	O
p	O
(	O
d	O
jhc	O
)	O
,	O
we	O
must	O
similarly	O
say	O
what	O
values	O
the	O
fractions	O
c	O
;	O
d	O
and	O
e	O
might	O
take	O
on	O
.	O
[	O
i	O
choose	O
to	O
represent	O
these	O
numbers	O
as	O
fractions	O
rather	O
than	O
real	O
numbers	O
because	O
if	O
we	O
used	O
real	O
numbers	O
,	O
the	O
model	B
would	O
assign	O
,	O
relative	B
to	O
ha	O
,	O
an	O
in	O
(	O
cid:12	O
)	O
nitesimal	O
probability	B
to	O
d.	O
real	O
parameters	B
are	O
the	O
norm	O
however	O
,	O
and	O
are	O
assumed	O
in	O
the	O
rest	O
of	O
this	O
chapter	O
.	O
]	O
a	O
reasonable	O
prior	B
might	O
state	O
that	O
for	O
each	O
fraction	O
the	O
numerator	O
could	O
be	O
any	O
number	O
between	O
(	O
cid:0	O
)	O
50	O
and	O
50	O
,	O
and	O
the	O
denominator	O
is	O
any	O
number	O
between	O
1	O
and	O
50.	O
as	O
for	O
the	O
initial	O
value	O
in	O
the	O
sequence	B
,	O
let	O
us	O
leave	O
its	O
probability	B
distribution	O
the	O
same	O
as	O
in	O
ha	O
.	O
there	O
are	O
four	O
ways	O
of	O
expressing	O
the	O
fraction	O
c	O
=	O
(	O
cid:0	O
)	O
1=11	O
=	O
(	O
cid:0	O
)	O
2=22	O
=	O
(	O
cid:0	O
)	O
3=33	O
=	O
(	O
cid:0	O
)	O
4=44	O
under	O
this	O
prior	B
,	O
and	O
similarly	O
there	O
are	O
four	O
and	O
two	O
possible	O
solutions	O
for	O
d	O
and	O
e	O
,	O
respectively	O
.	O
so	O
the	O
probability	O
of	O
the	O
observed	O
data	O
,	O
given	O
hc	O
,	O
is	O
found	O
to	O
be	O
:	O
p	O
(	O
d	O
jhc	O
)	O
=	O
(	O
cid:18	O
)	O
1	O
50	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
2	O
=	O
0:0000000000025	O
=	O
2:5	O
(	O
cid:2	O
)	O
10	O
(	O
cid:0	O
)	O
12	O
:	O
101	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
4	O
1	O
50	O
(	O
cid:19	O
)	O
(	O
28.3	O
)	O
101	O
1	O
1	O
50	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
4	O
101	O
101	O
thus	O
comparing	O
p	O
(	O
d	O
jhc	O
)	O
with	O
p	O
(	O
d	O
jha	O
)	O
=	O
0:00010	O
,	O
even	O
if	O
our	O
prior	B
prob-	O
abilities	O
for	O
ha	O
and	O
hc	O
are	O
equal	O
,	O
the	O
odds	B
,	O
p	O
(	O
d	O
jha	O
)	O
:	O
p	O
(	O
d	O
jhc	O
)	O
,	O
in	O
favour	O
of	O
ha	O
over	O
hc	O
,	O
given	O
the	O
sequence	B
d	O
=	O
(	O
(	O
cid:0	O
)	O
1	O
,	O
3	O
,	O
7	O
,	O
11	O
)	O
,	O
are	O
about	O
forty	O
million	O
to	O
one	O
.	O
this	O
answer	O
depends	O
on	O
several	O
subjective	O
assumptions	O
;	O
in	O
particular	O
,	O
the	O
probability	B
assigned	O
to	O
the	O
free	O
parameters	O
n	O
,	O
c	O
,	O
d	O
,	O
e	O
of	O
the	O
theories	O
.	O
bayesians	O
make	O
no	O
apologies	O
for	O
this	O
:	O
there	O
is	O
no	O
such	O
thing	O
as	B
inference	I
or	O
prediction	B
without	O
assumptions	B
.	O
however	O
,	O
the	O
quantitative	O
details	O
of	O
the	O
prior	O
proba-	O
bilities	O
have	O
no	O
e	O
(	O
cid:11	O
)	O
ect	O
on	O
the	O
qualitative	O
occam	O
’	O
s	O
razor	O
e	O
(	O
cid:11	O
)	O
ect	O
;	O
the	O
complex	B
theory	O
hc	O
always	O
su	O
(	O
cid:11	O
)	O
ers	O
an	O
‘	O
occam	O
factor	O
’	O
because	O
it	O
has	O
more	O
parameters	B
,	O
and	O
so	O
can	O
predict	O
a	O
greater	O
variety	O
of	O
data	O
sets	O
(	O
(	O
cid:12	O
)	O
gure	O
28.3	O
)	O
.	O
this	O
was	O
only	O
a	O
small	O
example	O
,	O
and	O
there	O
were	O
only	O
four	O
data	O
points	O
;	O
as	O
we	O
move	O
to	O
larger	O
2	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
346	O
28	O
|	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
figure	O
28.4.	O
where	O
bayesian	O
inference	B
(	O
cid:12	O
)	O
ts	O
into	O
the	O
data	B
modelling	I
process	O
.	O
this	O
(	O
cid:12	O
)	O
gure	O
illustrates	O
an	O
abstraction	O
of	O
the	O
part	O
of	O
the	O
scienti	O
(	O
cid:12	O
)	O
c	O
process	O
in	O
which	O
data	O
are	O
collected	O
and	O
modelled	O
.	O
in	O
particular	O
,	O
this	O
(	O
cid:12	O
)	O
gure	O
applies	O
to	O
pattern	O
classi	O
(	O
cid:12	O
)	O
cation	O
,	O
learning	B
,	O
interpolation	O
,	O
etc	O
.	O
the	O
two	O
double-framed	O
boxes	O
denote	O
the	O
two	O
steps	O
which	O
involve	O
inference	B
.	O
it	O
is	O
only	O
in	O
those	O
two	O
steps	O
that	O
bayes	O
’	O
theorem	B
can	O
be	O
used	O
.	O
bayes	O
does	O
not	O
tell	O
you	O
how	O
to	O
invent	O
models	O
,	O
for	O
example	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
box	B
,	O
‘	O
(	O
cid:12	O
)	O
tting	O
each	O
model	B
to	O
the	O
data	O
’	O
,	O
is	O
the	O
task	O
of	O
inferring	O
what	O
the	O
model	B
parameters	O
might	O
be	O
given	O
the	O
model	B
and	O
the	O
data	O
.	O
bayesian	O
methods	B
may	O
be	O
used	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
most	O
probable	O
parameter	O
values	O
,	O
and	O
error	O
bars	O
on	O
those	O
parameters	B
.	O
the	O
result	O
of	O
applying	O
bayesian	O
methods	B
to	O
this	O
problem	O
is	O
often	O
little	O
di	O
(	O
cid:11	O
)	O
erent	O
from	O
the	O
answers	O
given	O
by	O
orthodox	O
statistics	O
.	O
the	O
second	O
inference	B
task	O
,	O
model	B
comparison	I
in	O
the	O
light	O
of	O
the	O
data	O
,	O
is	O
where	O
bayesian	O
methods	B
are	O
in	O
a	O
class	O
of	O
their	O
own	O
.	O
this	O
second	O
inference	B
problem	O
requires	O
a	O
quantitative	O
occam	O
’	O
s	O
razor	O
to	O
penalize	O
over-complex	O
models	O
.	O
bayesian	O
methods	B
can	O
assign	O
objective	O
preferences	O
to	O
the	O
alternative	O
models	O
in	O
a	O
way	O
that	O
automatically	O
embodies	O
occam	O
’	O
s	O
razor	O
.	O
gather	O
data	O
create	O
alternative	O
models	O
(	O
cid:19	O
)	O
-	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
@	O
@	O
fit	O
each	O
model	B
to	O
the	O
data	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
@	O
@	O
(	O
cid:16	O
)	O
(	O
cid:27	O
)	O
gather	O
more	O
data	O
6	O
choose	O
what	O
data	O
to	O
gather	O
next	O
assign	O
preferences	O
to	O
the	O
alternative	O
models	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:9	O
)	O
@	O
@	O
@	O
r	O
?	O
choose	O
future	O
actions	O
create	O
new	O
models	O
6	O
decide	O
whether	O
to	O
create	O
new	O
models	O
and	O
more	O
sophisticated	O
problems	O
the	O
magnitude	O
of	O
the	O
occam	O
factors	O
typi-	O
cally	O
increases	O
,	O
and	O
the	O
degree	B
to	O
which	O
our	O
inferences	O
are	O
in	O
(	O
cid:13	O
)	O
uenced	O
by	O
the	O
quantitative	O
details	O
of	O
our	O
subjective	O
assumptions	O
becomes	O
smaller	O
.	O
bayesian	O
methods	B
and	O
data	O
analysis	O
let	O
us	O
now	O
relate	O
the	O
discussion	O
above	O
to	O
real	O
problems	O
in	O
data	O
analysis	B
.	O
there	O
are	O
countless	O
problems	O
in	O
science	O
,	O
statistics	O
and	O
technology	O
which	O
require	O
that	O
,	O
given	O
a	O
limited	O
data	B
set	I
,	O
preferences	O
be	O
assigned	O
to	O
alternative	O
models	O
of	O
di	O
(	O
cid:11	O
)	O
ering	O
complexities	O
.	O
for	O
example	O
,	O
two	O
alternative	O
hypotheses	O
accounting	O
for	O
planetary	O
motion	O
are	O
mr.	O
inquisition	O
’	O
s	O
geocentric	O
model	B
based	O
on	O
‘	O
epicycles	B
’	O
,	O
and	O
mr.	O
copernicus	O
’	O
s	O
simpler	O
model	B
of	O
the	O
solar	B
system	I
with	O
the	O
sun	O
at	O
the	O
centre	O
.	O
the	O
epicyclic	O
model	B
(	O
cid:12	O
)	O
ts	O
data	O
on	O
planetary	O
motion	O
at	O
least	O
as	O
well	O
as	O
the	O
copernican	O
model	B
,	O
but	O
does	O
so	O
using	O
more	O
parameters	B
.	O
coincidentally	O
for	O
mr.	O
inquisition	O
,	O
two	O
of	O
the	O
extra	O
epicyclic	O
parameters	B
for	O
every	O
planet	O
are	O
found	O
to	O
be	O
identical	O
to	O
the	O
period	O
and	O
radius	O
of	O
the	O
sun	O
’	O
s	O
‘	O
cycle	O
around	O
the	O
earth	O
’	O
.	O
intuitively	O
we	O
(	O
cid:12	O
)	O
nd	O
mr.	O
copernicus	O
’	O
s	O
theory	B
more	O
probable	O
.	O
the	O
mechanism	O
of	O
the	O
bayesian	O
razor	O
:	O
the	O
evidence	B
and	O
the	O
occam	O
factor	O
two	O
levels	O
of	O
inference	O
can	O
often	O
be	O
distinguished	O
in	O
the	O
process	O
of	O
data	O
mod-	O
elling	O
.	O
at	O
the	O
(	O
cid:12	O
)	O
rst	O
level	O
of	O
inference	O
,	O
we	O
assume	O
that	O
a	O
particular	O
model	B
is	O
true	O
,	O
and	O
we	O
(	O
cid:12	O
)	O
t	O
that	O
model	B
to	O
the	O
data	O
,	O
i.e.	O
,	O
we	O
infer	O
what	O
values	O
its	O
free	O
param-	O
eters	O
should	O
plausibly	O
take	O
,	O
given	O
the	O
data	O
.	O
the	O
results	O
of	O
this	O
inference	B
are	O
often	O
summarized	O
by	O
the	O
most	O
probable	O
parameter	O
values	O
,	O
and	O
error	O
bars	O
on	O
those	O
parameters	B
.	O
this	O
analysis	B
is	O
repeated	O
for	O
each	O
model	B
.	O
the	O
second	O
level	O
of	O
inference	O
is	O
the	O
task	O
of	O
model	O
comparison	O
.	O
here	O
we	O
wish	O
to	O
compare	O
the	O
models	O
in	O
the	O
light	O
of	O
the	O
data	O
,	O
and	O
assign	O
some	O
sort	O
of	O
preference	O
or	O
ranking	O
to	O
the	O
alternatives	O
.	O
note	O
that	O
both	O
levels	O
of	O
inference	O
are	O
distinct	O
from	O
decision	O
theory	B
.	O
the	O
goal	O
of	O
inference	O
is	O
,	O
given	O
a	O
de	O
(	O
cid:12	O
)	O
ned	O
hypothesis	O
space	O
and	O
a	O
particular	O
data	B
set	I
,	O
to	O
assign	O
probabilities	O
to	O
hypotheses	O
.	O
decision	B
theory	I
typically	O
chooses	O
between	O
alternative	O
actions	O
on	O
the	O
basis	O
of	O
these	O
probabilities	O
so	O
as	O
to	O
minimize	O
the	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
28.1	O
:	O
occam	O
’	O
s	O
razor	O
347	O
expectation	B
of	O
a	O
‘	O
loss	B
function	I
’	O
.	O
this	O
chapter	O
concerns	O
inference	B
alone	O
and	O
no	O
loss	O
functions	O
are	O
involved	O
.	O
when	O
we	O
discuss	O
model	B
comparison	I
,	O
this	O
should	O
not	O
be	O
construed	O
as	O
implying	O
model	B
choice	O
.	O
ideal	O
bayesian	O
predictions	O
do	O
not	O
involve	O
choice	O
between	O
models	O
;	O
rather	O
,	O
predictions	O
are	O
made	O
by	O
summing	O
over	O
all	O
the	O
alternative	O
models	O
,	O
weighted	O
by	O
their	O
probabilities	O
.	O
bayesian	O
methods	B
are	O
able	O
consistently	O
and	O
quantitatively	O
to	O
solve	O
both	O
the	O
inference	B
tasks	O
.	O
there	O
is	O
a	O
popular	O
myth	B
that	O
states	O
that	O
bayesian	O
meth-	O
ods	O
di	O
(	O
cid:11	O
)	O
er	O
from	O
orthodox	O
statistical	B
methods	O
only	O
by	O
the	O
inclusion	O
of	O
subjective	O
priors	O
,	O
which	O
are	O
di	O
(	O
cid:14	O
)	O
cult	O
to	O
assign	O
,	O
and	O
which	O
usually	O
don	O
’	O
t	O
make	O
much	O
dif-	O
ference	O
to	O
the	O
conclusions	O
.	O
it	O
is	O
true	O
that	O
,	O
at	O
the	O
(	O
cid:12	O
)	O
rst	O
level	O
of	O
inference	O
,	O
a	O
bayesian	O
’	O
s	O
results	O
will	O
often	O
di	O
(	O
cid:11	O
)	O
er	O
little	O
from	O
the	O
outcome	O
of	O
an	O
orthodox	O
at-	O
tack	O
.	O
what	O
is	O
not	O
widely	O
appreciated	O
is	O
how	O
a	O
bayesian	O
performs	O
the	O
second	O
level	O
of	O
inference	O
;	O
this	O
chapter	O
will	O
therefore	O
focus	B
on	O
bayesian	O
model	B
compar-	O
ison	O
.	O
model	B
comparison	I
is	O
a	O
di	O
(	O
cid:14	O
)	O
cult	O
task	O
because	O
it	O
is	O
not	O
possible	O
simply	O
to	O
choose	O
the	O
model	B
that	O
(	O
cid:12	O
)	O
ts	O
the	O
data	O
best	O
:	O
more	O
complex	B
models	O
can	O
always	O
(	O
cid:12	O
)	O
t	O
the	O
data	O
better	O
,	O
so	O
the	O
maximum	B
likelihood	I
model	O
choice	O
would	O
lead	O
us	O
inevitably	O
to	O
implausible	O
,	O
over-parameterized	O
models	O
,	O
which	O
generalize	O
poorly	O
.	O
occam	O
’	O
s	O
razor	O
is	O
needed	O
.	O
let	O
us	O
write	O
down	O
bayes	O
’	O
theorem	B
for	O
the	O
two	O
levels	O
of	O
inference	O
described	O
above	O
,	O
so	O
as	O
to	O
see	O
explicitly	O
how	O
bayesian	O
model	B
comparison	I
works	O
.	O
each	O
model	B
hi	O
is	O
assumed	O
to	O
have	O
a	O
vector	O
of	O
parameters	B
w.	O
a	O
model	B
is	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
a	O
collection	O
of	O
probability	O
distributions	O
:	O
a	O
‘	O
prior	B
’	O
distribution	B
p	O
(	O
w	O
jhi	O
)	O
,	O
which	O
states	O
what	O
values	O
the	O
model	B
’	O
s	O
parameters	B
might	O
be	O
expected	O
to	O
take	O
;	O
and	O
a	O
set	B
of	O
conditional	B
distributions	O
,	O
one	O
for	O
each	O
value	O
of	O
w	O
,	O
de	O
(	O
cid:12	O
)	O
ning	O
the	O
predictions	O
p	O
(	O
d	O
j	O
w	O
;	O
hi	O
)	O
that	O
the	O
model	B
makes	O
about	O
the	O
data	O
d.	O
1.	O
model	B
(	O
cid:12	O
)	O
tting	O
.	O
at	O
the	O
(	O
cid:12	O
)	O
rst	O
level	O
of	O
inference	O
,	O
we	O
assume	O
that	O
one	O
model	B
,	O
the	O
ith	O
,	O
say	O
,	O
is	O
true	O
,	O
and	O
we	O
infer	O
what	O
the	O
model	B
’	O
s	O
parameters	B
w	O
might	O
be	O
,	O
given	O
the	O
data	O
d.	O
using	O
bayes	O
’	O
theorem	B
,	O
the	O
posterior	B
probability	I
of	O
the	O
parameters	B
w	O
is	O
:	O
p	O
(	O
w	O
j	O
d	O
;	O
hi	O
)	O
=	O
p	O
(	O
d	O
j	O
w	O
;	O
hi	O
)	O
p	O
(	O
w	O
jhi	O
)	O
p	O
(	O
d	O
jhi	O
)	O
;	O
(	O
28.4	O
)	O
that	O
is	O
,	O
posterior	O
=	O
likelihood	B
(	O
cid:2	O
)	O
prior	B
evidence	O
:	O
the	O
normalizing	B
constant	I
p	O
(	O
d	O
jhi	O
)	O
is	O
commonly	O
ignored	O
since	O
it	O
is	O
irrel-	O
evant	O
to	O
the	O
(	O
cid:12	O
)	O
rst	O
level	O
of	O
inference	O
,	O
i.e.	O
,	O
the	O
inference	B
of	O
w	O
;	O
but	O
it	O
becomes	O
important	O
in	O
the	O
second	O
level	O
of	O
inference	O
,	O
and	O
we	O
name	O
it	O
the	O
evidence	B
for	O
hi	O
.	O
it	O
is	O
common	O
practice	O
to	O
use	O
gradient-based	O
methods	B
to	O
(	O
cid:12	O
)	O
nd	O
the	O
maximum	O
of	O
the	O
posterior	O
,	O
which	O
de	O
(	O
cid:12	O
)	O
nes	O
the	O
most	O
probable	O
value	O
for	O
the	O
parameters	B
,	O
wmp	O
;	O
it	O
is	O
then	O
usual	O
to	O
summarize	O
the	O
posterior	O
distribution	O
by	O
the	O
value	O
of	O
wmp	O
,	O
and	O
error	O
bars	O
or	O
con	O
(	O
cid:12	O
)	O
dence	O
intervals	B
on	O
these	O
best-	O
(	O
cid:12	O
)	O
t	O
parameters	B
.	O
error	B
bars	I
can	O
be	O
obtained	O
from	O
the	O
curvature	O
of	O
the	O
pos-	O
terior	O
;	O
evaluating	O
the	O
hessian	O
at	O
wmp	O
,	O
a	O
=	O
(	O
cid:0	O
)	O
rr	O
ln	O
p	O
(	O
w	O
j	O
d	O
;	O
hi	O
)	O
jwmp	O
,	O
and	O
taylor-expanding	O
the	O
log	O
posterior	B
probability	I
with	O
(	O
cid:1	O
)	O
w	O
=	O
w	O
(	O
cid:0	O
)	O
wmp	O
:	O
(	O
28.5	O
)	O
p	O
(	O
w	O
j	O
d	O
;	O
hi	O
)	O
’	O
p	O
(	O
wmp	O
j	O
d	O
;	O
hi	O
)	O
exp	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
1/2	O
(	O
cid:1	O
)	O
wta	O
(	O
cid:1	O
)	O
w	O
(	O
cid:1	O
)	O
;	O
we	O
see	O
that	O
the	O
posterior	O
can	O
be	O
locally	O
approximated	O
as	O
a	O
gaussian	O
with	O
covariance	O
matrix	B
(	O
equivalent	O
to	O
error	B
bars	I
)	O
a	O
(	O
cid:0	O
)	O
1	O
.	O
[	O
whether	O
this	O
approximation	B
is	O
good	B
or	O
not	O
will	O
depend	O
on	O
the	O
problem	O
we	O
are	O
solv-	O
ing	O
.	O
indeed	O
,	O
the	O
maximum	O
and	O
mean	B
of	O
the	O
posterior	O
distribution	O
have	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
348	O
28	O
|	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
figure	O
28.5.	O
the	O
occam	O
factor	O
.	O
this	O
(	O
cid:12	O
)	O
gure	O
shows	O
the	O
quantities	O
that	O
determine	O
the	O
occam	O
factor	O
for	O
a	O
hypothesis	O
hi	O
having	O
a	O
single	O
parameter	O
w.	O
the	O
prior	B
distribution	O
(	O
solid	O
line	O
)	O
for	O
the	O
parameter	O
has	O
width	O
(	O
cid:27	O
)	O
w.	O
the	O
posterior	O
distribution	O
(	O
dashed	O
line	O
)	O
has	O
a	O
single	O
peak	O
at	O
wmp	O
with	O
characteristic	O
width	O
(	O
cid:27	O
)	O
wjd	O
.	O
the	O
occam	O
factor	O
is	O
(	O
cid:27	O
)	O
wjdp	O
(	O
wmp	O
jhi	O
)	O
=	O
(	O
cid:27	O
)	O
wjd	O
(	O
cid:27	O
)	O
w	O
:	O
p	O
(	O
w	O
j	O
d	O
;	O
hi	O
)	O
p	O
(	O
w	O
jhi	O
)	O
(	O
cid:27	O
)	O
wjd	O
wmp	O
(	O
cid:27	O
)	O
w	O
w	O
no	O
fundamental	O
status	O
in	O
bayesian	O
inference	B
{	O
they	O
both	O
change	O
under	O
nonlinear	B
reparameterizations	O
.	O
maximization	O
of	O
a	O
posterior	O
probabil-	O
ity	O
is	O
useful	O
only	O
if	O
an	O
approximation	B
like	O
equation	O
(	O
28.5	O
)	O
gives	O
a	O
good	B
summary	O
of	O
the	O
distribution	O
.	O
]	O
2.	O
model	B
comparison	I
.	O
at	O
the	O
second	O
level	O
of	O
inference	O
,	O
we	O
wish	O
to	O
infer	O
which	O
model	B
is	O
most	O
plausible	O
given	O
the	O
data	O
.	O
the	O
posterior	B
probability	I
of	O
each	O
model	B
is	O
:	O
p	O
(	O
hi	O
j	O
d	O
)	O
/	O
p	O
(	O
d	O
jhi	O
)	O
p	O
(	O
hi	O
)	O
:	O
(	O
28.6	O
)	O
notice	O
that	O
the	O
data-dependent	O
term	O
p	O
(	O
d	O
jhi	O
)	O
is	O
the	O
evidence	B
for	O
hi	O
,	O
which	O
appeared	O
as	O
the	O
normalizing	B
constant	I
in	O
(	O
28.4	O
)	O
.	O
the	O
second	O
term	O
,	O
p	O
(	O
hi	O
)	O
,	O
is	O
the	O
subjective	O
prior	O
over	O
our	O
hypothesis	O
space	O
,	O
which	O
expresses	O
how	O
plausible	O
we	O
thought	O
the	O
alternative	O
models	O
were	O
before	O
the	O
data	O
arrived	O
.	O
assuming	O
that	O
we	O
choose	O
to	O
assign	O
equal	O
priors	O
p	O
(	O
hi	O
)	O
to	O
the	O
alternative	O
models	O
,	O
models	O
hi	O
are	O
ranked	O
by	O
evaluating	O
the	O
evidence	B
.	O
the	O
normalizing	B
constant	I
p	O
(	O
d	O
)	O
=pi	O
p	O
(	O
d	O
jhi	O
)	O
p	O
(	O
hi	O
)	O
has	O
been	O
omitted	O
from	O
equation	O
(	O
28.6	O
)	O
because	O
in	O
the	O
data-modelling	O
process	O
we	O
may	O
develop	O
new	O
models	O
after	O
the	O
data	O
have	O
arrived	O
,	O
when	O
an	O
inadequacy	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
models	O
is	O
detected	O
,	O
for	O
example	O
.	O
inference	B
is	O
open	O
ended	O
:	O
we	O
continually	O
seek	O
more	O
probable	O
models	O
to	O
account	O
for	O
the	O
data	O
we	O
gather	O
.	O
to	O
repeat	O
the	O
key	O
idea	O
:	O
to	O
rank	O
alternative	O
models	O
hi	O
,	O
a	O
bayesian	O
eval-	O
uates	O
the	O
evidence	B
p	O
(	O
d	O
jhi	O
)	O
.	O
this	O
concept	O
is	O
very	O
general	O
:	O
the	O
ev-	O
idence	O
can	O
be	O
evaluated	O
for	O
parametric	O
and	O
‘	O
non-parametric	O
’	O
models	O
alike	O
;	O
whatever	O
our	O
data-modelling	O
task	O
,	O
a	O
regression	B
problem	O
,	O
a	O
clas-	O
si	O
(	O
cid:12	O
)	O
cation	O
problem	O
,	O
or	O
a	O
density	B
estimation	O
problem	O
,	O
the	O
evidence	B
is	O
a	O
transportable	O
quantity	O
for	O
comparing	O
alternative	O
models	O
.	O
in	O
all	O
these	O
cases	O
the	O
evidence	B
naturally	O
embodies	O
occam	O
’	O
s	O
razor	O
.	O
evaluating	O
the	O
evidence	B
let	O
us	O
now	O
study	O
the	O
evidence	B
more	O
closely	O
to	O
gain	B
insight	O
into	O
how	O
the	O
bayesian	O
occam	O
’	O
s	O
razor	O
works	O
.	O
the	O
evidence	B
is	O
the	O
normalizing	B
constant	I
for	O
equation	O
(	O
28.4	O
)	O
:	O
p	O
(	O
d	O
jhi	O
)	O
=z	O
p	O
(	O
d	O
j	O
w	O
;	O
hi	O
)	O
p	O
(	O
w	O
jhi	O
)	O
dw	O
:	O
(	O
28.7	O
)	O
for	O
many	O
problems	O
the	O
posterior	O
p	O
(	O
w	O
j	O
d	O
;	O
hi	O
)	O
/	O
p	O
(	O
d	O
j	O
w	O
;	O
hi	O
)	O
p	O
(	O
w	O
jhi	O
)	O
has	O
a	O
strong	O
peak	O
at	O
the	O
most	O
probable	O
parameters	O
wmp	O
(	O
(	O
cid:12	O
)	O
gure	O
28.5	O
)	O
.	O
then	O
,	O
taking	O
for	O
simplicity	O
the	O
one-dimensional	O
case	O
,	O
the	O
evidence	B
can	O
be	O
approx-	O
imated	O
,	O
using	O
laplace	O
’	O
s	O
method	B
,	O
by	O
the	O
height	O
of	O
the	O
peak	O
of	O
the	O
integrand	O
p	O
(	O
d	O
j	O
w	O
;	O
hi	O
)	O
p	O
(	O
w	O
jhi	O
)	O
times	O
its	O
width	O
,	O
(	O
cid:27	O
)	O
wjd	O
:	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
28.1	O
:	O
occam	O
’	O
s	O
razor	O
349	O
p	O
(	O
d	O
jhi	O
)	O
’	O
{	O
z	O
evidence	B
’	O
best	O
(	O
cid:12	O
)	O
t	O
likelihood	B
(	O
cid:2	O
)	O
occam	O
factor	O
(	O
cid:2	O
)	O
p	O
(	O
wmp	O
jhi	O
)	O
(	O
cid:27	O
)	O
wjd	O
|	O
}	O
p	O
(	O
d	O
j	O
wmp	O
;	O
hi	O
)	O
}	O
|	O
{	O
z	O
thus	O
the	O
evidence	B
is	O
found	O
by	O
taking	O
the	O
best-	O
(	O
cid:12	O
)	O
t	O
likelihood	B
that	O
the	O
model	B
can	O
achieve	O
and	O
multiplying	O
it	O
by	O
an	O
‘	O
occam	O
factor	O
’	O
,	O
which	O
is	O
a	O
term	O
with	O
magnitude	O
less	O
than	O
one	O
that	O
penalizes	O
hi	O
for	O
having	O
the	O
parameter	O
w.	O
interpretation	O
of	O
the	O
occam	O
factor	O
:	O
(	O
28.8	O
)	O
the	O
quantity	O
(	O
cid:27	O
)	O
wjd	O
is	O
the	O
posterior	O
uncertainty	O
in	O
w.	O
suppose	O
for	O
simplicity	O
that	O
the	O
prior	B
p	O
(	O
w	O
jhi	O
)	O
is	O
uniform	O
on	O
some	O
large	O
interval	O
(	O
cid:27	O
)	O
w	O
,	O
representing	O
the	O
range	O
of	O
values	O
of	O
w	O
that	O
were	O
possible	O
a	O
priori	O
,	O
according	O
to	O
hi	O
(	O
(	O
cid:12	O
)	O
gure	O
28.5	O
)	O
.	O
then	O
p	O
(	O
wmp	O
jhi	O
)	O
=	O
1=	O
(	O
cid:27	O
)	O
w	O
,	O
and	O
occam	O
factor	O
=	O
;	O
(	O
28.9	O
)	O
(	O
cid:27	O
)	O
wjd	O
(	O
cid:27	O
)	O
w	O
i.e.	O
,	O
the	O
occam	O
factor	O
is	O
equal	O
to	O
the	O
ratio	O
of	O
the	O
posterior	O
accessible	O
volume	B
of	O
hi	O
’	O
s	O
parameter	O
space	O
to	O
the	O
prior	B
accessible	O
volume	B
,	O
or	O
the	O
factor	O
by	O
which	O
hi	O
’	O
s	O
hypothesis	O
space	O
collapses	O
when	O
the	O
data	O
arrive	O
.	O
the	O
model	B
hi	O
can	O
be	O
viewed	O
as	O
consisting	O
of	O
a	O
certain	O
number	O
of	O
exclusive	O
submodels	O
,	O
of	O
which	O
only	O
one	O
survives	O
when	O
the	O
data	O
arrive	O
.	O
the	O
occam	O
factor	O
is	O
the	O
inverse	O
of	O
that	O
number	O
.	O
the	O
logarithm	O
of	O
the	O
occam	O
factor	O
is	O
a	O
measure	O
of	O
the	O
amount	O
of	O
information	O
we	O
gain	B
about	O
the	O
model	B
’	O
s	O
parameters	B
when	O
the	O
data	O
arrive	O
.	O
a	O
complex	B
model	O
having	O
many	O
parameters	B
,	O
each	O
of	O
which	O
is	O
free	O
to	O
vary	O
over	O
a	O
large	O
range	O
(	O
cid:27	O
)	O
w	O
,	O
will	O
typically	O
be	O
penalized	O
by	O
a	O
stronger	O
occam	O
factor	O
than	O
a	O
simpler	O
model	B
.	O
the	O
occam	O
factor	O
also	O
penalizes	O
models	O
that	O
have	O
to	O
be	O
(	O
cid:12	O
)	O
nely	O
tuned	O
to	O
(	O
cid:12	O
)	O
t	O
the	O
data	O
,	O
favouring	O
models	O
for	O
which	O
the	O
required	O
pre-	O
cision	O
of	O
the	O
parameters	O
(	O
cid:27	O
)	O
wjd	O
is	O
coarse	O
.	O
the	O
magnitude	O
of	O
the	O
occam	O
factor	O
is	O
thus	O
a	O
measure	O
of	O
complexity	O
of	O
the	O
model	O
;	O
it	O
relates	O
to	O
the	O
complexity	B
of	O
the	O
predictions	O
that	O
the	O
model	B
makes	O
in	O
data	O
space	O
.	O
this	O
depends	O
not	O
only	O
on	O
the	O
number	O
of	O
parameters	O
in	O
the	O
model	B
,	O
but	O
also	O
on	O
the	O
prior	B
probability	O
that	O
the	O
model	B
assigns	O
to	O
them	O
.	O
which	O
model	B
achieves	O
the	O
greatest	O
evidence	B
is	O
determined	O
by	O
a	O
trade-o	O
(	O
cid:11	O
)	O
between	O
minimizing	O
this	O
natural	B
complexity	O
mea-	O
sure	O
and	O
minimizing	O
the	O
data	O
mis	O
(	O
cid:12	O
)	O
t.	O
in	O
contrast	O
to	O
alternative	O
measures	O
of	B
model	I
complexity	I
,	O
the	O
occam	O
factor	O
for	O
a	O
model	B
is	O
straightforward	O
to	O
evalu-	O
ate	O
:	O
it	O
simply	O
depends	O
on	O
the	O
error	B
bars	I
on	O
the	O
parameters	B
,	O
which	O
we	O
already	O
evaluated	O
when	O
(	O
cid:12	O
)	O
tting	O
the	O
model	B
to	O
the	O
data	O
.	O
figure	O
28.6	O
displays	O
an	O
entire	O
hypothesis	O
space	O
so	O
as	O
to	O
illustrate	O
the	O
var-	O
ious	O
probabilities	O
in	O
the	O
analysis	B
.	O
there	O
are	O
three	O
models	O
,	O
h1	O
;	O
h2	O
;	O
h3	O
,	O
which	O
have	O
equal	O
prior	B
probabilities	O
.	O
each	O
model	B
has	O
one	O
parameter	O
w	O
(	O
each	O
shown	O
on	O
a	O
horizontal	O
axis	O
)	O
,	O
but	O
assigns	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
prior	B
range	O
(	O
cid:27	O
)	O
w	O
to	O
that	O
parame-	O
ter	O
.	O
h3	O
is	O
the	O
most	O
‘	O
(	O
cid:13	O
)	O
exible	O
’	O
or	O
‘	O
complex	B
’	O
model	B
,	O
assigning	B
the	O
broadest	O
prior	B
range	O
.	O
a	O
one-dimensional	O
data	O
space	O
is	O
shown	O
by	O
the	O
vertical	O
axis	O
.	O
each	O
model	B
assigns	O
a	O
joint	B
probability	O
distribution	B
p	O
(	O
d	O
;	O
w	O
jhi	O
)	O
to	O
the	O
data	O
and	O
the	O
parameters	B
,	O
illustrated	O
by	O
a	O
cloud	O
of	O
dots	O
.	O
these	O
dots	O
represent	O
random	B
samples	O
from	O
the	O
full	O
probability	O
distribution	B
.	O
the	O
total	O
number	O
of	O
dots	O
in	O
each	O
of	O
the	O
three	O
model	B
subspaces	O
is	O
the	O
same	O
,	O
because	O
we	O
assigned	O
equal	O
prior	B
probabilities	O
to	O
the	O
models	O
.	O
when	O
a	O
particular	O
data	B
set	I
d	O
is	O
received	O
(	O
horizontal	O
line	O
)	O
,	O
we	O
infer	O
the	O
pos-	O
terior	O
distribution	B
of	O
w	O
for	O
a	O
model	B
(	O
h3	O
,	O
say	O
)	O
by	O
reading	O
out	O
the	O
density	B
along	O
that	O
horizontal	O
line	O
,	O
and	O
normalizing	O
.	O
the	O
posterior	B
probability	I
p	O
(	O
w	O
j	O
d	O
;	O
h3	O
)	O
is	O
shown	O
by	O
the	O
dotted	O
curve	O
at	O
the	O
bottom	O
.	O
also	O
shown	O
is	O
the	O
prior	B
distribu-	O
tion	O
p	O
(	O
w	O
jh3	O
)	O
(	O
cf	O
.	O
(	O
cid:12	O
)	O
gure	O
28.5	O
)	O
.	O
[	O
in	O
the	O
case	O
of	O
model	O
h1	O
which	O
is	O
very	O
poorly	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
350	O
28	O
|	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
figure	O
28.6.	O
a	O
hypothesis	O
space	O
consisting	O
of	O
three	O
exclusive	O
models	O
,	O
each	O
having	O
one	O
parameter	O
w	O
,	O
and	O
a	O
one-dimensional	O
data	B
set	I
d.	O
the	O
‘	O
data	B
set	I
’	O
is	O
a	O
single	O
measured	O
value	O
which	O
di	O
(	O
cid:11	O
)	O
ers	O
from	O
the	O
parameter	O
w	O
by	O
a	O
small	O
amount	O
of	O
additive	O
noise	B
.	O
typical	B
samples	O
from	O
the	O
joint	B
distribution	O
p	O
(	O
d	O
;	O
w	O
;	O
h	O
)	O
are	O
shown	O
by	O
dots	O
.	O
(	O
n.b.	O
,	O
these	O
are	O
not	O
data	O
points	O
.	O
)	O
the	O
observed	O
‘	O
data	B
set	I
’	O
is	O
a	O
single	O
particular	O
value	O
for	O
d	O
shown	O
by	O
the	O
dashed	O
horizontal	O
line	O
.	O
the	O
dashed	O
curves	O
below	O
show	O
the	O
posterior	B
probability	I
of	O
w	O
for	O
each	O
model	B
given	O
this	O
data	B
set	I
(	O
cf	O
.	O
(	O
cid:12	O
)	O
gure	O
28.3	O
)	O
.	O
the	O
evidence	B
for	O
the	O
di	O
(	O
cid:11	O
)	O
erent	O
models	O
is	O
obtained	O
by	O
marginalizing	O
onto	O
the	O
d	O
axis	O
at	O
the	O
left-hand	O
side	O
(	O
cf	O
.	O
(	O
cid:12	O
)	O
gure	O
28.5	O
)	O
.	O
d	O
d	O
p	O
(	O
d	O
jh3	O
)	O
p	O
(	O
d	O
jh2	O
)	O
p	O
(	O
d	O
jh1	O
)	O
p	O
(	O
w	O
j	O
d	O
;	O
h1	O
)	O
p	O
(	O
w	O
j	O
d	O
;	O
h2	O
)	O
p	O
(	O
w	O
jh1	O
)	O
p	O
(	O
w	O
jh2	O
)	O
p	O
(	O
w	O
j	O
d	O
;	O
h3	O
)	O
p	O
(	O
w	O
jh3	O
)	O
(	O
cid:27	O
)	O
wjd	O
w	O
(	O
cid:27	O
)	O
w	O
w	O
w	O
matched	O
to	O
the	O
data	O
,	O
the	O
shape	O
of	O
the	O
posterior	O
distribution	B
will	O
depend	O
on	O
the	O
details	O
of	O
the	O
tails	O
of	O
the	O
prior	O
p	O
(	O
w	O
jh1	O
)	O
and	O
the	O
likelihood	B
p	O
(	O
d	O
j	O
w	O
;	O
h1	O
)	O
;	O
the	O
curve	O
shown	O
is	O
for	O
the	O
case	O
where	O
the	O
prior	B
falls	O
o	O
(	O
cid:11	O
)	O
more	O
strongly	O
.	O
]	O
we	O
obtain	O
(	O
cid:12	O
)	O
gure	O
28.3	O
by	O
marginalizing	O
the	O
joint	B
distributions	O
p	O
(	O
d	O
;	O
w	O
jhi	O
)	O
onto	O
the	O
d	O
axis	O
at	O
the	O
left-hand	O
side	O
.	O
for	O
the	O
data	B
set	I
d	O
shown	O
by	O
the	O
dotted	O
horizontal	O
line	O
,	O
the	O
evidence	B
p	O
(	O
d	O
jh3	O
)	O
for	O
the	O
more	O
(	O
cid:13	O
)	O
exible	O
model	B
h3	O
has	O
a	O
smaller	O
value	O
than	O
the	O
evidence	B
for	O
h2	O
.	O
this	O
is	O
because	O
h3	O
placed	O
less	O
predictive	O
probability	O
(	O
fewer	O
dots	O
)	O
on	O
that	O
line	O
.	O
in	O
terms	O
of	O
the	O
distributions	O
over	O
w	O
,	O
model	B
h3	O
has	O
smaller	O
evidence	B
because	O
the	O
occam	O
factor	O
(	O
cid:27	O
)	O
wjd=	O
(	O
cid:27	O
)	O
w	O
is	O
smaller	O
for	O
h3	O
than	O
for	O
h2	O
.	O
the	O
simplest	O
model	B
h1	O
has	O
the	O
smallest	O
evidence	B
of	O
all	O
,	O
because	O
the	O
best	O
(	O
cid:12	O
)	O
t	O
that	O
it	O
can	O
achieve	O
to	O
the	O
data	O
d	O
is	O
very	O
poor	O
.	O
given	O
this	O
data	B
set	I
,	O
the	O
most	O
probable	O
model	O
is	O
h2	O
.	O
occam	O
factor	O
for	O
several	O
parameters	B
if	O
the	O
posterior	O
is	O
well	O
approximated	O
by	O
a	O
gaussian	O
,	O
then	O
the	O
occam	O
factor	O
is	O
obtained	O
from	O
the	O
determinant	O
of	O
the	O
corresponding	O
covariance	B
matrix	I
(	O
cf	O
.	O
equation	O
(	O
28.8	O
)	O
and	O
chapter	O
27	O
)	O
:	O
2	O
(	O
a=2	O
(	O
cid:25	O
)	O
)	O
;	O
(	O
28.10	O
)	O
}	O
p	O
(	O
d	O
jhi	O
)	O
’	O
p	O
(	O
d	O
j	O
wmp	O
;	O
hi	O
)	O
}	O
|	O
evidence	B
’	O
best	O
(	O
cid:12	O
)	O
t	O
likelihood	B
(	O
cid:2	O
)	O
{	O
z	O
(	O
cid:2	O
)	O
p	O
(	O
wmp	O
jhi	O
)	O
det	O
(	O
cid:0	O
)	O
1	O
|	O
{	O
z	O
occam	O
factor	O
where	O
a	O
=	O
(	O
cid:0	O
)	O
rr	O
ln	O
p	O
(	O
w	O
j	O
d	O
;	O
hi	O
)	O
,	O
the	O
hessian	O
which	O
we	O
evaluated	O
when	O
we	O
calculated	O
the	O
error	B
bars	I
on	O
wmp	O
(	O
equation	O
28.5	O
and	O
chapter	O
27	O
)	O
.	O
as	O
the	O
amount	O
of	O
data	O
collected	O
increases	O
,	O
this	O
gaussian	O
approximation	B
is	O
expected	O
to	O
become	O
increasingly	O
accurate	O
.	O
in	O
summary	O
,	O
bayesian	O
model	B
comparison	I
is	O
a	O
simple	O
extension	O
of	O
maximum	O
likelihood	B
model	O
selection	O
:	O
the	O
evidence	B
is	O
obtained	O
by	O
multiplying	O
the	O
best-	O
(	O
cid:12	O
)	O
t	O
likelihood	B
by	O
the	O
occam	O
factor	O
.	O
to	O
evaluate	O
the	O
occam	O
factor	O
we	O
need	O
only	O
the	O
hessian	O
a	O
,	O
if	O
the	O
gaussian	O
approximation	B
is	O
good	B
.	O
thus	O
the	O
bayesian	O
method	B
of	O
model	B
comparison	I
by	O
evaluating	O
the	O
evidence	B
is	O
no	O
more	O
computationally	O
demanding	O
than	O
the	O
task	O
of	O
(	O
cid:12	O
)	O
nding	O
for	O
each	O
model	B
the	O
best-	O
(	O
cid:12	O
)	O
t	O
parameters	B
and	O
their	O
error	B
bars	I
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
351	O
1	O
?	O
or	O
2	O
?	O
figure	O
28.7.	O
how	O
many	O
boxes	O
are	O
behind	O
the	O
tree	B
?	O
28.2	O
:	O
example	O
28.2	O
example	O
let	O
’	O
s	O
return	O
to	O
the	O
example	O
that	O
opened	O
this	O
chapter	O
.	O
are	O
there	O
one	O
or	O
two	O
boxes	O
behind	O
the	O
tree	B
in	O
(	O
cid:12	O
)	O
gure	O
28.1	O
?	O
why	O
do	O
coincidences	O
make	O
us	O
suspicious	O
?	O
let	O
’	O
s	O
assume	O
the	O
image	B
of	O
the	O
area	O
round	O
the	O
trunk	O
and	O
box	O
has	O
a	O
size	O
of	O
50	O
pixels	O
,	O
that	O
the	O
trunk	O
is	O
10	O
pixels	O
wide	O
,	O
and	O
that	O
16	O
di	O
(	O
cid:11	O
)	O
erent	O
colours	O
of	O
boxes	O
can	O
be	O
distinguished	O
.	O
the	O
theory	B
h1	O
that	O
says	O
there	O
is	O
one	O
box	B
near	O
the	O
trunk	O
has	O
four	O
free	O
parameters	O
:	O
three	O
coordinates	O
de	O
(	O
cid:12	O
)	O
ning	O
the	O
top	O
three	O
edges	O
of	O
the	O
box	O
,	O
and	O
one	O
parameter	O
giving	O
the	O
box	B
’	O
s	O
colour	O
.	O
(	O
if	O
boxes	O
could	O
levitate	O
,	O
there	O
would	O
be	O
(	O
cid:12	O
)	O
ve	O
free	O
parameters	O
.	O
)	O
the	O
theory	B
h2	O
that	O
says	O
there	O
are	O
two	O
boxes	O
near	O
the	O
trunk	O
has	O
eight	O
free	O
parameters	O
(	O
twice	O
four	O
)	O
,	O
plus	O
a	O
ninth	O
,	O
a	O
binary	O
variable	O
that	O
indicates	O
which	O
of	O
the	O
two	O
boxes	O
is	O
the	O
closest	O
to	O
the	O
viewer	O
.	O
what	O
is	O
the	O
evidence	B
for	O
each	O
model	B
?	O
we	O
’	O
ll	O
do	O
h1	O
(	O
cid:12	O
)	O
rst	O
.	O
we	O
need	O
a	O
prior	B
on	O
the	O
parameters	B
to	O
evaluate	O
the	O
evidence	B
.	O
for	O
convenience	O
,	O
let	O
’	O
s	O
work	O
in	O
pixels	O
.	O
let	O
’	O
s	O
assign	O
a	O
separable	O
prior	B
to	O
the	O
horizontal	O
location	O
of	O
the	O
box	O
,	O
its	O
width	O
,	O
its	O
height	O
,	O
and	O
its	O
colour	O
.	O
the	O
height	O
could	O
have	O
any	O
of	O
,	O
say	O
,	O
20	O
distinguishable	O
values	O
,	O
so	O
could	O
the	O
width	O
,	O
and	O
so	O
could	O
the	O
location	O
.	O
the	O
colour	O
could	O
have	O
any	O
of	O
16	O
values	O
.	O
we	O
’	O
ll	O
put	O
uniform	O
priors	O
over	O
these	O
variables	O
.	O
we	O
’	O
ll	O
ignore	O
all	O
the	O
parameters	B
associated	O
with	O
other	O
objects	O
in	O
the	O
image	B
,	O
since	O
they	O
don	O
’	O
t	O
come	O
into	O
the	O
model	B
comparison	I
between	O
h1	O
and	O
h2	O
.	O
the	O
evidence	B
is	O
p	O
(	O
d	O
jh1	O
)	O
=	O
1	O
20	O
1	O
20	O
1	O
20	O
1	O
16	O
(	O
28.11	O
)	O
since	O
only	O
one	O
setting	O
of	O
the	O
parameters	O
(	O
cid:12	O
)	O
ts	O
the	O
data	O
,	O
and	O
it	O
predicts	O
the	O
data	O
perfectly	O
.	O
as	O
for	O
model	B
h2	O
,	O
six	B
of	O
its	O
nine	O
parameters	B
are	O
well-determined	O
,	O
and	O
three	O
of	O
them	O
are	O
partly-constrained	O
by	O
the	O
data	O
.	O
if	O
the	O
left-hand	O
box	B
is	O
furthest	O
away	O
,	O
for	O
example	O
,	O
then	O
its	O
width	O
is	O
at	O
least	O
8	O
pixels	O
and	O
at	O
most	O
30	O
;	O
if	O
it	O
’	O
s	O
the	O
closer	O
of	O
the	O
two	O
boxes	O
,	O
then	O
its	O
width	O
is	O
between	O
8	O
and	O
18	O
pixels	O
.	O
(	O
i	O
’	O
m	O
assuming	O
here	O
that	O
the	O
visible	O
portion	O
of	O
the	O
left-hand	O
box	B
is	O
about	O
8	O
pixels	O
wide	O
.	O
)	O
to	O
get	O
the	O
evidence	B
we	O
need	O
to	O
sum	O
up	O
the	O
prior	B
probabilities	O
of	O
all	O
viable	O
hypotheses	O
.	O
to	O
do	O
an	O
exact	O
calculation	O
,	O
we	O
need	O
to	O
be	O
more	O
speci	O
(	O
cid:12	O
)	O
c	O
about	O
the	O
data	O
and	O
the	O
priors	O
,	O
but	O
let	O
’	O
s	O
just	O
get	O
the	O
ballpark	O
answer	O
,	O
assuming	O
that	O
the	O
two	O
unconstrained	O
real	O
variables	O
have	O
half	O
their	O
values	O
available	O
,	O
and	O
that	O
the	O
binary	O
variable	O
is	O
completely	O
undetermined	O
.	O
(	O
as	O
an	O
exercise	O
,	O
you	O
can	O
make	O
an	O
explicit	O
model	B
and	O
work	O
out	O
the	O
exact	O
answer	O
.	O
)	O
p	O
(	O
d	O
jh2	O
)	O
’	O
1	O
20	O
1	O
20	O
10	O
20	O
1	O
16	O
1	O
20	O
1	O
20	O
10	O
20	O
1	O
16	O
2	O
2	O
:	O
(	O
28.12	O
)	O
thus	O
the	O
posterior	B
probability	I
ratio	O
is	O
(	O
assuming	O
equal	O
prior	B
probability	O
)	O
:	O
p	O
(	O
d	O
jh1	O
)	O
p	O
(	O
h1	O
)	O
p	O
(	O
d	O
jh2	O
)	O
p	O
(	O
h2	O
)	O
=	O
1	O
10	O
10	O
20	O
20	O
1	O
20	O
1	O
16	O
=	O
20	O
(	O
cid:2	O
)	O
2	O
(	O
cid:2	O
)	O
2	O
(	O
cid:2	O
)	O
16	O
’	O
1000=1	O
:	O
(	O
28.13	O
)	O
(	O
28.14	O
)	O
so	O
the	O
data	O
are	O
roughly	O
1000	O
to	O
1	O
in	O
favour	O
of	O
the	O
simpler	O
hypothesis	O
.	O
the	O
four	O
factors	O
in	O
(	O
28.13	O
)	O
can	O
be	O
interpreted	O
in	O
terms	O
of	O
occam	O
factors	O
.	O
the	O
more	O
complex	B
model	O
has	O
four	O
extra	O
parameters	O
for	O
sizes	O
and	O
colours	O
{	O
three	O
for	O
sizes	O
,	O
and	O
one	O
for	O
colour	O
.	O
it	O
has	O
to	O
pay	O
two	O
big	O
occam	O
factors	O
(	O
1/20	O
and	O
1/16	O
)	O
for	O
the	O
highly	O
suspicious	B
coincidences	I
that	O
the	O
two	O
box	B
heights	O
match	O
exactly	O
and	O
the	O
two	O
colours	O
match	O
exactly	O
;	O
and	O
it	O
also	O
pays	O
two	O
lesser	O
occam	O
factors	O
for	O
the	O
two	O
lesser	O
coincidences	O
that	O
both	O
boxes	O
happened	O
to	O
have	O
one	O
of	O
their	O
edges	O
conveniently	O
hidden	O
behind	O
a	O
tree	B
or	O
behind	O
each	O
other	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
352	O
28	O
|	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
figure	O
28.8.	O
a	O
popular	O
view	O
of	O
model	O
comparison	O
by	O
minimum	O
description	O
length	B
.	O
each	O
model	B
hi	O
communicates	O
the	O
data	O
d	O
by	O
sending	O
the	O
identity	O
of	O
the	O
model	B
,	O
sending	O
the	O
best-	O
(	O
cid:12	O
)	O
t	O
parameters	B
of	O
the	O
model	B
w	O
(	O
cid:3	O
)	O
,	O
then	O
sending	O
the	O
data	O
relative	O
to	O
those	O
parameters	B
.	O
as	O
we	O
proceed	O
to	O
more	O
complex	B
models	O
the	O
length	B
of	O
the	O
parameter	O
message	O
increases	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
length	B
of	O
the	O
data	O
message	O
decreases	O
,	O
because	O
a	O
complex	B
model	O
is	O
able	O
to	O
(	O
cid:12	O
)	O
t	O
the	O
data	O
better	O
,	O
making	O
the	O
residuals	O
smaller	O
.	O
in	O
this	O
example	O
the	O
intermediate	O
model	B
h2	O
achieves	O
the	O
optimum	O
trade-o	O
(	O
cid:11	O
)	O
between	O
these	O
two	O
trends	O
.	O
h1	O
:	O
l	O
(	O
h1	O
)	O
l	O
(	O
w	O
(	O
cid:3	O
)	O
h2	O
:	O
l	O
(	O
h2	O
)	O
h3	O
:	O
l	O
(	O
h3	O
)	O
(	O
1	O
)	O
jh1	O
)	O
l	O
(	O
w	O
(	O
cid:3	O
)	O
(	O
2	O
)	O
jh2	O
)	O
l	O
(	O
w	O
(	O
cid:3	O
)	O
(	O
3	O
)	O
jh3	O
)	O
l	O
(	O
d	O
j	O
w	O
(	O
cid:3	O
)	O
l	O
(	O
d	O
j	O
w	O
(	O
cid:3	O
)	O
(	O
1	O
)	O
;	O
h1	O
)	O
(	O
2	O
)	O
;	O
h2	O
)	O
l	O
(	O
d	O
j	O
w	O
(	O
cid:3	O
)	O
(	O
3	O
)	O
;	O
h3	O
)	O
28.3	O
minimum	B
description	I
length	I
(	O
mdl	O
)	O
a	O
complementary	O
view	O
of	O
bayesian	O
model	B
comparison	I
is	O
obtained	O
by	O
replacing	O
probabilities	O
of	O
events	O
by	O
the	O
lengths	O
in	O
bits	O
of	O
messages	O
that	O
communicate	O
the	O
events	O
without	O
loss	O
to	O
a	O
receiver	O
.	O
message	O
lengths	O
l	O
(	O
x	O
)	O
correspond	O
to	O
a	O
probabilistic	B
model	I
over	O
events	O
x	O
via	O
the	O
relations	O
:	O
p	O
(	O
x	O
)	O
=	O
2	O
(	O
cid:0	O
)	O
l	O
(	O
x	O
)	O
;	O
l	O
(	O
x	O
)	O
=	O
(	O
cid:0	O
)	O
log2	O
p	O
(	O
x	O
)	O
:	O
(	O
28.15	O
)	O
the	O
mdl	O
principle	O
(	O
wallace	O
and	O
boulton	O
,	O
1968	O
)	O
states	O
that	O
one	O
should	O
prefer	O
models	O
that	O
can	O
communicate	O
the	O
data	O
in	O
the	O
smallest	O
number	O
of	O
bits	O
.	O
consider	O
a	O
two-part	O
message	O
that	O
states	O
which	O
model	B
,	O
h	O
,	O
is	O
to	O
be	O
used	O
,	O
and	O
then	O
communicates	O
the	O
data	O
d	O
within	O
that	O
model	B
,	O
to	O
some	O
pre-arranged	O
pre-	O
cision	O
(	O
cid:14	O
)	O
d.	O
this	O
produces	O
a	O
message	O
of	O
length	B
l	O
(	O
d	O
;	O
h	O
)	O
=	O
l	O
(	O
h	O
)	O
+	O
l	O
(	O
d	O
jh	O
)	O
.	O
the	O
lengths	O
l	O
(	O
h	O
)	O
for	O
di	O
(	O
cid:11	O
)	O
erent	O
h	O
de	O
(	O
cid:12	O
)	O
ne	O
an	O
implicit	O
prior	O
p	O
(	O
h	O
)	O
over	O
the	O
alter-	O
native	O
models	O
.	O
similarly	O
l	O
(	O
d	O
jh	O
)	O
corresponds	O
to	O
a	O
density	B
p	O
(	O
d	O
jh	O
)	O
.	O
thus	O
,	O
a	O
procedure	O
for	O
assigning	O
message	O
lengths	O
can	O
be	O
mapped	O
onto	O
posterior	O
prob-	O
abilities	O
:	O
l	O
(	O
d	O
;	O
h	O
)	O
=	O
(	O
cid:0	O
)	O
log	O
p	O
(	O
h	O
)	O
(	O
cid:0	O
)	O
log	O
(	O
p	O
(	O
d	O
jh	O
)	O
(	O
cid:14	O
)	O
d	O
)	O
=	O
(	O
cid:0	O
)	O
log	O
p	O
(	O
h	O
j	O
d	O
)	O
+	O
const	O
:	O
(	O
28.16	O
)	O
(	O
28.17	O
)	O
in	O
principle	O
,	O
then	O
,	O
mdl	O
can	O
always	O
be	O
interpreted	O
as	O
bayesian	O
model	B
compar-	O
ison	O
and	O
vice	O
versa	O
.	O
however	O
,	O
this	O
simple	O
discussion	O
has	O
not	O
addressed	O
how	O
one	O
would	O
actually	O
evaluate	O
the	O
key	O
data-dependent	O
term	O
l	O
(	O
d	O
jh	O
)	O
,	O
which	O
corresponds	O
to	O
the	O
evidence	B
for	O
h.	O
often	O
,	O
this	O
message	O
is	O
imagined	O
as	O
being	O
subdivided	O
into	O
a	O
parameter	O
block	B
and	O
a	O
data	O
block	O
(	O
(	O
cid:12	O
)	O
gure	O
28.8	O
)	O
.	O
models	O
with	O
a	O
small	O
number	O
of	O
parameters	O
have	O
only	O
a	O
short	O
parameter	O
block	B
but	O
do	O
not	O
(	O
cid:12	O
)	O
t	O
the	O
data	O
well	O
,	O
and	O
so	O
the	O
data	O
message	O
(	O
a	O
list	O
of	O
large	O
residuals	O
)	O
is	O
long	O
.	O
as	O
the	O
number	O
of	O
parameters	O
increases	O
,	O
the	O
parameter	O
block	B
lengthens	O
,	O
and	O
the	O
data	O
message	O
becomes	O
shorter	O
.	O
there	O
is	O
an	O
optimum	O
model	B
complexity	O
(	O
h2	O
in	O
the	O
(	O
cid:12	O
)	O
gure	O
)	O
for	O
which	O
the	O
sum	O
is	O
minimized	O
.	O
this	O
picture	O
glosses	O
over	O
some	O
subtle	O
issues	O
.	O
we	O
have	O
not	O
speci	O
(	O
cid:12	O
)	O
ed	O
the	O
precision	B
to	O
which	O
the	O
parameters	B
w	O
should	O
be	O
sent	O
.	O
this	O
precision	B
has	O
an	O
important	O
e	O
(	O
cid:11	O
)	O
ect	O
(	O
unlike	O
the	O
precision	B
(	O
cid:14	O
)	O
d	O
to	O
which	O
real-valued	O
data	O
d	O
are	O
sent	O
,	O
which	O
,	O
assuming	O
(	O
cid:14	O
)	O
d	O
is	O
small	O
relative	B
to	O
the	O
noise	B
level	O
,	O
just	O
introduces	O
an	O
additive	O
constant	O
)	O
.	O
as	O
we	O
decrease	O
the	O
precision	B
to	O
which	O
w	O
is	O
sent	O
,	O
the	O
parameter	O
message	O
shortens	O
,	O
but	O
the	O
data	O
message	O
typically	O
lengthens	O
because	O
the	O
truncated	O
parameters	B
do	O
not	O
match	O
the	O
data	O
so	O
well	O
.	O
there	O
is	O
a	O
non-trivial	O
optimal	B
precision	O
.	O
in	O
simple	O
gaussian	O
cases	O
it	O
is	O
possible	O
to	O
solve	O
for	O
this	O
optimal	B
precision	O
(	O
wallace	O
and	O
freeman	O
,	O
1987	O
)	O
,	O
and	O
it	O
is	O
closely	O
related	O
to	O
the	O
posterior	O
error	O
bars	O
on	O
the	O
parameters	B
,	O
a	O
(	O
cid:0	O
)	O
1	O
,	O
where	O
a	O
=	O
(	O
cid:0	O
)	O
rr	O
ln	O
p	O
(	O
w	O
j	O
d	O
;	O
h	O
)	O
.	O
it	O
turns	O
out	O
that	O
the	O
optimal	B
parameter	O
message	O
length	O
is	O
virtually	O
identical	O
to	O
the	O
log	O
of	O
the	O
occam	O
factor	O
in	O
equation	O
(	O
28.10	O
)	O
.	O
(	O
the	O
random	B
element	O
involved	O
in	O
parameter	O
truncation	O
means	O
that	O
the	O
encoding	O
is	O
slightly	O
sub-optimal	O
.	O
)	O
with	O
care	O
,	O
therefore	O
,	O
one	O
can	O
replicate	O
bayesian	O
results	O
in	O
mdl	O
terms	O
.	O
although	O
some	O
of	O
the	O
earliest	O
work	O
on	O
complex	O
model	B
comparison	I
involved	O
the	O
mdl	O
framework	O
(	O
patrick	O
and	O
wallace	O
,	O
1982	O
)	O
,	O
mdl	O
has	O
no	O
apparent	O
ad-	O
vantages	O
over	O
the	O
direct	O
probabilistic	O
approach	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
28.3	O
:	O
minimum	B
description	I
length	I
(	O
mdl	O
)	O
353	O
mdl	O
does	O
have	O
its	O
uses	O
as	O
a	O
pedagogical	O
tool	O
.	O
the	O
description	O
length	B
concept	O
is	O
useful	O
for	O
motivating	O
prior	B
probability	O
distributions	O
.	O
also	O
,	O
di	O
(	O
cid:11	O
)	O
erent	O
ways	O
of	O
breaking	O
down	O
the	O
task	O
of	O
communicating	O
data	O
using	O
a	O
model	B
can	O
give	O
helpful	O
insights	O
into	O
the	O
modelling	B
process	O
,	O
as	O
will	O
now	O
be	O
illustrated	O
.	O
on-line	O
learning	B
and	O
cross-validation	B
.	O
in	O
cases	O
where	O
the	O
data	O
consist	O
of	O
a	O
sequence	B
of	O
points	O
d	O
=	O
t	O
(	O
1	O
)	O
;	O
t	O
(	O
2	O
)	O
;	O
:	O
:	O
:	O
;	O
t	O
(	O
n	O
)	O
,	O
the	O
log	O
evidence	B
can	O
be	O
decomposed	O
as	O
a	O
sum	O
of	O
‘	O
on-line	O
’	O
predictive	O
perfor-	O
mances	O
:	O
log	O
p	O
(	O
d	O
jh	O
)	O
=	O
log	O
p	O
(	O
t	O
(	O
1	O
)	O
jh	O
)	O
+	O
log	O
p	O
(	O
t	O
(	O
2	O
)	O
j	O
t	O
(	O
1	O
)	O
;	O
h	O
)	O
+	O
log	O
p	O
(	O
t	O
(	O
3	O
)	O
j	O
t	O
(	O
1	O
)	O
;	O
t	O
(	O
2	O
)	O
;	O
h	O
)	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
+	O
log	O
p	O
(	O
t	O
(	O
n	O
)	O
j	O
t	O
(	O
1	O
)	O
:	O
:	O
:	O
t	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
;	O
h	O
)	O
:	O
(	O
28.18	O
)	O
this	O
decomposition	O
can	O
be	O
used	O
to	O
explain	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
the	O
ev-	O
idence	O
and	O
‘	O
leave-one-out	O
cross-validation	B
’	O
as	O
measures	O
of	O
predictive	O
abil-	O
ity	O
.	O
cross-validation	B
examines	O
the	O
average	B
value	O
of	O
just	O
the	O
last	O
term	O
,	O
log	O
p	O
(	O
t	O
(	O
n	O
)	O
j	O
t	O
(	O
1	O
)	O
:	O
:	O
:	O
t	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
;	O
h	O
)	O
,	O
under	O
random	B
re-orderings	O
of	O
the	O
data	O
.	O
the	O
evi-	O
dence	O
,	O
on	O
the	O
other	O
hand	O
,	O
sums	O
up	O
how	O
well	O
the	O
model	B
predicted	O
all	O
the	O
data	O
,	O
starting	O
from	O
scratch	O
.	O
the	O
‘	O
bits-back	O
’	O
encoding	O
method	B
.	O
another	O
mdl	O
thought	O
experiment	O
(	O
hinton	O
and	O
van	O
camp	O
,	O
1993	O
)	O
involves	O
in-	O
corporating	O
random	B
bits	O
into	O
our	O
message	O
.	O
the	O
data	O
are	O
communicated	O
using	O
a	O
parameter	O
block	B
and	O
a	O
data	O
block	O
.	O
the	O
parameter	O
vector	O
sent	O
is	O
a	O
random	B
sam-	O
ple	O
from	O
the	O
posterior	O
,	O
p	O
(	O
w	O
j	O
d	O
;	O
h	O
)	O
=	O
p	O
(	O
d	O
j	O
w	O
;	O
h	O
)	O
p	O
(	O
w	O
jh	O
)	O
=p	O
(	O
d	O
jh	O
)	O
.	O
this	O
sample	B
w	O
is	O
sent	O
to	O
an	O
arbitrary	O
small	O
granularity	O
(	O
cid:14	O
)	O
w	O
using	O
a	O
message	O
length	O
l	O
(	O
w	O
jh	O
)	O
=	O
(	O
cid:0	O
)	O
log	O
[	O
p	O
(	O
w	O
jh	O
)	O
(	O
cid:14	O
)	O
w	O
]	O
.	O
the	O
data	O
are	O
encoded	O
relative	B
to	O
w	O
with	O
a	O
message	O
of	O
length	B
l	O
(	O
d	O
j	O
w	O
;	O
h	O
)	O
=	O
(	O
cid:0	O
)	O
log	O
[	O
p	O
(	O
d	O
j	O
w	O
;	O
h	O
)	O
(	O
cid:14	O
)	O
d	O
]	O
.	O
once	O
the	O
data	O
mes-	O
sage	O
has	O
been	O
received	O
,	O
the	O
random	B
bits	O
used	O
to	O
generate	O
the	O
sample	B
w	O
from	O
the	O
posterior	O
can	O
be	O
deduced	O
by	O
the	O
receiver	O
.	O
the	O
number	O
of	O
bits	O
so	O
recov-	O
ered	O
is	O
(	O
cid:0	O
)	O
log	O
[	O
p	O
(	O
w	O
j	O
d	O
;	O
h	O
)	O
(	O
cid:14	O
)	O
w	O
]	O
.	O
these	O
recovered	O
bits	O
need	O
not	O
count	O
towards	O
the	O
message	O
length	O
,	O
since	O
we	O
might	O
use	O
some	O
other	O
optimally-encoded	O
message	O
as	O
a	O
random	B
bit	O
string	O
,	O
thereby	O
communicating	O
that	O
message	O
at	O
the	O
same	O
time	O
.	O
the	O
net	O
description	O
cost	O
is	O
therefore	O
:	O
l	O
(	O
w	O
jh	O
)	O
+	O
l	O
(	O
d	O
j	O
w	O
;	O
h	O
)	O
(	O
cid:0	O
)	O
‘	O
bits	B
back	I
’	O
=	O
(	O
cid:0	O
)	O
log	O
=	O
(	O
cid:0	O
)	O
log	O
p	O
(	O
d	O
jh	O
)	O
(	O
cid:0	O
)	O
log	O
(	O
cid:14	O
)	O
d	O
:	O
(	O
28.19	O
)	O
thus	O
this	O
thought	O
experiment	O
has	O
yielded	O
the	O
optimal	B
description	O
length	B
.	O
bits-	O
back	O
encoding	O
has	O
been	O
turned	O
into	O
a	O
practical	B
compression	O
method	B
for	O
data	O
modelled	O
with	O
latent	O
variable	O
models	O
by	O
frey	O
(	O
1998	O
)	O
.	O
p	O
(	O
w	O
jh	O
)	O
p	O
(	O
d	O
j	O
w	O
;	O
h	O
)	O
(	O
cid:14	O
)	O
d	O
p	O
(	O
w	O
j	O
d	O
;	O
h	O
)	O
further	O
reading	O
bayesian	O
methods	B
are	O
introduced	O
and	O
contrasted	O
with	O
sampling-theory	O
statis-	O
tics	O
in	O
(	O
jaynes	O
,	O
1983	O
;	O
gull	O
,	O
1988	O
;	O
loredo	O
,	O
1990	O
)	O
.	O
the	O
bayesian	O
occam	O
’	O
s	O
razor	O
is	O
demonstrated	O
on	O
model	O
problems	O
in	O
(	O
gull	O
,	O
1988	O
;	O
mackay	O
,	O
1992a	O
)	O
.	O
useful	O
textbooks	O
are	O
(	O
box	B
and	O
tiao	O
,	O
1973	O
;	O
berger	O
,	O
1985	O
)	O
.	O
one	O
debate	O
worth	O
understanding	O
is	O
the	O
question	O
of	O
whether	O
it	O
’	O
s	O
permis-	O
sible	O
to	O
use	O
improper	B
priors	O
in	O
bayesian	O
inference	B
(	O
dawid	O
et	O
al.	O
,	O
1996	O
)	O
.	O
if	O
we	O
want	O
to	O
do	O
model	O
comparison	O
(	O
as	O
discussed	O
in	O
this	O
chapter	O
)	O
,	O
it	O
is	O
essen-	O
tial	O
to	O
use	O
proper	B
priors	O
{	O
otherwise	O
the	O
evidences	O
and	O
the	O
occam	O
factors	O
are	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
354	O
28	O
|	O
model	B
comparison	I
and	O
occam	O
’	O
s	O
razor	O
meaningless	O
.	O
only	O
when	O
one	O
has	O
no	O
intention	O
to	O
do	O
model	O
comparison	O
may	O
it	O
be	O
safe	O
to	O
use	O
improper	B
priors	O
,	O
and	O
even	O
in	O
such	O
cases	O
there	O
are	O
pitfalls	O
,	O
as	O
dawid	O
et	O
al	O
.	O
explain	O
.	O
i	O
would	O
agree	O
with	O
their	O
advice	O
to	O
always	O
use	O
proper	B
priors	O
,	O
tempered	O
by	O
an	O
encouragement	O
to	O
be	O
smart	O
when	O
making	O
calculations	O
,	O
recognizing	O
opportunities	O
for	O
approximation	O
.	O
28.4	O
exercises	O
p	O
(	O
xjh0	O
)	O
(	O
cid:0	O
)	O
1	O
x	O
1	O
p	O
(	O
xj	O
m	O
=	O
(	O
cid:0	O
)	O
0:4	O
;	O
h1	O
)	O
(	O
cid:0	O
)	O
1	O
1	O
x	O
y	O
=	O
w0	O
+	O
w1x	O
x	O
exercise	O
28.1	O
.	O
[	O
3	O
]	O
random	B
variables	O
x	O
come	O
independently	O
from	O
a	O
probability	B
distribution	O
p	O
(	O
x	O
)	O
.	O
according	O
to	O
model	B
h0	O
,	O
p	O
(	O
x	O
)	O
is	O
a	O
uniform	O
distribu-	O
tion	O
p	O
(	O
xjh0	O
)	O
=	O
1	O
2	O
x	O
2	O
(	O
(	O
cid:0	O
)	O
1	O
;	O
1	O
)	O
:	O
(	O
28.20	O
)	O
according	O
to	O
model	B
h1	O
,	O
p	O
(	O
x	O
)	O
is	O
a	O
nonuniform	O
distribution	B
with	O
an	O
un-	O
known	O
parameter	O
m	O
2	O
(	O
(	O
cid:0	O
)	O
1	O
;	O
1	O
)	O
:	O
1	O
p	O
(	O
xj	O
m	O
;	O
h1	O
)	O
=	O
2	O
x	O
2	O
(	O
(	O
cid:0	O
)	O
1	O
;	O
1	O
)	O
:	O
(	O
1	O
+	O
mx	O
)	O
(	O
28.21	O
)	O
given	O
the	O
data	O
d	O
=	O
f0:3	O
;	O
0:5	O
;	O
0:7	O
;	O
0:8	O
;	O
0:9g	O
,	O
what	O
is	O
the	O
evidence	B
for	O
h0	O
and	O
h1	O
?	O
exercise	O
28.2	O
.	O
[	O
3	O
]	O
datapoints	O
(	O
x	O
;	O
t	O
)	O
are	O
believed	O
to	O
come	O
from	O
a	O
straight	O
line	O
.	O
the	O
experimenter	O
chooses	O
x	O
,	O
and	O
t	O
is	O
gaussian-distributed	O
about	O
y	O
=	O
w0	O
+	O
w1x	O
(	O
28.22	O
)	O
with	O
variance	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
.	O
according	O
to	O
model	B
h1	O
,	O
the	O
straight	O
line	O
is	O
horizontal	O
,	O
so	O
w1	O
=	O
0.	O
according	O
to	O
model	B
h2	O
,	O
w1	O
is	O
a	O
parameter	O
with	O
prior	O
distribu-	O
tion	O
normal	B
(	O
0	O
;	O
1	O
)	O
.	O
both	O
models	O
assign	O
a	O
prior	B
distribution	O
normal	B
(	O
0	O
;	O
1	O
)	O
to	O
w0	O
.	O
given	O
the	O
data	B
set	I
d	O
=	O
f	O
(	O
(	O
cid:0	O
)	O
8	O
;	O
8	O
)	O
;	O
(	O
(	O
cid:0	O
)	O
2	O
;	O
10	O
)	O
;	O
(	O
6	O
;	O
11	O
)	O
g	O
,	O
and	O
assuming	O
the	O
noise	B
level	O
is	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
=	O
1	O
,	O
what	O
is	O
the	O
evidence	B
for	O
each	O
model	B
?	O
exercise	O
28.3	O
.	O
[	O
3	O
]	O
a	O
six-sided	O
die	B
is	O
rolled	O
30	O
times	O
and	O
the	O
numbers	O
of	O
times	O
each	O
face	O
came	O
up	O
were	O
f	O
=	O
f3	O
;	O
3	O
;	O
2	O
;	O
2	O
;	O
9	O
;	O
11g	O
.	O
what	O
is	O
the	O
probability	B
that	O
the	O
die	B
is	O
a	O
perfectly	O
fair	O
die	B
(	O
‘	O
h0	O
’	O
)	O
,	O
assuming	O
the	O
alternative	O
hy-	O
pothesis	O
h1	O
says	O
that	O
the	O
die	B
has	O
a	O
biased	O
distribution	B
p	O
,	O
and	O
the	O
prior	B
density	O
for	O
p	O
is	O
uniform	O
over	O
the	O
simplex	B
pi	O
(	O
cid:21	O
)	O
0	O
,	O
pi	O
pi	O
=	O
1	O
?	O
solve	O
this	O
problem	O
two	O
ways	O
:	O
exactly	O
,	O
using	O
the	O
helpful	O
dirichlet	O
formu-	O
lae	O
(	O
23.30	O
,	O
23.31	O
)	O
,	O
and	O
approximately	O
,	O
using	O
laplace	O
’	O
s	O
method	B
.	O
notice	O
that	O
your	O
choice	O
of	O
basis	O
for	O
the	O
laplace	O
approximation	B
is	O
important	O
.	O
see	O
mackay	O
(	O
1998a	O
)	O
for	O
discussion	O
of	O
this	O
exercise	O
.	O
exercise	O
28.4	O
.	O
[	O
3	O
]	O
the	O
in	O
(	O
cid:13	O
)	O
uence	O
of	O
race	O
on	O
the	O
imposition	O
of	O
the	O
death	O
penalty	O
for	O
murder	O
in	O
america	O
has	O
been	O
much	O
studied	O
.	O
the	O
following	O
three-way	O
table	O
classi	O
(	O
cid:12	O
)	O
es	O
326	O
cases	O
in	O
which	O
the	O
defendant	O
was	O
convicted	O
of	O
mur-	O
der	O
.	O
the	O
three	O
variables	O
are	O
the	O
defendant	O
’	O
s	O
race	B
,	O
the	O
victim	O
’	O
s	O
race	B
,	O
and	O
whether	O
the	O
defendant	O
was	O
sentenced	O
to	O
death	O
.	O
(	O
data	O
from	O
m.	O
radelet	O
,	O
‘	O
racial	O
characteristics	O
and	O
imposition	O
of	O
the	O
death	O
penalty	O
,	O
’	O
american	O
sociological	O
review	O
,	O
46	O
(	O
1981	O
)	O
,	O
pp	O
.	O
918-927	O
.	O
)	O
white	B
defendant	O
black	B
defendant	O
death	B
penalty	I
yes	O
no	O
death	B
penalty	I
yes	O
no	O
white	B
victim	O
19	O
black	B
victim	O
0	O
132	O
9	O
white	B
victim	O
11	O
black	B
victim	O
6	O
52	O
97	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
28.4	O
:	O
exercises	O
355	O
it	O
seems	O
that	O
the	O
death	B
penalty	I
was	O
applied	O
much	O
more	O
often	O
when	O
the	O
victim	O
was	O
white	B
then	O
when	O
the	O
victim	O
was	O
black	B
.	O
when	O
the	O
victim	O
was	O
white	B
14	O
%	O
of	O
defendants	O
got	O
the	O
death	B
penalty	I
,	O
but	O
when	O
the	O
victim	O
was	O
black	B
6	O
%	O
of	O
defendants	O
got	O
the	O
death	B
penalty	I
.	O
[	O
incidentally	O
,	O
these	O
data	O
provide	O
an	O
example	O
of	O
a	O
phenomenon	O
known	O
as	O
simpson	O
’	O
s	O
paradox	B
:	O
a	O
higher	O
fraction	O
of	O
white	O
defendants	O
are	O
sentenced	O
to	O
death	O
overall	O
,	O
but	O
in	O
cases	O
involving	O
black	B
victims	O
a	O
higher	O
fraction	O
of	O
black	O
defendants	O
are	O
sentenced	O
to	O
death	O
and	O
in	O
cases	O
involving	O
white	B
victims	O
a	O
higher	O
fraction	O
of	O
black	O
defendants	O
are	O
sentenced	O
to	O
death	O
.	O
]	O
quantify	O
the	O
evidence	B
for	O
the	O
four	O
alternative	O
hypotheses	O
shown	O
in	O
(	O
cid:12	O
)	O
g-	O
ure	O
28.9.	O
i	O
should	O
mention	O
that	O
i	O
don	O
’	O
t	O
believe	O
any	O
of	O
these	O
models	O
is	O
adequate	O
:	O
several	O
additional	O
variables	O
are	O
important	O
in	O
murder	O
cases	O
,	O
such	O
as	O
whether	O
the	O
victim	O
and	O
murderer	O
knew	O
each	O
other	O
,	O
whether	O
the	O
murder	B
was	O
premeditated	O
,	O
and	O
whether	O
the	O
defendant	O
had	O
a	O
prior	B
crim-	O
inal	O
record	O
;	O
none	O
of	O
these	O
variables	O
is	O
included	O
in	O
the	O
table	O
.	O
so	O
this	O
is	O
an	O
academic	O
exercise	O
in	O
model	O
comparison	O
rather	O
than	O
a	O
serious	O
study	O
of	O
racial	O
bias	B
in	O
the	O
state	O
of	O
florida	O
.	O
the	O
hypotheses	O
are	O
shown	O
as	O
graphical	O
models	O
,	O
with	O
arrows	O
showing	O
dependencies	O
between	O
the	O
variables	O
v	O
(	O
victim	O
race	B
)	O
,	O
m	O
(	O
murderer	O
race	B
)	O
,	O
and	O
d	O
(	O
whether	O
death	B
penalty	I
given	O
)	O
.	O
model	B
h00	O
has	O
only	O
one	O
free	O
parameter	O
,	O
the	O
probability	O
of	O
receiving	O
the	O
death	B
penalty	I
;	O
model	B
h11	O
has	O
four	O
such	O
parameters	B
,	O
one	O
for	O
each	O
state	O
of	O
the	O
variables	O
v	O
and	O
m.	O
assign	O
uniform	O
priors	O
to	O
these	O
variables	O
.	O
how	O
sensitive	O
are	O
the	O
conclusions	O
to	O
the	O
choice	O
of	O
prior	O
?	O
h11	O
h11	O
h10	O
h10	O
v	O
v	O
m	O
m	O
v	O
v	O
m	O
m	O
d	O
d	O
h01	O
h01	O
d	O
d	O
h00	O
h00	O
v	O
v	O
m	O
m	O
v	O
v	O
m	O
m	O
d	O
d	O
d	O
d	O
figure	O
28.9.	O
four	O
hypotheses	O
concerning	O
the	O
dependence	O
of	O
the	O
imposition	O
of	O
the	O
death	O
penalty	O
d	O
on	O
the	O
race	B
of	O
the	O
victim	O
v	O
and	O
the	O
race	B
of	O
the	O
convicted	O
murderer	O
m.	O
h01	O
,	O
for	O
example	O
,	O
asserts	O
that	O
the	O
probability	O
of	O
receiving	O
the	O
death	B
penalty	I
does	O
depend	O
on	O
the	O
murderer	O
’	O
s	O
race	B
,	O
but	O
not	O
on	O
the	O
victim	O
’	O
s	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
chapter	O
29	O
the	O
last	O
couple	O
of	O
chapters	O
have	O
assumed	O
that	O
a	O
gaussian	O
approximation	B
to	O
the	O
probability	B
distribution	O
we	O
are	O
interested	O
in	O
is	O
adequate	O
.	O
what	O
if	O
it	O
is	O
not	O
?	O
we	O
have	O
already	O
seen	O
an	O
example	O
{	O
clustering	B
{	O
where	O
the	O
likelihood	B
function	O
is	O
multimodal	O
,	O
and	O
has	O
nasty	O
unboundedly-high	O
spikes	O
in	O
certain	O
locations	O
in	O
the	O
parameter	O
space	O
;	O
so	O
maximizing	O
the	O
posterior	B
probability	I
and	O
(	O
cid:12	O
)	O
tting	O
a	O
gaussian	O
is	O
not	O
always	O
going	O
to	O
work	O
.	O
this	O
di	O
(	O
cid:14	O
)	O
culty	O
with	O
laplace	O
’	O
s	O
method	B
is	O
one	O
motivation	O
for	O
being	O
interested	O
in	O
monte	O
carlo	O
methods	B
.	O
in	O
fact	O
,	O
monte	O
carlo	O
methods	B
provide	O
a	O
general-purpose	O
set	B
of	O
tools	O
with	O
applications	O
in	O
bayesian	O
data	B
modelling	I
and	O
many	O
other	O
(	O
cid:12	O
)	O
elds	O
.	O
this	O
chapter	O
describes	O
a	O
sequence	B
of	O
methods	B
:	O
importance	B
sampling	I
,	O
re-	O
jection	O
sampling	O
,	O
the	O
metropolis	O
method	B
,	O
gibbs	O
sampling	O
and	O
slice	B
sampling	I
.	O
for	O
each	O
method	B
,	O
we	O
discuss	O
whether	O
the	O
method	B
is	O
expected	O
to	O
be	O
useful	O
for	O
high-dimensional	O
problems	O
such	O
as	O
arise	O
in	O
inference	O
with	O
graphical	O
models	O
.	O
[	O
a	O
graphical	O
model	B
is	O
a	O
probabilistic	B
model	I
in	O
which	O
dependencies	O
and	O
inde-	O
pendencies	O
of	O
variables	O
are	O
represented	O
by	O
edges	O
in	O
a	O
graph	B
whose	O
nodes	O
are	O
the	O
variables	O
.	O
]	O
along	O
the	O
way	O
,	O
the	O
terminology	B
of	O
markov	O
chain	B
monte	O
carlo	O
methods	B
is	O
presented	O
.	O
the	O
subsequent	O
chapter	O
discusses	O
advanced	O
methods	B
for	O
reducing	O
random	B
walk	I
behaviour	O
.	O
for	O
details	O
of	O
monte	O
carlo	O
methods	B
,	O
theorems	O
and	O
proofs	O
and	O
a	O
full	O
list	O
of	O
references	O
,	O
the	O
reader	O
is	O
directed	O
to	O
neal	O
(	O
1993b	O
)	O
,	O
gilks	O
et	O
al	O
.	O
(	O
1996	O
)	O
,	O
and	O
tanner	O
(	O
1996	O
)	O
.	O
in	O
this	O
chapter	O
i	O
will	O
use	O
the	O
word	O
‘	O
sample	B
’	O
in	O
the	O
following	O
sense	O
:	O
a	O
sample	B
from	I
a	O
distribution	B
p	O
(	O
x	O
)	O
is	O
a	O
single	O
realization	O
x	O
whose	O
probability	B
distribution	O
is	O
p	O
(	O
x	O
)	O
.	O
this	O
contrasts	O
with	O
the	O
alternative	O
usage	O
in	B
statistics	I
,	O
where	O
‘	O
sample	B
’	O
refers	O
to	O
a	O
collection	O
of	O
realizations	O
fxg	O
.	O
cation	O
convention	O
:	O
i	O
like	O
my	O
matrices	B
to	O
act	O
to	O
the	O
right	O
,	O
preferring	O
when	O
we	O
discuss	O
transition	B
probability	I
matrices	O
,	O
i	O
will	O
use	O
a	O
right-multipli-	O
u	O
=	O
mv	O
(	O
29.1	O
)	O
to	O
ut	O
=	O
vtmt	O
:	O
(	O
29.2	O
)	O
a	O
transition	B
probability	I
matrix	O
tij	O
or	O
tijj	O
speci	O
(	O
cid:12	O
)	O
es	O
the	O
probability	B
,	O
given	O
the	O
current	O
state	O
is	O
j	O
,	O
of	O
making	O
the	O
transition	B
from	O
j	O
to	O
i.	O
the	O
columns	O
of	O
t	O
are	O
probability	B
vectors	O
.	O
if	O
we	O
write	O
down	O
a	O
transition	B
probability	I
density	O
,	O
we	O
use	O
the	O
same	O
convention	O
for	O
the	O
order	O
of	O
its	O
arguments	O
:	O
t	O
(	O
x0	O
;	O
x	O
)	O
is	O
a	O
transition	B
probability	I
density	O
from	O
x	O
to	O
x0	O
.	O
this	O
unfortunately	O
means	O
that	O
you	O
have	O
to	O
get	O
used	O
to	O
reading	O
from	O
right	O
to	O
left	O
{	O
the	O
sequence	B
xyz	O
has	O
probability	B
t	O
(	O
z	O
;	O
y	O
)	O
t	O
(	O
y	O
;	O
x	O
)	O
(	O
cid:25	O
)	O
(	O
x	O
)	O
.	O
356	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29	O
monte	O
carlo	O
methods	B
29.1	O
the	O
problems	O
to	O
be	O
solved	O
monte	O
carlo	O
methods	B
are	O
computational	O
techniques	O
that	O
make	O
use	O
of	O
random	O
numbers	O
.	O
the	O
aims	O
of	O
monte	O
carlo	O
methods	B
are	O
to	O
solve	O
one	O
or	O
both	O
of	O
the	O
following	O
problems	O
.	O
problem	O
1	O
:	O
to	O
generate	O
samples	O
fx	O
(	O
r	O
)	O
gr	O
r=1	O
from	O
a	O
given	O
probability	B
distribu-	O
tion	O
p	O
(	O
x	O
)	O
.	O
problem	O
2	O
:	O
to	O
estimate	O
expectations	O
of	O
functions	O
under	O
this	O
distribution	B
,	O
for	O
example	O
(	O
cid:8	O
)	O
=	O
h	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
i	O
(	O
cid:17	O
)	O
z	O
dn	O
x	O
p	O
(	O
x	O
)	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
:	O
(	O
29.3	O
)	O
the	O
probability	B
distribution	O
p	O
(	O
x	O
)	O
,	O
which	O
we	O
call	O
the	O
target	O
density	B
,	O
might	O
be	O
a	O
distribution	B
from	O
statistical	B
physics	I
or	O
a	O
conditional	B
distribution	O
arising	O
in	O
data	O
modelling	B
{	O
for	O
example	O
,	O
the	O
posterior	B
probability	I
of	O
a	O
model	B
’	O
s	O
pa-	O
rameters	O
given	O
some	O
observed	O
data	O
.	O
we	O
will	O
generally	O
assume	O
that	O
x	O
is	O
an	O
n	O
-dimensional	O
vector	O
with	O
real	O
components	O
xn	O
,	O
but	O
we	O
will	O
sometimes	O
con-	O
sider	O
discrete	O
spaces	O
also	O
.	O
simple	O
examples	O
of	O
functions	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
whose	O
expectations	O
we	O
might	O
be	O
inter-	O
ested	O
in	O
include	O
the	O
(	O
cid:12	O
)	O
rst	O
and	O
second	O
moments	O
of	O
quantities	O
that	O
we	O
wish	O
to	O
predict	O
,	O
from	O
which	O
we	O
can	O
compute	O
means	O
and	O
variances	O
;	O
for	O
example	O
if	O
some	O
quantity	O
t	O
depends	O
on	O
x	O
,	O
we	O
can	O
(	O
cid:12	O
)	O
nd	O
the	O
mean	B
and	O
variance	B
of	O
t	O
under	O
p	O
(	O
x	O
)	O
by	O
(	O
cid:12	O
)	O
nding	O
the	O
expectations	O
of	O
the	O
functions	O
(	O
cid:30	O
)	O
1	O
(	O
x	O
)	O
=	O
t	O
(	O
x	O
)	O
and	O
(	O
cid:30	O
)	O
2	O
(	O
x	O
)	O
=	O
(	O
t	O
(	O
x	O
)	O
)	O
2	O
,	O
then	O
using	O
(	O
cid:8	O
)	O
1	O
(	O
cid:17	O
)	O
e	O
[	O
(	O
cid:30	O
)	O
1	O
(	O
x	O
)	O
]	O
and	O
(	O
cid:8	O
)	O
2	O
(	O
cid:17	O
)	O
e	O
[	O
(	O
cid:30	O
)	O
2	O
(	O
x	O
)	O
]	O
;	O
(	O
cid:22	O
)	O
t	O
=	O
(	O
cid:8	O
)	O
1	O
and	O
var	O
(	O
t	O
)	O
=	O
(	O
cid:8	O
)	O
2	O
(	O
cid:0	O
)	O
(	O
cid:8	O
)	O
2	O
1	O
:	O
(	O
29.4	O
)	O
(	O
29.5	O
)	O
it	O
is	O
assumed	O
that	O
p	O
(	O
x	O
)	O
is	O
su	O
(	O
cid:14	O
)	O
ciently	O
complex	B
that	O
we	O
can	O
not	O
evaluate	O
these	O
expectations	O
by	O
exact	O
methods	B
;	O
so	O
we	O
are	O
interested	O
in	O
monte	O
carlo	O
methods	B
.	O
we	O
will	O
concentrate	O
on	O
the	O
(	O
cid:12	O
)	O
rst	O
problem	O
(	O
sampling	O
)	O
,	O
because	O
if	O
we	O
have	O
solved	O
it	O
,	O
then	O
we	O
can	O
solve	O
the	O
second	O
problem	O
by	O
using	O
the	O
random	B
samples	O
fx	O
(	O
r	O
)	O
gr	O
r=1	O
to	O
give	O
the	O
estimator	B
^	O
(	O
cid:8	O
)	O
(	O
cid:17	O
)	O
(	O
cid:30	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
:	O
(	O
29.6	O
)	O
1	O
rxr	O
r=1	O
are	O
generated	O
from	O
p	O
(	O
x	O
)	O
then	O
the	O
expectation	B
of	O
^	O
(	O
cid:8	O
)	O
is	O
if	O
the	O
vectors	B
fx	O
(	O
r	O
)	O
gr	O
(	O
cid:8	O
)	O
.	O
also	O
,	O
as	O
the	O
number	O
of	O
samples	O
r	O
increases	O
,	O
the	O
variance	B
of	O
^	O
(	O
cid:8	O
)	O
will	O
decrease	O
as	O
(	O
cid:27	O
)	O
2/r	O
,	O
where	O
(	O
cid:27	O
)	O
2	O
is	O
the	O
variance	B
of	O
(	O
cid:30	O
)	O
,	O
(	O
cid:27	O
)	O
2	O
=z	O
dn	O
x	O
p	O
(	O
x	O
)	O
(	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
(	O
cid:8	O
)	O
)	O
2	O
:	O
357	O
(	O
29.7	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29	O
|	O
monte	O
carlo	O
methods	B
figure	O
29.1	O
.	O
(	O
a	O
)	O
the	O
function	B
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=	O
exp	O
(	O
cid:2	O
)	O
0:4	O
(	O
x	O
(	O
cid:0	O
)	O
0:4	O
)	O
2	O
(	O
cid:0	O
)	O
0:08x4	O
(	O
cid:3	O
)	O
.	O
how	O
to	O
draw	O
samples	O
from	O
this	O
density	B
?	O
(	O
b	O
)	O
the	O
function	B
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
evaluated	O
at	O
a	O
discrete	O
set	O
of	O
uniformly	O
spaced	O
points	O
fxig	O
.	O
how	O
to	O
draw	O
samples	O
from	O
this	O
discrete	O
distribution	O
?	O
358	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
(	O
a	O
)	O
p*	O
(	O
x	O
)	O
-4	O
-2	O
0	O
2	O
4	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
(	O
b	O
)	O
p*	O
(	O
x	O
)	O
-4	O
-2	O
0	O
2	O
4	O
this	O
is	O
one	O
of	O
the	O
important	O
properties	O
of	O
monte	O
carlo	O
methods	B
.	O
the	O
accuracy	O
of	O
the	O
monte	O
carlo	O
estimate	O
(	O
29.6	O
)	O
depends	O
only	O
on	O
the	O
variance	B
of	O
(	O
cid:30	O
)	O
,	O
not	O
on	O
the	O
dimensionality	O
of	O
the	O
space	O
sampled	O
.	O
to	O
be	O
precise	O
,	O
the	O
variance	B
of	O
^	O
(	O
cid:8	O
)	O
goes	O
as	O
(	O
cid:27	O
)	O
2=r	O
.	O
so	O
regardless	O
of	O
the	O
dimensionality	O
of	O
x	O
,	O
it	O
may	O
be	O
that	O
as	O
few	O
as	O
a	O
dozen	O
independent	O
samples	O
fx	O
(	O
r	O
)	O
g	O
su	O
(	O
cid:14	O
)	O
ce	O
to	O
estimate	O
(	O
cid:8	O
)	O
satisfactorily	O
.	O
we	O
will	O
(	O
cid:12	O
)	O
nd	O
later	O
,	O
however	O
,	O
that	O
high	O
dimensionality	O
can	O
cause	O
other	O
di	O
(	O
cid:14	O
)	O
-	O
culties	O
for	O
monte	O
carlo	O
methods	B
.	O
obtaining	O
independent	O
samples	O
from	O
a	O
given	O
distribution	B
p	O
(	O
x	O
)	O
is	O
often	O
not	O
easy	O
.	O
why	O
is	O
sampling	O
from	O
p	O
(	O
x	O
)	O
hard	O
?	O
we	O
will	O
assume	O
that	O
the	O
density	B
from	O
which	O
we	O
wish	O
to	O
draw	O
samples	O
,	O
p	O
(	O
x	O
)	O
,	O
can	O
be	O
evaluated	O
,	O
at	O
least	O
to	O
within	O
a	O
multiplicative	O
constant	O
;	O
that	O
is	O
,	O
we	O
can	O
evaluate	O
a	O
function	B
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
such	O
that	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=z	O
:	O
(	O
29.8	O
)	O
if	O
we	O
can	O
evaluate	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
,	O
why	O
can	O
we	O
not	O
easily	O
solve	O
problem	O
1	O
?	O
why	O
is	O
it	O
in	O
general	O
di	O
(	O
cid:14	O
)	O
cult	O
to	O
obtain	O
samples	O
from	O
p	O
(	O
x	O
)	O
?	O
there	O
are	O
two	O
di	O
(	O
cid:14	O
)	O
culties	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
is	O
that	O
we	O
typically	O
do	O
not	O
know	O
the	O
normalizing	B
constant	I
z	O
=z	O
dn	O
x	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
:	O
(	O
29.9	O
)	O
the	O
second	O
is	O
that	O
,	O
even	O
if	O
we	O
did	O
know	O
z	O
,	O
the	O
problem	O
of	O
drawing	O
samples	O
from	O
p	O
(	O
x	O
)	O
is	O
still	O
a	O
challenging	O
one	O
,	O
especially	O
in	O
high-dimensional	O
spaces	O
,	O
because	O
there	O
is	O
no	O
obvious	O
way	O
to	O
sample	B
from	I
p	O
without	O
enumerating	O
most	O
or	O
all	O
of	O
the	O
possible	O
states	O
.	O
correct	O
samples	O
from	O
p	O
will	O
by	O
de	O
(	O
cid:12	O
)	O
nition	O
tend	O
to	O
come	O
from	O
places	O
in	O
x-space	O
where	O
p	O
(	O
x	O
)	O
is	O
big	O
;	O
how	O
can	O
we	O
identify	O
those	O
places	O
where	O
p	O
(	O
x	O
)	O
is	O
big	O
,	O
without	O
evaluating	O
p	O
(	O
x	O
)	O
everywhere	O
?	O
there	O
are	O
only	O
a	O
few	O
high-dimensional	O
densities	O
from	O
which	O
it	O
is	O
easy	O
to	O
draw	O
samples	O
,	O
for	O
example	O
the	O
gaussian	O
distribution	B
.	O
let	O
us	O
start	O
with	O
a	O
simple	O
one-dimensional	O
example	O
.	O
imagine	O
that	O
we	O
wish	O
to	O
draw	O
samples	O
from	O
the	O
density	B
p	O
(	O
x	O
)	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=z	O
where	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=	O
exp	O
(	O
cid:2	O
)	O
0:4	O
(	O
x	O
(	O
cid:0	O
)	O
0:4	O
)	O
2	O
(	O
cid:0	O
)	O
0:08x4	O
(	O
cid:3	O
)	O
;	O
x	O
2	O
(	O
(	O
cid:0	O
)	O
1	O
;	O
1	O
)	O
:	O
we	O
can	O
plot	O
this	O
function	B
(	O
(	O
cid:12	O
)	O
gure	O
29.1a	O
)	O
.	O
but	O
that	O
does	O
not	O
mean	B
we	O
can	O
draw	O
samples	O
from	O
it	O
.	O
to	O
start	O
with	O
,	O
we	O
don	O
’	O
t	O
know	O
the	O
normalizing	B
constant	I
z	O
.	O
(	O
29.10	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29.1	O
:	O
the	O
problems	O
to	O
be	O
solved	O
359	O
to	O
give	O
ourselves	O
a	O
simpler	O
problem	O
,	O
we	O
could	O
discretize	O
the	O
variable	O
x	O
and	O
ask	O
for	O
samples	O
from	O
the	O
discrete	O
probability	O
distribution	B
over	O
a	O
(	O
cid:12	O
)	O
nite	O
set	B
of	O
uniformly	O
spaced	O
points	O
fxig	O
(	O
(	O
cid:12	O
)	O
gure	O
29.1b	O
)	O
.	O
how	O
could	O
we	O
solve	O
this	O
problem	O
?	O
if	O
we	O
evaluate	O
p	O
(	O
cid:3	O
)	O
i	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
xi	O
)	O
at	O
each	O
point	O
xi	O
,	O
we	O
can	O
compute	O
p	O
(	O
cid:3	O
)	O
i	O
z	O
=xi	O
(	O
29.11	O
)	O
and	O
pi	O
=	O
p	O
(	O
cid:3	O
)	O
i	O
=z	O
(	O
29.12	O
)	O
and	O
we	O
can	O
then	O
sample	B
from	I
the	O
probability	B
distribution	O
fpig	O
using	O
various	O
methods	B
based	O
on	O
a	O
source	O
of	O
random	B
bits	O
(	O
see	O
section	O
6.3	O
)	O
.	O
but	O
what	O
is	O
the	O
cost	O
of	O
this	O
procedure	O
,	O
and	O
how	O
does	O
it	O
scale	O
with	O
the	O
dimensionality	O
of	O
the	O
space	O
,	O
n	O
?	O
let	O
us	O
concentrate	O
on	O
the	O
initial	O
cost	O
of	O
evaluating	O
z	O
(	O
29.11	O
)	O
.	O
to	O
compute	O
z	O
we	O
have	O
to	O
visit	O
every	O
point	O
in	O
the	O
space	O
.	O
in	O
(	O
cid:12	O
)	O
gure	O
29.1b	O
there	O
are	O
50	O
uniformly	O
spaced	O
points	O
in	O
one	O
dimension	O
.	O
if	O
our	O
system	O
had	O
n	O
dimensions	B
,	O
n	O
=	O
1000	O
say	O
,	O
then	O
the	O
corresponding	O
number	O
of	O
points	O
would	O
be	O
501000	O
,	O
an	O
unimaginable	O
number	O
of	O
evaluations	O
of	O
p	O
(	O
cid:3	O
)	O
.	O
even	O
if	O
each	O
component	O
xn	O
took	O
only	O
two	O
discrete	O
values	O
,	O
the	O
number	O
of	O
evaluations	O
of	O
p	O
(	O
cid:3	O
)	O
would	O
be	O
21000	O
,	O
a	O
number	O
that	O
is	O
still	O
horribly	O
huge	O
.	O
if	O
every	O
electron	O
in	O
the	O
universe	O
(	O
there	O
are	O
about	O
2266	O
of	O
them	O
)	O
were	O
a	O
1000	O
gigahertz	O
computer	B
that	O
could	O
evaluate	O
p	O
(	O
cid:3	O
)	O
for	O
a	O
trillion	O
(	O
240	O
)	O
states	O
every	O
second	O
,	O
and	O
if	O
we	O
ran	O
those	O
2266	O
computers	O
for	O
a	O
time	O
equal	O
to	O
the	O
age	O
of	O
the	O
universe	O
(	O
258	O
seconds	O
)	O
,	O
they	O
would	O
still	O
only	O
visit	O
2364	O
states	O
.	O
we	O
’	O
d	O
have	O
to	O
wait	O
for	O
more	O
than	O
2636	O
’	O
10190	O
universe	O
ages	O
to	O
elapse	O
before	O
all	O
21000	O
states	O
had	O
been	O
visited	O
.	O
systems	O
with	O
21000	O
states	O
are	O
two	O
a	O
penny.	O
?	O
one	O
example	O
is	O
a	O
collection	O
of	O
1000	O
spins	O
such	O
as	O
a	O
30	O
(	O
cid:2	O
)	O
30	O
fragment	O
of	O
an	O
ising	O
model	B
whose	O
probability	B
distribution	O
is	O
proportional	O
to	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=	O
exp	O
[	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
)	O
]	O
where	O
xn	O
2	O
f	O
(	O
cid:6	O
)	O
1g	O
and	O
e	O
(	O
x	O
)	O
=	O
(	O
cid:0	O
)	O
''	O
1	O
2xm	O
;	O
n	O
jmnxmxn	O
+xn	O
hnxn	O
#	O
:	O
(	O
29.13	O
)	O
(	O
29.14	O
)	O
the	O
energy	B
function	O
e	O
(	O
x	O
)	O
is	O
readily	O
evaluated	O
for	O
any	O
x.	O
but	O
if	O
we	O
wish	O
to	O
evaluate	O
this	O
function	B
at	O
all	O
states	O
x	O
,	O
the	O
computer	B
time	O
required	O
would	O
be	O
21000	O
function	B
evaluations	O
.	O
the	O
ising	O
model	B
is	O
a	O
simple	O
model	O
which	O
has	O
been	O
around	O
for	O
a	O
long	O
time	O
,	O
but	O
the	O
task	O
of	O
generating	O
samples	O
from	O
the	O
distribution	B
p	O
(	O
x	O
)	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=z	O
is	O
still	O
an	O
active	O
research	O
area	O
;	O
the	O
(	O
cid:12	O
)	O
rst	O
‘	O
exact	O
’	O
samples	O
from	O
this	O
distribution	B
were	O
created	O
in	O
the	O
pioneering	O
work	O
of	O
propp	O
and	O
wilson	O
(	O
1996	O
)	O
,	O
as	O
we	O
’	O
ll	O
describe	O
in	O
chapter	O
32.	O
a	O
useful	O
analogy	O
imagine	O
the	O
tasks	O
of	O
drawing	O
random	B
water	O
samples	O
from	O
a	O
lake	B
and	O
(	O
cid:12	O
)	O
nding	O
the	O
average	B
plankton	O
concentration	O
(	O
(	O
cid:12	O
)	O
gure	O
29.2	O
)	O
.	O
the	O
depth	O
of	O
the	O
lake	B
at	O
x	O
=	O
(	O
x	O
;	O
y	O
)	O
is	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
,	O
and	O
we	O
assert	O
(	O
in	O
order	O
to	O
make	O
the	O
analogy	O
work	O
)	O
that	O
the	O
plankton	B
concentration	O
is	O
a	O
function	B
of	O
x	O
,	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
.	O
the	O
required	O
average	B
concentration	O
is	O
an	O
integral	B
like	O
(	O
29.3	O
)	O
,	O
namely	O
(	O
cid:8	O
)	O
=	O
h	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
i	O
(	O
cid:17	O
)	O
1	O
z	O
z	O
dn	O
x	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
;	O
(	O
29.15	O
)	O
?	O
translation	O
for	O
american	O
readers	O
:	O
‘	O
such	O
systems	O
are	O
a	O
dime	O
a	O
dozen	O
’	O
;	O
incidentally	O
,	O
this	O
equivalence	B
(	O
10c	O
=	O
6p	O
)	O
shows	O
that	O
the	O
correct	O
exchange	B
rate	I
between	O
our	O
currencies	O
is	O
$	O
1.00	O
=	O
$	O
1.67	O
.	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
figure	O
29.2.	O
a	O
lake	B
whose	O
depth	O
at	O
x	O
=	O
(	O
x	O
;	O
y	O
)	O
is	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
 	B
 	I
 	I
 	I
 	I
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
360	O
29	O
|	O
monte	O
carlo	O
methods	B
where	O
z	O
=	O
r	O
dx	O
dy	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
is	O
the	O
volume	B
of	O
the	O
lake	B
.	O
you	O
are	O
provided	O
with	O
a	O
boat	O
,	O
a	O
satellite	O
navigation	O
system	O
,	O
and	O
a	O
plumbline	O
.	O
using	O
the	O
navigator	O
,	O
you	O
can	O
take	O
your	O
boat	O
to	O
any	O
desired	O
location	O
x	O
on	O
the	O
map	O
;	O
using	O
the	O
plumbline	O
you	O
can	O
measure	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
at	O
that	O
point	O
.	O
you	O
can	O
also	O
measure	O
the	O
plankton	B
concentration	O
there	O
.	O
problem	O
1	O
is	O
to	O
draw	O
1	O
cm3	O
water	O
samples	O
at	O
random	B
from	O
the	O
lake	B
,	O
in	O
such	O
a	O
way	O
that	O
each	O
sample	B
is	O
equally	O
likely	O
to	O
come	O
from	O
any	O
point	O
within	O
the	O
lake	B
.	O
problem	O
2	O
is	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
average	B
plankton	O
concentration	O
.	O
these	O
are	O
di	O
(	O
cid:14	O
)	O
cult	O
problems	O
to	O
solve	O
because	O
at	O
the	O
outset	O
we	O
know	O
nothing	O
about	O
the	O
depth	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
perhaps	O
much	O
of	O
the	O
volume	O
of	O
the	O
lake	O
is	O
contained	O
in	O
narrow	O
,	O
deep	O
underwater	O
canyons	O
(	O
(	O
cid:12	O
)	O
gure	O
29.3	O
)	O
,	O
in	O
which	O
case	O
,	O
to	O
correctly	O
sample	B
from	I
the	O
lake	B
and	O
correctly	O
estimate	O
(	O
cid:8	O
)	O
our	O
method	B
must	O
implicitly	O
discover	O
the	O
canyons	O
and	O
(	O
cid:12	O
)	O
nd	O
their	O
volume	B
relative	O
to	O
the	O
rest	O
of	O
the	O
lake	O
.	O
di	O
(	O
cid:14	O
)	O
cult	O
problems	O
,	O
yes	O
;	O
nevertheless	O
,	O
we	O
’	O
ll	O
see	O
that	O
clever	O
monte	O
carlo	O
methods	B
can	O
solve	O
them	O
.	O
uniform	O
sampling	O
having	O
accepted	O
that	O
we	O
can	O
not	O
exhaustively	O
visit	O
every	O
location	O
x	O
in	O
the	O
state	O
space	O
,	O
we	O
might	O
consider	O
trying	O
to	O
solve	O
the	O
second	O
problem	O
(	O
estimating	O
the	O
expectation	B
of	O
a	O
function	B
(	O
cid:30	O
)	O
(	O
x	O
)	O
)	O
by	O
drawing	O
random	B
samples	O
fx	O
(	O
r	O
)	O
gr	O
r=1	O
uniformly	O
from	O
the	O
state	O
space	O
and	O
evaluating	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
at	O
those	O
points	O
.	O
then	O
we	O
could	O
introduce	O
a	O
normalizing	B
constant	I
zr	O
,	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
figure	O
29.3.	O
a	O
slice	O
through	O
a	O
lake	B
that	O
includes	O
some	O
canyons	O
.	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
;	O
(	O
29.16	O
)	O
r	O
zr	O
=	O
xr=1	O
and	O
estimate	O
(	O
cid:8	O
)	O
=r	O
dn	O
x	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
by	O
xr=1	O
r	O
^	O
(	O
cid:8	O
)	O
=	O
(	O
cid:30	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
zr	O
:	O
(	O
29.17	O
)	O
is	O
anything	O
wrong	O
with	O
this	O
strategy	O
?	O
well	O
,	O
it	O
depends	O
on	O
the	O
functions	B
(	O
cid:30	O
)	O
(	O
x	O
)	O
and	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
let	O
us	O
assume	O
that	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
is	O
a	O
benign	O
,	O
smoothly	O
varying	O
function	B
and	O
concentrate	O
on	O
the	O
nature	O
of	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
as	O
we	O
learnt	O
in	O
chapter	O
4	O
,	O
a	O
high-	O
dimensional	O
distribution	B
is	O
often	O
concentrated	O
in	O
a	O
small	O
region	O
of	O
the	O
state	O
space	O
known	O
as	O
its	O
typical	B
set	I
t	O
,	O
whose	O
volume	B
is	O
given	O
by	O
jtj	O
’	O
2h	O
(	O
x	O
)	O
,	O
where	O
h	O
(	O
x	O
)	O
is	O
the	O
entropy	B
of	O
the	O
probability	B
distribution	O
p	O
(	O
x	O
)	O
.	O
if	O
almost	O
all	O
the	O
probability	B
mass	O
is	O
located	O
in	O
the	O
typical	B
set	I
and	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
is	O
a	O
benign	O
function	B
,	O
the	O
value	O
of	O
(	O
cid:8	O
)	O
=r	O
dn	O
x	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
will	O
be	O
principally	O
determined	O
by	O
the	O
values	O
that	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
takes	O
on	O
in	O
the	O
typical	B
set	I
.	O
so	O
uniform	O
sampling	O
will	O
only	O
stand	O
a	O
chance	O
of	O
giving	O
a	O
good	B
estimate	O
of	O
(	O
cid:8	O
)	O
if	O
we	O
make	O
the	O
number	O
of	O
samples	O
r	O
su	O
(	O
cid:14	O
)	O
ciently	O
large	O
that	O
we	O
are	O
likely	O
to	O
hit	O
the	O
typical	B
set	I
at	O
least	O
once	O
or	O
twice	O
.	O
so	O
,	O
how	O
many	O
samples	O
are	O
required	O
?	O
let	O
us	O
take	O
the	O
case	O
of	O
the	O
ising	O
model	B
again	O
.	O
(	O
strictly	O
,	O
the	O
ising	O
model	B
may	O
not	O
be	O
a	O
good	B
example	O
,	O
since	O
it	O
doesn	O
’	O
t	O
necessarily	O
have	O
a	O
typical	B
set	I
,	O
as	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
chapter	O
4	O
;	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
a	O
typical	B
set	I
was	O
that	O
all	O
states	O
had	O
log	O
probability	B
close	O
to	O
the	O
entropy	B
,	O
which	O
for	O
an	O
ising	O
model	B
would	O
mean	B
that	O
the	O
energy	B
is	O
very	O
close	O
to	O
the	O
mean	B
energy	O
;	O
but	O
in	O
the	O
vicinity	O
of	O
phase	O
transitions	O
,	O
the	O
variance	B
of	O
energy	B
,	O
also	O
known	O
as	O
the	O
heat	B
capacity	I
,	O
may	O
diverge	O
,	O
which	O
means	O
that	O
the	O
energy	B
of	O
a	O
random	B
state	O
is	O
not	O
necessarily	O
expected	O
to	O
be	O
very	O
close	O
to	O
the	O
mean	B
energy	O
.	O
)	O
the	O
total	O
size	O
of	O
the	O
state	O
space	O
is	O
2n	O
states	O
,	O
and	O
the	O
typical	B
set	I
has	O
size	O
2h	O
.	O
so	O
each	O
sample	B
has	O
a	O
chance	O
of	O
2h	O
=2n	O
of	O
falling	O
in	O
the	O
typical	B
set	I
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29.2	O
:	O
importance	B
sampling	I
361	O
64	O
log	O
(	O
2	O
)	O
y	O
p	O
o	O
r	O
t	O
n	O
e	O
figure	O
29.4	O
.	O
(	O
a	O
)	O
entropy	B
of	O
a	O
64-spin	O
ising	O
model	B
as	O
a	O
function	B
of	O
temperature	B
.	O
(	O
b	O
)	O
one	O
state	O
of	O
a	O
1024-spin	O
ising	O
model	B
.	O
0	O
0	O
(	O
a	O
)	O
1	O
2	O
3	O
temperature	B
4	O
5	O
6	O
(	O
b	O
)	O
the	O
number	O
of	O
samples	O
required	O
to	O
hit	O
the	O
typical	B
set	I
once	O
is	O
thus	O
of	O
order	O
rmin	O
’	O
2n	O
(	O
cid:0	O
)	O
h	O
:	O
(	O
29.18	O
)	O
so	O
,	O
what	O
is	O
h	O
?	O
at	O
high	O
temperatures	O
,	O
the	O
probability	B
distribution	O
of	O
an	O
ising	O
model	B
tends	O
to	O
a	O
uniform	O
distribution	B
and	O
the	O
entropy	B
tends	O
to	O
hmax	O
=	O
n	O
bits	O
,	O
which	O
means	O
rmin	O
is	O
of	O
order	O
1.	O
under	O
these	O
conditions	O
,	O
uniform	O
sampling	O
may	O
well	O
be	O
a	O
satisfactory	O
technique	O
for	O
estimating	O
(	O
cid:8	O
)	O
.	O
but	O
high	O
temperatures	O
are	O
not	O
of	O
great	O
interest	O
.	O
considerably	O
more	O
interesting	O
are	O
intermediate	O
tem-	O
peratures	O
such	O
as	O
the	O
critical	O
temperature	O
at	O
which	O
the	O
ising	O
model	B
melts	O
from	O
an	O
ordered	O
phase	O
to	O
a	O
disordered	O
phase	O
.	O
the	O
critical	O
temperature	O
of	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
ising	O
model	B
,	O
at	O
which	O
it	O
melts	O
,	O
is	O
(	O
cid:18	O
)	O
c	O
=	O
2:27.	O
at	O
this	O
temperature	B
the	O
entropy	B
of	O
an	O
ising	O
model	B
is	O
roughly	O
n=2	O
bits	O
(	O
(	O
cid:12	O
)	O
gure	O
29.4	O
)	O
.	O
for	O
this	O
probability	B
dis-	O
tribution	O
the	O
number	O
of	O
samples	O
required	O
simply	O
to	O
hit	O
the	O
typical	B
set	I
once	O
is	O
of	O
order	O
rmin	O
’	O
2n	O
(	O
cid:0	O
)	O
n=2	O
=	O
2n=2	O
;	O
(	O
29.19	O
)	O
which	O
for	O
n	O
=	O
1000	O
is	O
about	O
10150.	O
this	O
is	O
roughly	O
the	O
square	B
of	O
the	O
number	O
of	O
particles	O
in	O
the	O
universe	O
.	O
thus	O
uniform	O
sampling	O
is	O
utterly	O
useless	O
for	O
the	O
study	O
of	O
ising	O
models	O
of	O
modest	O
size	O
.	O
and	O
in	O
most	O
high-dimensional	O
problems	O
,	O
if	O
the	O
distribution	B
p	O
(	O
x	O
)	O
is	O
not	O
actually	O
uniform	O
,	O
uniform	O
sampling	O
is	O
unlikely	O
to	O
be	O
useful	O
.	O
overview	O
having	O
established	O
that	O
drawing	O
samples	O
from	O
a	O
high-dimensional	O
distribution	B
p	O
(	O
x	O
)	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=z	O
is	O
di	O
(	O
cid:14	O
)	O
cult	O
even	O
if	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
is	O
easy	O
to	O
evaluate	O
,	O
we	O
will	O
now	O
study	O
a	O
sequence	B
of	O
more	O
sophisticated	O
monte	O
carlo	O
methods	B
:	O
importance	B
sampling	I
,	O
rejection	B
sampling	I
,	O
the	O
metropolis	O
method	B
,	O
gibbs	O
sampling	O
,	O
and	O
slice	O
sampling	O
.	O
29.2	O
importance	B
sampling	I
importance	O
sampling	O
is	O
not	O
a	O
method	B
for	O
generating	O
samples	O
from	O
p	O
(	O
x	O
)	O
(	O
prob-	O
lem	O
1	O
)	O
;	O
it	O
is	O
just	O
a	O
method	B
for	O
estimating	O
the	O
expectation	B
of	O
a	O
function	B
(	O
cid:30	O
)	O
(	O
x	O
)	O
(	O
problem	O
2	O
)	O
.	O
it	O
can	O
be	O
viewed	O
as	O
a	O
generalization	B
of	O
the	O
uniform	O
sampling	O
method	O
.	O
for	O
illustrative	O
purposes	O
,	O
let	O
us	O
imagine	O
that	O
the	O
target	O
distribution	B
is	O
a	O
one-dimensional	O
density	B
p	O
(	O
x	O
)	O
.	O
let	O
us	O
assume	O
that	O
we	O
are	O
able	O
to	O
evaluate	O
this	O
density	B
at	O
any	O
chosen	O
point	O
x	O
,	O
at	O
least	O
to	O
within	O
a	O
multiplicative	O
constant	O
;	O
thus	O
we	O
can	O
evaluate	O
a	O
function	B
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
such	O
that	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=z	O
:	O
(	O
29.20	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
362	O
29	O
|	O
monte	O
carlo	O
methods	B
but	O
p	O
(	O
x	O
)	O
is	O
too	O
complicated	O
a	O
function	B
for	O
us	O
to	O
be	O
able	O
to	O
sample	B
from	I
it	O
directly	O
.	O
we	O
now	O
assume	O
that	O
we	O
have	O
a	O
simpler	O
density	B
q	O
(	O
x	O
)	O
from	O
which	O
we	O
can	O
generate	O
samples	O
and	O
which	O
we	O
can	O
evaluate	O
to	O
within	O
a	O
multiplicative	O
constant	O
(	O
that	O
is	O
,	O
we	O
can	O
evaluate	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
,	O
where	O
q	O
(	O
x	O
)	O
=	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=zq	O
)	O
.	O
an	O
example	O
of	O
the	O
functions	O
p	O
(	O
cid:3	O
)	O
,	O
q	O
(	O
cid:3	O
)	O
and	O
(	O
cid:30	O
)	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
29.5.	O
we	O
call	O
q	O
the	O
sampler	B
density	I
.	O
in	O
importance	O
sampling	O
,	O
we	O
generate	O
r	O
samples	O
fx	O
(	O
r	O
)	O
gr	O
r=1	O
from	O
q	O
(	O
x	O
)	O
.	O
if	O
these	O
points	O
were	O
samples	O
from	O
p	O
(	O
x	O
)	O
then	O
we	O
could	O
estimate	O
(	O
cid:8	O
)	O
by	O
equa-	O
tion	O
(	O
29.6	O
)	O
.	O
but	O
when	O
we	O
generate	O
samples	O
from	O
q	O
,	O
values	O
of	O
x	O
where	O
q	O
(	O
x	O
)	O
is	O
greater	O
than	O
p	O
(	O
x	O
)	O
will	O
be	O
over-represented	O
in	O
this	O
estimator	B
,	O
and	O
points	O
where	O
q	O
(	O
x	O
)	O
is	O
less	O
than	O
p	O
(	O
x	O
)	O
will	O
be	O
under-represented	O
.	O
to	O
take	O
into	O
account	O
the	O
fact	O
that	O
we	O
have	O
sampled	O
from	O
the	O
wrong	O
distribution	B
,	O
we	O
introduce	O
weights	O
wr	O
(	O
cid:17	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
(	O
29.21	O
)	O
which	O
we	O
use	O
to	O
adjust	O
the	O
‘	O
importance	O
’	O
of	O
each	O
point	O
in	O
our	O
estimator	B
thus	O
:	O
^	O
(	O
cid:8	O
)	O
(	O
cid:17	O
)	O
pr	O
wr	O
(	O
cid:30	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
pr	O
wr	O
:	O
(	O
29.22	O
)	O
.	O
exercise	O
29.1	O
.	O
[	O
2	O
,	O
p.384	O
]	O
prove	O
that	O
,	O
if	O
q	O
(	O
x	O
)	O
is	O
non-zero	O
for	O
all	O
x	O
where	O
p	O
(	O
x	O
)	O
is	O
non-zero	O
,	O
the	O
estimator	B
^	O
(	O
cid:8	O
)	O
converges	O
to	O
(	O
cid:8	O
)	O
,	O
the	O
mean	B
value	O
of	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
,	O
as	O
r	O
increases	O
.	O
what	O
is	O
the	O
variance	B
of	O
this	O
estimator	B
,	O
asymptotically	O
?	O
hint	O
:	O
consider	O
the	O
statistics	O
of	O
the	O
numerator	O
and	O
the	O
denominator	O
separately	O
.	O
is	O
the	O
estimator	B
^	O
(	O
cid:8	O
)	O
an	O
unbiased	B
estimator	I
for	O
small	O
r	O
?	O
a	O
practical	B
di	O
(	O
cid:14	O
)	O
culty	O
with	O
importance	O
sampling	O
is	O
that	O
it	O
is	O
hard	O
to	O
estimate	O
how	O
reliable	O
the	O
estimator	B
^	O
(	O
cid:8	O
)	O
is	O
.	O
the	O
variance	B
of	O
the	O
estimator	B
is	O
unknown	O
beforehand	O
,	O
because	O
it	O
depends	O
on	O
an	O
integral	B
over	O
x	O
of	O
a	O
function	B
involving	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
and	O
the	O
variance	B
of	O
^	O
(	O
cid:8	O
)	O
is	O
hard	O
to	O
estimate	O
,	O
because	O
the	O
empirical	O
variances	O
of	O
the	O
quantities	O
wr	O
and	O
wr	O
(	O
cid:30	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
are	O
not	O
necessarily	O
a	O
good	B
guide	O
to	O
the	O
true	O
variances	O
of	O
the	O
numerator	O
and	O
denominator	O
in	O
equation	O
(	O
29.22	O
)	O
.	O
if	O
the	O
proposal	B
density	I
q	O
(	O
x	O
)	O
is	O
small	O
in	O
a	O
region	O
where	O
j	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
j	O
is	O
large	O
then	O
it	O
is	O
quite	O
possible	O
,	O
even	O
after	O
many	O
points	O
x	O
(	O
r	O
)	O
have	O
been	O
generated	O
,	O
that	O
none	O
of	O
them	O
will	O
have	O
fallen	O
in	O
that	O
region	O
.	O
in	O
this	O
case	O
the	O
estimate	O
of	O
(	O
cid:8	O
)	O
would	O
be	O
drastically	O
wrong	O
,	O
and	O
there	O
would	O
be	O
no	O
indication	O
in	O
the	O
empirical	O
variance	B
that	O
the	O
true	O
variance	B
of	O
the	O
estimator	B
^	O
(	O
cid:8	O
)	O
is	O
large	O
.	O
-6.2	O
-6.4	O
-6.6	O
-6.8	O
-7	O
-7.2	O
(	O
a	O
)	O
-6.2	O
-6.4	O
-6.6	O
-6.8	O
-7	O
(	O
b	O
)	O
-7.2	O
10	O
100	O
1000	O
10000	O
100000	O
1000000	O
10	O
100	O
1000	O
10000	O
100000	O
1000000	O
cautionary	O
illustration	O
of	O
importance	O
sampling	O
in	O
a	O
toy	O
problem	O
related	O
to	O
the	O
modelling	B
of	O
amino	B
acid	I
probability	O
distribu-	O
tions	O
with	O
a	O
one-dimensional	O
variable	O
x	O
,	O
i	O
evaluated	O
a	O
quantity	O
of	O
interest	O
us-	O
ing	O
importance	B
sampling	I
.	O
the	O
results	O
using	O
a	O
gaussian	O
sampler	O
and	O
a	O
cauchy	O
sampler	O
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
29.6.	O
the	O
horizontal	O
axis	O
shows	O
the	O
number	O
of	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
x	O
figure	O
29.5.	O
functions	B
involved	O
in	O
importance	O
sampling	O
.	O
we	O
wish	O
to	O
estimate	O
the	O
expectation	B
of	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
under	O
p	O
(	O
x	O
)	O
/	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
we	O
can	O
generate	O
samples	O
from	O
the	O
simpler	O
distribution	B
q	O
(	O
x	O
)	O
/	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
we	O
can	O
evaluate	O
q	O
(	O
cid:3	O
)	O
and	O
p	O
(	O
cid:3	O
)	O
at	O
any	O
point	O
.	O
figure	O
29.6.	O
importance	B
sampling	I
in	O
action	O
:	O
(	O
a	O
)	O
using	O
a	O
gaussian	O
sampler	B
density	I
;	O
(	O
b	O
)	O
using	O
a	O
cauchy	O
sampler	B
density	I
.	O
vertical	O
axis	O
shows	O
the	O
estimate	O
^	O
(	O
cid:8	O
)	O
.	O
the	O
horizontal	O
line	O
indicates	O
the	O
true	O
value	O
of	O
(	O
cid:8	O
)	O
.	O
horizontal	O
axis	O
shows	O
number	O
of	O
samples	O
on	O
a	O
log	O
scale	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
363	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
phi	O
(	O
x	O
)	O
-5	O
0	O
5	O
10	O
15	O
figure	O
29.7.	O
a	O
multimodal	O
distribution	B
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
and	O
a	O
unimodal	O
sampler	O
q	O
(	O
x	O
)	O
.	O
29.2	O
:	O
importance	B
sampling	I
samples	O
on	O
a	O
log	O
scale	O
.	O
in	O
the	O
case	O
of	O
the	O
gaussian	O
sampler	O
,	O
after	O
about	O
500	O
samples	O
had	O
been	O
evaluated	O
one	O
might	O
be	O
tempted	O
to	O
call	O
a	O
halt	O
;	O
but	O
evidently	O
there	O
are	O
infrequent	O
samples	O
that	O
make	O
a	O
huge	O
contribution	O
to	O
^	O
(	O
cid:8	O
)	O
,	O
and	O
the	O
value	O
of	O
the	O
estimate	O
at	O
500	O
samples	O
is	O
wrong	O
.	O
even	O
after	O
a	O
million	O
samples	O
have	O
been	O
taken	O
,	O
the	O
estimate	O
has	O
still	O
not	O
settled	O
down	O
close	O
to	O
the	O
true	O
value	O
.	O
in	O
contrast	O
,	O
the	O
cauchy	O
sampler	O
does	O
not	O
su	O
(	O
cid:11	O
)	O
er	O
from	O
glitches	O
;	O
it	O
converges	O
(	O
on	O
the	O
scale	O
shown	O
here	O
)	O
after	O
about	O
5000	O
samples	O
.	O
this	O
example	O
illustrates	O
the	O
fact	O
that	O
an	O
importance	O
sampler	O
should	O
have	O
heavy	O
tails	O
.	O
exercise	O
29.2	O
.	O
[	O
2	O
,	O
p.385	O
]	O
consider	O
the	O
situation	O
where	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
is	O
multimodal	O
,	O
con-	O
sisting	O
of	O
several	O
widely-separated	O
peaks	O
.	O
(	O
probability	B
distributions	I
like	O
this	O
arise	O
frequently	O
in	O
statistical	O
data	B
modelling	I
.	O
)	O
discuss	O
whether	O
it	O
is	O
a	O
wise	O
strategy	O
to	O
do	O
importance	O
sampling	O
using	O
a	O
sampler	O
q	O
(	O
x	O
)	O
that	O
is	O
a	O
unimodal	O
distribution	B
(	O
cid:12	O
)	O
tted	O
to	O
one	O
of	O
these	O
peaks	O
.	O
assume	O
that	O
the	O
function	B
(	O
cid:30	O
)	O
(	O
x	O
)	O
whose	O
mean	B
(	O
cid:8	O
)	O
is	O
to	O
be	O
estimated	O
is	O
a	O
smoothly	O
vary-	O
ing	O
function	B
of	O
x	O
such	O
as	O
mx	O
+	O
c.	O
describe	O
the	O
typical	B
evolution	O
of	O
the	O
estimator	O
^	O
(	O
cid:8	O
)	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
samples	O
r.	O
importance	B
sampling	I
in	O
many	O
dimensions	B
we	O
have	O
already	O
observed	O
that	O
care	O
is	O
needed	O
in	O
one-dimensional	O
importance	B
sampling	I
problems	O
.	O
is	O
importance	B
sampling	I
a	O
useful	O
technique	O
in	O
spaces	O
of	O
higher	O
dimensionality	O
,	O
say	O
n	O
=	O
1000	O
?	O
consider	O
a	O
simple	O
case-study	O
where	O
the	O
target	O
density	B
p	O
(	O
x	O
)	O
is	O
a	O
uniform	O
distribution	B
inside	O
a	O
sphere	O
,	O
where	O
(	O
cid:26	O
)	O
(	O
x	O
)	O
(	O
cid:17	O
)	O
(	O
pi	O
x2	O
the	O
origin	O
,	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=	O
(	O
cid:26	O
)	O
1	O
0	O
(	O
cid:20	O
)	O
(	O
cid:26	O
)	O
(	O
x	O
)	O
(	O
cid:20	O
)	O
rp	O
0	O
(	O
cid:26	O
)	O
(	O
x	O
)	O
>	O
rp	O
;	O
(	O
29.23	O
)	O
i	O
)	O
1=2	O
,	O
and	O
the	O
proposal	B
density	I
is	O
a	O
gaussian	O
centred	O
on	O
q	O
(	O
x	O
)	O
=yi	O
normal	B
(	O
xi	O
;	O
0	O
;	O
(	O
cid:27	O
)	O
2	O
)	O
:	O
(	O
29.24	O
)	O
an	O
importance-sampling	O
method	B
will	O
be	O
in	O
trouble	O
if	O
the	O
estimator	B
^	O
(	O
cid:8	O
)	O
is	O
dom-	O
inated	O
by	O
a	O
few	O
large	O
weights	O
wr	O
.	O
what	O
will	O
be	O
the	O
typical	B
range	O
of	O
values	O
of	O
the	O
weights	O
wr	O
?	O
we	O
know	O
from	O
our	O
discussions	O
of	O
typical	O
sequences	O
in	O
part	O
i	O
{	O
see	O
exercise	O
6.14	O
(	O
p.124	O
)	O
,	O
for	O
example	O
{	O
that	O
if	O
(	O
cid:26	O
)	O
is	O
the	O
distance	B
from	O
the	O
origin	O
of	O
a	O
sample	B
from	I
q	O
,	O
the	O
quantity	O
(	O
cid:26	O
)	O
2	O
has	O
a	O
roughly	O
gaussian	O
distribution	B
with	O
mean	B
and	O
standard	B
deviation	I
:	O
(	O
cid:26	O
)	O
2	O
(	O
cid:24	O
)	O
n	O
(	O
cid:27	O
)	O
2	O
(	O
cid:6	O
)	O
p2n	O
(	O
cid:27	O
)	O
2	O
:	O
(	O
29.25	O
)	O
thus	O
almost	O
all	O
samples	O
from	O
q	O
lie	O
in	O
a	O
typical	B
set	I
with	O
distance	B
from	O
the	O
origin	O
very	O
close	O
to	O
pn	O
(	O
cid:27	O
)	O
.	O
let	O
us	O
assume	O
that	O
(	O
cid:27	O
)	O
is	O
chosen	O
such	O
that	O
the	O
typical	B
set	I
of	O
q	O
lies	O
inside	O
the	O
sphere	O
of	O
radius	O
rp	O
.	O
[	O
if	O
it	O
does	O
not	O
,	O
then	O
the	O
law	B
of	I
large	I
numbers	I
implies	O
that	O
almost	O
all	O
the	O
samples	O
generated	O
from	O
q	O
will	O
fall	O
outside	O
rp	O
and	O
will	O
have	O
weight	B
zero	O
.	O
]	O
then	O
we	O
know	O
that	O
most	O
samples	O
from	O
q	O
will	O
have	O
a	O
value	O
of	O
q	O
that	O
lies	O
in	O
the	O
range	O
1	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
)	O
n=2	O
exp	O
(	O
cid:0	O
)	O
n	O
2	O
(	O
cid:6	O
)	O
p2n	O
2	O
!	O
:	O
thus	O
the	O
weights	O
wr	O
=	O
p	O
(	O
cid:3	O
)	O
=q	O
will	O
typically	O
have	O
values	O
in	O
the	O
range	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
)	O
n=2	O
exp	O
n	O
2	O
(	O
cid:6	O
)	O
p2n	O
2	O
!	O
:	O
(	O
29.26	O
)	O
(	O
29.27	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
364	O
(	O
a	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
cq	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
(	O
b	O
)	O
cq	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
u	O
x	O
x	O
x	O
so	O
if	O
we	O
draw	O
a	O
hundred	O
samples	O
,	O
what	O
will	O
the	O
typical	B
range	O
of	O
weights	O
be	O
?	O
we	O
can	O
roughly	O
estimate	O
the	O
ratio	O
of	O
the	O
largest	O
weight	B
to	O
the	O
median	O
weight	B
by	O
doubling	O
the	O
standard	B
deviation	I
in	O
equation	O
(	O
29.27	O
)	O
.	O
the	O
largest	O
weight	B
and	O
the	O
median	O
weight	B
will	O
typically	O
be	O
in	O
the	O
ratio	O
:	O
29	O
|	O
monte	O
carlo	O
methods	B
figure	O
29.8.	O
rejection	B
sampling	I
.	O
(	O
a	O
)	O
the	O
functions	B
involved	O
in	O
rejection	O
sampling	O
.	O
we	O
desire	O
samples	O
from	O
p	O
(	O
x	O
)	O
/	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
we	O
are	O
able	O
to	O
draw	O
samples	O
from	O
q	O
(	O
x	O
)	O
/	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
,	O
and	O
we	O
know	O
a	O
value	O
c	O
such	O
that	O
c	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
>	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
for	O
all	O
x	O
.	O
(	O
b	O
)	O
a	O
point	O
(	O
x	O
;	O
u	O
)	O
is	O
generated	O
at	O
random	B
in	O
the	O
lightly	O
shaded	O
area	O
under	O
the	O
curve	O
c	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
if	O
this	O
point	O
also	O
lies	O
below	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
then	O
it	O
is	O
accepted	O
.	O
r	O
wmax	O
wmed	O
r	O
=	O
exp	O
(	O
cid:16	O
)	O
p2n	O
(	O
cid:17	O
)	O
:	O
(	O
29.28	O
)	O
in	O
n	O
=	O
1000	O
dimensions	B
therefore	O
,	O
the	O
largest	O
weight	B
after	O
one	O
hundred	O
sam-	O
ples	O
is	O
likely	O
to	O
be	O
roughly	O
1019	O
times	O
greater	O
than	O
the	O
median	O
weight	B
.	O
thus	O
an	O
importance	B
sampling	I
estimate	O
for	O
a	O
high-dimensional	O
problem	O
will	O
very	O
likely	O
be	O
utterly	O
dominated	O
by	O
a	O
few	O
samples	O
with	O
huge	O
weights	O
.	O
in	O
conclusion	O
,	O
importance	B
sampling	I
in	O
high	B
dimensions	I
often	O
su	O
(	O
cid:11	O
)	O
ers	O
from	O
two	O
di	O
(	O
cid:14	O
)	O
culties	O
.	O
first	O
,	O
we	O
need	O
to	O
obtain	O
samples	O
that	O
lie	O
in	O
the	O
typical	B
set	I
of	O
p	O
,	O
and	O
this	O
may	O
take	O
a	O
long	O
time	O
unless	O
q	O
is	O
a	O
good	B
approximation	O
to	O
p	O
.	O
second	O
,	O
even	O
if	O
we	O
obtain	O
samples	O
in	O
the	O
typical	B
set	I
,	O
the	O
weights	O
associated	O
with	O
those	O
samples	O
are	O
likely	O
to	O
vary	O
by	O
large	O
factors	O
,	O
because	O
the	O
probabilities	O
of	O
points	O
in	O
a	O
typical	B
set	I
,	O
although	O
similar	O
to	O
each	O
other	O
,	O
still	O
di	O
(	O
cid:11	O
)	O
er	O
by	O
factors	O
of	O
order	O
exp	O
(	O
pn	O
)	O
,	O
so	O
the	O
weights	O
will	O
too	O
,	O
unless	O
q	O
is	O
a	O
near-perfect	O
approximation	B
to	O
p	O
.	O
29.3	O
rejection	B
sampling	I
we	O
assume	O
again	O
a	O
one-dimensional	O
density	B
p	O
(	O
x	O
)	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=z	O
that	O
is	O
too	O
com-	O
plicated	O
a	O
function	B
for	O
us	O
to	O
be	O
able	O
to	O
sample	B
from	I
it	O
directly	O
.	O
we	O
assume	O
that	O
we	O
have	O
a	O
simpler	O
proposal	B
density	I
q	O
(	O
x	O
)	O
which	O
we	O
can	O
evaluate	O
(	O
within	O
a	O
multiplicative	O
factor	O
zq	O
,	O
as	O
before	O
)	O
,	O
and	O
from	O
which	O
we	O
can	O
generate	O
samples	O
.	O
we	O
further	O
assume	O
that	O
we	O
know	O
the	O
value	O
of	O
a	O
constant	O
c	O
such	O
that	O
c	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
>	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
;	O
for	O
all	O
x	O
:	O
(	O
29.29	O
)	O
a	O
schematic	O
picture	O
of	O
the	O
two	O
functions	B
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
29.8a	O
.	O
we	O
generate	O
two	O
random	B
numbers	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
,	O
x	O
,	O
is	O
generated	O
from	O
the	O
proposal	B
density	I
q	O
(	O
x	O
)	O
.	O
we	O
then	O
evaluate	O
c	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
and	O
generate	O
a	O
uniformly	O
distributed	O
random	B
variable	I
u	O
from	O
the	O
interval	O
[	O
0	O
;	O
c	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
]	O
.	O
these	O
two	O
random	B
numbers	O
can	O
be	O
viewed	O
as	O
selecting	O
a	O
point	O
in	O
the	O
two-dimensional	B
plane	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
29.8b	O
.	O
we	O
now	O
evaluate	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
and	O
accept	O
or	O
reject	O
the	O
sample	B
x	O
by	O
comparing	O
the	O
value	O
of	O
u	O
with	O
the	O
value	O
of	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
if	O
u	O
>	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
then	O
x	O
is	O
rejected	O
;	O
otherwise	O
it	O
is	O
accepted	O
,	O
which	O
means	O
that	O
we	O
add	O
x	O
to	O
our	O
set	B
of	O
samples	O
fx	O
(	O
r	O
)	O
g.	O
the	O
value	O
of	O
u	O
is	O
discarded	O
.	O
why	O
does	O
this	O
procedure	O
generate	O
samples	O
from	O
p	O
(	O
x	O
)	O
?	O
the	O
proposed	O
point	O
(	O
x	O
;	O
u	O
)	O
comes	O
with	O
uniform	O
probability	B
from	O
the	O
lightly	O
shaded	O
area	O
underneath	O
the	O
curve	O
c	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
29.8b	O
.	O
the	O
rejection	B
rule	O
rejects	O
all	O
the	O
points	O
that	O
lie	O
above	O
the	O
curve	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
so	O
the	O
points	O
(	O
x	O
;	O
u	O
)	O
that	O
are	O
accepted	O
are	O
uniformly	O
distributed	O
in	O
the	O
heavily	O
shaded	O
area	O
under	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
this	O
implies	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29.4	O
:	O
the	O
metropolis	O
{	O
hastings	O
method	B
that	O
the	O
probability	B
density	O
of	O
the	O
x-coordinates	O
of	O
the	O
accepted	O
points	O
must	O
be	O
proportional	O
to	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
,	O
so	O
the	O
samples	O
must	O
be	O
independent	O
samples	O
from	O
p	O
(	O
x	O
)	O
.	O
rejection	B
sampling	I
will	O
work	O
best	O
if	O
q	O
is	O
a	O
good	B
approximation	O
to	O
p	O
.	O
if	O
q	O
is	O
very	O
di	O
(	O
cid:11	O
)	O
erent	O
from	O
p	O
then	O
,	O
for	O
c	O
q	O
to	O
exceed	O
p	O
everywhere	O
,	O
c	O
will	O
necessarily	O
have	O
to	O
be	O
large	O
and	O
the	O
frequency	B
of	O
rejection	B
will	O
be	O
large	O
.	O
rejection	B
sampling	I
in	O
many	O
dimensions	B
in	O
a	O
high-dimensional	O
problem	O
it	O
is	O
very	O
likely	O
that	O
the	O
requirement	O
that	O
c	O
q	O
(	O
cid:3	O
)	O
be	O
an	O
upper	O
bound	B
for	O
p	O
(	O
cid:3	O
)	O
will	O
force	O
c	O
to	O
be	O
so	O
huge	O
that	O
acceptances	O
will	O
be	O
very	O
rare	O
indeed	O
.	O
finding	O
such	O
a	O
value	O
of	O
c	O
may	O
be	O
di	O
(	O
cid:14	O
)	O
cult	O
too	O
,	O
since	O
in	O
many	O
problems	O
we	O
know	O
neither	O
where	O
the	O
modes	O
of	O
p	O
(	O
cid:3	O
)	O
are	O
located	O
nor	O
how	O
high	O
they	O
are	O
.	O
as	O
a	O
case	O
study	O
,	O
consider	O
a	O
pair	O
of	O
n	O
-dimensional	O
gaussian	O
distributions	O
with	O
mean	O
zero	O
(	O
(	O
cid:12	O
)	O
gure	O
29.9	O
)	O
.	O
imagine	O
generating	O
samples	O
from	O
one	O
with	O
stan-	O
dard	O
deviation	O
(	O
cid:27	O
)	O
q	O
and	O
using	O
rejection	B
sampling	I
to	O
obtain	O
samples	O
from	O
the	O
other	O
whose	O
standard	B
deviation	I
is	O
(	O
cid:27	O
)	O
p	O
.	O
let	O
us	O
assume	O
that	O
these	O
two	O
standard	O
deviations	O
are	O
close	O
in	O
value	O
{	O
say	O
,	O
(	O
cid:27	O
)	O
q	O
is	O
1	O
%	O
larger	O
than	O
(	O
cid:27	O
)	O
p	O
.	O
[	O
(	O
cid:27	O
)	O
q	O
must	O
be	O
larger	O
than	O
(	O
cid:27	O
)	O
p	O
because	O
if	O
this	O
is	O
not	O
the	O
case	O
,	O
there	O
is	O
no	O
c	O
such	O
that	O
c	O
q	O
exceeds	O
p	O
for	O
all	O
x	O
.	O
]	O
so	O
,	O
what	O
value	O
of	O
c	O
is	O
required	O
if	O
the	O
dimensionality	O
is	O
n	O
=	O
1000	O
?	O
the	O
density	B
of	O
q	O
(	O
x	O
)	O
at	O
the	O
origin	O
is	O
1=	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
q	O
)	O
n=2	O
,	O
so	O
for	O
c	O
q	O
to	O
exceed	O
p	O
we	O
need	O
to	O
set	B
c	O
=	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
q	O
)	O
n=2	O
p	O
)	O
n=2	O
=	O
exp	O
(	O
cid:18	O
)	O
n	O
ln	O
(	O
cid:27	O
)	O
q	O
(	O
cid:27	O
)	O
p	O
(	O
cid:19	O
)	O
:	O
(	O
29.30	O
)	O
with	O
n	O
=	O
1000	O
and	O
(	O
cid:27	O
)	O
q	O
=	O
1:01	O
,	O
we	O
(	O
cid:12	O
)	O
nd	O
c	O
=	O
exp	O
(	O
10	O
)	O
’	O
20,000.	O
what	O
will	O
the	O
(	O
cid:27	O
)	O
p	O
acceptance	B
rate	I
be	O
for	O
this	O
value	O
of	O
c	O
?	O
the	O
answer	O
is	O
immediate	O
:	O
since	O
the	O
acceptance	B
rate	I
is	O
the	O
ratio	O
of	O
the	O
volume	O
under	O
the	O
curve	O
p	O
(	O
x	O
)	O
to	O
the	O
volume	B
under	O
c	O
q	O
(	O
x	O
)	O
,	O
the	O
fact	O
that	O
p	O
and	O
q	O
are	O
both	O
normalized	O
here	O
implies	O
that	O
the	O
acceptance	B
rate	I
will	O
be	O
1=c	O
,	O
for	O
example	O
,	O
1/20,000	O
.	O
in	O
general	O
,	O
c	O
grows	O
exponentially	O
with	O
the	O
dimensionality	O
n	O
,	O
so	O
the	O
acceptance	B
rate	I
is	O
expected	O
to	O
be	O
exponentially	O
small	O
in	O
n	O
.	O
rejection	B
sampling	I
,	O
therefore	O
,	O
whilst	O
a	O
useful	O
method	B
for	O
one-dimensional	O
problems	O
,	O
is	O
not	O
expected	O
to	O
be	O
a	O
practical	B
technique	O
for	O
generating	O
samples	O
from	O
high-dimensional	O
distributions	O
p	O
(	O
x	O
)	O
.	O
29.4	O
the	O
metropolis	O
{	O
hastings	O
method	B
importance	O
sampling	O
and	O
rejection	B
sampling	I
work	O
well	O
only	O
if	O
the	O
proposal	B
density	I
q	O
(	O
x	O
)	O
is	O
similar	O
to	O
p	O
(	O
x	O
)	O
.	O
in	O
large	O
and	O
complex	O
problems	O
it	O
is	O
di	O
(	O
cid:14	O
)	O
cult	O
to	O
create	O
a	O
single	O
density	O
q	O
(	O
x	O
)	O
that	O
has	O
this	O
property	O
.	O
the	O
metropolis	O
{	O
hastings	O
algorithm	B
instead	O
makes	O
use	O
of	O
a	O
proposal	O
den-	O
sity	O
q	O
which	O
depends	O
on	O
the	O
current	O
state	O
x	O
(	O
t	O
)	O
.	O
the	O
density	B
q	O
(	O
x0	O
;	O
x	O
(	O
t	O
)	O
)	O
might	O
be	O
a	O
simple	O
distribution	O
such	O
as	O
a	O
gaussian	O
centred	O
on	O
the	O
current	O
x	O
(	O
t	O
)	O
.	O
the	O
proposal	B
density	I
q	O
(	O
x0	O
;	O
x	O
)	O
can	O
be	O
any	O
(	O
cid:12	O
)	O
xed	O
density	B
from	O
which	O
we	O
can	O
draw	O
samples	O
.	O
in	O
contrast	O
to	O
importance	B
sampling	I
and	O
rejection	B
sampling	I
,	O
it	O
is	O
not	O
necessary	O
that	O
q	O
(	O
x0	O
;	O
x	O
(	O
t	O
)	O
)	O
look	O
at	O
all	O
similar	O
to	O
p	O
(	O
x	O
)	O
in	O
order	O
for	O
the	O
algorithm	B
to	O
be	O
practically	O
useful	O
.	O
an	O
example	O
of	O
a	O
proposal	B
density	I
is	O
shown	O
in	O
(	O
cid:12	O
)	O
g-	O
ure	O
29.10	O
;	O
this	O
(	O
cid:12	O
)	O
gure	O
shows	O
the	O
density	B
q	O
(	O
x0	O
;	O
x	O
(	O
t	O
)	O
)	O
for	O
two	O
di	O
(	O
cid:11	O
)	O
erent	O
states	O
x	O
(	O
1	O
)	O
and	O
x	O
(	O
2	O
)	O
.	O
as	O
before	O
,	O
we	O
assume	O
that	O
we	O
can	O
evaluate	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
for	O
any	O
x.	O
a	O
tentative	O
new	O
state	O
x0	O
is	O
generated	O
from	O
the	O
proposal	B
density	I
q	O
(	O
x0	O
;	O
x	O
(	O
t	O
)	O
)	O
.	O
to	O
decide	O
365	O
p	O
(	O
x	O
)	O
cq	O
(	O
x	O
)	O
-4	O
-3	O
-2	O
-1	O
0	O
1	O
2	O
3	O
4	O
figure	O
29.9.	O
a	O
gaussian	O
p	O
(	O
x	O
)	O
and	O
a	O
slightly	O
broader	O
gaussian	O
q	O
(	O
x	O
)	O
scaled	O
up	O
by	O
a	O
factor	O
c	O
such	O
that	O
c	O
q	O
(	O
x	O
)	O
(	O
cid:21	O
)	O
p	O
(	O
x	O
)	O
.	O
q	O
(	O
x	O
;	O
x	O
(	O
1	O
)	O
)	O
x	O
(	O
1	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
x	O
q	O
(	O
x	O
;	O
x	O
(	O
2	O
)	O
)	O
x	O
(	O
2	O
)	O
x	O
figure	O
29.10.	O
metropolis	O
{	O
hastings	O
method	B
in	O
one	O
dimension	O
.	O
the	O
proposal	O
distribution	O
q	O
(	O
x0	O
;	O
x	O
)	O
is	O
here	O
shown	O
as	O
having	O
a	O
shape	O
that	O
changes	O
as	O
x	O
changes	O
,	O
though	O
this	O
is	O
not	O
typical	B
of	O
the	O
proposal	O
densities	O
used	O
in	O
practice	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
366	O
29	O
|	O
monte	O
carlo	O
methods	B
whether	O
to	O
accept	O
the	O
new	O
state	O
,	O
we	O
compute	O
the	O
quantity	O
a	O
=	O
:	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
(	O
t	O
)	O
)	O
q	O
(	O
x	O
(	O
t	O
)	O
;	O
x0	O
)	O
q	O
(	O
x0	O
;	O
x	O
(	O
t	O
)	O
)	O
if	O
a	O
(	O
cid:21	O
)	O
1	O
then	O
the	O
new	O
state	O
is	O
accepted	O
.	O
otherwise	O
,	O
the	O
new	O
state	O
is	O
accepted	O
with	O
probability	O
a	O
.	O
(	O
29.31	O
)	O
if	O
the	O
step	O
is	O
accepted	O
,	O
we	O
set	B
x	O
(	O
t+1	O
)	O
=	O
x0	O
.	O
if	O
the	O
step	O
is	O
rejected	O
,	O
then	O
we	O
set	B
x	O
(	O
t+1	O
)	O
=	O
x	O
(	O
t	O
)	O
.	O
note	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
from	O
rejection	O
sampling	O
:	O
in	O
rejection	O
sampling	O
,	O
rejected	O
points	O
are	O
discarded	O
and	O
have	O
no	O
in	O
(	O
cid:13	O
)	O
uence	O
on	O
the	O
list	O
of	O
samples	O
fx	O
(	O
r	O
)	O
g	O
that	O
we	O
collected	O
.	O
here	O
,	O
a	O
rejection	B
causes	O
the	O
current	O
state	O
to	O
be	O
written	O
again	O
onto	O
the	O
list	O
.	O
notation	B
.	O
i	O
have	O
used	O
the	O
superscript	O
r	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
r	O
to	O
label	O
points	O
that	O
are	O
independent	O
samples	O
from	O
a	O
distribution	B
,	O
and	O
the	O
superscript	O
t	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
t	O
to	O
label	O
the	O
sequence	B
of	O
states	O
in	O
a	O
markov	O
chain	B
.	O
it	O
is	O
important	O
to	O
note	O
that	O
a	O
metropolis	O
{	O
hastings	O
simulation	O
of	O
t	O
iterations	O
does	O
not	O
produce	O
t	O
indepen-	O
dent	O
samples	O
from	O
the	O
target	O
distribution	B
p	O
.	O
the	O
samples	O
are	O
dependent	O
.	O
to	O
compute	O
the	O
acceptance	O
probability	O
(	O
29.31	O
)	O
we	O
need	O
to	O
be	O
able	O
to	O
com-	O
pute	O
the	O
probability	B
ratios	O
p	O
(	O
x0	O
)	O
=p	O
(	O
x	O
(	O
t	O
)	O
)	O
and	O
q	O
(	O
x	O
(	O
t	O
)	O
;	O
x0	O
)	O
=q	O
(	O
x0	O
;	O
x	O
(	O
t	O
)	O
)	O
.	O
if	O
the	O
proposal	B
density	I
is	O
a	O
simple	O
symmetrical	O
density	B
such	O
as	O
a	O
gaussian	O
centred	O
on	O
the	O
current	O
point	O
,	O
then	O
the	O
latter	O
factor	O
is	O
unity	O
,	O
and	O
the	O
metropolis	O
{	O
hastings	O
method	B
simply	O
involves	O
comparing	O
the	O
value	O
of	O
the	O
target	O
density	B
at	O
the	O
two	O
points	O
.	O
this	O
special	O
case	O
is	O
sometimes	O
called	O
the	O
metropolis	O
method	B
.	O
how-	O
ever	O
,	O
with	O
apologies	O
to	O
hastings	O
,	O
i	O
will	O
call	O
the	O
general	O
metropolis	O
{	O
hastings	O
algorithm	B
for	O
asymmetric	O
q	O
‘	O
the	O
metropolis	O
method	B
’	O
since	O
i	O
believe	O
important	O
ideas	O
deserve	O
short	O
names	O
.	O
convergence	O
of	O
the	O
metropolis	O
method	B
to	O
the	O
target	O
density	B
it	O
can	O
be	O
shown	O
that	O
for	O
any	O
positive	O
q	O
(	O
that	O
is	O
,	O
any	O
q	O
such	O
that	O
q	O
(	O
x0	O
;	O
x	O
)	O
>	O
0	O
for	O
all	O
x	O
;	O
x0	O
)	O
,	O
as	O
t	O
!	O
1	O
,	O
the	O
probability	B
distribution	O
of	O
x	O
(	O
t	O
)	O
tends	O
to	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=z	O
.	O
[	O
this	O
statement	O
should	O
not	O
be	O
seen	O
as	O
implying	O
that	O
q	O
has	O
to	O
assign	O
positive	O
probability	O
to	O
every	O
point	O
x0	O
{	O
we	O
will	O
discuss	O
examples	O
later	O
where	O
q	O
(	O
x0	O
;	O
x	O
)	O
=	O
0	O
for	O
some	O
x	O
;	O
x0	O
;	O
notice	O
also	O
that	O
we	O
have	O
said	O
nothing	O
about	O
how	O
rapidly	O
the	O
convergence	O
to	O
p	O
(	O
x	O
)	O
takes	O
place	O
.	O
]	O
the	O
metropolis	O
method	B
is	O
an	O
example	O
of	O
a	O
markov	O
chain	B
monte	O
carlo	O
method	B
(	O
abbreviated	O
mcmc	O
)	O
.	O
in	O
contrast	O
to	O
rejection	B
sampling	I
,	O
where	O
the	O
accepted	O
points	O
fx	O
(	O
r	O
)	O
g	O
are	O
independent	O
samples	O
from	O
the	O
desired	O
distribution	B
,	O
markov	O
chain	B
monte	O
carlo	O
methods	B
involve	O
a	O
markov	O
process	O
in	O
which	O
a	O
se-	O
quence	O
of	O
states	O
fx	O
(	O
t	O
)	O
g	O
is	O
generated	O
,	O
each	O
sample	B
x	O
(	O
t	O
)	O
having	O
a	O
probability	B
distribution	O
that	O
depends	O
on	O
the	O
previous	O
value	O
,	O
x	O
(	O
t	O
(	O
cid:0	O
)	O
1	O
)	O
.	O
since	O
successive	O
sam-	O
ples	O
are	O
dependent	O
,	O
the	O
markov	O
chain	B
may	O
have	O
to	O
be	O
run	O
for	O
a	O
considerable	O
time	O
in	O
order	O
to	O
generate	O
samples	O
that	O
are	O
e	O
(	O
cid:11	O
)	O
ectively	O
independent	O
samples	O
from	O
p	O
.	O
just	O
as	O
it	O
was	O
di	O
(	O
cid:14	O
)	O
cult	O
to	O
estimate	O
the	O
variance	B
of	O
an	O
importance	B
sampling	I
estimator	O
,	O
so	O
it	O
is	O
di	O
(	O
cid:14	O
)	O
cult	O
to	O
assess	O
whether	O
a	O
markov	O
chain	B
monte	O
carlo	O
method	B
has	O
‘	O
converged	O
’	O
,	O
and	O
to	O
quantify	O
how	O
long	O
one	O
has	O
to	O
wait	O
to	O
obtain	O
samples	O
that	O
are	O
e	O
(	O
cid:11	O
)	O
ectively	O
independent	O
samples	O
from	O
p	O
.	O
demonstration	O
of	O
the	O
metropolis	O
method	B
the	O
metropolis	O
method	B
is	O
widely	O
used	O
for	O
high-dimensional	O
problems	O
.	O
many	O
implementations	O
of	O
the	O
metropolis	O
method	B
employ	O
a	O
proposal	O
distribution	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29.4	O
:	O
the	O
metropolis	O
{	O
hastings	O
method	B
367	O
(	O
cid:15	O
)	O
x	O
(	O
1	O
)	O
q	O
(	O
x	O
;	O
x	O
(	O
1	O
)	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
l	O
figure	O
29.11.	O
metropolis	O
method	B
in	O
two	O
dimensions	B
,	O
showing	O
a	O
traditional	O
proposal	B
density	I
that	O
has	O
a	O
su	O
(	O
cid:14	O
)	O
ciently	O
small	O
step	O
size	O
(	O
cid:15	O
)	O
that	O
the	O
acceptance	O
frequency	O
will	O
be	O
about	O
0.5.	O
with	O
a	O
length	B
scale	O
(	O
cid:15	O
)	O
that	O
is	O
short	O
relative	B
to	O
the	O
longest	O
length	B
scale	O
l	O
of	O
the	O
probable	O
region	O
(	O
(	O
cid:12	O
)	O
gure	O
29.11	O
)	O
.	O
a	O
reason	O
for	O
choosing	O
a	O
small	O
length	B
scale	O
is	O
that	O
for	O
most	O
high-dimensional	O
problems	O
,	O
a	O
large	O
random	B
step	O
from	O
a	O
typical	B
point	O
(	O
that	O
is	O
,	O
a	O
sample	B
from	I
p	O
(	O
x	O
)	O
)	O
is	O
very	O
likely	O
to	O
end	O
in	O
a	O
state	O
that	O
has	O
very	O
low	O
probability	B
;	O
such	O
steps	O
are	O
unlikely	O
to	O
be	O
accepted	O
.	O
if	O
(	O
cid:15	O
)	O
is	O
large	O
,	O
movement	O
around	O
the	O
state	O
space	O
will	O
only	O
occur	O
when	O
such	O
a	O
transition	B
to	O
a	O
low-probability	O
state	O
is	O
actually	O
accepted	O
,	O
or	O
when	O
a	O
large	O
random	B
step	O
chances	O
to	O
land	O
in	O
another	O
probable	O
state	O
.	O
so	O
the	O
rate	B
of	O
progress	O
will	O
be	O
slow	O
if	O
large	O
steps	O
are	O
used	O
.	O
the	O
disadvantage	O
of	O
small	O
steps	O
,	O
on	O
the	O
other	O
hand	O
,	O
is	O
that	O
the	O
metropolis	O
method	B
will	O
explore	B
the	O
probability	B
distribution	O
by	O
a	O
random	B
walk	I
,	O
and	O
a	O
random	B
walk	I
takes	O
a	O
long	O
time	O
to	O
get	O
anywhere	O
,	O
especially	O
if	O
the	O
walk	O
is	O
made	O
of	O
small	O
steps	O
.	O
exercise	O
29.3	O
.	O
[	O
1	O
]	O
consider	O
a	O
one-dimensional	O
random	B
walk	I
,	O
on	O
each	O
step	O
of	O
which	O
the	O
state	O
moves	O
randomly	O
to	O
the	O
left	O
or	O
to	O
the	O
right	O
with	O
equal	O
probability	B
.	O
show	O
that	O
after	O
t	O
steps	O
of	O
size	O
(	O
cid:15	O
)	O
,	O
the	O
state	O
is	O
likely	O
to	O
have	O
moved	O
only	O
a	O
distance	B
about	O
pt	O
(	O
cid:15	O
)	O
.	O
(	O
compute	O
the	O
root	O
mean	B
square	O
distance	B
travelled	O
.	O
)	O
recall	O
that	O
the	O
(	O
cid:12	O
)	O
rst	O
aim	O
of	O
monte	O
carlo	O
sampling	O
is	O
to	O
generate	O
a	O
number	O
of	O
independent	O
samples	O
from	O
the	O
given	O
distribution	B
(	O
a	O
dozen	O
,	O
say	O
)	O
.	O
if	O
the	O
largest	O
length	B
scale	O
of	O
the	O
state	O
space	O
is	O
l	O
,	O
then	O
we	O
have	O
to	O
simulate	O
a	O
random-walk	O
metropolis	O
method	B
for	O
a	O
time	O
t	O
’	O
(	O
l=	O
(	O
cid:15	O
)	O
)	O
2	O
before	O
we	O
can	O
expect	O
to	O
get	O
a	O
sample	B
that	O
is	O
roughly	O
independent	O
of	O
the	O
initial	O
condition	O
{	O
and	O
that	O
’	O
s	O
assuming	O
that	O
every	O
step	O
is	O
accepted	O
:	O
if	O
only	O
a	O
fraction	O
f	O
of	O
the	O
steps	O
are	O
accepted	O
on	O
average	O
,	O
then	O
this	O
time	O
is	O
increased	O
by	O
a	O
factor	O
1=f	O
.	O
rule	B
of	I
thumb	I
:	O
lower	O
bound	B
on	O
number	O
of	O
iterations	O
of	O
a	O
metropolis	O
method	B
.	O
if	O
the	O
largest	O
length	B
scale	O
of	O
the	O
space	O
of	O
probable	O
states	O
is	O
l	O
,	O
a	O
metropolis	O
method	B
whose	O
proposal	O
distribu-	O
tion	O
generates	O
a	O
random	B
walk	I
with	O
step	O
size	O
(	O
cid:15	O
)	O
must	O
be	O
run	O
for	O
at	O
least	O
t	O
’	O
(	O
l=	O
(	O
cid:15	O
)	O
)	O
2	O
(	O
29.32	O
)	O
iterations	O
to	O
obtain	O
an	O
independent	O
sample	O
.	O
this	O
rule	B
of	I
thumb	I
gives	O
only	O
a	O
lower	O
bound	B
;	O
the	O
situation	O
may	O
be	O
much	O
worse	O
,	O
if	O
,	O
for	O
example	O
,	O
the	O
probability	B
distribution	O
consists	O
of	O
several	O
islands	O
of	O
high	O
probability	B
separated	O
by	O
regions	O
of	O
low	O
probability	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
368	O
29	O
|	O
monte	O
carlo	O
methods	B
figure	O
29.12.	O
metropolis	O
method	B
for	O
a	O
toy	O
problem	O
.	O
(	O
a	O
)	O
the	O
state	O
sequence	O
for	O
t	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
600.	O
horizontal	O
direction	O
=	O
states	O
from	O
0	O
to	O
20	O
;	O
vertical	O
direction	O
=	O
time	O
from	O
1	O
to	O
600	O
;	O
the	O
cross	O
bars	O
mark	O
time	O
intervals	B
of	O
duration	O
50	O
.	O
(	O
b	O
)	O
histogram	O
of	O
occupancy	O
of	O
the	O
states	O
after	O
100	O
,	O
400	O
,	O
and	O
1200	O
iterations	O
.	O
(	O
c	O
)	O
for	O
comparison	O
,	O
histograms	O
resulting	O
when	O
successive	O
points	O
are	O
drawn	O
independently	O
from	O
the	O
target	O
distribution	B
.	O
(	O
a	O
)	O
(	O
b	O
)	O
metropolis	O
(	O
c	O
)	O
independent	O
sampling	O
100	O
iterations	O
100	O
iterations	O
12	O
10	O
8	O
6	O
4	O
2	O
0	O
40	O
35	O
30	O
25	O
20	O
15	O
10	O
5	O
0	O
90	O
80	O
70	O
60	O
50	O
40	O
30	O
20	O
10	O
0	O
0	O
5	O
10	O
15	O
20	O
400	O
iterations	O
0	O
5	O
10	O
15	O
20	O
1200	O
iterations	O
0	O
5	O
10	O
15	O
20	O
12	O
10	O
8	O
6	O
4	O
2	O
0	O
40	O
35	O
30	O
25	O
20	O
15	O
10	O
5	O
0	O
90	O
80	O
70	O
60	O
50	O
40	O
30	O
20	O
10	O
0	O
0	O
5	O
10	O
15	O
20	O
400	O
iterations	O
0	O
5	O
10	O
15	O
20	O
1200	O
iterations	O
0	O
5	O
10	O
15	O
20	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29.4	O
:	O
the	O
metropolis	O
{	O
hastings	O
method	B
369	O
to	O
illustrate	O
how	O
slowly	O
a	O
random	B
walk	I
explores	O
a	O
state	O
space	O
,	O
(	O
cid:12	O
)	O
gure	O
29.12	O
shows	O
a	O
simulation	O
of	O
a	O
metropolis	O
algorithm	B
for	O
generating	O
samples	O
from	O
the	O
distribution	B
:	O
(	O
29.33	O
)	O
p	O
(	O
x	O
)	O
=	O
(	O
cid:26	O
)	O
1/21	O
x	O
2	O
f0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
20g	O
otherwise	O
.	O
0	O
the	O
proposal	O
distribution	O
is	O
0	O
(	O
29.34	O
)	O
otherwise	O
.	O
q	O
(	O
x0	O
;	O
x	O
)	O
=	O
(	O
cid:26	O
)	O
1/2	O
x0	O
=	O
x	O
(	O
cid:6	O
)	O
1	O
because	O
the	O
target	O
distribution	B
p	O
(	O
x	O
)	O
is	O
uniform	O
,	O
rejections	O
occur	O
only	O
when	O
the	O
proposal	O
takes	O
the	O
state	O
to	O
x0	O
=	O
(	O
cid:0	O
)	O
1	O
or	O
x0	O
=	O
21.	O
the	O
simulation	O
was	O
started	O
in	O
the	O
state	O
x0	O
=	O
10	O
and	O
its	O
evolution	B
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
29.12a	O
.	O
how	O
long	O
does	O
it	O
take	O
to	O
reach	O
one	O
of	O
the	O
end	O
states	O
x	O
=	O
0	O
and	O
x	O
=	O
20	O
?	O
since	O
the	O
distance	B
is	O
10	O
steps	O
,	O
the	O
rule	B
of	I
thumb	I
(	O
29.32	O
)	O
predicts	O
that	O
it	O
will	O
typically	O
take	O
a	O
time	O
t	O
’	O
100	O
iterations	O
to	O
reach	O
an	O
end	O
state	O
.	O
this	O
is	O
con	O
(	O
cid:12	O
)	O
rmed	O
in	O
the	O
present	O
example	O
:	O
the	O
(	O
cid:12	O
)	O
rst	O
step	O
into	O
an	O
end	O
state	O
occurs	O
on	O
the	O
178th	O
iteration	O
.	O
how	O
long	O
does	O
it	O
take	O
to	O
visit	O
both	O
end	O
states	O
?	O
the	O
rule	B
of	I
thumb	I
predicts	O
about	O
400	O
iterations	O
are	O
required	O
to	O
traverse	O
the	O
whole	O
state	O
space	O
;	O
and	O
indeed	O
the	O
(	O
cid:12	O
)	O
rst	O
encounter	O
with	O
the	O
other	O
end	O
state	O
takes	O
place	O
on	O
the	O
540th	O
iteration	O
.	O
thus	O
e	O
(	O
cid:11	O
)	O
ectively-independent	O
samples	O
are	O
generated	O
only	O
by	O
simulating	O
for	O
about	O
four	O
hundred	O
iterations	O
per	O
independent	O
sample	O
.	O
this	O
simple	O
example	O
shows	O
that	O
it	O
is	O
important	O
to	O
try	O
to	O
abolish	O
random	B
walk	I
behaviour	O
in	O
monte	O
carlo	O
methods	B
.	O
a	O
systematic	B
exploration	O
of	O
the	O
toy	O
state	O
space	O
f0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
20g	O
could	O
get	O
around	O
it	O
,	O
using	O
the	O
same	O
step	O
sizes	O
,	O
in	O
about	O
twenty	O
steps	O
instead	O
of	O
four	O
hundred	O
.	O
methods	B
for	O
reducing	O
random	B
walk	I
behaviour	O
are	O
discussed	O
in	O
the	O
next	O
chapter	O
.	O
metropolis	O
method	B
in	O
high	B
dimensions	I
the	O
rule	B
of	I
thumb	I
(	O
29.32	O
)	O
,	O
which	O
gives	O
a	O
lower	O
bound	B
on	O
the	O
number	O
of	O
itera-	O
tions	O
of	O
a	O
random	B
walk	I
metropolis	O
method	B
,	O
also	O
applies	O
to	O
higher-dimensional	O
problems	O
.	O
consider	O
the	O
simple	O
case	O
of	O
a	O
target	O
distribution	B
that	O
is	O
an	O
n	O
-	O
dimensional	O
gaussian	O
,	O
and	O
a	O
proposal	O
distribution	O
that	O
is	O
a	O
spherical	O
gaussian	O
of	O
standard	O
deviation	O
(	O
cid:15	O
)	O
in	O
each	O
direction	O
.	O
without	O
loss	O
of	O
generality	O
,	O
we	O
can	O
assume	O
that	O
the	O
target	O
distribution	B
is	O
a	O
separable	O
distribution	B
aligned	O
with	O
the	O
axes	O
fxng	O
,	O
and	O
that	O
it	O
has	O
standard	B
deviation	I
(	O
cid:27	O
)	O
n	O
in	O
direction	O
n.	O
let	O
(	O
cid:27	O
)	O
max	O
and	O
(	O
cid:27	O
)	O
min	O
be	O
the	O
largest	O
and	O
smallest	O
of	O
these	O
standard	O
deviations	O
.	O
let	O
us	O
assume	O
that	O
(	O
cid:15	O
)	O
is	O
adjusted	O
such	O
that	O
the	O
acceptance	O
frequency	O
is	O
close	O
to	O
1.	O
under	O
this	O
assumption	O
,	O
each	O
variable	O
xn	O
evolves	O
independently	O
of	O
all	O
the	O
others	O
,	O
executing	O
a	O
random	B
walk	I
with	O
step	O
size	O
about	O
(	O
cid:15	O
)	O
.	O
the	O
time	O
taken	O
to	O
generate	O
e	O
(	O
cid:11	O
)	O
ectively	O
independent	O
samples	O
from	O
the	O
target	O
distribution	B
will	O
be	O
controlled	O
by	O
the	O
largest	O
lengthscale	O
(	O
cid:27	O
)	O
max	O
.	O
just	O
as	O
in	O
the	O
previous	O
section	B
,	O
where	O
we	O
needed	O
at	O
least	O
t	O
’	O
(	O
l=	O
(	O
cid:15	O
)	O
)	O
2	O
iterations	O
to	O
obtain	O
an	O
independent	O
sample	O
,	O
here	O
we	O
need	O
t	O
’	O
(	O
(	O
cid:27	O
)	O
max=	O
(	O
cid:15	O
)	O
)	O
2.	O
now	O
,	O
how	O
big	O
can	O
(	O
cid:15	O
)	O
be	O
?	O
the	O
bigger	O
it	O
is	O
,	O
the	O
smaller	O
this	O
number	O
t	O
be-	O
comes	O
,	O
but	O
if	O
(	O
cid:15	O
)	O
is	O
too	O
big	O
{	O
bigger	O
than	O
(	O
cid:27	O
)	O
min	O
{	O
then	O
the	O
acceptance	B
rate	I
will	O
fall	O
sharply	O
.	O
it	O
seems	O
plausible	O
that	O
the	O
optimal	B
(	O
cid:15	O
)	O
must	O
be	O
similar	O
to	O
(	O
cid:27	O
)	O
min	O
.	O
strictly	O
,	O
this	O
may	O
not	O
be	O
true	O
;	O
in	O
special	O
cases	O
where	O
the	O
second	O
smallest	O
(	O
cid:27	O
)	O
n	O
is	O
signi	O
(	O
cid:12	O
)	O
cantly	O
greater	O
than	O
(	O
cid:27	O
)	O
min	O
,	O
the	O
optimal	B
(	O
cid:15	O
)	O
may	O
be	O
closer	O
to	O
that	O
second	O
smallest	O
(	O
cid:27	O
)	O
n.	O
but	O
our	O
rough	O
conclusion	O
is	O
this	O
:	O
where	O
simple	O
spherical	O
pro-	O
posal	O
distributions	O
are	O
used	O
,	O
we	O
will	O
need	O
at	O
least	O
t	O
’	O
(	O
(	O
cid:27	O
)	O
max=	O
(	O
cid:27	O
)	O
min	O
)	O
2	O
iterations	O
to	O
obtain	O
an	O
independent	O
sample	O
,	O
where	O
(	O
cid:27	O
)	O
max	O
and	O
(	O
cid:27	O
)	O
min	O
are	O
the	O
longest	O
and	O
shortest	O
lengthscales	O
of	O
the	O
target	O
distribution	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29	O
|	O
monte	O
carlo	O
methods	B
figure	O
29.13.	O
gibbs	O
sampling	O
.	O
(	O
a	O
)	O
the	O
joint	B
density	O
p	O
(	O
x	O
)	O
from	O
which	O
samples	O
are	O
required	O
.	O
(	O
b	O
)	O
starting	O
from	O
a	O
state	O
x	O
(	O
t	O
)	O
,	O
x1	O
is	O
sampled	O
from	O
the	O
conditional	B
density	O
p	O
(	O
x1	O
j	O
x	O
(	O
t	O
)	O
2	O
)	O
.	O
(	O
c	O
)	O
a	O
sample	B
is	O
then	O
made	O
from	O
the	O
conditional	B
density	O
p	O
(	O
x2	O
j	O
x1	O
)	O
.	O
(	O
d	O
)	O
a	O
couple	O
of	O
iterations	O
of	O
gibbs	O
sampling	O
.	O
370	O
x2	O
(	O
a	O
)	O
x2	O
(	O
c	O
)	O
p	O
(	O
x	O
)	O
x2	O
(	O
b	O
)	O
x1	O
x2	O
p	O
(	O
x1	O
j	O
x	O
(	O
t	O
)	O
2	O
)	O
x	O
(	O
t	O
)	O
x1	O
x	O
(	O
t+2	O
)	O
x	O
(	O
t+1	O
)	O
p	O
(	O
x2	O
j	O
x1	O
)	O
(	O
d	O
)	O
x1	O
x	O
(	O
t	O
)	O
x1	O
this	O
is	O
good	B
news	O
and	O
bad	O
news	O
.	O
it	O
is	O
good	B
news	O
because	O
,	O
unlike	O
the	O
cases	O
of	O
rejection	O
sampling	O
and	O
importance	B
sampling	I
,	O
there	O
is	O
no	O
catastrophic	O
dependence	O
on	O
the	O
dimensionality	O
n	O
.	O
our	O
computer	B
will	O
give	O
useful	O
answers	O
in	O
a	O
time	O
shorter	O
than	O
the	O
age	O
of	O
the	O
universe	O
.	O
but	O
it	O
is	O
bad	B
news	O
all	O
the	O
same	O
,	O
because	O
this	O
quadratic	O
dependence	O
on	O
the	O
lengthscale-ratio	O
may	O
still	O
force	O
us	O
to	O
make	O
very	O
lengthy	O
simulations	O
.	O
fortunately	O
,	O
there	O
are	O
methods	B
for	O
suppressing	O
random	B
walks	O
in	O
monte	O
carlo	O
simulations	O
,	O
which	O
we	O
will	O
discuss	O
in	O
the	O
next	O
chapter	O
.	O
29.5	O
gibbs	O
sampling	O
we	O
introduced	O
importance	B
sampling	I
,	O
rejection	B
sampling	I
and	O
the	O
metropolis	O
method	B
using	O
one-dimensional	O
examples	O
.	O
gibbs	O
sampling	O
,	O
also	O
known	O
as	O
the	O
heat	B
bath	I
method	O
or	O
‘	O
glauber	O
dynamics	O
’	O
,	O
is	O
a	O
method	B
for	O
sampling	O
from	O
dis-	O
tributions	O
over	O
at	O
least	O
two	O
dimensions	B
.	O
gibbs	O
sampling	O
can	O
be	O
viewed	O
as	O
a	O
metropolis	O
method	B
in	O
which	O
a	O
sequence	B
of	O
proposal	O
distributions	O
q	O
are	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
terms	O
of	O
the	O
conditional	O
distributions	O
of	O
the	O
joint	O
distribution	B
p	O
(	O
x	O
)	O
.	O
it	O
is	O
assumed	O
that	O
,	O
whilst	O
p	O
(	O
x	O
)	O
is	O
too	O
complex	B
to	O
draw	O
samples	O
from	O
directly	O
,	O
its	O
conditional	B
distributions	O
p	O
(	O
xi	O
jfxjgj6=i	O
)	O
are	O
tractable	O
to	O
work	O
with	O
.	O
for	O
many	O
graphical	O
models	O
(	O
but	O
not	O
all	O
)	O
these	O
one-dimensional	O
conditional	B
distributions	O
are	O
straightforward	O
to	O
sample	B
from	I
.	O
for	O
example	O
,	O
if	O
a	O
gaussian	O
distribution	B
for	O
some	O
variables	O
d	O
has	O
an	O
unknown	O
mean	O
m	O
,	O
and	O
the	O
prior	B
distribution	O
of	O
m	O
is	O
gaussian	O
,	O
then	O
the	O
conditional	B
distribution	O
of	O
m	O
given	O
d	O
is	O
also	O
gaussian	O
.	O
conditional	B
distributions	O
that	O
are	O
not	O
of	O
standard	O
form	O
may	O
still	O
be	O
sampled	O
from	O
by	O
adaptive	B
rejection	I
sampling	I
if	O
the	O
conditional	B
distribution	O
satis	O
(	O
cid:12	O
)	O
es	O
certain	O
convexity	B
properties	O
(	O
gilks	O
and	O
wild	O
,	O
1992	O
)	O
.	O
gibbs	O
sampling	O
is	O
illustrated	O
for	O
a	O
case	O
with	O
two	O
variables	O
(	O
x1	O
;	O
x2	O
)	O
=	O
x	O
in	O
(	O
cid:12	O
)	O
gure	O
29.13.	O
on	O
each	O
iteration	O
,	O
we	O
start	O
from	O
the	O
current	O
state	O
x	O
(	O
t	O
)	O
,	O
and	O
x1	O
is	O
sampled	O
from	O
the	O
conditional	B
density	O
p	O
(	O
x1	O
j	O
x2	O
)	O
,	O
with	O
x2	O
(	O
cid:12	O
)	O
xed	O
to	O
x	O
(	O
t	O
)	O
2	O
.	O
a	O
sample	B
x2	O
is	O
then	O
made	O
from	O
the	O
conditional	B
density	O
p	O
(	O
x2	O
j	O
x1	O
)	O
,	O
using	O
the	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29.5	O
:	O
gibbs	O
sampling	O
371	O
new	O
value	O
of	O
x1	O
.	O
this	O
brings	O
us	O
to	O
the	O
new	O
state	O
x	O
(	O
t+1	O
)	O
,	O
and	O
completes	O
the	O
iteration	O
.	O
in	O
the	O
general	O
case	O
of	O
a	O
system	O
with	O
k	O
variables	O
,	O
a	O
single	O
iteration	O
involves	O
sampling	O
one	O
parameter	O
at	O
a	O
time	O
:	O
x	O
(	O
t+1	O
)	O
1	O
x	O
(	O
t+1	O
)	O
2	O
x	O
(	O
t+1	O
)	O
3	O
(	O
cid:24	O
)	O
p	O
(	O
x1	O
j	O
x	O
(	O
t	O
)	O
2	O
;	O
x	O
(	O
t	O
)	O
(	O
cid:24	O
)	O
p	O
(	O
x2	O
j	O
x	O
(	O
t+1	O
)	O
(	O
cid:24	O
)	O
p	O
(	O
x3	O
j	O
x	O
(	O
t+1	O
)	O
1	O
1	O
3	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
t	O
)	O
k	O
)	O
;	O
x	O
(	O
t	O
)	O
3	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
t	O
)	O
k	O
)	O
;	O
x	O
(	O
t+1	O
)	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
t	O
)	O
2	O
k	O
)	O
;	O
etc	O
.	O
(	O
29.35	O
)	O
(	O
29.36	O
)	O
(	O
29.37	O
)	O
convergence	O
of	O
gibbs	O
sampling	O
to	O
the	O
target	O
density	B
.	O
exercise	O
29.4	O
.	O
[	O
2	O
]	O
show	O
that	O
a	O
single	O
variable-update	O
of	O
gibbs	O
sampling	O
can	O
be	O
viewed	O
as	O
a	O
metropolis	O
method	B
with	O
target	O
density	B
p	O
(	O
x	O
)	O
,	O
and	O
that	O
this	O
metropolis	O
method	B
has	O
the	O
property	O
that	O
every	O
proposal	O
is	O
always	O
accepted	O
.	O
because	O
gibbs	O
sampling	O
is	O
a	O
metropolis	O
method	B
,	O
the	O
probability	B
distribution	O
of	O
x	O
(	O
t	O
)	O
tends	O
to	O
p	O
(	O
x	O
)	O
as	O
t	O
!	O
1	O
,	O
as	O
long	O
as	O
p	O
(	O
x	O
)	O
does	O
not	O
have	O
pathological	O
properties	O
.	O
.	O
exercise	O
29.5	O
.	O
[	O
2	O
,	O
p.385	O
]	O
discuss	O
whether	O
the	O
syndrome	B
decoding	I
problem	O
for	O
a	O
(	O
7	O
;	O
4	O
)	O
hamming	O
code	B
can	O
be	O
solved	O
using	O
gibbs	O
sampling	O
.	O
the	O
syndrome	B
decoding	I
problem	O
,	O
if	O
we	O
are	O
to	O
solve	O
it	O
with	O
a	O
monte	O
carlo	O
approach	O
,	O
is	O
to	O
draw	O
samples	O
from	O
the	O
posterior	O
distribution	O
of	O
the	O
noise	O
vector	O
n	O
=	O
(	O
n1	O
;	O
:	O
:	O
:	O
;	O
nn	O
;	O
:	O
:	O
:	O
;	O
nn	O
)	O
,	O
p	O
(	O
nj	O
f	O
;	O
z	O
)	O
=	O
1	O
z	O
n	O
yn=1	O
f	O
nn	O
n	O
(	O
1	O
(	O
cid:0	O
)	O
fn	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
nn	O
)	O
	O
[	O
hn	O
=	O
z	O
]	O
;	O
(	O
29.38	O
)	O
where	O
fn	O
is	O
the	O
normalized	O
likelihood	B
for	O
the	O
nth	O
transmitted	O
bit	B
and	O
z	O
is	O
the	O
observed	O
syndrome	B
.	O
the	O
factor	O
	O
[	O
hn	O
=	O
z	O
]	O
is	O
1	O
if	O
n	O
has	O
the	O
correct	O
syndrome	B
z	O
and	O
0	O
otherwise	O
.	O
what	O
about	O
the	O
syndrome	B
decoding	I
problem	O
for	O
any	O
linear	B
error-correcting	O
code	B
?	O
gibbs	O
sampling	O
in	O
high	B
dimensions	I
gibbs	O
sampling	O
su	O
(	O
cid:11	O
)	O
ers	O
from	O
the	O
same	O
defect	O
as	O
simple	O
metropolis	O
algorithms	B
{	O
the	O
state	O
space	O
is	O
explored	O
by	O
a	O
slow	O
random	B
walk	I
,	O
unless	O
a	O
fortuitous	O
pa-	O
rameterization	O
has	O
been	O
chosen	O
that	O
makes	O
the	O
probability	B
distribution	O
p	O
(	O
x	O
)	O
separable	O
.	O
if	O
,	O
say	O
,	O
two	O
variables	O
x1	O
and	O
x2	O
are	O
strongly	O
correlated	O
,	O
having	O
marginal	B
densities	O
of	O
width	O
l	O
and	O
conditional	O
densities	O
of	O
width	O
(	O
cid:15	O
)	O
,	O
then	O
it	O
will	O
take	O
at	O
least	O
about	O
(	O
l=	O
(	O
cid:15	O
)	O
)	O
2	O
iterations	O
to	O
generate	O
an	O
independent	O
sample	O
from	O
the	O
target	O
density	B
.	O
figure	O
30.3	O
,	O
p.390	O
,	O
illustrates	O
the	O
slow	O
progress	O
made	O
by	O
gibbs	O
sampling	O
when	O
l	O
(	O
cid:29	O
)	O
(	O
cid:15	O
)	O
.	O
however	O
gibbs	O
sampling	O
involves	O
no	O
adjustable	O
parameters	B
,	O
so	O
it	O
is	O
an	O
at-	O
tractive	O
strategy	O
when	O
one	O
wants	O
to	O
get	O
a	O
model	B
running	O
quickly	O
.	O
an	O
excellent	O
software	B
package	O
,	O
bugs	O
,	O
makes	O
it	O
easy	O
to	O
set	B
up	O
almost	O
arbitrary	O
probabilistic	O
models	O
and	O
simulate	O
them	O
by	O
gibbs	O
sampling	O
(	O
thomas	O
et	O
al.	O
,	O
1992	O
)	O
.1	O
1http	O
:	O
//www.mrc-bsu.cam.ac.uk/bugs/	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
372	O
29	O
|	O
monte	O
carlo	O
methods	B
29.6	O
terminology	B
for	O
markov	O
chain	B
monte	O
carlo	O
methods	B
we	O
now	O
spend	O
a	O
few	O
moments	O
sketching	O
the	O
theory	B
on	O
which	O
the	O
metropolis	O
method	B
and	O
gibbs	O
sampling	O
are	O
based	O
.	O
we	O
denote	O
by	O
p	O
(	O
t	O
)	O
(	O
x	O
)	O
the	O
probabil-	O
ity	O
distribution	B
of	O
the	O
state	O
of	O
a	O
markov	O
chain	B
simulator	O
.	O
(	O
to	O
visualize	O
this	O
distribution	B
,	O
imagine	O
running	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
collection	O
of	O
identical	O
simulators	O
in	O
parallel	O
.	O
)	O
our	O
aim	O
is	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
markov	O
chain	B
such	O
that	O
as	O
t	O
!	O
1	O
,	O
p	O
(	O
t	O
)	O
(	O
x	O
)	O
tends	O
to	O
the	O
desired	O
distribution	B
p	O
(	O
x	O
)	O
.	O
a	O
markov	O
chain	B
can	O
be	O
speci	O
(	O
cid:12	O
)	O
ed	O
by	O
an	O
initial	O
probability	B
distribution	O
p	O
(	O
0	O
)	O
(	O
x	O
)	O
and	O
a	O
transition	B
probability	I
t	O
(	O
x0	O
;	O
x	O
)	O
.	O
the	O
probability	B
distribution	O
of	O
the	O
state	O
at	O
the	O
(	O
t	O
+	O
1	O
)	O
th	O
iteration	O
of	O
the	O
markov	O
chain	B
,	O
p	O
(	O
t+1	O
)	O
(	O
x	O
)	O
,	O
is	O
given	O
by	O
p	O
(	O
t+1	O
)	O
(	O
x0	O
)	O
=z	O
dn	O
x	O
t	O
(	O
x0	O
;	O
x	O
)	O
p	O
(	O
t	O
)	O
(	O
x	O
)	O
:	O
(	O
29.39	O
)	O
example	O
29.6.	O
an	O
example	O
of	O
a	O
markov	O
chain	B
is	O
given	O
by	O
the	O
metropolis	O
demonstration	O
of	O
section	O
29.4	O
(	O
(	O
cid:12	O
)	O
gure	O
29.12	O
)	O
,	O
for	O
which	O
the	O
transition	B
proba-	O
bility	O
is	O
t	O
=	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
and	O
the	O
initial	O
distribution	B
was	O
1/2	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
1/2	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
p	O
(	O
0	O
)	O
(	O
x	O
)	O
=	O
(	O
cid:2	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:3	O
)	O
:	O
the	O
probability	B
distribution	O
p	O
(	O
t	O
)	O
(	O
x	O
)	O
of	O
the	O
state	O
at	O
the	O
tth	O
iteration	O
is	O
shown	O
for	O
t	O
=	O
0	O
;	O
1	O
,	O
2	O
,	O
3	O
,	O
5	O
,	O
10	O
,	O
100	O
,	O
200	O
,	O
400	O
in	O
(	O
cid:12	O
)	O
gure	O
29.14	O
;	O
an	O
equivalent	O
sequence	B
of	O
distributions	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
29.15	O
for	O
the	O
chain	B
that	O
begins	O
in	O
initial	O
state	O
x0	O
=	O
17.	O
both	O
chains	O
converge	O
to	O
the	O
target	O
density	B
,	O
the	O
uniform	O
density	B
,	O
as	O
t	O
!	O
1.	O
required	O
properties	O
(	O
29.40	O
)	O
when	O
designing	O
a	O
markov	O
chain	B
monte	O
carlo	O
method	B
,	O
we	O
construct	O
a	O
chain	B
with	O
the	O
following	O
properties	O
:	O
1.	O
the	O
desired	O
distribution	B
p	O
(	O
x	O
)	O
is	O
an	O
invariant	B
distribution	I
of	O
the	O
chain	B
.	O
p	O
(	O
0	O
)	O
(	O
x	O
)	O
p	O
(	O
1	O
)	O
(	O
x	O
)	O
0	O
0	O
p	O
(	O
2	O
)	O
(	O
x	O
)	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
0	O
5	O
10	O
15	O
20	O
p	O
(	O
3	O
)	O
(	O
x	O
)	O
p	O
(	O
10	O
)	O
(	O
x	O
)	O
p	O
(	O
100	O
)	O
(	O
x	O
)	O
p	O
(	O
200	O
)	O
(	O
x	O
)	O
p	O
(	O
400	O
)	O
(	O
x	O
)	O
0	O
0	O
0	O
0	O
0	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
figure	O
29.14.	O
the	O
probability	B
distribution	O
of	O
the	O
state	O
of	O
the	O
markov	O
chain	B
of	O
example	O
29.6.	O
a	O
distribution	B
(	O
cid:25	O
)	O
(	O
x	O
)	O
is	O
an	O
invariant	B
distribution	I
of	O
the	O
transition	B
proba-	O
bility	O
t	O
(	O
x0	O
;	O
x	O
)	O
if	O
an	O
invariant	B
distribution	I
is	O
an	O
eigenvector	O
of	O
the	O
transition	O
probability	B
matrix	O
that	O
has	O
eigenvalue	B
1	O
.	O
(	O
cid:25	O
)	O
(	O
x0	O
)	O
=z	O
dn	O
x	O
t	O
(	O
x0	O
;	O
x	O
)	O
(	O
cid:25	O
)	O
(	O
x	O
)	O
:	O
(	O
29.41	O
)	O
 	B
 	I
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29.6	O
:	O
terminology	B
for	O
markov	O
chain	B
monte	O
carlo	O
methods	B
373	O
2.	O
the	O
chain	B
must	O
also	O
be	O
ergodic	B
,	O
that	O
is	O
,	O
p	O
(	O
t	O
)	O
(	O
x	O
)	O
!	O
(	O
cid:25	O
)	O
(	O
x	O
)	O
as	O
t	O
!	O
1	O
,	O
for	O
any	O
p	O
(	O
0	O
)	O
(	O
x	O
)	O
.	O
a	O
couple	O
of	O
reasons	O
why	O
a	O
chain	B
might	O
not	O
be	O
ergodic	B
are	O
:	O
(	O
29.42	O
)	O
p	O
(	O
0	O
)	O
(	O
x	O
)	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
0	O
0	O
p	O
(	O
1	O
)	O
(	O
x	O
)	O
p	O
(	O
2	O
)	O
(	O
x	O
)	O
0	O
5	O
10	O
15	O
20	O
p	O
(	O
3	O
)	O
(	O
x	O
)	O
p	O
(	O
10	O
)	O
(	O
x	O
)	O
p	O
(	O
100	O
)	O
(	O
x	O
)	O
p	O
(	O
200	O
)	O
(	O
x	O
)	O
p	O
(	O
400	O
)	O
(	O
x	O
)	O
0	O
0	O
0	O
0	O
0	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
5	O
10	O
15	O
20	O
figure	O
29.15.	O
the	O
probability	B
distribution	O
of	O
the	O
state	O
of	O
the	O
markov	O
chain	B
for	O
initial	O
condition	O
x0	O
=	O
17	O
(	O
example	O
29.6	O
(	O
p.372	O
)	O
)	O
.	O
(	O
a	O
)	O
its	O
matrix	B
might	O
be	O
reducible	B
,	O
which	O
means	O
that	O
the	O
state	O
space	O
contains	O
two	O
or	O
more	O
subsets	O
of	O
states	O
that	O
can	O
never	O
be	O
reached	O
from	O
each	O
other	O
.	O
such	O
a	O
chain	B
has	O
many	O
invariant	O
distributions	O
;	O
which	O
one	O
p	O
(	O
t	O
)	O
(	O
x	O
)	O
would	O
tend	O
to	O
as	O
t	O
!	O
1	O
would	O
depend	O
on	O
the	O
initial	O
condition	O
p	O
(	O
0	O
)	O
(	O
x	O
)	O
.	O
the	O
transition	B
probability	I
matrix	O
of	O
such	O
a	O
chain	B
has	O
more	O
than	O
one	O
eigenvalue	B
equal	O
to	O
1	O
.	O
(	O
b	O
)	O
the	O
chain	B
might	O
have	O
a	O
periodic	O
set	O
,	O
which	O
means	O
that	O
,	O
for	O
some	O
initial	O
conditions	O
,	O
p	O
(	O
t	O
)	O
(	O
x	O
)	O
doesn	O
’	O
t	O
tend	O
to	O
an	O
invariant	B
distribution	I
,	O
but	O
instead	O
tends	O
to	O
a	O
periodic	O
limit-cycle	O
.	O
a	O
simple	O
markov	O
chain	B
with	O
this	O
property	O
is	O
the	O
random	B
walk	I
on	O
the	O
n	O
-dimensional	O
hypercube	O
.	O
the	O
chain	B
t	O
takes	O
the	O
state	O
from	O
one	O
corner	O
to	O
a	O
randomly	O
chosen	O
adjacent	O
corner	O
.	O
the	O
unique	O
invariant	B
distribution	I
of	O
this	O
chain	B
is	O
the	O
uniform	O
distribution	B
over	O
all	O
2n	O
states	O
,	O
but	O
the	O
chain	B
is	O
not	O
ergodic	B
;	O
it	O
is	O
periodic	O
with	O
period	O
two	O
:	O
if	O
we	O
divide	O
the	O
states	O
into	O
states	O
with	O
odd	O
parity	B
and	O
states	O
with	O
even	O
parity	B
,	O
we	O
notice	O
that	O
every	O
odd	O
state	O
is	O
surrounded	O
by	O
even	O
states	O
and	O
vice	O
versa	O
.	O
so	O
if	O
the	O
initial	O
condition	O
at	O
time	O
t	O
=	O
0	O
is	O
a	O
state	O
with	O
even	O
parity	B
,	O
then	O
at	O
time	O
t	O
=	O
1	O
{	O
and	O
at	O
all	O
odd	O
times	O
{	O
the	O
state	O
must	O
have	O
odd	O
parity	B
,	O
and	O
at	O
all	O
even	O
times	O
,	O
the	O
state	O
will	O
be	O
of	O
even	O
parity	B
.	O
the	O
transition	B
probability	I
matrix	O
of	O
such	O
a	O
chain	B
has	O
more	O
than	O
one	O
eigenvalue	B
with	O
magnitude	O
equal	O
to	O
1.	O
the	O
random	B
walk	I
on	O
the	O
hypercube	O
,	O
for	O
example	O
,	O
has	O
eigenvalues	O
equal	O
to	O
+1	O
and	O
(	O
cid:0	O
)	O
1.	O
methods	B
of	O
construction	B
of	O
markov	O
chains	O
it	O
is	O
often	O
convenient	O
to	O
construct	O
t	O
by	O
mixing	O
or	O
concatenating	O
simple	O
base	O
transitions	O
b	O
all	O
of	O
which	O
satisfy	O
p	O
(	O
x0	O
)	O
=z	O
dn	O
x	O
b	O
(	O
x0	O
;	O
x	O
)	O
p	O
(	O
x	O
)	O
;	O
(	O
29.43	O
)	O
for	O
the	O
desired	O
density	B
p	O
(	O
x	O
)	O
,	O
i.e.	O
,	O
they	O
all	O
have	O
the	O
desired	O
density	B
as	O
an	O
invariant	B
distribution	I
.	O
these	O
base	B
transitions	I
need	O
not	O
individually	O
be	O
ergodic	B
.	O
t	O
is	O
a	O
mixture	O
of	O
several	O
base	B
transitions	I
bb	O
(	O
x0	O
;	O
x	O
)	O
if	O
we	O
make	O
the	O
transition	B
by	O
picking	O
one	O
of	O
the	O
base	O
transitions	O
at	O
random	B
,	O
and	O
allowing	O
it	O
to	O
determine	O
the	O
transition	B
,	O
i.e.	O
,	O
pbbb	O
(	O
x0	O
;	O
x	O
)	O
;	O
(	O
29.44	O
)	O
t	O
(	O
x0	O
;	O
x	O
)	O
=xb	O
where	O
fpbg	O
is	O
a	O
probability	B
distribution	O
over	O
the	O
base	B
transitions	I
.	O
t	O
is	O
a	O
concatenation	B
of	O
two	O
base	B
transitions	I
b1	O
(	O
x0	O
;	O
x	O
)	O
and	O
b2	O
(	O
x0	O
;	O
x	O
)	O
if	O
we	O
(	O
cid:12	O
)	O
rst	O
make	O
a	O
transition	B
to	O
an	O
intermediate	O
state	O
x00	O
using	O
b1	O
,	O
and	O
then	O
make	O
a	O
transition	B
from	O
state	O
x00	O
to	O
x0	O
using	O
b2	O
.	O
t	O
(	O
x0	O
;	O
x	O
)	O
=z	O
dn	O
x00	O
b2	O
(	O
x0	O
;	O
x00	O
)	O
b1	O
(	O
x00	O
;	O
x	O
)	O
:	O
(	O
29.45	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29	O
|	O
monte	O
carlo	O
methods	B
374	O
detailed	B
balance	I
many	O
useful	O
transition	B
probabilities	O
satisfy	O
the	O
detailed	B
balance	I
property	O
:	O
t	O
(	O
xa	O
;	O
xb	O
)	O
p	O
(	O
xb	O
)	O
=	O
t	O
(	O
xb	O
;	O
xa	O
)	O
p	O
(	O
xa	O
)	O
;	O
for	O
all	O
xb	O
and	O
xa	O
:	O
(	O
29.46	O
)	O
this	O
equation	O
says	O
that	O
if	O
we	O
pick	O
(	O
by	O
magic	O
)	O
a	O
state	O
from	O
the	O
target	O
density	B
p	O
and	O
make	O
a	O
transition	B
under	O
t	O
to	O
another	O
state	O
,	O
it	O
is	O
just	O
as	O
likely	O
that	O
we	O
will	O
pick	O
xb	O
and	O
go	O
from	O
xb	O
to	O
xa	O
as	O
it	O
is	O
that	O
we	O
will	O
pick	O
xa	O
and	O
go	O
from	O
xa	O
to	O
xb	O
.	O
markov	O
chains	O
that	O
satisfy	O
detailed	B
balance	I
are	O
also	O
called	O
reversible	B
markov	O
chains	O
.	O
the	O
reason	O
why	O
the	O
detailed-balance	O
property	O
is	O
of	O
interest	O
is	O
that	O
detailed	B
balance	I
implies	O
invariance	B
of	O
the	O
distribution	B
p	O
(	O
x	O
)	O
under	O
the	O
markov	O
chain	B
t	O
,	O
which	O
is	O
a	O
necessary	O
condition	O
for	O
the	O
key	O
property	O
that	O
we	O
want	O
from	O
our	O
mcmc	O
simulation	O
{	O
that	O
the	O
probability	B
distribution	O
of	O
the	O
chain	O
should	O
converge	O
to	O
p	O
(	O
x	O
)	O
.	O
.	O
exercise	O
29.7	O
.	O
[	O
2	O
]	O
prove	O
that	O
detailed	B
balance	I
implies	O
invariance	B
of	O
the	O
distri-	O
bution	O
p	O
(	O
x	O
)	O
under	O
the	O
markov	O
chain	B
t	O
.	O
proving	O
that	O
detailed	B
balance	I
holds	O
is	O
often	O
a	O
key	O
step	O
when	O
proving	O
that	O
a	O
markov	O
chain	B
monte	O
carlo	O
simulation	O
will	O
converge	O
to	O
the	O
desired	O
distribu-	O
tion	O
.	O
the	O
metropolis	O
method	B
satis	O
(	O
cid:12	O
)	O
es	O
detailed	B
balance	I
,	O
for	O
example	O
.	O
detailed	B
balance	I
is	O
not	O
an	O
essential	O
condition	O
,	O
however	O
,	O
and	O
we	O
will	O
see	O
later	O
that	O
ir-	O
reversible	B
markov	O
chains	O
can	O
be	O
useful	O
in	O
practice	O
,	O
because	O
they	O
may	O
have	O
di	O
(	O
cid:11	O
)	O
erent	O
random	B
walk	I
properties	O
.	O
.	O
exercise	O
29.8	O
.	O
[	O
2	O
]	O
show	O
that	O
,	O
if	O
we	O
concatenate	O
two	O
base	B
transitions	I
b1	O
and	O
b2	O
that	O
satisfy	O
detailed	B
balance	I
,	O
it	O
is	O
not	O
necessarily	O
the	O
case	O
that	O
the	O
t	O
thus	O
de	O
(	O
cid:12	O
)	O
ned	O
(	O
29.45	O
)	O
satis	O
(	O
cid:12	O
)	O
es	O
detailed	B
balance	I
.	O
exercise	O
29.9	O
.	O
[	O
2	O
]	O
does	O
gibbs	O
sampling	O
,	O
with	O
several	O
variables	O
all	O
updated	O
in	O
a	O
deterministic	B
sequence	O
,	O
satisfy	O
detailed	B
balance	I
?	O
29.7	O
slice	B
sampling	I
slice	O
sampling	O
(	O
neal	O
,	O
1997a	O
;	O
neal	O
,	O
2003	O
)	O
is	O
a	O
markov	O
chain	B
monte	O
carlo	O
method	B
that	O
has	O
similarities	O
to	O
rejection	B
sampling	I
,	O
gibbs	O
sampling	O
and	O
the	O
metropolis	O
method	B
.	O
it	O
can	O
be	O
applied	O
wherever	O
the	O
metropolis	O
method	B
can	O
be	O
applied	O
,	O
that	O
is	O
,	O
to	O
any	O
system	O
for	O
which	O
the	O
target	O
density	B
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
can	O
be	O
evaluated	O
at	O
any	O
point	O
x	O
;	O
it	O
has	O
the	O
advantage	O
over	O
simple	O
metropolis	O
methods	B
that	O
it	O
is	O
more	O
robust	O
to	O
the	O
choice	O
of	O
parameters	O
like	O
step	O
sizes	O
.	O
the	O
sim-	O
plest	O
version	O
of	O
slice	O
sampling	O
is	O
similar	O
to	O
gibbs	O
sampling	O
in	O
that	O
it	O
consists	O
of	O
one-dimensional	O
transitions	O
in	O
the	O
state	O
space	O
;	O
however	O
there	O
is	O
no	O
requirement	O
that	O
the	O
one-dimensional	O
conditional	B
distributions	O
be	O
easy	O
to	O
sample	B
from	I
,	O
nor	O
that	O
they	O
have	O
any	O
convexity	B
properties	O
such	O
as	O
are	O
required	O
for	O
adaptive	O
re-	O
jection	O
sampling	O
.	O
and	O
slice	O
sampling	O
is	O
similar	O
to	O
rejection	B
sampling	I
in	O
that	O
it	O
is	O
a	O
method	B
that	O
asymptotically	O
draws	O
samples	O
from	O
the	O
volume	B
under	O
the	O
curve	O
described	O
by	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
;	O
but	O
there	O
is	O
no	O
requirement	O
for	O
an	O
upper-bounding	O
function	B
.	O
i	O
will	O
describe	O
slice	B
sampling	I
by	O
giving	O
a	O
sketch	O
of	O
a	O
one-dimensional	O
sam-	O
pling	O
algorithm	B
,	O
then	O
giving	O
a	O
pictorial	O
description	O
that	O
includes	O
the	O
details	O
that	O
make	O
the	O
method	B
valid	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29.7	O
:	O
slice	B
sampling	I
375	O
the	O
skeleton	O
of	O
slice	O
sampling	O
let	O
us	O
assume	O
that	O
we	O
want	O
to	O
draw	O
samples	O
from	O
p	O
(	O
x	O
)	O
/	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
where	O
x	O
is	O
a	O
real	O
number	O
.	O
a	O
one-dimensional	O
slice	B
sampling	I
algorithm	O
is	O
a	O
method	B
for	O
making	O
transitions	O
from	O
a	O
two-dimensional	B
point	O
(	O
x	O
;	O
u	O
)	O
lying	O
under	O
the	O
curve	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
to	O
another	O
point	O
(	O
x0	O
;	O
u0	O
)	O
lying	O
under	O
the	O
same	O
curve	O
,	O
such	O
that	O
the	O
probability	B
distribution	O
of	O
(	O
x	O
;	O
u	O
)	O
tends	O
to	O
a	O
uniform	O
distribution	B
over	O
the	O
area	O
under	O
the	O
curve	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
,	O
whatever	O
initial	O
point	O
we	O
start	O
from	O
{	O
like	O
the	O
uniform	O
distribution	B
under	O
the	O
curve	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
produced	O
by	O
rejection	O
sampling	O
(	O
section	B
29.3	O
)	O
.	O
a	O
single	O
transition	O
(	O
x	O
;	O
u	O
)	O
!	O
(	O
x0	O
;	O
u0	O
)	O
of	O
a	O
one-dimensional	O
slice	B
sampling	I
algorithm	O
has	O
the	O
following	O
steps	O
,	O
of	O
which	O
steps	O
3	O
and	O
8	O
will	O
require	O
further	O
elaboration	O
.	O
1	O
:	O
evaluate	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
2	O
:	O
draw	O
a	O
vertical	O
coordinate	O
u0	O
(	O
cid:24	O
)	O
uniform	O
(	O
0	O
;	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
)	O
3	O
:	O
create	O
a	O
horizontal	O
interval	O
(	O
xl	O
;	O
xr	O
)	O
enclosing	O
x	O
4	O
:	O
loop	O
f	O
5	O
:	O
6	O
:	O
7	O
:	O
8	O
:	O
9	O
:	O
g	O
draw	O
x0	O
(	O
cid:24	O
)	O
uniform	O
(	O
xl	O
;	O
xr	O
)	O
evaluate	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
if	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
>	O
u0	O
break	O
out	O
of	O
loop	O
4-9	O
else	O
modify	O
the	O
interval	O
(	O
xl	O
;	O
xr	O
)	O
there	O
are	O
several	O
methods	B
for	O
creating	O
the	O
interval	O
(	O
xl	O
;	O
xr	O
)	O
in	O
step	O
3	O
,	O
and	O
several	O
methods	B
for	O
modifying	O
it	O
at	O
step	O
8.	O
the	O
important	O
point	O
is	O
that	O
the	O
overall	O
method	B
must	O
satisfy	O
detailed	B
balance	I
,	O
so	O
that	O
the	O
uniform	O
distribution	B
for	O
(	O
x	O
;	O
u	O
)	O
under	O
the	O
curve	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
is	O
invariant	O
.	O
the	O
‘	O
stepping	O
out	O
’	O
method	B
for	O
step	O
3	O
in	O
the	O
‘	O
stepping	O
out	O
’	O
method	B
for	O
creating	O
an	O
interval	O
(	O
xl	O
;	O
xr	O
)	O
enclosing	O
x	O
,	O
we	O
step	O
out	O
in	O
steps	O
of	O
length	O
w	O
until	O
we	O
(	O
cid:12	O
)	O
nd	O
endpoints	O
xl	O
and	O
xr	O
at	O
which	O
p	O
(	O
cid:3	O
)	O
is	O
smaller	O
than	O
u.	O
the	O
algorithm	B
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
29.16	O
.	O
3a	O
:	O
draw	O
r	O
(	O
cid:24	O
)	O
uniform	O
(	O
0	O
;	O
1	O
)	O
3b	O
:	O
xl	O
:	O
=	O
x	O
(	O
cid:0	O
)	O
rw	O
3c	O
:	O
xr	O
:	O
=	O
x	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
r	O
)	O
w	O
3d	O
:	O
while	O
(	O
p	O
(	O
cid:3	O
)	O
(	O
xl	O
)	O
>	O
u0	O
)	O
f	O
xl	O
:	O
=	O
xl	O
(	O
cid:0	O
)	O
w	O
g	O
3e	O
:	O
while	O
(	O
p	O
(	O
cid:3	O
)	O
(	O
xr	O
)	O
>	O
u0	O
)	O
f	O
xr	O
:	O
=	O
xr	O
+	O
w	O
g	O
the	O
‘	O
shrinking	O
’	O
method	B
for	O
step	O
8	O
whenever	O
a	O
point	O
x0	O
is	O
drawn	O
such	O
that	O
(	O
x0	O
;	O
u0	O
)	O
lies	O
above	O
the	O
curve	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
,	O
we	O
shrink	O
the	O
interval	O
so	O
that	O
one	O
of	O
the	O
end	O
points	O
is	O
x0	O
,	O
and	O
such	O
that	O
the	O
original	O
point	O
x	O
is	O
still	O
enclosed	O
in	O
the	O
interval	O
.	O
8a	O
:	O
if	O
(	O
x0	O
>	O
x	O
)	O
f	O
xr	O
:	O
=	O
x0	O
g	O
8b	O
:	O
else	O
f	O
xl	O
:	O
=	O
x0	O
g	O
properties	O
of	O
slice	O
sampling	O
like	O
a	O
standard	O
metropolis	O
method	B
,	O
slice	B
sampling	I
gets	O
around	O
by	O
a	O
random	B
walk	I
,	O
but	O
whereas	O
in	O
the	O
metropolis	O
method	B
,	O
the	O
choice	O
of	O
the	O
step	O
size	O
is	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
376	O
29	O
|	O
monte	O
carlo	O
methods	B
1	O
2	O
3a,3b,3c	O
3d,3e	O
5,6	O
8	O
5,6,7	O
figure	O
29.16.	O
slice	B
sampling	I
.	O
each	O
panel	O
is	O
labelled	O
by	O
the	O
steps	O
of	O
the	O
algorithm	O
that	O
are	O
executed	O
in	O
it	O
.	O
at	O
step	O
1	O
,	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
is	O
evaluated	O
at	O
the	O
current	O
point	O
x.	O
at	O
step	O
2	O
,	O
a	O
vertical	O
coordinate	O
is	O
selected	O
giving	O
the	O
point	O
(	O
x	O
;	O
u0	O
)	O
shown	O
by	O
the	O
box	B
;	O
at	O
steps	O
3a-c	O
,	O
an	O
interval	O
of	O
size	O
w	O
containing	O
(	O
x	O
;	O
u0	O
)	O
is	O
created	O
at	O
random	B
.	O
at	O
step	O
3d	O
,	O
p	O
(	O
cid:3	O
)	O
is	O
evaluated	O
at	O
the	O
left	O
end	O
of	O
the	O
interval	O
and	O
is	O
found	O
to	O
be	O
larger	O
than	O
u0	O
,	O
so	O
a	O
step	O
to	O
the	O
left	O
of	O
size	O
w	O
is	O
made	O
.	O
at	O
step	O
3e	O
,	O
p	O
(	O
cid:3	O
)	O
is	O
evaluated	O
at	O
the	O
right	O
end	O
of	O
the	O
interval	O
and	O
is	O
found	O
to	O
be	O
smaller	O
than	O
u0	O
,	O
so	O
no	O
stepping	O
out	O
to	O
the	O
right	O
is	O
needed	O
.	O
when	O
step	O
3d	O
is	O
repeated	O
,	O
p	O
(	O
cid:3	O
)	O
is	O
found	O
to	O
be	O
smaller	O
than	O
u0	O
,	O
so	O
the	O
stepping	O
out	O
halts	O
.	O
at	O
step	O
5	O
a	O
point	O
is	O
drawn	O
from	O
the	O
interval	O
,	O
shown	O
by	O
a	O
(	O
cid:14	O
)	O
.	O
step	O
6	O
establishes	O
that	O
this	O
point	O
is	O
above	O
p	O
(	O
cid:3	O
)	O
and	O
step	O
8	O
shrinks	O
the	O
interval	O
to	O
the	O
rejected	O
point	O
in	O
such	O
a	O
way	O
that	O
the	O
original	O
point	O
x	O
is	O
still	O
in	O
the	O
interval	O
.	O
when	O
step	O
5	O
is	O
repeated	O
,	O
the	O
new	O
coordinate	O
x0	O
(	O
which	O
is	O
to	O
the	O
right-hand	O
side	O
of	O
the	O
interval	O
)	O
gives	O
a	O
value	O
of	O
p	O
(	O
cid:3	O
)	O
greater	O
than	O
u0	O
,	O
so	O
this	O
point	O
x0	O
is	O
the	O
outcome	O
at	O
step	O
7.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29.7	O
:	O
slice	B
sampling	I
377	O
10	O
1	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
figure	O
29.17.	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
critical	O
to	O
the	O
rate	B
of	O
progress	O
,	O
in	O
slice	O
sampling	O
the	O
step	O
size	O
is	O
self-tuning	O
.	O
if	O
the	O
initial	O
interval	O
size	O
w	O
is	O
too	O
small	O
by	O
a	O
factor	O
f	O
compared	O
with	O
the	O
width	O
of	O
the	O
probable	O
region	O
then	O
the	O
stepping-out	O
procedure	O
expands	O
the	O
interval	O
size	O
.	O
the	O
cost	O
of	O
this	O
stepping-out	O
is	O
only	O
linear	B
in	O
f	O
,	O
whereas	O
in	O
the	O
metropolis	O
method	B
the	O
computer-time	O
scales	O
as	O
the	O
square	B
of	O
f	O
if	O
the	O
step	O
size	O
is	O
too	O
small	O
.	O
if	O
the	O
chosen	O
value	O
of	O
w	O
is	O
too	O
large	O
by	O
a	O
factor	O
f	O
then	O
the	O
algorithm	B
spends	O
a	O
time	O
proportional	O
to	O
the	O
logarithm	O
of	O
f	O
shrinking	O
the	O
interval	O
down	O
to	O
the	O
right	O
size	O
,	O
since	O
the	O
interval	O
typically	O
shrinks	O
by	O
a	O
factor	O
in	O
the	O
ballpark	O
of	O
0:6	O
each	O
time	O
a	O
point	O
is	O
rejected	O
.	O
in	O
contrast	O
,	O
the	O
metropolis	O
algorithm	B
responds	O
to	O
a	O
too-large	O
step	O
size	O
by	O
rejecting	O
almost	O
all	O
proposals	O
,	O
so	O
the	O
rate	B
of	O
progress	O
is	O
exponentially	O
bad	B
in	O
f	O
.	O
there	O
are	O
no	O
rejections	O
in	O
slice	O
sampling	O
.	O
the	O
probability	O
of	O
staying	O
in	O
exactly	O
the	O
same	O
place	O
is	O
very	O
small	O
.	O
.	O
exercise	O
29.10	O
.	O
[	O
2	O
]	O
investigate	O
the	O
properties	O
of	O
slice	O
sampling	O
applied	O
to	O
the	O
density	B
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
29.17.	O
x	O
is	O
a	O
real	O
variable	O
between	O
0.0	O
and	O
11.0.	O
how	O
long	O
does	O
it	O
take	O
typically	O
for	O
slice	O
sampling	O
to	O
get	O
from	O
an	O
x	O
in	O
the	O
peak	O
region	O
x	O
2	O
(	O
0	O
;	O
1	O
)	O
to	O
an	O
x	O
in	O
the	O
tail	B
region	O
x	O
2	O
(	O
1	O
;	O
11	O
)	O
,	O
and	O
vice	O
versa	O
?	O
con	O
(	O
cid:12	O
)	O
rm	O
that	O
the	O
probabilities	O
of	O
these	O
transitions	O
do	O
yield	O
an	O
asymptotic	O
probability	O
density	B
that	O
is	O
correct	O
.	O
how	O
slice	O
sampling	O
is	O
used	O
in	O
real	O
problems	O
an	O
n	O
-dimensional	O
density	B
p	O
(	O
x	O
)	O
/	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
may	O
be	O
sampled	O
with	O
the	O
help	O
of	O
the	O
one-dimensional	O
slice	B
sampling	I
method	O
presented	O
above	O
by	O
picking	O
a	O
sequence	B
of	O
directions	O
y	O
(	O
1	O
)	O
;	O
y	O
(	O
2	O
)	O
;	O
:	O
:	O
:	O
and	O
de	O
(	O
cid:12	O
)	O
ning	O
x	O
=	O
x	O
(	O
t	O
)	O
+	O
xy	O
(	O
t	O
)	O
.	O
the	O
function	B
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
above	O
is	O
replaced	O
by	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
(	O
t	O
)	O
+	O
xy	O
(	O
t	O
)	O
)	O
.	O
the	O
directions	O
may	O
be	O
chosen	O
in	O
various	O
ways	O
;	O
for	O
example	O
,	O
as	O
in	O
gibbs	O
sampling	O
,	O
the	O
directions	O
could	O
be	O
the	O
coordinate	O
axes	O
;	O
alternatively	O
,	O
the	O
directions	O
y	O
(	O
t	O
)	O
may	O
be	O
selected	O
at	O
random	B
in	O
any	O
manner	O
such	O
that	O
the	O
overall	O
procedure	O
satis	O
(	O
cid:12	O
)	O
es	O
detailed	B
balance	I
.	O
computer-friendly	O
slice	B
sampling	I
the	O
real	O
variables	O
of	O
a	O
probabilistic	B
model	I
will	O
always	O
be	O
represented	O
in	O
a	O
computer	B
using	O
a	O
(	O
cid:12	O
)	O
nite	O
number	O
of	O
bits	O
.	O
in	O
the	O
following	O
implementation	O
of	O
slice	O
sampling	O
due	O
to	O
skilling	O
,	O
the	O
stepping-out	O
,	O
randomization	O
,	O
and	O
shrinking	O
operations	O
,	O
described	O
above	O
in	O
terms	O
of	O
(	O
cid:13	O
)	O
oating-point	O
operations	O
,	O
are	O
replaced	O
by	O
binary	O
and	O
integer	O
operations	O
.	O
we	O
assume	O
that	O
the	O
variable	O
x	O
that	O
is	O
being	O
slice-sampled	O
is	O
represented	O
by	O
a	O
b-bit	O
integer	O
x	O
taking	O
on	O
one	O
of	O
b	O
=	O
2b	O
values	O
,	O
0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
b	O
(	O
cid:0	O
)	O
1	O
,	O
many	O
or	O
all	O
of	O
which	O
correspond	O
to	O
valid	O
values	O
of	O
x.	O
using	O
an	O
integer	O
grid	O
eliminates	O
any	O
errors	B
in	O
detailed	B
balance	I
that	O
might	O
ensue	O
from	O
variable-precision	O
rounding	O
of	O
(	O
cid:13	O
)	O
oating-point	O
numbers	O
.	O
the	O
mapping	B
from	O
x	O
to	O
x	O
need	O
not	O
be	O
linear	B
;	O
if	O
it	O
is	O
nonlinear	B
,	O
we	O
assume	O
that	O
the	O
function	B
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
is	O
replaced	O
by	O
an	O
appropriately	O
transformed	O
function	B
{	O
for	O
example	O
,	O
p	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
/	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
jdx=dxj	O
.	O
we	O
assume	O
the	O
following	O
operators	O
on	O
b-bit	O
integers	O
are	O
available	O
:	O
x	O
+	O
n	O
x	O
(	O
cid:0	O
)	O
n	O
x	O
(	O
cid:8	O
)	O
n	O
n	O
:	O
=	O
randbits	O
(	O
l	O
)	O
arithmetic	O
sum	O
,	O
modulo	O
b	O
,	O
of	O
x	O
and	O
n	O
.	O
di	O
(	O
cid:11	O
)	O
erence	O
,	O
modulo	O
b	O
,	O
of	O
x	O
and	O
n	O
.	O
bitwise	B
exclusive-or	O
of	O
x	O
and	O
n	O
.	O
sets	O
n	O
to	O
a	O
random	B
l-bit	O
integer	O
.	O
a	O
slice-sampling	O
procedure	O
for	B
integers	I
is	O
then	O
as	O
follows	O
:	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
378	O
29	O
|	O
monte	O
carlo	O
methods	B
given	O
:	O
a	O
current	O
point	O
x	O
and	O
a	O
height	O
y	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
(	O
cid:2	O
)	O
uniform	O
(	O
0	O
;	O
1	O
)	O
(	O
cid:20	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
1	O
:	O
u	O
:	O
=	O
randbits	O
(	O
b	O
)	O
set	B
l	O
to	O
a	O
value	O
l	O
(	O
cid:20	O
)	O
b	O
do	O
f	O
n	O
:	O
=	O
randbits	O
(	O
l	O
)	O
2	O
:	O
3	O
:	O
4	O
:	O
5	O
:	O
6	O
:	O
x0	O
:	O
=	O
(	O
(	O
x	O
(	O
cid:0	O
)	O
u	O
)	O
(	O
cid:8	O
)	O
n	O
)	O
+	O
u	O
l	O
:	O
=	O
l	O
(	O
cid:0	O
)	O
1	O
7	O
:	O
g	O
until	O
(	O
x0	O
=	O
x	O
)	O
or	O
(	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
(	O
cid:21	O
)	O
y	O
)	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
random	B
translation	O
u	O
of	O
the	O
binary	O
coor-	O
dinate	O
system	O
.	O
set	B
initial	O
l-bit	O
sampling	O
range	O
.	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
random	B
move	O
within	O
the	O
current	O
interval	O
of	O
width	O
2l	O
.	O
randomize	O
the	O
lowest	O
l	O
bits	O
of	O
x	O
(	O
in	O
the	O
translated	O
coordinate	O
system	O
)	O
.	O
if	O
x0	O
is	O
not	O
acceptable	O
,	O
decrease	O
l	O
and	O
try	O
again	O
with	O
a	O
smaller	O
perturbation	O
of	O
x	O
;	O
termination	B
at	O
or	O
before	O
l	O
=	O
0	O
is	O
assured	O
.	O
0	O
x	O
b−1	O
figure	O
29.18.	O
the	O
sequence	B
of	O
intervals	B
from	O
which	O
the	O
new	O
candidate	O
points	O
are	O
drawn	O
.	O
the	O
translation	O
u	O
is	O
introduced	O
to	O
avoid	O
permanent	O
sharp	O
edges	O
,	O
where	O
for	O
example	O
the	O
adjacent	O
binary	O
integers	O
0111111111	O
and	O
1000000000	O
would	O
otherwise	O
be	O
permanently	O
in	O
di	O
(	O
cid:11	O
)	O
erent	O
sectors	O
,	O
making	O
it	O
di	O
(	O
cid:14	O
)	O
cult	O
for	O
x	O
to	O
move	O
from	O
one	O
to	O
the	O
other	O
.	O
the	O
sequence	B
of	O
intervals	B
from	O
which	O
the	O
new	O
candidate	O
points	O
are	O
drawn	O
is	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gure	O
29.18.	O
first	O
,	O
a	O
point	O
is	O
drawn	O
from	O
the	O
entire	O
interval	O
,	O
shown	O
by	O
the	O
top	O
horizontal	O
line	O
.	O
at	O
each	O
subsequent	O
draw	O
,	O
the	O
interval	O
is	O
halved	O
in	O
such	O
a	O
way	O
as	O
to	O
contain	O
the	O
previous	O
point	O
x.	O
if	O
preliminary	O
stepping-out	O
from	O
the	O
initial	O
range	O
is	O
required	O
,	O
step	O
2	O
above	O
can	O
be	O
replaced	O
by	O
the	O
following	O
similar	O
procedure	O
:	O
l	O
sets	O
the	O
initial	O
width	O
2a	O
:	O
set	B
l	O
to	O
a	O
value	O
l	O
<	O
b	O
2b	O
:	O
do	O
f	O
2c	O
:	O
2d	O
:	O
2e	O
:	O
n	O
:	O
=	O
randbits	O
(	O
l	O
)	O
x0	O
:	O
=	O
(	O
(	O
x	O
(	O
cid:0	O
)	O
u	O
)	O
(	O
cid:8	O
)	O
n	O
)	O
+	O
u	O
l	O
:	O
=	O
l	O
+	O
1	O
2f	O
:	O
g	O
until	O
(	O
l	O
=	O
b	O
)	O
or	O
(	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
<	O
y	O
)	O
these	O
shrinking	O
and	O
stepping	O
out	O
methods	B
shrink	O
and	O
expand	O
by	O
a	O
factor	O
of	O
two	O
per	O
evaluation	O
.	O
a	O
variant	O
is	O
to	O
shrink	O
or	O
expand	O
by	O
more	O
than	O
one	O
bit	B
each	O
time	O
,	O
setting	O
l	O
:	O
=	O
l	O
(	O
cid:6	O
)	O
(	O
cid:1	O
)	O
l	O
with	O
(	O
cid:1	O
)	O
l	O
>	O
1.	O
taking	O
(	O
cid:1	O
)	O
l	O
at	O
each	O
step	O
from	O
any	O
pre-assigned	O
distribution	B
(	O
which	O
may	O
include	O
(	O
cid:1	O
)	O
l	O
=	O
0	O
)	O
allows	O
extra	O
(	O
cid:13	O
)	O
exibility	O
.	O
exercise	O
29.11	O
.	O
[	O
4	O
]	O
in	O
the	O
shrinking	O
phase	O
,	O
after	O
an	O
unacceptable	O
x0	O
has	O
been	O
produced	O
,	O
the	O
choice	O
of	O
(	O
cid:1	O
)	O
l	O
is	O
allowed	O
to	O
depend	O
on	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
the	O
slice	O
’	O
s	O
height	O
y	O
and	O
the	O
value	O
of	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
,	O
without	O
spoiling	O
the	O
algo-	O
rithm	O
’	O
s	O
validity	O
.	O
(	O
prove	O
this	O
.	O
)	O
it	O
might	O
be	O
a	O
good	B
idea	O
to	O
choose	O
a	O
larger	O
value	O
of	O
(	O
cid:1	O
)	O
l	O
when	O
y	O
(	O
cid:0	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
is	O
large	O
.	O
investigate	O
this	O
idea	O
theoretically	O
or	O
empirically	O
.	O
a	O
feature	O
of	O
using	O
the	O
integer	O
representation	O
is	O
that	O
,	O
with	O
a	O
suitably	O
ex-	O
tended	O
number	O
of	O
bits	O
,	O
the	O
single	O
integer	O
x	O
can	O
represent	O
two	O
or	O
more	O
real	O
parameters	B
{	O
for	O
example	O
,	O
by	O
mapping	O
x	O
to	O
(	O
x1	O
;	O
x2	O
;	O
x3	O
)	O
through	O
a	O
space-	O
(	O
cid:12	O
)	O
lling	O
curve	O
such	O
as	O
a	O
peano	O
curve	O
.	O
thus	O
multi-dimensional	B
slice	O
sampling	O
can	O
be	O
performed	O
using	O
the	O
same	O
software	B
as	O
for	O
one	O
dimension	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
379	O
29.8	O
:	O
practicalities	O
29.8	O
practicalities	O
can	O
we	O
predict	O
how	O
long	O
a	O
markov	O
chain	B
monte	O
carlo	O
simulation	O
will	O
take	O
to	O
equilibrate	O
?	O
by	O
considering	O
the	O
random	B
walks	O
involved	O
in	O
a	O
markov	O
chain	B
monte	O
carlo	O
simulation	O
we	O
can	O
obtain	O
simple	O
lower	O
bounds	O
on	O
the	O
time	O
required	O
for	O
convergence	O
.	O
but	O
predicting	O
this	O
time	O
more	O
precisely	O
is	O
a	O
di	O
(	O
cid:14	O
)	O
cult	O
problem	O
,	O
and	O
most	O
of	O
the	O
theoretical	O
results	O
giving	O
upper	O
bounds	O
on	O
the	O
convergence	O
time	O
are	O
of	O
little	O
practical	B
use	O
.	O
the	O
exact	B
sampling	I
methods	O
of	O
chapter	O
32	O
o	O
(	O
cid:11	O
)	O
er	O
a	O
solution	O
to	O
this	O
problem	O
for	O
certain	O
markov	O
chains	O
.	O
can	O
we	O
diagnose	O
or	O
detect	O
convergence	O
in	O
a	O
running	O
simulation	O
?	O
this	O
is	O
also	O
a	O
di	O
(	O
cid:14	O
)	O
cult	O
problem	O
.	O
there	O
are	O
a	O
few	O
practical	B
tools	O
available	O
,	O
but	O
none	O
of	O
them	O
is	O
perfect	B
(	O
cowles	O
and	O
carlin	O
,	O
1996	O
)	O
.	O
can	O
we	O
speed	O
up	O
the	O
convergence	O
time	O
and	O
time	O
between	O
indepen-	O
dent	O
samples	O
of	O
a	O
markov	O
chain	B
monte	O
carlo	O
method	B
?	O
here	O
,	O
there	O
is	O
good	B
news	O
,	O
as	O
described	O
in	O
the	O
next	O
chapter	O
,	O
which	O
describes	O
the	O
hamiltonian	O
monte	O
carlo	O
method	B
,	O
overrelaxation	B
,	O
and	O
simulated	O
annealing	B
.	O
29.9	O
further	O
practical	B
issues	O
can	O
the	O
normalizing	B
constant	I
be	O
evaluated	O
?	O
if	O
the	O
target	O
density	B
p	O
(	O
x	O
)	O
is	O
given	O
in	O
the	O
form	O
of	O
an	O
unnormalized	O
density	B
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
with	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
,	O
the	O
value	O
of	O
z	O
may	O
well	O
be	O
of	O
interest	O
.	O
monte	O
carlo	O
methods	B
do	O
not	O
readily	O
yield	O
an	O
estimate	O
of	O
this	O
quantity	O
,	O
and	O
it	O
is	O
an	O
area	O
of	O
active	O
research	O
to	O
(	O
cid:12	O
)	O
nd	O
ways	O
of	O
evaluating	O
it	O
.	O
techniques	O
for	O
evaluating	O
z	O
include	O
:	O
1.	O
importance	B
sampling	I
(	O
reviewed	O
by	O
neal	O
(	O
1993b	O
)	O
)	O
and	O
annealed	O
impor-	O
tance	O
sampling	O
(	O
neal	O
,	O
1998	O
)	O
.	O
2	O
.	O
‘	O
thermodynamic	B
integration	I
’	O
during	O
simulated	B
annealing	I
,	O
the	O
‘	O
accep-	O
tance	O
ratio	O
’	O
method	B
,	O
and	O
‘	O
umbrella	B
sampling	I
’	O
(	O
reviewed	O
by	O
neal	O
(	O
1993b	O
)	O
)	O
.	O
3	O
.	O
‘	O
reversible	B
jump	I
markov	O
chain	B
monte	O
carlo	O
’	O
(	O
green	O
,	O
1995	O
)	O
.	O
one	O
way	O
of	O
dealing	O
with	O
z	O
,	O
however	O
,	O
may	O
be	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
solution	O
to	O
one	O
’	O
s	O
task	O
that	O
does	O
not	O
require	O
that	O
z	O
be	O
evaluated	O
.	O
in	O
bayesian	O
data	B
modelling	I
one	O
might	O
be	O
able	O
to	O
avoid	O
the	O
need	O
to	O
evaluate	O
z	O
{	O
which	O
would	O
be	O
important	O
for	O
model	O
comparison	O
{	O
by	O
not	O
having	O
more	O
than	O
one	O
model	B
.	O
instead	O
of	O
using	O
several	O
models	O
(	O
di	O
(	O
cid:11	O
)	O
ering	O
in	O
complexity	O
,	O
for	O
example	O
)	O
and	O
evaluating	O
their	O
rel-	O
ative	O
posterior	O
probabilities	O
,	O
one	O
can	O
make	O
a	O
single	O
hierarchical	O
model	B
having	O
,	O
for	O
example	O
,	O
various	O
continuous	B
hyperparameters	O
which	O
play	O
a	O
role	O
similar	O
to	O
that	O
played	O
by	O
the	O
distinct	O
models	O
(	O
neal	O
,	O
1996	O
)	O
.	O
in	O
noting	O
the	O
possibility	O
of	O
not	O
computing	O
z	O
,	O
i	O
am	O
not	O
endorsing	O
this	O
approach	O
.	O
the	O
normalizing	B
constant	I
z	O
is	O
often	O
the	O
single	O
most	O
important	O
number	O
in	O
the	O
problem	O
,	O
and	O
i	O
think	O
every	O
e	O
(	O
cid:11	O
)	O
ort	O
should	O
be	O
devoted	O
to	O
calculating	O
it	O
.	O
the	O
metropolis	O
method	B
for	O
big	O
models	O
our	O
original	O
description	O
of	O
the	O
metropolis	O
method	B
involved	O
a	O
joint	B
updating	O
of	O
all	O
the	O
variables	O
using	O
a	O
proposal	B
density	I
q	O
(	O
x0	O
;	O
x	O
)	O
.	O
for	O
big	O
problems	O
it	O
may	O
be	O
more	O
e	O
(	O
cid:14	O
)	O
cient	O
to	O
use	O
several	O
proposal	O
distributions	O
q	O
(	O
b	O
)	O
(	O
x0	O
;	O
x	O
)	O
,	O
each	O
of	O
which	O
updates	O
only	O
some	O
of	O
the	O
components	O
of	O
x.	O
each	O
proposal	O
is	O
individually	O
accepted	O
or	O
rejected	O
,	O
and	O
the	O
proposal	O
distributions	O
are	O
repeatedly	O
run	O
through	O
in	O
sequence	O
.	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
380	O
29	O
|	O
monte	O
carlo	O
methods	B
.	O
exercise	O
29.12	O
.	O
[	O
2	O
,	O
p.385	O
]	O
explain	O
why	O
the	O
rate	B
of	O
movement	O
through	O
the	O
state	O
space	O
will	O
be	O
greater	O
when	O
b	O
proposals	O
q	O
(	O
1	O
)	O
;	O
:	O
:	O
:	O
;	O
q	O
(	O
b	O
)	O
are	O
considered	O
individually	O
in	O
sequence	O
,	O
compared	O
with	O
the	O
case	O
of	O
a	O
single	O
proposal	O
q	O
(	O
cid:3	O
)	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
the	O
concatenation	B
of	O
q	O
(	O
1	O
)	O
;	O
:	O
:	O
:	O
;	O
q	O
(	O
b	O
)	O
.	O
assume	O
that	O
each	O
proposal	O
distribution	O
q	O
(	O
b	O
)	O
(	O
x0	O
;	O
x	O
)	O
has	O
an	O
acceptance	B
rate	I
f	O
<	O
1=2	O
.	O
in	O
the	O
metropolis	O
method	B
,	O
the	O
proposal	B
density	I
q	O
(	O
x0	O
;	O
x	O
)	O
typically	O
has	O
a	O
number	O
of	O
parameters	O
that	O
control	O
,	O
for	O
example	O
,	O
its	O
‘	O
width	O
’	O
.	O
these	O
parameters	B
are	O
usually	O
set	B
by	O
trial	O
and	O
error	O
with	O
the	O
rule	B
of	I
thumb	I
being	O
to	O
aim	O
for	O
a	O
rejection	B
frequency	O
of	O
about	O
0.5.	O
it	O
is	O
not	O
valid	O
to	O
have	O
the	O
width	O
parameters	B
be	O
dynamically	O
updated	O
during	O
the	O
simulation	O
in	O
a	O
way	O
that	O
depends	O
on	O
the	O
history	O
of	O
the	O
simulation	O
.	O
such	O
a	O
modi	O
(	O
cid:12	O
)	O
cation	O
of	O
the	O
proposal	O
density	B
would	O
violate	O
the	O
detailed-balance	O
condition	O
that	O
guarantees	O
that	O
the	O
markov	O
chain	B
has	O
the	O
correct	O
invariant	B
distribution	I
.	O
gibbs	O
sampling	O
in	O
big	O
models	O
our	O
description	O
of	O
gibbs	O
sampling	O
involved	O
sampling	O
one	O
parameter	O
at	O
a	O
time	O
,	O
as	O
described	O
in	O
equations	O
(	O
29.35	O
{	O
29.37	O
)	O
.	O
for	O
big	O
problems	O
it	O
may	O
be	O
more	O
e	O
(	O
cid:14	O
)	O
cient	O
to	O
sample	B
groups	O
of	O
variables	O
jointly	O
,	O
that	O
is	O
to	O
use	O
several	O
proposal	O
distributions	O
:	O
x	O
(	O
t+1	O
)	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
t+1	O
)	O
1	O
a+1	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
t+1	O
)	O
x	O
(	O
t+1	O
)	O
a	O
b	O
(	O
cid:24	O
)	O
p	O
(	O
x1	O
;	O
:	O
:	O
:	O
;	O
xa	O
j	O
x	O
(	O
t	O
)	O
(	O
cid:24	O
)	O
p	O
(	O
xa+1	O
;	O
:	O
:	O
:	O
;	O
xb	O
j	O
x	O
(	O
t+1	O
)	O
1	O
a+1	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
t	O
)	O
k	O
)	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
t+1	O
)	O
a	O
(	O
29.47	O
)	O
;	O
x	O
(	O
t	O
)	O
b+1	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
t	O
)	O
k	O
)	O
;	O
etc	O
.	O
how	O
many	O
samples	O
are	O
needed	O
?	O
at	O
the	O
start	O
of	O
this	O
chapter	O
,	O
we	O
observed	O
that	O
the	O
variance	B
of	O
an	O
estimator	B
^	O
(	O
cid:8	O
)	O
depends	O
only	O
on	O
the	O
number	O
of	O
independent	O
samples	O
r	O
and	O
the	O
value	O
of	O
(	O
cid:27	O
)	O
2	O
=z	O
dn	O
x	O
p	O
(	O
x	O
)	O
(	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
(	O
cid:8	O
)	O
)	O
2	O
:	O
(	O
29.48	O
)	O
we	O
have	O
now	O
discussed	O
a	O
variety	O
of	O
methods	O
for	O
generating	O
samples	O
from	O
p	O
(	O
x	O
)	O
.	O
how	O
many	O
independent	O
samples	O
r	O
should	O
we	O
aim	O
for	O
?	O
in	O
many	O
problems	O
,	O
we	O
really	O
only	O
need	O
about	O
twelve	O
independent	O
samples	O
from	O
p	O
(	O
x	O
)	O
.	O
imagine	O
that	O
x	O
is	O
an	O
unknown	O
vector	O
such	O
as	O
the	O
amount	O
of	O
corrosion	O
present	O
in	O
each	O
of	O
10	O
000	O
underground	O
pipelines	O
around	O
cambridge	O
,	O
and	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
is	O
the	O
total	O
cost	O
of	O
repairing	O
those	O
pipelines	O
.	O
the	O
distribution	B
p	O
(	O
x	O
)	O
describes	O
the	O
probability	O
of	O
a	O
state	O
x	O
given	O
the	O
tests	O
that	O
have	O
been	O
carried	O
out	O
on	O
some	O
pipelines	O
and	O
the	O
assumptions	B
about	O
the	O
physics	B
of	O
corrosion	O
.	O
the	O
quantity	O
(	O
cid:8	O
)	O
is	O
the	O
expected	O
cost	O
of	O
the	O
repairs	O
.	O
the	O
quantity	O
(	O
cid:27	O
)	O
2	O
is	O
the	O
variance	B
of	O
the	O
cost	O
{	O
(	O
cid:27	O
)	O
measures	O
by	O
how	O
much	O
we	O
should	O
expect	O
the	O
actual	O
cost	O
to	O
di	O
(	O
cid:11	O
)	O
er	O
from	O
the	O
expectation	B
(	O
cid:8	O
)	O
.	O
now	O
,	O
how	O
accurately	O
would	O
a	O
manager	O
like	O
to	O
know	O
(	O
cid:8	O
)	O
?	O
i	O
would	O
suggest	O
there	O
is	O
little	O
point	O
in	O
knowing	O
(	O
cid:8	O
)	O
to	O
a	O
precision	B
(	O
cid:12	O
)	O
ner	O
than	O
about	O
(	O
cid:27	O
)	O
=3	O
.	O
after	O
all	O
,	O
the	O
true	O
cost	O
is	O
likely	O
to	O
di	O
(	O
cid:11	O
)	O
er	O
by	O
(	O
cid:6	O
)	O
(	O
cid:27	O
)	O
from	O
(	O
cid:8	O
)	O
.	O
if	O
we	O
obtain	O
r	O
=	O
12	O
independent	O
samples	O
from	O
p	O
(	O
x	O
)	O
,	O
we	O
can	O
estimate	O
(	O
cid:8	O
)	O
to	O
a	O
precision	B
of	O
(	O
cid:27	O
)	O
=p12	O
{	O
which	O
is	O
smaller	O
than	O
(	O
cid:27	O
)	O
=3	O
.	O
so	O
twelve	O
samples	O
su	O
(	O
cid:14	O
)	O
ce	O
.	O
allocation	O
of	O
resources	O
assuming	O
we	O
have	O
decided	O
how	O
many	O
independent	O
samples	O
r	O
are	O
required	O
,	O
an	O
important	O
question	O
is	O
how	O
one	O
should	O
make	O
use	O
of	O
one	O
’	O
s	O
limited	O
computer	B
resources	O
to	O
obtain	O
these	O
samples	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
381	O
figure	O
29.19.	O
three	O
possible	O
markov	O
chain	B
monte	O
carlo	O
strategies	O
for	O
obtaining	O
twelve	O
samples	O
in	O
a	O
(	O
cid:12	O
)	O
xed	O
amount	O
of	O
computer	O
time	O
.	O
time	O
is	O
represented	O
by	O
horizontal	O
lines	O
;	O
samples	O
by	O
white	O
circles	O
.	O
(	O
1	O
)	O
a	O
single	O
run	O
consisting	O
of	O
one	O
long	O
‘	O
burn	O
in	O
’	O
period	O
followed	O
by	O
a	O
sampling	O
period	O
.	O
(	O
2	O
)	O
four	O
medium-length	O
runs	O
with	O
di	O
(	O
cid:11	O
)	O
erent	O
initial	O
conditions	O
and	O
a	O
medium-length	O
burn	O
in	O
period	O
.	O
(	O
3	O
)	O
twelve	O
short	O
runs	O
.	O
29.10	O
:	O
summary	B
(	O
1	O
)	O
(	O
2	O
)	O
(	O
3	O
)	O
a	O
typical	B
markov	O
chain	B
monte	O
carlo	O
experiment	O
involves	O
an	O
initial	O
pe-	O
riod	O
in	O
which	O
control	O
parameters	O
of	O
the	O
simulation	O
such	O
as	O
step	O
sizes	O
may	O
be	O
adjusted	O
.	O
this	O
is	O
followed	O
by	O
a	O
‘	O
burn	O
in	O
’	O
period	O
during	O
which	O
we	O
hope	O
the	O
simulation	O
‘	O
converges	O
’	O
to	O
the	O
desired	O
distribution	B
.	O
finally	O
,	O
as	O
the	O
simulation	O
continues	O
,	O
we	O
record	O
the	O
state	O
vector	O
occasionally	O
so	O
as	O
to	O
create	O
a	O
list	O
of	O
states	O
fx	O
(	O
r	O
)	O
gr	O
r=1	O
that	O
we	O
hope	O
are	O
roughly	O
independent	O
samples	O
from	O
p	O
(	O
x	O
)	O
.	O
there	O
are	O
several	O
possible	O
strategies	O
(	O
(	O
cid:12	O
)	O
gure	O
29.19	O
)	O
:	O
1.	O
make	O
one	O
long	O
run	O
,	O
obtaining	O
all	O
r	O
samples	O
from	O
it	O
.	O
2.	O
make	O
a	O
few	O
medium-length	O
runs	O
with	O
di	O
(	O
cid:11	O
)	O
erent	O
initial	O
conditions	O
,	O
obtain-	O
ing	O
some	O
samples	O
from	O
each	O
.	O
3.	O
make	O
r	O
short	O
runs	O
,	O
each	O
starting	O
from	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
random	B
initial	O
condi-	O
tion	O
,	O
with	O
the	O
only	O
state	O
that	O
is	O
recorded	O
being	O
the	O
(	O
cid:12	O
)	O
nal	O
state	O
of	O
each	O
simulation	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
strategy	O
has	O
the	O
best	O
chance	O
of	O
attaining	O
‘	O
convergence	O
’	O
.	O
the	O
last	O
strategy	O
may	O
have	O
the	O
advantage	O
that	O
the	O
correlations	B
between	O
the	O
recorded	O
samples	O
are	O
smaller	O
.	O
the	O
middle	O
path	O
is	O
popular	O
with	O
markov	O
chain	B
monte	O
carlo	O
experts	O
(	O
gilks	O
et	O
al.	O
,	O
1996	O
)	O
because	O
it	O
avoids	O
the	O
ine	O
(	O
cid:14	O
)	O
ciency	O
of	O
discarding	O
burn-in	O
iterations	O
in	O
many	O
runs	O
,	O
while	O
still	O
allowing	O
one	O
to	O
detect	O
problems	O
with	O
lack	O
of	O
convergence	O
that	O
would	O
not	O
be	O
apparent	O
from	O
a	O
single	O
run	O
.	O
finally	O
,	O
i	O
should	O
emphasize	O
that	O
there	O
is	O
no	O
need	O
to	O
make	O
the	O
points	O
in	O
the	O
estimate	O
nearly-independent	O
.	O
averaging	O
over	O
dependent	O
points	O
is	O
(	O
cid:12	O
)	O
ne	O
{	O
it	O
won	O
’	O
t	O
lead	O
to	O
any	O
bias	B
in	O
the	O
estimates	O
.	O
for	O
example	O
,	O
when	O
you	O
use	O
strategy	O
1	O
or	O
2	O
,	O
you	O
may	O
,	O
if	O
you	O
wish	O
,	O
include	O
all	O
the	O
points	O
between	O
the	O
(	O
cid:12	O
)	O
rst	O
and	O
last	O
sample	B
in	O
each	O
run	O
.	O
of	O
course	O
,	O
estimating	O
the	O
accuracy	O
of	O
the	O
estimate	O
is	O
harder	O
when	O
the	O
points	O
are	O
dependent	O
.	O
29.10	O
summary	B
(	O
cid:15	O
)	O
monte	O
carlo	O
methods	B
are	O
a	O
powerful	O
tool	O
that	O
allow	O
one	O
to	O
sample	B
from	I
any	O
probability	B
distribution	O
that	O
can	O
be	O
expressed	O
in	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
1	O
z	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
(	O
cid:15	O
)	O
monte	O
carlo	O
methods	B
can	O
answer	O
virtually	O
any	O
query	O
related	O
to	O
p	O
(	O
x	O
)	O
by	O
putting	O
the	O
query	O
in	O
the	O
form	O
z	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
’	O
1	O
rxr	O
(	O
cid:30	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
:	O
(	O
29.49	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
382	O
29	O
|	O
monte	O
carlo	O
methods	B
(	O
cid:15	O
)	O
in	O
high-dimensional	O
problems	O
the	O
only	O
satisfactory	O
methods	B
are	O
those	O
based	O
on	O
markov	O
chains	O
,	O
such	O
as	O
the	O
metropolis	O
method	B
,	O
gibbs	O
sam-	O
pling	O
and	O
slice	O
sampling	O
.	O
gibbs	O
sampling	O
is	O
an	O
attractive	O
method	B
be-	O
cause	O
it	O
has	O
no	O
adjustable	O
parameters	B
but	O
its	O
use	O
is	O
restricted	O
to	O
cases	O
where	O
samples	O
can	O
be	O
generated	O
from	O
the	O
conditional	B
distributions	O
.	O
slice	B
sampling	I
is	O
attractive	O
because	O
,	O
whilst	O
it	O
has	O
step-length	O
parameters	B
,	O
its	O
performance	O
is	O
not	O
very	O
sensitive	O
to	O
their	O
values	O
.	O
(	O
cid:15	O
)	O
simple	O
metropolis	O
algorithms	B
and	O
gibbs	O
sampling	O
algorithms	O
,	O
although	O
widely	O
used	O
,	O
perform	O
poorly	O
because	O
they	O
explore	B
the	O
space	O
by	O
a	O
slow	O
random	B
walk	I
.	O
the	O
next	O
chapter	O
will	O
discuss	O
methods	B
for	O
speeding	O
up	O
markov	O
chain	B
monte	O
carlo	O
simulations	O
.	O
(	O
cid:15	O
)	O
slice	B
sampling	I
does	O
not	O
avoid	O
random	B
walk	I
behaviour	O
,	O
but	O
it	O
automat-	O
ically	O
chooses	O
the	O
largest	O
appropriate	O
step	O
size	O
,	O
thus	O
reducing	O
the	O
bad	B
e	O
(	O
cid:11	O
)	O
ects	O
of	O
the	O
random	O
walk	O
compared	O
with	O
,	O
say	O
,	O
a	O
metropolis	O
method	B
with	O
a	O
tiny	O
step	O
size	O
.	O
29.11	O
exercises	O
exercise	O
29.13	O
.	O
[	O
2c	O
,	O
p.386	O
]	O
a	O
study	O
of	O
importance	O
sampling	O
.	O
we	O
already	O
estab-	O
lished	O
in	O
section	O
29.2	O
that	O
importance	B
sampling	I
is	O
likely	O
to	O
be	O
useless	O
in	O
high-dimensional	O
problems	O
.	O
this	O
exercise	O
explores	O
a	O
further	O
cautionary	O
tale	O
,	O
showing	O
that	O
importance	B
sampling	I
can	O
fail	O
even	O
in	O
one	O
dimension	O
,	O
even	O
with	O
friendly	O
gaussian	O
distributions	O
.	O
imagine	O
that	O
we	O
want	O
to	O
know	O
the	O
expectation	B
of	O
a	O
function	B
(	O
cid:30	O
)	O
(	O
x	O
)	O
under	O
a	O
distribution	B
p	O
(	O
x	O
)	O
,	O
(	O
cid:8	O
)	O
=z	O
dx	O
p	O
(	O
x	O
)	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
;	O
(	O
29.50	O
)	O
and	O
that	O
this	O
expectation	B
is	O
estimated	O
by	O
importance	O
sampling	O
with	O
a	O
distribution	B
q	O
(	O
x	O
)	O
.	O
alternatively	O
,	O
perhaps	O
we	O
wish	O
to	O
estimate	O
the	O
normalizing	B
constant	I
z	O
in	O
p	O
(	O
x	O
)	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=z	O
using	O
z	O
=z	O
dx	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=z	O
dx	O
q	O
(	O
x	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
=	O
(	O
cid:28	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
(	O
cid:29	O
)	O
x	O
(	O
cid:24	O
)	O
q	O
:	O
(	O
29.51	O
)	O
now	O
,	O
let	O
p	O
(	O
x	O
)	O
and	O
q	O
(	O
x	O
)	O
be	O
gaussian	O
distributions	O
with	O
mean	O
zero	O
and	O
standard	O
deviations	O
(	O
cid:27	O
)	O
p	O
and	O
(	O
cid:27	O
)	O
q.	O
each	O
point	O
x	O
drawn	O
from	O
q	O
will	O
have	O
an	O
associated	O
weight	B
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=q	O
(	O
x	O
)	O
.	O
what	O
is	O
the	O
variance	B
of	O
the	O
weights	O
?	O
[	O
assume	O
that	O
p	O
(	O
cid:3	O
)	O
=	O
p	O
,	O
so	O
p	O
is	O
actually	O
normalized	O
,	O
and	O
z	O
=	O
1	O
,	O
though	O
we	O
can	O
pretend	O
that	O
we	O
didn	O
’	O
t	O
know	O
that	O
.	O
]	O
what	O
happens	O
to	O
the	O
variance	B
of	O
the	O
weights	O
as	O
(	O
cid:27	O
)	O
2	O
check	O
your	O
theory	B
by	O
simulating	O
this	O
importance-sampling	O
problem	O
on	O
a	O
computer	B
.	O
q	O
!	O
(	O
cid:27	O
)	O
2	O
p=2	O
?	O
exercise	O
29.14	O
.	O
[	O
2	O
]	O
consider	O
the	O
metropolis	O
algorithm	B
for	O
the	O
one-dimensional	O
toy	O
problem	O
of	O
section	O
29.4	O
,	O
sampling	O
from	O
f0	O
;	O
1	O
;	O
:	O
:	O
:	O
;	O
20g	O
.	O
whenever	O
the	O
current	O
state	O
is	O
one	O
of	O
the	O
end	O
states	O
,	O
the	O
proposal	B
density	I
given	O
in	O
equation	O
(	O
29.34	O
)	O
will	O
propose	O
with	O
probability	O
50	O
%	O
a	O
state	O
that	O
will	O
be	O
rejected	O
.	O
to	O
reduce	O
this	O
‘	O
waste	O
’	O
,	O
fred	O
modi	O
(	O
cid:12	O
)	O
es	O
the	O
software	B
responsible	O
for	O
gen-	O
erating	O
samples	O
from	O
q	O
so	O
that	O
when	O
x	O
=	O
0	O
,	O
the	O
proposal	B
density	I
is	O
100	O
%	O
on	O
x0	O
=	O
1	O
,	O
and	O
similarly	O
when	O
x	O
=	O
20	O
,	O
x0	O
=	O
19	O
is	O
always	O
proposed	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29.11	O
:	O
exercises	O
383	O
fred	O
sets	O
the	O
software	B
that	O
implements	O
the	O
acceptance	O
rule	O
so	O
that	O
the	O
software	B
accepts	O
all	O
proposed	O
moves	O
.	O
what	O
probability	O
p	O
0	O
(	O
x	O
)	O
will	O
fred	O
’	O
s	O
modi	O
(	O
cid:12	O
)	O
ed	O
software	B
generate	O
samples	O
from	O
?	O
what	O
is	O
the	O
correct	O
acceptance	O
rule	O
for	O
fred	O
’	O
s	O
proposal	B
density	I
,	O
in	O
order	O
to	O
obtain	O
samples	O
from	O
p	O
(	O
x	O
)	O
?	O
.	O
exercise	O
29.15	O
.	O
[	O
3c	O
]	O
implement	O
gibbs	O
sampling	O
for	O
the	O
inference	B
of	O
a	O
single	O
one-dimensional	O
gaussian	O
,	O
which	O
we	O
studied	O
using	O
maximum	B
likelihood	I
in	O
section	B
22.1.	O
assign	O
a	O
broad	O
gaussian	O
prior	B
to	O
(	O
cid:22	O
)	O
and	O
a	O
broad	O
gamma	B
prior	O
(	O
24.2	O
)	O
to	O
the	O
precision	B
parameter	O
(	O
cid:12	O
)	O
=	O
1=	O
(	O
cid:27	O
)	O
2.	O
each	O
update	O
of	O
(	O
cid:22	O
)	O
will	O
involve	O
a	O
sample	B
from	I
a	O
gaussian	O
distribution	B
,	O
and	O
each	O
update	O
of	O
(	O
cid:27	O
)	O
requires	O
a	O
sample	B
from	I
a	O
gamma	B
distribution	I
.	O
exercise	O
29.16	O
.	O
[	O
3c	O
]	O
gibbs	O
sampling	O
for	O
clustering	B
.	O
implement	O
gibbs	O
sampling	O
for	O
the	O
inference	B
of	O
a	O
mixture	O
of	O
k	O
one-dimensional	O
gaussians	O
,	O
which	O
we	O
studied	O
using	O
maximum	B
likelihood	I
in	O
section	B
22.2.	O
allow	O
the	O
clusters	O
to	O
have	O
di	O
(	O
cid:11	O
)	O
erent	O
standard	O
deviations	O
(	O
cid:27	O
)	O
k.	O
assign	O
priors	O
to	O
the	O
means	O
and	O
standard	O
deviations	O
in	O
the	O
same	O
way	O
as	O
the	O
previous	O
exercise	O
.	O
either	O
(	O
cid:12	O
)	O
x	O
the	O
prior	B
probabilities	O
of	O
the	O
classes	O
f	O
(	O
cid:25	O
)	O
kg	O
to	O
be	O
equal	O
or	O
put	O
a	O
uniform	O
prior	B
over	O
the	O
parameters	B
(	O
cid:25	O
)	O
and	O
include	O
them	O
in	O
the	O
gibbs	O
sampling	O
.	O
notice	O
the	O
similarity	O
of	O
gibbs	O
sampling	O
to	O
the	O
soft	B
k-means	O
clustering	B
algorithm	O
(	O
algorithm	B
22.2	O
)	O
.	O
we	O
can	O
alternately	O
assign	O
the	O
class	O
labels	O
fkng	O
given	O
the	O
parameters	B
f	O
(	O
cid:22	O
)	O
k	O
;	O
(	O
cid:27	O
)	O
kg	O
,	O
then	O
update	O
the	O
parameters	B
given	O
the	O
class	O
labels	O
.	O
the	O
assignment	O
step	O
involves	O
sampling	O
from	O
the	O
proba-	O
bility	O
distributions	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
the	O
responsibilities	O
(	O
22.22	O
)	O
,	O
and	O
the	O
update	O
step	O
updates	O
the	O
means	O
and	O
variances	O
using	O
probability	B
distributions	I
centred	O
on	O
the	O
k-means	O
algorithm	B
’	O
s	O
values	O
(	O
22.23	O
,	O
22.24	O
)	O
.	O
do	O
your	O
experiments	O
con	O
(	O
cid:12	O
)	O
rm	O
that	O
monte	O
carlo	O
methods	B
bypass	O
the	O
over-	O
(	O
cid:12	O
)	O
tting	O
di	O
(	O
cid:14	O
)	O
culties	O
of	O
maximum	O
likelihood	B
discussed	O
in	O
section	O
22.4	O
?	O
a	O
solution	O
to	O
this	O
exercise	O
and	O
the	O
previous	O
one	O
,	O
written	O
in	O
octave	O
,	O
is	O
available.2	O
.	O
exercise	O
29.17	O
.	O
[	O
3c	O
]	O
implement	O
gibbs	O
sampling	O
for	O
the	O
seven	O
scientists	B
inference	O
problem	O
,	O
which	O
we	O
encountered	O
in	O
exercise	O
22.15	O
(	O
p.309	O
)	O
,	O
and	O
which	O
you	O
may	O
have	O
solved	O
by	O
exact	O
marginalization	B
(	O
exercise	O
24.3	O
(	O
p.323	O
)	O
)	O
[	O
it	O
’	O
s	O
not	O
essential	O
to	O
have	O
done	O
the	O
latter	O
]	O
.	O
.	O
exercise	O
29.18	O
.	O
[	O
2	O
]	O
a	O
metropolis	O
method	B
is	O
used	O
to	O
explore	B
a	O
distribution	B
p	O
(	O
x	O
)	O
that	O
is	O
actually	O
a	O
1000-dimensional	O
spherical	O
gaussian	O
distribution	B
of	O
standard	B
deviation	I
1	O
in	O
all	O
dimensions	B
.	O
the	O
proposal	B
density	I
q	O
is	O
a	O
1000-dimensional	O
spherical	O
gaussian	O
distribution	B
of	O
standard	B
deviation	I
(	O
cid:15	O
)	O
.	O
roughly	O
what	O
is	O
the	O
step	O
size	O
(	O
cid:15	O
)	O
if	O
the	O
acceptance	B
rate	I
is	O
0.5	O
?	O
assuming	O
this	O
value	O
of	O
(	O
cid:15	O
)	O
,	O
(	O
a	O
)	O
roughly	O
how	O
long	O
would	O
the	O
method	B
take	O
to	O
traverse	O
the	O
distribution	B
and	O
generate	O
a	O
sample	B
independent	O
of	O
the	O
initial	O
condition	O
?	O
(	O
b	O
)	O
by	O
how	O
much	O
does	O
ln	O
p	O
(	O
x	O
)	O
change	O
in	O
a	O
typical	B
step	O
?	O
by	O
how	O
much	O
should	O
ln	O
p	O
(	O
x	O
)	O
vary	O
when	O
x	O
is	O
drawn	O
from	O
p	O
(	O
x	O
)	O
?	O
(	O
c	O
)	O
what	O
happens	O
if	O
,	O
rather	O
than	O
using	O
a	O
metropolis	O
method	B
that	O
tries	O
to	O
change	O
all	O
components	O
at	O
once	O
,	O
one	O
instead	O
uses	O
a	O
concatenation	B
of	O
metropolis	O
updates	O
changing	O
one	O
component	O
at	O
a	O
time	O
?	O
2http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
384	O
29	O
|	O
monte	O
carlo	O
methods	B
.	O
exercise	O
29.19	O
.	O
[	O
2	O
]	O
when	O
discussing	O
the	O
time	O
taken	O
by	O
the	O
metropolis	O
algo-	O
rithm	O
to	O
generate	O
independent	O
samples	O
we	O
considered	O
a	O
distribution	B
with	O
longest	O
spatial	O
length	B
scale	O
l	O
being	O
explored	O
using	O
a	O
proposal	O
distribu-	O
tion	O
with	O
step	O
size	O
(	O
cid:15	O
)	O
.	O
another	O
dimension	O
that	O
a	O
mcmc	O
method	B
must	O
explore	B
is	O
the	O
range	O
of	O
possible	O
values	O
of	O
the	O
log	O
probability	B
ln	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
.	O
assuming	O
that	O
the	O
state	O
x	O
contains	O
a	O
number	O
of	O
independent	O
random	B
variables	O
proportional	O
to	O
n	O
,	O
when	O
samples	O
are	O
drawn	O
from	O
p	O
(	O
x	O
)	O
,	O
the	O
‘	O
asymptotic	B
equipartition	I
’	O
principle	O
tell	O
us	O
that	O
the	O
value	O
of	O
(	O
cid:0	O
)	O
ln	O
p	O
(	O
x	O
)	O
is	O
likely	O
to	O
be	O
close	O
to	O
the	O
entropy	B
of	O
x	O
,	O
varying	O
either	O
side	O
with	O
a	O
standard	B
deviation	I
that	O
scales	O
as	O
pn	O
.	O
consider	O
a	O
metropolis	O
method	B
with	O
a	O
sym-	O
metrical	O
proposal	B
density	I
,	O
that	O
is	O
,	O
one	O
that	O
satis	O
(	O
cid:12	O
)	O
es	O
q	O
(	O
x	O
;	O
x0	O
)	O
=	O
q	O
(	O
x0	O
;	O
x	O
)	O
.	O
assuming	O
that	O
accepted	O
jumps	O
either	O
increase	O
ln	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
by	O
some	O
amount	O
or	O
decrease	O
it	O
by	O
a	O
small	O
amount	O
,	O
e.g	O
.	O
ln	O
e	O
=	O
1	O
(	O
is	O
this	O
a	O
reasonable	O
assumption	O
?	O
)	O
,	O
discuss	O
how	O
long	O
it	O
must	O
take	O
to	O
generate	O
roughly	O
inde-	O
pendent	O
samples	O
from	O
p	O
(	O
x	O
)	O
.	O
discuss	O
whether	O
gibbs	O
sampling	O
has	O
similar	O
properties	O
.	O
exercise	O
29.20	O
.	O
[	O
3	O
]	O
markov	O
chain	B
monte	O
carlo	O
methods	B
do	O
not	O
compute	O
parti-	O
tion	O
functions	B
z	O
,	O
yet	O
they	O
allow	O
ratios	O
of	O
quantities	O
like	O
z	O
to	O
be	O
esti-	O
mated	O
.	O
for	O
example	O
,	O
consider	O
a	O
random-walk	O
metropolis	O
algorithm	B
in	O
a	O
state	O
space	O
where	O
the	O
energy	B
is	O
zero	O
in	O
a	O
connected	O
accessible	O
region	O
,	O
and	O
in	O
(	O
cid:12	O
)	O
nitely	O
large	O
everywhere	O
else	O
;	O
and	O
imagine	O
that	O
the	O
accessible	O
space	O
can	O
be	O
chopped	O
into	O
two	O
regions	O
connected	O
by	O
one	O
or	O
more	O
corridor	O
states	O
.	O
the	O
fraction	O
of	O
times	O
spent	O
in	O
each	O
region	O
at	O
equilibrium	O
is	O
proportional	O
to	O
the	O
volume	B
of	O
the	O
region	O
.	O
how	O
does	O
the	O
monte	O
carlo	O
method	B
manage	O
to	O
do	O
this	O
without	O
measuring	O
the	O
volumes	O
?	O
exercise	O
29.21	O
.	O
[	O
5	O
]	O
philosophy	B
.	O
one	O
curious	O
defect	O
of	O
these	O
monte	O
carlo	O
methods	B
{	O
which	O
are	O
widely	O
used	O
by	O
bayesian	O
statisticians	O
{	O
is	O
that	O
they	O
are	O
all	O
non-bayesian	O
(	O
o	O
’	O
hagan	O
,	O
1987	O
)	O
.	O
they	O
involve	O
computer	B
experiments	O
from	O
which	O
estimators	O
of	O
quantities	O
of	O
interest	O
are	O
derived	O
.	O
these	O
estimators	O
depend	O
on	O
the	O
pro-	O
posal	O
distributions	O
that	O
were	O
used	O
to	O
generate	O
the	O
samples	O
and	O
on	O
the	O
random	B
numbers	O
that	O
happened	O
to	O
come	O
out	O
of	O
our	O
random	B
number	I
generator	I
.	O
in	O
contrast	O
,	O
an	O
alternative	O
bayesian	O
approach	O
to	O
the	O
problem	O
would	O
use	O
the	O
results	O
of	O
our	O
computer	B
experiments	O
to	O
infer	O
the	O
proper-	O
ties	O
of	O
the	O
target	O
function	B
p	O
(	O
x	O
)	O
and	O
generate	O
predictive	O
distributions	O
for	O
quantities	O
of	O
interest	O
such	O
as	O
(	O
cid:8	O
)	O
.	O
this	O
approach	O
would	O
give	O
answers	O
that	O
would	O
depend	O
only	O
on	O
the	O
computed	O
values	O
of	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
at	O
the	O
points	O
fx	O
(	O
r	O
)	O
g	O
;	O
the	O
answers	O
would	O
not	O
depend	O
on	O
how	O
those	O
points	O
were	O
chosen	O
.	O
can	O
you	O
make	O
a	O
bayesian	O
monte	O
carlo	O
method	B
?	O
(	O
see	O
rasmussen	O
and	O
ghahramani	O
(	O
2003	O
)	O
for	O
a	O
practical	B
attempt	O
.	O
)	O
29.12	O
solutions	O
solution	O
to	O
exercise	O
29.1	O
(	O
p.362	O
)	O
.	O
we	O
wish	O
to	O
show	O
that	O
^	O
(	O
cid:8	O
)	O
(	O
cid:17	O
)	O
pr	O
wr	O
(	O
cid:30	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
pr	O
wr	O
(	O
29.52	O
)	O
converges	O
to	O
the	O
expectation	B
of	O
(	O
cid:8	O
)	O
under	O
p	O
.	O
we	O
consider	O
the	O
numerator	O
and	O
the	O
denominator	O
separately	O
.	O
first	O
,	O
the	O
denominator	O
.	O
consider	O
a	O
single	O
importance	O
weight	B
wr	O
(	O
cid:17	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
:	O
(	O
29.53	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
29.12	O
:	O
solutions	O
385	O
what	O
is	O
its	O
expectation	B
,	O
averaged	O
under	O
the	O
distribution	B
q	O
=	O
q	O
(	O
cid:3	O
)	O
=zq	O
of	O
the	O
point	O
x	O
(	O
r	O
)	O
?	O
hwri	O
=z	O
dx	O
q	O
(	O
x	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=z	O
dx	O
1	O
zq	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=	O
zp	O
zq	O
:	O
so	O
the	O
expectation	B
of	O
the	O
denominator	O
is	O
*xr	O
wr+	O
=	O
r	O
zp	O
zq	O
:	O
(	O
29.54	O
)	O
(	O
29.55	O
)	O
as	O
long	O
as	O
the	O
variance	B
of	O
wr	O
is	O
(	O
cid:12	O
)	O
nite	O
,	O
the	O
denominator	O
,	O
divided	O
by	O
r	O
,	O
will	O
converge	O
to	O
zp	O
=zq	O
as	O
r	O
increases	O
.	O
[	O
in	O
fact	O
,	O
the	O
estimate	O
converges	O
to	O
the	O
right	O
answer	O
even	O
if	O
this	O
variance	B
is	O
in	O
(	O
cid:12	O
)	O
nite	O
,	O
as	O
long	O
as	O
the	O
expectation	B
is	O
well-de	O
(	O
cid:12	O
)	O
ned	O
.	O
]	O
similarly	O
,	O
the	O
expectation	B
of	O
one	O
term	O
in	O
the	O
numerator	O
is	O
hwr	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
i	O
=z	O
dx	O
q	O
(	O
x	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
q	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
=z	O
dx	O
1	O
zq	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
=	O
zp	O
zq	O
(	O
cid:8	O
)	O
;	O
(	O
29.56	O
)	O
where	O
(	O
cid:8	O
)	O
is	O
the	O
expectation	B
of	O
(	O
cid:30	O
)	O
under	O
p	O
.	O
so	O
the	O
numerator	O
,	O
divided	O
by	O
r	O
,	O
converges	O
to	O
zp	O
zq	O
(	O
cid:8	O
)	O
with	O
increasing	O
r.	O
thus	O
^	O
(	O
cid:8	O
)	O
converges	O
to	O
(	O
cid:8	O
)	O
.	O
the	O
numerator	O
and	O
the	O
denominator	O
are	O
unbiased	O
estimators	O
of	O
rzp	O
=zq	O
and	O
rzp	O
=zq	O
(	O
cid:8	O
)	O
respectively	O
,	O
but	O
their	O
ratio	O
^	O
(	O
cid:8	O
)	O
is	O
not	O
necessarily	O
an	O
unbiased	B
estimator	I
for	O
(	O
cid:12	O
)	O
nite	O
r.	O
solution	O
to	O
exercise	O
29.2	O
(	O
p.363	O
)	O
.	O
when	O
the	O
true	O
density	B
p	O
is	O
multimodal	O
,	O
it	O
is	O
unwise	O
to	O
use	O
importance	B
sampling	I
with	O
a	O
sampler	B
density	I
(	O
cid:12	O
)	O
tted	O
to	O
one	O
mode	O
,	O
because	O
on	O
the	O
rare	O
occasions	O
that	O
a	O
point	O
is	O
produced	O
that	O
lands	O
in	O
one	O
of	O
the	O
other	O
modes	O
,	O
the	O
weight	B
associated	O
with	O
that	O
point	O
will	O
be	O
enormous	O
.	O
the	O
estimates	O
will	O
have	O
enormous	O
variance	B
,	O
but	O
this	O
enormous	O
variance	B
may	O
not	O
be	O
evident	O
to	O
the	O
user	O
if	O
no	O
points	O
in	O
the	O
other	O
modes	O
have	O
been	O
seen	O
.	O
solution	O
to	O
exercise	O
29.5	O
(	O
p.371	O
)	O
.	O
the	O
posterior	O
distribution	O
for	O
the	O
syndrome	B
decoding	I
problem	O
is	O
a	O
pathological	O
distribution	B
from	O
the	O
point	O
of	O
view	O
of	O
gibbs	O
sampling	O
.	O
the	O
factor	O
	O
[	O
hn	O
=	O
z	O
]	O
is	O
1	O
only	O
on	O
a	O
small	O
fraction	O
of	O
the	O
space	O
of	O
possible	O
vectors	B
n	O
,	O
namely	O
the	O
2k	O
points	O
that	O
correspond	O
to	O
the	O
valid	O
code-	O
words	O
.	O
no	O
two	O
codewords	O
are	O
adjacent	O
,	O
so	O
similarly	O
,	O
any	O
single	O
bit	O
(	O
cid:13	O
)	O
ip	O
from	O
a	O
viable	O
state	O
n	O
will	O
take	O
us	O
to	O
a	O
state	O
with	O
zero	O
probability	B
and	O
so	O
the	O
state	O
will	O
never	O
move	O
in	O
gibbs	O
sampling	O
.	O
a	O
general	O
code	O
has	O
exactly	O
the	O
same	O
problem	O
.	O
the	O
points	O
corresponding	O
to	O
valid	O
codewords	O
are	O
relatively	O
few	O
in	O
number	O
and	O
they	O
are	O
not	O
adjacent	O
(	O
at	O
least	O
for	O
any	O
useful	O
code	B
)	O
.	O
so	O
gibbs	O
sampling	O
is	O
no	O
use	O
for	O
syndrome	O
decoding	B
for	O
two	O
reasons	O
.	O
first	O
,	O
(	O
cid:12	O
)	O
nding	O
any	O
reasonably	O
good	B
hypothesis	O
is	O
di	O
(	O
cid:14	O
)	O
cult	O
,	O
and	O
as	O
long	O
as	O
the	O
state	O
is	O
not	O
near	O
a	O
valid	O
codeword	B
,	O
gibbs	O
sampling	O
can	O
not	O
help	O
since	O
none	O
of	O
the	O
conditional	O
distributions	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
;	O
and	O
second	O
,	O
once	O
we	O
are	O
in	O
a	O
valid	O
hypothesis	O
,	O
gibbs	O
sampling	O
will	O
never	O
take	O
us	O
out	O
of	O
it	O
.	O
one	O
could	O
attempt	O
to	O
perform	O
gibbs	O
sampling	O
using	O
the	O
bits	O
of	O
the	O
original	O
message	O
s	O
as	O
the	O
variables	O
.	O
this	O
approach	O
would	O
not	O
get	O
locked	O
up	O
in	O
the	O
way	O
just	O
described	O
,	O
but	O
,	O
for	O
a	O
good	B
code	O
,	O
any	O
single	O
bit	O
(	O
cid:13	O
)	O
ip	O
would	O
substantially	O
alter	O
the	O
reconstructed	O
codeword	B
,	O
so	O
if	O
one	O
had	O
found	O
a	O
state	O
with	O
reasonably	O
large	O
likelihood	B
,	O
gibbs	O
sampling	O
would	O
take	O
an	O
impractically	O
large	O
time	O
to	O
escape	O
from	O
it	O
.	O
solution	O
to	O
exercise	O
29.12	O
(	O
p.380	O
)	O
.	O
each	O
metropolis	O
proposal	O
will	O
take	O
the	O
energy	B
of	O
the	O
state	O
up	O
or	O
down	O
by	O
some	O
amount	O
.	O
the	O
total	O
change	O
in	O
energy	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
386	O
29	O
|	O
monte	O
carlo	O
methods	B
when	O
b	O
proposals	O
are	O
concatenated	B
will	O
be	O
the	O
end-point	O
of	O
a	O
random	B
walk	I
with	O
b	O
steps	O
in	O
it	O
.	O
this	O
walk	O
might	O
have	O
mean	B
zero	O
,	O
or	O
it	O
might	O
have	O
a	O
tendency	O
to	O
drift	O
upwards	O
(	O
if	O
most	O
moves	O
increase	O
the	O
energy	B
and	O
only	O
a	O
few	O
decrease	O
it	O
)	O
.	O
in	O
general	O
the	O
latter	O
will	O
hold	O
,	O
if	O
the	O
acceptance	B
rate	I
f	O
is	O
small	O
:	O
the	O
mean	B
change	O
in	O
energy	O
from	O
any	O
one	O
move	O
will	O
be	O
some	O
(	O
cid:1	O
)	O
e	O
>	O
0	O
and	O
so	O
the	O
acceptance	O
probability	O
for	O
the	O
concatenation	B
of	O
b	O
moves	O
will	O
be	O
of	O
order	O
1=	O
(	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
b	O
(	O
cid:1	O
)	O
e	O
)	O
)	O
,	O
which	O
scales	O
roughly	O
as	O
f	O
b.	O
the	O
mean-square-distance	O
moved	O
will	O
be	O
of	O
order	O
f	O
bb	O
(	O
cid:15	O
)	O
2	O
,	O
where	O
(	O
cid:15	O
)	O
is	O
the	O
typical	B
step	O
size	O
.	O
in	O
contrast	O
,	O
the	O
mean-square-distance	O
moved	O
when	O
the	O
moves	O
are	O
considered	O
individually	O
will	O
be	O
of	O
order	O
f	O
b	O
(	O
cid:15	O
)	O
2	O
.	O
1.1	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
10	O
1000	O
10000	O
100000	O
theory	B
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
1.2	O
1.4	O
1.6	O
8	O
6	O
4	O
2	O
0	O
1000	O
10000	O
100000	O
theory	B
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
1.2	O
1.4	O
0	O
0	O
1.6	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
1.2	O
1.4	O
1.6	O
figure	O
29.20.	O
importance	B
sampling	I
in	O
one	O
dimension	O
.	O
for	O
r	O
=	O
1000	O
;	O
104	O
,	O
and	O
105	O
,	O
the	O
normalizing	B
constant	I
of	O
a	O
gaussian	O
distribution	B
(	O
known	O
in	O
fact	O
to	O
be	O
1	O
)	O
was	O
estimated	O
using	O
importance	B
sampling	I
with	O
a	O
sampler	B
density	I
of	O
standard	B
deviation	I
(	O
cid:27	O
)	O
q	O
(	O
horizontal	O
axis	O
)	O
.	O
the	O
same	O
random	O
number	O
seed	O
was	O
used	O
for	O
all	O
runs	O
.	O
the	O
three	O
plots	O
show	O
(	O
a	O
)	O
the	O
estimated	O
normalizing	B
constant	I
;	O
(	O
b	O
)	O
the	O
empirical	O
standard	B
deviation	I
of	O
the	O
r	O
weights	O
;	O
(	O
c	O
)	O
30	O
of	O
the	O
weights	O
.	O
(	O
29.57	O
)	O
(	O
29.58	O
)	O
(	O
29.59	O
)	O
(	O
29.60	O
)	O
(	O
29.61	O
)	O
(	O
29.62	O
)	O
solution	O
to	O
exercise	O
29.13	O
(	O
p.382	O
)	O
.	O
the	O
weights	O
are	O
w	O
=	O
p	O
(	O
x	O
)	O
=q	O
(	O
x	O
)	O
and	O
x	O
is	O
drawn	O
from	O
q.	O
the	O
mean	B
weight	O
is	O
z	O
dx	O
q	O
(	O
x	O
)	O
[	O
p	O
(	O
x	O
)	O
=q	O
(	O
x	O
)	O
]	O
=z	O
dx	O
p	O
(	O
x	O
)	O
=	O
1	O
;	O
assuming	O
the	O
integral	B
converges	O
.	O
the	O
variance	B
is	O
var	O
(	O
w	O
)	O
=	O
z	O
dx	O
q	O
(	O
x	O
)	O
(	O
cid:20	O
)	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
1	O
(	O
cid:21	O
)	O
2	O
=	O
z	O
dx	O
=	O
(	O
cid:20	O
)	O
z	O
dx	O
p	O
=	O
(	O
cid:27	O
)	O
q=	O
(	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
p	O
(	O
x	O
)	O
2	O
q	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
2p	O
(	O
x	O
)	O
+	O
q	O
(	O
x	O
)	O
2	O
(	O
cid:18	O
)	O
2	O
zq	O
p	O
(	O
cid:0	O
)	O
z	O
2	O
(	O
cid:27	O
)	O
2	O
p	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
x2	O
1	O
(	O
cid:27	O
)	O
2	O
q	O
(	O
cid:19	O
)	O
(	O
cid:19	O
)	O
(	O
cid:21	O
)	O
(	O
cid:0	O
)	O
1	O
;	O
where	O
zq=z	O
2	O
coe	O
(	O
cid:14	O
)	O
cient	O
of	O
x2	O
in	O
the	O
exponent	O
is	O
positive	O
,	O
i.e.	O
,	O
if	O
p	O
)	O
.	O
the	O
integral	B
in	O
(	O
29.60	O
)	O
is	O
(	O
cid:12	O
)	O
nite	O
only	O
if	O
the	O
if	O
this	O
condition	O
is	O
satis	O
(	O
cid:12	O
)	O
ed	O
,	O
the	O
variance	B
is	O
(	O
cid:27	O
)	O
2	O
q	O
>	O
1	O
2	O
(	O
cid:27	O
)	O
2	O
p	O
:	O
var	O
(	O
w	O
)	O
=	O
(	O
cid:27	O
)	O
qp2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
p	O
p2	O
(	O
cid:25	O
)	O
(	O
cid:18	O
)	O
2	O
p	O
(	O
cid:0	O
)	O
(	O
cid:27	O
)	O
2	O
1	O
(	O
cid:27	O
)	O
2	O
q	O
(	O
cid:19	O
)	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
cid:0	O
)	O
1	O
=	O
(	O
cid:27	O
)	O
2	O
q	O
q	O
(	O
cid:0	O
)	O
(	O
cid:27	O
)	O
2	O
p	O
(	O
cid:1	O
)	O
1=2	O
(	O
cid:0	O
)	O
1	O
:	O
(	O
cid:27	O
)	O
p	O
(	O
cid:0	O
)	O
2	O
(	O
cid:27	O
)	O
2	O
as	O
(	O
cid:27	O
)	O
q	O
approaches	O
the	O
critical	O
value	O
{	O
about	O
0:7	O
(	O
cid:27	O
)	O
p	O
{	O
the	O
variance	B
becomes	O
in	O
(	O
cid:12	O
)	O
nite	O
.	O
figure	O
29.20	O
illustrates	O
these	O
phenomena	O
for	O
(	O
cid:27	O
)	O
p	O
=	O
1	O
with	O
(	O
cid:27	O
)	O
q	O
varying	O
from	O
0.1	O
to	O
1.5.	O
the	O
same	O
random	O
number	O
seed	O
was	O
used	O
for	O
all	O
runs	O
,	O
so	O
the	O
weights	O
and	O
estimates	O
follow	O
smooth	O
curves	O
.	O
notice	O
that	O
the	O
empirical	O
standard	B
deviation	I
of	O
the	O
r	O
weights	O
can	O
look	O
quite	O
small	O
and	O
well-behaved	O
(	O
say	O
,	O
at	O
(	O
cid:27	O
)	O
q	O
’	O
0:3	O
)	O
when	O
the	O
true	O
standard	B
deviation	I
is	O
nevertheless	O
in	O
(	O
cid:12	O
)	O
nite	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
30	O
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
this	O
chapter	O
discusses	O
several	O
methods	B
for	O
reducing	O
random	B
walk	I
behaviour	O
in	O
metropolis	O
methods	B
.	O
the	O
aim	O
is	O
to	O
reduce	O
the	O
time	O
required	O
to	O
obtain	O
e	O
(	O
cid:11	O
)	O
ectively	O
independent	O
samples	O
.	O
for	O
brevity	O
,	O
we	O
will	O
say	O
‘	O
independent	O
samples	O
’	O
when	O
we	O
mean	B
‘	O
e	O
(	O
cid:11	O
)	O
ectively	O
independent	O
samples	O
’	O
.	O
30.1	O
hamiltonian	O
monte	O
carlo	O
the	O
hamiltonian	O
monte	O
carlo	O
method	B
is	O
a	O
metropolis	O
method	B
,	O
applicable	O
to	O
continuous	B
state	O
spaces	O
,	O
that	O
makes	O
use	O
of	O
gradient	O
information	B
to	O
reduce	O
random	B
walk	I
behaviour	O
.	O
[	O
the	O
hamiltonian	O
monte	O
carlo	O
method	B
was	O
originally	O
called	O
hybrid	O
monte	O
carlo	O
,	O
for	O
historical	O
reasons	O
.	O
]	O
for	O
many	O
systems	O
whose	O
probability	B
p	O
(	O
x	O
)	O
can	O
be	O
written	O
in	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
e	O
(	O
cid:0	O
)	O
e	O
(	O
x	O
)	O
z	O
;	O
(	O
30.1	O
)	O
not	O
only	O
e	O
(	O
x	O
)	O
but	O
also	O
its	O
gradient	O
with	O
respect	O
to	O
x	O
can	O
be	O
readily	O
evaluated	O
.	O
it	O
seems	O
wasteful	O
to	O
use	O
a	O
simple	O
random-walk	O
metropolis	O
method	B
when	O
this	O
gradient	O
is	O
available	O
{	O
the	O
gradient	O
indicates	O
which	O
direction	O
one	O
should	O
go	O
in	O
to	O
(	O
cid:12	O
)	O
nd	O
states	O
that	O
have	O
higher	O
probability	B
!	O
overview	O
of	O
hamiltonian	O
monte	O
carlo	O
in	O
the	O
hamiltonian	O
monte	O
carlo	O
method	B
,	O
the	O
state	O
space	O
x	O
is	O
augmented	O
by	O
momentum	O
variables	O
p	O
,	O
and	O
there	O
is	O
an	O
alternation	O
of	O
two	O
types	O
of	O
proposal	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
proposal	O
randomizes	O
the	O
momentum	B
variable	O
,	O
leaving	O
the	O
state	O
x	O
un-	O
changed	O
.	O
the	O
second	O
proposal	O
changes	O
both	O
x	O
and	O
p	O
using	O
simulated	O
hamil-	O
tonian	O
dynamics	O
as	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
the	O
hamiltonian	O
h	O
(	O
x	O
;	O
p	O
)	O
=	O
e	O
(	O
x	O
)	O
+	O
k	O
(	O
p	O
)	O
;	O
(	O
30.2	O
)	O
where	O
k	O
(	O
p	O
)	O
is	O
a	O
‘	O
kinetic	O
energy	B
’	O
such	O
as	O
k	O
(	O
p	O
)	O
=	O
ptp=2	O
.	O
these	O
two	O
proposals	O
are	O
used	O
to	O
create	O
(	O
asymptotically	O
)	O
samples	O
from	O
the	O
joint	B
density	O
ph	O
(	O
x	O
;	O
p	O
)	O
=	O
1	O
zh	O
exp	O
[	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
;	O
p	O
)	O
]	O
=	O
1	O
zh	O
exp	O
[	O
(	O
cid:0	O
)	O
e	O
(	O
x	O
)	O
]	O
exp	O
[	O
(	O
cid:0	O
)	O
k	O
(	O
p	O
)	O
]	O
:	O
(	O
30.3	O
)	O
this	O
density	B
is	O
separable	O
,	O
so	O
the	O
marginal	B
distribution	O
of	O
x	O
is	O
the	O
desired	O
distribution	B
exp	O
[	O
(	O
cid:0	O
)	O
e	O
(	O
x	O
)	O
]	O
=z	O
.	O
so	O
,	O
simply	O
discarding	O
the	O
momentum	B
variables	O
,	O
we	O
obtain	O
a	O
sequence	B
of	O
samples	O
fx	O
(	O
t	O
)	O
g	O
that	O
asymptotically	O
come	O
from	O
p	O
(	O
x	O
)	O
.	O
387	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
388	O
30	O
|	O
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
algorithm	O
30.1.	O
octave	B
source	O
code	B
for	O
the	O
hamiltonian	O
monte	O
carlo	O
method	B
.	O
g	O
=	O
grade	O
(	O
x	O
)	O
;	O
e	O
=	O
finde	O
(	O
x	O
)	O
;	O
#	O
set	B
gradient	O
using	O
initial	O
x	O
#	O
set	B
objective	O
function	B
too	O
for	O
l	O
=	O
1	O
:	O
l	O
p	O
=	O
randn	O
(	O
size	O
(	O
x	O
)	O
)	O
;	O
h	O
=	O
p	O
’	O
*	O
p	O
/	O
2	O
+	O
e	O
;	O
#	O
loop	O
l	O
times	O
#	O
initial	O
momentum	B
is	O
normal	B
(	O
0,1	O
)	O
#	O
evaluate	O
h	O
(	O
x	O
,	O
p	O
)	O
xnew	O
=	O
x	O
;	O
for	O
tau	O
=	O
1	O
:	O
tau	O
gnew	O
=	O
g	O
;	O
#	O
make	O
tau	O
‘	O
leapfrog	B
’	O
steps	O
p	O
=	O
p	O
-	O
epsilon	O
*	O
gnew	O
/	O
2	O
;	O
#	O
make	O
half-step	O
in	O
p	O
xnew	O
=	O
xnew	O
+	O
epsilon	O
*	O
p	O
;	O
gnew	O
=	O
grade	O
(	O
xnew	O
)	O
;	O
p	O
=	O
p	O
-	O
epsilon	O
*	O
gnew	O
/	O
2	O
;	O
#	O
make	O
half-step	O
in	O
p	O
#	O
make	O
step	O
in	O
x	O
#	O
find	O
new	O
gradient	O
endfor	O
enew	O
=	O
finde	O
(	O
xnew	O
)	O
;	O
hnew	O
=	O
p	O
’	O
*	O
p	O
/	O
2	O
+	O
enew	O
;	O
dh	O
=	O
hnew	O
-	O
h	O
;	O
#	O
find	O
new	O
value	O
of	O
h	O
#	O
decide	O
whether	O
to	O
accept	O
if	O
(	O
dh	O
<	O
0	O
)	O
accept	O
=	O
1	O
;	O
elseif	O
(	O
rand	O
(	O
)	O
<	O
exp	O
(	O
-dh	O
)	O
)	O
accept	O
=	O
1	O
;	O
else	O
accept	O
=	O
0	O
;	O
endif	O
if	O
(	O
accept	O
)	O
g	O
=	O
gnew	O
;	O
endif	O
endfor	O
x	O
=	O
xnew	O
;	O
e	O
=	O
enew	O
;	O
hamiltonian	O
monte	O
carlo	O
1	O
(	O
a	O
)	O
(	O
c	O
)	O
0.5	O
0	O
-0.5	O
-1	O
1	O
(	O
b	O
)	O
0.5	O
0	O
-0.5	O
-1	O
-1	O
-0.5	O
0	O
0.5	O
1	O
(	O
d	O
)	O
simple	O
metropolis	O
figure	O
30.2	O
.	O
(	O
a	O
,	O
b	O
)	O
hamiltonian	O
monte	O
carlo	O
used	O
to	O
generate	O
samples	O
from	O
a	O
bivariate	O
gaussian	O
with	O
correlation	O
(	O
cid:26	O
)	O
=	O
0:998	O
.	O
(	O
c	O
,	O
d	O
)	O
for	O
comparison	O
,	O
a	O
simple	O
random-walk	O
metropolis	O
method	B
,	O
given	O
equal	O
computer	B
time	O
.	O
-1	O
-0.5	O
0	O
0.5	O
1	O
1	O
0.5	O
0	O
-0.5	O
-1	O
1	O
0.5	O
0	O
-0.5	O
-1	O
-1.5	O
-1.5	O
-1	O
-0.5	O
0	O
0.5	O
1	O
-1	O
-0.5	O
0	O
0.5	O
1	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
389	O
30.1	O
:	O
hamiltonian	O
monte	O
carlo	O
details	O
of	O
hamiltonian	O
monte	O
carlo	O
the	O
(	O
cid:12	O
)	O
rst	O
proposal	O
,	O
which	O
can	O
be	O
viewed	O
as	O
a	O
gibbs	O
sampling	O
update	O
,	O
draws	O
a	O
new	O
momentum	B
from	O
the	O
gaussian	O
density	B
exp	O
[	O
(	O
cid:0	O
)	O
k	O
(	O
p	O
)	O
]	O
=zk	O
.	O
this	O
proposal	O
is	O
always	O
accepted	O
.	O
during	O
the	O
second	O
,	O
dynamical	O
proposal	O
,	O
the	O
momentum	B
vari-	O
able	O
determines	O
where	O
the	O
state	O
x	O
goes	O
,	O
and	O
the	O
gradient	O
of	O
e	O
(	O
x	O
)	O
determines	O
how	O
the	O
momentum	B
p	O
changes	O
,	O
in	O
accordance	O
with	O
the	O
equations	O
_x	O
=	O
p	O
_p	O
=	O
(	O
cid:0	O
)	O
@	O
e	O
(	O
x	O
)	O
@	O
x	O
:	O
(	O
30.4	O
)	O
(	O
30.5	O
)	O
because	O
of	O
the	O
persistent	O
motion	O
of	O
x	O
in	O
the	O
direction	O
of	O
the	O
momentum	O
p	O
during	O
each	O
dynamical	O
proposal	O
,	O
the	O
state	O
of	O
the	O
system	O
tends	O
to	O
move	O
a	O
distance	B
that	O
goes	O
linearly	O
with	O
the	O
computer	B
time	O
,	O
rather	O
than	O
as	O
the	O
square	B
root	O
.	O
the	O
second	O
proposal	O
is	O
accepted	O
in	O
accordance	O
with	O
the	O
metropolis	O
rule	O
.	O
if	O
the	O
simulation	O
of	O
the	O
hamiltonian	O
dynamics	O
is	O
numerically	O
perfect	B
then	O
the	O
proposals	O
are	O
accepted	O
every	O
time	O
,	O
because	O
the	O
total	O
energy	B
h	O
(	O
x	O
;	O
p	O
)	O
is	O
a	O
constant	O
of	O
the	O
motion	O
and	O
so	O
a	O
in	O
equation	O
(	O
29.31	O
)	O
is	O
equal	O
to	O
one	O
.	O
if	O
the	O
simulation	O
is	O
imperfect	O
,	O
because	O
of	O
(	O
cid:12	O
)	O
nite	O
step	O
sizes	O
for	O
example	O
,	O
then	O
some	O
of	O
the	O
dynamical	O
proposals	O
will	O
be	O
rejected	O
.	O
the	O
rejection	B
rule	O
makes	O
use	O
of	O
the	O
change	O
in	O
h	O
(	O
x	O
;	O
p	O
)	O
,	O
which	O
is	O
zero	O
if	O
the	O
simulation	O
is	O
perfect	B
.	O
the	O
occasional	O
rejections	O
ensure	O
that	O
,	O
asymptotically	O
,	O
we	O
obtain	O
samples	O
(	O
x	O
(	O
t	O
)	O
;	O
p	O
(	O
t	O
)	O
)	O
from	O
the	O
required	O
joint	B
density	O
ph	O
(	O
x	O
;	O
p	O
)	O
.	O
the	O
source	B
code	I
in	O
(	O
cid:12	O
)	O
gure	O
30.1	O
describes	O
a	O
hamiltonian	O
monte	O
carlo	O
method	B
that	O
uses	O
the	O
‘	O
leapfrog	B
’	O
algorithm	B
to	O
simulate	O
the	O
dynamics	O
on	O
the	O
function	B
finde	O
(	O
x	O
)	O
,	O
whose	O
gradient	O
is	O
found	O
by	O
the	O
function	B
grade	O
(	O
x	O
)	O
.	O
figure	O
30.2	O
shows	O
this	O
algorithm	B
generating	O
samples	O
from	O
a	O
bivariate	O
gaussian	O
whose	O
en-	O
ergy	O
function	B
is	O
e	O
(	O
x	O
)	O
=	O
1	O
2	O
xtax	O
with	O
a	O
=	O
(	O
cid:20	O
)	O
250:25	O
(	O
cid:21	O
)	O
;	O
250:25	O
(	O
cid:0	O
)	O
249:75	O
(	O
cid:0	O
)	O
249:75	O
corresponding	O
to	O
a	O
variance	B
{	O
covariance	B
matrix	I
of	O
(	O
cid:20	O
)	O
1	O
0:998	O
0:998	O
1	O
(	O
cid:21	O
)	O
:	O
(	O
30.6	O
)	O
(	O
30.7	O
)	O
in	O
(	O
cid:12	O
)	O
gure	O
30.2a	O
,	O
starting	O
from	O
the	O
state	O
marked	O
by	O
the	O
arrow	O
,	O
the	O
solid	O
line	O
represents	O
two	O
successive	O
trajectories	O
generated	O
by	O
the	O
hamiltonian	O
dynamics	O
.	O
the	O
squares	O
show	O
the	O
endpoints	O
of	O
these	O
two	O
trajectories	O
.	O
each	O
trajectory	O
consists	O
of	O
tau	O
=	O
19	O
‘	O
leapfrog	B
’	O
steps	O
with	O
epsilon	O
=	O
0:055.	O
these	O
steps	O
are	O
indicated	O
by	O
the	O
crosses	O
on	O
the	O
trajectory	O
in	O
the	O
magni	O
(	O
cid:12	O
)	O
ed	O
inset	O
.	O
after	O
each	O
trajectory	O
,	O
the	O
momentum	B
is	O
randomized	O
.	O
here	O
,	O
both	O
trajectories	O
are	O
accepted	O
;	O
the	O
errors	B
in	O
the	O
hamiltonian	O
were	O
only	O
+0:016	O
and	O
(	O
cid:0	O
)	O
0:06	O
respectively	O
.	O
figure	O
30.2b	O
shows	O
how	O
a	O
sequence	B
of	O
four	O
trajectories	O
converges	O
from	O
an	O
initial	O
condition	O
,	O
indicated	O
by	O
the	O
arrow	O
,	O
that	O
is	O
not	O
close	O
to	O
the	O
typical	B
set	I
of	O
the	O
target	O
distribution	B
.	O
the	O
trajectory	O
parameters	B
tau	O
and	O
epsilon	O
were	O
randomized	O
for	O
each	O
trajectory	O
using	O
uniform	O
distributions	O
with	O
means	O
19	O
and	O
0.055	O
respectively	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
trajectory	O
takes	O
us	O
to	O
a	O
new	O
state	O
,	O
(	O
(	O
cid:0	O
)	O
1:5	O
;	O
(	O
cid:0	O
)	O
0:5	O
)	O
,	O
similar	O
in	O
energy	O
to	O
the	O
(	O
cid:12	O
)	O
rst	O
state	O
.	O
the	O
second	O
trajectory	O
happens	O
to	O
end	O
in	O
a	O
state	O
nearer	O
the	O
bottom	O
of	O
the	O
energy	O
landscape	O
.	O
here	O
,	O
since	O
the	O
potential	O
energy	B
e	O
is	O
smaller	O
,	O
the	O
kinetic	O
energy	B
k	O
=	O
p2=2	O
is	O
necessarily	O
larger	O
than	O
it	O
was	O
at	O
the	O
start	O
of	O
the	O
trajectory	O
.	O
when	O
the	O
momentum	B
is	O
randomized	O
before	O
the	O
third	O
trajectory	O
,	O
its	O
kinetic	O
energy	B
becomes	O
much	O
smaller	O
.	O
after	O
the	O
fourth	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
390	O
(	O
a	O
)	O
(	O
b	O
)	O
1	O
0.5	O
0	O
-0.5	O
-1	O
gibbs	O
sampling	O
overrelaxation	O
-1	O
-0.5	O
0	O
0.5	O
1	O
1	O
0.5	O
0	O
-0.5	O
-1	O
0	O
-0.2	O
-0.4	O
-0.6	O
-0.8	O
-1	O
-1	O
-0.5	O
0	O
0.5	O
1	O
30	O
|	O
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
figure	O
30.3.	O
overrelaxation	B
contrasted	O
with	O
gibbs	O
sampling	O
for	O
a	O
bivariate	O
gaussian	O
with	O
correlation	O
(	O
cid:26	O
)	O
=	O
0:998	O
.	O
(	O
a	O
)	O
the	O
state	O
sequence	O
for	O
40	O
iterations	O
,	O
each	O
iteration	O
involving	O
one	O
update	O
of	O
both	O
variables	O
.	O
the	O
overrelaxation	B
method	O
had	O
(	O
cid:11	O
)	O
=	O
(	O
cid:0	O
)	O
0:98	O
.	O
(	O
this	O
excessively	O
large	O
value	O
is	O
chosen	O
to	O
make	O
it	O
easy	O
to	O
see	O
how	O
the	O
overrelaxation	B
method	O
reduces	O
random	B
walk	I
behaviour	O
.	O
)	O
the	O
dotted	O
line	O
shows	O
the	O
contour	O
xt	O
(	O
cid:6	O
)	O
(	O
cid:0	O
)	O
1x	O
=	O
1	O
.	O
(	O
b	O
)	O
detail	O
of	O
(	O
a	O
)	O
,	O
showing	O
the	O
two	O
steps	O
making	O
up	O
each	O
iteration	O
.	O
(	O
c	O
)	O
time-course	O
of	O
the	O
variable	O
x1	O
during	O
2000	O
iterations	O
of	O
the	O
two	O
methods	B
.	O
the	O
overrelaxation	B
method	O
had	O
(	O
cid:11	O
)	O
=	O
(	O
cid:0	O
)	O
0:89	O
.	O
(	O
after	O
neal	O
(	O
1995	O
)	O
.	O
)	O
(	O
c	O
)	O
gibbs	O
sampling	O
-1	O
-0.8-0.6-0.4-0.2	O
0	O
3	O
2	O
1	O
0	O
-1	O
-2	O
-3	O
3	O
2	O
1	O
0	O
-1	O
-2	O
-3	O
200	O
400	O
0	O
overrelaxation	B
600	O
800	O
1000	O
1200	O
1400	O
1600	O
1800	O
2000	O
0	O
200	O
400	O
600	O
800	O
1000	O
1200	O
1400	O
1600	O
1800	O
2000	O
trajectory	O
has	O
been	O
simulated	O
,	O
the	O
state	O
appears	O
to	O
have	O
become	O
typical	B
of	O
the	O
target	O
density	B
.	O
figures	O
30.2	O
(	O
c	O
)	O
and	O
(	O
d	O
)	O
show	O
a	O
random-walk	O
metropolis	O
method	B
using	O
a	O
gaussian	O
proposal	B
density	I
to	O
sample	B
from	I
the	O
same	O
gaussian	O
distribution	B
,	O
starting	O
from	O
the	O
initial	O
conditions	O
of	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
respectively	O
.	O
in	O
(	O
c	O
)	O
the	O
step	O
size	O
was	O
adjusted	O
such	O
that	O
the	O
acceptance	B
rate	I
was	O
58	O
%	O
.	O
the	O
number	O
of	O
proposals	O
was	O
38	O
so	O
the	O
total	O
amount	O
of	O
computer	O
time	O
used	O
was	O
similar	O
to	O
that	O
in	O
(	O
a	O
)	O
.	O
the	O
distance	B
moved	O
is	O
small	O
because	O
of	O
random	O
walk	O
behaviour	O
.	O
in	O
(	O
d	O
)	O
the	O
random-walk	O
metropolis	O
method	B
was	O
used	O
and	O
started	O
from	O
the	O
same	O
initial	O
condition	O
as	O
(	O
b	O
)	O
and	O
given	O
a	O
similar	O
amount	O
of	O
computer	O
time	O
.	O
30.2	O
overrelaxation	B
the	O
method	B
of	O
overrelaxation	B
is	O
a	O
method	B
for	O
reducing	O
random	B
walk	I
behaviour	O
in	O
gibbs	O
sampling	O
.	O
overrelaxation	B
was	O
originally	O
introduced	O
for	O
systems	O
in	O
which	O
all	O
the	O
conditional	B
distributions	O
are	O
gaussian	O
.	O
an	O
example	O
of	O
a	O
joint	B
distribution	O
that	O
is	O
not	O
gaussian	O
but	O
whose	O
conditional	B
distributions	O
are	O
all	O
gaussian	O
is	O
p	O
(	O
x	O
;	O
y	O
)	O
=	O
exp	O
(	O
(	O
cid:0	O
)	O
x2y2	O
(	O
cid:0	O
)	O
x2	O
(	O
cid:0	O
)	O
y2	O
)	O
=z	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
30.2	O
:	O
overrelaxation	B
391	O
overrelaxation	B
for	O
gaussian	O
conditional	B
distributions	O
in	O
ordinary	O
gibbs	O
sampling	O
,	O
one	O
draws	O
the	O
new	O
value	O
x	O
(	O
t+1	O
)	O
of	O
the	O
current	O
variable	O
xi	O
from	O
its	O
conditional	B
distribution	O
,	O
ignoring	O
the	O
old	O
value	O
x	O
(	O
t	O
)	O
.	O
the	O
i	O
state	O
makes	O
lengthy	O
random	B
walks	O
in	O
cases	O
where	O
the	O
variables	O
are	O
strongly	O
correlated	O
,	O
as	O
illustrated	O
in	O
the	O
left-hand	O
panel	O
of	O
(	O
cid:12	O
)	O
gure	O
30.3.	O
this	O
(	O
cid:12	O
)	O
gure	O
uses	O
a	O
correlated	O
gaussian	O
distribution	B
as	O
the	O
target	O
density	B
.	O
i	O
in	O
adler	O
’	O
s	O
(	O
1981	O
)	O
overrelaxation	B
method	O
,	O
one	O
instead	O
samples	O
x	O
(	O
t+1	O
)	O
from	O
a	O
gaussian	O
that	O
is	O
biased	O
to	O
the	O
opposite	O
side	O
of	O
the	O
conditional	O
distribution	B
.	O
if	O
the	O
conditional	B
distribution	O
of	O
xi	O
is	O
normal	B
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
)	O
and	O
the	O
current	O
value	O
of	O
xi	O
is	O
x	O
(	O
t	O
)	O
i	O
,	O
then	O
adler	O
’	O
s	O
method	B
sets	O
xi	O
to	O
i	O
x	O
(	O
t+1	O
)	O
i	O
=	O
(	O
cid:22	O
)	O
+	O
(	O
cid:11	O
)	O
(	O
x	O
(	O
t	O
)	O
(	O
30.8	O
)	O
where	O
(	O
cid:23	O
)	O
(	O
cid:24	O
)	O
normal	B
(	O
0	O
;	O
1	O
)	O
and	O
(	O
cid:11	O
)	O
is	O
a	O
parameter	O
between	O
(	O
cid:0	O
)	O
1	O
and	O
1	O
,	O
usually	O
set	B
to	O
a	O
negative	O
value	O
.	O
(	O
if	O
(	O
cid:11	O
)	O
is	O
positive	O
,	O
then	O
the	O
method	B
is	O
called	O
under-relaxation	O
.	O
)	O
exercise	O
30.1	O
.	O
[	O
2	O
]	O
show	O
that	O
this	O
individual	O
transition	B
leaves	O
invariant	O
the	O
con-	O
i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
)	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
2	O
)	O
1=2	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
;	O
ditional	O
distribution	B
xi	O
(	O
cid:24	O
)	O
normal	B
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
)	O
.	O
a	O
single	O
iteration	O
of	O
adler	O
’	O
s	O
overrelaxation	B
,	O
like	O
one	O
of	O
gibbs	O
sampling	O
,	O
updates	O
each	O
variable	O
in	O
turn	O
as	O
indicated	O
in	O
equation	O
(	O
30.8	O
)	O
.	O
the	O
transition	B
matrix	O
t	O
(	O
x0	O
;	O
x	O
)	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
a	O
complete	O
update	O
of	O
all	O
variables	O
in	O
some	O
(	O
cid:12	O
)	O
xed	O
order	O
does	O
not	O
satisfy	O
detailed	B
balance	I
.	O
each	O
individual	O
transition	B
for	O
one	O
coordinate	O
just	O
described	O
does	O
satisfy	O
detailed	B
balance	I
{	O
so	O
the	O
overall	O
chain	B
gives	O
a	O
valid	O
sampling	O
strategy	O
which	O
converges	O
to	O
the	O
target	O
density	B
p	O
(	O
x	O
)	O
{	O
but	O
when	O
we	O
form	O
a	O
chain	B
by	O
applying	O
the	O
individual	O
transitions	O
in	O
a	O
(	O
cid:12	O
)	O
xed	O
sequence	B
,	O
the	O
overall	O
chain	B
is	O
not	O
reversible	B
.	O
this	O
temporal	O
asymmetry	O
is	O
the	O
key	O
to	O
why	O
overrelaxation	B
can	O
be	O
bene	O
(	O
cid:12	O
)	O
cial	O
.	O
if	O
,	O
say	O
,	O
two	O
variables	O
are	O
positively	O
correlated	O
,	O
then	O
they	O
will	O
(	O
on	O
a	O
short	O
timescale	O
)	O
evolve	O
in	O
a	O
directed	O
manner	O
instead	O
of	O
by	O
random	B
walk	I
,	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
30.3.	O
this	O
may	O
signi	O
(	O
cid:12	O
)	O
cantly	O
reduce	O
the	O
time	O
required	O
to	O
obtain	O
independent	O
samples	O
.	O
exercise	O
30.2	O
.	O
[	O
3	O
]	O
the	O
transition	B
matrix	O
t	O
(	O
x0	O
;	O
x	O
)	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
a	O
complete	O
update	O
of	O
all	O
variables	O
in	O
some	O
(	O
cid:12	O
)	O
xed	O
order	O
does	O
not	O
satisfy	O
detailed	B
balance	I
.	O
if	O
the	O
updates	O
were	O
in	O
a	O
random	B
order	O
,	O
then	O
t	O
would	O
be	O
symmetric	B
.	O
inves-	O
tigate	O
,	O
for	O
the	O
toy	O
two-dimensional	B
gaussian	O
distribution	B
,	O
the	O
assertion	O
that	O
the	O
advantages	O
of	O
overrelaxation	O
are	O
lost	O
if	O
the	O
overrelaxed	O
updates	O
are	O
made	O
in	O
a	O
random	B
order	O
.	O
ordered	B
overrelaxation	I
the	O
overrelaxation	B
method	O
has	O
been	O
generalized	B
by	O
neal	O
(	O
1995	O
)	O
whose	O
ordered	B
overrelaxation	I
method	O
is	O
applicable	O
to	O
any	O
system	O
where	O
gibbs	O
sampling	O
is	O
used	O
.	O
in	O
ordered	O
overrelaxation	B
,	O
instead	O
of	O
taking	O
one	O
sample	B
from	I
the	O
condi-	O
tional	O
distribution	B
p	O
(	O
xi	O
jfxjgj6=i	O
)	O
,	O
we	O
create	O
k	O
such	O
samples	O
x	O
(	O
1	O
)	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
k	O
)	O
where	O
k	O
might	O
be	O
set	B
to	O
twenty	O
or	O
so	O
.	O
often	O
,	O
generating	O
k	O
(	O
cid:0	O
)	O
1	O
extra	O
samples	O
adds	O
a	O
negligible	O
computational	O
cost	O
to	O
the	O
initial	O
computations	O
required	O
for	O
making	O
the	O
(	O
cid:12	O
)	O
rst	O
sample	B
.	O
the	O
points	O
fx	O
(	O
k	O
)	O
i	O
g	O
are	O
then	O
sorted	O
numerically	O
,	O
and	O
the	O
current	O
value	O
of	O
xi	O
is	O
inserted	O
into	O
the	O
sorted	O
list	O
,	O
giving	O
a	O
list	O
of	O
k	O
+	O
1	O
points	O
.	O
we	O
give	O
them	O
ranks	O
0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
k.	O
let	O
(	O
cid:20	O
)	O
be	O
the	O
rank	O
of	O
the	O
current	O
value	O
of	O
xi	O
in	O
the	O
list	O
.	O
we	O
set	B
x0i	O
to	O
the	O
value	O
that	O
is	O
an	O
equal	O
distance	B
from	O
the	O
other	O
end	O
of	O
the	O
list	O
,	O
that	O
is	O
,	O
the	O
value	O
with	O
rank	O
k	O
(	O
cid:0	O
)	O
(	O
cid:20	O
)	O
.	O
the	O
role	O
played	O
by	O
adler	O
’	O
s	O
(	O
cid:11	O
)	O
parameter	O
is	O
here	O
played	O
by	O
the	O
parameter	O
k.	O
when	O
k	O
=	O
1	O
,	O
we	O
obtain	O
ordinary	O
gibbs	O
sampling	O
.	O
for	O
practical	O
purposes	O
neal	O
estimates	O
that	O
ordered	B
overrelaxation	I
may	O
speed	O
up	O
a	O
simulation	O
by	O
a	O
factor	O
of	O
ten	O
or	O
twenty	O
.	O
;	O
x	O
(	O
2	O
)	O
i	O
i	O
i	O
,	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
392	O
30	O
|	O
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
30.3	O
simulated	B
annealing	I
a	O
third	O
technique	O
for	O
speeding	O
convergence	O
is	O
simulated	B
annealing	I
.	O
in	O
simu-	O
lated	O
annealing	B
,	O
a	O
‘	O
temperature	B
’	O
parameter	O
is	O
introduced	O
which	O
,	O
when	O
large	O
,	O
allows	O
the	O
system	O
to	O
make	O
transitions	O
that	O
would	O
be	O
improbable	O
at	O
temper-	O
ature	O
1.	O
the	O
temperature	B
is	O
set	B
to	O
a	O
large	O
value	O
and	O
gradually	O
reduced	O
to	O
1.	O
this	O
procedure	O
is	O
supposed	O
to	O
reduce	O
the	O
chance	O
that	O
the	O
simulation	O
gets	O
stuck	O
in	O
an	O
unrepresentative	O
probability	B
island	O
.	O
we	O
asssume	O
that	O
we	O
wish	O
to	O
sample	B
from	I
a	O
distribution	B
of	O
the	O
form	O
p	O
(	O
x	O
)	O
=	O
e	O
(	O
cid:0	O
)	O
e	O
(	O
x	O
)	O
z	O
(	O
30.9	O
)	O
where	O
e	O
(	O
x	O
)	O
can	O
be	O
evaluated	O
.	O
in	O
the	O
simplest	O
simulated	B
annealing	I
method	O
,	O
we	O
instead	O
sample	B
from	I
the	O
distribution	B
pt	O
(	O
x	O
)	O
=	O
1	O
z	O
(	O
t	O
)	O
e	O
(	O
cid:0	O
)	O
e	O
(	O
x	O
)	O
t	O
(	O
30.10	O
)	O
and	O
decrease	O
t	O
gradually	O
to	O
1.	O
often	O
the	O
energy	B
function	O
can	O
be	O
separated	O
into	O
two	O
terms	O
,	O
e	O
(	O
x	O
)	O
=	O
e0	O
(	O
x	O
)	O
+	O
e1	O
(	O
x	O
)	O
;	O
(	O
30.11	O
)	O
of	O
which	O
the	O
(	O
cid:12	O
)	O
rst	O
term	O
is	O
‘	O
nice	O
’	O
(	O
for	O
example	O
,	O
a	O
separable	O
function	B
of	O
x	O
)	O
and	O
the	O
second	O
is	O
‘	O
nasty	O
’	O
.	O
in	O
these	O
cases	O
,	O
a	O
better	O
simulated	B
annealing	I
method	O
might	O
make	O
use	O
of	O
the	O
distribution	O
p	O
0t	O
(	O
x	O
)	O
=	O
1	O
z	O
0	O
(	O
t	O
)	O
e	O
(	O
cid:0	O
)	O
e0	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
e1	O
(	O
x	O
)	O
/t	O
(	O
30.12	O
)	O
with	O
t	O
gradually	O
decreasing	O
to	O
1.	O
in	O
this	O
way	O
,	O
the	O
distribution	B
at	O
high	O
tem-	O
peratures	O
reverts	O
to	O
a	O
well-behaved	O
distribution	B
de	O
(	O
cid:12	O
)	O
ned	O
by	O
e0	O
.	O
simulated	B
annealing	I
is	O
often	O
used	O
as	O
an	O
optimization	B
method	O
,	O
where	O
the	O
aim	O
is	O
to	O
(	O
cid:12	O
)	O
nd	O
an	O
x	O
that	O
minimizes	O
e	O
(	O
x	O
)	O
,	O
in	O
which	O
case	O
the	O
temperature	B
is	O
decreased	O
to	O
zero	O
rather	O
than	O
to	O
1.	O
as	O
a	O
monte	O
carlo	O
method	B
,	O
simulated	B
annealing	I
as	O
described	O
above	O
doesn	O
’	O
t	O
sample	B
exactly	O
from	O
the	O
right	O
distribution	B
,	O
because	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
probability	O
of	O
falling	O
into	O
one	O
basin	O
of	O
the	O
energy	O
is	O
equal	O
to	O
the	O
total	O
prob-	O
ability	O
of	O
all	O
the	O
states	O
in	O
that	O
basin	O
.	O
the	O
closely	O
related	O
‘	O
simulated	O
tempering	O
’	O
method	B
(	O
marinari	O
and	O
parisi	O
,	O
1992	O
)	O
corrects	O
the	O
biases	O
introduced	O
by	O
the	O
an-	O
nealing	O
process	O
by	O
making	O
the	O
temperature	B
itself	O
a	O
random	B
variable	I
that	O
is	O
updated	O
in	O
metropolis	O
fashion	O
during	O
the	O
simulation	O
.	O
neal	O
’	O
s	O
(	O
1998	O
)	O
‘	O
annealed	B
importance	I
sampling	I
’	O
method	B
removes	O
the	O
biases	O
introduced	O
by	O
annealing	O
by	O
computing	O
importance	O
weights	O
for	O
each	O
generated	O
point	O
.	O
30.4	O
skilling	O
’	O
s	O
multi-state	B
leapfrog	O
method	B
a	O
fourth	O
method	B
for	O
speeding	O
up	O
monte	O
carlo	O
simulations	O
,	O
due	O
to	O
john	O
skilling	O
,	O
has	O
a	O
similar	O
spirit	O
to	O
overrelaxation	B
,	O
but	O
works	O
in	O
more	O
dimensions	B
.	O
this	O
method	B
is	O
applicable	O
to	O
sampling	O
from	O
a	O
distribution	B
over	O
a	O
continuous	B
state	O
space	O
,	O
and	O
the	O
sole	O
requirement	O
is	O
that	O
the	O
energy	B
e	O
(	O
x	O
)	O
should	O
be	O
easy	O
to	O
evaluate	O
.	O
the	O
gradient	O
is	O
not	O
used	O
.	O
this	O
leapfrog	B
method	O
is	O
not	O
intended	O
to	O
be	O
used	O
on	O
its	O
own	O
but	O
rather	O
in	O
sequence	O
with	O
other	O
monte	O
carlo	O
operators	O
.	O
instead	O
of	O
moving	O
just	O
one	O
state	O
vector	O
x	O
around	O
the	O
state	O
space	O
,	O
as	O
was	O
the	O
case	O
for	O
all	O
the	O
monte	O
carlo	O
methods	B
discussed	O
thus	O
far	O
,	O
skilling	O
’	O
s	O
leapfrog	B
method	O
simultaneously	O
maintains	O
a	O
set	B
of	O
s	O
state	O
vectors	O
fx	O
(	O
s	O
)	O
g	O
,	O
where	O
s	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
30.4	O
:	O
skilling	O
’	O
s	O
multi-state	B
leapfrog	O
method	B
393	O
might	O
be	O
six	B
or	O
twelve	O
.	O
the	O
aim	O
is	O
that	O
all	O
s	O
of	O
these	O
vectors	B
will	O
represent	O
independent	O
samples	O
from	O
the	O
same	O
distribution	B
p	O
(	O
x	O
)	O
.	O
skilling	O
’	O
s	O
leapfrog	B
makes	O
a	O
proposal	O
for	O
the	O
new	O
state	O
x	O
(	O
s	O
)	O
0	O
,	O
which	O
is	O
ac-	O
cepted	O
or	O
rejected	O
in	O
accordance	O
with	O
the	O
metropolis	O
method	B
,	O
by	O
leapfrogging	O
the	O
current	O
state	O
x	O
(	O
s	O
)	O
over	O
another	O
state	O
vector	O
x	O
(	O
t	O
)	O
:	O
x	O
(	O
s	O
)	O
0	O
=	O
x	O
(	O
t	O
)	O
+	O
(	O
x	O
(	O
t	O
)	O
(	O
cid:0	O
)	O
x	O
(	O
s	O
)	O
)	O
=	O
2x	O
(	O
t	O
)	O
(	O
cid:0	O
)	O
x	O
(	O
s	O
)	O
:	O
(	O
30.13	O
)	O
all	O
the	O
other	O
state	O
vectors	O
are	O
left	O
where	O
they	O
are	O
,	O
so	O
the	O
acceptance	O
probability	O
depends	O
only	O
on	O
the	O
change	O
in	O
energy	O
of	O
x	O
(	O
s	O
)	O
.	O
which	O
vector	O
,	O
t	O
,	O
is	O
the	O
partner	O
for	O
the	O
leapfrog	B
event	O
can	O
be	O
chosen	O
in	O
various	O
ways	O
.	O
the	O
simplest	O
method	B
is	O
to	O
select	O
the	O
partner	O
at	O
random	B
from	O
the	O
other	O
vectors	B
.	O
it	O
might	O
be	O
better	O
to	O
choose	O
t	O
by	O
selecting	O
one	O
of	O
the	O
nearest	O
neighbours	O
x	O
(	O
s	O
)	O
{	O
nearest	O
by	O
any	O
chosen	O
distance	B
function	O
{	O
as	O
long	O
as	O
one	O
then	O
uses	O
an	O
acceptance	O
rule	O
that	O
ensures	O
detailed	B
balance	I
by	O
checking	O
whether	O
point	O
t	O
is	O
still	O
among	O
the	O
nearest	O
neighbours	O
of	O
the	O
new	O
point	O
,	O
x	O
(	O
s	O
)	O
0.	O
x	O
(	O
s	O
)	O
x	O
(	O
s	O
)	O
0	O
x	O
(	O
t	O
)	O
why	O
the	O
leapfrog	B
is	O
a	O
good	B
idea	O
imagine	O
that	O
the	O
target	O
density	B
p	O
(	O
x	O
)	O
has	O
strong	O
correlations	B
{	O
for	O
example	O
,	O
the	O
density	B
might	O
be	O
a	O
needle-like	O
gaussian	O
with	O
width	O
(	O
cid:15	O
)	O
and	O
length	O
l	O
(	O
cid:15	O
)	O
,	O
where	O
l	O
(	O
cid:29	O
)	O
1.	O
as	O
we	O
have	O
emphasized	O
,	O
motion	O
around	O
such	O
a	O
density	B
by	O
standard	O
methods	O
proceeds	O
by	O
a	O
slow	O
random	B
walk	I
.	O
imagine	O
now	O
that	O
our	O
set	B
of	O
s	O
points	O
is	O
lurking	O
initially	O
in	O
a	O
location	O
that	O
is	O
probable	O
under	O
the	O
density	B
,	O
but	O
in	O
an	O
inappropriately	O
small	O
ball	O
of	O
size	O
(	O
cid:15	O
)	O
.	O
now	O
,	O
under	O
skilling	O
’	O
s	O
leapfrog	B
method	O
,	O
a	O
typical	B
(	O
cid:12	O
)	O
rst	O
move	O
will	O
take	O
the	O
point	O
a	O
little	O
outside	O
the	O
current	O
ball	O
,	O
perhaps	O
doubling	O
its	O
distance	B
from	O
the	O
centre	O
of	O
the	O
ball	O
.	O
after	O
all	O
the	O
points	O
have	O
had	O
a	O
chance	O
to	O
move	O
,	O
the	O
ball	O
will	O
have	O
increased	O
in	O
size	O
;	O
if	O
all	O
the	O
moves	O
are	O
accepted	O
,	O
the	O
ball	O
will	O
be	O
bigger	O
by	O
a	O
factor	O
of	O
two	O
or	O
so	O
in	O
all	O
dimensions	B
.	O
the	O
rejection	B
of	O
some	O
moves	O
will	O
mean	B
that	O
the	O
ball	O
containing	O
the	O
points	O
will	O
probably	O
have	O
elongated	O
in	O
the	O
needle	B
’	O
s	O
long	O
direction	O
by	O
a	O
factor	O
of	O
,	O
say	O
,	O
two	O
.	O
after	O
another	O
cycle	O
through	O
the	O
points	O
,	O
the	O
ball	O
will	O
have	O
grown	O
in	O
the	O
long	O
direction	O
by	O
another	O
factor	O
of	O
two	O
.	O
so	O
the	O
typical	B
distance	O
travelled	O
in	O
the	O
long	O
dimension	O
grows	O
exponentially	O
with	O
the	O
number	O
of	O
iterations	O
.	O
now	O
,	O
maybe	O
a	O
factor	O
of	O
two	O
growth	O
per	O
iteration	O
is	O
on	O
the	O
optimistic	O
side	O
;	O
but	O
even	O
if	O
the	O
ball	O
only	O
grows	O
by	O
a	O
factor	O
of	O
,	O
let	O
’	O
s	O
say	O
,	O
1.1	O
per	O
iteration	O
,	O
the	O
growth	O
is	O
nevertheless	O
exponential	B
.	O
it	O
will	O
only	O
take	O
a	O
number	O
of	O
iterations	O
proportional	O
to	O
log	O
l=	O
log	O
(	O
1:1	O
)	O
for	O
the	O
long	O
dimension	O
to	O
be	O
explored	O
.	O
.	O
exercise	O
30.3	O
.	O
[	O
2	O
,	O
p.398	O
]	O
discuss	O
how	O
the	O
e	O
(	O
cid:11	O
)	O
ectiveness	O
of	O
skilling	O
’	O
s	O
method	B
scales	O
with	O
dimensionality	O
,	O
using	O
a	O
correlated	O
n	O
-dimensional	O
gaussian	O
distri-	O
bution	O
as	O
an	O
example	O
.	O
find	O
an	O
expression	O
for	O
the	O
rejection	B
probability	O
,	O
assuming	O
the	O
markov	O
chain	B
is	O
at	O
equilibrium	O
.	O
also	O
discuss	O
how	O
it	O
scales	O
with	O
the	O
strength	O
of	O
correlation	O
among	O
the	O
gaussian	O
variables	O
.	O
[	O
hint	O
:	O
skilling	O
’	O
s	O
method	B
is	O
invariant	O
under	O
a	O
(	O
cid:14	O
)	O
ne	O
transformations	O
,	O
so	O
the	O
rejec-	O
tion	O
probability	B
at	O
equilibrium	O
can	O
be	O
found	O
by	O
looking	O
at	O
the	O
case	O
of	O
a	O
separable	O
gaussian	O
.	O
]	O
this	O
method	B
has	O
some	O
similarity	O
to	O
the	O
‘	O
adaptive	B
direction	I
sampling	I
’	O
method	B
of	O
gilks	O
et	O
al	O
.	O
(	O
1994	O
)	O
but	O
the	O
leapfrog	B
method	O
is	O
simpler	O
and	O
can	O
be	O
applied	O
to	O
a	O
greater	O
variety	O
of	O
distributions	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
394	O
30	O
|	O
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
30.5	O
monte	O
carlo	O
algorithms	B
as	O
communication	B
channels	O
it	O
may	O
be	O
a	O
helpful	O
perspective	O
,	O
when	O
thinking	O
about	O
speeding	O
up	O
monte	O
carlo	O
methods	B
,	O
to	O
think	O
about	O
the	O
information	B
that	O
is	O
being	O
communicated	O
.	O
two	O
communications	O
take	O
place	O
when	O
a	O
sample	B
from	I
p	O
(	O
x	O
)	O
is	O
being	O
generated	O
.	O
first	O
,	O
the	O
selection	O
of	O
a	O
particular	O
x	O
from	O
p	O
(	O
x	O
)	O
necessarily	O
requires	O
that	O
at	O
least	O
log	O
1=p	O
(	O
x	O
)	O
random	B
bits	O
be	O
consumed	O
.	O
[	O
recall	O
the	O
use	O
of	O
inverse	O
arith-	O
metic	O
coding	O
as	O
a	O
method	B
for	O
generating	O
samples	O
from	O
given	O
distributions	O
(	O
section	B
6.3	O
)	O
.	O
]	O
second	O
,	O
the	O
generation	O
of	O
a	O
sample	B
conveys	O
information	B
about	O
p	O
(	O
x	O
)	O
from	O
the	O
subroutine	O
that	O
is	O
able	O
to	O
evaluate	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
(	O
and	O
from	O
any	O
other	O
subroutines	O
that	O
have	O
access	O
to	O
properties	O
of	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
)	O
.	O
consider	O
a	O
dumb	O
metropolis	O
method	B
,	O
for	O
example	O
.	O
in	O
a	O
dumb	O
metropolis	O
method	B
,	O
the	O
proposals	O
q	O
(	O
x0	O
;	O
x	O
)	O
have	O
nothing	O
to	O
do	O
with	O
p	O
(	O
x	O
)	O
.	O
properties	O
of	O
p	O
(	O
x	O
)	O
are	O
only	O
involved	O
in	O
the	O
algorithm	B
at	O
the	O
acceptance	O
step	O
,	O
when	O
the	O
ratio	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
=p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
is	O
computed	O
.	O
the	O
channel	B
from	O
the	O
true	O
distribution	B
p	O
(	O
x	O
)	O
to	O
the	O
user	O
who	O
is	O
interested	O
in	O
computing	O
properties	O
of	O
p	O
(	O
x	O
)	O
thus	O
passes	O
through	O
a	O
bottleneck	O
:	O
all	O
the	O
information	B
about	O
p	O
is	O
conveyed	O
by	O
the	O
string	O
of	O
acceptances	O
and	O
rejections	O
.	O
if	O
p	O
(	O
x	O
)	O
were	O
replaced	O
by	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
distribution	B
p2	O
(	O
x	O
)	O
,	O
the	O
only	O
way	O
in	O
which	O
this	O
change	O
would	O
have	O
an	O
in	O
(	O
cid:13	O
)	O
uence	O
is	O
that	O
the	O
string	O
of	O
acceptances	O
and	O
rejections	O
would	O
be	O
changed	O
.	O
i	O
am	O
not	O
aware	O
of	O
much	O
use	O
being	O
made	O
of	O
this	O
information-theoretic	O
view	O
of	O
monte	O
carlo	O
algorithms	B
,	O
but	O
i	O
think	O
it	O
is	O
an	O
instructive	O
viewpoint	O
:	O
if	O
the	O
aim	O
is	O
to	O
obtain	O
information	B
about	O
properties	O
of	O
p	O
(	O
x	O
)	O
then	O
presumably	O
it	O
is	O
helpful	O
to	O
identify	O
the	O
channel	B
through	O
which	O
this	O
information	B
(	O
cid:13	O
)	O
ows	O
,	O
and	O
maximize	O
the	O
rate	B
of	O
information	B
transfer	O
.	O
example	O
30.4.	O
the	O
information-theoretic	O
viewpoint	O
o	O
(	O
cid:11	O
)	O
ers	O
a	O
simple	O
justi	O
(	O
cid:12	O
)	O
cation	O
for	O
the	O
widely-adopted	O
rule	B
of	I
thumb	I
,	O
which	O
states	O
that	O
the	O
parameters	B
of	O
a	O
dumb	O
metropolis	O
method	B
should	O
be	O
adjusted	O
such	O
that	O
the	O
acceptance	B
rate	I
is	O
about	O
one	O
half	O
.	O
let	O
’	O
s	O
call	O
the	O
acceptance	O
history	O
,	O
that	O
is	O
,	O
the	O
binary	O
string	O
of	O
accept	O
or	O
reject	O
decisions	O
,	O
a.	O
the	O
information	B
learned	O
about	O
p	O
(	O
x	O
)	O
after	O
the	O
algorithm	B
has	O
run	O
for	O
t	O
steps	O
is	O
less	O
than	O
or	O
equal	O
to	O
the	O
information	B
content	I
of	O
a	O
,	O
since	O
all	O
information	B
about	O
p	O
is	O
mediated	O
by	O
a.	O
and	O
the	O
information	B
content	I
of	O
a	O
is	O
upper-bounded	O
by	O
t	O
h2	O
(	O
f	O
)	O
,	O
where	O
f	O
is	O
the	O
acceptance	B
rate	I
.	O
this	O
bound	B
on	O
information	B
acquired	O
about	O
p	O
is	O
maximized	O
by	O
setting	O
f	O
=	O
1=2	O
.	O
another	O
helpful	O
analogy	O
for	O
a	O
dumb	O
metropolis	O
method	B
is	O
an	O
evolutionary	O
one	O
.	O
each	O
proposal	O
generates	O
a	O
progeny	O
x0	O
from	O
the	O
current	O
state	O
x.	O
these	O
two	O
individuals	O
then	O
compete	O
with	O
each	O
other	O
,	O
and	O
the	O
metropolis	O
method	B
uses	O
a	O
noisy	B
survival-of-the-	O
(	O
cid:12	O
)	O
ttest	O
rule	O
.	O
if	O
the	O
progeny	O
x0	O
is	O
(	O
cid:12	O
)	O
tter	O
than	O
the	O
parent	B
(	O
i.e.	O
,	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
>	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
,	O
assuming	O
the	O
q=q	O
factor	O
is	O
unity	O
)	O
then	O
the	O
progeny	O
replaces	O
the	O
parent	B
.	O
the	O
survival	O
rule	O
also	O
allows	O
less-	O
(	O
cid:12	O
)	O
t	O
progeny	O
to	O
replace	O
the	O
parent	B
,	O
sometimes	O
.	O
insights	O
about	O
the	O
rate	B
of	O
evolution	B
can	O
thus	O
be	O
applied	O
to	O
monte	O
carlo	O
methods	B
.	O
exercise	O
30.5	O
.	O
[	O
3	O
]	O
let	O
x	O
2	O
f0	O
;	O
1gg	O
and	O
let	O
p	O
(	O
x	O
)	O
be	O
a	O
separable	O
distribution	B
,	O
p	O
(	O
x	O
)	O
=yg	O
p	O
(	O
xg	O
)	O
;	O
(	O
30.14	O
)	O
with	O
p	O
(	O
0	O
)	O
=	O
p0	O
and	O
p	O
(	O
1	O
)	O
=	O
p1	O
,	O
for	O
example	O
p1	O
=	O
0:1.	O
let	O
the	O
proposal	B
density	I
of	O
a	O
dumb	O
metropolis	O
algorithm	B
q	O
involve	O
(	O
cid:13	O
)	O
ipping	O
a	O
fraction	O
m	O
of	O
the	O
g	O
bits	O
in	O
the	O
state	O
x.	O
analyze	O
how	O
long	O
it	O
takes	O
for	O
the	O
chain	B
to	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
30.6	O
:	O
multi-state	B
methods	O
395	O
converge	O
to	O
the	O
target	O
density	B
as	O
a	O
function	B
of	O
m.	O
find	O
the	O
optimal	B
m	O
and	O
deduce	O
how	O
long	O
the	O
metropolis	O
method	B
must	O
run	O
for	O
.	O
compare	O
the	O
result	O
with	O
the	O
results	O
for	O
an	O
evolving	O
population	O
under	O
natural	B
selection	I
found	O
in	O
chapter	O
19.	O
the	O
insight	O
that	O
the	O
fastest	O
progress	O
that	O
a	O
standard	O
metropolis	O
method	B
can	O
make	O
,	O
in	O
information	O
terms	O
,	O
is	O
about	O
one	O
bit	B
per	O
iteration	O
,	O
gives	O
a	O
strong	O
motivation	O
for	O
speeding	O
up	O
the	O
algorithm	B
.	O
this	O
chapter	O
has	O
already	O
reviewed	O
several	O
methods	B
for	O
reducing	O
random-walk	O
behaviour	O
.	O
do	O
these	O
methods	B
also	O
speed	O
up	O
the	O
rate	B
at	O
which	O
information	B
is	O
acquired	O
?	O
exercise	O
30.6	O
.	O
[	O
4	O
]	O
does	O
gibbs	O
sampling	O
,	O
which	O
is	O
a	O
smart	O
metropolis	O
method	B
whose	O
proposal	O
distributions	O
do	O
depend	O
on	O
p	O
(	O
x	O
)	O
,	O
allow	O
information	B
about	O
p	O
(	O
x	O
)	O
to	O
leak	O
out	O
at	O
a	O
rate	B
faster	O
than	O
one	O
bit	B
per	O
iteration	O
?	O
find	O
toy	O
examples	O
in	O
which	O
this	O
question	O
can	O
be	O
precisely	O
investigated	O
.	O
exercise	O
30.7	O
.	O
[	O
4	O
]	O
hamiltonian	O
monte	O
carlo	O
is	O
another	O
smart	O
metropolis	O
method	B
in	O
which	O
the	O
proposal	O
distributions	O
depend	O
on	O
p	O
(	O
x	O
)	O
.	O
can	O
hamiltonian	O
monte	O
carlo	O
extract	O
information	B
about	O
p	O
(	O
x	O
)	O
at	O
a	O
rate	B
faster	O
than	O
one	O
bit	B
per	O
iteration	O
?	O
exercise	O
30.8	O
.	O
[	O
5	O
]	O
in	O
importance	O
sampling	O
,	O
the	O
weight	B
wr	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
=q	O
(	O
cid:3	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
,	O
a	O
(	O
cid:13	O
)	O
oating-point	O
number	O
,	O
is	O
computed	O
and	O
retained	O
until	O
the	O
end	O
of	O
the	O
computation	O
.	O
in	O
contrast	O
,	O
in	O
the	O
dumb	O
metropolis	O
method	B
,	O
the	O
ratio	O
a	O
=	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
=p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
is	O
reduced	O
to	O
a	O
single	O
bit	O
(	O
‘	O
is	O
a	O
bigger	O
than	O
or	O
smaller	O
than	O
the	O
random	O
number	O
u	O
?	O
’	O
)	O
.	O
thus	O
in	O
principle	O
importance	B
sampling	I
preserves	O
more	O
information	B
about	O
p	O
(	O
cid:3	O
)	O
than	O
does	O
dumb	O
metropolis	O
.	O
can	O
you	O
(	O
cid:12	O
)	O
nd	O
a	O
toy	O
example	O
in	O
which	O
this	O
extra	O
information	O
does	O
indeed	O
lead	O
to	O
faster	O
convergence	O
of	O
importance	O
sampling	O
than	O
metropolis	O
?	O
can	O
you	O
design	O
a	O
markov	O
chain	B
monte	O
carlo	O
algorithm	B
that	O
moves	O
around	O
adaptively	O
,	O
like	O
a	O
metropolis	O
method	B
,	O
and	O
that	O
retains	O
more	O
useful	O
in-	O
formation	O
about	O
the	O
value	O
of	O
p	O
(	O
cid:3	O
)	O
,	O
like	O
importance	B
sampling	I
?	O
in	O
chapter	O
19	O
we	O
noticed	O
that	O
an	O
evolving	O
population	O
of	O
n	O
individuals	O
can	O
make	O
faster	O
evolutionary	O
progress	O
if	O
the	O
individuals	O
engage	O
in	O
sexual	O
reproduc-	O
tion	O
.	O
this	O
observation	O
motivates	O
looking	O
at	O
monte	O
carlo	O
algorithms	B
in	O
which	O
multiple	O
parameter	O
vectors	B
x	O
are	O
evolved	O
and	O
interact	O
.	O
30.6	O
multi-state	B
methods	O
in	O
a	O
multi-state	B
method	O
,	O
multiple	O
parameter	O
vectors	B
x	O
are	O
maintained	O
;	O
they	O
evolve	O
individually	O
under	O
moves	O
such	O
as	O
metropolis	O
and	O
gibbs	O
;	O
there	O
are	O
also	O
interactions	O
among	O
the	O
vectors	B
.	O
the	O
intention	O
is	O
either	O
that	O
eventually	O
all	O
the	O
vectors	B
x	O
should	O
be	O
samples	O
from	O
p	O
(	O
x	O
)	O
(	O
as	O
illustrated	O
by	O
skilling	O
’	O
s	O
leapfrog	B
method	O
)	O
,	O
or	O
that	O
information	B
associated	O
with	O
the	O
(	O
cid:12	O
)	O
nal	O
vectors	B
x	O
should	O
allow	O
us	O
to	O
approximate	O
expectations	O
under	O
p	O
(	O
x	O
)	O
,	O
as	O
in	O
importance	B
sampling	I
.	O
genetic	B
methods	O
genetic	B
algorithms	O
are	O
not	O
often	O
described	O
by	O
their	O
proponents	O
as	O
monte	O
carlo	O
algorithms	B
,	O
but	O
i	O
think	O
this	O
is	O
the	O
correct	O
categorization	O
,	O
and	O
an	O
ideal	O
genetic	B
algorithm	I
would	O
be	O
one	O
that	O
can	O
be	O
proved	O
to	O
be	O
a	O
valid	O
monte	O
carlo	O
algorithm	B
that	O
converges	O
to	O
a	O
speci	O
(	O
cid:12	O
)	O
ed	O
density	B
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
396	O
30	O
|	O
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
i	O
’	O
ll	O
use	O
r	O
to	O
denote	O
the	O
number	O
of	O
vectors	O
in	O
the	O
population	O
.	O
we	O
aim	O
to	O
have	O
p	O
(	O
cid:3	O
)	O
(	O
fx	O
(	O
r	O
)	O
gr	O
three	O
types	O
.	O
1	O
)	O
=q	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
(	O
r	O
)	O
)	O
.	O
a	O
genetic	B
algorithm	I
involves	O
moves	O
of	O
two	O
or	O
first	O
,	O
individual	O
moves	O
in	O
which	O
one	O
state	O
vector	O
is	O
perturbed	O
,	O
x	O
(	O
r	O
)	O
!	O
x	O
(	O
r	O
)	O
0	O
,	O
which	O
could	O
be	O
performed	O
using	O
any	O
of	O
the	O
monte	O
carlo	O
methods	B
we	O
have	O
mentioned	O
so	O
far	O
.	O
second	O
,	O
we	O
allow	O
crossover	B
moves	O
of	O
the	O
form	O
x	O
;	O
y	O
!	O
x0	O
;	O
y0	O
;	O
in	O
a	O
typical	B
crossover	O
move	O
,	O
the	O
progeny	O
x0	O
receives	O
half	O
his	O
state	O
vector	O
from	O
one	O
parent	B
,	O
x	O
,	O
and	O
half	O
from	O
the	O
other	O
,	O
y	O
;	O
the	O
secret	B
of	O
success	O
in	O
a	O
genetic	B
algorithm	I
is	O
that	O
the	O
parameter	O
x	O
must	O
be	O
encoded	O
in	O
such	O
a	O
way	O
that	O
the	O
crossover	B
of	O
two	O
independent	O
states	O
x	O
and	O
y	O
,	O
both	O
of	O
which	O
have	O
good	B
(	O
cid:12	O
)	O
tness	O
p	O
(	O
cid:3	O
)	O
,	O
should	O
have	O
a	O
reasonably	O
good	B
chance	O
of	O
producing	O
progeny	O
who	O
are	O
equally	O
(	O
cid:12	O
)	O
t.	O
this	O
constraint	O
is	O
a	O
hard	O
one	O
to	O
satisfy	O
in	O
many	O
problems	O
,	O
which	O
is	O
why	O
genetic	B
algorithms	O
are	O
mainly	O
talked	O
about	O
and	O
hyped	O
up	O
,	O
and	O
rarely	O
used	O
by	O
serious	O
experts	O
.	O
having	O
introduced	O
a	O
crossover	B
move	O
x	O
;	O
y	O
!	O
x0	O
;	O
y0	O
,	O
we	O
need	O
to	O
choose	O
an	O
acceptance	O
rule	O
.	O
one	O
easy	O
way	O
to	O
obtain	O
a	O
valid	O
algorithm	B
is	O
to	O
accept	O
or	O
reject	O
the	O
crossover	B
proposal	O
using	O
the	O
metropolis	O
rule	O
with	O
p	O
(	O
cid:3	O
)	O
(	O
fx	O
(	O
r	O
)	O
gr	O
1	O
)	O
as	O
the	O
target	O
density	B
{	O
this	O
involves	O
comparing	O
the	O
(	O
cid:12	O
)	O
tnesses	O
before	O
and	O
after	O
the	O
crossover	B
using	O
the	O
ratio	O
p	O
(	O
cid:3	O
)	O
(	O
x0	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
y0	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
p	O
(	O
cid:3	O
)	O
(	O
y	O
)	O
:	O
(	O
30.15	O
)	O
if	O
the	O
crossover	B
operator	O
is	O
reversible	B
then	O
we	O
have	O
an	O
easy	O
proof	O
that	O
this	O
procedure	O
satis	O
(	O
cid:12	O
)	O
es	O
detailed	B
balance	I
and	O
so	O
is	O
a	O
valid	O
component	O
in	O
a	O
chain	B
converging	O
to	O
p	O
(	O
cid:3	O
)	O
(	O
fx	O
(	O
r	O
)	O
gr	O
1	O
)	O
.	O
.	O
exercise	O
30.9	O
.	O
[	O
3	O
]	O
discuss	O
whether	O
the	O
above	O
two	O
operators	O
,	O
individual	O
varia-	O
tion	O
and	O
crossover	O
with	O
the	O
metropolis	O
acceptance	O
rule	O
,	O
will	O
give	O
a	O
more	O
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
method	B
than	O
a	O
standard	O
method	O
with	O
only	O
one	O
state	O
vector	O
and	O
no	O
crossover	B
.	O
the	O
reason	O
why	O
the	O
sexual	O
community	O
could	O
acquire	O
information	B
faster	O
than	O
the	O
asexual	O
community	O
in	O
chapter	O
19	O
was	O
because	O
the	O
crossover	B
operation	O
produced	O
diversity	O
with	O
standard	O
deviation	O
pg	O
,	O
then	O
the	O
blind	O
watchmaker	O
was	O
able	O
to	O
convey	O
lots	O
of	O
information	O
about	O
the	O
(	O
cid:12	O
)	O
tness	O
function	B
by	O
killing	O
o	O
(	O
cid:11	O
)	O
the	O
less	O
(	O
cid:12	O
)	O
t	O
o	O
(	O
cid:11	O
)	O
spring	B
.	O
the	O
above	O
two	O
operators	O
do	O
not	O
o	O
(	O
cid:11	O
)	O
er	O
a	O
speed-up	O
of	O
pg	O
compared	O
with	O
standard	O
monte	O
carlo	O
methods	B
because	O
there	O
is	O
no	O
killing	O
.	O
what	O
’	O
s	O
required	O
,	O
in	O
order	O
to	O
obtain	O
a	O
speed-up	O
,	O
is	O
two	O
things	O
:	O
multiplication	O
and	O
death	O
;	O
and	O
at	O
least	O
one	O
of	O
these	O
must	O
operate	O
selectively	O
.	O
either	O
we	O
must	O
kill	O
o	O
(	O
cid:11	O
)	O
the	O
less-	O
(	O
cid:12	O
)	O
t	O
state	O
vectors	O
,	O
or	O
we	O
must	O
allow	O
the	O
more-	O
(	O
cid:12	O
)	O
t	O
state	O
vectors	O
to	O
give	O
rise	O
to	O
more	O
o	O
(	O
cid:11	O
)	O
spring	B
.	O
while	O
it	O
’	O
s	O
easy	O
to	O
sketch	O
these	O
ideas	O
,	O
it	O
is	O
hard	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
valid	O
method	B
for	O
doing	O
it	O
.	O
exercise	O
30.10	O
.	O
[	O
5	O
]	O
design	O
a	O
birth	O
rule	O
and	O
a	O
death	O
rule	O
such	O
that	O
the	O
chain	B
converges	O
to	O
p	O
(	O
cid:3	O
)	O
(	O
fx	O
(	O
r	O
)	O
gr	O
1	O
)	O
.	O
i	O
believe	O
this	O
is	O
still	O
an	O
open	O
research	O
problem	O
.	O
particle	O
(	O
cid:12	O
)	O
lters	O
particle	O
(	O
cid:12	O
)	O
lters	O
,	O
which	O
are	O
particularly	O
popular	O
in	O
inference	O
problems	O
involving	O
temporal	O
tracking	O
,	O
are	O
multistate	O
methods	B
that	O
mix	O
the	O
ideas	O
of	O
importance	O
sampling	O
and	O
markov	O
chain	B
monte	O
carlo	O
.	O
see	O
isard	O
and	O
blake	O
(	O
1996	O
)	O
,	O
isard	O
and	O
blake	O
(	O
1998	O
)	O
,	O
berzuini	O
et	O
al	O
.	O
(	O
1997	O
)	O
,	O
berzuini	O
and	O
gilks	O
(	O
2001	O
)	O
,	O
doucet	O
et	O
al	O
.	O
(	O
2001	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
30.7	O
:	O
methods	B
that	O
do	O
not	O
necessarily	O
help	O
397	O
30.7	O
methods	B
that	O
do	O
not	O
necessarily	O
help	O
it	O
is	O
common	O
practice	O
to	O
use	O
many	O
initial	O
conditions	O
for	O
a	O
particular	O
markov	O
chain	B
(	O
(	O
cid:12	O
)	O
gure	O
29.19	O
)	O
.	O
if	O
you	O
are	O
worried	O
about	O
sampling	O
well	O
from	O
a	O
complicated	O
density	B
p	O
(	O
x	O
)	O
,	O
can	O
you	O
ensure	O
the	O
states	O
produced	O
by	O
the	O
simulations	O
are	O
well	O
distributed	O
about	O
the	O
typical	B
set	I
of	O
p	O
(	O
x	O
)	O
by	O
ensuring	O
that	O
the	O
initial	O
points	O
are	O
‘	O
well	O
distributed	O
about	O
the	O
whole	O
state	O
space	O
’	O
?	O
the	O
answer	O
is	O
,	O
unfortunately	O
,	O
no	O
.	O
in	O
hierarchical	O
bayesian	O
models	O
,	O
for	O
example	O
,	O
a	O
large	O
number	O
of	O
parameters	O
fxng	O
may	O
be	O
coupled	O
together	O
via	O
an-	O
other	O
parameter	O
(	O
cid:12	O
)	O
(	O
known	O
as	O
a	O
hyperparameter	B
)	O
.	O
for	O
example	O
,	O
the	O
quantities	O
fxng	O
might	O
be	O
independent	O
noise	O
signals	O
,	O
and	O
(	O
cid:12	O
)	O
might	O
be	O
the	O
inverse-variance	O
of	O
the	O
noise	O
source	O
.	O
the	O
joint	B
distribution	O
of	O
(	O
cid:12	O
)	O
and	O
fxng	O
might	O
be	O
p	O
(	O
(	O
cid:12	O
)	O
;	O
fxng	O
)	O
=	O
p	O
(	O
(	O
cid:12	O
)	O
)	O
=	O
p	O
(	O
(	O
cid:12	O
)	O
)	O
p	O
(	O
xn	O
j	O
(	O
cid:12	O
)	O
)	O
1	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
e	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
x2	O
n=2	O
;	O
n	O
yn=1	O
yn=1	O
n	O
where	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
=	O
p2	O
(	O
cid:25	O
)	O
=	O
(	O
cid:12	O
)	O
and	O
p	O
(	O
(	O
cid:12	O
)	O
)	O
is	O
a	O
broad	O
distribution	B
describing	O
our	O
igno-	O
rance	O
about	O
the	O
noise	B
level	O
.	O
for	O
simplicity	O
,	O
let	O
’	O
s	O
leave	O
out	O
all	O
the	O
other	O
variables	O
{	O
data	O
and	O
such	O
{	O
that	O
might	O
be	O
involved	O
in	O
a	O
realistic	O
problem	O
.	O
let	O
’	O
s	O
imagine	O
that	O
we	O
want	O
to	O
sample	B
e	O
(	O
cid:11	O
)	O
ectively	O
from	O
p	O
(	O
(	O
cid:12	O
)	O
;	O
fxng	O
)	O
by	O
gibbs	O
sampling	O
{	O
alter-	O
nately	O
sampling	O
(	O
cid:12	O
)	O
from	O
the	O
conditional	B
distribution	O
p	O
(	O
(	O
cid:12	O
)	O
j	O
xn	O
)	O
then	O
sampling	O
all	O
the	O
xn	O
from	O
their	O
conditional	B
distributions	O
p	O
(	O
xn	O
j	O
(	O
cid:12	O
)	O
)	O
.	O
[	O
the	O
resulting	O
marginal	B
distribution	O
of	O
(	O
cid:12	O
)	O
should	O
asymptotically	O
be	O
the	O
broad	O
distribution	B
p	O
(	O
(	O
cid:12	O
)	O
)	O
.	O
]	O
if	O
n	O
is	O
large	O
then	O
the	O
conditional	B
distribution	O
of	O
(	O
cid:12	O
)	O
given	O
any	O
particular	O
setting	O
of	O
fxng	O
will	O
be	O
tightly	O
concentrated	O
on	O
a	O
particular	O
most-probable	O
value	O
of	O
(	O
cid:12	O
)	O
,	O
with	O
width	O
proportional	O
to	O
1=pn	O
.	O
progress	O
up	O
and	O
down	O
the	O
(	O
cid:12	O
)	O
-axis	O
will	O
therefore	O
take	O
place	O
by	O
a	O
slow	O
random	B
walk	I
with	O
steps	O
of	O
size	O
/	O
1=pn	O
.	O
so	O
,	O
to	O
the	O
initialization	O
strategy	O
.	O
can	O
we	O
(	O
cid:12	O
)	O
nesse	O
our	O
slow	O
convergence	O
problem	O
by	O
using	O
initial	O
conditions	O
located	O
‘	O
all	O
over	O
the	O
state	O
space	O
’	O
?	O
sadly	O
,	O
if	O
we	O
distribute	O
the	O
points	O
fxng	O
widely	O
,	O
what	O
we	O
are	O
actually	O
doing	O
is	O
no	O
.	O
favouring	O
an	O
initial	O
value	O
of	O
the	O
noise	O
level	O
1=	O
(	O
cid:12	O
)	O
that	O
is	O
large	O
.	O
the	O
random	B
walk	I
of	O
the	O
parameter	O
(	O
cid:12	O
)	O
will	O
thus	O
tend	O
,	O
after	O
the	O
(	O
cid:12	O
)	O
rst	O
drawing	O
of	O
(	O
cid:12	O
)	O
from	O
p	O
(	O
(	O
cid:12	O
)	O
j	O
xn	O
)	O
,	O
always	O
to	O
start	O
o	O
(	O
cid:11	O
)	O
from	O
one	O
end	O
of	O
the	O
(	O
cid:12	O
)	O
-axis	O
.	O
further	O
reading	O
the	O
hamiltonian	O
monte	O
carlo	O
method	B
(	O
duane	O
et	O
al.	O
,	O
1987	O
)	O
is	O
reviewed	O
in	O
neal	O
(	O
1993b	O
)	O
.	O
this	O
excellent	O
tome	O
also	O
reviews	O
a	O
huge	O
range	O
of	O
other	O
monte	O
carlo	O
methods	B
,	O
including	O
the	O
related	O
topics	O
of	O
simulated	O
annealing	B
and	O
free	B
energy	I
estimation	O
.	O
30.8	O
further	O
exercises	O
exercise	O
30.11	O
.	O
[	O
4	O
]	O
an	O
important	O
detail	O
of	O
the	O
hamiltonian	O
monte	O
carlo	O
method	B
is	O
that	O
the	O
simulation	O
of	O
the	O
hamiltonian	O
dynamics	O
,	O
while	O
it	O
may	O
be	O
in-	O
accurate	O
,	O
must	O
be	O
perfectly	O
reversible	B
,	O
in	O
the	O
sense	O
that	O
if	O
the	O
initial	O
con-	O
dition	O
(	O
x	O
;	O
p	O
)	O
goes	O
to	O
(	O
x0	O
;	O
p0	O
)	O
,	O
then	O
the	O
same	O
simulator	O
must	O
take	O
(	O
x0	O
;	O
(	O
cid:0	O
)	O
p0	O
)	O
to	O
(	O
x	O
;	O
(	O
cid:0	O
)	O
p	O
)	O
,	O
and	O
the	O
inaccurate	O
dynamics	O
must	O
conserve	O
state-space	O
vol-	O
ume	O
.	O
[	O
the	O
leapfrog	B
method	O
in	O
algorithm	O
30.1	O
satis	O
(	O
cid:12	O
)	O
es	O
these	O
rules	B
.	O
]	O
explain	O
why	O
these	O
rules	B
must	O
be	O
satis	O
(	O
cid:12	O
)	O
ed	O
and	O
create	O
an	O
example	O
illus-	O
trating	O
the	O
problems	O
that	O
arise	O
if	O
they	O
are	O
not	O
.	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
398	O
30	O
|	O
e	O
(	O
cid:14	O
)	O
cient	O
monte	O
carlo	O
methods	B
exercise	O
30.12	O
.	O
[	O
4	O
]	O
a	O
multi-state	B
idea	O
for	O
slice	O
sampling	O
.	O
investigate	O
the	O
follow-	O
ing	O
multi-state	B
method	O
for	O
slice	O
sampling	O
.	O
as	O
in	O
skilling	O
’	O
s	O
multi-state	B
leapfrog	O
method	B
(	O
section	B
30.4	O
)	O
,	O
maintain	O
a	O
set	B
of	O
s	O
state	O
vectors	O
fx	O
(	O
s	O
)	O
g.	O
update	O
one	O
state	O
vector	O
x	O
(	O
s	O
)	O
by	O
one-dimensional	O
slice	B
sampling	I
in	O
a	O
di-	O
rection	O
y	O
determined	O
by	O
picking	O
two	O
other	O
state	O
vectors	O
x	O
(	O
v	O
)	O
and	O
x	O
(	O
w	O
)	O
at	O
random	B
and	O
setting	O
y	O
=	O
x	O
(	O
v	O
)	O
(	O
cid:0	O
)	O
x	O
(	O
w	O
)	O
.	O
investigate	O
this	O
method	B
on	O
toy	O
problems	O
such	O
as	O
a	O
highly-correlated	O
multivariate	O
gaussian	O
distribution	B
.	O
bear	O
in	O
mind	O
that	O
if	O
s	O
(	O
cid:0	O
)	O
1	O
is	O
smaller	O
than	O
the	O
number	O
of	O
dimensions	O
n	O
then	O
this	O
method	B
will	O
not	O
be	O
ergodic	B
by	O
itself	O
,	O
so	O
it	O
may	O
need	O
to	O
be	O
mixed	O
with	O
other	O
methods	B
.	O
are	O
there	O
classes	O
of	O
problems	O
that	O
are	O
better	O
solved	O
by	O
this	O
slice-sampling	O
method	B
than	O
by	O
the	O
standard	O
methods	O
for	O
picking	O
y	O
such	O
as	O
cycling	O
through	O
the	O
coordinate	O
axes	O
or	O
picking	O
u	O
at	O
random	B
from	O
a	O
gaussian	O
distribution	B
?	O
30.9	O
solutions	O
x	O
(	O
v	O
)	O
x	O
(	O
s	O
)	O
x	O
(	O
w	O
)	O
solution	O
to	O
exercise	O
30.3	O
(	O
p.393	O
)	O
.	O
consider	O
the	O
spherical	O
gaussian	O
distribution	B
where	O
all	O
components	O
have	O
mean	B
zero	O
and	O
variance	O
1.	O
in	O
one	O
dimension	O
,	O
the	O
nth	O
,	O
if	O
x	O
(	O
1	O
)	O
n	O
,	O
we	O
obtain	O
the	O
proposed	O
coordinate	O
n	O
leapfrogs	O
over	O
x	O
(	O
2	O
)	O
(	O
x	O
(	O
1	O
)	O
n	O
)	O
0	O
=	O
2x	O
(	O
2	O
)	O
n	O
(	O
cid:0	O
)	O
x	O
(	O
1	O
)	O
n	O
:	O
(	O
30.16	O
)	O
n	O
and	O
x	O
(	O
2	O
)	O
assuming	O
that	O
x	O
(	O
1	O
)	O
(	O
x	O
(	O
1	O
)	O
in	O
energy	O
contributed	O
by	O
this	O
one	O
dimension	O
will	O
be	O
n	O
are	O
gaussian	O
random	B
variables	O
from	O
normal	O
(	O
0	O
;	O
1	O
)	O
,	O
n	O
)	O
0	O
is	O
gaussian	O
from	O
normal	O
(	O
0	O
;	O
(	O
cid:27	O
)	O
2	O
)	O
,	O
where	O
(	O
cid:27	O
)	O
2	O
=	O
22+	O
(	O
(	O
cid:0	O
)	O
1	O
)	O
2	O
=	O
5.	O
the	O
change	O
n	O
)	O
2i	O
=	O
2	O
(	O
x	O
(	O
2	O
)	O
n	O
)	O
2	O
(	O
cid:0	O
)	O
2x	O
(	O
2	O
)	O
n	O
x	O
(	O
1	O
)	O
n	O
(	O
30.17	O
)	O
1	O
2h	O
(	O
2x	O
(	O
2	O
)	O
n	O
(	O
cid:0	O
)	O
x	O
(	O
1	O
)	O
n	O
)	O
2	O
(	O
cid:0	O
)	O
(	O
x	O
(	O
1	O
)	O
so	O
the	O
typical	B
change	O
in	O
energy	O
is	O
2h	O
(	O
x	O
(	O
2	O
)	O
n	O
)	O
2i	O
=	O
2.	O
this	O
positive	O
change	O
is	O
bad	B
news	O
.	O
in	O
n	O
dimensions	B
,	O
the	O
typical	B
change	O
in	O
energy	O
when	O
a	O
leapfrog	B
move	O
is	O
made	O
,	O
at	O
equilibrium	O
,	O
is	O
thus	O
+2n	O
.	O
the	O
probability	O
of	O
acceptance	O
of	O
the	O
move	O
scales	O
as	O
e	O
(	O
cid:0	O
)	O
2n	O
:	O
(	O
30.18	O
)	O
this	O
implies	O
that	O
skilling	O
’	O
s	O
method	B
,	O
as	O
described	O
,	O
is	O
not	O
e	O
(	O
cid:11	O
)	O
ective	O
in	O
very	O
high-	O
dimensional	O
problems	O
{	O
at	O
least	O
,	O
not	O
once	O
convergence	O
has	O
occurred	O
.	O
nev-	O
ertheless	O
it	O
has	O
the	O
impressive	O
advantage	O
that	O
its	O
convergence	O
properties	O
are	O
independent	O
of	O
the	O
strength	O
of	O
correlations	O
between	O
the	O
variables	O
{	O
a	O
property	O
that	O
not	O
even	O
the	O
hamiltonian	O
monte	O
carlo	O
and	O
overrelaxation	O
methods	B
o	O
(	O
cid:11	O
)	O
er	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
chapter	O
31	O
some	O
of	O
the	O
neural	O
network	B
models	O
that	O
we	O
will	O
encounter	O
are	O
related	O
to	O
ising	O
models	O
,	O
which	O
are	O
idealized	O
magnetic	O
systems	O
.	O
it	O
is	O
not	O
essential	O
to	O
understand	O
the	O
statistical	B
physics	I
of	O
ising	O
models	O
to	O
understand	O
these	O
neural	O
networks	O
,	O
but	O
i	O
hope	O
you	O
’	O
ll	O
(	O
cid:12	O
)	O
nd	O
them	O
helpful	O
.	O
ising	O
models	O
are	O
also	O
related	O
to	O
several	O
other	O
topics	O
in	O
this	O
book	O
.	O
we	O
will	O
use	O
exact	O
tree-based	O
computation	O
methods	B
like	O
those	O
introduced	O
in	O
chapter	O
25	O
to	O
evaluate	O
properties	O
of	O
interest	O
in	O
ising	O
models	O
.	O
ising	O
models	O
o	O
(	O
cid:11	O
)	O
er	O
crude	O
models	O
for	O
binary	O
images	B
.	O
and	O
ising	O
models	O
relate	O
to	O
two-dimensional	B
con-	O
strained	O
channels	O
(	O
cf	O
.	O
chapter	O
17	O
)	O
:	O
a	O
two-dimensional	B
bar-code	O
in	O
which	O
a	O
black	B
dot	O
may	O
not	O
be	O
completely	O
surrounded	O
by	O
black	O
dots	O
,	O
and	O
a	O
white	B
dot	O
may	O
not	O
be	O
completely	O
surrounded	O
by	O
white	O
dots	O
,	O
is	O
similar	O
to	O
an	O
antiferro-	O
magnetic	O
ising	O
model	B
at	O
low	O
temperature	B
.	O
evaluating	O
the	O
entropy	B
of	O
this	O
ising	O
model	B
is	O
equivalent	O
to	O
evaluating	O
the	O
capacity	B
of	O
the	O
constrained	B
channel	I
for	O
conveying	O
bits	O
.	O
if	O
you	O
would	O
like	O
to	O
jog	O
your	O
memory	B
on	O
statistical	B
physics	I
and	O
thermody-	O
namics	O
,	O
you	O
might	O
(	O
cid:12	O
)	O
nd	O
appendix	O
b	O
helpful	O
.	O
i	O
also	O
recommend	O
the	O
book	O
by	O
reif	O
(	O
1965	O
)	O
.	O
399	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
31	O
ising	O
models	O
an	O
ising	O
model	B
is	O
an	O
array	O
of	O
spins	O
(	O
e.g.	O
,	O
atoms	O
that	O
can	O
take	O
states	O
(	O
cid:6	O
)	O
1	O
)	O
that	O
are	O
magnetically	O
coupled	O
to	O
each	O
other	O
.	O
if	O
one	O
spin	O
is	O
,	O
say	O
,	O
in	O
the	O
+1	O
state	O
then	O
it	O
is	O
energetically	O
favourable	O
for	O
its	O
immediate	O
neighbours	O
to	O
be	O
in	O
the	O
same	O
state	O
,	O
in	O
the	O
case	O
of	O
a	O
ferromagnetic	B
model	O
,	O
and	O
in	O
the	O
opposite	O
state	O
,	O
in	O
the	O
case	O
of	O
an	O
antiferromagnet	O
.	O
in	O
this	O
chapter	O
we	O
discuss	O
two	O
computational	O
techniques	O
for	O
studying	O
ising	O
models	O
.	O
let	O
the	O
state	O
x	O
of	O
an	O
ising	O
model	B
with	O
n	O
spins	O
be	O
a	O
vector	O
in	O
which	O
each	O
component	O
xn	O
takes	O
values	O
(	O
cid:0	O
)	O
1	O
or	O
+1	O
.	O
if	O
two	O
spins	O
m	O
and	O
n	O
are	O
neighbours	O
we	O
write	O
(	O
m	O
;	O
n	O
)	O
2	O
n	O
.	O
the	O
coupling	O
between	O
neighbouring	O
spins	O
is	O
j.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
jmn	O
=	O
j	O
if	O
m	O
and	O
n	O
are	O
neighbours	O
and	O
jmn	O
=	O
0	O
otherwise	O
.	O
the	O
energy	B
of	O
a	O
state	O
x	O
is	O
2xm	O
;	O
n	O
jmnxmxn	O
+xn	O
hxn	O
#	O
;	O
e	O
(	O
x	O
;	O
j	O
;	O
h	O
)	O
=	O
(	O
cid:0	O
)	O
''	O
1	O
(	O
31.1	O
)	O
where	O
h	O
is	O
the	O
applied	O
(	O
cid:12	O
)	O
eld	O
.	O
if	O
j	O
>	O
0	O
then	O
the	O
model	B
is	O
ferromagnetic	B
,	O
and	O
if	O
j	O
<	O
0	O
it	O
is	O
antiferromagnetic	B
.	O
we	O
’	O
ve	O
included	O
the	O
factor	O
of	O
1/2	O
because	O
each	O
pair	O
is	O
counted	O
twice	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
sum	O
,	O
once	O
as	O
(	O
m	O
;	O
n	O
)	O
and	O
once	O
as	O
(	O
n	O
;	O
m	O
)	O
.	O
at	O
equilibrium	O
at	O
temperature	B
t	O
,	O
the	O
probability	B
that	O
the	O
state	O
is	O
x	O
is	O
p	O
(	O
xj	O
(	O
cid:12	O
)	O
;	O
j	O
;	O
h	O
)	O
=	O
1	O
z	O
(	O
(	O
cid:12	O
)	O
;	O
j	O
;	O
h	O
)	O
exp	O
[	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
;	O
j	O
;	O
h	O
)	O
]	O
;	O
where	O
(	O
cid:12	O
)	O
=	O
1=kbt	O
,	O
kb	O
is	O
boltzmann	O
’	O
s	O
constant	O
,	O
and	O
z	O
(	O
(	O
cid:12	O
)	O
;	O
j	O
;	O
h	O
)	O
(	O
cid:17	O
)	O
xx	O
exp	O
[	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
;	O
j	O
;	O
h	O
)	O
]	O
:	O
relevance	O
of	O
ising	O
models	O
(	O
31.2	O
)	O
(	O
31.3	O
)	O
ising	O
models	O
are	O
relevant	O
for	O
three	O
reasons	O
.	O
ising	O
models	O
are	O
important	O
(	O
cid:12	O
)	O
rst	O
as	O
models	O
of	O
magnetic	O
systems	O
that	O
have	O
a	O
phase	B
transition	I
.	O
the	O
theory	B
of	O
universality	B
in	O
statistical	B
physics	I
shows	O
that	O
all	O
systems	O
with	O
the	O
same	O
dimension	O
(	O
here	O
,	O
two	O
)	O
,	O
and	O
the	O
same	O
symmetries	O
,	O
have	O
equivalent	O
critical	O
properties	O
,	O
i.e.	O
,	O
the	O
scaling	B
laws	O
shown	O
by	O
their	O
phase	O
transitions	O
are	O
identical	O
.	O
so	O
by	O
studying	O
ising	O
models	O
we	O
can	O
(	O
cid:12	O
)	O
nd	O
out	O
not	O
only	O
about	O
magnetic	O
phase	O
transitions	O
but	O
also	O
about	O
phase	O
transitions	O
in	O
many	O
other	O
systems	O
.	O
second	O
,	O
if	O
we	O
generalize	O
the	O
energy	B
function	O
to	O
e	O
(	O
x	O
;	O
j	O
;	O
h	O
)	O
=	O
(	O
cid:0	O
)	O
''	O
1	O
2xm	O
;	O
n	O
jmnxmxn	O
+xn	O
hnxn	O
#	O
;	O
(	O
31.4	O
)	O
where	O
the	O
couplings	O
jmn	O
and	O
applied	O
(	O
cid:12	O
)	O
elds	O
hn	O
are	O
not	O
constant	O
,	O
we	O
obtain	O
a	O
family	O
of	O
models	O
known	O
as	O
‘	O
spin	O
glasses	O
’	O
to	O
physicists	O
,	O
and	O
as	O
‘	O
hop	O
(	O
cid:12	O
)	O
eld	O
400	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
31	O
|	O
ising	O
models	O
401	O
networks	O
’	O
or	O
‘	O
boltzmann	O
machines	O
’	O
to	O
the	O
neural	B
network	I
community	O
.	O
in	O
some	O
of	O
these	O
models	O
,	O
all	O
spins	O
are	O
declared	O
to	O
be	O
neighbours	O
of	O
each	O
other	O
,	O
in	O
which	O
case	O
physicists	O
call	O
the	O
system	O
an	O
‘	O
in	O
(	O
cid:12	O
)	O
nite-range	O
’	O
spin	O
glass	O
,	O
and	O
networkers	O
call	O
it	O
a	O
‘	O
fully	O
connected	O
’	O
network	B
.	O
third	O
,	O
the	O
ising	O
model	B
is	O
also	O
useful	O
as	O
a	O
statistical	B
model	O
in	O
its	O
own	O
right	O
.	O
in	O
this	O
chapter	O
we	O
will	O
study	O
ising	O
models	O
using	O
two	O
di	O
(	O
cid:11	O
)	O
erent	O
computational	O
techniques	O
.	O
some	O
remarkable	O
relationships	O
in	O
statistical	O
physics	B
we	O
would	O
like	O
to	O
get	O
as	O
much	O
information	B
as	O
possible	O
out	O
of	O
our	O
computations	O
.	O
consider	O
for	O
example	O
the	O
heat	B
capacity	I
of	O
a	O
system	O
,	O
which	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
@	O
@	O
t	O
(	O
cid:22	O
)	O
e	O
;	O
c	O
(	O
cid:17	O
)	O
z	O
xx	O
1	O
(	O
31.5	O
)	O
(	O
31.6	O
)	O
where	O
(	O
cid:22	O
)	O
e	O
=	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
)	O
)	O
e	O
(	O
x	O
)	O
:	O
to	O
work	O
out	O
the	O
heat	B
capacity	I
of	O
a	O
system	O
,	O
we	O
might	O
naively	O
guess	O
that	O
we	O
have	O
to	O
increase	O
the	O
temperature	B
and	O
measure	O
the	O
energy	B
change	O
.	O
heat	B
capacity	I
,	O
however	O
,	O
is	O
intimately	O
related	O
to	O
energy	B
(	O
cid:13	O
)	O
uctuations	O
at	O
constant	O
temperature	B
.	O
let	O
’	O
s	O
start	O
from	O
the	O
partition	B
function	I
,	O
z	O
=xx	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
)	O
)	O
:	O
the	O
mean	B
energy	O
is	O
obtained	O
by	O
di	O
(	O
cid:11	O
)	O
erentiation	O
with	O
respect	O
to	O
(	O
cid:12	O
)	O
:	O
@	O
ln	O
z	O
@	O
(	O
cid:12	O
)	O
=	O
1	O
z	O
xx	O
(	O
cid:0	O
)	O
e	O
(	O
x	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
e	O
:	O
a	O
further	O
di	O
(	O
cid:11	O
)	O
erentiation	O
spits	O
out	O
the	O
variance	B
of	O
the	O
energy	B
:	O
(	O
31.7	O
)	O
(	O
31.8	O
)	O
@	O
2	O
ln	O
z	O
@	O
(	O
cid:12	O
)	O
2	O
=	O
1	O
z	O
xx	O
e	O
(	O
x	O
)	O
2	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
)	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
e2	O
=	O
he2i	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
e2	O
=	O
var	O
(	O
e	O
)	O
:	O
(	O
31.9	O
)	O
but	O
the	O
heat	B
capacity	I
is	O
also	O
the	O
derivative	O
of	O
(	O
cid:22	O
)	O
e	O
with	O
respect	O
to	O
temperature	B
:	O
@	O
(	O
cid:22	O
)	O
e	O
@	O
t	O
@	O
@	O
t	O
=	O
(	O
cid:0	O
)	O
@	O
ln	O
z	O
@	O
(	O
cid:12	O
)	O
@	O
2	O
ln	O
z	O
@	O
(	O
cid:12	O
)	O
2	O
@	O
(	O
cid:12	O
)	O
@	O
t	O
=	O
(	O
cid:0	O
)	O
=	O
(	O
cid:0	O
)	O
var	O
(	O
e	O
)	O
(	O
(	O
cid:0	O
)	O
1=kbt	O
2	O
)	O
:	O
(	O
31.10	O
)	O
so	O
for	O
any	O
system	O
at	O
temperature	B
t	O
,	O
c	O
=	O
var	O
(	O
e	O
)	O
kbt	O
2	O
=	O
kb	O
(	O
cid:12	O
)	O
2	O
var	O
(	O
e	O
)	O
:	O
(	O
31.11	O
)	O
thus	O
if	O
we	O
can	O
observe	O
the	O
variance	B
of	O
the	O
energy	B
of	O
a	O
system	O
at	O
equilibrium	O
,	O
we	O
can	O
estimate	O
its	O
heat	B
capacity	I
.	O
i	O
(	O
cid:12	O
)	O
nd	O
this	O
an	O
almost	O
paradoxical	O
relationship	O
.	O
consider	O
a	O
system	O
with	O
a	O
(	O
cid:12	O
)	O
nite	O
set	B
of	O
states	O
,	O
and	O
imagine	O
heating	O
it	O
up	O
.	O
at	O
high	O
temperature	O
,	O
all	O
states	O
will	O
be	O
equiprobable	O
,	O
so	O
the	O
mean	B
energy	O
will	O
be	O
essentially	O
constant	O
and	O
the	O
heat	B
capacity	I
will	O
be	O
essentially	O
zero	O
.	O
but	O
on	O
the	O
other	O
hand	O
,	O
with	O
all	O
states	O
being	O
equiprobable	O
,	O
there	O
will	O
certainly	O
be	O
(	O
cid:13	O
)	O
uctuations	O
in	O
energy	O
.	O
so	O
how	O
can	O
the	O
heat	B
capacity	I
be	O
related	O
to	O
the	O
(	O
cid:13	O
)	O
uctuations	O
?	O
the	O
answer	O
is	O
in	O
the	O
words	O
‘	O
essentially	O
zero	O
’	O
above	O
.	O
the	O
heat	B
capacity	I
is	O
not	O
quite	O
zero	O
at	O
high	O
temperature	O
,	O
it	O
just	O
tends	O
to	O
zero	O
.	O
and	O
it	O
tends	O
to	O
zero	O
as	O
var	O
(	O
e	O
)	O
kbt	O
2	O
,	O
with	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
402	O
31	O
|	O
ising	O
models	O
the	O
quantity	O
var	O
(	O
e	O
)	O
tending	O
to	O
a	O
constant	O
at	O
high	O
temperatures	O
.	O
this	O
1=t	O
2	O
behaviour	O
of	O
the	O
heat	O
capacity	B
of	O
(	O
cid:12	O
)	O
nite	O
systems	O
at	O
high	O
temperatures	O
is	O
thus	O
very	O
general	O
.	O
the	O
1=t	O
2	O
factor	O
can	O
be	O
viewed	O
as	O
an	O
accident	O
of	O
history	O
.	O
perature	O
scales	O
had	O
been	O
de	O
(	O
cid:12	O
)	O
ned	O
using	O
(	O
cid:12	O
)	O
=	O
1	O
capacity	B
would	O
be	O
if	O
only	O
tem-	O
kbt	O
,	O
then	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
heat	O
c	O
(	O
(	O
cid:12	O
)	O
)	O
(	O
cid:17	O
)	O
@	O
(	O
cid:22	O
)	O
e	O
@	O
(	O
cid:12	O
)	O
=	O
var	O
(	O
e	O
)	O
;	O
(	O
31.12	O
)	O
and	O
heat	O
capacity	B
and	O
(	O
cid:13	O
)	O
uctuations	O
would	O
be	O
identical	O
quantities	O
.	O
.	O
exercise	O
31.1	O
.	O
[	O
2	O
]	O
[	O
we	O
will	O
call	O
the	O
entropy	B
of	O
a	O
physical	O
system	O
s	O
rather	O
than	O
h	O
,	O
while	O
we	O
are	O
in	O
a	O
statistical	B
physics	I
chapter	O
;	O
we	O
set	B
kb	O
=	O
1	O
.	O
]	O
the	O
entropy	B
of	O
a	O
system	O
whose	O
states	O
are	O
x	O
,	O
at	O
temperature	B
t	O
=	O
1=	O
(	O
cid:12	O
)	O
,	O
is	O
where	O
(	O
a	O
)	O
show	O
that	O
s	O
=x	O
p	O
(	O
x	O
)	O
[	O
ln	O
1=p	O
(	O
x	O
)	O
]	O
exp	O
[	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
)	O
]	O
:	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
1	O
p	O
(	O
x	O
)	O
=	O
s	O
=	O
ln	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
+	O
(	O
cid:12	O
)	O
(	O
cid:22	O
)	O
e	O
(	O
(	O
cid:12	O
)	O
)	O
where	O
(	O
cid:22	O
)	O
e	O
(	O
(	O
cid:12	O
)	O
)	O
is	O
the	O
mean	B
energy	O
of	O
the	O
system	O
.	O
(	O
b	O
)	O
show	O
that	O
s	O
=	O
(	O
cid:0	O
)	O
@	O
f	O
@	O
t	O
;	O
where	O
the	O
free	B
energy	I
f	O
=	O
(	O
cid:0	O
)	O
kt	O
ln	O
z	O
and	O
kt	O
=	O
1=	O
(	O
cid:12	O
)	O
.	O
31.1	O
ising	O
models	O
{	O
monte	O
carlo	O
simulation	O
(	O
31.13	O
)	O
(	O
31.14	O
)	O
(	O
31.15	O
)	O
(	O
31.16	O
)	O
in	O
this	O
section	B
we	O
study	O
two-dimensional	B
planar	O
ising	O
models	O
using	O
a	O
simple	O
gibbs-sampling	O
method	B
.	O
starting	O
from	O
some	O
initial	O
state	O
,	O
a	O
spin	O
n	O
is	O
selected	O
at	O
random	B
,	O
and	O
the	O
probability	B
that	O
it	O
should	O
be	O
+1	O
given	O
the	O
state	O
of	O
the	O
other	O
spins	O
and	O
the	O
temperature	B
is	O
computed	O
,	O
p	O
(	O
+1j	O
bn	O
)	O
=	O
1	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
2	O
(	O
cid:12	O
)	O
bn	O
)	O
;	O
(	O
31.17	O
)	O
where	O
(	O
cid:12	O
)	O
=	O
1=kbt	O
and	O
bn	O
is	O
the	O
local	O
(	O
cid:12	O
)	O
eld	O
bn	O
=	O
xm	O
:	O
(	O
m	O
;	O
n	O
)	O
2n	O
jxm	O
+	O
h	O
:	O
(	O
31.18	O
)	O
[	O
the	O
factor	O
of	O
2	O
appears	O
in	O
equation	O
(	O
31.17	O
)	O
because	O
the	O
two	O
spin	O
states	O
are	O
f+1	O
;	O
(	O
cid:0	O
)	O
1g	O
rather	O
than	O
f+1	O
;	O
0g	O
.	O
]	O
spin	O
n	O
is	O
set	B
to	O
+1	O
with	O
that	O
probability	B
,	O
and	O
otherwise	O
to	O
(	O
cid:0	O
)	O
1	O
;	O
then	O
the	O
next	O
spin	O
to	O
update	O
is	O
selected	O
at	O
random	B
.	O
after	O
su	O
(	O
cid:14	O
)	O
ciently	O
many	O
iterations	O
,	O
this	O
procedure	O
converges	O
to	O
the	O
equilibrium	O
distribution	B
(	O
31.2	O
)	O
.	O
an	O
alternative	O
to	O
the	O
gibbs	O
sampling	O
formula	O
(	O
31.17	O
)	O
is	O
the	O
metropolis	O
algorithm	B
,	O
in	O
which	O
we	O
consider	O
the	O
change	O
in	O
energy	O
that	O
results	O
from	O
(	O
cid:13	O
)	O
ipping	O
the	O
chosen	O
spin	O
from	O
its	O
current	O
state	O
xn	O
,	O
(	O
cid:1	O
)	O
e	O
=	O
2xnbn	O
;	O
and	O
adopt	O
this	O
change	O
in	O
con	O
(	O
cid:12	O
)	O
guration	O
with	O
probability	O
p	O
(	O
accept	O
;	O
(	O
cid:1	O
)	O
e	O
;	O
(	O
cid:12	O
)	O
)	O
=	O
(	O
cid:26	O
)	O
1	O
(	O
cid:1	O
)	O
e	O
(	O
cid:20	O
)	O
0	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
(	O
cid:1	O
)	O
e	O
)	O
(	O
cid:1	O
)	O
e	O
>	O
0	O
:	O
(	O
31.19	O
)	O
(	O
31.20	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
403	O
bbbb	O
bbbb	O
bbbb	O
bbbb	O
bbbb	O
bbbb	O
figure	O
31.1.	O
rectangular	B
ising	O
model	B
.	O
t	O
5	O
2.5	O
2.4	O
2.3	O
2	O
figure	O
31.2.	O
sample	B
states	O
of	O
rectangular	O
ising	O
models	O
with	O
j	O
=	O
1	O
at	O
a	O
sequence	B
of	O
temperatures	O
t	O
.	O
31.1	O
:	O
ising	O
models	O
{	O
monte	O
carlo	O
simulation	O
this	O
procedure	O
has	O
roughly	O
double	O
the	O
probability	O
of	O
accepting	O
energetically	O
unfavourable	O
moves	O
,	O
so	O
may	O
be	O
a	O
more	O
e	O
(	O
cid:14	O
)	O
cient	O
sampler	O
{	O
but	O
at	O
very	O
low	O
tem-	O
peratures	O
the	O
relative	B
merits	O
of	O
gibbs	O
sampling	O
and	O
the	O
metropolis	O
algorithm	B
may	O
be	O
subtle	O
.	O
rectangular	B
geometry	O
i	O
(	O
cid:12	O
)	O
rst	O
simulated	O
an	O
ising	O
model	B
with	O
the	O
rectangular	B
geometry	O
shown	O
in	O
(	O
cid:12	O
)	O
g-	O
ure	O
31.1	O
,	O
and	O
with	O
periodic	O
boundary	O
conditions	O
.	O
a	O
line	O
between	O
two	O
spins	O
indicates	O
that	O
they	O
are	O
neighbours	O
.	O
i	O
set	B
the	O
external	O
(	O
cid:12	O
)	O
eld	O
h	O
=	O
0	O
and	O
con-	O
sidered	O
the	O
two	O
cases	O
j	O
=	O
(	O
cid:6	O
)	O
1	O
,	O
which	O
are	O
a	O
ferromagnet	O
and	O
antiferromagnet	O
respectively	O
.	O
i	O
started	O
at	O
a	O
large	O
temperature	B
(	O
t	O
=	O
33	O
;	O
(	O
cid:12	O
)	O
=	O
0:03	O
)	O
and	O
changed	O
the	O
temper-	O
ature	O
every	O
i	O
iterations	O
,	O
(	O
cid:12	O
)	O
rst	O
decreasing	O
it	O
gradually	O
to	O
t	O
=	O
0:1	O
;	O
(	O
cid:12	O
)	O
=	O
10	O
,	O
then	O
increasing	O
it	O
gradually	O
back	O
to	O
a	O
large	O
temperature	B
again	O
.	O
this	O
procedure	O
gives	O
a	O
crude	O
check	O
on	O
whether	O
‘	O
equilibrium	O
has	O
been	O
reached	O
’	O
at	O
each	O
tem-	O
perature	O
;	O
if	O
not	O
,	O
we	O
’	O
d	O
expect	O
to	O
see	O
some	O
hysteresis	O
in	O
the	O
graphs	O
we	O
plot	O
.	O
it	O
also	O
gives	O
an	O
idea	O
of	O
the	O
reproducibility	O
of	O
the	O
results	O
,	O
if	O
we	O
assume	O
that	O
the	O
two	O
runs	O
,	O
with	O
decreasing	O
and	O
increasing	O
temperature	B
,	O
are	O
e	O
(	O
cid:11	O
)	O
ectively	O
independent	O
of	O
each	O
other	O
.	O
at	O
each	O
temperature	B
i	O
recorded	O
the	O
mean	B
energy	O
per	O
spin	O
and	O
the	O
standard	B
deviation	I
of	O
the	O
energy	B
,	O
and	O
the	O
mean	B
square	O
value	O
of	O
the	O
magnetization	O
m	O
,	O
m	O
=	O
1	O
n	O
xn	O
xn	O
:	O
(	O
31.21	O
)	O
one	O
tricky	O
decision	O
that	O
has	O
to	O
be	O
made	O
is	O
how	O
soon	O
to	O
start	O
taking	O
these	O
measurements	O
after	O
a	O
new	O
temperature	B
has	O
been	O
established	O
;	O
it	O
is	O
di	O
(	O
cid:14	O
)	O
cult	O
to	O
detect	O
‘	O
equilibrium	O
’	O
{	O
or	O
even	O
to	O
give	O
a	O
clear	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
a	O
system	O
’	O
s	O
being	O
‘	O
at	O
equilibrium	O
’	O
!	O
[	O
but	O
in	O
chapter	O
32	O
we	O
will	O
see	O
a	O
solution	O
to	O
this	O
problem	O
.	O
]	O
my	O
crude	O
strategy	O
was	O
to	O
let	O
the	O
number	O
of	O
iterations	O
at	O
each	O
temperature	B
,	O
i	O
,	O
be	O
a	O
few	O
hundred	O
times	O
the	O
number	O
of	O
spins	O
n	O
,	O
and	O
to	O
discard	O
the	O
(	O
cid:12	O
)	O
rst	O
1/3	O
of	O
those	O
iterations	O
.	O
with	O
n	O
=	O
100	O
,	O
i	O
found	O
i	O
needed	O
more	O
than	O
100	O
000	O
iterations	O
to	O
reach	O
equilibrium	O
at	O
any	O
given	O
temperature	B
.	O
results	O
for	O
small	O
n	O
with	O
j	O
=	O
1.	O
i	O
simulated	O
an	O
l	O
(	O
cid:2	O
)	O
l	O
grid	O
for	O
l	O
=	O
4	O
;	O
5	O
;	O
:	O
:	O
:	O
;	O
10	O
;	O
40	O
;	O
64.	O
let	O
’	O
s	O
have	O
a	O
quick	O
think	O
about	O
what	O
results	O
we	O
expect	O
.	O
at	O
low	O
temperatures	O
the	O
system	O
is	O
expected	O
to	O
be	O
in	O
a	O
ground	O
state	O
.	O
the	O
rectangular	B
ising	O
model	B
with	O
j	O
=	O
1	O
has	O
two	O
ground	O
states	O
,	O
the	O
all	O
+1	O
state	O
and	O
the	O
all	O
(	O
cid:0	O
)	O
1	O
state	O
.	O
the	O
energy	B
per	O
spin	O
of	O
either	O
ground	O
state	O
is	O
(	O
cid:0	O
)	O
2.	O
at	O
high	O
temperatures	O
,	O
the	O
spins	O
are	O
independent	O
,	O
all	O
states	O
are	O
equally	O
probable	O
,	O
and	O
the	O
energy	B
is	O
expected	O
to	O
(	O
cid:13	O
)	O
uctuate	O
around	O
a	O
mean	B
of	O
0	O
with	O
a	O
standard	B
deviation	I
proportional	O
to	O
1=pn	O
.	O
let	O
’	O
s	O
look	O
at	O
some	O
results	O
.	O
in	O
all	O
(	O
cid:12	O
)	O
gures	O
temperature	B
t	O
is	O
shown	O
with	O
kb	O
=	O
1.	O
the	O
basic	O
picture	O
emerges	O
with	O
as	O
few	O
as	O
16	O
spins	O
(	O
(	O
cid:12	O
)	O
gure	O
31.3	O
,	O
top	O
)	O
:	O
the	O
energy	B
rises	O
monotonically	O
.	O
as	O
we	O
increase	O
the	O
number	O
of	O
spins	O
to	O
100	O
(	O
(	O
cid:12	O
)	O
gure	O
31.3	O
,	O
bottom	O
)	O
some	O
new	O
details	O
emerge	O
.	O
first	O
,	O
as	O
expected	O
,	O
the	O
(	O
cid:13	O
)	O
uctuations	O
at	O
large	O
temperature	B
decrease	O
as	O
1=pn	O
.	O
second	O
,	O
the	O
(	O
cid:13	O
)	O
uctuations	O
at	O
intermediate	O
temperature	B
become	O
relatively	O
bigger	O
.	O
this	O
is	O
the	O
signature	O
of	O
a	O
‘	O
collective	B
phenomenon	O
’	O
,	O
in	O
this	O
case	O
,	O
a	O
phase	B
transition	I
.	O
only	O
systems	O
with	O
in	O
(	O
cid:12	O
)	O
nite	O
n	O
show	O
true	O
phase	O
transitions	O
,	O
but	O
with	O
n	O
=	O
100	O
we	O
are	O
getting	O
a	O
hint	O
of	O
the	O
critical	O
(	O
cid:13	O
)	O
uctuations	O
.	O
figure	O
31.5	O
shows	O
details	O
of	O
the	O
graphs	O
for	O
n	O
=	O
100	O
and	O
n	O
=	O
4096.	O
figure	O
31.2	O
shows	O
a	O
sequence	B
of	O
typical	B
states	O
from	O
the	O
simulation	O
of	O
n	O
=	O
4096	O
spins	O
at	O
a	O
sequence	B
of	O
decreasing	O
temperatures	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
31	O
|	O
ising	O
models	O
figure	O
31.3.	O
monte	O
carlo	O
simulations	O
of	O
rectangular	O
ising	O
models	O
with	O
j	O
=	O
1.	O
mean	B
energy	O
and	O
(	O
cid:13	O
)	O
uctuations	O
in	O
energy	O
as	O
a	O
function	B
of	O
temperature	B
(	O
left	O
)	O
.	O
mean	B
square	O
magnetization	O
as	O
a	O
function	B
of	O
temperature	B
(	O
right	O
)	O
.	O
in	O
the	O
top	O
row	O
,	O
n	O
=	O
16	O
,	O
and	O
the	O
bottom	O
,	O
n	O
=	O
100.	O
for	O
even	O
larger	O
n	O
,	O
see	O
later	O
(	O
cid:12	O
)	O
gures	O
.	O
t	O
figure	O
31.4.	O
schematic	O
diagram	O
to	O
explain	O
the	O
meaning	O
of	O
a	O
schottky	O
anomaly	O
.	O
the	O
curve	O
shows	O
the	O
heat	B
capacity	I
of	O
two	O
gases	O
as	O
a	O
function	B
of	O
temperature	B
.	O
the	O
lower	O
curve	O
shows	O
a	O
normal	B
gas	O
whose	O
heat	B
capacity	I
is	O
an	O
increasing	O
function	B
of	O
temperature	B
.	O
the	O
upper	O
curve	O
has	O
a	O
small	O
peak	O
in	O
the	O
heat	B
capacity	I
,	O
which	O
is	O
known	O
as	O
a	O
schottky	O
anomaly	O
(	O
at	O
least	O
in	O
cambridge	O
)	O
.	O
the	O
peak	O
is	O
produced	O
by	O
the	O
gas	O
having	O
magnetic	O
degrees	O
of	O
freedom	O
with	O
a	O
(	O
cid:12	O
)	O
nite	O
number	O
of	O
accessible	O
states	O
.	O
404	O
n	O
mean	B
energy	O
and	O
(	O
cid:13	O
)	O
uctuations	O
mean	B
square	O
magnetization	O
0.5	O
0	O
-0.5	O
-1	O
-1.5	O
-2	O
0.5	O
0	O
-0.5	O
-1	O
-1.5	O
-2	O
y	O
g	O
r	O
e	O
n	O
e	O
y	O
g	O
r	O
e	O
n	O
e	O
16	O
100	O
1	O
1	O
temperature	B
10	O
temperature	B
10	O
n	O
o	O
i	O
t	O
a	O
z	O
i	O
t	O
e	O
n	O
g	O
a	O
m	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
n	O
o	O
i	O
t	O
a	O
z	O
i	O
t	O
e	O
n	O
g	O
a	O
m	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
1	O
1	O
temperature	B
10	O
temperature	B
10	O
contrast	O
with	O
schottky	O
anomaly	O
a	O
peak	O
in	O
the	O
heat	B
capacity	I
,	O
as	O
a	O
function	B
of	O
temperature	B
,	O
occurs	O
in	O
any	O
system	O
that	O
has	O
a	O
(	O
cid:12	O
)	O
nite	O
number	O
of	O
energy	O
levels	O
;	O
a	O
peak	O
is	O
not	O
in	O
itself	O
evidence	B
of	O
a	O
phase	B
transition	I
.	O
such	O
peaks	O
were	O
viewed	O
as	O
anomalies	O
in	O
classical	O
thermody-	O
namics	O
,	O
since	O
‘	O
normal	B
’	O
systems	O
with	O
in	O
(	O
cid:12	O
)	O
nite	O
numbers	O
of	O
energy	O
levels	O
(	O
such	O
as	O
a	O
particle	O
in	O
a	O
box	B
)	O
have	O
heat	O
capacities	O
that	O
are	O
either	O
constant	O
or	O
increasing	O
functions	B
of	O
temperature	B
.	O
in	O
contrast	O
,	O
systems	O
with	O
a	O
(	O
cid:12	O
)	O
nite	O
number	O
of	O
levels	O
produced	O
small	O
blips	O
in	O
the	O
heat	B
capacity	I
graph	O
(	O
(	O
cid:12	O
)	O
gure	O
31.4	O
)	O
.	O
let	O
us	O
refresh	O
our	O
memory	B
of	O
the	O
simplest	O
such	O
system	O
,	O
a	O
two-level	O
system	O
with	O
states	O
x	O
=	O
0	O
(	O
energy	B
0	O
)	O
and	O
x	O
=	O
1	O
(	O
energy	B
(	O
cid:15	O
)	O
)	O
.	O
the	O
mean	B
energy	O
is	O
e	O
(	O
(	O
cid:12	O
)	O
)	O
=	O
(	O
cid:15	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
(	O
cid:15	O
)	O
)	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
(	O
cid:15	O
)	O
)	O
=	O
(	O
cid:15	O
)	O
1	O
1	O
+	O
exp	O
(	O
(	O
cid:12	O
)	O
(	O
cid:15	O
)	O
)	O
and	O
the	O
derivative	O
with	O
respect	O
to	O
(	O
cid:12	O
)	O
is	O
de=d	O
(	O
cid:12	O
)	O
=	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
2	O
exp	O
(	O
(	O
cid:12	O
)	O
(	O
cid:15	O
)	O
)	O
[	O
1	O
+	O
exp	O
(	O
(	O
cid:12	O
)	O
(	O
cid:15	O
)	O
)	O
]	O
2	O
:	O
so	O
the	O
heat	B
capacity	I
is	O
c	O
=	O
de=dt	O
=	O
(	O
cid:0	O
)	O
de	O
d	O
(	O
cid:12	O
)	O
1	O
kbt	O
2	O
=	O
(	O
cid:15	O
)	O
2	O
exp	O
(	O
(	O
cid:12	O
)	O
(	O
cid:15	O
)	O
)	O
kbt	O
2	O
[	O
1	O
+	O
exp	O
(	O
(	O
cid:12	O
)	O
(	O
cid:15	O
)	O
)	O
]	O
2	O
(	O
31.22	O
)	O
(	O
31.23	O
)	O
(	O
31.24	O
)	O
and	O
the	O
(	O
cid:13	O
)	O
uctuations	O
in	O
energy	O
are	O
given	O
by	O
var	O
(	O
e	O
)	O
=	O
ckbt	O
2	O
=	O
(	O
cid:0	O
)	O
de=d	O
(	O
cid:12	O
)	O
,	O
which	O
was	O
evaluated	O
in	O
(	O
31.23	O
)	O
.	O
the	O
heat	B
capacity	I
and	O
(	O
cid:13	O
)	O
uctuations	O
are	O
plotted	O
in	O
(	O
cid:12	O
)	O
gure	O
31.6.	O
the	O
take-home	O
message	O
at	O
this	O
point	O
is	O
that	O
whilst	O
schottky	O
anomalies	O
do	O
have	O
a	O
peak	O
in	O
the	O
heat	B
capacity	I
,	O
there	O
is	O
no	O
peak	O
in	O
their	O
(	O
cid:13	O
)	O
uctuations	O
;	O
the	O
variance	B
of	O
the	O
energy	B
simply	O
increases	O
monotonically	O
with	O
temperature	O
to	O
a	O
value	O
proportional	O
to	O
the	O
number	O
of	O
independent	O
spins	O
.	O
thus	O
it	O
is	O
a	O
peak	O
in	O
the	O
(	O
cid:13	O
)	O
uctuations	O
that	O
is	O
interesting	O
,	O
rather	O
than	O
a	O
peak	O
in	O
the	O
heat	B
capacity	I
.	O
the	O
ising	O
model	B
has	O
such	O
a	O
peak	O
in	O
its	O
(	O
cid:13	O
)	O
uctuations	O
,	O
as	O
can	O
be	O
seen	O
in	O
the	O
second	O
row	O
of	O
(	O
cid:12	O
)	O
gure	O
31.5.	O
rectangular	B
ising	O
model	B
with	O
j	O
=	O
(	O
cid:0	O
)	O
1	O
what	O
do	O
we	O
expect	O
to	O
happen	O
in	O
the	O
case	O
j	O
=	O
(	O
cid:0	O
)	O
1	O
?	O
the	O
ground	O
states	O
of	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
system	O
are	O
the	O
two	O
checkerboard	B
patterns	O
(	O
(	O
cid:12	O
)	O
gure	O
31.7	O
)	O
,	O
and	O
they	O
have	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
31.1	O
:	O
ising	O
models	O
{	O
monte	O
carlo	O
simulation	O
405	O
y	O
g	O
r	O
e	O
n	O
e	O
(	O
a	O
)	O
y	O
g	O
r	O
e	O
n	O
e	O
f	O
o	O
d	O
s	O
(	O
b	O
)	O
n	O
o	O
i	O
t	O
a	O
z	O
i	O
t	O
e	O
n	O
g	O
a	O
m	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
(	O
c	O
)	O
y	O
t	O
i	O
c	O
a	O
p	O
a	O
c	O
t	O
a	O
e	O
h	O
(	O
d	O
)	O
figure	O
31.5.	O
detail	O
of	O
monte	O
carlo	O
simulations	O
of	O
rectangular	O
ising	O
models	O
with	O
j	O
=	O
1	O
.	O
(	O
a	O
)	O
mean	B
energy	O
and	O
(	O
cid:13	O
)	O
uctuations	O
in	O
energy	O
as	O
a	O
function	B
of	O
temperature	B
.	O
(	O
b	O
)	O
fluctuations	O
in	O
energy	O
(	O
standard	B
deviation	I
)	O
.	O
(	O
c	O
)	O
mean	B
square	O
magnetization	O
.	O
(	O
d	O
)	O
heat	B
capacity	I
.	O
0	O
-0.5	O
-1	O
-1.5	O
-2	O
0.28	O
0.26	O
0.24	O
0.22	O
0.2	O
0.18	O
0.16	O
0.14	O
0.12	O
0.1	O
0.08	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
1.6	O
1.4	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
n	O
=	O
100	O
n	O
=	O
4096	O
0	O
-0.5	O
-1	O
-1.5	O
-2	O
2	O
2.5	O
3	O
3.5	O
4	O
4.5	O
5	O
2	O
2.5	O
3	O
3.5	O
4	O
4.5	O
5	O
0.05	O
0.045	O
0.04	O
0.035	O
0.03	O
0.025	O
0.02	O
0.015	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
1.8	O
1.6	O
1.4	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
2	O
2.5	O
3	O
3.5	O
4	O
4.5	O
5	O
2	O
2.5	O
3	O
3.5	O
4	O
4.5	O
5	O
2	O
2.5	O
3	O
3.5	O
4	O
4.5	O
5	O
2	O
2.5	O
3	O
3.5	O
4	O
4.5	O
5	O
2	O
2.5	O
3	O
3.5	O
4	O
4.5	O
5	O
2	O
2.5	O
3	O
3.5	O
4	O
4.5	O
5	O
0.45	O
0.4	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
0.1	O
heat	B
capacity	I
var	O
(	O
e	O
)	O
figure	O
31.6.	O
schottky	O
anomaly	O
{	O
heat	B
capacity	I
and	O
(	O
cid:13	O
)	O
uctuations	O
in	O
energy	O
as	O
a	O
function	B
of	O
temperature	B
for	O
a	O
two-level	O
system	O
with	O
separation	O
(	O
cid:15	O
)	O
=	O
1	O
and	O
kb	O
=	O
1	O
.	O
1	O
temperature	B
10	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
406	O
31	O
|	O
ising	O
models	O
energy	B
per	O
spin	O
(	O
cid:0	O
)	O
2	O
,	O
like	O
the	O
ground	O
states	O
of	O
the	O
j	O
=	O
1	O
model	B
.	O
can	O
this	O
analogy	O
be	O
pressed	O
further	O
?	O
a	O
moment	O
’	O
s	O
re	O
(	O
cid:13	O
)	O
ection	O
will	O
con	O
(	O
cid:12	O
)	O
rm	O
that	O
the	O
two	O
systems	O
are	O
equivalent	O
to	O
each	O
other	O
under	O
a	O
checkerboard	B
symmetry	O
operation	O
.	O
if	O
you	O
take	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
j	O
=	O
1	O
system	O
in	O
some	O
state	O
and	O
(	O
cid:13	O
)	O
ip	O
all	O
the	O
spins	O
that	O
lie	O
on	O
the	O
black	B
squares	O
of	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
checkerboard	B
,	O
and	O
set	O
j	O
=	O
(	O
cid:0	O
)	O
1	O
(	O
(	O
cid:12	O
)	O
gure	O
31.8	O
)	O
,	O
then	O
the	O
energy	B
is	O
unchanged	O
.	O
(	O
the	O
magnetization	O
changes	O
,	O
of	O
course	O
.	O
)	O
so	O
all	O
thermodynamic	O
properties	O
of	O
the	O
two	O
systems	O
are	O
expected	O
to	O
be	O
identical	O
in	O
the	O
case	O
of	O
zero	O
applied	O
(	O
cid:12	O
)	O
eld	O
.	O
but	O
there	O
is	O
a	O
subtlety	O
lurking	O
here	O
.	O
have	O
you	O
spotted	O
it	O
?	O
we	O
are	O
simu-	O
lating	O
(	O
cid:12	O
)	O
nite	O
grids	O
with	O
periodic	O
boundary	O
conditions	O
.	O
if	O
the	O
size	O
of	O
the	O
grid	O
in	O
any	O
direction	O
is	O
odd	O
,	O
then	O
the	O
checkerboard	B
operation	O
is	O
no	O
longer	O
a	O
symme-	O
try	O
operation	O
relating	O
j	O
=	O
+1	O
to	O
j	O
=	O
(	O
cid:0	O
)	O
1	O
,	O
because	O
the	O
checkerboard	B
doesn	O
’	O
t	O
match	O
up	O
at	O
the	O
boundaries	O
.	O
this	O
means	O
that	O
for	O
systems	O
of	O
odd	O
size	O
,	O
the	O
ground	O
state	O
of	O
a	O
system	O
with	O
j	O
=	O
(	O
cid:0	O
)	O
1	O
will	O
have	O
degeneracy	O
greater	O
than	O
2	O
,	O
and	O
the	O
energy	B
of	O
those	O
ground	O
states	O
will	O
not	O
be	O
as	O
low	O
as	O
(	O
cid:0	O
)	O
2	O
per	O
spin	O
.	O
so	O
we	O
expect	O
qualitative	O
di	O
(	O
cid:11	O
)	O
erences	O
between	O
the	O
cases	O
j	O
=	O
(	O
cid:6	O
)	O
1	O
in	O
odd-sized	O
systems	O
.	O
these	O
di	O
(	O
cid:11	O
)	O
erences	O
are	O
expected	O
to	O
be	O
most	O
prominent	O
for	O
small	O
systems	O
.	O
the	O
frustrations	O
are	O
introduced	O
by	O
the	O
boundaries	O
,	O
and	O
the	O
length	B
of	O
the	O
boundary	O
grows	O
as	O
the	O
square	B
root	O
of	O
the	O
system	O
size	O
,	O
so	O
the	O
fractional	O
in	O
(	O
cid:13	O
)	O
uence	O
of	O
this	O
boundary-related	O
frustration	B
on	O
the	O
energy	B
and	O
entropy	B
of	O
the	O
system	O
will	O
de-	O
crease	O
as	O
1=pn	O
.	O
figure	O
31.9	O
compares	O
the	O
energies	O
of	O
the	O
ferromagnetic	O
and	O
antiferromagnetic	O
models	O
with	O
n	O
=	O
25.	O
here	O
,	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
is	O
striking	O
.	O
figure	O
31.7.	O
the	O
two	O
ground	O
states	O
of	O
a	O
rectangular	B
ising	O
model	B
with	O
j	O
=	O
(	O
cid:0	O
)	O
1.	O
j	O
=	O
(	O
cid:0	O
)	O
1	O
j	O
=	O
+1	O
figure	O
31.8.	O
two	O
states	O
of	O
rectangular	O
ising	O
models	O
with	O
j	O
=	O
(	O
cid:6	O
)	O
1	O
that	O
have	O
identical	O
energy	O
.	O
j	O
=	O
+1	O
y	O
g	O
r	O
e	O
n	O
e	O
0.5	O
0	O
-0.5	O
-1	O
-1.5	O
-2	O
0.5	O
0	O
-0.5	O
-1	O
-1.5	O
-2	O
j	O
=	O
(	O
cid:0	O
)	O
1	O
figure	O
31.9.	O
monte	O
carlo	O
simulations	O
of	O
rectangular	O
ising	O
models	O
with	O
j	O
=	O
(	O
cid:6	O
)	O
1	O
and	O
n	O
=	O
25.	O
mean	B
energy	O
and	O
(	O
cid:13	O
)	O
uctuations	O
in	O
energy	O
as	O
a	O
function	B
of	O
temperature	B
.	O
1	O
temperature	B
10	O
1	O
temperature	B
10	O
triangular	O
ising	O
model	B
we	O
can	O
repeat	O
these	O
computations	O
for	O
a	O
triangular	O
ising	O
model	B
.	O
do	O
we	O
expect	O
the	O
triangular	O
ising	O
model	B
with	O
j	O
=	O
(	O
cid:6	O
)	O
1	O
to	O
show	O
di	O
(	O
cid:11	O
)	O
erent	O
physical	O
properties	O
from	O
the	O
rectangular	B
ising	O
model	B
?	O
presumably	O
the	O
j	O
=	O
1	O
model	B
will	O
have	O
broadly	O
similar	O
properties	O
to	O
its	O
rectangular	B
counterpart	O
.	O
but	O
the	O
case	O
j	O
=	O
(	O
cid:0	O
)	O
1	O
is	O
radically	O
di	O
(	O
cid:11	O
)	O
erent	O
from	O
what	O
’	O
s	O
gone	O
before	O
.	O
think	O
about	O
it	O
:	O
there	O
is	O
no	O
unfrustrated	O
ground	O
state	O
;	O
in	O
any	O
state	O
,	O
there	O
must	O
be	O
frustrations	O
{	O
pairs	O
of	O
neighbours	O
who	O
have	O
the	O
same	O
sign	O
as	O
each	O
other	O
.	O
unlike	O
the	O
case	O
of	O
the	O
rectangular	O
model	B
with	O
odd	O
size	O
,	O
the	O
frustrations	O
are	O
not	O
introduced	O
by	O
the	O
periodic	O
boundary	O
conditions	O
.	O
every	O
set	B
of	O
three	O
mutually	O
neighbouring	O
spins	O
must	O
be	O
in	O
a	O
state	O
of	O
frustration	B
,	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
31.10	O
.	O
(	O
solid	O
lines	O
show	O
‘	O
happy	O
’	O
couplings	O
which	O
contribute	O
(	O
cid:0	O
)	O
jjj	O
to	O
the	O
energy	B
;	O
dashed	O
lines	O
show	O
‘	O
unhappy	O
’	O
couplings	O
which	O
contribute	O
jjj	O
.	O
)	O
thus	O
we	O
certainly	O
expect	O
di	O
(	O
cid:11	O
)	O
erent	O
behaviour	O
at	O
low	O
temperatures	O
.	O
in	O
fact	O
we	O
might	O
expect	O
this	O
system	O
to	O
have	O
a	O
non-zero	O
entropy	B
at	O
absolute	O
zero	O
.	O
(	O
‘	O
triangular	O
model	B
violates	O
third	B
law	I
of	I
thermodynamics	I
!	O
’	O
)	O
let	O
’	O
s	O
look	O
at	O
some	O
results	O
.	O
sample	B
states	O
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
31.12	O
,	O
and	O
(	O
cid:12	O
)	O
gure	O
31.11	O
shows	O
the	O
energy	B
,	O
(	O
cid:13	O
)	O
uctuations	O
,	O
and	O
heat	O
capacity	B
for	O
n	O
=	O
4096	O
.	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
-1	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
+1	O
+1	O
+1	O
+1	O
+1	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
31.10.	O
in	O
an	O
antiferromagnetic	B
triangular	O
ising	O
model	B
,	O
any	O
three	O
neighbouring	O
spins	O
are	O
frustrated	O
.	O
of	O
the	O
eight	O
possible	O
con	O
(	O
cid:12	O
)	O
gurations	O
of	O
three	O
spins	O
,	O
six	B
have	O
energy	B
(	O
cid:0	O
)	O
jjj	O
(	O
a	O
)	O
,	O
and	O
two	O
have	O
energy	B
3jjj	O
(	O
b	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
31.2	O
:	O
direct	O
computation	O
of	O
partition	O
function	B
of	O
ising	O
models	O
407	O
note	O
how	O
di	O
(	O
cid:11	O
)	O
erent	O
the	O
results	O
for	O
j	O
=	O
(	O
cid:6	O
)	O
1	O
are	O
.	O
there	O
is	O
no	O
peak	O
at	O
all	O
in	O
the	O
standard	B
deviation	I
of	O
the	O
energy	B
in	O
the	O
case	O
j	O
=	O
(	O
cid:0	O
)	O
1.	O
this	O
indicates	O
that	O
the	O
antiferromagnetic	B
system	O
does	O
not	O
have	O
a	O
phase	B
transition	I
to	O
a	O
state	O
with	O
long-range	O
order	O
.	O
0.5	O
0	O
-0.5	O
-1	O
-1.5	O
-2	O
-2.5	O
-3	O
0.08	O
0.07	O
0.06	O
0.05	O
0.04	O
0.03	O
0.02	O
0.01	O
0	O
1.6	O
1.4	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
y	O
g	O
r	O
e	O
n	O
e	O
(	O
a	O
)	O
y	O
g	O
r	O
e	O
n	O
e	O
f	O
o	O
d	O
s	O
(	O
b	O
)	O
y	O
t	O
i	O
c	O
a	O
p	O
a	O
c	O
t	O
a	O
e	O
h	O
(	O
c	O
)	O
1	O
1	O
1	O
j	O
=	O
+1	O
10	O
temperature	B
10	O
temperature	B
10	O
temperature	B
0.5	O
0	O
-0.5	O
-1	O
-1.5	O
-2	O
-2.5	O
-3	O
0.03	O
0.025	O
0.02	O
0.015	O
0.01	O
0.005	O
0	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
y	O
g	O
r	O
e	O
n	O
e	O
(	O
d	O
)	O
y	O
g	O
r	O
e	O
n	O
e	O
f	O
o	O
d	O
s	O
(	O
e	O
)	O
y	O
t	O
i	O
c	O
a	O
p	O
a	O
c	O
t	O
a	O
e	O
h	O
(	O
f	O
)	O
1	O
1	O
1	O
j	O
=	O
(	O
cid:0	O
)	O
1	O
10	O
temperature	B
10	O
temperature	B
10	O
temperature	B
31.2	O
direct	O
computation	O
of	O
partition	O
function	B
of	O
ising	O
models	O
we	O
now	O
examine	O
a	O
completely	O
di	O
(	O
cid:11	O
)	O
erent	O
approach	O
to	O
ising	O
models	O
.	O
the	O
trans-	O
fer	O
matrix	B
method	O
is	O
an	O
exact	O
and	O
abstract	O
approach	O
that	O
obtains	O
physical	O
properties	O
of	O
the	O
model	O
from	O
the	O
partition	B
function	I
z	O
(	O
(	O
cid:12	O
)	O
;	O
j	O
;	O
b	O
)	O
(	O
cid:17	O
)	O
xx	O
exp	O
[	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
;	O
j	O
;	O
b	O
)	O
]	O
;	O
(	O
31.25	O
)	O
[	O
as	O
usual	O
,	O
let	O
kb	O
=	O
1	O
.	O
]	O
the	O
free	B
energy	I
is	O
given	O
by	O
f	O
=	O
(	O
cid:0	O
)	O
1	O
where	O
the	O
summation	O
is	O
over	O
all	O
states	O
x	O
,	O
and	O
the	O
inverse	O
temperature	O
is	O
(	O
cid:12	O
)	O
ln	O
z	O
.	O
(	O
cid:12	O
)	O
=	O
1=t	O
.	O
the	O
number	O
of	O
states	O
is	O
2n	O
,	O
so	O
direct	O
computation	O
of	O
the	O
partition	O
function	B
is	O
not	O
possible	O
for	O
large	O
n	O
.	O
to	O
avoid	O
enumerating	O
all	O
global	O
states	O
explicitly	O
,	O
we	O
can	O
use	O
a	O
trick	O
similar	O
to	O
the	O
sum	O
{	O
product	O
algorithm	O
discussed	O
in	O
chapter	O
25.	O
we	O
concentrate	O
on	O
models	O
that	O
have	O
the	O
form	O
of	O
a	O
long	B
thin	I
strip	I
of	O
width	O
w	O
with	O
periodic	O
boundary	O
conditions	O
in	O
both	O
directions	O
,	O
and	O
we	O
iterate	O
along	O
the	O
length	B
of	O
our	O
model	B
,	O
working	O
out	O
a	O
set	B
of	O
partial	B
partition	I
functions	I
at	O
one	O
location	O
l	O
in	O
terms	O
of	O
partial	O
partition	B
functions	O
at	O
the	O
previous	O
location	O
l	O
(	O
cid:0	O
)	O
1.	O
each	O
iteration	O
involves	O
a	O
summation	O
over	O
all	O
the	O
states	O
at	O
the	O
boundary	O
.	O
this	O
operation	O
is	O
exponential	B
in	O
the	O
width	O
of	O
the	O
strip	O
,	O
w	O
.	O
the	O
(	O
cid:12	O
)	O
nal	O
clever	O
trick	O
figure	O
31.11.	O
monte	O
carlo	O
simulations	O
of	O
triangular	O
ising	O
models	O
with	O
j	O
=	O
(	O
cid:6	O
)	O
1	O
and	O
n	O
=	O
4096	O
.	O
(	O
a	O
{	O
c	O
)	O
j	O
=	O
1	O
.	O
(	O
d	O
{	O
f	O
)	O
j	O
=	O
(	O
cid:0	O
)	O
1	O
.	O
(	O
a	O
,	O
d	O
)	O
mean	B
energy	O
and	O
(	O
cid:13	O
)	O
uctuations	O
in	O
energy	O
as	O
a	O
function	B
of	O
temperature	B
.	O
(	O
b	O
,	O
e	O
)	O
fluctuations	O
in	O
energy	O
(	O
standard	B
deviation	I
)	O
.	O
(	O
c	O
,	O
f	O
)	O
heat	B
capacity	I
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
408	O
31	O
|	O
ising	O
models	O
t	O
j	O
=	O
+1	O
t	O
j	O
=	O
(	O
cid:0	O
)	O
1	O
50	O
5	O
2	O
0.5	O
20	O
6	O
4	O
3	O
2	O
figure	O
31.12.	O
sample	B
states	O
of	O
triangular	O
ising	O
models	O
with	O
j	O
=	O
1	O
and	O
j	O
=	O
(	O
cid:0	O
)	O
1.	O
high	O
temperatures	O
at	O
the	O
top	O
;	O
low	O
at	O
the	O
bottom	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
31.2	O
:	O
direct	O
computation	O
of	O
partition	O
function	B
of	O
ising	O
models	O
409	O
is	O
to	O
note	O
that	O
if	O
the	O
system	O
is	O
translation-invariant	B
along	O
its	O
length	B
then	O
we	O
need	O
to	O
do	O
only	O
one	O
iteration	O
in	O
order	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
properties	O
of	O
a	O
system	O
of	O
any	O
length	O
.	O
the	O
computational	O
task	O
becomes	O
the	O
evaluation	O
of	O
an	O
s	O
(	O
cid:2	O
)	O
s	O
matrix	B
,	O
where	O
s	O
is	O
the	O
number	O
of	O
microstates	O
that	O
need	O
to	O
be	O
considered	O
at	O
the	O
boundary	O
,	O
and	O
the	O
computation	O
of	O
its	O
eigenvalues	O
.	O
the	O
eigenvalue	B
of	O
largest	O
magnitude	O
gives	O
the	O
partition	B
function	I
for	O
an	O
in	O
(	O
cid:12	O
)	O
nite-length	O
thin	O
strip	O
.	O
here	O
is	O
a	O
more	O
detailed	O
explanation	O
.	O
label	O
the	O
states	O
of	O
the	O
c	O
columns	O
of	O
the	O
thin	O
strip	O
s1	O
;	O
s2	O
;	O
:	O
:	O
:	O
;	O
sc	O
,	O
with	O
each	O
s	O
an	O
integer	O
from	O
0	O
to	O
2w	O
(	O
cid:0	O
)	O
1.	O
the	O
rth	O
bit	B
of	O
sc	O
indicates	O
whether	O
the	O
spin	O
in	O
row	O
r	O
,	O
column	O
c	O
is	O
up	O
or	O
down	O
.	O
the	O
partition	B
function	I
is	O
z	O
=	O
xx	O
=	O
xs1	O
xs2	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
)	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
xsc	O
exp	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
c	O
xc=1	O
(	O
31.26	O
)	O
(	O
31.27	O
)	O
e	O
(	O
sc	O
;	O
sc+1	O
)	O
!	O
;	O
where	O
e	O
(	O
sc	O
;	O
sc+1	O
)	O
is	O
an	O
appropriately	O
de	O
(	O
cid:12	O
)	O
ned	O
energy	B
,	O
and	O
,	O
if	O
we	O
want	O
periodic	O
boundary	O
conditions	O
,	O
sc+1	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
s1	O
.	O
one	O
de	O
(	O
cid:12	O
)	O
nition	O
for	O
e	O
is	O
:	O
j	O
xmxn	O
+	O
1	O
j	O
xmxn	O
+	O
1	O
j	O
xmxn	O
:	O
(	O
31.28	O
)	O
e	O
(	O
sc	O
;	O
sc+1	O
)	O
=	O
x	O
(	O
m	O
;	O
n	O
)	O
2n	O
:	O
m2c	O
;	O
n2c+1	O
4	O
x	O
(	O
m	O
;	O
n	O
)	O
2n	O
:	O
m2c	O
;	O
n2c	O
4	O
x	O
(	O
m	O
;	O
n	O
)	O
2n	O
:	O
m2c+1	O
;	O
n2c+1	O
this	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
the	O
energy	O
has	O
the	O
nice	O
property	O
that	O
(	O
for	O
the	O
rectangular	B
ising	O
model	B
)	O
it	O
de	O
(	O
cid:12	O
)	O
nes	O
a	O
matrix	B
that	O
is	O
symmetric	B
in	O
its	O
two	O
indices	O
sc	O
;	O
sc+1	O
.	O
the	O
factors	O
of	O
1=4	O
are	O
needed	O
because	O
vertical	O
links	O
are	O
counted	O
four	O
times	O
.	O
let	O
us	O
de	O
(	O
cid:12	O
)	O
ne	O
+	O
(	O
cid:0	O
)	O
+	O
+	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
s2	O
+	O
+	O
(	O
cid:0	O
)	O
s3	O
+	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
+	O
+	O
+	O
figure	O
31.13.	O
illustration	O
to	O
help	O
explain	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
(	O
31.28	O
)	O
.	O
e	O
(	O
s2	O
;	O
s3	O
)	O
counts	O
all	O
the	O
contributions	O
to	O
the	O
energy	B
in	O
the	O
rectangle	O
.	O
the	O
total	O
energy	B
is	O
given	O
by	O
stepping	O
the	O
rectangle	O
along	O
.	O
each	O
horizontal	O
bond	O
inside	O
the	O
rectangle	O
is	O
counted	O
once	O
;	O
each	O
vertical	O
bond	O
is	O
half-inside	O
the	O
rectangle	O
(	O
and	O
will	O
be	O
half-inside	O
an	O
adjacent	O
rectangle	O
)	O
so	O
half	O
its	O
energy	B
is	O
included	O
in	O
e	O
(	O
s2	O
;	O
s3	O
)	O
;	O
the	O
factor	O
of	O
1=4	O
appears	O
in	O
the	O
second	O
term	O
because	O
m	O
and	O
n	O
both	O
run	O
over	O
all	O
nodes	O
in	O
column	O
c	O
,	O
so	O
each	O
bond	O
is	O
visited	O
twice	O
.	O
for	O
the	O
state	O
shown	O
here	O
,	O
s2	O
=	O
(	O
100	O
)	O
2	O
,	O
s3	O
=	O
(	O
110	O
)	O
2	O
,	O
the	O
horizontal	O
bonds	O
contribute	O
+j	O
to	O
e	O
(	O
s2	O
;	O
s3	O
)	O
,	O
and	O
the	O
vertical	O
bonds	O
contribute	O
(	O
cid:0	O
)	O
j=2	O
on	O
the	O
left	O
and	O
(	O
cid:0	O
)	O
j=2	O
on	O
the	O
right	O
,	O
assuming	O
periodic	O
boundary	O
conditions	O
between	O
top	O
and	O
bottom	O
.	O
so	O
e	O
(	O
s2	O
;	O
s3	O
)	O
=	O
0.	O
then	O
continuing	O
from	O
equation	O
(	O
31.27	O
)	O
,	O
mss0	O
=	O
exp	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
s	O
;	O
s0	O
)	O
(	O
cid:1	O
)	O
:	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
xsc	O
``	O
c	O
yc=1	O
msc	O
;	O
sc+1	O
#	O
z	O
=	O
xs1	O
xs2	O
=	O
trace	O
(	O
cid:2	O
)	O
mc	O
(	O
cid:3	O
)	O
=	O
xa	O
(	O
cid:22	O
)	O
c	O
a	O
;	O
(	O
31.29	O
)	O
(	O
31.30	O
)	O
(	O
31.31	O
)	O
(	O
31.32	O
)	O
where	O
f	O
(	O
cid:22	O
)	O
ag2w	O
z	O
becomes	O
dominated	O
by	O
the	O
largest	O
eigenvalue	B
(	O
cid:22	O
)	O
max	O
:	O
a=1	O
are	O
the	O
eigenvalues	O
of	O
m.	O
as	O
the	O
length	B
of	O
the	O
strip	O
c	O
increases	O
,	O
z	O
!	O
(	O
cid:22	O
)	O
c	O
max	O
:	O
(	O
31.33	O
)	O
so	O
the	O
free	B
energy	I
per	O
spin	O
in	O
the	O
limit	O
of	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
thin	O
strip	O
is	O
given	O
by	O
:	O
f	O
=	O
(	O
cid:0	O
)	O
kt	O
ln	O
z=	O
(	O
w	O
c	O
)	O
=	O
(	O
cid:0	O
)	O
kt	O
c	O
ln	O
(	O
cid:22	O
)	O
max=	O
(	O
w	O
c	O
)	O
=	O
(	O
cid:0	O
)	O
kt	O
ln	O
(	O
cid:22	O
)	O
max=w	O
:	O
(	O
31.34	O
)	O
it	O
’	O
s	O
really	O
neat	O
that	O
all	O
the	O
thermodynamic	O
properties	O
of	O
a	O
long	B
thin	I
strip	I
can	O
be	O
obtained	O
from	O
just	O
the	O
largest	O
eigenvalue	B
of	O
this	O
matrix	B
m	O
!	O
computations	O
i	O
computed	O
the	O
partition	B
functions	O
of	O
long-thin-strip	O
ising	O
models	O
with	O
the	O
geometries	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
31.14.	O
as	O
in	O
the	O
last	O
section	B
,	O
i	O
set	B
the	O
applied	O
(	O
cid:12	O
)	O
eld	O
h	O
to	O
zero	O
and	O
considered	O
the	O
two	O
cases	O
j	O
=	O
(	O
cid:6	O
)	O
1	O
which	O
are	O
a	O
ferromagnet	O
and	O
antiferromagnet	O
respectively	O
.	O
i	O
computed	O
the	O
free	B
energy	I
per	O
spin	O
,	O
f	O
(	O
(	O
cid:12	O
)	O
;	O
j	O
;	O
h	O
)	O
=	O
f=n	O
for	O
widths	O
from	O
w	O
=	O
2	O
to	O
8	O
as	O
a	O
function	B
of	O
(	O
cid:12	O
)	O
for	O
h	O
=	O
0.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
410	O
6	O
w	O
?	O
bbbb	O
bbbb	O
bbbb	O
bbbb	O
bbbb	O
computational	O
ideas	O
:	O
triangular	O
:	O
bbbb	O
bbbb	O
bbbb	O
6	O
w	O
?	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
bbbb	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
bbbb	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
31	O
|	O
ising	O
models	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
hh	O
bbbb	O
bbbb	O
rectangular	B
:	O
bbbb	O
bbbb	O
bbbb	O
figure	O
31.14.	O
two	O
long-thin-strip	O
ising	O
models	O
.	O
a	O
line	O
between	O
two	O
spins	O
indicates	O
that	O
they	O
are	O
neighbours	O
.	O
the	O
strips	O
have	O
width	O
w	O
and	O
in	O
(	O
cid:12	O
)	O
nite	O
length	B
.	O
only	O
the	O
largest	O
eigenvalue	B
is	O
needed	O
.	O
there	O
are	O
several	O
ways	O
of	O
getting	O
this	O
quantity	O
,	O
for	O
example	O
,	O
iterative	O
multiplication	O
of	O
the	O
matrix	O
by	O
an	O
initial	O
vec-	O
tor	O
.	O
because	O
the	O
matrix	B
is	O
all	O
positive	O
we	O
know	O
that	O
the	O
principal	O
eigenvector	O
is	O
all	O
positive	O
too	O
(	O
frobenius	O
{	O
perron	O
theorem	B
)	O
,	O
so	O
a	O
reasonable	O
initial	O
vector	O
is	O
(	O
1	O
;	O
1	O
;	O
:	O
:	O
:	O
;	O
1	O
)	O
.	O
this	O
iterative	O
procedure	O
may	O
be	O
faster	O
than	O
explicit	O
computation	O
of	O
all	O
eigenvalues	O
.	O
i	O
computed	O
them	O
all	O
anyway	O
,	O
which	O
has	O
the	O
advantage	O
that	O
we	O
can	O
(	O
cid:12	O
)	O
nd	O
the	O
free	B
energy	I
of	O
(	O
cid:12	O
)	O
nite	O
length	B
strips	O
{	O
using	O
equation	O
(	O
31.32	O
)	O
{	O
as	O
well	O
as	O
in	O
(	O
cid:12	O
)	O
nite	O
ones	O
.	O
ferromagnets	O
of	O
width	O
8	O
triangular	O
rectangular	B
y	O
g	O
r	O
e	O
n	O
e	O
e	O
e	O
r	O
f	O
-1	O
-2	O
-3	O
-4	O
-5	O
-6	O
-7	O
0	O
2	O
6	O
4	O
temperature	B
8	O
10	O
-1	O
-2	O
-3	O
-4	O
-5	O
-6	O
-7	O
-8	O
antiferromagnets	O
of	O
width	O
8	O
triangular	O
rectangular	B
figure	O
31.15.	O
free	B
energy	I
per	O
spin	O
of	O
long-thin-strip	O
ising	O
models	O
.	O
note	O
the	O
non-zero	O
gradient	O
at	O
t	O
=	O
0	O
in	O
the	O
case	O
of	O
the	O
triangular	O
antiferromagnet	O
.	O
0	O
2	O
4	O
6	O
temperature	B
8	O
10	O
comments	O
on	O
graphs	O
:	O
for	O
large	O
temperatures	O
all	O
ising	O
models	O
should	O
show	O
the	O
same	O
behaviour	O
:	O
the	O
free	B
energy	I
is	O
entropy-dominated	O
,	O
and	O
the	O
entropy	B
per	O
spin	O
is	O
ln	O
(	O
2	O
)	O
.	O
the	O
mean	B
energy	O
per	O
spin	O
goes	O
to	O
zero	O
.	O
the	O
free	B
energy	I
per	O
spin	O
should	O
tend	O
to	O
(	O
cid:0	O
)	O
ln	O
(	O
2	O
)	O
=	O
(	O
cid:12	O
)	O
.	O
the	O
free	O
energies	O
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
31.15.	O
one	O
of	O
the	O
interesting	O
properties	O
we	O
can	O
obtain	O
from	O
the	O
free	B
energy	I
is	O
the	O
degeneracy	O
of	O
the	O
ground	O
state	O
.	O
as	O
the	O
temperature	B
goes	O
to	O
zero	O
,	O
the	O
boltzmann	O
distribution	B
becomes	O
concentrated	O
in	O
the	O
ground	O
state	O
.	O
if	O
the	O
ground	O
state	O
is	O
degenerate	O
(	O
i.e.	O
,	O
there	O
are	O
multiple	O
ground	O
states	O
with	O
identical	O
y	O
p	O
o	O
r	O
t	O
n	O
e	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
triangular	O
(	O
-	O
)	O
rectangular	B
triangular	O
(	O
+	O
)	O
2	O
4	O
6	O
temperature	B
8	O
10	O
figure	O
31.16.	O
entropies	O
(	O
in	O
nats	O
)	O
of	O
width	O
8	O
ising	O
systems	O
as	O
a	O
function	B
of	O
temperature	B
,	O
obtained	O
by	O
di	O
(	O
cid:11	O
)	O
erentiating	O
the	O
free	B
energy	I
curves	O
in	O
(	O
cid:12	O
)	O
gure	O
31.15.	O
the	O
rectangular	B
ferromagnet	O
and	O
antiferromagnet	O
have	O
identical	O
thermal	O
properties	O
.	O
for	O
the	O
triangular	O
systems	O
,	O
the	O
upper	O
curve	O
(	O
(	O
cid:0	O
)	O
)	O
denotes	O
the	O
antiferromagnet	O
and	O
the	O
lower	O
curve	O
(	O
+	O
)	O
the	O
ferromagnet	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
31.2	O
:	O
direct	O
computation	O
of	O
partition	O
function	B
of	O
ising	O
models	O
411	O
0	O
-0.5	O
-1	O
-1.5	O
-2	O
-2.5	O
-3	O
-3.5	O
rectangular	B
ferromagnet	O
width	O
4	O
width	O
8	O
triangular	O
(	O
-	O
)	O
rectangular	B
(	O
+/-	O
)	O
triangular	O
(	O
+	O
)	O
1	O
10	O
triangular	O
ising	O
models	O
width	O
4	O
(	O
-	O
)	O
width	O
8	O
(	O
-	O
)	O
width	O
4	O
(	O
+	O
)	O
width	O
8	O
(	O
+	O
)	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
1	O
temperature	B
-0.2	O
10	O
1	O
temperature	B
10	O
y	O
t	O
i	O
c	O
a	O
p	O
a	O
c	O
t	O
a	O
e	O
h	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
-0.2	O
energy	B
)	O
then	O
the	O
entropy	B
as	O
t	O
!	O
0	O
is	O
non-zero	O
.	O
we	O
can	O
(	O
cid:12	O
)	O
nd	O
the	O
entropy	B
from	O
the	O
free	B
energy	I
using	O
s	O
=	O
(	O
cid:0	O
)	O
@	O
f=	O
@	O
t	O
.	O
the	O
entropy	B
of	O
the	O
triangular	O
antiferromagnet	O
at	O
absolute	O
zero	O
appears	O
to	O
be	O
about	O
0.3	O
,	O
that	O
is	O
,	O
about	O
half	O
its	O
high	O
temperature	O
value	O
(	O
(	O
cid:12	O
)	O
gure	O
31.16	O
)	O
.	O
the	O
mean	B
energy	O
as	O
a	O
function	B
of	O
temperature	B
is	O
plotted	O
in	O
(	O
cid:12	O
)	O
gure	O
31.17.	O
it	O
is	O
evaluated	O
using	O
the	O
identity	O
hei	O
=	O
(	O
cid:0	O
)	O
@	O
ln	O
z=	O
@	O
(	O
cid:12	O
)	O
.	O
figure	O
31.18	O
shows	O
the	O
estimated	O
heat	B
capacity	I
(	O
taking	O
raw	O
derivatives	O
of	O
the	O
mean	O
energy	B
)	O
as	O
a	O
function	B
of	O
temperature	B
for	O
the	O
triangular	O
models	O
with	O
widths	O
4	O
and	O
8.	O
figure	O
31.19	O
shows	O
the	O
(	O
cid:13	O
)	O
uctuations	O
in	O
energy	O
as	O
a	O
function	B
of	O
temperature	B
.	O
all	O
of	O
these	O
(	O
cid:12	O
)	O
gures	O
should	O
show	O
smooth	O
graphs	O
;	O
the	O
roughness	O
of	O
the	O
curves	O
is	O
due	O
to	O
inaccurate	O
numerics	O
.	O
the	O
nature	O
of	O
any	O
phase	O
transition	B
is	O
not	O
obvious	O
,	O
but	O
the	O
graphs	O
seem	O
compatible	O
with	O
the	O
assertion	O
that	O
the	O
ferromagnet	O
shows	O
,	O
and	O
the	O
antiferromagnet	O
does	O
not	O
show	O
a	O
phase	B
transition	I
.	O
the	O
pictures	O
of	O
the	O
free	O
energy	B
in	O
(	O
cid:12	O
)	O
gure	O
31.15	O
give	O
some	O
insight	O
into	O
how	O
we	O
could	O
predict	O
the	O
transition	B
temperature	O
.	O
we	O
can	O
see	O
how	O
the	O
two	O
phases	O
of	O
the	O
ferromagnetic	O
systems	O
each	O
have	O
simple	O
free	O
energies	O
:	O
a	O
straight	O
sloping	O
line	O
through	O
f	O
=	O
0	O
,	O
t	O
=	O
0	O
for	O
the	O
high	O
temperature	O
phase	O
,	O
and	O
a	O
horizontal	O
line	O
for	O
the	O
low	O
temperature	B
phase	O
.	O
(	O
the	O
slope	O
of	O
each	O
line	O
shows	O
what	O
the	O
entropy	B
per	O
spin	O
of	O
that	O
phase	O
is	O
.	O
)	O
the	O
phase	B
transition	I
occurs	O
roughly	O
at	O
the	O
intersection	B
of	O
these	O
lines	O
.	O
so	O
we	O
predict	O
the	O
transition	B
temperature	O
to	O
be	O
linearly	O
related	O
to	O
the	O
ground	O
state	O
energy.	O
)	O
e	O
(	O
r	O
a	O
v	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
0	O
-1	O
rectangular	B
ferromagnet	O
width	O
4	O
width	O
8	O
1	O
temperature	B
10	O
16	O
14	O
12	O
10	O
8	O
6	O
4	O
2	O
0	O
-2	O
triangular	O
ising	O
models	O
width	O
4	O
(	O
-	O
)	O
width	O
8	O
(	O
-	O
)	O
width	O
4	O
(	O
+	O
)	O
width	O
8	O
(	O
+	O
)	O
1	O
temperature	B
10	O
figure	O
31.17.	O
mean	B
energy	O
versus	O
temperature	B
of	O
long	B
thin	I
strip	I
ising	O
models	O
with	O
width	O
8.	O
compare	O
with	O
(	O
cid:12	O
)	O
gure	O
31.3.	O
figure	O
31.18.	O
heat	O
capacities	O
of	O
(	O
a	O
)	O
rectangular	B
model	O
;	O
(	O
b	O
)	O
triangular	O
models	O
with	O
di	O
(	O
cid:11	O
)	O
erent	O
widths	O
,	O
(	O
+	O
)	O
and	O
(	O
(	O
cid:0	O
)	O
)	O
denoting	O
ferromagnet	O
and	O
antiferromagnet	O
.	O
compare	O
with	O
(	O
cid:12	O
)	O
gure	O
31.11.	O
figure	O
31.19.	O
energy	B
variances	O
,	O
per	O
spin	O
,	O
of	O
(	O
a	O
)	O
rectangular	B
model	O
;	O
(	O
b	O
)	O
triangular	O
models	O
with	O
di	O
(	O
cid:11	O
)	O
erent	O
widths	O
,	O
(	O
+	O
)	O
and	O
(	O
(	O
cid:0	O
)	O
)	O
denoting	O
ferromagnet	O
and	O
antiferromagnet	O
.	O
compare	O
with	O
(	O
cid:12	O
)	O
gure	O
31.11.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
412	O
31	O
|	O
ising	O
models	O
comparison	O
with	O
the	O
monte	O
carlo	O
results	O
the	O
agreement	O
between	O
the	O
results	O
of	O
the	O
two	O
experiments	O
seems	O
very	B
good	I
.	O
the	O
two	O
systems	O
simulated	O
(	O
the	O
long	B
thin	I
strip	I
and	O
the	O
periodic	O
square	O
)	O
are	O
not	O
quite	O
identical	O
.	O
one	O
could	O
a	O
more	O
accurate	O
comparison	O
by	O
(	O
cid:12	O
)	O
nding	O
all	O
eigenvalues	O
for	O
the	O
strip	O
of	O
width	O
w	O
and	O
computingp	O
(	O
cid:21	O
)	O
w	O
to	O
get	O
the	O
partition	B
function	I
of	O
a	O
w	O
(	O
cid:2	O
)	O
w	O
patch	O
.	O
31.3	O
exercises	O
.	O
exercise	O
31.2	O
.	O
[	O
4	O
]	O
what	O
would	O
be	O
the	O
best	O
way	O
to	O
extract	O
the	O
entropy	B
from	O
the	O
monte	O
carlo	O
simulations	O
?	O
what	O
would	O
be	O
the	O
best	O
way	O
to	O
obtain	O
the	O
entropy	B
and	O
the	O
heat	B
capacity	I
from	O
the	O
partition	B
function	I
computation	O
?	O
exercise	O
31.3	O
.	O
[	O
3	O
]	O
an	O
ising	O
model	B
may	O
be	O
generalized	B
to	O
have	O
a	O
coupling	O
jmn	O
between	O
any	O
spins	O
m	O
and	O
n	O
,	O
and	O
the	O
value	O
of	O
jmn	O
could	O
be	O
di	O
(	O
cid:11	O
)	O
erent	O
for	O
each	O
m	O
and	O
n.	O
in	O
the	O
special	O
case	O
where	O
all	O
the	O
couplings	O
are	O
positive	O
we	O
know	O
that	O
the	O
system	O
has	O
two	O
ground	O
states	O
,	O
the	O
all-up	O
and	O
all-down	O
states	O
.	O
for	O
a	O
more	O
general	O
setting	O
of	O
jmn	O
it	O
is	O
conceivable	O
that	O
there	O
could	O
be	O
many	O
ground	O
states	O
.	O
imagine	O
that	O
it	O
is	O
required	O
to	O
make	O
a	O
spin	B
system	I
whose	O
local	O
minima	O
are	O
a	O
given	O
list	O
of	O
states	O
x	O
(	O
1	O
)	O
;	O
x	O
(	O
2	O
)	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
s	O
)	O
.	O
can	O
you	O
think	O
of	O
a	O
way	O
of	O
setting	O
j	O
such	O
that	O
the	O
chosen	O
states	O
are	O
low	O
energy	B
states	O
?	O
you	O
are	O
allowed	O
to	O
adjust	O
all	O
the	O
fjmng	O
to	O
whatever	O
values	O
you	O
wish	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
32	O
exact	O
monte	O
carlo	O
sampling	O
32.1	O
the	O
problem	O
with	O
monte	O
carlo	O
methods	B
for	O
high-dimensional	O
problems	O
,	O
the	O
most	O
widely	O
used	O
random	B
sampling	O
meth-	O
ods	O
are	O
markov	O
chain	B
monte	O
carlo	O
methods	B
like	O
the	O
metropolis	O
method	B
,	O
gibbs	O
sampling	O
,	O
and	O
slice	O
sampling	O
.	O
the	O
problem	O
with	O
all	O
these	O
methods	B
is	O
this	O
:	O
yes	O
,	O
a	O
given	O
algorithm	B
can	O
be	O
guaranteed	O
to	O
produce	O
samples	O
from	O
the	O
target	O
density	B
p	O
(	O
x	O
)	O
asymptotically	O
,	O
‘	O
once	O
the	O
chain	B
has	O
converged	O
to	O
the	O
equilibrium	O
distribution	B
’	O
.	O
but	O
if	O
one	O
runs	O
the	O
chain	B
for	O
too	O
short	O
a	O
time	O
t	O
,	O
then	O
the	O
samples	O
will	O
come	O
from	O
some	O
other	O
distribution	B
p	O
(	O
t	O
)	O
(	O
x	O
)	O
.	O
for	O
how	O
long	O
must	O
the	O
markov	O
chain	B
be	O
run	O
before	O
it	O
has	O
‘	O
converged	O
’	O
?	O
as	O
was	O
mentioned	O
in	O
chapter	O
29	O
,	O
this	O
question	O
is	O
usually	O
very	O
hard	O
to	O
answer	O
.	O
however	O
,	O
the	O
pioneering	O
work	O
of	O
propp	O
and	O
wilson	O
(	O
1996	O
)	O
allows	O
one	O
,	O
for	O
certain	O
chains	O
,	O
to	O
answer	O
this	O
very	O
question	O
;	O
furthermore	O
propp	O
and	O
wilson	O
show	O
how	O
to	O
obtain	O
‘	O
exact	O
’	O
samples	O
from	O
the	O
target	O
density	B
.	O
32.2	O
exact	B
sampling	I
concepts	O
propp	O
and	O
wilson	O
’	O
s	O
exact	B
sampling	I
method	O
(	O
also	O
known	O
as	O
‘	O
perfect	B
simulation	I
’	O
or	O
‘	O
coupling	B
from	I
the	I
past	I
’	O
)	O
depends	O
on	O
three	O
ideas	O
.	O
coalescence	B
of	O
coupled	O
markov	O
chains	O
first	O
,	O
if	O
several	O
markov	O
chains	O
starting	O
from	O
di	O
(	O
cid:11	O
)	O
erent	O
initial	O
conditions	O
share	O
a	O
single	O
random-number	O
generator	O
,	O
then	O
their	O
trajectories	O
in	O
state	O
space	O
may	O
coalesce	O
;	O
and	O
,	O
having	O
,	O
coalesced	O
,	O
will	O
not	O
separate	O
again	O
.	O
if	O
all	O
initial	O
condi-	O
tions	O
lead	O
to	O
trajectories	O
that	O
coalesce	O
into	O
a	O
single	O
trajectory	O
,	O
then	O
we	O
can	O
be	O
sure	O
that	O
the	O
markov	O
chain	B
has	O
‘	O
forgotten	O
’	O
its	O
initial	O
condition	O
.	O
figure	O
32.1a-i	O
shows	O
twenty-one	O
markov	O
chains	O
identical	O
to	O
the	O
one	O
described	O
in	O
section	O
29.4	O
,	O
which	O
samples	O
from	O
f0	O
;	O
1	O
;	O
:	O
:	O
:	O
;	O
20g	O
using	O
the	O
metropolis	O
algorithm	B
(	O
(	O
cid:12	O
)	O
gure	O
29.12	O
,	O
p.368	O
)	O
;	O
each	O
of	O
the	O
chains	O
has	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
initial	O
condition	O
but	O
they	O
are	O
all	O
driven	O
by	O
a	O
single	O
random	O
number	O
generator	O
;	O
the	O
chains	O
coalesce	O
after	O
about	O
80	O
steps	O
.	O
figure	O
32.1a-ii	O
shows	O
the	O
same	O
markov	O
chains	O
with	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
random	O
number	O
seed	O
;	O
in	O
this	O
case	O
,	O
coalescence	B
does	O
not	O
occur	O
until	O
400	O
steps	O
have	O
elapsed	O
(	O
not	O
shown	O
)	O
.	O
figure	O
32.1b	O
shows	O
similar	O
markov	O
chains	O
,	O
each	O
of	O
which	O
has	O
identical	O
proposal	O
density	B
to	O
those	O
in	O
section	O
29.4	O
and	O
(	O
cid:12	O
)	O
gure	O
32.1a	O
;	O
but	O
in	O
(	O
cid:12	O
)	O
gure	O
32.1b	O
,	O
the	O
proposed	O
move	O
at	O
each	O
step	O
,	O
‘	O
left	O
’	O
or	O
‘	O
right	O
’	O
,	O
is	O
obtained	O
in	O
the	O
same	O
way	O
by	O
all	O
the	O
chains	O
at	O
any	O
timestep	O
,	O
independent	O
of	O
the	O
current	O
state	O
.	O
this	O
coupling	O
of	O
the	O
chains	O
changes	O
the	O
statistics	O
of	O
coalescence	O
.	O
because	O
two	O
neighbouring	O
paths	O
merge	O
only	O
when	O
a	O
rejection	B
occurs	O
,	O
and	O
rejections	O
occur	O
only	O
at	O
the	O
walls	O
(	O
for	O
this	O
particular	O
markov	O
chain	B
)	O
,	O
coalescence	B
will	O
occur	O
only	O
when	O
the	O
chains	O
are	O
all	O
in	O
the	O
leftmost	O
state	O
or	O
all	O
in	O
the	O
rightmost	O
state	O
.	O
413	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
414	O
32	O
|	O
exact	O
monte	O
carlo	O
sampling	O
figure	O
32.1.	O
coalescence	B
,	O
the	O
(	O
cid:12	O
)	O
rst	O
idea	O
behind	O
the	O
exact	B
sampling	I
method	O
.	O
time	O
runs	O
from	O
bottom	O
to	O
top	O
.	O
in	O
the	O
leftmost	O
panel	O
,	O
coalescence	B
occurred	O
within	O
100	O
steps	O
.	O
di	O
(	O
cid:11	O
)	O
erent	O
coalescence	B
properties	O
are	O
obtained	O
depending	O
on	O
the	O
way	O
each	O
state	O
uses	O
the	O
random	B
numbers	O
it	O
is	O
supplied	O
with	O
.	O
(	O
a	O
)	O
two	O
runs	O
of	O
a	O
metropolis	O
simulator	O
in	O
which	O
the	O
random	B
bits	O
that	O
determine	O
the	O
proposed	O
step	O
depend	O
on	O
the	O
current	O
state	O
;	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
random	O
number	O
seed	O
was	O
used	O
in	O
each	O
case	O
.	O
(	O
b	O
)	O
in	O
this	O
simulator	O
the	O
random	B
proposal	O
(	O
‘	O
left	O
’	O
or	O
‘	O
right	O
’	O
)	O
is	O
the	O
same	O
for	O
all	O
states	O
.	O
in	O
each	O
panel	O
,	O
one	O
of	O
the	O
paths	O
,	O
the	O
one	O
starting	O
at	O
location	O
x	O
=	O
8	O
,	O
has	O
been	O
highlighted	O
.	O
250	O
250	O
250	O
250	O
200	O
200	O
200	O
200	O
150	O
150	O
150	O
150	O
100	O
100	O
100	O
100	O
50	O
50	O
50	O
50	O
0	O
0	O
0	O
0	O
0	O
5	O
10	O
15	O
20	O
0	O
5	O
10	O
15	O
20	O
0	O
5	O
10	O
15	O
20	O
0	O
5	O
10	O
15	O
20	O
(	O
i	O
)	O
(	O
ii	O
)	O
(	O
a	O
)	O
(	O
i	O
)	O
(	O
ii	O
)	O
(	O
b	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
415	O
32.2	O
:	O
exact	B
sampling	I
concepts	O
coupling	B
from	I
the	I
past	I
how	O
can	O
we	O
use	O
the	O
coalescence	B
property	O
to	O
(	O
cid:12	O
)	O
nd	O
an	O
exact	O
sample	O
from	O
the	O
equilibrium	O
distribution	B
of	O
the	O
chain	B
?	O
the	O
state	O
of	O
the	O
system	O
at	O
the	O
moment	O
when	O
complete	O
coalescence	B
occurs	O
is	O
not	O
a	O
valid	O
sample	B
from	I
the	O
equilibrium	O
distribution	B
;	O
for	O
example	O
in	O
(	O
cid:12	O
)	O
gure	O
32.1b	O
,	O
(	O
cid:12	O
)	O
nal	O
coalescence	B
always	O
occurs	O
when	O
the	O
state	O
is	O
against	O
one	O
of	O
the	O
two	O
walls	O
,	O
because	O
trajectories	O
merge	O
only	O
at	O
the	O
walls	O
.	O
so	O
sampling	O
forward	O
in	O
time	O
until	O
coalescence	B
occurs	O
is	O
not	O
a	O
valid	O
method	B
.	O
the	O
second	O
key	O
idea	O
of	O
exact	O
sampling	O
is	O
that	O
we	O
can	O
obtain	O
exact	O
samples	O
by	O
sampling	O
from	O
a	O
time	O
t0	O
in	O
the	O
past	O
,	O
up	O
to	O
the	O
present	O
.	O
if	O
coalescence	B
has	O
occurred	O
,	O
the	O
present	O
sample	B
is	O
an	O
unbiased	O
sample	O
from	O
the	O
equilibrium	O
distribution	B
;	O
if	O
not	O
,	O
we	O
restart	O
the	O
simulation	O
from	O
a	O
time	O
t0	O
further	O
into	O
the	O
past	O
,	O
reusing	O
the	O
same	O
random	B
numbers	O
.	O
the	O
simulation	O
is	O
repeated	O
at	O
a	O
sequence	B
of	O
ever	O
more	O
distant	O
times	O
t0	O
,	O
with	O
a	O
doubling	O
of	O
t0	O
from	O
one	O
run	O
to	O
the	O
next	O
being	O
a	O
convenient	O
choice	O
.	O
when	O
coalescence	B
occurs	O
at	O
a	O
time	O
before	O
‘	O
the	O
present	O
’	O
,	O
we	O
can	O
record	O
x	O
(	O
0	O
)	O
as	O
an	O
exact	O
sample	O
from	O
the	O
equilibrium	O
distribution	B
of	O
the	O
markov	O
chain	B
.	O
figure	O
32.2	O
shows	O
two	O
exact	O
samples	O
produced	O
in	O
this	O
way	O
.	O
in	O
the	O
leftmost	O
panel	O
of	O
(	O
cid:12	O
)	O
gure	O
32.2a	O
,	O
we	O
start	O
twenty-one	O
chains	O
in	O
all	O
possible	O
initial	O
condi-	O
tions	O
at	O
t0	O
=	O
(	O
cid:0	O
)	O
50	O
and	O
run	O
them	O
forward	O
in	O
time	O
.	O
coalescence	B
does	O
not	O
occur	O
.	O
we	O
restart	O
the	O
simulation	O
from	O
all	O
possible	O
initial	O
conditions	O
at	O
t0	O
=	O
(	O
cid:0	O
)	O
100	O
,	O
and	O
reset	O
the	O
random	B
number	I
generator	I
in	O
such	O
a	O
way	O
that	O
the	O
random	B
num-	O
bers	O
generated	O
at	O
each	O
time	O
t	O
(	O
in	O
particular	O
,	O
from	O
t	O
=	O
(	O
cid:0	O
)	O
50	O
to	O
t	O
=	O
0	O
)	O
will	O
be	O
identical	O
to	O
what	O
they	O
were	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
run	O
.	O
notice	O
that	O
the	O
trajectories	O
pro-	O
duced	O
from	O
t	O
=	O
(	O
cid:0	O
)	O
50	O
to	O
t	O
=	O
0	O
by	O
these	O
runs	O
that	O
started	O
from	O
t0	O
=	O
(	O
cid:0	O
)	O
100	O
are	O
identical	O
to	O
a	O
subset	B
of	O
the	O
trajectories	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
simulation	O
with	O
t0	O
=	O
(	O
cid:0	O
)	O
50.	O
coalescence	B
still	O
does	O
not	O
occur	O
,	O
so	O
we	O
double	O
t0	O
again	O
to	O
t0	O
=	O
(	O
cid:0	O
)	O
200.	O
this	O
time	O
,	O
all	O
the	O
trajectories	O
coalesce	O
and	O
we	O
obtain	O
an	O
exact	O
sample	O
,	O
shown	O
by	O
the	O
arrow	O
.	O
if	O
we	O
pick	O
an	O
earlier	O
time	O
such	O
as	O
t0	O
=	O
(	O
cid:0	O
)	O
500	O
,	O
all	O
the	O
trajectories	O
must	O
still	O
end	O
in	O
the	O
same	O
point	O
at	O
t	O
=	O
0	O
,	O
since	O
every	O
trajectory	O
must	O
pass	O
through	O
some	O
state	O
at	O
t	O
=	O
(	O
cid:0	O
)	O
200	O
,	O
and	O
all	O
those	O
states	O
lead	O
to	O
the	O
same	O
(	O
cid:12	O
)	O
nal	O
point	O
.	O
so	O
if	O
we	O
ran	O
the	O
markov	O
chain	B
for	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
time	O
in	O
the	O
past	O
,	O
from	O
any	O
initial	O
condition	O
,	O
it	O
would	O
end	O
in	O
the	O
same	O
state	O
.	O
figure	O
32.2b	O
shows	O
an	O
exact	O
sample	O
produced	O
in	O
the	O
same	O
way	O
with	O
the	O
markov	O
chains	O
of	O
(	O
cid:12	O
)	O
gure	O
32.1b	O
.	O
this	O
method	B
,	O
called	O
coupling	B
from	I
the	I
past	I
,	O
is	O
important	O
because	O
it	O
allows	O
us	O
to	O
obtain	O
exact	O
samples	O
from	O
the	O
equilibrium	O
distribution	B
;	O
but	O
,	O
as	O
described	O
here	O
,	O
it	O
is	O
of	O
little	O
practical	B
use	O
,	O
since	O
we	O
are	O
obliged	O
to	O
simulate	O
chains	O
starting	O
in	O
all	O
initial	O
states	O
.	O
in	O
the	O
examples	O
shown	O
,	O
there	O
are	O
only	O
twenty-one	O
states	O
,	O
but	O
in	O
any	O
realistic	O
sampling	O
problem	O
there	O
will	O
be	O
an	O
utterly	O
enormous	O
number	O
of	O
states	O
{	O
think	O
of	O
the	O
21000	O
states	O
of	O
a	O
system	O
of	O
1000	O
binary	O
spins	O
,	O
for	O
example	O
.	O
the	O
whole	O
point	O
of	O
introducing	O
monte	O
carlo	O
methods	B
was	O
to	O
try	O
to	O
avoid	O
having	O
to	O
visit	O
all	O
the	O
states	O
of	O
such	O
a	O
system	O
!	O
monotonicity	O
having	O
established	O
that	O
we	O
can	O
obtain	O
valid	O
samples	O
by	O
simulating	O
forward	O
from	O
times	O
in	O
the	O
past	O
,	O
starting	O
in	O
all	O
possible	O
states	O
at	O
those	O
times	O
,	O
the	O
third	O
trick	O
of	O
propp	O
and	O
wilson	O
,	O
which	O
makes	O
the	O
exact	B
sampling	I
method	O
useful	O
in	O
practice	O
,	O
is	O
the	O
idea	O
that	O
,	O
for	O
some	O
markov	O
chains	O
,	O
it	O
may	O
be	O
possible	O
to	O
detect	O
coalescence	B
of	O
all	O
trajectories	O
without	O
simulating	O
all	O
those	O
trajectories	O
.	O
this	O
property	O
holds	O
,	O
for	O
example	O
,	O
in	O
the	O
chain	B
of	O
(	O
cid:12	O
)	O
gure	O
32.1b	O
,	O
which	O
has	O
the	O
property	O
that	O
two	O
trajectories	O
never	O
cross	O
.	O
so	O
if	O
we	O
simply	O
track	O
the	O
two	O
tra-	O
jectories	O
starting	O
from	O
the	O
leftmost	O
and	O
rightmost	O
states	O
,	O
we	O
will	O
know	O
that	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
416	O
32	O
|	O
exact	O
monte	O
carlo	O
sampling	O
0	O
0	O
0	O
0	O
0	O
0	O
-50	O
-50	O
-50	O
-50	O
-50	O
-50	O
-100	O
-100	O
-100	O
-100	O
-100	O
-100	O
-150	O
-150	O
-150	O
-150	O
-150	O
-150	O
-200	O
-200	O
-200	O
-200	O
-200	O
-200	O
-250	O
-250	O
-250	O
-250	O
-250	O
-250	O
0	O
10	O
20	O
t0	O
=	O
(	O
cid:0	O
)	O
50	O
0	O
10	O
20	O
t0	O
=	O
(	O
cid:0	O
)	O
100	O
(	O
a	O
)	O
0	O
10	O
20	O
t0	O
=	O
(	O
cid:0	O
)	O
200	O
0	O
10	O
20	O
t0	O
=	O
(	O
cid:0	O
)	O
50	O
0	O
10	O
20	O
t0	O
=	O
(	O
cid:0	O
)	O
100	O
(	O
b	O
)	O
0	O
10	O
20	O
t0	O
=	O
(	O
cid:0	O
)	O
200	O
figure	O
32.2	O
.	O
‘	O
coupling	B
from	I
the	I
past	I
’	O
,	O
the	O
second	O
idea	O
behind	O
the	O
exact	B
sampling	I
method	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
32.2	O
:	O
exact	B
sampling	I
concepts	O
417	O
0	O
0	O
0	O
0	O
0	O
-50	O
-50	O
-50	O
-50	O
-50	O
-100	O
-100	O
-100	O
-100	O
-100	O
-150	O
-150	O
-150	O
-150	O
-150	O
-200	O
-200	O
-200	O
-200	O
-200	O
-250	O
-250	O
-250	O
-250	O
-250	O
0	O
10	O
20	O
t0	O
=	O
(	O
cid:0	O
)	O
50	O
0	O
10	O
20	O
t0	O
=	O
(	O
cid:0	O
)	O
100	O
(	O
a	O
)	O
0	O
10	O
20	O
t0	O
=	O
(	O
cid:0	O
)	O
200	O
0	O
10	O
20	O
t0	O
=	O
(	O
cid:0	O
)	O
50	O
(	O
b	O
)	O
0	O
10	O
20	O
t0	O
=	O
(	O
cid:0	O
)	O
1000	O
(	O
c	O
)	O
figure	O
32.3	O
.	O
(	O
a	O
)	O
ordering	O
of	O
states	O
,	O
the	O
third	O
idea	O
behind	O
the	O
exact	B
sampling	I
method	O
.	O
the	O
trajectories	O
shown	O
here	O
are	O
the	O
left-most	O
and	O
right-most	O
trajectories	O
of	O
(	O
cid:12	O
)	O
gure	O
32.2b	O
.	O
in	O
order	O
to	O
establish	O
what	O
the	O
state	O
at	O
time	O
zero	O
is	O
,	O
we	O
only	O
need	O
to	O
run	O
simulations	O
from	O
t0	O
=	O
(	O
cid:0	O
)	O
50	O
,	O
t0	O
=	O
(	O
cid:0	O
)	O
100	O
,	O
and	O
t0	O
=	O
(	O
cid:0	O
)	O
200	O
,	O
after	O
which	O
point	O
coalescence	O
occurs	O
.	O
(	O
b	O
,	O
c	O
)	O
two	O
more	O
exact	O
samples	O
from	O
the	O
target	O
density	B
,	O
generated	O
by	O
this	O
method	B
,	O
and	O
di	O
(	O
cid:11	O
)	O
erent	O
random	O
number	O
seeds	O
.	O
the	O
initial	O
times	O
required	O
were	O
t0	O
=	O
(	O
cid:0	O
)	O
50	O
and	O
t0	O
=	O
(	O
cid:0	O
)	O
1000	O
,	O
respectively	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
418	O
32	O
|	O
exact	O
monte	O
carlo	O
sampling	O
compute	O
ai	O
:	O
=pj	O
jijxj	O
draw	O
u	O
from	O
uniform	O
(	O
0	O
;	O
1	O
)	O
if	O
u	O
<	O
1=	O
(	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
2ai	O
)	O
xi	O
:	O
=	O
+1	O
else	O
xi	O
:	O
=	O
(	O
cid:0	O
)	O
1	O
algorithm	B
32.4.	O
gibbs	O
sampling	O
coupling	O
method	B
.	O
the	O
markov	O
chains	O
are	O
coupled	O
together	O
by	O
having	O
all	O
chains	O
update	O
the	O
same	O
spin	O
i	O
at	O
each	O
time	O
step	O
and	O
having	O
all	O
chains	O
share	O
a	O
common	O
sequence	B
of	O
random	B
numbers	O
u.	O
figure	O
32.5.	O
an	O
exact	O
sample	O
from	O
the	O
ising	O
model	B
at	O
its	O
critical	O
temperature	O
,	O
produced	O
by	O
d.b	O
.	O
wilson	O
.	O
such	O
samples	O
can	O
be	O
produced	O
within	O
seconds	O
on	O
an	O
ordinary	O
computer	B
by	O
exact	B
sampling	I
.	O
coalescence	B
of	O
all	O
trajectories	O
has	O
occurred	O
when	O
those	O
two	O
trajectories	O
co-	O
alesce	O
.	O
figure	O
32.3a	O
illustrates	O
this	O
idea	O
by	O
showing	O
only	O
the	O
left-most	O
and	O
right-most	O
trajectories	O
of	O
(	O
cid:12	O
)	O
gure	O
32.2b	O
.	O
figure	O
32.3	O
(	O
b	O
,	O
c	O
)	O
shows	O
two	O
more	O
ex-	O
act	O
samples	O
from	O
the	O
same	O
equilibrium	O
distribution	B
generated	O
by	O
running	O
the	O
‘	O
coupling	B
from	I
the	I
past	I
’	O
method	B
starting	O
from	O
the	O
two	O
end-states	O
alone	O
.	O
in	O
(	O
b	O
)	O
,	O
two	O
runs	O
coalesced	O
starting	O
from	O
t0	O
=	O
(	O
cid:0	O
)	O
50	O
;	O
in	O
(	O
c	O
)	O
,	O
it	O
was	O
necessary	O
to	O
try	O
times	O
up	O
to	O
t0	O
=	O
(	O
cid:0	O
)	O
1000	O
to	O
achieve	O
coalescence	B
.	O
32.3	O
exact	B
sampling	I
from	O
interesting	O
distributions	O
in	O
the	O
toy	O
problem	O
we	O
studied	O
,	O
the	O
states	O
could	O
be	O
put	O
in	O
a	O
one-dimensional	O
order	O
such	O
that	O
no	O
two	O
trajectories	O
crossed	O
.	O
the	O
states	O
of	O
many	O
interesting	O
state	O
spaces	O
can	O
also	O
be	O
put	O
into	O
a	O
partial	B
order	I
and	O
coupled	O
markov	O
chains	O
can	O
be	O
found	O
that	O
respect	O
this	O
partial	B
order	I
.	O
[	O
an	O
example	O
of	O
a	O
partial	B
order	I
on	O
the	O
four	O
possible	O
states	O
of	O
two	O
spins	O
is	O
this	O
:	O
(	O
+	O
;	O
+	O
)	O
>	O
(	O
+	O
;	O
(	O
cid:0	O
)	O
)	O
>	O
(	O
(	O
cid:0	O
)	O
;	O
(	O
cid:0	O
)	O
)	O
;	O
and	O
(	O
+	O
;	O
+	O
)	O
>	O
(	O
(	O
cid:0	O
)	O
;	O
+	O
)	O
>	O
(	O
(	O
cid:0	O
)	O
;	O
(	O
cid:0	O
)	O
)	O
;	O
and	O
the	O
states	O
(	O
+	O
;	O
(	O
cid:0	O
)	O
)	O
and	O
(	O
(	O
cid:0	O
)	O
;	O
+	O
)	O
are	O
not	O
ordered	O
.	O
]	O
for	O
such	O
systems	O
,	O
we	O
can	O
show	O
that	O
coalescence	B
has	O
occurred	O
merely	O
by	O
verifying	O
that	O
coalescence	B
has	O
occurred	O
for	O
all	O
the	O
histories	O
whose	O
initial	O
states	O
were	O
‘	O
maximal	O
’	O
and	O
‘	O
minimal	O
’	O
states	O
of	O
the	O
state	O
space	O
.	O
as	O
an	O
example	O
,	O
consider	O
the	O
gibbs	O
sampling	O
method	O
applied	O
to	O
a	O
ferro-	O
magnetic	O
ising	O
spin	B
system	I
,	O
with	O
the	O
partial	B
ordering	O
of	O
states	O
being	O
de	O
(	O
cid:12	O
)	O
ned	O
thus	O
:	O
state	O
x	O
is	O
‘	O
greater	O
than	O
or	O
equal	O
to	O
’	O
state	O
y	O
if	O
xi	O
(	O
cid:21	O
)	O
yi	O
for	O
all	O
spins	O
i.	O
the	O
maximal	O
and	O
minimal	O
states	O
are	O
the	O
the	O
all-up	O
and	O
all-down	O
states	O
.	O
the	O
markov	O
chains	O
are	O
coupled	O
together	O
as	O
shown	O
in	O
algorithm	O
32.4.	O
propp	O
and	O
wilson	O
(	O
1996	O
)	O
show	O
that	O
exact	O
samples	O
can	O
be	O
generated	O
for	O
this	O
system	O
,	O
al-	O
though	O
the	O
time	O
to	O
(	O
cid:12	O
)	O
nd	O
exact	O
samples	O
is	O
large	O
if	O
the	O
ising	O
model	B
is	O
below	O
its	O
critical	O
temperature	O
,	O
since	O
the	O
gibbs	O
sampling	O
method	O
itself	O
is	O
slowly-mixing	O
under	O
these	O
conditions	O
.	O
propp	O
and	O
wilson	O
have	O
improved	O
on	O
this	O
method	B
for	O
the	O
ising	O
model	B
by	O
using	O
a	O
markov	O
chain	B
called	O
the	O
single-bond	O
heat	B
bath	I
algorithm	O
to	O
sample	B
from	I
a	O
related	O
model	B
called	O
the	O
random	B
cluster	I
model	I
;	O
they	O
show	O
that	O
exact	O
samples	O
from	O
the	O
random	B
cluster	I
model	I
can	O
be	O
obtained	O
rapidly	O
and	O
can	O
be	O
converted	O
into	O
exact	O
samples	O
from	O
the	O
ising	O
model	B
.	O
their	O
ground-breaking	O
paper	O
includes	O
an	O
exact	O
sample	O
from	O
a	O
16-million-spin	O
ising	O
model	B
at	O
its	O
critical	O
temperature	O
.	O
a	O
sample	B
for	O
a	O
smaller	O
ising	O
model	B
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
32.5.	O
a	O
generalization	B
of	O
the	O
exact	B
sampling	I
method	O
for	O
‘	O
non-attractive	O
’	O
distri-	O
butions	O
the	O
method	B
of	O
propp	O
and	O
wilson	O
for	O
the	O
ising	O
model	B
,	O
sketched	O
above	O
,	O
can	O
be	O
applied	O
only	O
to	O
probability	B
distributions	I
that	O
are	O
,	O
as	O
they	O
call	O
them	O
,	O
‘	O
at-	O
tractive	O
’	O
.	O
rather	O
than	O
de	O
(	O
cid:12	O
)	O
ne	O
this	O
term	O
,	O
let	O
’	O
s	O
say	O
what	O
it	O
means	O
,	O
for	O
practical	O
purposes	O
:	O
the	O
method	B
can	O
be	O
applied	O
to	O
spin	O
systems	O
in	O
which	O
all	O
the	O
cou-	O
plings	O
are	O
positive	O
(	O
e.g.	O
,	O
the	O
ferromagnet	O
)	O
,	O
and	O
to	O
a	O
few	O
special	O
spin	O
systems	O
with	O
negative	O
couplings	O
(	O
e.g.	O
,	O
as	O
we	O
already	O
observed	O
in	O
chapter	O
31	O
,	O
the	O
rect-	O
angular	O
ferromagnet	O
and	O
antiferromagnet	O
are	O
equivalent	O
)	O
;	O
but	O
it	O
can	O
not	O
be	O
applied	O
to	O
general	O
spin	O
systems	O
in	O
which	O
some	O
couplings	O
are	O
negative	O
,	O
because	O
in	O
such	O
systems	O
the	O
trajectories	O
followed	O
by	O
the	O
all-up	O
and	O
all-down	O
states	O
are	O
not	O
guaranteed	O
to	O
be	O
upper	O
and	O
lower	O
bounds	O
for	O
the	O
set	B
of	O
all	O
trajecto-	O
ries	O
.	O
fortunately	O
,	O
however	O
,	O
we	O
do	O
not	O
need	O
to	O
be	O
so	O
strict	O
.	O
it	O
is	O
possible	O
to	O
re-express	O
the	O
propp	O
and	O
wilson	O
algorithm	B
in	O
a	O
way	O
that	O
generalizes	O
to	O
the	O
case	O
of	O
spin	O
systems	O
with	O
negative	O
couplings	O
.	O
the	O
idea	O
of	O
the	O
summary	O
state	O
version	O
of	O
exact	O
sampling	O
is	O
still	O
that	O
we	O
keep	O
track	O
of	O
bounds	O
on	O
the	O
set	B
of	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
32.3	O
:	O
exact	B
sampling	I
from	O
interesting	O
distributions	O
419	O
all	O
trajectories	O
,	O
and	O
detect	O
when	O
these	O
bounds	O
are	O
equal	O
,	O
so	O
as	O
to	O
(	O
cid:12	O
)	O
nd	O
exact	O
samples	O
.	O
but	O
the	O
bounds	O
will	O
not	O
themselves	O
be	O
actual	O
trajectories	O
,	O
and	O
they	O
will	O
not	O
necessarily	O
be	O
tight	O
bounds	O
.	O
instead	O
of	O
simulating	O
two	O
trajectories	O
,	O
each	O
of	O
which	O
moves	O
in	O
a	O
state	O
space	O
f	O
(	O
cid:0	O
)	O
1	O
;	O
+1gn	O
,	O
we	O
simulate	O
one	O
trajectory	O
envelope	O
in	O
an	O
augmented	O
state	O
space	O
f	O
(	O
cid:0	O
)	O
1	O
;	O
+1	O
;	O
?	O
gn	O
,	O
where	O
the	O
symbol	O
?	O
denotes	O
‘	O
either	O
(	O
cid:0	O
)	O
1	O
or	O
+1	O
’	O
.	O
we	O
call	O
the	O
state	O
of	O
this	O
augmented	O
system	O
the	O
‘	O
summary	B
state	I
’	O
.	O
an	O
example	O
summary	B
state	I
of	O
a	O
six-spin	O
system	O
is	O
++-	O
?	O
+	O
?	O
.	O
this	O
summary	B
state	I
is	O
shorthand	O
for	O
the	O
set	B
of	O
states	O
++-+++	O
,	O
++-++-	O
,	O
++	O
--	O
++	O
,	O
++	O
--	O
+-	O
.	O
the	O
update	O
rule	O
at	O
each	O
step	O
of	O
the	O
markov	O
chain	B
takes	O
a	O
single	O
spin	O
,	O
enu-	O
merates	O
all	O
possible	O
states	O
of	O
the	O
neighbouring	O
spins	O
that	O
are	O
compatible	O
with	O
the	O
current	O
summary	B
state	I
,	O
and	O
,	O
for	O
each	O
of	O
these	O
local	O
scenarios	O
,	O
computes	O
the	O
new	O
value	O
(	O
+	O
or	O
-	O
)	O
of	O
the	O
spin	O
using	O
gibbs	O
sampling	O
(	O
coupled	O
to	O
a	O
random	O
number	O
u	O
as	O
in	O
algorithm	B
32.4	O
)	O
.	O
if	O
all	O
these	O
new	O
values	O
agree	O
,	O
then	O
the	O
new	O
value	O
of	O
the	O
updated	O
spin	O
in	O
the	O
summary	B
state	I
is	O
set	B
to	O
the	O
unanimous	O
value	O
(	O
+	O
or	O
-	O
)	O
.	O
otherwise	O
,	O
the	O
new	O
value	O
of	O
the	O
spin	O
in	O
the	O
summary	B
state	I
is	O
‘	O
?	O
’	O
.	O
the	O
initial	O
condition	O
,	O
at	O
time	O
t0	O
,	O
is	O
given	O
by	O
setting	O
all	O
the	O
spins	O
in	O
the	O
summary	B
state	I
to	O
‘	O
?	O
’	O
,	O
which	O
corresponds	O
to	O
considering	O
all	O
possible	O
start	O
con	O
(	O
cid:12	O
)	O
gurations	O
.	O
in	O
the	O
case	O
of	O
a	O
spin	B
system	I
with	O
positive	O
couplings	O
,	O
this	O
summary	B
state	I
simulation	O
will	O
be	O
identical	O
to	O
the	O
simulation	O
of	O
the	O
uppermost	O
state	O
and	O
low-	O
ermost	O
states	O
,	O
in	O
the	O
style	O
of	O
propp	O
and	O
wilson	O
,	O
with	O
coalescence	O
occuring	O
when	O
all	O
the	O
‘	O
?	O
’	O
symbols	O
have	O
disappeared	O
.	O
the	O
summary	B
state	I
method	O
can	O
be	O
applied	O
to	O
general	O
spin	O
systems	O
with	O
any	O
couplings	O
.	O
the	O
only	O
shortcoming	O
of	O
this	O
method	B
is	O
that	O
the	O
envelope	O
may	O
describe	O
an	O
unnecessarily	O
large	O
set	B
of	O
states	O
,	O
so	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
summary	B
state	I
algorithm	O
will	O
con-	O
verge	O
;	O
the	O
time	O
for	O
coalescence	O
to	O
be	O
detected	O
may	O
be	O
considerably	O
larger	O
than	O
the	O
actual	O
time	O
taken	O
for	O
the	O
underlying	O
markov	O
chain	B
to	O
coalesce	O
.	O
the	O
summary	B
state	I
scheme	O
has	O
been	O
applied	O
to	O
exact	B
sampling	I
in	O
belief	B
networks	O
by	O
harvey	O
and	O
neal	O
(	O
2000	O
)	O
,	O
and	O
to	O
the	O
triangular	O
antiferromagnetic	B
ising	O
model	B
by	O
childs	O
et	O
al	O
.	O
(	O
2001	O
)	O
.	O
summary	B
state	I
methods	O
were	O
(	O
cid:12	O
)	O
rst	O
intro-	O
duced	O
by	O
huber	O
(	O
1998	O
)	O
;	O
they	O
also	O
go	O
by	O
the	O
names	O
sandwiching	O
methods	O
and	O
bounding	O
chains	O
.	O
further	O
reading	O
for	O
further	O
reading	O
,	O
impressive	O
pictures	O
of	O
exact	O
samples	O
from	O
other	O
distribu-	O
tions	O
,	O
and	O
generalizations	O
of	O
the	O
exact	O
sampling	O
method	O
,	O
browse	O
the	O
perfectly-	O
random	B
sampling	O
website.1	O
for	O
beautiful	O
exact-sampling	O
demonstrations	O
running	O
live	O
in	O
your	O
web-	O
browser	O
,	O
see	O
jim	O
propp	O
’	O
s	O
website.2	O
other	O
uses	O
for	O
coupling	O
the	O
idea	O
of	O
coupling	O
together	O
markov	O
chains	O
by	O
having	O
them	O
share	O
a	O
random	B
number	I
generator	I
has	O
other	O
applications	O
beyond	O
exact	B
sampling	I
.	O
pinto	O
and	O
neal	O
(	O
2001	O
)	O
have	O
shown	O
that	O
the	O
accuracy	O
of	O
estimates	O
obtained	O
from	O
a	O
markov	O
chain	B
monte	O
carlo	O
simulation	O
(	O
the	O
second	O
problem	O
discussed	O
in	O
section	O
29.1	O
,	O
p.357	O
)	O
,	O
using	O
the	O
estimator	B
^	O
(	O
cid:8	O
)	O
p	O
(	O
cid:17	O
)	O
1	O
t	O
xt	O
(	O
cid:30	O
)	O
(	O
x	O
(	O
t	O
)	O
)	O
;	O
(	O
32.1	O
)	O
1http	O
:	O
//www.dbwilson.com/exact/	O
2http	O
:	O
//www.math.wisc.edu/	O
(	O
cid:24	O
)	O
propp/tiling/www/applets/	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
420	O
32	O
|	O
exact	O
monte	O
carlo	O
sampling	O
figure	O
32.6.	O
a	O
perfectly	O
random	B
tiling	O
of	O
a	O
hexagon	O
by	O
lozenges	O
,	O
provided	O
by	O
j.g	O
.	O
propp	O
and	O
d.b	O
.	O
wilson	O
.	O
can	O
be	O
improved	O
by	O
coupling	O
the	O
chain	B
of	O
interest	O
,	O
which	O
converges	O
to	O
p	O
,	O
to	O
a	O
second	O
chain	B
,	O
which	O
generates	O
samples	O
from	O
a	O
second	O
,	O
simpler	O
distribution	B
,	O
q.	O
the	O
coupling	O
must	O
be	O
set	B
up	O
in	O
such	O
a	O
way	O
that	O
the	O
states	O
of	O
the	O
two	O
chains	O
are	O
strongly	O
correlated	O
.	O
the	O
idea	O
is	O
that	O
we	O
(	O
cid:12	O
)	O
rst	O
estimate	O
the	O
expectations	O
of	O
a	O
function	B
of	O
interest	O
,	O
(	O
cid:30	O
)	O
,	O
under	O
p	O
and	O
under	O
q	O
in	O
the	O
normal	B
way	O
(	O
32.1	O
)	O
and	O
compare	O
the	O
estimate	O
under	O
q	O
,	O
^	O
(	O
cid:8	O
)	O
q	O
,	O
with	O
the	O
true	O
value	O
of	O
the	O
expectation	O
under	O
q	O
,	O
(	O
cid:8	O
)	O
q	O
which	O
we	O
assume	O
can	O
be	O
evaluated	O
exactly	O
.	O
if	O
^	O
(	O
cid:8	O
)	O
q	O
is	O
an	O
overes-	O
timate	O
then	O
it	O
is	O
likely	O
that	O
^	O
(	O
cid:8	O
)	O
p	O
will	O
be	O
an	O
overestimate	O
too	O
.	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
(	O
^	O
(	O
cid:8	O
)	O
q	O
(	O
cid:0	O
)	O
(	O
cid:8	O
)	O
q	O
)	O
can	O
thus	O
be	O
used	O
to	O
correct	O
^	O
(	O
cid:8	O
)	O
p	O
.	O
32.4	O
exercises	O
.	O
exercise	O
32.1	O
.	O
[	O
2	O
,	O
p.421	O
]	O
is	O
there	O
any	O
relationship	O
between	O
the	O
probability	B
dis-	O
tribution	O
of	O
the	O
time	O
taken	O
for	O
all	O
trajectories	O
to	O
coalesce	O
,	O
and	O
the	O
equi-	O
libration	O
time	O
of	O
a	O
markov	O
chain	B
?	O
prove	O
that	O
there	O
is	O
a	O
relationship	O
,	O
or	O
(	O
cid:12	O
)	O
nd	O
a	O
single	O
chain	O
that	O
can	O
be	O
realized	O
in	O
two	O
di	O
(	O
cid:11	O
)	O
erent	O
ways	O
that	O
have	O
di	O
(	O
cid:11	O
)	O
erent	O
coalescence	B
times	O
.	O
.	O
exercise	O
32.2	O
.	O
[	O
2	O
]	O
imagine	O
that	O
fred	O
ignores	O
the	O
requirement	O
that	O
the	O
random	B
bits	O
used	O
at	O
some	O
time	O
t	O
,	O
in	O
every	O
run	O
from	O
increasingly	O
distant	O
times	O
t0	O
,	O
must	O
be	O
identical	O
,	O
and	O
makes	O
a	O
coupled-markov-chain	O
simulator	O
that	O
uses	O
fresh	O
random	B
numbers	O
every	O
time	O
t0	O
is	O
changed	O
.	O
describe	O
what	O
happens	O
if	O
fred	O
applies	O
his	O
method	B
to	O
the	O
markov	O
chain	B
that	O
is	O
intended	O
to	O
sample	B
from	I
the	O
uniform	O
distribution	B
over	O
the	O
states	O
0	O
,	O
1	O
,	O
and	O
2	O
,	O
using	O
the	O
metropolis	O
method	B
,	O
driven	O
by	O
a	O
random	B
bit	O
source	O
as	O
in	O
(	O
cid:12	O
)	O
gure	O
32.1b	O
.	O
exercise	O
32.3	O
.	O
[	O
5	O
]	O
investigate	O
the	O
application	O
of	O
perfect	O
sampling	O
to	O
linear	B
re-	O
gression	O
in	O
holmes	O
and	O
mallick	O
(	O
1998	O
)	O
or	O
holmes	O
and	O
denison	O
(	O
2002	O
)	O
and	O
try	O
to	O
generalize	O
it	O
.	O
exercise	O
32.4	O
.	O
[	O
3	O
]	O
the	O
concept	O
of	O
coalescence	O
has	O
many	O
applications	O
.	O
some	O
sur-	O
names	O
are	O
more	O
frequent	O
than	O
others	O
,	O
and	O
some	O
die	B
out	O
altogether	O
.	O
make	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
32.5	O
:	O
solutions	O
421	O
a	O
model	B
of	O
this	O
process	O
;	O
how	O
long	O
will	O
it	O
take	O
until	O
everyone	O
has	O
the	O
same	O
surname	O
?	O
similarly	O
,	O
variability	O
in	O
any	O
particular	O
portion	O
of	O
the	O
human	O
genome	B
(	O
which	O
forms	O
the	O
basis	O
of	O
forensic	B
dna	O
(	O
cid:12	O
)	O
ngerprinting	O
)	O
is	O
inherited	O
like	O
a	O
surname	O
.	O
a	O
dna	O
(	O
cid:12	O
)	O
ngerprint	O
is	O
like	O
a	O
string	O
of	O
surnames	O
.	O
should	O
the	O
fact	O
that	O
these	O
surnames	O
are	O
subject	O
to	O
coalescences	O
,	O
so	O
that	O
some	O
surnames	O
are	O
by	O
chance	O
more	O
prevalent	O
than	O
others	O
,	O
a	O
(	O
cid:11	O
)	O
ect	O
the	O
way	O
in	O
which	O
dna	O
(	O
cid:12	O
)	O
ngerprint	O
evidence	B
is	O
used	O
in	O
court	O
?	O
.	O
exercise	O
32.5	O
.	O
[	O
2	O
]	O
how	O
can	O
you	O
use	O
a	O
coin	B
to	O
create	O
a	O
random	B
ranking	O
of	O
3	O
people	O
?	O
construct	O
a	O
solution	O
that	O
uses	O
exact	O
sampling	O
.	O
for	O
example	O
,	O
you	O
could	O
apply	O
exact	B
sampling	I
to	O
a	O
markov	O
chain	B
in	O
which	O
the	O
coin	B
is	O
repeatedly	O
used	O
alternately	O
to	O
decide	O
whether	O
to	O
switch	O
(	O
cid:12	O
)	O
rst	O
and	O
second	O
,	O
then	O
whether	O
to	O
switch	O
second	O
and	O
third	O
.	O
exercise	O
32.6	O
.	O
[	O
5	O
]	O
finding	O
the	O
partition	B
function	I
z	O
of	O
a	O
probability	B
distribution	O
is	O
a	O
di	O
(	O
cid:14	O
)	O
cult	O
problem	O
.	O
many	O
markov	O
chain	B
monte	O
carlo	O
methods	B
produce	O
valid	O
samples	O
from	O
a	O
distribution	B
without	O
ever	O
(	O
cid:12	O
)	O
nding	O
out	O
what	O
z	O
is	O
.	O
is	O
there	O
any	O
probability	B
distribution	O
and	O
markov	O
chain	B
such	O
that	O
either	O
the	O
time	O
taken	O
to	O
produce	O
a	O
perfect	B
sample	O
or	O
the	O
number	O
of	O
random	O
bits	O
used	O
to	O
create	O
a	O
perfect	B
sample	O
are	O
related	O
to	O
the	O
value	O
of	O
z	O
?	O
are	O
there	O
some	O
situations	O
in	O
which	O
the	O
time	O
to	O
coalescence	B
conveys	O
information	B
about	O
z	O
?	O
32.5	O
solutions	O
solution	O
to	O
exercise	O
32.1	O
(	O
p.420	O
)	O
.	O
it	O
is	O
perhaps	O
surprising	O
that	O
there	O
is	O
no	O
di-	O
rect	O
relationship	O
between	O
the	O
equilibration	O
time	O
and	O
the	O
time	O
to	O
coalescence	B
.	O
we	O
can	O
prove	O
this	O
using	O
the	O
example	O
of	O
the	O
uniform	O
distribution	B
over	O
the	O
inte-	O
gers	O
a	O
=	O
f0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
20g	O
.	O
a	O
markov	O
chain	B
that	O
converges	O
to	O
this	O
distribution	B
in	O
exactly	O
one	O
iteration	O
is	O
the	O
chain	B
for	O
which	O
the	O
probability	O
of	O
state	O
st+1	O
given	O
st	O
is	O
the	O
uniform	O
distribution	B
,	O
for	O
all	O
st.	O
such	O
a	O
chain	B
can	O
be	O
coupled	O
to	O
a	O
random	B
number	I
generator	I
in	O
two	O
ways	O
:	O
(	O
a	O
)	O
we	O
could	O
draw	O
a	O
random	B
integer	O
u	O
2	O
a	O
,	O
and	O
set	O
st+1	O
equal	O
to	O
u	O
regardless	O
of	O
st	O
;	O
or	O
(	O
b	O
)	O
we	O
could	O
draw	O
a	O
random	B
integer	O
u	O
2	O
a	O
,	O
and	O
set	O
st+1	O
equal	O
to	O
(	O
st	O
+	O
u	O
)	O
mod	O
21.	O
method	B
(	O
b	O
)	O
would	O
produce	O
a	O
cohort	O
of	O
trajectories	O
locked	O
together	O
,	O
similar	O
to	O
the	O
trajec-	O
tories	O
in	O
(	O
cid:12	O
)	O
gure	O
32.1	O
,	O
except	O
that	O
no	O
coalescence	B
ever	O
occurs	O
.	O
thus	O
,	O
while	O
the	O
equilibration	O
times	O
of	O
methods	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
are	O
both	O
one	O
,	O
the	O
coalescence	B
times	O
are	O
respectively	O
one	O
and	O
in	O
(	O
cid:12	O
)	O
nity	O
.	O
it	O
seems	O
plausible	O
on	O
the	O
other	O
hand	O
that	O
coalescence	B
time	O
provides	O
some	O
sort	O
of	O
upper	O
bound	B
on	O
equilibration	O
time	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
gibbs	O
’	O
inequality	B
(	O
cid:12	O
)	O
rst	O
appeared	O
in	O
equation	O
(	O
1.24	O
)	O
;	O
see	O
also	O
exercise	O
2.26	O
(	O
p.37	O
)	O
.	O
33	O
variational	B
methods	I
variational	O
methods	B
are	O
an	O
important	O
technique	O
for	O
the	O
approximation	B
of	O
com-	O
plicated	O
probability	B
distributions	I
,	O
having	O
applications	O
in	O
statistical	O
physics	B
,	O
data	B
modelling	I
and	O
neural	O
networks	O
.	O
33.1	O
variational	B
free	I
energy	I
minimization	O
one	O
method	B
for	O
approximating	O
a	O
complex	B
distribution	O
in	O
a	O
physical	O
system	O
is	O
mean	B
(	O
cid:12	O
)	O
eld	O
theory	B
.	O
mean	B
(	O
cid:12	O
)	O
eld	O
theory	B
is	O
a	O
special	O
case	O
of	O
a	O
general	O
variational	O
free	B
energy	I
approach	O
of	O
feynman	O
and	O
bogoliubov	O
which	O
we	O
will	O
now	O
study	O
.	O
the	O
key	O
piece	O
of	O
mathematics	O
needed	O
to	O
understand	O
this	O
method	B
is	O
gibbs	O
’	O
inequality	B
,	O
which	O
we	O
repeat	O
here	O
.	O
the	O
relative	B
entropy	I
between	O
two	O
probability	B
distributions	I
q	O
(	O
x	O
)	O
and	O
p	O
(	O
x	O
)	O
that	O
are	O
de	O
(	O
cid:12	O
)	O
ned	O
over	O
the	O
same	O
alphabet	O
ax	O
is	O
dkl	O
(	O
qjjp	O
)	O
=xx	O
q	O
(	O
x	O
)	O
log	O
q	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
:	O
(	O
33.1	O
)	O
the	O
relative	B
entropy	I
satis	O
(	O
cid:12	O
)	O
es	O
dkl	O
(	O
qjjp	O
)	O
(	O
cid:21	O
)	O
0	O
(	O
gibbs	O
’	O
inequality	B
)	O
with	O
equality	O
only	O
if	O
q	O
=	O
p	O
.	O
in	O
general	O
dkl	O
(	O
qjjp	O
)	O
6=	O
dkl	O
(	O
pjjq	O
)	O
.	O
in	O
this	O
chapter	O
we	O
will	O
replace	O
the	O
log	O
by	O
ln	O
,	O
and	O
measure	O
the	O
divergence	B
in	O
nats	O
.	O
probability	B
distributions	I
in	O
statistical	B
physics	I
in	O
statistical	B
physics	I
one	O
often	O
encounters	O
probability	B
distributions	I
of	O
the	O
form	O
p	O
(	O
xj	O
(	O
cid:12	O
)	O
;	O
j	O
)	O
=	O
1	O
z	O
(	O
(	O
cid:12	O
)	O
;	O
j	O
)	O
exp	O
[	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
;	O
j	O
)	O
]	O
;	O
(	O
33.2	O
)	O
where	O
for	O
example	O
the	O
state	O
vector	O
is	O
x	O
2	O
f	O
(	O
cid:0	O
)	O
1	O
;	O
+1gn	O
,	O
and	O
e	O
(	O
x	O
;	O
j	O
)	O
is	O
some	O
energy	B
function	O
such	O
as	O
e	O
(	O
x	O
;	O
j	O
)	O
=	O
(	O
cid:0	O
)	O
1	O
2xm	O
;	O
n	O
jmnxmxn	O
(	O
cid:0	O
)	O
xn	O
hnxn	O
:	O
the	O
partition	B
function	I
(	O
normalizing	B
constant	I
)	O
is	O
z	O
(	O
(	O
cid:12	O
)	O
;	O
j	O
)	O
(	O
cid:17	O
)	O
xx	O
exp	O
[	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
;	O
j	O
)	O
]	O
:	O
(	O
33.3	O
)	O
(	O
33.4	O
)	O
the	O
probability	B
distribution	O
of	O
equation	O
(	O
33.2	O
)	O
is	O
complex	B
.	O
not	O
unbearably	O
complex	B
{	O
we	O
can	O
,	O
after	O
all	O
,	O
evaluate	O
e	O
(	O
x	O
;	O
j	O
)	O
for	O
any	O
particular	O
x	O
in	O
a	O
time	O
422	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
33.1	O
:	O
variational	B
free	I
energy	I
minimization	O
423	O
polynomial	O
in	O
the	O
number	O
of	O
spins	O
.	O
but	O
evaluating	O
the	O
normalizing	B
constant	I
z	O
(	O
(	O
cid:12	O
)	O
;	O
j	O
)	O
is	O
di	O
(	O
cid:14	O
)	O
cult	O
,	O
as	O
we	O
saw	O
in	O
chapter	O
29	O
,	O
and	O
describing	O
the	O
properties	O
of	O
the	O
probability	O
distribution	B
is	O
also	O
hard	O
.	O
knowing	O
the	O
value	O
of	O
e	O
(	O
x	O
;	O
j	O
)	O
at	O
a	O
few	O
arbitrary	O
points	O
x	O
,	O
for	O
example	O
,	O
gives	O
no	O
useful	O
information	B
about	O
what	O
the	O
average	B
properties	O
of	O
the	O
system	O
are	O
.	O
an	O
evaluation	O
of	O
z	O
(	O
(	O
cid:12	O
)	O
;	O
j	O
)	O
would	O
be	O
particularly	O
desirable	O
because	O
from	O
z	O
we	O
can	O
derive	O
all	O
the	O
thermodynamic	O
properties	O
of	O
the	O
system	O
.	O
variational	B
free	I
energy	I
minimization	O
is	O
a	O
method	B
for	O
approximating	O
the	O
complex	B
distribution	O
p	O
(	O
x	O
)	O
by	O
a	O
simpler	O
ensemble	B
q	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
that	O
is	O
parameterized	O
by	O
adjustable	O
parameters	B
(	O
cid:18	O
)	O
.	O
we	O
adjust	O
these	O
parameters	B
so	O
as	O
to	O
get	O
q	O
to	O
best	O
approximate	O
p	O
,	O
in	O
some	O
sense	O
.	O
a	O
by-product	O
of	O
this	O
approximation	B
is	O
a	O
lower	O
bound	B
on	O
z	O
(	O
(	O
cid:12	O
)	O
;	O
j	O
)	O
.	O
the	O
variational	B
free	I
energy	I
the	O
objective	B
function	I
chosen	O
to	O
measure	O
the	O
quality	O
of	O
the	O
approximation	O
is	O
the	O
variational	B
free	I
energy	I
(	O
cid:12	O
)	O
~f	O
(	O
(	O
cid:18	O
)	O
)	O
=xx	O
q	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
ln	O
q	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
exp	O
[	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
;	O
j	O
)	O
]	O
:	O
(	O
33.5	O
)	O
this	O
expression	O
can	O
be	O
manipulated	O
into	O
a	O
couple	O
of	O
interesting	O
forms	O
:	O
(	O
cid:12	O
)	O
rst	O
,	O
(	O
cid:12	O
)	O
~f	O
(	O
(	O
cid:18	O
)	O
)	O
=	O
(	O
cid:12	O
)	O
xx	O
q	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
e	O
(	O
x	O
;	O
j	O
)	O
(	O
cid:0	O
)	O
xx	O
(	O
cid:17	O
)	O
(	O
cid:12	O
)	O
he	O
(	O
x	O
;	O
j	O
)	O
iq	O
(	O
cid:0	O
)	O
sq	O
;	O
q	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
ln	O
1	O
q	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
(	O
33.6	O
)	O
(	O
33.7	O
)	O
where	O
he	O
(	O
x	O
;	O
j	O
)	O
iq	O
is	O
the	O
average	B
of	O
the	O
energy	B
function	O
under	O
the	O
distribution	B
q	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
,	O
and	O
sq	O
is	O
the	O
entropy	B
of	O
the	O
distribution	B
q	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
(	O
we	O
set	B
kb	O
to	O
one	O
in	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
s	O
so	O
that	O
it	O
is	O
identical	O
to	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
the	O
entropy	O
h	O
in	O
part	O
i	O
)	O
.	O
second	O
,	O
we	O
can	O
use	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
p	O
(	O
xj	O
(	O
cid:12	O
)	O
;	O
j	O
)	O
to	O
write	O
:	O
(	O
cid:12	O
)	O
~f	O
(	O
(	O
cid:18	O
)	O
)	O
=	O
xx	O
q	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
ln	O
q	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
p	O
(	O
xj	O
(	O
cid:12	O
)	O
;	O
j	O
)	O
(	O
cid:0	O
)	O
ln	O
z	O
(	O
(	O
cid:12	O
)	O
;	O
j	O
)	O
=	O
dkl	O
(	O
qjjp	O
)	O
+	O
(	O
cid:12	O
)	O
f	O
;	O
where	O
f	O
is	O
the	O
true	O
free	B
energy	I
,	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
(	O
cid:12	O
)	O
f	O
(	O
cid:17	O
)	O
(	O
cid:0	O
)	O
ln	O
z	O
(	O
(	O
cid:12	O
)	O
;	O
j	O
)	O
;	O
(	O
33.8	O
)	O
(	O
33.9	O
)	O
(	O
33.10	O
)	O
and	O
dkl	O
(	O
qjjp	O
)	O
is	O
the	O
relative	B
entropy	I
between	O
the	O
approximating	O
distribution	B
q	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
and	O
the	O
true	O
distribution	B
p	O
(	O
xj	O
(	O
cid:12	O
)	O
;	O
j	O
)	O
.	O
thus	O
by	O
gibbs	O
’	O
inequality	B
,	O
the	O
variational	B
free	I
energy	I
~f	O
(	O
(	O
cid:18	O
)	O
)	O
is	O
bounded	O
below	O
by	O
f	O
and	O
attains	O
this	O
value	O
only	O
for	O
q	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
=	O
p	O
(	O
xj	O
(	O
cid:12	O
)	O
;	O
j	O
)	O
.	O
our	O
strategy	O
is	O
thus	O
to	O
vary	O
(	O
cid:18	O
)	O
in	O
such	O
a	O
way	O
that	O
(	O
cid:12	O
)	O
~f	O
(	O
(	O
cid:18	O
)	O
)	O
is	O
minimized	O
.	O
the	O
approximating	O
distribution	B
then	O
gives	O
a	O
simpli	O
(	O
cid:12	O
)	O
ed	O
approximation	B
to	O
the	O
true	O
distribution	B
that	O
may	O
be	O
useful	O
,	O
and	O
the	O
value	O
of	O
(	O
cid:12	O
)	O
~f	O
(	O
(	O
cid:18	O
)	O
)	O
will	O
be	O
an	O
upper	O
bound	B
for	O
(	O
cid:12	O
)	O
f	O
.	O
equivalently	O
,	O
~z	O
(	O
cid:17	O
)	O
e	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
~f	O
(	O
can	O
the	O
objective	B
function	I
(	O
cid:12	O
)	O
~f	O
be	O
evaluated	O
?	O
)	O
is	O
a	O
lower	O
bound	B
for	O
z.	O
we	O
have	O
already	O
agreed	O
that	O
the	O
evaluation	O
of	O
various	O
interesting	O
sums	O
over	O
x	O
is	O
intractable	O
.	O
for	O
example	O
,	O
the	O
partition	B
function	I
z	O
=xx	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
;	O
j	O
)	O
)	O
;	O
(	O
33.11	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
33	O
|	O
variational	B
methods	I
(	O
33.12	O
)	O
(	O
33.13	O
)	O
424	O
the	O
energy	B
and	O
the	O
entropy	B
heip	O
=	O
e	O
(	O
x	O
;	O
j	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
;	O
j	O
)	O
)	O
;	O
1	O
z	O
xx	O
s	O
(	O
cid:17	O
)	O
xx	O
p	O
(	O
xj	O
(	O
cid:12	O
)	O
;	O
j	O
)	O
ln	O
1	O
p	O
(	O
xj	O
(	O
cid:12	O
)	O
;	O
j	O
)	O
are	O
all	O
presumed	O
to	O
be	O
impossible	O
to	O
evaluate	O
.	O
so	O
why	O
should	O
we	O
suppose	O
that	O
this	O
objective	B
function	I
(	O
cid:12	O
)	O
~f	O
(	O
(	O
cid:18	O
)	O
)	O
,	O
which	O
is	O
also	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
terms	O
of	O
a	O
sum	O
over	O
all	O
x	O
(	O
33.5	O
)	O
,	O
should	O
be	O
a	O
convenient	O
quantity	O
to	O
deal	O
with	O
?	O
well	O
,	O
for	O
a	O
range	O
of	O
interesting	O
energy	B
functions	O
,	O
and	O
for	O
su	O
(	O
cid:14	O
)	O
ciently	O
simple	O
approximating	O
distributions	O
,	O
the	O
variational	B
free	I
energy	I
can	O
be	O
e	O
(	O
cid:14	O
)	O
ciently	O
evaluated	O
.	O
33.2	O
variational	B
free	I
energy	I
minimization	O
for	O
spin	O
systems	O
an	O
example	O
of	O
a	O
tractable	O
variational	B
free	I
energy	I
is	O
given	O
by	O
the	O
spin	B
system	I
whose	O
energy	B
function	O
was	O
given	O
in	O
equation	O
(	O
33.3	O
)	O
,	O
which	O
we	O
can	O
approximate	O
with	O
a	O
separable	O
approximating	O
distribution	B
,	O
q	O
(	O
x	O
;	O
a	O
)	O
=	O
1	O
zq	O
exp	O
xn	O
anxn	O
!	O
:	O
(	O
33.14	O
)	O
the	O
variational	B
parameters	O
(	O
cid:18	O
)	O
of	O
the	O
variational	O
free	B
energy	I
(	O
33.5	O
)	O
are	O
the	O
components	O
of	O
the	O
vector	O
a.	O
to	O
evaluate	O
the	O
variational	B
free	I
energy	I
we	O
need	O
the	O
entropy	B
of	O
this	O
distribution	B
,	O
sq	O
=xx	O
q	O
(	O
x	O
;	O
a	O
)	O
ln	O
1	O
q	O
(	O
x	O
;	O
a	O
)	O
;	O
(	O
33.15	O
)	O
and	O
the	O
mean	B
of	O
the	O
energy	B
,	O
he	O
(	O
x	O
;	O
j	O
)	O
iq	O
=xx	O
q	O
(	O
x	O
;	O
a	O
)	O
e	O
(	O
x	O
;	O
j	O
)	O
:	O
(	O
33.16	O
)	O
the	O
entropy	B
of	O
the	O
separable	O
approximating	O
distribution	B
is	O
simply	O
the	O
sum	O
of	O
the	O
entropies	O
of	O
the	O
individual	O
spins	O
(	O
exercise	O
4.2	O
,	O
p.68	O
)	O
,	O
sq	O
=xn	O
h	O
(	O
e	O
)	O
2	O
(	O
qn	O
)	O
;	O
(	O
33.17	O
)	O
where	O
qn	O
is	O
the	O
probability	B
that	O
spin	O
n	O
is	O
+1	O
,	O
qn	O
=	O
ean	O
ean	O
+	O
e	O
(	O
cid:0	O
)	O
an	O
=	O
1	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
2an	O
)	O
;	O
(	O
33.18	O
)	O
and	O
h	O
(	O
e	O
)	O
2	O
(	O
q	O
)	O
=	O
q	O
ln	O
1	O
q	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
q	O
)	O
ln	O
1	O
(	O
1	O
(	O
cid:0	O
)	O
q	O
)	O
:	O
(	O
33.19	O
)	O
the	O
mean	B
energy	O
under	O
q	O
is	O
easy	O
to	O
obtain	O
because	O
pm	O
;	O
n	O
jmnxmxn	O
is	O
a	O
sum	O
of	O
terms	O
each	O
involving	O
the	O
product	O
of	O
two	O
independent	O
random	O
variables	O
.	O
(	O
there	O
are	O
no	O
self-couplings	O
,	O
so	O
jmn	O
=	O
0	O
when	O
m	O
=	O
n.	O
)	O
if	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
mean	B
value	O
of	O
xn	O
to	O
be	O
(	O
cid:22	O
)	O
xn	O
,	O
which	O
is	O
given	O
by	O
(	O
cid:22	O
)	O
xn	O
=	O
ean	O
(	O
cid:0	O
)	O
e	O
(	O
cid:0	O
)	O
an	O
ean	O
+	O
e	O
(	O
cid:0	O
)	O
an	O
=	O
tanh	O
(	O
an	O
)	O
=	O
2qn	O
(	O
cid:0	O
)	O
1	O
;	O
(	O
33.20	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
33.2	O
:	O
variational	B
free	I
energy	I
minimization	O
for	O
spin	O
systems	O
425	O
we	O
obtain	O
he	O
(	O
x	O
;	O
j	O
)	O
iq	O
=	O
xx	O
=	O
(	O
cid:0	O
)	O
1	O
1	O
q	O
(	O
x	O
;	O
a	O
)	O
''	O
(	O
cid:0	O
)	O
2xm	O
;	O
n	O
2xm	O
;	O
n	O
jmn	O
(	O
cid:22	O
)	O
xm	O
(	O
cid:22	O
)	O
xn	O
(	O
cid:0	O
)	O
xn	O
jmnxmxn	O
(	O
cid:0	O
)	O
xn	O
hnxn	O
#	O
(	O
33.21	O
)	O
hn	O
(	O
cid:22	O
)	O
xn	O
:	O
(	O
33.22	O
)	O
so	O
the	O
variational	B
free	I
energy	I
is	O
given	O
by	O
(	O
cid:12	O
)	O
~f	O
(	O
a	O
)	O
=	O
(	O
cid:12	O
)	O
he	O
(	O
x	O
;	O
j	O
)	O
iq	O
(	O
cid:0	O
)	O
sq	O
=	O
(	O
cid:12	O
)	O
(	O
cid:0	O
)	O
1	O
2xm	O
;	O
n	O
jmn	O
(	O
cid:22	O
)	O
xm	O
(	O
cid:22	O
)	O
xn	O
(	O
cid:0	O
)	O
xn	O
hn	O
(	O
cid:22	O
)	O
xn	O
!	O
(	O
cid:0	O
)	O
xn	O
h	O
(	O
e	O
)	O
2	O
(	O
qn	O
)	O
:	O
(	O
33.23	O
)	O
we	O
now	O
consider	O
minimizing	O
this	O
function	B
with	O
respect	O
to	O
the	O
variational	B
parameters	O
a.	O
if	O
q	O
=	O
1=	O
(	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
2a	O
)	O
,	O
the	O
derivative	O
of	O
the	O
entropy	O
is	O
@	O
@	O
q	O
h	O
e	O
2	O
(	O
q	O
)	O
=	O
ln	O
1	O
(	O
cid:0	O
)	O
q	O
q	O
=	O
(	O
cid:0	O
)	O
2a	O
:	O
(	O
33.24	O
)	O
so	O
we	O
obtain	O
@	O
@	O
am	O
(	O
cid:12	O
)	O
~f	O
(	O
a	O
)	O
=	O
(	O
cid:12	O
)	O
''	O
(	O
cid:0	O
)	O
xn	O
=	O
2	O
(	O
cid:18	O
)	O
@	O
qm	O
jmn	O
(	O
cid:22	O
)	O
xn	O
(	O
cid:0	O
)	O
hm	O
#	O
(	O
cid:18	O
)	O
2	O
qm	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
@	O
qm	O
@	O
am	O
(	O
cid:19	O
)	O
@	O
qm	O
@	O
am	O
(	O
cid:19	O
)	O
(	O
cid:0	O
)	O
ln	O
(	O
cid:18	O
)	O
1	O
(	O
cid:0	O
)	O
qm	O
jmn	O
(	O
cid:22	O
)	O
xn	O
+	O
hm	O
!	O
+	O
am	O
#	O
:	O
@	O
am	O
(	O
cid:19	O
)	O
''	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
xn	O
this	O
derivative	O
is	O
equal	O
to	O
zero	O
when	O
am	O
=	O
(	O
cid:12	O
)	O
xn	O
jmn	O
(	O
cid:22	O
)	O
xn	O
+	O
hm	O
!	O
:	O
so	O
~f	O
(	O
a	O
)	O
is	O
extremized	O
at	O
any	O
point	O
that	O
satis	O
(	O
cid:12	O
)	O
es	O
equation	O
(	O
33.26	O
)	O
and	O
(	O
cid:22	O
)	O
xn	O
=	O
tanh	O
(	O
an	O
)	O
:	O
(	O
33.27	O
)	O
the	O
variational	B
free	I
energy	I
~f	O
(	O
a	O
)	O
may	O
be	O
a	O
multimodal	O
function	B
,	O
in	O
which	O
case	O
each	O
stationary	O
point	O
(	O
maximum	O
,	O
minimum	O
or	O
saddle	O
)	O
will	O
satisfy	O
equa-	O
tions	O
(	O
33.26	O
)	O
and	O
(	O
33.27	O
)	O
.	O
one	O
way	O
of	O
using	O
these	O
equations	O
,	O
in	O
the	O
case	O
of	O
a	O
system	O
with	O
an	O
arbitrary	O
coupling	O
matrix	O
j	O
,	O
is	O
to	O
update	O
each	O
parameter	O
am	O
and	O
the	O
corresponding	O
value	O
of	O
(	O
cid:22	O
)	O
xm	O
using	O
equation	O
(	O
33.26	O
)	O
,	O
one	O
at	O
a	O
time	O
.	O
this	O
asynchronous	O
updating	O
of	O
the	O
parameters	O
is	O
guaranteed	O
to	O
decrease	O
(	O
cid:12	O
)	O
~f	O
(	O
a	O
)	O
.	O
equations	O
(	O
33.26	O
)	O
and	O
(	O
33.27	O
)	O
may	O
be	O
recognized	O
as	O
the	O
mean	B
(	O
cid:12	O
)	O
eld	O
equa-	O
tions	O
for	O
a	O
spin	B
system	I
.	O
the	O
variational	B
parameter	O
an	O
may	O
be	O
thought	O
of	O
as	O
the	O
strength	O
of	O
a	O
(	O
cid:12	O
)	O
ctitious	O
(	O
cid:12	O
)	O
eld	O
applied	O
to	O
an	O
isolated	O
spin	O
n.	O
equation	O
(	O
33.27	O
)	O
describes	O
the	O
mean	B
response	O
of	O
spin	O
n	O
,	O
and	O
equation	O
(	O
33.26	O
)	O
describes	O
how	O
the	O
(	O
cid:12	O
)	O
eld	O
am	O
is	O
set	B
in	O
response	O
to	O
the	O
mean	B
state	O
of	O
all	O
the	O
other	O
spins	O
.	O
the	O
variational	B
free	I
energy	I
derivation	O
is	O
a	O
helpful	O
viewpoint	O
for	O
mean	O
(	O
cid:12	O
)	O
eld	O
theory	B
for	O
two	O
reasons	O
.	O
1.	O
this	O
approach	O
associates	O
an	O
objective	B
function	I
(	O
cid:12	O
)	O
~f	O
with	O
the	O
mean	B
(	O
cid:12	O
)	O
eld	O
equations	O
;	O
such	O
an	O
objective	B
function	I
is	O
useful	O
because	O
it	O
can	O
help	O
identify	O
alternative	O
dynamical	O
systems	O
that	O
minimize	O
the	O
same	O
function	B
.	O
1	O
1	O
0.5	O
0.5	O
1	O
1	O
0.5	O
0.5	O
0	O
0	O
0	O
0	O
figure	O
33.1.	O
the	O
variational	B
free	I
energy	I
of	O
the	O
two-spin	O
system	O
whose	O
energy	B
is	O
e	O
(	O
x	O
)	O
=	O
(	O
cid:0	O
)	O
x1x2	O
,	O
as	O
a	O
function	B
of	O
the	O
two	O
variational	B
parameters	O
q1	O
and	O
q2	O
.	O
the	O
inverse-temperature	O
is	O
(	O
cid:12	O
)	O
=	O
1:44.	O
the	O
function	B
plotted	O
is	O
2	O
(	O
q2	O
)	O
;	O
(	O
cid:12	O
)	O
~f	O
=	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
(	O
cid:22	O
)	O
x1	O
(	O
cid:22	O
)	O
x2	O
(	O
cid:0	O
)	O
h	O
(	O
e	O
)	O
2	O
(	O
q1	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
e	O
)	O
where	O
(	O
cid:22	O
)	O
xn	O
=	O
2qn	O
(	O
cid:0	O
)	O
1.	O
notice	O
that	O
for	O
(	O
cid:12	O
)	O
xed	O
q2	O
the	O
function	B
is	O
convex	B
^	O
with	O
respect	O
to	O
q1	O
,	O
and	O
for	O
(	O
cid:12	O
)	O
xed	O
q1	O
it	O
is	O
convex	B
^	O
with	O
respect	O
to	O
q2	O
.	O
(	O
33.25	O
)	O
(	O
33.26	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
426	O
1	O
0.5	O
0	O
-0.5	O
-1	O
h	O
=	O
0.00	O
h	O
=	O
0.40	O
h	O
=	O
0.80	O
33	O
|	O
variational	B
methods	I
figure	O
33.2.	O
solutions	O
of	O
the	O
variational	O
free	B
energy	I
extremization	O
problem	O
for	O
the	O
ising	O
model	B
,	O
for	O
three	O
di	O
(	O
cid:11	O
)	O
erent	O
applied	O
(	O
cid:12	O
)	O
elds	O
h.	O
horizontal	O
axis	O
:	O
temperature	B
t	O
=	O
1=	O
(	O
cid:12	O
)	O
.	O
vertical	O
axis	O
:	O
magnetization	O
(	O
cid:22	O
)	O
x.	O
the	O
critical	O
temperature	O
found	O
by	O
mean	O
(	O
cid:12	O
)	O
eld	O
theory	B
is	O
t	O
mft	O
c	O
=	O
4	O
.	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
2.	O
the	O
theory	B
is	O
readily	O
generalized	B
to	O
other	O
approximating	O
distributions	O
.	O
we	O
can	O
imagine	O
introducing	O
a	O
more	O
complex	B
approximation	O
q	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
that	O
might	O
for	O
example	O
capture	O
correlations	B
among	O
the	O
spins	O
instead	O
of	O
mod-	O
elling	O
the	O
spins	O
as	O
independent	O
.	O
one	O
could	O
then	O
evaluate	O
the	O
variational	B
free	I
energy	I
and	O
optimize	O
the	O
parameters	B
(	O
cid:18	O
)	O
of	O
this	O
more	O
complex	B
approx-	O
imation	O
.	O
the	O
more	O
degrees	B
of	I
freedom	I
the	O
approximating	O
distribution	B
has	O
,	O
the	O
tighter	O
the	O
bound	B
on	O
the	O
free	B
energy	I
becomes	O
.	O
however	O
,	O
if	O
the	O
complexity	B
of	O
an	O
approximation	B
is	O
increased	O
,	O
the	O
evaluation	O
of	O
either	O
the	O
mean	B
energy	O
or	O
the	O
entropy	B
typically	O
becomes	O
more	O
challenging	O
.	O
33.3	O
example	O
:	O
mean	B
(	O
cid:12	O
)	O
eld	O
theory	B
for	O
the	O
ferromagnetic	B
ising	O
model	B
in	O
the	O
simple	O
ising	O
model	B
studied	O
in	O
chapter	O
31	O
,	O
every	O
coupling	O
jmn	O
is	O
equal	O
to	O
j	O
if	O
m	O
and	O
n	O
are	O
neighbours	O
and	O
zero	O
otherwise	O
.	O
there	O
is	O
an	O
applied	O
(	O
cid:12	O
)	O
eld	O
hn	O
=	O
h	O
that	O
is	O
the	O
same	O
for	O
all	O
spins	O
.	O
a	O
very	O
simple	O
approximating	O
distribution	B
is	O
one	O
with	O
just	O
a	O
single	O
variational	O
parameter	O
a	O
,	O
which	O
de	O
(	O
cid:12	O
)	O
nes	O
a	O
separable	O
distribution	B
q	O
(	O
x	O
;	O
a	O
)	O
=	O
1	O
zq	O
exp	O
xn	O
axn	O
!	O
in	O
which	O
all	O
spins	O
are	O
independent	O
and	O
have	O
the	O
same	O
probability	B
qn	O
=	O
1	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
2a	O
)	O
of	O
being	O
up	O
.	O
the	O
mean	B
magnetization	O
is	O
(	O
33.28	O
)	O
(	O
33.29	O
)	O
(	O
cid:22	O
)	O
x	O
=	O
tanh	O
(	O
a	O
)	O
(	O
33.30	O
)	O
and	O
the	O
equation	O
(	O
33.26	O
)	O
which	O
de	O
(	O
cid:12	O
)	O
nes	O
the	O
minimum	O
of	O
the	O
variational	B
free	I
energy	I
becomes	O
a	O
=	O
(	O
cid:12	O
)	O
(	O
cj	O
(	O
cid:22	O
)	O
x	O
+	O
h	O
)	O
;	O
(	O
33.31	O
)	O
where	O
c	O
is	O
the	O
number	O
of	O
couplings	O
that	O
a	O
spin	O
is	O
involved	O
in	O
{	O
c	O
=	O
4	O
in	O
the	O
case	O
of	O
a	O
rectangular	B
two-dimensional	O
ising	O
model	B
.	O
we	O
can	O
solve	O
equations	O
(	O
33.30	O
)	O
and	O
(	O
33.31	O
)	O
for	O
(	O
cid:22	O
)	O
x	O
numerically	O
{	O
in	O
fact	O
,	O
it	O
is	O
easiest	O
to	O
vary	O
(	O
cid:22	O
)	O
x	O
and	O
solve	O
for	O
(	O
cid:12	O
)	O
{	O
and	O
obtain	O
graphs	O
of	O
the	O
free	B
energy	I
minima	O
and	O
maxima	O
as	O
a	O
function	B
of	O
temperature	B
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
33.2.	O
the	O
solid	O
line	O
shows	O
(	O
cid:22	O
)	O
x	O
versus	O
t	O
=	O
1=	O
(	O
cid:12	O
)	O
for	O
the	O
case	O
c	O
=	O
4	O
;	O
j	O
=	O
1.	O
when	O
h	O
=	O
0	O
,	O
there	O
is	O
a	O
pitchfork	B
bifurcation	I
at	O
a	O
critical	O
temperature	O
t	O
mft	O
.	O
[	O
a	O
pitchfork	B
bifurcation	I
is	O
a	O
transition	B
like	O
the	O
one	O
shown	O
by	O
the	O
solid	O
lines	O
in	O
c	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
33.4	O
:	O
variational	B
methods	I
in	O
inference	B
and	O
data	B
modelling	I
427	O
(	O
cid:12	O
)	O
gure	O
33.2	O
,	O
from	O
a	O
system	O
with	O
one	O
minimum	O
as	O
a	O
function	B
of	O
a	O
(	O
on	O
the	O
right	O
)	O
to	O
a	O
system	O
(	O
on	O
the	O
left	O
)	O
with	O
two	O
minima	O
and	O
one	O
maximum	O
;	O
the	O
maximum	O
is	O
the	O
middle	O
one	O
of	O
the	O
three	O
lines	O
.	O
the	O
solid	O
lines	O
look	O
like	O
a	O
pitchfork	O
.	O
]	O
above	O
this	O
temperature	B
,	O
there	O
is	O
only	O
one	O
minimum	O
in	O
the	O
variational	B
free	I
energy	I
,	O
at	O
a	O
=	O
0	O
and	O
(	O
cid:22	O
)	O
x	O
=	O
0	O
;	O
this	O
minimum	O
corresponds	O
to	O
an	O
approximating	O
distribution	B
that	O
is	O
uniform	O
over	O
all	O
states	O
.	O
below	O
the	O
critical	O
temperature	O
,	O
there	O
are	O
two	O
minima	O
corresponding	O
to	O
approximating	O
distributions	O
that	O
are	O
symmetry-broken	O
,	O
with	O
all	O
spins	O
more	O
likely	O
to	O
be	O
up	O
,	O
or	O
all	O
spins	O
more	O
likely	O
to	O
be	O
down	O
.	O
the	O
state	O
(	O
cid:22	O
)	O
x	O
=	O
0	O
persists	O
as	O
a	O
stationary	O
point	O
of	O
the	O
variational	B
free	I
energy	I
,	O
but	O
now	O
it	O
is	O
a	O
local	O
maximum	O
of	O
the	O
variational	B
free	I
energy	I
.	O
when	O
h	O
>	O
0	O
,	O
there	O
is	O
a	O
global	O
variational	B
free	I
energy	I
minimum	O
at	O
any	O
temperature	B
for	O
a	O
positive	O
value	O
of	O
(	O
cid:22	O
)	O
x	O
,	O
shown	O
by	O
the	O
upper	O
dotted	O
curves	O
in	O
(	O
cid:12	O
)	O
gure	O
33.2.	O
as	O
long	O
as	O
h	O
<	O
jc	O
,	O
there	O
is	O
also	O
a	O
second	O
local	O
minimum	O
in	O
the	O
free	B
energy	I
,	O
if	O
the	O
temperature	B
is	O
su	O
(	O
cid:14	O
)	O
ciently	O
small	O
.	O
this	O
second	O
minimum	O
cor-	O
responds	O
to	O
a	O
self-preserving	O
state	O
of	O
magnetization	O
in	O
the	O
opposite	O
direction	O
to	O
the	O
applied	O
(	O
cid:12	O
)	O
eld	O
.	O
the	O
temperature	B
at	O
which	O
the	O
second	O
minimum	O
appears	O
is	O
smaller	O
than	O
t	O
mft	O
,	O
and	O
when	O
it	O
appears	O
,	O
it	O
is	O
accompanied	O
by	O
a	O
saddle	O
point	O
located	O
between	O
the	O
two	O
minima	O
.	O
a	O
name	O
given	O
to	O
this	O
type	O
of	O
bifurcation	O
is	O
a	O
saddle-node	O
bifurcation	B
.	O
c	O
the	O
variational	B
free	I
energy	I
per	O
spin	O
is	O
given	O
by	O
(	O
cid:12	O
)	O
~f	O
=	O
(	O
cid:12	O
)	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
c	O
2	O
j	O
(	O
cid:22	O
)	O
x2	O
(	O
cid:0	O
)	O
h	O
(	O
cid:22	O
)	O
x	O
(	O
cid:19	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
e	O
)	O
2	O
(	O
cid:18	O
)	O
(	O
cid:22	O
)	O
x	O
+	O
1	O
2	O
(	O
cid:19	O
)	O
:	O
(	O
33.32	O
)	O
exercise	O
33.1	O
.	O
[	O
2	O
]	O
sketch	O
the	O
variational	B
free	I
energy	I
as	O
a	O
function	B
of	O
its	O
one	O
parameter	O
(	O
cid:22	O
)	O
x	O
for	O
a	O
variety	O
of	O
values	O
of	O
the	O
temperature	O
t	O
and	O
the	O
applied	O
(	O
cid:12	O
)	O
eld	O
h.	O
figure	O
33.2	O
reproduces	O
the	O
key	O
properties	O
of	O
the	O
real	O
ising	O
system	O
{	O
that	O
,	O
for	O
h	O
=	O
0	O
,	O
there	O
is	O
a	O
critical	O
temperature	O
below	O
which	O
the	O
system	O
has	O
long-	O
range	O
order	O
,	O
and	O
that	O
it	O
can	O
adopt	O
one	O
of	O
two	O
macroscopic	O
states	O
.	O
however	O
,	O
by	O
probing	O
a	O
little	O
more	O
we	O
can	O
reveal	O
some	O
inadequacies	O
of	O
the	O
variational	O
approximation	B
.	O
to	O
start	O
with	O
,	O
the	O
critical	O
temperature	O
t	O
mft	O
is	O
4	O
,	O
which	O
is	O
nearly	O
a	O
factor	O
of	O
2	O
greater	O
than	O
the	O
true	O
critical	O
temperature	O
tc	O
=	O
2:27.	O
also	O
,	O
the	O
variational	B
model	O
has	O
equivalent	O
properties	O
in	O
any	O
number	O
of	O
dimensions	O
,	O
including	O
d	O
=	O
1	O
,	O
where	O
the	O
true	O
system	O
does	O
not	O
have	O
a	O
phase	B
transition	I
.	O
so	O
the	O
bifurcation	B
at	O
t	O
mft	O
should	O
not	O
be	O
described	O
as	O
a	O
phase	B
transition	I
.	O
c	O
c	O
for	O
the	O
case	O
h	O
=	O
0	O
we	O
can	O
follow	O
the	O
trajectory	O
of	O
the	O
global	O
minimum	O
as	O
a	O
function	B
of	O
(	O
cid:12	O
)	O
and	O
(	O
cid:12	O
)	O
nd	O
the	O
entropy	B
,	O
heat	B
capacity	I
and	O
(	O
cid:13	O
)	O
uctuations	O
of	O
the	O
ap-	O
proximating	O
distribution	B
and	O
compare	O
them	O
with	O
those	O
of	O
a	O
real	O
8	O
(	O
cid:2	O
)	O
8	O
fragment	O
using	O
the	O
matrix	B
method	O
of	O
chapter	O
31.	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
33.3	O
,	O
one	O
of	O
the	O
biggest	O
di	O
(	O
cid:11	O
)	O
erences	O
is	O
in	O
the	O
(	O
cid:13	O
)	O
uctuations	O
in	O
energy	O
.	O
the	O
real	O
system	O
has	O
large	O
(	O
cid:13	O
)	O
uctuations	O
near	O
the	O
critical	O
temperature	O
,	O
whereas	O
the	O
approximating	O
distri-	O
bution	O
has	O
no	O
correlations	B
among	O
its	O
spins	O
and	O
thus	O
has	O
an	O
energy-variance	O
which	O
scales	O
simply	O
linearly	O
with	O
the	O
number	O
of	O
spins	O
.	O
33.4	O
variational	B
methods	I
in	O
inference	B
and	O
data	B
modelling	I
in	O
statistical	B
data	O
modelling	B
we	O
are	O
interested	O
in	O
the	O
posterior	B
probability	I
distribution	O
of	O
a	O
parameter	O
vector	O
w	O
given	O
data	O
d	O
and	O
model	O
assumptions	B
h	O
,	O
p	O
(	O
w	O
j	O
d	O
;	O
h	O
)	O
.	O
:	O
(	O
33.33	O
)	O
p	O
(	O
w	O
j	O
d	O
;	O
h	O
)	O
=	O
p	O
(	O
d	O
j	O
w	O
;	O
h	O
)	O
p	O
(	O
w	O
jh	O
)	O
p	O
(	O
d	O
jh	O
)	O
in	O
traditional	O
approaches	O
to	O
model	B
(	O
cid:12	O
)	O
tting	O
,	O
a	O
single	O
parameter	O
vector	O
w	O
is	O
op-	O
timized	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
mode	O
of	O
this	O
distribution	B
.	O
what	O
is	O
really	O
of	O
interest	O
is	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
428	O
33	O
|	O
variational	B
methods	I
free	O
energy	B
energy	O
figure	O
33.3.	O
comparison	O
of	O
approximating	O
distribution	B
’	O
s	O
properties	O
with	O
those	O
of	O
a	O
real	O
8	O
(	O
cid:2	O
)	O
8	O
fragment	O
.	O
notice	O
that	O
the	O
variational	B
free	I
energy	I
of	O
the	O
approximating	O
distribution	B
is	O
indeed	O
an	O
upper	O
bound	B
on	O
the	O
free	B
energy	I
of	O
the	O
real	O
system	O
.	O
all	O
quantities	O
are	O
shown	O
‘	O
per	O
spin	O
’	O
.	O
mean	B
field	O
theory	B
real	O
8x8	O
system	O
0	O
-0.5	O
-1	O
-1.5	O
-2	O
0	O
-2	O
-2.5	O
-3	O
-3.5	O
-4	O
-4.5	O
-5	O
-5.5	O
-6	O
0	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
6	O
5	O
4	O
3	O
2	O
1	O
0	O
-1	O
0	O
mean	B
field	O
theory	B
real	O
8x8	O
system	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
entropy	B
mean	O
field	O
theory	B
real	O
8x8	O
system	O
3	O
2	O
1	O
6	O
fluctuations	O
,	O
var	O
(	O
e	O
)	O
4	O
5	O
7	O
8	O
mean	B
field	O
theory	B
real	O
8x8	O
system	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
3	O
2	O
1	O
6	O
heat	B
capacity	I
,	O
de=dt	O
4	O
5	O
7	O
8	O
1.6	O
1.4	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
-0.2	O
mean	B
field	O
theory	B
real	O
8x8	O
system	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
33.5	O
:	O
the	O
case	O
of	O
an	O
unknown	O
gaussian	O
429	O
the	O
whole	O
distribution	B
.	O
we	O
may	O
also	O
be	O
interested	O
in	O
its	O
normalizing	B
constant	I
p	O
(	O
d	O
jh	O
)	O
if	O
we	O
wish	O
to	O
do	O
model	O
comparison	O
.	O
the	O
probability	B
distribution	O
p	O
(	O
w	O
j	O
d	O
;	O
h	O
)	O
is	O
often	O
a	O
complex	B
distribution	O
.	O
in	O
a	O
variational	B
approach	O
to	O
in-	O
ference	O
,	O
we	O
introduce	O
an	O
approximating	O
probability	B
distribution	O
over	O
the	O
pa-	O
rameters	O
,	O
q	O
(	O
w	O
;	O
(	O
cid:18	O
)	O
)	O
,	O
and	O
optimize	O
this	O
distribution	B
(	O
by	O
varying	O
its	O
own	O
param-	O
eters	O
(	O
cid:18	O
)	O
)	O
so	O
that	O
it	O
approximates	O
the	O
posterior	O
distribution	O
of	O
the	O
parameters	O
p	O
(	O
w	O
j	O
d	O
;	O
h	O
)	O
well	O
.	O
proximation	O
is	O
the	O
variational	B
free	I
energy	I
one	O
objective	B
function	I
we	O
may	O
choose	O
to	O
measure	O
the	O
quality	O
of	O
the	O
ap-	O
~f	O
(	O
(	O
cid:18	O
)	O
)	O
=z	O
dkw	O
q	O
(	O
w	O
;	O
(	O
cid:18	O
)	O
)	O
ln	O
q	O
(	O
w	O
;	O
(	O
cid:18	O
)	O
)	O
p	O
(	O
d	O
j	O
w	O
;	O
h	O
)	O
p	O
(	O
w	O
jh	O
)	O
:	O
(	O
33.34	O
)	O
the	O
denominator	O
p	O
(	O
d	O
j	O
w	O
;	O
h	O
)	O
p	O
(	O
w	O
jh	O
)	O
is	O
,	O
within	O
a	O
multiplicative	O
constant	O
,	O
the	O
posterior	B
probability	I
p	O
(	O
w	O
j	O
d	O
;	O
h	O
)	O
=	O
p	O
(	O
d	O
j	O
w	O
;	O
h	O
)	O
p	O
(	O
w	O
jh	O
)	O
=p	O
(	O
d	O
jh	O
)	O
:	O
so	O
the	O
variational	B
free	I
energy	I
~f	O
(	O
(	O
cid:18	O
)	O
)	O
can	O
be	O
viewed	O
as	O
the	O
sum	O
of	O
(	O
cid:0	O
)	O
ln	O
p	O
(	O
d	O
jh	O
)	O
and	O
the	O
relative	B
entropy	I
between	O
q	O
(	O
w	O
;	O
(	O
cid:18	O
)	O
)	O
and	O
p	O
(	O
w	O
j	O
d	O
;	O
h	O
)	O
.	O
~f	O
(	O
(	O
cid:18	O
)	O
)	O
is	O
bounded	O
below	O
by	O
(	O
cid:0	O
)	O
ln	O
p	O
(	O
d	O
jh	O
)	O
and	O
only	O
attains	O
this	O
value	O
for	O
q	O
(	O
w	O
;	O
(	O
cid:18	O
)	O
)	O
=	O
p	O
(	O
w	O
j	O
d	O
;	O
h	O
)	O
.	O
for	O
certain	O
models	O
and	O
certain	O
approximating	O
distributions	O
,	O
this	O
free	B
energy	I
,	O
and	O
its	O
derivatives	O
with	O
respect	O
to	O
the	O
approximating	O
distribution	B
’	O
s	O
parameters	B
,	O
can	O
be	O
evaluated	O
.	O
the	O
approximation	B
of	O
posterior	B
probability	I
distributions	O
using	O
variational	B
free	I
energy	I
minimization	O
provides	O
a	O
useful	O
approach	O
to	O
approximating	O
bayesian	O
inference	B
in	O
a	O
number	O
of	O
(	O
cid:12	O
)	O
elds	O
ranging	O
from	O
neural	O
networks	O
to	O
the	O
decoding	B
of	O
error-correcting	B
codes	I
(	O
hinton	O
and	O
van	O
camp	O
,	O
1993	O
;	O
hinton	O
and	O
zemel	O
,	O
1994	O
;	O
dayan	O
et	O
al.	O
,	O
1995	O
;	O
neal	O
and	O
hinton	O
,	O
1998	O
;	O
mackay	O
,	O
1995a	O
)	O
.	O
the	O
method	B
is	O
sometimes	O
called	O
ensemble	B
learning	I
to	O
contrast	O
it	O
with	O
traditional	O
learning	B
processes	O
in	O
which	O
a	O
single	O
parameter	O
vector	O
is	O
optimized	O
.	O
another	O
name	O
for	O
it	O
is	O
variational	B
bayes	O
.	O
let	O
us	O
examine	O
how	O
ensemble	O
learning	B
works	O
in	O
the	O
simple	O
case	O
of	O
a	O
gaussian	O
distribution	B
.	O
33.5	O
the	O
case	O
of	O
an	O
unknown	O
gaussian	O
:	O
approximating	O
the	O
posterior	O
distribution	O
of	O
(	O
cid:22	O
)	O
and	O
(	O
cid:27	O
)	O
we	O
will	O
(	O
cid:12	O
)	O
t	O
an	O
approximating	O
ensemble	B
q	O
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
to	O
the	O
posterior	O
distribution	O
that	O
we	O
studied	O
in	O
chapter	O
24	O
,	O
p	O
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
jfxngn	O
n=1	O
)	O
=	O
=	O
p	O
(	O
fxngn	O
n=1	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
p	O
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
p	O
(	O
fxngn	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
)	O
n=2	O
exp	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
n	O
(	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2+s	O
n=1	O
)	O
1	O
2	O
(	O
cid:27	O
)	O
2	O
n=1	O
)	O
p	O
(	O
fxngn	O
(	O
33.35	O
)	O
(	O
33.36	O
)	O
(	O
cid:17	O
)	O
1	O
(	O
cid:27	O
)	O
(	O
cid:22	O
)	O
1	O
(	O
cid:27	O
)	O
:	O
we	O
make	O
the	O
single	O
assumption	O
that	O
the	O
approximating	O
ensemble	B
is	O
separable	O
in	O
the	O
form	O
q	O
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
=	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
.	O
no	O
restrictions	O
on	O
the	O
functional	O
form	O
of	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
and	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
are	O
made	O
.	O
we	O
write	O
down	O
a	O
variational	B
free	I
energy	I
,	O
~f	O
(	O
q	O
)	O
=z	O
d	O
(	O
cid:22	O
)	O
d	O
(	O
cid:27	O
)	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
ln	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
p	O
(	O
d	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
p	O
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
:	O
(	O
33.37	O
)	O
we	O
can	O
(	O
cid:12	O
)	O
nd	O
the	O
optimal	B
separable	O
distribution	B
q	O
by	O
considering	O
separately	O
the	O
optimization	B
of	O
~f	O
over	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
for	O
(	O
cid:12	O
)	O
xed	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
,	O
and	O
then	O
the	O
optimization	B
of	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
for	O
(	O
cid:12	O
)	O
xed	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
(	O
c	O
)	O
(	O
e	O
)	O
(	O
cid:27	O
)	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
(	O
cid:27	O
)	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0	O
0.5	O
1	O
1.5	O
2	O
(	O
cid:22	O
)	O
0	O
0.5	O
1	O
1.5	O
2	O
(	O
cid:22	O
)	O
33	O
|	O
variational	B
methods	I
figure	O
33.4.	O
optimization	B
of	O
an	O
approximating	O
distribution	B
.	O
the	O
posterior	O
distribution	O
p	O
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
jfxng	O
)	O
,	O
which	O
is	O
the	O
same	O
as	O
that	O
in	O
(	O
cid:12	O
)	O
gure	O
24.1	O
,	O
is	O
shown	O
by	O
solid	O
contours	O
.	O
(	O
a	O
)	O
initial	O
condition	O
.	O
the	O
approximating	O
distribution	B
q	O
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
(	O
dotted	O
contours	O
)	O
is	O
an	O
arbitrary	O
separable	O
distribution	B
.	O
(	O
b	O
)	O
q	O
(	O
cid:22	O
)	O
has	O
been	O
updated	O
,	O
using	O
equation	O
(	O
33.41	O
)	O
.	O
(	O
c	O
)	O
q	O
(	O
cid:27	O
)	O
has	O
been	O
updated	O
,	O
using	O
equation	O
(	O
33.44	O
)	O
.	O
(	O
d	O
)	O
q	O
(	O
cid:22	O
)	O
updated	O
again	O
.	O
(	O
e	O
)	O
q	O
(	O
cid:27	O
)	O
updated	O
again	O
.	O
(	O
f	O
)	O
converged	O
approximation	B
(	O
after	O
15	O
iterations	O
)	O
.	O
the	O
arrows	O
point	O
to	O
the	O
peaks	O
of	O
the	O
two	O
distributions	O
,	O
which	O
are	O
at	O
(	O
cid:27	O
)	O
n	O
=	O
0:45	O
(	O
for	O
p	O
)	O
and	O
(	O
cid:27	O
)	O
n	O
(	O
cid:0	O
)	O
1	O
=	O
0:5	O
(	O
for	O
q	O
)	O
.	O
430	O
(	O
a	O
)	O
(	O
cid:27	O
)	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0	O
0.5	O
1	O
1.5	O
2	O
(	O
cid:22	O
)	O
:	O
:	O
:	O
(	O
b	O
)	O
(	O
d	O
)	O
(	O
f	O
)	O
(	O
cid:27	O
)	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
(	O
cid:27	O
)	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
(	O
cid:27	O
)	O
1	O
0.9	O
0.8	O
0.7	O
0.6	O
0.5	O
0.4	O
0.3	O
0.2	O
0	O
0.5	O
1	O
1.5	O
2	O
(	O
cid:22	O
)	O
0	O
0.5	O
1	O
1.5	O
2	O
(	O
cid:22	O
)	O
0	O
0.5	O
1	O
1.5	O
2	O
(	O
cid:22	O
)	O
optimization	B
of	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
as	O
a	O
functional	O
of	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
,	O
~f	O
is	O
:	O
~f	O
=	O
(	O
cid:0	O
)	O
z	O
d	O
(	O
cid:22	O
)	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
(	O
cid:20	O
)	O
z	O
d	O
(	O
cid:27	O
)	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
ln	O
p	O
(	O
d	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
+	O
ln	O
[	O
p	O
(	O
(	O
cid:22	O
)	O
)	O
=q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
]	O
(	O
cid:21	O
)	O
+	O
(	O
cid:20	O
)	O
(	O
33.38	O
)	O
=	O
z	O
d	O
(	O
cid:22	O
)	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
(	O
cid:20	O
)	O
z	O
d	O
(	O
cid:27	O
)	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
n	O
(	O
cid:12	O
)	O
(	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
+	O
ln	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
(	O
cid:21	O
)	O
+	O
(	O
cid:20	O
)	O
0	O
;	O
(	O
33.39	O
)	O
1	O
2	O
where	O
(	O
cid:12	O
)	O
(	O
cid:17	O
)	O
1=	O
(	O
cid:27	O
)	O
2	O
and	O
(	O
cid:20	O
)	O
denote	O
constants	O
that	O
do	O
not	O
depend	O
on	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
.	O
the	O
dependence	O
on	O
q	O
(	O
cid:27	O
)	O
thus	O
collapses	O
down	O
to	O
a	O
simple	O
dependence	O
on	O
the	O
mean	B
(	O
cid:22	O
)	O
(	O
cid:12	O
)	O
(	O
cid:17	O
)	O
z	O
d	O
(	O
cid:27	O
)	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
1=	O
(	O
cid:27	O
)	O
2	O
:	O
now	O
we	O
can	O
recognize	O
the	O
function	B
(	O
cid:0	O
)	O
n	O
(	O
cid:22	O
)	O
(	O
cid:12	O
)	O
1	O
2	O
(	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
x	O
)	O
2	O
as	O
the	O
logarithm	O
of	O
a	O
gaussian	O
identical	O
to	O
the	O
posterior	O
distribution	O
for	O
a	O
particular	O
value	O
of	O
(	O
cid:12	O
)	O
=	O
(	O
cid:22	O
)	O
(	O
cid:12	O
)	O
.	O
since	O
a	O
relative	B
entropy	I
r	O
q	O
ln	O
(	O
q=p	O
)	O
is	O
minimized	O
by	O
setting	O
q	O
=	O
p	O
,	O
we	O
can	O
immediately	O
write	O
down	O
the	O
distribution	B
qopt	O
q	O
(	O
cid:27	O
)	O
:	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
that	O
minimizes	O
~f	O
for	O
(	O
cid:12	O
)	O
xed	O
(	O
33.40	O
)	O
(	O
33.41	O
)	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
=	O
p	O
(	O
(	O
cid:22	O
)	O
j	O
d	O
;	O
(	O
cid:22	O
)	O
(	O
cid:12	O
)	O
;	O
h	O
)	O
=	O
normal	B
(	O
(	O
cid:22	O
)	O
;	O
(	O
cid:22	O
)	O
x	O
;	O
(	O
cid:27	O
)	O
2	O
qopt	O
(	O
cid:22	O
)	O
jd	O
)	O
:	O
where	O
(	O
cid:27	O
)	O
2	O
(	O
cid:22	O
)	O
jd	O
=	O
1=	O
(	O
n	O
(	O
cid:22	O
)	O
(	O
cid:12	O
)	O
)	O
.	O
optimization	B
of	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
we	O
represent	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
using	O
the	O
density	B
over	O
(	O
cid:12	O
)	O
,	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:12	O
)	O
)	O
(	O
cid:17	O
)	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
jd	O
(	O
cid:27	O
)	O
=d	O
(	O
cid:12	O
)	O
j.	O
as	O
a	O
functional	O
of	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:12	O
)	O
)	O
,	O
~f	O
is	O
(	O
neglecting	O
additive	O
constants	O
)	O
:	O
~f	O
=	O
(	O
cid:0	O
)	O
z	O
d	O
(	O
cid:12	O
)	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:12	O
)	O
)	O
(	O
cid:20	O
)	O
z	O
d	O
(	O
cid:22	O
)	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
ln	O
p	O
(	O
d	O
j	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
)	O
+	O
ln	O
[	O
p	O
(	O
(	O
cid:12	O
)	O
)	O
=q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:12	O
)	O
)	O
]	O
(	O
cid:21	O
)	O
(	O
33.42	O
)	O
=	O
z	O
d	O
(	O
cid:12	O
)	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:12	O
)	O
)	O
h	O
(	O
n	O
(	O
cid:27	O
)	O
2	O
2	O
(	O
cid:0	O
)	O
1	O
(	O
cid:1	O
)	O
ln	O
(	O
cid:12	O
)	O
+	O
ln	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:12	O
)	O
)	O
i	O
;	O
(	O
33.43	O
)	O
(	O
cid:22	O
)	O
jd	O
+	O
s	O
)	O
(	O
cid:12	O
)	O
=2	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
n	O
the	O
prior	B
p	O
(	O
(	O
cid:27	O
)	O
)	O
/	O
1=	O
(	O
cid:27	O
)	O
transforms	O
to	O
p	O
(	O
(	O
cid:12	O
)	O
)	O
/	O
1=	O
(	O
cid:12	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
33.6	O
:	O
interlude	O
431	O
where	O
the	O
integral	B
over	O
(	O
cid:22	O
)	O
is	O
performed	O
assuming	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
=	O
qopt	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
:	O
here	O
,	O
the	O
(	O
cid:12	O
)	O
-	O
dependent	O
expression	O
in	O
square	O
brackets	O
can	O
be	O
recognized	O
as	O
the	O
logarithm	O
of	O
a	O
gamma	B
distribution	I
over	O
(	O
cid:12	O
)	O
{	O
see	O
equation	O
(	O
23.15	O
)	O
{	O
giving	O
as	O
the	O
distribution	B
that	O
minimizes	O
~f	O
for	O
(	O
cid:12	O
)	O
xed	O
q	O
(	O
cid:22	O
)	O
:	O
with	O
qopt	O
(	O
cid:27	O
)	O
(	O
(	O
cid:12	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
(	O
(	O
cid:12	O
)	O
;	O
b0	O
;	O
c0	O
)	O
;	O
1	O
b0	O
=	O
1	O
2	O
(	O
n	O
(	O
cid:27	O
)	O
2	O
(	O
cid:22	O
)	O
jd	O
+	O
s	O
)	O
and	O
c0	O
=	O
n	O
2	O
:	O
(	O
33.44	O
)	O
(	O
33.45	O
)	O
in	O
(	O
cid:12	O
)	O
gure	O
33.4	O
,	O
these	O
two	O
update	O
rules	B
(	O
33.41	O
,	O
33.44	O
)	O
are	O
applied	O
alternately	O
,	O
starting	O
from	O
an	O
arbitrary	O
initial	O
condition	O
.	O
the	O
algorithm	B
converges	O
to	O
the	O
optimal	B
approximating	O
ensemble	B
in	O
a	O
few	O
iterations	O
.	O
direct	O
solution	O
for	O
the	O
joint	B
optimum	O
q	O
(	O
cid:22	O
)	O
(	O
(	O
cid:22	O
)	O
)	O
q	O
(	O
cid:27	O
)	O
(	O
(	O
cid:27	O
)	O
)	O
in	O
this	O
problem	O
,	O
we	O
do	O
not	O
need	O
to	O
resort	O
to	O
iterative	O
computation	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
optimal	B
approximating	O
ensemble	B
.	O
equations	O
(	O
33.41	O
)	O
and	O
(	O
33.44	O
)	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
optimum	O
implicitly	O
.	O
we	O
must	O
simultaneously	O
have	O
(	O
cid:27	O
)	O
2	O
(	O
cid:22	O
)	O
(	O
cid:12	O
)	O
=	O
b0c0	O
.	O
the	O
solution	O
is	O
:	O
(	O
cid:22	O
)	O
jd	O
=	O
1=	O
(	O
n	O
(	O
cid:22	O
)	O
(	O
cid:12	O
)	O
)	O
,	O
and	O
1=	O
(	O
cid:22	O
)	O
(	O
cid:12	O
)	O
=	O
s=	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
:	O
(	O
33.46	O
)	O
this	O
is	O
similar	O
to	O
the	O
true	O
posterior	O
distribution	O
of	O
(	O
cid:27	O
)	O
,	O
which	O
is	O
a	O
gamma	B
distri-	O
bution	O
with	O
c0	O
=	O
n	O
(	O
cid:0	O
)	O
1	O
and	O
1=b0	O
=	O
s=2	O
(	O
see	O
equation	O
24.13	O
)	O
.	O
this	O
true	O
posterior	O
2	O
also	O
has	O
a	O
mean	B
value	O
of	O
(	O
cid:12	O
)	O
satisfying	O
1=	O
(	O
cid:22	O
)	O
(	O
cid:12	O
)	O
=	O
s=	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
;	O
the	O
only	O
di	O
(	O
cid:11	O
)	O
erence	O
is	O
that	O
the	O
approximating	O
distribution	B
’	O
s	O
parameter	O
c0	O
is	O
too	O
large	O
by	O
1=2	O
.	O
the	O
approximations	O
given	O
by	O
variational	O
free	B
energy	I
minimization	O
always	O
tend	O
to	O
be	O
more	O
compact	O
than	O
the	O
true	O
distribution	B
.	O
in	O
conclusion	O
,	O
ensemble	B
learning	I
gives	O
an	O
approximation	B
to	O
the	O
posterior	O
that	O
agrees	O
nicely	O
with	O
the	O
conventional	O
estimators	O
.	O
the	O
approximate	O
poste-	O
rior	O
distribution	B
over	O
(	O
cid:12	O
)	O
is	O
a	O
gamma	B
distribution	I
with	O
mean	B
(	O
cid:22	O
)	O
(	O
cid:12	O
)	O
corresponding	O
to	O
a	O
variance	B
of	O
(	O
cid:27	O
)	O
2	O
=	O
s=	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
=	O
(	O
cid:27	O
)	O
2	O
n	O
(	O
cid:0	O
)	O
1.	O
and	O
the	O
approximate	O
posterior	O
dis-	O
tribution	O
over	O
(	O
cid:22	O
)	O
is	O
a	O
gaussian	O
with	O
mean	O
(	O
cid:22	O
)	O
x	O
and	O
standard	O
deviation	O
(	O
cid:27	O
)	O
n	O
(	O
cid:0	O
)	O
1=pn	O
.	O
the	O
variational	B
free	I
energy	I
minimization	O
approach	O
has	O
the	O
nice	O
prop-	O
erty	O
that	O
it	O
is	O
parameterization-independent	O
;	O
it	O
avoids	O
the	O
problem	O
of	O
basis-	O
dependence	O
from	O
which	O
map	O
methods	B
and	O
laplace	O
’	O
s	O
method	B
su	O
(	O
cid:11	O
)	O
er	O
.	O
a	O
convenient	O
software	B
package	O
for	O
automatic	O
implementation	O
of	O
variational	O
inference	B
in	O
graphical	O
models	O
is	O
vibes	O
(	O
bishop	O
et	O
al.	O
,	O
2002	O
)	O
.	O
it	O
plays	O
the	O
same	O
role	O
for	O
variational	O
inference	B
as	O
bugs	O
plays	O
for	O
monte	O
carlo	O
inference	B
.	O
33.6	O
interlude	O
one	O
of	O
my	O
students	O
asked	O
:	O
how	O
do	O
you	O
ever	O
come	O
up	O
with	O
a	O
useful	O
approximating	O
distribution	B
,	O
given	O
that	O
the	O
true	O
distribution	B
is	O
so	O
complex	B
you	O
can	O
’	O
t	O
compute	O
it	O
directly	O
?	O
let	O
’	O
s	O
answer	O
this	O
question	O
in	O
the	O
context	O
of	O
bayesian	O
data	B
modelling	I
.	O
let	O
the	O
‘	O
true	O
’	O
distribution	B
of	O
interest	O
be	O
the	O
posterior	B
probability	I
distribution	O
over	O
a	O
set	B
of	O
parameters	B
x	O
,	O
p	O
(	O
xj	O
d	O
)	O
.	O
a	O
standard	O
data	O
modelling	B
practice	O
is	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
single	O
,	O
‘	O
best-	O
(	O
cid:12	O
)	O
t	O
’	O
setting	O
of	O
the	O
parameters	O
,	O
x	O
(	O
cid:3	O
)	O
,	O
for	O
example	O
,	O
by	O
(	O
cid:12	O
)	O
nding	O
the	O
maximum	O
of	O
the	O
likelihood	B
function	O
p	O
(	O
d	O
j	O
x	O
)	O
,	O
or	O
of	O
the	O
posterior	O
distribution	B
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
432	O
33	O
|	O
variational	B
methods	I
one	O
interpretation	O
of	O
this	O
standard	O
practice	O
is	O
that	O
the	O
full	O
description	O
of	O
our	O
knowledge	O
about	O
x	O
,	O
p	O
(	O
xj	O
d	O
)	O
,	O
is	O
being	O
approximated	O
by	O
a	O
delta-function	O
,	O
a	O
probability	B
distribution	O
concentrated	O
on	O
x	O
(	O
cid:3	O
)	O
.	O
from	O
this	O
perspective	O
,	O
any	O
approximating	O
distribution	B
q	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
,	O
no	O
matter	O
how	O
crummy	O
it	O
is	O
,	O
has	O
to	O
be	O
an	O
improvement	O
on	O
the	O
spike	O
produced	O
by	O
the	O
standard	O
method	O
!	O
so	O
even	O
if	O
we	O
use	O
only	O
a	O
simple	O
gaussian	O
approximation	B
,	O
we	O
are	O
doing	O
well	O
.	O
we	O
now	O
study	O
an	O
application	O
of	O
the	O
variational	O
approach	O
to	O
a	O
realistic	O
example	O
{	O
data	O
clustering	O
.	O
33.7	O
k-means	O
clustering	B
and	O
the	O
expectation	B
{	O
maximization	O
algo-	O
rithm	O
as	O
a	O
variational	B
method	O
in	O
chapter	O
20	O
,	O
we	O
introduced	O
the	O
soft	B
k-means	O
clustering	B
algorithm	O
,	O
version	O
1.	O
in	O
chapter	O
22	O
,	O
we	O
introduced	O
versions	O
2	O
and	O
3	O
of	O
this	O
algorithm	B
,	O
and	O
motivated	O
the	O
algorithm	B
as	O
a	O
maximum	B
likelihood	I
algorithm	O
.	O
k-means	O
clustering	B
is	O
an	O
example	O
of	O
an	O
‘	O
expectation	B
{	O
maximization	O
’	O
(	O
em	O
)	O
algorithm	B
,	O
with	O
the	O
two	O
steps	O
,	O
which	O
we	O
called	O
‘	O
assignment	O
’	O
and	O
‘	O
update	O
’	O
,	O
being	O
known	O
as	O
the	O
‘	O
e-step	O
’	O
and	O
the	O
‘	O
m-step	O
’	O
respectively	O
.	O
we	O
now	O
give	O
a	O
more	O
general	O
view	O
of	O
k-means	O
clustering	B
,	O
due	O
to	O
neal	O
and	O
hinton	O
(	O
1998	O
)	O
,	O
in	O
which	O
the	O
algorithm	B
is	O
shown	O
to	O
optimize	O
a	O
variational	B
objective	O
function	B
.	O
neal	O
and	O
hinton	O
’	O
s	O
derivation	B
applies	O
to	O
any	O
em	O
algorithm	B
.	O
the	O
probability	O
of	O
everything	O
let	O
the	O
parameters	B
of	O
the	O
mixture	O
model	O
{	O
the	O
means	O
,	O
standard	O
deviations	O
,	O
and	O
weights	O
{	O
be	O
denoted	O
by	O
(	O
cid:18	O
)	O
.	O
for	O
each	O
data	O
point	O
,	O
there	O
is	O
a	O
missing	O
variable	O
(	O
also	O
known	O
as	O
a	O
latent	B
variable	I
)	O
,	O
the	O
class	O
label	O
kn	O
for	O
that	O
point	O
.	O
the	O
probability	O
of	O
everything	O
,	O
given	O
our	O
assumed	O
model	B
h	O
,	O
is	O
p	O
(	O
fx	O
(	O
n	O
)	O
;	O
kngn	O
n=1	O
;	O
(	O
cid:18	O
)	O
jh	O
)	O
=	O
p	O
(	O
(	O
cid:18	O
)	O
jh	O
)	O
n	O
yn=1hp	O
(	O
x	O
(	O
n	O
)	O
j	O
kn	O
;	O
(	O
cid:18	O
)	O
)	O
p	O
(	O
kn	O
j	O
(	O
cid:18	O
)	O
)	O
i	O
:	O
(	O
33.47	O
)	O
the	O
posterior	B
probability	I
of	O
everything	O
,	O
given	O
the	O
data	O
,	O
is	O
proportional	O
to	O
the	O
probability	O
of	O
everything	O
:	O
p	O
(	O
fkngn	O
n=1	O
;	O
(	O
cid:18	O
)	O
jfx	O
(	O
n	O
)	O
gn	O
n=1	O
;	O
h	O
)	O
=	O
p	O
(	O
fx	O
(	O
n	O
)	O
;	O
kngn	O
p	O
(	O
fx	O
(	O
n	O
)	O
gn	O
n=1	O
;	O
(	O
cid:18	O
)	O
jh	O
)	O
n=1	O
jh	O
)	O
:	O
(	O
33.48	O
)	O
we	O
now	O
approximate	O
this	O
posterior	O
distribution	O
by	O
a	O
separable	O
distribution	B
and	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
variational	B
free	I
energy	I
in	O
the	O
usual	O
way	O
:	O
qk	O
(	O
fkngn	O
n=1	O
)	O
q	O
(	O
(	O
cid:18	O
)	O
)	O
;	O
(	O
33.49	O
)	O
n=1	O
)	O
q	O
(	O
(	O
cid:18	O
)	O
)	O
ln	O
n=1	O
)	O
q	O
qk	O
(	O
fkngn	O
p	O
(	O
fx	O
(	O
n	O
)	O
;	O
kngn	O
(	O
(	O
cid:18	O
)	O
)	O
n=1	O
;	O
(	O
cid:18	O
)	O
jh	O
)	O
:	O
~f	O
(	O
qk	O
;	O
q	O
)	O
=	O
xfkngz	O
dd	O
(	O
cid:18	O
)	O
qk	O
(	O
fkngn	O
(	O
33.50	O
)	O
~f	O
is	O
bounded	O
below	O
by	O
minus	O
the	O
evidence	B
,	O
ln	O
p	O
(	O
fx	O
(	O
n	O
)	O
gn	O
n=1	O
jh	O
)	O
.	O
we	O
can	O
now	O
make	O
an	O
iterative	O
algorithm	O
with	O
an	O
‘	O
assignment	O
’	O
step	O
and	O
an	O
‘	O
update	O
’	O
step	O
.	O
in	O
the	O
assignment	O
step	O
,	O
qk	O
(	O
fkngn	O
;	O
in	O
the	O
update	O
step	O
,	O
q	O
n=1	O
)	O
is	O
adjusted	O
to	O
reduce	O
~f	O
,	O
for	O
(	O
cid:12	O
)	O
xed	O
q	O
is	O
adjusted	O
to	O
reduce	O
~f	O
,	O
for	O
(	O
cid:12	O
)	O
xed	O
qk	O
.	O
if	O
we	O
wish	O
to	O
obtain	O
exactly	O
the	O
soft	B
k-means	O
algorithm	B
,	O
we	O
impose	O
a	O
is	O
constrained	B
to	O
be	O
further	O
constraint	O
on	O
our	O
approximating	O
distribution	B
:	O
q	O
a	O
delta	B
function	I
centred	O
on	O
a	O
point	B
estimate	I
of	O
(	O
cid:18	O
)	O
,	O
(	O
cid:18	O
)	O
=	O
(	O
cid:18	O
)	O
(	O
cid:3	O
)	O
:	O
q	O
(	O
(	O
cid:18	O
)	O
)	O
=	O
(	O
cid:14	O
)	O
(	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:3	O
)	O
)	O
:	O
(	O
33.51	O
)	O
 	B
 	I
 	I
 	I
 	I
 	I
 	I
 	I
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
33.8	O
:	O
variational	B
methods	I
other	O
than	O
free	B
energy	I
minimization	O
433	O
2	O
1	O
0	O
-5	O
0	O
5	O
upper	O
bound	B
lower	O
bound	B
1	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
a	O
(	O
cid:20	O
)	O
exp	O
(	O
(	O
cid:22	O
)	O
a	O
(	O
cid:0	O
)	O
h	O
e	O
2	O
(	O
(	O
cid:22	O
)	O
)	O
)	O
(	O
cid:22	O
)	O
2	O
[	O
0	O
;	O
1	O
]	O
1	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
a	O
(	O
cid:21	O
)	O
g	O
(	O
(	O
cid:23	O
)	O
)	O
exp	O
(	O
cid:2	O
)	O
(	O
a	O
(	O
cid:0	O
)	O
(	O
cid:23	O
)	O
)	O
=2	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
(	O
(	O
cid:23	O
)	O
)	O
(	O
a2	O
(	O
cid:0	O
)	O
(	O
cid:23	O
)	O
2	O
)	O
(	O
cid:3	O
)	O
where	O
(	O
cid:21	O
)	O
(	O
(	O
cid:23	O
)	O
)	O
=	O
[	O
g	O
(	O
(	O
cid:23	O
)	O
)	O
(	O
cid:0	O
)	O
1=2	O
]	O
=2	O
(	O
cid:23	O
)	O
.	O
figure	O
33.5.	O
illustration	O
of	O
the	O
jaakkola	O
{	O
jordan	O
variational	B
method	O
.	O
upper	O
and	O
lower	O
bounds	O
on	O
the	O
logistic	O
function	B
(	O
solid	O
line	O
)	O
g	O
(	O
a	O
)	O
(	O
cid:17	O
)	O
1	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
a	O
:	O
these	O
upper	O
and	O
lower	O
bounds	O
are	O
exponential	B
or	O
gaussian	O
functions	B
of	O
a	O
,	O
and	O
so	O
easier	O
to	O
integrate	O
over	O
.	O
the	O
graph	B
shows	O
the	O
sigmoid	B
function	O
and	O
upper	O
and	O
lower	O
bounds	O
with	O
(	O
cid:22	O
)	O
=	O
0:505	O
and	O
(	O
cid:23	O
)	O
=	O
(	O
cid:0	O
)	O
2:015.	O
in	O
(	O
cid:12	O
)	O
nitely	O
large	O
integral	B
r	O
dd	O
(	O
cid:18	O
)	O
q	O
unfortunately	O
,	O
this	O
distribution	B
contributes	O
to	O
the	O
variational	B
free	I
energy	I
an	O
(	O
(	O
cid:18	O
)	O
)	O
,	O
so	O
we	O
’	O
d	O
better	O
leave	O
that	O
term	O
out	O
of	O
~f	O
,	O
treating	O
it	O
as	O
an	O
additive	O
constant	O
.	O
[	O
using	O
a	O
delta	B
function	I
q	O
is	O
not	O
a	O
good	B
idea	O
if	O
our	O
aim	O
is	O
to	O
minimize	O
~f	O
!	O
]	O
moving	O
on	O
,	O
our	O
aim	O
is	O
to	O
derive	O
the	O
soft	B
k-means	O
algorithm	B
.	O
(	O
(	O
cid:18	O
)	O
)	O
ln	O
q	O
.	O
exercise	O
33.2	O
.	O
[	O
2	O
]	O
show	O
that	O
,	O
given	O
q	O
(	O
(	O
cid:18	O
)	O
)	O
=	O
(	O
cid:14	O
)	O
(	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:3	O
)	O
)	O
,	O
the	O
optimal	B
qk	O
,	O
in	O
the	O
sense	O
of	O
minimizing	O
~f	O
,	O
is	O
a	O
separable	O
distribution	B
in	O
which	O
the	O
probabil-	O
ity	O
that	O
kn	O
=	O
k	O
is	O
given	O
by	O
the	O
responsibility	B
r	O
(	O
n	O
)	O
k	O
.	O
.	O
exercise	O
33.3	O
.	O
[	O
3	O
]	O
show	O
that	O
,	O
given	O
a	O
separable	O
qk	O
as	O
described	O
above	O
,	O
the	O
op-	O
timal	O
(	O
cid:18	O
)	O
(	O
cid:3	O
)	O
,	O
in	O
the	O
sense	O
of	O
minimizing	O
~f	O
,	O
is	O
obtained	O
by	O
the	O
update	O
step	O
of	O
the	O
soft	O
k-means	O
algorithm	B
.	O
(	O
assume	O
a	O
uniform	O
prior	B
on	O
(	O
cid:18	O
)	O
.	O
)	O
exercise	O
33.4	O
.	O
[	O
4	O
]	O
we	O
can	O
instantly	O
improve	O
on	O
the	O
in	O
(	O
cid:12	O
)	O
nitely	O
large	O
value	O
of	O
~f	O
achieved	O
by	O
soft	O
k-means	O
clustering	B
by	O
allowing	O
q	O
to	O
be	O
a	O
more	O
general	O
distribution	O
than	O
a	O
delta-function	O
.	O
derive	O
an	O
update	O
step	O
in	O
which	O
q	O
is	O
allowed	O
to	O
be	O
a	O
separable	O
distribution	B
,	O
a	O
product	O
of	O
q	O
(	O
cid:22	O
)	O
(	O
f	O
(	O
cid:22	O
)	O
g	O
)	O
,	O
q	O
(	O
cid:27	O
)	O
(	O
f	O
(	O
cid:27	O
)	O
g	O
)	O
,	O
and	O
q	O
(	O
cid:25	O
)	O
(	O
(	O
cid:25	O
)	O
)	O
.	O
discuss	O
whether	O
this	O
generalized	B
algorithm	O
still	O
su	O
(	O
cid:11	O
)	O
ers	O
from	O
soft	O
k-means	O
’	O
s	O
‘	O
kaboom	B
’	O
problem	O
,	O
where	O
the	O
algorithm	B
glues	O
an	O
ever-	O
shrinking	O
gaussian	O
to	O
one	O
data	O
point	O
.	O
sadly	O
,	O
while	O
it	O
sounds	O
like	O
a	O
promising	O
generalization	B
of	O
the	O
algorithm	B
to	O
be	O
a	O
non-delta-function	O
,	O
and	O
the	O
‘	O
kaboom	B
’	O
problem	O
goes	O
to	O
allow	O
q	O
away	O
,	O
other	O
artefacts	O
can	O
arise	O
in	O
this	O
approximate	O
inference	B
method	O
,	O
involving	O
local	O
minima	O
of	O
~f	O
.	O
for	O
further	O
reading	O
,	O
see	O
(	O
mackay	O
,	O
1997a	O
;	O
mackay	O
,	O
2001	O
)	O
.	O
33.8	O
variational	B
methods	I
other	O
than	O
free	B
energy	I
minimization	O
there	O
are	O
other	O
strategies	O
for	O
approximating	O
a	O
complicated	O
distribution	B
p	O
(	O
x	O
)	O
,	O
in	O
addition	O
to	O
those	O
based	O
on	O
minimizing	O
the	O
relative	B
entropy	I
between	O
an	O
approximating	O
distribution	B
,	O
q	O
,	O
and	O
p	O
.	O
one	O
approach	O
pioneered	O
by	O
jaakkola	O
and	O
jordan	O
is	O
to	O
create	O
adjustable	O
upper	O
and	O
lower	O
bounds	O
qu	O
and	O
ql	O
to	O
p	O
,	O
as	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gure	O
33.5.	O
these	O
bounds	O
(	O
which	O
are	O
unnormalized	O
densities	O
)	O
are	O
parameterized	O
by	O
variational	O
parameters	B
which	O
are	O
adjusted	O
in	O
order	O
to	O
obtain	O
the	O
tightest	O
possible	O
(	O
cid:12	O
)	O
t.	O
the	O
lower	O
bound	B
can	O
be	O
adjusted	O
to	O
maximize	O
ql	O
(	O
x	O
)	O
;	O
xx	O
and	O
the	O
upper	O
bound	B
can	O
be	O
adjusted	O
to	O
minimize	O
qu	O
(	O
x	O
)	O
:	O
xx	O
(	O
33.52	O
)	O
(	O
33.53	O
)	O
 	B
 	I
 	I
 	I
 	I
 	I
 	I
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
434	O
33	O
|	O
variational	B
methods	I
using	O
the	O
normalized	O
versions	O
of	O
the	O
optimized	O
bounds	O
we	O
then	O
compute	O
ap-	O
proximations	O
to	O
the	O
predictive	O
distributions	O
.	O
further	O
reading	O
on	O
such	O
methods	B
can	O
be	O
found	O
in	O
the	O
references	O
(	O
jaakkola	O
and	O
jordan	O
,	O
2000a	O
;	O
jaakkola	O
and	O
jor-	O
dan	O
,	O
2000b	O
;	O
jaakkola	O
and	O
jordan	O
,	O
1996	O
;	O
gibbs	O
and	O
mackay	O
,	O
2000	O
)	O
.	O
further	O
reading	O
the	O
bethe	O
and	O
kikuchi	O
free	O
energies	O
in	O
chapter	O
26	O
we	O
discussed	O
the	O
sum	O
{	O
product	O
algorithm	O
for	O
functions	O
of	O
the	O
factor-graph	O
form	O
(	O
26.1	O
)	O
.	O
if	O
the	O
factor	B
graph	I
is	O
tree-like	O
,	O
the	O
sum	O
{	O
product	O
algo-	O
rithm	O
converges	O
and	O
correctly	O
computes	O
the	O
marginal	B
function	O
of	O
any	O
variable	O
xn	O
and	O
can	O
also	O
yield	O
the	O
joint	B
marginal	O
function	B
of	O
subsets	O
of	O
variables	O
that	O
appear	O
in	O
a	O
common	O
factor	O
,	O
such	O
as	O
xm	O
.	O
the	O
sum	O
{	O
product	O
algorithm	O
may	O
also	O
be	O
applied	O
to	O
factor	O
graphs	O
that	O
are	O
not	O
tree-like	O
.	O
if	O
the	O
algorithm	B
converges	O
to	O
a	O
(	O
cid:12	O
)	O
xed	O
point	O
,	O
it	O
has	O
been	O
shown	O
that	O
that	O
(	O
cid:12	O
)	O
xed	O
point	O
is	O
a	O
stationary	O
point	O
(	O
usually	O
a	O
minimum	O
)	O
of	O
a	O
function	B
of	O
the	O
messages	O
called	O
the	O
kikuchi	O
free	B
energy	I
.	O
in	O
the	O
special	O
case	O
where	O
all	O
factors	O
in	O
factor	O
graph	B
are	O
functions	B
of	O
one	O
or	O
two	O
variables	O
,	O
the	O
kikuchi	O
free	B
energy	I
is	O
called	O
the	O
bethe	O
free	B
energy	I
.	O
for	O
articles	O
on	O
this	O
idea	O
,	O
and	O
new	O
approximate	O
inference	B
algorithms	O
mo-	O
tivated	O
by	O
it	O
,	O
see	O
yedidia	O
(	O
2000	O
)	O
;	O
yedidia	O
et	O
al	O
.	O
(	O
2000	O
)	O
;	O
welling	O
and	O
teh	O
(	O
2001	O
)	O
;	O
yuille	O
(	O
2001	O
)	O
;	O
yedidia	O
et	O
al	O
.	O
(	O
2001b	O
)	O
;	O
yedidia	O
et	O
al	O
.	O
(	O
2001a	O
)	O
.	O
33.9	O
further	O
exercises	O
exercise	O
33.5	O
.	O
[	O
2	O
,	O
p.435	O
]	O
this	O
exercise	O
explores	O
the	O
assertion	O
,	O
made	O
above	O
,	O
that	O
the	O
approximations	O
given	O
by	O
variational	O
free	B
energy	I
minimization	O
al-	O
ways	O
tend	O
to	O
be	O
more	O
compact	O
than	O
the	O
true	O
distribution	B
.	O
consider	O
a	O
two	O
dimensional	O
gaussian	O
distribution	B
p	O
(	O
x	O
)	O
with	O
axes	O
aligned	O
with	O
the	O
directions	O
e	O
(	O
1	O
)	O
=	O
(	O
1	O
;	O
1	O
)	O
and	O
e	O
(	O
2	O
)	O
=	O
(	O
1	O
;	O
(	O
cid:0	O
)	O
1	O
)	O
.	O
let	O
the	O
variances	O
in	O
these	O
two	O
directions	O
be	O
(	O
cid:27	O
)	O
2	O
2.	O
what	O
is	O
the	O
optimal	B
variance	O
if	O
this	O
distribution	B
is	O
approximated	O
by	O
a	O
spherical	O
gaussian	O
with	O
variance	O
(	O
cid:27	O
)	O
2	O
q	O
,	O
optimized	O
by	O
variational	O
free	B
energy	I
minimization	O
?	O
if	O
we	O
instead	O
optimized	O
the	O
objec-	O
tive	O
function	B
1	O
and	O
(	O
cid:27	O
)	O
2	O
p	O
(	O
x	O
)	O
q	O
(	O
x	O
;	O
(	O
cid:27	O
)	O
2	O
)	O
g	O
=z	O
dx	O
p	O
(	O
x	O
)	O
ln	O
;	O
(	O
33.54	O
)	O
what	O
would	O
be	O
the	O
optimal	B
value	O
of	O
(	O
cid:27	O
)	O
2	O
?	O
sketch	O
a	O
contour	O
of	O
the	O
true	O
distribution	B
p	O
(	O
x	O
)	O
and	O
the	O
two	O
approximating	O
distributions	O
in	O
the	O
case	O
(	O
cid:27	O
)	O
1=	O
(	O
cid:27	O
)	O
2	O
=	O
10	O
.	O
[	O
note	O
that	O
in	O
general	O
it	O
is	O
not	O
possible	O
to	O
evaluate	O
the	O
objective	O
func-	O
tion	O
g	O
,	O
because	O
integrals	O
under	O
the	O
true	O
distribution	B
p	O
(	O
x	O
)	O
are	O
usually	O
intractable	O
.	O
]	O
exercise	O
33.6	O
.	O
[	O
2	O
,	O
p.436	O
]	O
what	O
do	O
you	O
think	O
of	O
the	O
idea	O
of	O
using	O
a	O
variational	B
method	O
to	O
optimize	O
an	O
approximating	O
distribution	B
q	O
which	O
we	O
then	O
use	O
as	O
a	O
proposal	B
density	I
for	O
importance	B
sampling	I
?	O
exercise	O
33.7	O
.	O
[	O
2	O
]	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
relative	B
entropy	I
or	O
kullback	O
{	O
leibler	O
divergence	B
be-	O
tween	O
two	O
probability	B
distributions	I
p	O
and	O
q	O
,	O
and	O
state	O
gibbs	O
’	O
inequality	B
.	O
consider	O
the	O
problem	O
of	O
approximating	O
a	O
joint	B
distribution	O
p	O
(	O
x	O
;	O
y	O
)	O
by	O
a	O
separable	O
distribution	B
q	O
(	O
x	O
;	O
y	O
)	O
=	O
qx	O
(	O
x	O
)	O
qy	O
(	O
y	O
)	O
.	O
show	O
that	O
if	O
the	O
objec-	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
33.10	O
:	O
solutions	O
435	O
tive	O
function	B
for	O
this	O
approximation	B
is	O
g	O
(	O
qx	O
;	O
qy	O
)	O
=xx	O
;	O
y	O
p	O
(	O
x	O
;	O
y	O
)	O
log2	O
p	O
(	O
x	O
;	O
y	O
)	O
qx	O
(	O
x	O
)	O
qy	O
(	O
y	O
)	O
that	O
the	O
minimal	O
value	O
of	O
g	O
is	O
achieved	O
when	O
qx	O
and	O
qy	O
are	O
equal	O
to	O
the	O
marginal	B
distributions	O
over	O
x	O
and	O
y.	O
now	O
consider	O
the	O
alternative	O
objective	B
function	I
f	O
(	O
qx	O
;	O
qy	O
)	O
=xx	O
;	O
y	O
qx	O
(	O
x	O
)	O
qy	O
(	O
y	O
)	O
log2	O
qx	O
(	O
x	O
)	O
qy	O
(	O
y	O
)	O
p	O
(	O
x	O
;	O
y	O
)	O
;	O
the	O
probability	B
distribution	O
p	O
(	O
x	O
;	O
y	O
)	O
shown	O
in	O
the	O
margin	O
is	O
to	O
be	O
ap-	O
proximated	O
by	O
a	O
separable	O
distribution	B
q	O
(	O
x	O
;	O
y	O
)	O
=	O
qx	O
(	O
x	O
)	O
qy	O
(	O
y	O
)	O
.	O
state	O
the	O
value	O
of	O
f	O
(	O
qx	O
;	O
qy	O
)	O
if	O
qx	O
and	O
qy	O
are	O
set	B
to	O
the	O
marginal	B
distribu-	O
tions	O
over	O
x	O
and	O
y.	O
show	O
that	O
f	O
(	O
qx	O
;	O
qy	O
)	O
has	O
three	O
distinct	O
minima	O
,	O
identify	O
those	O
minima	O
,	O
and	O
evaluate	O
f	O
at	O
each	O
of	O
them	O
.	O
33.10	O
solutions	O
solution	O
to	O
exercise	O
33.5	O
(	O
p.434	O
)	O
.	O
we	O
need	O
to	O
know	O
the	O
relative	B
entropy	I
be-	O
tween	O
two	O
one-dimensional	O
gaussian	O
distributions	O
:	O
p	O
(	O
x	O
;	O
y	O
)	O
x	O
1	O
1/8	O
1/8	O
0	O
0	O
2	O
1/8	O
1/8	O
0	O
0	O
3	O
4	O
0	O
0	O
1/4	O
0	O
0	O
0	O
0	O
1/4	O
y	O
1	O
2	O
3	O
4	O
z	O
dx	O
normal	B
(	O
x	O
;	O
0	O
;	O
(	O
cid:27	O
)	O
q	O
)	O
ln	O
=	O
z	O
dx	O
normal	B
(	O
x	O
;	O
0	O
;	O
(	O
cid:27	O
)	O
q	O
)	O
''	O
ln	O
p	O
!	O
:	O
2	O
ln	O
(	O
cid:27	O
)	O
2	O
p	O
q	O
(	O
cid:0	O
)	O
1	O
+	O
(	O
cid:27	O
)	O
2	O
(	O
cid:27	O
)	O
2	O
q	O
(	O
cid:27	O
)	O
2	O
=	O
1	O
normal	B
(	O
x	O
;	O
0	O
;	O
(	O
cid:27	O
)	O
q	O
)	O
normal	B
(	O
x	O
;	O
0	O
;	O
(	O
cid:27	O
)	O
p	O
)	O
(	O
cid:27	O
)	O
p	O
(	O
cid:27	O
)	O
q	O
(	O
cid:0	O
)	O
1	O
2	O
x2	O
1	O
q	O
(	O
cid:0	O
)	O
(	O
cid:27	O
)	O
2	O
p	O
!	O
#	O
1	O
(	O
cid:27	O
)	O
2	O
(	O
33.55	O
)	O
(	O
33.56	O
)	O
so	O
,	O
if	O
we	O
approximate	O
p	O
,	O
whose	O
variances	O
are	O
(	O
cid:27	O
)	O
2	O
are	O
both	O
(	O
cid:27	O
)	O
2	O
q	O
,	O
we	O
(	O
cid:12	O
)	O
nd	O
1	O
and	O
(	O
cid:27	O
)	O
2	O
2	O
,	O
by	O
q	O
,	O
whose	O
variances	O
f	O
(	O
(	O
cid:27	O
)	O
2	O
q	O
)	O
=	O
1	O
2	O
ln	O
(	O
cid:27	O
)	O
2	O
1	O
q	O
(	O
cid:0	O
)	O
1	O
+	O
(	O
cid:27	O
)	O
2	O
(	O
cid:27	O
)	O
2	O
q	O
(	O
cid:27	O
)	O
2	O
1	O
+	O
ln	O
(	O
cid:27	O
)	O
2	O
2	O
q	O
(	O
cid:0	O
)	O
1	O
+	O
(	O
cid:27	O
)	O
2	O
(	O
cid:27	O
)	O
2	O
q	O
(	O
cid:27	O
)	O
2	O
2	O
!	O
;	O
di	O
(	O
cid:11	O
)	O
erentiating	O
,	O
which	O
is	O
zero	O
when	O
(	O
cid:27	O
)	O
2	O
q	O
(	O
cid:27	O
)	O
2	O
2	O
!	O
#	O
;	O
d	O
d	O
ln	O
(	O
(	O
cid:27	O
)	O
2	O
q	O
)	O
f	O
=	O
+	O
1	O
q	O
(	O
cid:27	O
)	O
2	O
1	O
2	O
''	O
(	O
cid:0	O
)	O
2	O
+	O
(	O
cid:27	O
)	O
2	O
2	O
(	O
cid:18	O
)	O
1	O
2	O
(	O
cid:19	O
)	O
:	O
1	O
(	O
cid:27	O
)	O
2	O
(	O
cid:27	O
)	O
2	O
1	O
+	O
1	O
1	O
(	O
cid:27	O
)	O
2	O
q	O
=	O
(	O
33.57	O
)	O
(	O
33.58	O
)	O
(	O
33.59	O
)	O
thus	O
we	O
set	B
the	O
approximating	O
distribution	B
’	O
s	O
inverse	O
variance	O
to	O
the	O
mean	B
inverse	O
variance	B
of	O
the	O
target	O
distribution	B
p	O
.	O
in	O
the	O
case	O
(	O
cid:27	O
)	O
1	O
=	O
10	O
and	O
(	O
cid:27	O
)	O
2	O
=	O
1	O
,	O
we	O
obtain	O
(	O
cid:27	O
)	O
q	O
’	O
p2	O
,	O
which	O
is	O
just	O
a	O
factor	O
of	O
p2	O
larger	O
than	O
(	O
cid:27	O
)	O
2	O
,	O
pretty	O
much	O
independent	O
of	O
the	O
value	O
of	O
the	O
larger	O
standard	B
deviation	I
(	O
cid:27	O
)	O
1.	O
variational	B
free	I
energy	I
minimization	O
typically	O
leads	O
to	O
approximating	O
distributions	O
whose	O
length	B
scales	O
match	O
the	O
shortest	O
length	B
scale	O
of	O
the	O
target	O
distribution	B
.	O
the	O
approximating	O
distribution	B
might	O
be	O
viewed	O
as	O
too	O
compact	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
436	O
(	O
a	O
)	O
(	O
b	O
)	O
in	O
contrast	O
,	O
if	O
we	O
use	O
the	O
objective	B
function	I
g	O
then	O
we	O
(	O
cid:12	O
)	O
nd	O
:	O
g	O
(	O
(	O
cid:27	O
)	O
2	O
q	O
)	O
=	O
1	O
2	O
ln	O
(	O
cid:27	O
)	O
2	O
q	O
+	O
(	O
cid:27	O
)	O
2	O
1	O
(	O
cid:27	O
)	O
2	O
q	O
+	O
ln	O
(	O
cid:27	O
)	O
2	O
q	O
+	O
(	O
cid:27	O
)	O
2	O
2	O
(	O
cid:27	O
)	O
2	O
q	O
!	O
+	O
constant	O
;	O
where	O
the	O
constant	O
depends	O
on	O
(	O
cid:27	O
)	O
1	O
and	O
(	O
cid:27	O
)	O
2	O
only	O
.	O
di	O
(	O
cid:11	O
)	O
erentiating	O
,	O
d	O
d	O
ln	O
(	O
cid:27	O
)	O
2	O
q	O
g	O
=	O
1	O
2	O
''	O
2	O
(	O
cid:0	O
)	O
(	O
cid:27	O
)	O
2	O
1	O
(	O
cid:27	O
)	O
2	O
q	O
+	O
(	O
cid:27	O
)	O
2	O
2	O
(	O
cid:27	O
)	O
2	O
q	O
!	O
#	O
;	O
33	O
|	O
variational	B
methods	I
figure	O
33.6.	O
two	O
separable	O
gaussian	O
approximations	O
(	O
dotted	O
lines	O
)	O
to	O
a	O
bivariate	O
gaussian	O
distribution	B
(	O
solid	O
line	O
)	O
.	O
(	O
a	O
)	O
the	O
approximation	B
that	O
minimizes	O
the	O
variational	B
free	I
energy	I
.	O
(	O
b	O
)	O
the	O
approximation	B
that	O
minimizes	O
the	O
objective	B
function	I
g.	O
in	O
each	O
(	O
cid:12	O
)	O
gure	O
,	O
the	O
lines	O
show	O
the	O
contours	O
at	O
which	O
xtax	O
=	O
1	O
,	O
where	O
a	O
is	O
the	O
inverse	O
covariance	O
matrix	B
of	O
the	O
gaussian	O
.	O
(	O
33.60	O
)	O
(	O
33.61	O
)	O
(	O
33.62	O
)	O
which	O
is	O
zero	O
when	O
(	O
cid:27	O
)	O
2	O
q	O
=	O
1	O
2	O
(	O
cid:0	O
)	O
(	O
cid:27	O
)	O
2	O
1	O
+	O
(	O
cid:27	O
)	O
2	O
2	O
(	O
cid:1	O
)	O
:	O
thus	O
we	O
set	B
the	O
approximating	O
distribution	B
’	O
s	O
variance	B
to	O
the	O
mean	B
variance	O
of	O
the	O
target	O
distribution	B
p	O
.	O
factor	O
of	O
p2	O
smaller	O
than	O
(	O
cid:27	O
)	O
1	O
,	O
independent	O
of	O
the	O
value	O
of	O
(	O
cid:27	O
)	O
2.	O
the	O
two	O
approximations	O
are	O
shown	O
to	O
scale	O
in	O
(	O
cid:12	O
)	O
gure	O
33.6.	O
in	O
the	O
case	O
(	O
cid:27	O
)	O
1	O
=	O
10	O
and	O
(	O
cid:27	O
)	O
2	O
=	O
1	O
,	O
we	O
obtain	O
(	O
cid:27	O
)	O
q	O
’	O
10=p2	O
,	O
which	O
is	O
just	O
a	O
solution	O
to	O
exercise	O
33.6	O
(	O
p.434	O
)	O
.	O
the	O
best	O
possible	O
variational	B
approximation	O
is	O
of	O
course	O
the	O
target	O
distribution	B
p	O
.	O
assuming	O
that	O
this	O
is	O
not	O
possible	O
,	O
a	O
good	B
variational	O
approximation	B
is	O
more	O
compact	O
than	O
the	O
true	O
distribution	B
.	O
in	O
contrast	O
,	O
a	O
good	B
sampler	O
is	O
more	O
heavy	O
tailed	O
than	O
the	O
true	O
distribution	B
.	O
an	O
over-compact	O
distribution	B
would	O
be	O
a	O
lousy	O
sampler	O
with	O
a	O
large	O
variance	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
s1	O
sk	O
g	O
y1	O
yn	O
figure	O
34.1.	O
error-correcting	B
codes	I
as	O
latent	B
variable	I
models	I
.	O
the	O
k	O
latent	O
variables	O
are	O
the	O
independent	O
source	O
bits	O
s1	O
;	O
:	O
:	O
:	O
;	O
sk	O
;	O
these	O
give	O
rise	O
to	O
the	O
observables	O
via	O
the	O
generator	B
matrix	I
g.	O
34	O
independent	B
component	I
analysis	I
and	O
latent	B
variable	I
modelling	O
34.1	O
latent	B
variable	I
models	I
many	O
statistical	B
models	O
are	O
generative	O
models	O
(	O
that	O
is	O
,	O
models	O
that	O
specify	O
a	O
full	O
probability	O
density	B
over	O
all	O
variables	O
in	O
the	O
situation	O
)	O
that	O
make	O
use	O
of	O
latent	O
variables	O
to	O
describe	O
a	O
probability	B
distribution	O
over	O
observables	O
.	O
examples	O
of	O
latent	O
variable	O
models	O
include	O
chapter	O
22	O
’	O
s	O
mixture	O
models	O
,	O
which	O
model	B
the	O
observables	O
as	O
coming	O
from	O
a	O
superposed	O
mixture	O
of	O
simple	O
probability	B
distributions	I
(	O
the	O
latent	O
variables	O
are	O
the	O
unknown	O
class	O
labels	O
of	O
the	O
examples	O
)	O
;	O
hidden	O
markov	O
models	O
(	O
rabiner	O
and	O
juang	O
,	O
1986	O
;	O
durbin	O
et	O
al.	O
,	O
1998	O
)	O
;	O
and	O
factor	O
analysis	B
.	O
the	O
decoding	B
problem	O
for	O
error-correcting	O
codes	O
can	O
also	O
be	O
viewed	O
in	O
in	O
that	O
case	O
,	O
the	O
encoding	O
terms	O
of	O
a	O
latent	B
variable	I
model	I
{	O
(	O
cid:12	O
)	O
gure	O
34.1.	O
matrix	B
g	O
is	O
normally	O
known	O
in	O
advance	O
.	O
in	O
latent	O
variable	O
modelling	O
,	O
the	O
parameters	B
equivalent	O
to	O
g	O
are	O
usually	O
not	O
known	O
,	O
and	O
must	O
be	O
inferred	O
from	O
the	O
data	O
along	O
with	O
the	O
latent	O
variables	O
s.	O
usually	O
,	O
the	O
latent	O
variables	O
have	O
a	O
simple	O
distribution	O
,	O
often	O
a	O
separable	O
distribution	B
.	O
thus	O
when	O
we	O
(	O
cid:12	O
)	O
t	O
a	O
latent	B
variable	I
model	I
,	O
we	O
are	O
(	O
cid:12	O
)	O
nding	O
a	O
de-	O
scription	O
of	O
the	O
data	O
in	O
terms	O
of	O
‘	O
independent	O
components	O
’	O
.	O
the	O
‘	O
independent	B
component	I
analysis	I
’	O
algorithm	B
corresponds	O
to	O
perhaps	O
the	O
simplest	O
possible	O
latent	B
variable	I
model	I
with	O
continuous	B
latent	O
variables	O
.	O
34.2	O
the	O
generative	B
model	I
for	O
independent	B
component	I
analysis	I
a	O
set	B
of	O
n	O
observations	O
d	O
=	O
fx	O
(	O
n	O
)	O
gn	O
n=1	O
are	O
assumed	O
to	O
be	O
generated	O
as	O
follows	O
.	O
each	O
j-dimensional	O
vector	O
x	O
is	O
a	O
linear	B
mixture	O
of	O
i	O
underlying	O
source	O
signals	O
,	O
s	O
:	O
x	O
=	O
gs	O
;	O
(	O
34.1	O
)	O
where	O
the	O
matrix	B
of	O
mixing	O
coe	O
(	O
cid:14	O
)	O
cients	O
g	O
is	O
not	O
known	O
.	O
the	O
simplest	O
algorithm	B
results	O
if	O
we	O
assume	O
that	O
the	O
number	O
of	O
sources	O
is	O
equal	O
to	O
the	O
number	O
of	O
observations	O
,	O
i.e.	O
,	O
i	O
=	O
j.	O
our	O
aim	O
is	O
to	O
recover	O
the	O
source	O
variables	O
s	O
(	O
within	O
some	O
multiplicative	O
factors	O
,	O
and	O
possibly	O
per-	O
muted	O
)	O
.	O
to	O
put	O
it	O
another	O
way	O
,	O
we	O
aim	O
to	O
create	O
the	O
inverse	O
of	O
g	O
(	O
within	O
a	O
post-multiplicative	O
factor	O
)	O
given	O
only	O
a	O
set	B
of	O
examples	O
fxg	O
.	O
we	O
assume	O
that	O
the	O
latent	O
variables	O
are	O
independently	O
distributed	O
,	O
with	O
marginal	O
distributions	O
p	O
(	O
si	O
jh	O
)	O
(	O
cid:17	O
)	O
pi	O
(	O
si	O
)	O
.	O
here	O
h	O
denotes	O
the	O
assumed	O
form	O
of	O
this	O
model	B
and	O
the	O
assumed	O
probability	B
distributions	I
pi	O
of	O
the	O
latent	O
variables	O
.	O
the	O
probability	O
of	O
the	O
observables	O
and	O
the	O
hidden	O
variables	O
,	O
given	O
g	O
and	O
437	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
34	O
|	O
independent	B
component	I
analysis	I
and	O
latent	B
variable	I
modelling	O
438	O
h	O
,	O
is	O
:	O
p	O
(	O
fx	O
(	O
n	O
)	O
;	O
s	O
(	O
n	O
)	O
gn	O
n=1	O
j	O
g	O
;	O
h	O
)	O
=	O
n	O
yn=1hp	O
(	O
x	O
(	O
n	O
)	O
j	O
s	O
(	O
n	O
)	O
;	O
g	O
;	O
h	O
)	O
p	O
(	O
s	O
(	O
n	O
)	O
jh	O
)	O
i	O
)	O
!	O
3	O
5	O
:	O
i	O
(	O
cid:17	O
)	O
1	O
a	O
yi	O
j	O
(	O
cid:0	O
)	O
pi	O
gjis	O
(	O
n	O
)	O
pi	O
(	O
s	O
(	O
n	O
)	O
i	O
(	O
34.2	O
)	O
(	O
34.3	O
)	O
=	O
n	O
yn=1	O
2	O
4	O
0	O
@	O
yj	O
(	O
cid:14	O
)	O
(	O
cid:16	O
)	O
x	O
(	O
n	O
)	O
we	O
assume	O
that	O
the	O
vector	O
x	O
is	O
generated	O
without	O
noise	B
.	O
this	O
assumption	O
is	O
not	O
usually	O
made	O
in	O
latent	O
variable	O
modelling	O
,	O
since	O
noise-free	O
data	O
are	O
rare	O
;	O
but	O
it	O
makes	O
the	O
inference	B
problem	O
far	O
simpler	O
to	O
solve	O
.	O
the	O
likelihood	B
function	O
for	O
learning	O
about	O
g	O
from	O
the	O
data	O
d	O
,	O
the	O
relevant	O
quantity	O
is	O
the	O
likelihood	B
function	O
p	O
(	O
d	O
j	O
g	O
;	O
h	O
)	O
=yn	O
p	O
(	O
x	O
(	O
n	O
)	O
j	O
g	O
;	O
h	O
)	O
(	O
34.4	O
)	O
which	O
is	O
a	O
product	O
of	O
factors	O
each	O
of	O
which	O
is	O
obtained	O
by	O
marginalizing	O
over	O
the	O
latent	O
variables	O
.	O
when	O
we	O
marginalize	O
over	O
delta	O
functions	B
,	O
remember	O
v	O
f	O
(	O
x=v	O
)	O
:	O
we	O
adopt	O
summation	B
convention	I
at	O
this	O
.	O
a	O
single	O
factor	O
in	O
the	O
that	O
r	O
ds	O
(	O
cid:14	O
)	O
(	O
x	O
(	O
cid:0	O
)	O
vs	O
)	O
f	O
(	O
s	O
)	O
=	O
1	O
point	O
,	O
such	O
that	O
,	O
for	O
example	O
,	O
gjis	O
(	O
n	O
)	O
likelihood	B
is	O
given	O
by	O
i	O
(	O
cid:17	O
)	O
pi	O
gjis	O
(	O
n	O
)	O
i	O
p	O
(	O
x	O
(	O
n	O
)	O
j	O
g	O
;	O
h	O
)	O
=	O
z	O
dis	O
(	O
n	O
)	O
p	O
(	O
x	O
(	O
n	O
)	O
j	O
s	O
(	O
n	O
)	O
;	O
g	O
;	O
h	O
)	O
p	O
(	O
s	O
(	O
n	O
)	O
jh	O
)	O
=	O
z	O
dis	O
(	O
n	O
)	O
yj	O
jdet	O
gjyi	O
=	O
1	O
(	O
cid:14	O
)	O
(	O
cid:16	O
)	O
x	O
(	O
n	O
)	O
j	O
(	O
cid:0	O
)	O
gjis	O
(	O
n	O
)	O
i	O
(	O
cid:17	O
)	O
yi	O
pi	O
(	O
s	O
(	O
n	O
)	O
i	O
pi	O
(	O
g	O
(	O
cid:0	O
)	O
1	O
ij	O
xj	O
)	O
(	O
34.5	O
)	O
)	O
(	O
34.6	O
)	O
(	O
34.7	O
)	O
(	O
34.8	O
)	O
)	O
ln	O
p	O
(	O
x	O
(	O
n	O
)	O
j	O
g	O
;	O
h	O
)	O
=	O
(	O
cid:0	O
)	O
lnjdet	O
gj	O
+xi	O
ln	O
pi	O
(	O
g	O
(	O
cid:0	O
)	O
1	O
ij	O
xj	O
)	O
:	O
to	O
obtain	O
a	O
maximum	B
likelihood	I
algorithm	O
we	O
(	O
cid:12	O
)	O
nd	O
the	O
gradient	O
of	O
the	O
log	O
if	O
we	O
introduce	O
w	O
(	O
cid:17	O
)	O
g	O
(	O
cid:0	O
)	O
1	O
,	O
the	O
log	O
likelihood	B
contributed	O
by	O
a	O
likelihood	B
.	O
single	O
example	O
may	O
be	O
written	O
:	O
ln	O
p	O
(	O
x	O
(	O
n	O
)	O
j	O
g	O
;	O
h	O
)	O
=	O
lnjdet	O
wj	O
+xi	O
ln	O
pi	O
(	O
wijxj	O
)	O
:	O
(	O
34.9	O
)	O
we	O
’	O
ll	O
assume	O
from	O
now	O
on	O
that	O
det	O
w	O
is	O
positive	O
,	O
so	O
that	O
we	O
can	O
omit	O
the	O
absolute	B
value	I
sign	O
.	O
we	O
will	O
need	O
the	O
following	O
identities	O
:	O
@	O
@	O
gji	O
ln	O
det	O
g	O
=	O
g	O
(	O
cid:0	O
)	O
1	O
ij	O
=	O
wij	O
@	O
@	O
gji	O
@	O
lj	O
g	O
(	O
cid:0	O
)	O
1	O
lm	O
=	O
(	O
cid:0	O
)	O
g	O
(	O
cid:0	O
)	O
1	O
g	O
(	O
cid:0	O
)	O
1	O
f	O
=	O
(	O
cid:0	O
)	O
gjm	O
(	O
cid:18	O
)	O
@	O
im	O
=	O
(	O
cid:0	O
)	O
wljwim	O
f	O
(	O
cid:19	O
)	O
gli	O
:	O
@	O
glm	O
@	O
wij	O
let	O
us	O
de	O
(	O
cid:12	O
)	O
ne	O
ai	O
(	O
cid:17	O
)	O
wijxj	O
,	O
(	O
cid:30	O
)	O
i	O
(	O
ai	O
)	O
(	O
cid:17	O
)	O
d	O
ln	O
pi	O
(	O
ai	O
)	O
=dai	O
;	O
(	O
34.10	O
)	O
(	O
34.11	O
)	O
(	O
34.12	O
)	O
(	O
34.13	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
34.2	O
:	O
the	O
generative	B
model	I
for	O
independent	B
component	I
analysis	I
439	O
algorithm	B
34.2.	O
independent	B
component	I
analysis	I
{	O
online	O
steepest	O
ascents	O
version	O
.	O
see	O
also	O
algorithm	B
34.4	O
,	O
which	O
is	O
to	O
be	O
preferred	O
.	O
repeat	O
for	O
each	O
datapoint	O
x	O
:	O
1.	O
put	O
x	O
through	O
a	O
linear	B
mapping	O
:	O
a	O
=	O
wx	O
:	O
2.	O
put	O
a	O
through	O
a	O
nonlinear	B
map	O
:	O
zi	O
=	O
(	O
cid:30	O
)	O
i	O
(	O
ai	O
)	O
;	O
where	O
a	O
popular	O
choice	O
for	O
(	O
cid:30	O
)	O
is	O
(	O
cid:30	O
)	O
=	O
(	O
cid:0	O
)	O
tanh	O
(	O
ai	O
)	O
.	O
3.	O
adjust	O
the	O
weights	O
in	O
accordance	O
with	O
(	O
cid:1	O
)	O
w	O
/	O
[	O
wt	O
]	O
(	O
cid:0	O
)	O
1	O
+	O
zxt	O
:	O
and	O
zi	O
=	O
(	O
cid:30	O
)	O
i	O
(	O
ai	O
)	O
,	O
which	O
indicates	O
in	O
which	O
direction	O
ai	O
needs	O
to	O
change	O
to	O
make	O
the	O
probability	O
of	O
the	O
data	O
greater	O
.	O
we	O
may	O
then	O
obtain	O
the	O
gradient	O
with	O
respect	O
to	O
gji	O
using	O
equations	O
(	O
34.10	O
)	O
and	O
(	O
34.11	O
)	O
:	O
@	O
@	O
gji	O
ln	O
p	O
(	O
x	O
(	O
n	O
)	O
j	O
g	O
;	O
h	O
)	O
=	O
(	O
cid:0	O
)	O
wij	O
(	O
cid:0	O
)	O
aizi0wi0j	O
:	O
or	O
alternatively	O
,	O
the	O
derivative	O
with	O
respect	O
to	O
wij	O
:	O
@	O
@	O
wij	O
ln	O
p	O
(	O
x	O
(	O
n	O
)	O
j	O
g	O
;	O
h	O
)	O
=	O
gji	O
+	O
xjzi	O
:	O
(	O
34.14	O
)	O
(	O
34.15	O
)	O
if	O
we	O
choose	O
to	O
change	O
w	O
so	O
as	O
to	O
ascend	O
this	O
gradient	O
,	O
we	O
obtain	O
the	O
learning	B
rule	I
(	O
cid:1	O
)	O
w	O
/	O
[	O
wt	O
]	O
(	O
cid:0	O
)	O
1	O
+	O
zxt	O
:	O
(	O
34.16	O
)	O
the	O
algorithm	B
so	O
far	O
is	O
summarized	O
in	O
algorithm	O
34.2.	O
choices	O
of	O
(	O
cid:30	O
)	O
the	O
choice	O
of	O
the	O
function	O
(	O
cid:30	O
)	O
de	O
(	O
cid:12	O
)	O
nes	O
the	O
assumed	O
prior	B
distribution	O
of	O
the	O
latent	O
variable	O
s.	O
let	O
’	O
s	O
(	O
cid:12	O
)	O
rst	O
consider	O
the	O
linear	B
choice	O
(	O
cid:30	O
)	O
i	O
(	O
ai	O
)	O
=	O
(	O
cid:0	O
)	O
(	O
cid:20	O
)	O
ai	O
,	O
which	O
implicitly	O
(	O
via	O
equation	O
34.13	O
)	O
assumes	O
a	O
gaussian	O
distribution	B
on	O
the	O
latent	O
variables	O
.	O
the	O
gaussian	O
distribution	B
on	O
the	O
latent	O
variables	O
is	O
invariant	O
under	O
rotation	O
of	O
the	O
latent	O
variables	O
,	O
so	O
there	O
can	O
be	O
no	O
evidence	B
favouring	O
any	O
particular	O
alignment	O
of	O
the	O
latent	O
variable	O
space	O
.	O
the	O
linear	B
algorithm	O
is	O
thus	O
uninteresting	O
in	O
that	O
it	O
will	O
never	O
recover	O
the	O
matrix	B
g	O
or	O
the	O
original	O
sources	O
.	O
our	O
only	O
hope	O
is	O
thus	O
that	O
the	O
sources	O
are	O
non-gaussian	O
.	O
thankfully	O
,	O
most	O
real	O
sources	O
have	O
non-gaussian	O
distributions	O
;	O
often	O
they	O
have	O
heavier	O
tails	O
than	O
gaussians	O
.	O
we	O
thus	O
move	O
on	O
to	O
the	O
popular	O
tanh	O
nonlinearity	O
.	O
if	O
then	O
implicitly	O
we	O
are	O
assuming	O
(	O
cid:30	O
)	O
i	O
(	O
ai	O
)	O
=	O
(	O
cid:0	O
)	O
tanh	O
(	O
ai	O
)	O
pi	O
(	O
si	O
)	O
/	O
1=	O
cosh	O
(	O
si	O
)	O
/	O
1	O
esi	O
+	O
e	O
(	O
cid:0	O
)	O
si	O
:	O
(	O
34.17	O
)	O
(	O
34.18	O
)	O
this	O
is	O
a	O
heavier-tailed	O
distribution	B
for	O
the	O
latent	O
variables	O
than	O
the	O
gaussian	O
distribution	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
440	O
34	O
|	O
independent	B
component	I
analysis	I
and	O
latent	B
variable	I
modelling	O
figure	O
34.3.	O
illustration	O
of	O
the	O
generative	O
models	O
implicit	O
in	O
the	O
learning	B
algorithm	O
.	O
(	O
a	O
)	O
distributions	O
over	O
two	O
observables	O
generated	O
by	O
1=	O
cosh	O
distributions	O
on	O
the	O
latent	O
(	O
compact	O
distribution	B
)	O
and	O
1	O
(	O
cid:21	O
)	O
variables	O
,	O
for	O
g	O
=	O
(	O
cid:20	O
)	O
3=4	O
1=2	O
g	O
=	O
(	O
cid:20	O
)	O
2	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
1	O
3=2	O
(	O
cid:21	O
)	O
(	O
broader	O
1=2	O
distribution	B
)	O
.	O
(	O
b	O
)	O
contours	O
of	O
the	O
generative	O
distributions	O
when	O
the	O
latent	O
variables	O
have	O
cauchy	O
distributions	O
.	O
the	O
learning	B
algorithm	O
(	O
cid:12	O
)	O
ts	O
this	O
amoeboid	O
object	O
to	O
the	O
empirical	O
data	O
in	O
such	O
a	O
way	O
as	O
to	O
maximize	O
the	O
likelihood	B
.	O
the	O
contour	O
plot	O
in	O
(	O
b	O
)	O
does	O
not	O
adequately	O
represent	O
this	O
heavy-tailed	O
distribution	B
.	O
(	O
c	O
)	O
part	O
of	O
the	O
tails	O
of	O
the	O
cauchy	O
distribution	B
,	O
giving	O
the	O
contours	O
0:01	O
:	O
:	O
:	O
0:1	O
times	O
the	O
density	B
at	O
the	O
origin	O
.	O
(	O
d	O
)	O
some	O
data	O
from	O
one	O
of	O
the	O
generative	O
distributions	O
illustrated	O
in	O
(	O
b	O
)	O
and	O
(	O
c	O
)	O
.	O
can	O
you	O
tell	O
which	O
?	O
200	O
samples	O
were	O
created	O
,	O
of	O
which	O
196	O
fell	O
in	O
the	O
plotted	O
region	O
.	O
-4	O
-2	O
x1	O
0	O
0	O
2	O
4	O
-4	O
-2	O
x1	O
0	O
0	O
2	O
4	O
4	O
2	O
0	O
0	O
x2	O
-2	O
-4	O
4	O
2	O
0	O
0	O
x2	O
-2	O
-4	O
(	O
a	O
)	O
-8	O
-6	O
-4	O
-2	O
x1	O
0	O
0	O
2	O
4	O
6	O
8	O
8	O
6	O
4	O
2	O
0	O
0	O
-2	O
-4	O
-6	O
-8	O
(	O
b	O
)	O
30	O
20	O
10	O
x2	O
0	O
x2	O
0	O
-10	O
-20	O
(	O
c	O
)	O
(	O
d	O
)	O
-30	O
-30	O
-20	O
-10	O
0	O
0	O
x1	O
10	O
20	O
30	O
we	O
could	O
also	O
use	O
a	O
tanh	O
nonlinearity	O
with	O
gain	O
(	O
cid:12	O
)	O
,	O
that	O
is	O
,	O
(	O
cid:30	O
)	O
i	O
(	O
ai	O
)	O
=	O
(	O
cid:0	O
)	O
tanh	O
(	O
(	O
cid:12	O
)	O
ai	O
)	O
,	O
whose	O
implicit	O
probabilistic	O
model	B
is	O
pi	O
(	O
si	O
)	O
/	O
1=	O
[	O
cosh	O
(	O
(	O
cid:12	O
)	O
si	O
)	O
]	O
1=	O
(	O
cid:12	O
)	O
.	O
in	O
the	O
limit	O
of	O
large	O
(	O
cid:12	O
)	O
,	O
the	O
nonlinearity	O
becomes	O
a	O
step	O
function	B
and	O
the	O
probabil-	O
ity	O
distribution	B
pi	O
(	O
si	O
)	O
becomes	O
a	O
biexponential	B
distribution	O
,	O
pi	O
(	O
si	O
)	O
/	O
exp	O
(	O
(	O
cid:0	O
)	O
jsj	O
)	O
.	O
in	O
the	O
limit	O
(	O
cid:12	O
)	O
!	O
0	O
,	O
pi	O
(	O
si	O
)	O
approaches	O
a	O
gaussian	O
with	O
mean	O
zero	O
and	O
variance	O
1=	O
(	O
cid:12	O
)	O
.	O
heavier-tailed	O
distributions	O
than	O
these	O
may	O
also	O
be	O
used	O
.	O
the	O
student	B
and	O
cauchy	O
distributions	O
spring	B
to	O
mind	O
.	O
example	O
distributions	O
figures	O
34.3	O
(	O
a	O
{	O
c	O
)	O
illustrate	O
typical	B
distributions	O
generated	O
by	O
the	O
independent	O
components	O
model	B
when	O
the	O
components	O
have	O
1=	O
cosh	O
and	O
cauchy	O
distribu-	O
tions	O
.	O
figure	O
34.3d	O
shows	O
some	O
samples	O
from	O
the	O
cauchy	O
model	B
.	O
the	O
cauchy	O
distribution	B
,	O
being	O
the	O
more	O
heavy-tailed	O
,	O
gives	O
the	O
clearest	O
picture	O
of	O
how	O
the	O
predictive	B
distribution	I
depends	O
on	O
the	O
assumed	O
generative	O
parameters	O
g.	O
34.3	O
a	O
covariant	B
,	O
simpler	O
,	O
and	O
faster	O
learning	B
algorithm	O
we	O
have	O
thus	O
derived	O
a	O
learning	B
algorithm	O
that	O
performs	O
steepest	B
descents	I
on	O
the	O
likelihood	B
function	O
.	O
the	O
algorithm	B
does	O
not	O
work	O
very	O
quickly	O
,	O
even	O
on	O
toy	O
data	O
;	O
the	O
algorithm	B
is	O
ill-conditioned	O
and	O
illustrates	O
nicely	O
the	O
general	O
advice	O
that	O
,	O
while	O
(	O
cid:12	O
)	O
nding	O
the	O
gradient	O
of	O
an	O
objective	B
function	I
is	O
a	O
splendid	O
idea	O
,	O
ascending	O
the	O
gradient	O
directly	O
may	O
not	O
be	O
.	O
the	O
fact	O
that	O
the	O
algorithm	B
is	O
ill-conditioned	O
can	O
be	O
seen	O
in	O
the	O
fact	O
that	O
it	O
involves	O
a	O
matrix	B
inverse	O
,	O
which	O
can	O
be	O
arbitrarily	O
large	O
or	O
even	O
unde	O
(	O
cid:12	O
)	O
ned	O
.	O
covariant	B
optimization	O
in	O
general	O
the	O
principle	O
of	O
covariance	O
says	O
that	O
a	O
consistent	O
algorithm	B
should	O
give	O
the	O
same	O
results	O
independent	O
of	O
the	O
units	B
in	O
which	O
quantities	O
are	O
measured	O
(	O
knuth	O
,	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
34.3	O
:	O
a	O
covariant	B
,	O
simpler	O
,	O
and	O
faster	O
learning	B
algorithm	O
441	O
here	O
n	O
is	O
the	O
number	O
of	O
iterations	O
.	O
1968	O
)	O
.	O
a	O
prime	O
example	O
of	O
a	O
non-covariant	O
algorithm	B
is	O
the	O
popular	O
steepest	B
descents	I
rule	O
.	O
a	O
dimensionless	O
objective	B
function	I
l	O
(	O
w	O
)	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
,	O
its	O
deriva-	O
tive	O
with	O
respect	O
to	O
some	O
parameters	B
w	O
is	O
computed	O
,	O
and	O
then	O
w	O
is	O
changed	O
by	O
the	O
rule	O
(	O
cid:1	O
)	O
wi	O
=	O
(	O
cid:17	O
)	O
@	O
l	O
@	O
wi	O
:	O
(	O
34.19	O
)	O
this	O
popular	O
equation	O
is	O
dimensionally	O
inconsistent	O
:	O
the	O
left-hand	O
side	O
of	O
this	O
equation	O
has	O
dimensions	B
of	O
[	O
wi	O
]	O
and	O
the	O
right-hand	O
side	O
has	O
dimensions	B
1=	O
[	O
wi	O
]	O
.	O
the	O
behaviour	O
of	O
the	O
learning	O
algorithm	B
(	O
34.19	O
)	O
is	O
not	O
covariant	B
with	O
respect	O
to	O
linear	B
rescaling	O
of	O
the	O
vector	O
w.	O
dimensional	O
inconsistency	O
is	O
not	O
the	O
end	O
of	O
the	O
world	O
,	O
as	O
the	O
success	O
of	O
numerous	O
gradient	B
descent	I
algorithms	O
has	O
demon-	O
strated	O
,	O
and	O
indeed	O
if	O
(	O
cid:17	O
)	O
decreases	O
with	O
n	O
(	O
during	O
on-line	O
learning	B
)	O
as	O
1=n	O
then	O
the	O
munro	O
{	O
robbins	O
theorem	B
(	O
bishop	O
,	O
1992	O
,	O
p.	O
41	O
)	O
shows	O
that	O
the	O
parameters	B
will	O
asymptotically	O
converge	O
to	O
the	O
maximum	B
likelihood	I
parameters	O
.	O
but	O
the	O
non-covariant	O
algorithm	B
may	O
take	O
a	O
very	O
large	O
number	O
of	O
iterations	O
to	O
achieve	O
this	O
convergence	O
;	O
indeed	O
many	O
former	O
users	O
of	O
steepest	O
descents	O
algorithms	B
prefer	O
to	O
use	O
algorithms	B
such	O
as	O
conjugate	O
gradients	O
that	O
adaptively	O
(	O
cid:12	O
)	O
gure	O
out	O
the	O
curvature	O
of	O
the	O
objective	O
function	B
.	O
the	O
defense	O
of	O
equation	O
(	O
34.19	O
)	O
that	O
points	O
out	O
(	O
cid:17	O
)	O
could	O
be	O
a	O
dimensional	O
constant	O
is	O
untenable	O
if	O
not	O
all	O
the	O
parameters	B
wi	O
have	O
the	O
same	O
dimensions	B
.	O
the	O
algorithm	B
would	O
be	O
covariant	B
if	O
it	O
had	O
the	O
form	O
(	O
cid:1	O
)	O
wi	O
=	O
(	O
cid:17	O
)	O
xi0	O
mii0	O
@	O
l	O
@	O
wi	O
;	O
(	O
34.20	O
)	O
where	O
m	O
is	O
a	O
positive-de	O
(	O
cid:12	O
)	O
nite	O
matrix	B
whose	O
i	O
;	O
i0	O
element	O
has	O
dimensions	B
[	O
wiwi0	O
]	O
.	O
from	O
where	O
can	O
we	O
obtain	O
such	O
a	O
matrix	B
?	O
two	O
sources	O
of	O
such	O
matrices	B
are	O
metrics	O
and	O
curvatures	O
.	O
metrics	O
and	O
curvatures	O
if	O
there	O
is	O
a	O
natural	B
metric	O
that	O
de	O
(	O
cid:12	O
)	O
nes	O
distances	O
in	O
our	O
parameter	O
space	O
w	O
,	O
then	O
a	O
matrix	B
m	O
can	O
be	O
obtained	O
from	O
the	O
metric	B
.	O
there	O
is	O
often	O
a	O
natural	B
choice	O
.	O
in	O
the	O
special	O
case	O
where	O
there	O
is	O
a	O
known	O
quadratic	O
metric	B
de	O
(	O
cid:12	O
)	O
ning	O
the	O
length	B
of	O
a	O
vector	O
w	O
,	O
then	O
the	O
matrix	B
can	O
be	O
obtained	O
from	O
the	O
quadratic	O
form	O
.	O
for	O
example	O
if	O
the	O
length	B
is	O
w2	O
then	O
the	O
natural	B
matrix	O
is	O
m	O
=	O
i	O
,	O
and	O
steepest	O
descents	O
is	O
appropriate	O
.	O
another	O
way	O
of	O
(	O
cid:12	O
)	O
nding	O
a	O
metric	B
is	O
to	O
look	O
at	O
the	O
curvature	O
of	O
the	O
objective	O
function	B
,	O
de	O
(	O
cid:12	O
)	O
ning	O
a	O
(	O
cid:17	O
)	O
(	O
cid:0	O
)	O
rrl	O
(	O
where	O
r	O
(	O
cid:17	O
)	O
@	O
=	O
@	O
w	O
)	O
.	O
then	O
the	O
matrix	B
m	O
=	O
a	O
(	O
cid:0	O
)	O
1	O
will	O
give	O
a	O
covariant	B
algorithm	I
;	O
what	O
is	O
more	O
,	O
this	O
algorithm	B
is	O
the	O
newton	O
algorithm	B
,	O
so	O
we	O
recognize	O
that	O
it	O
will	O
alleviate	O
one	O
of	O
the	O
principal	O
di	O
(	O
cid:14	O
)	O
culties	O
with	O
steepest	O
descents	O
,	O
namely	O
its	O
slow	O
convergence	O
to	O
a	O
minimum	O
when	O
the	O
objective	B
function	I
is	O
at	O
all	O
ill-conditioned	O
.	O
the	O
newton	O
algorithm	B
converges	O
to	O
the	O
minimum	O
in	O
a	O
single	O
step	O
if	O
l	O
is	O
quadratic	O
.	O
in	O
some	O
problems	O
it	O
may	O
be	O
that	O
the	O
curvature	O
a	O
consists	O
of	O
both	O
data-	O
dependent	O
terms	O
and	O
data-independent	O
terms	O
;	O
in	O
this	O
case	O
,	O
one	O
might	O
choose	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
metric	B
using	O
the	O
data-independent	O
terms	O
only	O
(	O
gull	O
,	O
1989	O
)	O
.	O
the	O
resulting	O
algorithm	B
will	O
still	O
be	O
covariant	B
but	O
it	O
will	O
not	O
implement	O
an	O
exact	O
newton	O
step	O
.	O
obviously	O
there	O
are	O
many	O
covariant	B
algorithms	O
;	O
there	O
is	O
no	O
unique	O
choice	O
.	O
but	O
covariant	B
algorithms	O
are	O
a	O
small	O
subset	B
of	O
the	O
set	B
of	O
all	O
algorithms	B
!	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
442	O
34	O
|	O
independent	B
component	I
analysis	I
and	O
latent	B
variable	I
modelling	O
back	O
to	O
independent	B
component	I
analysis	I
for	O
the	O
present	O
maximum	B
likelihood	I
problem	O
we	O
have	O
evaluated	O
the	O
gradient	O
with	O
respect	O
to	O
g	O
and	O
the	O
gradient	O
with	O
respect	O
to	O
w	O
=	O
g	O
(	O
cid:0	O
)	O
1.	O
steepest	O
ascents	O
in	O
w	O
is	O
not	O
covariant	B
.	O
let	O
us	O
construct	O
an	O
alternative	O
,	O
covariant	B
algorithm	I
with	O
the	O
help	O
of	O
the	O
curvature	O
of	O
the	O
log	O
likelihood	B
.	O
taking	O
the	O
second	O
derivative	O
of	O
the	O
log	O
likelihood	B
with	O
respect	O
to	O
w	O
we	O
obtain	O
two	O
terms	O
,	O
the	O
(	O
cid:12	O
)	O
rst	O
of	O
which	O
is	O
data-independent	O
:	O
@	O
gji	O
@	O
wkl	O
=	O
(	O
cid:0	O
)	O
gjkgli	O
;	O
(	O
34.21	O
)	O
and	O
the	O
second	O
of	O
which	O
is	O
data-dependent	O
:	O
@	O
(	O
zixj	O
)	O
@	O
wkl	O
=	O
xjxl	O
(	O
cid:14	O
)	O
ikz0i	O
;	O
(	O
no	O
sum	O
over	O
i	O
)	O
(	O
34.22	O
)	O
where	O
z0	O
is	O
the	O
derivative	O
of	O
z.	O
it	O
is	O
tempting	O
to	O
drop	O
the	O
data-dependent	O
term	O
and	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
matrix	B
m	O
by	O
[	O
m	O
(	O
cid:0	O
)	O
1	O
]	O
(	O
ij	O
)	O
(	O
kl	O
)	O
=	O
[	O
gjkgli	O
]	O
.	O
however	O
,	O
this	O
matrix	B
is	O
not	O
positive	O
de	O
(	O
cid:12	O
)	O
nite	O
(	O
it	O
has	O
at	O
least	O
one	O
non-positive	O
eigenvalue	B
)	O
,	O
so	O
it	O
is	O
a	O
poor	O
approximation	O
to	O
the	O
curvature	O
of	O
the	O
log	O
likelihood	B
,	O
which	O
must	O
be	O
positive	O
de	O
(	O
cid:12	O
)	O
nite	O
in	O
the	O
neighbourhood	O
of	O
a	O
maximum	B
likelihood	I
solution	O
.	O
we	O
must	O
therefore	O
consult	O
the	O
data-dependent	O
term	O
for	O
inspiration	O
.	O
the	O
aim	O
is	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
convenient	O
approximation	B
to	O
the	O
curvature	O
and	O
to	O
obtain	O
a	O
covariant	B
algorithm	I
,	O
not	O
necessarily	O
to	O
implement	O
an	O
exact	O
newton	O
step	O
.	O
what	O
is	O
the	O
average	B
value	O
of	O
xjxl	O
(	O
cid:14	O
)	O
ikz0i	O
?	O
if	O
the	O
true	O
value	O
of	O
g	O
is	O
g	O
(	O
cid:3	O
)	O
,	O
then	O
(	O
cid:10	O
)	O
xjxl	O
(	O
cid:14	O
)	O
ikz0i	O
(	O
cid:11	O
)	O
=	O
(	O
cid:10	O
)	O
g	O
(	O
cid:3	O
)	O
jmsmsng	O
(	O
cid:3	O
)	O
ln	O
(	O
cid:14	O
)	O
ikz0i	O
(	O
cid:11	O
)	O
:	O
(	O
34.23	O
)	O
we	O
now	O
make	O
several	O
severe	O
approximations	O
:	O
we	O
replace	O
g	O
(	O
cid:3	O
)	O
by	O
the	O
present	O
value	O
of	O
g	O
,	O
and	O
replace	O
the	O
correlated	O
average	O
hsmsnz0ii	O
by	O
hsmsnihz0ii	O
(	O
cid:17	O
)	O
(	O
cid:6	O
)	O
mndi	O
.	O
here	O
(	O
cid:6	O
)	O
is	O
the	O
variance	B
{	O
covariance	B
matrix	I
of	O
the	O
latent	O
variables	O
(	O
which	O
is	O
assumed	O
to	O
exist	O
)	O
,	O
and	O
di	O
is	O
the	O
typical	B
value	O
of	O
the	O
curvature	O
d2	O
ln	O
pi	O
(	O
a	O
)	O
=da2	O
.	O
given	O
that	O
the	O
sources	O
are	O
assumed	O
to	O
be	O
independent	O
,	O
(	O
cid:6	O
)	O
and	O
d	O
are	O
both	O
diagonal	O
matrices	B
.	O
these	O
approximations	O
motivate	O
the	O
ma-	O
trix	O
m	O
given	O
by	O
:	O
that	O
is	O
,	O
[	O
m	O
(	O
cid:0	O
)	O
1	O
]	O
(	O
ij	O
)	O
(	O
kl	O
)	O
=	O
gjm	O
(	O
cid:6	O
)	O
mngln	O
(	O
cid:14	O
)	O
ikdi	O
;	O
m	O
(	O
ij	O
)	O
(	O
kl	O
)	O
=	O
wmj	O
(	O
cid:6	O
)	O
(	O
cid:0	O
)	O
1	O
mnwnl	O
(	O
cid:14	O
)	O
ikd	O
(	O
cid:0	O
)	O
1	O
i	O
:	O
(	O
34.24	O
)	O
(	O
34.25	O
)	O
for	O
simplicity	O
,	O
we	O
further	O
assume	O
that	O
the	O
sources	O
are	O
similar	O
to	O
each	O
other	O
so	O
that	O
(	O
cid:6	O
)	O
and	O
d	O
are	O
both	O
homogeneous	B
,	O
and	O
that	O
(	O
cid:6	O
)	O
d	O
=	O
1.	O
this	O
will	O
lead	O
us	O
to	O
an	O
algorithm	B
that	O
is	O
covariant	B
with	O
respect	O
to	O
linear	B
rescaling	O
of	O
the	O
data	O
x	O
,	O
but	O
not	O
with	O
respect	O
to	O
linear	B
rescaling	O
of	O
the	O
latent	O
variables	O
.	O
we	O
thus	O
use	O
:	O
m	O
(	O
ij	O
)	O
(	O
kl	O
)	O
=	O
wmjwml	O
(	O
cid:14	O
)	O
ik	O
:	O
(	O
34.26	O
)	O
multiplying	O
this	O
matrix	B
by	O
the	O
gradient	O
in	O
equation	O
(	O
34.15	O
)	O
we	O
obtain	O
the	O
following	O
covariant	B
learning	O
algorithm	B
:	O
(	O
cid:1	O
)	O
wij	O
=	O
(	O
cid:17	O
)	O
(	O
cid:0	O
)	O
wij	O
+	O
wi0jai0zi	O
(	O
cid:1	O
)	O
:	O
(	O
34.27	O
)	O
notice	O
that	O
this	O
expression	O
does	O
not	O
require	O
any	O
inversion	O
of	O
the	O
matrix	B
w.	O
the	O
only	O
additional	O
computation	O
once	O
z	O
has	O
been	O
computed	O
is	O
a	O
single	O
back-	O
ward	O
pass	O
through	O
the	O
weights	O
to	O
compute	O
the	O
quantity	O
x0j	O
=	O
wi0jai0	O
(	O
34.28	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
34.3	O
:	O
a	O
covariant	B
,	O
simpler	O
,	O
and	O
faster	O
learning	B
algorithm	O
443	O
repeat	O
for	O
each	O
datapoint	O
x	O
:	O
1.	O
put	O
x	O
through	O
a	O
linear	B
mapping	O
:	O
a	O
=	O
wx	O
:	O
2.	O
put	O
a	O
through	O
a	O
nonlinear	B
map	O
:	O
zi	O
=	O
(	O
cid:30	O
)	O
i	O
(	O
ai	O
)	O
;	O
where	O
a	O
popular	O
choice	O
for	O
(	O
cid:30	O
)	O
is	O
(	O
cid:30	O
)	O
=	O
(	O
cid:0	O
)	O
tanh	O
(	O
ai	O
)	O
.	O
3.	O
put	O
a	O
back	O
through	O
w	O
:	O
x0	O
=	O
wta	O
:	O
4.	O
adjust	O
the	O
weights	O
in	O
accordance	O
with	O
(	O
cid:1	O
)	O
w	O
/	O
w	O
+	O
zx0t	O
:	O
in	O
terms	O
of	O
which	O
the	O
covariant	B
algorithm	I
reads	O
:	O
(	O
cid:1	O
)	O
wij	O
=	O
(	O
cid:17	O
)	O
(	O
cid:0	O
)	O
wij	O
+	O
x0jzi	O
(	O
cid:1	O
)	O
:	O
algorithm	B
34.4.	O
independent	B
component	I
analysis	I
{	O
covariant	B
version	O
.	O
(	O
34.29	O
)	O
the	O
quantity	O
(	O
cid:16	O
)	O
wij	O
+	O
x0jzi	O
(	O
cid:17	O
)	O
on	O
the	O
right-hand	O
side	O
is	O
sometimes	O
called	O
the	O
natural	B
gradient	I
.	O
the	O
covariant	B
independent	O
component	O
analysis	B
algorithm	O
is	O
summarized	O
in	O
algorithm	O
34.4.	O
further	O
reading	O
ica	O
was	O
originally	O
derived	O
using	O
an	O
information	B
maximization	I
approach	O
(	O
bell	O
and	O
sejnowski	O
,	O
1995	O
)	O
.	O
another	O
view	O
of	O
ica	O
,	O
in	O
terms	O
of	O
energy	O
functions	B
,	O
which	O
motivates	O
more	O
general	O
models	O
,	O
is	O
given	O
by	O
hinton	O
et	O
al	O
.	O
(	O
2001	O
)	O
.	O
another	O
generalization	B
of	O
ica	O
can	O
be	O
found	O
in	O
pearlmutter	O
and	O
parra	O
(	O
1996	O
,	O
1997	O
)	O
.	O
there	O
is	O
now	O
an	O
enormous	O
literature	O
on	O
applications	O
of	O
ica	O
.	O
a	O
variational	B
free	I
energy	I
minimization	O
approach	O
to	O
ica-like	O
models	O
is	O
given	O
in	O
(	O
miskin	O
,	O
2001	O
;	O
miskin	O
and	O
mackay	O
,	O
2000	O
;	O
miskin	O
and	O
mackay	O
,	O
2001	O
)	O
.	O
further	O
reading	O
on	O
blind	O
separation	B
,	O
including	O
non-ica	O
algorithms	B
,	O
can	O
be	O
found	O
in	O
(	O
jutten	O
and	O
herault	O
,	O
1991	O
;	O
comon	O
et	O
al.	O
,	O
1991	O
;	O
hendin	O
et	O
al.	O
,	O
1994	O
;	O
amari	O
et	O
al.	O
,	O
1996	O
;	O
hojen-sorensen	O
et	O
al.	O
,	O
2002	O
)	O
.	O
in	O
(	O
cid:12	O
)	O
nite	O
models	O
while	O
latent	B
variable	I
models	I
with	O
a	O
(	O
cid:12	O
)	O
nite	O
number	O
of	O
latent	O
variables	O
are	O
widely	O
used	O
,	O
it	O
is	O
often	O
the	O
case	O
that	O
our	O
beliefs	O
about	O
the	O
situation	O
would	O
be	O
most	O
accurately	O
captured	O
by	O
a	O
very	O
large	O
number	O
of	O
latent	O
variables	O
.	O
consider	O
clustering	B
,	O
for	O
example	O
.	O
if	O
we	O
attack	O
speech	O
recognition	B
by	O
mod-	O
elling	O
words	O
using	O
a	O
cluster	O
model	B
,	O
how	O
many	O
clusters	O
should	O
we	O
use	O
?	O
the	O
number	O
of	O
possible	O
words	O
is	O
unbounded	O
(	O
section	B
18.2	O
)	O
,	O
so	O
we	O
would	O
really	O
like	O
to	O
use	O
a	O
model	B
in	O
which	O
it	O
’	O
s	O
always	O
possible	O
for	O
new	O
clusters	O
to	O
arise	O
.	O
furthermore	O
,	O
if	O
we	O
do	O
a	O
careful	O
job	O
of	O
modelling	O
the	O
cluster	O
corresponding	O
to	O
just	O
one	O
english	O
word	O
,	O
we	O
will	O
probably	O
(	O
cid:12	O
)	O
nd	O
that	O
the	O
cluster	O
for	O
one	O
word	O
should	O
itself	O
be	O
modelled	O
as	O
composed	O
of	O
clusters	O
{	O
indeed	O
,	O
a	O
hierarchy	O
of	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
444	O
34	O
|	O
independent	B
component	I
analysis	I
and	O
latent	B
variable	I
modelling	O
clusters	O
within	O
clusters	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
levels	O
of	O
the	O
hierarchy	O
would	O
divide	O
male	B
speakers	O
from	O
female	O
,	O
and	O
would	O
separate	O
speakers	O
from	O
di	O
(	O
cid:11	O
)	O
erent	O
regions	O
{	O
india	O
,	O
britain	O
,	O
europe	O
,	O
and	O
so	O
forth	O
.	O
within	O
each	O
of	O
those	O
clusters	O
would	O
be	O
subclusters	O
for	O
the	O
di	O
(	O
cid:11	O
)	O
erent	O
accents	O
within	O
each	O
region	O
.	O
the	O
subclusters	O
could	O
have	O
subsubclusters	O
right	O
down	O
to	O
the	O
level	O
of	O
villages	O
,	O
streets	O
,	O
or	O
families	O
.	O
thus	O
we	O
would	O
often	O
like	O
to	O
have	O
in	O
(	O
cid:12	O
)	O
nite	O
numbers	O
of	O
clusters	O
;	O
in	O
some	O
cases	O
the	O
clusters	O
would	O
have	O
a	O
hierarchical	O
structure	O
,	O
and	O
in	O
other	O
cases	O
the	O
hierarchy	O
would	O
be	O
(	O
cid:13	O
)	O
at	O
.	O
so	O
,	O
how	O
should	O
such	O
in	O
(	O
cid:12	O
)	O
nite	O
models	O
be	O
implemented	O
in	O
(	O
cid:12	O
)	O
nite	O
computers	O
?	O
and	O
how	O
should	O
we	O
set	B
up	O
our	O
bayesian	O
models	O
so	O
as	O
to	O
avoid	O
getting	O
silly	O
answers	O
?	O
in	O
(	O
cid:12	O
)	O
nite	O
mixture	O
models	O
for	O
categorical	O
data	O
are	O
presented	O
in	O
neal	O
(	O
1991	O
)	O
,	O
along	O
with	O
a	O
monte	O
carlo	O
method	B
for	O
simulating	O
inferences	O
and	O
predictions	O
.	O
in	O
(	O
cid:12	O
)	O
nite	O
gaussian	O
mixture	O
models	O
with	O
a	O
(	O
cid:13	O
)	O
at	O
hierarchical	O
structure	O
are	O
pre-	O
sented	O
in	O
rasmussen	O
(	O
2000	O
)	O
.	O
neal	O
(	O
2001	O
)	O
shows	O
how	O
to	O
use	O
dirichlet	O
di	O
(	O
cid:11	O
)	O
usion	O
trees	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
models	O
of	O
hierarchical	O
clusters	O
.	O
most	O
of	O
these	O
ideas	O
build	O
on	O
the	O
dirichlet	O
process	O
(	O
section	B
18.2	O
)	O
.	O
this	O
remains	O
an	O
active	O
research	O
area	O
(	O
rasmussen	O
and	O
ghahramani	O
,	O
2002	O
;	O
beal	O
et	O
al.	O
,	O
2002	O
)	O
.	O
34.4	O
exercises	O
exercise	O
34.1	O
.	O
[	O
3	O
]	O
repeat	O
the	O
derivation	B
of	O
the	O
algorithm	B
,	O
but	O
assume	O
a	O
small	O
amount	O
of	O
noise	O
in	O
x	O
:	O
x	O
=	O
gs	O
+	O
n	O
;	O
so	O
the	O
term	O
(	O
cid:14	O
)	O
(	O
cid:16	O
)	O
x	O
(	O
n	O
)	O
in	O
the	O
joint	B
probability	O
(	O
34.3	O
)	O
is	O
replaced	O
by	O
a	O
probability	B
distribution	O
over	O
x	O
(	O
n	O
)	O
.	O
show	O
that	O
,	O
if	O
this	O
noise	B
distribution	O
has	O
su	O
(	O
cid:14	O
)	O
ciently	O
small	O
standard	B
deviation	I
,	O
the	O
identical	O
algorithm	O
results	O
.	O
i	O
(	O
cid:17	O
)	O
j	O
(	O
cid:0	O
)	O
pi	O
gjis	O
(	O
n	O
)	O
j	O
with	O
meanpi	O
gjis	O
(	O
n	O
)	O
i	O
exercise	O
34.2	O
.	O
[	O
3	O
]	O
implement	O
the	O
covariant	B
ica	O
algorithm	B
and	O
apply	O
it	O
to	O
toy	O
data	O
.	O
exercise	O
34.3	O
.	O
[	O
4-5	O
]	O
create	O
algorithms	B
appropriate	O
for	O
the	O
situations	O
:	O
(	O
a	O
)	O
x	O
in-	O
cludes	O
substantial	O
gaussian	O
noise	B
;	O
(	O
b	O
)	O
more	O
measurements	O
than	O
latent	O
variables	O
(	O
j	O
>	O
i	O
)	O
;	O
(	O
c	O
)	O
fewer	O
measurements	O
than	O
latent	O
variables	O
(	O
j	O
<	O
i	O
)	O
.	O
factor	B
analysis	I
assumes	O
that	O
the	O
observations	O
x	O
can	O
be	O
described	O
in	O
terms	O
of	O
independent	O
latent	O
variables	O
fskg	O
and	O
independent	O
additive	O
noise	B
.	O
thus	O
the	O
observable	O
x	O
is	O
given	O
by	O
(	O
34.30	O
)	O
x	O
=	O
gs	O
+	O
n	O
;	O
where	O
n	O
is	O
a	O
noise	B
vector	O
whose	O
components	O
have	O
a	O
separable	O
probability	B
distri-	O
bution	O
.	O
in	O
factor	O
analysis	B
it	O
is	O
often	O
assumed	O
that	O
the	O
probability	B
distributions	I
of	O
fskg	O
and	O
fnig	O
are	O
zero-mean	O
gaussians	O
;	O
the	O
noise	B
terms	O
may	O
have	O
di	O
(	O
cid:11	O
)	O
erent	O
variances	O
(	O
cid:27	O
)	O
2	O
i	O
.	O
exercise	O
34.4	O
.	O
[	O
4	O
]	O
make	O
a	O
maximum	B
likelihood	I
algorithm	O
for	O
inferring	O
g	O
from	O
data	O
,	O
assuming	O
the	O
generative	B
model	I
x	O
=	O
gs	O
+	O
n	O
is	O
correct	O
and	O
that	O
s	O
and	O
n	O
have	O
independent	O
gaussian	O
distributions	O
.	O
include	O
parameters	B
(	O
cid:27	O
)	O
2	O
j	O
to	O
describe	O
the	O
variance	B
of	O
each	O
nj	O
,	O
and	O
maximize	O
the	O
likelihood	B
with	O
respect	O
to	O
them	O
too	O
.	O
let	O
the	O
variance	B
of	O
each	O
si	O
be	O
1.	O
exercise	O
34.5	O
.	O
[	O
4c	O
]	O
implement	O
the	O
in	O
(	O
cid:12	O
)	O
nite	O
gaussian	O
mixture	O
model	O
of	O
rasmussen	O
(	O
2000	O
)	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
35	O
random	B
inference	O
topics	O
35.1	O
what	O
do	O
you	O
know	O
if	O
you	O
are	O
ignorant	O
?	O
6	O
6	O
6	O
example	O
35.1.	O
a	O
real	O
variable	O
x	O
is	O
measured	O
in	O
an	O
accurate	O
experiment	O
.	O
for	O
example	O
,	O
x	O
might	O
be	O
the	O
half-life	O
of	O
the	O
neutron	O
,	O
the	O
wavelength	O
of	O
light	O
emitted	O
by	O
a	O
(	O
cid:12	O
)	O
re	O
(	O
cid:13	O
)	O
y	O
,	O
the	O
depth	B
of	I
lake	I
vostok	O
,	O
or	O
the	O
mass	O
of	O
jupiter	O
’	O
s	O
moon	O
io	O
.	O
what	O
is	O
the	O
probability	B
that	O
the	O
value	O
of	O
x	O
starts	O
with	O
a	O
‘	O
1	O
’	O
,	O
like	O
the	O
charge	O
of	O
the	O
electron	O
(	O
in	O
s.i	O
.	O
units	B
)	O
,	O
e	O
=	O
1:602	O
:	O
:	O
:	O
(	O
cid:2	O
)	O
10	O
(	O
cid:0	O
)	O
19	O
c	O
;	O
and	O
the	O
boltzmann	O
constant	O
,	O
k	O
=	O
1:380	O
66	O
:	O
:	O
:	O
(	O
cid:2	O
)	O
10	O
(	O
cid:0	O
)	O
23	O
j	O
k	O
(	O
cid:0	O
)	O
1	O
?	O
and	O
what	O
is	O
the	O
probability	B
that	O
it	O
starts	O
with	O
a	O
‘	O
9	O
’	O
,	O
like	O
the	O
faraday	O
constant	O
,	O
f	O
=	O
9:648	O
:	O
:	O
:	O
(	O
cid:2	O
)	O
104	O
c	O
mol	O
(	O
cid:0	O
)	O
1	O
?	O
what	O
about	O
the	O
second	O
digit	O
?	O
what	O
is	O
the	O
probability	B
that	O
the	O
mantissa	O
of	O
x	O
starts	O
‘	O
1.1	O
...	O
’	O
,	O
and	O
what	O
is	O
the	O
probability	B
that	O
x	O
starts	O
‘	O
9.9	O
...	O
’	O
?	O
solution	O
.	O
an	O
expert	O
on	O
neutrons	O
,	O
(	O
cid:12	O
)	O
re	O
(	O
cid:13	O
)	O
ies	O
,	O
antarctica	O
,	O
or	O
jove	O
might	O
be	O
able	O
to	O
predict	O
the	O
value	O
of	O
x	O
,	O
and	O
thus	O
predict	O
the	O
(	O
cid:12	O
)	O
rst	O
digit	O
with	O
some	O
con	O
(	O
cid:12	O
)	O
dence	O
,	O
but	O
what	O
about	O
someone	O
with	O
no	O
knowledge	O
of	O
the	O
topic	O
?	O
what	O
is	O
the	O
probability	B
distribution	O
corresponding	O
to	O
‘	O
knowing	O
nothing	O
’	O
?	O
one	O
way	O
to	O
attack	O
this	O
question	O
is	O
to	O
notice	O
that	O
the	O
units	B
of	O
x	O
have	O
not	O
been	O
speci	O
(	O
cid:12	O
)	O
ed	O
.	O
if	O
the	O
half-life	O
of	O
the	O
neutron	O
were	O
measured	O
in	O
fortnights	O
instead	O
of	O
seconds	O
,	O
the	O
number	O
x	O
would	O
be	O
divided	O
by	O
1	O
209	O
600	O
;	O
if	O
it	O
were	O
measured	O
in	O
years	O
,	O
it	O
would	O
be	O
divided	O
by	O
3	O
(	O
cid:2	O
)	O
107.	O
now	O
,	O
is	O
our	O
knowledge	O
about	O
x	O
,	O
and	O
,	O
in	O
particular	O
,	O
our	O
knowledge	O
of	O
its	O
(	O
cid:12	O
)	O
rst	O
digit	O
,	O
a	O
(	O
cid:11	O
)	O
ected	O
by	O
the	O
change	O
in	O
units	O
?	O
for	O
the	O
expert	O
,	O
the	O
answer	O
is	O
yes	O
;	O
but	O
let	O
us	O
take	O
someone	O
truly	O
ignorant	O
,	O
for	O
whom	O
the	O
answer	O
is	O
no	O
;	O
their	O
predictions	O
about	O
the	O
(	O
cid:12	O
)	O
rst	O
digit	O
of	O
x	O
are	O
independent	O
of	O
the	O
units	B
.	O
the	O
arbitrariness	O
of	O
the	O
units	O
corresponds	O
to	O
invariance	B
of	O
the	O
probability	B
distribution	O
when	O
x	O
is	O
multiplied	O
by	O
any	O
number	O
.	O
80	O
70	O
60	O
50	O
40	O
30	O
20	O
10	O
9	O
8	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
metres	O
200	O
100	O
90	O
80	O
70	O
60	O
50	O
40	O
30	O
20	O
10	O
9	O
8	O
7	O
6	O
5	O
4	O
3000	O
2000	O
1000	O
900	O
800	O
700	O
600	O
500	O
400	O
300	O
200	O
100	O
90	O
80	O
70	O
60	O
50	O
40	O
3	O
feet	O
inches	O
figure	O
35.1.	O
when	O
viewed	O
on	O
a	O
logarithmic	O
scale	O
,	O
scales	O
using	O
di	O
(	O
cid:11	O
)	O
erent	O
units	B
are	O
translated	O
relative	B
to	O
each	O
other	O
.	O
if	O
you	O
don	O
’	O
t	O
know	O
the	O
units	B
that	O
a	O
quantity	O
is	O
measured	O
in	O
,	O
the	O
probability	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
digit	O
must	O
be	O
proportional	O
to	O
the	O
length	B
of	O
the	O
corresponding	O
piece	O
of	O
logarithmic	O
scale	O
.	O
the	O
probability	B
that	O
the	O
(	O
cid:12	O
)	O
rst	O
digit	O
of	O
a	O
number	O
is	O
1	O
is	O
thus	O
=	O
log	O
2	O
log	O
10	O
:	O
(	O
35.1	O
)	O
p1	O
=	O
log	O
2	O
(	O
cid:0	O
)	O
log	O
1	O
log	O
10	O
(	O
cid:0	O
)	O
log	O
1	O
445	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
446	O
35	O
|	O
random	B
inference	O
topics	O
6	O
?	O
p	O
(	O
9	O
)	O
p	O
(	O
3	O
)	O
6	O
?	O
10	O
9	O
8	O
7	O
6	O
5	O
4	O
3	O
p	O
(	O
1	O
)	O
2	O
6	O
?	O
1	O
now	O
,	O
210	O
=	O
1024	O
’	O
103	O
=	O
1000	O
,	O
so	O
without	O
needing	O
a	O
calculator	B
,	O
we	O
have	O
10	O
log	O
2	O
’	O
3	O
log	O
10	O
and	O
:	O
(	O
35.2	O
)	O
p1	O
’	O
3	O
10	O
more	O
generally	O
,	O
the	O
probability	B
that	O
the	O
(	O
cid:12	O
)	O
rst	O
digit	O
is	O
d	O
is	O
(	O
log	O
(	O
d	O
+	O
1	O
)	O
(	O
cid:0	O
)	O
log	O
(	O
d	O
)	O
)	O
=	O
(	O
log	O
10	O
(	O
cid:0	O
)	O
log	O
1	O
)	O
=	O
log10	O
(	O
1	O
+	O
1=d	O
)	O
:	O
(	O
35.3	O
)	O
this	O
observation	O
about	O
initial	O
digits	O
is	O
known	O
as	O
benford	O
’	O
s	O
law	O
.	O
does	O
not	O
correspond	O
to	O
a	O
uniform	O
probability	B
distribution	O
over	O
d.	O
ignorance	B
2	O
.	O
exercise	O
35.2	O
.	O
[	O
2	O
]	O
a	O
pin	O
is	O
thrown	O
tumbling	O
in	O
the	O
air	O
.	O
what	O
is	O
the	O
probability	B
distribution	O
of	O
the	O
angle	O
(	O
cid:18	O
)	O
1	O
between	O
the	O
pin	O
and	O
the	O
vertical	O
at	O
a	O
moment	O
while	O
it	O
is	O
in	O
the	O
air	O
?	O
the	O
tumbling	O
pin	O
is	O
photographed	O
.	O
what	O
is	O
the	O
probability	B
distribution	O
of	O
the	O
angle	O
(	O
cid:18	O
)	O
3	O
between	O
the	O
pin	O
and	O
the	O
vertical	O
as	O
imaged	O
in	O
the	O
photograph	O
?	O
.	O
exercise	O
35.3	O
.	O
[	O
2	O
]	O
record	B
breaking	I
.	O
consider	O
keeping	O
track	O
of	O
the	O
world	O
record	O
for	O
some	O
quantity	O
x	O
,	O
say	O
earthquake	B
magnitude	O
,	O
or	O
longjump	O
distances	O
jumped	O
at	O
world	O
championships	O
.	O
if	O
we	O
assume	O
that	O
attempts	O
to	O
break	O
the	O
record	O
take	O
place	O
at	O
a	O
steady	O
rate	B
,	O
and	O
if	O
we	O
assume	O
that	O
the	O
under-	O
lying	O
probability	B
distribution	O
of	O
the	O
outcome	O
x	O
,	O
p	O
(	O
x	O
)	O
,	O
is	O
not	O
changing	O
{	O
an	O
assumption	O
that	O
i	O
think	O
is	O
unlikely	O
to	O
be	O
true	O
in	O
the	O
case	O
of	O
sports	O
endeavours	O
,	O
but	O
an	O
interesting	O
assumption	O
to	O
consider	O
nonetheless	O
{	O
and	O
assuming	O
no	O
knowledge	O
at	O
all	O
about	O
p	O
(	O
x	O
)	O
,	O
what	O
can	O
be	O
predicted	O
about	O
successive	O
intervals	B
between	O
the	O
dates	O
when	O
records	O
are	O
broken	O
?	O
35.2	O
the	O
luria	O
{	O
delbr	O
(	O
cid:127	O
)	O
uck	O
distribution	B
exercise	O
35.4	O
.	O
[	O
3c	O
,	O
p.449	O
]	O
in	O
their	O
landmark	O
paper	O
demonstrating	O
that	O
bacteria	O
could	O
mutate	O
from	O
virus	O
sensitivity	O
to	O
virus	O
resistance	O
,	O
luria	O
and	O
delbr	O
(	O
cid:127	O
)	O
uck	O
(	O
1943	O
)	O
wanted	O
to	O
estimate	O
the	O
mutation	B
rate	I
in	O
an	O
exponentially-growing	O
pop-	O
ulation	O
from	O
the	O
total	O
number	O
of	O
mutants	O
found	O
at	O
the	O
end	O
of	O
the	O
experi-	O
ment	O
.	O
this	O
problem	O
is	O
di	O
(	O
cid:14	O
)	O
cult	O
because	O
the	O
quantity	O
measured	O
(	O
the	O
number	O
of	O
mutated	O
bacteria	O
)	O
has	O
a	O
heavy-tailed	O
probability	B
distribution	O
:	O
a	O
mutation	O
occuring	O
early	O
in	O
the	O
experiment	O
can	O
give	O
rise	O
to	O
a	O
huge	O
number	O
of	O
mutants	O
.	O
unfortunately	O
,	O
luria	O
and	O
delbr	O
(	O
cid:127	O
)	O
uck	O
didn	O
’	O
t	O
know	O
bayes	O
’	O
theorem	B
,	O
and	O
their	O
way	O
of	O
coping	O
with	O
the	O
heavy-tailed	O
distribution	B
involves	O
arbitrary	O
hacks	O
leading	O
to	O
two	O
di	O
(	O
cid:11	O
)	O
erent	O
estimators	O
of	O
the	O
mutation	O
rate	B
.	O
one	O
of	O
these	O
estimators	O
(	O
based	O
on	O
the	O
mean	B
number	O
of	O
mutated	O
bacteria	O
,	O
averaging	O
over	O
several	O
experiments	O
)	O
has	O
appallingly	O
large	O
variance	B
,	O
yet	O
sampling	O
theorists	O
continue	O
to	O
use	O
it	O
and	O
base	O
con	O
(	O
cid:12	O
)	O
dence	O
intervals	B
around	O
it	O
(	O
kepler	O
and	O
oprea	O
,	O
2001	O
)	O
.	O
in	O
this	O
exercise	O
you	O
’	O
ll	O
do	O
the	O
inference	O
right	O
.	O
in	O
each	O
culture	O
,	O
a	O
single	O
bacterium	O
that	O
is	O
not	O
resistant	O
gives	O
rise	O
,	O
after	O
g	O
generations	O
,	O
to	O
n	O
=	O
2g	O
descendants	O
,	O
all	O
clones	O
except	O
for	O
di	O
(	O
cid:11	O
)	O
erences	O
arising	O
from	O
mutations	O
.	O
the	O
(	O
cid:12	O
)	O
nal	O
culture	O
is	O
then	O
exposed	O
to	O
a	O
virus	O
,	O
and	O
the	O
number	O
of	O
resistant	O
bacteria	O
n	O
is	O
measured	O
.	O
according	O
to	O
the	O
now	O
accepted	O
mutation	O
hypothesis	O
,	O
these	O
resistant	O
bacteria	O
got	O
their	O
resistance	O
from	O
random	O
mutations	O
that	O
took	O
place	O
during	O
the	O
growth	O
of	O
the	O
colony	O
.	O
the	O
mutation	B
rate	I
(	O
per	O
cell	O
per	O
generation	O
)	O
,	O
a	O
,	O
is	O
about	O
one	O
in	O
a	O
hundred	O
million	O
.	O
the	O
total	O
number	O
of	O
i=0	O
2i	O
’	O
2g	O
=	O
n	O
.	O
if	O
a	O
bacterium	O
mutates	O
at	O
the	O
ith	O
generation	O
,	O
its	O
descendants	O
all	O
inherit	O
the	O
mutation	O
,	O
and	O
the	O
(	O
cid:12	O
)	O
nal	O
number	O
of	O
resistant	O
bacteria	O
contributed	O
by	O
that	O
one	O
ancestor	O
is	O
2g	O
(	O
cid:0	O
)	O
i.	O
opportunities	O
to	O
mutate	O
is	O
n	O
,	O
sincepg	O
(	O
cid:0	O
)	O
1	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
35.3	O
:	O
inferring	O
causation	O
447	O
given	O
m	O
separate	O
experiments	O
,	O
in	O
each	O
of	O
which	O
a	O
colony	O
of	O
size	O
n	O
is	O
m=1	O
,	O
created	O
,	O
and	O
where	O
the	O
measured	O
numbers	O
of	O
resistant	O
bacteria	O
are	O
fnmgm	O
what	O
can	O
we	O
infer	O
about	O
the	O
mutation	B
rate	I
,	O
a	O
?	O
make	O
the	O
inference	B
given	O
the	O
following	O
dataset	O
from	O
luria	O
and	O
delbr	O
(	O
cid:127	O
)	O
uck	O
,	O
for	O
n	O
=	O
2:4	O
(	O
cid:2	O
)	O
108	O
:	O
fnmg	O
=	O
f1	O
;	O
0	O
;	O
3	O
;	O
0	O
;	O
0	O
;	O
5	O
;	O
0	O
;	O
5	O
;	O
0	O
;	O
6	O
;	O
107	O
;	O
0	O
;	O
0	O
;	O
0	O
;	O
1	O
;	O
0	O
;	O
0	O
;	O
64	O
;	O
0	O
;	O
35g	O
.	O
[	O
a	O
small	O
amount	O
of	O
computation	O
is	O
required	O
to	O
solve	O
this	O
problem	O
.	O
]	O
35.3	O
inferring	O
causation	O
exercise	O
35.5	O
.	O
[	O
2	O
,	O
p.450	O
]	O
in	O
the	O
bayesian	O
graphical	O
model	B
community	O
,	O
the	O
task	O
of	O
inferring	O
which	O
way	O
the	O
arrows	O
point	O
{	O
that	O
is	O
,	O
which	O
nodes	O
are	O
parents	O
,	O
and	O
which	O
children	O
{	O
is	O
one	O
on	O
which	O
much	O
has	O
been	O
written	O
.	O
inferring	O
causation	O
is	O
tricky	O
because	O
of	O
‘	O
likelihood	B
equivalence	I
’	O
.	O
two	O
graph-	O
ical	O
models	O
are	O
likelihood-equivalent	O
if	O
for	O
any	O
setting	O
of	O
the	O
parameters	O
of	O
either	O
,	O
there	O
exists	O
a	O
setting	O
of	O
the	O
parameters	O
of	O
the	O
other	O
such	O
that	O
the	O
two	O
joint	B
probability	O
distributions	O
of	O
all	O
observables	O
are	O
identical	O
.	O
an	O
example	O
of	O
a	O
pair	O
of	O
likelihood-equivalent	O
models	O
are	O
a	O
!	O
b	O
and	O
b	O
!	O
a.	O
the	O
model	B
a	O
!	O
b	O
asserts	O
that	O
a	O
is	O
the	O
parent	B
of	O
b	O
,	O
or	O
,	O
in	O
very	O
sloppy	O
terminology	B
,	O
‘	O
a	O
causes	O
b	O
’	O
.	O
an	O
example	O
of	O
a	O
situation	O
where	O
‘	O
b	O
!	O
a	O
’	O
is	O
true	O
is	O
the	O
case	O
where	O
b	O
is	O
the	O
variable	O
‘	O
burglar	O
in	O
house	O
’	O
and	O
a	O
is	O
the	O
variable	O
‘	O
alarm	O
is	O
ringing	O
’	O
.	O
here	O
it	O
is	O
literally	O
true	O
that	O
b	O
causes	O
a.	O
but	O
this	O
choice	O
of	O
words	O
is	O
confusing	O
if	O
applied	O
to	O
another	O
example	O
,	O
r	O
!	O
d	O
,	O
where	O
r	O
denotes	O
‘	O
it	O
rained	O
this	O
morning	O
’	O
and	O
d	O
denotes	O
‘	O
the	O
pavement	O
is	O
dry	O
’	O
.	O
‘	O
r	O
causes	O
d	O
’	O
is	O
confusing	O
.	O
i	O
’	O
ll	O
therefore	O
use	O
the	O
words	O
‘	O
b	O
is	O
a	O
parent	B
of	O
a	O
’	O
to	O
denote	O
causation	O
.	O
some	O
statistical	B
meth-	O
ods	O
that	O
use	O
the	O
likelihood	B
alone	O
are	O
unable	O
to	O
use	O
data	O
to	O
distinguish	O
between	O
likelihood-equivalent	O
models	O
.	O
in	O
a	O
bayesian	O
approach	O
,	O
on	O
the	O
other	O
hand	O
,	O
two	O
likelihood-equivalent	O
models	O
may	O
nevertheless	O
be	O
somewhat	O
distinguished	O
,	O
in	O
the	O
light	O
of	O
data	O
,	O
since	O
likelihood-equivalence	O
does	O
not	O
force	O
a	O
bayesian	O
to	O
use	O
priors	O
that	O
assign	O
equivalent	O
densities	O
over	O
the	O
two	O
parameter	O
spaces	O
of	O
the	O
models	O
.	O
however	O
,	O
many	O
bayesian	O
graphical	O
modelling	B
folks	O
,	O
perhaps	O
out	O
of	O
sym-	O
pathy	O
for	O
their	O
non-bayesian	O
colleagues	O
,	O
or	O
from	O
a	O
latent	O
urge	O
not	O
to	O
appear	O
di	O
(	O
cid:11	O
)	O
erent	O
from	O
them	O
,	O
deliberately	O
discard	O
this	O
potential	O
advantage	O
of	O
bayesian	O
methods	B
{	O
the	O
ability	O
to	O
infer	O
causation	O
from	O
data	O
{	O
by	O
skewing	O
their	O
models	O
so	O
that	O
the	O
ability	O
goes	O
away	O
;	O
a	O
widespread	O
orthodoxy	O
holds	O
that	O
one	O
should	O
identify	O
the	O
choices	O
of	O
prior	O
for	O
which	O
‘	O
prior	B
equivalence	I
’	O
holds	O
,	O
i.e.	O
,	O
the	O
priors	O
such	O
that	O
models	O
that	O
are	O
likelihood-equivalent	O
also	O
have	O
identical	O
posterior	O
probabilities	O
;	O
and	O
then	O
one	O
should	O
use	O
one	O
of	O
those	O
priors	O
in	O
inference	O
and	O
prediction	O
.	O
this	O
argument	O
motivates	O
the	O
use	O
,	O
as	O
the	O
prior	B
over	O
all	O
probability	B
vectors	O
,	O
of	O
specially-constructed	O
dirichlet	O
distributions	O
.	O
in	O
my	O
view	O
it	O
is	O
a	O
philosophical	O
error	O
to	O
use	O
only	O
those	O
priors	O
such	O
that	O
causation	O
can	O
not	O
be	O
inferred	O
.	O
priors	O
should	O
be	O
set	B
to	O
describe	O
one	O
’	O
s	O
assump-	O
tions	O
;	O
when	O
this	O
is	O
done	O
,	O
it	O
’	O
s	O
likely	O
that	O
interesting	O
inferences	O
about	O
causation	O
can	O
be	O
made	O
from	O
data	O
.	O
in	O
this	O
exercise	O
,	O
you	O
’	O
ll	O
make	O
an	O
example	O
of	O
such	O
an	O
inference	B
.	O
consider	O
the	O
toy	O
problem	O
where	O
a	O
and	O
b	O
are	O
binary	O
variables	O
.	O
the	O
two	O
models	O
are	O
ha	O
!	O
b	O
and	O
hb	O
!	O
a	O
.	O
ha	O
!	O
b	O
asserts	O
that	O
the	O
marginal	B
probabil-	O
ity	O
of	O
a	O
comes	O
from	O
a	O
beta	B
distribution	I
with	O
parameters	B
(	O
1	O
;	O
1	O
)	O
,	O
i.e.	O
,	O
the	O
uni-	O
form	O
distribution	B
;	O
and	O
that	O
the	O
two	O
conditional	B
distributions	O
p	O
(	O
bj	O
a	O
=	O
0	O
)	O
and	O
p	O
(	O
bj	O
a	O
=	O
1	O
)	O
also	O
come	O
independently	O
from	O
beta	O
distributions	O
with	O
parameters	O
(	O
1	O
;	O
1	O
)	O
.	O
the	O
other	O
model	B
assigns	O
similar	O
priors	O
to	O
the	O
marginal	B
probability	I
of	O
b	O
and	O
the	O
conditional	B
distributions	O
of	O
a	O
given	O
b.	O
data	O
are	O
gathered	O
,	O
and	O
the	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
448	O
35	O
|	O
random	B
inference	O
topics	O
counts	O
,	O
given	O
f	O
=	O
1000	O
outcomes	O
,	O
are	O
b	O
=	O
0	O
b	O
=	O
1	O
a	O
=	O
0	O
a	O
=	O
1	O
760	O
190	O
950	O
5	O
45	O
50	O
765	O
235	O
(	O
35.4	O
)	O
what	O
are	O
the	O
posterior	O
probabilities	O
of	O
the	O
two	O
hypotheses	O
?	O
it	O
’	O
s	O
a	O
good	B
idea	O
to	O
work	O
this	O
exercise	O
out	O
symbolically	O
in	O
order	O
to	O
spot	O
hint	O
:	O
all	O
the	O
simpli	O
(	O
cid:12	O
)	O
cations	O
that	O
emerge	O
.	O
(	O
cid:9	O
)	O
(	O
x	O
)	O
=	O
d	O
dx	O
ln	O
(	O
cid:0	O
)	O
(	O
x	O
)	O
’	O
ln	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
1	O
2x	O
+	O
o	O
(	O
1=x2	O
)	O
:	O
(	O
35.5	O
)	O
the	O
topic	O
of	O
inferring	O
causation	O
is	O
a	O
complex	B
one	O
.	O
the	O
fact	O
that	O
bayesian	O
inference	B
can	O
sensibly	O
be	O
used	O
to	O
infer	O
the	O
directions	O
of	O
arrows	O
in	O
graphs	O
seems	O
to	O
be	O
a	O
neglected	O
view	O
,	O
but	O
it	O
is	O
certainly	O
not	O
the	O
whole	O
story	O
.	O
see	O
pearl	O
(	O
2000	O
)	O
for	O
discussion	O
of	O
many	O
other	O
aspects	O
of	O
causality	O
.	O
35.4	O
further	O
exercises	O
exercise	O
35.6	O
.	O
[	O
3	O
]	O
photons	O
arriving	O
at	O
a	O
photon	O
detector	O
are	O
believed	O
to	O
be	O
emit-	O
ted	O
as	O
a	O
poisson	O
process	O
with	O
a	O
time-varying	O
rate	B
,	O
(	O
cid:21	O
)	O
(	O
t	O
)	O
=	O
exp	O
(	O
a	O
+	O
b	O
sin	O
(	O
!	O
t	O
+	O
(	O
cid:30	O
)	O
)	O
)	O
;	O
(	O
35.6	O
)	O
where	O
the	O
parameters	B
a	O
,	O
b	O
,	O
!	O
,	O
and	O
(	O
cid:30	O
)	O
are	O
known	O
.	O
data	O
are	O
collected	O
during	O
the	O
time	O
t	O
=	O
0	O
:	O
:	O
:	O
t	O
.	O
given	O
that	O
n	O
photons	O
arrived	O
at	O
times	O
ftngn	O
n=1	O
,	O
[	O
further	O
reading	O
:	O
gregory	O
and	O
discuss	O
the	O
inference	B
of	O
a	O
,	O
b	O
,	O
!	O
,	O
and	O
(	O
cid:30	O
)	O
.	O
loredo	O
(	O
1992	O
)	O
.	O
]	O
.	O
exercise	O
35.7	O
.	O
[	O
2	O
]	O
a	O
data	O
(	O
cid:12	O
)	O
le	O
consisting	O
of	O
two	O
columns	O
of	O
numbers	O
has	O
been	O
printed	O
in	O
such	O
a	O
way	O
that	O
the	O
boundaries	O
between	O
the	O
columns	O
are	O
unclear	O
.	O
here	O
are	O
the	O
resulting	O
strings	O
.	O
891.10.0	O
903.10.0	O
924.20.0	O
849.20.0	O
898.20.0	O
966.20.0	O
950.20.0	O
923.50.0	O
912.20.0	O
937.10.0	O
861.10.0	O
891.10.0	O
924.10.0	O
908.10.0	O
911.10.0	O
874.10.0	O
850.20.0	O
899.20.0	O
916.20.0	O
950.20.0	O
924.20.0	O
913.20.0	O
870.20.0	O
916.20.0	O
849.10.0	O
891.10.0	O
958.10.0	O
983.10.0	O
921.25.0	O
836.10.0	O
899.10.0	O
887.20.0	O
912.20.0	O
971.20.0	O
924.20.0	O
912.20.0	O
861.20.0	O
907.10.0	O
840.10.0	O
875.10.0	O
933.10.0	O
908.10.0	O
917.30.0	O
discuss	O
how	O
probable	O
it	O
is	O
,	O
given	O
these	O
data	O
,	O
that	O
the	O
correct	O
parsing	O
of	O
each	O
item	O
is	O
:	O
(	O
a	O
)	O
891:10:0	O
!	O
891	O
:	O
10:0	O
,	O
etc	O
.	O
(	O
b	O
)	O
891:10:0	O
!	O
891:1	O
0:0	O
,	O
etc	O
.	O
[	O
a	O
parsing	O
of	O
a	O
string	O
is	O
a	O
grammatical	O
interpretation	O
of	O
the	O
string	O
.	O
for	O
example	O
,	O
‘	O
punch	O
bores	O
’	O
could	O
be	O
parsed	O
as	O
‘	O
punch	O
(	O
noun	O
)	O
bores	O
(	O
verb	O
)	O
’	O
,	O
or	O
‘	O
punch	O
(	O
imperative	O
verb	O
)	O
bores	O
(	O
plural	O
noun	O
)	O
’	O
.	O
]	O
.	O
exercise	O
35.8	O
.	O
[	O
2	O
]	O
in	O
an	O
experiment	O
,	O
the	O
measured	O
quantities	O
fxng	O
come	O
inde-	O
pendently	O
from	O
a	O
biexponential	B
distribution	O
with	O
mean	O
(	O
cid:22	O
)	O
,	O
p	O
(	O
xj	O
(	O
cid:22	O
)	O
)	O
=	O
1	O
z	O
exp	O
(	O
(	O
cid:0	O
)	O
jx	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
j	O
)	O
;	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
35.5	O
:	O
solutions	O
449	O
where	O
z	O
is	O
the	O
normalizing	B
constant	I
,	O
z	O
=	O
2.	O
the	O
mean	B
(	O
cid:22	O
)	O
is	O
not	O
known	O
.	O
an	O
example	O
of	O
this	O
distribution	B
,	O
with	O
(	O
cid:22	O
)	O
=	O
1	O
,	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
35.2.	O
assuming	O
the	O
four	O
datapoints	O
are	O
-3	O
-2	O
-1	O
0	O
1	O
2	O
3	O
figure	O
35.2.	O
the	O
biexponential	B
distribution	O
p	O
(	O
xj	O
(	O
cid:22	O
)	O
=	O
1	O
)	O
.	O
fxng	O
=	O
f0	O
;	O
0:9	O
;	O
2	O
;	O
6g	O
;	O
what	O
do	O
these	O
data	O
tell	O
us	O
about	O
(	O
cid:22	O
)	O
?	O
include	O
detailed	O
sketches	O
in	O
your	O
answer	O
.	O
give	O
a	O
range	O
of	O
plausible	O
values	O
of	O
(	O
cid:22	O
)	O
.	O
0	O
3	O
1	O
2	O
6	O
7	O
4	O
5	O
8	O
35.5	O
solutions	O
solution	O
to	O
exercise	O
35.4	O
(	O
p.446	O
)	O
.	O
a	O
population	O
of	O
size	O
n	O
has	O
n	O
opportunities	O
to	O
mutate	O
.	O
the	O
probability	O
of	O
the	O
number	O
of	O
mutations	O
that	O
occurred	O
,	O
r	O
,	O
is	O
roughly	O
poisson	O
p	O
(	O
r	O
j	O
a	O
;	O
n	O
)	O
=	O
e	O
(	O
cid:0	O
)	O
an	O
(	O
an	O
)	O
r	O
r	O
!	O
:	O
(	O
35.7	O
)	O
(	O
this	O
is	O
slightly	O
inaccurate	O
because	O
the	O
descendants	O
of	O
a	O
mutant	O
can	O
not	O
them-	O
selves	O
undergo	O
the	O
same	O
mutation	O
.	O
)	O
each	O
mutation	O
gives	O
rise	O
to	O
a	O
number	O
of	O
(	O
cid:12	O
)	O
nal	O
mutant	O
cells	O
ni	O
that	O
depends	O
on	O
the	O
generation	O
time	O
of	O
the	O
mutation	O
.	O
if	O
multiplication	O
went	O
like	O
clockwork	O
then	O
the	O
probability	O
of	O
ni	O
being	O
1	O
would	O
be	O
1=2	O
,	O
the	O
probability	O
of	O
2	O
would	O
be	O
1=4	O
,	O
the	O
probability	O
of	O
4	O
would	O
be	O
1=8	O
,	O
and	O
p	O
(	O
ni	O
)	O
=	O
1=	O
(	O
2n	O
)	O
for	O
all	O
ni	O
that	O
are	O
powers	O
of	O
two	O
.	O
but	O
we	O
don	O
’	O
t	O
expect	O
the	O
mutant	O
progeny	O
to	O
divide	O
in	O
exact	O
synchrony	O
,	O
and	O
we	O
don	O
’	O
t	O
know	O
the	O
pre-	O
cise	O
timing	B
of	O
the	O
end	O
of	O
the	O
experiment	O
compared	O
to	O
the	O
division	O
times	O
.	O
a	O
smoothed	O
version	O
of	O
this	O
distribution	B
that	O
permits	O
all	O
integers	O
to	O
occur	O
is	O
p	O
(	O
ni	O
)	O
=	O
1	O
z	O
1	O
n2	O
i	O
;	O
(	O
35.8	O
)	O
where	O
z	O
=	O
(	O
cid:25	O
)	O
2=6	O
=	O
1:645	O
.	O
[	O
this	O
distribution	B
’	O
s	O
moments	O
are	O
all	O
wrong	O
,	O
since	O
ni	O
can	O
never	O
exceed	O
n	O
,	O
but	O
who	O
cares	O
about	O
moments	O
?	O
{	O
only	O
sampling	B
theory	I
statisticians	O
who	O
are	O
barking	O
up	O
the	O
wrong	O
tree	B
,	O
constructing	O
‘	O
unbiased	O
estimators	O
’	O
such	O
as	O
^a	O
=	O
(	O
(	O
cid:22	O
)	O
n=n	O
)	O
=	O
log	O
n	O
.	O
the	O
error	O
that	O
we	O
introduce	O
in	O
the	O
likelihood	B
function	O
by	O
using	O
the	O
approximation	B
to	O
p	O
(	O
ni	O
)	O
is	O
negligible	O
.	O
]	O
the	O
observed	O
number	O
of	O
mutants	O
n	O
is	O
the	O
sum	O
n	O
=	O
ni	O
:	O
r	O
xi=1	O
(	O
35.9	O
)	O
the	O
probability	B
distribution	O
of	O
n	O
given	O
r	O
is	O
the	O
convolution	B
of	O
r	O
identical	O
distributions	O
of	O
the	O
form	O
(	O
35.8	O
)	O
.	O
for	O
example	O
,	O
p	O
(	O
nj	O
r	O
=	O
2	O
)	O
=	O
n	O
(	O
cid:0	O
)	O
1	O
xn1=1	O
1	O
z	O
2	O
1	O
n2	O
1	O
1	O
(	O
n	O
(	O
cid:0	O
)	O
n1	O
)	O
2	O
for	O
n	O
(	O
cid:21	O
)	O
2	O
:	O
(	O
35.10	O
)	O
the	O
probability	B
distribution	O
of	O
n	O
given	O
a	O
,	O
which	O
is	O
what	O
we	O
need	O
for	O
the	O
bayesian	O
inference	B
,	O
is	O
given	O
by	O
summing	O
over	O
r.	O
p	O
(	O
nj	O
a	O
)	O
=	O
n	O
xr=0	O
p	O
(	O
nj	O
r	O
)	O
p	O
(	O
r	O
j	O
a	O
;	O
n	O
)	O
:	O
(	O
35.11	O
)	O
this	O
quantity	O
can	O
’	O
t	O
be	O
evaluated	O
analytically	O
,	O
but	O
for	O
small	O
a	O
,	O
it	O
’	O
s	O
easy	O
to	O
evaluate	O
to	O
any	O
desired	O
numerical	O
precision	B
by	O
explicitly	O
summing	O
over	O
r	O
from	O
 	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
450	O
35	O
|	O
random	B
inference	O
topics	O
r	O
=	O
0	O
to	O
some	O
rmax	O
,	O
with	O
p	O
(	O
nj	O
r	O
)	O
also	O
being	O
found	O
for	O
each	O
r	O
by	O
rmax	O
explicit	O
convolutions	O
for	O
all	O
required	O
values	O
of	O
n	O
;	O
if	O
rmax	O
=	O
nmax	O
,	O
the	O
largest	O
value	O
of	O
n	O
encountered	O
in	O
the	O
data	O
,	O
then	O
p	O
(	O
nj	O
a	O
)	O
is	O
computed	O
exactly	O
;	O
but	O
for	O
this	O
question	O
’	O
s	O
data	O
,	O
rmax	O
=	O
9	O
is	O
plenty	O
for	O
an	O
accurate	O
result	O
;	O
i	O
used	O
rmax	O
=	O
74	O
to	O
make	O
the	O
graphs	O
in	O
(	O
cid:12	O
)	O
gure	O
35.3.	O
octave	B
source	O
code	B
is	O
available.1	O
incidentally	O
,	O
for	O
data	O
sets	O
like	O
the	O
one	O
in	O
this	O
exercise	O
,	O
which	O
have	O
a	O
substantial	O
number	O
of	O
zero	O
counts	O
,	O
very	O
little	O
is	O
lost	O
by	O
making	O
luria	O
and	O
delbruck	O
’	O
s	O
second	O
approximation	B
,	O
which	O
is	O
to	O
retain	O
only	O
the	O
count	O
of	O
how	O
many	O
n	O
were	O
equal	O
to	O
zero	O
,	O
and	O
how	O
many	O
were	O
non-zero	O
.	O
the	O
likelihood	B
function	O
found	O
using	O
this	O
weakened	O
data	B
set	I
,	O
l	O
(	O
a	O
)	O
=	O
(	O
e	O
(	O
cid:0	O
)	O
an	O
)	O
11	O
(	O
1	O
(	O
cid:0	O
)	O
e	O
(	O
cid:0	O
)	O
an	O
)	O
9	O
;	O
(	O
35.12	O
)	O
is	O
scarcely	O
distinguishable	O
from	O
the	O
likelihood	B
computed	O
using	O
full	O
information	O
.	O
solution	O
to	O
exercise	O
35.5	O
(	O
p.447	O
)	O
.	O
from	O
the	O
six	B
terms	O
of	O
the	O
form	O
p	O
(	O
fj	O
(	O
cid:11	O
)	O
m	O
)	O
=	O
qi	O
(	O
cid:0	O
)	O
(	O
fi	O
+	O
(	O
cid:11	O
)	O
mi	O
)	O
(	O
cid:0	O
)	O
(	O
pi	O
fi	O
+	O
(	O
cid:11	O
)	O
)	O
most	O
factors	O
cancel	O
and	O
all	O
that	O
remains	O
is	O
;	O
(	O
cid:0	O
)	O
(	O
(	O
cid:11	O
)	O
)	O
qi	O
(	O
cid:0	O
)	O
(	O
(	O
cid:11	O
)	O
mi	O
)	O
p	O
(	O
ha	O
!	O
b	O
j	O
data	O
)	O
p	O
(	O
hb	O
!	O
a	O
j	O
data	O
)	O
=	O
(	O
765	O
+	O
1	O
)	O
(	O
235	O
+	O
1	O
)	O
(	O
950	O
+	O
1	O
)	O
(	O
50	O
+	O
1	O
)	O
=	O
3:8	O
1	O
:	O
(	O
35.13	O
)	O
(	O
35.14	O
)	O
there	O
is	O
modest	O
evidence	B
in	O
favour	O
of	O
ha	O
!	O
b	O
because	O
the	O
three	O
probabilities	O
inferred	O
for	O
that	O
hypothesis	O
(	O
roughly	O
0.95	O
,	O
0.8	O
,	O
and	O
0.1	O
)	O
are	O
more	O
typical	B
of	O
the	O
prior	B
than	O
are	O
the	O
three	O
probabilities	O
inferred	O
for	O
the	O
other	O
(	O
0.24	O
,	O
0.008	O
,	O
and	O
0.19	O
)	O
.	O
this	O
statement	O
sounds	O
absurd	O
if	O
we	O
think	O
of	O
the	O
priors	O
as	O
‘	O
uniform	O
’	O
over	O
the	O
three	O
probabilities	O
{	O
surely	O
,	O
under	O
a	O
uniform	O
prior	B
,	O
any	O
settings	O
of	O
the	O
probabilities	O
are	O
equally	O
probable	O
?	O
but	O
in	O
the	O
natural	B
basis	O
,	O
the	O
logit	B
basis	O
,	O
the	O
prior	B
is	O
proportional	O
to	O
p	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
,	O
and	O
the	O
posterior	B
probability	I
ratio	O
can	O
be	O
estimated	O
by	O
0:95	O
(	O
cid:2	O
)	O
0:05	O
(	O
cid:2	O
)	O
0:8	O
(	O
cid:2	O
)	O
0:2	O
(	O
cid:2	O
)	O
0:1	O
(	O
cid:2	O
)	O
0:9	O
0:24	O
(	O
cid:2	O
)	O
0:76	O
(	O
cid:2	O
)	O
0:008	O
(	O
cid:2	O
)	O
0:992	O
(	O
cid:2	O
)	O
0:19	O
(	O
cid:2	O
)	O
0:81	O
’	O
3	O
1	O
;	O
(	O
35.15	O
)	O
which	O
is	O
not	O
exactly	O
right	O
,	O
but	O
it	O
does	O
illustrate	O
where	O
the	O
preference	O
for	O
a	O
!	O
b	O
is	O
coming	O
from	O
.	O
1.2	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
1e-10	O
1	O
0.01	O
0.0001	O
1e-06	O
1e-08	O
1e-09	O
1e-08	O
1e-07	O
1e-10	O
1e-10	O
1e-09	O
1e-08	O
1e-07	O
figure	O
35.3.	O
likelihood	B
of	O
the	O
mutation	B
rate	I
a	O
on	O
a	O
linear	B
scale	O
and	O
log	O
scale	O
,	O
given	O
luria	O
and	O
delbruck	O
’	O
s	O
data	O
.	O
vertical	O
axis	O
:	O
likelihood/10	O
(	O
cid:0	O
)	O
23	O
;	O
horizontal	O
axis	O
:	O
a	O
.	O
1www.inference.phy.cam.ac.uk/itprnn/code/octave/luria0.m	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
36	O
decision	B
theory	I
decision	O
theory	B
is	O
trivial	O
,	O
apart	O
from	O
computational	O
details	O
(	O
just	O
like	O
playing	O
chess	B
!	O
)	O
.	O
you	O
have	O
a	O
choice	O
of	O
various	O
actions	O
,	O
a.	O
the	O
world	O
may	O
be	O
in	O
one	O
of	O
many	O
states	O
x	O
;	O
which	O
one	O
occurs	O
may	O
be	O
in	O
(	O
cid:13	O
)	O
uenced	O
by	O
your	O
action	O
.	O
the	O
world	O
’	O
s	O
state	O
has	O
a	O
probability	B
distribution	O
p	O
(	O
xj	O
a	O
)	O
.	O
finally	O
,	O
there	O
is	O
a	O
utility	B
function	O
u	O
(	O
x	O
;	O
a	O
)	O
which	O
speci	O
(	O
cid:12	O
)	O
es	O
the	O
payo	O
(	O
cid:11	O
)	O
you	O
receive	O
when	O
the	O
world	O
is	O
in	O
state	O
x	O
and	O
you	O
chose	O
action	O
a.	O
the	O
task	O
of	O
decision	O
theory	B
is	O
to	O
select	O
the	O
action	O
that	O
maximizes	O
the	O
expected	O
utility	B
,	O
e	O
[	O
u	O
j	O
a	O
]	O
=z	O
dkx	O
u	O
(	O
x	O
;	O
a	O
)	O
p	O
(	O
xj	O
a	O
)	O
:	O
(	O
36.1	O
)	O
that	O
’	O
s	O
all	O
.	O
the	O
computational	O
problem	O
is	O
to	O
maximize	O
e	O
[	O
u	O
j	O
a	O
]	O
over	O
a	O
.	O
[	O
pes-	O
simists	O
may	O
prefer	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
loss	B
function	I
l	O
instead	O
of	O
a	O
utility	B
function	O
u	O
and	O
minimize	O
the	O
expected	O
loss	O
.	O
]	O
is	O
there	O
anything	O
more	O
to	O
be	O
said	O
about	O
decision	B
theory	I
?	O
well	O
,	O
in	O
a	O
real	O
problem	O
,	O
the	O
choice	O
of	O
an	O
appropriate	O
utility	B
function	O
may	O
be	O
quite	O
di	O
(	O
cid:14	O
)	O
cult	O
.	O
furthermore	O
,	O
when	O
a	O
sequence	B
of	O
actions	O
is	O
to	O
be	O
taken	O
,	O
with	O
each	O
action	O
providing	O
information	B
about	O
x	O
,	O
we	O
have	O
to	O
take	O
into	O
account	O
the	O
e	O
(	O
cid:11	O
)	O
ect	O
that	O
this	O
anticipated	O
information	B
may	O
have	O
on	O
our	O
subsequent	O
ac-	O
tions	O
.	O
the	O
resulting	O
mixture	O
of	O
forward	O
probability	B
and	O
inverse	B
probability	I
computations	O
in	O
a	O
decision	O
problem	O
is	O
distinctive	O
.	O
in	O
a	O
realistic	O
problem	O
such	O
as	O
playing	O
a	O
board	O
game	B
,	O
the	O
tree	B
of	O
possible	O
cogitations	O
and	O
actions	O
that	O
must	O
be	O
considered	O
becomes	O
enormous	O
,	O
and	O
‘	O
doing	O
the	O
right	O
thing	O
’	O
is	O
not	O
simple	O
,	O
because	O
the	O
expected	O
utility	B
of	O
an	O
action	O
can	O
not	O
be	O
computed	O
exactly	O
(	O
russell	O
and	O
wefald	O
,	O
1991	O
;	O
baum	O
and	O
smith	O
,	O
1993	O
;	O
baum	O
and	O
smith	O
,	O
1997	O
)	O
.	O
let	O
’	O
s	O
explore	B
an	O
example	O
.	O
36.1	O
rational	O
prospecting	B
suppose	O
you	O
have	O
the	O
task	O
of	O
choosing	O
the	O
site	O
for	O
a	O
tanzanite	O
mine	O
.	O
your	O
(	O
cid:12	O
)	O
nal	O
action	O
will	O
be	O
to	O
select	O
the	O
site	O
from	O
a	O
list	O
of	O
n	O
sites	O
.	O
the	O
nth	O
site	O
has	O
a	O
net	O
value	O
called	O
the	O
return	O
xn	O
which	O
is	O
initially	O
unknown	O
,	O
and	O
will	O
be	O
found	O
out	O
exactly	O
only	O
after	O
site	O
n	O
has	O
been	O
chosen	O
.	O
[	O
xn	O
equals	O
the	O
revenue	O
earned	O
from	O
selling	O
the	O
tanzanite	O
from	O
that	O
site	O
,	O
minus	O
the	O
costs	O
of	O
buying	O
the	O
site	O
,	O
paying	O
the	O
sta	O
(	O
cid:11	O
)	O
,	O
and	O
so	O
forth	O
.	O
]	O
at	O
the	O
outset	O
,	O
the	O
return	O
xn	O
has	O
a	O
probability	B
distribution	O
p	O
(	O
xn	O
)	O
,	O
based	O
on	O
the	O
information	B
already	O
available	O
.	O
before	O
you	O
take	O
your	O
(	O
cid:12	O
)	O
nal	O
action	O
you	O
have	O
the	O
opportunity	O
to	O
do	O
some	O
prospecting	B
.	O
prospecting	B
at	O
the	O
nth	O
site	O
has	O
a	O
cost	O
cn	O
and	O
yields	O
data	O
dn	O
which	O
reduce	O
the	O
uncertainty	O
about	O
xn	O
.	O
[	O
we	O
’	O
ll	O
assume	O
that	O
the	O
returns	O
of	O
451	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
452	O
36	O
|	O
decision	B
theory	I
the	O
n	O
sites	O
are	O
unrelated	O
to	O
each	O
other	O
,	O
and	O
that	O
prospecting	B
at	O
one	O
site	O
only	O
yields	O
information	B
about	O
that	O
site	O
and	O
doesn	O
’	O
t	O
a	O
(	O
cid:11	O
)	O
ect	O
the	O
return	O
from	O
that	O
site	O
.	O
]	O
your	O
decision	O
problem	O
is	O
:	O
given	O
the	O
initial	O
probability	B
distributions	I
p	O
(	O
x1	O
)	O
,	O
p	O
(	O
x2	O
)	O
,	O
.	O
.	O
.	O
,	O
p	O
(	O
xn	O
)	O
,	O
(	O
cid:12	O
)	O
rst	O
,	O
decide	O
whether	O
to	O
prospect	O
,	O
and	O
at	O
which	O
sites	O
;	O
then	O
,	O
in	O
the	O
light	O
of	O
your	O
prospecting	B
results	O
,	O
choose	O
which	O
site	O
to	O
mine	O
.	O
for	O
simplicity	O
,	O
let	O
’	O
s	O
make	O
everything	O
in	O
the	O
problem	O
gaussian	O
and	O
focus	O
on	O
the	O
question	O
of	O
whether	O
to	O
prospect	O
once	O
or	O
not	O
.	O
we	O
’	O
ll	O
assume	O
our	O
utility	B
function	O
is	O
linear	B
in	O
xn	O
;	O
we	O
wish	O
to	O
maximize	O
our	O
expected	O
return	O
.	O
the	O
utility	B
function	O
is	O
the	O
notation	B
p	O
(	O
y	O
)	O
=	O
normal	B
(	O
y	O
;	O
(	O
cid:22	O
)	O
;	O
(	O
cid:27	O
)	O
2	O
)	O
indicates	O
that	O
y	O
has	O
gaussian	O
distribution	B
with	O
mean	B
(	O
cid:22	O
)	O
and	O
variance	O
(	O
cid:27	O
)	O
2.	O
if	O
no	O
prospecting	B
is	O
done	O
,	O
where	O
na	O
is	O
the	O
chosen	O
‘	O
action	O
’	O
site	O
;	O
and	O
,	O
if	O
prospect-	O
ing	O
is	O
done	O
,	O
the	O
utility	B
is	O
u	O
=	O
xna	O
;	O
(	O
36.2	O
)	O
where	O
np	O
is	O
the	O
site	O
at	O
which	O
prospecting	B
took	O
place	O
.	O
u	O
=	O
(	O
cid:0	O
)	O
cnp	O
+	O
xna	O
;	O
the	O
prior	B
distribution	O
of	O
the	O
return	O
of	O
site	O
n	O
is	O
p	O
(	O
xn	O
)	O
=	O
normal	B
(	O
xn	O
;	O
(	O
cid:22	O
)	O
n	O
;	O
(	O
cid:27	O
)	O
2	O
n	O
)	O
:	O
if	O
you	O
prospect	O
at	O
site	O
n	O
,	O
the	O
datum	O
dn	O
is	O
a	O
noisy	B
version	O
of	O
xn	O
:	O
p	O
(	O
dn	O
j	O
xn	O
)	O
=	O
normal	B
(	O
dn	O
;	O
xn	O
;	O
(	O
cid:27	O
)	O
2	O
)	O
:	O
(	O
36.3	O
)	O
(	O
36.4	O
)	O
(	O
36.5	O
)	O
.	O
exercise	O
36.1	O
.	O
[	O
2	O
]	O
given	O
these	O
assumptions	B
,	O
show	O
that	O
the	O
prior	B
probability	O
dis-	O
tribution	O
of	O
dn	O
is	O
p	O
(	O
dn	O
)	O
=	O
normal	B
(	O
dn	O
;	O
(	O
cid:22	O
)	O
n	O
;	O
(	O
cid:27	O
)	O
2	O
+	O
(	O
cid:27	O
)	O
2	O
n	O
)	O
(	O
36.6	O
)	O
(	O
mnemonic	O
:	O
when	O
independent	O
variables	O
add	O
,	O
variances	B
add	I
)	O
,	O
and	O
that	O
the	O
posterior	O
distribution	O
of	O
xn	O
given	O
dn	O
is	O
where	O
p	O
(	O
xn	O
j	O
dn	O
)	O
=	O
normal	B
(	O
cid:16	O
)	O
xn	O
;	O
(	O
cid:22	O
)	O
0n	O
;	O
(	O
cid:27	O
)	O
2	O
n0	O
(	O
cid:17	O
)	O
1	O
1	O
n0	O
=	O
(	O
cid:27	O
)	O
2	O
+	O
(	O
cid:27	O
)	O
2	O
(	O
mnemonic	O
:	O
when	O
gaussians	O
multiply	O
,	O
precisions	B
add	I
)	O
.	O
dn=	O
(	O
cid:27	O
)	O
2	O
+	O
(	O
cid:22	O
)	O
n=	O
(	O
cid:27	O
)	O
2	O
n	O
1=	O
(	O
cid:27	O
)	O
2	O
+	O
1=	O
(	O
cid:27	O
)	O
2	O
n	O
(	O
cid:22	O
)	O
0n	O
=	O
and	O
(	O
36.7	O
)	O
(	O
36.8	O
)	O
1	O
(	O
cid:27	O
)	O
2	O
n	O
to	O
start	O
with	O
,	O
let	O
’	O
s	O
evaluate	O
the	O
expected	O
utility	B
if	O
we	O
do	O
no	O
prospecting	B
(	O
i.e.	O
,	O
choose	O
the	O
site	O
immediately	O
)	O
;	O
then	O
we	O
’	O
ll	O
evaluate	O
the	O
expected	O
utility	B
if	O
we	O
(	O
cid:12	O
)	O
rst	O
prospect	O
at	O
one	O
site	O
and	O
then	O
make	O
our	O
choice	O
.	O
from	O
these	O
two	O
results	O
we	O
will	O
be	O
able	O
to	O
decide	O
whether	O
to	O
prospect	O
once	O
or	O
zero	O
times	O
,	O
and	O
,	O
if	O
we	O
prospect	O
once	O
,	O
at	O
which	O
site	O
.	O
so	O
,	O
(	O
cid:12	O
)	O
rst	O
we	O
consider	O
the	O
expected	O
utility	B
without	O
any	O
prospecting	B
.	O
exercise	O
36.2	O
.	O
[	O
2	O
]	O
show	O
that	O
the	O
optimal	B
action	O
,	O
assuming	O
no	O
prospecting	B
,	O
is	O
to	O
select	O
the	O
site	O
with	O
biggest	O
mean	B
and	O
the	O
expected	O
utility	B
of	O
this	O
action	O
is	O
na	O
=	O
argmax	O
n	O
(	O
cid:22	O
)	O
n	O
;	O
e	O
[	O
u	O
j	O
optimal	B
n	O
]	O
=	O
max	O
n	O
(	O
cid:22	O
)	O
n	O
:	O
(	O
36.9	O
)	O
(	O
36.10	O
)	O
[	O
if	O
your	O
intuition	O
says	O
‘	O
surely	O
the	O
optimal	B
decision	O
should	O
take	O
into	O
ac-	O
count	O
the	O
di	O
(	O
cid:11	O
)	O
erent	O
uncertainties	O
(	O
cid:27	O
)	O
n	O
too	O
?	O
’	O
,	O
the	O
answer	O
to	O
this	O
question	O
is	O
‘	O
reasonable	O
{	O
if	O
so	O
,	O
then	O
the	O
utility	B
function	O
should	O
be	O
nonlinear	B
in	O
x	O
’	O
.	O
]	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
36.2	O
:	O
further	O
reading	O
453	O
now	O
the	O
exciting	O
bit	B
.	O
should	O
we	O
prospect	O
?	O
once	O
we	O
have	O
prospected	O
at	O
site	O
np	O
,	O
we	O
will	O
choose	O
the	O
site	O
using	O
the	O
decision	O
rule	O
(	O
36.9	O
)	O
with	O
the	O
value	O
of	O
mean	O
(	O
cid:22	O
)	O
np	O
replaced	O
by	O
the	O
updated	O
value	O
(	O
cid:22	O
)	O
0n	O
given	O
by	O
(	O
36.8	O
)	O
.	O
what	O
makes	O
the	O
problem	O
exciting	O
is	O
that	O
we	O
don	O
’	O
t	O
yet	O
know	O
the	O
value	O
of	O
dn	O
,	O
so	O
we	O
don	O
’	O
t	O
know	O
what	O
our	O
action	O
na	O
will	O
be	O
;	O
indeed	O
the	O
whole	O
value	O
of	O
doing	O
the	O
prospecting	B
comes	O
from	O
the	O
fact	O
that	O
the	O
outcome	O
dn	O
may	O
alter	O
the	O
action	O
from	O
the	O
one	O
that	O
we	O
would	O
have	O
taken	O
in	O
the	O
absence	O
of	O
the	O
experimental	O
information	B
.	O
from	O
the	O
expression	O
for	O
the	O
new	O
mean	B
in	O
terms	O
of	O
dn	O
(	O
36.8	O
)	O
,	O
and	O
the	O
known	O
variance	B
of	O
dn	O
(	O
36.6	O
)	O
,	O
we	O
can	O
compute	O
the	O
probability	B
distribution	O
of	O
the	O
key	O
quantity	O
,	O
(	O
cid:22	O
)	O
0n	O
,	O
and	O
can	O
work	O
out	O
the	O
expected	O
utility	B
by	O
integrating	O
over	O
all	O
possible	O
outcomes	O
and	O
their	O
associated	O
actions	O
.	O
exercise	O
36.3	O
.	O
[	O
2	O
]	O
show	O
that	O
the	O
probability	B
distribution	O
of	O
the	O
new	O
mean	B
(	O
cid:22	O
)	O
0n	O
(	O
36.8	O
)	O
is	O
gaussian	O
with	O
mean	O
(	O
cid:22	O
)	O
n	O
and	O
variance	O
s2	O
(	O
cid:17	O
)	O
(	O
cid:27	O
)	O
2	O
n	O
(	O
cid:27	O
)	O
2	O
n	O
(	O
cid:27	O
)	O
2	O
+	O
(	O
cid:27	O
)	O
2	O
n	O
:	O
(	O
36.11	O
)	O
consider	O
prospecting	B
at	O
site	O
n.	O
let	O
the	O
biggest	O
mean	B
of	O
the	O
other	O
sites	O
be	O
(	O
cid:22	O
)	O
1.	O
when	O
we	O
obtain	O
the	O
new	O
value	O
of	O
the	O
mean	O
,	O
(	O
cid:22	O
)	O
0n	O
,	O
we	O
will	O
choose	O
site	O
n	O
and	O
get	O
an	O
expected	O
return	O
of	O
(	O
cid:22	O
)	O
0n	O
if	O
(	O
cid:22	O
)	O
0n	O
>	O
(	O
cid:22	O
)	O
1	O
,	O
and	O
we	O
will	O
choose	O
site	O
1	O
and	O
get	O
an	O
expected	O
return	O
of	O
(	O
cid:22	O
)	O
1	O
if	O
(	O
cid:22	O
)	O
0n	O
<	O
(	O
cid:22	O
)	O
1.	O
so	O
the	O
expected	O
utility	B
of	O
prospecting	B
at	O
site	O
n	O
,	O
then	O
picking	O
the	O
best	O
site	O
,	O
is	O
e	O
[	O
u	O
j	O
prospect	O
at	O
n	O
]	O
=	O
(	O
cid:0	O
)	O
cn	O
+	O
p	O
(	O
(	O
cid:22	O
)	O
0n	O
<	O
(	O
cid:22	O
)	O
1	O
)	O
(	O
cid:22	O
)	O
1	O
+z	O
1	O
(	O
cid:22	O
)	O
1	O
d	O
(	O
cid:22	O
)	O
0n	O
(	O
cid:22	O
)	O
0n	O
normal	B
(	O
(	O
cid:22	O
)	O
0n	O
;	O
(	O
cid:22	O
)	O
n	O
;	O
s2	O
)	O
:	O
(	O
36.12	O
)	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
in	O
utility	O
between	O
prospecting	B
and	O
not	O
prospecting	B
is	O
the	O
quantity	O
of	O
interest	O
,	O
and	O
it	O
depends	O
on	O
what	O
we	O
would	O
have	O
done	O
without	O
prospecting	B
;	O
and	O
that	O
depends	O
on	O
whether	O
(	O
cid:22	O
)	O
1	O
is	O
bigger	O
than	O
(	O
cid:22	O
)	O
n.	O
e	O
[	O
u	O
j	O
no	O
prospecting	B
]	O
=	O
(	O
cid:26	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
n	O
if	O
(	O
cid:22	O
)	O
1	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
n	O
if	O
(	O
cid:22	O
)	O
1	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
n	O
:	O
(	O
36.13	O
)	O
so	O
-6	O
-4	O
-2	O
0	O
2	O
4	O
(	O
(	O
cid:22	O
)	O
n	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
1	O
)	O
(	O
cid:27	O
)	O
n	O
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
6	O
e	O
[	O
u	O
j	O
prospect	O
at	O
n	O
]	O
(	O
cid:0	O
)	O
e	O
[	O
u	O
j	O
no	O
prospecting	B
]	O
d	O
(	O
cid:22	O
)	O
0n	O
(	O
(	O
cid:22	O
)	O
0n	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
1	O
)	O
normal	B
(	O
(	O
cid:22	O
)	O
0n	O
;	O
(	O
cid:22	O
)	O
n	O
;	O
s2	O
)	O
d	O
(	O
cid:22	O
)	O
0n	O
(	O
(	O
cid:22	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
0n	O
)	O
normal	B
(	O
(	O
cid:22	O
)	O
0n	O
;	O
(	O
cid:22	O
)	O
n	O
;	O
s2	O
)	O
if	O
(	O
cid:22	O
)	O
1	O
(	O
cid:21	O
)	O
(	O
cid:22	O
)	O
n	O
if	O
(	O
cid:22	O
)	O
1	O
(	O
cid:20	O
)	O
(	O
cid:22	O
)	O
n	O
:	O
(	O
36.14	O
)	O
(	O
cid:0	O
)	O
cn	O
+z	O
1	O
(	O
cid:0	O
)	O
cn	O
+z	O
(	O
cid:22	O
)	O
1	O
(	O
cid:0	O
)	O
1	O
(	O
cid:22	O
)	O
1	O
=	O
8	O
>	O
>	O
<	O
>	O
>	O
:	O
we	O
can	O
plot	O
the	O
change	O
in	O
expected	O
utility	B
due	O
to	O
prospecting	B
(	O
omitting	O
cn	O
)	O
as	O
a	O
function	B
of	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
(	O
(	O
cid:22	O
)	O
n	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
1	O
)	O
(	O
horizontal	O
axis	O
)	O
and	O
the	O
initial	O
standard	B
deviation	I
(	O
cid:27	O
)	O
n	O
(	O
vertical	O
axis	O
)	O
.	O
in	O
the	O
(	O
cid:12	O
)	O
gure	O
the	O
noise	B
variance	O
is	O
(	O
cid:27	O
)	O
2	O
=	O
1	O
.	O
36.2	O
further	O
reading	O
if	O
the	O
world	O
in	O
which	O
we	O
act	O
is	O
a	O
little	O
more	O
complicated	O
than	O
the	O
prospecting	B
problem	O
{	O
for	O
example	O
,	O
if	O
multiple	O
iterations	O
of	O
prospecting	O
are	O
possible	O
,	O
and	O
the	O
cost	O
of	O
prospecting	O
is	O
uncertain	O
{	O
then	O
(	O
cid:12	O
)	O
nding	O
the	O
optimal	B
balance	O
between	O
exploration	O
and	O
exploitation	O
becomes	O
a	O
much	O
harder	O
computational	O
problem	O
.	O
reinforcement	B
learning	I
addresses	O
approximate	O
methods	B
for	O
this	O
problem	O
(	O
sut-	O
ton	O
and	O
barto	O
,	O
1998	O
)	O
.	O
figure	O
36.1.	O
contour	O
plot	O
of	O
the	O
gain	O
in	O
expected	O
utility	B
due	O
to	O
prospecting	B
.	O
the	O
contours	O
are	O
equally	O
spaced	O
from	O
0.1	O
to	O
1.2	O
in	O
steps	O
of	O
0.1.	O
to	O
decide	O
whether	O
it	O
is	O
worth	O
prospecting	B
at	O
site	O
n	O
,	O
(	O
cid:12	O
)	O
nd	O
the	O
contour	O
equal	O
to	O
cn	O
(	O
the	O
cost	O
of	O
prospecting	O
)	O
;	O
all	O
points	O
[	O
(	O
(	O
cid:22	O
)	O
n	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
1	O
)	O
;	O
(	O
cid:27	O
)	O
n	O
]	O
above	O
that	O
contour	O
are	O
worthwhile	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
454	O
36	O
|	O
decision	B
theory	I
36.3	O
further	O
exercises	O
.	O
exercise	O
36.4	O
.	O
[	O
2	O
]	O
the	O
four	O
doors	B
problem	O
.	O
a	O
new	O
game	B
show	I
uses	O
rules	B
similar	O
to	O
those	O
of	O
the	O
three	O
doors	B
(	O
exer-	O
cise	O
3.8	O
(	O
p.57	O
)	O
)	O
,	O
but	O
there	O
are	O
four	O
doors	B
,	O
and	O
the	O
host	O
explains	O
:	O
‘	O
first	O
you	O
will	O
point	O
to	O
one	O
of	O
the	O
doors	O
,	O
and	O
then	O
i	O
will	O
open	O
one	O
of	O
the	O
other	O
doors	B
,	O
guaranteeing	O
to	O
choose	O
a	O
non-winner	O
.	O
then	O
you	O
decide	O
whether	O
to	O
stick	O
with	O
your	O
original	O
pick	O
or	O
switch	O
to	O
one	O
of	O
the	O
remaining	O
doors	B
.	O
then	O
i	O
will	O
open	O
another	O
non-winner	O
(	O
but	O
never	O
the	O
current	O
pick	O
)	O
.	O
you	O
will	O
then	O
make	O
your	O
(	O
cid:12	O
)	O
nal	O
decision	O
by	O
sticking	O
with	O
the	O
door	O
picked	O
on	O
the	O
previous	O
decision	O
or	O
by	O
switching	O
to	O
the	O
only	O
other	O
remaining	O
door.	O
’	O
what	O
is	O
the	O
optimal	B
strategy	O
?	O
should	O
you	O
switch	O
on	O
the	O
(	O
cid:12	O
)	O
rst	O
opportu-	O
nity	O
?	O
should	O
you	O
switch	O
on	O
the	O
second	O
opportunity	O
?	O
.	O
exercise	O
36.5	O
.	O
[	O
3	O
]	O
one	O
of	O
the	O
challenges	O
of	O
decision	O
theory	B
is	O
(	O
cid:12	O
)	O
guring	O
out	O
ex-	O
actly	O
what	O
the	O
utility	B
function	O
is	O
.	O
the	O
utility	B
of	O
money	O
,	O
for	O
example	O
,	O
is	O
notoriously	O
nonlinear	B
for	O
most	O
people	O
.	O
in	O
fact	O
,	O
the	O
behaviour	O
of	O
many	O
people	O
can	O
not	O
be	O
captured	O
by	O
a	O
coher-	O
ent	O
utility	B
function	O
,	O
as	O
illustrated	O
by	O
the	O
allais	O
paradox	B
,	O
which	O
runs	O
as	O
follows	O
.	O
which	O
of	O
these	O
choices	O
do	O
you	O
(	O
cid:12	O
)	O
nd	O
most	O
attractive	O
?	O
a	O
.	O
$	O
1	O
million	O
guaranteed	O
.	O
b	O
.	O
89	O
%	O
chance	O
of	O
$	O
1	O
million	O
;	O
10	O
%	O
chance	O
of	O
$	O
2.5	O
million	O
;	O
1	O
%	O
chance	O
of	O
nothing	O
.	O
now	O
consider	O
these	O
choices	O
:	O
c.	O
d.	O
89	O
%	O
chance	O
of	O
nothing	O
;	O
11	O
%	O
chance	O
of	O
$	O
1	O
million	O
.	O
90	O
%	O
chance	O
of	O
nothing	O
;	O
10	O
%	O
chance	O
of	O
$	O
2.5	O
million	O
.	O
many	O
people	O
prefer	O
a	O
to	O
b	O
,	O
and	O
,	O
at	O
the	O
same	O
time	O
,	O
d	O
to	O
c.	O
prove	O
that	O
these	O
preferences	O
are	O
inconsistent	O
with	O
any	O
utility	B
function	O
u	O
(	O
x	O
)	O
for	O
money	O
.	O
exercise	O
36.6	O
.	O
[	O
4	O
]	O
optimal	B
stopping	I
.	O
a	O
large	O
queue	B
of	O
n	O
potential	O
partners	O
is	O
waiting	O
at	O
your	O
door	O
,	O
all	O
asking	O
to	O
marry	O
you	O
.	O
they	O
have	O
arrived	O
in	O
random	O
order	O
.	O
as	O
you	O
meet	O
each	O
partner	O
,	O
you	O
have	O
to	O
decide	O
on	O
the	O
spot	O
,	O
based	O
on	O
the	O
information	B
so	O
far	O
,	O
whether	O
to	O
marry	O
them	O
or	O
say	O
no	O
.	O
each	O
potential	O
partner	O
has	O
a	O
desirability	O
dn	O
,	O
which	O
you	O
(	O
cid:12	O
)	O
nd	O
out	O
if	O
and	O
when	O
you	O
meet	O
them	O
.	O
you	O
must	O
marry	O
one	O
of	O
them	O
,	O
but	O
you	O
are	O
not	O
allowed	O
to	O
go	O
back	O
to	O
anyone	O
you	O
have	O
said	O
no	O
to	O
.	O
there	O
are	O
several	O
ways	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
precise	O
problem	O
.	O
(	O
a	O
)	O
assuming	O
your	O
aim	O
is	O
to	O
maximize	O
the	O
desirability	O
dn	O
,	O
i.e.	O
,	O
your	O
utility	B
function	O
is	O
d^n	O
,	O
where	O
^n	O
is	O
the	O
partner	O
selected	O
,	O
what	O
strategy	O
should	O
you	O
use	O
?	O
(	O
b	O
)	O
assuming	O
you	O
wish	O
very	O
much	O
to	O
marry	O
the	O
most	O
desirable	O
person	O
(	O
i.e.	O
,	O
your	O
utility	B
function	O
is	O
1	O
if	O
you	O
achieve	O
that	O
,	O
and	O
zero	O
other-	O
wise	O
)	O
;	O
what	O
strategy	O
should	O
you	O
use	O
?	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
36.3	O
:	O
further	O
exercises	O
455	O
action	O
buy	O
don	O
’	O
t	O
buy	O
outcome	O
no	O
win	O
(	O
cid:0	O
)	O
1	O
wins	O
+9	O
0	O
0	O
table	O
36.2.	O
utility	B
in	O
the	O
lottery	O
ticket	O
problem	O
.	O
action	O
buy	O
don	O
’	O
t	O
buy	O
outcome	O
no	O
win	O
wins	O
1	O
0	O
0	O
9	O
table	O
36.3.	O
regret	B
in	O
the	O
lottery	O
ticket	O
problem	O
.	O
(	O
c	O
)	O
assuming	O
you	O
wish	O
very	O
much	O
to	O
marry	O
the	O
most	O
desirable	O
person	O
,	O
and	O
that	O
your	O
strategy	O
will	O
be	O
‘	O
strategy	O
m	O
’	O
:	O
strategy	O
m	O
{	O
meet	O
the	O
(	O
cid:12	O
)	O
rst	O
m	O
partners	O
and	O
say	O
no	O
to	O
all	O
of	O
them	O
.	O
memorize	O
the	O
maximum	O
desirability	O
dmax	O
among	O
them	O
.	O
then	O
meet	O
the	O
others	O
in	O
sequence	O
,	O
waiting	O
until	O
a	O
partner	O
with	O
dn	O
>	O
dmax	O
comes	O
along	O
,	O
and	O
marry	O
them	O
.	O
if	O
none	O
more	O
desirable	O
comes	O
along	O
,	O
marry	O
the	O
(	O
cid:12	O
)	O
nal	O
n	O
th	O
partner	O
(	O
and	O
feel	O
miserable	O
)	O
.	O
{	O
what	O
is	O
the	O
optimal	B
value	O
of	O
m	O
?	O
exercise	O
36.7	O
.	O
[	O
3	O
]	O
regret	B
as	O
an	O
objective	B
function	I
?	O
the	O
preceding	O
exercise	O
(	O
parts	O
b	O
and	O
c	O
)	O
involved	O
a	O
utility	B
function	O
based	O
on	O
regret	O
.	O
if	O
one	O
married	O
the	O
tenth	O
most	O
desirable	O
candidate	O
,	O
the	O
utility	B
function	O
asserts	O
that	O
one	O
would	O
feel	O
regret	B
for	O
having	O
not	O
chosen	O
the	O
most	O
desirable	O
.	O
many	O
people	O
working	O
in	O
learning	O
theory	B
and	O
decision	B
theory	I
use	O
‘	O
mini-	O
mizing	O
the	O
maximal	O
possible	O
regret	B
’	O
as	O
an	O
objective	B
function	I
,	O
but	O
does	O
this	O
make	O
sense	O
?	O
imagine	O
that	O
fred	O
has	O
bought	O
a	O
lottery	O
ticket	O
,	O
and	O
o	O
(	O
cid:11	O
)	O
ers	O
to	O
sell	O
it	O
to	O
you	O
before	O
it	O
’	O
s	O
known	O
whether	O
the	O
ticket	O
is	O
a	O
winner	O
.	O
for	O
simplicity	O
say	O
the	O
probability	B
that	O
the	O
ticket	O
is	O
a	O
winner	O
is	O
1=100	O
,	O
and	O
if	O
it	O
is	O
a	O
winner	O
,	O
it	O
is	O
worth	O
$	O
10	O
.	O
fred	O
o	O
(	O
cid:11	O
)	O
ers	O
to	O
sell	O
you	O
the	O
ticket	O
for	O
$	O
1	O
.	O
do	O
you	O
buy	O
it	O
?	O
the	O
possible	O
actions	O
are	O
‘	O
buy	O
’	O
and	O
‘	O
don	O
’	O
t	O
buy	O
’	O
.	O
the	O
utilities	O
of	O
the	O
four	O
possible	O
action	O
{	O
outcome	O
pairs	O
are	O
shown	O
in	O
table	O
36.2.	O
i	O
have	O
assumed	O
that	O
the	O
utility	B
of	O
small	O
amounts	O
of	O
money	O
for	O
you	O
is	O
linear	B
.	O
if	O
you	O
don	O
’	O
t	O
buy	O
the	O
ticket	O
then	O
the	O
utility	B
is	O
zero	O
regardless	O
of	O
whether	O
the	O
ticket	O
proves	O
to	O
be	O
a	O
winner	O
.	O
if	O
you	O
do	O
buy	O
the	O
ticket	O
you	O
end	O
up	O
either	O
losing	O
one	O
pound	O
(	O
with	O
probability	O
99=100	O
)	O
or	O
gaining	O
nine	O
(	O
with	O
probability	O
1=100	O
)	O
.	O
in	O
the	O
minimax	B
regret	O
community	O
,	O
actions	O
are	O
chosen	O
to	O
mini-	O
mize	O
the	O
maximum	O
possible	O
regret	B
.	O
the	O
four	O
possible	O
regret	B
outcomes	O
are	O
shown	O
in	O
table	O
36.3.	O
if	O
you	O
buy	O
the	O
ticket	O
and	O
it	O
doesn	O
’	O
t	O
win	O
,	O
you	O
have	O
a	O
regret	B
of	O
$	O
1	O
,	O
because	O
if	O
you	O
had	O
not	O
bought	O
it	O
you	O
would	O
have	O
been	O
$	O
1	O
better	O
o	O
(	O
cid:11	O
)	O
.	O
if	O
you	O
do	O
not	O
buy	O
the	O
ticket	O
and	O
it	O
wins	O
,	O
you	O
have	O
a	O
regret	B
of	O
$	O
9	O
,	O
because	O
if	O
you	O
had	O
bought	O
it	O
you	O
would	O
have	O
been	O
$	O
9	O
better	O
o	O
(	O
cid:11	O
)	O
.	O
the	O
action	O
that	O
minimizes	O
the	O
maximum	O
possible	O
regret	B
is	O
thus	O
to	O
buy	O
the	O
ticket	O
.	O
discuss	O
whether	O
this	O
use	O
of	O
regret	O
to	O
choose	O
actions	O
can	O
be	O
philosophi-	O
cally	O
justi	O
(	O
cid:12	O
)	O
ed	O
.	O
the	O
above	O
problem	O
can	O
be	O
turned	O
into	O
an	O
investment	B
portfolio	I
decision	O
problem	O
by	O
imagining	O
that	O
you	O
have	O
been	O
given	O
one	O
pound	O
to	O
invest	O
in	O
two	O
possible	O
funds	O
for	O
one	O
day	O
:	O
fred	O
’	O
s	O
lottery	O
fund	O
,	O
and	O
the	O
cash	O
fund	O
.	O
if	O
you	O
put	O
$	O
f1	O
into	O
fred	O
’	O
s	O
lottery	O
fund	O
,	O
fred	O
promises	O
to	O
return	O
$	O
9f1	O
to	O
you	O
if	O
the	O
lottery	O
ticket	O
is	O
a	O
winner	O
,	O
and	O
otherwise	O
nothing	O
.	O
the	O
remaining	O
$	O
f0	O
(	O
with	O
f0	O
=	O
1	O
(	O
cid:0	O
)	O
f1	O
)	O
is	O
kept	O
as	O
cash	O
.	O
what	O
is	O
the	O
best	O
investment	O
?	O
show	O
that	O
the	O
minimax	B
regret	O
community	O
will	O
invest	O
f1	O
=	O
9=10	O
of	O
their	O
money	O
in	O
the	O
high	O
risk	O
,	O
high	O
return	O
lottery	O
fund	O
,	O
and	O
only	O
f0	O
=	O
1=10	O
in	O
cash	O
.	O
can	O
this	O
investment	O
method	O
be	O
justi	O
(	O
cid:12	O
)	O
ed	O
?	O
exercise	O
36.8	O
.	O
[	O
3	O
]	O
gambling	B
oddities	O
(	O
from	O
cover	O
and	O
thomas	O
(	O
1991	O
)	O
)	O
.	O
a	O
horse	B
race	I
involving	O
i	O
horses	O
occurs	O
repeatedly	O
,	O
and	O
you	O
are	O
obliged	O
to	O
bet	B
all	O
your	O
money	O
each	O
time	O
.	O
your	O
bet	B
at	O
time	O
t	O
can	O
be	O
represented	O
by	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
456	O
36	O
|	O
decision	B
theory	I
a	O
normalized	O
probability	B
vector	O
b	O
multiplied	O
by	O
your	O
money	O
m	O
(	O
t	O
)	O
.	O
the	O
odds	B
o	O
(	O
cid:11	O
)	O
ered	O
by	O
the	O
bookies	B
are	O
such	O
that	O
if	O
horse	O
i	O
wins	O
then	O
your	O
return	O
is	O
m	O
(	O
t+1	O
)	O
=	O
bioim	O
(	O
t	O
)	O
.	O
assuming	O
the	O
bookies	B
’	O
odds	B
are	O
‘	O
fair	O
’	O
,	O
that	O
is	O
,	O
=	O
1	O
;	O
1	O
oi	O
xi	O
(	O
36.15	O
)	O
and	O
assuming	O
that	O
the	O
probability	B
that	O
horse	O
i	O
wins	O
is	O
pi	O
,	O
work	O
out	O
the	O
optimal	B
betting	O
strategy	O
if	O
your	O
aim	O
is	O
cover	O
’	O
s	O
aim	O
,	O
namely	O
,	O
to	O
maximize	O
the	O
expected	O
value	O
of	O
log	O
m	O
(	O
t	O
)	O
.	O
show	O
that	O
the	O
optimal	B
strategy	O
sets	O
b	O
equal	O
to	O
p	O
,	O
independent	O
of	O
the	O
bookies	B
’	O
odds	B
o.	O
show	O
that	O
when	O
this	O
strategy	O
is	O
used	O
,	O
the	O
money	O
is	O
expected	O
to	O
grow	O
exponentially	O
as	O
:	O
2nw	O
(	O
b	O
;	O
p	O
)	O
(	O
36.16	O
)	O
where	O
w	O
=pi	O
pi	O
log	O
bioi	O
.	O
if	O
you	O
only	O
bet	B
once	O
,	O
is	O
the	O
optimal	B
strategy	O
any	O
di	O
(	O
cid:11	O
)	O
erent	O
?	O
do	O
you	O
think	O
this	O
optimal	B
strategy	O
makes	O
sense	O
?	O
do	O
you	O
think	O
that	O
it	O
’	O
s	O
‘	O
optimal	B
’	O
,	O
in	O
common	O
language	O
,	O
to	O
ignore	O
the	O
bookies	B
’	O
odds	B
?	O
what	O
can	O
you	O
conclude	O
about	O
‘	O
cover	O
’	O
s	O
aim	O
’	O
?	O
exercise	O
36.9	O
.	O
[	O
3	O
]	O
two	O
ordinary	O
dice	O
are	O
thrown	O
repeatedly	O
;	O
the	O
outcome	O
of	O
each	O
throw	O
is	O
the	O
sum	O
of	O
the	O
two	O
numbers	O
.	O
joe	O
shark	O
,	O
who	O
says	O
that	O
6	O
and	O
8	O
are	O
his	O
lucky	O
numbers	O
,	O
bets	O
even	O
money	O
that	O
a	O
6	O
will	O
be	O
thrown	O
before	O
the	O
(	O
cid:12	O
)	O
rst	O
7	O
is	O
thrown	O
.	O
if	O
you	O
were	O
a	O
gambler	O
,	O
would	O
you	O
take	O
the	O
bet	B
?	O
what	O
is	O
your	O
probability	O
of	O
winning	O
?	O
joe	O
then	O
bets	O
even	O
money	O
that	O
an	O
8	O
will	O
be	O
thrown	O
before	O
the	O
(	O
cid:12	O
)	O
rst	O
7	O
is	O
thrown	O
.	O
would	O
you	O
take	O
the	O
bet	B
?	O
having	O
gained	O
your	O
con	O
(	O
cid:12	O
)	O
dence	O
,	O
joe	O
suggests	O
combining	O
the	O
two	O
bets	O
into	O
a	O
single	O
bet	O
:	O
he	O
bets	O
a	O
larger	O
sum	O
,	O
still	O
at	O
even	O
odds	B
,	O
that	O
an	O
8	O
and	O
a	O
6	O
will	O
be	O
thrown	O
before	O
two	O
7s	O
have	O
been	O
thrown	O
.	O
would	O
you	O
take	O
the	O
bet	B
?	O
what	O
is	O
your	O
probability	O
of	O
winning	O
?	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
37	O
bayesian	O
inference	B
and	O
sampling	B
theory	I
there	O
are	O
two	O
schools	O
of	O
statistics	O
.	O
sampling	O
theorists	O
concentrate	O
on	O
having	O
methods	B
guaranteed	O
to	O
work	O
most	O
of	O
the	O
time	O
,	O
given	O
minimal	O
assumptions	B
.	O
bayesians	O
try	O
to	O
make	O
inferences	O
that	O
take	O
into	O
account	O
all	O
available	O
informa-	O
tion	O
and	O
answer	O
the	O
question	O
of	O
interest	O
given	O
the	O
particular	O
data	B
set	I
.	O
as	O
you	O
have	O
probably	O
gathered	O
,	O
i	O
strongly	O
recommend	O
the	O
use	O
of	O
bayesian	O
methods	B
.	O
sampling	B
theory	I
is	O
the	O
widely	O
used	O
approach	O
to	O
statistics	O
,	O
and	O
most	O
pa-	O
pers	O
in	O
most	O
journals	O
report	O
their	O
experiments	O
using	O
quantities	O
like	O
con	O
(	O
cid:12	O
)	O
dence	O
intervals	B
,	O
signi	O
(	O
cid:12	O
)	O
cance	O
levels	O
,	O
and	O
p-values	O
.	O
a	O
p-value	B
(	O
e.g	O
.	O
p	O
=	O
0:05	O
)	O
is	O
the	O
prob-	O
ability	O
,	O
given	O
a	O
null	O
hypothesis	O
for	O
the	O
probability	B
distribution	O
of	O
the	O
data	O
,	O
that	O
the	O
outcome	O
would	O
be	O
as	O
extreme	O
as	O
,	O
or	O
more	O
extreme	O
than	O
,	O
the	O
observed	O
out-	O
come	O
.	O
untrained	O
readers	O
{	O
and	O
perhaps	O
,	O
more	O
worryingly	O
,	O
the	O
authors	O
of	O
many	O
papers	O
{	O
usually	O
interpret	O
such	O
a	O
p-value	B
as	O
if	O
it	O
is	O
a	O
bayesian	O
probability	B
(	O
for	O
example	O
,	O
the	O
posterior	B
probability	I
of	O
the	O
null	O
hypothesis	O
)	O
,	O
an	O
interpretation	O
that	O
both	O
sampling	O
theorists	O
and	O
bayesians	O
would	O
agree	O
is	O
incorrect	O
.	O
in	O
this	O
chapter	O
we	O
study	O
a	O
couple	O
of	O
simple	O
inference	B
problems	O
in	O
order	O
to	O
compare	O
these	O
two	O
approaches	O
to	O
statistics	O
.	O
while	O
in	O
some	O
cases	O
,	O
the	O
answers	O
from	O
a	O
bayesian	O
approach	O
and	O
from	O
sam-	O
pling	O
theory	B
are	O
very	O
similar	O
,	O
we	O
can	O
also	O
(	O
cid:12	O
)	O
nd	O
cases	O
where	O
there	O
are	O
signi	O
(	O
cid:12	O
)	O
cant	O
di	O
(	O
cid:11	O
)	O
erences	O
.	O
we	O
have	O
already	O
seen	O
such	O
an	O
example	O
in	O
exercise	O
3.15	O
(	O
p.59	O
)	O
,	O
where	O
a	O
sampling	O
theorist	O
got	O
a	O
p-value	B
smaller	O
than	O
7	O
%	O
,	O
and	O
viewed	O
this	O
as	O
strong	O
evidence	B
against	O
the	O
null	O
hypothesis	O
,	O
whereas	O
the	O
data	O
actually	O
favoured	O
the	O
null	O
hypothesis	O
over	O
the	O
simplest	O
alternative	O
.	O
on	O
p.64	O
,	O
another	O
example	O
was	O
given	O
where	O
the	O
p-value	B
was	O
smaller	O
than	O
the	O
mystical	O
value	O
of	O
5	O
%	O
,	O
yet	O
the	O
data	O
again	O
favoured	O
the	O
null	O
hypothesis	O
.	O
thus	O
in	O
some	O
cases	O
,	O
sampling	B
theory	I
can	O
be	O
trigger-happy	O
,	O
declaring	O
results	O
to	O
be	O
‘	O
su	O
(	O
cid:14	O
)	O
ciently	O
improbable	O
that	O
the	O
null	O
hypothesis	O
should	O
be	O
rejected	O
’	O
,	O
when	O
those	O
results	O
actually	O
weakly	O
sup-	O
port	O
the	O
null	O
hypothesis	O
.	O
as	O
we	O
will	O
now	O
see	O
,	O
there	O
are	O
also	O
inference	B
problems	O
where	O
sampling	B
theory	I
fails	O
to	O
detect	O
‘	O
signi	O
(	O
cid:12	O
)	O
cant	O
’	O
evidence	B
where	O
a	O
bayesian	O
approach	O
and	O
everyday	O
intuition	O
agree	O
that	O
the	O
evidence	B
is	O
strong	O
.	O
most	O
telling	O
of	O
all	O
are	O
the	O
inference	B
problems	O
where	O
the	O
‘	O
signi	O
(	O
cid:12	O
)	O
cance	O
’	O
assigned	O
by	O
sampling	O
theory	B
changes	O
depending	O
on	O
irrelevant	O
factors	O
concerned	O
with	O
the	O
design	O
of	O
the	O
experiment	O
.	O
this	O
chapter	O
is	O
only	O
provided	O
for	O
those	O
readers	O
who	O
are	O
curious	O
about	O
the	O
sampling	B
theory	I
/	O
bayesian	O
methods	B
debate	O
.	O
if	O
you	O
(	O
cid:12	O
)	O
nd	O
any	O
of	O
this	O
chapter	O
tough	O
to	O
understand	O
,	O
please	O
skip	O
it	O
.	O
there	O
is	O
no	O
point	O
trying	O
to	O
understand	O
the	O
debate	O
.	O
just	O
use	O
bayesian	O
methods	B
{	O
they	O
are	O
much	O
easier	O
to	O
understand	O
than	O
the	O
debate	O
itself	O
!	O
457	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
458	O
37	O
|	O
bayesian	O
inference	B
and	O
sampling	B
theory	I
37.1	O
a	O
medical	O
example	O
we	O
are	O
trying	O
to	O
reduce	O
the	O
incidence	O
of	O
an	O
unpleasant	O
disease	B
called	O
microsoftus	B
.	O
two	O
vaccinations	O
,	O
a	O
and	O
b	O
,	O
are	O
tested	O
on	O
a	O
group	O
of	O
volunteers	O
.	O
vaccination	B
b	O
is	O
a	O
control	B
treatment	I
,	O
a	O
placebo	O
treatment	O
with	O
no	O
active	O
ingredients	O
.	O
of	O
the	O
40	O
subjects	O
,	O
30	O
are	O
randomly	O
assigned	O
to	O
have	O
treatment	O
a	O
and	O
the	O
other	O
10	O
are	O
given	O
the	O
control	B
treatment	I
b.	O
we	O
observe	O
the	O
subjects	O
for	O
one	O
year	O
after	O
their	O
vaccinations	O
.	O
of	O
the	O
30	O
in	O
group	O
a	O
,	O
one	O
contracts	O
microsoftus	B
.	O
of	O
the	O
10	O
in	O
group	O
b	O
,	O
three	O
contract	O
microsoftus	B
.	O
is	O
treatment	O
a	O
better	O
than	O
treatment	O
b	O
?	O
sampling	B
theory	I
has	O
a	O
go	O
the	O
standard	O
sampling	O
theory	B
approach	O
to	O
the	O
question	O
‘	O
is	O
a	O
better	O
than	O
b	O
?	O
’	O
is	O
to	O
construct	O
a	O
statistical	B
test	I
.	O
the	O
test	B
usually	O
compares	O
a	O
hypothesis	O
such	O
as	O
h1	O
:	O
‘	O
a	O
and	O
b	O
have	O
di	O
(	O
cid:11	O
)	O
erent	O
e	O
(	O
cid:11	O
)	O
ectivenesses	O
’	O
with	O
a	O
null	O
hypothesis	O
such	O
as	O
h0	O
:	O
‘	O
a	O
and	O
b	O
have	O
exactly	O
the	O
same	O
e	O
(	O
cid:11	O
)	O
ectivenesses	O
as	O
each	O
other	O
’	O
.	O
a	O
novice	O
might	O
object	O
‘	O
no	O
,	O
no	O
,	O
i	O
want	O
to	O
compare	O
the	O
hypothesis	O
\a	O
is	O
better	O
than	O
b	O
''	O
with	O
the	O
alternative	O
\b	O
is	O
better	O
than	O
a	O
''	O
!	O
’	O
but	O
such	O
objections	O
are	O
not	O
welcome	O
in	O
sampling	O
theory	B
.	O
once	O
the	O
two	O
hypotheses	O
have	O
been	O
de	O
(	O
cid:12	O
)	O
ned	O
,	O
the	O
(	O
cid:12	O
)	O
rst	O
hypothesis	O
is	O
scarcely	O
mentioned	O
again	O
{	O
attention	O
focuses	O
solely	O
on	O
the	O
null	O
hypothesis	O
.	O
it	O
makes	O
me	O
laugh	O
to	O
write	O
this	O
,	O
but	O
it	O
’	O
s	O
true	O
!	O
the	O
null	O
hypothesis	O
is	O
accepted	O
or	O
rejected	O
purely	O
on	O
the	O
basis	O
of	O
how	O
unexpected	O
the	O
data	O
were	O
to	O
h0	O
,	O
not	O
on	O
how	O
much	O
better	O
h1	O
predicted	O
the	O
data	O
.	O
one	O
chooses	O
a	O
statistic	B
which	O
measures	O
how	O
much	O
a	O
data	B
set	I
deviates	O
from	O
the	O
null	O
hypothesis	O
.	O
in	O
the	O
example	O
here	O
,	O
the	O
standard	O
statistic	O
to	O
use	O
would	O
be	O
one	O
called	O
(	O
cid:31	O
)	O
2	O
(	O
chi-squared	B
)	O
.	O
to	O
compute	O
(	O
cid:31	O
)	O
2	O
,	O
we	O
take	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
each	O
data	O
measurement	O
and	O
its	O
expected	O
value	O
assuming	O
the	O
null	O
hypothesis	O
to	O
be	O
true	O
,	O
and	O
divide	O
the	O
square	B
of	O
that	O
di	O
(	O
cid:11	O
)	O
erence	O
by	O
the	O
variance	B
of	O
the	O
measurement	O
,	O
assuming	O
the	O
null	O
hypothesis	O
to	O
be	O
true	O
.	O
in	O
the	O
present	O
problem	O
,	O
the	O
four	O
data	O
measurements	O
are	O
the	O
integers	O
fa+	O
,	O
fa	O
(	O
cid:0	O
)	O
,	O
fb+	O
,	O
and	O
fb	O
(	O
cid:0	O
)	O
,	O
that	O
is	O
,	O
the	O
number	O
of	O
subjects	O
given	O
treatment	O
a	O
who	O
contracted	O
microsoftus	B
(	O
fa+	O
)	O
,	O
the	O
number	O
of	O
subjects	O
given	O
treatment	O
a	O
who	O
didn	O
’	O
t	O
(	O
fa	O
(	O
cid:0	O
)	O
)	O
,	O
and	O
so	O
forth	O
.	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
(	O
cid:31	O
)	O
2	O
is	O
:	O
(	O
cid:31	O
)	O
2	O
=xi	O
(	O
fi	O
(	O
cid:0	O
)	O
hfii	O
)	O
2	O
hfii	O
:	O
(	O
37.1	O
)	O
actually	O
,	O
in	O
my	O
elementary	O
statistics	O
book	O
(	O
spiegel	O
,	O
1988	O
)	O
i	O
(	O
cid:12	O
)	O
nd	O
yates	O
’	O
s	O
cor-	O
rection	O
is	O
recommended	O
:	O
(	O
cid:31	O
)	O
2	O
=xi	O
(	O
jfi	O
(	O
cid:0	O
)	O
hfiij	O
(	O
cid:0	O
)	O
0:5	O
)	O
2	O
hfii	O
:	O
(	O
37.2	O
)	O
in	O
this	O
case	O
,	O
given	O
the	O
null	O
hypothesis	O
that	O
treatments	O
a	O
and	O
b	O
are	O
equally	O
e	O
(	O
cid:11	O
)	O
ective	O
,	O
and	O
have	O
rates	O
f+	O
and	O
f	O
(	O
cid:0	O
)	O
for	O
the	O
two	O
outcomes	O
,	O
the	O
expected	O
counts	O
are	O
:	O
hfa+i=f+na	O
hfb+i=f+nb	O
hfa	O
(	O
cid:0	O
)	O
i=	O
f	O
(	O
cid:0	O
)	O
na	O
hfb	O
(	O
cid:0	O
)	O
i=f	O
(	O
cid:0	O
)	O
nb	O
:	O
(	O
37.3	O
)	O
if	O
you	O
want	O
to	O
know	O
about	O
yates	O
’	O
s	O
correction	O
,	O
read	O
a	O
sampling	B
theory	I
textbook	O
.	O
the	O
point	O
of	O
this	O
chapter	O
is	O
not	O
to	O
teach	O
sampling	B
theory	I
;	O
i	O
merely	O
mention	O
yates	O
’	O
s	O
correction	O
because	O
it	O
is	O
what	O
a	O
professional	O
sampling	O
theorist	O
might	O
use	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
37.1	O
:	O
a	O
medical	O
example	O
459	O
the	O
sampling	B
distribution	I
of	O
a	O
statistic	B
is	O
the	O
probability	B
distribution	O
of	O
its	O
value	O
under	O
repetitions	O
of	O
the	O
experiment	O
,	O
assuming	O
that	O
the	O
null	O
hypothesis	O
is	O
true	O
.	O
the	O
test	B
accepts	O
or	O
rejects	O
the	O
null	O
hypothesis	O
on	O
the	O
basis	O
of	O
how	O
big	O
(	O
cid:31	O
)	O
2	O
is	O
.	O
to	O
make	O
this	O
test	B
precise	O
,	O
and	O
give	O
it	O
a	O
‘	O
signi	O
(	O
cid:12	O
)	O
cance	O
level	O
’	O
,	O
we	O
have	O
to	O
work	O
out	O
what	O
the	O
sampling	B
distribution	I
of	O
(	O
cid:31	O
)	O
2	O
is	O
,	O
taking	O
into	O
account	O
the	O
fact	O
that	O
the	O
four	O
data	O
points	O
are	O
not	O
independent	O
(	O
they	O
satisfy	O
the	O
two	O
constraints	O
fa+	O
+	O
fa	O
(	O
cid:0	O
)	O
=	O
na	O
and	O
fb+	O
+	O
fb	O
(	O
cid:0	O
)	O
=	O
nb	O
)	O
and	O
the	O
fact	O
that	O
the	O
parameters	B
f	O
(	O
cid:6	O
)	O
are	O
not	O
known	O
.	O
these	O
three	O
constraints	O
reduce	O
the	O
number	O
of	O
degrees	O
of	O
freedom	O
in	O
the	O
data	O
from	O
four	O
to	O
one	O
.	O
[	O
if	O
you	O
want	O
to	O
learn	O
more	O
about	O
computing	O
the	O
‘	O
number	O
of	O
degrees	O
of	O
freedom	O
’	O
,	O
read	O
a	O
sampling	B
theory	I
book	O
;	O
in	O
bayesian	O
methods	B
we	O
don	O
’	O
t	O
need	O
to	O
know	O
all	O
that	O
,	O
and	O
quantities	O
equivalent	O
to	O
the	O
number	O
of	O
degrees	O
of	O
freedom	O
pop	O
straight	O
out	O
of	O
a	O
bayesian	O
analysis	B
when	O
they	O
are	O
appropriate	O
.	O
]	O
these	O
sampling	O
distributions	O
are	O
tabulated	O
by	O
sampling	O
theory	B
gnomes	O
and	O
come	O
accompanied	O
by	O
warnings	O
about	O
the	O
conditions	O
under	O
which	O
they	O
are	O
accurate	O
.	O
for	O
example	O
,	O
standard	O
tabulated	O
distributions	O
for	O
(	O
cid:31	O
)	O
2	O
are	O
only	O
accurate	O
if	O
the	O
expected	O
numbers	O
fi	O
are	O
about	O
5	O
or	O
more	O
.	O
once	O
the	O
data	O
arrive	O
,	O
sampling	O
theorists	O
estimate	O
the	O
unknown	O
parameters	O
f	O
(	O
cid:6	O
)	O
of	O
the	O
null	O
hypothesis	O
from	O
the	O
data	O
:	O
^f+	O
=	O
fa+	O
+	O
fb+	O
na	O
+	O
nb	O
;	O
^f	O
(	O
cid:0	O
)	O
=	O
fa	O
(	O
cid:0	O
)	O
+	O
fb	O
(	O
cid:0	O
)	O
na	O
+	O
nb	O
;	O
(	O
37.4	O
)	O
and	O
evaluate	O
(	O
cid:31	O
)	O
2.	O
at	O
this	O
point	O
,	O
the	O
sampling	B
theory	I
school	O
divides	O
itself	O
into	O
two	O
camps	O
.	O
one	O
camp	O
uses	O
the	O
following	O
protocol	B
:	O
(	O
cid:12	O
)	O
rst	O
,	O
before	O
looking	O
at	O
the	O
data	O
,	O
pick	O
the	O
signi	O
(	O
cid:12	O
)	O
cance	O
level	O
of	O
the	O
test	O
(	O
e.g	O
.	O
5	O
%	O
)	O
,	O
and	O
determine	O
the	O
critical	O
value	O
of	O
(	O
cid:31	O
)	O
2	O
above	O
which	O
the	O
null	O
hypothesis	O
will	O
be	O
rejected	O
.	O
(	O
the	O
signi	O
(	O
cid:12	O
)	O
cance	O
level	O
is	O
the	O
fraction	O
of	O
times	O
that	O
the	O
statistic	B
(	O
cid:31	O
)	O
2	O
would	O
exceed	O
the	O
critical	O
value	O
,	O
if	O
the	O
null	O
hypothesis	O
were	O
true	O
.	O
)	O
then	O
evaluate	O
(	O
cid:31	O
)	O
2	O
,	O
compare	O
with	O
the	O
critical	O
value	O
,	O
and	O
declare	O
the	O
outcome	O
of	O
the	O
test	O
,	O
and	O
its	O
signi	O
(	O
cid:12	O
)	O
cance	O
level	O
(	O
which	O
was	O
(	O
cid:12	O
)	O
xed	O
beforehand	O
)	O
.	O
the	O
second	O
camp	O
looks	O
at	O
the	O
data	O
,	O
(	O
cid:12	O
)	O
nds	O
(	O
cid:31	O
)	O
2	O
,	O
then	O
looks	O
in	O
the	O
table	O
of	O
(	O
cid:31	O
)	O
2-distributions	O
for	O
the	O
signi	O
(	O
cid:12	O
)	O
cance	O
level	O
,	O
p	O
,	O
for	O
which	O
the	O
observed	O
value	O
of	O
(	O
cid:31	O
)	O
2	O
would	O
be	O
the	O
critical	O
value	O
.	O
the	O
result	O
of	O
the	O
test	O
is	O
then	O
reported	O
by	O
giving	O
this	O
value	O
of	O
p	O
,	O
which	O
is	O
the	O
fraction	O
of	O
times	O
that	O
a	O
result	O
as	O
extreme	O
as	O
the	O
one	O
observed	O
,	O
or	O
more	O
extreme	O
,	O
would	O
be	O
expected	O
to	O
arise	O
if	O
the	O
null	O
hypothesis	O
were	O
true	O
.	O
let	O
’	O
s	O
apply	O
these	O
two	O
methods	B
.	O
first	O
camp	O
:	O
cance	O
level	O
.	O
the	O
critical	O
value	O
for	O
(	O
cid:31	O
)	O
2	O
with	O
one	O
degree	B
of	O
freedom	O
is	O
(	O
cid:31	O
)	O
2	O
the	O
estimated	O
values	O
of	O
f	O
(	O
cid:6	O
)	O
are	O
let	O
’	O
s	O
pick	O
5	O
%	O
as	O
our	O
signi	O
(	O
cid:12	O
)	O
-	O
0:05	O
=	O
3:84.	O
f	O
(	O
cid:0	O
)	O
=	O
9=10	O
:	O
the	O
expected	O
values	O
of	O
the	O
four	O
measurements	O
are	O
f+	O
=	O
1=10	O
;	O
hfa+i	O
=	O
3	O
hfa	O
(	O
cid:0	O
)	O
i	O
=	O
27	O
hfb+i	O
=	O
1	O
hfb	O
(	O
cid:0	O
)	O
i	O
=	O
9	O
(	O
37.5	O
)	O
(	O
37.6	O
)	O
(	O
37.7	O
)	O
(	O
37.8	O
)	O
(	O
37.9	O
)	O
and	O
(	O
cid:31	O
)	O
2	O
(	O
as	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
equation	O
(	O
37.1	O
)	O
)	O
is	O
(	O
cid:31	O
)	O
2	O
=	O
5:93	O
:	O
(	O
37.10	O
)	O
since	O
this	O
value	O
exceeds	O
3.84	O
,	O
we	O
reject	O
the	O
null	O
hypothesis	O
that	O
the	O
two	O
treat-	O
ments	O
are	O
equivalent	O
at	O
the	O
0.05	O
signi	O
(	O
cid:12	O
)	O
cance	O
level	O
.	O
however	O
,	O
if	O
we	O
use	O
yates	O
’	O
s	O
correction	O
,	O
we	O
(	O
cid:12	O
)	O
nd	O
(	O
cid:31	O
)	O
2	O
=	O
3:33	O
,	O
and	O
therefore	O
accept	O
the	O
null	O
hypothesis	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
460	O
37	O
|	O
bayesian	O
inference	B
and	O
sampling	B
theory	I
camp	O
two	O
runs	O
a	O
(	O
cid:12	O
)	O
nger	O
across	O
the	O
(	O
cid:31	O
)	O
2	O
table	O
found	O
at	O
the	O
back	O
of	O
any	O
good	O
:10	O
and	O
:10	O
=	O
2:71.	O
interpolating	O
between	O
(	O
cid:31	O
)	O
2	O
sampling	B
theory	I
book	O
and	O
(	O
cid:12	O
)	O
nds	O
(	O
cid:31	O
)	O
2	O
(	O
cid:31	O
)	O
2	O
:05	O
,	O
camp	O
two	O
reports	O
‘	O
the	O
p-value	B
is	O
p	O
=	O
0:07	O
’	O
.	O
notice	O
that	O
this	O
answer	O
does	O
not	O
say	O
how	O
much	O
more	O
e	O
(	O
cid:11	O
)	O
ective	O
a	O
is	O
than	O
b	O
,	O
it	O
simply	O
says	O
that	O
a	O
is	O
‘	O
signi	O
(	O
cid:12	O
)	O
cantly	O
’	O
di	O
(	O
cid:11	O
)	O
erent	O
from	O
b.	O
and	O
here	O
,	O
‘	O
signi	O
(	O
cid:12	O
)	O
cant	O
’	O
means	O
only	O
‘	O
statistically	O
signi	O
(	O
cid:12	O
)	O
cant	O
’	O
,	O
not	O
practically	O
signi	O
(	O
cid:12	O
)	O
cant	O
.	O
the	O
man	O
in	O
the	O
street	O
,	O
reading	O
the	O
statement	O
that	O
‘	O
the	O
treatment	O
was	O
sig-	O
ni	O
(	O
cid:12	O
)	O
cantly	O
di	O
(	O
cid:11	O
)	O
erent	O
from	O
the	O
control	O
(	O
p	O
=	O
0:07	O
)	O
’	O
,	O
might	O
come	O
to	O
the	O
conclusion	O
that	O
‘	O
there	O
is	O
a	O
93	O
%	O
chance	O
that	O
the	O
treatments	O
di	O
(	O
cid:11	O
)	O
er	O
in	O
e	O
(	O
cid:11	O
)	O
ectiveness	O
’	O
.	O
but	O
what	O
‘	O
p	O
=	O
0:07	O
’	O
actually	O
means	O
is	O
‘	O
if	O
you	O
did	O
this	O
experiment	O
many	O
times	O
,	O
and	O
the	O
two	O
treatments	O
had	O
equal	O
e	O
(	O
cid:11	O
)	O
ectiveness	O
,	O
then	O
7	O
%	O
of	O
the	O
time	O
you	O
would	O
(	O
cid:12	O
)	O
nd	O
a	O
value	O
of	O
(	O
cid:31	O
)	O
2	O
more	O
extreme	O
than	O
the	O
one	O
that	O
happened	O
here	O
’	O
.	O
this	O
has	O
almost	O
nothing	O
to	O
do	O
with	O
what	O
we	O
want	O
to	O
know	O
,	O
which	O
is	O
how	O
likely	O
it	O
is	O
that	O
treatment	O
a	O
is	O
better	O
than	O
b.	O
let	O
me	O
through	O
,	O
i	O
’	O
m	O
a	O
bayesian	O
ok	O
,	O
now	O
let	O
’	O
s	O
infer	O
what	O
we	O
really	O
want	O
to	O
know	O
.	O
we	O
scrap	O
the	O
hypothesis	O
that	O
the	O
two	O
treatments	O
have	O
exactly	O
equal	O
e	O
(	O
cid:11	O
)	O
ectivenesses	O
,	O
since	O
we	O
do	O
not	O
believe	O
it	O
.	O
there	O
are	O
two	O
unknown	O
parameters	O
,	O
pa+	O
and	O
pb+	O
,	O
which	O
are	O
the	O
probabilities	O
that	O
people	O
given	O
treatments	O
a	O
and	O
b	O
,	O
respectively	O
,	O
contract	O
the	O
disease	B
.	O
given	O
the	O
data	O
,	O
we	O
can	O
infer	O
these	O
two	O
probabilities	O
,	O
and	O
we	O
can	O
answer	O
questions	O
of	O
interest	O
by	O
examining	O
the	O
posterior	O
distribution	O
.	O
the	O
posterior	O
distribution	O
is	O
p	O
(	O
pa+	O
;	O
pb+	O
jffig	O
)	O
=	O
p	O
(	O
ffigj	O
pa+	O
;	O
pb+	O
)	O
p	O
(	O
pa+	O
;	O
pb+	O
)	O
p	O
(	O
ffig	O
)	O
:	O
(	O
37.11	O
)	O
the	O
likelihood	B
function	O
is	O
fa+	O
(	O
cid:19	O
)	O
pfa+	O
p	O
(	O
ffigj	O
pa+	O
;	O
pb+	O
)	O
=	O
(	O
cid:18	O
)	O
na	O
=	O
(	O
cid:18	O
)	O
30	O
1	O
(	O
cid:19	O
)	O
p1	O
fb+	O
(	O
cid:19	O
)	O
pfb+	O
a	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
nb	O
a	O
(	O
cid:0	O
)	O
(	O
cid:18	O
)	O
10	O
3	O
(	O
cid:19	O
)	O
p3	O
b	O
(	O
cid:0	O
)	O
:	O
a+	O
pfa	O
(	O
cid:0	O
)	O
a+p29	O
b+p7	O
b+	O
pfb	O
(	O
cid:0	O
)	O
b	O
(	O
cid:0	O
)	O
(	O
37.12	O
)	O
(	O
37.13	O
)	O
what	O
prior	O
distribution	B
should	O
we	O
use	O
?	O
the	O
prior	B
distribution	O
gives	O
us	O
the	O
opportunity	O
to	O
include	O
knowledge	O
from	O
other	O
experiments	O
,	O
or	O
a	O
prior	B
belief	O
that	O
the	O
two	O
parameters	B
pa+	O
and	O
pb+	O
,	O
while	O
di	O
(	O
cid:11	O
)	O
erent	O
from	O
each	O
other	O
,	O
are	O
expected	O
to	O
have	O
similar	O
values	O
.	O
here	O
we	O
will	O
use	O
the	O
simplest	O
vanilla	O
prior	B
distribution	O
,	O
a	O
uniform	O
distri-	O
bution	O
over	O
each	O
parameter	O
.	O
p	O
(	O
pa+	O
;	O
pb+	O
)	O
=	O
1	O
:	O
(	O
37.14	O
)	O
we	O
can	O
now	O
plot	O
the	O
posterior	O
distribution	O
.	O
given	O
the	O
assumption	O
of	O
a	O
sepa-	O
rable	O
prior	B
on	O
pa+	O
and	O
pb+	O
,	O
the	O
posterior	O
distribution	O
is	O
also	O
separable	O
:	O
p	O
(	O
pa+	O
;	O
pb+	O
jffig	O
)	O
=	O
p	O
(	O
pa+	O
j	O
fa+	O
;	O
fa	O
(	O
cid:0	O
)	O
)	O
p	O
(	O
pb+	O
j	O
fb+	O
;	O
fb	O
(	O
cid:0	O
)	O
)	O
:	O
(	O
37.15	O
)	O
the	O
two	O
posterior	O
distributions	O
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
37.1	O
(	O
except	O
the	O
graphs	O
are	O
not	O
normalized	O
)	O
and	O
the	O
joint	B
posterior	O
probability	B
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
37.2.	O
if	O
we	O
want	O
to	O
know	O
the	O
answer	O
to	O
the	O
question	O
‘	O
how	O
probable	O
is	O
it	O
that	O
pa+	O
is	O
smaller	O
than	O
pb+	O
?	O
’	O
,	O
we	O
can	O
answer	O
exactly	O
that	O
question	O
by	O
computing	O
the	O
posterior	B
probability	I
p	O
(	O
pa+	O
<	O
pb+	O
j	O
data	O
)	O
;	O
(	O
37.16	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
37.1	O
:	O
a	O
medical	O
example	O
461	O
figure	O
37.1.	O
posterior	O
probabilities	O
of	O
the	O
two	O
e	O
(	O
cid:11	O
)	O
ectivenesses	O
.	O
treatment	O
a	O
{	O
solid	O
line	O
;	O
b	O
{	O
dotted	O
line	O
.	O
figure	O
37.2.	O
joint	B
posterior	O
probability	O
of	O
the	O
two	O
e	O
(	O
cid:11	O
)	O
ectivenesses	O
{	O
contour	O
plot	O
and	O
surface	O
plot	O
.	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
pb+	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
pa+	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
1	O
0	O
1	O
0.8	O
0.2	O
0.6	O
0.4	O
which	O
is	O
the	O
integral	B
of	O
the	O
joint	B
posterior	O
probability	B
p	O
(	O
pa+	O
;	O
pb+	O
j	O
data	O
)	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
37.2	O
over	O
the	O
region	O
in	O
which	O
pa+	O
<	O
pb+	O
,	O
i.e.	O
,	O
the	O
shaded	O
triangle	B
in	O
(	O
cid:12	O
)	O
gure	O
37.3.	O
the	O
value	O
of	O
this	O
integral	B
(	O
obtained	O
by	O
a	O
straightfor-	O
ward	O
numerical	O
integration	O
of	O
the	O
likelihood	O
function	B
(	O
37.13	O
)	O
over	O
the	O
relevant	O
region	O
)	O
is	O
p	O
(	O
pa+	O
<	O
pb+	O
j	O
data	O
)	O
=	O
0:990.	O
thus	O
there	O
is	O
a	O
99	O
%	O
chance	O
,	O
given	O
the	O
data	O
and	O
our	O
prior	B
assumptions	O
,	O
that	O
treatment	O
a	O
is	O
superior	O
to	O
treatment	O
b.	O
in	O
conclusion	O
,	O
according	O
to	O
our	O
bayesian	O
model	B
,	O
the	O
data	O
(	O
1	O
out	O
of	O
30	O
contracted	O
the	O
disease	B
after	O
vaccination	B
a	O
,	O
and	O
3	O
out	O
of	O
10	O
contracted	O
the	O
disease	B
after	O
vaccination	B
b	O
)	O
give	O
very	O
strong	O
evidence	B
{	O
about	O
99	O
to	O
one	O
{	O
that	O
treatment	O
a	O
is	O
superior	O
to	O
treatment	O
b.	O
in	O
the	O
bayesian	O
approach	O
,	O
it	O
is	O
also	O
easy	O
to	O
answer	O
other	O
relevant	O
questions	O
.	O
for	O
example	O
,	O
if	O
we	O
want	O
to	O
know	O
‘	O
how	O
likely	O
is	O
it	O
that	O
treatment	O
a	O
is	O
ten	O
times	O
more	O
e	O
(	O
cid:11	O
)	O
ective	O
than	O
treatment	O
b	O
?	O
’	O
,	O
we	O
can	O
integrate	O
the	O
joint	B
posterior	O
proba-	O
bility	O
p	O
(	O
pa+	O
;	O
pb+	O
j	O
data	O
)	O
over	O
the	O
region	O
in	O
which	O
pa+	O
<	O
10	O
pb+	O
(	O
(	O
cid:12	O
)	O
gure	O
37.4	O
)	O
.	O
model	B
comparison	I
1	O
pb+	O
0	O
0	O
pa+	O
1	O
figure	O
37.3.	O
the	O
proposition	O
pa+	O
<	O
pb+	O
is	O
true	O
for	O
all	O
points	O
in	O
the	O
shaded	O
triangle	B
.	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
probability	O
of	O
this	O
proposition	O
we	O
integrate	O
the	O
joint	B
posterior	O
probability	B
p	O
(	O
pa+	O
;	O
pb+	O
j	O
data	O
)	O
(	O
(	O
cid:12	O
)	O
gure	O
37.2	O
)	O
over	O
this	O
region	O
.	O
1	O
pb+	O
if	O
there	O
were	O
a	O
situation	O
in	O
which	O
we	O
really	O
did	O
want	O
to	O
compare	O
the	O
two	O
hypotheses	O
h0	O
:	O
pa+	O
=	O
pb+	O
and	O
h1	O
:	O
pa+	O
6=	O
pb+	O
,	O
we	O
can	O
of	O
course	O
do	O
this	O
directly	O
with	O
bayesian	O
methods	B
also	O
.	O
as	O
an	O
example	O
,	O
consider	O
the	O
data	B
set	I
:	O
d	O
:	O
one	O
subject	O
,	O
given	O
treatment	O
a	O
,	O
subsequently	O
contracted	O
microsoftus	B
.	O
one	O
subject	O
,	O
given	O
treatment	O
b	O
,	O
did	O
not	O
.	O
0	O
0	O
pa+	O
1	O
figure	O
37.4.	O
the	O
proposition	O
pa+	O
<	O
10	O
pb+	O
is	O
true	O
for	O
all	O
points	O
in	O
the	O
shaded	O
triangle	B
.	O
treatment	O
a	O
b	O
got	O
disease	B
did	O
not	O
total	O
treated	O
1	O
0	O
1	O
0	O
1	O
1	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
462	O
37	O
|	O
bayesian	O
inference	B
and	O
sampling	B
theory	I
how	O
strongly	O
does	O
this	O
data	B
set	I
favour	O
h1	O
over	O
h0	O
?	O
we	O
answer	O
this	O
question	O
by	O
computing	O
the	O
evidence	B
for	O
each	O
hypothesis	O
.	O
let	O
’	O
s	O
assume	O
uniform	O
priors	O
over	O
the	O
unknown	O
parameters	O
of	O
the	O
models	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
hypothesis	O
h0	O
:	O
pa+	O
=	O
pb+	O
has	O
just	O
one	O
unknown	O
parameter	O
,	O
let	O
’	O
s	O
call	O
it	O
p.	O
(	O
37.17	O
)	O
we	O
’	O
ll	O
use	O
the	O
uniform	O
prior	B
over	O
the	O
two	O
parameters	B
of	O
model	B
h1	O
that	O
we	O
used	O
before	O
:	O
p	O
(	O
pjh0	O
)	O
=	O
1	O
p	O
2	O
(	O
0	O
;	O
1	O
)	O
:	O
p	O
(	O
pa+	O
;	O
pb+	O
jh1	O
)	O
=	O
1	O
pa+	O
2	O
(	O
0	O
;	O
1	O
)	O
;	O
pb+	O
2	O
(	O
0	O
;	O
1	O
)	O
:	O
(	O
37.18	O
)	O
now	O
,	O
the	O
probability	O
of	O
the	O
data	O
d	O
under	O
model	B
h0	O
is	O
the	O
normalizing	B
constant	I
from	O
the	O
inference	B
of	O
p	O
given	O
d	O
:	O
p	O
(	O
d	O
jh0	O
)	O
=	O
z	O
dp	O
p	O
(	O
d	O
j	O
p	O
)	O
p	O
(	O
pjh0	O
)	O
=	O
z	O
dp	O
p	O
(	O
1	O
(	O
cid:0	O
)	O
p	O
)	O
(	O
cid:2	O
)	O
1	O
=	O
1=6	O
:	O
(	O
37.19	O
)	O
(	O
37.20	O
)	O
(	O
37.21	O
)	O
the	O
probability	O
of	O
the	O
data	O
d	O
under	O
model	B
h1	O
is	O
given	O
by	O
a	O
simple	O
two-	O
dimensional	O
integral	B
:	O
p	O
(	O
d	O
jh1	O
)	O
=	O
z	O
z	O
dpa+	O
dpb+	O
p	O
(	O
d	O
j	O
pa+	O
;	O
pb+	O
)	O
p	O
(	O
pa+	O
;	O
pb+	O
jh1	O
)	O
(	O
37.22	O
)	O
=	O
z	O
dpa+	O
pa+	O
z	O
dpb+	O
(	O
1	O
(	O
cid:0	O
)	O
pb+	O
)	O
=	O
1=2	O
(	O
cid:2	O
)	O
1=2	O
=	O
1=4	O
:	O
(	O
37.23	O
)	O
(	O
37.24	O
)	O
(	O
37.25	O
)	O
thus	O
the	O
evidence	B
ratio	O
in	O
favour	O
of	O
model	O
h1	O
,	O
which	O
asserts	O
that	O
the	O
two	O
e	O
(	O
cid:11	O
)	O
ectivenesses	O
are	O
unequal	O
,	O
is	O
p	O
(	O
d	O
jh1	O
)	O
p	O
(	O
d	O
jh0	O
)	O
=	O
1=4	O
1=6	O
=	O
0:6	O
0:4	O
:	O
(	O
37.26	O
)	O
so	O
if	O
the	O
prior	B
probability	O
over	O
the	O
two	O
hypotheses	O
was	O
50:50	O
,	O
the	O
posterior	B
probability	I
is	O
60:40	O
in	O
favour	O
of	O
h1	O
.	O
2	O
is	O
it	O
not	O
easy	O
to	O
get	O
sensible	O
answers	O
to	O
well-posed	O
questions	O
using	O
bayesian	O
methods	B
?	O
[	O
the	O
sampling	B
theory	I
answer	O
to	O
this	O
question	O
would	O
involve	O
the	O
identical	O
signi	O
(	O
cid:12	O
)	O
cance	O
test	B
that	O
was	O
used	O
in	O
the	O
preceding	O
problem	O
;	O
that	O
test	B
would	O
yield	O
a	O
‘	O
not	O
signi	O
(	O
cid:12	O
)	O
cant	O
’	O
result	O
.	O
i	O
think	O
it	O
is	O
greatly	O
preferable	O
to	O
acknowledge	O
what	O
is	O
obvious	O
to	O
the	O
intuition	O
,	O
namely	O
that	O
the	O
data	O
d	O
do	O
give	O
weak	O
evidence	B
in	O
favour	O
of	O
h1	O
.	O
bayesian	O
methods	B
quantify	O
how	O
weak	O
the	O
evidence	B
is	O
.	O
]	O
37.2	O
dependence	O
of	O
p-values	O
on	O
irrelevant	O
information	B
in	O
an	O
expensive	O
laboratory	O
,	O
dr.	O
bloggs	O
tosses	O
a	O
coin	B
labelled	O
a	O
and	O
b	O
twelve	O
times	O
and	O
the	O
outcome	O
is	O
the	O
string	O
aaabaaaabaab	O
;	O
which	O
contains	O
three	O
bs	O
and	O
nine	O
as	O
.	O
what	O
evidence	O
do	O
these	O
data	O
give	O
that	O
the	O
coin	B
is	O
biased	O
in	O
favour	O
of	O
a	O
?	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
37.2	O
:	O
dependence	O
of	O
p-values	O
on	O
irrelevant	O
information	B
463	O
dr.	O
bloggs	O
consults	O
his	O
sampling	B
theory	I
friend	O
who	O
says	O
‘	O
let	O
r	O
be	O
the	O
num-	O
ber	O
of	O
bs	O
and	O
n	O
=	O
12	O
be	O
the	O
total	O
number	O
of	O
tosses	O
;	O
i	O
view	O
r	O
as	O
the	O
random	B
variable	I
and	O
(	O
cid:12	O
)	O
nd	O
the	O
probability	O
of	O
r	O
taking	O
on	O
the	O
value	O
r	O
=	O
3	O
or	O
a	O
more	O
extreme	B
value	I
,	O
assuming	O
the	O
null	O
hypothesis	O
pa	O
=	O
0:5	O
to	O
be	O
true	O
’	O
.	O
he	O
thus	O
computes	O
p	O
(	O
r	O
(	O
cid:20	O
)	O
3j	O
n	O
=	O
12	O
;	O
h0	O
)	O
=	O
n	O
3	O
r	O
(	O
cid:19	O
)	O
1/2	O
xr=0	O
(	O
cid:18	O
)	O
n	O
=	O
0:07	O
;	O
=	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
12	O
0	O
(	O
cid:1	O
)	O
+	O
(	O
cid:0	O
)	O
12	O
1	O
(	O
cid:1	O
)	O
+	O
(	O
cid:0	O
)	O
12	O
2	O
(	O
cid:1	O
)	O
+	O
(	O
cid:0	O
)	O
12	O
3	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1/2	O
12	O
(	O
37.27	O
)	O
and	O
reports	O
‘	O
at	O
the	O
signi	O
(	O
cid:12	O
)	O
cance	O
level	O
of	O
5	O
%	O
,	O
there	O
is	O
not	O
signi	O
(	O
cid:12	O
)	O
cant	O
evidence	B
of	O
bias	B
in	O
favour	O
of	O
a	O
’	O
.	O
or	O
,	O
if	O
the	O
friend	O
prefers	O
to	O
report	O
p-values	O
rather	O
than	O
simply	O
compare	O
p	O
with	O
5	O
%	O
,	O
he	O
would	O
report	O
‘	O
the	O
p-value	B
is	O
7	O
%	O
,	O
which	O
is	O
not	O
conventionally	O
viewed	O
as	O
signi	O
(	O
cid:12	O
)	O
cantly	O
small	O
’	O
.	O
if	O
a	O
two-tailed	O
test	B
seemed	O
more	O
appropriate	O
,	O
he	O
might	O
compute	O
the	O
two-tailed	O
area	O
,	O
which	O
is	O
twice	O
the	O
above	O
probability	B
,	O
and	O
report	O
‘	O
the	O
p-value	B
is	O
15	O
%	O
,	O
which	O
is	O
not	O
signi	O
(	O
cid:12	O
)	O
cantly	O
small	O
’	O
.	O
we	O
won	O
’	O
t	O
focus	B
on	O
the	O
issue	O
of	O
the	O
choice	O
between	O
the	O
one-tailed	O
and	O
two-tailed	O
tests	O
,	O
as	O
we	O
have	O
bigger	O
(	O
cid:12	O
)	O
sh	O
to	O
catch	O
.	O
dr.	O
bloggs	O
pays	O
careful	O
attention	O
to	O
the	O
calculation	O
(	O
37.27	O
)	O
,	O
and	O
responds	O
‘	O
no	O
,	O
no	O
,	O
the	O
random	B
variable	I
in	O
the	O
experiment	O
was	O
not	O
r	O
:	O
i	O
decided	O
before	O
running	O
the	O
experiment	O
that	O
i	O
would	O
keep	O
tossing	O
the	O
coin	B
until	O
i	O
saw	O
three	O
bs	O
;	O
the	O
random	B
variable	I
is	O
thus	O
n	O
’	O
.	O
such	O
experimental	O
designs	O
are	O
not	O
unusual	O
.	O
in	O
my	O
experiments	O
on	O
error-	O
correcting	O
codes	O
i	O
often	O
simulate	O
the	O
decoding	B
of	O
a	O
code	B
until	O
a	O
chosen	O
number	O
r	O
of	O
block	O
errors	B
(	O
bs	O
)	O
has	O
occurred	O
,	O
since	O
the	O
error	O
on	O
the	O
inferred	O
value	O
of	O
log	O
pb	O
goes	O
roughly	O
as	O
pr	O
,	O
independent	O
of	O
n.	O
exercise	O
37.1	O
.	O
[	O
2	O
]	O
find	O
the	O
bayesian	O
inference	B
about	O
the	O
bias	B
pa	O
of	O
the	O
coin	O
given	O
the	O
data	O
,	O
and	O
determine	O
whether	O
a	O
bayesian	O
’	O
s	O
inferences	O
depend	O
on	O
what	O
stopping	B
rule	I
was	O
in	O
force	O
.	O
according	O
to	O
sampling	B
theory	I
,	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
calculation	O
is	O
required	O
in	O
order	O
to	O
assess	O
the	O
‘	O
signi	O
(	O
cid:12	O
)	O
cance	O
’	O
of	O
the	O
result	O
n	O
=	O
12.	O
the	O
probability	B
distribution	O
of	O
n	O
given	O
h0	O
is	O
the	O
probability	B
that	O
the	O
(	O
cid:12	O
)	O
rst	O
n	O
(	O
cid:0	O
)	O
1	O
tosses	O
contain	O
exactly	O
r	O
(	O
cid:0	O
)	O
1	O
bs	O
and	O
then	O
the	O
nth	O
toss	O
is	O
a	O
b.	O
p	O
(	O
njh0	O
;	O
r	O
)	O
=	O
(	O
cid:18	O
)	O
n	O
(	O
cid:0	O
)	O
1	O
r	O
(	O
cid:0	O
)	O
1	O
(	O
cid:19	O
)	O
1/2	O
n	O
:	O
the	O
sampling	O
theorist	O
thus	O
computes	O
p	O
(	O
n	O
(	O
cid:21	O
)	O
12j	O
r	O
=	O
3	O
;	O
h0	O
)	O
=	O
0:03	O
:	O
(	O
37.28	O
)	O
(	O
37.29	O
)	O
he	O
reports	O
back	O
to	O
dr.	O
bloggs	O
,	O
‘	O
the	O
p-value	B
is	O
3	O
%	O
{	O
there	O
is	O
signi	O
(	O
cid:12	O
)	O
cant	O
evidence	B
of	O
bias	B
after	O
all	O
!	O
’	O
what	O
do	O
you	O
think	O
dr.	O
bloggs	O
should	O
do	O
?	O
should	O
he	O
publish	O
the	O
result	O
,	O
with	O
this	O
marvellous	O
p-value	B
,	O
in	O
one	O
of	O
the	O
journals	O
that	O
insists	O
that	O
all	O
exper-	O
imental	O
results	O
have	O
their	O
‘	O
signi	O
(	O
cid:12	O
)	O
cance	O
’	O
assessed	O
using	O
sampling	B
theory	I
?	O
or	O
should	O
he	O
boot	O
the	O
sampling	O
theorist	O
out	O
of	O
the	O
door	O
and	O
seek	O
a	O
coherent	O
method	B
of	O
assessing	O
signi	O
(	O
cid:12	O
)	O
cance	O
,	O
one	O
that	O
does	O
not	O
depend	O
on	O
the	O
stopping	B
rule	I
?	O
at	O
this	O
point	O
the	O
audience	O
divides	O
in	O
two	O
.	O
half	O
the	O
audience	O
intuitively	O
feel	O
that	O
the	O
stopping	B
rule	I
is	O
irrelevant	O
,	O
and	O
don	O
’	O
t	O
need	O
any	O
convincing	O
that	O
the	O
answer	O
to	O
exercise	O
37.1	O
(	O
p.463	O
)	O
is	O
‘	O
the	O
inferences	O
about	O
pa	O
do	O
not	O
depend	O
on	O
the	O
stopping	B
rule	I
’	O
.	O
the	O
other	O
half	O
,	O
perhaps	O
on	O
account	O
of	O
a	O
thorough	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
464	O
37	O
|	O
bayesian	O
inference	B
and	O
sampling	B
theory	I
training	O
in	O
sampling	O
theory	B
,	O
intuitively	O
feel	O
that	O
dr.	O
bloggs	O
’	O
s	O
stopping	B
rule	I
,	O
which	O
stopped	O
tossing	O
the	O
moment	O
the	O
third	O
b	O
appeared	O
,	O
may	O
have	O
biased	O
the	O
experiment	O
somehow	O
.	O
if	O
you	O
are	O
in	O
the	O
second	O
group	O
,	O
i	O
encourage	O
you	O
to	O
re	O
(	O
cid:13	O
)	O
ect	O
on	O
the	O
situation	O
,	O
and	O
hope	O
you	O
’	O
ll	O
eventually	O
come	O
round	O
to	O
the	O
view	O
that	O
is	O
consistent	O
with	O
the	O
likelihood	B
principle	I
,	O
which	O
is	O
that	O
the	O
stopping	B
rule	I
is	O
not	O
relevant	O
to	O
what	O
we	O
have	O
learned	O
about	O
pa.	O
as	O
a	O
thought	O
experiment	O
,	O
consider	O
some	O
onlookers	O
who	O
(	O
in	O
order	O
to	O
save	O
money	O
)	O
are	O
spying	O
on	O
dr.	O
bloggs	O
’	O
s	O
experiments	O
:	O
each	O
time	O
he	O
tosses	O
the	O
coin	B
,	O
the	O
spies	O
update	O
the	O
values	O
of	O
r	O
and	O
n.	O
the	O
spies	O
are	O
eager	O
to	O
make	O
inferences	O
from	O
the	O
data	O
as	O
soon	O
as	O
each	O
new	O
result	O
occurs	O
.	O
should	O
the	O
spies	O
’	O
beliefs	O
about	O
the	O
bias	B
of	O
the	O
coin	B
depend	O
on	O
dr.	O
bloggs	O
’	O
s	O
intentions	O
regarding	O
the	O
continuation	O
of	O
the	O
experiment	O
?	O
the	O
fact	O
that	O
the	O
p-values	O
of	O
sampling	O
theory	B
do	O
depend	O
on	O
the	O
stopping	B
rule	I
(	O
indeed	O
,	O
whole	O
volumes	O
of	O
the	O
sampling	O
theory	B
literature	O
are	O
concerned	O
with	O
the	O
task	O
of	O
assessing	O
‘	O
signi	O
(	O
cid:12	O
)	O
cance	O
’	O
when	O
a	O
complicated	O
stopping	B
rule	I
is	O
required	O
{	O
‘	O
sequential	O
probability	O
ratio	O
tests	O
’	O
,	O
for	O
example	O
)	O
seems	O
to	O
me	O
a	O
com-	O
pelling	O
argument	O
for	O
having	O
nothing	O
to	O
do	O
with	O
p-values	O
at	O
all	O
.	O
a	O
bayesian	O
solution	O
to	O
this	O
inference	B
problem	O
was	O
given	O
in	O
sections	O
3.2	O
and	O
3.3	O
and	O
exer-	O
cise	O
3.15	O
(	O
p.59	O
)	O
.	O
would	O
it	O
help	O
clarify	O
this	O
issue	O
if	O
i	O
added	O
one	O
more	O
scene	O
to	O
the	O
story	O
?	O
the	O
janitor	B
,	O
who	O
’	O
s	O
been	O
eavesdropping	O
on	O
dr.	O
bloggs	O
’	O
s	O
conversation	O
,	O
comes	O
in	O
and	O
says	O
‘	O
i	O
happened	O
to	O
notice	O
that	O
just	O
after	O
you	O
stopped	O
doing	O
the	O
experi-	O
ments	O
on	O
the	O
coin	B
,	O
the	O
o	O
(	O
cid:14	O
)	O
cer	O
for	O
whimsical	O
departmental	O
rules	B
ordered	O
the	O
immediate	O
destruction	O
of	O
all	O
such	O
coins	O
.	O
your	O
coin	B
was	O
therefore	O
destroyed	O
by	O
the	O
departmental	O
safety	O
o	O
(	O
cid:14	O
)	O
cer	O
.	O
there	O
is	O
no	O
way	O
you	O
could	O
have	O
continued	O
the	O
experiment	O
much	O
beyond	O
n	O
=	O
12	O
tosses	O
.	O
seems	O
to	O
me	O
,	O
you	O
need	O
to	O
recompute	O
your	O
p-value	B
?	O
’	O
37.3	O
con	O
(	O
cid:12	O
)	O
dence	O
intervals	B
in	O
an	O
experiment	O
in	O
which	O
data	O
d	O
are	O
obtained	O
from	O
a	O
system	O
with	O
an	O
unknown	O
parameter	O
(	O
cid:18	O
)	O
,	O
a	O
standard	O
concept	O
in	O
sampling	O
theory	B
is	O
the	O
idea	O
of	O
a	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
for	O
(	O
cid:18	O
)	O
.	O
such	O
an	O
interval	O
(	O
(	O
cid:18	O
)	O
min	O
(	O
d	O
)	O
;	O
(	O
cid:18	O
)	O
max	O
(	O
d	O
)	O
)	O
has	O
associated	O
with	O
it	O
a	O
con	O
(	O
cid:12	O
)	O
dence	O
level	O
such	O
as	O
95	O
%	O
which	O
is	O
informally	O
interpreted	O
as	O
‘	O
the	O
probability	B
that	O
(	O
cid:18	O
)	O
lies	O
in	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
’	O
.	O
let	O
’	O
s	O
make	O
precise	O
what	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
level	O
really	O
means	O
,	O
then	O
give	O
an	O
example	O
.	O
a	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
is	O
a	O
function	B
(	O
(	O
cid:18	O
)	O
min	O
(	O
d	O
)	O
;	O
(	O
cid:18	O
)	O
max	O
(	O
d	O
)	O
)	O
of	O
the	O
data	O
set	B
d.	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
level	O
of	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
is	O
a	O
property	O
that	O
we	O
can	O
compute	O
before	O
the	O
data	O
arrive	O
.	O
we	O
imagine	O
generating	O
many	O
data	O
sets	O
from	O
a	O
particular	O
true	O
value	O
of	O
(	O
cid:18	O
)	O
,	O
and	O
calculating	O
the	O
interval	O
(	O
(	O
cid:18	O
)	O
min	O
(	O
d	O
)	O
;	O
(	O
cid:18	O
)	O
max	O
(	O
d	O
)	O
)	O
,	O
and	O
then	O
checking	O
whether	O
the	O
true	O
value	O
of	O
(	O
cid:18	O
)	O
lies	O
in	O
that	O
interval	O
.	O
if	O
,	O
averaging	O
over	O
all	O
these	O
imagined	O
repetitions	O
of	O
the	O
experiment	O
,	O
the	O
true	O
value	O
of	O
(	O
cid:18	O
)	O
lies	O
in	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
a	O
fraction	O
f	O
of	O
the	O
time	O
,	O
and	O
this	O
property	O
holds	O
for	O
all	O
true	O
values	O
of	O
(	O
cid:18	O
)	O
,	O
then	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
level	O
of	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
is	O
f	O
.	O
for	O
example	O
,	O
if	O
(	O
cid:18	O
)	O
is	O
the	O
mean	B
of	O
a	O
gaussian	O
distribution	B
which	O
is	O
known	O
to	O
have	O
standard	B
deviation	I
1	O
,	O
and	O
d	O
is	O
a	O
sample	B
from	I
that	O
gaussian	O
,	O
then	O
(	O
(	O
cid:18	O
)	O
min	O
(	O
d	O
)	O
;	O
(	O
cid:18	O
)	O
max	O
(	O
d	O
)	O
)	O
=	O
(	O
d	O
(	O
cid:0	O
)	O
2	O
;	O
d+2	O
)	O
is	O
a	O
95	O
%	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
for	O
(	O
cid:18	O
)	O
.	O
let	O
us	O
now	O
look	O
at	O
a	O
simple	O
example	O
where	O
the	O
meaning	O
of	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
level	O
becomes	O
clearer	O
.	O
let	O
the	O
parameter	O
(	O
cid:18	O
)	O
be	O
an	O
integer	O
,	O
and	O
let	O
the	O
data	O
be	O
a	O
pair	O
of	O
points	O
x1	O
;	O
x2	O
,	O
drawn	O
independently	O
from	O
the	O
following	O
distribution	B
:	O
p	O
(	O
xj	O
(	O
cid:18	O
)	O
)	O
=8	O
<	O
:	O
1/2	O
x	O
=	O
(	O
cid:18	O
)	O
1/2	O
x	O
=	O
(	O
cid:18	O
)	O
+	O
1	O
0	O
for	O
other	O
values	O
of	O
x	O
.	O
(	O
37.30	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
37.4	O
:	O
some	O
compromise	O
positions	O
465	O
for	O
example	O
,	O
if	O
(	O
cid:18	O
)	O
were	O
39	O
,	O
then	O
we	O
could	O
expect	O
the	O
following	O
data	O
sets	O
:	O
d	O
=	O
(	O
x1	O
;	O
x2	O
)	O
=	O
(	O
39	O
;	O
39	O
)	O
with	O
probability	O
1/4	O
;	O
(	O
x1	O
;	O
x2	O
)	O
=	O
(	O
39	O
;	O
40	O
)	O
with	O
probability	O
1/4	O
;	O
(	O
x1	O
;	O
x2	O
)	O
=	O
(	O
40	O
;	O
39	O
)	O
with	O
probability	O
1/4	O
;	O
(	O
x1	O
;	O
x2	O
)	O
=	O
(	O
40	O
;	O
40	O
)	O
with	O
probability	O
1/4	O
.	O
(	O
37.31	O
)	O
we	O
now	O
consider	O
the	O
following	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
:	O
[	O
(	O
cid:18	O
)	O
min	O
(	O
d	O
)	O
;	O
(	O
cid:18	O
)	O
max	O
(	O
d	O
)	O
]	O
=	O
[	O
min	O
(	O
x1	O
;	O
x2	O
)	O
;	O
min	O
(	O
x1	O
;	O
x2	O
)	O
]	O
:	O
(	O
37.32	O
)	O
for	O
example	O
,	O
if	O
(	O
x1	O
;	O
x2	O
)	O
=	O
(	O
40	O
;	O
39	O
)	O
,	O
then	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
for	O
(	O
cid:18	O
)	O
would	O
be	O
[	O
(	O
cid:18	O
)	O
min	O
(	O
d	O
)	O
;	O
(	O
cid:18	O
)	O
max	O
(	O
d	O
)	O
]	O
=	O
[	O
39	O
;	O
39	O
]	O
.	O
let	O
’	O
s	O
think	O
about	O
this	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
.	O
what	O
is	O
its	O
con	O
(	O
cid:12	O
)	O
dence	O
level	O
?	O
by	O
considering	O
the	O
four	O
possibilities	O
shown	O
in	O
(	O
37.31	O
)	O
,	O
we	O
can	O
see	O
that	O
there	O
is	O
a	O
75	O
%	O
chance	O
that	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
will	O
contain	O
the	O
true	O
value	O
.	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
therefore	O
has	O
a	O
con	O
(	O
cid:12	O
)	O
dence	O
level	O
of	O
75	O
%	O
,	O
by	O
de	O
(	O
cid:12	O
)	O
nition	O
.	O
now	O
,	O
what	O
if	O
the	O
data	O
we	O
acquire	O
are	O
(	O
x1	O
;	O
x2	O
)	O
=	O
(	O
29	O
;	O
29	O
)	O
?	O
well	O
,	O
we	O
can	O
compute	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
,	O
and	O
it	O
is	O
[	O
29	O
;	O
29	O
]	O
.	O
so	O
shall	O
we	O
report	O
this	O
interval	O
,	O
and	O
its	O
associated	O
con	O
(	O
cid:12	O
)	O
dence	O
level	O
,	O
75	O
%	O
?	O
this	O
would	O
be	O
correct	O
by	O
the	O
rules	B
of	O
sampling	B
theory	I
.	O
but	O
does	O
this	O
make	O
sense	O
?	O
what	O
do	O
we	O
actually	O
know	O
in	O
this	O
case	O
?	O
intuitively	O
,	O
or	O
by	O
bayes	O
’	O
theorem	B
,	O
it	O
is	O
clear	O
that	O
(	O
cid:18	O
)	O
could	O
either	O
be	O
29	O
or	O
28	O
,	O
and	O
both	O
possibilities	O
are	O
equally	O
likely	O
(	O
if	O
the	O
prior	B
probabilities	O
of	O
28	O
and	O
29	O
were	O
equal	O
)	O
.	O
the	O
posterior	B
probability	I
of	O
(	O
cid:18	O
)	O
is	O
50	O
%	O
on	O
29	O
and	O
50	O
%	O
on	O
28.	O
what	O
if	O
the	O
data	O
are	O
(	O
x1	O
;	O
x2	O
)	O
=	O
(	O
29	O
;	O
30	O
)	O
?	O
in	O
this	O
case	O
,	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
is	O
still	O
[	O
29	O
;	O
29	O
]	O
,	O
and	O
its	O
associated	O
con	O
(	O
cid:12	O
)	O
dence	O
level	O
is	O
75	O
%	O
.	O
but	O
in	O
this	O
case	O
,	O
by	O
bayes	O
’	O
theorem	B
,	O
or	O
common	O
sense	O
,	O
we	O
are	O
100	O
%	O
sure	O
that	O
(	O
cid:18	O
)	O
is	O
29.	O
in	O
neither	O
case	O
is	O
the	O
probability	B
that	O
(	O
cid:18	O
)	O
lies	O
in	O
the	O
‘	O
75	O
%	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
’	O
equal	O
to	O
75	O
%	O
!	O
thus	O
1.	O
the	O
way	O
in	O
which	O
many	O
people	O
interpret	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
levels	O
of	O
sampling	O
theory	B
is	O
incorrect	O
;	O
2.	O
given	O
some	O
data	O
,	O
what	O
people	O
usually	O
want	O
to	O
know	O
(	O
whether	O
they	O
know	O
it	O
or	O
not	O
)	O
is	O
a	O
bayesian	O
posterior	B
probability	I
distribution	O
.	O
are	O
all	O
these	O
examples	O
contrived	O
?	O
am	O
i	O
making	O
a	O
fuss	O
about	O
nothing	O
?	O
if	O
you	O
are	O
sceptical	O
about	O
the	O
dogmatic	O
views	O
i	O
have	O
expressed	O
,	O
i	O
encourage	O
you	O
to	O
look	O
at	O
a	O
case	O
study	O
:	O
look	O
in	O
depth	O
at	O
exercise	O
35.4	O
(	O
p.446	O
)	O
and	O
the	O
reference	O
(	O
kepler	O
and	O
oprea	O
,	O
2001	O
)	O
,	O
in	O
which	O
sampling	B
theory	I
estimates	O
and	O
con	O
(	O
cid:12	O
)	O
dence	O
intervals	B
for	O
a	O
mutation	B
rate	I
are	O
constructed	O
.	O
try	O
both	O
methods	B
on	O
simulated	O
data	O
{	O
the	O
bayesian	O
approach	O
based	O
on	O
simply	O
computing	O
the	O
likelihood	B
function	O
,	O
and	O
the	O
con	O
(	O
cid:12	O
)	O
dence	O
interval	O
from	O
sampling	O
theory	B
;	O
and	O
let	O
me	O
know	O
if	O
you	O
don	O
’	O
t	O
(	O
cid:12	O
)	O
nd	O
that	O
the	O
bayesian	O
answer	O
is	O
always	O
better	O
than	O
the	O
sampling	B
theory	I
answer	O
;	O
and	O
often	O
much	O
,	O
much	O
better	O
.	O
this	O
suboptimality	O
of	O
sampling	O
theory	B
,	O
achieved	O
with	O
great	O
e	O
(	O
cid:11	O
)	O
ort	O
,	O
is	O
why	O
i	O
am	O
passionate	O
about	O
bayesian	O
methods	B
.	O
bayesian	O
methods	B
are	O
straightforward	O
,	O
and	O
they	O
optimally	O
use	O
all	O
the	O
information	B
in	O
the	O
data	O
.	O
37.4	O
some	O
compromise	O
positions	O
let	O
’	O
s	O
end	O
on	O
a	O
conciliatory	O
note	O
.	O
many	O
sampling	O
theorists	O
are	O
pragmatic	O
{	O
they	O
are	O
happy	O
to	O
choose	O
from	O
a	O
selection	O
of	O
statistical	O
methods	B
,	O
choosing	O
whichever	O
has	O
the	O
‘	O
best	O
’	O
long-run	O
properties	O
.	O
in	O
contrast	O
,	O
i	O
have	O
no	O
problem	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
466	O
37	O
|	O
bayesian	O
inference	B
and	O
sampling	B
theory	I
with	O
the	O
idea	O
that	O
there	O
is	O
only	O
one	O
answer	O
to	O
a	O
well-posed	O
problem	O
;	O
but	O
it	O
’	O
s	O
not	O
essential	O
to	O
convert	O
sampling	O
theorists	O
to	O
this	O
viewpoint	O
:	O
instead	O
,	O
we	O
can	O
o	O
(	O
cid:11	O
)	O
er	O
them	O
bayesian	O
estimators	O
and	O
bayesian	O
con	O
(	O
cid:12	O
)	O
dence	O
intervals	B
,	O
and	O
request	O
that	O
the	O
sampling	O
theoretical	O
properties	O
of	O
these	O
methods	B
be	O
evaluated	O
.	O
we	O
don	O
’	O
t	O
need	O
to	O
mention	O
that	O
the	O
methods	B
are	O
derived	O
from	O
a	O
bayesian	O
per-	O
spective	O
.	O
if	O
the	O
sampling	O
properties	O
are	O
good	B
then	O
the	O
pragmatic	O
sampling	O
theorist	O
will	O
choose	O
to	O
use	O
the	O
bayesian	O
methods	B
.	O
it	O
is	O
indeed	O
the	O
case	O
that	O
many	O
bayesian	O
methods	B
have	O
good	B
sampling-theoretical	O
properties	O
.	O
perhaps	O
it	O
’	O
s	O
not	O
surprising	O
that	O
a	O
method	B
that	O
gives	O
the	O
optimal	B
answer	O
for	O
each	O
indi-	O
vidual	O
case	O
should	O
also	O
be	O
good	B
in	O
the	O
long	O
run	O
!	O
another	O
piece	O
of	O
common	O
ground	O
can	O
be	O
conceded	O
:	O
while	O
i	O
believe	O
that	O
most	O
well-posed	O
inference	B
problems	O
have	O
a	O
unique	O
correct	O
answer	O
,	O
which	O
can	O
be	O
found	O
by	O
bayesian	O
methods	B
,	O
not	O
all	O
problems	O
are	O
well-posed	O
.	O
a	O
common	O
question	O
arising	O
in	O
data	O
modelling	B
is	O
‘	O
am	O
i	O
using	O
an	O
appropriate	O
model	B
?	O
’	O
model	B
criticism	O
,	O
that	O
is	O
,	O
hunting	O
for	O
defects	O
in	O
a	O
current	O
model	B
,	O
is	O
a	O
task	O
that	O
may	O
be	O
aided	O
by	O
sampling	O
theory	B
tests	O
,	O
in	O
which	O
the	O
null	O
hypothesis	O
(	O
‘	O
the	O
current	O
model	B
is	O
correct	O
’	O
)	O
is	O
well	O
de	O
(	O
cid:12	O
)	O
ned	O
,	O
but	O
the	O
alternative	O
model	B
is	O
not	O
speci	O
(	O
cid:12	O
)	O
ed	O
.	O
one	O
could	O
use	O
sampling	B
theory	I
measures	O
such	O
as	O
p-values	O
to	O
guide	O
one	O
’	O
s	O
search	O
for	O
the	O
aspects	O
of	O
the	O
model	O
most	O
in	O
need	O
of	O
scrutiny	O
.	O
further	O
reading	O
my	O
favourite	O
reading	O
on	O
this	O
topic	O
includes	O
(	O
jaynes	O
,	O
1983	O
;	O
gull	O
,	O
1988	O
;	O
loredo	O
,	O
1990	O
;	O
berger	O
,	O
1985	O
;	O
jaynes	O
,	O
2003	O
)	O
.	O
treatises	O
on	O
bayesian	O
statistics	O
from	O
the	O
statistics	O
community	O
include	O
(	O
box	B
and	O
tiao	O
,	O
1973	O
;	O
o	O
’	O
hagan	O
,	O
1994	O
)	O
.	O
37.5	O
further	O
exercises	O
.	O
exercise	O
37.2	O
.	O
[	O
3c	O
]	O
a	O
tra	O
(	O
cid:14	O
)	O
c	O
survey	O
records	O
tra	O
(	O
cid:14	O
)	O
c	O
on	O
two	O
successive	O
days	O
.	O
on	O
friday	O
morning	O
,	O
there	O
are	O
12	O
vehicles	O
in	O
one	O
hour	O
.	O
on	O
saturday	O
morn-	O
ing	O
,	O
there	O
are	O
9	O
vehicles	O
in	O
half	O
an	O
hour	O
.	O
assuming	O
that	O
the	O
vehicles	O
are	O
poisson	O
distributed	O
with	O
rates	O
(	O
cid:21	O
)	O
f	O
and	O
(	O
cid:21	O
)	O
s	O
(	O
in	O
vehicles	O
per	O
hour	O
)	O
respec-	O
tively	O
,	O
(	O
a	O
)	O
is	O
(	O
cid:21	O
)	O
s	O
greater	O
than	O
(	O
cid:21	O
)	O
f	O
?	O
(	O
b	O
)	O
by	O
what	O
factor	O
is	O
(	O
cid:21	O
)	O
s	O
bigger	O
or	O
smaller	O
than	O
(	O
cid:21	O
)	O
f	O
?	O
.	O
exercise	O
37.3	O
.	O
[	O
3c	O
]	O
write	O
a	O
program	O
to	O
compare	O
treatments	O
a	O
and	O
b	O
given	O
data	O
fa+	O
,	O
fa	O
(	O
cid:0	O
)	O
,	O
fb+	O
,	O
fb	O
(	O
cid:0	O
)	O
as	O
described	O
in	O
section	O
37.1.	O
the	O
outputs	O
of	O
the	O
program	O
should	O
be	O
(	O
a	O
)	O
the	O
probability	B
that	O
treatment	O
a	O
is	O
more	O
e	O
(	O
cid:11	O
)	O
ective	O
than	O
treatment	O
b	O
;	O
(	O
b	O
)	O
the	O
probability	B
that	O
pa+	O
<	O
10	O
pb+	O
;	O
(	O
c	O
)	O
the	O
probability	B
that	O
pb+	O
<	O
10	O
pa+	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
part	O
v	O
neural	O
networks	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
38	O
introduction	O
to	O
neural	O
networks	O
in	O
the	O
(	O
cid:12	O
)	O
eld	O
of	O
neural	O
networks	O
,	O
we	O
study	O
the	O
properties	O
of	O
networks	O
of	O
idealized	O
‘	O
neurons	O
’	O
.	O
three	O
motivations	O
underlie	O
work	O
in	O
this	O
broad	O
and	O
interdisciplinary	O
(	O
cid:12	O
)	O
eld	O
.	O
biology	O
.	O
the	O
task	O
of	O
understanding	O
how	O
the	O
brain	B
works	O
is	O
one	O
of	O
the	O
out-	O
standing	O
unsolved	O
problems	O
in	O
science	O
.	O
some	O
neural	B
network	I
models	O
are	O
intended	O
to	O
shed	O
light	O
on	O
the	O
way	O
in	O
which	O
computation	O
and	O
memory	O
are	O
performed	O
by	O
brains	O
.	O
engineering	O
.	O
many	O
researchers	O
would	O
like	O
to	O
create	O
machines	O
that	O
can	O
‘	O
learn	O
’	O
,	O
perform	O
‘	O
pattern	B
recognition	I
’	O
or	O
‘	O
discover	O
patterns	O
in	O
data	O
’	O
.	O
complex	B
systems	O
.	O
a	O
third	O
motivation	O
for	O
being	O
interested	O
in	O
neural	O
net-	O
works	O
is	O
that	O
they	O
are	O
complex	B
adaptive	O
systems	O
whose	O
properties	O
are	O
interesting	O
in	O
their	O
own	O
right	O
.	O
i	O
should	O
emphasize	O
several	O
points	O
at	O
the	O
outset	O
.	O
(	O
cid:15	O
)	O
this	O
book	O
gives	O
only	O
a	O
taste	O
of	O
this	O
(	O
cid:12	O
)	O
eld	O
.	O
there	O
are	O
many	O
interesting	O
neural	B
network	I
models	O
which	O
we	O
will	O
not	O
have	O
time	O
to	O
touch	O
on	O
.	O
(	O
cid:15	O
)	O
the	O
models	O
that	O
we	O
discuss	O
are	O
not	O
intended	O
to	O
be	O
faithful	O
models	O
of	O
biological	O
systems	O
.	O
if	O
they	O
are	O
at	O
all	O
relevant	O
to	O
biology	O
,	O
their	O
relevance	O
is	O
on	O
an	O
abstract	O
level	O
.	O
(	O
cid:15	O
)	O
i	O
will	O
describe	O
some	O
neural	B
network	I
methods	O
that	O
are	O
widely	O
used	O
in	O
nonlinear	O
data	B
modelling	I
,	O
but	O
i	O
will	O
not	O
be	O
able	O
to	O
give	O
a	O
full	O
description	O
of	O
the	O
state	O
of	O
the	O
art	O
.	O
if	O
you	O
wish	O
to	O
solve	O
real	O
problems	O
with	O
neural	O
networks	O
,	O
please	O
read	O
the	O
relevant	O
papers	O
.	O
38.1	O
memories	O
in	O
the	O
next	O
few	O
chapters	O
we	O
will	O
meet	O
several	O
neural	B
network	I
models	O
which	O
come	O
with	O
simple	O
learning	B
algorithms	I
which	O
make	O
them	O
function	B
as	O
memories	O
.	O
perhaps	O
we	O
should	O
dwell	O
for	O
a	O
moment	O
on	O
the	O
conventional	O
idea	O
of	O
memory	O
in	O
digital	O
computation	O
.	O
a	O
memory	B
(	O
a	O
string	O
of	O
5000	O
bits	O
describing	O
the	O
name	O
of	O
a	O
person	O
and	O
an	O
image	B
of	O
their	O
face	O
,	O
say	O
)	O
is	O
stored	O
in	O
a	O
digital	O
computer	O
at	O
an	O
address	B
.	O
to	O
retrieve	O
the	O
memory	B
you	O
need	O
to	O
know	O
the	O
address	B
.	O
the	O
address	B
has	O
nothing	O
to	O
do	O
with	O
the	O
memory	B
itself	O
.	O
notice	O
the	O
properties	O
that	O
this	O
scheme	O
does	O
not	O
have	O
:	O
1.	O
address-based	B
memory	O
is	O
not	O
associative	B
.	O
imagine	O
you	O
know	O
half	O
of	O
a	O
memory	B
,	O
say	O
someone	O
’	O
s	O
face	O
,	O
and	O
you	O
would	O
like	O
to	O
recall	O
the	O
rest	O
of	O
the	O
468	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
38.1	O
:	O
memories	O
469	O
memory	B
{	O
their	O
name	O
.	O
if	O
your	O
memory	B
is	O
address-based	B
then	O
you	O
can	O
’	O
t	O
get	O
at	O
a	O
memory	B
without	O
knowing	O
the	O
address	B
.	O
[	O
computer	B
scientists	O
have	O
devoted	O
e	O
(	O
cid:11	O
)	O
ort	O
to	O
wrapping	O
traditional	O
address-based	B
memories	O
inside	O
cunning	O
software	B
to	O
produce	O
content-addressable	B
memories	O
,	O
but	O
content-	O
addressability	O
does	O
not	O
come	O
naturally	O
.	O
it	O
has	O
to	O
be	O
added	O
on	O
.	O
]	O
2.	O
address-based	B
memory	O
is	O
not	O
robust	O
or	O
fault-tolerant	O
.	O
if	O
a	O
one-bit	O
mis-	O
take	O
is	O
made	O
in	O
specifying	O
the	O
address	B
then	O
a	O
completely	O
di	O
(	O
cid:11	O
)	O
erent	O
mem-	O
ory	O
will	O
be	O
retrieved	O
.	O
if	O
one	O
bit	B
of	O
a	O
memory	B
is	O
(	O
cid:13	O
)	O
ipped	O
then	O
whenever	O
that	O
memory	B
is	O
retrieved	O
the	O
error	O
will	O
be	O
present	O
.	O
of	O
course	O
,	O
in	O
all	O
mod-	O
ern	O
computers	O
,	O
error-correcting	B
codes	I
are	O
used	O
in	O
the	O
memory	B
,	O
so	O
that	O
small	O
numbers	O
of	O
errors	O
can	O
be	O
detected	O
and	O
corrected	O
.	O
but	O
this	O
error-	O
tolerance	O
is	O
not	O
an	O
intrinsic	O
property	O
of	O
the	O
memory	O
system	O
.	O
if	O
minor	O
damage	O
occurs	O
to	O
certain	O
hardware	O
that	O
implements	O
memory	B
retrieval	O
,	O
it	O
is	O
likely	O
that	O
all	O
functionality	O
will	O
be	O
catastrophically	O
lost	O
.	O
3.	O
address-based	B
memory	O
is	O
not	O
distributed	O
.	O
in	O
a	O
serial	O
computer	B
that	O
is	O
accessing	O
a	O
particular	O
memory	B
,	O
only	O
a	O
tiny	O
fraction	O
of	O
the	O
devices	O
participate	O
in	O
the	O
memory	B
recall	O
:	O
the	O
cpu	O
and	O
the	O
circuits	O
that	O
are	O
storing	O
the	O
required	O
byte	B
.	O
all	O
the	O
other	O
millions	O
of	O
devices	O
in	O
the	O
machine	O
are	O
sitting	O
idle	O
.	O
are	O
there	O
models	O
of	O
truly	O
parallel	O
computation	O
,	O
in	O
which	O
multiple	O
de-	O
vices	O
participate	O
in	O
all	O
computations	O
?	O
[	O
present-day	O
parallel	O
computers	O
scarcely	O
di	O
(	O
cid:11	O
)	O
er	O
from	O
serial	O
computers	O
from	O
this	O
point	O
of	O
view	O
.	O
memory	B
retrieval	O
works	O
in	O
just	O
the	O
same	O
way	O
,	O
and	O
control	O
of	O
the	O
computation	O
process	O
resides	O
in	O
cpus	O
.	O
there	O
are	O
simply	O
a	O
few	O
more	O
cpus	O
.	O
most	O
of	O
the	O
devices	O
sit	O
idle	O
most	O
of	O
the	O
time	O
.	O
]	O
biological	O
memory	B
systems	O
are	O
completely	O
di	O
(	O
cid:11	O
)	O
erent	O
.	O
1.	O
biological	O
memory	B
is	O
associative	B
.	O
memory	B
recall	O
is	O
content-addressable	B
.	O
given	O
a	O
person	O
’	O
s	O
name	O
,	O
we	O
can	O
often	O
recall	O
their	O
face	O
;	O
and	O
vice	O
versa	O
.	O
memories	O
are	O
apparently	O
recalled	O
spontaneously	O
,	O
not	O
just	O
at	O
the	O
request	O
of	O
some	O
cpu	O
.	O
2.	O
biological	O
memory	B
recall	O
is	O
error-tolerant	O
and	O
robust	O
.	O
(	O
cid:15	O
)	O
errors	B
in	O
the	O
cues	O
for	O
memory	O
recall	O
can	O
be	O
corrected	O
.	O
an	O
example	O
asks	O
you	O
to	O
recall	O
‘	O
an	O
american	O
politician	O
who	O
was	O
very	O
intelligent	O
and	O
whose	O
politician	O
father	O
did	O
not	O
like	O
broccoli	O
’	O
.	O
many	O
people	O
think	O
of	O
president	O
bush	O
{	O
even	O
though	O
one	O
of	O
the	O
cues	O
contains	O
an	O
error	O
.	O
(	O
cid:15	O
)	O
hardware	O
faults	O
can	O
also	O
be	O
tolerated	O
.	O
our	O
brains	O
are	O
noisy	B
lumps	O
of	O
meat	O
that	O
are	O
in	O
a	O
continual	O
state	O
of	O
change	O
,	O
with	O
cells	O
being	O
damaged	O
by	O
natural	O
processes	O
,	O
alcohol	O
,	O
and	O
boxing	O
.	O
while	O
the	O
cells	O
in	O
our	O
brains	O
and	O
the	O
proteins	O
in	O
our	O
cells	O
are	O
continually	O
changing	O
,	O
many	O
of	O
our	O
memories	O
persist	O
una	O
(	O
cid:11	O
)	O
ected	O
.	O
3.	O
biological	O
memory	B
is	O
parallel	O
and	O
distributed	O
{	O
not	O
completely	O
distributed	O
throughout	O
the	O
whole	O
brain	B
:	O
there	O
does	O
appear	O
to	O
be	O
some	O
functional	O
specialization	O
{	O
but	O
in	O
the	O
parts	O
of	O
the	O
brain	O
where	O
memories	O
are	O
stored	O
,	O
it	O
seems	O
that	O
many	O
neurons	O
participate	O
in	O
the	O
storage	O
of	O
multiple	O
mem-	O
ories	O
.	O
these	O
properties	O
of	O
biological	O
memory	B
systems	O
motivate	O
the	O
study	O
of	O
‘	O
arti-	O
(	O
cid:12	O
)	O
cial	O
neural	O
networks	O
’	O
{	O
parallel	O
distributed	O
computational	O
systems	O
consisting	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
470	O
38	O
|	O
introduction	O
to	O
neural	O
networks	O
of	O
many	O
interacting	O
simple	O
elements	O
.	O
the	O
hope	O
is	O
that	O
these	O
model	B
systems	O
might	O
give	O
some	O
hints	O
as	O
to	O
how	O
neural	O
computation	O
is	O
achieved	O
in	O
real	O
bio-	O
logical	O
neural	O
networks	O
.	O
38.2	O
terminology	B
each	O
time	O
we	O
describe	O
a	O
neural	B
network	I
algorithm	O
we	O
will	O
typically	O
specify	O
three	O
things	O
.	O
[	O
if	O
any	O
of	O
this	O
terminology	B
is	O
hard	O
to	O
understand	O
,	O
it	O
’	O
s	O
probably	O
best	O
to	O
dive	O
straight	O
into	O
the	O
next	O
chapter	O
.	O
]	O
architecture	B
.	O
the	O
architecture	B
speci	O
(	O
cid:12	O
)	O
es	O
what	O
variables	O
are	O
involved	O
in	O
the	O
network	B
and	O
their	O
topological	O
relationships	O
{	O
for	O
example	O
,	O
the	O
variables	O
involved	O
in	O
a	O
neural	O
net	O
might	O
be	O
the	O
weights	O
of	O
the	O
connections	O
between	O
the	O
neurons	O
,	O
along	O
with	O
the	O
activities	O
of	O
the	O
neurons	O
.	O
activity	B
rule	I
.	O
most	O
neural	O
network	B
models	O
have	O
short	O
time-scale	O
dynamics	O
:	O
local	O
rules	B
de	O
(	O
cid:12	O
)	O
ne	O
how	O
the	O
activities	O
of	O
the	O
neurons	O
change	O
in	O
response	O
to	O
each	O
other	O
.	O
typically	O
the	O
activity	B
rule	I
depends	O
on	O
the	O
weights	O
(	O
the	O
parameters	B
)	O
in	O
the	O
network	B
.	O
learning	B
rule	I
.	O
the	O
learning	B
rule	I
speci	O
(	O
cid:12	O
)	O
es	O
the	O
way	O
in	O
which	O
the	O
neural	O
net-	O
work	O
’	O
s	O
weights	O
change	O
with	O
time	O
.	O
this	O
learning	B
is	O
usually	O
viewed	O
as	O
taking	O
place	O
on	O
a	O
longer	O
time	O
scale	O
than	O
the	O
time	O
scale	O
of	O
the	O
dynamics	O
under	O
the	O
activity	B
rule	I
.	O
usually	O
the	O
learning	B
rule	I
will	O
depend	O
on	O
the	O
activities	O
of	O
the	O
neurons	O
.	O
it	O
may	O
also	O
depend	O
on	O
the	O
values	O
of	O
target	O
values	O
supplied	O
by	O
a	O
teacher	O
and	O
on	O
the	O
current	O
value	O
of	O
the	O
weights	O
.	O
where	O
do	O
these	O
rules	B
come	O
from	O
?	O
often	O
,	O
activity	O
rules	O
and	B
learning	I
rules	O
are	O
invented	O
by	O
imaginative	O
researchers	O
.	O
alternatively	O
,	O
activity	O
rules	O
and	B
learning	I
rules	O
may	O
be	O
derived	O
from	O
carefully	O
chosen	O
objective	O
functions	O
.	O
neural	B
network	I
algorithms	O
can	O
be	O
roughly	O
divided	O
into	O
two	O
classes	O
.	O
supervised	O
neural	O
networks	O
are	O
given	O
data	O
in	O
the	O
form	O
of	O
inputs	O
and	O
tar-	O
gets	O
,	O
the	O
targets	O
being	O
a	O
teacher	O
’	O
s	O
speci	O
(	O
cid:12	O
)	O
cation	O
of	O
what	O
the	O
neural	O
net-	O
work	O
’	O
s	O
response	O
to	O
the	O
input	O
should	O
be	O
.	O
unsupervised	O
neural	O
networks	O
are	O
given	O
data	O
in	O
an	O
undivided	O
form	O
{	O
sim-	O
ply	O
a	O
set	B
of	O
examples	O
fxg	O
.	O
some	O
learning	B
algorithms	I
are	O
intended	O
simply	O
to	O
memorize	O
these	O
data	O
in	O
such	O
a	O
way	O
that	O
the	O
examples	O
can	O
be	O
recalled	O
in	O
the	O
future	O
.	O
other	O
algorithms	B
are	O
intended	O
to	O
‘	O
generalize	O
’	O
,	O
to	O
discover	O
‘	O
patterns	O
’	O
in	O
the	O
data	O
,	O
or	O
extract	O
the	O
underlying	O
‘	O
features	O
’	O
from	O
them	O
.	O
some	O
unsupervised	O
algorithms	B
are	O
able	O
to	O
make	O
predictions	O
{	O
for	O
exam-	O
ple	O
,	O
some	O
algorithms	B
can	O
‘	O
(	O
cid:12	O
)	O
ll	O
in	O
’	O
missing	O
variables	O
in	O
an	O
example	O
x	O
{	O
and	O
so	O
can	O
also	O
be	O
viewed	O
as	O
supervised	O
networks	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
39	O
the	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
39.1	O
the	O
single	B
neuron	I
we	O
will	O
study	O
a	O
single	B
neuron	I
for	O
two	O
reasons	O
.	O
first	O
,	O
many	O
neural	B
network	I
models	O
are	O
built	O
out	O
of	O
single	O
neurons	O
,	O
so	O
it	O
is	O
good	B
to	O
understand	O
them	O
in	O
detail	O
.	O
and	O
second	O
,	O
a	O
single	B
neuron	I
is	O
itself	O
capable	O
of	O
‘	O
learning	B
’	O
{	O
indeed	O
,	O
various	O
standard	O
statistical	O
methods	B
can	O
be	O
viewed	O
in	O
terms	O
of	O
single	O
neurons	O
{	O
so	O
this	O
model	B
will	O
serve	O
as	O
a	O
(	O
cid:12	O
)	O
rst	O
example	O
of	O
a	O
supervised	O
neural	O
network	B
.	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
a	O
single	B
neuron	I
we	O
will	O
start	O
by	O
de	O
(	O
cid:12	O
)	O
ning	O
the	O
architecture	B
and	O
the	O
activity	B
rule	I
of	O
a	O
single	B
neuron	I
,	O
and	O
we	O
will	O
then	O
derive	O
a	O
learning	B
rule	I
.	O
architecture	B
.	O
a	O
single	B
neuron	I
has	O
a	O
number	O
i	O
of	O
inputs	O
xi	O
and	O
one	O
output	O
which	O
we	O
will	O
here	O
call	O
y	O
.	O
(	O
see	O
(	O
cid:12	O
)	O
gure	O
39.1	O
.	O
)	O
associated	O
with	O
each	O
input	O
is	O
a	O
weight	B
wi	O
(	O
i	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
i	O
)	O
.	O
there	O
may	O
be	O
an	O
additional	O
parameter	O
w0	O
of	O
the	O
neuron	O
called	O
a	O
bias	B
which	O
we	O
may	O
view	O
as	O
being	O
the	O
weight	B
associated	O
with	O
an	O
input	O
x0	O
that	O
is	O
permanently	O
set	B
to	O
1.	O
the	O
single	B
neuron	I
is	O
a	O
feedforward	O
device	O
{	O
the	O
connections	O
are	O
directed	O
from	O
the	O
inputs	O
to	O
the	O
output	O
of	O
the	O
neuron	O
.	O
activity	B
rule	I
.	O
the	O
activity	B
rule	I
has	O
two	O
steps	O
.	O
1.	O
first	O
,	O
in	O
response	O
to	O
the	O
imposed	O
inputs	O
x	O
,	O
we	O
compute	O
the	O
activa-	O
tion	O
of	O
the	O
neuron	O
,	O
a	O
=xi	O
wixi	O
;	O
(	O
39.1	O
)	O
where	O
the	O
sum	O
is	O
over	O
i	O
=	O
0	O
;	O
:	O
:	O
:	O
;	O
i	O
if	O
there	O
is	O
a	O
bias	B
and	O
i	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
i	O
otherwise	O
.	O
2.	O
second	O
,	O
the	O
output	O
y	O
is	O
set	B
as	O
a	O
function	B
f	O
(	O
a	O
)	O
of	O
the	O
activation	O
.	O
the	O
output	O
is	O
also	O
called	O
the	O
activity	O
of	O
the	O
neuron	B
,	O
not	O
to	O
be	O
confused	O
with	O
the	O
activation	O
a.	O
there	O
are	O
several	O
possible	O
activation	O
functions	O
;	O
here	O
are	O
the	O
most	O
popular	O
.	O
(	O
a	O
)	O
deterministic	B
activation	O
functions	B
:	O
i.	O
linear	B
.	O
ii	O
.	O
sigmoid	B
(	O
logistic	O
function	B
)	O
.	O
y	O
(	O
a	O
)	O
=	O
a	O
:	O
(	O
39.2	O
)	O
y	O
(	O
a	O
)	O
=	O
1	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
a	O
(	O
y	O
2	O
(	O
0	O
;	O
1	O
)	O
)	O
:	O
(	O
39.3	O
)	O
471	O
b	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
w0	O
(	O
cid:18	O
)	O
(	O
cid:17	O
)	O
(	O
cid:19	O
)	O
(	O
cid:16	O
)	O
6	O
b	O
b	O
b	O
b	O
(	O
cid:1	O
)	O
w1	O
(	O
cid:1	O
)	O
x1	O
a	O
wi	O
a	O
xi	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
aa	O
ee	O
e	O
a	O
:	O
:	O
:	O
e	O
e	O
y	O
(	O
cid:1	O
)	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
figure	O
39.1.	O
a	O
single	B
neuron	I
activation	O
activity	O
a	O
!	O
y	O
(	O
a	O
)	O
1	O
0	O
-5	O
0	O
5	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
472	O
39	O
|	O
the	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
iii	O
.	O
sigmoid	B
(	O
tanh	O
)	O
.	O
y	O
(	O
a	O
)	O
=	O
tanh	O
(	O
a	O
)	O
(	O
y	O
2	O
(	O
(	O
cid:0	O
)	O
1	O
;	O
1	O
)	O
)	O
:	O
(	O
39.4	O
)	O
iv	O
.	O
threshold	B
function	O
.	O
y	O
(	O
a	O
)	O
=	O
(	O
cid:2	O
)	O
(	O
a	O
)	O
(	O
cid:17	O
)	O
(	O
cid:26	O
)	O
1	O
a	O
>	O
0	O
(	O
cid:0	O
)	O
1	O
a	O
(	O
cid:20	O
)	O
0	O
:	O
(	O
39.5	O
)	O
(	O
b	O
)	O
stochastic	B
activation	O
functions	B
:	O
y	O
is	O
stochastically	O
selected	O
from	O
(	O
cid:6	O
)	O
1.	O
i.	O
heat	B
bath	I
.	O
1	O
0	O
-1	O
1	O
0	O
-1	O
-5	O
-5	O
0	O
0	O
5	O
5	O
y	O
(	O
a	O
)	O
=8	O
<	O
:	O
with	O
probability	O
1	O
(	O
cid:0	O
)	O
1	O
otherwise	O
.	O
1	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
a	O
(	O
39.6	O
)	O
ii	O
.	O
the	O
metropolis	O
rule	O
produces	O
the	O
output	O
in	O
a	O
way	O
that	O
depends	O
on	O
the	O
previous	O
output	O
state	O
y	O
:	O
compute	O
(	O
cid:1	O
)	O
=	O
ay	O
if	O
(	O
cid:1	O
)	O
(	O
cid:20	O
)	O
0	O
;	O
(	O
cid:13	O
)	O
ip	O
y	O
to	O
the	O
other	O
state	O
else	O
(	O
cid:13	O
)	O
ip	O
y	O
to	O
the	O
other	O
state	O
with	O
probability	B
e	O
(	O
cid:0	O
)	O
(	O
cid:1	O
)	O
.	O
39.2	O
basic	O
neural	B
network	I
concepts	O
a	O
neural	B
network	I
implements	O
a	O
function	B
y	O
(	O
x	O
;	O
w	O
)	O
;	O
the	O
‘	O
output	O
’	O
of	O
the	O
network	O
,	O
y	O
,	O
is	O
a	O
nonlinear	B
function	O
of	O
the	O
‘	O
inputs	O
’	O
x	O
;	O
this	O
function	B
is	O
parameterized	O
by	O
‘	O
weights	O
’	O
w.	O
we	O
will	O
study	O
a	O
single	B
neuron	I
which	O
produces	O
an	O
output	O
between	O
0	O
and	O
1	O
as	O
the	O
following	O
function	B
of	O
x	O
:	O
y	O
(	O
x	O
;	O
w	O
)	O
=	O
1	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
w	O
(	O
cid:1	O
)	O
x	O
:	O
(	O
39.7	O
)	O
exercise	O
39.1	O
.	O
[	O
1	O
]	O
in	O
what	O
contexts	O
have	O
we	O
encountered	O
the	O
function	B
y	O
(	O
x	O
;	O
w	O
)	O
=	O
1=	O
(	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
w	O
(	O
cid:1	O
)	O
x	O
)	O
already	O
?	O
motivations	O
for	O
the	O
linear	B
logistic	O
function	B
in	O
section	B
11.2	O
we	O
studied	O
‘	O
the	O
best	O
detection	O
of	O
pulses	O
’	O
,	O
assuming	O
that	O
one	O
of	O
two	O
signals	O
x0	O
and	O
x1	O
had	O
been	O
transmitted	O
over	O
a	O
gaussian	O
channel	O
with	O
variance	O
{	O
covariance	B
matrix	I
a	O
(	O
cid:0	O
)	O
1.	O
we	O
found	O
that	O
the	O
probability	B
that	O
the	O
source	O
signal	O
was	O
s	O
=	O
1	O
rather	O
than	O
s	O
=	O
0	O
,	O
given	O
the	O
received	O
signal	O
y	O
,	O
was	O
p	O
(	O
s	O
=	O
1j	O
y	O
)	O
=	O
1	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
a	O
(	O
y	O
)	O
)	O
;	O
(	O
39.8	O
)	O
where	O
a	O
(	O
y	O
)	O
was	O
a	O
linear	B
function	O
of	O
the	O
received	O
vector	O
,	O
a	O
(	O
y	O
)	O
=	O
wty	O
+	O
(	O
cid:18	O
)	O
;	O
(	O
39.9	O
)	O
with	O
w	O
(	O
cid:17	O
)	O
a	O
(	O
x1	O
(	O
cid:0	O
)	O
x0	O
)	O
.	O
the	O
exercises	O
.	O
the	O
linear	B
logistic	O
function	B
can	O
be	O
motivated	O
in	O
several	O
other	O
ways	O
{	O
see	O
 	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
39.2	O
:	O
basic	O
neural	B
network	I
concepts	O
473	O
figure	O
39.2.	O
output	O
of	O
a	O
simple	O
neural	O
network	B
as	O
a	O
function	B
of	O
its	O
input	O
.	O
1	O
0.5	O
-10	O
-5	O
10	O
5	O
x2	O
0	O
0	O
x1	O
5	O
10	O
-5	O
-10	O
w	O
=	O
(	O
0	O
;	O
2	O
)	O
input	O
space	O
and	O
weight	O
space	O
for	O
convenience	O
let	O
us	O
study	O
the	O
case	O
where	O
the	O
input	O
vector	O
x	O
and	O
the	O
param-	O
eter	O
vector	O
w	O
are	O
both	O
two-dimensional	B
:	O
x	O
=	O
(	O
x1	O
;	O
x2	O
)	O
,	O
w	O
=	O
(	O
w1	O
;	O
w2	O
)	O
.	O
then	O
we	O
can	O
spell	B
out	O
the	O
function	B
performed	O
by	O
the	O
neuron	B
thus	O
:	O
y	O
(	O
x	O
;	O
w	O
)	O
=	O
1	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
(	O
w1x1+w2x2	O
)	O
:	O
(	O
39.10	O
)	O
figure	O
39.2	O
shows	O
the	O
output	O
of	O
the	O
neuron	O
as	O
a	O
function	B
of	O
the	O
input	O
vector	O
,	O
for	O
w	O
=	O
(	O
0	O
;	O
2	O
)	O
.	O
the	O
two	O
horizontal	O
axes	O
of	O
this	O
(	O
cid:12	O
)	O
gure	O
are	O
the	O
inputs	O
x1	O
and	O
x2	O
,	O
with	O
the	O
output	O
y	O
on	O
the	O
vertical	O
axis	O
.	O
notice	O
that	O
on	O
any	O
line	O
perpendicular	O
to	O
w	O
,	O
the	O
output	O
is	O
constant	O
;	O
and	O
along	O
a	O
line	O
in	O
the	O
direction	O
of	O
w	O
,	O
the	O
output	O
is	O
a	O
sigmoid	B
function	O
.	O
we	O
now	O
introduce	O
the	O
idea	O
of	O
weight	O
space	O
,	O
that	O
is	O
,	O
the	O
parameter	O
space	O
of	O
the	O
network	O
.	O
in	O
this	O
case	O
,	O
there	O
are	O
two	O
parameters	B
w1	O
and	O
w2	O
,	O
so	O
the	O
weight	B
space	I
is	O
two	O
dimensional	O
.	O
this	O
weight	B
space	I
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
39.3.	O
for	O
a	O
selection	O
of	O
values	O
of	O
the	O
parameter	O
vector	O
w	O
,	O
smaller	O
inset	O
(	O
cid:12	O
)	O
gures	O
show	O
the	O
function	B
of	O
x	O
performed	O
by	O
the	O
network	B
when	O
w	O
is	O
set	B
to	O
those	O
values	O
.	O
each	O
of	O
these	O
smaller	O
(	O
cid:12	O
)	O
gures	O
is	O
equivalent	O
to	O
(	O
cid:12	O
)	O
gure	O
39.2.	O
thus	O
each	O
point	O
in	O
w	O
space	O
corresponds	O
to	O
a	O
function	B
of	O
x.	O
notice	O
that	O
the	O
gain	B
of	O
the	O
sigmoid	B
function	O
(	O
the	O
gradient	O
of	O
the	O
ramp	O
)	O
increases	O
as	O
the	O
magnitude	O
of	O
w	O
increases	O
.	O
now	O
,	O
the	O
central	O
idea	O
of	O
supervised	O
neural	O
networks	O
is	O
this	O
.	O
given	O
examples	O
of	O
a	O
relationship	O
between	O
an	O
input	O
vector	O
x	O
,	O
and	O
a	O
target	O
t	O
,	O
we	O
hope	O
to	O
make	O
the	O
neural	B
network	I
‘	O
learn	O
’	O
a	O
model	B
of	O
the	O
relationship	O
between	O
x	O
and	O
t.	O
a	O
successfully	O
trained	O
network	B
will	O
,	O
for	O
any	O
given	O
x	O
,	O
give	O
an	O
output	O
y	O
that	O
is	O
close	O
(	O
in	O
some	O
sense	O
)	O
to	O
the	O
target	O
value	O
t.	O
training	O
the	O
network	B
involves	O
searching	O
in	O
the	O
weight	B
space	I
of	O
the	O
network	B
for	O
a	O
value	O
of	O
w	O
that	O
produces	O
a	O
function	B
that	O
(	O
cid:12	O
)	O
ts	O
the	O
provided	O
training	B
data	I
well	O
.	O
typically	O
an	O
objective	B
function	I
or	O
error	B
function	I
is	O
de	O
(	O
cid:12	O
)	O
ned	O
,	O
as	O
a	O
function	B
of	O
w	O
,	O
to	O
measure	O
how	O
well	O
the	O
network	B
with	O
weights	O
set	B
to	O
w	O
solves	O
the	O
task	O
.	O
the	O
objective	B
function	I
is	O
a	O
sum	O
of	O
terms	O
,	O
one	O
for	O
each	O
input/target	O
pair	O
fx	O
;	O
tg	O
,	O
measuring	O
how	O
close	O
the	O
output	O
y	O
(	O
x	O
;	O
w	O
)	O
is	O
to	O
the	O
target	O
t.	O
the	O
training	O
process	O
is	O
an	O
exercise	O
in	O
function	O
minimization	B
{	O
i.e.	O
,	O
adjusting	O
w	O
in	O
such	O
a	O
way	O
as	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
w	O
that	O
minimizes	O
the	O
objective	B
function	I
.	O
many	O
function-minimization	O
algorithms	B
make	O
use	O
not	O
only	O
of	O
the	O
objective	O
function	B
,	O
but	O
also	O
its	O
gradient	O
with	O
respect	O
to	O
the	O
parameters	B
w.	O
for	O
general	O
feedforward	O
neural	O
networks	O
the	O
backpropagation	B
algorithm	O
e	O
(	O
cid:14	O
)	O
ciently	O
evaluates	O
the	O
gradient	O
of	O
the	O
output	O
y	O
with	O
respect	O
to	O
the	O
parameters	B
w	O
,	O
and	O
thence	O
the	O
gradient	O
of	O
the	O
objective	B
function	I
with	O
respect	O
to	O
w.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
474	O
39	O
|	O
the	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
5	O
4	O
3	O
2	O
1	O
0	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
2	O
1	O
0.5	O
0	O
-10	O
-5	O
10	O
5	O
x2	O
0	O
0	O
x1	O
5	O
10	O
-5	O
-10	O
w	O
=	O
(	O
(	O
cid:0	O
)	O
2	O
;	O
3	O
)	O
1	O
0.5	O
-10	O
-5	O
0	O
x1	O
5	O
10	O
-5	O
-10	O
10	O
5	O
x2	O
0	O
w	O
=	O
(	O
(	O
cid:0	O
)	O
2	O
;	O
(	O
cid:0	O
)	O
1	O
)	O
w2	O
1	O
0.5	O
6	O
-10	O
0	O
10	O
5	O
x2	O
0	O
-5	O
0	O
x1	O
5	O
10	O
-5	O
-10	O
w	O
=	O
(	O
1	O
;	O
4	O
)	O
1	O
0.5	O
10	O
5	O
x2	O
0	O
0	O
-10	O
-5	O
1	O
0.5	O
-10	O
-5	O
0	O
x1	O
5	O
10	O
-5	O
-10	O
w	O
=	O
(	O
0	O
;	O
2	O
)	O
10	O
5	O
x2	O
0	O
0	O
x1	O
5	O
10	O
-5	O
-10	O
w	O
=	O
(	O
2	O
;	O
2	O
)	O
1	O
0.5	O
10	O
5	O
x2	O
0	O
-10	O
-5	O
1	O
0.5	O
-10	O
-5	O
0	O
x1	O
5	O
10	O
-5	O
-10	O
w	O
=	O
(	O
1	O
;	O
0	O
)	O
10	O
5	O
x2	O
0	O
0	O
x1	O
5	O
10	O
-5	O
-10	O
w	O
=	O
(	O
3	O
;	O
0	O
)	O
1	O
0.5	O
0	O
-10	O
-5	O
10	O
5	O
x2	O
0	O
0	O
x1	O
5	O
10	O
-5	O
-10	O
w	O
=	O
(	O
2	O
;	O
(	O
cid:0	O
)	O
2	O
)	O
1	O
0.5	O
-10	O
-5	O
1	O
0.5	O
-10	O
-5	O
10	O
5	O
x2	O
0	O
0	O
x1	O
5	O
10	O
-5	O
-10	O
w	O
=	O
(	O
5	O
;	O
4	O
)	O
10	O
5	O
x2	O
0	O
0	O
x1	O
5	O
10	O
-5	O
-10	O
w	O
=	O
(	O
5	O
;	O
1	O
)	O
-	O
w1	O
(	O
cid:0	O
)	O
3	O
(	O
cid:0	O
)	O
2	O
(	O
cid:0	O
)	O
1	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
figure	O
39.3.	O
weight	B
space	I
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
39.3	O
:	O
training	O
the	O
single	B
neuron	I
as	O
a	O
binary	O
classi	O
(	O
cid:12	O
)	O
er	O
475	O
39.3	O
training	O
the	O
single	B
neuron	I
as	O
a	O
binary	O
classi	O
(	O
cid:12	O
)	O
er	O
we	O
assume	O
we	O
have	O
a	O
data	B
set	I
of	O
inputs	O
fx	O
(	O
n	O
)	O
gn	O
n=1	O
,	O
and	O
a	O
neuron	B
whose	O
output	O
y	O
(	O
x	O
;	O
w	O
)	O
is	O
bounded	O
between	O
0	O
and	O
1.	O
we	O
can	O
then	O
write	O
down	O
the	O
following	O
error	B
function	I
:	O
n=1	O
with	O
binary	O
labels	O
ft	O
(	O
n	O
)	O
gn	O
g	O
(	O
w	O
)	O
=	O
(	O
cid:0	O
)	O
xn	O
ht	O
(	O
n	O
)	O
ln	O
y	O
(	O
x	O
(	O
n	O
)	O
;	O
w	O
)	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
t	O
(	O
n	O
)	O
)	O
ln	O
(	O
1	O
(	O
cid:0	O
)	O
y	O
(	O
x	O
(	O
n	O
)	O
;	O
w	O
)	O
)	O
i	O
:	O
(	O
39.11	O
)	O
each	O
term	O
in	O
this	O
objective	B
function	I
may	O
be	O
recognized	O
as	O
the	O
information	B
content	I
of	O
one	O
outcome	O
.	O
it	O
may	O
also	O
be	O
described	O
as	O
the	O
relative	B
entropy	I
be-	O
tween	O
the	O
empirical	O
probability	B
distribution	O
(	O
t	O
(	O
n	O
)	O
;	O
1	O
(	O
cid:0	O
)	O
t	O
(	O
n	O
)	O
)	O
and	O
the	O
probability	B
distribution	O
implied	O
by	O
the	O
output	O
of	O
the	O
neuron	O
(	O
y	O
;	O
1	O
(	O
cid:0	O
)	O
y	O
)	O
.	O
the	O
objective	O
func-	O
tion	O
is	O
bounded	O
below	O
by	O
zero	O
and	O
only	O
attains	O
this	O
value	O
if	O
y	O
(	O
x	O
(	O
n	O
)	O
;	O
w	O
)	O
=	O
t	O
(	O
n	O
)	O
for	O
all	O
n.	O
we	O
now	O
di	O
(	O
cid:11	O
)	O
erentiate	O
this	O
objective	B
function	I
with	O
respect	O
to	O
w.	O
exercise	O
39.2	O
.	O
[	O
2	O
]	O
the	O
backpropagation	B
algorithm	O
.	O
show	O
that	O
the	O
derivative	O
g	O
=	O
@	O
g=	O
@	O
w	O
is	O
given	O
by	O
:	O
gj	O
=	O
@	O
g	O
@	O
wj	O
=	O
n	O
xn=1	O
(	O
cid:0	O
)	O
(	O
t	O
(	O
n	O
)	O
(	O
cid:0	O
)	O
y	O
(	O
n	O
)	O
)	O
x	O
(	O
n	O
)	O
j	O
:	O
(	O
39.12	O
)	O
notice	O
that	O
the	O
quantity	O
e	O
(	O
n	O
)	O
(	O
cid:17	O
)	O
t	O
(	O
n	O
)	O
(	O
cid:0	O
)	O
y	O
(	O
n	O
)	O
is	O
the	O
error	O
on	O
example	O
n	O
{	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
the	O
target	O
and	O
the	O
output	O
.	O
the	O
simplest	O
thing	O
to	O
do	O
with	O
a	O
gradient	O
of	O
an	O
error	B
function	I
is	O
to	O
descend	O
it	O
(	O
even	O
though	O
this	O
is	O
often	O
di-	O
mensionally	O
incorrect	O
,	O
since	O
a	O
gradient	O
has	O
dimensions	B
[	O
1/parameter	O
]	O
,	O
whereas	O
a	O
change	O
in	O
a	O
parameter	O
has	O
dimensions	B
[	O
parameter	O
]	O
)	O
.	O
since	O
the	O
derivative	O
@	O
g=	O
@	O
w	O
is	O
a	O
sum	O
of	O
terms	O
g	O
(	O
n	O
)	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
g	O
(	O
n	O
)	O
j	O
(	O
cid:17	O
)	O
(	O
cid:0	O
)	O
(	O
t	O
(	O
n	O
)	O
(	O
cid:0	O
)	O
y	O
(	O
n	O
)	O
)	O
x	O
(	O
n	O
)	O
j	O
(	O
39.13	O
)	O
for	O
n	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
n	O
,	O
we	O
can	O
obtain	O
a	O
simple	O
on-line	O
algorithm	B
by	O
putting	O
each	O
input	O
through	O
the	O
network	B
one	O
at	O
a	O
time	O
,	O
and	O
adjusting	O
w	O
a	O
little	O
in	O
a	O
direction	O
opposite	O
to	O
g	O
(	O
n	O
)	O
.	O
we	O
summarize	O
the	O
whole	O
learning	B
algorithm	O
.	O
the	O
on-line	O
gradient-descent	O
learning	B
algorithm	O
architecture	B
.	O
a	O
single	B
neuron	I
has	O
a	O
number	O
i	O
of	O
inputs	O
xi	O
and	O
one	O
output	O
y.	O
associated	O
with	O
each	O
input	O
is	O
a	O
weight	B
wi	O
(	O
i	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
i	O
)	O
.	O
activity	B
rule	I
.	O
1.	O
first	O
,	O
in	O
response	O
to	O
the	O
received	O
inputs	O
x	O
(	O
which	O
may	O
be	O
arbitrary	O
real	O
numbers	O
)	O
,	O
we	O
compute	O
the	O
activation	O
of	O
the	O
neuron	B
,	O
a	O
=xi	O
wixi	O
;	O
(	O
39.14	O
)	O
where	O
the	O
sum	O
is	O
over	O
i	O
=	O
0	O
;	O
:	O
:	O
:	O
;	O
i	O
if	O
there	O
is	O
a	O
bias	B
and	O
i	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
i	O
otherwise	O
.	O
2.	O
second	O
,	O
the	O
output	O
y	O
is	O
set	B
as	O
a	O
sigmoid	B
function	O
of	O
the	O
activation	O
.	O
y	O
(	O
a	O
)	O
=	O
1	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
a	O
:	O
(	O
39.15	O
)	O
this	O
output	O
might	O
be	O
viewed	O
as	O
stating	O
the	O
probability	B
,	O
according	O
to	O
the	O
neuron	B
,	O
that	O
the	O
given	O
input	O
is	O
in	O
class	O
1	O
rather	O
than	O
class	O
0	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
476	O
39	O
|	O
the	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
learning	B
rule	I
.	O
the	O
teacher	O
supplies	O
a	O
target	O
value	O
t	O
2	O
f0	O
;	O
1g	O
which	O
says	O
what	O
the	O
correct	O
answer	O
is	O
for	O
the	O
given	O
input	O
.	O
we	O
compute	O
the	O
error	O
signal	O
e	O
=	O
t	O
(	O
cid:0	O
)	O
y	O
(	O
39.16	O
)	O
then	O
adjust	O
the	O
weights	O
w	O
in	O
a	O
direction	O
that	O
would	O
reduce	O
the	O
magnitude	O
of	O
this	O
error	O
:	O
(	O
cid:1	O
)	O
wi	O
=	O
(	O
cid:17	O
)	O
exi	O
;	O
(	O
39.17	O
)	O
where	O
(	O
cid:17	O
)	O
is	O
the	O
‘	O
learning	B
rate	O
’	O
.	O
commonly	O
(	O
cid:17	O
)	O
is	O
set	B
by	O
trial	O
and	O
error	O
to	O
a	O
constant	O
value	O
or	O
to	O
a	O
decreasing	O
function	B
of	O
simulation	O
time	O
(	O
cid:28	O
)	O
such	O
as	O
(	O
cid:17	O
)	O
0=	O
(	O
cid:28	O
)	O
.	O
the	O
activity	B
rule	I
and	O
learning	B
rule	I
are	O
repeated	O
for	O
each	O
input/target	O
pair	O
(	O
x	O
;	O
t	O
)	O
that	O
is	O
presented	O
.	O
if	O
there	O
is	O
a	O
(	O
cid:12	O
)	O
xed	O
data	B
set	I
of	O
size	O
n	O
,	O
we	O
can	O
cycle	O
through	O
the	O
data	O
multiple	O
times	O
.	O
batch	O
learning	B
versus	O
on-line	O
learning	B
here	O
we	O
have	O
described	O
the	O
on-line	O
learning	B
algorithm	O
,	O
in	O
which	O
a	O
change	O
in	O
the	O
weights	O
is	O
made	O
after	O
every	O
example	O
is	O
presented	O
.	O
an	O
alternative	O
paradigm	O
is	O
to	O
go	O
through	O
a	O
batch	O
of	O
examples	O
,	O
computing	O
the	O
outputs	O
and	O
errors	O
and	O
accumulating	O
the	O
changes	O
speci	O
(	O
cid:12	O
)	O
ed	O
in	O
equation	O
(	O
39.17	O
)	O
which	O
are	O
then	O
made	O
at	O
the	O
end	O
of	O
the	O
batch	O
.	O
batch	O
learning	B
for	O
the	O
single	B
neuron	I
classi	O
(	O
cid:12	O
)	O
er	O
for	O
each	O
input/target	O
pair	O
(	O
x	O
(	O
n	O
)	O
;	O
t	O
(	O
n	O
)	O
)	O
(	O
n	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
n	O
)	O
,	O
compute	O
y	O
(	O
n	O
)	O
=	O
y	O
(	O
x	O
(	O
n	O
)	O
;	O
w	O
)	O
,	O
where	O
y	O
(	O
x	O
;	O
w	O
)	O
=	O
;	O
(	O
39.18	O
)	O
de	O
(	O
cid:12	O
)	O
ne	O
e	O
(	O
n	O
)	O
=	O
t	O
(	O
n	O
)	O
(	O
cid:0	O
)	O
y	O
(	O
n	O
)	O
,	O
and	O
compute	O
for	O
each	O
weight	B
wi	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
pi	O
wixi	O
)	O
g	O
(	O
n	O
)	O
i	O
=	O
(	O
cid:0	O
)	O
e	O
(	O
n	O
)	O
x	O
(	O
n	O
)	O
:	O
1	O
i	O
(	O
39.19	O
)	O
(	O
39.20	O
)	O
then	O
let	O
(	O
cid:1	O
)	O
wi	O
=	O
(	O
cid:0	O
)	O
(	O
cid:17	O
)	O
xn	O
g	O
(	O
n	O
)	O
i	O
:	O
this	O
batch	O
learning	B
algorithm	O
is	O
a	O
gradient	B
descent	I
algorithm	O
,	O
whereas	O
the	O
on-line	O
algorithm	B
is	O
a	O
stochastic	B
gradient	I
descent	O
algorithm	B
.	O
source	B
code	I
implementing	O
batch	O
learning	B
is	O
given	O
in	O
algorithm	O
39.5.	O
this	O
algorithm	B
is	O
demonstrated	O
in	O
(	O
cid:12	O
)	O
gure	O
39.4	O
for	O
a	O
neuron	B
with	O
two	O
inputs	O
with	O
weights	O
w1	O
and	O
w2	O
and	O
a	O
bias	B
w0	O
,	O
performing	O
the	O
function	B
y	O
(	O
x	O
;	O
w	O
)	O
=	O
1	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
(	O
w0+w1x1+w2x2	O
)	O
:	O
(	O
39.21	O
)	O
the	O
bias	B
w0	O
is	O
included	O
,	O
in	O
contrast	O
to	O
(	O
cid:12	O
)	O
gure	O
39.3	O
,	O
where	O
it	O
was	O
omitted	O
.	O
the	O
neuron	B
is	O
trained	O
on	O
a	O
data	B
set	I
of	O
ten	O
labelled	O
examples	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
39.3	O
:	O
training	O
the	O
single	B
neuron	I
as	O
a	O
binary	O
classi	O
(	O
cid:12	O
)	O
er	O
477	O
x2	O
(	O
a	O
)	O
(	O
f	O
)	O
10	O
8	O
6	O
4	O
2	O
0	O
10	O
8	O
6	O
4	O
2	O
0	O
10	O
8	O
6	O
4	O
2	O
0	O
(	O
h	O
)	O
10	O
8	O
6	O
4	O
2	O
0	O
(	O
j	O
)	O
w2	O
(	O
c	O
)	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
-0.5	O
-0.5	O
0	O
0.5	O
1	O
1.5	O
2	O
2.5	O
3	O
2	O
0	O
-2	O
-4	O
-6	O
-8	O
(	O
b	O
)	O
-10	O
-12	O
w1	O
10	O
8	O
6	O
4	O
2	O
0	O
10	O
8	O
6	O
4	O
2	O
0	O
10	O
0	O
2	O
4	O
6	O
8	O
10	O
0	O
2	O
4	O
6	O
8	O
10	O
(	O
d	O
)	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
0	O
400	O
350	O
300	O
250	O
200	O
150	O
100	O
50	O
0	O
2	O
4	O
6	O
8	O
10	O
x1	O
0	O
2	O
4	O
6	O
8	O
10	O
0	O
2	O
4	O
6	O
8	O
10	O
(	O
g	O
)	O
(	O
i	O
)	O
8	O
6	O
4	O
2	O
0	O
(	O
k	O
)	O
0	O
2	O
4	O
6	O
8	O
10	O
w0	O
w1	O
w2	O
1	O
10	O
100	O
1000	O
10000	O
100000	O
g	O
(	O
w	O
)	O
1	O
10	O
100	O
1000	O
10000	O
100000	O
e_w	O
(	O
w	O
)	O
0	O
2	O
4	O
6	O
8	O
10	O
(	O
e	O
)	O
0	O
1	O
10	O
100	O
1000	O
10000	O
100000	O
figure	O
39.4.	O
a	O
single	B
neuron	I
learning	O
to	O
classify	O
by	O
gradient	O
descent	O
.	O
the	O
neuron	B
has	O
two	O
weights	O
w1	O
and	O
w2	O
and	O
a	O
bias	B
w0	O
.	O
the	O
learning	B
rate	O
was	O
set	B
to	O
(	O
cid:17	O
)	O
=	O
0:01	O
and	O
batch-mode	O
gradient	B
descent	I
was	O
performed	O
using	O
the	O
code	B
displayed	O
in	O
algorithm	O
39.5	O
.	O
(	O
a	O
)	O
the	O
training	B
data	I
.	O
(	O
b	O
)	O
evolution	B
of	O
weights	O
w0	O
,	O
w1	O
and	O
w2	O
as	O
a	O
function	B
of	O
number	O
of	O
iterations	O
(	O
on	O
log	O
scale	O
)	O
.	O
(	O
c	O
)	O
evolution	B
of	O
weights	O
w1	O
and	O
w2	O
in	O
weight	O
space	O
.	O
(	O
d	O
)	O
the	O
objective	B
function	I
g	O
(	O
w	O
)	O
as	O
a	O
function	B
of	O
number	O
of	O
iterations	O
.	O
(	O
e	O
)	O
the	O
magnitude	O
of	O
the	O
weights	O
ew	O
(	O
w	O
)	O
as	O
a	O
function	B
of	O
time	O
.	O
(	O
f	O
{	O
k	O
)	O
the	O
function	B
performed	O
by	O
the	O
neuron	B
(	O
shown	O
by	O
three	O
of	O
its	O
contours	O
)	O
after	O
30	O
,	O
80	O
,	O
500	O
,	O
3000	O
,	O
10	O
000	O
and	O
40	O
000	O
iterations	O
.	O
the	O
contours	O
shown	O
are	O
those	O
corresponding	O
to	O
a	O
=	O
0	O
;	O
(	O
cid:6	O
)	O
1	O
,	O
namely	O
y	O
=	O
0:5	O
;	O
0:27	O
and	O
0:73.	O
also	O
shown	O
is	O
a	O
vector	O
proportional	O
to	O
(	O
w1	O
;	O
w2	O
)	O
.	O
the	O
larger	O
the	O
weights	O
are	O
,	O
the	O
bigger	O
this	O
vector	O
becomes	O
,	O
and	O
the	O
closer	O
together	O
are	O
the	O
contours	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
478	O
39	O
|	O
the	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
algorithm	B
39.5.	O
octave	B
source	O
code	B
for	O
a	O
gradient	B
descent	I
optimizer	O
of	O
a	O
single	B
neuron	I
,	O
batch	O
learning	B
,	O
with	O
optional	O
weight	B
decay	I
(	O
rate	B
alpha	O
)	O
.	O
octave	B
notation	O
:	O
the	O
instruction	O
a	O
=	O
x	O
*	O
w	O
causes	O
the	O
(	O
n	O
(	O
cid:2	O
)	O
i	O
)	O
matrix	B
x	O
consisting	O
of	O
all	O
the	O
input	O
vectors	O
to	O
be	O
multiplied	O
by	O
the	O
weight	B
vector	O
w	O
,	O
giving	O
the	O
vector	O
a	O
listing	O
the	O
activations	O
for	O
all	O
n	O
input	O
vectors	O
;	O
x	O
’	O
means	O
x-transpose	O
;	O
the	O
single	O
command	O
y	O
=	O
sigmoid	B
(	O
a	O
)	O
computes	O
the	O
sigmoid	B
function	O
of	O
all	O
elements	O
of	O
the	O
vector	O
a.	O
figure	O
39.6.	O
the	O
in	O
(	O
cid:13	O
)	O
uence	O
of	O
weight	O
decay	O
on	O
a	O
single	B
neuron	I
’	O
s	O
learning	B
.	O
the	O
objective	B
function	I
is	O
m	O
(	O
w	O
)	O
=	O
g	O
(	O
w	O
)	O
+	O
(	O
cid:11	O
)	O
ew	O
(	O
w	O
)	O
.	O
the	O
learning	B
method	O
was	O
as	O
in	O
(	O
cid:12	O
)	O
gure	O
39.4	O
.	O
(	O
a	O
)	O
evolution	B
of	O
weights	O
w0	O
,	O
w1	O
and	O
w2	O
.	O
(	O
b	O
)	O
evolution	B
of	O
weights	O
w1	O
and	O
w2	O
in	O
weight	O
space	O
shown	O
by	O
points	O
,	O
contrasted	O
with	O
the	O
trajectory	O
followed	O
in	O
the	O
case	O
of	O
zero	O
weight	B
decay	I
,	O
shown	O
by	O
a	O
thin	O
line	O
(	O
from	O
(	O
cid:12	O
)	O
gure	O
39.4	O
)	O
.	O
notice	O
that	O
for	O
this	O
problem	O
weight	B
decay	I
has	O
an	O
e	O
(	O
cid:11	O
)	O
ect	O
very	O
similar	O
to	O
‘	O
early	O
stopping	O
’	O
.	O
(	O
c	O
)	O
the	O
objective	B
function	I
m	O
(	O
w	O
)	O
and	O
the	O
error	B
function	I
g	O
(	O
w	O
)	O
as	O
a	O
function	B
of	O
number	O
of	O
iterations	O
.	O
(	O
d	O
)	O
the	O
function	B
performed	O
by	O
the	O
neuron	B
after	O
40	O
000	O
iterations	O
.	O
global	O
x	O
;	O
global	O
t	O
;	O
#	O
x	O
is	O
an	O
n	O
*	O
i	O
matrix	B
containing	O
all	O
the	O
input	O
vectors	O
#	O
t	O
is	O
a	O
vector	O
of	O
length	B
n	O
containing	O
all	O
the	O
targets	O
for	O
l	O
=	O
1	O
:	O
l	O
#	O
loop	O
l	O
times	O
;	O
a	O
=	O
x	O
*	O
w	O
y	O
=	O
sigmoid	B
(	O
a	O
)	O
;	O
e	O
=	O
t	O
-	O
y	O
g	O
=	O
-	O
x	O
’	O
*	O
e	O
;	O
w	O
=	O
w	O
-	O
eta	O
*	O
(	O
g	O
+	O
alpha	O
*	O
w	O
)	O
;	O
#	O
compute	O
all	O
activations	O
#	O
compute	O
outputs	O
#	O
compute	O
errors	B
#	O
compute	O
the	O
gradient	O
vector	O
#	O
make	O
step	O
,	O
using	O
learning	B
rate	O
eta	O
#	O
and	O
weight	O
decay	O
alpha	O
;	O
endfor	O
function	B
f	O
=	O
sigmoid	B
(	O
v	O
)	O
f	O
=	O
1.0	O
./	O
(	O
1.0	O
.+	O
exp	O
(	O
-	O
v	O
)	O
)	O
;	O
endfunction	O
(	O
cid:11	O
)	O
=	O
0:01	O
(	O
cid:11	O
)	O
=	O
0:1	O
(	O
cid:11	O
)	O
=	O
1	O
2	O
0	O
-2	O
-4	O
-6	O
-8	O
-10	O
-12	O
w0	O
w1	O
w2	O
1	O
10	O
100	O
1000	O
10000	O
100000	O
2	O
1	O
0	O
-1	O
-2	O
-3	O
-4	O
-5	O
w0	O
w1	O
w2	O
1	O
10	O
100	O
1000	O
10000	O
100000	O
2	O
1	O
0	O
-1	O
-2	O
-3	O
-4	O
-5	O
w0	O
w1	O
w2	O
1	O
10	O
100	O
1000	O
10000	O
100000	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
-0.5	O
-0.5	O
0	O
0.5	O
1	O
1.5	O
2	O
2.5	O
3	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
-0.5	O
-0.5	O
0	O
0.5	O
1	O
1.5	O
2	O
2.5	O
3	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
0	O
-0.5	O
-0.5	O
0	O
0.5	O
1	O
1.5	O
2	O
2.5	O
3	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
0	O
1	O
10	O
8	O
6	O
4	O
2	O
0	O
g	O
(	O
w	O
)	O
m	O
(	O
w	O
)	O
10	O
100	O
1000	O
10000	O
100000	O
0	O
2	O
4	O
6	O
8	O
10	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
0	O
1	O
10	O
8	O
6	O
4	O
2	O
0	O
g	O
(	O
w	O
)	O
m	O
(	O
w	O
)	O
10	O
100	O
1000	O
10000	O
100000	O
0	O
2	O
4	O
6	O
8	O
10	O
7	O
6	O
5	O
4	O
3	O
2	O
1	O
0	O
1	O
10	O
8	O
6	O
4	O
2	O
0	O
g	O
(	O
w	O
)	O
m	O
(	O
w	O
)	O
10	O
100	O
1000	O
10000	O
100000	O
0	O
2	O
4	O
6	O
8	O
10	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
39.4	O
:	O
beyond	O
descent	O
on	O
the	O
error	B
function	I
:	O
regularization	B
479	O
39.4	O
beyond	O
descent	O
on	O
the	O
error	B
function	I
:	O
regularization	B
if	O
the	O
parameter	O
(	O
cid:17	O
)	O
is	O
set	B
to	O
an	O
appropriate	O
value	O
,	O
this	O
algorithm	B
works	O
:	O
the	O
algorithm	B
(	O
cid:12	O
)	O
nds	O
a	O
setting	O
of	O
w	O
that	O
correctly	O
classi	O
(	O
cid:12	O
)	O
es	O
as	O
many	O
of	O
the	O
examples	O
as	O
possible	O
.	O
if	O
the	O
examples	O
are	O
in	O
fact	O
linearly	O
separable	O
then	O
the	O
neuron	B
(	O
cid:12	O
)	O
nds	O
this	O
lin-	O
ear	O
separation	B
and	O
its	O
weights	O
diverge	O
to	O
ever-larger	O
values	O
as	O
the	O
simulation	O
continues	O
.	O
this	O
can	O
be	O
seen	O
happening	O
in	O
(	O
cid:12	O
)	O
gure	O
39.4	O
(	O
f	O
{	O
k	O
)	O
.	O
this	O
is	O
an	O
exam-	O
ple	O
of	O
over	O
(	O
cid:12	O
)	O
tting	O
,	O
where	O
a	O
model	B
(	O
cid:12	O
)	O
ts	O
the	O
data	O
so	O
well	O
that	O
its	O
generalization	B
performance	O
is	O
likely	O
to	O
be	O
adversely	O
a	O
(	O
cid:11	O
)	O
ected	O
.	O
this	O
behaviour	O
may	O
be	O
viewed	O
as	O
undesirable	O
.	O
how	O
can	O
it	O
be	O
recti	O
(	O
cid:12	O
)	O
ed	O
?	O
an	O
ad	O
hoc	O
solution	O
to	O
over	O
(	O
cid:12	O
)	O
tting	O
is	O
to	O
use	O
early	O
stopping	O
,	O
that	O
is	O
,	O
use	O
an	O
algorithm	B
originally	O
intended	O
to	O
minimize	O
the	O
error	B
function	I
g	O
(	O
w	O
)	O
,	O
then	O
prevent	O
it	O
from	O
doing	O
so	O
by	O
halting	O
the	O
algorithm	B
at	O
some	O
point	O
.	O
a	O
more	O
principled	O
solution	O
to	O
over	O
(	O
cid:12	O
)	O
tting	O
makes	O
use	O
of	O
regularization	O
.	O
reg-	O
ularization	O
involves	O
modifying	O
the	O
objective	B
function	I
in	O
such	O
a	O
way	O
as	O
to	O
in-	O
corporate	O
a	O
bias	B
against	O
the	O
sorts	O
of	O
solution	O
w	O
which	O
we	O
dislike	O
.	O
in	O
the	O
above	O
example	O
,	O
what	O
we	O
dislike	O
is	O
the	O
development	O
of	O
a	O
very	O
sharp	O
decision	O
bound-	O
ary	O
in	O
(	O
cid:12	O
)	O
gure	O
39.4k	O
;	O
this	O
sharp	O
boundary	O
is	O
associated	O
with	O
large	O
weight	B
values	O
,	O
so	O
we	O
use	O
a	O
regularizer	O
that	O
penalizes	O
large	O
weight	B
values	O
.	O
we	O
modify	O
the	O
objective	B
function	I
to	O
:	O
m	O
(	O
w	O
)	O
=	O
g	O
(	O
w	O
)	O
+	O
(	O
cid:11	O
)	O
ew	O
(	O
w	O
)	O
(	O
39.22	O
)	O
where	O
the	O
simplest	O
choice	O
of	O
regularizer	O
is	O
the	O
weight	B
decay	I
regularizer	O
ew	O
(	O
w	O
)	O
=	O
w2	O
i	O
:	O
1	O
2xi	O
(	O
39.23	O
)	O
the	O
regularization	B
constant	I
(	O
cid:11	O
)	O
is	O
called	O
the	O
weight	B
decay	I
rate	O
.	O
this	O
additional	O
term	O
favours	O
small	O
values	O
of	O
w	O
and	O
decreases	O
the	O
tendency	O
of	O
a	O
model	B
to	O
over	O
(	O
cid:12	O
)	O
t	O
(	O
cid:12	O
)	O
ne	O
details	O
of	O
the	O
training	O
data	O
.	O
the	O
quantity	O
(	O
cid:11	O
)	O
is	O
known	O
as	O
a	O
hyperparameter	B
.	O
hyperparameters	O
play	O
a	O
role	O
in	O
the	O
learning	B
algorithm	O
but	O
play	O
no	O
role	O
in	O
the	O
activity	B
rule	I
of	O
the	O
network	B
.	O
exercise	O
39.3	O
.	O
[	O
1	O
]	O
compute	O
the	O
derivative	O
of	O
m	O
(	O
w	O
)	O
with	O
respect	O
to	O
wi	O
.	O
why	O
is	O
the	O
above	O
regularizer	O
known	O
as	O
the	O
‘	O
weight	B
decay	I
’	O
regularizer	O
?	O
the	O
gradient	B
descent	I
source	O
code	B
of	O
algorithm	B
39.5	O
implements	O
weight	B
decay	I
.	O
this	O
gradient	B
descent	I
algorithm	O
is	O
demonstrated	O
in	O
(	O
cid:12	O
)	O
gure	O
39.6	O
using	O
weight	B
decay	I
rates	O
(	O
cid:11	O
)	O
=	O
0:01	O
,	O
0:1	O
,	O
and	O
1.	O
as	O
the	O
weight	B
decay	I
rate	O
is	O
increased	O
the	O
solution	O
becomes	O
biased	O
towards	O
broader	O
sigmoid	B
functions	O
with	O
decision	O
boundaries	O
that	O
are	O
closer	O
to	O
the	O
origin	O
.	O
note	O
gradient	B
descent	I
with	O
a	O
step	O
size	O
(	O
cid:17	O
)	O
is	O
in	O
general	O
not	O
the	O
most	O
e	O
(	O
cid:14	O
)	O
cient	O
way	O
to	O
minimize	O
a	O
function	B
.	O
a	O
modi	O
(	O
cid:12	O
)	O
cation	O
of	O
gradient	O
descent	O
known	O
as	O
momentum	O
,	O
while	O
improving	O
convergence	O
,	O
is	O
also	O
not	O
recommended	O
.	O
most	O
neural	O
network	B
experts	O
use	O
more	O
advanced	O
optimizers	O
such	O
as	O
conjugate	O
gradient	O
algorithms	O
.	O
[	O
please	O
do	O
not	O
confuse	O
momentum	B
,	O
which	O
is	O
sometimes	O
given	O
the	O
symbol	O
(	O
cid:11	O
)	O
,	O
with	O
weight	O
decay	O
.	O
]	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
480	O
39	O
|	O
the	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
39.5	O
further	O
exercises	O
more	O
motivations	O
for	O
the	O
linear	B
neuron	O
.	O
exercise	O
39.4	O
.	O
[	O
2	O
]	O
consider	O
the	O
task	O
of	O
recognizing	O
which	O
of	O
two	O
gaussian	O
distri-	O
butions	O
a	O
vector	O
z	O
comes	O
from	O
.	O
unlike	O
the	O
case	O
studied	O
in	O
section	O
11.2	O
,	O
where	O
the	O
distributions	O
had	O
di	O
(	O
cid:11	O
)	O
erent	O
means	O
but	O
a	O
common	O
variance	B
{	O
covariance	B
matrix	I
,	O
we	O
will	O
assume	O
that	O
the	O
two	O
distributions	O
have	O
ex-	O
actly	O
the	O
same	O
mean	B
but	O
di	O
(	O
cid:11	O
)	O
erent	O
variances	O
.	O
let	O
the	O
probability	O
of	O
z	O
given	O
s	O
(	O
s	O
2	O
f0	O
;	O
1g	O
)	O
be	O
p	O
(	O
zj	O
s	O
)	O
=	O
i	O
yi=1	O
normal	B
(	O
zi	O
;	O
0	O
;	O
(	O
cid:27	O
)	O
2	O
si	O
)	O
;	O
(	O
39.24	O
)	O
si	O
is	O
the	O
variance	B
of	O
zi	O
when	O
the	O
source	O
symbol	O
is	O
s.	O
show	O
that	O
where	O
(	O
cid:27	O
)	O
2	O
p	O
(	O
s	O
=	O
1j	O
z	O
)	O
can	O
be	O
written	O
in	O
the	O
form	O
p	O
(	O
s	O
=	O
1j	O
z	O
)	O
=	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
wtx	O
+	O
(	O
cid:18	O
)	O
)	O
where	O
xi	O
is	O
an	O
appropriate	O
function	B
of	O
zi	O
,	O
xi	O
=	O
g	O
(	O
zi	O
)	O
.	O
1	O
;	O
(	O
39.25	O
)	O
exercise	O
39.5	O
.	O
[	O
2	O
]	O
the	O
noisy	B
led	O
.	O
2	O
5	O
1	O
4	O
7	O
3	O
6	O
c	O
(	O
2	O
)	O
=	O
c	O
(	O
3	O
)	O
=	O
c	O
(	O
8	O
)	O
=	O
consider	O
an	O
led	O
display	O
with	O
7	O
elements	O
numbered	O
as	O
shown	O
above	O
.	O
the	O
state	O
of	O
the	O
display	O
is	O
a	O
vector	O
x.	O
when	O
the	O
controller	O
wants	O
the	O
display	O
to	O
show	O
character	O
number	O
s	O
,	O
e.g	O
.	O
s	O
=	O
2	O
,	O
each	O
element	O
xj	O
(	O
j	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
7	O
)	O
either	O
adopts	O
its	O
intended	O
state	O
cj	O
(	O
s	O
)	O
,	O
with	O
probability	O
1	O
(	O
cid:0	O
)	O
f	O
,	O
or	O
is	O
(	O
cid:13	O
)	O
ipped	O
,	O
with	O
probability	O
f	O
.	O
let	O
’	O
s	O
call	O
the	O
two	O
states	O
of	O
x	O
‘	O
+1	O
’	O
and	O
‘	O
(	O
cid:0	O
)	O
1	O
’	O
.	O
(	O
a	O
)	O
assuming	O
that	O
the	O
intended	O
character	O
s	O
is	O
actually	O
a	O
2	O
or	O
a	O
3	O
,	O
what	O
is	O
the	O
probability	O
of	O
s	O
,	O
given	O
the	O
state	O
x	O
?	O
show	O
that	O
p	O
(	O
s	O
=	O
2j	O
x	O
)	O
can	O
be	O
written	O
in	O
the	O
form	O
p	O
(	O
s	O
=	O
2j	O
x	O
)	O
=	O
1	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
wtx	O
+	O
(	O
cid:18	O
)	O
)	O
;	O
(	O
39.26	O
)	O
and	O
compute	O
the	O
values	O
of	O
the	O
weights	O
w	O
in	O
the	O
case	O
f	O
=	O
0:1	O
.	O
(	O
b	O
)	O
assuming	O
that	O
s	O
is	O
one	O
of	O
f0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
9g	O
,	O
with	O
prior	O
probabilities	O
ps	O
,	O
what	O
is	O
the	O
probability	O
of	O
s	O
,	O
given	O
the	O
state	O
x	O
?	O
put	O
your	O
answer	O
in	O
the	O
form	O
p	O
(	O
sj	O
x	O
)	O
=	O
eas	O
eas0	O
xs0	O
;	O
(	O
39.27	O
)	O
where	O
fasg	O
are	O
functions	B
of	O
fcj	O
(	O
s	O
)	O
g	O
and	O
x.	O
could	O
you	O
make	O
a	O
better	O
alphabet	O
of	O
10	O
characters	O
for	O
a	O
noisy	B
led	O
,	O
i.e.	O
,	O
an	O
alphabet	O
less	O
susceptible	O
to	O
confusion	O
?	O
.	O
exercise	O
39.6	O
.	O
[	O
2	O
]	O
a	O
(	O
3	O
;	O
1	O
)	O
error-correcting	B
code	I
consists	O
of	O
the	O
two	O
codewords	O
x	O
(	O
1	O
)	O
=	O
(	O
1	O
;	O
0	O
;	O
0	O
)	O
and	O
x	O
(	O
2	O
)	O
=	O
(	O
0	O
;	O
0	O
;	O
1	O
)	O
.	O
a	O
source	O
bit	O
s	O
2	O
f1	O
;	O
2g	O
having	O
proba-	O
bility	O
distribution	B
fp1	O
;	O
p2g	O
is	O
used	O
to	O
select	O
one	O
of	O
the	O
two	O
codewords	O
for	O
transmission	O
over	O
a	O
binary	B
symmetric	I
channel	I
with	O
noise	B
level	O
f	O
.	O
the	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
13	O
14	O
table	O
39.7.	O
an	O
alternative	O
15-character	O
alphabet	O
for	O
the	O
7-element	O
led	O
display	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
39.5	O
:	O
further	O
exercises	O
481	O
received	O
vector	O
is	O
r.	O
show	O
that	O
the	O
posterior	B
probability	I
of	O
s	O
given	O
r	O
can	O
be	O
written	O
in	O
the	O
form	O
p	O
(	O
s	O
=	O
1j	O
r	O
)	O
=	O
1	O
+	O
exp	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
w0	O
(	O
cid:0	O
)	O
p3	O
and	O
give	O
expressions	O
for	O
the	O
coe	O
(	O
cid:14	O
)	O
cients	O
fwng3	O
describe	O
,	O
with	O
a	O
diagram	O
,	O
how	O
this	O
optimal	B
decoder	I
can	O
be	O
expressed	O
in	O
terms	O
of	O
a	O
‘	O
neuron	B
’	O
.	O
n=1	O
and	O
the	O
bias	B
,	O
w0	O
.	O
1	O
n=1	O
wnrn	O
(	O
cid:17	O
)	O
;	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
problems	O
to	O
look	O
at	O
before	O
chapter	O
40	O
.	O
exercise	O
40.1	O
.	O
[	O
2	O
]	O
what	O
is	O
pn	O
k=0	O
(	O
cid:0	O
)	O
n	O
k	O
(	O
cid:1	O
)	O
?	O
[	O
the	O
symbol	O
(	O
cid:0	O
)	O
n	O
k	O
(	O
cid:1	O
)	O
means	O
the	O
combination	B
n	O
!	O
k	O
!	O
(	O
n	O
(	O
cid:0	O
)	O
k	O
)	O
!	O
.	O
]	O
.	O
exercise	O
40.2	O
.	O
[	O
2	O
]	O
if	O
the	O
top	O
row	O
of	O
pascal	O
’	O
s	O
triangle	B
(	O
which	O
contains	O
the	O
single	O
number	O
‘	O
1	O
’	O
)	O
is	O
denoted	O
row	O
zero	O
,	O
what	O
is	O
the	O
sum	O
of	O
all	O
the	O
numbers	O
in	O
the	O
triangle	B
above	O
row	O
n	O
?	O
.	O
exercise	O
40.3	O
.	O
[	O
2	O
]	O
3	O
points	O
are	O
selected	O
at	O
random	B
on	O
the	O
surface	O
of	O
a	O
sphere	O
.	O
what	O
is	O
the	O
probability	B
that	O
all	O
of	O
them	O
lie	O
on	O
a	O
single	O
hemisphere	O
?	O
this	O
chapter	O
’	O
s	O
material	O
is	O
originally	O
due	O
to	O
polya	O
(	O
1954	O
)	O
and	O
cover	O
(	O
1965	O
)	O
and	O
the	O
exposition	O
that	O
follows	O
is	O
yaser	O
abu-mostafa	O
’	O
s	O
.	O
482	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
40	O
capacity	B
of	O
a	O
single	B
neuron	I
ftngn	O
n=1	O
-	O
learning	B
algorithm	O
-	O
w	O
6	O
fxngn	O
n=1	O
-	O
w	O
6	O
-	O
f^tngn	O
n=1	O
figure	O
40.1.	O
neural	B
network	I
learning	O
viewed	O
as	B
communication	I
.	O
fxngn	O
n=1	O
40.1	O
neural	B
network	I
learning	O
as	B
communication	I
n=1	O
at	O
given	O
locations	O
fxngn	O
many	O
neural	B
network	I
models	O
involve	O
the	O
adaptation	O
of	O
a	O
set	B
of	O
weights	O
w	O
in	O
response	O
to	O
a	O
set	B
of	O
data	O
points	O
,	O
for	O
example	O
a	O
set	B
of	O
n	O
target	O
values	O
dn	O
=	O
ftngn	O
n=1	O
.	O
the	O
adapted	O
weights	O
are	O
then	O
used	O
to	O
process	O
subsequent	O
input	O
data	O
.	O
this	O
process	O
can	O
be	O
viewed	O
as	O
a	O
communication	B
process	O
,	O
in	O
which	O
the	O
sender	O
examines	O
the	O
data	O
dn	O
and	O
creates	O
a	O
message	O
w	O
that	O
depends	O
on	O
those	O
data	O
.	O
the	O
receiver	O
then	O
uses	O
w	O
;	O
for	O
example	O
,	O
the	O
receiver	O
might	O
use	O
the	O
weights	O
to	O
try	O
to	O
reconstruct	O
what	O
the	O
data	O
dn	O
was	O
.	O
[	O
in	O
neural	O
network	O
parlance	O
,	O
this	O
is	O
using	O
the	O
neuron	B
for	O
‘	O
memory	B
’	O
rather	O
than	O
for	O
‘	O
generalization	B
’	O
;	O
‘	O
generalizing	O
’	O
means	O
extrapolating	O
from	O
the	O
observed	O
data	O
to	O
the	O
value	O
of	O
tn	O
+1	O
at	O
some	O
new	O
location	O
xn	O
+1	O
.	O
]	O
just	O
as	O
a	O
disk	B
drive	I
is	O
a	O
communication	B
channel	O
,	O
the	O
adapted	O
network	B
weights	O
w	O
therefore	O
play	O
the	O
role	O
of	O
a	O
communication	B
channel	O
,	O
conveying	O
information	B
about	O
the	O
training	B
data	I
to	O
a	O
future	O
user	O
of	O
that	O
neural	O
net	O
.	O
the	O
question	O
we	O
now	O
address	B
is	O
,	O
‘	O
what	O
is	O
the	O
capacity	B
of	O
this	O
channel	B
?	O
’	O
{	O
that	O
is	O
,	O
‘	O
how	O
much	O
information	O
can	O
be	O
stored	O
by	O
training	O
a	O
neural	B
network	I
?	O
’	O
if	O
we	O
had	O
a	O
learning	B
algorithm	O
that	O
either	O
produces	O
a	O
network	B
whose	O
re-	O
sponse	O
to	O
all	O
inputs	O
is	O
+1	O
or	O
a	O
network	B
whose	O
response	O
to	O
all	O
inputs	O
is	O
0	O
,	O
depending	O
on	O
the	O
training	B
data	I
,	O
then	O
the	O
weights	O
allow	O
us	O
to	O
distinguish	O
be-	O
tween	O
just	O
two	O
sorts	O
of	O
data	O
set	B
.	O
the	O
maximum	O
information	O
such	O
a	O
learning	B
algorithm	O
could	O
convey	O
about	O
the	O
data	O
is	O
therefore	O
1	O
bit	B
,	O
this	O
information	B
con-	O
tent	O
being	O
achieved	O
if	O
the	O
two	O
sorts	O
of	O
data	O
set	B
are	O
equiprobable	O
.	O
how	O
much	O
more	O
information	B
can	O
be	O
conveyed	O
if	O
we	O
make	O
full	O
use	O
of	O
a	O
neural	B
network	I
’	O
s	O
ability	O
to	O
represent	O
other	O
functions	B
?	O
40.2	O
the	O
capacity	B
of	O
a	O
single	B
neuron	I
we	O
will	O
look	O
at	O
the	O
simplest	O
case	O
,	O
that	O
of	O
a	O
single	O
binary	O
threshold	B
neuron	O
.	O
we	O
will	O
(	O
cid:12	O
)	O
nd	O
that	O
the	O
capacity	B
of	O
such	O
a	O
neuron	B
is	O
two	O
bits	O
per	O
weight	B
.	O
a	O
neuron	B
with	O
k	O
inputs	O
can	O
store	O
2k	O
bits	O
of	O
information	B
.	O
to	O
obtain	O
this	O
interesting	O
result	O
we	O
lay	O
down	O
some	O
rules	B
to	O
exclude	O
less	O
interesting	O
answers	O
,	O
such	O
as	O
:	O
‘	O
the	O
capacity	B
of	O
a	O
neuron	B
is	O
in	O
(	O
cid:12	O
)	O
nite	O
,	O
because	O
each	O
483	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
484	O
40	O
|	O
capacity	B
of	O
a	O
single	B
neuron	I
of	O
its	O
weights	O
is	O
a	O
real	O
number	O
and	O
so	O
can	O
convey	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
number	O
of	O
bits	O
’	O
.	O
we	O
exclude	O
this	O
answer	O
by	O
saying	O
that	O
the	O
receiver	O
is	O
not	O
able	O
to	O
examine	O
the	O
weights	O
directly	O
,	O
nor	O
is	O
the	O
receiver	O
allowed	O
to	O
probe	O
the	O
weights	O
by	O
observing	O
the	O
output	O
of	O
the	O
neuron	O
for	O
arbitrarily	O
chosen	O
inputs	O
.	O
we	O
constrain	O
the	O
receiver	O
to	O
observe	O
the	O
output	O
of	O
the	O
neuron	O
at	O
the	O
same	O
(	O
cid:12	O
)	O
xed	O
set	B
of	O
n	O
points	O
fxng	O
that	O
were	O
in	O
the	O
training	O
set	O
.	O
what	O
matters	O
now	O
is	O
how	O
many	O
di	O
(	O
cid:11	O
)	O
erent	O
distinguishable	O
functions	B
our	O
neuron	B
can	O
produce	O
,	O
given	O
that	O
we	O
can	O
observe	O
the	O
function	B
only	O
at	O
these	O
n	O
points	O
.	O
how	O
many	O
di	O
(	O
cid:11	O
)	O
erent	O
binary	O
labellings	O
of	O
n	O
points	O
can	O
a	O
linear	B
threshold	O
function	B
produce	O
?	O
and	O
how	O
does	O
this	O
number	O
compare	O
with	O
the	O
maximum	O
possible	O
number	O
of	O
binary	O
labellings	O
,	O
2n	O
?	O
if	O
nearly	O
all	O
of	O
the	O
2n	O
labellings	O
can	O
be	O
realized	O
by	O
our	O
neuron	B
,	O
then	O
it	O
is	O
a	O
communication	B
channel	O
that	O
can	O
convey	O
all	O
n	O
bits	O
(	O
the	O
target	O
values	O
ftng	O
)	O
with	O
small	O
probability	B
of	I
error	I
.	O
we	O
will	O
identify	O
the	O
capacity	B
of	O
the	O
neuron	B
as	O
the	O
maximum	O
value	O
that	O
n	O
can	O
have	O
such	O
that	O
the	O
probability	B
of	I
error	I
is	O
very	O
small	O
.	O
[	O
we	O
are	O
departing	O
a	O
little	O
from	O
the	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
capacity	O
in	O
chapter	O
9	O
.	O
]	O
we	O
thus	O
examine	O
the	O
following	O
scenario	O
.	O
the	O
sender	O
is	O
given	O
a	O
neuron	B
with	O
k	O
inputs	O
and	O
a	O
data	B
set	I
dn	O
which	O
is	O
a	O
labelling	O
of	O
n	O
points	O
.	O
the	O
sender	O
uses	O
an	O
adaptive	B
algorithm	O
to	O
try	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
w	O
that	O
can	O
reproduce	O
this	O
labelling	O
exactly	O
.	O
we	O
will	O
assume	O
the	O
algorithm	B
(	O
cid:12	O
)	O
nds	O
such	O
a	O
w	O
if	O
it	O
exists	O
.	O
the	O
receiver	O
then	O
evaluates	O
the	O
threshold	B
function	O
on	O
the	O
n	O
input	O
values	O
.	O
what	O
is	O
the	O
probability	B
that	O
all	O
n	O
bits	O
are	O
correctly	O
reproduced	O
?	O
how	O
large	O
can	O
n	O
become	O
,	O
for	O
a	O
given	O
k	O
,	O
without	O
this	O
probability	B
becoming	O
substantially	O
less	O
than	O
one	O
?	O
general	B
position	I
one	O
technical	O
detail	O
needs	O
to	O
be	O
pinned	O
down	O
:	O
what	O
set	O
of	O
inputs	O
fxng	O
are	O
we	O
considering	O
?	O
our	O
answer	O
might	O
depend	O
on	O
this	O
choice	O
.	O
we	O
will	O
assume	O
that	O
the	O
points	O
are	O
in	O
general	O
position	O
.	O
de	O
(	O
cid:12	O
)	O
nition	O
40.1	O
a	O
set	B
of	O
points	O
fxng	O
in	O
k-dimensional	O
space	O
are	O
in	O
general	O
position	O
if	O
any	O
subset	B
of	O
size	O
(	O
cid:20	O
)	O
k	O
is	O
linearly	O
independent	O
,	O
and	O
no	O
k	O
+	O
1	O
of	O
them	O
lie	O
in	O
a	O
(	O
k	O
(	O
cid:0	O
)	O
1	O
)	O
-dimensional	O
plane	O
.	O
in	O
k	O
=	O
3	O
dimensions	B
,	O
for	O
example	O
,	O
a	O
set	B
of	O
points	O
are	O
in	O
general	O
position	O
if	O
no	O
three	O
points	O
are	O
colinear	O
and	O
no	O
four	O
points	O
are	O
coplanar	O
.	O
the	O
intuitive	O
idea	O
is	O
that	O
points	O
in	O
general	O
position	O
are	O
like	O
random	B
points	O
in	O
the	O
space	O
,	O
in	O
terms	O
of	O
the	O
linear	O
dependences	O
between	O
points	O
.	O
you	O
don	O
’	O
t	O
expect	O
three	O
random	O
points	O
in	O
three	O
dimensions	B
to	O
lie	O
on	O
a	O
straight	O
line	O
.	O
the	O
linear	B
threshold	O
function	B
the	O
neuron	B
we	O
will	O
consider	O
performs	O
the	O
function	B
where	O
y	O
=	O
f	O
k	O
xk=1	O
f	O
(	O
a	O
)	O
=	O
(	O
cid:26	O
)	O
1	O
wkxk	O
!	O
a	O
>	O
0	O
0	O
a	O
(	O
cid:20	O
)	O
0	O
:	O
(	O
40.1	O
)	O
(	O
40.2	O
)	O
we	O
will	O
not	O
have	O
a	O
bias	B
w0	O
;	O
the	O
capacity	B
for	O
a	O
neuron	B
with	O
a	O
bias	B
can	O
be	O
obtained	O
by	O
replacing	O
k	O
by	O
k	O
+	O
1	O
in	O
the	O
(	O
cid:12	O
)	O
nal	O
result	O
below	O
,	O
i.e.	O
,	O
considering	O
one	O
of	O
the	O
inputs	O
to	O
be	O
(	O
cid:12	O
)	O
xed	O
to	O
1	O
.	O
(	O
these	O
input	O
points	O
would	O
not	O
then	O
be	O
in	O
general	O
position	O
;	O
the	O
derivation	B
still	O
works	O
.	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
40.3	O
:	O
counting	B
threshold	O
functions	B
485	O
w2	O
x2	O
(	O
0	O
)	O
figure	O
40.2.	O
one	O
data	O
point	O
in	O
a	O
two-dimensional	B
input	O
space	O
,	O
and	O
the	O
two	O
regions	O
of	O
weight	O
space	O
that	O
give	O
the	O
two	O
alternative	O
labellings	O
of	O
that	O
point	O
.	O
x1	O
x	O
(	O
1	O
)	O
(	O
a	O
)	O
(	O
b	O
)	O
w1	O
(	O
1	O
)	O
40.3	O
counting	B
threshold	O
functions	B
let	O
us	O
denote	O
by	O
t	O
(	O
n	O
;	O
k	O
)	O
the	O
number	O
of	O
distinct	O
threshold	B
functions	O
on	O
n	O
points	O
in	O
general	O
position	O
in	O
k	O
dimensions	B
.	O
we	O
will	O
derive	O
a	O
formula	O
for	O
t	O
(	O
n	O
;	O
k	O
)	O
.	O
to	O
start	O
with	O
,	O
let	O
us	O
work	O
out	O
a	O
few	O
cases	O
by	O
hand	O
.	O
in	O
k	O
=	O
1	O
dimension	O
,	O
for	O
any	O
n	O
the	O
n	O
points	O
lie	O
on	O
a	O
line	O
.	O
by	O
changing	O
the	O
sign	O
of	O
the	O
one	O
weight	B
w1	O
we	O
can	O
label	O
all	O
points	O
on	O
the	O
right	O
side	O
of	O
the	O
origin	O
1	O
and	O
the	O
others	O
0	O
,	O
or	O
vice	O
versa	O
.	O
thus	O
there	O
are	O
two	O
distinct	O
threshold	B
functions	O
.	O
t	O
(	O
n	O
;	O
1	O
)	O
=	O
2.	O
with	O
n	O
=	O
1	O
point	O
,	O
for	O
any	O
k	O
if	O
there	O
is	O
just	O
one	O
point	O
x	O
(	O
1	O
)	O
then	O
we	O
can	O
realize	O
both	O
possible	O
labellings	O
by	O
setting	O
w	O
=	O
(	O
cid:6	O
)	O
x	O
(	O
1	O
)	O
.	O
thus	O
t	O
(	O
1	O
;	O
k	O
)	O
=	O
2.	O
in	O
k	O
=	O
2	O
dimensions	B
in	O
two	O
dimensions	B
with	O
n	O
points	O
,	O
we	O
are	O
free	O
to	O
spin	O
the	O
separating	O
line	O
around	O
the	O
origin	O
.	O
each	O
time	O
the	O
line	O
passes	O
over	O
a	O
point	O
we	O
obtain	O
a	O
new	O
function	B
.	O
once	O
we	O
have	O
spun	O
the	O
line	O
through	O
360	O
degrees	O
we	O
reproduce	O
the	O
function	B
we	O
started	O
from	O
.	O
because	O
the	O
points	O
are	O
in	O
general	O
position	O
,	O
the	O
separating	O
plane	O
(	O
line	O
)	O
crosses	O
only	O
one	O
point	O
at	O
a	O
time	O
.	O
in	O
one	O
revolution	O
,	O
every	O
point	O
is	O
passed	O
over	O
twice	O
.	O
there	O
are	O
therefore	O
2n	O
distinct	O
threshold	B
functions	O
.	O
t	O
(	O
n	O
;	O
2	O
)	O
=	O
2n	O
.	O
comparing	O
with	O
the	O
total	O
number	O
of	O
binary	O
functions	O
,	O
2n	O
,	O
we	O
may	O
note	O
that	O
for	O
n	O
(	O
cid:21	O
)	O
3	O
,	O
not	O
all	O
binary	O
functions	O
can	O
be	O
realized	O
by	O
a	O
linear	B
threshold	O
function	B
.	O
one	O
famous	O
example	O
of	O
an	O
unrealizable	O
function	B
with	O
n	O
=	O
4	O
and	O
k	O
=	O
2	O
is	O
the	O
exclusive-or	O
function	B
on	O
the	O
points	O
x	O
=	O
(	O
(	O
cid:6	O
)	O
1	O
;	O
(	O
cid:6	O
)	O
1	O
)	O
.	O
[	O
these	O
points	O
are	O
not	O
in	O
general	O
position	O
,	O
but	O
you	O
may	O
con	O
(	O
cid:12	O
)	O
rm	O
that	O
the	O
function	B
remains	O
unrealizable	O
even	O
if	O
the	O
points	O
are	O
perturbed	O
into	O
general	B
position	I
.	O
]	O
in	O
k	O
=	O
2	O
dimensions	B
,	O
from	O
the	O
point	O
of	O
view	O
of	O
weight	O
space	O
there	O
is	O
another	O
way	O
of	O
visualizing	O
this	O
problem	O
.	O
instead	O
of	O
visualizing	O
a	O
plane	O
separating	O
points	O
in	O
the	O
two-dimensional	B
input	O
space	O
,	O
we	O
can	O
consider	O
the	O
two-dimensional	B
weight	O
space	O
,	O
colouring	O
regions	O
in	O
weight	O
space	O
di	O
(	O
cid:11	O
)	O
erent	O
colours	O
if	O
they	O
label	O
the	O
given	O
datapoints	O
di	O
(	O
cid:11	O
)	O
erently	O
.	O
we	O
can	O
then	O
count	O
the	O
number	O
of	O
threshold	O
functions	B
by	O
counting	B
how	O
many	O
distinguishable	O
regions	O
there	O
are	O
in	O
weight	O
space	O
.	O
consider	O
(	O
cid:12	O
)	O
rst	O
the	O
set	B
of	O
weight	B
vectors	O
in	O
weight	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
486	O
40	O
|	O
capacity	B
of	O
a	O
single	B
neuron	I
x2	O
(	O
0,1	O
)	O
x1	O
x	O
(	O
2	O
)	O
x	O
(	O
1	O
)	O
(	O
a	O
)	O
(	O
b	O
)	O
w2	O
(	O
0,0	O
)	O
(	O
1,1	O
)	O
w2	O
(	O
0,0,1	O
)	O
x2	O
x	O
(	O
3	O
)	O
x1	O
x	O
(	O
2	O
)	O
x	O
(	O
1	O
)	O
(	O
a	O
)	O
(	O
0,1,1	O
)	O
       	O
       	O
       	O
	O
       	O
	O
       	O
	O
       	O
	O
       	O
	O
       	O
	O
       	O
	O
       	O
	O
       	O
	O
       	O
	O
       	O
	O
       	O
	O
       	O
	O
       	O
	O
       	O
	O
	O
	O
(	O
0,1,0	O
)	O
(	O
b	O
)	O
(	O
1,1,0	O
)	O
figure	O
40.3.	O
two	O
data	O
points	O
in	O
a	O
two-dimensional	B
input	O
space	O
,	O
and	O
the	O
four	O
regions	O
of	O
weight	O
space	O
that	O
give	O
the	O
four	O
alternative	O
labellings	O
.	O
figure	O
40.4.	O
three	O
data	O
points	O
in	O
a	O
two-dimensional	B
input	O
space	O
,	O
and	O
the	O
six	B
regions	O
of	O
weight	O
space	O
that	O
give	O
alternative	O
labellings	O
of	O
those	O
points	O
.	O
in	O
this	O
case	O
,	O
the	O
labellings	O
(	O
0	O
;	O
0	O
;	O
0	O
)	O
and	O
(	O
1	O
;	O
1	O
;	O
1	O
)	O
can	O
not	O
be	O
realized	O
.	O
for	O
any	O
three	O
points	O
in	O
general	O
position	O
there	O
are	O
always	O
two	O
labellings	O
that	O
can	O
not	O
be	O
realized	O
.	O
(	O
1,0	O
)	O
w1	O
(	O
1,0,1	O
)	O
w1	O
(	O
1,0,0	O
)	O
space	O
that	O
classify	O
a	O
particular	O
example	O
x	O
(	O
n	O
)	O
as	O
a	O
1.	O
for	O
example	O
,	O
(	O
cid:12	O
)	O
gure	O
40.2a	O
shows	O
a	O
single	O
point	O
in	O
our	O
two-dimensional	B
x-space	O
,	O
and	O
(	O
cid:12	O
)	O
gure	O
40.2b	O
shows	O
the	O
two	O
corresponding	O
sets	O
of	O
points	O
in	O
w-space	O
.	O
one	O
set	B
of	O
weight	B
vectors	O
occupy	O
the	O
half	O
space	O
x	O
(	O
n	O
)	O
(	O
cid:1	O
)	O
w	O
>	O
0	O
;	O
(	O
40.3	O
)	O
and	O
the	O
others	O
occupy	O
x	O
(	O
n	O
)	O
(	O
cid:1	O
)	O
w	O
<	O
0.	O
in	O
(	O
cid:12	O
)	O
gure	O
40.3a	O
we	O
have	O
added	O
a	O
second	O
point	O
in	O
the	O
input	O
space	O
.	O
there	O
are	O
now	O
4	O
possible	O
labellings	O
:	O
(	O
1	O
;	O
1	O
)	O
,	O
(	O
1	O
;	O
0	O
)	O
,	O
(	O
0	O
;	O
1	O
)	O
,	O
and	O
(	O
0	O
;	O
0	O
)	O
.	O
figure	O
40.3b	O
shows	O
the	O
two	O
hyperplanes	O
x	O
(	O
1	O
)	O
(	O
cid:1	O
)	O
w	O
=	O
0	O
and	O
x	O
(	O
2	O
)	O
(	O
cid:1	O
)	O
w	O
=	O
0	O
which	O
separate	O
the	O
sets	O
of	O
weight	O
vectors	B
that	O
produce	O
each	O
of	O
these	O
labellings	O
.	O
when	O
n	O
=	O
3	O
(	O
(	O
cid:12	O
)	O
gure	O
40.4	O
)	O
,	O
weight	B
space	I
is	O
divided	O
by	O
three	O
hyperplanes	O
into	O
six	B
regions	O
.	O
not	O
all	O
of	O
the	O
eight	O
conceivable	O
labellings	O
can	O
be	O
realized	O
.	O
thus	O
t	O
(	O
3	O
;	O
2	O
)	O
=	O
6.	O
in	O
k	O
=	O
3	O
dimensions	B
we	O
now	O
use	O
this	O
weight	B
space	I
visualization	O
to	O
study	O
the	O
three	O
dimensional	O
case	O
.	O
let	O
us	O
imagine	O
adding	O
one	O
point	O
at	O
a	O
time	O
and	O
count	O
the	O
number	O
of	O
thresh-	O
old	O
functions	B
as	O
we	O
do	O
so	O
.	O
when	O
n	O
=	O
2	O
,	O
weight	B
space	I
is	O
divided	O
by	O
two	O
hy-	O
perplanes	O
x	O
(	O
1	O
)	O
(	O
cid:1	O
)	O
w	O
=	O
0	O
and	O
x	O
(	O
2	O
)	O
(	O
cid:1	O
)	O
w	O
=	O
0	O
into	O
four	O
regions	O
;	O
in	O
any	O
one	O
region	O
all	O
vectors	B
w	O
produce	O
the	O
same	O
function	B
on	O
the	O
2	O
input	O
vectors	O
.	O
thus	O
t	O
(	O
2	O
;	O
3	O
)	O
=	O
4.	O
adding	O
a	O
third	O
point	O
in	O
general	O
position	O
produces	O
a	O
third	O
plane	O
in	O
w	O
space	O
,	O
so	O
that	O
there	O
are	O
8	O
distinguishable	O
regions	O
.	O
t	O
(	O
3	O
;	O
3	O
)	O
=	O
8.	O
the	O
three	O
bisecting	O
planes	O
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
40.5a	O
.	O
at	O
this	O
point	O
matters	O
become	O
slightly	O
more	O
tricky	O
.	O
as	O
(	O
cid:12	O
)	O
gure	O
40.5b	O
illus-	O
trates	O
,	O
the	O
fourth	O
plane	O
in	O
the	O
three-dimensional	O
w	O
space	O
can	O
not	O
transect	O
all	O
eight	O
of	O
the	O
sets	O
created	O
by	O
the	O
(	O
cid:12	O
)	O
rst	O
three	O
planes	O
.	O
six	B
of	O
the	O
existing	O
regions	O
are	O
cut	O
in	O
two	O
and	O
the	O
remaining	O
two	O
are	O
una	O
(	O
cid:11	O
)	O
ected	O
.	O
so	O
t	O
(	O
4	O
;	O
3	O
)	O
=	O
14.	O
two	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
40.3	O
:	O
counting	B
threshold	O
functions	B
487	O
(	O
a	O
)	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
    	O
	O
    	O
	O
    	O
	O
    	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
(	O
b	O
)	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
k	O
4	O
2	O
5	O
2	O
6	O
2	O
7	O
2	O
8	O
2	O
n	O
1	O
2	O
3	O
4	O
5	O
6	O
1	O
2	O
2	O
2	O
2	O
2	O
2	O
3	O
2	O
4	O
8	O
14	O
2	O
2	O
4	O
6	O
8	O
10	O
12	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
(	O
b	O
)	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
(	O
a	O
)	O
(	O
c	O
)	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
``	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
``	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
``	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
``	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
``	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
``	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
``	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
``	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
``	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
``	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
``	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
	O
!	O
``	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
''	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
figure	O
40.5.	O
weight	B
space	I
illustrations	O
for	O
t	O
(	O
3	O
;	O
3	O
)	O
and	O
t	O
(	O
4	O
;	O
3	O
)	O
.	O
(	O
a	O
)	O
t	O
(	O
3	O
;	O
3	O
)	O
=	O
8.	O
three	O
hyperplanes	O
(	O
corresponding	O
to	O
three	O
points	O
in	O
general	O
position	O
)	O
divide	O
3-space	O
into	O
8	O
regions	O
,	O
shown	O
here	O
by	O
colouring	O
the	O
relevant	O
part	O
of	O
the	O
surface	O
of	O
a	O
hollow	O
,	O
semi-transparent	O
cube	O
centred	O
on	O
the	O
origin	O
.	O
(	O
b	O
)	O
t	O
(	O
4	O
;	O
3	O
)	O
=	O
14.	O
four	O
hyperplanes	O
divide	O
3-space	O
into	O
14	O
regions	O
,	O
of	O
which	O
this	O
(	O
cid:12	O
)	O
gure	O
shows	O
13	O
(	O
the	O
14th	O
region	O
is	O
out	O
of	O
view	O
on	O
the	O
right-hand	O
face	O
.	O
compare	O
with	O
(	O
cid:12	O
)	O
gure	O
40.5a	O
:	O
all	O
of	O
the	O
regions	O
that	O
are	O
not	O
coloured	B
white	O
have	O
been	O
cut	O
into	O
two	O
.	O
table	O
40.6.	O
values	O
of	O
t	O
(	O
n	O
;	O
k	O
)	O
deduced	O
by	O
hand	O
.	O
figure	O
40.7.	O
illustration	O
of	O
the	O
cutting	O
process	O
going	O
from	O
t	O
(	O
3	O
;	O
3	O
)	O
to	O
t	O
(	O
4	O
;	O
3	O
)	O
.	O
(	O
a	O
)	O
the	O
eight	O
regions	O
of	O
(	O
cid:12	O
)	O
gure	O
40.5a	O
with	O
one	O
added	O
hyperplane	O
.	O
all	O
of	O
the	O
regions	O
that	O
are	O
not	O
coloured	B
white	O
have	O
been	O
cut	O
into	O
two	O
.	O
(	O
b	O
)	O
here	O
,	O
the	O
hollow	O
cube	O
has	O
been	O
made	O
solid	O
,	O
so	O
we	O
can	O
see	O
which	O
regions	O
are	O
cut	O
by	O
the	O
fourth	O
plane	O
.	O
the	O
front	O
half	O
of	O
the	O
cube	O
has	O
been	O
cut	O
away	O
.	O
(	O
c	O
)	O
this	O
(	O
cid:12	O
)	O
gure	O
shows	O
the	O
new	O
two	O
dimensional	O
hyperplane	O
,	O
which	O
is	O
divided	O
into	O
six	B
regions	O
by	O
the	O
three	O
one-dimensional	O
hyperplanes	O
(	O
lines	O
)	O
which	O
cross	O
it	O
.	O
each	O
of	O
these	O
regions	O
corresponds	O
to	O
one	O
of	O
the	O
three-dimensional	O
regions	O
in	O
(	O
cid:12	O
)	O
gure	O
40.7a	O
which	O
is	O
cut	O
into	O
two	O
by	O
this	O
new	O
hyperplane	O
.	O
this	O
shows	O
that	O
t	O
(	O
4	O
;	O
3	O
)	O
(	O
cid:0	O
)	O
t	O
(	O
3	O
;	O
3	O
)	O
=	O
6.	O
figure	O
40.7c	O
should	O
be	O
compared	O
with	O
(	O
cid:12	O
)	O
gure	O
40.4b	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
488	O
40	O
|	O
capacity	B
of	O
a	O
single	B
neuron	I
of	O
the	O
binary	O
functions	O
on	O
4	O
points	O
in	O
3	O
dimensions	B
can	O
not	O
be	O
realized	O
by	O
a	O
linear	B
threshold	O
function	B
.	O
we	O
have	O
now	O
(	O
cid:12	O
)	O
lled	O
in	O
the	O
values	O
of	O
t	O
(	O
n	O
;	O
k	O
)	O
shown	O
in	O
table	O
40.6.	O
can	O
we	O
obtain	O
any	O
insights	O
into	O
our	O
derivation	B
of	O
t	O
(	O
4	O
;	O
3	O
)	O
in	O
order	O
to	O
(	O
cid:12	O
)	O
ll	O
in	O
the	O
rest	O
of	O
the	O
table	O
for	O
t	O
(	O
n	O
;	O
k	O
)	O
?	O
why	O
was	O
t	O
(	O
4	O
;	O
3	O
)	O
greater	O
than	O
t	O
(	O
3	O
;	O
3	O
)	O
by	O
six	O
?	O
six	B
is	O
the	O
number	O
of	O
regions	O
that	O
the	O
new	O
hyperplane	O
bisected	O
in	O
w-space	O
(	O
(	O
cid:12	O
)	O
gure	O
40.7a	O
b	O
)	O
.	O
equivalently	O
,	O
if	O
we	O
look	O
in	O
the	O
k	O
(	O
cid:0	O
)	O
1	O
dimensional	O
subspace	O
that	O
is	O
the	O
n	O
th	O
hyperplane	O
,	O
that	O
subspace	O
is	O
divided	O
into	O
six	B
regions	O
by	O
the	O
n	O
(	O
cid:0	O
)	O
1	O
previous	O
hyperplanes	O
(	O
(	O
cid:12	O
)	O
gure	O
40.7c	O
)	O
.	O
now	O
this	O
is	O
a	O
concept	O
we	O
have	O
met	O
before	O
.	O
compare	O
(	O
cid:12	O
)	O
gure	O
40.7c	O
with	O
(	O
cid:12	O
)	O
gure	O
40.4b	O
.	O
how	O
many	O
regions	O
are	O
created	O
by	O
n	O
(	O
cid:0	O
)	O
1	O
hyperplanes	O
in	O
a	O
k	O
(	O
cid:0	O
)	O
1	O
dimensional	O
space	O
?	O
why	O
,	O
t	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
;	O
k	O
(	O
cid:0	O
)	O
1	O
)	O
,	O
of	O
course	O
!	O
in	O
the	O
present	O
case	O
n	O
=	O
4	O
,	O
k	O
=	O
3	O
,	O
we	O
can	O
look	O
up	O
t	O
(	O
3	O
;	O
2	O
)	O
=	O
6	O
in	O
the	O
previous	O
section	B
.	O
so	O
t	O
(	O
4	O
;	O
3	O
)	O
=	O
t	O
(	O
3	O
;	O
3	O
)	O
+	O
t	O
(	O
3	O
;	O
2	O
)	O
:	O
(	O
40.4	O
)	O
recurrence	O
relation	O
for	O
any	O
n	O
;	O
k	O
generalizing	O
this	O
picture	O
,	O
we	O
see	O
that	O
when	O
we	O
add	O
an	O
n	O
th	O
hyperplane	O
in	O
k	O
dimensions	B
,	O
it	O
will	O
bisect	O
t	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
;	O
k	O
(	O
cid:0	O
)	O
1	O
)	O
of	O
the	O
t	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
;	O
k	O
)	O
regions	O
that	O
were	O
created	O
by	O
the	O
previous	O
n	O
(	O
cid:0	O
)	O
1	O
hyperplanes	O
.	O
therefore	O
,	O
the	O
total	O
number	O
of	O
regions	O
obtained	O
after	O
adding	O
the	O
n	O
th	O
hyperplane	O
is	O
2t	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
;	O
k	O
(	O
cid:0	O
)	O
1	O
)	O
(	O
since	O
t	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
;	O
k	O
(	O
cid:0	O
)	O
1	O
)	O
out	O
of	O
t	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
;	O
k	O
)	O
regions	O
are	O
split	O
in	O
two	O
)	O
plus	O
the	O
remaining	O
t	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
;	O
k	O
)	O
(	O
cid:0	O
)	O
t	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
;	O
k	O
(	O
cid:0	O
)	O
1	O
)	O
regions	O
not	O
split	O
by	O
the	O
n	O
th	O
hyperplane	O
,	O
which	O
gives	O
the	O
following	O
equation	O
for	O
t	O
(	O
n	O
;	O
k	O
)	O
:	O
t	O
(	O
n	O
;	O
k	O
)	O
=	O
t	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
;	O
k	O
)	O
+	O
t	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
;	O
k	O
(	O
cid:0	O
)	O
1	O
)	O
:	O
(	O
40.5	O
)	O
now	O
all	O
that	O
remains	O
is	O
to	O
solve	O
this	O
recurrence	O
relation	O
given	O
the	O
boundary	O
conditions	O
t	O
(	O
n	O
;	O
1	O
)	O
=	O
2	O
and	O
t	O
(	O
1	O
;	O
k	O
)	O
=	O
2.	O
does	O
the	O
recurrence	O
relation	O
(	O
40.5	O
)	O
look	O
familiar	O
?	O
maybe	O
you	O
remember	O
building	O
pascal	O
’	O
s	O
triangle	B
by	O
adding	O
together	O
two	O
adjacent	O
numbers	O
in	O
one	O
row	O
to	O
get	O
the	O
number	O
below	O
.	O
the	O
n	O
;	O
k	O
element	O
of	O
pascal	O
’	O
s	O
triangle	B
is	O
equal	O
to	O
c	O
(	O
n	O
;	O
k	O
)	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
n	O
k	O
(	O
cid:19	O
)	O
(	O
cid:17	O
)	O
n	O
!	O
(	O
n	O
(	O
cid:0	O
)	O
k	O
)	O
!	O
k	O
!	O
:	O
(	O
40.6	O
)	O
table	O
40.8.	O
pascal	O
’	O
s	O
triangle	B
.	O
n	O
0	O
1	O
2	O
3	O
4	O
5	O
k	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
1	O
1	O
1	O
1	O
2	O
1	O
1	O
3	O
3	O
1	O
1	O
4	O
6	O
4	O
1	O
1	O
5	O
10	O
10	O
5	O
1	O
k	O
(	O
cid:1	O
)	O
satisfy	O
the	O
equation	O
combinations	O
(	O
cid:0	O
)	O
n	O
c	O
(	O
n	O
;	O
k	O
)	O
=	O
c	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
;	O
k	O
(	O
cid:0	O
)	O
1	O
)	O
+	O
c	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
;	O
k	O
)	O
;	O
[	O
here	O
we	O
are	O
adopting	O
the	O
convention	O
that	O
(	O
cid:0	O
)	O
n	O
k	O
(	O
cid:1	O
)	O
(	O
cid:17	O
)	O
0	O
if	O
k	O
>	O
n	O
or	O
k	O
<	O
0	O
.	O
]	O
k	O
(	O
cid:1	O
)	O
satis	O
(	O
cid:12	O
)	O
es	O
the	O
required	O
recurrence	O
relation	O
(	O
40.5	O
)	O
.	O
this	O
doesn	O
’	O
t	O
mean	B
so	O
(	O
cid:0	O
)	O
n	O
k	O
(	O
cid:1	O
)	O
,	O
since	O
many	O
functions	B
can	O
satisfy	O
one	O
recurrence	O
relation	O
.	O
t	O
(	O
n	O
;	O
k	O
)	O
=	O
(	O
cid:0	O
)	O
n	O
for	O
all	O
n	O
>	O
0	O
:	O
(	O
40.7	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
40.3	O
:	O
counting	B
threshold	O
functions	B
489	O
1	O
0.75	O
0.5	O
0.25	O
(	O
a	O
)	O
n	O
0	O
10	O
20	O
30	O
40	O
50	O
60	O
70	O
1	O
0.75	O
0.5	O
0.25	O
(	O
c	O
)	O
0	O
0	O
1	O
0.75	O
0.5	O
0.25	O
0	O
70	O
60	O
50	O
40	O
30	O
20	O
10	O
k	O
(	O
b	O
)	O
n=k	O
k=n/2	O
10	O
20	O
30	O
k	O
40	O
50	O
60	O
70	O
n=k	O
n=2k	O
50	O
n	O
100	O
150	O
log	O
t	O
(	O
n	O
,	O
k	O
)	O
log	O
2^n	O
2400	O
2300	O
2200	O
2100	O
2000	O
1900	O
n=k	O
n=2k	O
0.5	O
1	O
1.5	O
n/k	O
2	O
2.5	O
3	O
(	O
d	O
)	O
1800	O
1800	O
1900	O
2000	O
2100	O
2200	O
2300	O
2400	O
figure	O
40.9.	O
the	O
fraction	O
of	O
functions	O
on	O
n	O
points	O
in	O
k	O
dimensions	B
that	O
are	O
linear	B
threshold	O
functions	B
,	O
t	O
(	O
n	O
;	O
k	O
)	O
=2n	O
,	O
shown	O
from	O
various	O
viewpoints	O
.	O
in	O
(	O
a	O
)	O
we	O
see	O
the	O
dependence	O
on	O
k	O
,	O
which	O
is	O
approximately	O
an	O
error	B
function	I
passing	O
through	O
0.5	O
at	O
k	O
=	O
n=2	O
;	O
the	O
fraction	O
reaches	O
1	O
at	O
k	O
=	O
n	O
.	O
in	O
(	O
b	O
)	O
we	O
see	O
the	O
dependence	O
on	O
n	O
,	O
which	O
is	O
1	O
up	O
to	O
n	O
=	O
k	O
and	O
drops	O
sharply	O
at	O
n	O
=	O
2k	O
.	O
panel	O
(	O
c	O
)	O
shows	O
the	O
dependence	O
on	O
n=k	O
for	O
k	O
=	O
1000.	O
there	O
is	O
a	O
sudden	O
drop	O
in	O
the	O
fraction	O
of	O
realizable	O
labellings	O
when	O
n	O
=	O
2k	O
.	O
panel	O
(	O
d	O
)	O
shows	O
the	O
values	O
of	O
log2	O
t	O
(	O
n	O
;	O
k	O
)	O
and	O
log2	O
2n	O
as	O
a	O
function	B
of	O
n	O
for	O
k	O
=	O
1000.	O
these	O
(	O
cid:12	O
)	O
gures	O
were	O
plotted	O
using	O
the	O
approximation	B
of	O
t	O
=2n	O
by	O
the	O
error	B
function	I
.	O
but	O
perhaps	O
we	O
can	O
express	O
t	O
(	O
n	O
;	O
k	O
)	O
as	O
a	O
linear	B
superposition	O
of	O
combination	O
functions	B
of	O
the	O
form	O
c	O
(	O
cid:11	O
)	O
;	O
(	O
cid:12	O
)	O
(	O
n	O
;	O
k	O
)	O
(	O
cid:17	O
)	O
(	O
cid:0	O
)	O
n	O
+	O
(	O
cid:11	O
)	O
40.6	O
we	O
can	O
see	O
how	O
to	O
satisfy	O
the	O
boundary	O
conditions	O
:	O
we	O
simply	O
need	O
to	O
translate	O
pascal	O
’	O
s	O
triangle	B
to	O
the	O
right	O
by	O
1	O
,	O
2	O
,	O
3	O
,	O
:	O
:	O
:	O
;	O
superpose	O
;	O
add	O
;	O
multiply	O
by	O
two	O
,	O
and	O
drop	O
the	O
whole	O
table	O
by	O
one	O
line	O
.	O
thus	O
:	O
k+	O
(	O
cid:12	O
)	O
(	O
cid:1	O
)	O
.	O
by	O
comparing	O
tables	O
40.8	O
and	O
t	O
(	O
n	O
;	O
k	O
)	O
=	O
2	O
k	O
(	O
cid:0	O
)	O
1	O
k	O
(	O
cid:19	O
)	O
:	O
xk=0	O
(	O
cid:18	O
)	O
n	O
(	O
cid:0	O
)	O
1	O
(	O
40.8	O
)	O
using	O
the	O
fact	O
that	O
the	O
n	O
th	O
row	O
of	O
pascal	O
’	O
s	O
triangle	B
sums	O
to	O
2n	O
,	O
that	O
is	O
,	O
k=0	O
(	O
cid:0	O
)	O
n	O
(	O
cid:0	O
)	O
1	O
pn	O
(	O
cid:0	O
)	O
1	O
k	O
(	O
cid:1	O
)	O
=	O
2n	O
(	O
cid:0	O
)	O
1	O
,	O
we	O
can	O
simplify	O
the	O
cases	O
where	O
k	O
(	O
cid:0	O
)	O
1	O
(	O
cid:21	O
)	O
n	O
(	O
cid:0	O
)	O
1	O
.	O
2n	O
t	O
(	O
n	O
;	O
k	O
)	O
=	O
(	O
cid:26	O
)	O
2pk	O
(	O
cid:0	O
)	O
1	O
k=0	O
(	O
cid:0	O
)	O
n	O
(	O
cid:0	O
)	O
1	O
k	O
(	O
cid:21	O
)	O
n	O
k	O
(	O
cid:1	O
)	O
k	O
<	O
n	O
:	O
(	O
40.9	O
)	O
interpretation	O
it	O
is	O
natural	B
to	O
compare	O
t	O
(	O
n	O
;	O
k	O
)	O
with	O
the	O
total	O
number	O
of	O
binary	O
functions	O
on	O
n	O
points	O
,	O
2n	O
.	O
the	O
ratio	O
t	O
(	O
n	O
;	O
k	O
)	O
=2n	O
tells	O
us	O
the	O
probability	B
that	O
an	O
arbitrary	O
labelling	O
ftngn	O
n=1	O
can	O
be	O
memorized	O
by	O
our	O
neuron	B
.	O
the	O
two	O
functions	B
are	O
equal	O
for	O
all	O
n	O
(	O
cid:20	O
)	O
k.	O
the	O
line	O
n	O
=	O
k	O
is	O
thus	O
a	O
special	O
line	O
,	O
de	O
(	O
cid:12	O
)	O
ning	O
the	O
maximum	O
number	O
of	O
points	O
on	O
which	O
any	O
arbitrary	O
labelling	O
can	O
be	O
realized	O
.	O
this	O
number	O
of	O
points	O
is	O
referred	O
to	O
as	O
the	O
vapnik	O
{	O
chervonenkis	O
dimension	O
(	O
vc	O
dimension	O
)	O
of	O
the	O
class	O
of	O
functions	O
.	O
the	O
vc	O
dimension	O
of	O
a	O
binary	O
threshold	O
function	B
on	O
k	O
dimensions	B
is	O
thus	O
k.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
490	O
40	O
|	O
capacity	B
of	O
a	O
single	B
neuron	I
what	O
is	O
interesting	O
is	O
(	O
for	O
large	O
k	O
)	O
the	O
number	O
of	O
points	O
n	O
such	O
that	O
almost	O
any	O
labelling	O
can	O
be	O
realized	O
.	O
the	O
ratio	O
t	O
(	O
n	O
;	O
k	O
)	O
=2n	O
is	O
,	O
for	O
n	O
<	O
2k	O
,	O
still	O
greater	O
than	O
1/2	O
,	O
and	O
for	O
large	O
k	O
the	O
ratio	O
is	O
very	O
close	O
to	O
1.	O
for	O
our	O
purposes	O
the	O
sum	O
in	O
equation	O
(	O
40.9	O
)	O
is	O
well	O
approximated	O
by	O
the	O
error	B
function	I
,	O
k	O
pn	O
=2	O
(	O
cid:19	O
)	O
;	O
k	O
(	O
cid:19	O
)	O
’	O
2n	O
(	O
cid:8	O
)	O
(	O
cid:18	O
)	O
k	O
(	O
cid:0	O
)	O
(	O
n=2	O
)	O
x0	O
(	O
cid:18	O
)	O
n	O
exp	O
(	O
(	O
cid:0	O
)	O
z2=2	O
)	O
=p2	O
(	O
cid:25	O
)	O
.	O
figure	O
40.9	O
shows	O
the	O
realizable	O
fraction	O
(	O
40.10	O
)	O
where	O
(	O
cid:8	O
)	O
(	O
z	O
)	O
(	O
cid:17	O
)	O
r	O
z	O
(	O
cid:0	O
)	O
1	O
t	O
(	O
n	O
;	O
k	O
)	O
=2n	O
as	O
a	O
function	B
of	O
n	O
and	O
k.	O
the	O
take-home	O
message	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
40.9c	O
:	O
although	O
the	O
fraction	O
t	O
(	O
n	O
;	O
k	O
)	O
=2n	O
is	O
less	O
than	O
1	O
for	O
n	O
>	O
k	O
,	O
it	O
is	O
only	O
negligibly	O
less	O
than	O
1	O
up	O
to	O
n	O
=	O
2k	O
;	O
there	O
,	O
there	O
is	O
a	O
catastrophic	O
drop	O
to	O
zero	O
,	O
so	O
that	O
for	O
n	O
>	O
2k	O
,	O
only	O
a	O
tiny	O
fraction	O
of	O
the	O
binary	O
labellings	O
can	O
be	O
realized	O
by	O
the	O
threshold	B
function	O
.	O
conclusion	O
the	O
capacity	B
of	O
a	O
linear	B
threshold	O
neuron	B
,	O
for	O
large	O
k	O
,	O
is	O
2	O
bits	O
per	O
weight	B
.	O
a	O
single	B
neuron	I
can	O
almost	O
certainly	O
memorize	O
up	O
to	O
n	O
=	O
2k	O
random	B
binary	O
labels	O
perfectly	O
,	O
but	O
will	O
almost	O
certainly	O
fail	O
to	O
memorize	O
more	O
.	O
40.4	O
further	O
exercises	O
.	O
exercise	O
40.4	O
.	O
[	O
2	O
]	O
can	O
a	O
(	O
cid:12	O
)	O
nite	O
set	B
of	O
2n	O
distinct	O
points	O
in	O
a	O
two-dimensional	B
space	O
be	O
split	O
in	O
half	O
by	O
a	O
straight	O
line	O
(	O
cid:15	O
)	O
if	O
the	O
points	O
are	O
in	O
general	O
position	O
?	O
(	O
cid:15	O
)	O
if	O
the	O
points	O
are	O
not	O
in	O
general	O
position	O
?	O
can	O
2n	O
points	O
in	O
a	O
k	O
dimensional	O
space	O
be	O
split	O
in	O
half	O
by	O
a	O
k	O
(	O
cid:0	O
)	O
1	O
dimensional	O
hyperplane	O
?	O
exercise	O
40.5	O
.	O
[	O
2	O
,	O
p.491	O
]	O
four	O
points	O
are	O
selected	O
at	O
random	B
on	O
the	O
surface	O
of	O
a	O
sphere	O
.	O
what	O
is	O
the	O
probability	B
that	O
all	O
of	O
them	O
lie	O
on	O
a	O
single	O
hemi-	O
sphere	O
?	O
how	O
does	O
this	O
question	O
relate	O
to	O
t	O
(	O
n	O
;	O
k	O
)	O
?	O
exercise	O
40.6	O
.	O
[	O
2	O
]	O
consider	O
the	O
binary	O
threshold	O
neuron	B
in	O
k	O
=	O
3	O
dimensions	B
,	O
and	O
the	O
set	B
of	O
points	O
fxg	O
=	O
f	O
(	O
1	O
;	O
0	O
;	O
0	O
)	O
;	O
(	O
0	O
;	O
1	O
;	O
0	O
)	O
;	O
(	O
0	O
;	O
0	O
;	O
1	O
)	O
;	O
(	O
1	O
;	O
1	O
;	O
1	O
)	O
g.	O
find	O
a	O
parameter	O
vector	O
w	O
such	O
that	O
the	O
neuron	B
memorizes	O
the	O
labels	O
:	O
(	O
a	O
)	O
ftg	O
=	O
f1	O
;	O
1	O
;	O
1	O
;	O
1g	O
;	O
(	O
b	O
)	O
ftg	O
=	O
f1	O
;	O
1	O
;	O
0	O
;	O
0g	O
.	O
find	O
an	O
unrealizable	O
labelling	O
ftg	O
.	O
.	O
exercise	O
40.7	O
.	O
[	O
3	O
]	O
in	O
this	O
chapter	O
we	O
constrained	B
all	O
our	O
hyperplanes	O
to	O
go	O
through	O
the	O
origin	O
.	O
in	O
this	O
exercise	O
,	O
we	O
remove	O
this	O
constraint	O
.	O
how	O
many	O
regions	O
in	O
a	O
plane	O
are	O
created	O
by	O
n	O
lines	O
in	O
general	O
position	O
?	O
exercise	O
40.8	O
.	O
[	O
2	O
]	O
estimate	O
in	O
bits	O
the	O
total	O
sensory	O
experience	O
that	O
you	O
have	O
had	O
in	O
your	O
life	B
{	O
visual	O
information	B
,	O
auditory	O
information	B
,	O
etc	O
.	O
estimate	O
how	O
much	O
information	O
you	O
have	O
memorized	O
.	O
estimate	O
the	O
information	B
content	I
of	O
the	O
works	O
of	O
shakespeare	O
.	O
compare	O
these	O
with	O
the	O
capacity	B
of	O
your	O
brain	B
assuming	O
you	O
have	O
1011	O
neurons	O
each	O
making	O
1000	O
synaptic	O
connections	O
,	O
and	O
that	O
the	O
capacity	B
result	O
for	O
one	O
neuron	B
(	O
two	O
bits	O
per	O
connection	O
)	O
applies	O
.	O
is	O
your	O
brain	B
full	O
yet	O
?	O
figure	O
40.10.	O
three	O
lines	O
in	O
a	O
plane	O
create	O
seven	O
regions	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
40.5	O
:	O
solutions	O
491	O
.	O
exercise	O
40.9	O
.	O
[	O
3	O
]	O
what	O
is	O
the	O
capacity	B
of	O
the	O
axon	O
of	O
a	O
spiking	O
neuron	B
,	O
viewed	O
in	O
bits	O
per	O
second	O
?	O
[	O
see	O
mackay	O
and	O
as	O
a	O
communication	B
channel	O
,	O
mcculloch	O
(	O
1952	O
)	O
for	O
an	O
early	O
publication	O
on	O
this	O
topic	O
.	O
]	O
multiply	O
by	O
the	O
number	O
of	O
axons	O
in	O
the	O
optic	B
nerve	I
(	O
about	O
106	O
)	O
or	O
cochlear	O
nerve	O
(	O
about	O
50	O
000	O
per	O
ear	O
)	O
to	O
estimate	O
again	O
the	O
rate	B
of	O
acquisition	O
sensory	O
experience	O
.	O
40.5	O
solutions	O
solution	O
to	O
exercise	O
40.5	O
(	O
p.490	O
)	O
.	O
the	O
probability	B
that	O
all	O
four	O
points	O
lie	O
on	O
a	O
single	O
hemisphere	O
is	O
t	O
(	O
4	O
;	O
3	O
)	O
=24	O
=	O
14=16	O
=	O
7=8	O
:	O
(	O
40.11	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
41	O
learning	B
as	I
inference	I
41.1	O
neural	B
network	I
learning	O
as	B
inference	I
in	O
chapter	O
39	O
we	O
trained	O
a	O
simple	O
neural	O
network	B
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
by	O
minimizing	O
an	O
objective	B
function	I
m	O
(	O
w	O
)	O
=	O
g	O
(	O
w	O
)	O
+	O
(	O
cid:11	O
)	O
ew	O
(	O
w	O
)	O
made	O
up	O
of	O
an	O
error	B
function	I
g	O
(	O
w	O
)	O
=	O
(	O
cid:0	O
)	O
xn	O
ht	O
(	O
n	O
)	O
ln	O
y	O
(	O
x	O
(	O
n	O
)	O
;	O
w	O
)	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
t	O
(	O
n	O
)	O
)	O
ln	O
(	O
1	O
(	O
cid:0	O
)	O
y	O
(	O
x	O
(	O
n	O
)	O
;	O
w	O
)	O
)	O
i	O
and	O
a	O
regularizer	O
ew	O
(	O
w	O
)	O
=	O
(	O
41.1	O
)	O
(	O
41.2	O
)	O
(	O
41.3	O
)	O
w2	O
i	O
:	O
1	O
2xi	O
this	O
neural	B
network	I
learning	O
process	O
can	O
be	O
given	O
the	O
following	O
probabilistic	O
interpretation	O
.	O
we	O
interpret	O
the	O
output	O
y	O
(	O
x	O
;	O
w	O
)	O
of	O
the	O
neuron	O
literally	O
as	O
de	O
(	O
cid:12	O
)	O
ning	O
(	O
when	O
its	O
parameters	B
w	O
are	O
speci	O
(	O
cid:12	O
)	O
ed	O
)	O
the	O
probability	B
that	O
an	O
input	O
x	O
belongs	O
to	O
class	O
t	O
=	O
1	O
,	O
rather	O
than	O
the	O
alternative	O
t	O
=	O
0.	O
thus	O
y	O
(	O
x	O
;	O
w	O
)	O
(	O
cid:17	O
)	O
p	O
(	O
t	O
=	O
1j	O
x	O
;	O
w	O
)	O
.	O
then	O
each	O
value	O
of	O
w	O
de	O
(	O
cid:12	O
)	O
nes	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
hypothesis	O
about	O
the	O
probability	O
of	O
class	O
1	O
relative	B
to	O
class	O
0	O
as	O
a	O
function	B
of	O
x.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
observed	O
data	O
d	O
to	O
be	O
the	O
targets	O
ftg	O
{	O
the	O
inputs	O
fxg	O
are	O
assumed	O
to	O
be	O
given	O
,	O
and	O
not	O
to	O
be	O
modelled	O
.	O
to	O
infer	O
w	O
given	O
the	O
data	O
,	O
we	O
require	O
a	O
likelihood	B
function	O
and	O
a	O
prior	B
probability	O
over	O
w.	O
the	O
likelihood	B
function	O
measures	O
how	O
well	O
the	O
parameters	B
w	O
predict	O
the	O
observed	O
data	O
;	O
it	O
is	O
the	O
probability	B
assigned	O
to	O
the	O
observed	O
t	O
values	O
by	O
the	O
model	B
with	O
parameters	B
set	O
to	O
w.	O
now	O
the	O
two	O
equations	O
p	O
(	O
t	O
=	O
1j	O
w	O
;	O
x	O
)	O
=	O
y	O
p	O
(	O
t	O
=	O
0j	O
w	O
;	O
x	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
y	O
can	O
be	O
rewritten	O
as	O
the	O
single	O
equation	O
p	O
(	O
tj	O
w	O
;	O
x	O
)	O
=	O
yt	O
(	O
1	O
(	O
cid:0	O
)	O
y	O
)	O
1	O
(	O
cid:0	O
)	O
t	O
=	O
exp	O
[	O
t	O
ln	O
y	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
t	O
)	O
ln	O
(	O
1	O
(	O
cid:0	O
)	O
y	O
)	O
]	O
:	O
so	O
the	O
error	B
function	I
g	O
can	O
be	O
interpreted	O
as	O
minus	O
the	O
log	O
likelihood	B
:	O
p	O
(	O
d	O
j	O
w	O
)	O
=	O
exp	O
[	O
(	O
cid:0	O
)	O
g	O
(	O
w	O
)	O
]	O
:	O
(	O
41.4	O
)	O
(	O
41.5	O
)	O
(	O
41.6	O
)	O
similarly	O
the	O
regularizer	O
can	O
be	O
interpreted	O
in	O
terms	O
of	O
a	O
log	O
prior	B
proba-	O
bility	O
distribution	B
over	O
the	O
parameters	B
:	O
p	O
(	O
w	O
j	O
(	O
cid:11	O
)	O
)	O
=	O
1	O
zw	O
(	O
(	O
cid:11	O
)	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
ew	O
)	O
:	O
(	O
41.7	O
)	O
492	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
41.2	O
:	O
illustration	O
for	O
a	O
neuron	B
with	O
two	O
weights	O
493	O
if	O
ew	O
is	O
quadratic	O
as	O
de	O
(	O
cid:12	O
)	O
ned	O
above	O
,	O
then	O
the	O
corresponding	O
prior	B
distribution	O
w	O
=	O
1=	O
(	O
cid:11	O
)	O
,	O
and	O
1=zw	O
(	O
(	O
cid:11	O
)	O
)	O
is	O
equal	O
to	O
(	O
(	O
cid:11	O
)	O
=2	O
(	O
cid:25	O
)	O
)	O
k=2	O
,	O
is	O
a	O
gaussian	O
with	O
variance	O
(	O
cid:27	O
)	O
2	O
where	O
k	O
is	O
the	O
number	O
of	O
parameters	O
in	O
the	O
vector	O
w.	O
the	O
objective	B
function	I
m	O
(	O
w	O
)	O
then	O
corresponds	O
to	O
the	O
inference	B
of	O
the	O
parameters	B
w	O
,	O
given	O
the	O
data	O
:	O
p	O
(	O
w	O
j	O
d	O
;	O
(	O
cid:11	O
)	O
)	O
=	O
=	O
=	O
p	O
(	O
d	O
j	O
w	O
)	O
p	O
(	O
w	O
j	O
(	O
cid:11	O
)	O
)	O
p	O
(	O
d	O
j	O
(	O
cid:11	O
)	O
)	O
e	O
(	O
cid:0	O
)	O
g	O
(	O
w	O
)	O
e	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
ew	O
(	O
w	O
)	O
=zw	O
(	O
(	O
cid:11	O
)	O
)	O
p	O
(	O
d	O
j	O
(	O
cid:11	O
)	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
m	O
(	O
w	O
)	O
)	O
:	O
1	O
zm	O
(	O
41.8	O
)	O
(	O
41.9	O
)	O
(	O
41.10	O
)	O
so	O
the	O
w	O
found	O
by	O
(	O
locally	O
)	O
minimizing	O
m	O
(	O
w	O
)	O
can	O
be	O
interpreted	O
as	O
the	O
(	O
locally	O
)	O
most	O
probable	O
parameter	O
vector	O
,	O
w	O
(	O
cid:3	O
)	O
.	O
from	O
now	O
on	O
we	O
will	O
refer	O
to	O
w	O
(	O
cid:3	O
)	O
as	O
wmp	O
.	O
why	O
is	O
it	O
natural	B
to	O
interpret	O
the	O
error	O
functions	O
as	O
log	O
probabilities	O
?	O
error	O
functions	O
are	O
usually	O
additive	O
.	O
for	O
example	O
,	O
g	O
is	O
a	O
sum	O
of	O
information	B
con-	O
tents	O
,	O
and	O
ew	O
is	O
a	O
sum	O
of	O
squared	O
weights	O
.	O
probabilities	O
,	O
on	O
the	O
other	O
hand	O
,	O
are	O
multiplicative	O
:	O
for	O
independent	O
events	O
x	O
and	O
y	O
,	O
the	O
joint	B
probability	O
is	O
p	O
(	O
x	O
;	O
y	O
)	O
=	O
p	O
(	O
x	O
)	O
p	O
(	O
y	O
)	O
.	O
the	O
logarithmic	O
mapping	B
maintains	O
this	O
correspondence	O
.	O
the	O
interpretation	O
of	O
m	O
(	O
w	O
)	O
as	O
a	O
log	O
probability	B
has	O
numerous	O
bene	O
(	O
cid:12	O
)	O
ts	O
,	O
some	O
of	O
which	O
we	O
will	O
discuss	O
in	O
a	O
moment	O
.	O
41.2	O
illustration	O
for	O
a	O
neuron	B
with	O
two	O
weights	O
in	O
the	O
case	O
of	O
a	O
neuron	B
with	O
just	O
two	O
inputs	O
and	O
no	O
bias	B
,	O
y	O
(	O
x	O
;	O
w	O
)	O
=	O
1	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
(	O
w1x1+w2x2	O
)	O
;	O
(	O
41.11	O
)	O
we	O
can	O
plot	O
the	O
posterior	B
probability	I
of	O
w	O
,	O
p	O
(	O
w	O
j	O
d	O
;	O
(	O
cid:11	O
)	O
)	O
/	O
exp	O
(	O
(	O
cid:0	O
)	O
m	O
(	O
w	O
)	O
)	O
.	O
imag-	O
ine	O
that	O
we	O
receive	O
some	O
data	O
as	O
shown	O
in	O
the	O
left	O
column	O
of	O
(	O
cid:12	O
)	O
gure	O
41.1.	O
each	O
data	O
point	O
consists	O
of	O
a	O
two-dimensional	B
input	O
vector	O
x	O
and	O
a	O
t	O
value	O
indicated	O
by	O
(	O
cid:2	O
)	O
(	O
t	O
=	O
1	O
)	O
or	O
2	O
(	O
t	O
=	O
0	O
)	O
.	O
the	O
likelihood	B
function	O
exp	O
(	O
(	O
cid:0	O
)	O
g	O
(	O
w	O
)	O
)	O
is	O
shown	O
as	O
a	O
function	B
of	O
w	O
in	O
the	O
second	O
column	O
.	O
it	O
is	O
a	O
product	O
of	O
functions	B
of	O
the	O
form	O
(	O
41.11	O
)	O
.	O
the	O
product	O
of	O
traditional	O
learning	B
is	O
a	O
point	O
in	O
w-space	O
,	O
the	O
estimator	B
w	O
(	O
cid:3	O
)	O
,	O
which	O
maximizes	O
the	O
posterior	B
probability	I
density	O
.	O
in	O
contrast	O
,	O
in	O
the	O
bayesian	O
view	O
,	O
the	O
product	O
of	O
learning	B
is	O
an	O
ensemble	B
of	O
plausible	O
parameter	O
values	O
(	O
bottom	O
right	O
of	O
(	O
cid:12	O
)	O
gure	O
41.1	O
)	O
.	O
we	O
do	O
not	O
choose	O
one	O
particular	O
hypothesis	O
w	O
;	O
rather	O
we	O
evaluate	O
their	O
posterior	O
probabilities	O
.	O
the	O
posterior	O
distribution	O
is	O
obtained	O
by	O
multiplying	O
the	O
likelihood	B
by	O
a	O
prior	B
distribution	O
over	O
w	O
space	O
(	O
shown	O
as	O
a	O
broad	O
gaussian	O
at	O
the	O
upper	O
right	O
of	O
(	O
cid:12	O
)	O
gure	O
41.1	O
)	O
.	O
the	O
posterior	O
ensemble	O
(	O
within	O
a	O
multiplicative	O
constant	O
)	O
is	O
shown	O
in	O
the	O
third	O
column	O
of	O
(	O
cid:12	O
)	O
gure	O
41.1	O
,	O
and	O
as	O
a	O
contour	O
plot	O
in	O
the	O
fourth	O
column	O
.	O
as	O
the	O
amount	O
of	O
data	O
increases	O
(	O
from	O
top	O
to	O
bottom	O
)	O
,	O
the	O
posterior	O
ensemble	O
becomes	O
increasingly	O
concentrated	O
around	O
the	O
most	O
probable	O
value	O
w	O
(	O
cid:3	O
)	O
.	O
41.3	O
beyond	O
optimization	B
:	O
making	O
predictions	O
let	O
us	O
consider	O
the	O
task	O
of	O
making	O
predictions	O
with	O
the	O
neuron	B
which	O
we	O
trained	O
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
in	O
section	O
39.3.	O
this	O
was	O
a	O
neuron	B
with	O
two	O
inputs	O
and	O
a	O
bias	B
.	O
y	O
(	O
x	O
;	O
w	O
)	O
=	O
:	O
(	O
41.12	O
)	O
1	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
(	O
w0+w1x1+w2x2	O
)	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
data	B
set	I
likelihood	O
probability	O
of	O
parameters	O
41	O
|	O
learning	B
as	I
inference	I
494	O
10	O
2	O
x	O
5	O
0	O
-5	O
n	O
=	O
0	O
(	O
constant	O
)	O
n	O
=	O
2	O
-5	O
0	O
w1	O
5	O
5	O
0	O
w2	O
-5	O
-5	O
0	O
w1	O
5	O
-5	O
0	O
w1	O
5	O
5	O
0	O
w2	O
-5	O
-5	O
0	O
w1	O
5	O
-5	O
0	O
w1	O
5	O
0.5	O
0.1	O
0.05	O
0.05	O
5	O
0	O
w2	O
-5	O
-10	O
-10	O
-5	O
0	O
x1	O
5	O
10	O
n	O
=	O
4	O
10	O
2	O
x	O
5	O
0	O
-5	O
-10	O
-10	O
-5	O
0	O
x1	O
5	O
10	O
n	O
=	O
6	O
10	O
2	O
x	O
5	O
0	O
-5	O
-5	O
0	O
w1	O
-5	O
0	O
w1	O
-5	O
0	O
w1	O
5	O
0	O
w2	O
-5	O
5	O
0	O
w2	O
-5	O
5	O
0	O
w2	O
-5	O
5	O
0	O
w2	O
-5	O
5	O
0	O
-5	O
5	O
0	O
-5	O
5	O
0	O
-5	O
5	O
0	O
-5	O
w2	O
w2	O
w2	O
w2	O
5	O
5	O
5	O
5	O
-10	O
-10	O
-5	O
0	O
x1	O
5	O
10	O
-5	O
0	O
w1	O
5	O
-5	O
0	O
w1	O
5	O
-5	O
0	O
w1	O
figure	O
41.1.	O
the	O
bayesian	O
interpretation	O
and	O
generalization	O
of	O
traditional	O
neural	B
network	I
learning	O
.	O
evolution	B
of	O
the	O
probability	B
distribution	O
over	O
parameters	O
as	O
data	O
arrive	O
.	O
a	O
b	O
(	O
a	O
)	O
(	O
b	O
)	O
0	O
2w	O
2	O
samples	O
from	O
p	O
(	O
w|d	O
,	O
h	O
)	O
10	O
5	O
0	O
10	O
(	O
c	O
)	O
wmp	O
1	O
w	O
1	O
a	O
b	O
5	O
figure	O
41.2.	O
making	O
predictions	O
.	O
(	O
a	O
)	O
the	O
function	B
performed	O
by	O
an	O
optimized	O
neuron	B
wmp	O
(	O
shown	O
by	O
three	O
of	O
its	O
contours	O
)	O
trained	O
with	O
weight	O
decay	O
,	O
(	O
cid:11	O
)	O
=	O
0:01	O
(	O
from	O
(	O
cid:12	O
)	O
gure	O
39.6	O
)	O
.	O
the	O
contours	O
shown	O
are	O
those	O
corresponding	O
to	O
a	O
=	O
0	O
;	O
(	O
cid:6	O
)	O
1	O
,	O
namely	O
y	O
=	O
0:5	O
;	O
0:27	O
and	O
0:73	O
.	O
(	O
b	O
)	O
are	O
these	O
predictions	O
more	O
reasonable	O
?	O
(	O
contours	O
shown	O
are	O
for	O
y	O
=	O
0:5	O
;	O
0:27	O
,	O
0:73	O
,	O
0:12	O
and	O
0:88	O
.	O
)	O
(	O
c	O
)	O
the	O
posterior	B
probability	I
of	O
w	O
(	O
schematic	O
)	O
;	O
the	O
bayesian	O
predictions	O
shown	O
in	O
(	O
b	O
)	O
were	O
obtained	O
by	O
averaging	O
together	O
the	O
predictions	O
made	O
by	O
each	O
possible	O
value	O
of	O
the	O
weights	O
w	O
,	O
with	O
each	O
value	O
of	O
w	O
receiving	O
a	O
vote	O
proportional	O
to	O
its	O
probability	B
under	O
the	O
posterior	O
ensemble	O
.	O
the	O
method	B
used	O
to	O
create	O
(	O
b	O
)	O
is	O
described	O
in	O
section	O
41.4.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
41.3	O
:	O
beyond	O
optimization	B
:	O
making	O
predictions	O
495	O
when	O
we	O
last	O
played	O
with	O
it	O
,	O
we	O
trained	O
it	O
by	O
minimizing	O
the	O
objective	B
function	I
m	O
(	O
w	O
)	O
=	O
g	O
(	O
w	O
)	O
+	O
(	O
cid:11	O
)	O
e	O
(	O
w	O
)	O
:	O
(	O
41.13	O
)	O
the	O
resulting	O
optimized	O
function	B
for	O
the	O
case	O
(	O
cid:11	O
)	O
=	O
0:01	O
is	O
reproduced	O
in	O
(	O
cid:12	O
)	O
g-	O
ure	O
41.2a	O
.	O
we	O
now	O
consider	O
the	O
task	O
of	O
predicting	O
the	O
class	O
t	O
(	O
n+1	O
)	O
corresponding	O
to	O
a	O
new	O
input	O
x	O
(	O
n+1	O
)	O
.	O
it	O
is	O
common	O
practice	O
,	O
when	O
making	O
predictions	O
,	O
simply	O
to	O
use	O
a	O
neural	B
network	I
with	O
its	O
weights	O
(	O
cid:12	O
)	O
xed	O
to	O
their	O
optimized	O
value	O
wmp	O
,	O
but	O
this	O
is	O
not	O
optimal	B
,	O
as	O
can	O
be	O
seen	O
intuitively	O
by	O
considering	O
the	O
predictions	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
41.2a	O
.	O
are	O
these	O
reasonable	O
predictions	O
?	O
consider	O
new	O
data	O
arriving	O
at	O
points	O
a	O
and	O
b.	O
the	O
best-	O
(	O
cid:12	O
)	O
t	O
model	B
assigns	O
both	O
of	O
these	O
examples	O
probability	B
0.2	O
of	O
being	O
in	O
class	O
1	O
,	O
because	O
they	O
have	O
the	O
same	O
value	O
of	O
w	O
mp	O
(	O
cid:1	O
)	O
x.	O
if	O
we	O
really	O
knew	O
that	O
w	O
was	O
equal	O
to	O
wmp	O
,	O
then	O
these	O
predictions	O
would	O
be	O
correct	O
.	O
but	O
we	O
do	O
not	O
know	O
w.	O
the	O
parameters	B
are	O
uncertain	O
.	O
intuitively	O
we	O
might	O
be	O
inclined	O
to	O
assign	O
a	O
less	O
con	O
(	O
cid:12	O
)	O
dent	O
probability	B
(	O
closer	O
to	O
0.5	O
)	O
at	O
b	O
than	O
at	O
a	O
,	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
41.2b	O
,	O
since	O
point	O
b	O
is	O
far	O
from	O
the	O
training	B
data	I
.	O
the	O
best-	O
(	O
cid:12	O
)	O
t	O
parameters	B
wmp	O
often	O
give	O
over-con	O
(	O
cid:12	O
)	O
dent	O
predictions	O
.	O
a	O
non-bayesian	O
approach	O
to	O
this	O
problem	O
is	O
to	O
downweight	O
all	O
predictions	O
uniformly	O
,	O
by	O
an	O
empirically	O
determined	O
factor	O
(	O
copas	O
,	O
1983	O
)	O
.	O
this	O
is	O
not	O
ideal	O
,	O
since	O
intuition	O
suggests	O
the	O
strength	O
of	O
the	O
predictions	O
at	O
b	O
should	O
be	O
downweighted	O
more	O
than	O
those	O
at	O
a.	O
a	O
bayesian	O
viewpoint	O
helps	O
us	O
to	O
understand	O
the	O
cause	O
of	O
the	O
problem	O
,	O
and	O
provides	O
a	O
straightforward	O
solution	O
.	O
in	O
a	O
nutshell	O
,	O
we	O
obtain	O
bayesian	O
predictions	O
by	O
taking	O
into	O
account	O
the	O
whole	O
posterior	O
ensemble	O
,	O
shown	O
schematically	O
in	O
(	O
cid:12	O
)	O
gure	O
41.2c	O
.	O
the	O
bayesian	O
prediction	B
of	O
a	O
new	O
datum	O
t	O
(	O
n+1	O
)	O
involves	O
marginalizing	O
over	O
the	O
parameters	B
(	O
and	O
over	O
anything	O
else	O
about	O
which	O
we	O
are	O
uncertain	O
)	O
.	O
for	O
simplicity	O
,	O
let	O
us	O
assume	O
that	O
the	O
weights	O
w	O
are	O
the	O
only	O
uncertain	O
quantities	O
{	O
the	O
weight	B
decay	I
rate	O
(	O
cid:11	O
)	O
and	O
the	O
model	B
h	O
itself	O
are	O
assumed	O
to	O
be	O
(	O
cid:12	O
)	O
xed	O
.	O
then	O
by	O
the	O
sum	B
rule	I
,	O
the	O
predictive	O
probability	O
of	O
a	O
new	O
target	O
t	O
(	O
n+1	O
)	O
at	O
a	O
location	O
x	O
(	O
n+1	O
)	O
is	O
:	O
p	O
(	O
t	O
(	O
n+1	O
)	O
j	O
x	O
(	O
n+1	O
)	O
;	O
d	O
;	O
(	O
cid:11	O
)	O
)	O
=z	O
dkw	O
p	O
(	O
t	O
(	O
n+1	O
)	O
j	O
x	O
(	O
n+1	O
)	O
;	O
w	O
;	O
(	O
cid:11	O
)	O
)	O
p	O
(	O
w	O
j	O
d	O
;	O
(	O
cid:11	O
)	O
)	O
;	O
(	O
41.14	O
)	O
where	O
k	O
is	O
the	O
dimensionality	O
of	O
w	O
,	O
three	O
in	O
the	O
toy	O
problem	O
.	O
thus	O
the	O
predictions	O
are	O
obtained	O
by	O
weighting	O
the	O
prediction	B
for	O
each	O
possible	O
w	O
,	O
p	O
(	O
t	O
(	O
n+1	O
)	O
=	O
1j	O
x	O
(	O
n+1	O
)	O
;	O
w	O
;	O
(	O
cid:11	O
)	O
)	O
=	O
y	O
(	O
x	O
(	O
n+1	O
)	O
;	O
w	O
)	O
p	O
(	O
t	O
(	O
n+1	O
)	O
=	O
0j	O
x	O
(	O
n+1	O
)	O
;	O
w	O
;	O
(	O
cid:11	O
)	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
y	O
(	O
x	O
(	O
n+1	O
)	O
;	O
w	O
)	O
;	O
(	O
41.15	O
)	O
with	O
a	O
weight	B
given	O
by	O
the	O
posterior	B
probability	I
of	O
w	O
,	O
p	O
(	O
w	O
j	O
d	O
;	O
(	O
cid:11	O
)	O
)	O
,	O
which	O
we	O
most	O
recently	O
wrote	O
down	O
in	O
equation	O
(	O
41.10	O
)	O
.	O
this	O
posterior	B
probability	I
is	O
1	O
zm	O
p	O
(	O
w	O
j	O
d	O
;	O
(	O
cid:11	O
)	O
)	O
=	O
exp	O
(	O
(	O
cid:0	O
)	O
m	O
(	O
w	O
)	O
)	O
;	O
zm	O
=z	O
dkw	O
exp	O
(	O
(	O
cid:0	O
)	O
m	O
(	O
w	O
)	O
)	O
:	O
(	O
41.16	O
)	O
(	O
41.17	O
)	O
where	O
in	O
summary	O
,	O
we	O
can	O
get	O
the	O
bayesian	O
predictions	O
if	O
we	O
can	O
(	O
cid:12	O
)	O
nd	O
a	O
way	O
of	O
computing	O
the	O
integral	B
p	O
(	O
t	O
(	O
n+1	O
)	O
=	O
1j	O
x	O
(	O
n+1	O
)	O
;	O
d	O
;	O
(	O
cid:11	O
)	O
)	O
=z	O
dkw	O
y	O
(	O
x	O
(	O
n+1	O
)	O
;	O
w	O
)	O
1	O
zm	O
exp	O
(	O
(	O
cid:0	O
)	O
m	O
(	O
w	O
)	O
)	O
;	O
(	O
41.18	O
)	O
which	O
is	O
the	O
average	B
of	O
the	O
output	O
of	O
the	O
neuron	O
at	O
x	O
(	O
n+1	O
)	O
under	O
the	O
posterior	O
distribution	O
of	O
w.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
496	O
41	O
|	O
learning	B
as	I
inference	I
figure	O
41.3.	O
one	O
step	O
of	O
the	O
langevin	O
method	B
in	O
two	O
dimensions	B
(	O
c	O
)	O
,	O
contrasted	O
with	O
a	O
traditional	O
‘	O
dumb	O
’	O
metropolis	O
method	B
(	O
a	O
)	O
and	O
with	O
gradient	B
descent	I
(	O
b	O
)	O
.	O
the	O
proposal	B
density	I
of	O
the	O
langevin	O
method	B
is	O
given	O
by	O
‘	O
gradient	B
descent	I
with	O
noise	B
’	O
.	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
(	O
cid:15	O
)	O
x	O
(	O
1	O
)	O
q	O
(	O
x	O
;	O
x	O
(	O
1	O
)	O
)	O
(	O
cid:0	O
)	O
(	O
cid:17	O
)	O
g	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
dumb	O
metropolis	O
gradient	B
descent	I
langevin	O
implementation	O
how	O
shall	O
we	O
compute	O
the	O
integral	B
(	O
41.18	O
)	O
?	O
for	O
our	O
toy	O
problem	O
,	O
the	O
weight	B
space	I
is	O
three	O
dimensional	O
;	O
for	O
a	O
realistic	O
neural	B
network	I
the	O
dimensionality	O
k	O
might	O
be	O
in	O
the	O
thousands	O
.	O
bayesian	O
inference	B
for	O
general	O
data	O
modelling	B
problems	O
may	O
be	O
imple-	O
mented	O
by	O
exact	O
methods	B
(	O
chapter	O
25	O
)	O
,	O
by	O
monte	O
carlo	O
sampling	O
(	O
chapter	O
29	O
)	O
,	O
or	O
by	O
deterministic	O
approximate	O
methods	B
,	O
for	O
example	O
,	O
methods	B
that	O
make	O
gaussian	O
approximations	O
to	O
p	O
(	O
w	O
j	O
d	O
;	O
(	O
cid:11	O
)	O
)	O
using	O
laplace	O
’	O
s	O
method	B
(	O
chap-	O
ter	O
27	O
)	O
or	O
variational	B
methods	I
(	O
chapter	O
33	O
)	O
.	O
for	O
neural	O
networks	O
there	O
are	O
few	O
exact	O
methods	O
.	O
the	O
two	O
main	O
approaches	O
to	O
implementing	O
bayesian	O
inference	B
for	O
neural	O
networks	O
are	O
the	O
monte	O
carlo	O
methods	B
developed	O
by	O
neal	O
(	O
1996	O
)	O
and	O
the	O
gaussian	O
approximation	B
methods	O
developed	O
by	O
mackay	O
(	O
1991	O
)	O
.	O
41.4	O
monte	O
carlo	O
implementation	O
of	O
a	O
single	B
neuron	I
first	O
we	O
will	O
use	O
a	O
monte	O
carlo	O
approach	O
in	O
which	O
the	O
task	O
of	O
evaluating	O
the	O
integral	B
(	O
41.18	O
)	O
is	O
solved	O
by	O
treating	O
y	O
(	O
x	O
(	O
n+1	O
)	O
;	O
w	O
)	O
as	O
a	O
function	B
f	O
of	O
w	O
whose	O
mean	B
we	O
compute	O
using	O
hf	O
(	O
w	O
)	O
i	O
’	O
f	O
(	O
w	O
(	O
r	O
)	O
)	O
1	O
rxr	O
(	O
41.19	O
)	O
zm	O
where	O
fw	O
(	O
r	O
)	O
g	O
are	O
samples	O
from	O
the	O
posterior	O
distribution	O
1	O
exp	O
(	O
(	O
cid:0	O
)	O
m	O
(	O
w	O
)	O
)	O
(	O
cf	O
.	O
equation	O
(	O
29.6	O
)	O
)	O
.	O
we	O
obtain	O
the	O
samples	O
using	O
a	O
metropolis	O
method	B
(	O
section	B
29.4	O
)	O
.	O
as	O
an	O
aside	O
,	O
a	O
possible	O
disadvantage	O
of	O
this	O
monte	O
carlo	O
approach	O
is	O
that	O
it	O
is	O
a	O
poor	O
way	O
of	O
estimating	O
the	O
probability	O
of	O
an	O
improbable	O
event	O
,	O
i.e.	O
,	O
a	O
p	O
(	O
tj	O
d	O
;	O
h	O
)	O
that	O
is	O
very	O
close	O
to	O
zero	O
,	O
if	O
the	O
improbable	O
event	O
is	O
most	O
likely	O
to	O
occur	O
in	O
conjunction	O
with	O
improbable	O
parameter	O
values	O
.	O
how	O
to	O
generate	O
the	O
samples	O
fw	O
(	O
r	O
)	O
g	O
?	O
radford	O
neal	O
introduced	O
the	O
hamil-	O
tonian	O
monte	O
carlo	O
method	B
to	O
neural	O
networks	O
.	O
we	O
met	O
this	O
sophisticated	O
metropolis	O
method	B
,	O
which	O
makes	O
use	O
of	O
gradient	O
information	B
,	O
in	O
chapter	O
30.	O
the	O
method	B
we	O
now	O
demonstrate	O
is	O
a	O
simple	O
version	O
of	O
hamiltonian	O
monte	O
carlo	O
called	O
the	O
langevin	O
monte	O
carlo	O
method	B
.	O
the	O
langevin	O
monte	O
carlo	O
method	B
the	O
langevin	O
method	B
(	O
algorithm	B
41.4	O
)	O
may	O
be	O
summarized	O
as	O
‘	O
gradient	O
de-	O
scent	O
with	O
added	O
noise	B
’	O
,	O
as	O
shown	O
pictorially	O
in	O
(	O
cid:12	O
)	O
gure	O
41.3.	O
a	O
noise	B
vector	O
p	O
is	O
generated	O
from	O
a	O
gaussian	O
with	O
unit	O
variance	B
.	O
the	O
gradient	O
g	O
is	O
computed	O
,	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
41.4	O
:	O
monte	O
carlo	O
implementation	O
of	O
a	O
single	B
neuron	I
497	O
g	O
=	O
gradm	O
(	O
w	O
)	O
;	O
m	O
=	O
findm	O
(	O
w	O
)	O
;	O
#	O
set	B
gradient	O
using	O
initial	O
w	O
#	O
set	B
objective	O
function	B
too	O
for	O
l	O
=	O
1	O
:	O
l	O
p	O
=	O
randn	O
(	O
size	O
(	O
w	O
)	O
)	O
;	O
h	O
=	O
p	O
’	O
*	O
p	O
/	O
2	O
+	O
m	O
;	O
#	O
loop	O
l	O
times	O
#	O
initial	O
momentum	B
is	O
normal	B
(	O
0,1	O
)	O
#	O
evaluate	O
h	O
(	O
w	O
,	O
p	O
)	O
*	O
*	O
*	O
*	O
p	O
=	O
p	O
-	O
epsilon	O
*	O
g	O
/	O
2	O
;	O
wnew	O
=	O
w	O
+	O
epsilon	O
*	O
p	O
;	O
gnew	O
=	O
gradm	O
(	O
wnew	O
)	O
;	O
p	O
=	O
p	O
-	O
epsilon	O
*	O
gnew	O
/	O
2	O
;	O
#	O
make	O
half-step	O
in	O
p	O
#	O
make	O
step	O
in	O
w	O
#	O
find	O
new	O
gradient	O
#	O
make	O
half-step	O
in	O
p	O
algorithm	B
41.4.	O
octave	B
source	O
code	B
for	O
the	O
langevin	O
monte	O
carlo	O
method	B
.	O
to	O
obtain	O
the	O
hamiltonian	O
monte	O
carlo	O
method	B
,	O
we	O
repeat	O
the	O
four	O
lines	O
marked	O
*	O
multiple	O
times	O
(	O
algorithm	B
41.8	O
)	O
.	O
mnew	O
=	O
findm	O
(	O
wnew	O
)	O
;	O
hnew	O
=	O
p	O
’	O
*	O
p	O
/	O
2	O
+	O
mnew	O
;	O
dh	O
=	O
hnew	O
-	O
h	O
;	O
if	O
(	O
dh	O
<	O
0	O
)	O
accept	O
=	O
1	O
;	O
elseif	O
(	O
rand	O
(	O
)	O
<	O
exp	O
(	O
-dh	O
)	O
)	O
accept	O
=	O
1	O
;	O
else	O
accept	O
=	O
0	O
;	O
endif	O
if	O
(	O
accept	O
)	O
g	O
=	O
gnew	O
;	O
w	O
=	O
wnew	O
;	O
#	O
find	O
new	O
objective	B
function	I
#	O
evaluate	O
new	O
value	O
of	O
h	O
#	O
decide	O
whether	O
to	O
accept	O
#	O
compare	O
with	O
a	O
uniform	O
#	O
variate	O
m	O
=	O
mnew	O
;	O
endif	O
endfor	O
function	B
gm	O
=	O
gradm	O
(	O
w	O
)	O
;	O
a	O
=	O
x	O
*	O
w	O
y	O
=	O
sigmoid	B
(	O
a	O
)	O
;	O
e	O
=	O
t	O
-	O
y	O
g	O
=	O
-	O
x	O
’	O
*	O
e	O
;	O
gm	O
=	O
alpha	O
*	O
w	O
+	O
g	O
;	O
;	O
#	O
gradient	O
of	O
objective	B
function	I
#	O
compute	O
activations	O
#	O
compute	O
outputs	O
#	O
compute	O
errors	B
#	O
compute	O
the	O
gradient	O
of	O
g	O
(	O
w	O
)	O
endfunction	O
function	B
m	O
=	O
findm	O
(	O
w	O
)	O
#	O
objective	B
function	I
g	O
=	O
-	O
(	O
t	O
’	O
*	O
log	O
(	O
y	O
)	O
+	O
(	O
1-t	O
’	O
)	O
*	O
log	O
(	O
1-y	O
)	O
)	O
;	O
ew	O
=	O
w	O
’	O
*	O
w	O
/	O
2	O
;	O
m	O
=	O
g	O
+	O
alpha	O
*	O
ew	O
;	O
endfunction	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
498	O
10	O
5	O
0	O
-5	O
-10	O
-15	O
-20	O
-25	O
-30	O
0	O
(	O
a	O
)	O
10000	O
20000	O
30000	O
40000	O
14	O
12	O
10	O
8	O
6	O
4	O
2	O
0	O
0	O
(	O
c	O
)	O
g	O
(	O
w	O
)	O
-	O
langevin	O
g	O
(	O
w	O
)	O
-	O
optimizer	O
10000	O
20000	O
30000	O
40000	O
41	O
|	O
learning	B
as	I
inference	I
12	O
10	O
8	O
6	O
4	O
2	O
0	O
0	O
(	O
d	O
)	O
m	O
(	O
w	O
)	O
-	O
langevin	O
m	O
(	O
w	O
)	O
-	O
optimizer	O
10000	O
20000	O
30000	O
40000	O
5	O
4	O
3	O
2	O
1	O
0	O
-1	O
-2	O
-3	O
-1	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
(	O
b	O
)	O
figure	O
41.5.	O
a	O
single	B
neuron	I
learning	O
under	O
the	O
langevin	O
monte	O
carlo	O
method	B
.	O
(	O
a	O
)	O
evolution	B
of	O
weights	O
w0	O
,	O
w1	O
and	O
w2	O
as	O
a	O
function	B
of	O
number	O
of	O
iterations	O
.	O
(	O
b	O
)	O
evolution	B
of	O
weights	O
w1	O
and	O
w2	O
in	O
weight	O
space	O
.	O
also	O
shown	O
by	O
a	O
line	O
is	O
the	O
evolution	B
of	O
the	O
weights	O
using	O
the	O
optimizer	O
of	O
(	O
cid:12	O
)	O
gure	O
39.6	O
.	O
(	O
c	O
)	O
the	O
error	B
function	I
g	O
(	O
w	O
)	O
as	O
a	O
function	B
of	O
number	O
of	O
iterations	O
.	O
also	O
shown	O
is	O
the	O
error	B
function	I
during	O
the	O
optimization	B
of	O
(	O
cid:12	O
)	O
gure	O
39.6	O
.	O
(	O
d	O
)	O
the	O
objective	B
function	I
m	O
(	O
x	O
)	O
as	O
a	O
function	B
of	O
number	O
of	O
iterations	O
.	O
see	O
also	O
(	O
cid:12	O
)	O
gures	O
41.6	O
and	O
41.7.	O
and	O
a	O
step	O
in	O
w	O
is	O
made	O
,	O
given	O
by	O
(	O
cid:1	O
)	O
w	O
=	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
cid:15	O
)	O
2g	O
+	O
(	O
cid:15	O
)	O
p	O
:	O
(	O
41.20	O
)	O
notice	O
that	O
if	O
the	O
(	O
cid:15	O
)	O
p	O
term	O
were	O
omitted	O
this	O
would	O
simply	O
be	O
gradient	B
descent	I
with	O
learning	B
rate	O
(	O
cid:17	O
)	O
=	O
1	O
2	O
(	O
cid:15	O
)	O
2.	O
this	O
step	O
in	O
w	O
is	O
accepted	O
or	O
rejected	O
depending	O
on	O
the	O
change	O
in	O
the	O
value	O
of	O
the	O
objective	O
function	B
m	O
(	O
w	O
)	O
and	O
on	O
the	O
change	O
in	O
gradient	O
,	O
with	O
a	O
probability	O
of	O
acceptance	O
such	O
that	O
detailed	B
balance	I
holds	O
.	O
the	O
langevin	O
method	B
has	O
one	O
free	O
parameter	O
,	O
(	O
cid:15	O
)	O
,	O
which	O
controls	O
the	O
typical	B
step	O
size	O
.	O
if	O
(	O
cid:15	O
)	O
is	O
set	B
to	O
too	O
large	O
a	O
value	O
,	O
moves	O
may	O
be	O
rejected	O
.	O
if	O
it	O
is	O
set	B
to	O
a	O
very	O
small	O
value	O
,	O
progress	O
around	O
the	O
state	O
space	O
will	O
be	O
slow	O
.	O
demonstration	O
of	O
langevin	O
method	B
the	O
langevin	O
method	B
is	O
demonstrated	O
in	O
(	O
cid:12	O
)	O
gures	O
41.5	O
,	O
41.6	O
and	O
41.7.	O
here	O
,	O
the	O
objective	B
function	I
is	O
m	O
(	O
w	O
)	O
=	O
g	O
(	O
w	O
)	O
+	O
(	O
cid:11	O
)	O
ew	O
(	O
w	O
)	O
,	O
with	O
(	O
cid:11	O
)	O
=	O
0:01.	O
these	O
(	O
cid:12	O
)	O
gures	O
include	O
,	O
for	O
comparison	O
,	O
the	O
results	O
of	O
the	O
previous	O
optimization	B
method	O
using	O
gradient	B
descent	I
on	O
the	O
same	O
objective	B
function	I
(	O
(	O
cid:12	O
)	O
gure	O
39.6	O
)	O
.	O
it	O
can	O
be	O
seen	O
that	O
the	O
mean	B
evolution	O
of	O
w	O
is	O
similar	O
to	O
the	O
evolution	B
of	O
the	O
parameters	B
under	O
gradient	B
descent	I
.	O
the	O
monte	O
carlo	O
method	B
appears	O
to	O
have	O
converged	O
to	O
the	O
posterior	O
distribution	O
after	O
about	O
10	O
000	O
iterations	O
.	O
the	O
average	B
acceptance	O
rate	B
during	O
this	O
simulation	O
was	O
93	O
%	O
;	O
only	O
7	O
%	O
of	O
the	O
proposed	O
moves	O
were	O
rejected	O
.	O
probably	O
,	O
faster	O
progress	O
around	O
the	O
state	O
space	O
would	O
have	O
been	O
made	O
if	O
a	O
larger	O
step	O
size	O
(	O
cid:15	O
)	O
had	O
been	O
used	O
,	O
but	O
the	O
value	O
was	O
chosen	O
so	O
that	O
the	O
‘	O
descent	O
rate	B
’	O
(	O
cid:17	O
)	O
=	O
1	O
2	O
(	O
cid:15	O
)	O
2	O
matched	O
the	O
step	O
size	O
of	O
the	O
earlier	O
simulations	O
.	O
making	O
bayesian	O
predictions	O
from	O
iteration	O
10,000	O
to	O
40,000	O
,	O
the	O
weights	O
were	O
sampled	O
every	O
1000	O
itera-	O
tions	O
and	O
the	O
corresponding	O
functions	B
of	O
x	O
are	O
plotted	O
in	O
(	O
cid:12	O
)	O
gure	O
41.6.	O
there	O
is	O
a	O
considerable	O
variety	O
of	O
plausible	O
functions	B
.	O
we	O
obtain	O
a	O
monte	O
carlo	O
ap-	O
proximation	O
to	O
the	O
bayesian	O
predictions	O
by	O
averaging	O
these	O
thirty	O
functions	B
of	O
x	O
together	O
.	O
the	O
result	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
41.7	O
and	O
contrasted	O
with	O
the	O
predic-	O
tions	O
given	O
by	O
the	O
optimized	O
parameters	B
.	O
the	O
bayesian	O
predictions	O
become	O
satisfyingly	O
moderate	O
as	O
we	O
move	O
away	O
from	O
the	O
region	O
of	O
highest	O
data	O
density	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
41.4	O
:	O
monte	O
carlo	O
implementation	O
of	O
a	O
single	B
neuron	I
499	O
figure	O
41.6.	O
samples	O
obtained	O
by	O
the	O
langevin	O
monte	O
carlo	O
method	B
.	O
the	O
learning	B
rate	O
was	O
set	B
to	O
(	O
cid:17	O
)	O
=	O
0:01	O
and	O
the	O
weight	B
decay	I
rate	O
to	O
(	O
cid:11	O
)	O
=	O
0:01.	O
the	O
step	O
size	O
is	O
given	O
by	O
(	O
cid:15	O
)	O
=	O
p2	O
(	O
cid:17	O
)	O
.	O
the	O
function	B
performed	O
by	O
the	O
neuron	B
is	O
shown	O
by	O
three	O
of	O
its	O
contours	O
every	O
1000	O
iterations	O
from	O
iteration	O
10	O
000	O
to	O
40	O
000.	O
the	O
contours	O
shown	O
are	O
those	O
corresponding	O
to	O
a	O
=	O
0	O
;	O
(	O
cid:6	O
)	O
1	O
,	O
namely	O
y	O
=	O
0:5	O
;	O
0:27	O
and	O
0:73.	O
also	O
shown	O
is	O
a	O
vector	O
proportional	O
to	O
(	O
w1	O
;	O
w2	O
)	O
.	O
(	O
a	O
)	O
10	O
5	O
0	O
(	O
b	O
)	O
10	O
10	O
5	O
5	O
0	O
0	O
0	O
5	O
10	O
0	O
0	O
5	O
5	O
10	O
10	O
figure	O
41.7.	O
bayesian	O
predictions	O
found	O
by	O
the	O
langevin	O
monte	O
carlo	O
method	B
compared	O
with	O
the	O
predictions	O
using	O
the	O
optimized	O
parameters	B
.	O
(	O
a	O
)	O
the	O
predictive	O
function	O
obtained	O
by	O
av-	O
eraging	O
the	O
predictions	O
for	O
30	O
samples	O
uniformly	O
spaced	O
between	O
iterations	O
10	O
000	O
and	O
40	O
000	O
,	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
41.6.	O
the	O
contours	O
shown	O
are	O
those	O
corresponding	O
to	O
a	O
=	O
0	O
;	O
(	O
cid:6	O
)	O
1	O
;	O
(	O
cid:6	O
)	O
2	O
,	O
namely	O
y	O
=	O
0:5	O
;	O
0:27	O
,	O
0:73	O
,	O
0:12	O
and	O
0:88	O
.	O
(	O
b	O
)	O
for	O
contrast	O
,	O
the	O
predictions	O
given	O
by	O
the	O
‘	O
most	O
probable	O
’	O
setting	O
of	O
the	O
neuron	O
’	O
s	O
parameters	B
,	O
as	O
given	O
by	O
optimization	O
of	O
m	O
(	O
w	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
41	O
|	O
learning	B
as	I
inference	I
algorithm	O
41.8.	O
octave	B
source	O
code	B
for	O
the	O
hamiltonian	O
monte	O
carlo	O
method	B
.	O
the	O
algorithm	B
is	O
identical	O
to	O
the	O
langevin	O
method	B
in	O
algorithm	B
41.4	O
,	O
except	O
for	O
the	O
replacement	O
of	O
the	O
four	O
lines	O
marked	O
*	O
in	O
that	O
algorithm	B
by	O
the	O
fragment	O
shown	O
here	O
.	O
figure	O
41.9.	O
comparison	O
of	O
sampling	O
properties	O
of	O
the	O
langevin	O
monte	O
carlo	O
method	B
and	O
the	O
hamiltonian	O
monte	O
carlo	O
(	O
hmc	O
)	O
method	B
.	O
the	O
horizontal	O
axis	O
is	O
the	O
number	O
of	O
gradient	O
evaluations	O
made	O
.	O
each	O
(	O
cid:12	O
)	O
gure	O
shows	O
the	O
weights	O
during	O
the	O
(	O
cid:12	O
)	O
rst	O
10,000	O
iterations	O
.	O
the	O
rejection	B
rate	O
during	O
this	O
hamiltonian	O
monte	O
carlo	O
simulation	O
was	O
8	O
%	O
.	O
500	O
wnew	O
=	O
w	O
;	O
gnew	O
=	O
g	O
;	O
for	O
tau	O
=	O
1	O
:	O
tau	O
p	O
=	O
p	O
-	O
epsilon	O
*	O
gnew	O
/	O
2	O
;	O
wnew	O
=	O
wnew	O
+	O
epsilon	O
*	O
p	O
;	O
#	O
make	O
half-step	O
in	O
p	O
#	O
make	O
step	O
in	O
w	O
gnew	O
=	O
gradm	O
(	O
wnew	O
)	O
;	O
p	O
=	O
p	O
-	O
epsilon	O
*	O
gnew	O
/	O
2	O
;	O
#	O
find	O
new	O
gradient	O
#	O
make	O
half-step	O
in	O
p	O
endfor	O
10	O
5	O
0	O
-5	O
-10	O
-15	O
-20	O
-25	O
langevin	O
-30	O
5	O
0	O
-5	O
-10	O
-15	O
-20	O
-25	O
-30	O
-35	O
-40	O
hmc	O
0	O
0	O
2000	O
4000	O
6000	O
8000	O
10000	O
2000	O
4000	O
6000	O
8000	O
10000	O
the	O
bayesian	O
classi	O
(	O
cid:12	O
)	O
er	O
is	O
better	O
able	O
to	O
identify	O
the	O
points	O
where	O
the	O
classi-	O
(	O
cid:12	O
)	O
cation	O
is	O
uncertain	O
.	O
this	O
pleasing	O
behaviour	O
results	O
simply	O
from	O
a	O
mechanical	O
application	O
of	O
the	O
rules	O
of	O
probability	O
.	O
optimization	B
and	O
typicality	B
a	O
(	O
cid:12	O
)	O
nal	O
observation	O
concerns	O
the	O
behaviour	O
of	O
the	O
functions	O
g	O
(	O
w	O
)	O
and	O
m	O
(	O
w	O
)	O
during	O
the	O
monte	O
carlo	O
sampling	O
process	O
,	O
compared	O
with	O
the	O
values	O
of	O
g	O
and	O
m	O
at	O
the	O
optimum	O
wmp	O
(	O
(	O
cid:12	O
)	O
gure	O
41.5	O
)	O
.	O
the	O
function	B
g	O
(	O
w	O
)	O
(	O
cid:13	O
)	O
uctuates	O
around	O
the	O
value	O
of	O
g	O
(	O
wmp	O
)	O
,	O
though	O
not	O
in	O
a	O
symmetrical	O
way	O
.	O
the	O
function	B
m	O
(	O
w	O
)	O
also	O
(	O
cid:13	O
)	O
uctuates	O
,	O
but	O
it	O
does	O
not	O
(	O
cid:13	O
)	O
uctuate	O
around	O
m	O
(	O
wmp	O
)	O
{	O
obviously	O
it	O
can	O
not	O
,	O
because	O
m	O
is	O
minimized	O
at	O
wmp	O
,	O
so	O
m	O
could	O
not	O
go	O
any	O
smaller	O
{	O
furthermore	O
,	O
m	O
only	O
rarely	O
drops	O
close	O
to	O
m	O
(	O
wmp	O
)	O
.	O
in	O
the	O
language	O
of	O
information	B
theory	I
,	O
the	O
typical	B
set	I
of	O
w	O
has	O
di	O
(	O
cid:11	O
)	O
erent	O
properties	O
from	O
the	O
most	O
probable	O
state	O
w	O
mp	O
.	O
a	O
general	O
message	O
therefore	O
emerges	O
{	O
applicable	O
to	O
all	O
data	O
models	O
,	O
not	O
just	O
neural	O
networks	O
:	O
one	O
should	O
be	O
cautious	O
about	O
making	O
use	O
of	O
optimized	O
parameters	B
,	O
as	O
the	O
properties	O
of	O
optimized	O
parameters	B
may	O
be	O
unrepresen-	O
tative	O
of	O
the	O
properties	O
of	O
typical	O
,	O
plausible	O
parameters	B
;	O
and	O
the	O
predictions	O
obtained	O
using	O
optimized	O
parameters	B
alone	O
will	O
often	O
be	O
unreasonably	O
over-	O
con	O
(	O
cid:12	O
)	O
dent	O
.	O
reducing	O
random	B
walk	I
behaviour	O
using	O
hamiltonian	O
monte	O
carlo	O
as	O
a	O
(	O
cid:12	O
)	O
nal	O
study	O
of	O
monte	O
carlo	O
methods	B
,	O
we	O
now	O
compare	O
the	O
langevin	O
monte	O
carlo	O
method	B
with	O
its	O
big	O
brother	O
,	O
the	O
hamiltonian	O
monte	O
carlo	O
method	B
.	O
the	O
change	O
to	O
hamiltonian	O
monte	O
carlo	O
is	O
simple	O
to	O
implement	O
,	O
as	O
shown	O
in	O
algo-	O
rithm	O
41.8.	O
each	O
single	O
proposal	O
makes	O
use	O
of	O
multiple	O
gradient	O
evaluations	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
41.5	O
:	O
implementing	O
inference	B
with	O
gaussian	O
approximations	O
501	O
along	O
a	O
dynamical	O
trajectory	O
in	O
w	O
;	O
p	O
space	O
,	O
where	O
p	O
are	O
the	O
extra	O
‘	O
momentum	B
’	O
variables	O
of	O
the	O
langevin	O
and	O
hamiltonian	O
monte	O
carlo	O
methods	B
.	O
the	O
num-	O
ber	O
of	O
steps	O
‘	O
tau	O
’	O
was	O
set	B
at	O
random	B
to	O
a	O
number	O
between	O
100	O
and	O
200	O
for	O
each	O
trajectory	O
.	O
the	O
step	O
size	O
(	O
cid:15	O
)	O
was	O
kept	O
(	O
cid:12	O
)	O
xed	O
so	O
as	O
to	O
retain	O
comparability	O
with	O
the	O
simulations	O
that	O
have	O
gone	O
before	O
;	O
it	O
is	O
recommended	O
that	O
one	O
randomize	O
the	O
step	O
size	O
in	O
practical	O
applications	O
,	O
however	O
.	O
figure	O
41.9	O
compares	O
the	O
sampling	O
properties	O
of	O
the	O
langevin	O
and	O
hamil-	O
tonian	O
monte	O
carlo	O
methods	B
.	O
the	O
autocorrelation	O
of	O
the	O
state	O
of	O
the	O
hamil-	O
tonian	O
monte	O
carlo	O
simulation	O
falls	O
much	O
more	O
rapidly	O
with	O
simulation	O
time	O
than	O
that	O
of	O
the	O
langevin	O
method	B
.	O
for	O
this	O
toy	O
problem	O
,	O
hamiltonian	O
monte	O
carlo	O
is	O
at	O
least	O
ten	O
times	O
more	O
e	O
(	O
cid:14	O
)	O
cient	O
in	O
its	O
use	O
of	O
computer	O
time	O
.	O
41.5	O
implementing	O
inference	B
with	O
gaussian	O
approximations	O
physicists	O
love	O
to	O
take	O
nonlinearities	O
and	O
locally	O
linearize	O
them	O
,	O
and	O
they	O
love	O
to	O
approximate	O
probability	B
distributions	I
by	O
gaussians	O
.	O
such	O
approximations	O
o	O
(	O
cid:11	O
)	O
er	O
an	O
alternative	O
strategy	O
for	O
dealing	O
with	O
the	O
integral	B
p	O
(	O
t	O
(	O
n+1	O
)	O
=	O
1j	O
x	O
(	O
n+1	O
)	O
;	O
d	O
;	O
(	O
cid:11	O
)	O
)	O
=z	O
dkw	O
y	O
(	O
x	O
(	O
n+1	O
)	O
;	O
w	O
)	O
1	O
zm	O
which	O
we	O
just	O
evaluated	O
using	O
monte	O
carlo	O
methods	B
.	O
exp	O
(	O
(	O
cid:0	O
)	O
m	O
(	O
w	O
)	O
)	O
;	O
(	O
41.21	O
)	O
we	O
start	O
by	O
making	O
a	O
gaussian	O
approximation	B
to	O
the	O
posterior	B
probability	I
.	O
we	O
go	O
to	O
the	O
minimum	O
of	O
m	O
(	O
w	O
)	O
(	O
using	O
a	O
gradient-based	O
optimizer	O
)	O
and	O
taylor-	O
expand	O
m	O
there	O
:	O
m	O
(	O
w	O
)	O
’	O
m	O
(	O
wmp	O
)	O
+	O
1	O
2	O
(	O
w	O
(	O
cid:0	O
)	O
wmp	O
)	O
ta	O
(	O
w	O
(	O
cid:0	O
)	O
wmp	O
)	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
;	O
(	O
41.22	O
)	O
where	O
a	O
is	O
the	O
matrix	B
of	O
second	O
derivatives	O
,	O
also	O
known	O
as	O
the	O
hessian	O
,	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
aij	O
(	O
cid:17	O
)	O
@	O
wi	O
@	O
wj	O
@	O
2	O
:	O
(	O
41.23	O
)	O
m	O
(	O
w	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
w=wmp	O
(	O
w	O
(	O
cid:0	O
)	O
wmp	O
)	O
ta	O
(	O
w	O
(	O
cid:0	O
)	O
wmp	O
)	O
(	O
cid:21	O
)	O
:	O
(	O
41.24	O
)	O
1	O
2	O
we	O
thus	O
de	O
(	O
cid:12	O
)	O
ne	O
our	O
gaussian	O
approximation	B
:	O
q	O
(	O
w	O
;	O
wmp	O
;	O
a	O
)	O
=	O
[	O
det	O
(	O
a=2	O
(	O
cid:25	O
)	O
)	O
]	O
1=2	O
exp	O
(	O
cid:20	O
)	O
(	O
cid:0	O
)	O
we	O
can	O
think	O
of	O
the	O
matrix	O
a	O
as	O
de	O
(	O
cid:12	O
)	O
ning	O
error	B
bars	I
on	O
w.	O
to	O
be	O
precise	O
,	O
q	O
is	O
a	O
normal	B
distribution	O
whose	O
variance	B
{	O
covariance	B
matrix	I
is	O
a	O
(	O
cid:0	O
)	O
1.	O
exercise	O
41.1	O
.	O
[	O
2	O
]	O
show	O
that	O
the	O
second	O
derivative	O
of	O
m	O
(	O
w	O
)	O
with	O
respect	O
to	O
w	O
is	O
given	O
by	O
@	O
2	O
@	O
wi	O
@	O
wj	O
m	O
(	O
w	O
)	O
=	O
n	O
xn=1	O
f0	O
(	O
a	O
(	O
n	O
)	O
)	O
x	O
(	O
n	O
)	O
i	O
x	O
(	O
n	O
)	O
j	O
+	O
(	O
cid:11	O
)	O
(	O
cid:14	O
)	O
ij	O
;	O
(	O
41.25	O
)	O
where	O
f0	O
(	O
a	O
)	O
is	O
the	O
(	O
cid:12	O
)	O
rst	O
derivative	O
of	O
f	O
(	O
a	O
)	O
(	O
cid:17	O
)	O
1=	O
(	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
a	O
)	O
,	O
which	O
is	O
f0	O
(	O
a	O
)	O
=	O
and	O
d	O
da	O
f	O
(	O
a	O
)	O
=	O
f	O
(	O
a	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
(	O
a	O
)	O
)	O
;	O
a	O
(	O
n	O
)	O
=xj	O
wjx	O
(	O
n	O
)	O
:	O
j	O
(	O
41.26	O
)	O
(	O
41.27	O
)	O
having	O
computed	O
the	O
hessian	O
,	O
our	O
task	O
is	O
then	O
to	O
perform	O
the	O
integral	B
(	O
41.21	O
)	O
using	O
our	O
gaussian	O
approximation	B
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
41	O
|	O
learning	B
as	I
inference	I
figure	O
41.10.	O
the	O
marginalized	O
probability	B
,	O
and	O
an	O
approximation	B
to	O
it	O
.	O
(	O
a	O
)	O
the	O
function	B
(	O
a	O
;	O
s2	O
)	O
,	O
evaluated	O
numerically	O
.	O
in	O
(	O
b	O
)	O
the	O
functions	B
(	O
a	O
;	O
s2	O
)	O
and	O
(	O
cid:30	O
)	O
(	O
a	O
;	O
s2	O
)	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
the	O
text	O
are	O
shown	O
as	O
a	O
function	B
of	O
a	O
for	O
s2	O
=	O
4.	O
from	O
mackay	O
(	O
1992b	O
)	O
.	O
figure	O
41.11.	O
the	O
gaussian	O
approximation	B
in	O
weight	B
space	I
and	O
its	O
approximate	O
predictions	O
in	O
input	O
space	O
.	O
(	O
a	O
)	O
a	O
projection	O
of	O
the	O
gaussian	O
approximation	B
onto	O
the	O
(	O
w1	O
;	O
w2	O
)	O
plane	O
of	O
weight	O
space	O
.	O
the	O
one-	O
and	O
two-standard-deviation	O
contours	O
are	O
shown	O
.	O
also	O
shown	O
are	O
the	O
trajectory	O
of	O
the	O
optimizer	O
,	O
and	O
the	O
monte	O
carlo	O
method	B
’	O
s	O
samples	O
.	O
(	O
b	O
)	O
the	O
predictive	O
function	O
obtained	O
from	O
the	O
gaussian	O
approximation	B
and	O
equation	O
(	O
41.30	O
)	O
.	O
(	O
cf	O
.	O
(	O
cid:12	O
)	O
gure	O
41.2	O
.	O
)	O
502	O
(	O
a	O
;	O
s2	O
)	O
	O
	O
	O
	O
	O
(	O
a	O
)	O
 	O
 	O
5	O
4	O
3	O
2	O
1	O
0	O
-1	O
-2	O
-3	O
(	O
a	O
)	O
(	O
b	O
)	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
 	O
	O
a	O
(	O
b	O
)	O
0	O
0	O
b	O
5	O
5	O
10	O
10	O
0	O
1	O
2	O
3	O
4	O
5	O
6	O
10	O
10	O
5	O
5	O
0	O
0	O
calculating	O
the	O
marginalized	O
probability	B
the	O
output	O
y	O
(	O
x	O
;	O
w	O
)	O
depends	O
on	O
w	O
only	O
through	O
the	O
scalar	O
a	O
(	O
x	O
;	O
w	O
)	O
,	O
so	O
we	O
can	O
reduce	O
the	O
dimensionality	O
of	O
the	O
integral	O
by	O
(	O
cid:12	O
)	O
nding	O
the	O
probability	B
density	O
of	O
a.	O
we	O
are	O
assuming	O
a	O
locally	O
gaussian	O
posterior	B
probability	I
distribution	O
over	O
w	O
=	O
wmp	O
+	O
(	O
cid:1	O
)	O
w	O
,	O
p	O
(	O
w	O
j	O
d	O
;	O
(	O
cid:11	O
)	O
)	O
’	O
(	O
1=zq	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
cid:1	O
)	O
wta	O
(	O
cid:1	O
)	O
w	O
)	O
.	O
for	O
our	O
single	B
neuron	I
,	O
the	O
activation	O
a	O
(	O
x	O
;	O
w	O
)	O
is	O
a	O
linear	B
function	O
of	O
w	O
with	O
@	O
a=	O
@	O
w	O
=	O
x	O
,	O
so	O
for	O
any	O
x	O
,	O
the	O
activation	O
a	O
is	O
gaussian-distributed	O
.	O
.	O
exercise	O
41.2	O
.	O
[	O
2	O
]	O
assuming	O
w	O
is	O
gaussian-distributed	O
with	O
mean	O
wmp	O
and	O
variance	O
{	O
covariance	B
matrix	I
a	O
(	O
cid:0	O
)	O
1	O
,	O
show	O
that	O
the	O
probability	B
distribution	O
of	O
a	O
(	O
x	O
)	O
is	O
p	O
(	O
aj	O
x	O
;	O
d	O
;	O
(	O
cid:11	O
)	O
)	O
=	O
normal	B
(	O
amp	O
;	O
s2	O
)	O
=	O
1	O
p2	O
(	O
cid:25	O
)	O
s2	O
exp	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
(	O
a	O
(	O
cid:0	O
)	O
amp	O
)	O
2	O
2s2	O
(	O
cid:19	O
)	O
;	O
(	O
41.28	O
)	O
where	O
amp	O
=	O
a	O
(	O
x	O
;	O
wmp	O
)	O
and	O
s2	O
=	O
xta	O
(	O
cid:0	O
)	O
1x	O
.	O
this	O
means	O
that	O
the	O
marginalized	O
output	O
is	O
:	O
p	O
(	O
t	O
=	O
1j	O
x	O
;	O
d	O
;	O
(	O
cid:11	O
)	O
)	O
=	O
(	O
amp	O
;	O
s2	O
)	O
(	O
cid:17	O
)	O
z	O
da	O
f	O
(	O
a	O
)	O
normal	B
(	O
amp	O
;	O
s2	O
)	O
:	O
(	O
41.29	O
)	O
this	O
is	O
to	O
be	O
contrasted	O
with	O
y	O
(	O
x	O
;	O
wmp	O
)	O
=	O
f	O
(	O
amp	O
)	O
,	O
the	O
output	O
of	O
the	O
most	O
prob-	O
able	O
network	B
.	O
the	O
integral	B
of	O
a	O
sigmoid	B
times	O
a	O
gaussian	O
can	O
be	O
approximated	O
by	O
:	O
(	O
amp	O
;	O
s2	O
)	O
’	O
(	O
cid:30	O
)	O
(	O
amp	O
;	O
s2	O
)	O
(	O
cid:17	O
)	O
f	O
(	O
(	O
cid:20	O
)	O
(	O
s	O
)	O
amp	O
)	O
with	O
(	O
cid:20	O
)	O
=	O
1=p1	O
+	O
(	O
cid:25	O
)	O
s2=8	O
(	O
(	O
cid:12	O
)	O
gure	O
41.10	O
)	O
.	O
demonstration	O
(	O
41.30	O
)	O
figure	O
41.11	O
shows	O
the	O
result	O
of	O
(	O
cid:12	O
)	O
tting	O
a	O
gaussian	O
approximation	B
at	O
the	O
op-	O
timum	O
wmp	O
,	O
and	O
the	O
results	O
of	O
using	O
that	O
gaussian	O
approximation	B
and	O
equa-	O
 	B
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
	O
 	B
	O
	O
	O
	O
	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
41.5	O
:	O
implementing	O
inference	B
with	O
gaussian	O
approximations	O
503	O
tion	O
(	O
41.30	O
)	O
to	O
make	O
predictions	O
.	O
comparing	O
these	O
predictions	O
with	O
those	O
of	O
the	O
langevin	O
monte	O
carlo	O
method	B
(	O
(	O
cid:12	O
)	O
gure	O
41.7	O
)	O
we	O
observe	O
that	O
,	O
whilst	O
quali-	O
tatively	O
the	O
same	O
,	O
the	O
two	O
are	O
clearly	O
numerically	O
di	O
(	O
cid:11	O
)	O
erent	O
.	O
so	O
at	O
least	O
one	O
of	O
the	O
two	O
methods	B
is	O
not	O
completely	O
accurate	O
.	O
.	O
exercise	O
41.3	O
.	O
[	O
2	O
]	O
is	O
the	O
gaussian	O
approximation	B
to	O
p	O
(	O
w	O
j	O
d	O
;	O
(	O
cid:11	O
)	O
)	O
too	O
heavy-tailed	O
or	O
too	O
light-tailed	O
,	O
or	O
both	O
?	O
it	O
may	O
help	O
to	O
consider	O
p	O
(	O
w	O
j	O
d	O
;	O
(	O
cid:11	O
)	O
)	O
as	O
a	O
function	B
of	O
one	O
parameter	O
wi	O
and	O
to	O
think	O
of	O
the	O
two	O
distributions	O
on	O
a	O
logarithmic	O
scale	O
.	O
discuss	O
the	O
conditions	O
under	O
which	O
the	O
gaussian	O
approximation	B
is	O
most	O
accurate	O
.	O
why	O
marginalize	O
?	O
if	O
the	O
output	O
is	O
immediately	O
used	O
to	O
make	O
a	O
(	O
0/1	O
)	O
decision	O
and	O
the	O
costs	O
asso-	O
ciated	O
with	O
error	O
are	O
symmetrical	O
,	O
then	O
the	O
use	O
of	O
marginalized	O
outputs	O
under	O
this	O
gaussian	O
approximation	B
will	O
make	O
no	O
di	O
(	O
cid:11	O
)	O
erence	O
to	O
the	O
performance	O
of	O
the	O
classi	O
(	O
cid:12	O
)	O
er	O
,	O
compared	O
with	O
using	O
the	O
outputs	O
given	O
by	O
the	O
most	O
probable	O
param-	O
eters	O
,	O
since	O
both	O
functions	B
pass	O
through	O
0:5	O
at	O
amp	O
=	O
0.	O
but	O
these	O
bayesian	O
outputs	O
will	O
make	O
a	O
di	O
(	O
cid:11	O
)	O
erence	O
if	O
,	O
for	O
example	O
,	O
there	O
is	O
an	O
option	O
of	O
saying	O
‘	O
i	O
don	O
’	O
t	O
know	O
’	O
,	O
in	O
addition	O
to	O
saying	O
‘	O
i	O
guess	O
0	O
’	O
and	O
‘	O
i	O
guess	O
1	O
’	O
.	O
and	O
even	O
if	O
there	O
are	O
just	O
the	O
two	O
choices	O
‘	O
0	O
’	O
and	O
‘	O
1	O
’	O
,	O
if	O
the	O
costs	O
associated	O
with	O
error	O
are	O
unequal	O
,	O
then	O
the	O
decision	O
boundary	O
will	O
be	O
some	O
contour	O
other	O
than	O
the	O
0.5	O
contour	O
,	O
and	O
the	O
boundary	O
will	O
be	O
a	O
(	O
cid:11	O
)	O
ected	O
by	O
marginalization	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
postscript	O
on	O
supervised	O
neural	O
networks	O
one	O
of	O
my	O
students	O
,	O
robert	O
,	O
asked	O
:	O
maybe	O
i	O
’	O
m	O
missing	O
something	O
fundamental	O
,	O
but	O
supervised	O
neural	O
networks	O
seem	O
equivalent	O
to	O
(	O
cid:12	O
)	O
tting	O
a	O
pre-de	O
(	O
cid:12	O
)	O
ned	O
function	B
to	O
some	O
given	O
data	O
,	O
then	O
extrapolating	O
{	O
what	O
’	O
s	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
?	O
i	O
agree	O
with	O
robert	O
.	O
the	O
supervised	O
neural	O
networks	O
we	O
have	O
studied	O
so	O
far	O
are	O
simply	O
parameterized	O
nonlinear	B
functions	O
which	O
can	O
be	O
(	O
cid:12	O
)	O
tted	O
to	O
data	O
.	O
hopefully	O
you	O
will	O
agree	O
with	O
another	O
comment	O
that	O
robert	O
made	O
:	O
unsupervised	O
networks	O
seem	O
much	O
more	O
interesting	O
than	O
their	O
su-	O
pervised	O
counterparts	O
.	O
i	O
’	O
m	O
amazed	O
that	O
it	O
works	O
!	O
504	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
42	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
we	O
have	O
now	O
spent	O
three	O
chapters	O
studying	O
the	O
single	B
neuron	I
.	O
the	O
time	O
has	O
come	O
to	O
connect	O
multiple	O
neurons	O
together	O
,	O
making	O
the	O
output	O
of	O
one	O
neuron	B
be	O
the	O
input	O
to	O
another	O
,	O
so	O
as	O
to	O
make	O
neural	O
networks	O
.	O
neural	O
networks	O
can	O
be	O
divided	O
into	O
two	O
classes	O
on	O
the	O
basis	O
of	O
their	O
con-	O
nectivity	O
.	O
figure	O
42.1	O
.	O
(	O
a	O
)	O
a	O
feedforward	O
network	B
.	O
(	O
b	O
)	O
a	O
feedback	B
network	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
feedforward	O
networks	O
.	O
in	O
a	O
feedforward	O
network	B
,	O
all	O
the	O
connections	O
are	O
directed	O
such	O
that	O
the	O
network	B
forms	O
a	O
directed	O
acyclic	O
graph	B
.	O
feedback	B
networks	O
.	O
any	O
network	B
that	O
is	O
not	O
a	O
feedforward	O
network	B
will	O
be	O
called	O
a	O
feedback	B
network	O
.	O
in	O
this	O
chapter	O
we	O
will	O
discuss	O
a	O
fully	O
connected	O
feedback	B
network	O
called	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
.	O
the	O
weights	O
in	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
are	O
constrained	B
to	O
be	O
symmetric	B
,	O
i.e.	O
,	O
the	O
weight	B
from	O
neuron	B
i	O
to	O
neuron	B
j	O
is	O
equal	O
to	O
the	O
weight	B
from	O
neuron	B
j	O
to	O
neuron	B
i.	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
have	O
two	O
applications	O
.	O
first	O
,	O
they	O
can	O
act	O
as	O
associative	O
memories	O
.	O
second	O
,	O
they	O
can	O
be	O
used	O
to	O
solve	O
optimization	B
problems	O
.	O
we	O
will	O
(	O
cid:12	O
)	O
rst	O
discuss	O
the	O
idea	O
of	O
associative	O
memory	B
,	O
also	O
known	O
as	O
content-addressable	O
memory	B
.	O
42.1	O
hebbian	O
learning	B
in	O
chapter	O
38	O
,	O
we	O
discussed	O
the	O
contrast	O
between	O
traditional	O
digital	O
memories	O
and	O
biological	O
memories	O
.	O
perhaps	O
the	O
most	O
striking	O
di	O
(	O
cid:11	O
)	O
erence	O
is	O
the	O
associative	B
nature	O
of	O
biological	O
memory	B
.	O
a	O
simple	O
model	O
due	O
to	O
donald	O
hebb	O
(	O
1949	O
)	O
captures	O
the	O
idea	O
of	O
associa-	O
tive	O
memory	B
.	O
imagine	O
that	O
the	O
weights	O
between	O
neurons	O
whose	O
activities	O
are	O
positively	O
correlated	O
are	O
increased	O
:	O
dwij	O
dt	O
(	O
cid:24	O
)	O
correlation	O
(	O
xi	O
;	O
xj	O
)	O
:	O
(	O
42.1	O
)	O
now	O
imagine	O
that	O
when	O
stimulus	O
m	O
is	O
present	O
(	O
for	O
example	O
,	O
the	O
smell	O
of	O
a	O
banana	O
)	O
,	O
the	O
activity	O
of	O
neuron	B
m	O
increases	O
;	O
and	O
that	O
neuron	B
n	O
is	O
associated	O
505	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
506	O
42	O
|	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
with	O
another	O
stimulus	O
,	O
n	O
(	O
for	O
example	O
,	O
the	O
sight	O
of	O
a	O
yellow	O
object	O
)	O
.	O
if	O
these	O
two	O
stimuli	O
{	O
a	O
yellow	O
sight	O
and	O
a	O
banana	O
smell	O
{	O
co-occur	O
in	O
the	O
environment	O
,	O
then	O
the	O
hebbian	O
learning	B
rule	I
(	O
42.1	O
)	O
will	O
increase	O
the	O
weights	O
wnm	O
and	O
wmn	O
.	O
this	O
means	O
that	O
when	O
,	O
on	O
a	O
later	O
occasion	O
,	O
stimulus	O
n	O
occurs	O
in	O
isolation	O
,	O
mak-	O
ing	O
the	O
activity	O
xn	O
large	O
,	O
the	O
positive	O
weight	O
from	O
n	O
to	O
m	O
will	O
cause	O
neuron	B
m	O
also	O
to	O
be	O
activated	O
.	O
thus	O
the	O
response	O
to	O
the	O
sight	O
of	O
a	O
yellow	O
object	O
is	O
an	O
automatic	O
association	O
with	O
the	O
smell	O
of	O
a	O
banana	O
.	O
we	O
could	O
call	O
this	O
‘	O
pattern	O
completion	O
’	O
.	O
no	O
teacher	O
is	O
required	O
for	O
this	O
associative	B
memory	I
to	O
work	O
.	O
no	O
signal	O
is	O
needed	O
to	O
indicate	O
that	O
a	O
correlation	O
has	O
been	O
detected	O
or	O
that	O
an	O
as-	O
sociation	O
should	O
be	O
made	O
.	O
the	O
unsupervised	O
,	O
local	O
learning	B
algorithm	O
and	O
the	O
unsupervised	O
,	O
local	O
activity	B
rule	I
spontaneously	O
produce	O
associative	B
memory	I
.	O
this	O
idea	O
seems	O
so	O
simple	O
and	O
so	O
e	O
(	O
cid:11	O
)	O
ective	O
that	O
it	O
must	O
be	O
relevant	O
to	O
how	O
memories	O
work	O
in	O
the	O
brain	B
.	O
42.2	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
the	O
binary	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
convention	O
for	O
weights	O
.	O
our	O
convention	O
in	O
general	O
will	O
be	O
that	O
wij	O
denotes	O
the	O
connection	O
from	O
neuron	B
j	O
to	O
neuron	B
i.	O
architecture	B
.	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
consists	O
of	O
i	O
neurons	O
.	O
they	O
are	O
fully	O
connected	O
through	O
symmetric	B
,	O
bidirectional	O
connections	O
with	O
weights	O
wij	O
=	O
wji	O
.	O
there	O
are	O
no	O
self-connections	O
,	O
so	O
wii	O
=	O
0	O
for	O
all	O
i.	O
biases	O
wi0	O
may	O
be	O
included	O
(	O
these	O
may	O
be	O
viewed	O
as	O
weights	O
from	O
a	O
neuron	B
‘	O
0	O
’	O
whose	O
activity	O
is	O
permanently	O
x0	O
=	O
1	O
)	O
.	O
we	O
will	O
denote	O
the	O
activity	O
of	O
neuron	B
i	O
(	O
its	O
output	O
)	O
by	O
xi	O
.	O
activity	B
rule	I
.	O
roughly	O
,	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
’	O
s	O
activity	B
rule	I
is	O
for	O
each	O
neu-	O
ron	O
to	O
update	O
its	O
state	O
as	O
if	O
it	O
were	O
a	O
single	B
neuron	I
with	O
the	O
threshold	B
activation	O
function	B
x	O
(	O
a	O
)	O
=	O
(	O
cid:2	O
)	O
(	O
a	O
)	O
(	O
cid:17	O
)	O
(	O
cid:26	O
)	O
1	O
a	O
(	O
cid:21	O
)	O
0	O
(	O
cid:0	O
)	O
1	O
a	O
<	O
0	O
:	O
(	O
42.2	O
)	O
since	O
there	O
is	O
feedback	B
in	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
(	O
every	O
neuron	B
’	O
s	O
output	O
is	O
an	O
input	O
to	O
all	O
the	O
other	O
neurons	O
)	O
we	O
will	O
have	O
to	O
specify	O
an	O
order	O
for	O
the	O
updates	O
to	O
occur	O
.	O
the	O
updates	O
may	O
be	O
synchronous	O
or	O
asynchronous	O
.	O
synchronous	O
updates	O
{	O
all	O
neurons	O
compute	O
their	O
activations	O
ai	O
=xj	O
wijxj	O
(	O
42.3	O
)	O
then	O
update	O
their	O
states	O
simultaneously	O
to	O
xi	O
=	O
(	O
cid:2	O
)	O
(	O
ai	O
)	O
:	O
(	O
42.4	O
)	O
asynchronous	O
updates	O
{	O
one	O
neuron	B
at	O
a	O
time	O
computes	O
its	O
activa-	O
tion	O
and	O
updates	O
its	O
state	O
.	O
the	O
sequence	B
of	O
selected	O
neurons	O
may	O
be	O
a	O
(	O
cid:12	O
)	O
xed	O
sequence	B
or	O
a	O
random	B
sequence	O
.	O
the	O
properties	O
of	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
may	O
be	O
sensitive	O
to	O
the	O
above	O
choices	O
.	O
learning	B
rule	I
.	O
the	O
learning	B
rule	I
is	O
intended	O
to	O
make	O
a	O
set	B
of	O
desired	O
memo-	O
ries	O
fx	O
(	O
n	O
)	O
g	O
be	O
stable	O
states	O
of	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
’	O
s	O
activity	B
rule	I
.	O
each	O
memory	B
is	O
a	O
binary	O
pattern	O
,	O
with	O
xi	O
2	O
f	O
(	O
cid:0	O
)	O
1	O
;	O
1g	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
42.3	O
:	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
the	O
continuous	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
507	O
(	O
a	O
)	O
moscow	O
--	O
--	O
--	O
russia	O
lima	O
--	O
--	O
--	O
--	O
--	O
peru	O
london	O
--	O
--	O
-england	O
tokyo	O
--	O
--	O
--	O
--	O
japan	O
edinburgh-scotland	O
ottawa	O
--	O
--	O
--	O
canada	O
oslo	O
--	O
--	O
--	O
--	O
norway	O
stockholm	O
--	O
-sweden	O
paris	O
--	O
--	O
--	O
-france	O
(	O
b	O
)	O
moscow	O
--	O
-	O
:	O
:	O
:	O
:	O
:	O
:	O
:	O
:	O
:	O
=	O
)	O
moscow	O
--	O
--	O
--	O
russia	O
:	O
:	O
:	O
:	O
:	O
:	O
:	O
:	O
:	O
:	O
--	O
canada	O
=	O
)	O
ottawa	O
--	O
--	O
--	O
canada	O
(	O
c	O
)	O
otowa	O
--	O
--	O
--	O
-canada	O
=	O
)	O
ottawa	O
--	O
--	O
--	O
canada	O
egindurrh-sxotland	O
=	O
)	O
edinburgh-scotland	O
the	O
weights	O
are	O
set	B
using	O
the	O
sum	O
of	O
outer	O
products	O
or	O
hebb	O
rule	O
,	O
wij	O
=	O
(	O
cid:17	O
)	O
xn	O
x	O
(	O
n	O
)	O
i	O
x	O
(	O
n	O
)	O
j	O
;	O
(	O
42.5	O
)	O
where	O
(	O
cid:17	O
)	O
is	O
an	O
unimportant	O
constant	O
.	O
to	O
prevent	O
the	O
largest	O
possible	O
weight	B
from	O
growing	O
with	O
n	O
we	O
might	O
choose	O
to	O
set	B
(	O
cid:17	O
)	O
=	O
1=n	O
.	O
exercise	O
42.1	O
.	O
[	O
1	O
]	O
explain	O
why	O
the	O
value	O
of	O
(	O
cid:17	O
)	O
is	O
not	O
important	O
for	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
de	O
(	O
cid:12	O
)	O
ned	O
above	O
.	O
42.3	O
de	O
(	O
cid:12	O
)	O
nition	O
of	O
the	O
continuous	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
figure	O
42.2.	O
associative	B
memory	I
(	O
schematic	O
)	O
.	O
(	O
a	O
)	O
a	O
list	O
of	O
desired	O
memories	O
.	O
(	O
b	O
)	O
the	O
(	O
cid:12	O
)	O
rst	O
purpose	O
of	O
an	O
associative	B
memory	I
is	O
pattern	O
completion	O
,	O
given	O
a	O
partial	B
pattern	O
.	O
(	O
c	O
)	O
the	O
second	O
purpose	O
of	O
a	O
memory	B
is	O
error	B
correction	I
.	O
ai	O
=xj	O
wijxj	O
(	O
42.6	O
)	O
(	O
42.7	O
)	O
using	O
the	O
identical	O
architecture	O
and	B
learning	I
rule	O
we	O
can	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
whose	O
activities	O
are	O
real	O
numbers	O
between	O
(	O
cid:0	O
)	O
1	O
and	O
1.	O
activity	B
rule	I
.	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
’	O
s	O
activity	B
rule	I
is	O
for	O
each	O
neuron	B
to	O
up-	O
date	O
its	O
state	O
as	O
if	O
it	O
were	O
a	O
single	B
neuron	I
with	O
a	O
sigmoid	B
activation	O
function	B
.	O
the	O
updates	O
may	O
be	O
synchronous	O
or	O
asynchronous	O
,	O
and	O
in-	O
volve	O
the	O
equations	O
and	O
xi	O
=	O
tanh	O
(	O
ai	O
)	O
:	O
the	O
learning	B
rule	I
is	O
the	O
same	O
as	O
in	O
the	O
binary	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
,	O
but	O
the	O
value	O
of	O
(	O
cid:17	O
)	O
becomes	O
relevant	O
.	O
alternatively	O
,	O
we	O
may	O
(	O
cid:12	O
)	O
x	O
(	O
cid:17	O
)	O
and	O
introduce	O
a	O
gain	B
(	O
cid:12	O
)	O
2	O
(	O
0	O
;	O
1	O
)	O
into	O
the	O
activation	B
function	I
:	O
xi	O
=	O
tanh	O
(	O
(	O
cid:12	O
)	O
ai	O
)	O
:	O
(	O
42.8	O
)	O
exercise	O
42.2	O
.	O
[	O
1	O
]	O
where	O
have	O
we	O
encountered	O
equations	O
42.6	O
,	O
42.7	O
,	O
and	O
42.8	O
before	O
?	O
42.4	O
convergence	O
of	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
the	O
hope	O
is	O
that	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
we	O
have	O
de	O
(	O
cid:12	O
)	O
ned	O
will	O
perform	O
associa-	O
tive	O
memory	B
recall	O
,	O
as	O
shown	O
schematically	O
in	O
(	O
cid:12	O
)	O
gure	O
42.2.	O
we	O
hope	O
that	O
the	O
activity	B
rule	I
of	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
will	O
take	O
a	O
partial	B
memory	O
or	O
a	O
corrupted	O
memory	B
,	O
and	O
perform	O
pattern	O
completion	O
or	O
error	B
correction	I
to	O
restore	O
the	O
original	O
memory	B
.	O
but	O
why	O
should	O
we	O
expect	O
any	O
pattern	O
to	O
be	O
stable	O
under	O
the	O
activity	B
rule	I
,	O
let	O
alone	O
the	O
desired	O
memories	O
?	O
we	O
address	B
the	O
continuous	B
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
,	O
since	O
the	O
binary	O
network	O
is	O
a	O
special	O
case	O
of	O
it	O
.	O
we	O
have	O
already	O
encountered	O
the	O
activity	B
rule	I
(	O
42.6	O
,	O
42.8	O
)	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
508	O
42	O
|	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
when	O
we	O
discussed	O
variational	B
methods	I
(	O
section	B
33.2	O
)	O
:	O
when	O
we	O
approximated	O
the	O
spin	B
system	I
whose	O
energy	B
function	O
was	O
(	O
42.9	O
)	O
(	O
42.10	O
)	O
with	O
a	O
separable	O
distribution	B
e	O
(	O
x	O
;	O
j	O
)	O
=	O
(	O
cid:0	O
)	O
hnxn	O
1	O
zq	O
1	O
2xm	O
;	O
n	O
q	O
(	O
x	O
;	O
a	O
)	O
=	O
jmnxmxn	O
(	O
cid:0	O
)	O
xn	O
anxn	O
!	O
exp	O
xn	O
q	O
(	O
x	O
;	O
a	O
)	O
e	O
(	O
x	O
;	O
j	O
)	O
(	O
cid:0	O
)	O
xx	O
am	O
=	O
(	O
cid:12	O
)	O
xn	O
jmn	O
(	O
cid:22	O
)	O
xn	O
+	O
hm	O
!	O
(	O
cid:12	O
)	O
~f	O
(	O
a	O
)	O
=	O
(	O
cid:12	O
)	O
xx	O
we	O
found	O
that	O
the	O
pair	O
of	O
iterative	O
equations	O
and	O
optimized	O
the	O
latter	O
so	O
as	O
to	O
minimize	O
the	O
variational	B
free	I
energy	I
q	O
(	O
x	O
;	O
a	O
)	O
ln	O
1	O
q	O
(	O
x	O
;	O
a	O
)	O
;	O
(	O
42.11	O
)	O
and	O
were	O
guaranteed	O
to	O
decrease	O
the	O
variational	B
free	I
energy	I
(	O
cid:22	O
)	O
xn	O
=	O
tanh	O
(	O
an	O
)	O
(	O
cid:12	O
)	O
~f	O
(	O
a	O
)	O
=	O
(	O
cid:12	O
)	O
(	O
cid:0	O
)	O
1	O
2xm	O
;	O
n	O
jmn	O
(	O
cid:22	O
)	O
xm	O
(	O
cid:22	O
)	O
xn	O
(	O
cid:0	O
)	O
xn	O
hn	O
(	O
cid:22	O
)	O
xn	O
!	O
(	O
cid:0	O
)	O
xn	O
(	O
42.12	O
)	O
(	O
42.13	O
)	O
h	O
(	O
e	O
)	O
2	O
(	O
qn	O
)	O
:	O
(	O
42.14	O
)	O
if	O
we	O
simply	O
replace	O
j	O
by	O
w	O
,	O
(	O
cid:22	O
)	O
x	O
by	O
x	O
,	O
and	O
hn	O
by	O
wi0	O
,	O
we	O
see	O
that	O
the	O
equations	O
of	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
are	O
identical	O
to	O
a	O
set	B
of	O
mean-	O
(	O
cid:12	O
)	O
eld	O
equations	O
that	O
minimize	O
(	O
cid:12	O
)	O
~f	O
(	O
x	O
)	O
=	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
1	O
2	O
xtwx	O
(	O
cid:0	O
)	O
xi	O
h	O
(	O
e	O
)	O
2	O
[	O
(	O
1	O
+	O
xi	O
)	O
=2	O
]	O
:	O
(	O
42.15	O
)	O
there	O
is	O
a	O
general	O
name	O
for	O
a	O
function	B
that	O
decreases	O
under	O
the	O
dynamical	O
evolution	B
of	O
a	O
system	O
and	O
that	O
is	O
bounded	O
below	O
:	O
such	O
a	O
function	B
is	O
a	O
lyapunov	O
function	B
for	O
the	O
system	O
.	O
it	O
is	O
useful	O
to	O
be	O
able	O
to	O
prove	O
the	O
existence	O
of	O
lyapunov	O
functions	B
:	O
if	O
a	O
system	O
has	O
a	O
lyapunov	O
function	B
then	O
its	O
dynamics	O
are	O
bound	B
to	O
settle	O
down	O
to	O
a	O
(	O
cid:12	O
)	O
xed	O
point	O
,	O
which	O
is	O
a	O
local	O
minimum	O
of	O
the	O
lyapunov	O
function	B
,	O
or	O
a	O
limit	B
cycle	I
,	O
along	O
which	O
the	O
lyapunov	O
function	B
is	O
a	O
constant	O
.	O
chaotic	O
behaviour	O
is	O
not	O
possible	O
for	O
a	O
system	O
with	O
a	O
lyapunov	O
function	B
.	O
if	O
a	O
system	O
has	O
a	O
lyapunov	O
function	B
then	O
its	O
state	O
space	O
can	O
be	O
divided	O
into	O
basins	O
of	O
attraction	O
,	O
one	O
basin	O
associated	O
with	O
each	O
attractor	O
.	O
so	O
,	O
the	O
continuous	B
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
’	O
s	O
activity	O
rules	O
(	O
if	O
implemented	O
asyn-	O
chronously	O
)	O
have	O
a	O
lyapunov	O
function	B
.	O
this	O
lyapunov	O
function	B
is	O
a	O
convex	B
function	O
of	O
each	O
parameter	O
ai	O
so	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
’	O
s	O
dynamics	O
will	O
always	O
converge	O
to	O
a	O
stable	O
(	O
cid:12	O
)	O
xed	O
point	O
.	O
this	O
convergence	O
proof	O
depends	O
crucially	O
on	O
the	O
fact	O
that	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
’	O
s	O
connections	O
are	O
symmetric	B
.	O
it	O
also	O
depends	O
on	O
the	O
updates	O
being	O
made	O
asynchronously	O
.	O
exercise	O
42.3	O
.	O
[	O
2	O
,	O
p.520	O
]	O
show	O
by	O
constructing	O
an	O
example	O
that	O
if	O
a	O
feedback	B
network	O
does	O
not	O
have	O
symmetric	B
connections	O
then	O
its	O
dynamics	O
may	O
fail	O
to	O
converge	O
to	O
a	O
(	O
cid:12	O
)	O
xed	O
point	O
.	O
exercise	O
42.4	O
.	O
[	O
2	O
,	O
p.521	O
]	O
show	O
by	O
constructing	O
an	O
example	O
that	O
if	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
is	O
updated	O
synchronously	O
that	O
,	O
from	O
some	O
initial	O
conditions	O
,	O
it	O
may	O
fail	O
to	O
converge	O
to	O
a	O
(	O
cid:12	O
)	O
xed	O
point	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
42.4	O
:	O
convergence	O
of	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
509	O
(	O
a	O
)	O
.	O
0	O
0	O
0	O
0	O
-2	O
2	O
-2	O
2	O
2	O
-2	O
0	O
0	O
0	O
2	O
0	O
0	O
-2	O
0	O
2	O
2	O
0	O
0	O
-2	O
-2	O
0	O
.	O
4	O
4	O
0	O
-2	O
-2	O
-2	O
-2	O
-2	O
-2	O
0	O
-4	O
0	O
-2	O
0	O
0	O
-2	O
0	O
-2	O
-2	O
4	O
4	O
2	O
-2	O
0	O
4	O
.	O
4	O
0	O
-2	O
-2	O
-2	O
-2	O
-2	O
-2	O
0	O
-4	O
0	O
-2	O
0	O
0	O
-2	O
0	O
-2	O
-2	O
4	O
4	O
2	O
-2	O
0	O
4	O
4	O
.	O
0	O
-2	O
-2	O
-2	O
-2	O
-2	O
-2	O
0	O
-4	O
0	O
-2	O
0	O
0	O
-2	O
0	O
-2	O
-2	O
4	O
4	O
2	O
-2	O
0	O
0	O
0	O
0	O
.	O
2	O
-2	O
-2	O
2	O
-2	O
2	O
-4	O
0	O
0	O
-2	O
4	O
-4	O
-2	O
0	O
-2	O
2	O
0	O
0	O
-2	O
2	O
-2	O
-2	O
-2	O
-2	O
2	O
.	O
0	O
0	O
0	O
0	O
4	O
-2	O
2	O
-2	O
0	O
2	O
-2	O
0	O
-2	O
0	O
0	O
-2	O
-2	O
0	O
4	O
2	O
-2	O
-2	O
-2	O
-2	O
0	O
.	O
0	O
0	O
4	O
0	O
2	O
2	O
-2	O
4	O
-2	O
2	O
0	O
-2	O
4	O
0	O
-2	O
-2	O
0	O
0	O
-2	O
-2	O
-2	O
-2	O
-2	O
0	O
0	O
.	O
0	O
0	O
0	O
2	O
2	O
2	O
0	O
-2	O
2	O
4	O
2	O
0	O
0	O
-2	O
-2	O
0	O
0	O
2	O
-2	O
-2	O
-2	O
2	O
0	O
0	O
0	O
.	O
0	O
0	O
-2	O
2	O
2	O
0	O
2	O
-2	O
0	O
2	O
0	O
4	O
-2	O
-2	O
-4	O
0	O
2	O
-2	O
-2	O
-2	O
-2	O
0	O
4	O
0	O
0	O
.	O
0	O
2	O
2	O
-2	O
4	O
-2	O
2	O
0	O
-2	O
4	O
0	O
-2	O
-2	O
0	O
0	O
-2	O
-2	O
-2	O
-2	O
2	O
4	O
0	O
0	O
0	O
0	O
.	O
-2	O
2	O
-2	O
0	O
2	O
-2	O
0	O
-2	O
0	O
0	O
-2	O
-2	O
0	O
4	O
0	O
0	O
0	O
0	O
-4	O
-2	O
2	O
2	O
-2	O
2	O
-2	O
.	O
0	O
0	O
2	O
-4	O
4	O
2	O
0	O
2	O
-2	O
0	O
0	O
2	O
-2	O
0	O
-4	O
-4	O
-4	O
0	O
2	O
2	O
2	O
2	O
2	O
2	O
0	O
.	O
0	O
2	O
0	O
0	O
2	O
0	O
2	O
2	O
-4	O
-4	O
-2	O
2	O
0	O
0	O
0	O
0	O
0	O
-2	O
-2	O
2	O
2	O
-2	O
-2	O
0	O
0	O
.	O
-2	O
0	O
0	O
2	O
4	O
-2	O
2	O
0	O
0	O
-2	O
-2	O
2	O
-2	O
-2	O
-2	O
-2	O
0	O
4	O
0	O
0	O
4	O
0	O
2	O
2	O
-2	O
.	O
-2	O
2	O
0	O
-2	O
4	O
0	O
-2	O
-2	O
0	O
0	O
0	O
0	O
0	O
0	O
4	O
2	O
-2	O
-2	O
2	O
-2	O
2	O
-4	O
0	O
0	O
-2	O
.	O
-4	O
-2	O
0	O
-2	O
2	O
0	O
0	O
-2	O
2	O
0	O
0	O
0	O
0	O
-4	O
-2	O
2	O
2	O
-2	O
2	O
-2	O
4	O
0	O
0	O
2	O
-4	O
.	O
2	O
0	O
2	O
-2	O
0	O
0	O
2	O
-2	O
-2	O
-2	O
-2	O
-2	O
-2	O
0	O
0	O
4	O
0	O
0	O
0	O
2	O
2	O
2	O
0	O
-2	O
2	O
.	O
2	O
0	O
0	O
-2	O
-2	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
-2	O
-2	O
2	O
2	O
-2	O
-2	O
0	O
0	O
4	O
-2	O
0	O
0	O
2	O
.	O
-2	O
2	O
0	O
0	O
-2	O
-2	O
2	O
-2	O
-2	O
-2	O
-2	O
0	O
4	O
0	O
0	O
4	O
0	O
2	O
2	O
-2	O
4	O
-2	O
2	O
0	O
-2	O
.	O
0	O
-2	O
-2	O
0	O
0	O
2	O
-2	O
-2	O
-2	O
2	O
0	O
0	O
0	O
4	O
0	O
0	O
-2	O
2	O
2	O
0	O
2	O
-2	O
0	O
2	O
0	O
.	O
-2	O
-2	O
-4	O
0	O
0	O
4	O
4	O
4	O
0	O
-2	O
-2	O
-2	O
-2	O
-2	O
-2	O
0	O
-4	O
0	O
-2	O
0	O
0	O
-2	O
0	O
-2	O
-2	O
.	O
4	O
2	O
-2	O
0	O
4	O
4	O
4	O
0	O
-2	O
-2	O
-2	O
-2	O
-2	O
-2	O
0	O
-4	O
0	O
-2	O
0	O
0	O
-2	O
0	O
-2	O
-2	O
4	O
.	O
2	O
-2	O
-2	O
2	O
2	O
2	O
-2	O
0	O
0	O
0	O
-4	O
0	O
0	O
2	O
-2	O
-2	O
0	O
-2	O
2	O
0	O
-2	O
0	O
-4	O
2	O
2	O
.	O
0	O
-2	O
-2	O
-2	O
-2	O
2	O
4	O
0	O
0	O
0	O
0	O
4	O
-2	O
2	O
-2	O
0	O
2	O
-2	O
0	O
-2	O
0	O
0	O
-2	O
-2	O
0	O
.	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
(	O
e	O
)	O
(	O
f	O
)	O
(	O
g	O
)	O
(	O
h	O
)	O
(	O
i	O
)	O
(	O
l	O
)	O
!	O
!	O
!	O
(	O
j	O
)	O
(	O
m	O
)	O
!	O
!	O
!	O
(	O
k	O
)	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
figure	O
42.3.	O
binary	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
storing	O
four	O
memories	O
.	O
(	O
a	O
)	O
the	O
four	O
memories	O
,	O
and	O
the	O
weight	B
matrix	O
.	O
(	O
b	O
{	O
h	O
)	O
initial	O
states	O
that	O
di	O
(	O
cid:11	O
)	O
er	O
by	O
one	O
,	O
two	O
,	O
three	O
,	O
four	O
,	O
or	O
even	O
(	O
cid:12	O
)	O
ve	O
bits	O
from	O
a	O
desired	O
memory	B
are	O
restored	O
to	O
that	O
memory	B
in	O
one	O
or	O
two	O
iterations	O
.	O
(	O
i	O
{	O
m	O
)	O
some	O
initial	O
conditions	O
that	O
are	O
far	O
from	O
the	O
memories	O
lead	O
to	O
stable	O
states	O
other	O
than	O
the	O
four	O
memories	O
;	O
in	O
(	O
i	O
)	O
,	O
the	O
stable	O
state	O
looks	O
like	O
a	O
mixture	O
of	O
two	O
memories	O
,	O
‘	O
d	O
’	O
and	O
‘	O
j	O
’	O
;	O
stable	O
state	O
(	O
j	O
)	O
is	O
like	O
a	O
mixture	O
of	O
‘	O
j	O
’	O
and	O
‘	O
c	O
’	O
;	O
in	O
(	O
k	O
)	O
,	O
we	O
(	O
cid:12	O
)	O
nd	O
a	O
corrupted	O
version	O
of	O
the	O
‘	O
m	O
’	O
memory	B
(	O
two	O
bits	O
distant	O
)	O
;	O
in	O
(	O
l	O
)	O
a	O
corrupted	O
version	O
of	O
‘	O
j	O
’	O
(	O
four	O
bits	O
distant	O
)	O
and	O
in	O
(	O
m	O
)	O
,	O
a	O
state	O
which	O
looks	O
spurious	O
until	O
we	O
recognize	O
that	O
it	O
is	O
the	O
inverse	O
of	O
the	O
stable	O
state	O
(	O
l	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
510	O
42	O
|	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
42.5	O
the	O
associative	B
memory	I
in	O
action	O
figure	O
42.3	O
shows	O
the	O
dynamics	O
of	O
a	O
25-unit	O
binary	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
that	O
has	O
learnt	O
four	O
patterns	O
by	O
hebbian	O
learning	B
.	O
the	O
four	O
patterns	O
are	O
displayed	O
as	O
(	O
cid:12	O
)	O
ve	O
by	O
(	O
cid:12	O
)	O
ve	O
binary	B
images	I
in	O
(	O
cid:12	O
)	O
gure	O
42.3a	O
.	O
for	O
twelve	O
initial	O
conditions	O
,	O
panels	O
(	O
b	O
{	O
m	O
)	O
show	O
the	O
state	O
of	O
the	O
network	B
,	O
iteration	O
by	O
iteration	O
,	O
all	O
25	O
units	B
being	O
updated	O
asynchronously	O
in	O
each	O
iteration	O
.	O
for	O
an	O
initial	O
condition	O
randomly	O
perturbed	O
from	O
a	O
memory	B
,	O
it	O
often	O
only	O
takes	O
one	O
iteration	O
for	O
all	O
the	O
errors	B
to	O
be	O
corrected	O
.	O
the	O
network	B
has	O
more	O
stable	O
states	O
in	O
addition	O
to	O
the	O
four	O
desired	O
memories	O
:	O
the	O
inverse	O
of	O
any	O
stable	O
state	O
is	O
also	O
a	O
stable	O
state	O
;	O
and	O
there	O
are	O
several	O
stable	O
states	O
that	O
can	O
be	O
interpreted	O
as	O
mixtures	O
of	O
the	O
memories	O
.	O
brain	B
damage	O
the	O
network	B
can	O
be	O
severely	O
damaged	O
and	O
still	O
work	O
(	O
cid:12	O
)	O
ne	O
as	O
an	O
associative	B
memory	I
.	O
if	O
we	O
take	O
the	O
300	O
weights	O
of	O
the	O
network	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
42.3	O
and	O
randomly	O
set	B
50	O
or	O
100	O
of	O
them	O
to	O
zero	O
,	O
we	O
still	O
(	O
cid:12	O
)	O
nd	O
that	O
the	O
desired	O
memories	O
are	O
attracting	O
stable	O
states	O
.	O
imagine	O
a	O
digital	O
computer	O
that	O
still	O
works	O
(	O
cid:12	O
)	O
ne	O
even	O
when	O
20	O
%	O
of	O
its	O
components	O
are	O
destroyed	O
!	O
.	O
exercise	O
42.5	O
.	O
[	O
3c	O
]	O
implement	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
and	O
con	O
(	O
cid:12	O
)	O
rm	O
this	O
amazing	O
robust	O
error-correcting	O
capability	O
.	O
more	O
memories	O
we	O
can	O
squash	O
more	O
memories	O
into	O
the	O
network	B
too	O
.	O
figure	O
42.4a	O
shows	O
a	O
set	B
of	O
(	O
cid:12	O
)	O
ve	O
memories	O
.	O
when	O
we	O
train	O
the	O
network	B
with	O
hebbian	O
learning	B
,	O
all	O
(	O
cid:12	O
)	O
ve	O
memories	O
are	O
stable	O
states	O
,	O
even	O
when	O
26	O
of	O
the	O
weights	O
are	O
randomly	O
deleted	O
(	O
as	O
shown	O
by	O
the	O
‘	O
x	O
’	O
s	O
in	O
the	O
weight	B
matrix	O
)	O
.	O
however	O
,	O
the	O
basins	O
of	O
attraction	O
are	O
smaller	O
than	O
before	O
:	O
(	O
cid:12	O
)	O
gures	O
42.4	O
(	O
b	O
{	O
f	O
)	O
show	O
the	O
dynamics	O
resulting	O
from	O
randomly	O
chosen	O
starting	O
states	O
close	O
to	O
each	O
of	O
the	O
memories	O
(	O
3	O
bits	O
(	O
cid:13	O
)	O
ipped	O
)	O
.	O
only	O
three	O
of	O
the	O
memories	O
are	O
recovered	O
correctly	O
.	O
if	O
we	O
try	O
to	O
store	O
too	O
many	O
patterns	O
,	O
the	O
associative	B
memory	I
fails	O
catas-	O
trophically	O
.	O
when	O
we	O
add	O
a	O
sixth	O
pattern	O
,	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
42.5	O
,	O
only	O
one	O
of	O
the	O
patterns	O
is	O
stable	O
;	O
the	O
others	O
all	O
(	O
cid:13	O
)	O
ow	O
into	O
one	O
of	O
two	O
spurious	O
stable	O
states	O
.	O
42.6	O
the	O
continuous-time	O
continuous	B
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
the	O
fact	O
that	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
’	O
s	O
properties	O
are	O
not	O
robust	O
to	O
the	O
minor	O
change	O
from	O
asynchronous	O
to	O
synchronous	O
updates	O
might	O
be	O
a	O
cause	O
for	O
con-	O
cern	O
;	O
can	O
this	O
model	B
be	O
a	O
useful	O
model	B
of	O
biological	O
networks	O
?	O
it	O
turns	O
out	O
that	O
once	O
we	O
move	O
to	O
a	O
continuous-time	O
version	O
of	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
,	O
this	O
issue	O
melts	O
away	O
.	O
we	O
assume	O
that	O
each	O
neuron	B
’	O
s	O
activity	O
xi	O
is	O
a	O
continuous	B
function	O
of	O
time	O
xi	O
(	O
t	O
)	O
and	O
that	O
the	O
activations	O
ai	O
(	O
t	O
)	O
are	O
computed	O
instantaneously	O
in	O
accordance	O
with	O
wijxj	O
(	O
t	O
)	O
:	O
(	O
42.16	O
)	O
ai	O
(	O
t	O
)	O
=xj	O
the	O
neuron	B
’	O
s	O
response	O
to	O
its	O
activation	O
is	O
assumed	O
to	O
be	O
mediated	O
by	O
the	O
di	O
(	O
cid:11	O
)	O
erential	O
equation	O
:	O
d	O
dt	O
xi	O
(	O
t	O
)	O
=	O
(	O
cid:0	O
)	O
1	O
(	O
cid:28	O
)	O
(	O
xi	O
(	O
t	O
)	O
(	O
cid:0	O
)	O
f	O
(	O
ai	O
)	O
)	O
;	O
(	O
42.17	O
)	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
42.6	O
:	O
the	O
continuous-time	O
continuous	B
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
511	O
(	O
a	O
)	O
.	O
-1	O
1	O
-1	O
1	O
x	O
x	O
-3	O
3	O
x	O
x	O
-1	O
1	O
-1	O
x	O
-1	O
1	O
-3	O
x	O
1	O
3	O
-1	O
1	O
x	O
-1	O
-1	O
.	O
3	O
5	O
-1	O
-1	O
-3	O
-1	O
-3	O
-1	O
-3	O
1	O
x	O
1	O
-3	O
1	O
-1	O
-1	O
-1	O
-1	O
-3	O
5	O
3	O
3	O
-3	O
1	O
3	O
.	O
3	O
1	O
-3	O
-1	O
x	O
-1	O
-3	O
-1	O
-1	O
x	O
-1	O
-1	O
-1	O
1	O
-3	O
1	O
-3	O
-1	O
3	O
5	O
1	O
-1	O
-1	O
5	O
3	O
.	O
-1	O
-1	O
-3	O
-1	O
-3	O
-1	O
-3	O
1	O
-5	O
1	O
-3	O
1	O
-1	O
-1	O
-1	O
-1	O
-3	O
5	O
x	O
3	O
-3	O
1	O
-1	O
1	O
-1	O
.	O
1	O
-1	O
-3	O
x	O
x	O
3	O
-5	O
1	O
-1	O
-1	O
3	O
x	O
-3	O
1	O
-3	O
3	O
-1	O
1	O
-3	O
3	O
x	O
-1	O
-3	O
-1	O
1	O
.	O
-1	O
1	O
-1	O
1	O
3	O
-1	O
1	O
-1	O
-1	O
3	O
-3	O
1	O
x	O
1	O
x	O
-1	O
-3	O
1	O
3	O
x	O
-3	O
-1	O
-3	O
-1	O
-1	O
.	O
-1	O
1	O
3	O
1	O
1	O
3	O
-3	O
5	O
-3	O
3	O
-1	O
-1	O
x	O
1	O
-3	O
-1	O
-1	O
1	O
-3	O
-1	O
x	O
-1	O
-3	O
1	O
-1	O
.	O
-1	O
1	O
-1	O
3	O
1	O
x	O
-1	O
-1	O
1	O
5	O
1	O
1	O
-1	O
x	O
-3	O
1	O
-1	O
3	O
-3	O
-1	O
-3	O
x	O
-1	O
1	O
-1	O
.	O
-1	O
1	O
-3	O
3	O
1	O
1	O
1	O
-1	O
-1	O
3	O
-1	O
5	O
-3	O
-1	O
x	O
1	O
x	O
-1	O
-3	O
-1	O
x	O
1	O
3	O
1	O
-1	O
.	O
-1	O
3	O
1	O
-1	O
3	O
-1	O
x	O
1	O
-3	O
5	O
-1	O
-1	O
-3	O
1	O
-1	O
x	O
-3	O
-1	O
-3	O
3	O
3	O
1	O
-1	O
1	O
-1	O
.	O
-3	O
3	O
-3	O
1	O
1	O
-1	O
-1	O
-1	O
-1	O
1	O
-3	O
-1	O
-1	O
5	O
-1	O
1	O
-1	O
1	O
-5	O
-1	O
1	O
3	O
-3	O
3	O
-3	O
.	O
-1	O
1	O
1	O
-3	O
3	O
x	O
-1	O
3	O
-3	O
1	O
-1	O
3	O
-3	O
1	O
x	O
x	O
-5	O
1	O
1	O
3	O
1	O
3	O
1	O
3	O
-1	O
.	O
-1	O
3	O
-1	O
1	O
1	O
1	O
1	O
3	O
-5	O
-3	O
-3	O
3	O
-1	O
1	O
-1	O
1	O
-1	O
-1	O
-3	O
x	O
1	O
-1	O
-3	O
1	O
-1	O
.	O
x	O
1	O
-1	O
3	O
3	O
-1	O
1	O
1	O
-1	O
-1	O
-3	O
x	O
-3	O
-1	O
-3	O
-1	O
-1	O
5	O
-1	O
1	O
3	O
1	O
1	O
3	O
x	O
.	O
x	O
3	O
-1	O
-1	O
3	O
1	O
-3	O
-1	O
-1	O
1	O
-1	O
1	O
-1	O
1	O
3	O
3	O
-3	O
-1	O
1	O
-1	O
1	O
-3	O
-1	O
1	O
x	O
.	O
-5	O
-1	O
-1	O
-1	O
1	O
1	O
-1	O
-1	O
1	O
1	O
-1	O
1	O
-1	O
x	O
-3	O
3	O
1	O
-1	O
x	O
-1	O
3	O
1	O
-1	O
3	O
-5	O
.	O
1	O
1	O
1	O
-1	O
-1	O
1	O
1	O
-1	O
-3	O
-1	O
-3	O
-1	O
-3	O
1	O
-1	O
5	O
-1	O
1	O
-1	O
x	O
1	O
3	O
-1	O
-1	O
1	O
.	O
1	O
1	O
-1	O
-1	O
-3	O
1	O
-1	O
x	O
-1	O
1	O
-1	O
1	O
x	O
-1	O
1	O
3	O
-3	O
-1	O
-1	O
1	O
3	O
-1	O
-1	O
1	O
1	O
.	O
-3	O
3	O
-1	O
1	O
-3	O
-1	O
1	O
-1	O
-3	O
-1	O
-3	O
1	O
x	O
1	O
-1	O
5	O
-1	O
3	O
1	O
-1	O
3	O
-1	O
1	O
1	O
-3	O
.	O
x	O
-1	O
-3	O
1	O
-1	O
3	O
-3	O
-1	O
-3	O
3	O
x	O
1	O
-1	O
5	O
-1	O
1	O
-3	O
3	O
1	O
1	O
1	O
-1	O
-1	O
3	O
x	O
.	O
-3	O
-1	O
-5	O
1	O
-1	O
5	O
3	O
5	O
-1	O
-1	O
-3	O
x	O
-3	O
-1	O
-3	O
1	O
-5	O
1	O
-3	O
1	O
-1	O
-1	O
-1	O
-1	O
-3	O
.	O
3	O
x	O
-3	O
1	O
3	O
5	O
x	O
1	O
-3	O
-1	O
-3	O
-1	O
-3	O
-1	O
-1	O
-3	O
-1	O
-1	O
-1	O
1	O
-3	O
1	O
-3	O
-1	O
3	O
.	O
1	O
-1	O
x	O
3	O
1	O
3	O
-3	O
1	O
-1	O
1	O
x	O
1	O
-1	O
3	O
-3	O
-1	O
-1	O
-1	O
1	O
1	O
-3	O
1	O
-5	O
x	O
1	O
.	O
-1	O
-1	O
-3	O
-1	O
-3	O
3	O
3	O
1	O
-1	O
1	O
-1	O
5	O
-3	O
3	O
-3	O
1	O
1	O
-1	O
-1	O
-1	O
-1	O
1	O
-3	O
-1	O
-1	O
.	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
(	O
e	O
)	O
(	O
f	O
)	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
figure	O
42.4.	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
storing	O
(	O
cid:12	O
)	O
ve	O
memories	O
,	O
and	O
su	O
(	O
cid:11	O
)	O
ering	O
deletion	O
of	O
26	O
of	O
its	O
300	O
weights	O
.	O
(	O
a	O
)	O
the	O
(	O
cid:12	O
)	O
ve	O
memories	O
,	O
and	O
the	O
weights	O
of	O
the	O
network	O
,	O
with	O
deleted	O
weights	O
shown	O
by	O
‘	O
x	O
’	O
.	O
(	O
b	O
{	O
f	O
)	O
initial	O
states	O
that	O
di	O
(	O
cid:11	O
)	O
er	O
by	O
three	O
random	B
bits	O
from	O
a	O
memory	B
:	O
some	O
are	O
restored	O
,	O
but	O
some	O
converge	O
to	O
other	O
states	O
.	O
desired	O
memories	O
:	O
figure	O
42.5.	O
an	O
overloaded	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
trained	O
on	O
six	O
memories	O
,	O
most	O
of	O
which	O
are	O
not	O
stable	O
.	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
!	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
512	O
desired	O
memories	O
moscow	O
--	O
--	O
--	O
russia	O
lima	O
--	O
--	O
--	O
--	O
--	O
peru	O
london	O
--	O
--	O
-england	O
tokyo	O
--	O
--	O
--	O
--	O
japan	O
edinburgh-scotland	O
ottawa	O
--	O
--	O
--	O
canada	O
oslo	O
--	O
--	O
--	O
--	O
norway	O
stockholm	O
--	O
-sweden	O
paris	O
--	O
--	O
--	O
-france	O
!	O
w	O
!	O
attracting	O
stable	O
states	O
moscow	O
--	O
--	O
--	O
russia	O
lima	O
--	O
--	O
--	O
--	O
--	O
peru	O
londog	O
--	O
--	O
-englard	O
tonco	O
--	O
--	O
--	O
--	O
japan	O
edinburgh-scotland	O
oslo	O
--	O
--	O
--	O
--	O
norway	O
stockholm	O
--	O
-sweden	O
paris	O
--	O
--	O
--	O
-france	O
wzkmhewn	O
--	O
xqwqwpoq	O
paris	O
--	O
--	O
--	O
-sweden	O
ecnarf	O
--	O
--	O
--	O
-sirap	O
(	O
1	O
)	O
(	O
1	O
)	O
(	O
2	O
)	O
(	O
3	O
)	O
(	O
4	O
)	O
(	O
4	O
)	O
where	O
f	O
(	O
a	O
)	O
is	O
the	O
activation	B
function	I
,	O
for	O
example	O
f	O
(	O
a	O
)	O
=	O
tanh	O
(	O
a	O
)	O
.	O
for	O
a	O
steady	O
activation	O
ai	O
,	O
the	O
activity	O
xi	O
(	O
t	O
)	O
relaxes	O
exponentially	O
to	O
f	O
(	O
ai	O
)	O
with	O
time-constant	O
(	O
cid:28	O
)	O
.	O
now	O
,	O
here	O
is	O
the	O
nice	O
result	O
:	O
as	O
long	O
as	O
the	O
weight	B
matrix	O
is	O
symmetric	B
,	O
this	O
system	O
has	O
the	O
variational	B
free	I
energy	I
(	O
42.15	O
)	O
as	O
its	O
lyapunov	O
function	B
.	O
.	O
exercise	O
42.6	O
.	O
[	O
1	O
]	O
by	O
computing	O
d	O
dt	O
~f	O
,	O
prove	O
that	O
the	O
variational	B
free	I
energy	I
~f	O
(	O
x	O
)	O
is	O
a	O
lyapunov	O
function	B
for	O
the	O
continuous-time	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
.	O
42	O
|	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
figure	O
42.6.	O
failure	O
modes	O
of	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
(	O
highly	O
schematic	O
)	O
.	O
a	O
list	O
of	O
desired	O
memories	O
,	O
and	O
the	O
resulting	O
list	O
of	O
attracting	O
stable	O
states	O
.	O
notice	O
(	O
1	O
)	O
some	O
memories	O
that	O
are	O
retained	O
with	O
a	O
small	O
number	O
of	O
errors	O
;	O
(	O
2	O
)	O
desired	O
memories	O
that	O
are	O
completely	O
lost	O
(	O
there	O
is	O
no	O
attracting	O
stable	O
state	O
at	O
the	O
desired	O
memory	B
or	O
near	O
it	O
)	O
;	O
(	O
3	O
)	O
spurious	O
stable	O
states	O
unrelated	O
to	O
the	O
original	O
list	O
;	O
(	O
4	O
)	O
spurious	O
stable	O
states	O
that	O
are	O
confabulations	O
of	O
desired	O
memories	O
.	O
it	O
is	O
particularly	O
easy	O
to	O
prove	O
that	O
a	O
function	B
l	O
is	O
a	O
lyapunov	O
function	B
if	O
the	O
system	O
’	O
s	O
dynamics	O
perform	O
steepest	O
descent	O
on	O
l	O
,	O
with	O
d	O
l.	O
in	O
the	O
case	O
of	O
the	O
continuous-time	O
continuous	B
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
,	O
it	O
is	O
not	O
quite	O
~f	O
,	O
so	O
simple	O
,	O
but	O
every	O
component	O
of	O
d	O
which	O
means	O
that	O
with	O
an	O
appropriately	O
de	O
(	O
cid:12	O
)	O
ned	O
metric	B
,	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
dynamics	O
do	O
perform	O
steepest	B
descents	I
on	O
~f	O
(	O
x	O
)	O
.	O
dt	O
xi	O
(	O
t	O
)	O
/	O
@	O
dt	O
xi	O
(	O
t	O
)	O
does	O
have	O
the	O
same	O
sign	O
as	O
@	O
xi	O
@	O
@	O
xi	O
42.7	O
the	O
capacity	B
of	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
one	O
way	O
in	O
which	O
we	O
viewed	O
learning	B
in	O
the	O
single	B
neuron	I
was	O
as	O
communica-	O
tion	O
{	O
communication	B
of	O
the	O
labels	O
of	O
the	O
training	O
data	B
set	I
from	O
one	O
point	O
in	O
time	O
to	O
a	O
later	O
point	O
in	O
time	O
.	O
we	O
found	O
that	O
the	O
capacity	B
of	O
a	O
linear	B
threshold	O
neuron	B
was	O
2	O
bits	O
per	O
weight	B
.	O
similarly	O
,	O
we	O
might	O
view	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
associative	B
memory	I
as	O
a	O
commu-	O
nication	O
channel	B
(	O
(	O
cid:12	O
)	O
gure	O
42.6	O
)	O
.	O
a	O
list	O
of	O
desired	O
memories	O
is	O
encoded	O
into	O
a	O
set	B
of	O
weights	O
w	O
using	O
the	O
hebb	O
rule	O
of	O
equation	O
(	O
42.5	O
)	O
,	O
or	O
perhaps	O
some	O
other	O
learning	B
rule	I
.	O
the	O
receiver	O
,	O
receiving	O
the	O
weights	O
w	O
only	O
,	O
(	O
cid:12	O
)	O
nds	O
the	O
stable	O
states	O
of	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
,	O
which	O
he	O
interprets	O
as	O
the	O
original	O
mem-	O
ories	O
.	O
this	O
communication	B
system	O
can	O
fail	O
in	O
various	O
ways	O
,	O
as	O
illustrated	O
in	O
the	O
(	O
cid:12	O
)	O
gure	O
.	O
1.	O
individual	O
bits	O
in	O
some	O
memories	O
might	O
be	O
corrupted	O
,	O
that	O
is	O
,	O
a	O
sta-	O
ble	O
state	O
of	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
is	O
displaced	O
a	O
little	O
from	O
the	O
desired	O
memory	B
.	O
2.	O
entire	O
memories	O
might	O
be	O
absent	O
from	O
the	O
list	O
of	O
attractors	O
of	O
the	O
net-	O
work	O
;	O
or	O
a	O
stable	O
state	O
might	O
be	O
present	O
but	O
have	O
such	O
a	O
small	O
basin	O
of	O
attraction	O
that	O
it	O
is	O
of	O
no	O
use	O
for	O
pattern	O
completion	O
and	O
error	O
correction	O
.	O
3.	O
spurious	O
additional	O
memories	O
unrelated	O
to	O
the	O
desired	O
memories	O
might	O
be	O
present	O
.	O
4.	O
spurious	O
additional	O
memories	O
derived	O
from	O
the	O
desired	O
memories	O
by	O
op-	O
erations	O
such	O
as	O
mixing	O
and	O
inversion	O
may	O
also	O
be	O
present	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
42.7	O
:	O
the	O
capacity	B
of	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
513	O
of	O
these	O
failure	O
modes	O
,	O
modes	O
1	O
and	O
2	O
are	O
clearly	O
undesirable	O
,	O
mode	O
2	O
espe-	O
cially	O
so	O
.	O
mode	O
3	O
might	O
not	O
matter	O
so	O
much	O
as	O
long	O
as	O
each	O
of	O
the	O
desired	O
memories	O
has	O
a	O
large	O
basin	O
of	O
attraction	O
.	O
the	O
fourth	O
failure	O
mode	O
might	O
in	O
some	O
contexts	O
actually	O
be	O
viewed	O
as	O
bene	O
(	O
cid:12	O
)	O
cial	O
.	O
for	O
example	O
,	O
if	O
a	O
network	B
is	O
required	O
to	O
memorize	O
examples	O
of	O
valid	O
sentences	O
such	O
as	O
‘	O
john	O
loves	O
mary	O
’	O
and	O
‘	O
john	O
gets	O
cake	O
’	O
,	O
we	O
might	O
be	O
happy	O
to	O
(	O
cid:12	O
)	O
nd	O
that	O
‘	O
john	O
loves	O
cake	O
’	O
was	O
also	O
a	O
stable	O
state	O
of	O
the	O
network	B
.	O
we	O
might	O
call	O
this	O
behaviour	O
‘	O
generalization	B
’	O
.	O
the	O
capacity	B
of	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
with	O
i	O
neurons	O
might	O
be	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
the	O
number	O
of	O
random	O
patterns	O
n	O
that	O
can	O
be	O
stored	O
without	O
failure-mode	O
2	O
having	O
substantial	O
probability	B
.	O
if	O
we	O
also	O
require	O
failure-mode	O
1	O
to	O
have	O
tiny	O
probability	B
then	O
the	O
resulting	O
capacity	B
is	O
much	O
smaller	O
.	O
we	O
now	O
study	O
these	O
alternative	O
de	O
(	O
cid:12	O
)	O
nitions	O
of	O
the	O
capacity	O
.	O
the	O
capacity	B
of	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
{	O
stringent	O
de	O
(	O
cid:12	O
)	O
nition	O
we	O
will	O
(	O
cid:12	O
)	O
rst	O
explore	B
the	O
information	B
storage	O
capabilities	O
of	O
a	O
binary	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
that	O
learns	O
using	O
the	O
hebb	O
rule	O
by	O
considering	O
the	O
stability	O
of	O
just	O
one	O
bit	B
of	O
one	O
of	O
the	O
desired	O
patterns	O
,	O
assuming	O
that	O
the	O
state	O
of	O
the	O
network	B
is	O
set	B
to	O
that	O
desired	O
pattern	O
x	O
(	O
n	O
)	O
.	O
we	O
will	O
assume	O
that	O
the	O
patterns	O
to	O
be	O
stored	O
are	O
randomly	O
selected	O
binary	O
patterns	O
.	O
the	O
activation	O
of	O
a	O
particular	O
neuron	B
i	O
is	O
wijx	O
(	O
n	O
)	O
j	O
;	O
ai	O
=xj	O
where	O
the	O
weights	O
are	O
,	O
for	O
i	O
6=	O
j	O
,	O
wij	O
=	O
x	O
(	O
n	O
)	O
i	O
x	O
(	O
n	O
)	O
j	O
+	O
xm6=n	O
x	O
(	O
m	O
)	O
i	O
x	O
(	O
m	O
)	O
j	O
:	O
(	O
42.18	O
)	O
(	O
42.19	O
)	O
here	O
we	O
have	O
split	O
w	O
into	O
two	O
terms	O
,	O
the	O
(	O
cid:12	O
)	O
rst	O
of	O
which	O
will	O
contribute	O
‘	O
signal	O
’	O
,	O
reinforcing	O
the	O
desired	O
memory	B
,	O
and	O
the	O
second	O
‘	O
noise	B
’	O
.	O
substituting	O
for	O
wij	O
,	O
the	O
activation	O
is	O
ai	O
=	O
xj6=i	O
x	O
(	O
n	O
)	O
i	O
x	O
(	O
n	O
)	O
j	O
x	O
(	O
n	O
)	O
j	O
+xj6=i	O
xm6=n	O
x	O
(	O
m	O
)	O
i	O
x	O
(	O
m	O
)	O
j	O
x	O
(	O
n	O
)	O
j	O
(	O
42.20	O
)	O
x	O
(	O
m	O
)	O
i	O
x	O
(	O
m	O
)	O
j	O
x	O
(	O
n	O
)	O
j	O
:	O
(	O
42.21	O
)	O
=	O
(	O
i	O
(	O
cid:0	O
)	O
1	O
)	O
x	O
(	O
n	O
)	O
i	O
+xj6=i	O
xm6=n	O
the	O
(	O
cid:12	O
)	O
rst	O
term	O
is	O
(	O
i	O
(	O
cid:0	O
)	O
1	O
)	O
times	O
the	O
desired	O
state	O
x	O
(	O
n	O
)	O
second	O
term	O
is	O
a	O
sum	O
of	O
(	O
i	O
(	O
cid:0	O
)	O
1	O
)	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
random	B
quantities	O
x	O
(	O
m	O
)	O
.	O
if	O
this	O
were	O
the	O
only	O
term	O
,	O
it	O
would	O
keep	O
the	O
neuron	B
(	O
cid:12	O
)	O
rmly	O
clamped	O
in	O
the	O
desired	O
state	O
.	O
the	O
.	O
a	O
moment	O
’	O
s	O
re	O
(	O
cid:13	O
)	O
ection	O
con	O
(	O
cid:12	O
)	O
rms	O
that	O
these	O
quantities	O
are	O
independent	O
random	O
binary	O
variables	O
with	O
mean	O
0	O
and	O
variance	O
1.	O
i	O
x	O
(	O
m	O
)	O
j	O
x	O
(	O
n	O
)	O
j	O
i	O
thus	O
,	O
considering	O
the	O
statistics	O
of	O
ai	O
under	O
the	O
ensemble	B
of	O
random	B
pat-	O
terns	O
,	O
we	O
conclude	O
that	O
ai	O
has	O
mean	B
(	O
i	O
(	O
cid:0	O
)	O
1	O
)	O
x	O
(	O
n	O
)	O
and	O
variance	O
(	O
i	O
(	O
cid:0	O
)	O
1	O
)	O
(	O
n	O
(	O
cid:0	O
)	O
1	O
)	O
.	O
for	O
brevity	O
,	O
we	O
will	O
now	O
assume	O
i	O
and	O
n	O
are	O
large	O
enough	O
that	O
we	O
can	O
neglect	O
the	O
distinction	O
between	O
i	O
and	O
i	O
(	O
cid:0	O
)	O
1	O
,	O
and	O
between	O
n	O
and	O
n	O
(	O
cid:0	O
)	O
1.	O
then	O
we	O
can	O
restate	O
our	O
conclusion	O
:	O
ai	O
is	O
gaussian-distributed	O
with	O
mean	O
ix	O
(	O
n	O
)	O
and	O
variance	O
in	O
.	O
i	O
i	O
what	O
then	O
is	O
the	O
probability	B
that	O
the	O
selected	O
bit	B
is	O
stable	O
,	O
if	O
we	O
put	O
the	O
network	B
into	O
the	O
state	O
x	O
(	O
n	O
)	O
?	O
the	O
probability	B
that	O
bit	B
i	O
will	O
(	O
cid:13	O
)	O
ip	O
on	O
the	O
(	O
cid:12	O
)	O
rst	O
iteration	O
of	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
’	O
s	O
dynamics	O
is	O
p	O
(	O
i	O
unstable	O
)	O
=	O
(	O
cid:8	O
)	O
(	O
cid:18	O
)	O
(	O
cid:0	O
)	O
i	O
pin	O
(	O
cid:19	O
)	O
=	O
(	O
cid:8	O
)	O
(	O
cid:0	O
)	O
1	O
pn=i	O
!	O
;	O
(	O
42.22	O
)	O
pin	O
i	O
ai	O
figure	O
42.7.	O
the	O
probability	B
density	O
of	O
the	O
activation	O
ai	O
in	O
the	O
case	O
x	O
(	O
n	O
)	O
i	O
=	O
1	O
;	O
the	O
probability	B
that	O
bit	B
i	O
becomes	O
(	O
cid:13	O
)	O
ipped	O
is	O
the	O
area	O
of	O
the	O
tail	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
42	O
|	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
figure	O
42.8.	O
overlap	O
between	O
a	O
desired	O
memory	B
and	O
the	O
stable	O
state	O
nearest	O
to	O
it	O
as	O
a	O
function	B
of	O
the	O
loading	O
fraction	O
n=i	O
.	O
the	O
overlap	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
the	O
scaled	O
i	O
=i	O
,	O
which	O
is	O
1	O
when	O
recall	O
is	O
perfect	B
and	O
zero	O
when	O
the	O
stable	O
state	O
has	O
50	O
%	O
of	O
the	O
bits	O
(	O
cid:13	O
)	O
ipped	O
.	O
there	O
is	O
an	O
abrupt	O
transition	B
at	O
n=i	O
=	O
0:138	O
,	O
where	O
the	O
overlap	O
drops	O
from	O
0.97	O
to	O
zero	O
.	O
inner	O
productpi	O
xix	O
(	O
n	O
)	O
514	O
1	O
0.8	O
0.6	O
0.4	O
0.2	O
0	O
where	O
1	O
0.99	O
0.98	O
0.97	O
0.96	O
0	O
0.02	O
0.04	O
0.06	O
0.08	O
0.1	O
0.12	O
0.14	O
0.16	O
0.95	O
0.09	O
0.1	O
0.11	O
0.12	O
0.13	O
0.14	O
0.15	O
(	O
cid:8	O
)	O
(	O
z	O
)	O
(	O
cid:17	O
)	O
z	O
z	O
(	O
cid:0	O
)	O
1	O
dz	O
1p2	O
(	O
cid:25	O
)	O
e	O
(	O
cid:0	O
)	O
z2=2	O
:	O
(	O
42.23	O
)	O
the	O
important	O
quantity	O
n=i	O
is	O
the	O
ratio	O
of	O
the	O
number	O
of	O
patterns	O
stored	O
to	O
the	O
number	O
of	O
neurons	O
.	O
if	O
,	O
for	O
example	O
,	O
we	O
try	O
to	O
store	O
n	O
’	O
0:18i	O
patterns	O
in	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
then	O
there	O
is	O
a	O
chance	O
of	O
1	O
%	O
that	O
a	O
speci	O
(	O
cid:12	O
)	O
ed	O
bit	B
in	O
a	O
speci	O
(	O
cid:12	O
)	O
ed	O
pattern	O
will	O
be	O
unstable	O
on	O
the	O
(	O
cid:12	O
)	O
rst	O
iteration	O
.	O
we	O
are	O
now	O
in	O
a	O
position	O
to	O
derive	O
our	O
(	O
cid:12	O
)	O
rst	O
capacity	B
result	O
,	O
for	O
the	O
case	O
where	O
no	O
corruption	O
of	O
the	O
desired	O
memories	O
is	O
permitted	O
.	O
.	O
exercise	O
42.7	O
.	O
[	O
2	O
]	O
assume	O
that	O
we	O
wish	O
all	O
the	O
desired	O
patterns	O
to	O
be	O
completely	O
stable	O
{	O
we	O
don	O
’	O
t	O
want	O
any	O
of	O
the	O
bits	O
to	O
(	O
cid:13	O
)	O
ip	O
when	O
the	O
network	B
is	O
put	O
into	O
any	O
desired	O
pattern	O
state	O
{	O
and	O
the	O
total	O
probability	O
of	O
any	O
error	O
at	O
all	O
is	O
required	O
to	O
be	O
less	O
than	O
a	O
small	O
number	O
(	O
cid:15	O
)	O
.	O
using	O
the	O
approximation	B
to	O
the	O
error	B
function	I
for	O
large	O
z	O
,	O
(	O
cid:8	O
)	O
(	O
(	O
cid:0	O
)	O
z	O
)	O
’	O
1	O
p2	O
(	O
cid:25	O
)	O
e	O
(	O
cid:0	O
)	O
z2=2	O
z	O
;	O
(	O
42.24	O
)	O
show	O
that	O
the	O
maximum	O
number	O
of	O
patterns	O
that	O
can	O
be	O
stored	O
,	O
nmax	O
,	O
is	O
nmax	O
’	O
4	O
ln	O
i	O
+	O
2	O
ln	O
(	O
1=	O
(	O
cid:15	O
)	O
)	O
i	O
:	O
(	O
42.25	O
)	O
if	O
,	O
however	O
,	O
we	O
allow	O
a	O
small	O
amount	O
of	O
corruption	O
of	O
memories	O
to	O
occur	O
,	O
the	O
number	O
of	O
patterns	O
that	O
can	O
be	O
stored	O
increases	O
.	O
the	O
statistical	B
physicists	O
’	O
capacity	B
the	O
analysis	B
that	O
led	O
to	O
equation	O
(	O
42.22	O
)	O
tells	O
us	O
that	O
if	O
we	O
try	O
to	O
store	O
n	O
’	O
0:18i	O
patterns	O
in	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
then	O
,	O
starting	O
from	O
a	O
desired	O
memory	B
,	O
about	O
1	O
%	O
of	O
the	O
bits	O
will	O
be	O
unstable	O
on	O
the	O
(	O
cid:12	O
)	O
rst	O
iteration	O
.	O
our	O
analysis	B
does	O
not	O
shed	O
light	O
on	O
what	O
is	O
expected	O
to	O
happen	O
on	O
subsequent	O
iterations	O
.	O
the	O
(	O
cid:13	O
)	O
ipping	O
of	O
these	O
bits	O
might	O
make	O
some	O
of	O
the	O
other	O
bits	O
unstable	O
too	O
,	O
causing	O
an	O
increasing	O
number	O
of	O
bits	O
to	O
be	O
(	O
cid:13	O
)	O
ipped	O
.	O
this	O
process	O
might	O
lead	O
to	O
an	O
avalanche	O
in	O
which	O
the	O
network	B
’	O
s	O
state	O
ends	O
up	O
a	O
long	O
way	O
from	O
the	O
desired	O
memory	B
.	O
in	O
fact	O
,	O
when	O
n=i	O
is	O
large	O
,	O
such	O
avalanches	O
do	O
happen	O
.	O
when	O
n=i	O
is	O
small	O
,	O
they	O
tend	O
not	O
to	O
{	O
there	O
is	O
a	O
stable	O
state	O
near	O
to	O
each	O
desired	O
memory	B
.	O
for	O
the	O
limit	O
of	O
large	O
i	O
,	O
amit	O
et	O
al	O
.	O
(	O
1985	O
)	O
have	O
used	O
methods	B
from	O
statistical	B
physics	I
to	O
(	O
cid:12	O
)	O
nd	O
numerically	O
the	O
transition	B
between	O
these	O
two	O
behaviours	O
.	O
there	O
is	O
a	O
sharp	O
discontinuity	O
at	O
ncrit	O
=	O
0:138i	O
:	O
(	O
42.26	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
42.8	O
:	O
improving	O
on	O
the	O
capacity	B
of	O
the	O
hebb	O
rule	O
515	O
below	O
this	O
critical	O
value	O
,	O
there	O
is	O
likely	O
to	O
be	O
a	O
stable	O
state	O
near	O
every	O
desired	O
memory	B
,	O
in	O
which	O
a	O
small	O
fraction	O
of	O
the	O
bits	O
are	O
(	O
cid:13	O
)	O
ipped	O
.	O
when	O
n=i	O
exceeds	O
0.138	O
,	O
the	O
system	O
has	O
only	O
spurious	O
stable	O
states	O
,	O
known	O
as	O
spin	O
glass	O
states	O
,	O
none	O
of	O
which	O
is	O
correlated	O
with	O
any	O
of	O
the	O
desired	O
memories	O
.	O
just	O
below	O
the	O
critical	O
value	O
,	O
the	O
fraction	O
of	O
bits	O
that	O
are	O
(	O
cid:13	O
)	O
ipped	O
when	O
a	O
desired	O
memory	B
has	O
evolved	O
to	O
its	O
associated	O
stable	O
state	O
is	O
1.6	O
%	O
.	O
figure	O
42.8	O
shows	O
the	O
overlap	O
between	O
the	O
desired	O
memory	B
and	O
the	O
nearest	O
stable	O
state	O
as	O
a	O
function	B
of	O
n=i	O
.	O
some	O
other	O
transitions	O
in	O
properties	O
of	O
the	O
model	O
occur	O
at	O
some	O
additional	O
values	O
of	O
n=i	O
,	O
as	O
summarized	O
below	O
.	O
for	O
all	O
n=i	O
,	O
stable	O
spin	O
glass	O
states	O
exist	O
,	O
uncorrelated	O
with	O
the	O
desired	O
memories	O
.	O
for	O
n=i	O
>	O
0:138	O
,	O
these	O
spin	O
glass	O
states	O
are	O
the	O
only	O
stable	O
states	O
.	O
for	O
n=i	O
2	O
(	O
0	O
;	O
0:138	O
)	O
,	O
there	O
are	O
stable	O
states	O
close	O
to	O
the	O
desired	O
memories	O
.	O
for	O
n=i	O
2	O
(	O
0	O
;	O
0:05	O
)	O
,	O
the	O
stable	O
states	O
associated	O
with	O
the	O
desired	O
memories	O
have	O
lower	O
energy	B
than	O
the	O
spurious	O
spin	O
glass	O
states	O
.	O
for	O
n=i	O
2	O
(	O
0:05	O
;	O
0:138	O
)	O
,	O
the	O
spin	O
glass	O
states	O
dominate	O
{	O
there	O
are	O
spin	O
glass	O
states	O
that	O
have	O
lower	O
energy	B
than	O
the	O
stable	O
states	O
associated	O
with	O
the	O
desired	O
memories	O
.	O
for	O
n=i	O
2	O
(	O
0	O
;	O
0:03	O
)	O
,	O
there	O
are	O
additional	O
mixture	O
states	O
,	O
which	O
are	O
combina-	O
tions	O
of	O
several	O
desired	O
memories	O
.	O
these	O
stable	O
states	O
do	O
not	O
have	O
as	O
low	O
energy	B
as	O
the	O
stable	O
states	O
associated	O
with	O
the	O
desired	O
memories	O
.	O
in	O
conclusion	O
,	O
the	O
capacity	B
of	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
with	O
i	O
neurons	O
,	O
if	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
capacity	B
in	O
terms	O
of	O
the	O
abrupt	O
discontinuity	O
discussed	O
above	O
,	O
is	O
0:138i	O
random	B
binary	O
patterns	O
,	O
each	O
of	O
length	O
i	O
,	O
each	O
of	O
which	O
is	O
received	O
with	O
1.6	O
%	O
of	O
its	O
bits	O
(	O
cid:13	O
)	O
ipped	O
.	O
in	O
bits	O
,	O
this	O
capacity	B
is	O
0:138i	O
2	O
(	O
cid:2	O
)	O
(	O
1	O
(	O
cid:0	O
)	O
h2	O
(	O
0:016	O
)	O
)	O
=	O
0:122	O
i	O
2	O
bits	O
:	O
(	O
42.27	O
)	O
since	O
there	O
are	O
i	O
2=2	O
weights	O
in	O
the	O
network	B
,	O
we	O
can	O
also	O
express	O
the	O
capacity	B
as	O
0.24	O
bits	O
per	O
weight	B
.	O
42.8	O
improving	O
on	O
the	O
capacity	B
of	O
the	O
hebb	O
rule	O
the	O
capacities	O
discussed	O
in	O
the	O
previous	O
section	B
are	O
the	O
capacities	O
of	O
the	O
hop-	O
(	O
cid:12	O
)	O
eld	O
network	B
whose	O
weights	O
are	O
set	B
using	O
the	O
hebbian	O
learning	B
rule	I
.	O
we	O
can	O
do	O
better	O
than	O
the	O
hebb	O
rule	O
by	O
de	O
(	O
cid:12	O
)	O
ning	O
an	O
objective	B
function	I
that	O
measures	O
how	O
well	O
the	O
network	B
stores	O
all	O
the	O
memories	O
,	O
and	O
minimizing	O
it	O
.	O
for	O
an	O
associative	B
memory	I
to	O
be	O
useful	O
,	O
it	O
must	O
be	O
able	O
to	O
correct	O
at	O
least	O
one	O
(	O
cid:13	O
)	O
ipped	O
bit	B
.	O
let	O
’	O
s	O
make	O
an	O
objective	B
function	I
that	O
measures	O
whether	O
(	O
cid:13	O
)	O
ipped	O
bits	O
tend	O
to	O
be	O
restored	O
correctly	O
.	O
our	O
intention	O
is	O
that	O
,	O
for	O
every	O
neuron	B
i	O
in	O
the	O
network	B
,	O
the	O
weights	O
to	O
that	O
neuron	B
should	O
satisfy	O
this	O
rule	O
:	O
this	O
expression	O
for	O
the	O
capacity	B
omits	O
a	O
smaller	O
negative	O
term	O
of	O
order	O
n	O
log2	O
n	O
bits	O
,	O
associated	O
with	O
the	O
arbitrary	O
order	O
of	O
the	O
memories	O
.	O
for	O
every	O
pattern	O
x	O
(	O
n	O
)	O
,	O
if	O
the	O
neurons	O
other	O
than	O
i	O
are	O
set	B
correctly	O
to	O
xj	O
=	O
x	O
(	O
n	O
)	O
,	O
then	O
the	O
activation	O
of	O
neuron	B
i	O
should	O
be	O
such	O
that	O
its	O
preferred	O
output	O
is	O
xi	O
=	O
x	O
(	O
n	O
)	O
.	O
j	O
i	O
is	O
this	O
rule	O
a	O
familiar	O
idea	O
?	O
yes	O
,	O
it	O
is	O
precisely	O
what	O
we	O
wanted	O
the	O
single	B
neuron	I
of	O
chapter	O
39	O
to	O
do	O
.	O
each	O
pattern	O
x	O
(	O
n	O
)	O
de	O
(	O
cid:12	O
)	O
nes	O
an	O
input	O
,	O
target	O
pair	O
for	O
the	O
single	B
neuron	I
i.	O
and	O
it	O
de	O
(	O
cid:12	O
)	O
nes	O
an	O
input	O
,	O
target	O
pair	O
for	O
all	O
the	O
other	O
neurons	O
too	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
42	O
|	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
algorithm	B
42.9.	O
octave	B
source	O
code	B
for	O
optimizing	O
the	O
weights	O
of	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
,	O
so	O
that	O
it	O
works	O
as	O
an	O
associative	B
memory	I
.	O
cf	O
.	O
algorithm	B
39.5.	O
the	O
data	O
matrix	O
x	O
has	O
i	O
columns	O
and	O
n	O
rows	O
.	O
the	O
matrix	B
t	O
is	O
identical	O
to	O
x	O
except	O
that	O
(	O
cid:0	O
)	O
1s	O
are	O
replaced	O
by	O
0s	O
.	O
516	O
w	O
=	O
x	O
’	O
*	O
x	O
;	O
#	O
initialize	O
the	O
weights	O
using	O
hebb	O
rule	O
for	O
l	O
=	O
1	O
:	O
l	O
#	O
loop	O
l	O
times	O
for	O
i=1	O
:	O
i	O
w	O
(	O
i	O
,	O
i	O
)	O
=	O
0	O
;	O
end	O
#	O
#	O
#	O
ensure	O
the	O
self-weights	O
are	O
zero	O
.	O
a	O
y	O
e	O
gw	O
=	O
x	O
’	O
*	O
e	O
gw	O
=	O
gw	O
+	O
gw	O
’	O
=	O
x	O
*	O
w	O
;	O
=	O
sigmoid	B
(	O
a	O
)	O
;	O
;	O
=	O
t	O
-	O
y	O
;	O
;	O
#	O
compute	O
all	O
activations	O
#	O
compute	O
all	O
outputs	O
#	O
compute	O
all	O
errors	B
#	O
compute	O
the	O
gradients	O
#	O
symmetrize	O
gradients	O
w	O
=	O
w	O
+	O
eta	O
*	O
(	O
gw	O
-	O
alpha	O
*	O
w	O
)	O
;	O
#	O
make	O
step	O
endfor	O
so	O
,	O
just	O
as	O
we	O
de	O
(	O
cid:12	O
)	O
ned	O
an	O
objective	B
function	I
(	O
39.11	O
)	O
for	O
the	O
training	O
of	O
a	O
single	B
neuron	I
as	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
,	O
we	O
can	O
de	O
(	O
cid:12	O
)	O
ne	O
g	O
(	O
w	O
)	O
=	O
(	O
cid:0	O
)	O
xi	O
xn	O
t	O
(	O
n	O
)	O
i	O
ln	O
y	O
(	O
n	O
)	O
i	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
t	O
(	O
n	O
)	O
i	O
)	O
ln	O
(	O
1	O
(	O
cid:0	O
)	O
y	O
(	O
n	O
)	O
i	O
)	O
(	O
42.28	O
)	O
where	O
and	O
t	O
(	O
n	O
)	O
i	O
=	O
(	O
1	O
x	O
(	O
n	O
)	O
i	O
=	O
1	O
0	O
x	O
(	O
n	O
)	O
i	O
=	O
(	O
cid:0	O
)	O
1	O
y	O
(	O
n	O
)	O
i	O
=	O
1	O
1	O
+	O
exp	O
(	O
(	O
cid:0	O
)	O
a	O
(	O
n	O
)	O
i	O
)	O
(	O
42.29	O
)	O
:	O
(	O
42.30	O
)	O
;	O
where	O
a	O
(	O
n	O
)	O
i	O
=p	O
wijx	O
(	O
n	O
)	O
j	O
we	O
can	O
then	O
steal	O
the	O
algorithm	B
(	O
algorithm	B
39.5	O
,	O
p.478	O
)	O
which	O
we	O
wrote	O
for	O
the	O
single	B
neuron	I
,	O
to	O
write	O
an	O
algorithm	B
for	O
optimizing	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
,	O
algorithm	B
42.9.	O
the	O
convenient	O
syntax	O
of	O
octave	O
requires	O
very	O
few	O
changes	O
;	O
the	O
extra	O
lines	O
enforce	O
the	O
constraints	O
that	O
the	O
self-weights	O
wii	O
should	O
all	O
be	O
zero	O
and	O
that	O
the	O
weight	B
matrix	O
should	O
be	O
symmetrical	O
(	O
wij	O
=	O
wji	O
)	O
.	O
as	O
expected	O
,	O
this	O
learning	B
algorithm	O
does	O
a	O
better	O
job	O
than	O
the	O
one-shot	O
hebbian	O
learning	B
rule	I
.	O
when	O
the	O
six	B
patterns	O
of	O
(	O
cid:12	O
)	O
gure	O
42.5	O
,	O
which	O
can	O
not	O
be	O
memorized	O
by	O
the	O
hebb	O
rule	O
,	O
are	O
learned	O
using	O
algorithm	B
42.9	O
,	O
all	O
six	B
patterns	O
become	O
stable	O
states	O
.	O
exercise	O
42.8	O
.	O
[	O
4c	O
]	O
implement	O
this	O
learning	B
rule	I
and	O
investigate	O
empirically	O
its	O
capacity	B
for	O
memorizing	O
random	B
patterns	O
;	O
also	O
compare	O
its	O
avalanche	O
properties	O
with	O
those	O
of	O
the	O
hebb	O
rule	O
.	O
42.9	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
for	O
optimization	O
problems	O
since	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
’	O
s	O
dynamics	O
minimize	O
an	O
energy	B
function	O
,	O
it	O
is	O
natural	B
to	O
ask	O
whether	O
we	O
can	O
map	O
interesting	O
optimization	B
problems	O
onto	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
.	O
biological	O
data	O
processing	O
problems	O
often	O
involve	O
an	O
element	O
of	O
constraint	O
satisfaction	O
{	O
in	O
scene	O
interpretation	O
,	O
for	O
example	O
,	O
one	O
might	O
wish	O
to	O
infer	O
the	O
spatial	O
location	O
,	O
orientation	O
,	O
brightness	O
and	O
texture	O
of	O
each	O
visible	O
element	O
,	O
and	O
which	O
visible	O
elements	O
are	O
connected	O
together	O
in	O
objects	O
.	O
these	O
inferences	O
are	O
constrained	B
by	O
the	O
given	O
data	O
and	O
by	O
prior	O
knowledge	O
about	O
continuity	O
of	O
objects	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
42.9	O
:	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
for	O
optimization	O
problems	O
517	O
place	O
in	O
tour	O
1	O
2	O
3	O
4	O
place	O
in	O
tour	O
1	O
2	O
3	O
4	O
city	O
a	O
b	O
c	O
d	O
a	O
c	O
city	O
a	O
b	O
c	O
d	O
a	O
c	O
b	O
d	O
b	O
d	O
(	O
a1	O
)	O
(	O
a2	O
)	O
figure	O
42.10.	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
for	O
solving	O
a	O
travelling	B
salesman	I
problem	I
with	O
k	O
=	O
4	O
cities	O
.	O
(	O
a1,2	O
)	O
two	O
solution	O
states	O
of	O
the	O
16-neuron	O
network	B
,	O
with	O
activites	O
represented	O
by	O
black	O
=	O
1	O
,	O
white	B
=	O
0	O
;	O
and	O
the	O
tours	O
corresponding	O
to	O
these	O
network	B
states	O
.	O
(	O
b	O
)	O
the	O
negative	O
weights	O
between	O
node	O
b2	O
and	O
other	O
nodes	O
;	O
these	O
weights	O
enforce	O
validity	O
of	O
a	O
tour	O
.	O
(	O
c	O
)	O
the	O
negative	O
weights	O
that	O
embody	O
the	O
distance	B
objective	O
function	B
.	O
1	O
2	O
3	O
4	O
1	O
2	O
3	O
4	O
(	O
cid:0	O
)	O
dbd	O
a	O
b	O
c	O
d	O
a	O
b	O
c	O
d	O
(	O
b	O
)	O
(	O
c	O
)	O
hop	O
(	O
cid:12	O
)	O
eld	O
and	O
tank	O
(	O
1985	O
)	O
suggested	O
that	O
one	O
might	O
take	O
an	O
interesting	O
constraint	B
satisfaction	I
problem	O
and	O
design	O
the	O
weights	O
of	O
a	O
binary	O
or	O
contin-	O
uous	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
such	O
that	O
the	O
settling	O
process	O
of	O
the	O
network	O
would	O
minimize	O
the	O
objective	B
function	I
of	O
the	O
problem	O
.	O
the	O
travelling	B
salesman	I
problem	I
a	O
classic	O
constraint	B
satisfaction	I
problem	O
to	O
which	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
have	O
been	O
applied	O
is	O
the	O
travelling	B
salesman	I
problem	I
.	O
a	O
set	B
of	O
k	O
cities	O
is	O
given	O
,	O
and	O
a	O
matrix	B
of	O
the	O
k	O
(	O
k	O
(	O
cid:0	O
)	O
1	O
)	O
=2	O
distances	O
between	O
those	O
cities	O
.	O
the	O
task	O
is	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
closed	O
tour	O
of	O
the	O
cities	O
,	O
visiting	O
each	O
city	O
once	O
,	O
that	O
has	O
the	O
smallest	O
total	O
distance	B
.	O
the	O
travelling	B
salesman	I
problem	I
is	O
equivalent	O
in	O
di	O
(	O
cid:14	O
)	O
culty	O
to	O
an	O
np-complete	O
problem	O
.	O
the	O
method	B
suggested	O
by	O
hop	O
(	O
cid:12	O
)	O
eld	O
and	O
tank	O
is	O
to	O
represent	O
a	O
tentative	O
so-	O
lution	O
to	O
the	O
problem	O
by	O
the	O
state	O
of	O
a	O
network	B
with	O
i	O
=	O
k	O
2	O
neurons	O
arranged	O
in	O
a	O
square	B
,	O
with	O
each	O
neuron	B
representing	O
the	O
hypothesis	O
that	O
a	O
particular	O
city	O
comes	O
at	O
a	O
particular	O
point	O
in	O
the	O
tour	O
.	O
it	O
will	O
be	O
convenient	O
to	O
consider	O
the	O
states	O
of	O
the	O
neurons	O
as	O
being	O
between	O
0	O
and	O
1	O
rather	O
than	O
(	O
cid:0	O
)	O
1	O
and	O
1.	O
two	O
solution	O
states	O
for	O
a	O
four-city	O
travelling	B
salesman	I
problem	I
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
42.10a	O
.	O
the	O
weights	O
in	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
play	O
two	O
roles	O
.	O
first	O
,	O
they	O
must	O
de	O
(	O
cid:12	O
)	O
ne	O
an	O
energy	B
function	O
which	O
is	O
minimized	O
only	O
when	O
the	O
state	O
of	O
the	O
network	B
represents	O
a	O
valid	O
tour	O
.	O
a	O
valid	O
state	O
is	O
one	O
that	O
looks	O
like	O
a	O
permutation	B
matrix	O
,	O
having	O
exactly	O
one	O
‘	O
1	O
’	O
in	O
every	O
row	O
and	O
one	O
‘	O
1	O
’	O
in	O
every	O
column	O
.	O
this	O
rule	O
can	O
be	O
enforced	O
by	O
putting	O
large	O
negative	O
weights	O
between	O
any	O
pair	O
of	O
neurons	O
that	O
are	O
in	O
the	O
same	O
row	O
or	O
the	O
same	O
column	O
,	O
and	O
setting	O
a	O
positive	O
bias	O
for	O
all	O
neurons	O
to	O
ensure	O
that	O
k	O
neurons	O
do	O
turn	O
on	O
.	O
figure	O
42.10b	O
shows	O
the	O
negative	O
weights	O
that	O
are	O
connected	O
to	O
one	O
neuron	B
,	O
‘	O
b2	O
’	O
,	O
which	O
represents	O
the	O
statement	O
‘	O
city	O
b	O
comes	O
second	O
in	O
the	O
tour	O
’	O
.	O
second	O
,	O
the	O
weights	O
must	O
encode	O
the	O
objective	B
function	I
that	O
we	O
want	O
to	O
minimize	O
{	O
the	O
total	O
distance	B
.	O
this	O
can	O
be	O
done	O
by	O
putting	O
negative	O
weights	O
proportional	O
to	O
the	O
appropriate	O
distances	O
between	O
the	O
nodes	O
in	O
adja-	O
cent	O
columns	O
.	O
for	O
example	O
,	O
between	O
the	O
b	O
and	O
d	O
nodes	O
in	O
adjacent	O
columns	O
,	O
the	O
weight	B
would	O
be	O
(	O
cid:0	O
)	O
dbd	O
.	O
the	O
negative	O
weights	O
that	O
are	O
connected	O
to	O
neu-	O
ron	O
b2	O
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
42.10c	O
.	O
the	O
result	O
is	O
that	O
when	O
the	O
network	B
is	O
in	O
a	O
valid	O
state	O
,	O
its	O
total	O
energy	B
will	O
be	O
the	O
total	O
distance	B
of	O
the	O
corresponding	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
518	O
42	O
|	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
figure	O
42.11	O
.	O
(	O
a	O
)	O
evolution	B
of	O
the	O
state	O
of	O
a	O
continuous	B
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
solving	O
a	O
travelling	B
salesman	I
problem	I
using	O
aiyer	O
’	O
s	O
(	O
1991	O
)	O
graduated	B
non-convexity	I
method	O
;	O
the	O
state	O
of	O
the	O
network	B
is	O
projected	O
into	O
the	O
two-dimensional	B
space	O
in	O
which	O
the	O
cities	O
are	O
located	O
by	O
(	O
cid:12	O
)	O
nding	O
the	O
centre	O
of	O
mass	O
for	O
each	O
point	O
in	O
the	O
tour	O
,	O
using	O
the	O
neuron	B
activities	O
as	O
the	O
mass	O
function	B
.	O
(	O
b	O
)	O
the	O
travelling	O
scholar	O
problem	O
.	O
the	O
shortest	O
tour	O
linking	O
the	O
27	O
cambridge	O
colleges	O
,	O
the	O
engineering	O
department	O
,	O
the	O
university	O
library	O
,	O
and	O
sree	O
aiyer	O
’	O
s	O
house	O
.	O
from	O
aiyer	O
(	O
1991	O
)	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
tour	O
,	O
plus	O
a	O
constant	O
given	O
by	O
the	O
energy	B
associated	O
with	O
the	O
biases	O
.	O
now	O
,	O
since	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
minimizes	O
its	O
energy	B
,	O
it	O
is	O
hoped	O
that	O
the	O
binary	O
or	O
continuous	B
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
’	O
s	O
dynamics	O
will	O
take	O
the	O
state	O
to	O
a	O
minimum	O
that	O
is	O
a	O
valid	O
tour	O
and	O
which	O
might	O
be	O
an	O
optimal	B
tour	O
.	O
this	O
hope	O
is	O
not	O
ful	O
(	O
cid:12	O
)	O
lled	O
for	O
large	O
travelling	O
salesman	O
problems	O
,	O
however	O
,	O
without	O
some	O
careful	O
modi	O
(	O
cid:12	O
)	O
cations	O
.	O
we	O
have	O
not	O
speci	O
(	O
cid:12	O
)	O
ed	O
the	O
size	O
of	O
the	O
weights	O
that	O
enforce	O
the	O
tour	O
’	O
s	O
validity	O
,	O
relative	B
to	O
the	O
size	O
of	O
the	O
distance	O
weights	O
,	O
and	O
setting	O
this	O
scale	O
factor	O
poses	O
di	O
(	O
cid:14	O
)	O
culties	O
.	O
if	O
‘	O
large	O
’	O
validity-enforcing	O
weights	O
are	O
used	O
,	O
the	O
network	B
’	O
s	O
dynamics	O
will	O
rattle	O
into	O
a	O
valid	O
state	O
with	O
little	O
regard	O
for	O
the	O
distances	O
.	O
if	O
‘	O
small	O
’	O
validity-enforcing	O
weights	O
are	O
used	O
,	O
it	O
is	O
possible	O
that	O
the	O
distance	B
weights	O
will	O
cause	O
the	O
network	B
to	O
adopt	O
an	O
invalid	O
state	O
that	O
has	O
lower	O
energy	B
than	O
any	O
valid	O
state	O
.	O
our	O
original	O
formulation	O
of	O
the	O
energy	O
function	B
puts	O
the	O
objective	B
function	I
and	O
the	O
solution	O
’	O
s	O
validity	O
in	O
potential	O
con	O
(	O
cid:13	O
)	O
ict	O
with	O
each	O
other	O
.	O
this	O
di	O
(	O
cid:14	O
)	O
culty	O
has	O
been	O
resolved	O
by	O
the	O
work	O
of	O
sree	O
aiyer	O
(	O
1991	O
)	O
,	O
who	O
showed	O
how	O
to	O
modify	O
the	O
distance	B
weights	O
so	O
that	O
they	O
would	O
not	O
interfere	O
with	O
the	O
solution	O
’	O
s	O
validity	O
,	O
and	O
how	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
continuous	B
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
whose	O
dynamics	O
are	O
at	O
all	O
times	O
con	O
(	O
cid:12	O
)	O
ned	O
to	O
a	O
‘	O
valid	O
subspace	O
’	O
.	O
aiyer	O
used	O
a	O
graduated	B
non-convexity	I
or	O
deterministic	B
annealing	I
approach	O
to	O
(	O
cid:12	O
)	O
nd	O
good	B
solutions	O
using	O
these	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
.	O
the	O
deterministic	B
annealing	I
approach	O
involves	O
gradually	O
increasing	O
the	O
gain	B
(	O
cid:12	O
)	O
of	O
the	O
neurons	O
in	O
the	O
network	B
from	O
0	O
to	O
1	O
,	O
at	O
which	O
point	O
the	O
state	O
of	O
the	O
network	B
corresponds	O
to	O
a	O
valid	O
tour	O
.	O
a	O
sequence	B
of	O
trajectories	O
generated	O
by	O
applying	O
this	O
method	B
to	O
a	O
thirty-	O
city	O
travelling	B
salesman	I
problem	I
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
42.11a	O
.	O
a	O
solution	O
to	O
the	O
‘	O
travelling	O
scholar	O
problem	O
’	O
found	O
by	O
aiyer	O
using	O
a	O
con-	O
tinuous	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
42.11b	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
519	O
42.10	O
:	O
further	O
exercises	O
42.10	O
further	O
exercises	O
.	O
exercise	O
42.9	O
.	O
[	O
3	O
]	O
storing	O
two	O
memories	O
.	O
two	O
binary	O
memories	O
m	O
and	O
n	O
(	O
mi	O
;	O
ni	O
2	O
f	O
(	O
cid:0	O
)	O
1	O
;	O
+1g	O
)	O
are	O
stored	O
by	O
heb-	O
bian	O
learning	B
in	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
using	O
wij	O
=	O
(	O
cid:26	O
)	O
mimj	O
+	O
ninj	O
0	O
for	O
i	O
6=	O
j	O
for	O
i	O
=	O
j	O
.	O
(	O
42.31	O
)	O
the	O
biases	O
bi	O
are	O
set	B
to	O
zero	O
.	O
the	O
network	B
is	O
put	O
in	O
the	O
state	O
x	O
=	O
m.	O
evaluate	O
the	O
activation	O
ai	O
of	O
neuron	O
i	O
and	O
show	O
that	O
in	O
can	O
be	O
written	O
in	O
the	O
form	O
ai	O
=	O
(	O
cid:22	O
)	O
mi	O
+	O
(	O
cid:23	O
)	O
ni	O
:	O
(	O
42.32	O
)	O
by	O
comparing	O
the	O
signal	O
strength	O
,	O
(	O
cid:22	O
)	O
,	O
with	O
the	O
magnitude	O
of	O
the	O
noise	O
strength	O
,	O
j	O
(	O
cid:23	O
)	O
j	O
,	O
show	O
that	O
x	O
=	O
m	O
is	O
a	O
stable	O
state	O
of	O
the	O
dynamics	O
of	O
the	O
network	O
.	O
the	O
network	B
is	O
put	O
in	O
a	O
state	O
x	O
di	O
(	O
cid:11	O
)	O
ering	O
in	O
d	O
places	O
from	O
m	O
,	O
x	O
=	O
m	O
+	O
2d	O
;	O
(	O
42.33	O
)	O
where	O
the	O
perturbation	O
d	O
satis	O
(	O
cid:12	O
)	O
es	O
di	O
2	O
f	O
(	O
cid:0	O
)	O
1	O
;	O
0	O
;	O
+1g	O
.	O
d	O
is	O
the	O
number	O
of	O
components	O
of	O
d	O
that	O
are	O
non-zero	O
,	O
and	O
for	O
each	O
di	O
that	O
is	O
non-zero	O
,	O
di	O
=	O
(	O
cid:0	O
)	O
mi	O
.	O
de	O
(	O
cid:12	O
)	O
ning	O
the	O
overlap	O
between	O
m	O
and	O
n	O
to	O
be	O
omn	O
=	O
mini	O
;	O
i	O
xi=1	O
(	O
42.34	O
)	O
evaluate	O
the	O
activation	O
ai	O
of	O
neuron	O
i	O
again	O
and	O
show	O
that	O
the	O
dynamics	O
of	O
the	O
network	O
will	O
restore	O
x	O
to	O
m	O
if	O
the	O
number	O
of	O
(	O
cid:13	O
)	O
ipped	O
bits	O
satis	O
(	O
cid:12	O
)	O
es	O
d	O
<	O
1	O
4	O
(	O
i	O
(	O
cid:0	O
)	O
jomnj	O
(	O
cid:0	O
)	O
2	O
)	O
:	O
(	O
42.35	O
)	O
how	O
does	O
this	O
number	O
compare	O
with	O
the	O
maximum	O
number	O
of	O
(	O
cid:13	O
)	O
ipped	O
bits	O
that	O
can	O
be	O
corrected	O
by	O
the	O
optimal	B
decoder	I
,	O
assuming	O
the	O
vector	O
x	O
is	O
either	O
a	O
noisy	B
version	O
of	O
m	O
or	O
of	O
n	O
?	O
exercise	O
42.10	O
.	O
[	O
3	O
]	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
as	O
a	O
collection	O
of	O
binary	O
classi	O
(	O
cid:12	O
)	O
ers	O
.	O
this	O
ex-	O
ercise	O
explores	O
the	O
link	O
between	O
unsupervised	O
networks	O
and	O
supervised	O
networks	O
.	O
if	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
’	O
s	O
desired	O
memories	O
are	O
all	O
attracting	O
stable	O
states	O
,	O
then	O
every	O
neuron	B
in	O
the	O
network	B
has	O
weights	O
going	O
to	O
it	O
that	O
solve	O
a	O
classi	O
(	O
cid:12	O
)	O
cation	O
problem	O
personal	O
to	O
that	O
neuron	B
.	O
take	O
the	O
set	B
of	O
memories	O
and	O
write	O
them	O
in	O
the	O
form	O
x0	O
(	O
n	O
)	O
;	O
x	O
(	O
n	O
)	O
,	O
where	O
x0	O
denotes	O
all	O
the	O
components	O
xi0	O
for	O
all	O
i0	O
6=	O
i	O
,	O
and	O
let	O
w0	O
denote	O
the	O
vector	O
of	O
weights	O
wii0	O
,	O
for	O
i0	O
6=	O
i.	O
using	O
what	O
we	O
know	O
about	O
the	O
capacity	B
of	O
the	O
single	B
neuron	I
,	O
show	O
that	O
it	O
is	O
almost	O
certainly	O
impossible	O
to	O
store	O
more	O
than	O
2i	O
random	B
memories	O
in	O
a	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
of	O
i	O
neurons	O
.	O
i	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
520	O
lyapunov	O
functions	B
exercise	O
42.11	O
.	O
[	O
3	O
]	O
erik	O
’	O
s	O
puzzle	B
.	O
in	O
a	O
stripped-down	O
version	O
of	O
conway	O
’	O
s	O
game	B
of	O
life	B
,	O
cells	O
are	O
arranged	O
on	O
a	O
square	B
grid	O
.	O
each	O
cell	O
is	O
either	O
alive	O
or	O
dead	O
.	O
live	O
cells	O
do	O
not	O
die	B
.	O
dead	O
cells	O
become	O
alive	O
if	O
two	O
or	O
more	O
of	O
their	O
immediate	O
neighbours	O
are	O
alive	O
.	O
(	O
neighbours	O
to	O
north	O
,	O
south	O
,	O
east	O
and	O
west	O
.	O
)	O
what	O
is	O
the	O
smallest	O
number	O
of	O
live	O
cells	O
needed	O
in	O
order	O
that	O
these	O
rules	B
lead	O
to	O
an	O
entire	O
n	O
(	O
cid:2	O
)	O
n	O
square	B
being	O
alive	O
?	O
in	O
a	O
d-dimensional	O
version	O
of	O
the	O
same	O
game	B
,	O
the	O
rule	O
is	O
that	O
if	O
d	O
neigh-	O
bours	O
are	O
alive	O
then	O
you	O
come	O
to	O
life	B
.	O
what	O
is	O
the	O
smallest	O
number	O
of	O
live	O
cells	O
needed	O
in	O
order	O
that	O
an	O
entire	O
n	O
(	O
cid:2	O
)	O
n	O
(	O
cid:2	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:2	O
)	O
n	O
hypercube	O
becomes	O
alive	O
?	O
(	O
and	O
how	O
should	O
those	O
live	O
cells	O
be	O
arranged	O
?	O
)	O
42	O
|	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
!	O
!	O
figure	O
42.12.	O
erik	O
’	O
s	O
dynamics	O
.	O
the	O
southeast	B
puzzle	I
u	O
(	O
a	O
)	O
-	O
u	O
?	O
-	O
(	O
b	O
)	O
-u	O
u	O
?	O
-	O
(	O
c	O
)	O
-u	O
u	O
?	O
u	O
-	O
(	O
d	O
)	O
u	O
uu	O
u	O
-	O
:	O
:	O
:	O
-	O
(	O
z	O
)	O
eee	O
ee	O
e	O
eeee	O
the	O
southeast	B
puzzle	I
is	O
played	O
on	O
a	O
semi-in	O
(	O
cid:12	O
)	O
nite	O
chess	B
board	I
,	O
starting	O
at	O
its	O
northwest	O
(	O
top	O
left	O
)	O
corner	O
.	O
there	O
are	O
three	O
rules	O
:	O
figure	O
42.13.	O
the	O
southeast	B
puzzle	I
.	O
1.	O
in	O
the	O
starting	O
position	O
,	O
one	O
piece	O
is	O
placed	O
in	O
the	O
northwest-most	O
square	B
(	O
(	O
cid:12	O
)	O
gure	O
42.13a	O
)	O
.	O
2.	O
it	O
is	O
not	O
permitted	O
for	O
more	O
than	O
one	O
piece	O
to	O
be	O
on	O
any	O
given	O
square	B
.	O
3.	O
at	O
each	O
step	O
,	O
you	O
remove	O
one	O
piece	O
from	O
the	O
board	O
,	O
and	O
replace	O
it	O
with	O
two	O
pieces	O
,	O
one	O
in	O
the	O
square	B
immediately	O
to	O
the	O
east	O
,	O
and	O
one	O
in	O
the	O
the	O
square	B
immediately	O
to	O
the	O
south	O
,	O
as	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gure	O
42.13b	O
.	O
every	O
such	O
step	O
increases	O
the	O
number	O
of	O
pieces	O
on	O
the	O
board	O
by	O
one	O
.	O
after	O
move	O
(	O
b	O
)	O
has	O
been	O
made	O
,	O
either	O
piece	O
may	O
be	O
selected	O
for	O
the	O
next	O
move	O
.	O
figure	O
42.13c	O
shows	O
the	O
outcome	O
of	O
moving	O
the	O
lower	O
piece	O
.	O
at	O
the	O
next	O
move	O
,	O
either	O
the	O
lowest	O
piece	O
or	O
the	O
middle	O
piece	O
of	O
the	O
three	O
may	O
be	O
selected	O
;	O
the	O
uppermost	O
piece	O
may	O
not	O
be	O
selected	O
,	O
since	O
that	O
would	O
violate	O
rule	O
2.	O
at	O
move	O
(	O
d	O
)	O
we	O
have	O
selected	O
the	O
middle	O
piece	O
.	O
now	O
any	O
of	O
the	O
pieces	O
may	O
be	O
moved	O
,	O
except	O
for	O
the	O
leftmost	O
piece	O
.	O
now	O
,	O
here	O
is	O
the	O
puzzle	B
:	O
.	O
exercise	O
42.12	O
.	O
[	O
4	O
,	O
p.521	O
]	O
is	O
it	O
possible	O
to	O
obtain	O
a	O
position	O
in	O
which	O
all	O
the	O
ten	O
squares	O
closest	O
to	O
the	O
northwest	O
corner	O
,	O
marked	O
in	O
(	O
cid:12	O
)	O
gure	O
42.13z	O
,	O
are	O
empty	O
?	O
[	O
hint	O
:	O
this	O
puzzle	B
has	O
a	O
connection	O
to	O
data	B
compression	I
.	O
]	O
42.11	O
solutions	O
solution	O
to	O
exercise	O
42.3	O
(	O
p.508	O
)	O
.	O
take	O
a	O
binary	O
feedback	O
network	B
with	O
2	O
neu-	O
rons	O
and	O
let	O
w12	O
=	O
1	O
and	O
w21	O
=	O
(	O
cid:0	O
)	O
1.	O
then	O
whenever	O
neuron	B
1	O
is	O
updated	O
,	O
it	O
will	O
match	O
neuron	B
2	O
,	O
and	O
whenever	O
neuron	B
2	O
is	O
updated	O
,	O
it	O
will	O
(	O
cid:13	O
)	O
ip	O
to	O
the	O
opposite	O
state	O
from	O
neuron	B
1.	O
there	O
is	O
no	O
stable	O
state	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
42.11	O
:	O
solutions	O
521	O
solution	O
to	O
exercise	O
42.4	O
(	O
p.508	O
)	O
.	O
take	O
a	O
binary	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
with	O
2	O
neu-	O
rons	O
and	O
let	O
w12	O
=	O
w21	O
=	O
1	O
,	O
and	O
let	O
the	O
initial	O
condition	O
be	O
x1	O
=	O
1	O
,	O
x2	O
=	O
(	O
cid:0	O
)	O
1.	O
then	O
if	O
the	O
dynamics	O
are	O
synchronous	O
,	O
on	O
every	O
iteration	O
both	O
neurons	O
will	O
(	O
cid:13	O
)	O
ip	O
their	O
state	O
.	O
the	O
dynamics	O
do	O
not	O
converge	O
to	O
a	O
(	O
cid:12	O
)	O
xed	O
point	O
.	O
solution	O
to	O
exercise	O
42.12	O
(	O
p.520	O
)	O
.	O
the	O
key	O
to	O
this	O
problem	O
is	O
to	O
notice	O
its	O
similarity	O
to	O
the	O
construction	B
of	O
a	O
binary	O
symbol	O
code	B
.	O
starting	O
from	O
the	O
empty	B
string	I
,	O
we	O
can	O
build	O
a	O
binary	O
tree	O
by	O
repeatedly	O
splitting	O
a	O
codeword	B
into	O
two	O
.	O
every	O
codeword	B
has	O
an	O
implicit	O
probability	O
2	O
(	O
cid:0	O
)	O
l	O
,	O
where	O
l	O
is	O
the	O
depth	O
of	O
the	O
codeword	B
in	O
the	O
binary	O
tree	O
.	O
whenever	O
we	O
split	O
a	O
codeword	B
in	O
two	O
and	O
create	O
two	O
new	O
codewords	O
whose	O
length	B
is	O
increased	O
by	O
one	O
,	O
the	O
two	O
new	O
codewords	O
each	O
have	O
implicit	O
probability	O
equal	O
to	O
half	O
that	O
of	O
the	O
old	O
codeword	B
.	O
for	O
a	O
complete	O
binary	O
code	O
,	O
the	O
kraft	O
equality	O
a	O
(	O
cid:14	O
)	O
rms	O
that	O
the	O
sum	O
of	O
these	O
implicit	B
probabilities	I
is	O
1.	O
similarly	O
,	O
in	O
southeast	O
,	O
we	O
can	O
associate	O
a	O
‘	O
weight	B
’	O
with	O
each	O
piece	O
on	O
the	O
board	O
.	O
if	O
we	O
assign	O
a	O
weight	B
of	O
1	O
to	O
any	O
piece	O
sitting	O
on	O
the	O
top	O
left	O
square	B
;	O
a	O
weight	B
of	O
1/2	O
to	O
any	O
piece	O
on	O
a	O
square	B
whose	O
distance	B
from	O
the	O
top	O
left	O
is	O
one	O
;	O
a	O
weight	B
of	O
1/4	O
to	O
any	O
piece	O
whose	O
distance	B
from	O
the	O
top	O
left	O
is	O
two	O
;	O
and	O
so	O
forth	O
,	O
with	O
‘	O
distance	B
’	O
being	O
the	O
city-block	O
distance	B
;	O
then	O
every	O
legal	O
move	O
in	O
southeast	O
leaves	O
unchanged	O
the	O
total	O
weight	B
of	O
all	O
pieces	O
on	O
the	O
board	O
.	O
lyapunov	O
functions	B
come	O
in	O
two	O
(	O
cid:13	O
)	O
avours	O
:	O
the	O
function	B
may	O
be	O
a	O
function	B
of	O
state	O
whose	O
value	O
is	O
known	O
to	O
stay	O
constant	O
;	O
or	O
it	O
may	O
be	O
a	O
function	B
of	O
state	O
that	O
is	O
bounded	O
below	O
,	O
and	O
whose	O
value	O
always	O
decreases	O
or	O
stays	O
constant	O
.	O
the	O
total	O
weight	B
is	O
a	O
lyapunov	O
function	B
of	O
the	O
second	O
type	O
.	O
the	O
starting	O
weight	B
is	O
1	O
,	O
so	O
now	O
we	O
have	O
a	O
powerful	O
tool	O
:	O
a	O
conserved	O
function	B
of	O
the	O
state	O
.	O
is	O
it	O
possible	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
position	O
in	O
which	O
the	O
ten	O
highest-	O
weight	B
squares	O
are	O
vacant	O
,	O
and	O
the	O
total	O
weight	B
is	O
1	O
?	O
what	O
is	O
the	O
total	O
weight	B
if	O
all	O
the	O
other	O
squares	O
on	O
the	O
board	O
are	O
occupied	O
(	O
(	O
cid:12	O
)	O
gure	O
42.14	O
)	O
?	O
the	O
total	O
weight	B
would	O
be	O
p1l=4	O
(	O
l	O
+	O
1	O
)	O
2	O
(	O
cid:0	O
)	O
l	O
,	O
which	O
is	O
equal	O
to	O
3=4	O
.	O
so	O
it	O
is	O
impossible	O
to	O
empty	O
all	O
ten	O
of	O
those	O
squares	O
.	O
uuuuu	O
:	O
:	O
:	O
:	O
:	O
:	O
.	O
.	O
.	O
uu	O
uu	O
uuuuuu	O
...	O
...	O
figure	O
42.14.	O
a	O
possible	O
position	O
for	O
the	O
southeast	B
puzzle	I
?	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
43	O
boltzmann	O
machines	O
43.1	O
from	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
to	O
boltzmann	O
machines	O
we	O
have	O
noticed	O
that	O
the	O
binary	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
minimizes	O
an	O
energy	B
func-	O
tion	O
xtwx	O
(	O
43.1	O
)	O
e	O
(	O
x	O
)	O
=	O
(	O
cid:0	O
)	O
1	O
2	O
and	O
that	O
the	O
continuous	B
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
with	O
activation	B
function	I
xn	O
=	O
tanh	O
(	O
an	O
)	O
can	O
be	O
viewed	O
as	O
approximating	O
the	O
probability	B
distribution	O
asso-	O
ciated	O
with	O
that	O
energy	B
function	O
,	O
p	O
(	O
xj	O
w	O
)	O
=	O
1	O
z	O
(	O
w	O
)	O
exp	O
[	O
(	O
cid:0	O
)	O
e	O
(	O
x	O
)	O
]	O
=	O
1	O
z	O
(	O
w	O
)	O
exp	O
(	O
cid:20	O
)	O
1	O
2	O
xtwx	O
(	O
cid:21	O
)	O
:	O
(	O
43.2	O
)	O
these	O
observations	O
motivate	O
the	O
idea	O
of	O
working	O
with	O
a	O
neural	B
network	I
model	O
that	O
actually	O
implements	O
the	O
above	O
probability	B
distribution	O
.	O
the	O
stochastic	B
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
or	O
boltzmann	O
machine	O
(	O
hinton	O
and	O
se-	O
jnowski	O
,	O
1986	O
)	O
has	O
the	O
following	O
activity	B
rule	I
:	O
activity	B
rule	I
of	O
boltzmann	O
machine	O
:	O
after	O
computing	O
the	O
activa-	O
tion	O
ai	O
(	O
42.3	O
)	O
,	O
set	B
xi	O
=	O
+1	O
with	O
probability	O
else	O
set	B
xi	O
=	O
(	O
cid:0	O
)	O
1	O
:	O
1	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
2ai	O
(	O
43.3	O
)	O
this	O
rule	O
implements	O
gibbs	O
sampling	O
for	O
the	O
probability	B
distribution	O
(	O
43.2	O
)	O
.	O
boltzmann	O
machine	B
learning	I
given	O
a	O
set	B
of	O
examples	O
fx	O
(	O
n	O
)	O
gn	O
in	O
adjusting	O
the	O
weights	O
w	O
such	O
that	O
the	O
generative	B
model	I
1	O
from	O
the	O
real	O
world	O
,	O
we	O
might	O
be	O
interested	O
p	O
(	O
xj	O
w	O
)	O
=	O
1	O
z	O
(	O
w	O
)	O
exp	O
(	O
cid:20	O
)	O
1	O
2	O
xtwx	O
(	O
cid:21	O
)	O
(	O
43.4	O
)	O
is	O
well	O
matched	O
to	O
those	O
examples	O
.	O
we	O
can	O
derive	O
a	O
learning	B
algorithm	O
by	O
writing	O
down	O
bayes	O
’	O
theorem	B
to	O
obtain	O
the	O
posterior	B
probability	I
of	O
the	O
weights	O
given	O
the	O
data	O
:	O
p	O
(	O
w	O
jfx	O
(	O
n	O
)	O
gn	O
1	O
g	O
)	O
=	O
''	O
n	O
yn=1	O
p	O
(	O
x	O
(	O
n	O
)	O
j	O
w	O
)	O
#	O
p	O
(	O
w	O
)	O
p	O
(	O
fx	O
(	O
n	O
)	O
gn	O
1	O
g	O
)	O
:	O
522	O
(	O
43.5	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
43.1	O
:	O
from	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
to	O
boltzmann	O
machines	O
523	O
we	O
concentrate	O
on	O
the	O
(	O
cid:12	O
)	O
rst	O
term	O
in	O
the	O
numerator	O
,	O
the	O
likelihood	B
,	O
and	O
derive	O
a	O
maximum	B
likelihood	I
algorithm	O
(	O
though	O
there	O
might	O
be	O
advantages	O
in	O
pursuing	O
a	O
full	O
bayesian	O
approach	O
as	O
we	O
did	O
in	O
the	O
case	O
of	O
the	O
single	O
neuron	B
)	O
.	O
we	O
di	O
(	O
cid:11	O
)	O
erentiate	O
the	O
logarithm	O
of	O
the	O
likelihood	O
,	O
ln	O
''	O
n	O
yn=1	O
p	O
(	O
x	O
(	O
n	O
)	O
j	O
w	O
)	O
#	O
=	O
n	O
xn=1	O
(	O
cid:20	O
)	O
1	O
2	O
x	O
(	O
n	O
)	O
t	O
wx	O
(	O
n	O
)	O
(	O
cid:0	O
)	O
ln	O
z	O
(	O
w	O
)	O
(	O
cid:21	O
)	O
;	O
(	O
43.6	O
)	O
with	O
respect	O
to	O
wij	O
,	O
bearing	B
in	O
mind	O
that	O
w	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
to	O
be	O
symmetric	B
with	O
wji	O
=	O
wij	O
.	O
exercise	O
43.1	O
.	O
[	O
2	O
]	O
show	O
that	O
the	O
derivative	O
of	O
ln	O
z	O
(	O
w	O
)	O
with	O
respect	O
to	O
wij	O
is	O
@	O
@	O
wij	O
ln	O
z	O
(	O
w	O
)	O
=xx	O
xixjp	O
(	O
xj	O
w	O
)	O
=	O
hxixjip	O
(	O
xj	O
w	O
)	O
:	O
(	O
43.7	O
)	O
[	O
this	O
exercise	O
is	O
similar	O
to	O
exercise	O
22.12	O
(	O
p.307	O
)	O
.	O
]	O
the	O
derivative	O
of	O
the	O
log	O
likelihood	B
is	O
therefore	O
:	O
@	O
@	O
wij	O
ln	O
p	O
(	O
fx	O
(	O
n	O
)	O
gn	O
1	O
gj	O
w	O
)	O
=	O
n	O
i	O
x	O
(	O
n	O
)	O
xn=1hx	O
(	O
n	O
)	O
j	O
(	O
cid:0	O
)	O
hxixjip	O
(	O
x	O
j	O
w	O
)	O
i	O
=	O
nhhxixjidata	O
(	O
cid:0	O
)	O
hxixjip	O
(	O
xj	O
w	O
)	O
i	O
:	O
(	O
43.8	O
)	O
(	O
43.9	O
)	O
this	O
gradient	O
is	O
proportional	O
to	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
of	O
two	O
terms	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
term	O
is	O
the	O
empirical	O
correlation	O
between	O
xi	O
and	O
xj	O
,	O
hxixjidata	O
(	O
cid:17	O
)	O
1	O
n	O
n	O
xn=1hx	O
(	O
n	O
)	O
i	O
x	O
(	O
n	O
)	O
j	O
i	O
;	O
(	O
43.10	O
)	O
and	O
the	O
second	O
term	O
is	O
the	O
correlation	O
between	O
xi	O
and	O
xj	O
under	O
the	O
current	O
model	B
,	O
hxixjip	O
(	O
x	O
j	O
w	O
)	O
(	O
cid:17	O
)	O
xx	O
xixjp	O
(	O
xj	O
w	O
)	O
:	O
(	O
43.11	O
)	O
the	O
(	O
cid:12	O
)	O
rst	O
correlation	O
hxixjidata	O
is	O
readily	O
evaluated	O
{	O
it	O
is	O
just	O
the	O
empirical	O
correlation	O
between	O
the	O
activities	O
in	O
the	O
real	O
world	O
.	O
the	O
second	O
correlation	O
,	O
hxixjip	O
(	O
x	O
j	O
w	O
)	O
,	O
is	O
not	O
so	O
easy	O
to	O
evaluate	O
,	O
but	O
it	O
can	O
be	O
estimated	O
by	O
monte	O
carlo	O
methods	B
,	O
that	O
is	O
,	O
by	O
observing	O
the	O
average	B
value	O
of	O
xixj	O
while	O
the	O
ac-	O
tivity	O
rule	O
of	O
the	O
boltzmann	O
machine	O
,	O
equation	O
(	O
43.3	O
)	O
,	O
is	O
iterated	O
.	O
in	O
the	O
special	O
case	O
w	O
=	O
0	O
,	O
we	O
can	O
evaluate	O
the	O
gradient	O
exactly	O
because	O
,	O
by	O
symmetry	O
,	O
the	O
correlation	O
hxixjip	O
(	O
x	O
j	O
w	O
)	O
must	O
be	O
zero	O
.	O
if	O
the	O
weights	O
are	O
adjusted	O
by	O
gradient	O
descent	O
with	O
learning	O
rate	B
(	O
cid:17	O
)	O
,	O
then	O
,	O
after	O
one	O
iteration	O
,	O
the	O
weights	O
will	O
be	O
wij	O
=	O
(	O
cid:17	O
)	O
n	O
xn=1hx	O
(	O
n	O
)	O
i	O
x	O
(	O
n	O
)	O
j	O
i	O
;	O
(	O
43.12	O
)	O
precisely	O
the	O
value	O
of	O
the	O
weights	O
given	O
by	O
the	O
hebb	O
rule	O
,	O
equation	O
(	O
16.5	O
)	O
,	O
with	O
which	O
we	O
trained	O
the	O
hop	O
(	O
cid:12	O
)	O
eld	O
network	B
.	O
interpretation	O
of	O
boltzmann	O
machine	B
learning	I
one	O
way	O
of	O
viewing	O
the	O
two	O
terms	O
in	O
the	O
gradient	O
(	O
43.9	O
)	O
is	O
as	O
‘	O
waking	O
’	O
and	O
‘	O
sleeping	O
’	O
rules	B
.	O
while	O
the	O
network	B
is	O
‘	O
awake	O
’	O
,	O
it	O
measures	O
the	O
correlation	O
between	O
xi	O
and	O
xj	O
in	O
the	O
real	O
world	O
,	O
and	O
weights	O
are	O
increased	O
in	O
proportion	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
524	O
43	O
|	O
boltzmann	O
machines	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
43.1.	O
the	O
‘	O
shifter	O
’	O
ensembles	O
.	O
(	O
a	O
)	O
four	O
samples	O
from	O
the	O
plain	O
shifter	B
ensemble	I
.	O
(	O
b	O
)	O
four	O
corresponding	O
samples	O
from	O
the	O
labelled	O
shifter	B
ensemble	I
.	O
while	O
the	O
network	B
is	O
‘	O
asleep	O
’	O
,	O
it	O
‘	O
dreams	O
’	O
about	O
the	O
world	O
using	O
the	O
generative	B
model	I
(	O
43.4	O
)	O
,	O
and	O
measures	O
the	O
correlations	B
between	O
xi	O
and	O
xj	O
in	O
the	O
model	B
world	O
;	O
these	O
correlations	B
determine	O
a	O
proportional	O
decrease	O
in	O
the	O
weights	O
.	O
if	O
the	O
second-order	O
correlations	B
in	O
the	O
dream	B
world	O
match	O
the	O
correlations	B
in	O
the	O
real	O
world	O
,	O
then	O
the	O
two	O
terms	O
balance	B
and	O
the	O
weights	O
do	O
not	O
change	O
.	O
criticism	O
of	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
and	O
simple	O
boltzmann	O
machines	O
up	O
to	O
this	O
point	O
we	O
have	O
discussed	O
hop	O
(	O
cid:12	O
)	O
eld	O
networks	O
and	O
boltzmann	O
machines	O
in	O
which	O
all	O
of	O
the	O
neurons	O
correspond	O
to	O
visible	O
variables	O
xi	O
.	O
the	O
result	O
is	O
a	O
probabilistic	B
model	I
that	O
,	O
when	O
optimized	O
,	O
can	O
capture	O
the	O
second-order	O
statistics	O
of	O
the	O
environment	O
.	O
[	O
the	O
second-order	O
statistics	O
of	O
an	O
ensemble	B
p	O
(	O
x	O
)	O
are	O
the	O
expected	O
values	O
hxixji	O
of	O
all	O
the	O
pairwise	O
products	O
xixj	O
.	O
]	O
the	O
real	O
world	O
,	O
however	O
,	O
often	O
has	O
higher-order	O
correlations	B
that	O
must	O
be	O
included	O
if	O
our	O
description	O
of	O
it	O
is	O
to	O
be	O
e	O
(	O
cid:11	O
)	O
ective	O
.	O
often	O
the	O
second-order	O
correlations	B
in	O
themselves	O
may	O
carry	O
little	O
or	O
no	O
useful	O
information	B
.	O
consider	O
,	O
for	O
example	O
,	O
the	O
ensemble	B
of	O
binary	B
images	I
of	O
chairs	O
.	O
we	O
can	O
imagine	O
images	B
of	O
chairs	O
with	O
various	O
designs	O
{	O
four-legged	O
chairs	O
,	O
comfy	O
chairs	O
,	O
chairs	O
with	O
(	O
cid:12	O
)	O
ve	O
legs	O
and	O
wheels	O
,	O
wooden	O
chairs	O
,	O
cushioned	O
chairs	O
,	O
chairs	O
with	O
rockers	O
instead	O
of	O
legs	O
.	O
a	O
child	O
can	O
easily	O
learn	O
to	O
distinguish	O
these	O
images	B
from	O
images	B
of	O
carrots	O
and	O
parrots	O
.	O
but	O
i	O
expect	O
the	O
second-order	O
statistics	O
of	O
the	O
raw	O
data	O
are	O
useless	O
for	O
describing	O
the	O
ensemble	B
.	O
second-order	O
statistics	O
only	O
capture	O
whether	O
two	O
pixels	O
are	O
likely	O
to	O
be	O
in	O
the	O
same	O
state	O
as	O
each	O
other	O
.	O
higher-order	O
concepts	O
are	O
needed	O
to	O
make	O
a	O
good	B
generative	O
model	B
of	O
images	B
of	O
chairs	O
.	O
a	O
simpler	O
ensemble	B
of	O
images	B
in	O
which	O
high-order	B
statistics	O
are	O
important	O
is	O
the	O
‘	O
shifter	B
ensemble	I
’	O
,	O
which	O
comes	O
in	O
two	O
(	O
cid:13	O
)	O
avours	O
.	O
figure	O
43.1a	O
shows	O
a	O
few	O
samples	O
from	O
the	O
‘	O
plain	O
shifter	B
ensemble	I
’	O
.	O
in	O
each	O
image	B
,	O
the	O
bottom	O
eight	O
pixels	O
are	O
a	O
copy	O
of	O
the	O
top	O
eight	O
pixels	O
,	O
either	O
shifted	O
one	O
pixel	O
to	O
the	O
left	O
,	O
or	O
unshifted	O
,	O
or	O
shifted	O
one	O
pixel	O
to	O
the	O
right	O
.	O
(	O
the	O
top	O
eight	O
pixels	O
are	O
set	B
at	O
random	B
.	O
)	O
this	O
ensemble	B
is	O
a	O
simple	O
model	O
of	O
the	O
visual	O
signals	O
from	O
the	O
two	O
eyes	O
arriving	O
at	O
early	O
levels	O
of	O
the	O
brain	O
.	O
the	O
signals	O
from	O
the	O
two	O
eyes	O
are	O
similar	O
to	O
each	O
other	O
but	O
may	O
di	O
(	O
cid:11	O
)	O
er	O
by	O
small	O
translations	O
because	O
of	O
the	O
varying	O
depth	O
of	O
the	O
visual	O
world	O
.	O
this	O
ensemble	B
is	O
simple	O
to	O
describe	O
,	O
but	O
its	O
second-order	O
statistics	O
convey	O
no	O
useful	O
information	B
.	O
the	O
correlation	O
between	O
one	O
pixel	O
and	O
any	O
of	O
the	O
three	O
pixels	O
above	O
it	O
is	O
1=3	O
.	O
the	O
correlation	O
between	O
any	O
other	O
two	O
pixels	O
is	O
zero	O
.	O
figure	O
43.1b	O
shows	O
a	O
few	O
samples	O
from	O
the	O
‘	O
labelled	O
shifter	B
ensemble	I
’	O
.	O
here	O
,	O
the	O
problem	O
has	O
been	O
made	O
easier	O
by	O
including	O
an	O
extra	O
three	O
neu-	O
rons	O
that	O
label	O
the	O
visual	O
image	B
as	O
being	O
an	O
instance	O
of	O
either	O
the	O
‘	O
shift	O
left	O
’	O
,	O
‘	O
no	O
shift	O
’	O
,	O
or	O
‘	O
shift	O
right	O
’	O
sub-ensemble	O
.	O
but	O
with	O
this	O
extra	O
information	O
,	O
the	O
ensemble	B
is	O
still	O
not	O
learnable	O
using	O
second-order	O
statistics	O
alone	O
.	O
the	O
second-	O
order	O
correlation	O
between	O
any	O
label	O
neuron	B
and	O
any	O
image	B
neuron	O
is	O
zero	O
.	O
we	O
need	O
models	O
that	O
can	O
capture	O
higher-order	O
statistics	O
of	O
an	O
environment	O
.	O
so	O
,	O
how	O
can	O
we	O
develop	O
such	O
models	O
?	O
one	O
idea	O
might	O
be	O
to	O
create	O
models	O
that	O
directly	O
capture	O
higher-order	O
correlations	B
,	O
such	O
as	O
:	O
p	O
0	O
(	O
xj	O
w	O
;	O
v	O
;	O
:	O
:	O
:	O
)	O
=	O
1	O
z0	O
exp0	O
@	O
1	O
2xij	O
wijxixj	O
+	O
1	O
6xij	O
vijkxixjxk	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
1	O
a	O
:	O
(	O
43.13	O
)	O
such	O
higher-order	O
boltzmann	O
machines	O
are	O
equally	O
easy	O
to	O
simulate	O
using	O
stochastic	B
updates	O
,	O
and	O
the	O
learning	B
rule	I
for	O
the	O
higher-order	O
parameters	B
vijk	O
is	O
equivalent	O
to	O
the	O
learning	B
rule	I
for	O
wij	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
43.2	O
:	O
boltzmann	O
machine	O
with	O
hidden	O
units	O
525	O
.	O
exercise	O
43.2	O
.	O
[	O
2	O
]	O
derive	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	B
with	O
respect	O
to	O
vijk	O
.	O
it	O
is	O
possible	O
that	O
the	O
spines	B
found	O
on	O
biological	O
neurons	O
are	O
responsible	O
for	O
detecting	O
correlations	B
between	O
small	O
numbers	O
of	O
incoming	O
signals	O
.	O
however	O
,	O
to	O
capture	O
statistics	O
of	O
high	O
enough	O
order	O
to	O
describe	O
the	O
ensemble	B
of	O
images	B
of	O
chairs	O
well	O
would	O
require	O
an	O
unimaginable	O
number	O
of	O
terms	O
.	O
to	O
capture	O
merely	O
the	O
fourth-order	O
statistics	O
in	O
a	O
128	O
(	O
cid:2	O
)	O
128	O
pixel	O
image	B
,	O
we	O
need	O
more	O
than	O
107	O
parameters	B
.	O
so	O
measuring	O
moments	O
of	O
images	O
is	O
not	O
a	O
good	B
way	O
to	O
describe	O
their	O
un-	O
derlying	O
structure	O
.	O
perhaps	O
what	O
we	O
need	O
instead	O
or	O
in	O
addition	O
are	O
hidden	O
variables	O
,	O
also	O
known	O
to	O
statisticians	O
as	O
latent	O
variables	O
.	O
this	O
is	O
the	O
important	O
innovation	O
introduced	O
by	O
hinton	O
and	O
sejnowski	O
(	O
1986	O
)	O
.	O
the	O
idea	O
is	O
that	O
the	O
high-order	B
correlations	O
among	O
the	O
visible	O
variables	O
are	O
described	O
by	O
includ-	O
ing	O
extra	O
hidden	O
variables	O
and	O
sticking	O
to	O
a	O
model	B
that	O
has	O
only	O
second-order	O
interactions	O
between	O
its	O
variables	O
;	O
the	O
hidden	O
variables	O
induce	O
higher-order	O
correlations	B
between	O
the	O
visible	O
variables	O
.	O
43.2	O
boltzmann	O
machine	O
with	O
hidden	O
units	O
we	O
now	O
add	O
hidden	B
neurons	I
to	O
our	O
stochastic	B
model	O
.	O
these	O
are	O
neurons	O
that	O
do	O
not	O
correspond	O
to	O
observed	O
variables	O
;	O
they	O
are	O
free	O
to	O
play	O
any	O
role	O
in	O
the	O
probabilistic	B
model	I
de	O
(	O
cid:12	O
)	O
ned	O
by	O
equation	O
(	O
43.4	O
)	O
.	O
they	O
might	O
actually	O
take	O
on	O
interpretable	O
roles	O
,	O
e	O
(	O
cid:11	O
)	O
ectively	O
performing	O
‘	O
feature	O
extraction	O
’	O
.	O
learning	B
in	O
boltzmann	O
machines	O
with	O
hidden	O
units	B
the	O
activity	B
rule	I
of	O
a	O
boltzmann	O
machine	O
with	O
hidden	O
units	O
is	O
identical	O
to	O
that	O
of	O
the	O
original	O
boltzmann	O
machine	O
.	O
the	O
learning	B
rule	I
can	O
again	O
be	O
derived	O
by	O
maximum	O
likelihood	B
,	O
but	O
now	O
we	O
need	O
to	O
take	O
into	O
account	O
the	O
fact	O
that	O
the	O
states	O
of	O
the	O
hidden	O
units	B
are	O
unknown	O
.	O
we	O
will	O
denote	O
the	O
states	O
of	O
the	O
visible	O
units	B
by	O
x	O
,	O
the	O
states	O
of	O
the	O
hidden	O
units	B
by	O
h	O
,	O
and	O
the	O
generic	O
state	O
of	O
a	O
neuron	B
(	O
either	O
visible	O
or	O
hidden	O
)	O
by	O
yi	O
,	O
with	O
y	O
(	O
cid:17	O
)	O
(	O
x	O
;	O
h	O
)	O
.	O
the	O
state	O
of	O
the	O
network	B
when	O
the	O
visible	O
neurons	O
are	O
clamped	O
in	O
state	O
x	O
(	O
n	O
)	O
is	O
y	O
(	O
n	O
)	O
(	O
cid:17	O
)	O
(	O
x	O
(	O
n	O
)	O
;	O
h	O
)	O
.	O
the	O
likelihood	B
of	O
w	O
given	O
a	O
single	O
data	O
example	O
x	O
(	O
n	O
)	O
is	O
p	O
(	O
x	O
(	O
n	O
)	O
j	O
w	O
)	O
=xh	O
where	O
p	O
(	O
x	O
(	O
n	O
)	O
;	O
hj	O
w	O
)	O
=xh	O
exp	O
(	O
cid:20	O
)	O
1	O
z	O
(	O
w	O
)	O
=xx	O
;	O
h	O
2	O
ytwy	O
(	O
cid:21	O
)	O
:	O
equation	O
(	O
43.14	O
)	O
may	O
also	O
be	O
written	O
1	O
z	O
(	O
w	O
)	O
exp	O
(	O
cid:20	O
)	O
1	O
2	O
[	O
y	O
(	O
n	O
)	O
]	O
twy	O
(	O
n	O
)	O
(	O
cid:21	O
)	O
;	O
(	O
43.14	O
)	O
(	O
43.15	O
)	O
(	O
43.16	O
)	O
(	O
43.17	O
)	O
where	O
p	O
(	O
x	O
(	O
n	O
)	O
j	O
w	O
)	O
=	O
exp	O
(	O
cid:20	O
)	O
1	O
2	O
zx	O
(	O
n	O
)	O
(	O
w	O
)	O
=xh	O
zx	O
(	O
n	O
)	O
(	O
w	O
)	O
z	O
(	O
w	O
)	O
[	O
y	O
(	O
n	O
)	O
]	O
twy	O
(	O
n	O
)	O
(	O
cid:21	O
)	O
:	O
di	O
(	O
cid:11	O
)	O
erentiating	O
the	O
likelihood	B
as	O
before	O
,	O
we	O
(	O
cid:12	O
)	O
nd	O
that	O
the	O
derivative	O
with	O
re-	O
spect	O
to	O
any	O
weight	B
wij	O
is	O
again	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
a	O
‘	O
waking	O
’	O
term	O
and	O
a	O
‘	O
sleeping	O
’	O
term	O
,	O
@	O
@	O
wij	O
ln	O
p	O
(	O
fx	O
(	O
n	O
)	O
gn	O
1	O
j	O
w	O
)	O
=xn	O
nhyiyjip	O
(	O
h	O
j	O
x	O
(	O
n	O
)	O
;	O
w	O
)	O
(	O
cid:0	O
)	O
hyiyjip	O
(	O
x	O
;	O
hj	O
w	O
)	O
o	O
:	O
(	O
43.18	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
526	O
43	O
|	O
boltzmann	O
machines	O
the	O
(	O
cid:12	O
)	O
rst	O
term	O
hyiyjip	O
(	O
h	O
j	O
x	O
(	O
n	O
)	O
;	O
w	O
)	O
is	O
the	O
correlation	O
between	O
yi	O
and	O
yj	O
if	O
the	O
boltzmann	O
machine	O
is	O
simulated	O
with	O
the	O
visible	O
variables	O
clamped	O
to	O
x	O
(	O
n	O
)	O
and	O
the	O
hidden	O
variables	O
freely	O
sampling	O
from	O
their	O
conditional	B
distribution	O
.	O
the	O
second	O
term	O
hyiyjip	O
(	O
x	O
;	O
hj	O
w	O
)	O
is	O
the	O
correlation	O
between	O
yi	O
and	O
yj	O
when	O
hinton	O
and	O
sejnowski	O
demonstrated	O
that	O
non-trivial	O
ensembles	O
such	O
as	O
the	O
labelled	O
shifter	B
ensemble	I
can	O
be	O
learned	O
using	O
a	O
boltzmann	O
machine	O
with	O
hidden	O
units	O
.	O
the	O
hidden	O
units	O
take	O
on	O
the	O
role	O
of	O
feature	O
detectors	O
that	O
spot	O
patterns	O
likely	O
to	O
be	O
associated	O
with	O
one	O
of	O
the	O
three	O
shifts	O
.	O
the	O
boltzmann	O
machine	O
generates	O
samples	O
from	O
its	O
model	B
distribution	O
.	O
the	O
boltzmann	O
machine	O
is	O
time-consuming	O
to	O
simulate	O
because	O
the	O
compu-	O
tation	O
of	O
the	O
gradient	O
of	O
the	O
log	O
likelihood	B
depends	O
on	O
taking	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
of	O
two	O
gradients	O
,	O
both	O
found	O
by	O
monte	O
carlo	O
methods	B
.	O
so	O
boltzmann	O
machines	O
are	O
not	O
in	O
widespread	O
use	O
.	O
it	O
is	O
an	O
area	O
of	O
active	O
research	O
to	O
create	O
models	O
that	O
embody	O
the	O
same	O
capabilities	O
using	O
more	O
e	O
(	O
cid:14	O
)	O
cient	O
computations	O
(	O
hinton	O
et	O
al.	O
,	O
1995	O
;	O
dayan	O
et	O
al.	O
,	O
1995	O
;	O
hinton	O
and	O
ghahramani	O
,	O
1997	O
;	O
hinton	O
,	O
2001	O
;	O
hinton	O
and	O
teh	O
,	O
2001	O
)	O
.	O
43.3	O
exercise	O
.	O
exercise	O
43.3	O
.	O
[	O
3	O
]	O
can	O
the	O
‘	O
bars	O
and	O
stripes	O
’	O
ensemble	B
(	O
(	O
cid:12	O
)	O
gure	O
43.2	O
)	O
be	O
learned	O
by	O
a	O
boltzmann	O
machine	O
with	O
no	O
hidden	O
units	O
?	O
[	O
you	O
may	O
be	O
surprised	O
!	O
]	O
figure	O
43.2.	O
four	O
samples	O
from	O
the	O
‘	O
bars	O
and	O
stripes	O
’	O
ensemble	B
.	O
each	O
sample	B
is	O
generated	O
by	O
(	O
cid:12	O
)	O
rst	O
picking	O
an	O
orientation	O
,	O
horizontal	O
or	O
vertical	O
;	O
then	O
,	O
for	O
each	O
row	O
of	O
spins	O
in	O
that	O
orientation	O
(	O
each	O
bar	O
or	O
stripe	O
respectively	O
)	O
,	O
switching	O
all	O
spins	O
on	O
with	O
probability	B
1/2	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
44	O
supervised	O
learning	O
in	O
multilayer	O
networks	O
44.1	O
multilayer	O
perceptrons	O
no	O
course	O
on	O
neural	O
networks	O
could	O
be	O
complete	O
without	O
a	O
discussion	O
of	O
su-	O
pervised	O
multilayer	O
networks	O
,	O
also	O
known	O
as	O
backpropagation	O
networks	O
.	O
the	O
multilayer	B
perceptron	I
is	O
a	O
feedforward	O
network	B
.	O
it	O
has	O
input	O
neurons	O
,	O
hidden	B
neurons	I
and	O
output	O
neurons	O
.	O
the	O
hidden	B
neurons	I
may	O
be	O
arranged	O
in	O
a	O
sequence	B
of	O
layers	O
.	O
the	O
most	O
common	O
multilayer	O
perceptrons	O
have	O
a	O
single	O
hidden	O
layer	O
,	O
and	O
are	O
known	O
as	O
‘	O
two-layer	O
’	O
networks	O
,	O
the	O
number	O
‘	O
two	O
’	O
counting	B
the	O
number	O
of	O
layers	O
of	O
neurons	O
not	O
including	O
the	O
inputs	O
.	O
such	O
a	O
feedforward	O
network	B
de	O
(	O
cid:12	O
)	O
nes	O
a	O
nonlinear	B
parameterized	O
mapping	B
from	O
an	O
input	O
x	O
to	O
an	O
output	O
y	O
=	O
y	O
(	O
x	O
;	O
w	O
;	O
a	O
)	O
.	O
the	O
output	O
is	O
a	O
continuous	B
function	O
of	O
the	O
input	O
and	O
of	O
the	O
parameters	B
w	O
;	O
the	O
architecture	B
of	O
the	O
net	O
,	O
i.e.	O
,	O
the	O
functional	O
form	O
of	O
the	O
mapping	O
,	O
is	O
denoted	O
by	O
a.	O
feedforward	O
networks	O
can	O
be	O
‘	O
trained	O
’	O
to	O
perform	O
regression	B
and	O
classi	O
(	O
cid:12	O
)	O
cation	O
tasks	O
.	O
regression	B
networks	O
in	O
the	O
case	O
of	O
a	O
regression	B
problem	O
,	O
the	O
mapping	B
for	O
a	O
network	B
with	O
one	O
hidden	O
layer	O
may	O
have	O
the	O
form	O
:	O
hidden	O
layer	O
:	O
output	O
layer	O
:	O
a	O
(	O
1	O
)	O
j	O
=xl	O
i	O
=xj	O
a	O
(	O
2	O
)	O
jl	O
xl	O
+	O
(	O
cid:18	O
)	O
(	O
1	O
)	O
w	O
(	O
1	O
)	O
j	O
;	O
hj	O
=	O
f	O
(	O
1	O
)	O
(	O
a	O
(	O
1	O
)	O
j	O
)	O
(	O
44.1	O
)	O
w	O
(	O
2	O
)	O
ij	O
hj	O
+	O
(	O
cid:18	O
)	O
(	O
2	O
)	O
i	O
;	O
yi	O
=	O
f	O
(	O
2	O
)	O
(	O
a	O
(	O
2	O
)	O
i	O
)	O
(	O
44.2	O
)	O
where	O
,	O
for	O
example	O
,	O
f	O
(	O
1	O
)	O
(	O
a	O
)	O
=	O
tanh	O
(	O
a	O
)	O
,	O
and	O
f	O
(	O
2	O
)	O
(	O
a	O
)	O
=	O
a.	O
here	O
l	O
runs	O
over	O
the	O
inputs	O
x1	O
;	O
:	O
:	O
:	O
;	O
xl	O
,	O
j	O
runs	O
over	O
the	O
hidden	O
units	O
,	O
and	O
i	O
runs	O
over	O
the	O
out-	O
puts	O
.	O
the	O
‘	O
weights	O
’	O
w	O
and	O
‘	O
biases	O
’	O
(	O
cid:18	O
)	O
together	O
make	O
up	O
the	O
parameter	O
vector	O
w.	O
the	O
nonlinear	B
sigmoid	O
function	B
f	O
(	O
1	O
)	O
at	O
the	O
hidden	O
layer	O
gives	O
the	O
neu-	O
ral	O
network	B
greater	O
computational	O
(	O
cid:13	O
)	O
exibility	O
than	O
a	O
standard	O
linear	O
regression	B
model	O
.	O
graphically	O
,	O
we	O
can	O
represent	O
the	O
neural	B
network	I
as	O
a	O
set	B
of	O
layers	O
of	O
connected	O
neurons	O
(	O
(	O
cid:12	O
)	O
gure	O
44.1	O
)	O
.	O
what	O
sorts	O
of	O
functions	O
can	O
these	O
networks	O
implement	O
?	O
just	O
as	O
we	O
explored	O
the	O
weight	B
space	I
of	O
the	O
single	B
neuron	I
in	O
chapter	O
39	O
,	O
examining	O
the	O
functions	B
it	O
could	O
produce	O
,	O
let	O
us	O
explore	B
the	O
weight	B
space	I
of	O
a	O
multilayer	O
network	O
.	O
in	O
(	O
cid:12	O
)	O
gures	O
44.2	O
and	O
44.3	O
i	O
take	O
a	O
network	B
with	O
one	O
input	O
and	O
one	O
output	O
and	O
a	O
large	O
number	O
h	O
of	O
hidden	O
units	B
,	O
set	B
the	O
biases	O
527	O
outputs	O
hiddens	O
inputs	O
figure	O
44.1.	O
a	O
typical	B
two-layer	O
network	B
,	O
with	O
six	O
inputs	O
,	O
seven	O
hidden	O
units	O
,	O
and	O
three	O
outputs	O
.	O
each	O
line	O
represents	O
one	O
weight	B
.	O
0.4	O
0.2	O
0	O
-0.2	O
-0.4	O
-0.6	O
-0.8	O
-1	O
-1.2	O
-1.4	O
-2	O
-1	O
0	O
1	O
2	O
3	O
4	O
5	O
figure	O
44.2.	O
samples	O
from	O
the	O
prior	B
over	O
functions	B
of	O
a	O
one-input	O
network	B
.	O
for	O
each	O
of	O
a	O
sequence	B
of	O
values	O
of	O
(	O
cid:27	O
)	O
bias	B
=	O
8	O
,	O
6	O
,	O
4	O
,	O
3	O
,	O
2	O
,	O
1.6	O
,	O
1.2	O
,	O
0.8	O
,	O
0.4	O
,	O
0.3	O
,	O
0.2	O
,	O
and	O
(	O
cid:27	O
)	O
in	O
=	O
5	O
(	O
cid:27	O
)	O
w	O
is	O
shown	O
.	O
the	O
other	O
hyperparameters	O
of	O
the	O
network	O
were	O
h	O
=	O
400	O
,	O
(	O
cid:27	O
)	O
w	O
bias	B
,	O
one	O
random	B
function	O
out	O
=	O
0:05	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
528	O
(	O
cid:27	O
)	O
bias	B
output	O
(	O
cid:27	O
)	O
(	O
cid:27	O
)	O
out	O
(	O
cid:0	O
)	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
@	O
ty	O
ttttt	O
t	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
tx	O
t1	O
@	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:27	O
)	O
(	O
cid:27	O
)	O
in	O
@	O
@	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
@	O
-	O
(	O
cid:0	O
)	O
@	O
hidden	O
layer	O
input	O
44	O
|	O
supervised	O
learning	O
in	O
multilayer	O
networks	O
10	O
5	O
0	O
-5	O
-10	O
t	O
u	O
p	O
t	O
u	O
o	O
ph	O
(	O
cid:27	O
)	O
out	O
(	O
cid:24	O
)	O
1=	O
(	O
cid:27	O
)	O
in	O
(	O
cid:24	O
)	O
(	O
cid:27	O
)	O
bias=	O
(	O
cid:27	O
)	O
in	O
-2	O
-1	O
0	O
1	O
input	O
2	O
3	O
4	O
figure	O
44.3.	O
properties	O
of	O
a	O
function	B
produced	O
by	O
a	O
random	B
network	O
.	O
the	O
vertical	O
scale	O
of	O
a	O
typical	B
function	O
produced	O
by	O
the	O
network	B
with	O
random	B
weights	O
is	O
of	O
order	O
ph	O
(	O
cid:27	O
)	O
out	O
;	O
the	O
horizontal	O
range	O
in	O
which	O
the	O
function	B
varies	O
signi	O
(	O
cid:12	O
)	O
cantly	O
is	O
of	O
order	O
(	O
cid:27	O
)	O
bias=	O
(	O
cid:27	O
)	O
in	O
;	O
and	O
the	O
shortest	O
horizontal	O
length	B
scale	O
is	O
of	O
order	O
1=	O
(	O
cid:27	O
)	O
in	O
.	O
the	O
function	B
shown	O
was	O
produced	O
by	O
making	O
a	O
random	B
network	O
with	O
h	O
=	O
400	O
hidden	O
units	O
,	O
and	O
gaussian	O
weights	O
with	O
(	O
cid:27	O
)	O
bias	B
=	O
4	O
,	O
(	O
cid:27	O
)	O
in	O
=	O
8	O
,	O
and	O
(	O
cid:27	O
)	O
out	O
=	O
0:5.	O
i	O
j	O
jl	O
,	O
(	O
cid:18	O
)	O
(	O
2	O
)	O
and	O
w	O
(	O
2	O
)	O
ij	O
to	O
random	B
values	O
,	O
and	O
plot	O
the	O
resulting	O
,	O
w	O
(	O
1	O
)	O
i	O
set	B
the	O
hidden	O
units	O
’	O
biases	O
(	O
cid:18	O
)	O
(	O
1	O
)	O
and	O
weights	O
(	O
cid:18	O
)	O
(	O
1	O
)	O
j	O
function	B
y	O
(	O
x	O
)	O
.	O
to	O
random	B
values	O
from	O
a	O
gaussian	O
with	O
zero	O
mean	B
and	O
standard	B
deviation	I
(	O
cid:27	O
)	O
bias	B
;	O
the	O
input-to-hidden	O
weights	O
w	O
(	O
1	O
)	O
to	O
random	B
values	O
with	O
standard	O
deviation	O
(	O
cid:27	O
)	O
in	O
;	O
and	O
the	O
bias	B
and	O
jl	O
output	O
weights	O
(	O
cid:18	O
)	O
(	O
2	O
)	O
to	O
random	B
values	O
with	O
standard	O
deviation	O
(	O
cid:27	O
)	O
out	O
.	O
the	O
sort	O
of	O
functions	O
that	O
we	O
obtain	O
depend	O
on	O
the	O
values	O
of	O
(	O
cid:27	O
)	O
bias	B
,	O
(	O
cid:27	O
)	O
in	O
and	O
(	O
cid:27	O
)	O
out	O
.	O
as	O
the	O
weights	O
and	O
biases	O
are	O
made	O
bigger	O
we	O
obtain	O
more	O
complex	B
functions	O
with	O
more	O
features	O
and	O
a	O
greater	O
sensitivity	O
to	O
the	O
input	O
variable	O
.	O
the	O
vertical	O
scale	O
of	O
a	O
typical	B
function	O
produced	O
by	O
the	O
network	B
with	O
random	B
weights	O
is	O
of	O
order	O
ph	O
(	O
cid:27	O
)	O
out	O
;	O
the	O
horizontal	O
range	O
in	O
which	O
the	O
function	B
varies	O
signi	O
(	O
cid:12	O
)	O
cantly	O
is	O
of	O
order	O
(	O
cid:27	O
)	O
bias=	O
(	O
cid:27	O
)	O
in	O
;	O
and	O
the	O
shortest	O
horizontal	O
length	B
scale	O
is	O
of	O
order	O
1=	O
(	O
cid:27	O
)	O
in	O
.	O
and	O
w	O
(	O
2	O
)	O
ij	O
i	O
radford	O
neal	O
(	O
1996	O
)	O
has	O
also	O
shown	O
that	O
in	O
the	O
limit	O
as	O
h	O
!	O
1	O
the	O
statistical	B
properties	O
of	O
the	O
functions	O
generated	O
by	O
randomizing	O
the	O
weights	O
are	O
independent	O
of	O
the	O
number	O
of	O
hidden	O
units	B
;	O
so	O
,	O
interestingly	O
,	O
the	O
complexity	B
of	O
the	O
functions	B
becomes	O
independent	O
of	O
the	O
number	O
of	O
parameters	O
in	O
the	O
model	B
.	O
what	O
determines	O
the	O
complexity	B
of	O
the	O
typical	B
functions	O
is	O
the	O
characteristic	O
magnitude	O
of	O
the	O
weights	O
.	O
thus	O
we	O
anticipate	O
that	O
when	O
we	O
(	O
cid:12	O
)	O
t	O
these	O
models	O
to	O
real	O
data	O
,	O
an	O
important	O
way	O
of	O
controlling	O
the	O
complexity	B
of	O
the	O
(	O
cid:12	O
)	O
tted	O
function	B
will	O
be	O
to	O
control	O
the	O
characteristic	O
magnitude	O
of	O
the	O
weights	O
.	O
figure	O
44.4	O
shows	O
one	O
typical	B
function	O
produced	O
by	O
a	O
network	B
with	O
two	O
inputs	O
and	O
one	O
output	O
.	O
this	O
should	O
be	O
contrasted	O
with	O
the	O
function	B
produced	O
by	O
a	O
traditional	O
linear	B
regression	I
model	O
,	O
which	O
is	O
a	O
(	O
cid:13	O
)	O
at	O
plane	O
.	O
neural	O
networks	O
can	O
create	O
functions	B
with	O
more	O
complexity	B
than	O
a	O
linear	B
regression	I
.	O
44.2	O
how	O
a	O
regression	B
network	O
is	O
traditionally	O
trained	O
this	O
network	B
is	O
trained	O
using	O
a	O
data	B
set	I
d	O
=	O
fx	O
(	O
n	O
)	O
;	O
t	O
(	O
n	O
)	O
g	O
by	O
adjusting	O
w	O
so	O
as	O
to	O
minimize	O
an	O
error	B
function	I
,	O
e.g.	O
,	O
ed	O
(	O
w	O
)	O
=	O
1	O
2xn	O
xi	O
(	O
cid:16	O
)	O
t	O
(	O
n	O
)	O
i	O
(	O
cid:0	O
)	O
yi	O
(	O
x	O
(	O
n	O
)	O
;	O
w	O
)	O
(	O
cid:17	O
)	O
2	O
:	O
(	O
44.3	O
)	O
this	O
objective	B
function	I
is	O
a	O
sum	O
of	O
terms	O
,	O
one	O
for	O
each	O
input/target	O
pair	O
fx	O
;	O
tg	O
,	O
measuring	O
how	O
close	O
the	O
output	O
y	O
(	O
x	O
;	O
w	O
)	O
is	O
to	O
the	O
target	O
t.	O
this	O
minimization	B
is	O
based	O
on	O
repeated	O
evaluation	O
of	O
the	O
gradient	O
of	O
ed	O
.	O
this	O
gradient	O
can	O
be	O
e	O
(	O
cid:14	O
)	O
ciently	O
computed	O
using	O
the	O
backpropagation	B
algorithm	O
(	O
rumelhart	O
et	O
al.	O
,	O
1986	O
)	O
,	O
which	O
uses	O
the	O
chain	B
rule	I
to	O
(	O
cid:12	O
)	O
nd	O
the	O
derivatives	O
.	O
1	O
0	O
-1	O
-2	O
-1	O
-0.5	O
0	O
0.5	O
1	O
-1	O
1	O
0.5	O
0	O
-0.5	O
figure	O
44.4.	O
one	O
sample	B
from	I
the	O
prior	B
of	O
a	O
two-input	O
network	B
with	O
fh	O
;	O
(	O
cid:27	O
)	O
w	O
outg	O
=	O
f400	O
;	O
8:0	O
;	O
8:0	O
;	O
0:05g	O
.	O
bias	B
;	O
(	O
cid:27	O
)	O
w	O
in	O
;	O
(	O
cid:27	O
)	O
w	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
44.3	O
:	O
neural	B
network	I
learning	O
as	B
inference	I
529	O
often	O
,	O
regularization	B
(	O
also	O
known	O
as	O
weight	O
decay	O
)	O
is	O
included	O
,	O
modifying	O
the	O
objective	B
function	I
to	O
:	O
m	O
(	O
w	O
)	O
=	O
(	O
cid:12	O
)	O
ed	O
+	O
(	O
cid:11	O
)	O
ew	O
(	O
44.4	O
)	O
where	O
,	O
for	O
example	O
,	O
ew	O
=	O
1	O
i	O
.	O
this	O
additional	O
term	O
favours	O
small	O
values	O
of	O
w	O
and	O
decreases	O
the	O
tendency	O
of	O
a	O
model	B
to	O
over	O
(	O
cid:12	O
)	O
t	O
noise	B
in	O
the	O
training	B
data	I
.	O
2pi	O
w2	O
rumelhart	O
et	O
al	O
.	O
(	O
1986	O
)	O
showed	O
that	O
multilayer	O
perceptrons	O
can	O
be	O
trained	O
,	O
by	O
gradient	O
descent	O
on	O
m	O
(	O
w	O
)	O
,	O
to	O
discover	O
solutions	O
to	O
non-trivial	O
problems	O
such	O
as	O
deciding	O
whether	O
an	O
image	B
is	O
symmetric	B
or	O
not	O
.	O
these	O
networks	O
have	O
been	O
successfully	O
applied	O
to	O
real-world	O
tasks	O
as	O
varied	O
as	O
pronouncing	O
english	O
text	O
(	O
sejnowski	O
and	O
rosenberg	O
,	O
1987	O
)	O
and	O
focussing	O
multiple-mirror	O
telescopes	O
(	O
angel	O
et	O
al.	O
,	O
1990	O
)	O
.	O
44.3	O
neural	B
network	I
learning	O
as	B
inference	I
the	O
neural	B
network	I
learning	O
process	O
above	O
can	O
be	O
given	O
the	O
following	O
proba-	O
bilistic	O
interpretation	O
.	O
[	O
here	O
we	O
repeat	O
and	O
generalize	O
the	O
discussion	O
of	O
chap-	O
ter	O
41	O
.	O
]	O
the	O
error	B
function	I
is	O
interpreted	O
as	O
de	O
(	O
cid:12	O
)	O
ning	O
a	O
noise	B
model	O
.	O
(	O
cid:12	O
)	O
ed	O
is	O
the	O
negative	O
log	O
likelihood	B
:	O
p	O
(	O
d	O
j	O
w	O
;	O
(	O
cid:12	O
)	O
;	O
h	O
)	O
=	O
1	O
zd	O
(	O
(	O
cid:12	O
)	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
ed	O
)	O
:	O
(	O
44.5	O
)	O
thus	O
,	O
the	O
use	O
of	O
the	O
sum-squared	O
error	O
ed	O
(	O
44.3	O
)	O
corresponds	O
to	O
an	O
assump-	O
tion	O
of	O
gaussian	O
noise	B
on	O
the	O
target	O
variables	O
,	O
and	O
the	O
parameter	O
(	O
cid:12	O
)	O
de	O
(	O
cid:12	O
)	O
nes	O
a	O
noise	B
level	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
=	O
1=	O
(	O
cid:12	O
)	O
.	O
similarly	O
the	O
regularizer	O
is	O
interpreted	O
in	O
terms	O
of	O
a	O
log	O
prior	B
probability	O
distribution	B
over	O
the	O
parameters	B
:	O
p	O
(	O
w	O
j	O
(	O
cid:11	O
)	O
;	O
h	O
)	O
=	O
1	O
zw	O
(	O
(	O
cid:11	O
)	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
ew	O
)	O
:	O
(	O
44.6	O
)	O
if	O
ew	O
is	O
quadratic	O
as	O
de	O
(	O
cid:12	O
)	O
ned	O
above	O
,	O
then	O
the	O
corresponding	O
prior	B
distribution	O
is	O
a	O
gaussian	O
with	O
variance	O
(	O
cid:27	O
)	O
2	O
w	O
=	O
1=	O
(	O
cid:11	O
)	O
.	O
the	O
probabilistic	B
model	I
h	O
speci	O
(	O
cid:12	O
)	O
es	O
the	O
architecture	B
a	O
of	O
the	O
network	O
,	O
the	O
likelihood	B
(	O
44.5	O
)	O
,	O
and	O
the	O
prior	B
(	O
44.6	O
)	O
.	O
the	O
objective	B
function	I
m	O
(	O
w	O
)	O
then	O
corresponds	O
to	O
the	O
inference	B
of	O
the	O
parameters	B
w	O
,	O
given	O
the	O
data	O
:	O
p	O
(	O
w	O
j	O
d	O
;	O
(	O
cid:11	O
)	O
;	O
(	O
cid:12	O
)	O
;	O
h	O
)	O
=	O
p	O
(	O
d	O
j	O
w	O
;	O
(	O
cid:12	O
)	O
;	O
h	O
)	O
p	O
(	O
w	O
j	O
(	O
cid:11	O
)	O
;	O
h	O
)	O
p	O
(	O
d	O
j	O
(	O
cid:11	O
)	O
;	O
(	O
cid:12	O
)	O
;	O
h	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
m	O
(	O
w	O
)	O
)	O
:	O
=	O
1	O
zm	O
(	O
44.7	O
)	O
(	O
44.8	O
)	O
the	O
w	O
found	O
by	O
(	O
locally	O
)	O
minimizing	O
m	O
(	O
w	O
)	O
is	O
then	O
interpreted	O
as	O
the	O
(	O
locally	O
)	O
most	O
probable	O
parameter	O
vector	O
,	O
wmp	O
.	O
the	O
interpretation	O
of	O
m	O
(	O
w	O
)	O
as	O
a	O
log	O
probability	B
adds	O
little	O
new	O
at	O
this	O
stage	O
.	O
but	O
new	O
tools	O
will	O
emerge	O
when	O
we	O
proceed	O
to	O
other	O
inferences	O
.	O
first	O
,	O
though	O
,	O
let	O
us	O
establish	O
the	O
probabilistic	O
interpretation	O
of	O
classi	O
(	O
cid:12	O
)	O
cation	O
net-	O
works	O
,	O
to	O
which	O
the	O
same	O
tools	O
apply	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
530	O
44	O
|	O
supervised	O
learning	O
in	O
multilayer	O
networks	O
binary	O
classi	O
(	O
cid:12	O
)	O
cation	O
networks	O
if	O
the	O
targets	O
t	O
in	O
a	O
data	B
set	I
are	O
binary	O
classi	O
(	O
cid:12	O
)	O
cation	O
labels	O
(	O
0	O
;	O
1	O
)	O
,	O
it	O
is	O
natural	B
to	O
use	O
a	O
neural	B
network	I
whose	O
output	O
y	O
(	O
x	O
;	O
w	O
;	O
a	O
)	O
is	O
bounded	O
between	O
0	O
and	O
1	O
,	O
and	O
is	O
interpreted	O
as	O
a	O
probability	B
p	O
(	O
t	O
=	O
1	O
j	O
x	O
;	O
w	O
;	O
a	O
)	O
.	O
for	O
example	O
,	O
a	O
network	B
with	O
one	O
hidden	O
layer	O
could	O
be	O
described	O
by	O
the	O
feedforward	O
equations	O
(	O
44.1	O
)	O
and	O
(	O
44.2	O
)	O
,	O
with	O
f	O
(	O
2	O
)	O
(	O
a	O
)	O
=	O
1=	O
(	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
a	O
)	O
.	O
the	O
error	B
function	I
(	O
cid:12	O
)	O
ed	O
is	O
replaced	O
by	O
the	O
negative	O
log	O
likelihood	B
:	O
g	O
(	O
w	O
)	O
=	O
(	O
cid:0	O
)	O
''	O
xn	O
t	O
(	O
n	O
)	O
ln	O
y	O
(	O
x	O
(	O
n	O
)	O
;	O
w	O
)	O
+	O
(	O
1	O
(	O
cid:0	O
)	O
t	O
(	O
n	O
)	O
)	O
ln	O
(	O
1	O
(	O
cid:0	O
)	O
y	O
(	O
x	O
(	O
n	O
)	O
;	O
w	O
)	O
)	O
#	O
:	O
(	O
44.9	O
)	O
the	O
total	O
objective	B
function	I
is	O
then	O
m	O
=	O
g	O
+	O
(	O
cid:11	O
)	O
ew	O
.	O
note	O
that	O
this	O
includes	O
no	O
parameter	O
(	O
cid:12	O
)	O
(	O
because	O
there	O
is	O
no	O
gaussian	O
noise	B
)	O
.	O
multi-class	O
classi	O
(	O
cid:12	O
)	O
cation	O
networks	O
for	O
a	O
multi-class	O
classi	O
(	O
cid:12	O
)	O
cation	O
problem	O
,	O
we	O
can	O
represent	O
the	O
targets	O
by	O
a	O
vector	O
,	O
t	O
,	O
in	O
which	O
a	O
single	O
element	O
is	O
set	B
to	O
1	O
,	O
indicating	O
the	O
correct	O
class	O
,	O
and	O
all	O
other	O
elements	O
are	O
set	B
to	O
0.	O
in	O
this	O
case	O
it	O
is	O
appropriate	O
to	O
use	O
a	O
‘	O
softmax	B
’	O
network	B
having	O
coupled	O
outputs	O
which	O
sum	O
to	O
one	O
and	O
are	O
interpreted	O
as	O
class	O
probabilities	O
yi	O
=	O
p	O
(	O
ti	O
=	O
1	O
j	O
x	O
;	O
w	O
;	O
a	O
)	O
.	O
the	O
last	O
part	O
of	O
equation	O
(	O
44.2	O
)	O
is	O
replaced	O
by	O
:	O
eai	O
eai0	O
xi0	O
yi	O
=	O
:	O
(	O
44.10	O
)	O
the	O
negative	O
log	O
likelihood	B
in	O
this	O
case	O
is	O
g	O
(	O
w	O
)	O
=	O
(	O
cid:0	O
)	O
xn	O
xi	O
t	O
(	O
n	O
)	O
i	O
ln	O
yi	O
(	O
x	O
(	O
n	O
)	O
;	O
w	O
)	O
:	O
(	O
44.11	O
)	O
as	O
in	O
the	O
case	O
of	O
the	O
regression	O
network	B
,	O
the	O
minimization	B
of	O
the	O
objective	B
function	I
m	O
(	O
w	O
)	O
=	O
g	O
+	O
(	O
cid:11	O
)	O
ew	O
corresponds	O
to	O
an	O
inference	B
of	O
the	O
form	O
(	O
44.8	O
)	O
.	O
a	O
variety	O
of	O
useful	O
results	O
can	O
be	O
built	O
on	O
this	O
interpretation	O
.	O
44.4	O
bene	O
(	O
cid:12	O
)	O
ts	O
of	O
the	O
bayesian	O
approach	O
to	O
supervised	O
feedforward	O
neural	O
networks	O
from	O
the	O
statistical	B
perspective	O
,	O
supervised	O
neural	O
networks	O
are	O
nothing	O
more	O
than	O
nonlinear	B
curve-	O
(	O
cid:12	O
)	O
tting	O
devices	O
.	O
curve	O
(	O
cid:12	O
)	O
tting	O
is	O
not	O
a	O
trivial	O
task	O
however	O
.	O
the	O
e	O
(	O
cid:11	O
)	O
ective	O
complexity	B
of	O
an	O
interpolating	O
model	B
is	O
of	O
crucial	O
importance	O
,	O
as	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gure	O
44.5.	O
consider	O
a	O
control	O
parameter	O
that	O
in	O
(	O
cid:13	O
)	O
uences	O
the	O
complexity	B
of	O
a	O
model	B
,	O
for	O
example	O
a	O
regularization	B
constant	I
(	O
cid:11	O
)	O
(	O
weight	B
decay	I
parameter	O
)	O
.	O
as	O
the	O
control	O
parameter	O
is	O
varied	O
to	O
increase	O
the	O
complexity	B
of	O
the	O
model	B
(	O
descending	O
from	O
(	O
cid:12	O
)	O
gure	O
44.5a	O
{	O
c	O
and	O
going	O
from	O
left	O
to	O
right	O
across	O
(	O
cid:12	O
)	O
gure	O
44.5d	O
)	O
,	O
the	O
best	O
(	O
cid:12	O
)	O
t	O
to	O
the	O
training	B
data	I
that	O
the	O
model	B
can	O
achieve	O
becomes	O
increasingly	O
good	B
.	O
however	O
,	O
the	O
empirical	O
performance	O
of	O
the	O
model	O
,	O
the	O
test	B
error	O
,	O
(	O
cid:12	O
)	O
rst	O
decreases	O
then	O
increases	O
again	O
.	O
an	O
over-complex	O
model	B
over	O
(	O
cid:12	O
)	O
ts	O
the	O
data	O
and	O
generalizes	O
poorly	O
.	O
this	O
problem	O
may	O
also	O
complicate	O
the	O
choice	O
of	O
architecture	O
in	O
a	O
multilayer	B
perceptron	I
,	O
the	O
radius	O
of	O
the	O
basis	O
functions	B
in	O
a	O
radial	B
basis	I
function	I
network	O
,	O
and	O
the	O
choice	O
of	O
the	O
input	O
vari-	O
ables	O
themselves	O
in	O
any	O
multidimensional	O
regression	B
problem	O
.	O
finding	O
values	O
for	O
model	O
control	O
parameters	O
that	O
are	O
appropriate	O
for	O
the	O
data	O
is	O
therefore	O
an	O
important	O
and	O
non-trivial	O
problem	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
44.4	O
:	O
bene	O
(	O
cid:12	O
)	O
ts	O
of	O
the	O
bayesian	O
approach	O
to	O
supervised	O
feedforward	O
neural	O
networks	O
531	O
figure	O
44.5.	O
optimization	B
of	O
model	B
complexity	O
.	O
panels	O
(	O
a	O
{	O
c	O
)	O
show	O
a	O
radial	B
basis	I
function	I
model	O
interpolating	O
a	O
simple	O
data	O
set	B
with	O
one	O
input	O
variable	O
and	O
one	O
output	O
variable	O
.	O
as	O
the	O
regularization	B
constant	I
is	O
varied	O
to	O
increase	O
the	O
complexity	B
of	O
the	O
model	B
(	O
from	O
(	O
a	O
)	O
to	O
(	O
c	O
)	O
)	O
,	O
the	O
interpolant	O
is	O
able	O
to	O
(	O
cid:12	O
)	O
t	O
the	O
training	B
data	I
increasingly	O
well	O
,	O
but	O
beyond	O
a	O
certain	O
point	O
the	O
generalization	B
ability	O
(	O
test	B
error	O
)	O
of	O
the	O
model	O
deteriorates	O
.	O
probability	B
theory	O
allows	O
us	O
to	O
optimize	O
the	O
control	O
parameters	O
without	O
needing	O
a	O
test	B
set	O
.	O
test	B
error	O
training	O
error	O
(	O
d	O
)	O
model	B
control	O
parameters	B
log	O
probability	B
(	O
training	B
data	I
|	O
control	O
parameters	O
)	O
(	O
e	O
)	O
model	B
control	O
parameters	B
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
the	O
over	O
(	O
cid:12	O
)	O
tting	O
problem	O
can	O
be	O
solved	O
by	O
using	O
a	O
bayesian	O
approach	O
to	O
control	O
model	O
complexity	B
.	O
if	O
we	O
give	O
a	O
probabilistic	O
interpretation	O
to	O
the	O
model	B
,	O
then	O
we	O
can	O
evaluate	O
the	O
evidence	B
for	O
alternative	O
values	O
of	O
the	O
control	O
parameters	B
.	O
as	O
was	O
explained	O
in	O
chapter	O
28	O
,	O
over-complex	O
models	O
turn	O
out	O
to	O
be	O
less	O
probable	O
,	O
and	O
the	O
evidence	B
p	O
(	O
data	O
j	O
control	O
parameters	O
)	O
can	O
be	O
used	O
as	O
an	O
objective	B
function	I
for	O
optimization	B
of	O
model	B
control	O
parameters	B
(	O
(	O
cid:12	O
)	O
gure	O
44.5e	O
)	O
.	O
the	O
setting	O
of	O
(	O
cid:11	O
)	O
that	O
maximizes	O
the	O
evidence	B
is	O
displayed	O
in	O
(	O
cid:12	O
)	O
gure	O
44.5b	O
.	O
bayesian	O
optimization	B
of	O
model	B
control	O
parameters	B
has	O
four	O
important	O
ad-	O
vantages	O
.	O
(	O
1	O
)	O
no	O
‘	O
test	B
set	O
’	O
or	O
‘	O
validation	O
set	B
’	O
is	O
involved	O
,	O
so	O
all	O
available	O
training	B
data	I
can	O
be	O
devoted	O
to	O
both	O
model	B
(	O
cid:12	O
)	O
tting	O
and	O
model	O
comparison	O
.	O
(	O
2	O
)	O
reg-	O
ularization	O
constants	O
can	O
be	O
optimized	O
on-line	O
,	O
i.e.	O
,	O
simultaneously	O
with	O
the	O
optimization	B
of	O
ordinary	O
model	B
parameters	O
.	O
(	O
3	O
)	O
the	O
bayesian	O
objective	O
func-	O
tion	O
is	O
not	O
noisy	B
,	O
in	O
contrast	O
to	O
a	O
cross-validation	B
measure	O
.	O
(	O
4	O
)	O
the	O
gradient	O
of	O
the	O
evidence	B
with	O
respect	O
to	O
the	O
control	O
parameters	O
can	O
be	O
evaluated	O
,	O
making	O
it	O
possible	O
to	O
simultaneously	O
optimize	O
a	O
large	O
number	O
of	O
control	O
parameters	B
.	O
probabilistic	O
modelling	O
also	O
handles	O
uncertainty	O
in	O
a	O
natural	B
manner	O
.	O
it	O
o	O
(	O
cid:11	O
)	O
ers	O
a	O
unique	O
prescription	O
,	O
marginalization	B
,	O
for	O
incorporating	O
uncertainty	O
about	O
parameters	B
into	O
predictions	O
;	O
this	O
procedure	O
yields	O
better	O
predictions	O
,	O
as	O
we	O
saw	O
in	O
chapter	O
41.	O
figure	O
44.6	O
shows	O
error	B
bars	I
on	O
the	O
predictions	O
of	O
a	O
trained	O
neural	B
network	I
.	O
implementation	O
of	O
bayesian	O
inference	B
as	O
was	O
mentioned	O
in	O
chapter	O
41	O
,	O
bayesian	O
inference	B
for	O
multilayer	O
networks	O
may	O
be	O
implemented	O
by	O
monte	O
carlo	O
sampling	O
,	O
or	O
by	O
deterministic	O
methods	B
employing	O
gaussian	O
approximations	O
(	O
neal	O
,	O
1996	O
;	O
mackay	O
,	O
1992c	O
)	O
.	O
figure	O
44.6.	O
error	B
bars	I
on	O
the	O
predictions	O
of	O
a	O
trained	O
regression	B
network	O
.	O
the	O
solid	O
line	O
gives	O
the	O
predictions	O
of	O
the	O
best-	O
(	O
cid:12	O
)	O
t	O
parameters	B
of	O
a	O
multilayer	B
perceptron	I
trained	O
on	O
the	O
data	O
points	O
.	O
the	O
error	B
bars	I
(	O
dotted	O
lines	O
)	O
are	O
those	O
produced	O
by	O
the	O
uncertainty	O
of	O
the	O
parameters	O
w.	O
notice	O
that	O
the	O
error	B
bars	I
become	O
larger	O
where	O
the	O
data	O
are	O
sparse	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
532	O
44	O
|	O
supervised	O
learning	O
in	O
multilayer	O
networks	O
within	O
the	O
bayesian	O
framework	O
for	O
data	O
modelling	B
,	O
it	O
is	O
easy	O
to	O
improve	O
our	O
probabilistic	O
models	O
.	O
for	O
example	O
,	O
if	O
we	O
believe	O
that	O
some	O
input	O
variables	O
in	O
a	O
problem	O
may	O
be	O
irrelevant	O
to	O
the	O
predicted	O
quantity	O
,	O
but	O
we	O
don	O
’	O
t	O
know	O
which	O
,	O
we	O
can	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
new	O
model	B
with	O
multiple	O
hyperparameters	O
that	O
captures	O
the	O
idea	O
of	O
uncertain	O
input	O
variable	O
relevance	O
(	O
mackay	O
,	O
1994b	O
;	O
neal	O
,	O
1996	O
;	O
mackay	O
,	O
1995b	O
)	O
;	O
these	O
models	O
then	O
infer	O
automatically	O
from	O
the	O
data	O
which	O
are	O
the	O
relevant	O
input	O
variables	O
for	O
a	O
problem	O
.	O
44.5	O
exercises	O
exercise	O
44.1	O
.	O
[	O
4	O
]	O
how	B
to	I
measure	I
a	O
classi	O
(	O
cid:12	O
)	O
er	O
’	O
s	O
quality	O
.	O
you	O
’	O
ve	O
just	O
written	O
a	O
new	O
classi	O
(	O
cid:12	O
)	O
cation	O
algorithm	B
and	O
want	O
to	O
measure	O
how	O
well	O
it	O
performs	O
on	O
a	O
test	B
set	O
,	O
and	O
compare	O
it	O
with	O
other	O
classi	O
(	O
cid:12	O
)	O
ers	O
.	O
what	O
performance	O
measure	O
should	O
you	O
use	O
?	O
there	O
are	O
several	O
standard	O
answers	O
.	O
let	O
’	O
s	O
assume	O
the	O
classi	O
(	O
cid:12	O
)	O
er	O
gives	O
an	O
output	O
y	O
(	O
x	O
)	O
,	O
where	O
x	O
is	O
the	O
input	O
,	O
which	O
we	O
won	O
’	O
t	O
discuss	O
further	O
,	O
and	O
that	O
the	O
true	O
target	O
value	O
is	O
t.	O
in	O
the	O
simplest	O
discussions	O
of	O
classi	O
(	O
cid:12	O
)	O
ers	O
,	O
both	O
y	O
and	O
t	O
are	O
binary	O
variables	O
,	O
but	O
you	O
might	O
care	O
to	O
consider	O
cases	O
where	O
y	O
and	O
t	O
are	O
more	O
general	O
objects	O
also	O
.	O
the	O
most	O
widely	O
used	O
measure	O
of	O
performance	O
on	O
a	O
test	B
set	O
is	O
the	O
error	O
rate	O
{	O
the	O
fraction	O
of	O
misclassi	O
(	O
cid:12	O
)	O
cations	O
made	O
by	O
the	O
classi	O
(	O
cid:12	O
)	O
er	O
.	O
this	O
measure	O
forces	O
the	O
classi	O
(	O
cid:12	O
)	O
er	O
to	O
give	O
a	O
0/1	O
output	O
and	O
ignores	O
any	O
additional	O
information	B
that	O
the	O
classi	O
(	O
cid:12	O
)	O
er	O
might	O
be	O
able	O
to	O
o	O
(	O
cid:11	O
)	O
er	O
{	O
for	O
example	O
,	O
an	O
indication	O
of	O
the	O
(	O
cid:12	O
)	O
rmness	O
of	O
a	O
prediction	B
.	O
unfortunately	O
,	O
the	O
error	O
rate	O
does	O
not	O
necessarily	O
measure	O
how	O
informative	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
’	O
s	O
output	O
is	O
.	O
consider	O
frequency	B
tables	O
showing	O
the	O
joint	B
frequency	O
of	O
the	O
0/1	O
output	O
of	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
(	O
horizontal	O
axis	O
)	O
,	O
and	O
the	O
true	O
0/1	O
variable	O
(	O
vertical	O
axis	O
)	O
.	O
the	O
numbers	O
that	O
we	O
’	O
ll	O
show	O
are	O
percentages	O
.	O
the	O
error	O
rate	O
e	O
is	O
the	O
sum	O
of	O
the	O
two	O
o	O
(	O
cid:11	O
)	O
-diagonal	O
numbers	O
,	O
which	O
we	O
could	O
call	O
the	O
false	O
positive	O
rate	O
e+	O
and	O
the	O
false	O
negative	O
rate	B
e	O
(	O
cid:0	O
)	O
.	O
of	O
the	O
following	O
three	O
classi	O
(	O
cid:12	O
)	O
ers	O
,	O
a	O
and	O
b	O
have	O
the	O
same	O
error	O
rate	O
of	O
10	O
%	O
and	O
c	O
has	O
a	O
greater	O
error	O
rate	O
of	O
12	O
%	O
.	O
classi	O
(	O
cid:12	O
)	O
er	O
a	O
y	O
0	O
t	O
0	O
1	O
90	O
10	O
1	O
0	O
0	O
classi	O
(	O
cid:12	O
)	O
er	O
b	O
y	O
0	O
1	O
classi	O
(	O
cid:12	O
)	O
er	O
c	O
y	O
0	O
1	O
t	O
0	O
1	O
80	O
0	O
10	O
10	O
t	O
0	O
1	O
78	O
0	O
12	O
10	O
but	O
clearly	O
classi	O
(	O
cid:12	O
)	O
er	O
a	O
,	O
which	O
simply	O
guesses	O
that	O
the	O
outcome	O
is	O
0	O
for	O
all	O
cases	O
,	O
is	O
conveying	O
no	O
information	B
at	O
all	O
about	O
t	O
;	O
whereas	O
classi	O
(	O
cid:12	O
)	O
er	O
b	O
has	O
an	O
informative	O
output	O
:	O
if	O
y	O
=	O
0	O
then	O
we	O
are	O
sure	O
that	O
t	O
really	O
is	O
zero	O
;	O
and	O
if	O
y	O
=	O
1	O
then	O
there	O
is	O
a	O
50	O
%	O
chance	O
that	O
t	O
=	O
1	O
,	O
as	O
compared	O
to	O
the	O
prior	B
probability	O
p	O
(	O
t	O
=	O
1	O
)	O
=	O
0:1.	O
classi	O
(	O
cid:12	O
)	O
er	O
c	O
is	O
slightly	O
less	O
informative	O
than	O
b	O
,	O
but	O
it	O
is	O
still	O
much	O
more	O
useful	O
than	O
the	O
information-free	O
classi	O
(	O
cid:12	O
)	O
er	O
a.	O
one	O
way	O
to	O
improve	O
on	O
the	O
error	O
rate	O
as	O
a	O
performance	O
measure	O
is	O
to	O
report	O
the	O
pair	O
(	O
e+	O
;	O
e	O
(	O
cid:0	O
)	O
)	O
,	O
the	O
false	O
positive	O
error	O
rate	B
and	O
the	O
false	O
negative	O
error	O
rate	O
,	O
which	O
are	O
(	O
0	O
;	O
0:1	O
)	O
and	O
(	O
0:1	O
;	O
0	O
)	O
for	O
classi	O
(	O
cid:12	O
)	O
ers	O
a	O
and	O
b.	O
it	O
is	O
especially	O
important	O
to	O
distinguish	O
between	O
these	O
two	O
error	O
probabilities	O
in	O
applications	O
where	O
the	O
two	O
sorts	O
of	O
error	O
have	O
di	O
(	O
cid:11	O
)	O
erent	O
associated	O
costs	O
.	O
however	O
,	O
there	O
are	O
a	O
couple	O
of	O
problems	O
with	O
the	O
‘	O
error	O
rate	O
pair	O
’	O
:	O
(	O
cid:15	O
)	O
first	O
,	O
if	O
i	O
simply	O
told	O
you	O
that	O
classi	O
(	O
cid:12	O
)	O
er	O
a	O
has	O
error	O
rates	O
(	O
0	O
;	O
0:1	O
)	O
and	O
b	O
has	O
error	O
rates	O
(	O
0:1	O
;	O
0	O
)	O
,	O
it	O
would	O
not	O
be	O
immediately	O
evident	O
that	O
classi	O
(	O
cid:12	O
)	O
er	O
a	O
is	O
actually	O
utterly	O
worthless	O
.	O
surely	O
we	O
should	O
have	O
a	O
performance	O
measure	O
that	O
gives	O
the	O
worst	O
possible	O
score	O
to	O
a	O
!	O
how	O
common	O
sense	O
ranks	O
the	O
classi	O
(	O
cid:12	O
)	O
ers	O
:	O
(	O
best	O
)	O
b	O
>	O
c	O
>	O
a	O
(	O
worst	O
)	O
.	O
how	O
error	O
rate	B
ranks	O
the	O
classi	O
(	O
cid:12	O
)	O
ers	O
:	O
(	O
best	O
)	O
a	O
=	O
b	O
>	O
c	O
(	O
worst	O
)	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
44.5	O
:	O
exercises	O
533	O
(	O
cid:15	O
)	O
second	O
,	O
if	O
we	O
turn	O
to	O
a	O
multiple-class	O
classi	O
(	O
cid:12	O
)	O
cation	O
problem	O
such	O
as	O
digit	O
recognition	B
,	O
then	O
the	O
number	O
of	O
types	O
of	O
error	O
increases	O
from	O
two	O
to	O
10	O
(	O
cid:2	O
)	O
9	O
=	O
90	O
{	O
one	O
for	O
each	O
possible	O
confusion	O
of	O
class	O
t	O
with	O
t0	O
.	O
it	O
would	O
be	O
nice	O
to	O
have	O
some	O
sensible	O
way	O
of	O
collapsing	O
these	O
90	O
numbers	O
into	O
a	O
single	O
rankable	O
number	O
that	O
makes	O
more	O
sense	O
than	O
the	O
error	O
rate	O
.	O
another	O
reason	O
for	O
not	O
liking	O
the	O
error	O
rate	O
is	O
that	O
it	O
doesn	O
’	O
t	O
give	O
a	O
classi	O
(	O
cid:12	O
)	O
er	O
credit	O
for	O
accurately	O
specifying	O
its	O
uncertainty	O
.	O
consider	O
classi	O
(	O
cid:12	O
)	O
ers	O
that	O
have	O
three	O
outputs	O
available	O
,	O
‘	O
0	O
’	O
,	O
‘	O
1	O
’	O
and	O
a	O
rejection	B
class	O
,	O
‘	O
?	O
’	O
,	O
which	O
indicates	O
that	O
the	O
classi	O
(	O
cid:12	O
)	O
er	O
is	O
not	O
sure	O
.	O
consider	O
classi	O
(	O
cid:12	O
)	O
ers	O
d	O
and	O
e	O
with	O
the	O
following	O
frequency	B
tables	O
,	O
in	O
percentages	O
:	O
classi	O
(	O
cid:12	O
)	O
er	O
d	O
y	O
0	O
?	O
t	O
0	O
1	O
74	O
0	O
10	O
1	O
1	O
6	O
9	O
classi	O
(	O
cid:12	O
)	O
er	O
e	O
y	O
0	O
t	O
0	O
1	O
78	O
0	O
?	O
6	O
5	O
1	O
6	O
5	O
both	O
of	O
these	O
classi	O
(	O
cid:12	O
)	O
ers	O
have	O
(	O
e+	O
;	O
e	O
(	O
cid:0	O
)	O
;	O
r	O
)	O
=	O
(	O
6	O
%	O
;	O
0	O
%	O
;	O
11	O
%	O
)	O
.	O
but	O
are	O
they	O
equally	O
good	B
classi	O
(	O
cid:12	O
)	O
ers	O
?	O
compare	O
classi	O
(	O
cid:12	O
)	O
er	O
e	O
with	O
c.	O
the	O
two	O
classi	O
(	O
cid:12	O
)	O
ers	O
are	O
equiva-	O
lent	O
.	O
e	O
is	O
just	O
c	O
in	O
disguise	O
{	O
we	O
could	O
make	O
e	O
by	O
taking	O
the	O
output	O
of	O
c	O
and	O
tossing	O
a	O
coin	B
when	O
c	O
says	O
‘	O
1	O
’	O
in	O
order	O
to	O
decide	O
whether	O
to	O
give	O
output	O
‘	O
1	O
’	O
or	O
‘	O
?	O
’	O
.	O
so	O
e	O
is	O
equal	O
to	O
c	O
and	O
thus	O
inferior	O
to	O
b.	O
now	O
compare	O
d	O
with	O
b.	O
can	O
you	O
justify	O
the	O
suggestion	O
that	O
d	O
is	O
a	O
more	O
informative	O
classi	O
(	O
cid:12	O
)	O
er	O
than	O
b	O
,	O
and	O
thus	O
is	O
superior	O
to	O
e	O
?	O
yet	O
d	O
and	O
e	O
have	O
the	O
same	O
(	O
e+	O
;	O
e	O
(	O
cid:0	O
)	O
;	O
r	O
)	O
scores	O
.	O
people	O
often	O
plot	O
error-reject	B
curves	I
(	O
also	O
known	O
as	O
roc	O
curves	O
;	O
roc	O
stands	O
for	O
‘	O
receiver	B
operating	I
characteristic	I
’	O
)	O
which	O
show	O
the	O
total	O
e	O
=	O
(	O
e+	O
+	O
e	O
(	O
cid:0	O
)	O
)	O
versus	O
r	O
as	O
r	O
is	O
allowed	O
to	O
vary	O
from	O
0	O
to	O
1	O
,	O
and	O
use	O
these	O
curves	O
to	O
compare	O
classi	O
(	O
cid:12	O
)	O
ers	O
(	O
(	O
cid:12	O
)	O
gure	O
44.7	O
)	O
.	O
[	O
in	O
the	O
special	O
case	O
of	O
binary	O
classi	O
(	O
cid:12	O
)	O
cation	O
problems	O
,	O
e+	O
may	O
be	O
plotted	O
versus	O
e	O
(	O
cid:0	O
)	O
instead	O
.	O
]	O
but	O
as	O
we	O
have	O
seen	O
,	O
error	O
rates	O
can	O
be	O
undiscerning	O
performance	O
measures	O
.	O
does	O
plotting	O
one	O
error	O
rate	O
as	O
a	O
function	B
of	O
another	O
make	O
this	O
weakness	B
of	I
error	O
rates	O
go	O
away	O
?	O
for	O
this	O
exercise	O
,	O
either	O
construct	O
an	O
explicit	O
example	O
demonstrating	O
that	O
the	O
error-reject	O
curve	O
,	O
and	O
the	O
area	O
under	O
it	O
,	O
are	O
not	O
necessarily	O
good	B
ways	O
to	O
compare	O
classi	O
(	O
cid:12	O
)	O
ers	O
;	O
or	O
prove	O
that	O
they	O
are	O
.	O
as	O
a	O
suggested	O
alternative	O
method	B
for	O
comparing	O
classi	O
(	O
cid:12	O
)	O
ers	O
,	O
consider	O
the	O
mutual	B
information	I
between	O
the	O
output	O
and	O
the	O
target	O
,	O
i	O
(	O
t	O
;	O
y	O
)	O
(	O
cid:17	O
)	O
h	O
(	O
t	O
)	O
(	O
cid:0	O
)	O
h	O
(	O
t	O
j	O
y	O
)	O
=xy	O
;	O
t	O
p	O
(	O
y	O
)	O
p	O
(	O
tj	O
y	O
)	O
log	O
p	O
(	O
t	O
)	O
p	O
(	O
tj	O
y	O
)	O
;	O
(	O
44.12	O
)	O
which	O
measures	O
how	O
many	O
bits	O
the	O
classi	O
(	O
cid:12	O
)	O
er	O
’	O
s	O
output	O
conveys	O
about	O
the	O
target	O
.	O
evaluate	O
the	O
mutual	B
information	I
for	O
classi	O
(	O
cid:12	O
)	O
ers	O
a	O
{	O
e	O
above	O
.	O
investigate	O
this	O
performance	O
measure	O
and	O
discuss	O
whether	O
it	O
is	O
a	O
useful	O
one	O
.	O
does	O
it	O
have	O
practical	B
drawbacks	O
?	O
error	O
rate	O
rejection	B
rate	O
figure	O
44.7.	O
an	O
error-reject	O
curve	O
.	O
some	O
people	O
use	O
the	O
area	O
under	O
this	O
curve	O
as	O
a	O
measure	O
of	O
classi	O
(	O
cid:12	O
)	O
er	O
quality	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
chapter	O
45	O
feedforward	O
neural	O
networks	O
such	O
as	O
multilayer	O
perceptrons	O
are	O
popular	O
tools	O
for	O
nonlinear	O
regression	B
and	O
classi	O
(	O
cid:12	O
)	O
cation	O
problems	O
.	O
from	O
a	O
bayesian	O
per-	O
spective	O
,	O
a	O
choice	O
of	O
a	O
neural	B
network	I
model	O
can	O
be	O
viewed	O
as	O
de	O
(	O
cid:12	O
)	O
ning	O
a	O
prior	B
probability	O
distribution	B
over	O
nonlinear	B
functions	O
,	O
and	O
the	O
neural	B
network	I
’	O
s	O
learning	B
process	O
can	O
be	O
interpreted	O
in	O
terms	O
of	O
the	O
posterior	O
probability	B
dis-	O
tribution	O
over	O
the	O
unknown	O
function	O
.	O
(	O
some	O
learning	B
algorithms	I
search	O
for	O
the	O
function	B
with	O
maximum	O
posterior	O
probability	B
and	O
other	O
monte	O
carlo	O
methods	B
draw	O
samples	O
from	O
this	O
posterior	B
probability	I
.	O
)	O
in	O
the	O
limit	O
of	O
large	O
but	O
otherwise	O
standard	O
networks	O
,	O
neal	O
(	O
1996	O
)	O
has	O
shown	O
that	O
the	O
prior	B
distribution	O
over	O
nonlinear	O
functions	B
implied	O
by	O
the	O
bayesian	O
neural	B
network	I
falls	O
in	O
a	O
class	O
of	O
probability	O
distributions	O
known	O
as	O
gaussian	O
processes	O
.	O
the	O
hyperparameters	O
of	O
the	O
neural	O
network	B
model	O
determine	O
the	O
characteristic	O
lengthscales	O
of	O
the	O
gaussian	O
process	O
.	O
neal	O
’	O
s	O
ob-	O
servation	O
motivates	O
the	O
idea	O
of	O
discarding	O
parameterized	O
networks	O
and	O
working	O
directly	O
with	O
gaussian	O
processes	O
.	O
computations	O
in	O
which	O
the	O
parameters	B
of	O
the	O
network	B
are	O
optimized	O
are	O
then	O
replaced	O
by	O
simple	O
matrix	B
operations	O
using	O
the	O
covariance	B
matrix	I
of	O
the	O
gaussian	O
process	O
.	O
in	O
this	O
chapter	O
i	O
will	O
review	O
work	O
on	O
this	O
idea	O
by	O
williams	O
and	O
rasmussen	O
(	O
1996	O
)	O
,	O
neal	O
(	O
1997b	O
)	O
,	O
barber	O
and	O
williams	O
(	O
1997	O
)	O
and	O
gibbs	O
and	O
mackay	O
(	O
2000	O
)	O
,	O
and	O
will	O
assess	O
whether	O
,	O
for	O
supervised	O
regression	B
and	O
classi	O
(	O
cid:12	O
)	O
cation	O
tasks	O
,	O
the	O
feedforward	O
network	B
has	O
been	O
superceded	O
.	O
.	O
exercise	O
45.1	O
.	O
[	O
3	O
]	O
i	O
regret	B
that	O
this	O
chapter	O
is	O
rather	O
dry	O
.	O
there	O
’	O
s	O
no	O
simple	O
explanatory	O
examples	O
in	O
it	O
,	O
and	O
few	O
pictures	O
.	O
this	O
exercise	O
asks	O
you	O
to	O
create	O
interesting	O
pictures	O
to	O
explain	O
to	O
yourself	O
this	O
chapter	O
’	O
s	O
ideas	O
.	O
source	B
code	I
for	O
computer	B
demonstrations	O
written	O
in	O
the	O
free	O
language	O
octave	B
is	O
available	O
at	O
:	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itprnn/software.html	O
.	O
radford	O
neal	O
’	O
s	O
software	B
for	O
gaussian	O
processes	O
is	O
available	O
at	O
:	O
http	O
:	O
//www.cs.toronto.edu/~radford/	O
.	O
534	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
45	O
gaussian	O
processes	O
after	O
the	O
publication	O
of	O
rumelhart	O
,	O
hinton	O
and	O
williams	O
’	O
s	O
(	O
1986	O
)	O
paper	O
on	O
supervised	O
learning	B
in	O
neural	O
networks	O
there	O
was	O
a	O
surge	O
of	O
interest	O
in	O
the	O
empirical	O
modelling	B
of	O
relationships	O
in	O
high-dimensional	O
data	O
using	O
nonlinear	B
parametric	O
models	O
such	O
as	O
multilayer	O
perceptrons	O
and	O
radial	O
basis	O
functions	O
.	O
in	O
the	O
bayesian	O
interpretation	O
of	O
these	O
modelling	B
methods	O
,	O
a	O
nonlinear	B
func-	O
tion	O
y	O
(	O
x	O
)	O
parameterized	O
by	O
parameters	O
w	O
is	O
assumed	O
to	O
underlie	O
the	O
data	O
fx	O
(	O
n	O
)	O
;	O
tngn	O
n=1	O
,	O
and	O
the	O
adaptation	O
of	O
the	O
model	O
to	O
the	O
data	O
corresponds	O
to	O
an	O
inference	B
of	O
the	O
function	B
given	O
the	O
data	O
.	O
we	O
will	O
denote	O
the	O
set	B
of	O
input	O
vectors	O
by	O
xn	O
(	O
cid:17	O
)	O
fx	O
(	O
n	O
)	O
gn	O
n=1	O
and	O
the	O
set	B
of	O
corresponding	O
target	O
values	O
by	O
the	O
vector	O
tn	O
(	O
cid:17	O
)	O
ftngn	O
n=1	O
.	O
the	O
inference	B
of	O
y	O
(	O
x	O
)	O
is	O
described	O
by	O
the	O
posterior	B
probability	I
distribution	O
p	O
(	O
y	O
(	O
x	O
)	O
j	O
tn	O
;	O
xn	O
)	O
=	O
p	O
(	O
tn	O
j	O
y	O
(	O
x	O
)	O
;	O
xn	O
)	O
p	O
(	O
y	O
(	O
x	O
)	O
)	O
p	O
(	O
tn	O
j	O
xn	O
)	O
:	O
(	O
45.1	O
)	O
of	O
the	O
two	O
terms	O
on	O
the	O
right-hand	O
side	O
,	O
the	O
(	O
cid:12	O
)	O
rst	O
,	O
p	O
(	O
tn	O
j	O
y	O
(	O
x	O
)	O
;	O
xn	O
)	O
,	O
is	O
the	O
probability	O
of	O
the	O
target	O
values	O
given	O
the	O
function	B
y	O
(	O
x	O
)	O
,	O
which	O
in	O
the	O
case	O
of	O
regression	O
problems	O
is	O
often	O
assumed	O
to	O
be	O
a	O
separable	O
gaussian	O
distribution	B
;	O
and	O
the	O
second	O
term	O
,	O
p	O
(	O
y	O
(	O
x	O
)	O
)	O
,	O
is	O
the	O
prior	B
distribution	O
on	O
functions	O
assumed	O
by	O
the	O
model	B
.	O
this	O
prior	B
is	O
implicit	O
in	O
the	O
choice	O
of	O
parametric	O
model	B
and	O
the	O
choice	O
of	O
regularizers	O
used	O
during	O
the	O
model	B
(	O
cid:12	O
)	O
tting	O
.	O
the	O
prior	B
typically	O
speci	O
(	O
cid:12	O
)	O
es	O
that	O
the	O
function	B
y	O
(	O
x	O
)	O
is	O
expected	O
to	O
be	O
continuous	B
and	O
smooth	O
,	O
and	O
has	O
less	O
high	O
frequency	O
power	O
than	O
low	O
frequency	B
power	O
,	O
but	O
the	O
precise	O
meaning	O
of	O
the	O
prior	O
is	O
somewhat	O
obscured	O
by	O
the	O
use	O
of	O
the	O
parametric	O
model	B
.	O
now	O
,	O
for	O
the	O
prediction	B
of	O
future	O
values	O
of	O
t	O
,	O
all	O
that	O
matters	O
is	O
the	O
as-	O
sumed	O
prior	B
p	O
(	O
y	O
(	O
x	O
)	O
)	O
and	O
the	O
assumed	O
noise	B
model	O
p	O
(	O
tn	O
j	O
y	O
(	O
x	O
)	O
;	O
xn	O
)	O
{	O
the	O
parameterization	O
of	O
the	O
function	O
y	O
(	O
x	O
;	O
w	O
)	O
is	O
irrelevant	O
.	O
the	O
idea	O
of	O
gaussian	O
process	O
modelling	B
is	O
to	O
place	O
a	O
prior	B
p	O
(	O
y	O
(	O
x	O
)	O
)	O
directly	O
on	O
the	O
space	O
of	O
functions	O
,	O
without	O
parameterizing	O
y	O
(	O
x	O
)	O
.	O
the	O
simplest	O
type	O
of	O
prior	O
over	O
functions	O
is	O
called	O
a	O
gaussian	O
process	O
.	O
it	O
can	O
be	O
thought	O
of	O
as	O
the	O
generalization	B
of	O
a	O
gaussian	O
distribution	B
over	O
a	O
(	O
cid:12	O
)	O
nite	O
vector	O
space	O
to	O
a	O
function	B
space	O
of	O
in	O
(	O
cid:12	O
)	O
nite	O
dimension	O
.	O
just	O
as	O
a	O
gaussian	O
distribution	B
is	O
fully	O
speci	O
(	O
cid:12	O
)	O
ed	O
by	O
its	O
mean	B
and	O
covariance	B
matrix	I
,	O
a	O
gaussian	O
process	O
is	O
speci	O
(	O
cid:12	O
)	O
ed	O
by	O
a	O
mean	B
and	O
a	O
covariance	B
function	I
.	O
here	O
,	O
the	O
mean	B
is	O
a	O
function	B
of	O
x	O
(	O
which	O
we	O
will	O
often	O
take	O
to	O
be	O
the	O
zero	O
function	B
)	O
,	O
and	O
the	O
covariance	B
is	O
a	O
function	B
c	O
(	O
x	O
;	O
x0	O
)	O
that	O
expresses	O
the	O
expected	O
covariance	B
between	O
the	O
values	O
of	O
the	O
function	O
y	O
at	O
the	O
points	O
x	O
and	O
x0	O
.	O
the	O
function	B
y	O
(	O
x	O
)	O
in	O
any	O
one	O
data	B
modelling	I
problem	O
is	O
assumed	O
to	O
be	O
a	O
single	O
sample	O
from	O
this	O
gaussian	O
distribution	B
.	O
gaussian	O
processes	O
are	O
already	O
well	O
established	O
models	O
for	O
various	O
spatial	O
and	O
temporal	O
problems	O
{	O
for	O
example	O
,	O
brownian	O
motion	O
,	O
langevin	O
processes	O
and	O
wiener	O
processes	O
are	O
all	O
examples	O
of	O
gaussian	O
processes	O
;	O
kalman	O
(	O
cid:12	O
)	O
lters	O
,	O
widely	O
used	O
535	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
536	O
45	O
|	O
gaussian	O
processes	O
to	O
model	B
speech	O
waveforms	O
,	O
also	O
correspond	O
to	O
gaussian	O
process	O
models	O
;	O
the	O
method	B
of	O
‘	O
kriging	B
’	O
in	O
geostatistics	O
is	O
a	O
gaussian	O
process	O
regression	B
method	O
.	O
reservations	O
about	O
gaussian	O
processes	O
it	O
might	O
be	O
thought	O
that	O
it	O
is	O
not	O
possible	O
to	O
reproduce	O
the	O
interesting	O
prop-	O
erties	O
of	O
neural	O
network	B
interpolation	O
methods	B
with	O
something	O
so	O
simple	O
as	O
a	O
gaussian	O
distribution	B
,	O
but	O
as	O
we	O
shall	O
now	O
see	O
,	O
many	O
popular	O
nonlinear	B
inter-	O
polation	O
methods	B
are	O
equivalent	O
to	O
particular	O
gaussian	O
processes	O
.	O
(	O
i	O
use	O
the	O
term	O
‘	O
interpolation	O
’	O
to	O
cover	O
both	O
the	O
problem	O
of	O
‘	O
regression	B
’	O
{	O
(	O
cid:12	O
)	O
tting	O
a	O
curve	O
through	O
noisy	B
data	O
{	O
and	O
the	O
task	O
of	O
(	O
cid:12	O
)	O
tting	O
an	O
interpolant	O
that	O
passes	O
exactly	O
through	O
the	O
given	O
data	O
points	O
.	O
)	O
it	O
might	O
also	O
be	O
thought	O
that	O
the	O
computational	O
complexity	B
of	O
inference	B
when	O
we	O
work	O
with	O
priors	O
over	O
in	O
(	O
cid:12	O
)	O
nite-dimensional	O
function	B
spaces	O
might	O
be	O
in	O
(	O
cid:12	O
)	O
nitely	O
large	O
.	O
but	O
by	O
concentrating	O
on	O
the	O
joint	B
probability	O
distribution	B
of	O
the	O
observed	O
data	O
and	O
the	O
quantities	O
we	O
wish	O
to	O
predict	O
,	O
it	O
is	O
possible	O
to	O
make	O
predictions	O
with	O
resources	O
that	O
scale	O
as	O
polynomial	O
functions	B
of	O
n	O
,	O
the	O
number	O
of	O
data	O
points	O
.	O
45.1	O
standard	O
methods	O
for	O
nonlinear	O
regression	B
the	O
problem	O
we	O
are	O
given	O
n	O
data	O
points	O
xn	O
;	O
tn	O
=	O
fx	O
(	O
n	O
)	O
;	O
tngn	O
n=1	O
.	O
the	O
inputs	O
x	O
are	O
vec-	O
tors	O
of	O
some	O
(	O
cid:12	O
)	O
xed	O
input	O
dimension	O
i.	O
the	O
targets	O
t	O
are	O
either	O
real	O
numbers	O
,	O
in	O
which	O
case	O
the	O
task	O
will	O
be	O
a	O
regression	B
or	O
interpolation	O
task	O
,	O
or	O
they	O
are	O
categorical	O
variables	O
,	O
for	O
example	O
t	O
2	O
f0	O
;	O
1g	O
,	O
in	O
which	O
case	O
the	O
task	O
is	O
a	O
clas-	O
si	O
(	O
cid:12	O
)	O
cation	O
task	O
.	O
we	O
will	O
concentrate	O
on	O
the	O
case	O
of	O
regression	O
for	O
the	O
time	O
being	O
.	O
assuming	O
that	O
a	O
function	B
y	O
(	O
x	O
)	O
underlies	O
the	O
observed	O
data	O
,	O
the	O
task	O
is	O
to	O
infer	O
the	O
function	B
from	O
the	O
given	O
data	O
,	O
and	O
predict	O
the	O
function	B
’	O
s	O
value	O
{	O
or	O
the	O
value	O
of	O
the	O
observation	O
tn	O
+1	O
{	O
at	O
a	O
new	O
point	O
x	O
(	O
n	O
+1	O
)	O
.	O
parametric	O
approaches	O
to	O
the	O
problem	O
in	O
a	O
parametric	O
approach	O
to	O
regression	B
we	O
express	O
the	O
unknown	O
function	O
y	O
(	O
x	O
)	O
in	O
terms	O
of	O
a	O
nonlinear	B
function	O
y	O
(	O
x	O
;	O
w	O
)	O
parameterized	O
by	O
parameters	O
w.	O
example	O
45.2.	O
fixed	O
basis	O
functions	O
.	O
using	O
a	O
set	B
of	O
basis	O
functions	O
f	O
(	O
cid:30	O
)	O
h	O
(	O
x	O
)	O
gh	O
h=1	O
,	O
we	O
can	O
write	O
h	O
y	O
(	O
x	O
;	O
w	O
)	O
=	O
wh	O
(	O
cid:30	O
)	O
h	O
(	O
x	O
)	O
:	O
xh=1	O
if	O
the	O
basis	O
functions	O
are	O
nonlinear	B
functions	O
of	O
x	O
such	O
as	O
radial	O
basis	O
functions	O
centred	O
at	O
(	O
cid:12	O
)	O
xed	O
points	O
fchgh	O
(	O
cid:30	O
)	O
h	O
(	O
x	O
)	O
=	O
exp	O
(	O
cid:20	O
)	O
(	O
cid:0	O
)	O
h=1	O
,	O
(	O
x	O
(	O
cid:0	O
)	O
ch	O
)	O
2	O
(	O
cid:21	O
)	O
;	O
(	O
45.2	O
)	O
2r2	O
(	O
45.3	O
)	O
then	O
y	O
(	O
x	O
;	O
w	O
)	O
is	O
a	O
nonlinear	B
function	O
of	O
x	O
;	O
however	O
,	O
since	O
the	O
dependence	O
of	O
y	O
on	O
the	O
parameters	B
w	O
is	O
linear	B
,	O
we	O
might	O
sometimes	O
refer	O
to	O
this	O
as	O
a	O
‘	O
linear	B
’	O
model	B
.	O
in	O
neural	O
network	O
terms	O
,	O
this	O
model	B
is	O
like	O
a	O
multilayer	O
network	O
whose	O
connections	O
from	O
the	O
input	O
layer	O
to	O
the	O
nonlinear	B
hidden	O
layer	O
are	O
(	O
cid:12	O
)	O
xed	O
;	O
only	O
the	O
output	O
weights	O
w	O
are	O
adaptive	B
.	O
other	O
possible	O
sets	O
of	O
(	O
cid:12	O
)	O
xed	O
basis	O
functions	O
include	O
polynomials	O
such	O
as	O
(	O
cid:30	O
)	O
h	O
(	O
x	O
)	O
=	O
xp	O
j	O
where	O
p	O
and	O
q	O
are	O
integer	O
powers	O
that	O
depend	O
on	O
h.	O
i	O
xq	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
45.1	O
:	O
standard	O
methods	O
for	O
nonlinear	O
regression	B
537	O
example	O
45.3.	O
adaptive	B
basis	O
functions	B
.	O
alternatively	O
,	O
we	O
might	O
make	O
a	O
func-	O
tion	O
y	O
(	O
x	O
)	O
from	O
basis	O
functions	B
that	O
depend	O
on	O
additional	O
parameters	B
included	O
in	O
the	O
vector	O
w.	O
in	O
a	O
two-layer	O
feedforward	O
neural	B
network	I
with	O
nonlinear	B
hidden	O
units	B
and	O
a	O
linear	B
output	O
,	O
the	O
function	B
can	O
be	O
written	O
y	O
(	O
x	O
;	O
w	O
)	O
=	O
h	O
xh=1	O
w	O
(	O
2	O
)	O
h	O
tanh	O
i	O
xi=1	O
w	O
(	O
1	O
)	O
hi	O
xi	O
+	O
w	O
(	O
1	O
)	O
h0	O
!	O
+	O
w	O
(	O
2	O
)	O
0	O
(	O
45.4	O
)	O
where	O
i	O
is	O
the	O
dimensionality	O
of	O
the	O
input	O
space	O
and	O
the	O
weight	B
vector	O
hi	O
g	O
,	O
the	O
hidden	O
unit	O
biases	O
fw	O
(	O
1	O
)	O
h0	O
g	O
,	O
0	O
.	O
in	O
this	O
model	B
,	O
the	O
w	O
consists	O
of	O
the	O
input	O
weights	O
fw	O
(	O
1	O
)	O
the	O
output	O
weights	O
fw	O
(	O
2	O
)	O
h	O
g	O
and	O
the	O
output	O
bias	B
w	O
(	O
2	O
)	O
dependence	O
of	O
y	O
on	O
w	O
is	O
nonlinear	B
.	O
having	O
chosen	O
the	O
parameterization	O
,	O
we	O
then	O
infer	O
the	O
function	B
y	O
(	O
x	O
;	O
w	O
)	O
by	O
inferring	O
the	O
parameters	B
w.	O
the	O
posterior	B
probability	I
of	O
the	O
parameters	B
is	O
p	O
(	O
w	O
j	O
tn	O
;	O
xn	O
)	O
=	O
p	O
(	O
tn	O
j	O
w	O
;	O
xn	O
)	O
p	O
(	O
w	O
)	O
p	O
(	O
tn	O
j	O
xn	O
)	O
:	O
(	O
45.5	O
)	O
the	O
factor	O
p	O
(	O
tn	O
j	O
w	O
;	O
xn	O
)	O
states	O
the	O
probability	O
of	O
the	O
observed	O
data	O
points	O
when	O
the	O
parameters	B
w	O
(	O
and	O
hence	O
,	O
the	O
function	B
y	O
)	O
are	O
known	O
.	O
this	O
proba-	O
bility	O
distribution	B
is	O
often	O
taken	O
to	O
be	O
a	O
separable	O
gaussian	O
,	O
each	O
data	O
point	O
tn	O
di	O
(	O
cid:11	O
)	O
ering	O
from	O
the	O
underlying	O
value	O
y	O
(	O
x	O
(	O
n	O
)	O
;	O
w	O
)	O
by	O
additive	O
noise	B
.	O
the	O
factor	O
p	O
(	O
w	O
)	O
speci	O
(	O
cid:12	O
)	O
es	O
the	O
prior	B
probability	O
distribution	B
of	O
the	O
parameters	B
.	O
this	O
too	O
is	O
often	O
taken	O
to	O
be	O
a	O
separable	O
gaussian	O
distribution	B
.	O
if	O
the	O
dependence	O
of	O
y	O
on	O
w	O
is	O
nonlinear	B
the	O
posterior	O
distribution	O
p	O
(	O
w	O
j	O
tn	O
;	O
xn	O
)	O
is	O
in	O
general	O
not	O
a	O
gaussian	O
distribution	B
.	O
the	O
inference	B
can	O
be	O
implemented	O
in	O
various	O
ways	O
.	O
in	O
the	O
laplace	O
method	B
,	O
we	O
minimize	O
an	O
objective	B
function	I
m	O
(	O
w	O
)	O
=	O
(	O
cid:0	O
)	O
ln	O
[	O
p	O
(	O
tn	O
j	O
w	O
;	O
xn	O
)	O
p	O
(	O
w	O
)	O
]	O
(	O
45.6	O
)	O
with	O
respect	O
to	O
w	O
,	O
locating	O
the	O
locally	O
most	O
probable	O
parameters	O
,	O
then	O
use	O
the	O
curvature	O
of	O
m	O
,	O
@	O
2m	O
(	O
w	O
)	O
=	O
@	O
wi	O
@	O
wj	O
,	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
error	B
bars	I
on	O
w.	O
alternatively	O
we	O
can	O
use	O
more	O
general	O
markov	O
chain	B
monte	O
carlo	O
techniques	O
to	O
create	O
samples	O
from	O
the	O
posterior	O
distribution	O
p	O
(	O
w	O
j	O
tn	O
;	O
xn	O
)	O
.	O
having	O
obtained	O
one	O
of	O
these	O
representations	O
of	O
the	O
inference	O
of	O
w	O
given	O
the	O
data	O
,	O
predictions	O
are	O
then	O
made	O
by	O
marginalizing	O
over	O
the	O
parameters	B
:	O
p	O
(	O
tn	O
+1	O
j	O
tn	O
;	O
xn	O
+1	O
)	O
=z	O
dhw	O
p	O
(	O
tn	O
+1	O
j	O
w	O
;	O
x	O
(	O
n	O
+1	O
)	O
)	O
p	O
(	O
w	O
j	O
tn	O
;	O
xn	O
)	O
:	O
if	O
we	O
have	O
a	O
gaussian	O
representation	O
of	O
the	O
posterior	O
p	O
(	O
w	O
j	O
tn	O
;	O
xn	O
)	O
,	O
then	O
this	O
integral	B
can	O
typically	O
be	O
evaluated	O
directly	O
.	O
in	O
the	O
alternative	O
monte	O
carlo	O
approach	O
,	O
which	O
generates	O
r	O
samples	O
w	O
(	O
r	O
)	O
that	O
are	O
intended	O
to	O
be	O
samples	O
from	O
the	O
posterior	O
distribution	O
p	O
(	O
w	O
j	O
tn	O
;	O
xn	O
)	O
,	O
we	O
approximate	O
the	O
predictive	B
distribution	I
by	O
(	O
45.7	O
)	O
p	O
(	O
tn	O
+1	O
j	O
tn	O
;	O
xn	O
+1	O
)	O
’	O
1	O
r	O
r	O
xr=1	O
p	O
(	O
tn	O
+1	O
j	O
w	O
(	O
r	O
)	O
;	O
x	O
(	O
n	O
+1	O
)	O
)	O
:	O
(	O
45.8	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
45	O
|	O
gaussian	O
processes	O
538	O
nonparametric	B
approaches	O
.	O
in	O
nonparametric	O
methods	B
,	O
predictions	O
are	O
obtained	O
without	O
explicitly	O
pa-	O
rameterizing	O
the	O
unknown	O
function	O
y	O
(	O
x	O
)	O
;	O
y	O
(	O
x	O
)	O
lives	O
in	O
the	O
in	O
(	O
cid:12	O
)	O
nite-dimensional	O
space	O
of	O
all	O
continuous	B
functions	O
of	O
x.	O
one	O
well	O
known	O
nonparametric	B
ap-	O
proach	O
to	O
the	O
regression	B
problem	O
is	O
the	O
spline	B
smoothing	O
method	B
(	O
kimeldorf	O
and	O
wahba	O
,	O
1970	O
)	O
.	O
a	O
spline	B
solution	O
to	O
a	O
one-dimensional	O
regression	B
problem	O
can	O
be	O
described	O
as	O
follows	O
:	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
estimator	B
of	O
y	O
(	O
x	O
)	O
to	O
be	O
the	O
function	B
^y	O
(	O
x	O
)	O
that	O
minimizes	O
the	O
functional	O
m	O
(	O
y	O
(	O
x	O
)	O
)	O
=	O
1	O
2	O
(	O
cid:12	O
)	O
n	O
xn=1	O
(	O
y	O
(	O
x	O
(	O
n	O
)	O
)	O
(	O
cid:0	O
)	O
tn	O
)	O
2	O
+	O
1	O
2	O
(	O
cid:11	O
)	O
z	O
dx	O
[	O
y	O
(	O
p	O
)	O
(	O
x	O
)	O
]	O
2	O
;	O
(	O
45.9	O
)	O
where	O
y	O
(	O
p	O
)	O
is	O
the	O
pth	O
derivative	O
of	O
y	O
and	O
p	O
is	O
a	O
positive	O
number	O
.	O
if	O
p	O
is	O
set	B
to	O
2	O
then	O
the	O
resulting	O
function	B
^y	O
(	O
x	O
)	O
is	O
a	O
cubic	O
spline	B
,	O
that	O
is	O
,	O
a	O
piecewise	O
cubic	O
function	B
that	O
has	O
‘	O
knots	O
’	O
{	O
discontinuities	O
in	O
its	O
second	O
derivative	O
{	O
at	O
the	O
data	O
points	O
fx	O
(	O
n	O
)	O
g.	O
tifying	O
the	O
prior	B
for	O
the	O
function	B
y	O
(	O
x	O
)	O
as	O
:	O
this	O
estimation	O
method	B
can	O
be	O
interpreted	O
as	O
a	O
bayesian	O
method	B
by	O
iden-	O
ln	O
p	O
(	O
y	O
(	O
x	O
)	O
j	O
(	O
cid:11	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
z	O
dx	O
[	O
y	O
(	O
p	O
)	O
(	O
x	O
)	O
]	O
2	O
+	O
const	O
;	O
and	O
the	O
probability	O
of	O
the	O
data	O
measurements	O
tn	O
=	O
ftngn	O
pendent	O
gaussian	O
noise	B
as	O
:	O
1	O
2	O
(	O
45.10	O
)	O
n=1	O
assuming	O
inde-	O
ln	O
p	O
(	O
tn	O
j	O
y	O
(	O
x	O
)	O
;	O
(	O
cid:12	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
cid:12	O
)	O
n	O
xn=1	O
(	O
y	O
(	O
x	O
(	O
n	O
)	O
)	O
(	O
cid:0	O
)	O
tn	O
)	O
2	O
+	O
const	O
:	O
(	O
45.11	O
)	O
[	O
the	O
constants	O
in	O
equations	O
(	O
45.10	O
)	O
and	O
(	O
45.11	O
)	O
are	O
functions	B
of	O
(	O
cid:11	O
)	O
and	O
(	O
cid:12	O
)	O
re-	O
spectively	O
.	O
strictly	O
the	O
prior	B
(	O
45.10	O
)	O
is	O
improper	B
since	O
addition	O
of	O
an	O
arbitrary	O
polynomial	O
of	O
degree	O
(	O
p	O
(	O
cid:0	O
)	O
1	O
)	O
to	O
y	O
(	O
x	O
)	O
is	O
not	O
constrained	B
.	O
this	O
impropriety	O
is	O
easily	O
recti	O
(	O
cid:12	O
)	O
ed	O
by	O
the	O
addition	O
of	O
(	O
p	O
(	O
cid:0	O
)	O
1	O
)	O
appropriate	O
terms	O
to	O
(	O
45.10	O
)	O
.	O
]	O
given	O
this	O
interpretation	O
of	O
the	O
functions	O
in	O
equation	O
(	O
45.9	O
)	O
,	O
m	O
(	O
y	O
(	O
x	O
)	O
)	O
is	O
equal	O
to	O
mi-	O
nus	O
the	O
log	O
of	O
the	O
posterior	O
probability	B
p	O
(	O
y	O
(	O
x	O
)	O
j	O
tn	O
;	O
(	O
cid:11	O
)	O
;	O
(	O
cid:12	O
)	O
)	O
,	O
within	O
an	O
additive	O
constant	O
,	O
and	O
the	O
splines	O
estimation	O
procedure	O
can	O
be	O
interpreted	O
as	O
yielding	O
a	O
bayesian	O
map	O
estimate	O
.	O
the	O
bayesian	O
perspective	O
allows	O
us	O
additionally	O
to	O
put	O
error	B
bars	I
on	O
the	O
splines	O
estimate	O
and	O
to	O
draw	O
typical	O
samples	O
from	O
the	O
posterior	O
distribution	O
,	O
and	O
it	O
gives	O
an	O
automatic	O
method	O
for	O
inferring	O
the	O
hyperparameters	O
(	O
cid:11	O
)	O
and	O
(	O
cid:12	O
)	O
.	O
comments	O
splines	O
priors	O
are	O
gaussian	O
processes	O
the	O
prior	B
distribution	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
equation	O
(	O
45.10	O
)	O
is	O
our	O
(	O
cid:12	O
)	O
rst	O
example	O
of	O
a	O
gaussian	O
process	O
.	O
throwing	O
mathematical	O
precision	B
to	O
the	O
winds	O
,	O
a	O
gaussian	O
process	O
can	O
be	O
de	O
(	O
cid:12	O
)	O
ned	O
as	O
a	O
probability	B
distribution	O
on	O
a	O
space	O
of	O
functions	O
y	O
(	O
x	O
)	O
that	O
can	O
be	O
written	O
in	O
the	O
form	O
p	O
(	O
y	O
(	O
x	O
)	O
j	O
(	O
cid:22	O
)	O
(	O
x	O
)	O
;	O
a	O
)	O
=	O
1	O
z	O
exp	O
(	O
cid:20	O
)	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
y	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
x	O
)	O
)	O
ta	O
(	O
y	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
(	O
cid:22	O
)	O
(	O
x	O
)	O
)	O
(	O
cid:21	O
)	O
;	O
(	O
45.12	O
)	O
where	O
(	O
cid:22	O
)	O
(	O
x	O
)	O
is	O
the	O
mean	B
function	O
and	O
a	O
is	O
a	O
linear	B
operator	O
,	O
and	O
where	O
the	O
inner	O
product	O
of	O
two	O
functions	B
y	O
(	O
x	O
)	O
tz	O
(	O
x	O
)	O
is	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
,	O
for	O
example	O
,	O
r	O
dx	O
y	O
(	O
x	O
)	O
z	O
(	O
x	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
45.1	O
:	O
standard	O
methods	O
for	O
nonlinear	O
regression	B
539	O
here	O
,	O
if	O
we	O
denote	O
by	O
d	O
the	O
linear	B
operator	O
that	O
maps	O
y	O
(	O
x	O
)	O
to	O
the	O
derivative	O
of	O
y	O
(	O
x	O
)	O
,	O
we	O
can	O
write	O
equation	O
(	O
45.10	O
)	O
as	O
ln	O
p	O
(	O
y	O
(	O
x	O
)	O
j	O
(	O
cid:11	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
cid:11	O
)	O
z	O
dx	O
[	O
dpy	O
(	O
x	O
)	O
]	O
2	O
+	O
const	O
=	O
(	O
cid:0	O
)	O
1	O
2	O
y	O
(	O
x	O
)	O
tay	O
(	O
x	O
)	O
+	O
const	O
;	O
(	O
45.13	O
)	O
which	O
has	O
the	O
same	O
form	O
as	O
equation	O
(	O
45.12	O
)	O
with	O
(	O
cid:22	O
)	O
(	O
x	O
)	O
=	O
0	O
,	O
and	O
a	O
(	O
cid:17	O
)	O
[	O
d	O
p	O
]	O
tdp	O
.	O
in	O
order	O
for	O
the	O
prior	B
in	O
equation	O
(	O
45.12	O
)	O
to	O
be	O
a	O
proper	B
prior	O
,	O
a	O
must	O
be	O
a	O
positive	O
de	O
(	O
cid:12	O
)	O
nite	O
operator	O
,	O
i.e.	O
,	O
one	O
satisfying	O
y	O
(	O
x	O
)	O
tay	O
(	O
x	O
)	O
>	O
0	O
for	O
all	O
functions	B
y	O
(	O
x	O
)	O
other	O
than	O
y	O
(	O
x	O
)	O
=	O
0.	O
splines	O
can	O
be	O
written	O
as	O
parametric	O
models	O
splines	O
may	O
be	O
written	O
in	O
terms	O
of	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
set	B
of	O
(	O
cid:12	O
)	O
xed	O
basis	O
functions	O
,	O
as	O
in	O
equation	O
(	O
45.2	O
)	O
,	O
as	O
follows	O
.	O
first	O
rescale	O
the	O
x	O
axis	O
so	O
that	O
the	O
interval	O
(	O
0	O
;	O
2	O
(	O
cid:25	O
)	O
)	O
is	O
much	O
wider	O
than	O
the	O
range	O
of	O
x	O
values	O
of	O
interest	O
.	O
let	O
the	O
basis	O
functions	O
be	O
a	O
fourier	O
set	B
fcos	O
hx	O
;	O
sin	O
hx	O
,	O
h	O
=	O
0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
g	O
,	O
so	O
the	O
function	B
is	O
wh	O
(	O
cos	O
)	O
cos	O
(	O
hx	O
)	O
+	O
wh	O
(	O
sin	O
)	O
sin	O
(	O
hx	O
)	O
:	O
y	O
(	O
x	O
)	O
=	O
use	O
the	O
regularizer	O
1xh=0	O
ew	O
(	O
w	O
)	O
=	O
1	O
2	O
h	O
1xh=0	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
gaussian	O
prior	B
on	O
w	O
,	O
1xh=1	O
1xh=1	O
p	O
2	O
w2	O
h	O
(	O
cos	O
)	O
+	O
1	O
2	O
h	O
p	O
2	O
w2	O
h	O
(	O
sin	O
)	O
(	O
45.14	O
)	O
(	O
45.15	O
)	O
(	O
45.16	O
)	O
p	O
(	O
w	O
j	O
(	O
cid:11	O
)	O
)	O
=	O
1	O
zw	O
(	O
(	O
cid:11	O
)	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:11	O
)	O
ew	O
)	O
:	O
if	O
p	O
=	O
2	O
then	O
we	O
have	O
the	O
cubic	O
splines	O
regularizer	O
ew	O
(	O
w	O
)	O
=r	O
y	O
(	O
2	O
)	O
(	O
x	O
)	O
2	O
dx	O
,	O
as	O
if	O
p	O
=	O
1	O
we	O
have	O
the	O
regularizer	O
ew	O
(	O
w	O
)	O
=r	O
y	O
(	O
1	O
)	O
(	O
x	O
)	O
2	O
dx	O
,	O
in	O
equation	O
(	O
45.9	O
)	O
;	O
(	O
to	O
make	O
the	O
prior	B
proper	O
we	O
must	O
add	O
an	O
extra	O
regularizer	O
on	O
the	O
etc	O
.	O
term	O
w0	O
(	O
cos	O
)	O
.	O
)	O
thus	O
in	O
terms	O
of	O
the	O
prior	O
p	O
(	O
y	O
(	O
x	O
)	O
)	O
there	O
is	O
no	O
fundamental	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
the	O
‘	O
nonparametric	B
’	O
splines	O
approach	O
and	O
other	O
parametric	O
approaches	O
.	O
representation	O
is	O
irrelevant	O
for	O
prediction	O
from	O
the	O
point	O
of	O
view	O
of	O
prediction	O
at	O
least	O
,	O
there	O
are	O
two	O
objects	O
of	O
inter-	O
est	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
is	O
the	O
conditional	B
distribution	O
p	O
(	O
tn	O
+1	O
j	O
tn	O
;	O
xn	O
+1	O
)	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
equation	O
(	O
45.7	O
)	O
.	O
the	O
other	O
object	O
of	O
interest	O
,	O
should	O
we	O
wish	O
to	O
compare	O
one	O
model	B
with	O
others	O
,	O
is	O
the	O
joint	B
probability	O
of	O
all	O
the	O
observed	O
data	O
given	O
the	O
model	B
,	O
the	O
evidence	B
p	O
(	O
tn	O
j	O
xn	O
)	O
,	O
which	O
appeared	O
as	O
the	O
normalizing	B
constant	I
in	O
equation	O
(	O
45.5	O
)	O
.	O
neither	O
of	O
these	O
quantities	O
makes	O
any	O
reference	O
to	O
the	O
rep-	O
resentation	O
of	O
the	O
unknown	O
function	B
y	O
(	O
x	O
)	O
.	O
so	O
at	O
the	O
end	O
of	O
the	O
day	O
,	O
our	O
choice	O
of	O
representation	O
is	O
irrelevant	O
.	O
the	O
question	O
we	O
now	O
address	B
is	O
,	O
in	O
the	O
case	O
of	O
popular	O
parametric	O
models	O
,	O
what	O
form	O
do	O
these	O
two	O
quantities	O
take	O
?	O
we	O
will	O
see	O
that	O
for	O
standard	O
models	O
with	O
(	O
cid:12	O
)	O
xed	O
basis	O
functions	O
and	O
gaussian	O
distributions	O
on	O
the	O
unknown	O
parame-	O
ters	O
,	O
the	O
joint	B
probability	O
of	O
all	O
the	O
observed	O
data	O
given	O
the	O
model	B
,	O
p	O
(	O
tn	O
j	O
xn	O
)	O
,	O
is	O
a	O
multivariate	O
gaussian	O
distribution	B
with	O
mean	B
zero	O
and	O
with	O
a	O
covariance	B
matrix	I
determined	O
by	O
the	O
basis	O
functions	O
;	O
this	O
implies	O
that	O
the	O
conditional	B
distribution	O
p	O
(	O
tn	O
+1	O
j	O
tn	O
;	O
xn	O
+1	O
)	O
is	O
also	O
a	O
gaussian	O
distribution	B
,	O
whose	O
mean	B
depends	O
linearly	O
on	O
the	O
values	O
of	O
the	O
targets	O
tn	O
.	O
standard	O
parametric	O
models	O
are	O
simple	O
examples	O
of	O
gaussian	O
processes	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
540	O
45	O
|	O
gaussian	O
processes	O
45.2	O
from	O
parametric	O
models	O
to	O
gaussian	O
processes	O
linear	B
models	O
let	O
us	O
consider	O
a	O
regression	B
problem	O
using	O
h	O
(	O
cid:12	O
)	O
xed	O
basis	O
functions	O
,	O
for	O
example	O
one-dimensional	O
radial	O
basis	O
functions	O
as	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
equation	O
(	O
45.3	O
)	O
.	O
let	O
us	O
assume	O
that	O
a	O
list	O
of	O
n	O
input	O
points	O
fx	O
(	O
n	O
)	O
g	O
has	O
been	O
speci	O
(	O
cid:12	O
)	O
ed	O
and	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
n	O
(	O
cid:2	O
)	O
h	O
matrix	B
r	O
to	O
be	O
the	O
matrix	B
of	O
values	O
of	O
the	O
basis	O
functions	B
f	O
(	O
cid:30	O
)	O
h	O
(	O
x	O
)	O
gh	O
h=1	O
at	O
the	O
points	O
fxng	O
,	O
rnh	O
(	O
cid:17	O
)	O
(	O
cid:30	O
)	O
h	O
(	O
x	O
(	O
n	O
)	O
)	O
:	O
(	O
45.17	O
)	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
vector	O
yn	O
to	O
be	O
the	O
vector	O
of	O
values	O
of	O
y	O
(	O
x	O
)	O
at	O
the	O
n	O
points	O
,	O
yn	O
(	O
cid:17	O
)	O
xh	O
rnhwh	O
:	O
(	O
45.18	O
)	O
if	O
the	O
prior	B
distribution	O
of	O
w	O
is	O
gaussian	O
with	O
zero	O
mean	B
,	O
p	O
(	O
w	O
)	O
=	O
normal	B
(	O
w	O
;	O
0	O
;	O
(	O
cid:27	O
)	O
2	O
wi	O
)	O
;	O
(	O
45.19	O
)	O
then	O
y	O
,	O
being	O
a	O
linear	B
function	O
of	O
w	O
,	O
is	O
also	O
gaussian	O
distributed	O
,	O
with	O
mean	O
zero	O
.	O
the	O
covariance	B
matrix	I
of	O
y	O
is	O
q	O
=	O
hyyti	O
=	O
hrwwtrti	O
=	O
rhwwti	O
rt	O
=	O
(	O
cid:27	O
)	O
2	O
wrrt	O
:	O
(	O
45.20	O
)	O
(	O
45.21	O
)	O
so	O
the	O
prior	B
distribution	O
of	O
y	O
is	O
:	O
p	O
(	O
y	O
)	O
=	O
normal	B
(	O
y	O
;	O
0	O
;	O
q	O
)	O
=	O
normal	B
(	O
y	O
;	O
0	O
;	O
(	O
cid:27	O
)	O
2	O
wrrt	O
)	O
:	O
(	O
45.22	O
)	O
this	O
result	O
,	O
that	O
the	O
vector	O
of	O
n	O
function	B
values	O
y	O
has	O
a	O
gaussian	O
distribu-	O
tion	O
,	O
is	O
true	O
for	O
any	O
selected	O
points	O
xn	O
.	O
this	O
is	O
the	O
de	O
(	O
cid:12	O
)	O
ning	O
property	O
of	O
a	O
gaussian	O
process	O
.	O
the	O
probability	B
distribution	O
of	O
a	O
function	B
y	O
(	O
x	O
)	O
is	O
a	O
gaus-	O
sian	O
process	O
if	O
for	O
any	O
(	O
cid:12	O
)	O
nite	O
selection	O
of	O
points	O
x	O
(	O
1	O
)	O
;	O
x	O
(	O
2	O
)	O
;	O
:	O
:	O
:	O
;	O
x	O
(	O
n	O
)	O
,	O
the	O
density	B
p	O
(	O
y	O
(	O
x	O
(	O
1	O
)	O
)	O
;	O
y	O
(	O
x	O
(	O
2	O
)	O
)	O
;	O
:	O
:	O
:	O
;	O
y	O
(	O
x	O
(	O
n	O
)	O
)	O
)	O
is	O
a	O
gaussian	O
.	O
now	O
,	O
if	O
the	O
number	O
of	O
basis	O
functions	B
h	O
is	O
smaller	O
than	O
the	O
number	O
of	O
data	O
points	O
n	O
,	O
then	O
the	O
matrix	B
q	O
will	O
not	O
have	O
full	O
rank	O
.	O
in	O
this	O
case	O
the	O
probability	B
distribution	O
of	O
y	O
might	O
be	O
thought	O
of	O
as	O
a	O
(	O
cid:13	O
)	O
at	O
elliptical	O
pancake	O
con	O
(	O
cid:12	O
)	O
ned	O
to	O
an	O
h-dimensional	O
subspace	O
in	O
the	O
n	O
-dimensional	O
space	O
in	O
which	O
y	O
lives	O
.	O
what	O
about	O
the	O
target	O
values	O
?	O
if	O
each	O
target	O
tn	O
is	O
assumed	O
to	O
di	O
(	O
cid:11	O
)	O
er	O
by	O
(	O
cid:23	O
)	O
from	O
the	O
corresponding	O
function	B
value	O
additive	O
gaussian	O
noise	B
of	O
variance	B
(	O
cid:27	O
)	O
2	O
yn	O
then	O
t	O
also	O
has	O
a	O
gaussian	O
prior	B
distribution	O
,	O
p	O
(	O
t	O
)	O
=	O
normal	B
(	O
t	O
;	O
0	O
;	O
q	O
+	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
i	O
)	O
:	O
we	O
will	O
denote	O
the	O
covariance	B
matrix	I
of	O
t	O
by	O
c	O
:	O
c	O
=	O
q	O
+	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
i	O
=	O
(	O
cid:27	O
)	O
2	O
wrrt	O
+	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
i	O
:	O
(	O
45.23	O
)	O
(	O
45.24	O
)	O
whether	O
or	O
not	O
q	O
has	O
full	O
rank	O
,	O
the	O
covariance	B
matrix	I
c	O
has	O
full	O
rank	O
since	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
i	O
is	O
full	O
rank	O
.	O
what	O
does	O
the	O
covariance	B
matrix	I
q	O
look	O
like	O
?	O
in	O
general	O
,	O
the	O
(	O
n	O
;	O
n0	O
)	O
entry	O
of	O
q	O
is	O
qnn0	O
=	O
[	O
(	O
cid:27	O
)	O
2	O
wrrt	O
]	O
nn0	O
=	O
(	O
cid:27	O
)	O
2	O
wxh	O
(	O
cid:30	O
)	O
h	O
(	O
x	O
(	O
n	O
)	O
)	O
(	O
cid:30	O
)	O
h	O
(	O
x	O
(	O
n0	O
)	O
)	O
(	O
45.25	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
45.2	O
:	O
from	O
parametric	O
models	O
to	O
gaussian	O
processes	O
541	O
and	O
the	O
(	O
n	O
;	O
n0	O
)	O
entry	O
of	O
c	O
is	O
cnn0	O
=	O
(	O
cid:27	O
)	O
2	O
wxh	O
(	O
cid:30	O
)	O
h	O
(	O
x	O
(	O
n	O
)	O
)	O
(	O
cid:30	O
)	O
h	O
(	O
x	O
(	O
n0	O
)	O
)	O
+	O
(	O
cid:14	O
)	O
nn0	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
;	O
(	O
45.26	O
)	O
where	O
(	O
cid:14	O
)	O
nn0	O
=	O
1	O
if	O
n	O
=	O
n0	O
and	O
0	O
otherwise	O
.	O
example	O
45.4.	O
let	O
’	O
s	O
take	O
as	O
an	O
example	O
a	O
one-dimensional	O
case	O
,	O
with	O
radial	O
basis	O
functions	O
.	O
the	O
expression	O
for	O
qnn0	O
becomes	O
simplest	O
if	O
we	O
assume	O
we	O
have	O
uniformly-spaced	O
basis	O
functions	O
with	O
the	O
basis	O
function	O
labelled	O
h	O
cen-	O
tred	O
on	O
the	O
point	O
x	O
=	O
h	O
,	O
and	O
take	O
the	O
limit	O
h	O
!	O
1	O
,	O
so	O
that	O
the	O
sum	O
over	O
h	O
becomes	O
an	O
integral	B
;	O
to	O
avoid	O
having	O
a	O
covariance	B
that	O
diverges	O
with	O
h	O
,	O
we	O
had	O
better	O
make	O
(	O
cid:27	O
)	O
2	O
w	O
scale	O
as	O
s=	O
(	O
(	O
cid:1	O
)	O
h	O
)	O
,	O
where	O
(	O
cid:1	O
)	O
h	O
is	O
the	O
number	O
of	O
basis	O
functions	B
per	O
unit	O
length	B
of	O
the	O
x-axis	O
,	O
and	O
s	O
is	O
a	O
constant	O
;	O
then	O
qnn0	O
=	O
sz	O
hmax	O
=	O
sz	O
hmax	O
hmin	O
hmin	O
#	O
:	O
if	O
we	O
let	O
the	O
limits	O
of	O
integration	O
be	O
(	O
cid:6	O
)	O
1	O
,	O
we	O
can	O
solve	O
this	O
integral	B
:	O
dh	O
exp	O
''	O
(	O
cid:0	O
)	O
#	O
exp	O
''	O
(	O
cid:0	O
)	O
(	O
x	O
(	O
n0	O
)	O
(	O
cid:0	O
)	O
h	O
)	O
2	O
2r2	O
dh	O
(	O
cid:30	O
)	O
h	O
(	O
x	O
(	O
n	O
)	O
)	O
(	O
cid:30	O
)	O
h	O
(	O
x	O
(	O
n0	O
)	O
)	O
(	O
x	O
(	O
n	O
)	O
(	O
cid:0	O
)	O
h	O
)	O
2	O
2r2	O
qnn0	O
=	O
p	O
(	O
cid:25	O
)	O
r2	O
s	O
exp	O
''	O
(	O
cid:0	O
)	O
(	O
x	O
(	O
n0	O
)	O
(	O
cid:0	O
)	O
x	O
(	O
n	O
)	O
)	O
2	O
4r2	O
#	O
:	O
(	O
45.27	O
)	O
(	O
45.28	O
)	O
(	O
45.29	O
)	O
we	O
are	O
arriving	O
at	O
a	O
new	O
perspective	O
on	O
the	O
interpolation	O
problem	O
.	O
instead	O
of	O
specifying	O
the	O
prior	B
distribution	O
on	O
functions	O
in	O
terms	O
of	O
basis	O
functions	B
and	O
priors	O
on	O
parameters	O
,	O
the	O
prior	B
can	O
be	O
summarized	O
simply	O
by	O
a	O
covariance	B
function	I
,	O
c	O
(	O
x	O
(	O
n	O
)	O
;	O
x	O
(	O
n0	O
)	O
)	O
(	O
cid:17	O
)	O
(	O
cid:18	O
)	O
1	O
exp	O
''	O
(	O
cid:0	O
)	O
(	O
x	O
(	O
n0	O
)	O
(	O
cid:0	O
)	O
x	O
(	O
n	O
)	O
)	O
2	O
4r2	O
#	O
;	O
(	O
45.30	O
)	O
where	O
we	O
have	O
given	O
a	O
new	O
name	O
,	O
(	O
cid:18	O
)	O
1	O
,	O
to	O
the	O
constant	O
out	O
front	O
.	O
generalizing	O
from	O
this	O
particular	O
case	O
,	O
a	O
vista	O
of	O
interpolation	O
methods	B
opens	O
up	O
.	O
given	O
any	O
valid	O
covariance	B
function	I
c	O
(	O
x	O
;	O
x0	O
)	O
{	O
we	O
’	O
ll	O
discuss	O
in	O
a	O
moment	O
what	O
‘	O
valid	O
’	O
means	O
{	O
we	O
can	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
covariance	B
matrix	I
for	O
n	O
function	B
values	O
at	O
locations	O
xn	O
to	O
be	O
the	O
matrix	B
q	O
given	O
by	O
qnn0	O
=	O
c	O
(	O
x	O
(	O
n	O
)	O
;	O
x	O
(	O
n0	O
)	O
)	O
(	O
45.31	O
)	O
and	O
the	O
covariance	B
matrix	I
for	O
n	O
corresponding	O
target	O
values	O
,	O
assuming	O
gaus-	O
sian	O
noise	B
,	O
to	O
be	O
the	O
matrix	B
c	O
given	O
by	O
cnn0	O
=	O
c	O
(	O
x	O
(	O
n	O
)	O
;	O
x	O
(	O
n0	O
)	O
)	O
+	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
(	O
cid:14	O
)	O
nn0	O
:	O
(	O
45.32	O
)	O
in	O
conclusion	O
,	O
the	O
prior	B
probability	O
of	O
the	O
n	O
target	O
values	O
t	O
in	O
the	O
data	B
set	I
is	O
:	O
p	O
(	O
t	O
)	O
=	O
normal	B
(	O
t	O
;	O
0	O
;	O
c	O
)	O
=	O
1	O
z	O
e	O
(	O
cid:0	O
)	O
1	O
2	O
ttc	O
(	O
cid:0	O
)	O
1t	O
:	O
(	O
45.33	O
)	O
samples	O
from	O
this	O
gaussian	O
process	O
and	O
a	O
few	O
other	O
simple	O
gaussian	O
processes	O
are	O
displayed	O
in	O
(	O
cid:12	O
)	O
gure	O
45.1.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
542	O
3.0	O
2.0	O
1.0	O
0.0	O
t	O
−1.0	O
−2.0	O
−3.0	O
4.0	O
2.0	O
t	O
0.0	O
−2.0	O
−4.0	O
−3.0	O
−1.0	O
1.0	O
3.0	O
5.0	O
x	O
2	O
(	O
1:5	O
)	O
2	O
(	O
cid:17	O
)	O
(	O
a	O
)	O
2	O
exp	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
(	O
x	O
(	O
cid:0	O
)	O
x0	O
)	O
2	O
−1.0	O
1.0	O
3.0	O
5.0	O
4.0	O
2.0	O
t	O
0.0	O
−2.0	O
−4.0	O
−3.0	O
6.0	O
4.0	O
2.0	O
0.0	O
t	O
−2.0	O
−4.0	O
−3.0	O
−1.0	O
1.0	O
3.0	O
5.0	O
x	O
(	O
b	O
)	O
2	O
exp	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
(	O
x	O
(	O
cid:0	O
)	O
x0	O
)	O
2	O
2	O
(	O
0:35	O
)	O
2	O
(	O
cid:17	O
)	O
−1.0	O
1.0	O
3.0	O
5.0	O
45	O
|	O
gaussian	O
processes	O
figure	O
45.1.	O
samples	O
drawn	O
from	O
gaussian	O
process	O
priors	O
.	O
each	O
panel	O
shows	O
two	O
functions	B
drawn	O
from	O
a	O
gaussian	O
process	O
prior	B
.	O
the	O
four	O
corresponding	O
covariance	B
functions	O
are	O
given	O
below	O
each	O
plot	O
.	O
the	O
decrease	O
in	O
lengthscale	O
from	O
(	O
a	O
)	O
to	O
(	O
b	O
)	O
produces	O
more	O
rapidly	O
(	O
cid:13	O
)	O
uctuating	O
functions	B
.	O
the	O
periodic	O
properties	O
of	O
the	O
covariance	O
function	B
in	O
(	O
c	O
)	O
can	O
be	O
seen	O
.	O
the	O
covariance	B
function	I
in	O
(	O
d	O
)	O
contains	O
the	O
non-stationary	O
term	O
xx0	O
corresponding	O
to	O
the	O
covariance	B
of	O
a	O
straight	O
line	O
,	O
so	O
that	O
typical	B
functions	O
include	O
linear	B
trends	O
.	O
from	O
gibbs	O
(	O
1997	O
)	O
.	O
x	O
(	O
c	O
)	O
2	O
exp	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
sin2	O
(	O
(	O
cid:25	O
)	O
(	O
x	O
(	O
cid:0	O
)	O
x0	O
)	O
=3:0	O
)	O
2	O
(	O
0:5	O
)	O
2	O
(	O
cid:17	O
)	O
x	O
(	O
d	O
)	O
2	O
exp	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
(	O
x	O
(	O
cid:0	O
)	O
x0	O
)	O
2	O
2	O
(	O
1:5	O
)	O
2	O
(	O
cid:17	O
)	O
+	O
xx0	O
multilayer	O
neural	O
networks	O
and	O
gaussian	O
processes	O
figures	O
44.2	O
and	O
44.3	O
show	O
some	O
random	B
samples	O
from	O
the	O
prior	B
distribution	O
over	O
functions	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
a	O
selection	O
of	O
standard	O
multilayer	O
perceptrons	O
with	O
large	O
numbers	O
of	O
hidden	O
units	B
.	O
those	O
samples	O
don	O
’	O
t	O
seem	O
a	O
million	O
miles	O
away	O
from	O
the	O
gaussian	O
process	O
samples	O
of	O
(	O
cid:12	O
)	O
gure	O
45.1.	O
and	O
indeed	O
neal	O
(	O
1996	O
)	O
showed	O
that	O
the	O
properties	O
of	O
a	O
neural	B
network	I
with	O
one	O
hidden	O
layer	O
(	O
as	O
in	O
equation	O
(	O
45.4	O
)	O
)	O
converge	O
to	O
those	O
of	O
a	O
gaussian	O
process	O
as	O
the	O
number	O
of	O
hidden	O
neurons	O
tends	O
to	O
in	O
(	O
cid:12	O
)	O
nity	O
,	O
if	O
standard	O
‘	O
weight	B
decay	I
’	O
priors	O
are	O
assumed	O
.	O
the	O
covariance	B
function	I
of	O
this	O
gaussian	O
process	O
depends	O
on	O
the	O
details	O
of	O
the	O
priors	O
assumed	O
for	O
the	O
weights	O
in	O
the	O
network	B
and	O
the	O
activation	O
functions	O
of	O
the	O
hidden	O
units	B
.	O
45.3	O
using	O
a	O
given	O
gaussian	O
process	O
model	B
in	O
regression	B
we	O
have	O
spent	O
some	O
time	O
talking	O
about	O
priors	O
.	O
we	O
now	O
return	O
to	O
our	O
data	O
and	O
the	O
problem	O
of	O
prediction	O
.	O
how	O
do	O
we	O
make	O
predictions	O
with	O
a	O
gaussian	O
process	O
?	O
having	O
formed	O
the	O
covariance	B
matrix	I
c	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
equation	O
(	O
45.32	O
)	O
our	O
task	O
is	O
to	O
infer	O
tn	O
+1	O
given	O
the	O
observed	O
vector	O
tn	O
.	O
the	O
joint	B
density	O
p	O
(	O
tn	O
+1	O
;	O
tn	O
)	O
is	O
a	O
gaussian	O
;	O
so	O
the	O
conditional	B
distribution	O
p	O
(	O
tn	O
+1	O
j	O
tn	O
)	O
=	O
p	O
(	O
tn	O
+1	O
;	O
tn	O
)	O
p	O
(	O
tn	O
)	O
(	O
45.34	O
)	O
is	O
also	O
a	O
gaussian	O
.	O
we	O
now	O
distinguish	O
between	O
di	O
(	O
cid:11	O
)	O
erent	O
sizes	O
of	O
covariance	O
matrix	B
c	O
with	O
a	O
subscript	O
,	O
such	O
that	O
cn	O
+1	O
is	O
the	O
(	O
n	O
+	O
1	O
)	O
(	O
cid:2	O
)	O
(	O
n	O
+	O
1	O
)	O
covariance	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
45.4	O
:	O
examples	O
of	O
covariance	O
functions	B
543	O
matrix	B
for	O
the	O
vector	O
tn	O
+1	O
(	O
cid:17	O
)	O
(	O
t1	O
;	O
:	O
:	O
:	O
;	O
tn	O
+1	O
)	O
t.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
submatrices	O
of	O
cn	O
+1	O
as	O
follows	O
:	O
cn	O
+1	O
(	O
cid:17	O
)	O
:	O
3	O
2	O
4	O
k3	O
2	O
4	O
cn	O
3	O
2	O
7775	O
6664	O
5	O
5	O
(	O
cid:3	O
)	O
(	O
cid:2	O
)	O
(	O
cid:20	O
)	O
(	O
cid:3	O
)	O
(	O
cid:2	O
)	O
kt	O
tn	O
+1	O
(	O
cid:21	O
)	O
(	O
cid:21	O
)	O
:	O
n	O
+1	O
(	O
cid:20	O
)	O
tn	O
2	O
(	O
cid:2	O
)	O
tn	O
tn	O
+1	O
(	O
cid:3	O
)	O
c	O
(	O
cid:0	O
)	O
1	O
1	O
p	O
(	O
tn	O
+1	O
j	O
tn	O
)	O
/	O
exp	O
(	O
cid:20	O
)	O
(	O
cid:0	O
)	O
the	O
posterior	O
distribution	O
(	O
45.34	O
)	O
is	O
given	O
by	O
(	O
45.35	O
)	O
(	O
45.36	O
)	O
we	O
can	O
evaluate	O
the	O
mean	B
and	O
standard	B
deviation	I
of	O
the	O
posterior	O
distribution	O
of	O
tn	O
+1	O
by	O
brute-force	O
inversion	O
of	O
cn	O
+1	O
.	O
there	O
is	O
a	O
more	O
elegant	O
expression	O
for	O
the	O
predictive	B
distribution	I
,	O
however	O
,	O
which	O
is	O
useful	O
whenever	O
predictions	O
are	O
to	O
be	O
made	O
at	O
a	O
number	O
of	O
new	O
points	O
on	O
the	O
basis	O
of	O
the	O
data	B
set	I
of	O
size	O
n	O
.	O
we	O
can	O
write	O
c	O
(	O
cid:0	O
)	O
1	O
n	O
using	O
the	O
partitioned	B
inverse	I
equations	O
(	O
barnett	O
,	O
1979	O
)	O
:	O
n	O
+1	O
in	O
terms	O
of	O
cn	O
and	O
c	O
(	O
cid:0	O
)	O
1	O
c	O
(	O
cid:0	O
)	O
1	O
n	O
+1	O
=	O
(	O
cid:20	O
)	O
m	O
m	O
mt	O
m	O
(	O
cid:21	O
)	O
where	O
m	O
=	O
(	O
cid:0	O
)	O
(	O
cid:20	O
)	O
(	O
cid:0	O
)	O
kt	O
c	O
(	O
cid:0	O
)	O
1	O
m	O
=	O
(	O
cid:0	O
)	O
m	O
c	O
(	O
cid:0	O
)	O
1	O
n	O
k	O
1	O
m	O
=	O
c	O
(	O
cid:0	O
)	O
1	O
n	O
+	O
m	O
n	O
k	O
(	O
cid:1	O
)	O
(	O
cid:0	O
)	O
1	O
mmt	O
:	O
when	O
we	O
substitute	O
this	O
matrix	B
into	O
equation	O
(	O
45.36	O
)	O
we	O
(	O
cid:12	O
)	O
nd	O
p	O
(	O
tn	O
+1	O
j	O
tn	O
)	O
=	O
1	O
z	O
exp	O
''	O
(	O
cid:0	O
)	O
where	O
(	O
tn	O
+1	O
(	O
cid:0	O
)	O
^tn	O
+1	O
)	O
2	O
2	O
(	O
cid:27	O
)	O
2	O
^tn	O
+1	O
#	O
^tn	O
+1	O
=	O
kt	O
c	O
(	O
cid:0	O
)	O
1	O
(	O
cid:27	O
)	O
2	O
^tn	O
+1	O
n	O
tn	O
=	O
(	O
cid:20	O
)	O
(	O
cid:0	O
)	O
kt	O
c	O
(	O
cid:0	O
)	O
1	O
n	O
k	O
:	O
(	O
45.37	O
)	O
(	O
45.38	O
)	O
(	O
45.39	O
)	O
(	O
45.40	O
)	O
(	O
45.41	O
)	O
(	O
45.42	O
)	O
(	O
45.43	O
)	O
the	O
predictive	O
mean	O
at	O
the	O
new	O
point	O
is	O
given	O
by	O
^tn	O
+1	O
and	O
(	O
cid:27	O
)	O
^tn	O
+1	O
de	O
(	O
cid:12	O
)	O
nes	O
the	O
error	B
bars	I
on	O
this	O
prediction	B
.	O
notice	O
that	O
we	O
do	O
not	O
need	O
to	O
invert	O
cn	O
+1	O
in	O
order	O
to	O
make	O
predictions	O
at	O
x	O
(	O
n	O
+1	O
)	O
.	O
only	O
cn	O
needs	O
to	O
be	O
inverted	O
.	O
thus	O
gaussian	O
processes	O
allow	O
one	O
to	O
implement	O
a	O
model	B
with	O
a	O
number	O
of	O
basis	O
functions	B
h	O
much	O
larger	O
than	O
the	O
number	O
of	O
data	O
points	O
n	O
,	O
with	O
the	O
com-	O
putational	O
requirement	O
being	O
of	O
order	O
n	O
3	O
,	O
independent	O
of	O
h.	O
[	O
we	O
’	O
ll	O
discuss	O
ways	O
of	O
reducing	O
this	O
cost	O
later	O
.	O
]	O
the	O
predictions	O
produced	O
by	O
a	O
gaussian	O
process	O
depend	O
entirely	O
on	O
the	O
covariance	B
matrix	I
c.	O
we	O
now	O
discuss	O
the	O
sorts	O
of	O
covariance	O
functions	B
one	O
might	O
choose	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
c	O
,	O
and	O
how	O
we	O
can	O
automate	O
the	O
selection	O
of	O
the	O
covariance	O
function	B
in	O
response	O
to	O
data	O
.	O
45.4	O
examples	O
of	O
covariance	O
functions	B
the	O
only	O
constraint	O
on	O
our	O
choice	O
of	O
covariance	O
function	B
is	O
that	O
it	O
must	O
gen-	O
erate	O
a	O
non-negative-de	O
(	O
cid:12	O
)	O
nite	O
covariance	B
matrix	I
for	O
any	O
set	B
of	O
points	O
fxngn	O
n=1	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
544	O
45	O
|	O
gaussian	O
processes	O
we	O
will	O
denote	O
the	O
parameters	B
of	O
a	O
covariance	B
function	I
by	O
(	O
cid:18	O
)	O
.	O
the	O
covariance	B
matrix	I
of	O
t	O
has	O
entries	O
given	O
by	O
cmn	O
=	O
c	O
(	O
x	O
(	O
m	O
)	O
;	O
x	O
(	O
n	O
)	O
;	O
(	O
cid:18	O
)	O
)	O
+	O
(	O
cid:14	O
)	O
mnn	O
(	O
x	O
(	O
n	O
)	O
;	O
(	O
cid:18	O
)	O
)	O
(	O
45.44	O
)	O
where	O
c	O
is	O
the	O
covariance	B
function	I
and	O
n	O
is	O
a	O
noise	B
model	O
which	O
might	O
be	O
stationary	O
or	O
spatially	O
varying	O
,	O
for	O
example	O
,	O
n	O
(	O
x	O
;	O
(	O
cid:18	O
)	O
)	O
=	O
(	O
(	O
cid:18	O
)	O
3	O
j=1	O
(	O
cid:12	O
)	O
j	O
(	O
cid:30	O
)	O
j	O
(	O
x	O
)	O
(	O
cid:17	O
)	O
for	O
input-dependent	O
noise	B
.	O
the	O
continuity	O
properties	O
of	O
c	O
determine	O
the	O
continuity	O
properties	O
of	O
typical	O
samples	O
from	O
the	O
gaussian	O
process	O
prior	B
.	O
an	O
encyclopaedic	O
paper	O
on	O
gaus-	O
sian	O
processes	O
giving	O
many	O
valid	O
covariance	B
functions	O
has	O
been	O
written	O
by	O
abrahamsen	O
(	O
1997	O
)	O
.	O
for	O
input-independent	O
noise	B
(	O
45.45	O
)	O
exp	O
(	O
cid:16	O
)	O
pj	O
stationary	O
covariance	B
functions	O
a	O
stationary	O
covariance	B
function	I
is	O
one	O
that	O
is	O
translation	O
invariant	O
in	O
that	O
it	O
satis	O
(	O
cid:12	O
)	O
es	O
c	O
(	O
x	O
;	O
x0	O
;	O
(	O
cid:18	O
)	O
)	O
=	O
d	O
(	O
x	O
(	O
cid:0	O
)	O
x0	O
;	O
(	O
cid:18	O
)	O
)	O
(	O
45.46	O
)	O
for	O
some	O
function	B
d	O
,	O
i.e.	O
,	O
the	O
covariance	B
is	O
a	O
function	B
of	O
separation	B
only	O
,	O
also	O
known	O
as	O
the	O
autocovariance	O
function	B
.	O
if	O
additionally	O
c	O
depends	O
only	O
on	O
the	O
magnitude	O
of	O
the	O
distance	O
between	O
x	O
and	O
x0	O
then	O
the	O
covariance	B
function	I
is	O
said	O
to	O
be	O
homogeneous	B
.	O
stationary	O
covariance	B
functions	O
may	O
also	O
be	O
described	O
in	O
terms	O
of	O
the	O
fourier	O
transform	O
of	O
the	O
function	O
d	O
,	O
which	O
is	O
known	O
as	O
the	O
power	O
spectrum	O
of	O
the	O
gaussian	O
process	O
.	O
this	O
fourier	O
transform	O
is	O
necessarily	O
a	O
positive	O
function	O
of	O
frequency	O
.	O
one	O
way	O
of	O
constructing	O
a	O
valid	O
stationary	O
covariance	B
function	I
is	O
to	O
invent	O
a	O
positive	O
function	O
of	O
frequency	O
and	O
de	O
(	O
cid:12	O
)	O
ne	O
d	O
to	O
be	O
its	O
inverse	O
fourier	O
transform	O
.	O
example	O
45.5.	O
let	O
the	O
power	O
spectrum	O
be	O
a	O
gaussian	O
function	B
of	O
frequency	B
.	O
since	O
the	O
fourier	O
transform	O
of	O
a	O
gaussian	O
is	O
a	O
gaussian	O
,	O
the	O
autoco-	O
variance	B
function	O
corresponding	O
to	O
this	O
power	O
spectrum	O
is	O
a	O
gaussian	O
function	B
of	O
separation	B
.	O
this	O
argument	O
rederives	O
the	O
covariance	B
function	I
we	O
derived	O
at	O
equation	O
(	O
45.30	O
)	O
.	O
generalizing	O
slightly	O
,	O
a	O
popular	O
form	O
for	O
c	O
with	O
hyperparameters	O
(	O
cid:18	O
)	O
=	O
(	O
(	O
cid:18	O
)	O
1	O
;	O
(	O
cid:18	O
)	O
2	O
;	O
frig	O
)	O
is	O
c	O
(	O
x	O
;	O
x0	O
;	O
(	O
cid:18	O
)	O
)	O
=	O
(	O
cid:18	O
)	O
1	O
exp	O
''	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
xi	O
(	O
cid:0	O
)	O
x0i	O
)	O
2	O
r2	O
i	O
#	O
+	O
(	O
cid:18	O
)	O
2	O
:	O
i	O
xi=1	O
(	O
45.47	O
)	O
x	O
is	O
an	O
i-dimensional	O
vector	O
and	O
ri	O
is	O
a	O
lengthscale	O
associated	O
with	O
input	O
xi	O
,	O
the	O
lengthscale	O
in	O
direction	O
i	O
on	O
which	O
y	O
is	O
expected	O
to	O
vary	O
signi	O
(	O
cid:12	O
)	O
cantly	O
.	O
a	O
very	O
large	O
lengthscale	O
means	O
that	O
y	O
is	O
expected	O
to	O
be	O
essentially	O
a	O
constant	O
function	B
of	O
that	O
input	O
.	O
such	O
an	O
input	O
could	O
be	O
said	O
to	O
be	O
irrelevant	O
,	O
as	O
in	O
the	O
automatic	B
relevance	I
determination	I
method	O
for	O
neural	O
networks	O
(	O
mackay	O
,	O
1994a	O
;	O
neal	O
,	O
1996	O
)	O
.	O
the	O
(	O
cid:18	O
)	O
1	O
hyperparameter	B
de	O
(	O
cid:12	O
)	O
nes	O
the	O
vertical	O
scale	O
of	O
variations	O
of	O
a	O
typical	B
function	O
.	O
the	O
(	O
cid:18	O
)	O
2	O
hyperparameter	B
allows	O
the	O
whole	O
function	B
to	O
be	O
o	O
(	O
cid:11	O
)	O
set	B
away	O
from	O
zero	O
by	O
some	O
unknown	O
constant	O
{	O
to	O
understand	O
this	O
term	O
,	O
examine	O
equation	O
(	O
45.25	O
)	O
and	O
consider	O
the	O
basis	O
function	O
(	O
cid:30	O
)	O
(	O
x	O
)	O
=	O
1.	O
another	O
stationary	O
covariance	B
function	I
is	O
c	O
(	O
x	O
;	O
x0	O
)	O
=	O
exp	O
(	O
(	O
cid:0	O
)	O
jx	O
(	O
cid:0	O
)	O
x0j	O
(	O
cid:23	O
)	O
)	O
0	O
<	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
2	O
:	O
(	O
45.48	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
45.5	O
:	O
adaptation	O
of	O
gaussian	O
process	O
models	O
545	O
figure	O
45.2.	O
multimodal	O
likelihood	B
functions	O
for	O
gaussian	O
processes	O
.	O
a	O
data	B
set	I
of	O
(	O
cid:12	O
)	O
ve	O
points	O
is	O
modelled	O
with	O
the	O
simple	O
covariance	O
function	B
(	O
45.47	O
)	O
,	O
with	O
one	O
hyperparameter	B
(	O
cid:18	O
)	O
3	O
controlling	O
the	O
noise	B
variance	O
.	O
panels	O
a	O
and	O
b	O
show	O
the	O
most	O
probable	O
interpolant	O
and	O
its	O
1	O
(	O
cid:27	O
)	O
error	B
bars	I
when	O
the	O
hyperparameters	O
(	O
cid:18	O
)	O
are	O
set	B
to	O
two	O
di	O
(	O
cid:11	O
)	O
erent	O
values	O
that	O
(	O
locally	O
)	O
maximize	O
the	O
likelihood	B
p	O
(	O
tn	O
j	O
xn	O
;	O
(	O
cid:18	O
)	O
)	O
:	O
(	O
a	O
)	O
r1	O
=	O
0:95	O
,	O
(	O
cid:18	O
)	O
3	O
=	O
0:0	O
;	O
(	O
b	O
)	O
r1	O
=	O
3:5	O
,	O
(	O
cid:18	O
)	O
3	O
=	O
3:0.	O
panel	O
c	O
shows	O
a	O
contour	O
plot	O
of	O
the	O
likelihood	O
as	O
a	O
function	B
of	O
r1	O
and	O
(	O
cid:18	O
)	O
3	O
,	O
with	O
the	O
two	O
maxima	O
shown	O
by	O
crosses	O
.	O
from	O
gibbs	O
(	O
1997	O
)	O
.	O
7.0	O
5.0	O
3.0	O
1.0	O
−1.0	O
7.0	O
5.0	O
3.0	O
1.0	O
−1.0	O
(	O
a	O
)	O
−3.0	O
0.0	O
2.0	O
4.0	O
6.0	O
(	O
b	O
)	O
−3.0	O
0.0	O
2.0	O
4.0	O
6.0	O
(	O
cid:2	O
)	O
4	O
3.5	O
3	O
2.5	O
2	O
1.5	O
1	O
0.5	O
(	O
cid:18	O
)	O
3	O
(	O
cid:2	O
)	O
1	O
0.5	O
1.5	O
(	O
c	O
)	O
2.5	O
3	O
3.5	O
4	O
2	O
r1	O
for	O
(	O
cid:23	O
)	O
=	O
2	O
,	O
this	O
is	O
a	O
special	O
case	O
of	O
the	O
previous	O
covariance	B
function	I
.	O
for	O
(	O
cid:23	O
)	O
2	O
(	O
1	O
;	O
2	O
)	O
,	O
the	O
typical	B
functions	O
from	O
this	O
prior	B
are	O
smooth	O
but	O
not	O
analytic	O
functions	B
.	O
for	O
(	O
cid:23	O
)	O
(	O
cid:20	O
)	O
1	O
typical	B
functions	O
are	O
continuous	B
but	O
not	O
smooth	O
.	O
a	O
covariance	B
function	I
that	O
models	O
a	O
function	B
that	O
is	O
periodic	O
with	O
known	O
period	O
(	O
cid:21	O
)	O
i	O
in	O
the	O
ith	O
input	O
direction	O
is	O
c	O
(	O
x	O
;	O
x0	O
;	O
(	O
cid:18	O
)	O
)	O
=	O
(	O
cid:18	O
)	O
1	O
exp2	O
64	O
(	O
cid:0	O
)	O
1	O
2xi	O
0	O
@	O
:	O
(	O
45.49	O
)	O
sin	O
(	O
cid:16	O
)	O
(	O
cid:25	O
)	O
(	O
cid:21	O
)	O
i	O
(	O
xi	O
(	O
cid:0	O
)	O
x0i	O
)	O
(	O
cid:17	O
)	O
ri	O
23	O
1	O
75	O
a	O
figure	O
45.1	O
shows	O
some	O
random	B
samples	O
drawn	O
from	O
gaussian	O
processes	O
with	O
a	O
variety	O
of	O
di	O
(	O
cid:11	O
)	O
erent	O
covariance	B
functions	O
.	O
nonstationary	O
covariance	B
functions	O
the	O
simplest	O
nonstationary	O
covariance	B
function	I
is	O
the	O
one	O
corresponding	O
to	O
a	O
linear	B
trend	O
.	O
consider	O
the	O
plane	O
y	O
(	O
x	O
)	O
=	O
pi	O
wixi	O
+	O
c.	O
if	O
the	O
fwig	O
and	O
c	O
have	O
gaussian	O
distributions	O
with	O
zero	O
mean	B
and	O
variances	O
(	O
cid:27	O
)	O
2	O
then	O
the	O
plane	O
has	O
a	O
covariance	B
function	I
w	O
and	O
(	O
cid:27	O
)	O
2	O
c	O
respectively	O
clin	O
(	O
x	O
;	O
x0	O
;	O
f	O
(	O
cid:27	O
)	O
w	O
;	O
(	O
cid:27	O
)	O
cg	O
)	O
=	O
i	O
xi=1	O
wxix0i	O
+	O
(	O
cid:27	O
)	O
2	O
(	O
cid:27	O
)	O
2	O
c	O
:	O
(	O
45.50	O
)	O
an	O
example	O
of	O
random	O
sample	B
functions	O
incorporating	O
the	O
linear	B
term	O
can	O
be	O
seen	O
in	O
(	O
cid:12	O
)	O
gure	O
45.1d	O
.	O
45.5	O
adaptation	O
of	O
gaussian	O
process	O
models	O
let	O
us	O
assume	O
that	O
a	O
form	O
of	O
covariance	O
function	B
has	O
been	O
chosen	O
,	O
but	O
that	O
it	O
depends	O
on	O
undetermined	O
hyperparameters	O
(	O
cid:18	O
)	O
.	O
we	O
would	O
like	O
to	O
‘	O
learn	O
’	O
these	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
546	O
45	O
|	O
gaussian	O
processes	O
hyperparameters	O
from	O
the	O
data	O
.	O
this	O
learning	B
process	O
is	O
equivalent	O
to	O
the	O
inference	B
of	O
the	O
hyperparameters	O
of	O
a	O
neural	B
network	I
,	O
for	O
example	O
,	O
weight	B
decay	I
hyperparameters	O
.	O
it	O
is	O
a	O
complexity-control	O
problem	O
,	O
one	O
that	O
is	O
solved	O
nicely	O
by	O
the	O
bayesian	O
occam	O
’	O
s	O
razor	O
.	O
ideally	O
we	O
would	O
like	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
prior	B
distribution	O
on	O
the	O
hyperparameters	O
and	O
integrate	O
over	O
them	O
in	O
order	O
to	O
make	O
our	O
predictions	O
,	O
i.e.	O
,	O
we	O
would	O
like	O
to	O
(	O
cid:12	O
)	O
nd	O
p	O
(	O
tn	O
+1	O
j	O
xn	O
+1	O
;	O
d	O
)	O
=z	O
p	O
(	O
tn	O
+1	O
j	O
xn	O
+1	O
;	O
(	O
cid:18	O
)	O
;	O
d	O
)	O
p	O
(	O
(	O
cid:18	O
)	O
jd	O
)	O
d	O
(	O
cid:18	O
)	O
:	O
(	O
45.51	O
)	O
but	O
this	O
integral	B
is	O
usually	O
intractable	O
.	O
there	O
are	O
two	O
approaches	O
we	O
can	O
take	O
.	O
1.	O
we	O
can	O
approximate	O
the	O
integral	B
by	O
using	O
the	O
most	O
probable	O
values	O
of	O
hyperparameters	O
.	O
p	O
(	O
tn	O
+1	O
j	O
xn	O
+1	O
;	O
d	O
)	O
’	O
p	O
(	O
tn	O
+1	O
j	O
xn	O
+1	O
;	O
d	O
;	O
(	O
cid:18	O
)	O
mp	O
)	O
(	O
45.52	O
)	O
2.	O
or	O
we	O
can	O
perform	O
the	O
integration	O
over	O
(	O
cid:18	O
)	O
numerically	O
using	O
monte	O
carlo	O
methods	B
(	O
williams	O
and	O
rasmussen	O
,	O
1996	O
;	O
neal	O
,	O
1997b	O
)	O
.	O
either	O
of	O
these	O
approaches	O
is	O
implemented	O
most	O
e	O
(	O
cid:14	O
)	O
ciently	O
if	O
the	O
gradient	O
of	O
the	O
posterior	B
probability	I
of	O
(	O
cid:18	O
)	O
can	O
be	O
evaluated	O
.	O
gradient	O
the	O
posterior	B
probability	I
of	O
(	O
cid:18	O
)	O
is	O
p	O
(	O
(	O
cid:18	O
)	O
jd	O
)	O
/	O
p	O
(	O
tn	O
j	O
xn	O
;	O
(	O
cid:18	O
)	O
)	O
p	O
(	O
(	O
cid:18	O
)	O
)	O
:	O
(	O
45.53	O
)	O
the	O
log	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
term	O
(	O
the	O
evidence	B
for	O
the	O
hyperparameters	O
)	O
is	O
ln	O
p	O
(	O
tn	O
j	O
xn	O
;	O
(	O
cid:18	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
n	O
tn	O
(	O
cid:0	O
)	O
and	O
its	O
derivative	O
with	O
respect	O
to	O
a	O
hyperparameter	B
(	O
cid:18	O
)	O
is	O
ln	O
det	O
cn	O
(	O
cid:0	O
)	O
1	O
2	O
1	O
2	O
n	O
c	O
(	O
cid:0	O
)	O
1	O
tt	O
n	O
2	O
ln	O
2	O
(	O
cid:25	O
)	O
;	O
(	O
45.54	O
)	O
@	O
@	O
(	O
cid:18	O
)	O
ln	O
p	O
(	O
tn	O
j	O
xn	O
;	O
(	O
cid:18	O
)	O
)	O
=	O
(	O
cid:0	O
)	O
1	O
2	O
trace	O
(	O
cid:18	O
)	O
c	O
(	O
cid:0	O
)	O
1	O
n	O
@	O
cn	O
@	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
+	O
1	O
2	O
n	O
c	O
(	O
cid:0	O
)	O
1	O
tt	O
n	O
@	O
cn	O
@	O
(	O
cid:18	O
)	O
c	O
(	O
cid:0	O
)	O
1	O
n	O
tn	O
:	O
(	O
45.55	O
)	O
comments	O
assuming	O
that	O
(	O
cid:12	O
)	O
nding	O
the	O
derivatives	O
of	O
the	O
priors	O
is	O
straightforward	O
,	O
we	O
can	O
now	O
search	O
for	O
(	O
cid:18	O
)	O
mp	O
.	O
however	O
there	O
are	O
two	O
problems	O
that	O
we	O
need	O
to	O
be	O
aware	O
of	O
.	O
firstly	O
,	O
as	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gure	O
45.2	O
,	O
the	O
evidence	B
may	O
be	O
multimodal	O
.	O
suitable	O
priors	O
and	O
sensible	O
optimization	B
strategies	O
often	O
eliminate	O
poor	O
op-	O
tima	O
.	O
secondly	O
and	O
perhaps	O
most	O
importantly	O
the	O
evaluation	O
of	O
the	O
gradi-	O
ent	O
of	O
the	O
log	O
likelihood	B
requires	O
the	O
evaluation	O
of	O
c	O
(	O
cid:0	O
)	O
1	O
n	O
.	O
any	O
exact	O
inversion	O
method	B
(	O
such	O
as	O
cholesky	O
decomposition	O
,	O
lu	O
decomposition	O
or	O
gauss	O
{	O
jordan	O
elimination	O
)	O
has	O
an	O
associated	O
computational	O
cost	O
that	O
is	O
of	O
order	O
n	O
3	O
and	O
so	O
calculating	O
gradients	O
becomes	O
time	O
consuming	O
for	O
large	O
training	B
data	I
sets	O
.	O
ap-	O
proximate	O
methods	B
for	O
implementing	O
the	O
predictions	O
(	O
equations	O
(	O
45.42	O
)	O
and	O
(	O
45.43	O
)	O
)	O
and	O
gradient	O
computation	O
(	O
equation	O
(	O
45.55	O
)	O
)	O
are	O
an	O
active	O
research	O
area	O
.	O
one	O
approach	O
based	O
on	O
the	O
ideas	O
of	O
skilling	O
(	O
1993	O
)	O
makes	O
approxima-	O
tions	O
to	O
c	O
(	O
cid:0	O
)	O
1t	O
and	O
trace	O
c	O
(	O
cid:0	O
)	O
1	O
using	O
iterative	O
methods	O
with	O
cost	O
o	O
(	O
n	O
2	O
)	O
(	O
gibbs	O
and	O
mackay	O
,	O
1996	O
;	O
gibbs	O
,	O
1997	O
)	O
.	O
further	O
references	O
on	O
this	O
topic	O
are	O
given	O
at	O
the	O
end	O
of	O
the	O
chapter	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
547	O
45.6	O
:	O
classi	O
(	O
cid:12	O
)	O
cation	O
45.6	O
classi	O
(	O
cid:12	O
)	O
cation	O
gaussian	O
processes	O
can	O
be	O
integrated	O
into	O
classi	O
(	O
cid:12	O
)	O
cation	O
modelling	B
once	O
we	O
identify	O
a	O
variable	O
that	O
can	O
sensibly	O
be	O
given	O
a	O
gaussian	O
process	O
prior	B
.	O
in	O
a	O
binary	O
classi	O
(	O
cid:12	O
)	O
cation	O
problem	O
,	O
we	O
can	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
quantity	O
an	O
(	O
cid:17	O
)	O
a	O
(	O
x	O
(	O
n	O
)	O
)	O
such	O
that	O
the	O
probability	B
that	O
the	O
class	O
is	O
1	O
rather	O
than	O
0	O
is	O
p	O
(	O
tn	O
=	O
1j	O
an	O
)	O
=	O
1	O
1	O
+	O
e	O
(	O
cid:0	O
)	O
an	O
:	O
(	O
45.56	O
)	O
large	O
positive	O
values	O
of	O
a	O
correspond	O
to	O
probabilities	O
close	O
to	O
one	O
;	O
large	O
neg-	O
ative	O
values	O
of	O
a	O
de	O
(	O
cid:12	O
)	O
ne	O
probabilities	O
that	O
are	O
close	O
to	O
zero	O
.	O
in	O
a	O
classi	O
(	O
cid:12	O
)	O
ca-	O
tion	O
problem	O
,	O
we	O
typically	O
intend	O
that	O
the	O
probability	B
p	O
(	O
tn	O
=	O
1	O
)	O
should	O
be	O
a	O
smoothly	O
varying	O
function	B
of	O
x.	O
we	O
can	O
embody	O
this	O
prior	B
belief	O
by	O
de	O
(	O
cid:12	O
)	O
ning	O
a	O
(	O
x	O
)	O
to	O
have	O
a	O
gaussian	O
process	O
prior	B
.	O
implementation	O
it	O
is	O
not	O
so	O
easy	O
to	O
perform	O
inferences	O
and	O
adapt	O
the	O
gaussian	O
process	O
model	B
to	O
data	O
in	O
a	O
classi	O
(	O
cid:12	O
)	O
cation	O
model	B
as	O
in	O
regression	O
problems	O
because	O
the	O
like-	O
lihood	O
function	B
(	O
45.56	O
)	O
is	O
not	O
a	O
gaussian	O
function	B
of	O
an	O
.	O
so	O
the	O
posterior	O
distribution	O
of	O
a	O
given	O
some	O
observations	O
t	O
is	O
not	O
gaussian	O
and	O
the	O
normal-	O
ization	O
constant	O
p	O
(	O
tn	O
j	O
xn	O
)	O
can	O
not	O
be	O
written	O
down	O
analytically	O
.	O
barber	O
and	O
williams	O
(	O
1997	O
)	O
have	O
implemented	O
classi	O
(	O
cid:12	O
)	O
ers	O
based	O
on	O
gaussian	O
process	O
priors	O
using	O
laplace	O
approximations	O
(	O
chapter	O
27	O
)	O
.	O
neal	O
(	O
1997b	O
)	O
has	O
implemented	O
a	O
monte	O
carlo	O
approach	O
to	O
implementing	O
a	O
gaussian	O
process	O
classi	O
(	O
cid:12	O
)	O
er	O
.	O
gibbs	O
and	O
mackay	O
(	O
2000	O
)	O
have	O
implemented	O
another	O
cheap	O
and	O
cheerful	O
approach	O
based	O
on	O
the	O
methods	B
of	O
jaakkola	O
and	O
jordan	O
(	O
section	B
33.8	O
)	O
.	O
in	O
this	O
varia-	O
tional	O
gaussian	O
process	O
classi	O
(	O
cid:12	O
)	O
er	O
,	O
we	O
obtain	O
tractable	O
upper	O
and	O
lower	O
bounds	O
for	O
the	O
unnormalized	O
posterior	O
density	O
over	O
a	O
,	O
p	O
(	O
tn	O
j	O
a	O
)	O
p	O
(	O
a	O
)	O
.	O
these	O
bounds	O
are	O
parameterized	O
by	O
variational	O
parameters	B
which	O
are	O
adjusted	O
in	O
order	O
to	O
obtain	O
the	O
tightest	O
possible	O
(	O
cid:12	O
)	O
t.	O
using	O
normalized	O
versions	O
of	O
the	O
optimized	O
bounds	O
we	O
then	O
compute	O
approximations	O
to	O
the	O
predictive	O
distributions	O
.	O
multi-class	O
classi	O
(	O
cid:12	O
)	O
cation	O
problems	O
can	O
also	O
be	O
solved	O
with	O
monte	O
carlo	O
methods	B
(	O
neal	O
,	O
1997b	O
)	O
and	O
variational	O
methods	B
(	O
gibbs	O
,	O
1997	O
)	O
.	O
45.7	O
discussion	O
gaussian	O
processes	O
are	O
moderately	O
simple	O
to	O
implement	O
and	O
use	O
.	O
because	O
very	O
few	O
parameters	B
of	O
the	O
model	B
need	O
to	O
be	O
determined	O
by	O
hand	O
(	O
generally	O
only	O
the	O
priors	O
on	O
the	O
hyperparameters	O
)	O
,	O
gaussian	O
processes	O
are	O
useful	O
tools	O
for	O
automated	O
tasks	O
where	O
(	O
cid:12	O
)	O
ne	O
tuning	O
for	O
each	O
problem	O
is	O
not	O
possible	O
.	O
we	O
do	O
not	O
appear	O
to	O
sacri	O
(	O
cid:12	O
)	O
ce	O
any	O
performance	O
for	O
this	O
simplicity	O
.	O
it	O
is	O
easy	O
to	O
construct	O
gaussian	O
processes	O
that	O
have	O
particular	O
desired	O
properties	O
;	O
for	O
example	O
we	O
can	O
make	O
a	O
straightforward	O
automatic	B
relevance	I
determination	I
model	O
.	O
one	O
obvious	O
problem	O
with	O
gaussian	O
processes	O
is	O
the	O
computational	O
cost	O
associated	O
with	O
inverting	O
an	O
n	O
(	O
cid:2	O
)	O
n	O
matrix	B
.	O
the	O
cost	O
of	O
direct	O
methods	B
of	O
inversion	O
becomes	O
prohibitive	O
when	O
the	O
number	O
of	O
data	O
points	O
n	O
is	O
greater	O
than	O
about	O
1000.	O
have	O
we	O
thrown	O
the	O
baby	O
out	O
with	O
the	O
bath	O
water	O
?	O
according	O
to	O
the	O
hype	O
of	O
1987	O
,	O
neural	O
networks	O
were	O
meant	O
to	O
be	O
intelligent	O
models	O
that	O
discovered	O
features	O
and	O
patterns	O
in	O
data	O
.	O
gaussian	O
processes	O
in	O
 	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
548	O
45	O
|	O
gaussian	O
processes	O
contrast	O
are	O
simply	O
smoothing	O
devices	O
.	O
how	O
can	O
gaussian	O
processes	O
possi-	O
bly	O
replace	O
neural	O
networks	O
?	O
were	O
neural	O
networks	O
over-hyped	O
,	O
or	O
have	O
we	O
underestimated	O
the	O
power	O
of	O
smoothing	O
methods	B
?	O
i	O
think	O
both	O
these	O
propositions	O
are	O
true	O
.	O
the	O
success	O
of	O
gaussian	O
processes	O
shows	O
that	O
many	O
real-world	O
data	B
modelling	I
problems	O
are	O
perfectly	O
well	O
solved	O
by	O
sensible	O
smoothing	O
methods	B
.	O
the	O
most	O
interesting	O
problems	O
,	O
the	O
task	O
of	O
feature	O
discovery	O
for	O
example	O
,	O
are	O
not	O
ones	O
that	O
gaussian	O
processes	O
will	O
solve	O
.	O
but	O
maybe	O
multilayer	O
perceptrons	O
can	O
’	O
t	O
solve	O
them	O
either	O
.	O
perhaps	O
a	O
fresh	O
start	O
is	O
needed	O
,	O
approaching	O
the	O
problem	O
of	O
machine	O
learning	B
from	O
a	O
paradigm	O
di	O
(	O
cid:11	O
)	O
erent	O
from	O
the	O
supervised	O
feedforward	O
mapping	B
.	O
further	O
reading	O
the	O
study	O
of	O
gaussian	O
processes	O
for	O
regression	O
is	O
far	O
from	O
new	O
.	O
time	O
series	O
analysis	B
was	O
being	O
performed	O
by	O
the	O
astronomer	O
t.n	O
.	O
thiele	O
using	O
gaussian	O
processes	O
in	O
1880	O
(	O
lauritzen	O
,	O
1981	O
)	O
.	O
in	O
the	O
1940s	O
,	O
wiener	O
{	O
kolmogorov	O
pre-	O
diction	O
theory	B
was	O
introduced	O
for	O
prediction	O
of	O
trajectories	O
of	O
military	O
targets	O
(	O
wiener	O
,	O
1948	O
)	O
.	O
within	O
the	O
geostatistics	B
(	O
cid:12	O
)	O
eld	O
,	O
matheron	O
(	O
1963	O
)	O
proposed	O
a	O
framework	O
for	O
regression	O
using	O
optimal	O
linear	O
estimators	O
which	O
he	O
called	O
‘	O
krig-	O
ing	O
’	O
after	O
d.g	O
.	O
krige	O
,	O
a	O
south	O
african	O
mining	O
engineer	O
.	O
this	O
framework	O
is	O
identical	O
to	O
the	O
gaussian	O
process	O
approach	O
to	O
regression	B
.	O
kriging	B
has	O
been	O
developed	O
considerably	O
in	O
the	O
last	O
thirty	O
years	O
(	O
see	O
cressie	O
(	O
1993	O
)	O
for	O
a	O
re-	O
view	O
)	O
including	O
several	O
bayesian	O
treatments	O
(	O
omre	O
,	O
1987	O
;	O
kitanidis	O
,	O
1986	O
)	O
.	O
however	O
the	O
geostatistics	B
approach	O
to	O
the	O
gaussian	O
process	O
model	B
has	O
con-	O
centrated	O
mainly	O
on	O
low-dimensional	O
problems	O
and	O
has	O
largely	O
ignored	O
any	O
probabilistic	O
interpretation	O
of	O
the	O
model	O
.	O
kalman	O
(	O
cid:12	O
)	O
lters	O
are	O
widely	O
used	O
to	O
implement	O
inferences	O
for	O
stationary	O
one-dimensional	O
gaussian	O
processes	O
,	O
and	O
are	O
popular	O
models	O
for	O
speech	O
and	O
music	O
modelling	B
(	O
bar-shalom	O
and	O
fort-	O
mann	O
,	O
1988	O
)	O
.	O
generalized	B
radial	O
basis	O
functions	O
(	O
poggio	O
and	O
girosi	O
,	O
1989	O
)	O
,	O
arma	O
models	O
(	O
wahba	O
,	O
1990	O
)	O
and	O
variable	O
metric	B
kernel	O
methods	B
(	O
lowe	O
,	O
1995	O
)	O
are	O
all	O
closely	O
related	O
to	O
gaussian	O
processes	O
.	O
see	O
also	O
o	O
’	O
hagan	O
(	O
1978	O
)	O
.	O
the	O
idea	O
of	O
replacing	O
supervised	O
neural	O
networks	O
by	O
gaussian	O
processes	O
was	O
(	O
cid:12	O
)	O
rst	O
explored	O
by	O
williams	O
and	O
rasmussen	O
(	O
1996	O
)	O
and	O
neal	O
(	O
1997b	O
)	O
.	O
a	O
thorough	O
comparison	O
of	O
gaussian	O
processes	O
with	O
other	O
methods	B
such	O
as	O
neural	O
networks	O
and	O
mars	O
was	O
made	O
by	O
rasmussen	O
(	O
1996	O
)	O
.	O
methods	B
for	O
reducing	O
the	O
complexity	B
of	O
data	B
modelling	I
with	O
gaussian	O
processes	O
remain	O
an	O
active	O
research	O
area	O
(	O
poggio	O
and	O
girosi	O
,	O
1990	O
;	O
luo	O
and	O
wahba	O
,	O
1997	O
;	O
tresp	O
,	O
2000	O
;	O
williams	O
and	O
seeger	O
,	O
2001	O
;	O
smola	O
and	O
bartlett	O
,	O
2001	O
;	O
rasmussen	O
,	O
2002	O
;	O
seeger	O
et	O
al.	O
,	O
2003	O
;	O
opper	O
and	O
winther	O
,	O
2000	O
)	O
.	O
a	O
longer	O
review	O
of	O
gaussian	O
processes	O
is	O
in	O
(	O
mackay	O
,	O
1998b	O
)	O
.	O
a	O
review	O
paper	O
on	O
regression	O
with	O
complexity	O
control	O
using	O
hierarchical	O
bayesian	O
models	O
is	O
(	O
mackay	O
,	O
1992a	O
)	O
.	O
gaussian	O
processes	O
and	O
support	O
vector	O
learning	O
machines	O
(	O
scholkopf	O
et	O
al.	O
,	O
1995	O
;	O
vapnik	O
,	O
1995	O
)	O
have	O
a	O
lot	O
in	O
common	O
.	O
both	O
are	O
kernel-based	O
predictors	O
,	O
the	O
kernel	B
being	O
another	O
name	O
for	O
the	O
covariance	B
function	I
.	O
a	O
bayesian	O
version	O
of	O
support	O
vectors	B
,	O
exploiting	O
this	O
connection	O
,	O
can	O
be	O
found	O
in	O
(	O
chu	O
et	O
al.	O
,	O
2001	O
;	O
chu	O
et	O
al.	O
,	O
2002	O
;	O
chu	O
et	O
al.	O
,	O
2003b	O
;	O
chu	O
et	O
al.	O
,	O
2003a	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
46	O
deconvolution	B
46.1	O
traditional	O
image	B
reconstruction	I
methods	O
optimal	O
linear	O
(	O
cid:12	O
)	O
lters	O
in	O
many	O
imaging	O
problems	O
,	O
the	O
data	O
measurements	O
fdng	O
are	O
linearly	O
related	O
to	O
the	O
underlying	O
image	B
f	O
:	O
dn	O
=xk	O
rnkfk	O
+	O
nn	O
:	O
(	O
46.1	O
)	O
the	O
vector	O
n	O
denotes	O
the	O
inevitable	O
noise	B
that	O
corrupts	O
real	O
data	O
.	O
in	O
the	O
case	O
of	O
a	O
camera	B
which	O
produces	O
a	O
blurred	O
picture	O
,	O
the	O
vector	O
f	O
denotes	O
the	O
true	O
image	B
,	O
d	O
denotes	O
the	O
blurred	O
and	O
noisy	O
picture	O
,	O
and	O
the	O
linear	B
operator	O
r	O
is	O
a	O
convolution	B
de	O
(	O
cid:12	O
)	O
ned	O
by	O
the	O
point	B
spread	I
function	I
of	O
the	O
camera	B
.	O
in	O
this	O
special	O
case	O
,	O
the	O
true	O
image	B
and	O
the	O
data	O
vector	O
reside	O
in	O
the	O
same	O
space	O
;	O
but	O
it	O
is	O
important	O
to	O
maintain	O
a	O
distinction	O
between	O
them	O
.	O
we	O
will	O
use	O
the	O
subscript	O
n	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
n	O
to	O
run	O
over	O
data	O
measurements	O
,	O
and	O
the	O
subscripts	O
k	O
;	O
k0	O
=	O
1	O
;	O
:	O
:	O
:	O
;	O
k	O
to	O
run	O
over	O
image	O
pixels	O
.	O
one	O
might	O
speculate	O
that	O
since	O
the	O
blur	B
was	O
created	O
by	O
a	O
linear	B
operation	O
,	O
then	O
perhaps	O
it	O
might	O
be	O
deblurred	O
by	O
another	O
linear	B
operation	O
.	O
we	O
can	O
derive	O
the	O
optimal	O
linear	O
(	O
cid:12	O
)	O
lter	O
in	O
two	O
ways	O
.	O
bayesian	O
derivation	B
we	O
assume	O
that	O
the	O
linear	B
operator	O
r	O
is	O
known	O
,	O
and	O
that	O
the	O
noise	B
n	O
is	O
gaussian	O
and	O
independent	O
,	O
with	O
a	O
known	O
standard	B
deviation	I
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
.	O
p	O
(	O
dj	O
f	O
;	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
;	O
h	O
)	O
=	O
1	O
(	O
cid:23	O
)	O
)	O
n=2	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
exp	O
(	O
cid:0	O
)	O
xn	O
(	O
cid:23	O
)	O
)	O
!	O
:	O
(	O
dn	O
(	O
cid:0	O
)	O
pk	O
rnkfk	O
)	O
2	O
.	O
(	O
2	O
(	O
cid:27	O
)	O
2	O
(	O
46.2	O
)	O
we	O
assume	O
that	O
the	O
prior	B
probability	O
of	O
the	O
image	O
is	O
also	O
gaussian	O
,	O
with	O
a	O
scale	O
parameter	O
(	O
cid:27	O
)	O
f	O
.	O
p	O
(	O
f	O
j	O
(	O
cid:27	O
)	O
f	O
;	O
h	O
)	O
=	O
det	O
(	O
cid:0	O
)	O
1	O
(	O
2	O
(	O
cid:25	O
)	O
(	O
cid:27	O
)	O
2	O
2	O
c	O
f	O
)	O
k=2	O
(	O
46.3	O
)	O
exp0	O
@	O
(	O
cid:0	O
)	O
xk	O
;	O
k0	O
f	O
)	O
1	O
fkckk0f0k	O
(	O
cid:14	O
)	O
(	O
2	O
(	O
cid:27	O
)	O
2	O
a	O
:	O
if	O
we	O
assume	O
no	O
correlations	B
among	O
the	O
pixels	O
then	O
the	O
symmetric	B
,	O
full	O
rank	O
matrix	B
c	O
is	O
equal	O
to	O
the	O
identity	B
matrix	I
i.	O
the	O
more	O
sophisticated	O
‘	O
intrinsic	B
correlation	I
function	I
’	O
model	B
uses	O
c	O
=	O
[	O
ggt	O
]	O
(	O
cid:0	O
)	O
1	O
,	O
where	O
g	O
is	O
a	O
convolution	B
that	O
takes	O
us	O
from	O
an	O
imaginary	O
‘	O
hidden	O
’	O
image	B
,	O
which	O
is	O
uncorrelated	O
,	O
to	O
the	O
real	O
correlated	O
image	O
.	O
the	O
intrinsic	B
correlation	I
function	I
should	O
not	O
be	O
confused	O
with	O
the	O
point	B
spread	I
function	I
r	O
which	O
de	O
(	O
cid:12	O
)	O
nes	O
the	O
image-to-data	O
mapping	B
.	O
549	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
550	O
46	O
|	O
deconvolution	B
a	O
zero-mean	O
gaussian	O
prior	B
is	O
clearly	O
a	O
poor	O
assumption	O
if	O
it	O
is	O
known	O
that	O
all	O
elements	O
of	O
the	O
image	O
f	O
are	O
positive	O
,	O
but	O
let	O
us	O
proceed	O
.	O
we	O
can	O
now	O
write	O
down	O
the	O
posterior	B
probability	I
of	O
an	O
image	B
f	O
given	O
the	O
data	O
d.	O
in	O
words	O
,	O
p	O
(	O
f	O
j	O
d	O
;	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
;	O
(	O
cid:27	O
)	O
f	O
;	O
h	O
)	O
=	O
p	O
(	O
dj	O
f	O
;	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
;	O
h	O
)	O
p	O
(	O
f	O
j	O
(	O
cid:27	O
)	O
f	O
;	O
h	O
)	O
)	O
p	O
(	O
dj	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
;	O
(	O
cid:27	O
)	O
f	O
;	O
h	O
)	O
:	O
(	O
46.4	O
)	O
posterior	O
=	O
likelihood	B
(	O
cid:2	O
)	O
prior	B
evidence	O
:	O
(	O
46.5	O
)	O
the	O
‘	O
evidence	B
’	O
p	O
(	O
dj	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
;	O
(	O
cid:27	O
)	O
f	O
;	O
h	O
)	O
is	O
the	O
normalizing	B
constant	I
for	O
this	O
posterior	O
distribution	O
.	O
here	O
it	O
is	O
unimportant	O
,	O
but	O
it	O
is	O
used	O
in	O
a	O
more	O
sophisticated	O
analysis	B
to	O
compare	O
,	O
for	O
example	O
,	O
di	O
(	O
cid:11	O
)	O
erent	O
values	O
of	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
and	O
(	O
cid:27	O
)	O
f	O
,	O
or	O
di	O
(	O
cid:11	O
)	O
erent	O
point	O
spread	O
functions	O
r.	O
since	O
the	O
posterior	O
distribution	O
is	O
the	O
product	O
of	O
two	O
gaussian	O
functions	B
of	O
f	O
,	O
it	O
is	O
also	O
a	O
gaussian	O
,	O
and	O
can	O
therefore	O
be	O
summarized	O
by	O
its	O
mean	B
,	O
which	O
is	O
also	O
the	O
most	O
probable	O
image	O
,	O
fmp	O
,	O
and	O
its	O
covariance	B
matrix	I
:	O
(	O
cid:6	O
)	O
fjd	O
(	O
cid:17	O
)	O
[	O
(	O
cid:0	O
)	O
rr	O
log	O
p	O
(	O
f	O
j	O
d	O
;	O
(	O
cid:27	O
)	O
(	O
cid:23	O
)	O
;	O
(	O
cid:27	O
)	O
f	O
;	O
h	O
)	O
]	O
(	O
cid:0	O
)	O
1	O
;	O
(	O
46.6	O
)	O
c	O
#	O
(	O
cid:0	O
)	O
1	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
(	O
cid:27	O
)	O
2	O
f	O
fmp	O
=	O
''	O
rtr	O
+	O
c	O
(	O
cid:21	O
)	O
(	O
cid:0	O
)	O
1	O
rt	O
is	O
called	O
the	O
optimal	O
linear	O
(	O
cid:12	O
)	O
lter	O
.	O
when	O
which	O
de	O
(	O
cid:12	O
)	O
nes	O
the	O
joint	B
error	O
bars	O
on	O
f	O
.	O
in	O
this	O
equation	O
,	O
the	O
symbol	O
r	O
denotes	O
di	O
(	O
cid:11	O
)	O
erentiation	O
with	O
respect	O
to	O
the	O
image	B
parameters	O
f	O
.	O
we	O
can	O
(	O
cid:12	O
)	O
nd	O
fmp	O
by	O
di	O
(	O
cid:11	O
)	O
erentiating	O
the	O
log	O
of	O
the	O
posterior	O
,	O
and	O
solving	O
for	O
the	O
derivative	O
being	O
zero	O
.	O
we	O
obtain	O
:	O
rtd	O
:	O
(	O
46.7	O
)	O
the	O
operator	O
(	O
cid:20	O
)	O
rtr	O
+	O
(	O
cid:27	O
)	O
2	O
the	O
term	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
(	O
cid:27	O
)	O
2	O
f	O
[	O
rtr	O
]	O
(	O
cid:0	O
)	O
1	O
rt	O
.	O
the	O
term	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
(	O
cid:27	O
)	O
2	O
f	O
(	O
cid:23	O
)	O
(	O
cid:27	O
)	O
2	O
f	O
c	O
can	O
be	O
neglected	O
,	O
the	O
optimal	O
linear	O
(	O
cid:12	O
)	O
lter	O
is	O
the	O
pseudoinverse	B
c	O
regularizes	O
this	O
ill-conditioned	O
inverse	O
.	O
the	O
optimal	O
linear	O
(	O
cid:12	O
)	O
lter	O
can	O
also	O
be	O
manipulated	O
into	O
the	O
form	O
:	O
optimal	O
linear	O
(	O
cid:12	O
)	O
lter	O
=	O
c	O
(	O
cid:0	O
)	O
1rt	O
''	O
rc	O
(	O
cid:0	O
)	O
1rt	O
+	O
i	O
#	O
(	O
cid:0	O
)	O
1	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
(	O
cid:27	O
)	O
2	O
f	O
:	O
(	O
46.8	O
)	O
minimum	O
square	O
error	O
derivation	O
the	O
non-bayesian	O
derivation	B
of	O
the	O
optimal	O
linear	O
(	O
cid:12	O
)	O
lter	O
starts	O
by	O
assuming	O
that	O
we	O
will	O
‘	O
estimate	O
’	O
the	O
true	O
image	B
f	O
by	O
a	O
linear	B
function	O
of	O
the	O
data	O
:	O
^f	O
=	O
wd	O
:	O
(	O
46.9	O
)	O
the	O
linear	B
operator	O
w	O
is	O
then	O
‘	O
optimized	O
’	O
by	O
minimizing	O
the	O
expected	O
sum-	O
squared	O
error	O
between	O
^f	O
and	O
the	O
unknown	O
true	O
image	B
.	O
in	O
the	O
following	O
equa-	O
tions	O
,	O
summations	O
over	O
repeated	O
indices	O
k	O
,	O
k0	O
,	O
n	O
are	O
implicit	O
.	O
the	O
expectation	B
h	O
(	O
cid:1	O
)	O
i	O
is	O
over	O
both	O
the	O
statistics	O
of	O
the	O
random	O
variables	O
fnng	O
,	O
and	O
the	O
ensemble	B
of	O
images	B
f	O
which	O
we	O
expect	O
to	O
bump	O
into	O
.	O
we	O
assume	O
that	O
the	O
noise	B
is	O
zero	O
mean	B
and	O
uncorrelated	O
to	O
second	O
order	O
with	O
itself	O
and	O
everything	O
else	O
,	O
with	O
hnnnn0i	O
=	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
(	O
cid:14	O
)	O
nn0	O
.	O
hei	O
=	O
=	O
1	O
2d	O
(	O
wkndn	O
(	O
cid:0	O
)	O
fk	O
)	O
2e	O
2d	O
(	O
wknrnjfj	O
(	O
cid:0	O
)	O
fk	O
)	O
2e	O
+	O
1	O
1	O
2	O
wknwkn	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
:	O
(	O
46.10	O
)	O
(	O
46.11	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
46.1	O
:	O
traditional	O
image	B
reconstruction	I
methods	O
551	O
di	O
(	O
cid:11	O
)	O
erentiating	O
with	O
respect	O
to	O
w	O
,	O
and	O
introducing	O
f	O
(	O
cid:17	O
)	O
(	O
cid:10	O
)	O
fj0fj	O
(	O
cid:11	O
)	O
(	O
cf	O
.	O
(	O
cid:27	O
)	O
2	O
the	O
bayesian	O
derivation	B
above	O
)	O
,	O
we	O
(	O
cid:12	O
)	O
nd	O
that	O
the	O
optimal	O
linear	O
(	O
cid:12	O
)	O
lter	O
is	O
:	O
f	O
c	O
(	O
cid:0	O
)	O
1	O
in	O
wopt	O
=	O
frt	O
(	O
cid:2	O
)	O
rfrt	O
+	O
(	O
cid:27	O
)	O
2	O
(	O
cid:23	O
)	O
i	O
(	O
cid:3	O
)	O
(	O
cid:0	O
)	O
1	O
:	O
(	O
46.12	O
)	O
f	O
c	O
(	O
cid:0	O
)	O
1	O
,	O
we	O
obtain	O
the	O
optimal	O
linear	O
(	O
cid:12	O
)	O
lter	O
(	O
46.8	O
)	O
of	O
the	O
if	O
we	O
identify	O
f	O
=	O
(	O
cid:27	O
)	O
2	O
bayesian	O
derivation	B
.	O
the	O
ad	O
hoc	O
assumptions	B
made	O
in	O
this	O
derivation	B
were	O
the	O
choice	O
of	O
a	O
quadratic	O
error	O
measure	O
,	O
and	O
the	O
decision	O
to	O
use	O
a	O
linear	B
estimator	O
.	O
it	O
is	O
interesting	O
that	O
without	O
explicit	O
assumptions	B
of	O
gaussian	O
distributions	O
,	O
this	O
derivation	B
has	O
reproduced	O
the	O
same	O
estimator	B
as	O
the	O
bayesian	O
posterior	O
mode	O
,	O
fmp	O
.	O
the	O
advantage	O
of	O
a	O
bayesian	O
approach	O
is	O
that	O
we	O
can	O
criticize	O
these	O
as-	O
sumptions	O
and	O
modify	O
them	O
in	O
order	O
to	O
make	O
better	O
reconstructions	O
.	O
other	O
image	B
models	I
the	O
better	O
matched	O
our	O
model	B
of	O
images	B
p	O
(	O
f	O
jh	O
)	O
is	O
to	O
the	O
real	O
world	O
,	O
the	O
bet-	O
ter	O
our	O
image	B
reconstructions	O
will	O
be	O
,	O
and	O
the	O
less	O
data	O
we	O
will	O
need	O
to	O
answer	O
any	O
given	O
question	O
.	O
the	O
gaussian	O
models	O
which	O
lead	O
to	O
the	O
optimal	O
linear	O
(	O
cid:12	O
)	O
lter	O
are	O
spectacularly	O
poorly	O
matched	O
to	O
the	O
real	O
world	O
.	O
for	O
example	O
,	O
the	O
gaussian	O
prior	B
(	O
46.3	O
)	O
fails	O
to	O
specify	O
that	O
all	O
pixel	O
intensities	O
in	O
an	O
image	B
are	O
positive	O
.	O
this	O
omission	O
leads	O
to	O
the	O
most	O
pronounced	O
artefacts	O
where	O
the	O
im-	O
age	O
under	O
observation	O
has	O
high	O
contrast	O
or	O
large	O
black	B
patches	O
.	O
optimal	O
linear	O
(	O
cid:12	O
)	O
lters	O
applied	O
to	O
astronomical	O
data	O
give	O
reconstructions	O
with	O
negative	O
areas	O
in	O
them	O
,	O
corresponding	O
to	O
patches	O
of	O
sky	O
that	O
suck	O
energy	B
out	O
of	O
telescopes	O
!	O
the	O
maximum	B
entropy	I
model	O
for	O
image	O
deconvolution	B
(	O
gull	O
and	O
daniell	O
,	O
1978	O
)	O
was	O
a	O
great	O
success	O
principally	O
because	O
this	O
model	B
forced	O
the	O
reconstructed	O
image	B
to	O
be	O
positive	O
.	O
the	O
spurious	O
negative	O
areas	O
and	O
complementary	O
spu-	O
rious	O
positive	O
areas	O
are	O
eliminated	O
,	O
and	O
the	O
quality	O
of	O
the	O
reconstruction	O
is	O
greatly	O
enhanced	O
.	O
the	O
‘	O
classic	O
maximum	B
entropy	I
’	O
model	B
assigns	O
an	O
entropic	O
prior	O
p	O
(	O
f	O
j	O
(	O
cid:11	O
)	O
;	O
m	O
;	O
hclassic	O
)	O
=	O
exp	O
(	O
(	O
cid:11	O
)	O
s	O
(	O
f	O
;	O
m	O
)	O
)	O
=z	O
;	O
s	O
(	O
f	O
;	O
m	O
)	O
=xi	O
(	O
fi	O
ln	O
(	O
mi=fi	O
)	O
+	O
fi	O
(	O
cid:0	O
)	O
mi	O
)	O
(	O
46.13	O
)	O
(	O
46.14	O
)	O
where	O
(	O
skilling	O
,	O
1989	O
)	O
.	O
this	O
model	B
enforces	O
positivity	B
;	O
the	O
parameter	O
(	O
cid:11	O
)	O
de	O
(	O
cid:12	O
)	O
nes	O
a	O
characteristic	O
dynamic	O
range	O
by	O
which	O
the	O
pixel	O
values	O
are	O
expected	O
to	O
di	O
(	O
cid:11	O
)	O
er	O
from	O
the	O
default	O
image	B
m.	O
the	O
‘	O
intrinsic-correlation-function	O
maximum-entropy	O
’	O
model	B
(	O
gull	O
,	O
1989	O
)	O
introduces	O
an	O
expectation	B
of	O
spatial	O
correlations	B
into	O
the	O
prior	B
on	O
f	O
by	O
writing	O
f	O
=	O
gh	O
,	O
where	O
g	O
is	O
a	O
convolution	B
with	O
an	O
intrinsic	B
correlation	I
function	I
,	O
and	O
putting	O
a	O
classic	O
maxent	B
prior	O
on	O
the	O
underlying	O
hidden	O
image	O
h.	O
probabilistic	O
movies	O
having	O
found	O
not	O
only	O
the	O
most	O
probable	O
image	O
fmp	O
but	O
also	O
error	B
bars	I
on	O
it	O
,	O
(	O
cid:6	O
)	O
fjd	O
,	O
one	O
task	O
is	O
to	O
visualize	O
those	O
error	B
bars	I
.	O
whether	O
or	O
not	O
we	O
use	O
monte	O
carlo	O
methods	B
to	O
infer	O
f	O
,	O
a	O
correlated	O
random	O
walk	O
around	O
the	O
posterior	O
distribution	O
can	O
be	O
used	O
to	O
visualize	O
the	O
uncertainties	O
and	O
correlations	O
.	O
for	O
a	O
gaussian	O
posterior	O
distribution	O
,	O
we	O
can	O
create	O
a	O
correlated	O
sequence	O
of	O
unit	O
normal	B
random	O
vectors	B
n	O
using	O
n	O
(	O
t+1	O
)	O
=	O
cn	O
(	O
t	O
)	O
+	O
sz	O
;	O
(	O
46.15	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
552	O
46	O
|	O
deconvolution	B
where	O
z	O
is	O
a	O
unit	O
normal	B
random	O
vector	O
and	O
c2	O
+	O
s2	O
=	O
1	O
(	O
c	O
controls	O
how	O
persistent	O
the	O
memory	B
of	O
the	O
sequence	B
is	O
)	O
.	O
we	O
then	O
render	O
the	O
image	B
sequence	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
f	O
(	O
t	O
)	O
=	O
fmp	O
+	O
(	O
cid:6	O
)	O
1=2	O
fjd	O
n	O
(	O
t	O
)	O
fjd	O
is	O
the	O
cholesky	O
decomposition	O
of	O
(	O
cid:6	O
)	O
fjd	O
.	O
where	O
(	O
cid:6	O
)	O
1=2	O
(	O
46.16	O
)	O
46.2	O
supervised	O
neural	O
networks	O
for	O
image	O
deconvolution	B
neural	O
network	B
researchers	O
often	O
exploit	B
the	O
following	O
strategy	O
.	O
given	O
a	O
prob-	O
lem	O
currently	O
solved	O
with	O
a	O
standard	O
algorithm	O
:	O
interpret	O
the	O
computations	O
performed	O
by	O
the	O
algorithm	B
as	O
a	O
parameterized	O
mapping	B
from	O
an	O
input	O
to	O
an	O
output	O
,	O
and	O
call	O
this	O
mapping	B
a	O
neural	B
network	I
;	O
then	O
adapt	O
the	O
parameters	B
to	O
data	O
so	O
as	O
to	O
produce	O
another	O
mapping	B
that	O
solves	O
the	O
task	O
better	O
.	O
by	O
construction	O
,	O
the	O
neural	B
network	I
can	O
reproduce	O
the	O
standard	O
algorithm	O
,	O
so	O
this	O
data-driven	O
adaptation	O
can	O
only	O
make	O
the	O
performance	O
better	O
.	O
there	O
are	O
several	O
reasons	O
why	O
standard	O
algorithms	O
can	O
be	O
bettered	O
in	O
this	O
way	O
.	O
1.	O
algorithms	B
are	O
often	O
not	O
designed	O
to	O
optimize	O
the	O
real	O
objective	O
func-	O
tion	O
.	O
for	O
example	O
,	O
in	O
speech	O
recognition	B
,	O
a	O
hidden	O
markov	O
model	B
is	O
designed	O
to	O
model	B
the	O
speech	O
signal	O
,	O
and	O
is	O
(	O
cid:12	O
)	O
tted	O
so	O
as	O
to	O
to	O
maximize	O
the	O
generative	O
probability	O
given	O
the	O
known	O
string	O
of	O
words	O
in	O
the	O
training	B
data	I
;	O
but	O
the	O
real	O
objective	O
is	O
to	O
discriminate	O
between	O
di	O
(	O
cid:11	O
)	O
erent	O
words	O
.	O
if	O
an	O
inadequate	O
model	B
is	O
being	O
used	O
,	O
the	O
neural-net-style	O
training	O
of	O
the	O
model	B
will	O
focus	B
the	O
limited	O
resources	O
of	O
the	O
model	O
on	O
the	O
aspects	O
relevant	O
to	O
the	O
discrimination	O
task	O
.	O
discriminative	B
training	I
of	O
hidden	O
markov	O
models	O
for	O
speech	O
recognition	B
does	O
improve	O
their	O
performance	O
.	O
2.	O
the	O
neural	B
network	I
can	O
be	O
more	O
(	O
cid:13	O
)	O
exible	O
than	O
the	O
standard	O
model	O
;	O
some	O
of	O
the	O
adaptive	O
parameters	B
might	O
have	O
been	O
viewed	O
as	O
(	O
cid:12	O
)	O
xed	O
features	O
by	O
the	O
original	O
designers	O
.	O
a	O
(	O
cid:13	O
)	O
exible	O
network	B
can	O
(	O
cid:12	O
)	O
nd	O
properties	O
in	O
the	O
data	O
that	O
were	O
not	O
included	O
in	O
the	O
original	O
model	B
.	O
46.3	O
deconvolution	B
in	O
humans	O
a	O
huge	O
fraction	O
of	O
our	O
brain	B
is	O
devoted	O
to	O
vision	B
.	O
one	O
of	O
the	O
neglected	O
features	O
of	O
our	O
visual	O
system	O
is	O
that	O
the	O
raw	O
image	B
falling	O
on	O
the	O
retina	O
is	O
severely	O
blurred	O
:	O
while	O
most	O
people	O
can	O
see	O
with	O
a	O
resolution	O
of	O
about	O
1	O
arcminute	O
(	O
one	O
sixtieth	O
of	O
a	O
degree	B
)	O
under	O
any	O
daylight	O
conditions	O
,	O
bright	O
or	O
dim	O
,	O
the	O
image	B
on	O
our	O
retina	O
is	O
blurred	O
through	O
a	O
point	B
spread	I
function	I
of	O
width	O
as	O
large	O
as	O
5	O
arcminutes	O
(	O
wald	O
and	O
gri	O
(	O
cid:14	O
)	O
n	O
,	O
1947	O
;	O
howarth	O
and	O
bradley	O
,	O
1986	O
)	O
.	O
it	O
is	O
amazing	O
that	O
we	O
are	O
able	O
to	O
resolve	O
pixels	O
that	O
are	O
twenty-	O
(	O
cid:12	O
)	O
ve	O
times	O
smaller	O
in	O
area	O
than	O
the	O
blob	O
produced	O
on	O
our	O
retina	O
by	O
any	O
point	O
source	O
.	O
isaac	O
newton	O
was	O
aware	O
of	O
this	O
conundrum	O
.	O
it	O
’	O
s	O
hard	O
to	O
make	O
a	O
lens	O
that	O
does	O
not	O
have	O
chromatic	B
aberration	I
,	O
and	O
our	O
cornea	O
and	O
lens	O
,	O
like	O
a	O
lens	O
made	O
of	O
ordinary	O
glass	O
,	O
refract	O
blue	O
light	O
more	O
strongly	O
than	O
red	O
.	O
typically	O
our	O
eyes	O
focus	B
correctly	O
for	O
the	O
middle	O
of	O
the	O
visible	O
spectrum	O
(	O
green	O
)	O
,	O
so	O
if	O
we	O
look	O
at	O
a	O
single	O
white	O
dot	O
made	O
of	O
red	O
,	O
green	O
,	O
and	O
blue	O
light	O
,	O
the	O
image	B
on	O
our	O
retina	O
consists	O
of	O
a	O
sharply	O
focussed	O
green	O
dot	O
surrounded	O
by	O
a	O
broader	O
red	O
blob	O
superposed	O
on	O
an	O
even	O
broader	O
blue	O
blob	O
.	O
the	O
width	O
of	O
the	O
red	O
and	O
blue	O
blobs	O
is	O
proportional	O
to	O
the	O
diameter	O
of	O
the	O
pupil	O
,	O
which	O
is	O
largest	O
under	O
dim	O
lighting	O
conditions	O
.	O
[	O
the	O
blobs	O
are	O
roughly	O
concentric	O
,	O
though	O
most	O
people	O
have	O
a	O
slight	O
bias	B
,	O
such	O
that	O
in	O
one	O
eye	O
the	O
red	O
blob	O
is	O
centred	O
a	O
tiny	O
distance	B
 	I
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
46.3	O
:	O
deconvolution	B
in	O
humans	O
553	O
to	O
the	O
left	O
and	O
the	O
blue	O
is	O
centred	O
a	O
tiny	O
distance	B
to	O
the	O
right	O
,	O
and	O
in	O
the	O
other	O
eye	O
it	O
’	O
s	O
the	O
other	O
way	O
round	O
.	O
this	O
slight	O
bias	B
explains	O
why	O
when	O
we	O
look	O
at	O
blue	O
and	O
red	O
writing	B
on	O
a	O
dark	O
background	O
most	O
people	O
perceive	O
the	O
blue	O
writing	B
to	O
be	O
at	O
a	O
slightly	O
greater	O
depth	O
than	O
the	O
red	O
.	O
in	O
a	O
minority	O
of	O
people	O
,	O
this	O
small	O
bias	B
is	O
the	O
other	O
way	O
round	O
and	O
the	O
red/blue	O
depth	O
perception	O
is	O
reversed	O
.	O
but	O
this	O
e	O
(	O
cid:11	O
)	O
ect	O
(	O
which	O
many	O
people	O
are	O
aware	O
of	O
,	O
having	O
noticed	O
it	O
in	O
cinemas	O
,	O
for	O
example	O
)	O
is	O
tiny	O
compared	O
with	O
the	O
chromatic	B
aberration	I
we	O
are	O
discussing	O
.	O
]	O
you	O
can	O
vividly	O
demonstrate	O
to	O
yourself	O
how	O
enormous	O
the	O
chromatic	O
aber-	O
ration	O
in	O
your	O
eye	O
is	O
with	O
the	O
help	O
of	O
a	O
sheet	O
of	O
card	O
and	O
a	O
colour	O
computer	O
screen	O
.	O
for	O
the	O
most	O
impressive	O
results	O
{	O
i	O
guarantee	O
you	O
will	O
be	O
amazed	O
{	O
use	O
a	O
dim	O
room	O
with	O
no	O
light	O
apart	O
from	O
the	O
computer	B
screen	O
;	O
a	O
pretty	O
strong	O
e	O
(	O
cid:11	O
)	O
ect	O
will	O
still	O
be	O
seen	O
even	O
if	O
the	O
room	O
has	O
daylight	O
coming	O
into	O
it	O
,	O
as	O
long	O
as	O
it	O
is	O
not	O
bright	O
sunshine	O
.	O
cut	O
a	O
slit	O
about	O
1.5	O
mm	O
wide	O
in	O
the	O
card	B
.	O
on	O
the	O
screen	O
,	O
display	O
a	O
few	O
small	O
coloured	B
objects	O
on	O
a	O
black	B
background	O
.	O
i	O
especially	O
recommend	O
thin	O
vertical	O
objects	O
coloured	B
pure	O
red	O
,	O
pure	O
blue	O
,	O
magenta	O
(	O
i.e.	O
,	O
red	O
plus	O
blue	O
)	O
,	O
and	O
white	O
(	O
red	O
plus	O
blue	O
plus	O
green	O
)	O
.1	O
include	O
a	O
little	O
black-	O
and-white	O
text	O
on	O
the	O
screen	O
too	O
.	O
stand	O
or	O
sit	O
su	O
(	O
cid:14	O
)	O
ciently	O
far	O
away	O
that	O
you	O
can	O
only	O
just	O
read	O
the	O
text	O
{	O
perhaps	O
a	O
distance	B
of	O
four	O
metres	O
or	O
so	O
,	O
if	O
you	O
have	O
normal	B
vision	O
.	O
now	O
,	O
hold	O
the	O
slit	O
vertically	O
in	O
front	O
of	O
one	O
of	O
your	O
eyes	O
,	O
and	O
close	O
the	O
other	O
eye	O
.	O
hold	O
the	O
slit	O
near	O
to	O
your	O
eye	O
{	O
brushing	O
your	O
eyelashes	O
{	O
and	O
look	O
through	O
it	O
.	O
waggle	O
the	O
slit	O
slowly	O
to	O
the	O
left	O
and	O
to	O
the	O
right	O
,	O
so	O
that	O
the	O
slit	O
is	O
alternately	O
in	O
front	O
of	O
the	O
left	O
and	O
right	O
sides	O
of	O
your	O
pupil	B
.	O
what	O
do	O
you	O
see	O
?	O
i	O
see	O
the	O
red	O
objects	O
waggling	O
to	O
and	O
fro	O
,	O
and	O
the	O
blue	O
objects	O
waggling	O
to	O
and	O
fro	O
,	O
through	O
huge	O
distances	O
and	O
in	O
opposite	O
directions	O
,	O
while	O
white	B
objects	O
appear	O
to	O
stay	O
still	O
and	O
are	O
negligibly	O
distorted	O
.	O
thin	O
magenta	O
objects	O
can	O
be	O
seen	O
splitting	O
into	O
their	O
constituent	O
red	O
and	O
blue	O
parts	O
.	O
measure	O
how	O
large	O
the	O
motion	O
of	O
the	O
red	O
and	O
blue	O
objects	O
is	O
{	O
it	O
’	O
s	O
more	O
than	O
5	O
minutes	O
of	O
arc	O
for	O
me	O
,	O
in	O
a	O
dim	O
room	O
.	O
then	O
check	O
how	O
sharply	O
you	O
can	O
see	O
under	O
these	O
conditions	O
{	O
look	O
at	O
the	O
text	O
on	O
the	O
screen	O
,	O
for	O
example	O
:	O
is	O
it	O
not	O
the	O
case	O
that	O
you	O
can	O
see	O
(	O
through	O
your	O
whole	O
pupil	B
)	O
features	O
far	O
smaller	O
than	O
the	O
distance	B
through	O
which	O
the	O
red	O
and	O
blue	O
components	O
were	O
waggling	O
?	O
yet	O
when	O
you	O
are	O
using	O
the	O
whole	O
pupil	B
,	O
what	O
is	O
falling	O
on	O
your	O
retina	O
must	O
be	O
an	O
image	B
blurred	O
with	O
a	O
blurring	O
diameter	O
equal	O
to	O
the	O
waggling	O
amplitude	O
.	O
one	O
of	O
the	O
main	O
functions	B
of	O
early	O
visual	O
processing	O
must	O
be	O
to	O
deconvolve	O
this	O
chromatic	B
aberration	I
.	O
neuroscientists	O
sometimes	O
conjecture	O
that	O
the	O
rea-	O
son	O
why	O
retinal	O
ganglion	B
cells	I
and	O
cells	O
in	O
the	O
lateral	O
geniculate	O
nucleus	O
(	O
the	O
main	O
brain	B
area	O
to	O
which	O
retinal	O
ganglion	B
cells	I
project	O
)	O
have	O
centre-surround	O
receptive	O
(	O
cid:12	O
)	O
elds	O
with	O
colour	O
opponency	O
(	O
long	O
wavelength	O
in	O
the	O
centre	O
and	O
medium	O
wavelength	O
in	O
the	O
surround	O
,	O
for	O
example	O
)	O
is	O
in	O
order	O
to	O
perform	O
‘	O
fea-	O
ture	O
extraction	O
’	O
or	O
‘	O
edge	B
detection	O
’	O
,	O
but	O
i	O
think	O
this	O
view	O
is	O
mistaken	O
.	O
the	O
reason	O
we	O
have	O
centre-surround	O
(	O
cid:12	O
)	O
lters	O
at	O
the	O
(	O
cid:12	O
)	O
rst	O
stage	O
of	O
visual	O
processing	O
(	O
in	O
the	O
fovea	B
at	O
least	O
)	O
is	O
for	O
the	O
huge	O
task	O
of	O
deconvolution	O
of	O
chromatic	O
aber-	O
ration	O
.	O
i	O
speculate	O
that	O
the	O
mccollough	O
e	O
(	O
cid:11	O
)	O
ect	O
,	O
an	O
extremely	O
long-lasting	O
associ-	O
ation	O
of	O
colours	O
with	O
orientation	O
(	O
mccollough	O
,	O
1965	O
;	O
mackay	O
and	O
mackay	O
,	O
1974	O
)	O
,	O
is	O
produced	O
by	O
the	O
adaptation	O
mechanism	O
that	O
tunes	O
our	O
chromatic-	O
aberration-deconvolution	O
circuits	O
.	O
our	O
deconvolution	B
circuits	O
need	O
to	O
be	O
rapidly	O
tuneable	O
,	O
because	O
the	O
point	B
spread	I
function	I
of	O
our	O
eye	O
changes	O
with	O
our	O
pupil	B
diameter	O
,	O
which	O
can	O
change	O
within	O
seconds	O
;	O
and	O
indeed	O
the	O
mccollough	O
e	O
(	O
cid:11	O
)	O
ect	O
can	O
be	O
induced	O
within	O
30	O
seconds	O
.	O
at	O
the	O
same	O
time	O
,	O
the	O
e	O
(	O
cid:11	O
)	O
ect	O
is	O
long-lasting	O
1http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/files.html	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
554	O
46	O
|	O
deconvolution	B
when	O
an	O
eye	O
is	O
covered	O
,	O
because	O
it	O
’	O
s	O
in	O
our	O
interests	O
that	O
our	O
deconvolution	B
circuits	O
should	O
stay	O
well-tuned	O
while	O
we	O
sleep	B
,	O
so	O
that	O
we	O
can	O
see	O
sharply	O
the	O
instant	O
we	O
wake	O
up	O
.	O
i	O
also	O
wonder	O
whether	O
the	O
main	O
reason	O
that	O
we	O
evolved	O
colour	B
vision	I
was	O
not	O
‘	O
in	O
order	O
to	O
see	O
fruit	O
better	O
’	O
but	O
‘	O
so	O
as	O
to	O
be	O
able	O
to	O
see	O
black	O
and	O
white	O
sharper	O
’	O
{	O
deconvolving	O
chromatic	B
aberration	I
is	O
easier	O
,	O
even	O
in	O
an	O
entirely	O
black	B
and	O
white	B
world	O
,	O
if	O
one	O
has	O
access	O
to	O
chromatic	O
information	O
in	O
the	O
image	B
.	O
and	O
a	O
(	O
cid:12	O
)	O
nal	O
speculation	O
:	O
why	O
do	O
our	O
eyes	O
make	O
micro-saccades	B
when	O
we	O
look	O
at	O
things	O
?	O
these	O
miniature	O
eye-movements	O
are	O
of	O
an	O
angular	O
size	O
big-	O
ger	O
than	O
the	O
spacing	O
between	O
the	O
cones	B
in	O
the	O
fovea	B
(	O
which	O
are	O
spaced	O
at	O
roughly	O
1	O
minute	O
of	O
arc	O
,	O
the	O
perceived	O
resolution	O
of	O
the	O
eye	O
)	O
.	O
the	O
typical	B
size	O
of	O
a	O
microsaccade	O
is	O
5	O
{	O
10	O
minutes	O
of	O
arc	O
(	O
ratli	O
(	O
cid:11	O
)	O
and	O
riggs	O
,	O
1950	O
)	O
.	O
is	O
it	O
a	O
coincidence	B
that	O
this	O
is	O
the	O
same	O
as	O
the	O
size	O
of	O
chromatic	O
aberration	O
?	O
surely	O
micro-saccades	B
must	O
play	O
an	O
essential	O
role	O
in	O
the	O
deconvolution	B
mechanism	O
that	O
delivers	O
our	O
high-resolution	O
vision	B
.	O
46.4	O
exercises	O
exercise	O
46.1	O
.	O
[	O
3c	O
]	O
blur	B
an	O
image	B
with	O
a	O
circular	O
(	O
top	O
hat	B
)	O
point	O
spread	O
func-	O
tion	O
and	O
add	O
noise	B
.	O
then	O
deconvolve	O
the	O
blurry	O
noisy	B
image	O
using	O
the	O
optimal	O
linear	O
(	O
cid:12	O
)	O
lter	O
.	O
find	O
error	B
bars	I
and	O
visualize	O
them	O
by	O
making	O
a	O
probabilistic	B
movie	I
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
part	O
vi	O
sparse	B
graph	I
codes	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
part	O
vi	O
the	O
central	O
problem	O
of	O
communication	O
theory	B
is	O
to	O
construct	O
an	O
encoding	O
and	O
a	O
decoding	B
system	O
that	O
make	O
it	O
possible	O
to	O
communicate	O
reliably	O
over	O
a	O
noisy	B
channel	I
.	O
during	O
the	O
1990s	O
,	O
remarkable	O
progress	O
was	O
made	O
towards	O
the	O
shannon	O
limit	O
,	O
using	O
codes	O
that	O
are	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
terms	O
of	O
sparse	O
random	B
graphs	O
,	O
and	O
which	O
are	O
decoded	O
by	O
a	O
simple	O
probability-based	O
message-passing	B
algorithm	O
.	O
in	O
a	O
sparse-graph	B
code	I
,	O
the	O
nodes	O
in	O
the	O
graph	B
represent	O
the	O
transmitted	O
bits	O
and	O
the	O
constraints	O
they	O
satisfy	O
.	O
for	O
a	O
linear	B
code	O
with	O
a	O
codeword	B
length	O
n	O
and	O
rate	O
r	O
=	O
k=n	O
,	O
the	O
number	O
of	O
constraints	O
is	O
of	O
order	O
m	O
=	O
n	O
(	O
cid:0	O
)	O
k.	O
any	O
linear	B
code	O
can	O
be	O
described	O
by	O
a	O
graph	B
,	O
but	O
what	O
makes	O
a	O
sparse-graph	B
code	I
special	O
is	O
that	O
each	O
constraint	O
involves	O
only	O
a	O
small	O
number	O
of	O
variables	O
in	O
the	O
graph	B
:	O
so	O
the	O
number	O
of	O
edges	O
in	O
the	O
graph	B
scales	O
roughly	O
linearly	O
with	O
n	O
,	O
rather	O
than	O
quadratically	O
.	O
in	O
the	O
following	O
four	O
chapters	O
we	O
will	O
look	O
at	O
four	O
families	O
of	O
sparse-graph	O
codes	O
:	O
three	O
families	O
that	O
are	O
excellent	O
for	O
error-correction	O
:	O
low-density	O
parity-	O
check	O
codes	O
,	O
turbo	B
codes	I
,	O
and	O
repeat	O
{	O
accumulate	O
codes	O
;	O
and	O
the	O
family	O
of	O
digital	O
fountain	O
codes	O
,	O
which	O
are	O
outstanding	O
for	O
erasure-correction	O
.	O
all	O
these	O
codes	O
can	O
be	O
decoded	O
by	O
a	O
local	O
message-passing	B
algorithm	O
on	O
the	O
graph	B
,	O
the	O
sum	O
{	O
product	O
algorithm	O
,	O
and	O
,	O
while	O
this	O
algorithm	B
is	O
not	O
a	O
perfect	B
maximum	O
likelihood	B
decoder	O
,	O
the	O
empirical	O
results	O
are	O
record-breaking	O
.	O
556	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
47	O
low-density	B
parity-check	I
codes	O
h	O
=	O
figure	O
47.1.	O
a	O
low-density	B
parity-check	I
matrix	O
and	O
the	O
corresponding	O
graph	B
of	O
a	O
rate-1/4	O
low-density	B
parity-check	I
code	I
with	O
blocklength	O
n	O
=	O
16	O
,	O
and	O
m	O
=	O
12	O
constraints	O
.	O
each	O
white	B
circle	O
represents	O
a	O
transmitted	O
bit	B
.	O
each	O
bit	B
participates	O
in	O
j	O
=	O
3	O
constraints	O
,	O
represented	O
by	O
squares	O
.	O
each	O
constraint	O
forces	O
the	O
sum	O
of	O
the	O
k	O
=	O
4	O
bits	O
to	O
which	O
it	O
is	O
connected	O
to	O
be	O
even	O
.	O
a	O
low-density	B
parity-check	I
code	I
(	O
or	O
gallager	O
code	B
)	O
is	O
a	O
block	B
code	I
that	O
has	O
a	O
parity-check	B
matrix	I
,	O
h	O
,	O
every	O
row	O
and	O
column	O
of	O
which	O
is	O
‘	O
sparse	O
’	O
.	O
a	O
regular	B
gallager	O
code	B
is	O
a	O
low-density	B
parity-check	I
code	I
in	O
which	O
every	O
column	O
of	O
h	O
has	O
the	O
same	O
weight	B
j	O
and	O
every	O
row	O
has	O
the	O
same	O
weight	B
k	O
;	O
reg-	O
ular	O
gallager	O
codes	O
are	O
constructed	O
at	O
random	B
subject	O
to	O
these	O
constraints	O
.	O
a	O
low-density	B
parity-check	I
code	I
with	O
j	O
=	O
3	O
and	O
k	O
=	O
4	O
is	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gure	O
47.1	O
.	O
47.1	O
theoretical	O
properties	O
low-density	B
parity-check	I
codes	O
lend	O
themselves	O
to	O
theoretical	O
study	O
.	O
the	O
fol-	O
lowing	O
results	O
are	O
proved	O
in	O
gallager	O
(	O
1963	O
)	O
and	O
mackay	O
(	O
1999b	O
)	O
.	O
low-density	B
parity-check	I
codes	O
,	O
in	O
spite	O
of	O
their	O
simple	O
construction	O
,	O
are	O
good	B
codes	O
,	O
given	O
an	O
optimal	B
decoder	I
(	O
good	B
codes	O
in	O
the	O
sense	O
of	O
section	O
11.4	O
)	O
.	O
furthermore	O
,	O
they	O
have	O
good	B
distance	O
(	O
in	O
the	O
sense	O
of	O
section	O
13.2	O
)	O
.	O
these	O
two	O
results	O
hold	O
for	O
any	O
column	O
weight	B
j	O
(	O
cid:21	O
)	O
3.	O
furthermore	O
,	O
there	O
are	O
sequences	O
of	O
low-density	O
parity-check	O
codes	O
in	O
which	O
j	O
increases	O
gradually	O
with	O
n	O
,	O
in	O
such	O
a	O
way	O
that	O
the	O
ratio	O
j=n	O
still	O
goes	O
to	O
zero	O
,	O
that	O
are	O
very	B
good	I
,	O
and	O
that	O
have	O
very	B
good	I
distance	O
.	O
however	O
,	O
we	O
don	O
’	O
t	O
have	O
an	O
optimal	B
decoder	I
,	O
and	O
decoding	O
low-density	B
parity-check	I
codes	O
is	O
an	O
np-complete	O
problem	O
.	O
so	O
what	O
can	O
we	O
do	O
in	O
practice	O
?	O
47.2	O
practical	B
decoding	O
given	O
a	O
channel	B
output	O
r	O
,	O
we	O
wish	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
codeword	B
t	O
whose	O
likelihood	B
p	O
(	O
rj	O
t	O
)	O
is	O
biggest	O
.	O
all	O
the	O
e	O
(	O
cid:11	O
)	O
ective	O
decoding	B
strategies	O
for	O
low-density	O
parity-	O
check	O
codes	O
are	O
message-passing	B
algorithms	O
.	O
the	O
best	O
algorithm	B
known	O
is	O
the	O
sum	O
{	O
product	O
algorithm	O
,	O
also	O
known	O
as	O
iterative	O
probabilistic	O
decoding	O
or	O
belief	B
propagation	I
.	O
we	O
’	O
ll	O
assume	O
that	O
the	O
channel	B
is	O
a	O
memoryless	O
channel	B
(	O
though	O
more	O
com-	O
plex	O
channels	O
can	O
easily	O
be	O
handled	O
by	O
running	O
the	O
sum	O
{	O
product	O
algorithm	O
on	O
a	O
more	O
complex	B
graph	O
that	O
represents	O
the	O
expected	O
correlations	B
among	O
the	O
errors	B
(	O
worthen	O
and	O
stark	O
,	O
1998	O
)	O
)	O
.	O
for	O
any	O
memoryless	O
channel	B
,	O
there	O
are	O
two	O
approaches	O
to	O
the	O
decoding	B
problem	O
,	O
both	O
of	O
which	O
lead	O
to	O
the	O
generic	O
problem	O
‘	O
(	O
cid:12	O
)	O
nd	O
the	O
x	O
that	O
maximizes	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
)	O
	O
[	O
hx	O
=	O
z	O
]	O
’	O
;	O
(	O
47.1	O
)	O
where	O
p	O
(	O
x	O
)	O
is	O
a	O
separable	O
distribution	B
on	O
a	O
binary	O
vector	O
x	O
,	O
and	O
z	O
is	O
another	O
binary	O
vector	O
.	O
each	O
of	O
these	O
two	O
approaches	O
represents	O
the	O
decoding	B
problem	O
in	O
terms	O
of	O
a	O
factor	B
graph	I
(	O
chapter	O
26	O
)	O
.	O
557	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
558	O
47	O
|	O
low-density	B
parity-check	I
codes	O
tn	O
(	O
a	O
)	O
the	O
prior	B
distribution	O
over	O
codewords	O
p	O
(	O
t	O
)	O
/	O
 	B
[	O
ht	O
=	O
0	O
]	O
:	O
p	O
(	O
rn	O
j	O
tn	O
)	O
tn	O
p	O
(	O
nn	O
)	O
nn	O
zm	O
the	O
variable	O
nodes	O
are	O
the	O
transmitted	O
bits	O
ftng	O
.	O
each	O
node	O
represents	O
the	O
factor	O
[	O
pn2n	O
(	O
m	O
)	O
tn	O
=	O
0	O
mod	O
2	O
]	O
.	O
(	O
b	O
)	O
the	O
posterior	O
distribution	O
over	O
codewords	O
,	O
p	O
(	O
tj	O
r	O
)	O
/	O
p	O
(	O
t	O
)	O
p	O
(	O
rj	O
t	O
)	O
:	O
each	O
upper	O
function	B
node	O
represents	O
a	O
likelihood	B
factor	O
p	O
(	O
rn	O
j	O
tn	O
)	O
.	O
(	O
c	O
)	O
the	O
joint	B
probability	O
of	O
the	O
noise	O
n	O
and	O
syndrome	O
z	O
,	O
p	O
(	O
n	O
;	O
z	O
)	O
=	O
p	O
(	O
n	O
)	O
[	O
z	O
=	O
hn	O
]	O
:	O
the	O
top	O
variable	O
nodes	O
are	O
now	O
the	O
noise	B
bits	O
fnng	O
.	O
the	O
added	O
variable	O
nodes	O
at	O
the	O
base	O
are	O
the	O
syndrome	B
values	O
fzmg	O
.	O
each	O
de	O
(	O
cid:12	O
)	O
nition	O
zm	O
=pn	O
hmnnn	O
mod	O
2	O
is	O
enforced	O
by	O
a	O
factor	O
.	O
figure	O
47.2.	O
factor	O
graphs	O
associated	O
with	O
a	O
low-density	B
parity-check	I
code	I
.	O
the	O
codeword	B
decoding	O
viewpoint	O
first	O
,	O
we	O
note	O
that	O
the	O
prior	B
distribution	O
over	O
codewords	O
,	O
p	O
(	O
t	O
)	O
/	O
	O
[	O
ht	O
=	O
0	O
mod	O
2	O
]	O
;	O
(	O
47.2	O
)	O
can	O
be	O
represented	O
by	O
a	O
factor	B
graph	I
(	O
(	O
cid:12	O
)	O
gure	O
47.2a	O
)	O
,	O
with	O
the	O
factorization	O
being	O
tn	O
=	O
0	O
mod	O
2	O
]	O
:	O
(	O
47.3	O
)	O
p	O
(	O
t	O
)	O
/	O
ym	O
	O
[	O
xn2n	O
(	O
m	O
)	O
(	O
we	O
’	O
ll	O
omit	O
the	O
‘	O
mod	O
2	O
’	O
s	O
from	O
now	O
on	O
.	O
)	O
the	O
posterior	O
distribution	O
over	O
code-	O
words	O
is	O
given	O
by	O
multiplying	O
this	O
prior	B
by	O
the	O
likelihood	B
,	O
which	O
introduces	O
another	O
n	O
factors	O
,	O
one	O
for	O
each	O
received	O
bit	B
.	O
p	O
(	O
tj	O
r	O
)	O
/	O
p	O
(	O
t	O
)	O
p	O
(	O
rj	O
t	O
)	O
/	O
ym	O
	O
[	O
xn2n	O
(	O
m	O
)	O
tn	O
=	O
0	O
]	O
yn	O
p	O
(	O
rn	O
j	O
tn	O
)	O
(	O
47.4	O
)	O
the	O
factor	B
graph	I
corresponding	O
to	O
this	O
function	B
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
47.2b	O
.	O
it	O
is	O
the	O
same	O
as	O
the	O
graph	B
for	O
the	O
prior	B
,	O
except	O
for	O
the	O
addition	O
of	O
likelihood	O
‘	O
dongles	O
’	O
to	O
the	O
transmitted	O
bits	O
.	O
in	O
this	O
viewpoint	O
,	O
the	O
received	O
signal	O
rn	O
can	O
live	O
in	O
any	O
alphabet	O
;	O
all	O
that	O
matters	O
are	O
the	O
values	O
of	O
p	O
(	O
rn	O
j	O
tn	O
)	O
.	O
the	O
syndrome	B
decoding	I
viewpoint	O
alternatively	O
,	O
we	O
can	O
view	O
the	O
channel	B
output	O
in	O
terms	O
of	O
a	O
binary	O
received	O
vector	O
r	O
and	O
a	O
noise	B
vector	O
n	O
,	O
with	O
a	O
probability	B
distribution	O
p	O
(	O
n	O
)	O
that	O
can	O
be	O
derived	O
from	O
the	O
channel	B
properties	O
and	O
whatever	O
additional	O
information	B
is	O
available	O
at	O
the	O
channel	B
outputs	O
.	O
for	O
example	O
,	O
with	O
a	O
binary	B
symmetric	I
channel	I
,	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
noise	B
by	O
r	O
=	O
t	O
+	O
n	O
,	O
the	O
syndrome	B
z	O
=	O
hr	O
,	O
and	O
noise	O
model	B
p	O
(	O
nn	O
=	O
1	O
)	O
=	O
f	O
.	O
for	O
other	O
channels	O
such	O
as	O
the	O
gaussian	O
channel	O
with	O
output	O
y	O
,	O
we	O
may	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
received	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
47.3	O
:	O
decoding	B
with	O
the	O
sum	O
{	O
product	O
algorithm	O
559	O
binary	O
vector	O
r	O
however	O
we	O
wish	O
and	O
obtain	O
an	O
e	O
(	O
cid:11	O
)	O
ective	O
binary	O
noise	O
model	B
p	O
(	O
n	O
)	O
from	O
y	O
(	O
exercises	O
9.18	O
(	O
p.155	O
)	O
and	O
25.1	O
(	O
p.325	O
)	O
)	O
.	O
the	O
joint	B
probability	O
of	O
the	O
noise	O
n	O
and	O
syndrome	O
z	O
=	O
hn	O
can	O
be	O
factored	O
as	O
p	O
(	O
n	O
;	O
z	O
)	O
=	O
p	O
(	O
n	O
)	O
	O
[	O
z	O
=	O
hn	O
]	O
p	O
(	O
nn	O
)	O
ym	O
=	O
yn	O
	O
[	O
zm	O
=	O
xn2n	O
(	O
m	O
)	O
nn	O
]	O
:	O
(	O
47.5	O
)	O
the	O
factor	B
graph	I
of	O
this	O
function	B
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
47.2c	O
.	O
the	O
variables	O
n	O
and	O
z	O
can	O
also	O
be	O
drawn	O
in	O
a	O
‘	O
belief	B
network	O
’	O
(	O
also	O
known	O
as	O
a	O
‘	O
bayesian	O
network	B
’	O
,	O
‘	O
causal	O
network	B
’	O
,	O
or	O
‘	O
in	O
(	O
cid:13	O
)	O
uence	O
diagram	O
’	O
)	O
similar	O
to	O
(	O
cid:12	O
)	O
gure	O
47.2a	O
,	O
but	O
with	O
arrows	O
on	O
the	O
edges	O
from	O
the	O
upper	O
circular	O
nodes	O
(	O
which	O
represent	O
the	O
variables	O
n	O
)	O
to	O
the	O
lower	O
square	B
nodes	O
(	O
which	O
now	O
represent	O
the	O
variables	O
z	O
)	O
.	O
we	O
can	O
say	O
that	O
every	O
bit	B
xn	O
is	O
the	O
parent	B
of	O
j	O
checks	O
zm	O
,	O
and	O
each	O
check	O
zm	O
is	O
the	O
child	O
of	O
k	O
bits	O
.	O
both	O
decoding	B
viewpoints	O
involve	O
essentially	O
the	O
same	O
graph	B
.	O
either	O
ver-	O
sion	O
of	O
the	O
decoding	O
problem	O
can	O
be	O
expressed	O
as	O
the	O
generic	O
decoding	B
problem	O
‘	O
(	O
cid:12	O
)	O
nd	O
the	O
x	O
that	O
maximizes	O
p	O
(	O
cid:3	O
)	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
)	O
	O
[	O
hx	O
=	O
z	O
]	O
’	O
;	O
(	O
47.6	O
)	O
in	O
the	O
codeword	B
decoding	O
viewpoint	O
,	O
x	O
is	O
the	O
codeword	B
t	O
,	O
and	O
z	O
is	O
0	O
;	O
in	O
the	O
syndrome	B
decoding	I
viewpoint	O
,	O
x	O
is	O
the	O
noise	B
n	O
,	O
and	O
z	O
is	O
the	O
syndrome	B
.	O
it	O
doesn	O
’	O
t	O
matter	O
which	O
viewpoint	O
we	O
take	O
when	O
we	O
apply	O
the	O
sum	O
{	O
product	O
algorithm	O
.	O
the	O
two	O
decoding	B
algorithms	O
are	O
isomorphic	O
and	O
will	O
give	O
equiva-	O
lent	O
outcomes	O
(	O
unless	O
numerical	O
errors	B
intervene	O
)	O
.	O
i	O
tend	O
to	O
use	O
the	O
syndrome	B
decoding	I
viewpoint	O
because	O
it	O
has	O
one	O
advantage	O
:	O
one	O
does	O
not	O
need	O
to	O
implement	O
an	O
encoder	B
for	O
a	O
code	B
in	O
order	O
to	O
be	O
able	O
to	O
simulate	O
a	O
decoding	B
problem	O
realistically	O
.	O
we	O
’	O
ll	O
now	O
talk	O
in	O
terms	O
of	O
the	O
generic	O
decoding	B
problem	O
.	O
47.3	O
decoding	B
with	O
the	O
sum	O
{	O
product	O
algorithm	O
we	O
aim	O
,	O
given	O
the	O
observed	O
checks	O
,	O
to	O
compute	O
the	O
marginal	B
posterior	O
proba-	O
bilities	O
p	O
(	O
xn	O
=	O
1j	O
z	O
;	O
h	O
)	O
for	O
each	O
n.	O
it	O
is	O
hard	O
to	O
compute	O
these	O
exactly	O
because	O
the	O
graph	B
contains	O
many	O
cycles	O
.	O
however	O
,	O
it	O
is	O
interesting	O
to	O
implement	O
the	O
decoding	B
algorithm	O
that	O
would	O
be	O
appropriate	O
if	O
there	O
were	O
no	O
cycles	O
,	O
on	O
the	O
assumption	O
that	O
the	O
errors	B
introduced	O
might	O
be	O
relatively	O
small	O
.	O
this	O
ap-	O
proach	O
of	O
ignoring	O
cycles	O
has	O
been	O
used	O
in	O
the	O
arti	O
(	O
cid:12	O
)	O
cial	O
intelligence	O
literature	O
but	O
is	O
now	O
frowned	O
upon	O
because	O
it	O
produces	O
inaccurate	O
probabilities	O
.	O
how-	O
ever	O
,	O
if	O
we	O
are	O
decoding	B
a	O
good	B
error-correcting	O
code	B
,	O
we	O
don	O
’	O
t	O
care	O
about	O
accurate	O
marginal	B
probabilities	O
{	O
we	O
just	O
want	O
the	O
correct	O
codeword	B
.	O
also	O
,	O
the	O
posterior	B
probability	I
,	O
in	O
the	O
case	O
of	O
a	O
good	B
code	O
communicating	O
at	O
an	O
achievable	O
rate	B
,	O
is	O
expected	O
typically	O
to	O
be	O
hugely	O
concentrated	O
on	O
the	O
most	O
probable	O
decoding	O
;	O
so	O
we	O
are	O
dealing	O
with	O
a	O
distinctive	O
probability	B
distribution	O
to	O
which	O
experience	O
gained	O
in	O
other	O
(	O
cid:12	O
)	O
elds	O
may	O
not	O
apply	O
.	O
the	O
sum	O
{	O
product	O
algorithm	O
was	O
presented	O
in	O
chapter	O
26.	O
we	O
now	O
write	O
out	O
explicitly	O
how	O
it	O
works	O
for	O
solving	O
the	O
decoding	B
problem	O
hx	O
=	O
z	O
(	O
mod	O
2	O
)	O
:	O
for	O
brevity	O
,	O
we	O
reabsorb	O
the	O
dongles	O
hanging	O
o	O
(	O
cid:11	O
)	O
the	O
x	O
and	O
z	O
nodes	O
in	O
(	O
cid:12	O
)	O
g-	O
ure	O
47.2c	O
and	O
modify	O
the	O
sum	O
{	O
product	O
algorithm	O
accordingly	O
.	O
the	O
graph	B
in	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
560	O
47	O
|	O
low-density	B
parity-check	I
codes	O
which	O
x	O
and	O
z	O
live	O
is	O
then	O
the	O
original	O
graph	B
(	O
(	O
cid:12	O
)	O
gure	O
47.2a	O
)	O
whose	O
edges	O
are	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
the	O
1s	O
in	O
h.	O
the	O
graph	B
contains	O
nodes	O
of	O
two	O
types	O
,	O
which	O
we	O
’	O
ll	O
call	O
checks	O
and	O
bits	O
.	O
the	O
graph	B
connecting	O
the	O
checks	O
and	O
bits	O
is	O
a	O
bipartite	B
graph	I
:	O
bits	O
connect	O
only	O
to	O
checks	O
,	O
and	O
vice	O
versa	O
.	O
on	O
each	O
iteration	O
,	O
a	O
prob-	O
ability	O
ratio	O
is	O
propagated	O
along	O
each	O
edge	B
in	O
the	O
graph	B
,	O
and	O
each	O
bit	B
node	O
xn	O
updates	O
its	O
probability	B
that	O
it	O
should	O
be	O
in	O
state	O
1.	O
we	O
denote	O
the	O
set	B
of	O
bits	O
n	O
that	O
participate	O
in	O
check	O
m	O
by	O
n	O
(	O
m	O
)	O
(	O
cid:17	O
)	O
fn	O
:	O
hmn	O
=	O
1g	O
.	O
similarly	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
set	B
of	O
checks	O
in	O
which	O
bit	B
n	O
participates	O
,	O
m	O
(	O
n	O
)	O
(	O
cid:17	O
)	O
fm	O
:	O
hmn	O
=	O
1g	O
.	O
we	O
denote	O
a	O
set	B
n	O
(	O
m	O
)	O
with	O
bit	O
n	O
excluded	O
by	O
n	O
(	O
m	O
)	O
nn	O
.	O
the	O
algorithm	B
has	O
two	O
alternating	O
parts	O
,	O
in	O
which	O
quantities	O
qmn	O
and	O
rmn	O
associated	O
with	O
each	O
edge	B
in	O
the	O
graph	B
are	O
iteratively	O
updated	O
.	O
the	O
quantity	O
qx	O
mn	O
is	O
meant	O
to	O
be	O
the	O
probability	B
that	O
bit	B
n	O
of	O
x	O
has	O
the	O
value	O
x	O
,	O
given	O
the	O
information	B
obtained	O
via	O
checks	O
other	O
than	O
check	O
m.	O
the	O
quantity	O
rx	O
mn	O
is	O
meant	O
to	O
be	O
the	O
probability	O
of	O
check	O
m	O
being	O
satis	O
(	O
cid:12	O
)	O
ed	O
if	O
bit	B
n	O
of	O
x	O
is	O
considered	O
(	O
cid:12	O
)	O
xed	O
at	O
x	O
and	O
the	O
other	O
bits	O
have	O
a	O
separable	O
distribution	B
given	O
by	O
the	O
probabilities	O
fqmn0	O
:	O
n0	O
2	O
n	O
(	O
m	O
)	O
nng	O
.	O
the	O
algorithm	B
would	O
produce	O
the	O
exact	O
posterior	O
probabilities	O
of	O
all	O
the	O
bits	O
after	O
a	O
(	O
cid:12	O
)	O
xed	O
number	O
of	O
iterations	O
if	O
the	O
bipartite	B
graph	I
de	O
(	O
cid:12	O
)	O
ned	O
by	O
the	O
matrix	B
h	O
contained	O
no	O
cycles	O
.	O
n	O
=	O
p	O
(	O
xn	O
=	O
1	O
)	O
=	O
1	O
(	O
cid:0	O
)	O
p0	O
initialization	O
.	O
let	O
p0	O
n	O
=	O
p	O
(	O
xn	O
=	O
0	O
)	O
(	O
the	O
prior	B
probability	O
that	O
bit	B
xn	O
is	O
0	O
)	O
,	O
and	O
let	O
p1	O
n.	O
if	O
we	O
are	O
taking	O
the	O
syndrome	B
decoding	I
viewpoint	O
and	O
the	O
channel	B
is	O
a	O
binary	B
symmetric	I
channel	I
then	O
p1	O
n	O
will	O
equal	O
f	O
.	O
if	O
the	O
noise	B
level	O
varies	O
in	O
a	O
known	O
way	O
(	O
for	O
example	O
if	O
the	O
channel	B
is	O
a	O
binary-input	O
gaussian	O
channel	O
with	O
a	O
real	O
output	O
)	O
then	O
p1	O
n	O
is	O
initialized	O
to	O
the	O
appropriate	O
normalized	O
likelihood	B
.	O
for	O
every	O
(	O
n	O
;	O
m	O
)	O
such	O
that	O
hmn	O
=	O
1	O
the	O
variables	O
q0	O
mn	O
are	O
initialized	O
to	O
the	O
values	O
p0	O
n	O
respectively	O
.	O
mn	O
and	O
q1	O
n	O
and	O
p1	O
horizontal	O
step	O
.	O
in	O
the	O
horizontal	O
step	O
of	O
the	O
algorithm	O
(	O
horizontal	O
from	O
the	O
point	O
of	O
view	O
of	O
the	O
matrix	O
h	O
)	O
,	O
we	O
run	O
through	O
the	O
checks	O
m	O
and	O
compute	O
for	O
each	O
n	O
2	O
n	O
(	O
m	O
)	O
two	O
probabilities	O
:	O
(	O
cid:12	O
)	O
rst	O
,	O
r0	O
mn	O
,	O
the	O
probability	O
of	O
the	O
observed	O
value	O
of	O
zm	O
arising	O
when	O
xn	O
=	O
0	O
,	O
given	O
that	O
the	O
other	O
bits	O
fxn0	O
:	O
n0	O
6=	O
ng	O
have	O
a	O
separable	O
distribution	B
given	O
by	O
the	O
probabilities	O
fq	O
0	O
mn0	O
;	O
q1	O
r0	O
mn	O
=	O
xfxn0	O
:	O
n02n	O
(	O
m	O
)	O
nng	O
p	O
(	O
cid:0	O
)	O
zm	O
j	O
xn	O
=	O
0	O
;	O
(	O
cid:8	O
)	O
xn0	O
:	O
n0	O
2	O
n	O
(	O
m	O
)	O
nn	O
(	O
cid:9	O
)	O
(	O
cid:1	O
)	O
yn02n	O
(	O
m	O
)	O
nn	O
(	O
47.7	O
)	O
mn	O
,	O
the	O
probability	O
of	O
the	O
observed	O
value	O
of	O
zm	O
arising	O
when	O
mn0g	O
,	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
:	O
qxn0	O
mn0	O
and	O
second	O
,	O
r1	O
xn	O
=	O
1	O
,	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
:	O
r1	O
qxn0	O
mn0	O
:	O
mn	O
=	O
xfxn0	O
:	O
n02n	O
(	O
m	O
)	O
nng	O
p	O
(	O
cid:0	O
)	O
zm	O
j	O
xn	O
=	O
1	O
;	O
(	O
cid:8	O
)	O
xn0	O
:	O
n0	O
2	O
n	O
(	O
m	O
)	O
nn	O
(	O
cid:9	O
)	O
(	O
cid:1	O
)	O
yn02n	O
(	O
m	O
)	O
nn	O
(	O
47.8	O
)	O
the	O
conditional	B
probabilities	O
in	O
these	O
summations	O
are	O
either	O
zero	O
or	O
one	O
,	O
de-	O
pending	O
on	O
whether	O
the	O
observed	O
zm	O
matches	O
the	O
hypothesized	O
values	O
for	O
xn	O
and	O
the	O
fxn0g	O
.	O
these	O
probabilities	O
can	O
be	O
computed	O
in	O
various	O
obvious	O
ways	O
based	O
on	O
equation	O
(	O
47.7	O
)	O
and	O
(	O
47.8	O
)	O
.	O
the	O
computations	O
may	O
be	O
done	O
most	O
e	O
(	O
cid:14	O
)	O
ciently	O
(	O
if	O
jn	O
(	O
m	O
)	O
j	O
is	O
large	O
)	O
by	O
regarding	O
zm	O
+xn	O
as	O
the	O
(	O
cid:12	O
)	O
nal	O
state	O
of	O
a	O
markov	O
chain	B
with	O
states	O
0	O
and	O
1	O
,	O
this	O
chain	B
being	O
started	O
in	O
state	O
0	O
,	O
and	O
undergoing	O
transitions	O
corresponding	O
to	O
additions	O
of	O
the	O
various	O
xn0	O
,	O
with	O
transition	O
probabilities	O
given	O
by	O
the	O
corresponding	O
q0	O
mn0	O
.	O
the	O
probabilities	O
for	O
zm	O
having	O
its	O
observed	O
value	O
given	O
either	O
xn	O
=	O
0	O
or	O
xn	O
=	O
1	O
can	O
then	O
be	O
found	O
e	O
(	O
cid:14	O
)	O
ciently	O
by	O
use	O
of	O
the	O
forward	O
{	O
backward	O
algorithm	O
(	O
section	B
25.3	O
)	O
.	O
mn0	O
and	O
q1	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
47.3	O
:	O
decoding	B
with	O
the	O
sum	O
{	O
product	O
algorithm	O
561	O
a	O
particularly	O
convenient	O
implementation	O
of	O
this	O
method	B
uses	O
forward	O
and	O
mn	O
are	O
backward	O
passes	O
in	O
which	O
products	O
of	O
the	O
di	O
(	O
cid:11	O
)	O
erences	O
(	O
cid:14	O
)	O
qmn	O
(	O
cid:17	O
)	O
q0	O
computed	O
.	O
we	O
obtain	O
(	O
cid:14	O
)	O
rmn	O
(	O
cid:17	O
)	O
r0	O
mn	O
from	O
the	O
identity	O
:	O
mn	O
(	O
cid:0	O
)	O
q1	O
mn	O
(	O
cid:0	O
)	O
r1	O
(	O
cid:14	O
)	O
rmn	O
=	O
(	O
(	O
cid:0	O
)	O
1	O
)	O
zm	O
yn02n	O
(	O
m	O
)	O
nn	O
(	O
cid:14	O
)	O
qmn0	O
:	O
(	O
47.9	O
)	O
this	O
identity	O
is	O
derived	O
by	O
iterating	O
the	O
following	O
observation	O
:	O
x	O
(	O
cid:23	O
)	O
mod	O
2	O
,	O
and	O
x	O
(	O
cid:22	O
)	O
and	O
x	O
(	O
cid:23	O
)	O
have	O
probabilities	O
q0	O
then	O
p	O
(	O
(	O
cid:16	O
)	O
=	O
1	O
)	O
=	O
q1	O
(	O
cid:22	O
)	O
(	O
cid:0	O
)	O
q1	O
p	O
(	O
(	O
cid:16	O
)	O
=	O
1	O
)	O
=	O
(	O
q0	O
we	O
recover	O
r0	O
(	O
cid:22	O
)	O
;	O
q0	O
(	O
cid:23	O
)	O
and	O
p	O
(	O
(	O
cid:16	O
)	O
=	O
0	O
)	O
=	O
q0	O
(	O
cid:22	O
)	O
q1	O
(	O
cid:23	O
)	O
+	O
q0	O
(	O
cid:23	O
)	O
(	O
cid:0	O
)	O
q1	O
(	O
cid:23	O
)	O
)	O
.	O
mn	O
and	O
r1	O
(	O
cid:23	O
)	O
and	O
q1	O
(	O
cid:23	O
)	O
+	O
q1	O
(	O
cid:22	O
)	O
q0	O
(	O
cid:22	O
)	O
q0	O
(	O
cid:22	O
)	O
)	O
(	O
q0	O
mn	O
using	O
if	O
(	O
cid:16	O
)	O
=	O
x	O
(	O
cid:22	O
)	O
+	O
(	O
cid:22	O
)	O
;	O
q1	O
(	O
cid:23	O
)	O
of	O
being	O
0	O
and	O
1	O
,	O
(	O
cid:22	O
)	O
q1	O
(	O
cid:23	O
)	O
.	O
thus	O
p	O
(	O
(	O
cid:16	O
)	O
=	O
0	O
)	O
(	O
cid:0	O
)	O
r0	O
mn	O
=	O
1/2	O
(	O
1	O
+	O
(	O
cid:14	O
)	O
rmn	O
)	O
;	O
r1	O
mn	O
=	O
1/2	O
(	O
1	O
(	O
cid:0	O
)	O
(	O
cid:14	O
)	O
rmn	O
)	O
:	O
(	O
47.10	O
)	O
the	O
transformations	O
into	O
di	O
(	O
cid:11	O
)	O
erences	O
(	O
cid:14	O
)	O
q	O
and	O
back	O
from	O
(	O
cid:14	O
)	O
r	O
to	O
frg	O
may	O
be	O
viewed	O
as	O
a	O
fourier	O
transform	O
and	O
an	O
inverse	O
fourier	O
transformation	O
.	O
vertical	O
step	O
.	O
the	O
vertical	O
step	O
takes	O
the	O
computed	O
values	O
of	O
r	O
0	O
and	O
updates	O
the	O
values	O
of	O
the	O
probabilities	O
q0	O
compute	O
:	O
mn	O
and	O
q1	O
mn	O
mn	O
.	O
for	O
each	O
n	O
we	O
mn	O
and	O
r1	O
q0	O
mn	O
=	O
(	O
cid:11	O
)	O
mn	O
p0	O
q1	O
mn	O
=	O
(	O
cid:11	O
)	O
mn	O
p1	O
n	O
ym02m	O
(	O
n	O
)	O
nm	O
n	O
ym02m	O
(	O
n	O
)	O
nm	O
r0	O
m0n	O
r1	O
m0n	O
(	O
47.11	O
)	O
(	O
47.12	O
)	O
where	O
(	O
cid:11	O
)	O
mn	O
is	O
chosen	O
such	O
that	O
q0	O
computed	O
in	O
a	O
downward	O
pass	O
and	O
an	O
upward	O
pass	O
.	O
mn+q1	O
mn	O
=	O
1.	O
these	O
products	O
can	O
be	O
e	O
(	O
cid:14	O
)	O
ciently	O
we	O
can	O
also	O
compute	O
the	O
‘	O
pseudoposterior	O
probabilities	O
’	O
q	O
0	O
n	O
and	O
q1	O
n	O
at	O
this	O
iteration	O
,	O
given	O
by	O
:	O
q0	O
n	O
=	O
(	O
cid:11	O
)	O
n	O
p0	O
n	O
=	O
(	O
cid:11	O
)	O
n	O
p1	O
q1	O
n	O
ym2m	O
(	O
n	O
)	O
n	O
ym2m	O
(	O
n	O
)	O
r0	O
mn	O
;	O
r1	O
mn	O
:	O
(	O
47.13	O
)	O
(	O
47.14	O
)	O
these	O
quantities	O
are	O
used	O
to	O
create	O
a	O
tentative	O
decoding	B
^x	O
,	O
the	O
consistency	O
of	O
which	O
is	O
used	O
to	O
decide	O
whether	O
the	O
decoding	B
algorithm	O
can	O
halt	O
.	O
(	O
halt	O
if	O
h^x	O
=	O
z	O
.	O
)	O
at	O
this	O
point	O
,	O
the	O
algorithm	B
repeats	O
from	O
the	O
horizontal	O
step	O
.	O
the	O
stop-when-it	O
’	O
s-done	O
decoding	B
method	O
.	O
the	O
recommended	O
decod-	O
ing	O
procedure	O
is	O
to	O
set	B
^xn	O
to	O
1	O
if	O
q1	O
n	O
>	O
0:5	O
and	O
see	O
if	O
the	O
checks	O
h^x	O
=	O
z	O
mod	O
2	O
are	O
all	O
satis	O
(	O
cid:12	O
)	O
ed	O
,	O
halting	O
when	O
they	O
are	O
,	O
and	O
declaring	O
a	O
failure	O
if	O
some	O
maximum	O
number	O
of	O
iterations	O
(	O
e.g	O
.	O
200	O
or	O
1000	O
)	O
occurs	O
without	O
successful	O
decoding	B
.	O
in	O
the	O
event	O
of	O
a	O
failure	O
,	O
we	O
may	O
still	O
report	O
^x	O
,	O
but	O
we	O
(	O
cid:13	O
)	O
ag	O
the	O
whole	O
block	B
as	O
a	O
failure	O
.	O
we	O
note	O
in	O
passing	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
this	O
decoding	B
procedure	O
and	O
the	O
widespread	O
practice	O
in	O
the	O
turbo	B
code	I
community	O
,	O
where	O
the	O
decoding	B
algorithm	O
is	O
run	O
for	O
a	O
(	O
cid:12	O
)	O
xed	O
number	O
of	O
iterations	O
(	O
irrespective	O
of	O
whether	O
the	O
decoder	B
(	O
cid:12	O
)	O
nds	O
a	O
consistent	O
state	O
at	O
some	O
earlier	O
time	O
)	O
.	O
this	O
practice	O
is	O
wasteful	O
of	O
computer	O
time	O
,	O
and	O
it	O
blurs	O
the	O
distinction	O
between	O
undetected	O
and	O
detected	O
errors	B
.	O
in	O
our	O
procedure	O
,	O
‘	O
undetected	O
’	O
errors	B
occur	O
if	O
the	O
decoder	B
(	O
cid:12	O
)	O
nds	O
an	O
^x	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
562	O
47	O
|	O
low-density	B
parity-check	I
codes	O
(	O
a	O
)	O
!	O
parity	B
bits	O
8	O
>	O
>	O
>	O
>	O
>	O
>	O
>	O
>	O
<	O
>	O
>	O
>	O
>	O
>	O
>	O
>	O
>	O
:	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
47.3.	O
demonstration	O
of	O
encoding	O
with	O
a	O
rate-1=2	O
gallager	O
code	B
.	O
the	O
encoder	B
is	O
derived	O
from	O
a	O
very	O
sparse	O
10	O
000	O
(	O
cid:2	O
)	O
20	O
000	O
parity-check	B
matrix	I
with	O
three	O
1s	O
per	O
column	O
(	O
(	O
cid:12	O
)	O
gure	O
47.4	O
)	O
.	O
(	O
a	O
)	O
the	O
code	B
creates	O
transmitted	O
vectors	B
consisting	O
of	O
10	O
000	O
source	O
bits	O
and	O
10	O
000	O
parity-	O
check	O
bits	O
.	O
(	O
b	O
)	O
here	O
,	O
the	O
source	O
sequence	O
has	O
been	O
altered	O
by	O
changing	O
the	O
(	O
cid:12	O
)	O
rst	O
bit	B
.	O
notice	O
that	O
many	O
of	O
the	O
parity-check	O
bits	O
are	O
changed	O
.	O
each	O
parity	B
bit	O
depends	O
on	O
about	O
half	O
of	O
the	O
source	O
bits	O
.	O
(	O
c	O
)	O
the	O
transmission	O
for	O
the	O
case	O
s	O
=	O
(	O
1	O
;	O
0	O
;	O
0	O
;	O
:	O
:	O
:	O
;	O
0	O
)	O
.	O
this	O
vector	O
is	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
(	O
modulo	O
2	O
)	O
between	O
transmissions	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
.	O
[	O
dilbert	O
image	B
copyright	O
c	O
(	O
cid:13	O
)	O
1997	O
united	O
feature	O
syndicate	O
,	O
inc.	O
,	O
used	O
with	O
permission	O
.	O
]	O
satisfying	O
h^x	O
=	O
z	O
mod	O
2	O
that	O
is	O
not	O
equal	O
to	O
the	O
true	O
x	O
.	O
‘	O
detected	O
’	O
errors	B
occur	O
if	O
the	O
algorithm	B
runs	O
for	O
the	O
maximum	O
number	O
of	O
iterations	O
without	O
(	O
cid:12	O
)	O
nding	O
a	O
valid	O
decoding	B
.	O
undetected	O
errors	B
are	O
of	O
scienti	O
(	O
cid:12	O
)	O
c	O
interest	O
because	O
they	O
reveal	O
distance	B
properties	O
of	O
a	O
code	B
.	O
and	O
in	O
engineering	O
practice	O
,	O
it	O
would	O
seem	O
preferable	O
for	O
the	O
blocks	O
that	O
are	O
known	O
to	O
contain	O
detected	O
errors	B
to	O
be	O
so	O
labelled	O
if	O
practically	O
possible	O
.	O
cost	O
.	O
in	O
a	O
brute-force	O
approach	O
,	O
the	O
time	O
to	O
create	O
the	O
generator	B
matrix	I
scales	O
as	O
n	O
3	O
,	O
where	O
n	O
is	O
the	O
block	B
size	O
.	O
the	O
encoding	O
time	O
scales	O
as	O
n	O
2	O
,	O
but	O
encoding	O
involves	O
only	O
binary	O
arithmetic	O
,	O
so	O
for	O
the	O
block	B
lengths	O
studied	O
here	O
it	O
takes	O
considerably	O
less	O
time	O
than	O
the	O
simulation	O
of	O
the	O
gaussian	O
channel	B
.	O
decoding	B
involves	O
approximately	O
6n	O
j	O
(	O
cid:13	O
)	O
oating-point	O
multiplies	O
per	O
iteration	O
,	O
so	O
the	O
total	O
number	O
of	O
operations	O
per	O
decoded	O
bit	B
(	O
assuming	O
20	O
iterations	O
)	O
is	O
about	O
120t=r	O
,	O
independent	O
of	O
blocklength	O
.	O
for	O
the	O
codes	O
presented	O
in	O
the	O
next	O
section	B
,	O
this	O
is	O
about	O
800	O
operations	O
.	O
the	O
encoding	O
complexity	B
can	O
be	O
reduced	O
by	O
clever	O
encoding	O
tricks	O
invented	O
by	O
richardson	O
and	O
urbanke	O
(	O
2001b	O
)	O
or	O
by	O
specially	O
constructing	O
the	O
parity-	O
check	O
matrix	B
(	O
mackay	O
et	O
al.	O
,	O
1999	O
)	O
.	O
the	O
decoding	B
complexity	O
can	O
be	O
reduced	O
,	O
with	O
only	O
a	O
small	O
loss	O
in	O
perfor-	O
mance	O
,	O
by	O
passing	O
low-precision	O
messages	O
in	O
place	O
of	O
real	O
numbers	O
(	O
richardson	O
and	O
urbanke	O
,	O
2001a	O
)	O
.	O
47.4	O
pictorial	O
demonstration	O
of	O
gallager	O
codes	O
figures	O
47.3	O
{	O
47.7	O
illustrate	O
visually	O
the	O
conditions	O
under	O
which	O
low-density	B
parity-check	I
codes	O
can	O
give	O
reliable	O
communication	B
over	O
binary	B
symmetric	I
channels	O
and	O
gaussian	O
channels	O
.	O
these	O
demonstrations	O
may	O
be	O
viewed	O
as	O
animations	O
on	O
the	O
world	O
wide	O
web.1	O
1http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/codes/gifs/	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
47.4	O
:	O
pictorial	O
demonstration	O
of	O
gallager	O
codes	O
563	O
h	O
=	O
figure	O
47.4.	O
a	O
low-density	B
parity-check	I
matrix	O
with	O
n	O
=	O
20	O
000	O
columns	O
of	O
weight	O
j	O
=	O
3	O
and	O
m	O
=	O
10	O
000	O
rows	O
of	O
weight	O
k	O
=	O
6.	O
encoding	O
figure	O
47.3	O
illustrates	O
the	O
encoding	O
operation	O
for	O
the	O
case	O
of	O
a	O
gallager	O
code	B
whose	O
parity-check	B
matrix	I
is	O
a	O
10	O
000	O
(	O
cid:2	O
)	O
20	O
000	O
matrix	B
with	O
three	O
1s	O
per	O
col-	O
umn	O
(	O
(	O
cid:12	O
)	O
gure	O
47.4	O
)	O
.	O
the	O
high	O
density	O
of	O
the	O
generator	O
matrix	B
is	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gure	O
47.3b	O
and	O
c	O
by	O
showing	O
the	O
change	O
in	O
the	O
transmitted	O
vector	O
when	O
one	O
of	O
the	O
10	O
000	O
source	O
bits	O
is	O
altered	O
.	O
of	O
course	O
,	O
the	O
source	O
images	O
shown	O
here	O
are	O
highly	O
redundant	O
,	O
and	O
such	O
images	B
should	O
really	O
be	O
compressed	O
before	O
encoding	O
.	O
redundant	O
images	O
are	O
chosen	O
in	O
these	O
demonstrations	O
to	O
make	O
it	O
easier	O
to	O
see	O
the	O
correction	O
process	O
during	O
the	O
iterative	O
decoding	O
.	O
the	O
decod-	O
ing	O
algorithm	B
does	O
not	O
take	O
advantage	O
of	O
the	O
redundancy	O
of	O
the	O
source	O
vector	O
,	O
and	O
it	O
would	O
work	O
in	O
exactly	O
the	O
same	O
way	O
irrespective	O
of	O
the	O
choice	O
of	O
source	O
vector	O
.	O
iterative	O
decoding	O
the	O
transmission	O
is	O
sent	O
over	O
a	O
channel	O
with	O
noise	O
level	O
f	O
=	O
7:5	O
%	O
and	O
the	O
received	O
vector	O
is	O
shown	O
in	O
the	O
upper	O
left	O
of	O
(	O
cid:12	O
)	O
gure	O
47.5.	O
the	O
subsequent	O
pictures	O
in	O
(	O
cid:12	O
)	O
gure	O
47.5	O
show	O
the	O
iterative	B
probabilistic	I
decoding	I
process	O
.	O
the	O
sequence	B
of	O
(	O
cid:12	O
)	O
gures	O
shows	O
the	O
best	O
guess	O
,	O
bit	B
by	O
bit	B
,	O
given	O
by	O
the	O
iterative	O
decoder	O
,	O
after	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
,	O
10	O
,	O
11	O
,	O
12	O
,	O
and	O
13	O
iterations	O
.	O
the	O
decoder	B
halts	O
after	O
the	O
13th	O
iteration	O
when	O
the	O
best	O
guess	O
violates	O
no	O
parity	B
checks	O
.	O
this	O
(	O
cid:12	O
)	O
nal	O
decoding	B
is	O
error	O
free	O
.	O
in	O
the	O
case	O
of	O
an	O
unusually	O
noisy	B
transmission	O
,	O
the	O
decoding	B
algorithm	O
fails	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
valid	O
decoding	B
.	O
for	O
this	O
code	B
and	O
a	O
channel	O
with	O
f	O
=	O
7:5	O
%	O
,	O
such	O
failures	O
happen	O
about	O
once	O
in	O
every	O
100	O
000	O
transmissions	O
.	O
figure	O
47.6	O
shows	O
this	O
error	O
rate	O
compared	O
with	O
the	O
block	B
error	O
rates	O
of	O
classical	O
error-correcting	B
codes	I
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
564	O
47	O
|	O
low-density	B
parity-check	I
codes	O
received	O
:	O
0	O
1	O
2	O
3	O
10	O
11	O
12	O
13	O
!	O
decoded	O
:	O
figure	O
47.5.	O
iterative	B
probabilistic	I
decoding	I
of	O
a	O
low-density	B
parity-check	I
code	I
for	O
a	O
transmission	O
received	O
over	O
a	O
channel	O
with	O
noise	O
level	O
f	O
=	O
7:5	O
%	O
.	O
the	O
sequence	B
of	O
(	O
cid:12	O
)	O
gures	O
shows	O
the	O
best	O
guess	O
,	O
bit	B
by	O
bit	B
,	O
given	O
by	O
the	O
iterative	O
decoder	O
,	O
after	O
0	O
,	O
1	O
,	O
2	O
,	O
3	O
,	O
10	O
,	O
11	O
,	O
12	O
,	O
and	O
13	O
iterations	O
.	O
the	O
decoder	B
halts	O
after	O
the	O
13th	O
iteration	O
when	O
the	O
best	O
guess	O
violates	O
no	O
parity	B
checks	O
.	O
this	O
(	O
cid:12	O
)	O
nal	O
decoding	B
is	O
error	O
free	O
.	O
0.1	O
0.01	O
0.001	O
0.0001	O
1e-05	O
1e-06	O
r	O
o	O
r	O
r	O
e	O
r	O
e	O
d	O
o	O
c	O
e	O
d	O
f	O
o	O
y	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
low-density	B
parity-check	I
code	I
shannon	O
limit	O
gv	O
0	O
0.2	O
0.4	O
c	O
0.6	O
rate	B
0.8	O
1	O
figure	O
47.6.	O
error	B
probability	I
of	O
the	O
low-density	B
parity-check	I
code	I
(	O
with	O
error	O
bars	O
)	O
for	O
binary	O
symmetric	B
channel	I
with	O
f	O
=	O
7:5	O
%	O
,	O
compared	O
with	O
algebraic	O
codes	O
.	O
squares	O
:	O
repetition	B
codes	O
and	O
hamming	O
(	O
7	O
;	O
4	O
)	O
code	B
;	O
other	O
points	O
:	O
reed	O
{	O
muller	O
and	O
bch	O
codes	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
47.4	O
:	O
pictorial	O
demonstration	O
of	O
gallager	O
codes	O
565	O
figure	O
47.7.	O
demonstration	O
of	O
a	O
gallager	O
code	B
for	O
a	O
gaussian	O
channel	B
.	O
(	O
a1	O
)	O
the	O
received	O
vector	O
after	O
transmission	O
over	O
a	O
gaussian	O
channel	O
with	O
x=	O
(	O
cid:27	O
)	O
=	O
1:185	O
(	O
eb=n0	O
=	O
1:47	O
db	O
)	O
.	O
the	O
greyscale	O
represents	O
the	O
value	O
of	O
the	O
normalized	O
likelihood	B
.	O
this	O
transmission	O
can	O
be	O
perfectly	O
decoded	O
by	O
the	O
sum	O
{	O
product	O
decoder	O
.	O
the	O
empirical	O
probability	O
of	O
decoding	O
failure	O
is	O
about	O
10	O
(	O
cid:0	O
)	O
5	O
.	O
(	O
a2	O
)	O
the	O
probability	B
distribution	O
of	O
the	O
output	O
y	O
of	O
the	O
channel	O
with	O
x=	O
(	O
cid:27	O
)	O
=	O
1:185	O
for	O
each	O
of	O
the	O
two	O
possible	O
inputs	O
.	O
(	O
b1	O
)	O
the	O
received	O
transmission	O
over	O
a	O
gaussian	O
channel	O
with	O
x=	O
(	O
cid:27	O
)	O
=	O
1:0	O
,	O
which	O
corresponds	O
to	O
the	O
shannon	O
limit	O
.	O
(	O
b2	O
)	O
the	O
probability	B
distribution	O
of	O
the	O
output	O
y	O
of	O
the	O
channel	O
with	O
x=	O
(	O
cid:27	O
)	O
=	O
1:0	O
for	O
each	O
of	O
the	O
two	O
possible	O
inputs	O
.	O
figure	O
47.8.	O
performance	O
of	O
rate-1/2	O
gallager	O
codes	O
on	O
the	O
gaussian	O
channel	B
.	O
vertical	O
axis	O
:	O
block	B
error	O
probability	B
.	O
horizontal	O
axis	O
:	O
signal-to-noise	B
ratio	I
eb=n0	O
.	O
(	O
a	O
)	O
dependence	O
on	O
blocklength	O
n	O
for	O
(	O
j	O
;	O
k	O
)	O
=	O
(	O
3	O
;	O
6	O
)	O
codes	O
.	O
from	O
left	O
to	O
right	O
:	O
n	O
=	O
816	O
,	O
n	O
=	O
408	O
,	O
n	O
=	O
204	O
,	O
n	O
=	O
96.	O
the	O
dashed	O
lines	O
show	O
the	O
frequency	B
of	O
undetected	O
errors	B
,	O
which	O
is	O
measurable	O
only	O
when	O
the	O
blocklength	O
is	O
as	O
small	O
as	O
n	O
=	O
96	O
or	O
n	O
=	O
204	O
.	O
(	O
b	O
)	O
dependence	O
on	O
column	O
weight	B
j	O
for	O
codes	O
of	O
blocklength	O
n	O
=	O
816	O
.	O
(	O
a1	O
)	O
(	O
b1	O
)	O
0.4	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
(	O
a2	O
)	O
p	O
(	O
y|	O
‘	O
0	O
’	O
)	O
p	O
(	O
y|	O
‘	O
1	O
’	O
)	O
-4	O
-2	O
0	O
2	O
4	O
1	O
0.1	O
0.01	O
0.001	O
0.0001	O
1e-05	O
1e-06	O
n=816	O
n=408	O
(	O
n=204	O
)	O
n=96	O
(	O
n=96	O
)	O
n=204	O
1	O
1.5	O
2	O
2.5	O
3	O
3.5	O
4	O
4.5	O
5	O
5.5	O
(	O
a	O
)	O
gaussian	O
channel	B
(	O
b2	O
)	O
1	O
0.1	O
0.01	O
0.001	O
0.0001	O
1e-05	O
0.4	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
p	O
(	O
y|	O
‘	O
0	O
’	O
)	O
p	O
(	O
y|	O
‘	O
1	O
’	O
)	O
-4	O
-2	O
0	O
2	O
4	O
j=4	O
j=3	O
3	O
j=5	O
3.5	O
j=6	O
4	O
1	O
1.5	O
2	O
2.5	O
(	O
b	O
)	O
in	O
(	O
cid:12	O
)	O
gure	O
47.7	O
the	O
left	O
picture	O
shows	O
the	O
received	O
vector	O
after	O
transmission	O
over	O
a	O
gaussian	O
channel	O
with	O
x=	O
(	O
cid:27	O
)	O
=	O
1:185.	O
the	O
greyscale	O
represents	O
the	O
value	O
.	O
this	O
signal-to-noise	B
ratio	I
of	O
the	O
normalized	O
likelihood	B
,	O
x=	O
(	O
cid:27	O
)	O
=	O
1:185	O
is	O
a	O
noise	B
level	O
at	O
which	O
this	O
rate-1/2	O
gallager	O
code	B
communicates	O
reliably	O
(	O
the	O
probability	B
of	I
error	I
is	O
’	O
10	O
(	O
cid:0	O
)	O
5	O
)	O
.	O
to	O
show	O
how	O
close	O
we	O
are	O
to	O
the	O
shannon	O
limit	O
,	O
the	O
right	O
panel	O
shows	O
the	O
received	O
vector	O
when	O
the	O
signal-to-	O
noise	B
ratio	O
is	O
reduced	O
to	O
x=	O
(	O
cid:27	O
)	O
=	O
1:0	O
,	O
which	O
corresponds	O
to	O
the	O
shannon	O
limit	O
for	O
codes	O
of	O
rate	O
1/2	O
.	O
p	O
(	O
y	O
j	O
t	O
=	O
1	O
)	O
+p	O
(	O
y	O
j	O
t	O
=	O
0	O
)	O
p	O
(	O
y	O
j	O
t	O
=	O
1	O
)	O
variation	O
of	O
performance	O
with	O
code	O
parameters	B
figure	O
47.8	O
shows	O
how	O
the	O
parameters	B
n	O
and	O
j	O
a	O
(	O
cid:11	O
)	O
ect	O
the	O
performance	O
of	O
low-density	O
parity-check	O
codes	O
.	O
as	O
shannon	O
would	O
predict	O
,	O
increasing	O
the	O
blocklength	O
leads	O
to	O
improved	O
performance	O
.	O
the	O
dependence	O
on	O
j	O
follows	O
a	O
di	O
(	O
cid:11	O
)	O
erent	O
pattern	O
.	O
given	O
an	O
optimal	B
decoder	I
,	O
the	O
best	O
performance	O
would	O
be	O
obtained	O
for	O
the	O
codes	O
closest	O
to	O
random	B
codes	O
,	O
that	O
is	O
,	O
the	O
codes	O
with	O
largest	O
j.	O
however	O
,	O
the	O
sum	O
{	O
product	O
decoder	O
makes	O
poor	O
progress	O
in	O
dense	O
graphs	O
,	O
so	O
the	O
best	O
performance	O
is	O
obtained	O
for	O
a	O
small	O
value	O
of	O
j.	O
among	O
the	O
values	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
566	O
47	O
|	O
low-density	B
parity-check	I
codes	O
3	O
3	O
3	O
(	O
a	O
)	O
(	O
b	O
)	O
0.45	O
0.4	O
0.35	O
0.3	O
0.25	O
0.2	O
0.15	O
0.1	O
0.05	O
0	O
0	O
f	O
=	O
0:080	O
f	O
=	O
0:075	O
5	O
10	O
15	O
20	O
25	O
30	O
figure	O
47.9.	O
schematic	O
illustration	O
of	O
constructions	O
(	O
a	O
)	O
of	O
a	O
completely	O
regular	B
gallager	O
code	B
with	O
j	O
=	O
3	O
,	O
k	O
=	O
6	O
and	O
r	O
=	O
1=2	O
;	O
(	O
b	O
)	O
of	O
a	O
nearly-regular	O
gallager	O
code	B
with	O
rate	B
1=3	O
.	O
notation	B
:	O
an	O
integer	O
represents	O
a	O
number	O
of	O
permutation	O
matrices	B
superposed	O
on	O
the	O
surrounding	O
square	B
.	O
a	O
diagonal	O
line	O
represents	O
an	O
identity	B
matrix	I
.	O
figure	O
47.10.	O
monte	O
carlo	O
simulation	O
of	B
density	I
evolution	I
,	O
following	O
the	O
decoding	B
process	O
for	O
j	O
=	O
4	O
;	O
k	O
=	O
8.	O
each	O
curve	O
shows	O
the	O
average	B
entropy	O
of	O
a	O
bit	B
as	O
a	O
function	B
of	O
number	O
of	O
iterations	O
,	O
as	O
estimated	O
by	O
a	O
monte	O
carlo	O
algorithm	B
using	O
10	O
000	O
samples	O
per	O
iteration	O
.	O
the	O
noise	B
level	O
of	O
the	O
binary	O
symmetric	B
channel	I
f	O
increases	O
by	O
steps	O
of	O
0:005	O
from	O
bottom	O
graph	B
(	O
f	O
=	O
0:010	O
)	O
to	O
top	O
graph	B
(	O
f	O
=	O
0:100	O
)	O
.	O
there	O
is	O
evidently	O
a	O
threshold	B
at	O
about	O
f	O
=	O
0:075	O
,	O
above	O
which	O
the	O
algorithm	B
can	O
not	O
determine	O
x.	O
from	O
mackay	O
(	O
1999b	O
)	O
.	O
of	O
j	O
shown	O
in	O
the	O
(	O
cid:12	O
)	O
gure	O
,	O
j	O
=	O
3	O
is	O
the	O
best	O
,	O
for	O
a	O
blocklength	O
of	O
816	O
,	O
down	O
to	O
a	O
block	B
error	O
probability	O
of	O
10	O
(	O
cid:0	O
)	O
5.	O
this	O
observation	O
motivates	O
construction	B
of	O
gallager	O
codes	O
with	O
some	O
col-	O
umns	O
of	O
weight	O
2.	O
a	O
construction	B
with	O
m=2	O
columns	O
of	O
weight	O
2	O
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
47.9b	O
.	O
too	O
many	O
columns	O
of	O
weight	O
2	O
,	O
and	O
the	O
code	B
becomes	O
a	O
much	O
poorer	O
code	B
.	O
as	O
we	O
’	O
ll	O
discuss	O
later	O
,	O
we	O
can	O
do	O
even	O
better	O
by	O
making	O
the	O
code	B
even	O
more	O
irregular	B
.	O
47.5	O
density	B
evolution	I
one	O
way	O
to	O
study	O
the	O
decoding	B
algorithm	O
is	O
to	O
imagine	O
it	O
running	O
on	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
tree-like	O
graph	B
with	O
the	O
same	O
local	O
topology	O
as	O
the	O
gallager	O
code	B
’	O
s	O
graph	B
.	O
the	O
larger	O
the	O
matrix	B
h	O
,	O
the	O
closer	O
its	O
decoding	B
properties	O
should	O
approach	O
those	O
of	O
the	O
in	O
(	O
cid:12	O
)	O
nite	O
graph	B
.	O
imagine	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
belief	B
network	O
with	O
no	O
loops	O
,	O
in	O
which	O
every	O
bit	B
xn	O
connects	O
to	O
j	O
checks	O
and	O
every	O
check	O
zm	O
connects	O
to	O
k	O
bits	O
(	O
(	O
cid:12	O
)	O
gure	O
47.11	O
)	O
.	O
we	O
consider	O
the	O
iterative	O
(	O
cid:13	O
)	O
ow	O
of	O
information	O
in	O
this	O
network	B
,	O
and	O
examine	O
the	O
average	B
entropy	O
of	O
one	O
bit	B
as	O
a	O
function	B
of	O
number	O
of	O
iterations	O
.	O
at	O
each	O
iteration	O
,	O
a	O
bit	B
has	O
accumulated	O
information	B
from	O
its	O
local	O
network	B
out	O
to	O
a	O
radius	O
equal	O
to	O
the	O
number	O
of	O
iterations	O
.	O
successful	O
decoding	B
will	O
occur	O
only	O
if	O
the	O
average	B
entropy	O
of	O
a	O
bit	B
decreases	O
to	O
zero	O
as	O
the	O
number	O
of	O
iterations	O
increases	O
.	O
the	O
iterations	O
of	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
belief	B
network	O
can	O
be	O
simulated	O
by	O
monte	O
carlo	O
methods	B
{	O
a	O
technique	O
(	O
cid:12	O
)	O
rst	O
used	O
by	O
gallager	O
(	O
1963	O
)	O
.	O
imagine	O
a	O
network	B
of	O
radius	O
i	O
(	O
the	O
total	O
number	O
of	O
iterations	O
)	O
centred	O
on	O
one	O
bit	B
.	O
our	O
aim	O
is	O
to	O
compute	O
the	O
conditional	B
entropy	I
of	O
the	O
central	O
bit	B
x	O
given	O
the	O
state	O
z	O
of	O
all	O
checks	O
out	O
to	O
radius	O
i.	O
to	O
evaluate	O
the	O
probability	B
that	O
the	O
central	O
bit	B
is	O
1	O
given	O
a	O
particular	O
syndrome	B
z	O
involves	O
an	O
i-step	O
propagation	O
from	O
the	O
outside	O
of	O
the	O
network	O
into	O
the	O
centre	O
.	O
at	O
the	O
ith	O
iteration	O
,	O
probabilities	O
r	O
at	O
figure	O
47.11.	O
local	O
topology	O
of	O
the	O
graph	O
of	O
a	O
gallager	O
code	B
with	O
column	O
weight	B
j	O
=	O
3	O
and	O
row	O
weight	B
k	O
=	O
4.	O
white	B
nodes	O
represent	O
bits	O
,	O
xl	O
;	O
black	B
nodes	O
represent	O
checks	O
,	O
zm	O
;	O
each	O
edge	B
corresponds	O
to	O
a	O
1	O
in	O
h.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
47.6	O
:	O
improving	O
gallager	O
codes	O
radius	O
i	O
(	O
cid:0	O
)	O
i	O
+	O
1	O
are	O
transformed	O
into	O
qs	O
and	O
then	O
into	O
rs	O
at	O
radius	O
i	O
(	O
cid:0	O
)	O
i	O
in	O
a	O
way	O
that	O
depends	O
on	O
the	O
states	O
x	O
of	O
the	O
unknown	O
bits	O
at	O
radius	O
i	O
(	O
cid:0	O
)	O
i.	O
in	O
the	O
monte	O
carlo	O
method	B
,	O
rather	O
than	O
simulating	O
this	O
network	B
exactly	O
,	O
which	O
would	O
take	O
a	O
time	O
that	O
grows	O
exponentially	O
with	O
i	O
,	O
we	O
create	O
for	O
each	O
iteration	O
a	O
representative	O
sample	B
(	O
of	O
size	O
100	O
,	O
say	O
)	O
of	O
the	O
values	O
of	O
fr	O
;	O
xg	O
.	O
in	O
the	O
case	O
of	O
a	O
regular	B
network	O
with	O
parameters	O
j	O
;	O
k	O
,	O
each	O
new	O
pair	O
fr	O
;	O
xg	O
in	O
the	O
list	O
at	O
the	O
ith	O
iteration	O
is	O
created	O
by	O
drawing	O
the	O
new	O
x	O
from	O
its	O
distribution	B
and	O
drawing	O
at	O
random	B
with	O
replacement	O
(	O
j	O
(	O
cid:0	O
)	O
1	O
)	O
(	O
k	O
(	O
cid:0	O
)	O
1	O
)	O
pairs	O
fr	O
;	O
xg	O
from	O
the	O
list	O
at	O
the	O
(	O
i	O
(	O
cid:0	O
)	O
1	O
)	O
th	O
iteration	O
;	O
these	O
are	O
assembled	O
into	O
a	O
tree	B
fragment	O
(	O
(	O
cid:12	O
)	O
gure	O
47.12	O
)	O
and	O
the	O
sum	O
{	O
product	O
algorithm	O
is	O
run	O
from	O
top	O
to	O
bottom	O
to	O
(	O
cid:12	O
)	O
nd	O
the	O
new	O
r	O
value	O
associated	O
with	O
the	O
new	O
node	O
.	O
as	O
an	O
example	O
,	O
the	O
results	O
of	O
runs	O
with	O
j	O
=	O
4	O
,	O
k	O
=	O
8	O
and	O
noise	O
densities	O
f	O
between	O
0.01	O
and	O
0.10	O
,	O
using	O
10	O
000	O
samples	O
at	O
each	O
iteration	O
,	O
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
47.10.	O
runs	O
with	O
low	O
enough	O
noise	B
level	O
show	O
a	O
collapse	O
to	O
zero	O
entropy	B
after	O
a	O
small	O
number	O
of	O
iterations	O
,	O
and	O
those	O
with	O
high	O
noise	B
level	O
decrease	O
to	O
a	O
non-zero	O
entropy	B
corresponding	O
to	O
a	O
failure	O
to	O
decode	O
.	O
the	O
boundary	O
between	O
these	O
two	O
behaviours	O
is	O
called	O
the	O
threshold	B
of	O
the	O
decoding	B
algorithm	O
for	O
the	O
binary	B
symmetric	I
channel	I
.	O
figure	O
47.10	O
shows	O
by	O
monte	O
carlo	O
simulation	O
that	O
the	O
threshold	B
for	O
regular	B
(	O
j	O
;	O
k	O
)	O
=	O
(	O
4	O
;	O
8	O
)	O
codes	O
is	O
about	O
0.075.	O
richardson	O
and	O
urbanke	O
(	O
2001a	O
)	O
have	O
derived	O
thresholds	O
for	O
regular	O
codes	O
by	O
a	O
tour	O
de	O
force	O
of	O
direct	O
analytic	O
methods	B
.	O
some	O
of	O
these	O
thresholds	O
are	O
shown	O
in	O
table	O
47.13.	O
approximate	O
density	B
evolution	I
for	O
practical	B
purposes	O
,	O
the	O
computational	O
cost	O
of	O
density	O
evolution	B
can	O
be	O
reduced	O
by	O
making	O
gaussian	O
approximations	O
to	O
the	O
probability	B
distributions	I
over	O
the	O
messages	O
in	O
density	O
evolution	B
,	O
and	O
updating	O
only	O
the	O
parameters	B
of	O
these	O
approximations	O
.	O
for	O
further	O
information	B
about	O
these	O
techniques	O
,	O
which	O
produce	O
diagrams	O
known	O
as	O
exit	O
charts	O
,	O
see	O
(	O
ten	O
brink	O
,	O
1999	O
;	O
chung	O
et	O
al.	O
,	O
2001	O
;	O
ten	O
brink	O
et	O
al.	O
,	O
2002	O
)	O
.	O
47.6	O
improving	O
gallager	O
codes	O
since	O
the	O
rediscovery	O
of	O
gallager	O
codes	O
,	O
two	O
methods	B
have	O
been	O
found	O
for	O
enhancing	O
their	O
performance	O
.	O
clump	O
bits	O
and	O
checks	O
together	O
first	O
,	O
we	O
can	O
make	O
gallager	O
codes	O
in	O
which	O
the	O
variable	O
nodes	O
are	O
grouped	O
together	O
into	O
metavariables	O
consisting	O
of	O
say	O
3	O
binary	O
variables	O
,	O
and	O
the	O
check	O
nodes	O
are	O
similarly	O
grouped	O
together	O
into	O
metachecks	O
.	O
as	O
before	O
,	O
a	O
sparse	B
graph	I
can	O
be	O
constructed	O
connecting	O
metavariables	O
to	O
metachecks	O
,	O
with	O
a	O
lot	O
of	O
freedom	O
about	O
the	O
details	O
of	O
how	O
the	O
variables	O
and	O
checks	O
within	O
are	O
wired	O
up	O
.	O
one	O
way	O
to	O
set	B
the	O
wiring	O
is	O
to	O
work	O
in	O
a	O
(	O
cid:12	O
)	O
nite	O
(	O
cid:12	O
)	O
eld	O
gf	O
(	O
q	O
)	O
such	O
as	O
gf	O
(	O
4	O
)	O
or	O
gf	O
(	O
8	O
)	O
,	O
de	O
(	O
cid:12	O
)	O
ne	O
low-density	B
parity-check	I
matrices	O
using	O
elements	O
of	O
gf	O
(	O
q	O
)	O
,	O
and	O
translate	O
our	O
binary	O
messages	O
into	O
gf	O
(	O
q	O
)	O
using	O
a	O
mapping	B
such	O
as	O
the	O
one	O
for	O
gf	O
(	O
4	O
)	O
given	O
in	O
table	O
47.14.	O
now	O
,	O
when	O
messages	O
are	O
passed	O
during	O
decoding	B
,	O
those	O
messages	O
are	O
probabilities	O
and	O
likelihoods	O
over	O
conjunctions	O
of	O
binary	O
variables	O
.	O
for	O
example	O
if	O
each	O
clump	O
contains	O
three	O
binary	O
variables	O
then	O
the	O
likelihoods	O
will	O
describe	O
the	O
likelihoods	O
of	O
the	O
eight	O
alternative	O
states	O
of	O
those	O
bits	O
.	O
with	O
carefully	O
optimized	O
constructions	O
,	O
the	O
resulting	O
codes	O
over	O
gf	O
(	O
4	O
)	O
,	O
rf	O
f	O
f	O
x	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:9	O
)	O
?	O
@	O
@	O
@	O
r	O
567	O
(	O
cid:21	O
)	O
iteration	O
i	O
(	O
cid:0	O
)	O
1	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:9	O
)	O
?	O
@	O
@	O
r	O
(	O
cid:0	O
)	O
f	O
f	O
f	O
(	O
cid:0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:9	O
)	O
f	O
?	O
x	O
r	O
(	O
cid:21	O
)	O
iteration	O
i	O
figure	O
47.12.	O
a	O
tree-fragment	O
constructed	O
during	O
monte	O
carlo	O
simulation	O
of	B
density	I
evolution	I
.	O
this	O
fragment	O
is	O
appropriate	O
for	O
a	O
regular	B
j	O
=	O
3	O
,	O
k	O
=	O
4	O
gallager	O
code	B
.	O
(	O
j	O
;	O
k	O
)	O
(	O
3,6	O
)	O
(	O
4,8	O
)	O
(	O
5,10	O
)	O
fmax	O
0.084	O
0.076	O
0.068	O
table	O
47.13.	O
thresholds	O
fmax	O
for	O
regular	O
low-density	B
parity-check	I
codes	O
,	O
assuming	O
sum	O
{	O
product	O
decoding	O
algorithm	B
,	O
from	O
richardson	O
and	O
urbanke	O
(	O
2001a	O
)	O
.	O
the	O
shannon	O
limit	O
for	O
rate-1/2	O
codes	O
is	O
fmax	O
=	O
0:11.	O
gf	O
(	O
4	O
)	O
$	O
binary	O
0	O
$	O
00	O
1	O
$	O
01	O
a	O
$	O
10	O
b	O
$	O
11	O
table	O
47.14.	O
translation	O
between	O
gf	O
(	O
4	O
)	O
and	O
binary	O
for	O
message	O
symbols	O
.	O
00	O
gf	O
(	O
4	O
)	O
!	O
binary	O
0	O
!	O
00	O
1	O
!	O
10	O
a	O
!	O
11	O
b	O
!	O
01	O
01	O
10	O
11	O
table	O
47.15.	O
translation	O
between	O
gf	O
(	O
4	O
)	O
and	O
binary	O
for	O
matrix	O
entries	O
.	O
an	O
m	O
(	O
cid:2	O
)	O
n	O
parity-check	B
matrix	I
over	O
gf	O
(	O
4	O
)	O
can	O
be	O
turned	O
into	O
a	O
2m	O
(	O
cid:2	O
)	O
2n	O
binary	O
parity-check	O
matrix	B
in	O
this	O
way	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
568	O
47	O
|	O
low-density	B
parity-check	I
codes	O
f	O
0	O
=	O
[	O
f	O
0	O
+	O
f	O
1	O
]	O
+	O
[	O
f	O
a	O
+	O
f	O
b	O
]	O
f	O
1	O
=	O
[	O
f	O
0	O
(	O
cid:0	O
)	O
f	O
1	O
]	O
+	O
[	O
f	O
a	O
(	O
cid:0	O
)	O
f	O
b	O
]	O
f	O
a	O
=	O
[	O
f	O
0	O
+	O
f	O
1	O
]	O
(	O
cid:0	O
)	O
[	O
f	O
a	O
+	O
f	O
b	O
]	O
f	O
b	O
=	O
[	O
f	O
0	O
(	O
cid:0	O
)	O
f	O
1	O
]	O
(	O
cid:0	O
)	O
[	O
f	O
a	O
(	O
cid:0	O
)	O
f	O
b	O
]	O
luby	O
reg	O
gf	O
(	O
2	O
)	O
irreg	O
gf	O
(	O
2	O
)	O
irreg	O
gf	O
(	O
8	O
)	O
reg	O
gf	O
(	O
16	O
)	O
gallileo	O
turbo	O
0.1	O
0.01	O
0.001	O
0.0001	O
1e-05	O
1e-06	O
y	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
r	O
o	O
r	O
r	O
e	O
-	O
t	O
i	O
b	O
l	O
a	O
c	O
i	O
r	O
i	O
p	O
m	O
e	O
algorithm	B
47.16.	O
the	O
fourier	O
transform	O
over	O
gf	O
(	O
4	O
)	O
.	O
the	O
fourier	O
transform	O
f	O
of	O
a	O
function	B
f	O
over	O
gf	O
(	O
2	O
)	O
is	O
given	O
by	O
f	O
0	O
=	O
f	O
0	O
+	O
f	O
1	O
,	O
f	O
1	O
=	O
f	O
0	O
(	O
cid:0	O
)	O
f	O
1.	O
transforms	O
over	O
gf	O
(	O
2k	O
)	O
can	O
be	O
viewed	O
as	O
a	O
sequence	B
of	O
binary	O
transforms	O
in	O
each	O
of	O
k	O
dimensions	B
.	O
the	O
inverse	O
transform	O
is	O
identical	O
to	O
the	O
fourier	O
transform	O
,	O
except	O
that	O
we	O
also	O
divide	O
by	O
2k	O
.	O
-0.4	O
-0.2	O
0	O
0.2	O
0.4	O
0.6	O
0.8	O
signal	O
to	O
noise	B
ratio	O
(	O
db	O
)	O
figure	O
47.17.	O
comparison	O
of	O
regular	O
binary	O
gallager	O
codes	O
with	O
irregular	O
codes	O
,	O
codes	O
over	O
gf	O
(	O
q	O
)	O
,	O
and	O
other	O
outstanding	O
codes	O
of	O
rate	O
1/4	O
.	O
from	O
left	O
(	O
best	O
performance	O
)	O
to	O
right	O
:	O
irregular	B
low-density	O
parity-check	B
code	I
over	O
gf	O
(	O
8	O
)	O
,	O
blocklength	O
48	O
000	O
bits	O
(	O
davey	O
,	O
1999	O
)	O
;	O
jpl	O
turbo	B
code	I
(	O
jpl	O
,	O
1996	O
)	O
blocklength	O
65	O
536	O
;	O
regular	B
low-density	O
parity-check	O
over	O
gf	O
(	O
16	O
)	O
,	O
blocklength	O
24	O
448	O
bits	O
(	O
davey	O
and	O
mackay	O
,	O
1998	O
)	O
;	O
irregular	B
binary	O
low-density	O
parity-	O
check	O
code	B
,	O
blocklength	O
16	O
000	O
bits	O
(	O
davey	O
,	O
1999	O
)	O
;	O
luby	O
et	O
al	O
.	O
(	O
1998	O
)	O
irregular	B
binary	O
low-	O
density	B
parity-check	O
code	B
,	O
blocklength	O
64	O
000	O
bits	O
;	O
jpl	O
code	B
for	O
galileo	O
(	O
in	O
1992	O
,	O
this	O
was	O
the	O
best	O
known	O
code	B
of	O
rate	B
1/4	O
)	O
;	O
regular	B
binary	O
low-density	B
parity-check	I
code	I
:	O
blocklength	O
40	O
000	O
bits	O
(	O
mackay	O
,	O
1999b	O
)	O
.	O
the	O
shannon	O
limit	O
is	O
at	O
about	O
(	O
cid:0	O
)	O
0:79	O
db	O
.	O
as	O
of	O
2003	O
,	O
even	O
better	O
sparse-graph	O
codes	O
have	O
been	O
constructed	O
.	O
gf	O
(	O
8	O
)	O
,	O
and	O
gf	O
(	O
16	O
)	O
perform	O
nearly	O
one	O
decibel	B
better	O
than	O
comparable	O
binary	O
gallager	O
codes	O
.	O
the	O
computational	O
cost	O
for	O
decoding	B
in	O
gf	O
(	O
q	O
)	O
scales	O
as	O
q	O
log	O
q	O
,	O
if	O
the	O
ap-	O
propriate	O
fourier	O
transform	O
is	O
used	O
in	O
the	O
check	O
nodes	O
:	O
the	O
update	O
rule	O
for	O
the	O
check-to-variable	O
message	O
,	O
ra	O
mn	O
=	O
xx	O
:	O
xn=a	O
hmn0xn0	O
=	O
zm3	O
	O
2	O
4	O
xn02n	O
(	O
m	O
)	O
qxj	O
mj	O
;	O
(	O
47.15	O
)	O
5	O
yj2n	O
(	O
m	O
)	O
nn	O
is	O
a	O
convolution	B
of	O
the	O
quantities	O
qa	O
mj	O
,	O
so	O
the	O
summation	O
can	O
be	O
replaced	O
by	O
a	O
product	O
of	O
the	O
fourier	O
transforms	O
of	O
qa	O
mj	O
for	O
j	O
2	O
n	O
(	O
m	O
)	O
nn	O
,	O
followed	O
by	O
an	O
inverse	O
fourier	O
transform	O
.	O
the	O
fourier	O
transform	O
for	O
gf	O
(	O
4	O
)	O
is	O
shown	O
in	O
algorithm	O
47.16.	O
make	O
the	O
graph	B
irregular	O
the	O
second	O
way	O
of	O
improving	O
gallager	O
codes	O
,	O
introduced	O
by	O
luby	O
et	O
al	O
.	O
(	O
2001b	O
)	O
,	O
is	O
to	O
make	O
their	O
graphs	O
irregular	O
.	O
instead	O
of	O
giving	O
all	O
variable	O
nodes	O
the	O
same	O
degree	B
j	O
,	O
we	O
can	O
have	O
some	O
variable	O
nodes	O
with	O
degree	O
2	O
,	O
some	O
3	O
,	O
some	O
4	O
,	O
and	O
a	O
few	O
with	O
degree	O
20.	O
check	O
nodes	O
can	O
also	O
be	O
given	O
unequal	O
degrees	O
{	O
this	O
helps	O
improve	O
performance	O
on	O
erasure	O
channels	O
,	O
but	O
it	O
turns	O
out	O
that	O
for	O
the	O
gaussian	O
channel	B
,	O
the	O
best	O
graphs	O
have	O
regular	B
check	O
degrees	O
.	O
figure	O
47.17	O
illustrates	O
the	O
bene	O
(	O
cid:12	O
)	O
ts	O
o	O
(	O
cid:11	O
)	O
ered	O
by	O
these	O
two	O
methods	B
for	O
im-	O
proving	O
gallager	O
codes	O
,	O
focussing	O
on	O
codes	O
of	O
rate	O
1/4	O
.	O
making	O
the	O
binary	O
code	O
irregular	B
gives	O
a	O
win	O
of	O
about	O
0.4	O
db	O
;	O
switching	O
from	O
gf	O
(	O
2	O
)	O
to	O
gf	O
(	O
16	O
)	O
gives	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
47.7	O
:	O
fast	B
encoding	I
of	O
low-density	B
parity-check	I
codes	O
569	O
figure	O
47.18.	O
an	O
algebraically	O
constructed	O
low-density	B
parity-check	I
code	I
satisfying	O
many	O
redundant	O
constraints	O
outperforms	O
an	O
equivalent	O
random	B
gallager	O
code	B
.	O
the	O
table	O
shows	O
the	O
n	O
,	O
m	O
,	O
k	O
,	O
distance	B
d	O
,	O
and	O
row	O
weight	B
k	O
of	O
some	O
di	O
(	O
cid:11	O
)	O
erence-set	O
cyclic	B
codes	O
,	O
highlighting	O
the	O
codes	O
that	O
have	O
large	O
d=n	O
,	O
small	O
k	O
,	O
and	O
large	O
n=m	O
.	O
in	O
the	O
comparison	O
the	O
gallager	O
code	B
had	O
(	O
j	O
;	O
k	O
)	O
=	O
(	O
4	O
;	O
13	O
)	O
,	O
and	O
rate	O
identical	O
to	O
the	O
n	O
=	O
273	O
di	O
(	O
cid:11	O
)	O
erence-set	O
cyclic	B
code	O
.	O
vertical	O
axis	O
:	O
block	B
error	O
probability	B
.	O
horizontal	O
axis	O
:	O
signal-to-noise	B
ratio	I
eb=n0	O
(	O
db	O
)	O
.	O
difference	O
set	B
cyclic	O
codes	O
21	O
n	O
7	O
73	O
m	O
4	O
10	O
28	O
11	O
k	O
3	O
45	O
d	O
4	O
6	O
10	O
9	O
5	O
3	O
k	O
273	O
82	O
191	O
18	O
17	O
1057	O
244	O
813	O
34	O
33	O
4161	O
730	O
3431	O
66	O
65	O
1	O
0.1	O
0.01	O
0.001	O
gallager	O
(	O
273,82	O
)	O
dsc	O
(	O
273,82	O
)	O
0.0001	O
1.5	O
2	O
2.5	O
3	O
3.5	O
4	O
about	O
0.6	O
db	O
;	O
and	O
matthew	O
davey	O
’	O
s	O
code	B
that	O
combines	O
both	O
these	O
features	O
{	O
it	O
’	O
s	O
irregular	B
over	O
gf	O
(	O
8	O
)	O
{	O
gives	O
a	O
win	O
of	O
about	O
0.9	O
db	O
over	O
the	O
regular	B
binary	O
gallager	O
code	B
.	O
methods	B
for	O
optimizing	O
the	O
pro	O
(	O
cid:12	O
)	O
le	O
of	O
a	O
gallager	O
code	B
(	O
that	O
is	O
,	O
its	O
number	O
of	O
rows	O
and	O
columns	O
of	O
each	O
degree	B
)	O
,	O
have	O
been	O
developed	O
by	O
richardson	O
et	O
al	O
.	O
(	O
2001	O
)	O
and	O
have	O
led	O
to	O
low-density	B
parity-check	I
codes	O
whose	O
performance	O
,	O
when	O
decoded	O
by	O
the	O
sum	O
{	O
product	O
algorithm	O
,	O
is	O
within	O
a	O
hair	O
’	O
s	O
breadth	O
of	O
the	O
shannon	O
limit	O
.	O
algebraic	O
constructions	O
of	O
gallager	O
codes	O
the	O
performance	O
of	O
regular	O
gallager	O
codes	O
can	O
be	O
enhanced	O
in	O
a	O
third	O
man-	O
ner	O
:	O
by	O
designing	O
the	O
code	B
to	O
have	O
redundant	O
sparse	O
constraints	O
.	O
there	O
is	O
a	O
di	O
(	O
cid:11	O
)	O
erence-set	O
cyclic	B
code	O
,	O
for	O
example	O
,	O
that	O
has	O
n	O
=	O
273	O
and	O
k	O
=	O
191	O
,	O
but	O
the	O
code	B
satis	O
(	O
cid:12	O
)	O
es	O
not	O
m	O
=	O
82	O
but	O
n	O
,	O
i.e.	O
,	O
273	O
low-weight	O
constraints	O
(	O
(	O
cid:12	O
)	O
gure	O
47.18	O
)	O
.	O
it	O
is	O
impossible	O
to	O
make	O
random	B
gallager	O
codes	O
that	O
have	O
anywhere	O
near	O
this	O
much	O
redundancy	B
among	O
their	O
checks	O
.	O
the	O
di	O
(	O
cid:11	O
)	O
erence-set	O
cyclic	B
code	O
performs	O
about	O
0.7	O
db	O
better	O
than	O
an	O
equivalent	O
random	B
gallager	O
code	B
.	O
an	O
open	O
problem	O
is	O
to	O
discover	O
codes	O
sharing	O
the	O
remarkable	O
properties	O
of	O
the	O
di	O
(	O
cid:11	O
)	O
erence-set	O
cyclic	B
codes	O
but	O
with	O
di	O
(	O
cid:11	O
)	O
erent	O
blocklengths	O
and	O
rates	O
.	O
i	O
call	O
this	O
task	O
the	O
tanner	O
challenge	O
.	O
47.7	O
fast	B
encoding	I
of	O
low-density	B
parity-check	I
codes	O
we	O
now	O
discuss	O
methods	B
for	O
fast	B
encoding	I
of	O
low-density	B
parity-check	I
codes	O
{	O
faster	O
than	O
the	O
standard	O
method	O
,	O
in	O
which	O
a	O
generator	B
matrix	I
g	O
is	O
found	O
by	O
gaussian	O
elimination	O
(	O
at	O
a	O
cost	O
of	O
order	O
m	O
3	O
)	O
and	O
then	O
each	O
block	B
is	O
encoded	O
by	O
multiplying	O
it	O
by	O
g	O
(	O
at	O
a	O
cost	O
of	O
order	O
m	O
2	O
)	O
.	O
staircase	B
codes	O
certain	O
low-density	B
parity-check	I
matrices	O
with	O
m	O
columns	O
of	O
weight	O
2	O
or	O
less	O
can	O
be	O
encoded	O
easily	O
in	O
linear	O
time	O
.	O
for	O
example	O
,	O
if	O
the	O
matrix	B
has	O
a	O
staircase	B
structure	O
as	O
illustrated	O
by	O
the	O
right-hand	O
side	O
of	O
h	O
=2	O
6664	O
;	O
3	O
7775	O
(	O
47.16	O
)	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
570	O
47	O
|	O
low-density	B
parity-check	I
codes	O
and	O
if	O
the	O
data	O
s	O
are	O
loaded	O
into	O
the	O
(	O
cid:12	O
)	O
rst	O
k	O
bits	O
,	O
then	O
the	O
m	O
parity	B
bits	O
p	O
can	O
be	O
computed	O
from	O
left	O
to	O
right	O
in	O
linear	O
time	O
.	O
p1	O
=	O
pk	O
p2	O
=	O
p1	O
+	O
pk	O
p3	O
=	O
p2	O
+	O
pk	O
pm	O
=	O
pm	O
(	O
cid:0	O
)	O
1+	O
pk	O
...	O
n=1	O
h1nsn	O
n=1	O
h2nsn	O
n=1	O
h3nsn	O
n=1	O
hm	O
nsn	O
:	O
(	O
47.17	O
)	O
if	O
we	O
call	O
two	O
parts	O
of	O
the	O
h	O
matrix	B
[	O
hsjhp	O
]	O
,	O
we	O
can	O
describe	O
the	O
encoding	O
operation	O
in	O
two	O
steps	O
:	O
(	O
cid:12	O
)	O
rst	O
compute	O
an	O
intermediate	O
parity	B
vector	O
v	O
=	O
hss	O
;	O
then	O
pass	O
v	O
through	O
an	O
accumulator	B
to	O
create	O
p.	O
figure	O
47.19.	O
the	O
parity-check	B
matrix	I
in	O
approximate	O
lower-triangular	O
form	O
.	O
the	O
cost	O
of	O
this	O
encoding	O
method	B
is	O
linear	B
if	O
the	O
sparsity	O
of	O
h	O
is	O
exploited	O
when	O
computing	O
the	O
sums	O
in	O
(	O
47.17	O
)	O
.	O
fast	B
encoding	I
of	O
general	O
low-density	O
parity-check	O
codes	O
richardson	O
and	O
urbanke	O
(	O
2001b	O
)	O
demonstrated	O
an	O
elegant	O
method	B
by	O
which	O
the	O
encoding	O
cost	O
of	O
any	O
low-density	B
parity-check	I
code	I
can	O
be	O
reduced	O
from	O
the	O
straightforward	O
method	B
’	O
s	O
m	O
2	O
to	O
a	O
cost	O
of	O
n	O
+	O
g2	O
,	O
where	O
g	O
,	O
the	O
gap	O
,	O
is	O
hopefully	O
a	O
small	O
constant	O
,	O
and	O
in	O
the	O
worst	O
cases	O
scales	O
as	O
a	O
small	O
fraction	O
of	O
n	O
.	O
(	O
cid:27	O
)	O
-	O
(	O
cid:27	O
)	O
g	O
m	O
-	O
a	O
c	O
(	O
cid:27	O
)	O
b	O
d	O
n	O
@	O
@	O
@	O
t	O
0	O
@	O
@	O
@	O
e	O
-	O
6	O
m	O
6	O
g	O
?	O
?	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
step	O
,	O
the	O
parity-check	B
matrix	I
is	O
rearranged	O
,	O
by	O
row-interchange	O
and	O
column-interchange	O
,	O
into	O
the	O
approximate	O
lower-triangular	O
form	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
47.19.	O
the	O
original	O
matrix	B
h	O
was	O
very	O
sparse	O
,	O
so	O
the	O
six	B
matrices	O
a	O
,	O
b	O
,	O
t	O
,	O
c	O
,	O
d	O
;	O
and	O
e	O
are	O
also	O
very	O
sparse	O
.	O
the	O
matrix	B
t	O
is	O
lower	O
triangular	O
and	O
has	O
1s	O
everywhere	O
on	O
the	O
diagonal	O
.	O
h	O
=	O
(	O
cid:20	O
)	O
a	O
b	O
t	O
c	O
d	O
e	O
(	O
cid:21	O
)	O
:	O
(	O
47.18	O
)	O
the	O
source	O
vector	O
s	O
of	O
length	O
k	O
=	O
n	O
(	O
cid:0	O
)	O
m	O
is	O
encoded	O
into	O
a	O
transmission	O
t	O
=	O
[	O
s	O
;	O
p1	O
;	O
p2	O
]	O
as	O
follows	O
.	O
1.	O
compute	O
the	O
upper	O
syndrome	B
of	O
the	O
source	O
vector	O
,	O
za	O
=	O
as	O
:	O
(	O
47.19	O
)	O
this	O
can	O
be	O
done	O
in	O
linear	O
time	O
.	O
2.	O
find	O
a	O
setting	O
of	O
the	O
second	O
parity	B
bits	O
,	O
pa	O
2	O
,	O
such	O
that	O
the	O
upper	O
syn-	O
drome	O
is	O
zero	O
.	O
2	O
=	O
(	O
cid:0	O
)	O
t	O
(	O
cid:0	O
)	O
1za	O
:	O
pa	O
(	O
47.20	O
)	O
this	O
vector	O
can	O
be	O
found	O
in	O
linear	O
time	O
by	O
back-substitution	O
,	O
i.e.	O
,	O
com-	O
puting	O
the	O
(	O
cid:12	O
)	O
rst	O
bit	B
of	O
pa	O
2	O
,	O
then	O
the	O
second	O
,	O
then	O
the	O
third	O
,	O
and	O
so	O
forth	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
47.8	O
:	O
further	O
reading	O
571	O
3.	O
compute	O
the	O
lower	O
syndrome	B
of	O
the	O
vector	O
[	O
s	O
;	O
0	O
;	O
pa	O
2	O
]	O
:	O
zb	O
=	O
cs	O
(	O
cid:0	O
)	O
epa	O
2	O
:	O
this	O
can	O
be	O
done	O
in	O
linear	O
time	O
.	O
4.	O
now	O
we	O
get	O
to	O
the	O
clever	O
bit	B
.	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
matrix	B
f	O
(	O
cid:17	O
)	O
(	O
cid:0	O
)	O
et	O
(	O
cid:0	O
)	O
1b	O
+	O
d	O
;	O
(	O
47.21	O
)	O
(	O
47.22	O
)	O
and	O
(	O
cid:12	O
)	O
nd	O
its	O
inverse	O
,	O
f	O
(	O
cid:0	O
)	O
1.	O
this	O
computation	O
needs	O
to	O
be	O
done	O
once	O
only	O
,	O
and	O
its	O
cost	O
is	O
of	O
order	O
g3	O
.	O
this	O
inverse	O
f	O
(	O
cid:0	O
)	O
1	O
is	O
a	O
dense	O
g	O
(	O
cid:2	O
)	O
g	O
matrix	B
.	O
[	O
if	O
f	O
is	O
not	O
invertible	O
then	O
either	O
h	O
is	O
not	O
of	O
full	O
rank	O
,	O
or	O
else	O
further	O
column	O
permutations	O
of	O
h	O
can	O
produce	O
an	O
f	O
that	O
is	O
invertible	O
.	O
]	O
set	B
the	O
(	O
cid:12	O
)	O
rst	O
parity	B
bits	O
,	O
p1	O
,	O
to	O
p1	O
=	O
(	O
cid:0	O
)	O
f	O
(	O
cid:0	O
)	O
1zb	O
:	O
(	O
47.23	O
)	O
this	O
operation	O
has	O
a	O
cost	O
of	O
order	O
g2	O
.	O
claim	O
:	O
at	O
this	O
point	O
,	O
we	O
have	O
found	O
the	O
correct	O
setting	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
parity	B
bits	O
,	O
p1	O
.	O
5.	O
discard	O
the	O
tentative	O
parity	B
bits	O
pa	O
2	O
and	O
(	O
cid:12	O
)	O
nd	O
the	O
new	O
upper	O
syndrome	B
,	O
zc	O
=	O
za	O
+	O
bp1	O
:	O
(	O
47.24	O
)	O
this	O
can	O
be	O
done	O
in	O
linear	O
time	O
.	O
6.	O
find	O
a	O
setting	O
of	O
the	O
second	O
parity	B
bits	O
,	O
p2	O
,	O
such	O
that	O
the	O
upper	O
syndrome	B
is	O
zero	O
,	O
this	O
vector	O
can	O
be	O
found	O
in	O
linear	O
time	O
by	O
back-substitution	O
.	O
p2	O
=	O
(	O
cid:0	O
)	O
t	O
(	O
cid:0	O
)	O
1zc	O
(	O
47.25	O
)	O
47.8	O
further	O
reading	O
low-density	O
parity-check	O
codes	O
codes	O
were	O
(	O
cid:12	O
)	O
rst	O
studied	O
in	O
1962	O
by	O
gallager	O
,	O
then	O
were	O
generally	O
forgotten	O
by	O
the	O
coding	B
theory	I
community	O
.	O
tanner	O
(	O
1981	O
)	O
generalized	B
gallager	O
’	O
s	O
work	O
by	O
introducing	O
more	O
general	O
constraint	O
nodes	O
;	O
the	O
codes	O
that	O
are	O
now	O
called	O
turbo	O
product	O
codes	O
should	O
in	O
fact	O
be	O
called	O
tanner	O
product	O
codes	O
,	O
since	O
tanner	O
proposed	O
them	O
,	O
and	O
his	O
colleagues	O
(	O
karplus	O
and	O
krit	O
,	O
1991	O
)	O
implemented	O
them	O
in	O
hardware	O
.	O
publications	O
on	O
gallager	O
codes	O
contributing	O
to	O
their	O
1990s	O
rebirth	O
include	O
(	O
wiberg	O
et	O
al.	O
,	O
1995	O
;	O
mackay	O
and	O
neal	O
,	O
1995	O
;	O
mackay	O
and	O
neal	O
,	O
1996	O
;	O
wiberg	O
,	O
1996	O
;	O
mackay	O
,	O
1999b	O
;	O
spielman	O
,	O
1996	O
;	O
sipser	O
and	O
spielman	O
,	O
1996	O
)	O
.	O
low-precision	O
decoding	B
algorithms	O
and	O
fast	O
encoding	O
algorithms	B
for	O
gallager	O
codes	O
are	O
discussed	O
in	O
(	O
richardson	O
and	O
ur-	O
banke	O
,	O
2001a	O
;	O
richardson	O
and	O
urbanke	O
,	O
2001b	O
)	O
.	O
mackay	O
and	O
davey	O
(	O
2000	O
)	O
showed	O
that	O
low-density	B
parity-check	I
codes	O
can	O
outperform	O
reed	O
{	O
solomon	O
codes	O
,	O
even	O
on	O
the	O
reed	O
{	O
solomon	O
codes	O
’	O
home	O
turf	O
:	O
high	O
rate	O
and	O
short	O
block-	O
lengths	O
.	O
other	O
important	O
papers	O
include	O
(	O
luby	O
et	O
al.	O
,	O
2001a	O
;	O
luby	O
et	O
al.	O
,	O
2001b	O
;	O
luby	O
et	O
al.	O
,	O
1997	O
;	O
davey	O
and	O
mackay	O
,	O
1998	O
;	O
richardson	O
et	O
al.	O
,	O
2001	O
;	O
chung	O
et	O
al.	O
,	O
2001	O
)	O
.	O
useful	O
tools	O
for	O
the	O
design	O
of	O
irregular	B
low-density	O
parity-	O
check	O
codes	O
include	O
(	O
chung	O
et	O
al.	O
,	O
1999	O
;	O
urbanke	O
,	O
2001	O
)	O
.	O
see	O
(	O
wiberg	O
,	O
1996	O
;	O
frey	O
,	O
1998	O
;	O
mceliece	O
et	O
al.	O
,	O
1998	O
)	O
for	O
further	O
discussion	O
of	O
the	O
sum	O
{	O
product	O
algorithm	O
.	O
for	O
a	O
view	O
of	O
low-density	O
parity-check	B
code	I
decoding	O
in	O
terms	O
of	O
group	O
theory	B
and	O
coding	B
theory	I
,	O
see	O
(	O
forney	O
,	O
2001	O
;	O
o	O
(	O
cid:11	O
)	O
er	O
and	O
soljanin	O
,	O
2000	O
;	O
o	O
(	O
cid:11	O
)	O
er	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
572	O
47	O
|	O
low-density	B
parity-check	I
codes	O
and	O
soljanin	O
,	O
2001	O
)	O
;	O
and	O
for	O
background	O
reading	O
on	O
this	O
topic	O
see	O
(	O
hartmann	O
and	O
rudolph	O
,	O
1976	O
;	O
terras	O
,	O
1999	O
)	O
.	O
there	O
is	O
a	O
growing	O
literature	O
on	O
the	O
prac-	O
tical	O
design	O
of	O
low-density	B
parity-check	I
codes	O
(	O
mao	O
and	O
banihashemi	O
,	O
2000	O
;	O
mao	O
and	O
banihashemi	O
,	O
2001	O
;	O
ten	O
brink	O
et	O
al.	O
,	O
2002	O
)	O
;	O
they	O
are	O
now	O
being	O
adopted	O
for	O
applications	O
from	O
hard	O
drives	O
to	O
satellite	B
communications	I
.	O
for	O
low-density	O
parity-check	O
codes	O
applicable	O
to	O
quantum	B
error-correction	I
,	O
see	O
mackay	O
et	O
al	O
.	O
(	O
2004	O
)	O
.	O
47.9	O
exercises	O
exercise	O
47.1	O
.	O
[	O
2	O
]	O
the	O
‘	O
hyperbolic	O
tangent	O
’	O
version	O
of	O
the	O
decoding	O
algorithm	B
.	O
in	O
section	O
47.3	O
,	O
the	O
sum	O
{	O
product	O
decoding	O
algorithm	B
for	O
low-density	O
parity-	O
check	O
codes	O
was	O
presented	O
(	O
cid:12	O
)	O
rst	O
in	O
terms	O
of	O
quantities	O
q	O
0=1	O
mn	O
,	O
then	O
in	O
terms	O
of	O
quantities	O
(	O
cid:14	O
)	O
q	O
and	O
(	O
cid:14	O
)	O
r.	O
there	O
is	O
a	O
third	O
description	O
,	O
in	O
which	O
the	O
fqg	O
are	O
replaced	O
by	O
log	O
probability-ratios	O
,	O
mn	O
and	O
r0=1	O
lmn	O
(	O
cid:17	O
)	O
ln	O
q0	O
mn	O
q1	O
mn	O
:	O
show	O
that	O
(	O
cid:14	O
)	O
qmn	O
(	O
cid:17	O
)	O
q0	O
mn	O
(	O
cid:0	O
)	O
q1	O
mn	O
=	O
tanh	O
(	O
lmn=2	O
)	O
:	O
(	O
47.26	O
)	O
(	O
47.27	O
)	O
derive	O
the	O
update	O
rules	B
for	O
frg	O
and	O
flg	O
.	O
exercise	O
47.2	O
.	O
[	O
2	O
,	O
p.572	O
]	O
i	O
am	O
sometimes	O
asked	O
‘	O
why	O
not	O
decode	O
other	O
linear	B
codes	I
,	O
for	O
example	O
algebraic	O
codes	O
,	O
by	O
transforming	O
their	O
parity-check	O
matrices	O
so	O
that	O
they	O
are	O
low-density	O
,	O
and	O
applying	O
the	O
sum	O
{	O
product	O
algorithm	O
?	O
’	O
[	O
recall	O
that	O
any	O
linear	B
combination	O
of	O
rows	O
of	O
h	O
,	O
h0	O
=	O
ph	O
,	O
is	O
a	O
valid	O
parity-check	B
matrix	I
for	O
a	O
code	B
,	O
as	O
long	O
as	O
the	O
matrix	B
p	O
is	O
invertible	O
;	O
so	O
there	O
are	O
many	O
parity	B
check	O
matrices	B
for	O
any	O
one	O
code	B
.	O
]	O
explain	O
why	O
a	O
random	B
linear	I
code	O
does	O
not	O
have	O
a	O
low-density	O
parity-	O
check	O
matrix	B
.	O
[	O
here	O
,	O
low-density	O
means	O
‘	O
having	O
row-weight	O
at	O
most	O
k	O
’	O
,	O
where	O
k	O
is	O
some	O
small	O
constant	O
(	O
cid:28	O
)	O
n	O
.	O
]	O
exercise	O
47.3	O
.	O
[	O
3	O
]	O
show	O
that	O
if	O
a	O
low-density	B
parity-check	I
code	I
has	O
more	O
than	O
m	O
columns	O
of	O
weight	O
2	O
{	O
say	O
(	O
cid:11	O
)	O
m	O
columns	O
,	O
where	O
(	O
cid:11	O
)	O
>	O
1	O
{	O
then	O
the	O
code	B
will	O
have	O
words	O
with	O
weight	O
of	O
order	O
log	O
m	O
.	O
exercise	O
47.4	O
.	O
[	O
5	O
]	O
in	O
section	O
13.5	O
we	O
found	O
the	O
expected	O
value	O
of	O
the	O
weight	O
enumerator	O
function	B
a	O
(	O
w	O
)	O
,	O
averaging	O
over	O
the	O
ensemble	B
of	O
all	O
random	B
linear	I
codes	O
.	O
this	O
calculation	O
can	O
also	O
be	O
carried	O
out	O
for	O
the	O
ensemble	B
of	O
low-density	B
parity-check	I
codes	O
(	O
gallager	O
,	O
1963	O
;	O
mackay	O
,	O
1999b	O
;	O
litsyn	O
and	O
shevelev	O
,	O
2002	O
)	O
.	O
it	O
is	O
plausible	O
,	O
however	O
,	O
that	O
the	O
mean	B
value	O
of	O
a	O
(	O
w	O
)	O
is	O
not	O
always	O
a	O
good	B
indicator	O
of	O
the	O
typical	O
value	O
of	O
a	O
(	O
w	O
)	O
in	O
the	O
ensemble	B
.	O
for	O
example	O
,	O
if	O
,	O
at	O
a	O
particular	O
value	O
of	O
w	O
,	O
99	O
%	O
of	O
codes	O
have	O
a	O
(	O
w	O
)	O
=	O
0	O
,	O
and	O
1	O
%	O
have	O
a	O
(	O
w	O
)	O
=	O
100	O
000	O
,	O
then	O
while	O
we	O
might	O
say	O
the	O
typical	B
value	O
of	O
a	O
(	O
w	O
)	O
is	O
zero	O
,	O
the	O
mean	B
is	O
found	O
to	O
be	O
1000.	O
find	O
the	O
typical	B
weight	O
enumerator	O
function	B
of	O
low-density	B
parity-check	I
codes	O
.	O
47.10	O
solutions	O
solution	O
to	O
exercise	O
47.2	O
(	O
p.572	O
)	O
.	O
consider	O
codes	O
of	O
rate	O
r	O
and	O
blocklength	O
n	O
,	O
having	O
k	O
=	O
rn	O
source	O
bits	O
and	O
m	O
=	O
(	O
1	O
(	O
cid:0	O
)	O
r	O
)	O
n	O
parity-check	B
bits	I
.	O
let	O
all	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
47.10	O
:	O
solutions	O
573	O
the	O
codes	O
have	O
their	O
bits	O
ordered	O
so	O
that	O
the	O
(	O
cid:12	O
)	O
rst	O
k	O
bits	O
are	O
independent	O
,	O
so	O
that	O
we	O
could	O
if	O
we	O
wish	O
put	O
the	O
code	B
in	O
systematic	B
form	O
,	O
g	O
=	O
[	O
1kjpt	O
]	O
;	O
h	O
=	O
[	O
pj1m	O
]	O
:	O
(	O
47.28	O
)	O
the	O
number	O
of	O
distinct	O
linear	B
codes	I
is	O
the	O
number	O
of	O
matrices	O
p	O
,	O
which	O
is	O
n1	O
=	O
2m	O
k	O
=	O
2n	O
2r	O
(	O
1	O
(	O
cid:0	O
)	O
r	O
)	O
.	O
can	O
these	O
all	O
be	O
expressed	O
as	O
distinct	O
low-density	B
parity-check	I
codes	O
?	O
the	O
number	O
of	O
low-density	O
parity-check	O
matrices	O
with	O
row-weight	O
k	O
is	O
log	O
n1	O
’	O
n	O
2r	O
(	O
1	O
(	O
cid:0	O
)	O
r	O
)	O
k	O
(	O
cid:19	O
)	O
m	O
(	O
cid:18	O
)	O
n	O
and	O
the	O
number	O
of	O
distinct	O
codes	O
that	O
they	O
de	O
(	O
cid:12	O
)	O
ne	O
is	O
at	O
most	O
n2	O
=	O
(	O
cid:18	O
)	O
n	O
k	O
(	O
cid:19	O
)	O
m	O
,	O
m	O
!	O
;	O
(	O
47.29	O
)	O
(	O
47.30	O
)	O
which	O
is	O
much	O
smaller	O
than	O
n1	O
,	O
so	O
,	O
by	O
the	O
pigeon-hole	B
principle	I
,	O
it	O
is	O
not	O
possible	O
for	O
every	O
random	B
linear	I
code	O
to	O
map	O
on	O
to	O
a	O
low-density	O
h.	O
log	O
n2	O
<	O
n	O
k	O
log	O
n	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
48	O
convolutional	B
codes	O
and	O
turbo	O
codes	O
this	O
chapter	O
follows	O
tightly	O
on	O
from	O
chapter	O
25.	O
it	O
makes	O
use	O
of	O
the	O
ideas	O
of	O
codes	O
and	O
trellises	O
and	O
the	O
forward	O
{	O
backward	O
algorithm	O
.	O
48.1	O
introduction	O
to	O
convolutional	B
codes	O
when	O
we	O
studied	O
linear	O
block	O
codes	O
,	O
we	O
described	O
them	O
in	O
three	O
ways	O
:	O
1.	O
the	O
generator	B
matrix	I
describes	O
how	O
to	O
turn	O
a	O
string	O
of	O
k	O
arbitrary	O
source	O
bits	O
into	O
a	O
transmission	O
of	O
n	O
bits	O
.	O
2.	O
the	O
parity-check	B
matrix	I
speci	O
(	O
cid:12	O
)	O
es	O
the	O
m	O
=	O
n	O
(	O
cid:0	O
)	O
k	O
parity-check	O
con-	O
straints	O
that	O
a	O
valid	O
codeword	B
satis	O
(	O
cid:12	O
)	O
es	O
.	O
3.	O
the	O
trellis	B
of	O
the	O
code	B
describes	O
its	O
valid	O
codewords	O
in	O
terms	O
of	O
paths	O
through	O
a	O
trellis	B
with	O
labelled	O
edges	O
.	O
a	O
fourth	O
way	O
of	O
describing	O
some	O
block	B
codes	O
,	O
the	O
algebraic	O
approach	O
,	O
is	O
not	O
covered	O
in	O
this	O
book	O
(	O
a	O
)	O
because	O
it	O
has	O
been	O
well	O
covered	O
by	O
numerous	O
other	O
books	O
in	O
coding	O
theory	B
;	O
(	O
b	O
)	O
because	O
,	O
as	O
this	O
part	O
of	O
the	O
book	O
discusses	O
,	O
the	O
state-of-the-art	O
in	O
error-correcting	O
codes	O
makes	O
little	O
use	O
of	O
algebraic	O
coding	B
theory	I
;	O
and	O
(	O
c	O
)	O
because	O
i	O
am	O
not	O
competent	O
to	O
teach	O
this	O
subject	O
.	O
we	O
will	O
now	O
describe	O
convolutional	B
codes	O
in	O
two	O
ways	O
:	O
(	O
cid:12	O
)	O
rst	O
,	O
in	O
terms	O
of	O
mechanisms	O
for	O
generating	O
transmissions	O
t	O
from	O
source	O
bits	O
s	O
;	O
and	O
second	O
,	O
in	O
terms	O
of	O
trellises	O
that	O
describe	O
the	O
constraints	O
satis	O
(	O
cid:12	O
)	O
ed	O
by	O
valid	O
transmissions	O
.	O
48.2	O
linear-feedback	O
shift-registers	O
we	O
generate	O
a	O
transmission	O
with	O
a	O
convolutional	B
code	I
by	O
putting	O
a	O
source	O
stream	O
through	O
a	O
linear	B
(	O
cid:12	O
)	O
lter	O
.	O
this	O
(	O
cid:12	O
)	O
lter	O
makes	O
use	O
of	O
a	O
shift	O
register	O
,	O
linear	B
output	O
functions	B
,	O
and	O
,	O
possibly	O
,	O
linear	B
feedback	O
.	O
i	O
will	O
draw	O
the	O
shift-register	O
in	O
a	O
right-to-left	O
orientation	O
:	O
bits	O
roll	O
from	O
right	O
to	O
left	O
as	O
time	O
goes	O
on	O
.	O
figure	O
48.1	O
shows	O
three	O
linear-feedback	O
shift-registers	O
which	O
could	O
be	O
used	O
to	O
de	O
(	O
cid:12	O
)	O
ne	O
convolutional	B
codes	O
.	O
the	O
rectangular	B
box	O
surrounding	O
the	O
bits	O
z1	O
:	O
:	O
:	O
z7	O
indicates	O
the	O
memory	B
of	O
the	O
(	O
cid:12	O
)	O
lter	O
,	O
also	O
known	O
as	O
its	O
state	O
.	O
all	O
three	O
(	O
cid:12	O
)	O
lters	O
have	O
one	O
input	O
and	O
two	O
outputs	O
.	O
on	O
each	O
clock	O
cycle	O
,	O
the	O
source	O
sup-	O
plies	O
one	O
bit	B
,	O
and	O
the	O
(	O
cid:12	O
)	O
lter	O
outputs	O
two	O
bits	O
t	O
(	O
a	O
)	O
and	O
t	O
(	O
b	O
)	O
.	O
by	O
concatenating	O
together	O
these	O
bits	O
we	O
can	O
obtain	O
from	O
our	O
source	O
stream	O
s1s2s3	O
:	O
:	O
:	O
a	O
trans-	O
mission	O
stream	O
t	O
(	O
a	O
)	O
3	O
:	O
:	O
:	O
:	O
because	O
there	O
are	O
two	O
transmitted	O
bits	O
for	O
every	O
source	O
bit	O
,	O
the	O
codes	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
48.1	O
have	O
rate	B
1/2	O
.	O
because	O
2	O
t	O
(	O
b	O
)	O
1	O
t	O
(	O
a	O
)	O
1	O
t	O
(	O
b	O
)	O
2	O
t	O
(	O
a	O
)	O
3	O
t	O
(	O
b	O
)	O
574	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
48.2	O
:	O
linear-feedback	O
shift-registers	O
575	O
figure	O
48.1.	O
linear-feedback	O
shift-registers	O
for	O
generating	O
convolutional	B
codes	O
with	O
rate	O
1=2	O
.	O
the	O
symbol	O
hd	O
indicates	O
a	O
copying	O
with	O
a	O
delay	O
of	O
one	O
clock	O
cycle	O
.	O
the	O
symbol	O
(	O
cid:8	O
)	O
denotes	O
linear	B
addition	O
modulo	O
2	O
with	O
no	O
delay	O
.	O
the	O
(	O
cid:12	O
)	O
lters	O
are	O
(	O
a	O
)	O
systematic	B
and	O
nonrecursive	B
;	O
(	O
b	O
)	O
nonsystematic	O
and	O
nonrecursive	O
;	O
(	O
c	O
)	O
systematic	B
and	O
recursive	O
.	O
11	O
101	O
011	O
#	O
#	O
#	O
3	O
3	O
5	O
table	O
48.2.	O
how	O
taps	O
in	O
the	O
delay	B
line	I
are	O
converted	O
to	O
octal	B
.	O
hdz7	O
hdz6	O
?	O
--	O
(	O
cid:8	O
)	O
-	O
hdz7	O
hdz6	O
?	O
--	O
(	O
cid:8	O
)	O
-	O
hdz7	O
hdz6	O
?	O
--	O
(	O
cid:8	O
)	O
hdz5	O
?	O
(	O
cid:8	O
)	O
hdz4	O
-	O
hdz3	O
?	O
(	O
cid:8	O
)	O
hdz2	O
-	O
-	O
(	O
cid:8	O
)	O
6	O
hdz2	O
-	O
(	O
cid:8	O
)	O
6	O
hdz2	O
(	O
cid:8	O
)	O
6	O
hdz5	O
?	O
-	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
6	O
hdz5	O
?	O
-	O
(	O
cid:8	O
)	O
-	O
hdz4	O
?	O
-	O
(	O
cid:8	O
)	O
hdz3	O
?	O
(	O
cid:8	O
)	O
-	O
hdz4	O
?	O
-	O
(	O
cid:8	O
)	O
hdz3	O
?	O
(	O
cid:8	O
)	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
‘	O
-	O
hd	O
(	O
cid:27	O
)	O
z1	O
?	O
-	O
(	O
cid:8	O
)	O
z0	O
?	O
-	O
(	O
cid:8	O
)	O
octal	B
name	O
t	O
(	O
a	O
)	O
s	O
t	O
(	O
b	O
)	O
(	O
1	O
;	O
353	O
)	O
8	O
-	O
t	O
(	O
b	O
)	O
-	O
z1	O
(	O
cid:8	O
)	O
6	O
(	O
cid:8	O
)	O
6	O
hd	O
(	O
cid:27	O
)	O
-	O
z0	O
?	O
-	O
(	O
cid:8	O
)	O
s	O
t	O
(	O
a	O
)	O
(	O
247	O
;	O
371	O
)	O
8	O
-	O
(	O
cid:8	O
)	O
6	O
hd	O
-	O
z1	O
(	O
cid:8	O
)	O
6	O
z0	O
6	O
(	O
cid:27	O
)	O
(	O
cid:8	O
)	O
-	O
t	O
(	O
b	O
)	O
t	O
(	O
a	O
)	O
s	O
‘	O
-	O
(	O
cid:0	O
)	O
1	O
;	O
247	O
371	O
(	O
cid:1	O
)	O
8	O
these	O
(	O
cid:12	O
)	O
lters	O
require	O
k	O
=	O
7	O
bits	O
of	O
memory	B
,	O
the	O
codes	O
they	O
de	O
(	O
cid:12	O
)	O
ne	O
are	O
known	O
as	O
a	O
constraint-length	O
7	O
codes	O
.	O
convolutional	B
codes	O
come	O
in	O
three	O
(	O
cid:13	O
)	O
avours	O
,	O
corresponding	O
to	O
the	O
three	O
types	O
of	O
(	O
cid:12	O
)	O
lter	O
in	O
(	O
cid:12	O
)	O
gure	O
48.1.	O
systematic	B
nonrecursive	O
the	O
(	O
cid:12	O
)	O
lter	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
48.1a	O
has	O
no	O
feedback	B
.	O
it	O
also	O
has	O
the	O
property	O
that	O
one	O
of	O
the	O
output	O
bits	O
,	O
t	O
(	O
a	O
)	O
,	O
is	O
identical	O
to	O
the	O
source	O
bit	O
s.	O
this	O
encoder	B
is	O
thus	O
called	O
systematic	B
,	O
because	O
the	O
source	O
bits	O
are	O
reproduced	O
transparently	O
in	O
the	O
transmitted	O
stream	O
,	O
and	O
nonrecursive	O
,	O
because	O
it	O
has	O
no	O
feedback	B
.	O
the	O
other	O
transmitted	O
bit	B
t	O
(	O
b	O
)	O
is	O
a	O
linear	B
function	O
of	O
the	O
state	O
of	O
the	O
(	O
cid:12	O
)	O
lter	O
.	O
one	O
way	O
of	O
describing	O
that	O
function	B
is	O
as	O
a	O
dot	O
product	O
(	O
modulo	O
2	O
)	O
between	O
two	O
binary	O
vectors	O
of	O
length	O
k	O
+	O
1	O
:	O
a	O
binary	O
vector	O
g	O
(	O
b	O
)	O
=	O
(	O
1	O
;	O
1	O
;	O
1	O
;	O
0	O
;	O
1	O
;	O
0	O
;	O
1	O
;	O
1	O
)	O
and	O
the	O
state	O
vector	O
z	O
=	O
(	O
zk	O
;	O
zk	O
(	O
cid:0	O
)	O
1	O
;	O
:	O
:	O
:	O
;	O
z1	O
;	O
z0	O
)	O
.	O
we	O
include	O
in	O
the	O
state	O
vector	O
the	O
bit	B
z0	O
that	O
will	O
be	O
put	O
into	O
the	O
(	O
cid:12	O
)	O
rst	O
bit	B
of	O
the	O
memory	B
on	O
the	O
next	O
cycle	O
.	O
the	O
vector	O
g	O
(	O
b	O
)	O
has	O
g	O
(	O
b	O
)	O
(	O
cid:20	O
)	O
=	O
1	O
for	O
every	O
(	O
cid:20	O
)	O
where	O
there	O
is	O
a	O
tap	B
(	O
a	O
downward	O
pointing	O
arrow	O
)	O
from	O
state	O
bit	B
z	O
(	O
cid:20	O
)	O
into	O
the	O
transmitted	O
bit	B
t	O
(	O
b	O
)	O
.	O
a	O
convenient	O
way	O
to	O
describe	O
these	O
binary	O
tap	O
vectors	B
is	O
in	O
octal	O
.	O
thus	O
,	O
i	O
have	O
drawn	O
the	O
delay	O
lines	O
from	O
this	O
(	O
cid:12	O
)	O
lter	O
makes	O
use	O
of	O
the	O
tap	O
vector	O
3538.	O
right	O
to	O
left	O
to	O
make	O
it	O
easy	O
to	O
relate	O
the	O
diagrams	O
to	O
these	O
octal	B
numbers	O
.	O
nonsystematic	O
nonrecursive	B
the	O
(	O
cid:12	O
)	O
lter	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
48.1b	O
also	O
has	O
no	O
feedback	B
,	O
but	O
it	O
is	O
not	O
systematic	B
.	O
it	O
makes	O
use	O
of	O
two	O
tap	B
vectors	O
g	O
(	O
a	O
)	O
and	O
g	O
(	O
b	O
)	O
to	O
create	O
its	O
two	O
transmitted	O
bits	O
.	O
this	O
encoder	B
is	O
thus	O
nonsystematic	O
and	O
nonrecursive	O
.	O
because	O
of	O
their	O
added	O
complexity	B
,	O
nonsystematic	O
codes	O
can	O
have	O
error-correcting	O
abilities	O
superior	O
to	O
those	O
of	O
systematic	O
nonrecursive	B
codes	O
with	O
the	O
same	O
constraint	O
length	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
576	O
systematic	B
recursive	O
48	O
|	O
convolutional	B
codes	O
and	O
turbo	O
codes	O
the	O
(	O
cid:12	O
)	O
lter	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
48.1c	O
is	O
similar	O
to	O
the	O
nonsystematic	O
nonrecursive	B
(	O
cid:12	O
)	O
lter	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
48.1b	O
,	O
but	O
it	O
uses	O
the	O
taps	O
that	O
formerly	O
made	O
up	O
g	O
(	O
a	O
)	O
to	O
make	O
a	O
linear	B
signal	O
that	O
is	O
fed	O
back	O
into	O
the	O
shift	O
register	O
along	O
with	O
the	O
source	O
bit	O
.	O
the	O
output	O
t	O
(	O
b	O
)	O
is	O
a	O
linear	B
function	O
of	O
the	O
state	O
vector	O
as	O
before	O
.	O
the	O
other	O
output	O
is	O
t	O
(	O
a	O
)	O
=	O
s	O
,	O
so	O
this	O
(	O
cid:12	O
)	O
lter	O
is	O
systematic	B
.	O
a	O
recursive	O
code	B
is	O
conventionally	O
identi	O
(	O
cid:12	O
)	O
ed	O
by	O
an	O
octal	B
ratio	O
,	O
e.g.	O
,	O
(	O
cid:12	O
)	O
g-	O
ure	O
48.1c	O
’	O
s	O
code	B
is	O
denoted	O
by	O
(	O
247=371	O
)	O
8.	O
equivalence	B
of	O
systematic	B
recursive	O
and	O
nonsystematic	O
nonrecursive	B
codes	O
the	O
two	O
(	O
cid:12	O
)	O
lters	O
in	O
(	O
cid:12	O
)	O
gure	O
48.1b	O
,	O
c	O
are	O
code-equivalent	B
in	O
that	O
the	O
sets	O
of	O
code-	O
words	O
that	O
they	O
de	O
(	O
cid:12	O
)	O
ne	O
are	O
identical	O
.	O
for	O
every	O
codeword	B
of	O
the	O
nonsystematic	O
nonrecursive	B
code	O
we	O
can	O
choose	O
a	O
source	O
stream	O
for	O
the	O
other	O
encoder	B
such	O
that	O
its	O
output	O
is	O
identical	O
(	O
and	O
vice	O
versa	O
)	O
.	O
(	O
cid:20	O
)	O
=1	O
g	O
(	O
a	O
)	O
to	O
prove	O
this	O
,	O
we	O
denote	O
by	O
p	O
the	O
quantity	O
pk	O
(	O
cid:20	O
)	O
z	O
(	O
cid:20	O
)	O
,	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
g-	O
ure	O
48.3a	O
and	O
b	O
,	O
which	O
shows	O
a	O
pair	O
of	O
smaller	O
but	O
otherwise	O
equivalent	O
(	O
cid:12	O
)	O
lters	O
.	O
if	O
the	O
two	O
transmissions	O
are	O
to	O
be	O
equivalent	O
{	O
that	O
is	O
,	O
the	O
t	O
(	O
a	O
)	O
s	O
are	O
equal	O
in	O
both	O
(	O
cid:12	O
)	O
gures	O
and	O
so	O
are	O
the	O
t	O
(	O
b	O
)	O
s	O
{	O
then	O
on	O
every	O
cycle	O
the	O
source	O
bit	O
in	O
the	O
systematic	B
code	O
must	O
be	O
s	O
=	O
t	O
(	O
a	O
)	O
.	O
so	O
now	O
we	O
must	O
simply	O
con	O
(	O
cid:12	O
)	O
rm	O
that	O
for	O
this	O
choice	O
of	O
s	O
,	O
the	O
systematic	B
code	O
’	O
s	O
shift	O
register	O
will	O
follow	O
the	O
same	O
state	O
sequence	O
as	O
that	O
of	O
the	O
nonsystematic	O
code	B
,	O
assuming	O
that	O
the	O
states	O
match	O
initially	O
.	O
in	O
(	O
cid:12	O
)	O
gure	O
48.3a	O
we	O
have	O
-	O
t	O
(	O
b	O
)	O
-	O
(	O
cid:8	O
)	O
6	O
hd	O
(	O
cid:27	O
)	O
hdz2	O
z1	O
z0	O
?	O
-	O
(	O
cid:8	O
)	O
?	O
--	O
p	O
(	O
a	O
)	O
(	O
5	O
;	O
7	O
)	O
8	O
(	O
cid:8	O
)	O
s	O
t	O
(	O
a	O
)	O
-	O
(	O
cid:8	O
)	O
6	O
-	O
t	O
(	O
b	O
)	O
hdz2	O
z1	O
hd	O
?	O
--	O
p	O
z0	O
‘	O
-	O
6	O
(	O
cid:27	O
)	O
(	O
cid:8	O
)	O
7	O
(	O
cid:1	O
)	O
8	O
(	O
b	O
)	O
(	O
cid:0	O
)	O
1	O
;	O
5	O
(	O
cid:8	O
)	O
t	O
(	O
a	O
)	O
s	O
figure	O
48.3.	O
two	O
rate-1/2	O
convolutional	B
codes	O
with	O
constraint	O
length	B
k	O
=	O
2	O
:	O
(	O
a	O
)	O
non-recursive	O
;	O
(	O
b	O
)	O
recursive	O
.	O
the	O
two	O
codes	O
are	O
equivalent	O
.	O
t	O
(	O
a	O
)	O
=	O
p	O
(	O
cid:8	O
)	O
znonrecursive	O
0	O
whereas	O
in	O
(	O
cid:12	O
)	O
gure	O
48.3b	O
we	O
have	O
zrecursive	O
0	O
=	O
t	O
(	O
a	O
)	O
(	O
cid:8	O
)	O
p	O
:	O
substituting	O
for	O
t	O
(	O
a	O
)	O
,	O
and	O
using	O
p	O
(	O
cid:8	O
)	O
p	O
=	O
0	O
we	O
immediately	O
(	O
cid:12	O
)	O
nd	O
zrecursive	O
0	O
=	O
znonrecursive	O
0	O
:	O
(	O
48.1	O
)	O
(	O
48.2	O
)	O
(	O
48.3	O
)	O
thus	O
,	O
any	O
codeword	B
of	O
a	O
nonsystematic	O
nonrecursive	B
code	O
is	O
a	O
codeword	B
of	O
a	O
systematic	B
recursive	O
code	B
with	O
the	O
same	O
taps	O
{	O
the	O
same	O
taps	O
in	O
the	O
sense	O
that	O
there	O
are	O
vertical	O
arrows	O
in	O
all	O
the	O
same	O
places	O
in	O
(	O
cid:12	O
)	O
gures	O
48.3	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
,	O
though	O
one	O
of	O
the	O
arrows	O
points	O
up	O
instead	O
of	O
down	O
in	O
(	O
b	O
)	O
.	O
now	O
,	O
while	O
these	O
two	O
codes	O
are	O
equivalent	O
,	O
the	O
two	O
encoders	O
behave	O
dif-	O
ferently	O
.	O
the	O
nonrecursive	B
encoder	O
has	O
a	O
(	O
cid:12	O
)	O
nite	O
impulse	O
response	O
,	O
that	O
is	O
,	O
if	O
one	O
puts	O
in	O
a	O
string	O
that	O
is	O
all	O
zeroes	O
except	O
for	O
a	O
single	O
one	O
,	O
the	O
resulting	O
output	O
stream	O
contains	O
a	O
(	O
cid:12	O
)	O
nite	O
number	O
of	O
ones	O
.	O
once	O
the	O
one	O
bit	B
has	O
passed	O
through	O
all	O
the	O
states	O
of	O
the	O
memory	O
,	O
the	O
delay	B
line	I
returns	O
to	O
the	O
all-zero	O
state	O
.	O
figure	O
48.4a	O
shows	O
the	O
state	O
sequence	O
resulting	O
from	O
the	O
source	O
string	O
s	O
=	O
(	O
0	O
,	O
0	O
,	O
1	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
)	O
.	O
figure	O
48.4b	O
shows	O
the	O
trellis	B
of	O
the	O
recursive	O
code	B
of	O
(	O
cid:12	O
)	O
gure	O
48.3b	O
and	O
the	O
response	O
of	O
this	O
(	O
cid:12	O
)	O
lter	O
to	O
the	O
same	O
source	O
string	O
s	O
=	O
(	O
0	O
,	O
0	O
,	O
1	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
,	O
0	O
)	O
.	O
the	O
(	O
cid:12	O
)	O
lter	O
has	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
impulse	O
response	O
.	O
the	O
response	O
settles	O
into	O
a	O
periodic	O
state	O
with	O
period	O
equal	O
to	O
three	O
clock	O
cycles	O
.	O
.	O
exercise	O
48.1	O
.	O
[	O
1	O
]	O
what	O
is	O
the	O
input	O
to	O
the	O
recursive	O
(	O
cid:12	O
)	O
lter	O
such	O
that	O
its	O
state	O
sequence	O
and	O
the	O
transmission	O
are	O
the	O
same	O
as	O
those	O
of	O
the	O
nonrecursive	O
(	O
cid:12	O
)	O
lter	O
?	O
(	O
hint	O
:	O
see	O
(	O
cid:12	O
)	O
gure	O
48.5	O
.	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
48.2	O
:	O
linear-feedback	O
shift-registers	O
577	O
(	O
a	O
)	O
(	O
b	O
)	O
11	O
10	O
01	O
00	O
transmit	O
0	O
0	O
source	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
11	O
10	O
01	O
00	O
transmit	O
0	O
0	O
source	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
0	O
0	O
1	O
11	O
10	O
01	O
00	O
transmit	O
0	O
0	O
source	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
0	O
1	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
figure	O
48.4.	O
trellises	O
of	O
the	O
rate-1/2	O
convolutional	B
codes	O
of	O
(	O
cid:12	O
)	O
gure	O
48.3.	O
it	O
is	O
assumed	O
that	O
the	O
initial	O
state	O
of	O
the	O
(	O
cid:12	O
)	O
lter	O
is	O
(	O
z2	O
;	O
z1	O
)	O
=	O
(	O
0	O
;	O
0	O
)	O
.	O
time	O
is	O
on	O
the	O
horizontal	O
axis	O
and	O
the	O
state	O
of	O
the	O
(	O
cid:12	O
)	O
lter	O
at	O
each	O
time	O
step	O
is	O
the	O
vertical	O
coordinate	O
.	O
on	O
the	O
line	O
segments	O
are	O
shown	O
the	O
emitted	O
symbols	O
t	O
(	O
a	O
)	O
and	O
t	O
(	O
b	O
)	O
,	O
with	O
stars	O
for	O
‘	O
1	O
’	O
and	O
boxes	O
for	O
‘	O
0	O
’	O
.	O
the	O
paths	O
taken	O
through	O
the	O
trellises	O
when	O
the	O
source	O
sequence	O
is	O
00100000	O
are	O
highlighted	O
with	O
a	O
solid	O
line	O
.	O
the	O
light	O
dotted	O
lines	O
show	O
the	O
state	O
trajectories	O
that	O
are	O
possible	O
for	O
other	O
source	O
sequences	O
.	O
figure	O
48.5.	O
the	O
source	O
sequence	O
for	O
the	O
systematic	B
recursive	O
code	B
00111000	O
produces	O
the	O
same	O
path	O
through	O
the	O
trellis	B
as	O
00100000	O
does	O
in	O
the	O
nonsystematic	O
nonrecursive	B
case	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
578	O
48	O
|	O
convolutional	B
codes	O
and	O
turbo	O
codes	O
-	O
hdz4	O
hdz3	O
?	O
--	O
(	O
cid:8	O
)	O
hdz2	O
?	O
-	O
(	O
cid:8	O
)	O
z1	O
hd	O
?	O
-	O
(	O
cid:8	O
)	O
(	O
cid:8	O
)	O
6	O
z0	O
6	O
(	O
cid:27	O
)	O
(	O
cid:8	O
)	O
-	O
t	O
(	O
b	O
)	O
t	O
(	O
a	O
)	O
s	O
‘	O
-	O
(	O
cid:0	O
)	O
1	O
;	O
21	O
37	O
(	O
cid:1	O
)	O
8	O
figure	O
48.6.	O
the	O
trellis	B
for	O
a	O
k	O
=	O
4	O
code	B
painted	O
with	O
the	O
likelihood	B
function	O
when	O
the	O
received	O
vector	O
is	O
equal	O
to	O
a	O
codeword	B
with	O
just	O
one	O
bit	B
(	O
cid:13	O
)	O
ipped	O
.	O
there	O
are	O
three	O
line	O
styles	O
,	O
depending	O
on	O
the	O
value	O
of	O
the	O
likelihood	O
:	O
thick	O
solid	O
lines	O
show	O
the	O
edges	O
in	O
the	O
trellis	B
that	O
match	O
the	O
corresponding	O
two	O
bits	O
of	O
the	O
received	O
string	O
exactly	O
;	O
thick	O
dotted	O
lines	O
show	O
edges	O
that	O
match	O
one	O
bit	B
but	O
mismatch	O
the	O
other	O
;	O
and	O
thin	O
dotted	O
lines	O
show	O
the	O
edges	O
that	O
mismatch	O
both	O
bits	O
.	O
0	O
1	O
1	O
0	O
1	O
0	O
1	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
1111	O
1110	O
1101	O
1100	O
1011	O
1010	O
1001	O
1000	O
0111	O
0110	O
0101	O
0100	O
0011	O
0010	O
0001	O
0000	O
received	O
in	O
general	O
a	O
linear-feedback	B
shift-register	I
with	O
k	O
bits	O
of	O
memory	B
has	O
an	O
impulse	O
response	O
that	O
is	O
periodic	O
with	O
a	O
period	O
that	O
is	O
at	O
most	O
2k	O
(	O
cid:0	O
)	O
1	O
,	O
corresponding	O
to	O
the	O
(	O
cid:12	O
)	O
lter	O
visiting	O
every	O
non-zero	O
state	O
in	O
its	O
state	O
space	O
.	O
incidentally	O
,	O
cheap	O
pseudorandom	O
number	O
generators	O
and	O
cheap	O
crypto-	O
graphic	O
products	O
make	O
use	O
of	O
exactly	O
these	O
periodic	O
sequences	O
,	O
though	O
with	O
larger	O
values	O
of	O
k	O
than	O
7	O
;	O
the	O
random	O
number	O
seed	O
or	O
cryptographic	O
key	O
se-	O
lects	O
the	O
initial	O
state	O
of	O
the	O
memory	B
.	O
there	O
is	O
thus	O
a	O
close	O
connection	B
between	I
certain	O
cryptanalysis	B
problems	O
and	O
the	O
decoding	B
of	O
convolutional	B
codes	O
.	O
48.3	O
decoding	B
convolutional	O
codes	O
the	O
receiver	O
receives	O
a	O
bit	B
stream	O
,	O
and	O
wishes	O
to	O
infer	O
the	O
state	O
sequence	O
and	O
thence	O
the	O
source	O
stream	O
.	O
the	O
posterior	B
probability	I
of	O
each	O
bit	B
can	O
be	O
found	O
by	O
the	O
sum	O
{	O
product	O
algorithm	O
(	O
also	O
known	O
as	O
the	O
forward	O
{	O
backward	O
or	O
bcjr	O
algorithm	B
)	O
,	O
which	O
was	O
introduced	O
in	O
section	O
25.3.	O
the	O
most	O
probable	O
state	O
sequence	B
can	O
be	O
found	O
using	O
the	O
min	O
{	O
sum	O
algorithm	O
of	O
section	O
25.3	O
(	O
also	O
known	O
as	O
the	O
viterbi	O
algorithm	B
)	O
.	O
the	O
nature	O
of	O
this	O
task	O
is	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gure	O
48.6	O
,	O
which	O
shows	O
the	O
cost	O
associated	O
with	O
each	O
edge	B
in	O
the	O
trellis	B
for	O
the	O
case	O
of	O
a	O
sixteen-state	O
code	B
;	O
the	O
channel	B
is	O
assumed	O
to	O
be	O
a	O
binary	B
symmetric	I
channel	I
and	O
the	O
received	O
vector	O
is	O
equal	O
to	O
a	O
codeword	B
except	O
that	O
one	O
bit	B
has	O
been	O
(	O
cid:13	O
)	O
ipped	O
.	O
there	O
are	O
three	O
line	O
styles	O
,	O
depending	O
on	O
the	O
value	O
of	O
the	O
likelihood	O
:	O
thick	O
solid	O
lines	O
show	O
the	O
edges	O
in	O
the	O
trellis	B
that	O
match	O
the	O
corresponding	O
two	O
bits	O
of	O
the	O
received	O
string	O
exactly	O
;	O
thick	O
dotted	O
lines	O
show	O
edges	O
that	O
match	O
one	O
bit	B
but	O
mismatch	O
the	O
other	O
;	O
and	O
thin	O
dotted	O
lines	O
show	O
the	O
edges	O
that	O
mismatch	O
both	O
bits	O
.	O
the	O
min	O
{	O
sum	O
algorithm	O
seeks	O
the	O
path	O
through	O
the	O
trellis	B
that	O
uses	O
as	O
many	O
solid	O
lines	O
as	O
possible	O
;	O
more	O
precisely	O
,	O
it	O
minimizes	O
the	O
cost	O
of	O
the	O
path	O
,	O
where	O
the	O
cost	O
is	O
zero	O
for	O
a	O
solid	O
line	O
,	O
one	O
for	O
a	O
thick	O
dotted	O
line	O
,	O
and	O
two	O
for	O
a	O
thin	O
dotted	O
line	O
.	O
.	O
exercise	O
48.2	O
.	O
[	O
1	O
,	O
p.581	O
]	O
can	O
you	O
spot	O
the	O
most	O
probable	O
path	O
and	O
the	O
(	O
cid:13	O
)	O
ipped	O
bit	B
?	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
579	O
figure	O
48.7.	O
two	O
paths	O
that	O
di	O
(	O
cid:11	O
)	O
er	O
in	O
two	O
transmitted	O
bits	O
only	O
.	O
1	O
1	O
1	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
1	O
1	O
0	O
1	O
1	O
1	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
1	O
1	O
0	O
0	O
1	O
figure	O
48.8.	O
a	O
terminated	O
trellis	B
.	O
when	O
any	O
codeword	B
is	O
completed	O
,	O
the	O
(	O
cid:12	O
)	O
lter	O
state	O
is	O
0000	O
.	O
48.4	O
:	O
turbo	B
codes	I
1111	O
1110	O
1101	O
1100	O
1011	O
1010	O
1001	O
1000	O
0111	O
0110	O
0101	O
0100	O
0011	O
0010	O
0001	O
0000	O
transmit	O
1	O
source	O
1	O
1111	O
1110	O
1101	O
1100	O
1011	O
1010	O
1001	O
1000	O
0111	O
0110	O
0101	O
0100	O
0011	O
0010	O
0001	O
0000	O
transmit	O
1	O
source	O
1	O
1111	O
1110	O
1101	O
1100	O
1011	O
1010	O
1001	O
1000	O
0111	O
0110	O
0101	O
0100	O
0011	O
0010	O
0001	O
0000	O
unequal	O
protection	O
a	O
defect	O
of	O
the	O
convolutional	O
codes	O
presented	O
thus	O
far	O
is	O
that	O
they	O
o	O
(	O
cid:11	O
)	O
er	O
un-	O
equal	O
protection	O
to	O
the	O
source	O
bits	O
.	O
figure	O
48.7	O
shows	O
two	O
paths	O
through	O
the	O
trellis	B
that	O
di	O
(	O
cid:11	O
)	O
er	O
in	O
only	O
two	O
transmitted	O
bits	O
.	O
the	O
last	O
source	O
bit	O
is	O
less	O
well	O
protected	O
than	O
the	O
other	O
source	O
bits	O
.	O
this	O
unequal	O
protection	O
of	O
bits	O
motivates	O
the	O
termination	B
of	O
the	O
trellis	B
.	O
a	O
terminated	O
trellis	B
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
48.8.	O
termination	B
slightly	O
reduces	O
the	O
number	O
of	O
source	O
bits	O
used	O
per	O
codeword	B
.	O
here	O
,	O
four	O
source	O
bits	O
are	O
turned	O
into	O
parity	B
bits	O
because	O
the	O
k	O
=	O
4	O
memory	B
bits	O
must	O
be	O
returned	O
to	O
zero	O
.	O
48.4	O
turbo	B
codes	I
an	O
(	O
n	O
;	O
k	O
)	O
turbo	B
code	I
is	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
a	O
number	O
of	O
constituent	O
convolutional	B
encoders	O
(	O
often	O
,	O
two	O
)	O
and	O
an	O
equal	O
number	O
of	O
interleavers	O
which	O
are	O
k	O
(	O
cid:2	O
)	O
k	O
permutation	B
matrices	O
.	O
without	O
loss	O
of	O
generality	O
,	O
we	O
take	O
the	O
(	O
cid:12	O
)	O
rst	O
interleaver	O
to	O
be	O
the	O
identity	B
matrix	I
.	O
a	O
string	O
of	O
k	O
source	O
bits	O
is	O
encoded	O
by	O
feeding	O
them	O
into	O
each	O
constituent	O
encoder	B
in	O
the	O
order	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
the	O
associated	O
interleaver	O
,	O
and	O
transmitting	O
the	O
bits	O
that	O
come	O
out	O
of	O
each	O
constituent	O
encoder	B
.	O
often	O
the	O
(	O
cid:12	O
)	O
rst	O
constituent	O
encoder	B
is	O
chosen	O
to	O
be	O
a	O
systematic	B
encoder	O
,	O
just	O
like	O
the	O
recursive	O
(	O
cid:12	O
)	O
lter	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
48.6	O
,	O
and	O
the	O
second	O
is	O
a	O
non-systematic	O
one	O
of	O
rate	O
1	O
that	O
emits	O
parity	B
bits	O
only	O
.	O
the	O
transmitted	O
codeword	B
then	O
consists	O
of	O
-	O
c1	O
-	O
-	O
-	O
-	O
c2	O
(	O
cid:14	O
)	O
(	O
cid:13	O
)	O
(	O
cid:15	O
)	O
(	O
cid:12	O
)	O
(	O
cid:25	O
)	O
figure	O
48.10.	O
the	O
encoder	B
of	O
a	O
turbo	B
code	I
.	O
each	O
box	B
c1	O
,	O
c2	O
,	O
contains	O
a	O
convolutional	B
code	I
.	O
the	O
source	O
bits	O
are	O
reordered	O
using	O
a	O
permutation	B
(	O
cid:25	O
)	O
before	O
they	O
are	O
fed	O
to	O
c2	O
.	O
the	O
transmitted	O
codeword	B
is	O
obtained	O
by	O
concatenating	O
or	O
interleaving	B
the	O
outputs	O
of	O
the	O
two	O
convolutional	B
codes	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
580	O
48	O
|	O
convolutional	B
codes	O
and	O
turbo	O
codes	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
48.9.	O
rate-1/3	O
(	O
a	O
)	O
and	O
rate-1/2	O
(	O
b	O
)	O
turbo	B
codes	I
represented	O
as	O
factor	O
graphs	O
.	O
the	O
circles	O
represent	O
the	O
codeword	B
bits	O
.	O
the	O
two	O
rectangles	O
represent	O
trellises	O
of	O
rate-1/2	O
convolutional	B
codes	O
,	O
with	O
the	O
systematic	B
bits	O
occupying	O
the	O
left	O
half	O
of	O
the	O
rectangle	O
and	O
the	O
parity	B
bits	O
occupying	O
the	O
right	O
half	O
.	O
the	O
puncturing	B
of	O
these	O
constituent	O
codes	O
in	O
the	O
rate-1/2	O
turbo	B
code	I
is	O
represented	O
by	O
the	O
lack	O
of	O
connections	O
to	O
half	O
of	O
the	O
parity	O
bits	O
in	O
each	O
trellis	B
.	O
k	O
source	O
bits	O
followed	O
by	O
m1	O
parity	B
bits	O
generated	O
by	O
the	O
(	O
cid:12	O
)	O
rst	O
convolutional	B
code	I
and	O
m2	O
parity	B
bits	O
from	O
the	O
second	O
.	O
the	O
resulting	O
turbo	B
code	I
has	O
rate	B
1/3	O
.	O
the	O
turbo	B
code	I
can	O
be	O
represented	O
by	O
a	O
factor	B
graph	I
in	O
which	O
the	O
two	O
trellises	O
are	O
represented	O
by	O
two	O
large	O
rectangular	B
nodes	O
(	O
(	O
cid:12	O
)	O
gure	O
48.9a	O
)	O
;	O
the	O
k	O
source	O
bits	O
and	O
the	O
(	O
cid:12	O
)	O
rst	O
m1	O
parity	B
bits	O
participate	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
trellis	B
and	O
the	O
k	O
source	O
bits	O
and	O
the	O
last	O
m2	O
parity	B
bits	O
participate	O
in	O
the	O
second	O
trellis	B
.	O
each	O
codeword	B
bit	O
participates	O
in	O
either	O
one	O
or	O
two	O
trellises	O
,	O
depending	O
on	O
whether	O
it	O
is	O
a	O
parity	B
bit	O
or	O
a	O
source	O
bit	O
.	O
each	O
trellis	B
node	O
contains	O
a	O
trellis	B
exactly	O
like	O
the	O
terminated	O
trellis	B
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
48.8	O
,	O
except	O
one	O
thousand	O
times	O
as	O
long	O
.	O
[	O
there	O
are	O
other	O
factor	B
graph	I
representations	O
for	O
turbo	O
codes	O
that	O
make	O
use	O
of	O
more	O
elementary	O
nodes	O
,	O
but	O
the	O
factor	B
graph	I
given	O
here	O
yields	O
the	O
standard	O
version	O
of	O
the	O
sum	O
{	O
product	O
algorithm	O
used	O
for	O
turbo	O
codes	O
.	O
]	O
if	O
a	O
turbo	B
code	I
of	O
smaller	O
rate	B
such	O
as	O
1/2	O
is	O
required	O
,	O
a	O
standard	O
modi	O
(	O
cid:12	O
)	O
ca-	O
tion	O
to	O
the	O
rate-1/3	O
code	B
is	O
to	O
puncture	O
some	O
of	O
the	O
parity	O
bits	O
(	O
(	O
cid:12	O
)	O
gure	O
48.9b	O
)	O
.	O
turbo	B
codes	I
are	O
decoded	O
using	O
the	O
sum	O
{	O
product	O
algorithm	O
described	O
in	O
chapter	O
26.	O
on	O
the	O
(	O
cid:12	O
)	O
rst	O
iteration	O
,	O
each	O
trellis	B
receives	O
the	O
channel	B
likelihoods	O
,	O
and	O
runs	O
the	O
forward	O
{	O
backward	O
algorithm	O
to	O
compute	O
,	O
for	O
each	O
bit	B
,	O
the	O
relative	B
likelihood	O
of	O
its	O
being	O
1	O
or	O
0	O
,	O
given	O
the	O
information	B
about	O
the	O
other	O
bits	O
.	O
these	O
likelihoods	O
are	O
then	O
passed	O
across	O
from	O
each	O
trellis	B
to	O
the	O
other	O
,	O
and	O
multiplied	O
by	O
the	O
channel	B
likelihoods	O
on	O
the	O
way	O
.	O
we	O
are	O
then	O
ready	O
for	O
the	O
second	O
iteration	O
:	O
the	O
forward	O
{	O
backward	O
algorithm	O
is	O
run	O
again	O
in	O
each	O
trellis	B
using	O
the	O
updated	O
probabilities	O
.	O
after	O
about	O
ten	O
or	O
twenty	O
such	O
iterations	O
,	O
it	O
’	O
s	O
hoped	O
that	O
the	O
correct	O
decoding	B
will	O
be	O
found	O
.	O
it	O
is	O
common	O
practice	O
to	O
stop	O
after	O
some	O
(	O
cid:12	O
)	O
xed	O
number	O
of	O
iterations	O
,	O
but	O
we	O
can	O
do	O
better	O
.	O
as	O
a	O
stopping	O
criterion	O
,	O
the	O
following	O
procedure	O
can	O
be	O
used	O
at	O
every	O
iter-	O
ation	O
.	O
for	O
each	O
time-step	O
in	O
each	O
trellis	B
,	O
we	O
identify	O
the	O
most	O
probable	O
edge	O
,	O
according	O
to	O
the	O
local	O
messages	O
.	O
if	O
these	O
most	O
probable	O
edges	O
join	O
up	O
into	O
two	O
valid	O
paths	O
,	O
one	O
in	O
each	O
trellis	B
,	O
and	O
if	O
these	O
two	O
paths	O
are	O
consistent	O
with	O
each	O
other	O
,	O
it	O
is	O
reasonable	O
to	O
stop	O
,	O
as	O
subsequent	O
iterations	O
are	O
unlikely	O
to	O
take	O
the	O
decoder	B
away	O
from	O
this	O
codeword	B
.	O
if	O
a	O
maximum	O
number	O
of	O
iterations	O
is	O
reached	O
without	O
this	O
stopping	O
criterion	O
being	O
satis	O
(	O
cid:12	O
)	O
ed	O
,	O
a	O
decoding	B
error	O
can	O
be	O
reported	O
.	O
this	O
stopping	O
procedure	O
is	O
recommended	O
for	O
several	O
reasons	O
:	O
it	O
allows	O
a	O
big	O
saving	O
in	O
decoding	O
time	O
with	O
no	O
loss	O
in	O
error	B
probability	I
;	O
it	O
allows	O
decoding	B
failures	O
that	O
are	O
detected	O
by	O
the	O
decoder	B
to	O
be	O
so	O
identi	O
(	O
cid:12	O
)	O
ed	O
{	O
knowing	O
that	O
a	O
particular	O
block	B
is	O
de	O
(	O
cid:12	O
)	O
nitely	O
corrupted	O
is	O
surely	O
useful	O
information	B
for	O
the	O
receiver	O
!	O
and	O
when	O
we	O
distinguish	O
between	O
detected	O
and	O
undetected	O
er-	O
rors	O
,	O
the	O
undetected	O
errors	B
give	O
helpful	O
insights	O
into	O
the	O
low-weight	O
codewords	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
48.5	O
:	O
parity-check	O
matrices	O
of	O
convolutional	O
codes	O
and	O
turbo	O
codes	O
581	O
of	O
the	O
code	O
,	O
which	O
may	O
improve	O
the	O
process	O
of	B
code	I
design	O
.	O
turbo	B
codes	I
as	O
described	O
here	O
have	O
excellent	O
performance	O
down	O
to	O
decoded	O
error	O
probabilities	O
of	O
about	O
10	O
(	O
cid:0	O
)	O
5	O
,	O
but	O
randomly-constructed	O
turbo	B
codes	I
tend	O
to	O
have	O
an	O
error	O
(	O
cid:13	O
)	O
oor	O
starting	O
at	O
that	O
level	O
.	O
this	O
error	O
(	O
cid:13	O
)	O
oor	O
is	O
caused	O
by	O
low-	O
weight	B
codewords	O
.	O
to	O
reduce	O
the	O
height	O
of	O
the	O
error	O
(	O
cid:13	O
)	O
oor	O
,	O
one	O
can	O
attempt	O
to	O
modify	O
the	O
random	B
construction	O
to	O
increase	O
the	O
weight	B
of	O
these	O
low-weight	O
codewords	O
.	O
the	O
tweaking	O
of	O
turbo	O
codes	O
is	O
a	O
black	B
art	O
,	O
and	O
it	O
never	O
succeeds	O
in	O
totalling	O
eliminating	O
low-weight	O
codewords	O
;	O
more	O
precisely	O
,	O
the	O
low-weight	O
codewords	O
can	O
be	O
eliminated	O
only	O
by	O
sacri	O
(	O
cid:12	O
)	O
cing	O
the	O
turbo	B
code	I
’	O
s	O
excellent	O
per-	O
formance	O
.	O
in	O
contrast	O
,	O
low-density	B
parity-check	I
codes	O
rarely	O
have	O
error	O
(	O
cid:13	O
)	O
oors	O
,	O
as	O
long	O
as	O
their	O
number	O
of	O
weight	O
{	O
2	O
columns	O
is	O
not	O
too	O
large	O
(	O
cf	O
.	O
exercise	O
47.3	O
,	O
p.572	O
)	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
48.11.	O
schematic	O
pictures	O
of	O
the	O
parity-check	O
matrices	B
of	O
(	O
a	O
)	O
a	O
convolutional	B
code	I
,	O
rate	B
1/2	O
,	O
and	O
(	O
b	O
)	O
a	O
turbo	B
code	I
,	O
rate	B
1/3	O
.	O
notation	B
:	O
a	O
diagonal	O
line	O
represents	O
an	O
identity	B
matrix	I
.	O
a	O
band	O
of	O
diagonal	O
lines	O
represent	O
a	O
band	O
of	O
diagonal	O
1s	O
.	O
a	O
circle	B
inside	O
a	O
square	B
represents	O
the	O
random	B
permutation	O
of	O
all	O
the	O
columns	O
in	O
that	O
square	B
.	O
a	O
number	O
inside	O
a	O
square	B
represents	O
the	O
number	O
of	O
random	O
permutation	B
matrices	O
superposed	O
in	O
that	O
square	B
.	O
horizontal	O
and	O
vertical	O
lines	O
indicate	O
the	O
boundaries	O
of	O
the	O
blocks	O
within	O
the	O
matrix	B
.	O
48.5	O
parity-check	O
matrices	O
of	O
convolutional	O
codes	O
and	O
turbo	O
codes	O
we	O
close	O
by	O
discussing	O
the	O
parity-check	B
matrix	I
of	O
a	O
rate-1/2	O
convolutional	B
code	I
viewed	O
as	O
a	O
linear	B
block	I
code	I
.	O
we	O
adopt	O
the	O
convention	O
that	O
the	O
n	O
bits	O
of	O
one	O
block	B
are	O
made	O
up	O
of	O
the	O
n=2	O
bits	O
t	O
(	O
a	O
)	O
followed	O
by	O
the	O
n=2	O
bits	O
t	O
(	O
b	O
)	O
.	O
.	O
exercise	O
48.3	O
.	O
[	O
2	O
]	O
prove	O
that	O
a	O
convolutional	B
code	I
has	O
a	O
low-density	O
parity-	O
check	O
matrix	B
as	O
shown	O
schematically	O
in	O
(	O
cid:12	O
)	O
gure	O
48.11a	O
.	O
hint	O
:	O
it	O
’	O
s	O
easiest	O
to	O
(	O
cid:12	O
)	O
gure	O
out	O
the	O
parity	B
constraints	O
satis	O
(	O
cid:12	O
)	O
ed	O
by	O
a	O
convo-	O
lutional	O
code	B
by	O
thinking	O
about	O
the	O
nonsystematic	O
nonrecursive	B
encoder	O
(	O
(	O
cid:12	O
)	O
gure	O
48.1b	O
)	O
.	O
consider	O
putting	O
through	O
(	O
cid:12	O
)	O
lter	O
a	O
a	O
stream	O
that	O
’	O
s	O
been	O
through	O
convolutional	B
(	O
cid:12	O
)	O
lter	O
b	O
,	O
and	O
vice	O
versa	O
;	O
compare	O
the	O
two	O
resulting	O
streams	O
.	O
ignore	O
termination	B
of	O
the	O
trellises	O
.	O
the	O
parity-check	B
matrix	I
of	O
a	O
turbo	B
code	I
can	O
be	O
written	O
down	O
by	O
listing	O
the	O
constraints	O
satis	O
(	O
cid:12	O
)	O
ed	O
by	O
the	O
two	O
constituent	O
trellises	O
(	O
(	O
cid:12	O
)	O
gure	O
48.11b	O
)	O
.	O
so	O
turbo	B
codes	I
are	O
also	O
special	O
cases	O
of	O
low-density	O
parity-check	O
codes	O
.	O
if	O
a	O
turbo	B
code	I
is	O
punctured	O
,	O
it	O
no	O
longer	O
necessarily	O
has	O
a	O
low-density	B
parity-check	I
matrix	O
,	O
but	O
it	O
always	O
has	O
a	O
generalized	B
parity-check	I
matrix	I
that	O
is	O
sparse	O
,	O
as	O
explained	O
in	O
the	O
next	O
chapter	O
.	O
further	O
reading	O
for	O
further	O
reading	O
about	O
convolutional	B
codes	O
,	O
johannesson	O
and	O
zigangirov	O
(	O
1999	O
)	O
is	O
highly	O
recommended	O
.	O
one	O
topic	O
i	O
would	O
have	O
liked	O
to	O
include	O
is	O
sequential	B
decoding	I
.	O
sequential	B
decoding	I
explores	O
only	O
the	O
most	O
promising	O
paths	O
in	O
the	O
trellis	B
,	O
and	O
backtracks	O
when	O
evidence	B
accumulates	O
that	O
a	O
wrong	O
turning	O
has	O
been	O
taken	O
.	O
sequential	B
decoding	I
is	O
used	O
when	O
the	O
trellis	B
is	O
too	O
big	O
for	O
us	O
to	O
be	O
able	O
to	O
apply	O
the	O
maximum	B
likelihood	I
algorithm	O
,	O
the	O
min	O
{	O
sum	O
algorithm	O
.	O
you	O
can	O
read	O
about	O
sequential	B
decoding	I
in	O
johannesson	O
and	O
zigangirov	O
(	O
1999	O
)	O
.	O
for	O
further	O
information	B
about	O
the	O
use	O
of	O
the	O
sum	O
{	O
product	O
algorithm	O
in	O
turbo	O
codes	O
,	O
and	O
the	O
rarely-used	O
but	O
highly	O
recommended	O
stopping	O
criteria	O
for	O
halting	O
their	O
decoding	B
,	O
frey	O
(	O
1998	O
)	O
is	O
essential	O
reading	O
.	O
(	O
and	O
there	O
’	O
s	O
lots	O
more	O
good	B
stu	O
(	O
cid:11	O
)	O
in	O
the	O
same	O
book	O
!	O
)	O
48.6	O
solutions	O
solution	O
to	O
exercise	O
48.2	O
(	O
p.578	O
)	O
.	O
the	O
(	O
cid:12	O
)	O
rst	O
bit	B
was	O
(	O
cid:13	O
)	O
ipped	O
.	O
the	O
most	O
probable	O
path	O
is	O
the	O
upper	O
one	O
in	O
(	O
cid:12	O
)	O
gure	O
48.7	O
.	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
49	O
repeat	O
{	O
accumulate	O
codes	O
in	O
chapter	O
1	O
we	O
discussed	O
a	O
very	O
simple	O
and	O
not	O
very	O
e	O
(	O
cid:11	O
)	O
ective	O
method	B
for	O
communicating	O
over	O
a	O
noisy	B
channel	I
:	O
the	O
repetition	B
code	I
.	O
we	O
now	O
discuss	O
a	O
code	B
that	O
is	O
almost	O
as	O
simple	O
,	O
and	O
whose	O
performance	O
is	O
outstandingly	O
good	B
.	O
repeat	O
{	O
accumulate	O
codes	O
were	O
studied	O
by	O
divsalar	O
et	O
al	O
.	O
(	O
1998	O
)	O
for	O
theo-	O
retical	O
purposes	O
,	O
as	O
simple	O
turbo-like	O
codes	O
that	O
might	O
be	O
more	O
amenable	O
to	O
analysis	B
than	O
messy	O
turbo	B
codes	I
.	O
their	O
practical	B
performance	O
turned	O
out	O
to	O
be	O
just	O
as	O
good	O
as	O
other	O
sparse-graph	O
codes	O
.	O
49.1	O
the	O
encoder	B
1.	O
take	O
k	O
source	O
bits	O
.	O
s1s2s3	O
:	O
:	O
:	O
sk	O
2.	O
repeat	O
each	O
bit	B
three	O
times	O
,	O
giving	O
n	O
=	O
3k	O
bits	O
.	O
s1s1s1s2s2s2s3s3s3	O
:	O
:	O
:	O
sksksk	O
3.	O
permute	O
these	O
n	O
bits	O
using	O
a	O
random	B
permutation	O
(	O
a	O
(	O
cid:12	O
)	O
xed	O
random	B
permutation	O
{	O
the	O
same	O
one	O
for	O
every	O
codeword	B
)	O
.	O
call	O
the	O
permuted	O
string	O
u.	O
u1u2u3u4u5u6u7u8u9	O
:	O
:	O
:	O
un	O
4.	O
transmit	O
the	O
accumulated	O
sum	O
.	O
t1	O
=	O
u1	O
t2	O
=	O
t1	O
+	O
u2	O
(	O
mod	O
2	O
)	O
tn	O
=	O
tn	O
(	O
cid:0	O
)	O
1	O
+	O
un	O
(	O
mod	O
2	O
)	O
tn	O
=	O
tn	O
(	O
cid:0	O
)	O
1	O
+	O
un	O
(	O
mod	O
2	O
)	O
:	O
:	O
:	O
:	O
:	O
:	O
:	O
(	O
49.1	O
)	O
5.	O
that	O
’	O
s	O
it	O
!	O
49.2	O
graph	B
figure	O
49.1a	O
shows	O
the	O
graph	B
of	O
a	O
repeat	O
{	O
accumulate	O
code	B
,	O
using	O
four	O
types	O
of	O
node	O
:	O
equality	O
constraints	O
,	O
intermediate	O
binary	O
variables	O
(	O
black	B
circles	O
)	O
,	O
parity	B
constraints	O
,	O
and	O
the	O
transmitted	O
bits	O
(	O
white	B
circles	O
)	O
.	O
the	O
source	O
sets	O
the	O
values	O
of	O
the	O
black	O
bits	O
at	O
the	O
bottom	O
,	O
three	O
at	O
a	O
time	O
,	O
and	O
the	O
accumulator	B
computes	O
the	O
transmitted	O
bits	O
along	O
the	O
top	O
.	O
582	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
49.3	O
:	O
decoding	B
(	O
a	O
)	O
(	O
b	O
)	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
1	O
0	O
0	O
1	O
0	O
1	O
0.1	O
0.01	O
0.001	O
0.0001	O
1e-05	O
total	O
undetected	O
n=204	O
408	O
816	O
9999	O
3000	O
3	O
4	O
5	O
n=30000	O
1	O
2	O
583	O
figure	O
49.1.	O
factor	O
graphs	O
for	O
a	O
repeat	O
{	O
accumulate	O
code	B
with	O
rate	B
1/3	O
.	O
(	O
a	O
)	O
using	O
elementary	O
nodes	O
.	O
each	O
white	B
circle	O
represents	O
a	O
transmitted	O
bit	B
.	O
each	O
constraint	O
forces	O
the	O
sum	O
of	O
the	O
3	O
bits	O
to	O
which	O
it	O
is	O
connected	O
to	O
be	O
even	O
.	O
each	O
black	B
circle	O
represents	O
an	O
intermediate	O
binary	O
variable	O
.	O
each	O
variables	O
to	O
which	O
it	O
is	O
connected	O
to	O
be	O
equal	O
.	O
(	O
b	O
)	O
factor	B
graph	I
normally	O
used	O
for	O
decoding	O
.	O
the	O
top	O
rectangle	O
represents	O
the	O
trellis	B
of	O
the	O
accumulator	B
,	O
shown	O
in	O
the	O
inset	O
.	O
constraint	O
forces	O
the	O
three	O
figure	O
49.2.	O
performance	O
of	O
six	O
rate-1/3	O
repeat	O
{	O
accumulate	O
codes	O
on	O
the	O
gaussian	O
channel	B
.	O
the	O
blocklengths	O
range	O
from	O
n	O
=	O
204	O
to	O
n	O
=	O
30	O
000.	O
vertical	O
axis	O
:	O
block	B
error	O
probability	B
;	O
horizontal	O
axis	O
:	O
eb=n0	O
.	O
the	O
dotted	O
lines	O
show	O
the	O
frequency	B
of	O
undetected	O
errors	B
.	O
this	O
graph	B
is	O
a	O
factor	B
graph	I
for	O
the	O
prior	B
probability	O
over	O
codewords	O
,	O
with	O
the	O
circles	O
being	O
binary	O
variable	O
nodes	O
,	O
and	O
the	O
squares	O
representing	O
contributes	O
a	O
factor	O
of	O
the	O
form	O
two	O
types	O
of	O
factor	O
nodes	O
.	O
as	O
usual	O
,	O
each	O
contributes	O
a	O
factor	O
of	O
the	O
form	O
	O
[	O
x1	O
=	O
x2	O
=	O
x3	O
]	O
.	O
	O
[	O
p	O
x	O
=	O
0	O
mod	O
2	O
]	O
;	O
each	O
49.3	O
decoding	B
the	O
repeat	O
{	O
accumulate	O
code	B
is	O
normally	O
decoded	O
using	O
the	O
sum	O
{	O
product	O
algo-	O
rithm	O
on	O
the	O
factor	B
graph	I
depicted	O
in	O
(	O
cid:12	O
)	O
gure	O
49.1b	O
.	O
the	O
top	O
box	B
represents	O
the	O
trellis	B
of	O
the	O
accumulator	B
,	O
including	O
the	O
channel	B
likelihoods	O
.	O
in	O
the	O
(	O
cid:12	O
)	O
rst	O
half	O
of	O
each	O
iteration	O
,	O
the	O
top	O
trellis	B
receives	O
likelihoods	O
for	O
every	O
transition	B
in	O
the	O
trellis	B
,	O
and	O
runs	O
the	O
forward	O
{	O
backward	O
algorithm	O
so	O
as	O
to	O
produce	O
likelihoods	O
for	O
each	O
variable	O
node	O
.	O
in	O
the	O
second	O
half	O
of	O
the	O
iteration	O
,	O
these	O
likelihoods	O
nodes	O
to	O
produce	O
new	O
likelihood	B
messages	O
to	O
are	O
multiplied	O
together	O
at	O
the	O
send	O
back	O
to	O
the	O
trellis	B
.	O
as	O
with	O
gallager	O
codes	O
and	O
turbo	O
codes	O
,	O
the	O
stop-when-it	O
’	O
s-done	O
decoding	B
method	O
can	O
be	O
applied	O
,	O
so	O
it	O
is	O
possible	O
to	O
distinguish	O
between	O
undetected	O
errors	B
(	O
which	O
are	O
caused	O
by	O
low-weight	O
codewords	O
in	O
the	O
code	B
)	O
and	O
detected	O
errors	B
(	O
where	O
the	O
decoder	B
gets	O
stuck	O
and	O
knows	O
that	O
it	O
has	O
failed	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
valid	O
answer	O
)	O
.	O
figure	O
49.2	O
shows	O
the	O
performance	O
of	O
six	O
randomly-constructed	O
repeat	O
{	O
accumulate	O
codes	O
on	O
the	O
gaussian	O
channel	B
.	O
if	O
one	O
does	O
not	O
mind	O
the	O
error	O
(	O
cid:13	O
)	O
oor	O
which	O
kicks	O
in	O
at	O
about	O
a	O
block	B
error	O
probability	O
of	O
10	O
(	O
cid:0	O
)	O
4	O
,	O
the	O
performance	O
is	O
staggeringly	O
good	B
for	O
such	O
a	O
simple	O
code	O
(	O
cf	O
.	O
(	O
cid:12	O
)	O
gure	O
47.17	O
)	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
584	O
1	O
0.1	O
0.01	O
0.001	O
0.0001	O
total	O
detected	O
undetected	O
1e-05	O
0.3	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1	O
1.1	O
(	O
a	O
)	O
2000	O
1800	O
1600	O
1400	O
1200	O
1000	O
800	O
600	O
400	O
200	O
0	O
0	O
1000	O
100	O
10	O
1	O
10	O
49	O
|	O
repeat	O
{	O
accumulate	O
codes	O
3000	O
2500	O
2000	O
1500	O
1000	O
500	O
0	O
0	O
1000	O
100	O
10	O
1	O
10	O
60	O
40	O
20	O
(	O
ii.b	O
)	O
eb=n0	O
=	O
0:749	O
db	O
80	O
100	O
120	O
140	O
160	O
180	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
90100	O
(	O
iii.b	O
)	O
60	O
40	O
20	O
(	O
ii.c	O
)	O
eb=n0	O
=	O
0:846	O
db	O
80	O
100	O
120	O
140	O
160	O
180	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
90	O
100	O
(	O
iii.c	O
)	O
49.4	O
empirical	O
distribution	B
of	O
decoding	B
times	O
it	O
is	O
interesting	O
to	O
study	O
the	O
number	O
of	O
iterations	O
(	O
cid:28	O
)	O
of	O
the	O
sum	O
{	O
product	O
algo-	O
rithm	O
required	O
to	O
decode	O
a	O
sparse-graph	B
code	I
.	O
given	O
one	O
code	B
and	O
a	O
set	B
of	O
channel	B
conditions	O
,	O
the	O
decoding	B
time	O
varies	O
randomly	O
from	O
trial	O
to	O
trial	O
.	O
we	O
(	O
cid:12	O
)	O
nd	O
that	O
the	O
histogram	O
of	O
decoding	O
times	O
follows	O
a	O
power	B
law	I
,	O
p	O
(	O
(	O
cid:28	O
)	O
)	O
/	O
(	O
cid:28	O
)	O
(	O
cid:0	O
)	O
p	O
,	O
for	O
large	O
(	O
cid:28	O
)	O
.	O
the	O
power	O
p	O
depends	O
on	O
the	O
signal-to-noise	B
ratio	I
and	O
becomes	O
smaller	O
(	O
so	O
that	O
the	O
distribution	B
is	O
more	O
heavy-tailed	O
)	O
as	O
the	O
signal-to-noise	B
ratio	I
decreases	O
.	O
we	O
have	O
observed	O
power	O
laws	O
in	O
repeat	O
{	O
accumulate	O
codes	O
and	O
in	O
irregular	B
and	O
regular	B
gallager	O
codes	O
.	O
figures	O
49.3	O
(	O
ii	O
)	O
and	O
(	O
iii	O
)	O
show	O
the	O
distribution	B
of	O
decoding	B
times	O
of	O
a	O
repeat	O
{	O
accumulate	O
code	B
at	O
two	O
di	O
(	O
cid:11	O
)	O
erent	O
signal-to-noise	O
ratios	O
.	O
the	O
power	O
laws	O
extend	O
over	O
several	O
orders	O
of	O
magnitude	O
.	O
exercise	O
49.1	O
.	O
[	O
5	O
]	O
investigate	O
these	O
power	O
laws	O
.	O
does	O
density	B
evolution	I
predict	O
them	O
?	O
can	O
the	O
design	O
of	O
a	O
code	B
be	O
used	O
to	O
manipulate	O
the	O
power	B
law	I
in	O
a	O
useful	O
way	O
?	O
49.5	O
generalized	O
parity-check	O
matrices	O
figure	O
49.3.	O
histograms	O
of	O
number	O
of	O
iterations	O
to	O
(	O
cid:12	O
)	O
nd	O
a	O
valid	O
decoding	B
for	O
a	O
repeat	O
{	O
accumulate	O
code	B
with	O
source	O
block	O
length	B
k	O
=	O
10	O
000	O
and	O
transmitted	O
blocklength	O
n	O
=	O
30	O
000	O
.	O
(	O
a	O
)	O
block	B
error	O
probability	B
versus	O
signal-to-noise	B
ratio	I
for	O
the	O
ra	O
code	B
.	O
(	O
ii.b	O
)	O
histogram	O
for	O
x=	O
(	O
cid:27	O
)	O
=	O
0:89	O
,	O
eb=n0	O
=	O
0:749	O
db	O
.	O
(	O
ii.c	O
)	O
x=	O
(	O
cid:27	O
)	O
=	O
0:90	O
,	O
eb=n0	O
=	O
0:846	O
db	O
.	O
(	O
iii.b	O
,	O
iii.c	O
)	O
fits	O
of	O
power	O
laws	O
to	O
(	O
ii.b	O
)	O
(	O
1=	O
(	O
cid:28	O
)	O
6	O
)	O
and	O
(	O
ii.c	O
)	O
(	O
1=	O
(	O
cid:28	O
)	O
9	O
)	O
.	O
and	O
node	O
to	O
a	O
i	O
(	O
cid:12	O
)	O
nd	O
that	O
it	O
is	O
helpful	O
when	O
relating	O
sparse-graph	O
codes	O
to	O
each	O
other	O
to	O
use	O
a	O
common	O
representation	O
for	O
them	O
all	O
.	O
forney	O
(	O
2001	O
)	O
introduced	O
the	O
idea	O
of	O
and	O
all	O
variable	O
nodes	O
a	O
normal	B
graph	I
in	O
which	O
the	O
only	O
nodes	O
are	O
have	O
degree	B
one	O
or	O
two	O
;	O
variable	O
nodes	O
with	O
degree	O
two	O
can	O
be	O
represented	O
on	O
node	O
.	O
the	O
generalized	B
parity-check	I
matrix	I
edges	O
that	O
connect	O
a	O
is	O
a	O
graphical	O
way	O
of	O
representing	O
normal	B
graphs	O
.	O
in	O
a	O
parity-check	B
matrix	I
,	O
the	O
columns	O
are	O
transmitted	O
bits	O
,	O
and	O
the	O
rows	O
are	O
linear	B
constraints	O
.	O
in	O
a	O
generalized	B
parity-check	I
matrix	I
,	O
additional	O
columns	O
may	O
be	O
included	O
,	O
which	O
represent	O
state	O
variables	O
that	O
are	O
not	O
transmitted	O
.	O
one	O
way	O
of	O
thinking	O
of	O
these	O
state	O
variables	O
is	O
that	O
they	O
are	O
punctured	O
from	O
the	O
code	B
before	O
transmission	O
.	O
state	O
variables	O
are	O
indicated	O
by	O
a	O
horizontal	O
line	O
above	O
the	O
corresponding	O
columns	O
.	O
the	O
other	O
pieces	O
of	O
diagrammatic	O
notation	B
for	O
generalized	B
parity-	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
49.5	O
:	O
generalized	O
parity-check	O
matrices	O
585	O
gt	O
=	O
h	O
=	O
figure	O
49.4.	O
the	O
generator	B
matrix	I
,	O
parity-check	B
matrix	I
,	O
and	O
a	O
generalized	B
parity-check	I
matrix	I
of	O
a	O
repetition	B
code	I
with	O
rate	B
1/3	O
.	O
fa	O
;	O
pg	O
=	O
check	O
matrices	B
are	O
,	O
as	O
in	O
(	O
mackay	O
,	O
1999b	O
;	O
mackay	O
et	O
al.	O
,	O
1998	O
)	O
:	O
(	O
cid:15	O
)	O
a	O
diagonal	O
line	O
in	O
a	O
square	B
indicates	O
that	O
that	O
part	O
of	O
the	O
matrix	O
contains	O
an	O
identity	B
matrix	I
.	O
(	O
cid:15	O
)	O
two	O
or	O
more	O
parallel	O
diagonal	O
lines	O
indicate	O
a	O
band-diagonal	O
matrix	B
with	O
a	O
corresponding	O
number	O
of	O
1s	O
per	O
row	O
.	O
(	O
cid:15	O
)	O
a	O
horizontal	O
ellipse	O
with	O
an	O
arrow	O
on	O
it	O
indicates	O
that	O
the	O
corresponding	O
columns	O
in	O
a	O
block	B
are	O
randomly	O
permuted	O
.	O
(	O
cid:15	O
)	O
a	O
vertical	O
ellipse	O
with	O
an	O
arrow	O
on	O
it	O
indicates	O
that	O
the	O
corresponding	O
rows	O
in	O
a	O
block	B
are	O
randomly	O
permuted	O
.	O
(	O
cid:15	O
)	O
an	O
integer	O
surrounded	O
by	O
a	O
circle	B
represents	O
that	O
number	O
of	O
superposed	O
random	B
permutation	O
matrices	B
.	O
de	O
(	O
cid:12	O
)	O
nition	O
.	O
a	O
generalized	B
parity-check	I
matrix	I
is	O
a	O
pair	O
fa	O
;	O
pg	O
,	O
where	O
a	O
is	O
a	O
binary	O
matrix	O
and	O
p	O
is	O
a	O
list	O
of	O
the	O
punctured	O
bits	O
.	O
the	O
matrix	B
de	O
(	O
cid:12	O
)	O
nes	O
a	O
set	B
of	O
valid	O
vectors	B
x	O
,	O
satisfying	O
ax	O
=	O
0	O
;	O
(	O
49.2	O
)	O
for	O
each	O
valid	O
vector	O
there	O
is	O
a	O
codeword	B
t	O
(	O
x	O
)	O
that	O
is	O
obtained	O
by	O
puncturing	O
from	O
x	O
the	O
bits	O
indicated	O
by	O
p.	O
for	O
any	O
one	O
code	B
there	O
are	O
many	O
generalized	O
parity-check	O
matrices	O
.	O
the	O
rate	B
of	O
a	O
code	B
with	O
generalized	B
parity-check	I
matrix	I
fa	O
;	O
pg	O
can	O
be	O
estimated	O
as	O
follows	O
.	O
if	O
a	O
is	O
l	O
(	O
cid:2	O
)	O
m	O
0	O
,	O
and	O
p	O
punctures	O
s	O
bits	O
and	O
selects	O
n	O
bits	O
for	O
transmission	O
(	O
l	O
=	O
n	O
+	O
s	O
)	O
,	O
then	O
the	O
e	O
(	O
cid:11	O
)	O
ective	O
number	O
of	O
constraints	O
on	O
the	O
codeword	B
,	O
m	O
,	O
is	O
the	O
number	O
of	O
source	O
bits	O
is	O
m	O
=	O
m0	O
(	O
cid:0	O
)	O
s	O
;	O
k	O
=	O
n	O
(	O
cid:0	O
)	O
m	O
=	O
l	O
(	O
cid:0	O
)	O
m0	O
;	O
and	O
the	O
rate	B
is	O
greater	O
than	O
or	O
equal	O
to	O
r	O
=	O
1	O
(	O
cid:0	O
)	O
m	O
n	O
=	O
1	O
(	O
cid:0	O
)	O
m0	O
(	O
cid:0	O
)	O
s	O
l	O
(	O
cid:0	O
)	O
s	O
:	O
(	O
49.3	O
)	O
(	O
49.4	O
)	O
(	O
49.5	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
586	O
gt	O
=	O
gt	O
=	O
3	O
3	O
3	O
3	O
h	O
=	O
a	O
;	O
p	O
=	O
3	O
3	O
3	O
3	O
49	O
|	O
repeat	O
{	O
accumulate	O
codes	O
figure	O
49.5.	O
the	O
generator	B
matrix	I
and	O
parity-check	B
matrix	I
of	O
a	O
systematic	B
low-density	O
generator-matrix	O
code	B
.	O
the	O
code	B
has	O
rate	B
1/3	O
.	O
figure	O
49.6.	O
the	O
generator	B
matrix	I
and	O
generalized	B
parity-check	I
matrix	I
of	O
a	O
non-systematic	O
low-density	B
generator-matrix	I
code	I
.	O
the	O
code	B
has	O
rate	B
1/2	O
.	O
examples	O
repetition	B
code	I
.	O
the	O
generator	B
matrix	I
,	O
parity-check	B
matrix	I
,	O
and	O
generalized	O
parity-check	B
matrix	I
of	O
a	O
simple	O
rate-1/3	O
repetition	B
code	I
are	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
49.4.	O
in	O
an	O
(	O
n	O
;	O
k	O
)	O
systematic	B
low-	O
systematic	B
low-density	O
generator-matrix	O
code	B
.	O
density	B
generator-matrix	O
code	B
,	O
there	O
are	O
no	O
state	O
variables	O
.	O
a	O
transmitted	O
codeword	B
t	O
of	O
length	O
n	O
is	O
given	O
by	O
where	O
t	O
=	O
gts	O
;	O
gt	O
=	O
(	O
cid:20	O
)	O
ik	O
p	O
(	O
cid:21	O
)	O
;	O
(	O
49.6	O
)	O
(	O
49.7	O
)	O
with	O
ik	O
denoting	O
the	O
k	O
(	O
cid:2	O
)	O
k	O
identity	B
matrix	I
,	O
and	O
p	O
being	O
a	O
very	O
sparse	O
m	O
(	O
cid:2	O
)	O
k	O
matrix	B
,	O
where	O
m	O
=	O
n	O
(	O
cid:0	O
)	O
k.	O
the	O
parity-check	B
matrix	I
of	O
this	O
code	B
is	O
h	O
=	O
[	O
pjim	O
]	O
:	O
(	O
49.8	O
)	O
in	O
the	O
case	O
of	O
a	O
rate-1/3	O
code	B
,	O
this	O
parity-check	B
matrix	I
might	O
be	O
represented	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
49.5.	O
non-systematic	O
low-density	B
generator-matrix	I
code	I
.	O
in	O
an	O
(	O
n	O
;	O
k	O
)	O
non-systematic	O
low-density	B
generator-matrix	I
code	I
,	O
a	O
transmitted	O
codeword	B
t	O
of	O
length	O
n	O
is	O
given	O
by	O
(	O
49.9	O
)	O
where	O
gt	O
is	O
a	O
very	O
sparse	O
n	O
(	O
cid:2	O
)	O
k	O
matrix	B
.	O
the	O
generalized	B
parity-check	I
matrix	I
of	O
this	O
code	B
is	O
(	O
49.10	O
)	O
t	O
=	O
gts	O
;	O
and	O
the	O
corresponding	O
generalized	O
parity-check	O
equation	O
is	O
a	O
=	O
(	O
cid:2	O
)	O
gtjin	O
(	O
cid:3	O
)	O
;	O
ax	O
=	O
0	O
;	O
where	O
x	O
=	O
(	O
cid:20	O
)	O
s	O
t	O
(	O
cid:21	O
)	O
.	O
(	O
49.11	O
)	O
whereas	O
the	O
parity-check	B
matrix	I
of	O
this	O
simple	O
code	O
is	O
typically	O
a	O
com-	O
plex	O
,	O
dense	O
matrix	B
,	O
the	O
generalized	B
parity-check	I
matrix	I
retains	O
the	O
underlying	O
simplicity	O
of	O
the	O
code	O
.	O
in	O
the	O
case	O
of	O
a	O
rate-1/2	O
code	B
,	O
this	O
generalized	B
parity-check	I
matrix	I
might	O
be	O
represented	O
as	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
49.6.	O
low-density	B
parity-check	I
codes	O
and	O
linear	O
mn	O
codes	O
.	O
the	O
parity-check	B
matrix	I
of	O
a	O
rate-1/3	O
low-density	B
parity-check	I
code	I
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
49.7a	O
.	O
3	O
3	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
49.7.	O
the	O
generalized	O
parity-check	O
matrices	O
of	O
(	O
a	O
)	O
a	O
rate-1/3	O
gallager	O
code	B
with	O
m=2	O
columns	O
of	O
weight	O
2	O
;	O
(	O
b	O
)	O
a	O
rate-1/2	O
linear	B
mn	O
code	B
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
49.5	O
:	O
generalized	O
parity-check	O
matrices	O
587	O
figure	O
49.8.	O
the	O
generalized	O
parity-check	O
matrices	O
of	O
(	O
a	O
)	O
a	O
convolutional	B
code	I
with	O
rate	B
1/2	O
.	O
(	O
b	O
)	O
a	O
rate-1/3	O
turbo	B
code	I
built	O
by	O
parallel	O
concatenation	B
of	O
two	O
convolutional	B
codes	O
.	O
(	O
a	O
)	O
(	O
b	O
)	O
a	O
linear	B
mn	O
code	B
is	O
a	O
non-systematic	O
low-density	B
parity-check	I
code	I
.	O
the	O
k	O
state	O
bits	O
of	O
an	O
mn	O
code	B
are	O
the	O
source	O
bits	O
.	O
figure	O
49.7b	O
shows	O
the	O
generalized	B
parity-check	I
matrix	I
of	O
a	O
rate-1=2	O
linear	B
mn	O
code	B
.	O
in	O
a	O
non-systematic	O
,	O
non-recursive	O
convolutional	B
code	I
,	O
convolutional	B
codes	O
.	O
the	O
source	O
bits	O
,	O
which	O
play	O
the	O
role	O
of	O
state	O
bits	O
,	O
are	O
fed	O
into	O
a	O
delay-line	O
and	O
two	O
linear	B
functions	O
of	O
the	O
delay-line	O
are	O
transmitted	O
.	O
in	O
(	O
cid:12	O
)	O
gure	O
49.8a	O
,	O
these	O
two	O
parity	B
streams	O
are	O
shown	O
as	O
two	O
successive	O
vectors	B
of	O
length	B
k.	O
[	O
it	O
is	O
common	O
to	O
interleave	O
these	O
two	O
parity	B
streams	O
,	O
a	O
bit-reordering	O
that	O
is	O
not	O
relevant	O
here	O
,	O
and	O
is	O
not	O
illustrated	O
.	O
]	O
concatenation	B
.	O
‘	O
parallel	O
concatenation	B
’	O
of	O
two	O
codes	O
is	O
represented	O
in	O
one	O
of	O
these	O
diagrams	O
by	O
aligning	O
the	O
matrices	B
of	O
two	O
codes	O
in	O
such	O
a	O
way	O
that	O
the	O
‘	O
source	O
bits	O
’	O
line	O
up	O
,	O
and	O
by	O
adding	O
blocks	O
of	O
zero-entries	O
to	O
the	O
matrix	B
such	O
that	O
the	O
state	O
bits	O
and	O
parity	O
bits	O
of	O
the	O
two	O
codes	O
occupy	O
separate	O
columns	O
.	O
an	O
example	O
is	O
given	O
by	O
the	O
turbo	B
code	I
that	O
follows	O
.	O
in	O
‘	O
serial	O
concatenation	B
’	O
,	O
the	O
columns	O
corresponding	O
to	O
the	O
transmitted	O
bits	O
of	O
the	O
(	O
cid:12	O
)	O
rst	O
code	B
are	O
aligned	O
with	O
the	O
columns	O
corresponding	O
to	O
the	O
source	O
bits	O
of	O
the	O
second	O
code	B
.	O
turbo	B
codes	I
.	O
a	O
turbo	B
code	I
is	O
the	O
parallel	O
concatenation	B
of	O
two	O
convolutional	B
codes	O
.	O
the	O
generalized	B
parity-check	I
matrix	I
of	O
a	O
rate-1/3	O
turbo	B
code	I
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
49.8b	O
.	O
repeat	O
{	O
accumulate	O
codes	O
.	O
the	O
generalized	O
parity-check	O
matrices	O
of	O
a	O
rate-1/3	O
repeat	O
{	O
accumulate	O
code	B
is	O
shown	O
in	O
(	O
cid:12	O
)	O
gure	O
49.9.	O
repeat-accumulate	O
codes	O
are	O
equivalent	O
to	O
staircase	B
codes	O
(	O
section	B
47.7	O
,	O
p.569	O
)	O
.	O
intersection	B
.	O
the	O
generalized	B
parity-check	I
matrix	I
of	O
the	O
intersection	B
of	O
two	O
codes	O
is	O
made	O
by	O
stacking	O
their	O
generalized	O
parity-check	O
matrices	O
on	O
top	O
of	O
each	O
other	O
in	O
such	O
a	O
way	O
that	O
all	O
the	O
transmitted	O
bits	O
’	O
columns	O
are	O
correctly	O
aligned	O
,	O
and	O
any	O
punctured	O
bits	O
associated	O
with	O
the	O
two	O
component	O
codes	O
occupy	O
separate	O
columns	O
.	O
figure	O
49.9.	O
the	O
generalized	B
parity-check	I
matrix	I
of	O
a	O
repeat	O
{	O
accumulate	O
code	B
with	O
rate	B
1/3	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
about	O
chapter	O
50	O
the	O
following	O
exercise	O
provides	O
a	O
helpful	O
background	O
for	O
digital	B
fountain	I
codes	O
.	O
.	O
exercise	O
50.1	O
.	O
[	O
3	O
]	O
an	O
author	O
proofreads	O
his	O
k	O
=	O
700-page	O
book	O
by	O
inspecting	O
random	B
pages	O
.	O
he	O
makes	O
n	O
page-inspections	O
,	O
and	O
does	O
not	O
take	O
any	O
precautions	O
to	O
avoid	O
inspecting	O
the	O
same	O
page	O
twice	O
.	O
(	O
a	O
)	O
after	O
n	O
=	O
k	O
page-inspections	O
,	O
what	O
fraction	O
of	O
pages	O
do	O
you	O
expect	O
have	O
never	O
been	O
inspected	O
?	O
(	O
b	O
)	O
after	O
n	O
>	O
k	O
page-inspections	O
,	O
what	O
is	O
the	O
probability	B
that	O
one	O
or	O
more	O
pages	O
have	O
never	O
been	O
inspected	O
?	O
(	O
c	O
)	O
show	O
that	O
in	O
order	O
for	O
the	O
probability	B
that	O
all	O
k	O
pages	O
have	O
been	O
inspected	O
to	O
be	O
1	O
(	O
cid:0	O
)	O
(	O
cid:14	O
)	O
,	O
we	O
require	O
n	O
’	O
k	O
ln	O
(	O
k=	O
(	O
cid:14	O
)	O
)	O
page-inspections	O
.	O
[	O
this	O
problem	O
is	O
commonly	O
presented	O
in	O
terms	O
of	O
throwing	O
n	O
balls	O
at	O
random	B
into	O
k	O
bins	O
;	O
what	O
’	O
s	O
the	O
probability	B
that	O
every	O
bin	O
gets	O
at	O
least	O
one	O
ball	O
?	O
]	O
588	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
50	O
digital	B
fountain	I
codes	O
digital	B
fountain	I
codes	O
are	O
record-breaking	O
sparse-graph	O
codes	O
for	O
channels	O
with	O
erasures	O
.	O
channels	O
with	O
erasures	O
are	O
of	O
great	O
importance	O
.	O
for	O
example	O
,	O
(	O
cid:12	O
)	O
les	O
sent	O
over	O
the	O
internet	B
are	O
chopped	O
into	O
packets	O
,	O
and	O
each	O
packet	B
is	O
either	O
received	O
without	O
error	O
or	O
not	O
received	O
.	O
a	O
simple	O
channel	O
model	B
describing	O
this	O
situation	O
is	O
a	O
q-ary	O
erasure	B
channel	I
,	O
which	O
has	O
(	O
for	O
all	O
inputs	O
in	O
the	O
input	O
alphabet	O
f0	O
;	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
;	O
q	O
(	O
cid:0	O
)	O
1g	O
)	O
a	O
probability	B
1	O
(	O
cid:0	O
)	O
f	O
of	O
transmitting	O
the	O
input	O
without	O
error	O
,	O
and	O
probability	O
f	O
of	O
delivering	O
the	O
output	O
‘	O
?	O
’	O
.	O
the	O
alphabet	O
size	O
q	O
is	O
2l	O
,	O
where	O
l	O
is	O
the	O
number	O
of	O
bits	O
in	O
a	O
packet	B
.	O
common	O
methods	B
for	O
communicating	O
over	O
such	O
channels	O
employ	O
a	O
feed-	O
back	O
channel	B
from	O
receiver	O
to	O
sender	O
that	O
is	O
used	O
to	O
control	O
the	O
retransmission	B
of	O
erased	O
packets	O
.	O
for	O
example	O
,	O
the	O
receiver	O
might	O
send	O
back	O
messages	O
that	O
identify	O
the	O
missing	O
packets	O
,	O
which	O
are	O
then	O
retransmitted	O
.	O
alternatively	O
,	O
the	O
receiver	O
might	O
send	O
back	O
messages	O
that	O
acknowledge	O
each	O
received	O
packet	B
;	O
the	O
sender	O
keeps	O
track	O
of	O
which	O
packets	O
have	O
been	O
acknowledged	O
and	O
retransmits	O
the	O
others	O
until	O
all	O
packets	O
have	O
been	O
acknowledged	O
.	O
these	O
simple	O
retransmission	O
protocols	O
have	O
the	O
advantage	O
that	O
they	O
will	O
work	O
regardless	O
of	O
the	O
erasure	O
probability	B
f	O
,	O
but	O
purists	O
who	O
have	O
learned	O
their	O
shannon	O
theory	O
will	O
feel	O
that	O
these	O
retransmission	B
protocols	O
are	O
wasteful	O
.	O
if	O
the	O
erasure	B
probability	O
f	O
is	O
large	O
,	O
the	O
number	O
of	O
feedback	O
messages	O
sent	O
by	O
the	O
(	O
cid:12	O
)	O
rst	O
protocol	B
will	O
be	O
large	O
.	O
under	O
the	O
second	O
protocol	B
,	O
it	O
’	O
s	O
likely	O
that	O
the	O
receiver	O
will	O
end	O
up	O
receiving	O
multiple	O
redundant	O
copies	O
of	O
some	O
packets	O
,	O
and	O
heavy	O
use	O
is	O
made	O
of	O
the	O
feedback	O
channel	B
.	O
according	O
to	O
shannon	O
,	O
there	O
is	O
no	O
need	O
for	O
the	O
feedback	B
channel	O
:	O
the	O
capacity	B
of	O
the	O
forward	O
channel	O
is	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
l	O
bits	O
,	O
whether	O
or	O
not	O
we	O
have	O
feedback	B
.	O
the	O
wastefulness	O
of	O
the	O
simple	O
retransmission	B
protocols	O
is	O
especially	O
evi-	O
dent	O
in	O
the	O
case	O
of	O
a	O
broadcast	B
channel	I
with	O
erasures	O
{	O
channels	O
where	O
one	O
sender	O
broadcasts	O
to	O
many	O
receivers	O
,	O
and	O
each	O
receiver	O
receives	O
a	O
random	B
fraction	O
(	O
1	O
(	O
cid:0	O
)	O
f	O
)	O
of	O
the	O
packets	O
.	O
if	O
every	O
packet	B
that	O
is	O
missed	O
by	O
one	O
or	O
more	O
receivers	O
has	O
to	O
be	O
retransmitted	O
,	O
those	O
retransmissions	O
will	O
be	O
terribly	O
re-	O
dundant	O
.	O
every	O
receiver	O
will	O
have	O
already	O
received	O
most	O
of	O
the	O
retransmitted	O
packets	O
.	O
so	O
,	O
we	O
would	O
like	O
to	O
make	O
erasure-correcting	O
codes	O
that	O
require	O
no	O
feed-	O
back	O
or	O
almost	O
no	O
feedback	B
.	O
the	O
classic	O
block	B
codes	O
for	O
erasure	O
correction	O
are	O
called	O
reed	O
{	O
solomon	O
codes	O
.	O
an	O
(	O
n	O
;	O
k	O
)	O
reed	O
{	O
solomon	O
code	B
(	O
over	O
an	O
alpha-	O
bet	B
of	O
size	O
q	O
=	O
2l	O
)	O
has	O
the	O
ideal	O
property	O
that	O
if	O
any	O
k	O
of	O
the	O
n	O
transmitted	O
symbols	O
are	O
received	O
then	O
the	O
original	O
k	O
source	O
symbols	O
can	O
be	O
recovered	O
.	O
[	O
see	O
berlekamp	O
(	O
1968	O
)	O
or	O
lin	O
and	O
costello	O
(	O
1983	O
)	O
for	O
further	O
information	B
;	O
reed	O
{	O
solomon	O
codes	O
exist	O
for	O
n	O
<	O
q	O
.	O
]	O
but	O
reed	O
{	O
solomon	O
codes	O
have	O
the	O
disadvantage	O
that	O
they	O
are	O
practical	B
only	O
for	O
small	O
k	O
,	O
n	O
,	O
and	O
q	O
:	O
standard	O
im-	O
589	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
590	O
50	O
|	O
digital	B
fountain	I
codes	O
lt	O
stands	O
for	O
‘	O
luby	O
transform	O
’	O
.	O
plementations	O
of	O
encoding	O
and	O
decoding	O
have	O
a	O
cost	O
of	O
order	O
k	O
(	O
n	O
(	O
cid:0	O
)	O
k	O
)	O
log	O
2	O
n	O
packet	B
operations	O
.	O
furthermore	O
,	O
with	O
a	O
reed	O
{	O
solomon	O
code	B
,	O
as	O
with	O
any	O
block	B
code	I
,	O
one	O
must	O
estimate	O
the	O
erasure	B
probability	O
f	O
and	O
choose	O
the	O
code	B
rate	O
r	O
=	O
k=n	O
before	O
transmission	O
.	O
if	O
we	O
are	O
unlucky	O
and	O
f	O
is	O
larger	O
than	O
expected	O
and	O
the	O
receiver	O
receives	O
fewer	O
than	O
k	O
symbols	O
,	O
what	O
are	O
we	O
to	O
do	O
?	O
we	O
’	O
d	O
like	O
a	O
simple	O
way	O
to	O
extend	O
the	O
code	B
on	O
the	O
(	O
cid:13	O
)	O
y	O
to	O
create	O
a	O
lower-rate	O
(	O
n	O
0	O
;	O
k	O
)	O
code	B
.	O
for	O
reed	O
{	O
solomon	O
codes	O
,	O
no	O
such	O
on-the-	O
(	O
cid:13	O
)	O
y	O
method	B
exists	O
.	O
there	O
is	O
a	O
better	O
way	O
,	O
pioneered	O
by	O
michael	O
luby	O
(	O
2002	O
)	O
at	O
his	O
company	O
digital	B
fountain	I
,	O
the	O
(	O
cid:12	O
)	O
rst	O
company	O
whose	O
business	O
is	O
based	O
on	O
sparse-graph	O
codes	O
.	O
the	O
digital	B
fountain	I
codes	O
i	O
describe	O
here	O
,	O
lt	O
codes	O
,	O
were	O
invented	O
by	O
luby	O
in	O
1998.	O
the	O
idea	O
of	O
a	O
digital	B
fountain	I
code	O
is	O
as	O
follows	O
.	O
the	O
encoder	B
is	O
a	O
fountain	O
that	O
produces	O
an	O
endless	O
supply	O
of	O
water	O
drops	O
(	O
encoded	O
packets	O
)	O
;	O
let	O
’	O
s	O
say	O
the	O
original	O
source	O
(	O
cid:12	O
)	O
le	O
has	O
a	O
size	O
of	O
kl	O
bits	O
,	O
and	O
each	O
drop	O
contains	O
l	O
encoded	O
bits	O
.	O
now	O
,	O
anyone	O
who	O
wishes	O
to	O
receive	O
the	O
encoded	O
(	O
cid:12	O
)	O
le	O
holds	O
a	O
bucket	O
under	O
the	O
fountain	O
and	O
collects	O
drops	O
until	O
the	O
number	O
of	O
drops	O
in	O
the	O
bucket	O
is	O
a	O
little	O
larger	O
than	O
k.	O
they	O
can	O
then	O
recover	O
the	O
original	O
(	O
cid:12	O
)	O
le	O
.	O
digital	B
fountain	I
codes	O
are	O
rateless	B
in	O
the	O
sense	O
that	O
the	O
number	O
of	O
encoded	O
packets	O
that	O
can	O
be	O
generated	O
from	O
the	O
source	O
message	O
is	O
potentially	O
limitless	O
;	O
and	O
the	O
number	O
of	O
encoded	O
packets	O
generated	O
can	O
be	O
determined	O
on	O
the	O
(	O
cid:13	O
)	O
y.	O
regardless	O
of	O
the	O
statistics	O
of	O
the	O
erasure	O
events	O
on	O
the	O
channel	B
,	O
we	O
can	O
send	O
as	O
many	O
encoded	O
packets	O
as	O
are	O
needed	O
in	O
order	O
for	O
the	O
decoder	B
to	O
recover	O
the	O
source	O
data	O
.	O
the	O
source	O
data	O
can	O
be	O
decoded	O
from	O
any	O
set	B
of	O
k	O
0	O
encoded	O
packets	O
,	O
for	O
k0	O
slightly	O
larger	O
than	O
k	O
(	O
in	O
practice	O
,	O
about	O
5	O
%	O
larger	O
)	O
.	O
digital	B
fountain	I
codes	O
also	O
have	O
fantastically	O
small	O
encoding	O
and	O
decod-	O
ing	O
complexities	O
.	O
with	O
probability	O
1	O
(	O
cid:0	O
)	O
(	O
cid:14	O
)	O
,	O
k	O
packets	O
can	O
be	O
communicated	O
with	O
average	O
encoding	O
and	O
decoding	O
costs	O
both	O
of	O
order	O
k	O
ln	O
(	O
k=	O
(	O
cid:14	O
)	O
)	O
packet	B
operations	O
.	O
luby	O
calls	O
these	O
codes	O
universal	B
because	O
they	O
are	O
simultaneously	O
near-	O
optimal	B
for	O
every	O
erasure	B
channel	I
,	O
and	O
they	O
are	O
very	O
e	O
(	O
cid:14	O
)	O
cient	O
as	O
the	O
(	O
cid:12	O
)	O
le	O
length	B
k	O
grows	O
.	O
the	O
overhead	O
k0	O
(	O
cid:0	O
)	O
k	O
is	O
of	O
order	O
pk	O
(	O
ln	O
(	O
k=	O
(	O
cid:14	O
)	O
)	O
)	O
2	O
.	O
50.1	O
a	O
digital	B
fountain	I
’	O
s	O
encoder	B
each	O
encoded	O
packet	B
tn	O
is	O
produced	O
from	O
the	O
source	O
(	O
cid:12	O
)	O
le	O
s1s2s3	O
:	O
:	O
:	O
sk	O
as	O
follows	O
:	O
1.	O
randomly	O
choose	O
the	O
degree	B
dn	O
of	O
the	O
packet	O
from	O
a	O
degree	B
distri-	O
bution	O
(	O
cid:26	O
)	O
(	O
d	O
)	O
;	O
the	O
appropriate	O
choice	O
of	O
(	O
cid:26	O
)	O
depends	O
on	O
the	O
source	O
(	O
cid:12	O
)	O
le	O
size	O
k	O
,	O
as	O
we	O
’	O
ll	O
discuss	O
later	O
.	O
2.	O
choose	O
,	O
uniformly	O
at	O
random	B
,	O
dn	O
distinct	O
input	O
packets	O
,	O
and	O
set	O
tn	O
equal	O
to	O
the	O
bitwise	B
sum	O
,	O
modulo	O
2	O
of	O
those	O
dn	O
packets	O
.	O
this	O
sum	O
can	O
be	O
done	O
by	O
successively	O
exclusive-or-ing	O
the	O
packets	O
together	O
.	O
this	O
encoding	O
operation	O
de	O
(	O
cid:12	O
)	O
nes	O
a	O
graph	B
connecting	O
encoded	O
packets	O
to	O
source	O
packets	O
.	O
if	O
the	O
mean	B
degree	O
(	O
cid:22	O
)	O
d	O
is	O
signi	O
(	O
cid:12	O
)	O
cantly	O
smaller	O
than	O
k	O
then	O
the	O
graph	B
is	O
sparse	O
.	O
we	O
can	O
think	O
of	O
the	O
resulting	O
code	B
as	O
an	O
irregular	B
low-density	O
generator-matrix	O
code	B
.	O
the	O
decoder	B
needs	O
to	O
know	O
the	O
degree	B
of	O
each	O
packet	B
that	O
is	O
received	O
,	O
and	O
which	O
source	O
packets	O
it	O
is	O
connected	O
to	O
in	O
the	O
graph	B
.	O
this	O
information	B
can	O
be	O
communicated	O
to	O
the	O
decoder	B
in	O
various	O
ways	O
.	O
for	O
example	O
,	O
if	O
the	O
sender	O
and	O
receiver	O
have	O
synchronized	O
clocks	O
,	O
they	O
could	O
use	O
identical	O
pseudo-random	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
50.2	O
:	O
the	O
decoder	B
591	O
s1	O
s2	O
s3	O
a	O
)	O
1	O
0	O
11	O
1	O
b	O
)	O
0	O
11	O
1	O
c	O
)	O
1	O
01	O
01	O
d	O
)	O
1	O
1	O
01	O
1	O
1	O
01	O
1	O
e	O
)	O
f	O
)	O
figure	O
50.1.	O
example	O
decoding	B
for	O
a	O
digital	B
fountain	I
code	O
with	O
k	O
=	O
3	O
source	O
bits	O
and	O
n	O
=	O
4	O
encoded	O
bits	O
.	O
number	O
generators	O
,	O
seeded	O
by	O
the	O
clock	O
,	O
to	O
choose	O
each	O
random	B
degree	O
and	O
each	O
set	B
of	O
connections	O
.	O
alternatively	O
,	O
the	O
sender	O
could	O
pick	O
a	O
random	B
key	O
,	O
(	O
cid:20	O
)	O
n	O
,	O
given	O
which	O
the	O
degree	B
and	O
the	O
connections	O
are	O
determined	O
by	O
a	O
pseudo-	O
random	B
process	O
,	O
and	O
send	O
that	O
key	O
in	O
the	O
header	O
of	O
the	O
packet	O
.	O
as	O
long	O
as	O
the	O
packet	B
size	O
l	O
is	O
much	O
bigger	O
than	O
the	O
key	O
size	O
(	O
which	O
need	O
only	O
be	O
32	O
bits	O
or	O
so	O
)	O
,	O
this	O
key	O
introduces	O
only	O
a	O
small	O
overhead	O
cost	O
.	O
50.2	O
the	O
decoder	B
decoding	O
a	O
sparse-graph	B
code	I
is	O
especially	O
easy	O
in	O
the	O
case	O
of	O
an	O
erasure	B
chan-	O
nel	O
.	O
the	O
decoder	B
’	O
s	O
task	O
is	O
to	O
recover	O
s	O
from	O
t	O
=	O
gs	O
,	O
where	O
g	O
is	O
the	O
matrix	B
associated	O
with	O
the	O
graph	B
.	O
the	O
simple	O
way	O
to	O
attempt	O
to	O
solve	O
this	O
prob-	O
lem	O
is	O
by	O
message-passing	O
.	O
we	O
can	O
think	O
of	O
the	O
decoding	O
algorithm	B
as	O
the	O
sum	O
{	O
product	O
algorithm	O
if	O
we	O
wish	O
,	O
but	O
all	O
messages	O
are	O
either	O
completely	O
un-	O
certain	O
messages	O
or	O
completely	O
certain	O
messages	O
.	O
uncertain	O
messages	O
assert	O
that	O
a	O
message	O
packet	O
sk	O
could	O
have	O
any	O
value	O
,	O
with	O
equal	O
probability	B
;	O
certain	O
messages	O
assert	O
that	O
sk	O
has	O
a	O
particular	O
value	O
,	O
with	O
probability	O
one	O
.	O
this	O
simplicity	O
of	O
the	O
messages	O
allows	O
a	O
simple	O
description	O
of	O
the	O
decoding	O
process	O
.	O
we	O
’	O
ll	O
call	O
the	O
encoded	O
packets	O
ftng	O
check	O
nodes	O
.	O
1.	O
find	O
a	O
check	O
node	O
tn	O
that	O
is	O
connected	O
to	O
only	O
one	O
source	O
packet	O
sk	O
.	O
(	O
if	O
there	O
is	O
no	O
such	O
check	O
node	O
,	O
this	O
decoding	B
algorithm	O
halts	O
at	O
this	O
point	O
,	O
and	O
fails	O
to	O
recover	O
all	O
the	O
source	O
packets	O
.	O
)	O
(	O
a	O
)	O
set	B
sk	O
=	O
tn	O
.	O
(	O
b	O
)	O
add	O
sk	O
to	O
all	O
checks	O
tn0	O
that	O
are	O
connected	O
to	O
sk	O
:	O
tn0	O
:	O
=	O
tn0	O
+	O
sk	O
for	O
all	O
n0	O
such	O
that	O
gn0k	O
=	O
1	O
.	O
(	O
50.1	O
)	O
(	O
c	O
)	O
remove	O
all	O
the	O
edges	O
connected	O
to	O
the	O
source	O
packet	O
sk	O
.	O
2.	O
repeat	O
(	O
1	O
)	O
until	O
all	O
fskg	O
are	O
determined	O
.	O
this	O
decoding	B
process	O
is	O
illustrated	O
in	O
(	O
cid:12	O
)	O
gure	O
50.1	O
for	O
a	O
toy	O
case	O
where	O
each	O
packet	B
is	O
just	O
one	O
bit	B
.	O
there	O
are	O
three	O
source	O
packets	O
(	O
shown	O
by	O
the	O
upper	O
circles	O
)	O
and	O
four	O
received	O
packets	O
(	O
shown	O
by	O
the	O
lower	O
check	O
symbols	O
)	O
,	O
which	O
have	O
the	O
values	O
t1t2t3t4	O
=	O
1011	O
at	O
the	O
start	O
of	O
the	O
algorithm	O
.	O
at	O
the	O
(	O
cid:12	O
)	O
rst	O
iteration	O
,	O
the	O
only	O
check	O
node	O
that	O
is	O
connected	O
to	O
a	O
sole	O
source	O
bit	O
is	O
the	O
(	O
cid:12	O
)	O
rst	O
check	O
node	O
(	O
panel	O
a	O
)	O
.	O
we	O
set	B
that	O
source	O
bit	O
s1	O
accordingly	O
(	O
panel	O
b	O
)	O
,	O
discard	O
the	O
check	O
node	O
,	O
then	O
add	O
the	O
value	O
of	O
s1	O
(	O
1	O
)	O
to	O
the	O
checks	O
to	O
which	O
it	O
is	O
connected	O
(	O
panel	O
c	O
)	O
,	O
disconnecting	O
s1	O
from	O
the	O
graph	B
.	O
at	O
the	O
start	O
of	O
the	O
second	O
iteration	O
(	O
panel	O
c	O
)	O
,	O
the	O
fourth	O
check	O
node	O
is	O
connected	O
to	O
a	O
sole	O
source	O
bit	O
,	O
s2	O
.	O
we	O
set	B
s2	O
to	O
t4	O
(	O
0	O
,	O
in	O
panel	O
d	O
)	O
,	O
and	O
add	O
s2	O
to	O
the	O
two	O
checks	O
it	O
is	O
connected	O
to	O
(	O
panel	O
e	O
)	O
.	O
finally	O
,	O
we	O
(	O
cid:12	O
)	O
nd	O
that	O
two	O
check	O
nodes	O
are	O
both	O
connected	O
to	O
s3	O
,	O
and	O
they	O
agree	O
about	O
the	O
value	O
of	O
s3	O
(	O
as	O
we	O
would	O
hope	O
!	O
)	O
,	O
which	O
is	O
restored	O
in	O
panel	O
f.	O
50.3	O
designing	O
the	O
degree	B
distribution	O
the	O
probability	B
distribution	O
(	O
cid:26	O
)	O
(	O
d	O
)	O
of	O
the	O
degree	O
is	O
a	O
critical	O
part	O
of	O
the	O
design	O
:	O
occasional	O
encoded	O
packets	O
must	O
have	O
high	O
degree	O
(	O
i.e.	O
,	O
d	O
similar	O
to	O
k	O
)	O
in	O
order	O
to	O
ensure	O
that	O
there	O
are	O
not	O
some	O
source	O
packets	O
that	O
are	O
connected	O
to	O
no-one	O
.	O
many	O
packets	O
must	O
have	O
low	O
degree	B
,	O
so	O
that	O
the	O
decoding	B
process	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
592	O
50	O
|	O
digital	B
fountain	I
codes	O
can	O
get	O
started	O
,	O
and	O
keep	O
going	O
,	O
and	O
so	O
that	O
the	O
total	O
number	O
of	O
addition	O
operations	O
involved	O
in	O
the	O
encoding	O
and	O
decoding	O
is	O
kept	O
small	O
.	O
for	O
a	O
given	O
degree	B
distribution	O
(	O
cid:26	O
)	O
(	O
d	O
)	O
,	O
the	O
statistics	O
of	O
the	O
decoding	O
process	O
can	O
be	O
predicted	O
by	O
an	O
appropriate	O
version	O
of	B
density	I
evolution	I
.	O
ideally	O
,	O
to	O
avoid	O
redundancy	B
,	O
we	O
’	O
d	O
like	O
the	O
received	O
graph	B
to	O
have	O
the	O
prop-	O
erty	O
that	O
just	O
one	O
check	O
node	O
has	O
degree	B
one	O
at	O
each	O
iteration	O
.	O
at	O
each	O
itera-	O
tion	O
,	O
when	O
this	O
check	O
node	O
is	O
processed	O
,	O
the	O
degrees	O
in	O
the	O
graph	B
are	O
reduced	O
in	O
such	O
a	O
way	O
that	O
one	O
new	O
degree-one	O
check	O
node	O
appears	O
.	O
in	O
expectation	O
,	O
this	O
ideal	O
behaviour	O
is	O
achieved	O
by	O
the	O
ideal	O
soliton	B
distribution	I
,	O
(	O
cid:26	O
)	O
(	O
1	O
)	O
=	O
1=k	O
1	O
(	O
cid:26	O
)	O
(	O
d	O
)	O
=	O
d	O
(	O
d	O
(	O
cid:0	O
)	O
1	O
)	O
for	O
d	O
=	O
2	O
;	O
3	O
;	O
:	O
:	O
:	O
;	O
k.	O
(	O
50.2	O
)	O
the	O
expected	O
degree	B
under	O
this	O
distribution	B
is	O
roughly	O
ln	O
k.	O
.	O
exercise	O
50.2	O
.	O
[	O
2	O
]	O
derive	O
the	O
ideal	O
soliton	B
distribution	I
.	O
at	O
the	O
(	O
cid:12	O
)	O
rst	O
iteration	O
(	O
t	O
=	O
0	O
)	O
let	O
the	O
number	O
of	O
packets	O
of	O
degree	O
d	O
be	O
h0	O
(	O
d	O
)	O
;	O
show	O
that	O
(	O
for	O
d	O
>	O
1	O
)	O
the	O
expected	O
number	O
of	O
packets	O
of	O
degree	O
d	O
that	O
have	O
their	O
degree	B
reduced	O
to	O
d	O
(	O
cid:0	O
)	O
1	O
is	O
h0	O
(	O
d	O
)	O
d=k	O
;	O
and	O
at	O
the	O
tth	O
iteration	O
,	O
when	O
t	O
of	O
the	O
k	O
packets	O
have	O
been	O
recovered	O
and	O
the	O
number	O
of	O
packets	O
of	O
degree	O
d	O
is	O
ht	O
(	O
d	O
)	O
,	O
the	O
expected	O
number	O
of	O
packets	O
of	O
degree	O
d	O
that	O
have	O
their	O
degree	B
reduced	O
to	O
d	O
(	O
cid:0	O
)	O
1	O
is	O
ht	O
(	O
d	O
)	O
d=	O
(	O
k	O
(	O
cid:0	O
)	O
t	O
)	O
.	O
hence	O
show	O
that	O
in	O
order	O
to	O
have	O
the	O
expected	O
number	O
of	O
packets	O
of	O
degree	O
1	O
satisfy	O
ht	O
(	O
1	O
)	O
=	O
1	O
for	O
all	O
t	O
2	O
f0	O
;	O
:	O
:	O
:	O
k	O
(	O
cid:0	O
)	O
1g	O
,	O
we	O
must	O
to	O
start	O
with	O
have	O
h0	O
(	O
1	O
)	O
=	O
1	O
and	O
h0	O
(	O
2	O
)	O
=	O
k=2	O
;	O
and	O
more	O
generally	O
,	O
ht	O
(	O
2	O
)	O
=	O
(	O
k	O
(	O
cid:0	O
)	O
t	O
)	O
=2	O
;	O
then	O
by	O
recursion	O
solve	O
for	O
h0	O
(	O
d	O
)	O
for	O
d	O
=	O
3	O
upwards	O
.	O
this	O
degree	B
distribution	O
works	O
poorly	O
in	O
practice	O
,	O
because	O
(	O
cid:13	O
)	O
uctuations	O
around	O
the	O
expected	O
behaviour	O
make	O
it	O
very	O
likely	O
that	O
at	O
some	O
point	O
in	O
the	O
decoding	B
process	O
there	O
will	O
be	O
no	O
degree-one	O
check	O
nodes	O
;	O
and	O
,	O
furthermore	O
,	O
a	O
few	O
source	O
nodes	O
will	O
receive	O
no	O
connections	O
at	O
all	O
.	O
a	O
small	O
modi	O
(	O
cid:12	O
)	O
cation	O
(	O
cid:12	O
)	O
xes	O
these	O
problems	O
.	O
the	O
robust	O
soliton	B
distribution	I
has	O
two	O
extra	O
parameters	O
,	O
c	O
and	O
(	O
cid:14	O
)	O
;	O
it	O
is	O
designed	O
to	O
ensure	O
that	O
the	O
expected	O
number	O
of	O
degree-one	O
checks	O
is	O
about	O
s	O
(	O
cid:17	O
)	O
c	O
ln	O
(	O
k=	O
(	O
cid:14	O
)	O
)	O
pk	O
;	O
(	O
50.3	O
)	O
rather	O
than	O
1	O
,	O
throughout	O
the	O
decoding	B
process	O
.	O
the	O
parameter	O
(	O
cid:14	O
)	O
is	O
a	O
bound	B
on	O
the	O
probability	B
that	O
the	O
decoding	B
fails	O
to	O
run	O
to	O
completion	O
after	O
a	O
certain	O
number	O
k0	O
of	O
packets	O
have	O
been	O
received	O
.	O
the	O
parameter	O
c	O
is	O
a	O
constant	O
of	O
order	O
1	O
,	O
if	O
our	O
aim	O
is	O
to	O
prove	O
luby	O
’	O
s	O
main	O
theorem	B
about	O
lt	O
codes	O
;	O
in	O
practice	O
however	O
it	O
can	O
be	O
viewed	O
as	O
a	O
free	O
parameter	O
,	O
with	O
a	O
value	O
somewhat	O
smaller	O
than	O
1	O
giving	O
good	B
results	O
.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
a	O
positive	O
function	O
1	O
d	O
ln	O
(	O
s=	O
(	O
cid:14	O
)	O
)	O
s	O
k	O
s	O
k	O
0	O
for	O
d	O
=	O
1	O
;	O
2	O
;	O
:	O
:	O
:	O
(	O
k=s	O
)	O
(	O
cid:0	O
)	O
1	O
for	O
d	O
=	O
k=s	O
for	O
d	O
>	O
k=s	O
(	O
50.4	O
)	O
(	O
cid:28	O
)	O
(	O
d	O
)	O
=8	O
>	O
<	O
>	O
:	O
(	O
see	O
(	O
cid:12	O
)	O
gure	O
50.2	O
and	O
exercise	O
50.4	O
(	O
p.594	O
)	O
)	O
then	O
add	O
the	O
ideal	O
soliton	O
distribu-	O
tion	O
(	O
cid:26	O
)	O
to	O
(	O
cid:28	O
)	O
and	O
normalize	O
to	O
obtain	O
the	O
robust	O
soliton	B
distribution	I
,	O
(	O
cid:22	O
)	O
:	O
(	O
cid:22	O
)	O
(	O
d	O
)	O
=	O
(	O
cid:26	O
)	O
(	O
d	O
)	O
+	O
(	O
cid:28	O
)	O
(	O
d	O
)	O
z	O
;	O
(	O
50.5	O
)	O
where	O
z	O
=	O
pd	O
(	O
cid:26	O
)	O
(	O
d	O
)	O
+	O
(	O
cid:28	O
)	O
(	O
d	O
)	O
:	O
the	O
number	O
of	O
encoded	O
packets	O
required	O
at	O
the	O
receiving	O
end	O
to	O
ensure	O
that	O
the	O
decoding	B
can	O
run	O
to	O
completion	O
,	O
with	O
proba-	O
bility	O
at	O
least	O
1	O
(	O
cid:0	O
)	O
(	O
cid:14	O
)	O
,	O
is	O
k0	O
=	O
kz	O
.	O
rho	O
tau	O
0.5	O
0.4	O
0.3	O
0.2	O
0.1	O
0	O
0	O
10	O
20	O
30	O
40	O
50	O
figure	O
50.2.	O
the	O
distributions	O
(	O
cid:26	O
)	O
(	O
d	O
)	O
and	O
(	O
cid:28	O
)	O
(	O
d	O
)	O
for	O
the	O
case	O
k	O
=	O
10	O
000	O
,	O
c	O
=	O
0:2	O
,	O
(	O
cid:14	O
)	O
=	O
0:05	O
,	O
which	O
gives	O
s	O
=	O
244	O
,	O
k=s	O
=	O
41	O
,	O
and	O
z	O
’	O
1:3.	O
the	O
distribution	B
(	O
cid:28	O
)	O
is	O
largest	O
at	O
d	O
=	O
1	O
and	O
d	O
=	O
k=s	O
.	O
140	O
120	O
100	O
80	O
60	O
40	O
20	O
0	O
11000	O
10800	O
10600	O
10400	O
10200	O
10000	O
delta=0.01	O
delta=0.1	O
delta=0.9	O
0.01	O
0.1	O
delta=0.01	O
delta=0.1	O
delta=0.9	O
0.01	O
0.1	O
c	O
figure	O
50.3.	O
the	O
number	O
of	O
degree-one	O
checks	O
s	O
(	O
upper	O
(	O
cid:12	O
)	O
gure	O
)	O
and	O
the	O
quantity	O
k	O
0	O
(	O
lower	O
(	O
cid:12	O
)	O
gure	O
)	O
as	O
a	O
function	B
of	O
the	O
two	O
parameters	B
c	O
and	O
(	O
cid:14	O
)	O
,	O
for	O
k	O
=	O
10	O
000.	O
luby	O
’	O
s	O
main	O
theorem	B
proves	O
that	O
there	O
exists	O
a	O
value	O
of	O
c	O
such	O
that	O
,	O
given	O
k	O
0	O
received	O
packets	O
,	O
the	O
decoding	B
algorithm	O
will	O
recover	O
the	O
k	O
source	O
packets	O
with	O
probability	O
1	O
(	O
cid:0	O
)	O
(	O
cid:14	O
)	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
50.4	O
:	O
applications	O
593	O
10000	O
10500	O
11000	O
11500	O
12000	O
10000	O
10500	O
11000	O
11500	O
12000	O
10000	O
10500	O
11000	O
11500	O
12000	O
figure	O
50.4.	O
histograms	O
of	O
the	O
actual	O
number	O
of	O
packets	O
n	O
required	O
in	O
order	O
to	O
recover	O
a	O
(	O
cid:12	O
)	O
le	O
of	O
size	O
k	O
=	O
10	O
000	O
packets	O
.	O
the	O
parameters	B
were	O
as	O
follows	O
:	O
top	O
histogram	O
:	O
c	O
=	O
0:01	O
,	O
(	O
cid:14	O
)	O
=	O
0:5	O
(	O
s	O
=	O
10	O
,	O
k=s	O
=	O
1010	O
,	O
and	O
z	O
’	O
1:01	O
)	O
;	O
middle	O
:	O
c	O
=	O
0:03	O
,	O
(	O
cid:14	O
)	O
=	O
0:5	O
(	O
s	O
=	O
30	O
,	O
k=s	O
=	O
337	O
,	O
and	O
z	O
’	O
1:03	O
)	O
;	O
bottom	O
:	O
c	O
=	O
0:1	O
,	O
(	O
cid:14	O
)	O
=	O
0:5	O
(	O
s	O
=	O
99	O
,	O
k=s	O
=	O
101	O
,	O
and	O
z	O
’	O
1:1	O
)	O
.	O
luby	O
’	O
s	O
(	O
2002	O
)	O
analysis	B
explains	O
how	O
the	O
small-d	O
end	O
of	O
(	O
cid:28	O
)	O
has	O
the	O
role	O
of	O
ensuring	O
that	O
the	O
decoding	B
process	O
gets	O
started	O
,	O
and	O
the	O
spike	O
in	O
(	O
cid:28	O
)	O
at	O
d	O
=	O
k=s	O
is	O
included	O
to	O
ensure	O
that	O
every	O
source	O
packet	O
is	O
likely	O
to	O
be	O
connected	O
to	O
a	O
check	O
at	O
least	O
once	O
.	O
luby	O
’	O
s	O
key	O
result	O
is	O
that	O
(	O
for	O
an	O
appropriate	O
value	O
of	O
the	O
constant	O
c	O
)	O
receiving	O
k0	O
=	O
k	O
+	O
2	O
ln	O
(	O
s=	O
(	O
cid:14	O
)	O
)	O
s	O
checks	O
ensures	O
that	O
all	O
packets	O
can	O
be	O
recovered	O
with	O
probability	O
at	O
least	O
1	O
(	O
cid:0	O
)	O
(	O
cid:14	O
)	O
.	O
in	O
the	O
illustrative	O
(	O
cid:12	O
)	O
gures	O
i	O
have	O
set	B
the	O
allowable	O
decoder	B
failure	O
probability	B
(	O
cid:14	O
)	O
quite	O
large	O
,	O
because	O
the	O
actual	O
failure	O
probability	B
is	O
much	O
smaller	O
than	O
is	O
suggested	O
by	O
luby	O
’	O
s	O
conservative	O
analysis	B
.	O
in	O
practice	O
,	O
lt	O
codes	O
can	O
be	O
tuned	O
so	O
that	O
a	O
(	O
cid:12	O
)	O
le	O
of	O
original	O
size	O
k	O
’	O
10	O
000	O
packets	O
is	O
recovered	O
with	O
an	O
overhead	O
of	O
about	O
5	O
%	O
.	O
figure	O
50.4	O
shows	O
his-	O
tograms	O
of	O
the	O
actual	O
number	O
of	O
packets	O
required	O
for	O
a	O
couple	O
of	O
settings	O
of	O
the	O
parameters	O
,	O
achieving	O
mean	B
overheads	O
smaller	O
than	O
5	O
%	O
and	O
10	O
%	O
respec-	O
tively	O
.	O
50.4	O
applications	O
digital	B
fountain	I
codes	O
are	O
an	O
excellent	O
solution	O
in	O
a	O
wide	O
variety	O
of	O
situations	O
.	O
let	O
’	O
s	O
mention	O
two	O
.	O
storage	O
you	O
wish	O
to	O
make	O
a	O
backup	O
of	O
a	O
large	O
(	O
cid:12	O
)	O
le	O
,	O
but	O
you	O
are	O
aware	O
that	O
your	O
magnetic	O
tapes	O
and	O
hard	O
drives	O
are	O
all	O
unreliable	O
in	O
the	O
sense	O
that	O
catastrophic	O
failures	O
,	O
in	O
which	O
some	O
stored	O
packets	O
are	O
permanently	O
lost	O
within	O
one	O
device	O
,	O
occur	O
at	O
a	O
rate	B
of	O
something	O
like	O
10	O
(	O
cid:0	O
)	O
3	O
per	O
day	O
.	O
how	O
should	O
you	O
store	O
your	O
(	O
cid:12	O
)	O
le	O
?	O
a	O
digital	B
fountain	I
can	O
be	O
used	O
to	O
spray	O
encoded	O
packets	O
all	O
over	O
the	O
place	O
,	O
on	O
every	O
storage	O
device	O
available	O
.	O
then	O
to	O
recover	O
the	O
backup	O
(	O
cid:12	O
)	O
le	O
,	O
whose	O
size	O
was	O
k	O
packets	O
,	O
one	O
simply	O
needs	O
to	O
(	O
cid:12	O
)	O
nd	O
k0	O
’	O
k	O
packets	O
from	O
anywhere	O
.	O
corrupted	O
packets	O
do	O
not	O
matter	O
;	O
we	O
simply	O
skip	O
over	O
them	O
and	O
(	O
cid:12	O
)	O
nd	O
more	O
packets	O
elsewhere	O
.	O
this	O
method	B
of	O
storage	O
also	O
has	O
advantages	O
in	O
terms	O
of	O
speed	O
of	O
(	O
cid:12	O
)	O
le	O
re-	O
covery	O
.	O
in	O
a	O
hard	B
drive	I
,	O
it	O
is	O
standard	O
practice	O
to	O
store	O
a	O
(	O
cid:12	O
)	O
le	O
in	O
successive	O
sectors	O
of	O
a	O
hard	B
drive	I
,	O
to	O
allow	O
rapid	O
reading	O
of	O
the	O
(	O
cid:12	O
)	O
le	O
;	O
but	O
if	O
,	O
as	O
occasion-	O
ally	O
happens	O
,	O
a	O
packet	B
is	O
lost	O
(	O
owing	O
to	O
the	O
reading	O
head	O
being	O
o	O
(	O
cid:11	O
)	O
track	O
for	O
a	O
moment	O
,	O
giving	O
a	O
burst	O
of	O
errors	B
that	O
can	O
not	O
be	O
corrected	O
by	O
the	O
packet	B
’	O
s	O
error-correcting	B
code	I
)	O
,	O
a	O
whole	O
revolution	O
of	O
the	O
drive	O
must	O
be	O
performed	O
to	O
bring	O
back	O
the	O
packet	B
to	O
the	O
head	O
for	O
a	O
second	O
read	O
.	O
the	O
time	O
taken	O
for	O
one	O
revolution	O
produces	O
an	O
undesirable	O
delay	O
in	O
the	O
(	O
cid:12	O
)	O
le	O
system	O
.	O
if	O
(	O
cid:12	O
)	O
les	O
were	O
instead	O
stored	O
using	O
the	O
digital	B
fountain	I
principle	O
,	O
with	O
the	O
digital	O
drops	O
stored	O
in	O
one	O
or	O
more	O
consecutive	O
sectors	O
on	O
the	O
drive	O
,	O
then	O
one	O
would	O
never	O
need	O
to	O
endure	O
the	O
delay	O
of	O
re-reading	O
a	O
packet	B
;	O
packet	B
loss	O
would	O
become	O
less	O
important	O
,	O
and	O
the	O
hard	B
drive	I
could	O
consequently	O
be	O
operated	O
faster	O
,	O
with	O
higher	O
noise	B
level	O
,	O
and	O
with	O
fewer	O
resources	O
devoted	O
to	O
noisy-	O
channel	B
coding	O
.	O
.	O
exercise	O
50.3	O
.	O
[	O
2	O
]	O
compare	O
the	O
digital	B
fountain	I
method	O
of	O
robust	O
storage	O
on	O
multiple	O
hard	O
drives	O
with	O
raid	O
(	O
the	O
redundant	B
array	I
of	I
independent	I
disks	I
)	O
.	O
broadcast	B
imagine	O
that	O
ten	O
thousand	O
subscribers	O
in	O
an	O
area	O
wish	O
to	O
receive	O
a	O
digital	O
movie	O
from	O
a	O
broadcaster	O
.	O
the	O
broadcaster	O
can	O
send	O
the	O
movie	B
in	O
packets	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
594	O
50	O
|	O
digital	B
fountain	I
codes	O
over	O
a	O
broadcast	B
network	O
{	O
for	O
example	O
,	O
by	O
a	O
wide-bandwidth	O
phone	B
line	O
,	O
or	O
by	O
satellite	O
.	O
imagine	O
that	O
not	O
all	O
packets	O
are	O
received	O
at	O
all	O
the	O
houses	O
.	O
let	O
’	O
s	O
say	O
f	O
=	O
0:1	O
%	O
of	O
them	O
are	O
lost	O
at	O
each	O
house	O
.	O
in	O
a	O
standard	O
approach	O
in	O
which	O
the	O
(	O
cid:12	O
)	O
le	O
is	O
transmitted	O
as	O
a	O
plain	O
sequence	B
of	O
packets	O
with	O
no	O
encoding	O
,	O
each	O
house	O
would	O
have	O
to	O
notify	O
the	O
broadcaster	O
of	O
the	O
f	O
k	O
missing	O
packets	O
,	O
and	O
request	O
that	O
they	O
be	O
retransmitted	O
.	O
and	O
with	O
ten	O
thousand	O
subscribers	O
all	O
requesting	O
such	O
retransmissions	O
,	O
there	O
would	O
be	O
a	O
retransmission	B
request	O
for	O
almost	O
every	O
packet	B
.	O
thus	O
the	O
broadcaster	O
would	O
have	O
to	O
repeat	O
the	O
entire	O
broadcast	B
twice	O
in	O
order	O
to	O
ensure	O
that	O
most	O
subscribers	O
have	O
received	O
the	O
whole	O
movie	B
,	O
and	O
most	O
users	O
would	O
have	O
to	O
wait	O
roughly	O
twice	O
as	O
long	O
as	O
the	O
ideal	O
time	O
before	O
the	O
download	O
was	O
complete	O
.	O
if	O
the	O
broadcaster	O
uses	O
a	O
digital	B
fountain	I
to	O
encode	O
the	O
movie	B
,	O
each	O
sub-	O
scriber	O
can	O
recover	O
the	O
movie	B
from	O
any	O
k0	O
’	O
k	O
packets	O
.	O
so	O
the	O
broadcast	B
needs	O
to	O
last	O
for	O
only	O
,	O
say	O
,	O
1.1k	O
packets	O
,	O
and	O
every	O
house	O
is	O
very	O
likely	O
to	O
have	O
successfully	O
recovered	O
the	O
whole	O
(	O
cid:12	O
)	O
le	O
.	O
another	O
application	O
is	O
broadcasting	O
data	O
to	O
cars	O
.	O
imagine	O
that	O
we	O
want	O
to	O
send	O
updates	O
to	O
in-car	B
navigation	I
databases	O
by	O
satellite	O
.	O
there	O
are	O
hundreds	O
of	O
thousands	O
of	O
vehicles	O
,	O
and	O
they	O
can	O
receive	O
data	O
only	O
when	O
they	O
are	O
out	O
on	O
the	O
open	O
road	O
;	O
there	O
are	O
no	O
feedback	B
channels	O
.	O
a	O
standard	O
method	O
for	O
sending	O
the	O
data	O
is	O
to	O
put	O
it	O
in	O
a	O
carousel	O
,	O
broadcasting	O
the	O
packets	O
in	O
a	O
(	O
cid:12	O
)	O
xed	O
periodic	O
sequence	O
.	O
‘	O
yes	O
,	O
a	O
car	O
may	O
go	O
through	O
a	O
tunnel	O
,	O
and	O
miss	O
out	O
on	O
a	O
few	O
hundred	O
packets	O
,	O
but	O
it	O
will	O
be	O
able	O
to	O
collect	O
those	O
missed	O
packets	O
an	O
hour	O
later	O
when	O
the	O
carousel	O
has	O
gone	O
through	O
a	O
full	O
revolution	O
(	O
we	O
hope	O
)	O
;	O
or	O
maybe	O
the	O
following	O
day	O
:	O
:	O
:	O
’	O
if	O
instead	O
the	O
satellite	O
uses	O
a	O
digital	B
fountain	I
,	O
each	O
car	O
needs	O
to	O
receive	O
only	O
an	O
amount	O
of	O
data	O
equal	O
to	O
the	O
original	O
(	O
cid:12	O
)	O
le	O
size	O
(	O
plus	O
5	O
%	O
)	O
.	O
further	O
reading	O
the	O
encoders	O
and	O
decoders	O
sold	O
by	O
digital	O
fountain	O
have	O
even	O
higher	O
e	O
(	O
cid:14	O
)	O
ciency	O
than	O
the	O
lt	O
codes	O
described	O
here	O
,	O
and	O
they	O
work	O
well	O
for	O
all	O
blocklengths	O
,	O
not	O
only	O
large	O
lengths	O
such	O
as	O
k	O
 	B
10	O
000.	O
shokrollahi	O
(	O
2003	O
)	O
presents	O
raptor	B
codes	I
,	O
which	O
are	O
an	O
extension	O
of	O
lt	O
codes	O
with	O
linear-time	O
encoding	O
and	O
de-	O
coding	O
.	O
50.5	O
further	O
exercises	O
.	O
exercise	O
50.4	O
.	O
[	O
2	O
]	O
understanding	O
the	O
robust	O
soliton	B
distribution	I
.	O
repeat	O
the	O
analysis	B
of	O
exercise	O
50.2	O
(	O
p.592	O
)	O
but	O
now	O
aim	O
to	O
have	O
the	O
expected	O
number	O
of	O
packets	O
of	O
degree	O
1	O
be	O
ht	O
(	O
1	O
)	O
=	O
1	O
+	O
s	O
for	O
all	O
t	O
,	O
instead	O
of	O
1.	O
show	O
that	O
the	O
initial	O
required	O
number	O
of	O
packets	O
is	O
h0	O
(	O
d	O
)	O
=	O
k	O
d	O
(	O
d	O
(	O
cid:0	O
)	O
1	O
)	O
+	O
s	O
d	O
for	O
d	O
>	O
1	O
.	O
(	O
50.6	O
)	O
the	O
reason	O
for	O
truncating	O
the	O
second	O
term	O
beyond	O
d	O
=	O
k=s	O
and	O
replac-	O
ing	O
it	O
by	O
the	O
spike	O
at	O
d	O
=	O
k=s	O
(	O
see	O
equation	O
(	O
50.4	O
)	O
)	O
is	O
to	O
ensure	O
that	O
the	O
decoding	B
complexity	O
does	O
not	O
grow	O
larger	O
than	O
o	O
(	O
k	O
ln	O
k	O
)	O
.	O
estimate	O
the	O
expected	O
number	O
of	O
packets	O
pd	O
h0	O
(	O
d	O
)	O
and	O
the	O
expected	O
number	O
of	O
edges	O
in	O
the	O
sparse	B
graph	I
pd	O
h0	O
(	O
d	O
)	O
d	O
(	O
which	O
determines	O
the	O
decoding	B
complexity	O
)	O
if	O
the	O
histogram	O
of	O
packets	O
is	O
as	O
given	O
in	O
(	O
50.6	O
)	O
.	O
compare	O
with	O
the	O
expected	O
numbers	O
of	O
packets	O
and	O
edges	O
when	O
the	O
robust	O
soliton	B
distribution	I
(	O
50.4	O
)	O
is	O
used	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
50.5	O
:	O
further	O
exercises	O
595	O
exercise	O
50.5	O
.	O
[	O
4	O
]	O
show	O
that	O
the	O
spike	O
at	O
d	O
=	O
k=s	O
(	O
equation	O
(	O
50.4	O
)	O
)	O
is	O
an	O
ade-	O
quate	O
replacement	O
for	O
the	O
tail	B
of	O
high-weight	O
packets	O
in	O
(	O
50.6	O
)	O
.	O
exercise	O
50.6	O
.	O
[	O
3c	O
]	O
investigate	O
experimentally	O
how	O
necessary	O
the	O
spike	O
at	O
d	O
=	O
k=s	O
(	O
equation	O
(	O
50.4	O
)	O
)	O
is	O
for	O
successful	O
decoding	B
.	O
investigate	O
also	O
whether	O
the	O
tail	B
of	O
(	O
cid:26	O
)	O
(	O
d	O
)	O
beyond	O
d	O
=	O
k=s	O
is	O
necessary	O
.	O
what	O
happens	O
if	O
all	O
high-	O
weight	B
degrees	O
are	O
removed	O
,	O
both	O
the	O
spike	O
at	O
d	O
=	O
k=s	O
and	O
the	O
tail	B
of	O
(	O
cid:26	O
)	O
(	O
d	O
)	O
beyond	O
d	O
=	O
k=s	O
?	O
exercise	O
50.7	O
.	O
[	O
4	O
]	O
fill	O
in	O
the	O
details	O
in	O
the	O
proof	O
of	O
luby	O
’	O
s	O
main	O
theorem	B
,	O
that	O
receiving	O
k0	O
=	O
k	O
+	O
2	O
ln	O
(	O
s=	O
(	O
cid:14	O
)	O
)	O
s	O
checks	O
ensures	O
that	O
all	O
the	O
source	O
packets	O
can	O
be	O
recovered	O
with	O
probability	O
at	O
least	O
1	O
(	O
cid:0	O
)	O
(	O
cid:14	O
)	O
.	O
exercise	O
50.8	O
.	O
[	O
4c	O
]	O
optimize	O
the	O
degree	B
distribution	O
of	O
a	O
digital	B
fountain	I
code	O
for	O
a	O
(	O
cid:12	O
)	O
le	O
of	O
k	O
=	O
10	O
000	O
packets	O
.	O
pick	O
a	O
sensible	O
objective	B
function	I
for	O
your	O
optimization	B
,	O
such	O
as	O
minimizing	O
the	O
mean	B
of	O
n	O
,	O
the	O
number	O
of	O
packets	O
required	O
for	O
complete	O
decoding	B
,	O
or	O
the	O
95th	O
percentile	O
of	O
the	O
histogram	O
of	O
n	O
(	O
(	O
cid:12	O
)	O
gure	O
50.4	O
)	O
.	O
.	O
exercise	O
50.9	O
.	O
[	O
3	O
]	O
make	O
a	O
model	B
of	O
the	O
situation	O
where	O
a	O
data	O
stream	O
is	O
broad-	O
cast	O
to	O
cars	O
,	O
and	O
quantify	O
the	O
advantage	O
that	O
the	O
digital	B
fountain	I
has	O
over	O
the	O
carousel	O
method	B
.	O
exercise	O
50.10	O
.	O
[	O
2	O
]	O
construct	O
a	O
simple	O
example	O
to	O
illustrate	O
the	O
fact	O
that	O
the	O
digital	B
fountain	I
decoder	O
of	O
section	O
50.2	O
is	O
suboptimal	O
{	O
it	O
sometimes	O
gives	O
up	O
even	O
though	O
the	O
information	B
available	O
is	O
su	O
(	O
cid:14	O
)	O
cient	O
to	O
decode	O
the	O
whole	O
(	O
cid:12	O
)	O
le	O
.	O
how	O
does	O
the	O
cost	O
of	O
the	O
optimal	B
decoder	I
compare	O
?	O
.	O
exercise	O
50.11	O
.	O
[	O
2	O
]	O
if	O
every	O
transmitted	O
packet	B
were	O
created	O
by	O
adding	O
together	O
source	O
packets	O
at	O
random	B
with	O
probability	B
1/2	O
of	O
each	O
source	O
packet	O
’	O
s	O
being	O
included	O
,	O
show	O
that	O
the	O
probability	B
that	O
k0	O
=	O
k	O
received	O
packets	O
su	O
(	O
cid:14	O
)	O
ce	O
for	O
the	O
optimal	B
decoder	I
to	O
be	O
able	O
to	O
recover	O
the	O
k	O
source	O
packets	O
is	O
just	O
a	O
little	O
below	O
1=2	O
.	O
[	O
to	O
put	O
it	O
another	O
way	O
,	O
what	O
is	O
the	O
probability	B
that	O
a	O
random	B
k	O
(	O
cid:2	O
)	O
k	O
matrix	B
has	O
full	O
rank	O
?	O
]	O
show	O
that	O
if	O
k0	O
=	O
k	O
+	O
(	O
cid:1	O
)	O
packets	O
are	O
received	O
,	O
the	O
probability	B
that	O
they	O
will	O
not	O
su	O
(	O
cid:14	O
)	O
ce	O
for	O
the	O
optimal	B
decoder	I
is	O
roughly	O
2	O
(	O
cid:0	O
)	O
(	O
cid:1	O
)	O
.	O
.	O
exercise	O
50.12	O
.	O
[	O
4c	O
]	O
implement	O
an	O
optimal	B
digital	O
fountain	O
decoder	O
that	O
uses	O
the	O
method	B
of	O
richardson	O
and	O
urbanke	O
(	O
2001b	O
)	O
derived	O
for	O
fast	O
encod-	O
ing	O
of	O
sparse-graph	O
codes	O
(	O
section	B
47.7	O
)	O
to	O
handle	O
the	O
matrix	B
inversion	O
required	O
for	O
optimal	O
decoding	B
.	O
now	O
that	O
you	O
have	O
changed	O
the	O
decoder	B
,	O
you	O
can	O
reoptimize	O
the	O
degree	B
distribution	O
,	O
using	O
higher-weight	O
packets	O
.	O
by	O
how	O
much	O
can	O
you	O
reduce	O
the	O
overhead	O
?	O
con	O
(	O
cid:12	O
)	O
rm	O
the	O
assertion	O
that	O
this	O
approach	O
makes	O
digital	B
fountain	I
codes	O
viable	O
as	O
erasure-correcting	O
codes	O
for	O
all	O
blocklengths	O
,	O
not	O
just	O
the	O
large	O
blocklengths	O
for	O
which	O
lt	O
codes	O
are	O
excellent	O
.	O
.	O
exercise	O
50.13	O
.	O
[	O
5	O
]	O
digital	B
fountain	I
codes	O
are	O
excellent	O
rateless	B
codes	O
for	O
erasure	O
channels	O
.	O
make	O
a	O
rateless	B
code	I
for	O
a	O
channel	B
that	O
has	O
both	O
erasures	O
and	O
noise	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
596	O
50	O
|	O
digital	B
fountain	I
codes	O
50.6	O
summary	B
of	O
sparse-graph	O
codes	O
a	O
simple	O
method	O
for	O
designing	O
error-correcting	B
codes	I
for	O
noisy	B
channels	O
,	O
(	O
cid:12	O
)	O
rst	O
pioneered	O
by	O
gallager	O
(	O
1962	O
)	O
,	O
has	O
recently	O
been	O
rediscovered	O
and	O
generalized	O
,	O
and	B
communication	I
theory	O
has	O
been	O
transformed	O
.	O
the	O
practical	B
performance	O
of	O
gallager	O
’	O
s	O
low-density	B
parity-check	I
codes	O
and	O
their	O
modern	O
cousins	O
is	O
vastly	O
better	O
than	O
the	O
performance	O
of	O
the	O
codes	O
with	O
which	O
textbooks	O
have	O
been	O
(	O
cid:12	O
)	O
lled	O
in	O
the	O
intervening	O
years	O
.	O
which	O
sparse-graph	B
code	I
is	O
‘	O
best	O
’	O
for	O
a	O
noisy	B
channel	I
depends	O
on	O
the	O
cho-	O
sen	O
rate	B
and	O
blocklength	O
,	O
the	O
permitted	O
encoding	O
and	O
decoding	O
complexity	B
,	O
and	O
the	O
question	O
of	O
whether	O
occasional	O
undetected	O
errors	B
are	O
acceptable	O
.	O
low-	O
density	B
parity-check	O
codes	O
are	O
the	O
most	O
versatile	O
;	O
it	O
’	O
s	O
easy	O
to	O
make	O
a	O
compet-	O
itive	O
low-density	B
parity-check	I
code	I
with	O
almost	O
any	O
rate	B
and	O
blocklength	O
,	O
and	O
low-density	O
parity-check	O
codes	O
virtually	O
never	O
make	O
undetected	O
errors	B
.	O
for	O
the	O
special	O
case	O
of	O
the	O
erasure	O
channel	B
,	O
the	O
sparse-graph	O
codes	O
that	O
are	O
best	O
are	O
digital	B
fountain	I
codes	O
.	O
50.7	O
conclusion	O
the	O
best	O
solution	O
to	O
the	O
communication	B
problem	O
is	O
:	O
combine	O
a	O
simple	O
,	O
pseudo-random	O
code	B
with	O
a	O
message-passing	B
decoder	O
.	O
 	B
 	I
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
part	O
vii	O
appendices	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
a	O
notation	B
what	O
does	O
p	O
(	O
aj	O
b	O
;	O
c	O
)	O
mean	B
?	O
p	O
(	O
aj	O
b	O
;	O
c	O
)	O
is	O
pronounced	O
‘	O
the	O
probability	B
that	O
a	O
is	O
true	O
given	O
that	O
b	O
is	O
true	O
and	O
c	O
is	O
true	O
’	O
.	O
or	O
,	O
more	O
brie	O
(	O
cid:13	O
)	O
y	O
,	O
‘	O
the	O
probability	O
of	O
a	O
given	O
b	O
and	O
c	O
’	O
.	O
(	O
see	O
chapter	O
2	O
,	O
p.22	O
.	O
)	O
what	O
do	O
log	O
and	O
ln	O
mean	B
?	O
in	O
this	O
book	O
,	O
log	O
x	O
means	O
the	O
base-two	O
loga-	O
rithm	O
,	O
log2	O
x	O
;	O
ln	O
x	O
means	O
the	O
natural	B
logarithm	O
,	O
loge	O
x.	O
what	O
does	O
^s	O
mean	B
?	O
usually	O
,	O
a	O
‘	O
hat	B
’	O
over	O
a	O
variable	O
denotes	O
a	O
guess	O
or	O
es-	O
timator	O
.	O
so	O
^s	O
is	O
a	O
guess	O
at	O
the	O
value	O
of	O
s.	O
integrals	O
.	O
there	O
is	O
no	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
r	O
f	O
(	O
u	O
)	O
du	O
andr	O
du	O
f	O
(	O
u	O
)	O
.	O
the	O
inte-	O
grand	O
is	O
f	O
(	O
u	O
)	O
in	O
both	O
cases	O
.	O
what	O
does	O
n=1	O
but	O
it	O
denotes	O
a	O
product	O
.	O
it	O
’	O
s	O
pronounced	O
‘	O
product	O
over	O
n	O
from	O
1	O
to	O
n	O
’	O
.	O
so	O
,	O
for	O
example	O
,	O
n	O
yn=1	O
n	O
yn=1	O
mean	B
?	O
this	O
is	O
like	O
the	O
summation	O
pn	O
n	O
=	O
1	O
(	O
cid:2	O
)	O
2	O
(	O
cid:2	O
)	O
3	O
(	O
cid:2	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:2	O
)	O
n	O
=	O
n	O
!	O
=	O
exp	O
''	O
n	O
xn=1	O
ln	O
n	O
#	O
:	O
(	O
a.1	O
)	O
i	O
like	O
to	O
choose	O
the	O
name	O
of	O
the	O
free	O
variable	O
in	O
a	O
sum	O
or	O
a	O
product	O
{	O
here	O
,	O
n	O
{	O
to	O
be	O
the	O
lower	O
case	O
version	O
of	O
the	O
range	O
of	O
the	O
sum	O
.	O
so	O
n	O
usually	O
runs	O
from	O
1	O
to	O
n	O
,	O
and	O
m	O
usually	O
runs	O
from	O
1	O
to	O
m	O
.	O
this	O
is	O
a	O
habit	O
i	O
learnt	O
from	O
yaser	O
abu-mostafa	O
,	O
and	O
i	O
think	O
it	O
makes	O
formulae	O
easier	O
to	O
understand	O
.	O
what	O
does	O
(	O
cid:18	O
)	O
n	O
n	O
(	O
cid:19	O
)	O
mean	B
?	O
this	O
is	O
pronounced	O
‘	O
n	O
choose	O
n	O
’	O
,	O
and	O
it	O
is	O
the	O
number	O
of	O
ways	O
of	O
selecting	O
an	O
unordered	O
set	B
of	O
n	O
objects	O
from	O
a	O
set	B
of	O
size	O
n	O
.	O
n	O
(	O
cid:19	O
)	O
=	O
(	O
cid:18	O
)	O
n	O
n	O
!	O
(	O
n	O
(	O
cid:0	O
)	O
n	O
)	O
!	O
n	O
!	O
:	O
(	O
a.2	O
)	O
this	O
function	B
is	O
known	O
as	O
the	O
combination	B
function	O
.	O
what	O
is	O
(	O
cid:0	O
)	O
(	O
x	O
)	O
?	O
the	O
gamma	B
function	I
is	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
(	O
cid:0	O
)	O
(	O
x	O
)	O
(	O
cid:17	O
)	O
r	O
10	O
du	O
ux	O
(	O
cid:0	O
)	O
1e	O
(	O
cid:0	O
)	O
u	O
,	O
for	O
x	O
>	O
0.	O
the	O
gamma	B
function	I
is	O
an	O
extension	O
of	O
the	O
factorial	O
function	B
to	O
real	O
number	O
arguments	O
.	O
in	O
general	O
,	O
(	O
cid:0	O
)	O
(	O
x	O
+	O
1	O
)	O
=	O
x	O
(	O
cid:0	O
)	O
(	O
x	O
)	O
,	O
and	O
for	O
integer	O
arguments	O
,	O
(	O
cid:0	O
)	O
(	O
x	O
+	O
1	O
)	O
=	O
x	O
!	O
.	O
the	O
digamma	B
function	I
is	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
(	O
cid:9	O
)	O
(	O
x	O
)	O
(	O
cid:17	O
)	O
d	O
dx	O
ln	O
(	O
cid:0	O
)	O
(	O
x	O
)	O
.	O
for	O
large	O
x	O
(	O
for	O
practical	O
purposes	O
,	O
0:1	O
(	O
cid:20	O
)	O
x	O
(	O
cid:20	O
)	O
1	O
)	O
,	O
ln	O
(	O
cid:0	O
)	O
(	O
x	O
)	O
’	O
(	O
cid:0	O
)	O
x	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
cid:1	O
)	O
ln	O
(	O
x	O
)	O
(	O
cid:0	O
)	O
x	O
+	O
1	O
598	O
2	O
ln	O
2	O
(	O
cid:25	O
)	O
+	O
o	O
(	O
1=x	O
)	O
;	O
(	O
a.3	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
a	O
|	O
notation	B
599	O
and	O
for	O
small	O
x	O
(	O
for	O
practical	O
purposes	O
,	O
0	O
(	O
cid:20	O
)	O
x	O
(	O
cid:20	O
)	O
0:5	O
)	O
:	O
ln	O
(	O
cid:0	O
)	O
(	O
x	O
)	O
’	O
ln	O
where	O
(	O
cid:13	O
)	O
e	O
is	O
euler	O
’	O
s	O
constant	O
.	O
1	O
x	O
(	O
cid:0	O
)	O
(	O
cid:13	O
)	O
ex	O
+	O
o	O
(	O
x2	O
)	O
(	O
a.4	O
)	O
what	O
does	O
h	O
(	O
cid:0	O
)	O
1	O
2	O
(	O
1	O
(	O
cid:0	O
)	O
r=c	O
)	O
mean	B
?	O
just	O
as	O
sin	O
(	O
cid:0	O
)	O
1	O
(	O
s	O
)	O
denotes	O
the	O
inverse	O
func-	O
tion	O
to	O
s	O
=	O
sin	O
(	O
x	O
)	O
,	O
so	O
h	O
(	O
cid:0	O
)	O
1	O
there	O
is	O
potential	O
confusion	O
when	O
people	O
use	O
sin2	O
x	O
to	O
denote	O
(	O
sin	O
x	O
)	O
2	O
,	O
since	O
then	O
we	O
might	O
expect	O
sin	O
(	O
cid:0	O
)	O
1	O
s	O
to	O
denote	O
1=	O
sin	O
(	O
s	O
)	O
;	O
i	O
therefore	O
like	O
to	O
avoid	O
using	O
the	O
notation	B
sin2	O
x	O
.	O
2	O
(	O
h	O
)	O
is	O
the	O
inverse	O
function	O
to	O
h	O
=	O
h2	O
(	O
x	O
)	O
.	O
what	O
does	O
f0	O
(	O
x	O
)	O
mean	B
?	O
the	O
answer	O
depends	O
on	O
the	O
context	O
.	O
often	O
,	O
a	O
‘	O
prime	O
’	O
is	O
used	O
to	O
denote	O
di	O
(	O
cid:11	O
)	O
erentiation	O
:	O
f0	O
(	O
x	O
)	O
(	O
cid:17	O
)	O
d	O
dx	O
f	O
(	O
x	O
)	O
;	O
similarly	O
,	O
a	O
dot	O
denotes	O
di	O
(	O
cid:11	O
)	O
erentiation	O
with	O
respect	O
to	O
time	O
,	O
t	O
:	O
d	O
dt	O
x	O
:	O
_x	O
(	O
cid:17	O
)	O
(	O
a.5	O
)	O
(	O
a.6	O
)	O
however	O
,	O
the	O
prime	O
is	O
also	O
a	O
useful	O
indicator	O
for	O
‘	O
another	O
variable	O
’	O
,	O
for	O
example	O
‘	O
a	O
new	O
value	O
for	O
a	O
variable	O
’	O
.	O
so	O
,	O
for	O
example	O
,	O
x0	O
might	O
denote	O
‘	O
the	O
new	O
value	O
of	O
x	O
’	O
.	O
also	O
,	O
if	O
there	O
are	O
two	O
integers	O
that	O
both	O
range	O
from	O
1	O
to	O
n	O
,	O
i	O
will	O
often	O
name	O
those	O
integers	O
n	O
and	O
n0	O
.	O
so	O
my	O
rule	O
is	O
:	O
if	O
a	O
prime	O
occurs	O
in	O
an	O
expression	O
that	O
could	O
be	O
a	O
func-	O
tion	O
,	O
such	O
as	O
f0	O
(	O
x	O
)	O
or	O
h0	O
(	O
y	O
)	O
,	O
then	O
it	O
denotes	O
di	O
(	O
cid:11	O
)	O
erentiation	O
;	O
otherwise	O
it	O
indicates	O
‘	O
another	O
variable	O
’	O
.	O
what	O
is	O
the	O
error	B
function	I
?	O
de	O
(	O
cid:12	O
)	O
nitions	O
of	O
this	O
function	B
vary	O
.	O
i	O
de	O
(	O
cid:12	O
)	O
ne	O
it	O
to	O
be	O
the	O
cumulative	O
probability	O
of	O
a	O
standard	O
(	O
variance	B
=	O
1	O
)	O
normal	B
distribution	O
,	O
(	O
cid:8	O
)	O
(	O
z	O
)	O
(	O
cid:17	O
)	O
z	O
z	O
(	O
cid:0	O
)	O
1	O
exp	O
(	O
(	O
cid:0	O
)	O
z2=2	O
)	O
=p2	O
(	O
cid:25	O
)	O
dz	O
:	O
(	O
a.7	O
)	O
what	O
does	O
e	O
(	O
r	O
)	O
mean	B
?	O
e	O
[	O
r	O
]	O
is	O
pronounced	O
‘	O
the	O
expected	O
value	O
of	O
r	O
’	O
or	O
‘	O
the	O
expectation	B
of	O
r	O
’	O
,	O
and	O
it	O
is	O
the	O
mean	B
value	O
of	O
r.	O
another	O
symbol	O
for	O
‘	O
expected	O
value	O
’	O
is	O
the	O
pair	O
of	O
angle-brackets	O
,	O
hri	O
:	O
what	O
does	O
jxj	O
mean	B
?	O
the	O
vertical	O
bars	O
‘	O
j	O
(	O
cid:1	O
)	O
j	O
’	O
have	O
two	O
meanings	O
.	O
if	O
a	O
is	O
a	O
set	B
,	O
then	O
jaj	O
denotes	O
the	O
number	O
of	O
elements	O
in	O
the	O
set	B
;	O
if	O
x	O
is	O
a	O
number	O
,	O
then	O
jxj	O
is	O
the	O
absolute	B
value	I
of	O
x.	O
what	O
does	O
[	O
ajp	O
]	O
mean	B
?	O
here	O
,	O
a	O
and	O
p	O
are	O
matrices	B
with	O
the	O
same	O
num-	O
ber	O
of	O
rows	O
.	O
[	O
ajp	O
]	O
denotes	O
the	O
double-width	O
matrix	B
obtained	O
by	O
putting	O
a	O
alongside	O
p.	O
the	O
vertical	O
bar	O
is	O
used	O
to	O
avoid	O
confusion	O
with	O
the	O
product	O
ap	O
.	O
what	O
does	O
xt	O
mean	B
?	O
the	O
superscript	O
t	O
is	O
pronounced	O
‘	O
transpose	O
’	O
.	O
trans-	O
posing	O
a	O
row-vector	O
turns	O
it	O
into	O
a	O
column	O
vector	O
:	O
(	O
1	O
;	O
2	O
;	O
3	O
)	O
t	O
=0	O
@	O
1	O
2	O
3	O
1	O
a	O
;	O
(	O
a.8	O
)	O
and	O
vice	O
versa	O
.	O
are	O
column	O
vectors	B
.	O
]	O
[	O
normally	O
my	O
vectors	B
,	O
indicated	O
by	O
bold	O
face	O
type	O
(	O
x	O
)	O
,	O
similarly	O
,	O
matrices	B
can	O
be	O
transposed	O
.	O
if	O
mij	O
is	O
the	O
entry	O
in	O
row	O
i	O
and	O
column	O
j	O
of	O
matrix	O
m	O
,	O
and	O
n	O
=	O
mt	O
,	O
then	O
nji	O
=	O
mij	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
600	O
a	O
|	O
notation	B
what	O
are	O
trace	O
m	O
and	O
det	O
m	O
?	O
the	O
trace	O
of	O
a	O
matrix	B
is	O
the	O
sum	O
of	O
its	O
di-	O
agonal	O
elements	O
,	O
trace	O
m	O
=xi	O
mii	O
:	O
(	O
a.9	O
)	O
the	O
determinant	O
of	O
m	O
is	O
denoted	O
det	O
m.	O
what	O
does	O
(	O
cid:14	O
)	O
mn	O
mean	B
?	O
the	O
(	O
cid:14	O
)	O
matrix	B
is	O
the	O
identity	B
matrix	I
.	O
(	O
cid:14	O
)	O
mn	O
=	O
(	O
cid:26	O
)	O
1	O
0	O
if	O
m	O
=	O
n	O
if	O
m	O
6=	O
n.	O
another	O
name	O
for	O
the	O
identity	B
matrix	I
is	O
i	O
or	O
1.	O
sometimes	O
i	O
include	O
a	O
subscript	O
on	O
this	O
symbol	O
{	O
1k	O
{	O
which	O
indicates	O
the	O
size	O
of	O
the	O
matrix	O
(	O
k	O
(	O
cid:2	O
)	O
k	O
)	O
.	O
what	O
does	O
(	O
cid:14	O
)	O
(	O
x	O
)	O
mean	B
?	O
the	O
delta	B
function	I
has	O
the	O
property	O
z	O
dx	O
f	O
(	O
x	O
)	O
(	O
cid:14	O
)	O
(	O
x	O
)	O
=	O
f	O
(	O
0	O
)	O
:	O
(	O
a.10	O
)	O
another	O
possible	O
meaning	O
for	O
(	O
cid:14	O
)	O
(	O
s	O
)	O
is	O
the	O
truth	B
function	I
,	O
which	O
is	O
1	O
if	O
the	O
proposition	O
s	O
is	O
true	O
but	O
i	O
have	O
adopted	O
another	O
notation	B
for	O
that	O
.	O
after	O
all	O
,	O
the	O
symbol	O
(	O
cid:14	O
)	O
is	O
quite	O
busy	O
already	O
,	O
with	O
the	O
two	O
roles	O
mentioned	O
above	O
in	O
addition	O
to	O
its	O
role	O
as	O
a	O
small	O
real	O
number	O
(	O
cid:14	O
)	O
and	O
an	O
increment	O
operator	O
(	O
as	O
in	O
(	O
cid:14	O
)	O
x	O
)	O
!	O
what	O
does	O
	O
[	O
s	O
]	O
mean	B
?	O
	O
[	O
s	O
]	O
is	O
the	O
truth	B
function	I
,	O
which	O
is	O
1	O
if	O
the	O
propo-	O
sition	O
s	O
is	O
true	O
and	O
0	O
otherwise	O
.	O
for	O
example	O
,	O
the	O
number	O
of	O
positive	O
numbers	O
in	O
the	O
set	B
t	O
=	O
f	O
(	O
cid:0	O
)	O
2	O
;	O
1	O
;	O
3g	O
can	O
be	O
written	O
	O
[	O
x	O
>	O
0	O
]	O
:	O
xx2t	O
(	O
a.11	O
)	O
what	O
is	O
the	O
di	O
(	O
cid:11	O
)	O
erence	O
between	O
‘	O
:	O
=	O
’	O
and	O
‘	O
=	O
’	O
?	O
in	O
an	O
algorithm	B
,	O
x	O
:	O
=	O
y	O
means	O
that	O
the	O
variable	O
x	O
is	O
updated	O
by	O
assigning	O
it	O
the	O
value	O
of	O
y.	O
in	O
contrast	O
,	O
x	O
=	O
y	O
is	O
a	O
proposition	O
,	O
a	O
statement	O
that	O
x	O
is	O
equal	O
to	O
y.	O
see	O
chapters	O
23	O
and	O
29	O
for	O
further	O
de	O
(	O
cid:12	O
)	O
nitions	O
and	O
notation	O
relating	O
to	O
probability	B
distributions	I
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
b	O
some	O
physics	B
b.1	O
about	O
phase	O
transitions	O
a	O
system	O
with	O
states	O
x	O
in	O
contact	O
with	O
a	O
heat	B
bath	I
at	O
temperature	B
t	O
=	O
1=	O
(	O
cid:12	O
)	O
has	O
probability	B
distribution	O
p	O
(	O
xj	O
(	O
cid:12	O
)	O
)	O
=	O
1	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
)	O
)	O
:	O
the	O
partition	B
function	I
is	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
=xx	O
exp	O
(	O
(	O
cid:0	O
)	O
(	O
cid:12	O
)	O
e	O
(	O
x	O
)	O
)	O
:	O
(	O
b.1	O
)	O
(	O
b.2	O
)	O
the	O
inverse	O
temperature	O
(	O
cid:12	O
)	O
can	O
be	O
interpreted	O
as	O
de	O
(	O
cid:12	O
)	O
ning	O
an	O
exchange	B
rate	I
between	O
entropy	B
and	O
energy	B
.	O
(	O
1=	O
(	O
cid:12	O
)	O
)	O
is	O
the	O
amount	O
of	O
energy	O
that	O
must	O
be	O
given	O
to	O
a	O
heat	B
bath	I
to	O
increase	O
its	O
entropy	B
by	O
one	O
nat	O
.	O
often	O
,	O
the	O
system	O
will	O
be	O
a	O
(	O
cid:11	O
)	O
ected	O
by	O
some	O
other	O
parameters	B
such	O
as	O
the	O
volume	B
of	O
the	O
box	B
it	O
is	O
in	O
,	O
v	O
,	O
in	O
which	O
case	O
z	O
is	O
a	O
function	B
of	O
v	O
too	O
,	O
z	O
(	O
(	O
cid:12	O
)	O
;	O
v	O
)	O
.	O
for	O
any	O
system	O
with	O
a	O
(	O
cid:12	O
)	O
nite	O
number	O
of	O
states	O
,	O
the	O
function	B
z	O
(	O
(	O
cid:12	O
)	O
)	O
is	O
evi-	O
dently	O
a	O
continuous	B
function	O
of	O
(	O
cid:12	O
)	O
,	O
since	O
it	O
is	O
simply	O
a	O
sum	O
of	O
exponentials	O
.	O
moreover	O
,	O
all	O
the	O
derivatives	O
of	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
with	O
respect	O
to	O
(	O
cid:12	O
)	O
are	O
continuous	B
too	O
.	O
what	O
phase	O
transitions	O
are	O
all	O
about	O
,	O
however	O
,	O
is	O
this	O
:	O
phase	O
transitions	O
correspond	O
to	O
values	O
of	O
(	O
cid:12	O
)	O
and	O
v	O
(	O
called	O
critical	O
points	O
)	O
at	O
which	O
the	O
derivatives	O
of	O
z	O
have	O
discontinuities	O
or	O
divergences	O
.	O
immediately	O
we	O
can	O
deduce	O
:	O
only	O
systems	O
with	O
an	O
in	O
(	O
cid:12	O
)	O
nite	O
number	O
of	O
states	O
can	O
show	O
phase	O
transitions	O
.	O
often	O
,	O
we	O
include	O
a	O
parameter	O
n	O
describing	O
the	O
size	O
of	O
the	O
system	O
.	O
phase	O
transitions	O
may	O
appear	O
in	O
the	O
limit	O
n	O
!	O
1.	O
real	O
systems	O
may	O
have	O
a	O
value	O
of	O
n	O
like	O
1023.	O
if	O
we	O
make	O
the	O
system	O
large	O
by	O
simply	O
grouping	O
together	O
n	O
independent	O
systems	O
whose	O
partition	B
function	I
is	O
z	O
(	O
1	O
)	O
(	O
(	O
cid:12	O
)	O
)	O
,	O
then	O
nothing	O
interesting	O
happens	O
.	O
the	O
partition	B
function	I
for	O
n	O
independent	O
identical	O
systems	O
is	O
simply	O
z	O
(	O
n	O
)	O
(	O
(	O
cid:12	O
)	O
)	O
=	O
[	O
z	O
(	O
1	O
)	O
(	O
(	O
cid:12	O
)	O
)	O
]	O
n	O
:	O
(	O
b.3	O
)	O
now	O
,	O
while	O
this	O
function	B
z	O
(	O
n	O
)	O
(	O
(	O
cid:12	O
)	O
)	O
may	O
be	O
a	O
very	O
rapidly	O
varying	O
function	B
of	O
(	O
cid:12	O
)	O
,	O
that	O
doesn	O
’	O
t	O
mean	B
it	O
is	O
showing	O
phase	O
transitions	O
.	O
the	O
natural	B
way	O
to	O
look	O
at	O
the	O
partition	B
function	I
is	O
in	O
the	O
logarithm	O
ln	O
z	O
(	O
n	O
)	O
(	O
(	O
cid:12	O
)	O
)	O
=	O
n	O
ln	O
z	O
(	O
1	O
)	O
(	O
(	O
cid:12	O
)	O
)	O
:	O
(	O
b.4	O
)	O
601	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
602	O
b	O
|	O
some	O
physics	B
duplicating	O
the	O
original	O
system	O
n	O
times	O
simply	O
scales	O
up	O
all	O
properties	O
like	O
the	O
energy	B
and	O
heat	B
capacity	I
of	O
the	O
system	O
by	O
a	O
factor	O
of	O
n	O
.	O
so	O
if	O
the	O
original	O
system	O
showed	O
no	O
phase	O
transitions	O
then	O
the	O
scaled	O
up	O
system	O
won	O
’	O
t	O
have	O
any	O
either	O
.	O
only	O
systems	O
with	O
long-range	O
correlations	B
show	O
phase	O
transitions	O
.	O
long-range	O
correlations	B
do	O
not	O
require	O
long-range	O
energetic	O
couplings	O
;	O
for	O
example	O
,	O
a	O
magnet	B
has	O
only	O
short-range	O
couplings	O
(	O
between	O
adjacent	O
spins	O
)	O
but	O
these	O
are	O
su	O
(	O
cid:14	O
)	O
cient	O
to	O
create	O
long-range	O
order	O
.	O
why	O
are	O
points	O
at	O
which	O
derivatives	O
diverge	O
interesting	O
?	O
the	O
derivatives	O
of	O
ln	O
z	O
describe	O
properties	O
like	O
the	O
heat	B
capacity	I
of	O
the	O
sys-	O
tem	O
(	O
that	O
’	O
s	O
the	O
second	O
derivative	O
)	O
or	O
its	O
(	O
cid:13	O
)	O
uctuations	O
in	O
energy	O
.	O
if	O
the	O
second	O
derivative	O
of	O
ln	O
z	O
diverges	O
at	O
a	O
temperature	B
1=	O
(	O
cid:12	O
)	O
,	O
then	O
the	O
heat	B
capacity	I
of	O
the	O
system	O
diverges	O
there	O
,	O
which	O
means	O
it	O
can	O
absorb	O
or	O
release	O
energy	B
without	O
changing	O
temperature	B
(	O
think	O
of	O
ice	O
melting	O
in	O
ice	O
water	O
)	O
;	O
when	O
the	O
system	O
is	O
at	O
equilibrium	O
at	O
that	O
temperature	B
,	O
its	O
energy	B
(	O
cid:13	O
)	O
uctuates	O
a	O
lot	O
,	O
in	O
contrast	O
to	O
the	O
normal	B
law-of-large-numbers	O
behaviour	O
,	O
where	O
the	O
energy	B
only	O
varies	O
by	O
one	O
part	O
in	O
pn	O
.	O
a	O
toy	O
system	O
that	O
shows	O
a	O
phase	B
transition	I
imagine	O
a	O
collection	O
of	O
n	O
coupled	O
spins	O
that	O
have	O
the	O
following	O
energy	B
as	O
a	O
function	B
of	O
their	O
state	O
x	O
2	O
f0	O
;	O
1gn	O
.	O
e	O
(	O
x	O
)	O
=	O
(	O
cid:26	O
)	O
(	O
cid:0	O
)	O
n	O
(	O
cid:15	O
)	O
x	O
=	O
(	O
0	O
;	O
0	O
;	O
0	O
;	O
:	O
:	O
:	O
;	O
0	O
)	O
otherwise	O
.	O
(	O
b.5	O
)	O
0	O
this	O
energy	B
function	O
describes	O
a	O
ground	O
state	O
in	O
which	O
all	O
the	O
spins	O
are	O
aligned	O
in	O
the	O
zero	O
direction	O
;	O
the	O
energy	B
per	O
spin	O
in	O
this	O
state	O
is	O
(	O
cid:0	O
)	O
(	O
cid:15	O
)	O
.	O
if	O
any	O
spin	O
changes	O
state	O
then	O
the	O
energy	B
is	O
zero	O
.	O
this	O
model	B
is	O
like	O
an	O
extreme	O
version	O
of	O
a	O
magnetic	O
interaction	O
,	O
which	O
encourages	O
pairs	O
of	O
spins	O
to	O
be	O
aligned	O
.	O
we	O
can	O
contrast	O
it	O
with	O
an	O
ordinary	O
system	O
of	O
n	O
independent	O
spins	O
whose	O
energy	B
is	O
:	O
e0	O
(	O
x	O
)	O
=	O
(	O
cid:15	O
)	O
xn	O
(	O
2xn	O
(	O
cid:0	O
)	O
1	O
)	O
:	O
(	O
b.6	O
)	O
like	O
the	O
(	O
cid:12	O
)	O
rst	O
system	O
,	O
the	O
system	O
of	O
independent	O
spins	O
has	O
a	O
single	O
ground	O
state	O
(	O
0	O
;	O
0	O
;	O
0	O
;	O
:	O
:	O
:	O
;	O
0	O
)	O
with	O
energy	O
(	O
cid:0	O
)	O
n	O
(	O
cid:15	O
)	O
,	O
and	O
it	O
has	O
roughly	O
2n	O
states	O
with	O
energy	O
very	O
close	O
to	O
0	O
,	O
so	O
the	O
low-temperature	O
and	O
high-temperature	O
properties	O
of	O
the	O
independent-spin	O
system	O
and	O
the	O
coupled-spin	O
system	O
are	O
virtually	O
identical	O
.	O
the	O
partition	B
function	I
of	O
the	O
coupled-spin	O
system	O
is	O
the	O
function	B
is	O
sketched	O
in	O
(	O
cid:12	O
)	O
gure	O
b.1a	O
along	O
with	O
its	O
low	O
temperature	B
behaviour	O
,	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
=	O
e	O
(	O
cid:12	O
)	O
n	O
(	O
cid:15	O
)	O
+	O
2n	O
(	O
cid:0	O
)	O
1	O
:	O
ln	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
=	O
ln	O
(	O
cid:16	O
)	O
e	O
(	O
cid:12	O
)	O
n	O
(	O
cid:15	O
)	O
+	O
2n	O
(	O
cid:0	O
)	O
1	O
(	O
cid:17	O
)	O
ln	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
’	O
n	O
(	O
cid:12	O
)	O
(	O
cid:15	O
)	O
;	O
(	O
cid:12	O
)	O
!	O
1	O
;	O
and	O
its	O
high	O
temperature	O
behaviour	O
,	O
ln	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
’	O
n	O
ln	O
2	O
;	O
(	O
cid:12	O
)	O
!	O
0	O
:	O
(	O
b.7	O
)	O
(	O
b.8	O
)	O
(	O
b.9	O
)	O
(	O
b.10	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
b.1	O
:	O
about	O
phase	O
transitions	O
603	O
log	O
z	O
n	O
beta	B
epsilon	O
n	O
log	O
(	O
2	O
)	O
var	O
(	O
e	O
)	O
n=24	O
var	O
(	O
e	O
)	O
n=8	O
(	O
a	O
)	O
beta	B
(	O
c	O
)	O
beta	B
figure	O
b.1	O
.	O
(	O
a	O
)	O
partition	B
function	I
of	O
toy	O
system	O
which	O
shows	O
a	O
phase	B
transition	I
for	O
large	O
n	O
.	O
the	O
arrow	O
marks	O
the	O
point	O
(	O
cid:12	O
)	O
c	O
=	O
log	O
2=	O
(	O
cid:15	O
)	O
.	O
(	O
b	O
)	O
the	O
same	O
,	O
for	O
larger	O
n	O
.	O
(	O
c	O
)	O
the	O
variance	B
of	O
the	O
energy	B
of	O
the	O
system	O
as	O
a	O
function	B
of	O
(	O
cid:12	O
)	O
for	O
two	O
system	O
sizes	O
.	O
as	O
n	O
increases	O
the	O
variance	B
has	O
an	O
increasingly	O
sharp	O
peak	O
at	O
the	O
critical	O
point	O
(	O
cid:12	O
)	O
c.	O
contrast	O
with	O
(	O
cid:12	O
)	O
gure	O
b.2	O
.	O
figure	O
b.2	O
.	O
the	O
partition	B
function	I
(	O
a	O
)	O
and	O
energy-variance	O
(	O
b	O
)	O
of	O
a	O
system	O
consisting	O
of	O
n	O
independent	O
spins	O
.	O
the	O
partition	B
function	I
changes	O
gradually	O
from	O
one	O
asymptote	O
to	O
the	O
other	O
,	O
regardless	O
of	O
how	O
large	O
n	O
is	O
;	O
the	O
variance	B
of	O
the	O
energy	B
does	O
not	O
have	O
a	O
peak	O
.	O
the	O
(	O
cid:13	O
)	O
uctuations	O
are	O
largest	O
at	O
high	O
temperature	O
(	O
small	O
(	O
cid:12	O
)	O
)	O
and	O
scale	O
linearly	O
with	O
system	O
size	O
n	O
.	O
log	O
z	O
n	O
beta	B
epsilon	O
n	O
log	O
(	O
2	O
)	O
(	O
b	O
)	O
beta	B
log	O
z	O
n	O
beta	B
epsilon	O
n	O
log	O
(	O
2	O
)	O
var	O
(	O
e	O
)	O
n=24	O
var	O
(	O
e	O
)	O
n=8	O
(	O
a	O
)	O
beta	B
(	O
b	O
)	O
beta	B
the	O
arrow	O
marks	O
the	O
point	O
(	O
cid:12	O
)	O
=	O
ln	O
2	O
(	O
cid:15	O
)	O
(	O
b.11	O
)	O
at	O
which	O
these	O
two	O
asymptotes	O
intersect	O
.	O
in	O
the	O
limit	O
n	O
!	O
1	O
,	O
the	O
graph	B
of	O
ln	O
z	O
(	O
(	O
cid:12	O
)	O
)	O
becomes	O
more	O
and	O
more	O
sharply	O
bent	O
at	O
this	O
point	O
(	O
(	O
cid:12	O
)	O
gure	O
b.1b	O
)	O
.	O
the	O
second	O
derivative	O
of	O
ln	O
z	O
,	O
which	O
describes	O
the	O
variance	B
of	O
the	O
energy	B
of	O
the	O
system	O
,	O
has	O
a	O
peak	O
value	O
,	O
at	O
(	O
cid:12	O
)	O
=	O
ln	O
2=	O
(	O
cid:15	O
)	O
,	O
roughly	O
equal	O
to	O
n	O
2	O
(	O
cid:15	O
)	O
2	O
4	O
;	O
(	O
b.12	O
)	O
which	O
corresponds	O
to	O
the	O
system	O
spending	O
half	O
of	O
its	O
time	O
in	O
the	O
ground	O
state	O
and	O
half	O
its	O
time	O
in	O
the	O
other	O
states	O
.	O
at	O
this	O
critical	O
point	O
,	O
the	O
heat	B
capacity	I
of	O
this	O
system	O
is	O
thus	O
proportional	O
to	O
n	O
2	O
;	O
the	O
heat	B
capacity	I
per	O
spin	O
is	O
proportional	O
to	O
n	O
,	O
which	O
,	O
for	O
in	O
(	O
cid:12	O
)	O
nite	O
n	O
,	O
is	O
in	O
(	O
cid:12	O
)	O
nite	O
,	O
in	O
contrast	O
to	O
the	O
behaviour	O
of	O
systems	O
away	O
from	O
phase	O
transitions	O
,	O
whose	O
capacity	B
per	O
atom	O
is	O
a	O
(	O
cid:12	O
)	O
nite	O
number	O
.	O
for	O
comparison	O
,	O
(	O
cid:12	O
)	O
gure	O
b.2	O
shows	O
the	O
partition	B
function	I
and	O
energy-variance	O
of	O
the	O
ordinary	O
independent-spin	O
system	O
.	O
more	O
generally	O
phase	O
transitions	O
can	O
be	O
categorized	O
into	O
‘	O
(	O
cid:12	O
)	O
rst-order	O
’	O
and	O
‘	O
continuous	B
’	O
transi-	O
tions	O
.	O
in	O
a	O
(	O
cid:12	O
)	O
rst-order	O
phase	B
transition	I
,	O
there	O
is	O
a	O
discontinuous	O
change	O
of	O
one	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
604	O
b	O
|	O
some	O
physics	B
or	O
more	O
order-parameters	O
;	O
in	O
a	O
continuous	B
transition	O
,	O
all	O
order-parameters	O
change	O
continuously	O
.	O
[	O
what	O
’	O
s	O
an	O
order-parameter	O
?	O
{	O
a	O
scalar	O
function	B
of	O
the	O
state	O
of	O
the	O
system	O
;	O
or	O
,	O
to	O
be	O
precise	O
,	O
the	O
expectation	B
of	O
such	O
a	O
function	B
.	O
]	O
in	O
the	O
vicinity	O
of	O
a	O
critical	O
point	O
,	O
the	O
concept	O
of	O
‘	O
typicality	B
’	O
de	O
(	O
cid:12	O
)	O
ned	O
in	O
chapter	O
4	O
does	O
not	O
hold	O
.	O
for	O
example	O
,	O
our	O
toy	O
system	O
,	O
at	O
its	O
critical	O
point	O
,	O
has	O
a	O
50	O
%	O
chance	O
of	O
being	O
in	O
a	O
state	O
with	O
energy	B
(	O
cid:0	O
)	O
n	O
(	O
cid:15	O
)	O
,	O
and	O
roughly	O
a	O
1=2n	O
+1	O
chance	O
of	O
being	O
in	O
each	O
of	O
the	O
other	O
states	O
that	O
have	O
energy	B
zero	O
.	O
it	O
is	O
thus	O
not	O
the	O
case	O
that	O
ln	O
1=p	O
(	O
x	O
)	O
is	O
very	O
likely	O
to	O
be	O
close	O
to	O
the	O
entropy	B
of	O
the	O
system	O
at	O
this	O
point	O
,	O
unlike	O
a	O
system	O
with	O
n	O
i.i.d	O
.	O
components	O
.	O
remember	O
that	O
information	B
content	I
(	O
ln	O
1=p	O
(	O
x	O
)	O
)	O
and	O
energy	O
are	O
very	O
closely	O
related	O
.	O
if	O
typicality	B
holds	O
,	O
then	O
the	O
system	O
’	O
s	O
energy	B
has	O
negligible	O
(	O
cid:13	O
)	O
uctua-	O
tions	O
,	O
and	O
vice	O
versa	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
c	O
some	O
mathematics	O
c.1	O
finite	O
(	O
cid:12	O
)	O
eld	O
theory	B
most	O
linear	B
codes	I
are	O
expressed	O
in	O
the	O
language	O
of	O
galois	O
theory	B
why	O
are	O
galois	O
(	O
cid:12	O
)	O
elds	O
an	O
appropriate	O
language	O
for	O
linear	B
codes	I
?	O
first	O
,	O
a	O
de	O
(	O
cid:12	O
)	O
-	O
nition	O
and	O
some	O
examples	O
.	O
a	O
(	O
cid:12	O
)	O
eld	O
f	O
is	O
a	O
set	B
f	O
=	O
f0	O
;	O
f	O
0g	O
such	O
that	O
1.	O
f	O
forms	O
an	O
abelian	O
group	O
under	O
an	O
addition	O
operation	O
‘	O
+	O
’	O
,	O
with	O
0	O
being	O
the	O
identity	O
;	O
[	O
abelian	O
means	O
all	O
elements	O
commute	O
,	O
i.e.	O
,	O
satisfy	O
a	O
+	O
b	O
=	O
b	O
+	O
a	O
.	O
]	O
2.	O
f	O
0	O
forms	O
an	O
abelian	O
group	O
under	O
a	O
multiplication	O
operation	O
‘	O
(	O
cid:1	O
)	O
’	O
;	O
multiplication	O
of	O
any	O
element	O
by	O
0	O
yields	O
0	O
;	O
3.	O
these	O
operations	O
satisfy	O
the	O
distributive	O
rule	O
(	O
a	O
+	O
b	O
)	O
(	O
cid:1	O
)	O
c	O
=	O
a	O
(	O
cid:1	O
)	O
c	O
+	O
b	O
(	O
cid:1	O
)	O
c.	O
for	O
example	O
,	O
the	O
real	O
numbers	O
form	O
a	O
(	O
cid:12	O
)	O
eld	O
,	O
with	O
‘	O
+	O
’	O
and	O
‘	O
(	O
cid:1	O
)	O
’	O
denoting	O
ordinary	O
addition	O
and	O
multiplication	O
.	O
a	O
galois	O
(	O
cid:12	O
)	O
eld	O
gf	O
(	O
q	O
)	O
is	O
a	O
(	O
cid:12	O
)	O
eld	O
with	O
a	O
(	O
cid:12	O
)	O
nite	O
number	O
of	O
elements	O
q.	O
a	O
unique	O
galois	O
(	O
cid:12	O
)	O
eld	O
exists	O
for	O
any	O
q	O
=	O
pm	O
,	O
where	O
p	O
is	O
a	O
prime	O
number	O
and	O
m	O
is	O
a	O
positive	O
integer	O
;	O
there	O
are	O
no	O
other	O
(	O
cid:12	O
)	O
nite	O
(	O
cid:12	O
)	O
elds	O
.	O
gf	O
(	O
2	O
)	O
:	O
the	O
addition	O
and	O
multiplication	O
tables	O
for	O
gf	O
(	O
2	O
)	O
are	O
shown	O
in	O
ta-	O
ble	O
c.1	O
.	O
these	O
are	O
the	O
rules	B
of	O
addition	O
and	O
multiplication	O
modulo	O
2.	O
gf	O
(	O
p	O
)	O
:	O
for	O
any	O
prime	O
number	O
p	O
,	O
the	O
addition	O
and	O
multiplication	O
rules	B
are	O
those	O
for	O
ordinary	O
addition	O
and	O
multiplication	O
,	O
modulo	O
p.	O
gf	O
(	O
4	O
)	O
:	O
the	O
rules	B
for	O
gf	O
(	O
pm	O
)	O
,	O
with	O
m	O
>	O
1	O
,	O
are	O
not	O
those	O
of	O
ordinary	O
addition	O
and	O
multiplication	O
.	O
for	O
example	O
the	O
tables	O
for	O
gf	O
(	O
4	O
)	O
(	O
table	O
c.2	O
)	O
are	O
not	O
the	O
rules	B
of	O
addition	O
and	O
multiplication	O
modulo	O
4.	O
notice	O
that	O
1	O
+	O
1	O
=	O
0	O
,	O
for	O
example	O
.	O
so	O
how	O
can	O
gf	O
(	O
4	O
)	O
be	O
described	O
?	O
it	O
turns	O
out	O
that	O
the	O
elements	O
can	O
be	O
related	O
to	O
polynomials	O
.	O
consider	O
polynomial	O
functions	B
of	O
x	O
of	O
degree	O
1	O
and	O
with	O
coe	O
(	O
cid:14	O
)	O
cients	O
that	O
are	O
elements	O
of	O
gf	O
(	O
2	O
)	O
.	O
the	O
polynomials	O
shown	O
in	O
table	O
c.3	O
obey	O
the	O
addition	O
and	O
multiplication	O
rules	B
of	O
gf	O
(	O
4	O
)	O
if	O
addition	O
and	O
multiplication	O
are	O
modulo	O
the	O
polynomial	O
x2	O
+	O
x	O
+	O
1	O
,	O
and	O
the	O
coe	O
(	O
cid:14	O
)	O
cients	O
of	O
the	O
polynomials	O
are	O
from	O
gf	O
(	O
2	O
)	O
.	O
for	O
example	O
,	O
b	O
(	O
cid:1	O
)	O
b	O
=	O
x2	O
+	O
(	O
1	O
+	O
1	O
)	O
x	O
+	O
1	O
=	O
x	O
=	O
a.	O
each	O
element	O
may	O
also	O
be	O
represented	O
as	O
a	O
bit	B
pattern	O
as	O
shown	O
in	O
table	O
c.3	O
,	O
with	O
addition	O
being	O
bitwise	B
modulo	O
2	O
,	O
and	O
multiplication	O
de	O
(	O
cid:12	O
)	O
ned	O
with	O
an	O
appropriate	O
carry	O
operation	O
.	O
605	O
+	O
0	O
1	O
0	O
0	O
1	O
1	O
0	O
1	O
(	O
cid:1	O
)	O
0	O
1	O
0	O
0	O
0	O
1	O
0	O
1	O
table	O
c.1	O
.	O
addition	O
and	O
multiplication	O
tables	O
for	O
gf	O
(	O
2	O
)	O
.	O
1	O
a	O
b	O
+	O
0	O
1	O
a	O
b	O
0	O
0	O
0	O
b	O
a	O
1	O
1	O
1	O
a	O
a	O
b	O
0	O
0	O
b	O
b	O
a	O
1	O
(	O
cid:1	O
)	O
1	O
a	O
b	O
0	O
0	O
0	O
1	O
1	O
a	O
b	O
a	O
0	O
a	O
b	O
1	O
b	O
0	O
b	O
1	O
a	O
0	O
0	O
0	O
0	O
table	O
c.2	O
.	O
addition	O
and	O
multiplication	O
tables	O
for	O
gf	O
(	O
4	O
)	O
.	O
element	O
polynomial	O
bit	B
pattern	O
0	O
1	O
a	O
b	O
0	O
1	O
x	O
x	O
+	O
1	O
00	O
01	O
10	O
11	O
table	O
c.3	O
.	O
representations	O
of	O
the	O
elements	O
of	O
gf	O
(	O
4	O
)	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
606	O
c	O
|	O
some	O
mathematics	O
gf	O
(	O
8	O
)	O
.	O
we	O
can	O
denote	O
the	O
elements	O
of	O
gf	O
(	O
8	O
)	O
by	O
f0	O
;	O
1	O
;	O
a	O
;	O
b	O
;	O
c	O
;	O
d	O
;	O
e	O
;	O
fg	O
.	O
each	O
element	O
can	O
be	O
mapped	O
onto	O
a	O
polynomial	O
over	O
gf	O
(	O
2	O
)	O
.	O
the	O
multiplica-	O
tion	O
and	O
addition	O
operations	O
are	O
given	O
by	O
multiplication	O
and	O
addition	O
of	O
the	O
polynomials	O
,	O
modulo	O
x3	O
+	O
x	O
+	O
1.	O
the	O
multiplication	O
table	O
is	O
given	O
below	O
.	O
element	O
polynomial	O
binary	O
representation	O
0	O
1	O
a	O
b	O
c	O
d	O
e	O
f	O
0	O
1	O
x	O
x	O
+	O
1	O
x2	O
x2	O
+	O
1	O
x2	O
+	O
x	O
x2	O
+	O
x	O
+	O
1	O
000	O
001	O
010	O
011	O
100	O
101	O
110	O
111	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
0	O
(	O
cid:1	O
)	O
1	O
a	O
b	O
c	O
d	O
e	O
f	O
0	O
0	O
0	O
1	O
1	O
a	O
b	O
c	O
d	O
e	O
f	O
a	O
0	O
a	O
c	O
e	O
b	O
1	O
f	O
d	O
b	O
0	O
b	O
e	O
d	O
f	O
c	O
1	O
a	O
c	O
0	O
c	O
b	O
f	O
e	O
a	O
d	O
1	O
d	O
0	O
d	O
1	O
c	O
a	O
f	O
b	O
e	O
e	O
0	O
e	O
f	O
1	O
d	O
b	O
a	O
c	O
f	O
0	O
f	O
d	O
a	O
1	O
e	O
c	O
b	O
why	O
are	O
galois	O
(	O
cid:12	O
)	O
elds	O
relevant	O
to	O
linear	B
codes	I
?	O
imagine	O
generalizing	O
a	O
binary	O
generator	O
matrix	B
g	O
and	O
binary	O
vector	O
s	O
to	O
a	O
matrix	B
and	O
vector	O
with	O
elements	O
from	O
a	O
larger	O
set	B
,	O
and	O
generalizing	O
the	O
addition	O
and	O
multiplication	O
operations	O
that	O
de	O
(	O
cid:12	O
)	O
ne	O
the	O
product	O
gs	O
.	O
in	O
order	O
to	O
produce	O
an	O
appropriate	O
input	O
for	O
a	O
symmetric	B
channel	I
,	O
it	O
would	O
be	O
convenient	O
if	O
,	O
for	O
random	O
s	O
,	O
the	O
product	O
gs	O
produced	O
all	O
elements	O
in	O
the	O
enlarged	O
set	B
with	O
equal	O
probability	B
.	O
this	O
uniform	O
distribution	B
is	O
easiest	O
to	O
guarantee	O
if	O
these	O
elements	O
form	O
a	O
group	O
under	O
both	O
addition	O
and	O
multiplication	O
,	O
because	O
then	O
these	O
operations	O
do	O
not	O
break	O
the	O
symmetry	O
among	O
the	O
elements	O
.	O
when	O
two	O
random	B
elements	O
of	O
a	O
multiplicative	O
group	O
are	O
multiplied	O
together	O
,	O
all	O
elements	O
are	O
produced	O
with	O
equal	O
probability	B
.	O
this	O
is	O
not	O
true	O
of	O
other	O
sets	O
such	O
as	O
the	O
integers	O
,	O
for	O
which	O
the	O
multiplication	O
operation	O
is	O
more	O
likely	O
to	O
give	O
rise	O
to	O
some	O
elements	O
(	O
the	O
composite	O
numbers	O
)	O
than	O
others	O
.	O
galois	O
(	O
cid:12	O
)	O
elds	O
,	O
by	O
their	O
de	O
(	O
cid:12	O
)	O
nition	O
,	O
avoid	O
such	O
symmetry-breaking	O
e	O
(	O
cid:11	O
)	O
ects	O
.	O
c.2	O
eigenvectors	O
and	O
eigenvalues	O
a	O
right-eigenvector	O
of	O
a	O
square	B
matrix	O
a	O
is	O
a	O
non-zero	O
vector	O
er	O
that	O
satis	O
(	O
cid:12	O
)	O
es	O
aer	O
=	O
(	O
cid:21	O
)	O
er	O
;	O
(	O
c.1	O
)	O
where	O
(	O
cid:21	O
)	O
is	O
the	O
eigenvalue	B
associated	O
with	O
that	O
eigenvector	O
.	O
the	O
eigenvalue	B
may	O
be	O
a	O
real	O
number	O
or	O
complex	B
number	O
and	O
it	O
may	O
be	O
zero	O
.	O
eigenvectors	O
may	O
be	O
real	O
or	O
complex	B
.	O
a	O
left-eigenvector	O
of	O
a	O
matrix	B
a	O
is	O
a	O
vector	O
el	O
that	O
satis	O
(	O
cid:12	O
)	O
es	O
la	O
=	O
(	O
cid:21	O
)	O
et	O
et	O
l	O
:	O
(	O
c.2	O
)	O
the	O
following	O
statements	O
for	O
right-eigenvectors	O
also	O
apply	O
to	O
left-eigenvectors	O
.	O
(	O
cid:15	O
)	O
if	O
a	O
matrix	B
has	O
two	O
or	O
more	O
linearly	O
independent	O
right-eigenvectors	O
with	O
the	O
same	O
eigenvalue	B
then	O
that	O
eigenvalue	B
is	O
called	O
a	O
degenerate	O
eigenvalue	B
of	O
the	O
matrix	B
,	O
or	O
a	O
repeated	O
eigenvalue	B
.	O
any	O
linear	B
combination	O
of	O
those	O
eigenvectors	O
is	O
another	O
right-eigenvector	O
with	O
the	O
same	O
eigenvalue	B
.	O
(	O
cid:15	O
)	O
the	O
principal	O
right-eigenvector	O
of	O
a	O
matrix	B
is	O
,	O
by	O
de	O
(	O
cid:12	O
)	O
nition	O
,	O
the	O
right-	O
eigenvector	O
with	O
the	O
largest	O
associated	O
eigenvalue	B
.	O
(	O
cid:15	O
)	O
if	O
a	O
real	O
matrix	B
has	O
a	O
right-eigenvector	O
with	O
complex	O
eigenvalue	B
(	O
cid:21	O
)	O
=	O
x	O
+	O
yi	O
then	O
it	O
also	O
has	O
a	O
right-eigenvector	O
with	O
the	O
conjugate	O
eigenvalue	O
(	O
cid:21	O
)	O
(	O
cid:3	O
)	O
=	O
x	O
(	O
cid:0	O
)	O
yi	O
.	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
c.2	O
:	O
eigenvectors	O
and	O
eigenvalues	O
607	O
symmetric	B
matrices	O
if	O
a	O
is	O
a	O
real	O
symmetric	B
n	O
(	O
cid:2	O
)	O
n	O
matrix	B
then	O
1.	O
all	O
the	O
eigenvalues	O
and	O
eigenvectors	O
of	O
a	O
are	O
real	O
;	O
2.	O
every	O
left-eigenvector	O
of	O
a	O
is	O
also	O
a	O
right-eigenvector	O
of	O
a	O
with	O
the	O
same	O
eigenvalue	B
,	O
and	O
vice	O
versa	O
;	O
3.	O
a	O
set	B
of	O
n	O
eigenvectors	O
and	O
eigenvalues	O
fe	O
(	O
a	O
)	O
;	O
(	O
cid:21	O
)	O
agn	O
are	O
orthonormal	O
,	O
that	O
is	O
,	O
a=1	O
can	O
be	O
found	O
that	O
e	O
(	O
a	O
)	O
(	O
cid:1	O
)	O
e	O
(	O
b	O
)	O
=	O
(	O
cid:14	O
)	O
ab	O
;	O
(	O
c.3	O
)	O
the	O
matrix	B
can	O
be	O
expressed	O
as	O
a	O
weighted	O
sum	O
of	O
outer	O
products	O
of	O
the	O
eigenvectors	O
:	O
a	O
=	O
n	O
xa=1	O
(	O
cid:21	O
)	O
a	O
[	O
e	O
(	O
a	O
)	O
]	O
[	O
e	O
(	O
a	O
)	O
]	O
t	O
:	O
(	O
c.4	O
)	O
(	O
whereas	O
i	O
often	O
use	O
i	O
and	O
n	O
as	O
indices	O
for	O
sets	O
of	O
size	O
i	O
and	O
n	O
,	O
i	O
will	O
use	O
the	O
indices	O
a	O
and	O
b	O
to	O
run	O
over	O
eigenvectors	O
,	O
even	O
if	O
there	O
are	O
n	O
of	O
them	O
.	O
this	O
is	O
to	O
avoid	O
confusion	O
with	O
the	O
components	O
of	O
the	O
eigenvectors	O
,	O
which	O
are	O
indexed	O
by	O
n	O
,	O
e.g	O
.	O
e	O
(	O
a	O
)	O
n	O
.	O
)	O
general	O
square	O
matrices	B
an	O
n	O
(	O
cid:2	O
)	O
n	O
matrix	B
can	O
have	O
up	O
to	O
n	O
distinct	O
eigenvalues	O
.	O
generically	O
,	O
there	O
are	O
n	O
eigenvalues	O
,	O
all	O
distinct	O
,	O
and	O
each	O
has	O
one	O
left-eigenvector	O
and	O
one	O
right-	O
eigenvector	O
.	O
in	O
cases	O
where	O
two	O
or	O
more	O
eigenvalues	O
coincide	O
,	O
for	O
each	O
distinct	O
eigenvalue	B
that	O
is	O
non-zero	O
there	O
is	O
at	O
least	O
one	O
left-eigenvector	O
and	O
one	O
right-	O
eigenvector	O
.	O
left-	O
and	O
right-eigenvectors	O
that	O
have	O
di	O
(	O
cid:11	O
)	O
erent	O
eigenvalue	B
are	O
orthogonal	O
,	O
that	O
is	O
,	O
if	O
(	O
cid:21	O
)	O
a	O
6=	O
(	O
cid:21	O
)	O
b	O
then	O
e	O
(	O
a	O
)	O
l	O
(	O
cid:1	O
)	O
e	O
(	O
b	O
)	O
r	O
=	O
0	O
:	O
(	O
c.5	O
)	O
non-negative	O
matrices	B
de	O
(	O
cid:12	O
)	O
nition	O
.	O
if	O
all	O
the	O
elements	O
of	O
a	O
non-zero	O
matrix	B
c	O
satisfy	O
cmn	O
(	O
cid:21	O
)	O
0	O
then	O
c	O
is	O
a	O
non-negative	O
matrix	B
.	O
similarly	O
,	O
if	O
all	O
the	O
elements	O
of	O
a	O
non-zero	O
vector	O
c	O
satisfy	O
cn	O
(	O
cid:21	O
)	O
0	O
then	O
c	O
is	O
a	O
non-negative	O
vector	O
.	O
properties	O
.	O
a	O
non-negative	O
matrix	B
has	O
a	O
principal	O
eigenvector	O
that	O
is	O
non-	O
negative	O
.	O
it	O
may	O
also	O
have	O
other	O
eigenvectors	O
with	O
the	O
same	O
eigenvalue	B
that	O
are	O
not	O
non-negative	O
.	O
but	O
if	O
the	O
principal	O
eigenvalue	B
of	O
a	O
non-negative	O
matrix	B
is	O
not	O
degenerate	O
,	O
then	O
the	O
matrix	B
has	O
only	O
one	O
principal	O
eigenvector	O
e	O
(	O
1	O
)	O
,	O
and	O
it	O
is	O
non-negative	O
.	O
generically	O
,	O
all	O
the	O
other	O
eigenvalues	O
are	O
smaller	O
in	O
absolute	O
magnitude	O
.	O
[	O
there	O
can	O
be	O
several	O
eigenvalues	O
of	O
identical	O
magnitude	O
in	O
special	O
cases	O
.	O
]	O
transition	B
probability	I
matrices	O
an	O
important	O
example	O
of	O
a	O
non-negative	O
matrix	B
is	O
a	O
transition	B
probability	I
matrix	O
q.	O
de	O
(	O
cid:12	O
)	O
nition	O
.	O
a	O
transition	B
probability	I
matrix	O
q	O
has	O
columns	O
that	O
are	O
probability	B
vectors	O
,	O
that	O
is	O
,	O
it	O
satis	O
(	O
cid:12	O
)	O
es	O
q	O
(	O
cid:21	O
)	O
0	O
and	O
qij	O
=	O
1	O
for	O
all	O
j	O
:	O
(	O
c.6	O
)	O
xi	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
608	O
matrix	B
2	O
4	O
1	O
2	O
0	O
1	O
1	O
0	O
0	O
0	O
13	O
5	O
1	O
1	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
0	O
1	O
2	O
664	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
0	O
0	O
0	O
0	O
0	O
1	O
1	O
3	O
775	O
2	O
664	O
:60	O
:37	O
:37	O
:60	O
1:62	O
3	O
775	O
2	O
664	O
:60	O
:37	O
:37	O
:60	O
3	O
775	O
eigenvalues	O
and	O
eigenvectors	O
el	O
;	O
er	O
c	O
|	O
some	O
mathematics	O
2:41	O
2	O
4	O
:58	O
:82	O
03	O
5	O
0:5+0:9i	O
2	O
664	O
3	O
:1	O
(	O
cid:0	O
)	O
:5i	O
(	O
cid:0	O
)	O
:3	O
(	O
cid:0	O
)	O
:4i	O
775	O
:3+:4i	O
(	O
cid:0	O
)	O
:1+:5i	O
03	O
(	O
cid:0	O
)	O
:82	O
:58	O
5	O
1	O
0	O
0	O
0	O
0	O
1:62	O
:82	O
:58	O
2	O
4	O
(	O
cid:0	O
)	O
0:41	O
03	O
2	O
2	O
13	O
13	O
03	O
2	O
2	O
(	O
cid:0	O
)	O
:58	O
:82	O
5	O
4	O
4	O
5	O
5	O
5	O
4	O
4	O
(	O
cid:0	O
)	O
0:62	O
(	O
cid:0	O
)	O
:53	O
(	O
cid:21	O
)	O
(	O
cid:0	O
)	O
:53	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
:85	O
:85	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
:85	O
:85	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
:53	O
(	O
cid:20	O
)	O
:53	O
0:5	O
(	O
cid:0	O
)	O
0:9i	O
2	O
3	O
2	O
3	O
2	O
:1	O
(	O
cid:0	O
)	O
:5i	O
:1+:5i	O
(	O
cid:0	O
)	O
:3+:4i	O
:3+:4i	O
664	O
775	O
664	O
775	O
664	O
:3	O
(	O
cid:0	O
)	O
:4i	O
(	O
cid:0	O
)	O
:3	O
(	O
cid:0	O
)	O
:4i	O
(	O
cid:0	O
)	O
:1	O
(	O
cid:0	O
)	O
:5i	O
(	O
cid:0	O
)	O
:1+:5i	O
3	O
:1+:5i	O
:3	O
(	O
cid:0	O
)	O
:4i	O
775	O
(	O
cid:0	O
)	O
:3+:4i	O
(	O
cid:0	O
)	O
:1	O
(	O
cid:0	O
)	O
:5i	O
(	O
cid:0	O
)	O
0:62	O
2	O
3	O
:37	O
(	O
cid:0	O
)	O
:60	O
664	O
775	O
(	O
cid:0	O
)	O
:60	O
:37	O
3	O
:37	O
(	O
cid:0	O
)	O
:60	O
775	O
(	O
cid:0	O
)	O
:60	O
:37	O
2	O
664	O
table	O
c.4	O
.	O
some	O
matrices	B
and	O
their	O
eigenvectors	O
.	O
matrix	B
(	O
cid:20	O
)	O
0	O
:38	O
1	O
:62	O
(	O
cid:21	O
)	O
1	O
:65	O
:543	O
2	O
5	O
4	O
0	O
:35	O
0	O
0	O
0	O
:46	O
2	O
4	O
eigenvalues	O
and	O
eigenvectors	O
el	O
;	O
er	O
1	O
(	O
cid:0	O
)	O
0:38	O
(	O
cid:20	O
)	O
:71	O
:71	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
:36	O
:93	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
(	O
cid:0	O
)	O
:93	O
:36	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
(	O
cid:0	O
)	O
:71	O
:71	O
(	O
cid:21	O
)	O
(	O
cid:0	O
)	O
0:2+0:3i	O
(	O
cid:0	O
)	O
0:2	O
(	O
cid:0	O
)	O
0:3i	O
2	O
:2	O
(	O
cid:0	O
)	O
:2i3	O
2	O
:4+:3i3	O
2	O
:2+:2i3	O
(	O
cid:0	O
)	O
:8	O
(	O
cid:0	O
)	O
:1i	O
:2	O
(	O
cid:0	O
)	O
:5i	O
(	O
cid:0	O
)	O
:8+:1i	O
(	O
cid:0	O
)	O
:2+:5i	O
(	O
cid:0	O
)	O
:6+:2i	O
(	O
cid:0	O
)	O
:2	O
(	O
cid:0	O
)	O
:5i	O
4	O
5	O
5	O
4	O
5	O
4	O
:4	O
(	O
cid:0	O
)	O
:3i3	O
:2+:5i	O
(	O
cid:0	O
)	O
:6	O
(	O
cid:0	O
)	O
:2i	O
5	O
:14	O
:41	O
:903	O
5	O
2	O
4	O
1	O
:58	O
:58	O
:583	O
5	O
2	O
4	O
table	O
c.5	O
.	O
transition	B
probability	I
matrices	O
for	O
generating	O
random	B
paths	O
through	O
trellises	O
.	O
this	O
property	O
can	O
be	O
rewritten	O
in	O
terms	O
of	O
the	O
all-ones	O
vector	O
n	O
=	O
(	O
1	O
;	O
1	O
;	O
:	O
:	O
:	O
;	O
1	O
)	O
t	O
:	O
so	O
n	O
is	O
the	O
principal	O
left-eigenvector	O
of	O
q	O
with	O
eigenvalue	O
(	O
cid:21	O
)	O
1	O
=	O
1.	O
ntq	O
=	O
nt	O
:	O
e	O
(	O
1	O
)	O
l	O
=	O
n	O
:	O
(	O
c.7	O
)	O
(	O
c.8	O
)	O
because	O
it	O
is	O
a	O
non-negative	O
matrix	B
,	O
q	O
has	O
a	O
principal	O
right-eigenvector	O
that	O
is	O
non-negative	O
,	O
e	O
(	O
1	O
)	O
r	O
.	O
generically	O
,	O
for	O
markov	O
processes	O
that	O
are	O
ergodic	B
,	O
this	O
eigenvector	O
is	O
the	O
only	O
right-eigenvector	O
with	O
eigenvalue	O
of	O
magnitude	O
1	O
(	O
see	O
table	O
c.6	O
for	O
illustrative	O
exceptions	O
)	O
.	O
this	O
vector	O
,	O
if	O
we	O
normalize	O
it	O
such	O
that	O
e	O
(	O
1	O
)	O
r	O
(	O
cid:1	O
)	O
n	O
=	O
1	O
,	O
is	O
called	O
the	O
invariant	B
distribution	I
of	O
the	O
transition	B
probability	I
matrix	O
.	O
it	O
is	O
the	O
probability	B
density	O
that	O
is	O
left	O
unchanged	O
under	O
q.	O
unlike	O
the	O
principal	O
left-eigenvector	O
,	O
which	O
we	O
explicitly	O
identi	O
(	O
cid:12	O
)	O
ed	O
above	O
,	O
we	O
can	O
’	O
t	O
usually	O
identify	O
the	O
principal	O
right-eigenvector	O
without	O
computation	O
.	O
the	O
matrix	B
may	O
have	O
up	O
to	O
n	O
(	O
cid:0	O
)	O
1	O
other	O
right-eigenvectors	O
all	O
of	O
which	O
are	O
orthogonal	O
to	O
the	O
left-eigenvector	O
n	O
,	O
that	O
is	O
,	O
they	O
are	O
zero-sum	O
vectors	B
.	O
c.3	O
perturbation	O
theory	B
perturbation	O
theory	B
is	O
not	O
used	O
in	O
this	O
book	O
,	O
but	O
it	O
is	O
useful	O
in	O
this	O
book	O
’	O
s	O
(	O
cid:12	O
)	O
elds	O
.	O
in	O
this	O
section	B
we	O
derive	O
(	O
cid:12	O
)	O
rst-order	O
perturbation	O
theory	B
for	O
the	O
eigen-	O
vectors	B
and	O
eigenvalues	O
of	O
square	O
,	O
not	O
necessarily	O
symmetric	B
,	O
matrices	B
.	O
most	O
presentations	O
of	O
perturbation	O
theory	B
focus	O
on	O
symmetric	O
matrices	B
,	O
but	O
non-	O
symmetric	B
matrices	O
(	O
such	O
as	O
transition	O
matrices	B
)	O
also	O
deserve	O
to	O
be	O
perturbed	O
!	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
c.3	O
:	O
perturbation	O
theory	B
609	O
matrix	B
eigenvalues	O
and	O
eigenvectors	O
el	O
;	O
er	O
(	O
a	O
)	O
2	O
664	O
(	O
a0	O
)	O
2	O
664	O
(	O
b	O
)	O
2	O
664	O
:90	O
:20	O
0	O
:10	O
:80	O
0	O
0	O
0	O
0	O
0	O
0	O
:90	O
:20	O
0	O
:10	O
:80	O
:90	O
:20	O
0	O
0	O
:10	O
:79	O
:02	O
0	O
0	O
:01	O
:88	O
:20	O
0	O
0	O
:10	O
:80	O
0	O
0	O
:90	O
:20	O
0	O
:10	O
:80	O
0	O
0	O
:90	O
:20	O
0	O
:10	O
:80	O
0	O
0	O
3	O
775	O
3	O
775	O
3	O
775	O
0	O
0	O
:71	O
:71	O
2	O
664	O
1	O
:50	O
:50	O
:50	O
:50	O
:50	O
:50	O
:50	O
:50	O
2	O
664	O
2	O
664	O
3	O
775	O
3	O
775	O
1	O
3	O
775	O
2	O
664	O
2	O
664	O
1	O
2	O
664	O
0	O
0	O
:89	O
:45	O
3	O
775	O
:87	O
:43	O
:22	O
:11	O
:63	O
:32	O
:63	O
:32	O
3	O
775	O
3	O
775	O
2	O
664	O
2	O
664	O
3	O
:71	O
(	O
cid:0	O
)	O
:71	O
775	O
0	O
0	O
2	O
664	O
2	O
664	O
1	O
0:98	O
2	O
664	O
:71	O
:71	O
0	O
0	O
3	O
775	O
3	O
(	O
cid:0	O
)	O
:18	O
(	O
cid:0	O
)	O
:15	O
775	O
:66	O
:72	O
3	O
(	O
cid:0	O
)	O
:32	O
:63	O
775	O
(	O
cid:0	O
)	O
:32	O
:63	O
2	O
664	O
2	O
664	O
2	O
664	O
0:70	O
:89	O
:45	O
0	O
0	O
3	O
775	O
3	O
(	O
cid:0	O
)	O
:66	O
(	O
cid:0	O
)	O
:28	O
775	O
:61	O
:33	O
3	O
:50	O
(	O
cid:0	O
)	O
:50	O
775	O
:50	O
(	O
cid:0	O
)	O
:50	O
0:70	O
0:70	O
3	O
:45	O
(	O
cid:0	O
)	O
:89	O
775	O
0	O
0	O
2	O
664	O
3	O
2	O
:20	O
(	O
cid:0	O
)	O
:40	O
775	O
664	O
(	O
cid:0	O
)	O
:40	O
:80	O
(	O
cid:0	O
)	O
0:70	O
3	O
2	O
:32	O
(	O
cid:0	O
)	O
:63	O
775	O
664	O
(	O
cid:0	O
)	O
:32	O
:63	O
2	O
664	O
2	O
664	O
3	O
:63	O
(	O
cid:0	O
)	O
:63	O
775	O
(	O
cid:0	O
)	O
:32	O
:32	O
3	O
(	O
cid:0	O
)	O
:50	O
:50	O
775	O
:50	O
(	O
cid:0	O
)	O
:50	O
0:69	O
0:70	O
3	O
0	O
0	O
775	O
(	O
cid:0	O
)	O
:45	O
:89	O
2	O
664	O
3	O
2	O
(	O
cid:0	O
)	O
:19	O
:41	O
775	O
664	O
(	O
cid:0	O
)	O
:44	O
:77	O
(	O
cid:0	O
)	O
1	O
3	O
2	O
:50	O
:50	O
775	O
664	O
(	O
cid:0	O
)	O
:50	O
(	O
cid:0	O
)	O
:50	O
2	O
664	O
2	O
664	O
3	O
0	O
0	O
775	O
(	O
cid:0	O
)	O
:71	O
:71	O
3	O
(	O
cid:0	O
)	O
:61	O
:65	O
775	O
(	O
cid:0	O
)	O
:35	O
:30	O
3	O
:63	O
:32	O
775	O
(	O
cid:0	O
)	O
:63	O
(	O
cid:0	O
)	O
:32	O
table	O
c.6	O
.	O
illustrative	O
transition	B
probability	I
matrices	O
and	O
their	O
eigenvectors	O
showing	O
the	O
two	O
ways	O
of	O
being	O
non-ergodic	O
.	O
(	O
a	O
)	O
more	O
than	O
one	O
principal	O
eigenvector	O
with	O
eigenvalue	O
1	O
because	O
the	O
state	O
space	O
falls	O
into	O
two	O
unconnected	O
pieces	O
.	O
(	O
a0	O
)	O
a	O
small	O
perturbation	O
breaks	O
the	O
degen-	O
eracy	O
of	O
the	O
principal	O
eigenvectors	O
.	O
(	O
b	O
)	O
under	O
this	O
chain	B
,	O
the	O
density	B
may	O
oscillate	O
between	O
two	O
parts	O
of	O
the	O
state	O
space	O
.	O
in	O
addition	O
to	O
the	O
invariant	B
distribution	I
,	O
there	O
is	O
another	O
right-eigenvector	O
with	O
eigenvalue	O
(	O
cid:0	O
)	O
1.	O
in	O
general	O
such	O
circulating	O
densities	O
correspond	O
to	O
complex	B
eigenvalues	O
with	O
magnitude	O
1.	O
we	O
assume	O
that	O
we	O
have	O
an	O
n	O
(	O
cid:2	O
)	O
n	O
matrix	B
h	O
that	O
is	O
a	O
function	B
h	O
(	O
(	O
cid:15	O
)	O
)	O
of	O
a	O
real	O
parameter	O
(	O
cid:15	O
)	O
,	O
with	O
(	O
cid:15	O
)	O
=	O
0	O
being	O
our	O
starting	O
point	O
.	O
we	O
assume	O
that	O
a	O
taylor	O
expansion	O
of	O
h	O
(	O
(	O
cid:15	O
)	O
)	O
is	O
appropriate	O
:	O
where	O
h	O
(	O
(	O
cid:15	O
)	O
)	O
=	O
h	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
v	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
v	O
(	O
cid:17	O
)	O
@	O
h	O
@	O
(	O
cid:15	O
)	O
:	O
(	O
c.9	O
)	O
(	O
c.10	O
)	O
we	O
assume	O
that	O
for	O
all	O
(	O
cid:15	O
)	O
of	O
interest	O
,	O
h	O
(	O
(	O
cid:15	O
)	O
)	O
has	O
a	O
complete	O
set	B
of	O
n	O
right-	O
eigenvectors	O
and	O
left-eigenvectors	O
,	O
and	O
that	O
these	O
eigenvectors	O
and	O
their	O
eigen-	O
values	O
are	O
continuous	B
functions	O
of	O
(	O
cid:15	O
)	O
.	O
this	O
last	O
assumption	O
is	O
not	O
necessarily	O
a	O
good	B
one	O
:	O
if	O
h	O
(	O
0	O
)	O
has	O
degenerate	O
eigenvalues	O
then	O
it	O
is	O
possible	O
for	O
the	O
eigen-	O
vectors	B
to	O
be	O
discontinuous	O
in	O
(	O
cid:15	O
)	O
;	O
in	O
such	O
cases	O
,	O
degenerate	O
perturbation	O
theory	B
is	O
needed	O
.	O
that	O
’	O
s	O
a	O
fun	O
topic	O
,	O
but	O
let	O
’	O
s	O
stick	O
with	O
the	O
non-degenerate	O
case	O
here	O
.	O
we	O
write	O
the	O
eigenvectors	O
and	O
eigenvalues	O
as	O
follows	O
:	O
h	O
(	O
(	O
cid:15	O
)	O
)	O
e	O
(	O
a	O
)	O
r	O
(	O
(	O
cid:15	O
)	O
)	O
=	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
(	O
cid:15	O
)	O
)	O
e	O
(	O
a	O
)	O
r	O
(	O
(	O
cid:15	O
)	O
)	O
;	O
and	O
we	O
taylor-expand	O
with	O
and	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
(	O
cid:15	O
)	O
)	O
=	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
(	O
cid:22	O
)	O
(	O
a	O
)	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:22	O
)	O
(	O
a	O
)	O
(	O
cid:17	O
)	O
@	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
(	O
cid:15	O
)	O
)	O
@	O
(	O
cid:15	O
)	O
e	O
(	O
a	O
)	O
r	O
(	O
(	O
cid:15	O
)	O
)	O
=	O
e	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
f	O
(	O
a	O
)	O
r	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
c.11	O
)	O
(	O
c.12	O
)	O
(	O
c.13	O
)	O
(	O
c.14	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
610	O
with	O
c	O
|	O
some	O
mathematics	O
@	O
e	O
(	O
a	O
)	O
r	O
@	O
(	O
cid:15	O
)	O
;	O
(	O
c.15	O
)	O
f	O
(	O
a	O
)	O
r	O
(	O
cid:17	O
)	O
l	O
and	O
f	O
(	O
a	O
)	O
and	O
similar	O
de	O
(	O
cid:12	O
)	O
nitions	O
for	O
e	O
(	O
a	O
)	O
l	O
.	O
we	O
de	O
(	O
cid:12	O
)	O
ne	O
these	O
left-vectors	O
to	O
be	O
row	O
vectors	B
,	O
so	O
that	O
the	O
‘	O
transpose	O
’	O
operation	O
is	O
not	O
needed	O
and	O
can	O
be	O
banished	O
.	O
we	O
are	O
free	O
to	O
constrain	O
the	O
magnitudes	O
of	O
the	O
eigenvectors	O
in	O
whatever	O
way	O
we	O
please	O
.	O
each	O
left-eigenvector	O
and	O
each	O
right-eigenvector	O
has	O
an	O
arbitrary	O
magnitude	O
.	O
the	O
natural	B
constraints	O
to	O
use	O
are	O
as	O
follows	O
.	O
first	O
,	O
we	O
constrain	O
the	O
inner	O
products	O
with	O
:	O
e	O
(	O
a	O
)	O
l	O
(	O
(	O
cid:15	O
)	O
)	O
e	O
(	O
a	O
)	O
r	O
(	O
(	O
cid:15	O
)	O
)	O
=	O
1	O
;	O
for	O
all	O
a	O
:	O
expanding	O
the	O
eigenvectors	O
in	O
(	O
cid:15	O
)	O
,	O
equation	O
(	O
c.19	O
)	O
implies	O
(	O
e	O
(	O
a	O
)	O
l	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
f	O
(	O
a	O
)	O
l	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
(	O
e	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
f	O
(	O
a	O
)	O
r	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
=	O
1	O
;	O
from	O
which	O
we	O
can	O
extract	O
the	O
terms	O
in	O
(	O
cid:15	O
)	O
,	O
which	O
say	O
:	O
e	O
(	O
a	O
)	O
l	O
(	O
0	O
)	O
f	O
(	O
a	O
)	O
r	O
+	O
f	O
(	O
a	O
)	O
l	O
e	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
=	O
0	O
we	O
are	O
now	O
free	O
to	O
choose	O
the	O
two	O
constraints	O
:	O
l	O
(	O
0	O
)	O
f	O
(	O
a	O
)	O
e	O
(	O
a	O
)	O
r	O
=	O
0	O
;	O
l	O
e	O
(	O
a	O
)	O
f	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
=	O
0	O
;	O
(	O
c.16	O
)	O
(	O
c.17	O
)	O
(	O
c.18	O
)	O
(	O
c.19	O
)	O
which	O
in	O
the	O
special	O
case	O
of	O
a	O
symmetric	B
matrix	O
correspond	O
to	O
constraining	O
the	O
eigenvectors	O
to	O
be	O
of	O
constant	O
length	B
,	O
as	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
the	O
euclidean	O
norm	O
.	O
ok	O
,	O
now	O
that	O
we	O
have	O
de	O
(	O
cid:12	O
)	O
ned	O
our	O
cast	O
of	O
characters	O
,	O
what	O
do	O
the	O
de	O
(	O
cid:12	O
)	O
ning	O
equations	O
(	O
c.11	O
)	O
and	O
(	O
c.9	O
)	O
tell	O
us	O
about	O
our	O
taylor	O
expansions	O
(	O
c.13	O
)	O
and	O
(	O
c.15	O
)	O
?	O
we	O
expand	O
equation	O
(	O
c.11	O
)	O
in	O
(	O
cid:15	O
)	O
.	O
(	O
h	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
v+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
(	O
e	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
f	O
(	O
a	O
)	O
r	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
=	O
(	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
(	O
cid:22	O
)	O
(	O
a	O
)	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
(	O
e	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
f	O
(	O
a	O
)	O
r	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
:	O
(	O
c.20	O
)	O
identifying	O
the	O
terms	O
of	O
order	O
(	O
cid:15	O
)	O
,	O
we	O
have	O
:	O
h	O
(	O
0	O
)	O
f	O
(	O
a	O
)	O
r	O
+	O
ve	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
=	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
f	O
(	O
a	O
)	O
r	O
+	O
(	O
cid:22	O
)	O
(	O
a	O
)	O
e	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
:	O
(	O
c.21	O
)	O
we	O
can	O
extract	O
interesting	O
results	O
from	O
this	O
equation	O
by	O
hitting	O
it	O
with	O
e	O
(	O
b	O
)	O
l	O
(	O
0	O
)	O
:	O
l	O
(	O
0	O
)	O
h	O
(	O
0	O
)	O
f	O
(	O
a	O
)	O
e	O
(	O
b	O
)	O
)	O
(	O
cid:21	O
)	O
(	O
b	O
)	O
e	O
(	O
b	O
)	O
r	O
+	O
e	O
(	O
b	O
)	O
l	O
(	O
0	O
)	O
ve	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
=	O
e	O
(	O
b	O
)	O
l	O
(	O
0	O
)	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
f	O
(	O
a	O
)	O
r	O
+	O
(	O
cid:22	O
)	O
(	O
a	O
)	O
e	O
(	O
b	O
)	O
l	O
(	O
0	O
)	O
e	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
:	O
l	O
(	O
0	O
)	O
f	O
(	O
a	O
)	O
r	O
+	O
e	O
(	O
b	O
)	O
l	O
(	O
0	O
)	O
ve	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
=	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
e	O
(	O
b	O
)	O
l	O
(	O
0	O
)	O
f	O
(	O
a	O
)	O
r	O
+	O
(	O
cid:22	O
)	O
(	O
a	O
)	O
(	O
cid:14	O
)	O
ab	O
:	O
setting	O
b	O
=	O
a	O
we	O
obtain	O
e	O
(	O
a	O
)	O
l	O
(	O
0	O
)	O
ve	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
=	O
(	O
cid:22	O
)	O
(	O
a	O
)	O
:	O
alternatively	O
,	O
choosing	O
b	O
6=	O
a	O
,	O
we	O
obtain	O
:	O
e	O
(	O
b	O
)	O
l	O
(	O
0	O
)	O
ve	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
=h	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
(	O
b	O
)	O
(	O
0	O
)	O
i	O
e	O
(	O
b	O
)	O
l	O
(	O
0	O
)	O
f	O
(	O
a	O
)	O
r	O
1	O
e	O
(	O
b	O
)	O
l	O
(	O
0	O
)	O
ve	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
:	O
l	O
(	O
0	O
)	O
f	O
(	O
a	O
)	O
r	O
=	O
)	O
e	O
(	O
b	O
)	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
(	O
b	O
)	O
(	O
0	O
)	O
now	O
,	O
assuming	O
that	O
the	O
right-eigenvectors	O
fe	O
(	O
b	O
)	O
we	O
must	O
be	O
able	O
to	O
write	O
r	O
(	O
0	O
)	O
gn	O
(	O
c.22	O
)	O
(	O
c.23	O
)	O
(	O
c.24	O
)	O
(	O
c.25	O
)	O
b=1	O
form	O
a	O
complete	O
basis	O
,	O
f	O
(	O
a	O
)	O
r	O
=xb	O
wbe	O
(	O
b	O
)	O
r	O
(	O
0	O
)	O
;	O
(	O
c.26	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
611	O
c.3	O
:	O
perturbation	O
theory	B
where	O
wb	O
=	O
e	O
(	O
b	O
)	O
l	O
(	O
0	O
)	O
f	O
(	O
a	O
)	O
r	O
;	O
so	O
,	O
comparing	O
(	O
c.25	O
)	O
and	O
(	O
c.27	O
)	O
,	O
we	O
have	O
:	O
f	O
(	O
a	O
)	O
r	O
=xb6=a	O
e	O
(	O
b	O
)	O
l	O
(	O
0	O
)	O
ve	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
(	O
b	O
)	O
(	O
0	O
)	O
e	O
(	O
b	O
)	O
r	O
(	O
0	O
)	O
:	O
(	O
c.27	O
)	O
(	O
c.28	O
)	O
equations	O
(	O
c.23	O
)	O
and	O
(	O
c.28	O
)	O
are	O
the	O
solution	O
to	O
the	O
(	O
cid:12	O
)	O
rst-order	O
perturbation	O
theory	B
problem	O
,	O
giving	O
respectively	O
the	O
(	O
cid:12	O
)	O
rst	O
derivative	O
of	O
the	O
eigenvalue	O
and	O
the	O
eigenvectors	O
.	O
second-order	O
perturbation	O
theory	B
if	O
we	O
expand	O
the	O
eigenvector	O
equation	O
(	O
c.11	O
)	O
to	O
second	O
order	O
in	O
(	O
cid:15	O
)	O
,	O
and	O
assume	O
that	O
the	O
equation	O
h	O
(	O
(	O
cid:15	O
)	O
)	O
=	O
h	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
v	O
(	O
c.29	O
)	O
is	O
exact	O
,	O
that	O
is	O
,	O
h	O
is	O
a	O
purely	O
linear	B
function	O
of	O
(	O
cid:15	O
)	O
,	O
then	O
we	O
have	O
:	O
(	O
h	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
v	O
)	O
(	O
e	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
f	O
(	O
a	O
)	O
1	O
2	O
r	O
+	O
(	O
cid:15	O
)	O
2g	O
(	O
a	O
)	O
2	O
(	O
cid:15	O
)	O
2	O
(	O
cid:23	O
)	O
(	O
a	O
)	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
(	O
e	O
(	O
a	O
)	O
r	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
r	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
f	O
(	O
a	O
)	O
=	O
(	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
(	O
cid:22	O
)	O
(	O
a	O
)	O
+	O
1	O
r	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
)	O
(	O
c.30	O
)	O
r	O
and	O
(	O
cid:23	O
)	O
(	O
a	O
)	O
are	O
the	O
second	O
derivatives	O
of	O
the	O
eigenvector	O
and	O
eigenvalue	O
.	O
where	O
g	O
(	O
a	O
)	O
equating	O
the	O
second-order	O
terms	O
in	O
(	O
cid:15	O
)	O
in	O
equation	O
(	O
c.30	O
)	O
,	O
r	O
+	O
1	O
2	O
(	O
cid:15	O
)	O
2g	O
(	O
a	O
)	O
vf	O
(	O
a	O
)	O
r	O
+	O
1	O
2	O
h	O
(	O
0	O
)	O
g	O
(	O
a	O
)	O
r	O
=	O
1	O
2	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
g	O
(	O
a	O
)	O
r	O
+	O
1	O
2	O
(	O
cid:23	O
)	O
(	O
a	O
)	O
e	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
+	O
(	O
cid:22	O
)	O
(	O
a	O
)	O
f	O
(	O
a	O
)	O
r	O
:	O
(	O
c.31	O
)	O
hitting	O
this	O
equation	O
on	O
the	O
left	O
with	O
e	O
(	O
a	O
)	O
l	O
(	O
0	O
)	O
,	O
we	O
obtain	O
:	O
l	O
(	O
0	O
)	O
vf	O
(	O
a	O
)	O
e	O
(	O
a	O
)	O
r	O
+	O
2	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
e	O
(	O
a	O
)	O
l	O
(	O
0	O
)	O
f	O
(	O
a	O
)	O
r	O
=	O
1	O
the	O
term	O
e	O
(	O
a	O
)	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
e	O
(	O
a	O
)	O
1	O
2	O
r	O
l	O
(	O
0	O
)	O
g	O
(	O
a	O
)	O
2	O
(	O
cid:23	O
)	O
(	O
a	O
)	O
e	O
(	O
a	O
)	O
l	O
(	O
0	O
)	O
g	O
(	O
a	O
)	O
r	O
+	O
1	O
l	O
(	O
0	O
)	O
e	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
+	O
(	O
cid:22	O
)	O
(	O
a	O
)	O
e	O
(	O
a	O
)	O
l	O
(	O
0	O
)	O
f	O
(	O
a	O
)	O
r	O
:	O
(	O
c.32	O
)	O
is	O
equal	O
to	O
zero	O
because	O
of	O
our	O
constraints	O
(	O
c.19	O
)	O
,	O
so	O
so	O
the	O
second	O
derivative	O
of	O
the	O
eigenvalue	O
with	O
respect	O
to	O
(	O
cid:15	O
)	O
is	O
given	O
by	O
e	O
(	O
a	O
)	O
l	O
(	O
0	O
)	O
vf	O
(	O
a	O
)	O
r	O
=	O
(	O
cid:23	O
)	O
(	O
a	O
)	O
;	O
1	O
2	O
(	O
c.33	O
)	O
(	O
cid:23	O
)	O
(	O
a	O
)	O
=	O
e	O
(	O
a	O
)	O
1	O
2	O
l	O
(	O
0	O
)	O
vxb6=a	O
=	O
xb6=a	O
e	O
(	O
b	O
)	O
l	O
(	O
0	O
)	O
ve	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
(	O
b	O
)	O
(	O
0	O
)	O
r	O
(	O
0	O
)	O
]	O
[	O
e	O
(	O
a	O
)	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
(	O
b	O
)	O
(	O
0	O
)	O
e	O
(	O
b	O
)	O
r	O
(	O
0	O
)	O
(	O
c.34	O
)	O
[	O
e	O
(	O
b	O
)	O
l	O
(	O
0	O
)	O
ve	O
(	O
a	O
)	O
l	O
(	O
0	O
)	O
ve	O
(	O
b	O
)	O
r	O
(	O
0	O
)	O
]	O
:	O
(	O
c.35	O
)	O
this	O
is	O
as	O
far	O
as	O
we	O
will	O
take	O
the	O
perturbation	O
expansion	O
.	O
summary	B
if	O
we	O
introduce	O
the	O
abbreviation	O
vba	O
for	O
e	O
(	O
b	O
)	O
eigenvectors	O
of	O
h	O
(	O
(	O
cid:15	O
)	O
)	O
=	O
h	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
v	O
to	O
(	O
cid:12	O
)	O
rst	O
order	O
as	O
l	O
(	O
0	O
)	O
ve	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
,	O
we	O
can	O
write	O
the	O
e	O
(	O
a	O
)	O
r	O
(	O
(	O
cid:15	O
)	O
)	O
=	O
e	O
(	O
a	O
)	O
r	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
xb6=a	O
vba	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
(	O
b	O
)	O
(	O
0	O
)	O
e	O
(	O
b	O
)	O
r	O
(	O
0	O
)	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
and	O
the	O
eigenvalues	O
to	O
second	O
order	O
as	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
(	O
cid:15	O
)	O
)	O
=	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
+	O
(	O
cid:15	O
)	O
vaa	O
+	O
(	O
cid:15	O
)	O
2xb6=a	O
vbavab	O
(	O
cid:21	O
)	O
(	O
a	O
)	O
(	O
0	O
)	O
(	O
cid:0	O
)	O
(	O
cid:21	O
)	O
(	O
b	O
)	O
(	O
0	O
)	O
+	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
(	O
cid:1	O
)	O
:	O
(	O
c.36	O
)	O
(	O
c.37	O
)	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
612	O
c.4	O
some	O
numbers	O
c	O
|	O
some	O
mathematics	O
28192	O
21024	O
2469	O
2266	O
2190	O
2171	O
298	O
258	O
232	O
225	O
e7	O
2	O
(	O
cid:0	O
)	O
2	O
21000	O
2500	O
2200	O
2100	O
250	O
240	O
230	O
220	O
210	O
20	O
2	O
(	O
cid:0	O
)	O
10	O
2	O
(	O
cid:0	O
)	O
20	O
2	O
(	O
cid:0	O
)	O
30	O
2	O
(	O
cid:0	O
)	O
60	O
102466	O
10308	O
10301	O
3	O
(	O
cid:2	O
)	O
10150	O
10141	O
1080	O
1:6	O
(	O
cid:2	O
)	O
1060	O
1057	O
3	O
(	O
cid:2	O
)	O
1051	O
1030	O
3	O
(	O
cid:2	O
)	O
1029	O
3	O
(	O
cid:2	O
)	O
1017	O
1015	O
1012	O
1011	O
1011	O
3	O
(	O
cid:2	O
)	O
1010	O
6	O
(	O
cid:2	O
)	O
109	O
6	O
(	O
cid:2	O
)	O
109	O
109	O
number	O
of	O
distinct	O
1-kilobyte	O
(	O
cid:12	O
)	O
les	O
number	O
of	O
states	O
of	O
a	O
2d	O
ising	O
model	B
with	O
32	O
(	O
cid:2	O
)	O
32	O
spins	O
number	O
of	O
binary	O
strings	O
of	O
length	O
1000	O
number	O
of	O
binary	O
strings	O
of	O
length	O
1000	O
having	O
100	O
1s	O
and	O
900	O
0s	O
number	O
of	O
electrons	O
in	O
universe	O
number	O
of	O
electrons	O
in	O
solar	O
system	O
number	O
of	O
electrons	O
in	O
the	O
earth	O
age	O
of	O
universe/picoseconds	O
age	O
of	O
universe/seconds	O
number	O
of	O
neurons	O
in	O
human	O
brain	B
number	O
of	O
bits	O
stored	O
on	O
a	O
dvd	O
number	O
of	O
bits	O
in	O
the	O
wheat	O
genome	B
number	O
of	O
bits	O
in	O
the	O
human	B
genome	O
population	O
of	O
earth	O
number	O
of	O
bits	O
in	O
c.	O
elegans	O
(	O
a	O
worm	O
)	O
genome	B
number	O
of	O
bits	O
in	O
arabidopsis	O
thaliana	O
(	O
a	O
(	O
cid:13	O
)	O
owering	O
plant	O
related	O
to	O
broccoli	O
)	O
genome	B
one	O
year/seconds	O
number	O
of	O
bits	O
in	O
the	O
compressed	O
postscript	O
(	O
cid:12	O
)	O
le	O
that	O
is	O
this	O
book	O
number	O
of	O
bits	O
in	O
unix	O
kernel	B
number	O
of	O
bits	O
in	O
the	O
e.	O
coli	O
genome	B
,	O
or	O
in	O
a	O
(	O
cid:13	O
)	O
oppy	O
disk	O
number	O
of	O
years	O
since	O
human/chimpanzee	O
divergence	B
1	O
048	O
576	O
2:5	O
(	O
cid:2	O
)	O
108	O
number	O
of	O
(	O
cid:12	O
)	O
bres	O
in	O
the	O
corpus	O
callosum	O
2	O
(	O
cid:2	O
)	O
108	O
2	O
(	O
cid:2	O
)	O
108	O
3	O
(	O
cid:2	O
)	O
107	O
2	O
(	O
cid:2	O
)	O
107	O
2	O
(	O
cid:2	O
)	O
107	O
107	O
4	O
(	O
cid:2	O
)	O
106	O
106	O
2	O
(	O
cid:2	O
)	O
105	O
3	O
(	O
cid:2	O
)	O
104	O
3	O
(	O
cid:2	O
)	O
104	O
1:5	O
(	O
cid:2	O
)	O
103	O
number	O
of	O
base	O
pairs	O
in	O
a	O
gene	O
103	O
100	O
210	O
=	O
1024	O
;	O
e7	O
=	O
1096	O
1	O
number	O
of	O
generations	O
since	O
human/chimpanzee	O
divergence	B
number	O
of	O
genes	O
in	O
human	O
genome	B
number	O
of	O
genes	O
in	O
arabidopsis	O
thaliana	O
genome	B
2:5	O
(	O
cid:2	O
)	O
10	O
(	O
cid:0	O
)	O
1	O
lifetime	O
probability	O
of	O
dying	O
from	O
smoking	O
one	O
pack	O
of	O
cigarettes	O
per	O
day	O
.	O
lifetime	O
probability	O
of	O
dying	O
in	O
a	O
motor	O
vehicle	O
accident	O
lifetime	O
probability	O
of	O
developing	O
cancer	O
because	O
of	O
drinking	O
2	O
litres	O
per	O
day	O
of	O
water	O
containing	O
12	O
p.p.b	O
.	O
benzene	O
probability	B
of	I
error	I
in	O
transmission	O
of	O
coding	O
dna	O
,	O
per	O
nucleotide	B
,	O
per	O
generation	O
probability	O
of	O
undetected	O
error	O
in	O
a	O
hard	O
disk	O
drive	O
,	O
after	O
error	B
correction	I
10	O
(	O
cid:0	O
)	O
2	O
10	O
(	O
cid:0	O
)	O
3	O
10	O
(	O
cid:0	O
)	O
5	O
10	O
(	O
cid:0	O
)	O
6	O
3	O
(	O
cid:2	O
)	O
10	O
(	O
cid:0	O
)	O
8	O
10	O
(	O
cid:0	O
)	O
9	O
10	O
(	O
cid:0	O
)	O
18	O
 	B
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
bibliography	O
abrahamsen	O
,	O
p.	O
(	O
1997	O
)	O
a	O
review	O
of	O
gaussian	O
random	B
(	O
cid:12	O
)	O
elds	O
and	O
correlation	O
functions	B
.	O
technical	O
report	O
917	O
,	O
norwegian	O
com-	O
puting	O
center	O
,	O
blindern	O
,	O
n-0314	O
oslo	O
,	O
norway	O
.	O
2nd	O
edition	O
.	O
abramson	O
,	O
n.	O
(	O
1963	O
)	O
information	B
theory	I
and	O
coding	O
.	O
mcgraw-	O
hill	O
.	O
adler	O
,	O
s.	O
l.	O
(	O
1981	O
)	O
over-relaxation	O
method	B
for	O
the	O
monte-carlo	O
evaluation	O
of	O
the	O
partition	O
function	B
for	O
multiquadratic	O
actions	O
.	O
physical	O
review	O
d	O
{	O
particles	O
and	O
fields	O
23	O
(	O
12	O
)	O
:	O
2901	O
{	O
2904.	O
aiyer	O
,	O
s.	O
v.	O
b	O
.	O
(	O
1991	O
)	O
solving	O
combinatorial	O
optimization	B
prob-	O
lems	O
using	O
neural	O
networks	O
.	O
cambridge	O
univ	O
.	O
engineering	O
dept	O
.	O
phd	O
dissertation	O
.	O
cued/f-infeng/tr	O
89.	O
aji	O
,	O
s.	O
,	O
jin	O
,	O
h.	O
,	O
khandekar	O
,	O
a.	O
,	O
mceliece	O
,	O
r.	O
j.	O
,	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
2000	O
)	O
bsc	O
thresholds	O
for	O
code	O
ensembles	O
based	O
on	O
‘	O
typical	B
pairs	O
’	O
decoding	B
.	O
in	O
codes	O
,	O
systems	O
and	O
graph-	O
ical	O
models	O
,	O
ed	O
.	O
by	O
b.	O
marcus	O
and	O
j.	O
rosenthal	O
,	O
volume	B
123	O
of	O
ima	O
volumes	O
in	O
mathematics	O
and	O
its	O
applications	O
,	O
pp	O
.	O
195	O
{	O
210.	O
springer	O
.	O
amari	O
,	O
s.	O
,	O
cichocki	O
,	O
a.	O
,	O
and	O
yang	O
,	O
h.	O
h.	O
(	O
1996	O
)	O
a	O
new	O
learn-	O
ing	O
algorithm	B
for	O
blind	O
signal	O
separation	B
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
ed	O
.	O
by	O
d.	O
s.	O
touretzky	O
,	O
m.	O
c.	O
mozer	O
,	O
and	O
m.	O
e.	O
hasselmo	O
,	O
volume	B
8	O
,	O
pp	O
.	O
757	O
{	O
763.	O
mit	O
press	O
.	O
(	O
1985	O
)	O
storing	O
in	O
(	O
cid:12	O
)	O
nite	O
numbers	O
of	O
patterns	O
in	O
a	O
spin	O
glass	O
model	B
of	O
neural	O
networks	O
.	O
phys	O
.	O
rev	O
.	O
lett	O
.	O
55	O
:	O
1530	O
{	O
1533.	O
amit	O
,	O
d.	O
j.	O
,	O
gutfreund	O
,	O
h.	O
,	O
and	O
sompolinsky	O
,	O
h.	O
angel	O
,	O
j.	O
r.	O
p.	O
,	O
wizinowich	O
,	O
p.	O
,	O
lloyd-hart	O
,	O
m.	O
,	O
and	O
san-	O
dler	O
,	O
d.	O
(	O
1990	O
)	O
adaptive	B
optics	O
for	O
array	O
telescopes	O
using	O
neural-network	O
techniques	O
.	O
nature	O
348	O
:	O
221	O
{	O
224.	O
bahl	O
,	O
l.	O
r.	O
,	O
cocke	O
,	O
j.	O
,	O
jelinek	O
,	O
f.	O
,	O
and	O
raviv	O
,	O
j	O
.	O
(	O
1974	O
)	O
optimal	B
decoding	O
of	O
linear	O
codes	O
for	O
minimizing	O
symbol	O
error	O
rate	B
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
it-20	O
:	O
284	O
{	O
287.	O
baldwin	O
,	O
j	O
.	O
(	O
1896	O
)	O
a	O
new	O
factor	O
in	O
evolution	B
.	O
american	O
natu-	O
ralist	O
30	O
:	O
441	O
{	O
451.	O
bar-shalom	O
,	O
y.	O
,	O
and	O
fortmann	O
,	O
t.	O
(	O
1988	O
)	O
tracking	O
and	O
data	O
association	O
.	O
academic	O
press	O
.	O
barber	O
,	O
d.	O
,	O
and	O
williams	O
,	O
c.	O
k.	O
i	O
.	O
(	O
1997	O
)	O
gaussian	O
processes	O
for	O
bayesian	O
classi	O
(	O
cid:12	O
)	O
cation	O
via	O
hybrid	O
monte	O
carlo	O
.	O
in	O
neural	O
information	O
processing	O
systems	O
9	O
,	O
ed	O
.	O
by	O
m.	O
c.	O
mozer	O
,	O
m.	O
i.	O
jordan	O
,	O
and	O
t.	O
petsche	O
,	O
pp	O
.	O
340	O
{	O
346.	O
mit	O
press	O
.	O
barnett	O
,	O
s.	O
(	O
1979	O
)	O
matrix	B
methods	O
for	O
engineers	O
and	O
scientists	O
.	O
mcgraw-hill	O
.	O
battail	O
,	O
g.	O
(	O
1993	O
)	O
we	O
can	O
think	O
of	O
good	O
codes	O
,	O
and	O
even	O
de-	O
code	B
them	O
.	O
in	O
eurocode	O
’	O
92	O
.	O
udine	O
,	O
italy	O
,	O
26-30	O
october	O
,	O
ed	O
.	O
by	O
p.	O
camion	O
,	O
p.	O
charpin	O
,	O
and	O
s.	O
harari	O
,	O
number	O
339	O
in	O
cism	O
courses	O
and	O
lectures	O
,	O
pp	O
.	O
353	O
{	O
368.	O
springer	O
.	O
baum	O
,	O
e.	O
,	O
boneh	O
,	O
d.	O
,	O
and	O
garrett	O
,	O
c.	O
(	O
1995	O
)	O
on	O
genetic	O
in	O
proc	O
.	O
eighth	O
annual	O
conf	O
.	O
on	O
computational	O
algorithms	B
.	O
learning	B
theory	O
,	O
pp	O
.	O
230	O
{	O
239.	O
acm	O
.	O
baum	O
,	O
e.	O
b.	O
,	O
and	O
smith	O
,	O
w.	O
d.	O
(	O
1993	O
)	O
best	O
play	O
for	O
imperfect	O
players	O
and	O
game	O
tree	B
search	O
.	O
technical	O
report	O
,	O
nec	O
,	O
prince-	O
ton	O
,	O
nj	O
.	O
baum	O
,	O
e.	O
b.	O
,	O
and	O
smith	O
,	O
w.	O
d.	O
(	O
1997	O
)	O
a	O
bayesian	O
approach	O
to	O
relevance	O
in	O
game	O
playing	O
.	O
arti	O
(	O
cid:12	O
)	O
cial	O
intelligence	O
97	O
(	O
1-2	O
)	O
:	O
195	O
{	O
242.	O
baum	O
,	O
l.	O
e.	O
,	O
and	O
petrie	O
,	O
t.	O
inference	B
for	O
probabilistic	O
functions	O
of	O
(	O
cid:12	O
)	O
nite-state	O
markov	O
chains	O
.	O
ann	O
.	O
math	O
.	O
stat	O
.	O
37	O
:	O
1559	O
{	O
1563.	O
statistical	B
(	O
1966	O
)	O
beal	O
,	O
m.	O
j.	O
,	O
ghahramani	O
,	O
z.	O
,	O
and	O
rasmussen	O
,	O
c.	O
e.	O
(	O
2002	O
)	O
the	O
in	O
(	O
cid:12	O
)	O
nite	O
hidden	O
markov	O
model	B
.	O
in	O
advances	O
in	O
neural	O
in-	O
formation	O
processing	O
systems	O
14	O
.	O
mit	O
press	O
.	O
bell	O
,	O
a.	O
j.	O
,	O
and	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1995	O
)	O
an	O
information	B
maxi-	O
mization	O
approach	O
to	O
blind	O
separation	B
and	O
blind	O
deconvolution	B
.	O
neural	O
computation	O
7	O
(	O
6	O
)	O
:	O
1129	O
{	O
1159.	O
bentley	O
,	O
j	O
.	O
(	O
2000	O
)	O
programming	O
pearls	O
.	O
addison-wesley	O
,	O
sec-	O
ond	O
edition	O
.	O
berger	O
,	O
j	O
.	O
(	O
1985	O
)	O
statistical	B
decision	O
theory	B
and	O
bayesian	O
anal-	O
ysis	O
.	O
springer	O
.	O
berlekamp	O
,	O
e.	O
r.	O
(	O
1968	O
)	O
algebraic	B
coding	I
theory	I
.	O
mcgraw-	O
hill	O
.	O
berlekamp	O
,	O
e.	O
r.	O
(	O
1980	O
)	O
the	O
technology	O
of	O
error-correcting	O
codes	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
68	O
:	O
564	O
{	O
593.	O
berlekamp	O
,	O
e.	O
r.	O
,	O
mceliece	O
,	O
r.	O
j.	O
,	O
and	O
van	O
tilborg	O
,	O
h.	O
c.	O
a	O
.	O
(	O
1978	O
)	O
on	O
the	O
intractability	O
of	O
certain	O
coding	O
problems	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
24	O
(	O
3	O
)	O
:	O
384	O
{	O
386.	O
berrou	O
,	O
c.	O
,	O
and	O
glavieux	O
,	O
a	O
.	O
(	O
1996	O
)	O
near	O
optimum	O
error	O
cor-	O
ieee	O
trans	O
.	O
on	O
recting	O
coding	O
and	O
decoding	B
:	O
turbo-codes	O
.	O
communications	O
44	O
:	O
1261	O
{	O
1271.	O
berrou	O
,	O
c.	O
,	O
glavieux	O
,	O
a.	O
,	O
and	O
thitimajshima	O
,	O
p.	O
(	O
1993	O
)	O
near	O
shannon	O
limit	O
error-correcting	O
coding	O
and	O
decoding	O
:	O
turbo-	O
codes	O
.	O
in	O
proc	O
.	O
1993	O
ieee	O
international	O
conf	O
.	O
on	O
communi-	O
cations	O
,	O
geneva	O
,	O
switzerland	O
,	O
pp	O
.	O
1064	O
{	O
1070.	O
berzuini	O
,	O
c.	O
,	O
best	O
,	O
n.	O
g.	O
,	O
gilks	O
,	O
w.	O
r.	O
,	O
and	O
larizza	O
,	O
c.	O
(	O
1997	O
)	O
dynamic	O
conditional	B
independence	O
models	O
and	O
markov	O
chain	B
monte	O
carlo	O
methods	B
.	O
j.	O
american	O
statistical	B
assoc	O
.	O
92	O
(	O
440	O
)	O
:	O
1403	O
{	O
1412.	O
berzuini	O
,	O
c.	O
,	O
and	O
gilks	O
,	O
w.	O
r.	O
(	O
2001	O
)	O
following	O
a	O
moving	O
tar-	O
get	O
{	O
monte	O
carlo	O
inference	B
for	O
dynamic	O
bayesian	O
models	O
.	O
j.	O
royal	O
statistical	B
society	O
series	O
b	O
{	O
statistical	B
methodology	O
63	O
(	O
1	O
)	O
:	O
127	O
{	O
146.	O
bhattacharyya	O
,	O
a	O
.	O
(	O
1943	O
)	O
on	O
a	O
measure	O
of	O
divergence	O
between	O
two	O
statistical	B
populations	O
de	O
(	O
cid:12	O
)	O
ned	O
by	O
their	O
probability	B
distri-	O
butions	O
.	O
bull	O
.	O
calcutta	O
math	O
.	O
soc	O
.	O
35	O
:	O
99	O
{	O
110.	O
bishop	O
,	O
c.	O
m.	O
(	O
1992	O
)	O
exact	O
calculation	O
of	O
the	O
hessian	O
matrix	B
for	O
the	O
multilayer	B
perceptron	I
.	O
neural	O
computation	O
4	O
(	O
4	O
)	O
:	O
494	O
{	O
501.	O
bishop	O
,	O
c.	O
m.	O
(	O
1995	O
)	O
neural	O
networks	O
for	O
pattern	O
recognition	B
.	O
oxford	O
univ	O
.	O
press	O
.	O
bishop	O
,	O
c.	O
m.	O
,	O
winn	O
,	O
j.	O
m.	O
,	O
and	O
spiegelhalter	O
,	O
d.	O
(	O
2002	O
)	O
vibes	O
:	O
a	O
variational	B
inference	O
engine	O
for	O
bayesian	O
networks	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
xv	O
,	O
ed	O
.	O
by	O
s.	O
becker	O
,	O
s.	O
thrun	O
,	O
and	O
k.	O
obermayer	O
.	O
blahut	O
,	O
r.	O
e.	O
(	O
1987	O
)	O
principles	O
and	O
practice	O
of	O
information	O
theory	B
.	O
addison-wesley	O
.	O
bottou	O
,	O
l.	O
,	O
howard	O
,	O
p.	O
g.	O
,	O
and	O
bengio	O
,	O
y	O
.	O
(	O
1998	O
)	O
the	O
z-	O
coder	O
adaptive	B
binary	O
coder	O
.	O
in	O
proc	O
.	O
data	B
compression	I
conf.	O
,	O
snowbird	O
,	O
utah	O
,	O
march	O
1998	O
,	O
pp	O
.	O
13	O
{	O
22.	O
box	B
,	O
g.	O
e.	O
p.	O
,	O
and	O
tiao	O
,	O
g.	O
c.	O
(	O
1973	O
)	O
bayesian	O
inference	B
in	O
statistical	B
analysis	O
.	O
addison	O
{	O
wesley	O
.	O
braunstein	O
,	O
a.	O
,	O
m	O
(	O
cid:19	O
)	O
ezard	O
,	O
m.	O
,	O
and	O
zecchina	O
,	O
r.	O
,	O
(	O
2003	O
)	O
survey	B
propagation	I
:	O
an	O
algorithm	B
for	O
satis	O
(	O
cid:12	O
)	O
ability	O
.	O
cs.cc/0212002	O
.	O
613	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
614	O
bibliography	O
bretthorst	O
,	O
g.	O
(	O
1988	O
)	O
bayesian	O
spectrum	O
analysis	B
and	O
param-	O
eter	O
estimation	O
.	O
springer	O
.	O
also	O
available	O
at	O
bayes.wustl.edu	O
.	O
bridle	O
,	O
j.	O
s.	O
(	O
1989	O
)	O
probabilistic	O
interpretation	O
of	O
feedforward	O
classi	O
(	O
cid:12	O
)	O
cation	O
network	B
outputs	O
,	O
with	O
relationships	O
to	O
statisti-	O
cal	O
pattern	B
recognition	I
.	O
in	O
neuro-computing	O
:	O
algorithms	B
,	O
ar-	O
chitectures	O
and	O
applications	O
,	O
ed	O
.	O
by	O
f.	O
fougelman-soulie	O
and	O
j.	O
h	O
(	O
cid:19	O
)	O
erault	O
.	O
springer	O
{	O
verlag	O
.	O
bulmer	O
,	O
m.	O
(	O
1985	O
)	O
the	O
mathematical	O
theory	B
of	O
quantitative	O
genetics	O
.	O
oxford	O
univ	O
.	O
press	O
.	O
burrows	O
,	O
m.	O
,	O
and	O
wheeler	O
,	O
d.	O
j	O
.	O
(	O
1994	O
)	O
a	O
block-sorting	B
loss-	O
less	O
data	B
compression	I
algorithm	O
.	O
technical	O
report	O
124	O
,	O
digital	O
src	O
.	O
byers	O
,	O
j.	O
,	O
luby	O
,	O
m.	O
,	O
mitzenmacher	O
,	O
m.	O
,	O
and	O
rege	O
,	O
a	O
.	O
(	O
1998	O
)	O
a	O
digital	B
fountain	I
approach	O
to	O
reliable	O
distribution	B
of	O
bulk	O
data	O
.	O
in	O
proc	O
.	O
acm	O
sigcomm	O
’	O
98	O
,	O
september	O
2	O
{	O
4	O
,	O
1998	O
.	O
cairns-smith	O
,	O
a.	O
g.	O
(	O
1985	O
)	O
seven	O
clues	O
to	O
the	O
origin	O
of	O
life	O
.	O
cambridge	O
univ	O
.	O
press	O
.	O
calderbank	O
,	O
a.	O
r.	O
,	O
and	O
shor	O
,	O
p.	O
w.	O
(	O
1996	O
)	O
good	B
quantum	O
error-correcting	B
codes	I
exist	O
.	O
phys	O
.	O
rev	O
.	O
a	O
54	O
:	O
1098.	O
quant-ph/	O
9512032.	O
carroll	O
,	O
l.	O
(	O
1998	O
)	O
alice	O
’	O
s	O
adventures	O
in	O
wonderland	O
;	O
and	O
,	O
and	O
what	O
alice	O
found	O
there	O
.	O
through	O
the	O
looking-glass	O
:	O
macmillan	O
children	O
’	O
s	O
books	O
.	O
childs	O
,	O
a.	O
m.	O
,	O
patterson	O
,	O
r.	O
b.	O
,	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
2001	O
)	O
exact	B
sampling	I
from	O
non-attractive	O
distributions	O
using	O
summary	B
states	O
.	O
physical	O
review	O
e	O
63	O
:	O
036113.	O
chu	O
,	O
w.	O
,	O
keerthi	O
,	O
s.	O
s.	O
,	O
and	O
ong	O
,	O
c.	O
j	O
.	O
(	O
2001	O
)	O
a	O
uni	O
(	O
cid:12	O
)	O
ed	O
loss	B
function	I
in	O
bayesian	O
framework	O
for	O
support	O
vector	O
regres-	O
sion	O
.	O
in	O
proc	O
.	O
18th	O
international	O
conf	O
.	O
on	O
machine	O
learning	B
,	O
pp	O
.	O
51	O
{	O
58.	O
chu	O
,	O
w.	O
,	O
keerthi	O
,	O
s.	O
s.	O
,	O
and	O
ong	O
,	O
c.	O
j	O
.	O
(	O
2002	O
)	O
a	O
new	O
bayesian	O
design	O
method	O
for	O
support	O
vector	O
classi	O
(	O
cid:12	O
)	O
cation	O
.	O
in	O
special	O
sec-	O
tion	O
on	O
support	O
vector	O
machines	O
of	O
the	O
9th	O
international	O
conf	O
.	O
on	O
neural	O
information	B
processing	O
.	O
chu	O
,	O
w.	O
,	O
keerthi	O
,	O
s.	O
s.	O
,	O
and	O
ong	O
,	O
c.	O
j	O
.	O
(	O
2003a	O
)	O
bayesian	O
support	B
vector	I
regression	O
using	O
a	O
uni	O
(	O
cid:12	O
)	O
ed	O
loss	B
function	I
.	O
ieee	O
trans	O
.	O
on	O
neural	O
networks	O
.	O
submitted	O
.	O
chu	O
,	O
w.	O
,	O
keerthi	O
,	O
s.	O
s.	O
,	O
and	O
ong	O
,	O
c.	O
j	O
.	O
(	O
2003b	O
)	O
bayesian	O
trigonometric	O
support	B
vector	I
classi	O
(	O
cid:12	O
)	O
er	O
.	O
neural	O
computation	O
.	O
chung	O
,	O
s.-y.	O
,	O
richardson	O
,	O
t.	O
j.	O
,	O
and	O
urbanke	O
,	O
r.	O
l.	O
(	O
2001	O
)	O
analysis	B
of	O
sum-product	O
decoding	B
of	O
low-density	B
parity-check	I
codes	O
using	O
a	O
gaussian	O
approximation	B
.	O
ieee	O
trans	O
.	O
info	O
.	O
the-	O
ory	O
47	O
(	O
2	O
)	O
:	O
657	O
{	O
670.	O
chung	O
,	O
s.-y.	O
,	O
urbanke	O
,	O
r.	O
l.	O
,	O
and	O
richardson	O
,	O
t.	O
j.	O
,	O
(	O
1999	O
)	O
ldpc	O
code	B
design	O
applet	O
.	O
lids.mit.edu/~sychung/	O
gaopt.html	O
.	O
comon	O
,	O
p.	O
,	O
jutten	O
,	O
c.	O
,	O
and	O
herault	O
,	O
j	O
.	O
(	O
1991	O
)	O
blind	O
sepa-	O
ration	O
of	O
sources	O
.	O
2.	O
problems	O
statement	O
.	O
signal	O
processing	O
24	O
(	O
1	O
)	O
:	O
11	O
{	O
20.	O
copas	O
,	O
j.	O
b	O
.	O
(	O
1983	O
)	O
regression	B
,	O
prediction	B
and	O
shrinkage	O
(	O
with	O
discussion	O
)	O
.	O
j.	O
r.	O
statist	O
.	O
soc	O
.	O
b	O
45	O
(	O
3	O
)	O
:	O
311	O
{	O
354.	O
cover	O
,	O
t.	O
m.	O
(	O
1965	O
)	O
geometrical	O
and	O
statistical	O
properties	O
of	O
systems	O
of	O
linear	O
inequalities	O
with	O
applications	O
in	O
pattern	O
recog-	O
nition	O
.	O
ieee	O
trans	O
.	O
on	O
electronic	O
computers	O
14	O
:	O
326	O
{	O
334.	O
cover	O
,	O
t.	O
m.	O
,	O
and	O
thomas	O
,	O
j.	O
a	O
.	O
(	O
1991	O
)	O
elements	O
of	O
informa-	O
tion	O
theory	B
.	O
wiley	O
.	O
cowles	O
,	O
m.	O
k.	O
,	O
and	O
carlin	O
,	O
b.	O
p.	O
(	O
1996	O
)	O
markov-chain	O
monte-	O
carlo	O
convergence	O
diagnostics	O
{	O
a	O
comparative	O
review	O
.	O
j.	O
amer-	O
ican	O
statistical	B
assoc	O
.	O
91	O
(	O
434	O
)	O
:	O
883	O
{	O
904.	O
cox	O
,	O
r.	O
(	O
1946	O
)	O
probability	B
,	O
frequency	B
,	O
and	O
reasonable	O
expecta-	O
tion	O
.	O
am	O
.	O
j.	O
physics	B
14	O
:	O
1	O
{	O
13.	O
cressie	O
,	O
n.	O
(	O
1993	O
)	O
statistics	O
for	O
spatial	O
data	O
.	O
wiley	O
.	O
davey	O
,	O
m.	O
c.	O
(	O
1999	O
)	O
error-correction	B
using	O
low-density	O
parity-	O
check	O
codes	O
.	O
univ	O
.	O
of	O
cambridge	O
phd	O
dissertation	O
.	O
davey	O
,	O
m.	O
c.	O
,	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1998	O
)	O
low	O
density	B
par-	O
ity	O
check	O
codes	O
over	O
gf	O
(	O
q	O
)	O
.	O
ieee	O
communications	O
letters	O
2	O
(	O
6	O
)	O
:	O
165	O
{	O
167.	O
davey	O
,	O
m.	O
c.	O
,	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
2000	O
)	O
watermark	O
codes	O
:	O
reliable	O
communication	B
over	O
insertion/deletion	O
channels	O
.	O
in	O
proc	O
.	O
2000	O
ieee	O
international	O
symposium	O
on	O
info	O
.	O
theory	B
,	O
p.	O
477.	O
davey	O
,	O
m.	O
c.	O
,	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
2001	O
)	O
reliable	O
commu-	O
nication	O
over	O
channels	O
with	O
insertions	O
,	O
deletions	B
and	O
substitu-	O
tions	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
47	O
(	O
2	O
)	O
:	O
687	O
{	O
698.	O
dawid	O
,	O
a.	O
,	O
stone	O
,	O
m.	O
,	O
and	O
zidek	O
,	O
j	O
.	O
(	O
1996	O
)	O
critique	O
of	O
e.t	O
jaynes	O
’	O
s	O
‘	O
paradoxes	O
of	O
probability	O
theory	B
’	O
.	O
technical	O
re-	O
port	O
172	O
,	O
dept	O
.	O
of	O
statistical	O
science	O
,	O
univ	O
.	O
college	O
london	O
.	O
dayan	O
,	O
p.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
neal	O
,	O
r.	O
m.	O
,	O
and	O
zemel	O
,	O
r.	O
s.	O
(	O
1995	O
)	O
the	O
helmholtz	O
machine	O
.	O
neural	O
computation	O
7	O
(	O
5	O
)	O
:	O
889	O
{	O
904.	O
divsalar	O
,	O
d.	O
,	O
jin	O
,	O
h.	O
,	O
and	O
mceliece	O
,	O
r.	O
j	O
.	O
(	O
1998	O
)	O
coding	O
the-	O
orems	O
for	O
‘	O
turbo-like	O
’	O
codes	O
.	O
in	O
proc	O
.	O
36th	O
allerton	O
conf	O
.	O
on	O
communication	O
,	O
control	O
,	O
and	O
computing	O
,	O
sept.	O
1998	O
,	O
pp	O
.	O
201	O
{	O
210.	O
allerton	O
house	O
.	O
doucet	O
,	O
a.	O
,	O
de	O
freitas	O
,	O
j.	O
,	O
and	O
gordon	O
,	O
n.	O
eds	O
.	O
(	O
2001	O
)	O
se-	O
quential	O
monte	O
carlo	O
methods	B
in	O
practice	O
.	O
springer	O
.	O
duane	O
,	O
s.	O
,	O
kennedy	O
,	O
a.	O
d.	O
,	O
pendleton	O
,	O
b.	O
j.	O
,	O
and	O
roweth	O
,	O
d.	O
(	O
1987	O
)	O
hybrid	O
monte	O
carlo	O
.	O
physics	B
letters	O
b	O
195	O
:	O
216	O
{	O
222.	O
durbin	O
,	O
r.	O
,	O
eddy	O
,	O
s.	O
r.	O
,	O
krogh	O
,	O
a.	O
,	O
and	O
mitchison	O
,	O
g.	O
(	O
1998	O
)	O
biological	O
sequence	B
analysis	O
.	O
probabilistic	O
models	O
of	O
proteins	O
and	O
nucleic	O
acids	O
.	O
cambridge	O
univ	O
.	O
press	O
.	O
dyson	O
,	O
f.	O
j	O
.	O
(	O
1985	O
)	O
origins	O
of	O
life	O
.	O
cambridge	O
univ	O
.	O
press	O
.	O
elias	O
,	O
p.	O
(	O
1975	O
)	O
universal	B
codeword	O
sets	O
and	O
representations	O
of	O
the	O
integers	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
21	O
(	O
2	O
)	O
:	O
194	O
{	O
203.	O
felsenstein	O
,	O
j.	O
eyre-walker	O
,	O
a.	O
,	O
and	O
keightley	O
,	O
p.	O
(	O
1999	O
)	O
high	O
genomic	O
deleterious	O
mutation	O
rates	O
in	O
hominids	O
.	O
nature	O
397	O
:	O
344	O
{	O
347.	O
is	O
maynard	O
smith	O
necessary	O
?	O
in	B
evolution	I
.	O
essays	O
in	O
honour	O
of	O
john	O
maynard	O
smith	O
,	O
ed	O
.	O
by	O
p.	O
j.	O
greenwood	O
,	O
p.	O
h.	O
harvey	O
,	O
and	O
m.	O
slatkin	O
,	O
pp	O
.	O
209	O
{	O
220.	O
cambridge	O
univ	O
.	O
press	O
.	O
(	O
1985	O
)	O
recombination	O
and	O
sex	O
:	O
ferreira	O
,	O
h.	O
,	O
clarke	O
,	O
w.	O
,	O
helberg	O
,	O
a.	O
,	O
abdel-ghaffar	O
,	O
k.	O
s.	O
,	O
and	O
vinck	O
,	O
a.	O
h.	O
(	O
1997	O
)	O
insertion/deletion	O
correction	O
with	O
spectral	O
nulls	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
43	O
(	O
2	O
)	O
:	O
722	O
{	O
732.	O
feynman	O
,	O
r.	O
p.	O
(	O
1972	O
)	O
statistical	B
mechanics	O
.	O
addison	O
{	O
wesley	O
.	O
forney	O
,	O
jr.	O
,	O
g.	O
d.	O
(	O
1966	O
)	O
concatenated	B
codes	O
.	O
mit	O
press	O
.	O
forney	O
,	O
jr.	O
,	O
g.	O
d.	O
(	O
2001	O
)	O
codes	O
on	O
graphs	O
:	O
normal	B
realiza-	O
tions	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
47	O
(	O
2	O
)	O
:	O
520	O
{	O
548.	O
frey	O
,	O
b.	O
j	O
.	O
(	O
1998	O
)	O
graphical	O
models	O
for	O
machine	O
learning	B
and	O
digital	O
communication	O
.	O
mit	O
press	O
.	O
gallager	O
,	O
r.	O
g.	O
(	O
1962	O
)	O
low	O
density	B
parity	O
check	O
codes	O
.	O
ire	O
trans	O
.	O
info	O
.	O
theory	B
it-8	O
:	O
21	O
{	O
28.	O
gallager	O
,	O
r.	O
g.	O
(	O
1963	O
)	O
low	O
density	B
parity	O
check	O
codes	O
.	O
num-	O
ber	O
21	O
in	O
mit	O
research	O
monograph	O
series	O
.	O
mit	O
press	O
.	O
avail-	O
able	O
from	O
www.inference.phy.cam.ac.uk/mackay/gallager/	O
papers/	O
.	O
gallager	O
,	O
r.	O
g.	O
(	O
1968	O
)	O
information	B
theory	I
and	O
reliable	O
com-	O
munication	O
.	O
wiley	O
.	O
gallager	O
,	O
r.	O
g.	O
(	O
1978	O
)	O
variations	O
on	O
a	O
theme	O
by	O
hu	O
(	O
cid:11	O
)	O
man	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
it-24	O
(	O
6	O
)	O
:	O
668	O
{	O
674.	O
gibbs	O
,	O
m.	O
n.	O
(	O
1997	O
)	O
bayesian	O
gaussian	O
processes	O
for	O
regres-	O
sion	O
and	O
classi	O
(	O
cid:12	O
)	O
cation	O
.	O
cambridge	O
univ	O
.	O
phd	O
dissertation	O
.	O
www.inference.phy.cam.ac.uk/mng10/	O
.	O
gibbs	O
,	O
m.	O
n.	O
,	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
,	O
ef-	O
for	O
interpo-	O
www.inference.phy.cam.ac.uk/mackay/abstracts/	O
(	O
cid:12	O
)	O
cient	O
implementation	O
of	O
gaussian	O
processes	O
lation	O
.	O
gpros.html	O
.	O
(	O
1996	O
)	O
gibbs	O
,	O
m.	O
n.	O
,	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
2000	O
)	O
variational	B
gaus-	O
sian	O
process	O
classi	O
(	O
cid:12	O
)	O
ers	O
.	O
ieee	O
trans	O
.	O
on	O
neural	O
networks	O
11	O
(	O
6	O
)	O
:	O
1458	O
{	O
1464.	O
gilks	O
,	O
w.	O
,	O
roberts	O
,	O
g.	O
,	O
and	O
george	O
,	O
e.	O
(	O
1994	O
)	O
adaptive	B
direction	I
sampling	I
.	O
statistician	O
43	O
:	O
179	O
{	O
189.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
bibliography	O
615	O
gilks	O
,	O
w.	O
,	O
and	O
wild	O
,	O
p.	O
(	O
1992	O
)	O
adaptive	B
rejection	I
sampling	I
for	O
gibbs	O
sampling	O
.	O
applied	O
statistics	O
41	O
:	O
337	O
{	O
348.	O
gilks	O
,	O
w.	O
r.	O
,	O
richardson	O
,	O
s.	O
,	O
and	O
spiegelhalter	O
,	O
d.	O
j	O
.	O
(	O
1996	O
)	O
markov	O
chain	B
monte	O
carlo	O
in	O
practice	O
.	O
chapman	O
and	O
hall	O
.	O
goldie	O
,	O
c.	O
m.	O
,	O
and	O
pinch	O
,	O
r.	O
g.	O
e.	O
(	O
1991	O
)	O
communication	B
theory	O
.	O
cambridge	O
univ	O
.	O
press	O
.	O
golomb	O
,	O
s.	O
w.	O
,	O
peile	O
,	O
r.	O
e.	O
,	O
and	O
scholtz	O
,	O
r.	O
a	O
.	O
(	O
1994	O
)	O
basic	O
concepts	O
in	O
information	O
theory	B
and	O
coding	O
:	O
the	O
adventures	O
of	O
secret	O
agent	O
00111	O
.	O
plenum	O
press	O
.	O
good	B
,	O
i.	O
j	O
.	O
(	O
1979	O
)	O
studies	O
in	O
the	O
history	O
of	O
probability	O
and	O
statis-	O
tics	O
.	O
xxxvii	O
.	O
a.m.	O
turing	O
’	O
s	O
statistical	B
work	O
in	O
world	O
war	O
ii	O
.	O
biometrika	O
66	O
(	O
2	O
)	O
:	O
393	O
{	O
396.	O
graham	O
,	O
r.	O
l.	O
(	O
1966	O
)	O
on	O
partitions	O
of	O
a	O
(	O
cid:12	O
)	O
nite	O
set	B
.	O
journal	O
of	O
combinatorial	O
theory	B
1	O
:	O
215	O
{	O
223.	O
graham	O
,	O
r.	O
l.	O
,	O
and	O
knowlton	O
,	O
k.	O
c.	O
,	O
(	O
1968	O
)	O
method	B
of	O
iden-	O
tifying	O
conductors	O
in	O
a	O
cable	O
by	O
establishing	O
conductor	O
connec-	O
tion	O
groupings	O
at	O
both	O
ends	O
of	O
the	O
cable	O
.	O
u.s.	O
patent	O
3,369,177.	O
green	O
,	O
p.	O
j	O
.	O
(	O
1995	O
)	O
reversible	B
jump	I
markov	O
chain	B
monte	O
carlo	O
computation	O
and	O
bayesian	O
model	B
determination	O
.	O
biometrika	O
82	O
:	O
711	O
{	O
732.	O
gregory	O
,	O
p.	O
c.	O
,	O
and	O
loredo	O
,	O
t.	O
j	O
.	O
(	O
1992	O
)	O
a	O
new	O
method	B
for	O
the	O
detection	O
of	O
a	O
periodic	O
signal	O
of	O
unknown	O
shape	O
and	O
period	O
.	O
in	O
maximum	O
entropy	B
and	O
bayesian	O
methods	B
,	O
,	O
ed	O
.	O
by	O
g.	O
erick-	O
son	O
and	O
c.	O
smith	O
.	O
kluwer	O
.	O
also	O
in	O
astrophysical	O
journal	O
,	O
398	O
,	O
pp	O
.	O
146	O
{	O
168	O
,	O
oct	O
10	O
,	O
1992.	O
gull	O
,	O
s.	O
f.	O
(	O
1988	O
)	O
bayesian	O
inductive	O
inference	B
and	O
maximum	B
entropy	I
.	O
in	O
maximum	O
entropy	B
and	O
bayesian	O
methods	B
in	O
sci-	O
ence	O
and	O
engineering	O
,	O
vol	O
.	O
1	O
:	O
foundations	O
,	O
ed	O
.	O
by	O
g.	O
erickson	O
and	O
c.	O
smith	O
,	O
pp	O
.	O
53	O
{	O
74.	O
kluwer	O
.	O
gull	O
,	O
s.	O
f.	O
(	O
1989	O
)	O
developments	O
in	O
maximum	O
entropy	B
data	O
anal-	O
ysis	O
.	O
in	O
maximum	O
entropy	B
and	O
bayesian	O
methods	B
,	O
cambridge	O
1988	O
,	O
ed	O
.	O
by	O
j.	O
skilling	O
,	O
pp	O
.	O
53	O
{	O
71.	O
kluwer	O
.	O
gull	O
,	O
s.	O
f.	O
,	O
and	O
daniell	O
,	O
g.	O
(	O
1978	O
)	O
image	B
reconstruction	I
from	O
incomplete	O
and	O
noisy	O
data	O
.	O
nature	O
272	O
:	O
686	O
{	O
690.	O
hamilton	O
,	O
w.	O
d.	O
(	O
2002	O
)	O
narrow	O
roads	O
of	O
gene	O
land	O
,	O
volume	B
2	O
:	O
evolution	B
of	O
sex	O
.	O
oxford	O
univ	O
.	O
press	O
.	O
hanson	O
,	O
r.	O
,	O
stutz	O
,	O
j.	O
,	O
and	O
cheeseman	O
,	O
p.	O
(	O
1991a	O
)	O
bayesian	O
classi	O
(	O
cid:12	O
)	O
cation	O
theory	B
.	O
technical	O
report	O
fia	O
{	O
90-12-7-01	O
,	O
nasa	O
ames	O
.	O
hanson	O
,	O
r.	O
,	O
stutz	O
,	O
j.	O
,	O
and	O
cheeseman	O
,	O
p.	O
(	O
1991b	O
)	O
bayesian	O
classi	O
(	O
cid:12	O
)	O
cation	O
with	O
correlation	O
and	O
inheritance	O
.	O
in	O
proc	O
.	O
12th	O
in-	O
tern	O
.	O
joint	B
conf	O
.	O
on	O
arti	O
(	O
cid:12	O
)	O
cial	O
intelligence	O
,	O
sydney	O
,	O
australia	O
,	O
volume	B
2	O
,	O
pp	O
.	O
692	O
{	O
698.	O
morgan	O
kaufmann	O
.	O
hartmann	O
,	O
c.	O
r.	O
p.	O
,	O
and	O
rudolph	O
,	O
l.	O
d.	O
(	O
1976	O
)	O
an	O
optimum	O
symbol	O
by	O
symbol	O
decoding	O
rule	O
for	O
linear	B
codes	I
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
it-22	O
:	O
514	O
{	O
517.	O
harvey	O
,	O
m.	O
,	O
and	O
neal	O
,	O
r.	O
m.	O
(	O
2000	O
)	O
inference	B
for	O
belief	B
net-	O
works	O
using	O
coupling	B
from	I
the	I
past	I
.	O
in	O
uncertainty	O
in	O
arti	O
(	O
cid:12	O
)	O
cial	O
intelligence	O
:	O
proc	O
.	O
sixteenth	O
conf.	O
,	O
pp	O
.	O
256	O
{	O
263.	O
hebb	O
,	O
d.	O
o	O
.	O
(	O
1949	O
)	O
the	O
organization	O
of	O
behavior	O
.	O
wiley	O
.	O
hendin	O
,	O
o.	O
,	O
horn	O
,	O
d.	O
,	O
and	O
hopfield	O
,	O
j.	O
j	O
.	O
(	O
1994	O
)	O
decompo-	O
sition	O
of	O
a	O
mixture	O
of	O
signals	O
in	O
a	O
model	B
of	O
the	O
olfactory	O
bulb	O
.	O
proc	O
.	O
natl	O
.	O
acad	O
.	O
sci	O
.	O
usa	O
91	O
(	O
13	O
)	O
:	O
5942	O
{	O
5946.	O
hertz	O
,	O
j.	O
,	O
krogh	O
,	O
a.	O
,	O
and	O
palmer	O
,	O
r.	O
g.	O
(	O
1991	O
)	O
introduction	O
to	O
the	O
theory	B
of	O
neural	O
computation	O
.	O
addison-wesley	O
.	O
hinton	O
,	O
g.	O
(	O
2001	O
)	O
training	O
products	O
of	O
experts	O
by	O
minimiz-	O
ing	O
contrastive	O
divergence	B
.	O
technical	O
report	O
2000-004	O
,	O
gatsby	O
computational	O
neuroscience	O
unit	O
,	O
univ	O
.	O
college	O
london	O
.	O
hinton	O
,	O
g.	O
,	O
and	O
nowlan	O
,	O
s.	O
(	O
1987	O
)	O
how	O
learning	O
can	O
guide	O
evolution	B
.	O
complex	B
systems	O
1	O
:	O
495	O
{	O
502.	O
hinton	O
,	O
g.	O
e.	O
,	O
dayan	O
,	O
p.	O
,	O
frey	O
,	O
b.	O
j.	O
,	O
and	O
neal	O
,	O
r.	O
m.	O
(	O
1995	O
)	O
the	O
wake-sleep	O
algorithm	B
for	O
unsupervised	O
neural	O
net-	O
works	O
.	O
science	O
268	O
(	O
5214	O
)	O
:	O
1158	O
{	O
1161.	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1986	O
)	O
learning	B
and	O
relearning	O
in	O
boltzmann	O
machines	O
.	O
in	O
parallel	O
distributed	O
pro-	O
cessing	O
,	O
ed	O
.	O
by	O
d.	O
e.	O
rumelhart	O
and	O
j.	O
e.	O
mcclelland	O
,	O
pp	O
.	O
282	O
{	O
317.	O
mit	O
press	O
.	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
teh	O
,	O
y.	O
w.	O
(	O
2001	O
)	O
discovering	O
multi-	O
ple	O
constraints	O
that	O
are	O
frequently	O
approximately	O
satis	O
(	O
cid:12	O
)	O
ed	O
.	O
in	O
uncertainty	O
in	O
arti	O
(	O
cid:12	O
)	O
cial	O
intelligence	O
:	O
proc	O
.	O
seventeenth	O
conf	O
.	O
(	O
uai-2001	O
)	O
,	O
pp	O
.	O
227	O
{	O
234.	O
morgan	O
kaufmann	O
.	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
van	O
camp	O
,	O
d.	O
(	O
1993	O
)	O
keeping	O
neural	O
networks	O
simple	O
by	O
minimizing	O
the	O
description	O
length	B
of	O
the	O
weights	O
.	O
in	O
proc	O
.	O
6th	O
annual	O
workshop	O
on	O
comput	O
.	O
learning	B
theory	O
,	O
pp	O
.	O
5	O
{	O
13.	O
acm	O
press	O
,	O
new	O
york	O
,	O
ny	O
.	O
hinton	O
,	O
g.	O
e.	O
,	O
welling	O
,	O
m.	O
,	O
teh	O
,	O
y.	O
w.	O
,	O
and	O
osindero	O
,	O
s.	O
(	O
2001	O
)	O
a	O
new	O
view	O
of	O
ica	O
.	O
in	O
proc	O
.	O
international	O
conf	O
.	O
on	O
independent	O
component	O
analysis	B
and	O
blind	O
signal	O
separation	B
,	O
volume	B
3.	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
zemel	O
,	O
r.	O
s.	O
(	O
1994	O
)	O
autoencoders	O
,	O
min-	O
imum	O
description	O
length	B
and	O
helmholtz	O
free	B
energy	I
.	O
in	O
ad-	O
vances	O
in	O
neural	O
information	O
processing	O
systems	O
6	O
,	O
ed	O
.	O
by	O
j.	O
d.	O
cowan	O
,	O
g.	O
tesauro	O
,	O
and	O
j.	O
alspector	O
.	O
morgan	O
kaufmann	O
.	O
hodges	O
,	O
a	O
.	O
(	O
1983	O
)	O
alan	O
turing	O
:	O
the	O
enigma	O
.	O
simon	O
and	O
schus-	O
ter	O
.	O
hojen-sorensen	O
,	O
p.	O
a.	O
,	O
winther	O
,	O
o.	O
,	O
and	O
hansen	O
,	O
l.	O
k.	O
(	O
2002	O
)	O
mean	B
(	O
cid:12	O
)	O
eld	O
approaches	O
to	O
independent	O
component	O
anal-	O
ysis	O
.	O
neural	O
computation	O
14	O
:	O
889	O
{	O
918.	O
holmes	O
,	O
c.	O
,	O
and	O
denison	O
,	O
d.	O
(	O
2002	O
)	O
perfect	B
sampling	O
for	O
wavelet	O
reconstruction	O
of	O
signals	O
.	O
ieee	O
trans	O
.	O
signal	O
process-	O
ing	O
50	O
:	O
237	O
{	O
244.	O
holmes	O
,	O
c.	O
,	O
and	O
mallick	O
,	O
b	O
.	O
(	O
1998	O
)	O
perfect	B
simulation	I
for	O
orthogonal	O
model	B
mixing	O
.	O
technical	O
report	O
,	O
imperial	O
college	O
,	O
london	O
.	O
hopfield	O
,	O
j.	O
j	O
.	O
(	O
1974	O
)	O
kinetic	O
proofreading	O
:	O
a	O
new	O
mecha-	O
nism	O
for	O
reducing	O
errors	B
in	O
biosynthetic	O
processes	O
requiring	O
high	O
speci	O
(	O
cid:12	O
)	O
city	O
.	O
proc	O
.	O
natl	O
.	O
acad	O
.	O
sci	O
.	O
usa	O
71	O
(	O
10	O
)	O
:	O
4135	O
{	O
4139.	O
hopfield	O
,	O
j.	O
j	O
.	O
(	O
1978	O
)	O
origin	O
of	B
the	I
genetic	I
code	I
:	O
a	O
testable	O
hy-	O
pothesis	O
based	O
on	O
trna	O
structure	O
,	O
sequence	B
,	O
and	O
kinetic	O
proof-	O
reading	O
.	O
proc	O
.	O
natl	O
.	O
acad	O
.	O
sci	O
.	O
usa	O
75	O
(	O
9	O
)	O
:	O
4334	O
{	O
4338.	O
hopfield	O
,	O
j.	O
j	O
.	O
(	O
1980	O
)	O
the	O
energy	B
relay	O
:	O
a	O
proofreading	O
scheme	O
based	O
on	O
dynamic	O
cooperativity	O
and	O
lacking	O
all	O
characteristic	O
symptoms	O
of	O
kinetic	O
proofreading	O
in	O
dna	O
replication	B
and	O
pro-	O
tein	O
synthesis	B
.	O
proc	O
.	O
natl	O
.	O
acad	O
.	O
sci	O
.	O
usa	O
77	O
(	O
9	O
)	O
:	O
5248	O
{	O
5252.	O
hopfield	O
,	O
j.	O
j	O
.	O
(	O
1982	O
)	O
neural	O
networks	O
and	O
physical	O
systems	O
with	O
emergent	O
collective	B
computational	O
abilities	O
.	O
proc	O
.	O
natl	O
.	O
acad	O
.	O
sci	O
.	O
usa	O
79	O
:	O
2554	O
{	O
8.	O
hopfield	O
,	O
j.	O
j	O
.	O
(	O
1984	O
)	O
neurons	O
with	O
graded	O
response	O
properties	O
have	O
collective	B
computational	O
properties	O
like	O
those	O
of	O
two-state	O
neurons	O
.	O
proc	O
.	O
natl	O
.	O
acad	O
.	O
sci	O
.	O
usa	O
81	O
:	O
3088	O
{	O
92.	O
hopfield	O
,	O
j.	O
j	O
.	O
(	O
1987	O
)	O
learning	B
algorithms	I
and	O
probability	B
dis-	O
tributions	O
in	O
feed-forward	O
and	O
feed-back	O
networks	O
.	O
proc	O
.	O
natl	O
.	O
acad	O
.	O
sci	O
.	O
usa	O
84	O
:	O
8429	O
{	O
33.	O
hopfield	O
,	O
j.	O
j.	O
,	O
and	O
brody	O
,	O
c.	O
d.	O
(	O
2000	O
)	O
what	O
is	O
a	O
moment	O
?	O
\cortical	O
''	O
sensory	O
integration	O
over	O
a	O
brief	O
interval	O
.	O
proc	O
.	O
natl	O
.	O
acad	O
.	O
sci	O
97	O
:	O
13919	O
{	O
13924.	O
hopfield	O
,	O
j.	O
j.	O
,	O
and	O
brody	O
,	O
c.	O
d.	O
(	O
2001	O
)	O
what	O
is	O
a	O
moment	O
?	O
transient	O
synchrony	O
as	O
a	O
collective	B
mechanism	O
for	O
spatiotem-	O
poral	O
integration	O
.	O
proc	O
.	O
natl	O
.	O
acad	O
.	O
sci	O
98	O
:	O
1282	O
{	O
1287.	O
hopfield	O
,	O
j.	O
j.	O
,	O
and	O
tank	O
,	O
d.	O
w.	O
(	O
1985	O
)	O
neural	O
computation	O
of	O
decisions	O
in	O
optimization	O
problems	O
.	O
biol	O
.	O
cybernetics	O
52	O
:	O
1	O
{	O
25.	O
howarth	O
,	O
p.	O
,	O
and	O
bradley	O
,	O
a	O
.	O
(	O
1986	O
)	O
the	O
longitudinal	O
aberra-	O
tion	O
of	O
the	O
human	O
eye	O
and	O
its	O
correction	O
.	O
vision	B
res	O
.	O
26	O
:	O
361	O
{	O
366.	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
ghahramani	O
,	O
z	O
.	O
(	O
1997	O
)	O
generative	O
models	O
for	O
discovering	O
sparse	O
distributed	O
representations	O
.	O
philosophical	O
trans	O
.	O
royal	O
society	O
b	O
.	O
huber	O
,	O
m.	O
(	O
1998	O
)	O
exact	B
sampling	I
and	O
approximate	O
counting	B
techniques	O
.	O
in	O
proc	O
.	O
30th	O
acm	O
symposium	O
on	O
the	O
theory	B
of	O
computing	O
,	O
pp	O
.	O
31	O
{	O
40.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
616	O
bibliography	O
huffman	O
,	O
d.	O
(	O
1952	O
)	O
a	O
method	B
for	O
construction	B
of	O
minimum-	O
redundancy	B
codes	O
.	O
proc	O
.	O
of	O
ire	O
40	O
(	O
9	O
)	O
:	O
1098	O
{	O
1101.	O
ichikawa	O
,	O
k.	O
,	O
bhadeshia	O
,	O
h.	O
k.	O
d.	O
h.	O
,	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1996	O
)	O
model	B
for	O
hot	O
cracking	O
in	O
low-alloy	O
steel	O
weld	O
metals	O
.	O
science	O
and	O
technology	O
of	O
welding	O
and	O
joining	O
1	O
:	O
43	O
{	O
50.	O
levenshtein	O
,	O
v.	O
i	O
.	O
(	O
1966	O
)	O
binary	O
codes	O
capable	O
of	O
correcting	O
deletions	B
,	O
insertions	B
,	O
and	O
reversals	O
.	O
soviet	O
physics	B
{	O
doklady	O
10	O
(	O
8	O
)	O
:	O
707	O
{	O
710.	O
lin	O
,	O
s.	O
,	O
and	O
costello	O
,	O
jr.	O
,	O
d.	O
j	O
.	O
(	O
1983	O
)	O
error	O
control	O
coding	O
:	O
fundamentals	O
and	O
applications	O
.	O
prentice-hall	O
.	O
isard	O
,	O
m.	O
,	O
and	O
blake	O
,	O
a	O
.	O
(	O
1996	O
)	O
visual	O
tracking	O
by	O
stochastic	O
propagation	O
of	O
conditional	O
density	B
.	O
in	O
proc	O
.	O
fourth	O
european	O
conf	O
.	O
computer	B
vision	O
,	O
pp	O
.	O
343	O
{	O
356.	O
litsyn	O
,	O
s.	O
,	O
and	O
shevelev	O
,	O
v.	O
(	O
2002	O
)	O
on	O
ensembles	O
of	O
low-	O
density	B
parity-check	O
codes	O
:	O
asymptotic	O
distance	O
distributions	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
48	O
(	O
4	O
)	O
:	O
887	O
{	O
908.	O
isard	O
,	O
m.	O
,	O
and	O
blake	O
,	O
a	O
.	O
(	O
1998	O
)	O
condensation	O
{	O
conditional	B
density	O
propagation	O
for	O
visual	O
tracking	O
.	O
international	O
journal	O
of	O
computer	B
vision	O
29	O
(	O
1	O
)	O
:	O
5	O
{	O
28.	O
jaakkola	O
,	O
t.	O
s.	O
,	O
and	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
1996	O
)	O
computing	O
upper	O
and	O
lower	O
bounds	O
on	O
likelihoods	O
in	O
intractable	O
networks	O
.	O
in	O
proc	O
.	O
twelfth	O
conf	O
.	O
on	O
uncertainty	O
in	O
ai	O
.	O
morgan	O
kaufman	O
.	O
jaakkola	O
,	O
t.	O
s.	O
,	O
and	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
2000a	O
)	O
bayesian	O
logistic	O
regression	B
:	O
a	O
variational	B
approach	O
.	O
statistics	O
and	O
computing	O
10	O
:	O
25	O
{	O
37.	O
jaakkola	O
,	O
t.	O
s.	O
,	O
and	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
2000b	O
)	O
bayesian	O
parame-	O
ter	O
estimation	O
via	O
variational	B
methods	I
.	O
statistics	O
and	O
comput-	O
ing	O
10	O
(	O
1	O
)	O
:	O
25	O
{	O
37.	O
jaynes	O
,	O
e.	O
t.	O
(	O
1983	O
)	O
bayesian	O
intervals	B
versus	O
con	O
(	O
cid:12	O
)	O
dence	O
in-	O
tervals	O
.	O
in	O
e.t	O
.	O
jaynes	O
.	O
papers	O
on	O
probability	O
,	O
statistics	O
and	O
statistical	O
physics	B
,	O
ed	O
.	O
by	O
r.	O
d.	O
rosenkrantz	O
,	O
p.	O
151.	O
kluwer	O
.	O
jaynes	O
,	O
e.	O
t.	O
(	O
2003	O
)	O
probability	B
theory	O
:	O
the	O
logic	O
of	O
science	O
.	O
cambridge	O
univ	O
.	O
press	O
.	O
edited	O
by	O
g.	O
larry	O
bretthorst	O
.	O
jensen	O
,	O
f.	O
v.	O
(	O
1996	O
)	O
an	O
introduction	O
to	O
bayesian	O
networks	O
.	O
ucl	O
press	O
.	O
johannesson	O
,	O
r.	O
,	O
and	O
zigangirov	O
,	O
k.	O
s.	O
(	O
1999	O
)	O
fundamentals	O
of	O
convolutional	O
coding	O
.	O
ieee	O
press	O
.	O
jordan	O
,	O
m.	O
i.	O
ed	O
.	O
(	O
1998	O
)	O
learning	B
in	O
graphical	O
models	O
.	O
nato	O
science	O
series	O
.	O
kluwer	O
academic	O
publishers	O
.	O
jpl	O
,	O
(	O
1996	O
)	O
turbo	B
codes	I
performance	O
.	O
available	O
from	O
www331.jpl.nasa.gov/public/turboperf.html	O
.	O
jutten	O
,	O
c.	O
,	O
and	O
herault	O
,	O
j	O
.	O
(	O
1991	O
)	O
blind	O
separation	B
of	O
sources	O
.	O
1.	O
an	O
adaptive	B
algorithm	O
based	O
on	O
neuromimetic	O
architecture	B
.	O
signal	O
processing	O
24	O
(	O
1	O
)	O
:	O
1	O
{	O
10.	O
karplus	O
,	O
k.	O
,	O
and	O
krit	O
,	O
h.	O
(	O
1991	O
)	O
a	O
semi-systolic	O
decoder	B
for	O
the	O
pdsc	O
{	O
73	O
error-correcting	B
code	I
.	O
discrete	O
applied	O
mathe-	O
matics	O
33	O
:	O
109	O
{	O
128.	O
kepler	O
,	O
t.	O
,	O
and	O
oprea	O
,	O
m.	O
(	O
2001	O
)	O
improved	O
inference	B
of	O
muta-	O
tion	O
rates	O
:	O
i.	O
an	O
integral	B
representation	O
of	O
the	O
luria-delbr	O
(	O
cid:127	O
)	O
uck	O
distribution	B
.	O
theoretical	O
population	O
biology	O
59	O
:	O
41	O
{	O
48.	O
kimeldorf	O
,	O
g.	O
s.	O
,	O
and	O
wahba	O
,	O
g.	O
(	O
1970	O
)	O
a	O
correspondence	O
be-	O
tween	O
bayesian	O
estimation	O
of	O
stochastic	O
processes	O
and	O
smooth-	O
ing	O
by	O
splines	O
.	O
annals	O
of	O
math	O
.	O
statistics	O
41	O
(	O
2	O
)	O
:	O
495	O
{	O
502.	O
kitanidis	O
,	O
p.	O
k.	O
(	O
1986	O
)	O
parameter	O
uncertainty	O
in	O
estimation	O
of	O
spatial	O
functions	B
:	O
bayesian	O
analysis	B
.	O
water	O
resources	O
research	O
22	O
:	O
499	O
{	O
507.	O
loredo	O
,	O
t.	O
j	O
.	O
(	O
1990	O
)	O
from	O
laplace	O
to	O
supernova	O
sn	O
1987a	O
:	O
bayesian	O
inference	B
in	O
astrophysics	O
.	O
in	O
maximum	O
entropy	B
and	O
bayesian	O
methods	B
,	O
dartmouth	O
,	O
u.s.a.	O
,	O
1989	O
,	O
ed	O
.	O
by	O
p.	O
fougere	O
,	O
pp	O
.	O
81	O
{	O
142.	O
kluwer	O
.	O
lowe	O
,	O
d.	O
g.	O
(	O
1995	O
)	O
similarity	O
metric	B
learning	O
for	O
a	O
variable	O
kernel	O
classi	O
(	O
cid:12	O
)	O
er	O
.	O
neural	O
computation	O
7	O
:	O
72	O
{	O
85.	O
luby	O
,	O
m.	O
(	O
2002	O
)	O
lt	O
codes	O
.	O
in	O
proc	O
.	O
the	O
43rd	O
annual	O
ieee	O
sym-	O
posium	O
on	O
foundations	O
of	O
computer	O
science	O
,	O
november	O
16	O
{	O
19	O
2002	O
,	O
pp	O
.	O
271	O
{	O
282.	O
luby	O
,	O
m.	O
g.	O
,	O
mitzenmacher	O
,	O
m.	O
,	O
shokrollahi	O
,	O
m.	O
a.	O
,	O
and	O
spielman	O
,	O
d.	O
a	O
.	O
(	O
1998	O
)	O
improved	O
low-density	B
parity-check	I
codes	O
using	O
irregular	B
graphs	O
and	O
belief	O
propagation	O
.	O
in	O
proc	O
.	O
ieee	O
international	O
symposium	O
on	O
info	O
.	O
theory	B
,	O
p.	O
117.	O
luby	O
,	O
m.	O
g.	O
,	O
mitzenmacher	O
,	O
m.	O
,	O
shokrollahi	O
,	O
m.	O
a.	O
,	O
and	O
spielman	O
,	O
d.	O
a	O
.	O
(	O
2001a	O
)	O
e	O
(	O
cid:14	O
)	O
cient	O
erasure	B
correcting	O
codes	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
47	O
(	O
2	O
)	O
:	O
569	O
{	O
584.	O
luby	O
,	O
m.	O
g.	O
,	O
mitzenmacher	O
,	O
m.	O
,	O
shokrollahi	O
,	O
m.	O
a.	O
,	O
and	O
spielman	O
,	O
d.	O
a	O
.	O
(	O
2001b	O
)	O
improved	O
low-density	B
parity-check	I
codes	O
using	O
irregular	B
graphs	O
and	O
belief	O
propagation	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
47	O
(	O
2	O
)	O
:	O
585	O
{	O
598.	O
luby	O
,	O
m.	O
g.	O
,	O
mitzenmacher	O
,	O
m.	O
,	O
shokrollahi	O
,	O
m.	O
a.	O
,	O
spiel-	O
man	O
,	O
d.	O
a.	O
,	O
and	O
stemann	O
,	O
v.	O
(	O
1997	O
)	O
practical	B
loss-resilient	O
codes	O
.	O
in	O
proc	O
.	O
twenty-ninth	O
annual	O
acm	O
symposium	O
on	O
theory	O
of	O
computing	O
(	O
stoc	O
)	O
.	O
luo	O
,	O
z.	O
,	O
and	O
wahba	O
,	O
g.	O
(	O
1997	O
)	O
hybrid	O
adaptive	O
splines	O
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc	O
.	O
92	O
:	O
107	O
{	O
116.	O
luria	O
,	O
s.	O
e.	O
,	O
and	O
delbr	O
(	O
cid:127	O
)	O
uck	O
,	O
m.	O
(	O
1943	O
)	O
mutations	O
of	O
bacteria	O
from	O
virus	O
sensitivity	O
to	O
virus	O
resistance	O
.	O
genetics	O
28	O
:	O
491	O
{	O
511.	O
reprinted	O
in	O
microbiology	O
:	O
a	O
centenary	O
perspective	O
,	O
wolfgang	O
k.	O
joklik	O
,	O
ed.	O
,	O
1999	O
,	O
asm	O
press	O
,	O
and	O
available	O
from	O
www.esp.org/	O
.	O
luttrell	O
,	O
s.	O
p.	O
(	O
1989	O
)	O
hierarchical	O
vector	O
quantisation	O
.	O
proc	O
.	O
iee	O
part	O
i	O
136	O
:	O
405	O
{	O
413.	O
luttrell	O
,	O
s.	O
p.	O
(	O
1990	O
)	O
derivation	B
of	O
a	O
class	O
of	O
training	O
algo-	O
rithms	O
.	O
ieee	O
trans	O
.	O
on	O
neural	O
networks	O
1	O
(	O
2	O
)	O
:	O
229	O
{	O
232.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1991	O
)	O
bayesian	O
methods	B
for	O
adaptive	B
models	I
.	O
california	O
institute	O
of	O
technology	O
phd	O
dissertation	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1992a	O
)	O
bayesian	O
interpolation	O
.	O
neural	O
com-	O
putation	O
4	O
(	O
3	O
)	O
:	O
415	O
{	O
447.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1992b	O
)	O
the	O
evidence	B
framework	O
applied	O
to	O
knuth	O
,	O
d.	O
e.	O
(	O
1968	O
)	O
the	O
art	O
of	O
computer	O
programming	O
.	O
ad-	O
classi	O
(	O
cid:12	O
)	O
cation	O
networks	O
.	O
neural	O
computation	O
4	O
(	O
5	O
)	O
:	O
698	O
{	O
714.	O
dison	O
wesley	O
.	O
kondrashov	O
,	O
a.	O
s.	O
(	O
1988	O
)	O
deleterious	O
mutations	O
and	O
the	O
evo-	O
lution	O
of	O
sexual	O
reproduction	O
.	O
nature	O
336	O
(	O
6198	O
)	O
:	O
435	O
{	O
440.	O
kschischang	O
,	O
f.	O
r.	O
,	O
frey	O
,	O
b.	O
j.	O
,	O
and	O
loeliger	O
,	O
h.-a	O
.	O
(	O
2001	O
)	O
ieee	O
trans	O
.	O
factor	O
graphs	O
and	O
the	O
sum-product	O
algorithm	B
.	O
info	O
.	O
theory	B
47	O
(	O
2	O
)	O
:	O
498	O
{	O
519.	O
kschischang	O
,	O
f.	O
r.	O
,	O
and	O
sorokine	O
,	O
v.	O
(	O
1995	O
)	O
on	O
the	O
trel-	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
41	O
lis	O
structure	O
of	O
block	O
codes	O
.	O
(	O
6	O
)	O
:	O
1924	O
{	O
1937.	O
lauritzen	O
,	O
s.	O
l.	O
(	O
1981	O
)	O
time	O
series	O
analysis	B
in	O
1880	O
,	O
a	O
discussion	O
of	O
contributions	O
made	O
by	O
t.	O
n.	O
thiele	O
.	O
isi	O
review	O
49	O
:	O
319	O
{	O
333.	O
lauritzen	O
,	O
s.	O
l.	O
(	O
1996	O
)	O
graphical	O
models	O
.	O
number	O
17	O
in	O
oxford	O
statistical	B
science	O
series	O
.	O
clarendon	O
press	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1992c	O
)	O
a	O
practical	B
bayesian	O
framework	O
for	O
backpropagation	O
networks	O
.	O
neural	O
computation	O
4	O
(	O
3	O
)	O
:	O
448	O
{	O
472	O
.	O
(	O
1994a	O
)	O
bayesian	O
methods	B
for	O
backprop-	O
mackay	O
,	O
d.	O
j.	O
c.	O
in	O
models	O
of	O
neural	O
networks	O
iii	O
,	O
ed	O
.	O
by	O
agation	O
networks	O
.	O
e.	O
domany	O
,	O
j.	O
l.	O
van	O
hemmen	O
,	O
and	O
k.	O
schulten	O
,	O
chapter	O
6	O
,	O
pp	O
.	O
211	O
{	O
254.	O
springer	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1994b	O
)	O
bayesian	O
non-linear	O
modelling	B
for	O
the	O
prediction	B
competition	O
.	O
in	O
ashrae	O
trans.	O
,	O
v.100	O
,	O
pt.2	O
,	O
pp	O
.	O
1053	O
{	O
1062.	O
american	O
society	O
of	O
heating	O
,	O
refrigeration	O
,	O
and	O
air-conditioning	O
engineers	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1995a	O
)	O
free	B
energy	I
minimization	O
algorithm	B
for	O
decoding	B
and	O
cryptanalysis	B
.	O
electronics	O
letters	O
31	O
(	O
6	O
)	O
:	O
446	O
{	O
447.	O
lauritzen	O
,	O
s.	O
l.	O
,	O
and	O
spiegelhalter	O
,	O
d.	O
j	O
.	O
(	O
1988	O
)	O
local	O
com-	O
putations	O
with	O
probabilities	O
on	O
graphical	O
structures	O
and	O
their	O
application	O
to	O
expert	O
systems	O
.	O
j.	O
royal	O
statistical	B
society	O
b	O
50	O
:	O
157	O
{	O
224.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1995b	O
)	O
probable	O
networks	O
and	O
plausible	O
predictions	O
{	O
a	O
review	O
of	O
practical	O
bayesian	O
methods	B
for	O
su-	O
pervised	O
neural	O
networks	O
.	O
network	B
:	O
computation	O
in	O
neural	O
systems	O
6	O
:	O
469	O
{	O
505.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
bibliography	O
617	O
mackay	O
,	O
d.	O
j.	O
c.	O
,	O
(	O
1997a	O
)	O
ensemble	B
learning	I
for	O
hidden	O
markov	O
www.inference.phy.cam.ac.uk/mackay/abstracts/	O
models	O
.	O
ensemblepaper.html	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
,	O
(	O
1997b	O
)	O
iterative	B
probabilistic	I
decoding	I
of	O
low	O
density	B
parity	O
check	O
codes	O
.	O
animations	O
available	O
on	O
world	O
wide	O
web	O
.	O
www.inference.phy.cam.ac.uk/mackay/codes/gifs/	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1998a	O
)	O
choice	O
of	O
basis	O
for	O
laplace	O
approxi-	O
matheron	O
,	O
g.	O
(	O
1963	O
)	O
principles	O
of	O
geostatistics	O
.	O
economic	O
ge-	O
ology	O
58	O
:	O
1246	O
{	O
1266.	O
maynard	O
smith	O
,	O
j	O
.	O
(	O
1968	O
)	O
‘	O
haldane	O
’	O
s	O
dilemma	O
’	O
and	O
the	O
rate	B
of	O
evolution	B
.	O
nature	O
219	O
(	O
5159	O
)	O
:	O
1114	O
{	O
1116.	O
maynard	O
smith	O
,	O
j	O
.	O
(	O
1978	O
)	O
the	O
evolution	B
of	O
sex	O
.	O
cambridge	O
univ	O
.	O
press	O
.	O
maynard	O
smith	O
,	O
j	O
.	O
(	O
1988	O
)	O
games	O
,	O
sex	O
and	O
evolution	O
.	O
mation	O
.	O
machine	B
learning	I
33	O
(	O
1	O
)	O
:	O
77	O
{	O
86.	O
harvester	O
{	O
wheatsheaf	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1998b	O
)	O
introduction	O
to	O
gaussian	O
processes	O
.	O
in	O
neural	O
networks	O
and	O
machine	O
learning	B
,	O
ed	O
.	O
by	O
c.	O
m.	O
bishop	O
,	O
nato	O
asi	O
series	O
,	O
pp	O
.	O
133	O
{	O
166.	O
kluwer	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1999a	O
)	O
comparison	O
of	O
approximate	O
meth-	O
ods	O
for	O
handling	O
hyperparameters	O
.	O
neural	O
computation	O
11	O
(	O
5	O
)	O
:	O
1035	O
{	O
1068.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1999b	O
)	O
good	B
error	O
correcting	O
codes	O
based	O
on	O
very	O
sparse	O
matrices	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
45	O
(	O
2	O
)	O
:	O
399	O
{	O
431.	O
mackay	O
,	O
d.	O
j.	O
c.	O
,	O
(	O
2000	O
)	O
an	O
alternative	O
to	O
runlength-limiting	O
codes	O
:	O
turn	O
timing	B
errors	O
into	O
substitution	O
errors	B
.	O
available	O
from	O
www.inference.phy.cam.ac.uk/mackay/	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
,	O
(	O
2001	O
)	O
a	O
problem	O
with	O
variational	O
free	B
energy	I
minimization	O
.	O
www.inference.phy.cam.ac.uk/mackay/	O
abstracts/minima.html	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
,	O
and	O
davey	O
,	O
m.	O
c.	O
(	O
2000	O
)	O
evaluation	O
of	O
gal-	O
lager	O
codes	O
for	O
short	O
block	B
length	O
and	O
high	O
rate	B
applications	O
.	O
in	O
codes	O
,	O
systems	O
and	O
graphical	O
models	O
,	O
ed	O
.	O
by	O
b.	O
marcus	O
and	O
j.	O
rosenthal	O
,	O
volume	B
123	O
of	O
ima	O
volumes	O
in	O
mathematics	O
and	O
its	O
applications	O
,	O
pp	O
.	O
113	O
{	O
130.	O
springer	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
,	O
mitchison	O
,	O
g.	O
j.	O
,	O
and	O
mcfadden	O
,	O
p.	O
l.	O
(	O
2004	O
)	O
sparse-graph	O
codes	O
for	O
quantum	O
error-correction	B
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
50	O
(	O
10	O
)	O
:	O
2315	O
{	O
2330.	O
mackay	O
,	O
d.	O
j.	O
c.	O
,	O
and	O
neal	O
,	O
r.	O
m.	O
(	O
1995	O
)	O
good	B
codes	O
based	O
on	O
very	O
sparse	O
matrices	O
.	O
in	O
cryptography	O
and	O
coding	O
.	O
5th	O
ima	O
conf.	O
,	O
lncs	O
1025	O
,	O
ed	O
.	O
by	O
c.	O
boyd	O
,	O
pp	O
.	O
100	O
{	O
111.	O
springer	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
,	O
and	O
neal	O
,	O
r.	O
m.	O
(	O
1996	O
)	O
near	O
shannon	O
limit	O
performance	O
of	O
low	O
density	B
parity	O
check	O
codes	O
.	O
electron-	O
ics	O
letters	O
32	O
(	O
18	O
)	O
:	O
1645	O
{	O
1646.	O
reprinted	O
electronics	O
letters	O
,	O
33	O
(	O
6	O
)	O
:457	O
{	O
458	O
,	O
march	O
1997.	O
mackay	O
,	O
d.	O
j.	O
c.	O
,	O
and	O
peto	O
,	O
l.	O
(	O
1995	O
)	O
a	O
hierarchical	O
dirichlet	O
language	B
model	I
.	O
natural	B
language	O
engineering	O
1	O
(	O
3	O
)	O
:	O
1	O
{	O
19.	O
mackay	O
,	O
d.	O
j.	O
c.	O
,	O
wilson	O
,	O
s.	O
t.	O
,	O
and	O
davey	O
,	O
m.	O
c.	O
(	O
1998	O
)	O
comparison	O
of	O
constructions	O
of	O
irregular	O
gallager	O
codes	O
.	O
in	O
proc	O
.	O
36th	O
allerton	O
conf	O
.	O
on	O
communication	O
,	O
control	O
,	O
and	O
computing	O
,	O
sept.	O
1998	O
,	O
pp	O
.	O
220	O
{	O
229.	O
allerton	O
house	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
,	O
wilson	O
,	O
s.	O
t.	O
,	O
and	O
davey	O
,	O
m.	O
c.	O
(	O
1999	O
)	O
comparison	O
of	O
constructions	O
of	O
irregular	O
gallager	O
codes	O
.	O
ieee	O
trans	O
.	O
on	O
communications	O
47	O
(	O
10	O
)	O
:	O
1449	O
{	O
1454.	O
mackay	O
,	O
d.	O
m.	O
,	O
and	O
mackay	O
,	O
v.	O
(	O
1974	O
)	O
the	O
time	O
course	O
of	O
the	O
mccollough	O
e	O
(	O
cid:11	O
)	O
ect	O
and	O
its	O
physiological	O
implications	O
.	O
j.	O
physiol	O
.	O
237	O
:	O
38	O
{	O
39.	O
mackay	O
,	O
d.	O
m.	O
,	O
and	O
mcculloch	O
,	O
w.	O
s.	O
(	O
1952	O
)	O
the	O
limiting	O
information	B
capacity	O
of	O
a	O
neuronal	O
link	O
.	O
bull	O
.	O
math	O
.	O
biophys	O
.	O
14	O
:	O
127	O
{	O
135.	O
macwilliams	O
,	O
f.	O
j.	O
,	O
and	O
sloane	O
,	O
n.	O
j.	O
a	O
.	O
(	O
1977	O
)	O
the	O
theory	B
of	O
error-correcting	B
codes	I
.	O
north-holland	O
.	O
mandelbrot	O
,	O
b	O
.	O
(	O
1982	O
)	O
the	O
fractal	O
geometry	O
of	O
nature	O
.	O
w.h	O
.	O
freeman	O
.	O
mao	O
,	O
y.	O
,	O
and	O
banihashemi	O
,	O
a	O
.	O
(	O
2000	O
)	O
design	O
of	O
good	B
ldpc	O
codes	O
using	O
girth	O
distribution	B
.	O
in	O
ieee	O
international	O
sympo-	O
sium	O
on	O
info	O
.	O
theory	B
,	O
italy	O
,	O
june	O
,	O
2000	O
.	O
mao	O
,	O
y.	O
,	O
and	O
banihashemi	O
,	O
a	O
.	O
(	O
2001	O
)	O
a	O
heuristic	O
search	O
for	O
in	O
ieee	O
interna-	O
good	B
ldpc	O
codes	O
at	O
short	O
block	B
lengths	O
.	O
tional	O
conf	O
.	O
on	O
communications	O
.	O
marinari	O
,	O
e.	O
,	O
and	O
parisi	O
,	O
g.	O
(	O
1992	O
)	O
simulated	O
tempering	O
{	O
a	O
new	O
monte-carlo	O
scheme	O
.	O
europhysics	O
letters	O
19	O
(	O
6	O
)	O
:	O
451	O
{	O
458.	O
maynard	O
smith	O
,	O
j.	O
,	O
and	O
sz	O
(	O
cid:19	O
)	O
athmary	O
,	O
e.	O
(	O
1995	O
)	O
the	O
major	O
transitions	O
in	B
evolution	I
.	O
freeman	O
.	O
maynard	O
smith	O
,	O
j.	O
,	O
and	O
sz	O
(	O
cid:19	O
)	O
athmary	O
,	O
e.	O
(	O
1999	O
)	O
the	O
origins	O
of	O
life	O
.	O
oxford	O
univ	O
.	O
press	O
.	O
mccollough	O
,	O
c.	O
(	O
1965	O
)	O
color	O
adaptation	O
of	O
edge-detectors	O
in	O
the	O
human	B
visual	O
system	O
.	O
science	O
149	O
:	O
1115	O
{	O
1116.	O
mceliece	O
,	O
r.	O
j	O
.	O
(	O
2002	O
)	O
the	O
theory	B
of	O
information	B
and	O
coding	O
.	O
cambridge	O
univ	O
.	O
press	O
,	O
second	O
edition	O
.	O
mceliece	O
,	O
r.	O
j.	O
,	O
mackay	O
,	O
d.	O
j.	O
c.	O
,	O
and	O
cheng	O
,	O
j.-f.	O
(	O
1998	O
)	O
turbo	O
decoding	O
as	O
an	O
instance	O
of	O
pearl	O
’	O
s	O
‘	O
belief	B
propagation	I
’	O
al-	O
gorithm	O
.	O
ieee	O
journal	O
on	O
selected	O
areas	O
in	O
communications	O
16	O
(	O
2	O
)	O
:	O
140	O
{	O
152.	O
mcmillan	O
,	O
b	O
.	O
(	O
1956	O
)	O
two	O
inequalities	O
implied	O
by	O
unique	O
deci-	O
pherability	O
.	O
ire	O
trans	O
.	O
inform	O
.	O
theory	B
2	O
:	O
115	O
{	O
116.	O
minka	O
,	O
t.	O
(	O
2001	O
)	O
a	O
family	O
of	O
algorithms	O
for	O
approximate	O
bayesian	O
inference	B
.	O
mit	O
phd	O
dissertation	O
.	O
miskin	O
,	O
j.	O
w.	O
(	O
2001	O
)	O
ensemble	B
learning	I
for	O
independent	O
com-	O
ponent	O
analysis	B
.	O
dept	O
.	O
of	O
physics	O
,	O
univ	O
.	O
of	O
cambridge	O
phd	O
dissertation	O
.	O
miskin	O
,	O
j.	O
w.	O
,	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
2000	O
)	O
ensemble	B
learning	I
for	O
blind	O
image	B
separation	O
and	O
deconvolution	O
.	O
in	O
ad-	O
vances	O
in	O
independent	O
component	O
analysis	B
,	O
ed	O
.	O
by	O
m.	O
giro-	O
lami	O
.	O
springer	O
.	O
miskin	O
,	O
j.	O
w.	O
,	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
2001	O
)	O
ensemble	B
learning	I
for	O
blind	O
source	O
separation	O
.	O
in	O
ica	O
:	O
principles	O
and	O
practice	O
,	O
ed	O
.	O
by	O
s.	O
roberts	O
and	O
r.	O
everson	O
.	O
cambridge	O
univ	O
.	O
press	O
.	O
mosteller	O
,	O
f.	O
,	O
and	O
wallace	O
,	O
d.	O
l.	O
(	O
1984	O
)	O
applied	O
bayesian	O
and	O
classical	O
inference	B
.	O
the	O
case	O
of	O
the	O
federalist	O
papers	O
.	O
springer	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
1991	O
)	O
bayesian	O
mixture	B
modelling	I
by	O
monte	O
carlo	O
simulation	O
.	O
technical	O
report	O
crg	O
{	O
tr	O
{	O
91	O
{	O
2	O
,	O
computer	B
sci-	O
ence	O
,	O
univ	O
.	O
of	O
toronto	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
1993a	O
)	O
bayesian	O
learning	B
via	O
stochastic	B
dynamics	I
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
5	O
,	O
ed	O
.	O
by	O
c.	O
l.	O
giles	O
,	O
s.	O
j.	O
hanson	O
,	O
and	O
j.	O
d.	O
cowan	O
,	O
pp	O
.	O
475	O
{	O
482.	O
morgan	O
kaufmann	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
1993b	O
)	O
probabilistic	O
inference	O
using	O
markov	O
chain	B
monte	O
carlo	O
methods	B
.	O
technical	O
report	O
crg	O
{	O
tr	O
{	O
93	O
{	O
1	O
,	O
dept	O
.	O
of	O
computer	O
science	O
,	O
univ	O
.	O
of	O
toronto	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
1995	O
)	O
suppressing	O
random	B
walks	O
in	O
markov	O
chain	B
monte	O
carlo	O
using	O
ordered	B
overrelaxation	I
.	O
technical	O
report	O
9508	O
,	O
dept	O
.	O
of	O
statistics	O
,	O
univ	O
.	O
of	O
toronto	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
1996	O
)	O
bayesian	O
learning	B
for	O
neural	O
networks	O
.	O
springer	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
1997a	O
)	O
markov	O
chain	B
monte	O
carlo	O
methods	B
based	O
on	O
‘	O
slicing	O
’	O
the	O
density	B
function	O
.	O
technical	O
report	O
9722	O
,	O
dept	O
.	O
of	O
statistics	O
,	O
univ	O
.	O
of	O
toronto	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
1997b	O
)	O
monte	O
carlo	O
implementation	O
of	O
gaussian	O
process	O
models	O
for	O
bayesian	O
regression	B
and	O
classi	O
(	O
cid:12	O
)	O
cation	O
.	O
tech-	O
nical	O
report	O
crg	O
{	O
tr	O
{	O
97	O
{	O
2	O
,	O
dept	O
.	O
of	O
computer	O
science	O
,	O
univ	O
.	O
of	O
toronto	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
1998	O
)	O
annealed	B
importance	I
sampling	I
.	O
technical	O
report	O
9805	O
,	O
dept	O
.	O
of	O
statistics	O
,	O
univ	O
.	O
of	O
toronto	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
2001	O
)	O
de	O
(	O
cid:12	O
)	O
ning	O
priors	O
for	O
distributions	O
using	O
dirich-	O
let	O
di	O
(	O
cid:11	O
)	O
usion	O
trees	O
.	O
technical	O
report	O
0104	O
,	O
dept	O
.	O
of	O
statistics	O
,	O
univ	O
.	O
of	O
toronto	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
2003	O
)	O
slice	B
sampling	I
.	O
annals	O
of	O
statistics	O
31	O
(	O
3	O
)	O
:	O
705	O
{	O
767.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
618	O
bibliography	O
neal	O
,	O
r.	O
m.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
1998	O
)	O
a	O
new	O
view	O
of	O
the	O
em	O
algorithm	B
that	O
justi	O
(	O
cid:12	O
)	O
es	O
incremental	O
,	O
sparse	O
,	O
and	O
other	O
variants	O
.	O
in	O
learning	O
in	O
graphical	O
models	O
,	O
ed	O
.	O
by	O
m.	O
i.	O
jordan	O
,	O
nato	O
science	O
series	O
,	O
pp	O
.	O
355	O
{	O
368.	O
kluwer	O
.	O
nielsen	O
,	O
m.	O
,	O
and	O
chuang	O
,	O
i	O
.	O
(	O
2000	O
)	O
quantum	B
computation	O
and	O
quantum	O
information	B
.	O
cambridge	O
univ	O
.	O
press	O
.	O
offer	O
,	O
e.	O
,	O
and	O
soljanin	O
,	O
e.	O
(	O
2000	O
)	O
an	O
algebraic	O
description	O
of	O
iterative	O
decoding	B
schemes	O
.	O
in	O
codes	O
,	O
systems	O
and	O
graphi-	O
cal	O
models	O
,	O
ed	O
.	O
by	O
b.	O
marcus	O
and	O
j.	O
rosenthal	O
,	O
volume	B
123	O
of	O
ima	O
volumes	O
in	O
mathematics	O
and	O
its	O
applications	O
,	O
pp	O
.	O
283	O
{	O
298.	O
springer	O
.	O
offer	O
,	O
e.	O
,	O
and	O
soljanin	O
,	O
e.	O
(	O
2001	O
)	O
ldpc	O
codes	O
:	O
a	O
group	O
al-	O
gebra	O
formulation	O
.	O
in	O
proc	O
.	O
internat	O
.	O
workshop	O
on	O
coding	O
and	O
cryptography	O
wcc	O
2001	O
,	O
8-12	O
jan.	O
2001	O
,	O
paris	O
.	O
o	O
’	O
hagan	O
,	O
a	O
.	O
(	O
1978	O
)	O
on	O
curve	O
(	O
cid:12	O
)	O
tting	O
and	O
optimal	O
design	O
for	O
regression	B
.	O
j.	O
royal	O
statistical	B
society	O
,	O
b	O
40	O
:	O
1	O
{	O
42.	O
o	O
’	O
hagan	O
,	O
a	O
.	O
(	O
1987	O
)	O
monte	O
carlo	O
is	O
fundamentally	O
unsound	O
.	O
the	O
statistician	O
36	O
:	O
247	O
{	O
249.	O
o	O
’	O
hagan	O
,	O
a	O
.	O
(	O
1994	O
)	O
bayesian	O
inference	B
,	O
volume	B
2b	O
of	O
kendall	O
’	O
s	O
advanced	O
theory	B
of	O
statistics	O
.	O
edward	O
arnold	O
.	O
omre	O
,	O
h.	O
(	O
1987	O
)	O
bayesian	O
kriging	B
{	O
merging	O
observations	O
and	O
quali	O
(	O
cid:12	O
)	O
ed	O
guesses	O
in	O
kriging	O
.	O
mathematical	O
geology	O
19	O
:	O
25	O
{	O
39	O
.	O
(	O
2000	O
)	O
gaussian	O
processes	O
for	O
classi	O
(	O
cid:12	O
)	O
cation	O
:	O
mean-	O
(	O
cid:12	O
)	O
eld	O
algorithms	B
.	O
neural	O
computation	O
12	O
(	O
11	O
)	O
:	O
2655	O
{	O
2684.	O
opper	O
,	O
m.	O
,	O
and	O
winther	O
,	O
o.	O
rasmussen	O
,	O
c.	O
e.	O
,	O
and	O
ghahramani	O
,	O
z	O
.	O
(	O
2003	O
)	O
bayesian	O
monte	O
carlo	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
xv	O
,	O
ed	O
.	O
by	O
s.	O
becker	O
,	O
s.	O
thrun	O
,	O
and	O
k.	O
obermayer	O
.	O
ratliff	O
,	O
f.	O
,	O
and	O
riggs	O
,	O
l.	O
a	O
.	O
(	O
1950	O
)	O
involuntary	O
motions	O
of	O
the	O
eye	O
during	O
monocular	O
(	O
cid:12	O
)	O
xation	O
.	O
j.	O
exptl	O
.	O
psychol	O
.	O
40	O
:	O
687	O
{	O
701.	O
ratzer	O
,	O
e.	O
a.	O
,	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
2003	O
)	O
sparse	O
low-density	O
parity-check	O
codes	O
for	O
channels	O
with	O
cross-talk	O
.	O
in	O
proc	O
.	O
2003	O
ieee	O
info	O
.	O
theory	B
workshop	O
,	O
paris	O
.	O
reif	O
,	O
f.	O
(	O
1965	O
)	O
fundamentals	O
of	O
statistical	O
and	O
thermal	O
physics	B
.	O
mcgraw	O
{	O
hill	O
.	O
richardson	O
,	O
t.	O
,	O
shokrollahi	O
,	O
m.	O
a.	O
,	O
and	O
urbanke	O
,	O
r.	O
(	O
2001	O
)	O
design	O
of	O
capacity-approaching	O
irregular	B
low-density	O
parity	B
check	O
codes	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
47	O
(	O
2	O
)	O
:	O
619	O
{	O
637	O
.	O
(	O
2001a	O
)	O
the	O
capacity	B
of	O
low-density	O
parity	O
check	O
codes	O
under	O
message-passing	B
decod-	O
ing	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
47	O
(	O
2	O
)	O
:	O
599	O
{	O
618.	O
richardson	O
,	O
t.	O
,	O
and	O
urbanke	O
,	O
r.	O
richardson	O
,	O
t.	O
,	O
and	O
urbanke	O
,	O
r.	O
(	O
2001b	O
)	O
e	O
(	O
cid:14	O
)	O
cient	O
encoding	O
of	O
low-density	O
parity-check	O
codes	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
47	O
(	O
2	O
)	O
:	O
638	O
{	O
656.	O
ridley	O
,	O
m.	O
(	O
2000	O
)	O
mendel	O
’	O
s	O
demon	O
:	O
gene	O
justice	O
and	O
the	O
com-	O
plexity	O
of	O
life	O
.	O
phoenix	O
.	O
ripley	O
,	O
b.	O
d.	O
(	O
1991	O
)	O
statistical	B
inference	O
for	O
spatial	O
processes	O
.	O
cambridge	O
univ	O
.	O
press	O
.	O
ripley	O
,	O
b.	O
d.	O
(	O
1996	O
)	O
pattern	B
recognition	I
and	I
neural	O
networks	O
.	O
cambridge	O
univ	O
.	O
press	O
.	O
patrick	O
,	O
j.	O
d.	O
,	O
and	O
wallace	O
,	O
c.	O
s.	O
(	O
1982	O
)	O
stone	O
circle	B
geome-	O
tries	O
:	O
an	O
information	B
theory	I
approach	O
.	O
in	O
archaeoastronomy	O
in	O
the	O
old	O
world	O
,	O
ed	O
.	O
by	O
d.	O
c.	O
heggie	O
,	O
pp	O
.	O
231	O
{	O
264.	O
cambridge	O
univ	O
.	O
press	O
.	O
rumelhart	O
,	O
d.	O
e.	O
,	O
hinton	O
,	O
g.	O
e.	O
,	O
and	O
williams	O
,	O
r.	O
j	O
.	O
(	O
1986	O
)	O
learning	B
representations	O
by	O
back-propagating	O
errors	B
.	O
nature	O
323	O
:	O
533	O
{	O
536.	O
russell	O
,	O
s.	O
,	O
and	O
wefald	O
,	O
e.	O
(	O
1991	O
)	O
do	B
the	I
right	I
thing	I
:	O
studies	O
pearl	O
,	O
j	O
.	O
(	O
1988	O
)	O
probabilistic	O
reasoning	O
in	O
intelligent	O
systems	O
:	O
in	O
limited	O
rationality	O
.	O
mit	O
press	O
.	O
networks	O
of	O
plausible	O
inference	B
.	O
morgan	O
kaufmann	O
.	O
pearl	O
,	O
j	O
.	O
(	O
2000	O
)	O
causality	O
.	O
cambridge	O
univ	O
.	O
press	O
.	O
pearlmutter	O
,	O
b.	O
a.	O
,	O
and	O
parra	O
,	O
l.	O
c.	O
(	O
1996	O
)	O
a	O
context-	O
sensitive	O
generalization	B
of	O
ica	O
.	O
in	O
international	O
conf	O
.	O
on	O
neu-	O
ral	O
information	B
processing	O
,	O
hong	O
kong	O
,	O
pp	O
.	O
151	O
{	O
157.	O
pearlmutter	O
,	O
b.	O
a.	O
,	O
and	O
parra	O
,	O
l.	O
c.	O
(	O
1997	O
)	O
maximum	B
likelihood	I
blind	O
source	O
separation	O
:	O
a	O
context-sensitive	O
general-	O
ization	O
of	O
ica	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
,	O
ed	O
.	O
by	O
m.	O
c.	O
mozer	O
,	O
m.	O
i.	O
jordan	O
,	O
and	O
t.	O
petsche	O
,	O
volume	B
9	O
,	O
p.	O
613.	O
mit	O
press	O
.	O
pinto	O
,	O
r.	O
l.	O
,	O
and	O
neal	O
,	O
r.	O
m.	O
(	O
2001	O
)	O
improving	O
markov	O
chain	B
monte	O
carlo	O
estimators	O
by	O
coupling	O
to	O
an	O
approximating	O
chain	B
.	O
technical	O
report	O
0101	O
,	O
dept	O
.	O
of	O
statistics	O
,	O
univ	O
.	O
of	O
toronto	O
.	O
poggio	O
,	O
t.	O
,	O
and	O
girosi	O
,	O
f.	O
(	O
1989	O
)	O
a	O
theory	B
of	O
networks	O
for	O
approximation	O
and	B
learning	I
.	O
technical	O
report	O
a.i	O
.	O
1140	O
,	O
mit	O
.	O
poggio	O
,	O
t.	O
,	O
and	O
girosi	O
,	O
f.	O
(	O
1990	O
)	O
networks	O
for	O
approximation	O
and	B
learning	I
.	O
proc	O
.	O
ieee	O
78	O
:	O
1481	O
{	O
1497.	O
polya	O
,	O
g.	O
(	O
1954	O
)	O
induction	O
and	O
analogy	O
in	O
mathematics	O
.	O
prince-	O
ton	O
univ	O
.	O
press	O
.	O
schneier	O
,	O
b	O
.	O
(	O
1996	O
)	O
applied	O
cryptography	B
.	O
wiley	O
.	O
scholkopf	O
,	O
b.	O
,	O
burges	O
,	O
c.	O
,	O
and	O
vapnik	O
,	O
v.	O
(	O
1995	O
)	O
extract-	O
ing	O
support	O
data	O
for	O
a	O
given	O
task	O
.	O
in	O
proc	O
.	O
first	O
international	O
conf	O
.	O
on	O
knowledge	O
discovery	O
and	O
data	O
mining	O
,	O
ed	O
.	O
by	O
u.	O
m.	O
fayyad	O
and	O
r.	O
uthurusamy	O
.	O
aaai	O
press	O
.	O
scholtz	O
,	O
r.	O
a	O
.	O
(	O
1982	O
)	O
the	O
origins	O
of	O
spread-spectrum	O
commu-	O
nications	O
.	O
ieee	O
trans	O
.	O
on	O
communications	O
30	O
(	O
5	O
)	O
:	O
822	O
{	O
854.	O
seeger	O
,	O
m.	O
,	O
williams	O
,	O
c.	O
k.	O
i.	O
,	O
and	O
lawrence	O
,	O
n.	O
(	O
2003	O
)	O
fast	O
forward	O
selection	O
to	O
speed	O
up	O
sparse	O
gaussian	O
process	O
re-	O
gression	O
.	O
in	O
proc	O
.	O
ninth	O
international	O
workshop	O
on	O
arti	O
(	O
cid:12	O
)	O
cial	O
intelligence	O
and	O
statistics	O
,	O
ed	O
.	O
by	O
c.	O
bishop	O
and	O
b.	O
j.	O
frey	O
.	O
society	O
for	O
arti	O
(	O
cid:12	O
)	O
cial	O
intelligence	O
and	O
statistics	O
.	O
sejnowski	O
,	O
t.	O
j	O
.	O
(	O
1986	O
)	O
higher	O
order	O
boltzmann	O
machines	O
.	O
in	O
neural	O
networks	O
for	O
computing	O
,	O
ed	O
.	O
by	O
j.	O
denker	O
,	O
pp	O
.	O
398	O
{	O
403.	O
american	O
institute	O
of	O
physics	O
.	O
sejnowski	O
,	O
t.	O
j.	O
,	O
and	O
rosenberg	O
,	O
c.	O
r.	O
(	O
1987	O
)	O
parallel	O
net-	O
works	O
that	O
learn	O
to	O
pronounce	O
english	O
text	O
.	O
journal	O
of	O
complex	B
systems	O
1	O
(	O
1	O
)	O
:	O
145	O
{	O
168.	O
shannon	O
,	O
c.	O
e.	O
(	O
1948	O
)	O
a	O
mathematical	O
theory	B
of	O
communica-	O
propp	O
,	O
j.	O
g.	O
,	O
and	O
wilson	O
,	O
d.	O
b	O
.	O
(	O
1996	O
)	O
exact	B
sampling	I
with	O
coupled	O
markov	O
chains	O
and	O
applications	O
to	O
statistical	B
mechan-	O
ics	O
.	O
random	B
structures	O
and	O
algorithms	O
9	O
(	O
1-2	O
)	O
:	O
223	O
{	O
252.	O
rabiner	O
,	O
l.	O
r.	O
,	O
and	O
juang	O
,	O
b.	O
h.	O
(	O
1986	O
)	O
an	O
introduction	O
to	O
tion	O
.	O
bell	O
sys	O
.	O
tech	O
.	O
j	O
.	O
27	O
:	O
379	O
{	O
423	O
,	O
623	O
{	O
656.	O
shannon	O
,	O
c.	O
e.	O
(	O
1993	O
)	O
the	O
best	O
detection	O
of	O
pulses	O
.	O
in	O
collected	O
papers	O
of	O
claude	O
shannon	O
,	O
ed	O
.	O
by	O
n.	O
j.	O
a.	O
sloane	O
and	O
a.	O
d.	O
wyner	O
,	O
pp	O
.	O
148	O
{	O
150.	O
ieee	O
press	O
.	O
hidden	O
markov	O
models	O
.	O
ieee	O
assp	O
magazine	O
pp	O
.	O
4	O
{	O
16.	O
shannon	O
,	O
c.	O
e.	O
,	O
and	O
weaver	O
,	O
w.	O
(	O
1949	O
)	O
the	O
mathematical	O
rasmussen	O
,	O
c.	O
e.	O
(	O
1996	O
)	O
evaluation	O
of	O
gaussian	O
processes	O
and	O
other	O
methods	B
for	O
non-linear	O
regression	B
.	O
univ	O
.	O
of	O
toronto	O
phd	O
dissertation	O
.	O
rasmussen	O
,	O
c.	O
e.	O
(	O
2000	O
)	O
the	O
in	O
(	O
cid:12	O
)	O
nite	O
gaussian	O
mixture	O
model	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
12	O
,	O
ed	O
.	O
by	O
s.	O
solla	O
,	O
t.	O
leen	O
,	O
and	O
k.-r.	O
m	O
(	O
cid:127	O
)	O
uller	O
,	O
pp	O
.	O
554	O
{	O
560.	O
mit	O
press	O
.	O
rasmussen	O
,	O
c.	O
e.	O
,	O
(	O
2002	O
)	O
reduced	O
rank	O
gaussian	O
process	O
learn-	O
ing	O
.	O
unpublished	O
manuscript	O
.	O
rasmussen	O
,	O
c.	O
e.	O
,	O
and	O
ghahramani	O
,	O
z	O
.	O
(	O
2002	O
)	O
in	O
(	O
cid:12	O
)	O
nite	O
mixtures	O
of	O
gaussian	O
process	O
experts	O
.	O
in	O
advances	O
in	O
neural	O
informa-	O
tion	O
processing	O
systems	O
14	O
,	O
ed	O
.	O
by	O
t.	O
g.	O
diettrich	O
,	O
s.	O
becker	O
,	O
and	O
z.	O
ghahramani	O
.	O
mit	O
press	O
.	O
theory	B
of	O
communication	B
.	O
univ	O
.	O
of	O
illinois	O
press	O
.	O
shokrollahi	O
,	O
a	O
.	O
(	O
2003	O
)	O
raptor	B
codes	I
.	O
technical	O
report	O
,	O
labo-	O
ratoire	O
d	O
’	O
algorithmique	O
,	O
(	O
cid:19	O
)	O
ecole	O
polytechnique	O
f	O
(	O
cid:19	O
)	O
ed	O
(	O
cid:19	O
)	O
erale	O
de	O
lau-	O
sanne	O
,	O
lausanne	O
,	O
switzerland	O
.	O
available	O
from	O
algo.epfl.ch/	O
.	O
sipser	O
,	O
m.	O
,	O
and	O
spielman	O
,	O
d.	O
a	O
.	O
(	O
1996	O
)	O
expander	O
codes	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
42	O
(	O
6.1	O
)	O
:	O
1710	O
{	O
1722.	O
skilling	O
,	O
j	O
.	O
(	O
1989	O
)	O
classic	O
maximum	B
entropy	I
.	O
in	O
maxi-	O
mum	O
entropy	B
and	O
bayesian	O
methods	B
,	O
cambridge	O
1988	O
,	O
ed	O
.	O
by	O
j.	O
skilling	O
.	O
kluwer	O
.	O
skilling	O
,	O
j	O
.	O
(	O
1993	O
)	O
bayesian	O
numerical	O
analysis	B
.	O
in	O
physics	O
and	O
probability	O
,	O
ed	O
.	O
by	O
w.	O
t.	O
grandy	O
,	O
jr.	O
and	O
p.	O
milonni	O
.	O
cam-	O
bridge	O
univ	O
.	O
press	O
.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O
bibliography	O
619	O
skilling	O
,	O
j.	O
,	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
2003	O
)	O
slice	B
sampling	I
{	O
a	O
binary	O
implementation	O
.	O
annals	O
of	O
statistics	O
31	O
(	O
3	O
)	O
:	O
753	O
{	O
755.	O
discussion	O
of	O
slice	O
sampling	O
by	O
radford	O
m.	O
neal	O
.	O
slepian	O
,	O
d.	O
,	O
and	O
wolf	O
,	O
j	O
.	O
(	O
1973	O
)	O
noiseless	B
coding	O
of	O
correlated	O
information	B
sources	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
19	O
:	O
471	O
{	O
480.	O
smola	O
,	O
a.	O
j.	O
,	O
and	O
bartlett	O
,	O
p.	O
(	O
2001	O
)	O
sparse	O
greedy	O
gaus-	O
sian	O
process	O
regression	B
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
13	O
,	O
ed	O
.	O
by	O
t.	O
k.	O
leen	O
,	O
t.	O
g.	O
diettrich	O
,	O
and	O
v.	O
tresp	O
,	O
pp	O
.	O
619	O
{	O
625.	O
mit	O
press	O
.	O
spiegel	O
,	O
m.	O
r.	O
(	O
1988	O
)	O
statistics	O
.	O
schaum	O
’	O
s	O
outline	O
series	O
.	O
mcgraw-hill	O
,	O
2nd	O
edition	O
.	O
spielman	O
,	O
d.	O
a.	O
able	O
error-correcting	B
codes	I
.	O
(	O
6.1	O
)	O
:	O
1723	O
{	O
1731	O
.	O
(	O
1996	O
)	O
linear-time	O
encodable	O
and	O
decod-	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
42	O
sutton	O
,	O
r.	O
s.	O
,	O
and	O
barto	O
,	O
a.	O
g.	O
(	O
1998	O
)	O
reinforcement	O
learn-	O
ing	O
:	O
an	O
introduction	O
.	O
mit	O
press	O
.	O
swanson	O
,	O
l.	O
(	O
1988	O
)	O
a	O
new	O
code	B
for	O
galileo	O
.	O
in	O
proc	O
.	O
1988	O
ieee	O
international	O
symposium	O
info	O
.	O
theory	B
,	O
pp	O
.	O
94	O
{	O
95.	O
tanner	O
,	O
m.	O
a	O
.	O
(	O
1996	O
)	O
tools	O
for	O
statistical	O
inference	B
:	O
methods	B
for	O
the	O
exploration	O
of	O
posterior	O
distributions	O
and	O
likelihood	O
functions	B
.	O
springer	O
series	O
in	B
statistics	I
.	O
springer	O
,	O
3rd	O
edition	O
.	O
tanner	O
,	O
r.	O
m.	O
(	O
1981	O
)	O
a	O
recursive	O
approach	O
to	O
low	O
complexity	B
codes	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
27	O
(	O
5	O
)	O
:	O
533	O
{	O
547.	O
teahan	O
,	O
w.	O
j	O
.	O
(	O
1995	O
)	O
probability	B
estimation	O
for	O
ppm	O
.	O
in	O
proc	O
.	O
nzcsrsc	O
’	O
95	O
.	O
available	O
from	O
citeseer.nj.nec.com/	O
teahan95probability.html	O
.	O
ten	O
brink	O
,	O
s.	O
(	O
1999	O
)	O
convergence	O
of	O
iterative	O
decoding	B
.	O
elec-	O
tronics	O
letters	O
35	O
(	O
10	O
)	O
:	O
806	O
{	O
808.	O
ten	O
brink	O
,	O
s.	O
,	O
kramer	O
,	O
g.	O
,	O
and	O
ashikhmin	O
,	O
a.	O
,	O
(	O
2002	O
)	O
design	O
of	O
low-density	B
parity-check	I
codes	O
for	O
multi-antenna	O
modulation	O
and	O
detection	O
.	O
submitted	O
to	O
ieee	O
trans	O
.	O
on	O
communications	O
.	O
terras	O
,	O
a	O
.	O
(	O
1999	O
)	O
fourier	O
analysis	B
on	O
finite	O
groups	O
and	O
ap-	O
plications	O
.	O
cambridge	O
univ	O
.	O
press	O
.	O
thomas	O
,	O
a.	O
,	O
spiegelhalter	O
,	O
d.	O
j.	O
,	O
and	O
gilks	O
,	O
w.	O
r.	O
(	O
1992	O
)	O
bugs	O
:	O
a	O
program	O
to	O
perform	O
bayesian	O
inference	B
using	O
gibbs	O
sampling	O
.	O
in	O
bayesian	O
statistics	O
4	O
,	O
ed	O
.	O
by	O
j.	O
m.	O
bernardo	O
,	O
j.	O
o.	O
berger	O
,	O
a.	O
p.	O
dawid	O
,	O
and	O
a.	O
f.	O
m.	O
smith	O
,	O
pp	O
.	O
837	O
{	O
842.	O
clarendon	O
press	O
.	O
tresp	O
,	O
v.	O
(	O
2000	O
)	O
a	O
bayesian	O
committee	O
machine	O
.	O
neural	O
com-	O
putation	O
12	O
(	O
11	O
)	O
:	O
2719	O
{	O
2741.	O
urbanke	O
,	O
r.	O
,	O
(	O
2001	O
)	O
ldpcopt	O
{	O
a	O
fast	O
and	O
accurate	O
degree	B
distri-	O
bution	O
optimizer	O
for	O
ldpc	O
code	B
ensembles	O
.	O
lthcwww.epfl.ch/	O
research/ldpcopt/	O
.	O
vapnik	O
,	O
v.	O
(	O
1995	O
)	O
the	O
nature	O
of	O
statistical	O
learning	B
theory	O
.	O
springer	O
.	O
viterbi	O
,	O
a.	O
j	O
.	O
(	O
1967	O
)	O
error	O
bounds	O
for	O
convolutional	O
codes	O
and	O
an	O
asymptotically	O
optimum	O
decoding	B
algorithm	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
it-13	O
:	O
260	O
{	O
269.	O
wahba	O
,	O
g.	O
(	O
1990	O
)	O
spline	B
models	O
for	O
observational	O
data	O
.	O
society	O
for	O
industrial	O
and	O
applied	O
mathematics	O
.	O
cbms-nsf	O
regional	O
conf	O
.	O
series	O
in	O
applied	O
mathematics	O
.	O
wainwright	O
,	O
m.	O
j.	O
,	O
jaakkola	O
,	O
t.	O
,	O
and	O
willsky	O
,	O
a.	O
s.	O
(	O
2003	O
)	O
tree-based	O
reparameterization	O
framework	O
for	O
analysis	O
of	O
sum-	O
product	O
and	O
related	O
algorithms	B
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
45	O
(	O
9	O
)	O
:	O
1120	O
{	O
1146.	O
wald	O
,	O
g.	O
,	O
and	O
griffin	O
,	O
d.	O
(	O
1947	O
)	O
the	O
change	O
in	O
refractive	O
power	O
of	O
the	O
eye	O
in	O
bright	O
and	O
dim	O
light	O
.	O
j.	O
opt	O
.	O
soc	O
.	O
am	O
.	O
37	O
:	O
321	O
{	O
336.	O
wallace	O
,	O
c.	O
,	O
and	O
boulton	O
,	O
d.	O
(	O
1968	O
)	O
an	O
information	B
measure	O
for	O
classi	O
(	O
cid:12	O
)	O
cation	O
.	O
comput	O
.	O
j	O
.	O
11	O
(	O
2	O
)	O
:	O
185	O
{	O
194.	O
wallace	O
,	O
c.	O
s.	O
,	O
and	O
freeman	O
,	O
p.	O
r.	O
(	O
1987	O
)	O
estimation	O
and	O
inference	O
by	O
compact	O
coding	O
.	O
j.	O
r.	O
statist	O
.	O
soc	O
.	O
b	O
49	O
(	O
3	O
)	O
:	O
240	O
{	O
265.	O
ward	O
,	O
d.	O
j.	O
,	O
blackwell	O
,	O
a.	O
f.	O
,	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
2000	O
)	O
dasher	O
{	O
a	O
data	B
entry	I
interface	O
using	O
continuous	B
gestures	O
and	O
language	O
models	O
.	O
in	O
proc	O
.	O
user	O
interface	O
software	B
and	O
tech-	O
nology	O
2000	O
,	O
pp	O
.	O
129	O
{	O
137.	O
ward	O
,	O
d.	O
j.	O
,	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
2002	O
)	O
fast	O
hands-free	O
writing	B
by	O
gaze	O
direction	O
.	O
nature	O
418	O
(	O
6900	O
)	O
:	O
838.	O
welch	O
,	O
t.	O
a	O
.	O
(	O
1984	O
)	O
a	O
technique	O
for	O
high-performance	O
data	B
compression	I
.	O
ieee	O
computer	B
17	O
(	O
6	O
)	O
:	O
8	O
{	O
19.	O
welling	O
,	O
m.	O
,	O
and	O
teh	O
,	O
y.	O
w.	O
(	O
2001	O
)	O
belief	B
optimization	O
for	O
binary	O
networks	O
:	O
a	O
stable	O
alternative	O
to	O
loopy	O
belief	O
propaga-	O
tion	O
.	O
in	O
uncertainty	O
in	O
arti	O
(	O
cid:12	O
)	O
cial	O
intelligence	O
:	O
proc	O
.	O
seventeenth	O
conf	O
.	O
(	O
uai-2001	O
)	O
,	O
pp	O
.	O
554	O
{	O
561.	O
morgan	O
kaufmann	O
.	O
wiberg	O
,	O
n.	O
(	O
1996	O
)	O
codes	O
and	O
decoding	O
on	O
general	O
graphs	O
.	O
dept	O
.	O
of	O
elec	O
.	O
eng.	O
,	O
link	O
(	O
cid:127	O
)	O
oping	O
,	O
sweden	O
phd	O
dissertation	O
.	O
link	O
(	O
cid:127	O
)	O
oping	O
studies	O
in	O
science	O
and	O
technology	O
no	O
.	O
440.	O
wiberg	O
,	O
n.	O
,	O
loeliger	O
,	O
h.-a.	O
,	O
and	O
k	O
(	O
cid:127	O
)	O
otter	O
,	O
r.	O
(	O
1995	O
)	O
codes	O
and	O
iterative	O
decoding	B
on	O
general	O
graphs	O
.	O
european	O
trans	O
.	O
on	O
telecommunications	O
6	O
:	O
513	O
{	O
525.	O
wiener	O
,	O
n.	O
(	O
1948	O
)	O
cybernetics	O
.	O
wiley	O
.	O
williams	O
,	O
c.	O
k.	O
i.	O
,	O
and	O
rasmussen	O
,	O
c.	O
e.	O
(	O
1996	O
)	O
gaussian	O
processes	O
for	O
regression	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
8	O
,	O
ed	O
.	O
by	O
d.	O
s.	O
touretzky	O
,	O
m.	O
c.	O
mozer	O
,	O
and	O
m.	O
e.	O
hasselmo	O
.	O
mit	O
press	O
.	O
williams	O
,	O
c.	O
k.	O
i.	O
,	O
and	O
seeger	O
,	O
m.	O
(	O
2001	O
)	O
using	O
the	O
nystr	O
(	O
cid:127	O
)	O
om	O
method	B
to	O
speed	O
up	O
kernel	B
machines	O
.	O
in	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
13	O
,	O
ed	O
.	O
by	O
t.	O
k.	O
leen	O
,	O
t.	O
g.	O
diettrich	O
,	O
and	O
v.	O
tresp	O
,	O
pp	O
.	O
682	O
{	O
688.	O
mit	O
press	O
.	O
witten	O
,	O
i.	O
h.	O
,	O
neal	O
,	O
r.	O
m.	O
,	O
and	O
cleary	O
,	O
j.	O
g.	O
(	O
1987	O
)	O
arith-	O
metic	O
coding	O
for	O
data	B
compression	I
.	O
communications	O
of	O
the	O
acm	O
30	O
(	O
6	O
)	O
:	O
520	O
{	O
540.	O
wolf	O
,	O
j.	O
k.	O
,	O
and	O
siegel	O
,	O
p.	O
(	O
1998	O
)	O
on	O
two-dimensional	O
arrays	O
and	O
crossword	O
puzzles	O
.	O
in	O
proc	O
.	O
36th	O
allerton	O
conf	O
.	O
on	O
com-	O
munication	O
,	O
control	O
,	O
and	O
computing	O
,	O
sept.	O
1998	O
,	O
pp	O
.	O
366	O
{	O
371.	O
allerton	O
house	O
.	O
worthen	O
,	O
a.	O
p.	O
,	O
and	O
stark	O
,	O
w.	O
e.	O
(	O
1998	O
)	O
low-density	O
parity	O
check	O
codes	O
for	O
fading	O
channels	O
with	B
memory	I
.	O
in	O
proc	O
.	O
36th	O
allerton	O
conf	O
.	O
on	O
communication	O
,	O
control	O
,	O
and	O
computing	O
,	O
sept.	O
1998	O
,	O
pp	O
.	O
117	O
{	O
125.	O
yedidia	O
,	O
j.	O
s.	O
(	O
2000	O
)	O
an	O
idiosyncratic	O
journey	O
beyond	O
mean	B
(	O
cid:12	O
)	O
eld	O
theory	B
.	O
technical	O
report	O
,	O
mitsubishi	O
electric	O
res	O
.	O
labs	O
.	O
tr-2000-27	O
.	O
yedidia	O
,	O
j.	O
s.	O
,	O
freeman	O
,	O
w.	O
t.	O
,	O
and	O
weiss	O
,	O
y	O
.	O
(	O
2000	O
)	O
gener-	O
alized	O
belief	B
propagation	I
.	O
technical	O
report	O
,	O
mitsubishi	O
electric	O
res	O
.	O
labs	O
.	O
tr-2000-26	O
.	O
yedidia	O
,	O
j.	O
s.	O
,	O
freeman	O
,	O
w.	O
t.	O
,	O
and	O
weiss	O
,	O
y	O
.	O
(	O
2001a	O
)	O
bethe	O
free	B
energy	I
,	O
kikuchi	O
approximations	O
and	O
belief	O
propagation	O
al-	O
gorithms	O
.	O
technical	O
report	O
,	O
mitsubishi	O
electric	O
res	O
.	O
labs	O
.	O
tr-	O
2001-16.	O
yedidia	O
,	O
j.	O
s.	O
,	O
freeman	O
,	O
w.	O
t.	O
,	O
and	O
weiss	O
,	O
y	O
.	O
(	O
2001b	O
)	O
char-	O
acterization	O
of	O
belief	O
propagation	O
and	O
its	O
generalizations	O
.	O
tech-	O
nical	O
report	O
,	O
mitsubishi	O
electric	O
res	O
.	O
labs	O
.	O
tr-2001-15	O
.	O
yedidia	O
,	O
j.	O
s.	O
,	O
freeman	O
,	O
w.	O
t.	O
,	O
and	O
weiss	O
,	O
y	O
.	O
(	O
2002	O
)	O
con-	O
structing	O
free	B
energy	I
approximations	O
and	O
generalized	O
belief	B
propagation	I
algorithms	O
.	O
technical	O
report	O
,	O
mitsubishi	O
electric	O
res	O
.	O
labs	O
.	O
tr-2002-35	O
.	O
yeung	O
,	O
r.	O
w.	O
(	O
1991	O
)	O
a	O
new	O
outlook	O
on	O
shannon-information	O
measures	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
37	O
(	O
3.1	O
)	O
:	O
466	O
{	O
474.	O
yuille	O
,	O
a.	O
l.	O
(	O
2001	O
)	O
a	O
double-loop	O
algorithm	B
to	O
minimize	O
the	O
bethe	O
and	O
kikuchi	O
free	O
energies	O
.	O
in	O
energy	O
minimization	B
methods	O
in	O
computer	O
vision	B
and	O
pattern	B
recognition	I
,	O
ed	O
.	O
by	O
m.	O
figueiredo	O
,	O
j.	O
zerubia	O
,	O
and	O
a.	O
jain	O
,	O
number	O
2134	O
in	O
lncs	O
,	O
pp	O
.	O
3	O
{	O
18.	O
springer	O
.	O
zipf	O
,	O
g.	O
k.	O
(	O
1949	O
)	O
human	B
behavior	O
and	O
the	O
principle	O
of	O
least	O
e	O
(	O
cid:11	O
)	O
ort	O
.	O
addison-wesley	O
.	O
ziv	O
,	O
j.	O
,	O
and	O
lempel	O
,	O
a	O
.	O
(	O
1977	O
)	O
a	O
universal	B
algorithm	O
for	O
sequen-	O
tial	O
data	B
compression	I
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
23	O
(	O
3	O
)	O
:	O
337	O
{	O
343.	O
ziv	O
,	O
j.	O
,	O
and	O
lempel	O
,	O
a	O
.	O
(	O
1978	O
)	O
compression	B
of	O
individual	O
se-	O
quences	O
via	O
variable-rate	O
coding	O
.	O
ieee	O
trans	O
.	O
info	O
.	O
theory	B
24	O
(	O
5	O
)	O
:	O
530	O
{	O
536.	O
copyright	O
cambridge	O
university	O
press	O
2003.	O
on-screen	O
viewing	O
permitted	O
.	O
printing	O
not	O
permitted	O
.	O
http	O
:	O
//www.cambridge.org/0521642981	O
you	O
can	O
buy	O
this	O
book	O
for	O
30	O
pounds	O
or	O
$	O
50	O
.	O
see	O
http	O
:	O
//www.inference.phy.cam.ac.uk/mackay/itila/	O
for	O
links	O
.	O