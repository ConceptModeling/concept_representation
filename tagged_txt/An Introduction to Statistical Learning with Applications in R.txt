springer	O
texts	O
in	O
statistics	O
©	O
springer	O
science+business	O
media	O
new	O
york	O
2013	O
(	O
corrected	O
at	O
this	O
work	O
is	O
subject	O
to	O
copyright	O
.	O
all	O
rights	O
are	O
reserved	O
by	O
the	O
publisher	O
,	O
whether	O
the	O
whole	O
or	O
part	O
of	O
the	O
material	O
is	O
concerned	O
,	O
speciﬁcally	O
the	O
rights	O
of	O
translation	O
,	O
reprinting	O
,	O
reuse	O
of	O
illustrations	O
,	O
recitation	O
,	O
broadcasting	O
,	O
reproduction	O
on	O
microﬁlms	O
or	O
in	O
any	O
other	O
physical	O
way	O
,	O
and	O
transmission	O
or	O
information	O
storage	O
and	O
retrieval	O
,	O
electronic	O
adaptation	O
,	O
computer	O
software	O
,	O
or	O
by	O
similar	O
or	O
dissim-	O
ilar	O
methodology	O
now	O
known	O
or	O
hereafter	O
developed	O
.	O
exempted	O
from	O
this	O
legal	O
reservation	O
are	O
brief	O
excerpts	O
in	O
connection	O
with	O
reviews	O
or	O
scholarly	O
analysis	B
or	O
material	O
supplied	O
speciﬁcally	O
for	O
the	O
pur-	O
pose	O
of	O
being	O
entered	O
and	O
executed	O
on	O
a	O
computer	O
system	O
,	O
for	O
exclusive	O
use	O
by	O
the	O
purchaser	O
of	O
the	O
work	O
.	O
duplication	O
of	O
this	O
publication	O
or	O
parts	O
thereof	O
is	O
permitted	O
only	O
under	O
the	O
provisions	O
of	O
the	O
copyright	O
law	O
of	O
the	O
publisher	O
’	O
s	O
location	O
,	O
in	O
its	O
current	O
version	O
,	O
and	O
permission	O
for	O
use	O
must	O
always	O
be	O
obtained	O
from	O
springer	O
.	O
permissions	O
for	O
use	O
may	O
be	O
obtained	O
through	O
rightslink	O
at	O
the	O
copyright	O
clearance	O
center	O
.	O
violations	O
are	O
liable	O
to	O
prosecution	O
under	O
the	O
respective	O
copyright	O
law	O
.	O
the	O
use	O
of	O
general	O
descriptive	O
names	O
,	O
registered	O
names	O
,	O
trademarks	O
,	O
service	O
marks	O
,	O
etc	O
.	O
in	O
this	O
publi-	O
cation	O
does	O
not	O
imply	O
,	O
even	O
in	O
the	O
absence	O
of	O
a	O
speciﬁc	O
statement	O
,	O
that	O
such	O
names	O
are	O
exempt	O
from	O
the	O
relevant	O
protective	O
laws	O
and	O
regulations	O
and	O
therefore	O
free	O
for	O
general	O
use	O
.	O
while	O
the	O
advice	O
and	O
information	O
in	O
this	O
book	O
are	O
believed	O
to	O
be	O
true	O
and	O
accurate	O
at	O
the	O
date	O
of	O
publication	O
,	O
neither	O
the	O
authors	O
nor	O
the	O
editors	O
nor	O
the	O
publisher	O
can	O
accept	O
any	O
legal	O
responsibility	O
for	O
any	O
errors	O
or	O
omissions	O
that	O
may	O
be	O
made	O
.	O
the	O
publisher	O
makes	O
no	O
warranty	O
,	O
express	O
or	O
implied	O
,	O
with	O
respect	O
to	O
the	O
material	O
contained	O
herein	O
.	O
printed	O
on	O
acid-free	O
paper	O
springer	O
is	O
part	O
of	O
springer	O
science+business	O
media	O
(	O
www.springer.com	O
)	O
to	O
our	O
parents	O
:	O
alison	O
and	O
michael	O
james	O
chiara	O
nappi	O
and	O
edward	O
witten	O
valerie	O
and	O
patrick	O
hastie	O
vera	O
and	O
sami	O
tibshirani	O
and	O
to	O
our	O
families	O
:	O
michael	O
,	O
daniel	O
,	O
and	O
catherine	O
tessa	O
,	O
theo	O
,	O
and	O
ari	O
samantha	O
,	O
timothy	O
,	O
and	O
lynda	O
charlie	O
,	O
ryan	O
,	O
julie	O
,	O
and	O
cheryl	O
preface	O
statistical	O
learning	O
refers	O
to	O
a	O
set	B
of	O
tools	O
for	O
modeling	O
and	O
understanding	O
complex	O
datasets	O
.	O
it	O
is	O
a	O
recently	O
developed	O
area	O
in	O
statistics	O
and	O
blends	O
with	O
parallel	O
developments	O
in	O
computer	O
science	O
and	O
,	O
in	O
particular	O
,	O
machine	B
learning	O
.	O
the	O
ﬁeld	O
encompasses	O
many	O
methods	O
such	O
as	O
the	O
lasso	B
and	O
sparse	B
regression	O
,	O
classiﬁcation	B
and	O
regression	B
trees	O
,	O
and	O
boosting	B
and	O
support	B
vector	I
machines	O
.	O
with	O
the	O
explosion	O
of	O
“	O
big	O
data	B
”	O
problems	O
,	O
statistical	O
learning	O
has	O
be-	O
come	O
a	O
very	O
hot	O
ﬁeld	O
in	O
many	O
scientiﬁc	O
areas	O
as	O
well	O
as	O
marketing	O
,	O
ﬁnance	O
,	O
and	O
other	O
business	O
disciplines	O
.	O
people	O
with	O
statistical	O
learning	O
skills	O
are	O
in	O
high	O
demand	O
.	O
one	O
of	O
the	O
ﬁrst	O
books	O
in	O
this	O
area—the	O
elements	O
of	O
statistical	O
learning	O
(	O
esl	O
)	O
(	O
hastie	O
,	O
tibshirani	O
,	O
and	O
friedman	O
)	O
—was	O
published	O
in	O
2001	O
,	O
with	O
a	O
second	O
edition	O
in	O
2009.	O
esl	O
has	O
become	O
a	O
popular	O
text	O
not	O
only	O
in	O
statis-	O
tics	O
but	O
also	O
in	O
related	O
ﬁelds	O
.	O
one	O
of	O
the	O
reasons	O
for	O
esl	O
’	O
s	O
popularity	O
is	O
its	O
relatively	O
accessible	O
style	O
.	O
but	O
esl	O
is	O
intended	O
for	O
individuals	O
with	O
ad-	O
vanced	O
training	B
in	O
the	O
mathematical	O
sciences	O
.	O
an	O
introduction	O
to	O
statistical	O
learning	O
(	O
isl	O
)	O
arose	O
from	O
the	O
perceived	O
need	O
for	O
a	O
broader	O
and	O
less	O
tech-	O
nical	O
treatment	O
of	O
these	O
topics	O
.	O
in	O
this	O
new	O
book	O
,	O
we	O
cover	O
many	O
of	O
the	O
same	O
topics	O
as	O
esl	O
,	O
but	O
we	O
concentrate	O
more	O
on	O
the	O
applications	O
of	O
the	O
methods	O
and	O
less	O
on	O
the	O
mathematical	O
details	O
.	O
we	O
have	O
created	O
labs	O
illus-	O
trating	O
how	O
to	O
implement	O
each	O
of	O
the	O
statistical	O
learning	O
methods	O
using	O
the	O
popular	O
statistical	O
software	O
package	O
r.	O
these	O
labs	O
provide	O
the	O
reader	O
with	O
valuable	O
hands-on	O
experience	O
.	O
this	O
book	O
is	O
appropriate	O
for	O
advanced	O
undergraduates	O
or	O
master	O
’	O
s	O
stu-	O
dents	O
in	O
statistics	O
or	O
related	O
quantitative	B
ﬁelds	O
or	O
for	O
individuals	O
in	O
other	O
vii	O
viii	O
preface	O
disciplines	O
who	O
wish	O
to	O
use	O
statistical	O
learning	O
tools	O
to	O
analyze	O
their	O
data	B
.	O
it	O
can	O
be	O
used	O
as	O
a	O
textbook	O
for	O
a	O
course	O
spanning	O
one	O
or	O
two	O
semesters	O
.	O
we	O
would	O
like	O
to	O
thank	O
several	O
readers	O
for	O
valuable	O
comments	O
on	O
prelim-	O
inary	O
drafts	O
of	O
this	O
book	O
:	O
pallavi	O
basu	O
,	O
alexandra	O
chouldechova	O
,	O
patrick	O
danaher	O
,	O
will	O
fithian	O
,	O
luella	O
fu	O
,	O
sam	O
gross	O
,	O
max	O
grazier	O
g	O
’	O
sell	O
,	O
court-	O
ney	O
paulson	O
,	O
xinghao	O
qiao	O
,	O
elisa	O
sheng	O
,	O
noah	O
simon	O
,	O
kean	O
ming	O
tan	O
,	O
and	O
xin	O
lu	O
tan	O
.	O
it	O
’	O
s	O
tough	O
to	O
make	O
predictions	O
,	O
especially	O
about	O
the	O
future	O
.	O
los	O
angeles	O
,	O
usa	O
seattle	O
,	O
usa	O
palo	O
alto	O
,	O
usa	O
palo	O
alto	O
,	O
usa	O
-yogi	O
berra	O
gareth	O
james	O
daniela	O
witten	O
trevor	O
hastie	O
robert	O
tibshirani	O
contents	O
preface	O
1	O
introduction	O
2	O
statistical	O
learning	O
2.1	O
what	O
is	O
statistical	O
learning	O
?	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.1.1	O
why	O
estimate	O
f	O
?	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.1.2	O
how	O
do	O
we	O
estimate	O
f	O
?	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.1.3	O
the	O
trade-oﬀ	B
between	O
prediction	B
accuracy	O
and	O
model	B
interpretability	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.1.4	O
supervised	O
versus	O
unsupervised	B
learning	I
.	O
.	O
.	O
.	O
.	O
.	O
2.1.5	O
regression	B
versus	O
classiﬁcation	B
problems	O
.	O
.	O
.	O
.	O
.	O
.	O
2.2	O
assessing	O
model	B
accuracy	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.2.1	O
measuring	O
the	O
quality	O
of	O
fit	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.2.2	O
the	O
bias-variance	B
trade-oﬀ	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.2.3	O
the	O
classiﬁcation	B
setting	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.3	O
lab	O
:	O
introduction	O
to	O
r	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.3.1	O
basic	O
commands	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.3.2	O
graphics	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
indexing	O
data	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.3.3	O
2.3.4	O
loading	O
data	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.3.5	O
additional	O
graphical	O
and	O
numerical	O
summaries	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.4	O
exercises	O
vii	O
1	O
15	O
15	O
17	O
21	O
24	O
26	O
28	O
29	O
29	O
33	O
37	O
42	O
42	O
45	O
47	O
48	O
49	O
52	O
ix	O
x	O
contents	O
3	O
linear	B
regression	I
3.1	O
simple	B
linear	O
regression	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3.1.1	O
estimating	O
the	O
coeﬃcients	O
3.1.2	O
assessing	O
the	O
accuracy	O
of	O
the	O
coeﬃcient	B
59	O
61	O
61	O
some	O
important	O
questions	O
estimates	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3.1.3	O
assessing	O
the	O
accuracy	O
of	O
the	O
model	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3.2	O
multiple	B
linear	O
regression	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3.2.1	O
estimating	O
the	O
regression	B
coeﬃcients	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3.2.2	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3.3	O
other	O
considerations	O
in	O
the	O
regression	B
model	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3.3.1	O
qualitative	B
predictors	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3.3.2	O
extensions	O
of	O
the	O
linear	B
model	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3.3.3	O
potential	O
problems	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
63	O
68	O
71	O
72	O
75	O
82	O
82	O
86	O
92	O
3.4	O
the	O
marketing	O
plan	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
102	O
3.5	O
comparison	O
of	O
linear	B
regression	I
with	O
k-nearest	O
neighbors	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
104	O
3.6	O
lab	O
:	O
linear	B
regression	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
109	O
3.6.1	O
libraries	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
109	O
3.6.2	O
simple	B
linear	O
regression	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
110	O
3.6.3	O
multiple	B
linear	O
regression	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
113	O
3.6.4	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
115	O
3.6.5	O
non-linear	B
transformations	O
of	O
the	O
predictors	O
.	O
.	O
.	O
.	O
115	O
3.6.6	O
qualitative	B
predictors	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
117	O
3.6.7	O
writing	O
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
119	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
120	O
interaction	B
terms	O
3.7	O
exercises	O
4	O
classiﬁcation	B
127	O
4.1	O
an	O
overview	O
of	O
classiﬁcation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
128	O
4.2	O
why	O
not	O
linear	B
regression	I
?	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
129	O
4.3	O
logistic	B
regression	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
130	O
4.3.1	O
the	O
logistic	B
model	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
131	O
4.3.2	O
estimating	O
the	O
regression	B
coeﬃcients	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
133	O
4.3.3	O
making	O
predictions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
134	O
4.3.4	O
multiple	B
logistic	O
regression	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
135	O
4.3.5	O
logistic	B
regression	I
for	O
>	O
2	O
response	B
classes	O
.	O
.	O
.	O
.	O
.	O
137	O
4.4	O
linear	B
discriminant	I
analysis	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
138	O
4.4.1	O
using	O
bayes	O
’	O
theorem	O
for	O
classiﬁcation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
138	O
4.4.2	O
linear	B
discriminant	I
analysis	I
for	O
p	O
=	O
1	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
139	O
4.4.3	O
linear	B
discriminant	I
analysis	I
for	O
p	O
>	O
1	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
142	O
4.4.4	O
quadratic	B
discriminant	I
analysis	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
149	O
4.5	O
a	O
comparison	O
of	O
classiﬁcation	B
methods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
151	O
4.6	O
lab	O
:	O
logistic	B
regression	I
,	O
lda	O
,	O
qda	O
,	O
and	O
knn	O
.	O
.	O
.	O
.	O
.	O
.	O
154	O
4.6.1	O
the	O
stock	O
market	O
data	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
154	O
4.6.2	O
logistic	B
regression	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
156	O
4.6.3	O
linear	B
discriminant	I
analysis	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
161	O
contents	O
xi	O
4.6.4	O
quadratic	B
discriminant	I
analysis	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
163	O
4.6.5	O
k-nearest	O
neighbors	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
163	O
4.6.6	O
an	O
application	O
to	O
caravan	O
insurance	O
data	B
.	O
.	O
.	O
.	O
.	O
165	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
168	O
4.7	O
exercises	O
5	O
resampling	B
methods	O
175	O
5.1	O
cross-validation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
176	O
5.1.1	O
the	O
validation	B
set	I
approach	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
176	O
5.1.2	O
leave-one-out	B
cross-validation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
178	O
k-fold	B
cross-validation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
181	O
5.1.3	O
5.1.4	O
bias-variance	B
trade-oﬀ	O
for	O
k-fold	O
cross-validation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
183	O
5.1.5	O
cross-validation	B
on	O
classiﬁcation	B
problems	O
.	O
.	O
.	O
.	O
.	O
184	O
5.2	O
the	O
bootstrap	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
187	O
5.3	O
lab	O
:	O
cross-validation	B
and	O
the	O
bootstrap	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
190	O
5.3.1	O
the	O
validation	B
set	I
approach	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
191	O
5.3.2	O
leave-one-out	B
cross-validation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
192	O
k-fold	B
cross-validation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
193	O
5.3.3	O
5.3.4	O
the	O
bootstrap	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
194	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
197	O
5.4	O
exercises	O
6	O
linear	B
model	I
selection	O
and	O
regularization	B
6.3	O
dimension	B
reduction	I
methods	O
203	O
6.1	O
subset	B
selection	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
205	O
6.1.1	O
best	B
subset	I
selection	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
205	O
stepwise	O
selection	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
207	O
6.1.2	O
6.1.3	O
choosing	O
the	O
optimal	O
model	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
210	O
6.2	O
shrinkage	B
methods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
214	O
6.2.1	O
ridge	B
regression	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
215	O
6.2.2	O
the	O
lasso	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
219	O
selecting	O
the	O
tuning	B
parameter	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
227	O
6.2.3	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
228	O
6.3.1	O
principal	B
components	I
regression	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
230	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
237	O
6.3.2	O
partial	B
least	I
squares	I
6.4	O
considerations	O
in	O
high	O
dimensions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
238	O
6.4.1	O
high-dimensional	B
data	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
238	O
6.4.2	O
what	O
goes	O
wrong	O
in	O
high	O
dimensions	O
?	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
239	O
6.4.3	O
regression	B
in	O
high	O
dimensions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
241	O
6.4.4	O
interpreting	O
results	O
in	O
high	O
dimensions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
243	O
6.5	O
lab	O
1	O
:	O
subset	B
selection	I
methods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
244	O
6.5.1	O
best	B
subset	I
selection	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
244	O
6.5.2	O
forward	O
and	O
backward	B
stepwise	I
selection	I
.	O
.	O
.	O
.	O
.	O
.	O
247	O
6.5.3	O
choosing	O
among	O
models	O
using	O
the	O
validation	B
set	I
approach	O
and	O
cross-validation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
248	O
xii	O
contents	O
6.6	O
lab	O
2	O
:	O
ridge	B
regression	I
and	O
the	O
lasso	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
251	O
6.6.1	O
ridge	B
regression	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
251	O
6.6.2	O
the	O
lasso	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
255	O
6.7	O
lab	O
3	O
:	O
pcr	O
and	O
pls	O
regression	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
256	O
6.7.1	O
principal	B
components	I
regression	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
256	O
6.7.2	O
partial	B
least	I
squares	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
258	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
259	O
6.8	O
exercises	O
7	O
moving	O
beyond	O
linearity	O
265	O
7.1	O
polynomial	B
regression	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
266	O
7.2	O
step	O
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
268	O
7.3	O
basis	B
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
270	O
7.4	O
regression	B
splines	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
271	O
7.4.1	O
piecewise	O
polynomials	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
271	O
7.4.2	O
constraints	O
and	O
splines	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
271	O
7.4.3	O
the	O
spline	B
basis	O
representation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
273	O
7.4.4	O
choosing	O
the	O
number	O
and	O
locations	O
of	O
the	O
knots	O
7.5	O
smoothing	B
splines	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
274	O
7.4.5	O
comparison	O
to	O
polynomial	B
regression	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
276	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
277	O
7.5.1	O
an	O
overview	O
of	O
smoothing	B
splines	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
277	O
7.5.2	O
choosing	O
the	O
smoothing	B
parameter	O
λ	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
278	O
7.6	O
local	B
regression	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
280	O
7.7	O
generalized	O
additive	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
282	O
7.7.1	O
gams	O
for	O
regression	O
problems	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
283	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
286	O
7.7.2	O
gams	O
for	O
classiﬁcation	O
problems	O
7.8	O
lab	O
:	O
non-linear	B
modeling	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
287	O
.	O
.	O
.	O
.	O
.	O
288	O
7.8.1	O
polynomial	B
regression	O
and	O
step	O
functions	O
splines	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
293	O
7.8.2	O
7.8.3	O
gams	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
294	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
297	O
7.9	O
exercises	O
8	O
tree-based	O
methods	O
8.1	O
the	O
basics	O
of	O
decision	O
trees	O
303	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
303	O
8.1.1	O
regression	B
trees	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
304	O
8.1.2	O
classiﬁcation	B
trees	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
311	O
8.1.3	O
trees	O
versus	O
linear	B
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
314	O
8.1.4	O
advantages	O
and	O
disadvantages	O
of	O
trees	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
315	O
8.2	O
bagging	B
,	O
random	O
forests	O
,	O
boosting	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
316	O
8.2.1	O
bagging	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
316	O
8.2.2	O
random	O
forests	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
319	O
8.2.3	O
boosting	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
321	O
8.3	O
lab	O
:	O
decision	O
trees	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
323	O
8.3.1	O
fitting	O
classiﬁcation	B
trees	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
323	O
8.3.2	O
fitting	O
regression	B
trees	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
327	O
8.3.3	O
bagging	B
and	O
random	O
forests	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
328	O
8.3.4	O
boosting	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
330	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
332	O
8.4	O
exercises	O
contents	O
xiii	O
9	O
support	B
vector	I
machines	O
337	O
9.1	O
maximal	B
margin	I
classiﬁer	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
338	O
9.1.1	O
what	O
is	O
a	O
hyperplane	B
?	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
338	O
9.1.2	O
classiﬁcation	B
using	O
a	O
separating	B
hyperplane	I
.	O
.	O
.	O
.	O
339	O
9.1.3	O
the	O
maximal	B
margin	I
classiﬁer	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
341	O
9.1.4	O
construction	O
of	O
the	O
maximal	B
margin	I
classiﬁer	O
.	O
.	O
.	O
342	O
9.1.5	O
the	O
non-separable	O
case	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
343	O
9.2	O
support	B
vector	I
classiﬁers	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
344	O
9.2.1	O
overview	O
of	O
the	O
support	B
vector	I
classiﬁer	O
.	O
.	O
.	O
.	O
.	O
.	O
344	O
9.2.2	O
details	O
of	O
the	O
support	B
vector	I
classiﬁer	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
345	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
349	O
9.3	O
support	B
vector	I
machines	O
9.3.1	O
classiﬁcation	B
with	O
non-linear	B
decision	O
boundaries	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
349	O
9.3.2	O
the	O
support	B
vector	I
machine	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
350	O
9.3.3	O
an	O
application	O
to	O
the	O
heart	O
disease	O
data	B
.	O
.	O
.	O
.	O
.	O
.	O
354	O
9.4	O
svms	O
with	O
more	O
than	O
two	O
classes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
355	O
9.4.1	O
one-versus-one	B
classiﬁcation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
355	O
9.4.2	O
one-versus-all	B
classiﬁcation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
356	O
9.5	O
relationship	O
to	O
logistic	B
regression	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
356	O
9.6	O
lab	O
:	O
support	B
vector	I
machines	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
359	O
support	B
vector	I
classiﬁer	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
359	O
9.6.1	O
support	B
vector	I
machine	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
363	O
9.6.2	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
365	O
9.6.3	O
roc	O
curves	O
svm	O
with	O
multiple	B
classes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
366	O
9.6.4	O
9.6.5	O
application	O
to	O
gene	O
expression	O
data	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
366	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
368	O
9.7	O
exercises	O
10	O
unsupervised	B
learning	I
373	O
10.1	O
the	O
challenge	O
of	O
unsupervised	B
learning	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
373	O
10.2	O
principal	B
components	I
analysis	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
374	O
10.2.1	O
what	O
are	O
principal	B
components	I
?	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
375	O
10.2.2	O
another	O
interpretation	O
of	O
principal	B
components	I
.	O
.	O
379	O
10.2.3	O
more	O
on	O
pca	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
380	O
10.2.4	O
other	O
uses	O
for	O
principal	O
components	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
385	O
10.3	O
clustering	B
methods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
385	O
10.3.1	O
k-means	O
clustering	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
386	O
10.3.2	O
hierarchical	B
clustering	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
390	O
10.3.3	O
practical	O
issues	O
in	O
clustering	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
399	O
10.4	O
lab	O
1	O
:	O
principal	B
components	I
analysis	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
401	O
xiv	O
contents	O
10.5	O
lab	O
2	O
:	O
clustering	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
404	O
10.5.1	O
k-means	O
clustering	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
404	O
10.5.2	O
hierarchical	B
clustering	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
406	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
407	O
10.6.1	O
pca	O
on	O
the	O
nci60	O
data	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
408	O
10.6.2	O
clustering	B
the	O
observations	B
of	O
the	O
nci60	O
data	B
.	O
.	O
.	O
410	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
413	O
10.6	O
lab	O
3	O
:	O
nci60	O
data	B
example	O
10.7	O
exercises	O
index	O
419	O
1	O
introduction	O
an	O
overview	O
of	O
statistical	O
learning	O
statistical	O
learning	O
refers	O
to	O
a	O
vast	O
set	B
of	O
tools	O
for	O
understanding	O
data	B
.	O
these	O
tools	O
can	O
be	O
classiﬁed	O
as	O
supervised	O
or	O
unsupervised	O
.	O
broadly	O
speaking	O
,	O
supervised	O
statistical	O
learning	O
involves	O
building	O
a	O
statistical	B
model	I
for	O
pre-	O
dicting	O
,	O
or	O
estimating	O
,	O
an	O
output	B
based	O
on	O
one	O
or	O
more	O
inputs	O
.	O
problems	O
of	O
this	O
nature	O
occur	O
in	O
ﬁelds	O
as	O
diverse	O
as	O
business	O
,	O
medicine	O
,	O
astrophysics	O
,	O
and	O
public	O
policy	O
.	O
with	O
unsupervised	O
statistical	O
learning	O
,	O
there	O
are	O
inputs	O
but	O
no	O
supervising	O
output	B
;	O
nevertheless	O
we	O
can	O
learn	O
relationships	O
and	O
struc-	O
ture	O
from	O
such	O
data	B
.	O
to	O
provide	O
an	O
illustration	O
of	O
some	O
applications	O
of	O
statistical	O
learning	O
,	O
we	O
brieﬂy	O
discuss	O
three	O
real-world	O
data	B
sets	O
that	O
are	O
considered	O
in	O
this	O
book	O
.	O
wage	O
data	B
in	O
this	O
application	O
(	O
which	O
we	O
refer	O
to	O
as	O
the	O
wage	O
data	B
set	O
throughout	O
this	O
book	O
)	O
,	O
we	O
examine	O
a	O
number	O
of	O
factors	O
that	O
relate	O
to	O
wages	O
for	O
a	O
group	O
of	O
males	O
from	O
the	O
atlantic	O
region	O
of	O
the	O
united	O
states	O
.	O
in	O
particular	O
,	O
we	O
wish	O
to	O
understand	O
the	O
association	O
between	O
an	O
employee	O
’	O
s	O
age	O
and	O
education	O
,	O
as	O
well	O
as	O
the	O
calendar	O
year	O
,	O
on	O
his	O
wage	O
.	O
consider	O
,	O
for	O
example	O
,	O
the	O
left-hand	O
panel	O
of	O
figure	O
1.1	O
,	O
which	O
displays	O
wage	O
versus	O
age	O
for	O
each	O
of	O
the	O
individu-	O
als	O
in	O
the	O
data	B
set	O
.	O
there	O
is	O
evidence	O
that	O
wage	O
increases	O
with	O
age	O
but	O
then	O
decreases	O
again	O
after	O
approximately	O
age	O
60.	O
the	O
blue	O
line	B
,	O
which	O
provides	O
an	O
estimate	O
of	O
the	O
average	B
wage	O
for	O
a	O
given	O
age	O
,	O
makes	O
this	O
trend	O
clearer	O
.	O
g.	O
james	O
et	O
al.	O
,	O
an	O
introduction	O
to	O
statistical	O
learning	O
:	O
with	O
applications	O
in	O
r	O
,	O
springer	O
texts	O
in	O
statistics	O
,	O
doi	O
10.1007/978-1-4614-7138-7	O
1	O
,	O
©	O
springer	O
science+business	O
media	O
new	O
york	O
2013	O
1	O
2	O
1.	O
introduction	O
e	O
g	O
a	O
w	O
0	O
0	O
3	O
0	O
0	O
2	O
0	O
0	O
1	O
0	O
5	O
e	O
g	O
a	O
w	O
0	O
0	O
3	O
0	O
0	O
2	O
0	O
0	O
1	O
0	O
5	O
e	O
g	O
a	O
w	O
0	O
0	O
3	O
0	O
0	O
2	O
0	O
0	O
1	O
0	O
5	O
20	O
40	O
60	O
80	O
2003	O
2006	O
2009	O
age	O
year	O
1	O
2	O
3	O
4	O
5	O
education	O
level	B
figure	O
1.1.	O
wage	O
data	B
,	O
which	O
contains	O
income	O
survey	O
information	O
for	O
males	O
from	O
the	O
central	O
atlantic	O
region	O
of	O
the	O
united	O
states	O
.	O
left	O
:	O
wage	O
as	O
a	O
function	B
of	O
age	O
.	O
on	O
average	B
,	O
wage	O
increases	O
with	O
age	O
until	O
about	O
60	O
years	O
of	O
age	O
,	O
at	O
which	O
point	O
it	O
begins	O
to	O
decline	O
.	O
center	O
:	O
wage	O
as	O
a	O
function	B
of	O
year	O
.	O
there	O
is	O
a	O
slow	O
but	O
steady	O
increase	O
of	O
approximately	O
$	O
10,000	O
in	O
the	O
average	B
wage	O
between	O
2003	O
and	O
2009.	O
right	O
:	O
boxplots	O
displaying	O
wage	O
as	O
a	O
function	B
of	O
education	O
,	O
with	O
1	O
indicating	O
the	O
lowest	O
level	B
(	O
no	O
high	O
school	O
diploma	O
)	O
and	O
5	O
the	O
highest	O
level	B
(	O
an	O
advanced	O
graduate	O
degree	O
)	O
.	O
on	O
average	B
,	O
wage	O
increases	O
with	O
the	O
level	B
of	O
education	O
.	O
given	O
an	O
employee	O
’	O
s	O
age	O
,	O
we	O
can	O
use	O
this	O
curve	O
to	O
predict	O
his	O
wage	O
.	O
however	O
,	O
it	O
is	O
also	O
clear	O
from	O
figure	O
1.1	O
that	O
there	O
is	O
a	O
signiﬁcant	O
amount	O
of	O
vari-	O
ability	O
associated	O
with	O
this	O
average	B
value	O
,	O
and	O
so	O
age	O
alone	O
is	O
unlikely	O
to	O
provide	O
an	O
accurate	O
prediction	B
of	O
a	O
particular	O
man	O
’	O
s	O
wage	O
.	O
we	O
also	O
have	O
information	O
regarding	O
each	O
employee	O
’	O
s	O
education	O
level	B
and	O
the	O
year	O
in	O
which	O
the	O
wage	O
was	O
earned	O
.	O
the	O
center	O
and	O
right-hand	O
panels	O
of	O
figure	O
1.1	O
,	O
which	O
display	O
wage	O
as	O
a	O
function	B
of	O
both	O
year	O
and	O
education	O
,	O
in-	O
dicate	O
that	O
both	O
of	O
these	O
factors	O
are	O
associated	O
with	O
wage	O
.	O
wages	O
increase	O
by	O
approximately	O
$	O
10,000	O
,	O
in	O
a	O
roughly	O
linear	B
(	O
or	O
straight-line	O
)	O
fashion	O
,	O
between	O
2003	O
and	O
2009	O
,	O
though	O
this	O
rise	O
is	O
very	O
slight	O
relative	O
to	O
the	O
vari-	O
ability	O
in	O
the	O
data	B
.	O
wages	O
are	O
also	O
typically	O
greater	O
for	O
individuals	O
with	O
higher	O
education	O
levels	O
:	O
men	O
with	O
the	O
lowest	O
education	O
level	B
(	O
1	O
)	O
tend	O
to	O
have	O
substantially	O
lower	O
wages	O
than	O
those	O
with	O
the	O
highest	O
education	O
level	B
(	O
5	O
)	O
.	O
clearly	O
,	O
the	O
most	O
accurate	O
prediction	B
of	O
a	O
given	O
man	O
’	O
s	O
wage	O
will	O
be	O
obtained	O
by	O
combining	O
his	O
age	O
,	O
his	O
education	O
,	O
and	O
the	O
year	O
.	O
in	O
chapter	O
3	O
,	O
we	O
discuss	O
linear	B
regression	I
,	O
which	O
can	O
be	O
used	O
to	O
predict	O
wage	O
from	O
this	O
data	B
set	O
.	O
ideally	O
,	O
we	O
should	O
predict	O
wage	O
in	O
a	O
way	O
that	O
accounts	O
for	O
the	O
non-linear	B
relationship	O
between	O
wage	O
and	O
age	O
.	O
in	O
chapter	O
7	O
,	O
we	O
discuss	O
a	O
class	O
of	O
approaches	O
for	O
addressing	O
this	O
problem	O
.	O
stock	O
market	O
data	B
the	O
wage	O
data	B
involves	O
predicting	O
a	O
continuous	B
or	O
quantitative	B
output	O
value	O
.	O
this	O
is	O
often	O
referred	O
to	O
as	O
a	O
regression	B
problem	O
.	O
however	O
,	O
in	O
certain	O
cases	O
we	O
may	O
instead	O
wish	O
to	O
predict	O
a	O
non-numerical	O
value—that	O
is	O
,	O
a	O
categorical	B
yesterday	O
two	O
days	O
previous	O
three	O
days	O
previous	O
1.	O
introduction	O
3	O
6	O
4	O
2	O
0	O
2	O
−	O
4	O
−	O
p	O
&	O
s	O
n	O
i	O
e	O
g	O
n	O
a	O
h	O
c	O
e	O
g	O
a	O
n	O
e	O
c	O
r	O
e	O
p	O
t	O
6	O
4	O
2	O
0	O
2	O
−	O
4	O
−	O
p	O
&	O
s	O
n	O
i	O
e	O
g	O
n	O
a	O
h	O
c	O
e	O
g	O
a	O
t	O
n	O
e	O
c	O
r	O
e	O
p	O
6	O
4	O
2	O
0	O
2	O
−	O
4	O
−	O
p	O
&	O
s	O
n	O
i	O
e	O
g	O
n	O
a	O
h	O
c	O
e	O
g	O
a	O
n	O
e	O
c	O
r	O
e	O
p	O
t	O
down	O
up	O
today	O
’	O
s	O
direction	O
down	O
up	O
today	O
’	O
s	O
direction	O
down	O
up	O
today	O
’	O
s	O
direction	O
figure	O
1.2.	O
left	O
:	O
boxplots	O
of	O
the	O
previous	O
day	O
’	O
s	O
percentage	O
change	O
in	O
the	O
s	O
&	O
p	O
index	O
for	O
the	O
days	O
for	O
which	O
the	O
market	O
increased	O
or	O
decreased	O
,	O
obtained	O
from	O
the	O
smarket	O
data	B
.	O
center	O
and	O
right	O
:	O
same	O
as	O
left	O
panel	O
,	O
but	O
the	O
percentage	O
changes	O
for	O
2	O
and	O
3	O
days	O
previous	O
are	O
shown	O
.	O
or	O
qualitative	B
output	O
.	O
for	O
example	O
,	O
in	O
chapter	O
4	O
we	O
examine	O
a	O
stock	O
mar-	O
ket	O
data	B
set	O
that	O
contains	O
the	O
daily	O
movements	O
in	O
the	O
standard	O
&	O
poor	O
’	O
s	O
500	O
(	O
s	O
&	O
p	O
)	O
stock	O
index	O
over	O
a	O
5-year	O
period	O
between	O
2001	O
and	O
2005.	O
we	O
refer	O
to	O
this	O
as	O
the	O
smarket	O
data	B
.	O
the	O
goal	O
is	O
to	O
predict	O
whether	O
the	O
index	O
will	O
increase	O
or	O
decrease	O
on	O
a	O
given	O
day	O
using	O
the	O
past	O
5	O
days	O
’	O
percentage	O
changes	O
in	O
the	O
index	O
.	O
here	O
the	O
statistical	O
learning	O
problem	O
does	O
not	O
in-	O
volve	O
predicting	O
a	O
numerical	O
value	O
.	O
instead	O
it	O
involves	O
predicting	O
whether	O
a	O
given	O
day	O
’	O
s	O
stock	O
market	O
performance	O
will	O
fall	O
into	O
the	O
up	O
bucket	O
or	O
the	O
down	O
bucket	O
.	O
this	O
is	O
known	O
as	O
a	O
classiﬁcation	B
problem	O
.	O
a	O
model	B
that	O
could	O
accurately	O
predict	O
the	O
direction	O
in	O
which	O
the	O
market	O
will	O
move	O
would	O
be	O
very	O
useful	O
!	O
the	O
left-hand	O
panel	O
of	O
figure	O
1.2	O
displays	O
two	O
boxplots	O
of	O
the	O
previous	O
day	O
’	O
s	O
percentage	O
changes	O
in	O
the	O
stock	O
index	O
:	O
one	O
for	O
the	O
648	O
days	O
for	O
which	O
the	O
market	O
increased	O
on	O
the	O
subsequent	O
day	O
,	O
and	O
one	O
for	O
the	O
602	O
days	O
for	O
which	O
the	O
market	O
decreased	O
.	O
the	O
two	O
plots	O
look	O
almost	O
identical	O
,	O
suggest-	O
ing	O
that	O
there	O
is	O
no	O
simple	B
strategy	O
for	O
using	O
yesterday	O
’	O
s	O
movement	O
in	O
the	O
s	O
&	O
p	O
to	O
predict	O
today	O
’	O
s	O
returns	O
.	O
the	O
remaining	O
panels	O
,	O
which	O
display	O
box-	O
plots	O
for	O
the	O
percentage	O
changes	O
2	O
and	O
3	O
days	O
previous	O
to	O
today	O
,	O
similarly	O
indicate	O
little	O
association	O
between	O
past	O
and	O
present	O
returns	O
.	O
of	O
course	O
,	O
this	O
lack	O
of	O
pattern	O
is	O
to	O
be	O
expected	O
:	O
in	O
the	O
presence	O
of	O
strong	O
correlations	O
be-	O
tween	O
successive	O
days	O
’	O
returns	O
,	O
one	O
could	O
adopt	O
a	O
simple	B
trading	O
strategy	O
to	O
generate	O
proﬁts	O
from	O
the	O
market	O
.	O
nevertheless	O
,	O
in	O
chapter	O
4	O
,	O
we	O
explore	O
these	O
data	B
using	O
several	O
diﬀerent	O
statistical	O
learning	O
methods	O
.	O
interestingly	O
,	O
there	O
are	O
hints	O
of	O
some	O
weak	O
trends	O
in	O
the	O
data	B
that	O
suggest	O
that	O
,	O
at	O
least	O
for	O
this	O
5-year	O
period	O
,	O
it	O
is	O
possible	O
to	O
correctly	O
predict	O
the	O
direction	O
of	O
movement	O
in	O
the	O
market	O
approximately	O
60	O
%	O
of	O
the	O
time	O
(	O
figure	O
1.3	O
)	O
.	O
4	O
1.	O
introduction	O
y	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
d	O
e	O
i	O
t	O
c	O
d	O
e	O
r	O
p	O
2	O
5	O
.	O
0	O
0	O
5	O
.	O
0	O
8	O
4	O
.	O
0	O
6	O
4	O
.	O
0	O
down	O
up	O
today	O
’	O
s	O
direction	O
figure	O
1.3.	O
we	O
ﬁt	B
a	O
quadratic	B
discriminant	I
analysis	I
model	O
to	O
the	O
subset	O
of	O
the	O
smarket	O
data	B
corresponding	O
to	O
the	O
2001–2004	O
time	O
period	O
,	O
and	O
predicted	O
the	O
probability	B
of	O
a	O
stock	O
market	O
decrease	O
using	O
the	O
2005	O
data	B
.	O
on	O
average	B
,	O
the	O
predicted	O
probability	B
of	O
decrease	O
is	O
higher	O
for	O
the	O
days	O
in	O
which	O
the	O
market	O
does	O
decrease	O
.	O
based	O
on	O
these	O
results	O
,	O
we	O
are	O
able	O
to	O
correctly	O
predict	O
the	O
direction	O
of	O
movement	O
in	O
the	O
market	O
60	O
%	O
of	O
the	O
time	O
.	O
gene	O
expression	O
data	B
the	O
previous	O
two	O
applications	O
illustrate	O
data	B
sets	O
with	O
both	O
input	B
and	O
output	B
variables	O
.	O
however	O
,	O
another	O
important	O
class	O
of	O
problems	O
involves	O
situations	O
in	O
which	O
we	O
only	O
observe	O
input	B
variables	O
,	O
with	O
no	O
corresponding	O
output	B
.	O
for	O
example	O
,	O
in	O
a	O
marketing	O
setting	O
,	O
we	O
might	O
have	O
demographic	O
information	O
for	O
a	O
number	O
of	O
current	O
or	O
potential	O
customers	O
.	O
we	O
may	O
wish	O
to	O
understand	O
which	O
types	O
of	O
customers	O
are	O
similar	O
to	O
each	O
other	O
by	O
grouping	O
individuals	O
according	O
to	O
their	O
observed	O
characteristics	O
.	O
this	O
is	O
known	O
as	O
a	O
clustering	B
problem	O
.	O
unlike	O
in	O
the	O
previous	O
examples	O
,	O
here	O
we	O
are	O
not	O
trying	O
to	O
predict	O
an	O
output	B
variable	I
.	O
we	O
devote	O
chapter	O
10	O
to	O
a	O
discussion	O
of	O
statistical	O
learning	O
methods	O
for	O
problems	O
in	O
which	O
no	O
natural	B
output	O
variable	B
is	O
available	O
.	O
we	O
consider	O
the	O
nci60	O
data	B
set	O
,	O
which	O
consists	O
of	O
6,830	O
gene	O
expression	O
measurements	O
for	O
each	O
of	O
64	O
cancer	O
cell	O
lines	O
.	O
instead	O
of	O
predicting	O
a	O
particular	O
output	B
variable	I
,	O
we	O
are	O
interested	O
in	O
determining	O
whether	O
there	O
are	O
groups	O
,	O
or	O
clusters	O
,	O
among	O
the	O
cell	O
lines	O
based	O
on	O
their	O
gene	O
expression	O
measurements	O
.	O
this	O
is	O
a	O
diﬃcult	O
question	O
to	O
address	O
,	O
in	O
part	O
because	O
there	O
are	O
thousands	O
of	O
gene	O
expression	O
measurements	O
per	O
cell	O
line	B
,	O
making	O
it	O
hard	O
to	O
visualize	O
the	O
data	B
.	O
the	O
left-hand	O
panel	O
of	O
figure	O
1.4	O
addresses	O
this	O
problem	O
by	O
represent-	O
ing	O
each	O
of	O
the	O
64	O
cell	O
lines	O
using	O
just	O
two	O
numbers	O
,	O
z1	O
and	O
z2	O
.	O
these	O
are	O
the	O
ﬁrst	O
two	O
principal	B
components	I
of	O
the	O
data	B
,	O
which	O
summarize	O
the	O
6	O
,	O
830	O
expression	O
measurements	O
for	O
each	O
cell	O
line	B
down	O
to	O
two	O
numbers	O
or	O
dimensions	O
.	O
while	O
it	O
is	O
likely	O
that	O
this	O
dimension	B
reduction	I
has	O
resulted	O
in	O
1.	O
introduction	O
5	O
2	O
z	O
0	O
2	O
0	O
0	O
2	O
−	O
0	O
4	O
−	O
0	O
6	O
−	O
2	O
z	O
0	O
2	O
0	O
0	O
2	O
−	O
0	O
4	O
−	O
0	O
6	O
−	O
−40	O
−20	O
0	O
20	O
40	O
60	O
−40	O
−20	O
0	O
20	O
40	O
60	O
z1	O
z1	O
figure	O
1.4.	O
left	O
:	O
representation	O
of	O
the	O
nci60	O
gene	O
expression	O
data	B
set	O
in	O
a	O
two-dimensional	O
space	O
,	O
z1	O
and	O
z2	O
.	O
each	O
point	O
corresponds	O
to	O
one	O
of	O
the	O
64	O
cell	O
lines	O
.	O
there	O
appear	O
to	O
be	O
four	O
groups	O
of	O
cell	O
lines	O
,	O
which	O
we	O
have	O
represented	O
using	O
diﬀerent	O
colors	O
.	O
right	O
:	O
same	O
as	O
left	O
panel	O
except	O
that	O
we	O
have	O
represented	O
each	O
of	O
the	O
14	O
diﬀerent	O
types	O
of	O
cancer	O
using	O
a	O
diﬀerent	O
colored	O
symbol	O
.	O
cell	O
lines	O
corresponding	O
to	O
the	O
same	O
cancer	O
type	O
tend	O
to	O
be	O
nearby	O
in	O
the	O
two-dimensional	O
space	O
.	O
some	O
loss	O
of	O
information	O
,	O
it	O
is	O
now	O
possible	O
to	O
visually	O
examine	O
the	O
data	B
for	O
evidence	O
of	O
clustering	B
.	O
deciding	O
on	O
the	O
number	O
of	O
clusters	O
is	O
often	O
a	O
diﬃ-	O
cult	O
problem	O
.	O
but	O
the	O
left-hand	O
panel	O
of	O
figure	O
1.4	O
suggests	O
at	O
least	O
four	O
groups	O
of	O
cell	O
lines	O
,	O
which	O
we	O
have	O
represented	O
using	O
separate	O
colors	O
.	O
we	O
can	O
now	O
examine	O
the	O
cell	O
lines	O
within	O
each	O
cluster	O
for	O
similarities	O
in	O
their	O
types	O
of	O
cancer	O
,	O
in	O
order	O
to	O
better	O
understand	O
the	O
relationship	O
between	O
gene	O
expression	O
levels	O
and	O
cancer	O
.	O
in	O
this	O
particular	O
data	B
set	O
,	O
it	O
turns	O
out	O
that	O
the	O
cell	O
lines	O
correspond	O
to	O
14	O
diﬀerent	O
types	O
of	O
cancer	O
.	O
(	O
however	O
,	O
this	O
information	O
was	O
not	O
used	O
to	O
create	O
the	O
left-hand	O
panel	O
of	O
figure	O
1.4	O
.	O
)	O
the	O
right-hand	O
panel	O
of	O
fig-	O
ure	O
1.4	O
is	O
identical	O
to	O
the	O
left-hand	O
panel	O
,	O
except	O
that	O
the	O
14	O
cancer	O
types	O
are	O
shown	O
using	O
distinct	O
colored	O
symbols	O
.	O
there	O
is	O
clear	O
evidence	O
that	O
cell	O
lines	O
with	O
the	O
same	O
cancer	O
type	O
tend	O
to	O
be	O
located	O
near	O
each	O
other	O
in	O
this	O
two-dimensional	O
representation	O
.	O
in	O
addition	O
,	O
even	O
though	O
the	O
cancer	O
infor-	O
mation	O
was	O
not	O
used	O
to	O
produce	O
the	O
left-hand	O
panel	O
,	O
the	O
clustering	B
obtained	O
does	O
bear	O
some	O
resemblance	O
to	O
some	O
of	O
the	O
actual	O
cancer	O
types	O
observed	O
in	O
the	O
right-hand	O
panel	O
.	O
this	O
provides	O
some	O
independent	B
veriﬁcation	O
of	O
the	O
accuracy	O
of	O
our	O
clustering	B
analysis	O
.	O
a	O
brief	O
history	O
of	O
statistical	O
learning	O
though	O
the	O
term	B
statistical	O
learning	O
is	O
fairly	O
new	O
,	O
many	O
of	O
the	O
concepts	O
that	O
underlie	O
the	O
ﬁeld	O
were	O
developed	O
long	O
ago	O
.	O
at	O
the	O
beginning	O
of	O
the	O
nineteenth	O
century	O
,	O
legendre	O
and	O
gauss	O
published	O
papers	O
on	O
the	O
method	O
6	O
1.	O
introduction	O
of	O
least	B
squares	I
,	O
which	O
implemented	O
the	O
earliest	O
form	O
of	O
what	O
is	O
now	O
known	O
as	O
linear	B
regression	I
.	O
the	O
approach	B
was	O
ﬁrst	O
successfully	O
applied	O
to	O
problems	O
in	O
astronomy	O
.	O
linear	B
regression	I
is	O
used	O
for	O
predicting	O
quantitative	B
values	O
,	O
such	O
as	O
an	O
individual	O
’	O
s	O
salary	O
.	O
in	O
order	O
to	O
predict	O
qualitative	B
values	O
,	O
such	O
as	O
whether	O
a	O
patient	O
survives	O
or	O
dies	O
,	O
or	O
whether	O
the	O
stock	O
market	O
increases	O
or	O
decreases	O
,	O
fisher	O
proposed	O
linear	B
discriminant	I
analysis	I
in	O
1936.	O
in	O
the	O
1940s	O
,	O
various	O
authors	O
put	O
forth	O
an	O
alternative	O
approach	O
,	O
logistic	B
regression	I
.	O
in	O
the	O
early	O
1970s	O
,	O
nelder	O
and	O
wedderburn	O
coined	O
the	O
term	B
generalized	O
linear	B
models	O
for	O
an	O
entire	O
class	O
of	O
statistical	O
learning	O
methods	O
that	O
include	O
both	O
linear	B
and	O
logistic	B
regression	I
as	O
special	O
cases	O
.	O
by	O
the	O
end	O
of	O
the	O
1970s	O
,	O
many	O
more	O
techniques	O
for	O
learning	O
from	O
data	B
were	O
available	O
.	O
however	O
,	O
they	O
were	O
almost	O
exclusively	O
linear	B
methods	O
,	O
be-	O
cause	O
ﬁtting	O
non-linear	B
relationships	O
was	O
computationally	O
infeasible	O
at	O
the	O
time	O
.	O
by	O
the	O
1980s	O
,	O
computing	O
technology	O
had	O
ﬁnally	O
improved	O
suﬃciently	O
that	O
non-linear	B
methods	O
were	O
no	O
longer	O
computationally	O
prohibitive	O
.	O
in	O
mid	O
1980s	O
breiman	O
,	O
friedman	O
,	O
olshen	O
and	O
stone	O
introduced	O
classiﬁcation	B
and	O
regression	B
trees	O
,	O
and	O
were	O
among	O
the	O
ﬁrst	O
to	O
demonstrate	O
the	O
power	B
of	O
a	O
detailed	O
practical	O
implementation	O
of	O
a	O
method	O
,	O
including	O
cross-validation	B
for	O
model	B
selection	I
.	O
hastie	O
and	O
tibshirani	O
coined	O
the	O
term	B
generalized	O
addi-	O
tive	O
models	O
in	O
1986	O
for	O
a	O
class	O
of	O
non-linear	B
extensions	O
to	O
generalized	O
linear	O
models	O
,	O
and	O
also	O
provided	O
a	O
practical	O
software	O
implementation	O
.	O
since	O
that	O
time	O
,	O
inspired	O
by	O
the	O
advent	O
of	O
machine	B
learning	O
and	O
other	O
disciplines	O
,	O
statistical	O
learning	O
has	O
emerged	O
as	O
a	O
new	O
subﬁeld	O
in	O
statistics	O
,	O
focused	O
on	O
supervised	O
and	O
unsupervised	O
modeling	O
and	O
prediction	B
.	O
in	O
recent	O
years	O
,	O
progress	O
in	O
statistical	O
learning	O
has	O
been	O
marked	O
by	O
the	O
increasing	O
availability	O
of	O
powerful	O
and	O
relatively	O
user-friendly	O
software	O
,	O
such	O
as	O
the	O
popular	O
and	O
freely	O
available	O
r	O
system	O
.	O
this	O
has	O
the	O
potential	O
to	O
continue	O
the	O
transformation	O
of	O
the	O
ﬁeld	O
from	O
a	O
set	B
of	O
techniques	O
used	O
and	O
developed	O
by	O
statisticians	O
and	O
computer	O
scientists	O
to	O
an	O
essential	O
toolkit	O
for	O
a	O
much	O
broader	O
community	O
.	O
this	O
book	O
the	O
elements	O
of	O
statistical	O
learning	O
(	O
esl	O
)	O
by	O
hastie	O
,	O
tibshirani	O
,	O
and	O
friedman	O
was	O
ﬁrst	O
published	O
in	O
2001.	O
since	O
that	O
time	O
,	O
it	O
has	O
become	O
an	O
important	O
reference	O
on	O
the	O
fundamentals	O
of	O
statistical	O
machine	O
learning	O
.	O
its	O
success	O
derives	O
from	O
its	O
comprehensive	O
and	O
detailed	O
treatment	O
of	O
many	O
important	O
topics	O
in	O
statistical	O
learning	O
,	O
as	O
well	O
as	O
the	O
fact	O
that	O
(	O
relative	O
to	O
many	O
upper-level	O
statistics	O
textbooks	O
)	O
it	O
is	O
accessible	O
to	O
a	O
wide	O
audience	O
.	O
however	O
,	O
the	O
greatest	O
factor	B
behind	O
the	O
success	O
of	O
esl	O
has	O
been	O
its	O
topical	O
nature	O
.	O
at	O
the	O
time	O
of	O
its	O
publication	O
,	O
interest	O
in	O
the	O
ﬁeld	O
of	O
statistical	O
1.	O
introduction	O
7	O
learning	O
was	O
starting	O
to	O
explode	O
.	O
esl	O
provided	O
one	O
of	O
the	O
ﬁrst	O
accessible	O
and	O
comprehensive	O
introductions	O
to	O
the	O
topic	O
.	O
since	O
esl	O
was	O
ﬁrst	O
published	O
,	O
the	O
ﬁeld	O
of	O
statistical	O
learning	O
has	O
con-	O
tinued	O
to	O
ﬂourish	O
.	O
the	O
ﬁeld	O
’	O
s	O
expansion	O
has	O
taken	O
two	O
forms	O
.	O
the	O
most	O
obvious	O
growth	O
has	O
involved	O
the	O
development	O
of	O
new	O
and	O
improved	O
statis-	O
tical	O
learning	O
approaches	O
aimed	O
at	O
answering	O
a	O
range	O
of	O
scientiﬁc	O
questions	O
across	O
a	O
number	O
of	O
ﬁelds	O
.	O
however	O
,	O
the	O
ﬁeld	O
of	O
statistical	O
learning	O
has	O
also	O
expanded	O
its	O
audience	O
.	O
in	O
the	O
1990s	O
,	O
increases	O
in	O
computational	O
power	B
generated	O
a	O
surge	O
of	O
interest	O
in	O
the	O
ﬁeld	O
from	O
non-statisticians	O
who	O
were	O
eager	O
to	O
use	O
cutting-edge	O
statistical	O
tools	O
to	O
analyze	O
their	O
data	B
.	O
unfortu-	O
nately	O
,	O
the	O
highly	O
technical	O
nature	O
of	O
these	O
approaches	O
meant	O
that	O
the	O
user	O
community	O
remained	O
primarily	O
restricted	O
to	O
experts	O
in	O
statistics	O
,	O
computer	O
science	O
,	O
and	O
related	O
ﬁelds	O
with	O
the	O
training	B
(	O
and	O
time	O
)	O
to	O
understand	O
and	O
implement	O
them	O
.	O
in	O
recent	O
years	O
,	O
new	O
and	O
improved	O
software	O
packages	O
have	O
signiﬁcantly	O
eased	O
the	O
implementation	O
burden	O
for	O
many	O
statistical	O
learning	O
methods	O
.	O
at	O
the	O
same	O
time	O
,	O
there	O
has	O
been	O
growing	O
recognition	O
across	O
a	O
number	O
of	O
ﬁelds	O
,	O
from	O
business	O
to	O
health	O
care	O
to	O
genetics	O
to	O
the	O
social	O
sciences	O
and	O
beyond	O
,	O
that	O
statistical	O
learning	O
is	O
a	O
powerful	O
tool	O
with	O
important	O
practical	O
applications	O
.	O
as	O
a	O
result	O
,	O
the	O
ﬁeld	O
has	O
moved	O
from	O
one	O
of	O
primarily	O
academic	O
interest	O
to	O
a	O
mainstream	O
discipline	O
,	O
with	O
an	O
enormous	O
potential	O
audience	O
.	O
this	O
trend	O
will	O
surely	O
continue	O
with	O
the	O
increasing	O
availability	O
of	O
enormous	O
quantities	O
of	O
data	B
and	O
the	O
software	O
to	O
analyze	O
it	O
.	O
the	O
purpose	O
of	O
an	O
introduction	O
to	O
statistical	O
learning	O
(	O
isl	O
)	O
is	O
to	O
facili-	O
tate	O
the	O
transition	O
of	O
statistical	O
learning	O
from	O
an	O
academic	O
to	O
a	O
mainstream	O
ﬁeld	O
.	O
isl	O
is	O
not	O
intended	O
to	O
replace	O
esl	O
,	O
which	O
is	O
a	O
far	O
more	O
comprehen-	O
sive	O
text	O
both	O
in	O
terms	O
of	O
the	O
number	O
of	O
approaches	O
considered	O
and	O
the	O
depth	O
to	O
which	O
they	O
are	O
explored	O
.	O
we	O
consider	O
esl	O
to	O
be	O
an	O
important	O
companion	O
for	O
professionals	O
(	O
with	O
graduate	O
degrees	O
in	O
statistics	O
,	O
machine	B
learning	O
,	O
or	O
related	O
ﬁelds	O
)	O
who	O
need	O
to	O
understand	O
the	O
technical	O
details	O
behind	O
statistical	O
learning	O
approaches	O
.	O
however	O
,	O
the	O
community	O
of	O
users	O
of	O
statistical	O
learning	O
techniques	O
has	O
expanded	O
to	O
include	O
individuals	O
with	O
a	O
wider	O
range	O
of	O
interests	O
and	O
backgrounds	O
.	O
therefore	O
,	O
we	O
believe	O
that	O
there	O
is	O
now	O
a	O
place	O
for	O
a	O
less	O
technical	O
and	O
more	O
accessible	O
version	O
of	O
esl	O
.	O
in	O
teaching	O
these	O
topics	O
over	O
the	O
years	O
,	O
we	O
have	O
discovered	O
that	O
they	O
are	O
of	O
interest	O
to	O
master	O
’	O
s	O
and	O
phd	O
students	O
in	O
ﬁelds	O
as	O
disparate	O
as	O
business	O
administration	O
,	O
biology	O
,	O
and	O
computer	O
science	O
,	O
as	O
well	O
as	O
to	O
quantitatively-	O
oriented	O
upper-division	O
undergraduates	O
.	O
it	O
is	O
important	O
for	O
this	O
diverse	O
group	O
to	O
be	O
able	O
to	O
understand	O
the	O
models	O
,	O
intuitions	O
,	O
and	O
strengths	O
and	O
weaknesses	O
of	O
the	O
various	O
approaches	O
.	O
but	O
for	O
this	O
audience	O
,	O
many	O
of	O
the	O
technical	O
details	O
behind	O
statistical	O
learning	O
methods	O
,	O
such	O
as	O
optimiza-	O
tion	O
algorithms	O
and	O
theoretical	O
properties	O
,	O
are	O
not	O
of	O
primary	O
interest	O
.	O
we	O
believe	O
that	O
these	O
students	O
do	O
not	O
need	O
a	O
deep	O
understanding	O
of	O
these	O
aspects	O
in	O
order	O
to	O
become	O
informed	O
users	O
of	O
the	O
various	O
methodologies	O
,	O
and	O
8	O
1.	O
introduction	O
in	O
order	O
to	O
contribute	O
to	O
their	O
chosen	O
ﬁelds	O
through	O
the	O
use	O
of	O
statistical	O
learning	O
tools	O
.	O
islr	O
is	O
based	O
on	O
the	O
following	O
four	O
premises	O
.	O
1.	O
many	O
statistical	O
learning	O
methods	O
are	O
relevant	O
and	O
useful	O
in	O
a	O
wide	O
range	O
of	O
academic	O
and	O
non-academic	O
disciplines	O
,	O
beyond	O
just	O
the	O
sta-	O
tistical	O
sciences	O
.	O
we	O
believe	O
that	O
many	O
contemporary	O
statistical	O
learn-	O
ing	O
procedures	O
should	O
,	O
and	O
will	O
,	O
become	O
as	O
widely	O
available	O
and	O
used	O
as	O
is	O
currently	O
the	O
case	O
for	O
classical	O
methods	O
such	O
as	O
linear	B
regres-	O
sion	O
.	O
as	O
a	O
result	O
,	O
rather	O
than	O
attempting	O
to	O
consider	O
every	O
possible	O
approach	B
(	O
an	O
impossible	O
task	O
)	O
,	O
we	O
have	O
concentrated	O
on	O
presenting	O
the	O
methods	O
that	O
we	O
believe	O
are	O
most	O
widely	O
applicable	O
.	O
2.	O
statistical	O
learning	O
should	O
not	O
be	O
viewed	O
as	O
a	O
series	O
of	O
black	O
boxes	O
.	O
no	O
single	B
approach	O
will	O
perform	O
well	O
in	O
all	O
possible	O
applications	O
.	O
with-	O
out	O
understanding	O
all	O
of	O
the	O
cogs	O
inside	O
the	O
box	O
,	O
or	O
the	O
interaction	B
between	O
those	O
cogs	O
,	O
it	O
is	O
impossible	O
to	O
select	O
the	O
best	O
box	O
.	O
hence	O
,	O
we	O
have	O
attempted	O
to	O
carefully	O
describe	O
the	O
model	B
,	O
intuition	O
,	O
assump-	O
tions	O
,	O
and	O
trade-oﬀs	O
behind	O
each	O
of	O
the	O
methods	O
that	O
we	O
consider	O
.	O
3.	O
while	O
it	O
is	O
important	O
to	O
know	O
what	O
job	O
is	O
performed	O
by	O
each	O
cog	O
,	O
it	O
is	O
not	O
necessary	O
to	O
have	O
the	O
skills	O
to	O
construct	O
the	O
machine	B
inside	O
the	O
box	O
!	O
thus	O
,	O
we	O
have	O
minimized	O
discussion	O
of	O
technical	O
details	O
related	O
to	O
ﬁtting	O
procedures	O
and	O
theoretical	O
properties	O
.	O
we	O
assume	O
that	O
the	O
reader	O
is	O
comfortable	O
with	O
basic	O
mathematical	O
concepts	O
,	O
but	O
we	O
do	O
not	O
assume	O
a	O
graduate	O
degree	O
in	O
the	O
mathematical	O
sciences	O
.	O
for	O
in-	O
stance	O
,	O
we	O
have	O
almost	O
completely	O
avoided	O
the	O
use	O
of	O
matrix	O
algebra	O
,	O
and	O
it	O
is	O
possible	O
to	O
understand	O
the	O
entire	O
book	O
without	O
a	O
detailed	O
knowledge	O
of	O
matrices	O
and	O
vectors	O
.	O
4.	O
we	O
presume	O
that	O
the	O
reader	O
is	O
interested	O
in	O
applying	O
statistical	O
learn-	O
ing	O
methods	O
to	O
real-world	O
problems	O
.	O
in	O
order	O
to	O
facilitate	O
this	O
,	O
as	O
well	O
as	O
to	O
motivate	O
the	O
techniques	O
discussed	O
,	O
we	O
have	O
devoted	O
a	O
section	O
within	O
each	O
chapter	O
to	O
r	O
computer	O
labs	O
.	O
in	O
each	O
lab	O
,	O
we	O
walk	O
the	O
reader	O
through	O
a	O
realistic	O
application	O
of	O
the	O
methods	O
considered	O
in	O
that	O
chapter	O
.	O
when	O
we	O
have	O
taught	O
this	O
material	O
in	O
our	O
courses	O
,	O
we	O
have	O
allocated	O
roughly	O
one-third	O
of	O
classroom	O
time	O
to	O
working	O
through	O
the	O
labs	O
,	O
and	O
we	O
have	O
found	O
them	O
to	O
be	O
extremely	O
useful	O
.	O
many	O
of	O
the	O
less	O
computationally-oriented	O
students	O
who	O
were	O
ini-	O
tially	O
intimidated	O
by	O
r	O
’	O
s	O
command	O
level	B
interface	O
got	O
the	O
hang	O
of	O
things	O
over	O
the	O
course	O
of	O
the	O
quarter	O
or	O
semester	O
.	O
we	O
have	O
used	O
r	O
because	O
it	O
is	O
freely	O
available	O
and	O
is	O
powerful	O
enough	O
to	O
implement	O
all	O
of	O
the	O
methods	O
discussed	O
in	O
the	O
book	O
.	O
it	O
also	O
has	O
optional	O
packages	O
that	O
can	O
be	O
downloaded	O
to	O
implement	O
literally	O
thousands	O
of	O
addi-	O
tional	O
methods	O
.	O
most	O
importantly	O
,	O
r	O
is	O
the	O
language	O
of	O
choice	O
for	O
academic	O
statisticians	O
,	O
and	O
new	O
approaches	O
often	O
become	O
available	O
in	O
1.	O
introduction	O
9	O
r	O
years	O
before	O
they	O
are	O
implemented	O
in	O
commercial	O
packages	O
.	O
how-	O
ever	O
,	O
the	O
labs	O
in	O
isl	O
are	O
self-contained	O
,	O
and	O
can	O
be	O
skipped	O
if	O
the	O
reader	O
wishes	O
to	O
use	O
a	O
diﬀerent	O
software	O
package	O
or	O
does	O
not	O
wish	O
to	O
apply	O
the	O
methods	O
discussed	O
to	O
real-world	O
problems	O
.	O
who	O
should	O
read	O
this	O
book	O
?	O
this	O
book	O
is	O
intended	O
for	O
anyone	O
who	O
is	O
interested	O
in	O
using	O
modern	O
statis-	O
tical	O
methods	O
for	O
modeling	O
and	O
prediction	B
from	O
data	B
.	O
this	O
group	O
includes	O
scientists	O
,	O
engineers	O
,	O
data	B
analysts	O
,	O
or	O
quants	O
,	O
but	O
also	O
less	O
technical	O
indi-	O
viduals	O
with	O
degrees	O
in	O
non-quantitative	O
ﬁelds	O
such	O
as	O
the	O
social	O
sciences	O
or	O
business	O
.	O
we	O
expect	O
that	O
the	O
reader	O
will	O
have	O
had	O
at	O
least	O
one	O
elementary	O
course	O
in	O
statistics	O
.	O
background	O
in	O
linear	B
regression	I
is	O
also	O
useful	O
,	O
though	O
not	O
required	O
,	O
since	O
we	O
review	O
the	O
key	O
concepts	O
behind	O
linear	B
regression	I
in	O
chapter	O
3.	O
the	O
mathematical	O
level	B
of	O
this	O
book	O
is	O
modest	O
,	O
and	O
a	O
detailed	O
knowledge	O
of	O
matrix	O
operations	O
is	O
not	O
required	O
.	O
this	O
book	O
provides	O
an	O
in-	O
troduction	O
to	O
the	O
statistical	O
programming	O
language	O
r.	O
previous	O
exposure	O
to	O
a	O
programming	O
language	O
,	O
such	O
as	O
matlab	O
or	O
python	O
,	O
is	O
useful	O
but	O
not	O
required	O
.	O
we	O
have	O
successfully	O
taught	O
material	O
at	O
this	O
level	B
to	O
master	O
’	O
s	O
and	O
phd	O
students	O
in	O
business	O
,	O
computer	O
science	O
,	O
biology	O
,	O
earth	O
sciences	O
,	O
psychology	O
,	O
and	O
many	O
other	O
areas	O
of	O
the	O
physical	O
and	O
social	O
sciences	O
.	O
this	O
book	O
could	O
also	O
be	O
appropriate	O
for	O
advanced	O
undergraduates	O
who	O
have	O
already	O
taken	O
a	O
course	O
on	O
linear	B
regression	I
.	O
in	O
the	O
context	O
of	O
a	O
more	O
mathematically	O
rigorous	O
course	O
in	O
which	O
esl	O
serves	O
as	O
the	O
primary	O
textbook	O
,	O
isl	O
could	O
be	O
used	O
as	O
a	O
supplementary	O
text	O
for	O
teaching	O
computational	O
aspects	O
of	O
the	O
various	O
approaches	O
.	O
notation	O
and	O
simple	B
matrix	O
algebra	O
choosing	O
notation	O
for	O
a	O
textbook	O
is	O
always	O
a	O
diﬃcult	O
task	O
.	O
for	O
the	O
most	O
part	O
we	O
adopt	O
the	O
same	O
notational	O
conventions	O
as	O
esl	O
.	O
we	O
will	O
use	O
n	O
to	O
represent	O
the	O
number	O
of	O
distinct	O
data	B
points	O
,	O
or	O
observa-	O
tions	O
,	O
in	O
our	O
sample	O
.	O
we	O
will	O
let	O
p	O
denote	O
the	O
number	O
of	O
variables	O
that	O
are	O
available	O
for	O
use	O
in	O
making	O
predictions	O
.	O
for	O
example	O
,	O
the	O
wage	O
data	B
set	O
con-	O
sists	O
of	O
12	O
variables	O
for	O
3,000	O
people	O
,	O
so	O
we	O
have	O
n	O
=	O
3,000	O
observations	B
and	O
p	O
=	O
12	O
variables	O
(	O
such	O
as	O
year	O
,	O
age	O
,	O
,	O
and	O
more	O
)	O
.	O
note	O
that	O
throughout	O
this	O
book	O
,	O
we	O
indicate	O
variable	B
names	O
using	O
colored	O
font	O
:	O
variable	B
name	O
.	O
sex	O
in	O
some	O
examples	O
,	O
p	O
might	O
be	O
quite	O
large	O
,	O
such	O
as	O
on	O
the	O
order	O
of	O
thou-	O
sands	O
or	O
even	O
millions	O
;	O
this	O
situation	O
arises	O
quite	O
often	O
,	O
for	O
example	O
,	O
in	O
the	O
analysis	O
of	O
modern	O
biological	O
data	B
or	O
web-based	O
advertising	O
data	B
.	O
10	O
1.	O
introduction	O
in	O
general	O
,	O
we	O
will	O
let	O
xij	O
represent	O
the	O
value	O
of	O
the	O
jth	O
variable	B
for	O
the	O
ith	O
observation	O
,	O
where	O
i	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
and	O
j	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
p.	O
throughout	O
this	O
book	O
,	O
i	O
will	O
be	O
used	O
to	O
index	O
the	O
samples	O
or	O
observations	B
(	O
from	O
1	O
to	O
n	O
)	O
and	O
j	O
will	O
be	O
used	O
to	O
index	O
the	O
variables	O
(	O
from	O
1	O
to	O
p	O
)	O
.	O
we	O
let	O
x	O
denote	O
a	O
n×	O
p	O
matrix	O
whose	O
(	O
i	O
,	O
j	O
)	O
th	O
element	O
is	O
xij	O
.	O
that	O
is	O
,	O
⎛	O
⎜⎜⎜⎝	O
x	O
=	O
⎞	O
⎟⎟⎟⎠	O
.	O
x11	O
x12	O
x21	O
x22	O
...	O
...	O
xn1	O
xn2	O
.	O
.	O
.	O
x1p	O
.	O
.	O
.	O
x2p	O
...	O
.	O
.	O
.	O
.	O
.	O
.	O
xnp	O
for	O
readers	O
who	O
are	O
unfamiliar	O
with	O
matrices	O
,	O
it	O
is	O
useful	O
to	O
visualize	O
x	O
as	O
a	O
spreadsheet	O
of	O
numbers	O
with	O
n	O
rows	O
and	O
p	O
columns	O
.	O
at	O
times	O
we	O
will	O
be	O
interested	O
in	O
the	O
rows	O
of	O
x	O
,	O
which	O
we	O
write	O
as	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
here	O
xi	O
is	O
a	O
vector	B
of	O
length	O
p	O
,	O
containing	O
the	O
p	O
variable	B
measurements	O
for	O
the	O
ith	O
observation	O
.	O
that	O
is	O
,	O
⎞	O
⎟⎟⎟⎠	O
.	O
⎛	O
⎜⎜⎜⎝	O
xi1	O
xi2	O
...	O
xip	O
xi	O
=	O
(	O
1.1	O
)	O
(	O
vectors	O
are	O
by	O
default	O
represented	O
as	O
columns	O
.	O
)	O
for	O
example	O
,	O
for	O
the	O
wage	O
data	B
,	O
xi	O
is	O
a	O
vector	B
of	O
length	O
12	O
,	O
consisting	O
of	O
year	O
,	O
age	O
,	O
,	O
and	O
other	O
values	O
for	O
the	O
ith	O
individual	O
.	O
at	O
other	O
times	O
we	O
will	O
instead	O
be	O
interested	O
in	O
the	O
columns	O
of	O
x	O
,	O
which	O
we	O
write	O
as	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
.	O
each	O
is	O
a	O
vector	B
of	O
length	O
n.	O
that	O
is	O
,	O
sex	O
⎞	O
⎟⎟⎟⎠	O
.	O
⎛	O
⎜⎜⎜⎝	O
x1j	O
x2j	O
...	O
xnj	O
xj	O
=	O
for	O
example	O
,	O
for	O
the	O
wage	O
data	B
,	O
x1	O
contains	O
the	O
n	O
=	O
3,000	O
values	O
for	O
year	O
.	O
using	O
this	O
notation	O
,	O
the	O
matrix	O
x	O
can	O
be	O
written	O
as	O
x	O
=	O
or	O
(	O
cid:8	O
)	O
x1	O
x2	B
···	O
xp	O
(	O
cid:9	O
)	O
,	O
⎞	O
⎟⎟⎟⎠	O
.	O
⎜⎜⎜⎝	O
⎛	O
xt	O
1	O
xt	O
2	O
...	O
xt	O
n	O
x	O
=	O
1.	O
introduction	O
11	O
the	O
t	O
notation	O
denotes	O
the	O
transpose	O
of	O
a	O
matrix	O
or	O
vector	B
.	O
so	O
,	O
for	O
example	O
,	O
⎛	O
⎜⎜⎜⎝	O
xt	O
=	O
⎞	O
⎟⎟⎟⎠	O
,	O
x11	O
x21	O
x12	O
x22	O
...	O
...	O
x1p	O
x2p	O
.	O
.	O
.	O
xn1	O
.	O
.	O
.	O
xn2	O
...	O
.	O
.	O
.	O
xnp	O
while	O
(	O
cid:8	O
)	O
xi1	O
xi2	O
···	O
(	O
cid:9	O
)	O
.	O
xip	O
xt	O
i	O
=	O
we	O
use	O
yi	O
to	O
denote	O
the	O
ith	O
observation	O
of	O
the	O
variable	B
on	O
which	O
we	O
wish	O
to	O
make	O
predictions	O
,	O
such	O
as	O
wage	O
.	O
hence	O
,	O
we	O
write	O
the	O
set	B
of	O
all	O
n	O
observations	B
in	O
vector	B
form	O
as	O
⎞	O
⎟⎟⎟⎠	O
.	O
⎛	O
⎜⎜⎜⎝	O
y1	O
y2	O
...	O
yn	O
y	O
=	O
then	O
our	O
observed	O
data	B
consists	O
of	O
{	O
(	O
x1	O
,	O
y1	O
)	O
,	O
(	O
x2	B
,	O
y2	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
}	O
,	O
where	O
each	O
xi	O
is	O
a	O
vector	B
of	O
length	O
p.	O
(	O
if	O
p	O
=	O
1	O
,	O
then	O
xi	O
is	O
simply	O
a	O
scalar	O
.	O
)	O
in	O
this	O
text	O
,	O
a	O
vector	B
of	O
length	O
n	O
will	O
always	O
be	O
denoted	O
in	O
lower	O
case	O
bold	O
;	O
e.g	O
.	O
⎞	O
⎟⎟⎟⎠	O
.	O
⎛	O
⎜⎜⎜⎝	O
a1	O
a2	O
...	O
an	O
a	O
=	O
however	O
,	O
vectors	O
that	O
are	O
not	O
of	O
length	O
n	O
(	O
such	O
as	O
feature	B
vectors	O
of	O
length	O
p	O
,	O
as	O
in	O
(	O
1.1	O
)	O
)	O
will	O
be	O
denoted	O
in	O
lower	O
case	O
normal	O
font	O
,	O
e.g	O
.	O
a.	O
scalars	O
will	O
also	O
be	O
denoted	O
in	O
lower	O
case	O
normal	O
font	O
,	O
e.g	O
.	O
a.	O
in	O
the	O
rare	O
cases	O
in	O
which	O
these	O
two	O
uses	O
for	O
lower	O
case	O
normal	O
font	O
lead	O
to	O
ambiguity	O
,	O
we	O
will	O
clarify	O
which	O
use	O
is	O
intended	O
.	O
matrices	O
will	O
be	O
denoted	O
using	O
bold	O
capitals	O
,	O
such	O
as	O
a.	O
random	O
variables	O
will	O
be	O
denoted	O
using	O
capital	O
normal	O
font	O
,	O
e.g	O
.	O
a	O
,	O
regardless	O
of	O
their	O
dimensions	O
.	O
occasionally	O
we	O
will	O
want	O
to	O
indicate	O
the	O
dimension	O
of	O
a	O
particular	O
ob-	O
ject	O
.	O
to	O
indicate	O
that	O
an	O
object	O
is	O
a	O
scalar	O
,	O
we	O
will	O
use	O
the	O
notation	O
a	O
∈	O
r.	O
to	O
indicate	O
that	O
it	O
is	O
a	O
vector	B
of	O
length	O
k	O
,	O
we	O
will	O
use	O
a	O
∈	O
rk	O
(	O
or	O
a	O
∈	O
rn	O
if	O
it	O
is	O
of	O
length	O
n	O
)	O
.	O
we	O
will	O
indicate	O
that	O
an	O
object	O
is	O
a	O
r	O
×	O
s	O
matrix	O
using	O
a	O
∈	O
rr×s	O
.	O
we	O
have	O
avoided	O
using	O
matrix	O
algebra	O
whenever	O
possible	O
.	O
however	O
,	O
in	O
a	O
few	O
instances	O
it	O
becomes	O
too	O
cumbersome	O
to	O
avoid	O
it	O
entirely	O
.	O
in	O
these	O
rare	O
instances	O
it	O
is	O
important	O
to	O
understand	O
the	O
concept	O
of	O
multiplying	O
two	O
matrices	O
.	O
suppose	O
that	O
a	O
∈	O
rr×d	O
and	O
b	O
∈	O
rd×s	O
.	O
then	O
the	O
product	O
12	O
1.	O
introduction	O
of	O
a	O
and	O
b	O
is	O
denoted	O
ab	O
.	O
the	O
(	O
i	O
,	O
j	O
)	O
th	O
element	O
of	O
ab	O
is	O
computed	O
by	O
multiplying	O
each	O
element	O
of	O
the	O
ith	O
row	O
of	O
a	O
by	O
the	O
corresponding	O
element	O
d	O
of	O
the	O
jth	O
column	O
of	O
b.	O
that	O
is	O
,	O
(	O
ab	O
)	O
ij	O
=	O
k=1	O
aikbkj	O
.	O
as	O
an	O
example	O
,	O
(	O
cid:11	O
)	O
consider	O
5	O
7	O
(	O
cid:11	O
)	O
1	O
2	O
3	O
4	O
(	O
cid:12	O
)	O
6	O
8	O
and	O
b	O
=	O
a	O
=	O
(	O
cid:10	O
)	O
(	O
cid:12	O
)	O
.	O
then	O
(	O
cid:11	O
)	O
1	O
2	O
3	O
4	O
(	O
cid:12	O
)	O
(	O
cid:11	O
)	O
(	O
cid:12	O
)	O
5	O
6	O
7	O
8	O
(	O
cid:11	O
)	O
1	O
×	O
5	O
+	O
2	O
×	O
7	O
1	O
×	O
6	O
+	O
2	O
×	O
8	O
3	O
×	O
5	O
+	O
4	O
×	O
7	O
3	O
×	O
6	O
+	O
4	O
×	O
8	O
=	O
ab	O
=	O
.	O
note	O
that	O
this	O
operation	O
produces	O
an	O
r	O
×	O
s	O
matrix	O
.	O
it	O
is	O
only	O
possible	O
to	O
compute	O
ab	O
if	O
the	O
number	O
of	O
columns	O
of	O
a	O
is	O
the	O
same	O
as	O
the	O
number	O
of	O
rows	O
of	O
b	O
.	O
=	O
(	O
cid:12	O
)	O
(	O
cid:11	O
)	O
(	O
cid:12	O
)	O
19	O
22	O
43	O
50	O
organization	O
of	O
this	O
book	O
chapter	O
2	O
introduces	O
the	O
basic	O
terminology	O
and	O
concepts	O
behind	O
statisti-	O
cal	O
learning	O
.	O
this	O
chapter	O
also	O
presents	O
the	O
k-nearest	O
neighbor	O
classiﬁer	B
,	O
a	O
very	O
simple	B
method	O
that	O
works	O
surprisingly	O
well	O
on	O
many	O
problems	O
.	O
chap-	O
ters	O
3	O
and	O
4	O
cover	O
classical	O
linear	B
methods	O
for	O
regression	O
and	O
classiﬁcation	B
.	O
in	O
particular	O
,	O
chapter	O
3	O
reviews	O
linear	B
regression	I
,	O
the	O
fundamental	O
start-	O
ing	O
point	O
for	O
all	O
regression	B
methods	O
.	O
in	O
chapter	O
4	O
we	O
discuss	O
two	O
of	O
the	O
most	O
important	O
classical	O
classiﬁcation	B
methods	O
,	O
logistic	B
regression	I
and	O
lin-	O
ear	O
discriminant	O
analysis	O
.	O
a	O
central	O
problem	O
in	O
all	O
statistical	O
learning	O
situations	O
involves	O
choosing	O
the	O
best	O
method	O
for	O
a	O
given	O
application	O
.	O
hence	O
,	O
in	O
chapter	O
5	O
we	O
intro-	O
duce	O
cross-validation	B
and	O
the	O
bootstrap	B
,	O
which	O
can	O
be	O
used	O
to	O
estimate	O
the	O
accuracy	O
of	O
a	O
number	O
of	O
diﬀerent	O
methods	O
in	O
order	O
to	O
choose	O
the	O
best	O
one	O
.	O
much	O
of	O
the	O
recent	O
research	O
in	O
statistical	O
learning	O
has	O
concentrated	O
on	O
non-linear	B
methods	O
.	O
however	O
,	O
linear	B
methods	O
often	O
have	O
advantages	O
over	O
their	O
non-linear	B
competitors	O
in	O
terms	O
of	O
interpretability	B
and	O
sometimes	O
also	O
accuracy	O
.	O
hence	O
,	O
in	O
chapter	O
6	O
we	O
consider	O
a	O
host	O
of	O
linear	B
methods	O
,	O
both	O
classical	O
and	O
more	O
modern	O
,	O
which	O
oﬀer	O
potential	O
improvements	O
over	O
stan-	O
dard	O
linear	B
regression	I
.	O
these	O
include	O
stepwise	O
selection	O
,	O
ridge	B
regression	I
,	O
principal	B
components	I
regression	O
,	O
partial	B
least	I
squares	I
,	O
and	O
the	O
lasso	B
.	O
the	O
remaining	O
chapters	O
move	O
into	O
the	O
world	O
of	O
non-linear	B
statistical	O
learning	O
.	O
we	O
ﬁrst	O
introduce	O
in	O
chapter	O
7	O
a	O
number	O
of	O
non-linear	B
methods	O
that	O
work	O
well	O
for	O
problems	O
with	O
a	O
single	B
input	O
variable	B
.	O
we	O
then	O
show	O
how	O
these	O
methods	O
can	O
be	O
used	O
to	O
ﬁt	B
non-linear	O
additive	B
models	O
for	O
which	O
there	O
is	O
more	O
than	O
one	O
input	B
.	O
in	O
chapter	O
8	O
,	O
we	O
investigate	O
tree-based	O
methods	O
,	O
including	O
bagging	B
,	O
boosting	B
,	O
and	O
random	O
forests	O
.	O
support	B
vector	I
machines	O
,	O
a	O
set	B
of	O
approaches	O
for	O
performing	O
both	O
linear	B
and	O
non-linear	B
classiﬁcation	O
,	O
1.	O
introduction	O
13	O
are	O
discussed	O
in	O
chapter	O
9.	O
finally	O
,	O
in	O
chapter	O
10	O
,	O
we	O
consider	O
a	O
setting	O
in	O
which	O
we	O
have	O
input	B
variables	O
but	O
no	O
output	B
variable	I
.	O
in	O
particular	O
,	O
we	O
present	O
principal	B
components	I
analysis	O
,	O
k-means	O
clustering	B
,	O
and	O
hierarchi-	O
cal	O
clustering	B
.	O
at	O
the	O
end	O
of	O
each	O
chapter	O
,	O
we	O
present	O
one	O
or	O
more	O
r	O
lab	O
sections	O
in	O
which	O
we	O
systematically	O
work	O
through	O
applications	O
of	O
the	O
various	O
meth-	O
ods	O
discussed	O
in	O
that	O
chapter	O
.	O
these	O
labs	O
demonstrate	O
the	O
strengths	O
and	O
weaknesses	O
of	O
the	O
various	O
approaches	O
,	O
and	O
also	O
provide	O
a	O
useful	O
reference	O
for	O
the	O
syntax	O
required	O
to	O
implement	O
the	O
various	O
methods	O
.	O
the	O
reader	O
may	O
choose	O
to	O
work	O
through	O
the	O
labs	O
at	O
his	O
or	O
her	O
own	O
pace	O
,	O
or	O
the	O
labs	O
may	O
be	O
the	O
focus	O
of	O
group	O
sessions	O
as	O
part	O
of	O
a	O
classroom	O
environment	O
.	O
within	O
each	O
r	O
lab	O
,	O
we	O
present	O
the	O
results	O
that	O
we	O
obtained	O
when	O
we	O
performed	O
the	O
lab	O
at	O
the	O
time	O
of	O
writing	O
this	O
book	O
.	O
however	O
,	O
new	O
versions	O
of	O
r	O
are	O
continuously	O
released	O
,	O
and	O
over	O
time	O
,	O
the	O
packages	O
called	O
in	O
the	O
labs	O
will	O
be	O
updated	O
.	O
therefore	O
,	O
in	O
the	O
future	O
,	O
it	O
is	O
possible	O
that	O
the	O
results	O
shown	O
in	O
the	O
lab	O
sections	O
may	O
no	O
longer	O
correspond	O
precisely	O
to	O
the	O
results	O
obtained	O
by	O
the	O
reader	O
who	O
performs	O
the	O
labs	O
.	O
as	O
necessary	O
,	O
we	O
will	O
post	O
updates	O
to	O
the	O
labs	O
on	O
the	O
book	O
website	O
.	O
we	O
use	O
the	O
symbol	O
to	O
denote	O
sections	O
or	O
exercises	O
that	O
contain	O
more	O
challenging	O
concepts	O
.	O
these	O
can	O
be	O
easily	O
skipped	O
by	O
readers	O
who	O
do	O
not	O
wish	O
to	O
delve	O
as	O
deeply	O
into	O
the	O
material	O
,	O
or	O
who	O
lack	O
the	O
mathematical	O
background	O
.	O
data	B
sets	O
used	O
in	O
labs	O
and	O
exercises	O
in	O
this	O
textbook	O
,	O
we	O
illustrate	O
statistical	O
learning	O
methods	O
using	O
applica-	O
tions	O
from	O
marketing	O
,	O
ﬁnance	O
,	O
biology	O
,	O
and	O
other	O
areas	O
.	O
the	O
islr	O
package	O
available	O
on	O
the	O
book	O
website	O
contains	O
a	O
number	O
of	O
data	B
sets	O
that	O
are	O
required	O
in	O
order	O
to	O
perform	O
the	O
labs	O
and	O
exercises	O
associated	O
with	O
this	O
book	O
.	O
one	O
other	O
data	B
set	O
is	O
contained	O
in	O
the	O
mass	O
library	O
,	O
and	O
yet	O
another	O
is	O
part	O
of	O
the	O
base	O
r	O
distribution	B
.	O
table	O
1.1	O
contains	O
a	O
summary	O
of	O
the	O
data	B
sets	O
required	O
to	O
perform	O
the	O
labs	O
and	O
exercises	O
.	O
a	O
couple	O
of	O
these	O
data	B
sets	O
are	O
also	O
available	O
as	O
text	O
ﬁles	O
on	O
the	O
book	O
website	O
,	O
for	O
use	O
in	O
chapter	O
2.	O
book	O
website	O
the	O
website	O
for	O
this	O
book	O
is	O
located	O
at	O
www.statlearning.com	O
14	O
1.	O
introduction	O
description	O
gas	O
mileage	O
,	O
horsepower	O
,	O
and	O
other	O
information	O
for	O
cars	O
.	O
housing	O
values	O
and	O
other	O
information	O
about	O
boston	O
suburbs	O
.	O
information	O
about	O
individuals	O
oﬀered	O
caravan	O
insurance	O
.	O
information	O
about	O
car	O
seat	O
sales	O
in	O
400	O
stores	O
.	O
demographic	O
characteristics	O
,	O
tuition	O
,	O
and	O
more	O
for	O
usa	O
colleges	O
.	O
customer	O
default	O
records	O
for	O
a	O
credit	O
card	O
company	O
.	O
records	O
and	O
salaries	O
for	O
baseball	O
players	O
.	O
gene	O
expression	O
measurements	O
for	O
four	O
cancer	O
types	O
.	O
gene	O
expression	O
measurements	O
for	O
64	O
cancer	O
cell	O
lines	O
.	O
sales	O
information	O
for	O
citrus	O
hill	O
and	O
minute	O
maid	O
orange	O
juice	O
.	O
name	O
auto	O
boston	O
caravan	O
carseats	O
college	O
default	O
hitters	O
khan	O
nci60	O
oj	O
portfolio	O
past	O
values	O
of	O
ﬁnancial	O
assets	O
,	O
for	O
use	O
in	O
portfolio	O
allocation	O
.	O
daily	O
percentage	O
returns	O
for	O
s	O
&	O
p	O
500	O
over	O
a	O
5-year	O
period	O
.	O
smarket	O
usarrests	O
crime	O
statistics	O
per	O
100,000	O
residents	O
in	O
50	O
states	O
of	O
usa	O
.	O
wage	O
weekly	O
income	O
survey	O
data	B
for	O
males	O
in	O
central	O
atlantic	O
region	O
of	O
usa	O
.	O
1,089	O
weekly	O
stock	O
market	O
returns	O
for	O
21	O
years	O
.	O
table	O
1.1.	O
a	O
list	O
of	O
data	B
sets	O
needed	O
to	O
perform	O
the	O
labs	O
and	O
exercises	O
in	O
this	O
textbook	O
.	O
all	O
data	B
sets	O
are	O
available	O
in	O
the	O
islr	O
library	O
,	O
with	O
the	O
exception	O
of	O
boston	O
(	O
part	O
of	O
mass	O
)	O
and	O
usarrests	O
(	O
part	O
of	O
the	O
base	O
r	O
distribution	B
)	O
.	O
it	O
contains	O
a	O
number	O
of	O
resources	O
,	O
including	O
the	O
r	O
package	O
associated	O
with	O
this	O
book	O
,	O
and	O
some	O
additional	O
data	B
sets	O
.	O
acknowledgements	O
a	O
few	O
of	O
the	O
plots	O
in	O
this	O
book	O
were	O
taken	O
from	O
esl	O
:	O
figures	O
6.7	O
,	O
8.3	O
,	O
and	O
10.12.	O
all	O
other	O
plots	O
are	O
new	O
to	O
this	O
book	O
.	O
2	O
statistical	O
learning	O
2.1	O
what	O
is	O
statistical	O
learning	O
?	O
in	O
order	O
to	O
motivate	O
our	O
study	O
of	O
statistical	O
learning	O
,	O
we	O
begin	O
with	O
a	O
simple	B
example	O
.	O
suppose	O
that	O
we	O
are	O
statistical	O
consultants	O
hired	O
by	O
a	O
client	O
to	O
provide	O
advice	O
on	O
how	O
to	O
improve	O
sales	O
of	O
a	O
particular	O
product	O
.	O
the	O
advertising	O
data	B
set	O
consists	O
of	O
the	O
sales	O
of	O
that	O
product	O
in	O
200	O
diﬀerent	O
markets	O
,	O
along	O
with	O
advertising	O
budgets	O
for	O
the	O
product	O
in	O
each	O
of	O
those	O
markets	O
for	O
three	O
diﬀerent	O
media	O
:	O
tv	O
,	O
radio	O
,	O
and	O
newspaper	O
.	O
the	O
data	B
are	O
displayed	O
in	O
figure	O
2.1.	O
it	O
is	O
not	O
possible	O
for	O
our	O
client	O
to	O
directly	O
increase	O
sales	O
of	O
the	O
product	O
.	O
on	O
the	O
other	O
hand	O
,	O
they	O
can	O
control	O
the	O
advertising	O
expenditure	O
in	O
each	O
of	O
the	O
three	O
media	O
.	O
therefore	O
,	O
if	O
we	O
determine	O
that	O
there	O
is	O
an	O
association	O
between	O
advertising	O
and	O
sales	O
,	O
then	O
we	O
can	O
instruct	O
our	O
client	O
to	O
adjust	O
advertising	O
budgets	O
,	O
thereby	O
indirectly	O
increasing	O
sales	O
.	O
in	O
other	O
words	O
,	O
our	O
goal	O
is	O
to	O
develop	O
an	O
accurate	O
model	B
that	O
can	O
be	O
used	O
to	O
predict	O
sales	O
on	O
the	O
basis	B
of	O
the	O
three	O
media	O
budgets	O
.	O
in	O
this	O
setting	O
,	O
the	O
advertising	O
budgets	O
are	O
input	B
variables	O
while	O
sales	O
is	O
an	O
output	B
variable	I
.	O
the	O
input	B
variables	O
are	O
typically	O
denoted	O
using	O
the	O
symbol	O
x	O
,	O
with	O
a	O
subscript	O
to	O
distinguish	O
them	O
.	O
so	O
x1	O
might	O
be	O
the	O
tv	O
budget	O
,	O
x2	B
the	O
radio	O
budget	O
,	O
and	O
x3	O
the	O
newspaper	O
budget	O
.	O
the	O
inputs	O
go	O
by	O
diﬀerent	O
names	O
,	O
such	O
as	O
predictors	O
,	O
independent	B
variables	O
,	O
features	O
,	O
or	O
sometimes	O
just	O
variables	O
.	O
the	O
output	B
variable—in	O
this	O
case	O
,	O
sales—is	O
often	O
called	O
the	O
response	B
or	O
dependent	B
variable	I
,	O
and	O
is	O
typically	O
denoted	O
using	O
the	O
symbol	O
y	O
.	O
throughout	O
this	O
book	O
,	O
we	O
will	O
use	O
all	O
of	O
these	O
terms	O
interchangeably	O
.	O
g.	O
james	O
et	O
al.	O
,	O
an	O
introduction	O
to	O
statistical	O
learning	O
:	O
with	O
applications	O
in	O
r	O
,	O
springer	O
texts	O
in	O
statistics	O
,	O
doi	O
10.1007/978-1-4614-7138-7	O
2	O
,	O
©	O
springer	O
science+business	O
media	O
new	O
york	O
2013	O
15	O
input	B
variable	I
output	O
variable	B
predictor	O
independent	B
variable	I
feature	O
variable	B
response	O
dependent	B
variable	I
16	O
2.	O
statistical	O
learning	O
5	O
2	O
0	O
2	O
5	O
1	O
0	O
1	O
5	O
l	O
s	O
e	O
a	O
s	O
5	O
2	O
0	O
2	O
5	O
1	O
0	O
1	O
5	O
l	O
s	O
e	O
a	O
s	O
5	O
2	O
0	O
2	O
5	O
1	O
0	O
1	O
5	O
l	O
s	O
e	O
a	O
s	O
0	O
50	O
100	O
200	O
300	O
0	O
10	O
20	O
30	O
40	O
50	O
0	O
20	O
40	O
60	O
80	O
100	O
tv	O
radio	O
newspaper	O
figure	O
2.1.	O
the	O
advertising	O
data	B
set	O
.	O
the	O
plot	B
displays	O
sales	O
,	O
in	O
thousands	O
of	O
units	O
,	O
as	O
a	O
function	B
of	O
tv	O
,	O
radio	O
,	O
and	O
newspaper	O
budgets	O
,	O
in	O
thousands	O
of	O
dollars	O
,	O
for	O
200	O
diﬀerent	O
markets	O
.	O
in	O
each	O
plot	B
we	O
show	O
the	O
simple	B
least	O
squares	O
ﬁt	B
of	O
sales	O
to	O
that	O
variable	B
,	O
as	O
described	O
in	O
chapter	O
3.	O
in	O
other	O
words	O
,	O
each	O
blue	O
line	B
represents	O
a	O
simple	B
model	O
that	O
can	O
be	O
used	O
to	O
predict	O
sales	O
using	O
tv	O
,	O
radio	O
,	O
and	O
newspaper	O
,	O
respectively	O
.	O
more	O
generally	O
,	O
suppose	O
that	O
we	O
observe	O
a	O
quantitative	B
response	O
y	O
and	O
p	O
diﬀerent	O
predictors	O
,	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
.	O
we	O
assume	O
that	O
there	O
is	O
some	O
relationship	O
between	O
y	O
and	O
x	O
=	O
(	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
)	O
,	O
which	O
can	O
be	O
written	O
in	O
the	O
very	O
general	O
form	O
y	O
=	O
f	O
(	O
x	O
)	O
+	O
	O
.	O
(	O
2.1	O
)	O
here	O
f	O
is	O
some	O
ﬁxed	O
but	O
unknown	O
function	B
of	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
,	O
and	O
	O
is	O
a	O
random	O
error	O
term	B
,	O
which	O
is	O
independent	B
of	O
x	O
and	O
has	O
mean	O
zero	O
.	O
in	O
this	O
formula-	O
tion	O
,	O
f	O
represents	O
the	O
systematic	B
information	O
that	O
x	O
provides	O
about	O
y	O
.	O
as	O
another	O
example	O
,	O
consider	O
the	O
left-hand	O
panel	O
of	O
figure	O
2.2	O
,	O
a	O
plot	B
of	O
income	O
versus	O
years	O
of	O
education	O
for	O
30	O
individuals	O
in	O
the	O
income	O
data	B
set	O
.	O
the	O
plot	B
suggests	O
that	O
one	O
might	O
be	O
able	O
to	O
predict	O
income	O
using	O
years	O
of	O
education	O
.	O
however	O
,	O
the	O
function	B
f	O
that	O
connects	O
the	O
input	B
variable	I
to	O
the	O
output	B
variable	I
is	O
in	O
general	O
unknown	O
.	O
in	O
this	O
situation	O
one	O
must	O
estimate	O
f	O
based	O
on	O
the	O
observed	O
points	O
.	O
since	O
income	O
is	O
a	O
simulated	O
data	B
set	O
,	O
f	O
is	O
known	O
and	O
is	O
shown	O
by	O
the	O
blue	O
curve	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
2.2.	O
the	O
vertical	O
lines	O
represent	O
the	O
error	B
terms	O
	O
.	O
we	O
note	O
that	O
some	O
of	O
the	O
30	O
observations	B
lie	O
above	O
the	O
blue	O
curve	O
and	O
some	O
lie	O
below	O
it	O
;	O
overall	O
,	O
the	O
errors	O
have	O
approximately	O
mean	O
zero	O
.	O
in	O
general	O
,	O
the	O
function	B
f	O
may	O
involve	O
more	O
than	O
one	O
input	B
variable	I
.	O
in	O
figure	O
2.3	O
we	O
plot	B
income	O
as	O
a	O
function	B
of	O
years	O
of	O
education	O
and	O
seniority	O
.	O
here	O
f	O
is	O
a	O
two-dimensional	O
surface	O
that	O
must	O
be	O
estimated	O
based	O
on	O
the	O
observed	O
data	B
.	O
error	B
term	O
systematic	B
2.1	O
what	O
is	O
statistical	O
learning	O
?	O
17	O
0	O
8	O
0	O
7	O
0	O
6	O
0	O
5	O
0	O
4	O
0	O
3	O
0	O
2	O
e	O
m	O
o	O
c	O
n	O
i	O
0	O
8	O
0	O
7	O
0	O
6	O
0	O
5	O
0	O
4	O
0	O
3	O
0	O
2	O
e	O
m	O
o	O
c	O
n	O
i	O
10	O
12	O
14	O
16	O
18	O
20	O
22	O
10	O
12	O
14	O
16	O
18	O
20	O
22	O
years	O
of	O
education	O
years	O
of	O
education	O
figure	O
2.2.	O
the	O
income	O
data	B
set	O
.	O
left	O
:	O
the	O
red	O
dots	O
are	O
the	O
observed	O
values	O
of	O
income	O
(	O
in	O
tens	O
of	O
thousands	O
of	O
dollars	O
)	O
and	O
years	O
of	O
education	O
for	O
30	O
indi-	O
viduals	O
.	O
right	O
:	O
the	O
blue	O
curve	O
represents	O
the	O
true	O
underlying	O
relationship	O
between	O
income	O
and	O
years	O
of	O
education	O
,	O
which	O
is	O
generally	O
unknown	O
(	O
but	O
is	O
known	O
in	O
this	O
case	O
because	O
the	O
data	B
were	O
simulated	O
)	O
.	O
the	O
black	O
lines	O
represent	O
the	O
error	B
associated	O
with	O
each	O
observation	O
.	O
note	O
that	O
some	O
errors	O
are	O
positive	O
(	O
if	O
an	O
ob-	O
servation	O
lies	O
above	O
the	O
blue	O
curve	O
)	O
and	O
some	O
are	O
negative	O
(	O
if	O
an	O
observation	O
lies	O
below	O
the	O
curve	O
)	O
.	O
overall	O
,	O
these	O
errors	O
have	O
approximately	O
mean	O
zero	O
.	O
in	O
essence	O
,	O
statistical	O
learning	O
refers	O
to	O
a	O
set	B
of	O
approaches	O
for	O
estimating	O
f	O
.	O
in	O
this	O
chapter	O
we	O
outline	O
some	O
of	O
the	O
key	O
theoretical	O
concepts	O
that	O
arise	O
in	O
estimating	O
f	O
,	O
as	O
well	O
as	O
tools	O
for	O
evaluating	O
the	O
estimates	O
obtained	O
.	O
2.1.1	O
why	O
estimate	O
f	O
?	O
there	O
are	O
two	O
main	O
reasons	O
that	O
we	O
may	O
wish	O
to	O
estimate	O
f	O
:	O
prediction	B
and	O
inference	B
.	O
we	O
discuss	O
each	O
in	O
turn	O
.	O
prediction	B
in	O
many	O
situations	O
,	O
a	O
set	B
of	O
inputs	O
x	O
are	O
readily	O
available	O
,	O
but	O
the	O
output	B
y	O
can	O
not	O
be	O
easily	O
obtained	O
.	O
in	O
this	O
setting	O
,	O
since	O
the	O
error	B
term	O
averages	O
to	O
zero	O
,	O
we	O
can	O
predict	O
y	O
using	O
ˆy	O
=	O
ˆf	O
(	O
x	O
)	O
,	O
(	O
2.2	O
)	O
where	O
ˆf	O
represents	O
our	O
estimate	O
for	O
f	O
,	O
and	O
ˆy	O
represents	O
the	O
resulting	O
pre-	O
diction	O
for	O
y	O
.	O
in	O
this	O
setting	O
,	O
ˆf	O
is	O
often	O
treated	O
as	O
a	O
black	O
box	O
,	O
in	O
the	O
sense	O
that	O
one	O
is	O
not	O
typically	O
concerned	O
with	O
the	O
exact	O
form	O
of	O
ˆf	O
,	O
provided	O
that	O
it	O
yields	O
accurate	O
predictions	O
for	O
y	O
.	O
18	O
2.	O
statistical	O
learning	O
i	O
n	O
c	O
o	O
m	O
e	O
ye	O
ars	O
of	O
e	O
d	O
ucatio	O
n	O
seniority	O
figure	O
2.3.	O
the	O
plot	B
displays	O
income	O
as	O
a	O
function	B
of	O
years	O
of	O
education	O
and	O
seniority	O
in	O
the	O
income	O
data	B
set	O
.	O
the	O
blue	O
surface	O
represents	O
the	O
true	O
un-	O
derlying	O
relationship	O
between	O
income	O
and	O
years	O
of	O
education	O
and	O
seniority	O
,	O
which	O
is	O
known	O
since	O
the	O
data	B
are	O
simulated	O
.	O
the	O
red	O
dots	O
indicate	O
the	O
observed	O
values	O
of	O
these	O
quantities	O
for	O
30	O
individuals	O
.	O
as	O
an	O
example	O
,	O
suppose	O
that	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
are	O
characteristics	O
of	O
a	O
patient	O
’	O
s	O
blood	O
sample	O
that	O
can	O
be	O
easily	O
measured	O
in	O
a	O
lab	O
,	O
and	O
y	O
is	O
a	O
variable	B
encoding	O
the	O
patient	O
’	O
s	O
risk	O
for	O
a	O
severe	O
adverse	O
reaction	O
to	O
a	O
particular	O
drug	O
.	O
it	O
is	O
natural	B
to	O
seek	O
to	O
predict	O
y	O
using	O
x	O
,	O
since	O
we	O
can	O
then	O
avoid	O
giving	O
the	O
drug	O
in	O
question	O
to	O
patients	O
who	O
are	O
at	O
high	O
risk	O
of	O
an	O
adverse	O
reaction—that	O
is	O
,	O
patients	O
for	O
whom	O
the	O
estimate	O
of	O
y	O
is	O
high	O
.	O
the	O
accuracy	O
of	O
ˆy	O
as	O
a	O
prediction	B
for	O
y	O
depends	O
on	O
two	O
quantities	O
,	O
which	O
we	O
will	O
call	O
the	O
reducible	B
error	I
and	O
the	O
irreducible	B
error	I
.	O
in	O
general	O
,	O
ˆf	O
will	O
not	O
be	O
a	O
perfect	O
estimate	O
for	O
f	O
,	O
and	O
this	O
inaccuracy	O
will	O
introduce	O
some	O
error	B
.	O
this	O
error	B
is	O
reducible	B
because	O
we	O
can	O
potentially	O
improve	O
the	O
accuracy	O
of	O
ˆf	O
by	O
using	O
the	O
most	O
appropriate	O
statistical	O
learning	O
technique	O
to	O
estimate	O
f	O
.	O
however	O
,	O
even	O
if	O
it	O
were	O
possible	O
to	O
form	O
a	O
perfect	O
estimate	O
for	O
f	O
,	O
so	O
that	O
our	O
estimated	O
response	B
took	O
the	O
form	O
ˆy	O
=	O
f	O
(	O
x	O
)	O
,	O
our	O
prediction	B
would	O
still	O
have	O
some	O
error	B
in	O
it	O
!	O
this	O
is	O
because	O
y	O
is	O
also	O
a	O
function	B
of	O
	O
,	O
which	O
,	O
by	O
deﬁnition	O
,	O
can	O
not	O
be	O
predicted	O
using	O
x.	O
therefore	O
,	O
variability	O
associated	O
with	O
	O
also	O
aﬀects	O
the	O
accuracy	O
of	O
our	O
predictions	O
.	O
this	O
is	O
known	O
as	O
the	O
irreducible	B
error	I
,	O
because	O
no	O
matter	O
how	O
well	O
we	O
estimate	O
f	O
,	O
we	O
can	O
not	O
reduce	O
the	O
error	B
introduced	O
by	O
	O
.	O
why	O
is	O
the	O
irreducible	B
error	I
larger	O
than	O
zero	O
?	O
the	O
quantity	O
	O
may	O
con-	O
tain	O
unmeasured	O
variables	O
that	O
are	O
useful	O
in	O
predicting	O
y	O
:	O
since	O
we	O
don	O
’	O
t	O
measure	O
them	O
,	O
f	O
can	O
not	O
use	O
them	O
for	O
its	O
prediction	B
.	O
the	O
quantity	O
	O
may	O
also	O
contain	O
unmeasurable	O
variation	O
.	O
for	O
example	O
,	O
the	O
risk	O
of	O
an	O
adverse	O
reaction	O
might	O
vary	O
for	O
a	O
given	O
patient	O
on	O
a	O
given	O
day	O
,	O
depending	O
on	O
reducible	B
error	I
irreducible	O
error	B
2.1	O
what	O
is	O
statistical	O
learning	O
?	O
19	O
manufacturing	O
variation	O
in	O
the	O
drug	O
itself	O
or	O
the	O
patient	O
’	O
s	O
general	O
feeling	O
of	O
well-being	O
on	O
that	O
day	O
.	O
consider	O
a	O
given	O
estimate	O
ˆf	O
and	O
a	O
set	B
of	O
predictors	O
x	O
,	O
which	O
yields	O
the	O
prediction	B
ˆy	O
=	O
ˆf	O
(	O
x	O
)	O
.	O
assume	O
for	O
a	O
moment	O
that	O
both	O
ˆf	O
and	O
x	O
are	O
ﬁxed	O
.	O
then	O
,	O
it	O
is	O
easy	O
to	O
show	O
that	O
e	O
(	O
y	O
−	O
ˆy	O
)	O
2	O
=	O
e	O
[	O
f	O
(	O
x	O
)	O
+	O
	O
−	O
ˆf	O
(	O
x	O
)	O
]	O
2	O
=	O
[	O
f	O
(	O
x	O
)	O
−	O
ˆf	O
(	O
x	O
)	O
]	O
2	O
(	O
cid:16	O
)	O
(	O
cid:14	O
)	O
(	O
cid:15	O
)	O
(	O
cid:13	O
)	O
(	O
cid:13	O
)	O
(	O
cid:14	O
)	O
(	O
cid:15	O
)	O
(	O
cid:16	O
)	O
+	O
var	O
(	O
	O
)	O
,	O
(	O
2.3	O
)	O
expected	B
value	I
variance	O
reducible	B
irreducible	O
where	O
e	O
(	O
y	O
−	O
ˆy	O
)	O
2	O
represents	O
the	O
average	B
,	O
or	O
expected	B
value	I
,	O
of	O
the	O
squared	O
diﬀerence	O
between	O
the	O
predicted	O
and	O
actual	O
value	O
of	O
y	O
,	O
and	O
var	O
(	O
	O
)	O
repre-	O
sents	O
the	O
variance	B
associated	O
with	O
the	O
error	B
term	O
	O
.	O
the	O
focus	O
of	O
this	O
book	O
is	O
on	O
techniques	O
for	O
estimating	O
f	O
with	O
the	O
aim	O
of	O
minimizing	O
the	O
reducible	B
error	I
.	O
it	O
is	O
important	O
to	O
keep	O
in	O
mind	O
that	O
the	O
irreducible	B
error	I
will	O
always	O
provide	O
an	O
upper	O
bound	O
on	O
the	O
accuracy	O
of	O
our	O
prediction	B
for	O
y	O
.	O
this	O
bound	O
is	O
almost	O
always	O
unknown	O
in	O
practice	O
.	O
inference	B
we	O
are	O
often	O
interested	O
in	O
understanding	O
the	O
way	O
that	O
y	O
is	O
aﬀected	O
as	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
change	O
.	O
in	O
this	O
situation	O
we	O
wish	O
to	O
estimate	O
f	O
,	O
but	O
our	O
goal	O
is	O
not	O
necessarily	O
to	O
make	O
predictions	O
for	O
y	O
.	O
we	O
instead	O
want	O
to	O
understand	O
the	O
relationship	O
between	O
x	O
and	O
y	O
,	O
or	O
more	O
speciﬁcally	O
,	O
to	O
understand	O
how	O
y	O
changes	O
as	O
a	O
function	B
of	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
.	O
now	O
ˆf	O
can	O
not	O
be	O
treated	O
as	O
a	O
black	O
box	O
,	O
because	O
we	O
need	O
to	O
know	O
its	O
exact	O
form	O
.	O
in	O
this	O
setting	O
,	O
one	O
may	O
be	O
interested	O
in	O
answering	O
the	O
following	O
questions	O
:	O
•	O
which	O
predictors	O
are	O
associated	O
with	O
the	O
response	B
?	O
it	O
is	O
often	O
the	O
case	O
that	O
only	O
a	O
small	O
fraction	O
of	O
the	O
available	O
predictors	O
are	O
substantially	O
associated	O
with	O
y	O
.	O
identifying	O
the	O
few	O
important	O
predictors	O
among	O
a	O
large	O
set	B
of	O
possible	O
variables	O
can	O
be	O
extremely	O
useful	O
,	O
depending	O
on	O
the	O
application	O
.	O
•	O
what	O
is	O
the	O
relationship	O
between	O
the	O
response	B
and	O
each	O
predictor	B
?	O
some	O
predictors	O
may	O
have	O
a	O
positive	O
relationship	O
with	O
y	O
,	O
in	O
the	O
sense	O
that	O
increasing	O
the	O
predictor	B
is	O
associated	O
with	O
increasing	O
values	O
of	O
y	O
.	O
other	O
predictors	O
may	O
have	O
the	O
opposite	O
relationship	O
.	O
depending	O
on	O
the	O
complexity	O
of	O
f	O
,	O
the	O
relationship	O
between	O
the	O
response	B
and	O
a	O
given	O
predictor	B
may	O
also	O
depend	O
on	O
the	O
values	O
of	O
the	O
other	O
predictors	O
.	O
•	O
can	O
the	O
relationship	O
between	O
y	O
and	O
each	O
predictor	B
be	O
adequately	O
sum-	O
marized	O
using	O
a	O
linear	B
equation	O
,	O
or	O
is	O
the	O
relationship	O
more	O
compli-	O
cated	O
?	O
historically	O
,	O
most	O
methods	O
for	O
estimating	O
f	O
have	O
taken	O
a	O
linear	B
form	O
.	O
in	O
some	O
situations	O
,	O
such	O
an	O
assumption	O
is	O
reasonable	O
or	O
even	O
de-	O
sirable	O
.	O
but	O
often	O
the	O
true	O
relationship	O
is	O
more	O
complicated	O
,	O
in	O
which	O
case	O
a	O
linear	B
model	I
may	O
not	O
provide	O
an	O
accurate	O
representation	O
of	O
the	O
relationship	O
between	O
the	O
input	B
and	O
output	B
variables	O
.	O
20	O
2.	O
statistical	O
learning	O
in	O
this	O
book	O
,	O
we	O
will	O
see	O
a	O
number	O
of	O
examples	O
that	O
fall	O
into	O
the	O
prediction	B
setting	O
,	O
the	O
inference	B
setting	O
,	O
or	O
a	O
combination	O
of	O
the	O
two	O
.	O
for	O
instance	O
,	O
consider	O
a	O
company	O
that	O
is	O
interested	O
in	O
conducting	O
a	O
direct-marketing	O
campaign	O
.	O
the	O
goal	O
is	O
to	O
identify	O
individuals	O
who	O
will	O
respond	O
positively	O
to	O
a	O
mailing	O
,	O
based	O
on	O
observations	B
of	O
demographic	O
vari-	O
ables	O
measured	O
on	O
each	O
individual	O
.	O
in	O
this	O
case	O
,	O
the	O
demographic	O
variables	O
serve	O
as	O
predictors	O
,	O
and	O
response	B
to	O
the	O
marketing	O
campaign	O
(	O
either	O
pos-	O
itive	O
or	O
negative	O
)	O
serves	O
as	O
the	O
outcome	O
.	O
the	O
company	O
is	O
not	O
interested	O
in	O
obtaining	O
a	O
deep	O
understanding	O
of	O
the	O
relationships	O
between	O
each	O
in-	O
dividual	O
predictor	B
and	O
the	O
response	B
;	O
instead	O
,	O
the	O
company	O
simply	O
wants	O
an	O
accurate	O
model	B
to	O
predict	O
the	O
response	B
using	O
the	O
predictors	O
.	O
this	O
is	O
an	O
example	O
of	O
modeling	O
for	O
prediction	O
.	O
in	O
contrast	B
,	O
consider	O
the	O
advertising	O
data	B
illustrated	O
in	O
figure	O
2.1.	O
one	O
may	O
be	O
interested	O
in	O
answering	O
questions	O
such	O
as	O
:	O
–	O
which	O
media	O
contribute	O
to	O
sales	O
?	O
–	O
which	O
media	O
generate	O
the	O
biggest	O
boost	O
in	O
sales	O
?	O
or	O
–	O
how	O
much	O
increase	O
in	O
sales	O
is	O
associated	O
with	O
a	O
given	O
increase	O
in	O
tv	O
advertising	O
?	O
this	O
situation	O
falls	O
into	O
the	O
inference	B
paradigm	O
.	O
another	O
example	O
involves	O
modeling	O
the	O
brand	O
of	O
a	O
product	O
that	O
a	O
customer	O
might	O
purchase	O
based	O
on	O
variables	O
such	O
as	O
price	O
,	O
store	O
location	O
,	O
discount	O
levels	O
,	O
competition	O
price	O
,	O
and	O
so	O
forth	O
.	O
in	O
this	O
situation	O
one	O
might	O
really	O
be	O
most	O
interested	O
in	O
how	O
each	O
of	O
the	O
individual	O
variables	O
aﬀects	O
the	O
probability	B
of	O
purchase	O
.	O
for	O
instance	O
,	O
what	O
eﬀect	O
will	O
changing	O
the	O
price	O
of	O
a	O
product	O
have	O
on	O
sales	O
?	O
this	O
is	O
an	O
example	O
of	O
modeling	O
for	O
inference	O
.	O
finally	O
,	O
some	O
modeling	O
could	O
be	O
conducted	O
both	O
for	O
prediction	O
and	O
infer-	O
ence	O
.	O
for	O
example	O
,	O
in	O
a	O
real	O
estate	O
setting	O
,	O
one	O
may	O
seek	O
to	O
relate	O
values	O
of	O
homes	O
to	O
inputs	O
such	O
as	O
crime	O
rate	B
,	O
zoning	O
,	O
distance	B
from	O
a	O
river	O
,	O
air	O
qual-	O
ity	O
,	O
schools	O
,	O
income	O
level	B
of	O
community	O
,	O
size	O
of	O
houses	O
,	O
and	O
so	O
forth	O
.	O
in	O
this	O
case	O
one	O
might	O
be	O
interested	O
in	O
how	O
the	O
individual	O
input	B
variables	O
aﬀect	O
the	O
prices—that	O
is	O
,	O
how	O
much	O
extra	O
will	O
a	O
house	O
be	O
worth	O
if	O
it	O
has	O
a	O
view	O
of	O
the	O
river	O
?	O
this	O
is	O
an	O
inference	B
problem	O
.	O
alternatively	O
,	O
one	O
may	O
simply	O
be	O
interested	O
in	O
predicting	O
the	O
value	O
of	O
a	O
home	O
given	O
its	O
characteristics	O
:	O
is	O
this	O
house	O
under-	O
or	O
over-valued	O
?	O
this	O
is	O
a	O
prediction	B
problem	O
.	O
depending	O
on	O
whether	O
our	O
ultimate	O
goal	O
is	O
prediction	B
,	O
inference	B
,	O
or	O
a	O
combination	O
of	O
the	O
two	O
,	O
diﬀerent	O
methods	O
for	O
estimating	O
f	O
may	O
be	O
appro-	O
priate	O
.	O
for	O
example	O
,	O
linear	B
models	O
allow	O
for	O
relatively	O
simple	B
and	O
inter-	O
pretable	O
inference	B
,	O
but	O
may	O
not	O
yield	O
as	O
accurate	O
predictions	O
as	O
some	O
other	O
approaches	O
.	O
in	O
contrast	B
,	O
some	O
of	O
the	O
highly	O
non-linear	B
approaches	O
that	O
we	O
discuss	O
in	O
the	O
later	O
chapters	O
of	O
this	O
book	O
can	O
potentially	O
provide	O
quite	O
accu-	O
rate	B
predictions	O
for	O
y	O
,	O
but	O
this	O
comes	O
at	O
the	O
expense	O
of	O
a	O
less	O
interpretable	O
model	B
for	O
which	O
inference	B
is	O
more	O
challenging	O
.	O
linear	B
model	I
2.1	O
what	O
is	O
statistical	O
learning	O
?	O
21	O
2.1.2	O
how	O
do	O
we	O
estimate	O
f	O
?	O
throughout	O
this	O
book	O
,	O
we	O
explore	O
many	O
linear	B
and	O
non-linear	B
approaches	O
for	O
estimating	O
f	O
.	O
however	O
,	O
these	O
methods	O
generally	O
share	O
certain	O
charac-	O
teristics	O
.	O
we	O
provide	O
an	O
overview	O
of	O
these	O
shared	O
characteristics	O
in	O
this	O
section	O
.	O
we	O
will	O
always	O
assume	O
that	O
we	O
have	O
observed	O
a	O
set	B
of	O
n	O
diﬀerent	O
data	B
points	O
.	O
for	O
example	O
in	O
figure	O
2.2	O
we	O
observed	O
n	O
=	O
30	O
data	B
points	O
.	O
these	O
observations	B
are	O
called	O
the	O
training	B
data	O
because	O
we	O
will	O
use	O
these	O
observations	B
to	O
train	B
,	O
or	O
teach	O
,	O
our	O
method	O
how	O
to	O
estimate	O
f	O
.	O
let	O
xij	O
represent	O
the	O
value	O
of	O
the	O
jth	O
predictor	B
,	O
or	O
input	B
,	O
for	O
observation	O
i	O
,	O
where	O
i	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
and	O
j	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
p.	O
correspondingly	O
,	O
let	O
yi	O
represent	O
the	O
response	B
variable	O
for	O
the	O
ith	O
observation	O
.	O
then	O
our	O
training	B
data	O
consist	O
of	O
{	O
(	O
x1	O
,	O
y1	O
)	O
,	O
(	O
x2	B
,	O
y2	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
}	O
where	O
xi	O
=	O
(	O
xi1	O
,	O
xi2	O
,	O
.	O
.	O
.	O
,	O
xip	O
)	O
t	O
.	O
our	O
goal	O
is	O
to	O
apply	O
a	O
statistical	O
learning	O
method	O
to	O
the	O
training	B
data	O
in	O
order	O
to	O
estimate	O
the	O
unknown	O
function	B
f	O
.	O
in	O
other	O
words	O
,	O
we	O
want	O
to	O
ﬁnd	O
a	O
function	B
ˆf	O
such	O
that	O
y	O
≈	O
ˆf	O
(	O
x	O
)	O
for	O
any	O
observation	O
(	O
x	O
,	O
y	O
)	O
.	O
broadly	O
speaking	O
,	O
most	O
statistical	O
learning	O
methods	O
for	O
this	O
task	O
can	O
be	O
character-	O
ized	O
as	O
either	O
parametric	B
or	O
non-parametric	B
.	O
we	O
now	O
brieﬂy	O
discuss	O
these	O
two	O
types	O
of	O
approaches	O
.	O
parametric	B
methods	O
parametric	B
methods	O
involve	O
a	O
two-step	O
model-based	O
approach	B
.	O
1.	O
first	O
,	O
we	O
make	O
an	O
assumption	O
about	O
the	O
functional	O
form	O
,	O
or	O
shape	O
,	O
of	O
f	O
.	O
for	O
example	O
,	O
one	O
very	O
simple	B
assumption	O
is	O
that	O
f	O
is	O
linear	B
in	O
x	O
:	O
f	O
(	O
x	O
)	O
=	O
β0	O
+	O
β1x1	O
+	O
β2x2	O
+	O
.	O
.	O
.	O
+	O
βpxp	O
.	O
(	O
2.4	O
)	O
this	O
is	O
a	O
linear	B
model	I
,	O
which	O
will	O
be	O
discussed	O
extensively	O
in	O
chap-	O
ter	O
3.	O
once	O
we	O
have	O
assumed	O
that	O
f	O
is	O
linear	B
,	O
the	O
problem	O
of	O
estimat-	O
ing	O
f	O
is	O
greatly	O
simpliﬁed	O
.	O
instead	O
of	O
having	O
to	O
estimate	O
an	O
entirely	O
arbitrary	O
p-dimensional	O
function	B
f	O
(	O
x	O
)	O
,	O
one	O
only	O
needs	O
to	O
estimate	O
the	O
p	O
+	O
1	O
coeﬃcients	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
.	O
training	B
data	O
parametric	B
non-	O
parametric	B
2.	O
after	O
a	O
model	B
has	O
been	O
selected	O
,	O
we	O
need	O
a	O
procedure	O
that	O
uses	O
the	O
training	B
data	O
to	O
ﬁt	B
or	O
train	B
the	O
model	B
.	O
in	O
the	O
case	O
of	O
the	O
linear	B
model	I
(	O
2.4	O
)	O
,	O
we	O
need	O
to	O
estimate	O
the	O
parameters	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
.	O
that	O
is	O
,	O
we	O
want	O
to	O
ﬁnd	O
values	O
of	O
these	O
parameters	O
such	O
that	O
y	O
≈	O
β0	O
+	O
β1x1	O
+	O
β2x2	O
+	O
.	O
.	O
.	O
+	O
βpxp	O
.	O
ﬁt	B
train	O
the	O
most	O
common	O
approach	B
to	O
ﬁtting	O
the	O
model	B
(	O
2.4	O
)	O
is	O
referred	O
to	O
as	O
(	O
ordinary	O
)	O
least	B
squares	I
,	O
which	O
we	O
discuss	O
in	O
chapter	O
3.	O
however	O
,	O
least	B
squares	I
is	O
one	O
of	O
many	O
possible	O
ways	O
to	O
ﬁt	B
the	O
linear	B
model	I
.	O
in	O
chapter	O
6	O
,	O
we	O
discuss	O
other	O
approaches	O
for	O
estimating	O
the	O
parameters	O
in	O
(	O
2.4	O
)	O
.	O
least	B
squares	I
the	O
model-based	O
approach	B
just	O
described	O
is	O
referred	O
to	O
as	O
parametric	B
;	O
it	O
reduces	O
the	O
problem	O
of	O
estimating	O
f	O
down	O
to	O
one	O
of	O
estimating	O
a	O
set	B
of	O
22	O
2.	O
statistical	O
learning	O
i	O
n	O
c	O
o	O
m	O
e	O
ye	O
ars	O
of	O
e	O
d	O
ucatio	O
n	O
seniority	O
figure	O
2.4.	O
a	O
linear	B
model	I
ﬁt	O
by	O
least	B
squares	I
to	O
the	O
income	O
data	B
from	O
fig-	O
ure	O
2.3.	O
the	O
observations	B
are	O
shown	O
in	O
red	O
,	O
and	O
the	O
yellow	O
plane	O
indicates	O
the	O
least	B
squares	I
ﬁt	O
to	O
the	O
data	B
.	O
parameters	O
.	O
assuming	O
a	O
parametric	B
form	O
for	O
f	O
simpliﬁes	O
the	O
problem	O
of	O
estimating	O
f	O
because	O
it	O
is	O
generally	O
much	O
easier	O
to	O
estimate	O
a	O
set	B
of	O
pa-	O
rameters	O
,	O
such	O
as	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
in	O
the	O
linear	B
model	I
(	O
2.4	O
)	O
,	O
than	O
it	O
is	O
to	O
ﬁt	B
an	O
entirely	O
arbitrary	O
function	B
f	O
.	O
the	O
potential	O
disadvantage	O
of	O
a	O
paramet-	O
ric	O
approach	B
is	O
that	O
the	O
model	B
we	O
choose	O
will	O
usually	O
not	O
match	O
the	O
true	O
unknown	O
form	O
of	O
f	O
.	O
if	O
the	O
chosen	O
model	B
is	O
too	O
far	O
from	O
the	O
true	O
f	O
,	O
then	O
our	O
estimate	O
will	O
be	O
poor	O
.	O
we	O
can	O
try	O
to	O
address	O
this	O
problem	O
by	O
choos-	O
ing	O
ﬂexible	B
models	O
that	O
can	O
ﬁt	B
many	O
diﬀerent	O
possible	O
functional	O
forms	O
for	O
f	O
.	O
but	O
in	O
general	O
,	O
ﬁtting	O
a	O
more	O
ﬂexible	B
model	O
requires	O
estimating	O
a	O
greater	O
number	O
of	O
parameters	O
.	O
these	O
more	O
complex	O
models	O
can	O
lead	O
to	O
a	O
phenomenon	O
known	O
as	O
overﬁtting	B
the	O
data	B
,	O
which	O
essentially	O
means	O
they	O
follow	O
the	O
errors	O
,	O
or	O
noise	B
,	O
too	O
closely	O
.	O
these	O
issues	O
are	O
discussed	O
through-	O
out	O
this	O
book	O
.	O
figure	O
2.4	O
shows	O
an	O
example	O
of	O
the	O
parametric	B
approach	O
applied	O
to	O
the	O
income	O
data	B
from	O
figure	O
2.3.	O
we	O
have	O
ﬁt	B
a	O
linear	B
model	I
of	O
the	O
form	O
income	O
≈	O
β0	O
+	O
β1	O
×	O
education	O
+	O
β2	O
×	O
seniority	O
.	O
since	O
we	O
have	O
assumed	O
a	O
linear	B
relationship	O
between	O
the	O
response	B
and	O
the	O
two	O
predictors	O
,	O
the	O
entire	O
ﬁtting	O
problem	O
reduces	O
to	O
estimating	O
β0	O
,	O
β1	O
,	O
and	O
β2	O
,	O
which	O
we	O
do	O
using	O
least	B
squares	I
linear	O
regression	B
.	O
comparing	O
figure	O
2.3	O
to	O
figure	O
2.4	O
,	O
we	O
can	O
see	O
that	O
the	O
linear	B
ﬁt	O
given	O
in	O
figure	O
2.4	O
is	O
not	O
quite	O
right	O
:	O
the	O
true	O
f	O
has	O
some	O
curvature	O
that	O
is	O
not	O
captured	O
in	O
the	O
linear	B
ﬁt	O
.	O
however	O
,	O
the	O
linear	B
ﬁt	O
still	O
appears	O
to	O
do	O
a	O
reasonable	O
job	O
of	O
capturing	O
the	O
positive	O
relationship	O
between	O
years	O
of	O
education	O
and	O
income	O
,	O
as	O
well	O
as	O
the	O
ﬂexible	B
overﬁtting	O
noise	B
2.1	O
what	O
is	O
statistical	O
learning	O
?	O
23	O
seniority	O
i	O
n	O
c	O
o	O
m	O
e	O
ars	O
of	O
e	O
d	O
ye	O
ucatio	O
n	O
figure	O
2.5.	O
a	O
smooth	O
thin-plate	B
spline	O
ﬁt	B
to	O
the	O
income	O
data	B
from	O
figure	O
2.3	O
is	O
shown	O
in	O
yellow	O
;	O
the	O
observations	B
are	O
displayed	O
in	O
red	O
.	O
splines	O
are	O
discussed	O
in	O
chapter	O
7.	O
slightly	O
less	O
positive	O
relationship	O
between	O
seniority	O
and	O
income	O
.	O
it	O
may	O
be	O
that	O
with	O
such	O
a	O
small	O
number	O
of	O
observations	B
,	O
this	O
is	O
the	O
best	O
we	O
can	O
do	O
.	O
non-parametric	B
methods	O
non-parametric	B
methods	O
do	O
not	O
make	O
explicit	O
assumptions	O
about	O
the	O
func-	O
tional	O
form	O
of	O
f	O
.	O
instead	O
they	O
seek	O
an	O
estimate	O
of	O
f	O
that	O
gets	O
as	O
close	O
to	O
the	O
data	B
points	O
as	O
possible	O
without	O
being	O
too	O
rough	O
or	O
wiggly	O
.	O
such	O
approaches	O
can	O
have	O
a	O
major	O
advantage	O
over	O
parametric	B
approaches	O
:	O
by	O
avoiding	O
the	O
assumption	O
of	O
a	O
particular	O
functional	O
form	O
for	O
f	O
,	O
they	O
have	O
the	O
potential	O
to	O
accurately	O
ﬁt	B
a	O
wider	O
range	O
of	O
possible	O
shapes	O
for	O
f	O
.	O
any	O
parametric	B
approach	O
brings	O
with	O
it	O
the	O
possibility	O
that	O
the	O
functional	O
form	O
used	O
to	O
estimate	O
f	O
is	O
very	O
diﬀerent	O
from	O
the	O
true	O
f	O
,	O
in	O
which	O
case	O
the	O
resulting	O
model	B
will	O
not	O
ﬁt	B
the	O
data	B
well	O
.	O
in	O
contrast	B
,	O
non-parametric	B
approaches	O
completely	O
avoid	O
this	O
danger	O
,	O
since	O
essentially	O
no	O
assumption	O
about	O
the	O
form	O
of	O
f	O
is	O
made	O
.	O
but	O
non-parametric	B
approaches	O
do	O
suﬀer	O
from	O
a	O
major	O
disadvantage	O
:	O
since	O
they	O
do	O
not	O
reduce	O
the	O
problem	O
of	O
estimating	O
f	O
to	O
a	O
small	O
number	O
of	O
parameters	O
,	O
a	O
very	O
large	O
number	O
of	O
observations	B
(	O
far	O
more	O
than	O
is	O
typically	O
needed	O
for	O
a	O
parametric	B
approach	O
)	O
is	O
required	O
in	O
order	O
to	O
obtain	O
an	O
accurate	O
estimate	O
for	O
f	O
.	O
an	O
example	O
of	O
a	O
non-parametric	B
approach	O
to	O
ﬁtting	O
the	O
income	O
data	B
is	O
shown	O
in	O
figure	O
2.5.	O
a	O
thin-plate	B
spline	O
is	O
used	O
to	O
estimate	O
f	O
.	O
this	O
ap-	O
proach	O
does	O
not	O
impose	O
any	O
pre-speciﬁed	O
model	B
on	O
f	O
.	O
it	O
instead	O
attempts	O
to	O
produce	O
an	O
estimate	O
for	O
f	O
that	O
is	O
as	O
close	O
as	O
possible	O
to	O
the	O
observed	O
data	B
,	O
subject	O
to	O
the	O
ﬁt—that	O
is	O
,	O
the	O
yellow	O
surface	O
in	O
figure	O
2.5—being	O
thin-plate	B
spline	O
24	O
2.	O
statistical	O
learning	O
i	O
n	O
c	O
o	O
m	O
e	O
ye	O
ars	O
of	O
e	O
d	O
ucatio	O
n	O
seniority	O
figure	O
2.6.	O
a	O
rough	O
thin-plate	B
spline	O
ﬁt	B
to	O
the	O
income	O
data	B
from	O
figure	O
2.3.	O
this	O
ﬁt	B
makes	O
zero	O
errors	O
on	O
the	O
training	B
data	O
.	O
smooth	O
.	O
in	O
this	O
case	O
,	O
the	O
non-parametric	B
ﬁt	O
has	O
produced	O
a	O
remarkably	O
ac-	O
curate	O
estimate	O
of	O
the	O
true	O
f	O
shown	O
in	O
figure	O
2.3.	O
in	O
order	O
to	O
ﬁt	B
a	O
thin-plate	B
spline	O
,	O
the	O
data	B
analyst	O
must	O
select	O
a	O
level	B
of	O
smoothness	O
.	O
figure	O
2.6	O
shows	O
the	O
same	O
thin-plate	B
spline	O
ﬁt	B
using	O
a	O
lower	O
level	B
of	O
smoothness	O
,	O
allowing	O
for	O
a	O
rougher	O
ﬁt	B
.	O
the	O
resulting	O
estimate	O
ﬁts	O
the	O
observed	O
data	B
perfectly	O
!	O
however	O
,	O
the	O
spline	B
ﬁt	O
shown	O
in	O
figure	O
2.6	O
is	O
far	O
more	O
variable	B
than	O
the	O
true	O
function	O
f	O
,	O
from	O
figure	O
2.3.	O
this	O
is	O
an	O
example	O
of	O
overﬁtting	B
the	O
data	B
,	O
which	O
we	O
discussed	O
previously	O
.	O
it	O
is	O
an	O
undesirable	O
situation	O
because	O
the	O
ﬁt	B
obtained	O
will	O
not	O
yield	O
accurate	O
estimates	O
of	O
the	O
response	B
on	O
new	O
observations	B
that	O
were	O
not	O
part	O
of	O
the	O
original	O
training	B
data	O
set	B
.	O
we	O
dis-	O
cuss	O
methods	O
for	O
choosing	O
the	O
correct	O
amount	O
of	O
smoothness	O
in	O
chapter	O
5.	O
splines	O
are	O
discussed	O
in	O
chapter	O
7.	O
as	O
we	O
have	O
seen	O
,	O
there	O
are	O
advantages	O
and	O
disadvantages	O
to	O
parametric	B
and	O
non-parametric	B
methods	O
for	O
statistical	O
learning	O
.	O
we	O
explore	O
both	O
types	O
of	O
methods	O
throughout	O
this	O
book	O
.	O
2.1.3	O
the	O
trade-oﬀ	B
between	O
prediction	B
accuracy	O
and	O
model	B
interpretability	O
of	O
the	O
many	O
methods	O
that	O
we	O
examine	O
in	O
this	O
book	O
,	O
some	O
are	O
less	O
ﬂexible	B
,	O
or	O
more	O
restrictive	O
,	O
in	O
the	O
sense	O
that	O
they	O
can	O
produce	O
just	O
a	O
relatively	O
small	O
range	O
of	O
shapes	O
to	O
estimate	O
f	O
.	O
for	O
example	O
,	O
linear	B
regression	I
is	O
a	O
relatively	O
inﬂexible	O
approach	B
,	O
because	O
it	O
can	O
only	O
generate	O
linear	B
functions	O
such	O
as	O
the	O
lines	O
shown	O
in	O
figure	O
2.1	O
or	O
the	O
plane	O
shown	O
in	O
figure	O
2.4	O
.	O
2.1	O
what	O
is	O
statistical	O
learning	O
?	O
25	O
h	O
g	O
h	O
i	O
subset	B
selection	I
lasso	O
y	O
t	O
i	O
l	O
i	O
b	O
a	O
t	O
e	O
r	O
p	O
r	O
e	O
t	O
n	O
i	O
w	O
o	O
l	O
low	O
least	B
squares	I
generalized	O
additive	B
models	O
trees	O
bagging	B
,	O
boosting	B
support	O
vector	B
machines	O
flexibility	O
high	O
figure	O
2.7.	O
a	O
representation	O
of	O
the	O
tradeoﬀ	O
between	O
ﬂexibility	O
and	O
inter-	O
pretability	O
,	O
using	O
diﬀerent	O
statistical	O
learning	O
methods	O
.	O
in	O
general	O
,	O
as	O
the	O
ﬂexibil-	O
ity	O
of	O
a	O
method	O
increases	O
,	O
its	O
interpretability	B
decreases	O
.	O
other	O
methods	O
,	O
such	O
as	O
the	O
thin	O
plate	O
splines	O
shown	O
in	O
figures	O
2.5	O
and	O
2.6	O
,	O
are	O
considerably	O
more	O
ﬂexible	B
because	O
they	O
can	O
generate	O
a	O
much	O
wider	O
range	O
of	O
possible	O
shapes	O
to	O
estimate	O
f	O
.	O
one	O
might	O
reasonably	O
ask	O
the	O
following	O
question	O
:	O
why	O
would	O
we	O
ever	O
choose	O
to	O
use	O
a	O
more	O
restrictive	O
method	O
instead	O
of	O
a	O
very	O
ﬂexible	B
approach	O
?	O
there	O
are	O
several	O
reasons	O
that	O
we	O
might	O
prefer	O
a	O
more	O
restrictive	O
model	B
.	O
if	O
we	O
are	O
mainly	O
interested	O
in	O
inference	B
,	O
then	O
restrictive	O
models	O
are	O
much	O
more	O
interpretable	O
.	O
for	O
instance	O
,	O
when	O
inference	B
is	O
the	O
goal	O
,	O
the	O
linear	B
model	I
may	O
be	O
a	O
good	O
choice	O
since	O
it	O
will	O
be	O
quite	O
easy	O
to	O
understand	O
the	O
relationship	O
between	O
y	O
and	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
.	O
in	O
contrast	B
,	O
very	O
ﬂexible	B
approaches	O
,	O
such	O
as	O
the	O
splines	O
discussed	O
in	O
chapter	O
7	O
and	O
displayed	O
in	O
figures	O
2.5	O
and	O
2.6	O
,	O
and	O
the	O
boosting	B
methods	O
discussed	O
in	O
chapter	O
8	O
,	O
can	O
lead	O
to	O
such	O
complicated	O
estimates	O
of	O
f	O
that	O
it	O
is	O
diﬃcult	O
to	O
understand	O
how	O
any	O
individual	O
predictor	B
is	O
associated	O
with	O
the	O
response	B
.	O
figure	O
2.7	O
provides	O
an	O
illustration	O
of	O
the	O
trade-oﬀ	B
between	O
ﬂexibility	O
and	O
interpretability	B
for	O
some	O
of	O
the	O
methods	O
that	O
we	O
cover	O
in	O
this	O
book	O
.	O
least	B
squares	I
linear	O
regression	B
,	O
discussed	O
in	O
chapter	O
3	O
,	O
is	O
relatively	O
inﬂexible	O
but	O
is	O
quite	O
interpretable	O
.	O
the	O
lasso	B
,	O
discussed	O
in	O
chapter	O
6	O
,	O
relies	O
upon	O
the	O
linear	B
model	I
(	O
2.4	O
)	O
but	O
uses	O
an	O
alternative	O
ﬁtting	O
procedure	O
for	O
estimating	O
the	O
coeﬃcients	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
.	O
the	O
new	O
procedure	O
is	O
more	O
restrictive	O
in	O
es-	O
timating	O
the	O
coeﬃcients	O
,	O
and	O
sets	O
a	O
number	O
of	O
them	O
to	O
exactly	O
zero	O
.	O
hence	O
in	O
this	O
sense	O
the	O
lasso	B
is	O
a	O
less	O
ﬂexible	B
approach	O
than	O
linear	B
regression	I
.	O
it	O
is	O
also	O
more	O
interpretable	O
than	O
linear	B
regression	I
,	O
because	O
in	O
the	O
ﬁnal	O
model	B
the	O
response	B
variable	O
will	O
only	O
be	O
related	O
to	O
a	O
small	O
subset	O
of	O
the	O
predictors—namely	O
,	O
those	O
with	O
nonzero	O
coeﬃcient	B
estimates	O
.	O
generalized	O
lasso	O
generalized	B
additive	I
model	I
bagging	O
boosting	B
support	O
vector	B
machine	O
26	O
2.	O
statistical	O
learning	O
additive	B
models	O
(	O
gams	O
)	O
,	O
discussed	O
in	O
chapter	O
7	O
,	O
instead	O
extend	O
the	O
lin-	O
ear	O
model	B
(	O
2.4	O
)	O
to	O
allow	O
for	O
certain	O
non-linear	B
relationships	O
.	O
consequently	O
,	O
gams	O
are	O
more	O
ﬂexible	B
than	O
linear	B
regression	I
.	O
they	O
are	O
also	O
somewhat	O
less	O
interpretable	O
than	O
linear	B
regression	I
,	O
because	O
the	O
relationship	O
between	O
each	O
predictor	B
and	O
the	O
response	B
is	O
now	O
modeled	O
using	O
a	O
curve	O
.	O
finally	O
,	O
fully	O
non-linear	B
methods	O
such	O
as	O
bagging	B
,	O
boosting	B
,	O
and	O
support	B
vector	I
machines	O
with	O
non-linear	B
kernels	O
,	O
discussed	O
in	O
chapters	O
8	O
and	O
9	O
,	O
are	O
highly	O
ﬂexible	B
approaches	O
that	O
are	O
harder	O
to	O
interpret	O
.	O
we	O
have	O
established	O
that	O
when	O
inference	B
is	O
the	O
goal	O
,	O
there	O
are	O
clear	O
ad-	O
vantages	O
to	O
using	O
simple	B
and	O
relatively	O
inﬂexible	O
statistical	O
learning	O
meth-	O
ods	O
.	O
in	O
some	O
settings	O
,	O
however	O
,	O
we	O
are	O
only	O
interested	O
in	O
prediction	B
,	O
and	O
the	O
interpretability	B
of	O
the	O
predictive	O
model	B
is	O
simply	O
not	O
of	O
interest	O
.	O
for	O
instance	O
,	O
if	O
we	O
seek	O
to	O
develop	O
an	O
algorithm	O
to	O
predict	O
the	O
price	O
of	O
a	O
stock	O
,	O
our	O
sole	O
requirement	O
for	O
the	O
algorithm	O
is	O
that	O
it	O
predict	O
accurately—	O
interpretability	B
is	O
not	O
a	O
concern	O
.	O
in	O
this	O
setting	O
,	O
we	O
might	O
expect	O
that	O
it	O
will	O
be	O
best	O
to	O
use	O
the	O
most	O
ﬂexible	B
model	O
available	O
.	O
surprisingly	O
,	O
this	O
is	O
not	O
always	O
the	O
case	O
!	O
we	O
will	O
often	O
obtain	O
more	O
accurate	O
predictions	O
using	O
a	O
less	O
ﬂexible	B
method	O
.	O
this	O
phenomenon	O
,	O
which	O
may	O
seem	O
counterintuitive	O
at	O
ﬁrst	O
glance	O
,	O
has	O
to	O
do	O
with	O
the	O
potential	O
for	O
overﬁtting	O
in	O
highly	O
ﬂexible	B
methods	O
.	O
we	O
saw	O
an	O
example	O
of	O
overﬁtting	B
in	O
figure	O
2.6.	O
we	O
will	O
discuss	O
this	O
very	O
important	O
concept	O
further	O
in	O
section	O
2.2	O
and	O
throughout	O
this	O
book	O
.	O
2.1.4	O
supervised	O
versus	O
unsupervised	B
learning	I
most	O
statistical	O
learning	O
problems	O
fall	O
into	O
one	O
of	O
two	O
categories	O
:	O
supervised	O
or	O
unsupervised	O
.	O
the	O
examples	O
that	O
we	O
have	O
discussed	O
so	O
far	O
in	O
this	O
chap-	O
ter	O
all	O
fall	O
into	O
the	O
supervised	B
learning	I
domain	O
.	O
for	O
each	O
observation	O
of	O
the	O
predictor	B
measurement	O
(	O
s	O
)	O
xi	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
there	O
is	O
an	O
associated	O
response	B
measurement	O
yi	O
.	O
we	O
wish	O
to	O
ﬁt	B
a	O
model	B
that	O
relates	O
the	O
response	B
to	O
the	O
predictors	O
,	O
with	O
the	O
aim	O
of	O
accurately	O
predicting	O
the	O
response	B
for	O
future	O
observations	B
(	O
prediction	B
)	O
or	O
better	O
understanding	O
the	O
relationship	O
between	O
the	O
response	B
and	O
the	O
predictors	O
(	O
inference	B
)	O
.	O
many	O
classical	O
statistical	O
learn-	O
ing	O
methods	O
such	O
as	O
linear	B
regression	I
and	O
logistic	B
regression	I
(	O
chapter	O
4	O
)	O
,	O
as	O
well	O
as	O
more	O
modern	O
approaches	O
such	O
as	O
gam	O
,	O
boosting	B
,	O
and	O
support	O
vec-	O
tor	O
machines	O
,	O
operate	O
in	O
the	O
supervised	B
learning	I
domain	O
.	O
the	O
vast	O
majority	O
of	O
this	O
book	O
is	O
devoted	O
to	O
this	O
setting	O
.	O
in	O
contrast	B
,	O
unsupervised	B
learning	I
describes	O
the	O
somewhat	O
more	O
chal-	O
lenging	O
situation	O
in	O
which	O
for	O
every	O
observation	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
we	O
observe	O
a	O
vector	B
of	O
measurements	O
xi	O
but	O
no	O
associated	O
response	B
yi	O
.	O
it	O
is	O
not	O
pos-	O
sible	O
to	O
ﬁt	B
a	O
linear	B
regression	I
model	O
,	O
since	O
there	O
is	O
no	O
response	B
variable	O
to	O
predict	O
.	O
in	O
this	O
setting	O
,	O
we	O
are	O
in	O
some	O
sense	O
working	O
blind	O
;	O
the	O
sit-	O
uation	O
is	O
referred	O
to	O
as	O
unsupervised	O
because	O
we	O
lack	O
a	O
response	B
vari-	O
able	O
that	O
can	O
supervise	O
our	O
analysis	B
.	O
what	O
sort	O
of	O
statistical	O
analysis	O
is	O
supervised	O
unsupervised	O
logistic	B
regression	I
2.1	O
what	O
is	O
statistical	O
learning	O
?	O
27	O
2	O
1	O
0	O
1	O
8	O
6	O
4	O
2	O
8	O
6	O
4	O
2	O
0	O
2	O
4	O
6	O
8	O
10	O
12	O
0	O
2	O
4	O
6	O
figure	O
2.8.	O
a	O
clustering	B
data	O
set	B
involving	O
three	O
groups	O
.	O
each	O
group	O
is	O
shown	O
using	O
a	O
diﬀerent	O
colored	O
symbol	O
.	O
left	O
:	O
the	O
three	O
groups	O
are	O
well-separated	O
.	O
in	O
this	O
setting	O
,	O
a	O
clustering	B
approach	O
should	O
successfully	O
identify	O
the	O
three	O
groups	O
.	O
right	O
:	O
there	O
is	O
some	O
overlap	O
among	O
the	O
groups	O
.	O
now	O
the	O
clustering	B
task	O
is	O
more	O
challenging	O
.	O
possible	O
?	O
we	O
can	O
seek	O
to	O
understand	O
the	O
relationships	O
between	O
the	O
variables	O
or	O
between	O
the	O
observations	B
.	O
one	O
statistical	O
learning	O
tool	O
that	O
we	O
may	O
use	O
in	O
this	O
setting	O
is	O
cluster	B
analysis	I
,	O
or	O
clustering	B
.	O
the	O
goal	O
of	O
cluster	B
analysis	I
is	O
to	O
ascertain	O
,	O
on	O
the	O
basis	B
of	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
whether	O
the	O
observations	B
fall	O
into	O
relatively	O
distinct	O
groups	O
.	O
for	O
example	O
,	O
in	O
a	O
market	O
segmentation	O
study	O
we	O
might	O
observe	O
multiple	B
characteristics	O
(	O
variables	O
)	O
for	O
potential	O
customers	O
,	O
such	O
as	O
zip	O
code	O
,	O
family	O
income	O
,	O
and	O
shopping	O
habits	O
.	O
we	O
might	O
believe	O
that	O
the	O
customers	O
fall	O
into	O
diﬀerent	O
groups	O
,	O
such	O
as	O
big	O
spenders	O
versus	O
low	O
spenders	O
.	O
if	O
the	O
information	O
about	O
each	O
customer	O
’	O
s	O
spending	O
patterns	O
were	O
available	O
,	O
then	O
a	O
supervised	O
analysis	O
would	O
be	O
possible	O
.	O
however	O
,	O
this	O
information	O
is	O
not	O
available—that	O
is	O
,	O
we	O
do	O
not	O
know	O
whether	O
each	O
poten-	O
tial	O
customer	O
is	O
a	O
big	O
spender	O
or	O
not	O
.	O
in	O
this	O
setting	O
,	O
we	O
can	O
try	O
to	O
cluster	O
the	O
customers	O
on	O
the	O
basis	B
of	O
the	O
variables	O
measured	O
,	O
in	O
order	O
to	O
identify	O
distinct	O
groups	O
of	O
potential	O
customers	O
.	O
identifying	O
such	O
groups	O
can	O
be	O
of	O
interest	O
because	O
it	O
might	O
be	O
that	O
the	O
groups	O
diﬀer	O
with	O
respect	O
to	O
some	O
property	O
of	O
interest	O
,	O
such	O
as	O
spending	O
habits	O
.	O
figure	O
2.8	O
provides	O
a	O
simple	B
illustration	O
of	O
the	O
clustering	B
problem	O
.	O
we	O
have	O
plotted	O
150	O
observations	B
with	O
measurements	O
on	O
two	O
variables	O
,	O
x1	O
and	O
x2	B
.	O
each	O
observation	O
corresponds	O
to	O
one	O
of	O
three	O
distinct	O
groups	O
.	O
for	O
illustrative	O
purposes	O
,	O
we	O
have	O
plotted	O
the	O
members	O
of	O
each	O
group	O
using	O
diﬀerent	O
colors	O
and	O
symbols	O
.	O
however	O
,	O
in	O
practice	O
the	O
group	O
memberships	O
are	O
unknown	O
,	O
and	O
the	O
goal	O
is	O
to	O
determine	O
the	O
group	O
to	O
which	O
each	O
ob-	O
servation	O
belongs	O
.	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
2.8	O
,	O
this	O
is	O
a	O
relatively	O
easy	O
task	O
because	O
the	O
groups	O
are	O
well-separated	O
.	O
in	O
contrast	B
,	O
the	O
right-hand	O
panel	O
illustrates	O
a	O
more	O
challenging	O
problem	O
in	O
which	O
there	O
is	O
some	O
overlap	O
cluster	B
analysis	I
28	O
2.	O
statistical	O
learning	O
between	O
the	O
groups	O
.	O
a	O
clustering	B
method	O
could	O
not	O
be	O
expected	O
to	O
assign	O
all	O
of	O
the	O
overlapping	O
points	O
to	O
their	O
correct	O
group	O
(	O
blue	O
,	O
green	O
,	O
or	O
orange	O
)	O
.	O
in	O
the	O
examples	O
shown	O
in	O
figure	O
2.8	O
,	O
there	O
are	O
only	O
two	O
variables	O
,	O
and	O
so	O
one	O
can	O
simply	O
visually	O
inspect	O
the	O
scatterplots	O
of	O
the	O
observations	B
in	O
order	O
to	O
identify	O
clusters	O
.	O
however	O
,	O
in	O
practice	O
,	O
we	O
often	O
encounter	O
data	B
sets	O
that	O
contain	O
many	O
more	O
than	O
two	O
variables	O
.	O
in	O
this	O
case	O
,	O
we	O
can	O
not	O
easily	O
plot	B
the	O
observations	B
.	O
for	O
instance	O
,	O
if	O
there	O
are	O
p	O
variables	O
in	O
our	O
data	B
set	O
,	O
then	O
p	O
(	O
p	O
−	O
1	O
)	O
/2	O
distinct	O
scatterplots	O
can	O
be	O
made	O
,	O
and	O
visual	O
inspection	O
is	O
simply	O
not	O
a	O
viable	O
way	O
to	O
identify	O
clusters	O
.	O
for	O
this	O
reason	O
,	O
automated	O
clustering	B
methods	O
are	O
important	O
.	O
we	O
discuss	O
clustering	B
and	O
other	O
unsupervised	B
learning	I
approaches	O
in	O
chapter	O
10.	O
many	O
problems	O
fall	O
naturally	O
into	O
the	O
supervised	O
or	O
unsupervised	O
learn-	O
ing	O
paradigms	O
.	O
however	O
,	O
sometimes	O
the	O
question	O
of	O
whether	O
an	O
analysis	B
should	O
be	O
considered	O
supervised	O
or	O
unsupervised	O
is	O
less	O
clear-cut	O
.	O
for	O
in-	O
stance	O
,	O
suppose	O
that	O
we	O
have	O
a	O
set	B
of	O
n	O
observations	B
.	O
for	O
m	O
of	O
the	O
observa-	O
tions	O
,	O
where	O
m	O
<	O
n	O
,	O
we	O
have	O
both	O
predictor	B
measurements	O
and	O
a	O
response	B
measurement	O
.	O
for	O
the	O
remaining	O
n	O
−	O
m	O
observations	B
,	O
we	O
have	O
predictor	B
measurements	O
but	O
no	O
response	B
measurement	O
.	O
such	O
a	O
scenario	O
can	O
arise	O
if	O
the	O
predictors	O
can	O
be	O
measured	O
relatively	O
cheaply	O
but	O
the	O
corresponding	O
responses	O
are	O
much	O
more	O
expensive	O
to	O
collect	O
.	O
we	O
refer	O
to	O
this	O
setting	O
as	O
a	O
semi-supervised	B
learning	I
problem	O
.	O
in	O
this	O
setting	O
,	O
we	O
wish	O
to	O
use	O
a	O
sta-	O
tistical	O
learning	O
method	O
that	O
can	O
incorporate	O
the	O
m	O
observations	B
for	O
which	O
response	B
measurements	O
are	O
available	O
as	O
well	O
as	O
the	O
n	O
−	O
m	O
observations	B
for	O
which	O
they	O
are	O
not	O
.	O
although	O
this	O
is	O
an	O
interesting	O
topic	O
,	O
it	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
semi-	O
supervised	B
learning	I
2.1.5	O
regression	B
versus	O
classiﬁcation	B
problems	O
variables	O
can	O
be	O
characterized	O
as	O
either	O
quantitative	B
or	O
qualitative	B
(	O
also	O
known	O
as	O
categorical	B
)	O
.	O
quantitative	B
variables	O
take	O
on	O
numerical	O
values	O
.	O
examples	O
include	O
a	O
person	O
’	O
s	O
age	O
,	O
height	O
,	O
or	O
income	O
,	O
the	O
value	O
of	O
a	O
house	O
,	O
and	O
the	O
price	O
of	O
a	O
stock	O
.	O
in	O
contrast	B
,	O
qualitative	B
variables	O
take	O
on	O
val-	O
ues	O
in	O
one	O
of	O
k	O
diﬀerent	O
classes	O
,	O
or	O
categories	O
.	O
examples	O
of	O
qualitative	B
variables	O
include	O
a	O
person	O
’	O
s	O
gender	O
(	O
male	O
or	O
female	O
)	O
,	O
the	O
brand	O
of	O
prod-	O
uct	O
purchased	O
(	O
brand	O
a	O
,	O
b	O
,	O
or	O
c	O
)	O
,	O
whether	O
a	O
person	O
defaults	O
on	O
a	O
debt	O
(	O
yes	O
or	O
no	O
)	O
,	O
or	O
a	O
cancer	O
diagnosis	O
(	O
acute	O
myelogenous	O
leukemia	O
,	O
acute	O
lymphoblastic	O
leukemia	O
,	O
or	O
no	O
leukemia	O
)	O
.	O
we	O
tend	O
to	O
refer	O
to	O
problems	O
with	O
a	O
quantitative	B
response	O
as	O
regression	B
problems	O
,	O
while	O
those	O
involv-	O
ing	O
a	O
qualitative	B
response	O
are	O
often	O
referred	O
to	O
as	O
classiﬁcation	B
problems	O
.	O
however	O
,	O
the	O
distinction	O
is	O
not	O
always	O
that	O
crisp	O
.	O
least	B
squares	I
linear	O
re-	O
gression	O
(	O
chapter	O
3	O
)	O
is	O
used	O
with	O
a	O
quantitative	B
response	O
,	O
whereas	O
logistic	B
regression	I
(	O
chapter	O
4	O
)	O
is	O
typically	O
used	O
with	O
a	O
qualitative	B
(	O
two-class	O
,	O
or	O
binary	B
)	O
response	B
.	O
as	O
such	O
it	O
is	O
often	O
used	O
as	O
a	O
classiﬁcation	B
method	O
.	O
but	O
since	O
it	O
estimates	O
class	O
probabilities	O
,	O
it	O
can	O
be	O
thought	O
of	O
as	O
a	O
regression	B
quantitative	O
qualitative	B
categorical	O
class	O
regression	B
classiﬁcation	O
binary	B
2.2	O
assessing	O
model	B
accuracy	O
29	O
method	O
as	O
well	O
.	O
some	O
statistical	O
methods	O
,	O
such	O
as	O
k-nearest	O
neighbors	O
(	O
chapters	O
2	O
and	O
4	O
)	O
and	O
boosting	B
(	O
chapter	O
8	O
)	O
,	O
can	O
be	O
used	O
in	O
the	O
case	O
of	O
either	O
quantitative	B
or	O
qualitative	B
responses	O
.	O
we	O
tend	O
to	O
select	O
statistical	O
learning	O
methods	O
on	O
the	O
basis	B
of	O
whether	O
the	O
response	B
is	O
quantitative	B
or	O
qualitative	B
;	O
i.e	O
.	O
we	O
might	O
use	O
linear	B
regres-	O
sion	O
when	O
quantitative	B
and	O
logistic	B
regression	I
when	O
qualitative	B
.	O
however	O
,	O
whether	O
the	O
predictors	O
are	O
qualitative	B
or	O
quantitative	B
is	O
generally	O
consid-	O
ered	O
less	O
important	O
.	O
most	O
of	O
the	O
statistical	O
learning	O
methods	O
discussed	O
in	O
this	O
book	O
can	O
be	O
applied	O
regardless	O
of	O
the	O
predictor	B
variable	O
type	O
,	O
provided	O
that	O
any	O
qualitative	B
predictors	O
are	O
properly	O
coded	O
before	O
the	O
analysis	B
is	O
performed	O
.	O
this	O
is	O
discussed	O
in	O
chapter	O
3	O
.	O
2.2	O
assessing	O
model	B
accuracy	O
one	O
of	O
the	O
key	O
aims	O
of	O
this	O
book	O
is	O
to	O
introduce	O
the	O
reader	O
to	O
a	O
wide	O
range	O
of	O
statistical	O
learning	O
methods	O
that	O
extend	O
far	O
beyond	O
the	O
standard	O
linear	O
regression	B
approach	O
.	O
why	O
is	O
it	O
necessary	O
to	O
introduce	O
so	O
many	O
diﬀerent	O
statistical	O
learning	O
approaches	O
,	O
rather	O
than	O
just	O
a	O
single	B
best	O
method	O
?	O
there	O
is	O
no	O
free	O
lunch	O
in	O
statistics	O
:	O
no	O
one	O
method	O
dominates	O
all	O
others	O
over	O
all	O
possible	O
data	B
sets	O
.	O
on	O
a	O
particular	O
data	B
set	O
,	O
one	O
speciﬁc	O
method	O
may	O
work	O
best	O
,	O
but	O
some	O
other	O
method	O
may	O
work	O
better	O
on	O
a	O
similar	O
but	O
diﬀerent	O
data	B
set	O
.	O
hence	O
it	O
is	O
an	O
important	O
task	O
to	O
decide	O
for	O
any	O
given	O
set	B
of	O
data	B
which	O
method	O
produces	O
the	O
best	O
results	O
.	O
selecting	O
the	O
best	O
approach	O
can	O
be	O
one	O
of	O
the	O
most	O
challenging	O
parts	O
of	O
performing	O
statistical	O
learning	O
in	O
practice	O
.	O
in	O
this	O
section	O
,	O
we	O
discuss	O
some	O
of	O
the	O
most	O
important	O
concepts	O
that	O
arise	O
in	O
selecting	O
a	O
statistical	O
learning	O
procedure	O
for	O
a	O
speciﬁc	O
data	B
set	O
.	O
as	O
the	O
book	O
progresses	O
,	O
we	O
will	O
explain	O
how	O
the	O
concepts	O
presented	O
here	O
can	O
be	O
applied	O
in	O
practice	O
.	O
2.2.1	O
measuring	O
the	O
quality	O
of	O
fit	O
in	O
order	O
to	O
evaluate	O
the	O
performance	O
of	O
a	O
statistical	O
learning	O
method	O
on	O
a	O
given	O
data	B
set	O
,	O
we	O
need	O
some	O
way	O
to	O
measure	O
how	O
well	O
its	O
predictions	O
actually	O
match	O
the	O
observed	O
data	B
.	O
that	O
is	O
,	O
we	O
need	O
to	O
quantify	O
the	O
extent	O
to	O
which	O
the	O
predicted	O
response	B
value	O
for	O
a	O
given	O
observation	O
is	O
close	O
to	O
the	O
true	O
response	O
value	O
for	O
that	O
observation	O
.	O
in	O
the	O
regression	B
setting	O
,	O
the	O
most	O
commonly-used	O
measure	O
is	O
the	O
mean	B
squared	I
error	I
(	O
mse	O
)	O
,	O
given	O
by	O
mean	O
m	O
se	O
=	O
1	O
n	O
(	O
yi	O
−	O
ˆf	O
(	O
xi	O
)	O
)	O
2	O
,	O
squared	O
error	B
(	O
2.5	O
)	O
n	O
(	O
cid:17	O
)	O
i=1	O
30	O
2.	O
statistical	O
learning	O
where	O
ˆf	O
(	O
xi	O
)	O
is	O
the	O
prediction	B
that	O
ˆf	O
gives	O
for	O
the	O
ith	O
observation	O
.	O
the	O
mse	O
will	O
be	O
small	O
if	O
the	O
predicted	O
responses	O
are	O
very	O
close	O
to	O
the	O
true	O
responses	O
,	O
and	O
will	O
be	O
large	O
if	O
for	O
some	O
of	O
the	O
observations	B
,	O
the	O
predicted	O
and	O
true	O
responses	O
diﬀer	O
substantially	O
.	O
the	O
mse	O
in	O
(	O
2.5	O
)	O
is	O
computed	O
using	O
the	O
training	B
data	O
that	O
was	O
used	O
to	O
ﬁt	B
the	O
model	B
,	O
and	O
so	O
should	O
more	O
accurately	O
be	O
referred	O
to	O
as	O
the	O
training	B
mse	O
.	O
but	O
in	O
general	O
,	O
we	O
do	O
not	O
really	O
care	O
how	O
well	O
the	O
method	O
works	O
on	O
the	O
training	B
data	O
.	O
rather	O
,	O
we	O
are	O
interested	O
in	O
the	O
accuracy	O
of	O
the	O
pre-	O
dictions	O
that	O
we	O
obtain	O
when	O
we	O
apply	O
our	O
method	O
to	O
previously	O
unseen	O
test	B
data	O
.	O
why	O
is	O
this	O
what	O
we	O
care	O
about	O
?	O
suppose	O
that	O
we	O
are	O
interested	O
in	O
developing	O
an	O
algorithm	O
to	O
predict	O
a	O
stock	O
’	O
s	O
price	O
based	O
on	O
previous	O
stock	O
returns	O
.	O
we	O
can	O
train	B
the	O
method	O
using	O
stock	O
returns	O
from	O
the	O
past	O
6	O
months	O
.	O
but	O
we	O
don	O
’	O
t	O
really	O
care	O
how	O
well	O
our	O
method	O
predicts	O
last	O
week	O
’	O
s	O
stock	O
price	O
.	O
we	O
instead	O
care	O
about	O
how	O
well	O
it	O
will	O
predict	O
tomorrow	O
’	O
s	O
price	O
or	O
next	O
month	O
’	O
s	O
price	O
.	O
on	O
a	O
similar	O
note	O
,	O
suppose	O
that	O
we	O
have	O
clinical	O
measurements	O
(	O
e.g	O
.	O
weight	O
,	O
blood	O
pressure	O
,	O
height	O
,	O
age	O
,	O
family	O
history	O
of	O
disease	O
)	O
for	O
a	O
number	O
of	O
patients	O
,	O
as	O
well	O
as	O
information	O
about	O
whether	O
each	O
patient	O
has	O
diabetes	O
.	O
we	O
can	O
use	O
these	O
patients	O
to	O
train	B
a	O
statistical	O
learn-	O
ing	O
method	O
to	O
predict	O
risk	O
of	O
diabetes	O
based	O
on	O
clinical	O
measurements	O
.	O
in	O
practice	O
,	O
we	O
want	O
this	O
method	O
to	O
accurately	O
predict	O
diabetes	O
risk	O
for	O
future	O
patients	O
based	O
on	O
their	O
clinical	O
measurements	O
.	O
we	O
are	O
not	O
very	O
interested	O
in	O
whether	O
or	O
not	O
the	O
method	O
accurately	O
predicts	O
diabetes	O
risk	O
for	O
patients	O
used	O
to	O
train	B
the	O
model	B
,	O
since	O
we	O
already	O
know	O
which	O
of	O
those	O
patients	O
have	O
diabetes	O
.	O
to	O
state	O
it	O
more	O
mathematically	O
,	O
suppose	O
that	O
we	O
ﬁt	B
our	O
statistical	O
learn-	O
ing	O
method	O
on	O
our	O
training	B
observations	O
{	O
(	O
x1	O
,	O
y1	O
)	O
,	O
(	O
x2	B
,	O
y2	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
}	O
,	O
and	O
we	O
obtain	O
the	O
estimate	O
ˆf	O
.	O
we	O
can	O
then	O
compute	O
ˆf	O
(	O
x1	O
)	O
,	O
ˆf	O
(	O
x2	B
)	O
,	O
.	O
.	O
.	O
,	O
ˆf	O
(	O
xn	O
)	O
.	O
if	O
these	O
are	O
approximately	O
equal	O
to	O
y1	O
,	O
y2	O
,	O
.	O
.	O
.	O
,	O
yn	O
,	O
then	O
the	O
training	B
mse	O
given	O
by	O
(	O
2.5	O
)	O
is	O
small	O
.	O
however	O
,	O
we	O
are	O
really	O
not	O
interested	O
in	O
whether	O
ˆf	O
(	O
xi	O
)	O
≈	O
yi	O
;	O
instead	O
,	O
we	O
want	O
to	O
know	O
whether	O
ˆf	O
(	O
x0	O
)	O
is	O
approximately	O
equal	O
to	O
y0	O
,	O
where	O
(	O
x0	O
,	O
y0	O
)	O
is	O
a	O
previously	O
unseen	O
test	B
observation	O
not	O
used	O
to	O
train	B
the	O
statistical	O
learning	O
method	O
.	O
we	O
want	O
to	O
choose	O
the	O
method	O
that	O
gives	O
the	O
lowest	O
test	B
mse	O
,	O
as	O
opposed	O
to	O
the	O
lowest	O
training	B
mse	O
.	O
in	O
other	O
words	O
,	O
if	O
we	O
had	O
a	O
large	O
number	O
of	O
test	B
observations	O
,	O
we	O
could	O
compute	O
ave	O
(	O
y0	O
−	O
ˆf	O
(	O
x0	O
)	O
)	O
2	O
,	O
(	O
2.6	O
)	O
the	O
average	B
squared	O
prediction	B
error	O
for	O
these	O
test	B
observations	O
(	O
x0	O
,	O
y0	O
)	O
.	O
we	O
’	O
d	O
like	O
to	O
select	O
the	O
model	B
for	O
which	O
the	O
average	B
of	O
this	O
quantity—the	O
test	B
mse—is	O
as	O
small	O
as	O
possible	O
.	O
how	O
can	O
we	O
go	O
about	O
trying	O
to	O
select	O
a	O
method	O
that	O
minimizes	O
the	O
test	B
mse	O
?	O
in	O
some	O
settings	O
,	O
we	O
may	O
have	O
a	O
test	B
data	O
set	B
available—that	O
is	O
,	O
we	O
may	O
have	O
access	O
to	O
a	O
set	B
of	O
observations	B
that	O
were	O
not	O
used	O
to	O
train	B
the	O
statistical	O
learning	O
method	O
.	O
we	O
can	O
then	O
simply	O
evaluate	O
(	O
2.6	O
)	O
on	O
the	O
test	B
observations	O
,	O
and	O
select	O
the	O
learning	O
method	O
for	O
which	O
the	O
test	B
mse	O
is	O
training	B
mse	O
test	B
data	O
test	B
mse	O
2.2	O
assessing	O
model	B
accuracy	O
31	O
y	O
2	O
1	O
0	O
1	O
8	O
6	O
4	O
2	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
5	O
.	O
2	O
0	O
.	O
2	O
5	O
.	O
1	O
0	O
.	O
1	O
5	O
.	O
0	O
0	O
.	O
0	O
0	O
20	O
40	O
60	O
80	O
100	O
2	O
5	O
10	O
20	O
x	O
flexibility	O
figure	O
2.9.	O
left	O
:	O
data	B
simulated	O
from	O
f	O
,	O
shown	O
in	O
black	O
.	O
three	O
estimates	O
of	O
f	O
are	O
shown	O
:	O
the	O
linear	B
regression	I
line	O
(	O
orange	O
curve	O
)	O
,	O
and	O
two	O
smoothing	B
spline	I
ﬁts	O
(	O
blue	O
and	O
green	O
curves	O
)	O
.	O
right	O
:	O
training	B
mse	O
(	O
grey	O
curve	O
)	O
,	O
test	B
mse	O
(	O
red	O
curve	O
)	O
,	O
and	O
minimum	O
possible	O
test	B
mse	O
over	O
all	O
methods	O
(	O
dashed	O
line	B
)	O
.	O
squares	O
represent	O
the	O
training	B
and	O
test	B
mses	O
for	O
the	O
three	O
ﬁts	O
shown	O
in	O
the	O
left-hand	O
panel	O
.	O
smallest	O
.	O
but	O
what	O
if	O
no	O
test	B
observations	O
are	O
available	O
?	O
in	O
that	O
case	O
,	O
one	O
might	O
imagine	O
simply	O
selecting	O
a	O
statistical	O
learning	O
method	O
that	O
minimizes	O
the	O
training	B
mse	O
(	O
2.5	O
)	O
.	O
this	O
seems	O
like	O
it	O
might	O
be	O
a	O
sensible	O
approach	B
,	O
since	O
the	O
training	B
mse	O
and	O
the	O
test	B
mse	O
appear	O
to	O
be	O
closely	O
related	O
.	O
unfortunately	O
,	O
there	O
is	O
a	O
fundamental	O
problem	O
with	O
this	O
strategy	O
:	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
method	O
with	O
the	O
lowest	O
training	B
mse	O
will	O
also	O
have	O
the	O
lowest	O
test	B
mse	O
.	O
roughly	O
speaking	O
,	O
the	O
problem	O
is	O
that	O
many	O
statistical	O
methods	O
speciﬁcally	O
estimate	O
coeﬃcients	O
so	O
as	O
to	O
minimize	O
the	O
training	B
set	O
mse	O
.	O
for	O
these	O
methods	O
,	O
the	O
training	B
set	O
mse	O
can	O
be	O
quite	O
small	O
,	O
but	O
the	O
test	B
mse	O
is	O
often	O
much	O
larger	O
.	O
figure	O
2.9	O
illustrates	O
this	O
phenomenon	O
on	O
a	O
simple	B
example	O
.	O
in	O
the	O
left-	O
hand	O
panel	O
of	O
figure	O
2.9	O
,	O
we	O
have	O
generated	O
observations	B
from	O
(	O
2.1	O
)	O
with	O
the	O
true	O
f	O
given	O
by	O
the	O
black	O
curve	O
.	O
the	O
orange	O
,	O
blue	O
and	O
green	O
curves	O
illus-	O
trate	O
three	O
possible	O
estimates	O
for	O
f	O
obtained	O
using	O
methods	O
with	O
increasing	O
levels	O
of	O
ﬂexibility	O
.	O
the	O
orange	O
line	B
is	O
the	O
linear	B
regression	I
ﬁt	O
,	O
which	O
is	O
rela-	O
tively	O
inﬂexible	O
.	O
the	O
blue	O
and	O
green	O
curves	O
were	O
produced	O
using	O
smoothing	B
splines	O
,	O
discussed	O
in	O
chapter	O
7	O
,	O
with	O
diﬀerent	O
levels	O
of	O
smoothness	O
.	O
it	O
is	O
clear	O
that	O
as	O
the	O
level	B
of	O
ﬂexibility	O
increases	O
,	O
the	O
curves	O
ﬁt	B
the	O
observed	O
data	B
more	O
closely	O
.	O
the	O
green	O
curve	O
is	O
the	O
most	O
ﬂexible	B
and	O
matches	O
the	O
data	B
very	O
well	O
;	O
however	O
,	O
we	O
observe	O
that	O
it	O
ﬁts	O
the	O
true	O
f	O
(	O
shown	O
in	O
black	O
)	O
poorly	O
because	O
it	O
is	O
too	O
wiggly	O
.	O
by	O
adjusting	O
the	O
level	B
of	O
ﬂexibility	O
of	O
the	O
smoothing	B
spline	I
ﬁt	O
,	O
we	O
can	O
produce	O
many	O
diﬀerent	O
ﬁts	O
to	O
this	O
data	B
.	O
smoothing	B
spline	I
degrees	O
of	O
freedom	O
32	O
2.	O
statistical	O
learning	O
we	O
now	O
move	O
on	O
to	O
the	O
right-hand	O
panel	O
of	O
figure	O
2.9.	O
the	O
grey	O
curve	O
displays	O
the	O
average	B
training	O
mse	O
as	O
a	O
function	B
of	O
ﬂexibility	O
,	O
or	O
more	O
for-	O
mally	O
the	O
degrees	B
of	I
freedom	I
,	O
for	O
a	O
number	O
of	O
smoothing	B
splines	O
.	O
the	O
de-	O
grees	O
of	O
freedom	O
is	O
a	O
quantity	O
that	O
summarizes	O
the	O
ﬂexibility	O
of	O
a	O
curve	O
;	O
it	O
is	O
discussed	O
more	O
fully	O
in	O
chapter	O
7.	O
the	O
orange	O
,	O
blue	O
and	O
green	O
squares	O
indicate	O
the	O
mses	O
associated	O
with	O
the	O
corresponding	O
curves	O
in	O
the	O
left-	O
hand	O
panel	O
.	O
a	O
more	O
restricted	O
and	O
hence	O
smoother	B
curve	O
has	O
fewer	O
degrees	B
of	I
freedom	I
than	O
a	O
wiggly	O
curve—note	O
that	O
in	O
figure	O
2.9	O
,	O
linear	B
regression	I
is	O
at	O
the	O
most	O
restrictive	O
end	O
,	O
with	O
two	O
degrees	B
of	I
freedom	I
.	O
the	O
training	B
mse	O
declines	O
monotonically	O
as	O
ﬂexibility	O
increases	O
.	O
in	O
this	O
example	O
the	O
true	O
f	O
is	O
non-linear	B
,	O
and	O
so	O
the	O
orange	O
linear	B
ﬁt	O
is	O
not	O
ﬂexible	B
enough	O
to	O
estimate	O
f	O
well	O
.	O
the	O
green	O
curve	O
has	O
the	O
lowest	O
training	B
mse	O
of	O
all	O
three	O
methods	O
,	O
since	O
it	O
corresponds	O
to	O
the	O
most	O
ﬂexible	B
of	O
the	O
three	O
curves	O
ﬁt	B
in	O
the	O
left-hand	O
panel	O
.	O
in	O
this	O
example	O
,	O
we	O
know	O
the	O
true	O
function	O
f	O
,	O
and	O
so	O
we	O
can	O
also	O
com-	O
pute	O
the	O
test	B
mse	O
over	O
a	O
very	O
large	O
test	B
set	O
,	O
as	O
a	O
function	B
of	O
ﬂexibility	O
.	O
(	O
of	O
course	O
,	O
in	O
general	O
f	O
is	O
unknown	O
,	O
so	O
this	O
will	O
not	O
be	O
possible	O
.	O
)	O
the	O
test	B
mse	O
is	O
displayed	O
using	O
the	O
red	O
curve	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
2.9.	O
as	O
with	O
the	O
training	B
mse	O
,	O
the	O
test	B
mse	O
initially	O
declines	O
as	O
the	O
level	B
of	O
ﬂex-	O
ibility	O
increases	O
.	O
however	O
,	O
at	O
some	O
point	O
the	O
test	B
mse	O
levels	O
oﬀ	O
and	O
then	O
starts	O
to	O
increase	O
again	O
.	O
consequently	O
,	O
the	O
orange	O
and	O
green	O
curves	O
both	O
have	O
high	O
test	B
mse	O
.	O
the	O
blue	O
curve	O
minimizes	O
the	O
test	B
mse	O
,	O
which	O
should	O
not	O
be	O
surprising	O
given	O
that	O
visually	O
it	O
appears	O
to	O
estimate	O
f	O
the	O
best	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
2.9.	O
the	O
horizontal	O
dashed	O
line	B
indicates	O
var	O
(	O
	O
)	O
,	O
the	O
irreducible	B
error	I
in	O
(	O
2.3	O
)	O
,	O
which	O
corresponds	O
to	O
the	O
lowest	O
achievable	O
test	B
mse	O
among	O
all	O
possible	O
methods	O
.	O
hence	O
,	O
the	O
smoothing	B
spline	I
repre-	O
sented	O
by	O
the	O
blue	O
curve	O
is	O
close	O
to	O
optimal	O
.	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
2.9	O
,	O
as	O
the	O
ﬂexibility	O
of	O
the	O
statistical	O
learning	O
method	O
increases	O
,	O
we	O
observe	O
a	O
monotone	O
decrease	O
in	O
the	O
training	B
mse	O
and	O
a	O
u-shape	O
in	O
the	O
test	B
mse	O
.	O
this	O
is	O
a	O
fundamental	O
property	O
of	O
statistical	O
learning	O
that	O
holds	O
regardless	O
of	O
the	O
particular	O
data	B
set	O
at	O
hand	O
and	O
regardless	O
of	O
the	O
statistical	O
method	O
being	O
used	O
.	O
as	O
model	B
ﬂexibility	O
increases	O
,	O
training	B
mse	O
will	O
decrease	O
,	O
but	O
the	O
test	B
mse	O
may	O
not	O
.	O
when	O
a	O
given	O
method	O
yields	O
a	O
small	O
training	B
mse	O
but	O
a	O
large	O
test	B
mse	O
,	O
we	O
are	O
said	O
to	O
be	O
overﬁtting	B
the	O
data	B
.	O
this	O
happens	O
because	O
our	O
statistical	O
learning	O
procedure	O
is	O
working	O
too	O
hard	O
to	O
ﬁnd	O
patterns	O
in	O
the	O
training	B
data	O
,	O
and	O
may	O
be	O
picking	O
up	O
some	O
patterns	O
that	O
are	O
just	O
caused	O
by	O
random	O
chance	O
rather	O
than	O
by	O
true	O
properties	O
of	O
the	O
unknown	O
function	B
f	O
.	O
when	O
we	O
overﬁt	O
the	O
training	B
data	O
,	O
the	O
test	B
mse	O
will	O
be	O
very	O
large	O
because	O
the	O
supposed	O
patterns	O
that	O
the	O
method	O
found	O
in	O
the	O
training	B
data	O
simply	O
don	O
’	O
t	O
exist	O
in	O
the	O
test	B
data	O
.	O
note	O
that	O
regardless	O
of	O
whether	O
or	O
not	O
overﬁtting	B
has	O
occurred	O
,	O
we	O
almost	O
always	O
expect	O
the	O
training	B
mse	O
to	O
be	O
smaller	O
than	O
the	O
test	B
mse	O
because	O
most	O
statistical	O
learning	O
methods	O
either	O
directly	O
or	O
indirectly	O
seek	O
to	O
minimize	O
the	O
training	B
mse	O
.	O
overﬁtting	B
refers	O
speciﬁcally	O
to	O
the	O
case	O
in	O
which	O
a	O
less	O
ﬂexible	B
model	O
would	O
have	O
yielded	O
a	O
smaller	O
test	B
mse	O
.	O
2.2	O
assessing	O
model	B
accuracy	O
33	O
y	O
2	O
1	O
0	O
1	O
8	O
6	O
4	O
2	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
5	O
.	O
2	O
0	O
.	O
2	O
5	O
.	O
1	O
0	O
.	O
1	O
5	O
.	O
0	O
0	O
.	O
0	O
0	O
20	O
40	O
60	O
80	O
100	O
2	O
5	O
10	O
20	O
x	O
flexibility	O
figure	O
2.10.	O
details	O
are	O
as	O
in	O
figure	O
2.9	O
,	O
using	O
a	O
diﬀerent	O
true	O
f	O
that	O
is	O
much	O
closer	O
to	O
linear	B
.	O
in	O
this	O
setting	O
,	O
linear	B
regression	I
provides	O
a	O
very	O
good	O
ﬁt	B
to	O
the	O
data	B
.	O
figure	O
2.10	O
provides	O
another	O
example	O
in	O
which	O
the	O
true	O
f	O
is	O
approxi-	O
mately	O
linear	B
.	O
again	O
we	O
observe	O
that	O
the	O
training	B
mse	O
decreases	O
mono-	O
tonically	O
as	O
the	O
model	B
ﬂexibility	O
increases	O
,	O
and	O
that	O
there	O
is	O
a	O
u-shape	O
in	O
the	O
test	B
mse	O
.	O
however	O
,	O
because	O
the	O
truth	O
is	O
close	O
to	O
linear	B
,	O
the	O
test	B
mse	O
only	O
decreases	O
slightly	O
before	O
increasing	O
again	O
,	O
so	O
that	O
the	O
orange	O
least	B
squares	I
ﬁt	O
is	O
substantially	O
better	O
than	O
the	O
highly	O
ﬂexible	B
green	O
curve	O
.	O
fi-	O
nally	O
,	O
figure	O
2.11	O
displays	O
an	O
example	O
in	O
which	O
f	O
is	O
highly	O
non-linear	B
.	O
the	O
training	B
and	O
test	B
mse	O
curves	O
still	O
exhibit	O
the	O
same	O
general	O
patterns	O
,	O
but	O
now	O
there	O
is	O
a	O
rapid	O
decrease	O
in	O
both	O
curves	O
before	O
the	O
test	B
mse	O
starts	O
to	O
increase	O
slowly	O
.	O
in	O
practice	O
,	O
one	O
can	O
usually	O
compute	O
the	O
training	B
mse	O
with	O
relative	O
ease	O
,	O
but	O
estimating	O
test	B
mse	O
is	O
considerably	O
more	O
diﬃcult	O
because	O
usually	O
no	O
test	B
data	O
are	O
available	O
.	O
as	O
the	O
previous	O
three	O
examples	O
illustrate	O
,	O
the	O
ﬂexibility	O
level	B
corresponding	O
to	O
the	O
model	B
with	O
the	O
minimal	O
test	B
mse	O
can	O
vary	O
considerably	O
among	O
data	B
sets	O
.	O
throughout	O
this	O
book	O
,	O
we	O
discuss	O
a	O
variety	O
of	O
approaches	O
that	O
can	O
be	O
used	O
in	O
practice	O
to	O
estimate	O
this	O
minimum	O
point	O
.	O
one	O
important	O
method	O
is	O
cross-validation	B
(	O
chapter	O
5	O
)	O
,	O
which	O
is	O
a	O
cross-	O
method	O
for	O
estimating	O
test	B
mse	O
using	O
the	O
training	B
data	O
.	O
validation	O
2.2.2	O
the	O
bias-variance	B
trade-oﬀ	O
the	O
u-shape	O
observed	O
in	O
the	O
test	B
mse	O
curves	O
(	O
figures	O
2.9–2.11	O
)	O
turns	O
out	O
to	O
be	O
the	O
result	O
of	O
two	O
competing	O
properties	O
of	O
statistical	O
learning	O
methods	O
.	O
though	O
the	O
mathematical	O
proof	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
,	O
it	O
is	O
possible	O
to	O
show	O
that	O
the	O
expected	O
test	O
mse	O
,	O
for	O
a	O
given	O
value	O
x0	O
,	O
can	O
34	O
2.	O
statistical	O
learning	O
y	O
0	O
2	O
0	O
1	O
0	O
0	O
1	O
−	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0	O
2	O
5	O
1	O
0	O
1	O
5	O
0	O
0	O
20	O
40	O
60	O
80	O
100	O
2	O
5	O
10	O
20	O
x	O
flexibility	O
figure	O
2.11.	O
details	O
are	O
as	O
in	O
figure	O
2.9	O
,	O
using	O
a	O
diﬀerent	O
f	O
that	O
is	O
far	O
from	O
linear	B
.	O
in	O
this	O
setting	O
,	O
linear	B
regression	I
provides	O
a	O
very	O
poor	O
ﬁt	B
to	O
the	O
data	B
.	O
always	O
be	O
decomposed	O
into	O
the	O
sum	O
of	O
three	O
fundamental	O
quantities	O
:	O
the	O
variance	B
of	O
ˆf	O
(	O
x0	O
)	O
,	O
the	O
squared	O
bias	B
of	O
ˆf	O
(	O
x0	O
)	O
and	O
the	O
variance	B
of	O
the	O
error	B
terms	O
	O
.	O
that	O
is	O
,	O
=	O
var	O
(	O
ˆf	O
(	O
x0	O
)	O
)	O
+	O
[	O
bias	B
(	O
ˆf	O
(	O
x0	O
)	O
)	O
]	O
2	O
+	O
var	O
(	O
	O
)	O
.	O
(	O
2.7	O
)	O
e	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
2	O
y0	O
−	O
ˆf	O
(	O
x0	O
)	O
(	O
cid:18	O
)	O
(	O
cid:19	O
)	O
2	O
y0	O
−	O
ˆf	O
(	O
x0	O
)	O
here	O
the	O
notation	O
e	O
deﬁnes	O
the	O
expected	O
test	O
mse	O
,	O
and	O
refers	O
to	O
the	O
average	B
test	O
mse	O
that	O
we	O
would	O
obtain	O
if	O
we	O
repeatedly	O
estimated	O
f	O
using	O
a	O
large	O
number	O
of	O
training	B
sets	O
,	O
and	O
tested	O
each	O
at	O
x0	O
.	O
the	O
overall	O
(	O
cid:19	O
)	O
2	O
(	O
cid:18	O
)	O
y0	O
−	O
ˆf	O
(	O
x0	O
)	O
over	O
all	O
expected	O
test	O
mse	O
can	O
be	O
computed	O
by	O
averaging	O
e	O
possible	O
values	O
of	O
x0	O
in	O
the	O
test	B
set	O
.	O
equation	O
2.7	O
tells	O
us	O
that	O
in	O
order	O
to	O
minimize	O
the	O
expected	O
test	O
error	B
,	O
we	O
need	O
to	O
select	O
a	O
statistical	O
learning	O
method	O
that	O
simultaneously	O
achieves	O
low	O
variance	B
and	O
low	O
bias	B
.	O
note	O
that	O
variance	B
is	O
inherently	O
a	O
nonnegative	O
quantity	O
,	O
and	O
squared	O
bias	B
is	O
also	O
nonnegative	O
.	O
hence	O
,	O
we	O
see	O
that	O
the	O
expected	O
test	O
mse	O
can	O
never	O
lie	O
below	O
var	O
(	O
	O
)	O
,	O
the	O
irreducible	B
error	I
from	O
(	O
2.3	O
)	O
.	O
what	O
do	O
we	O
mean	O
by	O
the	O
variance	B
and	O
bias	B
of	O
a	O
statistical	O
learning	O
method	O
?	O
variance	B
refers	O
to	O
the	O
amount	O
by	O
which	O
ˆf	O
would	O
change	O
if	O
we	O
estimated	O
it	O
using	O
a	O
diﬀerent	O
training	B
data	O
set	B
.	O
since	O
the	O
training	B
data	O
are	O
used	O
to	O
ﬁt	B
the	O
statistical	O
learning	O
method	O
,	O
diﬀerent	O
training	B
data	O
sets	O
will	O
result	O
in	O
a	O
diﬀerent	O
ˆf	O
.	O
but	O
ideally	O
the	O
estimate	O
for	O
f	O
should	O
not	O
vary	O
too	O
much	O
between	O
training	B
sets	O
.	O
however	O
,	O
if	O
a	O
method	O
has	O
high	O
variance	B
then	O
small	O
changes	O
in	O
the	O
training	B
data	O
can	O
result	O
in	O
large	O
changes	O
in	O
ˆf	O
.	O
in	O
general	O
,	O
more	O
ﬂexible	B
statistical	O
methods	O
have	O
higher	O
variance	B
.	O
consider	O
the	O
variance	B
bias	O
expected	O
test	O
mse	O
2.2	O
assessing	O
model	B
accuracy	O
35	O
green	O
and	O
orange	O
curves	O
in	O
figure	O
2.9.	O
the	O
ﬂexible	B
green	O
curve	O
is	O
following	O
the	O
observations	B
very	O
closely	O
.	O
it	O
has	O
high	O
variance	B
because	O
changing	O
any	O
one	O
of	O
these	O
data	B
points	O
may	O
cause	O
the	O
estimate	O
ˆf	O
to	O
change	O
considerably	O
.	O
in	O
contrast	B
,	O
the	O
orange	O
least	B
squares	I
line	O
is	O
relatively	O
inﬂexible	O
and	O
has	O
low	O
variance	B
,	O
because	O
moving	O
any	O
single	B
observation	O
will	O
likely	O
cause	O
only	O
a	O
small	O
shift	O
in	O
the	O
position	O
of	O
the	O
line	B
.	O
on	O
the	O
other	O
hand	O
,	O
bias	B
refers	O
to	O
the	O
error	B
that	O
is	O
introduced	O
by	O
approxi-	O
mating	O
a	O
real-life	O
problem	O
,	O
which	O
may	O
be	O
extremely	O
complicated	O
,	O
by	O
a	O
much	O
simpler	O
model	B
.	O
for	O
example	O
,	O
linear	B
regression	I
assumes	O
that	O
there	O
is	O
a	O
linear	B
relationship	O
between	O
y	O
and	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
.	O
it	O
is	O
unlikely	O
that	O
any	O
real-life	O
problem	O
truly	O
has	O
such	O
a	O
simple	B
linear	O
relationship	O
,	O
and	O
so	O
performing	O
lin-	O
ear	O
regression	B
will	O
undoubtedly	O
result	O
in	O
some	O
bias	B
in	O
the	O
estimate	O
of	O
f	O
.	O
in	O
figure	O
2.11	O
,	O
the	O
true	O
f	O
is	O
substantially	O
non-linear	B
,	O
so	O
no	O
matter	O
how	O
many	O
training	B
observations	O
we	O
are	O
given	O
,	O
it	O
will	O
not	O
be	O
possible	O
to	O
produce	O
an	O
accurate	O
estimate	O
using	O
linear	B
regression	I
.	O
in	O
other	O
words	O
,	O
linear	B
regression	I
results	O
in	O
high	O
bias	B
in	O
this	O
example	O
.	O
however	O
,	O
in	O
figure	O
2.10	O
the	O
true	O
f	O
is	O
very	O
close	O
to	O
linear	B
,	O
and	O
so	O
given	O
enough	O
data	B
,	O
it	O
should	O
be	O
possible	O
for	O
linear	O
regression	B
to	O
produce	O
an	O
accurate	O
estimate	O
.	O
generally	O
,	O
more	O
ﬂexible	B
methods	O
result	O
in	O
less	O
bias	B
.	O
as	O
a	O
general	O
rule	O
,	O
as	O
we	O
use	O
more	O
ﬂexible	B
methods	O
,	O
the	O
variance	B
will	O
increase	O
and	O
the	O
bias	B
will	O
decrease	O
.	O
the	O
relative	O
rate	B
of	O
change	O
of	O
these	O
two	O
quantities	O
determines	O
whether	O
the	O
test	B
mse	O
increases	O
or	O
decreases	O
.	O
as	O
we	O
increase	O
the	O
ﬂexibility	O
of	O
a	O
class	O
of	O
methods	O
,	O
the	O
bias	B
tends	O
to	O
initially	O
decrease	O
faster	O
than	O
the	O
variance	B
increases	O
.	O
consequently	O
,	O
the	O
expected	O
test	O
mse	O
declines	O
.	O
however	O
,	O
at	O
some	O
point	O
increasing	O
ﬂexibility	O
has	O
little	O
impact	O
on	O
the	O
bias	B
but	O
starts	O
to	O
signiﬁcantly	O
increase	O
the	O
variance	B
.	O
when	O
this	O
happens	O
the	O
test	B
mse	O
increases	O
.	O
note	O
that	O
we	O
observed	O
this	O
pattern	O
of	O
decreasing	O
test	B
mse	O
followed	O
by	O
increasing	O
test	B
mse	O
in	O
the	O
right-hand	O
panels	O
of	O
figures	O
2.9–2.11	O
.	O
the	O
three	O
plots	O
in	O
figure	O
2.12	O
illustrate	O
equation	O
2.7	O
for	O
the	O
examples	O
in	O
figures	O
2.9–2.11	O
.	O
in	O
each	O
case	O
the	O
blue	O
solid	O
curve	O
represents	O
the	O
squared	O
bias	B
,	O
for	O
diﬀerent	O
levels	O
of	O
ﬂexibility	O
,	O
while	O
the	O
orange	O
curve	O
corresponds	O
to	O
the	O
variance	B
.	O
the	O
horizontal	O
dashed	O
line	B
represents	O
var	O
(	O
	O
)	O
,	O
the	O
irreducible	B
error	I
.	O
finally	O
,	O
the	O
red	O
curve	O
,	O
corresponding	O
to	O
the	O
test	B
set	O
mse	O
,	O
is	O
the	O
sum	O
of	O
these	O
three	O
quantities	O
.	O
in	O
all	O
three	O
cases	O
,	O
the	O
variance	B
increases	O
and	O
the	O
bias	B
decreases	O
as	O
the	O
method	O
’	O
s	O
ﬂexibility	O
increases	O
.	O
however	O
,	O
the	O
ﬂexibility	O
level	B
corresponding	O
to	O
the	O
optimal	O
test	O
mse	O
diﬀers	O
considerably	O
among	O
the	O
three	O
data	B
sets	O
,	O
because	O
the	O
squared	O
bias	B
and	O
variance	B
change	O
at	O
diﬀerent	O
rates	O
in	O
each	O
of	O
the	O
data	B
sets	O
.	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
2.12	O
,	O
the	O
bias	B
initially	O
decreases	O
rapidly	O
,	O
resulting	O
in	O
an	O
initial	O
sharp	O
decrease	O
in	O
the	O
expected	O
test	O
mse	O
.	O
on	O
the	O
other	O
hand	O
,	O
in	O
the	O
center	O
panel	O
of	O
figure	O
2.12	O
the	O
true	O
f	O
is	O
close	O
to	O
linear	B
,	O
so	O
there	O
is	O
only	O
a	O
small	O
decrease	O
in	O
bias	B
as	O
ﬂex-	O
ibility	O
increases	O
,	O
and	O
the	O
test	B
mse	O
only	O
declines	O
slightly	O
before	O
increasing	O
rapidly	O
as	O
the	O
variance	B
increases	O
.	O
finally	O
,	O
in	O
the	O
right-hand	O
panel	O
of	O
fig-	O
ure	O
2.12	O
,	O
as	O
ﬂexibility	O
increases	O
,	O
there	O
is	O
a	O
dramatic	O
decline	O
in	O
bias	B
because	O
36	O
2.	O
statistical	O
learning	O
5	O
.	O
2	O
0	O
.	O
2	O
5	O
.	O
1	O
0	O
.	O
1	O
5	O
.	O
0	O
0	O
.	O
0	O
5	O
.	O
2	O
0	O
.	O
2	O
5	O
.	O
1	O
0	O
.	O
1	O
5	O
.	O
0	O
0	O
.	O
0	O
mse	O
bias	B
var	O
0	O
2	O
5	O
1	O
0	O
1	O
5	O
0	O
2	O
5	O
10	O
20	O
2	O
5	O
10	O
20	O
2	O
5	O
10	O
20	O
flexibility	O
flexibility	O
flexibility	O
figure	O
2.12.	O
squared	O
bias	B
(	O
blue	O
curve	O
)	O
,	O
variance	B
(	O
orange	O
curve	O
)	O
,	O
var	O
(	O
	O
)	O
(	O
dashed	O
line	B
)	O
,	O
and	O
test	B
mse	O
(	O
red	O
curve	O
)	O
for	O
the	O
three	O
data	B
sets	O
in	O
figures	O
2.9–2.11	O
.	O
the	O
vertical	O
dotted	O
line	B
indicates	O
the	O
ﬂexibility	O
level	B
corresponding	O
to	O
the	O
smallest	O
test	B
mse	O
.	O
the	O
true	O
f	O
is	O
very	O
non-linear	B
.	O
there	O
is	O
also	O
very	O
little	O
increase	O
in	O
variance	B
as	O
ﬂexibility	O
increases	O
.	O
consequently	O
,	O
the	O
test	B
mse	O
declines	O
substantially	O
before	O
experiencing	O
a	O
small	O
increase	O
as	O
model	B
ﬂexibility	O
increases	O
.	O
the	O
relationship	O
between	O
bias	B
,	O
variance	B
,	O
and	O
test	B
set	O
mse	O
given	O
in	O
equa-	O
tion	O
2.7	O
and	O
displayed	O
in	O
figure	O
2.12	O
is	O
referred	O
to	O
as	O
the	O
bias-variance	B
trade-oﬀ	O
.	O
good	O
test	B
set	O
performance	O
of	O
a	O
statistical	O
learning	O
method	O
re-	O
quires	O
low	O
variance	B
as	O
well	O
as	O
low	O
squared	O
bias	B
.	O
this	O
is	O
referred	O
to	O
as	O
a	O
trade-oﬀ	B
because	O
it	O
is	O
easy	O
to	O
obtain	O
a	O
method	O
with	O
extremely	O
low	O
bias	B
but	O
high	O
variance	B
(	O
for	O
instance	O
,	O
by	O
drawing	O
a	O
curve	O
that	O
passes	O
through	O
every	O
single	B
training	O
observation	O
)	O
or	O
a	O
method	O
with	O
very	O
low	O
variance	B
but	O
high	O
bias	B
(	O
by	O
ﬁtting	O
a	O
horizontal	O
line	B
to	O
the	O
data	B
)	O
.	O
the	O
challenge	O
lies	O
in	O
ﬁnding	O
a	O
method	O
for	O
which	O
both	O
the	O
variance	B
and	O
the	O
squared	O
bias	B
are	O
low	O
.	O
this	O
trade-oﬀ	B
is	O
one	O
of	O
the	O
most	O
important	O
recurring	O
themes	O
in	O
this	O
book	O
.	O
in	O
a	O
real-life	O
situation	O
in	O
which	O
f	O
is	O
unobserved	O
,	O
it	O
is	O
generally	O
not	O
pos-	O
sible	O
to	O
explicitly	O
compute	O
the	O
test	B
mse	O
,	O
bias	B
,	O
or	O
variance	B
for	O
a	O
statistical	O
learning	O
method	O
.	O
nevertheless	O
,	O
one	O
should	O
always	O
keep	O
the	O
bias-variance	B
trade-oﬀ	O
in	O
mind	O
.	O
in	O
this	O
book	O
we	O
explore	O
methods	O
that	O
are	O
extremely	O
ﬂexible	B
and	O
hence	O
can	O
essentially	O
eliminate	O
bias	B
.	O
however	O
,	O
this	O
does	O
not	O
guarantee	O
that	O
they	O
will	O
outperform	O
a	O
much	O
simpler	O
method	O
such	O
as	O
linear	B
regression	I
.	O
to	O
take	O
an	O
extreme	O
example	O
,	O
suppose	O
that	O
the	O
true	O
f	O
is	O
linear	B
.	O
in	O
this	O
situation	O
linear	B
regression	I
will	O
have	O
no	O
bias	B
,	O
making	O
it	O
very	O
hard	O
for	O
a	O
more	O
ﬂexible	B
method	O
to	O
compete	O
.	O
in	O
contrast	B
,	O
if	O
the	O
true	O
f	O
is	O
highly	O
non-linear	B
and	O
we	O
have	O
an	O
ample	O
number	O
of	O
training	B
observations	O
,	O
then	O
we	O
may	O
do	O
better	O
using	O
a	O
highly	O
ﬂexible	B
approach	O
,	O
as	O
in	O
figure	O
2.11.	O
in	O
chapter	O
5	O
we	O
discuss	O
cross-validation	B
,	O
which	O
is	O
a	O
way	O
to	O
estimate	O
the	O
test	B
mse	O
using	O
the	O
training	B
data	O
.	O
bias-variance	B
trade-oﬀ	O
2.2	O
assessing	O
model	B
accuracy	O
37	O
2.2.3	O
the	O
classiﬁcation	B
setting	O
thus	O
far	O
,	O
our	O
discussion	O
of	O
model	B
accuracy	O
has	O
been	O
focused	O
on	O
the	O
regres-	O
sion	O
setting	O
.	O
but	O
many	O
of	O
the	O
concepts	O
that	O
we	O
have	O
encountered	O
,	O
such	O
as	O
the	O
bias-variance	B
trade-oﬀ	O
,	O
transfer	O
over	O
to	O
the	O
classiﬁcation	B
setting	O
with	O
only	O
some	O
modiﬁcations	O
due	O
to	O
the	O
fact	O
that	O
yi	O
is	O
no	O
longer	O
numer-	O
ical	O
.	O
suppose	O
that	O
we	O
seek	O
to	O
estimate	O
f	O
on	O
the	O
basis	B
of	O
training	B
obser-	O
vations	O
{	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
}	O
,	O
where	O
now	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
are	O
qualitative	B
.	O
the	O
most	O
common	O
approach	B
for	O
quantifying	O
the	O
accuracy	O
of	O
our	O
estimate	O
ˆf	O
is	O
the	O
training	B
error	O
rate	B
,	O
the	O
proportion	O
of	O
mistakes	O
that	O
are	O
made	O
if	O
we	O
apply	O
our	O
estimate	O
ˆf	O
to	O
the	O
training	B
observations	O
:	O
n	O
(	O
cid:17	O
)	O
i=1	O
1	O
n	O
i	O
(	O
yi	O
(	O
cid:4	O
)	O
=	O
ˆyi	O
)	O
.	O
(	O
2.8	O
)	O
here	O
ˆyi	O
is	O
the	O
predicted	O
class	O
label	O
for	O
the	O
ith	O
observation	O
using	O
ˆf	O
.	O
and	O
i	O
(	O
yi	O
(	O
cid:4	O
)	O
=	O
ˆyi	O
)	O
is	O
an	O
indicator	B
variable	O
that	O
equals	O
1	O
if	O
yi	O
(	O
cid:4	O
)	O
=	O
ˆyi	O
and	O
zero	O
if	O
yi	O
=	O
ˆyi	O
.	O
if	O
i	O
(	O
yi	O
(	O
cid:4	O
)	O
=	O
ˆyi	O
)	O
=	O
0	O
then	O
the	O
ith	O
observation	O
was	O
classiﬁed	O
correctly	O
by	O
our	O
classiﬁcation	B
method	O
;	O
otherwise	O
it	O
was	O
misclassiﬁed	O
.	O
hence	O
equation	O
2.8	O
computes	O
the	O
fraction	O
of	O
incorrect	O
classiﬁcations	O
.	O
equation	O
2.8	O
is	O
referred	O
to	O
as	O
the	O
training	B
error	O
rate	B
because	O
it	O
is	O
com-	O
puted	O
based	O
on	O
the	O
data	B
that	O
was	O
used	O
to	O
train	B
our	O
classiﬁer	B
.	O
as	O
in	O
the	O
regression	B
setting	O
,	O
we	O
are	O
most	O
interested	O
in	O
the	O
error	B
rates	O
that	O
result	O
from	O
applying	O
our	O
classiﬁer	B
to	O
test	B
observations	O
that	O
were	O
not	O
used	O
in	O
training	B
.	O
the	O
test	B
error	O
rate	B
associated	O
with	O
a	O
set	B
of	O
test	B
observations	O
of	O
the	O
form	O
(	O
x0	O
,	O
y0	O
)	O
is	O
given	O
by	O
ave	O
(	O
i	O
(	O
y0	O
(	O
cid:4	O
)	O
=	O
ˆy0	O
)	O
)	O
,	O
(	O
2.9	O
)	O
error	B
rate	I
indicator	O
variable	B
training	O
error	B
test	O
error	B
where	O
ˆy0	O
is	O
the	O
predicted	O
class	O
label	O
that	O
results	O
from	O
applying	O
the	O
classiﬁer	B
to	O
the	O
test	B
observation	O
with	O
predictor	B
x0	O
.	O
a	O
good	O
classiﬁer	B
is	O
one	O
for	O
which	O
the	O
test	B
error	O
(	O
2.9	O
)	O
is	O
smallest	O
.	O
the	O
bayes	O
classiﬁer	B
it	O
is	O
possible	O
to	O
show	O
(	O
though	O
the	O
proof	O
is	O
outside	O
of	O
the	O
scope	O
of	O
this	O
book	O
)	O
that	O
the	O
test	B
error	O
rate	B
given	O
in	O
(	O
2.9	O
)	O
is	O
minimized	O
,	O
on	O
average	B
,	O
by	O
a	O
very	O
simple	B
classiﬁer	O
that	O
assigns	O
each	O
observation	O
to	O
the	O
most	O
likely	O
class	O
,	O
given	O
its	O
predictor	B
values	O
.	O
in	O
other	O
words	O
,	O
we	O
should	O
simply	O
assign	O
a	O
test	B
observation	O
with	O
predictor	B
vector	O
x0	O
to	O
the	O
class	O
j	O
for	O
which	O
pr	O
(	O
y	O
=	O
j|x	O
=	O
x0	O
)	O
(	O
2.10	O
)	O
is	O
largest	O
.	O
note	O
that	O
(	O
2.10	O
)	O
is	O
a	O
conditional	B
probability	I
:	O
it	O
is	O
the	O
probability	B
that	O
y	O
=	O
j	O
,	O
given	O
the	O
observed	O
predictor	B
vector	O
x0	O
.	O
this	O
very	O
simple	B
clas-	O
siﬁer	O
is	O
called	O
the	O
bayes	O
classiﬁer	B
.	O
in	O
a	O
two-class	O
problem	O
where	O
there	O
are	O
only	O
two	O
possible	O
response	B
values	O
,	O
say	O
class	O
1	O
or	O
class	O
2	O
,	O
the	O
bayes	O
classiﬁer	B
conditional	O
probability	B
bayes	O
classiﬁer	B
38	O
2.	O
statistical	O
learning	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
figure	O
2.13.	O
a	O
simulated	O
data	B
set	O
consisting	O
of	O
100	O
observations	B
in	O
each	O
of	O
two	O
groups	O
,	O
indicated	O
in	O
blue	O
and	O
in	O
orange	O
.	O
the	O
purple	O
dashed	O
line	B
represents	O
the	O
bayes	O
decision	B
boundary	I
.	O
the	O
orange	O
background	O
grid	O
indicates	O
the	O
region	O
in	O
which	O
a	O
test	B
observation	O
will	O
be	O
assigned	O
to	O
the	O
orange	O
class	O
,	O
and	O
the	O
blue	O
background	O
grid	O
indicates	O
the	O
region	O
in	O
which	O
a	O
test	B
observation	O
will	O
be	O
assigned	O
to	O
the	O
blue	O
class	O
.	O
corresponds	O
to	O
predicting	O
class	O
one	O
if	O
pr	O
(	O
y	O
=	O
1|x	O
=	O
x0	O
)	O
>	O
0.5	O
,	O
and	O
class	O
two	O
otherwise	O
.	O
figure	O
2.13	O
provides	O
an	O
example	O
using	O
a	O
simulated	O
data	B
set	O
in	O
a	O
two-	O
dimensional	O
space	O
consisting	O
of	O
predictors	O
x1	O
and	O
x2	B
.	O
the	O
orange	O
and	O
blue	O
circles	O
correspond	O
to	O
training	B
observations	O
that	O
belong	O
to	O
two	O
diﬀerent	O
classes	O
.	O
for	O
each	O
value	O
of	O
x1	O
and	O
x2	B
,	O
there	O
is	O
a	O
diﬀerent	O
probability	B
of	O
the	O
response	B
being	O
orange	O
or	O
blue	O
.	O
since	O
this	O
is	O
simulated	O
data	B
,	O
we	O
know	O
how	O
the	O
data	B
were	O
generated	O
and	O
we	O
can	O
calculate	O
the	O
conditional	O
probabilities	O
for	O
each	O
value	O
of	O
x1	O
and	O
x2	B
.	O
the	O
orange	O
shaded	O
region	O
reﬂects	O
the	O
set	B
of	O
points	O
for	O
which	O
pr	O
(	O
y	O
=	O
orange|x	O
)	O
is	O
greater	O
than	O
50	O
%	O
,	O
while	O
the	O
blue	O
shaded	O
region	O
indicates	O
the	O
set	B
of	O
points	O
for	O
which	O
the	O
probability	B
is	O
below	O
50	O
%	O
.	O
the	O
purple	O
dashed	O
line	B
represents	O
the	O
points	O
where	O
the	O
probability	B
is	O
exactly	O
50	O
%	O
.	O
this	O
is	O
called	O
the	O
bayes	O
decision	B
boundary	I
.	O
the	O
bayes	O
classiﬁer	B
’	O
s	O
prediction	B
is	O
determined	O
by	O
the	O
bayes	O
decision	B
boundary	I
;	O
an	O
observation	O
that	O
falls	O
on	O
the	O
orange	O
side	O
of	O
the	O
boundary	O
will	O
be	O
assigned	O
to	O
the	O
orange	O
class	O
,	O
and	O
similarly	O
an	O
observation	O
on	O
the	O
blue	O
side	O
of	O
the	O
boundary	O
will	O
be	O
assigned	O
to	O
the	O
blue	O
class	O
.	O
the	O
bayes	O
classiﬁer	B
produces	O
the	O
lowest	O
possible	O
test	B
error	O
rate	B
,	O
called	O
the	O
bayes	O
error	B
rate	I
.	O
since	O
the	O
bayes	O
classiﬁer	B
will	O
always	O
choose	O
the	O
class	O
for	O
which	O
(	O
2.10	O
)	O
is	O
largest	O
,	O
the	O
error	B
rate	I
at	O
x	O
=	O
x0	O
will	O
be	O
1−maxj	O
pr	O
(	O
y	O
=	O
j|x	O
=	O
x0	O
)	O
.	O
in	O
general	O
,	O
the	O
overall	O
bayes	O
error	B
rate	I
is	O
given	O
by	O
(	O
cid:11	O
)	O
max	O
(	O
cid:12	O
)	O
pr	O
(	O
y	O
=	O
j|x	O
)	O
1	O
−	O
e	O
j	O
,	O
(	O
2.11	O
)	O
bayes	O
decision	B
boundary	I
bayes	O
error	B
rate	I
k-nearest	O
neighbors	O
2.2	O
assessing	O
model	B
accuracy	O
39	O
where	O
the	O
expectation	O
averages	O
the	O
probability	B
over	O
all	O
possible	O
values	O
of	O
x.	O
for	O
our	O
simulated	O
data	B
,	O
the	O
bayes	O
error	B
rate	I
is	O
0.1304.	O
it	O
is	O
greater	O
than	O
zero	O
,	O
because	O
the	O
classes	O
overlap	O
in	O
the	O
true	O
population	O
so	O
maxj	O
pr	O
(	O
y	O
=	O
j|x	O
=	O
x0	O
)	O
<	O
1	O
for	O
some	O
values	O
of	O
x0	O
.	O
the	O
bayes	O
error	B
rate	I
is	O
analogous	O
to	O
the	O
irreducible	B
error	I
,	O
discussed	O
earlier	O
.	O
k-nearest	O
neighbors	O
in	O
theory	O
we	O
would	O
always	O
like	O
to	O
predict	O
qualitative	B
responses	O
using	O
the	O
bayes	O
classiﬁer	B
.	O
but	O
for	O
real	O
data	B
,	O
we	O
do	O
not	O
know	O
the	O
conditional	O
distri-	O
bution	O
of	O
y	O
given	O
x	O
,	O
and	O
so	O
computing	O
the	O
bayes	O
classiﬁer	B
is	O
impossi-	O
ble	O
.	O
therefore	O
,	O
the	O
bayes	O
classiﬁer	B
serves	O
as	O
an	O
unattainable	O
gold	O
standard	O
against	O
which	O
to	O
compare	O
other	O
methods	O
.	O
many	O
approaches	O
attempt	O
to	O
estimate	O
the	O
conditional	O
distribution	O
of	O
y	O
given	O
x	O
,	O
and	O
then	O
classify	O
a	O
given	O
observation	O
to	O
the	O
class	O
with	O
highest	O
estimated	O
probability	B
.	O
one	O
such	O
method	O
is	O
the	O
k-nearest	O
neighbors	O
(	O
knn	O
)	O
classiﬁer	B
.	O
given	O
a	O
positive	O
in-	O
teger	O
k	O
and	O
a	O
test	B
observation	O
x0	O
,	O
the	O
knn	O
classiﬁer	B
ﬁrst	O
identiﬁes	O
the	O
k	O
points	O
in	O
the	O
training	B
data	O
that	O
are	O
closest	O
to	O
x0	O
,	O
represented	O
by	O
n0	O
.	O
points	O
in	O
n0	O
whose	O
response	B
values	O
equal	O
j	O
:	O
it	O
then	O
estimates	O
the	O
conditional	B
probability	I
for	O
class	O
j	O
as	O
the	O
fraction	O
of	O
pr	O
(	O
y	O
=	O
j|x	O
=	O
x0	O
)	O
=	O
(	O
cid:17	O
)	O
i∈n0	O
1	O
k	O
i	O
(	O
yi	O
=	O
j	O
)	O
.	O
(	O
2.12	O
)	O
finally	O
,	O
knn	O
applies	O
bayes	O
rule	O
and	O
classiﬁes	O
the	O
test	B
observation	O
x0	O
to	O
the	O
class	O
with	O
the	O
largest	O
probability	B
.	O
figure	O
2.14	O
provides	O
an	O
illustrative	O
example	O
of	O
the	O
knn	O
approach	B
.	O
in	O
the	O
left-hand	O
panel	O
,	O
we	O
have	O
plotted	O
a	O
small	O
training	B
data	O
set	B
consisting	O
of	O
six	O
blue	O
and	O
six	O
orange	O
observations	B
.	O
our	O
goal	O
is	O
to	O
make	O
a	O
prediction	B
for	O
the	O
point	O
labeled	O
by	O
the	O
black	O
cross	O
.	O
suppose	O
that	O
we	O
choose	O
k	O
=	O
3.	O
then	O
knn	O
will	O
ﬁrst	O
identify	O
the	O
three	O
observations	B
that	O
are	O
closest	O
to	O
the	O
cross	O
.	O
this	O
neighborhood	O
is	O
shown	O
as	O
a	O
circle	O
.	O
it	O
consists	O
of	O
two	O
blue	O
points	O
and	O
one	O
orange	O
point	O
,	O
resulting	O
in	O
estimated	O
probabilities	O
of	O
2/3	O
for	O
the	O
blue	O
class	O
and	O
1/3	O
for	O
the	O
orange	O
class	O
.	O
hence	O
knn	O
will	O
predict	O
that	O
the	O
black	O
cross	O
belongs	O
to	O
the	O
blue	O
class	O
.	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
2.14	O
we	O
have	O
applied	O
the	O
knn	O
approach	B
with	O
k	O
=	O
3	O
at	O
all	O
of	O
the	O
possible	O
values	O
for	O
x1	O
and	O
x2	B
,	O
and	O
have	O
drawn	O
in	O
the	O
corresponding	O
knn	O
decision	B
boundary	I
.	O
despite	O
the	O
fact	O
that	O
it	O
is	O
a	O
very	O
simple	B
approach	O
,	O
knn	O
can	O
often	O
pro-	O
duce	O
classiﬁers	O
that	O
are	O
surprisingly	O
close	O
to	O
the	O
optimal	O
bayes	O
classiﬁer	B
.	O
figure	O
2.15	O
displays	O
the	O
knn	O
decision	B
boundary	I
,	O
using	O
k	O
=	O
10	O
,	O
when	O
ap-	O
plied	O
to	O
the	O
larger	O
simulated	O
data	B
set	O
from	O
figure	O
2.13.	O
notice	O
that	O
even	O
though	O
the	O
true	O
distribution	O
is	O
not	O
known	O
by	O
the	O
knn	O
classiﬁer	B
,	O
the	O
knn	O
decision	B
boundary	I
is	O
very	O
close	O
to	O
that	O
of	O
the	O
bayes	O
classiﬁer	B
.	O
the	O
test	B
error	O
rate	B
using	O
knn	O
is	O
0.1363	O
,	O
which	O
is	O
close	O
to	O
the	O
bayes	O
error	B
rate	I
of	O
0.1304	O
.	O
40	O
2.	O
statistical	O
learning	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
figure	O
2.14.	O
the	O
knn	O
approach	B
,	O
using	O
k	O
=	O
3	O
,	O
is	O
illustrated	O
in	O
a	O
simple	B
situation	O
with	O
six	O
blue	O
observations	B
and	O
six	O
orange	O
observations	B
.	O
left	O
:	O
a	O
test	B
ob-	O
servation	O
at	O
which	O
a	O
predicted	O
class	O
label	O
is	O
desired	O
is	O
shown	O
as	O
a	O
black	O
cross	O
.	O
the	O
three	O
closest	O
points	O
to	O
the	O
test	B
observation	O
are	O
identiﬁed	O
,	O
and	O
it	O
is	O
predicted	O
that	O
the	O
test	B
observation	O
belongs	O
to	O
the	O
most	O
commonly-occurring	O
class	O
,	O
in	O
this	O
case	O
blue	O
.	O
right	O
:	O
the	O
knn	O
decision	B
boundary	I
for	O
this	O
example	O
is	O
shown	O
in	O
black	O
.	O
the	O
blue	O
grid	O
indicates	O
the	O
region	O
in	O
which	O
a	O
test	B
observation	O
will	O
be	O
assigned	O
to	O
the	O
blue	O
class	O
,	O
and	O
the	O
orange	O
grid	O
indicates	O
the	O
region	O
in	O
which	O
it	O
will	O
be	O
assigned	O
to	O
the	O
orange	O
class	O
.	O
the	O
choice	O
of	O
k	O
has	O
a	O
drastic	O
eﬀect	O
on	O
the	O
knn	O
classiﬁer	B
obtained	O
.	O
figure	O
2.16	O
displays	O
two	O
knn	O
ﬁts	O
to	O
the	O
simulated	O
data	B
from	O
figure	O
2.13	O
,	O
using	O
k	O
=	O
1	O
and	O
k	O
=	O
100.	O
when	O
k	O
=	O
1	O
,	O
the	O
decision	B
boundary	I
is	O
overly	O
ﬂexible	B
and	O
ﬁnds	O
patterns	O
in	O
the	O
data	B
that	O
don	O
’	O
t	O
correspond	O
to	O
the	O
bayes	O
decision	B
boundary	I
.	O
this	O
corresponds	O
to	O
a	O
classiﬁer	B
that	O
has	O
low	O
bias	B
but	O
very	O
high	O
variance	B
.	O
as	O
k	O
grows	O
,	O
the	O
method	O
becomes	O
less	O
ﬂexible	B
and	O
produces	O
a	O
decision	B
boundary	I
that	O
is	O
close	O
to	O
linear	B
.	O
this	O
corresponds	O
to	O
a	O
low-variance	O
but	O
high-bias	O
classiﬁer	B
.	O
on	O
this	O
simulated	O
data	B
set	O
,	O
neither	O
k	O
=	O
1	O
nor	O
k	O
=	O
100	O
give	O
good	O
predictions	O
:	O
they	O
have	O
test	B
error	O
rates	O
of	O
0.1695	O
and	O
0.1925	O
,	O
respectively	O
.	O
just	O
as	O
in	O
the	O
regression	B
setting	O
,	O
there	O
is	O
not	O
a	O
strong	O
relationship	O
be-	O
tween	O
the	O
training	B
error	O
rate	B
and	O
the	O
test	B
error	O
rate	B
.	O
with	O
k	O
=	O
1	O
,	O
the	O
knn	O
training	B
error	O
rate	B
is	O
0	O
,	O
but	O
the	O
test	B
error	O
rate	B
may	O
be	O
quite	O
high	O
.	O
in	O
general	O
,	O
as	O
we	O
use	O
more	O
ﬂexible	B
classiﬁcation	O
methods	O
,	O
the	O
training	B
error	O
rate	B
will	O
decline	O
but	O
the	O
test	B
error	O
rate	B
may	O
not	O
.	O
in	O
figure	O
2.17	O
,	O
we	O
have	O
plotted	O
the	O
knn	O
test	B
and	O
training	B
errors	O
as	O
a	O
function	B
of	O
1/k	O
.	O
as	O
1/k	O
in-	O
creases	O
,	O
the	O
method	O
becomes	O
more	O
ﬂexible	B
.	O
as	O
in	O
the	O
regression	B
setting	O
,	O
the	O
training	B
error	O
rate	B
consistently	O
declines	O
as	O
the	O
ﬂexibility	O
increases	O
.	O
however	O
,	O
the	O
test	B
error	O
exhibits	O
a	O
characteristic	O
u-shape	O
,	O
declining	O
at	O
ﬁrst	O
(	O
with	O
a	O
minimum	O
at	O
approximately	O
k	O
=	O
10	O
)	O
before	O
increasing	O
again	O
when	O
the	O
method	O
becomes	O
excessively	O
ﬂexible	B
and	O
overﬁts	O
.	O
2.2	O
assessing	O
model	B
accuracy	O
41	O
knn	O
:	O
k=10	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
figure	O
2.15.	O
the	O
black	O
curve	O
indicates	O
the	O
knn	O
decision	B
boundary	I
on	O
the	O
data	B
from	O
figure	O
2.13	O
,	O
using	O
k	O
=	O
10.	O
the	O
bayes	O
decision	B
boundary	I
is	O
shown	O
as	O
a	O
purple	O
dashed	O
line	B
.	O
the	O
knn	O
and	O
bayes	O
decision	O
boundaries	O
are	O
very	O
similar	O
.	O
knn	O
:	O
k=1	O
knn	O
:	O
k=100	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
figure	O
2.16.	O
a	O
comparison	O
of	O
the	O
knn	O
decision	O
boundaries	O
(	O
solid	O
black	O
curves	O
)	O
obtained	O
using	O
k	O
=	O
1	O
and	O
k	O
=	O
100	O
on	O
the	O
data	B
from	O
figure	O
2.13.	O
with	O
k	O
=	O
1	O
,	O
the	O
decision	B
boundary	I
is	O
overly	O
ﬂexible	B
,	O
while	O
with	O
k	O
=	O
100	O
it	O
is	O
not	O
suﬃciently	O
ﬂexible	B
.	O
the	O
bayes	O
decision	B
boundary	I
is	O
shown	O
as	O
a	O
purple	O
dashed	O
line	B
.	O
42	O
2.	O
statistical	O
learning	O
e	O
t	O
a	O
r	O
r	O
o	O
r	O
r	O
e	O
0	O
2	O
.	O
0	O
5	O
1	O
.	O
0	O
0	O
1	O
.	O
0	O
5	O
0	O
.	O
0	O
0	O
0	O
.	O
0	O
training	B
errors	O
test	B
errors	O
0.01	O
0.02	O
0.05	O
0.20	O
0.50	O
1.00	O
0.10	O
1/k	O
figure	O
2.17.	O
the	O
knn	O
training	B
error	O
rate	B
(	O
blue	O
,	O
200	O
observations	B
)	O
and	O
test	B
error	O
rate	B
(	O
orange	O
,	O
5,000	O
observations	B
)	O
on	O
the	O
data	B
from	O
figure	O
2.13	O
,	O
as	O
the	O
level	B
of	O
ﬂexibility	O
(	O
assessed	O
using	O
1/k	O
)	O
increases	O
,	O
or	O
equivalently	O
as	O
the	O
number	O
of	O
neighbors	O
k	O
decreases	O
.	O
the	O
black	O
dashed	O
line	B
indicates	O
the	O
bayes	O
error	B
rate	I
.	O
the	O
jumpiness	O
of	O
the	O
curves	O
is	O
due	O
to	O
the	O
small	O
size	O
of	O
the	O
training	B
data	O
set	B
.	O
in	O
both	O
the	O
regression	B
and	O
classiﬁcation	B
settings	O
,	O
choosing	O
the	O
correct	O
level	B
of	O
ﬂexibility	O
is	O
critical	O
to	O
the	O
success	O
of	O
any	O
statistical	O
learning	O
method	O
.	O
the	O
bias-variance	B
tradeoﬀ	O
,	O
and	O
the	O
resulting	O
u-shape	O
in	O
the	O
test	B
error	O
,	O
can	O
make	O
this	O
a	O
diﬃcult	O
task	O
.	O
in	O
chapter	O
5	O
,	O
we	O
return	O
to	O
this	O
topic	O
and	O
discuss	O
various	O
methods	O
for	O
estimating	O
test	B
error	O
rates	O
and	O
thereby	O
choosing	O
the	O
optimal	O
level	O
of	O
ﬂexibility	O
for	O
a	O
given	O
statistical	O
learning	O
method	O
.	O
2.3	O
lab	O
:	O
introduction	O
to	O
r	O
in	O
this	O
lab	O
,	O
we	O
will	O
introduce	O
some	O
simple	B
r	O
commands	O
.	O
the	O
best	O
way	O
to	O
learn	O
a	O
new	O
language	O
is	O
to	O
try	O
out	O
the	O
commands	O
.	O
r	O
can	O
be	O
downloaded	O
from	O
http	O
:	O
//cran.r-project.org/	O
2.3.1	O
basic	O
commands	O
r	O
uses	O
functions	O
to	O
perform	O
operations	O
.	O
to	O
run	O
a	O
function	B
called	O
funcname	O
,	O
we	O
type	O
funcname	O
(	O
input1	O
,	O
input2	O
)	O
,	O
where	O
the	O
inputs	O
(	O
or	O
arguments	O
)	O
input1	O
function	B
argument	O
2.3	O
lab	O
:	O
introduction	O
to	O
r	O
43	O
and	O
input2	O
tell	O
r	O
how	O
to	O
run	O
the	O
function	B
.	O
a	O
function	B
can	O
have	O
any	O
number	O
of	O
inputs	O
.	O
for	O
example	O
,	O
to	O
create	O
a	O
vector	B
of	O
numbers	O
,	O
we	O
use	O
the	O
function	B
c	O
(	O
)	O
(	O
for	O
concatenate	O
)	O
.	O
any	O
numbers	O
inside	O
the	O
parentheses	O
are	O
joined	O
to-	O
gether	O
.	O
the	O
following	O
command	O
instructs	O
r	O
to	O
join	O
together	O
the	O
numbers	O
1	O
,	O
3	O
,	O
2	O
,	O
and	O
5	O
,	O
and	O
to	O
save	O
them	O
as	O
a	O
vector	B
named	O
x.	O
when	O
we	O
type	O
x	O
,	O
it	O
gives	O
us	O
back	O
the	O
vector	B
.	O
c	O
(	O
)	O
vector	B
>	O
x	O
<	O
-	O
c	O
(	O
1	O
,3	O
,2	O
,5	O
)	O
>	O
x	O
[	O
1	O
]	O
1	O
3	O
2	O
5	O
note	O
that	O
the	O
>	O
is	O
not	O
part	O
of	O
the	O
command	O
;	O
rather	O
,	O
it	O
is	O
printed	O
by	O
r	O
to	O
indicate	O
that	O
it	O
is	O
ready	O
for	O
another	O
command	O
to	O
be	O
entered	O
.	O
we	O
can	O
also	O
save	O
things	O
using	O
=	O
rather	O
than	O
<	O
-	O
:	O
>	O
x	O
=	O
c	O
(	O
1	O
,6	O
,2	O
)	O
>	O
x	O
[	O
1	O
]	O
1	O
6	O
2	O
>	O
y	O
=	O
c	O
(	O
1	O
,4	O
,3	O
)	O
hitting	O
the	O
up	O
arrow	O
multiple	B
times	O
will	O
display	O
the	O
previous	O
commands	O
,	O
which	O
can	O
then	O
be	O
edited	O
.	O
this	O
is	O
useful	O
since	O
one	O
often	O
wishes	O
to	O
repeat	O
a	O
similar	O
command	O
.	O
in	O
addition	O
,	O
typing	O
?	O
funcname	O
will	O
always	O
cause	O
r	O
to	O
open	O
a	O
new	O
help	O
ﬁle	O
window	O
with	O
additional	O
information	O
about	O
the	O
function	B
funcname	O
.	O
we	O
can	O
tell	O
r	O
to	O
add	O
two	O
sets	O
of	O
numbers	O
together	O
.	O
it	O
will	O
then	O
add	O
the	O
ﬁrst	O
number	O
from	O
x	O
to	O
the	O
ﬁrst	O
number	O
from	O
y	O
,	O
and	O
so	O
on	O
.	O
however	O
,	O
x	O
and	O
y	O
should	O
be	O
the	O
same	O
length	O
.	O
we	O
can	O
check	O
their	O
length	O
using	O
the	O
length	O
(	O
)	O
function	B
.	O
length	O
(	O
)	O
>	O
length	O
(	O
x	O
)	O
[	O
1	O
]	O
3	O
>	O
length	O
(	O
y	O
)	O
[	O
1	O
]	O
3	O
>	O
x	O
+	O
y	O
[	O
1	O
]	O
2	O
10	O
5	O
the	O
ls	O
(	O
)	O
function	B
allows	O
us	O
to	O
look	O
at	O
a	O
list	O
of	O
all	O
of	O
the	O
objects	O
,	O
such	O
as	O
data	B
and	O
functions	O
,	O
that	O
we	O
have	O
saved	O
so	O
far	O
.	O
the	O
rm	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
delete	O
any	O
that	O
we	O
don	O
’	O
t	O
want	O
.	O
ls	O
(	O
)	O
rm	O
(	O
)	O
>	O
ls	O
(	O
)	O
[	O
1	O
]	O
``	O
x	O
``	O
``	O
y	O
``	O
>	O
rm	O
(	O
x	O
,	O
y	O
)	O
>	O
ls	O
(	O
)	O
character	O
(	O
0	O
)	O
it	O
’	O
s	O
also	O
possible	O
to	O
remove	O
all	O
objects	O
at	O
once	O
:	O
>	O
rm	O
(	O
list	O
=	O
ls	O
(	O
)	O
)	O
44	O
2.	O
statistical	O
learning	O
the	O
matrix	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
create	O
a	O
matrix	O
of	O
numbers	O
.	O
before	O
we	O
use	O
the	O
matrix	O
(	O
)	O
function	B
,	O
we	O
can	O
learn	O
more	O
about	O
it	O
:	O
matrix	O
(	O
)	O
>	O
?	O
matrix	O
the	O
help	O
ﬁle	O
reveals	O
that	O
the	O
matrix	O
(	O
)	O
function	B
takes	O
a	O
number	O
of	O
inputs	O
,	O
but	O
for	O
now	O
we	O
focus	O
on	O
the	O
ﬁrst	O
three	O
:	O
the	O
data	B
(	O
the	O
entries	O
in	O
the	O
matrix	O
)	O
,	O
the	O
number	O
of	O
rows	O
,	O
and	O
the	O
number	O
of	O
columns	O
.	O
first	O
,	O
we	O
create	O
a	O
simple	B
matrix	O
.	O
>	O
x	O
=	O
matrix	O
(	O
data	B
=	O
c	O
(	O
1	O
,2	O
,3	O
,4	O
)	O
,	O
nrow	O
=2	O
,	O
ncol	O
=2	O
)	O
>	O
x	O
[	O
,1	O
]	O
[	O
,2	O
]	O
3	O
4	O
1	O
2	O
[	O
1	O
,	O
]	O
[	O
2	O
,	O
]	O
note	O
that	O
we	O
could	O
just	O
as	O
well	O
omit	O
typing	O
data=	O
,	O
nrow=	O
,	O
and	O
ncol=	O
in	O
the	O
matrix	O
(	O
)	O
command	O
above	O
:	O
that	O
is	O
,	O
we	O
could	O
just	O
type	O
>	O
x	O
=	O
matrix	O
(	O
c	O
(	O
1	O
,2	O
,3	O
,4	O
)	O
,2	O
,2	O
)	O
and	O
this	O
would	O
have	O
the	O
same	O
eﬀect	O
.	O
however	O
,	O
it	O
can	O
sometimes	O
be	O
useful	O
to	O
specify	O
the	O
names	O
of	O
the	O
arguments	O
passed	O
in	O
,	O
since	O
otherwise	O
r	O
will	O
assume	O
that	O
the	O
function	B
arguments	O
are	O
passed	O
into	O
the	O
function	B
in	O
the	O
same	O
order	O
that	O
is	O
given	O
in	O
the	O
function	B
’	O
s	O
help	O
ﬁle	O
.	O
as	O
this	O
example	O
illustrates	O
,	O
by	O
default	O
r	O
creates	O
matrices	O
by	O
successively	O
ﬁlling	O
in	O
columns	O
.	O
alternatively	O
,	O
the	O
byrow=true	O
option	O
can	O
be	O
used	O
to	O
populate	O
the	O
matrix	O
in	O
order	O
of	O
the	O
rows	O
.	O
>	O
matrix	O
(	O
c	O
(	O
1	O
,2	O
,3	O
,4	O
)	O
,2	O
,2	O
,	O
byrow	O
=	O
true	O
)	O
[	O
,1	O
]	O
[	O
,2	O
]	O
2	O
4	O
1	O
3	O
[	O
1	O
,	O
]	O
[	O
2	O
,	O
]	O
notice	O
that	O
in	O
the	O
above	O
command	O
we	O
did	O
not	O
assign	O
the	O
matrix	O
to	O
a	O
value	O
such	O
as	O
x.	O
in	O
this	O
case	O
the	O
matrix	O
is	O
printed	O
to	O
the	O
screen	O
but	O
is	O
not	O
saved	O
for	O
future	O
calculations	O
.	O
the	O
sqrt	O
(	O
)	O
function	B
returns	O
the	O
square	O
root	O
of	O
each	O
element	O
of	O
a	O
vector	B
or	O
matrix	O
.	O
the	O
command	O
x^2	O
raises	O
each	O
element	O
of	O
x	O
to	O
the	O
power	B
2	O
;	O
any	O
powers	O
are	O
possible	O
,	O
including	O
fractional	O
or	O
negative	O
powers	O
.	O
sqrt	O
(	O
)	O
>	O
sqrt	O
(	O
x	O
)	O
[	O
,1	O
]	O
[	O
,2	O
]	O
[	O
1	O
,	O
]	O
1.00	O
1.73	O
[	O
2	O
,	O
]	O
1.41	O
2.00	O
>	O
x	O
^2	O
[	O
,1	O
]	O
[	O
,2	O
]	O
9	O
16	O
1	O
4	O
[	O
1	O
,	O
]	O
[	O
2	O
,	O
]	O
the	O
rnorm	O
(	O
)	O
function	B
generates	O
a	O
vector	B
of	O
random	O
normal	O
variables	O
,	O
with	O
ﬁrst	O
argument	B
n	O
the	O
sample	O
size	O
.	O
each	O
time	O
we	O
call	O
this	O
function	B
,	O
we	O
will	O
get	O
a	O
diﬀerent	O
answer	O
.	O
here	O
we	O
create	O
two	O
correlated	O
sets	O
of	O
numbers	O
,	O
x	O
and	O
y	O
,	O
and	O
use	O
the	O
cor	O
(	O
)	O
function	B
to	O
compute	O
the	O
correlation	B
between	O
them	O
.	O
rnorm	O
(	O
)	O
cor	O
(	O
)	O
2.3	O
lab	O
:	O
introduction	O
to	O
r	O
45	O
>	O
x	O
=	O
rnorm	O
(	O
50	O
)	O
>	O
y	O
=	O
x	O
+	O
rnorm	O
(	O
50	O
,	O
mean	O
=50	O
,	O
sd	O
=.1	O
)	O
>	O
cor	O
(	O
x	O
,	O
y	O
)	O
[	O
1	O
]	O
0.995	O
by	O
default	O
,	O
rnorm	O
(	O
)	O
creates	O
standard	O
normal	O
random	O
variables	O
with	O
a	O
mean	O
of	O
0	O
and	O
a	O
standard	O
deviation	O
of	O
1.	O
however	O
,	O
the	O
mean	O
and	O
standard	O
devi-	O
ation	O
can	O
be	O
altered	O
using	O
the	O
mean	O
and	O
sd	O
arguments	O
,	O
as	O
illustrated	O
above	O
.	O
sometimes	O
we	O
want	O
our	O
code	O
to	O
reproduce	O
the	O
exact	O
same	O
set	B
of	O
random	O
numbers	O
;	O
we	O
can	O
use	O
the	O
set.seed	O
(	O
)	O
function	B
to	O
do	O
this	O
.	O
the	O
set.seed	O
(	O
)	O
function	B
takes	O
an	O
(	O
arbitrary	O
)	O
integer	O
argument	B
.	O
set.seed	O
(	O
)	O
>	O
set	B
.	O
seed	B
(	O
1303	O
)	O
>	O
rnorm	O
(	O
50	O
)	O
[	O
1	O
]	O
-1.1440	O
.	O
.	O
.	O
1.3421	O
2.1854	O
0.5364	O
0.0632	O
0.5022	O
-0.0004	O
we	O
use	O
set.seed	O
(	O
)	O
throughout	O
the	O
labs	O
whenever	O
we	O
perform	O
calculations	O
involving	O
random	O
quantities	O
.	O
in	O
general	O
this	O
should	O
allow	O
the	O
user	O
to	O
re-	O
produce	O
our	O
results	O
.	O
however	O
,	O
it	O
should	O
be	O
noted	O
that	O
as	O
new	O
versions	O
of	O
r	O
become	O
available	O
it	O
is	O
possible	O
that	O
some	O
small	O
discrepancies	O
may	O
form	O
between	O
the	O
book	O
and	O
the	O
output	B
from	O
r.	O
the	O
mean	O
(	O
)	O
and	O
var	O
(	O
)	O
functions	O
can	O
be	O
used	O
to	O
compute	O
the	O
mean	O
and	O
variance	B
of	O
a	O
vector	B
of	O
numbers	O
.	O
applying	O
sqrt	O
(	O
)	O
to	O
the	O
output	B
of	O
var	O
(	O
)	O
will	O
give	O
the	O
standard	O
deviation	O
.	O
or	O
we	O
can	O
simply	O
use	O
the	O
sd	O
(	O
)	O
function	B
.	O
mean	O
(	O
)	O
var	O
(	O
)	O
sd	O
(	O
)	O
>	O
set	B
.	O
seed	B
(	O
3	O
)	O
>	O
y	O
=	O
rnorm	O
(	O
100	O
)	O
>	O
mean	O
(	O
y	O
)	O
[	O
1	O
]	O
0.0110	O
>	O
var	O
(	O
y	O
)	O
[	O
1	O
]	O
0.7329	O
>	O
sqrt	O
(	O
var	O
(	O
y	O
)	O
)	O
[	O
1	O
]	O
0.8561	O
>	O
sd	O
(	O
y	O
)	O
[	O
1	O
]	O
0.8561	O
2.3.2	O
graphics	O
the	O
plot	B
(	O
)	O
function	B
is	O
the	O
primary	O
way	O
to	O
plot	B
data	O
in	O
r.	O
for	O
instance	O
,	O
plot	B
(	O
x	O
,	O
y	O
)	O
produces	O
a	O
scatterplot	B
of	O
the	O
numbers	O
in	O
x	O
versus	O
the	O
numbers	O
in	O
y.	O
there	O
are	O
many	O
additional	O
options	O
that	O
can	O
be	O
passed	O
in	O
to	O
the	O
plot	B
(	O
)	O
function	B
.	O
for	O
example	O
,	O
passing	O
in	O
the	O
argument	B
xlab	O
will	O
result	O
in	O
a	O
label	O
on	O
the	O
x-axis	O
.	O
to	O
ﬁnd	O
out	O
more	O
information	O
about	O
the	O
plot	B
(	O
)	O
function	B
,	O
type	O
?	O
plot	B
.	O
plot	B
(	O
)	O
>	O
x	O
=	O
rnorm	O
(	O
100	O
)	O
>	O
y	O
=	O
rnorm	O
(	O
100	O
)	O
>	O
plot	B
(	O
x	O
,	O
y	O
)	O
>	O
plot	B
(	O
x	O
,	O
y	O
,	O
xlab	O
=	O
''	O
this	O
is	O
the	O
x	O
-	O
axis	O
``	O
,	O
ylab	O
=	O
''	O
this	O
is	O
the	O
y	O
-	O
axis	O
``	O
,	O
main	O
=	O
''	O
plot	B
of	O
x	O
vs	O
y	O
``	O
)	O
46	O
2.	O
statistical	O
learning	O
we	O
will	O
often	O
want	O
to	O
save	O
the	O
output	B
of	O
an	O
r	O
plot	B
.	O
the	O
command	O
that	O
we	O
use	O
to	O
do	O
this	O
will	O
depend	O
on	O
the	O
ﬁle	O
type	O
that	O
we	O
would	O
like	O
to	O
create	O
.	O
for	O
instance	O
,	O
to	O
create	O
a	O
pdf	O
,	O
we	O
use	O
the	O
pdf	O
(	O
)	O
function	B
,	O
and	O
to	O
create	O
a	O
jpeg	O
,	O
we	O
use	O
the	O
jpeg	O
(	O
)	O
function	B
.	O
pdf	O
(	O
)	O
jpeg	O
(	O
)	O
>	O
pdf	O
(	O
``	O
figure	O
.	O
pdf	O
``	O
)	O
>	O
plot	B
(	O
x	O
,	O
y	O
,	O
col	O
=	O
''	O
green	O
``	O
)	O
>	O
dev	O
.	O
off	O
(	O
)	O
null	B
device	O
1	O
the	O
function	B
dev.off	O
(	O
)	O
indicates	O
to	O
r	O
that	O
we	O
are	O
done	O
creating	O
the	O
plot	B
.	O
alternatively	O
,	O
we	O
can	O
simply	O
copy	O
the	O
plot	B
window	O
and	O
paste	O
it	O
into	O
an	O
appropriate	O
ﬁle	O
type	O
,	O
such	O
as	O
a	O
word	O
document	O
.	O
the	O
function	B
seq	O
(	O
)	O
can	O
be	O
used	O
to	O
create	O
a	O
sequence	O
of	O
numbers	O
.	O
for	O
instance	O
,	O
seq	O
(	O
a	O
,	O
b	O
)	O
makes	O
a	O
vector	B
of	O
integers	O
between	O
a	O
and	O
b.	O
there	O
are	O
many	O
other	O
options	O
:	O
for	O
instance	O
,	O
seq	O
(	O
0,1	O
,	O
length=10	O
)	O
makes	O
a	O
sequence	O
of	O
10	O
numbers	O
that	O
are	O
equally	O
spaced	O
between	O
0	O
and	O
1.	O
typing	O
3:11	O
is	O
a	O
shorthand	O
for	O
seq	O
(	O
3,11	O
)	O
for	O
integer	O
arguments	O
.	O
dev.off	O
(	O
)	O
seq	O
(	O
)	O
>	O
x	O
=	O
seq	O
(	O
1	O
,10	O
)	O
>	O
x	O
[	O
1	O
]	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
>	O
x	O
=1:10	O
>	O
x	O
[	O
1	O
]	O
7	O
>	O
x	O
=	O
seq	O
(	O
-	O
pi	O
,	O
pi	O
,	O
length	O
=50	O
)	O
1	O
5	O
2	O
3	O
4	O
6	O
8	O
9	O
10	O
we	O
will	O
now	O
create	O
some	O
more	O
sophisticated	O
plots	O
.	O
the	O
contour	O
(	O
)	O
func-	O
tion	O
produces	O
a	O
contour	B
plot	I
in	O
order	O
to	O
represent	O
three-dimensional	O
data	B
;	O
it	O
is	O
like	O
a	O
topographical	O
map	O
.	O
it	O
takes	O
three	O
arguments	O
:	O
contour	O
(	O
)	O
contour	B
plot	I
1.	O
a	O
vector	B
of	O
the	O
x	O
values	O
(	O
the	O
ﬁrst	O
dimension	O
)	O
,	O
2.	O
a	O
vector	B
of	O
the	O
y	O
values	O
(	O
the	O
second	O
dimension	O
)	O
,	O
and	O
3.	O
a	O
matrix	O
whose	O
elements	O
correspond	O
to	O
the	O
z	O
value	O
(	O
the	O
third	O
dimen-	O
sion	O
)	O
for	O
each	O
pair	O
of	O
(	O
x	O
,	O
y	O
)	O
coordinates	O
.	O
as	O
with	O
the	O
plot	B
(	O
)	O
function	B
,	O
there	O
are	O
many	O
other	O
inputs	O
that	O
can	O
be	O
used	O
to	O
ﬁne-tune	O
the	O
output	B
of	O
the	O
contour	O
(	O
)	O
function	B
.	O
to	O
learn	O
more	O
about	O
these	O
,	O
take	O
a	O
look	O
at	O
the	O
help	O
ﬁle	O
by	O
typing	O
?	O
contour	O
.	O
>	O
y	O
=	O
x	O
>	O
f	O
=	O
outer	O
(	O
x	O
,	O
y	O
,	O
function	B
(	O
x	O
,	O
y	O
)	O
cos	O
(	O
y	O
)	O
/	O
(	O
1+	O
x	O
^2	O
)	O
)	O
>	O
contour	O
(	O
x	O
,	O
y	O
,	O
f	O
)	O
>	O
contour	O
(	O
x	O
,	O
y	O
,	O
f	O
,	O
nlevels	O
=45	O
,	O
add	O
=	O
t	O
)	O
>	O
fa	O
=	O
(	O
f	O
-	O
t	O
(	O
f	O
)	O
)	O
/2	O
>	O
contour	O
(	O
x	O
,	O
y	O
,	O
fa	O
,	O
nlevels	O
=15	O
)	O
the	O
image	O
(	O
)	O
function	B
works	O
the	O
same	O
way	O
as	O
contour	O
(	O
)	O
,	O
except	O
that	O
it	O
produces	O
a	O
color-coded	O
plot	B
whose	O
colors	O
depend	O
on	O
the	O
z	O
value	O
.	O
this	O
is	O
image	O
(	O
)	O
2.3	O
lab	O
:	O
introduction	O
to	O
r	O
47	O
known	O
as	O
a	O
heatmap	B
,	O
and	O
is	O
sometimes	O
used	O
to	O
plot	B
temperature	O
in	O
weather	O
forecasts	O
.	O
alternatively	O
,	O
persp	O
(	O
)	O
can	O
be	O
used	O
to	O
produce	O
a	O
three-dimensional	O
plot	B
.	O
the	O
arguments	O
theta	O
and	O
phi	O
control	O
the	O
angles	O
at	O
which	O
the	O
plot	B
is	O
viewed	O
.	O
heatmap	B
persp	O
(	O
)	O
>	O
image	O
(	O
x	O
,	O
y	O
,	O
fa	O
)	O
>	O
persp	O
(	O
x	O
,	O
y	O
,	O
fa	O
)	O
>	O
persp	O
(	O
x	O
,	O
y	O
,	O
fa	O
,	O
theta	O
=30	O
)	O
>	O
persp	O
(	O
x	O
,	O
y	O
,	O
fa	O
,	O
theta	O
=30	O
,	O
phi	O
=20	O
)	O
>	O
persp	O
(	O
x	O
,	O
y	O
,	O
fa	O
,	O
theta	O
=30	O
,	O
phi	O
=70	O
)	O
>	O
persp	O
(	O
x	O
,	O
y	O
,	O
fa	O
,	O
theta	O
=30	O
,	O
phi	O
=40	O
)	O
2.3.3	O
indexing	O
data	B
we	O
often	O
wish	O
to	O
examine	O
part	O
of	O
a	O
set	B
of	O
data	B
.	O
suppose	O
that	O
our	O
data	B
is	O
stored	O
in	O
the	O
matrix	O
a	O
.	O
>	O
a	O
=	O
matrix	O
(	O
1:16	O
,4	O
,4	O
)	O
>	O
a	O
[	O
,1	O
]	O
[	O
,2	O
]	O
[	O
,3	O
]	O
[	O
,4	O
]	O
13	O
14	O
15	O
16	O
9	O
10	O
11	O
12	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
[	O
1	O
,	O
]	O
[	O
2	O
,	O
]	O
[	O
3	O
,	O
]	O
[	O
4	O
,	O
]	O
then	O
,	O
typing	O
>	O
a	O
[	O
2	O
,3	O
]	O
[	O
1	O
]	O
10	O
will	O
select	O
the	O
element	O
corresponding	O
to	O
the	O
second	O
row	O
and	O
the	O
third	O
col-	O
umn	O
.	O
the	O
ﬁrst	O
number	O
after	O
the	O
open-bracket	O
symbol	O
[	O
always	O
refers	O
to	O
the	O
row	O
,	O
and	O
the	O
second	O
number	O
always	O
refers	O
to	O
the	O
column	O
.	O
we	O
can	O
also	O
select	O
multiple	B
rows	O
and	O
columns	O
at	O
a	O
time	O
,	O
by	O
providing	O
vectors	O
as	O
the	O
indices	O
.	O
>	O
a	O
[	O
c	O
(	O
1	O
,3	O
)	O
,	O
c	O
(	O
2	O
,4	O
)	O
]	O
[	O
,1	O
]	O
[	O
,2	O
]	O
13	O
15	O
[	O
1	O
,	O
]	O
[	O
2	O
,	O
]	O
>	O
a	O
[	O
1:3	O
,2:4	O
]	O
5	O
7	O
[	O
,1	O
]	O
[	O
,2	O
]	O
[	O
,3	O
]	O
13	O
14	O
15	O
9	O
10	O
11	O
[	O
1	O
,	O
]	O
5	O
[	O
2	O
,	O
]	O
6	O
[	O
3	O
,	O
]	O
7	O
>	O
a	O
[	O
1:2	O
,	O
]	O
[	O
,1	O
]	O
[	O
,2	O
]	O
[	O
,3	O
]	O
[	O
,4	O
]	O
13	O
14	O
9	O
10	O
5	O
6	O
1	O
[	O
1	O
,	O
]	O
[	O
2	O
,	O
]	O
2	O
>	O
a	O
[	O
,1:2	O
]	O
[	O
,1	O
]	O
[	O
,2	O
]	O
5	O
6	O
1	O
2	O
[	O
1	O
,	O
]	O
[	O
2	O
,	O
]	O
48	O
2.	O
statistical	O
learning	O
[	O
3	O
,	O
]	O
[	O
4	O
,	O
]	O
3	O
4	O
7	O
8	O
the	O
last	O
two	O
examples	O
include	O
either	O
no	O
index	O
for	O
the	O
columns	O
or	O
no	O
index	O
for	O
the	O
rows	O
.	O
these	O
indicate	O
that	O
r	O
should	O
include	O
all	O
columns	O
or	O
all	O
rows	O
,	O
respectively	O
.	O
r	O
treats	O
a	O
single	B
row	O
or	O
column	O
of	O
a	O
matrix	O
as	O
a	O
vector	B
.	O
>	O
a	O
[	O
1	O
,	O
]	O
[	O
1	O
]	O
1	O
5	O
9	O
13	O
the	O
use	O
of	O
a	O
negative	O
sign	O
-	O
in	O
the	O
index	O
tells	O
r	O
to	O
keep	O
all	O
rows	O
or	O
columns	O
except	O
those	O
indicated	O
in	O
the	O
index	O
.	O
>	O
a	O
[	O
-	O
c	O
(	O
1	O
,3	O
)	O
,	O
]	O
[	O
,1	O
]	O
[	O
,2	O
]	O
[	O
,3	O
]	O
[	O
,4	O
]	O
14	O
16	O
[	O
1	O
,	O
]	O
[	O
2	O
,	O
]	O
>	O
a	O
[	O
-	O
c	O
(	O
1	O
,3	O
)	O
,	O
-c	O
(	O
1	O
,3	O
,4	O
)	O
]	O
[	O
1	O
]	O
6	O
8	O
2	O
4	O
6	O
8	O
10	O
12	O
the	O
dim	O
(	O
)	O
function	B
outputs	O
the	O
number	O
of	O
rows	O
followed	O
by	O
the	O
number	O
of	O
columns	O
of	O
a	O
given	O
matrix	O
.	O
dim	O
(	O
)	O
>	O
dim	O
(	O
a	O
)	O
[	O
1	O
]	O
4	O
4	O
2.3.4	O
loading	O
data	O
for	O
most	O
analyses	O
,	O
the	O
ﬁrst	O
step	O
involves	O
importing	O
a	O
data	B
set	O
into	O
r.	O
the	O
read.table	O
(	O
)	O
function	B
is	O
one	O
of	O
the	O
primary	O
ways	O
to	O
do	O
this	O
.	O
the	O
help	O
ﬁle	O
contains	O
details	O
about	O
how	O
to	O
use	O
this	O
function	B
.	O
we	O
can	O
use	O
the	O
function	B
write.table	O
(	O
)	O
to	O
export	O
data	B
.	O
before	O
attempting	O
to	O
load	O
a	O
data	B
set	O
,	O
we	O
must	O
make	O
sure	O
that	O
r	O
knows	O
to	O
search	O
for	O
the	O
data	B
in	O
the	O
proper	O
directory	O
.	O
for	O
example	O
on	O
a	O
windows	O
system	O
one	O
could	O
select	O
the	O
directory	O
using	O
the	O
change	O
dir	O
.	O
.	O
.	O
option	O
under	O
the	O
file	O
menu	O
.	O
however	O
,	O
the	O
details	O
of	O
how	O
to	O
do	O
this	O
depend	O
on	O
the	O
op-	O
erating	O
system	O
(	O
e.g	O
.	O
windows	O
,	O
mac	O
,	O
unix	O
)	O
that	O
is	O
being	O
used	O
,	O
and	O
so	O
we	O
do	O
not	O
give	O
further	O
details	O
here	O
.	O
we	O
begin	O
by	O
loading	O
in	O
the	O
auto	O
data	B
set	O
.	O
this	O
data	B
is	O
part	O
of	O
the	O
islr	O
library	O
(	O
we	O
discuss	O
libraries	O
in	O
chapter	O
3	O
)	O
but	O
to	O
illustrate	O
the	O
read.table	O
(	O
)	O
function	B
we	O
load	O
it	O
now	O
from	O
a	O
text	O
ﬁle	O
.	O
the	O
following	O
command	O
will	O
load	O
the	O
auto.data	O
ﬁle	O
into	O
r	O
and	O
store	O
it	O
as	O
an	O
object	O
called	O
auto	O
,	O
in	O
a	O
format	O
referred	O
to	O
as	O
a	O
data	B
frame	I
.	O
(	O
the	O
text	O
ﬁle	O
can	O
be	O
obtained	O
from	O
this	O
book	O
’	O
s	O
website	O
.	O
)	O
once	O
the	O
data	B
has	O
been	O
loaded	O
,	O
the	O
fix	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
view	O
it	O
in	O
a	O
spreadsheet	O
like	O
window	O
.	O
however	O
,	O
the	O
window	O
must	O
be	O
closed	O
before	O
further	O
r	O
commands	O
can	O
be	O
entered	O
.	O
read.table	O
(	O
)	O
write	O
.	O
table	O
(	O
)	O
data	B
frame	I
>	O
auto	O
=	O
read	O
.	O
table	O
(	O
``	O
auto	O
.	O
data	B
``	O
)	O
>	O
fix	O
(	O
auto	O
)	O
2.3	O
lab	O
:	O
introduction	O
to	O
r	O
49	O
note	O
that	O
auto.data	O
is	O
simply	O
a	O
text	O
ﬁle	O
,	O
which	O
you	O
could	O
alternatively	O
open	O
on	O
your	O
computer	O
using	O
a	O
standard	O
text	O
editor	O
.	O
it	O
is	O
often	O
a	O
good	O
idea	O
to	O
view	O
a	O
data	B
set	O
using	O
a	O
text	O
editor	O
or	O
other	O
software	O
such	O
as	O
excel	O
before	O
loading	O
it	O
into	O
r.	O
this	O
particular	O
data	B
set	O
has	O
not	O
been	O
loaded	O
correctly	O
,	O
because	O
r	O
has	O
assumed	O
that	O
the	O
variable	B
names	O
are	O
part	O
of	O
the	O
data	B
and	O
so	O
has	O
included	O
them	O
in	O
the	O
ﬁrst	O
row	O
.	O
the	O
data	B
set	O
also	O
includes	O
a	O
number	O
of	O
missing	O
observations	O
,	O
indicated	O
by	O
a	O
question	O
mark	O
?	O
.	O
missing	O
values	O
are	O
a	O
common	O
occurrence	O
in	O
real	O
data	B
sets	O
.	O
using	O
the	O
option	O
header=t	O
(	O
or	O
header=true	O
)	O
in	O
the	O
read.table	O
(	O
)	O
function	B
tells	O
r	O
that	O
the	O
ﬁrst	O
line	B
of	O
the	O
ﬁle	O
contains	O
the	O
variable	B
names	O
,	O
and	O
using	O
the	O
option	O
na.strings	O
tells	O
r	O
that	O
any	O
time	O
it	O
sees	O
a	O
particular	O
character	O
or	O
set	B
of	O
characters	O
(	O
such	O
as	O
a	O
question	O
mark	O
)	O
,	O
it	O
should	O
be	O
treated	O
as	O
a	O
missing	O
element	O
of	O
the	O
data	B
matrix	O
.	O
>	O
auto	O
=	O
read	O
.	O
table	O
(	O
``	O
auto	O
.	O
data	B
``	O
,	O
header	O
=t	O
,	O
na	O
.	O
strings	O
=	O
''	O
?	O
''	O
)	O
>	O
fix	O
(	O
auto	O
)	O
excel	O
is	O
a	O
common-format	O
data	B
storage	O
program	O
.	O
an	O
easy	O
way	O
to	O
load	O
such	O
data	B
into	O
r	O
is	O
to	O
save	O
it	O
as	O
a	O
csv	O
(	O
comma	O
separated	O
value	O
)	O
ﬁle	O
and	O
then	O
use	O
the	O
read.csv	O
(	O
)	O
function	B
to	O
load	O
it	O
in	O
.	O
>	O
auto	O
=	O
read	O
.	O
csv	O
(	O
``	O
auto	O
.	O
csv	O
``	O
,	O
header	O
=t	O
,	O
na	O
.	O
strings	O
=	O
''	O
?	O
''	O
)	O
>	O
fix	O
(	O
auto	O
)	O
>	O
dim	O
(	O
auto	O
)	O
[	O
1	O
]	O
397	O
9	O
>	O
auto	O
[	O
1:4	O
,	O
]	O
the	O
dim	O
(	O
)	O
function	B
tells	O
us	O
that	O
the	O
data	B
has	O
397	O
observations	B
,	O
or	O
rows	O
,	O
and	O
nine	O
variables	O
,	O
or	O
columns	O
.	O
there	O
are	O
various	O
ways	O
to	O
deal	O
with	O
the	O
missing	B
data	I
.	O
in	O
this	O
case	O
,	O
only	O
ﬁve	O
of	O
the	O
rows	O
contain	O
missing	O
observations	O
,	O
and	O
so	O
we	O
choose	O
to	O
use	O
the	O
na.omit	O
(	O
)	O
function	B
to	O
simply	O
remove	O
these	O
rows	O
.	O
>	O
auto	O
=	O
na	O
.	O
omit	O
(	O
auto	O
)	O
>	O
dim	O
(	O
auto	O
)	O
[	O
1	O
]	O
392	O
9	O
once	O
the	O
data	B
are	O
loaded	O
correctly	O
,	O
we	O
can	O
use	O
names	O
(	O
)	O
to	O
check	O
the	O
variable	B
names	O
.	O
>	O
names	O
(	O
auto	O
)	O
[	O
1	O
]	O
``	O
mpg	O
``	O
[	O
5	O
]	O
``	O
weight	O
``	O
[	O
9	O
]	O
``	O
name	O
``	O
''	O
cylinders	O
``	O
''	O
a	O
c	O
c	O
e	O
l	O
e	O
r	O
a	O
t	O
i	O
o	O
n	O
``	O
``	O
year	O
``	O
''	O
d	O
i	O
s	O
p	O
l	O
a	O
c	O
e	O
m	O
e	O
n	O
t	O
``	O
``	O
horsepower	O
``	O
''	O
origin	O
``	O
dim	O
(	O
)	O
na.omit	O
(	O
)	O
names	O
(	O
)	O
2.3.5	O
additional	O
graphical	O
and	O
numerical	O
summaries	O
we	O
can	O
use	O
the	O
plot	B
(	O
)	O
function	B
to	O
produce	O
scatterplots	O
of	O
the	O
quantitative	B
variables	O
.	O
however	O
,	O
simply	O
typing	O
the	O
variable	B
names	O
will	O
produce	O
an	O
error	B
message	O
,	O
because	O
r	O
does	O
not	O
know	O
to	O
look	O
in	O
the	O
auto	O
data	B
set	O
for	O
those	O
variables	O
.	O
scatterplot	B
50	O
2.	O
statistical	O
learning	O
>	O
plot	B
(	O
cylinders	O
,	O
mpg	O
)	O
error	B
in	O
plot	B
(	O
cylinders	O
,	O
mpg	O
)	O
:	O
object	O
’	O
cylinders	O
’	O
not	O
found	O
to	O
refer	O
to	O
a	O
variable	B
,	O
we	O
must	O
type	O
the	O
data	B
set	O
and	O
the	O
variable	B
name	O
joined	O
with	O
a	O
$	O
symbol	O
.	O
alternatively	O
,	O
we	O
can	O
use	O
the	O
attach	O
(	O
)	O
function	B
in	O
order	O
to	O
tell	O
r	O
to	O
make	O
the	O
variables	O
in	O
this	O
data	B
frame	I
available	O
by	O
name	O
.	O
attach	O
(	O
)	O
>	O
plot	B
(	O
auto	O
$	O
cylinders	O
,	O
auto	O
$	O
mpg	O
)	O
>	O
attach	O
(	O
auto	O
)	O
>	O
plot	B
(	O
cylinders	O
,	O
mpg	O
)	O
the	O
cylinders	O
variable	B
is	O
stored	O
as	O
a	O
numeric	O
vector	B
,	O
so	O
r	O
has	O
treated	O
it	O
as	O
quantitative	B
.	O
however	O
,	O
since	O
there	O
are	O
only	O
a	O
small	O
number	O
of	O
possible	O
values	O
for	O
cylinders	O
,	O
one	O
may	O
prefer	O
to	O
treat	O
it	O
as	O
a	O
qualitative	B
variable	O
.	O
the	O
as.factor	O
(	O
)	O
function	B
converts	O
quantitative	B
variables	O
into	O
qualitative	B
variables	O
.	O
as.factor	O
(	O
)	O
>	O
cylinders	O
=	O
as	O
.	O
factor	B
(	O
cylinders	O
)	O
if	O
the	O
variable	B
plotted	O
on	O
the	O
x-axis	O
is	O
categorial	O
,	O
then	O
boxplots	O
will	O
automatically	O
be	O
produced	O
by	O
the	O
plot	B
(	O
)	O
function	B
.	O
as	O
usual	O
,	O
a	O
number	O
of	O
options	O
can	O
be	O
speciﬁed	O
in	O
order	O
to	O
customize	O
the	O
plots	O
.	O
boxplot	B
>	O
plot	B
(	O
cylinders	O
,	O
mpg	O
)	O
>	O
plot	B
(	O
cylinders	O
,	O
mpg	O
,	O
col	O
=	O
''	O
red	O
``	O
)	O
>	O
plot	B
(	O
cylinders	O
,	O
mpg	O
,	O
col	O
=	O
''	O
red	O
``	O
,	O
varwidth	O
=	O
t	O
)	O
>	O
plot	B
(	O
cylinders	O
,	O
mpg	O
,	O
col	O
=	O
''	O
red	O
``	O
,	O
varwidth	O
=t	O
,	O
horizontal	O
=	O
t	O
)	O
>	O
plot	B
(	O
cylinders	O
,	O
mpg	O
,	O
col	O
=	O
''	O
red	O
``	O
,	O
varwidth	O
=t	O
,	O
xlab	O
=	O
''	O
cylinders	O
``	O
,	O
ylab	O
=	O
''	O
mpg	O
``	O
)	O
the	O
hist	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
plot	B
a	O
histogram	B
.	O
note	O
that	O
col=2	O
has	O
the	O
same	O
eﬀect	O
as	O
col=	O
''	O
red	O
''	O
.	O
>	O
hist	O
(	O
mpg	O
)	O
>	O
hist	O
(	O
mpg	O
,	O
col	O
=2	O
)	O
>	O
hist	O
(	O
mpg	O
,	O
col	O
=2	O
,	O
breaks	O
=15	O
)	O
hist	O
(	O
)	O
histogram	B
the	O
pairs	O
(	O
)	O
function	B
creates	O
a	O
scatterplot	B
matrix	I
i.e	O
.	O
a	O
scatterplot	B
for	O
every	O
pair	O
of	O
variables	O
for	O
any	O
given	O
data	B
set	O
.	O
we	O
can	O
also	O
produce	O
scatterplots	O
for	O
just	O
a	O
subset	O
of	O
the	O
variables	O
.	O
scatterplot	B
matrix	I
>	O
pairs	O
(	O
auto	O
)	O
>	O
pairs	O
(	O
∼	O
mpg	O
+	O
d	O
i	O
s	O
p	O
l	O
a	O
c	O
e	O
m	O
e	O
n	O
t	O
+	O
horsepowe	O
r	O
+	O
weight	O
+	O
acceleration	O
,	O
auto	O
)	O
in	O
conjunction	O
with	O
the	O
plot	B
(	O
)	O
function	B
,	O
identify	O
(	O
)	O
provides	O
a	O
useful	O
interactive	O
method	O
for	O
identifying	O
the	O
value	O
for	O
a	O
particular	O
variable	B
for	O
points	O
on	O
a	O
plot	B
.	O
we	O
pass	O
in	O
three	O
arguments	O
to	O
identify	O
(	O
)	O
:	O
the	O
x-axis	O
variable	B
,	O
the	O
y-axis	O
variable	B
,	O
and	O
the	O
variable	B
whose	O
values	O
we	O
would	O
like	O
to	O
see	O
printed	O
for	O
each	O
point	O
.	O
then	O
clicking	O
on	O
a	O
given	O
point	O
in	O
the	O
plot	B
will	O
cause	O
r	O
to	O
print	O
the	O
value	O
of	O
the	O
variable	B
of	O
interest	O
.	O
right-clicking	O
on	O
the	O
plot	B
will	O
exit	O
the	O
identify	O
(	O
)	O
function	B
(	O
control-click	O
on	O
a	O
mac	O
)	O
.	O
the	O
numbers	O
printed	O
under	O
the	O
identify	O
(	O
)	O
function	B
correspond	O
to	O
the	O
rows	O
for	O
the	O
selected	O
points	O
.	O
identify	O
(	O
)	O
>	O
plot	B
(	O
horsepower	O
,	O
mpg	O
)	O
>	O
identify	O
(	O
horsepower	O
,	O
mpg	O
,	O
name	O
)	O
2.3	O
lab	O
:	O
introduction	O
to	O
r	O
51	O
the	O
summary	O
(	O
)	O
function	B
produces	O
a	O
numerical	O
summary	O
of	O
each	O
variable	B
in	O
a	O
particular	O
data	B
set	O
.	O
summary	O
(	O
)	O
>	O
summary	O
(	O
auto	O
)	O
mpg	O
min	O
.	O
:	O
9.00	O
1	O
st	O
qu	O
.	O
:17.00	O
median	O
:22.75	O
mean	O
:23.45	O
3	O
rd	O
qu	O
.	O
:29.00	O
:46.60	O
max	O
.	O
horsepower	O
min	O
.	O
:	O
46.0	O
1	O
st	O
qu	O
.	O
:	O
75.0	O
median	O
:	O
93.5	O
mean	O
:104.5	O
3	O
rd	O
qu	O
.	O
:126.0	O
max	O
.	O
:230.0	O
cylinders	O
min	O
.	O
:3.000	O
1	O
st	O
qu	O
.	O
:4.000	O
median	O
:4.000	O
mean	O
:5.472	O
3	O
rd	O
qu	O
.	O
:8.000	O
:8.000	O
max	O
.	O
d	O
i	O
s	O
p	O
l	O
a	O
c	O
e	O
m	O
e	O
n	O
t	O
min	O
.	O
:	O
68.0	O
1	O
st	O
qu	O
.	O
:105.0	O
median	O
:151.0	O
mean	O
:194.4	O
3	O
rd	O
qu	O
.	O
:275.8	O
:455.0	O
max	O
.	O
weight	O
min	O
.	O
:1613	O
1	O
st	O
qu	O
.	O
:2225	O
median	O
:2804	O
mean	O
:2978	O
3	O
rd	O
qu	O
.	O
:3615	O
max	O
.	O
:5140	O
a	O
c	O
c	O
e	O
l	O
e	O
r	O
a	O
t	O
i	O
o	O
n	O
min	O
.	O
:	O
8.00	O
1	O
st	O
qu	O
.	O
:13.78	O
median	O
:15.50	O
mean	O
:15.54	O
3	O
rd	O
qu	O
.	O
:17.02	O
max	O
.	O
:24.80	O
year	O
origin	O
name	O
min	O
.	O
:70.00	O
1	O
st	O
qu	O
.	O
:73.00	O
median	O
:76.00	O
mean	O
:75.98	O
3	O
rd	O
qu	O
.	O
:79.00	O
max	O
.	O
:82.00	O
min	O
.	O
:1.000	O
1	O
st	O
qu	O
.	O
:1.000	O
median	O
:1.000	O
mean	O
:1.577	O
3	O
rd	O
qu	O
.	O
:2.000	O
max	O
.	O
:3.000	O
:	O
amc	O
matador	O
:	O
ford	O
pinto	O
:	O
toyota	O
corolla	O
:	O
amc	O
gremlin	O
amc	O
hornet	O
:	O
chevrolet	O
chevette	O
:	O
(	O
other	O
)	O
5	O
5	O
5	O
4	O
4	O
4	O
:365	O
for	O
qualitative	O
variables	O
such	O
as	O
name	O
,	O
r	O
will	O
list	O
the	O
number	O
of	O
observations	B
that	O
fall	O
in	O
each	O
category	O
.	O
we	O
can	O
also	O
produce	O
a	O
summary	O
of	O
just	O
a	O
single	B
variable	O
.	O
>	O
summary	O
(	O
mpg	O
)	O
min	O
.	O
1	O
st	O
qu	O
.	O
9.00	O
17.00	O
median	O
22.75	O
mean	O
3	O
rd	O
qu	O
.	O
29.00	O
23.45	O
max	O
.	O
46.60	O
once	O
we	O
have	O
ﬁnished	O
using	O
r	O
,	O
we	O
type	O
q	O
(	O
)	O
in	O
order	O
to	O
shut	O
it	O
down	O
,	O
or	O
quit	O
.	O
when	O
exiting	O
r	O
,	O
we	O
have	O
the	O
option	O
to	O
save	O
the	O
current	O
workspace	B
so	O
that	O
all	O
objects	O
(	O
such	O
as	O
data	B
sets	O
)	O
that	O
we	O
have	O
created	O
in	O
this	O
r	O
session	O
will	O
be	O
available	O
next	O
time	O
.	O
before	O
exiting	O
r	O
,	O
we	O
may	O
want	O
to	O
save	O
a	O
record	O
of	O
all	O
of	O
the	O
commands	O
that	O
we	O
typed	O
in	O
the	O
most	O
recent	O
session	O
;	O
this	O
can	O
be	O
accomplished	O
using	O
the	O
savehistory	O
(	O
)	O
function	B
.	O
next	O
time	O
we	O
enter	O
r	O
,	O
we	O
can	O
load	O
that	O
history	O
using	O
the	O
loadhistory	O
(	O
)	O
function	B
.	O
q	O
(	O
)	O
workspace	B
savehistory	O
(	O
)	O
loadhistory	O
(	O
)	O
52	O
2.	O
statistical	O
learning	O
2.4	O
exercises	O
conceptual	O
1.	O
for	O
each	O
of	O
parts	O
(	O
a	O
)	O
through	O
(	O
d	O
)	O
,	O
indicate	O
whether	O
we	O
would	O
generally	O
expect	O
the	O
performance	O
of	O
a	O
ﬂexible	B
statistical	O
learning	O
method	O
to	O
be	O
better	O
or	O
worse	O
than	O
an	O
inﬂexible	O
method	O
.	O
justify	O
your	O
answer	O
.	O
(	O
a	O
)	O
the	O
sample	O
size	O
n	O
is	O
extremely	O
large	O
,	O
and	O
the	O
number	O
of	O
predic-	O
tors	O
p	O
is	O
small	O
.	O
(	O
b	O
)	O
the	O
number	O
of	O
predictors	O
p	O
is	O
extremely	O
large	O
,	O
and	O
the	O
number	O
of	O
observations	B
n	O
is	O
small	O
.	O
(	O
c	O
)	O
the	O
relationship	O
between	O
the	O
predictors	O
and	O
response	B
is	O
highly	O
non-linear	B
.	O
(	O
d	O
)	O
the	O
variance	B
of	O
the	O
error	B
terms	O
,	O
i.e	O
.	O
σ2	O
=	O
var	O
(	O
	O
)	O
,	O
is	O
extremely	O
high	O
.	O
2.	O
explain	O
whether	O
each	O
scenario	O
is	O
a	O
classiﬁcation	B
or	O
regression	B
prob-	O
lem	O
,	O
and	O
indicate	O
whether	O
we	O
are	O
most	O
interested	O
in	O
inference	B
or	O
pre-	O
diction	O
.	O
finally	O
,	O
provide	O
n	O
and	O
p.	O
(	O
a	O
)	O
we	O
collect	O
a	O
set	B
of	O
data	B
on	O
the	O
top	O
500	O
ﬁrms	O
in	O
the	O
us	O
.	O
for	O
each	O
ﬁrm	O
we	O
record	O
proﬁt	O
,	O
number	O
of	O
employees	O
,	O
industry	O
and	O
the	O
ceo	O
salary	O
.	O
we	O
are	O
interested	O
in	O
understanding	O
which	O
factors	O
aﬀect	O
ceo	O
salary	O
.	O
(	O
b	O
)	O
we	O
are	O
considering	O
launching	O
a	O
new	O
product	O
and	O
wish	O
to	O
know	O
whether	O
it	O
will	O
be	O
a	O
success	O
or	O
a	O
failure	O
.	O
we	O
collect	O
data	B
on	O
20	O
similar	O
products	O
that	O
were	O
previously	O
launched	O
.	O
for	O
each	O
prod-	O
uct	O
we	O
have	O
recorded	O
whether	O
it	O
was	O
a	O
success	O
or	O
failure	O
,	O
price	O
charged	O
for	O
the	O
product	O
,	O
marketing	O
budget	O
,	O
competition	O
price	O
,	O
and	O
ten	O
other	O
variables	O
.	O
(	O
c	O
)	O
we	O
are	O
interest	O
ed	O
in	O
predicting	O
the	O
%	O
change	O
in	O
the	O
usd/euro	O
exchange	O
rate	B
in	O
relation	O
to	O
the	O
weekly	O
changes	O
in	O
the	O
world	O
stock	O
markets	O
.	O
hence	O
we	O
collect	O
weekly	O
data	B
for	O
all	O
of	O
2012.	O
for	O
each	O
week	O
we	O
record	O
the	O
%	O
change	O
in	O
the	O
usd/euro	O
,	O
the	O
%	O
change	O
in	O
the	O
us	O
market	O
,	O
the	O
%	O
change	O
in	O
the	O
british	O
market	O
,	O
and	O
the	O
%	O
change	O
in	O
the	O
german	O
market	O
.	O
3.	O
we	O
now	O
revisit	O
the	O
bias-variance	B
decomposition	O
.	O
(	O
a	O
)	O
provide	O
a	O
sketch	O
of	O
typical	O
(	O
squared	O
)	O
bias	B
,	O
variance	B
,	O
training	B
er-	O
ror	O
,	O
test	B
error	O
,	O
and	O
bayes	O
(	O
or	O
irreducible	B
)	O
error	B
curves	O
,	O
on	O
a	O
sin-	O
gle	O
plot	B
,	O
as	O
we	O
go	O
from	O
less	O
ﬂexible	B
statistical	O
learning	O
methods	O
towards	O
more	O
ﬂexible	B
approaches	O
.	O
the	O
x-axis	O
should	O
represent	O
2.4	O
exercises	O
53	O
the	O
amount	O
of	O
ﬂexibility	O
in	O
the	O
method	O
,	O
and	O
the	O
y-axis	O
should	O
represent	O
the	O
values	O
for	O
each	O
curve	O
.	O
there	O
should	O
be	O
ﬁve	O
curves	O
.	O
make	O
sure	O
to	O
label	O
each	O
one	O
.	O
(	O
b	O
)	O
explain	O
why	O
each	O
of	O
the	O
ﬁve	O
curves	O
has	O
the	O
shape	O
displayed	O
in	O
part	O
(	O
a	O
)	O
.	O
4.	O
you	O
will	O
now	O
think	O
of	O
some	O
real-life	O
applications	O
for	O
statistical	O
learn-	O
ing	O
.	O
(	O
a	O
)	O
describe	O
three	O
real-life	O
applications	O
in	O
which	O
classiﬁcation	B
might	O
be	O
useful	O
.	O
describe	O
the	O
response	B
,	O
as	O
well	O
as	O
the	O
predictors	O
.	O
is	O
the	O
goal	O
of	O
each	O
application	O
inference	B
or	O
prediction	B
?	O
explain	O
your	O
answer	O
.	O
(	O
b	O
)	O
describe	O
three	O
real-life	O
applications	O
in	O
which	O
regression	B
might	O
be	O
useful	O
.	O
describe	O
the	O
response	B
,	O
as	O
well	O
as	O
the	O
predictors	O
.	O
is	O
the	O
goal	O
of	O
each	O
application	O
inference	B
or	O
prediction	B
?	O
explain	O
your	O
answer	O
.	O
(	O
c	O
)	O
describe	O
three	O
real-life	O
applications	O
in	O
which	O
cluster	B
analysis	I
might	O
be	O
useful	O
.	O
5.	O
what	O
are	O
the	O
advantages	O
and	O
disadvantages	O
of	O
a	O
very	O
ﬂexible	B
(	O
versus	O
a	O
less	O
ﬂexible	B
)	O
approach	B
for	O
regression	B
or	O
classiﬁcation	B
?	O
under	O
what	O
circumstances	O
might	O
a	O
more	O
ﬂexible	B
approach	O
be	O
preferred	O
to	O
a	O
less	O
ﬂexible	B
approach	O
?	O
when	O
might	O
a	O
less	O
ﬂexible	B
approach	O
be	O
preferred	O
?	O
6.	O
describe	O
the	O
diﬀerences	O
between	O
a	O
parametric	B
and	O
a	O
non-parametric	B
statistical	O
learning	O
approach	B
.	O
what	O
are	O
the	O
advantages	O
of	O
a	O
para-	O
metric	O
approach	B
to	O
regression	B
or	O
classiﬁcation	B
(	O
as	O
opposed	O
to	O
a	O
non-	O
parametric	B
approach	O
)	O
?	O
what	O
are	O
its	O
disadvantages	O
?	O
7.	O
the	O
table	O
below	O
provides	O
a	O
training	B
data	O
set	B
containing	O
six	O
observa-	O
tions	O
,	O
three	O
predictors	O
,	O
and	O
one	O
qualitative	B
response	O
variable	B
.	O
obs	O
.	O
x1	O
x2	B
x3	O
1	O
2	O
3	O
4	O
5	O
6	O
0	O
2	O
0	O
0	O
−1	O
1	O
3	O
0	O
1	O
1	O
0	O
1	O
y	O
0	O
red	O
0	O
red	O
3	O
red	O
2	O
green	O
1	O
green	O
1	O
red	O
suppose	O
we	O
wish	O
to	O
use	O
this	O
data	B
set	O
to	O
make	O
a	O
prediction	B
for	O
y	O
when	O
x1	O
=	O
x2	B
=	O
x3	O
=	O
0	O
using	O
k-nearest	O
neighbors	O
.	O
(	O
a	O
)	O
compute	O
the	O
euclidean	O
distance	B
between	O
each	O
observation	O
and	O
the	O
test	B
point	O
,	O
x1	O
=	O
x2	B
=	O
x3	O
=	O
0	O
.	O
54	O
2.	O
statistical	O
learning	O
(	O
b	O
)	O
what	O
is	O
our	O
prediction	B
with	O
k	O
=	O
1	O
?	O
why	O
?	O
(	O
c	O
)	O
what	O
is	O
our	O
prediction	B
with	O
k	O
=	O
3	O
?	O
why	O
?	O
(	O
d	O
)	O
if	O
the	O
bayes	O
decision	B
boundary	I
in	O
this	O
problem	O
is	O
highly	O
non-	O
linear	B
,	O
then	O
would	O
we	O
expect	O
the	O
best	O
value	O
for	O
k	O
to	O
be	O
large	O
or	O
small	O
?	O
why	O
?	O
applied	O
8.	O
this	O
exercise	O
relates	O
to	O
the	O
college	O
data	B
set	O
,	O
which	O
can	O
be	O
found	O
in	O
the	O
ﬁle	O
college.csv	O
.	O
it	O
contains	O
a	O
number	O
of	O
variables	O
for	O
777	O
diﬀerent	O
universities	O
and	O
colleges	O
in	O
the	O
us	O
.	O
the	O
variables	O
are	O
•	O
private	O
:	O
public/private	O
indicator	B
•	O
apps	O
:	O
number	O
of	O
applications	O
received	O
•	O
accept	O
:	O
number	O
of	O
applicants	O
accepted	O
•	O
enroll	O
:	O
number	O
of	O
new	O
students	O
enrolled	O
•	O
top10perc	O
:	O
new	O
students	O
from	O
top	O
10	O
%	O
of	O
high	O
school	O
class	O
•	O
top25perc	O
:	O
new	O
students	O
from	O
top	O
25	O
%	O
of	O
high	O
school	O
class	O
•	O
f.undergrad	O
:	O
number	O
of	O
full-time	O
undergraduates	O
•	O
p.undergrad	O
:	O
number	O
of	O
part-time	O
undergraduates	O
•	O
outstate	O
:	O
out-of-state	O
tuition	O
•	O
room.board	O
:	O
room	O
and	O
board	O
costs	O
•	O
books	O
:	O
estimated	O
book	O
costs	O
•	O
personal	O
:	O
estimated	O
personal	O
spending	O
•	O
phd	O
:	O
percent	O
of	O
faculty	O
with	O
ph.d.	O
’	O
s	O
•	O
terminal	B
:	O
percent	O
of	O
faculty	O
with	O
terminal	B
degree	O
•	O
s.f.ratio	O
:	O
student/faculty	O
ratio	O
•	O
perc.alumni	O
:	O
percent	O
of	O
alumni	O
who	O
donate	O
•	O
expend	O
:	O
instructional	O
expenditure	O
per	O
student	O
•	O
grad.rate	O
:	O
graduation	O
rate	B
before	O
reading	O
the	O
data	B
into	O
r	O
,	O
it	O
can	O
be	O
viewed	O
in	O
excel	O
or	O
a	O
text	O
editor	O
.	O
(	O
a	O
)	O
use	O
the	O
read.csv	O
(	O
)	O
function	B
to	O
read	O
the	O
data	B
into	O
r.	O
call	O
the	O
loaded	O
data	B
college	O
.	O
make	O
sure	O
that	O
you	O
have	O
the	O
directory	O
set	B
to	O
the	O
correct	O
location	O
for	O
the	O
data	B
.	O
(	O
b	O
)	O
look	O
at	O
the	O
data	B
using	O
the	O
fix	O
(	O
)	O
function	B
.	O
you	O
should	O
notice	O
that	O
the	O
ﬁrst	O
column	O
is	O
just	O
the	O
name	O
of	O
each	O
university	O
.	O
we	O
don	O
’	O
t	O
really	O
want	O
r	O
to	O
treat	O
this	O
as	O
data	B
.	O
however	O
,	O
it	O
may	O
be	O
handy	O
to	O
have	O
these	O
names	O
for	O
later	O
.	O
try	O
the	O
following	O
commands	O
:	O
2.4	O
exercises	O
55	O
>	O
rownames	O
(	O
college	O
)	O
=	O
college	O
[	O
,1	O
]	O
>	O
fix	O
(	O
college	O
)	O
you	O
should	O
see	O
that	O
there	O
is	O
now	O
a	O
row.names	O
column	O
with	O
the	O
name	O
of	O
each	O
university	O
recorded	O
.	O
this	O
means	O
that	O
r	O
has	O
given	O
each	O
row	O
a	O
name	O
corresponding	O
to	O
the	O
appropriate	O
university	O
.	O
r	O
will	O
not	O
try	O
to	O
perform	O
calculations	O
on	O
the	O
row	O
names	O
.	O
however	O
,	O
we	O
still	O
need	O
to	O
eliminate	O
the	O
ﬁrst	O
column	O
in	O
the	O
data	B
where	O
the	O
names	O
are	O
stored	O
.	O
try	O
>	O
college	O
=	O
college	O
[	O
,	O
-1	O
]	O
>	O
fix	O
(	O
college	O
)	O
now	O
you	O
should	O
see	O
that	O
the	O
ﬁrst	O
data	B
column	O
is	O
private	O
.	O
note	O
that	O
another	O
column	O
labeled	O
row.names	O
now	O
appears	O
before	O
the	O
private	O
column	O
.	O
however	O
,	O
this	O
is	O
not	O
a	O
data	B
column	O
but	O
rather	O
the	O
name	O
that	O
r	O
is	O
giving	O
to	O
each	O
row	O
.	O
(	O
c	O
)	O
i.	O
use	O
the	O
summary	O
(	O
)	O
function	B
to	O
produce	O
a	O
numerical	O
summary	O
of	O
the	O
variables	O
in	O
the	O
data	B
set	O
.	O
ii	O
.	O
use	O
the	O
pairs	O
(	O
)	O
function	B
to	O
produce	O
a	O
scatterplot	B
matrix	I
of	O
the	O
ﬁrst	O
ten	O
columns	O
or	O
variables	O
of	O
the	O
data	B
.	O
recall	B
that	O
you	O
can	O
reference	O
the	O
ﬁrst	O
ten	O
columns	O
of	O
a	O
matrix	O
a	O
using	O
a	O
[	O
,1:10	O
]	O
.	O
iii	O
.	O
use	O
the	O
plot	B
(	O
)	O
function	B
to	O
produce	O
side-by-side	O
boxplots	O
of	O
outstate	O
versus	O
private	O
.	O
iv	O
.	O
create	O
a	O
new	O
qualitative	B
variable	O
,	O
called	O
elite	O
,	O
by	O
binning	O
the	O
top10perc	O
variable	B
.	O
we	O
are	O
going	O
to	O
divide	O
universities	O
into	O
two	O
groups	O
based	O
on	O
whether	O
or	O
not	O
the	O
proportion	O
of	O
students	O
coming	O
from	O
the	O
top	O
10	O
%	O
of	O
their	O
high	O
school	O
classes	O
exceeds	O
50	O
%	O
.	O
>	O
elite	O
=	O
rep	O
(	O
``	O
no	O
``	O
,	O
nrow	O
(	O
college	O
)	O
)	O
>	O
elite	O
[	O
college	O
$	O
top1	O
0	O
pe	O
rc	O
>	O
50	O
]	O
=	O
''	O
yes	O
``	O
>	O
elite	O
=	O
as	O
.	O
factor	B
(	O
elite	O
)	O
>	O
college	O
=	O
data	B
.	O
frame	O
(	O
college	O
,	O
elite	O
)	O
use	O
the	O
summary	O
(	O
)	O
function	B
to	O
see	O
how	O
many	O
elite	O
univer-	O
sities	O
there	O
are	O
.	O
now	O
use	O
the	O
plot	B
(	O
)	O
function	B
to	O
produce	O
side-by-side	O
boxplots	O
of	O
outstate	O
versus	O
elite	O
.	O
v.	O
use	O
the	O
hist	O
(	O
)	O
function	B
to	O
produce	O
some	O
histograms	O
with	O
diﬀering	O
numbers	O
of	O
bins	O
for	O
a	O
few	O
of	O
the	O
quantitative	B
vari-	O
ables	O
.	O
you	O
may	O
ﬁnd	O
the	O
command	O
par	O
(	O
mfrow=c	O
(	O
2,2	O
)	O
)	O
useful	O
:	O
it	O
will	O
divide	O
the	O
print	O
window	O
into	O
four	O
regions	O
so	O
that	O
four	O
plots	O
can	O
be	O
made	O
simultaneously	O
.	O
modifying	O
the	O
arguments	O
to	O
this	O
function	B
will	O
divide	O
the	O
screen	O
in	O
other	O
ways	O
.	O
vi	O
.	O
continue	O
exploring	O
the	O
data	B
,	O
and	O
provide	O
a	O
brief	O
summary	O
of	O
what	O
you	O
discover	O
.	O
56	O
2.	O
statistical	O
learning	O
9.	O
this	O
exercise	O
involves	O
the	O
auto	O
data	B
set	O
studied	O
in	O
the	O
lab	O
.	O
make	O
sure	O
that	O
the	O
missing	O
values	O
have	O
been	O
removed	O
from	O
the	O
data	B
.	O
(	O
a	O
)	O
which	O
of	O
the	O
predictors	O
are	O
quantitative	B
,	O
and	O
which	O
are	O
quali-	O
tative	O
?	O
(	O
b	O
)	O
what	O
is	O
the	O
range	O
of	O
each	O
quantitative	B
predictor	O
?	O
you	O
can	O
an-	O
swer	O
this	O
using	O
the	O
range	O
(	O
)	O
function	B
.	O
(	O
c	O
)	O
what	O
is	O
the	O
mean	O
and	O
standard	O
deviation	O
of	O
each	O
quantitative	B
range	O
(	O
)	O
predictor	B
?	O
(	O
d	O
)	O
now	O
remove	O
the	O
10th	O
through	O
85th	O
observations	B
.	O
what	O
is	O
the	O
range	O
,	O
mean	O
,	O
and	O
standard	O
deviation	O
of	O
each	O
predictor	B
in	O
the	O
subset	O
of	O
the	O
data	B
that	O
remains	O
?	O
(	O
e	O
)	O
using	O
the	O
full	O
data	B
set	O
,	O
investigate	O
the	O
predictors	O
graphically	O
,	O
using	O
scatterplots	O
or	O
other	O
tools	O
of	O
your	O
choice	O
.	O
create	O
some	O
plots	O
highlighting	O
the	O
relationships	O
among	O
the	O
predictors	O
.	O
comment	O
on	O
your	O
ﬁndings	O
.	O
(	O
f	O
)	O
suppose	O
that	O
we	O
wish	O
to	O
predict	O
gas	O
mileage	O
(	O
mpg	O
)	O
on	O
the	O
basis	B
of	O
the	O
other	O
variables	O
.	O
do	O
your	O
plots	O
suggest	O
that	O
any	O
of	O
the	O
other	O
variables	O
might	O
be	O
useful	O
in	O
predicting	O
mpg	O
?	O
justify	O
your	O
answer	O
.	O
10.	O
this	O
exercise	O
involves	O
the	O
boston	O
housing	O
data	B
set	O
.	O
(	O
a	O
)	O
to	O
begin	O
,	O
load	O
in	O
the	O
boston	O
data	B
set	O
.	O
the	O
boston	O
data	B
set	O
is	O
part	O
of	O
the	O
mass	O
library	O
in	O
r.	O
>	O
library	O
(	O
mass	O
)	O
now	O
the	O
data	B
set	O
is	O
contained	O
in	O
the	O
object	O
boston	O
.	O
>	O
boston	O
read	O
about	O
the	O
data	B
set	O
:	O
>	O
?	O
boston	O
how	O
many	O
rows	O
are	O
in	O
this	O
data	B
set	O
?	O
how	O
many	O
columns	O
?	O
what	O
do	O
the	O
rows	O
and	O
columns	O
represent	O
?	O
(	O
b	O
)	O
make	O
some	O
pairwise	O
scatterplots	O
of	O
the	O
predictors	O
(	O
columns	O
)	O
in	O
this	O
data	B
set	O
.	O
describe	O
your	O
ﬁndings	O
.	O
(	O
c	O
)	O
are	O
any	O
of	O
the	O
predictors	O
associated	O
with	O
per	O
capita	O
crime	O
rate	B
?	O
if	O
so	O
,	O
explain	O
the	O
relationship	O
.	O
(	O
d	O
)	O
do	O
any	O
of	O
the	O
suburbs	O
of	O
boston	O
appear	O
to	O
have	O
particularly	O
high	O
crime	O
rates	O
?	O
tax	O
rates	O
?	O
pupil-teacher	O
ratios	O
?	O
comment	O
on	O
the	O
range	O
of	O
each	O
predictor	B
.	O
(	O
e	O
)	O
how	O
many	O
of	O
the	O
suburbs	O
in	O
this	O
data	B
set	O
bound	O
the	O
charles	O
river	O
?	O
2.4	O
exercises	O
57	O
(	O
f	O
)	O
what	O
is	O
the	O
median	O
pupil-teacher	O
ratio	O
among	O
the	O
towns	O
in	O
this	O
data	B
set	O
?	O
(	O
g	O
)	O
which	O
suburb	O
of	O
boston	O
has	O
lowest	O
median	O
value	O
of	O
owner-	O
occupied	O
homes	O
?	O
what	O
are	O
the	O
values	O
of	O
the	O
other	O
predictors	O
for	O
that	O
suburb	O
,	O
and	O
how	O
do	O
those	O
values	O
compare	O
to	O
the	O
overall	O
ranges	O
for	O
those	O
predictors	O
?	O
comment	O
on	O
your	O
ﬁndings	O
.	O
(	O
h	O
)	O
in	O
this	O
data	B
set	O
,	O
how	O
many	O
of	O
the	O
suburbs	O
average	B
more	O
than	O
seven	O
rooms	O
per	O
dwelling	O
?	O
more	O
than	O
eight	O
rooms	O
per	O
dwelling	O
?	O
comment	O
on	O
the	O
suburbs	O
that	O
average	B
more	O
than	O
eight	O
rooms	O
per	O
dwelling	O
.	O
3	O
linear	B
regression	I
this	O
chapter	O
is	O
about	O
linear	B
regression	I
,	O
a	O
very	O
simple	B
approach	O
for	O
supervised	O
learning	O
.	O
in	O
particular	O
,	O
linear	B
regression	I
is	O
a	O
useful	O
tool	O
for	O
pre-	O
dicting	O
a	O
quantitative	B
response	O
.	O
linear	B
regression	I
has	O
been	O
around	O
for	O
a	O
long	O
time	O
and	O
is	O
the	O
topic	O
of	O
innumerable	O
textbooks	O
.	O
though	O
it	O
may	O
seem	O
somewhat	O
dull	O
compared	O
to	O
some	O
of	O
the	O
more	O
modern	O
statistical	O
learning	O
approaches	O
described	O
in	O
later	O
chapters	O
of	O
this	O
book	O
,	O
linear	B
regression	I
is	O
still	O
a	O
useful	O
and	O
widely	O
used	O
statistical	O
learning	O
method	O
.	O
moreover	O
,	O
it	O
serves	O
as	O
a	O
good	O
jumping-oﬀ	O
point	O
for	O
newer	O
approaches	O
:	O
as	O
we	O
will	O
see	O
in	O
later	O
chapters	O
,	O
many	O
fancy	O
statistical	O
learning	O
approaches	O
can	O
be	O
seen	O
as	O
gener-	O
alizations	O
or	O
extensions	O
of	O
linear	B
regression	I
.	O
consequently	O
,	O
the	O
importance	B
of	O
having	O
a	O
good	O
understanding	O
of	O
linear	B
regression	I
before	O
studying	O
more	O
complex	O
learning	O
methods	O
can	O
not	O
be	O
overstated	O
.	O
in	O
this	O
chapter	O
,	O
we	O
review	O
some	O
of	O
the	O
key	O
ideas	O
underlying	O
the	O
linear	B
regression	I
model	O
,	O
as	O
well	O
as	O
the	O
least	B
squares	I
approach	O
that	O
is	O
most	O
commonly	O
used	O
to	O
ﬁt	B
this	O
model	B
.	O
recall	B
the	O
advertising	O
data	B
from	O
chapter	O
2.	O
figure	O
2.1	O
displays	O
sales	O
(	O
in	O
thousands	O
of	O
units	O
)	O
for	O
a	O
particular	O
product	O
as	O
a	O
function	B
of	O
advertis-	O
ing	O
budgets	O
(	O
in	O
thousands	O
of	O
dollars	O
)	O
for	O
tv	O
,	O
radio	O
,	O
and	O
newspaper	O
media	O
.	O
suppose	O
that	O
in	O
our	O
role	O
as	O
statistical	O
consultants	O
we	O
are	O
asked	O
to	O
suggest	O
,	O
on	O
the	O
basis	B
of	O
this	O
data	B
,	O
a	O
marketing	O
plan	O
for	O
next	O
year	O
that	O
will	O
result	O
in	O
high	O
product	O
sales	O
.	O
what	O
information	O
would	O
be	O
useful	O
in	O
order	O
to	O
provide	O
such	O
a	O
recommendation	O
?	O
here	O
are	O
a	O
few	O
important	O
questions	O
that	O
we	O
might	O
seek	O
to	O
address	O
:	O
1.	O
is	O
there	O
a	O
relationship	O
between	O
advertising	O
budget	O
and	O
sales	O
?	O
our	O
ﬁrst	O
goal	O
should	O
be	O
to	O
determine	O
whether	O
the	O
data	B
provide	O
g.	O
james	O
et	O
al.	O
,	O
an	O
introduction	O
to	O
statistical	O
learning	O
:	O
with	O
applications	O
in	O
r	O
,	O
springer	O
texts	O
in	O
statistics	O
,	O
doi	O
10.1007/978-1-4614-7138-7	O
3	O
,	O
©	O
springer	O
science+business	O
media	O
new	O
york	O
2013	O
59	O
60	O
3.	O
linear	B
regression	I
evidence	O
of	O
an	O
association	O
between	O
advertising	O
expenditure	O
and	O
sales	O
.	O
if	O
the	O
evidence	O
is	O
weak	O
,	O
then	O
one	O
might	O
argue	O
that	O
no	O
money	O
should	O
be	O
spent	O
on	O
advertising	O
!	O
2.	O
how	O
strong	O
is	O
the	O
relationship	O
between	O
advertising	O
budget	O
and	O
sales	O
?	O
assuming	O
that	O
there	O
is	O
a	O
relationship	O
between	O
advertising	O
and	O
sales	O
,	O
we	O
would	O
like	O
to	O
know	O
the	O
strength	O
of	O
this	O
relationship	O
.	O
in	O
other	O
words	O
,	O
given	O
a	O
certain	O
advertising	O
budget	O
,	O
can	O
we	O
predict	O
sales	O
with	O
a	O
high	O
level	B
of	O
accuracy	O
?	O
this	O
would	O
be	O
a	O
strong	O
relationship	O
.	O
or	O
is	O
a	O
prediction	B
of	O
sales	O
based	O
on	O
advertising	O
expenditure	O
only	O
slightly	O
better	O
than	O
a	O
random	O
guess	O
?	O
this	O
would	O
be	O
a	O
weak	O
relationship	O
.	O
3.	O
which	O
media	O
contribute	O
to	O
sales	O
?	O
do	O
all	O
three	O
media—tv	O
,	O
radio	O
,	O
and	O
newspaper—contribute	O
to	O
sales	O
,	O
or	O
do	O
just	O
one	O
or	O
two	O
of	O
the	O
media	O
contribute	O
?	O
to	O
answer	O
this	O
question	O
,	O
we	O
must	O
ﬁnd	O
a	O
way	O
to	O
separate	O
out	O
the	O
individual	O
eﬀects	O
of	O
each	O
medium	O
when	O
we	O
have	O
spent	O
money	O
on	O
all	O
three	O
media	O
.	O
4.	O
how	O
accurately	O
can	O
we	O
estimate	O
the	O
eﬀect	O
of	O
each	O
medium	O
on	O
sales	O
?	O
for	O
every	O
dollar	O
spent	O
on	O
advertising	O
in	O
a	O
particular	O
medium	O
,	O
by	O
what	O
amount	O
will	O
sales	O
increase	O
?	O
how	O
accurately	O
can	O
we	O
predict	O
this	O
amount	O
of	O
increase	O
?	O
5.	O
how	O
accurately	O
can	O
we	O
predict	O
future	O
sales	O
?	O
for	O
any	O
given	O
level	B
of	O
television	O
,	O
radio	O
,	O
or	O
newspaper	O
advertising	O
,	O
what	O
is	O
our	O
prediction	B
for	O
sales	O
,	O
and	O
what	O
is	O
the	O
accuracy	O
of	O
this	O
prediction	B
?	O
6.	O
is	O
the	O
relationship	O
linear	B
?	O
if	O
there	O
is	O
approximately	O
a	O
straight-line	O
relationship	O
between	O
advertis-	O
ing	O
expenditure	O
in	O
the	O
various	O
media	O
and	O
sales	O
,	O
then	O
linear	B
regression	I
is	O
an	O
appropriate	O
tool	O
.	O
if	O
not	O
,	O
then	O
it	O
may	O
still	O
be	O
possible	O
to	O
trans-	O
form	O
the	O
predictor	B
or	O
the	O
response	B
so	O
that	O
linear	B
regression	I
can	O
be	O
used	O
.	O
7.	O
is	O
there	O
synergy	B
among	O
the	O
advertising	O
media	O
?	O
perhaps	O
spending	O
$	O
50,000	O
on	O
television	O
advertising	O
and	O
$	O
50,000	O
on	O
radio	O
advertising	O
results	O
in	O
more	O
sales	O
than	O
allocating	O
$	O
100,000	O
to	O
either	O
television	O
or	O
radio	O
individually	O
.	O
in	O
marketing	O
,	O
this	O
is	O
known	O
as	O
a	O
synergy	B
eﬀect	O
,	O
while	O
in	O
statistics	O
it	O
is	O
called	O
an	O
interaction	B
eﬀect	O
.	O
it	O
turns	O
out	O
that	O
linear	B
regression	I
can	O
be	O
used	O
to	O
answer	O
each	O
of	O
these	O
questions	O
.	O
we	O
will	O
ﬁrst	O
discuss	O
all	O
of	O
these	O
questions	O
in	O
a	O
general	O
context	O
,	O
and	O
then	O
return	O
to	O
them	O
in	O
this	O
speciﬁc	O
context	O
in	O
section	O
3.4.	O
synergy	B
interaction	O
3.1	O
simple	B
linear	O
regression	B
3.1	O
simple	B
linear	O
regression	B
61	O
simple	B
linear	O
regression	B
lives	O
up	O
to	O
its	O
name	O
:	O
it	O
is	O
a	O
very	O
straightforward	O
approach	B
for	O
predicting	O
a	O
quantitative	B
response	O
y	O
on	O
the	O
basis	B
of	O
a	O
sin-	O
gle	O
predictor	B
variable	O
x.	O
it	O
assumes	O
that	O
there	O
is	O
approximately	O
a	O
linear	B
relationship	O
between	O
x	O
and	O
y	O
.	O
mathematically	O
,	O
we	O
can	O
write	O
this	O
linear	B
relationship	O
as	O
(	O
3.1	O
)	O
you	O
might	O
read	O
“	O
≈	O
”	O
as	O
“	O
is	O
approximately	O
modeled	O
as	O
”	O
.	O
we	O
will	O
sometimes	O
describe	O
(	O
3.1	O
)	O
by	O
saying	O
that	O
we	O
are	O
regressing	O
y	O
on	O
x	O
(	O
or	O
y	O
onto	O
x	O
)	O
.	O
for	O
example	O
,	O
x	O
may	O
represent	O
tv	O
advertising	O
and	O
y	O
may	O
represent	O
sales	O
.	O
then	O
we	O
can	O
regress	O
sales	O
onto	O
tv	O
by	O
ﬁtting	O
the	O
model	B
y	O
≈	O
β0	O
+	O
β1x	O
.	O
sales	O
≈	O
β0	O
+	O
β1	O
×	O
tv	O
.	O
simple	B
linear	O
regression	B
in	O
equation	O
3.1	O
,	O
β0	O
and	O
β1	O
are	O
two	O
unknown	O
constants	O
that	O
represent	O
the	O
intercept	B
and	O
slope	B
terms	O
in	O
the	O
linear	B
model	I
.	O
together	O
,	O
β0	O
and	O
β1	O
are	O
known	O
as	O
the	O
model	B
coeﬃcients	O
or	O
parameters	O
.	O
once	O
we	O
have	O
used	O
our	O
training	B
data	O
to	O
produce	O
estimates	O
ˆβ0	O
and	O
ˆβ1	O
for	O
the	O
model	B
coeﬃcients	O
,	O
we	O
can	O
predict	O
future	O
sales	O
on	O
the	O
basis	B
of	O
a	O
particular	O
value	O
of	O
tv	O
advertising	O
by	O
computing	O
intercept	B
slope	O
coeﬃcient	B
parameter	O
ˆy	O
=	O
ˆβ0	O
+	O
ˆβ1x	O
,	O
(	O
3.2	O
)	O
where	O
ˆy	O
indicates	O
a	O
prediction	B
of	O
y	O
on	O
the	O
basis	B
of	O
x	O
=	O
x.	O
here	O
we	O
use	O
a	O
hat	O
symbol	O
,	O
ˆ	O
,	O
to	O
denote	O
the	O
estimated	O
value	O
for	O
an	O
unknown	O
parameter	B
or	O
coeﬃcient	B
,	O
or	O
to	O
denote	O
the	O
predicted	O
value	O
of	O
the	O
response	B
.	O
3.1.1	O
estimating	O
the	O
coeﬃcients	O
in	O
practice	O
,	O
β0	O
and	O
β1	O
are	O
unknown	O
.	O
so	O
before	O
we	O
can	O
use	O
(	O
3.1	O
)	O
to	O
make	O
predictions	O
,	O
we	O
must	O
use	O
data	B
to	O
estimate	O
the	O
coeﬃcients	O
.	O
let	O
(	O
x1	O
,	O
y1	O
)	O
,	O
(	O
x2	B
,	O
y2	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
represent	O
n	O
observation	O
pairs	O
,	O
each	O
of	O
which	O
consists	O
of	O
a	O
measurement	O
of	O
x	O
and	O
a	O
measurement	O
of	O
y	O
.	O
in	O
the	O
advertising	O
example	O
,	O
this	O
data	B
set	O
consists	O
of	O
the	O
tv	O
advertising	O
budget	O
and	O
product	O
sales	O
in	O
n	O
=	O
200	O
diﬀerent	O
markets	O
.	O
(	O
recall	B
that	O
the	O
data	B
are	O
displayed	O
in	O
figure	O
2.1	O
.	O
)	O
our	O
goal	O
is	O
to	O
obtain	O
coeﬃcient	B
estimates	O
ˆβ0	O
and	O
ˆβ1	O
such	O
that	O
the	O
linear	B
model	I
(	O
3.1	O
)	O
ﬁts	O
the	O
available	O
data	B
well—that	O
is	O
,	O
so	O
that	O
yi	O
≈	O
ˆβ0	O
+	O
ˆβ1xi	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
in	O
other	O
words	O
,	O
we	O
want	O
to	O
ﬁnd	O
an	O
intercept	B
ˆβ0	O
and	O
a	O
slope	B
ˆβ1	O
such	O
that	O
the	O
resulting	O
line	B
is	O
as	O
close	O
as	O
possible	O
to	O
the	O
n	O
=	O
200	O
data	B
points	O
.	O
there	O
are	O
a	O
number	O
of	O
ways	O
of	O
measuring	O
closeness	O
.	O
however	O
,	O
by	O
far	O
the	O
most	O
common	O
approach	B
involves	O
minimizing	O
the	O
least	B
squares	I
criterion	O
,	O
and	O
we	O
take	O
that	O
approach	B
in	O
this	O
chapter	O
.	O
alternative	O
approaches	O
will	O
be	O
considered	O
in	O
chapter	O
6.	O
least	B
squares	I
62	O
3.	O
linear	B
regression	I
5	O
2	O
0	O
2	O
5	O
1	O
0	O
1	O
5	O
l	O
s	O
e	O
a	O
s	O
0	O
50	O
100	O
150	O
tv	O
200	O
250	O
300	O
figure	O
3.1.	O
for	O
the	O
advertising	O
data	B
,	O
the	O
least	B
squares	I
ﬁt	O
for	O
the	O
regression	B
of	O
sales	O
onto	O
tv	O
is	O
shown	O
.	O
the	O
ﬁt	B
is	O
found	O
by	O
minimizing	O
the	O
sum	O
of	O
squared	O
errors	O
.	O
each	O
grey	O
line	B
segment	O
represents	O
an	O
error	B
,	O
and	O
the	O
ﬁt	B
makes	O
a	O
compro-	O
mise	O
by	O
averaging	O
their	O
squares	O
.	O
in	O
this	O
case	O
a	O
linear	B
ﬁt	O
captures	O
the	O
essence	O
of	O
the	O
relationship	O
,	O
although	O
it	O
is	O
somewhat	O
deﬁcient	O
in	O
the	O
left	O
of	O
the	O
plot	B
.	O
let	O
ˆyi	O
=	O
ˆβ0	O
+	O
ˆβ1xi	O
be	O
the	O
prediction	B
for	O
y	O
based	O
on	O
the	O
ith	O
value	O
of	O
x.	O
then	O
ei	O
=	O
yi	O
−	O
ˆyi	O
represents	O
the	O
ith	O
residual—this	O
is	O
the	O
diﬀerence	O
between	O
the	O
ith	O
observed	O
response	B
value	O
and	O
the	O
ith	O
response	B
value	O
that	O
is	O
predicted	O
by	O
our	O
linear	B
model	I
.	O
we	O
deﬁne	O
the	O
residual	B
sum	O
of	O
squares	O
(	O
rss	O
)	O
as	O
rss	O
=	O
e2	O
1	O
+	O
e2	O
2	O
+	O
···	O
+	O
e2	O
n	O
,	O
or	O
equivalently	O
as	O
residual	B
residual	O
sum	B
of	I
squares	I
rss	O
=	O
(	O
y1−	O
ˆβ0−	O
ˆβ1x1	O
)	O
2	O
+	O
(	O
y2−	O
ˆβ0−	O
ˆβ1x2	O
)	O
2	O
+	O
.	O
.	O
.+	O
(	O
yn−	O
ˆβ0−	O
ˆβ1xn	O
)	O
2	O
.	O
(	O
3.3	O
)	O
the	O
least	B
squares	I
approach	O
chooses	O
ˆβ0	O
and	O
ˆβ1	O
to	O
minimize	O
the	O
rss	O
.	O
using	O
some	O
calculus	O
,	O
one	O
can	O
show	O
that	O
the	O
minimizers	O
are	O
where	O
¯y	O
≡	O
1	O
n	O
i=1	O
xi	O
are	O
the	O
sample	O
means	O
.	O
in	O
other	O
words	O
,	O
(	O
3.4	O
)	O
deﬁnes	O
the	O
least	B
squares	I
coeﬃcient	O
estimates	O
for	O
simple	O
linear	B
regression	I
.	O
i=1	O
yi	O
and	O
¯x	O
≡	O
1	O
n	O
n	O
n	O
figure	O
3.1	O
displays	O
the	O
simple	B
linear	O
regression	B
ﬁt	O
to	O
the	O
advertising	O
data	B
,	O
where	O
ˆβ0	O
=	O
7.03	O
and	O
ˆβ1	O
=	O
0.0475.	O
in	O
other	O
words	O
,	O
according	O
to	O
(	O
cid:10	O
)	O
n	O
i=1	O
(	O
xi	O
−	O
¯x	O
)	O
(	O
yi	O
−	O
¯y	O
)	O
(	O
cid:10	O
)	O
ˆβ1	O
=	O
i=1	O
(	O
xi	O
−	O
¯x	O
)	O
2	O
ˆβ0	O
=	O
¯y	O
−	O
ˆβ1	O
¯x	O
,	O
(	O
cid:10	O
)	O
n	O
(	O
cid:10	O
)	O
,	O
(	O
3.4	O
)	O
3.1	O
simple	B
linear	O
regression	B
63	O
3	O
3	O
1	O
β	O
6	O
0	O
.	O
0	O
5	O
0	O
.	O
0	O
4	O
0	O
.	O
0	O
3	O
0	O
.	O
0	O
3	O
5	O
6	O
2.5	O
2.15	O
2.2	O
2.3	O
7	O
β0	O
r	O
s	O
s	O
3	O
8	O
9	O
β0	O
β1	O
figure	O
3.2.	O
contour	O
and	O
three-dimensional	O
plots	O
of	O
the	O
rss	O
on	O
the	O
advertising	O
data	B
,	O
using	O
sales	O
as	O
the	O
response	B
and	O
tv	O
as	O
the	O
predictor	B
.	O
the	O
red	O
dots	O
correspond	O
to	O
the	O
least	B
squares	I
estimates	O
ˆβ0	O
and	O
ˆβ1	O
,	O
given	O
by	O
(	O
3.4	O
)	O
.	O
this	O
approximation	O
,	O
an	O
additional	O
$	O
1,000	O
spent	O
on	O
tv	O
advertising	O
is	O
asso-	O
ciated	O
with	O
selling	O
approximately	O
47.5	O
additional	O
units	O
of	O
the	O
product	O
.	O
in	O
figure	O
3.2	O
,	O
we	O
have	O
computed	O
rss	O
for	O
a	O
number	O
of	O
values	O
of	O
β0	O
and	O
β1	O
,	O
using	O
the	O
advertising	O
data	B
with	O
sales	O
as	O
the	O
response	B
and	O
tv	O
as	O
the	O
predic-	O
tor	O
.	O
in	O
each	O
plot	B
,	O
the	O
red	O
dot	O
represents	O
the	O
pair	O
of	O
least	B
squares	I
estimates	O
(	O
ˆβ0	O
,	O
ˆβ1	O
)	O
given	O
by	O
(	O
3.4	O
)	O
.	O
these	O
values	O
clearly	O
minimize	O
the	O
rss	O
.	O
3.1.2	O
assessing	O
the	O
accuracy	O
of	O
the	O
coeﬃcient	B
estimates	O
recall	B
from	O
(	O
2.1	O
)	O
that	O
we	O
assume	O
that	O
the	O
true	O
relationship	O
between	O
x	O
and	O
y	O
takes	O
the	O
form	O
y	O
=	O
f	O
(	O
x	O
)	O
+	O
	O
for	O
some	O
unknown	O
function	B
f	O
,	O
where	O
	O
is	O
a	O
mean-zero	O
random	O
error	O
term	B
.	O
if	O
f	O
is	O
to	O
be	O
approximated	O
by	O
a	O
linear	B
function	O
,	O
then	O
we	O
can	O
write	O
this	O
relationship	O
as	O
y	O
=	O
β0	O
+	O
β1x	O
+	O
	O
.	O
(	O
3.5	O
)	O
here	O
β0	O
is	O
the	O
intercept	B
term—that	O
is	O
,	O
the	O
expected	B
value	I
of	O
y	O
when	O
x	O
=	O
0	O
,	O
and	O
β1	O
is	O
the	O
slope—the	O
average	B
increase	O
in	O
y	O
associated	O
with	O
a	O
one-unit	O
increase	O
in	O
x.	O
the	O
error	B
term	O
is	O
a	O
catch-all	O
for	O
what	O
we	O
miss	O
with	O
this	O
simple	B
model	O
:	O
the	O
true	O
relationship	O
is	O
probably	O
not	O
linear	B
,	O
there	O
may	O
be	O
other	O
variables	O
that	O
cause	O
variation	O
in	O
y	O
,	O
and	O
there	O
may	O
be	O
measurement	O
error	B
.	O
we	O
typically	O
assume	O
that	O
the	O
error	B
term	O
is	O
independent	B
of	O
x.	O
the	O
model	B
given	O
by	O
(	O
3.5	O
)	O
deﬁnes	O
the	O
population	B
regression	I
line	I
,	O
which	O
is	O
the	O
best	O
linear	O
approximation	O
to	O
the	O
true	O
relationship	O
between	O
x	O
and	O
y	O
.1	O
the	O
least	B
squares	I
regression	O
coeﬃcient	B
estimates	O
(	O
3.4	O
)	O
characterize	O
the	O
least	B
squares	I
line	O
(	O
3.2	O
)	O
.	O
the	O
left-hand	O
panel	O
of	O
figure	O
3.3	O
displays	O
these	O
1the	O
assumption	O
of	O
linearity	O
is	O
often	O
a	O
useful	O
working	O
model	B
.	O
however	O
,	O
despite	O
what	O
many	O
textbooks	O
might	O
tell	O
us	O
,	O
we	O
seldom	O
believe	O
that	O
the	O
true	O
relationship	O
is	O
linear	B
.	O
population	B
regression	I
line	I
least	O
squares	O
line	B
64	O
3.	O
linear	B
regression	I
y	O
0	O
1	O
5	O
0	O
5	O
−	O
0	O
1	O
−	O
y	O
0	O
1	O
5	O
0	O
5	O
−	O
0	O
1	O
−	O
−2	O
−1	O
0	O
x	O
1	O
2	O
−2	O
−1	O
1	O
2	O
0	O
x	O
figure	O
3.3.	O
a	O
simulated	O
data	B
set	O
.	O
left	O
:	O
the	O
red	O
line	B
represents	O
the	O
true	O
rela-	O
tionship	O
,	O
f	O
(	O
x	O
)	O
=	O
2	O
+	O
3x	O
,	O
which	O
is	O
known	O
as	O
the	O
population	B
regression	I
line	I
.	O
the	O
blue	O
line	B
is	O
the	O
least	B
squares	I
line	O
;	O
it	O
is	O
the	O
least	B
squares	I
estimate	O
for	O
f	O
(	O
x	O
)	O
based	O
on	O
the	O
observed	O
data	B
,	O
shown	O
in	O
black	O
.	O
right	O
:	O
the	O
population	B
regression	I
line	I
is	O
again	O
shown	O
in	O
red	O
,	O
and	O
the	O
least	B
squares	I
line	O
in	O
dark	O
blue	O
.	O
in	O
light	O
blue	O
,	O
ten	O
least	B
squares	I
lines	O
are	O
shown	O
,	O
each	O
computed	O
on	O
the	O
basis	B
of	O
a	O
separate	O
random	O
set	O
of	O
observations	B
.	O
each	O
least	B
squares	I
line	O
is	O
diﬀerent	O
,	O
but	O
on	O
average	B
,	O
the	O
least	B
squares	I
lines	O
are	O
quite	O
close	O
to	O
the	O
population	B
regression	I
line	I
.	O
two	O
lines	O
in	O
a	O
simple	B
simulated	O
example	O
.	O
we	O
created	O
100	O
random	O
xs	O
,	O
and	O
generated	O
100	O
corresponding	O
y	O
s	O
from	O
the	O
model	B
y	O
=	O
2	O
+	O
3x	O
+	O
	O
,	O
(	O
3.6	O
)	O
where	O
	O
was	O
generated	O
from	O
a	O
normal	O
distribution	O
with	O
mean	O
zero	O
.	O
the	O
red	O
line	B
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
3.3	O
displays	O
the	O
true	O
relationship	O
,	O
f	O
(	O
x	O
)	O
=	O
2	O
+	O
3x	O
,	O
while	O
the	O
blue	O
line	B
is	O
the	O
least	B
squares	I
estimate	O
based	O
on	O
the	O
observed	O
data	B
.	O
the	O
true	O
relationship	O
is	O
generally	O
not	O
known	O
for	O
real	O
data	B
,	O
but	O
the	O
least	B
squares	I
line	O
can	O
always	O
be	O
computed	O
using	O
the	O
coeﬃcient	B
estimates	O
given	O
in	O
(	O
3.4	O
)	O
.	O
in	O
other	O
words	O
,	O
in	O
real	O
applications	O
,	O
we	O
have	O
access	O
to	O
a	O
set	B
of	O
observations	B
from	O
which	O
we	O
can	O
compute	O
the	O
least	B
squares	I
line	O
;	O
however	O
,	O
the	O
population	B
regression	I
line	I
is	O
unobserved	O
.	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
3.3	O
we	O
have	O
generated	O
ten	O
diﬀerent	O
data	B
sets	O
from	O
the	O
model	B
given	O
by	O
(	O
3.6	O
)	O
and	O
plotted	O
the	O
corresponding	O
ten	O
least	B
squares	I
lines	O
.	O
notice	O
that	O
diﬀerent	O
data	B
sets	O
generated	O
from	O
the	O
same	O
true	O
model	O
result	O
in	O
slightly	O
diﬀerent	O
least	B
squares	I
lines	O
,	O
but	O
the	O
unobserved	O
population	B
regression	I
line	I
does	O
not	O
change	O
.	O
at	O
ﬁrst	O
glance	O
,	O
the	O
diﬀerence	O
between	O
the	O
population	B
regression	I
line	I
and	O
the	O
least	B
squares	I
line	O
may	O
seem	O
subtle	O
and	O
confusing	O
.	O
we	O
only	O
have	O
one	O
data	B
set	O
,	O
and	O
so	O
what	O
does	O
it	O
mean	O
that	O
two	O
diﬀerent	O
lines	O
describe	O
the	O
relationship	O
between	O
the	O
predictor	B
and	O
the	O
response	B
?	O
fundamentally	O
,	O
the	O
3.1	O
simple	B
linear	O
regression	B
65	O
concept	O
of	O
these	O
two	O
lines	O
is	O
a	O
natural	B
extension	O
of	O
the	O
standard	O
statistical	O
approach	B
of	O
using	O
information	O
from	O
a	O
sample	O
to	O
estimate	O
characteristics	O
of	O
a	O
large	O
population	O
.	O
for	O
example	O
,	O
suppose	O
that	O
we	O
are	O
interested	O
in	O
knowing	O
the	O
population	O
mean	O
μ	O
of	O
some	O
random	O
variable	O
y	O
.	O
unfortunately	O
,	O
μ	O
is	O
unknown	O
,	O
but	O
we	O
do	O
have	O
access	O
to	O
n	O
observations	B
from	O
y	O
,	O
which	O
we	O
can	O
write	O
as	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
,	O
and	O
which	O
we	O
can	O
use	O
to	O
estimate	O
μ.	O
a	O
reasonable	O
n	O
estimate	O
is	O
ˆμ	O
=	O
¯y	O
,	O
where	O
¯y	O
=	O
1	O
i=1	O
yi	O
is	O
the	O
sample	O
mean	O
.	O
the	O
sample	O
n	O
mean	O
and	O
the	O
population	O
mean	O
are	O
diﬀerent	O
,	O
but	O
in	O
general	O
the	O
sample	O
mean	O
will	O
provide	O
a	O
good	O
estimate	O
of	O
the	O
population	O
mean	O
.	O
in	O
the	O
same	O
way	O
,	O
the	O
unknown	O
coeﬃcients	O
β0	O
and	O
β1	O
in	O
linear	B
regression	I
deﬁne	O
the	O
population	B
regression	I
line	I
.	O
we	O
seek	O
to	O
estimate	O
these	O
unknown	O
coeﬃcients	O
using	O
ˆβ0	O
and	O
ˆβ1	O
given	O
in	O
(	O
3.4	O
)	O
.	O
these	O
coeﬃcient	B
estimates	O
deﬁne	O
the	O
least	B
squares	I
line	O
.	O
(	O
cid:10	O
)	O
the	O
analogy	O
between	O
linear	B
regression	I
and	O
estimation	O
of	O
the	O
mean	O
of	O
a	O
random	O
variable	O
is	O
an	O
apt	O
one	O
based	O
on	O
the	O
concept	O
of	O
bias	B
.	O
if	O
we	O
use	O
the	O
sample	O
mean	O
ˆμ	O
to	O
estimate	O
μ	O
,	O
this	O
estimate	O
is	O
unbiased	O
,	O
in	O
the	O
sense	O
that	O
on	O
average	B
,	O
we	O
expect	O
ˆμ	O
to	O
equal	O
μ.	O
what	O
exactly	O
does	O
this	O
mean	O
?	O
it	O
means	O
that	O
on	O
the	O
basis	B
of	O
one	O
particular	O
set	B
of	O
observations	B
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
,	O
ˆμ	O
might	O
overestimate	O
μ	O
,	O
and	O
on	O
the	O
basis	B
of	O
another	O
set	B
of	O
observations	B
,	O
ˆμ	O
might	O
underestimate	O
μ.	O
but	O
if	O
we	O
could	O
average	B
a	O
huge	O
number	O
of	O
estimates	O
of	O
μ	O
obtained	O
from	O
a	O
huge	O
number	O
of	O
sets	O
of	O
observations	B
,	O
then	O
this	O
average	B
would	O
exactly	O
equal	O
μ.	O
hence	O
,	O
an	O
unbiased	O
estimator	O
does	O
not	O
systematically	O
over-	O
or	O
under-estimate	O
the	O
true	O
parameter	O
.	O
the	O
property	O
of	O
unbiasedness	O
holds	O
for	O
the	O
least	B
squares	I
coeﬃcient	O
estimates	O
given	O
by	O
(	O
3.4	O
)	O
as	O
well	O
:	O
if	O
we	O
estimate	O
β0	O
and	O
β1	O
on	O
the	O
basis	B
of	O
a	O
particular	O
data	B
set	O
,	O
then	O
our	O
estimates	O
won	O
’	O
t	O
be	O
exactly	O
equal	O
to	O
β0	O
and	O
β1	O
.	O
but	O
if	O
we	O
could	O
average	B
the	O
estimates	O
obtained	O
over	O
a	O
huge	O
number	O
of	O
data	B
sets	O
,	O
then	O
the	O
average	B
of	O
these	O
estimates	O
would	O
be	O
spot	O
on	O
!	O
in	O
fact	O
,	O
we	O
can	O
see	O
from	O
the	O
right-	O
hand	O
panel	O
of	O
figure	O
3.3	O
that	O
the	O
average	B
of	O
many	O
least	B
squares	I
lines	O
,	O
each	O
estimated	O
from	O
a	O
separate	O
data	B
set	O
,	O
is	O
pretty	O
close	O
to	O
the	O
true	O
population	O
regression	B
line	O
.	O
we	O
continue	O
the	O
analogy	O
with	O
the	O
estimation	O
of	O
the	O
population	O
mean	O
μ	O
of	O
a	O
random	O
variable	O
y	O
.	O
a	O
natural	B
question	O
is	O
as	O
follows	O
:	O
how	O
accurate	O
is	O
the	O
sample	O
mean	O
ˆμ	O
as	O
an	O
estimate	O
of	O
μ	O
?	O
we	O
have	O
established	O
that	O
the	O
average	B
of	O
ˆμ	O
’	O
s	O
over	O
many	O
data	B
sets	O
will	O
be	O
very	O
close	O
to	O
μ	O
,	O
but	O
that	O
a	O
single	B
estimate	O
ˆμ	O
may	O
be	O
a	O
substantial	O
underestimate	O
or	O
overestimate	O
of	O
μ.	O
how	O
far	O
oﬀ	O
will	O
that	O
single	B
estimate	O
of	O
ˆμ	O
be	O
?	O
in	O
general	O
,	O
we	O
answer	O
this	O
question	O
by	O
computing	O
the	O
standard	B
error	I
of	O
ˆμ	O
,	O
written	O
as	O
se	O
(	O
ˆμ	O
)	O
.	O
we	O
have	O
the	O
well-known	O
formula	O
bias	B
unbiased	O
standard	B
error	I
var	O
(	O
ˆμ	O
)	O
=	O
se	O
(	O
ˆμ	O
)	O
2	O
=	O
σ2	O
n	O
,	O
(	O
3.7	O
)	O
66	O
3.	O
linear	B
regression	I
(	O
cid:20	O
)	O
1	O
n	O
(	O
cid:21	O
)	O
,	O
where	O
σ	O
is	O
the	O
standard	O
deviation	O
of	O
each	O
of	O
the	O
realizations	O
yi	O
of	O
y	O
.2	O
roughly	O
speaking	O
,	O
the	O
standard	B
error	I
tells	O
us	O
the	O
average	B
amount	O
that	O
this	O
estimate	O
ˆμ	O
diﬀers	O
from	O
the	O
actual	O
value	O
of	O
μ.	O
equation	O
3.7	O
also	O
tells	O
us	O
how	O
this	O
deviation	O
shrinks	O
with	O
n—the	O
more	O
observations	B
we	O
have	O
,	O
the	O
smaller	O
the	O
standard	B
error	I
of	O
ˆμ	O
.	O
in	O
a	O
similar	O
vein	O
,	O
we	O
can	O
wonder	O
how	O
close	O
ˆβ0	O
and	O
ˆβ1	O
are	O
to	O
the	O
true	O
values	O
β0	O
and	O
β1	O
.	O
to	O
compute	O
the	O
standard	O
errors	O
associated	O
with	O
ˆβ0	O
and	O
ˆβ1	O
,	O
we	O
use	O
the	O
following	O
formulas	O
:	O
¯x2	O
σ2	O
=	O
n	O
+	O
n	O
2	O
se	O
(	O
ˆβ0	O
)	O
=	O
σ2	O
2	O
se	O
(	O
ˆβ1	O
)	O
(	O
cid:10	O
)	O
i=1	O
(	O
xi	O
−	O
¯x	O
)	O
2	O
(	O
cid:10	O
)	O
i=1	O
(	O
xi	O
−	O
¯x	O
)	O
2	O
,	O
(	O
3.8	O
)	O
where	O
σ2	O
=	O
var	O
(	O
	O
)	O
.	O
for	O
these	O
formulas	O
to	O
be	O
strictly	O
valid	O
,	O
we	O
need	O
to	O
as-	O
sume	O
that	O
the	O
errors	O
i	O
for	O
each	O
observation	O
are	O
uncorrelated	O
with	O
common	O
variance	B
σ2	O
.	O
this	O
is	O
clearly	O
not	O
true	O
in	O
figure	O
3.1	O
,	O
but	O
the	O
formula	O
still	O
turns	O
out	O
to	O
be	O
a	O
good	O
approximation	O
.	O
notice	O
in	O
the	O
formula	O
that	O
se	O
(	O
ˆβ1	O
)	O
is	O
smaller	O
when	O
the	O
xi	O
are	O
more	O
spread	O
out	O
;	O
intuitively	O
we	O
have	O
more	O
leverage	B
to	O
estimate	O
a	O
slope	B
when	O
this	O
is	O
the	O
case	O
.	O
we	O
also	O
see	O
that	O
se	O
(	O
ˆβ0	O
)	O
would	O
be	O
the	O
same	O
as	O
se	O
(	O
ˆμ	O
)	O
if	O
¯x	O
were	O
zero	O
(	O
in	O
which	O
case	O
ˆβ0	O
would	O
be	O
equal	O
to	O
¯y	O
)	O
.	O
in	O
general	O
,	O
σ2	O
is	O
not	O
known	O
,	O
but	O
can	O
be	O
estimated	O
from	O
the	O
data	B
.	O
the	O
estimate	O
of	O
σ	O
is	O
known	O
as	O
the	O
residual	B
standard	O
error	B
,	O
and	O
is	O
given	O
by	O
the	O
formula	O
rss/	O
(	O
n	O
−	O
2	O
)	O
.	O
strictly	O
speaking	O
,	O
when	O
σ2	O
is	O
estimated	O
from	O
the	O
rse	O
=	O
data	B
we	O
should	O
write	O
(	O
cid:23	O
)	O
se	O
(	O
ˆβ1	O
)	O
to	O
indicate	O
that	O
an	O
estimate	O
has	O
been	O
made	O
,	O
(	O
cid:22	O
)	O
but	O
for	O
simplicity	O
of	O
notation	O
we	O
will	O
drop	O
this	O
extra	O
“	O
hat	O
”	O
.	O
standard	O
errors	O
can	O
be	O
used	O
to	O
compute	O
conﬁdence	O
intervals	O
.	O
a	O
95	O
%	O
conﬁdence	B
interval	I
is	O
deﬁned	O
as	O
a	O
range	O
of	O
values	O
such	O
that	O
with	O
95	O
%	O
probability	B
,	O
the	O
range	O
will	O
contain	O
the	O
true	O
unknown	O
value	O
of	O
the	O
parameter	B
.	O
the	O
range	O
is	O
deﬁned	O
in	O
terms	O
of	O
lower	O
and	O
upper	O
limits	O
computed	O
from	O
the	O
sample	O
of	O
data	B
.	O
for	O
linear	O
regression	B
,	O
the	O
95	O
%	O
conﬁdence	B
interval	I
for	O
β1	O
approximately	O
takes	O
the	O
form	O
that	O
is	O
,	O
there	O
is	O
approximately	O
a	O
95	O
%	O
chance	O
that	O
the	O
interval	B
(	O
cid:24	O
)	O
(	O
cid:25	O
)	O
ˆβ1	O
−	O
2	O
·	O
se	O
(	O
ˆβ1	O
)	O
,	O
ˆβ1	O
+	O
2	O
·	O
se	O
(	O
ˆβ1	O
)	O
will	O
contain	O
the	O
true	O
value	O
of	O
β1.3	O
similarly	O
,	O
a	O
conﬁdence	B
interval	I
for	O
β0	O
approximately	O
takes	O
the	O
form	O
ˆβ1	O
±	O
2	O
·	O
se	O
(	O
ˆβ1	O
)	O
.	O
ˆβ0	O
±	O
2	O
·	O
se	O
(	O
ˆβ0	O
)	O
.	O
residual	B
standard	O
error	B
conﬁdence	O
interval	B
(	O
3.9	O
)	O
(	O
3.10	O
)	O
(	O
3.11	O
)	O
2this	O
formula	O
holds	O
provided	O
that	O
the	O
n	O
observations	B
are	O
uncorrelated	O
.	O
3approximately	O
for	O
several	O
reasons	O
.	O
equation	O
3.10	O
relies	O
on	O
the	O
assumption	O
that	O
the	O
errors	O
are	O
gaussian	O
.	O
also	O
,	O
the	O
factor	B
of	O
2	O
in	O
front	O
of	O
the	O
se	O
(	O
ˆβ1	O
)	O
term	B
will	O
vary	O
slightly	O
depending	O
on	O
the	O
number	O
of	O
observations	B
n	O
in	O
the	O
linear	B
regression	I
.	O
to	O
be	O
precise	O
,	O
rather	O
than	O
the	O
number	O
2	O
,	O
(	O
3.10	O
)	O
should	O
contain	O
the	O
97.5	O
%	O
quantile	O
of	O
a	O
t-distribution	B
with	O
n−2	O
degrees	B
of	I
freedom	I
.	O
details	O
of	O
how	O
to	O
compute	O
the	O
95	O
%	O
conﬁdence	B
interval	I
precisely	O
in	O
r	O
will	O
be	O
provided	O
later	O
in	O
this	O
chapter	O
.	O
3.1	O
simple	B
linear	O
regression	B
67	O
in	O
the	O
case	O
of	O
the	O
advertising	O
data	B
,	O
the	O
95	O
%	O
conﬁdence	B
interval	I
for	O
β0	O
is	O
[	O
6.130	O
,	O
7.935	O
]	O
and	O
the	O
95	O
%	O
conﬁdence	B
interval	I
for	O
β1	O
is	O
[	O
0.042	O
,	O
0.053	O
]	O
.	O
therefore	O
,	O
we	O
can	O
conclude	O
that	O
in	O
the	O
absence	O
of	O
any	O
advertising	O
,	O
sales	O
will	O
,	O
on	O
average	B
,	O
fall	O
somewhere	O
between	O
6,130	O
and	O
7,940	O
units	O
.	O
furthermore	O
,	O
for	O
each	O
$	O
1,000	O
increase	O
in	O
television	O
advertising	O
,	O
there	O
will	O
be	O
an	O
average	B
increase	O
in	O
sales	O
of	O
between	O
42	O
and	O
53	O
units	O
.	O
standard	O
errors	O
can	O
also	O
be	O
used	O
to	O
perform	O
hypothesis	B
tests	O
on	O
the	O
coeﬃcients	O
.	O
the	O
most	O
common	O
hypothesis	B
test	I
involves	O
testing	O
the	O
null	B
hypothesis	O
of	O
h0	O
:	O
there	O
is	O
no	O
relationship	O
between	O
x	O
and	O
y	O
(	O
3.12	O
)	O
versus	O
the	O
alternative	B
hypothesis	I
ha	O
:	O
there	O
is	O
some	O
relationship	O
between	O
x	O
and	O
y	O
.	O
(	O
3.13	O
)	O
hypothesis	B
test	I
null	O
hypothesis	B
alternative	O
hypothesis	B
mathematically	O
,	O
this	O
corresponds	O
to	O
testing	O
versus	O
h0	O
:	O
β1	O
=	O
0	O
ha	O
:	O
β1	O
(	O
cid:4	O
)	O
=	O
0	O
,	O
since	O
if	O
β1	O
=	O
0	O
then	O
the	O
model	B
(	O
3.5	O
)	O
reduces	O
to	O
y	O
=	O
β0	O
+	O
	O
,	O
and	O
x	O
is	O
not	O
associated	O
with	O
y	O
.	O
to	O
test	B
the	O
null	B
hypothesis	O
,	O
we	O
need	O
to	O
determine	O
whether	O
ˆβ1	O
,	O
our	O
estimate	O
for	O
β1	O
,	O
is	O
suﬃciently	O
far	O
from	O
zero	O
that	O
we	O
can	O
be	O
conﬁdent	O
that	O
β1	O
is	O
non-zero	O
.	O
how	O
far	O
is	O
far	O
enough	O
?	O
this	O
of	O
course	O
depends	O
on	O
the	O
accuracy	O
of	O
ˆβ1—that	O
is	O
,	O
it	O
depends	O
on	O
se	O
(	O
ˆβ1	O
)	O
.	O
if	O
se	O
(	O
ˆβ1	O
)	O
is	O
small	O
,	O
then	O
even	O
relatively	O
small	O
values	O
of	O
ˆβ1	O
may	O
provide	O
strong	O
evidence	O
that	O
β1	O
(	O
cid:4	O
)	O
=	O
0	O
,	O
and	O
hence	O
that	O
there	O
is	O
a	O
relationship	O
between	O
x	O
and	O
y	O
.	O
in	O
contrast	B
,	O
if	O
se	O
(	O
ˆβ1	O
)	O
is	O
large	O
,	O
then	O
ˆβ1	O
must	O
be	O
large	O
in	O
absolute	O
value	O
in	O
order	O
for	O
us	O
to	O
reject	O
the	O
null	B
hypothesis	O
.	O
in	O
practice	O
,	O
we	O
compute	O
a	O
t-statistic	B
,	O
given	O
by	O
t-statistic	B
ˆβ1	O
−	O
0	O
se	O
(	O
ˆβ1	O
)	O
t	O
=	O
,	O
(	O
3.14	O
)	O
which	O
measures	O
the	O
number	O
of	O
standard	O
deviations	O
that	O
ˆβ1	O
is	O
away	O
from	O
0.	O
if	O
there	O
really	O
is	O
no	O
relationship	O
between	O
x	O
and	O
y	O
,	O
then	O
we	O
expect	O
that	O
(	O
3.14	O
)	O
will	O
have	O
a	O
t-distribution	B
with	O
n	O
−	O
2	O
degrees	B
of	I
freedom	I
.	O
the	O
t-	O
distribution	B
has	O
a	O
bell	O
shape	O
and	O
for	O
values	O
of	O
n	O
greater	O
than	O
approximately	O
30	O
it	O
is	O
quite	O
similar	O
to	O
the	O
normal	O
distribution	O
.	O
consequently	O
,	O
it	O
is	O
a	O
simple	B
matter	O
to	O
compute	O
the	O
probability	B
of	O
observing	O
any	O
number	O
equal	O
to	O
|t|	O
or	O
larger	O
in	O
absolute	O
value	O
,	O
assuming	O
β1	O
=	O
0.	O
we	O
call	O
this	O
probability	B
the	O
p-value	B
.	O
roughly	O
speaking	O
,	O
we	O
interpret	O
the	O
p-value	B
as	O
follows	O
:	O
a	O
small	O
p-value	B
indicates	O
that	O
it	O
is	O
unlikely	O
to	O
observe	O
such	O
a	O
substant	O
ial	O
association	O
between	O
the	O
pre-	O
dictor	O
and	O
the	O
response	B
due	O
to	O
chance	O
,	O
in	O
the	O
absence	O
of	O
any	O
real	O
association	O
between	O
the	O
predictor	B
and	O
the	O
response	B
.	O
hence	O
,	O
if	O
we	O
see	O
a	O
small	O
p-value	B
,	O
p-value	B
68	O
3.	O
linear	B
regression	I
then	O
we	O
can	O
infer	O
that	O
there	O
is	O
an	O
association	O
between	O
the	O
predictor	B
and	O
the	O
response	B
.	O
we	O
reject	O
the	O
null	B
hypothesis—that	O
is	O
,	O
we	O
declare	O
a	O
relationship	O
to	O
exist	O
between	O
x	O
and	O
y	O
—if	O
the	O
p-value	B
is	O
small	O
enough	O
.	O
typical	O
p-value	B
cutoﬀs	O
for	O
rejecting	O
the	O
null	B
hypothesis	O
are	O
5	O
or	O
1	O
%	O
.	O
when	O
n	O
=	O
30	O
,	O
these	O
correspond	O
to	O
t-statistics	O
(	O
3.14	O
)	O
of	O
around	O
2	O
and	O
2.75	O
,	O
respectively	O
.	O
intercept	B
tv	O
coeﬃcient	B
7.0325	O
0.0475	O
std	O
.	O
error	B
0.4578	O
0.0027	O
t-statistic	B
p-value	O
15.36	O
<	O
0.0001	O
17.67	O
<	O
0.0001	O
table	O
3.1.	O
for	O
the	O
advertising	O
data	B
,	O
coeﬃcients	O
of	O
the	O
least	B
squares	I
model	O
for	O
the	O
regression	B
of	O
number	O
of	O
units	O
sold	O
on	O
tv	O
advertising	O
budget	O
.	O
an	O
increase	O
of	O
$	O
1,000	O
in	O
the	O
tv	O
advertising	O
budget	O
is	O
associated	O
with	O
an	O
increase	O
in	O
sales	O
by	O
around	O
50	O
units	O
(	O
recall	B
that	O
the	O
sales	O
variable	B
is	O
in	O
thousands	O
of	O
units	O
,	O
and	O
the	O
tv	O
variable	B
is	O
in	O
thousands	O
of	O
dollars	O
)	O
.	O
table	O
3.1	O
provides	O
details	O
of	O
the	O
least	B
squares	I
model	O
for	O
the	O
regression	B
of	O
number	O
of	O
units	O
sold	O
on	O
tv	O
advertising	O
budget	O
for	O
the	O
advertising	O
data	B
.	O
notice	O
that	O
the	O
coeﬃcients	O
for	O
ˆβ0	O
and	O
ˆβ1	O
are	O
very	O
large	O
relative	O
to	O
their	O
standard	O
errors	O
,	O
so	O
the	O
t-statistics	O
are	O
also	O
large	O
;	O
the	O
probabilities	O
of	O
seeing	O
such	O
values	O
if	O
h0	O
is	O
true	O
are	O
virtually	O
zero	O
.	O
hence	O
we	O
can	O
conclude	O
that	O
β0	O
(	O
cid:4	O
)	O
=	O
0	O
and	O
β1	O
(	O
cid:4	O
)	O
=	O
0.4	O
3.1.3	O
assessing	O
the	O
accuracy	O
of	O
the	O
model	B
once	O
we	O
have	O
rejected	O
the	O
null	B
hypothesis	O
(	O
3.12	O
)	O
in	O
favor	O
of	O
the	O
alternative	B
hypothesis	I
(	O
3.13	O
)	O
,	O
it	O
is	O
natural	B
to	O
want	O
to	O
quantify	O
the	O
extent	O
to	O
which	O
the	O
model	B
ﬁts	O
the	O
data	B
.	O
the	O
quality	O
of	O
a	O
linear	B
regression	I
ﬁt	O
is	O
typically	O
assessed	O
using	O
two	O
related	O
quantities	O
:	O
the	O
residual	B
standard	O
error	B
(	O
rse	O
)	O
and	O
the	O
r2	O
statistic	O
.	O
table	O
3.2	O
displays	O
the	O
rse	O
,	O
the	O
r2	O
statistic	O
,	O
and	O
the	O
f-statistic	O
(	O
to	O
be	O
described	O
in	O
section	O
3.2.2	O
)	O
for	O
the	O
linear	B
regression	I
of	O
number	O
of	O
units	O
sold	O
on	O
tv	O
advertising	O
budget	O
.	O
r2	O
residual	B
standard	O
error	B
recall	O
from	O
the	O
model	B
(	O
3.5	O
)	O
that	O
associated	O
with	O
each	O
observation	O
is	O
an	O
error	B
term	O
	O
.	O
due	O
to	O
the	O
presence	O
of	O
these	O
error	B
terms	O
,	O
even	O
if	O
we	O
knew	O
the	O
true	O
regression	O
line	B
(	O
i.e	O
.	O
even	O
if	O
β0	O
and	O
β1	O
were	O
known	O
)	O
,	O
we	O
would	O
not	O
be	O
able	O
to	O
perfectly	O
predict	O
y	O
from	O
x.	O
the	O
rse	O
is	O
an	O
estimate	O
of	O
the	O
standard	O
4in	O
table	O
3.1	O
,	O
a	O
small	O
p-value	B
for	O
the	O
intercept	B
indicates	O
that	O
we	O
can	O
reject	O
the	O
null	B
hypothesis	O
that	O
β0	O
=	O
0	O
,	O
and	O
a	O
small	O
p-value	B
for	O
tv	O
indicates	O
that	O
we	O
can	O
reject	O
the	O
null	B
hypothesis	O
that	O
β1	O
=	O
0.	O
rejecting	O
the	O
latter	O
null	B
hypothesis	O
allows	O
us	O
to	O
conclude	O
that	O
there	O
is	O
a	O
relationship	O
between	O
tv	O
and	O
sales	O
.	O
rejecting	O
the	O
former	O
allows	O
us	O
to	O
conclude	O
that	O
in	O
the	O
absence	O
of	O
tv	O
expenditure	O
,	O
sales	O
are	O
non-zero	O
.	O
3.1	O
simple	B
linear	O
regression	B
69	O
quantity	O
residual	B
standard	O
error	B
r2	O
f-statistic	O
value	O
3.26	O
0.612	O
312.1	O
table	O
3.2.	O
for	O
the	O
advertising	O
data	B
,	O
more	O
information	O
about	O
the	O
least	B
squares	I
model	O
for	O
the	O
regression	B
of	O
number	O
of	O
units	O
sold	O
on	O
tv	O
advertising	O
budget	O
.	O
deviation	O
of	O
	O
.	O
roughly	O
speaking	O
,	O
it	O
is	O
the	O
average	B
amount	O
that	O
the	O
response	B
will	O
deviate	O
from	O
the	O
true	O
regression	O
line	B
.	O
it	O
is	O
computed	O
using	O
the	O
formula	O
(	O
cid:26	O
)	O
rse	O
=	O
1	O
n	O
−	O
2	O
rss	O
=	O
(	O
cid:27	O
)	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
(	O
cid:29	O
)	O
1	O
n	O
−	O
2	O
n	O
(	O
cid:17	O
)	O
i=1	O
(	O
yi	O
−	O
ˆyi	O
)	O
2	O
.	O
(	O
3.15	O
)	O
note	O
that	O
rss	O
was	O
deﬁned	O
in	O
section	O
3.1.1	O
,	O
and	O
is	O
given	O
by	O
the	O
formula	O
n	O
(	O
cid:17	O
)	O
rss	O
=	O
i=1	O
(	O
yi	O
−	O
ˆyi	O
)	O
2	O
.	O
(	O
3.16	O
)	O
in	O
the	O
case	O
of	O
the	O
advertising	O
data	B
,	O
we	O
see	O
from	O
the	O
linear	B
regression	I
output	O
in	O
table	O
3.2	O
that	O
the	O
rse	O
is	O
3.26.	O
in	O
other	O
words	O
,	O
actual	O
sales	O
in	O
each	O
market	O
deviate	O
from	O
the	O
true	O
regression	O
line	B
by	O
approximately	O
3,260	O
units	O
,	O
on	O
average	B
.	O
another	O
way	O
to	O
think	O
about	O
this	O
is	O
that	O
even	O
if	O
the	O
model	B
were	O
correct	O
and	O
the	O
true	O
values	O
of	O
the	O
unknown	O
coeﬃcients	O
β0	O
and	O
β1	O
were	O
known	O
exactly	O
,	O
any	O
prediction	B
of	O
sales	O
on	O
the	O
basis	B
of	O
tv	O
advertising	O
would	O
still	O
be	O
oﬀ	O
by	O
about	O
3,260	O
units	O
on	O
average	B
.	O
of	O
course	O
,	O
whether	O
or	O
not	O
3,260	O
units	O
is	O
an	O
acceptable	O
prediction	B
error	O
depends	O
on	O
the	O
problem	O
context	O
.	O
in	O
the	O
advertising	O
data	B
set	O
,	O
the	O
mean	O
value	O
of	O
sales	O
over	O
all	O
markets	O
is	O
approximately	O
14,000	O
units	O
,	O
and	O
so	O
the	O
percentage	O
error	B
is	O
3,260/14,000	O
=	O
23	O
%	O
.	O
the	O
rse	O
is	O
considered	O
a	O
measure	O
of	O
the	O
lack	O
of	O
ﬁt	B
of	O
the	O
model	B
(	O
3.5	O
)	O
to	O
the	O
data	B
.	O
if	O
the	O
predictions	O
obtained	O
using	O
the	O
model	B
are	O
very	O
close	O
to	O
the	O
true	O
outcome	O
values—that	O
is	O
,	O
if	O
ˆyi	O
≈	O
yi	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n—then	O
(	O
3.15	O
)	O
will	O
be	O
small	O
,	O
and	O
we	O
can	O
conclude	O
that	O
the	O
model	B
ﬁts	O
the	O
data	B
very	O
well	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
ˆyi	O
is	O
very	O
far	O
from	O
yi	O
for	O
one	O
or	O
more	O
observations	B
,	O
then	O
the	O
rse	O
may	O
be	O
quite	O
large	O
,	O
indicating	O
that	O
the	O
model	B
doesn	O
’	O
t	O
ﬁt	B
the	O
data	B
well	O
.	O
r2	O
statistic	O
the	O
rse	O
provides	O
an	O
absolute	O
measure	O
of	O
lack	O
of	O
ﬁt	B
of	O
the	O
model	B
(	O
3.5	O
)	O
to	O
the	O
data	B
.	O
but	O
since	O
it	O
is	O
measured	O
in	O
the	O
units	O
of	O
y	O
,	O
it	O
is	O
not	O
always	O
clear	O
what	O
constitutes	O
a	O
good	O
rse	O
.	O
the	O
r2	O
statistic	O
provides	O
an	O
alternative	O
measure	O
of	O
ﬁt	B
.	O
it	O
takes	O
the	O
form	O
of	O
a	O
proportion—the	O
proportion	B
of	I
variance	I
explained—and	O
so	O
it	O
always	O
takes	O
on	O
a	O
value	O
between	O
0	O
and	O
1	O
,	O
and	O
is	O
independent	B
of	O
the	O
scale	O
of	O
y	O
.	O
70	O
3.	O
linear	B
regression	I
to	O
calculate	O
r2	O
,	O
we	O
use	O
the	O
formula	O
tss	O
−	O
rss	O
r2	O
=	O
tss	O
=	O
1	O
−	O
rss	O
tss	O
(	O
3.17	O
)	O
(	O
cid:10	O
)	O
(	O
yi	O
−	O
¯y	O
)	O
2	O
is	O
the	O
total	B
sum	I
of	I
squares	I
,	O
and	O
rss	O
is	O
deﬁned	O
where	O
tss	O
=	O
in	O
(	O
3.16	O
)	O
.	O
tss	O
measures	O
the	O
total	O
variance	O
in	O
the	O
response	B
y	O
,	O
and	O
can	O
be	O
thought	O
of	O
as	O
the	O
amount	O
of	O
variability	O
inherent	O
in	O
the	O
response	B
before	O
the	O
regression	B
is	O
performed	O
.	O
in	O
contrast	B
,	O
rss	O
measures	O
the	O
amount	O
of	O
variability	O
that	O
is	O
left	O
unexplained	O
after	O
performing	O
the	O
regression	B
.	O
hence	O
,	O
tss−	O
rss	O
measures	O
the	O
amount	O
of	O
variability	O
in	O
the	O
response	B
that	O
is	O
explained	B
(	O
or	O
removed	O
)	O
by	O
performing	O
the	O
regression	B
,	O
and	O
r2	O
measures	O
the	O
proportion	O
of	O
variability	O
in	O
y	O
that	O
can	O
be	O
explained	B
using	O
x.	O
an	O
r2	O
statistic	O
that	O
is	O
close	O
to	O
1	O
indicates	O
that	O
a	O
large	O
proportion	O
of	O
the	O
variability	O
in	O
the	O
response	B
has	O
been	O
explained	B
by	O
the	O
regression	B
.	O
a	O
number	O
near	O
0	O
indicates	O
that	O
the	O
regression	B
did	O
not	O
explain	O
much	O
of	O
the	O
variability	O
in	O
the	O
response	B
;	O
this	O
might	O
occur	O
because	O
the	O
linear	B
model	I
is	O
wrong	O
,	O
or	O
the	O
inherent	O
error	B
σ2	O
is	O
high	O
,	O
or	O
both	O
.	O
in	O
table	O
3.2	O
,	O
the	O
r2	O
was	O
0.61	O
,	O
and	O
so	O
just	O
under	O
two-thirds	O
of	O
the	O
variability	O
in	O
sales	O
is	O
explained	B
by	O
a	O
linear	B
regression	I
on	O
tv	O
.	O
total	B
sum	I
of	I
squares	I
the	O
r2	O
statistic	O
(	O
3.17	O
)	O
has	O
an	O
interpretational	O
advantage	O
over	O
the	O
rse	O
(	O
3.15	O
)	O
,	O
since	O
unlike	O
the	O
rse	O
,	O
it	O
always	O
lies	O
between	O
0	O
and	O
1.	O
however	O
,	O
it	O
can	O
still	O
be	O
challenging	O
to	O
determine	O
what	O
is	O
a	O
good	O
r2	O
value	O
,	O
and	O
in	O
general	O
,	O
this	O
will	O
depend	O
on	O
the	O
application	O
.	O
for	O
instance	O
,	O
in	O
certain	O
problems	O
in	O
physics	O
,	O
we	O
may	O
know	O
that	O
the	O
data	B
truly	O
comes	O
from	O
a	O
linear	B
model	I
with	O
a	O
small	O
residual	B
error	O
.	O
in	O
this	O
case	O
,	O
we	O
would	O
expect	O
to	O
see	O
an	O
r2	O
value	O
that	O
is	O
extremely	O
close	O
to	O
1	O
,	O
and	O
a	O
substantially	O
smaller	O
r2	O
value	O
might	O
indicate	O
a	O
serious	O
problem	O
with	O
the	O
experiment	O
in	O
which	O
the	O
data	B
were	O
generated	O
.	O
on	O
the	O
other	O
hand	O
,	O
in	O
typical	O
applications	O
in	O
biology	O
,	O
psychology	O
,	O
marketing	O
,	O
and	O
other	O
domains	O
,	O
the	O
linear	B
model	I
(	O
3.5	O
)	O
is	O
at	O
best	O
an	O
extremely	O
rough	O
approximation	O
to	O
the	O
data	B
,	O
and	O
residual	B
errors	O
due	O
to	O
other	O
unmeasured	O
factors	O
are	O
often	O
very	O
large	O
.	O
in	O
this	O
setting	O
,	O
we	O
would	O
expect	O
only	O
a	O
very	O
small	O
proportion	O
of	O
the	O
variance	B
in	O
the	O
response	B
to	O
be	O
explained	B
by	O
the	O
predictor	B
,	O
and	O
an	O
r2	O
value	O
well	O
below	O
0.1	O
might	O
be	O
more	O
realistic	O
!	O
the	O
r2	O
statistic	O
is	O
a	O
measure	O
of	O
the	O
linear	B
relationship	O
between	O
x	O
and	O
y	O
.	O
recall	B
that	O
correlation	B
,	O
deﬁned	O
as	O
(	O
cid:10	O
)	O
i=1	O
(	O
xi	O
−	O
x	O
)	O
2	O
cor	O
(	O
x	O
,	O
y	O
)	O
=	O
(	O
cid:22	O
)	O
(	O
cid:10	O
)	O
n	O
n	O
i=1	O
(	O
xi	O
−	O
x	O
)	O
(	O
yi	O
−	O
y	O
)	O
(	O
cid:22	O
)	O
(	O
cid:10	O
)	O
n	O
i=1	O
(	O
yi	O
−	O
y	O
)	O
2	O
correlation	B
,	O
(	O
3.18	O
)	O
is	O
also	O
a	O
measure	O
of	O
the	O
linear	B
relationship	O
between	O
x	O
and	O
y	O
.5	O
this	O
sug-	O
gests	O
that	O
we	O
might	O
be	O
able	O
to	O
use	O
r	O
=	O
cor	O
(	O
x	O
,	O
y	O
)	O
instead	O
of	O
r2	O
in	O
order	O
to	O
assess	O
the	O
ﬁt	B
of	O
the	O
linear	B
model	I
.	O
in	O
fact	O
,	O
it	O
can	O
be	O
shown	O
that	O
in	O
the	O
simple	B
linear	O
regression	B
setting	O
,	O
r2	O
=	O
r2	O
.	O
in	O
other	O
words	O
,	O
the	O
squared	O
correlation	B
5we	O
note	O
that	O
in	O
fact	O
,	O
the	O
right-hand	O
side	O
of	O
(	O
3.18	O
)	O
is	O
the	O
sample	O
correlation	B
;	O
thus	O
,	O
it	O
would	O
be	O
more	O
correct	O
to	O
write	O
(	O
cid:2	O
)	O
cor	O
(	O
x	O
,	O
y	O
)	O
;	O
however	O
,	O
we	O
omit	O
the	O
“	O
hat	O
”	O
for	O
ease	O
of	O
notation	O
.	O
3.2	O
multiple	B
linear	O
regression	B
71	O
and	O
the	O
r2	O
statistic	O
are	O
identical	O
.	O
however	O
,	O
in	O
the	O
next	O
section	O
we	O
will	O
discuss	O
the	O
multiple	B
linear	O
regression	B
problem	O
,	O
in	O
which	O
we	O
use	O
several	O
pre-	O
dictors	O
simultaneously	O
to	O
predict	O
the	O
response	B
.	O
the	O
concept	O
of	O
correlation	B
between	O
the	O
predictors	O
and	O
the	O
response	B
does	O
not	O
extend	O
automatically	O
to	O
this	O
setting	O
,	O
since	O
correlation	B
quantiﬁes	O
the	O
association	O
between	O
a	O
single	B
pair	O
of	O
variables	O
rather	O
than	O
between	O
a	O
larger	O
number	O
of	O
variables	O
.	O
we	O
will	O
see	O
that	O
r2	O
ﬁlls	O
this	O
role	O
.	O
3.2	O
multiple	B
linear	O
regression	B
simple	O
linear	B
regression	I
is	O
a	O
useful	O
approach	B
for	O
predicting	O
a	O
response	B
on	O
the	O
basis	B
of	O
a	O
single	B
predictor	O
variable	B
.	O
however	O
,	O
in	O
practice	O
we	O
often	O
have	O
more	O
than	O
one	O
predictor	B
.	O
for	O
example	O
,	O
in	O
the	O
advertising	O
data	B
,	O
we	O
have	O
examined	O
the	O
relationship	O
between	O
sales	O
and	O
tv	O
advertising	O
.	O
we	O
also	O
have	O
data	B
for	O
the	O
amount	O
of	O
money	O
spent	O
advertising	O
on	O
the	O
radio	O
and	O
in	O
newspapers	O
,	O
and	O
we	O
may	O
want	O
to	O
know	O
whether	O
either	O
of	O
these	O
two	O
media	O
is	O
associated	O
with	O
sales	O
.	O
how	O
can	O
we	O
extend	O
our	O
analysis	O
of	O
the	O
advertising	O
data	B
in	O
order	O
to	O
accommodate	O
these	O
two	O
additional	O
predictors	O
?	O
one	O
option	O
is	O
to	O
run	O
three	O
separate	O
simple	B
linear	O
regressions	O
,	O
each	O
of	O
which	O
uses	O
a	O
diﬀerent	O
advertising	O
medium	O
as	O
a	O
predictor	B
.	O
for	O
instance	O
,	O
we	O
can	O
ﬁt	B
a	O
simple	B
linear	O
regression	B
to	O
predict	O
sales	O
on	O
the	O
basis	B
of	O
the	O
amount	O
spent	O
on	O
radio	O
advertisements	O
.	O
results	O
are	O
shown	O
in	O
table	O
3.3	O
(	O
top	O
table	O
)	O
.	O
we	O
ﬁnd	O
that	O
a	O
$	O
1,000	O
increase	O
in	O
spending	O
on	O
radio	O
advertising	O
is	O
associated	O
with	O
an	O
increase	O
in	O
sales	O
by	O
around	O
203	O
units	O
.	O
table	O
3.3	O
(	O
bottom	O
table	O
)	O
contains	O
the	O
least	B
squares	I
coeﬃcients	O
for	O
a	O
simple	B
linear	O
regression	B
of	O
sales	O
onto	O
newspaper	O
advertising	O
budget	O
.	O
a	O
$	O
1,000	O
increase	O
in	O
newspaper	O
advertising	O
budget	O
is	O
associated	O
with	O
an	O
increase	O
in	O
sales	O
by	O
approximately	O
55	O
units	O
.	O
however	O
,	O
the	O
approach	B
of	O
ﬁtting	O
a	O
separate	O
simple	B
linear	O
regression	B
model	O
for	O
each	O
predictor	B
is	O
not	O
entirely	O
satisfactory	O
.	O
first	O
of	O
all	O
,	O
it	O
is	O
unclear	O
how	O
to	O
make	O
a	O
single	B
prediction	O
of	O
sales	O
given	O
levels	O
of	O
the	O
three	O
advertising	O
media	O
budgets	O
,	O
since	O
each	O
of	O
the	O
budgets	O
is	O
associated	O
with	O
a	O
separate	O
regression	B
equation	O
.	O
second	O
,	O
each	O
of	O
the	O
three	O
regression	B
equations	O
ignores	O
the	O
other	O
two	O
media	O
in	O
forming	O
estimates	O
for	O
the	O
regression	B
coeﬃcients	O
.	O
we	O
will	O
see	O
shortly	O
that	O
if	O
the	O
media	O
budgets	O
are	O
correlated	O
with	O
each	O
other	O
in	O
the	O
200	O
markets	O
that	O
constitute	O
our	O
data	B
set	O
,	O
then	O
this	O
can	O
lead	O
to	O
very	O
misleading	O
estimates	O
of	O
the	O
individual	O
media	O
eﬀects	O
on	O
sales	O
.	O
instead	O
of	O
ﬁtting	O
a	O
separate	O
simple	B
linear	O
regression	B
model	O
for	O
each	O
pre-	O
dictor	O
,	O
a	O
better	O
approach	B
is	O
to	O
extend	O
the	O
simple	B
linear	O
regression	B
model	O
(	O
3.5	O
)	O
so	O
that	O
it	O
can	O
directly	O
accommodate	O
multiple	B
predictors	O
.	O
we	O
can	O
do	O
this	O
by	O
giving	O
each	O
predictor	B
a	O
separate	O
slope	B
coeﬃcient	O
in	O
a	O
single	B
model	O
.	O
in	O
general	O
,	O
suppose	O
that	O
we	O
have	O
p	O
distinct	O
predictors	O
.	O
then	O
the	O
multiple	B
linear	O
regression	B
model	O
takes	O
the	O
form	O
y	O
=	O
β0	O
+	O
β1x1	O
+	O
β2x2	O
+	O
···	O
+	O
βpxp	O
+	O
	O
,	O
(	O
3.19	O
)	O
72	O
3.	O
linear	B
regression	I
simple	O
regression	B
of	O
sales	O
on	O
radio	O
intercept	B
radio	O
coeﬃcient	B
9.312	O
0.203	O
std	O
.	O
error	B
0.563	O
0.020	O
t-statistic	B
p-value	O
16.54	O
<	O
0.0001	O
9.92	O
<	O
0.0001	O
simple	B
regression	O
of	O
sales	O
on	O
newspaper	O
intercept	B
newspaper	O
coeﬃcient	B
12.351	O
0.055	O
std	O
.	O
error	B
0.621	O
0.017	O
t-statistic	B
p-value	O
19.88	O
<	O
0.0001	O
3.30	O
0.00115	O
table	O
3.3.	O
more	O
simple	B
linear	O
regression	B
models	O
for	O
the	O
advertising	O
data	B
.	O
co-	O
eﬃcients	O
of	O
the	O
simple	B
linear	O
regression	B
model	O
for	O
number	O
of	O
units	O
sold	O
on	O
top	O
:	O
radio	O
advertising	O
budget	O
and	O
bottom	O
:	O
newspaper	O
advertising	O
budget	O
.	O
a	O
$	O
1,000	O
in-	O
crease	O
in	O
spending	O
on	O
radio	O
advertising	O
is	O
associated	O
with	O
an	O
average	B
increase	O
in	O
sales	O
by	O
around	O
203	O
units	O
,	O
while	O
the	O
same	O
increase	O
in	O
spending	O
on	O
newspaper	O
ad-	O
vertising	O
is	O
associated	O
with	O
an	O
average	B
increase	O
in	O
sales	O
by	O
around	O
55	O
units	O
(	O
note	O
that	O
the	O
sales	O
variable	B
is	O
in	O
thousands	O
of	O
units	O
,	O
and	O
the	O
radio	O
and	O
newspaper	O
variables	O
are	O
in	O
thousands	O
of	O
dollars	O
)	O
.	O
where	O
xj	O
represents	O
the	O
jth	O
predictor	B
and	O
βj	O
quantiﬁes	O
the	O
association	O
between	O
that	O
variable	B
and	O
the	O
response	B
.	O
we	O
interpret	O
βj	O
as	O
the	O
average	B
eﬀect	O
on	O
y	O
of	O
a	O
one	O
unit	O
increase	O
in	O
xj	O
,	O
holding	O
all	O
other	O
predictors	O
ﬁxed	O
.	O
in	O
the	O
advertising	O
example	O
,	O
(	O
3.19	O
)	O
becomes	O
sales	O
=	O
β0	O
+	O
β1	O
×	O
tv	O
+	O
β2	O
×	O
radio	O
+	O
β3	O
×	O
newspaper	O
+	O
	O
.	O
(	O
3.20	O
)	O
3.2.1	O
estimating	O
the	O
regression	B
coeﬃcients	O
as	O
was	O
the	O
case	O
in	O
the	O
simple	B
linear	O
regression	B
setting	O
,	O
the	O
regression	B
coef-	O
ﬁcients	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
in	O
(	O
3.19	O
)	O
are	O
unknown	O
,	O
and	O
must	O
be	O
estimated	O
.	O
given	O
estimates	O
ˆβ0	O
,	O
ˆβ1	O
,	O
.	O
.	O
.	O
,	O
ˆβp	O
,	O
we	O
can	O
make	O
predictions	O
using	O
the	O
formula	O
ˆy	O
=	O
ˆβ0	O
+	O
ˆβ1x1	O
+	O
ˆβ2x2	O
+	O
···	O
+	O
ˆβpxp	O
.	O
(	O
3.21	O
)	O
the	O
parameters	O
are	O
estimated	O
using	O
the	O
same	O
least	B
squares	I
approach	O
that	O
we	O
saw	O
in	O
the	O
context	O
of	O
simple	B
linear	O
regression	B
.	O
we	O
choose	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
to	O
minimize	O
the	O
sum	O
of	O
squared	O
residuals	B
n	O
(	O
cid:17	O
)	O
rss	O
=	O
i=1	O
n	O
(	O
cid:17	O
)	O
i=1	O
=	O
(	O
yi	O
−	O
ˆyi	O
)	O
2	O
(	O
yi	O
−	O
ˆβ0	O
−	O
ˆβ1xi1	O
−	O
ˆβ2xi2	O
−	O
···	O
−	O
ˆβpxip	O
)	O
2	O
.	O
(	O
3.22	O
)	O
3.2	O
multiple	B
linear	O
regression	B
73	O
y	O
x2	B
x1	O
figure	O
3.4.	O
in	O
a	O
three-dimensional	O
setting	O
,	O
with	O
two	O
predictors	O
and	O
one	O
re-	O
sponse	O
,	O
the	O
least	B
squares	I
regression	O
line	B
becomes	O
a	O
plane	O
.	O
the	O
plane	O
is	O
chosen	O
to	O
minimize	O
the	O
sum	O
of	O
the	O
squared	O
vertical	O
distances	O
between	O
each	O
observation	O
(	O
shown	O
in	O
red	O
)	O
and	O
the	O
plane	O
.	O
the	O
values	O
ˆβ0	O
,	O
ˆβ1	O
,	O
.	O
.	O
.	O
,	O
ˆβp	O
that	O
minimize	O
(	O
3.22	O
)	O
are	O
the	O
multiple	B
least	O
squares	O
regression	B
coeﬃcient	O
estimates	O
.	O
unlike	O
the	O
simple	B
linear	O
regression	B
estimates	O
given	O
in	O
(	O
3.4	O
)	O
,	O
the	O
multiple	B
regression	O
coeﬃcient	B
estimates	O
have	O
somewhat	O
complicated	O
forms	O
that	O
are	O
most	O
easily	O
represented	O
using	O
ma-	O
trix	O
algebra	O
.	O
for	O
this	O
reason	O
,	O
we	O
do	O
not	O
provide	O
them	O
here	O
.	O
any	O
statistical	O
software	O
package	O
can	O
be	O
used	O
to	O
compute	O
these	O
coeﬃcient	B
estimates	O
,	O
and	O
later	O
in	O
this	O
chapter	O
we	O
will	O
show	O
how	O
this	O
can	O
be	O
done	O
in	O
r.	O
figure	O
3.4	O
illustrates	O
an	O
example	O
of	O
the	O
least	B
squares	I
ﬁt	O
to	O
a	O
toy	O
data	B
set	O
with	O
p	O
=	O
2	O
predictors	O
.	O
table	O
3.4	O
displays	O
the	O
multiple	B
regression	O
coeﬃcient	B
estimates	O
when	O
tv	O
,	O
radio	O
,	O
and	O
newspaper	O
advertising	O
budgets	O
are	O
used	O
to	O
predict	O
product	O
sales	O
using	O
the	O
advertising	O
data	B
.	O
we	O
interpret	O
these	O
results	O
as	O
follows	O
:	O
for	O
a	O
given	O
amount	O
of	O
tv	O
and	O
newspaper	O
advertising	O
,	O
spending	O
an	O
additional	O
$	O
1,000	O
on	O
radio	O
advertising	O
leads	O
to	O
an	O
increase	O
in	O
sales	O
by	O
approximately	O
189	O
units	O
.	O
comparing	O
these	O
coeﬃcient	B
estimates	O
to	O
those	O
displayed	O
in	O
tables	O
3.1	O
and	O
3.3	O
,	O
we	O
notice	O
that	O
the	O
multiple	B
regression	O
coeﬃcient	B
estimates	O
for	O
tv	O
and	O
radio	O
are	O
pretty	O
similar	O
to	O
the	O
simple	B
linear	O
regression	B
coeﬃcient	O
estimates	O
.	O
however	O
,	O
while	O
the	O
newspaper	O
regression	B
coeﬃcient	O
estimate	O
in	O
table	O
3.3	O
was	O
signiﬁcantly	O
non-zero	O
,	O
the	O
coeﬃcient	B
estimate	O
for	O
newspaper	O
in	O
the	O
multiple	B
regression	O
model	B
is	O
close	O
to	O
zero	O
,	O
and	O
the	O
corresponding	O
p-value	B
is	O
no	O
longer	O
signiﬁcant	O
,	O
with	O
a	O
value	O
around	O
0.86.	O
this	O
illustrates	O
74	O
3.	O
linear	B
regression	I
intercept	O
tv	O
radio	O
newspaper	O
coeﬃcient	B
2.939	O
0.046	O
0.189	O
−0.001	O
std	O
.	O
error	B
0.3119	O
0.0014	O
0.0086	O
0.0059	O
t-statistic	B
p-value	O
9.42	O
<	O
0.0001	O
32.81	O
<	O
0.0001	O
21.89	O
<	O
0.0001	O
−0.18	O
0.8599	O
table	O
3.4.	O
for	O
the	O
advertising	O
data	B
,	O
least	B
squares	I
coeﬃcient	O
estimates	O
of	O
the	O
multiple	B
linear	O
regression	B
of	O
number	O
of	O
units	O
sold	O
on	O
radio	O
,	O
tv	O
,	O
and	O
newspaper	O
advertising	O
budgets	O
.	O
that	O
the	O
simple	B
and	O
multiple	B
regression	O
coeﬃcients	O
can	O
be	O
quite	O
diﬀerent	O
.	O
this	O
diﬀerence	O
stems	O
from	O
the	O
fact	O
that	O
in	O
the	O
simple	B
regression	O
case	O
,	O
the	O
slope	B
term	O
represents	O
the	O
average	B
eﬀect	O
of	O
a	O
$	O
1,000	O
increase	O
in	O
newspaper	O
advertising	O
,	O
ignoring	O
other	O
predictors	O
such	O
as	O
tv	O
and	O
radio	O
.	O
in	O
contrast	B
,	O
in	O
the	O
multiple	B
regression	O
setting	O
,	O
the	O
coeﬃcient	B
for	O
newspaper	O
represents	O
the	O
average	B
eﬀect	O
of	O
increasing	O
newspaper	O
spending	O
by	O
$	O
1,000	O
while	O
holding	O
tv	O
and	O
radio	O
ﬁxed	O
.	O
does	O
it	O
make	O
sense	O
for	O
the	O
multiple	B
regression	O
to	O
suggest	O
no	O
relationship	O
between	O
sales	O
and	O
newspaper	O
while	O
the	O
simple	B
linear	O
regression	B
implies	O
the	O
opposite	O
?	O
in	O
fact	O
it	O
does	O
.	O
consider	O
the	O
correlation	B
matrix	O
for	O
the	O
three	O
predictor	B
variables	O
and	O
response	B
variable	O
,	O
displayed	O
in	O
table	O
3.5.	O
notice	O
that	O
the	O
correlation	B
between	O
radio	O
and	O
newspaper	O
is	O
0.35.	O
this	O
reveals	O
a	O
tendency	O
to	O
spend	O
more	O
on	O
newspaper	O
advertising	O
in	O
markets	O
where	O
more	O
is	O
spent	O
on	O
radio	O
advertising	O
.	O
now	O
suppose	O
that	O
the	O
multiple	B
regression	O
is	O
correct	O
and	O
newspaper	O
advertising	O
has	O
no	O
direct	O
impact	O
on	O
sales	O
,	O
but	O
radio	O
advertising	O
does	O
increase	O
sales	O
.	O
then	O
in	O
markets	O
where	O
we	O
spend	O
more	O
on	O
radio	O
our	O
sales	O
will	O
tend	O
to	O
be	O
higher	O
,	O
and	O
as	O
our	O
correlation	B
matrix	O
shows	O
,	O
we	O
also	O
tend	O
to	O
spend	O
more	O
on	O
newspaper	O
advertising	O
in	O
those	O
same	O
markets	O
.	O
hence	O
,	O
in	O
a	O
simple	B
linear	O
regression	B
which	O
only	O
examines	O
sales	O
versus	O
newspaper	O
,	O
we	O
will	O
observe	O
that	O
higher	O
values	O
of	O
newspaper	O
tend	O
to	O
be	O
associated	O
with	O
higher	O
values	O
of	O
sales	O
,	O
even	O
though	O
newspaper	O
advertising	O
does	O
not	O
actually	O
aﬀect	O
sales	O
.	O
so	O
newspaper	O
sales	O
are	O
a	O
surrogate	O
for	O
radio	O
advertising	O
;	O
newspaper	O
gets	O
“	O
credit	O
”	O
for	O
the	O
eﬀect	O
of	O
radio	O
on	O
sales	O
.	O
this	O
slightly	O
counterintuitive	O
result	O
is	O
very	O
common	O
in	O
many	O
real	O
life	O
situations	O
.	O
consider	O
an	O
absurd	O
example	O
to	O
illustrate	O
the	O
point	O
.	O
running	O
a	O
regression	B
of	O
shark	O
attacks	O
versus	O
ice	O
cream	O
sales	O
for	O
data	O
collected	O
at	O
a	O
given	O
beach	O
community	O
over	O
a	O
period	O
of	O
time	O
would	O
show	O
a	O
positive	O
relationship	O
,	O
similar	O
to	O
that	O
seen	O
between	O
sales	O
and	O
newspaper	O
.	O
of	O
course	O
no	O
one	O
(	O
yet	O
)	O
has	O
suggested	O
that	O
ice	O
creams	O
should	O
be	O
banned	O
at	O
beaches	O
to	O
reduce	O
shark	O
attacks	O
.	O
in	O
reality	O
,	O
higher	O
temperatures	O
cause	O
more	O
people	O
to	O
visit	O
the	O
beach	O
,	O
which	O
in	O
turn	O
results	O
in	O
more	O
ice	O
cream	O
sales	O
and	O
more	O
shark	O
attacks	O
.	O
a	O
multiple	B
regression	O
of	O
attacks	O
versus	O
ice	O
cream	O
sales	O
and	O
temperature	O
reveals	O
that	O
,	O
as	O
intuition	O
implies	O
,	O
the	O
former	O
predictor	B
is	O
no	O
longer	O
signiﬁcant	O
after	O
adjusting	O
for	O
temperature	O
.	O
3.2	O
multiple	B
linear	O
regression	B
75	O
tv	O
radio	O
newspaper	O
sales	O
tv	O
1.0000	O
radio	O
0.0548	O
1.0000	O
newspaper	O
0.0567	O
0.3541	O
1.0000	O
sales	O
0.7822	O
0.5762	O
0.2283	O
1.0000	O
table	O
3.5.	O
correlation	B
matrix	O
for	O
tv	O
,	O
radio	O
,	O
newspaper	O
,	O
and	O
sales	O
for	O
the	O
advertising	O
data	B
.	O
3.2.2	O
some	O
important	O
questions	O
when	O
we	O
perform	O
multiple	B
linear	O
regression	B
,	O
we	O
usually	O
are	O
interested	O
in	O
answering	O
a	O
few	O
important	O
questions	O
.	O
1.	O
is	O
at	O
least	O
one	O
of	O
the	O
predictors	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
useful	O
in	O
predicting	O
the	O
response	B
?	O
2.	O
do	O
all	O
the	O
predictors	O
help	O
to	O
explain	O
y	O
,	O
or	O
is	O
only	O
a	O
subset	O
of	O
the	O
predictors	O
useful	O
?	O
3.	O
how	O
well	O
does	O
the	O
model	B
ﬁt	O
the	O
data	B
?	O
4.	O
given	O
a	O
set	B
of	O
predictor	B
values	O
,	O
what	O
response	B
value	O
should	O
we	O
predict	O
,	O
and	O
how	O
accurate	O
is	O
our	O
prediction	B
?	O
we	O
now	O
address	O
each	O
of	O
these	O
questions	O
in	O
turn	O
.	O
one	O
:	O
is	O
there	O
a	O
relationship	O
between	O
the	O
response	B
and	O
predictors	O
?	O
recall	B
that	O
in	O
the	O
simple	B
linear	O
regression	B
setting	O
,	O
in	O
order	O
to	O
determine	O
whether	O
there	O
is	O
a	O
relationship	O
between	O
the	O
response	B
and	O
the	O
predictor	B
we	O
can	O
simply	O
check	O
whether	O
β1	O
=	O
0.	O
in	O
the	O
multiple	B
regression	O
setting	O
with	O
p	O
predictors	O
,	O
we	O
need	O
to	O
ask	O
whether	O
all	O
of	O
the	O
regression	B
coeﬃcients	O
are	O
zero	O
,	O
i.e	O
.	O
whether	O
β1	O
=	O
β2	O
=	O
···	O
=	O
βp	O
=	O
0.	O
as	O
in	O
the	O
simple	B
linear	O
regression	B
setting	O
,	O
we	O
use	O
a	O
hypothesis	B
test	I
to	O
answer	O
this	O
question	O
.	O
we	O
test	B
the	O
null	B
hypothesis	O
,	O
h0	O
:	O
β1	O
=	O
β2	O
=	O
···	O
=	O
βp	O
=	O
0	O
versus	O
the	O
alternative	O
ha	O
:	O
at	O
least	O
one	O
βj	O
is	O
non-zero	O
.	O
this	O
hypothesis	B
test	I
is	O
performed	O
by	O
computing	O
the	O
f-statistic	O
,	O
(	O
tss	O
−	O
rss	O
)	O
/p	O
rss/	O
(	O
n	O
−	O
p	O
−	O
1	O
)	O
,	O
f	O
=	O
f-statistic	O
(	O
3.23	O
)	O
76	O
3.	O
linear	B
regression	I
quantity	O
residual	B
standard	O
error	B
r2	O
f-statistic	O
value	O
1.69	O
0.897	O
570	O
table	O
3.6.	O
more	O
information	O
about	O
the	O
least	B
squares	I
model	O
for	O
the	O
regression	B
of	O
number	O
of	O
units	O
sold	O
on	O
tv	O
,	O
newspaper	O
,	O
and	O
radio	O
advertising	O
budgets	O
in	O
the	O
advertising	O
data	B
.	O
other	O
information	O
about	O
this	O
model	B
was	O
displayed	O
in	O
table	O
3.4	O
.	O
(	O
cid:10	O
)	O
where	O
,	O
as	O
with	O
simple	B
linear	O
regression	B
,	O
tss	O
=	O
(	O
yi	O
−	O
¯y	O
)	O
2	O
and	O
rss	O
=	O
(	O
yi−	O
ˆyi	O
)	O
2.	O
if	O
the	O
linear	B
model	I
assumptions	O
are	O
correct	O
,	O
one	O
can	O
show	O
that	O
(	O
cid:10	O
)	O
e	O
{	O
rss/	O
(	O
n	O
−	O
p	O
−	O
1	O
)	O
}	O
=	O
σ2	O
and	O
that	O
,	O
provided	O
h0	O
is	O
true	O
,	O
e	O
{	O
(	O
tss	O
−	O
rss	O
)	O
/p	O
}	O
=	O
σ2	O
.	O
hence	O
,	O
when	O
there	O
is	O
no	O
relationship	O
between	O
the	O
response	B
and	O
predictors	O
,	O
one	O
would	O
expect	O
the	O
f-statistic	O
to	O
take	O
on	O
a	O
value	O
close	O
to	O
1.	O
on	O
the	O
other	O
hand	O
,	O
if	O
ha	O
is	O
true	O
,	O
then	O
e	O
{	O
(	O
tss	O
−	O
rss	O
)	O
/p	O
}	O
>	O
σ2	O
,	O
so	O
we	O
expect	O
f	O
to	O
be	O
greater	O
than	O
1.	O
the	O
f-statistic	O
for	O
the	O
multiple	B
linear	O
regression	B
model	O
obtained	O
by	O
re-	O
gressing	O
sales	O
onto	O
radio	O
,	O
tv	O
,	O
and	O
newspaper	O
is	O
shown	O
in	O
table	O
3.6.	O
in	O
this	O
example	O
the	O
f-statistic	O
is	O
570.	O
since	O
this	O
is	O
far	O
larger	O
than	O
1	O
,	O
it	O
provides	O
compelling	O
evidence	O
against	O
the	O
null	B
hypothesis	O
h0	O
.	O
in	O
other	O
words	O
,	O
the	O
large	O
f-statistic	O
suggests	O
that	O
at	O
least	O
one	O
of	O
the	O
advertising	O
media	O
must	O
be	O
related	O
to	O
sales	O
.	O
however	O
,	O
what	O
if	O
the	O
f-statistic	O
had	O
been	O
closer	O
to	O
1	O
?	O
how	O
large	O
does	O
the	O
f-statistic	O
need	O
to	O
be	O
before	O
we	O
can	O
reject	O
h0	O
and	O
conclude	O
that	O
there	O
is	O
a	O
relationship	O
?	O
it	O
turns	O
out	O
that	O
the	O
answer	O
depends	O
on	O
the	O
values	O
of	O
n	O
and	O
p.	O
when	O
n	O
is	O
large	O
,	O
an	O
f-statistic	O
that	O
is	O
just	O
a	O
little	O
larger	O
than	O
1	O
might	O
still	O
provide	O
evidence	O
against	O
h0	O
.	O
in	O
contrast	B
,	O
a	O
larger	O
f-statistic	O
is	O
needed	O
to	O
reject	O
h0	O
if	O
n	O
is	O
small	O
.	O
when	O
h0	O
is	O
true	O
and	O
the	O
errors	O
i	O
have	O
a	O
normal	O
distribution	O
,	O
the	O
f-statistic	O
follows	O
an	O
f-distribution.6	O
for	O
any	O
given	O
value	O
of	O
n	O
and	O
p	O
,	O
any	O
statistical	O
software	O
package	O
can	O
be	O
used	O
to	O
compute	O
the	O
p-value	B
associated	O
with	O
the	O
f-statistic	O
using	O
this	O
distribution	B
.	O
based	O
on	O
this	O
p-value	B
,	O
we	O
can	O
determine	O
whether	O
or	O
not	O
to	O
reject	O
h0	O
.	O
for	O
the	O
advertising	O
data	B
,	O
the	O
p-value	B
associated	O
with	O
the	O
f-statistic	O
in	O
table	O
3.6	O
is	O
essentially	O
zero	O
,	O
so	O
we	O
have	O
extremely	O
strong	O
evidence	O
that	O
at	O
least	O
one	O
of	O
the	O
media	O
is	O
associated	O
with	O
increased	O
sales	O
.	O
in	O
(	O
3.23	O
)	O
we	O
are	O
testing	O
h0	O
that	O
all	O
the	O
coeﬃcients	O
are	O
zero	O
.	O
sometimes	O
we	O
want	O
to	O
test	B
that	O
a	O
particular	O
subset	O
of	O
q	O
of	O
the	O
coeﬃcients	O
are	O
zero	O
.	O
this	O
corresponds	O
to	O
a	O
null	B
hypothesis	O
h0	O
:	O
βp−q+1	O
=	O
βp−q+2	O
=	O
.	O
.	O
.	O
=	O
βp	O
=	O
0	O
,	O
6even	O
if	O
the	O
errors	O
are	O
not	O
normally-distributed	O
,	O
the	O
f-statistic	O
approximately	O
follows	O
an	O
f-distribution	O
provided	O
that	O
the	O
sample	O
size	O
n	O
is	O
large	O
.	O
3.2	O
multiple	B
linear	O
regression	B
77	O
where	O
for	O
convenience	O
we	O
have	O
put	O
the	O
variables	O
chosen	O
for	O
omission	O
at	O
the	O
end	O
of	O
the	O
list	O
.	O
in	O
this	O
case	O
we	O
ﬁt	B
a	O
second	O
model	B
that	O
uses	O
all	O
the	O
variables	O
except	O
those	O
last	O
q.	O
suppose	O
that	O
the	O
residual	B
sum	O
of	O
squares	O
for	O
that	O
model	B
is	O
rss0	O
.	O
then	O
the	O
appropriate	O
f-statistic	O
is	O
(	O
rss0	O
−	O
rss	O
)	O
/q	O
rss/	O
(	O
n	O
−	O
p	O
−	O
1	O
)	O
.	O
f	O
=	O
(	O
3.24	O
)	O
notice	O
that	O
in	O
table	O
3.4	O
,	O
for	O
each	O
individual	O
predictor	B
a	O
t-statistic	B
and	O
a	O
p-value	B
were	O
reported	O
.	O
these	O
provide	O
information	O
about	O
whether	O
each	O
individual	O
predictor	B
is	O
related	O
to	O
the	O
response	B
,	O
after	O
adjusting	O
for	O
the	O
other	O
predictors	O
.	O
it	O
turns	O
out	O
that	O
each	O
of	O
these	O
are	O
exactly	O
equivalent7	O
to	O
the	O
f-test	O
that	O
omits	O
that	O
single	B
variable	O
from	O
the	O
model	B
,	O
leaving	O
all	O
the	O
others	O
in—i.e	O
.	O
q=1	O
in	O
(	O
3.24	O
)	O
.	O
so	O
it	O
reports	O
the	O
partial	O
eﬀect	O
of	O
adding	O
that	O
variable	B
to	O
the	O
model	B
.	O
for	O
instance	O
,	O
as	O
we	O
discussed	O
earlier	O
,	O
these	O
p-values	O
indicate	O
that	O
tv	O
and	O
radio	O
are	O
related	O
to	O
sales	O
,	O
but	O
that	O
there	O
is	O
no	O
evidence	O
that	O
newspaper	O
is	O
associated	O
with	O
sales	O
,	O
in	O
the	O
presence	O
of	O
these	O
two	O
.	O
given	O
these	O
individual	O
p-values	O
for	O
each	O
variable	B
,	O
why	O
do	O
we	O
need	O
to	O
look	O
at	O
the	O
overall	O
f-statistic	O
?	O
after	O
all	O
,	O
it	O
seems	O
likely	O
that	O
if	O
any	O
one	O
of	O
the	O
p-values	O
for	O
the	O
individual	O
variables	O
is	O
very	O
small	O
,	O
then	O
at	O
least	O
one	O
of	O
the	O
predictors	O
is	O
related	O
to	O
the	O
response	B
.	O
however	O
,	O
this	O
logic	O
is	O
ﬂawed	O
,	O
especially	O
when	O
the	O
number	O
of	O
predictors	O
p	O
is	O
large	O
.	O
for	O
instance	O
,	O
consider	O
an	O
example	O
in	O
which	O
p	O
=	O
100	O
and	O
h0	O
:	O
β1	O
=	O
β2	O
=	O
.	O
.	O
.	O
=	O
βp	O
=	O
0	O
is	O
true	O
,	O
so	O
no	O
variable	B
is	O
truly	O
associated	O
with	O
the	O
response	B
.	O
in	O
this	O
situation	O
,	O
about	O
5	O
%	O
of	O
the	O
p-values	O
associated	O
with	O
each	O
variable	B
(	O
of	O
the	O
type	O
shown	O
in	O
table	O
3.4	O
)	O
will	O
be	O
below	O
0.05	O
by	O
chance	O
.	O
in	O
other	O
words	O
,	O
we	O
expect	O
to	O
see	O
approximately	O
ﬁve	O
small	O
p-values	O
even	O
in	O
the	O
absence	O
of	O
any	O
true	O
association	O
between	O
the	O
predictors	O
and	O
the	O
response	B
.	O
in	O
fact	O
,	O
we	O
are	O
almost	O
guaranteed	O
that	O
we	O
will	O
observe	O
at	O
least	O
one	O
p-value	B
below	O
0.05	O
by	O
chance	O
!	O
hence	O
,	O
if	O
we	O
use	O
the	O
individual	O
t-statistics	O
and	O
associated	O
p-	O
values	O
in	O
order	O
to	O
decide	O
whether	O
or	O
not	O
there	O
is	O
any	O
association	O
between	O
the	O
variables	O
and	O
the	O
response	B
,	O
there	O
is	O
a	O
very	O
high	O
chance	O
that	O
we	O
will	O
incorrectly	O
conclude	O
that	O
there	O
is	O
a	O
relationship	O
.	O
however	O
,	O
the	O
f-statistic	O
does	O
not	O
suﬀer	O
from	O
this	O
problem	O
because	O
it	O
adjusts	O
for	O
the	O
number	O
of	O
predictors	O
.	O
hence	O
,	O
if	O
h0	O
is	O
true	O
,	O
there	O
is	O
only	O
a	O
5	O
%	O
chance	O
that	O
the	O
f-	O
statistic	O
will	O
result	O
in	O
a	O
p-value	B
below	O
0.05	O
,	O
regardless	O
of	O
the	O
number	O
of	O
predictors	O
or	O
the	O
number	O
of	O
observations	B
.	O
the	O
approach	B
of	O
using	O
an	O
f-statistic	O
to	O
test	B
for	O
any	O
association	O
between	O
the	O
predictors	O
and	O
the	O
response	B
works	O
when	O
p	O
is	O
relatively	O
small	O
,	O
and	O
cer-	O
tainly	O
small	O
compared	O
to	O
n.	O
however	O
,	O
sometimes	O
we	O
have	O
a	O
very	O
large	O
num-	O
ber	O
of	O
variables	O
.	O
if	O
p	O
>	O
n	O
then	O
there	O
are	O
more	O
coeﬃcients	O
βj	O
to	O
estimate	O
than	O
observations	B
from	O
which	O
to	O
estimate	O
them	O
.	O
in	O
this	O
case	O
we	O
can	O
not	O
even	O
ﬁt	B
the	O
multiple	B
linear	O
regression	B
model	O
using	O
least	B
squares	I
,	O
so	O
the	O
7the	O
square	O
of	O
each	O
t-statistic	B
is	O
the	O
corresponding	O
f-statistic	O
.	O
78	O
3.	O
linear	B
regression	I
f-statistic	O
can	O
not	O
be	O
used	O
,	O
and	O
neither	O
can	O
most	O
of	O
the	O
other	O
concepts	O
that	O
we	O
have	O
seen	O
so	O
far	O
in	O
this	O
chapter	O
.	O
when	O
p	O
is	O
large	O
,	O
some	O
of	O
the	O
approaches	O
discussed	O
in	O
the	O
next	O
section	O
,	O
such	O
as	O
forward	O
selection	O
,	O
can	O
be	O
used	O
.	O
this	O
high-dimensional	B
setting	O
is	O
discussed	O
in	O
greater	O
detail	O
in	O
chapter	O
6.	O
high-	O
dimensional	O
two	O
:	O
deciding	O
on	O
important	O
variables	O
as	O
discussed	O
in	O
the	O
previous	O
section	O
,	O
the	O
ﬁrst	O
step	O
in	O
a	O
multiple	B
regression	O
analysis	B
is	O
to	O
compute	O
the	O
f-statistic	O
and	O
to	O
examine	O
the	O
associated	O
p-	O
value	O
.	O
if	O
we	O
conclude	O
on	O
the	O
basis	B
of	O
that	O
p-value	B
that	O
at	O
least	O
one	O
of	O
the	O
predictors	O
is	O
related	O
to	O
the	O
response	B
,	O
then	O
it	O
is	O
natural	B
to	O
wonder	O
which	O
are	O
the	O
guilty	O
ones	O
!	O
we	O
could	O
look	O
at	O
the	O
individual	O
p-values	O
as	O
in	O
table	O
3.4	O
,	O
but	O
as	O
discussed	O
,	O
if	O
p	O
is	O
large	O
we	O
are	O
likely	O
to	O
make	O
some	O
false	O
discoveries	O
.	O
it	O
is	O
possible	O
that	O
all	O
of	O
the	O
predictors	O
are	O
associated	O
with	O
the	O
response	B
,	O
but	O
it	O
is	O
more	O
often	O
the	O
case	O
that	O
the	O
response	B
is	O
only	O
related	O
to	O
a	O
subset	O
of	O
the	O
predictors	O
.	O
the	O
task	O
of	O
determining	O
which	O
predictors	O
are	O
associated	O
with	O
the	O
response	B
,	O
in	O
order	O
to	O
ﬁt	B
a	O
single	B
model	O
involving	O
only	O
those	O
predictors	O
,	O
is	O
referred	O
to	O
as	O
variable	B
selection	O
.	O
the	O
variable	B
selection	O
problem	O
is	O
studied	O
extensively	O
in	O
chapter	O
6	O
,	O
and	O
so	O
here	O
we	O
will	O
provide	O
only	O
a	O
brief	O
outline	O
of	O
some	O
classical	O
approaches	O
.	O
ideally	O
,	O
we	O
would	O
like	O
to	O
perform	O
variable	B
selection	O
by	O
trying	O
out	O
a	O
lot	O
of	O
diﬀerent	O
models	O
,	O
each	O
containing	O
a	O
diﬀerent	O
subset	O
of	O
the	O
predictors	O
.	O
for	O
instance	O
,	O
if	O
p	O
=	O
2	O
,	O
then	O
we	O
can	O
consider	O
four	O
models	O
:	O
(	O
1	O
)	O
a	O
model	B
contain-	O
ing	O
no	O
variables	O
,	O
(	O
2	O
)	O
a	O
model	B
containing	O
x1	O
only	O
,	O
(	O
3	O
)	O
a	O
model	B
containing	O
x2	B
only	O
,	O
and	O
(	O
4	O
)	O
a	O
model	B
containing	O
both	O
x1	O
and	O
x2	B
.	O
we	O
can	O
then	O
se-	O
lect	O
the	O
best	O
model	O
out	O
of	O
all	O
of	O
the	O
models	O
that	O
we	O
have	O
considered	O
.	O
how	O
do	O
we	O
determine	O
which	O
model	B
is	O
best	O
?	O
various	O
statistics	O
can	O
be	O
used	O
to	O
judge	O
the	O
quality	O
of	O
a	O
model	B
.	O
these	O
include	O
mallow	O
’	O
s	O
cp	O
,	O
akaike	O
informa-	O
tion	O
criterion	O
(	O
aic	O
)	O
,	O
bayesian	O
information	O
criterion	O
(	O
bic	O
)	O
,	O
and	O
adjusted	O
r2	O
.	O
these	O
are	O
discussed	O
in	O
more	O
detail	O
in	O
chapter	O
6.	O
we	O
can	O
also	O
deter-	O
mine	O
which	O
model	B
is	O
best	O
by	O
plotting	O
various	O
model	B
outputs	O
,	O
such	O
as	O
the	O
residuals	B
,	O
in	O
order	O
to	O
search	O
for	O
patterns	O
.	O
unfortunately	O
,	O
there	O
are	O
a	O
total	O
of	O
2p	O
models	O
that	O
contain	O
subsets	O
of	O
p	O
variables	O
.	O
this	O
means	O
that	O
even	O
for	O
moderate	O
p	O
,	O
trying	O
out	O
every	O
possible	O
subset	O
of	O
the	O
predictors	O
is	O
infeasible	O
.	O
for	O
instance	O
,	O
we	O
saw	O
that	O
if	O
p	O
=	O
2	O
,	O
then	O
there	O
are	O
22	O
=	O
4	O
models	O
to	O
consider	O
.	O
but	O
if	O
p	O
=	O
30	O
,	O
then	O
we	O
must	O
consider	O
230	O
=	O
1,073,741,824	O
models	O
!	O
this	O
is	O
not	O
practical	O
.	O
therefore	O
,	O
unless	O
p	O
is	O
very	O
small	O
,	O
we	O
can	O
not	O
consider	O
all	O
2p	O
models	O
,	O
and	O
instead	O
we	O
need	O
an	O
automated	O
and	O
eﬃcient	O
approach	B
to	O
choose	O
a	O
smaller	O
set	B
of	O
models	O
to	O
consider	O
.	O
there	O
are	O
three	O
classical	O
approaches	O
for	O
this	O
task	O
:	O
variable	B
selection	O
mallow	O
’	O
s	O
cp	O
akaike	O
information	O
criterion	O
bayesian	O
information	O
criterion	O
adjusted	O
r2	O
•	O
forward	O
selection	O
.	O
we	O
begin	O
with	O
the	O
null	B
model—a	O
model	B
that	O
con-	O
tains	O
an	O
intercept	B
but	O
no	O
predictors	O
.	O
we	O
then	O
ﬁt	B
p	O
simple	B
linear	O
re-	O
gressions	O
and	O
add	O
to	O
the	O
null	B
model	O
the	O
variable	B
that	O
results	O
in	O
the	O
lowest	O
rss	O
.	O
we	O
then	O
add	O
to	O
that	O
model	B
the	O
variable	B
that	O
results	O
forward	O
selection	O
null	B
model	O
3.2	O
multiple	B
linear	O
regression	B
79	O
in	O
the	O
lowest	O
rss	O
for	O
the	O
new	O
two-variable	O
model	B
.	O
this	O
approach	B
is	O
continued	O
until	O
some	O
stopping	O
rule	O
is	O
satisﬁed	O
.	O
•	O
backward	O
selection	O
.	O
we	O
start	O
with	O
all	O
variables	O
in	O
the	O
model	B
,	O
and	O
remove	O
the	O
variable	B
with	O
the	O
largest	O
p-value—that	O
is	O
,	O
the	O
variable	B
that	O
is	O
the	O
least	O
statistically	O
signiﬁcant	O
.	O
the	O
new	O
(	O
p	O
−	O
1	O
)	O
-variable	O
model	B
is	O
ﬁt	B
,	O
and	O
the	O
variable	B
with	O
the	O
largest	O
p-value	B
is	O
removed	O
.	O
this	O
procedure	O
continues	O
until	O
a	O
stopping	O
rule	O
is	O
reached	O
.	O
for	O
instance	O
,	O
we	O
may	O
stop	O
when	O
all	O
remaining	O
variables	O
have	O
a	O
p-value	B
below	O
some	O
threshold	O
.	O
•	O
mixed	B
selection	I
.	O
this	O
is	O
a	O
combination	O
of	O
forward	O
and	O
backward	O
se-	O
lection	O
.	O
we	O
start	O
with	O
no	O
variables	O
in	O
the	O
model	B
,	O
and	O
as	O
with	O
forward	O
selection	O
,	O
we	O
add	O
the	O
variable	B
that	O
provides	O
the	O
best	O
ﬁt	O
.	O
we	O
con-	O
tinue	O
to	O
add	O
variables	O
one-by-one	O
.	O
of	O
course	O
,	O
as	O
we	O
noted	O
with	O
the	O
advertising	O
example	O
,	O
the	O
p-values	O
for	O
variables	O
can	O
become	O
larger	O
as	O
new	O
predictors	O
are	O
added	O
to	O
the	O
model	B
.	O
hence	O
,	O
if	O
at	O
any	O
point	O
the	O
p-value	B
for	O
one	O
of	O
the	O
variables	O
in	O
the	O
model	B
rises	O
above	O
a	O
certain	O
threshold	O
,	O
then	O
we	O
remove	O
that	O
variable	B
from	O
the	O
model	B
.	O
we	O
con-	O
tinue	O
to	O
perform	O
these	O
forward	O
and	O
backward	O
steps	O
until	O
all	O
variables	O
in	O
the	O
model	B
have	O
a	O
suﬃciently	O
low	O
p-value	B
,	O
and	O
all	O
variables	O
outside	O
the	O
model	B
would	O
have	O
a	O
large	O
p-value	B
if	O
added	O
to	O
the	O
model	B
.	O
backward	O
selection	O
mixed	B
selection	I
backward	O
selection	B
can	O
not	O
be	O
used	O
if	O
p	O
>	O
n	O
,	O
while	O
forward	O
selection	O
can	O
always	O
be	O
used	O
.	O
forward	O
selection	O
is	O
a	O
greedy	O
approach	B
,	O
and	O
might	O
include	O
variables	O
early	O
that	O
later	O
become	O
redundant	O
.	O
mixed	B
selection	I
can	O
remedy	O
this	O
.	O
three	O
:	O
model	B
fit	O
two	O
of	O
the	O
most	O
common	O
numerical	O
measures	O
of	O
model	B
ﬁt	O
are	O
the	O
rse	O
and	O
r2	O
,	O
the	O
fraction	O
of	O
variance	B
explained	O
.	O
these	O
quantities	O
are	O
computed	O
and	O
interpreted	O
in	O
the	O
same	O
fashion	O
as	O
for	O
simple	O
linear	B
regression	I
.	O
recall	B
that	O
in	O
simple	B
regression	O
,	O
r2	O
is	O
the	O
square	O
of	O
the	O
correlation	B
of	O
the	O
response	B
and	O
the	O
variable	B
.	O
in	O
multiple	B
linear	O
regression	B
,	O
it	O
turns	O
out	O
that	O
it	O
equals	O
cor	O
(	O
y	O
,	O
ˆy	O
)	O
2	O
,	O
the	O
square	O
of	O
the	O
correlation	B
between	O
the	O
response	B
and	O
the	O
ﬁtted	O
linear	O
model	B
;	O
in	O
fact	O
one	O
property	O
of	O
the	O
ﬁtted	O
linear	O
model	B
is	O
that	O
it	O
maximizes	O
this	O
correlation	B
among	O
all	O
possible	O
linear	B
models	O
.	O
an	O
r2	O
value	O
close	O
to	O
1	O
indicates	O
that	O
the	O
model	B
explains	O
a	O
large	O
portion	O
of	O
the	O
variance	B
in	O
the	O
response	B
variable	O
.	O
as	O
an	O
example	O
,	O
we	O
saw	O
in	O
table	O
3.6	O
that	O
for	O
the	O
advertising	O
data	B
,	O
the	O
model	B
that	O
uses	O
all	O
three	O
advertising	O
me-	O
dia	O
to	O
predict	O
sales	O
has	O
an	O
r2	O
of	O
0.8972.	O
on	O
the	O
other	O
hand	O
,	O
the	O
model	B
that	O
uses	O
only	O
tv	O
and	O
radio	O
to	O
predict	O
sales	O
has	O
an	O
r2	O
value	O
of	O
0.89719.	O
in	O
other	O
words	O
,	O
there	O
is	O
a	O
small	O
increase	O
in	O
r2	O
if	O
we	O
include	O
newspaper	O
advertising	O
in	O
the	O
model	B
that	O
already	O
contains	O
tv	O
and	O
radio	O
advertising	O
,	O
even	O
though	O
we	O
saw	O
earlier	O
that	O
the	O
p-value	B
for	O
newspaper	O
advertising	O
in	O
table	O
3.4	O
is	O
not	O
signiﬁcant	O
.	O
it	O
turns	O
out	O
that	O
r2	O
will	O
always	O
increase	O
when	O
more	O
variables	O
80	O
3.	O
linear	B
regression	I
are	O
added	O
to	O
the	O
model	B
,	O
even	O
if	O
those	O
variables	O
are	O
only	O
weakly	O
associated	O
with	O
the	O
response	B
.	O
this	O
is	O
due	O
to	O
the	O
fact	O
that	O
adding	O
another	O
variable	B
to	O
the	O
least	B
squares	I
equations	O
must	O
allow	O
us	O
to	O
ﬁt	B
the	O
training	B
data	O
(	O
though	O
not	O
necessarily	O
the	O
testing	O
data	B
)	O
more	O
accurately	O
.	O
thus	O
,	O
the	O
r2	O
statistic	O
,	O
which	O
is	O
also	O
computed	O
on	O
the	O
training	B
data	O
,	O
must	O
increase	O
.	O
the	O
fact	O
that	O
adding	O
newspaper	O
advertising	O
to	O
the	O
model	B
containing	O
only	O
tv	O
and	O
radio	O
advertising	O
leads	O
to	O
just	O
a	O
tiny	O
increase	O
in	O
r2	O
provides	O
additional	O
evidence	O
that	O
newspaper	O
can	O
be	O
dropped	O
from	O
the	O
model	B
.	O
essentially	O
,	O
newspaper	O
pro-	O
vides	O
no	O
real	O
improvement	O
in	O
the	O
model	B
ﬁt	O
to	O
the	O
training	B
samples	O
,	O
and	O
its	O
inclusion	O
will	O
likely	O
lead	O
to	O
poor	O
results	O
on	O
independent	B
test	O
samples	O
due	O
to	O
overﬁtting	B
.	O
in	O
contrast	B
,	O
the	O
model	B
containing	O
only	O
tv	O
as	O
a	O
predictor	B
had	O
an	O
r2	O
of	O
0.61	O
(	O
table	O
3.2	O
)	O
.	O
adding	O
radio	O
to	O
the	O
model	B
leads	O
to	O
a	O
substantial	O
improvement	O
in	O
r2	O
.	O
this	O
implies	O
that	O
a	O
model	B
that	O
uses	O
tv	O
and	O
radio	O
expenditures	O
to	O
predict	O
sales	O
is	O
substantially	O
better	O
than	O
one	O
that	O
uses	O
only	O
tv	O
advertis-	O
ing	O
.	O
we	O
could	O
further	O
quantify	O
this	O
improvement	O
by	O
looking	O
at	O
the	O
p-value	B
for	O
the	O
radio	O
coeﬃcient	B
in	O
a	O
model	B
that	O
contains	O
only	O
tv	O
and	O
radio	O
as	O
predictors	O
.	O
the	O
model	B
that	O
contains	O
only	O
tv	O
and	O
radio	O
as	O
predictors	O
has	O
an	O
rse	O
of	O
1.681	O
,	O
and	O
the	O
model	B
that	O
also	O
contains	O
newspaper	O
as	O
a	O
predictor	B
has	O
an	O
rse	O
of	O
1.686	O
(	O
table	O
3.6	O
)	O
.	O
in	O
contrast	B
,	O
the	O
model	B
that	O
contains	O
only	O
tv	O
has	O
an	O
rse	O
of	O
3.26	O
(	O
table	O
3.2	O
)	O
.	O
this	O
corroborates	O
our	O
previous	O
conclusion	O
that	O
a	O
model	B
that	O
uses	O
tv	O
and	O
radio	O
expenditures	O
to	O
predict	O
sales	O
is	O
much	O
more	O
accurate	O
(	O
on	O
the	O
training	B
data	O
)	O
than	O
one	O
that	O
only	O
uses	O
tv	O
spending	O
.	O
furthermore	O
,	O
given	O
that	O
tv	O
and	O
radio	O
expenditures	O
are	O
used	O
as	O
predictors	O
,	O
there	O
is	O
no	O
point	O
in	O
also	O
using	O
newspaper	O
spending	O
as	O
a	O
predictor	B
in	O
the	O
model	B
.	O
the	O
observant	O
reader	O
may	O
wonder	O
how	O
rse	O
can	O
increase	O
when	O
newspaper	O
is	O
added	O
to	O
the	O
model	B
given	O
that	O
rss	O
must	O
decrease	O
.	O
in	O
general	O
rse	O
is	O
deﬁned	O
as	O
(	O
cid:26	O
)	O
rse	O
=	O
1	O
n	O
−	O
p	O
−	O
1	O
rss	O
,	O
(	O
3.25	O
)	O
which	O
simpliﬁes	O
to	O
(	O
3.15	O
)	O
for	O
a	O
simple	B
linear	O
regression	B
.	O
thus	O
,	O
models	O
with	O
more	O
variables	O
can	O
have	O
higher	O
rse	O
if	O
the	O
decrease	O
in	O
rss	O
is	O
small	O
relative	O
to	O
the	O
increase	O
in	O
p.	O
in	O
addition	O
to	O
looking	O
at	O
the	O
rse	O
and	O
r2	O
statistics	O
just	O
discussed	O
,	O
it	O
can	O
be	O
useful	O
to	O
plot	B
the	O
data	B
.	O
graphical	O
summaries	O
can	O
reveal	O
problems	O
with	O
a	O
model	B
that	O
are	O
not	O
visible	O
from	O
numerical	O
statistics	O
.	O
for	O
example	O
,	O
figure	O
3.5	O
displays	O
a	O
three-dimensional	O
plot	B
of	O
tv	O
and	O
radio	O
versus	O
sales	O
.	O
we	O
see	O
that	O
some	O
observations	B
lie	O
above	O
and	O
some	O
observations	B
lie	O
below	O
the	O
least	B
squares	I
regression	O
plane	O
.	O
in	O
particular	O
,	O
the	O
linear	B
model	I
seems	O
to	O
overestimate	O
sales	O
for	O
instances	O
in	O
which	O
most	O
of	O
the	O
advertising	O
money	O
was	O
spent	O
exclusively	O
on	O
either	O
tv	O
or	O
radio	O
.	O
it	O
underestimates	O
sales	O
for	O
instances	O
where	O
the	O
budget	O
was	O
split	O
between	O
the	O
two	O
media	O
.	O
this	O
pro-	O
nounced	O
non-linear	B
pattern	O
can	O
not	O
be	O
modeled	O
accurately	O
using	O
linear	B
re-	O
sales	O
3.2	O
multiple	B
linear	O
regression	B
81	O
tv	O
radio	O
figure	O
3.5.	O
for	O
the	O
advertising	O
data	B
,	O
a	O
linear	B
regression	I
ﬁt	O
to	O
sales	O
using	O
tv	O
and	O
radio	O
as	O
predictors	O
.	O
from	O
the	O
pattern	O
of	O
the	O
residuals	B
,	O
we	O
can	O
see	O
that	O
there	O
is	O
a	O
pronounced	O
non-linear	B
relationship	O
in	O
the	O
data	B
.	O
the	O
positive	O
residuals	O
(	O
those	O
visible	O
above	O
the	O
surface	O
)	O
,	O
tend	O
to	O
lie	O
along	O
the	O
45-degree	O
line	B
,	O
where	O
tv	O
and	O
radio	O
budgets	O
are	O
split	O
evenly	O
.	O
the	O
negative	O
residuals	O
(	O
most	O
not	O
visible	O
)	O
,	O
tend	O
to	O
lie	O
away	O
from	O
this	O
line	B
,	O
where	O
budgets	O
are	O
more	O
lopsided	O
.	O
gression	O
.	O
it	O
suggests	O
a	O
synergy	B
or	O
interaction	B
eﬀect	O
between	O
the	O
advertising	O
media	O
,	O
whereby	O
combining	O
the	O
media	O
together	O
results	O
in	O
a	O
bigger	O
boost	O
to	O
sales	O
than	O
using	O
any	O
single	B
medium	O
.	O
in	O
section	O
3.3.2	O
,	O
we	O
will	O
discuss	O
ex-	O
tending	O
the	O
linear	B
model	I
to	O
accommodate	O
such	O
synergistic	O
eﬀects	O
through	O
the	O
use	O
of	O
interaction	B
terms	O
.	O
four	O
:	O
predictions	O
once	O
we	O
have	O
ﬁt	B
the	O
multiple	B
regression	O
model	B
,	O
it	O
is	O
straightforward	O
to	O
apply	O
(	O
3.21	O
)	O
in	O
order	O
to	O
predict	O
the	O
response	B
y	O
on	O
the	O
basis	B
of	O
a	O
set	B
of	O
values	O
for	O
the	O
predictors	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
.	O
however	O
,	O
there	O
are	O
three	O
sorts	O
of	O
uncertainty	O
associated	O
with	O
this	O
prediction	B
.	O
1.	O
the	O
coeﬃcient	B
estimates	O
ˆβ0	O
,	O
ˆβ1	O
,	O
.	O
.	O
.	O
,	O
ˆβp	O
are	O
estimates	O
for	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
.	O
that	O
is	O
,	O
the	O
least	B
squares	I
plane	O
ˆy	O
=	O
ˆβ0	O
+	O
ˆβ1x1	O
+	O
···	O
+	O
ˆβpxp	O
is	O
only	O
an	O
estimate	O
for	O
the	O
true	O
population	O
regression	B
plane	O
f	O
(	O
x	O
)	O
=	O
β0	O
+	O
β1x1	O
+	O
···	O
+	O
βpxp	O
.	O
the	O
inaccuracy	O
in	O
the	O
coeﬃcient	B
estimates	O
is	O
related	O
to	O
the	O
reducible	B
error	I
from	O
chapter	O
2.	O
we	O
can	O
compute	O
a	O
conﬁdence	B
interval	I
in	O
order	O
to	O
determine	O
how	O
close	O
ˆy	O
will	O
be	O
to	O
f	O
(	O
x	O
)	O
.	O
82	O
3.	O
linear	B
regression	I
2.	O
of	O
course	O
,	O
in	O
practice	O
assuming	O
a	O
linear	B
model	I
for	O
f	O
(	O
x	O
)	O
is	O
almost	O
always	O
an	O
approximation	O
of	O
reality	O
,	O
so	O
there	O
is	O
an	O
additional	O
source	O
of	O
potentially	O
reducible	B
error	I
which	O
we	O
call	O
model	B
bias	O
.	O
so	O
when	O
we	O
use	O
a	O
linear	B
model	I
,	O
we	O
are	O
in	O
fact	O
estimating	O
the	O
best	O
linear	O
approximation	O
to	O
the	O
true	O
surface	O
.	O
however	O
,	O
here	O
we	O
will	O
ignore	O
this	O
discrepancy	O
,	O
and	O
operate	O
as	O
if	O
the	O
linear	B
model	I
were	O
correct	O
.	O
3.	O
even	O
if	O
we	O
knew	O
f	O
(	O
x	O
)	O
—that	O
is	O
,	O
even	O
if	O
we	O
knew	O
the	O
true	O
values	O
for	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp—the	O
response	B
value	O
can	O
not	O
be	O
predicted	O
perfectly	O
because	O
of	O
the	O
random	O
error	O
	O
in	O
the	O
model	B
(	O
3.21	O
)	O
.	O
in	O
chapter	O
2	O
,	O
we	O
referred	O
to	O
this	O
as	O
the	O
irreducible	B
error	I
.	O
how	O
much	O
will	O
y	O
vary	O
from	O
ˆy	O
?	O
we	O
use	O
prediction	B
intervals	O
to	O
answer	O
this	O
question	O
.	O
prediction	B
intervals	O
are	O
always	O
wider	O
than	O
conﬁdence	O
intervals	O
,	O
because	O
they	O
incorporate	O
both	O
the	O
error	B
in	O
the	O
estimate	O
for	O
f	O
(	O
x	O
)	O
(	O
the	O
reducible	B
error	I
)	O
and	O
the	O
uncertainty	O
as	O
to	O
how	O
much	O
an	O
individual	O
point	O
will	O
diﬀer	O
from	O
the	O
population	O
regression	O
plane	O
(	O
the	O
irreducible	B
error	I
)	O
.	O
we	O
use	O
a	O
conﬁdence	B
interval	I
to	O
quantify	O
the	O
uncertainty	O
surrounding	O
the	O
average	B
sales	O
over	O
a	O
large	O
number	O
of	O
cities	O
.	O
for	O
example	O
,	O
given	O
that	O
$	O
100,000	O
is	O
spent	O
on	O
tv	O
advertising	O
and	O
$	O
20,000	O
is	O
spent	O
on	O
radio	O
advertising	O
in	O
each	O
city	O
,	O
the	O
95	O
%	O
conﬁdence	B
interval	I
is	O
[	O
10,985	O
,	O
11,528	O
]	O
.	O
we	O
interpret	O
this	O
to	O
mean	O
that	O
95	O
%	O
of	O
intervals	O
of	O
this	O
form	O
will	O
contain	O
the	O
true	O
value	O
of	O
f	O
(	O
x	O
)	O
.8	O
on	O
the	O
other	O
hand	O
,	O
a	O
prediction	B
interval	O
can	O
be	O
used	O
to	O
quantify	O
the	O
uncertainty	O
surrounding	O
sales	O
for	O
a	O
particular	O
city	O
.	O
given	O
that	O
$	O
100,000	O
is	O
spent	O
on	O
tv	O
advertising	O
and	O
$	O
20,000	O
is	O
spent	O
on	O
radio	O
advertising	O
in	O
that	O
city	O
the	O
95	O
%	O
prediction	B
interval	O
is	O
[	O
7,930	O
,	O
14,580	O
]	O
.	O
we	O
interpret	O
this	O
to	O
mean	O
that	O
95	O
%	O
of	O
intervals	O
of	O
this	O
form	O
will	O
contain	O
the	O
true	O
value	O
of	O
y	O
for	O
this	O
city	O
.	O
note	O
that	O
both	O
intervals	O
are	O
centered	O
at	O
11,256	O
,	O
but	O
that	O
the	O
prediction	B
interval	O
is	O
substantially	O
wider	O
than	O
the	O
conﬁdence	B
interval	I
,	O
reﬂecting	O
the	O
increased	O
uncertainty	O
about	O
sales	O
for	O
a	O
given	O
city	O
in	O
comparison	O
to	O
the	O
average	B
sales	O
over	O
many	O
locations	O
.	O
conﬁdence	B
interval	I
prediction	O
interval	B
3.3	O
other	O
considerations	O
in	O
the	O
regression	B
model	O
3.3.1	O
qualitative	B
predictors	O
in	O
our	O
discussion	O
so	O
far	O
,	O
we	O
have	O
assumed	O
that	O
all	O
variables	O
in	O
our	O
linear	B
regression	I
model	O
are	O
quantitative	B
.	O
but	O
in	O
practice	O
,	O
this	O
is	O
not	O
necessarily	O
the	O
case	O
;	O
often	O
some	O
predictors	O
are	O
qualitative	B
.	O
8in	O
other	O
words	O
,	O
if	O
we	O
collect	O
a	O
large	O
number	O
of	O
data	B
sets	O
like	O
the	O
advertising	O
data	B
set	O
,	O
and	O
we	O
construct	O
a	O
conﬁdence	B
interval	I
for	O
the	O
average	B
sales	O
on	O
the	O
basis	B
of	O
each	O
data	B
set	O
(	O
given	O
$	O
100,000	O
in	O
tv	O
and	O
$	O
20,000	O
in	O
radio	O
advertising	O
)	O
,	O
then	O
95	O
%	O
of	O
these	O
conﬁdence	O
intervals	O
will	O
contain	O
the	O
true	O
value	O
of	O
average	B
sales	O
.	O
3.3	O
other	O
considerations	O
in	O
the	O
regression	B
model	O
83	O
for	O
example	O
,	O
the	O
credit	O
data	B
set	O
displayed	O
in	O
figure	O
3.6	O
records	O
balance	O
(	O
average	B
credit	O
card	O
debt	O
for	O
a	O
number	O
of	O
individuals	O
)	O
as	O
well	O
as	O
several	O
quantitative	B
predictors	O
:	O
age	O
,	O
cards	O
(	O
number	O
of	O
credit	O
cards	O
)	O
,	O
education	O
(	O
years	O
of	O
education	O
)	O
,	O
income	O
(	O
in	O
thousands	O
of	O
dollars	O
)	O
,	O
limit	O
(	O
credit	O
limit	O
)	O
,	O
and	O
rating	O
(	O
credit	O
rating	O
)	O
.	O
each	O
panel	O
of	O
figure	O
3.6	O
is	O
a	O
scatterplot	B
for	O
a	O
pair	O
of	O
variables	O
whose	O
identities	O
are	O
given	O
by	O
the	O
corresponding	O
row	O
and	O
column	O
labels	O
.	O
for	O
example	O
,	O
the	O
scatterplot	B
directly	O
to	O
the	O
right	O
of	O
the	O
word	O
“	O
balance	O
”	O
depicts	O
balance	O
versus	O
age	O
,	O
while	O
the	O
plot	B
directly	O
to	O
the	O
right	O
of	O
“	O
age	O
”	O
corresponds	O
to	O
age	O
versus	O
cards	O
.	O
in	O
addition	O
to	O
these	O
quantitative	B
variables	O
,	O
we	O
also	O
have	O
four	O
qualitative	B
variables	O
:	O
gender	O
,	O
student	O
(	O
student	O
status	O
)	O
,	O
status	O
(	O
marital	O
status	O
)	O
,	O
and	O
ethnicity	O
(	O
caucasian	O
,	O
african	O
amer-	O
ican	O
or	O
asian	O
)	O
.	O
20	O
40	O
60	O
80	O
100	O
5	O
10	O
15	O
20	O
2000	O
8000	O
14000	O
balance	O
age	O
cards	O
0	O
0	O
1	O
0	O
8	O
0	O
6	O
0	O
4	O
0	O
2	O
0	O
2	O
5	O
1	O
0	O
1	O
5	O
0	O
0	O
0	O
4	O
1	O
0	O
0	O
0	O
8	O
0	O
0	O
0	O
2	O
0	O
0	O
5	O
1	O
0	O
0	O
5	O
0	O
8	O
6	O
4	O
2	O
0	O
5	O
1	O
0	O
0	O
1	O
0	O
5	O
0	O
0	O
0	O
1	O
0	O
0	O
6	O
0	O
0	O
2	O
education	O
income	O
limit	O
rating	O
0	O
500	O
1500	O
2	O
4	O
6	O
8	O
50	O
100	O
150	O
200	O
600	O
1000	O
figure	O
3.6.	O
the	O
credit	O
data	B
set	O
contains	O
information	O
about	O
balance	O
,	O
age	O
,	O
cards	O
,	O
education	O
,	O
income	O
,	O
limit	O
,	O
and	O
rating	O
for	O
a	O
number	O
of	O
potential	O
cus-	O
tomers	O
.	O
84	O
3.	O
linear	B
regression	I
intercept	O
gender	O
[	O
female	O
]	O
coeﬃcient	B
509.80	O
19.73	O
std	O
.	O
error	B
33.13	O
46.05	O
t-statistic	B
p-value	O
15.389	O
<	O
0.0001	O
0.6690	O
0.429	O
table	O
3.7.	O
least	B
squares	I
coeﬃcient	O
estimates	O
associated	O
with	O
the	O
regression	B
of	O
balance	O
onto	O
gender	O
in	O
the	O
credit	O
data	B
set	O
.	O
the	O
linear	B
model	I
is	O
given	O
in	O
(	O
3.27	O
)	O
.	O
that	O
is	O
,	O
gender	O
is	O
encoded	O
as	O
a	O
dummy	B
variable	I
,	O
as	O
in	O
(	O
3.26	O
)	O
.	O
predictors	O
with	O
only	O
two	O
levels	O
suppose	O
that	O
we	O
wish	O
to	O
investigate	O
diﬀerences	O
in	O
credit	O
card	O
balance	O
be-	O
tween	O
males	O
and	O
females	O
,	O
ignoring	O
the	O
other	O
variables	O
for	O
the	O
moment	O
.	O
if	O
a	O
qualitative	B
predictor	O
(	O
also	O
known	O
as	O
a	O
factor	B
)	O
only	O
has	O
two	O
levels	O
,	O
or	O
possi-	O
ble	O
values	O
,	O
then	O
incorporating	O
it	O
into	O
a	O
regression	B
model	O
is	O
very	O
simple	B
.	O
we	O
simply	O
create	O
an	O
indicator	B
or	O
dummy	B
variable	I
that	O
takes	O
on	O
two	O
possible	O
numerical	O
values	O
.	O
for	O
example	O
,	O
based	O
on	O
the	O
gender	O
variable	B
,	O
we	O
can	O
create	O
a	O
new	O
variable	B
that	O
takes	O
the	O
form	O
(	O
cid:30	O
)	O
factor	B
level	O
dummy	B
variable	I
xi	O
=	O
1	O
0	O
if	O
ith	O
person	O
is	O
female	O
if	O
ith	O
person	O
is	O
male	O
,	O
(	O
3.26	O
)	O
and	O
use	O
this	O
variable	B
as	O
a	O
predictor	B
in	O
the	O
regression	B
equation	O
.	O
this	O
results	O
in	O
the	O
model	B
(	O
cid:30	O
)	O
yi	O
=	O
β0	O
+	O
β1xi	O
+	O
i	O
=	O
β0	O
+	O
β1	O
+	O
i	O
β0	O
+	O
i	O
if	O
ith	O
person	O
is	O
female	O
if	O
ith	O
person	O
is	O
male	O
.	O
(	O
3.27	O
)	O
now	O
β0	O
can	O
be	O
interpreted	O
as	O
the	O
average	B
credit	O
card	O
balance	O
among	O
males	O
,	O
β0	O
+	O
β1	O
as	O
the	O
average	B
credit	O
card	O
balance	O
among	O
females	O
,	O
and	O
β1	O
as	O
the	O
average	B
diﬀerence	O
in	O
credit	O
card	O
balance	O
between	O
females	O
and	O
males	O
.	O
table	O
3.7	O
displays	O
the	O
coeﬃcient	B
estimates	O
and	O
other	O
information	O
asso-	O
ciated	O
with	O
the	O
model	B
(	O
3.27	O
)	O
.	O
the	O
average	B
credit	O
card	O
debt	O
for	O
males	O
is	O
estimated	O
to	O
be	O
$	O
509.80	O
,	O
whereas	O
females	O
are	O
estimated	O
to	O
carry	O
$	O
19.73	O
in	O
additional	O
debt	O
for	O
a	O
total	O
of	O
$	O
509.80	O
+	O
$	O
19.73	O
=	O
$	O
529.53	O
.	O
however	O
,	O
we	O
notice	O
that	O
the	O
p-value	B
for	O
the	O
dummy	B
variable	I
is	O
very	O
high	O
.	O
this	O
indicates	O
that	O
there	O
is	O
no	O
statistical	O
evidence	O
of	O
a	O
diﬀerence	O
in	O
average	B
credit	O
card	O
balance	O
between	O
the	O
genders	O
.	O
the	O
decision	O
to	O
code	O
females	O
as	O
1	O
and	O
males	O
as	O
0	O
in	O
(	O
3.27	O
)	O
is	O
arbitrary	O
,	O
and	O
has	O
no	O
eﬀect	O
on	O
the	O
regression	B
ﬁt	O
,	O
but	O
does	O
alter	O
the	O
interpretation	O
of	O
the	O
coeﬃcients	O
.	O
if	O
we	O
had	O
coded	O
males	O
as	O
1	O
and	O
females	O
as	O
0	O
,	O
then	O
the	O
estimates	O
for	O
β0	O
and	O
β1	O
would	O
have	O
been	O
529.53	O
and	O
−19.73	O
,	O
respectively	O
,	O
leading	O
once	O
again	O
to	O
a	O
prediction	B
of	O
credit	O
card	O
debt	O
of	O
$	O
529.53	O
−	O
$	O
19.73	O
=	O
$	O
509.80	O
for	O
males	O
and	O
a	O
prediction	B
of	O
$	O
529.53	O
for	O
females	O
.	O
alternatively	O
,	O
instead	O
of	O
a	O
0/1	O
coding	O
scheme	O
,	O
we	O
could	O
create	O
a	O
dummy	B
variable	I
3.3	O
other	O
considerations	O
in	O
the	O
regression	B
model	O
(	O
cid:30	O
)	O
85	O
xi	O
=	O
1	O
−1	O
if	O
ith	O
person	O
is	O
female	O
if	O
ith	O
person	O
is	O
male	O
and	O
use	O
this	O
variable	B
in	O
the	O
regression	B
equation	O
.	O
this	O
results	O
in	O
the	O
model	B
yi	O
=	O
β0	O
+	O
β1xi	O
+	O
i	O
=	O
(	O
cid:30	O
)	O
β0	O
+	O
β1	O
+	O
i	O
β0	O
−	O
β1	O
+	O
i	O
if	O
ith	O
person	O
is	O
female	O
if	O
ith	O
person	O
is	O
male	O
.	O
now	O
β0	O
can	O
be	O
interpreted	O
as	O
the	O
overall	O
average	B
credit	O
card	O
balance	O
(	O
ig-	O
noring	O
the	O
gender	O
eﬀect	O
)	O
,	O
and	O
β1	O
is	O
the	O
amount	O
that	O
females	O
are	O
above	O
the	O
average	B
and	O
males	O
are	O
below	O
the	O
average	B
.	O
in	O
this	O
example	O
,	O
the	O
estimate	O
for	O
β0	O
would	O
be	O
$	O
519.665	O
,	O
halfway	O
between	O
the	O
male	O
and	O
female	O
averages	O
of	O
$	O
509.80	O
and	O
$	O
529.53	O
.	O
the	O
estimate	O
for	O
β1	O
would	O
be	O
$	O
9.865	O
,	O
which	O
is	O
half	O
of	O
$	O
19.73	O
,	O
the	O
average	B
diﬀerence	O
between	O
females	O
and	O
males	O
.	O
it	O
is	O
important	O
to	O
note	O
that	O
the	O
ﬁnal	O
predictions	O
for	O
the	O
credit	O
balances	O
of	O
males	O
and	O
females	O
will	O
be	O
identical	O
regardless	O
of	O
the	O
coding	O
scheme	O
used	O
.	O
the	O
only	O
diﬀerence	O
is	O
in	O
the	O
way	O
that	O
the	O
coeﬃcients	O
are	O
interpreted	O
.	O
qualitative	B
predictors	O
with	O
more	O
than	O
two	O
levels	O
when	O
a	O
qualitative	B
predictor	O
has	O
more	O
than	O
two	O
levels	O
,	O
a	O
single	B
dummy	O
variable	B
can	O
not	O
represent	O
all	O
possible	O
values	O
.	O
in	O
this	O
situation	O
,	O
we	O
can	O
create	O
additional	O
dummy	B
variables	O
.	O
for	O
example	O
,	O
for	O
the	O
ethnicity	O
variable	B
we	O
create	O
two	O
dummy	B
variables	O
.	O
the	O
ﬁrst	O
could	O
be	O
(	O
cid:30	O
)	O
xi1	O
=	O
1	O
0	O
if	O
ith	O
person	O
is	O
asian	O
if	O
ith	O
person	O
is	O
not	O
asian	O
,	O
and	O
the	O
second	O
could	O
be	O
(	O
cid:30	O
)	O
xi2	O
=	O
1	O
0	O
if	O
ith	O
person	O
is	O
caucasian	O
if	O
ith	O
person	O
is	O
not	O
caucasian	O
.	O
(	O
3.28	O
)	O
(	O
3.29	O
)	O
then	O
both	O
of	O
these	O
variables	O
can	O
be	O
used	O
in	O
the	O
regression	B
equation	O
,	O
in	O
order	O
to	O
obtain	O
the	O
model	B
yi	O
=	O
β0+β1xi1+β2xi2+i	O
=	O
⎧	O
⎪⎨	O
⎪⎩	O
β0+β1+i	O
β0+β2+i	O
β0+i	O
if	O
ith	O
person	O
is	O
asian	O
if	O
ith	O
person	O
is	O
caucasian	O
if	O
ith	O
person	O
is	O
african	O
american	O
.	O
(	O
3.30	O
)	O
now	O
β0	O
can	O
be	O
interpreted	O
as	O
the	O
average	B
credit	O
card	O
balance	O
for	O
african	O
americans	O
,	O
β1	O
can	O
be	O
interpreted	O
as	O
the	O
diﬀerence	O
in	O
the	O
average	B
balance	O
between	O
the	O
asian	O
and	O
african	O
american	O
categories	O
,	O
and	O
β2	O
can	O
be	O
inter-	O
preted	O
as	O
the	O
diﬀerence	O
in	O
the	O
average	B
balance	O
between	O
the	O
caucasian	O
and	O
86	O
3.	O
linear	B
regression	I
intercept	O
ethnicity	O
[	O
asian	O
]	O
ethnicity	O
[	O
caucasian	O
]	O
coeﬃcient	B
531.00	O
−18.69	O
−12.50	O
std	O
.	O
error	B
46.32	O
65.02	O
56.68	O
t-statistic	B
p-value	O
11.464	O
<	O
0.0001	O
−0.287	O
0.7740	O
−0.221	O
0.8260	O
table	O
3.8.	O
least	B
squares	I
coeﬃcient	O
estimates	O
associated	O
with	O
the	O
regression	B
of	O
balance	O
onto	O
ethnicity	O
in	O
the	O
credit	O
data	B
set	O
.	O
the	O
linear	B
model	I
is	O
given	O
in	O
(	O
3.30	O
)	O
.	O
that	O
is	O
,	O
ethnicity	O
is	O
encoded	O
via	O
two	O
dummy	B
variables	O
(	O
3.28	O
)	O
and	O
(	O
3.29	O
)	O
.	O
african	O
american	O
categories	O
.	O
there	O
will	O
always	O
be	O
one	O
fewer	O
dummy	B
vari-	O
able	O
than	O
the	O
number	O
of	O
levels	O
.	O
the	O
level	B
with	O
no	O
dummy	B
variable—african	O
american	O
in	O
this	O
example—is	O
known	O
as	O
the	O
baseline	B
.	O
from	O
table	O
3.8	O
,	O
we	O
see	O
that	O
the	O
estimated	O
balance	O
for	O
the	O
baseline	B
,	O
african	O
american	O
,	O
is	O
$	O
531.00	O
.	O
it	O
is	O
estimated	O
that	O
the	O
asian	O
category	O
will	O
have	O
$	O
18.69	O
less	O
debt	O
than	O
the	O
african	O
american	O
category	O
,	O
and	O
that	O
the	O
caucasian	O
category	O
will	O
have	O
$	O
12.50	O
less	O
debt	O
than	O
the	O
african	O
american	O
category	O
.	O
however	O
,	O
the	O
p-values	O
associated	O
with	O
the	O
coeﬃcient	B
estimates	O
for	O
the	O
two	O
dummy	B
variables	O
are	O
very	O
large	O
,	O
suggesting	O
no	O
statistical	O
evidence	O
of	O
a	O
real	O
diﬀerence	O
in	O
credit	O
card	O
balance	O
between	O
the	O
ethnicities	O
.	O
once	O
again	O
,	O
the	O
level	B
selected	O
as	O
the	O
baseline	B
category	O
is	O
arbitrary	O
,	O
and	O
the	O
ﬁnal	O
predictions	O
for	O
each	O
group	O
will	O
be	O
the	O
same	O
regardless	O
of	O
this	O
choice	O
.	O
how-	O
ever	O
,	O
the	O
coeﬃcients	O
and	O
their	O
p-values	O
do	O
depend	O
on	O
the	O
choice	O
of	O
dummy	B
variable	I
coding	O
.	O
rather	O
than	O
rely	O
on	O
the	O
individual	O
coeﬃcients	O
,	O
we	O
can	O
use	O
an	O
f-test	O
to	O
test	B
h0	O
:	O
β1	O
=	O
β2	O
=	O
0	O
;	O
this	O
does	O
not	O
depend	O
on	O
the	O
coding	O
.	O
this	O
f-test	O
has	O
a	O
p-value	B
of	O
0.96	O
,	O
indicating	O
that	O
we	O
can	O
not	O
reject	O
the	O
null	B
hypothesis	O
that	O
there	O
is	O
no	O
relationship	O
between	O
balance	O
and	O
ethnicity	O
.	O
using	O
this	O
dummy	B
variable	I
approach	O
presents	O
no	O
diﬃculties	O
when	O
in-	O
corporating	O
both	O
quantitative	B
and	O
qualitative	B
predictors	O
.	O
for	O
example	O
,	O
to	O
regress	O
balance	O
on	O
both	O
a	O
quantitative	B
variable	O
such	O
as	O
income	O
and	O
a	O
qual-	O
itative	O
variable	B
such	O
as	O
student	O
,	O
we	O
must	O
simply	O
create	O
a	O
dummy	B
variable	I
for	O
student	O
and	O
then	O
ﬁt	B
a	O
multiple	B
regression	O
model	B
using	O
income	O
and	O
the	O
dummy	B
variable	I
as	O
predictors	O
for	O
credit	O
card	O
balance	O
.	O
there	O
are	O
many	O
diﬀerent	O
ways	O
of	O
coding	O
qualitative	B
variables	O
besides	O
the	O
dummy	B
variable	I
approach	O
taken	O
here	O
.	O
all	O
of	O
these	O
approaches	O
lead	O
to	O
equivalent	O
model	B
ﬁts	O
,	O
but	O
the	O
coeﬃcients	O
are	O
diﬀerent	O
and	O
have	O
diﬀerent	O
interpretations	O
,	O
and	O
are	O
designed	O
to	O
measure	O
particular	O
contrasts	O
.	O
this	O
topic	O
is	O
beyond	O
the	O
scope	O
of	O
the	O
book	O
,	O
and	O
so	O
we	O
will	O
not	O
pursue	O
it	O
further	O
.	O
3.3.2	O
extensions	O
of	O
the	O
linear	B
model	I
the	O
standard	O
linear	O
regression	B
model	O
(	O
3.19	O
)	O
provides	O
interpretable	O
results	O
and	O
works	O
quite	O
well	O
on	O
many	O
real-world	O
problems	O
.	O
however	O
,	O
it	O
makes	O
sev-	O
eral	O
highly	O
restrictive	O
assumptions	O
that	O
are	O
often	O
violated	O
in	O
practice	O
.	O
two	O
of	O
the	O
most	O
important	O
assumptions	O
state	O
that	O
the	O
relationship	O
between	O
the	O
predictors	O
and	O
response	B
are	O
additive	B
and	O
linear	B
.	O
the	O
additive	B
assumption	O
baseline	B
contrast	O
additive	B
linear	O
3.3	O
other	O
considerations	O
in	O
the	O
regression	B
model	O
87	O
means	O
that	O
the	O
eﬀect	O
of	O
changes	O
in	O
a	O
predictor	B
xj	O
on	O
the	O
response	B
y	O
is	O
independent	B
of	O
the	O
values	O
of	O
the	O
other	O
predictors	O
.	O
the	O
linear	B
assumption	O
states	O
that	O
the	O
change	O
in	O
the	O
response	B
y	O
due	O
to	O
a	O
one-unit	O
change	O
in	O
xj	O
is	O
constant	O
,	O
regardless	O
of	O
the	O
value	O
of	O
xj	O
.	O
in	O
this	O
book	O
,	O
we	O
examine	O
a	O
number	O
of	O
sophisticated	O
methods	O
that	O
relax	O
these	O
two	O
assumptions	O
.	O
here	O
,	O
we	O
brieﬂy	O
examine	O
some	O
common	O
classical	O
approaches	O
for	O
extending	O
the	O
linear	B
model	I
.	O
removing	O
the	O
additive	B
assumption	O
in	O
our	O
previous	O
analysis	O
of	O
the	O
advertising	O
data	B
,	O
we	O
concluded	O
that	O
both	O
tv	O
and	O
radio	O
seem	O
to	O
be	O
associated	O
with	O
sales	O
.	O
the	O
linear	B
models	O
that	O
formed	O
the	O
basis	B
for	O
this	O
conclusion	O
assumed	O
that	O
the	O
eﬀect	O
on	O
sales	O
of	O
increasing	O
one	O
advertising	O
medium	O
is	O
independent	B
of	O
the	O
amount	O
spent	O
on	O
the	O
other	O
media	O
.	O
for	O
example	O
,	O
the	O
linear	B
model	I
(	O
3.20	O
)	O
states	O
that	O
the	O
average	B
eﬀect	O
on	O
sales	O
of	O
a	O
one-unit	O
increase	O
in	O
tv	O
is	O
always	O
β1	O
,	O
regardless	O
of	O
the	O
amount	O
spent	O
on	O
radio	O
.	O
however	O
,	O
this	O
simple	B
model	O
may	O
be	O
incorrect	O
.	O
suppose	O
that	O
spending	O
money	O
on	O
radio	O
advertising	O
actually	O
increases	O
the	O
eﬀectiveness	O
of	O
tv	O
ad-	O
vertising	O
,	O
so	O
that	O
the	O
slope	B
term	O
for	O
tv	O
should	O
increase	O
as	O
radio	O
increases	O
.	O
in	O
this	O
situation	O
,	O
given	O
a	O
ﬁxed	O
budget	O
of	O
$	O
100,000	O
,	O
spending	O
half	O
on	O
radio	O
and	O
half	O
on	O
tv	O
may	O
increase	O
sales	O
more	O
than	O
allocating	O
the	O
entire	O
amount	O
to	O
either	O
tv	O
or	O
to	O
radio	O
.	O
in	O
marketing	O
,	O
this	O
is	O
known	O
as	O
a	O
synergy	B
eﬀect	O
,	O
and	O
in	O
statistics	O
it	O
is	O
referred	O
to	O
as	O
an	O
interaction	B
eﬀect	O
.	O
figure	O
3.5	O
sug-	O
gests	O
that	O
such	O
an	O
eﬀect	O
may	O
be	O
present	O
in	O
the	O
advertising	O
data	B
.	O
notice	O
that	O
when	O
levels	O
of	O
either	O
tv	O
or	O
radio	O
are	O
low	O
,	O
then	O
the	O
true	O
sales	O
are	O
lower	O
than	O
predicted	O
by	O
the	O
linear	B
model	I
.	O
but	O
when	O
advertising	O
is	O
split	O
between	O
the	O
two	O
media	O
,	O
then	O
the	O
model	B
tends	O
to	O
underestimate	O
sales	O
.	O
consider	O
the	O
standard	O
linear	O
regression	B
model	O
with	O
two	O
variables	O
,	O
y	O
=	O
β0	O
+	O
β1x1	O
+	O
β2x2	O
+	O
	O
.	O
according	O
to	O
this	O
model	B
,	O
if	O
we	O
increase	O
x1	O
by	O
one	O
unit	O
,	O
then	O
y	O
will	O
increase	O
by	O
an	O
average	B
of	O
β1	O
units	O
.	O
notice	O
that	O
the	O
presence	O
of	O
x2	B
does	O
not	O
alter	O
this	O
statement—that	O
is	O
,	O
regardless	O
of	O
the	O
value	O
of	O
x2	B
,	O
a	O
one-unit	O
increase	O
in	O
x1	O
will	O
lead	O
to	O
a	O
β1-unit	O
increase	O
in	O
y	O
.	O
one	O
way	O
of	O
extending	O
this	O
model	B
to	O
allow	O
for	O
interaction	O
eﬀects	O
is	O
to	O
include	O
a	O
third	O
predictor	B
,	O
called	O
an	O
interaction	B
term	O
,	O
which	O
is	O
constructed	O
by	O
computing	O
the	O
product	O
of	O
x1	O
and	O
x2	B
.	O
this	O
results	O
in	O
the	O
model	B
y	O
=	O
β0	O
+	O
β1x1	O
+	O
β2x2	O
+	O
β3x1x2	O
+	O
	O
.	O
(	O
3.31	O
)	O
how	O
does	O
inclusion	O
of	O
this	O
interaction	B
term	O
relax	O
the	O
additive	B
assumption	O
?	O
notice	O
that	O
(	O
3.31	O
)	O
can	O
be	O
rewritten	O
as	O
y	O
=	O
β0	O
+	O
(	O
β1	O
+	O
β3x2	O
)	O
x1	O
+	O
β2x2	O
+	O
	O
(	O
3.32	O
)	O
=	O
β0	O
+	O
˜β1x1	O
+	O
β2x2	O
+	O
	O
88	O
3.	O
linear	B
regression	I
intercept	O
tv	O
radio	O
tv×radio	O
coeﬃcient	B
6.7502	O
0.0191	O
0.0289	O
0.0011	O
std	O
.	O
error	B
0.248	O
0.002	O
0.009	O
0.000	O
t-statistic	B
p-value	O
27.23	O
<	O
0.0001	O
12.70	O
<	O
0.0001	O
3.24	O
0.0014	O
20.73	O
<	O
0.0001	O
table	O
3.9.	O
for	O
the	O
advertising	O
data	B
,	O
least	B
squares	I
coeﬃcient	O
estimates	O
asso-	O
ciated	O
with	O
the	O
regression	B
of	O
sales	O
onto	O
tv	O
and	O
radio	O
,	O
with	O
an	O
interaction	B
term	O
,	O
as	O
in	O
(	O
3.33	O
)	O
.	O
where	O
˜β1	O
=	O
β1	O
+	O
β3x2	O
.	O
since	O
˜β1	O
changes	O
with	O
x2	B
,	O
the	O
eﬀect	O
of	O
x1	O
on	O
y	O
is	O
no	O
longer	O
constant	O
:	O
adjusting	O
x2	B
will	O
change	O
the	O
impact	O
of	O
x1	O
on	O
y	O
.	O
for	O
example	O
,	O
suppose	O
that	O
we	O
are	O
interested	O
in	O
studying	O
the	O
productiv-	O
ity	O
of	O
a	O
factory	O
.	O
we	O
wish	O
to	O
predict	O
the	O
number	O
of	O
units	O
produced	O
on	O
the	O
basis	B
of	O
the	O
number	O
of	O
production	O
lines	O
and	O
the	O
total	O
number	O
of	O
workers	O
.	O
it	O
seems	O
likely	O
that	O
the	O
eﬀect	O
of	O
increasing	O
the	O
number	O
of	O
production	O
lines	O
will	O
depend	O
on	O
the	O
number	O
of	O
workers	O
,	O
since	O
if	O
no	O
workers	O
are	O
available	O
to	O
operate	O
the	O
lines	O
,	O
then	O
increasing	O
the	O
number	O
of	O
lines	O
will	O
not	O
increase	O
production	O
.	O
this	O
suggests	O
that	O
it	O
would	O
be	O
appropriate	O
to	O
include	O
an	O
inter-	O
action	O
term	B
between	O
lines	O
and	O
workers	O
in	O
a	O
linear	B
model	I
to	O
predict	O
units	O
.	O
suppose	O
that	O
when	O
we	O
ﬁt	B
the	O
model	B
,	O
we	O
obtain	O
units	O
≈	O
1.2	O
+	O
3.4	O
×	O
lines	O
+	O
0.22	O
×	O
workers	O
+	O
1.4	O
×	O
(	O
lines	O
×	O
workers	O
)	O
=	O
1.2	O
+	O
(	O
3.4	O
+	O
1.4	O
×	O
workers	O
)	O
×	O
lines	O
+	O
0.22	O
×	O
workers	O
.	O
in	O
other	O
words	O
,	O
adding	O
an	O
additional	O
line	B
will	O
increase	O
the	O
number	O
of	O
units	O
produced	O
by	O
3.4	O
+	O
1.4	O
×	O
workers	O
.	O
hence	O
the	O
more	O
workers	O
we	O
have	O
,	O
the	O
stronger	O
will	O
be	O
the	O
eﬀect	O
of	O
lines	O
.	O
we	O
now	O
return	O
to	O
the	O
advertising	O
example	O
.	O
a	O
linear	B
model	I
that	O
uses	O
radio	O
,	O
tv	O
,	O
and	O
an	O
interaction	B
between	O
the	O
two	O
to	O
predict	O
sales	O
takes	O
the	O
form	O
sales	O
=	O
β0	O
+	O
β1	O
×	O
tv	O
+	O
β2	O
×	O
radio	O
+	O
β3	O
×	O
(	O
radio	O
×	O
tv	O
)	O
+	O
	O
=	O
β0	O
+	O
(	O
β1	O
+	O
β3	O
×	O
radio	O
)	O
×	O
tv	O
+	O
β2	O
×	O
radio	O
+	O
	O
.	O
(	O
3.33	O
)	O
we	O
can	O
interpret	O
β3	O
as	O
the	O
increase	O
in	O
the	O
eﬀectiveness	O
of	O
tv	O
advertising	O
for	O
a	O
one	O
unit	O
increase	O
in	O
radio	O
advertising	O
(	O
or	O
vice-versa	O
)	O
.	O
the	O
coeﬃcients	O
that	O
result	O
from	O
ﬁtting	O
the	O
model	B
(	O
3.33	O
)	O
are	O
given	O
in	O
table	O
3.9.	O
the	O
results	O
in	O
table	O
3.9	O
strongly	O
suggest	O
that	O
the	O
model	B
that	O
includes	O
the	O
interaction	B
term	O
is	O
superior	O
to	O
the	O
model	B
that	O
contains	O
only	O
main	B
eﬀects	I
.	O
the	O
p-value	B
for	O
the	O
interaction	B
term	O
,	O
tv×radio	O
,	O
is	O
extremely	O
low	O
,	O
indicating	O
that	O
there	O
is	O
strong	O
evidence	O
for	O
ha	O
:	O
β3	O
(	O
cid:4	O
)	O
=	O
0.	O
in	O
other	O
words	O
,	O
it	O
is	O
clear	O
that	O
the	O
true	O
relationship	O
is	O
not	O
additive	B
.	O
the	O
r2	O
for	O
the	O
model	B
(	O
3.33	O
)	O
is	O
96.8	O
%	O
,	O
compared	O
to	O
only	O
89.7	O
%	O
for	O
the	O
model	B
that	O
predicts	O
sales	O
using	O
tv	O
and	O
radio	O
without	O
an	O
interaction	B
term	O
.	O
this	O
means	O
that	O
(	O
96.8	O
−	O
89.7	O
)	O
/	O
(	O
100	O
−	O
89.7	O
)	O
=	O
69	O
%	O
of	O
the	O
variability	O
in	O
sales	O
that	O
remains	O
after	O
ﬁtting	O
the	O
ad-	O
ditive	O
model	B
has	O
been	O
explained	B
by	O
the	O
interaction	B
term	O
.	O
the	O
coeﬃcient	B
main	O
eﬀect	O
3.3	O
other	O
considerations	O
in	O
the	O
regression	B
model	O
89	O
estimates	O
in	O
table	O
3.9	O
suggest	O
that	O
an	O
increase	O
in	O
tv	O
advertising	O
of	O
$	O
1,000	O
is	O
associated	O
with	O
increased	O
sales	O
of	O
(	O
ˆβ1	O
+	O
ˆβ3×radio	O
)	O
×1,000	O
=	O
19+1.1×radio	O
an	O
increase	O
in	O
sales	O
of	O
(	O
ˆβ2	O
+	O
ˆβ3	O
×	O
tv	O
)	O
×	O
1,000	O
=	O
29	O
+	O
1.1	O
×	O
tv	O
units	O
.	O
units	O
.	O
and	O
an	O
increase	O
in	O
radio	O
advertising	O
of	O
$	O
1,000	O
will	O
be	O
associated	O
with	O
in	O
this	O
example	O
,	O
the	O
p-values	O
associated	O
with	O
tv	O
,	O
radio	O
,	O
and	O
the	O
interac-	O
tion	O
term	B
all	O
are	O
statistically	O
signiﬁcant	O
(	O
table	O
3.9	O
)	O
,	O
and	O
so	O
it	O
is	O
obvious	O
that	O
all	O
three	O
variables	O
should	O
be	O
included	O
in	O
the	O
model	B
.	O
however	O
,	O
it	O
is	O
sometimes	O
the	O
case	O
that	O
an	O
interaction	B
term	O
has	O
a	O
very	O
small	O
p-value	B
,	O
but	O
the	O
associated	O
main	B
eﬀects	I
(	O
in	O
this	O
case	O
,	O
tv	O
and	O
radio	O
)	O
do	O
not	O
.	O
the	O
hier-	O
archical	O
principle	O
states	O
that	O
if	O
we	O
include	O
an	O
interaction	B
in	O
a	O
model	B
,	O
we	O
should	O
also	O
include	O
the	O
main	B
eﬀects	I
,	O
even	O
if	O
the	O
p-values	O
associated	O
with	O
their	O
coeﬃcients	O
are	O
not	O
signiﬁcant	O
.	O
in	O
other	O
words	O
,	O
if	O
the	O
interaction	B
be-	O
tween	O
x1	O
and	O
x2	B
seems	O
important	O
,	O
then	O
we	O
should	O
include	O
both	O
x1	O
and	O
x2	B
in	O
the	O
model	B
even	O
if	O
their	O
coeﬃcient	B
estimates	O
have	O
large	O
p-values	O
.	O
the	O
rationale	O
for	O
this	O
principle	O
is	O
that	O
if	O
x1	O
×	O
x2	B
is	O
related	O
to	O
the	O
response	B
,	O
then	O
whether	O
or	O
not	O
the	O
coeﬃcients	O
of	O
x1	O
or	O
x2	B
are	O
exactly	O
zero	O
is	O
of	O
lit-	O
tle	O
interest	O
.	O
also	O
x1	O
×	O
x2	B
is	O
typically	O
correlated	O
with	O
x1	O
and	O
x2	B
,	O
and	O
so	O
leaving	O
them	O
out	O
tends	O
to	O
alter	O
the	O
meaning	O
of	O
the	O
interaction	B
.	O
hierarchical	B
principle	I
in	O
the	O
previous	O
example	O
,	O
we	O
considered	O
an	O
interaction	B
between	O
tv	O
and	O
radio	O
,	O
both	O
of	O
which	O
are	O
quantitative	B
variables	O
.	O
however	O
,	O
the	O
concept	O
of	O
interactions	O
applies	O
just	O
as	O
well	O
to	O
qualitative	B
variables	O
,	O
or	O
to	O
a	O
combination	O
of	O
quantitative	B
and	O
qualitative	B
variables	O
.	O
in	O
fact	O
,	O
an	O
interaction	B
between	O
a	O
qualitative	B
variable	O
and	O
a	O
quantitative	B
variable	O
has	O
a	O
particularly	O
nice	O
interpretation	O
.	O
consider	O
the	O
credit	O
data	B
set	O
from	O
section	O
3.3.1	O
,	O
and	O
suppose	O
that	O
we	O
wish	O
to	O
predict	O
balance	O
using	O
the	O
income	O
(	O
quantitative	B
)	O
and	O
student	O
(	O
qualitative	B
)	O
variables	O
.	O
in	O
the	O
absence	O
of	O
an	O
interaction	B
term	O
,	O
the	O
model	B
takes	O
the	O
form	O
(	O
cid:30	O
)	O
balancei	O
≈	O
β0	O
+	O
β1	O
×	O
incomei	O
+	O
β2	O
0	O
if	O
ith	O
person	O
is	O
a	O
student	O
if	O
ith	O
person	O
is	O
not	O
a	O
student	O
(	O
cid:30	O
)	O
=	O
β1	O
×	O
incomei	O
+	O
β0	O
+	O
β2	O
β0	O
if	O
ith	O
person	O
is	O
a	O
student	O
if	O
ith	O
person	O
is	O
not	O
a	O
student	O
.	O
(	O
3.34	O
)	O
notice	O
that	O
this	O
amounts	O
to	O
ﬁtting	O
two	O
parallel	O
lines	O
to	O
the	O
data	B
,	O
one	O
for	O
students	O
and	O
one	O
for	O
non-students	O
.	O
the	O
lines	O
for	O
students	O
and	O
non-students	O
have	O
diﬀerent	O
intercepts	O
,	O
β0	O
+	O
β2	O
versus	O
β0	O
,	O
but	O
the	O
same	O
slope	B
,	O
β1	O
.	O
this	O
is	O
illustrated	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
3.7.	O
the	O
fact	O
that	O
the	O
lines	O
are	O
parallel	O
means	O
that	O
the	O
average	B
eﬀect	O
on	O
balance	O
of	O
a	O
one-unit	O
increase	O
in	O
income	O
does	O
not	O
depend	O
on	O
whether	O
or	O
not	O
the	O
individual	O
is	O
a	O
student	O
.	O
this	O
represents	O
a	O
potentially	O
serious	O
limitation	O
of	O
the	O
model	B
,	O
since	O
in	O
fact	O
a	O
change	O
in	O
income	O
may	O
have	O
a	O
very	O
diﬀerent	O
eﬀect	O
on	O
the	O
credit	O
card	O
balance	O
of	O
a	O
student	O
versus	O
a	O
non-student	O
.	O
this	O
limitation	O
can	O
be	O
addressed	O
by	O
adding	O
an	O
interaction	B
variable	O
,	O
cre-	O
ated	O
by	O
multiplying	O
income	O
with	O
the	O
dummy	B
variable	I
for	O
student	O
.	O
our	O
90	O
3.	O
linear	B
regression	I
e	O
c	O
n	O
a	O
a	O
b	O
l	O
0	O
0	O
4	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
6	O
0	O
0	O
2	O
student	O
non−student	O
e	O
c	O
n	O
a	O
a	O
b	O
l	O
0	O
0	O
4	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
6	O
0	O
0	O
2	O
0	O
50	O
100	O
150	O
0	O
50	O
100	O
150	O
income	O
income	O
figure	O
3.7.	O
for	O
the	O
credit	O
data	B
,	O
the	O
least	B
squares	I
lines	O
are	O
shown	O
for	O
pre-	O
diction	O
of	O
balance	O
from	O
income	O
for	O
students	O
and	O
non-students	O
.	O
left	O
:	O
the	O
model	B
(	O
3.34	O
)	O
was	O
ﬁt	B
.	O
there	O
is	O
no	O
interaction	B
between	O
income	O
and	O
student	O
.	O
right	O
:	O
the	O
model	B
(	O
3.35	O
)	O
was	O
ﬁt	B
.	O
there	O
is	O
an	O
interaction	B
term	O
between	O
income	O
and	O
student	O
.	O
model	B
now	O
becomes	O
balancei	O
≈	O
β0	O
+	O
β1	O
×	O
incomei	O
+	O
(	O
cid:30	O
)	O
β2	O
+	O
β3	O
×	O
incomei	O
0	O
if	O
student	O
if	O
not	O
student	O
(	O
cid:30	O
)	O
=	O
(	O
β0	O
+	O
β2	O
)	O
+	O
(	O
β1	O
+	O
β3	O
)	O
×	O
incomei	O
β0	O
+	O
β1	O
×	O
incomei	O
if	O
student	O
if	O
not	O
student	O
(	O
3.35	O
)	O
once	O
again	O
,	O
we	O
have	O
two	O
diﬀerent	O
regression	B
lines	O
for	O
the	O
students	O
and	O
the	O
non-students	O
.	O
but	O
now	O
those	O
regression	B
lines	O
have	O
diﬀerent	O
intercepts	O
,	O
β0+β2	O
versus	O
β0	O
,	O
as	O
well	O
as	O
diﬀerent	O
slopes	O
,	O
β1+β3	O
versus	O
β1	O
.	O
this	O
allows	O
for	O
the	O
possibility	O
that	O
changes	O
in	O
income	O
may	O
aﬀect	O
the	O
credit	O
card	O
balances	O
of	O
students	O
and	O
non-students	O
diﬀerently	O
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
3.7	O
shows	O
the	O
estimated	O
relationships	O
between	O
income	O
and	O
balance	O
for	O
students	O
and	O
non-students	O
in	O
the	O
model	B
(	O
3.35	O
)	O
.	O
we	O
note	O
that	O
the	O
slope	B
for	O
students	O
is	O
lower	O
than	O
the	O
slope	B
for	O
non-students	O
.	O
this	O
suggests	O
that	O
increases	O
in	O
income	O
are	O
associated	O
with	O
smaller	O
increases	O
in	O
credit	O
card	O
balance	O
among	O
students	O
as	O
compared	O
to	O
non-students	O
.	O
non-linear	B
relationships	O
as	O
discussed	O
previously	O
,	O
the	O
linear	B
regression	I
model	O
(	O
3.19	O
)	O
assumes	O
a	O
linear	B
relationship	O
between	O
the	O
response	B
and	O
predictors	O
.	O
but	O
in	O
some	O
cases	O
,	O
the	O
true	O
relationship	O
between	O
the	O
response	B
and	O
the	O
predictors	O
may	O
be	O
non-	O
linear	B
.	O
here	O
we	O
present	O
a	O
very	O
simple	B
way	O
to	O
directly	O
extend	O
the	O
linear	B
model	I
to	O
accommodate	O
non-linear	B
relationships	O
,	O
using	O
polynomial	B
regression	O
.	O
in	O
later	O
chapters	O
,	O
we	O
will	O
present	O
more	O
complex	O
approaches	O
for	O
performing	O
non-linear	B
ﬁts	O
in	O
more	O
general	O
settings	O
.	O
consider	O
figure	O
3.8	O
,	O
in	O
which	O
the	O
mpg	O
(	O
gas	O
mileage	O
in	O
miles	O
per	O
gallon	O
)	O
versus	O
horsepower	O
is	O
shown	O
for	O
a	O
number	O
of	O
cars	O
in	O
the	O
auto	O
data	B
set	O
.	O
the	O
polynomial	B
regression	O
3.3	O
other	O
considerations	O
in	O
the	O
regression	B
model	O
91	O
linear	B
degree	O
2	O
degree	O
5	O
0	O
5	O
0	O
4	O
0	O
3	O
0	O
2	O
0	O
1	O
n	O
o	O
l	O
l	O
a	O
g	O
r	O
e	O
p	O
s	O
e	O
l	O
i	O
m	O
50	O
100	O
150	O
200	O
horsepower	O
figure	O
3.8.	O
the	O
auto	O
data	B
set	O
.	O
for	O
a	O
number	O
of	O
cars	O
,	O
mpg	O
and	O
horsepower	O
are	O
shown	O
.	O
the	O
linear	B
regression	I
ﬁt	O
is	O
shown	O
in	O
orange	O
.	O
the	O
linear	B
regression	I
ﬁt	O
for	O
a	O
2	O
is	O
shown	O
as	O
a	O
blue	O
curve	O
.	O
the	O
linear	B
regression	I
model	O
that	O
includes	O
horsepower	O
ﬁt	B
for	O
a	O
model	B
that	O
includes	O
all	O
polynomials	O
of	O
horsepower	O
up	O
to	O
ﬁfth-degree	O
is	O
shown	O
in	O
green	O
.	O
orange	O
line	B
represents	O
the	O
linear	B
regression	I
ﬁt	O
.	O
there	O
is	O
a	O
pronounced	O
rela-	O
tionship	O
between	O
mpg	O
and	O
horsepower	O
,	O
but	O
it	O
seems	O
clear	O
that	O
this	O
relation-	O
ship	O
is	O
in	O
fact	O
non-linear	B
:	O
the	O
data	B
suggest	O
a	O
curved	O
relationship	O
.	O
a	O
simple	B
approach	O
for	O
incorporating	O
non-linear	B
associations	O
in	O
a	O
linear	B
model	I
is	O
to	O
include	O
transformed	O
versions	O
of	O
the	O
predictors	O
in	O
the	O
model	B
.	O
for	O
example	O
,	O
the	O
points	O
in	O
figure	O
3.8	O
seem	O
to	O
have	O
a	O
quadratic	B
shape	O
,	O
suggesting	O
that	O
a	O
model	B
of	O
the	O
form	O
mpg	O
=	O
β0	O
+	O
β1	O
×	O
horsepower	O
+	O
β2	O
×	O
horsepower	O
2	O
+	O
	O
(	O
3.36	O
)	O
quadratic	B
may	O
provide	O
a	O
better	O
ﬁt	B
.	O
equation	O
3.36	O
involves	O
predicting	O
mpg	O
using	O
a	O
non-linear	B
function	O
of	O
horsepower	O
.	O
but	O
it	O
is	O
still	O
a	O
linear	B
model	I
!	O
that	O
is	O
,	O
(	O
3.36	O
)	O
is	O
simply	O
a	O
multiple	B
linear	O
regression	B
model	O
with	O
x1	O
=	O
horsepower	O
2.	O
so	O
we	O
can	O
use	O
standard	O
linear	O
regression	B
software	O
to	O
and	O
x2	B
=	O
horsepower	O
estimate	O
β0	O
,	O
β1	O
,	O
and	O
β2	O
in	O
order	O
to	O
produce	O
a	O
non-linear	B
ﬁt	O
.	O
the	O
blue	O
curve	O
in	O
figure	O
3.8	O
shows	O
the	O
resulting	O
quadratic	B
ﬁt	O
to	O
the	O
data	B
.	O
the	O
quadratic	B
ﬁt	O
appears	O
to	O
be	O
substantially	O
better	O
than	O
the	O
ﬁt	B
obtained	O
when	O
just	O
the	O
linear	B
term	O
is	O
included	O
.	O
the	O
r2	O
of	O
the	O
quadratic	B
ﬁt	O
is	O
0.688	O
,	O
compared	O
to	O
0.606	O
for	O
the	O
linear	B
ﬁt	O
,	O
and	O
the	O
p-value	B
in	O
table	O
3.10	O
for	O
the	O
quadratic	B
term	O
is	O
highly	O
signiﬁcant	O
.	O
if	O
including	O
horsepower	O
2	O
led	O
to	O
such	O
a	O
big	O
improvement	O
in	O
the	O
model	B
,	O
why	O
5	O
?	O
the	O
green	O
curve	O
4	O
,	O
or	O
even	O
horsepower	O
not	O
include	O
horsepower	O
3	O
,	O
horsepower	O
92	O
3.	O
linear	B
regression	I
intercept	O
horsepower	O
2	O
horsepower	O
coeﬃcient	B
56.9001	O
−0.4662	O
0.0012	O
std	O
.	O
error	B
1.8004	O
0.0311	O
0.0001	O
t-statistic	B
p-value	O
31.6	O
<	O
0.0001	O
−15.0	O
<	O
0.0001	O
10.1	O
<	O
0.0001	O
table	O
3.10.	O
for	O
the	O
auto	O
data	B
set	O
,	O
least	B
squares	I
coeﬃcient	O
estimates	O
associated	O
with	O
the	O
regression	B
of	O
mpg	O
onto	O
horsepower	O
and	O
horsepower	O
2.	O
in	O
figure	O
3.8	O
displays	O
the	O
ﬁt	B
that	O
results	O
from	O
including	O
all	O
polynomials	O
up	O
to	O
ﬁfth	O
degree	O
in	O
the	O
model	B
(	O
3.36	O
)	O
.	O
the	O
resulting	O
ﬁt	B
seems	O
unnecessarily	O
wiggly—that	O
is	O
,	O
it	O
is	O
unclear	O
that	O
including	O
the	O
additional	O
terms	O
really	O
has	O
led	O
to	O
a	O
better	O
ﬁt	B
to	O
the	O
data	B
.	O
the	O
approach	B
that	O
we	O
have	O
just	O
described	O
for	O
extending	O
the	O
linear	B
model	I
to	O
accommodate	O
non-linear	B
relationships	O
is	O
known	O
as	O
polynomial	B
regres-	O
sion	O
,	O
since	O
we	O
have	O
included	O
polynomial	B
functions	O
of	O
the	O
predictors	O
in	O
the	O
regression	B
model	O
.	O
we	O
further	O
explore	O
this	O
approach	B
and	O
other	O
non-linear	B
extensions	O
of	O
the	O
linear	B
model	I
in	O
chapter	O
7	O
.	O
3.3.3	O
potential	O
problems	O
when	O
we	O
ﬁt	B
a	O
linear	B
regression	I
model	O
to	O
a	O
particular	O
data	B
set	O
,	O
many	O
prob-	O
lems	O
may	O
occur	O
.	O
most	O
common	O
among	O
these	O
are	O
the	O
following	O
:	O
1.	O
non-linearity	O
of	O
the	O
response-predictor	O
relationships	O
.	O
2.	O
correlation	B
of	O
error	B
terms	O
.	O
3.	O
non-constant	O
variance	B
of	O
error	B
terms	O
.	O
4.	O
outliers	O
.	O
5.	O
high-leverage	O
points	O
.	O
6.	O
collinearity	B
.	O
in	O
practice	O
,	O
identifying	O
and	O
overcoming	O
these	O
problems	O
is	O
as	O
much	O
an	O
art	O
as	O
a	O
science	O
.	O
many	O
pages	O
in	O
countless	O
books	O
have	O
been	O
written	O
on	O
this	O
topic	O
.	O
since	O
the	O
linear	B
regression	I
model	O
is	O
not	O
our	O
primary	O
focus	O
here	O
,	O
we	O
will	O
provide	O
only	O
a	O
brief	O
summary	O
of	O
some	O
key	O
points	O
.	O
1.	O
non-linearity	O
of	O
the	O
data	B
the	O
linear	B
regression	I
model	O
assumes	O
that	O
there	O
is	O
a	O
straight-line	O
relation-	O
ship	O
between	O
the	O
predictors	O
and	O
the	O
response	B
.	O
if	O
the	O
true	O
relationship	O
is	O
far	O
from	O
linear	B
,	O
then	O
virtually	O
all	O
of	O
the	O
conclusions	O
that	O
we	O
draw	O
from	O
the	O
ﬁt	B
are	O
suspect	O
.	O
in	O
addition	O
,	O
the	O
prediction	B
accuracy	O
of	O
the	O
model	B
can	O
be	O
signiﬁcantly	O
reduced	O
.	O
residual	B
plots	O
are	O
a	O
useful	O
graphical	O
tool	O
for	O
identifying	O
non-linearity	O
.	O
given	O
a	O
simple	B
linear	O
regression	B
model	O
,	O
we	O
can	O
plot	B
the	O
residuals	B
,	O
ei	O
=	O
yi	O
−	O
ˆyi	O
,	O
versus	O
the	O
predictor	B
xi	O
.	O
in	O
the	O
case	O
of	O
a	O
multiple	B
regression	O
model	B
,	O
residual	B
plot	O
3.3	O
other	O
considerations	O
in	O
the	O
regression	B
model	O
93	O
residual	B
plot	O
for	O
linear	O
fit	O
residual	B
plot	O
for	O
quadratic	O
fit	O
l	O
s	O
a	O
u	O
d	O
s	O
e	O
r	O
i	O
323	O
330	O
334	O
0	O
2	O
5	O
1	O
0	O
1	O
5	O
0	O
5	O
−	O
0	O
1	O
−	O
5	O
1	O
−	O
l	O
s	O
a	O
u	O
d	O
s	O
e	O
r	O
i	O
5	O
1	O
0	O
1	O
5	O
0	O
5	O
−	O
0	O
1	O
−	O
5	O
1	O
−	O
334	O
323	O
155	O
ﬁtted	O
5	O
10	O
15	O
20	O
25	O
30	O
15	O
20	O
25	O
30	O
35	O
fitted	O
values	O
fitted	O
values	O
figure	O
3.9.	O
plots	O
of	O
residuals	B
versus	O
predicted	O
(	O
or	O
ﬁtted	O
)	O
values	O
for	O
the	O
auto	O
data	B
set	O
.	O
in	O
each	O
plot	B
,	O
the	O
red	O
line	B
is	O
a	O
smooth	O
ﬁt	B
to	O
the	O
residuals	B
,	O
intended	O
to	O
make	O
it	O
easier	O
to	O
identify	O
a	O
trend	O
.	O
left	O
:	O
a	O
linear	B
regression	I
of	O
mpg	O
on	O
horsepower	O
.	O
a	O
strong	O
pattern	O
in	O
the	O
residuals	B
indicates	O
non-linearity	O
in	O
the	O
data	B
.	O
right	O
:	O
a	O
linear	B
2.	O
there	O
is	O
little	O
pattern	O
in	O
the	O
regression	B
of	O
mpg	O
on	O
horsepower	O
and	O
horsepower	O
residuals	B
.	O
since	O
there	O
are	O
multiple	B
predictors	O
,	O
we	O
instead	O
plot	B
the	O
residuals	B
versus	O
the	O
predicted	O
(	O
or	O
ﬁtted	O
)	O
values	O
ˆyi	O
.	O
ideally	O
,	O
the	O
residual	B
plot	O
will	O
show	O
no	O
discernible	O
pattern	O
.	O
the	O
presence	O
of	O
a	O
pattern	O
may	O
indicate	O
a	O
problem	O
with	O
some	O
aspect	O
of	O
the	O
linear	B
model	I
.	O
the	O
left	O
panel	O
of	O
figure	O
3.9	O
displays	O
a	O
residual	B
plot	O
from	O
the	O
linear	B
regression	I
of	O
mpg	O
onto	O
horsepower	O
on	O
the	O
auto	O
data	B
set	O
that	O
was	O
illustrated	O
in	O
figure	O
3.8.	O
the	O
red	O
line	B
is	O
a	O
smooth	O
ﬁt	B
to	O
the	O
residuals	B
,	O
which	O
is	O
displayed	O
in	O
order	O
to	O
make	O
it	O
easier	O
to	O
identify	O
any	O
trends	O
.	O
the	O
residuals	B
exhibit	O
a	O
clear	O
u-shape	O
,	O
which	O
provides	O
a	O
strong	O
indication	O
of	O
non-linearity	O
in	O
the	O
data	B
.	O
in	O
contrast	B
,	O
the	O
right-hand	O
panel	O
of	O
figure	O
3.9	O
displays	O
the	O
residual	B
plot	O
that	O
results	O
from	O
the	O
model	B
(	O
3.36	O
)	O
,	O
which	O
contains	O
a	O
quadratic	B
term	O
.	O
there	O
appears	O
to	O
be	O
little	O
pattern	O
in	O
the	O
residuals	B
,	O
suggesting	O
that	O
the	O
quadratic	B
term	O
improves	O
the	O
ﬁt	B
to	O
the	O
data	B
.	O
if	O
the	O
residual	B
plot	O
indicates	O
that	O
there	O
are	O
non-linear	B
associations	O
in	O
the	O
data	B
,	O
then	O
a	O
simple	B
approach	O
is	O
to	O
use	O
non-linear	B
transformations	O
of	O
the	O
x	O
,	O
and	O
x	O
2	O
,	O
in	O
the	O
regression	B
model	O
.	O
in	O
the	O
predictors	O
,	O
such	O
as	O
log	O
x	O
,	O
later	O
chapters	O
of	O
this	O
book	O
,	O
we	O
will	O
discuss	O
other	O
more	O
advanced	O
non-linear	B
approaches	O
for	O
addressing	O
this	O
issue	O
.	O
√	O
2.	O
correlation	B
of	O
error	B
terms	O
an	O
important	O
assumption	O
of	O
the	O
linear	B
regression	I
model	O
is	O
that	O
the	O
error	B
terms	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
are	O
uncorrelated	O
.	O
what	O
does	O
this	O
mean	O
?	O
for	O
instance	O
,	O
if	O
the	O
errors	O
are	O
uncorrelated	O
,	O
then	O
the	O
fact	O
that	O
i	O
is	O
positive	O
provides	O
little	O
or	O
no	O
information	O
about	O
the	O
sign	O
of	O
i+1	O
.	O
the	O
standard	O
errors	O
that	O
are	O
computed	O
for	O
the	O
estimated	O
regression	B
coeﬃcients	O
or	O
the	O
ﬁtted	O
values	O
94	O
3.	O
linear	B
regression	I
are	O
based	O
on	O
the	O
assumption	O
of	O
uncorrelated	O
error	B
terms	O
.	O
if	O
in	O
fact	O
there	O
is	O
correlation	B
among	O
the	O
error	B
terms	O
,	O
then	O
the	O
estimated	O
standard	O
errors	O
will	O
tend	O
to	O
underestimate	O
the	O
true	O
standard	O
errors	O
.	O
as	O
a	O
result	O
,	O
conﬁ-	O
dence	O
and	O
prediction	B
intervals	O
will	O
be	O
narrower	O
than	O
they	O
should	O
be	O
.	O
for	O
example	O
,	O
a	O
95	O
%	O
conﬁdence	B
interval	I
may	O
in	O
reality	O
have	O
a	O
much	O
lower	O
prob-	O
ability	O
than	O
0.95	O
of	O
containing	O
the	O
true	O
value	O
of	O
the	O
parameter	B
.	O
in	O
addition	O
,	O
p-values	O
associated	O
with	O
the	O
model	B
will	O
be	O
lower	O
than	O
they	O
should	O
be	O
;	O
this	O
could	O
cause	O
us	O
to	O
erroneously	O
conclude	O
that	O
a	O
parameter	B
is	O
statistically	O
signiﬁcant	O
.	O
in	O
short	O
,	O
if	O
the	O
error	B
terms	O
are	O
correlated	O
,	O
we	O
may	O
have	O
an	O
unwarranted	O
sense	O
of	O
conﬁdence	O
in	O
our	O
model	B
.	O
as	O
an	O
extreme	O
example	O
,	O
suppose	O
we	O
accidentally	O
doubled	O
our	O
data	B
,	O
lead-	O
ing	O
to	O
observations	B
and	O
error	B
terms	O
identical	O
in	O
pairs	O
.	O
if	O
we	O
ignored	O
this	O
,	O
our	O
standard	B
error	I
calculations	O
would	O
be	O
as	O
if	O
we	O
had	O
a	O
sample	O
of	O
size	O
2n	O
,	O
when	O
in	O
fact	O
we	O
have	O
only	O
n	O
samples	O
.	O
our	O
estimated	O
parameters	O
would	O
be	O
the	O
same	O
for	O
the	O
2n	O
samples	O
as	O
for	O
the	O
n	O
samples	O
,	O
but	O
the	O
conﬁdence	O
intervals	O
would	O
be	O
narrower	O
by	O
a	O
factor	B
of	O
√	O
2	O
!	O
why	O
might	O
correlations	O
among	O
the	O
error	B
terms	O
occur	O
?	O
such	O
correlations	O
frequently	O
occur	O
in	O
the	O
context	O
of	O
time	B
series	I
data	O
,	O
which	O
consists	O
of	O
ob-	O
servations	O
for	O
which	O
measurements	O
are	O
obtained	O
at	O
discrete	O
points	O
in	O
time	O
.	O
in	O
many	O
cases	O
,	O
observations	B
that	O
are	O
obtained	O
at	O
adjacent	O
time	O
points	O
will	O
have	O
positively	O
correlated	O
errors	O
.	O
in	O
order	O
to	O
determine	O
if	O
this	O
is	O
the	O
case	O
for	O
a	O
given	O
data	B
set	O
,	O
we	O
can	O
plot	B
the	O
residuals	B
from	O
our	O
model	B
as	O
a	O
function	B
of	O
time	O
.	O
if	O
the	O
errors	O
are	O
uncorrelated	O
,	O
then	O
there	O
should	O
be	O
no	O
discernible	O
pat-	O
tern	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
the	O
error	B
terms	O
are	O
positively	O
correlated	O
,	O
then	O
we	O
may	O
see	O
tracking	B
in	O
the	O
residuals—that	O
is	O
,	O
adjacent	O
residuals	B
may	O
have	O
similar	O
values	O
.	O
figure	O
3.10	O
provides	O
an	O
illustration	O
.	O
in	O
the	O
top	O
panel	O
,	O
we	O
see	O
the	O
residuals	B
from	O
a	O
linear	B
regression	I
ﬁt	O
to	O
data	B
generated	O
with	O
uncorre-	O
lated	O
errors	O
.	O
there	O
is	O
no	O
evidence	O
of	O
a	O
time-related	O
trend	O
in	O
the	O
residuals	B
.	O
in	O
contrast	B
,	O
the	O
residuals	B
in	O
the	O
bottom	O
panel	O
are	O
from	O
a	O
data	B
set	O
in	O
which	O
adjacent	O
errors	O
had	O
a	O
correlation	B
of	O
0.9.	O
now	O
there	O
is	O
a	O
clear	O
pattern	O
in	O
the	O
residuals—adjacent	O
residuals	B
tend	O
to	O
take	O
on	O
similar	O
values	O
.	O
finally	O
,	O
the	O
center	O
panel	O
illustrates	O
a	O
more	O
moderate	O
case	O
in	O
which	O
the	O
residuals	B
had	O
a	O
correlation	B
of	O
0.5.	O
there	O
is	O
still	O
evidence	O
of	O
tracking	B
,	O
but	O
the	O
pattern	O
is	O
less	O
clear	O
.	O
many	O
methods	O
have	O
been	O
developed	O
to	O
properly	O
take	O
account	O
of	O
corre-	O
lations	O
in	O
the	O
error	B
terms	O
in	O
time	B
series	I
data	O
.	O
correlation	B
among	O
the	O
error	B
terms	O
can	O
also	O
occur	O
outside	O
of	O
time	B
series	I
data	O
.	O
for	O
instance	O
,	O
consider	O
a	O
study	O
in	O
which	O
individuals	O
’	O
heights	O
are	O
predicted	O
from	O
their	O
weights	O
.	O
the	O
assumption	O
of	O
uncorrelated	O
errors	O
could	O
be	O
violated	O
if	O
some	O
of	O
the	O
individ-	O
uals	O
in	O
the	O
study	O
are	O
members	O
of	O
the	O
same	O
family	O
,	O
or	O
eat	O
the	O
same	O
diet	O
,	O
or	O
have	O
been	O
exposed	O
to	O
the	O
same	O
environmental	O
factors	O
.	O
in	O
general	O
,	O
the	O
assumption	O
of	O
uncorrelated	O
errors	O
is	O
extremely	O
important	O
for	O
linear	O
regres-	O
sion	O
as	O
well	O
as	O
for	O
other	O
statistical	O
methods	O
,	O
and	O
good	O
experimental	O
design	O
is	O
crucial	O
in	O
order	O
to	O
mitigate	O
the	O
risk	O
of	O
such	O
correlations	O
.	O
time	B
series	I
tracking	O
l	O
i	O
a	O
u	O
d	O
s	O
e	O
r	O
l	O
i	O
a	O
u	O
d	O
s	O
e	O
r	O
l	O
i	O
a	O
u	O
d	O
s	O
e	O
r	O
3	O
2	O
1	O
0	O
1	O
−	O
3	O
−	O
2	O
1	O
0	O
2	O
−	O
4	O
−	O
5	O
1	O
.	O
.	O
5	O
0	O
.	O
5	O
0	O
−	O
.	O
5	O
1	O
−	O
0	O
0	O
0	O
3.3	O
other	O
considerations	O
in	O
the	O
regression	B
model	O
95	O
ρ=0.0	O
20	O
40	O
60	O
80	O
100	O
ρ=0.5	O
20	O
40	O
60	O
80	O
100	O
ρ=0.9	O
20	O
40	O
60	O
80	O
100	O
observation	O
figure	O
3.10.	O
plots	O
of	O
residuals	B
from	O
simulated	O
time	B
series	I
data	O
sets	O
generated	O
with	O
diﬀering	O
levels	O
of	O
correlation	B
ρ	O
between	O
error	B
terms	O
for	O
adjacent	O
time	O
points	O
.	O
3.	O
non-constant	O
variance	B
of	O
error	B
terms	O
another	O
important	O
assumption	O
of	O
the	O
linear	B
regression	I
model	O
is	O
that	O
the	O
error	B
terms	O
have	O
a	O
constant	O
variance	B
,	O
var	O
(	O
i	O
)	O
=	O
σ2	O
.	O
the	O
standard	O
errors	O
,	O
conﬁdence	O
intervals	O
,	O
and	O
hypothesis	B
tests	O
associated	O
with	O
the	O
linear	B
model	I
rely	O
upon	O
this	O
assumption	O
.	O
unfortunately	O
,	O
it	O
is	O
often	O
the	O
case	O
that	O
the	O
variances	O
of	O
the	O
error	B
terms	O
are	O
non-constant	O
.	O
for	O
instance	O
,	O
the	O
variances	O
of	O
the	O
error	B
terms	O
may	O
increase	O
with	O
the	O
value	O
of	O
the	O
response	B
.	O
one	O
can	O
identify	O
non-constant	O
variances	O
in	O
the	O
errors	O
,	O
or	O
heteroscedasticity	B
,	O
from	O
the	O
presence	O
of	O
a	O
funnel	O
shape	O
in	O
the	O
residual	B
plot	O
.	O
an	O
example	O
is	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
3.11	O
,	O
in	O
which	O
the	O
magnitude	O
of	O
the	O
residuals	B
tends	O
to	O
increase	O
with	O
the	O
ﬁtted	O
√	O
values	O
.	O
when	O
faced	O
with	O
this	O
problem	O
,	O
one	O
possible	O
solution	O
is	O
to	O
trans-	O
y	O
.	O
such	O
form	O
the	O
response	B
y	O
using	O
a	O
concave	O
function	B
such	O
as	O
log	O
y	O
or	O
a	O
transformation	O
results	O
in	O
a	O
greater	O
amount	O
of	O
shrinkage	B
of	O
the	O
larger	O
re-	O
sponses	O
,	O
leading	O
to	O
a	O
reduction	O
in	O
heteroscedasticity	B
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
3.11	O
displays	O
the	O
residual	B
plot	O
after	O
transforming	O
the	O
response	B
heterosceda-	O
sticity	O
96	O
3.	O
linear	B
regression	I
response	O
y	O
response	B
log	O
(	O
y	O
)	O
l	O
s	O
a	O
u	O
d	O
s	O
e	O
r	O
i	O
998	O
975	O
845	O
5	O
1	O
0	O
1	O
5	O
0	O
5	O
−	O
0	O
1	O
−	O
l	O
s	O
a	O
u	O
d	O
s	O
e	O
r	O
i	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
2	O
.	O
0	O
−	O
4	O
.	O
0	O
−	O
6	O
.	O
0	O
−	O
8	O
.	O
0	O
−	O
605	O
671	O
437	O
10	O
15	O
20	O
25	O
30	O
2.4	O
2.6	O
2.8	O
3.0	O
3.2	O
3.4	O
fitted	O
values	O
fitted	O
values	O
figure	O
3.11.	O
residual	B
plots	O
.	O
in	O
each	O
plot	B
,	O
the	O
red	O
line	B
is	O
a	O
smooth	O
ﬁt	B
to	O
the	O
residuals	B
,	O
intended	O
to	O
make	O
it	O
easier	O
to	O
identify	O
a	O
trend	O
.	O
the	O
blue	O
lines	O
track	O
the	O
outer	O
quantiles	O
of	O
the	O
residuals	B
,	O
and	O
emphasize	O
patterns	O
.	O
left	O
:	O
the	O
funnel	O
shape	O
indicates	O
heteroscedasticity	B
.	O
right	O
:	O
the	O
response	B
has	O
been	O
log	O
transformed	O
,	O
and	O
there	O
is	O
now	O
no	O
evidence	O
of	O
heteroscedasticity	B
.	O
using	O
log	O
y	O
.	O
the	O
residuals	B
now	O
appear	O
to	O
have	O
constant	O
variance	B
,	O
though	O
there	O
is	O
some	O
evidence	O
of	O
a	O
slight	O
non-linear	B
relationship	O
in	O
the	O
data	B
.	O
sometimes	O
we	O
have	O
a	O
good	O
idea	O
of	O
the	O
variance	B
of	O
each	O
response	B
.	O
for	O
example	O
,	O
the	O
ith	O
response	B
could	O
be	O
an	O
average	B
of	O
ni	O
raw	O
observations	B
.	O
if	O
each	O
of	O
these	O
raw	O
observations	B
is	O
uncorrelated	O
with	O
variance	B
σ2	O
,	O
then	O
their	O
average	B
has	O
variance	B
σ2	O
i	O
=	O
σ2/ni	O
.	O
in	O
this	O
case	O
a	O
simple	B
remedy	O
is	O
to	O
ﬁt	B
our	O
model	B
by	O
weighted	B
least	I
squares	I
,	O
with	O
weights	O
proportional	O
to	O
the	O
inverse	O
variances—i.e	O
.	O
wi	O
=	O
ni	O
in	O
this	O
case	O
.	O
most	O
linear	B
regression	I
software	O
allows	O
for	O
observation	O
weights	O
.	O
4.	O
outliers	O
an	O
outlier	B
is	O
a	O
point	O
for	O
which	O
yi	O
is	O
far	O
from	O
the	O
value	O
predicted	O
by	O
the	O
model	B
.	O
outliers	O
can	O
arise	O
for	O
a	O
variety	O
of	O
reasons	O
,	O
such	O
as	O
incorrect	O
recording	O
of	O
an	O
observation	O
during	O
data	B
collection	O
.	O
the	O
red	O
point	O
(	O
observation	O
20	O
)	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
3.12	O
illustrates	O
a	O
typical	O
outlier	B
.	O
the	O
red	O
solid	O
line	B
is	O
the	O
least	B
squares	I
regression	O
ﬁt	B
,	O
while	O
the	O
blue	O
dashed	O
line	B
is	O
the	O
least	B
squares	I
ﬁt	O
after	O
removal	O
of	O
the	O
outlier	B
.	O
in	O
this	O
case	O
,	O
removing	O
the	O
outlier	B
has	O
little	O
eﬀect	O
on	O
the	O
least	B
squares	I
line	O
:	O
it	O
leads	O
to	O
almost	O
no	O
change	O
in	O
the	O
slope	B
,	O
and	O
a	O
miniscule	O
reduction	O
in	O
the	O
intercept	B
.	O
it	O
is	O
typical	O
for	O
an	O
outlier	B
that	O
does	O
not	O
have	O
an	O
unusual	O
predictor	B
value	O
to	O
have	O
little	O
eﬀect	O
on	O
the	O
least	B
squares	I
ﬁt	O
.	O
however	O
,	O
even	O
if	O
an	O
outlier	B
does	O
not	O
have	O
much	O
eﬀect	O
on	O
the	O
least	B
squares	I
ﬁt	O
,	O
it	O
can	O
cause	O
other	O
problems	O
.	O
for	O
instance	O
,	O
in	O
this	O
example	O
,	O
the	O
rse	O
is	O
1.09	O
when	O
the	O
outlier	B
is	O
included	O
in	O
the	O
regression	B
,	O
but	O
it	O
is	O
only	O
0.77	O
when	O
the	O
outlier	B
is	O
removed	O
.	O
since	O
the	O
rse	O
is	O
used	O
to	O
compute	O
all	O
conﬁdence	O
intervals	O
and	O
weighted	B
least	I
squares	I
outlier	O
3.3	O
other	O
considerations	O
in	O
the	O
regression	B
model	O
97	O
20	O
20	O
6	O
4	O
y	O
2	O
0	O
2	O
−	O
4	O
−	O
l	O
s	O
a	O
u	O
d	O
s	O
e	O
r	O
i	O
4	O
3	O
2	O
1	O
0	O
1	O
−	O
20	O
6	O
4	O
2	O
0	O
l	O
i	O
s	O
a	O
u	O
d	O
s	O
e	O
r	O
d	O
e	O
z	O
i	O
t	O
n	O
e	O
d	O
u	O
s	O
t	O
−2	O
−1	O
0	O
1	O
2	O
−2	O
0	O
2	O
4	O
6	O
−2	O
0	O
2	O
4	O
6	O
x	O
fitted	O
values	O
fitted	O
values	O
figure	O
3.12.	O
left	O
:	O
the	O
least	B
squares	I
regression	O
line	B
is	O
shown	O
in	O
red	O
,	O
and	O
the	O
regression	B
line	O
after	O
removing	O
the	O
outlier	B
is	O
shown	O
in	O
blue	O
.	O
center	O
:	O
the	O
residual	B
plot	O
clearly	O
identiﬁes	O
the	O
outlier	B
.	O
right	O
:	O
the	O
outlier	B
has	O
a	O
studentized	B
residual	O
of	O
6	O
;	O
typically	O
we	O
expect	O
values	O
between	O
−3	O
and	O
3.	O
p-values	O
,	O
such	O
a	O
dramatic	O
increase	O
caused	O
by	O
a	O
single	B
data	O
point	O
can	O
have	O
implications	O
for	O
the	O
interpretation	O
of	O
the	O
ﬁt	B
.	O
similarly	O
,	O
inclusion	O
of	O
the	O
outlier	B
causes	O
the	O
r2	O
to	O
decline	O
from	O
0.892	O
to	O
0.805.	O
residual	B
plots	O
can	O
be	O
used	O
to	O
identify	O
outliers	O
.	O
in	O
this	O
example	O
,	O
the	O
out-	O
lier	O
is	O
clearly	O
visible	O
in	O
the	O
residual	B
plot	O
illustrated	O
in	O
the	O
center	O
panel	O
of	O
figure	O
3.12.	O
but	O
in	O
practice	O
,	O
it	O
can	O
be	O
diﬃcult	O
to	O
decide	O
how	O
large	O
a	O
resid-	O
ual	O
needs	O
to	O
be	O
before	O
we	O
consider	O
the	O
point	O
to	O
be	O
an	O
outlier	B
.	O
to	O
address	O
this	O
problem	O
,	O
instead	O
of	O
plotting	O
the	O
residuals	B
,	O
we	O
can	O
plot	B
the	O
studentized	B
residuals	O
,	O
computed	O
by	O
dividing	O
each	O
residual	B
ei	O
by	O
its	O
estimated	O
standard	B
error	I
.	O
observations	B
whose	O
studentized	B
residuals	O
are	O
greater	O
than	O
3	O
in	O
abso-	O
lute	O
value	O
are	O
possible	O
outliers	O
.	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
3.12	O
,	O
the	O
outlier	B
’	O
s	O
studentized	B
residual	O
exceeds	O
6	O
,	O
while	O
all	O
other	O
observations	B
have	O
studentized	B
residuals	O
between	O
−2	O
and	O
2.	O
if	O
we	O
believe	O
that	O
an	O
outlier	B
has	O
occurred	O
due	O
to	O
an	O
error	B
in	O
data	B
collec-	O
tion	O
or	O
recording	O
,	O
then	O
one	O
solution	O
is	O
to	O
simply	O
remove	O
the	O
observation	O
.	O
however	O
,	O
care	O
should	O
be	O
taken	O
,	O
since	O
an	O
outlier	B
may	O
instead	O
indicate	O
a	O
deﬁciency	O
with	O
the	O
model	B
,	O
such	O
as	O
a	O
missing	O
predictor	O
.	O
5.	O
high	O
leverage	B
points	O
we	O
just	O
saw	O
that	O
outliers	O
are	O
observations	B
for	O
which	O
the	O
response	B
yi	O
is	O
unusual	O
given	O
the	O
predictor	B
xi	O
.	O
in	O
contrast	B
,	O
observations	B
with	O
high	O
leverage	B
have	O
an	O
unusual	O
value	O
for	O
xi	O
.	O
for	O
example	O
,	O
observation	O
41	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
3.13	O
has	O
high	O
leverage	B
,	O
in	O
that	O
the	O
predictor	B
value	O
for	O
this	O
observation	O
is	O
large	O
relative	O
to	O
the	O
other	O
observations	B
.	O
(	O
note	O
that	O
the	O
data	B
displayed	O
in	O
figure	O
3.13	O
are	O
the	O
same	O
as	O
the	O
data	B
displayed	O
in	O
figure	O
3.12	O
,	O
but	O
with	O
the	O
addition	O
of	O
a	O
single	B
high	O
leverage	B
observation	O
.	O
)	O
the	O
red	O
solid	O
line	B
is	O
the	O
least	B
squares	I
ﬁt	O
to	O
the	O
data	B
,	O
while	O
the	O
blue	O
dashed	O
line	B
is	O
the	O
ﬁt	B
produced	O
when	O
observation	O
41	O
is	O
removed	O
.	O
comparing	O
the	O
left-hand	O
panels	O
of	O
figures	O
3.12	O
and	O
3.13	O
,	O
we	O
observe	O
that	O
removing	O
the	O
high	O
leverage	B
observation	O
has	O
a	O
much	O
more	O
substantial	O
impact	O
on	O
the	O
least	B
squares	I
line	O
studentized	B
residual	O
high	O
leverage	B
98	O
3.	O
linear	B
regression	I
0	O
1	O
y	O
5	O
0	O
41	O
20	O
2	O
x	O
2	O
1	O
0	O
1	O
−	O
2	O
−	O
20	O
41	O
5	O
4	O
3	O
2	O
1	O
0	O
1	O
−	O
l	O
i	O
s	O
a	O
u	O
d	O
s	O
e	O
r	O
d	O
e	O
z	O
i	O
t	O
n	O
e	O
d	O
u	O
s	O
t	O
−2	O
−1	O
0	O
1	O
x	O
2	O
3	O
4	O
−2	O
−1	O
0	O
x1	O
1	O
2	O
0.00	O
0.05	O
0.10	O
0.15	O
0.20	O
0.25	O
leverage	B
figure	O
3.13.	O
left	O
:	O
observation	O
41	O
is	O
a	O
high	O
leverage	B
point	O
,	O
while	O
20	O
is	O
not	O
.	O
the	O
red	O
line	B
is	O
the	O
ﬁt	B
to	O
all	O
the	O
data	B
,	O
and	O
the	O
blue	O
line	B
is	O
the	O
ﬁt	B
with	O
observation	O
41	O
removed	O
.	O
center	O
:	O
the	O
red	O
observation	O
is	O
not	O
unusual	O
in	O
terms	O
of	O
its	O
x1	O
value	O
or	O
its	O
x2	B
value	O
,	O
but	O
still	O
falls	O
outside	O
the	O
bulk	O
of	O
the	O
data	B
,	O
and	O
hence	O
has	O
high	O
leverage	B
.	O
right	O
:	O
observation	O
41	O
has	O
a	O
high	O
leverage	B
and	O
a	O
high	O
residual	B
.	O
than	O
removing	O
the	O
outlier	B
.	O
in	O
fact	O
,	O
high	O
leverage	B
observations	O
tend	O
to	O
have	O
a	O
sizable	O
impact	O
on	O
the	O
estimated	O
regression	B
line	O
.	O
it	O
is	O
cause	O
for	O
concern	O
if	O
the	O
least	B
squares	I
line	O
is	O
heavily	O
aﬀected	O
by	O
just	O
a	O
couple	O
of	O
observations	B
,	O
because	O
any	O
problems	O
with	O
these	O
points	O
may	O
invalidate	O
the	O
entire	O
ﬁt	B
.	O
for	O
this	O
reason	O
,	O
it	O
is	O
important	O
to	O
identify	O
high	O
leverage	B
observations	O
.	O
in	O
a	O
simple	B
linear	O
regression	B
,	O
high	O
leverage	B
observations	O
are	O
fairly	O
easy	O
to	O
identify	O
,	O
since	O
we	O
can	O
simply	O
look	O
for	O
observations	O
for	O
which	O
the	O
predictor	B
value	O
is	O
outside	O
of	O
the	O
normal	O
range	O
of	O
the	O
observations	B
.	O
but	O
in	O
a	O
multiple	B
linear	O
regression	B
with	O
many	O
predictors	O
,	O
it	O
is	O
possible	O
to	O
have	O
an	O
observation	O
that	O
is	O
well	O
within	O
the	O
range	O
of	O
each	O
individual	O
predictor	B
’	O
s	O
values	O
,	O
but	O
that	O
is	O
unusual	O
in	O
terms	O
of	O
the	O
full	O
set	B
of	O
predictors	O
.	O
an	O
example	O
is	O
shown	O
in	O
the	O
center	O
panel	O
of	O
figure	O
3.13	O
,	O
for	O
a	O
data	B
set	O
with	O
two	O
predictors	O
,	O
x1	O
and	O
x2	B
.	O
most	O
of	O
the	O
observations	B
’	O
predictor	B
values	O
fall	O
within	O
the	O
blue	O
dashed	O
ellipse	O
,	O
but	O
the	O
red	O
observation	O
is	O
well	O
outside	O
of	O
this	O
range	O
.	O
but	O
neither	O
its	O
value	O
for	O
x1	O
nor	O
its	O
value	O
for	O
x2	O
is	O
unusual	O
.	O
so	O
if	O
we	O
examine	O
just	O
x1	O
or	O
just	O
x2	B
,	O
we	O
will	O
fail	O
to	O
notice	O
this	O
high	O
leverage	B
point	O
.	O
this	O
problem	O
is	O
more	O
pronounced	O
in	O
multiple	B
regression	O
settings	O
with	O
more	O
than	O
two	O
predictors	O
,	O
because	O
then	O
there	O
is	O
no	O
simple	B
way	O
to	O
plot	B
all	O
dimensions	O
of	O
the	O
data	B
simultaneously	O
.	O
in	O
order	O
to	O
quantify	O
an	O
observation	O
’	O
s	O
leverage	B
,	O
we	O
compute	O
the	O
leverage	B
statistic	O
.	O
a	O
large	O
value	O
of	O
this	O
statistic	O
indicates	O
an	O
observation	O
with	O
high	O
leverage	B
.	O
for	O
a	O
simple	B
linear	O
regression	B
,	O
hi	O
=	O
(	O
cid:10	O
)	O
+	O
1	O
n	O
(	O
xi	O
−	O
¯x	O
)	O
2	O
i	O
(	O
cid:2	O
)	O
=1	O
(	O
xi	O
(	O
cid:2	O
)	O
−	O
¯x	O
)	O
2	O
.	O
n	O
(	O
3.37	O
)	O
it	O
is	O
clear	O
from	O
this	O
equation	O
that	O
hi	O
increases	O
with	O
the	O
distance	B
of	O
xi	O
from	O
¯x	O
.	O
there	O
is	O
a	O
simple	B
extension	O
of	O
hi	O
to	O
the	O
case	O
of	O
multiple	B
predictors	O
,	O
though	O
we	O
do	O
not	O
provide	O
the	O
formula	O
here	O
.	O
the	O
leverage	B
statistic	O
hi	O
is	O
always	O
between	O
1/n	O
and	O
1	O
,	O
and	O
the	O
average	B
leverage	O
for	O
all	O
the	O
observations	B
is	O
always	O
equal	O
to	O
(	O
p	O
+	O
1	O
)	O
/n	O
.	O
so	O
if	O
a	O
given	O
observation	O
has	O
a	O
leverage	B
statistic	O
leverage	B
statistic	O
3.3	O
other	O
considerations	O
in	O
the	O
regression	B
model	O
99	O
e	O
g	O
a	O
0	O
8	O
0	O
7	O
0	O
6	O
0	O
5	O
0	O
4	O
0	O
3	O
g	O
n	O
i	O
t	O
a	O
r	O
0	O
0	O
8	O
0	O
0	O
6	O
0	O
0	O
4	O
0	O
0	O
2	O
2000	O
4000	O
6000	O
8000	O
12000	O
2000	O
4000	O
6000	O
8000	O
12000	O
limit	O
limit	O
figure	O
3.14.	O
scatterplots	O
of	O
the	O
observations	B
from	O
the	O
credit	O
data	B
set	O
.	O
left	O
:	O
a	O
plot	B
of	O
age	O
versus	O
limit	O
.	O
these	O
two	O
variables	O
are	O
not	O
collinear	O
.	O
right	O
:	O
a	O
plot	B
of	O
rating	O
versus	O
limit	O
.	O
there	O
is	O
high	O
collinearity	B
.	O
that	O
greatly	O
exceeds	O
(	O
p+	O
1	O
)	O
/n	O
,	O
then	O
we	O
may	O
suspect	O
that	O
the	O
corresponding	O
point	O
has	O
high	O
leverage	B
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
3.13	O
provides	O
a	O
plot	B
of	O
the	O
studentized	B
residuals	O
versus	O
hi	O
for	O
the	O
data	B
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
3.13.	O
ob-	O
servation	O
41	O
stands	O
out	O
as	O
having	O
a	O
very	O
high	O
leverage	B
statistic	O
as	O
well	O
as	O
a	O
high	O
studentized	B
residual	O
.	O
in	O
other	O
words	O
,	O
it	O
is	O
an	O
outlier	B
as	O
well	O
as	O
a	O
high	O
leverage	B
observation	O
.	O
this	O
is	O
a	O
particularly	O
dangerous	O
combination	O
!	O
this	O
plot	B
also	O
reveals	O
the	O
reason	O
that	O
observation	O
20	O
had	O
relatively	O
little	O
eﬀect	O
on	O
the	O
least	B
squares	I
ﬁt	O
in	O
figure	O
3.12	O
:	O
it	O
has	O
low	O
leverage	B
.	O
6.	O
collinearity	B
collinearity	O
refers	O
to	O
the	O
situation	O
in	O
which	O
two	O
or	O
more	O
predictor	B
variables	O
are	O
closely	O
related	O
to	O
one	O
another	O
.	O
the	O
concept	O
of	O
collinearity	B
is	O
illustrated	O
in	O
figure	O
3.14	O
using	O
the	O
credit	O
data	B
set	O
.	O
in	O
the	O
left-hand	O
panel	O
of	O
fig-	O
ure	O
3.14	O
,	O
the	O
two	O
predictors	O
limit	O
and	O
age	O
appear	O
to	O
have	O
no	O
obvious	O
rela-	O
tionship	O
.	O
in	O
contrast	B
,	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
3.14	O
,	O
the	O
predictors	O
limit	O
and	O
rating	O
are	O
very	O
highly	O
correlated	O
with	O
each	O
other	O
,	O
and	O
we	O
say	O
that	O
they	O
are	O
collinear	O
.	O
the	O
presence	O
of	O
collinearity	B
can	O
pose	O
problems	O
in	O
the	O
regression	B
context	O
,	O
since	O
it	O
can	O
be	O
diﬃcult	O
to	O
separate	O
out	O
the	O
indi-	O
vidual	O
eﬀects	O
of	O
collinear	O
variables	O
on	O
the	O
response	B
.	O
in	O
other	O
words	O
,	O
since	O
limit	O
and	O
rating	O
tend	O
to	O
increase	O
or	O
decrease	O
together	O
,	O
it	O
can	O
be	O
diﬃcult	O
to	O
determine	O
how	O
each	O
one	O
separately	O
is	O
associated	O
with	O
the	O
response	B
,	O
balance	O
.	O
collinearity	B
figure	O
3.15	O
illustrates	O
some	O
of	O
the	O
diﬃculties	O
that	O
can	O
result	O
from	O
collinear-	O
ity	O
.	O
the	O
left-hand	O
panel	O
of	O
figure	O
3.15	O
is	O
a	O
contour	B
plot	I
of	O
the	O
rss	O
(	O
3.22	O
)	O
associated	O
with	O
diﬀerent	O
possible	O
coeﬃcient	B
estimates	O
for	O
the	O
regression	B
of	O
balance	O
on	O
limit	O
and	O
age	O
.	O
each	O
ellipse	O
represents	O
a	O
set	B
of	O
coeﬃcients	O
that	O
correspond	O
to	O
the	O
same	O
rss	O
,	O
with	O
ellipses	O
nearest	O
to	O
the	O
center	O
tak-	O
ing	O
on	O
the	O
lowest	O
values	O
of	O
rss	O
.	O
the	O
black	O
dots	O
and	O
associated	O
dashed	O
100	O
3.	O
linear	B
regression	I
e	O
g	O
a	O
β	O
0	O
1	O
−	O
2	O
−	O
3	O
−	O
4	O
−	O
5	O
−	O
21.8	O
21.5	O
21.25	O
2	O
1.8	O
2	O
1.5	O
g	O
n	O
i	O
t	O
a	O
r	O
β	O
5	O
4	O
3	O
2	O
1	O
0	O
0.16	O
0.17	O
0.18	O
0.19	O
βlimit	O
−0.1	O
0.0	O
βlimit	O
0.1	O
0.2	O
figure	O
3.15.	O
contour	O
plots	O
for	O
the	O
rss	O
values	O
as	O
a	O
function	B
of	O
the	O
parameters	O
β	O
for	O
various	O
regressions	O
involving	O
the	O
credit	O
data	B
set	O
.	O
in	O
each	O
plot	B
,	O
the	O
black	O
dots	O
represent	O
the	O
coeﬃcient	B
values	O
corresponding	O
to	O
the	O
minimum	O
rss	O
.	O
left	O
:	O
a	O
contour	B
plot	I
of	O
rss	O
for	O
the	O
regression	B
of	O
balance	O
onto	O
age	O
and	O
limit	O
.	O
the	O
minimum	O
value	O
is	O
well	O
deﬁned	O
.	O
right	O
:	O
a	O
contour	B
plot	I
of	O
rss	O
for	O
the	O
regression	B
of	O
balance	O
onto	O
rating	O
and	O
limit	O
.	O
because	O
of	O
the	O
collinearity	B
,	O
there	O
are	O
many	O
pairs	O
(	O
βlimit	O
,	O
βrating	O
)	O
with	O
a	O
similar	O
value	O
for	O
rss	O
.	O
lines	O
represent	O
the	O
coeﬃcient	B
estimates	O
that	O
result	O
in	O
the	O
smallest	O
possible	O
rss—in	O
other	O
words	O
,	O
these	O
are	O
the	O
least	B
squares	I
estimates	O
.	O
the	O
axes	O
for	O
limit	O
and	O
age	O
have	O
been	O
scaled	O
so	O
that	O
the	O
plot	B
includes	O
possible	O
coeﬃ-	O
cient	O
estimates	O
that	O
are	O
up	O
to	O
four	O
standard	O
errors	O
on	O
either	O
side	O
of	O
the	O
least	B
squares	I
estimates	O
.	O
thus	O
the	O
plot	B
includes	O
all	O
plausible	O
values	O
for	O
the	O
coeﬃcients	O
.	O
for	O
example	O
,	O
we	O
see	O
that	O
the	O
true	O
limit	O
coeﬃcient	B
is	O
almost	O
certainly	O
somewhere	O
between	O
0.15	O
and	O
0.20.	O
in	O
contrast	B
,	O
the	O
right-hand	O
panel	O
of	O
figure	O
3.15	O
displays	O
contour	O
plots	O
of	O
the	O
rss	O
associated	O
with	O
possible	O
coeﬃcient	B
estimates	O
for	O
the	O
regression	B
of	O
balance	O
onto	O
limit	O
and	O
rating	O
,	O
which	O
we	O
know	O
to	O
be	O
highly	O
collinear	O
.	O
now	O
the	O
contours	O
run	O
along	O
a	O
narrow	O
valley	O
;	O
there	O
is	O
a	O
broad	O
range	O
of	O
values	O
for	O
the	O
coeﬃcient	B
estimates	O
that	O
result	O
in	O
equal	O
values	O
for	O
rss	O
.	O
hence	O
a	O
small	O
change	O
in	O
the	O
data	B
could	O
cause	O
the	O
pair	O
of	O
coeﬃcient	B
values	O
that	O
yield	O
the	O
smallest	O
rss—that	O
is	O
,	O
the	O
least	B
squares	I
estimates—to	O
move	O
anywhere	O
along	O
this	O
valley	O
.	O
this	O
results	O
in	O
a	O
great	O
deal	O
of	O
uncertainty	O
in	O
the	O
coeﬃcient	B
estimates	O
.	O
notice	O
that	O
the	O
scale	O
for	O
the	O
limit	O
coeﬃcient	B
now	O
runs	O
from	O
roughly	O
−0.2	O
to	O
0.2	O
;	O
this	O
is	O
an	O
eight-fold	O
increase	O
over	O
the	O
plausible	O
range	O
of	O
the	O
limit	O
coeﬃcient	B
in	O
the	O
regression	B
with	O
age	O
.	O
interestingly	O
,	O
even	O
though	O
the	O
limit	O
and	O
rating	O
coeﬃcients	O
now	O
have	O
much	O
more	O
individual	O
uncertainty	O
,	O
they	O
will	O
almost	O
certainly	O
lie	O
somewhere	O
in	O
this	O
contour	O
valley	O
.	O
for	O
example	O
,	O
we	O
would	O
not	O
expect	O
the	O
true	O
value	O
of	O
the	O
limit	O
and	O
rating	O
coeﬃcients	O
to	O
be	O
−0.1	O
and	O
1	O
respectively	O
,	O
even	O
though	O
such	O
a	O
value	O
is	O
plausible	O
for	O
each	O
coeﬃcient	B
individually	O
.	O
3.3	O
other	O
considerations	O
in	O
the	O
regression	B
model	O
101	O
model	B
1	O
model	B
2	O
intercept	B
age	O
limit	O
intercept	B
rating	O
limit	O
coeﬃcient	B
−173.411	O
−2.292	O
0.173	O
−377.537	O
2.202	O
0.025	O
std	O
.	O
error	B
43.828	O
0.672	O
0.005	O
45.254	O
0.952	O
0.064	O
t-statistic	B
p-value	O
−3.957	O
<	O
0.0001	O
−3.407	O
0.0007	O
34.496	O
<	O
0.0001	O
−8.343	O
<	O
0.0001	O
0.0213	O
2.312	O
0.384	O
0.7012	O
table	O
3.11.	O
the	O
results	O
for	O
two	O
multiple	B
regression	O
models	O
involving	O
the	O
credit	O
data	B
set	O
are	O
shown	O
.	O
model	B
1	O
is	O
a	O
regression	B
of	O
balance	O
on	O
age	O
and	O
limit	O
,	O
and	O
model	B
2	O
a	O
regression	B
of	O
balance	O
on	O
rating	O
and	O
limit	O
.	O
the	O
standard	B
error	I
of	O
ˆβlimit	O
increases	O
12-fold	O
in	O
the	O
second	O
regression	B
,	O
due	O
to	O
collinearity	B
.	O
since	O
collinearity	B
reduces	O
the	O
accuracy	O
of	O
the	O
estimates	O
of	O
the	O
regression	B
coeﬃcients	O
,	O
it	O
causes	O
the	O
standard	B
error	I
for	O
ˆβj	O
to	O
grow	O
.	O
recall	B
that	O
the	O
t-statistic	B
for	O
each	O
predictor	B
is	O
calculated	O
by	O
dividing	O
ˆβj	O
by	O
its	O
standard	B
error	I
.	O
consequently	O
,	O
collinearity	B
results	O
in	O
a	O
decline	O
in	O
the	O
t-statistic	B
.	O
as	O
a	O
result	O
,	O
in	O
the	O
presence	O
of	O
collinearity	B
,	O
we	O
may	O
fail	O
to	O
reject	O
h0	O
:	O
βj	O
=	O
0.	O
this	O
means	O
that	O
the	O
power	B
of	O
the	O
hypothesis	B
test—the	O
probability	B
of	O
correctly	O
power	B
detecting	O
a	O
non-zero	O
coeﬃcient—is	O
reduced	O
by	O
collinearity	B
.	O
table	O
3.11	O
compares	O
the	O
coeﬃcient	B
estimates	O
obtained	O
from	O
two	O
separate	O
multiple	B
regression	O
models	O
.	O
the	O
ﬁrst	O
is	O
a	O
regression	B
of	O
balance	O
on	O
age	O
and	O
limit	O
,	O
and	O
the	O
second	O
is	O
a	O
regression	B
of	O
balance	O
on	O
rating	O
and	O
limit	O
.	O
in	O
the	O
ﬁrst	O
regression	B
,	O
both	O
age	O
and	O
limit	O
are	O
highly	O
signiﬁcant	O
with	O
very	O
small	O
p-	O
values	O
.	O
in	O
the	O
second	O
,	O
the	O
collinearity	B
between	O
limit	O
and	O
rating	O
has	O
caused	O
the	O
standard	B
error	I
for	O
the	O
limit	O
coeﬃcient	B
estimate	O
to	O
increase	O
by	O
a	O
factor	B
of	O
12	O
and	O
the	O
p-value	B
to	O
increase	O
to	O
0.701.	O
in	O
other	O
words	O
,	O
the	O
importance	B
of	O
the	O
limit	O
variable	B
has	O
been	O
masked	O
due	O
to	O
the	O
presence	O
of	O
collinearity	B
.	O
to	O
avoid	O
such	O
a	O
situation	O
,	O
it	O
is	O
desirable	O
to	O
identify	O
and	O
address	O
potential	O
collinearity	B
problems	O
while	O
ﬁtting	O
the	O
model	B
.	O
a	O
simple	B
way	O
to	O
detect	O
collinearity	B
is	O
to	O
look	O
at	O
the	O
correlation	B
matrix	O
of	O
the	O
predictors	O
.	O
an	O
element	O
of	O
this	O
matrix	O
that	O
is	O
large	O
in	O
absolute	O
value	O
indicates	O
a	O
pair	O
of	O
highly	O
correlated	O
variables	O
,	O
and	O
therefore	O
a	O
collinearity	B
problem	O
in	O
the	O
data	B
.	O
unfortunately	O
,	O
not	O
all	O
collinearity	B
problems	O
can	O
be	O
detected	O
by	O
inspection	O
of	O
the	O
correlation	B
matrix	O
:	O
it	O
is	O
possible	O
for	O
collinear-	O
ity	O
to	O
exist	O
between	O
three	O
or	O
more	O
variables	O
even	O
if	O
no	O
pair	O
of	O
variables	O
has	O
a	O
particularly	O
high	O
correlation	B
.	O
we	O
call	O
this	O
situation	O
multicollinearity	B
.	O
instead	O
of	O
inspecting	O
the	O
correlation	B
matrix	O
,	O
a	O
better	O
way	O
to	O
assess	O
multi-	O
collinearity	B
is	O
to	O
compute	O
the	O
variance	B
inﬂation	O
factor	B
(	O
vif	O
)	O
.	O
the	O
vif	O
is	O
the	O
ratio	O
of	O
the	O
variance	B
of	O
ˆβj	O
when	O
ﬁtting	O
the	O
full	O
model	B
divided	O
by	O
the	O
variance	B
of	O
ˆβj	O
if	O
ﬁt	B
on	O
its	O
own	O
.	O
the	O
smallest	O
possible	O
value	O
for	O
vif	O
is	O
1	O
,	O
which	O
indicates	O
the	O
complete	B
absence	O
of	O
collinearity	B
.	O
typically	O
in	O
practice	O
there	O
is	O
a	O
small	O
amount	O
of	O
collinearity	B
among	O
the	O
predictors	O
.	O
as	O
a	O
rule	O
of	O
thumb	O
,	O
a	O
vif	O
value	O
that	O
exceeds	O
5	O
or	O
10	O
indicates	O
a	O
problematic	O
amount	O
of	O
multi-	O
collinearity	B
variance	O
inﬂation	B
factor	I
102	O
3.	O
linear	B
regression	I
collinearity	O
.	O
the	O
vif	O
for	O
each	O
variable	B
can	O
be	O
computed	O
using	O
the	O
formula	O
vif	O
(	O
ˆβj	O
)	O
=	O
1	O
1	O
−	O
r2	O
xj|x−j	O
,	O
xj|x−j	O
where	O
r2	O
predictors	O
.	O
if	O
r2	O
the	O
vif	O
will	O
be	O
large	O
.	O
is	O
the	O
r2	O
from	O
a	O
regression	B
of	O
xj	O
onto	O
all	O
of	O
the	O
other	O
xj|x−j	O
is	O
close	O
to	O
one	O
,	O
then	O
collinearity	B
is	O
present	O
,	O
and	O
so	O
in	O
the	O
credit	O
data	B
,	O
a	O
regression	B
of	O
balance	O
on	O
age	O
,	O
rating	O
,	O
and	O
limit	O
indicates	O
that	O
the	O
predictors	O
have	O
vif	O
values	O
of	O
1.01	O
,	O
160.67	O
,	O
and	O
160.59.	O
as	O
we	O
suspected	O
,	O
there	O
is	O
considerable	O
collinearity	B
in	O
the	O
data	B
!	O
when	O
faced	O
with	O
the	O
problem	O
of	O
collinearity	B
,	O
there	O
are	O
two	O
simple	B
solu-	O
tions	O
.	O
the	O
ﬁrst	O
is	O
to	O
drop	O
one	O
of	O
the	O
problematic	O
variables	O
from	O
the	O
regres-	O
sion	O
.	O
this	O
can	O
usually	O
be	O
done	O
without	O
much	O
compromise	O
to	O
the	O
regression	B
ﬁt	O
,	O
since	O
the	O
presence	O
of	O
collinearity	B
implies	O
that	O
the	O
information	O
that	O
this	O
variable	B
provides	O
about	O
the	O
response	B
is	O
redundant	O
in	O
the	O
presence	O
of	O
the	O
other	O
variables	O
.	O
for	O
instance	O
,	O
if	O
we	O
regress	O
balance	O
onto	O
age	O
and	O
limit	O
,	O
without	O
the	O
rating	O
predictor	B
,	O
then	O
the	O
resulting	O
vif	O
values	O
are	O
close	O
to	O
the	O
minimum	O
possible	O
value	O
of	O
1	O
,	O
and	O
the	O
r2	O
drops	O
from	O
0.754	O
to	O
0.75.	O
so	O
dropping	O
rating	O
from	O
the	O
set	B
of	O
predictors	O
has	O
eﬀectively	O
solved	O
the	O
collinearity	B
problem	O
without	O
compromising	O
the	O
ﬁt	B
.	O
the	O
second	O
solution	O
is	O
to	O
combine	O
the	O
collinear	O
variables	O
together	O
into	O
a	O
single	B
predictor	O
.	O
for	O
in-	O
stance	O
,	O
we	O
might	O
take	O
the	O
average	B
of	O
standardized	O
versions	O
of	O
limit	O
and	O
rating	O
in	O
order	O
to	O
create	O
a	O
new	O
variable	B
that	O
measures	O
credit	O
worthiness	O
.	O
3.4	O
the	O
marketing	O
plan	O
we	O
now	O
brieﬂy	O
return	O
to	O
the	O
seven	O
questions	O
about	O
the	O
advertising	O
data	B
that	O
we	O
set	B
out	O
to	O
answer	O
at	O
the	O
beginning	O
of	O
this	O
chapter	O
.	O
1.	O
is	O
there	O
a	O
relationship	O
between	O
advertising	O
sales	O
and	O
budget	O
?	O
this	O
question	O
can	O
be	O
answered	O
by	O
ﬁtting	O
a	O
multiple	B
regression	O
model	B
of	O
sales	O
onto	O
tv	O
,	O
radio	O
,	O
and	O
newspaper	O
,	O
as	O
in	O
(	O
3.20	O
)	O
,	O
and	O
testing	O
the	O
hypothesis	B
h0	O
:	O
βtv	O
=	O
βradio	O
=	O
βnewspaper	O
=	O
0.	O
in	O
section	O
3.2.2	O
,	O
we	O
showed	O
that	O
the	O
f-statistic	O
can	O
be	O
used	O
to	O
determine	O
whether	O
or	O
not	O
we	O
should	O
reject	O
this	O
null	B
hypothesis	O
.	O
in	O
this	O
case	O
the	O
p-value	B
corresponding	O
to	O
the	O
f-statistic	O
in	O
table	O
3.6	O
is	O
very	O
low	O
,	O
indicating	O
clear	O
evidence	O
of	O
a	O
relationship	O
between	O
advertising	O
and	O
sales	O
.	O
2.	O
how	O
strong	O
is	O
the	O
relationship	O
?	O
we	O
discussed	O
two	O
measures	O
of	O
model	B
accuracy	O
in	O
section	O
3.1.3.	O
first	O
,	O
the	O
rse	O
estimates	O
the	O
standard	O
deviation	O
of	O
the	O
response	B
from	O
the	O
population	B
regression	I
line	I
.	O
for	O
the	O
advertising	O
data	B
,	O
the	O
rse	O
is	O
1,681	O
3.4	O
the	O
marketing	O
plan	O
103	O
units	O
while	O
the	O
mean	O
value	O
for	O
the	O
response	B
is	O
14,022	O
,	O
indicating	O
a	O
percentage	O
error	B
of	O
roughly	O
12	O
%	O
.	O
second	O
,	O
the	O
r2	O
statistic	O
records	O
the	O
percentage	O
of	O
variability	O
in	O
the	O
response	B
that	O
is	O
explained	B
by	O
the	O
predictors	O
.	O
the	O
predictors	O
explain	O
almost	O
90	O
%	O
of	O
the	O
variance	B
in	O
sales	O
.	O
the	O
rse	O
and	O
r2	O
statistics	O
are	O
displayed	O
in	O
table	O
3.6	O
.	O
3.	O
which	O
media	O
contribute	O
to	O
sales	O
?	O
to	O
answer	O
this	O
question	O
,	O
we	O
can	O
examine	O
the	O
p-values	O
associated	O
with	O
each	O
predictor	B
’	O
s	O
t-statistic	B
(	O
section	O
3.1.2	O
)	O
.	O
in	O
the	O
multiple	B
linear	O
re-	O
gression	O
displayed	O
in	O
table	O
3.4	O
,	O
the	O
p-values	O
for	O
tv	O
and	O
radio	O
are	O
low	O
,	O
but	O
the	O
p-value	B
for	O
newspaper	O
is	O
not	O
.	O
this	O
suggests	O
that	O
only	O
tv	O
and	O
radio	O
are	O
related	O
to	O
sales	O
.	O
in	O
chapter	O
6	O
we	O
explore	O
this	O
question	O
in	O
greater	O
detail	O
.	O
4.	O
how	O
large	O
is	O
the	O
eﬀect	O
of	O
each	O
medium	O
on	O
sales	O
?	O
we	O
saw	O
in	O
section	O
3.1.2	O
that	O
the	O
standard	B
error	I
of	O
ˆβj	O
can	O
be	O
used	O
to	O
construct	O
conﬁdence	O
intervals	O
for	O
βj	O
.	O
for	O
the	O
advertising	O
data	B
,	O
the	O
95	O
%	O
conﬁdence	O
intervals	O
are	O
as	O
follows	O
:	O
(	O
0.043	O
,	O
0.049	O
)	O
for	O
tv	O
,	O
(	O
0.172	O
,	O
0.206	O
)	O
for	O
radio	O
,	O
and	O
(	O
−0.013	O
,	O
0.011	O
)	O
for	O
newspaper	O
.	O
the	O
conﬁ-	O
dence	O
intervals	O
for	O
tv	O
and	O
radio	O
are	O
narrow	O
and	O
far	O
from	O
zero	O
,	O
provid-	O
ing	O
evidence	O
that	O
these	O
media	O
are	O
related	O
to	O
sales	O
.	O
but	O
the	O
interval	B
for	O
newspaper	O
includes	O
zero	O
,	O
indicating	O
that	O
the	O
variable	B
is	O
not	O
statis-	O
tically	O
signiﬁcant	O
given	O
the	O
values	O
of	O
tv	O
and	O
radio	O
.	O
we	O
saw	O
in	O
section	O
3.3.3	O
that	O
collinearity	B
can	O
result	O
in	O
very	O
wide	O
stan-	O
dard	O
errors	O
.	O
could	O
collinearity	B
be	O
the	O
reason	O
that	O
the	O
conﬁdence	O
in-	O
terval	O
associated	O
with	O
newspaper	O
is	O
so	O
wide	O
?	O
the	O
vif	O
scores	O
are	O
1.005	O
,	O
1.145	O
,	O
and	O
1.145	O
for	O
tv	O
,	O
radio	O
,	O
and	O
newspaper	O
,	O
suggesting	O
no	O
evidence	O
of	O
collinearity	B
.	O
in	O
order	O
to	O
assess	O
the	O
association	O
of	O
each	O
medium	O
individually	O
on	O
sales	O
,	O
we	O
can	O
perform	O
three	O
separate	O
simple	B
linear	O
regressions	O
.	O
re-	O
sults	O
are	O
shown	O
in	O
tables	O
3.1	O
and	O
3.3.	O
there	O
is	O
evidence	O
of	O
an	O
ex-	O
tremely	O
strong	O
association	O
between	O
tv	O
and	O
sales	O
and	O
between	O
radio	O
and	O
sales	O
.	O
there	O
is	O
evidence	O
of	O
a	O
mild	O
association	O
between	O
newspaper	O
and	O
sales	O
,	O
when	O
the	O
values	O
of	O
tv	O
and	O
radio	O
are	O
ignored	O
.	O
5.	O
how	O
accurately	O
can	O
we	O
predict	O
future	O
sales	O
?	O
the	O
response	B
can	O
be	O
predicted	O
using	O
(	O
3.21	O
)	O
.	O
the	O
accuracy	O
associ-	O
ated	O
with	O
this	O
estimate	O
depends	O
on	O
whether	O
we	O
wish	O
to	O
predict	O
an	O
individual	O
response	B
,	O
y	O
=	O
f	O
(	O
x	O
)	O
+	O
	O
,	O
or	O
the	O
average	B
response	O
,	O
f	O
(	O
x	O
)	O
(	O
section	O
3.2.2	O
)	O
.	O
if	O
the	O
former	O
,	O
we	O
use	O
a	O
prediction	B
interval	O
,	O
and	O
if	O
the	O
latter	O
,	O
we	O
use	O
a	O
conﬁdence	B
interval	I
.	O
prediction	B
intervals	O
will	O
always	O
be	O
wider	O
than	O
conﬁdence	O
intervals	O
because	O
they	O
account	O
for	O
the	O
un-	O
certainty	O
associated	O
with	O
	O
,	O
the	O
irreducible	B
error	I
.	O
104	O
3.	O
linear	B
regression	I
6.	O
is	O
the	O
relationship	O
linear	B
?	O
in	O
section	O
3.3.3	O
,	O
we	O
saw	O
that	O
residual	B
plots	O
can	O
be	O
used	O
in	O
order	O
to	O
identify	O
non-linearity	O
.	O
if	O
the	O
relationships	O
are	O
linear	B
,	O
then	O
the	O
residual	B
plots	O
should	O
display	O
no	O
pattern	O
.	O
in	O
the	O
case	O
of	O
the	O
advertising	O
data	B
,	O
we	O
observe	O
a	O
non-linear	B
eﬀect	O
in	O
figure	O
3.5	O
,	O
though	O
this	O
eﬀect	O
could	O
also	O
be	O
observed	O
in	O
a	O
residual	B
plot	O
.	O
in	O
section	O
3.3.2	O
,	O
we	O
discussed	O
the	O
inclusion	O
of	O
transformations	O
of	O
the	O
predictors	O
in	O
the	O
linear	B
regression	I
model	O
in	O
order	O
to	O
accommodate	O
non-linear	B
relationships	O
.	O
7.	O
is	O
there	O
synergy	B
among	O
the	O
advertising	O
media	O
?	O
the	O
standard	O
linear	O
regression	B
model	O
assumes	O
an	O
additive	B
relation-	O
ship	O
between	O
the	O
predictors	O
and	O
the	O
response	B
.	O
an	O
additive	B
model	O
is	O
easy	O
to	O
interpret	O
because	O
the	O
eﬀect	O
of	O
each	O
predictor	B
on	O
the	O
response	B
is	O
unrelated	O
to	O
the	O
values	O
of	O
the	O
other	O
predictors	O
.	O
however	O
,	O
the	O
additive	B
assumption	O
may	O
be	O
unrealistic	O
for	O
certain	O
data	B
sets	O
.	O
in	O
section	O
3.3.2	O
,	O
we	O
showed	O
how	O
to	O
include	O
an	O
interaction	B
term	O
in	O
the	O
regression	B
model	O
in	O
order	O
to	O
accommodate	O
non-additive	O
relationships	O
.	O
a	O
small	O
p-value	B
associated	O
with	O
the	O
interaction	B
term	O
indicates	O
the	O
presence	O
of	O
such	O
relationships	O
.	O
figure	O
3.5	O
suggested	O
that	O
the	O
advertising	O
data	B
may	O
not	O
be	O
additive	B
.	O
including	O
an	O
interaction	B
term	O
in	O
the	O
model	B
results	O
in	O
a	O
substantial	O
increase	O
in	O
r2	O
,	O
from	O
around	O
90	O
%	O
to	O
almost	O
97	O
%	O
.	O
3.5	O
comparison	O
of	O
linear	B
regression	I
with	O
k-nearest	O
neighbors	O
as	O
discussed	O
in	O
chapter	O
2	O
,	O
linear	B
regression	I
is	O
an	O
example	O
of	O
a	O
parametric	B
approach	O
because	O
it	O
assumes	O
a	O
linear	B
functional	O
form	O
for	O
f	O
(	O
x	O
)	O
.	O
parametric	B
methods	O
have	O
several	O
advantages	O
.	O
they	O
are	O
often	O
easy	O
to	O
ﬁt	B
,	O
because	O
one	O
need	O
estimate	O
only	O
a	O
small	O
number	O
of	O
coeﬃcients	O
.	O
in	O
the	O
case	O
of	O
linear	B
re-	O
gression	O
,	O
the	O
coeﬃcients	O
have	O
simple	B
interpretations	O
,	O
and	O
tests	O
of	O
statistical	O
signiﬁcance	O
can	O
be	O
easily	O
performed	O
.	O
but	O
parametric	B
methods	O
do	O
have	O
a	O
disadvantage	O
:	O
by	O
construction	O
,	O
they	O
make	O
strong	O
assumptions	O
about	O
the	O
form	O
of	O
f	O
(	O
x	O
)	O
.	O
if	O
the	O
speciﬁed	O
functional	O
form	O
is	O
far	O
from	O
the	O
truth	O
,	O
and	O
prediction	B
accuracy	O
is	O
our	O
goal	O
,	O
then	O
the	O
parametric	B
method	O
will	O
perform	O
poorly	O
.	O
for	O
instance	O
,	O
if	O
we	O
assume	O
a	O
linear	B
relationship	O
between	O
x	O
and	O
y	O
but	O
the	O
true	O
relationship	O
is	O
far	O
from	O
linear	B
,	O
then	O
the	O
resulting	O
model	B
will	O
provide	O
a	O
poor	O
ﬁt	B
to	O
the	O
data	B
,	O
and	O
any	O
conclusions	O
drawn	O
from	O
it	O
will	O
be	O
suspect	O
.	O
in	O
contrast	B
,	O
non-parametric	B
methods	O
do	O
not	O
explicitly	O
assume	O
a	O
para-	O
metric	O
form	O
for	O
f	O
(	O
x	O
)	O
,	O
and	O
thereby	O
provide	O
an	O
alternative	O
and	O
more	O
ﬂexi-	O
ble	O
approach	B
for	O
performing	O
regression	B
.	O
we	O
discuss	O
various	O
non-parametric	B
methods	O
in	O
this	O
book	O
.	O
here	O
we	O
consider	O
one	O
of	O
the	O
simplest	O
and	O
best-known	O
non-parametric	B
methods	O
,	O
k-nearest	O
neighbors	O
regression	B
(	O
knn	O
regression	B
)	O
.	O
k-nearest	O
neighbors	O
regression	B
3.5	O
comparison	O
of	O
linear	B
regression	I
with	O
k-nearest	O
neighbors	O
105	O
y	O
y	O
x2	B
x2	O
x1	O
x1	O
figure	O
3.16.	O
plots	O
of	O
ˆf	O
(	O
x	O
)	O
using	O
knn	O
regression	B
on	O
a	O
two-dimensional	O
data	B
set	O
with	O
64	O
observations	B
(	O
orange	O
dots	O
)	O
.	O
left	O
:	O
k	O
=	O
1	O
results	O
in	O
a	O
rough	O
step	O
func-	O
tion	O
ﬁt	B
.	O
right	O
:	O
k	O
=	O
9	O
produces	O
a	O
much	O
smoother	B
ﬁt	O
.	O
the	O
knn	O
regression	B
method	O
is	O
closely	O
related	O
to	O
the	O
knn	O
classiﬁer	B
dis-	O
cussed	O
in	O
chapter	O
2.	O
given	O
a	O
value	O
for	O
k	O
and	O
a	O
prediction	B
point	O
x0	O
,	O
knn	O
regression	B
ﬁrst	O
identiﬁes	O
the	O
k	O
training	B
observations	O
that	O
are	O
closest	O
to	O
x0	O
,	O
represented	O
by	O
n0	O
.	O
it	O
then	O
estimates	O
f	O
(	O
x0	O
)	O
using	O
the	O
average	B
of	O
all	O
the	O
training	B
responses	O
in	O
n0	O
.	O
in	O
other	O
words	O
,	O
(	O
cid:17	O
)	O
ˆf	O
(	O
x0	O
)	O
=	O
1	O
k	O
yi	O
.	O
xi∈n0	O
figure	O
3.16	O
illustrates	O
two	O
knn	O
ﬁts	O
on	O
a	O
data	B
set	O
with	O
p	O
=	O
2	O
predictors	O
.	O
the	O
ﬁt	B
with	O
k	O
=	O
1	O
is	O
shown	O
in	O
the	O
left-hand	O
panel	O
,	O
while	O
the	O
right-hand	O
panel	O
corresponds	O
to	O
k	O
=	O
9.	O
we	O
see	O
that	O
when	O
k	O
=	O
1	O
,	O
the	O
knn	O
ﬁt	B
perfectly	O
interpolates	O
the	O
training	B
observations	O
,	O
and	O
consequently	O
takes	O
the	O
form	O
of	O
a	O
step	B
function	I
.	O
when	O
k	O
=	O
9	O
,	O
the	O
knn	O
ﬁt	B
still	O
is	O
a	O
step	B
function	I
,	O
but	O
averaging	O
over	O
nine	O
observations	B
results	O
in	O
much	O
smaller	O
regions	O
of	O
constant	O
prediction	B
,	O
and	O
consequently	O
a	O
smoother	B
ﬁt	O
.	O
in	O
general	O
,	O
the	O
optimal	O
value	O
for	O
k	O
will	O
depend	O
on	O
the	O
bias-variance	B
tradeoﬀ	O
,	O
which	O
we	O
introduced	O
in	O
chapter	O
2.	O
a	O
small	O
value	O
for	O
k	O
provides	O
the	O
most	O
ﬂexible	B
ﬁt	O
,	O
which	O
will	O
have	O
low	O
bias	B
but	O
high	O
variance	B
.	O
this	O
variance	B
is	O
due	O
to	O
the	O
fact	O
that	O
the	O
prediction	B
in	O
a	O
given	O
region	O
is	O
entirely	O
dependent	B
on	O
just	O
one	O
observation	O
.	O
in	O
contrast	B
,	O
larger	O
values	O
of	O
k	O
provide	O
a	O
smoother	B
and	O
less	O
variable	B
ﬁt	O
;	O
the	O
prediction	B
in	O
a	O
region	O
is	O
an	O
average	B
of	O
several	O
points	O
,	O
and	O
so	O
changing	O
one	O
observation	O
has	O
a	O
smaller	O
eﬀect	O
.	O
however	O
,	O
the	O
smoothing	B
may	O
cause	O
bias	B
by	O
masking	O
some	O
of	O
the	O
structure	O
in	O
f	O
(	O
x	O
)	O
.	O
in	O
chapter	O
5	O
,	O
we	O
introduce	O
several	O
approaches	O
for	O
estimating	O
test	B
error	O
rates	O
.	O
these	O
methods	O
can	O
be	O
used	O
to	O
identify	O
the	O
optimal	O
value	O
of	O
k	O
in	O
knn	O
regression	B
.	O
106	O
3.	O
linear	B
regression	I
in	O
what	O
setting	O
will	O
a	O
parametric	B
approach	O
such	O
as	O
least	B
squares	I
linear	O
re-	O
gression	O
outperform	O
a	O
non-parametric	B
approach	O
such	O
as	O
knn	O
regression	B
?	O
the	O
answer	O
is	O
simple	B
:	O
the	O
parametric	B
approach	O
will	O
outperform	O
the	O
non-	O
parametric	B
approach	O
if	O
the	O
parametric	B
form	O
that	O
has	O
been	O
selected	O
is	O
close	O
to	O
the	O
true	O
form	O
of	O
f	O
.	O
figure	O
3.17	O
provides	O
an	O
example	O
with	O
data	B
generated	O
from	O
a	O
one-dimensional	O
linear	B
regression	I
model	O
.	O
the	O
black	O
solid	O
lines	O
rep-	O
resent	O
f	O
(	O
x	O
)	O
,	O
while	O
the	O
blue	O
curves	O
correspond	O
to	O
the	O
knn	O
ﬁts	O
using	O
k	O
=	O
1	O
and	O
k	O
=	O
9.	O
in	O
this	O
case	O
,	O
the	O
k	O
=	O
1	O
predictions	O
are	O
far	O
too	O
variable	B
,	O
while	O
the	O
smoother	B
k	O
=	O
9	O
ﬁt	B
is	O
much	O
closer	O
to	O
f	O
(	O
x	O
)	O
.	O
however	O
,	O
since	O
the	O
true	O
relationship	O
is	O
linear	B
,	O
it	O
is	O
hard	O
for	O
a	O
non-parametric	B
approach	O
to	O
compete	O
with	O
linear	B
regression	I
:	O
a	O
non-parametric	B
approach	O
incurs	O
a	O
cost	O
in	O
variance	B
that	O
is	O
not	O
oﬀset	O
by	O
a	O
reduction	O
in	O
bias	B
.	O
the	O
blue	O
dashed	O
line	B
in	O
the	O
left-	O
hand	O
panel	O
of	O
figure	O
3.18	O
represents	O
the	O
linear	B
regression	I
ﬁt	O
to	O
the	O
same	O
data	B
.	O
it	O
is	O
almost	O
perfect	O
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
3.18	O
reveals	O
that	O
linear	B
regression	I
outperforms	O
knn	O
for	O
this	O
data	B
.	O
the	O
green	O
solid	O
line	B
,	O
plot-	O
ted	O
as	O
a	O
function	B
of	O
1/k	O
,	O
represents	O
the	O
test	B
set	O
mean	B
squared	I
error	I
(	O
mse	O
)	O
for	O
knn	O
.	O
the	O
knn	O
errors	O
are	O
well	O
above	O
the	O
black	O
dashed	O
line	B
,	O
which	O
is	O
the	O
test	B
mse	O
for	O
linear	O
regression	B
.	O
when	O
the	O
value	O
of	O
k	O
is	O
large	O
,	O
then	O
knn	O
performs	O
only	O
a	O
little	O
worse	O
than	O
least	B
squares	I
regression	O
in	O
terms	O
of	O
mse	O
.	O
it	O
performs	O
far	O
worse	O
when	O
k	O
is	O
small	O
.	O
in	O
practice	O
,	O
the	O
true	O
relationship	O
between	O
x	O
and	O
y	O
is	O
rarely	O
exactly	O
lin-	O
ear	O
.	O
figure	O
3.19	O
examines	O
the	O
relative	O
performances	O
of	O
least	B
squares	I
regres-	O
sion	O
and	O
knn	O
under	O
increasing	O
levels	O
of	O
non-linearity	O
in	O
the	O
relationship	O
between	O
x	O
and	O
y	O
.	O
in	O
the	O
top	O
row	O
,	O
the	O
true	O
relationship	O
is	O
nearly	O
linear	B
.	O
in	O
this	O
case	O
we	O
see	O
that	O
the	O
test	B
mse	O
for	O
linear	O
regression	B
is	O
still	O
superior	O
to	O
that	O
of	O
knn	O
for	O
low	O
values	O
of	O
k.	O
however	O
,	O
for	O
k	O
≥	O
4	O
,	O
knn	O
out-	O
performs	O
linear	B
regression	I
.	O
the	O
second	O
row	O
illustrates	O
a	O
more	O
substantial	O
deviation	O
from	O
linearity	O
.	O
in	O
this	O
situation	O
,	O
knn	O
substantially	O
outperforms	O
linear	B
regression	I
for	O
all	O
values	O
of	O
k.	O
note	O
that	O
as	O
the	O
extent	O
of	O
non-linearity	O
increases	O
,	O
there	O
is	O
little	O
change	O
in	O
the	O
test	B
set	O
mse	O
for	O
the	O
non-parametric	B
knn	O
method	O
,	O
but	O
there	O
is	O
a	O
large	O
increase	O
in	O
the	O
test	B
set	O
mse	O
of	O
linear	B
regression	I
.	O
figures	O
3.18	O
and	O
3.19	O
display	O
situations	O
in	O
which	O
knn	O
performs	O
slightly	O
worse	O
than	O
linear	B
regression	I
when	O
the	O
relationship	O
is	O
linear	B
,	O
but	O
much	O
better	O
than	O
linear	B
regression	I
for	O
non-linear	B
situations	O
.	O
in	O
a	O
real	O
life	O
situation	O
in	O
which	O
the	O
true	O
relationship	O
is	O
unknown	O
,	O
one	O
might	O
draw	O
the	O
conclusion	O
that	O
knn	O
should	O
be	O
favored	O
over	O
linear	B
regression	I
because	O
it	O
will	O
at	O
worst	O
be	O
slightly	O
inferior	O
than	O
linear	B
regression	I
if	O
the	O
true	O
relationship	O
is	O
linear	B
,	O
and	O
may	O
give	O
substantially	O
better	O
results	O
if	O
the	O
true	O
relationship	O
is	O
non-linear	B
.	O
but	O
in	O
reality	O
,	O
even	O
when	O
the	O
true	O
relationship	O
is	O
highly	O
non-linear	B
,	O
knn	O
may	O
still	O
provide	O
inferior	O
results	O
to	O
linear	B
regression	I
.	O
in	O
particular	O
,	O
both	O
figures	O
3.18	O
and	O
3.19	O
illustrate	O
settings	O
with	O
p	O
=	O
1	O
predictor	B
.	O
but	O
in	O
higher	O
dimensions	O
,	O
knn	O
often	O
performs	O
worse	O
than	O
linear	B
regression	I
.	O
figure	O
3.20	O
considers	O
the	O
same	O
strongly	O
non-linear	B
situation	O
as	O
in	O
the	O
second	O
row	O
of	O
figure	O
3.19	O
,	O
except	O
that	O
we	O
have	O
added	O
additional	O
noise	B
3.5	O
comparison	O
of	O
linear	B
regression	I
with	O
k-nearest	O
neighbors	O
107	O
4	O
3	O
y	O
2	O
1	O
4	O
3	O
y	O
2	O
1	O
−1.0	O
−0.5	O
0.0	O
x	O
0.5	O
1.0	O
−1.0	O
−0.5	O
0.5	O
1.0	O
0.0	O
x	O
figure	O
3.17.	O
plots	O
of	O
ˆf	O
(	O
x	O
)	O
using	O
knn	O
regression	B
on	O
a	O
one-dimensional	O
data	B
set	O
with	O
100	O
observations	B
.	O
the	O
true	O
relationship	O
is	O
given	O
by	O
the	O
black	O
solid	O
line	B
.	O
left	O
:	O
the	O
blue	O
curve	O
corresponds	O
to	O
k	O
=	O
1	O
and	O
interpolates	O
(	O
i.e	O
.	O
passes	O
directly	O
through	O
)	O
the	O
training	B
data	O
.	O
right	O
:	O
the	O
blue	O
curve	O
corresponds	O
to	O
k	O
=	O
9	O
,	O
and	O
represents	O
a	O
smoother	B
ﬁt	O
.	O
4	O
3	O
y	O
2	O
1	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
5	O
1	O
.	O
0	O
0	O
1	O
0	O
.	O
5	O
0	O
.	O
0	O
0	O
0	O
0	O
.	O
−1.0	O
−0.5	O
0.0	O
x	O
0.5	O
1.0	O
0.2	O
0.5	O
1.0	O
1/k	O
figure	O
3.18.	O
the	O
same	O
data	B
set	O
shown	O
in	O
figure	O
3.17	O
is	O
investigated	O
further	O
.	O
left	O
:	O
the	O
blue	O
dashed	O
line	B
is	O
the	O
least	B
squares	I
ﬁt	O
to	O
the	O
data	B
.	O
since	O
f	O
(	O
x	O
)	O
is	O
in	O
fact	O
linear	B
(	O
displayed	O
as	O
the	O
black	O
line	B
)	O
,	O
the	O
least	B
squares	I
regression	O
line	B
provides	O
a	O
very	O
good	O
estimate	O
of	O
f	O
(	O
x	O
)	O
.	O
right	O
:	O
the	O
dashed	O
horizontal	O
line	B
represents	O
the	O
least	B
squares	I
test	O
set	B
mse	O
,	O
while	O
the	O
green	O
solid	O
line	B
corresponds	O
to	O
the	O
mse	O
for	O
knn	O
as	O
a	O
function	B
of	O
1/k	O
(	O
on	O
the	O
log	O
scale	O
)	O
.	O
linear	B
regression	I
achieves	O
a	O
lower	O
test	B
mse	O
than	O
does	O
knn	O
regression	B
,	O
since	O
f	O
(	O
x	O
)	O
is	O
in	O
fact	O
linear	B
.	O
for	O
knn	O
regression	B
,	O
the	O
best	O
results	O
occur	O
with	O
a	O
very	O
large	O
value	O
of	O
k	O
,	O
corresponding	O
to	O
a	O
small	O
value	O
of	O
1/k	O
.	O
108	O
3.	O
linear	B
regression	I
−1.0	O
−0.5	O
0.5	O
1.0	O
0.0	O
x	O
y	O
y	O
5	O
3	O
.	O
0	O
3	O
.	O
5	O
2	O
.	O
0	O
.	O
2	O
5	O
1	O
.	O
0	O
.	O
1	O
5	O
.	O
0	O
5	O
.	O
3	O
0	O
.	O
3	O
5	O
.	O
2	O
0	O
2	O
.	O
5	O
.	O
1	O
0	O
.	O
1	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0.2	O
0.5	O
1.0	O
1/k	O
8	O
0	O
0	O
.	O
6	O
0	O
.	O
0	O
4	O
0	O
.	O
0	O
2	O
0	O
0	O
.	O
0	O
0	O
.	O
0	O
5	O
1	O
.	O
0	O
0	O
1	O
0	O
.	O
5	O
0	O
0	O
.	O
0	O
0	O
.	O
0	O
−1.0	O
−0.5	O
0.0	O
x	O
0.5	O
1.0	O
0.2	O
0.5	O
1.0	O
1/k	O
figure	O
3.19.	O
top	O
left	O
:	O
in	O
a	O
setting	O
with	O
a	O
slightly	O
non-linear	B
relationship	O
between	O
x	O
and	O
y	O
(	O
solid	O
black	O
line	B
)	O
,	O
the	O
knn	O
ﬁts	O
with	O
k	O
=	O
1	O
(	O
blue	O
)	O
and	O
k	O
=	O
9	O
(	O
red	O
)	O
are	O
displayed	O
.	O
top	O
right	O
:	O
for	O
the	O
slightly	O
non-linear	B
data	O
,	O
the	O
test	B
set	O
mse	O
for	O
least	O
squares	O
regression	B
(	O
horizontal	O
black	O
)	O
and	O
kn	O
n	O
with	O
various	O
values	O
of	O
1/k	O
(	O
green	O
)	O
are	O
displayed	O
.	O
bottom	O
left	O
and	O
bottom	O
right	O
:	O
as	O
in	O
the	O
top	O
panel	O
,	O
but	O
with	O
a	O
strongly	O
non-linear	B
relationship	O
between	O
x	O
and	O
y	O
.	O
predictors	O
that	O
are	O
not	O
associated	O
with	O
the	O
response	B
.	O
when	O
p	O
=	O
1	O
or	O
p	O
=	O
2	O
,	O
knn	O
outperforms	O
linear	B
regression	I
.	O
but	O
for	O
p	O
=	O
3	O
the	O
results	O
are	O
mixed	O
,	O
and	O
for	O
p	O
≥	O
4	O
linear	B
regression	I
is	O
superior	O
to	O
knn	O
.	O
in	O
fact	O
,	O
the	O
increase	O
in	O
dimension	O
has	O
only	O
caused	O
a	O
small	O
deterioration	O
in	O
the	O
linear	B
regression	I
test	O
set	B
mse	O
,	O
but	O
it	O
has	O
caused	O
more	O
than	O
a	O
ten-fold	O
increase	O
in	O
the	O
mse	O
for	O
knn	O
.	O
this	O
decrease	O
in	O
performance	O
as	O
the	O
dimension	O
increases	O
is	O
a	O
common	O
problem	O
for	O
knn	O
,	O
and	O
results	O
from	O
the	O
fact	O
that	O
in	O
higher	O
dimensions	O
there	O
is	O
eﬀectively	O
a	O
reduction	O
in	O
sample	O
size	O
.	O
in	O
this	O
data	B
set	O
there	O
are	O
100	O
training	B
observations	O
;	O
when	O
p	O
=	O
1	O
,	O
this	O
provides	O
enough	O
information	O
to	O
accurately	O
estimate	O
f	O
(	O
x	O
)	O
.	O
however	O
,	O
spreading	O
100	O
observations	B
over	O
p	O
=	O
20	O
dimensions	O
results	O
in	O
a	O
phenomenon	O
in	O
which	O
a	O
given	O
observation	O
has	O
no	O
nearby	O
neighbors—this	O
is	O
the	O
so-called	O
curse	B
of	I
dimensionality	I
.	O
that	O
is	O
,	O
the	O
k	O
observations	B
that	O
are	O
nearest	O
to	O
a	O
given	O
test	B
observation	O
x0	O
may	O
be	O
very	O
far	O
away	O
from	O
x0	O
in	O
p-dimensional	O
space	O
when	O
p	O
is	O
large	O
,	O
leading	O
to	O
a	O
curse	O
of	O
di-	O
mensionality	O
3.6	O
lab	O
:	O
linear	B
regression	I
109	O
p=1	O
p=2	O
p=3	O
p=4	O
p=10	O
p=20	O
0	O
.	O
1	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
0	O
.	O
1	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
0	O
.	O
1	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
0	O
.	O
1	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
0	O
.	O
1	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
0	O
.	O
1	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0.2	O
0.5	O
1.0	O
0.2	O
0.5	O
1.0	O
0.2	O
0.5	O
1.0	O
0.2	O
0.5	O
1.0	O
0.2	O
0.5	O
1.0	O
0.2	O
0.5	O
1.0	O
1/k	O
figure	O
3.20.	O
test	B
mse	O
for	O
linear	O
regression	B
(	O
black	O
dashed	O
lines	O
)	O
and	O
knn	O
(	O
green	O
curves	O
)	O
as	O
the	O
number	O
of	O
variables	O
p	O
increases	O
.	O
the	O
true	O
function	O
is	O
non–	O
linear	B
in	O
the	O
ﬁrst	O
variable	B
,	O
as	O
in	O
the	O
lower	O
panel	O
in	O
figure	O
3.19	O
,	O
and	O
does	O
not	O
depend	O
on	O
the	O
additional	O
variables	O
.	O
the	O
performance	O
of	O
linear	B
regression	I
deteri-	O
orates	O
slowly	O
in	O
the	O
presence	O
of	O
these	O
additional	O
noise	B
variables	O
,	O
whereas	O
knn	O
’	O
s	O
performance	O
degrades	O
much	O
more	O
quickly	O
as	O
p	O
increases	O
.	O
very	O
poor	O
prediction	B
of	O
f	O
(	O
x0	O
)	O
and	O
hence	O
a	O
poor	O
knn	O
ﬁt	B
.	O
as	O
a	O
general	O
rule	O
,	O
parametric	B
methods	O
will	O
tend	O
to	O
outperform	O
non-parametric	B
approaches	O
when	O
there	O
is	O
a	O
small	O
number	O
of	O
observations	B
per	O
predictor	B
.	O
even	O
in	O
problems	O
in	O
which	O
the	O
dimension	O
is	O
small	O
,	O
we	O
might	O
prefer	O
linear	B
regression	I
to	O
knn	O
from	O
an	O
interpretability	B
standpoint	O
.	O
if	O
the	O
test	B
mse	O
of	O
knn	O
is	O
only	O
slightly	O
lower	O
than	O
that	O
of	O
linear	B
regression	I
,	O
we	O
might	O
be	O
willing	O
to	O
forego	O
a	O
little	O
bit	O
of	O
prediction	B
accuracy	O
for	O
the	O
sake	O
of	O
a	O
simple	B
model	O
that	O
can	O
be	O
described	O
in	O
terms	O
of	O
just	O
a	O
few	O
coeﬃcients	O
,	O
and	O
for	O
which	O
p-values	O
are	O
available	O
.	O
3.6	O
lab	O
:	O
linear	B
regression	I
3.6.1	O
libraries	O
the	O
library	O
(	O
)	O
function	B
is	O
used	O
to	O
load	O
libraries	O
,	O
or	O
groups	O
of	O
functions	O
and	O
data	B
sets	O
that	O
are	O
not	O
included	O
in	O
the	O
base	O
r	O
distribution	B
.	O
basic	O
functions	O
that	O
perform	O
least	B
squares	I
linear	O
regression	B
and	O
other	O
simple	B
analyses	O
come	O
standard	O
with	O
the	O
base	O
distribution	B
,	O
but	O
more	O
exotic	O
functions	O
require	O
ad-	O
ditional	O
libraries	O
.	O
here	O
we	O
load	O
the	O
mass	O
package	O
,	O
which	O
is	O
a	O
very	O
large	O
collection	O
of	O
data	B
sets	O
and	O
functions	O
.	O
we	O
also	O
load	O
the	O
islr	O
package	O
,	O
which	O
includes	O
the	O
data	B
sets	O
associated	O
with	O
this	O
book	O
.	O
library	O
(	O
)	O
>	O
library	O
(	O
mass	O
)	O
>	O
library	O
(	O
islr	O
)	O
if	O
you	O
receive	O
an	O
error	B
message	O
when	O
loading	O
any	O
of	O
these	O
libraries	O
,	O
it	O
likely	O
indicates	O
that	O
the	O
corresponding	O
library	O
has	O
not	O
yet	O
been	O
installed	O
on	O
your	O
system	O
.	O
some	O
libraries	O
,	O
such	O
as	O
mass	O
,	O
come	O
with	O
r	O
and	O
do	O
not	O
need	O
to	O
be	O
separately	O
installed	O
on	O
your	O
computer	O
.	O
however	O
,	O
other	O
packages	O
,	O
such	O
as	O
110	O
3.	O
linear	B
regression	I
islr	O
,	O
must	O
be	O
downloaded	O
the	O
ﬁrst	O
time	O
they	O
are	O
used	O
.	O
this	O
can	O
be	O
done	O
di-	O
rectly	O
from	O
within	O
r.	O
for	O
example	O
,	O
on	O
a	O
windows	O
system	O
,	O
select	O
the	O
install	O
package	O
option	O
under	O
the	O
packages	O
tab	O
.	O
after	O
you	O
select	O
any	O
mirror	O
site	O
,	O
a	O
list	O
of	O
available	O
packages	O
will	O
appear	O
.	O
simply	O
select	O
the	O
package	O
you	O
wish	O
to	O
install	O
and	O
r	O
will	O
automatically	O
download	O
the	O
package	O
.	O
alternatively	O
,	O
this	O
can	O
be	O
done	O
at	O
the	O
r	O
command	O
line	B
via	O
install.packages	O
(	O
``	O
islr	O
''	O
)	O
.	O
this	O
in-	O
stallation	O
only	O
needs	O
to	O
be	O
done	O
the	O
ﬁrst	O
time	O
you	O
use	O
a	O
package	O
.	O
however	O
,	O
the	O
library	O
(	O
)	O
function	B
must	O
be	O
called	O
each	O
time	O
you	O
wish	O
to	O
use	O
a	O
given	O
package	O
.	O
3.6.2	O
simple	B
linear	O
regression	B
the	O
mass	O
library	O
contains	O
the	O
boston	O
data	B
set	O
,	O
which	O
records	O
medv	O
(	O
median	O
house	O
value	O
)	O
for	O
506	O
neighborhoods	O
around	O
boston	O
.	O
we	O
will	O
seek	O
to	O
predict	O
medv	O
using	O
13	O
predictors	O
such	O
as	O
rm	O
(	O
average	B
number	O
of	O
rooms	O
per	O
house	O
)	O
,	O
age	O
(	O
average	B
age	O
of	O
houses	O
)	O
,	O
and	O
lstat	O
(	O
percent	O
of	O
households	O
with	O
low	O
socioeconomic	O
status	O
)	O
.	O
>	O
fix	O
(	O
boston	O
)	O
>	O
names	O
(	O
boston	O
)	O
[	O
1	O
]	O
``	O
crim	O
``	O
[	O
8	O
]	O
``	O
dis	O
``	O
''	O
zn	O
``	O
''	O
rad	O
``	O
''	O
indus	O
``	O
''	O
tax	O
``	O
''	O
chas	O
``	O
''	O
nox	O
``	O
''	O
ptratio	O
``	O
``	O
black	O
``	O
''	O
rm	O
``	O
''	O
lstat	O
``	O
''	O
age	O
``	O
''	O
medv	O
``	O
to	O
ﬁnd	O
out	O
more	O
about	O
the	O
data	B
set	O
,	O
we	O
can	O
type	O
?	O
boston	O
.	O
we	O
will	O
start	O
by	O
using	O
the	O
lm	O
(	O
)	O
function	B
to	O
ﬁt	B
a	O
simple	B
linear	O
regression	B
model	O
,	O
with	O
medv	O
as	O
the	O
response	B
and	O
lstat	O
as	O
the	O
predictor	B
.	O
the	O
basic	O
syntax	O
is	O
lm	O
(	O
y∼x	O
,	O
data	B
)	O
,	O
where	O
y	O
is	O
the	O
response	B
,	O
x	O
is	O
the	O
predictor	B
,	O
and	O
data	B
is	O
the	O
data	B
set	O
in	O
which	O
these	O
two	O
variables	O
are	O
kept	O
.	O
>	O
lm	O
.	O
fit	O
=	O
lm	O
(	O
medv∼lstat	O
)	O
error	B
in	O
eval	O
(	O
expr	O
,	O
envir	O
,	O
enclos	O
)	O
:	O
object	O
``	O
medv	O
``	O
not	O
found	O
lm	O
(	O
)	O
the	O
command	O
causes	O
an	O
error	B
because	O
r	O
does	O
not	O
know	O
where	O
to	O
ﬁnd	O
the	O
variables	O
medv	O
and	O
lstat	O
.	O
the	O
next	O
line	B
tells	O
r	O
that	O
the	O
variables	O
are	O
in	O
boston	O
.	O
if	O
we	O
attach	O
boston	O
,	O
the	O
ﬁrst	O
line	B
works	O
ﬁne	O
because	O
r	O
now	O
recognizes	O
the	O
variables	O
.	O
>	O
lm	O
.	O
fit	O
=	O
lm	O
(	O
medv∼lstat	O
,	O
data	B
=	O
boston	O
)	O
>	O
lm	O
.	O
fit	O
=	O
lm	O
(	O
medv∼lstat	O
)	O
>	O
attach	O
(	O
boston	O
)	O
if	O
we	O
type	O
lm.fit	O
,	O
some	O
basic	O
information	O
about	O
the	O
model	B
is	O
output	B
.	O
for	O
more	O
detailed	O
information	O
,	O
we	O
use	O
summary	O
(	O
lm.fit	O
)	O
.	O
this	O
gives	O
us	O
p-	O
values	O
and	O
standard	O
errors	O
for	O
the	O
coeﬃcients	O
,	O
as	O
well	O
as	O
the	O
r2	O
statistic	O
and	O
f-statistic	O
for	O
the	O
model	B
.	O
>	O
lm	O
.	O
fit	O
call	O
:	O
lm	O
(	O
formula	O
=	O
medv	O
∼	O
lstat	O
)	O
3.6	O
lab	O
:	O
linear	B
regression	I
111	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
:	O
(	O
intercept	B
)	O
34.55	O
lstat	O
-0.95	O
>	O
summary	O
(	O
lm	O
.	O
fit	O
)	O
call	O
:	O
lm	O
(	O
formula	O
=	O
medv	O
∼	O
lstat	O
)	O
residuals	B
:	O
min	O
-15.17	O
1	O
q	O
median	O
-1.32	O
-3.99	O
3	O
q	O
2.03	O
max	O
24.50	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
:	O
estimate	O
std	O
.	O
error	B
t	O
value	O
pr	O
(	O
>	O
|	O
t	O
|	O
)	O
(	O
intercept	B
)	O
lstat	O
--	O
-	O
signif	O
.	O
codes	O
:	O
34.5538	O
-0.9500	O
0.5626	O
0.0387	O
61.4	O
-24.5	O
<	O
2e	O
-16	O
***	O
<	O
2e	O
-16	O
***	O
0	O
***	O
0.001	O
**	O
0.01	O
*	O
0.05	O
.	O
0.1	O
1	O
residual	B
standard	O
error	B
:	O
6.22	O
on	O
504	O
degrees	B
of	I
freedom	I
multiple	O
r	O
-	O
squared	O
:	O
0.544	O
,	O
f	O
-	O
statistic	O
:	O
602	O
on	O
1	O
and	O
504	O
df	O
,	O
adjusted	O
r	O
-	O
squared	O
:	O
0.543	O
p	O
-	O
value	O
:	O
<	O
2e	O
-16	O
we	O
can	O
use	O
the	O
names	O
(	O
)	O
function	B
in	O
order	O
to	O
ﬁnd	O
out	O
what	O
other	O
pieces	O
of	O
information	O
are	O
stored	O
in	O
lm.fit	O
.	O
although	O
we	O
can	O
extract	O
these	O
quan-	O
tities	O
by	O
name—e.g	O
.	O
lm.fit	O
$	O
coefficients—it	O
is	O
safer	O
to	O
use	O
the	O
extractor	O
functions	O
like	O
coef	O
(	O
)	O
to	O
access	O
them	O
.	O
names	O
(	O
)	O
coef	O
(	O
)	O
>	O
names	O
(	O
lm	O
.	O
fit	O
)	O
[	O
1	O
]	O
``	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
``	O
[	O
4	O
]	O
``	O
rank	O
``	O
[	O
7	O
]	O
``	O
qr	O
``	O
[	O
10	O
]	O
``	O
call	O
``	O
>	O
coef	O
(	O
lm	O
.	O
fit	O
)	O
(	O
intercept	B
)	O
34.55	O
lstat	O
-0.95	O
''	O
effects	O
``	O
''	O
residuals	B
``	O
''	O
fitted	O
.	O
values	O
``	O
``	O
assign	O
``	O
''	O
df	O
.	O
residual	B
``	O
''	O
terms	O
``	O
''	O
xlevels	O
``	O
''	O
model	B
``	O
in	O
order	O
to	O
obtain	O
a	O
conﬁdence	B
interval	I
for	O
the	O
coeﬃcient	B
estimates	O
,	O
we	O
can	O
use	O
the	O
confint	O
(	O
)	O
command	O
.	O
confint	O
(	O
)	O
>	O
confint	O
(	O
lm	O
.	O
fit	O
)	O
2.5	O
%	O
97.5	O
%	O
(	O
intercept	B
)	O
33.45	O
35.659	O
lstat	O
-1.03	O
-0.874	O
the	O
predict	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
produce	O
conﬁdence	O
intervals	O
and	O
prediction	B
intervals	O
for	O
the	O
prediction	B
of	O
medv	O
for	O
a	O
given	O
value	O
of	O
lstat	O
.	O
predict	O
(	O
)	O
>	O
predict	O
(	O
lm	O
.	O
fit	O
,	O
data	B
.	O
frame	O
(	O
lstat	O
=	O
c	O
(	O
5	O
,10	O
,15	O
)	O
)	O
,	O
interval	B
=	O
''	O
confidenc	O
e	O
``	O
)	O
lwr	O
fit	O
upr	O
1	O
29.80	O
29.01	O
30.60	O
2	O
25.05	O
24.47	O
25.63	O
3	O
20.30	O
19.73	O
20.87	O
112	O
3.	O
linear	B
regression	I
>	O
predict	O
(	O
lm	O
.	O
fit	O
,	O
data	B
.	O
frame	O
(	O
lstat	O
=	O
c	O
(	O
5	O
,10	O
,15	O
)	O
)	O
,	O
interval	B
=	O
''	O
predictio	O
n	O
``	O
)	O
lwr	O
fit	O
upr	O
1	O
29.80	O
17.566	O
42.04	O
2	O
25.05	O
12.828	O
37.28	O
8.078	O
32.53	O
3	O
20.30	O
for	O
instance	O
,	O
the	O
95	O
%	O
conﬁdence	B
interval	I
associated	O
with	O
a	O
lstat	O
value	O
of	O
10	O
is	O
(	O
24.47	O
,	O
25.63	O
)	O
,	O
and	O
the	O
95	O
%	O
prediction	B
interval	O
is	O
(	O
12.828	O
,	O
37.28	O
)	O
.	O
as	O
expected	O
,	O
the	O
conﬁdence	O
and	O
prediction	B
intervals	O
are	O
centered	O
around	O
the	O
same	O
point	O
(	O
a	O
predicted	O
value	O
of	O
25.05	O
for	O
medv	O
when	O
lstat	O
equals	O
10	O
)	O
,	O
but	O
the	O
latter	O
are	O
substantially	O
wider	O
.	O
we	O
will	O
now	O
plot	B
medv	O
and	O
lstat	O
along	O
with	O
the	O
least	B
squares	I
regression	O
line	B
using	O
the	O
plot	B
(	O
)	O
and	O
abline	O
(	O
)	O
functions	O
.	O
abline	O
(	O
)	O
>	O
plot	B
(	O
lstat	O
,	O
medv	O
)	O
>	O
abline	O
(	O
lm	O
.	O
fit	O
)	O
there	O
is	O
some	O
evidence	O
for	O
non-linearity	O
in	O
the	O
relationship	O
between	O
lstat	O
and	O
medv	O
.	O
we	O
will	O
explore	O
this	O
issue	O
later	O
in	O
this	O
lab	O
.	O
the	O
abline	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
draw	O
any	O
line	B
,	O
not	O
just	O
the	O
least	B
squares	I
regression	O
line	B
.	O
to	O
draw	O
a	O
line	B
with	O
intercept	B
a	O
and	O
slope	B
b	O
,	O
we	O
type	O
abline	O
(	O
a	O
,	O
b	O
)	O
.	O
below	O
we	O
experiment	O
with	O
some	O
additional	O
settings	O
for	O
plotting	O
lines	O
and	O
points	O
.	O
the	O
lwd=3	O
command	O
causes	O
the	O
width	O
of	O
the	O
regression	B
line	O
to	O
be	O
increased	O
by	O
a	O
factor	B
of	O
3	O
;	O
this	O
works	O
for	O
the	O
plot	B
(	O
)	O
and	O
lines	O
(	O
)	O
functions	O
also	O
.	O
we	O
can	O
also	O
use	O
the	O
pch	O
option	O
to	O
create	O
diﬀerent	O
plotting	O
symbols	O
.	O
>	O
abline	O
(	O
lm	O
.	O
fit	O
,	O
lwd	O
=3	O
)	O
>	O
abline	O
(	O
lm	O
.	O
fit	O
,	O
lwd	O
=3	O
,	O
col	O
=	O
''	O
red	O
``	O
)	O
>	O
plot	B
(	O
lstat	O
,	O
medv	O
,	O
col	O
=	O
''	O
red	O
``	O
)	O
>	O
plot	B
(	O
lstat	O
,	O
medv	O
,	O
pch	O
=20	O
)	O
>	O
plot	B
(	O
lstat	O
,	O
medv	O
,	O
pch	O
=	O
''	O
+	O
''	O
)	O
>	O
plot	B
(	O
1:20	O
,1:20	O
,	O
pch	O
=1:20	O
)	O
next	O
we	O
examine	O
some	O
diagnostic	O
plots	O
,	O
several	O
of	O
which	O
were	O
discussed	O
in	O
section	O
3.3.3.	O
four	O
diagnostic	O
plots	O
are	O
automatically	O
produced	O
by	O
ap-	O
plying	O
the	O
plot	B
(	O
)	O
function	B
directly	O
to	O
the	O
output	B
from	O
lm	O
(	O
)	O
.	O
in	O
general	O
,	O
this	O
command	O
will	O
produce	O
one	O
plot	B
at	O
a	O
time	O
,	O
and	O
hitting	O
enter	O
will	O
generate	O
the	O
next	O
plot	B
.	O
however	O
,	O
it	O
is	O
often	O
convenient	O
to	O
view	O
all	O
four	O
plots	O
together	O
.	O
we	O
can	O
achieve	O
this	O
by	O
using	O
the	O
par	O
(	O
)	O
function	B
,	O
which	O
tells	O
r	O
to	O
split	O
the	O
display	O
screen	O
into	O
separate	O
panels	O
so	O
that	O
multiple	B
plots	O
can	O
be	O
viewed	O
si-	O
multaneously	O
.	O
for	O
example	O
,	O
par	O
(	O
mfrow=c	O
(	O
2,2	O
)	O
)	O
divides	O
the	O
plotting	O
region	O
into	O
a	O
2	O
×	O
2	O
grid	O
of	O
panels	O
.	O
par	O
(	O
)	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
2	O
,2	O
)	O
)	O
>	O
plot	B
(	O
lm	O
.	O
fit	O
)	O
alternatively	O
,	O
we	O
can	O
compute	O
the	O
residuals	B
from	O
a	O
linear	B
regression	I
ﬁt	O
using	O
the	O
residuals	B
(	O
)	O
function	B
.	O
the	O
function	B
rstudent	O
(	O
)	O
will	O
return	O
the	O
studentized	B
residuals	O
,	O
and	O
we	O
can	O
use	O
this	O
function	B
to	O
plot	B
the	O
residuals	B
against	O
the	O
ﬁtted	O
values	O
.	O
residuals	B
(	O
)	O
rstudent	O
(	O
)	O
3.6	O
lab	O
:	O
linear	B
regression	I
113	O
>	O
plot	B
(	O
predict	O
(	O
lm	O
.	O
fit	O
)	O
,	O
residuals	B
(	O
lm	O
.	O
fit	O
)	O
)	O
>	O
plot	B
(	O
predict	O
(	O
lm	O
.	O
fit	O
)	O
,	O
rstudent	O
(	O
lm	O
.	O
fit	O
)	O
)	O
on	O
the	O
basis	B
of	O
the	O
residual	B
plots	O
,	O
there	O
is	O
some	O
evidence	O
of	O
non-linearity	O
.	O
leverage	B
statistics	O
can	O
be	O
computed	O
for	O
any	O
number	O
of	O
predictors	O
using	O
the	O
hatvalues	O
(	O
)	O
function	B
.	O
hatvalues	O
(	O
)	O
>	O
plot	B
(	O
hatvalues	O
(	O
lm	O
.	O
fit	O
)	O
)	O
>	O
which	O
.	O
max	O
(	O
hatvalues	O
(	O
lm	O
.	O
fit	O
)	O
)	O
375	O
the	O
which.max	O
(	O
)	O
function	B
identiﬁes	O
the	O
index	O
of	O
the	O
largest	O
element	O
of	O
a	O
vector	B
.	O
in	O
this	O
case	O
,	O
it	O
tells	O
us	O
which	O
observation	O
has	O
the	O
largest	O
leverage	B
statistic	O
.	O
which.max	O
(	O
)	O
3.6.3	O
multiple	B
linear	O
regression	B
in	O
order	O
to	O
ﬁt	B
a	O
multiple	B
linear	O
regression	B
model	O
using	O
least	B
squares	I
,	O
we	O
again	O
use	O
the	O
lm	O
(	O
)	O
function	B
.	O
the	O
syntax	O
lm	O
(	O
y∼x1+x2+x3	O
)	O
is	O
used	O
to	O
ﬁt	B
a	O
model	B
with	O
three	O
predictors	O
,	O
x1	O
,	O
x2	B
,	O
and	O
x3	O
.	O
the	O
summary	O
(	O
)	O
function	B
now	O
outputs	O
the	O
regression	B
coeﬃcients	O
for	O
all	O
the	O
predictors	O
.	O
>	O
lm	O
.	O
fit	O
=	O
lm	O
(	O
medv∼lstat	O
+	O
age	O
,	O
data	B
=	O
boston	O
)	O
>	O
summary	O
(	O
lm	O
.	O
fit	O
)	O
call	O
:	O
lm	O
(	O
formula	O
=	O
medv	O
∼	O
lstat	O
+	O
age	O
,	O
data	B
=	O
boston	O
)	O
residuals	B
:	O
min	O
-15.98	O
1	O
q	O
median	O
-1.28	O
-3.98	O
3	O
q	O
1.97	O
max	O
23.16	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
:	O
estimate	O
std	O
.	O
error	B
t	O
value	O
pr	O
(	O
>	O
|	O
t	O
|	O
)	O
(	O
intercept	B
)	O
lstat	O
age	O
--	O
-	O
signif	O
.	O
codes	O
:	O
33.2228	O
-1.0321	O
0.0345	O
0.7308	O
0.0482	O
0.0122	O
45.46	O
-21.42	O
2.83	O
<	O
2e	O
-16	O
***	O
<	O
2e	O
-16	O
***	O
0.0049	O
**	O
0	O
***	O
0.001	O
**	O
0.01	O
*	O
0.05	O
.	O
0.1	O
1	O
residual	B
standard	O
error	B
:	O
6.17	O
on	O
503	O
degrees	B
of	I
freedom	I
multiple	O
r	O
-	O
squared	O
:	O
0.551	O
,	O
f	O
-	O
statistic	O
:	O
309	O
on	O
2	O
and	O
503	O
df	O
,	O
adjusted	O
r	O
-	O
squared	O
:	O
0.549	O
p	O
-	O
value	O
:	O
<	O
2e	O
-16	O
the	O
boston	O
data	B
set	O
contains	O
13	O
variables	O
,	O
and	O
so	O
it	O
would	O
be	O
cumbersome	O
to	O
have	O
to	O
type	O
all	O
of	O
these	O
in	O
order	O
to	O
perform	O
a	O
regression	B
using	O
all	O
of	O
the	O
predictors	O
.	O
instead	O
,	O
we	O
can	O
use	O
the	O
following	O
short-hand	O
:	O
>	O
lm	O
.	O
fit	O
=	O
lm	O
(	O
medv∼	O
.	O
,	O
data	B
=	O
boston	O
)	O
>	O
summary	O
(	O
lm	O
.	O
fit	O
)	O
call	O
:	O
lm	O
(	O
formula	O
=	O
medv	O
∼	O
.	O
,	O
data	B
=	O
boston	O
)	O
114	O
3.	O
linear	B
regression	I
residuals	O
:	O
min	O
-15.594	O
1	O
q	O
-2.730	O
median	O
-0.518	O
3	O
q	O
1.777	O
max	O
26.199	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
:	O
estimate	O
std	O
.	O
error	B
t	O
value	O
pr	O
(	O
>	O
|	O
t	O
|	O
)	O
(	O
intercept	B
)	O
crim	O
zn	O
indus	O
chas	O
nox	O
rm	O
age	O
dis	O
rad	O
tax	O
ptratio	O
black	O
lstat	O
--	O
-	O
signif	O
.	O
codes	O
:	O
3.646	O
e	O
+01	O
-1.080	O
e	O
-01	O
4.642	O
e	O
-02	O
2.056	O
e	O
-02	O
2.687	O
e	O
+00	O
-1.777	O
e	O
+01	O
3.810	O
e	O
+00	O
6.922	O
e	O
-04	O
-1.476	O
e	O
+00	O
3.060	O
e	O
-01	O
-1.233	O
e	O
-02	O
-9.527	O
e	O
-01	O
9.312	O
e	O
-03	O
-5.248	O
e	O
-01	O
7.144	O
3.28	O
e	O
-12	O
***	O
-3.287	O
0.001087	O
**	O
3.382	O
0.000778	O
***	O
0.334	O
0.738288	O
3.118	O
0.001925	O
**	O
5.103	O
e	O
+00	O
3.286	O
e	O
-02	O
1.373	O
e	O
-02	O
6.150	O
e	O
-02	O
8.616	O
e	O
-01	O
3.820	O
e	O
+00	O
4.179	O
e	O
-01	O
1.321	O
e	O
-02	O
1.995	O
e	O
-01	O
6.635	O
e	O
-02	O
3.761	O
e	O
-03	O
1.308	O
e	O
-01	O
2.686	O
e	O
-03	O
5.072	O
e	O
-02	O
-10.347	O
-4.651	O
4.25	O
e	O
-06	O
***	O
<	O
2e	O
-16	O
***	O
9.116	O
0.052	O
0.958229	O
-7.398	O
6.01	O
e	O
-13	O
***	O
4.613	O
5.07	O
e	O
-06	O
***	O
-3.280	O
0.001112	O
**	O
-7.283	O
1.31	O
e	O
-12	O
***	O
3.467	O
0.000573	O
***	O
<	O
2e	O
-16	O
***	O
0	O
‘	O
***	O
’	O
0.001	O
‘	O
**	O
’	O
0.01	O
‘	O
*	O
’	O
0.05	O
‘	O
.	O
’	O
0.1	O
‘	O
’	O
1	O
residual	B
standard	O
error	B
:	O
4.745	O
on	O
492	O
degrees	B
of	I
freedom	I
multiple	O
r	O
-	O
squared	O
:	O
0.7406	O
,	O
f	O
-	O
statistic	O
:	O
108.1	O
on	O
13	O
and	O
492	O
df	O
,	O
adjusted	O
r	O
-	O
squared	O
:	O
0.7338	O
p	O
-	O
value	O
:	O
<	O
2.2	O
e	O
-16	O
we	O
can	O
access	O
the	O
individual	O
components	O
of	O
a	O
summary	O
object	O
by	O
name	O
(	O
type	O
?	O
summary.lm	O
to	O
see	O
what	O
is	O
available	O
)	O
.	O
hence	O
summary	O
(	O
lm.fit	O
)	O
$	O
r.sq	O
gives	O
us	O
the	O
r2	O
,	O
and	O
summary	O
(	O
lm.fit	O
)	O
$	O
sigma	O
gives	O
us	O
the	O
rse	O
.	O
the	O
vif	O
(	O
)	O
function	B
,	O
part	O
of	O
the	O
car	O
package	O
,	O
can	O
be	O
used	O
to	O
compute	O
variance	B
inﬂation	O
factors	O
.	O
most	O
vif	O
’	O
s	O
are	O
low	O
to	O
moderate	O
for	O
this	O
data	B
.	O
the	O
car	O
package	O
is	O
not	O
part	O
of	O
the	O
base	O
r	O
installation	O
so	O
it	O
must	O
be	O
downloaded	O
the	O
ﬁrst	O
time	O
you	O
use	O
it	O
via	O
the	O
install.packages	O
option	O
in	O
r.	O
vif	O
(	O
)	O
>	O
library	O
(	O
car	O
)	O
>	O
vif	O
(	O
lm	O
.	O
fit	O
)	O
crim	O
1.79	O
dis	O
3.96	O
zn	O
2.30	O
rad	O
7.48	O
indus	O
3.99	O
chas	O
1.07	O
tax	O
ptratio	O
1.80	O
9.01	O
nox	O
4.39	O
black	O
1.35	O
rm	O
1.93	O
lstat	O
2.94	O
age	O
3.10	O
what	O
if	O
we	O
would	O
like	O
to	O
perform	O
a	O
regression	B
using	O
all	O
of	O
the	O
variables	O
but	O
one	O
?	O
for	O
example	O
,	O
in	O
the	O
above	O
regression	B
output	O
,	O
age	O
has	O
a	O
high	O
p-value	B
.	O
so	O
we	O
may	O
wish	O
to	O
run	O
a	O
regression	B
excluding	O
this	O
predictor	B
.	O
the	O
following	O
syntax	O
results	O
in	O
a	O
regression	B
using	O
all	O
predictors	O
except	O
age	O
.	O
>	O
lm	O
.	O
fit1	O
=	O
lm	O
(	O
medv∼	O
.	O
-	O
age	O
,	O
data	B
=	O
boston	O
)	O
>	O
summary	O
(	O
lm	O
.	O
fit1	O
)	O
...	O
alternatively	O
,	O
the	O
update	O
(	O
)	O
function	B
can	O
be	O
used	O
.	O
update	O
(	O
)	O
3.6	O
lab	O
:	O
linear	B
regression	I
115	O
>	O
lm	O
.	O
fit1	O
=	O
update	O
(	O
lm	O
.	O
fit	O
,	O
∼	O
.	O
-	O
age	O
)	O
3.6.4	O
interaction	B
terms	O
it	O
is	O
easy	O
to	O
include	O
interaction	B
terms	O
in	O
a	O
linear	B
model	I
using	O
the	O
lm	O
(	O
)	O
func-	O
tion	O
.	O
the	O
syntax	O
lstat	O
:	O
black	O
tells	O
r	O
to	O
include	O
an	O
interaction	B
term	O
between	O
lstat	O
and	O
black	O
.	O
the	O
syntax	O
lstat*age	O
simultaneously	O
includes	O
lstat	O
,	O
age	O
,	O
and	O
the	O
interaction	B
term	O
lstat×age	O
as	O
predictors	O
;	O
it	O
is	O
a	O
shorthand	O
for	O
lstat+age+lstat	O
:	O
age	O
.	O
>	O
summary	O
(	O
lm	O
(	O
medv∼lstat	O
*	O
age	O
,	O
data	B
=	O
boston	O
)	O
)	O
call	O
:	O
lm	O
(	O
formula	O
=	O
medv	O
∼	O
lstat	O
*	O
age	O
,	O
data	B
=	O
boston	O
)	O
residuals	B
:	O
min	O
-15.81	O
1	O
q	O
median	O
-1.33	O
-4.04	O
3	O
q	O
2.08	O
max	O
27.55	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
:	O
estimate	O
std	O
.	O
error	B
t	O
value	O
pr	O
(	O
>	O
|	O
t	O
|	O
)	O
(	O
intercept	B
)	O
36.088536	O
lstat	O
-1.392117	O
-0.000721	O
age	O
lstat	O
:	O
age	O
0.004156	O
--	O
-	O
signif	O
.	O
codes	O
:	O
1.469835	O
0.167456	O
0.019879	O
0.001852	O
24.55	O
-8.31	O
-0.04	O
2.24	O
<	O
2e	O
-16	O
***	O
8.8	O
e	O
-16	O
***	O
0.971	O
0.025	O
*	O
0	O
’	O
***	O
’	O
0.001	O
’	O
**	O
’	O
0.01	O
’	O
*	O
’	O
0.05	O
’	O
.	O
’	O
0.1	O
’	O
’	O
1	O
residual	B
standard	O
error	B
:	O
6.15	O
on	O
502	O
degrees	B
of	I
freedom	I
multiple	O
r	O
-	O
squared	O
:	O
0.556	O
,	O
f	O
-	O
statistic	O
:	O
adjusted	O
r	O
-	O
squared	O
:	O
0.553	O
209	O
on	O
3	O
and	O
502	O
df	O
,	O
p	O
-	O
value	O
:	O
<	O
2e	O
-16	O
3.6.5	O
non-linear	B
transformations	O
of	O
the	O
predictors	O
the	O
lm	O
(	O
)	O
function	B
can	O
also	O
accommodate	O
non-linear	B
transformations	O
of	O
the	O
predictors	O
.	O
for	O
instance	O
,	O
given	O
a	O
predictor	B
x	O
,	O
we	O
can	O
create	O
a	O
predictor	B
x	O
2	O
using	O
i	O
(	O
x^2	O
)	O
.	O
the	O
function	B
i	O
(	O
)	O
is	O
needed	O
since	O
the	O
^	O
has	O
a	O
special	O
meaning	O
in	O
a	O
formula	O
;	O
wrapping	O
as	O
we	O
do	O
allows	O
the	O
standard	O
usage	O
in	O
r	O
,	O
which	O
is	O
to	O
raise	O
x	O
to	O
the	O
power	B
2.	O
we	O
now	O
perform	O
a	O
regression	B
of	O
medv	O
onto	O
lstat	O
and	O
lstat	O
>	O
lm	O
.	O
fit2	O
=	O
lm	O
(	O
medv∼lstat	O
+	O
i	O
(	O
lstat	O
^2	O
)	O
)	O
2.	O
i	O
(	O
)	O
>	O
summary	O
(	O
lm	O
.	O
fit2	O
)	O
call	O
:	O
lm	O
(	O
formula	O
=	O
medv	O
∼	O
lstat	O
+	O
i	O
(	O
lstat	O
^2	O
)	O
)	O
residuals	B
:	O
min	O
-15.28	O
1	O
q	O
median	O
-0.53	O
-3.83	O
3	O
q	O
2.31	O
max	O
25.41	O
116	O
3.	O
linear	B
regression	I
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
:	O
estimate	O
std	O
.	O
error	B
t	O
value	O
pr	O
(	O
>	O
|	O
t	O
|	O
)	O
(	O
intercept	B
)	O
42.86201	O
lstat	O
-2.33282	O
i	O
(	O
lstat	O
^2	O
)	O
0.04355	O
--	O
-	O
signif	O
.	O
codes	O
:	O
0.87208	O
0.12380	O
0.00375	O
49.1	O
-18.8	O
11.6	O
<	O
2e	O
-16	O
***	O
<	O
2e	O
-16	O
***	O
<	O
2e	O
-16	O
***	O
0	O
’	O
***	O
’	O
0.001	O
’	O
**	O
’	O
0.01	O
’	O
*	O
’	O
0.05	O
’	O
.	O
’	O
0.1	O
’	O
’	O
1	O
residual	B
standard	O
error	B
:	O
5.52	O
on	O
503	O
degrees	B
of	I
freedom	I
multiple	O
r	O
-	O
squared	O
:	O
0.641	O
,	O
f	O
-	O
statistic	O
:	O
adjusted	O
r	O
-	O
squared	O
:	O
0.639	O
449	O
on	O
2	O
and	O
503	O
df	O
,	O
p	O
-	O
value	O
:	O
<	O
2e	O
-16	O
the	O
near-zero	O
p-value	B
associated	O
with	O
the	O
quadratic	B
term	O
suggests	O
that	O
it	O
leads	O
to	O
an	O
improved	O
model	B
.	O
we	O
use	O
the	O
anova	O
(	O
)	O
function	B
to	O
further	O
quantify	O
the	O
extent	O
to	O
which	O
the	O
quadratic	B
ﬁt	O
is	O
superior	O
to	O
the	O
linear	B
ﬁt	O
.	O
>	O
lm	O
.	O
fit	O
=	O
lm	O
(	O
medv∼lstat	O
)	O
anova	O
(	O
)	O
>	O
anova	O
(	O
lm	O
.	O
fit	O
,	O
lm	O
.	O
fit2	O
)	O
analysis	B
of	I
variance	I
table	O
model	B
1	O
:	O
medv	O
∼	O
lstat	O
model	B
2	O
:	O
medv	O
∼	O
lstat	O
+	O
i	O
(	O
lstat	O
^2	O
)	O
res	O
.	O
df	O
rss	O
df	O
sum	O
of	O
sq	O
f	O
pr	O
(	O
>	O
f	O
)	O
504	O
19472	O
503	O
15347	O
1	O
2	O
--	O
-	O
signif	O
.	O
codes	O
:	O
1	O
4125	O
135	O
<	O
2e	O
-16	O
***	O
0	O
’	O
***	O
’	O
0.001	O
’	O
**	O
’	O
0.01	O
’	O
*	O
’	O
0.05	O
’	O
.	O
’	O
0.1	O
’	O
’	O
1	O
here	O
model	B
1	O
represents	O
the	O
linear	B
submodel	O
containing	O
only	O
one	O
predictor	B
,	O
lstat	O
,	O
while	O
model	B
2	O
corresponds	O
to	O
the	O
larger	O
quadratic	B
model	O
that	O
has	O
two	O
2.	O
the	O
anova	O
(	O
)	O
function	B
performs	O
a	O
hypothesis	B
predictors	O
,	O
lstat	O
and	O
lstat	O
test	B
comparing	O
the	O
two	O
models	O
.	O
the	O
null	B
hypothesis	O
is	O
that	O
the	O
two	O
models	O
ﬁt	B
the	O
data	B
equally	O
well	O
,	O
and	O
the	O
alternative	B
hypothesis	I
is	O
that	O
the	O
full	O
model	B
is	O
superior	O
.	O
here	O
the	O
f-statistic	O
is	O
135	O
and	O
the	O
associated	O
p-value	B
is	O
virtually	O
zero	O
.	O
this	O
provides	O
very	O
clear	O
evidence	O
that	O
the	O
model	B
containing	O
2	O
is	O
far	O
superior	O
to	O
the	O
model	B
that	O
only	O
the	O
predictors	O
lstat	O
and	O
lstat	O
contains	O
the	O
predictor	B
lstat	O
.	O
this	O
is	O
not	O
surprising	O
,	O
since	O
earlier	O
we	O
saw	O
evidence	O
for	O
non-linearity	O
in	O
the	O
relationship	O
between	O
medv	O
and	O
lstat	O
.	O
if	O
we	O
type	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
2	O
,2	O
)	O
)	O
>	O
plot	B
(	O
lm	O
.	O
fit2	O
)	O
then	O
we	O
see	O
that	O
when	O
the	O
lstat	O
little	O
discernible	O
pattern	O
in	O
the	O
residuals	B
.	O
2	O
term	B
is	O
included	O
in	O
the	O
model	B
,	O
there	O
is	O
in	O
order	O
to	O
create	O
a	O
cubic	B
ﬁt	O
,	O
we	O
can	O
include	O
a	O
predictor	B
of	O
the	O
form	O
i	O
(	O
x^3	O
)	O
.	O
however	O
,	O
this	O
approach	B
can	O
start	O
to	O
get	O
cumbersome	O
for	O
higher-	O
order	O
polynomials	O
.	O
a	O
better	O
approach	B
involves	O
using	O
the	O
poly	O
(	O
)	O
function	B
to	O
create	O
the	O
polynomial	B
within	O
lm	O
(	O
)	O
.	O
for	O
example	O
,	O
the	O
following	O
command	O
produces	O
a	O
ﬁfth-order	O
polynomial	B
ﬁt	O
:	O
poly	O
(	O
)	O
3.6	O
lab	O
:	O
linear	B
regression	I
117	O
>	O
lm	O
.	O
fit5	O
=	O
lm	O
(	O
medv∼poly	O
(	O
lstat	O
,5	O
)	O
)	O
>	O
summary	O
(	O
lm	O
.	O
fit5	O
)	O
call	O
:	O
lm	O
(	O
formula	O
=	O
medv	O
∼	O
poly	O
(	O
lstat	O
,	O
5	O
)	O
)	O
residuals	B
:	O
min	O
-13.543	O
1	O
q	O
-3.104	O
median	O
-0.705	O
3	O
q	O
2.084	O
max	O
27.115	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
:	O
estimate	O
std	O
.	O
error	B
t	O
value	O
pr	O
(	O
>	O
|	O
t	O
|	O
)	O
(	O
intercept	B
)	O
22.533	O
poly	O
(	O
lstat	O
,	O
5	O
)	O
1	O
-152.460	O
poly	O
(	O
lstat	O
,	O
5	O
)	O
2	O
64.227	O
-27.051	O
poly	O
(	O
lstat	O
,	O
5	O
)	O
3	O
25.452	O
poly	O
(	O
lstat	O
,	O
5	O
)	O
4	O
poly	O
(	O
lstat	O
,	O
5	O
)	O
5	O
-19.252	O
--	O
-	O
signif	O
.	O
codes	O
:	O
0.232	O
5.215	O
5.215	O
5.215	O
5.215	O
5.215	O
97.20	O
-29.24	O
12.32	O
-5.19	O
4.88	O
-3.69	O
<	O
2e	O
-16	O
***	O
<	O
2e	O
-16	O
***	O
<	O
2e	O
-16	O
***	O
3.1	O
e	O
-07	O
***	O
1.4	O
e	O
-06	O
***	O
0.00025	O
***	O
0	O
’	O
***	O
’	O
0.001	O
’	O
**	O
’	O
0.01	O
’	O
*	O
’	O
0.05	O
’	O
.	O
’	O
0.1	O
’	O
’	O
1	O
residual	B
standard	O
error	B
:	O
5.21	O
on	O
500	O
degrees	B
of	I
freedom	I
multiple	O
r	O
-	O
squared	O
:	O
0.682	O
,	O
f	O
-	O
statistic	O
:	O
adjusted	O
r	O
-	O
squared	O
:	O
0.679	O
214	O
on	O
5	O
and	O
500	O
df	O
,	O
p	O
-	O
value	O
:	O
<	O
2e	O
-16	O
this	O
suggests	O
that	O
including	O
additional	O
polynomial	B
terms	O
,	O
up	O
to	O
ﬁfth	O
order	O
,	O
leads	O
to	O
an	O
improvement	O
in	O
the	O
model	B
ﬁt	O
!	O
however	O
,	O
further	O
investigation	O
of	O
the	O
data	B
reveals	O
that	O
no	O
polynomial	B
terms	O
beyond	O
ﬁfth	O
order	O
have	O
signiﬁ-	O
cant	O
p-values	O
in	O
a	O
regression	B
ﬁt	O
.	O
of	O
course	O
,	O
we	O
are	O
in	O
no	O
way	O
restricted	O
to	O
using	O
polynomial	B
transforma-	O
tions	O
of	O
the	O
predictors	O
.	O
here	O
we	O
try	O
a	O
log	O
transformation	O
.	O
>	O
summary	O
(	O
lm	O
(	O
medv∼log	O
(	O
rm	O
)	O
,	O
data	B
=	O
boston	O
)	O
)	O
...	O
3.6.6	O
qualitative	B
predictors	O
we	O
will	O
now	O
examine	O
the	O
carseats	O
data	B
,	O
which	O
is	O
part	O
of	O
the	O
islr	O
library	O
.	O
we	O
will	O
attempt	O
to	O
predict	O
sales	O
(	O
child	O
car	O
seat	O
sales	O
)	O
in	O
400	O
locations	O
based	O
on	O
a	O
number	O
of	O
predictors	O
.	O
>	O
fix	O
(	O
carseats	O
)	O
>	O
names	O
(	O
carseats	O
)	O
[	O
1	O
]	O
``	O
sales	O
``	O
[	O
5	O
]	O
``	O
population	O
``	O
[	O
9	O
]	O
``	O
education	O
``	O
''	O
compprice	O
``	O
''	O
price	O
``	O
''	O
urban	O
``	O
''	O
income	O
``	O
''	O
shelveloc	O
``	O
''	O
us	O
``	O
''	O
advertisi	O
n	O
g	O
``	O
''	O
age	O
``	O
the	O
carseats	O
data	B
includes	O
qualitative	B
predictors	O
such	O
as	O
shelveloc	O
,	O
an	O
in-	O
dicator	O
of	O
the	O
quality	O
of	O
the	O
shelving	O
location—that	O
is	O
,	O
the	O
space	O
within	O
a	O
store	O
in	O
which	O
the	O
car	O
seat	O
is	O
displayed—at	O
each	O
location	O
.	O
the	O
pre-	O
dictor	O
shelveloc	O
takes	O
on	O
three	O
possible	O
values	O
,	O
bad	O
,	O
medium	O
,	O
and	O
good	O
.	O
118	O
3.	O
linear	B
regression	I
given	O
a	O
qualitative	B
variable	O
such	O
as	O
shelveloc	O
,	O
r	O
generates	O
dummy	B
variables	O
automatically	O
.	O
below	O
we	O
ﬁt	B
a	O
multiple	B
regression	O
model	B
that	O
includes	O
some	O
interaction	B
terms	O
.	O
>	O
lm	O
.	O
fit	O
=	O
lm	O
(	O
sales∼.+	O
income	O
:	O
advertisi	O
ng	O
+	O
price	O
:	O
age	O
,	O
data	B
=	O
carseats	O
)	O
>	O
summary	O
(	O
lm	O
.	O
fit	O
)	O
call	O
:	O
lm	O
(	O
formula	O
=	O
sales	O
∼	O
.	O
+	O
income	O
:	O
advertisin	O
g	O
+	O
price	O
:	O
age	O
,	O
data	B
=	O
carseats	O
)	O
residuals	B
:	O
min	O
-2.921	O
-0.750	O
1	O
q	O
median	O
0.018	O
3	O
q	O
0.675	O
max	O
3.341	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
:	O
(	O
intercept	B
)	O
compprice	O
income	O
advertis	O
i	O
ng	O
populatio	O
n	O
price	O
s	O
h	O
e	O
l	O
v	O
e	O
l	O
o	O
c	O
g	O
o	O
o	O
d	O
s	O
h	O
e	O
l	O
v	O
e	O
l	O
o	O
c	O
m	O
e	O
d	O
i	O
u	O
m	O
age	O
education	O
urbanyes	O
usyes	O
income	O
:	O
advertisi	O
n	O
g	O
price	O
:	O
age	O
--	O
-	O
signif	O
.	O
codes	O
:	O
estimate	O
std	O
.	O
error	B
t	O
value	O
pr	O
(	O
>	O
|	O
t	O
|	O
)	O
6.575565	O
0.092937	O
0.010894	O
0.070246	O
0.000159	O
-0.100806	O
4.848676	O
1.953262	O
-0.057947	O
-0.020852	O
0.140160	O
-0.157557	O
0.000751	O
0.000107	O
1.008747	O
0.004118	O
0.002604	O
0.022609	O
0.000368	O
0.007440	O
0.152838	O
0.125768	O
0.015951	O
0.019613	O
0.112402	O
0.148923	O
0.000278	O
0.000133	O
2.2	O
e	O
-10	O
***	O
<	O
2e	O
-16	O
***	O
3.6	O
e	O
-05	O
***	O
0.00203	O
**	O
0.66533	O
<	O
2e	O
-16	O
***	O
<	O
2e	O
-16	O
***	O
<	O
2e	O
-16	O
***	O
0.00032	O
***	O
0.28836	O
0.21317	O
0.29073	O
0.00729	O
**	O
0.42381	O
6.52	O
22.57	O
4.18	O
3.11	O
0.43	O
-13.55	O
31.72	O
15.53	O
-3.63	O
-1.06	O
1.25	O
-1.06	O
2.70	O
0.80	O
0	O
’	O
***	O
’	O
0.001	O
’	O
**	O
’	O
0.01	O
’	O
*	O
’	O
0.05	O
’	O
.	O
’	O
0.1	O
’	O
’	O
1	O
residual	B
standard	O
error	B
:	O
1.01	O
on	O
386	O
degrees	B
of	I
freedom	I
multiple	O
r	O
-	O
squared	O
:	O
0.876	O
,	O
f	O
-	O
statistic	O
:	O
210	O
on	O
13	O
and	O
386	O
df	O
,	O
adjusted	O
r	O
-	O
squared	O
:	O
0.872	O
p	O
-	O
value	O
:	O
<	O
2e	O
-16	O
the	O
contrasts	O
(	O
)	O
function	B
returns	O
the	O
coding	O
that	O
r	O
uses	O
for	O
the	O
dummy	B
variables	O
.	O
contrasts	O
(	O
)	O
>	O
attach	O
(	O
carseats	O
)	O
>	O
contrasts	O
(	O
shelveloc	O
)	O
bad	O
good	O
medium	O
good	O
medium	O
0	O
0	O
1	O
0	O
1	O
0	O
use	O
?	O
contrasts	O
to	O
learn	O
about	O
other	O
contrasts	O
,	O
and	O
how	O
to	O
set	B
them	O
.	O
r	O
has	O
created	O
a	O
shelvelocgood	O
dummy	B
variable	I
that	O
takes	O
on	O
a	O
value	O
of	O
1	O
if	O
the	O
shelving	O
location	O
is	O
good	O
,	O
and	O
0	O
otherwise	O
.	O
it	O
has	O
also	O
created	O
a	O
shelvelocmedium	O
dummy	B
variable	I
that	O
equals	O
1	O
if	O
the	O
shelving	O
location	O
is	O
medium	O
,	O
and	O
0	O
otherwise	O
.	O
a	O
bad	O
shelving	O
location	O
corresponds	O
to	O
a	O
zero	O
for	O
each	O
of	O
the	O
two	O
dummy	B
variables	O
.	O
the	O
fact	O
that	O
the	O
coeﬃcient	B
for	O
3.6	O
lab	O
:	O
linear	B
regression	I
119	O
shelvelocgood	O
in	O
the	O
regression	B
output	O
is	O
positive	O
indicates	O
that	O
a	O
good	O
shelving	O
location	O
is	O
associated	O
with	O
high	O
sales	O
(	O
relative	O
to	O
a	O
bad	O
location	O
)	O
.	O
and	O
shelvelocmedium	O
has	O
a	O
smaller	O
positive	O
coeﬃcient	O
,	O
indicating	O
that	O
a	O
medium	O
shelving	O
location	O
leads	O
to	O
higher	O
sales	O
than	O
a	O
bad	O
shelving	O
location	O
but	O
lower	O
sales	O
than	O
a	O
good	O
shelving	O
location	O
.	O
3.6.7	O
writing	O
functions	O
as	O
we	O
have	O
seen	O
,	O
r	O
comes	O
with	O
many	O
useful	O
functions	O
,	O
and	O
still	O
more	O
func-	O
tions	O
are	O
available	O
by	O
way	O
of	O
r	O
libraries	O
.	O
however	O
,	O
we	O
will	O
often	O
be	O
inter-	O
ested	O
in	O
performing	O
an	O
operation	O
for	O
which	O
no	O
function	B
is	O
available	O
.	O
in	O
this	O
setting	O
,	O
we	O
may	O
want	O
to	O
write	O
our	O
own	O
function	B
.	O
for	O
instance	O
,	O
below	O
we	O
provide	O
a	O
simple	B
function	O
that	O
reads	O
in	O
the	O
islr	O
and	O
mass	O
libraries	O
,	O
called	O
loadlibraries	O
(	O
)	O
.	O
before	O
we	O
have	O
created	O
the	O
function	B
,	O
r	O
returns	O
an	O
error	B
if	O
we	O
try	O
to	O
call	O
it	O
.	O
>	O
l	O
o	O
a	O
d	O
l	O
i	O
b	O
r	O
a	O
r	O
i	O
e	O
s	O
error	B
:	O
object	O
’	O
loadlibraries	O
’	O
not	O
found	O
>	O
l	O
o	O
a	O
d	O
l	O
i	O
b	O
r	O
a	O
r	O
i	O
e	O
s	O
(	O
)	O
error	B
:	O
could	O
not	O
find	O
function	B
``	O
l	O
o	O
a	O
d	O
l	O
i	O
b	O
r	O
a	O
r	O
i	O
e	O
s	O
``	O
we	O
now	O
create	O
the	O
function	B
.	O
note	O
that	O
the	O
+	O
symbols	O
are	O
printed	O
by	O
r	O
and	O
should	O
not	O
be	O
typed	O
in	O
.	O
the	O
{	O
symbol	O
informs	O
r	O
that	O
multiple	B
commands	O
are	O
about	O
to	O
be	O
input	B
.	O
hitting	O
enter	O
after	O
typing	O
{	O
will	O
cause	O
r	O
to	O
print	O
the	O
+	O
symbol	O
.	O
we	O
can	O
then	O
input	B
as	O
many	O
commands	O
as	O
we	O
wish	O
,	O
hitting	O
enter	O
after	O
each	O
one	O
.	O
finally	O
the	O
}	O
symbol	O
informs	O
r	O
that	O
no	O
further	O
commands	O
will	O
be	O
entered	O
.	O
>	O
l	O
o	O
a	O
d	O
l	O
i	O
b	O
r	O
a	O
r	O
i	O
e	O
s	O
=	O
function	B
(	O
)	O
{	O
+	O
library	O
(	O
islr	O
)	O
+	O
library	O
(	O
mass	O
)	O
+	O
print	O
(	O
``	O
the	O
libraries	O
have	O
been	O
loaded	O
.	O
''	O
)	O
+	O
}	O
now	O
if	O
we	O
type	O
in	O
loadlibraries	O
,	O
r	O
will	O
tell	O
us	O
what	O
is	O
in	O
the	O
function	B
.	O
>	O
l	O
o	O
a	O
d	O
l	O
i	O
b	O
r	O
a	O
r	O
i	O
e	O
s	O
function	B
(	O
)	O
{	O
library	O
(	O
islr	O
)	O
library	O
(	O
mass	O
)	O
print	O
(	O
``	O
the	O
libraries	O
have	O
been	O
loaded	O
.	O
''	O
)	O
}	O
if	O
we	O
call	O
the	O
function	B
,	O
the	O
libraries	O
are	O
loaded	O
in	O
and	O
the	O
print	O
statement	O
is	O
output	B
.	O
>	O
l	O
o	O
a	O
d	O
l	O
i	O
b	O
r	O
a	O
r	O
i	O
e	O
s	O
(	O
)	O
[	O
1	O
]	O
``	O
the	O
libraries	O
have	O
been	O
loaded	O
.	O
''	O
120	O
3.	O
linear	B
regression	I
3.7	O
exercises	O
conceptual	O
1.	O
describe	O
the	O
null	B
hypotheses	O
to	O
which	O
the	O
p-values	O
given	O
in	O
table	O
3.4	O
correspond	O
.	O
explain	O
what	O
conclusions	O
you	O
can	O
draw	O
based	O
on	O
these	O
p-values	O
.	O
your	O
explanation	O
should	O
be	O
phrased	O
in	O
terms	O
of	O
sales	O
,	O
tv	O
,	O
radio	O
,	O
and	O
newspaper	O
,	O
rather	O
than	O
in	O
terms	O
of	O
the	O
coeﬃcients	O
of	O
the	O
linear	B
model	I
.	O
2.	O
carefully	O
explain	O
the	O
diﬀerences	O
between	O
the	O
knn	O
classiﬁer	B
and	O
knn	O
regression	B
methods	O
.	O
3.	O
suppose	O
we	O
have	O
a	O
data	B
set	O
with	O
ﬁve	O
predictors	O
,	O
x1	O
=	O
gpa	O
,	O
x2	B
=	O
iq	O
,	O
x3	O
=	O
gender	O
(	O
1	O
for	O
female	O
and	O
0	O
for	O
male	O
)	O
,	O
x4	O
=	O
interaction	B
between	O
gpa	O
and	O
iq	O
,	O
and	O
x5	O
=	O
interaction	B
between	O
gpa	O
and	O
gender	O
.	O
the	O
response	B
is	O
starting	O
salary	O
after	O
graduation	O
(	O
in	O
thousands	O
of	O
dollars	O
)	O
.	O
suppose	O
we	O
use	O
least	B
squares	I
to	O
ﬁt	B
the	O
model	B
,	O
and	O
get	O
ˆβ0	O
=	O
50	O
,	O
ˆβ1	O
=	O
20	O
,	O
ˆβ2	O
=	O
0.07	O
,	O
ˆβ3	O
=	O
35	O
,	O
ˆβ4	O
=	O
0.01	O
,	O
ˆβ5	O
=	O
−10	O
.	O
(	O
a	O
)	O
which	O
answer	O
is	O
correct	O
,	O
and	O
why	O
?	O
i.	O
for	O
a	O
ﬁxed	O
value	O
of	O
iq	O
and	O
gpa	O
,	O
males	O
earn	O
more	O
on	O
average	B
than	O
females	O
.	O
ii	O
.	O
for	O
a	O
ﬁxed	O
value	O
of	O
iq	O
and	O
gpa	O
,	O
females	O
earn	O
more	O
on	O
average	B
than	O
males	O
.	O
iii	O
.	O
for	O
a	O
ﬁxed	O
value	O
of	O
iq	O
and	O
gpa	O
,	O
males	O
earn	O
more	O
on	O
average	B
than	O
females	O
provided	O
that	O
the	O
gpa	O
is	O
high	O
enough	O
.	O
iv	O
.	O
for	O
a	O
ﬁxed	O
value	O
of	O
iq	O
and	O
gpa	O
,	O
females	O
earn	O
more	O
on	O
average	B
than	O
males	O
provided	O
that	O
the	O
gpa	O
is	O
high	O
enough	O
.	O
(	O
b	O
)	O
predict	O
the	O
salary	O
of	O
a	O
female	O
with	O
iq	O
of	O
110	O
and	O
a	O
gpa	O
of	O
4.0	O
.	O
(	O
c	O
)	O
true	O
or	O
false	O
:	O
since	O
the	O
coeﬃcient	B
for	O
the	O
gpa/iq	O
interaction	B
term	O
is	O
very	O
small	O
,	O
there	O
is	O
very	O
little	O
evidence	O
of	O
an	O
interaction	B
eﬀect	O
.	O
justify	O
your	O
answer	O
.	O
4.	O
i	O
collect	O
a	O
set	B
of	O
data	B
(	O
n	O
=	O
100	O
observations	B
)	O
containing	O
a	O
single	B
predictor	O
and	O
a	O
quantitative	B
response	O
.	O
i	O
then	O
ﬁt	B
a	O
linear	B
regression	I
model	O
to	O
the	O
data	B
,	O
as	O
well	O
as	O
a	O
separate	O
cubic	B
regression	O
,	O
i.e	O
.	O
y	O
=	O
β0	O
+	O
β1x	O
+	O
β2x	O
2	O
+	O
β3x	O
3	O
+	O
	O
.	O
(	O
a	O
)	O
suppose	O
that	O
the	O
true	O
relationship	O
between	O
x	O
and	O
y	O
is	O
linear	B
,	O
i.e	O
.	O
y	O
=	O
β0	O
+	O
β1x	O
+	O
	O
.	O
consider	O
the	O
training	B
residual	O
sum	B
of	I
squares	I
(	O
rss	O
)	O
for	O
the	O
linear	B
regression	I
,	O
and	O
also	O
the	O
training	B
rss	O
for	O
the	O
cubic	B
regression	O
.	O
would	O
we	O
expect	O
one	O
to	O
be	O
lower	O
than	O
the	O
other	O
,	O
would	O
we	O
expect	O
them	O
to	O
be	O
the	O
same	O
,	O
or	O
is	O
there	O
not	O
enough	O
information	O
to	O
tell	O
?	O
justify	O
your	O
answer	O
.	O
3.7	O
exercises	O
121	O
(	O
b	O
)	O
answer	O
(	O
a	O
)	O
using	O
test	B
rather	O
than	O
training	B
rss	O
.	O
(	O
c	O
)	O
suppose	O
that	O
the	O
true	O
relationship	O
between	O
x	O
and	O
y	O
is	O
not	O
linear	B
,	O
but	O
we	O
don	O
’	O
t	O
know	O
how	O
far	O
it	O
is	O
from	O
linear	B
.	O
consider	O
the	O
training	B
rss	O
for	O
the	O
linear	B
regression	I
,	O
and	O
also	O
the	O
training	B
rss	O
for	O
the	O
cubic	B
regression	O
.	O
would	O
we	O
expect	O
one	O
to	O
be	O
lower	O
than	O
the	O
other	O
,	O
would	O
we	O
expect	O
them	O
to	O
be	O
the	O
same	O
,	O
or	O
is	O
there	O
not	O
enough	O
information	O
to	O
tell	O
?	O
justify	O
your	O
answer	O
.	O
(	O
d	O
)	O
answer	O
(	O
c	O
)	O
using	O
test	B
rather	O
than	O
training	B
rss	O
.	O
5.	O
consider	O
the	O
ﬁtted	O
values	O
that	O
result	O
from	O
performing	O
linear	B
regres-	O
sion	O
without	O
an	O
intercept	B
.	O
in	O
this	O
setting	O
,	O
the	O
ith	O
ﬁtted	B
value	I
takes	O
the	O
form	O
where	O
ˆyi	O
=	O
xi	O
ˆβ	O
,	O
(	O
cid:31	O
)	O
n	O
(	O
cid:17	O
)	O
xiyi	O
/	O
i	O
(	O
cid:2	O
)	O
=1	O
(	O
cid:31	O
)	O
n	O
(	O
cid:17	O
)	O
i=1	O
x2	B
i	O
(	O
cid:2	O
)	O
.	O
ˆβ	O
=	O
(	O
3.38	O
)	O
show	O
that	O
we	O
can	O
write	O
n	O
(	O
cid:17	O
)	O
i	O
(	O
cid:2	O
)	O
=1	O
ˆyi	O
=	O
ai	O
(	O
cid:2	O
)	O
yi	O
(	O
cid:2	O
)	O
.	O
what	O
is	O
ai	O
(	O
cid:2	O
)	O
?	O
note	O
:	O
we	O
interpret	O
this	O
result	O
by	O
saying	O
that	O
the	O
ﬁtted	O
values	O
from	O
linear	B
regression	I
are	O
linear	B
combinations	O
of	O
the	O
response	B
values	O
.	O
6.	O
using	O
(	O
3.4	O
)	O
,	O
argue	O
that	O
in	O
the	O
case	O
of	O
simple	B
linear	O
regression	B
,	O
the	O
least	B
squares	I
line	O
always	O
passes	O
through	O
the	O
point	O
(	O
¯x	O
,	O
¯y	O
)	O
.	O
7.	O
it	O
is	O
claimed	O
in	O
the	O
text	O
that	O
in	O
the	O
case	O
of	O
simple	B
linear	O
regression	B
of	O
y	O
onto	O
x	O
,	O
the	O
r2	O
statistic	O
(	O
3.17	O
)	O
is	O
equal	O
to	O
the	O
square	O
of	O
the	O
correlation	B
between	O
x	O
and	O
y	O
(	O
3.18	O
)	O
.	O
prove	O
that	O
this	O
is	O
the	O
case	O
.	O
for	O
simplicity	O
,	O
you	O
may	O
assume	O
that	O
¯x	O
=	O
¯y	O
=	O
0.	O
applied	O
8.	O
this	O
question	O
involves	O
the	O
use	O
of	O
simple	B
linear	O
regression	B
on	O
the	O
auto	O
data	B
set	O
.	O
(	O
a	O
)	O
use	O
the	O
lm	O
(	O
)	O
function	B
to	O
perform	O
a	O
simple	B
linear	O
regression	B
with	O
mpg	O
as	O
the	O
response	B
and	O
horsepower	O
as	O
the	O
predictor	B
.	O
use	O
the	O
summary	O
(	O
)	O
function	B
to	O
print	O
the	O
results	O
.	O
comment	O
on	O
the	O
output	B
.	O
for	O
example	O
:	O
122	O
3.	O
linear	B
regression	I
i.	O
is	O
there	O
a	O
relationship	O
between	O
the	O
predictor	B
and	O
the	O
re-	O
sponse	O
?	O
ii	O
.	O
how	O
strong	O
is	O
the	O
relationship	O
between	O
the	O
predictor	B
and	O
the	O
response	B
?	O
iii	O
.	O
is	O
the	O
relationship	O
between	O
the	O
predictor	B
and	O
the	O
response	B
positive	O
or	O
negative	O
?	O
iv	O
.	O
what	O
is	O
the	O
predicted	O
mpg	O
associated	O
with	O
a	O
horsepower	O
of	O
98	O
?	O
what	O
are	O
the	O
associated	O
95	O
%	O
conﬁdence	O
and	O
prediction	B
intervals	O
?	O
(	O
b	O
)	O
plot	B
the	O
response	B
and	O
the	O
predictor	B
.	O
use	O
the	O
abline	O
(	O
)	O
function	B
to	O
display	O
the	O
least	B
squares	I
regression	O
line	B
.	O
(	O
c	O
)	O
use	O
the	O
plot	B
(	O
)	O
function	B
to	O
produce	O
diagnostic	O
plots	O
of	O
the	O
least	B
squares	I
regression	O
ﬁt	B
.	O
comment	O
on	O
any	O
problems	O
you	O
see	O
with	O
the	O
ﬁt	B
.	O
9.	O
this	O
question	O
involves	O
the	O
use	O
of	O
multiple	B
linear	O
regression	B
on	O
the	O
auto	O
data	B
set	O
.	O
(	O
a	O
)	O
produce	O
a	O
scatterplot	B
matrix	I
which	O
includes	O
all	O
of	O
the	O
variables	O
in	O
the	O
data	B
set	O
.	O
(	O
b	O
)	O
compute	O
the	O
matrix	O
of	O
correlations	O
between	O
the	O
variables	O
using	O
the	O
function	B
cor	O
(	O
)	O
.	O
you	O
will	O
need	O
to	O
exclude	O
the	O
name	O
variable	B
,	O
which	O
is	O
qualitative	B
.	O
cor	O
(	O
)	O
(	O
c	O
)	O
use	O
the	O
lm	O
(	O
)	O
function	B
to	O
perform	O
a	O
multiple	B
linear	O
regression	B
with	O
mpg	O
as	O
the	O
response	B
and	O
all	O
other	O
variables	O
except	O
name	O
as	O
the	O
predictors	O
.	O
use	O
the	O
summary	O
(	O
)	O
function	B
to	O
print	O
the	O
results	O
.	O
comment	O
on	O
the	O
output	B
.	O
for	O
instance	O
:	O
i.	O
is	O
there	O
a	O
relationship	O
between	O
the	O
predictors	O
and	O
the	O
re-	O
sponse	O
?	O
ii	O
.	O
which	O
predictors	O
appear	O
to	O
have	O
a	O
statistically	O
signiﬁcant	O
relationship	O
to	O
the	O
response	B
?	O
iii	O
.	O
what	O
does	O
the	O
coeﬃcient	B
for	O
the	O
year	O
variable	B
suggest	O
?	O
(	O
d	O
)	O
use	O
the	O
plot	B
(	O
)	O
function	B
to	O
produce	O
diagnostic	O
plots	O
of	O
the	O
linear	B
regression	I
ﬁt	O
.	O
comment	O
on	O
any	O
problems	O
you	O
see	O
with	O
the	O
ﬁt	B
.	O
do	O
the	O
residual	B
plots	O
suggest	O
any	O
unusually	O
large	O
outliers	O
?	O
does	O
the	O
leverage	B
plot	O
identify	O
any	O
observations	B
with	O
unusually	O
high	O
leverage	B
?	O
(	O
e	O
)	O
use	O
the	O
*	O
and	O
:	O
symbols	O
to	O
ﬁt	B
linear	O
regression	B
models	O
with	O
interaction	B
eﬀects	O
.	O
do	O
any	O
interactions	O
appear	O
to	O
be	O
statistically	O
signiﬁcant	O
?	O
√	O
x	O
,	O
x	O
2.	O
comment	O
on	O
your	O
ﬁndings	O
.	O
(	O
f	O
)	O
try	O
a	O
few	O
diﬀerent	O
transformations	O
of	O
the	O
variables	O
,	O
such	O
as	O
log	O
(	O
x	O
)	O
,	O
10.	O
this	O
question	O
should	O
be	O
answered	O
using	O
the	O
carseats	O
data	B
set	O
.	O
3.7	O
exercises	O
123	O
(	O
a	O
)	O
fit	O
a	O
multiple	B
regression	O
model	B
to	O
predict	O
sales	O
using	O
price	O
,	O
urban	O
,	O
and	O
us	O
.	O
(	O
b	O
)	O
provide	O
an	O
interpretation	O
of	O
each	O
coeﬃcient	B
in	O
the	O
model	B
.	O
be	O
careful—some	O
of	O
the	O
variables	O
in	O
the	O
model	B
are	O
qualitative	B
!	O
(	O
c	O
)	O
write	O
out	O
the	O
model	B
in	O
equation	O
form	O
,	O
being	O
careful	O
to	O
handle	O
the	O
qualitative	B
variables	O
properly	O
.	O
(	O
d	O
)	O
for	O
which	O
of	O
the	O
predictors	O
can	O
you	O
reject	O
the	O
null	B
hypothesis	O
h0	O
:	O
βj	O
=	O
0	O
?	O
(	O
e	O
)	O
on	O
the	O
basis	B
of	O
your	O
response	B
to	O
the	O
previous	O
question	O
,	O
ﬁt	B
a	O
smaller	O
model	B
that	O
only	O
uses	O
the	O
predictors	O
for	O
which	O
there	O
is	O
evidence	O
of	O
association	O
with	O
the	O
outcome	O
.	O
(	O
f	O
)	O
how	O
well	O
do	O
the	O
models	O
in	O
(	O
a	O
)	O
and	O
(	O
e	O
)	O
ﬁt	B
the	O
data	B
?	O
(	O
g	O
)	O
using	O
the	O
model	B
from	O
(	O
e	O
)	O
,	O
obtain	O
95	O
%	O
conﬁdence	O
intervals	O
for	O
the	O
coeﬃcient	B
(	O
s	O
)	O
.	O
(	O
h	O
)	O
is	O
there	O
evidence	O
of	O
outliers	O
or	O
high	O
leverage	B
observations	O
in	O
the	O
model	B
from	O
(	O
e	O
)	O
?	O
11.	O
in	O
this	O
problem	O
we	O
will	O
investigate	O
the	O
t-statistic	B
for	O
the	O
null	B
hypoth-	O
esis	O
h0	O
:	O
β	O
=	O
0	O
in	O
simple	B
linear	O
regression	B
without	O
an	O
intercept	B
.	O
to	O
begin	O
,	O
we	O
generate	O
a	O
predictor	B
x	O
and	O
a	O
response	B
y	O
as	O
follows	O
.	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
x	O
=	O
rnorm	O
(	O
100	O
)	O
>	O
y	O
=2*	O
x	O
+	O
rnorm	O
(	O
100	O
)	O
(	O
a	O
)	O
perform	O
a	O
simple	B
linear	O
regression	B
of	O
y	O
onto	O
x	O
,	O
without	O
an	O
in-	O
tercept	O
.	O
report	O
the	O
coeﬃcient	B
estimate	O
ˆβ	O
,	O
the	O
standard	B
error	I
of	O
this	O
coeﬃcient	B
estimate	O
,	O
and	O
the	O
t-statistic	B
and	O
p-value	B
associ-	O
ated	O
with	O
the	O
null	B
hypothesis	O
h0	O
:	O
β	O
=	O
0.	O
comment	O
on	O
these	O
results	O
.	O
(	O
you	O
can	O
perform	O
regression	B
without	O
an	O
intercept	B
using	O
the	O
command	O
lm	O
(	O
y∼x+0	O
)	O
.	O
)	O
(	O
b	O
)	O
now	O
perform	O
a	O
simple	B
linear	O
regression	B
of	O
x	O
onto	O
y	O
without	O
an	O
intercept	B
,	O
and	O
report	O
the	O
coeﬃcient	B
estimate	O
,	O
its	O
standard	B
error	I
,	O
and	O
the	O
corresponding	O
t-statistic	B
and	O
p-values	O
associated	O
with	O
the	O
null	B
hypothesis	O
h0	O
:	O
β	O
=	O
0.	O
comment	O
on	O
these	O
results	O
.	O
(	O
c	O
)	O
what	O
is	O
the	O
relationship	O
between	O
the	O
results	O
obtained	O
in	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
?	O
(	O
d	O
)	O
for	O
the	O
regression	B
of	O
y	O
onto	O
x	O
without	O
an	O
intercept	B
,	O
the	O
t-	O
statistic	O
for	O
h0	O
:	O
β	O
=	O
0	O
takes	O
the	O
form	O
ˆβ/se	O
(	O
ˆβ	O
)	O
,	O
where	O
ˆβ	O
is	O
given	O
by	O
(	O
3.38	O
)	O
,	O
and	O
where	O
!	O
(	O
cid:10	O
)	O
(	O
n	O
−	O
1	O
)	O
n	O
se	O
(	O
ˆβ	O
)	O
=	O
i=1	O
(	O
yi	O
−	O
xi	O
ˆβ	O
)	O
2	O
n	O
i	O
(	O
cid:2	O
)	O
=1	O
x2	B
i	O
(	O
cid:2	O
)	O
(	O
cid:10	O
)	O
.	O
124	O
3.	O
linear	B
regression	I
(	O
these	O
formulas	O
are	O
slightly	O
diﬀerent	O
from	O
those	O
given	O
in	O
sec-	O
tions	O
3.1.1	O
and	O
3.1.2	O
,	O
since	O
here	O
we	O
are	O
performing	O
regression	B
without	O
an	O
intercept	B
.	O
)	O
show	O
algebraically	O
,	O
and	O
conﬁrm	O
numeri-	O
cally	O
in	O
r	O
,	O
that	O
the	O
t-statistic	B
can	O
be	O
written	O
as	O
√	O
n	O
−	O
1	O
)	O
(	O
cid:10	O
)	O
(	O
n	O
n	O
i	O
(	O
cid:2	O
)	O
=1	O
y2	O
i=1	O
x2	B
i	O
)	O
(	O
(	O
cid:10	O
)	O
n	O
(	O
cid:10	O
)	O
i=1	O
xiyi	O
i	O
(	O
cid:2	O
)	O
)	O
−	O
(	O
n	O
i	O
(	O
cid:2	O
)	O
=1	O
xi	O
(	O
cid:2	O
)	O
yi	O
(	O
cid:2	O
)	O
)	O
2	O
(	O
cid:22	O
)	O
(	O
cid:10	O
)	O
(	O
.	O
(	O
e	O
)	O
using	O
the	O
results	O
from	O
(	O
d	O
)	O
,	O
argue	O
that	O
the	O
t-statistic	B
for	O
the	O
re-	O
gression	O
of	O
y	O
onto	O
x	O
is	O
the	O
same	O
as	O
the	O
t-statistic	B
for	O
the	O
regression	B
of	O
x	O
onto	O
y	O
.	O
(	O
f	O
)	O
in	O
r	O
,	O
show	O
that	O
when	O
regression	B
is	O
performed	O
with	O
an	O
intercept	B
,	O
the	O
t-statistic	B
for	O
h0	O
:	O
β1	O
=	O
0	O
is	O
the	O
same	O
for	O
the	O
regression	B
of	O
y	O
onto	O
x	O
as	O
it	O
is	O
for	O
the	O
regression	B
of	O
x	O
onto	O
y	O
.	O
12.	O
this	O
problem	O
involves	O
simple	B
linear	O
regression	B
without	O
an	O
intercept	B
.	O
(	O
a	O
)	O
recall	B
that	O
the	O
coeﬃcient	B
estimate	O
ˆβ	O
for	O
the	O
linear	B
regression	I
of	O
y	O
onto	O
x	O
without	O
an	O
intercept	B
is	O
given	O
by	O
(	O
3.38	O
)	O
.	O
under	O
what	O
circumstance	O
is	O
the	O
coeﬃcient	B
estimate	O
for	O
the	O
regression	B
of	O
x	O
onto	O
y	O
the	O
same	O
as	O
the	O
coeﬃcient	B
estimate	O
for	O
the	O
regression	B
of	O
y	O
onto	O
x	O
?	O
(	O
b	O
)	O
generate	O
an	O
example	O
in	O
r	O
with	O
n	O
=	O
100	O
observations	B
in	O
which	O
the	O
coeﬃcient	B
estimate	O
for	O
the	O
regression	B
of	O
x	O
onto	O
y	O
is	O
diﬀerent	O
from	O
the	O
coeﬃcient	B
estimate	O
for	O
the	O
regression	B
of	O
y	O
onto	O
x	O
.	O
(	O
c	O
)	O
generate	O
an	O
example	O
in	O
r	O
with	O
n	O
=	O
100	O
observations	B
in	O
which	O
the	O
coeﬃcient	B
estimate	O
for	O
the	O
regression	B
of	O
x	O
onto	O
y	O
is	O
the	O
same	O
as	O
the	O
coeﬃcient	B
estimate	O
for	O
the	O
regression	B
of	O
y	O
onto	O
x	O
.	O
13.	O
in	O
this	O
exercise	O
you	O
will	O
create	O
some	O
simulated	O
data	B
and	O
will	O
ﬁt	B
simple	O
linear	B
regression	I
models	O
to	O
it	O
.	O
make	O
sure	O
to	O
use	O
set.seed	O
(	O
1	O
)	O
prior	B
to	O
starting	O
part	O
(	O
a	O
)	O
to	O
ensure	O
consistent	O
results	O
.	O
(	O
a	O
)	O
using	O
the	O
rnorm	O
(	O
)	O
function	B
,	O
create	O
a	O
vector	B
,	O
x	O
,	O
containing	O
100	O
observations	B
drawn	O
from	O
a	O
n	O
(	O
0	O
,	O
1	O
)	O
distribution	B
.	O
this	O
represents	O
a	O
feature	B
,	O
x	O
.	O
(	O
b	O
)	O
using	O
the	O
rnorm	O
(	O
)	O
function	B
,	O
create	O
a	O
vector	B
,	O
eps	O
,	O
containing	O
100	O
observations	B
drawn	O
from	O
a	O
n	O
(	O
0	O
,	O
0.25	O
)	O
distribution	B
i.e	O
.	O
a	O
normal	O
distribution	O
with	O
mean	O
zero	O
and	O
variance	B
0.25	O
.	O
(	O
c	O
)	O
using	O
x	O
and	O
eps	O
,	O
generate	O
a	O
vector	B
y	O
according	O
to	O
the	O
model	B
y	O
=	O
−1	O
+	O
0.5x	O
+	O
	O
.	O
(	O
3.39	O
)	O
what	O
is	O
the	O
length	O
of	O
the	O
vector	B
y	O
?	O
what	O
are	O
the	O
values	O
of	O
β0	O
and	O
β1	O
in	O
this	O
linear	B
model	I
?	O
3.7	O
exercises	O
125	O
(	O
d	O
)	O
create	O
a	O
scatterplot	B
displaying	O
the	O
relationship	O
between	O
x	O
and	O
y.	O
comment	O
on	O
what	O
you	O
observe	O
.	O
(	O
e	O
)	O
fit	O
a	O
least	B
squares	I
linear	O
model	B
to	O
predict	O
y	O
using	O
x.	O
comment	O
on	O
the	O
model	B
obtained	O
.	O
how	O
do	O
ˆβ0	O
and	O
ˆβ1	O
compare	O
to	O
β0	O
and	O
β1	O
?	O
(	O
f	O
)	O
display	O
the	O
least	B
squares	I
line	O
on	O
the	O
scatterplot	B
obtained	O
in	O
(	O
d	O
)	O
.	O
draw	O
the	O
population	B
regression	I
line	I
on	O
the	O
plot	B
,	O
in	O
a	O
diﬀerent	O
color	O
.	O
use	O
the	O
legend	O
(	O
)	O
command	O
to	O
create	O
an	O
appropriate	O
leg-	O
end	O
.	O
(	O
g	O
)	O
now	O
ﬁt	B
a	O
polynomial	B
regression	O
model	B
that	O
predicts	O
y	O
using	O
x	O
and	O
x2	B
.	O
is	O
there	O
evidence	O
that	O
the	O
quadratic	B
term	O
improves	O
the	O
model	B
ﬁt	O
?	O
explain	O
your	O
answer	O
.	O
(	O
h	O
)	O
repeat	O
(	O
a	O
)	O
–	O
(	O
f	O
)	O
after	O
modifying	O
the	O
data	B
generation	O
process	O
in	O
such	O
a	O
way	O
that	O
there	O
is	O
less	O
noise	B
in	O
the	O
data	B
.	O
the	O
model	B
(	O
3.39	O
)	O
should	O
remain	O
the	O
same	O
.	O
you	O
can	O
do	O
this	O
by	O
decreasing	O
the	O
vari-	O
ance	O
of	O
the	O
normal	O
distribution	O
used	O
to	O
generate	O
the	O
error	B
term	O
	O
in	O
(	O
b	O
)	O
.	O
describe	O
your	O
results	O
.	O
(	O
i	O
)	O
repeat	O
(	O
a	O
)	O
–	O
(	O
f	O
)	O
after	O
modifying	O
the	O
data	B
generation	O
process	O
in	O
such	O
a	O
way	O
that	O
there	O
is	O
more	O
noise	B
in	O
the	O
data	B
.	O
the	O
model	B
(	O
3.39	O
)	O
should	O
remain	O
the	O
same	O
.	O
you	O
can	O
do	O
this	O
by	O
increasing	O
the	O
variance	B
of	O
the	O
normal	O
distribution	O
used	O
to	O
generate	O
the	O
error	B
term	O
	O
in	O
(	O
b	O
)	O
.	O
describe	O
your	O
results	O
.	O
(	O
j	O
)	O
what	O
are	O
the	O
conﬁdence	O
intervals	O
for	O
β0	O
and	O
β1	O
based	O
on	O
the	O
original	O
data	B
set	O
,	O
the	O
noisier	O
data	B
set	O
,	O
and	O
the	O
less	O
noisy	O
data	B
set	O
?	O
comment	O
on	O
your	O
results	O
.	O
14.	O
this	O
problem	O
focuses	O
on	O
the	O
collinearity	B
problem	O
.	O
(	O
a	O
)	O
perform	O
the	O
following	O
commands	O
in	O
r	O
:	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
x1	O
=	O
runif	O
(	O
100	O
)	O
>	O
x2	B
=0.5*	O
x1	O
+	O
rnorm	O
(	O
100	O
)	O
/10	O
>	O
y	O
=2+2*	O
x1	O
+0.3*	O
x2	B
+	O
rnorm	O
(	O
100	O
)	O
the	O
last	O
line	B
corresponds	O
to	O
creating	O
a	O
linear	B
model	I
in	O
which	O
y	O
is	O
a	O
function	B
of	O
x1	O
and	O
x2	B
.	O
write	O
out	O
the	O
form	O
of	O
the	O
linear	B
model	I
.	O
what	O
are	O
the	O
regression	B
coeﬃcients	O
?	O
(	O
b	O
)	O
what	O
is	O
the	O
correlation	B
between	O
x1	O
and	O
x2	B
?	O
create	O
a	O
scatterplot	B
displaying	O
the	O
relationship	O
between	O
the	O
variables	O
.	O
(	O
c	O
)	O
using	O
this	O
data	B
,	O
ﬁt	B
a	O
least	B
squares	I
regression	O
to	O
predict	O
y	O
using	O
x1	O
and	O
x2	B
.	O
describe	O
the	O
results	O
obtained	O
.	O
what	O
are	O
ˆβ0	O
,	O
ˆβ1	O
,	O
and	O
ˆβ2	O
?	O
how	O
do	O
these	O
relate	O
to	O
the	O
true	O
β0	O
,	O
β1	O
,	O
and	O
β2	O
?	O
can	O
you	O
reject	O
the	O
null	B
hypothesis	O
h0	O
:	O
β1	O
=	O
0	O
?	O
how	O
about	O
the	O
null	B
hypothesis	O
h0	O
:	O
β2	O
=	O
0	O
?	O
126	O
3.	O
linear	B
regression	I
(	O
d	O
)	O
now	O
ﬁt	B
a	O
least	B
squares	I
regression	O
to	O
predict	O
y	O
using	O
only	O
x1	O
.	O
comment	O
on	O
your	O
results	O
.	O
can	O
you	O
reject	O
the	O
null	B
hypothesis	O
h0	O
:	O
β1	O
=	O
0	O
?	O
(	O
e	O
)	O
now	O
ﬁt	B
a	O
least	B
squares	I
regression	O
to	O
predict	O
y	O
using	O
only	O
x2	B
.	O
comment	O
on	O
your	O
results	O
.	O
can	O
you	O
reject	O
the	O
null	B
hypothesis	O
h0	O
:	O
β1	O
=	O
0	O
?	O
(	O
f	O
)	O
do	O
the	O
results	O
obtained	O
in	O
(	O
c	O
)	O
–	O
(	O
e	O
)	O
contradict	O
each	O
other	O
?	O
explain	O
your	O
answer	O
.	O
(	O
g	O
)	O
now	O
suppose	O
we	O
obtain	O
one	O
additional	O
observation	O
,	O
which	O
was	O
unfortunately	O
mismeasured	O
.	O
>	O
x1	O
=	O
c	O
(	O
x1	O
,	O
0.1	O
)	O
>	O
x2	B
=	O
c	O
(	O
x2	B
,	O
0.8	O
)	O
>	O
y	O
=	O
c	O
(	O
y	O
,6	O
)	O
re-ﬁt	O
the	O
linear	B
models	O
from	O
(	O
c	O
)	O
to	O
(	O
e	O
)	O
using	O
this	O
new	O
data	B
.	O
what	O
eﬀect	O
does	O
this	O
new	O
observation	O
have	O
on	O
the	O
each	O
of	O
the	O
models	O
?	O
in	O
each	O
model	B
,	O
is	O
this	O
observation	O
an	O
outlier	B
?	O
a	O
high-leverage	O
point	O
?	O
both	O
?	O
explain	O
your	O
answers	O
.	O
15.	O
this	O
problem	O
involves	O
the	O
boston	O
data	B
set	O
,	O
which	O
we	O
saw	O
in	O
the	O
lab	O
for	O
this	O
chapter	O
.	O
we	O
will	O
now	O
try	O
to	O
predict	O
per	O
capita	O
crime	O
rate	B
using	O
the	O
other	O
variables	O
in	O
this	O
data	B
set	O
.	O
in	O
other	O
words	O
,	O
per	O
capita	O
crime	O
rate	B
is	O
the	O
response	B
,	O
and	O
the	O
other	O
variables	O
are	O
the	O
predictors	O
.	O
(	O
a	O
)	O
for	O
each	O
predictor	B
,	O
ﬁt	B
a	O
simple	B
linear	O
regression	B
model	O
to	O
predict	O
the	O
response	B
.	O
describe	O
your	O
results	O
.	O
in	O
which	O
of	O
the	O
models	O
is	O
there	O
a	O
statistically	O
signiﬁcant	O
association	O
between	O
the	O
predictor	B
and	O
the	O
response	B
?	O
create	O
some	O
plots	O
to	O
back	O
up	O
your	O
assertions	O
.	O
(	O
b	O
)	O
fit	O
a	O
multiple	B
regression	O
model	B
to	O
predict	O
the	O
response	B
using	O
all	O
of	O
the	O
predictors	O
.	O
describe	O
your	O
results	O
.	O
for	O
which	O
predictors	O
can	O
we	O
reject	O
the	O
null	B
hypothesis	O
h0	O
:	O
βj	O
=	O
0	O
?	O
(	O
c	O
)	O
how	O
do	O
your	O
results	O
from	O
(	O
a	O
)	O
compare	O
to	O
your	O
results	O
from	O
(	O
b	O
)	O
?	O
create	O
a	O
plot	B
displaying	O
the	O
univariate	O
regression	B
coeﬃcients	O
from	O
(	O
a	O
)	O
on	O
the	O
x-axis	O
,	O
and	O
the	O
multiple	B
regression	O
coeﬃcients	O
from	O
(	O
b	O
)	O
on	O
the	O
y-axis	O
.	O
that	O
is	O
,	O
each	O
predictor	B
is	O
displayed	O
as	O
a	O
single	B
point	O
in	O
the	O
plot	B
.	O
its	O
coeﬃcient	B
in	O
a	O
simple	B
linear	O
regres-	O
sion	O
model	B
is	O
shown	O
on	O
the	O
x-axis	O
,	O
and	O
its	O
coeﬃcient	B
estimate	O
in	O
the	O
multiple	B
linear	O
regression	B
model	O
is	O
shown	O
on	O
the	O
y-axis	O
.	O
(	O
d	O
)	O
is	O
there	O
evidence	O
of	O
non-linear	B
association	O
between	O
any	O
of	O
the	O
predictors	O
and	O
the	O
response	B
?	O
to	O
answer	O
this	O
question	O
,	O
for	O
each	O
predictor	B
x	O
,	O
ﬁt	B
a	O
model	B
of	O
the	O
form	O
y	O
=	O
β0	O
+	O
β1x	O
+	O
β2x	O
2	O
+	O
β3x	O
3	O
+	O
	O
.	O
4	O
classiﬁcation	B
the	O
linear	B
regression	I
model	O
discussed	O
in	O
chapter	O
3	O
assumes	O
that	O
the	O
re-	O
sponse	O
variable	B
y	O
is	O
quantitative	B
.	O
but	O
in	O
many	O
situations	O
,	O
the	O
response	B
variable	O
is	O
instead	O
qualitative	B
.	O
for	O
example	O
,	O
eye	O
color	O
is	O
qualitative	B
,	O
taking	O
on	O
values	O
blue	O
,	O
brown	O
,	O
or	O
green	O
.	O
often	O
qualitative	B
variables	O
are	O
referred	O
to	O
as	O
categorical	B
;	O
we	O
will	O
use	O
these	O
terms	O
interchangeably	O
.	O
in	O
this	O
chapter	O
,	O
we	O
study	O
approaches	O
for	O
predicting	O
qualitative	B
responses	O
,	O
a	O
process	O
that	O
is	O
known	O
as	O
classiﬁcation	B
.	O
predicting	O
a	O
qualitative	B
response	O
for	O
an	O
obser-	O
vation	O
can	O
be	O
referred	O
to	O
as	O
classifying	O
that	O
observation	O
,	O
since	O
it	O
involves	O
assigning	O
the	O
observation	O
to	O
a	O
category	O
,	O
or	O
class	O
.	O
on	O
the	O
other	O
hand	O
,	O
often	O
the	O
methods	O
used	O
for	O
classiﬁcation	O
ﬁrst	O
predict	O
the	O
probability	B
of	O
each	O
of	O
the	O
categories	O
of	O
a	O
qualitative	B
variable	O
,	O
as	O
the	O
basis	B
for	O
making	O
the	O
classi-	O
ﬁcation	O
.	O
in	O
this	O
sense	O
they	O
also	O
behave	O
like	O
regression	B
methods	O
.	O
there	O
are	O
many	O
possible	O
classiﬁcation	B
techniques	O
,	O
or	O
classiﬁers	O
,	O
that	O
one	O
might	O
use	O
to	O
predict	O
a	O
qualitative	B
response	O
.	O
we	O
touched	O
on	O
some	O
of	O
these	O
in	O
sections	O
2.1.5	O
and	O
2.2.3.	O
in	O
this	O
chapter	O
we	O
discuss	O
three	O
of	O
the	O
most	O
widely-used	O
classiﬁers	O
:	O
logistic	B
regression	I
,	O
linear	B
discriminant	I
analysis	I
,	O
and	O
k-nearest	O
neighbors	O
.	O
we	O
discuss	O
more	O
computer-intensive	O
methods	O
in	O
later	O
chapters	O
,	O
such	O
as	O
generalized	O
additive	O
models	O
(	O
chapter	O
7	O
)	O
,	O
trees	O
,	O
random	O
forests	O
,	O
and	O
boosting	B
(	O
chapter	O
8	O
)	O
,	O
and	O
support	B
vector	I
machines	O
(	O
chap-	O
ter	O
9	O
)	O
.	O
qualitative	B
classiﬁcation	O
classiﬁer	B
logistic	O
regression	B
linear	O
discriminant	O
analysis	O
k-nearest	O
neighbors	O
g.	O
james	O
et	O
al.	O
,	O
an	O
introduction	O
to	O
statistical	O
learning	O
:	O
with	O
applications	O
in	O
r	O
,	O
springer	O
texts	O
in	O
statistics	O
,	O
doi	O
10.1007/978-1-4614-7138-7	O
4	O
,	O
©	O
springer	O
science+business	O
media	O
new	O
york	O
2013	O
127	O
128	O
4.	O
classiﬁcation	B
4.1	O
an	O
overview	O
of	O
classiﬁcation	B
classiﬁcation	O
problems	O
occur	O
often	O
,	O
perhaps	O
even	O
more	O
so	O
than	O
regression	B
problems	O
.	O
some	O
examples	O
include	O
:	O
1.	O
a	O
person	O
arrives	O
at	O
the	O
emergency	O
room	O
with	O
a	O
set	B
of	O
symptoms	O
that	O
could	O
possibly	O
be	O
attributed	O
to	O
one	O
of	O
three	O
medical	O
conditions	O
.	O
which	O
of	O
the	O
three	O
conditions	O
does	O
the	O
individual	O
have	O
?	O
2.	O
an	O
online	O
banking	O
service	O
must	O
be	O
able	O
to	O
determine	O
whether	O
or	O
not	O
a	O
transaction	O
being	O
performed	O
on	O
the	O
site	O
is	O
fraudulent	O
,	O
on	O
the	O
basis	B
of	O
the	O
user	O
’	O
s	O
ip	O
address	O
,	O
past	O
transaction	O
history	O
,	O
and	O
so	O
forth	O
.	O
3.	O
on	O
the	O
basis	B
of	O
dna	O
sequence	O
data	B
for	O
a	O
number	O
of	O
patients	O
with	O
and	O
without	O
a	O
given	O
disease	O
,	O
a	O
biologist	O
would	O
like	O
to	O
ﬁgure	O
out	O
which	O
dna	O
mutations	O
are	O
deleterious	O
(	O
disease-causing	O
)	O
and	O
which	O
are	O
not	O
.	O
just	O
as	O
in	O
the	O
regression	B
setting	O
,	O
in	O
the	O
classiﬁcation	B
setting	O
we	O
have	O
a	O
set	B
of	O
training	B
observations	O
(	O
x1	O
,	O
y1	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
that	O
we	O
can	O
use	O
to	O
build	O
a	O
classiﬁer	B
.	O
we	O
want	O
our	O
classiﬁer	B
to	O
perform	O
well	O
not	O
only	O
on	O
the	O
training	B
data	O
,	O
but	O
also	O
on	O
test	B
observations	O
that	O
were	O
not	O
used	O
to	O
train	B
the	O
classiﬁer	B
.	O
in	O
this	O
chapter	O
,	O
we	O
will	O
illustrate	O
the	O
concept	O
of	O
classiﬁcation	B
using	O
the	O
simulated	O
default	O
data	B
set	O
.	O
we	O
are	O
interested	O
in	O
predicting	O
whether	O
an	O
individual	O
will	O
default	O
on	O
his	O
or	O
her	O
credit	O
card	O
payment	O
,	O
on	O
the	O
basis	B
of	O
annual	O
income	O
and	O
monthly	O
credit	O
card	O
balance	O
.	O
the	O
data	B
set	O
is	O
displayed	O
in	O
figure	O
4.1.	O
we	O
have	O
plotted	O
annual	O
income	O
and	O
monthly	O
credit	O
card	O
balance	O
for	O
a	O
subset	O
of	O
10	O
,	O
000	O
individuals	O
.	O
the	O
left-hand	O
panel	O
of	O
figure	O
4.1	O
displays	O
individuals	O
who	O
defaulted	O
in	O
a	O
given	O
month	O
in	O
orange	O
,	O
and	O
those	O
who	O
did	O
not	O
in	O
blue	O
.	O
(	O
the	O
overall	O
default	O
rate	B
is	O
about	O
3	O
%	O
,	O
so	O
we	O
have	O
plotted	O
only	O
a	O
fraction	O
of	O
the	O
individuals	O
who	O
did	O
not	O
default	O
.	O
)	O
it	O
appears	O
that	O
individuals	O
who	O
defaulted	O
tended	O
to	O
have	O
higher	O
credit	O
card	O
balances	O
than	O
those	O
who	O
did	O
not	O
.	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
4.1	O
,	O
two	O
pairs	O
of	O
boxplots	O
are	O
shown	O
.	O
the	O
ﬁrst	O
shows	O
the	O
distribution	B
of	O
balance	O
split	O
by	O
the	O
binary	B
default	O
variable	B
;	O
the	O
second	O
is	O
a	O
similar	O
plot	B
for	O
income	O
.	O
in	O
this	O
chapter	O
,	O
we	O
learn	O
how	O
to	O
build	O
a	O
model	B
to	O
predict	O
default	O
(	O
y	O
)	O
for	O
any	O
given	O
value	O
of	O
balance	O
(	O
x1	O
)	O
and	O
income	O
(	O
x2	B
)	O
.	O
since	O
y	O
is	O
not	O
quantitative	B
,	O
the	O
simple	B
linear	O
regression	B
model	O
of	O
chapter	O
3	O
is	O
not	O
appropriate	O
.	O
it	O
is	O
worth	O
noting	O
that	O
figure	O
4.1	O
displays	O
a	O
very	O
pronounced	O
relation-	O
ship	O
between	O
the	O
predictor	B
balance	O
and	O
the	O
response	B
default	O
.	O
in	O
most	O
real	O
applications	O
,	O
the	O
relationship	O
between	O
the	O
predictor	B
and	O
the	O
response	B
will	O
not	O
be	O
nearly	O
so	O
strong	O
.	O
however	O
,	O
for	O
the	O
sake	O
of	O
illustrating	O
the	O
classiﬁca-	O
tion	O
procedures	O
discussed	O
in	O
this	O
chapter	O
,	O
we	O
use	O
an	O
example	O
in	O
which	O
the	O
relationship	O
between	O
the	O
predictor	B
and	O
the	O
response	B
is	O
somewhat	O
exagger-	O
ated	O
.	O
e	O
m	O
o	O
c	O
n	O
i	O
0	O
0	O
0	O
0	O
6	O
0	O
0	O
0	O
0	O
4	O
0	O
0	O
0	O
0	O
2	O
0	O
4.2	O
why	O
not	O
linear	B
regression	I
?	O
129	O
e	O
c	O
n	O
a	O
a	O
b	O
l	O
0	O
0	O
5	O
2	O
0	O
0	O
0	O
2	O
0	O
0	O
5	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
5	O
0	O
e	O
m	O
o	O
c	O
n	O
i	O
0	O
0	O
0	O
0	O
6	O
0	O
0	O
0	O
0	O
4	O
0	O
0	O
0	O
0	O
2	O
0	O
0	O
500	O
1000	O
1500	O
balance	O
2000	O
2500	O
yes	O
no	O
default	O
yes	O
no	O
default	O
figure	O
4.1.	O
the	O
default	O
data	B
set	O
.	O
left	O
:	O
the	O
annual	O
incomes	O
and	O
monthly	O
credit	O
card	O
balances	O
of	O
a	O
number	O
of	O
individuals	O
.	O
the	O
individuals	O
who	O
defaulted	O
on	O
their	O
credit	O
card	O
payments	O
are	O
shown	O
in	O
orange	O
,	O
and	O
those	O
who	O
did	O
not	O
are	O
shown	O
in	O
blue	O
.	O
center	O
:	O
boxplots	O
of	O
balance	O
as	O
a	O
function	B
of	O
default	O
status	O
.	O
right	O
:	O
boxplots	O
of	O
income	O
as	O
a	O
function	B
of	O
default	O
status	O
.	O
4.2	O
why	O
not	O
linear	B
regression	I
?	O
we	O
have	O
stated	O
that	O
linear	B
regression	I
is	O
not	O
appropriate	O
in	O
the	O
case	O
of	O
a	O
qualitative	B
response	O
.	O
why	O
not	O
?	O
suppose	O
that	O
we	O
are	O
trying	O
to	O
predict	O
the	O
medical	O
condition	O
of	O
a	O
patient	O
in	O
the	O
emergency	O
room	O
on	O
the	O
basis	B
of	O
her	O
symptoms	O
.	O
in	O
this	O
simpliﬁed	O
example	O
,	O
there	O
are	O
three	O
possible	O
diagnoses	O
:	O
stroke	O
,	O
drug	O
overdose	O
,	O
and	O
epileptic	O
seizure	O
.	O
we	O
could	O
consider	O
encoding	O
these	O
values	O
as	O
a	O
quantita-	O
tive	O
response	B
variable	O
,	O
y	O
,	O
as	O
follows	O
:	O
⎧⎪⎨	O
⎪⎩	O
y	O
=	O
1	O
if	O
stroke	O
;	O
2	O
if	O
drug	O
overdose	O
;	O
3	O
if	O
epileptic	O
seizure	O
.	O
using	O
this	O
coding	O
,	O
least	B
squares	I
could	O
be	O
used	O
to	O
ﬁt	B
a	O
linear	B
regression	I
model	O
to	O
predict	O
y	O
on	O
the	O
basis	B
of	O
a	O
set	B
of	O
predictors	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
.	O
unfortunately	O
,	O
this	O
coding	O
implies	O
an	O
ordering	O
on	O
the	O
outcomes	O
,	O
putting	O
drug	O
overdose	O
in	O
between	O
stroke	O
and	O
epileptic	O
seizure	O
,	O
and	O
insisting	O
that	O
the	O
diﬀerence	O
between	O
stroke	O
and	O
drug	O
overdose	O
is	O
the	O
same	O
as	O
the	O
diﬀerence	O
between	O
drug	O
overdose	O
and	O
epileptic	O
seizure	O
.	O
in	O
practice	O
there	O
is	O
no	O
particular	O
reason	O
that	O
this	O
needs	O
to	O
be	O
the	O
case	O
.	O
for	O
instance	O
,	O
one	O
could	O
choose	O
an	O
equally	O
reasonable	O
coding	O
,	O
⎧⎪⎨	O
⎪⎩	O
y	O
=	O
1	O
if	O
epileptic	O
seizure	O
;	O
2	O
if	O
stroke	O
;	O
3	O
if	O
drug	O
overdose	O
.	O
130	O
4.	O
classiﬁcation	B
which	O
would	O
imply	O
a	O
totally	O
diﬀerent	O
relationship	O
among	O
the	O
three	O
condi-	O
tions	O
.	O
each	O
of	O
these	O
codings	O
would	O
produce	O
fundamentally	O
diﬀerent	O
linear	B
models	O
that	O
would	O
ultimately	O
lead	O
to	O
diﬀerent	O
sets	O
of	O
predictions	O
on	O
test	B
observations	O
.	O
if	O
the	O
response	B
variable	O
’	O
s	O
values	O
did	O
take	O
on	O
a	O
natural	B
ordering	O
,	O
such	O
as	O
mild	O
,	O
moderate	O
,	O
and	O
severe	O
,	O
and	O
we	O
felt	O
the	O
gap	O
between	O
mild	O
and	O
moderate	O
was	O
similar	O
to	O
the	O
gap	O
between	O
moderate	O
and	O
severe	O
,	O
then	O
a	O
1	O
,	O
2	O
,	O
3	O
coding	O
would	O
be	O
reasonable	O
.	O
unfortunately	O
,	O
in	O
general	O
there	O
is	O
no	O
natural	B
way	O
to	O
convert	O
a	O
qualitative	B
response	O
variable	B
with	O
more	O
than	O
two	O
levels	O
into	O
a	O
quantitative	B
response	O
that	O
is	O
ready	O
for	O
linear	O
regression	B
.	O
for	O
a	O
binary	B
(	O
two	O
level	B
)	O
qualitative	B
response	O
,	O
the	O
situation	O
is	O
better	O
.	O
for	O
instance	O
,	O
perhaps	O
there	O
are	O
only	O
two	O
possibilities	O
for	O
the	O
patient	O
’	O
s	O
med-	O
ical	O
condition	O
:	O
stroke	O
and	O
drug	O
overdose	O
.	O
we	O
could	O
then	O
potentially	O
use	O
the	O
dummy	B
variable	I
approach	O
from	O
section	O
3.3.1	O
to	O
code	O
the	O
response	B
as	O
follows	O
:	O
(	O
cid:30	O
)	O
binary	B
y	O
=	O
0	O
if	O
stroke	O
;	O
1	O
if	O
drug	O
overdose	O
.	O
we	O
could	O
then	O
ﬁt	B
a	O
linear	B
regression	I
to	O
this	O
binary	B
response	O
,	O
and	O
predict	O
drug	O
overdose	O
if	O
ˆy	O
>	O
0.5	O
and	O
stroke	O
otherwise	O
.	O
in	O
the	O
binary	B
case	O
it	O
is	O
not	O
hard	O
to	O
show	O
that	O
even	O
if	O
we	O
ﬂip	O
the	O
above	O
coding	O
,	O
linear	B
regression	I
will	O
produce	O
the	O
same	O
ﬁnal	O
predictions	O
.	O
for	O
a	O
binary	B
response	O
with	O
a	O
0/1	O
coding	O
as	O
above	O
,	O
regression	B
by	O
least	B
squares	I
does	O
make	O
sense	O
;	O
it	O
can	O
be	O
shown	O
that	O
the	O
x	O
ˆβ	O
obtained	O
using	O
linear	B
regression	I
is	O
in	O
fact	O
an	O
estimate	O
of	O
pr	O
(	O
drug	O
overdose|x	O
)	O
in	O
this	O
special	O
case	O
.	O
however	O
,	O
if	O
we	O
use	O
linear	B
regression	I
,	O
some	O
of	O
our	O
estimates	O
might	O
be	O
outside	O
the	O
[	O
0	O
,	O
1	O
]	O
interval	B
(	O
see	O
figure	O
4.2	O
)	O
,	O
making	O
them	O
hard	O
to	O
interpret	O
as	O
probabilities	O
!	O
nevertheless	O
,	O
the	O
predictions	O
provide	O
an	O
ordering	O
and	O
can	O
be	O
interpreted	O
as	O
crude	O
probability	B
estimates	O
.	O
curiously	O
,	O
it	O
turns	O
out	O
that	O
the	O
classiﬁcations	O
that	O
we	O
get	O
if	O
we	O
use	O
linear	B
regression	I
to	O
predict	O
a	O
binary	B
response	O
will	O
be	O
the	O
same	O
as	O
for	O
the	O
linear	B
discriminant	I
analysis	I
(	O
lda	O
)	O
procedure	O
we	O
discuss	O
in	O
section	O
4.4.	O
however	O
,	O
the	O
dummy	B
variable	I
approach	O
can	O
not	O
be	O
easily	O
extended	O
to	O
accommodate	O
qualitative	B
responses	O
with	O
more	O
than	O
two	O
levels	O
.	O
for	O
these	O
reasons	O
,	O
it	O
is	O
preferable	O
to	O
use	O
a	O
classiﬁcation	B
method	O
that	O
is	O
truly	O
suited	O
for	O
qualitative	O
response	B
values	O
,	O
such	O
as	O
the	O
ones	O
presented	O
next	O
.	O
4.3	O
logistic	B
regression	I
consider	O
again	O
the	O
default	O
data	B
set	O
,	O
where	O
the	O
response	B
default	O
falls	O
into	O
one	O
of	O
two	O
categories	O
,	O
yes	O
or	O
no	O
.	O
rather	O
than	O
modeling	O
this	O
response	B
y	O
directly	O
,	O
logistic	B
regression	I
models	O
the	O
probability	B
that	O
y	O
belongs	O
to	O
a	O
par-	O
ticular	O
category	O
.	O
t	O
l	O
f	O
u	O
a	O
e	O
d	O
f	O
o	O
y	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
0	O
.	O
1	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
||	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
t	O
l	O
f	O
u	O
a	O
e	O
d	O
f	O
o	O
y	O
t	O
i	O
l	O
i	O
b	O
a	O
b	O
o	O
r	O
p	O
0	O
.	O
1	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
4.3	O
logistic	B
regression	I
131	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
||	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
0	O
500	O
1000	O
1500	O
2000	O
2500	O
0	O
500	O
1000	O
1500	O
2000	O
2500	O
balance	O
balance	O
figure	O
4.2.	O
classiﬁcation	B
using	O
the	O
default	O
data	B
.	O
left	O
:	O
estimated	O
probabil-	O
ity	O
of	O
default	O
using	O
linear	B
regression	I
.	O
some	O
estimated	O
probabilities	O
are	O
negative	O
!	O
the	O
orange	O
ticks	O
indicate	O
the	O
0/1	O
values	O
coded	O
for	O
default	O
(	O
no	O
or	O
yes	O
)	O
.	O
right	O
:	O
predicted	O
probabilities	O
of	O
default	O
using	O
logistic	B
regression	I
.	O
all	O
probabilities	O
lie	O
between	O
0	O
and	O
1.	O
for	O
the	O
default	O
data	B
,	O
logistic	B
regression	I
models	O
the	O
probability	B
of	O
default	O
.	O
for	O
example	O
,	O
the	O
probability	B
of	O
default	O
given	O
balance	O
can	O
be	O
written	O
as	O
pr	O
(	O
default	O
=	O
yes|balance	O
)	O
.	O
the	O
values	O
of	O
pr	O
(	O
default	O
=	O
yes|balance	O
)	O
,	O
which	O
we	O
abbreviate	O
p	O
(	O
balance	O
)	O
,	O
will	O
range	O
between	O
0	O
and	O
1.	O
then	O
for	O
any	O
given	O
value	O
of	O
balance	O
,	O
a	O
prediction	B
can	O
be	O
made	O
for	O
default	O
.	O
for	O
example	O
,	O
one	O
might	O
predict	O
default	O
=	O
yes	O
for	O
any	O
individual	O
for	O
whom	O
p	O
(	O
balance	O
)	O
>	O
0.5.	O
alterna-	O
tively	O
,	O
if	O
a	O
company	O
wishes	O
to	O
be	O
conservative	O
in	O
predicting	O
individuals	O
who	O
are	O
at	O
risk	O
for	O
default	O
,	O
then	O
they	O
may	O
choose	O
to	O
use	O
a	O
lower	O
threshold	O
,	O
such	O
as	O
p	O
(	O
balance	O
)	O
>	O
0.1	O
.	O
4.3.1	O
the	O
logistic	B
model	O
how	O
should	O
we	O
model	B
the	O
relationship	O
between	O
p	O
(	O
x	O
)	O
=	O
pr	O
(	O
y	O
=	O
1|x	O
)	O
and	O
x	O
?	O
(	O
for	O
convenience	O
we	O
are	O
using	O
the	O
generic	O
0/1	O
coding	O
for	O
the	O
response	B
)	O
.	O
in	O
section	O
4.2	O
we	O
talked	O
of	O
using	O
a	O
linear	B
regression	I
model	O
to	O
represent	O
these	O
probabilities	O
:	O
p	O
(	O
x	O
)	O
=	O
β0	O
+	O
β1x	O
.	O
(	O
4.1	O
)	O
if	O
we	O
use	O
this	O
approach	B
to	O
predict	O
default=yes	O
using	O
balance	O
,	O
then	O
we	O
obtain	O
the	O
model	B
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
4.2.	O
here	O
we	O
see	O
the	O
problem	O
with	O
this	O
approach	B
:	O
for	O
balances	O
close	O
to	O
zero	O
we	O
predict	O
a	O
negative	O
probability	O
of	O
default	O
;	O
if	O
we	O
were	O
to	O
predict	O
for	O
very	O
large	O
balances	O
,	O
we	O
would	O
get	O
values	O
bigger	O
than	O
1.	O
these	O
predictions	O
are	O
not	O
sensible	O
,	O
since	O
of	O
course	O
the	O
true	O
probability	O
of	O
default	O
,	O
regardless	O
of	O
credit	O
card	O
balance	O
,	O
must	O
fall	O
between	O
0	O
and	O
1.	O
this	O
problem	O
is	O
not	O
unique	O
to	O
the	O
credit	O
default	O
data	B
.	O
any	O
time	O
a	O
straight	O
line	B
is	O
ﬁt	B
to	O
a	O
binary	B
response	O
that	O
is	O
coded	O
as	O
132	O
4.	O
classiﬁcation	B
0	O
or	O
1	O
,	O
in	O
principle	O
we	O
can	O
always	O
predict	O
p	O
(	O
x	O
)	O
<	O
0	O
for	O
some	O
values	O
of	O
x	O
and	O
p	O
(	O
x	O
)	O
>	O
1	O
for	O
others	O
(	O
unless	O
the	O
range	O
of	O
x	O
is	O
limited	O
)	O
.	O
to	O
avoid	O
this	O
problem	O
,	O
we	O
must	O
model	B
p	O
(	O
x	O
)	O
using	O
a	O
function	B
that	O
gives	O
outputs	O
between	O
0	O
and	O
1	O
for	O
all	O
values	O
of	O
x.	O
many	O
functions	O
meet	O
this	O
description	O
.	O
in	O
logistic	B
regression	I
,	O
we	O
use	O
the	O
logistic	B
function	O
,	O
p	O
(	O
x	O
)	O
=	O
eβ0+β1x	O
1	O
+	O
eβ0+β1x	O
.	O
(	O
4.2	O
)	O
to	O
ﬁt	B
the	O
model	B
(	O
4.2	O
)	O
,	O
we	O
use	O
a	O
method	O
called	O
maximum	B
likelihood	I
,	O
which	O
we	O
discuss	O
in	O
the	O
next	O
section	O
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
4.2	O
illustrates	O
the	O
ﬁt	B
of	O
the	O
logistic	B
regression	I
model	O
to	O
the	O
default	O
data	B
.	O
notice	O
that	O
for	O
low	O
balances	O
we	O
now	O
predict	O
the	O
probability	B
of	O
default	O
as	O
close	O
to	O
,	O
but	O
never	O
below	O
,	O
zero	O
.	O
likewise	O
,	O
for	O
high	O
balances	O
we	O
predict	O
a	O
default	O
probability	B
close	O
to	O
,	O
but	O
never	O
above	O
,	O
one	O
.	O
the	O
logistic	B
function	O
will	O
always	O
produce	O
an	O
s-shaped	O
curve	O
of	O
this	O
form	O
,	O
and	O
so	O
regardless	O
of	O
the	O
value	O
of	O
x	O
,	O
we	O
will	O
obtain	O
a	O
sensible	O
prediction	B
.	O
we	O
also	O
see	O
that	O
the	O
logistic	B
model	O
is	O
better	O
able	O
to	O
capture	O
the	O
range	O
of	O
probabilities	O
than	O
is	O
the	O
linear	B
regression	I
model	O
in	O
the	O
left-hand	O
plot	B
.	O
the	O
average	B
ﬁtted	O
probability	B
in	O
both	O
cases	O
is	O
0.0333	O
(	O
averaged	O
over	O
the	O
training	B
data	O
)	O
,	O
which	O
is	O
the	O
same	O
as	O
the	O
overall	O
proportion	O
of	O
defaulters	O
in	O
the	O
data	B
set	O
.	O
after	O
a	O
bit	O
of	O
manipulation	O
of	O
(	O
4.2	O
)	O
,	O
we	O
ﬁnd	O
that	O
p	O
(	O
x	O
)	O
1	O
−	O
p	O
(	O
x	O
)	O
=	O
eβ0+β1x	O
.	O
(	O
4.3	O
)	O
the	O
quantity	O
p	O
(	O
x	O
)	O
/	O
[	O
1−	O
p	O
(	O
x	O
)	O
]	O
is	O
called	O
the	O
odds	B
,	O
and	O
can	O
take	O
on	O
any	O
value	O
between	O
0	O
and	O
∞	O
.	O
values	O
of	O
the	O
odds	B
close	O
to	O
0	O
and	O
∞	O
indicate	O
very	O
low	O
and	O
very	O
high	O
probabilities	O
of	O
default	O
,	O
respectively	O
.	O
for	O
example	O
,	O
on	O
average	B
1	O
in	O
5	O
people	O
with	O
an	O
odds	B
of	O
1/4	O
will	O
default	O
,	O
since	O
p	O
(	O
x	O
)	O
=	O
0.2	O
implies	O
an	O
0.2	O
1−0.2	O
=	O
1/4	O
.	O
likewise	O
on	O
average	B
nine	O
out	O
of	O
every	O
ten	O
people	O
with	O
odds	B
of	O
0.9	O
1−0.9	O
=	O
9.	O
an	O
odds	B
of	O
9	O
will	O
default	O
,	O
since	O
p	O
(	O
x	O
)	O
=	O
0.9	O
implies	O
an	O
odds	B
of	O
odds	B
are	O
traditionally	O
used	O
instead	O
of	O
probabilities	O
in	O
horse-racing	O
,	O
since	O
they	O
relate	O
more	O
naturally	O
to	O
the	O
correct	O
betting	O
strategy	O
.	O
logistic	B
function	O
maximum	B
likelihood	I
odds	O
by	O
taking	O
the	O
logarithm	O
of	O
both	O
sides	O
of	O
(	O
4.3	O
)	O
,	O
we	O
arrive	O
at	O
(	O
cid:11	O
)	O
(	O
cid:12	O
)	O
log	O
p	O
(	O
x	O
)	O
1	O
−	O
p	O
(	O
x	O
)	O
=	O
β0	O
+	O
β1x	O
.	O
(	O
4.4	O
)	O
the	O
left-hand	O
side	O
is	O
called	O
the	O
log-odds	O
or	O
logit	B
.	O
we	O
see	O
that	O
the	O
logistic	B
regression	I
model	O
(	O
4.2	O
)	O
has	O
a	O
logit	B
that	O
is	O
linear	B
in	O
x.	O
recall	B
from	O
chapter	O
3	O
that	O
in	O
a	O
linear	B
regression	I
model	O
,	O
β1	O
gives	O
the	O
average	B
change	O
in	O
y	O
associated	O
with	O
a	O
one-unit	O
increase	O
in	O
x.	O
in	O
contrast	B
,	O
in	O
a	O
logistic	B
regression	I
model	O
,	O
increasing	O
x	O
by	O
one	O
unit	O
changes	O
the	O
log	O
odds	B
by	O
β1	O
(	O
4.4	O
)	O
,	O
or	O
equivalently	O
it	O
multiplies	O
the	O
odds	B
by	O
eβ1	O
(	O
4.3	O
)	O
.	O
however	O
,	O
because	O
the	O
relationship	O
between	O
p	O
(	O
x	O
)	O
and	O
x	O
in	O
(	O
4.2	O
)	O
is	O
not	O
a	O
straight	O
line	B
,	O
log-odds	O
logit	B
4.3	O
logistic	B
regression	I
133	O
β1	O
does	O
not	O
correspond	O
to	O
the	O
change	O
in	O
p	O
(	O
x	O
)	O
associated	O
with	O
a	O
one-unit	O
increase	O
in	O
x.	O
the	O
amount	O
that	O
p	O
(	O
x	O
)	O
changes	O
due	O
to	O
a	O
one-unit	O
change	O
in	O
x	O
will	O
depend	O
on	O
the	O
current	O
value	O
of	O
x.	O
but	O
regardless	O
of	O
the	O
value	O
of	O
x	O
,	O
if	O
β1	O
is	O
positive	O
then	O
increasing	O
x	O
will	O
be	O
associated	O
with	O
increasing	O
p	O
(	O
x	O
)	O
,	O
and	O
if	O
β1	O
is	O
negative	O
then	O
increasing	O
x	O
will	O
be	O
associated	O
with	O
decreasing	O
p	O
(	O
x	O
)	O
.	O
the	O
fact	O
that	O
there	O
is	O
not	O
a	O
straight-line	O
relationship	O
between	O
p	O
(	O
x	O
)	O
and	O
x	O
,	O
and	O
the	O
fact	O
that	O
the	O
rate	B
of	O
change	O
in	O
p	O
(	O
x	O
)	O
per	O
unit	O
change	O
in	O
x	O
depends	O
on	O
the	O
current	O
value	O
of	O
x	O
,	O
can	O
also	O
be	O
seen	O
by	O
inspection	O
of	O
the	O
right-hand	O
panel	O
of	O
figure	O
4.2	O
.	O
4.3.2	O
estimating	O
the	O
regression	B
coeﬃcients	O
the	O
coeﬃcients	O
β0	O
and	O
β1	O
in	O
(	O
4.2	O
)	O
are	O
unknown	O
,	O
and	O
must	O
be	O
estimated	O
based	O
on	O
the	O
available	O
training	B
data	O
.	O
in	O
chapter	O
3	O
,	O
we	O
used	O
the	O
least	B
squares	I
approach	O
to	O
estimate	O
the	O
unknown	O
linear	B
regression	I
coeﬃcients	O
.	O
although	O
we	O
could	O
use	O
(	O
non-linear	B
)	O
least	B
squares	I
to	O
ﬁt	B
the	O
model	B
(	O
4.4	O
)	O
,	O
the	O
more	O
general	O
method	O
of	O
maximum	B
likelihood	I
is	O
preferred	O
,	O
since	O
it	O
has	O
better	O
sta-	O
tistical	O
properties	O
.	O
the	O
basic	O
intuition	O
behind	O
using	O
maximum	B
likelihood	I
to	O
ﬁt	B
a	O
logistic	B
regression	I
model	O
is	O
as	O
follows	O
:	O
we	O
seek	O
estimates	O
for	O
β0	O
and	O
β1	O
such	O
that	O
the	O
predicted	O
probability	B
ˆp	O
(	O
xi	O
)	O
of	O
default	O
for	O
each	O
individual	O
,	O
using	O
(	O
4.2	O
)	O
,	O
corresponds	O
as	O
closely	O
as	O
possible	O
to	O
the	O
individual	O
’	O
s	O
observed	O
default	O
status	O
.	O
in	O
other	O
words	O
,	O
we	O
try	O
to	O
ﬁnd	O
ˆβ0	O
and	O
ˆβ1	O
such	O
that	O
plugging	O
these	O
estimates	O
into	O
the	O
model	B
for	O
p	O
(	O
x	O
)	O
,	O
given	O
in	O
(	O
4.2	O
)	O
,	O
yields	O
a	O
number	O
close	O
to	O
one	O
for	O
all	O
individuals	O
who	O
defaulted	O
,	O
and	O
a	O
number	O
close	O
to	O
zero	O
for	O
all	O
individuals	O
who	O
did	O
not	O
.	O
this	O
intuition	O
can	O
be	O
formalized	O
using	O
a	O
mathematical	O
equation	O
called	O
a	O
likelihood	B
function	I
:	O
&	O
&	O
(	O
cid:6	O
)	O
(	O
β0	O
,	O
β1	O
)	O
=	O
p	O
(	O
xi	O
)	O
i	O
:	O
yi=1	O
i	O
(	O
cid:2	O
)	O
:	O
yi	O
(	O
cid:2	O
)	O
=0	O
(	O
1	O
−	O
p	O
(	O
xi	O
(	O
cid:2	O
)	O
)	O
)	O
.	O
likelihood	B
function	I
(	O
4.5	O
)	O
the	O
estimates	O
ˆβ0	O
and	O
ˆβ1	O
are	O
chosen	O
to	O
maximize	O
this	O
likelihood	B
function	I
.	O
maximum	B
likelihood	I
is	O
a	O
very	O
general	O
approach	B
that	O
is	O
used	O
to	O
ﬁt	B
many	O
of	O
the	O
non-linear	B
models	O
that	O
we	O
examine	O
throughout	O
this	O
book	O
.	O
in	O
the	O
linear	B
regression	I
setting	O
,	O
the	O
least	B
squares	I
approach	O
is	O
in	O
fact	O
a	O
special	O
case	O
of	O
maximum	B
likelihood	I
.	O
the	O
mathematical	O
details	O
of	O
maximum	B
likelihood	I
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
however	O
,	O
in	O
general	O
,	O
logistic	B
regression	I
and	O
other	O
models	O
can	O
be	O
easily	O
ﬁt	B
using	O
a	O
statistical	O
software	O
package	O
such	O
as	O
r	O
,	O
and	O
so	O
we	O
do	O
not	O
need	O
to	O
concern	O
ourselves	O
with	O
the	O
details	O
of	O
the	O
maximum	B
likelihood	I
ﬁtting	O
procedure	O
.	O
table	O
4.1	O
shows	O
the	O
coeﬃcient	B
estimates	O
and	O
related	O
information	O
that	O
result	O
from	O
ﬁtting	O
a	O
logistic	B
regression	I
model	O
on	O
the	O
default	O
data	B
in	O
order	O
to	O
predict	O
the	O
probability	B
of	O
default=yes	O
using	O
balance	O
.	O
we	O
see	O
that	O
ˆβ1	O
=	O
0.0055	O
;	O
this	O
indicates	O
that	O
an	O
increase	O
in	O
balance	O
is	O
associated	O
with	O
an	O
increase	O
in	O
the	O
probability	B
of	O
default	O
.	O
to	O
be	O
precise	O
,	O
a	O
one-unit	O
increase	O
in	O
balance	O
is	O
associated	O
with	O
an	O
increase	O
in	O
the	O
log	O
odds	B
of	O
default	O
by	O
0.0055	O
units	O
.	O
134	O
4.	O
classiﬁcation	B
intercept	O
balance	O
coeﬃcient	B
−10.6513	O
0.0055	O
std	O
.	O
error	B
z-statistic	O
0.3612	O
0.0002	O
p-value	B
−29.5	O
<	O
0.0001	O
24.9	O
<	O
0.0001	O
table	O
4.1.	O
for	O
the	O
default	O
data	B
,	O
estimated	O
coeﬃcients	O
of	O
the	O
logistic	B
regres-	O
sion	O
model	B
that	O
predicts	O
the	O
probability	B
of	O
default	O
using	O
balance	O
.	O
a	O
one-unit	O
increase	O
in	O
balance	O
is	O
associated	O
with	O
an	O
increase	O
in	O
the	O
log	O
odds	B
of	O
default	O
by	O
0.0055	O
units	O
.	O
many	O
aspects	O
of	O
the	O
logistic	B
regression	I
output	O
shown	O
in	O
table	O
4.1	O
are	O
similar	O
to	O
the	O
linear	B
regression	I
output	O
of	O
chapter	O
3.	O
for	O
example	O
,	O
we	O
can	O
measure	O
the	O
accuracy	O
of	O
the	O
coeﬃcient	B
estimates	O
by	O
computing	O
their	O
stan-	O
dard	O
errors	O
.	O
the	O
z-statistic	O
in	O
table	O
4.1	O
plays	O
the	O
same	O
role	O
as	O
the	O
t-statistic	B
in	O
the	O
linear	B
regression	I
output	O
,	O
for	O
example	O
in	O
table	O
3.1	O
on	O
page	O
68.	O
for	O
instance	O
,	O
the	O
z-statistic	O
associated	O
with	O
β1	O
is	O
equal	O
to	O
ˆβ1/se	O
(	O
ˆβ1	O
)	O
,	O
and	O
so	O
a	O
large	O
(	O
absolute	O
)	O
value	O
of	O
the	O
z-statistic	O
indicates	O
evidence	O
against	O
the	O
null	B
hypothesis	O
h0	O
:	O
β1	O
=	O
0.	O
this	O
null	B
hypothesis	O
implies	O
that	O
p	O
(	O
x	O
)	O
=	O
eβ0	O
1+eβ0	O
—	O
in	O
other	O
words	O
,	O
that	O
the	O
probability	B
of	O
default	O
does	O
not	O
depend	O
on	O
balance	O
.	O
since	O
the	O
p-value	B
associated	O
with	O
balance	O
in	O
table	O
4.1	O
is	O
tiny	O
,	O
we	O
can	O
reject	O
h0	O
.	O
in	O
other	O
words	O
,	O
we	O
conclude	O
that	O
there	O
is	O
indeed	O
an	O
association	O
between	O
balance	O
and	O
probability	B
of	O
default	O
.	O
the	O
estimated	O
intercept	B
in	O
table	O
4.1	O
is	O
typically	O
not	O
of	O
interest	O
;	O
its	O
main	O
purpose	O
is	O
to	O
adjust	O
the	O
average	B
ﬁtted	O
probabilities	O
to	O
the	O
proportion	O
of	O
ones	O
in	O
the	O
data	B
.	O
4.3.3	O
making	O
predictions	O
once	O
the	O
coeﬃcients	O
have	O
been	O
estimated	O
,	O
it	O
is	O
a	O
simple	B
matter	O
to	O
compute	O
the	O
probability	B
of	O
default	O
for	O
any	O
given	O
credit	O
card	O
balance	O
.	O
for	O
example	O
,	O
using	O
the	O
coeﬃcient	B
estimates	O
given	O
in	O
table	O
4.1	O
,	O
we	O
predict	O
that	O
the	O
default	O
probability	B
for	O
an	O
individual	O
with	O
a	O
balance	O
of	O
$	O
1	O
,	O
000	O
is	O
ˆp	O
(	O
x	O
)	O
=	O
e	O
ˆβ0+	O
ˆβ1x	O
1	O
+	O
e	O
ˆβ0+	O
ˆβ1x	O
=	O
−10.6513+0.0055×1,000	O
e	O
1	O
+	O
e−10.6513+0.0055×1,000	O
=	O
0.00576	O
,	O
which	O
is	O
below	O
1	O
%	O
.	O
in	O
contrast	B
,	O
the	O
predicted	O
probability	B
of	O
default	O
for	O
an	O
individual	O
with	O
a	O
balance	O
of	O
$	O
2	O
,	O
000	O
is	O
much	O
higher	O
,	O
and	O
equals	O
0.586	O
or	O
58.6	O
%	O
.	O
one	O
can	O
use	O
qualitative	B
predictors	O
with	O
the	O
logistic	B
regression	I
model	O
using	O
the	O
dummy	B
variable	I
approach	O
from	O
section	O
3.3.1.	O
as	O
an	O
example	O
,	O
the	O
default	O
data	B
set	O
contains	O
the	O
qualitative	B
variable	O
student	O
.	O
to	O
ﬁt	B
the	O
model	B
we	O
simply	O
create	O
a	O
dummy	B
variable	I
that	O
takes	O
on	O
a	O
value	O
of	O
1	O
for	O
students	O
and	O
0	O
for	O
non-students	O
.	O
the	O
logistic	B
regression	I
model	O
that	O
results	O
from	O
predicting	O
probability	B
of	O
default	O
from	O
student	O
status	O
can	O
be	O
seen	O
in	O
table	O
4.2.	O
the	O
coeﬃcient	B
associated	O
with	O
the	O
dummy	B
variable	I
is	O
positive	O
,	O
4.3	O
logistic	B
regression	I
135	O
intercept	B
student	O
[	O
yes	O
]	O
coeﬃcient	B
−3.5041	O
0.4049	O
std	O
.	O
error	B
z-statistic	O
0.0707	O
0.1150	O
p-value	B
−49.55	O
<	O
0.0001	O
0.0004	O
3.52	O
table	O
4.2.	O
for	O
the	O
default	O
data	B
,	O
estimated	O
coeﬃcients	O
of	O
the	O
logistic	B
regres-	O
sion	O
model	B
that	O
predicts	O
the	O
probability	B
of	O
default	O
using	O
student	O
status	O
.	O
student	O
status	O
is	O
encoded	O
as	O
a	O
dummy	B
variable	I
,	O
with	O
a	O
value	O
of	O
1	O
for	O
a	O
student	O
and	O
a	O
value	O
of	O
0	O
for	O
a	O
non-student	O
,	O
and	O
represented	O
by	O
the	O
variable	B
student	O
[	O
yes	O
]	O
in	O
the	O
table	O
.	O
and	O
the	O
associated	O
p-value	B
is	O
statistically	O
signiﬁcant	O
.	O
this	O
indicates	O
that	O
students	O
tend	O
to	O
have	O
higher	O
default	O
probabilities	O
than	O
non-students	O
:	O
(	O
cid:23	O
)	O
pr	O
(	O
default=yes|student=yes	O
)	O
=	O
(	O
cid:23	O
)	O
pr	O
(	O
default=yes|student=no	O
)	O
=	O
e	O
e	O
−3.5041+0.4049×1	O
1	O
+	O
e−3.5041+0.4049×1	O
=	O
0.0431	O
,	O
−3.5041+0.4049×0	O
1	O
+	O
e−3.5041+0.4049×0	O
=	O
0.0292	O
.	O
4.3.4	O
multiple	B
logistic	O
regression	B
we	O
now	O
consider	O
the	O
problem	O
of	O
predicting	O
a	O
binary	B
response	O
using	O
multiple	B
predictors	O
.	O
by	O
analogy	O
with	O
the	O
extension	O
from	O
simple	B
to	O
multiple	B
linear	O
regression	B
in	O
chapter	O
3	O
,	O
we	O
can	O
generalize	O
(	O
4.4	O
)	O
as	O
follows	O
:	O
(	O
cid:11	O
)	O
(	O
cid:12	O
)	O
log	O
p	O
(	O
x	O
)	O
1	O
−	O
p	O
(	O
x	O
)	O
=	O
β0	O
+	O
β1x1	O
+	O
···	O
+	O
βpxp	O
,	O
(	O
4.6	O
)	O
where	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
)	O
are	O
p	O
predictors	O
.	O
equation	O
4.6	O
can	O
be	O
rewritten	O
as	O
p	O
(	O
x	O
)	O
=	O
eβ0+β1x1+···+βpxp	O
1	O
+	O
eβ0+β1x1+···+βpxp	O
.	O
(	O
4.7	O
)	O
just	O
as	O
in	O
section	O
4.3.2	O
,	O
we	O
use	O
the	O
maximum	B
likelihood	I
method	O
to	O
estimate	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
.	O
table	O
4.3	O
shows	O
the	O
coeﬃcient	B
estimates	O
for	O
a	O
logistic	B
regression	I
model	O
that	O
uses	O
balance	O
,	O
income	O
(	O
in	O
thousands	O
of	O
dollars	O
)	O
,	O
and	O
student	O
status	O
to	O
predict	O
probability	B
of	O
default	O
.	O
there	O
is	O
a	O
surprising	O
result	O
here	O
.	O
the	O
p-	O
values	O
associated	O
with	O
balance	O
and	O
the	O
dummy	B
variable	I
for	O
student	O
status	O
are	O
very	O
small	O
,	O
indicating	O
that	O
each	O
of	O
these	O
variables	O
is	O
associated	O
with	O
the	O
probability	B
of	O
default	O
.	O
however	O
,	O
the	O
coeﬃcient	B
for	O
the	O
dummy	B
variable	I
is	O
negative	O
,	O
indicating	O
that	O
students	O
are	O
less	O
likely	O
to	O
default	O
than	O
non-	O
students	O
.	O
in	O
contrast	B
,	O
the	O
coeﬃcient	B
for	O
the	O
dummy	B
variable	I
is	O
positive	O
in	O
table	O
4.2.	O
how	O
is	O
it	O
possible	O
for	O
student	O
status	O
to	O
be	O
associated	O
with	O
an	O
increase	O
in	O
probability	B
of	O
default	O
in	O
table	O
4.2	O
and	O
a	O
decrease	O
in	O
probability	B
of	O
default	O
in	O
table	O
4.3	O
?	O
the	O
left-hand	O
panel	O
of	O
figure	O
4.3	O
provides	O
a	O
graph-	O
ical	O
illustration	O
of	O
this	O
apparent	O
paradox	O
.	O
the	O
orange	O
and	O
blue	O
solid	O
lines	O
show	O
the	O
average	B
default	O
rates	O
for	O
students	O
and	O
non-students	O
,	O
respectively	O
,	O
136	O
4.	O
classiﬁcation	B
intercept	O
balance	O
income	O
student	O
[	O
yes	O
]	O
coeﬃcient	B
−10.8690	O
0.0057	O
0.0030	O
−0.6468	O
std	O
.	O
error	B
z-statistic	O
0.4923	O
0.0002	O
0.0082	O
0.2362	O
p-value	B
−22.08	O
<	O
0.0001	O
24.74	O
<	O
0.0001	O
0.7115	O
0.37	O
−2.74	O
0.0062	O
table	O
4.3.	O
for	O
the	O
default	O
data	B
,	O
estimated	O
coeﬃcients	O
of	O
the	O
logistic	B
regres-	O
sion	O
model	B
that	O
predicts	O
the	O
probability	B
of	O
default	O
using	O
balance	O
,	O
income	O
,	O
and	O
student	O
status	O
.	O
student	O
status	O
is	O
encoded	O
as	O
a	O
dummy	B
variable	I
student	O
[	O
yes	O
]	O
,	O
with	O
a	O
value	O
of	O
1	O
for	O
a	O
student	O
and	O
a	O
value	O
of	O
0	O
for	O
a	O
non-student	O
.	O
in	O
ﬁtting	O
this	O
model	B
,	O
income	O
was	O
measured	O
in	O
thousands	O
of	O
dollars	O
.	O
as	O
a	O
function	B
of	O
credit	O
card	O
balance	O
.	O
the	O
negative	O
coeﬃcient	O
for	O
student	O
in	O
the	O
multiple	B
logistic	O
regression	B
indicates	O
that	O
for	O
a	O
ﬁxed	O
value	O
of	O
balance	O
and	O
income	O
,	O
a	O
student	O
is	O
less	O
likely	O
to	O
default	O
than	O
a	O
non-student	O
.	O
indeed	O
,	O
we	O
observe	O
from	O
the	O
left-hand	O
panel	O
of	O
figure	O
4.3	O
that	O
the	O
student	O
default	O
rate	B
is	O
at	O
or	O
below	O
that	O
of	O
the	O
non-student	O
default	O
rate	B
for	O
every	O
value	O
of	O
balance	O
.	O
but	O
the	O
horizontal	O
broken	O
lines	O
near	O
the	O
base	O
of	O
the	O
plot	B
,	O
which	O
show	O
the	O
default	O
rates	O
for	O
students	O
and	O
non-students	O
averaged	O
over	O
all	O
val-	O
ues	O
of	O
balance	O
and	O
income	O
,	O
suggest	O
the	O
opposite	O
eﬀect	O
:	O
the	O
overall	O
student	O
default	O
rate	B
is	O
higher	O
than	O
the	O
non-student	O
default	O
rate	B
.	O
consequently	O
,	O
there	O
is	O
a	O
positive	O
coeﬃcient	O
for	O
student	O
in	O
the	O
single	B
variable	O
logistic	B
regression	I
output	O
shown	O
in	O
table	O
4.2.	O
the	O
right-hand	O
panel	O
of	O
figure	O
4.3	O
provides	O
an	O
explanation	O
for	O
this	O
dis-	O
crepancy	O
.	O
the	O
variables	O
student	O
and	O
balance	O
are	O
correlated	O
.	O
students	O
tend	O
to	O
hold	O
higher	O
levels	O
of	O
debt	O
,	O
which	O
is	O
in	O
turn	O
associated	O
with	O
higher	O
prob-	O
ability	O
of	O
default	O
.	O
in	O
other	O
words	O
,	O
students	O
are	O
more	O
likely	O
to	O
have	O
large	O
credit	O
card	O
balances	O
,	O
which	O
,	O
as	O
we	O
know	O
from	O
the	O
left-hand	O
panel	O
of	O
fig-	O
ure	O
4.3	O
,	O
tend	O
to	O
be	O
associated	O
with	O
high	O
default	O
rates	O
.	O
thus	O
,	O
even	O
though	O
an	O
individual	O
student	O
with	O
a	O
given	O
credit	O
card	O
balance	O
will	O
tend	O
to	O
have	O
a	O
lower	O
probability	B
of	O
default	O
than	O
a	O
non-student	O
with	O
the	O
same	O
credit	O
card	O
balance	O
,	O
the	O
fact	O
that	O
students	O
on	O
the	O
whole	O
tend	O
to	O
have	O
higher	O
credit	O
card	O
balances	O
means	O
that	O
overall	O
,	O
students	O
tend	O
to	O
default	O
at	O
a	O
higher	O
rate	B
than	O
non-students	O
.	O
this	O
is	O
an	O
important	O
distinction	O
for	O
a	O
credit	O
card	O
company	O
that	O
is	O
trying	O
to	O
determine	O
to	O
whom	O
they	O
should	O
oﬀer	O
credit	O
.	O
a	O
student	O
is	O
riskier	O
than	O
a	O
non-student	O
if	O
no	O
information	O
about	O
the	O
student	O
’	O
s	O
credit	O
card	O
balance	O
is	O
available	O
.	O
however	O
,	O
that	O
student	O
is	O
less	O
risky	O
than	O
a	O
non-student	O
with	O
the	O
same	O
credit	O
card	O
balance	O
!	O
this	O
simple	B
example	O
illustrates	O
the	O
dangers	O
and	O
subtleties	O
associated	O
with	O
performing	O
regressions	O
involving	O
only	O
a	O
single	B
predictor	O
when	O
other	O
predictors	O
may	O
also	O
be	O
relevant	O
.	O
as	O
in	O
the	O
linear	B
regression	I
setting	O
,	O
the	O
results	O
obtained	O
using	O
one	O
predictor	B
may	O
be	O
quite	O
diﬀerent	O
from	O
those	O
ob-	O
tained	O
using	O
multiple	B
predictors	O
,	O
especially	O
when	O
there	O
is	O
correlation	B
among	O
the	O
predictors	O
.	O
in	O
general	O
,	O
the	O
phenomenon	O
seen	O
in	O
figure	O
4.3	O
is	O
known	O
as	O
confounding	B
.	O
confounding	B
e	O
t	O
a	O
r	O
t	O
l	O
f	O
u	O
a	O
e	O
d	O
8	O
0	O
.	O
6	O
0	O
.	O
4	O
.	O
0	O
2	O
0	O
.	O
0	O
0	O
.	O
4.3	O
logistic	B
regression	I
137	O
0	O
0	O
5	O
2	O
0	O
0	O
0	O
2	O
0	O
0	O
5	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
5	O
0	O
l	O
e	O
c	O
n	O
a	O
a	O
b	O
d	O
r	O
a	O
c	O
t	O
i	O
d	O
e	O
r	O
c	O
500	O
1000	O
1500	O
2000	O
credit	O
card	O
balance	O
no	O
yes	O
student	O
status	O
figure	O
4.3.	O
confounding	B
in	O
the	O
default	O
data	B
.	O
left	O
:	O
default	O
rates	O
are	O
shown	O
for	O
students	O
(	O
orange	O
)	O
and	O
non-students	O
(	O
blue	O
)	O
.	O
the	O
solid	O
lines	O
display	O
default	O
rate	B
as	O
a	O
function	B
of	O
balance	O
,	O
while	O
the	O
horizontal	O
broken	O
lines	O
display	O
the	O
overall	O
default	O
rates	O
.	O
right	O
:	O
boxplots	O
of	O
balance	O
for	O
students	O
(	O
orange	O
)	O
and	O
non-students	O
(	O
blue	O
)	O
are	O
shown	O
.	O
by	O
substituting	O
estimates	O
for	O
the	O
regression	B
coeﬃcients	O
from	O
table	O
4.3	O
into	O
(	O
4.7	O
)	O
,	O
we	O
can	O
make	O
predictions	O
.	O
for	O
example	O
,	O
a	O
student	O
with	O
a	O
credit	O
card	O
balance	O
of	O
$	O
1	O
,	O
500	O
and	O
an	O
income	O
of	O
$	O
40	O
,	O
000	O
has	O
an	O
estimated	O
proba-	O
bility	O
of	O
default	O
of	O
−10.869+0.00574×1,500+0.003×40−0.6468×1	O
ˆp	O
(	O
x	O
)	O
=	O
1	O
+	O
e−10.869+0.00574×1,500+0.003×40−0.6468×1	O
=	O
0.058	O
.	O
(	O
4.8	O
)	O
e	O
e	O
a	O
non-student	O
with	O
the	O
same	O
balance	O
and	O
income	O
has	O
an	O
estimated	O
prob-	O
ability	O
of	O
default	O
of	O
−10.869+0.00574×1,500+0.003×40−0.6468×0	O
ˆp	O
(	O
x	O
)	O
=	O
1	O
+	O
e−10.869+0.00574×1,500+0.003×40−0.6468×0	O
=	O
0.105	O
.	O
(	O
4.9	O
)	O
(	O
here	O
we	O
multiply	O
the	O
income	O
coeﬃcient	B
estimate	O
from	O
table	O
4.3	O
by	O
40	O
,	O
rather	O
than	O
by	O
40,000	O
,	O
because	O
in	O
that	O
table	O
the	O
model	B
was	O
ﬁt	B
with	O
income	O
measured	O
in	O
units	O
of	O
$	O
1	O
,	O
000	O
.	O
)	O
4.3.5	O
logistic	B
regression	I
for	O
>	O
2	O
response	B
classes	O
we	O
sometimes	O
wish	O
to	O
classify	O
a	O
response	B
variable	O
that	O
has	O
more	O
than	O
two	O
classes	O
.	O
for	O
example	O
,	O
in	O
section	O
4.2	O
we	O
had	O
three	O
categories	O
of	O
medical	O
con-	O
dition	O
in	O
the	O
emergency	O
room	O
:	O
stroke	O
,	O
drug	O
overdose	O
,	O
epileptic	O
seizure	O
.	O
in	O
this	O
setting	O
,	O
we	O
wish	O
to	O
model	B
both	O
pr	O
(	O
y	O
=	O
stroke|x	O
)	O
and	O
pr	O
(	O
y	O
=	O
drug	O
overdose|x	O
)	O
,	O
with	O
the	O
remaining	O
pr	O
(	O
y	O
=	O
epileptic	O
seizure|x	O
)	O
=	O
1	O
−	O
pr	O
(	O
y	O
=	O
stroke|x	O
)	O
−	O
pr	O
(	O
y	O
=	O
drug	O
overdose|x	O
)	O
.	O
the	O
two-class	O
logis-	O
tic	O
regression	B
models	O
discussed	O
in	O
the	O
previous	O
sections	O
have	O
multiple-class	O
extensions	O
,	O
but	O
in	O
practice	O
they	O
tend	O
not	O
to	O
be	O
used	O
all	O
that	O
often	O
.	O
one	O
of	O
the	O
reasons	O
is	O
that	O
the	O
method	O
we	O
discuss	O
in	O
the	O
next	O
section	O
,	O
discriminant	O
138	O
4.	O
classiﬁcation	B
analysis	O
,	O
is	O
popular	O
for	O
multiple-class	O
classiﬁcation	B
.	O
so	O
we	O
do	O
not	O
go	O
into	O
the	O
details	O
of	O
multiple-class	O
logistic	B
regression	I
here	O
,	O
but	O
simply	O
note	O
that	O
such	O
an	O
approach	B
is	O
possible	O
,	O
and	O
that	O
software	O
for	O
it	O
is	O
available	O
in	O
r.	O
4.4	O
linear	B
discriminant	I
analysis	I
logistic	O
regression	B
involves	O
directly	O
modeling	O
pr	O
(	O
y	O
=	O
k|x	O
=	O
x	O
)	O
using	O
the	O
logistic	B
function	O
,	O
given	O
by	O
(	O
4.7	O
)	O
for	O
the	O
case	O
of	O
two	O
response	B
classes	O
.	O
in	O
statistical	O
jargon	O
,	O
we	O
model	B
the	O
conditional	O
distribution	O
of	O
the	O
response	B
y	O
,	O
given	O
the	O
predictor	B
(	O
s	O
)	O
x.	O
we	O
now	O
consider	O
an	O
alternative	O
and	O
less	O
direct	O
approach	B
to	O
estimating	O
these	O
probabilities	O
.	O
in	O
this	O
alternative	O
approach	O
,	O
we	O
model	B
the	O
distribution	B
of	O
the	O
predictors	O
x	O
separately	O
in	O
each	O
of	O
the	O
response	B
classes	O
(	O
i.e	O
.	O
given	O
y	O
)	O
,	O
and	O
then	O
use	O
bayes	O
’	O
theorem	O
to	O
ﬂip	O
these	O
around	O
into	O
estimates	O
for	O
pr	O
(	O
y	O
=	O
k|x	O
=	O
x	O
)	O
.	O
when	O
these	O
distributions	O
are	O
assumed	O
to	O
be	O
normal	O
,	O
it	O
turns	O
out	O
that	O
the	O
model	B
is	O
very	O
similar	O
in	O
form	O
to	O
logistic	B
regression	I
.	O
why	O
do	O
we	O
need	O
another	O
method	O
,	O
when	O
we	O
have	O
logistic	B
regression	I
?	O
there	O
are	O
several	O
reasons	O
:	O
•	O
when	O
the	O
classes	O
are	O
well-separated	O
,	O
the	O
parameter	B
estimates	O
for	O
the	O
logistic	B
regression	I
model	O
are	O
surprisingly	O
unstable	O
.	O
linear	B
discrimi-	O
nant	O
analysis	B
does	O
not	O
suﬀer	O
from	O
this	O
problem	O
.	O
•	O
if	O
n	O
is	O
small	O
and	O
the	O
distribution	B
of	O
the	O
predictors	O
x	O
is	O
approximately	O
normal	O
in	O
each	O
of	O
the	O
classes	O
,	O
the	O
linear	O
discriminant	O
model	O
is	O
again	O
more	O
stable	O
than	O
the	O
logistic	B
regression	I
model	O
.	O
•	O
as	O
mentioned	O
in	O
section	O
4.3.5	O
,	O
linear	B
discriminant	I
analysis	I
is	O
popular	O
when	O
we	O
have	O
more	O
than	O
two	O
response	B
classes	O
.	O
4.4.1	O
using	O
bayes	O
’	O
theorem	O
for	O
classiﬁcation	O
suppose	O
that	O
we	O
wish	O
to	O
classify	O
an	O
observation	O
into	O
one	O
of	O
k	O
classes	O
,	O
where	O
k	O
≥	O
2.	O
in	O
other	O
words	O
,	O
the	O
qualitative	B
response	O
variable	B
y	O
can	O
take	O
on	O
k	O
possible	O
distinct	O
and	O
unordered	O
values	O
.	O
let	O
πk	O
represent	O
the	O
overall	O
or	O
prior	B
probability	O
that	O
a	O
randomly	O
chosen	O
observation	O
comes	O
from	O
the	O
kth	O
class	O
;	O
this	O
is	O
the	O
probability	B
that	O
a	O
given	O
observation	O
is	O
associated	O
with	O
the	O
kth	O
category	O
of	O
the	O
response	B
variable	O
y	O
.	O
let	O
fk	O
(	O
)	O
≡	O
pr	O
(	O
x	O
=	O
x|y	O
=	O
k	O
)	O
denote	O
x	O
1	O
prior	B
the	O
density	B
function	I
of	O
x	O
for	O
an	O
observation	O
that	O
comes	O
from	O
the	O
kth	O
class	O
.	O
in	O
other	O
words	O
,	O
fk	O
(	O
x	O
)	O
is	O
relatively	O
large	O
if	O
there	O
is	O
a	O
high	O
probability	B
that	O
an	O
observation	O
in	O
the	O
kth	O
class	O
has	O
x	O
≈	O
x	O
,	O
and	O
fk	O
(	O
x	O
)	O
is	O
small	O
if	O
it	O
is	O
very	O
density	B
function	I
1technically	O
this	O
definition	O
is	O
only	O
correct	O
if	O
x	O
is	O
continuous	B
then	O
region	O
dx	O
around	O
x.	O
fk	O
(	O
x	O
)	O
dx	O
would	O
correspond	O
to	O
the	O
probability	B
of	O
is	O
a	O
discrete	O
random	O
variabl.e	O
.	O
if	O
x	O
fa	O
ling	O
in	O
in	O
a	O
small	O
x	O
l	O
unlikely	O
that	O
an	O
observation	O
in	O
the	O
kth	O
class	O
has	O
x	O
≈	O
x.	O
then	O
bayes	O
’	O
theorem	O
states	O
that	O
4.4	O
linear	B
discriminant	I
analysis	I
139	O
pr	O
(	O
y	O
=	O
k|x	O
=	O
x	O
)	O
=	O
(	O
cid:10	O
)	O
πkfk	O
(	O
x	O
)	O
k	O
l=1	O
πlfl	O
(	O
x	O
)	O
.	O
(	O
4.10	O
)	O
in	O
accordance	O
with	O
our	O
earlier	O
notation	O
,	O
we	O
will	O
use	O
the	O
abbreviation	O
pk	O
(	O
x	O
)	O
=	O
pr	O
(	O
y	O
=	O
k|x	O
)	O
.	O
this	O
suggests	O
that	O
instead	O
of	O
directly	O
computing	O
pk	O
(	O
x	O
)	O
as	O
in	O
section	O
4.3.1	O
,	O
we	O
can	O
simply	O
plug	O
in	O
estimates	O
of	O
πk	O
and	O
fk	O
(	O
x	O
)	O
into	O
(	O
4.10	O
)	O
.	O
in	O
general	O
,	O
estimating	O
πk	O
is	O
easy	O
if	O
we	O
have	O
a	O
random	O
sample	O
of	O
y	O
s	O
from	O
the	O
population	O
:	O
we	O
simply	O
compute	O
the	O
fraction	O
of	O
the	O
training	B
observations	O
that	O
belong	O
to	O
the	O
kth	O
class	O
.	O
however	O
,	O
estimating	O
fk	O
(	O
x	O
)	O
tends	O
to	O
be	O
more	O
challenging	O
,	O
unless	O
we	O
assume	O
some	O
simple	B
forms	O
for	O
these	O
densities	O
.	O
we	O
refer	O
to	O
pk	O
(	O
x	O
)	O
as	O
the	O
posterior	B
probability	O
that	O
an	O
observation	O
x	O
=	O
x	O
belongs	O
to	O
the	O
kth	O
class	O
.	O
that	O
is	O
,	O
it	O
is	O
the	O
probability	B
that	O
the	O
observation	O
belongs	O
to	O
the	O
kth	O
class	O
,	O
given	O
the	O
predictor	B
value	O
for	O
that	O
observation	O
.	O
we	O
know	O
from	O
chapter	O
2	O
that	O
the	O
bayes	O
classiﬁer	B
,	O
which	O
classiﬁes	O
an	O
observation	O
to	O
the	O
class	O
for	O
which	O
pk	O
(	O
x	O
)	O
is	O
largest	O
,	O
has	O
the	O
lowest	O
possible	O
error	B
rate	I
out	O
of	O
all	O
classiﬁers	O
.	O
(	O
this	O
is	O
of	O
course	O
only	O
true	O
if	O
the	O
terms	O
in	O
(	O
4.10	O
)	O
are	O
all	O
correctly	O
speciﬁed	O
.	O
)	O
therefore	O
,	O
if	O
we	O
can	O
ﬁnd	O
a	O
way	O
to	O
estimate	O
fk	O
(	O
x	O
)	O
,	O
then	O
we	O
can	O
develop	O
a	O
classiﬁer	B
that	O
approximates	O
the	O
bayes	O
classiﬁer	B
.	O
such	O
an	O
approach	B
is	O
the	O
topic	O
of	O
the	O
following	O
sections	O
.	O
4.4.2	O
linear	B
discriminant	I
analysis	I
for	O
p	O
=	O
1	O
for	O
now	O
,	O
assume	O
that	O
p	O
=	O
1—that	O
is	O
,	O
we	O
have	O
only	O
one	O
predictor	B
.	O
we	O
would	O
like	O
to	O
obtain	O
an	O
estimate	O
for	O
fk	O
(	O
x	O
)	O
that	O
we	O
can	O
plug	O
into	O
(	O
4.10	O
)	O
in	O
order	O
to	O
estimate	O
pk	O
(	O
x	O
)	O
.	O
we	O
will	O
then	O
classify	O
an	O
observation	O
to	O
the	O
class	O
for	O
which	O
pk	O
(	O
x	O
)	O
is	O
greatest	O
.	O
in	O
order	O
to	O
estimate	O
fk	O
(	O
x	O
)	O
,	O
we	O
will	O
ﬁrst	O
make	O
some	O
assumptions	O
about	O
its	O
form	O
.	O
suppose	O
we	O
assume	O
that	O
fk	O
(	O
x	O
)	O
is	O
normal	O
or	O
gaussian	O
.	O
in	O
the	O
one-	O
(	O
cid:12	O
)	O
dimensional	O
setting	O
,	O
the	O
normal	O
density	O
takes	O
the	O
form	O
fk	O
(	O
x	O
)	O
=	O
1√	O
2πσk	O
exp	O
(	O
cid:11	O
)	O
−	O
1	O
2σ2	O
k	O
(	O
x	O
−	O
μk	O
)	O
2	O
,	O
(	O
4.11	O
)	O
bayes	O
’	O
theorem	O
posterior	B
normal	O
gaussian	O
where	O
μk	O
and	O
σ2	O
k	O
are	O
the	O
mean	O
and	O
variance	B
parameters	O
for	O
the	O
kth	O
class	O
.	O
for	O
now	O
,	O
let	O
us	O
further	O
assume	O
that	O
σ2	O
k	O
:	O
that	O
is	O
,	O
there	O
is	O
a	O
shared	O
variance	B
term	O
across	O
all	O
k	O
classes	O
,	O
which	O
for	O
simplicity	O
we	O
can	O
denote	O
by	O
σ2	O
.	O
plugging	O
(	O
4.11	O
)	O
into	O
(	O
4.10	O
)	O
,	O
we	O
ﬁnd	O
that	O
1	O
=	O
.	O
.	O
.	O
=	O
σ2	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
−	O
1	O
2σ2	O
(	O
x	O
−	O
μk	O
)	O
2	O
(	O
cid:8	O
)	O
−	O
1	O
2σ2	O
(	O
x	O
−	O
μl	O
)	O
2	O
exp	O
pk	O
(	O
x	O
)	O
=	O
(	O
cid:10	O
)	O
1√	O
πk	O
k	O
l=1	O
πl	O
2πσ	O
1√	O
exp	O
2πσ	O
(	O
cid:9	O
)	O
.	O
(	O
4.12	O
)	O
(	O
note	O
that	O
in	O
(	O
4.12	O
)	O
,	O
πk	O
denotes	O
the	O
prior	B
probability	O
that	O
an	O
observation	O
belongs	O
to	O
the	O
kth	O
class	O
,	O
not	O
to	O
be	O
confused	O
with	O
π	O
≈	O
3.14159	O
,	O
the	O
math-	O
ematical	O
constant	O
.	O
)	O
the	O
bayes	O
classiﬁer	B
involves	O
assigning	O
an	O
observation	O
140	O
4.	O
classiﬁcation	B
5	O
4	O
3	O
2	O
1	O
0	O
−4	O
−2	O
0	O
2	O
4	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
4	O
figure	O
4.4.	O
left	O
:	O
two	O
one-dimensional	O
normal	O
density	O
functions	O
are	O
shown	O
.	O
the	O
dashed	O
vertical	O
line	B
represents	O
the	O
bayes	O
decision	B
boundary	I
.	O
right	O
:	O
20	O
obser-	O
vations	O
were	O
drawn	O
from	O
each	O
of	O
the	O
two	O
classes	O
,	O
and	O
are	O
shown	O
as	O
histograms	O
.	O
the	O
bayes	O
decision	B
boundary	I
is	O
again	O
shown	O
as	O
a	O
dashed	O
vertical	O
line	B
.	O
the	O
solid	O
vertical	O
line	B
represents	O
the	O
lda	O
decision	B
boundary	I
estimated	O
from	O
the	O
training	B
data	O
.	O
x	O
=	O
x	O
to	O
the	O
class	O
for	O
which	O
(	O
4.12	O
)	O
is	O
largest	O
.	O
taking	O
the	O
log	O
of	O
(	O
4.12	O
)	O
and	O
rearranging	O
the	O
terms	O
,	O
it	O
is	O
not	O
hard	O
to	O
show	O
that	O
this	O
is	O
equivalent	O
to	O
assigning	O
the	O
observation	O
to	O
the	O
class	O
for	O
which	O
δk	O
(	O
x	O
)	O
=	O
x	O
·	O
μk	O
σ2	O
−	O
μ2	O
k	O
2σ2	O
+	O
log	O
(	O
πk	O
)	O
(	O
4.13	O
)	O
is	O
largest	O
.	O
for	O
instance	O
,	O
if	O
k	O
=	O
2	O
and	O
π1	O
=	O
π2	O
,	O
then	O
the	O
bayes	O
classiﬁer	B
assigns	O
an	O
observation	O
to	O
class	O
1	O
if	O
2x	O
(	O
μ1	O
−	O
μ2	O
)	O
>	O
μ2	O
2	O
,	O
and	O
to	O
class	O
2	O
otherwise	O
.	O
in	O
this	O
case	O
,	O
the	O
bayes	O
decision	B
boundary	I
corresponds	O
to	O
the	O
point	O
where	O
−	O
μ2	O
1	O
−	O
μ2	O
μ2	O
2	O
(	O
μ1	O
−	O
μ2	O
)	O
1	O
2	O
x	O
=	O
=	O
μ1	O
+	O
μ2	O
2	O
.	O
(	O
4.14	O
)	O
1	O
=	O
σ2	O
an	O
example	O
is	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
4.4.	O
the	O
two	O
normal	O
density	O
functions	O
that	O
are	O
displayed	O
,	O
f1	O
(	O
x	O
)	O
and	O
f2	O
(	O
x	O
)	O
,	O
represent	O
two	O
distinct	O
classes	O
.	O
the	O
mean	O
and	O
variance	B
parameters	O
for	O
the	O
two	O
density	O
functions	O
are	O
μ1	O
=	O
−1.25	O
,	O
μ2	O
=	O
1.25	O
,	O
and	O
σ2	O
2	O
=	O
1.	O
the	O
two	O
densities	O
overlap	O
,	O
and	O
so	O
given	O
that	O
x	O
=	O
x	O
,	O
there	O
is	O
some	O
uncertainty	O
about	O
the	O
class	O
to	O
which	O
the	O
observation	O
belongs	O
.	O
if	O
we	O
assume	O
that	O
an	O
observation	O
is	O
equally	O
likely	O
to	O
come	O
from	O
either	O
class—that	O
is	O
,	O
π1	O
=	O
π2	O
=	O
0.5—then	O
by	O
inspection	O
of	O
(	O
4.14	O
)	O
,	O
we	O
see	O
that	O
the	O
bayes	O
classiﬁer	B
assigns	O
the	O
observation	O
to	O
class	O
1	O
if	O
x	O
<	O
0	O
and	O
class	O
2	O
otherwise	O
.	O
note	O
that	O
in	O
this	O
case	O
,	O
we	O
can	O
compute	O
the	O
bayes	O
classiﬁer	B
because	O
we	O
know	O
that	O
x	O
is	O
drawn	O
from	O
a	O
gaussian	O
distribution	B
within	O
each	O
class	O
,	O
and	O
we	O
know	O
all	O
of	O
the	O
parameters	O
involved	O
.	O
in	O
a	O
real-life	O
situation	O
,	O
we	O
are	O
not	O
able	O
to	O
calculate	O
the	O
bayes	O
classiﬁer	B
.	O
in	O
practice	O
,	O
even	O
if	O
we	O
are	O
quite	O
certain	O
of	O
our	O
assumption	O
that	O
x	O
is	O
drawn	O
from	O
a	O
gaussian	O
distribution	B
within	O
each	O
class	O
,	O
we	O
still	O
have	O
to	O
estimate	O
the	O
parameters	O
μ1	O
,	O
.	O
.	O
.	O
,	O
μk	O
,	O
π1	O
,	O
.	O
.	O
.	O
,	O
πk	O
,	O
and	O
σ2	O
.	O
the	O
linear	O
discriminant	O
4.4	O
linear	B
discriminant	I
analysis	I
141	O
analysis	B
(	O
lda	O
)	O
method	O
approximates	O
the	O
bayes	O
classiﬁer	B
by	O
plugging	O
esti-	O
mates	O
for	O
πk	O
,	O
μk	O
,	O
and	O
σ2	O
into	O
(	O
4.13	O
)	O
.	O
in	O
particular	O
,	O
the	O
following	O
estimates	O
are	O
used	O
:	O
linear	B
discriminant	I
analysis	I
ˆμk	O
=	O
1	O
nk	O
(	O
cid:17	O
)	O
i	O
:	O
yi=k	O
xi	O
k	O
(	O
cid:17	O
)	O
ˆσ2	O
=	O
1	O
n	O
−	O
k	O
k=1	O
i	O
:	O
yi=k	O
(	O
cid:17	O
)	O
(	O
xi	O
−	O
ˆμk	O
)	O
2	O
(	O
4.15	O
)	O
where	O
n	O
is	O
the	O
total	O
number	O
of	O
training	B
observations	O
,	O
and	O
nk	O
is	O
the	O
number	O
of	O
training	B
observations	O
in	O
the	O
kth	O
class	O
.	O
the	O
estimate	O
for	O
μk	O
is	O
simply	O
the	O
average	B
of	O
all	O
the	O
training	B
observations	O
from	O
the	O
kth	O
class	O
,	O
while	O
ˆσ2	O
can	O
be	O
seen	O
as	O
a	O
weighted	B
average	O
of	O
the	O
sample	O
variances	O
for	O
each	O
of	O
the	O
k	O
classes	O
.	O
sometimes	O
we	O
have	O
knowledge	O
of	O
the	O
class	O
membership	O
probabili-	O
ties	O
π1	O
,	O
.	O
.	O
.	O
,	O
πk	O
,	O
which	O
can	O
be	O
used	O
directly	O
.	O
in	O
the	O
absence	O
of	O
any	O
additional	O
information	O
,	O
lda	O
estimates	O
πk	O
using	O
the	O
proportion	O
of	O
the	O
training	B
obser-	O
vations	O
that	O
belong	O
to	O
the	O
kth	O
class	O
.	O
in	O
other	O
words	O
,	O
ˆπk	O
=	O
nk/n	O
.	O
(	O
4.16	O
)	O
the	O
lda	O
classiﬁer	B
plugs	O
the	O
estimates	O
given	O
in	O
(	O
4.15	O
)	O
and	O
(	O
4.16	O
)	O
into	O
(	O
4.13	O
)	O
,	O
and	O
assigns	O
an	O
observation	O
x	O
=	O
x	O
to	O
the	O
class	O
for	O
which	O
ˆδk	O
(	O
x	O
)	O
=	O
x	O
·	O
ˆμk	O
ˆσ2	O
−	O
ˆμ2	O
k	O
2ˆσ2	O
+	O
log	O
(	O
ˆπk	O
)	O
(	O
4.17	O
)	O
discriminant	B
function	I
is	O
largest	O
.	O
the	O
word	O
linear	B
in	O
the	O
classiﬁer	B
’	O
s	O
name	O
stems	O
from	O
the	O
fact	O
that	O
the	O
discriminant	O
functions	O
ˆδk	O
(	O
x	O
)	O
in	O
(	O
4.17	O
)	O
are	O
linear	B
functions	O
of	O
x	O
(	O
as	O
opposed	O
to	O
a	O
more	O
complex	O
function	B
of	O
x	O
)	O
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
4.4	O
displays	O
a	O
histogram	B
of	O
a	O
random	O
sample	O
of	O
20	O
observations	B
from	O
each	O
class	O
.	O
to	O
implement	O
lda	O
,	O
we	O
began	O
by	O
estimating	O
πk	O
,	O
μk	O
,	O
and	O
σ2	O
using	O
(	O
4.15	O
)	O
and	O
(	O
4.16	O
)	O
.	O
we	O
then	O
computed	O
the	O
decision	B
boundary	I
,	O
shown	O
as	O
a	O
black	O
solid	O
line	B
,	O
that	O
results	O
from	O
assigning	O
an	O
observation	O
to	O
the	O
class	O
for	O
which	O
(	O
4.17	O
)	O
is	O
largest	O
.	O
all	O
points	O
to	O
the	O
left	O
of	O
this	O
line	B
will	O
be	O
assigned	O
to	O
the	O
green	O
class	O
,	O
while	O
points	O
to	O
the	O
right	O
of	O
this	O
line	B
are	O
assigned	O
to	O
the	O
purple	O
class	O
.	O
in	O
this	O
case	O
,	O
since	O
n1	O
=	O
n2	O
=	O
20	O
,	O
we	O
have	O
ˆπ1	O
=	O
ˆπ2	O
.	O
as	O
a	O
result	O
,	O
the	O
decision	B
boundary	I
corresponds	O
to	O
the	O
midpoint	O
between	O
the	O
sample	O
means	O
for	O
the	O
two	O
classes	O
,	O
(	O
ˆμ1	O
+	O
ˆμ2	O
)	O
/2	O
.	O
the	O
ﬁgure	O
indicates	O
that	O
the	O
lda	O
decision	B
boundary	I
is	O
slightly	O
to	O
the	O
left	O
of	O
the	O
optimal	O
bayes	O
decision	B
boundary	I
,	O
which	O
instead	O
equals	O
(	O
μ1	O
+	O
μ2	O
)	O
/2	O
=	O
0.	O
how	O
well	O
does	O
the	O
lda	O
classiﬁer	B
perform	O
on	O
this	O
data	B
?	O
since	O
this	O
is	O
simulated	O
data	B
,	O
we	O
can	O
generate	O
a	O
large	O
number	O
of	O
test	B
observations	O
in	O
order	O
to	O
compute	O
the	O
bayes	O
error	B
rate	I
and	O
the	O
lda	O
test	B
error	O
rate	B
.	O
these	O
are	O
10.6	O
%	O
and	O
11.1	O
%	O
,	O
respectively	O
.	O
in	O
other	O
words	O
,	O
the	O
lda	O
classiﬁer	B
’	O
s	O
error	B
rate	I
is	O
only	O
0.5	O
%	O
above	O
the	O
smallest	O
possible	O
error	B
rate	I
!	O
this	O
indicates	O
that	O
lda	O
is	O
performing	O
pretty	O
well	O
on	O
this	O
data	B
set	O
.	O
142	O
4.	O
classiﬁcation	B
x2	O
x2	B
x1	O
x1	O
figure	O
4.5.	O
two	O
multivariate	O
gaussian	O
density	O
functions	O
are	O
shown	O
,	O
with	O
p	O
=	O
2.	O
left	O
:	O
the	O
two	O
predictors	O
are	O
uncorrelated	O
.	O
right	O
:	O
the	O
two	O
variables	O
have	O
a	O
correlation	B
of	O
0.7.	O
to	O
reiterate	O
,	O
the	O
lda	O
classiﬁer	B
results	O
from	O
assuming	O
that	O
the	O
observa-	O
tions	O
within	O
each	O
class	O
come	O
from	O
a	O
normal	O
distribution	O
with	O
a	O
class-speciﬁc	O
mean	O
vector	O
and	O
a	O
common	O
variance	B
σ2	O
,	O
and	O
plugging	O
estimates	O
for	O
these	O
parameters	O
into	O
the	O
bayes	O
classiﬁer	B
.	O
in	O
section	O
4.4.4	O
,	O
we	O
will	O
consider	O
a	O
less	O
stringent	O
set	B
of	O
assumptions	O
,	O
by	O
allowing	O
the	O
observations	B
in	O
the	O
kth	O
class	O
to	O
have	O
a	O
class-speciﬁc	O
variance	B
,	O
σ2	O
k.	O
multivariate	O
gaussian	O
4.4.3	O
linear	B
discriminant	I
analysis	I
for	O
p	O
>	O
1	O
we	O
now	O
extend	O
the	O
lda	O
classiﬁer	B
to	O
the	O
case	O
of	O
multiple	B
predictors	O
.	O
to	O
do	O
this	O
,	O
we	O
will	O
assume	O
that	O
x	O
=	O
(	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
)	O
is	O
drawn	O
from	O
a	O
multi-	O
variate	O
gaussian	O
(	O
or	O
multivariate	B
normal	I
)	O
distribution	B
,	O
with	O
a	O
class-speciﬁc	O
mean	O
vector	O
and	O
a	O
common	O
covariance	O
matrix	O
.	O
we	O
begin	O
with	O
a	O
brief	O
review	O
of	O
such	O
a	O
distribution	B
.	O
the	O
multivariate	O
gaussian	O
distribution	B
assumes	O
that	O
each	O
individual	O
pre-	O
dictor	O
follows	O
a	O
one-dimensional	O
normal	O
distribution	O
,	O
as	O
in	O
(	O
4.11	O
)	O
,	O
with	O
some	O
correlation	B
between	O
each	O
pair	O
of	O
predictors	O
.	O
two	O
examples	O
of	O
multivariate	O
gaussian	O
distributions	O
with	O
p	O
=	O
2	O
are	O
shown	O
in	O
figure	O
4.5.	O
the	O
height	O
of	O
the	O
surface	O
at	O
any	O
particular	O
point	O
represents	O
the	O
probability	B
that	O
both	O
x1	O
and	O
x2	B
fall	O
in	O
a	O
small	O
region	O
around	O
that	O
point	O
.	O
in	O
either	O
panel	O
,	O
if	O
the	O
sur-	O
face	O
is	O
cut	O
along	O
the	O
x1	O
axis	O
or	O
along	O
the	O
x2	B
axis	O
,	O
the	O
resulting	O
cross-section	O
will	O
have	O
the	O
shape	O
of	O
a	O
one-dimensional	O
normal	O
distribution	O
.	O
the	O
left-hand	O
panel	O
of	O
figure	O
4.5	O
illustrates	O
an	O
example	O
in	O
which	O
var	O
(	O
x1	O
)	O
=	O
var	O
(	O
x2	B
)	O
and	O
cor	O
(	O
x1	O
,	O
x2	B
)	O
=	O
0	O
;	O
this	O
surface	O
has	O
a	O
characteristic	O
bell	O
shape	O
.	O
however	O
,	O
the	O
bell	O
shape	O
will	O
be	O
distorted	O
if	O
the	O
predictors	O
are	O
correlated	O
or	O
have	O
unequal	O
variances	O
,	O
as	O
is	O
illustrated	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
4.5.	O
in	O
this	O
situation	O
,	O
the	O
base	O
of	O
the	O
bell	O
will	O
have	O
an	O
elliptical	O
,	O
rather	O
than	O
circular	O
,	O
4.4	O
linear	B
discriminant	I
analysis	I
143	O
2	O
x	O
4	O
2	O
0	O
2	O
−	O
4	O
−	O
2	O
x	O
4	O
2	O
0	O
2	O
−	O
4	O
−	O
−4	O
−2	O
0	O
2	O
4	O
−4	O
−2	O
0	O
2	O
4	O
x1	O
x1	O
figure	O
4.6.	O
an	O
example	O
with	O
three	O
classes	O
.	O
the	O
observations	B
from	O
each	O
class	O
are	O
drawn	O
from	O
a	O
multivariate	O
gaussian	O
distribution	B
with	O
p	O
=	O
2	O
,	O
with	O
a	O
class-spe-	O
ciﬁc	O
mean	O
vector	O
and	O
a	O
common	O
covariance	O
matrix	O
.	O
left	O
:	O
ellipses	O
that	O
contain	O
95	O
%	O
of	O
the	O
probability	B
for	O
each	O
of	O
the	O
three	O
classes	O
are	O
shown	O
.	O
the	O
dashed	O
lines	O
are	O
the	O
bayes	O
decision	O
boundaries	O
.	O
right	O
:	O
20	O
observations	B
were	O
generated	O
from	O
each	O
class	O
,	O
and	O
the	O
corresponding	O
lda	O
decision	O
boundaries	O
are	O
indicated	O
using	O
solid	O
black	O
lines	O
.	O
the	O
bayes	O
decision	O
boundaries	O
are	O
once	O
again	O
shown	O
as	O
dashed	O
lines	O
.	O
shape	O
.	O
to	O
indicate	O
that	O
a	O
p-dimensional	O
random	O
variable	O
x	O
has	O
a	O
multi-	O
variate	O
gaussian	O
distribution	B
,	O
we	O
write	O
x	O
∼	O
n	O
(	O
μ	O
,	O
σ	O
)	O
.	O
here	O
e	O
(	O
x	O
)	O
=	O
μ	O
is	O
the	O
mean	O
of	O
x	O
(	O
a	O
vector	B
with	O
p	O
components	O
)	O
,	O
and	O
cov	O
(	O
x	O
)	O
=	O
σ	O
is	O
the	O
p	O
×	O
p	O
covariance	O
matrix	O
of	O
x.	O
formally	O
,	O
the	O
multivariate	O
gaussian	O
density	O
is	O
deﬁned	O
as	O
f	O
(	O
x	O
)	O
=	O
1	O
(	O
2π	O
)	O
p/2|σ|1/2	O
exp	O
(	O
cid:11	O
)	O
−	O
1	O
2	O
(	O
x	O
−	O
μ	O
)	O
t	O
σ	O
(	O
cid:12	O
)	O
−1	O
(	O
x	O
−	O
μ	O
)	O
.	O
(	O
4.18	O
)	O
in	O
the	O
case	O
of	O
p	O
>	O
1	O
predictors	O
,	O
the	O
lda	O
classiﬁer	B
assumes	O
that	O
the	O
observations	B
in	O
the	O
kth	O
class	O
are	O
drawn	O
from	O
a	O
multivariate	O
gaussian	O
dis-	O
tribution	O
n	O
(	O
μk	O
,	O
σ	O
)	O
,	O
where	O
μk	O
is	O
a	O
class-speciﬁc	O
mean	O
vector	O
,	O
and	O
σ	O
is	O
a	O
covariance	O
matrix	O
that	O
is	O
common	O
to	O
all	O
k	O
classes	O
.	O
plugging	O
the	O
density	B
function	I
for	O
the	O
kth	O
class	O
,	O
fk	O
(	O
x	O
=	O
x	O
)	O
,	O
into	O
(	O
4.10	O
)	O
and	O
performing	O
a	O
little	O
bit	O
of	O
algebra	O
reveals	O
that	O
the	O
bayes	O
classiﬁer	B
assigns	O
an	O
observation	O
x	O
=	O
x	O
to	O
the	O
class	O
for	O
which	O
δk	O
(	O
x	O
)	O
=	O
xt	O
σ	O
−1μk	O
−	O
1	O
2	O
−1μk	O
+	O
log	O
πk	O
μt	O
k	O
σ	O
(	O
4.19	O
)	O
is	O
largest	O
.	O
this	O
is	O
the	O
vector/matrix	O
version	O
of	O
(	O
4.13	O
)	O
.	O
an	O
example	O
is	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
4.6.	O
three	O
equally-	O
sized	O
gaussian	O
classes	O
are	O
shown	O
with	O
class-speciﬁc	O
mean	O
vectors	O
and	O
a	O
common	O
covariance	O
matrix	O
.	O
the	O
three	O
ellipses	O
represent	O
regions	O
that	O
con-	O
tain	O
95	O
%	O
of	O
the	O
probability	B
for	O
each	O
of	O
the	O
three	O
classes	O
.	O
the	O
dashed	O
lines	O
144	O
4.	O
classiﬁcation	B
are	O
the	O
bayes	O
decision	O
boundaries	O
.	O
in	O
other	O
words	O
,	O
they	O
represent	O
the	O
set	B
of	O
values	O
x	O
for	O
which	O
δk	O
(	O
x	O
)	O
=	O
δ	O
(	O
cid:5	O
)	O
(	O
x	O
)	O
;	O
i.e	O
.	O
xt	O
σ	O
−1μk	O
−	O
1	O
2	O
−1μk	O
=	O
xt	O
σ	O
μt	O
k	O
σ	O
−1μl	O
−	O
1	O
−1μl	O
μt	O
l	O
σ	O
(	O
4.20	O
)	O
2	O
for	O
k	O
(	O
cid:4	O
)	O
=	O
l.	O
(	O
the	O
log	O
πk	O
term	B
from	O
(	O
4.19	O
)	O
has	O
disappeared	O
because	O
each	O
of	O
the	O
three	O
classes	O
has	O
the	O
same	O
number	O
of	O
training	B
observations	O
;	O
i.e	O
.	O
πk	O
is	O
the	O
same	O
for	O
each	O
class	O
.	O
)	O
note	O
that	O
there	O
are	O
three	O
lines	O
representing	O
the	O
bayes	O
decision	O
boundaries	O
because	O
there	O
are	O
three	O
pairs	O
of	O
classes	O
among	O
the	O
three	O
classes	O
.	O
that	O
is	O
,	O
one	O
bayes	O
decision	B
boundary	I
separates	O
class	O
1	O
from	O
class	O
2	O
,	O
one	O
separates	O
class	O
1	O
from	O
class	O
3	O
,	O
and	O
one	O
separates	O
class	O
2	O
from	O
class	O
3.	O
these	O
three	O
bayes	O
decision	O
boundaries	O
divide	O
the	O
predictor	B
space	O
into	O
three	O
regions	O
.	O
the	O
bayes	O
classiﬁer	B
will	O
classify	O
an	O
observation	O
according	O
to	O
the	O
region	O
in	O
which	O
it	O
is	O
located	O
.	O
once	O
again	O
,	O
we	O
need	O
to	O
estimate	O
the	O
unknown	O
parameters	O
μ1	O
,	O
.	O
.	O
.	O
,	O
μk	O
,	O
π1	O
,	O
.	O
.	O
.	O
,	O
πk	O
,	O
and	O
σ	O
;	O
the	O
formulas	O
are	O
similar	O
to	O
those	O
used	O
in	O
the	O
one-	O
dimensional	O
case	O
,	O
given	O
in	O
(	O
4.15	O
)	O
.	O
to	O
assign	O
a	O
new	O
observation	O
x	O
=	O
x	O
,	O
lda	O
plugs	O
these	O
estimates	O
into	O
(	O
4.19	O
)	O
and	O
classiﬁes	O
to	O
the	O
class	O
for	O
which	O
ˆδk	O
(	O
x	O
)	O
is	O
largest	O
.	O
note	O
that	O
in	O
(	O
4.19	O
)	O
δk	O
(	O
x	O
)	O
is	O
a	O
linear	B
function	O
of	O
x	O
;	O
that	O
is	O
,	O
the	O
lda	O
decision	O
rule	O
depends	O
on	O
x	O
only	O
through	O
a	O
linear	B
combination	I
of	O
its	O
elements	O
.	O
once	O
again	O
,	O
this	O
is	O
the	O
reason	O
for	O
the	O
word	O
linear	B
in	O
lda	O
.	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
4.6	O
,	O
20	O
observations	B
drawn	O
from	O
each	O
of	O
the	O
three	O
classes	O
are	O
displayed	O
,	O
and	O
the	O
resulting	O
lda	O
decision	O
boundaries	O
are	O
shown	O
as	O
solid	O
black	O
lines	O
.	O
overall	O
,	O
the	O
lda	O
decision	O
boundaries	O
are	O
pretty	O
close	O
to	O
the	O
bayes	O
decision	O
boundaries	O
,	O
shown	O
again	O
as	O
dashed	O
lines	O
.	O
the	O
test	B
error	O
rates	O
for	O
the	O
bayes	O
and	O
lda	O
classiﬁers	O
are	O
0.0746	O
and	O
0.0770	O
,	O
respectively	O
.	O
this	O
indicates	O
that	O
lda	O
is	O
performing	O
well	O
on	O
this	O
data	B
.	O
we	O
can	O
perform	O
lda	O
on	O
the	O
default	O
data	B
in	O
order	O
to	O
predict	O
whether	O
or	O
not	O
an	O
individual	O
will	O
default	O
on	O
the	O
basis	B
of	O
credit	O
card	O
balance	O
and	O
student	O
status	O
.	O
the	O
lda	O
model	B
ﬁt	O
to	O
the	O
10	O
,	O
000	O
training	B
samples	O
results	O
in	O
a	O
training	B
error	O
rate	B
of	O
2.75	O
%	O
.	O
this	O
sounds	O
like	O
a	O
low	O
error	B
rate	I
,	O
but	O
two	O
caveats	O
must	O
be	O
noted	O
.	O
•	O
first	O
of	O
all	O
,	O
training	B
error	O
rates	O
will	O
usually	O
be	O
lower	O
than	O
test	B
error	O
rates	O
,	O
which	O
are	O
the	O
real	O
quantity	O
of	O
interest	O
.	O
in	O
other	O
words	O
,	O
we	O
might	O
expect	O
this	O
classiﬁer	B
to	O
perform	O
worse	O
if	O
we	O
use	O
it	O
to	O
predict	O
whether	O
or	O
not	O
a	O
new	O
set	B
of	O
individuals	O
will	O
default	O
.	O
the	O
reason	O
is	O
that	O
we	O
speciﬁcally	O
adjust	O
the	O
parameters	O
of	O
our	O
model	B
to	O
do	O
well	O
on	O
the	O
training	B
data	O
.	O
the	O
higher	O
the	O
ratio	O
of	O
parameters	O
p	O
to	O
number	O
of	O
samples	O
n	O
,	O
the	O
more	O
we	O
expect	O
this	O
overﬁtting	B
to	O
play	O
a	O
role	O
.	O
for	O
these	O
data	B
we	O
don	O
’	O
t	O
expect	O
this	O
to	O
be	O
a	O
problem	O
,	O
since	O
p	O
=	O
2	O
and	O
n	O
=	O
10	O
,	O
000	O
.	O
•	O
second	O
,	O
since	O
only	O
3.33	O
%	O
of	O
the	O
individuals	O
in	O
the	O
training	B
sample	O
defaulted	O
,	O
a	O
simple	B
but	O
useless	O
classiﬁer	B
that	O
always	O
predicts	O
that	O
overﬁtting	B
4.4	O
linear	B
discriminant	I
analysis	I
145	O
predicted	O
default	O
status	O
no	O
yes	O
total	O
9	O
,	O
644	O
9	O
,	O
667	O
true	O
default	O
status	O
total	O
no	O
9	O
,	O
896	O
104	O
23	O
yes	O
252	O
81	O
333	O
10	O
,	O
000	O
table	O
4.4.	O
a	O
confusion	B
matrix	I
compares	O
the	O
lda	O
predictions	O
to	O
the	O
true	O
de-	O
fault	O
statuses	O
for	O
the	O
10	O
,	O
000	O
training	B
observations	O
in	O
the	O
default	O
data	B
set	O
.	O
ele-	O
ments	O
on	O
the	O
diagonal	O
of	O
the	O
matrix	O
represent	O
individuals	O
whose	O
default	O
statuses	O
were	O
correctly	O
predicted	O
,	O
while	O
oﬀ-diagonal	O
elements	O
represent	O
individuals	O
that	O
were	O
misclassiﬁed	O
.	O
lda	O
made	O
incorrect	O
predictions	O
for	O
23	O
individuals	O
who	O
did	O
not	O
default	O
and	O
for	O
252	O
individuals	O
who	O
did	O
default	O
.	O
each	O
individual	O
will	O
not	O
default	O
,	O
regardless	O
of	O
his	O
or	O
her	O
credit	O
card	O
balance	O
and	O
student	O
status	O
,	O
will	O
result	O
in	O
an	O
error	B
rate	I
of	O
3.33	O
%	O
.	O
in	O
other	O
words	O
,	O
the	O
trivial	O
null	B
classiﬁer	O
will	O
achieve	O
an	O
error	B
rate	I
that	O
is	O
only	O
a	O
bit	O
higher	O
than	O
the	O
lda	O
training	B
set	O
error	B
rate	I
.	O
null	B
in	O
practice	O
,	O
a	O
binary	B
classiﬁer	O
such	O
as	O
this	O
one	O
can	O
make	O
two	O
types	O
of	O
errors	O
:	O
it	O
can	O
incorrectly	O
assign	O
an	O
individual	O
who	O
defaults	O
to	O
the	O
no	O
default	O
category	O
,	O
or	O
it	O
can	O
incorrectly	O
assign	O
an	O
individual	O
who	O
does	O
not	O
default	O
to	O
the	O
default	O
category	O
.	O
it	O
is	O
often	O
of	O
interest	O
to	O
determine	O
which	O
of	O
these	O
two	O
types	O
of	O
errors	O
are	O
being	O
made	O
.	O
a	O
confusion	B
matrix	I
,	O
shown	O
for	O
the	O
default	O
data	B
in	O
table	O
4.4	O
,	O
is	O
a	O
convenient	O
way	O
to	O
display	O
this	O
information	O
.	O
the	O
table	O
reveals	O
that	O
lda	O
predicted	O
that	O
a	O
total	O
of	O
104	O
people	O
would	O
default	O
.	O
of	O
these	O
people	O
,	O
81	O
actually	O
defaulted	O
and	O
23	O
did	O
not	O
.	O
hence	O
only	O
23	O
out	O
of	O
9	O
,	O
667	O
of	O
the	O
individuals	O
who	O
did	O
not	O
default	O
were	O
incorrectly	O
labeled	O
.	O
this	O
looks	O
like	O
a	O
pretty	O
low	O
error	B
rate	I
!	O
however	O
,	O
of	O
the	O
333	O
individuals	O
who	O
defaulted	O
,	O
252	O
(	O
or	O
75.7	O
%	O
)	O
were	O
missed	O
by	O
lda	O
.	O
so	O
while	O
the	O
overall	O
error	B
rate	I
is	O
low	O
,	O
the	O
error	B
rate	I
among	O
individuals	O
who	O
defaulted	O
is	O
very	O
high	O
.	O
from	O
the	O
perspective	O
of	O
a	O
credit	O
card	O
company	O
that	O
is	O
trying	O
to	O
identify	O
high-risk	O
individuals	O
,	O
an	O
error	B
rate	I
of	O
252/333	O
=	O
75.7	O
%	O
among	O
individuals	O
who	O
default	O
may	O
well	O
be	O
unacceptable	O
.	O
class-speciﬁc	O
performance	O
is	O
also	O
important	O
in	O
medicine	O
and	O
biology	O
,	O
where	O
the	O
terms	O
sensitivity	B
and	O
speciﬁcity	B
characterize	O
the	O
performance	O
of	O
a	O
classiﬁer	B
or	O
screening	O
test	B
.	O
in	O
this	O
case	O
the	O
sensitivity	B
is	O
the	O
percentage	O
of	O
true	O
defaulters	O
that	O
are	O
identiﬁed	O
,	O
a	O
low	O
24.3	O
%	O
in	O
this	O
case	O
.	O
the	O
speciﬁcity	B
is	O
the	O
percentage	O
of	O
non-defaulters	O
that	O
are	O
correctly	O
identiﬁed	O
,	O
here	O
(	O
1	O
−	O
23/9	O
,	O
667	O
)	O
×	O
100	O
=	O
99.8	O
%	O
.	O
why	O
does	O
lda	O
do	O
such	O
a	O
poor	O
job	O
of	O
classifying	O
the	O
customers	O
who	O
de-	O
fault	O
?	O
in	O
other	O
words	O
,	O
why	O
does	O
it	O
have	O
such	O
a	O
low	O
sensitivity	B
?	O
as	O
we	O
have	O
seen	O
,	O
lda	O
is	O
trying	O
to	O
approximate	O
the	O
bayes	O
classiﬁer	B
,	O
which	O
has	O
the	O
low-	O
est	O
total	O
error	O
rate	B
out	O
of	O
all	O
classiﬁers	O
(	O
if	O
the	O
gaussian	O
model	B
is	O
correct	O
)	O
.	O
that	O
is	O
,	O
the	O
bayes	O
classiﬁer	B
will	O
yield	O
the	O
smallest	O
possible	O
total	O
number	O
of	O
misclassiﬁed	O
observations	B
,	O
irrespective	O
of	O
which	O
class	O
the	O
errors	O
come	O
from	O
.	O
that	O
is	O
,	O
some	O
misclassiﬁcations	O
will	O
result	O
from	O
incorrectly	O
assigning	O
confusion	B
matrix	I
sensitivity	O
speciﬁcity	B
146	O
4.	O
classiﬁcation	B
predicted	O
default	O
status	O
no	O
yes	O
total	O
true	O
default	O
status	O
total	O
no	O
9	O
,	O
570	O
430	O
yes	O
138	O
195	O
333	O
9	O
,	O
432	O
235	O
9	O
,	O
667	O
10	O
,	O
000	O
table	O
4.5.	O
a	O
confusion	B
matrix	I
compares	O
the	O
lda	O
predictions	O
to	O
the	O
true	O
de-	O
fault	O
statuses	O
for	O
the	O
10	O
,	O
000	O
training	B
observations	O
in	O
the	O
default	O
data	B
set	O
,	O
using	O
a	O
modiﬁed	O
threshold	O
value	O
that	O
predicts	O
default	O
for	O
any	O
individuals	O
whose	O
posterior	B
default	O
probability	B
exceeds	O
20	O
%	O
.	O
a	O
customer	O
who	O
does	O
not	O
default	O
to	O
the	O
default	O
class	O
,	O
and	O
others	O
will	O
re-	O
sult	O
from	O
incorrectly	O
assigning	O
a	O
customer	O
who	O
defaults	O
to	O
the	O
non-default	O
class	O
.	O
in	O
contrast	B
,	O
a	O
credit	O
card	O
company	O
might	O
particularly	O
wish	O
to	O
avoid	O
incorrectly	O
classifying	O
an	O
individual	O
who	O
will	O
default	O
,	O
whereas	O
incorrectly	O
classifying	O
an	O
individual	O
who	O
will	O
not	O
default	O
,	O
though	O
still	O
to	O
be	O
avoided	O
,	O
is	O
less	O
problematic	O
.	O
we	O
will	O
now	O
see	O
that	O
it	O
is	O
possible	O
to	O
modify	O
lda	O
in	O
order	O
to	O
develop	O
a	O
classiﬁer	B
that	O
better	O
meets	O
the	O
credit	O
card	O
company	O
’	O
s	O
needs	O
.	O
the	O
bayes	O
classiﬁer	B
works	O
by	O
assigning	O
an	O
observation	O
to	O
the	O
class	O
for	O
which	O
the	O
posterior	B
probability	O
pk	O
(	O
x	O
)	O
is	O
greatest	O
.	O
in	O
the	O
two-class	O
case	O
,	O
this	O
amounts	O
to	O
assigning	O
an	O
observation	O
to	O
the	O
default	O
class	O
if	O
pr	O
(	O
default	O
=	O
yes|x	O
=	O
x	O
)	O
>	O
0.5	O
.	O
(	O
4.21	O
)	O
thus	O
,	O
the	O
bayes	O
classiﬁer	B
,	O
and	O
by	O
extension	O
lda	O
,	O
uses	O
a	O
threshold	O
of	O
50	O
%	O
for	O
the	O
posterior	B
probability	O
of	O
default	O
in	O
order	O
to	O
assign	O
an	O
observation	O
to	O
the	O
default	O
class	O
.	O
however	O
,	O
if	O
we	O
are	O
concerned	O
about	O
incorrectly	O
pre-	O
dicting	O
the	O
default	O
status	O
for	O
individuals	O
who	O
default	O
,	O
then	O
we	O
can	O
consider	O
lowering	O
this	O
threshold	O
.	O
for	O
instance	O
,	O
we	O
might	O
label	O
any	O
customer	O
with	O
a	O
posterior	B
probability	O
of	O
default	O
above	O
20	O
%	O
to	O
the	O
default	O
class	O
.	O
in	O
other	O
words	O
,	O
instead	O
of	O
assigning	O
an	O
observation	O
to	O
the	O
default	O
class	O
if	O
(	O
4.21	O
)	O
holds	O
,	O
we	O
could	O
instead	O
assign	O
an	O
observation	O
to	O
this	O
class	O
if	O
pr	O
(	O
default	O
=	O
yes|x	O
=	O
x	O
)	O
>	O
0.2	O
.	O
(	O
4.22	O
)	O
the	O
error	B
rates	O
that	O
result	O
from	O
taking	O
this	O
approach	B
are	O
shown	O
in	O
table	O
4.5.	O
now	O
lda	O
predicts	O
that	O
430	O
individuals	O
will	O
default	O
.	O
of	O
the	O
333	O
individuals	O
who	O
default	O
,	O
lda	O
correctly	O
predicts	O
all	O
but	O
138	O
,	O
or	O
41.4	O
%	O
.	O
this	O
is	O
a	O
vast	O
improvement	O
over	O
the	O
error	B
rate	I
of	O
75.7	O
%	O
that	O
resulted	O
from	O
using	O
the	O
threshold	O
of	O
50	O
%	O
.	O
however	O
,	O
this	O
improvement	O
comes	O
at	O
a	O
cost	O
:	O
now	O
235	O
individuals	O
who	O
do	O
not	O
default	O
are	O
incorrectly	O
classiﬁed	O
.	O
as	O
a	O
result	O
,	O
the	O
overall	O
error	B
rate	I
has	O
increased	O
slightly	O
to	O
3.73	O
%	O
.	O
but	O
a	O
credit	O
card	O
company	O
may	O
consider	O
this	O
slight	O
increase	O
in	O
the	O
total	O
error	O
rate	B
to	O
be	O
a	O
small	O
price	O
to	O
pay	O
for	O
more	O
accurate	O
identiﬁcation	O
of	O
individuals	O
who	O
do	O
indeed	O
default	O
.	O
figure	O
4.7	O
illustrates	O
the	O
trade-oﬀ	B
that	O
results	O
from	O
modifying	O
the	O
thresh-	O
old	O
value	O
for	O
the	O
posterior	B
probability	O
of	O
default	O
.	O
various	O
error	B
rates	O
are	O
4.4	O
linear	B
discriminant	I
analysis	I
147	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
t	O
e	O
a	O
r	O
r	O
o	O
r	O
r	O
e	O
0.0	O
0.1	O
0.2	O
0.3	O
0.4	O
0.5	O
threshold	O
figure	O
4.7.	O
for	O
the	O
default	O
data	B
set	O
,	O
error	B
rates	O
are	O
shown	O
as	O
a	O
function	B
of	O
the	O
threshold	O
value	O
for	O
the	O
posterior	B
probability	O
that	O
is	O
used	O
to	O
perform	O
the	O
assign-	O
ment	O
.	O
the	O
black	O
solid	O
line	B
displays	O
the	O
overall	O
error	B
rate	I
.	O
the	O
blue	O
dashed	O
line	B
represents	O
the	O
fraction	O
of	O
defaulting	O
customers	O
that	O
are	O
incorrectly	O
classiﬁed	O
,	O
and	O
the	O
orange	O
dotted	O
line	B
indicates	O
the	O
fraction	O
of	O
errors	O
among	O
the	O
non-defaulting	O
customers	O
.	O
shown	O
as	O
a	O
function	B
of	O
the	O
threshold	O
value	O
.	O
using	O
a	O
threshold	O
of	O
0.5	O
,	O
as	O
in	O
(	O
4.21	O
)	O
,	O
minimizes	O
the	O
overall	O
error	B
rate	I
,	O
shown	O
as	O
a	O
black	O
solid	O
line	B
.	O
this	O
is	O
to	O
be	O
expected	O
,	O
since	O
the	O
bayes	O
classiﬁer	B
uses	O
a	O
threshold	O
of	O
0.5	O
and	O
is	O
known	O
to	O
have	O
the	O
lowest	O
overall	O
error	B
rate	I
.	O
but	O
when	O
a	O
threshold	O
of	O
0.5	O
is	O
used	O
,	O
the	O
error	B
rate	I
among	O
the	O
individuals	O
who	O
default	O
is	O
quite	O
high	O
(	O
blue	O
dashed	O
line	B
)	O
.	O
as	O
the	O
threshold	O
is	O
reduced	O
,	O
the	O
error	B
rate	I
among	O
individuals	O
who	O
default	O
decreases	O
steadily	O
,	O
but	O
the	O
error	B
rate	I
among	O
the	O
individuals	O
who	O
do	O
not	O
default	O
increases	O
.	O
how	O
can	O
we	O
decide	O
which	O
threshold	O
value	O
is	O
best	O
?	O
such	O
a	O
decision	O
must	O
be	O
based	O
on	O
domain	O
knowledge	O
,	O
such	O
as	O
detailed	O
information	O
about	O
the	O
costs	O
associated	O
with	O
default	O
.	O
the	O
roc	O
curve	O
is	O
a	O
popular	O
graphic	O
for	O
simultaneously	O
displaying	O
the	O
two	O
types	O
of	O
errors	O
for	O
all	O
possible	O
thresholds	O
.	O
the	O
name	O
“	O
roc	O
”	O
is	O
his-	O
toric	O
,	O
and	O
comes	O
from	O
communications	O
theory	O
.	O
it	O
is	O
an	O
acronym	O
for	O
receiver	O
operating	O
characteristics	O
.	O
figure	O
4.8	O
displays	O
the	O
roc	O
curve	O
for	O
the	O
lda	O
classiﬁer	B
on	O
the	O
training	B
data	O
.	O
the	O
overall	O
performance	O
of	O
a	O
classiﬁer	B
,	O
sum-	O
marized	O
over	O
all	O
possible	O
thresholds	O
,	O
is	O
given	O
by	O
the	O
area	O
under	O
the	O
(	O
roc	O
)	O
curve	O
(	O
auc	O
)	O
.	O
an	O
ideal	O
roc	O
curve	O
will	O
hug	O
the	O
top	O
left	O
corner	O
,	O
so	O
the	O
larger	O
the	O
auc	O
the	O
better	O
the	O
classiﬁer	B
.	O
for	O
this	O
data	B
the	O
auc	O
is	O
0.95	O
,	O
which	O
is	O
close	O
to	O
the	O
maximum	O
of	O
one	O
so	O
would	O
be	O
considered	O
very	O
good	O
.	O
we	O
expect	O
a	O
classiﬁer	B
that	O
performs	O
no	O
better	O
than	O
chance	O
to	O
have	O
an	O
auc	O
of	O
0.5	O
(	O
when	O
evaluated	O
on	O
an	O
independent	B
test	O
set	B
not	O
used	O
in	O
model	B
training	O
)	O
.	O
roc	O
curves	O
are	O
useful	O
for	O
comparing	O
diﬀerent	O
classiﬁers	O
,	O
since	O
they	O
take	O
into	O
account	O
all	O
possible	O
thresholds	O
.	O
it	O
turns	O
out	O
that	O
the	O
roc	O
curve	O
for	O
the	O
logistic	B
regression	I
model	O
of	O
section	O
4.3.4	O
ﬁt	B
to	O
these	O
data	B
is	O
virtually	O
indis-	O
tinguishable	O
from	O
this	O
one	O
for	O
the	O
lda	O
model	B
,	O
so	O
we	O
do	O
not	O
display	O
it	O
here	O
.	O
as	O
we	O
have	O
seen	O
above	O
,	O
varying	O
the	O
classiﬁer	B
threshold	O
changes	O
its	O
true	B
positive	I
and	O
false	B
positive	I
rate	I
.	O
these	O
are	O
also	O
called	O
the	O
sensitivity	B
and	O
one	O
roc	O
curve	O
area	O
under	O
the	O
(	O
roc	O
)	O
curve	O
sensitivity	B
148	O
4.	O
classiﬁcation	B
roc	O
curve	O
t	O
e	O
a	O
r	O
e	O
v	O
i	O
t	O
i	O
s	O
o	O
p	O
e	O
u	O
r	O
t	O
0	O
.	O
1	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
0	O
.	O
0.0	O
0.2	O
0.4	O
0.6	O
0.8	O
1.0	O
false	B
positive	I
rate	I
figure	O
4.8.	O
a	O
roc	O
curve	O
for	O
the	O
lda	O
classiﬁer	B
on	O
the	O
default	O
data	B
.	O
it	O
traces	O
out	O
two	O
types	O
of	O
error	B
as	O
we	O
vary	O
the	O
threshold	O
value	O
for	O
the	O
posterior	B
probability	O
of	O
default	O
.	O
the	O
actual	O
thresholds	O
are	O
not	O
shown	O
.	O
the	O
true	B
positive	I
rate	I
is	O
the	O
sensitivity	B
:	O
the	O
fraction	O
of	O
defaulters	O
that	O
are	O
correctly	O
identiﬁed	O
,	O
using	O
a	O
given	O
threshold	O
value	O
.	O
the	O
false	B
positive	I
rate	I
is	O
1-speciﬁcity	O
:	O
the	O
fraction	O
of	O
non-defaulters	O
that	O
we	O
classify	O
incorrectly	O
as	O
defaulters	O
,	O
using	O
that	O
same	O
threshold	O
value	O
.	O
the	O
ideal	O
roc	O
curve	O
hugs	O
the	O
top	O
left	O
corner	O
,	O
indicating	O
a	O
high	O
true	B
positive	I
rate	I
and	O
a	O
low	O
false	B
positive	I
rate	I
.	O
the	O
dotted	O
line	B
represents	O
the	O
“	O
no	O
information	O
”	O
classiﬁer	B
;	O
this	O
is	O
what	O
we	O
would	O
expect	O
if	O
student	O
status	O
and	O
credit	O
card	O
balance	O
are	O
not	O
associated	O
with	O
probability	B
of	O
default	O
.	O
+	O
or	O
non-null	O
true	O
false	O
pos	O
.	O
(	O
fp	O
)	O
class	O
+	O
or	O
non-null	O
false	O
neg	O
.	O
(	O
fn	O
)	O
true	O
pos	O
.	O
(	O
tp	O
)	O
−	O
or	O
null	B
true	O
neg	O
.	O
(	O
tn	O
)	O
total	O
n	O
p	O
predicted	O
class	O
−	O
or	O
null	B
total	O
n	O
∗	O
∗	O
p	O
table	O
4.6.	O
possible	O
results	O
when	O
applying	O
a	O
classiﬁer	B
or	O
diagnostic	O
test	B
to	O
a	O
population	O
.	O
minus	O
the	O
speciﬁcity	B
of	O
our	O
classiﬁer	B
.	O
since	O
there	O
is	O
an	O
almost	O
bewildering	O
array	O
of	O
terms	O
used	O
in	O
this	O
context	O
,	O
we	O
now	O
give	O
a	O
summary	O
.	O
table	O
4.6	O
shows	O
the	O
possible	O
results	O
when	O
applying	O
a	O
classiﬁer	B
(	O
or	O
diagnostic	O
test	B
)	O
to	O
a	O
population	O
.	O
to	O
make	O
the	O
connection	O
with	O
the	O
epidemiology	O
literature	O
,	O
we	O
think	O
of	O
“	O
+	O
”	O
as	O
the	O
“	O
disease	O
”	O
that	O
we	O
are	O
trying	O
to	O
detect	O
,	O
and	O
“	O
−	O
”	O
as	O
the	O
“	O
non-disease	O
”	O
state	O
.	O
to	O
make	O
the	O
connection	O
to	O
the	O
classical	O
hypothesis	B
testing	O
literature	O
,	O
we	O
think	O
of	O
“	O
−	O
”	O
as	O
the	O
null	B
hypothesis	O
and	O
“	O
+	O
”	O
as	O
the	O
alternative	O
(	O
non-null	O
)	O
hypothesis	B
.	O
in	O
the	O
context	O
of	O
the	O
default	O
data	B
,	O
“	O
+	O
”	O
indicates	O
an	O
individual	O
who	O
defaults	O
,	O
and	O
“	O
−	O
”	O
indicates	O
one	O
who	O
does	O
not	O
.	O
speciﬁcity	B
4.4	O
linear	B
discriminant	I
analysis	I
149	O
name	O
false	O
pos	O
.	O
rate	B
true	O
pos	O
.	O
rate	B
pos	O
.	O
pred	O
.	O
value	O
neg	O
.	O
pred	O
.	O
value	O
deﬁnition	O
synonyms	O
fp/n	O
type	O
i	O
error	B
,	O
1−speciﬁcity	O
tp/p	O
1−type	O
ii	O
error	B
,	O
power	B
,	O
sensitivity	B
,	O
recall	B
∗	O
tp/p	O
∗	O
tn/n	O
precision	B
,	O
1−false	O
discovery	O
proportion	O
table	O
4.7.	O
important	O
measures	O
for	O
classiﬁcation	O
and	O
diagnostic	O
testing	O
,	O
derived	O
from	O
quantities	O
in	O
table	O
4.6.	O
table	O
4.7	O
lists	O
many	O
of	O
the	O
popular	O
performance	O
measures	O
that	O
are	O
used	O
in	O
this	O
context	O
.	O
the	O
denominators	O
for	O
the	O
false	B
positive	I
and	O
true	B
positive	I
rates	O
are	O
the	O
actual	O
population	O
counts	O
in	O
each	O
class	O
.	O
in	O
contrast	B
,	O
the	O
denominators	O
for	O
the	O
positive	B
predictive	I
value	I
and	O
the	O
negative	B
predictive	I
value	I
are	O
the	O
total	O
predicted	O
counts	O
for	O
each	O
class	O
.	O
4.4.4	O
quadratic	B
discriminant	I
analysis	I
as	O
we	O
have	O
discussed	O
,	O
lda	O
assumes	O
that	O
the	O
observations	B
within	O
each	O
class	O
are	O
drawn	O
from	O
a	O
multivariate	O
gaussian	O
distribution	B
with	O
a	O
class-	O
speciﬁc	O
mean	O
vector	O
and	O
a	O
covariance	O
matrix	O
that	O
is	O
common	O
to	O
all	O
k	O
classes	O
.	O
quadratic	B
discriminant	I
analysis	I
(	O
qda	O
)	O
provides	O
an	O
alternative	O
approach	O
.	O
like	O
lda	O
,	O
the	O
qda	O
classiﬁer	B
results	O
from	O
assuming	O
that	O
the	O
observations	B
from	O
each	O
class	O
are	O
drawn	O
from	O
a	O
gaussian	O
distribution	B
,	O
and	O
plugging	O
estimates	O
for	O
the	O
parameters	O
into	O
bayes	O
’	O
theorem	O
in	O
order	O
to	O
per-	O
form	O
prediction	B
.	O
however	O
,	O
unlike	O
lda	O
,	O
qda	O
assumes	O
that	O
each	O
class	O
has	O
its	O
own	O
covariance	O
matrix	O
.	O
that	O
is	O
,	O
it	O
assumes	O
that	O
an	O
observation	O
from	O
the	O
kth	O
class	O
is	O
of	O
the	O
form	O
x	O
∼	O
n	O
(	O
μk	O
,	O
σk	O
)	O
,	O
where	O
σk	O
is	O
a	O
covariance	O
matrix	O
for	O
the	O
kth	O
class	O
.	O
under	O
this	O
assumption	O
,	O
the	O
bayes	O
classiﬁer	B
assigns	O
an	O
observation	O
x	O
=	O
x	O
to	O
the	O
class	O
for	O
which	O
quadratic	B
discriminant	I
analysis	I
δk	O
(	O
x	O
)	O
=	O
−	O
1	O
2	O
=	O
−	O
1	O
2	O
(	O
x	O
−	O
μk	O
)	O
t	O
σ	O
xt	O
σ	O
−1	O
k	O
x	O
+	O
xt	O
σ	O
k	O
μk	O
−	O
1	O
−1	O
2	O
log	O
|σk|	O
+	O
log	O
πk	O
(	O
4.23	O
)	O
log	O
|σk|	O
+	O
log	O
πk	O
k	O
(	O
x	O
−	O
μk	O
)	O
−	O
1	O
−1	O
2	O
k	O
μk	O
−	O
1	O
−1	O
μt	O
k	O
σ	O
2	O
is	O
largest	O
.	O
so	O
the	O
qda	O
classiﬁer	B
involves	O
plugging	O
estimates	O
for	O
σk	O
,	O
μk	O
,	O
and	O
πk	O
into	O
(	O
4.23	O
)	O
,	O
and	O
then	O
assigning	O
an	O
observation	O
x	O
=	O
x	O
to	O
the	O
class	O
for	O
which	O
this	O
quantity	O
is	O
largest	O
.	O
unlike	O
in	O
(	O
4.19	O
)	O
,	O
the	O
quantity	O
x	O
appears	O
as	O
a	O
quadratic	B
function	O
in	O
(	O
4.23	O
)	O
.	O
this	O
is	O
where	O
qda	O
gets	O
its	O
name	O
.	O
why	O
does	O
it	O
matter	O
whether	O
or	O
not	O
we	O
assume	O
that	O
the	O
k	O
classes	O
share	O
a	O
common	O
covariance	O
matrix	O
?	O
in	O
other	O
words	O
,	O
why	O
would	O
one	O
prefer	O
lda	O
to	O
qda	O
,	O
or	O
vice-versa	O
?	O
the	O
answer	O
lies	O
in	O
the	O
bias-variance	B
trade-oﬀ	O
.	O
when	O
there	O
are	O
p	O
predictors	O
,	O
then	O
estimating	O
a	O
covariance	O
matrix	O
requires	O
esti-	O
mating	O
p	O
(	O
p+1	O
)	O
/2	O
parameters	O
.	O
qda	O
estimates	O
a	O
separate	O
covariance	O
matrix	O
for	O
each	O
class	O
,	O
for	O
a	O
total	O
of	O
kp	O
(	O
p+1	O
)	O
/2	O
parameters	O
.	O
with	O
50	O
predictors	O
this	O
150	O
4.	O
classiﬁcation	B
2	O
x	O
2	O
1	O
0	O
1	O
−	O
2	O
−	O
3	O
−	O
4	O
−	O
2	O
x	O
2	O
1	O
0	O
1	O
−	O
2	O
−	O
3	O
−	O
4	O
−	O
−4	O
−2	O
0	O
x1	O
2	O
4	O
−4	O
−2	O
2	O
4	O
0	O
x1	O
figure	O
4.9.	O
left	O
:	O
the	O
bayes	O
(	O
purple	O
dashed	O
)	O
,	O
lda	O
(	O
black	O
dotted	O
)	O
,	O
and	O
qda	O
(	O
green	O
solid	O
)	O
decision	O
boundaries	O
for	O
a	O
two-class	O
problem	O
with	O
σ1	O
=	O
σ2	O
.	O
the	O
shading	O
indicates	O
the	O
qda	O
decision	O
rule	O
.	O
since	O
the	O
bayes	O
decision	B
boundary	I
is	O
linear	B
,	O
it	O
is	O
more	O
accurately	O
approximated	O
by	O
lda	O
than	O
by	O
qda	O
.	O
right	O
:	O
details	O
are	O
as	O
given	O
in	O
the	O
left-hand	O
panel	O
,	O
except	O
that	O
σ1	O
(	O
cid:3	O
)	O
=	O
σ2	O
.	O
since	O
the	O
bayes	O
decision	B
boundary	I
is	O
non-linear	B
,	O
it	O
is	O
more	O
accurately	O
approximated	O
by	O
qda	O
than	O
by	O
lda	O
.	O
is	O
some	O
multiple	B
of	O
1,275	O
,	O
which	O
is	O
a	O
lot	O
of	O
parameters	O
.	O
by	O
instead	O
assum-	O
ing	O
that	O
the	O
k	O
classes	O
share	O
a	O
common	O
covariance	O
matrix	O
,	O
the	O
lda	O
model	B
becomes	O
linear	B
in	O
x	O
,	O
which	O
means	O
there	O
are	O
kp	O
linear	B
coeﬃcients	O
to	O
esti-	O
mate	O
.	O
consequently	O
,	O
lda	O
is	O
a	O
much	O
less	O
ﬂexible	B
classiﬁer	O
than	O
qda	O
,	O
and	O
so	O
has	O
substantially	O
lower	O
variance	B
.	O
this	O
can	O
potentially	O
lead	O
to	O
improved	O
prediction	B
performance	O
.	O
but	O
there	O
is	O
a	O
trade-oﬀ	B
:	O
if	O
lda	O
’	O
s	O
assumption	O
that	O
the	O
k	O
classes	O
share	O
a	O
common	O
covariance	O
matrix	O
is	O
badly	O
oﬀ	O
,	O
then	O
lda	O
can	O
suﬀer	O
from	O
high	O
bias	B
.	O
roughly	O
speaking	O
,	O
lda	O
tends	O
to	O
be	O
a	O
better	O
bet	O
than	O
qda	O
if	O
there	O
are	O
relatively	O
few	O
training	B
observations	O
and	O
so	O
reducing	O
variance	B
is	O
crucial	O
.	O
in	O
contrast	B
,	O
qda	O
is	O
recommended	O
if	O
the	O
training	B
set	O
is	O
very	O
large	O
,	O
so	O
that	O
the	O
variance	B
of	O
the	O
classiﬁer	B
is	O
not	O
a	O
major	O
concern	O
,	O
or	O
if	O
the	O
assumption	O
of	O
a	O
common	O
covariance	O
matrix	O
for	O
the	O
k	O
classes	O
is	O
clearly	O
untenable	O
.	O
figure	O
4.9	O
illustrates	O
the	O
performances	O
of	O
lda	O
and	O
qda	O
in	O
two	O
scenarios	O
.	O
in	O
the	O
left-hand	O
panel	O
,	O
the	O
two	O
gaussian	O
classes	O
have	O
a	O
common	O
correla-	O
tion	O
of	O
0.7	O
between	O
x1	O
and	O
x2	B
.	O
as	O
a	O
result	O
,	O
the	O
bayes	O
decision	B
boundary	I
is	O
linear	B
and	O
is	O
accurately	O
approximated	O
by	O
the	O
lda	O
decision	B
boundary	I
.	O
the	O
qda	O
decision	B
boundary	I
is	O
inferior	O
,	O
because	O
it	O
suﬀers	O
from	O
higher	O
vari-	O
ance	O
without	O
a	O
corresponding	O
decrease	O
in	O
bias	B
.	O
in	O
contrast	B
,	O
the	O
right-hand	O
panel	O
displays	O
a	O
situation	O
in	O
which	O
the	O
orange	O
class	O
has	O
a	O
correlation	B
of	O
0.7	O
between	O
the	O
variables	O
and	O
the	O
blue	O
class	O
has	O
a	O
correlation	B
of	O
−0.7	O
.	O
now	O
the	O
bayes	O
decision	B
boundary	I
is	O
quadratic	B
,	O
and	O
so	O
qda	O
more	O
accurately	O
approximates	O
this	O
boundary	O
than	O
does	O
lda	O
.	O
4.5	O
a	O
comparison	O
of	O
classiﬁcation	B
methods	O
151	O
4.5	O
a	O
comparison	O
of	O
classiﬁcation	B
methods	O
in	O
this	O
chapter	O
,	O
we	O
have	O
considered	O
three	O
diﬀerent	O
classiﬁcation	B
approaches	O
:	O
logistic	B
regression	I
,	O
lda	O
,	O
and	O
qda	O
.	O
in	O
chapter	O
2	O
,	O
we	O
also	O
discussed	O
the	O
k-nearest	O
neighbors	O
(	O
knn	O
)	O
method	O
.	O
we	O
now	O
consider	O
the	O
types	O
of	O
scenarios	O
in	O
which	O
one	O
approach	B
might	O
dominate	O
the	O
others	O
.	O
though	O
their	O
motivations	O
diﬀer	O
,	O
the	O
logistic	B
regression	I
and	O
lda	O
methods	O
are	O
closely	O
connected	O
.	O
consider	O
the	O
two-class	O
setting	O
with	O
p	O
=	O
1	O
predictor	B
,	O
and	O
let	O
p1	O
(	O
x	O
)	O
and	O
p2	O
(	O
x	O
)	O
=	O
1−p1	O
(	O
x	O
)	O
be	O
the	O
probabilities	O
that	O
the	O
observation	O
x	O
=	O
x	O
belongs	O
to	O
class	O
1	O
and	O
class	O
2	O
,	O
respectively	O
.	O
in	O
the	O
lda	O
framework	O
,	O
we	O
can	O
see	O
from	O
(	O
4.12	O
)	O
to	O
(	O
4.13	O
)	O
(	O
and	O
a	O
bit	O
of	O
simple	B
algebra	O
)	O
that	O
the	O
log	O
odds	B
is	O
given	O
by	O
(	O
cid:11	O
)	O
(	O
cid:12	O
)	O
log	O
p1	O
(	O
x	O
)	O
1	O
−	O
p1	O
(	O
x	O
)	O
(	O
cid:11	O
)	O
(	O
cid:12	O
)	O
p1	O
(	O
x	O
)	O
p2	O
(	O
x	O
)	O
=	O
log	O
=	O
c0	O
+	O
c1x	O
,	O
(	O
4.24	O
)	O
where	O
c0	O
and	O
c1	O
are	O
functions	O
of	O
μ1	O
,	O
μ2	O
,	O
and	O
σ2	O
.	O
from	O
(	O
4.4	O
)	O
,	O
we	O
know	O
that	O
in	O
logistic	B
regression	I
,	O
(	O
cid:11	O
)	O
(	O
cid:12	O
)	O
log	O
p1	O
1	O
−	O
p1	O
=	O
β0	O
+	O
β1x	O
.	O
(	O
4.25	O
)	O
both	O
(	O
4.24	O
)	O
and	O
(	O
4.25	O
)	O
are	O
linear	B
functions	O
of	O
x.	O
hence	O
,	O
both	O
logistic	B
re-	O
gression	O
and	O
lda	O
produce	O
linear	B
decision	O
boundaries	O
.	O
the	O
only	O
diﬀerence	O
between	O
the	O
two	O
approaches	O
lies	O
in	O
the	O
fact	O
that	O
β0	O
and	O
β1	O
are	O
estimated	O
using	O
maximum	B
likelihood	I
,	O
whereas	O
c0	O
and	O
c1	O
are	O
computed	O
using	O
the	O
esti-	O
mated	O
mean	O
and	O
variance	B
from	O
a	O
normal	O
distribution	O
.	O
this	O
same	O
connection	O
between	O
lda	O
and	O
logistic	B
regression	I
also	O
holds	O
for	O
multidimensional	O
data	B
with	O
p	O
>	O
1.	O
since	O
logistic	B
regression	I
and	O
lda	O
diﬀer	O
only	O
in	O
their	O
ﬁtting	O
procedures	O
,	O
one	O
might	O
expect	O
the	O
two	O
approaches	O
to	O
give	O
similar	O
results	O
.	O
this	O
is	O
often	O
,	O
but	O
not	O
always	O
,	O
the	O
case	O
.	O
lda	O
assumes	O
that	O
the	O
observations	B
are	O
drawn	O
from	O
a	O
gaussian	O
distribution	B
with	O
a	O
common	O
covariance	O
matrix	O
in	O
each	O
class	O
,	O
and	O
so	O
can	O
provide	O
some	O
improvements	O
over	O
logistic	B
regression	I
when	O
this	O
assumption	O
approximately	O
holds	O
.	O
conversely	O
,	O
logistic	B
regression	I
can	O
outperform	O
lda	O
if	O
these	O
gaussian	O
assumptions	O
are	O
not	O
met	O
.	O
recall	B
from	O
chapter	O
2	O
that	O
knn	O
takes	O
a	O
completely	O
diﬀerent	O
approach	B
from	O
the	O
classiﬁers	O
seen	O
in	O
this	O
chapter	O
.	O
in	O
order	O
to	O
make	O
a	O
prediction	B
for	O
an	O
observation	O
x	O
=	O
x	O
,	O
the	O
k	O
training	B
observations	O
that	O
are	O
closest	O
to	O
x	O
are	O
identiﬁed	O
.	O
then	O
x	O
is	O
assigned	O
to	O
the	O
class	O
to	O
which	O
the	O
plurality	O
of	O
these	O
observations	B
belong	O
.	O
hence	O
knn	O
is	O
a	O
completely	O
non-parametric	B
approach	O
:	O
no	O
assumptions	O
are	O
made	O
about	O
the	O
shape	O
of	O
the	O
decision	B
boundary	I
.	O
there-	O
fore	O
,	O
we	O
can	O
expect	O
this	O
approach	B
to	O
dominate	O
lda	O
and	O
logistic	B
regression	I
when	O
the	O
decision	B
boundary	I
is	O
highly	O
non-linear	B
.	O
on	O
the	O
other	O
hand	O
,	O
knn	O
does	O
not	O
tell	O
us	O
which	O
predictors	O
are	O
important	O
;	O
we	O
don	O
’	O
t	O
get	O
a	O
table	O
of	O
coeﬃcients	O
as	O
in	O
table	O
4.3	O
.	O
152	O
4.	O
classiﬁcation	B
scenario	O
1	O
scenario	O
2	O
scenario	O
3	O
5	O
4	O
0	O
.	O
0	O
4	O
.	O
0	O
5	O
3	O
.	O
0	O
0	O
3	O
.	O
0	O
5	O
2	O
.	O
0	O
0	O
3	O
0	O
.	O
5	O
2	O
.	O
0	O
0	O
2	O
.	O
0	O
5	O
1	O
.	O
0	O
5	O
4	O
0	O
.	O
0	O
4	O
.	O
0	O
5	O
3	O
.	O
0	O
0	O
3	O
0	O
.	O
5	O
2	O
.	O
0	O
0	O
2	O
.	O
0	O
knn−1	O
knn−cv	O
lda	O
logistic	B
qda	O
knn−1	O
knn−cv	O
lda	O
logistic	B
qda	O
knn−1	O
knn−cv	O
lda	O
logistic	B
qda	O
figure	O
4.10.	O
boxplots	O
of	O
the	O
test	B
error	O
rates	O
for	O
each	O
of	O
the	O
linear	B
scenarios	O
described	O
in	O
the	O
main	O
text	O
.	O
scenario	O
4	O
scenario	O
5	O
scenario	O
6	O
0	O
4	O
.	O
0	O
5	O
3	O
.	O
0	O
0	O
3	O
.	O
0	O
0	O
4	O
.	O
0	O
5	O
3	O
.	O
0	O
0	O
3	O
.	O
0	O
5	O
2	O
.	O
0	O
0	O
2	O
.	O
0	O
2	O
3	O
.	O
0	O
0	O
3	O
.	O
0	O
8	O
2	O
.	O
0	O
6	O
2	O
.	O
0	O
4	O
2	O
.	O
0	O
2	O
2	O
.	O
0	O
0	O
2	O
.	O
0	O
8	O
1	O
.	O
0	O
knn−1	O
knn−cv	O
lda	O
logistic	B
qda	O
knn−1	O
knn−cv	O
lda	O
logistic	B
qda	O
knn−1	O
knn−cv	O
lda	O
logistic	B
qda	O
figure	O
4.11.	O
boxplots	O
of	O
the	O
test	B
error	O
rates	O
for	O
each	O
of	O
the	O
non-linear	B
sce-	O
narios	O
described	O
in	O
the	O
main	O
text	O
.	O
finally	O
,	O
qda	O
serves	O
as	O
a	O
compromise	O
between	O
the	O
non-parametric	B
knn	O
method	O
and	O
the	O
linear	B
lda	O
and	O
logistic	B
regression	I
approaches	O
.	O
since	O
qda	O
assumes	O
a	O
quadratic	B
decision	O
boundary	O
,	O
it	O
can	O
accurately	O
model	B
a	O
wider	O
range	O
of	O
problems	O
than	O
can	O
the	O
linear	B
methods	O
.	O
though	O
not	O
as	O
ﬂexible	B
as	O
knn	O
,	O
qda	O
can	O
perform	O
better	O
in	O
the	O
presence	O
of	O
a	O
limited	O
number	O
of	O
training	B
observations	O
because	O
it	O
does	O
make	O
some	O
assumptions	O
about	O
the	O
form	O
of	O
the	O
decision	B
boundary	I
.	O
to	O
illustrate	O
the	O
performances	O
of	O
these	O
four	O
classiﬁcation	B
approaches	O
,	O
we	O
generated	O
data	B
from	O
six	O
diﬀerent	O
scenarios	O
.	O
in	O
three	O
of	O
the	O
scenarios	O
,	O
the	O
bayes	O
decision	B
boundary	I
is	O
linear	B
,	O
and	O
in	O
the	O
remaining	O
scenarios	O
it	O
is	O
non-linear	B
.	O
for	O
each	O
scenario	O
,	O
we	O
produced	O
100	O
random	O
training	O
data	B
sets	O
.	O
on	O
each	O
of	O
these	O
training	B
sets	O
,	O
we	O
ﬁt	B
each	O
method	O
to	O
the	O
data	B
and	O
computed	O
the	O
resulting	O
test	B
error	O
rate	B
on	O
a	O
large	O
test	B
set	O
.	O
results	O
for	O
the	O
linear	B
scenarios	O
are	O
shown	O
in	O
figure	O
4.10	O
,	O
and	O
the	O
results	O
for	O
the	O
non-linear	B
scenarios	O
are	O
in	O
figure	O
4.11.	O
the	O
knn	O
method	O
requires	O
selection	B
of	O
k	O
,	O
the	O
number	O
of	O
neighbors	O
.	O
we	O
performed	O
knn	O
with	O
two	O
values	O
of	O
k	O
:	O
k	O
=	O
1	O
,	O
4.5	O
a	O
comparison	O
of	O
classiﬁcation	B
methods	O
153	O
and	O
a	O
value	O
of	O
k	O
that	O
was	O
chosen	O
automatically	O
using	O
an	O
approach	B
called	O
cross-validation	B
,	O
which	O
we	O
discuss	O
further	O
in	O
chapter	O
5.	O
in	O
each	O
of	O
the	O
six	O
scenarios	O
,	O
there	O
were	O
p	O
=	O
2	O
predictors	O
.	O
the	O
scenarios	O
were	O
as	O
follows	O
:	O
scenario	O
1	O
:	O
there	O
were	O
20	O
training	B
observations	O
in	O
each	O
of	O
two	O
classes	O
.	O
the	O
observations	B
within	O
each	O
class	O
were	O
uncorrelated	O
random	O
normal	O
variables	O
with	O
a	O
diﬀerent	O
mean	O
in	O
each	O
class	O
.	O
the	O
left-hand	O
panel	O
of	O
figure	O
4.10	O
shows	O
that	O
lda	O
performed	O
well	O
in	O
this	O
setting	O
,	O
as	O
one	O
would	O
expect	O
since	O
this	O
is	O
the	O
model	B
assumed	O
by	O
lda	O
.	O
knn	O
performed	O
poorly	O
because	O
it	O
paid	O
a	O
price	O
in	O
terms	O
of	O
variance	B
that	O
was	O
not	O
oﬀset	O
by	O
a	O
reduction	O
in	O
bias	B
.	O
qda	O
also	O
performed	O
worse	O
than	O
lda	O
,	O
since	O
it	O
ﬁt	B
a	O
more	O
ﬂexible	B
classiﬁer	O
than	O
necessary	O
.	O
since	O
logistic	B
regression	I
assumes	O
a	O
linear	B
decision	O
boundary	O
,	O
its	O
results	O
were	O
only	O
slightly	O
inferior	O
to	O
those	O
of	O
lda	O
.	O
scenario	O
2	O
:	O
details	O
are	O
as	O
in	O
scenario	O
1	O
,	O
except	O
that	O
within	O
each	O
class	O
,	O
the	O
two	O
predictors	O
had	O
a	O
correlation	B
of	O
−0.5	O
.	O
the	O
center	O
panel	O
of	O
figure	O
4.10	O
indicates	O
little	O
change	O
in	O
the	O
relative	O
performances	O
of	O
the	O
methods	O
as	O
compared	O
to	O
the	O
previous	O
scenario	O
.	O
scenario	O
3	O
:	O
we	O
generated	O
x1	O
and	O
x2	B
from	O
the	O
t-distribution	B
,	O
with	O
50	O
observations	B
per	O
class	O
.	O
the	O
t-distribution	B
has	O
a	O
similar	O
shape	O
to	O
the	O
normal	O
distribution	O
,	O
but	O
it	O
has	O
a	O
tendency	O
to	O
yield	O
more	O
extreme	O
points—that	O
is	O
,	O
more	O
points	O
that	O
are	O
far	O
from	O
the	O
mean	O
.	O
in	O
this	O
set-	O
ting	O
,	O
the	O
decision	B
boundary	I
was	O
still	O
linear	B
,	O
and	O
so	O
ﬁt	B
into	O
the	O
logistic	B
regression	I
framework	O
.	O
the	O
set-up	O
violated	O
the	O
assumptions	O
of	O
lda	O
,	O
since	O
the	O
observations	B
were	O
not	O
drawn	O
from	O
a	O
normal	O
distribution	O
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
4.10	O
shows	O
that	O
logistic	B
regression	I
outperformed	O
lda	O
,	O
though	O
both	O
methods	O
were	O
superior	O
to	O
the	O
other	O
approaches	O
.	O
in	O
particular	O
,	O
the	O
qda	O
results	O
deteriorated	O
considerably	O
as	O
a	O
consequence	O
of	O
non-normality	O
.	O
scenario	O
4	O
:	O
the	O
data	B
were	O
generated	O
from	O
a	O
normal	O
distribution	O
,	O
with	O
a	O
correlation	B
of	O
0.5	O
between	O
the	O
predictors	O
in	O
the	O
ﬁrst	O
class	O
,	O
and	O
correlation	B
of	O
−0.5	O
between	O
the	O
predictors	O
in	O
the	O
second	O
class	O
.	O
this	O
setup	O
corresponded	O
to	O
the	O
qda	O
assumption	O
,	O
and	O
resulted	O
in	O
quadratic	B
decision	O
boundaries	O
.	O
the	O
left-hand	O
panel	O
of	O
figure	O
4.11	O
shows	O
that	O
qda	O
outperformed	O
all	O
of	O
the	O
other	O
approaches	O
.	O
scenario	O
5	O
:	O
within	O
each	O
class	O
,	O
the	O
observations	B
were	O
generated	O
from	O
a	O
normal	O
distribution	O
with	O
uncorrelated	O
predictors	O
.	O
however	O
,	O
the	O
re-	O
sponses	O
were	O
sampled	O
from	O
the	O
logistic	B
function	O
using	O
x	O
2	O
2	O
,	O
and	O
x1	O
×	O
x2	B
as	O
predictors	O
.	O
consequently	O
,	O
there	O
is	O
a	O
quadratic	B
decision	O
1	O
,	O
x	O
2	O
boundary	O
.	O
the	O
center	O
panel	O
of	O
figure	O
4.11	O
indicates	O
that	O
qda	O
once	O
again	O
performed	O
best	O
,	O
followed	O
closely	O
by	O
knn-cv	O
.	O
the	O
linear	B
meth-	O
ods	O
had	O
poor	O
performance	O
.	O
t-	O
distribution	B
154	O
4.	O
classiﬁcation	B
scenario	O
6	O
:	O
details	O
are	O
as	O
in	O
the	O
previous	O
scenario	O
,	O
but	O
the	O
responses	O
were	O
sampled	O
from	O
a	O
more	O
complicated	O
non-linear	B
function	O
.	O
as	O
a	O
re-	O
sult	O
,	O
even	O
the	O
quadratic	B
decision	O
boundaries	O
of	O
qda	O
could	O
not	O
ade-	O
quately	O
model	B
the	O
data	B
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
4.11	O
shows	O
that	O
qda	O
gave	O
slightly	O
better	O
results	O
than	O
the	O
linear	B
methods	O
,	O
while	O
the	O
much	O
more	O
ﬂexible	B
knn-cv	O
method	O
gave	O
the	O
best	O
results	O
.	O
but	O
knn	O
with	O
k	O
=	O
1	O
gave	O
the	O
worst	O
results	O
out	O
of	O
all	O
methods	O
.	O
this	O
highlights	O
the	O
fact	O
that	O
even	O
when	O
the	O
data	B
exhibits	O
a	O
complex	O
non-	O
linear	B
relationship	O
,	O
a	O
non-parametric	B
method	O
such	O
as	O
knn	O
can	O
still	O
give	O
poor	O
results	O
if	O
the	O
level	B
of	O
smoothness	O
is	O
not	O
chosen	O
correctly	O
.	O
these	O
six	O
examples	O
illustrate	O
that	O
no	O
one	O
method	O
will	O
dominate	O
the	O
oth-	O
ers	O
in	O
every	O
situation	O
.	O
when	O
the	O
true	O
decision	O
boundaries	O
are	O
linear	B
,	O
then	O
the	O
lda	O
and	O
logistic	B
regression	I
approaches	O
will	O
tend	O
to	O
perform	O
well	O
.	O
when	O
the	O
boundaries	O
are	O
moderately	O
non-linear	B
,	O
qda	O
may	O
give	O
better	O
results	O
.	O
finally	O
,	O
for	O
much	O
more	O
complicated	O
decision	O
boundaries	O
,	O
a	O
non-parametric	B
approach	O
such	O
as	O
knn	O
can	O
be	O
superior	O
.	O
but	O
the	O
level	B
of	O
smoothness	O
for	O
a	O
non-parametric	B
approach	O
must	O
be	O
chosen	O
carefully	O
.	O
in	O
the	O
next	O
chapter	O
we	O
examine	O
a	O
number	O
of	O
approaches	O
for	O
choosing	O
the	O
correct	O
level	B
of	O
smooth-	O
ness	O
and	O
,	O
in	O
general	O
,	O
for	O
selecting	O
the	O
best	O
overall	O
method	O
.	O
finally	O
,	O
recall	B
from	O
chapter	O
3	O
that	O
in	O
the	O
regression	B
setting	O
we	O
can	O
accom-	O
modate	O
a	O
non-linear	B
relationship	O
between	O
the	O
predictors	O
and	O
the	O
response	B
by	O
performing	O
regression	B
using	O
transformations	O
of	O
the	O
predictors	O
.	O
a	O
similar	O
approach	B
could	O
be	O
taken	O
in	O
the	O
classiﬁcation	B
setting	O
.	O
for	O
instance	O
,	O
we	O
could	O
create	O
a	O
more	O
ﬂexible	B
version	O
of	O
logistic	B
regression	I
by	O
including	O
x	O
2	O
,	O
x	O
3	O
,	O
and	O
even	O
x	O
4	O
as	O
predictors	O
.	O
this	O
may	O
or	O
may	O
not	O
improve	O
logistic	B
regres-	O
sion	O
’	O
s	O
performance	O
,	O
depending	O
on	O
whether	O
the	O
increase	O
in	O
variance	B
due	O
to	O
the	O
added	O
ﬂexibility	O
is	O
oﬀset	O
by	O
a	O
suﬃciently	O
large	O
reduction	O
in	O
bias	B
.	O
we	O
could	O
do	O
the	O
same	O
for	O
lda	O
.	O
if	O
we	O
added	O
all	O
possible	O
quadratic	B
terms	O
and	O
cross-products	O
to	O
lda	O
,	O
the	O
form	O
of	O
the	O
model	B
would	O
be	O
the	O
same	O
as	O
the	O
qda	O
model	B
,	O
although	O
the	O
parameter	B
estimates	O
would	O
be	O
diﬀerent	O
.	O
this	O
device	O
allows	O
us	O
to	O
move	O
somewhere	O
between	O
an	O
lda	O
and	O
a	O
qda	O
model	B
.	O
4.6	O
lab	O
:	O
logistic	B
regression	I
,	O
lda	O
,	O
qda	O
,	O
and	O
knn	O
4.6.1	O
the	O
stock	O
market	O
data	B
we	O
will	O
begin	O
by	O
examining	O
some	O
numerical	O
and	O
graphical	O
summaries	O
of	O
the	O
smarket	O
data	B
,	O
which	O
is	O
part	O
of	O
the	O
islr	O
library	O
.	O
this	O
data	B
set	O
consists	O
of	O
percentage	O
returns	O
for	O
the	O
s	O
&	O
p	O
500	O
stock	O
index	O
over	O
1	O
,	O
250	O
days	O
,	O
from	O
the	O
beginning	O
of	O
2001	O
until	O
the	O
end	O
of	O
2005.	O
for	O
each	O
date	O
,	O
we	O
have	O
recorded	O
the	O
percentage	O
returns	O
for	O
each	O
of	O
the	O
ﬁve	O
previous	O
trading	O
days	O
,	O
lag1	O
through	O
lag5	O
.	O
we	O
have	O
also	O
recorded	O
volume	O
(	O
the	O
number	O
of	O
shares	O
traded	O
4.6	O
lab	O
:	O
logistic	B
regression	I
,	O
lda	O
,	O
qda	O
,	O
and	O
knn	O
155	O
on	O
the	O
previous	O
day	O
,	O
in	O
billions	O
)	O
,	O
today	O
(	O
the	O
percentage	O
return	O
on	O
the	O
date	O
in	O
question	O
)	O
and	O
direction	O
(	O
whether	O
the	O
market	O
was	O
up	O
or	O
down	O
on	O
this	O
date	O
)	O
.	O
>	O
library	O
(	O
islr	O
)	O
>	O
names	O
(	O
smarket	O
)	O
[	O
1	O
]	O
``	O
year	O
``	O
[	O
6	O
]	O
``	O
lag5	O
``	O
>	O
dim	O
(	O
smarket	O
)	O
[	O
1	O
]	O
1250	O
>	O
summary	O
(	O
smarket	O
)	O
9	O
''	O
lag1	O
``	O
''	O
volume	O
``	O
''	O
lag2	O
``	O
''	O
today	O
``	O
''	O
lag3	O
``	O
''	O
direction	O
``	O
''	O
lag4	O
``	O
year	O
lag1	O
lag2	O
min	O
.	O
:2001	O
1	O
st	O
qu	O
.	O
:2002	O
median	O
:2003	O
mean	O
:2003	O
3	O
rd	O
qu	O
.	O
:2004	O
max	O
.	O
:2005	O
lag3	O
:	O
-4.92200	O
min	O
.	O
1	O
st	O
qu	O
.	O
:	O
-0.64000	O
median	O
:	O
0.03850	O
mean	O
:	O
0.00172	O
3	O
rd	O
qu	O
.	O
:	O
0.59675	O
max	O
.	O
:	O
5.73300	O
min	O
.	O
:	O
-4.92200	O
1	O
st	O
qu	O
.	O
:	O
-0.63950	O
median	O
:	O
0.03900	O
mean	O
:	O
0.00383	O
3	O
rd	O
qu	O
.	O
:	O
0.59675	O
max	O
.	O
:	O
5.73300	O
min	O
.	O
:	O
-4.92200	O
1	O
st	O
qu	O
.	O
:	O
-0.63950	O
median	O
:	O
0.03900	O
mean	O
:	O
0.00392	O
3	O
rd	O
qu	O
.	O
:	O
0.59675	O
max	O
.	O
:	O
5.73300	O
lag4	O
lag5	O
:	O
-4.92200	O
min	O
.	O
1	O
st	O
qu	O
.	O
:	O
-0.64000	O
median	O
:	O
0.03850	O
mean	O
:	O
0.00164	O
3	O
rd	O
qu	O
.	O
:	O
0.59675	O
max	O
.	O
:	O
5.73300	O
:	O
-4.92200	O
min	O
.	O
1	O
st	O
qu	O
.	O
:	O
-0.64000	O
median	O
:	O
0.03850	O
mean	O
:	O
0.00561	O
3	O
rd	O
qu	O
.	O
:	O
0.59700	O
max	O
.	O
:	O
5.73300	O
volume	O
today	O
min	O
.	O
:0.356	O
1	O
st	O
qu	O
.	O
:1.257	O
median	O
:1.423	O
mean	O
:1.478	O
3	O
rd	O
qu	O
.	O
:1.642	O
max	O
.	O
:3.152	O
min	O
.	O
:	O
-4.92200	O
1	O
st	O
qu	O
.	O
:	O
-0.63950	O
median	O
:	O
0.03850	O
mean	O
:	O
0.00314	O
3	O
rd	O
qu	O
.	O
:	O
0.59675	O
max	O
.	O
:	O
5.73300	O
>	O
pairs	O
(	O
smarket	O
)	O
direction	O
down	O
:602	O
up	O
:648	O
the	O
cor	O
(	O
)	O
function	B
produces	O
a	O
matrix	O
that	O
contains	O
all	O
of	O
the	O
pairwise	O
correlations	O
among	O
the	O
predictors	O
in	O
a	O
data	B
set	O
.	O
the	O
ﬁrst	O
command	O
below	O
gives	O
an	O
error	B
message	O
because	O
the	O
direction	O
variable	B
is	O
qualitative	B
.	O
>	O
cor	O
(	O
smarket	O
)	O
error	B
in	O
cor	O
(	O
smarket	O
)	O
:	O
’	O
x	O
’	O
must	O
be	O
numeric	O
>	O
cor	O
(	O
smarket	O
[	O
,	O
-9	O
]	O
)	O
lag2	O
0.03060	O
lag3	O
0.03319	O
year	O
lag1	O
lag2	O
lag3	O
lag4	O
lag5	O
volume	O
0.5390	O
today	O
year	O
1.0000	O
0.0297	O
0.0306	O
-0.02629	O
0.0332	O
-0.01080	O
-0.02590	O
0.0357	O
-0.00299	O
-0.01085	O
-0.02405	O
0.0298	O
-0.00567	O
-0.00356	O
-0.01881	O
-0.02708	O
lag5	O
lag1	O
0.02970	O
0.02979	O
1.00000	O
-0.02629	O
-0.01080	O
-0.00299	O
-0.00567	O
1.00000	O
-0.02590	O
-0.01085	O
-0.00356	O
1.00000	O
-0.02405	O
-0.01881	O
1.00000	O
-0.02708	O
1.00000	O
0.04091	O
-0.04338	O
-0.04182	O
-0.04841	O
-0.02200	O
0.0301	O
-0.02616	O
-0.01025	O
-0.00245	O
-0.00690	O
-0.03486	O
lag4	O
0.03569	O
year	O
volume	O
0.5390	O
today	O
0.03010	O
156	O
4.	O
classiﬁcation	B
lag1	O
lag2	O
lag3	O
lag4	O
lag5	O
volume	O
today	O
0.0409	O
-0.02616	O
-0.0434	O
-0.01025	O
-0.0418	O
-0.00245	O
-0.0484	O
-0.00690	O
-0.0220	O
-0.03486	O
0.01459	O
1.00000	O
1.0000	O
0.0146	O
as	O
one	O
would	O
expect	O
,	O
the	O
correlations	O
between	O
the	O
lag	O
variables	O
and	O
to-	O
day	O
’	O
s	O
returns	O
are	O
close	O
to	O
zero	O
.	O
in	O
other	O
words	O
,	O
there	O
appears	O
to	O
be	O
little	O
correlation	B
between	O
today	O
’	O
s	O
returns	O
and	O
previous	O
days	O
’	O
returns	O
.	O
the	O
only	O
substantial	O
correlation	B
is	O
between	O
year	O
and	O
volume	O
.	O
by	O
plotting	O
the	O
data	B
we	O
see	O
that	O
volume	O
is	O
increasing	O
over	O
time	O
.	O
in	O
other	O
words	O
,	O
the	O
average	B
number	O
of	O
shares	O
traded	O
daily	O
increased	O
from	O
2001	O
to	O
2005	O
.	O
>	O
attach	O
(	O
smarket	O
)	O
>	O
plot	B
(	O
volume	O
)	O
4.6.2	O
logistic	B
regression	I
next	O
,	O
we	O
will	O
ﬁt	B
a	O
logistic	B
regression	I
model	O
in	O
order	O
to	O
predict	O
direction	O
using	O
lag1	O
through	O
lag5	O
and	O
volume	O
.	O
the	O
glm	O
(	O
)	O
function	B
ﬁts	O
generalized	O
linear	O
models	O
,	O
a	O
class	O
of	O
models	O
that	O
includes	O
logistic	B
regression	I
.	O
the	O
syntax	O
of	O
the	O
glm	O
(	O
)	O
function	B
is	O
similar	O
to	O
that	O
of	O
lm	O
(	O
)	O
,	O
except	O
that	O
we	O
must	O
pass	O
in	O
the	O
argument	B
family=binomial	O
in	O
order	O
to	O
tell	O
r	O
to	O
run	O
a	O
logistic	B
regression	I
rather	O
than	O
some	O
other	O
type	O
of	O
generalized	B
linear	I
model	I
.	O
>	O
glm	O
.	O
fit	O
s	O
=	O
glm	O
(	O
direction∼lag1	O
+	O
lag2	O
+	O
lag3	O
+	O
lag4	O
+	O
lag5	O
+	O
volume	O
,	O
glm	O
(	O
)	O
generalized	B
linear	I
model	I
data	O
=	O
smarket	O
,	O
family	O
=	O
binomial	O
)	O
>	O
summary	O
(	O
glm	O
.	O
fit	O
s	O
)	O
call	O
:	O
glm	O
(	O
formula	O
=	O
direction	O
∼	O
lag1	O
+	O
lag2	O
+	O
lag3	O
+	O
lag4	O
+	O
lag5	O
+	O
volume	O
,	O
family	O
=	O
binomial	O
,	O
data	B
=	O
smarket	O
)	O
deviance	B
residuals	O
:	O
min	O
-1.45	O
1	O
q	O
-1.20	O
median	O
1.07	O
3	O
q	O
1.15	O
max	O
1.33	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
:	O
(	O
intercept	B
)	O
-0.12600	O
lag1	O
-0.07307	O
-0.04230	O
lag2	O
0.01109	O
lag3	O
0.00936	O
lag4	O
0.01031	O
lag5	O
volume	O
0.13544	O
-0.52	O
-1.46	O
-0.84	O
0.22	O
0.19	O
0.21	O
0.86	O
estimate	O
std	O
.	O
error	B
z	O
value	O
pr	O
(	O
>	O
|	O
z	O
|	O
)	O
0.60	O
0.15	O
0.40	O
0.82	O
0.85	O
0.83	O
0.39	O
0.24074	O
0.05017	O
0.05009	O
0.04994	O
0.04997	O
0.04951	O
0.15836	O
4.6	O
lab	O
:	O
logistic	B
regression	I
,	O
lda	O
,	O
qda	O
,	O
and	O
knn	O
157	O
(	O
dispersion	O
parameter	B
for	O
binomial	O
family	O
taken	O
to	O
be	O
1	O
)	O
null	B
deviance	O
:	O
1731.2	O
residual	B
deviance	O
:	O
1727.6	O
aic	O
:	O
1742	O
on	O
1249	O
on	O
1243	O
degrees	B
of	I
freedom	I
degrees	O
of	O
freedom	O
number	O
of	O
fisher	O
scoring	O
iteration	O
s	O
:	O
3	O
the	O
smallest	O
p-value	B
here	O
is	O
associated	O
with	O
lag1	O
.	O
the	O
negative	O
coeﬃcient	O
for	O
this	O
predictor	B
suggests	O
that	O
if	O
the	O
market	O
had	O
a	O
positive	O
return	O
yesterday	O
,	O
then	O
it	O
is	O
less	O
likely	O
to	O
go	O
up	O
today	O
.	O
however	O
,	O
at	O
a	O
value	O
of	O
0.15	O
,	O
the	O
p-value	B
is	O
still	O
relatively	O
large	O
,	O
and	O
so	O
there	O
is	O
no	O
clear	O
evidence	O
of	O
a	O
real	O
association	O
between	O
lag1	O
and	O
direction	O
.	O
we	O
use	O
the	O
coef	O
(	O
)	O
function	B
in	O
order	O
to	O
access	O
just	O
the	O
coeﬃcients	O
for	O
this	O
ﬁtted	O
model	O
.	O
we	O
can	O
also	O
use	O
the	O
summary	O
(	O
)	O
function	B
to	O
access	O
particular	O
aspects	O
of	O
the	O
ﬁtted	O
model	O
,	O
such	O
as	O
the	O
p-values	O
for	O
the	O
coeﬃcients	O
.	O
>	O
coef	O
(	O
glm	O
.	O
fit	O
s	O
)	O
(	O
intercept	B
)	O
-0.12600	O
lag5	O
0.01031	O
lag1	O
-0.07307	O
volume	O
0.13544	O
>	O
summary	O
(	O
glm	O
.	O
fit	O
s	O
)	O
$	O
coef	O
lag2	O
-0.04230	O
lag3	O
0.01109	O
lag4	O
0.00936	O
estimate	O
std	O
.	O
error	B
z	O
value	O
pr	O
(	O
>	O
|	O
z	O
|	O
)	O
0.601	O
0.145	O
0.398	O
0.824	O
0.851	O
0.835	O
0.392	O
0.2407	O
0.0502	O
0.0501	O
0.0499	O
0.0500	O
0.0495	O
0.1584	O
-0.523	O
-1.457	O
-0.845	O
0.222	O
0.187	O
0.208	O
0.855	O
(	O
intercept	B
)	O
-0.12600	O
-0.07307	O
lag1	O
-0.04230	O
lag2	O
0.01109	O
lag3	O
0.00936	O
lag4	O
lag5	O
0.01031	O
volume	O
0.13544	O
>	O
summary	O
(	O
glm	O
.	O
fit	O
s	O
)	O
$	O
coef	O
[	O
,4	O
]	O
(	O
intercept	B
)	O
0.601	O
lag5	O
0.835	O
lag1	O
0.145	O
volume	O
0.392	O
lag2	O
0.398	O
lag3	O
0.824	O
lag4	O
0.851	O
the	O
predict	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
predict	O
the	O
probability	B
that	O
the	O
market	O
will	O
go	O
up	O
,	O
given	O
values	O
of	O
the	O
predictors	O
.	O
the	O
type=	O
''	O
response	B
''	O
option	O
tells	O
r	O
to	O
output	B
probabilities	O
of	O
the	O
form	O
p	O
(	O
y	O
=	O
1|x	O
)	O
,	O
as	O
opposed	O
to	O
other	O
information	O
such	O
as	O
the	O
logit	B
.	O
if	O
no	O
data	B
set	O
is	O
supplied	O
to	O
the	O
predict	O
(	O
)	O
function	B
,	O
then	O
the	O
probabilities	O
are	O
computed	O
for	O
the	O
training	B
data	O
that	O
was	O
used	O
to	O
ﬁt	B
the	O
logistic	B
regression	I
model	O
.	O
here	O
we	O
have	O
printed	O
only	O
the	O
ﬁrst	O
ten	O
probabilities	O
.	O
we	O
know	O
that	O
these	O
values	O
correspond	O
to	O
the	O
probability	B
of	O
the	O
market	O
going	O
up	O
,	O
rather	O
than	O
down	O
,	O
because	O
the	O
contrasts	O
(	O
)	O
function	B
indicates	O
that	O
r	O
has	O
created	O
a	O
dummy	B
variable	I
with	O
a	O
1	O
for	O
up	O
.	O
>	O
glm	O
.	O
probs	O
=	O
predict	O
(	O
glm	O
.	O
fit	O
s	O
,	O
type	O
=	O
''	O
response	B
``	O
)	O
>	O
glm	O
.	O
probs	O
[	O
1:10	O
]	O
3	O
10	O
0.507	O
0.481	O
0.481	O
0.515	O
0.511	O
0.507	O
0.493	O
0.509	O
0.518	O
0.489	O
1	O
8	O
9	O
2	O
4	O
5	O
6	O
7	O
158	O
4.	O
classiﬁcation	B
>	O
contrasts	O
(	O
direction	O
)	O
up	O
0	O
1	O
down	O
up	O
in	O
order	O
to	O
make	O
a	O
prediction	B
as	O
to	O
whether	O
the	O
market	O
will	O
go	O
up	O
or	O
down	O
on	O
a	O
particular	O
day	O
,	O
we	O
must	O
convert	O
these	O
predicted	O
probabilities	O
into	O
class	O
labels	O
,	O
up	O
or	O
down	O
.	O
the	O
following	O
two	O
commands	O
create	O
a	O
vector	B
of	O
class	O
predictions	O
based	O
on	O
whether	O
the	O
predicted	O
probability	B
of	O
a	O
market	O
increase	O
is	O
greater	O
than	O
or	O
less	O
than	O
0.5	O
.	O
>	O
glm	O
.	O
pred	O
=	O
rep	O
(	O
``	O
down	O
``	O
,1250	O
)	O
>	O
glm	O
.	O
pred	O
[	O
glm	O
.	O
probs	O
>	O
.5	O
]	O
=	O
''	O
up	O
``	O
the	O
ﬁrst	O
command	O
creates	O
a	O
vector	B
of	O
1,250	O
down	O
elements	O
.	O
the	O
second	O
line	B
transforms	O
to	O
up	O
all	O
of	O
the	O
elements	O
for	O
which	O
the	O
predicted	O
probability	B
of	O
a	O
market	O
increase	O
exceeds	O
0.5.	O
given	O
these	O
predictions	O
,	O
the	O
table	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
produce	O
a	O
confusion	B
matrix	I
in	O
order	O
to	O
determine	O
how	O
many	O
observations	B
were	O
correctly	O
or	O
incorrectly	O
classiﬁed	O
.	O
table	O
(	O
)	O
>	O
table	O
(	O
glm	O
.	O
pred	O
,	O
direction	O
)	O
glm	O
.	O
pred	O
down	O
direction	O
up	O
145	O
141	O
457	O
507	O
down	O
up	O
>	O
(	O
507+145	O
)	O
/1250	O
[	O
1	O
]	O
0.5216	O
>	O
mean	O
(	O
glm	O
.	O
pred	O
==	O
direction	O
)	O
[	O
1	O
]	O
0.5216	O
the	O
diagonal	O
elements	O
of	O
the	O
confusion	B
matrix	I
indicate	O
correct	O
predictions	O
,	O
while	O
the	O
oﬀ-diagonals	O
represent	O
incorrect	O
predictions	O
.	O
hence	O
our	O
model	B
correctly	O
predicted	O
that	O
the	O
market	O
would	O
go	O
up	O
on	O
507	O
days	O
and	O
that	O
it	O
would	O
go	O
down	O
on	O
145	O
days	O
,	O
for	O
a	O
total	O
of	O
507	O
+	O
145	O
=	O
652	O
correct	O
predictions	O
.	O
the	O
mean	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
compute	O
the	O
fraction	O
of	O
days	O
for	O
which	O
the	O
prediction	B
was	O
correct	O
.	O
in	O
this	O
case	O
,	O
logistic	B
regression	I
correctly	O
predicted	O
the	O
movement	O
of	O
the	O
market	O
52.2	O
%	O
of	O
the	O
time	O
.	O
at	O
ﬁrst	O
glance	O
,	O
it	O
appears	O
that	O
the	O
logistic	B
regression	I
model	O
is	O
working	O
a	O
little	O
better	O
than	O
random	O
guessing	O
.	O
however	O
,	O
this	O
result	O
is	O
misleading	O
because	O
we	O
trained	O
and	O
tested	O
the	O
model	B
on	O
the	O
same	O
set	B
of	O
1	O
,	O
250	O
observa-	O
tions	O
.	O
in	O
other	O
words	O
,	O
100	O
−	O
52.2	O
=	O
47.8	O
%	O
is	O
the	O
training	B
error	O
rate	B
.	O
as	O
we	O
have	O
seen	O
previously	O
,	O
the	O
training	B
error	O
rate	B
is	O
often	O
overly	O
optimistic—it	O
tends	O
to	O
underestimate	O
the	O
test	B
error	O
rate	B
.	O
in	O
order	O
to	O
better	O
assess	O
the	O
ac-	O
curacy	O
of	O
the	O
logistic	B
regression	I
model	O
in	O
this	O
setting	O
,	O
we	O
can	O
ﬁt	B
the	O
model	B
using	O
part	O
of	O
the	O
data	B
,	O
and	O
then	O
examine	O
how	O
well	O
it	O
predicts	O
the	O
held	O
out	O
data	B
.	O
this	O
will	O
yield	O
a	O
more	O
realistic	O
error	B
rate	I
,	O
in	O
the	O
sense	O
that	O
in	O
prac-	O
tice	O
we	O
will	O
be	O
interested	O
in	O
our	O
model	B
’	O
s	O
performance	O
not	O
on	O
the	O
data	B
that	O
we	O
used	O
to	O
ﬁt	B
the	O
model	B
,	O
but	O
rather	O
on	O
days	O
in	O
the	O
future	O
for	O
which	O
the	O
market	O
’	O
s	O
movements	O
are	O
unknown	O
.	O
4.6	O
lab	O
:	O
logistic	B
regression	I
,	O
lda	O
,	O
qda	O
,	O
and	O
knn	O
159	O
to	O
implement	O
this	O
strategy	O
,	O
we	O
will	O
ﬁrst	O
create	O
a	O
vector	B
corresponding	O
to	O
the	O
observations	B
from	O
2001	O
through	O
2004.	O
we	O
will	O
then	O
use	O
this	O
vector	B
to	O
create	O
a	O
held	O
out	O
data	B
set	O
of	O
observations	B
from	O
2005	O
.	O
>	O
train	B
=	O
(	O
year	O
<	O
2005	O
)	O
>	O
smarket	O
.2005=	O
smarket	O
[	O
!	O
train	B
,	O
]	O
>	O
dim	O
(	O
smarket	O
.2005	O
)	O
[	O
1	O
]	O
252	O
>	O
direction	O
.2005=	O
direction	O
[	O
!	O
train	B
]	O
9	O
the	O
object	O
train	B
is	O
a	O
vector	B
of	O
1	O
,	O
250	O
elements	O
,	O
corresponding	O
to	O
the	O
ob-	O
servations	O
in	O
our	O
data	B
set	O
.	O
the	O
elements	O
of	O
the	O
vector	B
that	O
correspond	O
to	O
observations	B
that	O
occurred	O
before	O
2005	O
are	O
set	B
to	O
true	O
,	O
whereas	O
those	O
that	O
correspond	O
to	O
observations	B
in	O
2005	O
are	O
set	B
to	O
false	O
.	O
the	O
object	O
train	B
is	O
a	O
boolean	O
vector	B
,	O
since	O
its	O
elements	O
are	O
true	O
and	O
false	O
.	O
boolean	O
vectors	O
can	O
be	O
used	O
to	O
obtain	O
a	O
subset	O
of	O
the	O
rows	O
or	O
columns	O
of	O
a	O
matrix	O
.	O
for	O
instance	O
,	O
the	O
command	O
smarket	O
[	O
train	B
,	O
]	O
would	O
pick	O
out	O
a	O
submatrix	O
of	O
the	O
stock	O
market	O
data	B
set	O
,	O
corresponding	O
only	O
to	O
the	O
dates	O
before	O
2005	O
,	O
since	O
those	O
are	O
the	O
ones	O
for	O
which	O
the	O
elements	O
of	O
train	B
are	O
true	O
.	O
the	O
!	O
symbol	O
can	O
be	O
used	O
to	O
reverse	O
all	O
of	O
the	O
elements	O
of	O
a	O
boolean	O
vector	B
.	O
that	O
is	O
,	O
!	O
train	B
is	O
a	O
vector	B
similar	O
to	O
train	B
,	O
except	O
that	O
the	O
elements	O
that	O
are	O
true	O
in	O
train	B
get	O
swapped	O
to	O
false	O
in	O
!	O
train	B
,	O
and	O
the	O
elements	O
that	O
are	O
false	O
in	O
train	B
get	O
swapped	O
to	O
true	O
in	O
!	O
train	B
.	O
therefore	O
,	O
smarket	O
[	O
!	O
train	B
,	O
]	O
yields	O
a	O
submatrix	O
of	O
the	O
stock	O
market	O
data	B
containing	O
only	O
the	O
observations	B
for	O
which	O
train	B
is	O
false—that	O
is	O
,	O
the	O
observations	B
with	O
dates	O
in	O
2005.	O
the	O
output	B
above	O
indicates	O
that	O
there	O
are	O
252	O
such	O
observations	B
.	O
we	O
now	O
ﬁt	B
a	O
logistic	B
regression	I
model	O
using	O
only	O
the	O
subset	O
of	O
the	O
obser-	O
vations	O
that	O
correspond	O
to	O
dates	O
before	O
2005	O
,	O
using	O
the	O
subset	O
argument	O
.	O
we	O
then	O
obtain	O
predicted	O
probabilities	O
of	O
the	O
stock	O
market	O
going	O
up	O
for	O
each	O
of	O
the	O
days	O
in	O
our	O
test	B
set—that	O
is	O
,	O
for	O
the	O
days	O
in	O
2005	O
.	O
>	O
glm	O
.	O
fit	O
s	O
=	O
glm	O
(	O
direction∼lag1	O
+	O
lag2	O
+	O
lag3	O
+	O
lag4	O
+	O
lag5	O
+	O
volume	O
,	O
data	B
=	O
smarket	O
,	O
family	O
=	O
binomial	O
,	O
subset	O
=	O
train	B
)	O
>	O
glm	O
.	O
probs	O
=	O
predict	O
(	O
glm	O
.	O
fit	O
s	O
,	O
smarket	O
.2005	O
,	O
type	O
=	O
''	O
response	B
``	O
)	O
notice	O
that	O
we	O
have	O
trained	O
and	O
tested	O
our	O
model	B
on	O
two	O
completely	O
sep-	O
arate	O
data	B
sets	O
:	O
training	B
was	O
performed	O
using	O
only	O
the	O
dates	O
before	O
2005	O
,	O
and	O
testing	O
was	O
performed	O
using	O
only	O
the	O
dates	O
in	O
2005.	O
finally	O
,	O
we	O
com-	O
pute	O
the	O
predictions	O
for	O
2005	O
and	O
compare	O
them	O
to	O
the	O
actual	O
movements	O
of	O
the	O
market	O
over	O
that	O
time	O
period	O
.	O
boolean	O
>	O
glm	O
.	O
pred	O
=	O
rep	O
(	O
``	O
down	O
``	O
,252	O
)	O
>	O
glm	O
.	O
pred	O
[	O
glm	O
.	O
probs	O
>	O
.5	O
]	O
=	O
''	O
up	O
``	O
>	O
table	O
(	O
glm	O
.	O
pred	O
,	O
direction	O
.2005	O
)	O
direction	O
.2005	O
glm	O
.	O
pred	O
down	O
up	O
77	O
97	O
34	O
44	O
down	O
up	O
>	O
mean	O
(	O
glm	O
.	O
pred	O
==	O
direction	O
.2005	O
)	O
160	O
4.	O
classiﬁcation	B
[	O
1	O
]	O
0.48	O
>	O
mean	O
(	O
glm	O
.	O
pred	O
!	O
=	O
direction	O
.2005	O
)	O
[	O
1	O
]	O
0.52	O
the	O
!	O
=	O
notation	O
means	O
not	O
equal	O
to	O
,	O
and	O
so	O
the	O
last	O
command	O
computes	O
the	O
test	B
set	O
error	B
rate	I
.	O
the	O
results	O
are	O
rather	O
disappointing	O
:	O
the	O
test	B
error	O
rate	B
is	O
52	O
%	O
,	O
which	O
is	O
worse	O
than	O
random	O
guessing	O
!	O
of	O
course	O
this	O
result	O
is	O
not	O
all	O
that	O
surprising	O
,	O
given	O
that	O
one	O
would	O
not	O
generally	O
expect	O
to	O
be	O
able	O
to	O
use	O
previous	O
days	O
’	O
returns	O
to	O
predict	O
future	O
market	O
performance	O
.	O
(	O
after	O
all	O
,	O
if	O
it	O
were	O
possible	O
to	O
do	O
so	O
,	O
then	O
the	O
authors	O
of	O
this	O
book	O
would	O
be	O
out	O
striking	O
it	O
rich	O
rather	O
than	O
writing	O
a	O
statistics	O
textbook	O
.	O
)	O
we	O
recall	B
that	O
the	O
logistic	B
regression	I
model	O
had	O
very	O
underwhelming	O
p-	O
values	O
associated	O
with	O
all	O
of	O
the	O
predictors	O
,	O
and	O
that	O
the	O
smallest	O
p-value	B
,	O
though	O
not	O
very	O
small	O
,	O
corresponded	O
to	O
lag1	O
.	O
perhaps	O
by	O
removing	O
the	O
variables	O
that	O
appear	O
not	O
to	O
be	O
helpful	O
in	O
predicting	O
direction	O
,	O
we	O
can	O
obtain	O
a	O
more	O
eﬀective	O
model	O
.	O
after	O
all	O
,	O
using	O
predictors	O
that	O
have	O
no	O
relationship	O
with	O
the	O
response	B
tends	O
to	O
cause	O
a	O
deterioration	O
in	O
the	O
test	B
error	O
rate	B
(	O
since	O
such	O
predictors	O
cause	O
an	O
increase	O
in	O
variance	B
without	O
a	O
corresponding	O
decrease	O
in	O
bias	B
)	O
,	O
and	O
so	O
removing	O
such	O
predictors	O
may	O
in	O
turn	O
yield	O
an	O
improvement	O
.	O
below	O
we	O
have	O
reﬁt	O
the	O
logistic	B
regression	I
using	O
just	O
lag1	O
and	O
lag2	O
,	O
which	O
seemed	O
to	O
have	O
the	O
highest	O
predictive	O
power	B
in	O
the	O
original	O
logistic	B
regression	I
model	O
.	O
>	O
glm	O
.	O
fit	O
s	O
=	O
glm	O
(	O
direction∼lag1	O
+	O
lag2	O
,	O
data	B
=	O
smarket	O
,	O
family	O
=	O
binomial	O
,	O
subset	O
=	O
train	B
)	O
>	O
glm	O
.	O
probs	O
=	O
predict	O
(	O
glm	O
.	O
fit	O
s	O
,	O
smarket	O
.2005	O
,	O
type	O
=	O
''	O
response	B
``	O
)	O
>	O
glm	O
.	O
pred	O
=	O
rep	O
(	O
``	O
down	O
``	O
,252	O
)	O
>	O
glm	O
.	O
pred	O
[	O
glm	O
.	O
probs	O
>	O
.5	O
]	O
=	O
''	O
up	O
``	O
>	O
table	O
(	O
glm	O
.	O
pred	O
,	O
direction	O
.2005	O
)	O
direction	O
.2005	O
up	O
glm	O
.	O
pred	O
down	O
35	O
35	O
76	O
106	O
down	O
up	O
>	O
mean	O
(	O
glm	O
.	O
pred	O
==	O
direction	O
.2005	O
)	O
[	O
1	O
]	O
0.56	O
>	O
1	O
0	O
6	O
/	O
(	O
1	O
0	O
6	O
+	O
7	O
6	O
)	O
[	O
1	O
]	O
0.582	O
now	O
the	O
results	O
appear	O
to	O
be	O
a	O
little	O
better	O
:	O
56	O
%	O
of	O
the	O
daily	O
movements	O
have	O
been	O
correctly	O
predicted	O
.	O
it	O
is	O
worth	O
noting	O
that	O
in	O
this	O
case	O
,	O
a	O
much	O
simpler	O
strategy	O
of	O
predicting	O
that	O
the	O
market	O
will	O
increase	O
every	O
day	O
will	O
also	O
be	O
correct	O
56	O
%	O
of	O
the	O
time	O
!	O
hence	O
,	O
in	O
terms	O
of	O
overall	O
error	B
rate	I
,	O
the	O
logistic	B
regression	I
method	O
is	O
no	O
better	O
than	O
the	O
na¨ıve	O
approach	B
.	O
however	O
,	O
the	O
confusion	B
matrix	I
shows	O
that	O
on	O
days	O
when	O
logistic	B
regression	I
predicts	O
an	O
increase	O
in	O
the	O
market	O
,	O
it	O
has	O
a	O
58	O
%	O
accuracy	O
rate	B
.	O
this	O
suggests	O
a	O
possible	O
trading	O
strategy	O
of	O
buying	O
on	O
days	O
when	O
the	O
model	B
predicts	O
an	O
in-	O
creasing	O
market	O
,	O
and	O
avoiding	O
trades	O
on	O
days	O
when	O
a	O
decrease	O
is	O
predicted	O
.	O
of	O
course	O
one	O
would	O
need	O
to	O
investigate	O
more	O
carefully	O
whether	O
this	O
small	O
improvement	O
was	O
real	O
or	O
just	O
due	O
to	O
random	O
chance	O
.	O
4.6	O
lab	O
:	O
logistic	B
regression	I
,	O
lda	O
,	O
qda	O
,	O
and	O
knn	O
161	O
suppose	O
that	O
we	O
want	O
to	O
predict	O
the	O
returns	O
associated	O
with	O
particular	O
values	O
of	O
lag1	O
and	O
lag2	O
.	O
in	O
particular	O
,	O
we	O
want	O
to	O
predict	O
direction	O
on	O
a	O
day	O
when	O
lag1	O
and	O
lag2	O
equal	O
1.2	O
and	O
1.1	O
,	O
respectively	O
,	O
and	O
on	O
a	O
day	O
when	O
they	O
equal	O
1.5	O
and	O
−0.8	O
.	O
we	O
do	O
this	O
using	O
the	O
predict	O
(	O
)	O
function	B
.	O
>	O
predict	O
(	O
glm	O
.	O
fit	O
s	O
,	O
newdata	O
=	O
data	B
.	O
frame	O
(	O
lag1	O
=	O
c	O
(	O
1.2	O
,1.5	O
)	O
,	O
lag2	O
=	O
c	O
(	O
1.1	O
,	O
-0.8	O
)	O
)	O
,	O
type	O
=	O
''	O
response	B
``	O
)	O
1	O
2	O
0.4791	O
0.4961	O
4.6.3	O
linear	B
discriminant	I
analysis	I
now	O
we	O
will	O
perform	O
lda	O
on	O
the	O
smarket	O
data	B
.	O
in	O
r	O
,	O
we	O
ﬁt	B
an	O
lda	O
model	B
using	O
the	O
lda	O
(	O
)	O
function	B
,	O
which	O
is	O
part	O
of	O
the	O
mass	O
library	O
.	O
notice	O
that	O
the	O
syntax	O
for	O
the	O
lda	O
(	O
)	O
function	B
is	O
identical	O
to	O
that	O
of	O
lm	O
(	O
)	O
,	O
and	O
to	O
that	O
of	O
glm	O
(	O
)	O
except	O
for	O
the	O
absence	O
of	O
the	O
family	O
option	O
.	O
we	O
ﬁt	B
the	O
model	B
using	O
only	O
the	O
observations	B
before	O
2005.	O
lda	O
(	O
)	O
>	O
library	O
(	O
mass	O
)	O
>	O
lda	O
.	O
fit	O
=	O
lda	O
(	O
direction∼lag1	O
+	O
lag2	O
,	O
data	B
=	O
smarket	O
,	O
subset	O
=	O
train	B
)	O
>	O
lda	O
.	O
fit	O
call	O
:	O
lda	O
(	O
direction	O
∼	O
lag1	O
+	O
lag2	O
,	O
data	B
=	O
smarket	O
,	O
subset	O
=	O
train	B
)	O
prior	B
p	O
r	O
o	O
b	O
a	O
b	O
i	O
l	O
i	O
t	O
i	O
e	O
s	O
of	O
groups	O
:	O
down	O
up	O
0.492	O
0.508	O
group	O
means	O
:	O
lag1	O
0.0428	O
lag2	O
0.0339	O
-0.0395	O
-0.0313	O
down	O
up	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
of	O
linear	B
d	O
i	O
s	O
c	O
r	O
i	O
m	O
i	O
n	O
a	O
n	O
t	O
s	O
:	O
ld1	O
lag1	O
-0.642	O
lag2	O
-0.514	O
>	O
plot	B
(	O
lda	O
.	O
fit	O
)	O
the	O
lda	O
output	B
indicates	O
that	O
ˆπ1	O
=	O
0.492	O
and	O
ˆπ2	O
=	O
0.508	O
;	O
in	O
other	O
words	O
,	O
49.2	O
%	O
of	O
the	O
training	B
observations	O
correspond	O
to	O
days	O
during	O
which	O
the	O
market	O
went	O
down	O
.	O
it	O
also	O
provides	O
the	O
group	O
means	O
;	O
these	O
are	O
the	O
average	B
of	O
each	O
predictor	B
within	O
each	O
class	O
,	O
and	O
are	O
used	O
by	O
lda	O
as	O
estimates	O
of	O
μk	O
.	O
these	O
suggest	O
that	O
there	O
is	O
a	O
tendency	O
for	O
the	O
previous	O
2	O
days	O
’	O
returns	O
to	O
be	O
negative	O
on	O
days	O
when	O
the	O
market	O
increases	O
,	O
and	O
a	O
tendency	O
for	O
the	O
previous	O
days	O
’	O
returns	O
to	O
be	O
positive	O
on	O
days	O
when	O
the	O
market	O
declines	O
.	O
the	O
coeﬃcients	O
of	O
linear	B
discriminants	O
output	B
provides	O
the	O
linear	B
combination	I
of	O
lag1	O
and	O
lag2	O
that	O
are	O
used	O
to	O
form	O
the	O
lda	O
decision	O
rule	O
.	O
in	O
other	O
words	O
,	O
these	O
are	O
the	O
multipliers	O
of	O
the	O
elements	O
of	O
x	O
=	O
x	O
in	O
(	O
4.19	O
)	O
.	O
if	O
−0.642×	O
lag1−	O
0.514×	O
lag2	O
is	O
large	O
,	O
then	O
the	O
lda	O
classiﬁer	B
will	O
162	O
4.	O
classiﬁcation	B
predict	O
a	O
market	O
increase	O
,	O
and	O
if	O
it	O
is	O
small	O
,	O
then	O
the	O
lda	O
classiﬁer	B
will	O
predict	O
a	O
market	O
decline	O
.	O
the	O
plot	B
(	O
)	O
function	B
produces	O
plots	O
of	O
the	O
linear	B
discriminants	O
,	O
obtained	O
by	O
computing	O
−0.642	O
×	O
lag1	O
−	O
0.514	O
×	O
lag2	O
for	O
each	O
of	O
the	O
training	B
observations	O
.	O
the	O
predict	O
(	O
)	O
function	B
returns	O
a	O
list	O
with	O
three	O
elements	O
.	O
the	O
ﬁrst	O
ele-	O
ment	O
,	O
class	O
,	O
contains	O
lda	O
’	O
s	O
predictions	O
about	O
the	O
movement	O
of	O
the	O
market	O
.	O
the	O
second	O
element	O
,	O
posterior	B
,	O
is	O
a	O
matrix	O
whose	O
kth	O
column	O
contains	O
the	O
posterior	B
probability	O
that	O
the	O
corresponding	O
observation	O
belongs	O
to	O
the	O
kth	O
class	O
,	O
computed	O
from	O
(	O
4.10	O
)	O
.	O
finally	O
,	O
x	O
contains	O
the	O
linear	B
discriminants	O
,	O
described	O
earlier	O
.	O
>	O
lda	O
.	O
pred	O
=	O
predict	O
(	O
lda	O
.	O
fit	O
,	O
smarket	O
.2005	O
)	O
>	O
names	O
(	O
lda	O
.	O
pred	O
)	O
[	O
1	O
]	O
``	O
class	O
``	O
''	O
posterior	B
``	O
``	O
x	O
``	O
as	O
we	O
observed	O
in	O
section	O
4.5	O
,	O
the	O
lda	O
and	O
logistic	B
regression	I
predictions	O
are	O
almost	O
identical	O
.	O
>	O
lda	O
.	O
class	O
=	O
lda	O
.	O
pred	O
$	O
class	O
>	O
table	O
(	O
lda	O
.	O
class	O
,	O
direction	O
.2005	O
)	O
direction	O
.2005	O
up	O
lda	O
.	O
pred	O
down	O
35	O
35	O
76	O
106	O
down	O
up	O
>	O
mean	O
(	O
lda	O
.	O
class	O
==	O
direction	O
.2005	O
)	O
[	O
1	O
]	O
0.56	O
applying	O
a	O
50	O
%	O
threshold	O
to	O
the	O
posterior	B
probabilities	O
allows	O
us	O
to	O
recre-	O
ate	O
the	O
predictions	O
contained	O
in	O
lda.pred	O
$	O
class	O
.	O
>	O
sum	O
(	O
lda	O
.	O
p	O
r	O
e	O
d	O
$	O
p	O
o	O
s	O
t	O
e	O
r	O
i	O
o	O
r	O
[	O
,1	O
]	O
>	O
=.5	O
)	O
[	O
1	O
]	O
70	O
>	O
sum	O
(	O
lda	O
.	O
p	O
r	O
e	O
d	O
$	O
p	O
o	O
s	O
t	O
e	O
r	O
i	O
o	O
r	O
[	O
,1	O
]	O
<	O
.5	O
)	O
[	O
1	O
]	O
182	O
notice	O
that	O
the	O
posterior	B
probability	O
output	B
by	O
the	O
model	B
corresponds	O
to	O
the	O
probability	B
that	O
the	O
market	O
will	O
decrease	O
:	O
>	O
lda	O
.	O
p	O
r	O
e	O
d	O
$	O
p	O
o	O
s	O
t	O
e	O
r	O
i	O
o	O
r	O
[	O
1:20	O
,1	O
]	O
>	O
lda	O
.	O
class	O
[	O
1:20	O
]	O
if	O
we	O
wanted	O
to	O
use	O
a	O
posterior	B
probability	O
threshold	O
other	O
than	O
50	O
%	O
in	O
order	O
to	O
make	O
predictions	O
,	O
then	O
we	O
could	O
easily	O
do	O
so	O
.	O
for	O
instance	O
,	O
suppose	O
that	O
we	O
wish	O
to	O
predict	O
a	O
market	O
decrease	O
only	O
if	O
we	O
are	O
very	O
certain	O
that	O
the	O
market	O
will	O
indeed	O
decrease	O
on	O
that	O
day—say	O
,	O
if	O
the	O
posterior	B
probability	O
is	O
at	O
least	O
90	O
%	O
.	O
>	O
sum	O
(	O
lda	O
.	O
p	O
r	O
e	O
d	O
$	O
p	O
o	O
s	O
t	O
e	O
r	O
i	O
o	O
r	O
[	O
,1	O
]	O
>	O
.9	O
)	O
[	O
1	O
]	O
0	O
no	O
days	O
in	O
2005	O
meet	O
that	O
threshold	O
!	O
in	O
fact	O
,	O
the	O
greatest	O
posterior	B
prob-	O
ability	O
of	O
decrease	O
in	O
all	O
of	O
2005	O
was	O
52.02	O
%	O
.	O
4.6	O
lab	O
:	O
logistic	B
regression	I
,	O
lda	O
,	O
qda	O
,	O
and	O
knn	O
163	O
4.6.4	O
quadratic	B
discriminant	I
analysis	I
we	O
will	O
now	O
ﬁt	B
a	O
qda	O
model	B
to	O
the	O
smarket	O
data	B
.	O
qda	O
is	O
implemented	O
in	O
r	O
using	O
the	O
qda	O
(	O
)	O
function	B
,	O
which	O
is	O
also	O
part	O
of	O
the	O
mass	O
library	O
.	O
the	O
syntax	O
is	O
identical	O
to	O
that	O
of	O
lda	O
(	O
)	O
.	O
>	O
qda	O
.	O
fit	O
=	O
qda	O
(	O
direction∼lag1	O
+	O
lag2	O
,	O
data	B
=	O
smarket	O
,	O
subset	O
=	O
train	B
)	O
qda	O
(	O
)	O
>	O
qda	O
.	O
fit	O
call	O
:	O
qda	O
(	O
direction	O
∼	O
lag1	O
+	O
lag2	O
,	O
data	B
=	O
smarket	O
,	O
subset	O
=	O
train	B
)	O
prior	B
p	O
r	O
o	O
b	O
a	O
b	O
i	O
l	O
i	O
t	O
i	O
e	O
s	O
of	O
groups	O
:	O
down	O
up	O
0.492	O
0.508	O
group	O
means	O
:	O
lag1	O
0.0428	O
lag2	O
0.0339	O
-0.0395	O
-0.0313	O
down	O
up	O
the	O
output	B
contains	O
the	O
group	O
means	O
.	O
but	O
it	O
does	O
not	O
contain	O
the	O
coef-	O
ﬁcients	O
of	O
the	O
linear	B
discriminants	O
,	O
because	O
the	O
qda	O
classiﬁer	B
involves	O
a	O
quadratic	B
,	O
rather	O
than	O
a	O
linear	B
,	O
function	B
of	O
the	O
predictors	O
.	O
the	O
predict	O
(	O
)	O
function	B
works	O
in	O
exactly	O
the	O
same	O
fashion	O
as	O
for	O
lda	O
.	O
>	O
qda	O
.	O
class	O
=	O
predict	O
(	O
qda	O
.	O
fit	O
,	O
smarket	O
.2005	O
)	O
$	O
class	O
>	O
table	O
(	O
qda	O
.	O
class	O
,	O
direction	O
.2005	O
)	O
direction	O
.2005	O
qda	O
.	O
class	O
down	O
up	O
down	O
up	O
30	O
20	O
81	O
121	O
>	O
mean	O
(	O
qda	O
.	O
class	O
==	O
direction	O
.2005	O
)	O
[	O
1	O
]	O
0.599	O
interestingly	O
,	O
the	O
qda	O
predictions	O
are	O
accurate	O
almost	O
60	O
%	O
of	O
the	O
time	O
,	O
even	O
though	O
the	O
2005	O
data	B
was	O
not	O
used	O
to	O
ﬁt	B
the	O
model	B
.	O
this	O
level	B
of	O
accu-	O
racy	O
is	O
quite	O
impressive	O
for	O
stock	O
market	O
data	B
,	O
which	O
is	O
known	O
to	O
be	O
quite	O
hard	O
to	O
model	B
accurately	O
.	O
this	O
suggests	O
that	O
the	O
quadratic	B
form	O
assumed	O
by	O
qda	O
may	O
capture	O
the	O
true	O
relationship	O
more	O
accurately	O
than	O
the	O
linear	B
forms	O
assumed	O
by	O
lda	O
and	O
logistic	B
regression	I
.	O
however	O
,	O
we	O
recommend	O
evaluating	O
this	O
method	O
’	O
s	O
performance	O
on	O
a	O
larger	O
test	B
set	O
before	O
betting	O
that	O
this	O
approach	B
will	O
consistently	O
beat	O
the	O
market	O
!	O
4.6.5	O
k-nearest	O
neighbors	O
we	O
will	O
now	O
perform	O
knn	O
using	O
the	O
knn	O
(	O
)	O
function	B
,	O
which	O
is	O
part	O
of	O
the	O
class	O
library	O
.	O
this	O
function	B
works	O
rather	O
diﬀerently	O
from	O
the	O
other	O
model-	O
ﬁtting	O
functions	O
that	O
we	O
have	O
encountered	O
thus	O
far	O
.	O
rather	O
than	O
a	O
two-step	O
approach	B
in	O
which	O
we	O
ﬁrst	O
ﬁt	B
the	O
model	B
and	O
then	O
we	O
use	O
the	O
model	B
to	O
make	O
predictions	O
,	O
knn	O
(	O
)	O
forms	O
predictions	O
using	O
a	O
single	B
command	O
.	O
the	O
function	B
requires	O
four	O
inputs	O
.	O
knn	O
(	O
)	O
164	O
4.	O
classiﬁcation	B
1.	O
a	O
matrix	O
containing	O
the	O
predictors	O
associated	O
with	O
the	O
training	B
data	O
,	O
labeled	O
train.x	O
below	O
.	O
2.	O
a	O
matrix	O
containing	O
the	O
predictors	O
associated	O
with	O
the	O
data	B
for	O
which	O
we	O
wish	O
to	O
make	O
predictions	O
,	O
labeled	O
test.x	O
below	O
.	O
3.	O
a	O
vector	B
containing	O
the	O
class	O
labels	O
for	O
the	O
training	B
observations	O
,	O
labeled	O
train.direction	O
below	O
.	O
4.	O
a	O
value	O
for	O
k	O
,	O
the	O
number	O
of	O
nearest	O
neighbors	O
to	O
be	O
used	O
by	O
the	O
classiﬁer	B
.	O
we	O
use	O
the	O
cbind	O
(	O
)	O
function	B
,	O
short	O
for	O
column	O
bind	O
,	O
to	O
bind	O
the	O
lag1	O
and	O
lag2	O
variables	O
together	O
into	O
two	O
matrices	O
,	O
one	O
for	O
the	O
training	B
set	O
and	O
the	O
other	O
for	O
the	O
test	B
set	O
.	O
cbind	O
(	O
)	O
>	O
library	O
(	O
class	O
)	O
>	O
train	B
.	O
x	O
=	O
cbind	O
(	O
lag1	O
,	O
lag2	O
)	O
[	O
train	B
,	O
]	O
>	O
test	B
.	O
x	O
=	O
cbind	O
(	O
lag1	O
,	O
lag2	O
)	O
[	O
!	O
train	B
,	O
]	O
>	O
train	B
.	O
direction	O
=	O
direction	O
[	O
train	B
]	O
now	O
the	O
knn	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
predict	O
the	O
market	O
’	O
s	O
movement	O
for	O
the	O
dates	O
in	O
2005.	O
we	O
set	B
a	O
random	O
seed	O
before	O
we	O
apply	O
knn	O
(	O
)	O
because	O
if	O
several	O
observations	B
are	O
tied	O
as	O
nearest	O
neighbors	O
,	O
then	O
r	O
will	O
randomly	O
break	O
the	O
tie	O
.	O
therefore	O
,	O
a	O
seed	B
must	O
be	O
set	B
in	O
order	O
to	O
ensure	O
reproducibil-	O
ity	O
of	O
results	O
.	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
knn	O
.	O
pred	O
=	O
knn	O
(	O
train	B
.x	O
,	O
test	B
.x	O
,	O
train	B
.	O
direction	O
,	O
k	O
=1	O
)	O
>	O
table	O
(	O
knn	O
.	O
pred	O
,	O
direction	O
.2005	O
)	O
direction	O
.2005	O
knn	O
.	O
pred	O
down	O
up	O
43	O
58	O
68	O
83	O
down	O
up	O
>	O
(	O
83+43	O
)	O
/252	O
[	O
1	O
]	O
0.5	O
the	O
results	O
using	O
k	O
=	O
1	O
are	O
not	O
very	O
good	O
,	O
since	O
only	O
50	O
%	O
of	O
the	O
observa-	O
tions	O
are	O
correctly	O
predicted	O
.	O
of	O
course	O
,	O
it	O
may	O
be	O
that	O
k	O
=	O
1	O
results	O
in	O
an	O
overly	O
ﬂexible	B
ﬁt	O
to	O
the	O
data	B
.	O
below	O
,	O
we	O
repeat	O
the	O
analysis	B
using	O
k	O
=	O
3	O
.	O
>	O
knn	O
.	O
pred	O
=	O
knn	O
(	O
train	B
.x	O
,	O
test	B
.x	O
,	O
train	B
.	O
direction	O
,	O
k	O
=3	O
)	O
>	O
table	O
(	O
knn	O
.	O
pred	O
,	O
direction	O
.2005	O
)	O
direction	O
.2005	O
knn	O
.	O
pred	O
down	O
up	O
48	O
54	O
63	O
87	O
down	O
up	O
>	O
mean	O
(	O
knn	O
.	O
pred	O
==	O
direction	O
.2005	O
)	O
[	O
1	O
]	O
0.536	O
the	O
results	O
have	O
improved	O
slightly	O
.	O
but	O
increasing	O
k	O
further	O
turns	O
out	O
to	O
provide	O
no	O
further	O
improvements	O
.	O
it	O
appears	O
that	O
for	O
this	O
data	B
,	O
qda	O
provides	O
the	O
best	O
results	O
of	O
the	O
methods	O
that	O
we	O
have	O
examined	O
so	O
far	O
.	O
4.6	O
lab	O
:	O
logistic	B
regression	I
,	O
lda	O
,	O
qda	O
,	O
and	O
knn	O
165	O
4.6.6	O
an	O
application	O
to	O
caravan	O
insurance	O
data	B
finally	O
,	O
we	O
will	O
apply	O
the	O
knn	O
approach	B
to	O
the	O
caravan	O
data	B
set	O
,	O
which	O
is	O
part	O
of	O
the	O
islr	O
library	O
.	O
this	O
data	B
set	O
includes	O
85	O
predictors	O
that	O
measure	O
demographic	O
characteristics	O
for	O
5,822	O
individuals	O
.	O
the	O
response	B
variable	O
is	O
purchase	O
,	O
which	O
indicates	O
whether	O
or	O
not	O
a	O
given	O
individual	O
purchases	O
a	O
caravan	O
insurance	O
policy	O
.	O
in	O
this	O
data	B
set	O
,	O
only	O
6	O
%	O
of	O
people	O
purchased	O
caravan	O
insurance	O
.	O
>	O
dim	O
(	O
caravan	O
)	O
[	O
1	O
]	O
5822	O
>	O
attach	O
(	O
caravan	O
)	O
>	O
summary	O
(	O
purchase	O
)	O
86	O
yes	O
348	O
no	O
5474	O
>	O
348/5822	O
[	O
1	O
]	O
0.0598	O
because	O
the	O
knn	O
classiﬁer	B
predicts	O
the	O
class	O
of	O
a	O
given	O
test	B
observation	O
by	O
identifying	O
the	O
observations	B
that	O
are	O
nearest	O
to	O
it	O
,	O
the	O
scale	O
of	O
the	O
variables	O
matters	O
.	O
any	O
variables	O
that	O
are	O
on	O
a	O
large	O
scale	O
will	O
have	O
a	O
much	O
larger	O
eﬀect	O
on	O
the	O
distance	B
between	O
the	O
observations	B
,	O
and	O
hence	O
on	O
the	O
knn	O
classiﬁer	B
,	O
than	O
variables	O
that	O
are	O
on	O
a	O
small	O
scale	O
.	O
for	O
instance	O
,	O
imagine	O
a	O
data	B
set	O
that	O
contains	O
two	O
variables	O
,	O
salary	O
and	O
age	O
(	O
measured	O
in	O
dollars	O
and	O
years	O
,	O
respectively	O
)	O
.	O
as	O
far	O
as	O
knn	O
is	O
concerned	O
,	O
a	O
diﬀerence	O
of	O
$	O
1,000	O
in	O
salary	O
is	O
enormous	O
compared	O
to	O
a	O
diﬀerence	O
of	O
50	O
years	O
in	O
age	O
.	O
conse-	O
quently	O
,	O
salary	O
will	O
drive	O
the	O
knn	O
classiﬁcation	B
results	O
,	O
and	O
age	O
will	O
have	O
almost	O
no	O
eﬀect	O
.	O
this	O
is	O
contrary	O
to	O
our	O
intuition	O
that	O
a	O
salary	O
diﬀerence	O
of	O
$	O
1	O
,	O
000	O
is	O
quite	O
small	O
compared	O
to	O
an	O
age	O
diﬀerence	O
of	O
50	O
years	O
.	O
further-	O
more	O
,	O
the	O
importance	B
of	O
scale	O
to	O
the	O
knn	O
classiﬁer	B
leads	O
to	O
another	O
issue	O
:	O
if	O
we	O
measured	O
salary	O
in	O
japanese	O
yen	O
,	O
or	O
if	O
we	O
measured	O
age	O
in	O
minutes	O
,	O
then	O
we	O
’	O
d	O
get	O
quite	O
diﬀerent	O
classiﬁcation	B
results	O
from	O
what	O
we	O
get	O
if	O
these	O
two	O
variables	O
are	O
measured	O
in	O
dollars	O
and	O
years	O
.	O
a	O
good	O
way	O
to	O
handle	O
this	O
problem	O
is	O
to	O
standardize	B
the	O
data	B
so	O
that	O
all	O
variables	O
are	O
given	O
a	O
mean	O
of	O
zero	O
and	O
a	O
standard	O
deviation	O
of	O
one	O
.	O
then	O
all	O
variables	O
will	O
be	O
on	O
a	O
comparable	O
scale	O
.	O
the	O
scale	O
(	O
)	O
function	B
does	O
just	O
this	O
.	O
in	O
standardizing	O
the	O
data	B
,	O
we	O
exclude	O
column	O
86	O
,	O
because	O
that	O
is	O
the	O
qualitative	B
purchase	O
variable	B
.	O
standardize	B
scale	O
(	O
)	O
>	O
s	O
t	O
a	O
n	O
d	O
a	O
r	O
d	O
i	O
z	O
e	O
d	O
.	O
x	O
=	O
scale	O
(	O
caravan	O
[	O
,	O
-86	O
]	O
)	O
>	O
var	O
(	O
caravan	O
[	O
,1	O
]	O
)	O
[	O
1	O
]	O
165	O
>	O
var	O
(	O
caravan	O
[	O
,2	O
]	O
)	O
[	O
1	O
]	O
0.165	O
>	O
var	O
(	O
s	O
t	O
a	O
n	O
d	O
a	O
r	O
d	O
i	O
z	O
e	O
d	O
.	O
x	O
[	O
,1	O
]	O
)	O
[	O
1	O
]	O
1	O
>	O
var	O
(	O
s	O
t	O
a	O
n	O
d	O
a	O
r	O
d	O
i	O
z	O
e	O
d	O
.	O
x	O
[	O
,2	O
]	O
)	O
[	O
1	O
]	O
1	O
now	O
every	O
column	O
of	O
standardized.x	O
has	O
a	O
standard	O
deviation	O
of	O
one	O
and	O
a	O
mean	O
of	O
zero	O
.	O
166	O
4.	O
classiﬁcation	B
we	O
now	O
split	O
the	O
observations	B
into	O
a	O
test	B
set	O
,	O
containing	O
the	O
ﬁrst	O
1,000	O
observations	B
,	O
and	O
a	O
training	B
set	O
,	O
containing	O
the	O
remaining	O
observations	B
.	O
we	O
ﬁt	B
a	O
knn	O
model	B
on	O
the	O
training	B
data	O
using	O
k	O
=	O
1	O
,	O
and	O
evaluate	O
its	O
performance	O
on	O
the	O
test	B
data	O
.	O
>	O
test	B
=1:1000	O
>	O
train	B
.	O
x	O
=	O
s	O
t	O
a	O
n	O
d	O
a	O
r	O
d	O
i	O
z	O
e	O
d	O
.	O
x	O
[	O
-	O
test	B
,	O
]	O
>	O
test	B
.	O
x	O
=	O
s	O
t	O
a	O
n	O
d	O
a	O
r	O
d	O
i	O
z	O
e	O
d	O
.	O
x	O
[	O
test	B
,	O
]	O
>	O
train	B
.	O
y	O
=	O
purchase	O
[	O
-	O
test	B
]	O
>	O
test	B
.	O
y	O
=	O
purchase	O
[	O
test	B
]	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
knn	O
.	O
pred	O
=	O
knn	O
(	O
train	B
.x	O
,	O
test	B
.x	O
,	O
train	B
.y	O
,	O
k	O
=1	O
)	O
>	O
mean	O
(	O
test	B
.	O
y	O
!	O
=	O
knn	O
.	O
pred	O
)	O
[	O
1	O
]	O
0.118	O
>	O
mean	O
(	O
test	B
.	O
y	O
!	O
=	O
''	O
no	O
``	O
)	O
[	O
1	O
]	O
0.059	O
the	O
vector	B
test	O
is	O
numeric	O
,	O
with	O
values	O
from	O
1	O
through	O
1	O
,	O
000.	O
typing	O
standardized.x	O
[	O
test	B
,	O
]	O
yields	O
the	O
submatrix	O
of	O
the	O
data	B
containing	O
the	O
ob-	O
servations	O
whose	O
indices	O
range	O
from	O
1	O
to	O
1	O
,	O
000	O
,	O
whereas	O
typing	O
standardized.x	O
[	O
-test	O
,	O
]	O
yields	O
the	O
submatrix	O
containing	O
the	O
observations	B
whose	O
indices	O
do	O
not	O
range	O
from	O
1	O
to	O
1	O
,	O
000.	O
the	O
knn	O
error	B
rate	I
on	O
the	O
1,000	O
test	B
observations	O
is	O
just	O
under	O
12	O
%	O
.	O
at	O
ﬁrst	O
glance	O
,	O
this	O
may	O
ap-	O
pear	O
to	O
be	O
fairly	O
good	O
.	O
however	O
,	O
since	O
only	O
6	O
%	O
of	O
customers	O
purchased	O
insurance	O
,	O
we	O
could	O
get	O
the	O
error	B
rate	I
down	O
to	O
6	O
%	O
by	O
always	O
predicting	O
no	O
regardless	O
of	O
the	O
values	O
of	O
the	O
predictors	O
!	O
suppose	O
that	O
there	O
is	O
some	O
non-trivial	O
cost	O
to	O
trying	O
to	O
sell	O
insurance	O
to	O
a	O
given	O
individual	O
.	O
for	O
instance	O
,	O
perhaps	O
a	O
salesperson	O
must	O
visit	O
each	O
potential	O
customer	O
.	O
if	O
the	O
company	O
tries	O
to	O
sell	O
insurance	O
to	O
a	O
random	O
selection	O
of	O
customers	O
,	O
then	O
the	O
success	O
rate	B
will	O
be	O
only	O
6	O
%	O
,	O
which	O
may	O
be	O
far	O
too	O
low	O
given	O
the	O
costs	O
involved	O
.	O
instead	O
,	O
the	O
company	O
would	O
like	O
to	O
try	O
to	O
sell	O
insurance	O
only	O
to	O
customers	O
who	O
are	O
likely	O
to	O
buy	O
it	O
.	O
so	O
the	O
overall	O
error	B
rate	I
is	O
not	O
of	O
interest	O
.	O
instead	O
,	O
the	O
fraction	O
of	O
individuals	O
that	O
are	O
correctly	O
predicted	O
to	O
buy	O
insurance	O
is	O
of	O
interest	O
.	O
it	O
turns	O
out	O
that	O
knn	O
with	O
k	O
=	O
1	O
does	O
far	O
better	O
than	O
random	O
guessing	O
among	O
the	O
customers	O
that	O
are	O
predicted	O
to	O
buy	O
insurance	O
.	O
among	O
77	O
such	O
customers	O
,	O
9	O
,	O
or	O
11.7	O
%	O
,	O
actually	O
do	O
purchase	O
insurance	O
.	O
this	O
is	O
double	O
the	O
rate	B
that	O
one	O
would	O
obtain	O
from	O
random	O
guessing	O
.	O
>	O
table	O
(	O
knn	O
.	O
pred	O
,	O
test	B
.	O
y	O
)	O
knn	O
.	O
pred	O
no	O
yes	O
test	B
.	O
y	O
no	O
yes	O
50	O
9	O
873	O
68	O
>	O
9/	O
(	O
68+9	O
)	O
[	O
1	O
]	O
0.117	O
using	O
k	O
=	O
3	O
,	O
the	O
success	O
rate	B
increases	O
to	O
19	O
%	O
,	O
and	O
with	O
k	O
=	O
5	O
the	O
rate	B
is	O
26.7	O
%	O
.	O
this	O
is	O
over	O
four	O
times	O
the	O
rate	B
that	O
results	O
from	O
random	O
guessing	O
.	O
it	O
appears	O
that	O
knn	O
is	O
ﬁnding	O
some	O
real	O
patterns	O
in	O
a	O
diﬃcult	O
data	B
set	O
!	O
4.6	O
lab	O
:	O
logistic	B
regression	I
,	O
lda	O
,	O
qda	O
,	O
and	O
knn	O
167	O
>	O
knn	O
.	O
pred	O
=	O
knn	O
(	O
train	B
.x	O
,	O
test	B
.x	O
,	O
train	B
.y	O
,	O
k	O
=3	O
)	O
>	O
table	O
(	O
knn	O
.	O
pred	O
,	O
test	B
.	O
y	O
)	O
knn	O
.	O
pred	O
no	O
yes	O
test	B
.	O
y	O
no	O
yes	O
54	O
5	O
920	O
21	O
>	O
5/26	O
[	O
1	O
]	O
0.192	O
>	O
knn	O
.	O
pred	O
=	O
knn	O
(	O
train	B
.x	O
,	O
test	B
.x	O
,	O
train	B
.y	O
,	O
k	O
=5	O
)	O
>	O
table	O
(	O
knn	O
.	O
pred	O
,	O
test	B
.	O
y	O
)	O
knn	O
.	O
pred	O
no	O
yes	O
test	B
.	O
y	O
no	O
yes	O
55	O
4	O
930	O
11	O
>	O
4/15	O
[	O
1	O
]	O
0.267	O
as	O
a	O
comparison	O
,	O
we	O
can	O
also	O
ﬁt	B
a	O
logistic	B
regression	I
model	O
to	O
the	O
data	B
.	O
if	O
we	O
use	O
0.5	O
as	O
the	O
predicted	O
probability	B
cut-oﬀ	O
for	O
the	O
classiﬁer	B
,	O
then	O
we	O
have	O
a	O
problem	O
:	O
only	O
seven	O
of	O
the	O
test	B
observations	O
are	O
predicted	O
to	O
purchase	O
insurance	O
.	O
even	O
worse	O
,	O
we	O
are	O
wrong	O
about	O
all	O
of	O
these	O
!	O
however	O
,	O
we	O
are	O
not	O
required	O
to	O
use	O
a	O
cut-oﬀ	O
of	O
0.5.	O
if	O
we	O
instead	O
predict	O
a	O
purchase	O
any	O
time	O
the	O
predicted	O
probability	B
of	O
purchase	O
exceeds	O
0.25	O
,	O
we	O
get	O
much	O
better	O
results	O
:	O
we	O
predict	O
that	O
33	O
people	O
will	O
purchase	O
insurance	O
,	O
and	O
we	O
are	O
correct	O
for	O
about	O
33	O
%	O
of	O
these	O
people	O
.	O
this	O
is	O
over	O
ﬁve	O
times	O
better	O
than	O
random	O
guessing	O
!	O
>	O
glm	O
.	O
fit	O
s	O
=	O
glm	O
(	O
purchase∼	O
.	O
,	O
data	B
=	O
caravan	O
,	O
family	O
=	O
binomial	O
,	O
subset	O
=	O
-	O
test	B
)	O
:	O
fitted	O
p	O
r	O
o	O
b	O
a	O
b	O
i	O
l	O
i	O
t	O
i	O
e	O
s	O
numerical	O
l	O
y	O
0	O
or	O
1	O
occurred	O
,	O
caravan	O
[	O
test	B
,	O
]	O
,	O
type	O
=	O
''	O
response	B
``	O
)	O
warning	O
message	O
:	O
g	O
lm	O
.	O
fit	O
s	O
>	O
glm	O
.	O
probs	O
=	O
predict	O
(	O
>	O
glm	O
.	O
pred	O
=	O
rep	O
(	O
``	O
no	O
``	O
,1000	O
)	O
>	O
glm	O
.	O
pred	O
[	O
glm	O
.	O
probs	O
>	O
.5	O
]	O
=	O
''	O
yes	O
``	O
>	O
table	O
(	O
glm	O
.	O
pred	O
,	O
test	B
.	O
y	O
)	O
g	O
lm	O
.	O
fit	O
s	O
glm	O
.	O
pred	O
no	O
yes	O
test	B
.	O
y	O
no	O
yes	O
59	O
0	O
934	O
7	O
>	O
glm	O
.	O
pred	O
=	O
rep	O
(	O
``	O
no	O
``	O
,1000	O
)	O
>	O
glm	O
.	O
pred	O
[	O
glm	O
.	O
probs	O
>	O
.25	O
]	O
=	O
''	O
yes	O
``	O
>	O
table	O
(	O
glm	O
.	O
pred	O
,	O
test	B
.	O
y	O
)	O
test	B
.	O
y	O
no	O
yes	O
48	O
11	O
glm	O
.	O
pred	O
no	O
yes	O
919	O
22	O
>	O
11/	O
(	O
22+11	O
)	O
[	O
1	O
]	O
0.333	O
168	O
4.	O
classiﬁcation	B
4.7	O
exercises	O
conceptual	O
1.	O
using	O
a	O
little	O
bit	O
of	O
algebra	O
,	O
prove	O
that	O
(	O
4.2	O
)	O
is	O
equivalent	O
to	O
(	O
4.3	O
)	O
.	O
in	O
other	O
words	O
,	O
the	O
logistic	B
function	O
representation	O
and	O
logit	B
represen-	O
tation	O
for	O
the	O
logistic	B
regression	I
model	O
are	O
equivalent	O
.	O
2.	O
it	O
was	O
stated	O
in	O
the	O
text	O
that	O
classifying	O
an	O
observation	O
to	O
the	O
class	O
for	O
which	O
(	O
4.12	O
)	O
is	O
largest	O
is	O
equivalent	O
to	O
classifying	O
an	O
observation	O
to	O
the	O
class	O
for	O
which	O
(	O
4.13	O
)	O
is	O
largest	O
.	O
prove	O
that	O
this	O
is	O
the	O
case	O
.	O
in	O
other	O
words	O
,	O
under	O
the	O
assumption	O
that	O
the	O
observations	B
in	O
the	O
kth	O
class	O
are	O
drawn	O
from	O
a	O
n	O
(	O
μk	O
,	O
σ2	O
)	O
distribution	B
,	O
the	O
bayes	O
’	O
classiﬁer	B
assigns	O
an	O
observation	O
to	O
the	O
class	O
for	O
which	O
the	O
discriminant	B
function	I
is	O
maximized	O
.	O
3.	O
this	O
problem	O
relates	O
to	O
the	O
qda	O
model	B
,	O
in	O
which	O
the	O
observations	B
within	O
each	O
class	O
are	O
drawn	O
from	O
a	O
normal	O
distribution	O
with	O
a	O
class-	O
speciﬁc	O
mean	O
vector	O
and	O
a	O
class	O
speciﬁc	O
covariance	O
matrix	O
.	O
we	O
con-	O
sider	O
the	O
simple	B
case	O
where	O
p	O
=	O
1	O
;	O
i.e	O
.	O
there	O
is	O
only	O
one	O
feature	B
.	O
tribution	O
,	O
x	O
∼	O
n	O
(	O
μk	O
,	O
σ2	O
suppose	O
that	O
we	O
have	O
k	O
classes	O
,	O
and	O
that	O
if	O
an	O
observation	O
belongs	O
to	O
the	O
kth	O
class	O
then	O
x	O
comes	O
from	O
a	O
one-dimensional	O
normal	O
dis-	O
k	O
)	O
.	O
recall	B
that	O
the	O
density	B
function	I
for	O
the	O
one-dimensional	O
normal	O
distribution	O
is	O
given	O
in	O
(	O
4.11	O
)	O
.	O
prove	O
that	O
in	O
this	O
case	O
,	O
the	O
bayes	O
’	O
classiﬁer	B
is	O
not	O
linear	B
.	O
argue	O
that	O
it	O
is	O
in	O
fact	O
quadratic	B
.	O
hint	O
:	O
for	O
this	O
problem	O
,	O
you	O
should	O
follow	O
the	O
arguments	O
laid	O
out	O
in	O
section	O
4.4.2	O
,	O
but	O
without	O
making	O
the	O
assumption	O
that	O
σ2	O
1	O
=	O
.	O
.	O
.	O
=	O
σ2	O
k	O
.	O
4.	O
when	O
the	O
number	O
of	O
features	O
p	O
is	O
large	O
,	O
there	O
tends	O
to	O
be	O
a	O
deteri-	O
oration	O
in	O
the	O
performance	O
of	O
knn	O
and	O
other	O
local	B
approaches	O
that	O
perform	O
prediction	B
using	O
only	O
observations	B
that	O
are	O
near	O
the	O
test	B
ob-	O
servation	O
for	O
which	O
a	O
prediction	B
must	O
be	O
made	O
.	O
this	O
phenomenon	O
is	O
known	O
as	O
the	O
curse	B
of	I
dimensionality	I
,	O
and	O
it	O
ties	O
into	O
the	O
fact	O
that	O
non-parametric	B
approaches	O
often	O
perform	O
poorly	O
when	O
p	O
is	O
large	O
.	O
we	O
will	O
now	O
investigate	O
this	O
curse	O
.	O
curse	O
of	O
di-	O
mensionality	O
(	O
a	O
)	O
suppose	O
that	O
we	O
have	O
a	O
set	B
of	O
observations	B
,	O
each	O
with	O
measure-	O
ments	O
on	O
p	O
=	O
1	O
feature	B
,	O
x.	O
we	O
assume	O
that	O
x	O
is	O
uniformly	O
(	O
evenly	O
)	O
distributed	O
on	O
[	O
0	O
,	O
1	O
]	O
.	O
associated	O
with	O
each	O
observation	O
is	O
a	O
response	B
value	O
.	O
suppose	O
that	O
we	O
wish	O
to	O
predict	O
a	O
test	B
obser-	O
vation	O
’	O
s	O
response	B
using	O
only	O
observations	B
that	O
are	O
within	O
10	O
%	O
of	O
the	O
range	O
of	O
x	O
closest	O
to	O
that	O
test	B
observation	O
.	O
for	O
instance	O
,	O
in	O
order	O
to	O
predict	O
the	O
response	B
for	O
a	O
test	B
observation	O
with	O
x	O
=	O
0.6	O
,	O
4.7	O
exercises	O
169	O
we	O
will	O
use	O
observations	B
in	O
the	O
range	O
[	O
0.55	O
,	O
0.65	O
]	O
.	O
on	O
average	B
,	O
what	O
fraction	O
of	O
the	O
available	O
observations	B
will	O
we	O
use	O
to	O
make	O
the	O
prediction	B
?	O
(	O
b	O
)	O
now	O
suppose	O
that	O
we	O
have	O
a	O
set	B
of	O
observations	B
,	O
each	O
with	O
measurements	O
on	O
p	O
=	O
2	O
features	O
,	O
x1	O
and	O
x2	B
.	O
we	O
assume	O
that	O
(	O
x1	O
,	O
x2	B
)	O
are	O
uniformly	O
distributed	O
on	O
[	O
0	O
,	O
1	O
]	O
×	O
[	O
0	O
,	O
1	O
]	O
.	O
we	O
wish	O
to	O
predict	O
a	O
test	B
observation	O
’	O
s	O
response	B
using	O
only	O
observations	B
that	O
are	O
within	O
10	O
%	O
of	O
the	O
range	O
of	O
x1	O
and	O
within	O
10	O
%	O
of	O
the	O
range	O
of	O
x2	B
closest	O
to	O
that	O
test	B
observation	O
.	O
for	O
instance	O
,	O
in	O
order	O
to	O
predict	O
the	O
response	B
for	O
a	O
test	B
observation	O
with	O
x1	O
=	O
0.6	O
and	O
x2	B
=	O
0.35	O
,	O
we	O
will	O
use	O
observations	B
in	O
the	O
range	O
[	O
0.55	O
,	O
0.65	O
]	O
for	O
x1	O
and	O
in	O
the	O
range	O
[	O
0.3	O
,	O
0.4	O
]	O
for	O
x2	O
.	O
on	O
average	B
,	O
what	O
fraction	O
of	O
the	O
available	O
observations	B
will	O
we	O
use	O
to	O
make	O
the	O
prediction	B
?	O
(	O
c	O
)	O
now	O
suppose	O
that	O
we	O
have	O
a	O
set	B
of	O
observations	B
on	O
p	O
=	O
100	O
fea-	O
tures	O
.	O
again	O
the	O
observations	B
are	O
uniformly	O
distributed	O
on	O
each	O
feature	B
,	O
and	O
again	O
each	O
feature	B
ranges	O
in	O
value	O
from	O
0	O
to	O
1.	O
we	O
wish	O
to	O
predict	O
a	O
test	B
observation	O
’	O
s	O
response	B
using	O
observations	B
within	O
the	O
10	O
%	O
of	O
each	O
feature	B
’	O
s	O
range	O
that	O
is	O
closest	O
to	O
that	O
test	B
observation	O
.	O
what	O
fraction	O
of	O
the	O
available	O
observations	B
will	O
we	O
use	O
to	O
make	O
the	O
prediction	B
?	O
(	O
d	O
)	O
using	O
your	O
answers	O
to	O
parts	O
(	O
a	O
)	O
–	O
(	O
c	O
)	O
,	O
argue	O
that	O
a	O
drawback	O
of	O
knn	O
when	O
p	O
is	O
large	O
is	O
that	O
there	O
are	O
very	O
few	O
training	B
obser-	O
vations	O
“	O
near	O
”	O
any	O
given	O
test	B
observation	O
.	O
(	O
e	O
)	O
now	O
suppose	O
that	O
we	O
wish	O
to	O
make	O
a	O
prediction	B
for	O
a	O
test	B
obser-	O
vation	O
by	O
creating	O
a	O
p-dimensional	O
hypercube	O
centered	O
around	O
the	O
test	B
observation	O
that	O
contains	O
,	O
on	O
average	B
,	O
10	O
%	O
of	O
the	O
train-	O
ing	O
observations	B
.	O
for	O
p	O
=	O
1	O
,	O
2	O
,	O
and	O
100	O
,	O
what	O
is	O
the	O
length	O
of	O
each	O
side	O
of	O
the	O
hypercube	O
?	O
comment	O
on	O
your	O
answer	O
.	O
note	O
:	O
a	O
hypercube	O
is	O
a	O
generalization	O
of	O
a	O
cube	O
to	O
an	O
arbitrary	O
number	O
of	O
dimensions	O
.	O
when	O
p	O
=	O
1	O
,	O
a	O
hypercube	O
is	O
simply	O
a	O
line	B
segment	O
,	O
when	O
p	O
=	O
2	O
it	O
is	O
a	O
square	O
,	O
and	O
when	O
p	O
=	O
100	O
it	O
is	O
a	O
100-dimensional	O
cube	O
.	O
5.	O
we	O
now	O
examine	O
the	O
diﬀerences	O
between	O
lda	O
and	O
qda	O
.	O
(	O
a	O
)	O
if	O
the	O
bayes	O
decision	B
boundary	I
is	O
linear	B
,	O
do	O
we	O
expect	O
lda	O
or	O
qda	O
to	O
perform	O
better	O
on	O
the	O
training	B
set	O
?	O
on	O
the	O
test	B
set	O
?	O
(	O
b	O
)	O
if	O
the	O
bayes	O
decision	B
boundary	I
is	O
non-linear	B
,	O
do	O
we	O
expect	O
lda	O
or	O
qda	O
to	O
perform	O
better	O
on	O
the	O
training	B
set	O
?	O
on	O
the	O
test	B
set	O
?	O
(	O
c	O
)	O
in	O
general	O
,	O
as	O
the	O
sample	O
size	O
n	O
increases	O
,	O
do	O
we	O
expect	O
the	O
test	B
prediction	O
accuracy	O
of	O
qda	O
relative	O
to	O
lda	O
to	O
improve	O
,	O
decline	O
,	O
or	O
be	O
unchanged	O
?	O
why	O
?	O
170	O
4.	O
classiﬁcation	B
(	O
d	O
)	O
true	O
or	O
false	O
:	O
even	O
if	O
the	O
bayes	O
decision	B
boundary	I
for	O
a	O
given	O
problem	O
is	O
linear	B
,	O
we	O
will	O
probably	O
achieve	O
a	O
superior	O
test	B
er-	O
ror	O
rate	B
using	O
qda	O
rather	O
than	O
lda	O
because	O
qda	O
is	O
ﬂexible	B
enough	O
to	O
model	B
a	O
linear	B
decision	O
boundary	O
.	O
justify	O
your	O
answer	O
.	O
6.	O
suppose	O
we	O
collect	O
data	B
for	O
a	O
group	O
of	O
students	O
in	O
a	O
statistics	O
class	O
with	O
variables	O
x1	O
=	O
hours	O
studied	O
,	O
x2	B
=	O
undergrad	O
gpa	O
,	O
and	O
y	O
=	O
receive	O
an	O
a.	O
we	O
ﬁt	B
a	O
logistic	B
regression	I
and	O
produce	O
estimated	O
coeﬃcient	B
,	O
ˆβ0	O
=	O
−6	O
,	O
ˆβ1	O
=	O
0.05	O
,	O
ˆβ2	O
=	O
1	O
.	O
(	O
a	O
)	O
estimate	O
the	O
probability	B
that	O
a	O
student	O
who	O
studies	O
for	O
40	O
h	O
and	O
has	O
an	O
undergrad	O
gpa	O
of	O
3.5	O
gets	O
an	O
a	O
in	O
the	O
class	O
.	O
(	O
b	O
)	O
how	O
many	O
hours	O
would	O
the	O
student	O
in	O
part	O
(	O
a	O
)	O
need	O
to	O
study	O
to	O
have	O
a	O
50	O
%	O
chance	O
of	O
getting	O
an	O
a	O
in	O
the	O
class	O
?	O
7.	O
suppose	O
that	O
we	O
wish	O
to	O
predict	O
whether	O
a	O
given	O
stock	O
will	O
issue	O
a	O
dividend	O
this	O
year	O
(	O
“	O
yes	O
”	O
or	O
“	O
no	O
”	O
)	O
based	O
on	O
x	O
,	O
last	O
year	O
’	O
s	O
percent	O
proﬁt	O
.	O
we	O
examine	O
a	O
large	O
number	O
of	O
companies	O
and	O
discover	O
that	O
the	O
mean	O
value	O
of	O
x	O
for	O
companies	O
that	O
issued	O
a	O
dividend	O
was	O
¯x	O
=	O
10	O
,	O
while	O
the	O
mean	O
for	O
those	O
that	O
didn	O
’	O
t	O
was	O
¯x	O
=	O
0.	O
in	O
addition	O
,	O
the	O
variance	B
of	O
x	O
for	O
these	O
two	O
sets	O
of	O
companies	O
was	O
ˆσ2	O
=	O
36.	O
finally	O
,	O
80	O
%	O
of	O
companies	O
issued	O
dividends	O
.	O
assuming	O
that	O
x	O
follows	O
a	O
nor-	O
mal	O
distribution	B
,	O
predict	O
the	O
probability	B
that	O
a	O
company	O
will	O
issue	O
a	O
dividend	O
this	O
year	O
given	O
that	O
its	O
percentage	O
proﬁt	O
was	O
x	O
=	O
4	O
last	O
year	O
.	O
hint	O
:	O
recall	B
that	O
the	O
density	B
function	I
for	O
a	O
normal	O
random	O
variable	B
is	O
f	O
(	O
x	O
)	O
=	O
1√	O
.	O
you	O
will	O
need	O
to	O
use	O
bayes	O
’	O
theorem	O
.	O
−	O
(	O
x−μ	O
)	O
2/2σ2	O
2πσ2	O
e	O
8.	O
suppose	O
that	O
we	O
take	O
a	O
data	B
set	O
,	O
divide	O
it	O
into	O
equally-sized	O
training	B
and	O
test	B
sets	O
,	O
and	O
then	O
try	O
out	O
two	O
diﬀerent	O
classiﬁcation	B
procedures	O
.	O
first	O
we	O
use	O
logistic	B
regression	I
and	O
get	O
an	O
error	B
rate	I
of	O
20	O
%	O
on	O
the	O
training	B
data	O
and	O
30	O
%	O
on	O
the	O
test	B
data	O
.	O
next	O
we	O
use	O
1-nearest	O
neigh-	O
bors	O
(	O
i.e	O
.	O
k	O
=	O
1	O
)	O
and	O
get	O
an	O
average	B
error	O
rate	B
(	O
averaged	O
over	O
both	O
test	B
and	O
training	B
data	O
sets	O
)	O
of	O
18	O
%	O
.	O
based	O
on	O
these	O
results	O
,	O
which	O
method	O
should	O
we	O
prefer	O
to	O
use	O
for	O
classiﬁcation	O
of	O
new	O
observations	B
?	O
why	O
?	O
9.	O
this	O
problem	O
has	O
to	O
do	O
with	O
odds	B
.	O
(	O
a	O
)	O
on	O
average	B
,	O
what	O
fraction	O
of	O
people	O
with	O
an	O
odds	B
of	O
0.37	O
of	O
defaulting	O
on	O
their	O
credit	O
card	O
payment	O
will	O
in	O
fact	O
default	O
?	O
(	O
b	O
)	O
suppose	O
that	O
an	O
individual	O
has	O
a	O
16	O
%	O
chance	O
of	O
defaulting	O
on	O
her	O
credit	O
card	O
payment	O
.	O
what	O
are	O
the	O
odds	B
that	O
she	O
will	O
de-	O
fault	O
?	O
4.7	O
exercises	O
171	O
applied	O
10.	O
this	O
question	O
should	O
be	O
answered	O
using	O
the	O
weekly	O
data	B
set	O
,	O
which	O
is	O
part	O
of	O
the	O
islr	O
package	O
.	O
this	O
data	B
is	O
similar	O
in	O
nature	O
to	O
the	O
smarket	O
data	B
from	O
this	O
chapter	O
’	O
s	O
lab	O
,	O
except	O
that	O
it	O
contains	O
1	O
,	O
089	O
weekly	O
returns	O
for	O
21	O
years	O
,	O
from	O
the	O
beginning	O
of	O
1990	O
to	O
the	O
end	O
of	O
2010	O
.	O
(	O
a	O
)	O
produce	O
some	O
numerical	O
and	O
graphical	O
summaries	O
of	O
the	O
weekly	O
data	B
.	O
do	O
there	O
appear	O
to	O
be	O
any	O
patterns	O
?	O
(	O
b	O
)	O
use	O
the	O
full	O
data	B
set	O
to	O
perform	O
a	O
logistic	B
regression	I
with	O
direction	O
as	O
the	O
response	B
and	O
the	O
ﬁve	O
lag	O
variables	O
plus	O
volume	O
as	O
predictors	O
.	O
use	O
the	O
summary	O
function	B
to	O
print	O
the	O
results	O
.	O
do	O
any	O
of	O
the	O
predictors	O
appear	O
to	O
be	O
statistically	O
signiﬁcant	O
?	O
if	O
so	O
,	O
which	O
ones	O
?	O
(	O
c	O
)	O
compute	O
the	O
confusion	B
matrix	I
and	O
overall	O
fraction	O
of	O
correct	O
predictions	O
.	O
explain	O
what	O
the	O
confusion	B
matrix	I
is	O
telling	O
you	O
about	O
the	O
types	O
of	O
mistakes	O
made	O
by	O
logistic	B
regression	I
.	O
(	O
d	O
)	O
now	O
ﬁt	B
the	O
logistic	B
regression	I
model	O
using	O
a	O
training	B
data	O
period	O
from	O
1990	O
to	O
2008	O
,	O
with	O
lag2	O
as	O
the	O
only	O
predictor	B
.	O
compute	O
the	O
confusion	B
matrix	I
and	O
the	O
overall	O
fraction	O
of	O
correct	O
predictions	O
for	O
the	O
held	O
out	O
data	B
(	O
that	O
is	O
,	O
the	O
data	B
from	O
2009	O
and	O
2010	O
)	O
.	O
(	O
e	O
)	O
repeat	O
(	O
d	O
)	O
using	O
lda	O
.	O
(	O
f	O
)	O
repeat	O
(	O
d	O
)	O
using	O
qda	O
.	O
(	O
g	O
)	O
repeat	O
(	O
d	O
)	O
using	O
knn	O
with	O
k	O
=	O
1	O
.	O
(	O
h	O
)	O
which	O
of	O
these	O
methods	O
appears	O
to	O
provide	O
the	O
best	O
results	O
on	O
this	O
data	B
?	O
(	O
i	O
)	O
experiment	O
with	O
diﬀerent	O
combinations	O
of	O
predictors	O
,	O
includ-	O
ing	O
possible	O
transformations	O
and	O
interactions	O
,	O
for	O
each	O
of	O
the	O
methods	O
.	O
report	O
the	O
variables	O
,	O
method	O
,	O
and	O
associated	O
confu-	O
sion	O
matrix	O
that	O
appears	O
to	O
provide	O
the	O
best	O
results	O
on	O
the	O
held	O
out	O
data	B
.	O
note	O
that	O
you	O
should	O
also	O
experiment	O
with	O
values	O
for	O
k	O
in	O
the	O
knn	O
classiﬁer	B
.	O
11.	O
in	O
this	O
problem	O
,	O
you	O
will	O
develop	O
a	O
model	B
to	O
predict	O
whether	O
a	O
given	O
car	O
gets	O
high	O
or	O
low	O
gas	O
mileage	O
based	O
on	O
the	O
auto	O
data	B
set	O
.	O
(	O
a	O
)	O
create	O
a	O
binary	B
variable	O
,	O
mpg01	O
,	O
that	O
contains	O
a	O
1	O
if	O
mpg	O
contains	O
a	O
value	O
above	O
its	O
median	O
,	O
and	O
a	O
0	O
if	O
mpg	O
contains	O
a	O
value	O
below	O
its	O
median	O
.	O
you	O
can	O
compute	O
the	O
median	O
using	O
the	O
median	O
(	O
)	O
function	B
.	O
note	O
you	O
may	O
ﬁnd	O
it	O
helpful	O
to	O
use	O
the	O
data.frame	O
(	O
)	O
function	B
to	O
create	O
a	O
single	B
data	O
set	B
containing	O
both	O
mpg01	O
and	O
the	O
other	O
auto	O
variables	O
.	O
172	O
4.	O
classiﬁcation	B
(	O
b	O
)	O
explore	O
the	O
data	B
graphically	O
in	O
order	O
to	O
investigate	O
the	O
associ-	O
ation	O
between	O
mpg01	O
and	O
the	O
other	O
features	O
.	O
which	O
of	O
the	O
other	O
features	O
seem	O
most	O
likely	O
to	O
be	O
useful	O
in	O
predicting	O
mpg01	O
?	O
scat-	O
terplots	O
and	O
boxplots	O
may	O
be	O
useful	O
tools	O
to	O
answer	O
this	O
ques-	O
tion	O
.	O
describe	O
your	O
ﬁndings	O
.	O
(	O
c	O
)	O
split	O
the	O
data	B
into	O
a	O
training	B
set	O
and	O
a	O
test	B
set	O
.	O
(	O
d	O
)	O
perform	O
lda	O
on	O
the	O
training	B
data	O
in	O
order	O
to	O
predict	O
mpg01	O
using	O
the	O
variables	O
that	O
seemed	O
most	O
associated	O
with	O
mpg01	O
in	O
(	O
b	O
)	O
.	O
what	O
is	O
the	O
test	B
error	O
of	O
the	O
model	B
obtained	O
?	O
(	O
e	O
)	O
perform	O
qda	O
on	O
the	O
training	B
data	O
in	O
order	O
to	O
predict	O
mpg01	O
using	O
the	O
variables	O
that	O
seemed	O
most	O
associated	O
with	O
mpg01	O
in	O
(	O
b	O
)	O
.	O
what	O
is	O
the	O
test	B
error	O
of	O
the	O
model	B
obtained	O
?	O
(	O
f	O
)	O
perform	O
logistic	B
regression	I
on	O
the	O
training	B
data	O
in	O
order	O
to	O
pre-	O
dict	O
mpg01	O
using	O
the	O
variables	O
that	O
seemed	O
most	O
associated	O
with	O
mpg01	O
in	O
(	O
b	O
)	O
.	O
what	O
is	O
the	O
test	B
error	O
of	O
the	O
model	B
obtained	O
?	O
(	O
g	O
)	O
perform	O
knn	O
on	O
the	O
training	B
data	O
,	O
with	O
several	O
values	O
of	O
k	O
,	O
in	O
order	O
to	O
predict	O
mpg01	O
.	O
use	O
only	O
the	O
variables	O
that	O
seemed	O
most	O
associated	O
with	O
mpg01	O
in	O
(	O
b	O
)	O
.	O
what	O
test	B
errors	O
do	O
you	O
obtain	O
?	O
which	O
value	O
of	O
k	O
seems	O
to	O
perform	O
the	O
best	O
on	O
this	O
data	B
set	O
?	O
12.	O
this	O
problem	O
involves	O
writing	O
functions	O
.	O
(	O
a	O
)	O
write	O
a	O
function	B
,	O
power	B
(	O
)	O
,	O
that	O
prints	O
out	O
the	O
result	O
of	O
raising	O
2	O
to	O
the	O
3rd	O
power	B
.	O
in	O
other	O
words	O
,	O
your	O
function	B
should	O
compute	O
23	O
and	O
print	O
out	O
the	O
results	O
.	O
hint	O
:	O
recall	B
that	O
x^a	O
raises	O
x	O
to	O
the	O
power	B
a.	O
use	O
the	O
print	O
(	O
)	O
function	B
to	O
output	B
the	O
result	O
.	O
(	O
b	O
)	O
create	O
a	O
new	O
function	B
,	O
power2	O
(	O
)	O
,	O
that	O
allows	O
you	O
to	O
pass	O
any	O
two	O
numbers	O
,	O
x	O
and	O
a	O
,	O
and	O
prints	O
out	O
the	O
value	O
of	O
x^a	O
.	O
you	O
can	O
do	O
this	O
by	O
beginning	O
your	O
function	B
with	O
the	O
line	B
>	O
power2	O
=	O
function	B
(	O
x	O
,	O
a	O
)	O
{	O
you	O
should	O
be	O
able	O
to	O
call	O
your	O
function	B
by	O
entering	O
,	O
for	O
instance	O
,	O
>	O
power2	O
(	O
3	O
,8	O
)	O
on	O
the	O
command	O
line	B
.	O
this	O
should	O
output	B
the	O
value	O
of	O
38	O
,	O
namely	O
,	O
6	O
,	O
561	O
.	O
(	O
c	O
)	O
using	O
the	O
power2	O
(	O
)	O
function	B
that	O
you	O
just	O
wrote	O
,	O
compute	O
103	O
,	O
817	O
,	O
and	O
1313	O
.	O
(	O
d	O
)	O
now	O
create	O
a	O
new	O
function	B
,	O
power3	O
(	O
)	O
,	O
that	O
actually	O
returns	O
the	O
result	O
x^a	O
as	O
an	O
r	O
object	O
,	O
rather	O
than	O
simply	O
printing	O
it	O
to	O
the	O
screen	O
.	O
that	O
is	O
,	O
if	O
you	O
store	O
the	O
value	O
x^a	O
in	O
an	O
object	O
called	O
result	O
within	O
your	O
function	B
,	O
then	O
you	O
can	O
simply	O
return	O
(	O
)	O
this	O
result	O
,	O
using	O
the	O
following	O
line	B
:	O
return	O
(	O
)	O
4.7	O
exercises	O
173	O
return	O
(	O
result	O
)	O
the	O
line	B
above	O
should	O
be	O
the	O
last	O
line	B
in	O
your	O
function	B
,	O
before	O
the	O
}	O
symbol	O
.	O
(	O
e	O
)	O
now	O
using	O
the	O
power3	O
(	O
)	O
function	B
,	O
create	O
a	O
plot	B
of	O
f	O
(	O
x	O
)	O
=	O
x2	B
.	O
the	O
x-axis	O
should	O
display	O
a	O
range	O
of	O
integers	O
from	O
1	O
to	O
10	O
,	O
and	O
the	O
y-axis	O
should	O
display	O
x2	B
.	O
label	O
the	O
axes	O
appropriately	O
,	O
and	O
use	O
an	O
appropriate	O
title	O
for	O
the	O
ﬁgure	O
.	O
consider	O
displaying	O
either	O
the	O
x-axis	O
,	O
the	O
y-axis	O
,	O
or	O
both	O
on	O
the	O
log-scale	O
.	O
you	O
can	O
do	O
this	O
by	O
using	O
log=	O
‘	O
‘	O
x	O
’	O
’	O
,	O
log=	O
‘	O
‘	O
y	O
’	O
’	O
,	O
or	O
log=	O
‘	O
‘	O
xy	O
’	O
’	O
as	O
arguments	O
to	O
the	O
plot	B
(	O
)	O
function	B
.	O
(	O
f	O
)	O
create	O
a	O
function	B
,	O
plotpower	O
(	O
)	O
,	O
that	O
allows	O
you	O
to	O
create	O
a	O
plot	B
of	O
x	O
against	O
x^a	O
for	O
a	O
ﬁxed	O
a	O
and	O
for	O
a	O
range	O
of	O
values	O
of	O
x.	O
for	O
instance	O
,	O
if	O
you	O
call	O
>	O
plotpower	O
(	O
1:10	O
,3	O
)	O
then	O
a	O
plot	B
should	O
be	O
created	O
with	O
an	O
x-axis	O
taking	O
on	O
values	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
10	O
,	O
and	O
a	O
y-axis	O
taking	O
on	O
values	O
13	O
,	O
23	O
,	O
.	O
.	O
.	O
,	O
103	O
.	O
13.	O
using	O
the	O
boston	O
data	B
set	O
,	O
ﬁt	B
classiﬁcation	O
models	O
in	O
order	O
to	O
predict	O
whether	O
a	O
given	O
suburb	O
has	O
a	O
crime	O
rate	B
above	O
or	O
below	O
the	O
median	O
.	O
explore	O
logistic	B
regression	I
,	O
lda	O
,	O
and	O
knn	O
models	O
using	O
various	O
sub-	O
sets	O
of	O
the	O
predictors	O
.	O
describe	O
your	O
ﬁndings	O
.	O
5	O
resampling	B
methods	O
resampling	B
methods	O
are	O
an	O
indispensable	O
tool	O
in	O
modern	O
statistics	O
.	O
they	O
involve	O
repeatedly	O
drawing	O
samples	O
from	O
a	O
training	B
set	O
and	O
reﬁtting	O
a	O
model	B
of	O
interest	O
on	O
each	O
sample	O
in	O
order	O
to	O
obtain	O
additional	O
information	O
about	O
the	O
ﬁtted	O
model	O
.	O
for	O
example	O
,	O
in	O
order	O
to	O
estimate	O
the	O
variability	O
of	O
a	O
linear	B
regression	I
ﬁt	O
,	O
we	O
can	O
repeatedly	O
draw	O
diﬀerent	O
samples	O
from	O
the	O
training	B
data	O
,	O
ﬁt	B
a	O
linear	B
regression	I
to	O
each	O
new	O
sample	O
,	O
and	O
then	O
examine	O
the	O
extent	O
to	O
which	O
the	O
resulting	O
ﬁts	O
diﬀer	O
.	O
such	O
an	O
approach	B
may	O
allow	O
us	O
to	O
obtain	O
information	O
that	O
would	O
not	O
be	O
available	O
from	O
ﬁtting	O
the	O
model	B
only	O
once	O
using	O
the	O
original	O
training	B
sample	O
.	O
resampling	B
approaches	O
can	O
be	O
computationally	O
expensive	O
,	O
because	O
they	O
involve	O
ﬁtting	O
the	O
same	O
statistical	O
method	O
multiple	B
times	O
using	O
diﬀerent	O
subsets	O
of	O
the	O
training	B
data	O
.	O
however	O
,	O
due	O
to	O
recent	O
advances	O
in	O
computing	O
power	B
,	O
the	O
computational	O
requirements	O
of	O
resampling	B
methods	O
generally	O
are	O
not	O
prohibitive	O
.	O
in	O
this	O
chapter	O
,	O
we	O
discuss	O
two	O
of	O
the	O
most	O
commonly	O
used	O
resampling	B
methods	O
,	O
cross-validation	B
and	O
the	O
bootstrap	B
.	O
both	O
methods	O
are	O
important	O
tools	O
in	O
the	O
practical	O
application	O
of	O
many	O
statistical	O
learning	O
procedures	O
.	O
for	O
example	O
,	O
cross-validation	B
can	O
be	O
used	O
to	O
estimate	O
the	O
test	B
error	O
associated	O
with	O
a	O
given	O
statistical	O
learning	O
method	O
in	O
order	O
to	O
evaluate	O
its	O
performance	O
,	O
or	O
to	O
select	O
the	O
appropriate	O
level	B
of	O
ﬂexibility	O
.	O
the	O
process	O
of	O
evaluating	O
a	O
model	B
’	O
s	O
performance	O
is	O
known	O
as	O
model	B
assessment	I
,	O
whereas	O
the	O
process	O
of	O
selecting	O
the	O
proper	O
level	B
of	O
ﬂexibility	O
for	O
a	O
model	B
is	O
known	O
as	O
model	B
selection	I
.	O
the	O
bootstrap	B
is	O
used	O
in	O
several	O
contexts	O
,	O
most	O
commonly	O
to	O
provide	O
a	O
measure	O
of	O
accuracy	O
of	O
a	O
parameter	B
estimate	O
or	O
of	O
a	O
given	O
statistical	O
learning	O
method	O
.	O
g.	O
james	O
et	O
al.	O
,	O
an	O
introduction	O
to	O
statistical	O
learning	O
:	O
with	O
applications	O
in	O
r	O
,	O
springer	O
texts	O
in	O
statistics	O
,	O
doi	O
10.1007/978-1-4614-7138-7	O
5	O
,	O
©	O
springer	O
science+business	O
media	O
new	O
york	O
2013	O
175	O
model	B
assessment	I
model	O
selection	B
176	O
5.	O
resampling	B
methods	O
5.1	O
cross-validation	B
in	O
chapter	O
2	O
we	O
discuss	O
the	O
distinction	O
between	O
the	O
test	B
error	O
rate	B
and	O
the	O
training	B
error	O
rate	B
.	O
the	O
test	B
error	O
is	O
the	O
average	B
error	O
that	O
results	O
from	O
using	O
a	O
statistical	O
learning	O
method	O
to	O
predict	O
the	O
response	B
on	O
a	O
new	O
observation—	O
that	O
is	O
,	O
a	O
measurement	O
that	O
was	O
not	O
used	O
in	O
training	B
the	O
method	O
.	O
given	O
a	O
data	B
set	O
,	O
the	O
use	O
of	O
a	O
particular	O
statistical	O
learning	O
method	O
is	O
warranted	O
if	O
it	O
results	O
in	O
a	O
low	O
test	B
error	O
.	O
the	O
test	B
error	O
can	O
be	O
easily	O
calculated	O
if	O
a	O
designated	O
test	B
set	O
is	O
available	O
.	O
unfortunately	O
,	O
this	O
is	O
usually	O
not	O
the	O
case	O
.	O
in	O
contrast	B
,	O
the	O
training	B
error	O
can	O
be	O
easily	O
calculated	O
by	O
applying	O
the	O
statistical	O
learning	O
method	O
to	O
the	O
observations	B
used	O
in	O
its	O
training	B
.	O
but	O
as	O
we	O
saw	O
in	O
chapter	O
2	O
,	O
the	O
training	B
error	O
rate	B
often	O
is	O
quite	O
diﬀerent	O
from	O
the	O
test	B
error	O
rate	B
,	O
and	O
in	O
particular	O
the	O
former	O
can	O
dramatically	O
underestimate	O
the	O
latter	O
.	O
in	O
the	O
absence	O
of	O
a	O
very	O
large	O
designated	O
test	B
set	O
that	O
can	O
be	O
used	O
to	O
directly	O
estimate	O
the	O
test	B
error	O
rate	B
,	O
a	O
number	O
of	O
techniques	O
can	O
be	O
used	O
to	O
estimate	O
this	O
quantity	O
using	O
the	O
available	O
training	B
data	O
.	O
some	O
methods	O
make	O
a	O
mathematical	O
adjustment	O
to	O
the	O
training	B
error	O
rate	B
in	O
order	O
to	O
estimate	O
the	O
test	B
error	O
rate	B
.	O
such	O
approaches	O
are	O
discussed	O
in	O
chapter	O
6.	O
in	O
this	O
section	O
,	O
we	O
instead	O
consider	O
a	O
class	O
of	O
methods	O
that	O
estimate	O
the	O
test	B
error	O
rate	B
by	O
holding	O
out	O
a	O
subset	O
of	O
the	O
training	B
observations	O
from	O
the	O
ﬁtting	O
process	O
,	O
and	O
then	O
applying	O
the	O
statistical	O
learning	O
method	O
to	O
those	O
held	O
out	O
observations	B
.	O
in	O
sections	O
5.1.1–5.1.4	O
,	O
for	O
simplicity	O
we	O
assume	O
that	O
we	O
are	O
interested	O
in	O
performing	O
regression	B
with	O
a	O
quantitative	B
response	O
.	O
in	O
section	O
5.1.5	O
we	O
consider	O
the	O
case	O
of	O
classiﬁcation	B
with	O
a	O
qualitative	B
response	O
.	O
as	O
we	O
will	O
see	O
,	O
the	O
key	O
concepts	O
remain	O
the	O
same	O
regardless	O
of	O
whether	O
the	O
response	B
is	O
quantitative	B
or	O
qualitative	B
.	O
5.1.1	O
the	O
validation	B
set	I
approach	O
suppose	O
that	O
we	O
would	O
like	O
to	O
estimate	O
the	O
test	B
error	O
associated	O
with	O
ﬁt-	O
ting	O
a	O
particular	O
statistical	O
learning	O
method	O
on	O
a	O
set	B
of	O
observations	B
.	O
the	O
validation	B
set	I
approach	O
,	O
displayed	O
in	O
figure	O
5.1	O
,	O
is	O
a	O
very	O
simple	B
strategy	O
for	O
this	O
task	O
.	O
it	O
involves	O
randomly	O
dividing	O
the	O
available	O
set	B
of	O
observa-	O
tions	O
into	O
two	O
parts	O
,	O
a	O
training	B
set	O
and	O
a	O
validation	B
set	I
or	O
hold-out	B
set	I
.	O
the	O
model	B
is	O
ﬁt	B
on	O
the	O
training	B
set	O
,	O
and	O
the	O
ﬁtted	O
model	O
is	O
used	O
to	O
predict	O
the	O
responses	O
for	O
the	O
observations	B
in	O
the	O
validation	B
set	I
.	O
the	O
resulting	O
validation	B
set	I
error	O
rate—typically	O
assessed	O
using	O
mse	O
in	O
the	O
case	O
of	O
a	O
quantitative	B
response—provides	O
an	O
estimate	O
of	O
the	O
test	B
error	O
rate	B
.	O
we	O
illustrate	O
the	O
validation	B
set	I
approach	O
on	O
the	O
auto	O
data	B
set	O
.	O
recall	B
from	O
chapter	O
3	O
that	O
there	O
appears	O
to	O
be	O
a	O
non-linear	B
relationship	O
between	O
mpg	O
and	O
horsepower	O
,	O
and	O
that	O
a	O
model	B
that	O
predicts	O
mpg	O
using	O
horsepower	O
and	O
2	O
gives	O
better	O
results	O
than	O
a	O
model	B
that	O
uses	O
only	O
a	O
linear	B
term	O
.	O
horsepower	O
it	O
is	O
natural	B
to	O
wonder	O
whether	O
a	O
cubic	B
or	O
higher-order	O
ﬁt	B
might	O
provide	O
validation	B
set	I
approach	O
validation	B
set	I
hold-out	O
set	B
1	O
2	O
3	O
7	O
22	O
13	O
5.1	O
cross-validation	B
177	O
n	O
91	O
figure	O
5.1.	O
a	O
schematic	O
display	O
of	O
the	O
validation	B
set	I
approach	O
.	O
a	O
set	B
of	O
n	O
observations	B
are	O
randomly	O
split	O
into	O
a	O
training	B
set	O
(	O
shown	O
in	O
blue	O
,	O
containing	O
observations	B
7	O
,	O
22	O
,	O
and	O
13	O
,	O
among	O
others	O
)	O
and	O
a	O
validation	B
set	I
(	O
shown	O
in	O
beige	O
,	O
and	O
containing	O
observation	O
91	O
,	O
among	O
others	O
)	O
.	O
the	O
statistical	O
learning	O
method	O
is	O
ﬁt	B
on	O
the	O
training	B
set	O
,	O
and	O
its	O
performance	O
is	O
evaluated	O
on	O
the	O
validation	B
set	I
.	O
even	O
better	O
results	O
.	O
we	O
answer	O
this	O
question	O
in	O
chapter	O
3	O
by	O
looking	O
at	O
the	O
p-values	O
associated	O
with	O
a	O
cubic	B
term	O
and	O
higher-order	O
polynomial	B
terms	O
in	O
a	O
linear	B
regression	I
.	O
but	O
we	O
could	O
also	O
answer	O
this	O
question	O
using	O
the	O
validation	O
method	O
.	O
we	O
randomly	O
split	O
the	O
392	O
observations	B
into	O
two	O
sets	O
,	O
a	O
training	B
set	O
containing	O
196	O
of	O
the	O
data	B
points	O
,	O
and	O
a	O
validation	B
set	I
containing	O
the	O
remaining	O
196	O
observations	B
.	O
the	O
validation	B
set	I
error	O
rates	O
that	O
result	O
from	O
ﬁtting	O
various	O
regression	B
models	O
on	O
the	O
training	B
sample	O
and	O
evaluating	O
their	O
performance	O
on	O
the	O
validation	O
sample	O
,	O
using	O
mse	O
as	O
a	O
measure	O
of	O
validation	B
set	I
error	O
,	O
are	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
5.2.	O
the	O
validation	B
set	I
mse	O
for	O
the	O
quadratic	B
ﬁt	O
is	O
considerably	O
smaller	O
than	O
for	O
the	O
linear	B
ﬁt	O
.	O
however	O
,	O
the	O
validation	B
set	I
mse	O
for	O
the	O
cubic	B
ﬁt	O
is	O
actually	O
slightly	O
larger	O
than	O
for	O
the	O
quadratic	B
ﬁt	O
.	O
this	O
implies	O
that	O
including	O
a	O
cubic	B
term	O
in	O
the	O
regression	B
does	O
not	O
lead	O
to	O
better	O
prediction	B
than	O
simply	O
using	O
a	O
quadratic	B
term	O
.	O
recall	B
that	O
in	O
order	O
to	O
create	O
the	O
left-hand	O
panel	O
of	O
figure	O
5.2	O
,	O
we	O
ran-	O
domly	O
divided	O
the	O
data	B
set	O
into	O
two	O
parts	O
,	O
a	O
training	B
set	O
and	O
a	O
validation	B
set	I
.	O
if	O
we	O
repeat	O
the	O
process	O
of	O
randomly	O
splitting	O
the	O
sample	O
set	B
into	O
two	O
parts	O
,	O
we	O
will	O
get	O
a	O
somewhat	O
diﬀerent	O
estimate	O
for	O
the	O
test	B
mse	O
.	O
as	O
an	O
illustration	O
,	O
the	O
right-hand	O
panel	O
of	O
figure	O
5.2	O
displays	O
ten	O
diﬀerent	O
vali-	O
dation	O
set	B
mse	O
curves	O
from	O
the	O
auto	O
data	B
set	O
,	O
produced	O
using	O
ten	O
diﬀerent	O
random	O
splits	O
of	O
the	O
observations	B
into	O
training	B
and	O
validation	O
sets	O
.	O
all	O
ten	O
curves	O
indicate	O
that	O
the	O
model	B
with	O
a	O
quadratic	B
term	O
has	O
a	O
dramatically	O
smaller	O
validation	B
set	I
mse	O
than	O
the	O
model	B
with	O
only	O
a	O
linear	B
term	O
.	O
fur-	O
thermore	O
,	O
all	O
ten	O
curves	O
indicate	O
that	O
there	O
is	O
not	O
much	O
beneﬁt	O
in	O
including	O
cubic	B
or	O
higher-order	O
polynomial	B
terms	O
in	O
the	O
model	B
.	O
but	O
it	O
is	O
worth	O
noting	O
that	O
each	O
of	O
the	O
ten	O
curves	O
results	O
in	O
a	O
diﬀerent	O
test	B
mse	O
estimate	O
for	O
each	O
of	O
the	O
ten	O
regression	B
models	O
considered	O
.	O
and	O
there	O
is	O
no	O
consensus	O
among	O
the	O
curves	O
as	O
to	O
which	O
model	B
results	O
in	O
the	O
smallest	O
validation	B
set	I
mse	O
.	O
based	O
on	O
the	O
variability	O
among	O
these	O
curves	O
,	O
all	O
that	O
we	O
can	O
conclude	O
with	O
any	O
conﬁdence	O
is	O
that	O
the	O
linear	B
ﬁt	O
is	O
not	O
adequate	O
for	O
this	O
data	B
.	O
the	O
validation	B
set	I
approach	O
is	O
conceptually	O
simple	B
and	O
is	O
easy	O
to	O
imple-	O
ment	O
.	O
but	O
it	O
has	O
two	O
potential	O
drawbacks	O
:	O
178	O
5.	O
resampling	B
methods	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
8	O
2	O
6	O
2	O
4	O
2	O
2	O
2	O
0	O
2	O
8	O
1	O
6	O
1	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
8	O
2	O
6	O
2	O
4	O
2	O
2	O
2	O
0	O
2	O
8	O
1	O
6	O
1	O
2	O
4	O
6	O
8	O
10	O
2	O
4	O
6	O
8	O
10	O
degree	O
of	O
polynomial	B
degree	O
of	O
polynomial	B
figure	O
5.2.	O
the	O
validation	B
set	I
approach	O
was	O
used	O
on	O
the	O
auto	O
data	B
set	O
in	O
order	O
to	O
estimate	O
the	O
test	B
error	O
that	O
results	O
from	O
predicting	O
mpg	O
using	O
polynomial	B
functions	O
of	O
horsepower	O
.	O
left	O
:	O
validation	O
error	O
estimates	O
for	O
a	O
single	B
split	O
into	O
training	B
and	O
validation	O
data	O
sets	O
.	O
right	O
:	O
the	O
validation	O
method	O
was	O
repeated	O
ten	O
times	O
,	O
each	O
time	O
using	O
a	O
diﬀerent	O
random	O
split	O
of	O
the	O
observations	B
into	O
a	O
training	B
set	O
and	O
a	O
validation	B
set	I
.	O
this	O
illustrates	O
the	O
variability	O
in	O
the	O
estimated	O
test	B
mse	O
that	O
results	O
from	O
this	O
approach	B
.	O
1.	O
as	O
is	O
shown	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
5.2	O
,	O
the	O
validation	O
esti-	O
mate	O
of	O
the	O
test	B
error	O
rate	B
can	O
be	O
highly	O
variable	B
,	O
depending	O
on	O
pre-	O
cisely	O
which	O
observations	B
are	O
included	O
in	O
the	O
training	B
set	O
and	O
which	O
observations	B
are	O
included	O
in	O
the	O
validation	B
set	I
.	O
2.	O
in	O
the	O
validation	O
approach	O
,	O
only	O
a	O
subset	O
of	O
the	O
observations—those	O
that	O
are	O
included	O
in	O
the	O
training	B
set	O
rather	O
than	O
in	O
the	O
validation	O
set—are	O
used	O
to	O
ﬁt	B
the	O
model	B
.	O
since	O
statistical	O
methods	O
tend	O
to	O
per-	O
form	O
worse	O
when	O
trained	O
on	O
fewer	O
observations	B
,	O
this	O
suggests	O
that	O
the	O
validation	B
set	I
error	O
rate	B
may	O
tend	O
to	O
overestimate	O
the	O
test	B
error	O
rate	B
for	O
the	O
model	B
ﬁt	O
on	O
the	O
entire	O
data	B
set	O
.	O
in	O
the	O
coming	O
subsections	O
,	O
we	O
will	O
present	O
cross-validation	B
,	O
a	O
reﬁnement	O
of	O
the	O
validation	B
set	I
approach	O
that	O
addresses	O
these	O
two	O
issues	O
.	O
5.1.2	O
leave-one-out	B
cross-validation	O
leave-one-out	B
cross-validation	O
(	O
loocv	O
)	O
is	O
closely	O
related	O
to	O
the	O
validation	B
set	I
approach	O
of	O
section	O
5.1.1	O
,	O
but	O
it	O
attempts	O
to	O
address	O
that	O
method	O
’	O
s	O
drawbacks	O
.	O
like	O
the	O
validation	B
set	I
approach	O
,	O
loocv	O
involves	O
splitting	O
the	O
set	B
of	O
observations	B
into	O
two	O
parts	O
.	O
however	O
,	O
instead	O
of	O
creating	O
two	O
subsets	O
of	O
comparable	O
size	O
,	O
a	O
single	B
observation	O
(	O
x1	O
,	O
y1	O
)	O
is	O
used	O
for	O
the	O
validation	B
set	I
,	O
and	O
the	O
remaining	O
observations	B
{	O
(	O
x2	B
,	O
y2	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
}	O
make	O
up	O
the	O
training	B
set	O
.	O
the	O
statistical	O
learning	O
method	O
is	O
ﬁt	B
on	O
the	O
n	O
−	O
1	O
training	B
observations	O
,	O
and	O
a	O
prediction	B
ˆy1	O
is	O
made	O
for	O
the	O
excluded	O
observation	O
,	O
using	O
its	O
value	O
x1	O
.	O
since	O
(	O
x1	O
,	O
y1	O
)	O
was	O
not	O
used	O
in	O
the	O
ﬁtting	O
process	O
,	O
mse1	O
=	O
leave-one-	O
out	O
cross-	O
validation	O
5.1	O
cross-validation	B
179	O
1	O
2	O
3	O
1	O
2	O
3	O
1	O
2	O
3	O
1	O
2	O
3	O
1	O
2	O
3	O
·	O
·	O
·	O
n	O
n	O
n	O
n	O
n	O
figure	O
5.3.	O
a	O
schematic	O
display	O
of	O
loocv	O
.	O
a	O
set	B
of	O
n	O
data	B
points	O
is	O
repeat-	O
edly	O
split	O
into	O
a	O
training	B
set	O
(	O
shown	O
in	O
blue	O
)	O
containing	O
all	O
but	O
one	O
observation	O
,	O
and	O
a	O
validation	B
set	I
that	O
contains	O
only	O
that	O
observation	O
(	O
shown	O
in	O
beige	O
)	O
.	O
the	O
test	B
error	O
is	O
then	O
estimated	O
by	O
averaging	O
the	O
n	O
resulting	O
mse	O
’	O
s	O
.	O
the	O
ﬁrst	O
training	B
set	O
contains	O
all	O
but	O
observation	O
1	O
,	O
the	O
second	O
training	B
set	O
contains	O
all	O
but	O
observation	O
2	O
,	O
and	O
so	O
forth	O
.	O
(	O
y1	O
−	O
ˆy1	O
)	O
2	O
provides	O
an	O
approximately	O
unbiased	O
estimate	O
for	O
the	O
test	B
error	O
.	O
but	O
even	O
though	O
mse1	O
is	O
unbiased	O
for	O
the	O
test	B
error	O
,	O
it	O
is	O
a	O
poor	O
estimate	O
because	O
it	O
is	O
highly	O
variable	B
,	O
since	O
it	O
is	O
based	O
upon	O
a	O
single	B
observation	O
(	O
x1	O
,	O
y1	O
)	O
.	O
we	O
can	O
repeat	O
the	O
procedure	O
by	O
selecting	O
(	O
x2	B
,	O
y2	O
)	O
for	O
the	O
validation	O
data	O
,	O
training	B
the	O
statistical	O
learning	O
procedure	O
on	O
the	O
n	O
−	O
1	O
observations	B
{	O
(	O
x1	O
,	O
y1	O
)	O
,	O
(	O
x3	O
,	O
y3	O
)	O
,	O
.	O
.	O
.	O
,	O
(	O
xn	O
,	O
yn	O
)	O
}	O
,	O
and	O
computing	O
mse2	O
=	O
(	O
y2−ˆy2	O
)	O
2.	O
repeat-	O
ing	O
this	O
approach	B
n	O
times	O
produces	O
n	O
squared	O
errors	O
,	O
mse1	O
,	O
.	O
.	O
.	O
,	O
msen	O
.	O
the	O
loocv	O
estimate	O
for	O
the	O
test	B
mse	O
is	O
the	O
average	B
of	O
these	O
n	O
test	B
error	O
estimates	O
:	O
cv	O
(	O
n	O
)	O
=	O
n	O
(	O
cid:17	O
)	O
i=1	O
1	O
n	O
msei	O
.	O
(	O
5.1	O
)	O
a	O
schematic	O
of	O
the	O
loocv	O
approach	B
is	O
illustrated	O
in	O
figure	O
5.3.	O
loocv	O
has	O
a	O
couple	O
of	O
major	O
advantages	O
over	O
the	O
validation	B
set	I
ap-	O
proach	O
.	O
first	O
,	O
it	O
has	O
far	O
less	O
bias	B
.	O
in	O
loocv	O
,	O
we	O
repeatedly	O
ﬁt	B
the	O
sta-	O
tistical	O
learning	O
method	O
using	O
training	B
sets	O
that	O
contain	O
n	O
−	O
1	O
observa-	O
tions	O
,	O
almost	O
as	O
many	O
as	O
are	O
in	O
the	O
entire	O
data	B
set	O
.	O
this	O
is	O
in	O
contrast	B
to	O
the	O
validation	B
set	I
approach	O
,	O
in	O
which	O
the	O
training	B
set	O
is	O
typically	O
around	O
half	O
the	O
size	O
of	O
the	O
original	O
data	B
set	O
.	O
consequently	O
,	O
the	O
loocv	O
approach	B
tends	O
not	O
to	O
overestimate	O
the	O
test	B
error	O
rate	B
as	O
much	O
as	O
the	O
validation	B
set	I
approach	O
does	O
.	O
second	O
,	O
in	O
contrast	B
to	O
the	O
validation	O
approach	O
which	O
will	O
yield	O
diﬀerent	O
results	O
when	O
applied	O
repeatedly	O
due	O
to	O
randomness	O
in	O
the	O
training/validation	O
set	B
splits	O
,	O
performing	O
loocv	O
multiple	B
times	O
will	O
180	O
5.	O
resampling	B
methods	O
loocv	O
10−fold	O
cv	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
8	O
2	O
6	O
2	O
4	O
2	O
2	O
2	O
0	O
2	O
8	O
1	O
6	O
1	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
8	O
2	O
6	O
2	O
4	O
2	O
2	O
2	O
0	O
2	O
8	O
1	O
6	O
1	O
2	O
4	O
6	O
8	O
10	O
2	O
4	O
6	O
8	O
10	O
degree	O
of	O
polynomial	B
degree	O
of	O
polynomial	B
figure	O
5.4.	O
cross-validation	B
was	O
used	O
on	O
the	O
auto	O
data	B
set	O
in	O
order	O
to	O
es-	O
timate	O
the	O
test	B
error	O
that	O
results	O
from	O
predicting	O
mpg	O
using	O
polynomial	B
functions	O
of	O
horsepower	O
.	O
left	O
:	O
the	O
loocv	O
error	B
curve	O
.	O
right	O
:	O
10-fold	O
cv	O
was	O
run	O
nine	O
separate	O
times	O
,	O
each	O
with	O
a	O
diﬀerent	O
random	O
split	O
of	O
the	O
data	B
into	O
ten	O
parts	O
.	O
the	O
ﬁgure	O
shows	O
the	O
nine	O
slightly	O
diﬀerent	O
cv	O
error	B
curves	O
.	O
always	O
yield	O
the	O
same	O
results	O
:	O
there	O
is	O
no	O
randomness	O
in	O
the	O
training/vali-	O
dation	O
set	B
splits	O
.	O
we	O
used	O
loocv	O
on	O
the	O
auto	O
data	B
set	O
in	O
order	O
to	O
obtain	O
an	O
estimate	O
of	O
the	O
test	B
set	O
mse	O
that	O
results	O
from	O
ﬁtting	O
a	O
linear	B
regression	I
model	O
to	O
predict	O
mpg	O
using	O
polynomial	B
functions	O
of	O
horsepower	O
.	O
the	O
results	O
are	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
5.4.	O
loocv	O
has	O
the	O
potential	O
to	O
be	O
expensive	O
to	O
implement	O
,	O
since	O
the	O
model	B
has	O
to	O
be	O
ﬁt	B
n	O
times	O
.	O
this	O
can	O
be	O
very	O
time	O
consuming	O
if	O
n	O
is	O
large	O
,	O
and	O
if	O
each	O
individual	O
model	B
is	O
slow	O
to	O
ﬁt	B
.	O
with	O
least	B
squares	I
linear	O
or	O
polynomial	B
regression	O
,	O
an	O
amazing	O
shortcut	O
makes	O
the	O
cost	O
of	O
loocv	O
the	O
same	O
as	O
that	O
of	O
a	O
single	B
model	O
ﬁt	B
!	O
the	O
following	O
formula	O
holds	O
:	O
(	O
cid:11	O
)	O
n	O
(	O
cid:17	O
)	O
i=1	O
(	O
cid:12	O
)	O
2	O
,	O
yi	O
−	O
ˆyi	O
1	O
−	O
hi	O
1	O
n	O
cv	O
(	O
n	O
)	O
=	O
(	O
5.2	O
)	O
where	O
ˆyi	O
is	O
the	O
ith	O
ﬁtted	B
value	I
from	O
the	O
original	O
least	B
squares	I
ﬁt	O
,	O
and	O
hi	O
is	O
the	O
leverage	B
deﬁned	O
in	O
(	O
3.37	O
)	O
on	O
page	O
98.	O
this	O
is	O
like	O
the	O
ordinary	O
mse	O
,	O
except	O
the	O
ith	O
residual	B
is	O
divided	O
by	O
1	O
−	O
hi	O
.	O
the	O
leverage	B
lies	O
between	O
1/n	O
and	O
1	O
,	O
and	O
reﬂects	O
the	O
amount	O
that	O
an	O
observation	O
inﬂuences	O
its	O
own	O
ﬁt	B
.	O
hence	O
the	O
residuals	B
for	O
high-leverage	O
points	O
are	O
inﬂated	O
in	O
this	O
formula	O
by	O
exactly	O
the	O
right	O
amount	O
for	O
this	O
equality	O
to	O
hold	O
.	O
loocv	O
is	O
a	O
very	O
general	O
method	O
,	O
and	O
can	O
be	O
used	O
with	O
any	O
kind	O
of	O
predictive	O
modeling	O
.	O
for	O
example	O
we	O
could	O
use	O
it	O
with	O
logistic	B
regression	I
or	O
linear	B
discriminant	I
analysis	I
,	O
or	O
any	O
of	O
the	O
methods	O
discussed	O
in	O
later	O
1	O
2	O
3	O
11	O
76	O
5	O
11	O
76	O
5	O
11	O
76	O
5	O
11	O
76	O
5	O
11	O
76	O
5	O
5.1	O
cross-validation	B
181	O
n	O
47	O
47	O
47	O
47	O
47	O
figure	O
5.5.	O
a	O
schematic	O
display	O
of	O
5-fold	O
cv	O
.	O
a	O
set	B
of	O
n	O
observations	B
is	O
randomly	O
split	O
into	O
ﬁve	O
non-overlapping	O
groups	O
.	O
each	O
of	O
these	O
ﬁfths	O
acts	O
as	O
a	O
validation	B
set	I
(	O
shown	O
in	O
beige	O
)	O
,	O
and	O
the	O
remainder	O
as	O
a	O
training	B
set	O
(	O
shown	O
in	O
blue	O
)	O
.	O
the	O
test	B
error	O
is	O
estimated	O
by	O
averaging	O
the	O
ﬁve	O
resulting	O
mse	O
estimates	O
.	O
chapters	O
.	O
the	O
magic	O
formula	O
(	O
5.2	O
)	O
does	O
not	O
hold	O
in	O
general	O
,	O
in	O
which	O
case	O
the	O
model	B
has	O
to	O
be	O
reﬁt	O
n	O
times	O
.	O
5.1.3	O
k-fold	B
cross-validation	O
an	O
alternative	O
to	O
loocv	O
is	O
k-fold	B
cv	O
.	O
this	O
approach	B
involves	O
randomly	O
dividing	O
the	O
set	B
of	O
observations	B
into	O
k	O
groups	O
,	O
or	O
folds	O
,	O
of	O
approximately	O
equal	O
size	O
.	O
the	O
ﬁrst	O
fold	O
is	O
treated	O
as	O
a	O
validation	B
set	I
,	O
and	O
the	O
method	O
is	O
ﬁt	B
on	O
the	O
remaining	O
k	O
−	O
1	O
folds	O
.	O
the	O
mean	B
squared	I
error	I
,	O
mse1	O
,	O
is	O
k-fold	B
cv	O
then	O
computed	O
on	O
the	O
observations	B
in	O
the	O
held-out	O
fold	O
.	O
this	O
procedure	O
is	O
repeated	O
k	O
times	O
;	O
each	O
time	O
,	O
a	O
diﬀerent	O
group	O
of	O
observations	B
is	O
treated	O
as	O
a	O
validation	B
set	I
.	O
this	O
process	O
results	O
in	O
k	O
estimates	O
of	O
the	O
test	B
error	O
,	O
mse1	O
,	O
mse2	O
,	O
.	O
.	O
.	O
,	O
msek	O
.	O
the	O
k-fold	B
cv	O
estimate	O
is	O
computed	O
by	O
averaging	O
these	O
values	O
,	O
k	O
(	O
cid:17	O
)	O
cv	O
(	O
k	O
)	O
=	O
1	O
k	O
msei	O
.	O
i=1	O
(	O
5.3	O
)	O
figure	O
5.5	O
illustrates	O
the	O
k-fold	B
cv	O
approach	B
.	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
loocv	O
is	O
a	O
special	O
case	O
of	O
k-fold	B
cv	O
in	O
which	O
k	O
is	O
set	B
to	O
equal	O
n.	O
in	O
practice	O
,	O
one	O
typically	O
performs	O
k-fold	B
cv	O
using	O
k	O
=	O
5	O
or	O
k	O
=	O
10.	O
what	O
is	O
the	O
advantage	O
of	O
using	O
k	O
=	O
5	O
or	O
k	O
=	O
10	O
rather	O
than	O
k	O
=	O
n	O
?	O
the	O
most	O
obvious	O
advantage	O
is	O
computational	O
.	O
loocv	O
requires	O
ﬁtting	O
the	O
statistical	O
learning	O
method	O
n	O
times	O
.	O
this	O
has	O
the	O
potential	O
to	O
be	O
computationally	O
expensive	O
(	O
except	O
for	O
linear	O
models	O
ﬁt	B
by	O
least	B
squares	I
,	O
in	O
which	O
case	O
formula	O
(	O
5.2	O
)	O
can	O
be	O
used	O
)	O
.	O
but	O
cross-validation	B
is	O
a	O
very	O
general	O
approach	B
that	O
can	O
be	O
applied	O
to	O
almost	O
any	O
statistical	O
learning	O
method	O
.	O
some	O
statistical	O
learning	O
methods	O
have	O
computationally	O
intensive	O
ﬁtting	O
procedures	O
,	O
and	O
so	O
performing	O
loocv	O
may	O
pose	O
computational	O
problems	O
,	O
especially	O
if	O
n	O
is	O
extremely	O
large	O
.	O
in	O
contrast	B
,	O
performing	O
10-fold	O
182	O
5.	O
resampling	B
methods	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0	O
.	O
3	O
5	O
.	O
2	O
0	O
.	O
2	O
5	O
.	O
1	O
0	O
.	O
1	O
5	O
.	O
0	O
0	O
.	O
0	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0	O
.	O
3	O
5	O
.	O
2	O
0	O
.	O
2	O
5	O
.	O
1	O
0	O
.	O
1	O
5	O
.	O
0	O
0	O
.	O
0	O
0	O
2	O
5	O
1	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0	O
1	O
5	O
0	O
2	O
5	O
10	O
20	O
2	O
5	O
10	O
20	O
2	O
5	O
10	O
20	O
flexibility	O
flexibility	O
flexibility	O
figure	O
5.6.	O
true	O
and	O
estimated	O
test	B
mse	O
for	O
the	O
simulated	O
data	B
sets	O
in	O
fig-	O
ures	O
2.9	O
(	O
left	O
)	O
,	O
2.10	O
(	O
center	O
)	O
,	O
and	O
2.11	O
(	O
right	O
)	O
.	O
the	O
true	O
test	O
mse	O
is	O
shown	O
in	O
blue	O
,	O
the	O
loocv	O
estimate	O
is	O
shown	O
as	O
a	O
black	O
dashed	O
line	B
,	O
and	O
the	O
10-fold	O
cv	O
estimate	O
is	O
shown	O
in	O
orange	O
.	O
the	O
crosses	O
indicate	O
the	O
minimum	O
of	O
each	O
of	O
the	O
mse	O
curves	O
.	O
cv	O
requires	O
ﬁtting	O
the	O
learning	O
procedure	O
only	O
ten	O
times	O
,	O
which	O
may	O
be	O
much	O
more	O
feasible	O
.	O
as	O
we	O
see	O
in	O
section	O
5.1.4	O
,	O
there	O
also	O
can	O
be	O
other	O
non-computational	O
advantages	O
to	O
performing	O
5-fold	O
or	O
10-fold	O
cv	O
,	O
which	O
involve	O
the	O
bias-variance	B
trade-oﬀ	O
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
5.4	O
displays	O
nine	O
diﬀerent	O
10-fold	O
cv	O
estimates	O
for	O
the	O
auto	O
data	B
set	O
,	O
each	O
resulting	O
from	O
a	O
diﬀerent	O
random	O
split	O
of	O
the	O
observations	B
into	O
ten	O
folds	O
.	O
as	O
we	O
can	O
see	O
from	O
the	O
ﬁgure	O
,	O
there	O
is	O
some	O
variability	O
in	O
the	O
cv	O
estimates	O
as	O
a	O
result	O
of	O
the	O
variability	O
in	O
how	O
the	O
observations	B
are	O
divided	O
into	O
ten	O
folds	O
.	O
but	O
this	O
variability	O
is	O
typically	O
much	O
lower	O
than	O
the	O
variability	O
in	O
the	O
test	B
error	O
estimates	O
that	O
results	O
from	O
the	O
validation	B
set	I
approach	O
(	O
right-hand	O
panel	O
of	O
figure	O
5.2	O
)	O
.	O
when	O
we	O
examine	O
real	O
data	B
,	O
we	O
do	O
not	O
know	O
the	O
true	O
test	O
mse	O
,	O
and	O
so	O
it	O
is	O
diﬃcult	O
to	O
determine	O
the	O
accuracy	O
of	O
the	O
cross-validation	B
estimate	O
.	O
however	O
,	O
if	O
we	O
examine	O
simulated	O
data	B
,	O
then	O
we	O
can	O
compute	O
the	O
true	O
test	O
mse	O
,	O
and	O
can	O
thereby	O
evaluate	O
the	O
accuracy	O
of	O
our	O
cross-validation	B
results	O
.	O
in	O
figure	O
5.6	O
,	O
we	O
plot	B
the	O
cross-validation	B
estimates	O
and	O
true	O
test	O
error	B
rates	O
that	O
result	O
from	O
applying	O
smoothing	B
splines	O
to	O
the	O
simulated	O
data	B
sets	O
illustrated	O
in	O
figures	O
2.9–2.11	O
of	O
chapter	O
2.	O
the	O
true	O
test	O
mse	O
is	O
displayed	O
in	O
blue	O
.	O
the	O
black	O
dashed	O
and	O
orange	O
solid	O
lines	O
respectively	O
show	O
the	O
estimated	O
loocv	O
and	O
10-fold	O
cv	O
estimates	O
.	O
in	O
all	O
three	O
plots	O
,	O
the	O
two	O
cross-validation	B
estimates	O
are	O
very	O
similar	O
.	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
5.6	O
,	O
the	O
true	O
test	O
mse	O
and	O
the	O
cross-validation	B
curves	O
are	O
almost	O
identical	O
.	O
in	O
the	O
center	O
panel	O
of	O
figure	O
5.6	O
,	O
the	O
two	O
sets	O
of	O
curves	O
are	O
similar	O
at	O
the	O
lower	O
degrees	O
of	O
ﬂexibility	O
,	O
while	O
the	O
cv	O
curves	O
overestimate	O
the	O
test	B
set	O
mse	O
for	O
higher	O
degrees	O
of	O
ﬂexibility	O
.	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
5.6	O
,	O
the	O
cv	O
curves	O
have	O
the	O
correct	O
general	O
shape	O
,	O
but	O
they	O
underestimate	O
the	O
true	O
test	O
mse	O
.	O
5.1	O
cross-validation	B
183	O
when	O
we	O
perform	O
cross-validation	B
,	O
our	O
goal	O
might	O
be	O
to	O
determine	O
how	O
well	O
a	O
given	O
statistical	O
learning	O
procedure	O
can	O
be	O
expected	O
to	O
perform	O
on	O
independent	B
data	O
;	O
in	O
this	O
case	O
,	O
the	O
actual	O
estimate	O
of	O
the	O
test	B
mse	O
is	O
of	O
interest	O
.	O
but	O
at	O
other	O
times	O
we	O
are	O
interested	O
only	O
in	O
the	O
location	O
of	O
the	O
minimum	O
point	O
in	O
the	O
estimated	O
test	B
mse	O
curve	O
.	O
this	O
is	O
because	O
we	O
might	O
be	O
performing	O
cross-validation	B
on	O
a	O
number	O
of	O
statistical	O
learning	O
methods	O
,	O
or	O
on	O
a	O
single	B
method	O
using	O
diﬀerent	O
levels	O
of	O
ﬂexibility	O
,	O
in	O
order	O
to	O
identify	O
the	O
method	O
that	O
results	O
in	O
the	O
lowest	O
test	B
error	O
.	O
for	O
this	O
purpose	O
,	O
the	O
location	O
of	O
the	O
minimum	O
point	O
in	O
the	O
estimated	O
test	B
mse	O
curve	O
is	O
important	O
,	O
but	O
the	O
actual	O
value	O
of	O
the	O
estimated	O
test	B
mse	O
is	O
not	O
.	O
we	O
ﬁnd	O
in	O
figure	O
5.6	O
that	O
despite	O
the	O
fact	O
that	O
they	O
sometimes	O
underestimate	O
the	O
true	O
test	O
mse	O
,	O
all	O
of	O
the	O
cv	O
curves	O
come	O
close	O
to	O
identifying	O
the	O
correct	O
level	B
of	O
ﬂexibility—that	O
is	O
,	O
the	O
ﬂexibility	O
level	B
corresponding	O
to	O
the	O
smallest	O
test	B
mse	O
.	O
5.1.4	O
bias-variance	B
trade-oﬀ	O
for	O
k-fold	O
cross-validation	B
we	O
mentioned	O
in	O
section	O
5.1.3	O
that	O
k-fold	B
cv	O
with	O
k	O
<	O
n	O
has	O
a	O
compu-	O
tational	O
advantage	O
to	O
loocv	O
.	O
but	O
putting	O
computational	O
issues	O
aside	O
,	O
a	O
less	O
obvious	O
but	O
potentially	O
more	O
important	O
advantage	O
of	O
k-fold	B
cv	O
is	O
that	O
it	O
often	O
gives	O
more	O
accurate	O
estimates	O
of	O
the	O
test	B
error	O
rate	B
than	O
does	O
loocv	O
.	O
this	O
has	O
to	O
do	O
with	O
a	O
bias-variance	B
trade-oﬀ	O
.	O
it	O
was	O
mentioned	O
in	O
section	O
5.1.1	O
that	O
the	O
validation	B
set	I
approach	O
can	O
lead	O
to	O
overestimates	O
of	O
the	O
test	B
error	O
rate	B
,	O
since	O
in	O
this	O
approach	B
the	O
training	B
set	O
used	O
to	O
ﬁt	B
the	O
statistical	O
learning	O
method	O
contains	O
only	O
half	O
the	O
observations	B
of	O
the	O
entire	O
data	B
set	O
.	O
using	O
this	O
logic	O
,	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
loocv	O
will	O
give	O
approximately	O
unbiased	O
estimates	O
of	O
the	O
test	B
error	O
,	O
since	O
each	O
training	B
set	O
contains	O
n	O
−	O
1	O
observations	B
,	O
which	O
is	O
almost	O
as	O
many	O
as	O
the	O
number	O
of	O
observations	B
in	O
the	O
full	O
data	B
set	O
.	O
and	O
performing	O
k-fold	B
cv	O
for	O
,	O
say	O
,	O
k	O
=	O
5	O
or	O
k	O
=	O
10	O
will	O
lead	O
to	O
an	O
intermediate	O
level	B
of	O
bias	B
,	O
since	O
each	O
training	B
set	O
contains	O
(	O
k	O
−	O
1	O
)	O
n/k	O
observations—fewer	O
than	O
in	O
the	O
loocv	O
approach	B
,	O
but	O
substantially	O
more	O
than	O
in	O
the	O
validation	B
set	I
approach	O
.	O
therefore	O
,	O
from	O
the	O
perspective	O
of	O
bias	B
reduction	O
,	O
it	O
is	O
clear	O
that	O
loocv	O
is	O
to	O
be	O
preferred	O
to	O
k-fold	B
cv	O
.	O
however	O
,	O
we	O
know	O
that	O
bias	B
is	O
not	O
the	O
only	O
source	O
for	O
concern	O
in	O
an	O
esti-	O
mating	O
procedure	O
;	O
we	O
must	O
also	O
consider	O
the	O
procedure	O
’	O
s	O
variance	B
.	O
it	O
turns	O
out	O
that	O
loocv	O
has	O
higher	O
variance	B
than	O
does	O
k-fold	B
cv	O
with	O
k	O
<	O
n.	O
why	O
is	O
this	O
the	O
case	O
?	O
when	O
we	O
perform	O
loocv	O
,	O
we	O
are	O
in	O
eﬀect	O
averaging	O
the	O
outputs	O
of	O
n	O
ﬁtted	O
models	O
,	O
each	O
of	O
which	O
is	O
trained	O
on	O
an	O
almost	O
identical	O
set	B
of	O
observations	B
;	O
therefore	O
,	O
these	O
outputs	O
are	O
highly	O
(	O
positively	O
)	O
corre-	O
lated	O
with	O
each	O
other	O
.	O
in	O
contrast	B
,	O
when	O
we	O
perform	O
k-fold	B
cv	O
with	O
k	O
<	O
n	O
,	O
we	O
are	O
averaging	O
the	O
outputs	O
of	O
k	O
ﬁtted	O
models	O
that	O
are	O
somewhat	O
less	O
correlated	O
with	O
each	O
other	O
,	O
since	O
the	O
overlap	O
between	O
the	O
training	B
sets	O
in	O
each	O
model	B
is	O
smaller	O
.	O
since	O
the	O
mean	O
of	O
many	O
highly	O
correlated	O
quantities	O
184	O
5.	O
resampling	B
methods	O
has	O
higher	O
variance	B
than	O
does	O
the	O
mean	O
of	O
many	O
quantities	O
that	O
are	O
not	O
as	O
highly	O
correlated	O
,	O
the	O
test	B
error	O
estimate	O
resulting	O
from	O
loocv	O
tends	O
to	O
have	O
higher	O
variance	B
than	O
does	O
the	O
test	B
error	O
estimate	O
resulting	O
from	O
k-fold	B
cv	O
.	O
to	O
summarize	O
,	O
there	O
is	O
a	O
bias-variance	B
trade-oﬀ	O
associated	O
with	O
the	O
choice	O
of	O
k	O
in	O
k-fold	B
cross-validation	O
.	O
typically	O
,	O
given	O
these	O
considerations	O
,	O
one	O
performs	O
k-fold	B
cross-validation	O
using	O
k	O
=	O
5	O
or	O
k	O
=	O
10	O
,	O
as	O
these	O
values	O
have	O
been	O
shown	O
empirically	O
to	O
yield	O
test	B
error	O
rate	B
estimates	O
that	O
suﬀer	O
neither	O
from	O
excessively	O
high	O
bias	B
nor	O
from	O
very	O
high	O
variance	B
.	O
5.1.5	O
cross-validation	B
on	O
classiﬁcation	B
problems	O
in	O
this	O
chapter	O
so	O
far	O
,	O
we	O
have	O
illustrated	O
the	O
use	O
of	O
cross-validation	B
in	O
the	O
regression	B
setting	O
where	O
the	O
outcome	O
y	O
is	O
quantitative	B
,	O
and	O
so	O
have	O
used	O
mse	O
to	O
quantify	O
test	B
error	O
.	O
but	O
cross-validation	B
can	O
also	O
be	O
a	O
very	O
useful	O
approach	B
in	O
the	O
classiﬁcation	B
setting	O
when	O
y	O
is	O
qualitative	B
.	O
in	O
this	O
setting	O
,	O
cross-validation	B
works	O
just	O
as	O
described	O
earlier	O
in	O
this	O
chapter	O
,	O
except	O
that	O
rather	O
than	O
using	O
mse	O
to	O
quantify	O
test	B
error	O
,	O
we	O
instead	O
use	O
the	O
number	O
of	O
misclassiﬁed	O
observations	B
.	O
for	O
instance	O
,	O
in	O
the	O
classiﬁcation	B
setting	O
,	O
the	O
loocv	O
error	B
rate	I
takes	O
the	O
form	O
n	O
(	O
cid:17	O
)	O
cv	O
(	O
n	O
)	O
=	O
1	O
n	O
erri	O
,	O
i=1	O
(	O
5.4	O
)	O
where	O
erri	O
=	O
i	O
(	O
yi	O
(	O
cid:4	O
)	O
=	O
ˆyi	O
)	O
.	O
the	O
k-fold	B
cv	O
error	B
rate	I
and	O
validation	B
set	I
error	O
rates	O
are	O
deﬁned	O
analogously	O
.	O
as	O
an	O
example	O
,	O
we	O
ﬁt	B
various	O
logistic	B
regression	I
models	O
on	O
the	O
two-	O
dimensional	O
classiﬁcation	B
data	O
displayed	O
in	O
figure	O
2.13.	O
in	O
the	O
top-left	O
panel	O
of	O
figure	O
5.7	O
,	O
the	O
black	O
solid	O
line	B
shows	O
the	O
estimated	O
decision	O
bound-	O
ary	O
resulting	O
from	O
ﬁtting	O
a	O
standard	O
logistic	O
regression	B
model	O
to	O
this	O
data	B
set	O
.	O
since	O
this	O
is	O
simulated	O
data	B
,	O
we	O
can	O
compute	O
the	O
true	O
test	O
error	B
rate	I
,	O
which	O
takes	O
a	O
value	O
of	O
0.201	O
and	O
so	O
is	O
substantially	O
larger	O
than	O
the	O
bayes	O
error	B
rate	I
of	O
0.133.	O
clearly	O
logistic	B
regression	I
does	O
not	O
have	O
enough	O
ﬂexi-	O
bility	O
to	O
model	B
the	O
bayes	O
decision	B
boundary	I
in	O
this	O
setting	O
.	O
we	O
can	O
easily	O
extend	O
logistic	B
regression	I
to	O
obtain	O
a	O
non-linear	B
decision	O
boundary	O
by	O
using	O
polynomial	B
functions	O
of	O
the	O
predictors	O
,	O
as	O
we	O
did	O
in	O
the	O
regression	B
setting	O
in	O
section	O
3.3.2.	O
for	O
example	O
,	O
we	O
can	O
ﬁt	B
a	O
quadratic	B
logistic	O
regression	B
model	O
,	O
given	O
by	O
(	O
cid:11	O
)	O
(	O
cid:12	O
)	O
log	O
p	O
1	O
−	O
p	O
=	O
β0	O
+	O
β1x1	O
+	O
β2x	O
2	O
1	O
+	O
β3x2	O
+	O
β4x	O
2	O
2	O
.	O
(	O
5.5	O
)	O
the	O
top-right	O
panel	O
of	O
figure	O
5.7	O
displays	O
the	O
resulting	O
decision	B
boundary	I
,	O
which	O
is	O
now	O
curved	O
.	O
however	O
,	O
the	O
test	B
error	O
rate	B
has	O
improved	O
only	O
slightly	O
,	O
to	O
0.197.	O
a	O
much	O
larger	O
improvement	O
is	O
apparent	O
in	O
the	O
bottom-left	O
panel	O
degree=1	O
degree=2	O
5.1	O
cross-validation	B
185	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
degree=3	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
degree=4	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
figure	O
5.7.	O
logistic	B
regression	I
ﬁts	O
on	O
the	O
two-dimensional	O
classiﬁcation	B
data	O
displayed	O
in	O
figure	O
2.13.	O
the	O
bayes	O
decision	B
boundary	I
is	O
represented	O
using	O
a	O
purple	O
dashed	O
line	B
.	O
estimated	O
decision	O
boundaries	O
from	O
linear	B
,	O
quadratic	B
,	O
cubic	B
and	O
quartic	O
(	O
degrees	O
1–4	O
)	O
logistic	B
regressions	O
are	O
displayed	O
in	O
black	O
.	O
the	O
test	B
error	O
rates	O
for	O
the	O
four	O
logistic	B
regression	I
ﬁts	O
are	O
respectively	O
0.201	O
,	O
0.197	O
,	O
0.160	O
,	O
and	O
0.162	O
,	O
while	O
the	O
bayes	O
error	B
rate	I
is	O
0.133.	O
of	O
figure	O
5.7	O
,	O
in	O
which	O
we	O
have	O
ﬁt	B
a	O
logistic	B
regression	I
model	O
involving	O
cubic	B
polynomials	O
of	O
the	O
predictors	O
.	O
now	O
the	O
test	B
error	O
rate	B
has	O
decreased	O
to	O
0.160.	O
going	O
to	O
a	O
quartic	O
polynomial	B
(	O
bottom-right	O
)	O
slightly	O
increases	O
the	O
test	B
error	O
.	O
in	O
practice	O
,	O
for	O
real	O
data	B
,	O
the	O
bayes	O
decision	B
boundary	I
and	O
the	O
test	B
er-	O
ror	O
rates	O
are	O
unknown	O
.	O
so	O
how	O
might	O
we	O
decide	O
between	O
the	O
four	O
logistic	B
regression	I
models	O
displayed	O
in	O
figure	O
5.7	O
?	O
we	O
can	O
use	O
cross-validation	B
in	O
order	O
to	O
make	O
this	O
decision	O
.	O
the	O
left-hand	O
panel	O
of	O
figure	O
5.8	O
displays	O
in	O
186	O
5.	O
resampling	B
methods	O
e	O
t	O
a	O
r	O
r	O
o	O
r	O
r	O
e	O
0	O
2	O
.	O
0	O
8	O
1	O
.	O
0	O
6	O
1	O
0	O
.	O
4	O
1	O
.	O
0	O
2	O
1	O
0	O
.	O
e	O
t	O
a	O
r	O
r	O
o	O
r	O
r	O
e	O
0	O
2	O
.	O
0	O
8	O
1	O
.	O
0	O
6	O
1	O
0	O
.	O
4	O
1	O
.	O
0	O
2	O
1	O
0	O
.	O
2	O
4	O
6	O
8	O
10	O
0.01	O
0.02	O
0.05	O
0.10	O
0.20	O
0.50	O
1.00	O
order	O
of	O
polynomials	O
used	O
1/k	O
figure	O
5.8.	O
test	B
error	O
(	O
brown	O
)	O
,	O
training	B
error	O
(	O
blue	O
)	O
,	O
and	O
10-fold	O
cv	O
error	B
(	O
black	O
)	O
on	O
the	O
two-dimensional	O
classiﬁcation	B
data	O
displayed	O
in	O
figure	O
5.7.	O
left	O
:	O
logistic	B
regression	I
using	O
polynomial	B
functions	O
of	O
the	O
predictors	O
.	O
the	O
order	O
of	O
the	O
polynomials	O
used	O
is	O
displayed	O
on	O
the	O
x-axis	O
.	O
right	O
:	O
the	O
knn	O
classiﬁer	B
with	O
diﬀerent	O
values	O
of	O
k	O
,	O
the	O
number	O
of	O
neighbors	O
used	O
in	O
the	O
knn	O
classiﬁer	B
.	O
black	O
the	O
10-fold	O
cv	O
error	B
rates	O
that	O
result	O
from	O
ﬁtting	O
ten	O
logistic	B
regres-	O
sion	O
models	O
to	O
the	O
data	B
,	O
using	O
polynomial	B
functions	O
of	O
the	O
predictors	O
up	O
to	O
tenth	O
order	O
.	O
the	O
true	O
test	O
errors	O
are	O
shown	O
in	O
brown	O
,	O
and	O
the	O
training	B
errors	O
are	O
shown	O
in	O
blue	O
.	O
as	O
we	O
have	O
seen	O
previously	O
,	O
the	O
training	B
error	O
tends	O
to	O
decrease	O
as	O
the	O
ﬂexibility	O
of	O
the	O
ﬁt	B
increases	O
.	O
(	O
the	O
ﬁgure	O
indicates	O
that	O
though	O
the	O
training	B
error	O
rate	B
doesn	O
’	O
t	O
quite	O
decrease	O
monotonically	O
,	O
it	O
tends	O
to	O
decrease	O
on	O
the	O
whole	O
as	O
the	O
model	B
complexity	O
increases	O
.	O
)	O
in	O
contrast	B
,	O
the	O
test	B
error	O
displays	O
a	O
characteristic	O
u-shape	O
.	O
the	O
10-fold	O
cv	O
error	B
rate	I
provides	O
a	O
pretty	O
good	O
approximation	O
to	O
the	O
test	B
error	O
rate	B
.	O
while	O
it	O
somewhat	O
underestimates	O
the	O
error	B
rate	I
,	O
it	O
reaches	O
a	O
minimum	O
when	O
fourth-order	O
polynomials	O
are	O
used	O
,	O
which	O
is	O
very	O
close	O
to	O
the	O
min-	O
imum	O
of	O
the	O
test	B
curve	O
,	O
which	O
occurs	O
when	O
third-order	O
polynomials	O
are	O
used	O
.	O
in	O
fact	O
,	O
using	O
fourth-order	O
polynomials	O
would	O
likely	O
lead	O
to	O
good	O
test	B
set	O
performance	O
,	O
as	O
the	O
true	O
test	O
error	B
rate	I
is	O
approximately	O
the	O
same	O
for	O
third	O
,	O
fourth	O
,	O
ﬁfth	O
,	O
and	O
sixth-order	O
polynomials	O
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
5.8	O
displays	O
the	O
same	O
three	O
curves	O
us-	O
ing	O
the	O
knn	O
approach	B
for	O
classiﬁcation	B
,	O
as	O
a	O
function	B
of	O
the	O
value	O
of	O
k	O
(	O
which	O
in	O
this	O
context	O
indicates	O
the	O
number	O
of	O
neighbors	O
used	O
in	O
the	O
knn	O
classiﬁer	B
,	O
rather	O
than	O
the	O
number	O
of	O
cv	O
folds	O
used	O
)	O
.	O
again	O
the	O
training	B
error	O
rate	B
declines	O
as	O
the	O
method	O
becomes	O
more	O
ﬂexible	B
,	O
and	O
so	O
we	O
see	O
that	O
the	O
training	B
error	O
rate	B
can	O
not	O
be	O
used	O
to	O
select	O
the	O
optimal	O
value	O
for	O
k.	O
though	O
the	O
cross-validation	B
error	O
curve	O
slightly	O
underestimates	O
the	O
test	B
error	O
rate	B
,	O
it	O
takes	O
on	O
a	O
minimum	O
very	O
close	O
to	O
the	O
best	O
value	O
for	O
k.	O
5.2	O
the	O
bootstrap	B
187	O
5.2	O
the	O
bootstrap	B
bootstrap	O
the	O
bootstrap	B
is	O
a	O
widely	O
applicable	O
and	O
extremely	O
powerful	O
statistical	O
tool	O
that	O
can	O
be	O
used	O
to	O
quantify	O
the	O
uncertainty	O
associated	O
with	O
a	O
given	O
esti-	O
mator	O
or	O
statistical	O
learning	O
method	O
.	O
as	O
a	O
simple	B
example	O
,	O
the	O
bootstrap	B
can	O
be	O
used	O
to	O
estimate	O
the	O
standard	O
errors	O
of	O
the	O
coeﬃcients	O
from	O
a	O
linear	B
regression	I
ﬁt	O
.	O
in	O
the	O
speciﬁc	O
case	O
of	O
linear	B
regression	I
,	O
this	O
is	O
not	O
particularly	O
useful	O
,	O
since	O
we	O
saw	O
in	O
chapter	O
3	O
that	O
standard	O
statistical	O
software	O
such	O
as	O
r	O
outputs	O
such	O
standard	O
errors	O
automatically	O
.	O
however	O
,	O
the	O
power	B
of	O
the	O
bootstrap	B
lies	O
in	O
the	O
fact	O
that	O
it	O
can	O
be	O
easily	O
applied	O
to	O
a	O
wide	O
range	O
of	O
statistical	O
learning	O
methods	O
,	O
including	O
some	O
for	O
which	O
a	O
measure	O
of	O
vari-	O
ability	O
is	O
otherwise	O
diﬃcult	O
to	O
obtain	O
and	O
is	O
not	O
automatically	O
output	B
by	O
statistical	O
software	O
.	O
in	O
this	O
section	O
we	O
illustrate	O
the	O
bootstrap	B
on	O
a	O
toy	O
example	O
in	O
which	O
we	O
wish	O
to	O
determine	O
the	O
best	O
investment	O
allocation	O
under	O
a	O
simple	B
model	O
.	O
in	O
section	O
5.3	O
we	O
explore	O
the	O
use	O
of	O
the	O
bootstrap	B
to	O
assess	O
the	O
variability	O
associated	O
with	O
the	O
regression	B
coeﬃcients	O
in	O
a	O
linear	B
model	I
ﬁt	O
.	O
suppose	O
that	O
we	O
wish	O
to	O
invest	O
a	O
ﬁxed	O
sum	O
of	O
money	O
in	O
two	O
ﬁnancial	O
assets	O
that	O
yield	O
returns	O
of	O
x	O
and	O
y	O
,	O
respectively	O
,	O
where	O
x	O
and	O
y	O
are	O
random	O
quantities	O
.	O
we	O
will	O
invest	O
a	O
fraction	O
α	O
of	O
our	O
money	O
in	O
x	O
,	O
and	O
will	O
invest	O
the	O
remaining	O
1	O
−	O
α	O
in	O
y	O
.	O
since	O
there	O
is	O
variability	O
associated	O
with	O
the	O
returns	O
on	O
these	O
two	O
assets	O
,	O
we	O
wish	O
to	O
choose	O
α	O
to	O
minimize	O
the	O
total	O
risk	O
,	O
or	O
variance	B
,	O
of	O
our	O
investment	O
.	O
in	O
other	O
words	O
,	O
we	O
want	O
to	O
minimize	O
var	O
(	O
αx	O
+	O
(	O
1	O
−	O
α	O
)	O
y	O
)	O
.	O
one	O
can	O
show	O
that	O
the	O
value	O
that	O
minimizes	O
the	O
risk	O
is	O
given	O
by	O
−	O
σxy	O
σ2	O
y	O
−	O
2σxy	O
,	O
(	O
5.6	O
)	O
α	O
=	O
σ2	O
x	O
+	O
σ2	O
y	O
where	O
σ2	O
x	O
=	O
var	O
(	O
x	O
)	O
,	O
σ2	O
y	O
=	O
var	O
(	O
y	O
)	O
,	O
and	O
σxy	O
=	O
cov	O
(	O
x	O
,	O
y	O
)	O
.	O
in	O
reality	O
,	O
the	O
quantities	O
σ2	O
x	O
,	O
σ2	O
y	O
,	O
and	O
σxy	O
are	O
unknown	O
.	O
we	O
can	O
compute	O
estimates	O
for	O
these	O
quantities	O
,	O
ˆσ2	O
x	O
,	O
ˆσ2	O
y	O
,	O
and	O
ˆσxy	O
,	O
using	O
a	O
data	B
set	O
that	O
contains	O
past	O
measurements	O
for	O
x	O
and	O
y	O
.	O
we	O
can	O
then	O
estimate	O
the	O
value	O
of	O
α	O
that	O
minimizes	O
the	O
variance	B
of	O
our	O
investment	O
using	O
−	O
ˆσxy	O
ˆσ2	O
y	O
−	O
2ˆσxy	O
ˆσ2	O
x	O
+	O
ˆσ2	O
y	O
ˆα	O
=	O
.	O
(	O
5.7	O
)	O
figure	O
5.9	O
illustrates	O
this	O
approach	B
for	O
estimating	O
α	O
on	O
a	O
simulated	O
data	B
set	O
.	O
in	O
each	O
panel	O
,	O
we	O
simulated	O
100	O
pairs	O
of	O
returns	O
for	O
the	O
investments	O
x	O
and	O
y	O
.	O
we	O
used	O
these	O
returns	O
to	O
estimate	O
σ2	O
y	O
,	O
and	O
σxy	O
,	O
which	O
we	O
then	O
substituted	O
into	O
(	O
5.7	O
)	O
in	O
order	O
to	O
obtain	O
estimates	O
for	O
α.	O
the	O
value	O
of	O
ˆα	O
resulting	O
from	O
each	O
simulated	O
data	B
set	O
ranges	O
from	O
0.532	O
to	O
0.657.	O
x	O
,	O
σ2	O
it	O
is	O
natural	B
to	O
wish	O
to	O
quantify	O
the	O
accuracy	O
of	O
our	O
estimate	O
of	O
α.	O
to	O
estimate	O
the	O
standard	O
deviation	O
of	O
ˆα	O
,	O
we	O
repeated	O
the	O
process	O
of	O
simu-	O
lating	O
100	O
paired	O
observations	B
of	O
x	O
and	O
y	O
,	O
and	O
estimating	O
α	O
using	O
(	O
5.7	O
)	O
,	O
188	O
5.	O
resampling	B
methods	O
2	O
1	O
y	O
0	O
1	O
−	O
2	O
−	O
2	O
1	O
y	O
0	O
1	O
−	O
2	O
−	O
3	O
−	O
2	O
1	O
y	O
0	O
1	O
−	O
2	O
−	O
−2	O
−1	O
0	O
x	O
1	O
2	O
−2	O
−1	O
1	O
2	O
0	O
x	O
2	O
1	O
y	O
0	O
1	O
−	O
2	O
−	O
3	O
−	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
−2	O
−1	O
0	O
1	O
2	O
3	O
x	O
x	O
figure	O
5.9.	O
each	O
panel	O
displays	O
100	O
simulated	O
returns	O
for	O
investments	O
x	O
and	O
y	O
.	O
from	O
left	O
to	O
right	O
and	O
top	O
to	O
bottom	O
,	O
the	O
resulting	O
estimates	O
for	O
α	O
are	O
0.576	O
,	O
0.532	O
,	O
0.657	O
,	O
and	O
0.651	O
.	O
1,000	O
times	O
.	O
we	O
thereby	O
obtained	O
1,000	O
estimates	O
for	O
α	O
,	O
which	O
we	O
can	O
call	O
ˆα1	O
,	O
ˆα2	O
,	O
.	O
.	O
.	O
,	O
ˆα1,000	O
.	O
the	O
left-hand	O
panel	O
of	O
figure	O
5.10	O
displays	O
a	O
histogram	B
of	O
the	O
resulting	O
estimates	O
.	O
for	O
these	O
simulations	O
the	O
parameters	O
were	O
set	B
to	O
σ2	O
x	O
=	O
1	O
,	O
σ2	O
y	O
=	O
1.25	O
,	O
and	O
σxy	O
=	O
0.5	O
,	O
and	O
so	O
we	O
know	O
that	O
the	O
true	O
value	O
of	O
α	O
is	O
0.6.	O
we	O
indicated	O
this	O
value	O
using	O
a	O
solid	O
vertical	O
line	B
on	O
the	O
histogram	B
.	O
the	O
mean	O
over	O
all	O
1,000	O
estimates	O
for	O
α	O
is	O
1,000	O
(	O
cid:17	O
)	O
1	O
1	O
,	O
000	O
r=1	O
¯α	O
=	O
ˆαr	O
=	O
0.5996	O
,	O
very	O
close	O
to	O
α	O
=	O
0.6	O
,	O
and	O
the	O
standard	O
deviation	O
of	O
the	O
estimates	O
is	O
(	O
cid:27	O
)	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
(	O
cid:29	O
)	O
1	O
1	O
,	O
000	O
−	O
1	O
1,000	O
(	O
cid:17	O
)	O
r=1	O
(	O
ˆαr	O
−	O
¯α	O
)	O
2	O
=	O
0.083.	O
this	O
gives	O
us	O
a	O
very	O
good	O
idea	O
of	O
the	O
accuracy	O
of	O
ˆα	O
:	O
se	O
(	O
ˆα	O
)	O
≈	O
0.083.	O
so	O
roughly	O
speaking	O
,	O
for	O
a	O
random	O
sample	O
from	O
the	O
population	O
,	O
we	O
would	O
expect	O
ˆα	O
to	O
diﬀer	O
from	O
α	O
by	O
approximately	O
0.08	O
,	O
on	O
average	B
.	O
in	O
practice	O
,	O
however	O
,	O
the	O
procedure	O
for	O
estimating	O
se	O
(	O
ˆα	O
)	O
outlined	O
above	O
can	O
not	O
be	O
applied	O
,	O
because	O
for	O
real	O
data	B
we	O
can	O
not	O
generate	O
new	O
samples	O
from	O
the	O
original	O
population	O
.	O
however	O
,	O
the	O
bootstrap	B
approach	O
allows	O
us	O
to	O
use	O
a	O
computer	O
to	O
emulate	O
the	O
process	O
of	O
obtaining	O
new	O
sample	O
sets	O
,	O
5.2	O
the	O
bootstrap	B
189	O
0	O
0	O
2	O
0	O
5	O
1	O
0	O
0	O
1	O
0	O
5	O
0	O
0	O
0	O
2	O
0	O
5	O
1	O
0	O
0	O
1	O
0	O
5	O
0	O
α	O
9	O
.	O
0	O
8	O
.	O
0	O
7	O
.	O
0	O
6	O
.	O
0	O
5	O
.	O
0	O
4	O
.	O
0	O
3	O
.	O
0	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
0.3	O
0.4	O
0.5	O
α	O
0.6	O
α	O
0.7	O
0.8	O
0.9	O
true	O
bootstrap	O
figure	O
5.10.	O
left	O
:	O
a	O
histogram	B
of	O
the	O
estimates	O
of	O
α	O
obtained	O
by	O
generating	O
1,000	O
simulated	O
data	B
sets	O
from	O
the	O
true	O
population	O
.	O
center	O
:	O
a	O
histogram	B
of	O
the	O
estimates	O
of	O
α	O
obtained	O
from	O
1,000	O
bootstrap	B
samples	O
from	O
a	O
single	B
data	O
set	B
.	O
right	O
:	O
the	O
estimates	O
of	O
α	O
displayed	O
in	O
the	O
left	O
and	O
center	O
panels	O
are	O
shown	O
as	O
boxplots	O
.	O
in	O
each	O
panel	O
,	O
the	O
pink	O
line	B
indicates	O
the	O
true	O
value	O
of	O
α.	O
so	O
that	O
we	O
can	O
estimate	O
the	O
variability	O
of	O
ˆα	O
without	O
generating	O
additional	O
samples	O
.	O
rather	O
than	O
repeatedly	O
obtaining	O
independent	B
data	O
sets	O
from	O
the	O
population	O
,	O
we	O
instead	O
obtain	O
distinct	O
data	B
sets	O
by	O
repeatedly	O
sampling	O
observations	B
from	O
the	O
original	O
data	B
set	O
.	O
this	O
approach	B
is	O
illustrated	O
in	O
figure	O
5.11	O
on	O
a	O
simple	B
data	O
set	B
,	O
which	O
we	O
call	O
z	O
,	O
that	O
contains	O
only	O
n	O
=	O
3	O
observations	B
.	O
we	O
randomly	O
select	O
n	O
observations	B
from	O
the	O
data	B
set	O
in	O
order	O
to	O
produce	O
a	O
bootstrap	B
data	O
set	B
,	O
∗1	O
.	O
the	O
sampling	O
is	O
performed	O
with	O
replacement	B
,	O
which	O
means	O
that	O
the	O
z	O
same	O
observation	O
can	O
occur	O
more	O
than	O
once	O
in	O
the	O
bootstrap	B
data	O
set	B
.	O
in	O
∗1	O
contains	O
the	O
third	O
observation	O
twice	O
,	O
the	O
ﬁrst	O
observation	O
this	O
example	O
,	O
z	O
once	O
,	O
and	O
no	O
instances	O
of	O
the	O
second	O
observation	O
.	O
note	O
that	O
if	O
an	O
observation	O
∗1	O
,	O
then	O
both	O
its	O
x	O
and	O
y	O
values	O
are	O
included	O
.	O
we	O
can	O
use	O
is	O
contained	O
in	O
z	O
∗1	O
.	O
this	O
∗1	O
to	O
produce	O
a	O
new	O
bootstrap	B
estimate	O
for	O
α	O
,	O
which	O
we	O
call	O
ˆα	O
z	O
procedure	O
is	O
repeated	O
b	O
times	O
for	O
some	O
large	O
value	O
of	O
b	O
,	O
in	O
order	O
to	O
produce	O
∗b	O
,	O
and	O
b	O
corresponding	O
α	O
b	O
diﬀerent	O
bootstrap	B
data	O
sets	O
,	O
z	O
∗b	O
.	O
we	O
can	O
compute	O
the	O
standard	B
error	I
of	O
these	O
estimates	O
,	O
ˆα	O
bootstrap	B
estimates	O
using	O
the	O
formula	O
(	O
cid:31	O
)	O
b	O
(	O
cid:17	O
)	O
∗2	O
,	O
.	O
.	O
.	O
,	O
z	O
∗1	O
,	O
z	O
∗1	O
,	O
ˆα	O
∗2	O
,	O
.	O
.	O
.	O
,	O
ˆα	O
2	O
seb	O
(	O
ˆα	O
)	O
=	O
.	O
(	O
5.8	O
)	O
(	O
cid:27	O
)	O
(	O
cid:28	O
)	O
(	O
cid:28	O
)	O
(	O
cid:29	O
)	O
1	O
b	O
−	O
1	O
ˆα∗r	O
−	O
1	O
b	O
b	O
(	O
cid:17	O
)	O
r	O
(	O
cid:2	O
)	O
=1	O
ˆα∗r	O
(	O
cid:2	O
)	O
r=1	O
this	O
serves	O
as	O
an	O
estimate	O
of	O
the	O
standard	B
error	I
of	O
ˆα	O
estimated	O
from	O
the	O
original	O
data	B
set	O
.	O
the	O
bootstrap	B
approach	O
is	O
illustrated	O
in	O
the	O
center	O
panel	O
of	O
figure	O
5.10	O
,	O
which	O
displays	O
a	O
histogram	B
of	O
1,000	O
bootstrap	B
estimates	O
of	O
α	O
,	O
each	O
com-	O
puted	O
using	O
a	O
distinct	O
bootstrap	B
data	O
set	B
.	O
this	O
panel	O
was	O
constructed	O
on	O
the	O
basis	B
of	O
a	O
single	B
data	O
set	B
,	O
and	O
hence	O
could	O
be	O
created	O
using	O
real	O
data	B
.	O
replacement	B
190	O
5.	O
resampling	B
methods	O
obs	O
x	O
1	O
2	O
3	O
4.3	O
2.1	O
5.3	O
y	O
2.4	O
1.1	O
2.8	O
original	O
data	B
(	O
z	O
)	O
z*1	O
z*2	O
···	O
···	O
·	O
··	O
···	O
z*b	O
obs	O
x	O
3	O
1	O
3	O
5.3	O
4.3	O
5.3	O
obs	O
x	O
2	O
3	O
1	O
obs	O
2	O
2	O
1	O
2.1	O
5.3	O
4.3	O
···	O
···	O
···	O
x	O
2.1	O
2.1	O
4.3	O
y	O
2.8	O
2.4	O
2.8	O
y	O
1.1	O
2.8	O
2.4	O
y	O
1.1	O
1.1	O
2.4	O
a*1	O
ˆ	O
a*2	O
ˆ	O
a*b	O
ˆ	O
···	O
···	O
···	O
···	O
···	O
figure	O
5.11.	O
a	O
graphical	O
illustration	O
of	O
the	O
bootstrap	B
approach	O
on	O
a	O
small	O
sample	O
containing	O
n	O
=	O
3	O
observations	B
.	O
each	O
bootstrap	B
data	O
set	B
contains	O
n	O
obser-	O
vations	O
,	O
sampled	O
with	O
replacement	B
from	O
the	O
original	O
data	B
set	O
.	O
each	O
bootstrap	B
data	O
set	B
is	O
used	O
to	O
obtain	O
an	O
estimate	O
of	O
α.	O
note	O
that	O
the	O
histogram	B
looks	O
very	O
similar	O
to	O
the	O
left-hand	O
panel	O
which	O
dis-	O
plays	O
the	O
idealized	O
histogram	B
of	O
the	O
estimates	O
of	O
α	O
obtained	O
by	O
generating	O
1,000	O
simulated	O
data	B
sets	O
from	O
the	O
true	O
population	O
.	O
in	O
particular	O
the	O
boot-	O
strap	O
estimate	O
se	O
(	O
ˆα	O
)	O
from	O
(	O
5.8	O
)	O
is	O
0.087	O
,	O
very	O
close	O
to	O
the	O
estimate	O
of	O
0.083	O
obtained	O
using	O
1,000	O
simulated	O
data	B
sets	O
.	O
the	O
right-hand	O
panel	O
displays	O
the	O
information	O
in	O
the	O
center	O
and	O
left	O
panels	O
in	O
a	O
diﬀerent	O
way	O
,	O
via	O
boxplots	O
of	O
the	O
estimates	O
for	O
α	O
obtained	O
by	O
generating	O
1,000	O
simulated	O
data	B
sets	O
from	O
the	O
true	O
population	O
and	O
using	O
the	O
bootstrap	B
approach	O
.	O
again	O
,	O
the	O
boxplots	O
are	O
quite	O
similar	O
to	O
each	O
other	O
,	O
indicating	O
that	O
the	O
bootstrap	B
approach	O
can	O
be	O
used	O
to	O
eﬀectively	O
estimate	O
the	O
variability	O
associated	O
with	O
ˆα	O
.	O
5.3	O
lab	O
:	O
cross-validation	B
and	O
the	O
bootstrap	B
in	O
this	O
lab	O
,	O
we	O
explore	O
the	O
resampling	B
techniques	O
covered	O
in	O
this	O
chapter	O
.	O
some	O
of	O
the	O
commands	O
in	O
this	O
lab	O
may	O
take	O
a	O
while	O
to	O
run	O
on	O
your	O
com-	O
puter	O
.	O
5.3	O
lab	O
:	O
cross-validation	B
and	O
the	O
bootstrap	B
191	O
5.3.1	O
the	O
validation	B
set	I
approach	O
we	O
explore	O
the	O
use	O
of	O
the	O
validation	B
set	I
approach	O
in	O
order	O
to	O
estimate	O
the	O
test	B
error	O
rates	O
that	O
result	O
from	O
ﬁtting	O
various	O
linear	B
models	O
on	O
the	O
auto	O
data	B
set	O
.	O
before	O
we	O
begin	O
,	O
we	O
use	O
the	O
set.seed	O
(	O
)	O
function	B
in	O
order	O
to	O
set	B
a	O
seed	B
for	O
r	O
’	O
s	O
random	O
number	O
generator	O
,	O
so	O
that	O
the	O
reader	O
of	O
this	O
book	O
will	O
obtain	O
precisely	O
the	O
same	O
results	O
as	O
those	O
shown	O
below	O
.	O
it	O
is	O
generally	O
a	O
good	O
idea	O
to	O
set	B
a	O
random	O
seed	O
when	O
performing	O
an	O
analysis	B
such	O
as	O
cross-validation	B
that	O
contains	O
an	O
element	O
of	O
randomness	O
,	O
so	O
that	O
the	O
results	O
obtained	O
can	O
be	O
reproduced	O
precisely	O
at	O
a	O
later	O
time	O
.	O
we	O
begin	O
by	O
using	O
the	O
sample	O
(	O
)	O
function	B
to	O
split	O
the	O
set	B
of	O
observations	B
into	O
two	O
halves	O
,	O
by	O
selecting	O
a	O
random	O
subset	O
of	O
196	O
observations	B
out	O
of	O
the	O
original	O
392	O
observations	B
.	O
we	O
refer	O
to	O
these	O
observations	B
as	O
the	O
training	B
set	O
.	O
seed	B
sample	O
(	O
)	O
>	O
library	O
(	O
islr	O
)	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
train	B
=	O
sample	O
(	O
392	O
,196	O
)	O
(	O
here	O
we	O
use	O
a	O
shortcut	O
in	O
the	O
sample	O
command	O
;	O
see	O
?	O
sample	O
for	O
details	O
.	O
)	O
we	O
then	O
use	O
the	O
subset	O
option	O
in	O
lm	O
(	O
)	O
to	O
ﬁt	B
a	O
linear	B
regression	I
using	O
only	O
the	O
observations	B
corresponding	O
to	O
the	O
training	B
set	O
.	O
>	O
lm	O
.	O
fit	O
=	O
lm	O
(	O
mpg∼horsepower	O
,	O
data	B
=	O
auto	O
,	O
subset	O
=	O
train	B
)	O
we	O
now	O
use	O
the	O
predict	O
(	O
)	O
function	B
to	O
estimate	O
the	O
response	B
for	O
all	O
392	O
observations	B
,	O
and	O
we	O
use	O
the	O
mean	O
(	O
)	O
function	B
to	O
calculate	O
the	O
mse	O
of	O
the	O
196	O
observations	B
in	O
the	O
validation	B
set	I
.	O
note	O
that	O
the	O
-train	O
index	O
below	O
selects	O
only	O
the	O
observations	B
that	O
are	O
not	O
in	O
the	O
training	B
set	O
.	O
>	O
attach	O
(	O
auto	O
)	O
>	O
mean	O
(	O
(	O
mpg	O
-	O
predict	O
(	O
lm	O
.	O
fit	O
,	O
auto	O
)	O
)	O
[	O
-	O
train	B
]	O
^2	O
)	O
[	O
1	O
]	O
26.14	O
therefore	O
,	O
the	O
estimated	O
test	B
mse	O
for	O
the	O
linear	B
regression	I
ﬁt	O
is	O
26.14.	O
we	O
can	O
use	O
the	O
poly	O
(	O
)	O
function	B
to	O
estimate	O
the	O
test	B
error	O
for	O
the	O
quadratic	B
and	O
cubic	B
regressions	O
.	O
>	O
lm	O
.	O
fit2	O
=	O
lm	O
(	O
mpg∼poly	O
(	O
horsepower	O
,2	O
)	O
,	O
data	B
=	O
auto	O
,	O
subset	O
=	O
train	B
)	O
>	O
mean	O
(	O
(	O
mpg	O
-	O
predict	O
(	O
lm	O
.	O
fit2	O
,	O
auto	O
)	O
)	O
[	O
-	O
train	B
]	O
^2	O
)	O
[	O
1	O
]	O
19.82	O
>	O
lm	O
.	O
fit3	O
=	O
lm	O
(	O
mpg∼poly	O
(	O
horsepower	O
,3	O
)	O
,	O
data	B
=	O
auto	O
,	O
subset	O
=	O
train	B
)	O
>	O
mean	O
(	O
(	O
mpg	O
-	O
predict	O
(	O
lm	O
.	O
fit3	O
,	O
auto	O
)	O
)	O
[	O
-	O
train	B
]	O
^2	O
)	O
[	O
1	O
]	O
19.78	O
these	O
error	B
rates	O
are	O
19.82	O
and	O
19.78	O
,	O
respectively	O
.	O
if	O
we	O
choose	O
a	O
diﬀerent	O
training	B
set	O
instead	O
,	O
then	O
we	O
will	O
obtain	O
somewhat	O
diﬀerent	O
errors	O
on	O
the	O
validation	B
set	I
.	O
>	O
set	B
.	O
seed	B
(	O
2	O
)	O
>	O
train	B
=	O
sample	O
(	O
392	O
,196	O
)	O
>	O
lm	O
.	O
fit	O
=	O
lm	O
(	O
mpg∼horsepower	O
,	O
subset	O
=	O
train	B
)	O
192	O
5.	O
resampling	B
methods	O
>	O
mean	O
(	O
(	O
mpg	O
-	O
predict	O
(	O
lm	O
.	O
fit	O
,	O
auto	O
)	O
)	O
[	O
-	O
train	B
]	O
^2	O
)	O
[	O
1	O
]	O
23.30	O
>	O
lm	O
.	O
fit2	O
=	O
lm	O
(	O
mpg∼poly	O
(	O
horsepower	O
,2	O
)	O
,	O
data	B
=	O
auto	O
,	O
subset	O
=	O
train	B
)	O
>	O
mean	O
(	O
(	O
mpg	O
-	O
predict	O
(	O
lm	O
.	O
fit2	O
,	O
auto	O
)	O
)	O
[	O
-	O
train	B
]	O
^2	O
)	O
[	O
1	O
]	O
18.90	O
>	O
lm	O
.	O
fit3	O
=	O
lm	O
(	O
mpg∼poly	O
(	O
horsepower	O
,3	O
)	O
,	O
data	B
=	O
auto	O
,	O
subset	O
=	O
train	B
)	O
>	O
mean	O
(	O
(	O
mpg	O
-	O
predict	O
(	O
lm	O
.	O
fit3	O
,	O
auto	O
)	O
)	O
[	O
-	O
train	B
]	O
^2	O
)	O
[	O
1	O
]	O
19.26	O
using	O
this	O
split	O
of	O
the	O
observations	B
into	O
a	O
training	B
set	O
and	O
a	O
validation	B
set	I
,	O
we	O
ﬁnd	O
that	O
the	O
validation	B
set	I
error	O
rates	O
for	O
the	O
models	O
with	O
linear	B
,	O
quadratic	B
,	O
and	O
cubic	B
terms	O
are	O
23.30	O
,	O
18.90	O
,	O
and	O
19.26	O
,	O
respectively	O
.	O
these	O
results	O
are	O
consistent	O
with	O
our	O
previous	O
ﬁndings	O
:	O
a	O
model	B
that	O
predicts	O
mpg	O
using	O
a	O
quadratic	B
function	O
of	O
horsepower	O
performs	O
better	O
than	O
a	O
model	B
that	O
involves	O
only	O
a	O
linear	B
function	O
of	O
horsepower	O
,	O
and	O
there	O
is	O
little	O
evidence	O
in	O
favor	O
of	O
a	O
model	B
that	O
uses	O
a	O
cubic	B
function	O
of	O
horsepower	O
.	O
5.3.2	O
leave-one-out	B
cross-validation	O
the	O
loocv	O
estimate	O
can	O
be	O
automatically	O
computed	O
for	O
any	O
generalized	B
linear	I
model	I
using	O
the	O
glm	O
(	O
)	O
and	O
cv.glm	O
(	O
)	O
functions	O
.	O
in	O
the	O
lab	O
for	O
chap-	O
ter	O
4	O
,	O
we	O
used	O
the	O
glm	O
(	O
)	O
function	B
to	O
perform	O
logistic	B
regression	I
by	O
passing	O
in	O
the	O
family=	O
''	O
binomial	O
''	O
argument	B
.	O
but	O
if	O
we	O
use	O
glm	O
(	O
)	O
to	O
ﬁt	B
a	O
model	B
without	O
passing	O
in	O
the	O
family	O
argument	B
,	O
then	O
it	O
performs	O
linear	B
regression	I
,	O
just	O
like	O
the	O
lm	O
(	O
)	O
function	B
.	O
so	O
for	O
instance	O
,	O
>	O
glm	O
.	O
fit	O
=	O
glm	O
(	O
mpg∼horsepower	O
,	O
data	B
=	O
auto	O
)	O
cv.glm	O
(	O
)	O
>	O
coef	O
(	O
glm	O
.	O
fit	O
)	O
(	O
intercept	B
)	O
39.936	O
horsepower	O
-0.158	O
and	O
>	O
lm	O
.	O
fit	O
=	O
lm	O
(	O
mpg∼horsepower	O
,	O
data	B
=	O
auto	O
)	O
>	O
coef	O
(	O
lm	O
.	O
fit	O
)	O
(	O
intercept	B
)	O
39.936	O
horsepower	O
-0.158	O
yield	O
identical	O
linear	B
regression	I
models	O
.	O
in	O
this	O
lab	O
,	O
we	O
will	O
perform	O
linear	B
regression	I
using	O
the	O
glm	O
(	O
)	O
function	B
rather	O
than	O
the	O
lm	O
(	O
)	O
function	B
because	O
the	O
former	O
can	O
be	O
used	O
together	O
with	O
cv	O
.glm	O
(	O
)	O
.	O
the	O
cv	O
.glm	O
(	O
)	O
function	B
is	O
part	O
of	O
the	O
boot	O
library	O
.	O
>	O
library	O
(	O
boot	O
)	O
>	O
glm	O
.	O
fit	O
=	O
glm	O
(	O
mpg∼horsepower	O
,	O
data	B
=	O
auto	O
)	O
>	O
cv	O
.	O
err	O
=	O
cv	O
.	O
glm	O
(	O
auto	O
,	O
glm	O
.	O
fit	O
)	O
>	O
cv	O
.	O
err	O
$	O
delta	O
1	O
1	O
24.23	O
24.23	O
the	O
cv.glm	O
(	O
)	O
function	B
produces	O
a	O
list	O
with	O
several	O
components	O
.	O
the	O
two	O
numbers	O
in	O
the	O
delta	O
vector	B
contain	O
the	O
cross-validation	B
results	O
.	O
in	O
this	O
5.3	O
lab	O
:	O
cross-validation	B
and	O
the	O
bootstrap	B
193	O
case	O
the	O
numbers	O
are	O
identical	O
(	O
up	O
to	O
two	O
decimal	O
places	O
)	O
and	O
correspond	O
to	O
the	O
loocv	O
statistic	O
given	O
in	O
(	O
5.1	O
)	O
.	O
below	O
,	O
we	O
discuss	O
a	O
situation	O
in	O
which	O
the	O
two	O
numbers	O
diﬀer	O
.	O
our	O
cross-validation	B
estimate	O
for	O
the	O
test	B
error	O
is	O
approximately	O
24.23.	O
we	O
can	O
repeat	O
this	O
procedure	O
for	O
increasingly	O
complex	O
polynomial	B
ﬁts	O
.	O
to	O
automate	O
the	O
process	O
,	O
we	O
use	O
the	O
for	O
(	O
)	O
function	B
to	O
initiate	O
a	O
for	B
loop	I
which	O
iteratively	O
ﬁts	O
polynomial	B
regressions	O
for	O
polynomials	O
of	O
order	O
i	O
=	O
1	O
to	O
i	O
=	O
5	O
,	O
computes	O
the	O
associated	O
cross-validation	B
error	O
,	O
and	O
stores	O
it	O
in	O
the	O
ith	O
element	O
of	O
the	O
vector	B
cv.error	O
.	O
we	O
begin	O
by	O
initializing	O
the	O
vector	B
.	O
this	O
command	O
will	O
likely	O
take	O
a	O
couple	O
of	O
minutes	O
to	O
run	O
.	O
for	O
(	O
)	O
for	B
loop	I
>	O
cv	O
.	O
error	B
=	O
rep	O
(	O
0	O
,5	O
)	O
>	O
for	O
(	O
i	O
in	O
1:5	O
)	O
{	O
+	O
glm	O
.	O
fit	O
=	O
glm	O
(	O
mpg∼poly	O
(	O
horsepower	O
,	O
i	O
)	O
,	O
data	B
=	O
auto	O
)	O
+	O
cv	O
.	O
error	B
[	O
i	O
]	O
=	O
cv	O
.	O
glm	O
(	O
auto	O
,	O
glm	O
.	O
fit	O
)	O
$	O
delta	O
[	O
1	O
]	O
+	O
}	O
>	O
cv	O
.	O
error	B
[	O
1	O
]	O
24.23	O
19.25	O
19.33	O
19.42	O
19.03	O
as	O
in	O
figure	O
5.4	O
,	O
we	O
see	O
a	O
sharp	O
drop	O
in	O
the	O
estimated	O
test	B
mse	O
between	O
the	O
linear	B
and	O
quadratic	B
ﬁts	O
,	O
but	O
then	O
no	O
clear	O
improvement	O
from	O
using	O
higher-order	O
polynomials	O
.	O
5.3.3	O
k-fold	B
cross-validation	O
the	O
cv.glm	O
(	O
)	O
function	B
can	O
also	O
be	O
used	O
to	O
implement	O
k-fold	B
cv	O
.	O
below	O
we	O
use	O
k	O
=	O
10	O
,	O
a	O
common	O
choice	O
for	O
k	O
,	O
on	O
the	O
auto	O
data	B
set	O
.	O
we	O
once	O
again	O
set	B
a	O
random	O
seed	O
and	O
initialize	O
a	O
vector	B
in	O
which	O
we	O
will	O
store	O
the	O
cv	O
errors	O
corresponding	O
to	O
the	O
polynomial	B
ﬁts	O
of	O
orders	O
one	O
to	O
ten	O
.	O
>	O
set	B
.	O
seed	B
(	O
17	O
)	O
>	O
cv	O
.	O
error	B
.10=	O
rep	O
(	O
0	O
,10	O
)	O
>	O
for	O
(	O
i	O
in	O
1:10	O
)	O
{	O
+	O
glm	O
.	O
fit	O
=	O
glm	O
(	O
mpg∼poly	O
(	O
horsepower	O
,	O
i	O
)	O
,	O
data	B
=	O
auto	O
)	O
+	O
cv	O
.	O
error	B
.10	O
[	O
i	O
]	O
=	O
cv	O
.	O
glm	O
(	O
auto	O
,	O
glm	O
.	O
fit	O
,	O
k	O
=10	O
)	O
$	O
delta	O
[	O
1	O
]	O
+	O
}	O
>	O
cv	O
.	O
error	B
.10	O
[	O
1	O
]	O
24.21	O
19.19	O
19.31	O
19.34	O
18.88	O
19.02	O
18.90	O
19.71	O
18.95	O
19.50	O
notice	O
that	O
the	O
computation	O
time	O
is	O
much	O
shorter	O
than	O
that	O
of	O
loocv	O
.	O
(	O
in	O
principle	O
,	O
the	O
computation	O
time	O
for	O
loocv	O
for	O
a	O
least	B
squares	I
linear	O
model	B
should	O
be	O
faster	O
than	O
for	O
k-fold	O
cv	O
,	O
due	O
to	O
the	O
availability	O
of	O
the	O
formula	O
(	O
5.2	O
)	O
for	O
loocv	O
;	O
however	O
,	O
unfortunately	O
the	O
cv.glm	O
(	O
)	O
function	B
does	O
not	O
make	O
use	O
of	O
this	O
formula	O
.	O
)	O
we	O
still	O
see	O
little	O
evidence	O
that	O
using	O
cubic	B
or	O
higher-order	O
polynomial	B
terms	O
leads	O
to	O
lower	O
test	B
error	O
than	O
simply	O
using	O
a	O
quadratic	B
ﬁt	O
.	O
we	O
saw	O
in	O
section	O
5.3.2	O
that	O
the	O
two	O
numbers	O
associated	O
with	O
delta	O
are	O
essentially	O
the	O
same	O
when	O
loocv	O
is	O
performed	O
.	O
when	O
we	O
instead	O
perform	O
k-fold	B
cv	O
,	O
then	O
the	O
two	O
numbers	O
associated	O
with	O
delta	O
diﬀer	O
slightly	O
.	O
the	O
194	O
5.	O
resampling	B
methods	O
ﬁrst	O
is	O
the	O
standard	O
k-fold	O
cv	O
estimate	O
,	O
as	O
in	O
(	O
5.3	O
)	O
.	O
the	O
second	O
is	O
a	O
bias-	O
corrected	O
version	O
.	O
on	O
this	O
data	B
set	O
,	O
the	O
two	O
estimates	O
are	O
very	O
similar	O
to	O
each	O
other	O
.	O
5.3.4	O
the	O
bootstrap	B
we	O
illustrate	O
the	O
use	O
of	O
the	O
bootstrap	B
in	O
the	O
simple	B
example	O
of	O
section	O
5.2	O
,	O
as	O
well	O
as	O
on	O
an	O
example	O
involving	O
estimating	O
the	O
accuracy	O
of	O
the	O
linear	B
regression	I
model	O
on	O
the	O
auto	O
data	B
set	O
.	O
estimating	O
the	O
accuracy	O
of	O
a	O
statistic	O
of	O
interest	O
one	O
of	O
the	O
great	O
advantages	O
of	O
the	O
bootstrap	B
approach	O
is	O
that	O
it	O
can	O
be	O
applied	O
in	O
almost	O
all	O
situations	O
.	O
no	O
complicated	O
mathematical	O
calculations	O
are	O
required	O
.	O
performing	O
a	O
bootstrap	B
analysis	O
in	O
r	O
entails	O
only	O
two	O
steps	O
.	O
first	O
,	O
we	O
must	O
create	O
a	O
function	B
that	O
computes	O
the	O
statistic	O
of	O
interest	O
.	O
second	O
,	O
we	O
use	O
the	O
boot	O
(	O
)	O
function	B
,	O
which	O
is	O
part	O
of	O
the	O
boot	O
library	O
,	O
to	O
perform	O
the	O
bootstrap	B
by	O
repeatedly	O
sampling	O
observations	B
from	O
the	O
data	B
set	O
with	O
replacement	B
.	O
the	O
portfolio	O
data	B
set	O
in	O
the	O
islr	O
package	O
is	O
described	O
in	O
section	O
5.2.	O
to	O
illustrate	O
the	O
use	O
of	O
the	O
bootstrap	B
on	O
this	O
data	B
,	O
we	O
must	O
ﬁrst	O
create	O
a	O
function	B
,	O
alpha.fn	O
(	O
)	O
,	O
which	O
takes	O
as	O
input	B
the	O
(	O
x	O
,	O
y	O
)	O
data	B
as	O
well	O
as	O
a	O
vector	B
indicating	O
which	O
observations	B
should	O
be	O
used	O
to	O
estimate	O
α.	O
the	O
function	B
then	O
outputs	O
the	O
estimate	O
for	O
α	O
based	O
on	O
the	O
selected	O
observations	B
.	O
>	O
alpha	O
.	O
fn	O
=	O
function	B
(	O
data	B
,	O
index	O
)	O
{	O
+	O
x	O
=	O
data	B
$	O
x	O
[	O
index	O
]	O
+	O
y	O
=	O
data	B
$	O
y	O
[	O
index	O
]	O
+	O
return	O
(	O
(	O
var	O
(	O
y	O
)	O
-	O
cov	O
(	O
x	O
,	O
y	O
)	O
)	O
/	O
(	O
var	O
(	O
x	O
)	O
+	O
var	O
(	O
y	O
)	O
-2*	O
cov	O
(	O
x	O
,	O
y	O
)	O
)	O
)	O
+	O
}	O
this	O
function	B
returns	O
,	O
or	O
outputs	O
,	O
an	O
estimate	O
for	O
α	O
based	O
on	O
applying	O
(	O
5.7	O
)	O
to	O
the	O
observations	B
indexed	O
by	O
the	O
argument	B
index	O
.	O
for	O
instance	O
,	O
the	O
following	O
command	O
tells	O
r	O
to	O
estimate	O
α	O
using	O
all	O
100	O
observations	B
.	O
>	O
alpha	O
.	O
fn	O
(	O
portfolio	O
,1:100	O
)	O
[	O
1	O
]	O
0.576	O
the	O
next	O
command	O
uses	O
the	O
sample	O
(	O
)	O
function	B
to	O
randomly	O
select	O
100	O
ob-	O
servations	O
from	O
the	O
range	O
1	O
to	O
100	O
,	O
with	O
replacement	B
.	O
this	O
is	O
equivalent	O
to	O
constructing	O
a	O
new	O
bootstrap	B
data	O
set	B
and	O
recomputing	O
ˆα	O
based	O
on	O
the	O
new	O
data	B
set	O
.	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
alpha	O
.	O
fn	O
(	O
portfolio	O
,	O
sample	O
(	O
100	O
,100	O
,	O
replace	O
=	O
t	O
)	O
)	O
[	O
1	O
]	O
0.596	O
we	O
can	O
implement	O
a	O
bootstrap	B
analysis	O
by	O
performing	O
this	O
command	O
many	O
times	O
,	O
recording	O
all	O
of	O
the	O
corresponding	O
estimates	O
for	O
α	O
,	O
and	O
computing	O
boot	O
(	O
)	O
5.3	O
lab	O
:	O
cross-validation	B
and	O
the	O
bootstrap	B
195	O
the	O
resulting	O
standard	O
deviation	O
.	O
however	O
,	O
the	O
boot	O
(	O
)	O
function	B
automates	O
this	O
approach	B
.	O
below	O
we	O
produce	O
r	O
=	O
1	O
,	O
000	O
bootstrap	B
estimates	O
for	O
α.	O
boot	O
(	O
)	O
>	O
boot	O
(	O
portfolio	O
,	O
alpha	O
.	O
fn	O
,	O
r	O
=1000	O
)	O
ordinary	O
n	O
o	O
n	O
p	O
a	O
r	O
a	O
m	O
e	O
t	O
r	O
i	O
c	O
bootstrap	B
call	O
:	O
boot	O
(	O
data	B
=	O
portfolio	O
,	O
statistic	O
=	O
alpha	O
.	O
fn	O
,	O
r	O
=	O
1000	O
)	O
bootstrap	B
statistics	O
:	O
original	O
0.5758	O
t1	O
*	O
bias	B
-7.315	O
e	O
-05	O
std	O
.	O
error	B
0.0886	O
the	O
ﬁnal	O
output	B
shows	O
that	O
using	O
the	O
original	O
data	B
,	O
ˆα	O
=	O
0.5758	O
,	O
and	O
that	O
the	O
bootstrap	B
estimate	O
for	O
se	O
(	O
ˆα	O
)	O
is	O
0.0886.	O
estimating	O
the	O
accuracy	O
of	O
a	O
linear	B
regression	I
model	O
the	O
bootstrap	B
approach	O
can	O
be	O
used	O
to	O
assess	O
the	O
variability	O
of	O
the	O
coef-	O
ﬁcient	O
estimates	O
and	O
predictions	O
from	O
a	O
statistical	O
learning	O
method	O
.	O
here	O
we	O
use	O
the	O
bootstrap	B
approach	O
in	O
order	O
to	O
assess	O
the	O
variability	O
of	O
the	O
estimates	O
for	O
β0	O
and	O
β1	O
,	O
the	O
intercept	B
and	O
slope	B
terms	O
for	O
the	O
linear	B
regres-	O
sion	O
model	B
that	O
uses	O
horsepower	O
to	O
predict	O
mpg	O
in	O
the	O
auto	O
data	B
set	O
.	O
we	O
will	O
compare	O
the	O
estimates	O
obtained	O
using	O
the	O
bootstrap	B
to	O
those	O
obtained	O
using	O
the	O
formulas	O
for	O
se	O
(	O
ˆβ0	O
)	O
and	O
se	O
(	O
ˆβ1	O
)	O
described	O
in	O
section	O
3.1.2.	O
we	O
ﬁrst	O
create	O
a	O
simple	B
function	O
,	O
boot.fn	O
(	O
)	O
,	O
which	O
takes	O
in	O
the	O
auto	O
data	B
set	O
as	O
well	O
as	O
a	O
set	B
of	O
indices	O
for	O
the	O
observations	B
,	O
and	O
returns	O
the	O
intercept	B
and	O
slope	B
estimates	O
for	O
the	O
linear	B
regression	I
model	O
.	O
we	O
then	O
apply	O
this	O
function	B
to	O
the	O
full	O
set	B
of	O
392	O
observations	B
in	O
order	O
to	O
compute	O
the	O
esti-	O
mates	O
of	O
β0	O
and	O
β1	O
on	O
the	O
entire	O
data	B
set	O
using	O
the	O
usual	O
linear	B
regression	I
coeﬃcient	O
estimate	O
formulas	O
from	O
chapter	O
3.	O
note	O
that	O
we	O
do	O
not	O
need	O
the	O
{	O
and	O
}	O
at	O
the	O
beginning	O
and	O
end	O
of	O
the	O
function	B
because	O
it	O
is	O
only	O
one	O
line	B
long	O
.	O
>	O
boot	O
.	O
fn	O
=	O
function	B
(	O
data	B
,	O
index	O
)	O
+	O
return	O
(	O
coef	O
(	O
lm	O
(	O
mpg∼horsepower	O
,	O
data	B
=	O
data	B
,	O
subset	O
=	O
index	O
)	O
)	O
)	O
>	O
boot	O
.	O
fn	O
(	O
auto	O
,1:392	O
)	O
(	O
intercept	B
)	O
horsepower	O
39.936	O
-0.158	O
the	O
boot.fn	O
(	O
)	O
function	B
can	O
also	O
be	O
used	O
in	O
order	O
to	O
create	O
bootstrap	B
esti-	O
mates	O
for	O
the	O
intercept	B
and	O
slope	B
terms	O
by	O
randomly	O
sampling	O
from	O
among	O
the	O
observations	B
with	O
replacement	B
.	O
here	O
we	O
give	O
two	O
examples	O
.	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
boot	O
.	O
fn	O
(	O
auto	O
,	O
sample	O
(	O
392	O
,392	O
,	O
replace	O
=	O
t	O
)	O
)	O
(	O
intercept	B
)	O
horsepower	O
38.739	O
-0.148	O
>	O
boot	O
.	O
fn	O
(	O
auto	O
,	O
sample	O
(	O
392	O
,392	O
,	O
replace	O
=	O
t	O
)	O
)	O
(	O
intercept	B
)	O
horsepower	O
40.038	O
-0.160	O
196	O
5.	O
resampling	B
methods	O
next	O
,	O
we	O
use	O
the	O
boot	O
(	O
)	O
function	B
to	O
compute	O
the	O
standard	O
errors	O
of	O
1,000	O
bootstrap	B
estimates	O
for	O
the	O
intercept	B
and	O
slope	B
terms	O
.	O
>	O
boot	O
(	O
auto	O
,	O
boot	O
.	O
fn	O
,1000	O
)	O
ordinary	O
n	O
o	O
n	O
p	O
a	O
r	O
a	O
m	O
e	O
t	O
r	O
i	O
c	O
bootstrap	B
call	O
:	O
boot	O
(	O
data	B
=	O
auto	O
,	O
statistic	O
=	O
boot	O
.	O
fn	O
,	O
r	O
=	O
1000	O
)	O
bootstrap	B
statistics	O
:	O
original	O
t1	O
*	O
39.936	O
t2	O
*	O
-0.158	O
bias	B
0.0297	O
-0.0003	O
std	O
.	O
error	B
0.8600	O
0.0074	O
this	O
indicates	O
that	O
the	O
bootstrap	B
estimate	O
for	O
se	O
(	O
ˆβ0	O
)	O
is	O
0.86	O
,	O
and	O
that	O
the	O
bootstrap	B
estimate	O
for	O
se	O
(	O
ˆβ1	O
)	O
is	O
0.0074.	O
as	O
discussed	O
in	O
section	O
3.1.2	O
,	O
standard	O
formulas	O
can	O
be	O
used	O
to	O
compute	O
the	O
standard	O
errors	O
for	O
the	O
regression	B
coeﬃcients	O
in	O
a	O
linear	B
model	I
.	O
these	O
can	O
be	O
obtained	O
using	O
the	O
summary	O
(	O
)	O
function	B
.	O
>	O
summary	O
(	O
lm	O
(	O
mpg∼horsepower	O
,	O
data	B
=	O
auto	O
)	O
)	O
$	O
coef	O
estimate	O
std	O
.	O
error	B
t	O
value	O
(	O
intercept	B
)	O
horsepowe	O
r	O
39.936	O
-0.158	O
0.71750	O
0.00645	O
pr	O
(	O
>	O
|	O
t	O
|	O
)	O
55.7	O
1.22	O
e	O
-187	O
7.03	O
e	O
-81	O
-24.5	O
the	O
standard	B
error	I
estimates	O
for	O
ˆβ0	O
and	O
ˆβ1	O
obtained	O
using	O
the	O
formulas	O
from	O
section	O
3.1.2	O
are	O
0.717	O
for	O
the	O
intercept	B
and	O
0.0064	O
for	O
the	O
slope	B
.	O
interestingly	O
,	O
these	O
are	O
somewhat	O
diﬀerent	O
from	O
the	O
estimates	O
obtained	O
using	O
the	O
bootstrap	B
.	O
does	O
this	O
indicate	O
a	O
problem	O
with	O
the	O
bootstrap	B
?	O
in	O
fact	O
,	O
it	O
suggests	O
the	O
opposite	O
.	O
recall	B
that	O
the	O
standard	O
formulas	O
given	O
in	O
equation	O
3.8	O
on	O
page	O
66	O
rely	O
on	O
certain	O
assumptions	O
.	O
for	O
example	O
,	O
they	O
depend	O
on	O
the	O
unknown	O
parameter	B
σ2	O
,	O
the	O
noise	B
variance	O
.	O
we	O
then	O
estimate	O
σ2	O
using	O
the	O
rss	O
.	O
now	O
although	O
the	O
formula	O
for	O
the	O
standard	O
errors	O
do	O
not	O
rely	O
on	O
the	O
linear	B
model	I
being	O
correct	O
,	O
the	O
estimate	O
for	O
σ2	O
does	O
.	O
we	O
see	O
in	O
figure	O
3.8	O
on	O
page	O
91	O
that	O
there	O
is	O
a	O
non-linear	B
relationship	O
in	O
the	O
data	B
,	O
and	O
so	O
the	O
residuals	B
from	O
a	O
linear	B
ﬁt	O
will	O
be	O
inﬂated	O
,	O
and	O
so	O
will	O
ˆσ2	O
.	O
secondly	O
,	O
the	O
standard	O
formulas	O
assume	O
(	O
somewhat	O
unrealistically	O
)	O
that	O
the	O
xi	O
are	O
ﬁxed	O
,	O
and	O
all	O
the	O
variability	O
comes	O
from	O
the	O
variation	O
in	O
the	O
errors	O
i	O
.	O
the	O
bootstrap	B
approach	O
does	O
not	O
rely	O
on	O
any	O
of	O
these	O
assumptions	O
,	O
and	O
so	O
it	O
is	O
likely	O
giving	O
a	O
more	O
accurate	O
estimate	O
of	O
the	O
standard	O
errors	O
of	O
ˆβ0	O
and	O
ˆβ1	O
than	O
is	O
the	O
summary	O
(	O
)	O
function	B
.	O
below	O
we	O
compute	O
the	O
bootstrap	B
standard	O
error	B
estimates	O
and	O
the	O
stan-	O
dard	O
linear	B
regression	I
estimates	O
that	O
result	O
from	O
ﬁtting	O
the	O
quadratic	B
model	O
to	O
the	O
data	B
.	O
since	O
this	O
model	B
provides	O
a	O
good	O
ﬁt	B
to	O
the	O
data	B
(	O
figure	O
3.8	O
)	O
,	O
there	O
is	O
now	O
a	O
better	O
correspondence	O
between	O
the	O
bootstrap	B
estimates	O
and	O
the	O
standard	O
estimates	O
of	O
se	O
(	O
ˆβ0	O
)	O
,	O
se	O
(	O
ˆβ1	O
)	O
and	O
se	O
(	O
ˆβ2	O
)	O
.	O
5.4	O
exercises	O
197	O
>	O
boot	O
.	O
fn	O
=	O
function	B
(	O
data	B
,	O
index	O
)	O
+	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
(	O
lm	O
(	O
mpg∼horsepowe	O
r	O
+	O
i	O
(	O
horsepower	O
^2	O
)	O
,	O
data	B
=	O
data	B
,	O
subset	O
=	O
index	O
)	O
)	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
boot	O
(	O
auto	O
,	O
boot	O
.	O
fn	O
,1000	O
)	O
ordinary	O
n	O
o	O
n	O
p	O
a	O
r	O
a	O
m	O
e	O
t	O
r	O
i	O
c	O
bootstrap	B
call	O
:	O
boot	O
(	O
data	B
=	O
auto	O
,	O
statistic	O
=	O
boot	O
.	O
fn	O
,	O
r	O
=	O
1000	O
)	O
bootstrap	B
statistics	O
:	O
original	O
56.900	O
t1	O
*	O
t2	O
*	O
-0.466	O
0.001	O
t3	O
*	O
bias	B
std	O
.	O
error	B
6.098	O
e	O
-03	O
2.0945	O
-1.777	O
e	O
-04	O
0.0334	O
1.324	O
e	O
-06	O
0.0001	O
>	O
summary	O
(	O
lm	O
(	O
mpg∼horsepowe	O
r	O
+	O
i	O
(	O
horsepower	O
^2	O
)	O
,	O
data	B
=	O
auto	O
)	O
)	O
$	O
coef	O
(	O
intercept	B
)	O
horsepowe	O
r	O
i	O
(	O
horsepowe	O
r	O
^2	O
)	O
estimate	O
std	O
.	O
error	B
t	O
value	O
pr	O
(	O
>	O
|	O
t	O
|	O
)	O
32	O
1.7	O
e	O
-109	O
2.3	O
e	O
-40	O
2.2	O
e	O
-21	O
56.9001	O
-0.4662	O
0.0012	O
1.80043	O
0.03112	O
0.00012	O
-15	O
10	O
5.4	O
exercises	O
conceptual	O
1.	O
using	O
basic	O
statistical	O
properties	O
of	O
the	O
variance	B
,	O
as	O
well	O
as	O
single-	O
variable	B
calculus	O
,	O
derive	O
(	O
5.6	O
)	O
.	O
in	O
other	O
words	O
,	O
prove	O
that	O
α	O
given	O
by	O
(	O
5.6	O
)	O
does	O
indeed	O
minimize	O
var	O
(	O
αx	O
+	O
(	O
1	O
−	O
α	O
)	O
y	O
)	O
.	O
2.	O
we	O
will	O
now	O
derive	O
the	O
probability	B
that	O
a	O
given	O
observation	O
is	O
part	O
of	O
a	O
bootstrap	B
sample	O
.	O
suppose	O
that	O
we	O
obtain	O
a	O
bootstrap	B
sample	O
from	O
a	O
set	B
of	O
n	O
observations	B
.	O
(	O
a	O
)	O
what	O
is	O
the	O
probability	B
that	O
the	O
ﬁrst	O
bootstrap	B
observation	O
is	O
not	O
the	O
jth	O
observation	O
from	O
the	O
original	O
sample	O
?	O
justify	O
your	O
answer	O
.	O
(	O
b	O
)	O
what	O
is	O
the	O
probability	B
that	O
the	O
second	O
bootstrap	B
observation	O
(	O
c	O
)	O
argue	O
that	O
the	O
probability	B
that	O
the	O
jth	O
observation	O
is	O
not	O
in	O
the	O
is	O
not	O
the	O
jth	O
observation	O
from	O
the	O
original	O
sample	O
?	O
bootstrap	B
sample	O
is	O
(	O
1	O
−	O
1/n	O
)	O
n.	O
(	O
d	O
)	O
when	O
n	O
=	O
5	O
,	O
what	O
is	O
the	O
probability	B
that	O
the	O
jth	O
observation	O
is	O
in	O
the	O
bootstrap	B
sample	O
?	O
(	O
e	O
)	O
when	O
n	O
=	O
100	O
,	O
what	O
is	O
the	O
probability	B
that	O
the	O
jth	O
observation	O
is	O
in	O
the	O
bootstrap	B
sample	O
?	O
198	O
5.	O
resampling	B
methods	O
(	O
f	O
)	O
when	O
n	O
=	O
10	O
,	O
000	O
,	O
what	O
is	O
the	O
probability	B
that	O
the	O
jth	O
observa-	O
tion	O
is	O
in	O
the	O
bootstrap	B
sample	O
?	O
(	O
g	O
)	O
create	O
a	O
plot	B
that	O
displays	O
,	O
for	O
each	O
integer	O
value	O
of	O
n	O
from	O
1	O
to	O
100	O
,	O
000	O
,	O
the	O
probability	B
that	O
the	O
jth	O
observation	O
is	O
in	O
the	O
bootstrap	B
sample	O
.	O
comment	O
on	O
what	O
you	O
observe	O
.	O
(	O
h	O
)	O
we	O
will	O
now	O
investigate	O
numerically	O
the	O
probability	B
that	O
a	O
boot-	O
strap	O
sample	O
of	O
size	O
n	O
=	O
100	O
contains	O
the	O
jth	O
observation	O
.	O
here	O
j	O
=	O
4.	O
we	O
repeatedly	O
create	O
bootstrap	B
samples	O
,	O
and	O
each	O
time	O
we	O
record	O
whether	O
or	O
not	O
the	O
fourth	O
observation	O
is	O
contained	O
in	O
the	O
bootstrap	B
sample	O
.	O
>	O
store	O
=	O
rep	O
(	O
na	O
,	O
10000	O
)	O
>	O
for	O
(	O
i	O
in	O
1:10000	O
)	O
{	O
store	O
[	O
i	O
]	O
=	O
sum	O
(	O
sample	O
(	O
1:100	O
,	O
rep	O
=	O
true	O
)	O
==4	O
)	O
>	O
0	O
}	O
>	O
mean	O
(	O
store	O
)	O
comment	O
on	O
the	O
results	O
obtained	O
.	O
3.	O
we	O
now	O
review	O
k-fold	B
cross-validation	O
.	O
(	O
a	O
)	O
explain	O
how	O
k-fold	B
cross-validation	O
is	O
implemented	O
.	O
(	O
b	O
)	O
what	O
are	O
the	O
advantages	O
and	O
disadvantages	O
of	O
k-fold	B
cross-	O
validation	O
relative	O
to	O
:	O
i.	O
the	O
validation	B
set	I
approach	O
?	O
ii	O
.	O
loocv	O
?	O
4.	O
suppose	O
that	O
we	O
use	O
some	O
statistical	O
learning	O
method	O
to	O
make	O
a	O
pre-	O
diction	O
for	O
the	O
response	B
y	O
for	O
a	O
particular	O
value	O
of	O
the	O
predictor	B
x.	O
carefully	O
describe	O
how	O
we	O
might	O
estimate	O
the	O
standard	O
deviation	O
of	O
our	O
prediction	B
.	O
applied	O
5.	O
in	O
chapter	O
4	O
,	O
we	O
used	O
logistic	B
regression	I
to	O
predict	O
the	O
probability	B
of	O
default	O
using	O
income	O
and	O
balance	O
on	O
the	O
default	O
data	B
set	O
.	O
we	O
will	O
now	O
estimate	O
the	O
test	B
error	O
of	O
this	O
logistic	B
regression	I
model	O
using	O
the	O
validation	B
set	I
approach	O
.	O
do	O
not	O
forget	O
to	O
set	B
a	O
random	O
seed	O
before	O
beginning	O
your	O
analysis	B
.	O
(	O
a	O
)	O
fit	O
a	O
logistic	B
regression	I
model	O
that	O
uses	O
income	O
and	O
balance	O
to	O
predict	O
default	O
.	O
(	O
b	O
)	O
using	O
the	O
validation	B
set	I
approach	O
,	O
estimate	O
the	O
test	B
error	O
of	O
this	O
model	B
.	O
in	O
order	O
to	O
do	O
this	O
,	O
you	O
must	O
perform	O
the	O
following	O
steps	O
:	O
i.	O
split	O
the	O
sample	O
set	B
into	O
a	O
training	B
set	O
and	O
a	O
validation	B
set	I
.	O
5.4	O
exercises	O
199	O
ii	O
.	O
fit	O
a	O
multiple	B
logistic	O
regression	B
model	O
using	O
only	O
the	O
train-	O
ing	O
observations	B
.	O
iii	O
.	O
obtain	O
a	O
prediction	B
of	O
default	O
status	O
for	O
each	O
individual	O
in	O
the	O
validation	B
set	I
by	O
computing	O
the	O
posterior	B
probability	O
of	O
default	O
for	O
that	O
individual	O
,	O
and	O
classifying	O
the	O
individual	O
to	O
the	O
default	O
category	O
if	O
the	O
posterior	B
probability	O
is	O
greater	O
than	O
0.5.	O
iv	O
.	O
compute	O
the	O
validation	B
set	I
error	O
,	O
which	O
is	O
the	O
fraction	O
of	O
the	O
observations	B
in	O
the	O
validation	B
set	I
that	O
are	O
misclassiﬁed	O
.	O
(	O
c	O
)	O
repeat	O
the	O
process	O
in	O
(	O
b	O
)	O
three	O
times	O
,	O
using	O
three	O
diﬀerent	O
splits	O
of	O
the	O
observations	B
into	O
a	O
training	B
set	O
and	O
a	O
validation	B
set	I
.	O
com-	O
ment	O
on	O
the	O
results	O
obtained	O
.	O
(	O
d	O
)	O
now	O
consider	O
a	O
logistic	B
regression	I
model	O
that	O
predicts	O
the	O
prob-	O
ability	O
of	O
default	O
using	O
income	O
,	O
balance	O
,	O
and	O
a	O
dummy	B
variable	I
for	O
student	O
.	O
estimate	O
the	O
test	B
error	O
for	O
this	O
model	B
using	O
the	O
val-	O
idation	O
set	B
approach	O
.	O
comment	O
on	O
whether	O
or	O
not	O
including	O
a	O
dummy	B
variable	I
for	O
student	O
leads	O
to	O
a	O
reduction	O
in	O
the	O
test	B
error	O
rate	B
.	O
6.	O
we	O
continue	O
to	O
consider	O
the	O
use	O
of	O
a	O
logistic	B
regression	I
model	O
to	O
predict	O
the	O
probability	B
of	O
default	O
using	O
income	O
and	O
balance	O
on	O
the	O
default	O
data	B
set	O
.	O
in	O
particular	O
,	O
we	O
will	O
now	O
compute	O
estimates	O
for	O
the	O
standard	O
errors	O
of	O
the	O
income	O
and	O
balance	O
logistic	B
regression	I
co-	O
eﬃcients	O
in	O
two	O
diﬀerent	O
ways	O
:	O
(	O
1	O
)	O
using	O
the	O
bootstrap	B
,	O
and	O
(	O
2	O
)	O
using	O
the	O
standard	O
formula	O
for	O
computing	O
the	O
standard	O
errors	O
in	O
the	O
glm	O
(	O
)	O
function	B
.	O
do	O
not	O
forget	O
to	O
set	B
a	O
random	O
seed	O
before	O
beginning	O
your	O
analysis	B
.	O
(	O
a	O
)	O
using	O
the	O
summary	O
(	O
)	O
and	O
glm	O
(	O
)	O
functions	O
,	O
determine	O
the	O
esti-	O
mated	O
standard	O
errors	O
for	O
the	O
coeﬃcients	O
associated	O
with	O
income	O
and	O
balance	O
in	O
a	O
multiple	B
logistic	O
regression	B
model	O
that	O
uses	O
both	O
predictors	O
.	O
(	O
b	O
)	O
write	O
a	O
function	B
,	O
boot.fn	O
(	O
)	O
,	O
that	O
takes	O
as	O
input	B
the	O
default	O
data	B
set	O
as	O
well	O
as	O
an	O
index	O
of	O
the	O
observations	B
,	O
and	O
that	O
outputs	O
the	O
coeﬃcient	B
estimates	O
for	O
income	O
and	O
balance	O
in	O
the	O
multiple	B
logistic	O
regression	B
model	O
.	O
(	O
c	O
)	O
use	O
the	O
boot	O
(	O
)	O
function	B
together	O
with	O
your	O
boot.fn	O
(	O
)	O
function	B
to	O
estimate	O
the	O
standard	O
errors	O
of	O
the	O
logistic	B
regression	I
coeﬃcients	O
for	O
income	O
and	O
balance	O
.	O
(	O
d	O
)	O
comment	O
on	O
the	O
estimated	O
standard	O
errors	O
obtained	O
using	O
the	O
glm	O
(	O
)	O
function	B
and	O
using	O
your	O
bootstrap	B
function	O
.	O
7.	O
in	O
sections	O
5.3.2	O
and	O
5.3.3	O
,	O
we	O
saw	O
that	O
the	O
cv.glm	O
(	O
)	O
function	B
can	O
be	O
used	O
in	O
order	O
to	O
compute	O
the	O
loocv	O
test	B
error	O
estimate	O
.	O
alterna-	O
tively	O
,	O
one	O
could	O
compute	O
those	O
quantities	O
using	O
just	O
the	O
glm	O
(	O
)	O
and	O
200	O
5.	O
resampling	B
methods	O
predict.glm	O
(	O
)	O
functions	O
,	O
and	O
a	O
for	B
loop	I
.	O
you	O
will	O
now	O
take	O
this	O
ap-	O
proach	O
in	O
order	O
to	O
compute	O
the	O
loocv	O
error	B
for	O
a	O
simple	B
logistic	O
regression	B
model	O
on	O
the	O
weekly	O
data	B
set	O
.	O
recall	B
that	O
in	O
the	O
context	O
of	O
classiﬁcation	B
problems	O
,	O
the	O
loocv	O
error	B
is	O
given	O
in	O
(	O
5.4	O
)	O
.	O
(	O
a	O
)	O
fit	O
a	O
logistic	B
regression	I
model	O
that	O
predicts	O
direction	O
using	O
lag1	O
and	O
lag2	O
.	O
(	O
b	O
)	O
fit	O
a	O
logistic	B
regression	I
model	O
that	O
predicts	O
direction	O
using	O
lag1	O
and	O
lag2	O
using	O
all	O
but	O
the	O
ﬁrst	O
observation	O
.	O
(	O
c	O
)	O
use	O
the	O
model	B
from	O
(	O
b	O
)	O
to	O
predict	O
the	O
direction	O
of	O
the	O
ﬁrst	O
obser-	O
vation	O
.	O
you	O
can	O
do	O
this	O
by	O
predicting	O
that	O
the	O
ﬁrst	O
observation	O
will	O
go	O
up	O
if	O
p	O
(	O
direction=	O
''	O
up	O
''	O
|lag1	O
,	O
lag2	O
)	O
>	O
0.5.	O
was	O
this	O
ob-	O
servation	O
correctly	O
classiﬁed	O
?	O
(	O
d	O
)	O
write	O
a	O
for	B
loop	I
from	O
i	O
=	O
1	O
to	O
i	O
=	O
n	O
,	O
where	O
n	O
is	O
the	O
number	O
of	O
observations	B
in	O
the	O
data	B
set	O
,	O
that	O
performs	O
each	O
of	O
the	O
following	O
steps	O
:	O
i.	O
fit	O
a	O
logistic	B
regression	I
model	O
using	O
all	O
but	O
the	O
ith	O
obser-	O
vation	O
to	O
predict	O
direction	O
using	O
lag1	O
and	O
lag2	O
.	O
ii	O
.	O
compute	O
the	O
posterior	B
probability	O
of	O
the	O
market	O
moving	O
up	O
for	O
the	O
ith	O
observation	O
.	O
iii	O
.	O
use	O
the	O
posterior	B
probability	O
for	O
the	O
ith	O
observation	O
in	O
order	O
to	O
predict	O
whether	O
or	O
not	O
the	O
market	O
moves	O
up	O
.	O
iv	O
.	O
determine	O
whether	O
or	O
not	O
an	O
error	B
was	O
made	O
in	O
predicting	O
the	O
direction	O
for	O
the	O
ith	O
observation	O
.	O
if	O
an	O
error	B
was	O
made	O
,	O
then	O
indicate	O
this	O
as	O
a	O
1	O
,	O
and	O
otherwise	O
indicate	O
it	O
as	O
a	O
0	O
.	O
(	O
e	O
)	O
take	O
the	O
average	B
of	O
the	O
n	O
numbers	O
obtained	O
in	O
(	O
d	O
)	O
iv	O
in	O
order	O
to	O
obtain	O
the	O
loocv	O
estimate	O
for	O
the	O
test	B
error	O
.	O
comment	O
on	O
the	O
results	O
.	O
8.	O
we	O
will	O
now	O
perform	O
cross-validation	B
on	O
a	O
simulated	O
data	B
set	O
.	O
(	O
a	O
)	O
generate	O
a	O
simulated	O
data	B
set	O
as	O
follows	O
:	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
x	O
=	O
rnorm	O
(	O
100	O
)	O
>	O
y	O
=x	O
-2*	O
x	O
^2+	O
rnorm	O
(	O
100	O
)	O
in	O
this	O
data	B
set	O
,	O
what	O
is	O
n	O
and	O
what	O
is	O
p	O
?	O
write	O
out	O
the	O
model	B
used	O
to	O
generate	O
the	O
data	B
in	O
equation	O
form	O
.	O
(	O
b	O
)	O
create	O
a	O
scatterplot	B
of	O
x	O
against	O
y	O
.	O
comment	O
on	O
what	O
you	O
ﬁnd	O
.	O
(	O
c	O
)	O
set	B
a	O
random	O
seed	O
,	O
and	O
then	O
compute	O
the	O
loocv	O
errors	O
that	O
result	O
from	O
ﬁtting	O
the	O
following	O
four	O
models	O
using	O
least	B
squares	I
:	O
5.4	O
exercises	O
201	O
i.	O
y	O
=	O
β0	O
+	O
β1x	O
+	O
	O
ii	O
.	O
y	O
=	O
β0	O
+	O
β1x	O
+	O
β2x	O
2	O
+	O
	O
iii	O
.	O
y	O
=	O
β0	O
+	O
β1x	O
+	O
β2x	O
2	O
+	O
β3x	O
3	O
+	O
	O
iv	O
.	O
y	O
=	O
β0	O
+	O
β1x	O
+	O
β2x	O
2	O
+	O
β3x	O
3	O
+	O
β4x	O
4	O
+	O
	O
.	O
note	O
you	O
may	O
ﬁnd	O
it	O
helpful	O
to	O
use	O
the	O
data.frame	O
(	O
)	O
function	B
to	O
create	O
a	O
single	B
data	O
set	B
containing	O
both	O
x	O
and	O
y	O
.	O
(	O
d	O
)	O
repeat	O
(	O
c	O
)	O
using	O
another	O
random	O
seed	O
,	O
and	O
report	O
your	O
results	O
.	O
are	O
your	O
results	O
the	O
same	O
as	O
what	O
you	O
got	O
in	O
(	O
c	O
)	O
?	O
why	O
?	O
(	O
e	O
)	O
which	O
of	O
the	O
models	O
in	O
(	O
c	O
)	O
had	O
the	O
smallest	O
loocv	O
error	B
?	O
is	O
this	O
what	O
you	O
expected	O
?	O
explain	O
your	O
answer	O
.	O
(	O
f	O
)	O
comment	O
on	O
the	O
statistical	O
signiﬁcance	O
of	O
the	O
coeﬃcient	B
esti-	O
mates	O
that	O
results	O
from	O
ﬁtting	O
each	O
of	O
the	O
models	O
in	O
(	O
c	O
)	O
using	O
least	B
squares	I
.	O
do	O
these	O
results	O
agree	O
with	O
the	O
conclusions	O
drawn	O
based	O
on	O
the	O
cross-validation	B
results	O
?	O
9.	O
we	O
will	O
now	O
consider	O
the	O
boston	O
housing	O
data	B
set	O
,	O
from	O
the	O
mass	O
library	O
.	O
(	O
a	O
)	O
based	O
on	O
this	O
data	B
set	O
,	O
provide	O
an	O
estimate	O
for	O
the	O
population	O
mean	O
of	O
medv	O
.	O
call	O
this	O
estimate	O
ˆμ	O
.	O
(	O
b	O
)	O
provide	O
an	O
estimate	O
of	O
the	O
standard	B
error	I
of	O
ˆμ	O
.	O
interpret	O
this	O
result	O
.	O
hint	O
:	O
we	O
can	O
compute	O
the	O
standard	B
error	I
of	O
the	O
sample	O
mean	O
by	O
dividing	O
the	O
sample	O
standard	O
deviation	O
by	O
the	O
square	O
root	O
of	O
the	O
number	O
of	O
observations	B
.	O
(	O
c	O
)	O
now	O
estimate	O
the	O
standard	B
error	I
of	O
ˆμ	O
using	O
the	O
bootstrap	B
.	O
how	O
does	O
this	O
compare	O
to	O
your	O
answer	O
from	O
(	O
b	O
)	O
?	O
(	O
d	O
)	O
based	O
on	O
your	O
bootstrap	B
estimate	O
from	O
(	O
c	O
)	O
,	O
provide	O
a	O
95	O
%	O
con-	O
ﬁdence	O
interval	B
for	O
the	O
mean	O
of	O
medv	O
.	O
compare	O
it	O
to	O
the	O
results	O
obtained	O
using	O
t.test	O
(	O
boston	O
$	O
medv	O
)	O
.	O
hint	O
:	O
you	O
can	O
approximate	O
a	O
95	O
%	O
conﬁdence	B
interval	I
using	O
the	O
formula	O
[	O
ˆμ	O
−	O
2se	O
(	O
ˆμ	O
)	O
,	O
ˆμ	O
+	O
2se	O
(	O
ˆμ	O
)	O
]	O
.	O
(	O
e	O
)	O
based	O
on	O
this	O
data	B
set	O
,	O
provide	O
an	O
estimate	O
,	O
ˆμmed	O
,	O
for	O
the	O
median	O
value	O
of	O
medv	O
in	O
the	O
population	O
.	O
(	O
f	O
)	O
we	O
now	O
would	O
like	O
to	O
estimate	O
the	O
standard	B
error	I
of	O
ˆμmed	O
.	O
unfor-	O
tunately	O
,	O
there	O
is	O
no	O
simple	B
formula	O
for	O
computing	O
the	O
standard	B
error	I
of	O
the	O
median	O
.	O
instead	O
,	O
estimate	O
the	O
standard	B
error	I
of	O
the	O
median	O
using	O
the	O
bootstrap	B
.	O
comment	O
on	O
your	O
ﬁndings	O
.	O
(	O
g	O
)	O
based	O
on	O
this	O
data	B
set	O
,	O
provide	O
an	O
estimate	O
for	O
the	O
tenth	O
per-	O
centile	O
of	O
medv	O
in	O
boston	O
suburbs	O
.	O
call	O
this	O
quantity	O
ˆμ0.1	O
.	O
(	O
you	O
can	O
use	O
the	O
quantile	O
(	O
)	O
function	B
.	O
)	O
(	O
h	O
)	O
use	O
the	O
bootstrap	B
to	O
estimate	O
the	O
standard	B
error	I
of	O
ˆμ0.1	O
.	O
com-	O
ment	O
on	O
your	O
ﬁndings	O
.	O
6	O
linear	B
model	I
selection	O
and	O
regularization	B
in	O
the	O
regression	B
setting	O
,	O
the	O
standard	O
linear	O
model	B
y	O
=	O
β0	O
+	O
β1x1	O
+	O
···	O
+	O
βpxp	O
+	O
	O
(	O
6.1	O
)	O
is	O
commonly	O
used	O
to	O
describe	O
the	O
relationship	O
between	O
a	O
response	B
y	O
and	O
a	O
set	B
of	O
variables	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
.	O
we	O
have	O
seen	O
in	O
chapter	O
3	O
that	O
one	O
typically	O
ﬁts	O
this	O
model	B
using	O
least	B
squares	I
.	O
in	O
the	O
chapters	O
that	O
follow	O
,	O
we	O
consider	O
some	O
approaches	O
for	O
extending	O
the	O
linear	B
model	I
framework	O
.	O
in	O
chapter	O
7	O
we	O
generalize	O
(	O
6.1	O
)	O
in	O
order	O
to	O
accommodate	O
non-linear	B
,	O
but	O
still	O
additive	B
,	O
relationships	O
,	O
while	O
in	O
chap-	O
ter	O
8	O
we	O
consider	O
even	O
more	O
general	O
non-linear	B
models	O
.	O
however	O
,	O
the	O
linear	B
model	I
has	O
distinct	O
advantages	O
in	O
terms	O
of	O
inference	B
and	O
,	O
on	O
real-world	O
prob-	O
lems	O
,	O
is	O
often	O
surprisingly	O
competitive	O
in	O
relation	O
to	O
non-linear	B
methods	O
.	O
hence	O
,	O
before	O
moving	O
to	O
the	O
non-linear	B
world	O
,	O
we	O
discuss	O
in	O
this	O
chapter	O
some	O
ways	O
in	O
which	O
the	O
simple	B
linear	O
model	B
can	O
be	O
improved	O
,	O
by	O
replacing	O
plain	O
least	B
squares	I
ﬁtting	O
with	O
some	O
alternative	O
ﬁtting	O
procedures	O
.	O
why	O
might	O
we	O
want	O
to	O
use	O
another	O
ﬁtting	O
procedure	O
instead	O
of	O
least	B
squares	I
?	O
as	O
we	O
will	O
see	O
,	O
alternative	O
ﬁtting	O
procedures	O
can	O
yield	O
better	O
pre-	O
diction	O
accuracy	O
and	O
model	B
interpretability	O
.	O
•	O
prediction	B
accuracy	O
:	O
provided	O
that	O
the	O
true	O
relationship	O
between	O
the	O
response	B
and	O
the	O
predictors	O
is	O
approximately	O
linear	B
,	O
the	O
least	B
squares	I
estimates	O
will	O
have	O
low	O
bias	B
.	O
if	O
n	O
(	O
cid:10	O
)	O
p—that	O
is	O
,	O
if	O
n	O
,	O
the	O
number	O
of	O
observations	B
,	O
is	O
much	O
larger	O
than	O
p	O
,	O
the	O
number	O
of	O
variables—then	O
the	O
least	B
squares	I
estimates	O
tend	O
to	O
also	O
have	O
low	O
variance	B
,	O
and	O
hence	O
will	O
perform	O
well	O
on	O
test	B
observations	O
.	O
however	O
,	O
if	O
n	O
is	O
not	O
much	O
larger	O
g.	O
james	O
et	O
al.	O
,	O
an	O
introduction	O
to	O
statistical	O
learning	O
:	O
with	O
applications	O
in	O
r	O
,	O
springer	O
texts	O
in	O
statistics	O
,	O
doi	O
10.1007/978-1-4614-7138-7	O
6	O
,	O
©	O
springer	O
science+business	O
media	O
new	O
york	O
2013	O
203	O
204	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
than	O
p	O
,	O
then	O
there	O
can	O
be	O
a	O
lot	O
of	O
variability	O
in	O
the	O
least	B
squares	I
ﬁt	O
,	O
resulting	O
in	O
overﬁtting	B
and	O
consequently	O
poor	O
predictions	O
on	O
future	O
observations	B
not	O
used	O
in	O
model	B
training	O
.	O
and	O
if	O
p	O
>	O
n	O
,	O
then	O
there	O
is	O
no	O
longer	O
a	O
unique	O
least	B
squares	I
coeﬃcient	O
estimate	O
:	O
the	O
variance	B
is	O
inﬁnite	O
so	O
the	O
method	O
can	O
not	O
be	O
used	O
at	O
all	O
.	O
by	O
constraining	O
or	O
shrinking	O
the	O
estimated	O
coeﬃcients	O
,	O
we	O
can	O
often	O
substantially	O
reduce	O
the	O
variance	B
at	O
the	O
cost	O
of	O
a	O
negligible	O
increase	O
in	O
bias	B
.	O
this	O
can	O
lead	O
to	O
substantial	O
improvements	O
in	O
the	O
accuracy	O
with	O
which	O
we	O
can	O
predict	O
the	O
response	B
for	O
observations	B
not	O
used	O
in	O
model	B
training	O
.	O
•	O
model	B
interpretability	O
:	O
it	O
is	O
often	O
the	O
case	O
that	O
some	O
or	O
many	O
of	O
the	O
variables	O
used	O
in	O
a	O
multiple	B
regression	O
model	B
are	O
in	O
fact	O
not	O
associ-	O
ated	O
with	O
the	O
response	B
.	O
including	O
such	O
irrelevant	O
variables	O
leads	O
to	O
unnecessary	O
complexity	O
in	O
the	O
resulting	O
model	B
.	O
by	O
removing	O
these	O
variables—that	O
is	O
,	O
by	O
setting	O
the	O
corresponding	O
coeﬃcient	B
estimates	O
to	O
zero—we	O
can	O
obtain	O
a	O
model	B
that	O
is	O
more	O
easily	O
interpreted	O
.	O
now	O
least	B
squares	I
is	O
extremely	O
unlikely	O
to	O
yield	O
any	O
coeﬃcient	B
estimates	O
that	O
are	O
exactly	O
zero	O
.	O
in	O
this	O
chapter	O
,	O
we	O
see	O
some	O
approaches	O
for	O
au-	O
tomatically	O
performing	O
feature	B
selection	I
or	O
variable	B
selection—that	O
is	O
,	O
for	O
excluding	O
irrelevant	O
variables	O
from	O
a	O
multiple	B
regression	O
model	B
.	O
there	O
are	O
many	O
alternatives	O
,	O
both	O
classical	O
and	O
modern	O
,	O
to	O
using	O
least	B
squares	I
to	O
ﬁt	B
(	O
6.1	O
)	O
.	O
in	O
this	O
chapter	O
,	O
we	O
discuss	O
three	O
important	O
classes	O
of	O
methods	O
.	O
•	O
subset	B
selection	I
.	O
this	O
approach	B
involves	O
identifying	O
a	O
subset	O
of	O
the	O
p	O
predictors	O
that	O
we	O
believe	O
to	O
be	O
related	O
to	O
the	O
response	B
.	O
we	O
then	O
ﬁt	B
a	O
model	B
using	O
least	B
squares	I
on	O
the	O
reduced	O
set	B
of	O
variables	O
.	O
•	O
shrinkage	B
.	O
this	O
approach	B
involves	O
ﬁtting	O
a	O
model	B
involving	O
all	O
p	O
pre-	O
dictors	O
.	O
however	O
,	O
the	O
estimated	O
coeﬃcients	O
are	O
shrunken	O
towards	O
zero	O
relative	O
to	O
the	O
least	B
squares	I
estimates	O
.	O
this	O
shrinkage	B
(	O
also	O
known	O
as	O
regularization	B
)	O
has	O
the	O
eﬀect	O
of	O
reducing	O
variance	B
.	O
depending	O
on	O
what	O
type	O
of	O
shrinkage	B
is	O
performed	O
,	O
some	O
of	O
the	O
coeﬃcients	O
may	O
be	O
esti-	O
mated	O
to	O
be	O
exactly	O
zero	O
.	O
hence	O
,	O
shrinkage	B
methods	O
can	O
also	O
perform	O
variable	B
selection	O
.	O
•	O
dimension	B
reduction	I
.	O
this	O
approach	B
involves	O
projecting	O
the	O
p	O
predic-	O
tors	O
into	O
a	O
m	O
-dimensional	O
subspace	O
,	O
where	O
m	O
<	O
p.	O
this	O
is	O
achieved	O
by	O
computing	O
m	O
diﬀerent	O
linear	B
combinations	O
,	O
or	O
projections	O
,	O
of	O
the	O
variables	O
.	O
then	O
these	O
m	O
projections	O
are	O
used	O
as	O
predictors	O
to	O
ﬁt	B
a	O
linear	B
regression	I
model	O
by	O
least	B
squares	I
.	O
in	O
the	O
following	O
sections	O
we	O
describe	O
each	O
of	O
these	O
approaches	O
in	O
greater	O
de-	O
tail	O
,	O
along	O
with	O
their	O
advantages	O
and	O
disadvantages	O
.	O
although	O
this	O
chapter	O
describes	O
extensions	O
and	O
modiﬁcations	O
to	O
the	O
linear	B
model	I
for	O
regression	B
seen	O
in	O
chapter	O
3	O
,	O
the	O
same	O
concepts	O
apply	O
to	O
other	O
methods	O
,	O
such	O
as	O
the	O
classiﬁcation	B
models	O
seen	O
in	O
chapter	O
4.	O
feature	B
selection	I
variable	O
selection	B
6.1	O
subset	B
selection	I
205	O
6.1	O
subset	B
selection	I
in	O
this	O
section	O
we	O
consider	O
some	O
methods	O
for	O
selecting	O
subsets	O
of	O
predictors	O
.	O
these	O
include	O
best	O
subset	O
and	O
stepwise	B
model	I
selection	I
procedures	O
.	O
6.1.1	O
best	B
subset	I
selection	I
to	O
perform	O
best	B
subset	I
selection	I
,	O
we	O
ﬁt	B
a	O
separate	O
least	B
squares	I
regression	O
for	O
each	O
possible	O
combination	O
of	O
the	O
p	O
predictors	O
.	O
that	O
is	O
,	O
we	O
ﬁt	B
all	O
p	O
models	O
=	O
p	O
(	O
p−	O
1	O
)	O
/2	O
models	O
that	O
contain	O
that	O
contain	O
exactly	O
one	O
predictor	B
,	O
all	O
exactly	O
two	O
predictors	O
,	O
and	O
so	O
forth	O
.	O
we	O
then	O
look	O
at	O
all	O
of	O
the	O
resulting	O
models	O
,	O
with	O
the	O
goal	O
of	O
identifying	O
the	O
one	O
that	O
is	O
best	O
.	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
p	O
2	O
the	O
problem	O
of	O
selecting	O
the	O
best	O
model	O
from	O
among	O
the	O
2p	O
possibilities	O
considered	O
by	O
best	B
subset	I
selection	I
is	O
not	O
trivial	O
.	O
this	O
is	O
usually	O
broken	O
up	O
into	O
two	O
stages	O
,	O
as	O
described	O
in	O
algorithm	O
6.1.	O
algorithm	O
6.1	O
best	B
subset	I
selection	I
1.	O
let	O
m0	O
denote	O
the	O
null	B
model	O
,	O
which	O
contains	O
no	O
predictors	O
.	O
this	O
model	B
simply	O
predicts	O
the	O
sample	O
mean	O
for	O
each	O
observation	O
.	O
best	B
subset	I
selection	I
2.	O
for	O
k	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
p	O
:	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
p	O
k	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
(	O
a	O
)	O
fit	O
all	O
models	O
that	O
contain	O
exactly	O
k	O
predictors	O
.	O
(	O
b	O
)	O
pick	O
the	O
best	O
among	O
these	O
models	O
,	O
and	O
call	O
it	O
mk	O
.	O
here	O
best	O
is	O
deﬁned	O
as	O
having	O
the	O
smallest	O
rss	O
,	O
or	O
equivalently	O
largest	O
r2	O
.	O
3.	O
select	O
a	O
single	B
best	O
model	B
from	O
among	O
m0	O
,	O
.	O
.	O
.	O
,	O
mp	O
using	O
cross-	O
p	O
k	O
validated	O
prediction	B
error	O
,	O
cp	O
(	O
aic	O
)	O
,	O
bic	O
,	O
or	O
adjusted	O
r2	O
.	O
in	O
algorithm	O
6.1	O
,	O
step	O
2	O
identiﬁes	O
the	O
best	O
model	O
(	O
on	O
the	O
training	B
data	O
)	O
for	O
each	O
subset	O
size	O
,	O
in	O
order	O
to	O
reduce	O
the	O
problem	O
from	O
one	O
of	O
2p	O
possible	O
models	O
to	O
one	O
of	O
p	O
+	O
1	O
possible	O
models	O
.	O
in	O
figure	O
6.1	O
,	O
these	O
models	O
form	O
the	O
lower	O
frontier	O
depicted	O
in	O
red	O
.	O
now	O
in	O
order	O
to	O
select	O
a	O
single	B
best	O
model	B
,	O
we	O
must	O
simply	O
choose	O
among	O
these	O
p	O
+	O
1	O
options	O
.	O
this	O
task	O
must	O
be	O
performed	O
with	O
care	O
,	O
because	O
the	O
rss	O
of	O
these	O
p	O
+	O
1	O
models	O
decreases	O
monotonically	O
,	O
and	O
the	O
r2	O
increases	O
monotonically	O
,	O
as	O
the	O
number	O
of	O
features	O
included	O
in	O
the	O
models	O
increases	O
.	O
therefore	O
,	O
if	O
we	O
use	O
these	O
statistics	O
to	O
select	O
the	O
best	O
model	O
,	O
then	O
we	O
will	O
always	O
end	O
up	O
with	O
a	O
model	B
involving	O
all	O
of	O
the	O
variables	O
.	O
the	O
problem	O
is	O
that	O
a	O
low	O
rss	O
or	O
a	O
high	O
r2	O
indicates	O
a	O
model	B
with	O
a	O
low	O
training	B
error	O
,	O
whereas	O
we	O
wish	O
to	O
choose	O
a	O
model	B
that	O
has	O
a	O
low	O
test	B
error	O
.	O
(	O
as	O
shown	O
in	O
chapter	O
2	O
in	O
figures	O
2.9–2.11	O
,	O
training	B
error	O
tends	O
to	O
be	O
quite	O
a	O
bit	O
smaller	O
than	O
test	B
error	O
,	O
and	O
a	O
low	O
training	B
error	O
by	O
no	O
means	O
guarantees	O
a	O
low	O
test	B
error	O
.	O
)	O
therefore	O
,	O
in	O
step	O
3	O
,	O
we	O
use	O
cross-validated	O
prediction	B
206	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
s	O
e	O
r	O
a	O
u	O
q	O
s	O
f	O
o	O
m	O
u	O
s	O
l	O
i	O
a	O
u	O
d	O
s	O
e	O
r	O
7	O
0	O
+	O
e	O
8	O
7	O
0	O
+	O
e	O
6	O
7	O
0	O
+	O
e	O
4	O
7	O
0	O
+	O
e	O
2	O
2	O
r	O
0	O
.	O
1	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
2	O
4	O
6	O
8	O
10	O
2	O
4	O
6	O
8	O
10	O
number	O
of	O
predictors	O
number	O
of	O
predictors	O
figure	O
6.1.	O
for	O
each	O
possible	O
model	B
containing	O
a	O
subset	O
of	O
the	O
ten	O
predictors	O
in	O
the	O
credit	O
data	B
set	O
,	O
the	O
rss	O
and	O
r2	O
are	O
displayed	O
.	O
the	O
red	O
frontier	O
tracks	O
the	O
best	O
model	O
for	O
a	O
given	O
number	O
of	O
predictors	O
,	O
according	O
to	O
rss	O
and	O
r2	O
.	O
though	O
the	O
data	B
set	O
contains	O
only	O
ten	O
predictors	O
,	O
the	O
x-axis	O
ranges	O
from	O
1	O
to	O
11	O
,	O
since	O
one	O
of	O
the	O
variables	O
is	O
categorical	B
and	O
takes	O
on	O
three	O
values	O
,	O
leading	O
to	O
the	O
creation	O
of	O
two	O
dummy	B
variables	O
.	O
error	B
,	O
cp	O
,	O
bic	O
,	O
or	O
adjusted	O
r2	O
in	O
order	O
to	O
select	O
among	O
m0	O
,	O
m1	O
,	O
.	O
.	O
.	O
,	O
mp	O
.	O
these	O
approaches	O
are	O
discussed	O
in	O
section	O
6.1.3.	O
an	O
application	O
of	O
best	B
subset	I
selection	I
is	O
shown	O
in	O
figure	O
6.1.	O
each	O
plotted	O
point	O
corresponds	O
to	O
a	O
least	B
squares	I
regression	O
model	B
ﬁt	O
using	O
a	O
diﬀerent	O
subset	O
of	O
the	O
11	O
predictors	O
in	O
the	O
credit	O
data	B
set	O
,	O
discussed	O
in	O
chapter	O
3.	O
here	O
the	O
variable	B
ethnicity	O
is	O
a	O
three-level	O
qualitative	B
variable	O
,	O
and	O
so	O
is	O
represented	O
by	O
two	O
dummy	B
variables	O
,	O
which	O
are	O
selected	O
separately	O
in	O
this	O
case	O
.	O
we	O
have	O
plotted	O
the	O
rss	O
and	O
r2	O
statistics	O
for	O
each	O
model	B
,	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
variables	O
.	O
the	O
red	O
curves	O
connect	O
the	O
best	O
models	O
for	O
each	O
model	B
size	O
,	O
according	O
to	O
rss	O
or	O
r2	O
.	O
the	O
ﬁgure	O
shows	O
that	O
,	O
as	O
expected	O
,	O
these	O
quantities	O
improve	O
as	O
the	O
number	O
of	O
variables	O
increases	O
;	O
however	O
,	O
from	O
the	O
three-variable	O
model	B
on	O
,	O
there	O
is	O
little	O
improvement	O
in	O
rss	O
and	O
r2	O
as	O
a	O
result	O
of	O
including	O
additional	O
predictors	O
.	O
although	O
we	O
have	O
presented	O
best	B
subset	I
selection	I
here	O
for	O
least	O
squares	O
regression	B
,	O
the	O
same	O
ideas	O
apply	O
to	O
other	O
types	O
of	O
models	O
,	O
such	O
as	O
logistic	B
regression	I
.	O
in	O
the	O
case	O
of	O
logistic	B
regression	I
,	O
instead	O
of	O
ordering	O
models	O
by	O
rss	O
in	O
step	O
2	O
of	O
algorithm	O
6.1	O
,	O
we	O
instead	O
use	O
the	O
deviance	B
,	O
a	O
measure	O
that	O
plays	O
the	O
role	O
of	O
rss	O
for	O
a	O
broader	O
class	O
of	O
models	O
.	O
the	O
deviance	B
is	O
negative	O
two	O
times	O
the	O
maximized	O
log-likelihood	O
;	O
the	O
smaller	O
the	O
deviance	B
,	O
the	O
better	O
the	O
ﬁt	B
.	O
while	O
best	B
subset	I
selection	I
is	O
a	O
simple	B
and	O
conceptually	O
appealing	O
ap-	O
proach	O
,	O
it	O
suﬀers	O
from	O
computational	O
limitations	O
.	O
the	O
number	O
of	O
possible	O
models	O
that	O
must	O
be	O
considered	O
grows	O
rapidly	O
as	O
p	O
increases	O
.	O
in	O
general	O
,	O
there	O
are	O
2p	O
models	O
that	O
involve	O
subsets	O
of	O
p	O
predictors	O
.	O
so	O
if	O
p	O
=	O
10	O
,	O
then	O
there	O
are	O
approximately	O
1,000	O
possible	O
models	O
to	O
be	O
considered	O
,	O
and	O
if	O
deviance	B
6.1	O
subset	B
selection	I
207	O
p	O
=	O
20	O
,	O
then	O
there	O
are	O
over	O
one	O
million	O
possibilities	O
!	O
consequently	O
,	O
best	O
sub-	O
set	B
selection	O
becomes	O
computationally	O
infeasible	O
for	O
values	O
of	O
p	O
greater	O
than	O
around	O
40	O
,	O
even	O
with	O
extremely	O
fast	O
modern	O
computers	O
.	O
there	O
are	O
compu-	O
tational	O
shortcuts—so	O
called	O
branch-and-bound	O
techniques—for	O
eliminat-	O
ing	O
some	O
choices	O
,	O
but	O
these	O
have	O
their	O
limitations	O
as	O
p	O
gets	O
large	O
.	O
they	O
also	O
only	O
work	O
for	O
least	O
squares	O
linear	B
regression	I
.	O
we	O
present	O
computationally	O
eﬃcient	O
alternatives	O
to	O
best	B
subset	I
selection	I
next	O
.	O
6.1.2	O
stepwise	O
selection	O
for	O
computational	O
reasons	O
,	O
best	B
subset	I
selection	I
can	O
not	O
be	O
applied	O
with	O
very	O
large	O
p.	O
best	B
subset	I
selection	I
may	O
also	O
suﬀer	O
from	O
statistical	O
problems	O
when	O
p	O
is	O
large	O
.	O
the	O
larger	O
the	O
search	O
space	O
,	O
the	O
higher	O
the	O
chance	O
of	O
ﬁnding	O
models	O
that	O
look	O
good	O
on	O
the	O
training	B
data	O
,	O
even	O
though	O
they	O
might	O
not	O
have	O
any	O
predictive	O
power	B
on	O
future	O
data	B
.	O
thus	O
an	O
enormous	O
search	O
space	O
can	O
lead	O
to	O
overﬁtting	B
and	O
high	O
variance	B
of	O
the	O
coeﬃcient	B
estimates	O
.	O
for	O
both	O
of	O
these	O
reasons	O
,	O
stepwise	O
methods	O
,	O
which	O
explore	O
a	O
far	O
more	O
restricted	O
set	B
of	O
models	O
,	O
are	O
attractive	O
alternatives	O
to	O
best	B
subset	I
selection	I
.	O
forward	B
stepwise	I
selection	I
forward	O
stepwise	O
selection	O
is	O
a	O
computationally	O
eﬃcient	O
alternative	O
to	O
best	B
subset	I
selection	I
.	O
while	O
the	O
best	B
subset	I
selection	I
procedure	O
considers	O
all	O
2p	O
possible	O
models	O
containing	O
subsets	O
of	O
the	O
p	O
predictors	O
,	O
forward	O
step-	O
wise	O
considers	O
a	O
much	O
smaller	O
set	B
of	O
models	O
.	O
forward	B
stepwise	I
selection	I
begins	O
with	O
a	O
model	B
containing	O
no	O
predictors	O
,	O
and	O
then	O
adds	O
predictors	O
to	O
the	O
model	B
,	O
one-at-a-time	O
,	O
until	O
all	O
of	O
the	O
predictors	O
are	O
in	O
the	O
model	B
.	O
in	O
particular	O
,	O
at	O
each	O
step	O
the	O
variable	B
that	O
gives	O
the	O
greatest	O
additional	O
improvement	O
to	O
the	O
ﬁt	B
is	O
added	O
to	O
the	O
model	B
.	O
more	O
formally	O
,	O
the	O
forward	B
stepwise	I
selection	I
procedure	O
is	O
given	O
in	O
algorithm	O
6.2.	O
forward	B
stepwise	I
selection	I
algorithm	O
6.2	O
forward	B
stepwise	I
selection	I
1.	O
let	O
m0	O
denote	O
the	O
null	B
model	O
,	O
which	O
contains	O
no	O
predictors	O
.	O
2.	O
for	O
k	O
=	O
0	O
,	O
.	O
.	O
.	O
,	O
p	O
−	O
1	O
:	O
(	O
a	O
)	O
consider	O
all	O
p	O
−	O
k	O
models	O
that	O
augment	O
the	O
predictors	O
in	O
mk	O
(	O
b	O
)	O
choose	O
the	O
best	O
among	O
these	O
p	O
−	O
k	O
models	O
,	O
and	O
call	O
it	O
mk+1	O
.	O
with	O
one	O
additional	O
predictor	B
.	O
here	O
best	O
is	O
deﬁned	O
as	O
having	O
smallest	O
rss	O
or	O
highest	O
r2	O
.	O
3.	O
select	O
a	O
single	B
best	O
model	B
from	O
among	O
m0	O
,	O
.	O
.	O
.	O
,	O
mp	O
using	O
cross-	O
validated	O
prediction	B
error	O
,	O
cp	O
(	O
aic	O
)	O
,	O
bic	O
,	O
or	O
adjusted	O
r2	O
.	O
208	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
unlike	O
best	B
subset	I
selection	I
,	O
which	O
involved	O
ﬁtting	O
2p	O
models	O
,	O
forward	B
stepwise	I
selection	I
involves	O
ﬁtting	O
one	O
null	B
model	O
,	O
along	O
with	O
p	O
−	O
k	O
models	O
in	O
the	O
kth	O
iteration	O
,	O
for	O
k	O
=	O
0	O
,	O
.	O
.	O
.	O
,	O
p	O
−	O
1.	O
this	O
amounts	O
to	O
a	O
total	O
of	O
1	O
+	O
(	O
cid:10	O
)	O
k=0	O
(	O
p−	O
k	O
)	O
=	O
1	O
+	O
p	O
(	O
p	O
+	O
1	O
)	O
/2	O
models	O
.	O
this	O
is	O
a	O
substantial	O
diﬀerence	O
:	O
when	O
p−1	O
p	O
=	O
20	O
,	O
best	B
subset	I
selection	I
requires	O
ﬁtting	O
1,048,576	O
models	O
,	O
whereas	O
forward	B
stepwise	I
selection	I
requires	O
ﬁtting	O
only	O
211	O
models.1	O
among	O
those	O
p−	O
k	O
that	O
augment	O
mk	O
with	O
one	O
additional	O
predictor	B
.	O
we	O
can	O
in	O
step	O
2	O
(	O
b	O
)	O
of	O
algorithm	O
6.2	O
,	O
we	O
must	O
identify	O
the	O
best	O
model	O
from	O
do	O
this	O
by	O
simply	O
choosing	O
the	O
model	B
with	O
the	O
lowest	O
rss	O
or	O
the	O
highest	O
r2	O
.	O
however	O
,	O
in	O
step	O
3	O
,	O
we	O
must	O
identify	O
the	O
best	O
model	O
among	O
a	O
set	B
of	O
models	O
with	O
diﬀerent	O
numbers	O
of	O
variables	O
.	O
this	O
is	O
more	O
challenging	O
,	O
and	O
is	O
discussed	O
in	O
section	O
6.1.3.	O
forward	B
stepwise	I
selection	I
’	O
s	O
computational	O
advantage	O
over	O
best	B
subset	I
selection	I
is	O
clear	O
.	O
though	O
forward	O
stepwise	O
tends	O
to	O
do	O
well	O
in	O
practice	O
,	O
it	O
is	O
not	O
guaranteed	O
to	O
ﬁnd	O
the	O
best	O
possible	O
model	B
out	O
of	O
all	O
2p	O
mod-	O
els	O
containing	O
subsets	O
of	O
the	O
p	O
predictors	O
.	O
for	O
instance	O
,	O
suppose	O
that	O
in	O
a	O
given	O
data	B
set	O
with	O
p	O
=	O
3	O
predictors	O
,	O
the	O
best	O
possible	O
one-variable	O
model	B
contains	O
x1	O
,	O
and	O
the	O
best	O
possible	O
two-variable	O
model	B
instead	O
contains	O
x2	B
and	O
x3	O
.	O
then	O
forward	B
stepwise	I
selection	I
will	O
fail	O
to	O
select	O
the	O
best	O
possible	O
two-variable	O
model	B
,	O
because	O
m1	O
will	O
contain	O
x1	O
,	O
so	O
m2	O
must	O
also	O
contain	O
x1	O
together	O
with	O
one	O
additional	O
variable	B
.	O
table	O
6.1	O
,	O
which	O
shows	O
the	O
ﬁrst	O
four	O
selected	O
models	O
for	O
best	O
subset	O
and	O
forward	B
stepwise	I
selection	I
on	O
the	O
credit	O
data	B
set	O
,	O
illustrates	O
this	O
phe-	O
nomenon	O
.	O
both	O
best	B
subset	I
selection	I
and	O
forward	B
stepwise	I
selection	I
choose	O
rating	O
for	O
the	O
best	O
one-variable	O
model	B
and	O
then	O
include	O
income	O
and	O
student	O
for	O
the	O
two-	O
and	O
three-variable	O
models	O
.	O
however	O
,	O
best	B
subset	I
selection	I
re-	O
places	O
rating	O
by	O
cards	O
in	O
the	O
four-variable	O
model	B
,	O
while	O
forward	B
stepwise	I
selection	I
must	O
maintain	O
rating	O
in	O
its	O
four-variable	O
model	B
.	O
in	O
this	O
example	O
,	O
figure	O
6.1	O
indicates	O
that	O
there	O
is	O
not	O
much	O
diﬀerence	O
between	O
the	O
three-	O
and	O
four-variable	O
models	O
in	O
terms	O
of	O
rss	O
,	O
so	O
either	O
of	O
the	O
four-variable	O
models	O
will	O
likely	O
be	O
adequate	O
.	O
forward	B
stepwise	I
selection	I
can	O
be	O
applied	O
even	O
in	O
the	O
high-dimensional	B
setting	O
where	O
n	O
<	O
p	O
;	O
however	O
,	O
in	O
this	O
case	O
,	O
it	O
is	O
possible	O
to	O
construct	O
sub-	O
models	O
m0	O
,	O
.	O
.	O
.	O
,	O
mn−1	O
only	O
,	O
since	O
each	O
submodel	O
is	O
ﬁt	B
using	O
least	B
squares	I
,	O
which	O
will	O
not	O
yield	O
a	O
unique	O
solution	O
if	O
p	O
≥	O
n.	O
backward	B
stepwise	I
selection	I
like	O
forward	B
stepwise	I
selection	I
,	O
backward	B
stepwise	I
selection	I
provides	O
an	O
eﬃcient	O
alternative	O
to	O
best	B
subset	I
selection	I
.	O
however	O
,	O
unlike	O
forward	O
backward	O
stepwise	O
selection	O
1though	O
forward	B
stepwise	I
selection	I
considers	O
p	O
(	O
p	O
+	O
1	O
)	O
/2	O
+	O
1	O
models	O
,	O
it	O
performs	O
a	O
guided	O
search	O
over	O
model	B
space	O
,	O
and	O
so	O
the	O
eﬀective	O
model	O
space	O
considered	O
contains	O
substantially	O
more	O
than	O
p	O
(	O
p	O
+	O
1	O
)	O
/2	O
+	O
1	O
models	O
.	O
6.1	O
subset	B
selection	I
209	O
#	O
variables	O
best	O
subset	O
one	O
two	O
three	O
four	O
rating	O
rating	O
,	O
income	O
rating	O
,	O
income	O
,	O
student	O
cards	O
,	O
income	O
,	O
student	O
,	O
limit	O
forward	O
stepwise	O
rating	O
rating	O
,	O
income	O
rating	O
,	O
income	O
,	O
student	O
rating	O
,	O
income	O
,	O
student	O
,	O
limit	O
table	O
6.1.	O
the	O
ﬁrst	O
four	O
selected	O
models	O
for	O
best	O
subset	B
selection	I
and	O
forward	B
stepwise	I
selection	I
on	O
the	O
credit	O
data	B
set	O
.	O
the	O
ﬁrst	O
three	O
models	O
are	O
identical	O
but	O
the	O
fourth	O
models	O
diﬀer	O
.	O
stepwise	O
selection	O
,	O
it	O
begins	O
with	O
the	O
full	O
least	B
squares	I
model	O
containing	O
all	O
p	O
predictors	O
,	O
and	O
then	O
iteratively	O
removes	O
the	O
least	O
useful	O
predictor	B
,	O
one-at-a-time	O
.	O
details	O
are	O
given	O
in	O
algorithm	O
6.3.	O
algorithm	O
6.3	O
backward	B
stepwise	I
selection	I
1.	O
let	O
mp	O
denote	O
the	O
full	O
model	B
,	O
which	O
contains	O
all	O
p	O
predictors	O
.	O
2.	O
for	O
k	O
=	O
p	O
,	O
p	O
−	O
1	O
,	O
.	O
.	O
.	O
,	O
1	O
:	O
(	O
a	O
)	O
consider	O
all	O
k	O
models	O
that	O
contain	O
all	O
but	O
one	O
of	O
the	O
predictors	O
in	O
mk	O
,	O
for	O
a	O
total	O
of	O
k	O
−	O
1	O
predictors	O
.	O
(	O
b	O
)	O
choose	O
the	O
best	O
among	O
these	O
k	O
models	O
,	O
and	O
call	O
it	O
mk−1	O
.	O
here	O
best	O
is	O
deﬁned	O
as	O
having	O
smallest	O
rss	O
or	O
highest	O
r2	O
.	O
3.	O
select	O
a	O
single	B
best	O
model	B
from	O
among	O
m0	O
,	O
.	O
.	O
.	O
,	O
mp	O
using	O
cross-	O
validated	O
prediction	B
error	O
,	O
cp	O
(	O
aic	O
)	O
,	O
bic	O
,	O
or	O
adjusted	O
r2	O
.	O
like	O
forward	B
stepwise	I
selection	I
,	O
the	O
backward	O
selection	O
approach	B
searches	O
through	O
only	O
1	O
+	O
p	O
(	O
p	O
+	O
1	O
)	O
/2	O
models	O
,	O
and	O
so	O
can	O
be	O
applied	O
in	O
settings	O
where	O
p	O
is	O
too	O
large	O
to	O
apply	O
best	O
subset	O
selection.2	O
also	O
like	O
forward	B
stepwise	I
selection	I
,	O
backward	B
stepwise	I
selection	I
is	O
not	O
guaranteed	O
to	O
yield	O
the	O
best	O
model	O
containing	O
a	O
subset	O
of	O
the	O
p	O
predictors	O
.	O
backward	O
selection	O
requires	O
that	O
the	O
number	O
of	O
samples	O
n	O
is	O
larger	O
than	O
the	O
number	O
of	O
variables	O
p	O
(	O
so	O
that	O
the	O
full	O
model	B
can	O
be	O
ﬁt	B
)	O
.	O
in	O
contrast	B
,	O
forward	O
stepwise	O
can	O
be	O
used	O
even	O
when	O
n	O
<	O
p	O
,	O
and	O
so	O
is	O
the	O
only	O
viable	O
subset	O
method	O
when	O
p	O
is	O
very	O
large	O
.	O
2like	O
forward	B
stepwise	I
selection	I
,	O
backward	B
stepwise	I
selection	I
performs	O
a	O
guided	O
search	O
over	O
model	B
space	O
,	O
and	O
so	O
eﬀectively	O
considers	O
substantially	O
more	O
than	O
1+p	O
(	O
p+1	O
)	O
/2	O
models	O
.	O
210	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
hybrid	O
approaches	O
the	O
best	O
subset	O
,	O
forward	O
stepwise	O
,	O
and	O
backward	B
stepwise	I
selection	I
ap-	O
proaches	O
generally	O
give	O
similar	O
but	O
not	O
identical	O
models	O
.	O
as	O
another	O
al-	O
ternative	O
,	O
hybrid	O
versions	O
of	O
forward	O
and	O
backward	B
stepwise	I
selection	I
are	O
available	O
,	O
in	O
which	O
variables	O
are	O
added	O
to	O
the	O
model	B
sequentially	O
,	O
in	O
analogy	O
to	O
forward	O
selection	O
.	O
however	O
,	O
after	O
adding	O
each	O
new	O
variable	B
,	O
the	O
method	O
may	O
also	O
remove	O
any	O
variables	O
that	O
no	O
longer	O
provide	O
an	O
improvement	O
in	O
the	O
model	B
ﬁt	O
.	O
such	O
an	O
approach	B
attempts	O
to	O
more	O
closely	O
mimic	O
best	O
sub-	O
set	B
selection	O
while	O
retaining	O
the	O
computational	O
advantages	O
of	O
forward	O
and	O
backward	B
stepwise	I
selection	I
.	O
6.1.3	O
choosing	O
the	O
optimal	O
model	O
best	B
subset	I
selection	I
,	O
forward	O
selection	O
,	O
and	O
backward	O
selection	O
result	O
in	O
the	O
creation	O
of	O
a	O
set	B
of	O
models	O
,	O
each	O
of	O
which	O
contains	O
a	O
subset	O
of	O
the	O
p	O
pre-	O
dictors	O
.	O
in	O
order	O
to	O
implement	O
these	O
methods	O
,	O
we	O
need	O
a	O
way	O
to	O
determine	O
which	O
of	O
these	O
models	O
is	O
best	O
.	O
as	O
we	O
discussed	O
in	O
section	O
6.1.1	O
,	O
the	O
model	B
containing	O
all	O
of	O
the	O
predictors	O
will	O
always	O
have	O
the	O
smallest	O
rss	O
and	O
the	O
largest	O
r2	O
,	O
since	O
these	O
quantities	O
are	O
related	O
to	O
the	O
training	B
error	O
.	O
instead	O
,	O
we	O
wish	O
to	O
choose	O
a	O
model	B
with	O
a	O
low	O
test	B
error	O
.	O
as	O
is	O
evident	O
here	O
,	O
and	O
as	O
we	O
show	O
in	O
chapter	O
2	O
,	O
the	O
training	B
error	O
can	O
be	O
a	O
poor	O
estimate	O
of	O
the	O
test	B
error	O
.	O
therefore	O
,	O
rss	O
and	O
r2	O
are	O
not	O
suitable	O
for	O
selecting	O
the	O
best	O
model	O
among	O
a	O
collection	O
of	O
models	O
with	O
diﬀerent	O
numbers	O
of	O
predictors	O
.	O
in	O
order	O
to	O
select	O
the	O
best	O
model	O
with	O
respect	O
to	O
test	B
error	O
,	O
we	O
need	O
to	O
estimate	O
this	O
test	B
error	O
.	O
there	O
are	O
two	O
common	O
approaches	O
:	O
1.	O
we	O
can	O
indirectly	O
estimate	O
test	B
error	O
by	O
making	O
an	O
adjustment	O
to	O
the	O
training	B
error	O
to	O
account	O
for	O
the	O
bias	B
due	O
to	O
overﬁtting	B
.	O
2.	O
we	O
can	O
directly	O
estimate	O
the	O
test	B
error	O
,	O
using	O
either	O
a	O
validation	B
set	I
approach	O
or	O
a	O
cross-validation	B
approach	O
,	O
as	O
discussed	O
in	O
chapter	O
5.	O
we	O
consider	O
both	O
of	O
these	O
approaches	O
below	O
.	O
cp	O
,	O
aic	O
,	O
bic	O
,	O
and	O
adjusted	O
r2	O
we	O
show	O
in	O
chapter	O
2	O
that	O
the	O
training	B
set	O
mse	O
is	O
generally	O
an	O
under-	O
estimate	O
of	O
the	O
test	B
mse	O
.	O
(	O
recall	B
that	O
mse	O
=	O
rss/n	O
.	O
)	O
this	O
is	O
because	O
when	O
we	O
ﬁt	B
a	O
model	B
to	O
the	O
training	B
data	O
using	O
least	B
squares	I
,	O
we	O
speciﬁ-	O
cally	O
estimate	O
the	O
regression	B
coeﬃcients	O
such	O
that	O
the	O
training	B
rss	O
(	O
but	O
not	O
the	O
test	B
rss	O
)	O
is	O
as	O
small	O
as	O
possible	O
.	O
in	O
particular	O
,	O
the	O
training	B
error	O
will	O
decrease	O
as	O
more	O
variables	O
are	O
included	O
in	O
the	O
model	B
,	O
but	O
the	O
test	B
error	O
may	O
not	O
.	O
therefore	O
,	O
training	B
set	O
rss	O
and	O
training	B
set	O
r2	O
can	O
not	O
be	O
used	O
to	O
select	O
from	O
among	O
a	O
set	B
of	O
models	O
with	O
diﬀerent	O
numbers	O
of	O
variables	O
.	O
however	O
,	O
a	O
number	O
of	O
techniques	O
for	O
adjusting	O
the	O
training	B
error	O
for	O
the	O
model	B
size	O
are	O
available	O
.	O
these	O
approaches	O
can	O
be	O
used	O
to	O
select	O
among	O
a	O
set	B
6.1	O
subset	B
selection	I
211	O
p	O
c	O
0	O
0	O
0	O
0	O
3	O
0	O
0	O
0	O
5	O
2	O
0	O
0	O
0	O
0	O
2	O
0	O
0	O
0	O
5	O
1	O
0	O
0	O
0	O
0	O
1	O
c	O
b	O
i	O
0	O
0	O
0	O
0	O
3	O
0	O
0	O
0	O
5	O
2	O
0	O
0	O
0	O
0	O
2	O
0	O
0	O
0	O
5	O
1	O
0	O
0	O
0	O
0	O
1	O
6	O
9	O
.	O
0	O
4	O
9	O
0	O
.	O
2	O
9	O
.	O
0	O
0	O
9	O
.	O
0	O
8	O
8	O
.	O
0	O
6	O
8	O
.	O
0	O
2	O
r	O
d	O
e	O
t	O
s	O
u	O
d	O
a	O
j	O
2	O
4	O
6	O
8	O
10	O
2	O
4	O
6	O
8	O
10	O
2	O
4	O
6	O
8	O
10	O
number	O
of	O
predictors	O
number	O
of	O
predictors	O
number	O
of	O
predictors	O
figure	O
6.2.	O
cp	O
,	O
bic	O
,	O
and	O
adjusted	O
r2	O
are	O
shown	O
for	O
the	O
best	O
models	O
of	O
each	O
size	O
for	O
the	O
credit	O
data	B
set	O
(	O
the	O
lower	O
frontier	O
in	O
figure	O
6.1	O
)	O
.	O
cp	O
and	O
bic	O
are	O
estimates	O
of	O
test	B
mse	O
.	O
in	O
the	O
middle	O
plot	B
we	O
see	O
that	O
the	O
bic	O
estimate	O
of	O
test	B
error	O
shows	O
an	O
increase	O
after	O
four	O
variables	O
are	O
selected	O
.	O
the	O
other	O
two	O
plots	O
are	O
rather	O
ﬂat	O
after	O
four	O
variables	O
are	O
included	O
.	O
of	O
models	O
with	O
diﬀerent	O
numbers	O
of	O
variables	O
.	O
we	O
now	O
consider	O
four	O
such	O
approaches	O
:	O
cp	O
,	O
akaike	O
information	O
criterion	O
(	O
aic	O
)	O
,	O
bayesian	O
information	O
criterion	O
(	O
bic	O
)	O
,	O
and	O
adjusted	O
r2	O
.	O
figure	O
6.2	O
displays	O
cp	O
,	O
bic	O
,	O
and	O
adjusted	O
r2	O
for	O
the	O
best	O
model	O
of	O
each	O
size	O
produced	O
by	O
best	B
subset	I
selection	I
on	O
the	O
credit	O
data	B
set	O
.	O
for	O
a	O
ﬁtted	O
least	O
squares	O
model	B
containing	O
d	O
predictors	O
,	O
the	O
cp	O
estimate	O
of	O
test	B
mse	O
is	O
computed	O
using	O
the	O
equation	O
(	O
cid:8	O
)	O
rss	O
+	O
2dˆσ2	O
(	O
cid:9	O
)	O
,	O
(	O
6.2	O
)	O
cp	O
=	O
1	O
n	O
ˆσ2	O
where	O
ˆσ2	O
is	O
an	O
estimate	O
of	O
the	O
variance	B
of	O
the	O
error	B
	O
associated	O
with	O
each	O
response	B
measurement	O
in	O
(	O
6.1	O
)	O
.3	O
typically	O
is	O
estimated	O
using	O
the	O
full	O
essentially	O
,	O
the	O
cp	O
statistic	O
adds	O
a	O
penalty	B
model	O
containing	O
all	O
predictors	O
.	O
of	O
2dˆσ2	O
to	O
the	O
training	B
rss	O
in	O
order	O
to	O
adjust	O
for	O
the	O
fact	O
that	O
the	O
training	B
error	O
tends	O
to	O
underestimate	O
the	O
test	B
error	O
.	O
clearly	O
,	O
the	O
penalty	B
increases	O
as	O
the	O
number	O
of	O
predictors	O
in	O
the	O
model	B
increases	O
;	O
this	O
is	O
intended	O
to	O
adjust	O
for	O
the	O
corresponding	O
decrease	O
in	O
training	B
rss	O
.	O
though	O
it	O
is	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
,	O
one	O
can	O
show	O
that	O
if	O
ˆσ2	O
is	O
an	O
unbiased	O
estimate	O
of	O
σ2	O
in	O
(	O
6.2	O
)	O
,	O
then	O
cp	O
is	O
an	O
unbiased	O
estimate	O
of	O
test	B
mse	O
.	O
as	O
a	O
consequence	O
,	O
the	O
cp	O
statistic	O
tends	O
to	O
take	O
on	O
a	O
small	O
value	O
for	O
models	O
with	O
a	O
low	O
test	B
error	O
,	O
so	O
when	O
determining	O
which	O
of	O
a	O
set	B
of	O
models	O
is	O
best	O
,	O
we	O
choose	O
the	O
model	B
with	O
the	O
lowest	O
cp	O
value	O
.	O
in	O
figure	O
6.2	O
,	O
cp	O
selects	O
the	O
six-variable	O
model	B
containing	O
the	O
predictors	O
income	O
,	O
limit	O
,	O
rating	O
,	O
cards	O
,	O
age	O
and	O
student	O
.	O
3mallow	O
’	O
s	O
cp	O
is	O
sometimes	O
deﬁned	O
as	O
c	O
the	O
deﬁnition	O
given	O
above	O
in	O
the	O
sense	O
that	O
cp	O
=	O
1	O
smallest	O
cp	O
also	O
has	O
smallest	O
c	O
(	O
cid:3	O
)	O
p.	O
p	O
=	O
rss/ˆσ2	O
+	O
2d	O
−	O
n.	O
this	O
is	O
equivalent	O
to	O
(	O
cid:3	O
)	O
(	O
cid:3	O
)	O
p	O
+	O
n	O
)	O
,	O
and	O
so	O
the	O
model	B
with	O
n	O
ˆσ2	O
(	O
c	O
cp	O
akaike	O
information	O
criterion	O
bayesian	O
information	O
criterion	O
adjusted	O
r2	O
212	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
the	O
aic	O
criterion	O
is	O
deﬁned	O
for	O
a	O
large	O
class	O
of	O
models	O
ﬁt	B
by	O
maximum	B
likelihood	I
.	O
in	O
the	O
case	O
of	O
the	O
model	B
(	O
6.1	O
)	O
with	O
gaussian	O
errors	O
,	O
maximum	B
likelihood	I
and	O
least	B
squares	I
are	O
the	O
same	O
thing	O
.	O
in	O
this	O
case	O
aic	O
is	O
given	O
by	O
(	O
cid:8	O
)	O
rss	O
+	O
2dˆσ2	O
(	O
cid:9	O
)	O
,	O
aic	O
=	O
1	O
nˆσ2	O
where	O
,	O
for	O
simplicity	O
,	O
we	O
have	O
omitted	O
an	O
additive	B
constant	O
.	O
hence	O
for	O
least	O
squares	O
models	O
,	O
cp	O
and	O
aic	O
are	O
proportional	O
to	O
each	O
other	O
,	O
and	O
so	O
only	O
cp	O
is	O
displayed	O
in	O
figure	O
6.2.	O
bic	O
is	O
derived	O
from	O
a	O
bayesian	O
point	O
of	O
view	O
,	O
but	O
ends	O
up	O
looking	O
similar	O
to	O
cp	O
(	O
and	O
aic	O
)	O
as	O
well	O
.	O
for	O
the	O
least	B
squares	I
model	O
with	O
d	O
predictors	O
,	O
the	O
bic	O
is	O
,	O
up	O
to	O
irrelevant	O
constants	O
,	O
given	O
by	O
(	O
cid:8	O
)	O
rss	O
+	O
log	O
(	O
n	O
)	O
dˆσ2	O
(	O
cid:9	O
)	O
bic	O
=	O
1	O
nˆσ2	O
.	O
(	O
6.3	O
)	O
like	O
cp	O
,	O
the	O
bic	O
will	O
tend	O
to	O
take	O
on	O
a	O
small	O
value	O
for	O
a	O
model	B
with	O
a	O
low	O
test	B
error	O
,	O
and	O
so	O
generally	O
we	O
select	O
the	O
model	B
that	O
has	O
the	O
lowest	O
bic	O
value	O
.	O
notice	O
that	O
bic	O
replaces	O
the	O
2dˆσ2	O
used	O
by	O
cp	O
with	O
a	O
log	O
(	O
n	O
)	O
dˆσ2	O
term	B
,	O
where	O
n	O
is	O
the	O
number	O
of	O
observations	B
.	O
since	O
log	O
n	O
>	O
2	O
for	O
any	O
n	O
>	O
7	O
,	O
the	O
bic	O
statistic	O
generally	O
places	O
a	O
heavier	O
penalty	B
on	O
models	O
with	O
many	O
variables	O
,	O
and	O
hence	O
results	O
in	O
the	O
selection	B
of	O
smaller	O
models	O
than	O
cp	O
.	O
in	O
figure	O
6.2	O
,	O
we	O
see	O
that	O
this	O
is	O
indeed	O
the	O
case	O
for	O
the	O
credit	O
data	B
set	O
;	O
bic	O
chooses	O
a	O
model	B
that	O
contains	O
only	O
the	O
four	O
predictors	O
income	O
,	O
limit	O
,	O
cards	O
,	O
and	O
student	O
.	O
in	O
this	O
case	O
the	O
curves	O
are	O
very	O
ﬂat	O
and	O
so	O
there	O
does	O
not	O
appear	O
to	O
be	O
much	O
diﬀerence	O
in	O
accuracy	O
between	O
the	O
four-variable	O
and	O
six-variable	O
models	O
.	O
the	O
adjusted	O
r2	O
statistic	O
is	O
another	O
popular	O
approach	B
for	O
selecting	O
among	O
a	O
set	B
of	O
models	O
that	O
contain	O
diﬀerent	O
numbers	O
of	O
variables	O
.	O
recall	B
from	O
chapter	O
3	O
that	O
the	O
usual	O
r2	O
is	O
deﬁned	O
as	O
1	O
−	O
rss/tss	O
,	O
where	O
tss	O
=	O
(	O
cid:10	O
)	O
(	O
yi	O
−	O
y	O
)	O
2	O
is	O
the	O
total	B
sum	I
of	I
squares	I
for	O
the	O
response	B
.	O
since	O
rss	O
always	O
decreases	O
as	O
more	O
variables	O
are	O
added	O
to	O
the	O
model	B
,	O
the	O
r2	O
always	O
increases	O
as	O
more	O
variables	O
are	O
added	O
.	O
for	O
a	O
least	B
squares	I
model	O
with	O
d	O
variables	O
,	O
the	O
adjusted	O
r2	O
statistic	O
is	O
calculated	O
as	O
adjusted	O
r2	O
=	O
1	O
−	O
rss/	O
(	O
n	O
−	O
d	O
−	O
1	O
)	O
tss/	O
(	O
n	O
−	O
1	O
)	O
.	O
(	O
6.4	O
)	O
unlike	O
cp	O
,	O
aic	O
,	O
and	O
bic	O
,	O
for	O
which	O
a	O
small	O
value	O
indicates	O
a	O
model	B
with	O
a	O
low	O
test	B
error	O
,	O
a	O
large	O
value	O
of	O
adjusted	O
r2	O
indicates	O
a	O
model	B
with	O
a	O
small	O
test	B
error	O
.	O
maximizing	O
the	O
adjusted	O
r2	O
is	O
equivalent	O
to	O
minimizing	O
rss	O
n−d−1	O
.	O
while	O
rss	O
always	O
decreases	O
as	O
the	O
number	O
of	O
variables	O
in	O
the	O
model	B
increases	O
,	O
rss	O
n−d−1	O
may	O
increase	O
or	O
decrease	O
,	O
due	O
to	O
the	O
presence	O
of	O
d	O
in	O
the	O
denominator	O
.	O
the	O
intuition	O
behind	O
the	O
adjusted	O
r2	O
is	O
that	O
once	O
all	O
of	O
the	O
correct	O
variables	O
have	O
been	O
included	O
in	O
the	O
model	B
,	O
adding	O
additional	O
noise	B
variables	O
6.1	O
subset	B
selection	I
213	O
will	O
lead	O
to	O
only	O
a	O
very	O
small	O
decrease	O
in	O
rss	O
.	O
since	O
adding	O
noise	B
variables	O
leads	O
to	O
an	O
increase	O
in	O
d	O
,	O
such	O
variables	O
will	O
lead	O
to	O
an	O
increase	O
in	O
rss	O
n−d−1	O
,	O
and	O
consequently	O
a	O
decrease	O
in	O
the	O
adjusted	O
r2	O
.	O
therefore	O
,	O
in	O
theory	O
,	O
the	O
model	B
with	O
the	O
largest	O
adjusted	O
r2	O
will	O
have	O
only	O
correct	O
variables	O
and	O
no	O
noise	B
variables	O
.	O
unlike	O
the	O
r2	O
statistic	O
,	O
the	O
adjusted	O
r2	O
statistic	O
pays	O
a	O
price	O
for	O
the	O
inclusion	O
of	O
unnecessary	O
variables	O
in	O
the	O
model	B
.	O
figure	O
6.2	O
displays	O
the	O
adjusted	O
r2	O
for	O
the	O
credit	O
data	B
set	O
.	O
using	O
this	O
statistic	O
results	O
in	O
the	O
selection	B
of	O
a	O
model	B
that	O
contains	O
seven	O
variables	O
,	O
adding	O
gender	O
to	O
the	O
model	B
selected	O
by	O
cp	O
and	O
aic	O
.	O
cp	O
,	O
aic	O
,	O
and	O
bic	O
all	O
have	O
rigorous	O
theoretical	O
justiﬁcations	O
that	O
are	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
these	O
justiﬁcations	O
rely	O
on	O
asymptotic	O
ar-	O
guments	O
(	O
scenarios	O
where	O
the	O
sample	O
size	O
n	O
is	O
very	O
large	O
)	O
.	O
despite	O
its	O
pop-	O
ularity	O
,	O
and	O
even	O
though	O
it	O
is	O
quite	O
intuitive	O
,	O
the	O
adjusted	O
r2	O
is	O
not	O
as	O
well	O
motivated	O
in	O
statistical	O
theory	O
as	O
aic	O
,	O
bic	O
,	O
and	O
cp	O
.	O
all	O
of	O
these	O
measures	O
are	O
simple	B
to	O
use	O
and	O
compute	O
.	O
here	O
we	O
have	O
presented	O
the	O
formulas	O
for	O
aic	O
,	O
bic	O
,	O
and	O
cp	O
in	O
the	O
case	O
of	O
a	O
linear	B
model	I
ﬁt	O
using	O
least	B
squares	I
;	O
however	O
,	O
these	O
quantities	O
can	O
also	O
be	O
deﬁned	O
for	O
more	O
general	O
types	O
of	O
models	O
.	O
validation	O
and	O
cross-validation	B
as	O
an	O
alternative	O
to	O
the	O
approaches	O
just	O
discussed	O
,	O
we	O
can	O
directly	O
esti-	O
mate	O
the	O
test	B
error	O
using	O
the	O
validation	B
set	I
and	O
cross-validation	B
methods	O
discussed	O
in	O
chapter	O
5.	O
we	O
can	O
compute	O
the	O
validation	B
set	I
error	O
or	O
the	O
cross-validation	B
error	O
for	O
each	O
model	B
under	O
consideration	O
,	O
and	O
then	O
select	O
the	O
model	B
for	O
which	O
the	O
resulting	O
estimated	O
test	B
error	O
is	O
smallest	O
.	O
this	O
pro-	O
cedure	O
has	O
an	O
advantage	O
relative	O
to	O
aic	O
,	O
bic	O
,	O
cp	O
,	O
and	O
adjusted	O
r2	O
,	O
in	O
that	O
it	O
provides	O
a	O
direct	O
estimate	O
of	O
the	O
test	B
error	O
,	O
and	O
makes	O
fewer	O
assumptions	O
about	O
the	O
true	O
underlying	O
model	B
.	O
it	O
can	O
also	O
be	O
used	O
in	O
a	O
wider	O
range	O
of	O
model	B
selection	I
tasks	O
,	O
even	O
in	O
cases	O
where	O
it	O
is	O
hard	O
to	O
pinpoint	O
the	O
model	B
degrees	O
of	O
freedom	O
(	O
e.g	O
.	O
the	O
number	O
of	O
predictors	O
in	O
the	O
model	B
)	O
or	O
hard	O
to	O
estimate	O
the	O
error	B
variance	O
σ2	O
.	O
in	O
the	O
past	O
,	O
performing	O
cross-validation	B
was	O
computationally	O
prohibitive	O
for	O
many	O
problems	O
with	O
large	O
p	O
and/or	O
large	O
n	O
,	O
and	O
so	O
aic	O
,	O
bic	O
,	O
cp	O
,	O
and	O
adjusted	O
r2	O
were	O
more	O
attractive	O
approaches	O
for	O
choosing	O
among	O
a	O
set	B
of	O
models	O
.	O
however	O
,	O
nowadays	O
with	O
fast	O
computers	O
,	O
the	O
computations	O
required	O
to	O
perform	O
cross-validation	B
are	O
hardly	O
ever	O
an	O
issue	O
.	O
thus	O
,	O
cross-	O
validation	O
is	O
a	O
very	O
attractive	O
approach	B
for	O
selecting	O
from	O
among	O
a	O
number	O
of	O
models	O
under	O
consideration	O
.	O
figure	O
6.3	O
displays	O
,	O
as	O
a	O
function	B
of	O
d	O
,	O
the	O
bic	O
,	O
validation	B
set	I
errors	O
,	O
and	O
cross-validation	B
errors	O
on	O
the	O
credit	O
data	B
,	O
for	O
the	O
best	O
d-variable	O
model	B
.	O
the	O
validation	O
errors	O
were	O
calculated	O
by	O
randomly	O
selecting	O
three-quarters	O
of	O
the	O
observations	B
as	O
the	O
training	B
set	O
,	O
and	O
the	O
remainder	O
as	O
the	O
valida-	O
tion	O
set	B
.	O
the	O
cross-validation	B
errors	O
were	O
computed	O
using	O
k	O
=	O
10	O
folds	O
.	O
in	O
this	O
case	O
,	O
the	O
validation	O
and	O
cross-validation	B
methods	O
both	O
result	O
in	O
a	O
214	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
c	O
b	O
i	O
f	O
o	O
t	O
o	O
o	O
r	O
e	O
r	O
a	O
u	O
q	O
s	O
0	O
2	O
2	O
0	O
0	O
2	O
0	O
8	O
1	O
0	O
6	O
1	O
0	O
4	O
1	O
0	O
2	O
1	O
0	O
0	O
1	O
r	O
o	O
r	O
r	O
e	O
t	O
e	O
s	O
n	O
o	O
i	O
t	O
a	O
d	O
i	O
l	O
a	O
v	O
0	O
2	O
2	O
0	O
0	O
2	O
0	O
8	O
1	O
0	O
6	O
1	O
0	O
4	O
1	O
0	O
2	O
1	O
0	O
0	O
1	O
4	O
2	O
10	O
number	O
of	O
predictors	O
6	O
8	O
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
d	O
i	O
l	O
a	O
v	O
−	O
s	O
s	O
o	O
r	O
c	O
0	O
2	O
2	O
0	O
0	O
2	O
0	O
8	O
1	O
0	O
6	O
1	O
0	O
4	O
1	O
0	O
2	O
1	O
0	O
0	O
1	O
4	O
2	O
10	O
number	O
of	O
predictors	O
6	O
8	O
4	O
2	O
10	O
number	O
of	O
predictors	O
6	O
8	O
figure	O
6.3.	O
for	O
the	O
credit	O
data	B
set	O
,	O
three	O
quantities	O
are	O
displayed	O
for	O
the	O
best	O
model	O
containing	O
d	O
predictors	O
,	O
for	O
d	O
ranging	O
from	O
1	O
to	O
11.	O
the	O
overall	O
best	O
model	O
,	O
based	O
on	O
each	O
of	O
these	O
quantities	O
,	O
is	O
shown	O
as	O
a	O
blue	O
cross	O
.	O
left	O
:	O
square	O
root	O
of	O
bic	O
.	O
center	O
:	O
validation	B
set	I
errors	O
.	O
right	O
:	O
cross-validation	B
errors	O
.	O
six-variable	O
model	B
.	O
however	O
,	O
all	O
three	O
approaches	O
suggest	O
that	O
the	O
four-	O
,	O
ﬁve-	O
,	O
and	O
six-variable	O
models	O
are	O
roughly	O
equivalent	O
in	O
terms	O
of	O
their	O
test	B
errors	O
.	O
in	O
fact	O
,	O
the	O
estimated	O
test	B
error	O
curves	O
displayed	O
in	O
the	O
center	O
and	O
right-	O
hand	O
panels	O
of	O
figure	O
6.3	O
are	O
quite	O
ﬂat	O
.	O
while	O
a	O
three-variable	O
model	B
clearly	O
has	O
lower	O
estimated	O
test	B
error	O
than	O
a	O
two-variable	O
model	B
,	O
the	O
estimated	O
test	B
errors	O
of	O
the	O
3-	O
to	O
11-variable	O
models	O
are	O
quite	O
similar	O
.	O
furthermore	O
,	O
if	O
we	O
repeated	O
the	O
validation	B
set	I
approach	O
using	O
a	O
diﬀerent	O
split	O
of	O
the	O
data	B
into	O
a	O
training	B
set	O
and	O
a	O
validation	B
set	I
,	O
or	O
if	O
we	O
repeated	O
cross-validation	B
using	O
a	O
diﬀerent	O
set	B
of	O
cross-validation	B
folds	O
,	O
then	O
the	O
precise	O
model	B
with	O
the	O
lowest	O
estimated	O
test	B
error	O
would	O
surely	O
change	O
.	O
in	O
this	O
setting	O
,	O
we	O
can	O
select	O
a	O
model	B
using	O
the	O
one-standard-error	B
rule	I
.	O
we	O
ﬁrst	O
calculate	O
the	O
one-	O
standard	B
error	I
of	O
the	O
estimated	O
test	B
mse	O
for	O
each	O
model	B
size	O
,	O
and	O
then	O
select	O
the	O
smallest	O
model	B
for	O
which	O
the	O
estimated	O
test	B
error	O
is	O
within	O
one	O
standard	B
error	I
of	O
the	O
lowest	O
point	O
on	O
the	O
curve	O
.	O
the	O
rationale	O
here	O
is	O
that	O
if	O
a	O
set	B
of	O
models	O
appear	O
to	O
be	O
more	O
or	O
less	O
equally	O
good	O
,	O
then	O
we	O
might	O
as	O
well	O
choose	O
the	O
simplest	O
model—that	O
is	O
,	O
the	O
model	B
with	O
the	O
smallest	O
number	O
of	O
predictors	O
.	O
in	O
this	O
case	O
,	O
applying	O
the	O
one-standard-error	B
rule	I
to	O
the	O
validation	B
set	I
or	O
cross-validation	B
approach	O
leads	O
to	O
selection	B
of	O
the	O
three-variable	O
model	B
.	O
standard-	O
error	B
rule	O
6.2	O
shrinkage	B
methods	O
the	O
subset	B
selection	I
methods	O
described	O
in	O
section	O
6.1	O
involve	O
using	O
least	B
squares	I
to	O
ﬁt	B
a	O
linear	B
model	I
that	O
contains	O
a	O
subset	O
of	O
the	O
predictors	O
.	O
as	O
an	O
alternative	O
,	O
we	O
can	O
ﬁt	B
a	O
model	B
containing	O
all	O
p	O
predictors	O
using	O
a	O
technique	O
that	O
constrains	O
or	O
regularizes	O
the	O
coeﬃcient	B
estimates	O
,	O
or	O
equivalently	O
,	O
that	O
shrinks	O
the	O
coeﬃcient	B
estimates	O
towards	O
zero	O
.	O
it	O
may	O
not	O
be	O
immediately	O
6.2	O
shrinkage	B
methods	O
215	O
obvious	O
why	O
such	O
a	O
constraint	O
should	O
improve	O
the	O
ﬁt	B
,	O
but	O
it	O
turns	O
out	O
that	O
shrinking	O
the	O
coeﬃcient	B
estimates	O
can	O
signiﬁcantly	O
reduce	O
their	O
variance	B
.	O
the	O
two	O
best-known	O
techniques	O
for	O
shrinking	O
the	O
regression	B
coeﬃcients	O
towards	O
zero	O
are	O
ridge	B
regression	I
and	O
the	O
lasso	B
.	O
6.2.1	O
ridge	B
regression	I
recall	O
from	O
chapter	O
3	O
that	O
the	O
least	B
squares	I
ﬁtting	O
procedure	O
estimates	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
using	O
the	O
values	O
that	O
minimize	O
rss	O
=	O
n	O
(	O
cid:17	O
)	O
i=1	O
⎛	O
⎝yi	O
−	O
β0	O
−	O
p	O
(	O
cid:17	O
)	O
j=1	O
⎞	O
⎠2	O
βjxij	O
.	O
ridge	B
regression	I
is	O
very	O
similar	O
to	O
least	B
squares	I
,	O
except	O
that	O
the	O
coeﬃcients	O
are	O
estimated	O
by	O
minimizing	O
a	O
slightly	O
diﬀerent	O
quantity	O
.	O
in	O
particular	O
,	O
the	O
ridge	B
regression	I
coeﬃcient	O
estimates	O
ˆβr	O
are	O
the	O
values	O
that	O
minimize	O
⎛	O
⎝yi	O
−	O
β0	O
−	O
p	O
(	O
cid:17	O
)	O
n	O
(	O
cid:17	O
)	O
i=1	O
βjxij	O
j=1	O
⎞	O
⎠2	O
p	O
(	O
cid:17	O
)	O
j=1	O
+	O
λ	O
β2	O
j	O
=	O
rss	O
+	O
λ	O
p	O
(	O
cid:17	O
)	O
j=1	O
β2	O
j	O
,	O
(	O
6.5	O
)	O
(	O
cid:10	O
)	O
j	O
β2	O
where	O
λ	O
≥	O
0	O
is	O
a	O
tuning	B
parameter	I
,	O
to	O
be	O
determined	O
separately	O
.	O
equa-	O
tion	O
6.5	O
trades	O
oﬀ	O
two	O
diﬀerent	O
criteria	O
.	O
as	O
with	O
least	B
squares	I
,	O
ridge	O
regres-	O
sion	O
seeks	O
coeﬃcient	B
estimates	O
that	O
ﬁt	B
the	O
data	B
well	O
,	O
by	O
making	O
the	O
rss	O
small	O
.	O
however	O
,	O
the	O
second	O
term	B
,	O
λ	O
j	O
,	O
called	O
a	O
shrinkage	B
penalty	O
,	O
is	O
small	O
when	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
are	O
close	O
to	O
zero	O
,	O
and	O
so	O
it	O
has	O
the	O
eﬀect	O
of	O
shrinking	O
the	O
estimates	O
of	O
βj	O
towards	O
zero	O
.	O
the	O
tuning	B
parameter	I
λ	O
serves	O
to	O
control	O
the	O
relative	O
impact	O
of	O
these	O
two	O
terms	O
on	O
the	O
regression	B
coeﬃcient	O
esti-	O
mates	O
.	O
when	O
λ	O
=	O
0	O
,	O
the	O
penalty	B
term	O
has	O
no	O
eﬀect	O
,	O
and	O
ridge	B
regression	I
will	O
produce	O
the	O
least	B
squares	I
estimates	O
.	O
however	O
,	O
as	O
λ	O
→	O
∞	O
,	O
the	O
impact	O
of	O
the	O
shrinkage	B
penalty	O
grows	O
,	O
and	O
the	O
ridge	B
regression	I
coeﬃcient	O
estimates	O
will	O
approach	B
zero	O
.	O
unlike	O
least	B
squares	I
,	O
which	O
generates	O
only	O
one	O
set	B
of	O
co-	O
eﬃcient	O
estimates	O
,	O
ridge	B
regression	I
will	O
produce	O
a	O
diﬀerent	O
set	B
of	O
coeﬃcient	B
estimates	O
,	O
ˆβr	O
λ	O
,	O
for	O
each	O
value	O
of	O
λ.	O
selecting	O
a	O
good	O
value	O
for	O
λ	O
is	O
critical	O
;	O
we	O
defer	O
this	O
discussion	O
to	O
section	O
6.2.3	O
,	O
where	O
we	O
use	O
cross-validation	B
.	O
note	O
that	O
in	O
(	O
6.5	O
)	O
,	O
the	O
shrinkage	B
penalty	O
is	O
applied	O
to	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
,	O
but	O
not	O
to	O
the	O
intercept	B
β0	O
.	O
we	O
want	O
to	O
shrink	O
the	O
estimated	O
association	O
of	O
each	O
variable	B
with	O
the	O
response	B
;	O
however	O
,	O
we	O
do	O
not	O
want	O
to	O
shrink	O
the	O
intercept	B
,	O
which	O
is	O
simply	O
a	O
measure	O
of	O
the	O
mean	O
value	O
of	O
the	O
response	B
when	O
xi1	O
=	O
xi2	O
=	O
.	O
.	O
.	O
=	O
xip	O
=	O
0.	O
if	O
we	O
assume	O
that	O
the	O
variables—that	O
is	O
,	O
the	O
columns	O
of	O
the	O
data	B
matrix	O
x—have	O
been	O
centered	O
to	O
have	O
mean	O
zero	O
before	O
ridge	B
regression	I
is	O
performed	O
,	O
then	O
the	O
estimated	O
intercept	B
will	O
take	O
the	O
form	O
ˆβ0	O
=	O
¯y	O
=	O
(	O
cid:10	O
)	O
n	O
i=1	O
yi/n	O
.	O
ridge	B
regression	I
tuning	O
parameter	B
shrinkage	O
penalty	B
216	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
i	O
s	O
t	O
n	O
e	O
c	O
i	O
f	O
f	O
e	O
o	O
c	O
d	O
e	O
z	O
d	O
r	O
a	O
d	O
n	O
a	O
t	O
s	O
i	O
income	O
limit	O
rating	O
student	O
0	O
0	O
4	O
0	O
0	O
3	O
0	O
0	O
2	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
−	O
0	O
0	O
3	O
−	O
i	O
s	O
t	O
n	O
e	O
c	O
i	O
f	O
f	O
e	O
o	O
c	O
d	O
e	O
z	O
d	O
r	O
a	O
d	O
n	O
a	O
t	O
s	O
i	O
0	O
0	O
4	O
0	O
0	O
3	O
0	O
0	O
2	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
−	O
0	O
0	O
3	O
−	O
1e−02	O
1e+00	O
1e+02	O
λ	O
1e+04	O
0.0	O
0.2	O
0.6	O
0.4	O
λ	O
2/	O
ˆβ	O
2	O
ˆβr	O
0.8	O
1.0	O
figure	O
6.4.	O
the	O
standardized	O
ridge	B
regression	I
coeﬃcients	O
are	O
displayed	O
for	O
the	O
credit	O
data	B
set	O
,	O
as	O
a	O
function	B
of	O
λ	O
and	O
(	O
cid:4	O
)	O
ˆβr	O
λ	O
(	O
cid:4	O
)	O
2/	O
(	O
cid:4	O
)	O
ˆβ	O
(	O
cid:4	O
)	O
2.	O
an	O
application	O
to	O
the	O
credit	O
data	B
in	O
figure	O
6.4	O
,	O
the	O
ridge	B
regression	I
coeﬃcient	O
estimates	O
for	O
the	O
credit	O
data	B
set	O
are	O
displayed	O
.	O
in	O
the	O
left-hand	O
panel	O
,	O
each	O
curve	O
corresponds	O
to	O
the	O
ridge	B
regression	I
coeﬃcient	O
estimate	O
for	O
one	O
of	O
the	O
ten	O
variables	O
,	O
plotted	O
as	O
a	O
function	B
of	O
λ.	O
for	O
example	O
,	O
the	O
black	O
solid	O
line	B
represents	O
the	O
ridge	B
regression	I
estimate	O
for	O
the	O
income	O
coeﬃcient	B
,	O
as	O
λ	O
is	O
varied	O
.	O
at	O
the	O
extreme	O
left-hand	O
side	O
of	O
the	O
plot	B
,	O
λ	O
is	O
essentially	O
zero	O
,	O
and	O
so	O
the	O
corresponding	O
ridge	O
coeﬃcient	O
estimates	O
are	O
the	O
same	O
as	O
the	O
usual	O
least	B
squares	I
esti-	O
mates	O
.	O
but	O
as	O
λ	O
increases	O
,	O
the	O
ridge	O
coeﬃcient	O
estimates	O
shrink	O
towards	O
zero	O
.	O
when	O
λ	O
is	O
extremely	O
large	O
,	O
then	O
all	O
of	O
the	O
ridge	O
coeﬃcient	O
estimates	O
are	O
basically	O
zero	O
;	O
this	O
corresponds	O
to	O
the	O
null	B
model	O
that	O
contains	O
no	O
pre-	O
dictors	O
.	O
in	O
this	O
plot	B
,	O
the	O
income	O
,	O
limit	O
,	O
rating	O
,	O
and	O
student	O
variables	O
are	O
displayed	O
in	O
distinct	O
colors	O
,	O
since	O
these	O
variables	O
tend	O
to	O
have	O
by	O
far	O
the	O
largest	O
coeﬃcient	B
estimates	O
.	O
while	O
the	O
ridge	O
coeﬃcient	O
estimates	O
tend	O
to	O
decrease	O
in	O
aggregate	O
as	O
λ	O
increases	O
,	O
individual	O
coeﬃcients	O
,	O
such	O
as	O
rating	O
and	O
income	O
,	O
may	O
occasionally	O
increase	O
as	O
λ	O
increases	O
.	O
λ	O
λ	O
the	O
right-hand	O
panel	O
of	O
figure	O
6.4	O
displays	O
the	O
same	O
ridge	O
coeﬃcient	O
estimates	O
as	O
the	O
left-hand	O
panel	O
,	O
but	O
instead	O
of	O
displaying	O
λ	O
on	O
the	O
x-axis	O
,	O
(	O
cid:12	O
)	O
2/	O
(	O
cid:12	O
)	O
ˆβ	O
(	O
cid:12	O
)	O
2	O
,	O
where	O
ˆβ	O
denotes	O
the	O
vector	B
of	O
least	B
squares	I
we	O
now	O
display	O
(	O
cid:12	O
)	O
ˆβr	O
coeﬃcient	B
estimates	O
.	O
the	O
notation	O
(	O
cid:12	O
)	O
β	O
(	O
cid:12	O
)	O
2	O
denotes	O
the	O
(	O
cid:6	O
)	O
2	O
norm	O
(	O
pronounced	O
“	O
ell	O
2	O
”	O
)	O
of	O
a	O
vector	B
,	O
and	O
is	O
deﬁned	O
as	O
(	O
cid:12	O
)	O
β	O
(	O
cid:12	O
)	O
2	O
=	O
2.	O
it	O
measures	O
the	O
distance	B
of	O
β	O
from	O
zero	O
.	O
as	O
λ	O
increases	O
,	O
the	O
(	O
cid:6	O
)	O
2	O
norm	O
of	O
ˆβr	O
decrease	O
,	O
and	O
so	O
will	O
(	O
cid:12	O
)	O
ˆβr	O
λ	O
=	O
0	O
,	O
in	O
which	O
case	O
the	O
ridge	B
regression	I
coeﬃcient	O
estimate	O
is	O
the	O
same	O
as	O
the	O
least	B
squares	I
estimate	O
,	O
and	O
so	O
their	O
(	O
cid:6	O
)	O
2	O
norms	O
are	O
the	O
same	O
)	O
to	O
0	O
(	O
when	O
λ	O
=	O
∞	O
,	O
in	O
which	O
case	O
the	O
ridge	B
regression	I
coeﬃcient	O
estimate	O
is	O
a	O
vector	B
of	O
zeros	O
,	O
with	O
(	O
cid:6	O
)	O
2	O
norm	O
equal	O
to	O
zero	O
)	O
.	O
therefore	O
,	O
we	O
can	O
think	O
of	O
the	O
x-axis	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
6.4	O
as	O
the	O
amount	O
that	O
the	O
ridge	O
λ	O
will	O
always	O
(	O
cid:12	O
)	O
2/	O
(	O
cid:12	O
)	O
ˆβ	O
(	O
cid:12	O
)	O
2.	O
the	O
latter	O
quantity	O
ranges	O
from	O
1	O
(	O
when	O
’	O
(	O
cid:10	O
)	O
p	O
j=1	O
βj	O
(	O
cid:5	O
)	O
2	O
norm	O
6.2	O
shrinkage	B
methods	O
217	O
regression	B
coeﬃcient	O
estimates	O
have	O
been	O
shrunken	O
towards	O
zero	O
;	O
a	O
small	O
value	O
indicates	O
that	O
they	O
have	O
been	O
shrunken	O
very	O
close	O
to	O
zero	O
.	O
the	O
standard	O
least	O
squares	O
coeﬃcient	B
estimates	O
discussed	O
in	O
chapter	O
3	O
are	O
scale	B
equivariant	I
:	O
multiplying	O
xj	O
by	O
a	O
constant	O
c	O
simply	O
leads	O
to	O
a	O
scaling	O
of	O
the	O
least	B
squares	I
coeﬃcient	O
estimates	O
by	O
a	O
factor	B
of	O
1/c	O
.	O
in	O
other	O
words	O
,	O
regardless	O
of	O
how	O
the	O
jth	O
predictor	B
is	O
scaled	O
,	O
xj	O
ˆβj	O
will	O
remain	O
the	O
same	O
.	O
in	O
contrast	B
,	O
the	O
ridge	B
regression	I
coeﬃcient	O
estimates	O
can	O
change	O
sub-	O
stantially	O
when	O
multiplying	O
a	O
given	O
predictor	B
by	O
a	O
constant	O
.	O
for	O
instance	O
,	O
consider	O
the	O
income	O
variable	B
,	O
which	O
is	O
measured	O
in	O
dollars	O
.	O
one	O
could	O
rea-	O
sonably	O
have	O
measured	O
income	O
in	O
thousands	O
of	O
dollars	O
,	O
which	O
would	O
result	O
in	O
a	O
reduction	O
in	O
the	O
observed	O
values	O
of	O
income	O
by	O
a	O
factor	B
of	O
1,000.	O
now	O
due	O
to	O
the	O
sum	O
of	O
squared	O
coeﬃcients	O
term	B
in	O
the	O
ridge	B
regression	I
formulation	O
(	O
6.5	O
)	O
,	O
such	O
a	O
change	O
in	O
scale	O
will	O
not	O
simply	O
cause	O
the	O
ridge	B
regression	I
co-	O
eﬃcient	O
estimate	O
for	O
income	O
to	O
change	O
by	O
a	O
factor	B
of	O
1,000.	O
in	O
other	O
words	O
,	O
xj	O
ˆβr	O
j	O
,	O
λ	O
will	O
depend	O
not	O
only	O
on	O
the	O
value	O
of	O
λ	O
,	O
but	O
also	O
on	O
the	O
scaling	O
of	O
the	O
jth	O
predictor	B
.	O
in	O
fact	O
,	O
the	O
value	O
of	O
xj	O
ˆβr	O
j	O
,	O
λ	O
may	O
even	O
depend	O
on	O
the	O
scaling	O
of	O
the	O
other	O
predictors	O
!	O
therefore	O
,	O
it	O
is	O
best	O
to	O
apply	O
ridge	B
regression	I
after	O
standardizing	O
the	O
predictors	O
,	O
using	O
the	O
formula	O
scale	B
equivariant	I
˜xij	O
=	O
’	O
(	O
cid:10	O
)	O
1	O
n	O
xij	O
n	O
i=1	O
(	O
xij	O
−	O
xj	O
)	O
2	O
,	O
(	O
6.6	O
)	O
so	O
that	O
they	O
are	O
all	O
on	O
the	O
same	O
scale	O
.	O
in	O
(	O
6.6	O
)	O
,	O
the	O
denominator	O
is	O
the	O
estimated	O
standard	O
deviation	O
of	O
the	O
jth	O
predictor	B
.	O
consequently	O
,	O
all	O
of	O
the	O
standardized	O
predictors	O
will	O
have	O
a	O
standard	O
deviation	O
of	O
one	O
.	O
as	O
a	O
re-	O
sult	O
the	O
ﬁnal	O
ﬁt	B
will	O
not	O
depend	O
on	O
the	O
scale	O
on	O
which	O
the	O
predictors	O
are	O
measured	O
.	O
in	O
figure	O
6.4	O
,	O
the	O
y-axis	O
displays	O
the	O
standardized	O
ridge	O
regres-	O
sion	O
coeﬃcient	B
estimates—that	O
is	O
,	O
the	O
coeﬃcient	B
estimates	O
that	O
result	O
from	O
performing	O
ridge	B
regression	I
using	O
standardized	O
predictors	O
.	O
why	O
does	O
ridge	B
regression	I
improve	O
over	O
least	B
squares	I
?	O
ridge	B
regression	I
’	O
s	O
advantage	O
over	O
least	B
squares	I
is	O
rooted	O
in	O
the	O
bias-variance	B
trade-oﬀ	O
.	O
as	O
λ	O
increases	O
,	O
the	O
ﬂexibility	O
of	O
the	O
ridge	B
regression	I
ﬁt	O
decreases	O
,	O
leading	O
to	O
decreased	O
variance	B
but	O
increased	O
bias	B
.	O
this	O
is	O
illustrated	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
6.5	O
,	O
using	O
a	O
simulated	O
data	B
set	O
containing	O
p	O
=	O
45	O
predictors	O
and	O
n	O
=	O
50	O
observations	B
.	O
the	O
green	O
curve	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
6.5	O
displays	O
the	O
variance	B
of	O
the	O
ridge	B
regression	I
predictions	O
as	O
a	O
function	B
of	O
λ.	O
at	O
the	O
least	B
squares	I
coeﬃcient	O
estimates	O
,	O
which	O
correspond	O
to	O
ridge	B
regression	I
with	O
λ	O
=	O
0	O
,	O
the	O
variance	B
is	O
high	O
but	O
there	O
is	O
no	O
bias	B
.	O
but	O
as	O
λ	O
increases	O
,	O
the	O
shrinkage	B
of	O
the	O
ridge	O
coeﬃcient	O
estimates	O
leads	O
to	O
a	O
substantial	O
reduction	O
in	O
the	O
variance	B
of	O
the	O
predictions	O
,	O
at	O
the	O
expense	O
of	O
a	O
slight	O
increase	O
in	O
bias	B
.	O
recall	B
that	O
the	O
test	B
mean	O
squared	O
error	B
(	O
mse	O
)	O
,	O
plot-	O
ted	O
in	O
purple	O
,	O
is	O
a	O
function	B
of	O
the	O
variance	B
plus	O
the	O
squared	O
bias	B
.	O
for	O
values	O
218	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0	O
6	O
0	O
5	O
0	O
4	O
0	O
3	O
0	O
2	O
0	O
1	O
0	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0	O
6	O
0	O
5	O
0	O
4	O
0	O
3	O
0	O
2	O
0	O
1	O
0	O
1e−01	O
1e+01	O
λ	O
1e+03	O
0.0	O
0.2	O
0.6	O
0.4	O
λ	O
2/	O
ˆβ	O
2	O
ˆβr	O
0.8	O
1.0	O
figure	O
6.5.	O
squared	O
bias	B
(	O
black	O
)	O
,	O
variance	B
(	O
green	O
)	O
,	O
and	O
test	B
mean	O
squared	O
error	B
(	O
purple	O
)	O
for	O
the	O
ridge	B
regression	I
predictions	O
on	O
a	O
simulated	O
data	B
set	O
,	O
as	O
a	O
λ	O
(	O
cid:4	O
)	O
2/	O
(	O
cid:4	O
)	O
ˆβ	O
(	O
cid:4	O
)	O
2.	O
the	O
horizontal	O
dashed	O
lines	O
indicate	O
the	O
minimum	O
function	B
of	O
λ	O
and	O
(	O
cid:4	O
)	O
ˆβr	O
possible	O
mse	O
.	O
the	O
purple	O
crosses	O
indicate	O
the	O
ridge	B
regression	I
models	O
for	O
which	O
the	O
mse	O
is	O
smallest	O
.	O
of	O
λ	O
up	O
to	O
about	O
10	O
,	O
the	O
variance	B
decreases	O
rapidly	O
,	O
with	O
very	O
little	O
increase	O
in	O
bias	B
,	O
plotted	O
in	O
black	O
.	O
consequently	O
,	O
the	O
mse	O
drops	O
considerably	O
as	O
λ	O
increases	O
from	O
0	O
to	O
10.	O
beyond	O
this	O
point	O
,	O
the	O
decrease	O
in	O
variance	B
due	O
to	O
increasing	O
λ	O
slows	O
,	O
and	O
the	O
shrinkage	B
on	O
the	O
coeﬃcients	O
causes	O
them	O
to	O
be	O
signiﬁcantly	O
underestimated	O
,	O
resulting	O
in	O
a	O
large	O
increase	O
in	O
the	O
bias	B
.	O
the	O
minimum	O
mse	O
is	O
achieved	O
at	O
approximately	O
λ	O
=	O
30.	O
interestingly	O
,	O
because	O
of	O
its	O
high	O
variance	B
,	O
the	O
mse	O
associated	O
with	O
the	O
least	B
squares	I
ﬁt	O
,	O
when	O
λ	O
=	O
0	O
,	O
is	O
almost	O
as	O
high	O
as	O
that	O
of	O
the	O
null	B
model	O
for	O
which	O
all	O
coeﬃcient	B
estimates	O
are	O
zero	O
,	O
when	O
λ	O
=	O
∞	O
.	O
however	O
,	O
for	O
an	O
intermediate	O
value	O
of	O
λ	O
,	O
the	O
mse	O
is	O
considerably	O
lower	O
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
6.5	O
displays	O
the	O
same	O
curves	O
as	O
the	O
left-	O
hand	O
panel	O
,	O
this	O
time	O
plotted	O
against	O
the	O
(	O
cid:6	O
)	O
2	O
norm	O
of	O
the	O
ridge	B
regression	I
coeﬃcient	O
estimates	O
divided	O
by	O
the	O
(	O
cid:6	O
)	O
2	O
norm	O
of	O
the	O
least	B
squares	I
estimates	O
.	O
now	O
as	O
we	O
move	O
from	O
left	O
to	O
right	O
,	O
the	O
ﬁts	O
become	O
more	O
ﬂexible	B
,	O
and	O
so	O
the	O
bias	B
decreases	O
and	O
the	O
variance	B
increases	O
.	O
in	O
general	O
,	O
in	O
situations	O
where	O
the	O
relationship	O
between	O
the	O
response	B
and	O
the	O
predictors	O
is	O
close	O
to	O
linear	B
,	O
the	O
least	B
squares	I
estimates	O
will	O
have	O
low	O
bias	B
but	O
may	O
have	O
high	O
variance	B
.	O
this	O
means	O
that	O
a	O
small	O
change	O
in	O
the	O
training	B
data	O
can	O
cause	O
a	O
large	O
change	O
in	O
the	O
least	B
squares	I
coeﬃcient	O
estimates	O
.	O
in	O
particular	O
,	O
when	O
the	O
number	O
of	O
variables	O
p	O
is	O
almost	O
as	O
large	O
as	O
the	O
number	O
of	O
observations	B
n	O
,	O
as	O
in	O
the	O
example	O
in	O
figure	O
6.5	O
,	O
the	O
least	B
squares	I
estimates	O
will	O
be	O
extremely	O
variable	B
.	O
and	O
if	O
p	O
>	O
n	O
,	O
then	O
the	O
least	B
squares	I
estimates	O
do	O
not	O
even	O
have	O
a	O
unique	O
solution	O
,	O
whereas	O
ridge	B
regression	I
can	O
still	O
perform	O
well	O
by	O
trading	O
oﬀ	O
a	O
small	O
increase	O
in	O
bias	B
for	O
a	O
large	O
decrease	O
in	O
variance	B
.	O
hence	O
,	O
ridge	B
regression	I
works	O
best	O
in	O
situations	O
where	O
the	O
least	B
squares	I
estimates	O
have	O
high	O
variance	B
.	O
ridge	B
regression	I
also	O
has	O
substantial	O
computational	O
advantages	O
over	O
best	B
subset	I
selection	I
,	O
which	O
requires	O
searching	O
through	O
2p	O
models	O
.	O
as	O
we	O
6.2	O
shrinkage	B
methods	O
219	O
discussed	O
previously	O
,	O
even	O
for	O
moderate	O
values	O
of	O
p	O
,	O
such	O
a	O
search	O
can	O
be	O
computationally	O
infeasible	O
.	O
in	O
contrast	B
,	O
for	O
any	O
ﬁxed	O
value	O
of	O
λ	O
,	O
ridge	B
regression	I
only	O
ﬁts	O
a	O
single	B
model	O
,	O
and	O
the	O
model-ﬁtting	O
procedure	O
can	O
be	O
performed	O
quite	O
quickly	O
.	O
in	O
fact	O
,	O
one	O
can	O
show	O
that	O
the	O
computations	O
required	O
to	O
solve	O
(	O
6.5	O
)	O
,	O
simultaneously	O
for	O
all	O
values	O
of	O
λ	O
,	O
are	O
almost	O
iden-	O
tical	O
to	O
those	O
for	O
ﬁtting	O
a	O
model	B
using	O
least	B
squares	I
.	O
6.2.2	O
the	O
lasso	B
(	O
cid:10	O
)	O
ridge	B
regression	I
does	O
have	O
one	O
obvious	O
disadvantage	O
.	O
unlike	O
best	O
subset	O
,	O
forward	O
stepwise	O
,	O
and	O
backward	B
stepwise	I
selection	I
,	O
which	O
will	O
generally	O
select	O
models	O
that	O
involve	O
just	O
a	O
subset	O
of	O
the	O
variables	O
,	O
ridge	B
regression	I
β2	O
will	O
include	O
all	O
p	O
predictors	O
in	O
the	O
ﬁnal	O
model	B
.	O
the	O
penalty	B
λ	O
j	O
in	O
(	O
6.5	O
)	O
will	O
shrink	O
all	O
of	O
the	O
coeﬃcients	O
towards	O
zero	O
,	O
but	O
it	O
will	O
not	O
set	B
any	O
of	O
them	O
exactly	O
to	O
zero	O
(	O
unless	O
λ	O
=	O
∞	O
)	O
.	O
this	O
may	O
not	O
be	O
a	O
problem	O
for	O
prediction	O
accuracy	O
,	O
but	O
it	O
can	O
create	O
a	O
challenge	O
in	O
model	B
interpretation	O
in	O
settings	O
in	O
which	O
the	O
number	O
of	O
variables	O
p	O
is	O
quite	O
large	O
.	O
for	O
example	O
,	O
in	O
the	O
credit	O
data	B
set	O
,	O
it	O
appears	O
that	O
the	O
most	O
important	O
variables	O
are	O
income	O
,	O
limit	O
,	O
rating	O
,	O
and	O
student	O
.	O
so	O
we	O
might	O
wish	O
to	O
build	O
a	O
model	B
including	O
just	O
these	O
predictors	O
.	O
however	O
,	O
ridge	B
regression	I
will	O
always	O
generate	O
a	O
model	B
involving	O
all	O
ten	O
predictors	O
.	O
increasing	O
the	O
value	O
of	O
λ	O
will	O
tend	O
to	O
reduce	O
the	O
magnitudes	O
of	O
the	O
coeﬃcients	O
,	O
but	O
will	O
not	O
result	O
in	O
exclusion	O
of	O
any	O
of	O
the	O
variables	O
.	O
the	O
lasso	B
is	O
a	O
relatively	O
recent	O
alternative	O
to	O
ridge	B
regression	I
that	O
over-	O
λ	O
,	O
minimize	O
the	O
quantity	O
comes	O
this	O
disadvantage	O
.	O
the	O
lasso	B
coeﬃcients	O
,	O
ˆβl	O
lasso	B
βjxij	O
+	O
λ	O
|βj|	O
=	O
rss	O
+	O
λ	O
|βj|	O
.	O
(	O
6.7	O
)	O
p	O
(	O
cid:17	O
)	O
n	O
(	O
cid:17	O
)	O
⎛	O
⎝yi	O
−	O
β0	O
−	O
p	O
(	O
cid:17	O
)	O
⎞	O
⎠2	O
p	O
(	O
cid:17	O
)	O
i=1	O
j=1	O
j=1	O
j=1	O
comparing	O
(	O
6.7	O
)	O
to	O
(	O
6.5	O
)	O
,	O
we	O
see	O
that	O
the	O
lasso	B
and	O
ridge	B
regression	I
have	O
similar	O
formulations	O
.	O
the	O
only	O
diﬀerence	O
is	O
that	O
the	O
β2	O
j	O
term	B
in	O
the	O
ridge	B
regression	I
penalty	O
(	O
6.5	O
)	O
has	O
been	O
replaced	O
by	O
|βj|	O
in	O
the	O
lasso	B
penalty	O
(	O
6.7	O
)	O
.	O
in	O
statistical	O
parlance	O
,	O
the	O
lasso	B
uses	O
an	O
(	O
cid:6	O
)	O
1	O
(	O
pronounced	O
“	O
ell	O
1	O
”	O
)	O
penalty	B
instead	O
of	O
an	O
(	O
cid:6	O
)	O
2	O
penalty	B
.	O
the	O
(	O
cid:6	O
)	O
1	O
norm	O
of	O
a	O
coeﬃcient	B
vector	O
β	O
is	O
given	O
by	O
(	O
cid:12	O
)	O
β	O
(	O
cid:12	O
)	O
1	O
=	O
(	O
cid:10	O
)	O
|βj|	O
.	O
as	O
with	O
ridge	B
regression	I
,	O
the	O
lasso	B
shrinks	O
the	O
coeﬃcient	B
estimates	O
towards	O
zero	O
.	O
however	O
,	O
in	O
the	O
case	O
of	O
the	O
lasso	B
,	O
the	O
(	O
cid:6	O
)	O
1	O
penalty	B
has	O
the	O
eﬀect	O
of	O
forcing	O
some	O
of	O
the	O
coeﬃcient	B
estimates	O
to	O
be	O
exactly	O
equal	O
to	O
zero	O
when	O
the	O
tuning	B
parameter	I
λ	O
is	O
suﬃciently	O
large	O
.	O
hence	O
,	O
much	O
like	O
best	O
subset	O
se-	O
lection	O
,	O
the	O
lasso	B
performs	O
variable	B
selection	O
.	O
as	O
a	O
result	O
,	O
models	O
generated	O
from	O
the	O
lasso	B
are	O
generally	O
much	O
easier	O
to	O
interpret	O
than	O
those	O
produced	O
by	O
ridge	B
regression	I
.	O
we	O
say	O
that	O
the	O
lasso	B
yields	O
sparse	B
models—that	O
is	O
,	O
sparse	B
models	O
that	O
involve	O
only	O
a	O
subset	O
of	O
the	O
variables	O
.	O
as	O
in	O
ridge	B
regression	I
,	O
selecting	O
a	O
good	O
value	O
of	O
λ	O
for	O
the	O
lasso	B
is	O
critical	O
;	O
we	O
defer	O
this	O
discussion	O
to	O
section	O
6.2.3	O
,	O
where	O
we	O
use	O
cross-validation	B
.	O
220	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
i	O
s	O
t	O
n	O
e	O
c	O
i	O
f	O
f	O
e	O
o	O
c	O
d	O
e	O
z	O
d	O
r	O
a	O
d	O
n	O
a	O
t	O
s	O
i	O
0	O
0	O
4	O
0	O
0	O
3	O
0	O
0	O
2	O
0	O
0	O
1	O
0	O
0	O
0	O
2	O
−	O
i	O
s	O
t	O
n	O
e	O
c	O
i	O
f	O
f	O
e	O
o	O
c	O
d	O
e	O
z	O
d	O
r	O
a	O
d	O
n	O
a	O
t	O
s	O
i	O
0	O
0	O
4	O
0	O
0	O
3	O
0	O
0	O
2	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
−	O
0	O
0	O
3	O
−	O
income	O
limit	O
rating	O
student	O
20	O
50	O
100	O
200	O
500	O
λ	O
2000	O
5000	O
0.0	O
0.2	O
0.6	O
0.4	O
ˆβl	O
λ	O
1/	O
ˆβ	O
1	O
0.8	O
1.0	O
figure	O
6.6.	O
the	O
standardized	O
lasso	B
coeﬃcients	O
on	O
the	O
credit	O
data	B
set	O
are	O
shown	O
as	O
a	O
function	B
of	O
λ	O
and	O
(	O
cid:4	O
)	O
ˆβl	O
λ	O
(	O
cid:4	O
)	O
1/	O
(	O
cid:4	O
)	O
ˆβ	O
(	O
cid:4	O
)	O
1.	O
as	O
an	O
example	O
,	O
consider	O
the	O
coeﬃcient	B
plots	O
in	O
figure	O
6.6	O
,	O
which	O
are	O
gen-	O
erated	O
from	O
applying	O
the	O
lasso	B
to	O
the	O
credit	O
data	B
set	O
.	O
when	O
λ	O
=	O
0	O
,	O
then	O
the	O
lasso	B
simply	O
gives	O
the	O
least	B
squares	I
ﬁt	O
,	O
and	O
when	O
λ	O
becomes	O
suﬃciently	O
large	O
,	O
the	O
lasso	B
gives	O
the	O
null	B
model	O
in	O
which	O
all	O
coeﬃcient	B
estimates	O
equal	O
zero	O
.	O
however	O
,	O
in	O
between	O
these	O
two	O
extremes	O
,	O
the	O
ridge	B
regression	I
and	O
lasso	B
models	O
are	O
quite	O
diﬀerent	O
from	O
each	O
other	O
.	O
moving	O
from	O
left	O
to	O
right	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
6.6	O
,	O
we	O
observe	O
that	O
at	O
ﬁrst	O
the	O
lasso	B
re-	O
sults	O
in	O
a	O
model	B
that	O
contains	O
only	O
the	O
rating	O
predictor	B
.	O
then	O
student	O
and	O
limit	O
enter	O
the	O
model	B
almost	O
simultaneously	O
,	O
shortly	O
followed	O
by	O
income	O
.	O
eventually	O
,	O
the	O
remaining	O
variables	O
enter	O
the	O
model	B
.	O
hence	O
,	O
depending	O
on	O
the	O
value	O
of	O
λ	O
,	O
the	O
lasso	B
can	O
produce	O
a	O
model	B
involving	O
any	O
number	O
of	O
vari-	O
ables	O
.	O
in	O
contrast	B
,	O
ridge	B
regression	I
will	O
always	O
include	O
all	O
of	O
the	O
variables	O
in	O
the	O
model	B
,	O
although	O
the	O
magnitude	O
of	O
the	O
coeﬃcient	B
estimates	O
will	O
depend	O
on	O
λ.	O
another	O
formulation	O
for	O
ridge	O
regression	B
and	O
the	O
lasso	B
one	O
can	O
show	O
that	O
the	O
lasso	B
and	O
ridge	B
regression	I
coeﬃcient	O
estimates	O
solve	O
the	O
problems	O
⎧⎪⎨	O
⎪⎩	O
n	O
(	O
cid:17	O
)	O
i=1	O
⎛	O
⎝yi	O
−	O
β0	O
−	O
p	O
(	O
cid:17	O
)	O
⎞	O
⎠	O
2	O
⎫⎪⎬	O
⎪⎭	O
subject	O
to	O
βjxij	O
j=1	O
minimize	O
β	O
and	O
minimize	O
β	O
⎧⎪⎨	O
⎪⎩	O
n	O
(	O
cid:17	O
)	O
i=1	O
⎛	O
⎝yi	O
−	O
β0	O
−	O
p	O
(	O
cid:17	O
)	O
j=1	O
⎞	O
⎠2	O
⎫⎪⎬	O
⎪⎭	O
subject	O
to	O
βjxij	O
p	O
(	O
cid:17	O
)	O
j=1	O
|βj|	O
≤	O
s	O
(	O
6.8	O
)	O
p	O
(	O
cid:17	O
)	O
j=1	O
≤	O
s	O
,	O
β2	O
j	O
(	O
6.9	O
)	O
6.2	O
shrinkage	B
methods	O
221	O
respectively	O
.	O
in	O
other	O
words	O
,	O
for	O
every	O
value	O
of	O
λ	O
,	O
there	O
is	O
some	O
s	O
such	O
that	O
the	O
equations	O
(	O
6.7	O
)	O
and	O
(	O
6.8	O
)	O
will	O
give	O
the	O
same	O
lasso	B
coeﬃcient	O
estimates	O
.	O
similarly	O
,	O
for	O
every	O
value	O
of	O
λ	O
there	O
is	O
a	O
corresponding	O
s	O
such	O
that	O
equa-	O
tions	O
(	O
6.5	O
)	O
and	O
(	O
6.9	O
)	O
will	O
give	O
the	O
same	O
ridge	B
regression	I
coeﬃcient	O
estimates	O
.	O
when	O
p	O
=	O
2	O
,	O
then	O
(	O
6.8	O
)	O
indicates	O
that	O
the	O
lasso	B
coeﬃcient	O
estimates	O
have	O
the	O
smallest	O
rss	O
out	O
of	O
all	O
points	O
that	O
lie	O
within	O
the	O
diamond	O
deﬁned	O
by	O
|β1|	O
+	O
|β2|	O
≤	O
s.	O
similarly	O
,	O
the	O
ridge	B
regression	I
estimates	O
have	O
the	O
smallest	O
rss	O
out	O
of	O
all	O
points	O
that	O
lie	O
within	O
the	O
circle	O
deﬁned	O
by	O
β2	O
(	O
cid:10	O
)	O
we	O
can	O
think	O
of	O
(	O
6.8	O
)	O
as	O
follows	O
.	O
when	O
we	O
perform	O
the	O
lasso	B
we	O
are	O
trying	O
to	O
ﬁnd	O
the	O
set	B
of	O
coeﬃcient	B
estimates	O
that	O
lead	O
to	O
the	O
smallest	O
rss	O
,	O
subject	O
to	O
the	O
constraint	O
that	O
there	O
is	O
a	O
budget	O
s	O
for	O
how	O
large	O
when	O
s	O
is	O
extremely	O
large	O
,	O
then	O
this	O
budget	O
is	O
not	O
very	O
restrictive	O
,	O
and	O
so	O
the	O
coeﬃcient	B
estimates	O
can	O
be	O
large	O
.	O
in	O
fact	O
,	O
if	O
s	O
is	O
large	O
enough	O
that	O
the	O
least	B
squares	I
solution	O
falls	O
within	O
the	O
budget	O
,	O
then	O
(	O
6.8	O
)	O
will	O
simply	O
yield	O
the	O
least	B
squares	I
solution	O
.	O
in	O
contrast	B
,	O
if	O
s	O
is	O
small	O
,	O
then	O
small	O
in	O
order	O
to	O
avoid	O
violating	O
the	O
budget	O
.	O
similarly	O
,	O
(	O
6.9	O
)	O
indicates	O
that	O
when	O
we	O
perform	O
ridge	B
regression	I
,	O
we	O
seek	O
a	O
set	B
of	O
coeﬃcient	B
estimates	O
(	O
cid:10	O
)	O
such	O
that	O
the	O
rss	O
is	O
as	O
small	O
as	O
possible	O
,	O
subject	O
to	O
the	O
requirement	O
that	O
|βj|	O
must	O
be	O
|βj|	O
can	O
be	O
.	O
p	O
j=1	O
β2	O
the	O
formulations	O
(	O
6.8	O
)	O
and	O
(	O
6.9	O
)	O
reveal	O
a	O
close	O
connection	O
between	O
the	O
j	O
not	O
exceed	O
the	O
budget	O
s.	O
1	O
+	O
β2	O
2	O
≤	O
s.	O
(	O
cid:10	O
)	O
p	O
j=1	O
p	O
j=1	O
lasso	B
,	O
ridge	B
regression	I
,	O
and	O
best	B
subset	I
selection	I
.	O
consider	O
the	O
problem	O
⎞	O
⎠2	O
⎫⎪⎬	O
⎪⎭	O
subject	O
to	O
p	O
(	O
cid:17	O
)	O
j=1	O
i	O
(	O
βj	O
(	O
cid:4	O
)	O
=	O
0	O
)	O
≤	O
s.	O
⎧⎪⎨	O
⎪⎩	O
n	O
(	O
cid:17	O
)	O
i=1	O
⎛	O
⎝yi	O
−	O
β0	O
−	O
p	O
(	O
cid:17	O
)	O
βjxij	O
j=1	O
minimize	O
β	O
here	O
i	O
(	O
βj	O
(	O
cid:4	O
)	O
=	O
0	O
)	O
is	O
an	O
indicator	B
variable	O
:	O
it	O
takes	O
on	O
a	O
value	O
of	O
1	O
if	O
βj	O
(	O
cid:4	O
)	O
=	O
0	O
,	O
and	O
(	O
6.10	O
)	O
equals	O
zero	O
otherwise	O
.	O
then	O
(	O
6.10	O
)	O
amounts	O
to	O
ﬁnding	O
a	O
set	B
of	O
coeﬃcient	B
es-	O
timates	O
such	O
that	O
rss	O
is	O
as	O
small	O
as	O
possible	O
,	O
subject	O
to	O
the	O
constraint	O
that	O
no	O
more	O
than	O
s	O
coeﬃcients	O
can	O
be	O
nonzero	O
.	O
the	O
problem	O
(	O
6.10	O
)	O
is	O
equivalent	O
to	O
best	B
subset	I
selection	I
.	O
unfortunately	O
,	O
solving	O
(	O
6.10	O
)	O
is	O
computationally	O
infeasible	O
when	O
p	O
is	O
large	O
,	O
since	O
it	O
requires	O
considering	O
all	O
models	O
con-	O
taining	O
s	O
predictors	O
.	O
therefore	O
,	O
we	O
can	O
interpret	O
ridge	B
regression	I
and	O
the	O
lasso	B
as	O
computationally	O
feasible	O
alternatives	O
to	O
best	B
subset	I
selection	I
that	O
replace	O
the	O
intractable	O
form	O
of	O
the	O
budget	O
in	O
(	O
6.10	O
)	O
with	O
forms	O
that	O
are	O
much	O
easier	O
to	O
solve	O
.	O
of	O
course	O
,	O
the	O
lasso	B
is	O
much	O
more	O
closely	O
related	O
to	O
best	B
subset	I
selection	I
,	O
since	O
only	O
the	O
lasso	B
performs	O
feature	B
selection	I
for	O
s	O
suﬃciently	O
small	O
in	O
(	O
6.8	O
)	O
.	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
p	O
s	O
the	O
variable	B
selection	O
property	O
of	O
the	O
lasso	B
why	O
is	O
it	O
that	O
the	O
lasso	B
,	O
unlike	O
ridge	B
regression	I
,	O
results	O
in	O
coeﬃcient	B
estimates	O
that	O
are	O
exactly	O
equal	O
to	O
zero	O
?	O
the	O
formulations	O
(	O
6.8	O
)	O
and	O
(	O
6.9	O
)	O
can	O
be	O
used	O
to	O
shed	O
light	O
on	O
the	O
issue	O
.	O
figure	O
6.7	O
illustrates	O
the	O
situation	O
.	O
the	O
least	B
squares	I
solution	O
is	O
marked	O
as	O
ˆβ	O
,	O
while	O
the	O
blue	O
diamond	O
and	O
222	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
β2	O
^	O
β	O
β2	O
β^	O
β1	O
β1	O
figure	O
6.7.	O
contours	O
of	O
the	O
error	B
and	O
constraint	O
functions	O
for	O
the	O
lasso	B
(	O
left	O
)	O
and	O
ridge	B
regression	I
(	O
right	O
)	O
.	O
the	O
solid	O
blue	O
areas	O
are	O
the	O
constraint	O
re-	O
gions	O
,	O
|β1|	O
+	O
|β2|	O
≤	O
s	O
and	O
β2	O
2	O
≤	O
s	O
,	O
while	O
the	O
red	O
ellipses	O
are	O
the	O
contours	O
of	O
the	O
rss	O
.	O
1	O
+	O
β2	O
circle	O
represent	O
the	O
lasso	B
and	O
ridge	B
regression	I
constraints	O
in	O
(	O
6.8	O
)	O
and	O
(	O
6.9	O
)	O
,	O
respectively	O
.	O
if	O
s	O
is	O
suﬃciently	O
large	O
,	O
then	O
the	O
constraint	O
regions	O
will	O
con-	O
tain	O
ˆβ	O
,	O
and	O
so	O
the	O
ridge	B
regression	I
and	O
lasso	B
estimates	O
will	O
be	O
the	O
same	O
as	O
the	O
least	B
squares	I
estimates	O
.	O
(	O
such	O
a	O
large	O
value	O
of	O
s	O
corresponds	O
to	O
λ	O
=	O
0	O
in	O
(	O
6.5	O
)	O
and	O
(	O
6.7	O
)	O
.	O
)	O
however	O
,	O
in	O
figure	O
6.7	O
the	O
least	B
squares	I
estimates	O
lie	O
outside	O
of	O
the	O
diamond	O
and	O
the	O
circle	O
,	O
and	O
so	O
the	O
least	B
squares	I
estimates	O
are	O
not	O
the	O
same	O
as	O
the	O
lasso	B
and	O
ridge	B
regression	I
estimates	O
.	O
the	O
ellipses	O
that	O
are	O
centered	O
around	O
ˆβ	O
represent	O
regions	O
of	O
constant	O
rss	O
.	O
in	O
other	O
words	O
,	O
all	O
of	O
the	O
points	O
on	O
a	O
given	O
ellipse	O
share	O
a	O
common	O
value	O
of	O
the	O
rss	O
.	O
as	O
the	O
ellipses	O
expand	O
away	O
from	O
the	O
least	B
squares	I
co-	O
eﬃcient	O
estimates	O
,	O
the	O
rss	O
increases	O
.	O
equations	O
(	O
6.8	O
)	O
and	O
(	O
6.9	O
)	O
indicate	O
that	O
the	O
lasso	B
and	O
ridge	B
regression	I
coeﬃcient	O
estimates	O
are	O
given	O
by	O
the	O
ﬁrst	O
point	O
at	O
which	O
an	O
ellipse	O
contacts	O
the	O
constraint	O
region	O
.	O
since	O
ridge	B
regression	I
has	O
a	O
circular	O
constraint	O
with	O
no	O
sharp	O
points	O
,	O
this	O
intersection	O
will	O
not	O
generally	O
occur	O
on	O
an	O
axis	O
,	O
and	O
so	O
the	O
ridge	B
regression	I
coeﬃcient	O
estimates	O
will	O
be	O
exclusively	O
non-zero	O
.	O
however	O
,	O
the	O
lasso	B
constraint	O
has	O
corners	O
at	O
each	O
of	O
the	O
axes	O
,	O
and	O
so	O
the	O
ellipse	O
will	O
often	O
intersect	O
the	O
con-	O
straint	O
region	O
at	O
an	O
axis	O
.	O
when	O
this	O
occurs	O
,	O
one	O
of	O
the	O
coeﬃcients	O
will	O
equal	O
zero	O
.	O
in	O
higher	O
dimensions	O
,	O
many	O
of	O
the	O
coeﬃcient	B
estimates	O
may	O
equal	O
zero	O
simultaneously	O
.	O
in	O
figure	O
6.7	O
,	O
the	O
intersection	O
occurs	O
at	O
β1	O
=	O
0	O
,	O
and	O
so	O
the	O
resulting	O
model	B
will	O
only	O
include	O
β2	O
.	O
in	O
figure	O
6.7	O
,	O
we	O
considered	O
the	O
simple	B
case	O
of	O
p	O
=	O
2.	O
when	O
p	O
=	O
3	O
,	O
then	O
the	O
constraint	O
region	O
for	O
ridge	O
regression	B
becomes	O
a	O
sphere	O
,	O
and	O
the	O
constraint	O
region	O
for	O
the	O
lasso	B
becomes	O
a	O
polyhedron	O
.	O
when	O
p	O
>	O
3	O
,	O
the	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0	O
6	O
0	O
5	O
0	O
4	O
0	O
3	O
0	O
2	O
0	O
1	O
0	O
6.2	O
shrinkage	B
methods	O
223	O
0	O
6	O
0	O
5	O
0	O
4	O
0	O
3	O
0	O
2	O
0	O
1	O
0	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0.02	O
0.10	O
0.50	O
2.00	O
10.00	O
50.00	O
0.0	O
0.2	O
0.4	O
0.6	O
0.8	O
1.0	O
λ	O
r2	O
on	O
training	B
data	O
figure	O
6.8.	O
left	O
:	O
plots	O
of	O
squared	O
bias	B
(	O
black	O
)	O
,	O
variance	B
(	O
green	O
)	O
,	O
and	O
test	B
mse	O
(	O
purple	O
)	O
for	O
the	O
lasso	B
on	O
a	O
simulated	O
data	B
set	O
.	O
right	O
:	O
comparison	O
of	O
squared	O
bias	B
,	O
variance	B
and	O
test	B
mse	O
between	O
lasso	B
(	O
solid	O
)	O
and	O
ridge	O
(	O
dotted	O
)	O
.	O
both	O
are	O
plotted	O
against	O
their	O
r2	O
on	O
the	O
training	B
data	O
,	O
as	O
a	O
common	O
form	O
of	O
indexing	O
.	O
the	O
crosses	O
in	O
both	O
plots	O
indicate	O
the	O
lasso	B
model	O
for	O
which	O
the	O
mse	O
is	O
smallest	O
.	O
constraint	O
for	O
ridge	O
regression	B
becomes	O
a	O
hypersphere	O
,	O
and	O
the	O
constraint	O
for	O
the	O
lasso	B
becomes	O
a	O
polytope	O
.	O
however	O
,	O
the	O
key	O
ideas	O
depicted	O
in	O
fig-	O
ure	O
6.7	O
still	O
hold	O
.	O
in	O
particular	O
,	O
the	O
lasso	B
leads	O
to	O
feature	B
selection	I
when	O
p	O
>	O
2	O
due	O
to	O
the	O
sharp	O
corners	O
of	O
the	O
polyhedron	O
or	O
polytope	O
.	O
comparing	O
the	O
lasso	B
and	O
ridge	B
regression	I
it	O
is	O
clear	O
that	O
the	O
lasso	B
has	O
a	O
major	O
advantage	O
over	O
ridge	B
regression	I
,	O
in	O
that	O
it	O
produces	O
simpler	O
and	O
more	O
interpretable	O
models	O
that	O
involve	O
only	O
a	O
subset	O
of	O
the	O
predictors	O
.	O
however	O
,	O
which	O
method	O
leads	O
to	O
better	O
prediction	B
accuracy	O
?	O
figure	O
6.8	O
displays	O
the	O
variance	B
,	O
squared	O
bias	B
,	O
and	O
test	B
mse	O
of	O
the	O
lasso	B
applied	O
to	O
the	O
same	O
simulated	O
data	B
as	O
in	O
figure	O
6.5.	O
clearly	O
the	O
lasso	B
leads	O
to	O
qualitatively	O
similar	O
behavior	O
to	O
ridge	B
regression	I
,	O
in	O
that	O
as	O
λ	O
increases	O
,	O
the	O
variance	B
decreases	O
and	O
the	O
bias	B
increases	O
.	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
6.8	O
,	O
the	O
dotted	O
lines	O
represent	O
the	O
ridge	B
regression	I
ﬁts	O
.	O
here	O
we	O
plot	B
both	O
against	O
their	O
r2	O
on	O
the	O
training	B
data	O
.	O
this	O
is	O
another	O
useful	O
way	O
to	O
index	O
models	O
,	O
and	O
can	O
be	O
used	O
to	O
compare	O
models	O
with	O
diﬀerent	O
types	O
of	O
regularization	B
,	O
as	O
is	O
the	O
case	O
here	O
.	O
in	O
this	O
example	O
,	O
the	O
lasso	B
and	O
ridge	B
regression	I
result	O
in	O
almost	O
identical	O
biases	O
.	O
however	O
,	O
the	O
variance	B
of	O
ridge	B
regression	I
is	O
slightly	O
lower	O
than	O
the	O
variance	B
of	O
the	O
lasso	B
.	O
consequently	O
,	O
the	O
minimum	O
mse	O
of	O
ridge	B
regression	I
is	O
slightly	O
smaller	O
than	O
that	O
of	O
the	O
lasso	B
.	O
however	O
,	O
the	O
data	B
in	O
figure	O
6.8	O
were	O
generated	O
in	O
such	O
a	O
way	O
that	O
all	O
45	O
predictors	O
were	O
related	O
to	O
the	O
response—that	O
is	O
,	O
none	O
of	O
the	O
true	O
coeﬃcients	O
β1	O
,	O
.	O
.	O
.	O
,	O
β45	O
equaled	O
zero	O
.	O
the	O
lasso	B
implicitly	O
assumes	O
that	O
a	O
number	O
of	O
the	O
coeﬃcients	O
truly	O
equal	O
zero	O
.	O
consequently	O
,	O
it	O
is	O
not	O
surprising	O
that	O
ridge	B
regression	I
outperforms	O
the	O
lasso	B
in	O
terms	O
of	O
prediction	B
error	O
in	O
this	O
setting	O
.	O
figure	O
6.9	O
illustrates	O
a	O
similar	O
situation	O
,	O
except	O
that	O
now	O
the	O
response	B
is	O
a	O
224	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0	O
0	O
1	O
0	O
8	O
0	O
6	O
0	O
4	O
0	O
2	O
0	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0	O
0	O
1	O
0	O
8	O
0	O
6	O
0	O
4	O
0	O
2	O
0	O
0.02	O
0.10	O
0.50	O
2.00	O
10.00	O
50.00	O
0.4	O
0.5	O
0.6	O
0.7	O
0.8	O
0.9	O
1.0	O
λ	O
r2	O
on	O
training	B
data	O
figure	O
6.9.	O
left	O
:	O
plots	O
of	O
squared	O
bias	B
(	O
black	O
)	O
,	O
variance	B
(	O
green	O
)	O
,	O
and	O
test	B
mse	O
(	O
purple	O
)	O
for	O
the	O
lasso	B
.	O
the	O
simulated	O
data	B
is	O
similar	O
to	O
that	O
in	O
figure	O
6.8	O
,	O
except	O
that	O
now	O
only	O
two	O
predictors	O
are	O
related	O
to	O
the	O
response	B
.	O
right	O
:	O
comparison	O
of	O
squared	O
bias	B
,	O
variance	B
and	O
test	B
mse	O
between	O
lasso	B
(	O
solid	O
)	O
and	O
ridge	O
(	O
dotted	O
)	O
.	O
both	O
are	O
plotted	O
against	O
their	O
r2	O
on	O
the	O
training	B
data	O
,	O
as	O
a	O
common	O
form	O
of	O
indexing	O
.	O
the	O
crosses	O
in	O
both	O
plots	O
indicate	O
the	O
lasso	B
model	O
for	O
which	O
the	O
mse	O
is	O
smallest	O
.	O
function	B
of	O
only	O
2	O
out	O
of	O
45	O
predictors	O
.	O
now	O
the	O
lasso	B
tends	O
to	O
outperform	O
ridge	B
regression	I
in	O
terms	O
of	O
bias	B
,	O
variance	B
,	O
and	O
mse	O
.	O
these	O
two	O
examples	O
illustrate	O
that	O
neither	O
ridge	B
regression	I
nor	O
the	O
lasso	B
will	O
universally	O
dominate	O
the	O
other	O
.	O
in	O
general	O
,	O
one	O
might	O
expect	O
the	O
lasso	B
to	O
perform	O
better	O
in	O
a	O
setting	O
where	O
a	O
relatively	O
small	O
number	O
of	O
predictors	O
have	O
substantial	O
coeﬃcients	O
,	O
and	O
the	O
remaining	O
predictors	O
have	O
coeﬃcients	O
that	O
are	O
very	O
small	O
or	O
that	O
equal	O
zero	O
.	O
ridge	B
regression	I
will	O
perform	O
better	O
when	O
the	O
response	B
is	O
a	O
function	B
of	O
many	O
predictors	O
,	O
all	O
with	O
coeﬃcients	O
of	O
roughly	O
equal	O
size	O
.	O
however	O
,	O
the	O
number	O
of	O
predictors	O
that	O
is	O
related	O
to	O
the	O
response	B
is	O
never	O
known	O
a	O
priori	O
for	O
real	O
data	B
sets	O
.	O
a	O
technique	O
such	O
as	O
cross-validation	B
can	O
be	O
used	O
in	O
order	O
to	O
determine	O
which	O
approach	B
is	O
better	O
on	O
a	O
particular	O
data	B
set	O
.	O
as	O
with	O
ridge	B
regression	I
,	O
when	O
the	O
least	B
squares	I
estimates	O
have	O
exces-	O
sively	O
high	O
variance	B
,	O
the	O
lasso	B
solution	O
can	O
yield	O
a	O
reduction	O
in	O
variance	B
at	O
the	O
expense	O
of	O
a	O
small	O
increase	O
in	O
bias	B
,	O
and	O
consequently	O
can	O
gener-	O
ate	O
more	O
accurate	O
predictions	O
.	O
unlike	O
ridge	B
regression	I
,	O
the	O
lasso	B
performs	O
variable	B
selection	O
,	O
and	O
hence	O
results	O
in	O
models	O
that	O
are	O
easier	O
to	O
interpret	O
.	O
there	O
are	O
very	O
eﬃcient	O
algorithms	O
for	O
ﬁtting	O
both	O
ridge	O
and	O
lasso	B
models	O
;	O
in	O
both	O
cases	O
the	O
entire	O
coeﬃcient	B
paths	O
can	O
be	O
computed	O
with	O
about	O
the	O
same	O
amount	O
of	O
work	O
as	O
a	O
single	B
least	O
squares	O
ﬁt	B
.	O
we	O
will	O
explore	O
this	O
further	O
in	O
the	O
lab	O
at	O
the	O
end	O
of	O
this	O
chapter	O
.	O
a	O
simple	B
special	O
case	O
for	O
ridge	O
regression	B
and	O
the	O
lasso	B
in	O
order	O
to	O
obtain	O
a	O
better	O
intuition	O
about	O
the	O
behavior	O
of	O
ridge	B
regression	I
and	O
the	O
lasso	B
,	O
consider	O
a	O
simple	B
special	O
case	O
with	O
n	O
=	O
p	O
,	O
and	O
x	O
a	O
diag-	O
onal	O
matrix	O
with	O
1	O
’	O
s	O
on	O
the	O
diagonal	O
and	O
0	O
’	O
s	O
in	O
all	O
oﬀ-diagonal	O
elements	O
.	O
to	O
simplify	O
the	O
problem	O
further	O
,	O
assume	O
also	O
that	O
we	O
are	O
performing	O
regres-	O
6.2	O
shrinkage	B
methods	O
225	O
sion	O
without	O
an	O
intercept	B
.	O
with	O
these	O
assumptions	O
,	O
the	O
usual	O
least	B
squares	I
problem	O
simpliﬁes	O
to	O
ﬁnding	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
that	O
minimize	O
p	O
(	O
cid:17	O
)	O
(	O
yj	O
−	O
βj	O
)	O
2	O
.	O
(	O
6.11	O
)	O
in	O
this	O
case	O
,	O
the	O
least	B
squares	I
solution	O
is	O
given	O
by	O
j=1	O
ˆβj	O
=	O
yj	O
.	O
and	O
in	O
this	O
setting	O
,	O
ridge	B
regression	I
amounts	O
to	O
ﬁnding	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
such	O
that	O
(	O
yj	O
−	O
βj	O
)	O
2	O
+	O
λ	O
j=1	O
j=1	O
β2	O
j	O
(	O
6.12	O
)	O
p	O
(	O
cid:17	O
)	O
p	O
(	O
cid:17	O
)	O
p	O
(	O
cid:17	O
)	O
p	O
(	O
cid:17	O
)	O
is	O
minimized	O
,	O
and	O
the	O
lasso	B
amounts	O
to	O
ﬁnding	O
the	O
coeﬃcients	O
such	O
that	O
(	O
yj	O
−	O
βj	O
)	O
2	O
+	O
λ	O
|βj|	O
(	O
6.13	O
)	O
j=1	O
j=1	O
is	O
minimized	O
.	O
one	O
can	O
show	O
that	O
in	O
this	O
setting	O
,	O
the	O
ridge	B
regression	I
esti-	O
mates	O
take	O
the	O
form	O
ˆβr	O
j	O
=	O
yj/	O
(	O
1	O
+	O
λ	O
)	O
,	O
and	O
the	O
lasso	B
estimates	O
take	O
the	O
form	O
⎧⎪⎨	O
⎪⎩	O
ˆβl	O
j	O
=	O
yj	O
−	O
λ/2	O
yj	O
+	O
λ/2	O
0	O
if	O
yj	O
>	O
λ/2	O
;	O
if	O
yj	O
<	O
−λ/2	O
;	O
if	O
|yj|	O
≤	O
λ/2	O
.	O
(	O
6.14	O
)	O
(	O
6.15	O
)	O
figure	O
6.10	O
displays	O
the	O
situation	O
.	O
we	O
can	O
see	O
that	O
ridge	B
regression	I
and	O
the	O
lasso	B
perform	O
two	O
very	O
diﬀerent	O
types	O
of	O
shrinkage	B
.	O
in	O
ridge	B
regression	I
,	O
each	O
least	B
squares	I
coeﬃcient	O
estimate	O
is	O
shrunken	O
by	O
the	O
same	O
proportion	O
.	O
in	O
contrast	B
,	O
the	O
lasso	B
shrinks	O
each	O
least	B
squares	I
coeﬃcient	O
towards	O
zero	O
by	O
a	O
constant	O
amount	O
,	O
λ/2	O
;	O
the	O
least	B
squares	I
coeﬃcients	O
that	O
are	O
less	O
than	O
λ/2	O
in	O
absolute	O
value	O
are	O
shrunken	O
entirely	O
to	O
zero	O
.	O
the	O
type	O
of	O
shrink-	O
age	O
performed	O
by	O
the	O
lasso	B
in	O
this	O
simple	B
setting	O
(	O
6.15	O
)	O
is	O
known	O
as	O
soft-	O
thresholding	O
.	O
the	O
fact	O
that	O
some	O
lasso	B
coeﬃcients	O
are	O
shrunken	O
entirely	O
to	O
zero	O
explains	O
why	O
the	O
lasso	B
performs	O
feature	B
selection	I
.	O
in	O
the	O
case	O
of	O
a	O
more	O
general	O
data	B
matrix	O
x	O
,	O
the	O
story	O
is	O
a	O
little	O
more	O
complicated	O
than	O
what	O
is	O
depicted	O
in	O
figure	O
6.10	O
,	O
but	O
the	O
main	O
ideas	O
still	O
hold	O
approximately	O
:	O
ridge	B
regression	I
more	O
or	O
less	O
shrinks	O
every	O
dimension	O
of	O
the	O
data	B
by	O
the	O
same	O
proportion	O
,	O
whereas	O
the	O
lasso	B
more	O
or	O
less	O
shrinks	O
all	O
coeﬃcients	O
toward	O
zero	O
by	O
a	O
similar	O
amount	O
,	O
and	O
suﬃciently	O
small	O
co-	O
eﬃcients	O
are	O
shrunken	O
all	O
the	O
way	O
to	O
zero	O
.	O
soft-	O
thresholding	O
226	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
e	O
t	O
a	O
m	O
i	O
t	O
s	O
e	O
i	O
t	O
n	O
e	O
c	O
i	O
f	O
f	O
e	O
o	O
c	O
5	O
.	O
1	O
5	O
.	O
0	O
5	O
.	O
0	O
−	O
5	O
.	O
1	O
−	O
ridge	O
least	O
squares	O
e	O
t	O
a	O
m	O
i	O
t	O
s	O
e	O
i	O
t	O
n	O
e	O
c	O
i	O
f	O
f	O
e	O
o	O
c	O
lasso	B
least	O
squares	O
5	O
.	O
1	O
5	O
.	O
0	O
5	O
.	O
0	O
−	O
5	O
.	O
1	O
−	O
−1.5	O
−0.5	O
0.0	O
0.5	O
1.0	O
1.5	O
−1.5	O
−0.5	O
0.0	O
0.5	O
1.0	O
1.5	O
yj	O
yj	O
figure	O
6.10.	O
the	O
ridge	B
regression	I
and	O
lasso	B
coeﬃcient	O
estimates	O
for	O
a	O
simple	B
setting	O
with	O
n	O
=	O
p	O
and	O
x	O
a	O
diagonal	O
matrix	O
with	O
1	O
’	O
s	O
on	O
the	O
diagonal	O
.	O
left	O
:	O
the	O
ridge	B
regression	I
coeﬃcient	O
estimates	O
are	O
shrunken	O
proportionally	O
towards	O
zero	O
,	O
relative	O
to	O
the	O
least	B
squares	I
estimates	O
.	O
right	O
:	O
the	O
lasso	B
coeﬃcient	O
estimates	O
are	O
soft-thresholded	O
towards	O
zero	O
.	O
bayesian	O
interpretation	O
for	O
ridge	O
regression	B
and	O
the	O
lasso	B
we	O
now	O
show	O
that	O
one	O
can	O
view	O
ridge	B
regression	I
and	O
the	O
lasso	B
through	O
a	O
bayesian	O
lens	O
.	O
a	O
bayesian	O
viewpoint	O
for	O
regression	O
assumes	O
that	O
the	O
coeﬃcient	B
vector	O
β	O
has	O
some	O
prior	B
distribution	O
,	O
say	O
p	O
(	O
β	O
)	O
,	O
where	O
β	O
=	O
(	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
)	O
t	O
.	O
the	O
likelihood	O
of	O
the	O
data	B
can	O
be	O
written	O
as	O
f	O
(	O
y	O
|x	O
,	O
β	O
)	O
,	O
where	O
x	O
=	O
(	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
)	O
.	O
multiplying	O
the	O
prior	B
distribution	O
by	O
the	O
likeli-	O
hood	O
gives	O
us	O
(	O
up	O
to	O
a	O
proportionality	O
constant	O
)	O
the	O
posterior	B
distribution	O
,	O
which	O
takes	O
the	O
form	O
posterior	B
distribution	O
p	O
(	O
β|x	O
,	O
y	O
)	O
∝	O
f	O
(	O
y	O
|x	O
,	O
β	O
)	O
p	O
(	O
β|x	O
)	O
=	O
f	O
(	O
y	O
|x	O
,	O
β	O
)	O
p	O
(	O
β	O
)	O
,	O
where	O
the	O
proportionality	O
above	O
follows	O
from	O
bayes	O
’	O
theorem	O
,	O
and	O
the	O
equality	O
above	O
follows	O
from	O
the	O
assumption	O
that	O
x	O
is	O
ﬁxed	O
.	O
we	O
assume	O
the	O
usual	O
linear	B
model	I
,	O
y	O
=	O
β0	O
+	O
x1β1	O
+	O
.	O
.	O
.	O
+	O
xpβp	O
+	O
	O
,	O
and	O
suppose	O
that	O
the	O
errors	O
are	O
independent	B
and	O
drawn	O
from	O
a	O
normal	O
dis-	O
p	O
tribution	O
.	O
furthermore	O
,	O
assume	O
that	O
p	O
(	O
β	O
)	O
=	O
j=1	O
g	O
(	O
βj	O
)	O
,	O
for	O
some	O
density	B
function	I
g.	O
it	O
turns	O
out	O
that	O
ridge	B
regression	I
and	O
the	O
lasso	B
follow	O
naturally	O
from	O
two	O
special	O
cases	O
of	O
g	O
:	O
+	O
•	O
if	O
g	O
is	O
a	O
gaussian	O
distribution	B
with	O
mean	O
zero	O
and	O
standard	O
deviation	O
a	O
function	B
of	O
λ	O
,	O
then	O
it	O
follows	O
that	O
the	O
posterior	B
mode	O
for	O
β—that	O
is	O
,	O
the	O
most	O
likely	O
value	O
for	O
β	O
,	O
given	O
the	O
data—is	O
given	O
by	O
the	O
ridge	B
regression	I
solution	O
.	O
(	O
in	O
fact	O
,	O
the	O
ridge	B
regression	I
solution	O
is	O
also	O
the	O
posterior	B
mean	O
.	O
)	O
posterior	B
mode	O
6.2	O
shrinkage	B
methods	O
227	O
)	O
j	O
β	O
(	O
7	O
0	O
.	O
6	O
.	O
0	O
5	O
.	O
0	O
4	O
.	O
0	O
3	O
.	O
0	O
2	O
0	O
.	O
1	O
.	O
0	O
0	O
.	O
0	O
)	O
j	O
β	O
(	O
7	O
0	O
.	O
6	O
.	O
0	O
5	O
.	O
0	O
4	O
.	O
0	O
3	O
.	O
0	O
2	O
0	O
.	O
1	O
.	O
0	O
0	O
.	O
0	O
−3	O
−2	O
−1	O
0	O
βj	O
1	O
2	O
3	O
−3	O
−2	O
−1	O
1	O
2	O
3	O
0	O
βj	O
figure	O
6.11.	O
left	O
:	O
ridge	B
regression	I
is	O
the	O
posterior	B
mode	O
for	O
β	O
under	O
a	O
gaus-	O
sian	O
prior	B
.	O
right	O
:	O
the	O
lasso	B
is	O
the	O
posterior	B
mode	O
for	O
β	O
under	O
a	O
double-exponential	O
prior	O
.	O
•	O
if	O
g	O
is	O
a	O
double-exponential	O
(	O
laplace	O
)	O
distribution	B
with	O
mean	O
zero	O
and	O
scale	O
parameter	O
a	O
function	B
of	O
λ	O
,	O
then	O
it	O
follows	O
that	O
the	O
posterior	B
mode	O
for	O
β	O
is	O
the	O
lasso	B
solution	O
.	O
(	O
however	O
,	O
the	O
lasso	B
solution	O
is	O
not	O
the	O
posterior	B
mean	O
,	O
and	O
in	O
fact	O
,	O
the	O
posterior	B
mean	O
does	O
not	O
yield	O
a	O
sparse	B
coeﬃcient	O
vector	B
.	O
)	O
the	O
gaussian	O
and	O
double-exponential	O
priors	O
are	O
displayed	O
in	O
figure	O
6.11.	O
therefore	O
,	O
from	O
a	O
bayesian	O
viewpoint	O
,	O
ridge	B
regression	I
and	O
the	O
lasso	B
follow	O
directly	O
from	O
assuming	O
the	O
usual	O
linear	B
model	I
with	O
normal	O
errors	O
,	O
together	O
with	O
a	O
simple	B
prior	O
distribution	B
for	O
β.	O
notice	O
that	O
the	O
lasso	B
prior	O
is	O
steeply	O
peaked	O
at	O
zero	O
,	O
while	O
the	O
gaussian	O
is	O
ﬂatter	O
and	O
fatter	O
at	O
zero	O
.	O
hence	O
,	O
the	O
lasso	B
expects	O
a	O
priori	O
that	O
many	O
of	O
the	O
coeﬃcients	O
are	O
(	O
exactly	O
)	O
zero	O
,	O
while	O
ridge	O
assumes	O
the	O
coeﬃcients	O
are	O
randomly	O
distributed	O
about	O
zero	O
.	O
6.2.3	O
selecting	O
the	O
tuning	B
parameter	I
just	O
as	O
the	O
subset	B
selection	I
approaches	O
considered	O
in	O
section	O
6.1	O
require	O
a	O
method	O
to	O
determine	O
which	O
of	O
the	O
models	O
under	O
consideration	O
is	O
best	O
,	O
implementing	O
ridge	B
regression	I
and	O
the	O
lasso	B
requires	O
a	O
method	O
for	O
selecting	O
a	O
value	O
for	O
the	O
tuning	B
parameter	I
λ	O
in	O
(	O
6.5	O
)	O
and	O
(	O
6.7	O
)	O
,	O
or	O
equivalently	O
,	O
the	O
value	O
of	O
the	O
constraint	O
s	O
in	O
(	O
6.9	O
)	O
and	O
(	O
6.8	O
)	O
.	O
cross-validation	B
provides	O
a	O
sim-	O
ple	O
way	O
to	O
tackle	O
this	O
problem	O
.	O
we	O
choose	O
a	O
grid	O
of	O
λ	O
values	O
,	O
and	O
compute	O
the	O
cross-validation	B
error	O
for	O
each	O
value	O
of	O
λ	O
,	O
as	O
described	O
in	O
chapter	O
5.	O
we	O
then	O
select	O
the	O
tuning	B
parameter	I
value	O
for	O
which	O
the	O
cross-validation	B
error	O
is	O
smallest	O
.	O
finally	O
,	O
the	O
model	B
is	O
re-ﬁt	O
using	O
all	O
of	O
the	O
available	O
observations	B
and	O
the	O
selected	O
value	O
of	O
the	O
tuning	B
parameter	I
.	O
figure	O
6.12	O
displays	O
the	O
choice	O
of	O
λ	O
that	O
results	O
from	O
performing	O
leave-	O
one-out	O
cross-validation	B
on	O
the	O
ridge	B
regression	I
ﬁts	O
from	O
the	O
credit	O
data	B
set	O
.	O
the	O
dashed	O
vertical	O
lines	O
indicate	O
the	O
selected	O
value	O
of	O
λ.	O
in	O
this	O
case	O
the	O
value	O
is	O
relatively	O
small	O
,	O
indicating	O
that	O
the	O
optimal	O
ﬁt	O
only	O
involves	O
a	O
228	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
d	O
i	O
l	O
a	O
v	O
−	O
s	O
s	O
o	O
r	O
c	O
6	O
.	O
5	O
2	O
4	O
.	O
5	O
2	O
2	O
.	O
5	O
2	O
0	O
.	O
5	O
2	O
s	O
t	O
i	O
n	O
e	O
c	O
i	O
f	O
f	O
i	O
e	O
o	O
c	O
d	O
e	O
z	O
d	O
r	O
a	O
d	O
n	O
a	O
s	O
t	O
0	O
0	O
3	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
−	O
0	O
0	O
3	O
−	O
5e−03	O
5e−02	O
5e−01	O
5e+00	O
5e−03	O
5e−02	O
5e−01	O
5e+00	O
λ	O
λ	O
figure	O
6.12.	O
left	O
:	O
cross-validation	B
errors	O
that	O
result	O
from	O
applying	O
ridge	B
regression	I
to	O
the	O
credit	O
data	B
set	O
with	O
various	O
value	O
of	O
λ.	O
right	O
:	O
the	O
coeﬃcient	B
estimates	O
as	O
a	O
function	B
of	O
λ.	O
the	O
vertical	O
dashed	O
lines	O
indicate	O
the	O
value	O
of	O
λ	O
selected	O
by	O
cross-validation	B
.	O
small	O
amount	O
of	O
shrinkage	B
relative	O
to	O
the	O
least	B
squares	I
solution	O
.	O
in	O
addition	O
,	O
the	O
dip	O
is	O
not	O
very	O
pronounced	O
,	O
so	O
there	O
is	O
rather	O
a	O
wide	O
range	O
of	O
values	O
that	O
would	O
give	O
very	O
similar	O
error	B
.	O
in	O
a	O
case	O
like	O
this	O
we	O
might	O
simply	O
use	O
the	O
least	B
squares	I
solution	O
.	O
figure	O
6.13	O
provides	O
an	O
illustration	O
of	O
ten-fold	O
cross-validation	B
applied	O
to	O
the	O
lasso	B
ﬁts	O
on	O
the	O
sparse	B
simulated	O
data	B
from	O
figure	O
6.9.	O
the	O
left-hand	O
panel	O
of	O
figure	O
6.13	O
displays	O
the	O
cross-validation	B
error	O
,	O
while	O
the	O
right-hand	O
panel	O
displays	O
the	O
coeﬃcient	B
estimates	O
.	O
the	O
vertical	O
dashed	O
lines	O
indicate	O
the	O
point	O
at	O
which	O
the	O
cross-validation	B
error	O
is	O
smallest	O
.	O
the	O
two	O
colored	O
lines	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
6.13	O
represent	O
the	O
two	O
predictors	O
that	O
are	O
related	O
to	O
the	O
response	B
,	O
while	O
the	O
grey	O
lines	O
represent	O
the	O
unre-	O
lated	O
predictors	O
;	O
these	O
are	O
often	O
referred	O
to	O
as	O
signal	B
and	O
noise	B
variables	O
,	O
respectively	O
.	O
not	O
only	O
has	O
the	O
lasso	B
correctly	O
given	O
much	O
larger	O
coeﬃ-	O
cient	O
estimates	O
to	O
the	O
two	O
signal	B
predictors	O
,	O
but	O
also	O
the	O
minimum	O
cross-	O
validation	O
error	O
corresponds	O
to	O
a	O
set	B
of	O
coeﬃcient	B
estimates	O
for	O
which	O
only	O
the	O
signal	B
variables	O
are	O
non-zero	O
.	O
hence	O
cross-validation	B
together	O
with	O
the	O
lasso	B
has	O
correctly	O
identiﬁed	O
the	O
two	O
signal	B
variables	O
in	O
the	O
model	B
,	O
even	O
though	O
this	O
is	O
a	O
challenging	O
setting	O
,	O
with	O
p	O
=	O
45	O
variables	O
and	O
only	O
n	O
=	O
50	O
observations	B
.	O
in	O
contrast	B
,	O
the	O
least	B
squares	I
solution—displayed	O
on	O
the	O
far	O
right	O
of	O
the	O
right-hand	O
panel	O
of	O
figure	O
6.13—assigns	O
a	O
large	O
coeﬃcient	B
estimate	O
to	O
only	O
one	O
of	O
the	O
two	O
signal	B
variables	O
.	O
signal	B
6.3	O
dimension	B
reduction	I
methods	O
the	O
methods	O
that	O
we	O
have	O
discussed	O
so	O
far	O
in	O
this	O
chapter	O
have	O
controlled	O
variance	B
in	O
two	O
diﬀerent	O
ways	O
,	O
either	O
by	O
using	O
a	O
subset	O
of	O
the	O
original	O
vari-	O
ables	O
,	O
or	O
by	O
shrinking	O
their	O
coeﬃcients	O
toward	O
zero	O
.	O
all	O
of	O
these	O
methods	O
6.3	O
dimension	B
reduction	I
methods	O
229	O
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
d	O
i	O
l	O
a	O
v	O
−	O
s	O
s	O
o	O
r	O
c	O
0	O
0	O
4	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
6	O
0	O
0	O
2	O
0	O
0.0	O
0.2	O
s	O
t	O
i	O
n	O
e	O
c	O
i	O
f	O
f	O
i	O
e	O
o	O
c	O
d	O
e	O
z	O
d	O
r	O
a	O
d	O
n	O
a	O
s	O
t	O
5	O
1	O
0	O
1	O
5	O
0	O
5	O
−	O
0.8	O
1.0	O
0.0	O
0.2	O
0.4	O
0.6	O
ˆβl	O
λ	O
1/	O
ˆβ	O
1	O
0.6	O
0.4	O
ˆβl	O
λ	O
1/	O
ˆβ	O
1	O
0.8	O
1.0	O
dimension	B
reduction	I
linear	O
combination	O
figure	O
6.13.	O
left	O
:	O
ten-fold	O
cross-validation	B
mse	O
for	O
the	O
lasso	B
,	O
applied	O
to	O
the	O
sparse	B
simulated	O
data	B
set	O
from	O
figure	O
6.9.	O
right	O
:	O
the	O
corresponding	O
lasso	B
coeﬃcient	O
estimates	O
are	O
displayed	O
.	O
the	O
vertical	O
dashed	O
lines	O
indicate	O
the	O
lasso	B
ﬁt	O
for	O
which	O
the	O
cross-validation	B
error	O
is	O
smallest	O
.	O
are	O
deﬁned	O
using	O
the	O
original	O
predictors	O
,	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
.	O
we	O
now	O
explore	O
a	O
class	O
of	O
approaches	O
that	O
transform	O
the	O
predictors	O
and	O
then	O
ﬁt	B
a	O
least	B
squares	I
model	O
using	O
the	O
transformed	O
variables	O
.	O
we	O
will	O
refer	O
to	O
these	O
tech-	O
niques	O
as	O
dimension	B
reduction	I
methods	O
.	O
let	O
z1	O
,	O
z2	O
,	O
.	O
.	O
.	O
,	O
zm	O
represent	O
m	O
<	O
p	O
linear	B
combinations	O
of	O
our	O
original	O
p	O
predictors	O
.	O
that	O
is	O
,	O
p	O
(	O
cid:17	O
)	O
zm	O
=	O
φjmxj	O
j=1	O
(	O
6.16	O
)	O
for	O
some	O
constants	O
φ1m	O
,	O
φ2m	O
.	O
.	O
.	O
,	O
φpm	O
,	O
m	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
.	O
we	O
can	O
then	O
ﬁt	B
the	O
linear	B
regression	I
model	O
m	O
(	O
cid:17	O
)	O
yi	O
=	O
θ0	O
+	O
θmzim	O
+	O
i	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
(	O
6.17	O
)	O
m=1	O
using	O
least	B
squares	I
.	O
note	O
that	O
in	O
(	O
6.17	O
)	O
,	O
the	O
regression	B
coeﬃcients	O
are	O
given	O
by	O
θ0	O
,	O
θ1	O
,	O
.	O
.	O
.	O
,	O
θm	O
.	O
if	O
the	O
constants	O
φ1m	O
,	O
φ2m	O
,	O
.	O
.	O
.	O
,	O
φpm	O
are	O
chosen	O
wisely	O
,	O
then	O
such	O
dimension	B
reduction	I
approaches	O
can	O
often	O
outperform	O
least	B
squares	I
regression	O
.	O
in	O
other	O
words	O
,	O
ﬁtting	O
(	O
6.17	O
)	O
using	O
least	B
squares	I
can	O
lead	O
to	O
better	O
results	O
than	O
ﬁtting	O
(	O
6.1	O
)	O
using	O
least	B
squares	I
.	O
the	O
term	B
dimension	O
reduction	O
comes	O
from	O
the	O
fact	O
that	O
this	O
approach	B
reduces	O
the	O
problem	O
of	O
estimating	O
the	O
p	O
+	O
1	O
coeﬃcients	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
to	O
the	O
simpler	O
problem	O
of	O
estimating	O
the	O
m	O
+	O
1	O
coeﬃcients	O
θ0	O
,	O
θ1	O
,	O
.	O
.	O
.	O
,	O
θm	O
,	O
where	O
m	O
<	O
p.	O
in	O
other	O
words	O
,	O
the	O
dimension	O
of	O
the	O
problem	O
has	O
been	O
reduced	O
from	O
p	O
+	O
1	O
to	O
m	O
+	O
1.	O
notice	O
that	O
from	O
(	O
6.16	O
)	O
,	O
m	O
(	O
cid:17	O
)	O
m	O
(	O
cid:17	O
)	O
p	O
(	O
cid:17	O
)	O
p	O
(	O
cid:17	O
)	O
m	O
(	O
cid:17	O
)	O
p	O
(	O
cid:17	O
)	O
θmzim	O
=	O
θm	O
φjmxij	O
=	O
θmφjmxij	O
=	O
βjxij	O
,	O
m=1	O
m=1	O
j=1	O
j=1	O
m=1	O
j=1	O
230	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
i	O
g	O
n	O
d	O
n	O
e	O
p	O
s	O
d	O
a	O
5	O
3	O
0	O
3	O
5	O
2	O
0	O
2	O
5	O
1	O
0	O
1	O
5	O
0	O
10	O
20	O
30	O
40	O
50	O
60	O
70	O
population	O
figure	O
6.14.	O
the	O
population	O
size	O
(	O
pop	O
)	O
and	O
ad	O
spending	O
(	O
ad	O
)	O
for	O
100	O
diﬀerent	O
cities	O
are	O
shown	O
as	O
purple	O
circles	O
.	O
the	O
green	O
solid	O
line	B
indicates	O
the	O
ﬁrst	O
principal	O
component	O
,	O
and	O
the	O
blue	O
dashed	O
line	B
indicates	O
the	O
second	O
principal	O
component	O
.	O
where	O
m	O
(	O
cid:17	O
)	O
βj	O
=	O
θmφjm	O
.	O
m=1	O
(	O
6.18	O
)	O
hence	O
(	O
6.17	O
)	O
can	O
be	O
thought	O
of	O
as	O
a	O
special	O
case	O
of	O
the	O
original	O
linear	B
regression	I
model	O
given	O
by	O
(	O
6.1	O
)	O
.	O
dimension	B
reduction	I
serves	O
to	O
constrain	O
the	O
estimated	O
βj	O
coeﬃcients	O
,	O
since	O
now	O
they	O
must	O
take	O
the	O
form	O
(	O
6.18	O
)	O
.	O
this	O
constraint	O
on	O
the	O
form	O
of	O
the	O
coeﬃcients	O
has	O
the	O
potential	O
to	O
bias	B
the	O
coeﬃcient	B
estimates	O
.	O
however	O
,	O
in	O
situations	O
where	O
p	O
is	O
large	O
relative	O
to	O
n	O
,	O
selecting	O
a	O
value	O
of	O
m	O
(	O
cid:15	O
)	O
p	O
can	O
signiﬁcantly	O
reduce	O
the	O
variance	B
of	O
the	O
ﬁtted	O
coeﬃcients	O
.	O
if	O
m	O
=	O
p	O
,	O
and	O
all	O
the	O
zm	O
are	O
linearly	O
independent	B
,	O
then	O
(	O
6.18	O
)	O
poses	O
no	O
constraints	O
.	O
in	O
this	O
case	O
,	O
no	O
dimension	B
reduction	I
occurs	O
,	O
and	O
so	O
ﬁtting	O
(	O
6.17	O
)	O
is	O
equivalent	O
to	O
performing	O
least	B
squares	I
on	O
the	O
original	O
p	O
predictors	O
.	O
all	O
dimension	B
reduction	I
methods	O
work	O
in	O
two	O
steps	O
.	O
first	O
,	O
the	O
trans-	O
formed	O
predictors	O
z1	O
,	O
z2	O
,	O
.	O
.	O
.	O
,	O
zm	O
are	O
obtained	O
.	O
second	O
,	O
the	O
model	B
is	O
ﬁt	B
using	O
these	O
m	O
predictors	O
.	O
however	O
,	O
the	O
choice	O
of	O
z1	O
,	O
z2	O
,	O
.	O
.	O
.	O
,	O
zm	O
,	O
or	O
equiv-	O
alently	O
,	O
the	O
selection	B
of	O
the	O
φjm	O
’	O
s	O
,	O
can	O
be	O
achieved	O
in	O
diﬀerent	O
ways	O
.	O
in	O
this	O
chapter	O
,	O
we	O
will	O
consider	O
two	O
approaches	O
for	O
this	O
task	O
:	O
principal	B
components	I
and	O
partial	B
least	I
squares	I
.	O
6.3.1	O
principal	B
components	I
regression	O
principal	B
components	I
analysis	O
(	O
pca	O
)	O
is	O
a	O
popular	O
approach	B
for	O
deriving	O
a	O
low-dimensional	B
set	O
of	O
features	O
from	O
a	O
large	O
set	B
of	O
variables	O
.	O
pca	O
is	O
discussed	O
in	O
greater	O
detail	O
as	O
a	O
tool	O
for	O
unsupervised	O
learning	O
in	O
chapter	O
10.	O
here	O
we	O
describe	O
its	O
use	O
as	O
a	O
dimension	B
reduction	I
technique	O
for	O
regression	O
.	O
principal	B
components	I
analysis	O
6.3	O
dimension	B
reduction	I
methods	O
231	O
an	O
overview	O
of	O
principal	B
components	I
analysis	O
pca	O
is	O
a	O
technique	O
for	O
reducing	O
the	O
dimension	O
of	O
a	O
n	O
×	O
p	O
data	B
matrix	O
x.	O
the	O
ﬁrst	O
principal	O
component	O
direction	O
of	O
the	O
data	B
is	O
that	O
along	O
which	O
the	O
observations	B
vary	O
the	O
most	O
.	O
for	O
instance	O
,	O
consider	O
figure	O
6.14	O
,	O
which	O
shows	O
population	O
size	O
(	O
pop	O
)	O
in	O
tens	O
of	O
thousands	O
of	O
people	O
,	O
and	O
ad	O
spending	O
for	O
a	O
particular	O
company	O
(	O
ad	O
)	O
in	O
thousands	O
of	O
dollars	O
,	O
for	O
100	O
cities	O
.	O
the	O
green	O
solid	O
line	B
represents	O
the	O
ﬁrst	O
principal	O
component	O
direction	O
of	O
the	O
data	B
.	O
we	O
can	O
see	O
by	O
eye	O
that	O
this	O
is	O
the	O
direction	O
along	O
which	O
there	O
is	O
the	O
greatest	O
variability	O
in	O
the	O
data	B
.	O
that	O
is	O
,	O
if	O
we	O
projected	O
the	O
100	O
observations	B
onto	O
this	O
line	B
(	O
as	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
6.15	O
)	O
,	O
then	O
the	O
resulting	O
projected	O
observations	B
would	O
have	O
the	O
largest	O
possible	O
variance	B
;	O
projecting	O
the	O
observations	B
onto	O
any	O
other	O
line	B
would	O
yield	O
projected	O
observations	B
with	O
lower	O
variance	B
.	O
projecting	O
a	O
point	O
onto	O
a	O
line	B
simply	O
involves	O
ﬁnding	O
the	O
location	O
on	O
the	O
line	B
which	O
is	O
closest	O
to	O
the	O
point	O
.	O
the	O
ﬁrst	O
principal	O
component	O
is	O
displayed	O
graphically	O
in	O
figure	O
6.14	O
,	O
but	O
how	O
can	O
it	O
be	O
summarized	O
mathematically	O
?	O
it	O
is	O
given	O
by	O
the	O
formula	O
z1	O
=	O
0.839	O
×	O
(	O
pop	O
−	O
pop	O
)	O
+	O
0.544	O
×	O
(	O
ad	O
−	O
ad	O
)	O
.	O
(	O
6.19	O
)	O
here	O
φ11	O
=	O
0.839	O
and	O
φ21	O
=	O
0.544	O
are	O
the	O
principal	O
component	O
loadings	O
,	O
which	O
deﬁne	O
the	O
direction	O
referred	O
to	O
above	O
.	O
in	O
(	O
6.19	O
)	O
,	O
pop	O
indicates	O
the	O
mean	O
of	O
all	O
pop	O
values	O
in	O
this	O
data	B
set	O
,	O
and	O
ad	O
indicates	O
the	O
mean	O
of	O
all	O
ad-	O
vertising	O
spending	O
.	O
the	O
idea	O
is	O
that	O
out	O
of	O
every	O
possible	O
linear	B
combination	I
of	O
pop	O
and	O
ad	O
such	O
that	O
φ2	O
21	O
=	O
1	O
,	O
this	O
particular	O
linear	B
combination	I
yields	O
the	O
highest	O
variance	B
:	O
i.e	O
.	O
this	O
is	O
the	O
linear	B
combination	I
for	O
which	O
var	O
(	O
φ11	O
×	O
(	O
pop	O
−	O
pop	O
)	O
+	O
φ21	O
×	O
(	O
ad	O
−	O
ad	O
)	O
)	O
is	O
maximized	O
.	O
it	O
is	O
necessary	O
to	O
consider	O
only	O
linear	B
combinations	O
of	O
the	O
form	O
φ2	O
21	O
=	O
1	O
,	O
since	O
otherwise	O
we	O
could	O
increase	O
φ11	O
and	O
φ21	O
arbitrarily	O
in	O
order	O
to	O
blow	O
up	O
the	O
variance	B
.	O
in	O
(	O
6.19	O
)	O
,	O
the	O
two	O
loadings	O
are	O
both	O
positive	O
and	O
have	O
similar	O
size	O
,	O
and	O
so	O
z1	O
is	O
almost	O
an	O
average	B
of	O
the	O
two	O
variables	O
.	O
11	O
+	O
φ2	O
11	O
+φ2	O
since	O
n	O
=	O
100	O
,	O
pop	O
and	O
ad	O
are	O
vectors	O
of	O
length	O
100	O
,	O
and	O
so	O
is	O
z1	O
in	O
(	O
6.19	O
)	O
.	O
for	O
instance	O
,	O
zi1	O
=	O
0.839	O
×	O
(	O
popi	O
−	O
pop	O
)	O
+	O
0.544	O
×	O
(	O
adi	O
−	O
ad	O
)	O
.	O
(	O
6.20	O
)	O
the	O
values	O
of	O
z11	O
,	O
.	O
.	O
.	O
,	O
zn1	O
are	O
known	O
as	O
the	O
principal	O
component	O
scores	O
,	O
and	O
can	O
be	O
seen	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
6.15.	O
there	O
is	O
also	O
another	O
interpretation	O
for	O
pca	O
:	O
the	O
ﬁrst	O
principal	O
compo-	O
nent	O
vector	B
deﬁnes	O
the	O
line	B
that	O
is	O
as	O
close	O
as	O
possible	O
to	O
the	O
data	B
.	O
for	O
instance	O
,	O
in	O
figure	O
6.14	O
,	O
the	O
ﬁrst	O
principal	O
component	O
line	B
minimizes	O
the	O
sum	O
of	O
the	O
squared	O
perpendicular	B
distances	O
between	O
each	O
point	O
and	O
the	O
line	B
.	O
these	O
distances	O
are	O
plotted	O
as	O
dashed	O
line	B
segments	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
6.15	O
,	O
in	O
which	O
the	O
crosses	O
represent	O
the	O
projection	B
of	O
each	O
point	O
onto	O
the	O
ﬁrst	O
principal	O
component	O
line	B
.	O
the	O
ﬁrst	O
principal	O
component	O
has	O
been	O
chosen	O
so	O
that	O
the	O
projected	O
observations	B
are	O
as	O
close	O
as	O
possible	O
to	O
the	O
original	O
observations	B
.	O
232	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
i	O
g	O
n	O
d	O
n	O
e	O
p	O
s	O
d	O
a	O
0	O
3	O
5	O
2	O
0	O
2	O
5	O
1	O
0	O
1	O
5	O
t	O
n	O
e	O
n	O
o	O
p	O
m	O
o	O
c	O
l	O
i	O
a	O
p	O
c	O
n	O
i	O
r	O
p	O
d	O
n	O
2	O
0	O
1	O
5	O
0	O
5	O
−	O
0	O
1	O
−	O
20	O
30	O
40	O
50	O
−20	O
−10	O
0	O
10	O
20	O
population	O
1st	O
principal	O
component	O
figure	O
6.15.	O
a	O
subset	O
of	O
the	O
advertising	O
data	B
.	O
the	O
mean	O
pop	O
and	O
ad	O
budgets	O
are	O
indicated	O
with	O
a	O
blue	O
circle	O
.	O
left	O
:	O
the	O
ﬁrst	O
principal	O
component	O
direction	O
is	O
shown	O
in	O
green	O
.	O
it	O
is	O
the	O
dimension	O
along	O
which	O
the	O
data	B
vary	O
the	O
most	O
,	O
and	O
it	O
also	O
deﬁnes	O
the	O
line	B
that	O
is	O
closest	O
to	O
all	O
n	O
of	O
the	O
observations	B
.	O
the	O
distances	O
from	O
each	O
observation	O
to	O
the	O
principal	O
component	O
are	O
represented	O
using	O
the	O
black	O
dashed	O
line	B
segments	O
.	O
the	O
blue	O
dot	O
represents	O
(	O
pop	O
,	O
ad	O
)	O
.	O
right	O
:	O
the	O
left-hand	O
panel	O
has	O
been	O
rotated	O
so	O
that	O
the	O
ﬁrst	O
principal	O
component	O
direction	O
coincides	O
with	O
the	O
x-axis	O
.	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
6.15	O
,	O
the	O
left-hand	O
panel	O
has	O
been	O
rotated	O
so	O
that	O
the	O
ﬁrst	O
principal	O
component	O
direction	O
coincides	O
with	O
the	O
x-axis	O
.	O
it	O
is	O
possible	O
to	O
show	O
that	O
the	O
ﬁrst	O
principal	O
component	O
score	O
for	O
the	O
ith	O
observation	O
,	O
given	O
in	O
(	O
6.20	O
)	O
,	O
is	O
the	O
distance	B
in	O
the	O
x-direction	O
of	O
the	O
ith	O
cross	O
from	O
zero	O
.	O
so	O
for	O
example	O
,	O
the	O
point	O
in	O
the	O
bottom-left	O
corner	O
of	O
the	O
left-hand	O
panel	O
of	O
figure	O
6.15	O
has	O
a	O
large	O
negative	O
principal	O
component	O
score	O
,	O
zi1	O
=	O
−26.1	O
,	O
while	O
the	O
point	O
in	O
the	O
top-right	O
corner	O
has	O
a	O
large	O
positive	O
score	O
,	O
zi1	O
=	O
18.7.	O
these	O
scores	O
can	O
be	O
computed	O
directly	O
using	O
(	O
6.20	O
)	O
.	O
we	O
can	O
think	O
of	O
the	O
values	O
of	O
the	O
principal	O
component	O
z1	O
as	O
single-	O
number	O
summaries	O
of	O
the	O
joint	O
pop	O
and	O
ad	O
budgets	O
for	O
each	O
location	O
.	O
in	O
this	O
example	O
,	O
if	O
zi1	O
=	O
0.839	O
×	O
(	O
popi	O
−	O
pop	O
)	O
+	O
0.544	O
×	O
(	O
adi	O
−	O
ad	O
)	O
<	O
0	O
,	O
then	O
this	O
indicates	O
a	O
city	O
with	O
below-average	O
population	O
size	O
and	O
below-	O
average	B
ad	O
spending	O
.	O
a	O
positive	O
score	O
suggests	O
the	O
opposite	O
.	O
how	O
well	O
can	O
a	O
single	B
number	O
represent	O
both	O
pop	O
and	O
ad	O
?	O
in	O
this	O
case	O
,	O
figure	O
6.14	O
indicates	O
that	O
pop	O
and	O
ad	O
have	O
approximately	O
a	O
linear	B
relationship	O
,	O
and	O
so	O
we	O
might	O
expect	O
that	O
a	O
single-number	O
summary	O
will	O
work	O
well	O
.	O
figure	O
6.16	O
displays	O
zi1	O
versus	O
both	O
pop	O
and	O
ad	O
.	O
the	O
plots	O
show	O
a	O
strong	O
relationship	O
between	O
the	O
ﬁrst	O
principal	O
component	O
and	O
the	O
two	O
features	O
.	O
in	O
other	O
words	O
,	O
the	O
ﬁrst	O
principal	O
component	O
appears	O
to	O
capture	O
most	O
of	O
the	O
information	O
contained	O
in	O
the	O
pop	O
and	O
ad	O
predictors	O
.	O
4	O
so	O
far	O
we	O
have	O
concentrated	O
on	O
the	O
ﬁrst	O
principal	O
component	O
.	O
in	O
gen-	O
eral	O
,	O
one	O
can	O
construct	O
up	O
to	O
p	O
distinct	O
principal	B
components	I
.	O
the	O
second	O
principal	O
component	O
z2	O
is	O
a	O
linear	B
combination	I
of	O
the	O
variables	O
that	O
is	O
un-	O
correlated	O
with	O
z1	O
,	O
and	O
has	O
largest	O
variance	B
subject	O
to	O
this	O
constraint	O
.	O
the	O
second	O
principal	O
component	O
direction	O
is	O
illustrated	O
as	O
a	O
dashed	O
blue	O
line	B
in	O
figure	O
6.14.	O
it	O
turns	O
out	O
that	O
the	O
zero	O
correlation	B
condition	O
of	O
z1	O
with	O
z2	O
4the	O
principal	B
components	I
were	O
calculated	O
after	O
first	O
standardizing	O
both	O
pop	O
and	O
ad	O
,	O
a	O
common	O
approach	B
.	O
hence	O
,	O
the	O
x-axes	O
on	O
figures	O
6.15	O
and	O
6.16	O
are	O
not	O
on	O
the	O
same	O
scale	O
.	O
6.3	O
dimension	B
reduction	I
methods	O
233	O
0	O
6	O
0	O
5	O
0	O
4	O
0	O
3	O
0	O
2	O
n	O
o	O
i	O
t	O
l	O
a	O
u	O
p	O
o	O
p	O
i	O
g	O
n	O
d	O
n	O
e	O
p	O
s	O
d	O
a	O
0	O
3	O
5	O
2	O
0	O
2	O
5	O
1	O
0	O
1	O
5	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
1st	O
principal	O
component	O
1st	O
principal	O
component	O
figure	O
6.16.	O
plots	O
of	O
the	O
ﬁrst	O
principal	O
component	O
scores	O
zi1	O
versus	O
pop	O
and	O
ad	O
.	O
the	O
relationships	O
are	O
strong	O
.	O
is	O
equivalent	O
to	O
the	O
condition	O
that	O
the	O
direction	O
must	O
be	O
perpendicular	B
,	O
or	O
orthogonal	B
,	O
to	O
the	O
ﬁrst	O
principal	O
component	O
direction	O
.	O
the	O
second	O
principal	O
component	O
is	O
given	O
by	O
the	O
formula	O
z2	O
=	O
0.544	O
×	O
(	O
pop	O
−	O
pop	O
)	O
−	O
0.839	O
×	O
(	O
ad	O
−	O
ad	O
)	O
.	O
perpendicular	B
orthogonal	O
since	O
the	O
advertising	O
data	B
has	O
two	O
predictors	O
,	O
the	O
ﬁrst	O
two	O
principal	O
com-	O
ponents	O
contain	O
all	O
of	O
the	O
information	O
that	O
is	O
in	O
pop	O
and	O
ad	O
.	O
however	O
,	O
by	O
construction	O
,	O
the	O
ﬁrst	O
component	O
will	O
contain	O
the	O
most	O
information	O
.	O
con-	O
sider	O
,	O
for	O
example	O
,	O
the	O
much	O
larger	O
variability	O
of	O
zi1	O
(	O
the	O
x-axis	O
)	O
versus	O
zi2	O
(	O
the	O
y-axis	O
)	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
6.15.	O
the	O
fact	O
that	O
the	O
second	O
principal	O
component	O
scores	O
are	O
much	O
closer	O
to	O
zero	O
indicates	O
that	O
this	O
component	O
captures	O
far	O
less	O
information	O
.	O
as	O
another	O
illustration	O
,	O
fig-	O
ure	O
6.17	O
displays	O
zi2	O
versus	O
pop	O
and	O
ad	O
.	O
there	O
is	O
little	O
relationship	O
between	O
the	O
second	O
principal	O
component	O
and	O
these	O
two	O
predictors	O
,	O
again	O
suggesting	O
that	O
in	O
this	O
case	O
,	O
one	O
only	O
needs	O
the	O
ﬁrst	O
principal	O
component	O
in	O
order	O
to	O
accurately	O
represent	O
the	O
pop	O
and	O
ad	O
budgets	O
.	O
with	O
two-dimensional	O
data	B
,	O
such	O
as	O
in	O
our	O
advertising	O
example	O
,	O
we	O
can	O
construct	O
at	O
most	O
two	O
principal	B
components	I
.	O
however	O
,	O
if	O
we	O
had	O
other	O
predictors	O
,	O
such	O
as	O
population	O
age	O
,	O
income	O
level	B
,	O
education	O
,	O
and	O
so	O
forth	O
,	O
then	O
additional	O
components	O
could	O
be	O
constructed	O
.	O
they	O
would	O
successively	O
maximize	O
variance	B
,	O
subject	O
to	O
the	O
constraint	O
of	O
being	O
uncorrelated	O
with	O
the	O
preceding	O
components	O
.	O
the	O
principal	B
components	I
regression	O
approach	B
the	O
predictors	O
the	O
principal	B
components	I
regression	O
(	O
pcr	O
)	O
approach	B
involves	O
constructing	O
the	O
ﬁrst	O
m	O
principal	B
components	I
,	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
,	O
and	O
then	O
using	O
these	O
compo-	O
nents	O
as	O
is	O
ﬁt	B
using	O
least	B
squares	I
.	O
the	O
key	O
idea	O
is	O
that	O
often	O
a	O
small	O
number	O
of	O
prin-	O
cipal	O
components	O
suﬃce	O
to	O
explain	O
most	O
of	O
the	O
variability	O
in	O
the	O
data	B
,	O
as	O
well	O
as	O
the	O
relationship	O
with	O
the	O
response	B
.	O
in	O
other	O
words	O
,	O
we	O
assume	O
that	O
the	O
directions	O
in	O
which	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
show	O
the	O
most	O
variation	O
are	O
the	O
direc-	O
tions	O
that	O
are	O
associated	O
with	O
y	O
.	O
while	O
this	O
assumption	O
is	O
not	O
guaranteed	O
regression	B
model	O
in	O
a	O
linear	B
that	O
principal	B
components	I
regression	O
234	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
0	O
6	O
0	O
5	O
0	O
4	O
0	O
3	O
0	O
2	O
n	O
o	O
i	O
t	O
l	O
a	O
u	O
p	O
o	O
p	O
i	O
g	O
n	O
d	O
n	O
e	O
p	O
s	O
d	O
a	O
0	O
3	O
5	O
2	O
0	O
2	O
5	O
1	O
0	O
1	O
5	O
−1.0	O
−0.5	O
0.0	O
0.5	O
1.0	O
−1.0	O
−0.5	O
0.0	O
0.5	O
1.0	O
2nd	O
principal	O
component	O
2nd	O
principal	O
component	O
figure	O
6.17.	O
plots	O
of	O
the	O
second	O
principal	O
component	O
scores	O
zi2	O
versus	O
pop	O
and	O
ad	O
.	O
the	O
relationships	O
are	O
weak	O
.	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0	O
7	O
0	O
6	O
0	O
5	O
0	O
4	O
0	O
3	O
0	O
2	O
0	O
1	O
0	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
squared	O
bias	B
test	O
mse	O
variance	B
0	O
5	O
1	O
0	O
0	O
1	O
0	O
5	O
0	O
0	O
10	O
20	O
30	O
40	O
0	O
10	O
20	O
30	O
40	O
number	O
of	O
components	O
number	O
of	O
components	O
figure	O
6.18.	O
pcr	O
was	O
applied	O
to	O
two	O
simulated	O
data	B
sets	O
.	O
left	O
:	O
simulated	O
data	B
from	O
figure	O
6.8.	O
right	O
:	O
simulated	O
data	B
from	O
figure	O
6.9.	O
to	O
be	O
true	O
,	O
it	O
often	O
turns	O
out	O
to	O
be	O
a	O
reasonable	O
enough	O
approximation	O
to	O
give	O
good	O
results	O
.	O
if	O
the	O
assumption	O
underlying	O
pcr	O
holds	O
,	O
then	O
ﬁtting	O
a	O
least	B
squares	I
model	O
to	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
will	O
lead	O
to	O
better	O
results	O
than	O
ﬁtting	O
a	O
least	B
squares	I
model	O
to	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
,	O
since	O
most	O
or	O
all	O
of	O
the	O
information	O
in	O
the	O
data	B
that	O
relates	O
to	O
the	O
response	B
is	O
contained	O
in	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
,	O
and	O
by	O
estimating	O
only	O
m	O
(	O
cid:15	O
)	O
p	O
coeﬃcients	O
we	O
can	O
mitigate	O
overﬁtting	B
.	O
in	O
the	O
advertising	O
data	B
,	O
the	O
ﬁrst	O
principal	O
component	O
explains	O
most	O
of	O
the	O
variance	B
in	O
both	O
pop	O
and	O
ad	O
,	O
so	O
a	O
principal	O
component	O
regression	B
that	O
uses	O
this	O
single	B
variable	O
to	O
predict	O
some	O
response	B
of	O
interest	O
,	O
such	O
as	O
sales	O
,	O
will	O
likely	O
perform	O
quite	O
well	O
.	O
figure	O
6.18	O
displays	O
the	O
pcr	O
ﬁts	O
on	O
the	O
simulated	O
data	B
sets	O
from	O
figures	O
6.8	O
and	O
6.9.	O
recall	B
that	O
both	O
data	B
sets	O
were	O
generated	O
using	O
n	O
=	O
50	O
observations	B
and	O
p	O
=	O
45	O
predictors	O
.	O
however	O
,	O
while	O
the	O
response	B
in	O
the	O
ﬁrst	O
data	B
set	O
was	O
a	O
function	B
of	O
all	O
the	O
predictors	O
,	O
the	O
response	B
in	O
the	O
second	O
data	B
set	O
was	O
generated	O
using	O
only	O
two	O
of	O
the	O
predictors	O
.	O
the	O
curves	O
are	O
plotted	O
as	O
a	O
function	B
of	O
m	O
,	O
the	O
number	O
of	O
principal	B
components	I
used	O
as	O
predic-	O
tors	O
in	O
the	O
regression	B
model	O
.	O
as	O
more	O
principal	B
components	I
are	O
used	O
in	O
6.3	O
dimension	B
reduction	I
methods	O
235	O
pcr	O
ridge	B
regression	I
and	O
lasso	B
squared	O
bias	B
test	O
mse	O
variance	B
0	O
7	O
0	O
6	O
0	O
5	O
0	O
4	O
0	O
3	O
0	O
2	O
0	O
1	O
0	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0	O
7	O
0	O
6	O
0	O
5	O
0	O
4	O
0	O
3	O
0	O
2	O
0	O
1	O
0	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0	O
10	O
20	O
30	O
40	O
0.0	O
0.2	O
0.4	O
0.6	O
0.8	O
1.0	O
number	O
of	O
components	O
shrinkage	B
factor	O
figure	O
6.19.	O
pcr	O
,	O
ridge	B
regression	I
,	O
and	O
the	O
lasso	B
were	O
applied	O
to	O
a	O
simulated	O
data	B
set	O
in	O
which	O
the	O
ﬁrst	O
ﬁve	O
principal	B
components	I
of	O
x	O
contain	O
all	O
the	O
informa-	O
tion	O
about	O
the	O
response	B
y	O
.	O
in	O
each	O
panel	O
,	O
the	O
irreducible	B
error	I
var	O
(	O
	O
)	O
is	O
shown	O
as	O
a	O
horizontal	O
dashed	O
line	B
.	O
left	O
:	O
results	O
for	O
pcr	O
.	O
right	O
:	O
results	O
for	O
lasso	O
(	O
solid	O
)	O
and	O
ridge	B
regression	I
(	O
dotted	O
)	O
.	O
the	O
x-axis	O
displays	O
the	O
shrinkage	B
factor	O
of	O
the	O
co-	O
eﬃcient	O
estimates	O
,	O
deﬁned	O
as	O
the	O
(	O
cid:7	O
)	O
2	O
norm	O
of	O
the	O
shrunken	O
coeﬃcient	B
estimates	O
divided	O
by	O
the	O
(	O
cid:7	O
)	O
2	O
norm	O
of	O
the	O
least	B
squares	I
estimate	O
.	O
the	O
regression	B
model	O
,	O
the	O
bias	B
decreases	O
,	O
but	O
the	O
variance	B
increases	O
.	O
this	O
results	O
in	O
a	O
typical	O
u-shape	O
for	O
the	O
mean	B
squared	I
error	I
.	O
when	O
m	O
=	O
p	O
=	O
45	O
,	O
then	O
pcr	O
amounts	O
simply	O
to	O
a	O
least	B
squares	I
ﬁt	O
using	O
all	O
of	O
the	O
original	O
predictors	O
.	O
the	O
ﬁgure	O
indicates	O
that	O
performing	O
pcr	O
with	O
an	O
appropriate	O
choice	O
of	O
m	O
can	O
result	O
in	O
a	O
substantial	O
improvement	O
over	O
least	B
squares	I
,	O
es-	O
pecially	O
in	O
the	O
left-hand	O
panel	O
.	O
however	O
,	O
by	O
examining	O
the	O
ridge	B
regression	I
and	O
lasso	B
results	O
in	O
figures	O
6.5	O
,	O
6.8	O
,	O
and	O
6.9	O
,	O
we	O
see	O
that	O
pcr	O
does	O
not	O
perform	O
as	O
well	O
as	O
the	O
two	O
shrinkage	B
methods	O
in	O
this	O
example	O
.	O
the	O
relatively	O
worse	O
performance	O
of	O
pcr	O
in	O
figure	O
6.18	O
is	O
a	O
consequence	O
of	O
the	O
fact	O
that	O
the	O
data	B
were	O
generated	O
in	O
such	O
a	O
way	O
that	O
many	O
princi-	O
pal	O
components	O
are	O
required	O
in	O
order	O
to	O
adequately	O
model	B
the	O
response	B
.	O
in	O
contrast	B
,	O
pcr	O
will	O
tend	O
to	O
do	O
well	O
in	O
cases	O
when	O
the	O
ﬁrst	O
few	O
principal	B
components	I
are	O
suﬃcient	O
to	O
capture	O
most	O
of	O
the	O
variation	O
in	O
the	O
predictors	O
as	O
well	O
as	O
the	O
relationship	O
with	O
the	O
response	B
.	O
the	O
left-hand	O
panel	O
of	O
fig-	O
ure	O
6.19	O
illustrates	O
the	O
results	O
from	O
another	O
simulated	O
data	B
set	O
designed	O
to	O
be	O
more	O
favorable	O
to	O
pcr	O
.	O
here	O
the	O
response	B
was	O
generated	O
in	O
such	O
a	O
way	O
that	O
it	O
depends	O
exclusively	O
on	O
the	O
ﬁrst	O
ﬁve	O
principal	B
components	I
.	O
now	O
the	O
bias	B
drops	O
to	O
zero	O
rapidly	O
as	O
m	O
,	O
the	O
number	O
of	O
principal	B
components	I
used	O
in	O
pcr	O
,	O
increases	O
.	O
the	O
mean	B
squared	I
error	I
displays	O
a	O
clear	O
minimum	O
at	O
m	O
=	O
5.	O
the	O
right-hand	O
panel	O
of	O
figure	O
6.19	O
displays	O
the	O
results	O
on	O
these	O
data	B
using	O
ridge	B
regression	I
and	O
the	O
lasso	B
.	O
all	O
three	O
methods	O
oﬀer	O
a	O
signif-	O
icant	O
improvement	O
over	O
least	B
squares	I
.	O
however	O
,	O
pcr	O
and	O
ridge	B
regression	I
slightly	O
outperform	O
the	O
lasso	B
.	O
we	O
note	O
that	O
even	O
though	O
pcr	O
provides	O
a	O
simple	B
way	O
to	O
perform	O
regression	B
using	O
m	O
<	O
p	O
predictors	O
,	O
it	O
is	O
not	O
a	O
feature	B
selection	I
method	O
.	O
this	O
is	O
because	O
each	O
of	O
the	O
m	O
principal	B
components	I
used	O
in	O
the	O
regression	B
236	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
s	O
t	O
i	O
n	O
e	O
c	O
i	O
f	O
f	O
i	O
e	O
o	O
c	O
d	O
e	O
z	O
d	O
r	O
a	O
d	O
n	O
a	O
s	O
t	O
income	O
limit	O
rating	O
student	O
0	O
0	O
4	O
0	O
0	O
3	O
0	O
0	O
2	O
0	O
0	O
1	O
0	O
0	O
0	O
1	O
−	O
0	O
0	O
3	O
−	O
e	O
s	O
m	O
n	O
o	O
i	O
t	O
a	O
d	O
i	O
l	O
a	O
v	O
−	O
s	O
s	O
o	O
r	O
c	O
0	O
0	O
0	O
0	O
8	O
0	O
0	O
0	O
0	O
6	O
0	O
0	O
0	O
0	O
4	O
0	O
0	O
0	O
0	O
2	O
2	O
4	O
6	O
8	O
10	O
2	O
4	O
6	O
8	O
10	O
number	O
of	O
components	O
number	O
of	O
components	O
figure	O
6.20.	O
left	O
:	O
pcr	O
standardized	O
coeﬃcient	B
estimates	O
on	O
the	O
credit	O
data	B
set	O
for	O
diﬀerent	O
values	O
of	O
m	O
.	O
right	O
:	O
the	O
ten-fold	O
cross	O
validation	O
mse	O
obtained	O
using	O
pcr	O
,	O
as	O
a	O
function	B
of	O
m	O
.	O
is	O
a	O
linear	B
combination	I
of	O
all	O
p	O
of	O
the	O
original	O
features	O
.	O
for	O
instance	O
,	O
in	O
(	O
6.19	O
)	O
,	O
z1	O
was	O
a	O
linear	B
combination	I
of	O
both	O
pop	O
and	O
ad	O
.	O
therefore	O
,	O
while	O
pcr	O
often	O
performs	O
quite	O
well	O
in	O
many	O
practical	O
settings	O
,	O
it	O
does	O
not	O
result	O
in	O
the	O
development	O
of	O
a	O
model	B
that	O
relies	O
upon	O
a	O
small	O
set	B
of	O
the	O
original	O
features	O
.	O
in	O
this	O
sense	O
,	O
pcr	O
is	O
more	O
closely	O
related	O
to	O
ridge	B
regression	I
than	O
to	O
the	O
lasso	B
.	O
in	O
fact	O
,	O
one	O
can	O
show	O
that	O
pcr	O
and	O
ridge	B
regression	I
are	O
very	O
closely	O
related	O
.	O
one	O
can	O
even	O
think	O
of	O
ridge	B
regression	I
as	O
a	O
continuous	B
ver-	O
5	O
sion	O
of	O
pcr	O
!	O
in	O
pcr	O
,	O
the	O
number	O
of	O
principal	B
components	I
,	O
m	O
,	O
is	O
typically	O
chosen	O
by	O
cross-validation	B
.	O
the	O
results	O
of	O
applying	O
pcr	O
to	O
the	O
credit	O
data	B
set	O
are	O
shown	O
in	O
figure	O
6.20	O
;	O
the	O
right-hand	O
panel	O
displays	O
the	O
cross-validation	B
errors	O
obtained	O
,	O
as	O
a	O
function	B
of	O
m	O
.	O
on	O
these	O
data	B
,	O
the	O
lowest	O
cross-	O
validation	O
error	O
occurs	O
when	O
there	O
are	O
m	O
=	O
10	O
components	O
;	O
this	O
corre-	O
sponds	O
to	O
almost	O
no	O
dimension	B
reduction	I
at	O
all	O
,	O
since	O
pcr	O
with	O
m	O
=	O
11	O
is	O
equivalent	O
to	O
simply	O
performing	O
least	B
squares	I
.	O
when	O
performing	O
pcr	O
,	O
we	O
generally	O
recommend	O
standardizing	O
each	O
predictor	B
,	O
using	O
(	O
6.6	O
)	O
,	O
prior	B
to	O
generating	O
the	O
principal	B
components	I
.	O
this	O
standardization	O
ensures	O
that	O
all	O
variables	O
are	O
on	O
the	O
same	O
scale	O
.	O
in	O
the	O
absence	O
of	O
standardization	O
,	O
the	O
high-variance	O
variables	O
will	O
tend	O
to	O
play	O
a	O
larger	O
role	O
in	O
the	O
principal	B
components	I
obtained	O
,	O
and	O
the	O
scale	O
on	O
which	O
the	O
variables	O
are	O
measured	O
will	O
ultimately	O
have	O
an	O
eﬀect	O
on	O
the	O
ﬁnal	O
pcr	O
model	B
.	O
however	O
,	O
if	O
the	O
variables	O
are	O
all	O
measured	O
in	O
the	O
same	O
units	O
(	O
say	O
,	O
kilograms	O
,	O
or	O
inches	O
)	O
,	O
then	O
one	O
might	O
choose	O
not	O
to	O
standardize	B
them	O
.	O
5	O
more	O
details	O
can	O
be	O
found	O
in	O
section	O
3.5	O
of	O
elements	O
of	O
statistical	O
learning	O
by	O
hastie	O
,	O
tibshirani	O
,	O
and	O
friedman	O
.	O
6.3	O
dimension	B
reduction	I
methods	O
237	O
i	O
g	O
n	O
d	O
n	O
e	O
p	O
s	O
d	O
a	O
0	O
3	O
5	O
2	O
0	O
2	O
5	O
1	O
0	O
1	O
5	O
20	O
30	O
40	O
50	O
60	O
population	O
figure	O
6.21.	O
for	O
the	O
advertising	O
data	B
,	O
the	O
ﬁrst	O
pls	O
direction	O
(	O
solid	O
line	B
)	O
and	O
ﬁrst	O
pcr	O
direction	O
(	O
dotted	O
line	B
)	O
are	O
shown	O
.	O
6.3.2	O
partial	B
least	I
squares	I
the	O
pcr	O
approach	B
that	O
we	O
just	O
described	O
involves	O
identifying	O
linear	B
combi-	O
nations	O
,	O
or	O
directions	O
,	O
that	O
best	O
represent	O
the	O
predictors	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
.	O
these	O
directions	O
are	O
identiﬁed	O
in	O
an	O
unsupervised	O
way	O
,	O
since	O
the	O
response	B
y	O
is	O
not	O
used	O
to	O
help	O
determine	O
the	O
principal	O
component	O
directions	O
.	O
that	O
is	O
,	O
the	O
response	B
does	O
not	O
supervise	O
the	O
identiﬁcation	O
of	O
the	O
principal	B
components	I
.	O
consequently	O
,	O
pcr	O
suﬀers	O
from	O
a	O
drawback	O
:	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
directions	O
that	O
best	O
explain	O
the	O
predictors	O
will	O
also	O
be	O
the	O
best	O
directions	O
to	O
use	O
for	O
predicting	O
the	O
response	B
.	O
unsupervised	O
methods	O
are	O
discussed	O
further	O
in	O
chapter	O
10.	O
we	O
now	O
present	O
partial	B
least	I
squares	I
(	O
pls	O
)	O
,	O
a	O
supervised	O
alternative	O
to	O
pcr	O
.	O
like	O
pcr	O
,	O
pls	O
is	O
a	O
dimension	B
reduction	I
method	O
,	O
which	O
ﬁrst	O
identiﬁes	O
a	O
new	O
set	B
of	O
features	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
that	O
are	O
linear	B
combinations	O
of	O
the	O
original	O
features	O
,	O
and	O
then	O
ﬁts	O
a	O
linear	B
model	I
via	O
least	B
squares	I
using	O
these	O
m	O
new	O
features	O
.	O
but	O
unlike	O
pcr	O
,	O
pls	O
identiﬁes	O
these	O
new	O
features	O
in	O
a	O
supervised	O
way—that	O
is	O
,	O
it	O
makes	O
use	O
of	O
the	O
response	B
y	O
in	O
order	O
to	O
identify	O
new	O
features	O
that	O
not	O
only	O
approximate	O
the	O
old	O
features	O
well	O
,	O
but	O
also	O
that	O
are	O
related	O
to	O
the	O
response	B
.	O
roughly	O
speaking	O
,	O
the	O
pls	O
approach	B
attempts	O
to	O
ﬁnd	O
directions	O
that	O
help	O
explain	O
both	O
the	O
response	B
and	O
the	O
predictors	O
.	O
we	O
now	O
describe	O
how	O
the	O
ﬁrst	O
pls	O
direction	O
is	O
computed	O
.	O
after	O
stan-	O
dardizing	O
the	O
p	O
predictors	O
,	O
pls	O
computes	O
the	O
ﬁrst	O
direction	O
z1	O
by	O
setting	O
each	O
φj1	O
in	O
(	O
6.16	O
)	O
equal	O
to	O
the	O
coeﬃcient	B
from	O
the	O
simple	B
linear	O
regression	B
of	O
y	O
onto	O
xj	O
.	O
one	O
can	O
show	O
that	O
this	O
coeﬃcient	B
is	O
proportional	O
to	O
the	O
cor-	O
p	O
relation	O
between	O
y	O
and	O
xj	O
.	O
hence	O
,	O
in	O
computing	O
z1	O
=	O
j=1	O
φj1xj	O
,	O
pls	O
places	O
the	O
highest	O
weight	O
on	O
the	O
variables	O
that	O
are	O
most	O
strongly	O
related	O
to	O
the	O
response	B
.	O
(	O
cid:10	O
)	O
partial	B
least	I
squares	I
figure	O
6.21	O
displays	O
an	O
example	O
of	O
pls	O
on	O
set	B
with	O
sales	O
in	O
each	O
of	O
100	O
regions	O
as	O
the	O
response	B
,	O
and	O
two	O
predictors	O
;	O
population	O
size	O
and	O
advertising	O
spending	O
.	O
the	O
solid	O
green	O
line	B
indicates	O
the	O
ﬁrst	O
pls	O
direction	O
,	O
while	O
the	O
dotted	O
line	B
shows	O
the	O
ﬁrst	O
principal	O
component	O
direction	O
.	O
ad	O
dimension	O
per	O
unit	O
pls	O
has	O
chosen	O
a	O
direction	O
that	O
has	O
less	O
change	O
in	O
the	O
6	O
a	O
synthetic	O
data	B
6this	O
dataset	O
is	O
distinct	O
from	O
the	O
advertising	O
data	B
discussed	O
in	O
chapter	O
3	O
.	O
238	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
pop	O
is	O
ad	O
.	O
the	O
pls	O
direction	O
not	O
fit	O
the	O
predictors	O
as	O
closely	O
as	O
does	O
pca	O
,	O
but	O
it	O
does	O
a	O
better	O
job	O
change	O
in	O
the	O
pop	O
dimension	O
,	O
relative	O
to	O
pca	O
.	O
this	O
suggests	O
that	O
more	O
highly	O
correlated	O
with	O
the	O
response	B
than	O
is	O
does	O
explaining	O
the	O
response	B
.	O
to	O
identify	O
the	O
second	O
pls	O
direction	O
we	O
ﬁrst	O
adjust	O
each	O
of	O
the	O
variables	O
for	O
z1	O
,	O
by	O
regressing	O
each	O
variable	B
on	O
z1	O
and	O
taking	O
residuals	B
.	O
these	O
resid-	O
uals	O
can	O
be	O
interpreted	O
as	O
the	O
remaining	O
information	O
that	O
has	O
not	O
been	O
explained	B
by	O
the	O
ﬁrst	O
pls	O
direction	O
.	O
we	O
then	O
compute	O
z2	O
using	O
this	O
or-	O
thogonalized	O
data	B
in	O
exactly	O
the	O
same	O
fashion	O
as	O
z1	O
was	O
computed	O
based	O
on	O
the	O
original	O
data	B
.	O
this	O
iterative	O
approach	B
can	O
be	O
repeated	O
m	O
times	O
to	O
identify	O
multiple	B
pls	O
components	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
.	O
finally	O
,	O
at	O
the	O
end	O
of	O
this	O
procedure	O
,	O
we	O
use	O
least	B
squares	I
to	O
ﬁt	B
a	O
linear	B
model	I
to	O
predict	O
y	O
using	O
z1	O
,	O
.	O
.	O
.	O
,	O
zm	O
in	O
exactly	O
the	O
same	O
fashion	O
as	O
for	O
pcr	O
.	O
as	O
with	O
pcr	O
,	O
the	O
number	O
m	O
of	O
partial	B
least	I
squares	I
directions	O
used	O
in	O
pls	O
is	O
a	O
tuning	B
parameter	I
that	O
is	O
typically	O
chosen	O
by	O
cross-validation	B
.	O
we	O
generally	O
standardize	B
the	O
predictors	O
and	O
response	B
before	O
performing	O
pls	O
.	O
pls	O
is	O
popular	O
in	O
the	O
ﬁeld	O
of	O
chemometrics	O
,	O
where	O
many	O
variables	O
arise	O
from	O
digitized	O
spectrometry	O
signals	O
.	O
in	O
practice	O
it	O
often	O
performs	O
no	O
better	O
than	O
ridge	B
regression	I
or	O
pcr	O
.	O
while	O
the	O
supervised	O
dimension	O
reduction	O
of	O
pls	O
can	O
reduce	O
bias	B
,	O
it	O
also	O
has	O
the	O
potential	O
to	O
increase	O
variance	B
,	O
so	O
that	O
the	O
overall	O
beneﬁt	O
of	O
pls	O
relative	O
to	O
pcr	O
is	O
a	O
wash.	O
6.4	O
considerations	O
in	O
high	O
dimensions	O
6.4.1	O
high-dimensional	B
data	O
most	O
traditional	O
statistical	O
techniques	O
for	O
regression	O
and	O
classiﬁcation	B
are	O
intended	O
for	O
the	O
low-dimensional	B
setting	O
in	O
which	O
n	O
,	O
the	O
number	O
of	O
ob-	O
servations	O
,	O
is	O
much	O
greater	O
than	O
p	O
,	O
the	O
number	O
of	O
features	O
.	O
this	O
is	O
due	O
in	O
part	O
to	O
the	O
fact	O
that	O
throughout	O
most	O
of	O
the	O
ﬁeld	O
’	O
s	O
history	O
,	O
the	O
bulk	O
of	O
sci-	O
entiﬁc	O
problems	O
requiring	O
the	O
use	O
of	O
statistics	O
have	O
been	O
low-dimensional	B
.	O
for	O
instance	O
,	O
consider	O
the	O
task	O
of	O
developing	O
a	O
model	B
to	O
predict	O
a	O
patient	O
’	O
s	O
blood	O
pressure	O
on	O
the	O
basis	B
of	O
his	O
or	O
her	O
age	O
,	O
gender	O
,	O
and	O
body	O
mass	O
index	O
(	O
bmi	O
)	O
.	O
there	O
are	O
three	O
predictors	O
,	O
or	O
four	O
if	O
an	O
intercept	B
is	O
included	O
in	O
the	O
model	B
,	O
and	O
perhaps	O
several	O
thousand	O
patients	O
for	O
whom	O
blood	O
pressure	O
and	O
age	O
,	O
gender	O
,	O
and	O
bmi	O
are	O
available	O
.	O
hence	O
n	O
(	O
cid:10	O
)	O
p	O
,	O
and	O
so	O
the	O
problem	O
is	O
low-dimensional	B
.	O
(	O
by	O
dimension	O
here	O
we	O
are	O
referring	O
to	O
the	O
size	O
of	O
p.	O
)	O
in	O
the	O
past	O
20	O
years	O
,	O
new	O
technologies	O
have	O
changed	O
the	O
way	O
that	O
data	B
are	O
collected	O
in	O
ﬁelds	O
as	O
diverse	O
as	O
ﬁnance	O
,	O
marketing	O
,	O
and	O
medicine	O
.	O
it	O
is	O
now	O
commonplace	O
to	O
collect	O
an	O
almost	O
unlimited	O
number	O
of	O
feature	B
mea-	O
surements	O
(	O
p	O
very	O
large	O
)	O
.	O
while	O
p	O
can	O
be	O
extremely	O
large	O
,	O
the	O
number	O
of	O
observations	B
n	O
is	O
often	O
limited	O
due	O
to	O
cost	O
,	O
sample	O
availability	O
,	O
or	O
other	O
considerations	O
.	O
two	O
examples	O
are	O
as	O
follows	O
:	O
1.	O
rather	O
than	O
predicting	O
blood	O
pressure	O
on	O
the	O
basis	B
of	O
just	O
age	O
,	O
gen-	O
der	O
,	O
and	O
bmi	O
,	O
one	O
might	O
also	O
collect	O
measurements	O
for	O
half	O
a	O
million	O
low-	O
dimensional	O
high-	O
dimensional	O
6.4	O
considerations	O
in	O
high	O
dimensions	O
239	O
single	B
nucleotide	O
polymorphisms	O
(	O
snps	O
;	O
these	O
are	O
individual	O
dna	O
mutations	O
that	O
are	O
relatively	O
common	O
in	O
the	O
population	O
)	O
for	O
inclu-	O
sion	O
in	O
the	O
predictive	O
model	B
.	O
then	O
n	O
≈	O
200	O
and	O
p	O
≈	O
500,000	O
.	O
2.	O
a	O
marketing	O
analyst	O
interested	O
in	O
understanding	O
people	O
’	O
s	O
online	O
shop-	O
ping	O
patterns	O
could	O
treat	O
as	O
features	O
all	O
of	O
the	O
search	O
terms	O
entered	O
by	O
users	O
of	O
a	O
search	O
engine	O
.	O
this	O
is	O
sometimes	O
known	O
as	O
the	O
“	O
bag-of-	O
words	O
”	O
model	B
.	O
the	O
same	O
researcher	O
might	O
have	O
access	O
to	O
the	O
search	O
histories	O
of	O
only	O
a	O
few	O
hundred	O
or	O
a	O
few	O
thousand	O
search	O
engine	O
users	O
who	O
have	O
consented	O
to	O
share	O
their	O
information	O
with	O
the	O
researcher	O
.	O
for	O
a	O
given	O
user	O
,	O
each	O
of	O
the	O
p	O
search	O
terms	O
is	O
scored	O
present	O
(	O
0	O
)	O
or	O
absent	O
(	O
1	O
)	O
,	O
creating	O
a	O
large	O
binary	B
feature	O
vector	B
.	O
then	O
n	O
≈	O
1,000	O
and	O
p	O
is	O
much	O
larger	O
.	O
data	B
sets	O
containing	O
more	O
features	O
than	O
observations	B
are	O
often	O
referred	O
to	O
as	O
high-dimensional	B
.	O
classical	O
approaches	O
such	O
as	O
least	B
squares	I
linear	O
regression	B
are	O
not	O
appropriate	O
in	O
this	O
setting	O
.	O
many	O
of	O
the	O
issues	O
that	O
arise	O
in	O
the	O
analysis	O
of	O
high-dimensional	O
data	B
were	O
discussed	O
earlier	O
in	O
this	O
book	O
,	O
since	O
they	O
apply	O
also	O
when	O
n	O
>	O
p	O
:	O
these	O
include	O
the	O
role	O
of	O
the	O
bias-variance	B
trade-oﬀ	O
and	O
the	O
danger	O
of	O
overﬁtting	B
.	O
though	O
these	O
issues	O
are	O
always	O
rele-	O
vant	O
,	O
they	O
can	O
become	O
particularly	O
important	O
when	O
the	O
number	O
of	O
features	O
is	O
very	O
large	O
relative	O
to	O
the	O
number	O
of	O
observations	B
.	O
we	O
have	O
deﬁned	O
the	O
high-dimensional	B
setting	O
as	O
the	O
case	O
where	O
the	O
num-	O
ber	O
of	O
features	O
p	O
is	O
larger	O
than	O
the	O
number	O
of	O
observations	B
n.	O
but	O
the	O
con-	O
siderations	O
that	O
we	O
will	O
now	O
discuss	O
certainly	O
also	O
apply	O
if	O
p	O
is	O
slightly	O
smaller	O
than	O
n	O
,	O
and	O
are	O
best	O
always	O
kept	O
in	O
mind	O
when	O
performing	O
super-	O
vised	O
learning	O
.	O
6.4.2	O
what	O
goes	O
wrong	O
in	O
high	O
dimensions	O
?	O
in	O
order	O
to	O
illustrate	O
the	O
need	O
for	O
extra	O
care	O
and	O
specialized	O
techniques	O
for	O
regression	O
and	O
classiﬁcation	B
when	O
p	O
>	O
n	O
,	O
we	O
begin	O
by	O
examining	O
what	O
can	O
go	O
wrong	O
if	O
we	O
apply	O
a	O
statistical	O
technique	O
not	O
intended	O
for	O
the	O
high-	O
dimensional	O
setting	O
.	O
for	O
this	O
purpose	O
,	O
we	O
examine	O
least	B
squares	I
regression	O
.	O
but	O
the	O
same	O
concepts	O
apply	O
to	O
logistic	B
regression	I
,	O
linear	O
discriminant	O
anal-	O
ysis	O
,	O
and	O
other	O
classical	O
statistical	O
approaches	O
.	O
when	O
the	O
number	O
of	O
features	O
p	O
is	O
as	O
large	O
as	O
,	O
or	O
larger	O
than	O
,	O
the	O
number	O
of	O
observations	B
n	O
,	O
least	B
squares	I
as	O
described	O
in	O
chapter	O
3	O
can	O
not	O
(	O
or	O
rather	O
,	O
should	O
not	O
)	O
be	O
performed	O
.	O
the	O
reason	O
is	O
simple	B
:	O
regardless	O
of	O
whether	O
or	O
not	O
there	O
truly	O
is	O
a	O
relationship	O
between	O
the	O
features	O
and	O
the	O
response	B
,	O
least	B
squares	I
will	O
yield	O
a	O
set	B
of	O
coeﬃcient	B
estimates	O
that	O
result	O
in	O
a	O
perfect	O
ﬁt	B
to	O
the	O
data	B
,	O
such	O
that	O
the	O
residuals	B
are	O
zero	O
.	O
an	O
example	O
is	O
shown	O
in	O
figure	O
6.22	O
with	O
p	O
=	O
1	O
feature	B
(	O
plus	O
an	O
intercept	B
)	O
in	O
two	O
cases	O
:	O
when	O
there	O
are	O
20	O
observations	B
,	O
and	O
when	O
there	O
are	O
only	O
two	O
observations	B
.	O
when	O
there	O
are	O
20	O
observations	B
,	O
n	O
>	O
p	O
and	O
the	O
least	O
240	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
0	O
1	O
5	O
0	O
1	O
5	O
y	O
0	O
y	O
0	O
5	O
−	O
0	O
1	O
−	O
5	O
−	O
0	O
1	O
−	O
−1.5	O
−1.0	O
−0.5	O
0.0	O
0.5	O
1.0	O
−1.5	O
−1.0	O
−0.5	O
0.0	O
0.5	O
1.0	O
x	O
x	O
figure	O
6.22.	O
left	O
:	O
least	B
squares	I
regression	O
in	O
the	O
low-dimensional	B
setting	O
.	O
right	O
:	O
least	B
squares	I
regression	O
with	O
n	O
=	O
2	O
observations	B
and	O
two	O
parameters	O
to	O
be	O
estimated	O
(	O
an	O
intercept	B
and	O
a	O
coeﬃcient	B
)	O
.	O
squares	O
regression	B
line	O
does	O
not	O
perfectly	O
ﬁt	B
the	O
data	B
;	O
instead	O
,	O
the	O
regression	B
line	O
seeks	O
to	O
approximate	O
the	O
20	O
observations	B
as	O
well	O
as	O
possible	O
.	O
on	O
the	O
other	O
hand	O
,	O
when	O
there	O
are	O
only	O
two	O
observations	B
,	O
then	O
regardless	O
of	O
the	O
values	O
of	O
those	O
observations	B
,	O
the	O
regression	B
line	O
will	O
ﬁt	B
the	O
data	B
exactly	O
.	O
this	O
is	O
problematic	O
because	O
this	O
perfect	O
ﬁt	B
will	O
almost	O
certainly	O
lead	O
to	O
overﬁtting	B
of	O
the	O
data	B
.	O
in	O
other	O
words	O
,	O
though	O
it	O
is	O
possible	O
to	O
perfectly	O
ﬁt	B
the	O
training	B
data	O
in	O
the	O
high-dimensional	B
setting	O
,	O
the	O
resulting	O
linear	B
model	I
will	O
perform	O
extremely	O
poorly	O
on	O
an	O
independent	B
test	O
set	B
,	O
and	O
therefore	O
does	O
not	O
constitute	O
a	O
useful	O
model	B
.	O
in	O
fact	O
,	O
we	O
can	O
see	O
that	O
this	O
happened	O
in	O
figure	O
6.22	O
:	O
the	O
least	B
squares	I
line	O
obtained	O
in	O
the	O
right-hand	O
panel	O
will	O
perform	O
very	O
poorly	O
on	O
a	O
test	B
set	O
comprised	O
of	O
the	O
observations	B
in	O
the	O
left-	O
hand	O
panel	O
.	O
the	O
problem	O
is	O
simple	B
:	O
when	O
p	O
>	O
n	O
or	O
p	O
≈	O
n	O
,	O
a	O
simple	B
least	O
squares	O
regression	B
line	O
is	O
too	O
ﬂexible	B
and	O
hence	O
overﬁts	O
the	O
data	B
.	O
figure	O
6.23	O
further	O
illustrates	O
the	O
risk	O
of	O
carelessly	O
applying	O
least	B
squares	I
when	O
the	O
number	O
of	O
features	O
p	O
is	O
large	O
.	O
data	B
were	O
simulated	O
with	O
n	O
=	O
20	O
observations	B
,	O
and	O
regression	B
was	O
performed	O
with	O
between	O
1	O
and	O
20	O
features	O
,	O
each	O
of	O
which	O
was	O
completely	O
unrelated	O
to	O
the	O
response	B
.	O
as	O
shown	O
in	O
the	O
ﬁgure	O
,	O
the	O
model	B
r2	O
increases	O
to	O
1	O
as	O
the	O
number	O
of	O
features	O
included	O
in	O
the	O
model	B
increases	O
,	O
and	O
correspondingly	O
the	O
training	B
set	O
mse	O
decreases	O
to	O
0	O
as	O
the	O
number	O
of	O
features	O
increases	O
,	O
even	O
though	O
the	O
features	O
are	O
completely	O
unrelated	O
to	O
the	O
response	B
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
mse	O
on	O
an	O
independent	B
test	O
set	B
becomes	O
extremely	O
large	O
as	O
the	O
number	O
of	O
features	O
included	O
in	O
the	O
model	B
increases	O
,	O
because	O
including	O
the	O
additional	O
predictors	O
leads	O
to	O
a	O
vast	O
increase	O
in	O
the	O
variance	B
of	O
the	O
coeﬃcient	B
estimates	O
.	O
looking	O
at	O
the	O
test	B
set	O
mse	O
,	O
it	O
is	O
clear	O
that	O
the	O
best	O
model	O
contains	O
at	O
most	O
a	O
few	O
variables	O
.	O
however	O
,	O
someone	O
who	O
carelessly	O
examines	O
only	O
the	O
r2	O
or	O
the	O
training	B
set	O
mse	O
might	O
erroneously	O
conclude	O
that	O
the	O
model	B
with	O
the	O
greatest	O
number	O
of	O
variables	O
is	O
best	O
.	O
this	O
indicates	O
the	O
importance	B
of	O
applying	O
extra	O
care	O
0	O
1	O
.	O
8	O
0	O
.	O
6	O
0	O
.	O
4	O
0	O
.	O
2	O
0	O
.	O
2	O
r	O
6.4	O
considerations	O
in	O
high	O
dimensions	O
241	O
8	O
.	O
0	O
6	O
0	O
.	O
4	O
0	O
.	O
2	O
.	O
0	O
0	O
0	O
.	O
e	O
s	O
m	O
t	O
s	O
e	O
t	O
0	O
0	O
5	O
0	O
5	O
5	O
1	O
e	O
s	O
m	O
g	O
n	O
n	O
a	O
r	O
t	O
i	O
i	O
5	O
10	O
15	O
number	O
of	O
variables	O
5	O
10	O
15	O
number	O
of	O
variables	O
5	O
10	O
15	O
number	O
of	O
variables	O
figure	O
6.23.	O
on	O
a	O
simulated	O
example	O
with	O
n	O
=	O
20	O
training	B
observations	O
,	O
features	O
that	O
are	O
completely	O
unrelated	O
to	O
the	O
outcome	O
are	O
added	O
to	O
the	O
model	B
.	O
left	O
:	O
the	O
r2	O
increases	O
to	O
1	O
as	O
more	O
features	O
are	O
included	O
.	O
center	O
:	O
the	O
training	B
set	O
mse	O
decreases	O
to	O
0	O
as	O
more	O
features	O
are	O
included	O
.	O
right	O
:	O
the	O
test	B
set	O
mse	O
increases	O
as	O
more	O
features	O
are	O
included	O
.	O
when	O
analyzing	O
data	B
sets	O
with	O
a	O
large	O
number	O
of	O
variables	O
,	O
and	O
of	O
always	O
evaluating	O
model	B
performance	O
on	O
an	O
independent	B
test	O
set	B
.	O
in	O
section	O
6.1.3	O
,	O
we	O
saw	O
a	O
number	O
of	O
approaches	O
for	O
adjusting	O
the	O
training	B
set	O
rss	O
or	O
r2	O
in	O
order	O
to	O
account	O
for	O
the	O
number	O
of	O
variables	O
used	O
to	O
ﬁt	B
a	O
least	B
squares	I
model	O
.	O
unfortunately	O
,	O
the	O
cp	O
,	O
aic	O
,	O
and	O
bic	O
approaches	O
are	O
not	O
appropriate	O
in	O
the	O
high-dimensional	B
setting	O
,	O
because	O
estimating	O
ˆσ2	O
is	O
problematic	O
.	O
(	O
for	O
instance	O
,	O
the	O
formula	O
for	O
ˆσ2	O
from	O
chapter	O
3	O
yields	O
an	O
estimate	O
ˆσ2	O
=	O
0	O
in	O
this	O
setting	O
.	O
)	O
similarly	O
,	O
problems	O
arise	O
in	O
the	O
application	O
of	O
adjusted	O
r2	O
in	O
the	O
high-dimensional	B
setting	O
,	O
since	O
one	O
can	O
easily	O
obtain	O
a	O
model	B
with	O
an	O
adjusted	O
r2	O
value	O
of	O
1.	O
clearly	O
,	O
alternative	O
approaches	O
that	O
are	O
better-suited	O
to	O
the	O
high-dimensional	B
setting	O
are	O
required	O
.	O
6.4.3	O
regression	B
in	O
high	O
dimensions	O
it	O
turns	O
out	O
that	O
many	O
of	O
the	O
methods	O
seen	O
in	O
this	O
chapter	O
for	O
ﬁtting	O
less	O
ﬂexible	B
least	O
squares	O
models	O
,	O
such	O
as	O
forward	B
stepwise	I
selection	I
,	O
ridge	B
regression	I
,	O
the	O
lasso	B
,	O
and	O
principal	B
components	I
regression	O
,	O
are	O
particularly	O
useful	O
for	O
performing	O
regression	B
in	O
the	O
high-dimensional	B
setting	O
.	O
essentially	O
,	O
these	O
approaches	O
avoid	O
overﬁtting	B
by	O
using	O
a	O
less	O
ﬂexible	B
ﬁtting	O
approach	B
than	O
least	B
squares	I
.	O
figure	O
6.24	O
illustrates	O
the	O
performance	O
of	O
the	O
lasso	B
in	O
a	O
simple	B
simulated	O
example	O
.	O
there	O
are	O
p	O
=	O
20	O
,	O
50	O
,	O
or	O
2,000	O
features	O
,	O
of	O
which	O
20	O
are	O
truly	O
associated	O
with	O
the	O
outcome	O
.	O
the	O
lasso	B
was	O
performed	O
on	O
n	O
=	O
100	O
training	B
observations	O
,	O
and	O
the	O
mean	B
squared	I
error	I
was	O
evaluated	O
on	O
an	O
independent	B
test	O
set	B
.	O
as	O
the	O
number	O
of	O
features	O
increases	O
,	O
the	O
test	B
set	O
error	B
increases	O
.	O
when	O
p	O
=	O
20	O
,	O
the	O
lowest	O
validation	B
set	I
error	O
was	O
achieved	O
when	O
λ	O
in	O
(	O
6.7	O
)	O
was	O
small	O
;	O
however	O
,	O
when	O
p	O
was	O
larger	O
then	O
the	O
lowest	O
validation	B
set	I
error	O
was	O
achieved	O
using	O
a	O
larger	O
value	O
of	O
λ.	O
in	O
each	O
boxplot	B
,	O
rather	O
than	O
reporting	O
the	O
values	O
of	O
λ	O
used	O
,	O
the	O
degrees	B
of	I
freedom	I
of	O
the	O
resulting	O
242	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
p	O
=	O
20	O
p	O
=	O
50	O
p	O
=	O
2000	O
5	O
4	O
3	O
2	O
1	O
0	O
5	O
4	O
3	O
2	O
1	O
0	O
5	O
4	O
3	O
2	O
1	O
0	O
16	O
1	O
21	O
degrees	B
of	I
freedom	I
28	O
1	O
51	O
degrees	B
of	I
freedom	I
70	O
1	O
111	O
degrees	B
of	I
freedom	I
figure	O
6.24.	O
the	O
lasso	B
was	O
performed	O
with	O
n	O
=	O
100	O
observations	B
and	O
three	O
values	O
of	O
p	O
,	O
the	O
number	O
of	O
features	O
.	O
of	O
the	O
p	O
features	O
,	O
20	O
were	O
associated	O
with	O
the	O
response	B
.	O
the	O
boxplots	O
show	O
the	O
test	B
mses	O
that	O
result	O
using	O
three	O
diﬀerent	O
values	O
of	O
the	O
tuning	B
parameter	I
λ	O
in	O
(	O
6.7	O
)	O
.	O
for	O
ease	O
of	O
interpretation	O
,	O
rather	O
than	O
reporting	O
λ	O
,	O
the	O
degrees	B
of	I
freedom	I
are	O
reported	O
;	O
for	O
the	O
lasso	B
this	O
turns	O
out	O
to	O
be	O
simply	O
the	O
number	O
of	O
estimated	O
non-zero	O
coeﬃcients	O
.	O
when	O
p	O
=	O
20	O
,	O
the	O
lowest	O
test	B
mse	O
was	O
obtained	O
with	O
the	O
smallest	O
amount	O
of	O
regularization	B
.	O
when	O
p	O
=	O
50	O
,	O
the	O
lowest	O
test	B
mse	O
was	O
achieved	O
when	O
there	O
is	O
a	O
substantial	O
amount	O
of	O
regularization	B
.	O
when	O
p	O
=	O
2,000	O
the	O
lasso	B
performed	O
poorly	O
regardless	O
of	O
the	O
amount	O
of	O
regularization	B
,	O
due	O
to	O
the	O
fact	O
that	O
only	O
20	O
of	O
the	O
2,000	O
features	O
truly	O
are	O
associated	O
with	O
the	O
outcome	O
.	O
lasso	B
solution	O
is	O
displayed	O
;	O
this	O
is	O
simply	O
the	O
number	O
of	O
non-zero	O
coeﬃcient	B
estimates	O
in	O
the	O
lasso	B
solution	O
,	O
and	O
is	O
a	O
measure	O
of	O
the	O
ﬂexibility	O
of	O
the	O
lasso	B
ﬁt	O
.	O
figure	O
6.24	O
highlights	O
three	O
important	O
points	O
:	O
(	O
1	O
)	O
regularization	B
or	O
shrinkage	B
plays	O
a	O
key	O
role	O
in	O
high-dimensional	B
problems	O
,	O
(	O
2	O
)	O
appropriate	O
tuning	B
parameter	I
selection	O
is	O
crucial	O
for	O
good	O
predictive	O
performance	O
,	O
and	O
(	O
3	O
)	O
the	O
test	B
error	O
tends	O
to	O
increase	O
as	O
the	O
dimensionality	O
of	O
the	O
problem	O
(	O
i.e	O
.	O
the	O
number	O
of	O
features	O
or	O
predictors	O
)	O
increases	O
,	O
unless	O
the	O
additional	O
features	O
are	O
truly	O
associated	O
with	O
the	O
response	B
.	O
the	O
third	O
point	O
above	O
is	O
in	O
fact	O
a	O
key	O
principle	O
in	O
the	O
analysis	O
of	O
high-	O
dimensional	O
data	B
,	O
which	O
is	O
known	O
as	O
the	O
curse	B
of	I
dimensionality	I
.	O
one	O
might	O
think	O
that	O
as	O
the	O
number	O
of	O
features	O
used	O
to	O
ﬁt	B
a	O
model	B
increases	O
,	O
the	O
quality	O
of	O
the	O
ﬁtted	O
model	O
will	O
increase	O
as	O
well	O
.	O
however	O
,	O
comparing	O
the	O
left-hand	O
and	O
right-hand	O
panels	O
in	O
figure	O
6.24	O
,	O
we	O
see	O
that	O
this	O
is	O
not	O
necessarily	O
the	O
case	O
:	O
in	O
this	O
example	O
,	O
the	O
test	B
set	O
mse	O
almost	O
doubles	O
as	O
p	O
increases	O
from	O
20	O
to	O
2,000.	O
in	O
general	O
,	O
adding	O
additional	O
signal	B
features	O
that	O
are	O
truly	O
associated	O
with	O
the	O
response	B
will	O
improve	O
the	O
ﬁtted	O
model	O
,	O
in	O
the	O
sense	O
of	O
leading	O
to	O
a	O
reduction	O
in	O
test	B
set	O
error	B
.	O
however	O
,	O
adding	O
noise	B
features	O
that	O
are	O
not	O
truly	O
associated	O
with	O
the	O
response	B
will	O
lead	O
to	O
a	O
deterioration	O
in	O
the	O
ﬁtted	O
model	O
,	O
and	O
consequently	O
an	O
increased	O
test	B
set	O
error	B
.	O
this	O
is	O
because	O
noise	B
features	O
increase	O
the	O
dimensionality	O
of	O
the	O
curse	O
of	O
di-	O
mensionality	O
6.4	O
considerations	O
in	O
high	O
dimensions	O
243	O
problem	O
,	O
exacerbating	O
the	O
risk	O
of	O
overﬁtting	B
(	O
since	O
noise	B
features	O
may	O
be	O
assigned	O
nonzero	O
coeﬃcients	O
due	O
to	O
chance	O
associations	O
with	O
the	O
response	B
on	O
the	O
training	B
set	O
)	O
without	O
any	O
potential	O
upside	O
in	O
terms	O
of	O
improved	O
test	B
set	O
error	B
.	O
thus	O
,	O
we	O
see	O
that	O
new	O
technologies	O
that	O
allow	O
for	O
the	O
collection	O
of	O
measurements	O
for	O
thousands	O
or	O
millions	O
of	O
features	O
are	O
a	O
double-edged	O
sword	O
:	O
they	O
can	O
lead	O
to	O
improved	O
predictive	O
models	O
if	O
these	O
features	O
are	O
in	O
fact	O
relevant	O
to	O
the	O
problem	O
at	O
hand	O
,	O
but	O
will	O
lead	O
to	O
worse	O
results	O
if	O
the	O
features	O
are	O
not	O
relevant	O
.	O
even	O
if	O
they	O
are	O
relevant	O
,	O
the	O
variance	B
incurred	O
in	O
ﬁtting	O
their	O
coeﬃcients	O
may	O
outweigh	O
the	O
reduction	O
in	O
bias	B
that	O
they	O
bring	O
.	O
6.4.4	O
interpreting	O
results	O
in	O
high	O
dimensions	O
when	O
we	O
perform	O
the	O
lasso	B
,	O
ridge	B
regression	I
,	O
or	O
other	O
regression	B
proce-	O
dures	O
in	O
the	O
high-dimensional	B
setting	O
,	O
we	O
must	O
be	O
quite	O
cautious	O
in	O
the	O
way	O
that	O
we	O
report	O
the	O
results	O
obtained	O
.	O
in	O
chapter	O
3	O
,	O
we	O
learned	O
about	O
multi-	O
collinearity	B
,	O
the	O
concept	O
that	O
the	O
variables	O
in	O
a	O
regression	B
might	O
be	O
corre-	O
lated	O
with	O
each	O
other	O
.	O
in	O
the	O
high-dimensional	B
setting	O
,	O
the	O
multicollinearity	B
problem	O
is	O
extreme	O
:	O
any	O
variable	B
in	O
the	O
model	B
can	O
be	O
written	O
as	O
a	O
linear	B
combination	I
of	O
all	O
of	O
the	O
other	O
variables	O
in	O
the	O
model	B
.	O
essentially	O
,	O
this	O
means	O
that	O
we	O
can	O
never	O
know	O
exactly	O
which	O
variables	O
(	O
if	O
any	O
)	O
truly	O
are	O
predictive	O
of	O
the	O
outcome	O
,	O
and	O
we	O
can	O
never	O
identify	O
the	O
best	O
coeﬃcients	O
for	O
use	O
in	O
the	O
regression	B
.	O
at	O
most	O
,	O
we	O
can	O
hope	O
to	O
assign	O
large	O
regression	B
coeﬃcients	O
to	O
variables	O
that	O
are	O
correlated	O
with	O
the	O
variables	O
that	O
truly	O
are	O
predictive	O
of	O
the	O
outcome	O
.	O
for	O
instance	O
,	O
suppose	O
that	O
we	O
are	O
trying	O
to	O
predict	O
blood	O
pressure	O
on	O
the	O
basis	B
of	O
half	O
a	O
million	O
snps	O
,	O
and	O
that	O
forward	B
stepwise	I
selection	I
indicates	O
that	O
17	O
of	O
those	O
snps	O
lead	O
to	O
a	O
good	O
predictive	O
model	B
on	O
the	O
training	B
data	O
.	O
it	O
would	O
be	O
incorrect	O
to	O
conclude	O
that	O
these	O
17	O
snps	O
predict	O
blood	O
pressure	O
more	O
eﬀectively	O
than	O
the	O
other	O
snps	O
not	O
included	O
in	O
the	O
model	B
.	O
there	O
are	O
likely	O
to	O
be	O
many	O
sets	O
of	O
17	O
snps	O
that	O
would	O
predict	O
blood	O
pressure	O
just	O
as	O
well	O
as	O
the	O
selected	O
model	B
.	O
if	O
we	O
were	O
to	O
obtain	O
an	O
independent	B
data	O
set	B
and	O
perform	O
forward	B
stepwise	I
selection	I
on	O
that	O
data	B
set	O
,	O
we	O
would	O
likely	O
obtain	O
a	O
model	B
containing	O
a	O
diﬀerent	O
,	O
and	O
perhaps	O
even	O
non-overlapping	O
,	O
set	B
of	O
snps	O
.	O
this	O
does	O
not	O
detract	O
from	O
the	O
value	O
of	O
the	O
model	B
obtained—	O
for	O
instance	O
,	O
the	O
model	B
might	O
turn	O
out	O
to	O
be	O
very	O
eﬀective	O
in	O
predicting	O
blood	O
pressure	O
on	O
an	O
independent	B
set	O
of	O
patients	O
,	O
and	O
might	O
be	O
clinically	O
useful	O
for	O
physicians	O
.	O
but	O
we	O
must	O
be	O
careful	O
not	O
to	O
overstate	O
the	O
results	O
obtained	O
,	O
and	O
to	O
make	O
it	O
clear	O
that	O
what	O
we	O
have	O
identiﬁed	O
is	O
simply	O
one	O
of	O
many	O
possible	O
models	O
for	O
predicting	O
blood	O
pressure	O
,	O
and	O
that	O
it	O
must	O
be	O
further	O
validated	O
on	O
independent	B
data	O
sets	O
.	O
it	O
is	O
also	O
important	O
to	O
be	O
particularly	O
careful	O
in	O
reporting	O
errors	O
and	O
measures	O
of	O
model	B
ﬁt	O
in	O
the	O
high-dimensional	B
setting	O
.	O
we	O
have	O
seen	O
that	O
when	O
p	O
>	O
n	O
,	O
it	O
is	O
easy	O
to	O
obtain	O
a	O
useless	O
model	B
that	O
has	O
zero	O
residu-	O
als	O
.	O
therefore	O
,	O
one	O
should	O
never	O
use	O
sum	O
of	O
squared	O
errors	O
,	O
p-values	O
,	O
r2	O
244	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
statistics	O
,	O
or	O
other	O
traditional	O
measures	O
of	O
model	B
ﬁt	O
on	O
the	O
training	B
data	O
as	O
evidence	O
of	O
a	O
good	O
model	B
ﬁt	O
in	O
the	O
high-dimensional	B
setting	O
.	O
for	O
instance	O
,	O
as	O
we	O
saw	O
in	O
figure	O
6.23	O
,	O
one	O
can	O
easily	O
obtain	O
a	O
model	B
with	O
r2	O
=	O
1	O
when	O
p	O
>	O
n.	O
reporting	O
this	O
fact	O
might	O
mislead	O
others	O
into	O
thinking	O
that	O
a	O
sta-	O
tistically	O
valid	O
and	O
useful	O
model	B
has	O
been	O
obtained	O
,	O
whereas	O
in	O
fact	O
this	O
provides	O
absolutely	O
no	O
evidence	O
of	O
a	O
compelling	O
model	B
.	O
it	O
is	O
important	O
to	O
instead	O
report	O
results	O
on	O
an	O
independent	B
test	O
set	B
,	O
or	O
cross-validation	B
errors	O
.	O
for	O
instance	O
,	O
the	O
mse	O
or	O
r2	O
on	O
an	O
independent	B
test	O
set	B
is	O
a	O
valid	O
measure	O
of	O
model	B
ﬁt	O
,	O
but	O
the	O
mse	O
on	O
the	O
training	B
set	O
certainly	O
is	O
not	O
.	O
6.5	O
lab	O
1	O
:	O
subset	B
selection	I
methods	O
6.5.1	O
best	B
subset	I
selection	I
here	O
we	O
apply	O
the	O
best	B
subset	I
selection	I
approach	O
to	O
the	O
hitters	O
data	B
.	O
we	O
wish	O
to	O
predict	O
a	O
baseball	O
player	O
’	O
s	O
salary	O
on	O
the	O
basis	B
of	O
various	O
statistics	O
associated	O
with	O
performance	O
in	O
the	O
previous	O
year	O
.	O
first	O
of	O
all	O
,	O
we	O
note	O
that	O
the	O
salary	O
variable	B
is	O
missing	O
for	O
some	O
of	O
the	O
players	O
.	O
the	O
is.na	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
identify	O
the	O
missing	O
observa-	O
tions	O
.	O
it	O
returns	O
a	O
vector	B
of	O
the	O
same	O
length	O
as	O
the	O
input	B
vector	O
,	O
with	O
a	O
true	O
for	O
any	O
elements	O
that	O
are	O
missing	O
,	O
and	O
a	O
false	O
for	O
non-missing	O
elements	O
.	O
the	O
sum	O
(	O
)	O
function	B
can	O
then	O
be	O
used	O
to	O
count	O
all	O
of	O
the	O
missing	O
elements	O
.	O
is.na	O
(	O
)	O
sum	O
(	O
)	O
>	O
library	O
(	O
islr	O
)	O
>	O
fix	O
(	O
hitters	O
)	O
>	O
names	O
(	O
hitters	O
)	O
''	O
hits	O
``	O
''	O
years	O
``	O
''	O
crbi	O
``	O
''	O
assists	O
``	O
[	O
1	O
]	O
``	O
atbat	O
``	O
[	O
6	O
]	O
``	O
walks	O
``	O
[	O
11	O
]	O
``	O
cruns	O
``	O
[	O
16	O
]	O
``	O
putouts	O
``	O
>	O
dim	O
(	O
hitters	O
)	O
[	O
1	O
]	O
322	O
>	O
sum	O
(	O
is	O
.	O
na	O
(	O
h	O
i	O
t	O
t	O
e	O
r	O
s	O
$	O
s	O
a	O
l	O
a	O
r	O
y	O
)	O
)	O
[	O
1	O
]	O
59	O
20	O
''	O
hmrun	O
``	O
''	O
catbat	O
``	O
''	O
cwalks	O
``	O
''	O
errors	O
``	O
''	O
runs	O
``	O
''	O
chits	O
``	O
''	O
league	O
``	O
''	O
salary	O
``	O
''	O
rbi	O
``	O
''	O
chmrun	O
``	O
''	O
division	O
``	O
''	O
newleague	O
``	O
hence	O
we	O
see	O
that	O
salary	O
is	O
missing	O
for	O
59	O
players	O
.	O
the	O
na.omit	O
(	O
)	O
function	B
removes	O
all	O
of	O
the	O
rows	O
that	O
have	O
missing	O
values	O
in	O
any	O
variable	B
.	O
>	O
hitters	O
=	O
na	O
.	O
omit	O
(	O
hitters	O
)	O
>	O
dim	O
(	O
hitters	O
)	O
[	O
1	O
]	O
263	O
>	O
sum	O
(	O
is	O
.	O
na	O
(	O
hitters	O
)	O
)	O
[	O
1	O
]	O
0	O
20	O
the	O
regsubsets	O
(	O
)	O
function	B
(	O
part	O
of	O
the	O
leaps	O
library	O
)	O
performs	O
best	O
sub-	O
set	B
selection	O
by	O
identifying	O
the	O
best	O
model	O
that	O
contains	O
a	O
given	O
number	O
of	O
predictors	O
,	O
where	O
best	O
is	O
quantiﬁed	O
using	O
rss	O
.	O
the	O
syntax	O
is	O
the	O
same	O
as	O
for	O
lm	O
(	O
)	O
.	O
the	O
summary	O
(	O
)	O
command	O
outputs	O
the	O
best	O
set	O
of	O
variables	O
for	O
each	O
model	B
size	O
.	O
regsubsets	O
(	O
)	O
6.5	O
lab	O
1	O
:	O
subset	B
selection	I
methods	O
245	O
>	O
library	O
(	O
leaps	O
)	O
>	O
regfit	O
.	O
full	O
=	O
regsubset	O
s	O
(	O
salary∼	O
.	O
,	O
hitters	O
)	O
>	O
summary	O
(	O
regfit	O
.	O
full	O
)	O
subset	B
selection	I
object	O
call	O
:	O
regsubsets	O
.	O
formula	O
(	O
salary	O
∼	O
.	O
,	O
hitters	O
)	O
(	O
and	O
intercept	B
)	O
19	O
variables	O
...	O
1	O
subsets	O
of	O
each	O
size	O
up	O
to	O
8	O
selection	B
algorithm	O
:	O
exhaustive	O
atbat	O
hits	O
hmrun	O
runs	O
rbi	O
walks	O
years	O
catbat	O
chits	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
*	O
''	O
(	O
1	O
)	O
``	O
*	O
''	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
*	O
''	O
''	O
``	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
``	O
``	O
''	O
``	O
``	O
``	O
''	O
``	O
``	O
``	O
''	O
``	O
``	O
``	O
''	O
``	O
``	O
``	O
''	O
``	O
``	O
*	O
''	O
''	O
``	O
``	O
*	O
''	O
''	O
``	O
``	O
*	O
''	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
*	O
''	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
*	O
''	O
''	O
``	O
chmrun	O
cruns	O
crbi	O
cwalks	O
leaguen	O
divisionw	O
putouts	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
*	O
''	O
(	O
1	O
)	O
``	O
*	O
''	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
*	O
''	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
assists	O
errors	O
newleaguen	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
``	O
''	O
``	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
''	O
*	O
''	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
(	O
1	O
)	O
``	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
''	O
``	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
an	O
asterisk	O
indicates	O
that	O
a	O
given	O
variable	B
is	O
included	O
in	O
the	O
corresponding	O
model	B
.	O
for	O
instance	O
,	O
this	O
output	B
indicates	O
that	O
the	O
best	O
two-variable	O
model	B
contains	O
only	O
hits	O
and	O
crbi	O
.	O
by	O
default	O
,	O
regsubsets	O
(	O
)	O
only	O
reports	O
results	O
up	O
to	O
the	O
best	O
eight-variable	O
model	B
.	O
but	O
the	O
nvmax	O
option	O
can	O
be	O
used	O
in	O
order	O
to	O
return	O
as	O
many	O
variables	O
as	O
are	O
desired	O
.	O
here	O
we	O
ﬁt	B
up	O
to	O
a	O
19-variable	O
model	B
.	O
>	O
regfit	O
.	O
full	O
=	O
regsubset	O
s	O
(	O
salary∼	O
.	O
,	O
data	B
=	O
hitters	O
,	O
nvmax	O
=19	O
)	O
>	O
reg	O
.	O
summary	O
=	O
summary	O
(	O
regfit	O
.	O
full	O
)	O
the	O
summary	O
(	O
)	O
function	B
also	O
returns	O
r2	O
,	O
rss	O
,	O
adjusted	O
r2	O
,	O
cp	O
,	O
and	O
bic	O
.	O
we	O
can	O
examine	O
these	O
to	O
try	O
to	O
select	O
the	O
best	O
overall	O
model	B
.	O
>	O
names	O
(	O
reg	O
.	O
summary	O
)	O
[	O
1	O
]	O
``	O
which	O
``	O
''	O
rsq	O
``	O
[	O
7	O
]	O
``	O
outmat	O
``	O
``	O
obj	O
``	O
''	O
rss	O
``	O
''	O
adjr2	O
``	O
''	O
cp	O
``	O
''	O
bic	O
``	O
246	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
for	O
instance	O
,	O
we	O
see	O
that	O
the	O
r2	O
statistic	O
increases	O
from	O
32	O
%	O
,	O
when	O
only	O
one	O
variable	B
is	O
included	O
in	O
the	O
model	B
,	O
to	O
almost	O
55	O
%	O
,	O
when	O
all	O
variables	O
are	O
included	O
.	O
as	O
expected	O
,	O
the	O
r2	O
statistic	O
increases	O
monotonically	O
as	O
more	O
variables	O
are	O
included	O
.	O
>	O
reg	O
.	O
summary	O
$	O
r	O
sq	O
[	O
1	O
]	O
0.321	O
0.425	O
0.451	O
0.475	O
0.491	O
0.509	O
0.514	O
0.529	O
0.535	O
[	O
10	O
]	O
0.540	O
0.543	O
0.544	O
0.544	O
0.545	O
0.545	O
0.546	O
0.546	O
0.546	O
[	O
19	O
]	O
0.546	O
plotting	O
rss	O
,	O
adjusted	O
r2	O
,	O
cp	O
,	O
and	O
bic	O
for	O
all	O
of	O
the	O
models	O
at	O
once	O
will	O
help	O
us	O
decide	O
which	O
model	B
to	O
select	O
.	O
note	O
the	O
type=	O
''	O
l	O
''	O
option	O
tells	O
r	O
to	O
connect	O
the	O
plotted	O
points	O
with	O
lines	O
.	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
2	O
,2	O
)	O
)	O
>	O
plot	B
(	O
reg	O
.	O
summary	O
$	O
rss	O
,	O
xlab	O
=	O
''	O
number	O
of	O
variables	O
``	O
,	O
ylab	O
=	O
''	O
rss	O
``	O
,	O
type	O
=	O
''	O
l	O
``	O
)	O
>	O
plot	B
(	O
reg	O
.	O
summary	O
$	O
adjr2	O
,	O
xlab	O
=	O
''	O
number	O
of	O
variables	O
``	O
,	O
ylab	O
=	O
''	O
adjusted	O
rsq	O
``	O
,	O
type	O
=	O
''	O
l	O
``	O
)	O
the	O
points	O
(	O
)	O
command	O
works	O
like	O
the	O
plot	B
(	O
)	O
command	O
,	O
except	O
that	O
it	O
puts	O
points	O
on	O
a	O
plot	B
that	O
has	O
already	O
been	O
created	O
,	O
instead	O
of	O
creating	O
a	O
new	O
plot	B
.	O
the	O
which.max	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
identify	O
the	O
location	O
of	O
the	O
maximum	O
point	O
of	O
a	O
vector	B
.	O
we	O
will	O
now	O
plot	B
a	O
red	O
dot	O
to	O
indicate	O
the	O
model	B
with	O
the	O
largest	O
adjusted	O
r2	O
statistic	O
.	O
points	O
(	O
)	O
>	O
which	O
.	O
max	O
(	O
reg	O
.	O
s	O
u	O
m	O
m	O
a	O
r	O
y	O
$	O
a	O
d	O
j	O
r	O
2	O
)	O
[	O
1	O
]	O
11	O
>	O
points	O
(	O
11	O
,	O
reg	O
.	O
s	O
u	O
m	O
m	O
a	O
r	O
y	O
$	O
a	O
d	O
j	O
r	O
2	O
[	O
11	O
]	O
,	O
col	O
=	O
''	O
red	O
``	O
,	O
cex	O
=2	O
,	O
pch	O
=20	O
)	O
in	O
a	O
similar	O
fashion	O
we	O
can	O
plot	B
the	O
cp	O
and	O
bic	O
statistics	O
,	O
and	O
indicate	O
the	O
models	O
with	O
the	O
smallest	O
statistic	O
using	O
which.min	O
(	O
)	O
.	O
which.min	O
(	O
)	O
>	O
plot	B
(	O
reg	O
.	O
summary	O
$	O
cp	O
,	O
xlab	O
=	O
''	O
number	O
of	O
variables	O
``	O
,	O
ylab	O
=	O
''	O
cp	O
``	O
,	O
type	O
=	O
’	O
l	O
’	O
)	O
>	O
which	O
.	O
min	O
(	O
reg	O
.	O
summary	O
$	O
cp	O
)	O
[	O
1	O
]	O
10	O
>	O
points	O
(	O
10	O
,	O
reg	O
.	O
summary	O
$	O
cp	O
[	O
10	O
]	O
,	O
col	O
=	O
''	O
red	O
``	O
,	O
cex	O
=2	O
,	O
pch	O
=20	O
)	O
>	O
which	O
.	O
min	O
(	O
reg	O
.	O
summary	O
$	O
b	O
i	O
c	O
)	O
[	O
1	O
]	O
6	O
>	O
plot	B
(	O
reg	O
.	O
summary	O
$	O
bic	O
,	O
xlab	O
=	O
''	O
number	O
of	O
variables	O
``	O
,	O
ylab	O
=	O
''	O
bic	O
``	O
,	O
type	O
=	O
’	O
l	O
’	O
)	O
>	O
points	O
(	O
6	O
,	O
reg	O
.	O
summary	O
$	O
b	O
ic	O
[	O
6	O
]	O
,	O
col	O
=	O
''	O
red	O
``	O
,	O
cex	O
=2	O
,	O
pch	O
=20	O
)	O
the	O
regsubsets	O
(	O
)	O
function	B
has	O
a	O
built-in	O
plot	B
(	O
)	O
command	O
which	O
can	O
be	O
used	O
to	O
display	O
the	O
selected	O
variables	O
for	O
the	O
best	O
model	O
with	O
a	O
given	O
number	O
of	O
predictors	O
,	O
ranked	O
according	O
to	O
the	O
bic	O
,	O
cp	O
,	O
adjusted	O
r2	O
,	O
or	O
aic	O
.	O
to	O
ﬁnd	O
out	O
more	O
about	O
this	O
function	B
,	O
type	O
?	O
plot.regsubsets	O
.	O
>	O
plot	B
(	O
regfit	O
.	O
full	O
,	O
scale	O
=	O
''	O
r2	O
``	O
)	O
>	O
plot	B
(	O
regfit	O
.	O
full	O
,	O
scale	O
=	O
''	O
adjr2	O
``	O
)	O
>	O
plot	B
(	O
regfit	O
.	O
full	O
,	O
scale	O
=	O
''	O
cp	O
``	O
)	O
>	O
plot	B
(	O
regfit	O
.	O
full	O
,	O
scale	O
=	O
''	O
bic	O
``	O
)	O
6.5	O
lab	O
1	O
:	O
subset	B
selection	I
methods	O
247	O
the	O
top	O
row	O
of	O
each	O
plot	B
contains	O
a	O
black	O
square	O
for	O
each	O
variable	B
selected	O
according	O
to	O
the	O
optimal	O
model	O
associated	O
with	O
that	O
statistic	O
.	O
for	O
instance	O
,	O
we	O
see	O
that	O
several	O
models	O
share	O
a	O
bic	O
close	O
to	O
−150	O
.	O
however	O
,	O
the	O
model	B
with	O
the	O
lowest	O
bic	O
is	O
the	O
six-variable	O
model	B
that	O
contains	O
only	O
atbat	O
,	O
hits	O
,	O
walks	O
,	O
crbi	O
,	O
divisionw	O
,	O
and	O
putouts	O
.	O
we	O
can	O
use	O
the	O
coef	O
(	O
)	O
function	B
to	O
see	O
the	O
coeﬃcient	B
estimates	O
associated	O
with	O
this	O
model	B
.	O
>	O
coef	O
(	O
regfit	O
.	O
full	O
,6	O
)	O
(	O
intercept	B
)	O
91.512	O
divisionw	O
-122.952	O
atbat	O
-1.869	O
putouts	O
0.264	O
hits	O
7.604	O
walks	O
3.698	O
crbi	O
0.643	O
6.5.2	O
forward	O
and	O
backward	B
stepwise	I
selection	I
we	O
can	O
also	O
use	O
the	O
regsubsets	O
(	O
)	O
function	B
to	O
perform	O
forward	O
stepwise	O
or	O
backward	B
stepwise	I
selection	I
,	O
using	O
the	O
argument	B
method=	O
''	O
forward	O
''	O
or	O
method=	O
''	O
backward	O
''	O
.	O
>	O
regfit	O
.	O
fwd	O
=	O
regsubsets	O
(	O
salary∼	O
.	O
,	O
data	B
=	O
hitters	O
,	O
nvmax	O
=19	O
,	O
method	O
=	O
''	O
forward	O
``	O
)	O
>	O
summary	O
(	O
regfit	O
.	O
fwd	O
)	O
>	O
regfit	O
.	O
bwd	O
=	O
regsubsets	O
(	O
salary∼	O
.	O
,	O
data	B
=	O
hitters	O
,	O
nvmax	O
=19	O
,	O
method	O
=	O
''	O
backward	O
``	O
)	O
>	O
summary	O
(	O
regfit	O
.	O
bwd	O
)	O
for	O
instance	O
,	O
we	O
see	O
that	O
using	O
forward	B
stepwise	I
selection	I
,	O
the	O
best	O
one-	O
variable	B
model	O
contains	O
only	O
crbi	O
,	O
and	O
the	O
best	O
two-variable	O
model	B
ad-	O
ditionally	O
includes	O
hits	O
.	O
for	O
this	O
data	B
,	O
the	O
best	O
one-variable	O
through	O
six-	O
variable	B
models	O
are	O
each	O
identical	O
for	O
best	O
subset	O
and	O
forward	O
selection	O
.	O
however	O
,	O
the	O
best	O
seven-variable	O
models	O
identiﬁed	O
by	O
forward	O
stepwise	O
se-	O
lection	O
,	O
backward	B
stepwise	I
selection	I
,	O
and	O
best	B
subset	I
selection	I
are	O
diﬀerent	O
.	O
>	O
coef	O
(	O
regfit	O
.	O
full	O
,7	O
)	O
(	O
intercept	B
)	O
79.451	O
chmrun	O
1.442	O
hits	O
1.283	O
divisionw	O
-129.987	O
>	O
coef	O
(	O
regfit	O
.	O
fwd	O
,7	O
)	O
(	O
intercept	B
)	O
109.787	O
cwalks	O
-0.305	O
atbat	O
-1.959	O
divisionw	O
-127.122	O
>	O
coef	O
(	O
regfit	O
.	O
bwd	O
,7	O
)	O
(	O
intercept	B
)	O
105.649	O
cwalks	O
-0.716	O
atbat	O
-1.976	O
divisionw	O
-116.169	O
walks	O
3.227	O
putouts	O
0.237	O
hits	O
7.450	O
putouts	O
0.253	O
hits	O
6.757	O
putouts	O
0.303	O
catbat	O
-0.375	O
chits	O
1.496	O
walks	O
4.913	O
crbi	O
0.854	O
walks	O
6.056	O
cruns	O
1.129	O
248	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
6.5.3	O
choosing	O
among	O
models	O
using	O
the	O
validation	B
set	I
approach	O
and	O
cross-validation	B
we	O
just	O
saw	O
that	O
it	O
is	O
possible	O
to	O
choose	O
among	O
a	O
set	B
of	O
models	O
of	O
diﬀerent	O
sizes	O
using	O
cp	O
,	O
bic	O
,	O
and	O
adjusted	O
r2	O
.	O
we	O
will	O
now	O
consider	O
how	O
to	O
do	O
this	O
using	O
the	O
validation	B
set	I
and	O
cross-validation	B
approaches	O
.	O
in	O
order	O
for	O
these	O
approaches	O
to	O
yield	O
accurate	O
estimates	O
of	O
the	O
test	B
error	O
,	O
we	O
must	O
use	O
only	O
the	O
training	B
observations	O
to	O
perform	O
all	O
aspects	O
of	O
model-ﬁtting—including	O
variable	B
selection	O
.	O
therefore	O
,	O
the	O
determination	O
of	O
which	O
model	B
of	O
a	O
given	O
size	O
is	O
best	O
must	O
be	O
made	O
using	O
only	O
the	O
training	B
observations	O
.	O
this	O
point	O
is	O
subtle	O
but	O
important	O
.	O
if	O
the	O
full	O
data	B
set	O
is	O
used	O
to	O
perform	O
the	O
best	B
subset	I
selection	I
step	O
,	O
the	O
validation	B
set	I
errors	O
and	O
cross-validation	B
errors	O
that	O
we	O
obtain	O
will	O
not	O
be	O
accurate	O
estimates	O
of	O
the	O
test	B
error	O
.	O
in	O
order	O
to	O
use	O
the	O
validation	B
set	I
approach	O
,	O
we	O
begin	O
by	O
splitting	O
the	O
observations	B
into	O
a	O
training	B
set	O
and	O
a	O
test	B
set	O
.	O
we	O
do	O
this	O
by	O
creating	O
a	O
random	O
vector	O
,	O
train	B
,	O
of	O
elements	O
equal	O
to	O
true	O
if	O
the	O
corresponding	O
observation	O
is	O
in	O
the	O
training	B
set	O
,	O
and	O
false	O
otherwise	O
.	O
the	O
vector	B
test	O
has	O
a	O
true	O
if	O
the	O
observation	O
is	O
in	O
the	O
test	B
set	O
,	O
and	O
a	O
false	O
otherwise	O
.	O
note	O
the	O
!	O
in	O
the	O
command	O
to	O
create	O
test	B
causes	O
trues	O
to	O
be	O
switched	O
to	O
falses	O
and	O
vice	O
versa	O
.	O
we	O
also	O
set	B
a	O
random	O
seed	O
so	O
that	O
the	O
user	O
will	O
obtain	O
the	O
same	O
training	B
set/test	O
set	B
split	O
.	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
train	B
=	O
sample	O
(	O
c	O
(	O
true	O
,	O
false	O
)	O
,	O
nrow	O
(	O
hitters	O
)	O
,	O
rep	O
=	O
true	O
)	O
>	O
test	B
=	O
(	O
!	O
train	B
)	O
now	O
,	O
we	O
apply	O
regsubsets	O
(	O
)	O
to	O
the	O
training	B
set	O
in	O
order	O
to	O
perform	O
best	B
subset	I
selection	I
.	O
>	O
regfit	O
.	O
best	O
=	O
regsubset	O
s	O
(	O
salary∼	O
.	O
,	O
data	B
=	O
hitters	O
[	O
train	B
,	O
]	O
,	O
nvmax	O
=19	O
)	O
notice	O
that	O
we	O
subset	O
the	O
hitters	O
data	B
frame	I
directly	O
in	O
the	O
call	O
in	O
or-	O
der	O
to	O
access	O
only	O
the	O
training	B
subset	O
of	O
the	O
data	B
,	O
using	O
the	O
expression	O
hitters	O
[	O
train	B
,	O
]	O
.	O
we	O
now	O
compute	O
the	O
validation	B
set	I
error	O
for	O
the	O
best	O
model	O
of	O
each	O
model	B
size	O
.	O
we	O
ﬁrst	O
make	O
a	O
model	B
matrix	O
from	O
the	O
test	B
data	O
.	O
test	B
.	O
mat	O
=	O
model	B
.	O
matrix	O
(	O
salary∼	O
.	O
,	O
data	B
=	O
hitters	O
[	O
test	B
,	O
]	O
)	O
the	O
model.matrix	O
(	O
)	O
function	B
is	O
used	O
in	O
many	O
regression	B
packages	O
for	O
build-	O
ing	O
an	O
“	O
x	O
”	O
matrix	O
from	O
data	B
.	O
now	O
we	O
run	O
a	O
loop	O
,	O
and	O
for	O
each	O
size	O
i	O
,	O
we	O
extract	O
the	O
coeﬃcients	O
from	O
regfit.best	O
for	O
the	O
best	O
model	O
of	O
that	O
size	O
,	O
multiply	O
them	O
into	O
the	O
appropriate	O
columns	O
of	O
the	O
test	B
model	O
matrix	O
to	O
form	O
the	O
predictions	O
,	O
and	O
compute	O
the	O
test	B
mse	O
.	O
model	B
.	O
matrix	O
(	O
)	O
>	O
val	O
.	O
errors	O
=	O
rep	O
(	O
na	O
,19	O
)	O
>	O
for	O
(	O
i	O
in	O
1:19	O
)	O
{	O
+	O
coefi	O
=	O
coef	O
(	O
regfit	O
.	O
best	O
,	O
id	O
=	O
i	O
)	O
6.5	O
lab	O
1	O
:	O
subset	B
selection	I
methods	O
249	O
pred	O
=	O
test	B
.	O
mat	O
[	O
,	O
names	O
(	O
coefi	O
)	O
]	O
%	O
*	O
%	O
coefi	O
val	O
.	O
errors	O
[	O
i	O
]	O
=	O
mean	O
(	O
(	O
h	O
i	O
t	O
t	O
e	O
r	O
s	O
$	O
s	O
a	O
l	O
a	O
r	O
y	O
[	O
test	B
]	O
-	O
pred	O
)	O
^2	O
)	O
+	O
+	O
}	O
we	O
ﬁnd	O
that	O
the	O
best	O
model	O
is	O
the	O
one	O
that	O
contains	O
ten	O
variables	O
.	O
>	O
val	O
.	O
errors	O
[	O
1	O
]	O
220968	O
169157	O
178518	O
163426	O
168418	O
171271	O
162377	O
157909	O
[	O
9	O
]	O
154056	O
148162	O
151156	O
151742	O
152214	O
157359	O
158541	O
158743	O
[	O
17	O
]	O
159973	O
159860	O
160106	O
>	O
which	O
.	O
min	O
(	O
val	O
.	O
errors	O
)	O
[	O
1	O
]	O
10	O
>	O
coef	O
(	O
regfit	O
.	O
best	O
,10	O
)	O
(	O
intercept	B
)	O
-80.275	O
chits	O
1.105	O
putouts	O
0.238	O
atbat	O
-1.468	O
chmrun	O
1.384	O
hits	O
7.163	O
cwalks	O
-0.748	O
walks	O
3.643	O
leaguen	O
84.558	O
catbat	O
-0.186	O
divisionw	O
-53.029	O
this	O
was	O
a	O
little	O
tedious	O
,	O
partly	O
because	O
there	O
is	O
no	O
predict	O
(	O
)	O
method	O
for	O
regsubsets	O
(	O
)	O
.	O
since	O
we	O
will	O
be	O
using	O
this	O
function	B
again	O
,	O
we	O
can	O
capture	O
our	O
steps	O
above	O
and	O
write	O
our	O
own	O
predict	O
method	O
.	O
>	O
+	O
+	O
+	O
+	O
+	O
+	O
predict	O
.	O
regsubset	O
s	O
=	O
function	B
(	O
object	O
,	O
newdata	O
,	O
id	O
,	O
...	O
)	O
{	O
form	O
=	O
as	O
.	O
formula	O
(	O
object	O
$	O
ca	O
l	O
l	O
[	O
[	O
2	O
]	O
]	O
)	O
mat	O
=	O
model	B
.	O
matrix	O
(	O
form	O
,	O
newdata	O
)	O
coefi	O
=	O
coef	O
(	O
object	O
,	O
id	O
=	O
id	O
)	O
xvars	O
=	O
names	O
(	O
coefi	O
)	O
mat	O
[	O
,	O
xvars	O
]	O
%	O
*	O
%	O
coefi	O
}	O
our	O
function	B
pretty	O
much	O
mimics	O
what	O
we	O
did	O
above	O
.	O
the	O
only	O
complex	O
part	O
is	O
how	O
we	O
extracted	O
the	O
formula	O
used	O
in	O
the	O
call	O
to	O
regsubsets	O
(	O
)	O
.	O
we	O
demonstrate	O
how	O
we	O
use	O
this	O
function	B
below	O
,	O
when	O
we	O
do	O
cross-validation	B
.	O
finally	O
,	O
we	O
perform	O
best	B
subset	I
selection	I
on	O
the	O
full	O
data	B
set	O
,	O
and	O
select	O
the	O
best	O
ten-variable	O
model	B
.	O
it	O
is	O
important	O
that	O
we	O
make	O
use	O
of	O
the	O
full	O
data	B
set	O
in	O
order	O
to	O
obtain	O
more	O
accurate	O
coeﬃcient	B
estimates	O
.	O
note	O
that	O
we	O
perform	O
best	B
subset	I
selection	I
on	O
the	O
full	O
data	B
set	O
and	O
select	O
the	O
best	O
ten-	O
variable	B
model	O
,	O
rather	O
than	O
simply	O
using	O
the	O
variables	O
that	O
were	O
obtained	O
from	O
the	O
training	B
set	O
,	O
because	O
the	O
best	O
ten-variable	O
model	B
on	O
the	O
full	O
data	B
set	O
may	O
diﬀer	O
from	O
the	O
corresponding	O
model	B
on	O
the	O
training	B
set	O
.	O
>	O
regfit	O
.	O
best	O
=	O
regsubset	O
s	O
(	O
salary∼	O
.	O
,	O
data	B
=	O
hitters	O
,	O
nvmax	O
=19	O
)	O
>	O
coef	O
(	O
regfit	O
.	O
best	O
,10	O
)	O
(	O
intercept	B
)	O
162.535	O
cruns	O
1.408	O
assists	O
0.283	O
atbat	O
-2.169	O
crbi	O
0.774	O
hits	O
6.918	O
cwalks	O
-0.831	O
walks	O
5.773	O
divisionw	O
-112.380	O
catbat	O
-0.130	O
putouts	O
0.297	O
250	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
in	O
fact	O
,	O
we	O
see	O
that	O
the	O
best	O
ten-variable	O
model	B
on	O
the	O
full	O
data	B
set	O
has	O
a	O
diﬀerent	O
set	B
of	O
variables	O
than	O
the	O
best	O
ten-variable	O
model	B
on	O
the	O
training	B
set	O
.	O
we	O
now	O
try	O
to	O
choose	O
among	O
the	O
models	O
of	O
diﬀerent	O
sizes	O
using	O
cross-	O
validation	O
.	O
this	O
approach	B
is	O
somewhat	O
involved	O
,	O
as	O
we	O
must	O
perform	O
best	B
subset	I
selection	I
within	O
each	O
of	O
the	O
k	O
training	B
sets	O
.	O
despite	O
this	O
,	O
we	O
see	O
that	O
with	O
its	O
clever	O
subsetting	O
syntax	O
,	O
r	O
makes	O
this	O
job	O
quite	O
easy	O
.	O
first	O
,	O
we	O
create	O
a	O
vector	B
that	O
allocates	O
each	O
observation	O
to	O
one	O
of	O
k	O
=	O
10	O
folds	O
,	O
and	O
we	O
create	O
a	O
matrix	O
in	O
which	O
we	O
will	O
store	O
the	O
results	O
.	O
>	O
k	O
=10	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
folds	O
=	O
sample	O
(	O
1	O
:	O
k	O
,	O
nrow	O
(	O
hitters	O
)	O
,	O
replace	O
=	O
true	O
)	O
>	O
cv	O
.	O
errors	O
=	O
matrix	O
(	O
na	O
,	O
k	O
,19	O
,	O
dimnames	O
=	O
list	O
(	O
null	B
,	O
paste	O
(	O
1:19	O
)	O
)	O
)	O
now	O
we	O
write	O
a	O
for	B
loop	I
that	O
performs	O
cross-validation	B
.	O
in	O
the	O
jth	O
fold	O
,	O
the	O
elements	O
of	O
folds	O
that	O
equal	O
j	O
are	O
in	O
the	O
test	B
set	O
,	O
and	O
the	O
remainder	O
are	O
in	O
the	O
training	B
set	O
.	O
we	O
make	O
our	O
predictions	O
for	O
each	O
model	B
size	O
(	O
using	O
our	O
new	O
predict	O
(	O
)	O
method	O
)	O
,	O
compute	O
the	O
test	B
errors	O
on	O
the	O
appropriate	O
subset	O
,	O
and	O
store	O
them	O
in	O
the	O
appropriate	O
slot	O
in	O
the	O
matrix	O
cv.errors	O
.	O
>	O
for	O
(	O
j	O
in	O
1	O
:	O
k	O
)	O
{	O
+	O
best	O
.	O
fit	O
=	O
regsubsets	O
(	O
salary∼	O
.	O
,	O
data	B
=	O
hitters	O
[	O
folds	O
!	O
=	O
j	O
,	O
]	O
,	O
nvmax	O
=19	O
)	O
for	O
(	O
i	O
in	O
1:19	O
)	O
{	O
pred	O
=	O
predict	O
(	O
best	O
.	O
fit	O
,	O
hitters	O
[	O
folds	O
==	O
j	O
,	O
]	O
,	O
id	O
=	O
i	O
)	O
cv	O
.	O
errors	O
[	O
j	O
,	O
i	O
]	O
=	O
mean	O
(	O
(	O
h	O
i	O
t	O
t	O
e	O
r	O
s	O
$	O
s	O
a	O
l	O
a	O
r	O
y	O
[	O
folds	O
==	O
j	O
]	O
-	O
pred	O
)	O
^2	O
)	O
}	O
+	O
+	O
+	O
+	O
+	O
}	O
this	O
has	O
given	O
us	O
a	O
10×19	O
matrix	O
,	O
of	O
which	O
the	O
(	O
i	O
,	O
j	O
)	O
th	O
element	O
corresponds	O
to	O
the	O
test	B
mse	O
for	O
the	O
ith	O
cross-validation	B
fold	O
for	O
the	O
best	O
j-variable	O
model	B
.	O
we	O
use	O
the	O
apply	O
(	O
)	O
function	B
to	O
average	B
over	O
the	O
columns	O
of	O
this	O
matrix	O
in	O
order	O
to	O
obtain	O
a	O
vector	B
for	O
which	O
the	O
jth	O
element	O
is	O
the	O
cross-	O
validation	O
error	O
for	O
the	O
j-variable	O
model	B
.	O
apply	O
(	O
)	O
>	O
mean	O
.	O
cv	O
.	O
errors	O
=	O
apply	O
(	O
cv	O
.	O
errors	O
,2	O
,	O
mean	O
)	O
>	O
mean	O
.	O
cv	O
.	O
errors	O
[	O
1	O
]	O
160093	O
140197	O
153117	O
151159	O
146841	O
138303	O
144346	O
130208	O
[	O
9	O
]	O
129460	O
125335	O
125154	O
128274	O
133461	O
133975	O
131826	O
131883	O
[	O
17	O
]	O
132751	O
133096	O
132805	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
1	O
,1	O
)	O
)	O
>	O
plot	B
(	O
mean	O
.	O
cv	O
.	O
errors	O
,	O
type	O
=	O
’	O
b	O
’	O
)	O
we	O
see	O
that	O
cross-validation	B
selects	O
an	O
11-variable	O
model	B
.	O
we	O
now	O
perform	O
best	B
subset	I
selection	I
on	O
the	O
full	O
data	B
set	O
in	O
order	O
to	O
obtain	O
the	O
11-variable	O
model	B
.	O
>	O
reg	O
.	O
best	O
=	O
regsubset	O
s	O
(	O
salary∼	O
.	O
,	O
data	B
=	O
hitters	O
,	O
nvmax	O
=19	O
)	O
>	O
coef	O
(	O
reg	O
.	O
best	O
,11	O
)	O
(	O
intercept	B
)	O
135.751	O
atbat	O
-2.128	O
hits	O
6.924	O
walks	O
5.620	O
catbat	O
-0.139	O
glmnet	O
(	O
)	O
6.6	O
lab	O
2	O
:	O
ridge	B
regression	I
and	O
the	O
lasso	B
251	O
cruns	O
1.455	O
putouts	O
0.289	O
crbi	O
0.785	O
assists	O
0.269	O
cwalks	O
-0.823	O
leaguen	O
43.112	O
divisionw	O
-111.146	O
6.6	O
lab	O
2	O
:	O
ridge	B
regression	I
and	O
the	O
lasso	B
we	O
will	O
use	O
the	O
glmnet	O
package	O
in	O
order	O
to	O
perform	O
ridge	B
regression	I
and	O
the	O
lasso	B
.	O
the	O
main	O
function	O
in	O
this	O
package	O
is	O
glmnet	O
(	O
)	O
,	O
which	O
can	O
be	O
used	O
to	O
ﬁt	B
ridge	O
regression	B
models	O
,	O
lasso	B
models	O
,	O
and	O
more	O
.	O
this	O
function	B
has	O
slightly	O
diﬀerent	O
syntax	O
from	O
other	O
model-ﬁtting	O
functions	O
that	O
we	O
have	O
encountered	O
thus	O
far	O
in	O
this	O
book	O
.	O
in	O
particular	O
,	O
we	O
must	O
pass	O
in	O
an	O
x	O
matrix	O
as	O
well	O
as	O
a	O
y	O
vector	B
,	O
and	O
we	O
do	O
not	O
use	O
the	O
y	O
∼	O
x	O
syntax	O
.	O
we	O
will	O
now	O
perform	O
ridge	B
regression	I
and	O
the	O
lasso	B
in	O
order	O
to	O
predict	O
salary	O
on	O
the	O
hitters	O
data	B
.	O
before	O
proceeding	O
ensure	O
that	O
the	O
missing	O
values	O
have	O
been	O
removed	O
from	O
the	O
data	B
,	O
as	O
described	O
in	O
section	O
6.5	O
.	O
>	O
x	O
=	O
model	B
.	O
matrix	O
(	O
salary∼	O
.	O
,	O
hitters	O
)	O
[	O
,	O
-1	O
]	O
>	O
y	O
=	O
h	O
i	O
t	O
t	O
e	O
r	O
s	O
$	O
s	O
a	O
l	O
a	O
r	O
y	O
the	O
model.matrix	O
(	O
)	O
function	B
is	O
particularly	O
useful	O
for	O
creating	O
x	O
;	O
not	O
only	O
does	O
it	O
produce	O
a	O
matrix	O
corresponding	O
to	O
the	O
19	O
predictors	O
but	O
it	O
also	O
automatically	O
transforms	O
any	O
qualitative	B
variables	O
into	O
dummy	B
variables	O
.	O
the	O
latter	O
property	O
is	O
important	O
because	O
glmnet	O
(	O
)	O
can	O
only	O
take	O
numerical	O
,	O
quantitative	B
inputs	O
.	O
6.6.1	O
ridge	B
regression	I
the	O
glmnet	O
(	O
)	O
function	B
has	O
an	O
alpha	O
argument	B
that	O
determines	O
what	O
type	O
of	O
model	B
is	O
ﬁt	B
.	O
if	O
alpha=0	O
then	O
a	O
ridge	B
regression	I
model	O
is	O
ﬁt	B
,	O
and	O
if	O
alpha=1	O
then	O
a	O
lasso	B
model	O
is	O
ﬁt	B
.	O
we	O
ﬁrst	O
ﬁt	B
a	O
ridge	B
regression	I
model	O
.	O
>	O
library	O
(	O
glmnet	O
)	O
>	O
grid	O
=10^	O
seq	O
(	O
10	O
,	O
-2	O
,	O
length	O
=100	O
)	O
>	O
ridge	O
.	O
mod	O
=	O
glmnet	O
(	O
x	O
,	O
y	O
,	O
alpha	O
=0	O
,	O
lambda	O
=	O
grid	O
)	O
by	O
default	O
the	O
glmnet	O
(	O
)	O
function	B
performs	O
ridge	B
regression	I
for	O
an	O
automati-	O
cally	O
selected	O
range	O
of	O
λ	O
values	O
.	O
however	O
,	O
here	O
we	O
have	O
chosen	O
to	O
implement	O
−2	O
,	O
es-	O
the	O
function	B
over	O
a	O
grid	O
of	O
values	O
ranging	O
from	O
λ	O
=	O
1010	O
to	O
λ	O
=	O
10	O
sentially	O
covering	O
the	O
full	O
range	O
of	O
scenarios	O
from	O
the	O
null	B
model	O
containing	O
only	O
the	O
intercept	B
,	O
to	O
the	O
least	B
squares	I
ﬁt	O
.	O
as	O
we	O
will	O
see	O
,	O
we	O
can	O
also	O
com-	O
pute	O
model	B
ﬁts	O
for	O
a	O
particular	O
value	O
of	O
λ	O
that	O
is	O
not	O
one	O
of	O
the	O
original	O
grid	O
values	O
.	O
note	O
that	O
by	O
default	O
,	O
the	O
glmnet	O
(	O
)	O
function	B
standardizes	O
the	O
variables	O
so	O
that	O
they	O
are	O
on	O
the	O
same	O
scale	O
.	O
to	O
turn	O
oﬀ	O
this	O
default	O
setting	O
,	O
use	O
the	O
argument	B
standardize=false	O
.	O
associated	O
with	O
each	O
value	O
of	O
λ	O
is	O
a	O
vector	B
of	O
ridge	B
regression	I
coeﬃcients	O
,	O
stored	O
in	O
a	O
matrix	O
that	O
can	O
be	O
accessed	O
by	O
coef	O
(	O
)	O
.	O
in	O
this	O
case	O
,	O
it	O
is	O
a	O
20×100	O
252	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
matrix	O
,	O
with	O
20	O
rows	O
(	O
one	O
for	O
each	O
predictor	B
,	O
plus	O
an	O
intercept	B
)	O
and	O
100	O
columns	O
(	O
one	O
for	O
each	O
value	O
of	O
λ	O
)	O
.	O
>	O
dim	O
(	O
coef	O
(	O
ridge	O
.	O
mod	O
)	O
)	O
[	O
1	O
]	O
20	O
100	O
we	O
expect	O
the	O
coeﬃcient	B
estimates	O
to	O
be	O
much	O
smaller	O
,	O
in	O
terms	O
of	O
(	O
cid:6	O
)	O
2	O
norm	O
,	O
when	O
a	O
large	O
value	O
of	O
λ	O
is	O
used	O
,	O
as	O
compared	O
to	O
when	O
a	O
small	O
value	O
of	O
λ	O
is	O
used	O
.	O
these	O
are	O
the	O
coeﬃcients	O
when	O
λ	O
=	O
11,498	O
,	O
along	O
with	O
their	O
(	O
cid:6	O
)	O
2	O
norm	O
:	O
>	O
ridge	O
.	O
mod	O
$	O
lambd	O
a	O
[	O
50	O
]	O
[	O
1	O
]	O
11498	O
>	O
coef	O
(	O
ridge	O
.	O
mod	O
)	O
[	O
,50	O
]	O
(	O
intercept	B
)	O
407.356	O
rbi	O
0.240	O
chmrun	O
0.088	O
divisionw	O
-6.215	O
atbat	O
0.037	O
walks	O
0.290	O
cruns	O
0.023	O
putouts	O
0.016	O
hits	O
0.138	O
years	O
1.108	O
crbi	O
0.024	O
assists	O
0.003	O
hmrun	O
0.525	O
catbat	O
0.003	O
cwalks	O
0.025	O
errors	O
-0.021	O
runs	O
0.231	O
chits	O
0.012	O
leaguen	O
0.085	O
newleague	O
n	O
0.301	O
>	O
sqrt	O
(	O
sum	O
(	O
coef	O
(	O
ridge	O
.	O
mod	O
)	O
[	O
-1	O
,50	O
]	O
^2	O
)	O
)	O
[	O
1	O
]	O
6.36	O
in	O
contrast	B
,	O
here	O
are	O
the	O
coeﬃcients	O
when	O
λ	O
=	O
705	O
,	O
along	O
with	O
their	O
(	O
cid:6	O
)	O
2	O
norm	O
.	O
note	O
the	O
much	O
larger	O
(	O
cid:6	O
)	O
2	O
norm	O
of	O
the	O
coeﬃcients	O
associated	O
with	O
this	O
smaller	O
value	O
of	O
λ	O
.	O
>	O
ridge	O
.	O
mod	O
$	O
lambd	O
a	O
[	O
60	O
]	O
[	O
1	O
]	O
705	O
>	O
coef	O
(	O
ridge	O
.	O
mod	O
)	O
[	O
,60	O
]	O
(	O
intercept	B
)	O
54.325	O
rbi	O
0.847	O
chmrun	O
0.338	O
divisionw	O
-54.659	O
atbat	O
0.112	O
walks	O
1.320	O
cruns	O
0.094	O
putouts	O
0.119	O
hits	O
0.656	O
years	O
2.596	O
crbi	O
0.098	O
assists	O
0.016	O
hmrun	O
1.180	O
catbat	O
0.011	O
cwalks	O
0.072	O
errors	O
-0.704	O
runs	O
0.938	O
chits	O
0.047	O
leaguen	O
13.684	O
newleague	O
n	O
8.612	O
>	O
sqrt	O
(	O
sum	O
(	O
coef	O
(	O
ridge	O
.	O
mod	O
)	O
[	O
-1	O
,60	O
]	O
^2	O
)	O
)	O
[	O
1	O
]	O
57.1	O
we	O
can	O
use	O
the	O
predict	O
(	O
)	O
function	B
for	O
a	O
number	O
of	O
purposes	O
.	O
for	O
instance	O
,	O
we	O
can	O
obtain	O
the	O
ridge	B
regression	I
coeﬃcients	O
for	O
a	O
new	O
value	O
of	O
λ	O
,	O
say	O
50	O
:	O
>	O
predict	O
(	O
ridge	O
.	O
mod	O
,	O
s	O
=50	O
,	O
type	O
=	O
''	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
``	O
)	O
[	O
1:20	O
,	O
]	O
(	O
intercept	B
)	O
48.766	O
rbi	O
0.804	O
chmrun	O
0.624	O
divisionw	O
-118.201	O
hits	O
1.969	O
years	O
-6.218	O
crbi	O
0.219	O
assists	O
0.122	O
atbat	O
-0.358	O
walks	O
2.716	O
cruns	O
0.221	O
putouts	O
0.250	O
hmrun	O
-1.278	O
catbat	O
0.005	O
cwalks	O
-0.150	O
errors	O
-3.279	O
runs	O
1.146	O
chits	O
0.106	O
leaguen	O
45.926	O
newleague	O
n	O
-9.497	O
6.6	O
lab	O
2	O
:	O
ridge	B
regression	I
and	O
the	O
lasso	B
253	O
we	O
now	O
split	O
the	O
samples	O
into	O
a	O
training	B
set	O
and	O
a	O
test	B
set	O
in	O
order	O
to	O
estimate	O
the	O
test	B
error	O
of	O
ridge	B
regression	I
and	O
the	O
lasso	B
.	O
there	O
are	O
two	O
common	O
ways	O
to	O
randomly	O
split	O
a	O
data	B
set	O
.	O
the	O
ﬁrst	O
is	O
to	O
produce	O
a	O
random	O
vector	O
of	O
true	O
,	O
false	O
elements	O
and	O
select	O
the	O
observations	B
corresponding	O
to	O
true	O
for	O
the	O
training	B
data	O
.	O
the	O
second	O
is	O
to	O
randomly	O
choose	O
a	O
subset	O
of	O
numbers	O
between	O
1	O
and	O
n	O
;	O
these	O
can	O
then	O
be	O
used	O
as	O
the	O
indices	O
for	O
the	O
training	B
observations	O
.	O
the	O
two	O
approaches	O
work	O
equally	O
well	O
.	O
we	O
used	O
the	O
former	O
method	O
in	O
section	O
6.5.3.	O
here	O
we	O
demonstrate	O
the	O
latter	O
approach	B
.	O
we	O
ﬁrst	O
set	B
a	O
random	O
seed	O
so	O
that	O
the	O
results	O
obtained	O
will	O
be	O
repro-	O
ducible	O
.	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
train	B
=	O
sample	O
(	O
1	O
:	O
nrow	O
(	O
x	O
)	O
,	O
nrow	O
(	O
x	O
)	O
/2	O
)	O
>	O
test	B
=	O
(	O
-	O
train	B
)	O
>	O
y	O
.	O
test	B
=	O
y	O
[	O
test	B
]	O
next	O
we	O
ﬁt	B
a	O
ridge	B
regression	I
model	O
on	O
the	O
training	B
set	O
,	O
and	O
evaluate	O
its	O
mse	O
on	O
the	O
test	B
set	O
,	O
using	O
λ	O
=	O
4.	O
note	O
the	O
use	O
of	O
the	O
predict	O
(	O
)	O
function	B
again	O
.	O
this	O
time	O
we	O
get	O
predictions	O
for	O
a	O
test	B
set	O
,	O
by	O
replacing	O
type=	O
''	O
coefficients	O
''	O
with	O
the	O
newx	O
argument	B
.	O
>	O
ridge	O
.	O
mod	O
=	O
glmnet	O
(	O
x	O
[	O
train	B
,	O
]	O
,	O
y	O
[	O
train	B
]	O
,	O
alpha	O
=0	O
,	O
lambda	O
=	O
grid	O
,	O
thresh	O
=1	O
e	O
-12	O
)	O
>	O
ridge	O
.	O
pred	O
=	O
predict	O
(	O
ridge	O
.	O
mod	O
,	O
s	O
=4	O
,	O
newx	O
=	O
x	O
[	O
test	B
,	O
]	O
)	O
>	O
mean	O
(	O
(	O
ridge	O
.	O
pred	O
-	O
y	O
.	O
test	B
)	O
^2	O
)	O
[	O
1	O
]	O
101037	O
the	O
test	B
mse	O
is	O
101037.	O
note	O
that	O
if	O
we	O
had	O
instead	O
simply	O
ﬁt	B
a	O
model	B
with	O
just	O
an	O
intercept	B
,	O
we	O
would	O
have	O
predicted	O
each	O
test	B
observation	O
using	O
the	O
mean	O
of	O
the	O
training	B
observations	O
.	O
in	O
that	O
case	O
,	O
we	O
could	O
compute	O
the	O
test	B
set	O
mse	O
like	O
this	O
:	O
>	O
mean	O
(	O
(	O
mean	O
(	O
y	O
[	O
train	B
]	O
)	O
-	O
y	O
.	O
test	B
)	O
^2	O
)	O
[	O
1	O
]	O
193253	O
we	O
could	O
also	O
get	O
the	O
same	O
result	O
by	O
ﬁtting	O
a	O
ridge	B
regression	I
model	O
with	O
a	O
very	O
large	O
value	O
of	O
λ.	O
note	O
that	O
1e10	O
means	O
1010	O
.	O
>	O
ridge	O
.	O
pred	O
=	O
predict	O
(	O
ridge	O
.	O
mod	O
,	O
s	O
=1	O
e10	O
,	O
newx	O
=	O
x	O
[	O
test	B
,	O
]	O
)	O
>	O
mean	O
(	O
(	O
ridge	O
.	O
pred	O
-	O
y	O
.	O
test	B
)	O
^2	O
)	O
[	O
1	O
]	O
193253	O
so	O
ﬁtting	O
a	O
ridge	B
regression	I
model	O
with	O
λ	O
=	O
4	O
leads	O
to	O
a	O
much	O
lower	O
test	B
mse	O
than	O
ﬁtting	O
a	O
model	B
with	O
just	O
an	O
intercept	B
.	O
we	O
now	O
check	O
whether	O
there	O
is	O
any	O
beneﬁt	O
to	O
performing	O
ridge	B
regression	I
with	O
λ	O
=	O
4	O
instead	O
of	O
just	O
performing	O
least	B
squares	I
regression	O
.	O
recall	B
that	O
least	B
squares	I
is	O
simply	O
ridge	B
regression	I
with	O
λ	O
=	O
0	O
.	O
7	O
7	O
in	O
order	O
for	O
glmnet	O
(	O
)	O
to	O
yield	O
the	O
exact	O
least	B
squares	I
coeﬃcients	O
when	O
λ	O
=	O
0	O
,	O
we	O
use	O
the	O
argument	B
exact=t	O
when	O
calling	O
the	O
predict	O
(	O
)	O
function	B
.	O
otherwise	O
,	O
the	O
interpolate	O
over	O
the	O
grid	O
of	O
λ	O
values	O
used	O
in	O
ﬁtting	O
the	O
predict	O
(	O
)	O
function	B
will	O
254	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
>	O
ridge	O
.	O
pred	O
=	O
predict	O
(	O
ridge	O
.	O
mod	O
,	O
s	O
=0	O
,	O
newx	O
=	O
x	O
[	O
test	B
,	O
]	O
,	O
exact	O
=	O
t	O
)	O
>	O
mean	O
(	O
(	O
ridge	O
.	O
pred	O
-	O
y	O
.	O
test	B
)	O
^2	O
)	O
[	O
1	O
]	O
114783	O
>	O
lm	O
(	O
y∼x	O
,	O
subset	O
=	O
train	B
)	O
>	O
predict	O
(	O
ridge	O
.	O
mod	O
,	O
s	O
=0	O
,	O
exact	O
=t	O
,	O
type	O
=	O
''	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
``	O
)	O
[	O
1:20	O
,	O
]	O
in	O
general	O
,	O
if	O
we	O
want	O
to	O
ﬁt	B
a	O
(	O
unpenalized	O
)	O
least	B
squares	I
model	O
,	O
then	O
we	O
should	O
use	O
the	O
lm	O
(	O
)	O
function	B
,	O
since	O
that	O
function	B
provides	O
more	O
useful	O
outputs	O
,	O
such	O
as	O
standard	O
errors	O
and	O
p-values	O
for	O
the	O
coeﬃcients	O
.	O
in	O
general	O
,	O
instead	O
of	O
arbitrarily	O
choosing	O
λ	O
=	O
4	O
,	O
it	O
would	O
be	O
better	O
to	O
use	O
cross-validation	B
to	O
choose	O
the	O
tuning	B
parameter	I
λ.	O
we	O
can	O
do	O
this	O
using	O
the	O
built-in	O
cross-validation	B
function	O
,	O
cv.glmnet	O
(	O
)	O
.	O
by	O
default	O
,	O
the	O
function	B
performs	O
ten-fold	O
cross-validation	B
,	O
though	O
this	O
can	O
be	O
changed	O
using	O
the	O
argument	B
nfolds	O
.	O
note	O
that	O
we	O
set	B
a	O
random	O
seed	O
ﬁrst	O
so	O
our	O
results	O
will	O
be	O
reproducible	O
,	O
since	O
the	O
choice	O
of	O
the	O
cross-validation	B
folds	O
is	O
random	O
.	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
cv	O
.	O
out	O
=	O
cv	O
.	O
glmnet	O
(	O
x	O
[	O
train	B
,	O
]	O
,	O
y	O
[	O
train	B
]	O
,	O
alpha	O
=0	O
)	O
>	O
plot	B
(	O
cv	O
.	O
out	O
)	O
>	O
bestlam	O
=	O
cv	O
.	O
out	O
$	O
lambda	O
.	O
min	O
>	O
bestlam	O
[	O
1	O
]	O
212	O
therefore	O
,	O
we	O
see	O
that	O
the	O
value	O
of	O
λ	O
that	O
results	O
in	O
the	O
smallest	O
cross-	O
validation	O
error	O
is	O
212.	O
what	O
is	O
the	O
test	B
mse	O
associated	O
with	O
this	O
value	O
of	O
λ	O
?	O
>	O
ridge	O
.	O
pred	O
=	O
predict	O
(	O
ridge	O
.	O
mod	O
,	O
s	O
=	O
bestlam	O
,	O
newx	O
=	O
x	O
[	O
test	B
,	O
]	O
)	O
>	O
mean	O
(	O
(	O
ridge	O
.	O
pred	O
-	O
y	O
.	O
test	B
)	O
^2	O
)	O
[	O
1	O
]	O
96016	O
this	O
represents	O
a	O
further	O
improvement	O
over	O
the	O
test	B
mse	O
that	O
we	O
got	O
using	O
λ	O
=	O
4.	O
finally	O
,	O
we	O
reﬁt	O
our	O
ridge	B
regression	I
model	O
on	O
the	O
full	O
data	B
set	O
,	O
using	O
the	O
value	O
of	O
λ	O
chosen	O
by	O
cross-validation	B
,	O
and	O
examine	O
the	O
coeﬃcient	B
estimates	O
.	O
cv.glmnet	O
(	O
)	O
>	O
out	O
=	O
glmnet	O
(	O
x	O
,	O
y	O
,	O
alpha	O
=0	O
)	O
>	O
predict	O
(	O
out	O
,	O
type	O
=	O
''	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
``	O
,	O
s	O
=	O
bestlam	O
)	O
[	O
1:20	O
,	O
]	O
(	O
intercept	B
)	O
9.8849	O
rbi	O
0.8732	O
chmrun	O
0.4516	O
divisionw	O
-91.6341	O
hits	O
1.0088	O
years	O
0.1307	O
crbi	O
0.1374	O
assists	O
0.0425	O
atbat	O
0.0314	O
walks	O
1.8041	O
cruns	O
0.1290	O
putouts	O
0.1915	O
hmrun	O
0.1393	O
catbat	O
0.0111	O
cwalks	O
0.0291	O
errors	O
-1.8124	O
runs	O
1.1132	O
chits	O
0.0649	O
leaguen	O
27.1823	O
newleague	O
n	O
7.2121	O
glmnet	O
(	O
)	O
model	B
,	O
yielding	O
approximate	O
results	O
.	O
when	O
we	O
use	O
exact=t	O
,	O
there	O
remains	O
a	O
slight	O
discrepancy	O
in	O
the	O
third	O
decimal	O
place	O
between	O
the	O
output	B
of	O
glmnet	O
(	O
)	O
when	O
λ	O
=	O
0	O
and	O
the	O
output	B
of	O
lm	O
(	O
)	O
;	O
this	O
is	O
due	O
to	O
numerical	O
approximation	O
on	O
the	O
part	O
of	O
glmnet	O
(	O
)	O
.	O
6.6	O
lab	O
2	O
:	O
ridge	B
regression	I
and	O
the	O
lasso	B
255	O
as	O
expected	O
,	O
none	O
of	O
the	O
coeﬃcients	O
are	O
zero—ridge	O
regression	B
does	O
not	O
perform	O
variable	B
selection	O
!	O
6.6.2	O
the	O
lasso	B
we	O
saw	O
that	O
ridge	B
regression	I
with	O
a	O
wise	O
choice	O
of	O
λ	O
can	O
outperform	O
least	B
squares	I
as	O
well	O
as	O
the	O
null	B
model	O
on	O
the	O
hitters	O
data	B
set	O
.	O
we	O
now	O
ask	O
whether	O
the	O
lasso	B
can	O
yield	O
either	O
a	O
more	O
accurate	O
or	O
a	O
more	O
interpretable	O
model	B
than	O
ridge	B
regression	I
.	O
in	O
order	O
to	O
ﬁt	B
a	O
lasso	B
model	O
,	O
we	O
once	O
again	O
use	O
the	O
glmnet	O
(	O
)	O
function	B
;	O
however	O
,	O
this	O
time	O
we	O
use	O
the	O
argument	B
alpha=1	O
.	O
other	O
than	O
that	O
change	O
,	O
we	O
proceed	O
just	O
as	O
we	O
did	O
in	O
ﬁtting	O
a	O
ridge	O
model	O
.	O
>	O
lasso	B
.	O
mod	O
=	O
glmnet	O
(	O
x	O
[	O
train	B
,	O
]	O
,	O
y	O
[	O
train	B
]	O
,	O
alpha	O
=1	O
,	O
lambda	O
=	O
grid	O
)	O
>	O
plot	B
(	O
lasso	B
.	O
mod	O
)	O
we	O
can	O
see	O
from	O
the	O
coeﬃcient	B
plot	O
that	O
depending	O
on	O
the	O
choice	O
of	O
tuning	B
parameter	I
,	O
some	O
of	O
the	O
coeﬃcients	O
will	O
be	O
exactly	O
equal	O
to	O
zero	O
.	O
we	O
now	O
perform	O
cross-validation	B
and	O
compute	O
the	O
associated	O
test	B
error	O
.	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
cv	O
.	O
out	O
=	O
cv	O
.	O
glmnet	O
(	O
x	O
[	O
train	B
,	O
]	O
,	O
y	O
[	O
train	B
]	O
,	O
alpha	O
=1	O
)	O
>	O
plot	B
(	O
cv	O
.	O
out	O
)	O
>	O
bestlam	O
=	O
cv	O
.	O
out	O
$	O
lambda	O
.	O
min	O
>	O
lasso	B
.	O
pred	O
=	O
predict	O
(	O
lasso	B
.	O
mod	O
,	O
s	O
=	O
bestlam	O
,	O
newx	O
=	O
x	O
[	O
test	B
,	O
]	O
)	O
>	O
mean	O
(	O
(	O
lasso	B
.	O
pred	O
-	O
y	O
.	O
test	B
)	O
^2	O
)	O
[	O
1	O
]	O
100743	O
this	O
is	O
substantially	O
lower	O
than	O
the	O
test	B
set	O
mse	O
of	O
the	O
null	B
model	O
and	O
of	O
least	B
squares	I
,	O
and	O
very	O
similar	O
to	O
the	O
test	B
mse	O
of	O
ridge	B
regression	I
with	O
λ	O
chosen	O
by	O
cross-validation	B
.	O
however	O
,	O
the	O
lasso	B
has	O
a	O
substantial	O
advantage	O
over	O
ridge	B
regression	I
in	O
that	O
the	O
resulting	O
coeﬃcient	B
estimates	O
are	O
sparse	B
.	O
here	O
we	O
see	O
that	O
12	O
of	O
the	O
19	O
coeﬃcient	B
estimates	O
are	O
exactly	O
zero	O
.	O
so	O
the	O
lasso	B
model	O
with	O
λ	O
chosen	O
by	O
cross-validation	B
contains	O
only	O
seven	O
variables	O
.	O
>	O
out	O
=	O
glmnet	O
(	O
x	O
,	O
y	O
,	O
alpha	O
=1	O
,	O
lambda	O
=	O
grid	O
)	O
>	O
lasso	B
.	O
coef	O
=	O
predict	O
(	O
out	O
,	O
type	O
=	O
''	O
c	O
o	O
e	O
f	O
f	O
i	O
c	O
i	O
e	O
n	O
t	O
s	O
``	O
,	O
s	O
=	O
bestlam	O
)	O
[	O
1:20	O
,	O
]	O
>	O
lasso	B
.	O
coef	O
(	O
intercept	B
)	O
18.539	O
rbi	O
0.000	O
chmrun	O
0.000	O
divisionw	O
-103.485	O
runs	O
0.000	O
chits	O
0.000	O
leaguen	O
3.267	O
newleague	O
n	O
0.000	O
hmrun	O
0.000	O
catbat	O
0.000	O
cwalks	O
0.000	O
errors	O
0.000	O
atbat	O
0.000	O
walks	O
2.218	O
cruns	O
0.207	O
putouts	O
0.220	O
hits	O
1.874	O
years	O
0.000	O
crbi	O
0.413	O
assists	O
0.000	O
>	O
lasso	B
.	O
coef	O
[	O
lasso	B
.	O
coef	O
!	O
=0	O
]	O
(	O
intercept	B
)	O
18.539	O
leaguen	O
3.267	O
hits	O
1.874	O
divisionw	O
-103.485	O
walks	O
2.218	O
putouts	O
0.220	O
cruns	O
0.207	O
crbi	O
0.413	O
256	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
6.7	O
lab	O
3	O
:	O
pcr	O
and	O
pls	O
regression	B
6.7.1	O
principal	B
components	I
regression	O
principal	B
components	I
regression	O
(	O
pcr	O
)	O
can	O
be	O
performed	O
using	O
the	O
pcr	O
(	O
)	O
function	B
,	O
which	O
is	O
part	O
of	O
the	O
pls	O
library	O
.	O
we	O
now	O
apply	O
pcr	O
to	O
the	O
hitters	O
data	B
,	O
in	O
order	O
to	O
predict	O
salary	O
.	O
again	O
,	O
ensure	O
that	O
the	O
missing	O
values	O
have	O
been	O
removed	O
from	O
the	O
data	B
,	O
as	O
described	O
in	O
section	O
6.5.	O
pcr	O
(	O
)	O
>	O
library	O
(	O
pls	O
)	O
>	O
set	B
.	O
seed	B
(	O
2	O
)	O
>	O
pcr	O
.	O
fit	O
=	O
pcr	O
(	O
salary∼	O
.	O
,	O
data	B
=	O
hitters	O
,	O
scale	O
=	O
true	O
,	O
validation	O
=	O
''	O
cv	O
``	O
)	O
the	O
syntax	O
for	O
the	O
pcr	O
(	O
)	O
function	B
is	O
similar	O
to	O
that	O
for	O
lm	O
(	O
)	O
,	O
with	O
a	O
few	O
additional	O
options	O
.	O
setting	O
scale=true	O
has	O
the	O
eﬀect	O
of	O
standardizing	O
each	O
predictor	B
,	O
using	O
(	O
6.6	O
)	O
,	O
prior	B
to	O
generating	O
the	O
principal	B
components	I
,	O
so	O
that	O
the	O
scale	O
on	O
which	O
each	O
variable	B
is	O
measured	O
will	O
not	O
have	O
an	O
eﬀect	O
.	O
setting	O
validation=	O
''	O
cv	O
''	O
causes	O
pcr	O
(	O
)	O
to	O
compute	O
the	O
ten-fold	O
cross-validation	B
error	O
for	O
each	O
possible	O
value	O
of	O
m	O
,	O
the	O
number	O
of	O
principal	B
components	I
used	O
.	O
the	O
resulting	O
ﬁt	B
can	O
be	O
examined	O
using	O
summary	O
(	O
)	O
.	O
>	O
summary	O
(	O
pcr	O
.	O
fit	O
)	O
data	B
:	O
x	O
dimension	O
:	O
263	O
19	O
y	O
dimension	O
:	O
263	O
1	O
fit	O
method	O
:	O
svdpc	O
number	O
of	O
components	O
considered	O
:	O
19	O
validatio	O
n	O
:	O
rmsep	O
cross	O
-	O
validated	O
using	O
10	O
random	O
segments	O
.	O
(	O
intercept	B
)	O
452	O
452	O
1	O
comps	O
348.9	O
348.7	O
2	O
comps	O
352.2	O
351.8	O
3	O
comps	O
353.5	O
352.9	O
4	O
comps	O
352.8	O
352.1	O
cv	O
adjcv	O
...	O
training	B
:	O
%	O
variance	B
explained	O
1	O
comps	O
38.31	O
40.63	O
2	O
comps	O
60.16	O
41.58	O
3	O
comps	O
70.84	O
42.17	O
4	O
comps	O
79.03	O
43.22	O
5	O
comps	O
84.29	O
44.90	O
6	O
comps	O
88.63	O
46.48	O
x	O
salary	O
...	O
the	O
cv	O
score	O
is	O
provided	O
for	O
each	O
possible	O
number	O
of	O
components	O
,	O
ranging	O
from	O
m	O
=	O
0	O
onwards	O
.	O
(	O
we	O
have	O
printed	O
the	O
cv	O
output	B
only	O
up	O
to	O
m	O
=	O
4	O
.	O
)	O
note	O
that	O
pcr	O
(	O
)	O
reports	O
the	O
root	O
mean	B
squared	I
error	I
;	O
in	O
order	O
to	O
obtain	O
the	O
usual	O
mse	O
,	O
we	O
must	O
square	O
this	O
quantity	O
.	O
for	O
instance	O
,	O
a	O
root	O
mean	B
squared	I
error	I
of	O
352.8	O
corresponds	O
to	O
an	O
mse	O
of	O
352.82	O
=	O
124,468.	O
one	O
can	O
also	O
plot	B
the	O
cross-validation	B
scores	O
using	O
the	O
validationplot	O
(	O
)	O
function	B
.	O
using	O
val.type=	O
''	O
msep	O
''	O
will	O
cause	O
the	O
cross-validation	B
mse	O
to	O
be	O
plotted	O
.	O
>	O
v	O
a	O
l	O
i	O
d	O
a	O
t	O
i	O
o	O
n	O
p	O
l	O
o	O
t	O
(	O
pcr	O
.	O
fit	O
,	O
val	O
.	O
type	O
=	O
''	O
msep	O
``	O
)	O
validation	O
plot	O
(	O
)	O
6.7	O
lab	O
3	O
:	O
pcr	O
and	O
pls	O
regression	B
257	O
we	O
see	O
that	O
the	O
smallest	O
cross-validation	B
error	O
occurs	O
when	O
m	O
=	O
16	O
com-	O
ponents	O
are	O
used	O
.	O
this	O
is	O
barely	O
fewer	O
than	O
m	O
=	O
19	O
,	O
which	O
amounts	O
to	O
simply	O
performing	O
least	B
squares	I
,	O
because	O
when	O
all	O
of	O
the	O
components	O
are	O
used	O
in	O
pcr	O
no	O
dimension	B
reduction	I
occurs	O
.	O
however	O
,	O
from	O
the	O
plot	B
we	O
also	O
see	O
that	O
the	O
cross-validation	B
error	O
is	O
roughly	O
the	O
same	O
when	O
only	O
one	O
component	O
is	O
included	O
in	O
the	O
model	B
.	O
this	O
suggests	O
that	O
a	O
model	B
that	O
uses	O
just	O
a	O
small	O
number	O
of	O
components	O
might	O
suﬃce	O
.	O
the	O
summary	O
(	O
)	O
function	B
also	O
provides	O
the	O
percentage	O
of	O
variance	B
explained	O
in	O
the	O
predictors	O
and	O
in	O
the	O
response	B
using	O
diﬀerent	O
numbers	O
of	O
compo-	O
nents	O
.	O
this	O
concept	O
is	O
discussed	O
in	O
greater	O
detail	O
in	O
chapter	O
10.	O
brieﬂy	O
,	O
we	O
can	O
think	O
of	O
this	O
as	O
the	O
amount	O
of	O
information	O
about	O
the	O
predictors	O
or	O
the	O
response	B
that	O
is	O
captured	O
using	O
m	O
principal	B
components	I
.	O
for	O
example	O
,	O
setting	O
m	O
=	O
1	O
only	O
captures	O
38.31	O
%	O
of	O
all	O
the	O
variance	B
,	O
or	O
information	O
,	O
in	O
the	O
predictors	O
.	O
in	O
contrast	B
,	O
using	O
m	O
=	O
6	O
increases	O
the	O
value	O
to	O
88.63	O
%	O
.	O
if	O
we	O
were	O
to	O
use	O
all	O
m	O
=	O
p	O
=	O
19	O
components	O
,	O
this	O
would	O
increase	O
to	O
100	O
%	O
.	O
we	O
now	O
perform	O
pcr	O
on	O
the	O
training	B
data	O
and	O
evaluate	O
its	O
test	B
set	O
performance	O
.	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
pcr	O
.	O
fit	O
=	O
pcr	O
(	O
salary∼	O
.	O
,	O
data	B
=	O
hitters	O
,	O
subset	O
=	O
train	B
,	O
scale	O
=	O
true	O
,	O
validation	O
=	O
''	O
cv	O
``	O
)	O
>	O
v	O
a	O
l	O
i	O
d	O
a	O
t	O
i	O
o	O
n	O
p	O
l	O
o	O
t	O
(	O
pcr	O
.	O
fit	O
,	O
val	O
.	O
type	O
=	O
''	O
msep	O
``	O
)	O
now	O
we	O
ﬁnd	O
that	O
the	O
lowest	O
cross-validation	B
error	O
occurs	O
when	O
m	O
=	O
7	O
component	O
are	O
used	O
.	O
we	O
compute	O
the	O
test	B
mse	O
as	O
follows	O
.	O
>	O
pcr	O
.	O
pred	O
=	O
predict	O
(	O
pcr	O
.	O
fit	O
,	O
x	O
[	O
test	B
,	O
]	O
,	O
ncomp	O
=7	O
)	O
>	O
mean	O
(	O
(	O
pcr	O
.	O
pred	O
-	O
y	O
.	O
test	B
)	O
^2	O
)	O
[	O
1	O
]	O
96556	O
this	O
test	B
set	O
mse	O
is	O
competitive	O
with	O
the	O
results	O
obtained	O
using	O
ridge	O
re-	O
gression	O
and	O
the	O
lasso	B
.	O
however	O
,	O
as	O
a	O
result	O
of	O
the	O
way	O
pcr	O
is	O
implemented	O
,	O
the	O
ﬁnal	O
model	B
is	O
more	O
diﬃcult	O
to	O
interpret	O
because	O
it	O
does	O
not	O
perform	O
any	O
kind	O
of	O
variable	B
selection	O
or	O
even	O
directly	O
produce	O
coeﬃcient	B
estimates	O
.	O
finally	O
,	O
we	O
ﬁt	B
pcr	O
on	O
the	O
full	O
data	B
set	O
,	O
using	O
m	O
=	O
7	O
,	O
the	O
number	O
of	O
components	O
identiﬁed	O
by	O
cross-validation	B
.	O
>	O
pcr	O
.	O
fit	O
=	O
pcr	O
(	O
y∼x	O
,	O
scale	O
=	O
true	O
,	O
ncomp	O
=7	O
)	O
>	O
summary	O
(	O
pcr	O
.	O
fit	O
)	O
data	B
:	O
x	O
dimension	O
:	O
263	O
19	O
y	O
dimension	O
:	O
263	O
1	O
fit	O
method	O
:	O
svdpc	O
number	O
of	O
components	O
considered	O
:	O
7	O
training	B
:	O
%	O
variance	B
explained	O
2	O
comps	O
60.16	O
41.58	O
3	O
comps	O
70.84	O
42.17	O
4	O
comps	O
79.03	O
43.22	O
5	O
comps	O
84.29	O
44.90	O
6	O
comps	O
88.63	O
46.48	O
1	O
comps	O
38.31	O
40.63	O
7	O
comps	O
92.26	O
46.69	O
x	O
y	O
x	O
y	O
258	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
6.7.2	O
partial	B
least	I
squares	I
we	O
implement	O
partial	B
least	I
squares	I
(	O
pls	O
)	O
using	O
the	O
plsr	O
(	O
)	O
function	B
,	O
also	O
in	O
the	O
pls	O
library	O
.	O
the	O
syntax	O
is	O
just	O
like	O
that	O
of	O
the	O
pcr	O
(	O
)	O
function	B
.	O
plsr	O
(	O
)	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
pls	O
.	O
fit	O
=	O
plsr	O
(	O
salary∼	O
.	O
,	O
data	B
=	O
hitters	O
,	O
subset	O
=	O
train	B
,	O
scale	O
=	O
true	O
,	O
validation	O
=	O
''	O
cv	O
``	O
)	O
>	O
summary	O
(	O
pls	O
.	O
fit	O
)	O
data	B
:	O
x	O
dimension	O
:	O
131	O
19	O
y	O
dimension	O
:	O
131	O
1	O
fit	O
method	O
:	O
kernelpls	O
number	O
of	O
components	O
considered	O
:	O
19	O
validatio	O
n	O
:	O
rmsep	O
cross	O
-	O
validated	O
using	O
10	O
random	O
segments	O
.	O
(	O
intercept	B
)	O
464.6	O
464.6	O
1	O
comps	O
394.2	O
393.4	O
2	O
comps	O
391.5	O
390.2	O
3	O
comps	O
393.1	O
391.1	O
4	O
comps	O
395.0	O
392.9	O
cv	O
adjcv	O
...	O
training	B
:	O
%	O
variance	B
explained	O
1	O
comps	O
38.12	O
33.58	O
4	O
comps	O
74.49	O
42.43	O
5	O
comps	O
79.33	O
44.04	O
6	O
comps	O
84.56	O
45.59	O
2	O
comps	O
53.46	O
38.96	O
3	O
comps	O
66.05	O
41.57	O
x	O
salary	O
...	O
>	O
v	O
a	O
l	O
i	O
d	O
a	O
t	O
i	O
o	O
n	O
p	O
l	O
o	O
t	O
(	O
pls	O
.	O
fit	O
,	O
val	O
.	O
type	O
=	O
''	O
msep	O
``	O
)	O
the	O
lowest	O
cross-validation	B
error	O
occurs	O
when	O
only	O
m	O
=	O
2	O
partial	B
least	I
squares	I
directions	O
are	O
used	O
.	O
we	O
now	O
evaluate	O
the	O
corresponding	O
test	B
set	O
mse	O
.	O
>	O
pls	O
.	O
pred	O
=	O
predict	O
(	O
pls	O
.	O
fit	O
,	O
x	O
[	O
test	B
,	O
]	O
,	O
ncomp	O
=2	O
)	O
>	O
mean	O
(	O
(	O
pls	O
.	O
pred	O
-	O
y	O
.	O
test	B
)	O
^2	O
)	O
[	O
1	O
]	O
101417	O
the	O
test	B
mse	O
is	O
comparable	O
to	O
,	O
but	O
slightly	O
higher	O
than	O
,	O
the	O
test	B
mse	O
obtained	O
using	O
ridge	B
regression	I
,	O
the	O
lasso	B
,	O
and	O
pcr	O
.	O
finally	O
,	O
we	O
perform	O
pls	O
using	O
the	O
full	O
data	B
set	O
,	O
using	O
m	O
=	O
2	O
,	O
the	O
number	O
of	O
components	O
identiﬁed	O
by	O
cross-validation	B
.	O
>	O
pls	O
.	O
fit	O
=	O
plsr	O
(	O
salary∼	O
.	O
,	O
data	B
=	O
hitters	O
,	O
scale	O
=	O
true	O
,	O
ncomp	O
=2	O
)	O
>	O
summary	O
(	O
pls	O
.	O
fit	O
)	O
data	B
:	O
x	O
dimension	O
:	O
263	O
19	O
y	O
dimension	O
:	O
263	O
1	O
fit	O
method	O
:	O
kernelpls	O
number	O
of	O
components	O
considered	O
:	O
2	O
training	B
:	O
%	O
variance	B
explained	O
x	O
salary	O
1	O
comps	O
38.08	O
43.05	O
2	O
comps	O
51.03	O
46.40	O
notice	O
that	O
the	O
percentage	O
of	O
variance	B
in	O
salary	O
that	O
the	O
two-component	O
pls	O
ﬁt	B
explains	O
,	O
46.40	O
%	O
,	O
is	O
almost	O
as	O
much	O
as	O
that	O
explained	B
using	O
the	O
6.8	O
exercises	O
259	O
ﬁnal	O
seven-component	O
model	B
pcr	O
ﬁt	B
,	O
46.69	O
%	O
.	O
this	O
is	O
because	O
pcr	O
only	O
attempts	O
to	O
maximize	O
the	O
amount	O
of	O
variance	B
explained	O
in	O
the	O
predictors	O
,	O
while	O
pls	O
searches	O
for	O
directions	O
that	O
explain	O
variance	B
in	O
both	O
the	O
predic-	O
tors	O
and	O
the	O
response	B
.	O
6.8	O
exercises	O
conceptual	O
1.	O
we	O
perform	O
best	O
subset	O
,	O
forward	O
stepwise	O
,	O
and	O
backward	B
stepwise	I
selection	I
on	O
a	O
single	B
data	O
set	B
.	O
for	O
each	O
approach	B
,	O
we	O
obtain	O
p	O
+	O
1	O
models	O
,	O
containing	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
p	O
predictors	O
.	O
explain	O
your	O
answers	O
:	O
(	O
a	O
)	O
which	O
of	O
the	O
three	O
models	O
with	O
k	O
predictors	O
has	O
the	O
smallest	O
training	B
rss	O
?	O
(	O
b	O
)	O
which	O
of	O
the	O
three	O
models	O
with	O
k	O
predictors	O
has	O
the	O
smallest	O
test	B
rss	O
?	O
(	O
c	O
)	O
true	O
or	O
false	O
:	O
i.	O
the	O
predictors	O
in	O
the	O
k-variable	O
model	B
identiﬁed	O
by	O
forward	O
stepwise	O
are	O
a	O
subset	O
of	O
the	O
predictors	O
in	O
the	O
(	O
k	O
+1	O
)	O
-variable	O
model	B
identiﬁed	O
by	O
forward	B
stepwise	I
selection	I
.	O
ii	O
.	O
the	O
predictors	O
in	O
the	O
k-variable	O
model	B
identiﬁed	O
by	O
back-	O
ward	O
stepwise	O
are	O
a	O
subset	O
of	O
the	O
predictors	O
in	O
the	O
(	O
k	O
+	O
1	O
)	O
-	O
variable	B
model	O
identiﬁed	O
by	O
backward	B
stepwise	I
selection	I
.	O
iii	O
.	O
the	O
predictors	O
in	O
the	O
k-variable	O
model	B
identiﬁed	O
by	O
back-	O
ward	O
stepwise	O
are	O
a	O
subset	O
of	O
the	O
predictors	O
in	O
the	O
(	O
k	O
+	O
1	O
)	O
-	O
variable	B
model	O
identiﬁed	O
by	O
forward	B
stepwise	I
selection	I
.	O
iv	O
.	O
the	O
predictors	O
in	O
the	O
k-variable	O
model	B
identiﬁed	O
by	O
forward	O
stepwise	O
are	O
a	O
subset	O
of	O
the	O
predictors	O
in	O
the	O
(	O
k	O
+1	O
)	O
-variable	O
model	B
identiﬁed	O
by	O
backward	B
stepwise	I
selection	I
.	O
v.	O
the	O
predictors	O
in	O
the	O
k-variable	O
model	B
identiﬁed	O
by	O
best	O
subset	O
are	O
a	O
subset	O
of	O
the	O
predictors	O
in	O
the	O
(	O
k	O
+	O
1	O
)	O
-variable	O
model	B
identiﬁed	O
by	O
best	B
subset	I
selection	I
.	O
2.	O
for	O
parts	O
(	O
a	O
)	O
through	O
(	O
c	O
)	O
,	O
indicate	O
which	O
of	O
i.	O
through	O
iv	O
.	O
is	O
correct	O
.	O
justify	O
your	O
answer	O
.	O
(	O
a	O
)	O
the	O
lasso	B
,	O
relative	O
to	O
least	B
squares	I
,	O
is	O
:	O
i.	O
more	O
ﬂexible	B
and	O
hence	O
will	O
give	O
improved	O
prediction	B
ac-	O
curacy	O
when	O
its	O
increase	O
in	O
bias	B
is	O
less	O
than	O
its	O
decrease	O
in	O
variance	B
.	O
ii	O
.	O
more	O
ﬂexible	B
and	O
hence	O
will	O
give	O
improved	O
prediction	B
accu-	O
racy	O
when	O
its	O
increase	O
in	O
variance	B
is	O
less	O
than	O
its	O
decrease	O
in	O
bias	B
.	O
260	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
iii	O
.	O
less	O
ﬂexible	B
and	O
hence	O
will	O
give	O
improved	O
prediction	B
accu-	O
racy	O
when	O
its	O
increase	O
in	O
bias	B
is	O
less	O
than	O
its	O
decrease	O
in	O
variance	B
.	O
iv	O
.	O
less	O
ﬂexible	B
and	O
hence	O
will	O
give	O
improved	O
prediction	B
accu-	O
racy	O
when	O
its	O
increase	O
in	O
variance	B
is	O
less	O
than	O
its	O
decrease	O
in	O
bias	B
.	O
(	O
b	O
)	O
repeat	O
(	O
a	O
)	O
for	O
ridge	O
regression	B
relative	O
to	O
least	B
squares	I
.	O
(	O
c	O
)	O
repeat	O
(	O
a	O
)	O
for	O
non-linear	O
methods	O
relative	O
to	O
least	B
squares	I
.	O
3.	O
suppose	O
we	O
estimate	O
the	O
regression	B
coeﬃcients	O
in	O
a	O
linear	B
regression	I
model	O
by	O
minimizing	O
n	O
(	O
cid:17	O
)	O
i=1	O
⎛	O
⎝yi	O
−	O
β0	O
−	O
p	O
(	O
cid:17	O
)	O
j=1	O
⎞	O
⎠2	O
βjxij	O
subject	O
to	O
p	O
(	O
cid:17	O
)	O
j=1	O
|βj|	O
≤	O
s	O
for	O
a	O
particular	O
value	O
of	O
s.	O
for	O
parts	O
(	O
a	O
)	O
through	O
(	O
e	O
)	O
,	O
indicate	O
which	O
of	O
i.	O
through	O
v.	O
is	O
correct	O
.	O
justify	O
your	O
answer	O
.	O
(	O
a	O
)	O
as	O
we	O
increase	O
s	O
from	O
0	O
,	O
the	O
training	B
rss	O
will	O
:	O
i.	O
increase	O
initially	O
,	O
and	O
then	O
eventually	O
start	O
decreasing	O
in	O
an	O
inverted	O
u	O
shape	O
.	O
ii	O
.	O
decrease	O
initially	O
,	O
and	O
then	O
eventually	O
start	O
increasing	O
in	O
a	O
u	O
shape	O
.	O
iii	O
.	O
steadily	O
increase	O
.	O
iv	O
.	O
steadily	O
decrease	O
.	O
v.	O
remain	O
constant	O
.	O
(	O
b	O
)	O
repeat	O
(	O
a	O
)	O
for	O
test	O
rss	O
.	O
(	O
c	O
)	O
repeat	O
(	O
a	O
)	O
for	O
variance	O
.	O
(	O
d	O
)	O
repeat	O
(	O
a	O
)	O
for	O
(	O
squared	O
)	O
bias	B
.	O
(	O
e	O
)	O
repeat	O
(	O
a	O
)	O
for	O
the	O
irreducible	B
error	I
.	O
4.	O
suppose	O
we	O
estimate	O
the	O
regression	B
coeﬃcients	O
in	O
a	O
linear	B
regression	I
model	O
by	O
minimizing	O
⎛	O
⎝yi	O
−	O
β0	O
−	O
p	O
(	O
cid:17	O
)	O
n	O
(	O
cid:17	O
)	O
⎞	O
⎠2	O
βjxij	O
i=1	O
j=1	O
p	O
(	O
cid:17	O
)	O
j=1	O
β2	O
j	O
+	O
λ	O
for	O
a	O
particular	O
value	O
of	O
λ.	O
for	O
parts	O
(	O
a	O
)	O
through	O
(	O
e	O
)	O
,	O
indicate	O
which	O
of	O
i.	O
through	O
v.	O
is	O
correct	O
.	O
justify	O
your	O
answer	O
.	O
6.8	O
exercises	O
261	O
(	O
a	O
)	O
as	O
we	O
increase	O
λ	O
from	O
0	O
,	O
the	O
training	B
rss	O
will	O
:	O
i.	O
increase	O
initially	O
,	O
and	O
then	O
eventually	O
start	O
decreasing	O
in	O
an	O
inverted	O
u	O
shape	O
.	O
ii	O
.	O
decrease	O
initially	O
,	O
and	O
then	O
eventually	O
start	O
increasing	O
in	O
a	O
u	O
shape	O
.	O
iii	O
.	O
steadily	O
increase	O
.	O
iv	O
.	O
steadily	O
decrease	O
.	O
v.	O
remain	O
constant	O
.	O
(	O
b	O
)	O
repeat	O
(	O
a	O
)	O
for	O
test	O
rss	O
.	O
(	O
c	O
)	O
repeat	O
(	O
a	O
)	O
for	O
variance	O
.	O
(	O
d	O
)	O
repeat	O
(	O
a	O
)	O
for	O
(	O
squared	O
)	O
bias	B
.	O
(	O
e	O
)	O
repeat	O
(	O
a	O
)	O
for	O
the	O
irreducible	B
error	I
.	O
5.	O
it	O
is	O
well-known	O
that	O
ridge	B
regression	I
tends	O
to	O
give	O
similar	O
coeﬃcient	B
values	O
to	O
correlated	O
variables	O
,	O
whereas	O
the	O
lasso	B
may	O
give	O
quite	O
dif-	O
ferent	O
coeﬃcient	B
values	O
to	O
correlated	O
variables	O
.	O
we	O
will	O
now	O
explore	O
this	O
property	O
in	O
a	O
very	O
simple	B
setting	O
.	O
suppose	O
that	O
n	O
=	O
2	O
,	O
p	O
=	O
2	O
,	O
x11	O
=	O
x12	O
,	O
x21	O
=	O
x22	O
.	O
furthermore	O
,	O
suppose	O
that	O
y1	O
+	O
y2	O
=	O
0	O
and	O
x11	O
+	O
x21	O
=	O
0	O
and	O
x12	O
+	O
x22	O
=	O
0	O
,	O
so	O
that	O
the	O
estimate	O
for	O
the	O
intercept	B
in	O
a	O
least	B
squares	I
,	O
ridge	B
regression	I
,	O
or	O
lasso	B
model	O
is	O
zero	O
:	O
ˆβ0	O
=	O
0	O
.	O
(	O
a	O
)	O
write	O
out	O
the	O
ridge	B
regression	I
optimization	O
problem	O
in	O
this	O
set-	O
ting	O
.	O
(	O
b	O
)	O
argue	O
that	O
in	O
this	O
setting	O
,	O
the	O
ridge	O
coeﬃcient	O
estimates	O
satisfy	O
ˆβ1	O
=	O
ˆβ2	O
.	O
(	O
c	O
)	O
write	O
out	O
the	O
lasso	B
optimization	O
problem	O
in	O
this	O
setting	O
.	O
(	O
d	O
)	O
argue	O
that	O
in	O
this	O
setting	O
,	O
the	O
lasso	B
coeﬃcients	O
ˆβ1	O
and	O
ˆβ2	O
are	O
not	O
unique—in	O
other	O
words	O
,	O
there	O
are	O
many	O
possible	O
solutions	O
to	O
the	O
optimization	O
problem	O
in	O
(	O
c	O
)	O
.	O
describe	O
these	O
solutions	O
.	O
6.	O
we	O
will	O
now	O
explore	O
(	O
6.12	O
)	O
and	O
(	O
6.13	O
)	O
further	O
.	O
(	O
a	O
)	O
consider	O
(	O
6.12	O
)	O
with	O
p	O
=	O
1.	O
for	O
some	O
choice	O
of	O
y1	O
and	O
λ	O
>	O
0	O
,	O
plot	B
(	O
6.12	O
)	O
as	O
a	O
function	B
of	O
β1	O
.	O
your	O
plot	B
should	O
conﬁrm	O
that	O
(	O
6.12	O
)	O
is	O
solved	O
by	O
(	O
6.14	O
)	O
.	O
(	O
b	O
)	O
consider	O
(	O
6.13	O
)	O
with	O
p	O
=	O
1.	O
for	O
some	O
choice	O
of	O
y1	O
and	O
λ	O
>	O
0	O
,	O
plot	B
(	O
6.13	O
)	O
as	O
a	O
function	B
of	O
β1	O
.	O
your	O
plot	B
should	O
conﬁrm	O
that	O
(	O
6.13	O
)	O
is	O
solved	O
by	O
(	O
6.15	O
)	O
.	O
262	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
7.	O
we	O
will	O
now	O
derive	O
the	O
bayesian	O
connection	O
to	O
the	O
lasso	B
and	O
ridge	B
regression	I
discussed	O
in	O
section	O
6.2.2	O
.	O
(	O
cid:10	O
)	O
(	O
a	O
)	O
suppose	O
that	O
yi	O
=	O
β0	O
+	O
p	O
j=1	O
xij	O
βj	O
+i	O
where	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
are	O
inde-	O
pendent	O
and	O
identically	O
distributed	O
from	O
a	O
n	O
(	O
0	O
,	O
σ2	O
)	O
distribution	B
.	O
write	O
out	O
the	O
likelihood	O
for	O
the	O
data	B
.	O
(	O
b	O
)	O
assume	O
the	O
following	O
prior	B
for	O
β	O
:	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
are	O
independent	B
and	O
identically	O
distributed	O
according	O
to	O
a	O
double-exponential	B
distribution	I
with	O
mean	O
0	O
and	O
common	O
scale	O
parameter	O
b	O
:	O
i.e	O
.	O
p	O
(	O
β	O
)	O
=	O
1	O
setting	O
.	O
2b	O
exp	O
(	O
−|β|/b	O
)	O
.	O
write	O
out	O
the	O
posterior	B
for	O
β	O
in	O
this	O
(	O
c	O
)	O
argue	O
that	O
the	O
lasso	B
estimate	O
is	O
the	O
mode	B
for	O
β	O
under	O
this	O
pos-	O
terior	O
distribution	B
.	O
(	O
d	O
)	O
now	O
assume	O
the	O
following	O
prior	B
for	O
β	O
:	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
are	O
independent	B
and	O
identically	O
distributed	O
according	O
to	O
a	O
normal	O
distribution	O
with	O
mean	O
zero	O
and	O
variance	B
c.	O
write	O
out	O
the	O
posterior	B
for	O
β	O
in	O
this	O
setting	O
.	O
(	O
e	O
)	O
argue	O
that	O
the	O
ridge	B
regression	I
estimate	O
is	O
both	O
the	O
mode	B
and	O
the	O
mean	O
for	O
β	O
under	O
this	O
posterior	B
distribution	O
.	O
applied	O
8.	O
in	O
this	O
exercise	O
,	O
we	O
will	O
generate	O
simulated	O
data	B
,	O
and	O
will	O
then	O
use	O
this	O
data	B
to	O
perform	O
best	B
subset	I
selection	I
.	O
(	O
a	O
)	O
use	O
the	O
rnorm	O
(	O
)	O
function	B
to	O
generate	O
a	O
predictor	B
x	O
of	O
length	O
n	O
=	O
100	O
,	O
as	O
well	O
as	O
a	O
noise	B
vector	O
	O
of	O
length	O
n	O
=	O
100	O
.	O
(	O
b	O
)	O
generate	O
a	O
response	B
vector	O
y	O
of	O
length	O
n	O
=	O
100	O
according	O
to	O
the	O
model	B
y	O
=	O
β0	O
+	O
β1x	O
+	O
β2x	O
2	O
+	O
β3x	O
3	O
+	O
	O
,	O
where	O
β0	O
,	O
β1	O
,	O
β2	O
,	O
and	O
β3	O
are	O
constants	O
of	O
your	O
choice	O
.	O
(	O
c	O
)	O
use	O
the	O
regsubsets	O
(	O
)	O
function	B
to	O
perform	O
best	B
subset	I
selection	I
in	O
order	O
to	O
choose	O
the	O
best	O
model	O
containing	O
the	O
predictors	O
x	O
,	O
x	O
2	O
,	O
.	O
.	O
.	O
,	O
x	O
10.	O
what	O
is	O
the	O
best	O
model	O
obtained	O
according	O
to	O
cp	O
,	O
bic	O
,	O
and	O
adjusted	O
r2	O
?	O
show	O
some	O
plots	O
to	O
provide	O
evidence	O
for	O
your	O
answer	O
,	O
and	O
report	O
the	O
coeﬃcients	O
of	O
the	O
best	O
model	O
ob-	O
tained	O
.	O
note	O
you	O
will	O
need	O
to	O
use	O
the	O
data.frame	O
(	O
)	O
function	B
to	O
create	O
a	O
single	B
data	O
set	B
containing	O
both	O
x	O
and	O
y	O
.	O
6.8	O
exercises	O
263	O
(	O
d	O
)	O
repeat	O
(	O
c	O
)	O
,	O
using	O
forward	B
stepwise	I
selection	I
and	O
also	O
using	O
back-	O
wards	O
stepwise	O
selection	O
.	O
how	O
does	O
your	O
answer	O
compare	O
to	O
the	O
results	O
in	O
(	O
c	O
)	O
?	O
(	O
e	O
)	O
now	O
ﬁt	B
a	O
lasso	B
model	O
to	O
the	O
simulated	O
data	B
,	O
again	O
using	O
x	O
,	O
x	O
2	O
,	O
.	O
.	O
.	O
,	O
x	O
10	O
as	O
predictors	O
.	O
use	O
cross-validation	B
to	O
select	O
the	O
optimal	O
value	O
of	O
λ.	O
create	O
plots	O
of	O
the	O
cross-validation	B
error	O
as	O
a	O
function	B
of	O
λ.	O
report	O
the	O
resulting	O
coeﬃcient	B
estimates	O
,	O
and	O
discuss	O
the	O
results	O
obtained	O
.	O
(	O
f	O
)	O
now	O
generate	O
a	O
response	B
vector	O
y	O
according	O
to	O
the	O
model	B
y	O
=	O
β0	O
+	O
β7x	O
7	O
+	O
	O
,	O
and	O
perform	O
best	B
subset	I
selection	I
and	O
the	O
lasso	B
.	O
discuss	O
the	O
results	O
obtained	O
.	O
9.	O
in	O
this	O
exercise	O
,	O
we	O
will	O
predict	O
the	O
number	O
of	O
applications	O
received	O
using	O
the	O
other	O
variables	O
in	O
the	O
college	O
data	B
set	O
.	O
(	O
a	O
)	O
split	O
the	O
data	B
set	O
into	O
a	O
training	B
set	O
and	O
a	O
test	B
set	O
.	O
(	O
b	O
)	O
fit	O
a	O
linear	B
model	I
using	O
least	B
squares	I
on	O
the	O
training	B
set	O
,	O
and	O
report	O
the	O
test	B
error	O
obtained	O
.	O
(	O
c	O
)	O
fit	O
a	O
ridge	B
regression	I
model	O
on	O
the	O
training	B
set	O
,	O
with	O
λ	O
chosen	O
by	O
cross-validation	B
.	O
report	O
the	O
test	B
error	O
obtained	O
.	O
(	O
d	O
)	O
fit	O
a	O
lasso	B
model	O
on	O
the	O
training	B
set	O
,	O
with	O
λ	O
chosen	O
by	O
cross-	O
validation	O
.	O
report	O
the	O
test	B
error	O
obtained	O
,	O
along	O
with	O
the	O
num-	O
ber	O
of	O
non-zero	O
coeﬃcient	B
estimates	O
.	O
(	O
e	O
)	O
fit	O
a	O
pcr	O
model	B
on	O
the	O
training	B
set	O
,	O
with	O
m	O
chosen	O
by	O
cross-	O
validation	O
.	O
report	O
the	O
test	B
error	O
obtained	O
,	O
along	O
with	O
the	O
value	O
of	O
m	O
selected	O
by	O
cross-validation	B
.	O
(	O
f	O
)	O
fit	O
a	O
pls	O
model	B
on	O
the	O
training	B
set	O
,	O
with	O
m	O
chosen	O
by	O
cross-	O
validation	O
.	O
report	O
the	O
test	B
error	O
obtained	O
,	O
along	O
with	O
the	O
value	O
of	O
m	O
selected	O
by	O
cross-validation	B
.	O
(	O
g	O
)	O
comment	O
on	O
the	O
results	O
obtained	O
.	O
how	O
accurately	O
can	O
we	O
pre-	O
dict	O
the	O
number	O
of	O
college	O
applications	O
received	O
?	O
is	O
there	O
much	O
diﬀerence	O
among	O
the	O
test	B
errors	O
resulting	O
from	O
these	O
ﬁve	O
ap-	O
proaches	O
?	O
10.	O
we	O
have	O
seen	O
that	O
as	O
the	O
number	O
of	O
features	O
used	O
in	O
a	O
model	B
increases	O
,	O
the	O
training	B
error	O
will	O
necessarily	O
decrease	O
,	O
but	O
the	O
test	B
error	O
may	O
not	O
.	O
we	O
will	O
now	O
explore	O
this	O
in	O
a	O
simulated	O
data	B
set	O
.	O
(	O
a	O
)	O
generate	O
a	O
data	B
set	O
with	O
p	O
=	O
20	O
features	O
,	O
n	O
=	O
1,000	O
observa-	O
tions	O
,	O
and	O
an	O
associated	O
quantitative	B
response	O
vector	B
generated	O
according	O
to	O
the	O
model	B
where	O
β	O
has	O
some	O
elements	O
that	O
are	O
exactly	O
equal	O
to	O
zero	O
.	O
y	O
=	O
xβ	O
+	O
	O
,	O
264	O
6.	O
linear	B
model	I
selection	O
and	O
regularization	B
(	O
b	O
)	O
split	O
your	O
data	B
set	O
into	O
a	O
training	B
set	O
containing	O
100	O
observations	B
and	O
a	O
test	B
set	O
containing	O
900	O
observations	B
.	O
(	O
c	O
)	O
perform	O
best	B
subset	I
selection	I
on	O
the	O
training	B
set	O
,	O
and	O
plot	B
the	O
training	B
set	O
mse	O
associated	O
with	O
the	O
best	O
model	O
of	O
each	O
size	O
.	O
(	O
d	O
)	O
plot	B
the	O
test	B
set	O
mse	O
associated	O
with	O
the	O
best	O
model	O
of	O
each	O
size	O
.	O
(	O
e	O
)	O
for	O
which	O
model	B
size	O
does	O
the	O
test	B
set	O
mse	O
take	O
on	O
its	O
minimum	O
value	O
?	O
comment	O
on	O
your	O
results	O
.	O
if	O
it	O
takes	O
on	O
its	O
minimum	O
value	O
for	O
a	O
model	B
containing	O
only	O
an	O
intercept	B
or	O
a	O
model	B
containing	O
all	O
of	O
the	O
features	O
,	O
then	O
play	O
around	O
with	O
the	O
way	O
that	O
you	O
are	O
generating	O
the	O
data	B
in	O
(	O
a	O
)	O
until	O
you	O
come	O
up	O
with	O
a	O
scenario	O
in	O
which	O
the	O
test	B
set	O
mse	O
is	O
minimized	O
for	O
an	O
intermediate	O
model	B
size	O
.	O
(	O
f	O
)	O
how	O
does	O
the	O
model	B
at	O
which	O
the	O
test	B
set	O
mse	O
is	O
minimized	O
compare	O
to	O
the	O
true	O
model	O
used	O
to	O
generate	O
the	O
data	B
?	O
comment	O
on	O
the	O
coeﬃcient	B
values	O
.	O
’	O
(	O
cid:10	O
)	O
(	O
g	O
)	O
create	O
a	O
plot	B
displaying	O
j	O
)	O
2	O
for	O
a	O
range	O
of	O
values	O
of	O
r	O
,	O
where	O
ˆβr	O
j	O
is	O
the	O
jth	O
coeﬃcient	B
estimate	O
for	O
the	O
best	O
model	O
containing	O
r	O
coeﬃcients	O
.	O
comment	O
on	O
what	O
you	O
observe	O
.	O
how	O
does	O
this	O
compare	O
to	O
the	O
test	B
mse	O
plot	B
from	O
(	O
d	O
)	O
?	O
p	O
j=1	O
(	O
βj	O
−	O
ˆβr	O
11.	O
we	O
will	O
now	O
try	O
to	O
predict	O
per	O
capita	O
crime	O
rate	B
in	O
the	O
boston	O
data	B
set	O
.	O
(	O
a	O
)	O
try	O
out	O
some	O
of	O
the	O
regression	B
methods	O
explored	O
in	O
this	O
chapter	O
,	O
such	O
as	O
best	B
subset	I
selection	I
,	O
the	O
lasso	B
,	O
ridge	B
regression	I
,	O
and	O
pcr	O
.	O
present	O
and	O
discuss	O
results	O
for	O
the	O
approaches	O
that	O
you	O
consider	O
.	O
(	O
b	O
)	O
propose	O
a	O
model	B
(	O
or	O
set	B
of	O
models	O
)	O
that	O
seem	O
to	O
perform	O
well	O
on	O
this	O
data	B
set	O
,	O
and	O
justify	O
your	O
answer	O
.	O
make	O
sure	O
that	O
you	O
are	O
evaluating	O
model	B
performance	O
using	O
validation	B
set	I
error	O
,	O
cross-	O
validation	O
,	O
or	O
some	O
other	O
reasonable	O
alternative	O
,	O
as	O
opposed	O
to	O
using	O
training	B
error	O
.	O
(	O
c	O
)	O
does	O
your	O
chosen	O
model	B
involve	O
all	O
of	O
the	O
features	O
in	O
the	O
data	B
set	O
?	O
why	O
or	O
why	O
not	O
?	O
7	O
moving	O
beyond	O
linearity	O
so	O
far	O
in	O
this	O
book	O
,	O
we	O
have	O
mostly	O
focused	O
on	O
linear	B
models	O
.	O
linear	B
models	O
are	O
relatively	O
simple	B
to	O
describe	O
and	O
implement	O
,	O
and	O
have	O
advantages	O
over	O
other	O
approaches	O
in	O
terms	O
of	O
interpretation	O
and	O
inference	B
.	O
however	O
,	O
stan-	O
dard	O
linear	B
regression	I
can	O
have	O
signiﬁcant	O
limitations	O
in	O
terms	O
of	O
predic-	O
tive	O
power	B
.	O
this	O
is	O
because	O
the	O
linearity	O
assumption	O
is	O
almost	O
always	O
an	O
approximation	O
,	O
and	O
sometimes	O
a	O
poor	O
one	O
.	O
in	O
chapter	O
6	O
we	O
see	O
that	O
we	O
can	O
improve	O
upon	O
least	B
squares	I
using	O
ridge	B
regression	I
,	O
the	O
lasso	B
,	O
principal	O
com-	O
ponents	O
regression	B
,	O
and	O
other	O
techniques	O
.	O
in	O
that	O
setting	O
,	O
the	O
improvement	O
is	O
obtained	O
by	O
reducing	O
the	O
complexity	O
of	O
the	O
linear	B
model	I
,	O
and	O
hence	O
the	O
variance	B
of	O
the	O
estimates	O
.	O
but	O
we	O
are	O
still	O
using	O
a	O
linear	B
model	I
,	O
which	O
can	O
only	O
be	O
improved	O
so	O
far	O
!	O
in	O
this	O
chapter	O
we	O
relax	O
the	O
linearity	O
assumption	O
while	O
still	O
attempting	O
to	O
maintain	O
as	O
much	O
interpretability	B
as	O
possible	O
.	O
we	O
do	O
this	O
by	O
examining	O
very	O
simple	B
extensions	O
of	O
linear	B
models	O
like	O
polyno-	O
mial	O
regression	B
and	O
step	O
functions	O
,	O
as	O
well	O
as	O
more	O
sophisticated	O
approaches	O
such	O
as	O
splines	O
,	O
local	B
regression	I
,	O
and	O
generalized	O
additive	O
models	O
.	O
•	O
polynomial	B
regression	O
extends	O
the	O
linear	B
model	I
by	O
adding	O
extra	O
pre-	O
dictors	O
,	O
obtained	O
by	O
raising	O
each	O
of	O
the	O
original	O
predictors	O
to	O
a	O
power	B
.	O
for	O
example	O
,	O
a	O
cubic	B
regression	O
uses	O
three	O
variables	O
,	O
x	O
,	O
x	O
2	O
,	O
and	O
x	O
3	O
,	O
as	O
predictors	O
.	O
this	O
approach	B
provides	O
a	O
simple	B
way	O
to	O
provide	O
a	O
non-	O
linear	B
ﬁt	O
to	O
data	B
.	O
•	O
step	O
functions	O
cut	O
the	O
range	O
of	O
a	O
variable	B
into	O
k	O
distinct	O
regions	O
in	O
order	O
to	O
produce	O
a	O
qualitative	B
variable	O
.	O
this	O
has	O
the	O
eﬀect	O
of	O
ﬁtting	O
a	O
piecewise	O
constant	O
function	B
.	O
g.	O
james	O
et	O
al.	O
,	O
an	O
introduction	O
to	O
statistical	O
learning	O
:	O
with	O
applications	O
in	O
r	O
,	O
springer	O
texts	O
in	O
statistics	O
,	O
doi	O
10.1007/978-1-4614-7138-7	O
7	O
,	O
©	O
springer	O
science+business	O
media	O
new	O
york	O
2013	O
265	O
266	O
7.	O
moving	O
beyond	O
linearity	O
•	O
regression	B
splines	O
are	O
more	O
ﬂexible	B
than	O
polynomials	O
and	O
step	O
functions	O
,	O
and	O
in	O
fact	O
are	O
an	O
extension	O
of	O
the	O
two	O
.	O
they	O
involve	O
di-	O
viding	O
the	O
range	O
of	O
x	O
into	O
k	O
distinct	O
regions	O
.	O
within	O
each	O
region	O
,	O
a	O
polynomial	B
function	O
is	O
ﬁt	B
to	O
the	O
data	B
.	O
however	O
,	O
these	O
polynomials	O
are	O
constrained	O
so	O
that	O
they	O
join	O
smoothly	O
at	O
the	O
region	O
boundaries	O
,	O
or	O
knots	O
.	O
provided	O
that	O
the	O
interval	B
is	O
divided	O
into	O
enough	O
regions	O
,	O
this	O
can	O
produce	O
an	O
extremely	O
ﬂexible	B
ﬁt	O
.	O
•	O
smoothing	B
splines	O
are	O
similar	O
to	O
regression	B
splines	O
,	O
but	O
arise	O
in	O
a	O
slightly	O
diﬀerent	O
situation	O
.	O
smoothing	B
splines	O
result	O
from	O
minimizing	O
a	O
residual	B
sum	O
of	O
squares	O
criterion	O
subject	O
to	O
a	O
smoothness	O
penalty	B
.	O
•	O
local	B
regression	I
is	O
similar	O
to	O
splines	O
,	O
but	O
diﬀers	O
in	O
an	O
important	O
way	O
.	O
the	O
regions	O
are	O
allowed	O
to	O
overlap	O
,	O
and	O
indeed	O
they	O
do	O
so	O
in	O
a	O
very	O
smooth	O
way	O
.	O
•	O
generalized	O
additive	O
models	O
allow	O
us	O
to	O
extend	O
the	O
methods	O
above	O
to	O
deal	O
with	O
multiple	B
predictors	O
.	O
in	O
sections	O
7.1–7.6	O
,	O
we	O
present	O
a	O
number	O
of	O
approaches	O
for	O
modeling	O
the	O
relationship	O
between	O
a	O
response	B
y	O
and	O
a	O
single	B
predictor	O
x	O
in	O
a	O
ﬂexible	B
way	O
.	O
in	O
section	O
7.7	O
,	O
we	O
show	O
that	O
these	O
approaches	O
can	O
be	O
seamlessly	O
inte-	O
grated	O
in	O
order	O
to	O
model	B
a	O
response	B
y	O
as	O
a	O
function	B
of	O
several	O
predictors	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
.	O
7.1	O
polynomial	B
regression	O
historically	O
,	O
the	O
standard	O
way	O
to	O
extend	O
linear	B
regression	I
to	O
settings	O
in	O
which	O
the	O
relationship	O
between	O
the	O
predictors	O
and	O
the	O
response	B
is	O
non-	O
linear	B
has	O
been	O
to	O
replace	O
the	O
standard	O
linear	O
model	B
yi	O
=	O
β0	O
+	O
β1xi	O
+	O
i	O
with	O
a	O
polynomial	B
function	O
yi	O
=	O
β0	O
+	O
β1xi	O
+	O
β2x2	O
i	O
+	O
β3x3	O
i	O
+	O
.	O
.	O
.	O
+	O
βdxd	O
i	O
+	O
i	O
,	O
(	O
7.1	O
)	O
where	O
i	O
is	O
the	O
error	B
term	O
.	O
this	O
approach	B
is	O
known	O
as	O
polynomial	B
regression	O
,	O
and	O
in	O
fact	O
we	O
saw	O
an	O
example	O
of	O
this	O
method	O
in	O
section	O
3.3.2.	O
for	O
large	O
enough	O
degree	O
d	O
,	O
a	O
polynomial	B
regression	O
allows	O
us	O
to	O
produce	O
an	O
extremely	O
non-linear	B
curve	O
.	O
notice	O
that	O
the	O
coeﬃcients	O
in	O
(	O
7.1	O
)	O
can	O
be	O
easily	O
estimated	O
using	O
least	B
squares	I
linear	O
regression	B
because	O
this	O
is	O
just	O
a	O
standard	O
linear	O
model	B
with	O
predictors	O
xi	O
,	O
x2	B
i	O
.	O
generally	O
speaking	O
,	O
it	O
is	O
unusual	O
to	O
use	O
d	O
greater	O
than	O
3	O
or	O
4	O
because	O
for	O
large	O
values	O
of	O
d	O
,	O
the	O
polynomial	B
curve	O
can	O
become	O
overly	O
ﬂexible	B
and	O
can	O
take	O
on	O
some	O
very	O
strange	O
shapes	O
.	O
this	O
is	O
especially	O
true	O
near	O
the	O
boundary	O
of	O
the	O
x	O
variable	B
.	O
i	O
,	O
.	O
.	O
.	O
,	O
xd	O
i	O
,	O
x3	O
polynomial	B
regression	O
7.1	O
polynomial	B
regression	O
267	O
degree−4	O
polynomial	B
)	O
e	O
g	O
a	O
|	O
0	O
5	O
2	O
>	O
e	O
g	O
a	O
w	O
(	O
r	O
p	O
0	O
2	O
.	O
0	O
5	O
1	O
0	O
.	O
0	O
1	O
.	O
0	O
5	O
0	O
.	O
0	O
0	O
0	O
.	O
0	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
0	O
0	O
3	O
0	O
5	O
2	O
0	O
0	O
2	O
0	O
5	O
1	O
0	O
0	O
1	O
0	O
5	O
e	O
g	O
a	O
w	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
age	O
age	O
figure	O
7.1.	O
the	O
wage	O
data	B
.	O
left	O
:	O
the	O
solid	O
blue	O
curve	O
is	O
a	O
degree-4	O
polynomial	B
of	O
wage	O
(	O
in	O
thousands	O
of	O
dollars	O
)	O
as	O
a	O
function	B
of	O
age	O
,	O
ﬁt	B
by	O
least	B
squares	I
.	O
the	O
dotted	O
curves	O
indicate	O
an	O
estimated	O
95	O
%	O
conﬁdence	B
interval	I
.	O
right	O
:	O
we	O
model	B
the	O
binary	B
event	O
wage	O
>	O
250	O
using	O
logistic	B
regression	I
,	O
again	O
with	O
a	O
degree-4	O
polynomial	B
.	O
the	O
ﬁtted	O
posterior	O
probability	B
of	O
wage	O
exceeding	O
$	O
250,000	O
is	O
shown	O
in	O
blue	O
,	O
along	O
with	O
an	O
estimated	O
95	O
%	O
conﬁdence	B
interval	I
.	O
the	O
left-hand	O
panel	O
in	O
figure	O
7.1	O
is	O
a	O
plot	B
of	O
wage	O
against	O
age	O
for	O
the	O
wage	O
data	B
set	O
,	O
which	O
contains	O
income	O
and	O
demographic	O
information	O
for	O
males	O
who	O
reside	O
in	O
the	O
central	O
atlantic	O
region	O
of	O
the	O
united	O
states	O
.	O
we	O
see	O
the	O
results	O
of	O
ﬁtting	O
a	O
degree-4	O
polynomial	B
using	O
least	B
squares	I
(	O
solid	O
blue	O
curve	O
)	O
.	O
even	O
though	O
this	O
is	O
a	O
linear	B
regression	I
model	O
like	O
any	O
other	O
,	O
the	O
individual	O
coeﬃcients	O
are	O
not	O
of	O
particular	O
interest	O
.	O
instead	O
,	O
we	O
look	O
at	O
the	O
entire	O
ﬁtted	O
function	O
across	O
a	O
grid	O
of	O
62	O
values	O
for	O
age	O
from	O
18	O
to	O
80	O
in	O
order	O
to	O
understand	O
the	O
relationship	O
between	O
age	O
and	O
wage	O
.	O
in	O
figure	O
7.1	O
,	O
a	O
pair	O
of	O
dotted	O
curves	O
accompanies	O
the	O
ﬁt	B
;	O
these	O
are	O
(	O
2×	O
)	O
standard	B
error	I
curves	O
.	O
let	O
’	O
s	O
see	O
how	O
these	O
arise	O
.	O
suppose	O
we	O
have	O
computed	O
the	O
ﬁt	B
at	O
a	O
particular	O
value	O
of	O
age	O
,	O
x0	O
:	O
ˆf	O
(	O
x0	O
)	O
=	O
ˆβ0	O
+	O
ˆβ1x0	O
+	O
ˆβ2x2	O
0	O
+	O
ˆβ3x3	O
0	O
+	O
ˆβ4x4	O
0	O
.	O
(	O
7.2	O
)	O
what	O
is	O
the	O
variance	B
of	O
the	O
ﬁt	B
,	O
i.e	O
.	O
var	O
ˆf	O
(	O
x0	O
)	O
?	O
least	B
squares	I
returns	O
variance	B
estimates	O
for	O
each	O
of	O
the	O
ﬁtted	O
coeﬃcients	O
ˆβj	O
,	O
as	O
well	O
as	O
the	O
covariances	O
between	O
pairs	O
of	O
coeﬃcient	B
estimates	O
.	O
we	O
can	O
use	O
these	O
to	O
compute	O
the	O
estimated	O
variance	B
of	O
ˆf	O
(	O
x0	O
)	O
.1	O
the	O
estimated	O
pointwise	O
standard	B
error	I
of	O
ˆf	O
(	O
x0	O
)	O
is	O
the	O
square-root	O
of	O
this	O
variance	B
.	O
this	O
computation	O
is	O
repeated	O
1if	O
ˆc	O
is	O
the	O
5	O
×	O
5	O
covariance	O
matrix	O
of	O
the	O
ˆβj	O
,	O
and	O
if	O
(	O
cid:5	O
)	O
t	O
0	O
)	O
,	O
then	O
var	O
[	O
ˆf	O
(	O
x0	O
)	O
]	O
=	O
(	O
cid:5	O
)	O
t	O
0	O
ˆc	O
(	O
cid:5	O
)	O
0	O
.	O
0	O
=	O
(	O
1	O
,	O
x0	O
,	O
x2	B
0	O
,	O
x3	O
0	O
,	O
x4	O
268	O
7.	O
moving	O
beyond	O
linearity	O
at	O
each	O
reference	O
point	O
x0	O
,	O
and	O
we	O
plot	B
the	O
ﬁtted	O
curve	O
,	O
as	O
well	O
as	O
twice	O
the	O
standard	B
error	I
on	O
either	O
side	O
of	O
the	O
ﬁtted	O
curve	O
.	O
we	O
plot	B
twice	O
the	O
standard	B
error	I
because	O
,	O
for	O
normally	O
distributed	O
error	B
terms	O
,	O
this	O
quantity	O
corresponds	O
to	O
an	O
approximate	O
95	O
%	O
conﬁdence	B
interval	I
.	O
it	O
seems	O
like	O
the	O
wages	O
in	O
figure	O
7.1	O
are	O
from	O
two	O
distinct	O
populations	O
:	O
there	O
appears	O
to	O
be	O
a	O
high	O
earners	O
group	O
earning	O
more	O
than	O
$	O
250,000	O
per	O
annum	O
,	O
as	O
well	O
as	O
a	O
low	O
earners	O
group	O
.	O
we	O
can	O
treat	O
wage	O
as	O
a	O
binary	B
variable	O
by	O
splitting	O
it	O
into	O
these	O
two	O
groups	O
.	O
logistic	B
regression	I
can	O
then	O
be	O
used	O
to	O
predict	O
this	O
binary	B
response	O
,	O
using	O
polynomial	B
functions	O
of	O
age	O
as	O
predictors	O
.	O
in	O
other	O
words	O
,	O
we	O
ﬁt	B
the	O
model	B
pr	O
(	O
yi	O
>	O
250|xi	O
)	O
=	O
exp	O
(	O
β0	O
+	O
β1xi	O
+	O
β2x2	O
1	O
+	O
exp	O
(	O
β0	O
+	O
β1xi	O
+	O
β2x2	O
i	O
+	O
.	O
.	O
.	O
+	O
βdxd	O
i	O
)	O
i	O
+	O
.	O
.	O
.	O
+	O
βdxd	O
i	O
)	O
.	O
(	O
7.3	O
)	O
the	O
result	O
is	O
shown	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
7.1.	O
the	O
gray	O
marks	O
on	O
the	O
top	O
and	O
bottom	O
of	O
the	O
panel	O
indicate	O
the	O
ages	O
of	O
the	O
high	O
earners	O
and	O
the	O
low	O
earners	O
.	O
the	O
solid	O
blue	O
curve	O
indicates	O
the	O
ﬁtted	O
probabilities	O
of	O
being	O
a	O
high	O
earner	O
,	O
as	O
a	O
function	B
of	O
age	O
.	O
the	O
estimated	O
95	O
%	O
conﬁdence	B
interval	I
is	O
shown	O
as	O
well	O
.	O
we	O
see	O
that	O
here	O
the	O
conﬁdence	O
intervals	O
are	O
fairly	O
wide	O
,	O
especially	O
on	O
the	O
right-hand	O
side	O
.	O
although	O
the	O
sample	O
size	O
for	O
this	O
data	B
set	O
is	O
substantial	O
(	O
n	O
=	O
3,000	O
)	O
,	O
there	O
are	O
only	O
79	O
high	O
earners	O
,	O
which	O
results	O
in	O
a	O
high	O
variance	B
in	O
the	O
estimated	O
coeﬃcients	O
and	O
consequently	O
wide	O
conﬁdence	O
intervals	O
.	O
7.2	O
step	O
functions	O
using	O
polynomial	B
functions	O
of	O
the	O
features	O
as	O
predictors	O
in	O
a	O
linear	B
model	I
imposes	O
a	O
global	O
structure	O
on	O
the	O
non-linear	B
function	O
of	O
x.	O
we	O
can	O
instead	O
use	O
step	O
functions	O
in	O
order	O
to	O
avoid	O
imposing	O
such	O
a	O
global	O
structure	O
.	O
here	O
we	O
break	O
the	O
range	O
of	O
x	O
into	O
bins	O
,	O
and	O
ﬁt	B
a	O
diﬀerent	O
constant	O
in	O
each	O
bin	O
.	O
this	O
amounts	O
to	O
converting	O
a	O
continuous	B
variable	O
into	O
an	O
ordered	B
categorical	I
variable	I
.	O
in	O
greater	O
detail	O
,	O
we	O
create	O
cutpoints	O
c1	O
,	O
c2	O
,	O
.	O
.	O
.	O
,	O
ck	O
in	O
the	O
range	O
of	O
x	O
,	O
and	O
then	O
construct	O
k	O
+	O
1	O
new	O
variables	O
step	B
function	I
ordered	O
categorical	B
variable	O
c0	O
(	O
x	O
)	O
c1	O
(	O
x	O
)	O
c2	O
(	O
x	O
)	O
=	O
i	O
(	O
x	O
<	O
c1	O
)	O
,	O
=	O
i	O
(	O
c1	O
≤	O
x	O
<	O
c2	O
)	O
,	O
=	O
i	O
(	O
c2	O
≤	O
x	O
<	O
c3	O
)	O
,	O
...	O
=	O
i	O
(	O
ck	O
≤	O
x	O
)	O
,	O
ck−1	O
(	O
x	O
)	O
=	O
i	O
(	O
ck−1	O
≤	O
x	O
<	O
ck	O
)	O
,	O
ck	O
(	O
x	O
)	O
(	O
7.4	O
)	O
where	O
i	O
(	O
·	O
)	O
is	O
an	O
indicator	B
function	I
that	O
returns	O
a	O
1	O
if	O
the	O
condition	O
is	O
true	O
,	O
and	O
returns	O
a	O
0	O
otherwise	O
.	O
for	O
example	O
,	O
i	O
(	O
ck	O
≤	O
x	O
)	O
equals	O
1	O
if	O
ck	O
≤	O
x	O
,	O
and	O
indicator	B
function	I
0	O
0	O
3	O
0	O
5	O
2	O
0	O
0	O
2	O
0	O
5	O
1	O
0	O
0	O
1	O
0	O
5	O
e	O
g	O
a	O
w	O
7.2	O
step	O
functions	O
269	O
piecewise	O
constant	O
)	O
e	O
g	O
a	O
|	O
0	O
5	O
2	O
>	O
e	O
g	O
a	O
w	O
(	O
r	O
p	O
0	O
2	O
.	O
0	O
5	O
1	O
.	O
0	O
0	O
1	O
.	O
0	O
5	O
0	O
.	O
0	O
0	O
0	O
.	O
0	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
age	O
age	O
figure	O
7.2.	O
the	O
wage	O
data	B
.	O
left	O
:	O
the	O
solid	O
curve	O
displays	O
the	O
ﬁtted	B
value	I
from	O
a	O
least	B
squares	I
regression	O
of	O
wage	O
(	O
in	O
thousands	O
of	O
dollars	O
)	O
using	O
step	O
functions	O
of	O
age	O
.	O
the	O
dotted	O
curves	O
indicate	O
an	O
estimated	O
95	O
%	O
conﬁdence	B
interval	I
.	O
right	O
:	O
we	O
model	B
the	O
binary	B
event	O
wage	O
>	O
250	O
using	O
logistic	B
regression	I
,	O
again	O
using	O
step	O
functions	O
of	O
age	O
.	O
the	O
ﬁtted	O
posterior	O
probability	B
of	O
wage	O
exceeding	O
$	O
250,000	O
is	O
shown	O
,	O
along	O
with	O
an	O
estimated	O
95	O
%	O
conﬁdence	B
interval	I
.	O
equals	O
0	O
otherwise	O
.	O
these	O
are	O
sometimes	O
called	O
dummy	B
variables	O
.	O
notice	O
that	O
for	O
any	O
value	O
of	O
x	O
,	O
c0	O
(	O
x	O
)	O
+	O
c1	O
(	O
x	O
)	O
+	O
.	O
.	O
.	O
+	O
ck	O
(	O
x	O
)	O
=	O
1	O
,	O
since	O
x	O
must	O
be	O
in	O
exactly	O
one	O
of	O
the	O
k	O
+	O
1	O
intervals	O
.	O
we	O
then	O
use	O
least	B
squares	I
to	O
ﬁt	B
a	O
linear	B
model	I
using	O
c1	O
(	O
x	O
)	O
,	O
c2	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
ck	O
(	O
x	O
)	O
as	O
predictors2	O
:	O
yi	O
=	O
β0	O
+	O
β1c1	O
(	O
xi	O
)	O
+	O
β2c2	O
(	O
xi	O
)	O
+	O
.	O
.	O
.	O
+	O
βkck	O
(	O
xi	O
)	O
+	O
i	O
.	O
(	O
7.5	O
)	O
for	O
a	O
given	O
value	O
of	O
x	O
,	O
at	O
most	O
one	O
of	O
c1	O
,	O
c2	O
,	O
.	O
.	O
.	O
,	O
ck	O
can	O
be	O
non-zero	O
.	O
note	O
that	O
when	O
x	O
<	O
c1	O
,	O
all	O
of	O
the	O
predictors	O
in	O
(	O
7.5	O
)	O
are	O
zero	O
,	O
so	O
β0	O
can	O
be	O
interpreted	O
as	O
the	O
mean	O
value	O
of	O
y	O
for	O
x	O
<	O
c1	O
.	O
by	O
comparison	O
,	O
(	O
7.5	O
)	O
predicts	O
a	O
response	B
of	O
β0+βj	O
for	O
cj	O
≤	O
x	O
<	O
cj+1	O
,	O
so	O
βj	O
represents	O
the	O
average	B
increase	O
in	O
the	O
response	B
for	O
x	O
in	O
cj	O
≤	O
x	O
<	O
cj+1	O
relative	O
to	O
x	O
<	O
c1	O
.	O
an	O
example	O
of	O
ﬁtting	O
step	O
functions	O
to	O
the	O
wage	O
data	B
from	O
figure	O
7.1	O
is	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
7.2.	O
we	O
also	O
ﬁt	B
the	O
logistic	B
regression	I
model	O
2we	O
exclude	O
c0	O
(	O
x	O
)	O
as	O
a	O
predictor	B
in	O
(	O
7.5	O
)	O
because	O
it	O
is	O
redundant	O
with	O
the	O
intercept	B
.	O
this	O
is	O
similar	O
to	O
the	O
fact	O
that	O
we	O
need	O
only	O
two	O
dummy	B
variables	O
to	O
code	O
a	O
qualitative	B
variable	O
with	O
three	O
levels	O
,	O
provided	O
that	O
the	O
model	B
will	O
contain	O
an	O
intercept	B
.	O
the	O
decision	O
to	O
exclude	O
c0	O
(	O
x	O
)	O
instead	O
of	O
some	O
other	O
ck	O
(	O
x	O
)	O
in	O
(	O
7.5	O
)	O
is	O
arbitrary	O
.	O
alternatively	O
,	O
we	O
could	O
include	O
c0	O
(	O
x	O
)	O
,	O
c1	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
ck	O
(	O
x	O
)	O
,	O
and	O
exclude	O
the	O
intercept	B
.	O
270	O
7.	O
moving	O
beyond	O
linearity	O
pr	O
(	O
yi	O
>	O
250|xi	O
)	O
=	O
exp	O
(	O
β0	O
+	O
β1c1	O
(	O
xi	O
)	O
+	O
.	O
.	O
.	O
+	O
βkck	O
(	O
xi	O
)	O
)	O
1	O
+	O
exp	O
(	O
β0	O
+	O
β1c1	O
(	O
xi	O
)	O
+	O
.	O
.	O
.	O
+	O
βkck	O
(	O
xi	O
)	O
)	O
(	O
7.6	O
)	O
in	O
order	O
to	O
predict	O
the	O
probability	B
that	O
an	O
individual	O
is	O
a	O
high	O
earner	O
on	O
the	O
basis	B
of	O
age	O
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
7.2	O
displays	O
the	O
ﬁtted	O
posterior	O
probabilities	O
obtained	O
using	O
this	O
approach	B
.	O
unfortunately	O
,	O
unless	O
there	O
are	O
natural	B
breakpoints	O
in	O
the	O
predictors	O
,	O
piecewise-constant	O
functions	O
can	O
miss	O
the	O
action	O
.	O
for	O
example	O
,	O
in	O
the	O
left-	O
hand	O
panel	O
of	O
figure	O
7.2	O
,	O
the	O
ﬁrst	O
bin	O
clearly	O
misses	O
the	O
increasing	O
trend	O
of	O
wage	O
with	O
age	O
.	O
nevertheless	O
,	O
step	B
function	I
approaches	O
are	O
very	O
popular	O
in	O
biostatistics	O
and	O
epidemiology	O
,	O
among	O
other	O
disciplines	O
.	O
for	O
example	O
,	O
5-year	O
age	O
groups	O
are	O
often	O
used	O
to	O
deﬁne	O
the	O
bins	O
.	O
7.3	O
basis	B
functions	O
polynomial	B
and	O
piecewise-constant	O
regression	B
models	O
are	O
in	O
fact	O
special	O
cases	O
of	O
a	O
basis	B
function	I
approach	O
.	O
the	O
idea	O
is	O
to	O
have	O
at	O
hand	O
a	O
fam-	O
ily	O
of	O
functions	O
or	O
transformations	O
that	O
can	O
be	O
applied	O
to	O
a	O
variable	B
x	O
:	O
b1	O
(	O
x	O
)	O
,	O
b2	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
bk	O
(	O
x	O
)	O
.	O
instead	O
of	O
ﬁtting	O
a	O
linear	B
model	I
in	O
x	O
,	O
we	O
ﬁt	B
the	O
model	B
basis	O
function	B
yi	O
=	O
β0	O
+	O
β1b1	O
(	O
xi	O
)	O
+	O
β2b2	O
(	O
xi	O
)	O
+	O
β3b3	O
(	O
xi	O
)	O
+	O
.	O
.	O
.	O
+	O
βkbk	O
(	O
xi	O
)	O
+	O
i	O
.	O
(	O
7.7	O
)	O
note	O
that	O
the	O
basis	B
functions	O
b1	O
(	O
·	O
)	O
,	O
b2	O
(	O
·	O
)	O
,	O
.	O
.	O
.	O
,	O
bk	O
(	O
·	O
)	O
are	O
ﬁxed	O
and	O
known	O
.	O
(	O
in	O
other	O
words	O
,	O
we	O
choose	O
the	O
functions	O
ahead	O
of	O
time	O
.	O
)	O
for	O
polynomial	O
regression	B
,	O
the	O
basis	B
functions	O
are	O
bj	O
(	O
xi	O
)	O
=	O
xj	O
i	O
,	O
and	O
for	O
piecewise	O
constant	O
functions	O
they	O
are	O
bj	O
(	O
xi	O
)	O
=	O
i	O
(	O
cj	O
≤	O
xi	O
<	O
cj+1	O
)	O
.	O
we	O
can	O
think	O
of	O
(	O
7.7	O
)	O
as	O
a	O
standard	O
linear	O
model	B
with	O
predictors	O
b1	O
(	O
xi	O
)	O
,	O
b2	O
(	O
xi	O
)	O
,	O
.	O
.	O
.	O
,	O
bk	O
(	O
xi	O
)	O
.	O
hence	O
,	O
we	O
can	O
use	O
least	B
squares	I
to	O
estimate	O
the	O
unknown	O
regression	B
coeﬃcients	O
in	O
(	O
7.7	O
)	O
.	O
importantly	O
,	O
this	O
means	O
that	O
all	O
of	O
the	O
inference	B
tools	O
for	O
linear	O
models	O
that	O
are	O
discussed	O
in	O
chapter	O
3	O
,	O
such	O
as	O
standard	O
errors	O
for	O
the	O
coeﬃcient	B
estimates	O
and	O
f-statistics	O
for	O
the	O
model	B
’	O
s	O
overall	O
signiﬁcance	O
,	O
are	O
available	O
in	O
this	O
setting	O
.	O
thus	O
far	O
we	O
have	O
considered	O
the	O
use	O
of	O
polynomial	B
functions	O
and	O
piece-	O
wise	O
constant	O
functions	O
for	O
our	O
basis	B
functions	O
;	O
however	O
,	O
many	O
alternatives	O
are	O
possible	O
.	O
for	O
instance	O
,	O
we	O
can	O
use	O
wavelets	O
or	O
fourier	O
series	O
to	O
construct	O
basis	B
functions	O
.	O
in	O
the	O
next	O
section	O
,	O
we	O
investigate	O
a	O
very	O
common	O
choice	O
for	O
a	O
basis	B
function	I
:	O
regression	B
splines	O
.	O
regression	B
spline	O
7.4	O
regression	B
splines	O
271	O
7.4	O
regression	B
splines	O
now	O
we	O
discuss	O
a	O
ﬂexible	B
class	O
of	O
basis	B
functions	O
that	O
extends	O
upon	O
the	O
polynomial	B
regression	O
and	O
piecewise	O
constant	O
regression	B
approaches	O
that	O
we	O
have	O
just	O
seen	O
.	O
7.4.1	O
piecewise	O
polynomials	O
instead	O
of	O
ﬁtting	O
a	O
high-degree	O
polynomial	B
over	O
the	O
entire	O
range	O
of	O
x	O
,	O
piece-	O
wise	O
polynomial	B
regression	O
involves	O
ﬁtting	O
separate	O
low-degree	O
polynomials	O
over	O
diﬀerent	O
regions	O
of	O
x.	O
for	O
example	O
,	O
a	O
piecewise	O
cubic	O
polynomial	B
works	O
by	O
ﬁtting	O
a	O
cubic	B
regression	O
model	B
of	O
the	O
form	O
piecewise	B
polynomial	I
regression	O
yi	O
=	O
β0	O
+	O
β1xi	O
+	O
β2x2	O
i	O
+	O
β3x3	O
i	O
+	O
i	O
,	O
(	O
7.8	O
)	O
where	O
the	O
coeﬃcients	O
β0	O
,	O
β1	O
,	O
β2	O
,	O
and	O
β3	O
diﬀer	O
in	O
diﬀerent	O
parts	O
of	O
the	O
range	O
of	O
x.	O
the	O
points	O
where	O
the	O
coeﬃcients	O
change	O
are	O
called	O
knots	O
.	O
for	O
example	O
,	O
a	O
piecewise	O
cubic	O
with	O
no	O
knots	O
is	O
just	O
a	O
standard	O
cubic	O
polynomial	B
,	O
as	O
in	O
(	O
7.1	O
)	O
with	O
d	O
=	O
3.	O
a	O
piecewise	O
cubic	O
polynomial	B
with	O
a	O
single	B
knot	O
at	O
a	O
point	O
c	O
takes	O
the	O
form	O
(	O
cid:30	O
)	O
knot	B
yi	O
=	O
β01	O
+	O
β11xi	O
+	O
β21x2	O
β02	O
+	O
β12xi	O
+	O
β22x2	O
i	O
+	O
β31x3	O
i	O
+	O
β32x3	O
i	O
+	O
i	O
i	O
+	O
i	O
if	O
xi	O
<	O
c	O
;	O
if	O
xi	O
≥	O
c.	O
in	O
other	O
words	O
,	O
we	O
ﬁt	B
two	O
diﬀerent	O
polynomial	B
functions	O
to	O
the	O
data	B
,	O
one	O
on	O
the	O
subset	O
of	O
the	O
observations	B
with	O
xi	O
<	O
c	O
,	O
and	O
one	O
on	O
the	O
subset	O
of	O
the	O
observations	B
with	O
xi	O
≥	O
c.	O
the	O
ﬁrst	O
polynomial	B
function	O
has	O
coeﬃcients	O
β01	O
,	O
β11	O
,	O
β21	O
,	O
β31	O
,	O
and	O
the	O
second	O
has	O
coeﬃcients	O
β02	O
,	O
β12	O
,	O
β22	O
,	O
β32	O
.	O
each	O
of	O
these	O
polynomial	B
functions	O
can	O
be	O
ﬁt	B
using	O
least	B
squares	I
applied	O
to	O
simple	B
functions	O
of	O
the	O
original	O
predictor	B
.	O
using	O
more	O
knots	O
leads	O
to	O
a	O
more	O
ﬂexible	B
piecewise	O
polynomial	B
.	O
in	O
gen-	O
eral	O
,	O
if	O
we	O
place	O
k	O
diﬀerent	O
knots	O
throughout	O
the	O
range	O
of	O
x	O
,	O
then	O
we	O
will	O
end	O
up	O
ﬁtting	O
k	O
+	O
1	O
diﬀerent	O
cubic	B
polynomials	O
.	O
note	O
that	O
we	O
do	O
not	O
need	O
to	O
use	O
a	O
cubic	B
polynomial	O
.	O
for	O
example	O
,	O
we	O
can	O
instead	O
ﬁt	B
piecewise	O
linear	B
functions	O
.	O
in	O
fact	O
,	O
our	O
piecewise	O
constant	O
functions	O
of	O
section	O
7.2	O
are	O
piecewise	O
polynomials	O
of	O
degree	O
0	O
!	O
the	O
top	O
left	O
panel	O
of	O
figure	O
7.3	O
shows	O
a	O
piecewise	O
cubic	O
polynomial	B
ﬁt	O
to	O
a	O
subset	O
of	O
the	O
wage	O
data	B
,	O
with	O
a	O
single	B
knot	O
at	O
age=50	O
.	O
we	O
immediately	O
see	O
a	O
problem	O
:	O
the	O
function	B
is	O
discontinuous	O
and	O
looks	O
ridiculous	O
!	O
since	O
each	O
polynomial	B
has	O
four	O
parameters	O
,	O
we	O
are	O
using	O
a	O
total	O
of	O
eight	O
degrees	B
of	I
freedom	I
in	O
ﬁtting	O
this	O
piecewise	B
polynomial	I
model	O
.	O
7.4.2	O
constraints	O
and	O
splines	O
the	O
top	O
left	O
panel	O
of	O
figure	O
7.3	O
looks	O
wrong	O
because	O
the	O
ﬁtted	O
curve	O
is	O
just	O
too	O
ﬂexible	B
.	O
to	O
remedy	O
this	O
problem	O
,	O
we	O
can	O
ﬁt	B
a	O
piecewise	B
polynomial	I
degrees	O
of	O
freedom	O
272	O
7.	O
moving	O
beyond	O
linearity	O
piecewise	O
cubic	O
continuous	B
piecewise	O
cubic	B
e	O
g	O
a	O
w	O
e	O
g	O
a	O
w	O
0	O
5	O
2	O
0	O
0	O
2	O
0	O
5	O
1	O
0	O
0	O
1	O
0	O
5	O
0	O
5	O
2	O
0	O
0	O
2	O
0	O
5	O
1	O
0	O
0	O
1	O
0	O
5	O
e	O
g	O
a	O
w	O
0	O
5	O
2	O
0	O
0	O
2	O
0	O
5	O
1	O
0	O
0	O
1	O
0	O
5	O
20	O
30	O
40	O
50	O
60	O
70	O
20	O
30	O
40	O
50	O
60	O
70	O
age	O
cubic	B
spline	O
age	O
linear	B
spline	O
e	O
g	O
a	O
w	O
0	O
5	O
2	O
0	O
0	O
2	O
0	O
5	O
1	O
0	O
0	O
1	O
0	O
5	O
20	O
30	O
40	O
50	O
60	O
70	O
20	O
30	O
40	O
50	O
60	O
70	O
age	O
age	O
figure	O
7.3.	O
various	O
piecewise	O
polynomials	O
are	O
ﬁt	B
to	O
a	O
subset	O
of	O
the	O
wage	O
data	B
,	O
with	O
a	O
knot	B
at	O
age=50	O
.	O
top	O
left	O
:	O
the	O
cubic	B
polynomials	O
are	O
unconstrained	O
.	O
top	O
right	O
:	O
the	O
cubic	B
polynomials	O
are	O
constrained	O
to	O
be	O
continuous	B
at	O
age=50	O
.	O
bottom	O
left	O
:	O
the	O
cubic	B
polynomials	O
are	O
constrained	O
to	O
be	O
continuous	B
,	O
and	O
to	O
have	O
continuous	B
ﬁrst	O
and	O
second	O
derivatives	O
.	O
bottom	O
right	O
:	O
a	O
linear	B
spline	O
is	O
shown	O
,	O
which	O
is	O
constrained	O
to	O
be	O
continuous	B
.	O
under	O
the	O
constraint	O
that	O
the	O
ﬁtted	O
curve	O
must	O
be	O
continuous	B
.	O
in	O
other	O
words	O
,	O
there	O
can	O
not	O
be	O
a	O
jump	O
when	O
age=50	O
.	O
the	O
top	O
right	O
plot	B
in	O
figure	O
7.3	O
shows	O
the	O
resulting	O
ﬁt	B
.	O
this	O
looks	O
better	O
than	O
the	O
top	O
left	O
plot	B
,	O
but	O
the	O
v-	O
shaped	O
join	O
looks	O
unnatural	O
.	O
in	O
the	O
lower	O
left	O
plot	B
,	O
we	O
have	O
added	O
two	O
additional	O
constraints	O
:	O
now	O
both	O
the	O
ﬁrst	O
and	O
second	O
derivatives	O
of	O
the	O
piecewise	O
polynomials	O
are	O
continuous	B
at	O
age=50	O
.	O
in	O
other	O
words	O
,	O
we	O
are	O
requiring	O
that	O
the	O
piecewise	B
polynomial	I
be	O
not	O
only	O
continuous	B
when	O
age=50	O
,	O
but	O
also	O
very	O
smooth	O
.	O
each	O
constraint	O
that	O
we	O
impose	O
on	O
the	O
piecewise	O
cubic	O
polynomials	O
eﬀectively	O
frees	O
up	O
one	O
degree	O
of	O
freedom	O
,	O
by	O
reducing	O
the	O
complexity	O
of	O
the	O
resulting	O
piecewise	B
polynomial	I
ﬁt	O
.	O
so	O
in	O
the	O
top	O
left	O
plot	B
,	O
we	O
are	O
using	O
eight	O
degrees	O
of	O
free-	O
dom	O
,	O
but	O
in	O
the	O
bottom	O
left	O
plot	B
we	O
imposed	O
three	O
constraints	O
(	O
continuity	O
,	O
continuity	O
of	O
the	O
ﬁrst	O
derivative	B
,	O
and	O
continuity	O
of	O
the	O
second	O
derivative	B
)	O
and	O
so	O
are	O
left	O
with	O
ﬁve	O
degrees	B
of	I
freedom	I
.	O
the	O
curve	O
in	O
the	O
bottom	O
left	O
derivative	B
7.4	O
regression	B
splines	O
273	O
plot	B
is	O
called	O
a	O
cubic	B
spline.3	O
in	O
general	O
,	O
a	O
cubic	B
spline	O
with	O
k	O
knots	O
uses	O
a	O
total	O
of	O
4	O
+	O
k	O
degrees	B
of	I
freedom	I
.	O
in	O
figure	O
7.3	O
,	O
the	O
lower	O
right	O
plot	B
is	O
a	O
linear	B
spline	O
,	O
which	O
is	O
continuous	B
at	O
age=50	O
.	O
the	O
general	O
deﬁnition	O
of	O
a	O
degree-d	O
spline	B
is	O
that	O
it	O
is	O
a	O
piecewise	O
degree-d	O
polynomial	B
,	O
with	O
continuity	O
in	O
derivatives	O
up	O
to	O
degree	O
d	O
−	O
1	O
at	O
each	O
knot	B
.	O
therefore	O
,	O
a	O
linear	B
spline	O
is	O
obtained	O
by	O
ﬁtting	O
a	O
line	B
in	O
each	O
region	O
of	O
the	O
predictor	B
space	O
deﬁned	O
by	O
the	O
knots	O
,	O
requiring	O
continuity	O
at	O
each	O
knot	B
.	O
in	O
figure	O
7.3	O
,	O
there	O
is	O
a	O
single	B
knot	O
at	O
age=50	O
.	O
of	O
course	O
,	O
we	O
could	O
add	O
more	O
knots	O
,	O
and	O
impose	O
continuity	O
at	O
each	O
.	O
cubic	B
spline	O
linear	B
spline	O
7.4.3	O
the	O
spline	B
basis	O
representation	O
the	O
regression	B
splines	O
that	O
we	O
just	O
saw	O
in	O
the	O
previous	O
section	O
may	O
have	O
seemed	O
somewhat	O
complex	O
:	O
how	O
can	O
we	O
ﬁt	B
a	O
piecewise	O
degree-d	O
polynomial	B
under	O
the	O
constraint	O
that	O
it	O
(	O
and	O
possibly	O
its	O
ﬁrst	O
d	O
−	O
1	O
derivatives	O
)	O
be	O
continuous	B
?	O
it	O
turns	O
out	O
that	O
we	O
can	O
use	O
the	O
basis	B
model	O
(	O
7.7	O
)	O
to	O
represent	O
a	O
regression	B
spline	O
.	O
a	O
cubic	B
spline	O
with	O
k	O
knots	O
can	O
be	O
modeled	O
as	O
yi	O
=	O
β0	O
+	O
β1b1	O
(	O
xi	O
)	O
+	O
β2b2	O
(	O
xi	O
)	O
+	O
···	O
+	O
βk+3bk+3	O
(	O
xi	O
)	O
+	O
i	O
,	O
(	O
7.9	O
)	O
for	O
an	O
appropriate	O
choice	O
of	O
basis	B
functions	O
b1	O
,	O
b2	O
,	O
.	O
.	O
.	O
,	O
bk+3	O
.	O
the	O
model	B
(	O
7.9	O
)	O
can	O
then	O
be	O
ﬁt	B
using	O
least	B
squares	I
.	O
truncated	B
power	I
basis	I
just	O
as	O
there	O
were	O
several	O
ways	O
to	O
represent	O
polynomials	O
,	O
there	O
are	O
also	O
many	O
equivalent	O
ways	O
to	O
represent	O
cubic	B
splines	O
using	O
diﬀerent	O
choices	O
of	O
basis	B
functions	O
in	O
(	O
7.9	O
)	O
.	O
the	O
most	O
direct	O
way	O
to	O
represent	O
a	O
cubic	B
spline	O
using	O
(	O
7.9	O
)	O
is	O
to	O
start	O
oﬀ	O
with	O
a	O
basis	B
for	O
a	O
cubic	B
polynomial—namely	O
,	O
x	O
,	O
x2	B
,	O
x3—and	O
then	O
add	O
one	O
truncated	B
power	I
basis	I
function	O
per	O
knot	B
.	O
a	O
truncated	B
power	I
basis	I
function	O
is	O
deﬁned	O
as	O
(	O
x	O
−	O
ξ	O
)	O
3	O
,	O
h	O
(	O
x	O
,	O
ξ	O
)	O
=	O
(	O
x	O
−	O
ξ	O
)	O
3	O
+	O
=	O
if	O
x	O
>	O
ξ	O
otherwise	O
,	O
0	O
(	O
7.10	O
)	O
where	O
ξ	O
is	O
the	O
knot	B
.	O
one	O
can	O
show	O
that	O
adding	O
a	O
term	B
of	O
the	O
form	O
β4h	O
(	O
x	O
,	O
ξ	O
)	O
to	O
the	O
model	B
(	O
7.8	O
)	O
for	O
a	O
cubic	B
polynomial	O
will	O
lead	O
to	O
a	O
discontinuity	O
in	O
only	O
the	O
third	O
derivative	B
at	O
ξ	O
;	O
the	O
function	B
will	O
remain	O
continuous	B
,	O
with	O
continuous	B
ﬁrst	O
and	O
second	O
derivatives	O
,	O
at	O
each	O
of	O
the	O
knots	O
.	O
in	O
other	O
words	O
,	O
in	O
order	O
to	O
ﬁt	B
a	O
cubic	B
spline	O
to	O
a	O
data	B
set	O
with	O
k	O
knots	O
,	O
we	O
perform	O
least	B
squares	I
regression	O
with	O
an	O
intercept	B
and	O
3	O
+	O
k	O
predictors	O
,	O
of	O
the	O
form	O
x	O
,	O
x	O
2	O
,	O
x	O
3	O
,	O
h	O
(	O
x	O
,	O
ξ1	O
)	O
,	O
h	O
(	O
x	O
,	O
ξ2	O
)	O
,	O
.	O
.	O
.	O
,	O
h	O
(	O
x	O
,	O
ξk	O
)	O
,	O
where	O
ξ1	O
,	O
.	O
.	O
.	O
,	O
ξk	O
are	O
the	O
knots	O
.	O
this	O
amounts	O
to	O
estimating	O
a	O
total	O
of	O
k	O
+	O
4	O
regression	B
coeﬃ-	O
cients	O
;	O
for	O
this	O
reason	O
,	O
ﬁtting	O
a	O
cubic	B
spline	O
with	O
k	O
knots	O
uses	O
k	O
+4	O
degrees	B
of	I
freedom	I
.	O
3cubic	O
splines	O
are	O
popular	O
because	O
most	O
human	O
eyes	O
can	O
not	O
detect	O
the	O
discontinuity	O
at	O
the	O
knots	O
.	O
natural	B
cubic	O
spline	B
cubic	O
spline	B
274	O
7.	O
moving	O
beyond	O
linearity	O
e	O
g	O
a	O
w	O
0	O
5	O
2	O
0	O
0	O
2	O
0	O
5	O
1	O
0	O
0	O
1	O
0	O
5	O
20	O
30	O
40	O
50	O
60	O
70	O
age	O
figure	O
7.4.	O
a	O
cubic	B
spline	O
and	O
a	O
natural	B
cubic	O
spline	B
,	O
with	O
three	O
knots	O
,	O
ﬁt	B
to	O
a	O
subset	O
of	O
the	O
wage	O
data	B
.	O
unfortunately	O
,	O
splines	O
can	O
have	O
high	O
variance	B
at	O
the	O
outer	O
range	O
of	O
the	O
predictors—that	O
is	O
,	O
when	O
x	O
takes	O
on	O
either	O
a	O
very	O
small	O
or	O
very	O
large	O
value	O
.	O
figure	O
7.4	O
shows	O
a	O
ﬁt	B
to	O
the	O
wage	O
data	B
with	O
three	O
knots	O
.	O
we	O
see	O
that	O
the	O
conﬁdence	O
bands	O
in	O
the	O
boundary	O
region	O
appear	O
fairly	O
wild	O
.	O
a	O
natu-	O
ral	O
spline	B
is	O
a	O
regression	B
spline	O
with	O
additional	O
boundary	O
constraints	O
:	O
the	O
function	B
is	O
required	O
to	O
be	O
linear	B
at	O
the	O
boundary	O
(	O
in	O
the	O
region	O
where	O
x	O
is	O
smaller	O
than	O
the	O
smallest	O
knot	B
,	O
or	O
larger	O
than	O
the	O
largest	O
knot	B
)	O
.	O
this	O
addi-	O
tional	O
constraint	O
means	O
that	O
natural	B
splines	O
generally	O
produce	O
more	O
stable	O
estimates	O
at	O
the	O
boundaries	O
.	O
in	O
figure	O
7.4	O
,	O
a	O
natural	B
cubic	O
spline	B
is	O
also	O
displayed	O
as	O
a	O
red	O
line	B
.	O
note	O
that	O
the	O
corresponding	O
conﬁdence	O
intervals	O
are	O
narrower	O
.	O
natural	B
spline	I
7.4.4	O
choosing	O
the	O
number	O
and	O
locations	O
of	O
the	O
knots	O
when	O
we	O
ﬁt	B
a	O
spline	B
,	O
where	O
should	O
we	O
place	O
the	O
knots	O
?	O
the	O
regression	B
spline	O
is	O
most	O
ﬂexible	B
in	O
regions	O
that	O
contain	O
a	O
lot	O
of	O
knots	O
,	O
because	O
in	O
those	O
regions	O
the	O
polynomial	B
coeﬃcients	O
can	O
change	O
rapidly	O
.	O
hence	O
,	O
one	O
option	O
is	O
to	O
place	O
more	O
knots	O
in	O
places	O
where	O
we	O
feel	O
the	O
function	B
might	O
vary	O
most	O
rapidly	O
,	O
and	O
to	O
place	O
fewer	O
knots	O
where	O
it	O
seems	O
more	O
stable	O
.	O
while	O
this	O
option	O
can	O
work	O
well	O
,	O
in	O
practice	O
it	O
is	O
common	O
to	O
place	O
knots	O
in	O
a	O
uniform	O
fashion	O
.	O
one	O
way	O
to	O
do	O
this	O
is	O
to	O
specify	O
the	O
desired	O
degrees	B
of	I
freedom	I
,	O
and	O
then	O
have	O
the	O
software	O
automatically	O
place	O
the	O
corresponding	O
number	O
of	O
knots	O
at	O
uniform	O
quantiles	O
of	O
the	O
data	B
.	O
figure	O
7.5	O
shows	O
an	O
example	O
on	O
the	O
wage	O
data	B
.	O
as	O
in	O
figure	O
7.4	O
,	O
we	O
have	O
ﬁt	B
a	O
natural	B
cubic	O
spline	B
with	O
three	O
knots	O
,	O
except	O
this	O
time	O
the	O
knot	B
locations	O
were	O
chosen	O
automatically	O
as	O
the	O
25th	O
,	O
50th	O
,	O
and	O
75th	O
percentiles	O
0	O
0	O
3	O
0	O
5	O
2	O
0	O
0	O
2	O
0	O
5	O
1	O
0	O
0	O
1	O
0	O
5	O
e	O
g	O
a	O
w	O
7.4	O
regression	B
splines	O
275	O
natural	B
cubic	O
spline	B
)	O
e	O
g	O
a	O
|	O
0	O
5	O
2	O
>	O
e	O
g	O
a	O
w	O
(	O
r	O
p	O
0	O
2	O
.	O
0	O
5	O
1	O
.	O
0	O
0	O
1	O
0	O
.	O
5	O
0	O
0	O
.	O
0	O
0	O
.	O
0	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|||	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
||	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
|	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
age	O
age	O
figure	O
7.5.	O
a	O
natural	B
cubic	O
spline	B
function	O
with	O
four	O
degrees	B
of	I
freedom	I
is	O
ﬁt	B
to	O
the	O
wage	O
data	B
.	O
left	O
:	O
a	O
spline	B
is	O
ﬁt	B
to	O
wage	O
(	O
in	O
thousands	O
of	O
dollars	O
)	O
as	O
a	O
function	B
of	O
age	O
.	O
right	O
:	O
logistic	B
regression	I
is	O
used	O
to	O
model	B
the	O
binary	B
event	O
wage	O
>	O
250	O
as	O
a	O
function	B
of	O
age	O
.	O
the	O
ﬁtted	O
posterior	O
probability	B
of	O
wage	O
exceeding	O
$	O
250,000	O
is	O
shown	O
.	O
of	O
age	O
.	O
this	O
was	O
speciﬁed	O
by	O
requesting	O
four	O
degrees	B
of	I
freedom	I
.	O
the	O
ar-	O
gument	O
by	O
which	O
four	O
degrees	B
of	I
freedom	I
leads	O
to	O
three	O
interior	O
knots	O
is	O
somewhat	O
technical.4	O
how	O
many	O
knots	O
should	O
we	O
use	O
,	O
or	O
equivalently	O
how	O
many	O
degrees	B
of	I
freedom	I
should	O
our	O
spline	B
contain	O
?	O
one	O
option	O
is	O
to	O
try	O
out	O
diﬀerent	O
num-	O
bers	O
of	O
knots	O
and	O
see	O
which	O
produces	O
the	O
best	O
looking	O
curve	O
.	O
a	O
somewhat	O
more	O
objective	O
approach	B
is	O
to	O
use	O
cross-validation	B
,	O
as	O
discussed	O
in	O
chap-	O
ters	O
5	O
and	O
6.	O
with	O
this	O
method	O
,	O
we	O
remove	O
a	O
portion	O
of	O
the	O
data	B
(	O
say	O
10	O
%	O
)	O
,	O
ﬁt	B
a	O
spline	B
with	O
a	O
certain	O
number	O
of	O
knots	O
to	O
the	O
remaining	O
data	B
,	O
and	O
then	O
use	O
the	O
spline	B
to	O
make	O
predictions	O
for	O
the	O
held-out	O
portion	O
.	O
we	O
repeat	O
this	O
process	O
multiple	B
times	O
until	O
each	O
observation	O
has	O
been	O
left	O
out	O
once	O
,	O
and	O
then	O
compute	O
the	O
overall	O
cross-validated	O
rss	O
.	O
this	O
procedure	O
can	O
be	O
re-	O
peated	O
for	O
diﬀerent	O
numbers	O
of	O
knots	O
k.	O
then	O
the	O
value	O
of	O
k	O
giving	O
the	O
smallest	O
rss	O
is	O
chosen	O
.	O
4there	O
are	O
actually	O
ﬁve	O
knots	O
,	O
including	O
the	O
two	O
boundary	O
knots	O
.	O
a	O
cubic	B
spline	O
with	O
ﬁve	O
knots	O
would	O
have	O
nine	O
degrees	B
of	I
freedom	I
.	O
but	O
natural	B
cubic	O
splines	O
have	O
two	O
additional	O
natural	B
constraints	O
at	O
each	O
boundary	O
to	O
enforce	O
linearity	O
,	O
resulting	O
in	O
9−4	O
=	O
5	O
degrees	B
of	I
freedom	I
.	O
since	O
this	O
includes	O
a	O
constant	O
,	O
which	O
is	O
absorbed	O
in	O
the	O
intercept	B
,	O
we	O
count	O
it	O
as	O
four	O
degrees	B
of	I
freedom	I
.	O
276	O
7.	O
moving	O
beyond	O
linearity	O
0	O
8	O
6	O
1	O
0	O
6	O
6	O
1	O
0	O
4	O
6	O
1	O
0	O
2	O
6	O
1	O
0	O
0	O
6	O
1	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0	O
8	O
6	O
1	O
0	O
6	O
6	O
1	O
0	O
4	O
6	O
1	O
0	O
2	O
6	O
1	O
0	O
0	O
6	O
1	O
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
2	O
4	O
6	O
8	O
10	O
2	O
4	O
6	O
8	O
10	O
degrees	B
of	I
freedom	I
of	O
natural	B
spline	I
degrees	O
of	O
freedom	O
of	O
cubic	B
spline	O
figure	O
7.6.	O
ten-fold	O
cross-validated	O
mean	O
squared	O
errors	O
for	O
selecting	O
the	O
degrees	B
of	I
freedom	I
when	O
ﬁtting	O
splines	O
to	O
the	O
wage	O
data	B
.	O
the	O
response	B
is	O
wage	O
and	O
the	O
predictor	B
age	O
.	O
left	O
:	O
a	O
natural	B
cubic	O
spline	B
.	O
right	O
:	O
a	O
cubic	B
spline	O
.	O
figure	O
7.6	O
shows	O
ten-fold	O
cross-validated	O
mean	O
squared	O
errors	O
for	O
splines	O
with	O
various	O
degrees	B
of	I
freedom	I
ﬁt	O
to	O
the	O
wage	O
data	B
.	O
the	O
left-hand	O
panel	O
corresponds	O
to	O
a	O
natural	B
spline	I
and	O
the	O
right-hand	O
panel	O
to	O
a	O
cubic	B
spline	O
.	O
the	O
two	O
methods	O
produce	O
almost	O
identical	O
results	O
,	O
with	O
clear	O
evidence	O
that	O
a	O
one-degree	O
ﬁt	B
(	O
a	O
linear	B
regression	I
)	O
is	O
not	O
adequate	O
.	O
both	O
curves	O
ﬂatten	O
out	O
quickly	O
,	O
and	O
it	O
seems	O
that	O
three	O
degrees	B
of	I
freedom	I
for	O
the	O
natural	B
spline	I
and	O
four	O
degrees	B
of	I
freedom	I
for	O
the	O
cubic	B
spline	O
are	O
quite	O
adequate	O
.	O
in	O
section	O
7.7	O
we	O
ﬁt	B
additive	O
spline	B
models	O
simultaneously	O
on	O
several	O
variables	O
at	O
a	O
time	O
.	O
this	O
could	O
potentially	O
require	O
the	O
selection	B
of	O
degrees	B
of	I
freedom	I
for	O
each	O
variable	B
.	O
in	O
cases	O
like	O
this	O
we	O
typically	O
adopt	O
a	O
more	O
pragmatic	O
approach	B
and	O
set	B
the	O
degrees	B
of	I
freedom	I
to	O
a	O
ﬁxed	O
number	O
,	O
say	O
four	O
,	O
for	O
all	O
terms	O
.	O
7.4.5	O
comparison	O
to	O
polynomial	B
regression	O
regression	B
splines	O
often	O
give	O
superior	O
results	O
to	O
polynomial	B
regression	O
.	O
this	O
is	O
because	O
unlike	O
polynomials	O
,	O
which	O
must	O
use	O
a	O
high	O
degree	O
(	O
exponent	O
in	O
the	O
highest	O
monomial	O
term	B
,	O
e.g	O
.	O
x	O
15	O
)	O
to	O
produce	O
ﬂexible	B
ﬁts	O
,	O
splines	O
intro-	O
duce	O
ﬂexibility	O
by	O
increasing	O
the	O
number	O
of	O
knots	O
but	O
keeping	O
the	O
degree	O
ﬁxed	O
.	O
generally	O
,	O
this	O
approach	B
produces	O
more	O
stable	O
estimates	O
.	O
splines	O
also	O
allow	O
us	O
to	O
place	O
more	O
knots	O
,	O
and	O
hence	O
ﬂexibility	O
,	O
over	O
regions	O
where	O
the	O
function	B
f	O
seems	O
to	O
be	O
changing	O
rapidly	O
,	O
and	O
fewer	O
knots	O
where	O
f	O
appears	O
more	O
stable	O
.	O
figure	O
7.7	O
compares	O
a	O
natural	B
cubic	O
spline	B
with	O
15	O
degrees	B
of	I
freedom	I
to	O
a	O
degree-15	O
polynomial	B
on	O
the	O
wage	O
data	B
set	O
.	O
the	O
extra	O
ﬂexibil-	O
ity	O
in	O
the	O
polynomial	B
produces	O
undesirable	O
results	O
at	O
the	O
boundaries	O
,	O
while	O
the	O
natural	B
cubic	O
spline	B
still	O
provides	O
a	O
reasonable	O
ﬁt	B
to	O
the	O
data	B
.	O
7.5	O
smoothing	B
splines	O
277	O
natural	B
cubic	O
spline	B
polynomial	O
0	O
0	O
3	O
0	O
5	O
2	O
0	O
0	O
2	O
0	O
5	O
1	O
0	O
0	O
1	O
0	O
5	O
e	O
g	O
a	O
w	O
20	O
30	O
40	O
50	O
60	O
70	O
80	O
age	O
figure	O
7.7.	O
on	O
the	O
wage	O
data	B
set	O
,	O
a	O
natural	B
cubic	O
spline	B
with	O
15	O
degrees	B
of	I
freedom	I
is	O
compared	O
to	O
a	O
degree-15	O
polynomial	B
.	O
polynomials	O
can	O
show	O
wild	O
behavior	O
,	O
especially	O
near	O
the	O
tails	O
.	O
7.5	O
smoothing	B
splines	O
7.5.1	O
an	O
overview	O
of	O
smoothing	B
splines	O
in	O
the	O
last	O
section	O
we	O
discussed	O
regression	B
splines	O
,	O
which	O
we	O
create	O
by	O
spec-	O
ifying	O
a	O
set	B
of	O
knots	O
,	O
producing	O
a	O
sequence	O
of	O
basis	B
functions	O
,	O
and	O
then	O
using	O
least	B
squares	I
to	O
estimate	O
the	O
spline	B
coeﬃcients	O
.	O
we	O
now	O
introduce	O
a	O
somewhat	O
diﬀerent	O
approach	B
that	O
also	O
produces	O
a	O
spline	B
.	O
n	O
(	O
cid:10	O
)	O
i=1	O
(	O
yi	O
−	O
g	O
(	O
xi	O
)	O
)	O
2	O
to	O
be	O
small	O
.	O
however	O
,	O
there	O
is	O
a	O
problem	O
in	O
ﬁtting	O
a	O
smooth	O
curve	O
to	O
a	O
set	B
of	O
data	B
,	O
what	O
we	O
really	O
want	O
to	O
do	O
is	O
ﬁnd	O
some	O
function	B
,	O
say	O
g	O
(	O
x	O
)	O
,	O
that	O
ﬁts	O
the	O
observed	O
data	B
well	O
:	O
that	O
is	O
,	O
we	O
want	O
rss	O
=	O
with	O
this	O
approach	B
.	O
if	O
we	O
don	O
’	O
t	O
put	O
any	O
constraints	O
on	O
g	O
(	O
xi	O
)	O
,	O
then	O
we	O
can	O
always	O
make	O
rss	O
zero	O
simply	O
by	O
choosing	O
g	O
such	O
that	O
it	O
interpolates	O
all	O
of	O
the	O
yi	O
.	O
such	O
a	O
function	B
would	O
woefully	O
overﬁt	O
the	O
data—it	O
would	O
be	O
far	O
too	O
ﬂexible	B
.	O
what	O
we	O
really	O
want	O
is	O
a	O
function	B
g	O
that	O
makes	O
rss	O
small	O
,	O
but	O
that	O
is	O
also	O
smooth	O
.	O
how	O
might	O
we	O
ensure	O
that	O
g	O
is	O
smooth	O
?	O
there	O
are	O
a	O
number	O
of	O
ways	O
to	O
do	O
this	O
.	O
a	O
natural	B
approach	O
is	O
to	O
ﬁnd	O
the	O
function	B
g	O
that	O
minimizes	O
n	O
(	O
cid:17	O
)	O
-	O
(	O
yi	O
−	O
g	O
(	O
xi	O
)	O
)	O
2	O
+	O
λ	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
g	O
(	O
t	O
)	O
2dt	O
(	O
7.11	O
)	O
i=1	O
where	O
λ	O
is	O
a	O
nonnegative	O
tuning	B
parameter	I
.	O
the	O
function	B
g	O
that	O
minimizes	O
(	O
7.11	O
)	O
is	O
known	O
as	O
a	O
smoothing	B
spline	I
.	O
what	O
does	O
(	O
7.11	O
)	O
mean	O
?	O
equation	O
7.11	O
takes	O
the	O
“	O
loss+penalty	O
”	O
for-	O
mulation	O
that	O
we	O
encounter	O
in	O
the	O
context	O
of	O
ridge	B
regression	I
and	O
the	O
lasso	B
in	O
chapter	O
6.	O
the	O
term	B
ages	O
g	O
to	O
ﬁt	B
the	O
data	B
well	O
,	O
and	O
the	O
term	B
λ	O
(	O
cid:10	O
)	O
i=1	O
(	O
yi	O
−	O
g	O
(	O
xi	O
)	O
)	O
2	O
is	O
a	O
loss	B
function	I
that	O
encour-	O
(	O
t	O
)	O
2dt	O
is	O
a	O
penalty	B
term	O
.	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
g	O
n	O
smoothing	B
spline	I
loss	O
function	B
278	O
7.	O
moving	O
beyond	O
linearity	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
.	O
.	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
g	O
(	O
t	O
)	O
will	O
be	O
close	O
to	O
constant	O
and	O
(	O
t	O
)	O
indicates	O
the	O
second	O
that	O
penalizes	O
the	O
variability	O
in	O
g.	O
the	O
notation	O
g	O
(	O
cid:5	O
)	O
derivative	B
of	O
the	O
function	B
g.	O
the	O
ﬁrst	O
derivative	B
g	O
(	O
t	O
)	O
measures	O
the	O
slope	B
of	O
a	O
function	B
at	O
t	O
,	O
and	O
the	O
second	O
derivative	B
corresponds	O
to	O
the	O
amount	O
by	O
which	O
the	O
slope	B
is	O
changing	O
.	O
hence	O
,	O
broadly	O
speaking	O
,	O
the	O
second	O
derivative	B
of	O
a	O
function	B
is	O
a	O
measure	O
of	O
its	O
roughness	O
:	O
it	O
is	O
large	O
in	O
absolute	O
value	O
if	O
g	O
(	O
t	O
)	O
is	O
very	O
wiggly	O
near	O
t	O
,	O
and	O
it	O
is	O
close	O
to	O
zero	O
otherwise	O
.	O
(	O
the	O
second	O
derivative	B
of	O
a	O
straight	O
line	B
is	O
zero	O
;	O
note	O
that	O
a	O
line	B
is	O
perfectly	O
smooth	O
.	O
)	O
notation	O
is	O
an	O
integral	B
,	O
which	O
we	O
can	O
think	O
of	O
as	O
a	O
summation	O
over	O
the	O
(	O
t	O
)	O
2dt	O
is	O
simply	O
a	O
measure	O
of	O
the	O
total	O
the	O
range	O
of	O
t.	O
in	O
other	O
words	O
,	O
.	O
change	O
in	O
the	O
function	B
g	O
(	O
t	O
)	O
,	O
over	O
its	O
entire	O
range	O
.	O
if	O
g	O
is	O
very	O
smooth	O
,	O
then	O
(	O
cid:5	O
)	O
(	O
t	O
)	O
2dt	O
will	O
take	O
on	O
a	O
small	O
value	O
.	O
g	O
.	O
(	O
t	O
)	O
will	O
vary	O
signiﬁcantly	O
and	O
conversely	O
,	O
if	O
g	O
is	O
jumpy	O
and	O
variable	B
then	O
g	O
(	O
t	O
)	O
2dt	O
en-	O
courages	O
g	O
to	O
be	O
smooth	O
.	O
the	O
larger	O
the	O
value	O
of	O
λ	O
,	O
the	O
smoother	B
g	O
will	O
be	O
.	O
when	O
λ	O
=	O
0	O
,	O
then	O
the	O
penalty	B
term	O
in	O
(	O
7.11	O
)	O
has	O
no	O
eﬀect	O
,	O
and	O
so	O
the	O
function	B
g	O
will	O
be	O
very	O
jumpy	O
and	O
will	O
exactly	O
interpolate	O
the	O
training	B
observations	O
.	O
when	O
λ	O
→	O
∞	O
,	O
g	O
will	O
be	O
perfectly	O
smooth—it	O
will	O
just	O
be	O
a	O
straight	O
line	B
that	O
passes	O
as	O
closely	O
as	O
possible	O
to	O
the	O
training	B
points	O
.	O
in	O
fact	O
,	O
in	O
this	O
case	O
,	O
g	O
will	O
be	O
the	O
linear	B
least	O
squares	O
line	B
,	O
since	O
the	O
loss	B
function	I
in	O
(	O
7.11	O
)	O
amounts	O
to	O
minimizing	O
the	O
residual	B
sum	O
of	O
squares	O
.	O
for	O
an	O
intermediate	O
value	O
of	O
λ	O
,	O
g	O
will	O
approximate	O
the	O
training	B
observations	O
but	O
will	O
be	O
somewhat	O
smooth	O
.	O
we	O
see	O
that	O
λ	O
controls	O
the	O
bias-variance	B
trade-oﬀ	O
of	O
the	O
smoothing	B
spline	I
.	O
(	O
t	O
)	O
2dt	O
will	O
take	O
on	O
a	O
large	O
value	O
.	O
therefore	O
,	O
in	O
(	O
7.11	O
)	O
,	O
λ	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
g	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
g	O
(	O
cid:5	O
)	O
.	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
g	O
the	O
function	B
g	O
(	O
x	O
)	O
that	O
minimizes	O
(	O
7.11	O
)	O
can	O
be	O
shown	O
to	O
have	O
some	O
spe-	O
cial	O
properties	O
:	O
it	O
is	O
a	O
piecewise	O
cubic	O
polynomial	B
with	O
knots	O
at	O
the	O
unique	O
values	O
of	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
and	O
continuous	B
ﬁrst	O
and	O
second	O
derivatives	O
at	O
each	O
knot	B
.	O
furthermore	O
,	O
it	O
is	O
linear	B
in	O
the	O
region	O
outside	O
of	O
the	O
extreme	O
knots	O
.	O
in	O
other	O
words	O
,	O
the	O
function	B
g	O
(	O
x	O
)	O
that	O
minimizes	O
(	O
7.11	O
)	O
is	O
a	O
natural	B
cubic	O
spline	B
with	O
knots	O
at	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
!	O
however	O
,	O
it	O
is	O
not	O
the	O
same	O
natural	B
cubic	O
spline	B
that	O
one	O
would	O
get	O
if	O
one	O
applied	O
the	O
basis	B
function	I
approach	O
de-	O
scribed	O
in	O
section	O
7.4.3	O
with	O
knots	O
at	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn—rather	O
,	O
it	O
is	O
a	O
shrunken	O
version	O
of	O
such	O
a	O
natural	B
cubic	O
spline	B
,	O
where	O
the	O
value	O
of	O
the	O
tuning	O
pa-	O
rameter	O
λ	O
in	O
(	O
7.11	O
)	O
controls	O
the	O
level	B
of	O
shrinkage	B
.	O
7.5.2	O
choosing	O
the	O
smoothing	B
parameter	O
λ	O
we	O
have	O
seen	O
that	O
a	O
smoothing	B
spline	I
is	O
simply	O
a	O
natural	B
cubic	O
spline	B
with	O
knots	O
at	O
every	O
unique	O
value	O
of	O
xi	O
.	O
it	O
might	O
seem	O
that	O
a	O
smoothing	B
spline	I
will	O
have	O
far	O
too	O
many	O
degrees	B
of	I
freedom	I
,	O
since	O
a	O
knot	B
at	O
each	O
data	B
point	O
allows	O
a	O
great	O
deal	O
of	O
ﬂexibility	O
.	O
but	O
the	O
tuning	B
parameter	I
λ	O
controls	O
the	O
roughness	O
of	O
the	O
smoothing	B
spline	I
,	O
and	O
hence	O
the	O
eﬀective	B
degrees	I
of	I
freedom	I
.	O
it	O
is	O
possible	O
to	O
show	O
that	O
as	O
λ	O
increases	O
from	O
0	O
to	O
∞	O
,	O
the	O
eﬀective	B
degrees	I
of	I
freedom	I
,	O
which	O
we	O
write	O
dfλ	O
,	O
decrease	O
from	O
n	O
to	O
2.	O
in	O
the	O
context	O
of	O
smoothing	B
splines	O
,	O
why	O
do	O
we	O
discuss	O
eﬀective	B
degrees	I
of	I
freedom	I
instead	O
of	O
degrees	B
of	I
freedom	I
?	O
usually	O
degrees	B
of	I
freedom	I
refer	O
eﬀective	B
degrees	I
of	I
freedom	I
7.5	O
smoothing	B
splines	O
279	O
to	O
the	O
number	O
of	O
free	O
parameters	O
,	O
such	O
as	O
the	O
number	O
of	O
coeﬃcients	O
ﬁt	B
in	O
a	O
polynomial	B
or	O
cubic	B
spline	O
.	O
although	O
a	O
smoothing	B
spline	I
has	O
n	O
parameters	O
and	O
hence	O
n	O
nominal	O
degrees	B
of	I
freedom	I
,	O
these	O
n	O
parameters	O
are	O
heavily	O
constrained	O
or	O
shrunk	O
down	O
.	O
hence	O
dfλ	O
is	O
a	O
measure	O
of	O
the	O
ﬂexibility	O
of	O
the	O
smoothing	B
spline—the	O
higher	O
it	O
is	O
,	O
the	O
more	O
ﬂexible	B
(	O
and	O
the	O
lower-bias	O
but	O
higher-variance	O
)	O
the	O
smoothing	B
spline	I
.	O
the	O
deﬁnition	O
of	O
eﬀective	B
degrees	I
of	I
freedom	I
is	O
somewhat	O
technical	O
.	O
we	O
can	O
write	O
ˆgλ	O
=	O
sλy	O
,	O
(	O
7.12	O
)	O
where	O
ˆg	O
is	O
the	O
solution	O
to	O
(	O
7.11	O
)	O
for	O
a	O
particular	O
choice	O
of	O
λ—that	O
is	O
,	O
it	O
is	O
a	O
n-vector	O
containing	O
the	O
ﬁtted	O
values	O
of	O
the	O
smoothing	B
spline	I
at	O
the	O
training	B
points	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
equation	O
7.12	O
indicates	O
that	O
the	O
vector	B
of	O
ﬁtted	O
values	O
when	O
applying	O
a	O
smoothing	B
spline	I
to	O
the	O
data	B
can	O
be	O
written	O
as	O
a	O
n	O
×	O
n	O
matrix	O
sλ	O
(	O
for	O
which	O
there	O
is	O
a	O
formula	O
)	O
times	O
the	O
response	B
vector	O
y.	O
then	O
the	O
eﬀective	B
degrees	I
of	I
freedom	I
is	O
deﬁned	O
to	O
be	O
n	O
(	O
cid:17	O
)	O
dfλ	O
=	O
i=1	O
{	O
sλ	O
}	O
ii	O
,	O
(	O
7.13	O
)	O
the	O
sum	O
of	O
the	O
diagonal	O
elements	O
of	O
the	O
matrix	O
sλ	O
.	O
in	O
ﬁtting	O
a	O
smoothing	B
spline	I
,	O
we	O
do	O
not	O
need	O
to	O
select	O
the	O
number	O
or	O
location	O
of	O
the	O
knots—there	O
will	O
be	O
a	O
knot	B
at	O
each	O
training	B
observation	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
.	O
instead	O
,	O
we	O
have	O
another	O
problem	O
:	O
we	O
need	O
to	O
choose	O
the	O
value	O
of	O
λ.	O
it	O
should	O
come	O
as	O
no	O
surprise	O
that	O
one	O
possible	O
solution	O
to	O
this	O
problem	O
is	O
cross-validation	B
.	O
in	O
other	O
words	O
,	O
we	O
can	O
ﬁnd	O
the	O
value	O
of	O
λ	O
that	O
makes	O
the	O
cross-validated	O
rss	O
as	O
small	O
as	O
possible	O
.	O
it	O
turns	O
out	O
that	O
the	O
leave-	O
one-out	O
cross-validation	B
error	O
(	O
loocv	O
)	O
can	O
be	O
computed	O
very	O
eﬃciently	O
for	O
smoothing	O
splines	O
,	O
with	O
essentially	O
the	O
same	O
cost	O
as	O
computing	O
a	O
single	B
ﬁt	O
,	O
using	O
the	O
following	O
formula	O
:	O
rsscv	O
(	O
λ	O
)	O
=	O
(	O
yi	O
−	O
ˆg	O
(	O
−i	O
)	O
λ	O
(	O
xi	O
)	O
)	O
2	O
=	O
n	O
(	O
cid:17	O
)	O
i=1	O
(	O
cid:20	O
)	O
n	O
(	O
cid:17	O
)	O
i=1	O
yi	O
−	O
ˆgλ	O
(	O
xi	O
)	O
1	O
−	O
{	O
sλ	O
}	O
ii	O
(	O
cid:21	O
)	O
2	O
.	O
λ	O
the	O
notation	O
ˆg	O
(	O
−i	O
)	O
(	O
xi	O
)	O
indicates	O
the	O
ﬁtted	B
value	I
for	O
this	O
smoothing	B
spline	I
evaluated	O
at	O
xi	O
,	O
where	O
the	O
ﬁt	B
uses	O
all	O
of	O
the	O
training	B
observations	O
except	O
for	O
the	O
ith	O
observation	O
(	O
xi	O
,	O
yi	O
)	O
.	O
in	O
contrast	B
,	O
ˆgλ	O
(	O
xi	O
)	O
indicates	O
the	O
smoothing	B
spline	I
function	O
ﬁt	B
to	O
all	O
of	O
the	O
training	B
observations	O
and	O
evaluated	O
at	O
xi	O
.	O
this	O
remarkable	O
formula	O
says	O
that	O
we	O
can	O
compute	O
each	O
of	O
these	O
leave-	O
one-out	O
ﬁts	O
using	O
only	O
ˆgλ	O
,	O
the	O
original	O
ﬁt	B
to	O
all	O
of	O
the	O
data	B
!	O
5	O
we	O
have	O
a	O
very	O
similar	O
formula	O
(	O
5.2	O
)	O
on	O
page	O
180	O
in	O
chapter	O
5	O
for	O
least	O
squares	O
linear	B
regression	I
.	O
using	O
(	O
5.2	O
)	O
,	O
we	O
can	O
very	O
quickly	O
perform	O
loocv	O
for	O
the	O
regression	B
splines	O
discussed	O
earlier	O
in	O
this	O
chapter	O
,	O
as	O
well	O
as	O
for	O
least	O
squares	O
regression	B
using	O
arbitrary	O
basis	B
functions	O
.	O
5the	O
exact	O
formulas	O
for	O
computing	O
ˆg	O
(	O
xi	O
)	O
and	O
sλ	O
are	O
very	O
technical	O
;	O
however	O
,	O
eﬃcient	O
algorithms	O
are	O
available	O
for	O
computing	O
these	O
quantities	O
.	O
280	O
7.	O
moving	O
beyond	O
linearity	O
smoothing	B
spline	I
16	O
degrees	B
of	I
freedom	I
6.8	O
degrees	B
of	I
freedom	I
(	O
loocv	O
)	O
e	O
g	O
a	O
w	O
0	O
0	O
3	O
0	O
0	O
2	O
0	O
0	O
1	O
0	O
5	O
0	O
20	O
30	O
40	O
50	O
age	O
60	O
70	O
80	O
figure	O
7.8.	O
smoothing	B
spline	I
ﬁts	O
to	O
the	O
wage	O
data	B
.	O
the	O
red	O
curve	O
results	O
from	O
specifying	O
16	O
eﬀective	B
degrees	I
of	I
freedom	I
.	O
for	O
the	O
blue	O
curve	O
,	O
λ	O
was	O
found	O
automatically	O
by	O
leave-one-out	B
cross-validation	O
,	O
which	O
resulted	O
in	O
6.8	O
eﬀective	B
degrees	I
of	I
freedom	I
.	O
figure	O
7.8	O
shows	O
the	O
results	O
from	O
ﬁtting	O
a	O
smoothing	B
spline	I
to	O
the	O
wage	O
data	B
.	O
the	O
red	O
curve	O
indicates	O
the	O
ﬁt	B
obtained	O
from	O
pre-specifying	O
that	O
we	O
would	O
like	O
a	O
smoothing	B
spline	I
with	O
16	O
eﬀective	B
degrees	I
of	I
freedom	I
.	O
the	O
blue	O
curve	O
is	O
the	O
smoothing	B
spline	I
obtained	O
when	O
λ	O
is	O
chosen	O
using	O
loocv	O
;	O
in	O
this	O
case	O
,	O
the	O
value	O
of	O
λ	O
chosen	O
results	O
in	O
6.8	O
eﬀective	B
degrees	I
of	I
freedom	I
(	O
computed	O
using	O
(	O
7.13	O
)	O
)	O
.	O
for	O
this	O
data	B
,	O
there	O
is	O
little	O
discernible	O
diﬀerence	O
between	O
the	O
two	O
smoothing	B
splines	O
,	O
beyond	O
the	O
fact	O
that	O
the	O
one	O
with	O
16	O
degrees	B
of	I
freedom	I
seems	O
slightly	O
wigglier	O
.	O
since	O
there	O
is	O
little	O
diﬀerence	O
between	O
the	O
two	O
ﬁts	O
,	O
the	O
smoothing	B
spline	I
ﬁt	O
with	O
6.8	O
degrees	B
of	I
freedom	I
is	O
preferable	O
,	O
since	O
in	O
general	O
simpler	O
models	O
are	O
better	O
unless	O
the	O
data	B
provides	O
evidence	O
in	O
support	O
of	O
a	O
more	O
complex	O
model	B
.	O
7.6	O
local	B
regression	I
local	O
regression	B
is	O
a	O
diﬀerent	O
approach	B
for	O
ﬁtting	O
ﬂexible	B
non-linear	O
func-	O
tions	O
,	O
which	O
involves	O
computing	O
the	O
ﬁt	B
at	O
a	O
target	O
point	O
x0	O
using	O
only	O
the	O
nearby	O
training	B
observations	O
.	O
figure	O
7.9	O
illustrates	O
the	O
idea	O
on	O
some	O
simu-	O
lated	O
data	B
,	O
with	O
one	O
target	O
point	O
near	O
0.4	O
,	O
and	O
another	O
near	O
the	O
boundary	O
at	O
0.05.	O
in	O
this	O
ﬁgure	O
the	O
blue	O
line	B
represents	O
the	O
function	B
f	O
(	O
x	O
)	O
from	O
which	O
the	O
data	B
were	O
generated	O
,	O
and	O
the	O
light	O
orange	O
line	B
corresponds	O
to	O
the	O
local	B
regression	I
estimate	O
ˆf	O
(	O
x	O
)	O
.	O
local	B
regression	I
is	O
described	O
in	O
algorithm	O
7.1.	O
note	O
that	O
in	O
step	O
3	O
of	O
algorithm	O
7.1	O
,	O
the	O
weights	O
ki0	O
will	O
diﬀer	O
for	O
each	O
value	O
of	O
x0	O
.	O
in	O
other	O
words	O
,	O
in	O
order	O
to	O
obtain	O
the	O
local	B
regression	I
ﬁt	O
at	O
a	O
new	O
point	O
,	O
we	O
need	O
to	O
ﬁt	B
a	O
new	O
weighted	B
least	I
squares	I
regression	O
model	B
by	O
local	B
regression	I
7.6	O
local	B
regression	I
281	O
local	B
regression	I
5	O
.	O
1	O
0	O
.	O
1	O
5	O
0	O
.	O
0	O
0	O
.	O
5	O
.	O
0	O
−	O
0	O
.	O
1	O
−	O
o	O
oo	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
oo	O
oo	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
ooo	O
ooo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
5	O
.	O
1	O
0	O
.	O
1	O
5	O
0	O
.	O
0	O
0	O
.	O
5	O
.	O
0	O
−	O
0	O
.	O
1	O
−	O
o	O
o	O
oo	O
oo	O
o	O
o	O
oo	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
oo	O
o	O
o	O
o	O
o	O
oo	O
oo	O
oo	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
ooo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
o	O
oo	O
o	O
o	O
o	O
0.0	O
0.2	O
0.4	O
0.6	O
0.8	O
1.0	O
0.0	O
0.2	O
0.4	O
0.6	O
0.8	O
1.0	O
figure	O
7.9.	O
local	B
regression	I
illustrated	O
on	O
some	O
simulated	O
data	B
,	O
where	O
the	O
blue	O
curve	O
represents	O
f	O
(	O
x	O
)	O
from	O
which	O
the	O
data	B
were	O
generated	O
,	O
and	O
the	O
light	O
orange	O
curve	O
corresponds	O
to	O
the	O
local	B
regression	I
estimate	O
ˆf	O
(	O
x	O
)	O
.	O
the	O
orange	O
colored	O
points	O
are	O
local	B
to	O
the	O
target	O
point	O
x0	O
,	O
represented	O
by	O
the	O
orange	O
vertical	O
line	B
.	O
the	O
yellow	O
bell-shape	O
superimposed	O
on	O
the	O
plot	B
indicates	O
weights	O
assigned	O
to	O
each	O
point	O
,	O
decreasing	O
to	O
zero	O
with	O
distance	B
from	O
the	O
target	O
point	O
.	O
the	O
ﬁt	B
ˆf	O
(	O
x0	O
)	O
at	O
x0	O
is	O
obtained	O
by	O
ﬁtting	O
a	O
weighted	B
linear	O
regression	B
(	O
orange	O
line	B
segment	O
)	O
,	O
and	O
using	O
the	O
ﬁtted	B
value	I
at	O
x0	O
(	O
orange	O
solid	O
dot	O
)	O
as	O
the	O
estimate	O
ˆf	O
(	O
x0	O
)	O
.	O
minimizing	O
(	O
7.14	O
)	O
for	O
a	O
new	O
set	B
of	O
weights	O
.	O
local	B
regression	I
is	O
sometimes	O
referred	O
to	O
as	O
a	O
memory-based	O
procedure	O
,	O
because	O
like	O
nearest-neighbors	O
,	O
we	O
need	O
all	O
the	O
training	B
data	O
each	O
time	O
we	O
wish	O
to	O
compute	O
a	O
prediction	B
.	O
we	O
will	O
avoid	O
getting	O
into	O
the	O
technical	O
details	O
of	O
local	B
regression	I
here—there	O
are	O
books	O
written	O
on	O
the	O
topic	O
.	O
in	O
order	O
to	O
perform	O
local	B
regression	I
,	O
there	O
are	O
a	O
number	O
of	O
choices	O
to	O
be	O
made	O
,	O
such	O
as	O
how	O
to	O
deﬁne	O
the	O
weighting	O
function	B
k	O
,	O
and	O
whether	O
to	O
ﬁt	B
a	O
linear	B
,	O
constant	O
,	O
or	O
quadratic	B
regression	O
in	O
step	O
3	O
above	O
.	O
(	O
equation	O
7.14	O
corresponds	O
to	O
a	O
linear	B
regression	I
.	O
)	O
while	O
all	O
of	O
these	O
choices	O
make	O
some	O
diﬀerence	O
,	O
the	O
most	O
important	O
choice	O
is	O
the	O
span	O
s	O
,	O
deﬁned	O
in	O
step	O
1	O
above	O
.	O
the	O
span	O
plays	O
a	O
role	O
like	O
that	O
of	O
the	O
tuning	B
parameter	I
λ	O
in	O
smoothing	B
splines	O
:	O
it	O
controls	O
the	O
ﬂexibility	O
of	O
the	O
non-linear	B
ﬁt	O
.	O
the	O
smaller	O
the	O
value	O
of	O
s	O
,	O
the	O
more	O
local	B
and	O
wiggly	O
will	O
be	O
our	O
ﬁt	B
;	O
alternatively	O
,	O
a	O
very	O
large	O
value	O
of	O
s	O
will	O
lead	O
to	O
a	O
global	O
ﬁt	B
to	O
the	O
data	B
using	O
all	O
of	O
the	O
training	B
observations	O
.	O
we	O
can	O
again	O
use	O
cross-validation	B
to	O
choose	O
s	O
,	O
or	O
we	O
can	O
specify	O
it	O
directly	O
.	O
figure	O
7.10	O
displays	O
local	B
linear	O
regression	B
ﬁts	O
on	O
the	O
wage	O
data	B
,	O
using	O
two	O
values	O
of	O
s	O
:	O
0.7	O
and	O
0.2.	O
as	O
expected	O
,	O
the	O
ﬁt	B
obtained	O
using	O
s	O
=	O
0.7	O
is	O
smoother	B
than	O
that	O
obtained	O
using	O
s	O
=	O
0.2.	O
the	O
idea	O
of	O
local	B
regression	I
can	O
be	O
generalized	O
in	O
many	O
diﬀerent	O
ways	O
.	O
in	O
a	O
setting	O
with	O
multiple	B
features	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
,	O
one	O
very	O
useful	O
general-	O
ization	O
involves	O
ﬁtting	O
a	O
multiple	B
linear	O
regression	B
model	O
that	O
is	O
global	O
in	O
some	O
variables	O
,	O
but	O
local	B
in	O
another	O
,	O
such	O
as	O
time	O
.	O
such	O
varying	O
coeﬃcient	O
282	O
7.	O
moving	O
beyond	O
linearity	O
algorithm	O
7.1	O
local	B
regression	I
at	O
x	O
=	O
x0	O
1.	O
gather	O
the	O
fraction	O
s	O
=	O
k/n	O
of	O
training	B
points	O
whose	O
xi	O
are	O
closest	O
to	O
x0	O
.	O
2.	O
assign	O
a	O
weight	O
ki0	O
=	O
k	O
(	O
xi	O
,	O
x0	O
)	O
to	O
each	O
point	O
in	O
this	O
neighborhood	O
,	O
so	O
that	O
the	O
point	O
furthest	O
from	O
x0	O
has	O
weight	O
zero	O
,	O
and	O
the	O
closest	O
has	O
the	O
highest	O
weight	O
.	O
all	O
but	O
these	O
k	O
nearest	O
neighbors	O
get	O
weight	O
zero	O
.	O
3.	O
fit	O
a	O
weighted	B
least	I
squares	I
regression	O
of	O
the	O
yi	O
on	O
the	O
xi	O
using	O
the	O
aforementioned	O
weights	O
,	O
by	O
ﬁnding	O
ˆβ0	O
and	O
ˆβ1	O
that	O
minimize	O
n	O
(	O
cid:17	O
)	O
ki0	O
(	O
yi	O
−	O
β0	O
−	O
β1xi	O
)	O
2	O
.	O
(	O
7.14	O
)	O
i=1	O
4.	O
the	O
ﬁtted	B
value	I
at	O
x0	O
is	O
given	O
by	O
ˆf	O
(	O
x0	O
)	O
=	O
ˆβ0	O
+	O
ˆβ1x0	O
.	O
models	O
are	O
a	O
useful	O
way	O
of	O
adapting	O
a	O
model	B
to	O
the	O
most	O
recently	O
gathered	O
data	B
.	O
local	B
regression	I
also	O
generalizes	O
very	O
naturally	O
when	O
we	O
want	O
to	O
ﬁt	B
models	O
that	O
are	O
local	B
in	O
a	O
pair	O
of	O
variables	O
x1	O
and	O
x2	B
,	O
rather	O
than	O
one	O
.	O
we	O
can	O
simply	O
use	O
two-dimensional	O
neighborhoods	O
,	O
and	O
ﬁt	B
bivariate	O
linear	B
regression	I
models	O
using	O
the	O
observations	B
that	O
are	O
near	O
each	O
target	O
point	O
in	O
two-dimensional	O
space	O
.	O
theoretically	O
the	O
same	O
approach	B
can	O
be	O
imple-	O
mented	O
in	O
higher	O
dimensions	O
,	O
using	O
linear	B
regressions	O
ﬁt	B
to	O
p-dimensional	O
neighborhoods	O
.	O
however	O
,	O
local	B
regression	I
can	O
perform	O
poorly	O
if	O
p	O
is	O
much	O
larger	O
than	O
about	O
3	O
or	O
4	O
because	O
there	O
will	O
generally	O
be	O
very	O
few	O
training	B
observations	O
close	O
to	O
x0	O
.	O
nearest-neighbors	O
regression	B
,	O
discussed	O
in	O
chap-	O
ter	O
3	O
,	O
suﬀers	O
from	O
a	O
similar	O
problem	O
in	O
high	O
dimensions	O
.	O
varying	B
coeﬃcient	I
model	I
7.7	O
generalized	O
additive	O
models	O
in	O
sections	O
7.1–7.6	O
,	O
we	O
present	O
a	O
number	O
of	O
approaches	O
for	O
ﬂexibly	O
predict-	O
ing	O
a	O
response	B
y	O
on	O
the	O
basis	B
of	O
a	O
single	B
predictor	O
x.	O
these	O
approaches	O
can	O
be	O
seen	O
as	O
extensions	O
of	O
simple	B
linear	O
regression	B
.	O
here	O
we	O
explore	O
the	O
prob-	O
lem	O
of	O
ﬂexibly	O
predicting	O
y	O
on	O
the	O
basis	B
of	O
several	O
predictors	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
.	O
this	O
amounts	O
to	O
an	O
extension	O
of	O
multiple	B
linear	O
regression	B
.	O
generalized	O
additive	O
models	O
(	O
gams	O
)	O
provide	O
a	O
general	O
framework	O
for	O
extending	O
a	O
standard	O
linear	O
model	B
by	O
allowing	O
non-linear	B
functions	O
of	O
each	O
of	O
the	O
variables	O
,	O
while	O
maintaining	O
additivity	B
.	O
just	O
like	O
linear	B
models	O
,	O
gams	O
can	O
be	O
applied	O
with	O
both	O
quantitative	B
and	O
qualitative	B
responses	O
.	O
we	O
ﬁrst	O
examine	O
gams	O
for	O
a	O
quantitative	B
response	O
in	O
section	O
7.7.1	O
,	O
and	O
then	O
for	O
a	O
qualitative	B
response	O
in	O
section	O
7.7.2.	O
generalized	B
additive	I
model	I
additivity	O
7.7	O
generalized	O
additive	O
models	O
283	O
local	B
linear	O
regression	B
span	O
is	O
0.2	O
(	O
16.4	O
degrees	B
of	I
freedom	I
)	O
span	O
is	O
0.7	O
(	O
5.3	O
degrees	B
of	I
freedom	I
)	O
e	O
g	O
a	O
w	O
0	O
0	O
3	O
0	O
0	O
2	O
0	O
0	O
1	O
0	O
5	O
0	O
20	O
30	O
40	O
50	O
age	O
60	O
70	O
80	O
figure	O
7.10.	O
local	B
linear	O
ﬁts	O
to	O
the	O
wage	O
data	B
.	O
the	O
span	O
speciﬁes	O
the	O
fraction	O
of	O
the	O
data	B
used	O
to	O
compute	O
the	O
ﬁt	B
at	O
each	O
target	O
point	O
.	O
7.7.1	O
gams	O
for	O
regression	O
problems	O
a	O
natural	B
way	O
to	O
extend	O
the	O
multiple	B
linear	O
regression	B
model	O
yi	O
=	O
β0	O
+	O
β1xi1	O
+	O
β2xi2	O
+	O
···	O
+	O
βpxip	O
+	O
i	O
in	O
order	O
to	O
allow	O
for	O
non-linear	O
relationships	O
between	O
each	O
feature	B
and	O
the	O
response	B
is	O
to	O
replace	O
each	O
linear	B
component	O
βjxij	O
with	O
a	O
(	O
smooth	O
)	O
non-	O
linear	B
function	O
fj	O
(	O
xij	O
)	O
.	O
we	O
would	O
then	O
write	O
the	O
model	B
as	O
p	O
(	O
cid:17	O
)	O
yi	O
=	O
β0	O
+	O
fj	O
(	O
xij	O
)	O
+	O
i	O
j=1	O
=	O
β0	O
+	O
f1	O
(	O
xi1	O
)	O
+	O
f2	O
(	O
xi2	O
)	O
+	O
···	O
+	O
fp	O
(	O
xip	O
)	O
+	O
i	O
.	O
(	O
7.15	O
)	O
this	O
is	O
an	O
example	O
of	O
a	O
gam	O
.	O
it	O
is	O
called	O
an	O
additive	B
model	O
because	O
we	O
calculate	O
a	O
separate	O
fj	O
for	O
each	O
xj	O
,	O
and	O
then	O
add	O
together	O
all	O
of	O
their	O
contributions	O
.	O
in	O
sections	O
7.1–7.6	O
,	O
we	O
discuss	O
many	O
methods	O
for	O
ﬁtting	O
functions	O
to	O
a	O
single	B
variable	O
.	O
the	O
beauty	O
of	O
gams	O
is	O
that	O
we	O
can	O
use	O
these	O
methods	O
as	O
building	O
blocks	O
for	O
ﬁtting	O
an	O
additive	B
model	O
.	O
in	O
fact	O
,	O
for	O
most	O
of	O
the	O
methods	O
that	O
we	O
have	O
seen	O
so	O
far	O
in	O
this	O
chapter	O
,	O
this	O
can	O
be	O
done	O
fairly	O
trivially	O
.	O
take	O
,	O
for	O
example	O
,	O
natural	B
splines	O
,	O
and	O
consider	O
the	O
task	O
of	O
ﬁtting	O
the	O
model	B
wage	O
=	O
β0	O
+	O
f1	O
(	O
year	O
)	O
+	O
f2	O
(	O
age	O
)	O
+	O
f3	O
(	O
education	O
)	O
+	O
	O
(	O
7.16	O
)	O
284	O
7.	O
moving	O
beyond	O
linearity	O
<	O
hs	O
hs	O
<	O
coll	O
coll	O
>	O
coll	O
)	O
r	O
a	O
e	O
y	O
(	O
1	O
f	O
0	O
3	O
0	O
2	O
0	O
1	O
0	O
0	O
1	O
−	O
0	O
2	O
−	O
0	O
3	O
−	O
)	O
e	O
g	O
a	O
(	O
2	O
f	O
0	O
2	O
0	O
1	O
0	O
0	O
1	O
−	O
0	O
2	O
−	O
0	O
3	O
−	O
0	O
4	O
−	O
0	O
5	O
−	O
)	O
n	O
o	O
i	O
t	O
a	O
c	O
u	O
d	O
e	O
(	O
3	O
f	O
0	O
4	O
0	O
3	O
0	O
2	O
0	O
1	O
0	O
0	O
1	O
−	O
0	O
2	O
−	O
0	O
3	O
−	O
2003	O
2005	O
2007	O
2009	O
20	O
30	O
40	O
year	O
60	O
70	O
80	O
50	O
age	O
education	O
figure	O
7.11.	O
for	O
the	O
wage	O
data	B
,	O
plots	O
of	O
the	O
relationship	O
between	O
each	O
feature	B
and	O
the	O
response	B
,	O
wage	O
,	O
in	O
the	O
ﬁtted	O
model	O
(	O
7.16	O
)	O
.	O
each	O
plot	B
displays	O
the	O
ﬁtted	O
function	O
and	O
pointwise	O
standard	O
errors	O
.	O
the	O
ﬁrst	O
two	O
functions	O
are	O
natural	B
splines	O
in	O
year	O
and	O
age	O
,	O
with	O
four	O
and	O
ﬁve	O
degrees	B
of	I
freedom	I
,	O
respectively	O
.	O
the	O
third	O
function	B
is	O
a	O
step	B
function	I
,	O
ﬁt	B
to	O
the	O
qualitative	B
variable	O
education	O
.	O
on	O
the	O
wage	O
data	B
.	O
here	O
year	O
and	O
age	O
are	O
quantitative	B
variables	O
,	O
and	O
education	O
is	O
a	O
qualitative	B
variable	O
with	O
ﬁve	O
levels	O
:	O
<	O
hs	O
,	O
hs	O
,	O
<	O
coll	O
,	O
coll	O
,	O
>	O
coll	O
,	O
referring	O
to	O
the	O
amount	O
of	O
high	O
school	O
or	O
college	O
education	O
that	O
an	O
individual	O
has	O
completed	O
.	O
we	O
ﬁt	B
the	O
ﬁrst	O
two	O
functions	O
using	O
natural	B
splines	O
.	O
we	O
ﬁt	B
the	O
third	O
function	B
using	O
a	O
separate	O
constant	O
for	O
each	O
level	B
,	O
via	O
the	O
usual	O
dummy	B
variable	I
approach	O
of	O
section	O
3.3.1.	O
figure	O
7.11	O
shows	O
the	O
results	O
of	O
ﬁtting	O
the	O
model	B
(	O
7.16	O
)	O
using	O
least	B
squares	I
.	O
this	O
is	O
easy	O
to	O
do	O
,	O
since	O
as	O
discussed	O
in	O
section	O
7.4	O
,	O
natural	B
splines	O
can	O
be	O
constructed	O
using	O
an	O
appropriately	O
chosen	O
set	B
of	O
basis	B
functions	O
.	O
hence	O
the	O
entire	O
model	B
is	O
just	O
a	O
big	O
regression	B
onto	O
spline	B
basis	O
variables	O
and	O
dummy	B
variables	O
,	O
all	O
packed	O
into	O
one	O
big	O
regression	B
matrix	O
.	O
figure	O
7.11	O
can	O
be	O
easily	O
interpreted	O
.	O
the	O
left-hand	O
panel	O
indicates	O
that	O
holding	O
age	O
and	O
education	O
ﬁxed	O
,	O
wage	O
tends	O
to	O
increase	O
slightly	O
with	O
year	O
;	O
this	O
may	O
be	O
due	O
to	O
inﬂation	O
.	O
the	O
center	O
panel	O
indicates	O
that	O
holding	O
education	O
and	O
year	O
ﬁxed	O
,	O
wage	O
tends	O
to	O
be	O
highest	O
for	O
intermediate	O
val-	O
ues	O
of	O
age	O
,	O
and	O
lowest	O
for	O
the	O
very	O
young	O
and	O
very	O
old	O
.	O
the	O
right-hand	O
panel	O
indicates	O
that	O
holding	O
year	O
and	O
age	O
ﬁxed	O
,	O
wage	O
tends	O
to	O
increase	O
with	O
education	O
:	O
the	O
more	O
educated	O
a	O
person	O
is	O
,	O
the	O
higher	O
their	O
salary	O
,	O
on	O
average	B
.	O
all	O
of	O
these	O
ﬁndings	O
are	O
intuitive	O
.	O
figure	O
7.12	O
shows	O
a	O
similar	O
triple	O
of	O
plots	O
,	O
but	O
this	O
time	O
f1	O
and	O
f2	O
are	O
smoothing	B
splines	O
with	O
four	O
and	O
ﬁve	O
degrees	B
of	I
freedom	I
,	O
respectively	O
.	O
fit-	O
ting	O
a	O
gam	O
with	O
a	O
smoothing	B
spline	I
is	O
not	O
quite	O
as	O
simple	B
as	O
ﬁtting	O
a	O
gam	O
with	O
a	O
natural	B
spline	I
,	O
since	O
in	O
the	O
case	O
of	O
smoothing	B
splines	O
,	O
least	B
squares	I
can	O
not	O
be	O
used	O
.	O
however	O
,	O
standard	O
software	O
such	O
as	O
the	O
gam	O
(	O
)	O
function	B
in	O
r	O
can	O
be	O
used	O
to	O
ﬁt	B
gams	O
using	O
smoothing	B
splines	O
,	O
via	O
an	O
approach	B
known	O
as	O
backﬁtting	B
.	O
this	O
method	O
ﬁts	O
a	O
model	B
involving	O
multiple	B
predictors	O
by	O
backﬁtting	B
7.7	O
generalized	O
additive	O
models	O
285	O
<	O
hs	O
hs	O
<	O
coll	O
coll	O
>	O
coll	O
)	O
r	O
a	O
e	O
y	O
(	O
1	O
f	O
0	O
3	O
0	O
2	O
0	O
1	O
0	O
0	O
1	O
−	O
0	O
2	O
−	O
0	O
3	O
−	O
)	O
e	O
g	O
a	O
(	O
2	O
f	O
0	O
2	O
0	O
1	O
0	O
0	O
1	O
−	O
0	O
2	O
−	O
0	O
3	O
−	O
0	O
4	O
−	O
0	O
5	O
−	O
)	O
n	O
o	O
i	O
t	O
a	O
c	O
u	O
d	O
e	O
(	O
3	O
f	O
0	O
4	O
0	O
3	O
0	O
2	O
0	O
1	O
0	O
0	O
1	O
−	O
0	O
2	O
−	O
0	O
3	O
−	O
2003	O
2005	O
2007	O
2009	O
20	O
30	O
40	O
year	O
60	O
70	O
80	O
50	O
age	O
education	O
figure	O
7.12.	O
details	O
are	O
as	O
in	O
figure	O
7.11	O
,	O
but	O
now	O
f1	O
and	O
f2	O
are	O
smoothing	B
splines	O
with	O
four	O
and	O
ﬁve	O
degrees	B
of	I
freedom	I
,	O
respectively	O
.	O
repeatedly	O
updating	O
the	O
ﬁt	B
for	O
each	O
predictor	B
in	O
turn	O
,	O
holding	O
the	O
others	O
ﬁxed	O
.	O
the	O
beauty	O
of	O
this	O
approach	B
is	O
that	O
each	O
time	O
we	O
update	O
a	O
function	B
,	O
we	O
simply	O
apply	O
the	O
ﬁtting	O
method	O
for	O
that	O
variable	B
to	O
a	O
partial	O
residual.6	O
the	O
ﬁtted	O
functions	O
in	O
figures	O
7.11	O
and	O
7.12	O
look	O
rather	O
similar	O
.	O
in	O
most	O
situations	O
,	O
the	O
diﬀerences	O
in	O
the	O
gams	O
obtained	O
using	O
smoothing	B
splines	O
versus	O
natural	B
splines	O
are	O
small	O
.	O
we	O
do	O
not	O
have	O
to	O
use	O
splines	O
as	O
the	O
building	O
blocks	O
for	O
gams	O
:	O
we	O
can	O
just	O
as	O
well	O
use	O
local	B
regression	I
,	O
polynomial	B
regression	O
,	O
or	O
any	O
combination	O
of	O
the	O
approaches	O
seen	O
earlier	O
in	O
this	O
chapter	O
in	O
order	O
to	O
create	O
a	O
gam	O
.	O
gams	O
are	O
investigated	O
in	O
further	O
detail	O
in	O
the	O
lab	O
at	O
the	O
end	O
of	O
this	O
chapter	O
.	O
pros	O
and	O
cons	O
of	O
gams	O
before	O
we	O
move	O
on	O
,	O
let	O
us	O
summarize	O
the	O
advantages	O
and	O
limitations	O
of	O
a	O
gam	O
.	O
▲	O
gams	O
allow	O
us	O
to	O
ﬁt	B
a	O
non-linear	B
fj	O
to	O
each	O
xj	O
,	O
so	O
that	O
we	O
can	O
automatically	O
model	B
non-linear	O
relationships	O
that	O
standard	O
linear	O
re-	O
gression	O
will	O
miss	O
.	O
this	O
means	O
that	O
we	O
do	O
not	O
need	O
to	O
manually	O
try	O
out	O
many	O
diﬀerent	O
transformations	O
on	O
each	O
variable	B
individually	O
.	O
▲	O
the	O
non-linear	B
ﬁts	O
can	O
potentially	O
make	O
more	O
accurate	O
predictions	O
for	O
the	O
response	B
y	O
.	O
▲	O
because	O
the	O
model	B
is	O
additive	B
,	O
we	O
can	O
still	O
examine	O
the	O
eﬀect	O
of	O
each	O
xj	O
on	O
y	O
individually	O
while	O
holding	O
all	O
of	O
the	O
other	O
variables	O
ﬁxed	O
.	O
hence	O
if	O
we	O
are	O
interested	O
in	O
inference	B
,	O
gams	O
provide	O
a	O
useful	O
representation	O
.	O
6a	O
partial	O
residual	O
for	O
x3	O
,	O
for	O
example	O
,	O
has	O
the	O
form	O
ri	O
=	O
yi	O
−	O
f1	O
(	O
xi1	O
)	O
−	O
f2	O
(	O
xi2	O
)	O
.	O
if	O
we	O
know	O
f1	O
and	O
f2	O
,	O
then	O
we	O
can	O
ﬁt	B
f3	O
by	O
treating	O
this	O
residual	B
as	O
a	O
response	B
in	O
a	O
non-linear	B
regression	O
on	O
x3	O
.	O
286	O
7.	O
moving	O
beyond	O
linearity	O
▲	O
the	O
smoothness	O
of	O
the	O
function	B
fj	O
for	O
the	O
variable	B
xj	O
can	O
be	O
sum-	O
marized	O
via	O
degrees	B
of	I
freedom	I
.	O
◆	O
the	O
main	O
limitation	O
of	O
gams	O
is	O
that	O
the	O
model	B
is	O
restricted	O
to	O
be	O
additive	B
.	O
with	O
many	O
variables	O
,	O
important	O
interactions	O
can	O
be	O
missed	O
.	O
however	O
,	O
as	O
with	O
linear	B
regression	I
,	O
we	O
can	O
manually	O
add	O
interaction	B
terms	O
to	O
the	O
gam	O
model	B
by	O
including	O
additional	O
predictors	O
of	O
the	O
form	O
xj	O
×	O
xk	O
.	O
in	O
addition	O
we	O
can	O
add	O
low-dimensional	B
interaction	O
functions	O
of	O
the	O
form	O
fjk	O
(	O
xj	O
,	O
xk	O
)	O
into	O
the	O
model	B
;	O
such	O
terms	O
can	O
be	O
ﬁt	B
using	O
two-dimensional	O
smoothers	O
such	O
as	O
local	B
regression	I
,	O
or	O
two-dimensional	O
splines	O
(	O
not	O
covered	O
here	O
)	O
.	O
for	O
fully	O
general	O
models	O
,	O
we	O
have	O
to	O
look	O
for	O
even	O
more	O
ﬂexible	B
approaches	O
such	O
as	O
random	O
forests	O
and	O
boosting	B
,	O
described	O
in	O
chapter	O
8.	O
gams	O
provide	O
a	O
useful	O
compromise	O
between	O
linear	B
and	O
fully	O
nonparametric	O
models	O
.	O
7.7.2	O
gams	O
for	O
classiﬁcation	O
problems	O
gams	O
can	O
also	O
be	O
used	O
in	O
situations	O
where	O
y	O
is	O
qualitative	B
.	O
for	O
simplicity	O
,	O
here	O
we	O
will	O
assume	O
y	O
takes	O
on	O
values	O
zero	O
or	O
one	O
,	O
and	O
let	O
p	O
(	O
x	O
)	O
=	O
pr	O
(	O
y	O
=	O
1|x	O
)	O
be	O
the	O
conditional	B
probability	I
(	O
given	O
the	O
predictors	O
)	O
that	O
the	O
response	B
equals	O
one	O
.	O
recall	B
the	O
logistic	B
regression	I
model	O
(	O
4.6	O
)	O
:	O
(	O
cid:11	O
)	O
(	O
cid:12	O
)	O
log	O
p	O
(	O
x	O
)	O
1	O
−	O
p	O
(	O
x	O
)	O
(	O
cid:11	O
)	O
(	O
cid:12	O
)	O
=	O
β0	O
+	O
β1x1	O
+	O
β2x2	O
+	O
···	O
+	O
βpxp	O
.	O
(	O
7.17	O
)	O
this	O
logit	B
is	O
the	O
log	O
of	O
the	O
odds	B
of	O
p	O
(	O
y	O
=	O
1|x	O
)	O
versus	O
p	O
(	O
y	O
=	O
0|x	O
)	O
,	O
which	O
(	O
7.17	O
)	O
represents	O
as	O
a	O
linear	B
function	O
of	O
the	O
predictors	O
.	O
a	O
natural	B
way	O
to	O
extend	O
(	O
7.17	O
)	O
to	O
allow	O
for	O
non-linear	O
relationships	O
is	O
to	O
use	O
the	O
model	B
log	O
p	O
(	O
x	O
)	O
1	O
−	O
p	O
(	O
x	O
)	O
=	O
β0	O
+	O
f1	O
(	O
x1	O
)	O
+	O
f2	O
(	O
x2	B
)	O
+	O
···	O
+	O
fp	O
(	O
xp	O
)	O
.	O
(	O
7.18	O
)	O
equation	O
7.18	O
is	O
a	O
logistic	B
regression	I
gam	O
.	O
it	O
has	O
all	O
the	O
same	O
pros	O
and	O
cons	O
as	O
discussed	O
in	O
the	O
previous	O
section	O
for	O
quantitative	O
responses	O
.	O
we	O
ﬁt	B
a	O
gam	O
to	O
the	O
wage	O
data	B
in	O
order	O
to	O
predict	O
the	O
probability	B
that	O
an	O
individual	O
’	O
s	O
income	O
exceeds	O
$	O
250,000	O
per	O
year	O
.	O
the	O
gam	O
that	O
we	O
ﬁt	B
takes	O
the	O
form	O
=	O
β0	O
+	O
β1	O
×	O
year	O
+	O
f2	O
(	O
age	O
)	O
+	O
f3	O
(	O
education	O
)	O
,	O
(	O
7.19	O
)	O
(	O
cid:11	O
)	O
(	O
cid:12	O
)	O
p	O
(	O
x	O
)	O
1	O
−	O
p	O
(	O
x	O
)	O
log	O
where	O
p	O
(	O
x	O
)	O
=	O
pr	O
(	O
wage	O
>	O
250|year	O
,	O
age	O
,	O
education	O
)	O
.	O
7.8	O
lab	O
:	O
non-linear	B
modeling	O
287	O
<	O
hs	O
hs	O
<	O
coll	O
coll	O
>	O
coll	O
)	O
r	O
a	O
e	O
y	O
(	O
1	O
f	O
4	O
2	O
0	O
2	O
−	O
4	O
−	O
)	O
e	O
g	O
a	O
(	O
2	O
f	O
2	O
0	O
2	O
−	O
4	O
−	O
6	O
−	O
8	O
−	O
)	O
n	O
o	O
i	O
t	O
a	O
c	O
u	O
d	O
e	O
(	O
3	O
f	O
0	O
0	O
4	O
0	O
0	O
2	O
0	O
0	O
0	O
2	O
−	O
0	O
0	O
4	O
−	O
2003	O
2005	O
2007	O
2009	O
20	O
30	O
40	O
year	O
60	O
70	O
80	O
50	O
age	O
education	O
figure	O
7.13.	O
for	O
the	O
wage	O
data	B
,	O
the	O
logistic	B
regression	I
gam	O
given	O
in	O
(	O
7.19	O
)	O
is	O
ﬁt	B
to	O
the	O
binary	B
response	O
i	O
(	O
wage	O
>	O
250	O
)	O
.	O
each	O
plot	B
displays	O
the	O
ﬁtted	O
function	O
and	O
pointwise	O
standard	O
errors	O
.	O
the	O
ﬁrst	O
function	B
is	O
linear	B
in	O
year	O
,	O
the	O
second	O
function	B
a	O
smoothing	B
spline	I
with	O
ﬁve	O
degrees	B
of	I
freedom	I
in	O
age	O
,	O
and	O
the	O
third	O
a	O
step	B
function	I
for	O
education	O
.	O
there	O
are	O
very	O
wide	O
standard	O
errors	O
for	O
the	O
ﬁrst	O
level	B
<	O
hs	O
of	O
education	O
.	O
once	O
again	O
f2	O
is	O
ﬁt	B
using	O
a	O
smoothing	B
spline	I
with	O
ﬁve	O
degrees	B
of	I
freedom	I
,	O
and	O
f3	O
is	O
ﬁt	B
as	O
a	O
step	B
function	I
,	O
by	O
creating	O
dummy	B
variables	O
for	O
each	O
of	O
the	O
levels	O
of	O
education	O
.	O
the	O
resulting	O
ﬁt	B
is	O
shown	O
in	O
figure	O
7.13.	O
the	O
last	O
panel	O
looks	O
suspicious	O
,	O
with	O
very	O
wide	O
conﬁdence	O
intervals	O
for	O
level	O
<	O
hs	O
.	O
in	O
fact	O
,	O
there	O
are	O
no	O
ones	O
for	O
that	O
category	O
:	O
no	O
individuals	O
with	O
less	O
than	O
a	O
high	O
school	O
education	O
make	O
more	O
than	O
$	O
250,000	O
per	O
year	O
.	O
hence	O
we	O
reﬁt	O
the	O
gam	O
,	O
excluding	O
the	O
individuals	O
with	O
less	O
than	O
a	O
high	O
school	O
education	O
.	O
the	O
resulting	O
model	B
is	O
shown	O
in	O
figure	O
7.14.	O
as	O
in	O
figures	O
7.11	O
and	O
7.12	O
,	O
all	O
three	O
panels	O
have	O
the	O
same	O
vertical	O
scale	O
.	O
this	O
allows	O
us	O
to	O
visually	O
assess	O
the	O
relative	O
contributions	O
of	O
each	O
of	O
the	O
variables	O
.	O
we	O
observe	O
that	O
age	O
and	O
education	O
have	O
a	O
much	O
larger	O
eﬀect	O
than	O
year	O
on	O
the	O
probability	B
of	O
being	O
a	O
high	O
earner	O
.	O
7.8	O
lab	O
:	O
non-linear	B
modeling	O
in	O
this	O
lab	O
,	O
we	O
re-analyze	O
the	O
wage	O
data	B
considered	O
in	O
the	O
examples	O
through-	O
out	O
this	O
chapter	O
,	O
in	O
order	O
to	O
illustrate	O
the	O
fact	O
that	O
many	O
of	O
the	O
complex	O
non-linear	B
ﬁtting	O
procedures	O
discussed	O
can	O
be	O
easily	O
implemented	O
in	O
r.	O
we	O
begin	O
by	O
loading	O
the	O
islr	O
library	O
,	O
which	O
contains	O
the	O
data	B
.	O
>	O
library	O
(	O
islr	O
)	O
>	O
attach	O
(	O
wage	O
)	O
288	O
7.	O
moving	O
beyond	O
linearity	O
hs	O
<	O
coll	O
coll	O
>	O
coll	O
)	O
r	O
a	O
e	O
y	O
(	O
1	O
f	O
4	O
2	O
0	O
2	O
−	O
4	O
−	O
)	O
e	O
g	O
a	O
(	O
2	O
f	O
2	O
0	O
2	O
−	O
4	O
−	O
6	O
−	O
8	O
−	O
)	O
n	O
o	O
i	O
t	O
a	O
c	O
u	O
d	O
e	O
(	O
3	O
f	O
4	O
2	O
0	O
2	O
−	O
4	O
−	O
2003	O
2005	O
2007	O
2009	O
20	O
30	O
40	O
ryea	O
60	O
70	O
80	O
50	O
age	O
education	O
figure	O
7.14.	O
the	O
same	O
model	B
is	O
ﬁt	B
as	O
in	O
figure	O
7.13	O
,	O
this	O
time	O
excluding	O
the	O
observations	B
for	O
which	O
education	O
is	O
<	O
hs	O
.	O
now	O
we	O
see	O
that	O
increased	O
education	O
tends	O
to	O
be	O
associated	O
with	O
higher	O
salaries	O
.	O
7.8.1	O
polynomial	B
regression	O
and	O
step	O
functions	O
we	O
now	O
examine	O
how	O
figure	O
7.1	O
was	O
produced	O
.	O
we	O
ﬁrst	O
ﬁt	B
the	O
model	B
using	O
the	O
following	O
command	O
:	O
>	O
fit	O
=	O
lm	O
(	O
wage∼poly	O
(	O
age	O
,4	O
)	O
,	O
data	B
=	O
wage	O
)	O
>	O
coef	O
(	O
summary	O
(	O
fit	O
)	O
)	O
111.704	O
(	O
intercept	B
)	O
poly	O
(	O
age	O
,	O
4	O
)	O
1	O
447.068	O
poly	O
(	O
age	O
,	O
4	O
)	O
2	O
-478.316	O
125.522	O
poly	O
(	O
age	O
,	O
4	O
)	O
3	O
poly	O
(	O
age	O
,	O
4	O
)	O
4	O
-77.911	O
estimate	O
std	O
.	O
error	B
t	O
value	O
pr	O
(	O
>	O
|	O
t	O
|	O
)	O
<	O
2e	O
-16	O
<	O
2e	O
-16	O
<	O
2e	O
-16	O
0.0017	O
0.0510	O
153.28	O
11.20	O
-11.98	O
3.14	O
-1.95	O
0.729	O
39.915	O
39.915	O
39.915	O
39.915	O
this	O
syntax	O
ﬁts	O
a	O
linear	B
model	I
,	O
using	O
the	O
lm	O
(	O
)	O
function	B
,	O
in	O
order	O
to	O
predict	O
wage	O
using	O
a	O
fourth-degree	O
polynomial	B
in	O
age	O
:	O
poly	O
(	O
age,4	O
)	O
.	O
the	O
poly	O
(	O
)	O
com-	O
mand	O
allows	O
us	O
to	O
avoid	O
having	O
to	O
write	O
out	O
a	O
long	O
formula	O
with	O
powers	O
of	O
age	O
.	O
the	O
function	B
returns	O
a	O
matrix	O
whose	O
columns	O
are	O
a	O
basis	B
of	O
or-	O
thogonal	O
polynomials	O
,	O
which	O
essentially	O
means	O
that	O
each	O
column	O
is	O
a	O
linear	B
combination	I
of	O
the	O
variables	O
age	O
,	O
age^2	O
,	O
age^3	O
and	O
age^4	O
.	O
however	O
,	O
we	O
can	O
also	O
use	O
poly	O
(	O
)	O
to	O
obtain	O
age	O
,	O
age^2	O
,	O
age^3	O
and	O
age^4	O
directly	O
,	O
if	O
we	O
prefer	O
.	O
we	O
can	O
do	O
this	O
by	O
using	O
the	O
raw=true	O
argument	B
to	O
the	O
poly	O
(	O
)	O
function	B
.	O
later	O
we	O
see	O
that	O
this	O
does	O
not	O
aﬀect	O
the	O
model	B
in	O
a	O
meaningful	O
way—though	O
the	O
choice	O
of	O
basis	B
clearly	O
aﬀects	O
the	O
coeﬃcient	B
estimates	O
,	O
it	O
does	O
not	O
aﬀect	O
the	O
ﬁtted	O
values	O
obtained	O
.	O
>	O
fit2	O
=	O
lm	O
(	O
wage∼poly	O
(	O
age	O
,4	O
,	O
raw	O
=	O
t	O
)	O
,	O
data	B
=	O
wage	O
)	O
>	O
coef	O
(	O
summary	O
(	O
fit2	O
)	O
)	O
orthogonal	B
polynomial	O
-1.84	O
e	O
+02	O
(	O
intercept	B
)	O
poly	O
(	O
age	O
,	O
4	O
,	O
raw	O
=	O
t	O
)	O
1	O
2.12	O
e	O
+01	O
poly	O
(	O
age	O
,	O
4	O
,	O
raw	O
=	O
t	O
)	O
2	O
-5.64	O
e	O
-01	O
estimate	O
std	O
.	O
error	B
t	O
value	O
pr	O
(	O
>	O
|	O
t	O
|	O
)	O
-3.07	O
0.002180	O
3.61	O
0.000312	O
-2.74	O
0.006261	O
6.00	O
e	O
+01	O
5.89	O
e	O
+00	O
2.06	O
e	O
-01	O
7.8	O
lab	O
:	O
non-linear	B
modeling	O
289	O
poly	O
(	O
age	O
,	O
4	O
,	O
raw	O
=	O
t	O
)	O
3	O
6.81	O
e	O
-03	O
poly	O
(	O
age	O
,	O
4	O
,	O
raw	O
=	O
t	O
)	O
4	O
-3.20	O
e	O
-05	O
3.07	O
e	O
-03	O
1.64	O
e	O
-05	O
2.22	O
0.026398	O
-1.95	O
0.051039	O
there	O
are	O
several	O
other	O
equivalent	O
ways	O
of	O
ﬁtting	O
this	O
model	B
,	O
which	O
show-	O
case	O
the	O
ﬂexibility	O
of	O
the	O
formula	O
language	O
in	O
r.	O
for	O
example	O
>	O
fit2a	O
=	O
lm	O
(	O
wage∼age	O
+	O
i	O
(	O
age	O
^2	O
)	O
+	O
i	O
(	O
age	O
^3	O
)	O
+	O
i	O
(	O
age	O
^4	O
)	O
,	O
data	B
=	O
wage	O
)	O
>	O
coef	O
(	O
fit2a	O
)	O
(	O
intercept	B
)	O
-1.84	O
e	O
+02	O
age	O
2.12	O
e	O
+01	O
i	O
(	O
age	O
^2	O
)	O
-5.64	O
e	O
-01	O
i	O
(	O
age	O
^3	O
)	O
6.81	O
e	O
-03	O
i	O
(	O
age	O
^4	O
)	O
-3.20	O
e	O
-05	O
this	O
simply	O
creates	O
the	O
polynomial	B
basis	O
functions	O
on	O
the	O
ﬂy	O
,	O
taking	O
care	O
to	O
protect	O
terms	O
like	O
age^2	O
via	O
the	O
wrapper	B
function	O
i	O
(	O
)	O
(	O
the	O
^	O
symbol	O
has	O
wrapper	B
a	O
special	O
meaning	O
in	O
formulas	O
)	O
.	O
>	O
fit2b	O
=	O
lm	O
(	O
wage∼cbind	O
(	O
age	O
,	O
age	O
^2	O
,	O
age	O
^3	O
,	O
age	O
^4	O
)	O
,	O
data	B
=	O
wage	O
)	O
this	O
does	O
the	O
same	O
more	O
compactly	O
,	O
using	O
the	O
cbind	O
(	O
)	O
function	B
for	O
building	O
a	O
matrix	O
from	O
a	O
collection	O
of	O
vectors	O
;	O
any	O
function	B
call	O
such	O
as	O
cbind	O
(	O
)	O
inside	O
a	O
formula	O
also	O
serves	O
as	O
a	O
wrapper	B
.	O
we	O
now	O
create	O
a	O
grid	O
of	O
values	O
for	O
age	O
at	O
which	O
we	O
want	O
predictions	O
,	O
and	O
then	O
call	O
the	O
generic	O
predict	O
(	O
)	O
function	B
,	O
specifying	O
that	O
we	O
want	O
standard	O
errors	O
as	O
well	O
.	O
>	O
agelims	O
=	O
range	O
(	O
age	O
)	O
>	O
age	O
.	O
grid	O
=	O
seq	O
(	O
from	O
=	O
agelims	O
[	O
1	O
]	O
,	O
to	O
=	O
agelims	O
[	O
2	O
]	O
)	O
>	O
preds	O
=	O
predict	O
(	O
fit	O
,	O
newdata	O
=	O
list	O
(	O
age	O
=	O
age	O
.	O
grid	O
)	O
,	O
se	O
=	O
true	O
)	O
>	O
se	O
.	O
bands	O
=	O
cbind	O
(	O
preds	O
$	O
fit	O
+2*	O
preds	O
$	O
se	O
.	O
fit	O
,	O
preds	O
$	O
fit	O
-2*	O
preds	O
$	O
se	O
.	O
fit	O
)	O
finally	O
,	O
we	O
plot	B
the	O
data	B
and	O
add	O
the	O
ﬁt	B
from	O
the	O
degree-4	O
polynomial	B
.	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
1	O
,2	O
)	O
,	O
mar	O
=	O
c	O
(	O
4.5	O
,4.5	O
,1	O
,1	O
)	O
,	O
oma	O
=	O
c	O
(	O
0	O
,0	O
,4	O
,0	O
)	O
)	O
>	O
plot	B
(	O
age	O
,	O
wage	O
,	O
xlim	O
=	O
agelims	O
,	O
cex	O
=.5	O
,	O
col	O
=	O
''	O
darkgrey	O
``	O
)	O
>	O
title	O
(	O
``	O
degree	O
-4	O
polynomial	B
``	O
,	O
outer	O
=	O
t	O
)	O
>	O
lines	O
(	O
age	O
.	O
grid	O
,	O
preds	O
$	O
fit	O
,	O
lwd	O
=2	O
,	O
col	O
=	O
''	O
blue	O
``	O
)	O
>	O
matlines	O
(	O
age	O
.	O
grid	O
,	O
se	O
.	O
bands	O
,	O
lwd	O
=1	O
,	O
col	O
=	O
''	O
blue	O
``	O
,	O
lty	O
=3	O
)	O
here	O
the	O
mar	O
and	O
oma	O
arguments	O
to	O
par	O
(	O
)	O
allow	O
us	O
to	O
control	O
the	O
margins	O
of	O
the	O
plot	B
,	O
and	O
the	O
title	O
(	O
)	O
function	B
creates	O
a	O
ﬁgure	O
title	O
that	O
spans	O
both	O
subplots	O
.	O
we	O
mentioned	O
earlier	O
that	O
whether	O
or	O
not	O
an	O
orthogonal	B
set	O
of	O
basis	B
func-	O
tions	O
is	O
produced	O
in	O
the	O
poly	O
(	O
)	O
function	B
will	O
not	O
aﬀect	O
the	O
model	B
obtained	O
in	O
a	O
meaningful	O
way	O
.	O
what	O
do	O
we	O
mean	O
by	O
this	O
?	O
the	O
ﬁtted	O
values	O
obtained	O
in	O
either	O
case	O
are	O
identical	O
:	O
title	O
(	O
)	O
>	O
preds2	O
=	O
predict	O
(	O
fit2	O
,	O
newdata	O
=	O
list	O
(	O
age	O
=	O
age	O
.	O
grid	O
)	O
,	O
se	O
=	O
true	O
)	O
>	O
max	O
(	O
abs	O
(	O
preds	O
$	O
fit	O
-	O
preds2	O
$	O
fit	O
)	O
)	O
[	O
1	O
]	O
7.39	O
e	O
-13	O
in	O
performing	O
a	O
polynomial	B
regression	O
we	O
must	O
decide	O
on	O
the	O
degree	O
of	O
the	O
polynomial	B
to	O
use	O
.	O
one	O
way	O
to	O
do	O
this	O
is	O
by	O
using	O
hypothesis	B
tests	O
.	O
we	O
now	O
ﬁt	B
models	O
ranging	O
from	O
linear	B
to	O
a	O
degree-5	O
polynomial	B
and	O
seek	O
to	O
determine	O
the	O
simplest	O
model	B
which	O
is	O
suﬃcient	O
to	O
explain	O
the	O
relationship	O
290	O
7.	O
moving	O
beyond	O
linearity	O
anova	O
(	O
)	O
analysis	B
of	I
variance	I
between	O
wage	O
and	O
age	O
.	O
we	O
use	O
the	O
anova	O
(	O
)	O
function	B
,	O
which	O
performs	O
an	O
analysis	B
of	I
variance	I
(	O
anova	O
,	O
using	O
an	O
f-test	O
)	O
in	O
order	O
to	O
test	B
the	O
null	B
hypothesis	O
that	O
a	O
model	B
m1	O
is	O
suﬃcient	O
to	O
explain	O
the	O
data	B
against	O
the	O
alternative	B
hypothesis	I
that	O
a	O
more	O
complex	O
model	B
m2	O
is	O
required	O
.	O
in	O
order	O
to	O
use	O
the	O
anova	O
(	O
)	O
function	B
,	O
m1	O
and	O
m2	O
must	O
be	O
nested	O
models	O
:	O
the	O
predictors	O
in	O
m1	O
must	O
be	O
a	O
subset	O
of	O
the	O
predictors	O
in	O
m2	O
.	O
in	O
this	O
case	O
,	O
we	O
ﬁt	B
ﬁve	O
diﬀerent	O
models	O
and	O
sequentially	O
compare	O
the	O
simpler	O
model	B
to	O
the	O
more	O
complex	O
model	B
.	O
>	O
fit	O
.1=	O
lm	O
(	O
wage∼age	O
,	O
data	B
=	O
wage	O
)	O
>	O
fit	O
.2=	O
lm	O
(	O
wage∼poly	O
(	O
age	O
,2	O
)	O
,	O
data	B
=	O
wage	O
)	O
>	O
fit	O
.3=	O
lm	O
(	O
wage∼poly	O
(	O
age	O
,3	O
)	O
,	O
data	B
=	O
wage	O
)	O
>	O
fit	O
.4=	O
lm	O
(	O
wage∼poly	O
(	O
age	O
,4	O
)	O
,	O
data	B
=	O
wage	O
)	O
>	O
fit	O
.5=	O
lm	O
(	O
wage∼poly	O
(	O
age	O
,5	O
)	O
,	O
data	B
=	O
wage	O
)	O
>	O
anova	O
(	O
fit	O
.1	O
,	O
fit	O
.2	O
,	O
fit	O
.3	O
,	O
fit	O
.4	O
,	O
fit	O
.5	O
)	O
analysis	B
of	I
variance	I
table	O
model	B
1	O
:	O
wage	O
∼	O
age	O
model	B
2	O
:	O
wage	O
∼	O
poly	O
(	O
age	O
,	O
2	O
)	O
model	B
3	O
:	O
wage	O
∼	O
poly	O
(	O
age	O
,	O
3	O
)	O
model	B
4	O
:	O
wage	O
∼	O
poly	O
(	O
age	O
,	O
4	O
)	O
model	B
5	O
:	O
wage	O
∼	O
poly	O
(	O
age	O
,	O
5	O
)	O
res	O
.	O
df	O
rss	O
df	O
sum	O
of	O
sq	O
f	O
pr	O
(	O
>	O
f	O
)	O
2998	O
5022216	O
2997	O
4793430	O
2996	O
4777674	O
2995	O
4771604	O
2994	O
4770322	O
1	O
2	O
3	O
4	O
5	O
--	O
-	O
signif	O
.	O
codes	O
:	O
1	O
1	O
1	O
1	O
228786	O
143.59	O
<	O
2e	O
-16	O
***	O
15756	O
6070	O
1283	O
9.89	O
0.0017	O
**	O
3.81	O
0.0510	O
.	O
0.80	O
0.3697	O
0	O
’	O
***	O
’	O
0.001	O
’	O
**	O
’	O
0.01	O
’	O
*	O
’	O
0.05	O
’	O
.	O
’	O
0.1	O
’	O
’	O
1	O
the	O
p-value	B
comparing	O
the	O
linear	B
model	I
1	O
to	O
the	O
quadratic	B
model	O
2	O
is	O
−15	O
)	O
,	O
indicating	O
that	O
a	O
linear	B
ﬁt	O
is	O
not	O
suﬃcient	O
.	O
sim-	O
essentially	O
zero	O
(	O
<	O
10	O
ilarly	O
the	O
p-value	B
comparing	O
the	O
quadratic	B
model	O
2	O
to	O
the	O
cubic	B
model	O
3	O
is	O
very	O
low	O
(	O
0.0017	O
)	O
,	O
so	O
the	O
quadratic	B
ﬁt	O
is	O
also	O
insuﬃcient	O
.	O
the	O
p-value	B
comparing	O
the	O
cubic	B
and	O
degree-4	O
polynomials	O
,	O
model	B
3	O
and	O
model	B
4	O
,	O
is	O
ap-	O
proximately	O
5	O
%	O
while	O
the	O
degree-5	O
polynomial	B
model	O
5	O
seems	O
unnecessary	O
because	O
its	O
p-value	B
is	O
0.37.	O
hence	O
,	O
either	O
a	O
cubic	B
or	O
a	O
quartic	O
polynomial	B
appear	O
to	O
provide	O
a	O
reasonable	O
ﬁt	B
to	O
the	O
data	B
,	O
but	O
lower-	O
or	O
higher-order	O
models	O
are	O
not	O
justiﬁed	O
.	O
in	O
this	O
case	O
,	O
instead	O
of	O
using	O
the	O
anova	O
(	O
)	O
function	B
,	O
we	O
could	O
have	O
obtained	O
these	O
p-values	O
more	O
succinctly	O
by	O
exploiting	O
the	O
fact	O
that	O
poly	O
(	O
)	O
creates	O
orthogonal	B
polynomials	O
.	O
>	O
coef	O
(	O
summary	O
(	O
fit	O
.5	O
)	O
)	O
estimate	O
std	O
.	O
error	B
(	O
intercept	B
)	O
poly	O
(	O
age	O
,	O
5	O
)	O
1	O
poly	O
(	O
age	O
,	O
5	O
)	O
2	O
poly	O
(	O
age	O
,	O
5	O
)	O
3	O
111.70	O
447.07	O
-478.32	O
125.52	O
t	O
value	O
pr	O
(	O
>	O
|	O
t	O
|	O
)	O
0.7288	O
153.2780	O
0.000	O
e	O
+00	O
11.2002	O
1.491	O
e	O
-28	O
39.9161	O
39.9161	O
-11.9830	O
2.368	O
e	O
-32	O
39.9161	O
3.1446	O
1.679	O
e	O
-03	O
7.8	O
lab	O
:	O
non-linear	B
modeling	O
291	O
poly	O
(	O
age	O
,	O
5	O
)	O
4	O
poly	O
(	O
age	O
,	O
5	O
)	O
5	O
-77.91	O
-35.81	O
39.9161	O
39.9161	O
-1.9519	O
5.105	O
e	O
-02	O
-0.8972	O
3.697	O
e	O
-01	O
notice	O
that	O
the	O
p-values	O
are	O
the	O
same	O
,	O
and	O
in	O
fact	O
the	O
square	O
of	O
the	O
t-statistics	O
are	O
equal	O
to	O
the	O
f-statistics	O
from	O
the	O
anova	O
(	O
)	O
function	B
;	O
for	O
example	O
:	O
>	O
(	O
-11.983	O
)	O
^2	O
[	O
1	O
]	O
143.6	O
however	O
,	O
the	O
anova	O
method	O
works	O
whether	O
or	O
not	O
we	O
used	O
orthogonal	B
polynomials	O
;	O
it	O
also	O
works	O
when	O
we	O
have	O
other	O
terms	O
in	O
the	O
model	B
as	O
well	O
.	O
for	O
example	O
,	O
we	O
can	O
use	O
anova	O
(	O
)	O
to	O
compare	O
these	O
three	O
models	O
:	O
>	O
fit	O
.1=	O
lm	O
(	O
wage∼education	O
+	O
age	O
,	O
data	B
=	O
wage	O
)	O
>	O
fit	O
.2=	O
lm	O
(	O
wage∼education	O
+	O
poly	O
(	O
age	O
,2	O
)	O
,	O
data	B
=	O
wage	O
)	O
>	O
fit	O
.3=	O
lm	O
(	O
wage∼education	O
+	O
poly	O
(	O
age	O
,3	O
)	O
,	O
data	B
=	O
wage	O
)	O
>	O
anova	O
(	O
fit	O
.1	O
,	O
fit	O
.2	O
,	O
fit	O
.3	O
)	O
as	O
an	O
alternative	O
to	O
using	O
hypothesis	B
tests	O
and	O
anova	O
,	O
we	O
could	O
choose	O
the	O
polynomial	B
degree	O
using	O
cross-validation	B
,	O
as	O
discussed	O
in	O
chapter	O
5.	O
next	O
we	O
consider	O
the	O
task	O
of	O
predicting	O
whether	O
an	O
individual	O
earns	O
more	O
than	O
$	O
250,000	O
per	O
year	O
.	O
we	O
proceed	O
much	O
as	O
before	O
,	O
except	O
that	O
ﬁrst	O
we	O
create	O
the	O
appropriate	O
response	B
vector	O
,	O
and	O
then	O
apply	O
the	O
glm	O
(	O
)	O
function	B
using	O
family=	O
''	O
binomial	O
''	O
in	O
order	O
to	O
ﬁt	B
a	O
polynomial	B
logistic	O
regression	B
model	O
.	O
>	O
fit	O
=	O
glm	O
(	O
i	O
(	O
wage	O
>	O
250	O
)	O
∼poly	O
(	O
age	O
,4	O
)	O
,	O
data	B
=	O
wage	O
,	O
family	O
=	O
binomial	O
)	O
note	O
that	O
we	O
again	O
use	O
the	O
wrapper	B
i	O
(	O
)	O
to	O
create	O
this	O
binary	B
response	O
variable	B
on	O
the	O
ﬂy	O
.	O
the	O
expression	O
wage	O
>	O
250	O
evaluates	O
to	O
a	O
logical	O
variable	B
containing	O
trues	O
and	O
falses	O
,	O
which	O
glm	O
(	O
)	O
coerces	O
to	O
binary	B
by	O
setting	O
the	O
trues	O
to	O
1	O
and	O
the	O
falses	O
to	O
0.	O
once	O
again	O
,	O
we	O
make	O
predictions	O
using	O
the	O
predict	O
(	O
)	O
function	B
.	O
>	O
preds	O
=	O
predict	O
(	O
fit	O
,	O
newdata	O
=	O
list	O
(	O
age	O
=	O
age	O
.	O
grid	O
)	O
,	O
se	O
=	O
t	O
)	O
however	O
,	O
calculating	O
the	O
conﬁdence	O
intervals	O
is	O
slightly	O
more	O
involved	O
than	O
in	O
the	O
linear	B
regression	I
case	O
.	O
the	O
default	O
prediction	B
type	O
for	O
a	O
glm	O
(	O
)	O
model	B
is	O
type=	O
''	O
link	O
''	O
,	O
which	O
is	O
what	O
we	O
use	O
here	O
.	O
this	O
means	O
we	O
get	O
predictions	O
for	O
the	O
logit	B
:	O
that	O
is	O
,	O
we	O
have	O
ﬁt	B
a	O
model	B
of	O
the	O
form	O
(	O
cid:11	O
)	O
log	O
(	O
cid:12	O
)	O
pr	O
(	O
y	O
=	O
1|x	O
)	O
1	O
−	O
pr	O
(	O
y	O
=	O
1|x	O
)	O
=	O
xβ	O
,	O
and	O
the	O
predictions	O
given	O
are	O
of	O
the	O
form	O
x	O
ˆβ	O
.	O
the	O
standard	O
errors	O
given	O
are	O
also	O
of	O
this	O
form	O
.	O
in	O
order	O
to	O
obtain	O
conﬁdence	O
intervals	O
for	O
pr	O
(	O
y	O
=	O
1|x	O
)	O
,	O
we	O
use	O
the	O
transformation	O
pr	O
(	O
y	O
=	O
1|x	O
)	O
=	O
exp	O
(	O
xβ	O
)	O
1	O
+	O
exp	O
(	O
xβ	O
)	O
.	O
292	O
7.	O
moving	O
beyond	O
linearity	O
>	O
pfit	O
=	O
exp	O
(	O
preds	O
$	O
fit	O
)	O
/	O
(	O
1+	O
exp	O
(	O
preds	O
$	O
fit	O
)	O
)	O
>	O
se	O
.	O
bands	O
.	O
logit	B
=	O
cbind	O
(	O
preds	O
$	O
fit	O
+2*	O
preds	O
$	O
se	O
.	O
fit	O
,	O
preds	O
$	O
fit	O
-2*	O
preds	O
$	O
se	O
.	O
fit	O
)	O
>	O
se	O
.	O
bands	O
=	O
exp	O
(	O
se	O
.	O
bands	O
.	O
logit	B
)	O
/	O
(	O
1+	O
exp	O
(	O
se	O
.	O
bands	O
.	O
logit	B
)	O
)	O
note	O
that	O
we	O
could	O
have	O
directly	O
computed	O
the	O
probabilities	O
by	O
selecting	O
the	O
type=	O
''	O
response	B
''	O
option	O
in	O
the	O
predict	O
(	O
)	O
function	B
.	O
>	O
preds	O
=	O
predict	O
(	O
fit	O
,	O
newdata	O
=	O
list	O
(	O
age	O
=	O
age	O
.	O
grid	O
)	O
,	O
type	O
=	O
''	O
response	B
``	O
,	O
se	O
=	O
t	O
)	O
however	O
,	O
the	O
corresponding	O
conﬁdence	O
intervals	O
would	O
not	O
have	O
been	O
sen-	O
sible	O
because	O
we	O
would	O
end	O
up	O
with	O
negative	O
probabilities	O
!	O
finally	O
,	O
the	O
right-hand	O
plot	B
from	O
figure	O
7.1	O
was	O
made	O
as	O
follows	O
:	O
>	O
plot	B
(	O
age	O
,	O
i	O
(	O
wage	O
>	O
250	O
)	O
,	O
xlim	O
=	O
agelims	O
,	O
type	O
=	O
''	O
n	O
``	O
,	O
ylim	O
=	O
c	O
(	O
0	O
,	O
.2	O
)	O
)	O
>	O
points	O
(	O
jitter	O
(	O
age	O
)	O
,	O
i	O
(	O
(	O
wage	O
>	O
250	O
)	O
/5	O
)	O
,	O
cex	O
=.5	O
,	O
pch	O
=	O
''	O
|	O
''	O
,	O
col	O
=	O
''	O
darkgrey	O
``	O
)	O
>	O
lines	O
(	O
age	O
.	O
grid	O
,	O
pfit	O
,	O
lwd	O
=2	O
,	O
col	O
=	O
''	O
blue	O
``	O
)	O
>	O
matlines	O
(	O
age	O
.	O
grid	O
,	O
se	O
.	O
bands	O
,	O
lwd	O
=1	O
,	O
col	O
=	O
''	O
blue	O
``	O
,	O
lty	O
=3	O
)	O
we	O
have	O
drawn	O
the	O
age	O
values	O
corresponding	O
to	O
the	O
observations	B
with	O
wage	O
values	O
above	O
250	O
as	O
gray	O
marks	O
on	O
the	O
top	O
of	O
the	O
plot	B
,	O
and	O
those	O
with	O
wage	O
values	O
below	O
250	O
are	O
shown	O
as	O
gray	O
marks	O
on	O
the	O
bottom	O
of	O
the	O
plot	B
.	O
we	O
used	O
the	O
jitter	O
(	O
)	O
function	B
to	O
jitter	O
the	O
age	O
values	O
a	O
bit	O
so	O
that	O
observations	B
with	O
the	O
same	O
age	O
value	O
do	O
not	O
cover	O
each	O
other	O
up	O
.	O
this	O
is	O
often	O
called	O
a	O
rug	B
plot	I
.	O
in	O
order	O
to	O
ﬁt	B
a	O
step	B
function	I
,	O
as	O
discussed	O
in	O
section	O
7.2	O
,	O
we	O
use	O
the	O
cut	O
(	O
)	O
function	B
.	O
jitter	O
(	O
)	O
rug	B
plot	I
cut	O
(	O
)	O
>	O
table	O
(	O
cut	O
(	O
age	O
,4	O
)	O
)	O
(	O
17.9	O
,33.5	O
]	O
750	O
(	O
33.5	O
,49	O
]	O
1399	O
>	O
fit	O
=	O
lm	O
(	O
wage∼cut	O
(	O
age	O
,4	O
)	O
,	O
data	B
=	O
wage	O
)	O
(	O
49	O
,64.5	O
]	O
(	O
64.5	O
,80.1	O
]	O
72	O
779	O
>	O
coef	O
(	O
summary	O
(	O
fit	O
)	O
)	O
(	O
intercept	B
)	O
cut	O
(	O
age	O
,	O
4	O
)	O
(	O
33.5	O
,49	O
]	O
cut	O
(	O
age	O
,	O
4	O
)	O
(	O
49	O
,64.5	O
]	O
cut	O
(	O
age	O
,	O
4	O
)	O
(	O
64.5	O
,80.1	O
]	O
estimate	O
std	O
.	O
error	B
t	O
value	O
pr	O
(	O
>	O
|	O
t	O
|	O
)	O
63.79	O
0.00	O
e	O
+00	O
13.15	O
1.98	O
e	O
-38	O
11.44	O
1.04	O
e	O
-29	O
1.53	O
1.26	O
e	O
-01	O
94.16	O
24.05	O
23.66	O
7.64	O
1.48	O
1.83	O
2.07	O
4.99	O
here	O
cut	O
(	O
)	O
automatically	O
picked	O
the	O
cutpoints	O
at	O
33.5	O
,	O
49	O
,	O
and	O
64.5	O
years	O
of	O
age	O
.	O
we	O
could	O
also	O
have	O
speciﬁed	O
our	O
own	O
cutpoints	O
directly	O
using	O
the	O
breaks	O
option	O
.	O
the	O
function	B
cut	O
(	O
)	O
returns	O
an	O
ordered	B
categorical	I
variable	I
;	O
the	O
lm	O
(	O
)	O
function	B
then	O
creates	O
a	O
set	B
of	O
dummy	B
variables	O
for	O
use	O
in	O
the	O
re-	O
gression	O
.	O
the	O
age	O
<	O
33.5	O
category	O
is	O
left	O
out	O
,	O
so	O
the	O
intercept	B
coeﬃcient	O
of	O
$	O
94,160	O
can	O
be	O
interpreted	O
as	O
the	O
average	B
salary	O
for	O
those	O
under	O
33.5	O
years	O
of	O
age	O
,	O
and	O
the	O
other	O
coeﬃcients	O
can	O
be	O
interpreted	O
as	O
the	O
average	B
addi-	O
tional	O
salary	O
for	O
those	O
in	O
the	O
other	O
age	O
groups	O
.	O
we	O
can	O
produce	O
predictions	O
and	O
plots	O
just	O
as	O
we	O
did	O
in	O
the	O
case	O
of	O
the	O
polynomial	B
ﬁt	O
.	O
7.8	O
lab	O
:	O
non-linear	B
modeling	O
293	O
7.8.2	O
splines	O
in	O
order	O
to	O
ﬁt	B
regression	O
splines	O
in	O
r	O
,	O
we	O
use	O
the	O
splines	O
library	O
.	O
in	O
section	O
7.4	O
,	O
we	O
saw	O
that	O
regression	B
splines	O
can	O
be	O
ﬁt	B
by	O
constructing	O
an	O
appropriate	O
matrix	O
of	O
basis	B
functions	O
.	O
the	O
bs	O
(	O
)	O
function	B
generates	O
the	O
entire	O
matrix	O
of	O
basis	B
functions	O
for	O
splines	O
with	O
the	O
speciﬁed	O
set	B
of	O
knots	O
.	O
by	O
default	O
,	O
cubic	B
splines	O
are	O
produced	O
.	O
fitting	O
wage	O
to	O
age	O
using	O
a	O
regression	B
spline	O
is	O
simple	B
:	O
bs	O
(	O
)	O
>	O
library	O
(	O
splines	O
)	O
>	O
fit	O
=	O
lm	O
(	O
wage∼bs	O
(	O
age	O
,	O
knots	O
=	O
c	O
(	O
25	O
,40	O
,60	O
)	O
)	O
,	O
data	B
=	O
wage	O
)	O
>	O
pred	O
=	O
predict	O
(	O
fit	O
,	O
newdata	O
=	O
list	O
(	O
age	O
=	O
age	O
.	O
grid	O
)	O
,	O
se	O
=	O
t	O
)	O
>	O
plot	B
(	O
age	O
,	O
wage	O
,	O
col	O
=	O
''	O
gray	O
``	O
)	O
>	O
lines	O
(	O
age	O
.	O
grid	O
,	O
pred	O
$	O
fit	O
,	O
lwd	O
=2	O
)	O
>	O
lines	O
(	O
age	O
.	O
grid	O
,	O
pred	O
$	O
fit	O
+2*	O
pred	O
$	O
se	O
,	O
lty	O
=	O
''	O
dashed	O
``	O
)	O
>	O
lines	O
(	O
age	O
.	O
grid	O
,	O
pred	O
$	O
fit	O
-2*	O
pred	O
$	O
se	O
,	O
lty	O
=	O
''	O
dashed	O
``	O
)	O
here	O
we	O
have	O
prespeciﬁed	O
knots	O
at	O
ages	O
25	O
,	O
40	O
,	O
and	O
60.	O
this	O
produces	O
a	O
spline	B
with	O
six	O
basis	B
functions	O
.	O
(	O
recall	B
that	O
a	O
cubic	B
spline	O
with	O
three	O
knots	O
has	O
seven	O
degrees	B
of	I
freedom	I
;	O
these	O
degrees	B
of	I
freedom	I
are	O
used	O
up	O
by	O
an	O
intercept	B
,	O
plus	O
six	O
basis	B
functions	O
.	O
)	O
we	O
could	O
also	O
use	O
the	O
df	O
option	O
to	O
produce	O
a	O
spline	B
with	O
knots	O
at	O
uniform	O
quantiles	O
of	O
the	O
data	B
.	O
>	O
dim	O
(	O
bs	O
(	O
age	O
,	O
knots	O
=	O
c	O
(	O
25	O
,40	O
,60	O
)	O
)	O
)	O
[	O
1	O
]	O
3000	O
>	O
dim	O
(	O
bs	O
(	O
age	O
,	O
df	O
=6	O
)	O
)	O
[	O
1	O
]	O
3000	O
>	O
attr	O
(	O
bs	O
(	O
age	O
,	O
df	O
=6	O
)	O
,	O
''	O
knots	O
``	O
)	O
6	O
6	O
25	O
%	O
75	O
%	O
33.8	O
42.0	O
51.0	O
50	O
%	O
in	O
this	O
case	O
r	O
chooses	O
knots	O
at	O
ages	O
33.8	O
,	O
42.0	O
,	O
and	O
51.0	O
,	O
which	O
correspond	O
to	O
the	O
25th	O
,	O
50th	O
,	O
and	O
75th	O
percentiles	O
of	O
age	O
.	O
the	O
function	B
bs	O
(	O
)	O
also	O
has	O
a	O
degree	O
argument	B
,	O
so	O
we	O
can	O
ﬁt	B
splines	O
of	O
any	O
degree	O
,	O
rather	O
than	O
the	O
default	O
degree	O
of	O
3	O
(	O
which	O
yields	O
a	O
cubic	B
spline	O
)	O
.	O
in	O
order	O
to	O
instead	O
ﬁt	B
a	O
natural	B
spline	I
,	O
we	O
use	O
the	O
ns	O
(	O
)	O
function	B
.	O
here	O
we	O
ﬁt	B
a	O
natural	B
spline	I
with	O
four	O
degrees	B
of	I
freedom	I
.	O
>	O
fit2	O
=	O
lm	O
(	O
wage∼ns	O
(	O
age	O
,	O
df	O
=4	O
)	O
,	O
data	B
=	O
wage	O
)	O
>	O
pred2	O
=	O
predict	O
(	O
fit2	O
,	O
newdata	O
=	O
list	O
(	O
age	O
=	O
age	O
.	O
grid	O
)	O
,	O
se	O
=	O
t	O
)	O
>	O
lines	O
(	O
age	O
.	O
grid	O
,	O
pred2	O
$	O
fit	O
,	O
col	O
=	O
''	O
red	O
``	O
,	O
lwd	O
=2	O
)	O
ns	O
(	O
)	O
as	O
with	O
the	O
bs	O
(	O
)	O
function	B
,	O
we	O
could	O
instead	O
specify	O
the	O
knots	O
directly	O
using	O
the	O
knots	O
option	O
.	O
in	O
order	O
to	O
ﬁt	B
a	O
smoothing	B
spline	I
,	O
we	O
use	O
the	O
smooth.spline	O
(	O
)	O
function	B
.	O
figure	O
7.8	O
was	O
produced	O
with	O
the	O
following	O
code	O
:	O
smooth	O
.	O
spline	B
(	O
)	O
>	O
plot	B
(	O
age	O
,	O
wage	O
,	O
xlim	O
=	O
agelims	O
,	O
cex	O
=.5	O
,	O
col	O
=	O
''	O
darkgrey	O
``	O
)	O
>	O
title	O
(	O
``	O
smoothing	B
spline	I
``	O
)	O
>	O
fit	O
=	O
smooth	O
.	O
spline	B
(	O
age	O
,	O
wage	O
,	O
df	O
=16	O
)	O
>	O
fit2	O
=	O
smooth	O
.	O
spline	B
(	O
age	O
,	O
wage	O
,	O
cv	O
=	O
true	O
)	O
>	O
fit2	O
$	O
df	O
[	O
1	O
]	O
6.8	O
>	O
lines	O
(	O
fit	O
,	O
col	O
=	O
''	O
red	O
``	O
,	O
lwd	O
=2	O
)	O
294	O
7.	O
moving	O
beyond	O
linearity	O
>	O
lines	O
(	O
fit2	O
,	O
col	O
=	O
''	O
blue	O
``	O
,	O
lwd	O
=2	O
)	O
>	O
legend	O
(	O
``	O
topright	O
``	O
,	O
legend	O
=	O
c	O
(	O
``	O
16	O
df	O
``	O
,	O
''	O
6.8	O
df	O
``	O
)	O
,	O
col	O
=	O
c	O
(	O
``	O
red	O
``	O
,	O
''	O
blue	O
``	O
)	O
,	O
lty	O
=1	O
,	O
lwd	O
=2	O
,	O
cex	O
=.8	O
)	O
notice	O
that	O
in	O
the	O
ﬁrst	O
call	O
to	O
smooth.spline	O
(	O
)	O
,	O
we	O
speciﬁed	O
df=16	O
.	O
the	O
function	B
then	O
determines	O
which	O
value	O
of	O
λ	O
leads	O
to	O
16	O
degrees	B
of	I
freedom	I
.	O
in	O
the	O
second	O
call	O
to	O
smooth.spline	O
(	O
)	O
,	O
we	O
select	O
the	O
smoothness	O
level	B
by	O
cross-	O
validation	O
;	O
this	O
results	O
in	O
a	O
value	O
of	O
λ	O
that	O
yields	O
6.8	O
degrees	B
of	I
freedom	I
.	O
in	O
order	O
to	O
perform	O
local	B
regression	I
,	O
we	O
use	O
the	O
loess	O
(	O
)	O
function	B
.	O
loess	O
(	O
)	O
>	O
plot	B
(	O
age	O
,	O
wage	O
,	O
xlim	O
=	O
agelims	O
,	O
cex	O
=.5	O
,	O
col	O
=	O
''	O
darkgrey	O
``	O
)	O
>	O
title	O
(	O
``	O
local	B
regression	I
``	O
)	O
>	O
fit	O
=	O
loess	O
(	O
wage∼age	O
,	O
span	O
=.2	O
,	O
data	B
=	O
wage	O
)	O
>	O
fit2	O
=	O
loess	O
(	O
wage∼age	O
,	O
span	O
=.5	O
,	O
data	B
=	O
wage	O
)	O
>	O
lines	O
(	O
age	O
.	O
grid	O
,	O
predict	O
(	O
fit	O
,	O
data	B
.	O
frame	O
(	O
age	O
=	O
age	O
.	O
grid	O
)	O
)	O
,	O
col	O
=	O
''	O
red	O
``	O
,	O
lwd	O
=2	O
)	O
>	O
lines	O
(	O
age	O
.	O
grid	O
,	O
predict	O
(	O
fit2	O
,	O
data	B
.	O
frame	O
(	O
age	O
=	O
age	O
.	O
grid	O
)	O
)	O
,	O
col	O
=	O
''	O
blue	O
``	O
,	O
lwd	O
=2	O
)	O
>	O
legend	O
(	O
``	O
topright	O
``	O
,	O
legend	O
=	O
c	O
(	O
``	O
span	O
=0.2	O
''	O
,	O
''	O
span	O
=0.5	O
''	O
)	O
,	O
col	O
=	O
c	O
(	O
``	O
red	O
``	O
,	O
''	O
blue	O
``	O
)	O
,	O
lty	O
=1	O
,	O
lwd	O
=2	O
,	O
cex	O
=.8	O
)	O
here	O
we	O
have	O
performed	O
local	B
linear	O
regression	B
using	O
spans	O
of	O
0.2	O
and	O
0.5	O
:	O
that	O
is	O
,	O
each	O
neighborhood	O
consists	O
of	O
20	O
%	O
or	O
50	O
%	O
of	O
the	O
observations	B
.	O
the	O
larger	O
the	O
span	O
,	O
the	O
smoother	B
the	O
ﬁt	B
.	O
the	O
locfit	O
library	O
can	O
also	O
be	O
used	O
for	O
ﬁtting	O
local	B
regression	I
models	O
in	O
r.	O
7.8.3	O
gams	O
we	O
now	O
ﬁt	B
a	O
gam	O
to	O
predict	O
wage	O
using	O
natural	B
spline	I
functions	O
of	O
year	O
and	O
age	O
,	O
treating	O
education	O
as	O
a	O
qualitative	B
predictor	O
,	O
as	O
in	O
(	O
7.16	O
)	O
.	O
since	O
this	O
is	O
just	O
a	O
big	O
linear	B
regression	I
model	O
using	O
an	O
appropriate	O
choice	O
of	O
basis	B
functions	O
,	O
we	O
can	O
simply	O
do	O
this	O
using	O
the	O
lm	O
(	O
)	O
function	B
.	O
>	O
gam1	O
=	O
lm	O
(	O
wage∼ns	O
(	O
year	O
,4	O
)	O
+	O
ns	O
(	O
age	O
,5	O
)	O
+	O
education	O
,	O
data	B
=	O
wage	O
)	O
we	O
now	O
ﬁt	B
the	O
model	B
(	O
7.16	O
)	O
using	O
smoothing	B
splines	O
rather	O
than	O
natural	B
splines	O
.	O
in	O
order	O
to	O
ﬁt	B
more	O
general	O
sorts	O
of	O
gams	O
,	O
using	O
smoothing	B
splines	O
or	O
other	O
components	O
that	O
can	O
not	O
be	O
expressed	O
in	O
terms	O
of	O
basis	B
functions	O
and	O
then	O
ﬁt	B
using	O
least	B
squares	I
regression	O
,	O
we	O
will	O
need	O
to	O
use	O
the	O
gam	O
library	O
in	O
r.	O
the	O
s	O
(	O
)	O
function	B
,	O
which	O
is	O
part	O
of	O
the	O
gam	O
library	O
,	O
is	O
used	O
to	O
indicate	O
that	O
we	O
would	O
like	O
to	O
use	O
a	O
smoothing	B
spline	I
.	O
we	O
specify	O
that	O
the	O
function	B
of	O
year	O
should	O
have	O
4	O
degrees	B
of	I
freedom	I
,	O
and	O
that	O
the	O
function	B
of	O
age	O
will	O
have	O
5	O
degrees	B
of	I
freedom	I
.	O
since	O
education	O
is	O
qualitative	B
,	O
we	O
leave	O
it	O
as	O
is	O
,	O
and	O
it	O
is	O
converted	O
into	O
four	O
dummy	B
variables	O
.	O
we	O
use	O
the	O
gam	O
(	O
)	O
function	B
in	O
order	O
to	O
ﬁt	B
a	O
gam	O
using	O
these	O
components	O
.	O
all	O
of	O
the	O
terms	O
in	O
(	O
7.16	O
)	O
are	O
ﬁt	B
simultaneously	O
,	O
taking	O
each	O
other	O
into	O
account	O
to	O
explain	O
the	O
response	B
.	O
s	O
(	O
)	O
gam	O
(	O
)	O
>	O
library	O
(	O
gam	O
)	O
>	O
gam	O
.	O
m3	O
=	O
gam	O
(	O
wage∼s	O
(	O
year	O
,4	O
)	O
+	O
s	O
(	O
age	O
,5	O
)	O
+	O
education	O
,	O
data	B
=	O
wage	O
)	O
7.8	O
lab	O
:	O
non-linear	B
modeling	O
295	O
in	O
order	O
to	O
produce	O
figure	O
7.12	O
,	O
we	O
simply	O
call	O
the	O
plot	B
(	O
)	O
function	B
:	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
1	O
,3	O
)	O
)	O
>	O
plot	B
(	O
gam	O
.	O
m3	O
,	O
se	O
=	O
true	O
,	O
col	O
=	O
''	O
blue	O
``	O
)	O
the	O
generic	O
plot	B
(	O
)	O
function	B
recognizes	O
that	O
gam.m3	O
is	O
an	O
object	O
of	O
class	O
gam	O
,	O
and	O
invokes	O
the	O
appropriate	O
plot.gam	O
(	O
)	O
method	O
.	O
conveniently	O
,	O
even	O
though	O
gam1	O
is	O
not	O
of	O
class	O
gam	O
but	O
rather	O
of	O
class	O
lm	O
,	O
we	O
can	O
still	O
use	O
plot.gam	O
(	O
)	O
on	O
it	O
.	O
figure	O
7.11	O
was	O
produced	O
using	O
the	O
following	O
expression	O
:	O
plot.gam	O
(	O
)	O
>	O
plot	B
.	O
gam	O
(	O
gam1	O
,	O
se	O
=	O
true	O
,	O
col	O
=	O
''	O
red	O
``	O
)	O
notice	O
here	O
we	O
had	O
to	O
use	O
plot.gam	O
(	O
)	O
rather	O
than	O
the	O
generic	O
plot	B
(	O
)	O
function	B
.	O
in	O
these	O
plots	O
,	O
the	O
function	B
of	O
year	O
looks	O
rather	O
linear	B
.	O
we	O
can	O
perform	O
a	O
series	O
of	O
anova	O
tests	O
in	O
order	O
to	O
determine	O
which	O
of	O
these	O
three	O
models	O
is	O
best	O
:	O
a	O
gam	O
that	O
excludes	O
year	O
(	O
m1	O
)	O
,	O
a	O
gam	O
that	O
uses	O
a	O
linear	B
function	O
of	O
year	O
(	O
m2	O
)	O
,	O
or	O
a	O
gam	O
that	O
uses	O
a	O
spline	B
function	O
of	O
year	O
(	O
m3	O
)	O
.	O
>	O
gam	O
.	O
m1	O
=	O
gam	O
(	O
wage∼s	O
(	O
age	O
,5	O
)	O
+	O
education	O
,	O
data	B
=	O
wage	O
)	O
>	O
gam	O
.	O
m2	O
=	O
gam	O
(	O
wage∼year	O
+	O
s	O
(	O
age	O
,5	O
)	O
+	O
education	O
,	O
data	B
=	O
wage	O
)	O
>	O
anova	O
(	O
gam	O
.	O
m1	O
,	O
gam	O
.	O
m2	O
,	O
gam	O
.	O
m3	O
,	O
test	B
=	O
''	O
f	O
``	O
)	O
analysis	O
of	O
deviance	O
table	O
model	B
1	O
:	O
wage	O
∼	O
s	O
(	O
age	O
,	O
5	O
)	O
+	O
education	O
model	B
2	O
:	O
wage	O
∼	O
year	O
+	O
s	O
(	O
age	O
,	O
5	O
)	O
+	O
education	O
model	B
3	O
:	O
wage	O
∼	O
s	O
(	O
year	O
,	O
4	O
)	O
+	O
s	O
(	O
age	O
,	O
5	O
)	O
+	O
education	O
resid	O
.	O
df	O
resid	O
.	O
dev	O
df	O
deviance	B
f	O
pr	O
(	O
>	O
f	O
)	O
2990	O
2989	O
2986	O
1	O
2	O
3	O
--	O
-	O
signif	O
.	O
codes	O
:	O
3711730	O
3693841	O
3689770	O
1	O
3	O
17889	O
14.5	O
0.00014	O
***	O
4071	O
1.1	O
0.34857	O
0	O
’	O
***	O
’	O
0.001	O
’	O
**	O
’	O
0.01	O
’	O
*	O
’	O
0.05	O
’	O
.	O
’	O
0.1	O
’	O
’	O
1	O
we	O
ﬁnd	O
that	O
there	O
is	O
compelling	O
evidence	O
that	O
a	O
gam	O
with	O
a	O
linear	B
func-	O
tion	O
of	O
year	O
is	O
better	O
than	O
a	O
gam	O
that	O
does	O
not	O
include	O
year	O
at	O
all	O
(	O
p-value	B
=	O
0.00014	O
)	O
.	O
however	O
,	O
there	O
is	O
no	O
evidence	O
that	O
a	O
non-linear	B
func-	O
tion	O
of	O
year	O
is	O
needed	O
(	O
p-value	B
=	O
0.349	O
)	O
.	O
in	O
other	O
words	O
,	O
based	O
on	O
the	O
results	O
of	O
this	O
anova	O
,	O
m2	O
is	O
preferred	O
.	O
the	O
summary	O
(	O
)	O
function	B
produces	O
a	O
summary	O
of	O
the	O
gam	O
ﬁt	B
.	O
>	O
summary	O
(	O
gam	O
.	O
m3	O
)	O
call	O
:	O
gam	O
(	O
formula	O
=	O
wage	O
∼	O
s	O
(	O
year	O
,	O
4	O
)	O
+	O
s	O
(	O
age	O
,	O
5	O
)	O
+	O
education	O
,	O
data	B
=	O
wage	O
)	O
deviance	B
residuals	O
:	O
min	O
-119.43	O
1	O
q	O
-19.70	O
median	O
-3.33	O
3	O
q	O
14.17	O
max	O
213.48	O
(	O
dispersion	O
parameter	B
for	O
gaussian	O
family	O
taken	O
to	O
be	O
1236	O
)	O
null	B
deviance	O
:	O
5222086	O
on	O
2999	O
degrees	B
of	I
freedom	I
residual	O
deviance	B
:	O
3689770	O
on	O
2986	O
degrees	B
of	I
freedom	I
296	O
7.	O
moving	O
beyond	O
linearity	O
aic	O
:	O
29888	O
number	O
of	O
local	B
scoring	O
iterations	O
:	O
2	O
df	O
for	O
terms	O
and	O
f	O
-	O
values	O
for	O
n	O
o	O
n	O
p	O
a	O
r	O
a	O
m	O
e	O
t	O
r	O
i	O
c	O
effects	O
df	O
npar	O
df	O
npar	O
f	O
pr	O
(	O
f	O
)	O
(	O
intercept	B
)	O
s	O
(	O
year	O
,	O
4	O
)	O
s	O
(	O
age	O
,	O
5	O
)	O
education	O
--	O
-	O
signif	O
.	O
codes	O
:	O
1	O
1	O
1	O
4	O
3	O
4	O
1.1	O
0.35	O
32.4	O
<	O
2e	O
-16	O
***	O
0	O
’	O
***	O
’	O
0.001	O
’	O
**	O
’	O
0.01	O
’	O
*	O
’	O
0.05	O
’	O
.	O
’	O
0.1	O
’	O
’	O
1	O
the	O
p-values	O
for	O
year	O
and	O
age	O
correspond	O
to	O
a	O
null	B
hypothesis	O
of	O
a	O
linear	B
relationship	O
versus	O
the	O
alternative	O
of	O
a	O
non-linear	B
relationship	O
.	O
the	O
large	O
p-value	B
for	O
year	O
reinforces	O
our	O
conclusion	O
from	O
the	O
anova	O
test	B
that	O
a	O
lin-	O
ear	O
function	B
is	O
adequate	O
for	O
this	O
term	B
.	O
however	O
,	O
there	O
is	O
very	O
clear	O
evidence	O
that	O
a	O
non-linear	B
term	O
is	O
required	O
for	O
age	O
.	O
we	O
can	O
make	O
predictions	O
from	O
gam	O
objects	O
,	O
just	O
like	O
from	O
lm	O
objects	O
,	O
using	O
the	O
predict	O
(	O
)	O
method	O
for	O
the	O
class	O
gam	O
.	O
here	O
we	O
make	O
predictions	O
on	O
the	O
training	B
set	O
.	O
>	O
preds	O
=	O
predict	O
(	O
gam	O
.	O
m2	O
,	O
newdata	O
=	O
wage	O
)	O
we	O
can	O
also	O
use	O
local	B
regression	I
ﬁts	O
as	O
building	O
blocks	O
in	O
a	O
gam	O
,	O
using	O
the	O
lo	O
(	O
)	O
function	B
.	O
>	O
gam	O
.	O
lo	O
=	O
gam	O
(	O
wage∼s	O
(	O
year	O
,	O
df	O
=4	O
)	O
+	O
lo	O
(	O
age	O
,	O
span	O
=0.7	O
)	O
+	O
education	O
,	O
lo	O
(	O
)	O
data	B
=	O
wage	O
)	O
>	O
plot	B
.	O
gam	O
(	O
gam	O
.	O
lo	O
,	O
se	O
=	O
true	O
,	O
col	O
=	O
''	O
green	O
``	O
)	O
here	O
we	O
have	O
used	O
local	B
regression	I
for	O
the	O
age	O
term	B
,	O
with	O
a	O
span	O
of	O
0.7.	O
we	O
can	O
also	O
use	O
the	O
lo	O
(	O
)	O
function	B
to	O
create	O
interactions	O
before	O
calling	O
the	O
gam	O
(	O
)	O
function	B
.	O
for	O
example	O
,	O
>	O
gam	O
.	O
lo	O
.	O
i	O
=	O
gam	O
(	O
wage∼lo	O
(	O
year	O
,	O
age	O
,	O
span	O
=0.5	O
)	O
+	O
education	O
,	O
data	B
=	O
wage	O
)	O
ﬁts	O
a	O
two-term	O
model	B
,	O
in	O
which	O
the	O
ﬁrst	O
term	B
is	O
an	O
interaction	B
between	O
year	O
and	O
age	O
,	O
ﬁt	B
by	O
a	O
local	B
regression	I
surface	O
.	O
we	O
can	O
plot	B
the	O
resulting	O
two-dimensional	O
surface	O
if	O
we	O
ﬁrst	O
install	O
the	O
akima	O
package	O
.	O
>	O
library	O
(	O
akima	O
)	O
>	O
plot	B
(	O
gam	O
.	O
lo	O
.	O
i	O
)	O
in	O
order	O
to	O
ﬁt	B
a	O
logistic	B
regression	I
gam	O
,	O
we	O
once	O
again	O
use	O
the	O
i	O
(	O
)	O
func-	O
tion	O
in	O
constructing	O
the	O
binary	B
response	O
variable	B
,	O
and	O
set	B
family=binomial	O
.	O
>	O
gam	O
.	O
lr	O
=	O
gam	O
(	O
i	O
(	O
wage	O
>	O
250	O
)	O
∼year	O
+	O
s	O
(	O
age	O
,	O
df	O
=5	O
)	O
+	O
education	O
,	O
family	O
=	O
binomial	O
,	O
data	B
=	O
wage	O
)	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
1	O
,3	O
)	O
)	O
>	O
plot	B
(	O
gam	O
.	O
lr	O
,	O
se	O
=t	O
,	O
col	O
=	O
''	O
green	O
``	O
)	O
it	O
is	O
easy	O
to	O
see	O
that	O
there	O
are	O
no	O
high	O
earners	O
in	O
the	O
<	O
hs	O
category	O
:	O
7.9	O
exercises	O
297	O
>	O
table	O
(	O
education	O
,	O
i	O
(	O
wage	O
>	O
250	O
)	O
)	O
education	O
1	O
.	O
<	O
hs	O
grad	O
2.	O
hs	O
grad	O
3.	O
some	O
college	O
4.	O
college	O
grad	O
5.	O
advanced	O
degree	O
false	O
true	O
0	O
5	O
7	O
22	O
45	O
268	O
966	O
643	O
663	O
381	O
hence	O
,	O
we	O
ﬁt	B
a	O
logistic	B
regression	I
gam	O
using	O
all	O
but	O
this	O
category	O
.	O
this	O
provides	O
more	O
sensible	O
results	O
.	O
>	O
gam	O
.	O
lr	O
.	O
s	O
=	O
gam	O
(	O
i	O
(	O
wage	O
>	O
250	O
)	O
∼year	O
+	O
s	O
(	O
age	O
,	O
df	O
=5	O
)	O
+	O
education	O
,	O
family	O
=	O
binomial	O
,	O
data	B
=	O
wage	O
,	O
subset	O
=	O
(	O
education	O
!	O
=	O
''	O
1	O
.	O
<	O
hs	O
grad	O
``	O
)	O
)	O
>	O
plot	B
(	O
gam	O
.	O
lr	O
.s	O
,	O
se	O
=t	O
,	O
col	O
=	O
''	O
green	O
``	O
)	O
7.9	O
exercises	O
conceptual	O
1.	O
it	O
was	O
mentioned	O
in	O
the	O
chapter	O
that	O
a	O
cubic	B
regression	O
spline	B
with	O
one	O
knot	B
at	O
ξ	O
can	O
be	O
obtained	O
using	O
a	O
basis	B
of	O
the	O
form	O
x	O
,	O
x2	B
,	O
x3	O
,	O
(	O
x	O
−	O
ξ	O
)	O
3	O
+	O
=	O
(	O
x	O
−	O
ξ	O
)	O
3	O
if	O
x	O
>	O
ξ	O
and	O
equals	O
0	O
otherwise	O
.	O
we	O
will	O
now	O
show	O
that	O
a	O
function	B
of	O
the	O
form	O
+	O
,	O
where	O
(	O
x	O
−	O
ξ	O
)	O
3	O
f	O
(	O
x	O
)	O
=	O
β0	O
+	O
β1x	O
+	O
β2x2	O
+	O
β3x3	O
+	O
β4	O
(	O
x	O
−	O
ξ	O
)	O
3	O
+	O
is	O
indeed	O
a	O
cubic	B
regression	O
spline	B
,	O
regardless	O
of	O
the	O
values	O
of	O
β0	O
,	O
β1	O
,	O
β2	O
,	O
β3	O
,	O
β4	O
.	O
(	O
a	O
)	O
find	O
a	O
cubic	B
polynomial	O
f1	O
(	O
x	O
)	O
=	O
a1	O
+	O
b1x	O
+	O
c1x2	O
+	O
d1x3	O
such	O
that	O
f	O
(	O
x	O
)	O
=	O
f1	O
(	O
x	O
)	O
for	O
all	O
x	O
≤	O
ξ.	O
express	O
a1	O
,	O
b1	O
,	O
c1	O
,	O
d1	O
in	O
terms	O
of	O
β0	O
,	O
β1	O
,	O
β2	O
,	O
β3	O
,	O
β4	O
.	O
(	O
b	O
)	O
find	O
a	O
cubic	B
polynomial	O
f2	O
(	O
x	O
)	O
=	O
a2	O
+	O
b2x	O
+	O
c2x2	O
+	O
d2x3	O
such	O
that	O
f	O
(	O
x	O
)	O
=	O
f2	O
(	O
x	O
)	O
for	O
all	O
x	O
>	O
ξ.	O
express	O
a2	O
,	O
b2	O
,	O
c2	O
,	O
d2	O
in	O
terms	O
of	O
β0	O
,	O
β1	O
,	O
β2	O
,	O
β3	O
,	O
β4	O
.	O
we	O
have	O
now	O
established	O
that	O
f	O
(	O
x	O
)	O
is	O
a	O
piecewise	B
polynomial	I
.	O
(	O
c	O
)	O
show	O
that	O
f1	O
(	O
ξ	O
)	O
=	O
f2	O
(	O
ξ	O
)	O
.	O
that	O
is	O
,	O
f	O
(	O
x	O
)	O
is	O
continuous	B
at	O
ξ	O
.	O
(	O
d	O
)	O
show	O
that	O
f	O
(	O
x	O
)	O
is	O
continuous	B
at	O
ξ	O
.	O
(	O
cid:5	O
)	O
2	O
(	O
ξ	O
)	O
.	O
that	O
is	O
,	O
f	O
(	O
cid:5	O
)	O
1	O
(	O
ξ	O
)	O
=	O
f	O
(	O
cid:5	O
)	O
298	O
7.	O
moving	O
beyond	O
linearity	O
(	O
e	O
)	O
show	O
that	O
f	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
1	O
(	O
ξ	O
)	O
=	O
f	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
2	O
(	O
ξ	O
)	O
.	O
that	O
is	O
,	O
f	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
(	O
x	O
)	O
is	O
continuous	B
at	O
ξ.	O
therefore	O
,	O
f	O
(	O
x	O
)	O
is	O
indeed	O
a	O
cubic	B
spline	O
.	O
hint	O
:	O
parts	O
(	O
d	O
)	O
and	O
(	O
e	O
)	O
of	O
this	O
problem	O
require	O
knowledge	O
of	O
single-	O
variable	B
calculus	O
.	O
as	O
a	O
reminder	O
,	O
given	O
a	O
cubic	B
polynomial	O
f1	O
(	O
x	O
)	O
=	O
a1	O
+	O
b1x	O
+	O
c1x2	O
+	O
d1x3	O
,	O
the	O
ﬁrst	O
derivative	B
takes	O
the	O
form	O
(	O
cid:5	O
)	O
1	O
(	O
x	O
)	O
=	O
b1	O
+	O
2c1x	O
+	O
3d1x2	O
f	O
and	O
the	O
second	O
derivative	B
takes	O
the	O
form	O
(	O
cid:5	O
)	O
(	O
cid:5	O
)	O
1	O
(	O
x	O
)	O
=	O
2c1	O
+	O
6d1x	O
.	O
f	O
2.	O
suppose	O
that	O
a	O
curve	O
ˆg	O
is	O
computed	O
to	O
smoothly	O
ﬁt	B
a	O
set	B
of	O
n	O
points	O
using	O
the	O
following	O
formula	O
:	O
(	O
cid:31	O
)	O
n	O
(	O
cid:17	O
)	O
ˆg	O
=	O
arg	O
min	O
g	O
i=1	O
(	O
yi	O
−	O
g	O
(	O
xi	O
)	O
)	O
2	O
+	O
λ	O
-	O
(	O
cid:24	O
)	O
(	O
cid:25	O
)	O
2	O
g	O
(	O
m	O
)	O
(	O
x	O
)	O
dx	O
,	O
where	O
g	O
(	O
m	O
)	O
represents	O
the	O
mth	O
derivative	B
of	O
g	O
(	O
and	O
g	O
(	O
0	O
)	O
=	O
g	O
)	O
.	O
provide	O
example	O
sketches	O
of	O
ˆg	O
in	O
each	O
of	O
the	O
following	O
scenarios	O
.	O
(	O
a	O
)	O
λ	O
=	O
∞	O
,	O
m	O
=	O
0	O
.	O
(	O
b	O
)	O
λ	O
=	O
∞	O
,	O
m	O
=	O
1	O
.	O
(	O
c	O
)	O
λ	O
=	O
∞	O
,	O
m	O
=	O
2	O
.	O
(	O
d	O
)	O
λ	O
=	O
∞	O
,	O
m	O
=	O
3	O
.	O
(	O
e	O
)	O
λ	O
=	O
0	O
,	O
m	O
=	O
3	O
.	O
3.	O
suppose	O
we	O
ﬁt	B
a	O
curve	O
with	O
basis	B
functions	O
b1	O
(	O
x	O
)	O
=	O
x	O
,	O
b2	O
(	O
x	O
)	O
=	O
(	O
x	O
−	O
1	O
)	O
2i	O
(	O
x	O
≥	O
1	O
)	O
.	O
(	O
note	O
that	O
i	O
(	O
x	O
≥	O
1	O
)	O
equals	O
1	O
for	O
x	O
≥	O
1	O
and	O
0	O
otherwise	O
.	O
)	O
we	O
ﬁt	B
the	O
linear	B
regression	I
model	O
y	O
=	O
β0	O
+	O
β1b1	O
(	O
x	O
)	O
+	O
β2b2	O
(	O
x	O
)	O
+	O
	O
,	O
and	O
obtain	O
coeﬃcient	B
estimates	O
ˆβ0	O
=	O
1	O
,	O
ˆβ1	O
=	O
1	O
,	O
ˆβ2	O
=	O
−2	O
.	O
sketch	O
the	O
estimated	O
curve	O
between	O
x	O
=	O
−2	O
and	O
x	O
=	O
2.	O
note	O
the	O
intercepts	O
,	O
slopes	O
,	O
and	O
other	O
relevant	O
information	O
.	O
4.	O
suppose	O
we	O
ﬁt	B
a	O
curve	O
with	O
basis	B
functions	O
b1	O
(	O
x	O
)	O
=	O
i	O
(	O
0	O
≤	O
x	O
≤	O
2	O
)	O
−	O
(	O
x	O
−	O
1	O
)	O
i	O
(	O
1	O
≤	O
x	O
≤	O
2	O
)	O
,	O
b2	O
(	O
x	O
)	O
=	O
(	O
x	O
−	O
3	O
)	O
i	O
(	O
3	O
≤	O
x	O
≤	O
4	O
)	O
+	O
i	O
(	O
4	O
<	O
x	O
≤	O
5	O
)	O
.	O
we	O
ﬁt	B
the	O
linear	B
regression	I
model	O
y	O
=	O
β0	O
+	O
β1b1	O
(	O
x	O
)	O
+	O
β2b2	O
(	O
x	O
)	O
+	O
	O
,	O
and	O
obtain	O
coeﬃcient	B
estimates	O
ˆβ0	O
=	O
1	O
,	O
ˆβ1	O
=	O
1	O
,	O
ˆβ2	O
=	O
3.	O
sketch	O
the	O
estimated	O
curve	O
between	O
x	O
=	O
−2	O
and	O
x	O
=	O
2.	O
note	O
the	O
intercepts	O
,	O
slopes	O
,	O
and	O
other	O
relevant	O
information	O
.	O
5.	O
consider	O
two	O
curves	O
,	O
ˆg1	O
and	O
ˆg2	O
,	O
deﬁned	O
by	O
(	O
yi	O
−	O
g	O
(	O
xi	O
)	O
)	O
2	O
+	O
λ	O
ˆg1	O
=	O
arg	O
min	O
n	O
(	O
cid:17	O
)	O
(	O
cid:31	O
)	O
g	O
(	O
cid:31	O
)	O
i=1	O
n	O
(	O
cid:17	O
)	O
ˆg2	O
=	O
arg	O
min	O
g	O
i=1	O
(	O
yi	O
−	O
g	O
(	O
xi	O
)	O
)	O
2	O
+	O
λ	O
7.9	O
exercises	O
299	O
-	O
(	O
cid:24	O
)	O
-	O
(	O
cid:24	O
)	O
(	O
cid:25	O
)	O
2	O
g	O
(	O
3	O
)	O
(	O
x	O
)	O
(	O
cid:25	O
)	O
2	O
g	O
(	O
4	O
)	O
(	O
x	O
)	O
dx	O
dx	O
,	O
,	O
where	O
g	O
(	O
m	O
)	O
represents	O
the	O
mth	O
derivative	B
of	O
g.	O
(	O
a	O
)	O
as	O
λ	O
→	O
∞	O
,	O
will	O
ˆg1	O
or	O
ˆg2	O
have	O
the	O
smaller	O
training	B
rss	O
?	O
(	O
b	O
)	O
as	O
λ	O
→	O
∞	O
,	O
will	O
ˆg1	O
or	O
ˆg2	O
have	O
the	O
smaller	O
test	B
rss	O
?	O
(	O
c	O
)	O
for	O
λ	O
=	O
0	O
,	O
will	O
ˆg1	O
or	O
ˆg2	O
have	O
the	O
smaller	O
training	B
and	O
test	B
rss	O
?	O
applied	O
6.	O
in	O
this	O
exercise	O
,	O
you	O
will	O
further	O
analyze	O
the	O
wage	O
data	B
set	O
considered	O
throughout	O
this	O
chapter	O
.	O
(	O
a	O
)	O
perform	O
polynomial	B
regression	O
to	O
predict	O
wage	O
using	O
age	O
.	O
use	O
cross-validation	B
to	O
select	O
the	O
optimal	O
degree	O
d	O
for	O
the	O
polyno-	O
mial	O
.	O
what	O
degree	O
was	O
chosen	O
,	O
and	O
how	O
does	O
this	O
compare	O
to	O
the	O
results	O
of	O
hypothesis	B
testing	O
using	O
anova	O
?	O
make	O
a	O
plot	B
of	O
the	O
resulting	O
polynomial	B
ﬁt	O
to	O
the	O
data	B
.	O
(	O
b	O
)	O
fit	O
a	O
step	B
function	I
to	O
predict	O
wage	O
using	O
age	O
,	O
and	O
perform	O
cross-	O
validation	O
to	O
choose	O
the	O
optimal	O
number	O
of	O
cuts	O
.	O
make	O
a	O
plot	B
of	O
the	O
ﬁt	B
obtained	O
.	O
7.	O
the	O
wage	O
data	B
set	O
contains	O
a	O
number	O
of	O
other	O
features	O
not	O
explored	O
in	O
this	O
chapter	O
,	O
such	O
as	O
marital	O
status	O
(	O
maritl	O
)	O
,	O
job	O
class	O
(	O
jobclass	O
)	O
,	O
and	O
others	O
.	O
explore	O
the	O
relationships	O
between	O
some	O
of	O
these	O
other	O
predictors	O
and	O
wage	O
,	O
and	O
use	O
non-linear	B
ﬁtting	O
techniques	O
in	O
order	O
to	O
ﬁt	B
ﬂexible	O
models	O
to	O
the	O
data	B
.	O
create	O
plots	O
of	O
the	O
results	O
obtained	O
,	O
and	O
write	O
a	O
summary	O
of	O
your	O
ﬁndings	O
.	O
8.	O
fit	O
some	O
of	O
the	O
non-linear	B
models	O
investigated	O
in	O
this	O
chapter	O
to	O
the	O
auto	O
data	B
set	O
.	O
is	O
there	O
evidence	O
for	O
non-linear	O
relationships	O
in	O
this	O
data	B
set	O
?	O
create	O
some	O
informative	O
plots	O
to	O
justify	O
your	O
answer	O
.	O
9.	O
this	O
question	O
uses	O
the	O
variables	O
dis	O
(	O
the	O
weighted	B
mean	O
of	O
distances	O
to	O
ﬁve	O
boston	O
employment	O
centers	O
)	O
and	O
nox	O
(	O
nitrogen	O
oxides	O
concen-	O
tration	O
in	O
parts	O
per	O
10	O
million	O
)	O
from	O
the	O
boston	O
data	B
.	O
we	O
will	O
treat	O
dis	O
as	O
the	O
predictor	B
and	O
nox	O
as	O
the	O
response	B
.	O
(	O
a	O
)	O
use	O
the	O
poly	O
(	O
)	O
function	B
to	O
ﬁt	B
a	O
cubic	B
polynomial	O
regression	B
to	O
predict	O
nox	O
using	O
dis	O
.	O
report	O
the	O
regression	B
output	O
,	O
and	O
plot	B
the	O
resulting	O
data	B
and	O
polynomial	B
ﬁts	O
.	O
300	O
7.	O
moving	O
beyond	O
linearity	O
(	O
b	O
)	O
plot	B
the	O
polynomial	B
ﬁts	O
for	O
a	O
range	O
of	O
diﬀerent	O
polynomial	B
degrees	O
(	O
say	O
,	O
from	O
1	O
to	O
10	O
)	O
,	O
and	O
report	O
the	O
associated	O
residual	B
sum	O
of	O
squares	O
.	O
(	O
c	O
)	O
perform	O
cross-validation	B
or	O
another	O
approach	B
to	O
select	O
the	O
opti-	O
mal	O
degree	O
for	O
the	O
polynomial	B
,	O
and	O
explain	O
your	O
results	O
.	O
(	O
d	O
)	O
use	O
the	O
bs	O
(	O
)	O
function	B
to	O
ﬁt	B
a	O
regression	B
spline	O
to	O
predict	O
nox	O
using	O
dis	O
.	O
report	O
the	O
output	B
for	O
the	O
ﬁt	B
using	O
four	O
degrees	B
of	I
freedom	I
.	O
how	O
did	O
you	O
choose	O
the	O
knots	O
?	O
plot	B
the	O
resulting	O
ﬁt	B
.	O
(	O
e	O
)	O
now	O
ﬁt	B
a	O
regression	B
spline	O
for	O
a	O
range	O
of	O
degrees	B
of	I
freedom	I
,	O
and	O
plot	B
the	O
resulting	O
ﬁts	O
and	O
report	O
the	O
resulting	O
rss	O
.	O
describe	O
the	O
results	O
obtained	O
.	O
(	O
f	O
)	O
perform	O
cross-validation	B
or	O
another	O
approach	B
in	O
order	O
to	O
select	O
the	O
best	O
degrees	O
of	O
freedom	O
for	O
a	O
regression	B
spline	O
on	O
this	O
data	B
.	O
describe	O
your	O
results	O
.	O
10.	O
this	O
question	O
relates	O
to	O
the	O
college	O
data	B
set	O
.	O
(	O
a	O
)	O
split	O
the	O
data	B
into	O
a	O
training	B
set	O
and	O
a	O
test	B
set	O
.	O
using	O
out-of-state	O
tuition	O
as	O
the	O
response	B
and	O
the	O
other	O
variables	O
as	O
the	O
predictors	O
,	O
perform	O
forward	B
stepwise	I
selection	I
on	O
the	O
training	B
set	O
in	O
order	O
to	O
identify	O
a	O
satisfactory	O
model	B
that	O
uses	O
just	O
a	O
subset	O
of	O
the	O
predictors	O
.	O
(	O
b	O
)	O
fit	O
a	O
gam	O
on	O
the	O
training	B
data	O
,	O
using	O
out-of-state	O
tuition	O
as	O
the	O
response	B
and	O
the	O
features	O
selected	O
in	O
the	O
previous	O
step	O
as	O
the	O
predictors	O
.	O
plot	B
the	O
results	O
,	O
and	O
explain	O
your	O
ﬁndings	O
.	O
(	O
c	O
)	O
evaluate	O
the	O
model	B
obtained	O
on	O
the	O
test	B
set	O
,	O
and	O
explain	O
the	O
results	O
obtained	O
.	O
(	O
d	O
)	O
for	O
which	O
variables	O
,	O
if	O
any	O
,	O
is	O
there	O
evidence	O
of	O
a	O
non-linear	B
relationship	O
with	O
the	O
response	B
?	O
11.	O
in	O
section	O
7.7	O
,	O
it	O
was	O
mentioned	O
that	O
gams	O
are	O
generally	O
ﬁt	B
using	O
a	O
backﬁtting	B
approach	O
.	O
the	O
idea	O
behind	O
backﬁtting	B
is	O
actually	O
quite	O
simple	B
.	O
we	O
will	O
now	O
explore	O
backﬁtting	B
in	O
the	O
context	O
of	O
multiple	B
linear	O
regression	B
.	O
suppose	O
that	O
we	O
would	O
like	O
to	O
perform	O
multiple	B
linear	O
regression	B
,	O
but	O
we	O
do	O
not	O
have	O
software	O
to	O
do	O
so	O
.	O
instead	O
,	O
we	O
only	O
have	O
software	O
to	O
perform	O
simple	B
linear	O
regression	B
.	O
therefore	O
,	O
we	O
take	O
the	O
following	O
iterative	O
approach	B
:	O
we	O
repeatedly	O
hold	O
all	O
but	O
one	O
coeﬃcient	B
esti-	O
mate	O
ﬁxed	O
at	O
its	O
current	O
value	O
,	O
and	O
update	O
only	O
that	O
coeﬃcient	B
estimate	O
using	O
a	O
simple	B
linear	O
regression	B
.	O
the	O
process	O
is	O
continued	O
un-	O
til	O
convergence—that	O
is	O
,	O
until	O
the	O
coeﬃcient	B
estimates	O
stop	O
changing	O
.	O
we	O
now	O
try	O
this	O
out	O
on	O
a	O
toy	O
example	O
.	O
7.9	O
exercises	O
301	O
(	O
a	O
)	O
generate	O
a	O
response	B
y	O
and	O
two	O
predictors	O
x1	O
and	O
x2	B
,	O
with	O
n	O
=	O
100	O
.	O
(	O
b	O
)	O
initialize	O
ˆβ1	O
to	O
take	O
on	O
a	O
value	O
of	O
your	O
choice	O
.	O
it	O
does	O
not	O
matter	O
what	O
value	O
you	O
choose	O
.	O
(	O
c	O
)	O
keeping	O
ˆβ1	O
ﬁxed	O
,	O
ﬁt	B
the	O
model	B
y	O
−	O
ˆβ1x1	O
=	O
β0	O
+	O
β2x2	O
+	O
	O
.	O
you	O
can	O
do	O
this	O
as	O
follows	O
:	O
>	O
a	O
=y	O
-	O
beta1	O
*	O
x1	O
>	O
beta2	O
=	O
lm	O
(	O
a∼x2	O
)	O
$	O
coef	O
[	O
2	O
]	O
(	O
d	O
)	O
keeping	O
ˆβ2	O
ﬁxed	O
,	O
ﬁt	B
the	O
model	B
y	O
−	O
ˆβ2x2	O
=	O
β0	O
+	O
β1x1	O
+	O
	O
.	O
you	O
can	O
do	O
this	O
as	O
follows	O
:	O
>	O
a	O
=y	O
-	O
beta2	O
*	O
x2	B
>	O
beta1	O
=	O
lm	O
(	O
a∼x1	O
)	O
$	O
coef	O
[	O
2	O
]	O
(	O
e	O
)	O
write	O
a	O
for	B
loop	I
to	O
repeat	O
(	O
c	O
)	O
and	O
(	O
d	O
)	O
1,000	O
times	O
.	O
report	O
the	O
estimates	O
of	O
ˆβ0	O
,	O
ˆβ1	O
,	O
and	O
ˆβ2	O
at	O
each	O
iteration	O
of	O
the	O
for	B
loop	I
.	O
create	O
a	O
plot	B
in	O
which	O
each	O
of	O
these	O
values	O
is	O
displayed	O
,	O
with	O
ˆβ0	O
,	O
ˆβ1	O
,	O
and	O
ˆβ2	O
each	O
shown	O
in	O
a	O
diﬀerent	O
color	O
.	O
(	O
f	O
)	O
compare	O
your	O
answer	O
in	O
(	O
e	O
)	O
to	O
the	O
results	O
of	O
simply	O
performing	O
multiple	B
linear	O
regression	B
to	O
predict	O
y	O
using	O
x1	O
and	O
x2	B
.	O
use	O
the	O
abline	O
(	O
)	O
function	B
to	O
overlay	O
those	O
multiple	B
linear	O
regression	B
coeﬃcient	O
estimates	O
on	O
the	O
plot	B
obtained	O
in	O
(	O
e	O
)	O
.	O
(	O
g	O
)	O
on	O
this	O
data	B
set	O
,	O
how	O
many	O
backﬁtting	B
iterations	O
were	O
required	O
in	O
order	O
to	O
obtain	O
a	O
“	O
good	O
”	O
approximation	O
to	O
the	O
multiple	B
re-	O
gression	O
coeﬃcient	B
estimates	O
?	O
12.	O
this	O
problem	O
is	O
a	O
continuation	O
of	O
the	O
previous	O
exercise	O
.	O
in	O
a	O
toy	O
example	O
with	O
p	O
=	O
100	O
,	O
show	O
that	O
one	O
can	O
approximate	O
the	O
multiple	B
linear	O
regression	B
coeﬃcient	O
estimates	O
by	O
repeatedly	O
performing	O
simple	B
linear	O
regression	B
in	O
a	O
backﬁtting	B
procedure	O
.	O
how	O
many	O
backﬁtting	B
iterations	O
are	O
required	O
in	O
order	O
to	O
obtain	O
a	O
“	O
good	O
”	O
approximation	O
to	O
the	O
multiple	B
regression	O
coeﬃcient	B
estimates	O
?	O
create	O
a	O
plot	B
to	O
justify	O
your	O
answer	O
.	O
8	O
tree-based	O
methods	O
in	O
this	O
chapter	O
,	O
we	O
describe	O
tree-based	O
methods	O
for	O
regression	O
and	O
classiﬁcation	B
.	O
these	O
involve	O
stratifying	O
or	O
segmenting	O
the	O
predictor	B
space	O
into	O
a	O
number	O
of	O
simple	B
regions	O
.	O
in	O
order	O
to	O
make	O
a	O
prediction	B
for	O
a	O
given	O
observation	O
,	O
we	O
typically	O
use	O
the	O
mean	O
or	O
the	O
mode	B
of	O
the	O
training	B
observa-	O
tions	O
in	O
the	O
region	O
to	O
which	O
it	O
belongs	O
.	O
since	O
the	O
set	B
of	O
splitting	O
rules	O
used	O
to	O
segment	O
the	O
predictor	B
space	O
can	O
be	O
summarized	O
in	O
a	O
tree	B
,	O
these	O
types	O
of	O
approaches	O
are	O
known	O
as	O
decision	B
tree	I
methods	O
.	O
tree-based	O
methods	O
are	O
simple	B
and	O
useful	O
for	O
interpretation	O
.	O
however	O
,	O
they	O
typically	O
are	O
not	O
competitive	O
with	O
the	O
best	O
supervised	O
learning	O
ap-	O
proaches	O
,	O
such	O
as	O
those	O
seen	O
in	O
chapters	O
6	O
and	O
7	O
,	O
in	O
terms	O
of	O
prediction	B
accuracy	O
.	O
hence	O
in	O
this	O
chapter	O
we	O
also	O
introduce	O
bagging	B
,	O
random	O
forests	O
,	O
and	O
boosting	B
.	O
each	O
of	O
these	O
approaches	O
involves	O
producing	O
multiple	B
trees	O
which	O
are	O
then	O
combined	O
to	O
yield	O
a	O
single	B
consensus	O
prediction	B
.	O
we	O
will	O
see	O
that	O
combining	O
a	O
large	O
number	O
of	O
trees	O
can	O
often	O
result	O
in	O
dramatic	O
improvements	O
in	O
prediction	B
accuracy	O
,	O
at	O
the	O
expense	O
of	O
some	O
loss	O
in	O
inter-	O
pretation	O
.	O
decision	B
tree	I
8.1	O
the	O
basics	O
of	O
decision	O
trees	O
decision	O
trees	O
can	O
be	O
applied	O
to	O
both	O
regression	B
and	O
classiﬁcation	B
problems	O
.	O
we	O
ﬁrst	O
consider	O
regression	B
problems	O
,	O
and	O
then	O
move	O
on	O
to	O
classiﬁcation	B
.	O
g.	O
james	O
et	O
al.	O
,	O
an	O
introduction	O
to	O
statistical	O
learning	O
:	O
with	O
applications	O
in	O
r	O
,	O
springer	O
texts	O
in	O
statistics	O
,	O
doi	O
10.1007/978-1-4614-7138-7	O
8	O
,	O
©	O
springer	O
science+business	O
media	O
new	O
york	O
2013	O
303	O
304	O
8.	O
tree-based	O
methods	O
years	O
<	O
4.5	O
|	O
5.11	O
hits	O
<	O
117.5	O
6.00	O
6.74	O
figure	O
8.1.	O
for	O
the	O
hitters	O
data	B
,	O
a	O
regression	B
tree	O
for	O
predicting	O
the	O
log	O
salary	O
of	O
a	O
baseball	O
player	O
,	O
based	O
on	O
the	O
number	O
of	O
years	O
that	O
he	O
has	O
played	O
in	O
the	O
major	O
leagues	O
and	O
the	O
number	O
of	O
hits	O
that	O
he	O
made	O
in	O
the	O
previous	O
year	O
.	O
at	O
a	O
given	O
internal	B
node	O
,	O
the	O
label	O
(	O
of	O
the	O
form	O
xj	O
<	O
tk	O
)	O
indicates	O
the	O
left-hand	O
branch	B
emanating	O
from	O
that	O
split	O
,	O
and	O
the	O
right-hand	O
branch	B
corresponds	O
to	O
xj	O
≥	O
tk	O
.	O
for	O
instance	O
,	O
the	O
split	O
at	O
the	O
top	O
of	O
the	O
tree	B
results	O
in	O
two	O
large	O
branches	O
.	O
the	O
left-hand	O
branch	B
corresponds	O
to	O
years	O
<	O
4.5	O
,	O
and	O
the	O
right-hand	O
branch	B
corresponds	O
to	O
years	O
>	O
=4.5	O
.	O
the	O
tree	B
has	O
two	O
internal	B
nodes	O
and	O
three	O
terminal	B
nodes	O
,	O
or	O
leaves	O
.	O
the	O
number	O
in	O
each	O
leaf	B
is	O
the	O
mean	O
of	O
the	O
response	B
for	O
the	O
observations	B
that	O
fall	O
there	O
.	O
8.1.1	O
regression	B
trees	O
in	O
order	O
to	O
motivate	O
regression	B
trees	O
,	O
we	O
begin	O
with	O
a	O
simple	B
example	O
.	O
regression	B
tree	O
predicting	O
baseball	O
players	O
’	O
salaries	O
using	O
regression	B
trees	O
we	O
use	O
the	O
hitters	O
data	B
set	O
to	O
predict	O
a	O
baseball	O
player	O
’	O
s	O
salary	O
based	O
on	O
years	O
(	O
the	O
number	O
of	O
years	O
that	O
he	O
has	O
played	O
in	O
the	O
major	O
leagues	O
)	O
and	O
hits	O
(	O
the	O
number	O
of	O
hits	O
that	O
he	O
made	O
in	O
the	O
previous	O
year	O
)	O
.	O
we	O
ﬁrst	O
remove	O
observations	B
that	O
are	O
missing	O
salary	O
values	O
,	O
and	O
log-transform	O
salary	O
so	O
that	O
its	O
distribution	B
has	O
more	O
of	O
a	O
typical	O
bell-shape	O
.	O
(	O
recall	B
that	O
salary	O
is	O
measured	O
in	O
thousands	O
of	O
dollars	O
.	O
)	O
figure	O
8.1	O
shows	O
a	O
regression	B
tree	O
ﬁt	B
to	O
this	O
data	B
.	O
it	O
consists	O
of	O
a	O
series	O
of	O
splitting	O
rules	O
,	O
starting	O
at	O
the	O
top	O
of	O
the	O
tree	B
.	O
the	O
top	O
split	O
assigns	O
observations	B
having	O
years	O
<	O
4.5	O
to	O
the	O
left	O
branch.1	O
the	O
predicted	O
salary	O
1both	O
years	O
and	O
hits	O
are	O
integers	O
in	O
these	O
data	B
;	O
the	O
tree	B
(	O
)	O
function	B
in	O
r	O
labels	O
the	O
splits	O
at	O
the	O
midpoint	O
between	O
two	O
adjacent	O
values	O
.	O
8.1	O
the	O
basics	O
of	O
decision	O
trees	O
305	O
238	O
117.5	O
s	O
t	O
i	O
h	O
r1	O
r3	O
r2	O
1	O
4.5	O
years	O
1	O
24	O
figure	O
8.2.	O
the	O
three-region	O
partition	O
for	O
the	O
hitters	O
data	B
set	O
from	O
the	O
regression	B
tree	O
illustrated	O
in	O
figure	O
8.1.	O
for	O
these	O
players	O
is	O
given	O
by	O
the	O
mean	O
response	O
value	O
for	O
the	O
players	O
in	O
the	O
data	B
set	O
with	O
years	O
<	O
4.5	O
.	O
for	O
such	O
players	O
,	O
the	O
mean	O
log	O
salary	O
is	O
5.107	O
,	O
and	O
so	O
we	O
make	O
a	O
prediction	B
of	O
e5.107	O
thousands	O
of	O
dollars	O
,	O
i.e	O
.	O
$	O
165,174	O
,	O
for	O
these	O
players	O
.	O
players	O
with	O
years	O
>	O
=4.5	O
are	O
assigned	O
to	O
the	O
right	O
branch	B
,	O
and	O
then	O
that	O
group	O
is	O
further	O
subdivided	O
by	O
hits	O
.	O
overall	O
,	O
the	O
tree	B
stratiﬁes	O
or	O
segments	O
the	O
players	O
into	O
three	O
regions	O
of	O
predictor	B
space	O
:	O
players	O
who	O
have	O
played	O
for	O
four	O
or	O
fewer	O
years	O
,	O
players	O
who	O
have	O
played	O
for	O
ﬁve	O
or	O
more	O
years	O
and	O
who	O
made	O
fewer	O
than	O
118	O
hits	O
last	O
year	O
,	O
and	O
players	O
who	O
have	O
played	O
for	O
ﬁve	O
or	O
more	O
years	O
and	O
who	O
made	O
at	O
least	O
118	O
hits	O
last	O
year	O
.	O
these	O
three	O
regions	O
can	O
be	O
written	O
as	O
r1	O
=	O
{	O
x	O
|	O
years	O
<	O
4.5	O
}	O
,	O
r2	O
=	O
{	O
x	O
|	O
years	O
>	O
=4.5	O
,	O
hits	O
<	O
117.5	O
}	O
,	O
and	O
r3	O
=	O
{	O
x	O
|	O
years	O
>	O
=4.5	O
,	O
hits	O
>	O
=117.5	O
}	O
.	O
figure	O
8.2	O
illustrates	O
the	O
regions	O
as	O
a	O
function	B
of	O
years	O
and	O
hits	O
.	O
the	O
predicted	O
salaries	O
for	O
these	O
three	O
groups	O
are	O
$	O
1,000×e5.107	O
=	O
$	O
165,174	O
,	O
$	O
1,000×e5.999	O
=	O
$	O
402,834	O
,	O
and	O
$	O
1,000×e6.740	O
=	O
$	O
845,346	O
respectively	O
.	O
in	O
keeping	O
with	O
the	O
tree	B
analogy	O
,	O
the	O
regions	O
r1	O
,	O
r2	O
,	O
and	O
r3	O
are	O
known	O
as	O
terminal	B
nodes	O
or	O
leaves	O
of	O
the	O
tree	B
.	O
as	O
is	O
the	O
case	O
for	O
figure	O
8.1	O
,	O
decision	O
trees	O
are	O
typically	O
drawn	O
upside	O
down	O
,	O
in	O
the	O
sense	O
that	O
the	O
leaves	O
are	O
at	O
the	O
bottom	O
of	O
the	O
tree	B
.	O
the	O
points	O
along	O
the	O
tree	B
where	O
the	O
predictor	B
space	O
is	O
split	O
are	O
referred	O
to	O
as	O
internal	B
nodes	O
.	O
in	O
figure	O
8.1	O
,	O
the	O
two	O
internal	B
nodes	O
are	O
indicated	O
by	O
the	O
text	O
years	O
<	O
4.5	O
and	O
hits	O
<	O
117.5	O
.	O
we	O
refer	O
to	O
the	O
segments	O
of	O
the	O
trees	O
that	O
connect	O
the	O
nodes	O
as	O
branches	O
.	O
we	O
might	O
interpret	O
the	O
regression	B
tree	O
displayed	O
in	O
figure	O
8.1	O
as	O
follows	O
:	O
years	O
is	O
the	O
most	O
important	O
factor	B
in	O
determining	O
salary	O
,	O
and	O
players	O
with	O
less	O
experience	O
earn	O
lower	O
salaries	O
than	O
more	O
experienced	O
players	O
.	O
given	O
that	O
a	O
player	O
is	O
less	O
experienced	O
,	O
the	O
number	O
of	O
hits	O
that	O
he	O
made	O
in	O
the	O
previous	O
year	O
seems	O
to	O
play	O
little	O
role	O
in	O
his	O
salary	O
.	O
but	O
among	O
players	O
who	O
terminal	B
node	O
leaf	B
internal	O
node	B
branch	O
306	O
8.	O
tree-based	O
methods	O
have	O
been	O
in	O
the	O
major	O
leagues	O
for	O
ﬁve	O
or	O
more	O
years	O
,	O
the	O
number	O
of	O
hits	O
made	O
in	O
the	O
previous	O
year	O
does	O
aﬀect	O
salary	O
,	O
and	O
players	O
who	O
made	O
more	O
hits	O
last	O
year	O
tend	O
to	O
have	O
higher	O
salaries	O
.	O
the	O
regression	B
tree	O
shown	O
in	O
figure	O
8.1	O
is	O
likely	O
an	O
over-simpliﬁcation	O
of	O
the	O
true	O
relationship	O
between	O
hits	O
,	O
years	O
,	O
and	O
salary	O
.	O
however	O
,	O
it	O
has	O
advantages	O
over	O
other	O
types	O
of	O
regression	B
models	O
(	O
such	O
as	O
those	O
seen	O
in	O
chapters	O
3	O
and	O
6	O
)	O
:	O
it	O
is	O
easier	O
to	O
interpret	O
,	O
and	O
has	O
a	O
nice	O
graphical	O
representation	O
.	O
prediction	B
via	O
stratiﬁcation	O
of	O
the	O
feature	B
space	O
we	O
now	O
discuss	O
the	O
process	O
of	O
building	O
a	O
regression	B
tree	O
.	O
roughly	O
speaking	O
,	O
there	O
are	O
two	O
steps	O
.	O
1.	O
we	O
divide	O
the	O
predictor	B
space—that	O
is	O
,	O
the	O
set	B
of	O
possible	O
values	O
for	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp—into	O
j	O
distinct	O
and	O
non-overlapping	O
regions	O
,	O
r1	O
,	O
r2	O
,	O
.	O
.	O
.	O
,	O
rj	O
.	O
2.	O
for	O
every	O
observation	O
that	O
falls	O
into	O
the	O
region	O
rj	O
,	O
we	O
make	O
the	O
same	O
prediction	B
,	O
which	O
is	O
simply	O
the	O
mean	O
of	O
the	O
response	B
values	O
for	O
the	O
training	B
observations	O
in	O
rj	O
.	O
for	O
instance	O
,	O
suppose	O
that	O
in	O
step	O
1	O
we	O
obtain	O
two	O
regions	O
,	O
r1	O
and	O
r2	O
,	O
and	O
that	O
the	O
response	B
mean	O
of	O
the	O
training	B
observations	O
in	O
the	O
ﬁrst	O
region	O
is	O
10	O
,	O
while	O
the	O
response	B
mean	O
of	O
the	O
training	B
observations	O
in	O
the	O
second	O
region	O
is	O
20.	O
then	O
for	O
a	O
given	O
observation	O
x	O
=	O
x	O
,	O
if	O
x	O
∈	O
r1	O
we	O
will	O
predict	O
a	O
value	O
of	O
10	O
,	O
and	O
if	O
x	O
∈	O
r2	O
we	O
will	O
predict	O
a	O
value	O
of	O
20.	O
we	O
now	O
elaborate	O
on	O
step	O
1	O
above	O
.	O
how	O
do	O
we	O
construct	O
the	O
regions	O
r1	O
,	O
.	O
.	O
.	O
,	O
rj	O
?	O
in	O
theory	O
,	O
the	O
regions	O
could	O
have	O
any	O
shape	O
.	O
however	O
,	O
we	O
choose	O
to	O
divide	O
the	O
predictor	B
space	O
into	O
high-dimensional	B
rectangles	O
,	O
or	O
boxes	O
,	O
for	O
simplicity	O
and	O
for	O
ease	O
of	O
interpretation	O
of	O
the	O
resulting	O
predic-	O
tive	O
model	B
.	O
the	O
goal	O
is	O
to	O
ﬁnd	O
boxes	O
r1	O
,	O
.	O
.	O
.	O
,	O
rj	O
that	O
minimize	O
the	O
rss	O
,	O
given	O
by	O
(	O
yi	O
−	O
ˆyrj	O
)	O
2	O
,	O
(	O
8.1	O
)	O
j	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
i∈rj	O
j=1	O
where	O
ˆyrj	O
is	O
the	O
mean	O
response	O
for	O
the	O
training	B
observations	O
within	O
the	O
jth	O
box	O
.	O
unfortunately	O
,	O
it	O
is	O
computationally	O
infeasible	O
to	O
consider	O
every	O
possible	O
partition	O
of	O
the	O
feature	B
space	O
into	O
j	O
boxes	O
.	O
for	O
this	O
reason	O
,	O
we	O
take	O
a	O
top-down	O
,	O
greedy	O
approach	B
that	O
is	O
known	O
as	O
recursive	B
binary	I
splitting	I
.	O
the	O
approach	B
is	O
top-down	O
because	O
it	O
begins	O
at	O
the	O
top	O
of	O
the	O
tree	B
(	O
at	O
which	O
point	O
all	O
observations	B
belong	O
to	O
a	O
single	B
region	O
)	O
and	O
then	O
successively	O
splits	O
the	O
predictor	B
space	O
;	O
each	O
split	O
is	O
indicated	O
via	O
two	O
new	O
branches	O
further	O
down	O
on	O
the	O
tree	B
.	O
it	O
is	O
greedy	O
because	O
at	O
each	O
step	O
of	O
the	O
tree-building	O
process	O
,	O
the	O
best	O
split	O
is	O
made	O
at	O
that	O
particular	O
step	O
,	O
rather	O
than	O
looking	O
ahead	O
and	O
picking	O
a	O
split	O
that	O
will	O
lead	O
to	O
a	O
better	O
tree	B
in	O
some	O
future	O
step	O
.	O
recursive	B
binary	I
splitting	I
8.1	O
the	O
basics	O
of	O
decision	O
trees	O
307	O
in	O
order	O
to	O
perform	O
recursive	B
binary	I
splitting	I
,	O
we	O
ﬁrst	O
select	O
the	O
pre-	O
dictor	O
xj	O
and	O
the	O
cutpoint	O
s	O
such	O
that	O
splitting	O
the	O
predictor	B
space	O
into	O
the	O
regions	O
{	O
x|xj	O
<	O
s	O
}	O
and	O
{	O
x|xj	O
≥	O
s	O
}	O
leads	O
to	O
the	O
greatest	O
possible	O
reduction	O
in	O
rss	O
.	O
(	O
the	O
notation	O
{	O
x|xj	O
<	O
s	O
}	O
means	O
the	O
region	O
of	O
predictor	B
space	O
in	O
which	O
xj	O
takes	O
on	O
a	O
value	O
less	O
than	O
s.	O
)	O
that	O
is	O
,	O
we	O
consider	O
all	O
predictors	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
,	O
and	O
all	O
possible	O
values	O
of	O
the	O
cutpoint	O
s	O
for	O
each	O
of	O
the	O
predictors	O
,	O
and	O
then	O
choose	O
the	O
predictor	B
and	O
cutpoint	O
such	O
that	O
the	O
resulting	O
tree	B
has	O
the	O
lowest	O
rss	O
.	O
in	O
greater	O
detail	O
,	O
for	O
any	O
j	O
and	O
s	O
,	O
we	O
deﬁne	O
the	O
pair	O
of	O
half-planes	O
r1	O
(	O
j	O
,	O
s	O
)	O
=	O
{	O
x|xj	O
<	O
s	O
}	O
and	O
r2	O
(	O
j	O
,	O
s	O
)	O
=	O
{	O
x|xj	O
≥	O
s	O
}	O
,	O
and	O
we	O
seek	O
the	O
value	O
of	O
j	O
and	O
s	O
that	O
minimize	O
the	O
equation	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
(	O
yi	O
−	O
ˆyr1	O
)	O
2	O
+	O
(	O
yi	O
−	O
ˆyr2	O
)	O
2	O
,	O
i	O
:	O
xi∈r1	O
(	O
j	O
,	O
s	O
)	O
i	O
:	O
xi∈r2	O
(	O
j	O
,	O
s	O
)	O
(	O
8.2	O
)	O
(	O
8.3	O
)	O
where	O
ˆyr1	O
is	O
the	O
mean	O
response	O
for	O
the	O
training	B
observations	O
in	O
r1	O
(	O
j	O
,	O
s	O
)	O
,	O
and	O
ˆyr2	O
is	O
the	O
mean	O
response	O
for	O
the	O
training	B
observations	O
in	O
r2	O
(	O
j	O
,	O
s	O
)	O
.	O
finding	O
the	O
values	O
of	O
j	O
and	O
s	O
that	O
minimize	O
(	O
8.3	O
)	O
can	O
be	O
done	O
quite	O
quickly	O
,	O
especially	O
when	O
the	O
number	O
of	O
features	O
p	O
is	O
not	O
too	O
large	O
.	O
next	O
,	O
we	O
repeat	O
the	O
process	O
,	O
looking	O
for	O
the	O
best	O
predictor	O
and	O
best	O
cutpoint	O
in	O
order	O
to	O
split	O
the	O
data	B
further	O
so	O
as	O
to	O
minimize	O
the	O
rss	O
within	O
each	O
of	O
the	O
resulting	O
regions	O
.	O
however	O
,	O
this	O
time	O
,	O
instead	O
of	O
splitting	O
the	O
entire	O
predictor	B
space	O
,	O
we	O
split	O
one	O
of	O
the	O
two	O
previously	O
identiﬁed	O
regions	O
.	O
we	O
now	O
have	O
three	O
regions	O
.	O
again	O
,	O
we	O
look	O
to	O
split	O
one	O
of	O
these	O
three	O
regions	O
further	O
,	O
so	O
as	O
to	O
minimize	O
the	O
rss	O
.	O
the	O
process	O
continues	O
until	O
a	O
stopping	O
criterion	O
is	O
reached	O
;	O
for	O
instance	O
,	O
we	O
may	O
continue	O
until	O
no	O
region	O
contains	O
more	O
than	O
ﬁve	O
observations	B
.	O
once	O
the	O
regions	O
r1	O
,	O
.	O
.	O
.	O
,	O
rj	O
have	O
been	O
created	O
,	O
we	O
predict	O
the	O
response	B
for	O
a	O
given	O
test	B
observation	O
using	O
the	O
mean	O
of	O
the	O
training	B
observations	O
in	O
the	O
region	O
to	O
which	O
that	O
test	B
observation	O
belongs	O
.	O
a	O
ﬁve-region	O
example	O
of	O
this	O
approach	B
is	O
shown	O
in	O
figure	O
8.3.	O
tree	B
pruning	O
the	O
process	O
described	O
above	O
may	O
produce	O
good	O
predictions	O
on	O
the	O
training	B
set	O
,	O
but	O
is	O
likely	O
to	O
overﬁt	O
the	O
data	B
,	O
leading	O
to	O
poor	O
test	B
set	O
performance	O
.	O
this	O
is	O
because	O
the	O
resulting	O
tree	B
might	O
be	O
too	O
complex	O
.	O
a	O
smaller	O
tree	B
with	O
fewer	O
splits	O
(	O
that	O
is	O
,	O
fewer	O
regions	O
r1	O
,	O
.	O
.	O
.	O
,	O
rj	O
)	O
might	O
lead	O
to	O
lower	O
variance	B
and	O
better	O
interpretation	O
at	O
the	O
cost	O
of	O
a	O
little	O
bias	B
.	O
one	O
possible	O
alternative	O
to	O
the	O
process	O
described	O
above	O
is	O
to	O
build	O
the	O
tree	B
only	O
so	O
long	O
as	O
the	O
decrease	O
in	O
the	O
rss	O
due	O
to	O
each	O
split	O
exceeds	O
some	O
(	O
high	O
)	O
threshold	O
.	O
this	O
strategy	O
will	O
result	O
in	O
smaller	O
trees	O
,	O
but	O
is	O
too	O
short-sighted	O
since	O
a	O
seemingly	O
worthless	O
split	O
early	O
on	O
in	O
the	O
tree	B
might	O
be	O
followed	O
by	O
a	O
very	O
good	O
split—that	O
is	O
,	O
a	O
split	O
that	O
leads	O
to	O
a	O
large	O
reduction	O
in	O
rss	O
later	O
on	O
.	O
308	O
8.	O
tree-based	O
methods	O
2	O
x	O
x1	O
x1	O
≤	O
t1	O
|	O
t4	O
r5	O
r4	O
2	O
x	O
t2	O
r2	O
r1	O
r3	O
t1	O
t3	O
x1	O
x2	B
≤	O
t2	O
x1	O
≤	O
t3	O
y	O
x2	B
≤	O
t4	O
r1	O
r2	O
r3	O
r4	O
r5	O
x	O
2	O
x1	O
figure	O
8.3.	O
top	O
left	O
:	O
a	O
partition	O
of	O
two-dimensional	O
feature	B
space	O
that	O
could	O
not	O
result	O
from	O
recursive	B
binary	I
splitting	I
.	O
top	O
right	O
:	O
the	O
output	B
of	O
recursive	B
binary	I
splitting	I
on	O
a	O
two-dimensional	O
example	O
.	O
bottom	O
left	O
:	O
a	O
tree	B
corresponding	O
to	O
the	O
partition	O
in	O
the	O
top	O
right	O
panel	O
.	O
bottom	O
right	O
:	O
a	O
perspective	O
plot	B
of	O
the	O
prediction	B
surface	O
corresponding	O
to	O
that	O
tree	B
.	O
therefore	O
,	O
a	O
better	O
strategy	O
is	O
to	O
grow	O
a	O
very	O
large	O
tree	B
t0	O
,	O
and	O
then	O
prune	O
it	O
back	O
in	O
order	O
to	O
obtain	O
a	O
subtree	B
.	O
how	O
do	O
we	O
determine	O
the	O
best	O
prune	O
way	O
to	O
prune	O
the	O
tree	B
?	O
intuitively	O
,	O
our	O
goal	O
is	O
to	O
select	O
a	O
subtree	B
that	O
subtree	B
leads	O
to	O
the	O
lowest	O
test	B
error	O
rate	B
.	O
given	O
a	O
subtree	B
,	O
we	O
can	O
estimate	O
its	O
test	B
error	O
using	O
cross-validation	B
or	O
the	O
validation	B
set	I
approach	O
.	O
however	O
,	O
estimating	O
the	O
cross-validation	B
error	O
for	O
every	O
possible	O
subtree	B
would	O
be	O
too	O
cumbersome	O
,	O
since	O
there	O
is	O
an	O
extremely	O
large	O
number	O
of	O
possible	O
subtrees	O
.	O
instead	O
,	O
we	O
need	O
a	O
way	O
to	O
select	O
a	O
small	O
set	B
of	O
subtrees	O
for	O
consideration	O
.	O
cost	B
complexity	I
pruning—also	O
known	O
as	O
weakest	B
link	I
pruning—gives	O
us	O
a	O
way	O
to	O
do	O
just	O
this	O
.	O
rather	O
than	O
considering	O
every	O
possible	O
subtree	B
,	O
we	O
consider	O
a	O
sequence	O
of	O
trees	O
indexed	O
by	O
a	O
nonnegative	O
tuning	B
parameter	I
α.	O
cost	B
complexity	I
pruning	O
weakest	B
link	I
pruning	I
8.1	O
the	O
basics	O
of	O
decision	O
trees	O
309	O
algorithm	O
8.1	O
building	O
a	O
regression	B
tree	O
1.	O
use	O
recursive	B
binary	I
splitting	I
to	O
grow	O
a	O
large	O
tree	B
on	O
the	O
training	B
data	O
,	O
stopping	O
only	O
when	O
each	O
terminal	B
node	O
has	O
fewer	O
than	O
some	O
minimum	O
number	O
of	O
observations	B
.	O
2.	O
apply	O
cost	B
complexity	I
pruning	O
to	O
the	O
large	O
tree	B
in	O
order	O
to	O
obtain	O
a	O
sequence	O
of	O
best	O
subtrees	O
,	O
as	O
a	O
function	B
of	O
α	O
.	O
3.	O
use	O
k-fold	B
cross-validation	O
to	O
choose	O
α.	O
that	O
is	O
,	O
divide	O
the	O
training	B
observations	O
into	O
k	O
folds	O
.	O
for	O
each	O
k	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k	O
:	O
(	O
a	O
)	O
repeat	O
steps	O
1	O
and	O
2	O
on	O
all	O
but	O
the	O
kth	O
fold	O
of	O
the	O
training	B
data	O
.	O
(	O
b	O
)	O
evaluate	O
the	O
mean	O
squared	O
prediction	O
error	B
on	O
the	O
data	B
in	O
the	O
left-out	O
kth	O
fold	O
,	O
as	O
a	O
function	B
of	O
α.	O
average	B
the	O
results	O
for	O
each	O
value	O
of	O
α	O
,	O
and	O
pick	O
α	O
to	O
minimize	O
the	O
average	B
error	O
.	O
4.	O
return	O
the	O
subtree	B
from	O
step	O
2	O
that	O
corresponds	O
to	O
the	O
chosen	O
value	O
of	O
α.	O
for	O
each	O
value	O
of	O
α	O
there	O
corresponds	O
a	O
subtree	B
t	O
⊂	O
t0	O
such	O
that	O
|t|	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
m=1	O
i	O
:	O
xi∈rm	O
(	O
yi	O
−	O
ˆyrm	O
)	O
2	O
+	O
α|t|	O
(	O
8.4	O
)	O
is	O
as	O
small	O
as	O
possible	O
.	O
here	O
|t|	O
indicates	O
the	O
number	O
of	O
terminal	B
nodes	O
of	O
the	O
tree	B
t	O
,	O
rm	O
is	O
the	O
rectangle	O
(	O
i.e	O
.	O
the	O
subset	O
of	O
predictor	B
space	O
)	O
cor-	O
responding	O
to	O
the	O
mth	O
terminal	B
node	O
,	O
and	O
ˆyrm	O
is	O
the	O
predicted	O
response	B
associated	O
with	O
rm—that	O
is	O
,	O
the	O
mean	O
of	O
the	O
training	B
observations	O
in	O
rm	O
.	O
the	O
tuning	B
parameter	I
α	O
controls	O
a	O
trade-oﬀ	B
between	O
the	O
subtree	B
’	O
s	O
com-	O
plexity	O
and	O
its	O
ﬁt	B
to	O
the	O
training	B
data	O
.	O
when	O
α	O
=	O
0	O
,	O
then	O
the	O
subtree	B
t	O
will	O
simply	O
equal	O
t0	O
,	O
because	O
then	O
(	O
8.4	O
)	O
just	O
measures	O
the	O
training	B
error	O
.	O
however	O
,	O
as	O
α	O
increases	O
,	O
there	O
is	O
a	O
price	O
to	O
pay	O
for	O
having	O
a	O
tree	B
with	O
many	O
terminal	B
nodes	O
,	O
and	O
so	O
the	O
quantity	O
(	O
8.4	O
)	O
will	O
tend	O
to	O
be	O
minimized	O
for	O
a	O
smaller	O
subtree	B
.	O
equation	O
8.4	O
is	O
reminiscent	O
of	O
the	O
lasso	B
(	O
6.7	O
)	O
from	O
chapter	O
6	O
,	O
in	O
which	O
a	O
similar	O
formulation	O
was	O
used	O
in	O
order	O
to	O
control	O
the	O
complexity	O
of	O
a	O
linear	B
model	I
.	O
it	O
turns	O
out	O
that	O
as	O
we	O
increase	O
α	O
from	O
zero	O
in	O
(	O
8.4	O
)	O
,	O
branches	O
get	O
pruned	O
from	O
the	O
tree	B
in	O
a	O
nested	O
and	O
predictable	O
fashion	O
,	O
so	O
obtaining	O
the	O
whole	O
sequence	O
of	O
subtrees	O
as	O
a	O
function	B
of	O
α	O
is	O
easy	O
.	O
we	O
can	O
select	O
a	O
value	O
of	O
α	O
using	O
a	O
validation	B
set	I
or	O
using	O
cross-validation	B
.	O
we	O
then	O
return	O
to	O
the	O
full	O
data	B
set	O
and	O
obtain	O
the	O
subtree	B
corresponding	O
to	O
α.	O
this	O
process	O
is	O
summarized	O
in	O
algorithm	O
8.1	O
.	O
310	O
8.	O
tree-based	O
methods	O
years	O
<	O
4.5	O
|	O
rbi	O
<	O
60.5	O
hits	O
<	O
117.5	O
putouts	O
<	O
82	O
years	O
<	O
3.5	O
5.487	O
years	O
<	O
3.5	O
4.622	O
5.183	O
5.394	O
6.189	O
walks	O
<	O
43.5	O
walks	O
<	O
52.5	O
runs	O
<	O
47.5	O
6.015	O
5.571	O
6.407	O
6.549	O
rbi	O
<	O
80.5	O
years	O
<	O
6.5	O
6.459	O
7.007	O
7.289	O
figure	O
8.4.	O
regression	B
tree	O
analysis	B
for	O
the	O
hitters	O
data	B
.	O
the	O
unpruned	O
tree	B
that	O
results	O
from	O
top-down	O
greedy	O
splitting	O
on	O
the	O
training	B
data	O
is	O
shown	O
.	O
figures	O
8.4	O
and	O
8.5	O
display	O
the	O
results	O
of	O
ﬁtting	O
and	O
pruning	B
a	O
regression	B
tree	O
on	O
the	O
hitters	O
data	B
,	O
using	O
nine	O
of	O
the	O
features	O
.	O
first	O
,	O
we	O
randomly	O
divided	O
the	O
data	B
set	O
in	O
half	O
,	O
yielding	O
132	O
observations	B
in	O
the	O
training	B
set	O
and	O
131	O
observations	B
in	O
the	O
test	B
set	O
.	O
we	O
then	O
built	O
a	O
large	O
regression	B
tree	O
on	O
the	O
training	B
data	O
and	O
varied	O
α	O
in	O
(	O
8.4	O
)	O
in	O
order	O
to	O
create	O
subtrees	O
with	O
diﬀerent	O
numbers	O
of	O
terminal	B
nodes	O
.	O
finally	O
,	O
we	O
performed	O
six-fold	O
cross-	O
validation	O
in	O
order	O
to	O
estimate	O
the	O
cross-validated	O
mse	O
of	O
the	O
trees	O
as	O
a	O
function	B
of	O
α	O
.	O
(	O
we	O
chose	O
to	O
perform	O
six-fold	O
cross-validation	B
because	O
132	O
is	O
an	O
exact	O
multiple	B
of	O
six	O
.	O
)	O
the	O
unpruned	O
regression	B
tree	O
is	O
shown	O
in	O
figure	O
8.4.	O
the	O
green	O
curve	O
in	O
figure	O
8.5	O
shows	O
the	O
cv	O
error	B
as	O
a	O
function	B
of	O
the	O
number	O
of	O
leaves,2	O
while	O
the	O
orange	O
curve	O
indicates	O
the	O
test	B
error	O
.	O
also	O
shown	O
are	O
standard	B
error	I
bars	O
around	O
the	O
estimated	O
errors	O
.	O
for	O
reference	O
,	O
the	O
training	B
error	O
curve	O
is	O
shown	O
in	O
black	O
.	O
the	O
cv	O
error	B
is	O
a	O
reasonable	O
approximation	O
of	O
the	O
test	B
error	O
:	O
the	O
cv	O
error	B
takes	O
on	O
its	O
2although	O
cv	O
error	B
is	O
computed	O
as	O
a	O
function	B
of	O
α	O
,	O
it	O
is	O
convenient	O
to	O
display	O
the	O
result	O
as	O
a	O
function	B
of	O
|t|	O
,	O
the	O
number	O
of	O
leaves	O
;	O
this	O
is	O
based	O
on	O
the	O
relationship	O
between	O
α	O
and	O
|t|	O
in	O
the	O
original	O
tree	B
grown	O
to	O
all	O
the	O
training	B
data	O
.	O
8.1	O
the	O
basics	O
of	O
decision	O
trees	O
311	O
training	B
cross−validation	O
test	B
r	O
o	O
r	O
r	O
e	O
d	O
e	O
r	O
a	O
u	O
q	O
s	O
n	O
a	O
e	O
m	O
0	O
.	O
1	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
2	O
4	O
6	O
8	O
10	O
tree	B
size	O
figure	O
8.5.	O
regression	B
tree	O
analysis	B
for	O
the	O
hitters	O
data	B
.	O
the	O
training	B
,	O
cross-validation	B
,	O
and	O
test	B
mse	O
are	O
shown	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
termi-	O
nal	O
nodes	O
in	O
the	O
pruned	O
tree	B
.	O
standard	B
error	I
bands	O
are	O
displayed	O
.	O
the	O
minimum	O
cross-validation	B
error	O
occurs	O
at	O
a	O
tree	B
size	O
of	O
three	O
.	O
minimum	O
for	O
a	O
three-node	O
tree	B
,	O
while	O
the	O
test	B
error	O
also	O
dips	O
down	O
at	O
the	O
three-node	O
tree	B
(	O
though	O
it	O
takes	O
on	O
its	O
lowest	O
value	O
at	O
the	O
ten-node	O
tree	B
)	O
.	O
the	O
pruned	O
tree	B
containing	O
three	O
terminal	B
nodes	O
is	O
shown	O
in	O
figure	O
8.1	O
.	O
8.1.2	O
classiﬁcation	B
trees	O
a	O
classiﬁcation	B
tree	O
is	O
very	O
similar	O
to	O
a	O
regression	B
tree	O
,	O
except	O
that	O
it	O
is	O
used	O
to	O
predict	O
a	O
qualitative	B
response	O
rather	O
than	O
a	O
quantitative	B
one	O
.	O
re-	O
call	O
that	O
for	O
a	O
regression	B
tree	O
,	O
the	O
predicted	O
response	B
for	O
an	O
observation	O
is	O
given	O
by	O
the	O
mean	O
response	O
of	O
the	O
training	B
observations	O
that	O
belong	O
to	O
the	O
same	O
terminal	B
node	O
.	O
in	O
contrast	B
,	O
for	O
a	O
classiﬁcation	B
tree	O
,	O
we	O
predict	O
that	O
each	O
observation	O
belongs	O
to	O
the	O
most	O
commonly	O
occurring	O
class	O
of	O
training	B
observations	O
in	O
the	O
region	O
to	O
which	O
it	O
belongs	O
.	O
in	O
interpreting	O
the	O
results	O
of	O
a	O
classiﬁcation	B
tree	O
,	O
we	O
are	O
often	O
interested	O
not	O
only	O
in	O
the	O
class	O
prediction	B
corresponding	O
to	O
a	O
particular	O
terminal	B
node	O
region	O
,	O
but	O
also	O
in	O
the	O
class	O
proportions	O
among	O
the	O
training	B
observations	O
that	O
fall	O
into	O
that	O
region	O
.	O
the	O
task	O
of	O
growing	O
a	O
classiﬁcation	B
tree	O
is	O
quite	O
similar	O
to	O
the	O
task	O
of	O
growing	O
a	O
regression	B
tree	O
.	O
just	O
as	O
in	O
the	O
regression	B
setting	O
,	O
we	O
use	O
recursive	B
binary	I
splitting	I
to	O
grow	O
a	O
classiﬁcation	B
tree	O
.	O
however	O
,	O
in	O
the	O
classiﬁcation	B
setting	O
,	O
rss	O
can	O
not	O
be	O
used	O
as	O
a	O
criterion	O
for	O
making	O
the	O
binary	B
splits	O
.	O
a	O
natural	B
alternative	O
to	O
rss	O
is	O
the	O
classiﬁcation	B
error	O
rate	B
.	O
since	O
we	O
plan	O
to	O
assign	O
an	O
observation	O
in	O
a	O
given	O
region	O
to	O
the	O
most	O
commonly	O
occurring	O
class	O
of	O
training	B
observations	O
in	O
that	O
region	O
,	O
the	O
classiﬁcation	B
error	O
rate	B
is	O
simply	O
the	O
fraction	O
of	O
the	O
training	B
observations	O
in	O
that	O
region	O
that	O
do	O
not	O
belong	O
to	O
the	O
most	O
common	O
class	O
:	O
classiﬁcation	B
tree	O
classiﬁcation	B
error	O
rate	B
312	O
8.	O
tree-based	O
methods	O
e	O
=	O
1	O
−	O
max	O
k	O
(	O
ˆpmk	O
)	O
.	O
(	O
8.5	O
)	O
here	O
ˆpmk	O
represents	O
the	O
proportion	O
of	O
training	O
observations	B
in	O
the	O
mth	O
region	O
that	O
are	O
from	O
the	O
kth	O
class	O
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
classiﬁcation	B
error	O
is	O
not	O
suﬃciently	O
sensitive	O
for	O
tree-growing	O
,	O
and	O
in	O
practice	O
two	O
other	O
measures	O
are	O
preferable	O
.	O
the	O
gini	O
index	O
is	O
deﬁned	O
by	O
k	O
(	O
cid:17	O
)	O
k=1	O
g	O
=	O
ˆpmk	O
(	O
1	O
−	O
ˆpmk	O
)	O
,	O
gini	O
index	O
(	O
8.6	O
)	O
entropy	B
a	O
measure	O
of	O
total	O
variance	O
across	O
the	O
k	O
classes	O
.	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
the	O
gini	O
index	O
takes	O
on	O
a	O
small	O
value	O
if	O
all	O
of	O
the	O
ˆpmk	O
’	O
s	O
are	O
close	O
to	O
zero	O
or	O
one	O
.	O
for	O
this	O
reason	O
the	O
gini	O
index	O
is	O
referred	O
to	O
as	O
a	O
measure	O
of	O
node	B
purity—a	O
small	O
value	O
indicates	O
that	O
a	O
node	B
contains	O
predominantly	O
observations	B
from	O
a	O
single	B
class	O
.	O
an	O
alternative	O
to	O
the	O
gini	O
index	O
is	O
entropy	B
,	O
given	O
by	O
d	O
=	O
−	O
k	O
(	O
cid:17	O
)	O
ˆpmk	O
log	O
ˆpmk	O
.	O
(	O
8.7	O
)	O
k=1	O
since	O
0	O
≤	O
ˆpmk	O
≤	O
1	O
,	O
it	O
follows	O
that	O
0	O
≤	O
−ˆpmk	O
log	O
ˆpmk	O
.	O
one	O
can	O
show	O
that	O
the	O
entropy	B
will	O
take	O
on	O
a	O
value	O
near	O
zero	O
if	O
the	O
ˆpmk	O
’	O
s	O
are	O
all	O
near	O
zero	O
or	O
near	O
one	O
.	O
therefore	O
,	O
like	O
the	O
gini	O
index	O
,	O
the	O
entropy	B
will	O
take	O
on	O
a	O
small	O
value	O
if	O
the	O
mth	O
node	B
is	O
pure	O
.	O
in	O
fact	O
,	O
it	O
turns	O
out	O
that	O
the	O
gini	O
index	O
and	O
the	O
entropy	B
are	O
quite	O
similar	O
numerically	O
.	O
when	O
building	O
a	O
classiﬁcation	B
tree	O
,	O
either	O
the	O
gini	O
index	O
or	O
the	O
entropy	B
are	O
typically	O
used	O
to	O
evaluate	O
the	O
quality	O
of	O
a	O
particular	O
split	O
,	O
since	O
these	O
two	O
approaches	O
are	O
more	O
sensitive	O
to	O
node	B
purity	O
than	O
is	O
the	O
classiﬁcation	B
error	O
rate	B
.	O
any	O
of	O
these	O
three	O
approaches	O
might	O
be	O
used	O
when	O
pruning	B
the	O
tree	B
,	O
but	O
the	O
classiﬁcation	B
error	O
rate	B
is	O
preferable	O
if	O
prediction	B
accuracy	O
of	O
the	O
ﬁnal	O
pruned	O
tree	B
is	O
the	O
goal	O
.	O
figure	O
8.6	O
shows	O
an	O
example	O
on	O
the	O
heart	O
data	B
set	O
.	O
these	O
data	B
con-	O
tain	O
a	O
binary	B
outcome	O
hd	O
for	O
303	O
patients	O
who	O
presented	O
with	O
chest	O
pain	O
.	O
an	O
outcome	O
value	O
of	O
yes	O
indicates	O
the	O
presence	O
of	O
heart	O
disease	O
based	O
on	O
an	O
angiographic	O
test	B
,	O
while	O
no	O
means	O
no	O
heart	O
disease	O
.	O
there	O
are	O
13	O
predic-	O
tors	O
including	O
age	O
,	O
sex	O
,	O
chol	O
(	O
a	O
cholesterol	O
measurement	O
)	O
,	O
and	O
other	O
heart	O
and	O
lung	O
function	B
measurements	O
.	O
cross-validation	B
results	O
in	O
a	O
tree	B
with	O
six	O
terminal	B
nodes	O
.	O
in	O
our	O
discussion	O
thus	O
far	O
,	O
we	O
have	O
assumed	O
that	O
the	O
predictor	B
vari-	O
ables	O
take	O
on	O
continuous	B
values	O
.	O
however	O
,	O
decision	O
trees	O
can	O
be	O
constructed	O
even	O
in	O
the	O
presence	O
of	O
qualitative	B
predictor	O
variables	O
.	O
for	O
instance	O
,	O
in	O
the	O
heart	O
data	B
,	O
some	O
of	O
the	O
predictors	O
,	O
such	O
as	O
sex	O
,	O
thal	O
(	O
thallium	O
stress	O
test	B
)	O
,	O
and	O
chestpain	O
,	O
are	O
qualitative	B
.	O
therefore	O
,	O
a	O
split	O
on	O
one	O
of	O
these	O
variables	O
amounts	O
to	O
assigning	O
some	O
of	O
the	O
qualitative	B
values	O
to	O
one	O
branch	B
and	O
8.1	O
the	O
basics	O
of	O
decision	O
trees	O
313	O
thal	O
:	O
a	O
|	O
ca	O
<	O
0.5	O
ca	O
<	O
0.5	O
maxhr	O
<	O
161.5	O
chestpain	O
:	O
bc	O
age	O
<	O
52	O
thal	O
:	O
b	O
restecg	O
<	O
1	O
slope	B
<	O
1.5	O
oldpeak	O
<	O
1.1	O
yes	O
no	O
no	O
chestpain	O
:	O
a	O
yes	O
yes	O
yes	O
no	O
yes	O
maxhr	O
<	O
156	O
maxhr	O
<	O
145.5	O
no	O
no	O
restbp	O
<	O
157	O
chol	O
<	O
244	O
no	O
chol	O
<	O
244	O
sex	O
<	O
0.5	O
yes	O
no	O
no	O
no	O
yes	O
no	O
yes	O
r	O
o	O
r	O
r	O
e	O
6	O
.	O
0	O
5	O
.	O
0	O
4	O
.	O
0	O
3	O
.	O
0	O
2	O
.	O
0	O
1	O
.	O
0	O
0	O
.	O
0	O
training	B
cross−validation	O
test	B
thal	O
:	O
a	O
|	O
ca	O
<	O
0.5	O
ca	O
<	O
0.5	O
maxhr	O
<	O
161.5	O
chestpain	O
:	O
bc	O
yes	O
yes	O
no	O
no	O
no	O
yes	O
5	O
10	O
15	O
tree	B
size	O
figure	O
8.6.	O
heart	O
data	B
.	O
top	O
:	O
the	O
unpruned	O
tree	B
.	O
bottom	O
left	O
:	O
cross	O
-validation	O
error	B
,	O
training	B
,	O
and	O
test	B
error	O
,	O
for	O
diﬀerent	O
sizes	O
of	O
the	O
pruned	O
tree	B
.	O
bottom	O
right	O
:	O
the	O
pruned	O
tree	B
corresponding	O
to	O
the	O
minimal	O
cross-validation	B
error	O
.	O
assigning	O
the	O
remaining	O
to	O
the	O
other	O
branch	B
.	O
in	O
figure	O
8.6	O
,	O
some	O
of	O
the	O
in-	O
ternal	O
nodes	O
correspond	O
to	O
splitting	O
qualitative	B
variables	O
.	O
for	O
instance	O
,	O
the	O
top	O
internal	B
node	O
corresponds	O
to	O
splitting	O
thal	O
.	O
the	O
text	O
thal	O
:	O
a	O
indicates	O
that	O
the	O
left-hand	O
branch	B
coming	O
out	O
of	O
that	O
node	B
consists	O
of	O
observations	B
with	O
the	O
ﬁrst	O
value	O
of	O
the	O
thal	O
variable	B
(	O
normal	O
)	O
,	O
and	O
the	O
right-hand	O
node	B
consists	O
of	O
the	O
remaining	O
observations	B
(	O
ﬁxed	O
or	O
reversible	O
defects	O
)	O
.	O
the	O
text	O
chestpain	O
:	O
bc	O
two	O
splits	O
down	O
the	O
tree	B
on	O
the	O
left	O
indicates	O
that	O
the	O
left-hand	O
branch	B
coming	O
out	O
of	O
that	O
node	B
consists	O
of	O
observations	B
with	O
the	O
second	O
and	O
third	O
values	O
of	O
the	O
chestpain	O
variable	B
,	O
where	O
the	O
possible	O
values	O
are	O
typical	O
angina	O
,	O
atypical	O
angina	O
,	O
non-anginal	O
pain	O
,	O
and	O
asymptomatic	O
.	O
314	O
8.	O
tree-based	O
methods	O
figure	O
8.6	O
has	O
a	O
surprising	O
characteristic	O
:	O
some	O
of	O
the	O
splits	O
yield	O
two	O
terminal	B
nodes	O
that	O
have	O
the	O
same	O
predicted	O
value	O
.	O
for	O
instance	O
,	O
consider	O
the	O
split	O
restecg	O
<	O
1	O
near	O
the	O
bottom	O
right	O
of	O
the	O
unpruned	O
tree	B
.	O
regardless	O
of	O
the	O
value	O
of	O
restecg	O
,	O
a	O
response	B
value	O
of	O
yes	O
is	O
predicted	O
for	O
those	O
ob-	O
servations	O
.	O
why	O
,	O
then	O
,	O
is	O
the	O
split	O
performed	O
at	O
all	O
?	O
the	O
split	O
is	O
performed	O
because	O
it	O
leads	O
to	O
increased	O
node	B
purity	O
.	O
that	O
is	O
,	O
all	O
9	O
of	O
the	O
observations	B
corresponding	O
to	O
the	O
right-hand	O
leaf	B
have	O
a	O
response	B
value	O
of	O
yes	O
,	O
whereas	O
7/11	O
of	O
those	O
corresponding	O
to	O
the	O
left-hand	O
leaf	B
have	O
a	O
response	B
value	O
of	O
yes	O
.	O
why	O
is	O
node	B
purity	O
important	O
?	O
suppose	O
that	O
we	O
have	O
a	O
test	B
obser-	O
vation	O
that	O
belongs	O
to	O
the	O
region	O
given	O
by	O
that	O
right-hand	O
leaf	B
.	O
then	O
we	O
can	O
be	O
pretty	O
certain	O
that	O
its	O
response	B
value	O
is	O
yes	O
.	O
in	O
contrast	B
,	O
if	O
a	O
test	B
observation	O
belongs	O
to	O
the	O
region	O
given	O
by	O
the	O
left-hand	O
leaf	B
,	O
then	O
its	O
re-	O
sponse	O
value	O
is	O
probably	O
yes	O
,	O
but	O
we	O
are	O
much	O
less	O
certain	O
.	O
even	O
though	O
the	O
split	O
restecg	O
<	O
1	O
does	O
not	O
reduce	O
the	O
classiﬁcation	B
error	O
,	O
it	O
improves	O
the	O
gini	O
index	O
and	O
the	O
entropy	B
,	O
which	O
are	O
more	O
sensitive	O
to	O
node	B
purity	O
.	O
8.1.3	O
trees	O
versus	O
linear	B
models	O
regression	B
and	O
classiﬁcation	B
trees	O
have	O
a	O
very	O
diﬀerent	O
ﬂavor	O
from	O
the	O
more	O
classical	O
approaches	O
for	O
regression	O
and	O
classiﬁcation	B
presented	O
in	O
chapters	O
3	O
and	O
4.	O
in	O
particular	O
,	O
linear	B
regression	I
assumes	O
a	O
model	B
of	O
the	O
form	O
p	O
(	O
cid:17	O
)	O
f	O
(	O
x	O
)	O
=	O
β0	O
+	O
xjβj	O
,	O
j=1	O
cm	O
·	O
1	O
(	O
x∈rm	O
)	O
whereas	O
regression	B
trees	O
assume	O
a	O
model	B
of	O
the	O
form	O
m	O
(	O
cid:17	O
)	O
f	O
(	O
x	O
)	O
=	O
m=1	O
(	O
8.8	O
)	O
(	O
8.9	O
)	O
where	O
r1	O
,	O
.	O
.	O
.	O
,	O
rm	O
represent	O
a	O
partition	O
of	O
feature	B
space	O
,	O
as	O
in	O
figure	O
8.3.	O
which	O
model	B
is	O
better	O
?	O
it	O
depends	O
on	O
the	O
problem	O
at	O
hand	O
.	O
if	O
the	O
relationship	O
between	O
the	O
features	O
and	O
the	O
response	B
is	O
well	O
approximated	O
by	O
a	O
linear	B
model	I
as	O
in	O
(	O
8.8	O
)	O
,	O
then	O
an	O
approach	B
such	O
as	O
linear	B
regression	I
will	O
likely	O
work	O
well	O
,	O
and	O
will	O
outperform	O
a	O
method	O
such	O
as	O
a	O
regression	B
tree	O
that	O
does	O
not	O
exploit	O
this	O
linear	B
structure	O
.	O
if	O
instead	O
there	O
is	O
a	O
highly	O
non-linear	B
and	O
complex	O
relationship	O
between	O
the	O
features	O
and	O
the	O
response	B
as	O
indicated	O
by	O
model	B
(	O
8.9	O
)	O
,	O
then	O
decision	O
trees	O
may	O
outperform	O
classical	O
approaches	O
.	O
an	O
illustrative	O
example	O
is	O
displayed	O
in	O
figure	O
8.7.	O
the	O
rela-	O
tive	O
performances	O
of	O
tree-based	O
and	O
classical	O
approaches	O
can	O
be	O
assessed	O
by	O
estimating	O
the	O
test	B
error	O
,	O
using	O
either	O
cross-validation	B
or	O
the	O
validation	B
set	I
approach	O
(	O
chapter	O
5	O
)	O
.	O
of	O
course	O
,	O
other	O
considerations	O
beyond	O
simply	O
test	B
error	O
may	O
come	O
into	O
play	O
in	O
selecting	O
a	O
statistical	O
learning	O
method	O
;	O
for	O
instance	O
,	O
in	O
certain	O
set-	O
tings	O
,	O
prediction	B
using	O
a	O
tree	B
may	O
be	O
preferred	O
for	O
the	O
sake	O
of	O
interpretabil-	O
ity	O
and	O
visualization	O
.	O
8.1	O
the	O
basics	O
of	O
decision	O
trees	O
315	O
2	O
1	O
2	O
1	O
2	O
x	O
0	O
2	O
x	O
0	O
1	O
−	O
2	O
−	O
2	O
1	O
1	O
−	O
2	O
−	O
−2	O
−1	O
0	O
x1	O
1	O
2	O
−2	O
−1	O
2	O
1	O
2	O
x	O
0	O
2	O
x	O
0	O
1	O
−	O
2	O
−	O
1	O
−	O
2	O
−	O
−2	O
−1	O
0	O
x1	O
1	O
2	O
−2	O
−1	O
1	O
2	O
1	O
2	O
0	O
x1	O
0	O
x1	O
figure	O
8.7.	O
top	O
row	O
:	O
a	O
two-dimensional	O
classiﬁcation	B
example	O
in	O
which	O
the	O
true	O
decision	O
boundary	O
is	O
linear	B
,	O
and	O
is	O
indicated	O
by	O
the	O
shaded	O
regions	O
.	O
a	O
classical	O
approach	B
that	O
assumes	O
a	O
linear	B
boundary	O
(	O
left	O
)	O
will	O
outperform	O
a	O
de-	O
cision	O
tree	B
that	O
performs	O
splits	O
parallel	O
to	O
the	O
axes	O
(	O
right	O
)	O
.	O
bottom	O
row	O
:	O
here	O
the	O
true	O
decision	O
boundary	O
is	O
non-linear	B
.	O
here	O
a	O
linear	B
model	I
is	O
unable	O
to	O
capture	O
the	O
true	O
decision	O
boundary	O
(	O
left	O
)	O
,	O
whereas	O
a	O
decision	B
tree	I
is	O
successful	O
(	O
right	O
)	O
.	O
8.1.4	O
advantages	O
and	O
disadvantages	O
of	O
trees	O
decision	O
trees	O
for	O
regression	O
and	O
classiﬁcation	B
have	O
a	O
number	O
of	O
advantages	O
over	O
the	O
more	O
classical	O
approaches	O
seen	O
in	O
chapters	O
3	O
and	O
4	O
:	O
▲	O
trees	O
are	O
very	O
easy	O
to	O
explain	O
to	O
people	O
.	O
in	O
fact	O
,	O
they	O
are	O
even	O
easier	O
to	O
explain	O
than	O
linear	B
regression	I
!	O
some	O
people	O
believe	O
that	O
decision	O
trees	O
more	O
closely	O
mirror	O
human	O
decision-making	O
than	O
do	O
the	O
regression	B
and	O
classiﬁcation	B
approaches	O
seen	O
in	O
previous	O
chapters	O
.	O
▲	O
trees	O
can	O
be	O
displayed	O
graphically	O
,	O
and	O
are	O
easily	O
interpreted	O
even	O
by	O
a	O
non-expert	O
(	O
especially	O
if	O
they	O
are	O
small	O
)	O
.	O
▲	O
trees	O
can	O
easily	O
handle	O
qualitative	B
predictors	O
without	O
the	O
need	O
to	O
create	O
dummy	B
variables	O
.	O
▲	O
316	O
8.	O
tree-based	O
methods	O
▼	O
unfortunately	O
,	O
trees	O
generally	O
do	O
not	O
have	O
the	O
same	O
level	B
of	O
predictive	O
accuracy	O
as	O
some	O
of	O
the	O
other	O
regression	B
and	O
classiﬁcation	B
approaches	O
seen	O
in	O
this	O
book	O
.	O
▼	O
additionally	O
,	O
trees	O
can	O
be	O
very	O
non-robust	O
.	O
in	O
other	O
words	O
,	O
a	O
small	O
change	O
in	O
the	O
data	B
can	O
cause	O
a	O
large	O
change	O
in	O
the	O
ﬁnal	O
estimated	O
tree	B
.	O
however	O
,	O
by	O
aggregating	O
many	O
decision	O
trees	O
,	O
using	O
methods	O
like	O
bagging	B
,	O
random	O
forests	O
,	O
and	O
boosting	B
,	O
the	O
predictive	O
performance	O
of	O
trees	O
can	O
be	O
substantially	O
improved	O
.	O
we	O
introduce	O
these	O
concepts	O
in	O
the	O
next	O
section	O
.	O
8.2	O
bagging	B
,	O
random	O
forests	O
,	O
boosting	B
bagging	O
,	O
random	O
forests	O
,	O
and	O
boosting	B
use	O
trees	O
as	O
building	O
blocks	O
to	O
construct	O
more	O
powerful	O
prediction	B
models	O
.	O
8.2.1	O
bagging	B
the	O
bootstrap	B
,	O
introduced	O
in	O
chapter	O
5	O
,	O
is	O
an	O
extremely	O
powerful	O
idea	O
.	O
it	O
is	O
used	O
in	O
many	O
situations	O
in	O
which	O
it	O
is	O
hard	O
or	O
even	O
impossible	O
to	O
directly	O
compute	O
the	O
standard	O
deviation	O
of	O
a	O
quantity	O
of	O
interest	O
.	O
we	O
see	O
here	O
that	O
the	O
bootstrap	B
can	O
be	O
used	O
in	O
a	O
completely	O
diﬀerent	O
context	O
,	O
in	O
order	O
to	O
improve	O
statistical	O
learning	O
methods	O
such	O
as	O
decision	O
trees	O
.	O
the	O
decision	O
trees	O
discussed	O
in	O
section	O
8.1	O
suﬀer	O
from	O
high	O
variance	B
.	O
this	O
means	O
that	O
if	O
we	O
split	O
the	O
training	B
data	O
into	O
two	O
parts	O
at	O
random	O
,	O
and	O
ﬁt	B
a	O
decision	B
tree	I
to	O
both	O
halves	O
,	O
the	O
results	O
that	O
we	O
get	O
could	O
be	O
quite	O
diﬀerent	O
.	O
in	O
contrast	B
,	O
a	O
procedure	O
with	O
low	O
variance	B
will	O
yield	O
similar	O
results	O
if	O
applied	O
repeatedly	O
to	O
distinct	O
data	B
sets	O
;	O
linear	B
regression	I
tends	O
to	O
have	O
low	O
variance	B
,	O
if	O
the	O
ratio	O
of	O
n	O
to	O
p	O
is	O
moderately	O
large	O
.	O
bootstrap	B
aggregation	O
,	O
or	O
bagging	B
,	O
is	O
a	O
general-purpose	O
procedure	O
for	O
reducing	O
the	O
variance	B
of	O
a	O
statistical	O
learning	O
method	O
;	O
we	O
introduce	O
it	O
here	O
because	O
it	O
is	O
particularly	O
useful	O
and	O
frequently	O
used	O
in	O
the	O
context	O
of	O
decision	O
trees	O
.	O
recall	B
that	O
given	O
a	O
set	B
of	O
n	O
independent	B
observations	O
z1	O
,	O
.	O
.	O
.	O
,	O
zn	O
,	O
each	O
with	O
variance	B
σ2	O
,	O
the	O
variance	B
of	O
the	O
mean	O
¯z	O
of	O
the	O
observations	B
is	O
given	O
by	O
σ2/n	O
.	O
in	O
other	O
words	O
,	O
averaging	O
a	O
set	B
of	O
observations	B
reduces	O
variance	B
.	O
hence	O
a	O
natural	B
way	O
to	O
reduce	O
the	O
variance	B
and	O
hence	O
increase	O
the	O
predic-	O
tion	O
accuracy	O
of	O
a	O
statistical	O
learning	O
method	O
is	O
to	O
take	O
many	O
training	B
sets	O
from	O
the	O
population	O
,	O
build	O
a	O
separate	O
prediction	B
model	O
using	O
each	O
training	B
set	O
,	O
and	O
average	B
the	O
resulting	O
predictions	O
.	O
in	O
other	O
words	O
,	O
we	O
could	O
cal-	O
culate	O
ˆf	O
1	O
(	O
x	O
)	O
,	O
ˆf	O
2	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
ˆf	O
b	O
(	O
x	O
)	O
using	O
b	O
separate	O
training	B
sets	O
,	O
and	O
average	B
them	O
in	O
order	O
to	O
obtain	O
a	O
single	B
low-variance	O
statistical	O
learning	O
model	B
,	O
bagging	B
8.2	O
bagging	B
,	O
random	O
forests	O
,	O
boosting	B
317	O
given	O
by	O
ˆfavg	O
(	O
x	O
)	O
=	O
b	O
(	O
cid:17	O
)	O
b=1	O
1	O
b	O
ˆf	O
b	O
(	O
x	O
)	O
.	O
of	O
course	O
,	O
this	O
is	O
not	O
practical	O
because	O
we	O
generally	O
do	O
not	O
have	O
access	O
to	O
multiple	B
training	O
sets	O
.	O
instead	O
,	O
we	O
can	O
bootstrap	B
,	O
by	O
taking	O
repeated	O
samples	O
from	O
the	O
(	O
single	B
)	O
training	B
data	O
set	B
.	O
in	O
this	O
approach	B
we	O
generate	O
b	O
diﬀerent	O
bootstrapped	O
training	B
data	O
sets	O
.	O
we	O
then	O
train	B
our	O
method	O
on	O
∗b	O
(	O
x	O
)	O
,	O
and	O
ﬁnally	O
average	B
the	O
bth	O
bootstrapped	O
training	B
set	O
in	O
order	O
to	O
get	O
ˆf	O
all	O
the	O
predictions	O
,	O
to	O
obtain	O
ˆfbag	O
(	O
x	O
)	O
=	O
b	O
(	O
cid:17	O
)	O
b=1	O
1	O
b	O
∗b	O
(	O
x	O
)	O
.	O
ˆf	O
this	O
is	O
called	O
bagging	B
.	O
while	O
bagging	B
can	O
improve	O
predictions	O
for	O
many	O
regression	B
methods	O
,	O
it	O
is	O
particularly	O
useful	O
for	O
decision	O
trees	O
.	O
to	O
apply	O
bagging	B
to	O
regression	B
trees	O
,	O
we	O
simply	O
construct	O
b	O
regression	B
trees	O
using	O
b	O
bootstrapped	O
training	B
sets	O
,	O
and	O
average	B
the	O
resulting	O
predictions	O
.	O
these	O
trees	O
are	O
grown	O
deep	O
,	O
and	O
are	O
not	O
pruned	O
.	O
hence	O
each	O
individual	O
tree	B
has	O
high	O
variance	B
,	O
but	O
low	O
bias	B
.	O
averaging	O
these	O
b	O
trees	O
reduces	O
the	O
variance	B
.	O
bagging	B
has	O
been	O
demonstrated	O
to	O
give	O
impressive	O
improvements	O
in	O
accuracy	O
by	O
combining	O
together	O
hundreds	O
or	O
even	O
thousands	O
of	O
trees	O
into	O
a	O
single	B
procedure	O
.	O
thus	O
far	O
,	O
we	O
have	O
described	O
the	O
bagging	B
procedure	O
in	O
the	O
regression	B
context	O
,	O
to	O
predict	O
a	O
quantitative	B
outcome	O
y	O
.	O
how	O
can	O
bagging	B
be	O
extended	O
to	O
a	O
classiﬁcation	B
problem	O
where	O
y	O
is	O
qualitative	B
?	O
in	O
that	O
situation	O
,	O
there	O
are	O
a	O
few	O
possible	O
approaches	O
,	O
but	O
the	O
simplest	O
is	O
as	O
follows	O
.	O
for	O
a	O
given	O
test	B
observation	O
,	O
we	O
can	O
record	O
the	O
class	O
predicted	O
by	O
each	O
of	O
the	O
b	O
trees	O
,	O
and	O
take	O
a	O
majority	B
vote	I
:	O
the	O
overall	O
prediction	B
is	O
the	O
most	O
commonly	O
occurring	O
class	O
among	O
the	O
b	O
predictions	O
.	O
figure	O
8.8	O
shows	O
the	O
results	O
from	O
bagging	B
trees	O
on	O
the	O
heart	O
data	B
.	O
the	O
test	B
error	O
rate	B
is	O
shown	O
as	O
a	O
function	B
of	O
b	O
,	O
the	O
number	O
of	O
trees	O
constructed	O
using	O
bootstrapped	O
training	B
data	O
sets	O
.	O
we	O
see	O
that	O
the	O
bagging	B
test	O
error	B
rate	I
is	O
slightly	O
lower	O
in	O
this	O
case	O
than	O
the	O
test	B
error	O
rate	B
obtained	O
from	O
a	O
single	B
tree	O
.	O
the	O
number	O
of	O
trees	O
b	O
is	O
not	O
a	O
critical	O
parameter	B
with	O
bagging	B
;	O
using	O
a	O
very	O
large	O
value	O
of	O
b	O
will	O
not	O
lead	O
to	O
overﬁtting	B
.	O
in	O
practice	O
we	O
use	O
a	O
value	O
of	O
b	O
suﬃciently	O
large	O
that	O
the	O
error	B
has	O
settled	O
down	O
.	O
using	O
b	O
=	O
100	O
is	O
suﬃcient	O
to	O
achieve	O
good	O
performance	O
in	O
this	O
example	O
.	O
out-of-bag	B
error	O
estimation	O
it	O
turns	O
out	O
that	O
there	O
is	O
a	O
very	O
straightforward	O
way	O
to	O
estimate	O
the	O
test	B
error	O
of	O
a	O
bagged	O
model	B
,	O
without	O
the	O
need	O
to	O
perform	O
cross-validation	B
or	O
the	O
validation	B
set	I
approach	O
.	O
recall	B
that	O
the	O
key	O
to	O
bagging	B
is	O
that	O
trees	O
are	O
repeatedly	O
ﬁt	B
to	O
bootstrapped	O
subsets	O
of	O
the	O
observations	B
.	O
one	O
can	O
show	O
majority	B
vote	I
318	O
8.	O
tree-based	O
methods	O
0	O
3	O
0	O
.	O
5	O
2	O
0	O
.	O
r	O
o	O
r	O
r	O
e	O
0	O
2	O
.	O
0	O
5	O
1	O
.	O
0	O
0	O
1	O
0	O
.	O
test	B
:	O
bagging	B
test	O
:	O
randomforest	O
oob	O
:	O
bagging	B
oob	O
:	O
randomforest	O
0	O
50	O
100	O
150	O
200	O
250	O
300	O
number	O
of	O
trees	O
figure	O
8.8.	O
bagging	B
and	O
random	B
forest	I
results	O
for	O
the	O
heart	O
data	B
.	O
the	O
test	B
error	O
(	O
black	O
and	O
orange	O
)	O
is	O
shown	O
as	O
a	O
function	B
of	O
b	O
,	O
the	O
number	O
of	O
bootstrapped	O
p.	O
the	O
dashed	O
line	B
training	O
sets	O
used	O
.	O
random	O
forests	O
were	O
applied	O
with	O
m	O
=	O
indicates	O
the	O
test	B
error	O
resulting	O
from	O
a	O
single	B
classiﬁcation	O
tree	B
.	O
the	O
green	O
and	O
blue	O
traces	O
show	O
the	O
oob	O
error	B
,	O
which	O
in	O
this	O
case	O
is	O
considerably	O
lower	O
.	O
√	O
that	O
on	O
average	B
,	O
each	O
bagged	O
tree	B
makes	O
use	O
of	O
around	O
two-thirds	O
of	O
the	O
observations.3	O
the	O
remaining	O
one-third	O
of	O
the	O
observations	B
not	O
used	O
to	O
ﬁt	B
a	O
given	O
bagged	O
tree	B
are	O
referred	O
to	O
as	O
the	O
out-of-bag	B
(	O
oob	O
)	O
observations	B
.	O
we	O
can	O
predict	O
the	O
response	B
for	O
the	O
ith	O
observation	O
using	O
each	O
of	O
the	O
trees	O
in	O
which	O
that	O
observation	O
was	O
oob	O
.	O
this	O
will	O
yield	O
around	O
b/3	O
predictions	O
for	O
the	O
ith	O
observation	O
.	O
in	O
order	O
to	O
obtain	O
a	O
single	B
prediction	O
for	O
the	O
ith	O
observation	O
,	O
we	O
can	O
average	B
these	O
predicted	O
responses	O
(	O
if	O
regression	B
is	O
the	O
goal	O
)	O
or	O
can	O
take	O
a	O
majority	B
vote	I
(	O
if	O
classiﬁcation	B
is	O
the	O
goal	O
)	O
.	O
this	O
leads	O
to	O
a	O
single	B
oob	O
prediction	B
for	O
the	O
ith	O
observation	O
.	O
an	O
oob	O
prediction	B
can	O
be	O
obtained	O
in	O
this	O
way	O
for	O
each	O
of	O
the	O
n	O
observations	B
,	O
from	O
which	O
the	O
overall	O
oob	O
mse	O
(	O
for	O
a	O
regression	B
problem	O
)	O
or	O
classiﬁcation	B
error	O
(	O
for	O
a	O
classiﬁcation	B
problem	O
)	O
can	O
be	O
computed	O
.	O
the	O
resulting	O
oob	O
error	B
is	O
a	O
valid	O
estimate	O
of	O
the	O
test	B
error	O
for	O
the	O
bagged	O
model	B
,	O
since	O
the	O
response	B
for	O
each	O
observation	O
is	O
predicted	O
using	O
only	O
the	O
trees	O
that	O
were	O
not	O
ﬁt	B
using	O
that	O
observation	O
.	O
figure	O
8.8	O
displays	O
the	O
oob	O
error	B
on	O
the	O
heart	O
data	B
.	O
it	O
can	O
be	O
shown	O
that	O
with	O
b	O
suﬃciently	O
large	O
,	O
oob	O
error	B
is	O
virtually	O
equivalent	O
to	O
leave-one-out	B
cross-validation	O
error	B
.	O
the	O
oob	O
approach	B
for	O
estimating	O
out-of-bag	B
3this	O
relates	O
to	O
exercise	O
2	O
of	O
chapter	O
5	O
.	O
8.2	O
bagging	B
,	O
random	O
forests	O
,	O
boosting	B
319	O
the	O
test	B
error	O
is	O
particularly	O
convenient	O
when	O
performing	O
bagging	B
on	O
large	O
data	B
sets	O
for	O
which	O
cross-validation	B
would	O
be	O
computationally	O
onerous	O
.	O
variable	B
importance	O
measures	O
as	O
we	O
have	O
discussed	O
,	O
bagging	B
typically	O
results	O
in	O
improved	O
accuracy	O
over	O
prediction	B
using	O
a	O
single	B
tree	O
.	O
unfortunately	O
,	O
however	O
,	O
it	O
can	O
be	O
diﬃcult	O
to	O
interpret	O
the	O
resulting	O
model	B
.	O
recall	B
that	O
one	O
of	O
the	O
advantages	O
of	O
decision	O
trees	O
is	O
the	O
attractive	O
and	O
easily	O
interpreted	O
diagram	O
that	O
results	O
,	O
such	O
as	O
the	O
one	O
displayed	O
in	O
figure	O
8.1.	O
however	O
,	O
when	O
we	O
bag	O
a	O
large	O
number	O
of	O
trees	O
,	O
it	O
is	O
no	O
longer	O
possible	O
to	O
represent	O
the	O
resulting	O
statistical	O
learning	O
procedure	O
using	O
a	O
single	B
tree	O
,	O
and	O
it	O
is	O
no	O
longer	O
clear	O
which	O
variables	O
are	O
most	O
important	O
to	O
the	O
procedure	O
.	O
thus	O
,	O
bagging	B
improves	O
prediction	B
accuracy	O
at	O
the	O
expense	O
of	O
interpretability	B
.	O
although	O
the	O
collection	O
of	O
bagged	O
trees	O
is	O
much	O
more	O
diﬃcult	O
to	O
interpret	O
than	O
a	O
single	B
tree	O
,	O
one	O
can	O
obtain	O
an	O
overall	O
summary	O
of	O
the	O
importance	B
of	O
each	O
predictor	B
using	O
the	O
rss	O
(	O
for	O
bagging	O
regression	B
trees	O
)	O
or	O
the	O
gini	O
index	O
(	O
for	O
bagging	O
classiﬁcation	B
trees	O
)	O
.	O
in	O
the	O
case	O
of	O
bagging	B
regression	O
trees	O
,	O
we	O
can	O
record	O
the	O
total	O
amount	O
that	O
the	O
rss	O
(	O
8.1	O
)	O
is	O
decreased	O
due	O
to	O
splits	O
over	O
a	O
given	O
predictor	B
,	O
averaged	O
over	O
all	O
b	O
trees	O
.	O
a	O
large	O
value	O
indicates	O
an	O
important	O
predictor	B
.	O
similarly	O
,	O
in	O
the	O
context	O
of	O
bagging	B
classiﬁcation	O
trees	O
,	O
we	O
can	O
add	O
up	O
the	O
total	O
amount	O
that	O
the	O
gini	O
index	O
(	O
8.6	O
)	O
is	O
decreased	O
by	O
splits	O
over	O
a	O
given	O
predictor	B
,	O
averaged	O
over	O
all	O
b	O
trees	O
.	O
a	O
graphical	O
representation	O
of	O
the	O
variable	B
importances	O
in	O
the	O
heart	O
data	B
is	O
shown	O
in	O
figure	O
8.9.	O
we	O
see	O
the	O
mean	O
decrease	O
in	O
gini	O
index	O
for	O
each	O
vari-	O
able	O
,	O
relative	O
to	O
the	O
largest	O
.	O
the	O
variables	O
with	O
the	O
largest	O
mean	O
decrease	O
in	O
gini	O
index	O
are	O
thal	O
,	O
ca	O
,	O
and	O
chestpain	O
.	O
8.2.2	O
random	O
forests	O
random	O
forests	O
provide	O
an	O
improvement	O
over	O
bagged	O
trees	O
by	O
way	O
of	O
a	O
small	O
tweak	O
that	O
decorrelates	O
the	O
trees	O
.	O
as	O
in	O
bagging	B
,	O
we	O
build	O
a	O
number	O
of	O
decision	O
trees	O
on	O
bootstrapped	O
training	B
samples	O
.	O
but	O
when	O
building	O
these	O
decision	O
trees	O
,	O
each	O
time	O
a	O
split	O
in	O
a	O
tree	B
is	O
considered	O
,	O
a	O
random	O
sample	O
of	O
m	O
predictors	O
is	O
chosen	O
as	O
split	O
candidates	O
from	O
the	O
full	O
set	B
of	O
p	O
predictors	O
.	O
the	O
split	O
is	O
allowed	O
to	O
use	O
only	O
one	O
of	O
those	O
m	O
predictors	O
.	O
a	O
fresh	O
sample	O
of	O
p—that	O
is	O
,	O
the	O
number	O
of	O
predictors	O
considered	O
at	O
each	O
split	O
is	O
approximately	O
equal	O
to	O
the	O
square	O
root	O
of	O
the	O
total	O
number	O
of	O
predictors	O
(	O
4	O
out	O
of	O
the	O
13	O
for	O
the	O
heart	O
data	B
)	O
.	O
m	O
predictors	O
is	O
taken	O
at	O
each	O
split	O
,	O
and	O
typically	O
we	O
choose	O
m	O
≈	O
√	O
in	O
other	O
words	O
,	O
in	O
building	O
a	O
random	B
forest	I
,	O
at	O
each	O
split	O
in	O
the	O
tree	B
,	O
the	O
algorithm	O
is	O
not	O
even	O
allowed	O
to	O
consider	O
a	O
majority	O
of	O
the	O
available	O
predictors	O
.	O
this	O
may	O
sound	O
crazy	O
,	O
but	O
it	O
has	O
a	O
clever	O
rationale	O
.	O
suppose	O
that	O
there	O
is	O
one	O
very	O
strong	O
predictor	B
in	O
the	O
data	B
set	O
,	O
along	O
with	O
a	O
num-	O
ber	O
of	O
other	O
moderately	O
strong	O
predictors	O
.	O
then	O
in	O
the	O
collection	O
of	O
bagged	O
variable	B
importance	O
random	B
forest	I
320	O
8.	O
tree-based	O
methods	O
fbs	O
restecg	O
exang	O
sex	O
slope	B
chol	O
age	O
restbp	O
maxhr	O
oldpeak	O
chestpain	O
ca	O
thal	O
0	O
20	O
40	O
60	O
80	O
100	O
variable	B
importance	O
figure	O
8.9.	O
a	O
variable	B
importance	O
plot	B
for	O
the	O
heart	O
data	B
.	O
variable	B
impor-	O
tance	O
is	O
computed	O
using	O
the	O
mean	O
decrease	O
in	O
gini	O
index	O
,	O
and	O
expressed	O
relative	O
to	O
the	O
maximum	O
.	O
trees	O
,	O
most	O
or	O
all	O
of	O
the	O
trees	O
will	O
use	O
this	O
strong	O
predictor	B
in	O
the	O
top	O
split	O
.	O
consequently	O
,	O
all	O
of	O
the	O
bagged	O
trees	O
will	O
look	O
quite	O
similar	O
to	O
each	O
other	O
.	O
hence	O
the	O
predictions	O
from	O
the	O
bagged	O
trees	O
will	O
be	O
highly	O
correlated	O
.	O
un-	O
fortunately	O
,	O
averaging	O
many	O
highly	O
correlated	O
quantities	O
does	O
not	O
lead	O
to	O
as	O
large	O
of	O
a	O
reduction	O
in	O
variance	B
as	O
averaging	O
many	O
uncorrelated	O
quanti-	O
ties	O
.	O
in	O
particular	O
,	O
this	O
means	O
that	O
bagging	B
will	O
not	O
lead	O
to	O
a	O
substantial	O
reduction	O
in	O
variance	B
over	O
a	O
single	B
tree	O
in	O
this	O
setting	O
.	O
random	O
forests	O
overcome	O
this	O
problem	O
by	O
forcing	O
each	O
split	O
to	O
consider	O
only	O
a	O
subset	O
of	O
the	O
predictors	O
.	O
therefore	O
,	O
on	O
average	B
(	O
p	O
−	O
m	O
)	O
/p	O
of	O
the	O
splits	O
will	O
not	O
even	O
consider	O
the	O
strong	O
predictor	B
,	O
and	O
so	O
other	O
predictors	O
will	O
have	O
more	O
of	O
a	O
chance	O
.	O
we	O
can	O
think	O
of	O
this	O
process	O
as	O
decorrelating	O
the	O
trees	O
,	O
thereby	O
making	O
the	O
average	B
of	O
the	O
resulting	O
trees	O
less	O
variable	B
and	O
hence	O
more	O
reliable	O
.	O
the	O
main	O
diﬀerence	O
between	O
bagging	B
and	O
random	O
forests	O
is	O
the	O
choice	O
of	O
predictor	B
subset	O
size	O
m.	O
for	O
instance	O
,	O
if	O
a	O
random	B
forest	I
is	O
built	O
using	O
√	O
m	O
=	O
p	O
,	O
then	O
this	O
amounts	O
simply	O
to	O
bagging	B
.	O
on	O
the	O
heart	O
data	B
,	O
random	O
p	O
leads	O
to	O
a	O
reduction	O
in	O
both	O
test	B
error	O
and	O
oob	O
error	B
forests	O
using	O
m	O
=	O
over	O
bagging	B
(	O
figure	O
8.8	O
)	O
.	O
using	O
a	O
small	O
value	O
of	O
m	O
in	O
building	O
a	O
random	B
forest	I
will	O
typically	O
be	O
helpful	O
when	O
we	O
have	O
a	O
large	O
number	O
of	O
correlated	O
predictors	O
.	O
we	O
applied	O
random	O
forests	O
to	O
a	O
high-dimensional	B
biological	O
data	B
set	O
consisting	O
of	O
ex-	O
pression	O
measurements	O
of	O
4,718	O
genes	O
measured	O
on	O
tissue	O
samples	O
from	O
349	O
patients	O
.	O
there	O
are	O
around	O
20,000	O
genes	O
in	O
humans	O
,	O
and	O
individual	O
genes	O
8.2	O
bagging	B
,	O
random	O
forests	O
,	O
boosting	B
321	O
have	O
diﬀerent	O
levels	O
of	O
activity	O
,	O
or	O
expression	O
,	O
in	O
particular	O
cells	O
,	O
tissues	O
,	O
and	O
biological	O
conditions	O
.	O
in	O
this	O
data	B
set	O
,	O
each	O
of	O
the	O
patient	O
samples	O
has	O
a	O
qualitative	B
label	O
with	O
15	O
diﬀerent	O
levels	O
:	O
either	O
normal	O
or	O
1	O
of	O
14	O
diﬀerent	O
types	O
of	O
cancer	O
.	O
our	O
goal	O
was	O
to	O
use	O
random	O
forests	O
to	O
predict	O
cancer	O
type	O
based	O
on	O
the	O
500	O
genes	O
that	O
have	O
the	O
largest	O
variance	B
in	O
the	O
training	B
set	O
.	O
we	O
randomly	O
divided	O
the	O
observations	B
into	O
a	O
training	B
and	O
a	O
test	B
set	O
,	O
and	O
applied	O
random	O
forests	O
to	O
the	O
training	B
set	O
for	O
three	O
diﬀerent	O
values	O
of	O
the	O
number	O
of	O
splitting	O
variables	O
m.	O
the	O
results	O
are	O
shown	O
in	O
figure	O
8.10.	O
the	O
error	B
rate	I
of	O
a	O
single	B
tree	O
is	O
45.7	O
%	O
,	O
and	O
the	O
null	B
rate	O
is	O
75.4	O
%	O
.4	O
we	O
see	O
that	O
√	O
using	O
400	O
trees	O
is	O
suﬃcient	O
to	O
give	O
good	O
performance	O
,	O
and	O
that	O
the	O
choice	O
m	O
=	O
p	O
gave	O
a	O
small	O
improvement	O
in	O
test	B
error	O
over	O
bagging	B
(	O
m	O
=	O
p	O
)	O
in	O
this	O
example	O
.	O
as	O
with	O
bagging	B
,	O
random	O
forests	O
will	O
not	O
overﬁt	O
if	O
we	O
increase	O
b	O
,	O
so	O
in	O
practice	O
we	O
use	O
a	O
value	O
of	O
b	O
suﬃciently	O
large	O
for	O
the	O
error	B
rate	I
to	O
have	O
settled	O
down	O
.	O
8.2.3	O
boosting	B
we	O
now	O
discuss	O
boosting	B
,	O
yet	O
another	O
approach	B
for	O
improving	O
the	O
predic-	O
tions	O
resulting	O
from	O
a	O
decision	B
tree	I
.	O
like	O
bagging	B
,	O
boosting	B
is	O
a	O
general	O
approach	B
that	O
can	O
be	O
applied	O
to	O
many	O
statistical	O
learning	O
methods	O
for	O
re-	O
gression	O
or	O
classiﬁcation	B
.	O
here	O
we	O
restrict	O
our	O
discussion	O
of	O
boosting	B
to	O
the	O
context	O
of	O
decision	O
trees	O
.	O
recall	B
that	O
bagging	B
involves	O
creating	O
multiple	B
copies	O
of	O
the	O
original	O
train-	O
ing	O
data	B
set	O
using	O
the	O
bootstrap	B
,	O
ﬁtting	O
a	O
separate	O
decision	B
tree	I
to	O
each	O
copy	O
,	O
and	O
then	O
combining	O
all	O
of	O
the	O
trees	O
in	O
order	O
to	O
create	O
a	O
single	B
predic-	O
tive	O
model	B
.	O
notably	O
,	O
each	O
tree	B
is	O
built	O
on	O
a	O
bootstrap	B
data	O
set	B
,	O
independent	B
of	O
the	O
other	O
trees	O
.	O
boosting	B
works	O
in	O
a	O
similar	O
way	O
,	O
except	O
that	O
the	O
trees	O
are	O
grown	O
sequentially	O
:	O
each	O
tree	B
is	O
grown	O
using	O
information	O
from	O
previously	O
grown	O
trees	O
.	O
boosting	B
does	O
not	O
involve	O
bootstrap	B
sampling	O
;	O
instead	O
each	O
tree	B
is	O
ﬁt	B
on	O
a	O
modiﬁed	O
version	O
of	O
the	O
original	O
data	B
set	O
.	O
consider	O
ﬁrst	O
the	O
regression	B
setting	O
.	O
like	O
bagging	B
,	O
boosting	B
involves	O
com-	O
bining	O
a	O
large	O
number	O
of	O
decision	O
trees	O
,	O
ˆf	O
1	O
,	O
.	O
.	O
.	O
,	O
ˆf	O
b.	O
boosting	B
is	O
described	O
in	O
algorithm	O
8.2.	O
what	O
is	O
the	O
idea	O
behind	O
this	O
procedure	O
?	O
unlike	O
ﬁtting	O
a	O
single	B
large	O
deci-	O
sion	O
tree	B
to	O
the	O
data	B
,	O
which	O
amounts	O
to	O
ﬁtting	O
the	O
data	B
hard	O
and	O
potentially	O
overﬁtting	B
,	O
the	O
boosting	B
approach	O
instead	O
learns	O
slowly	O
.	O
given	O
the	O
current	O
model	B
,	O
we	O
ﬁt	B
a	O
decision	B
tree	I
to	O
the	O
residuals	B
from	O
the	O
model	B
.	O
that	O
is	O
,	O
we	O
ﬁt	B
a	O
tree	B
using	O
the	O
current	O
residuals	B
,	O
rather	O
than	O
the	O
outcome	O
y	O
,	O
as	O
the	O
re-	O
sponse	O
.	O
we	O
then	O
add	O
this	O
new	O
decision	B
tree	I
into	O
the	O
ﬁtted	O
function	O
in	O
order	O
to	O
update	O
the	O
residuals	B
.	O
each	O
of	O
these	O
trees	O
can	O
be	O
rather	O
small	O
,	O
with	O
just	O
a	O
few	O
terminal	B
nodes	O
,	O
determined	O
by	O
the	O
parameter	B
d	O
in	O
the	O
algorithm	O
.	O
by	O
4the	O
null	B
rate	O
results	O
from	O
simply	O
classifying	O
each	O
observation	O
to	O
the	O
dominant	O
class	O
overall	O
,	O
which	O
is	O
in	O
this	O
case	O
the	O
normal	O
class	O
.	O
boosting	B
322	O
8.	O
tree-based	O
methods	O
m=p	O
m=p/2	O
m=	O
p	O
5	O
.	O
0	O
4	O
.	O
0	O
3	O
.	O
0	O
2	O
0	O
.	O
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
c	O
i	O
f	O
i	O
s	O
s	O
a	O
c	O
l	O
t	O
s	O
e	O
t	O
0	O
100	O
200	O
300	O
400	O
500	O
number	O
of	O
trees	O
figure	O
8.10.	O
results	O
from	O
random	O
forests	O
for	O
the	O
15-class	O
gene	O
expression	O
data	B
set	O
with	O
p	O
=	O
500	O
predictors	O
.	O
the	O
test	B
error	O
is	O
displayed	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
trees	O
.	O
each	O
colored	O
line	B
corresponds	O
to	O
a	O
diﬀerent	O
value	O
of	O
m	O
,	O
the	O
number	O
of	O
predictors	O
available	O
for	O
splitting	O
at	O
each	O
interior	O
tree	B
node	O
.	O
random	O
forests	O
(	O
m	O
<	O
p	O
)	O
lead	O
to	O
a	O
slight	O
improvement	O
over	O
bagging	B
(	O
m	O
=	O
p	O
)	O
.	O
a	O
single	B
classiﬁcation	O
tree	B
has	O
an	O
error	B
rate	I
of	O
45.7	O
%	O
.	O
ﬁtting	O
small	O
trees	O
to	O
the	O
residuals	B
,	O
we	O
slowly	O
improve	O
ˆf	O
in	O
areas	O
where	O
it	O
does	O
not	O
perform	O
well	O
.	O
the	O
shrinkage	B
parameter	O
λ	O
slows	O
the	O
process	O
down	O
even	O
further	O
,	O
allowing	O
more	O
and	O
diﬀerent	O
shaped	O
trees	O
to	O
attack	O
the	O
resid-	O
uals	O
.	O
in	O
general	O
,	O
statistical	O
learning	O
approaches	O
that	O
learn	O
slowly	O
tend	O
to	O
perform	O
well	O
.	O
note	O
that	O
in	O
boosting	B
,	O
unlike	O
in	O
bagging	B
,	O
the	O
construction	O
of	O
each	O
tree	B
depends	O
strongly	O
on	O
the	O
trees	O
that	O
have	O
already	O
been	O
grown	O
.	O
we	O
have	O
just	O
described	O
the	O
process	O
of	O
boosting	B
regression	O
trees	O
.	O
boosting	B
classiﬁcation	O
trees	O
proceeds	O
in	O
a	O
similar	O
but	O
slightly	O
more	O
complex	O
way	O
,	O
and	O
the	O
details	O
are	O
omitted	O
here	O
.	O
boosting	B
has	O
three	O
tuning	O
parameters	O
:	O
1.	O
the	O
number	O
of	O
trees	O
b.	O
unlike	O
bagging	B
and	O
random	O
forests	O
,	O
boosting	B
can	O
overﬁt	O
if	O
b	O
is	O
too	O
large	O
,	O
although	O
this	O
overﬁtting	B
tends	O
to	O
occur	O
slowly	O
if	O
at	O
all	O
.	O
we	O
use	O
cross-validation	B
to	O
select	O
b	O
.	O
2.	O
the	O
shrinkage	B
parameter	O
λ	O
,	O
a	O
small	O
positive	O
number	O
.	O
this	O
controls	O
the	O
rate	B
at	O
which	O
boosting	B
learns	O
.	O
typical	O
values	O
are	O
0.01	O
or	O
0.001	O
,	O
and	O
the	O
right	O
choice	O
can	O
depend	O
on	O
the	O
problem	O
.	O
very	O
small	O
λ	O
can	O
require	O
using	O
a	O
very	O
large	O
value	O
of	O
b	O
in	O
order	O
to	O
achieve	O
good	O
performance	O
.	O
3.	O
the	O
number	O
d	O
of	O
splits	O
in	O
each	O
tree	B
,	O
which	O
controls	O
the	O
complexity	O
of	O
the	O
boosted	O
ensemble	O
.	O
often	O
d	O
=	O
1	O
works	O
well	O
,	O
in	O
which	O
case	O
each	O
tree	B
is	O
a	O
stump	B
,	O
consisting	O
of	O
a	O
single	B
split	O
.	O
in	O
this	O
case	O
,	O
the	O
boosted	O
ensemble	O
is	O
ﬁtting	O
an	O
additive	B
model	O
,	O
since	O
each	O
term	B
involves	O
only	O
a	O
single	B
variable	O
.	O
more	O
generally	O
d	O
is	O
the	O
interaction	B
depth	O
,	O
and	O
controls	O
stump	B
interaction	O
depth	O
8.3	O
lab	O
:	O
decision	O
trees	O
323	O
algorithm	O
8.2	O
boosting	B
for	O
regression	B
trees	O
1.	O
set	B
ˆf	O
(	O
x	O
)	O
=	O
0	O
and	O
ri	O
=	O
yi	O
for	O
all	O
i	O
in	O
the	O
training	B
set	O
.	O
2.	O
for	O
b	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
,	O
b	O
,	O
repeat	O
:	O
(	O
a	O
)	O
fit	O
a	O
tree	B
ˆf	O
b	O
with	O
d	O
splits	O
(	O
d	O
+	O
1	O
terminal	B
nodes	O
)	O
to	O
the	O
training	B
data	O
(	O
x	O
,	O
r	O
)	O
.	O
(	O
b	O
)	O
update	O
ˆf	O
by	O
adding	O
in	O
a	O
shrunken	O
version	O
of	O
the	O
new	O
tree	B
:	O
ˆf	O
(	O
x	O
)	O
←	O
ˆf	O
(	O
x	O
)	O
+	O
λ	O
ˆf	O
b	O
(	O
x	O
)	O
.	O
(	O
c	O
)	O
update	O
the	O
residuals	B
,	O
ri	O
←	O
ri	O
−	O
λ	O
ˆf	O
b	O
(	O
xi	O
)	O
.	O
3.	O
output	B
the	O
boosted	O
model	B
,	O
ˆf	O
(	O
x	O
)	O
=	O
b	O
(	O
cid:17	O
)	O
b=1	O
λ	O
ˆf	O
b	O
(	O
x	O
)	O
.	O
(	O
8.10	O
)	O
(	O
8.11	O
)	O
(	O
8.12	O
)	O
the	O
interaction	B
order	O
of	O
the	O
boosted	O
model	B
,	O
since	O
d	O
splits	O
can	O
involve	O
at	O
most	O
d	O
variables	O
.	O
in	O
figure	O
8.11	O
,	O
we	O
applied	O
boosting	B
to	O
the	O
15-class	O
cancer	O
gene	O
expression	O
data	B
set	O
,	O
in	O
order	O
to	O
develop	O
a	O
classiﬁer	B
that	O
can	O
distinguish	O
the	O
normal	O
class	O
from	O
the	O
14	O
cancer	O
classes	O
.	O
we	O
display	O
the	O
test	B
error	O
as	O
a	O
function	B
of	O
the	O
total	O
number	O
of	O
trees	O
and	O
the	O
interaction	B
depth	O
d.	O
we	O
see	O
that	O
simple	B
stumps	O
with	O
an	O
interaction	B
depth	O
of	O
one	O
perform	O
well	O
if	O
enough	O
of	O
them	O
are	O
included	O
.	O
this	O
model	B
outperforms	O
the	O
depth-two	O
model	B
,	O
and	O
both	O
out-	O
perform	O
a	O
random	B
forest	I
.	O
this	O
highlights	O
one	O
diﬀerence	O
between	O
boosting	B
and	O
random	O
forests	O
:	O
in	O
boosting	B
,	O
because	O
the	O
growth	O
of	O
a	O
particular	O
tree	B
takes	O
into	O
account	O
the	O
other	O
trees	O
that	O
have	O
already	O
been	O
grown	O
,	O
smaller	O
trees	O
are	O
typically	O
suﬃcient	O
.	O
using	O
smaller	O
trees	O
can	O
aid	O
in	O
interpretability	B
as	O
well	O
;	O
for	O
instance	O
,	O
using	O
stumps	O
leads	O
to	O
an	O
additive	B
model	O
.	O
8.3	O
lab	O
:	O
decision	O
trees	O
8.3.1	O
fitting	O
classiﬁcation	B
trees	O
the	O
tree	B
library	O
is	O
used	O
to	O
construct	O
classiﬁcation	B
and	O
regression	B
trees	O
.	O
>	O
library	O
(	O
tree	B
)	O
324	O
8.	O
tree-based	O
methods	O
r	O
o	O
r	O
r	O
e	O
n	O
o	O
i	O
t	O
a	O
c	O
i	O
f	O
i	O
s	O
s	O
a	O
c	O
l	O
t	O
s	O
e	O
t	O
boosting	B
:	O
depth=1	O
boosting	B
:	O
depth=2	O
randomforest	O
:	O
m=	O
p	O
5	O
2	O
.	O
0	O
0	O
2	O
.	O
0	O
5	O
1	O
.	O
0	O
0	O
1	O
.	O
0	O
5	O
0	O
0	O
.	O
0	O
1000	O
2000	O
3000	O
4000	O
5000	O
number	O
of	O
trees	O
figure	O
8.11.	O
results	O
from	O
performing	O
boosting	B
and	O
random	O
forests	O
on	O
the	O
15-class	O
gene	O
expression	O
data	B
set	O
in	O
order	O
to	O
predict	O
cancer	O
versus	O
normal	O
.	O
the	O
test	B
error	O
is	O
displayed	O
as	O
a	O
function	B
of	O
the	O
number	O
of	O
trees	O
.	O
for	O
the	O
two	O
boosted	O
models	O
,	O
λ	O
=	O
0.01.	O
depth-1	O
trees	O
slightly	O
outperform	O
depth-2	O
trees	O
,	O
and	O
both	O
out-	O
perform	O
the	O
random	B
forest	I
,	O
although	O
the	O
standard	O
errors	O
are	O
around	O
0.02	O
,	O
making	O
none	O
of	O
these	O
diﬀerences	O
signiﬁcant	O
.	O
the	O
test	B
error	O
rate	B
for	O
a	O
single	B
tree	O
is	O
24	O
%	O
.	O
we	O
ﬁrst	O
use	O
classiﬁcation	B
trees	O
to	O
analyze	O
the	O
carseats	O
data	B
set	O
.	O
in	O
these	O
data	B
,	O
sales	O
is	O
a	O
continuous	B
variable	O
,	O
and	O
so	O
we	O
begin	O
by	O
recoding	O
it	O
as	O
a	O
binary	B
variable	O
.	O
we	O
use	O
the	O
ifelse	O
(	O
)	O
function	B
to	O
create	O
a	O
variable	B
,	O
called	O
high	O
,	O
which	O
takes	O
on	O
a	O
value	O
of	O
yes	O
if	O
the	O
sales	O
variable	B
exceeds	O
8	O
,	O
and	O
takes	O
on	O
a	O
value	O
of	O
no	O
otherwise	O
.	O
ifelse	O
(	O
)	O
>	O
library	O
(	O
islr	O
)	O
>	O
attach	O
(	O
carseats	O
)	O
>	O
high	O
=	O
ifelse	O
(	O
sales	O
<	O
=8	O
,	O
''	O
no	O
``	O
,	O
''	O
yes	O
``	O
)	O
finally	O
,	O
we	O
use	O
the	O
data.frame	O
(	O
)	O
function	B
to	O
merge	O
high	O
with	O
the	O
rest	O
of	O
the	O
carseats	O
data	B
.	O
>	O
carseats	O
=	O
data	B
.	O
frame	O
(	O
carseats	O
,	O
high	O
)	O
we	O
now	O
use	O
the	O
tree	B
(	O
)	O
function	B
to	O
ﬁt	B
a	O
classiﬁcation	B
tree	O
in	O
order	O
to	O
predict	O
high	O
using	O
all	O
variables	O
but	O
sales	O
.	O
the	O
syntax	O
of	O
the	O
tree	B
(	O
)	O
function	B
is	O
quite	O
similar	O
to	O
that	O
of	O
the	O
lm	O
(	O
)	O
function	B
.	O
>	O
tree	B
.	O
carseats	O
=	O
tree	B
(	O
high∼	O
.	O
-	O
sales	O
,	O
carseats	O
)	O
tree	B
(	O
)	O
the	O
summary	O
(	O
)	O
function	B
lists	O
the	O
variables	O
that	O
are	O
used	O
as	O
internal	B
nodes	O
in	O
the	O
tree	B
,	O
the	O
number	O
of	O
terminal	B
nodes	O
,	O
and	O
the	O
(	O
training	B
)	O
error	B
rate	I
.	O
>	O
summary	O
(	O
tree	B
.	O
carseats	O
)	O
c	O
l	O
a	O
s	O
s	O
i	O
f	O
i	O
c	O
a	O
t	O
i	O
o	O
n	O
tree	B
:	O
tree	B
(	O
formula	O
=	O
high	O
∼	O
.	O
-	O
sales	O
,	O
data	B
=	O
carseats	O
)	O
variables	O
actually	O
used	O
in	O
tree	B
c	O
o	O
n	O
s	O
t	O
r	O
u	O
c	O
t	O
i	O
o	O
n	O
:	O
[	O
1	O
]	O
``	O
shelveloc	O
``	O
''	O
income	O
``	O
''	O
price	O
``	O
''	O
compprice	O
``	O
8.3	O
lab	O
:	O
decision	O
trees	O
325	O
[	O
5	O
]	O
``	O
populatio	O
n	O
``	O
number	O
of	O
terminal	B
nodes	O
:	O
residual	B
mean	O
deviance	B
:	O
0.4575	O
=	O
170.7	O
/	O
373	O
m	O
i	O
s	O
c	O
l	O
a	O
s	O
s	O
i	O
f	O
i	O
c	O
a	O
t	O
i	O
o	O
n	O
error	B
rate	I
:	O
0.09	O
=	O
36	O
/	O
400	O
''	O
advertisi	O
n	O
g	O
``	O
``	O
age	O
``	O
27	O
''	O
us	O
``	O
we	O
see	O
that	O
the	O
training	B
error	O
rate	B
is	O
9	O
%	O
.	O
for	O
classiﬁcation	O
trees	O
,	O
the	O
de-	O
viance	O
reported	O
in	O
the	O
output	B
of	O
summary	O
(	O
)	O
is	O
given	O
by	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
−2	O
nmk	O
log	O
ˆpmk	O
,	O
m	O
k	O
where	O
nmk	O
is	O
the	O
number	O
of	O
observations	B
in	O
the	O
mth	O
terminal	B
node	O
that	O
belong	O
to	O
the	O
kth	O
class	O
.	O
a	O
small	O
deviance	B
indicates	O
a	O
tree	B
that	O
provides	O
a	O
good	O
ﬁt	B
to	O
the	O
(	O
training	B
)	O
data	B
.	O
the	O
residual	B
mean	O
deviance	B
reported	O
is	O
simply	O
the	O
deviance	B
divided	O
by	O
n−|t0|	O
,	O
which	O
in	O
this	O
case	O
is	O
400−27	O
=	O
373.	O
one	O
of	O
the	O
most	O
attractive	O
properties	O
of	O
trees	O
is	O
that	O
they	O
can	O
be	O
graphically	O
displayed	O
.	O
we	O
use	O
the	O
plot	B
(	O
)	O
function	B
to	O
display	O
the	O
tree	B
struc-	O
ture	O
,	O
and	O
the	O
text	O
(	O
)	O
function	B
to	O
display	O
the	O
node	B
labels	O
.	O
the	O
argument	B
pretty=0	O
instructs	O
r	O
to	O
include	O
the	O
category	O
names	O
for	O
any	O
qualitative	B
pre-	O
dictors	O
,	O
rather	O
than	O
simply	O
displaying	O
a	O
letter	O
for	O
each	O
category	O
.	O
>	O
plot	B
(	O
tree	B
.	O
carseats	O
)	O
>	O
text	O
(	O
tree	B
.	O
carseats	O
,	O
pretty	O
=0	O
)	O
the	O
most	O
important	O
indicator	B
of	O
sales	O
appears	O
to	O
be	O
shelving	O
location	O
,	O
since	O
the	O
ﬁrst	O
branch	B
diﬀerentiates	O
good	O
locations	O
from	O
bad	O
and	O
medium	O
locations	O
.	O
if	O
we	O
just	O
type	O
the	O
name	O
of	O
the	O
tree	B
object	O
,	O
r	O
prints	O
output	B
corresponding	O
to	O
each	O
branch	B
of	O
the	O
tree	B
.	O
r	O
displays	O
the	O
split	O
criterion	O
(	O
e.g	O
.	O
price	O
<	O
92.5	O
)	O
,	O
the	O
number	O
of	O
observations	B
in	O
that	O
branch	B
,	O
the	O
deviance	B
,	O
the	O
overall	O
prediction	B
for	O
the	O
branch	B
(	O
yes	O
or	O
no	O
)	O
,	O
and	O
the	O
fraction	O
of	O
observations	B
in	O
that	O
branch	B
that	O
take	O
on	O
values	O
of	O
yes	O
and	O
no	O
.	O
branches	O
that	O
lead	O
to	O
terminal	B
nodes	O
are	O
indicated	O
using	O
asterisks	O
.	O
>	O
tree	B
.	O
carseats	O
node	B
)	O
,	O
split	O
,	O
n	O
,	O
deviance	B
,	O
yval	O
,	O
(	O
yprob	O
)	O
*	O
denotes	O
terminal	B
node	O
1	O
)	O
root	O
400	O
541.5	O
no	O
(	O
0.590	O
0.410	O
)	O
2	O
)	O
shelveloc	O
:	O
bad	O
,	O
medium	O
315	O
390.6	O
no	O
(	O
0.689	O
0.311	O
)	O
4	O
)	O
price	O
<	O
92.5	O
46	O
8	O
)	O
income	O
<	O
57	O
10	O
56.53	O
yes	O
(	O
0.304	O
0.696	O
)	O
12.22	O
no	O
(	O
0.700	O
0.300	O
)	O
in	O
order	O
to	O
properly	O
evaluate	O
the	O
performance	O
of	O
a	O
classiﬁcation	B
tree	O
on	O
these	O
data	B
,	O
we	O
must	O
estimate	O
the	O
test	B
error	O
rather	O
than	O
simply	O
computing	O
the	O
training	B
error	O
.	O
we	O
split	O
the	O
observations	B
into	O
a	O
training	B
set	O
and	O
a	O
test	B
set	O
,	O
build	O
the	O
tree	B
using	O
the	O
training	B
set	O
,	O
and	O
evaluate	O
its	O
performance	O
on	O
the	O
test	B
data	O
.	O
the	O
predict	O
(	O
)	O
function	B
can	O
be	O
used	O
for	O
this	O
purpose	O
.	O
in	O
the	O
case	O
of	O
a	O
classiﬁcation	B
tree	O
,	O
the	O
argument	B
type=	O
''	O
class	O
''	O
instructs	O
r	O
to	O
return	O
the	O
actual	O
class	O
prediction	B
.	O
this	O
approach	B
leads	O
to	O
correct	O
predictions	O
for	O
around	O
71.5	O
%	O
of	O
the	O
locations	O
in	O
the	O
test	B
data	O
set	B
.	O
326	O
8.	O
tree-based	O
methods	O
>	O
set	B
.	O
seed	B
(	O
2	O
)	O
>	O
train	B
=	O
sample	O
(	O
1	O
:	O
nrow	O
(	O
carseats	O
)	O
,	O
200	O
)	O
>	O
carseats	O
.	O
test	B
=	O
carseats	O
[	O
-	O
train	B
,	O
]	O
>	O
high	O
.	O
test	B
=	O
high	O
[	O
-	O
train	B
]	O
>	O
tree	B
.	O
carseats	O
=	O
tree	B
(	O
high∼	O
.	O
-	O
sales	O
,	O
carseats	O
,	O
subset	O
=	O
train	B
)	O
>	O
tree	B
.	O
pred	O
=	O
predict	O
(	O
tree	B
.	O
carseats	O
,	O
carseats	O
.	O
test	B
,	O
type	O
=	O
''	O
class	O
``	O
)	O
>	O
table	O
(	O
tree	B
.	O
pred	O
,	O
high	O
.	O
test	B
)	O
high	O
.	O
test	B
tree	O
.	O
pred	O
no	O
yes	O
27	O
57	O
86	O
no	O
yes	O
30	O
>	O
(	O
86+57	O
)	O
/200	O
[	O
1	O
]	O
0.715	O
next	O
,	O
we	O
consider	O
whether	O
pruning	B
the	O
tree	B
might	O
lead	O
to	O
improved	O
results	O
.	O
the	O
function	B
cv.tree	O
(	O
)	O
performs	O
cross-validation	B
in	O
order	O
to	O
determine	O
the	O
optimal	O
level	O
of	O
tree	B
complexity	O
;	O
cost	B
complexity	I
pruning	O
is	O
used	O
in	O
order	O
to	O
select	O
a	O
sequence	O
of	O
trees	O
for	O
consideration	O
.	O
we	O
use	O
the	O
argument	B
fun=prune.misclass	O
in	O
order	O
to	O
indicate	O
that	O
we	O
want	O
the	O
classiﬁcation	B
error	O
rate	B
to	O
guide	O
the	O
cross-validation	B
and	O
pruning	B
process	O
,	O
rather	O
than	O
the	O
default	O
for	O
the	O
cv.tree	O
(	O
)	O
function	B
,	O
which	O
is	O
deviance	B
.	O
the	O
cv.tree	O
(	O
)	O
function	B
reports	O
the	O
number	O
of	O
terminal	B
nodes	O
of	O
each	O
tree	B
con-	O
sidered	O
(	O
size	O
)	O
as	O
well	O
as	O
the	O
corresponding	O
error	B
rate	I
and	O
the	O
value	O
of	O
the	O
cost-complexity	O
parameter	B
used	O
(	O
k	O
,	O
which	O
corresponds	O
to	O
α	O
in	O
(	O
8.4	O
)	O
)	O
.	O
cv.tree	O
(	O
)	O
>	O
set	B
.	O
seed	B
(	O
3	O
)	O
>	O
cv	O
.	O
carseats	O
=	O
cv	O
.	O
tree	B
(	O
tree	B
.	O
carseats	O
,	O
fun	O
=	O
prune	O
.	O
misclass	O
)	O
>	O
names	O
(	O
cv	O
.	O
carseats	O
)	O
[	O
1	O
]	O
``	O
size	O
``	O
>	O
cv	O
.	O
carseats	O
$	O
size	O
[	O
1	O
]	O
19	O
17	O
14	O
13	O
''	O
method	O
``	O
''	O
dev	O
``	O
9	O
7	O
''	O
k	O
``	O
3	O
2	O
1	O
$	O
dev	O
[	O
1	O
]	O
55	O
55	O
53	O
52	O
50	O
56	O
69	O
65	O
80	O
$	O
k	O
[	O
1	O
]	O
-	O
inf	O
0.0000000	O
0.6666667	O
1.0000000	O
1.7500000	O
2.0000000	O
4.2500000	O
[	O
8	O
]	O
5.0000000	O
23.0000000	O
$	O
method	O
[	O
1	O
]	O
``	O
misclass	O
``	O
attr	O
(	O
,	O
''	O
class	O
``	O
)	O
[	O
1	O
]	O
``	O
prune	O
``	O
''	O
tree	B
.	O
sequence	O
``	O
note	O
that	O
,	O
despite	O
the	O
name	O
,	O
dev	O
corresponds	O
to	O
the	O
cross-validation	B
error	O
rate	B
in	O
this	O
instance	O
.	O
the	O
tree	B
with	O
9	O
terminal	B
nodes	O
results	O
in	O
the	O
lowest	O
cross-validation	B
error	O
rate	B
,	O
with	O
50	O
cross-validation	B
errors	O
.	O
we	O
plot	B
the	O
error	B
rate	I
as	O
a	O
function	B
of	O
both	O
size	O
and	O
k.	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
1	O
,2	O
)	O
)	O
8.3	O
lab	O
:	O
decision	O
trees	O
327	O
>	O
plot	B
(	O
cv	O
.	O
carseats	O
$	O
size	O
,	O
cv	O
.	O
carseats	O
$	O
dev	O
,	O
type	O
=	O
''	O
b	O
``	O
)	O
>	O
plot	B
(	O
cv	O
.	O
carseats	O
$	O
k	O
,	O
cv	O
.	O
carseats	O
$	O
dev	O
,	O
type	O
=	O
''	O
b	O
``	O
)	O
we	O
now	O
apply	O
the	O
prune.misclass	O
(	O
)	O
function	B
in	O
order	O
to	O
prune	O
the	O
tree	B
to	O
prune	O
.	O
obtain	O
the	O
nine-node	O
tree	B
.	O
misclass	O
(	O
)	O
>	O
prune	O
.	O
carseats	O
=	O
prune	O
.	O
misclass	O
(	O
tree	B
.	O
carseats	O
,	O
best	O
=9	O
)	O
>	O
plot	B
(	O
prune	O
.	O
carseats	O
)	O
>	O
text	O
(	O
prune	O
.	O
carseats	O
,	O
pretty	O
=0	O
)	O
how	O
well	O
does	O
this	O
pruned	O
tree	B
perform	O
on	O
the	O
test	B
data	O
set	B
?	O
once	O
again	O
,	O
we	O
apply	O
the	O
predict	O
(	O
)	O
function	B
.	O
>	O
tree	B
.	O
pred	O
=	O
predict	O
(	O
prune	O
.	O
carseats	O
,	O
carseats	O
.	O
test	B
,	O
type	O
=	O
''	O
class	O
``	O
)	O
>	O
table	O
(	O
tree	B
.	O
pred	O
,	O
high	O
.	O
test	B
)	O
high	O
.	O
test	B
tree	O
.	O
pred	O
no	O
yes	O
24	O
60	O
no	O
94	O
yes	O
22	O
>	O
(	O
94+60	O
)	O
/200	O
[	O
1	O
]	O
0.77	O
now	O
77	O
%	O
of	O
the	O
test	B
observations	O
are	O
correctly	O
classiﬁed	O
,	O
so	O
not	O
only	O
has	O
the	O
pruning	B
process	O
produced	O
a	O
more	O
interpretable	O
tree	B
,	O
but	O
it	O
has	O
also	O
improved	O
the	O
classiﬁcation	B
accuracy	O
.	O
if	O
we	O
increase	O
the	O
value	O
of	O
best	O
,	O
we	O
obtain	O
a	O
larger	O
pruned	O
tree	B
with	O
lower	O
classiﬁcation	B
accuracy	O
:	O
>	O
prune	O
.	O
carseats	O
=	O
prune	O
.	O
misclass	O
(	O
tree	B
.	O
carseats	O
,	O
best	O
=15	O
)	O
>	O
plot	B
(	O
prune	O
.	O
carseats	O
)	O
>	O
text	O
(	O
prune	O
.	O
carseats	O
,	O
pretty	O
=0	O
)	O
>	O
tree	B
.	O
pred	O
=	O
predict	O
(	O
prune	O
.	O
carseats	O
,	O
carseats	O
.	O
test	B
,	O
type	O
=	O
''	O
class	O
``	O
)	O
>	O
table	O
(	O
tree	B
.	O
pred	O
,	O
high	O
.	O
test	B
)	O
high	O
.	O
test	B
tree	O
.	O
pred	O
no	O
yes	O
22	O
62	O
86	O
no	O
yes	O
30	O
>	O
(	O
86+62	O
)	O
/200	O
[	O
1	O
]	O
0.74	O
8.3.2	O
fitting	O
regression	B
trees	O
here	O
we	O
ﬁt	B
a	O
regression	B
tree	O
to	O
the	O
boston	O
data	B
set	O
.	O
first	O
,	O
we	O
create	O
a	O
training	B
set	O
,	O
and	O
ﬁt	B
the	O
tree	B
to	O
the	O
training	B
data	O
.	O
>	O
library	O
(	O
mass	O
)	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
train	B
=	O
sample	O
(	O
1	O
:	O
nrow	O
(	O
boston	O
)	O
,	O
nrow	O
(	O
boston	O
)	O
/2	O
)	O
>	O
tree	B
.	O
boston	O
=	O
tree	B
(	O
medv∼	O
.	O
,	O
boston	O
,	O
subset	O
=	O
train	B
)	O
>	O
summary	O
(	O
tree	B
.	O
boston	O
)	O
regressio	O
n	O
tree	B
:	O
tree	B
(	O
formula	O
=	O
medv	O
∼	O
.	O
,	O
data	B
=	O
boston	O
,	O
subset	O
=	O
train	B
)	O
328	O
8.	O
tree-based	O
methods	O
variables	O
actually	O
used	O
in	O
tree	B
c	O
o	O
n	O
s	O
t	O
r	O
u	O
c	O
t	O
i	O
o	O
n	O
:	O
[	O
1	O
]	O
``	O
lstat	O
``	O
``	O
rm	O
``	O
''	O
dis	O
``	O
number	O
of	O
terminal	B
nodes	O
:	O
residual	B
mean	O
deviance	B
:	O
d	O
i	O
s	O
t	O
r	O
i	O
b	O
u	O
t	O
i	O
o	O
n	O
of	O
residuals	B
:	O
median	O
-0.0536	O
min	O
.	O
-14.1000	O
12.65	O
=	O
3099	O
/	O
245	O
1	O
st	O
qu	O
.	O
-2.0420	O
8	O
mean	O
0.0000	O
3	O
rd	O
qu	O
.	O
1.9600	O
max	O
.	O
12.6000	O
notice	O
that	O
the	O
output	B
of	O
summary	O
(	O
)	O
indicates	O
that	O
only	O
three	O
of	O
the	O
vari-	O
ables	O
have	O
been	O
used	O
in	O
constructing	O
the	O
tree	B
.	O
in	O
the	O
context	O
of	O
a	O
regression	B
tree	O
,	O
the	O
deviance	B
is	O
simply	O
the	O
sum	O
of	O
squared	O
errors	O
for	O
the	O
tree	B
.	O
we	O
now	O
plot	B
the	O
tree	B
.	O
>	O
plot	B
(	O
tree	B
.	O
boston	O
)	O
>	O
text	O
(	O
tree	B
.	O
boston	O
,	O
pretty	O
=0	O
)	O
the	O
variable	B
lstat	O
measures	O
the	O
percentage	O
of	O
individuals	O
with	O
lower	O
socioeconomic	O
status	O
.	O
the	O
tree	B
indicates	O
that	O
lower	O
values	O
of	O
lstat	O
cor-	O
respond	O
to	O
more	O
expensive	O
houses	O
.	O
the	O
tree	B
predicts	O
a	O
median	O
house	O
price	O
of	O
$	O
46	O
,	O
400	O
for	O
larger	O
homes	O
in	O
suburbs	O
in	O
which	O
residents	O
have	O
high	O
socioe-	O
conomic	O
status	O
(	O
rm	O
>	O
=7.437	O
and	O
lstat	O
<	O
9.715	O
)	O
.	O
now	O
we	O
use	O
the	O
cv.tree	O
(	O
)	O
function	B
to	O
see	O
whether	O
pruning	B
the	O
tree	B
will	O
improve	O
performance	O
.	O
>	O
cv	O
.	O
boston	O
=	O
cv	O
.	O
tree	B
(	O
tree	B
.	O
boston	O
)	O
>	O
plot	B
(	O
cv	O
.	O
boston	O
$	O
size	O
,	O
cv	O
.	O
boston	O
$	O
dev	O
,	O
type	O
=	O
’	O
b	O
’	O
)	O
in	O
this	O
case	O
,	O
the	O
most	O
complex	O
tree	B
is	O
selected	O
by	O
cross-validation	B
.	O
how-	O
ever	O
,	O
if	O
we	O
wish	O
to	O
prune	O
the	O
tree	B
,	O
we	O
could	O
do	O
so	O
as	O
follows	O
,	O
using	O
the	O
prune.tree	O
(	O
)	O
function	B
:	O
prune.tree	O
(	O
)	O
>	O
prune	O
.	O
boston	O
=	O
prune	O
.	O
tree	B
(	O
tree	B
.	O
boston	O
,	O
best	O
=5	O
)	O
>	O
plot	B
(	O
prune	O
.	O
boston	O
)	O
>	O
text	O
(	O
prune	O
.	O
boston	O
,	O
pretty	O
=0	O
)	O
in	O
keeping	O
with	O
the	O
cross-validation	B
results	O
,	O
we	O
use	O
the	O
unpruned	O
tree	B
to	O
make	O
predictions	O
on	O
the	O
test	B
set	O
.	O
>	O
yhat	O
=	O
predict	O
(	O
tree	B
.	O
boston	O
,	O
newdata	O
=	O
boston	O
[	O
-	O
train	B
,	O
]	O
)	O
>	O
boston	O
.	O
test	B
=	O
boston	O
[	O
-	O
train	B
,	O
''	O
medv	O
``	O
]	O
>	O
plot	B
(	O
yhat	O
,	O
boston	O
.	O
test	B
)	O
>	O
abline	O
(	O
0	O
,1	O
)	O
>	O
mean	O
(	O
(	O
yhat	O
-	O
boston	O
.	O
test	B
)	O
^2	O
)	O
[	O
1	O
]	O
25.05	O
in	O
other	O
words	O
,	O
the	O
test	B
set	O
mse	O
associated	O
with	O
the	O
regression	B
tree	O
is	O
25.05.	O
the	O
square	O
root	O
of	O
the	O
mse	O
is	O
therefore	O
around	O
5.005	O
,	O
indicating	O
that	O
this	O
model	B
leads	O
to	O
test	B
predictions	O
that	O
are	O
within	O
around	O
$	O
5	O
,	O
005	O
of	O
the	O
true	O
median	O
home	O
value	O
for	O
the	O
suburb	O
.	O
8.3.3	O
bagging	B
and	O
random	O
forests	O
here	O
we	O
apply	O
bagging	B
and	O
random	O
forests	O
to	O
the	O
boston	O
data	B
,	O
using	O
the	O
randomforest	O
package	O
in	O
r.	O
the	O
exact	O
results	O
obtained	O
in	O
this	O
section	O
may	O
8.3	O
lab	O
:	O
decision	O
trees	O
329	O
depend	O
on	O
the	O
version	O
of	O
r	O
and	O
the	O
version	O
of	O
the	O
randomforest	O
package	O
installed	O
on	O
your	O
computer	O
.	O
recall	B
that	O
bagging	B
is	O
simply	O
a	O
special	O
case	O
of	O
a	O
random	B
forest	I
with	O
m	O
=	O
p.	O
therefore	O
,	O
the	O
randomforest	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
perform	O
both	O
random	O
forests	O
and	O
bagging	B
.	O
we	O
perform	O
bagging	B
as	O
follows	O
:	O
random	B
forest	I
(	O
)	O
>	O
library	O
(	O
r	O
a	O
n	O
d	O
o	O
m	O
f	O
o	O
r	O
e	O
s	O
t	O
)	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
bag	O
.	O
boston	O
=	O
r	O
a	O
n	O
d	O
o	O
m	O
f	O
o	O
r	O
e	O
s	O
t	O
(	O
medv∼	O
.	O
,	O
data	B
=	O
boston	O
,	O
subset	O
=	O
train	B
,	O
mtry	O
=13	O
,	O
importanc	O
e	O
=	O
true	O
)	O
>	O
bag	O
.	O
boston	O
call	O
:	O
r	O
a	O
n	O
d	O
o	O
m	O
f	O
o	O
r	O
e	O
s	O
t	O
(	O
formula	O
=	O
medv	O
∼	O
.	O
,	O
data	B
=	O
boston	O
,	O
mtry	O
=	O
13	O
,	O
importance	B
=	O
true	O
,	O
subset	O
=	O
train	B
)	O
type	O
of	O
random	B
forest	I
:	O
regression	B
number	O
of	O
trees	O
:	O
500	O
no	O
.	O
of	O
variables	O
tried	O
at	O
each	O
split	O
:	O
13	O
mean	O
of	O
squared	O
residuals	B
:	O
10.77	O
%	O
var	O
explained	B
:	O
86.96	O
the	O
argument	B
mtry=13	O
indicates	O
that	O
all	O
13	O
predictors	O
should	O
be	O
considered	O
for	O
each	O
split	O
of	O
the	O
tree—in	O
other	O
words	O
,	O
that	O
bagging	B
should	O
be	O
done	O
.	O
how	O
well	O
does	O
this	O
bagged	O
model	B
perform	O
on	O
the	O
test	B
set	O
?	O
>	O
yhat	O
.	O
bag	O
=	O
predict	O
(	O
bag	O
.	O
boston	O
,	O
newdata	O
=	O
boston	O
[	O
-	O
train	B
,	O
]	O
)	O
>	O
plot	B
(	O
yhat	O
.	O
bag	O
,	O
boston	O
.	O
test	B
)	O
>	O
abline	O
(	O
0	O
,1	O
)	O
>	O
mean	O
(	O
(	O
yhat	O
.	O
bag	O
-	O
boston	O
.	O
test	B
)	O
^2	O
)	O
[	O
1	O
]	O
13.16	O
the	O
test	B
set	O
mse	O
associated	O
with	O
the	O
bagged	O
regression	B
tree	O
is	O
13.16	O
,	O
almost	O
half	O
that	O
obtained	O
using	O
an	O
optimally-pruned	O
single	B
tree	O
.	O
we	O
could	O
change	O
the	O
number	O
of	O
trees	O
grown	O
by	O
randomforest	O
(	O
)	O
using	O
the	O
ntree	O
argument	B
:	O
>	O
bag	O
.	O
boston	O
=	O
r	O
a	O
n	O
d	O
o	O
m	O
f	O
o	O
r	O
e	O
s	O
t	O
(	O
medv∼	O
.	O
,	O
data	B
=	O
boston	O
,	O
subset	O
=	O
train	B
,	O
mtry	O
=13	O
,	O
ntree	O
=25	O
)	O
>	O
yhat	O
.	O
bag	O
=	O
predict	O
(	O
bag	O
.	O
boston	O
,	O
newdata	O
=	O
boston	O
[	O
-	O
train	B
,	O
]	O
)	O
>	O
mean	O
(	O
(	O
yhat	O
.	O
bag	O
-	O
boston	O
.	O
test	B
)	O
^2	O
)	O
[	O
1	O
]	O
13.31	O
growing	O
a	O
random	B
forest	I
proceeds	O
in	O
exactly	O
the	O
same	O
way	O
,	O
except	O
that	O
we	O
use	O
a	O
smaller	O
value	O
of	O
the	O
mtry	O
argument	B
.	O
by	O
default	O
,	O
randomforest	O
(	O
)	O
√	O
uses	O
p/3	O
variables	O
when	O
building	O
a	O
random	B
forest	I
of	O
regression	B
trees	O
,	O
and	O
p	O
variables	O
when	O
building	O
a	O
random	B
forest	I
of	O
classiﬁcation	B
trees	O
.	O
here	O
we	O
use	O
mtry	O
=	O
6	O
.	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
rf	O
.	O
boston	O
=	O
r	O
a	O
n	O
d	O
o	O
m	O
f	O
o	O
r	O
e	O
s	O
t	O
(	O
medv∼	O
.	O
,	O
data	B
=	O
boston	O
,	O
subset	O
=	O
train	B
,	O
mtry	O
=6	O
,	O
importance	B
=	O
true	O
)	O
>	O
yhat	O
.	O
rf	O
=	O
predict	O
(	O
rf	O
.	O
boston	O
,	O
newdata	O
=	O
boston	O
[	O
-	O
train	B
,	O
]	O
)	O
>	O
mean	O
(	O
(	O
yhat	O
.	O
rf	O
-	O
boston	O
.	O
test	B
)	O
^2	O
)	O
[	O
1	O
]	O
11.31	O
330	O
8.	O
tree-based	O
methods	O
the	O
test	B
set	O
mse	O
is	O
11.31	O
;	O
this	O
indicates	O
that	O
random	O
forests	O
yielded	O
an	O
improvement	O
over	O
bagging	B
in	O
this	O
case	O
.	O
using	O
the	O
importance	B
(	O
)	O
function	B
,	O
we	O
can	O
view	O
the	O
importance	B
of	O
each	O
variable	B
.	O
>	O
importanc	O
e	O
(	O
rf	O
.	O
boston	O
)	O
crim	O
zn	O
indus	O
chas	O
nox	O
rm	O
age	O
dis	O
rad	O
tax	O
ptratio	O
black	O
lstat	O
%	O
incmse	O
i	O
n	O
c	O
n	O
o	O
d	O
e	O
p	O
u	O
r	O
i	O
t	O
y	O
1051.54	O
50.31	O
1017.64	O
56.32	O
1107.31	O
5917.26	O
552.27	O
1223.93	O
84.30	O
435.71	O
817.33	O
367.00	O
7713.63	O
12.384	O
2.103	O
8.390	O
2.294	O
12.791	O
30.754	O
10.334	O
14.641	O
3.583	O
8.139	O
11.274	O
8.097	O
30.962	O
importance	B
(	O
)	O
two	O
measures	O
of	O
variable	B
importance	O
are	O
reported	O
.	O
the	O
former	O
is	O
based	O
upon	O
the	O
mean	O
decrease	O
of	O
accuracy	O
in	O
predictions	O
on	O
the	O
out	O
of	O
bag	O
samples	O
when	O
a	O
given	O
variable	B
is	O
excluded	O
from	O
the	O
model	B
.	O
the	O
latter	O
is	O
a	O
measure	O
of	O
the	O
total	O
decrease	O
in	O
node	B
impurity	O
that	O
results	O
from	O
splits	O
over	O
that	O
variable	B
,	O
averaged	O
over	O
all	O
trees	O
(	O
this	O
was	O
plotted	O
in	O
figure	O
8.9	O
)	O
.	O
in	O
the	O
case	O
of	O
regression	B
trees	O
,	O
the	O
node	B
impurity	O
is	O
measured	O
by	O
the	O
training	B
rss	O
,	O
and	O
for	O
classiﬁcation	O
trees	O
by	O
the	O
deviance	B
.	O
plots	O
of	O
these	O
importance	B
measures	O
can	O
be	O
produced	O
using	O
the	O
varimpplot	O
(	O
)	O
function	B
.	O
>	O
varimpplo	O
t	O
(	O
rf	O
.	O
boston	O
)	O
the	O
results	O
indicate	O
that	O
across	O
all	O
of	O
the	O
trees	O
considered	O
in	O
the	O
random	B
forest	I
,	O
the	O
wealth	O
level	B
of	O
the	O
community	O
(	O
lstat	O
)	O
and	O
the	O
house	O
size	O
(	O
rm	O
)	O
are	O
by	O
far	O
the	O
two	O
most	O
important	O
variables	O
.	O
varimpplot	O
(	O
)	O
8.3.4	O
boosting	B
here	O
we	O
use	O
the	O
gbm	O
package	O
,	O
and	O
within	O
it	O
the	O
gbm	O
(	O
)	O
function	B
,	O
to	O
ﬁt	B
boosted	O
regression	B
trees	O
to	O
the	O
boston	O
data	B
set	O
.	O
we	O
run	O
gbm	O
(	O
)	O
with	O
the	O
option	O
distribution=	O
''	O
gaussian	O
''	O
since	O
this	O
is	O
a	O
regression	B
problem	O
;	O
if	O
it	O
were	O
a	O
bi-	O
nary	O
classiﬁcation	B
problem	O
,	O
we	O
would	O
use	O
distribution=	O
''	O
bernoulli	O
''	O
.	O
the	O
argument	B
n.trees=5000	O
indicates	O
that	O
we	O
want	O
5000	O
trees	O
,	O
and	O
the	O
option	O
interaction.depth=4	O
limits	O
the	O
depth	O
of	O
each	O
tree	B
.	O
gbm	O
(	O
)	O
>	O
library	O
(	O
gbm	O
)	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
boost	O
.	O
boston	O
=	O
gbm	O
(	O
medv∼	O
.	O
,	O
data	B
=	O
boston	O
[	O
train	B
,	O
]	O
,	O
d	O
i	O
s	O
t	O
r	O
i	O
b	O
u	O
t	O
i	O
o	O
n	O
=	O
''	O
gaussian	O
``	O
,	O
n	O
.	O
trees	O
=5000	O
,	O
interactio	O
n	O
.	O
depth	O
=4	O
)	O
the	O
summary	O
(	O
)	O
function	B
produces	O
a	O
relative	O
inﬂuence	O
plot	B
and	O
also	O
outputs	O
the	O
relative	O
inﬂuence	O
statistics	O
.	O
8.3	O
lab	O
:	O
decision	O
trees	O
331	O
>	O
summary	O
(	O
boost	O
.	O
boston	O
)	O
var	O
lstat	O
rm	O
dis	O
crim	O
nox	O
ptratio	O
black	O
age	O
tax	O
indus	O
chas	O
rad	O
zn	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
8	O
9	O
10	O
11	O
12	O
13	O
rel	O
.	O
inf	O
45.96	O
31.22	O
6.81	O
4.07	O
2.56	O
2.27	O
1.80	O
1.64	O
1.36	O
1.27	O
0.80	O
0.20	O
0.015	O
we	O
see	O
that	O
lstat	O
and	O
rm	O
are	O
by	O
far	O
the	O
most	O
important	O
variables	O
.	O
we	O
can	O
also	O
produce	O
partial	O
dependence	O
plots	O
for	O
these	O
two	O
variables	O
.	O
these	O
plots	O
illustrate	O
the	O
marginal	O
eﬀect	O
of	O
the	O
selected	O
variables	O
on	O
the	O
response	B
after	O
integrating	O
out	O
the	O
other	O
variables	O
.	O
in	O
this	O
case	O
,	O
as	O
we	O
might	O
expect	O
,	O
median	O
house	O
prices	O
are	O
increasing	O
with	O
rm	O
and	O
decreasing	O
with	O
lstat	O
.	O
partial	O
dependence	O
plot	B
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
1	O
,2	O
)	O
)	O
>	O
plot	B
(	O
boost	O
.	O
boston	O
,	O
i	O
=	O
''	O
rm	O
``	O
)	O
>	O
plot	B
(	O
boost	O
.	O
boston	O
,	O
i	O
=	O
''	O
lstat	O
``	O
)	O
we	O
now	O
use	O
the	O
boosted	O
model	B
to	O
predict	O
medv	O
on	O
the	O
test	B
set	O
:	O
>	O
yhat	O
.	O
boost	O
=	O
predict	O
(	O
boost	O
.	O
boston	O
,	O
newdata	O
=	O
boston	O
[	O
-	O
train	B
,	O
]	O
,	O
n	O
.	O
trees	O
=5000	O
)	O
>	O
mean	O
(	O
(	O
yhat	O
.	O
boost	O
-	O
boston	O
.	O
test	B
)	O
^2	O
)	O
[	O
1	O
]	O
11.8	O
the	O
test	B
mse	O
obtained	O
is	O
11.8	O
;	O
similar	O
to	O
the	O
test	B
mse	O
for	O
random	O
forests	O
and	O
superior	O
to	O
that	O
for	O
bagging	O
.	O
if	O
we	O
want	O
to	O
,	O
we	O
can	O
perform	O
boosting	B
with	O
a	O
diﬀerent	O
value	O
of	O
the	O
shrinkage	B
parameter	O
λ	O
in	O
(	O
8.10	O
)	O
.	O
the	O
default	O
value	O
is	O
0.001	O
,	O
but	O
this	O
is	O
easily	O
modiﬁed	O
.	O
here	O
we	O
take	O
λ	O
=	O
0.2	O
.	O
>	O
boost	O
.	O
boston	O
=	O
gbm	O
(	O
medv∼	O
.	O
,	O
data	B
=	O
boston	O
[	O
train	B
,	O
]	O
,	O
d	O
i	O
s	O
t	O
r	O
i	O
b	O
u	O
t	O
i	O
o	O
n	O
=	O
''	O
gaussian	O
``	O
,	O
n	O
.	O
trees	O
=5000	O
,	O
interacti	O
on	O
.	O
depth	O
=4	O
,	O
shrinkage	B
=0.2	O
,	O
verbose	O
=	O
f	O
)	O
>	O
yhat	O
.	O
boost	O
=	O
predict	O
(	O
boost	O
.	O
boston	O
,	O
newdata	O
=	O
boston	O
[	O
-	O
train	B
,	O
]	O
,	O
n	O
.	O
trees	O
=5000	O
)	O
>	O
mean	O
(	O
(	O
yhat	O
.	O
boost	O
-	O
boston	O
.	O
test	B
)	O
^2	O
)	O
[	O
1	O
]	O
11.5	O
in	O
this	O
case	O
,	O
using	O
λ	O
=	O
0.2	O
leads	O
to	O
a	O
slightly	O
lower	O
test	B
mse	O
than	O
λ	O
=	O
0.001	O
.	O
332	O
8.	O
tree-based	O
methods	O
8.4	O
exercises	O
conceptual	O
1.	O
draw	O
an	O
example	O
(	O
of	O
your	O
own	O
invention	O
)	O
of	O
a	O
partition	O
of	O
two-	O
dimensional	O
feature	B
space	O
that	O
could	O
result	O
from	O
recursive	B
binary	I
splitting	I
.	O
your	O
example	O
should	O
contain	O
at	O
least	O
six	O
regions	O
.	O
draw	O
a	O
decision	B
tree	I
corresponding	O
to	O
this	O
partition	O
.	O
be	O
sure	O
to	O
label	O
all	O
as-	O
pects	O
of	O
your	O
ﬁgures	O
,	O
including	O
the	O
regions	O
r1	O
,	O
r2	O
,	O
.	O
.	O
.	O
,	O
the	O
cutpoints	O
t1	O
,	O
t2	O
,	O
.	O
.	O
.	O
,	O
and	O
so	O
forth	O
.	O
hint	O
:	O
your	O
result	O
should	O
look	O
something	O
like	O
figures	O
8.1	O
and	O
8.2	O
.	O
2.	O
it	O
is	O
mentioned	O
in	O
section	O
8.2.3	O
that	O
boosting	B
using	O
depth-one	O
trees	O
(	O
or	O
stumps	O
)	O
leads	O
to	O
an	O
additive	B
model	O
:	O
that	O
is	O
,	O
a	O
model	B
of	O
the	O
form	O
p	O
(	O
cid:17	O
)	O
f	O
(	O
x	O
)	O
=	O
fj	O
(	O
xj	O
)	O
.	O
j=1	O
explain	O
why	O
this	O
is	O
the	O
case	O
.	O
you	O
can	O
begin	O
with	O
(	O
8.12	O
)	O
in	O
algorithm	O
8.2	O
.	O
3.	O
consider	O
the	O
gini	O
index	O
,	O
classiﬁcation	B
error	O
,	O
and	O
entropy	B
in	O
a	O
simple	B
classiﬁcation	O
setting	O
with	O
two	O
classes	O
.	O
create	O
a	O
single	B
plot	O
that	O
displays	O
each	O
of	O
these	O
quantities	O
as	O
a	O
function	B
of	O
ˆpm1	O
.	O
the	O
x-	O
axis	O
should	O
display	O
ˆpm1	O
,	O
ranging	O
from	O
0	O
to	O
1	O
,	O
and	O
the	O
y-axis	O
should	O
display	O
the	O
value	O
of	O
the	O
gini	O
index	O
,	O
classiﬁcation	B
error	O
,	O
and	O
entropy	B
.	O
hint	O
:	O
in	O
a	O
setting	O
with	O
two	O
classes	O
,	O
ˆpm1	O
=	O
1	O
−	O
ˆpm2	O
.	O
you	O
could	O
make	O
this	O
plot	B
by	O
hand	O
,	O
but	O
it	O
will	O
be	O
much	O
easier	O
to	O
make	O
in	O
r.	O
4.	O
this	O
question	O
relates	O
to	O
the	O
plots	O
in	O
figure	O
8.12	O
.	O
(	O
a	O
)	O
sketch	O
the	O
tree	B
corresponding	O
to	O
the	O
partition	O
of	O
the	O
predictor	B
space	O
illustrated	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
8.12.	O
the	O
num-	O
bers	O
inside	O
the	O
boxes	O
indicate	O
the	O
mean	O
of	O
y	O
within	O
each	O
region	O
.	O
(	O
b	O
)	O
create	O
a	O
diagram	O
similar	O
to	O
the	O
left-hand	O
panel	O
of	O
figure	O
8.12	O
,	O
using	O
the	O
tree	B
illustrated	O
in	O
the	O
right-hand	O
panel	O
of	O
the	O
same	O
ﬁgure	O
.	O
you	O
should	O
divide	O
up	O
the	O
predictor	B
space	O
into	O
the	O
correct	O
regions	O
,	O
and	O
indicate	O
the	O
mean	O
for	O
each	O
region	O
.	O
5.	O
suppose	O
we	O
produce	O
ten	O
bootstrapped	O
samples	O
from	O
a	O
data	B
set	O
containing	O
red	O
and	O
green	O
classes	O
.	O
we	O
then	O
apply	O
a	O
classiﬁcation	B
tree	O
to	O
each	O
bootstrapped	O
sample	O
and	O
,	O
for	O
a	O
speciﬁc	O
value	O
of	O
x	O
,	O
produce	O
10	O
estimates	O
of	O
p	O
(	O
class	O
is	O
red|x	O
)	O
:	O
0.1	O
,	O
0.15	O
,	O
0.2	O
,	O
0.2	O
,	O
0.55	O
,	O
0.6	O
,	O
0.6	O
,	O
0.65	O
,	O
0.7	O
,	O
and	O
0.75	O
.	O
8.4	O
exercises	O
333	O
x2	B
<	O
1	O
x2	B
1	O
0	O
3	O
5	O
15	O
0	O
0	O
10	O
1	O
x1	O
x1	O
<	O
1	O
x2	B
<	O
2	O
x1	O
<	O
0	O
2.49	O
−1.80	O
0.63	O
−1.06	O
0.21	O
figure	O
8.12.	O
left	O
:	O
a	O
partition	O
of	O
the	O
predictor	B
space	O
corresponding	O
to	O
exer-	O
cise	O
4a	O
.	O
right	O
:	O
a	O
tree	B
corresponding	O
to	O
exercise	O
4b	O
.	O
there	O
are	O
two	O
common	O
ways	O
to	O
combine	O
these	O
results	O
together	O
into	O
a	O
single	B
class	O
prediction	B
.	O
one	O
is	O
the	O
majority	B
vote	I
approach	O
discussed	O
in	O
this	O
chapter	O
.	O
the	O
second	O
approach	B
is	O
to	O
classify	O
based	O
on	O
the	O
average	B
probability	O
.	O
in	O
this	O
example	O
,	O
what	O
is	O
the	O
ﬁnal	O
classiﬁcation	B
under	O
each	O
of	O
these	O
two	O
approaches	O
?	O
6.	O
provide	O
a	O
detailed	O
explanation	O
of	O
the	O
algorithm	O
that	O
is	O
used	O
to	O
ﬁt	B
a	O
regression	B
tree	O
.	O
applied	O
7.	O
in	O
the	O
lab	O
,	O
we	O
applied	O
random	O
forests	O
to	O
the	O
boston	O
data	B
using	O
mtry=6	O
and	O
using	O
ntree=25	O
and	O
ntree=500	O
.	O
create	O
a	O
plot	B
displaying	O
the	O
test	B
error	O
resulting	O
from	O
random	O
forests	O
on	O
this	O
data	B
set	O
for	O
a	O
more	O
com-	O
prehensive	O
range	O
of	O
values	O
for	O
mtry	O
and	O
ntree	O
.	O
you	O
can	O
model	B
your	O
plot	B
after	O
figure	O
8.10.	O
describe	O
the	O
results	O
obtained	O
.	O
8.	O
in	O
the	O
lab	O
,	O
a	O
classiﬁcation	B
tree	O
was	O
applied	O
to	O
the	O
carseats	O
data	B
set	O
af-	O
ter	O
converting	O
sales	O
into	O
a	O
qualitative	B
response	O
variable	B
.	O
now	O
we	O
will	O
seek	O
to	O
predict	O
sales	O
using	O
regression	B
trees	O
and	O
related	O
approaches	O
,	O
treating	O
the	O
response	B
as	O
a	O
quantitative	B
variable	O
.	O
(	O
a	O
)	O
split	O
the	O
data	B
set	O
into	O
a	O
training	B
set	O
and	O
a	O
test	B
set	O
.	O
(	O
b	O
)	O
fit	O
a	O
regression	B
tree	O
to	O
the	O
training	B
set	O
.	O
plot	B
the	O
tree	B
,	O
and	O
inter-	O
pret	O
the	O
results	O
.	O
what	O
test	B
mse	O
do	O
you	O
obtain	O
?	O
(	O
c	O
)	O
use	O
cross-validation	B
in	O
order	O
to	O
determine	O
the	O
optimal	O
level	O
of	O
tree	B
complexity	O
.	O
does	O
pruning	B
the	O
tree	B
improve	O
the	O
test	B
mse	O
?	O
(	O
d	O
)	O
use	O
the	O
bagging	B
approach	O
in	O
order	O
to	O
analyze	O
this	O
data	B
.	O
what	O
test	B
mse	O
do	O
you	O
obtain	O
?	O
use	O
the	O
importance	B
(	O
)	O
function	B
to	O
de-	O
termine	O
which	O
variables	O
are	O
most	O
important	O
.	O
334	O
8.	O
tree-based	O
methods	O
(	O
e	O
)	O
use	O
random	O
forests	O
to	O
analyze	O
this	O
data	B
.	O
what	O
test	B
mse	O
do	O
you	O
obtain	O
?	O
use	O
the	O
importance	B
(	O
)	O
function	B
to	O
determine	O
which	O
vari-	O
ables	O
are	O
most	O
important	O
.	O
describe	O
the	O
eﬀect	O
of	O
m	O
,	O
the	O
number	O
of	O
variables	O
rate	B
obtained	O
.	O
considered	O
at	O
error	B
each	O
split	O
,	O
on	O
the	O
9.	O
this	O
problem	O
involves	O
the	O
oj	O
data	B
set	O
which	O
is	O
part	O
of	O
the	O
islr	O
package	O
.	O
(	O
a	O
)	O
create	O
a	O
training	B
set	O
containing	O
a	O
random	O
sample	O
of	O
800	O
obser-	O
vations	O
,	O
and	O
a	O
test	B
set	O
containing	O
the	O
remaining	O
observations	B
.	O
(	O
b	O
)	O
fit	O
a	O
tree	B
to	O
the	O
training	B
data	O
,	O
with	O
purchase	O
as	O
the	O
response	B
and	O
the	O
other	O
variables	O
as	O
predictors	O
.	O
use	O
the	O
summary	O
(	O
)	O
function	B
to	O
produce	O
summary	O
statistics	O
about	O
the	O
tree	B
,	O
and	O
describe	O
the	O
results	O
obtained	O
.	O
what	O
is	O
the	O
training	B
error	O
rate	B
?	O
how	O
many	O
terminal	B
nodes	O
does	O
the	O
tree	B
have	O
?	O
(	O
c	O
)	O
type	O
in	O
the	O
name	O
of	O
the	O
tree	B
object	O
in	O
order	O
to	O
get	O
a	O
detailed	O
text	O
output	B
.	O
pick	O
one	O
of	O
the	O
terminal	B
nodes	O
,	O
and	O
interpret	O
the	O
information	O
displayed	O
.	O
(	O
d	O
)	O
create	O
a	O
plot	B
of	O
the	O
tree	B
,	O
and	O
interpret	O
the	O
results	O
.	O
(	O
e	O
)	O
predict	O
the	O
response	B
on	O
the	O
test	B
data	O
,	O
and	O
produce	O
a	O
confusion	B
matrix	I
comparing	O
the	O
test	B
labels	O
to	O
the	O
predicted	O
test	B
labels	O
.	O
what	O
is	O
the	O
test	B
error	O
rate	B
?	O
(	O
f	O
)	O
apply	O
the	O
cv.tree	O
(	O
)	O
function	B
to	O
the	O
training	B
set	O
in	O
order	O
to	O
determine	O
the	O
optimal	O
tree	O
size	O
.	O
(	O
g	O
)	O
produce	O
a	O
plot	B
with	O
tree	B
size	O
on	O
the	O
x-axis	O
and	O
cross-validated	O
classiﬁcation	B
error	O
rate	B
on	O
the	O
y-axis	O
.	O
(	O
h	O
)	O
which	O
tree	B
size	O
corresponds	O
to	O
the	O
lowest	O
cross-validated	O
classi-	O
ﬁcation	O
error	B
rate	I
?	O
(	O
i	O
)	O
produce	O
a	O
pruned	O
tree	B
corresponding	O
to	O
the	O
optimal	O
tree	O
size	O
obtained	O
using	O
cross-validation	B
.	O
if	O
cross-validation	B
does	O
not	O
lead	O
to	O
selection	B
of	O
a	O
pruned	O
tree	B
,	O
then	O
create	O
a	O
pruned	O
tree	B
with	O
ﬁve	O
terminal	B
nodes	O
.	O
(	O
j	O
)	O
compare	O
the	O
training	B
error	O
rates	O
between	O
the	O
pruned	O
and	O
un-	O
pruned	O
trees	O
.	O
which	O
is	O
higher	O
?	O
(	O
k	O
)	O
compare	O
the	O
test	B
error	O
rates	O
between	O
the	O
pruned	O
and	O
unpruned	O
trees	O
.	O
which	O
is	O
higher	O
?	O
10.	O
we	O
now	O
use	O
boosting	B
to	O
predict	O
salary	O
in	O
the	O
hitters	O
data	B
set	O
.	O
(	O
a	O
)	O
remove	O
the	O
observations	B
for	O
whom	O
the	O
salary	O
information	O
is	O
unknown	O
,	O
and	O
then	O
log-transform	O
the	O
salaries	O
.	O
8.4	O
exercises	O
335	O
(	O
b	O
)	O
create	O
a	O
training	B
set	O
consisting	O
of	O
the	O
ﬁrst	O
200	O
observations	B
,	O
and	O
a	O
test	B
set	O
consisting	O
of	O
the	O
remaining	O
observations	B
.	O
(	O
c	O
)	O
perform	O
boosting	B
on	O
the	O
training	B
set	O
with	O
1,000	O
trees	O
for	O
a	O
range	O
of	O
values	O
of	O
the	O
shrinkage	B
parameter	O
λ.	O
produce	O
a	O
plot	B
with	O
diﬀerent	O
shrinkage	B
values	O
on	O
the	O
x-axis	O
and	O
the	O
corresponding	O
training	B
set	O
mse	O
on	O
the	O
y-axis	O
.	O
(	O
d	O
)	O
produce	O
a	O
plot	B
with	O
diﬀerent	O
shrinkage	B
values	O
on	O
the	O
x-axis	O
and	O
the	O
corresponding	O
test	B
set	O
mse	O
on	O
the	O
y-axis	O
.	O
(	O
e	O
)	O
compare	O
the	O
test	B
mse	O
of	O
boosting	B
to	O
the	O
test	B
mse	O
that	O
results	O
the	O
regression	B
approaches	O
seen	O
in	O
from	O
applying	O
two	O
of	O
chapters	O
3	O
and	O
6	O
.	O
(	O
f	O
)	O
which	O
variables	O
appear	O
to	O
be	O
the	O
most	O
important	O
predictors	O
in	O
the	O
boosted	O
model	B
?	O
(	O
g	O
)	O
now	O
apply	O
bagging	B
to	O
the	O
training	B
set	O
.	O
what	O
is	O
the	O
test	B
set	O
mse	O
for	O
this	O
approach	B
?	O
11.	O
this	O
question	O
uses	O
the	O
caravan	O
data	B
set	O
.	O
(	O
a	O
)	O
create	O
a	O
training	B
set	O
consisting	O
of	O
the	O
ﬁrst	O
1,000	O
observations	B
,	O
and	O
a	O
test	B
set	O
consisting	O
of	O
the	O
remaining	O
observations	B
.	O
(	O
b	O
)	O
fit	O
a	O
boosting	B
model	O
to	O
the	O
training	B
set	O
with	O
purchase	O
as	O
the	O
response	B
and	O
the	O
other	O
variables	O
as	O
predictors	O
.	O
use	O
1,000	O
trees	O
,	O
and	O
a	O
shrinkage	B
value	O
of	O
0.01.	O
which	O
predictors	O
appear	O
to	O
be	O
the	O
most	O
important	O
?	O
(	O
c	O
)	O
use	O
the	O
boosting	B
model	O
to	O
predict	O
the	O
response	B
on	O
the	O
test	B
data	O
.	O
predict	O
that	O
a	O
person	O
will	O
make	O
a	O
purchase	O
if	O
the	O
estimated	O
prob-	O
ability	O
of	O
purchase	O
is	O
greater	O
than	O
20	O
%	O
.	O
form	O
a	O
confusion	O
ma-	O
trix	O
.	O
what	O
fraction	O
of	O
the	O
people	O
predicted	O
to	O
make	O
a	O
purchase	O
do	O
in	O
fact	O
make	O
one	O
?	O
how	O
does	O
this	O
compare	O
with	O
the	O
results	O
obtained	O
from	O
applying	O
knn	O
or	O
logistic	B
regression	I
to	O
this	O
data	B
set	O
?	O
12.	O
apply	O
boosting	B
,	O
bagging	B
,	O
and	O
random	O
forests	O
to	O
a	O
data	B
set	O
of	O
your	O
choice	O
.	O
be	O
sure	O
to	O
ﬁt	B
the	O
models	O
on	O
a	O
training	B
set	O
and	O
to	O
evaluate	O
their	O
performance	O
on	O
a	O
test	B
set	O
.	O
how	O
accurate	O
are	O
the	O
results	O
compared	O
to	O
simple	B
methods	O
like	O
linear	B
or	O
logistic	B
regression	I
?	O
which	O
of	O
these	O
approaches	O
yields	O
the	O
best	O
performance	O
?	O
9	O
support	B
vector	I
machines	O
in	O
this	O
chapter	O
,	O
we	O
discuss	O
the	O
support	B
vector	I
machine	O
(	O
svm	O
)	O
,	O
an	O
approach	B
for	O
classiﬁcation	B
that	O
was	O
developed	O
in	O
the	O
computer	O
science	O
community	O
in	O
the	O
1990s	O
and	O
that	O
has	O
grown	O
in	O
popularity	O
since	O
then	O
.	O
svms	O
have	O
been	O
shown	O
to	O
perform	O
well	O
in	O
a	O
variety	O
of	O
settings	O
,	O
and	O
are	O
often	O
considered	O
one	O
of	O
the	O
best	O
“	O
out	O
of	O
the	O
box	O
”	O
classiﬁers	O
.	O
the	O
support	B
vector	I
machine	O
is	O
a	O
generalization	O
of	O
a	O
simple	B
and	O
intu-	O
itive	O
classiﬁer	B
called	O
the	O
maximal	B
margin	I
classiﬁer	O
,	O
which	O
we	O
introduce	O
in	O
section	O
9.1.	O
though	O
it	O
is	O
elegant	O
and	O
simple	B
,	O
we	O
will	O
see	O
that	O
this	O
classiﬁer	B
unfortunately	O
can	O
not	O
be	O
applied	O
to	O
most	O
data	B
sets	O
,	O
since	O
it	O
requires	O
that	O
the	O
classes	O
be	O
separable	O
by	O
a	O
linear	B
boundary	O
.	O
in	O
section	O
9.2	O
,	O
we	O
introduce	O
the	O
support	B
vector	I
classiﬁer	O
,	O
an	O
extension	O
of	O
the	O
maximal	B
margin	I
classiﬁer	O
that	O
can	O
be	O
applied	O
in	O
a	O
broader	O
range	O
of	O
cases	O
.	O
section	O
9.3	O
introduces	O
the	O
support	B
vector	I
machine	O
,	O
which	O
is	O
a	O
further	O
extension	O
of	O
the	O
support	O
vec-	O
tor	O
classiﬁer	B
in	O
order	O
to	O
accommodate	O
non-linear	B
class	O
boundaries	O
.	O
support	B
vector	I
machines	O
are	O
intended	O
for	O
the	O
binary	B
classiﬁcation	O
setting	O
in	O
which	O
there	O
are	O
two	O
classes	O
;	O
in	O
section	O
9.4	O
we	O
discuss	O
extensions	O
of	O
support	B
vector	I
machines	O
to	O
the	O
case	O
of	O
more	O
than	O
two	O
classes	O
.	O
in	O
section	O
9.5	O
we	O
discuss	O
the	O
close	O
connections	O
between	O
support	B
vector	I
machines	O
and	O
other	O
statistical	O
methods	O
such	O
as	O
logistic	B
regression	I
.	O
people	O
often	O
loosely	O
refer	O
to	O
the	O
maximal	B
margin	I
classiﬁer	O
,	O
the	O
support	B
vector	I
classiﬁer	O
,	O
and	O
the	O
support	B
vector	I
machine	O
as	O
“	O
support	B
vector	I
machines	O
”	O
.	O
to	O
avoid	O
confusion	O
,	O
we	O
will	O
carefully	O
distinguish	O
between	O
these	O
three	O
notions	O
in	O
this	O
chapter	O
.	O
g.	O
james	O
et	O
al.	O
,	O
an	O
introduction	O
to	O
statistical	O
learning	O
:	O
with	O
applications	O
in	O
r	O
,	O
springer	O
texts	O
in	O
statistics	O
,	O
doi	O
10.1007/978-1-4614-7138-7	O
9	O
,	O
©	O
springer	O
science+business	O
media	O
new	O
york	O
2013	O
337	O
338	O
9.	O
support	B
vector	I
machines	O
9.1	O
maximal	B
margin	I
classiﬁer	O
in	O
this	O
section	O
,	O
we	O
deﬁne	O
a	O
hyperplane	B
and	O
introduce	O
the	O
concept	O
of	O
an	O
optimal	B
separating	I
hyperplane	I
.	O
9.1.1	O
what	O
is	O
a	O
hyperplane	B
?	O
in	O
a	O
p-dimensional	O
space	O
,	O
a	O
hyperplane	B
is	O
a	O
ﬂat	O
aﬃne	O
subspace	O
of	O
dimension	O
p	O
−	O
1.1	O
for	O
instance	O
,	O
in	O
two	O
dimensions	O
,	O
a	O
hyperplane	B
is	O
a	O
ﬂat	O
one-dimensional	O
subspace—in	O
other	O
words	O
,	O
a	O
line	B
.	O
in	O
three	O
dimensions	O
,	O
a	O
hyperplane	B
is	O
a	O
ﬂat	O
two-dimensional	O
subspace—that	O
is	O
,	O
a	O
plane	O
.	O
in	O
p	O
>	O
3	O
dimensions	O
,	O
it	O
can	O
be	O
hard	O
to	O
visualize	O
a	O
hyperplane	B
,	O
but	O
the	O
notion	O
of	O
a	O
(	O
p	O
−	O
1	O
)	O
-dimensional	O
ﬂat	O
subspace	O
still	O
applies	O
.	O
the	O
mathematical	O
deﬁnition	O
of	O
a	O
hyperplane	B
is	O
quite	O
simple	B
.	O
in	O
two	O
di-	O
mensions	O
,	O
a	O
hyperplane	B
is	O
deﬁned	O
by	O
the	O
equation	O
hyperplane	B
β0	O
+	O
β1x1	O
+	O
β2x2	O
=	O
0	O
(	O
9.1	O
)	O
for	O
parameters	O
β0	O
,	O
β1	O
,	O
and	O
β2	O
.	O
when	O
we	O
say	O
that	O
(	O
9.1	O
)	O
“	O
deﬁnes	O
”	O
the	O
hyper-	O
plane	O
,	O
we	O
mean	O
that	O
any	O
x	O
=	O
(	O
x1	O
,	O
x2	B
)	O
t	O
for	O
which	O
(	O
9.1	O
)	O
holds	O
is	O
a	O
point	O
on	O
the	O
hyperplane	B
.	O
note	O
that	O
(	O
9.1	O
)	O
is	O
simply	O
the	O
equation	O
of	O
a	O
line	B
,	O
since	O
indeed	O
in	O
two	O
dimensions	O
a	O
hyperplane	B
is	O
a	O
line	B
.	O
equation	O
9.1	O
can	O
be	O
easily	O
extended	O
to	O
the	O
p-dimensional	O
setting	O
:	O
β0	O
+	O
β1x1	O
+	O
β2x2	O
+	O
.	O
.	O
.	O
+	O
βpxp	O
=	O
0	O
(	O
9.2	O
)	O
deﬁnes	O
a	O
p-dimensional	O
hyperplane	B
,	O
again	O
in	O
the	O
sense	O
that	O
if	O
a	O
point	O
x	O
=	O
(	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
)	O
t	O
in	O
p-dimensional	O
space	O
(	O
i.e	O
.	O
a	O
vector	B
of	O
length	O
p	O
)	O
satisﬁes	O
(	O
9.2	O
)	O
,	O
then	O
x	O
lies	O
on	O
the	O
hyperplane	B
.	O
now	O
,	O
suppose	O
that	O
x	O
does	O
not	O
satisfy	O
(	O
9.2	O
)	O
;	O
rather	O
,	O
β0	O
+	O
β1x1	O
+	O
β2x2	O
+	O
.	O
.	O
.	O
+	O
βpxp	O
>	O
0	O
.	O
(	O
9.3	O
)	O
then	O
this	O
tells	O
us	O
that	O
x	O
lies	O
to	O
one	O
side	O
of	O
the	O
hyperplane	B
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
β0	O
+	O
β1x1	O
+	O
β2x2	O
+	O
.	O
.	O
.	O
+	O
βpxp	O
<	O
0	O
,	O
(	O
9.4	O
)	O
then	O
x	O
lies	O
on	O
the	O
other	O
side	O
of	O
the	O
hyperplane	B
.	O
so	O
we	O
can	O
think	O
of	O
the	O
hyperplane	B
as	O
dividing	O
p-dimensional	O
space	O
into	O
two	O
halves	O
.	O
one	O
can	O
easily	O
determine	O
on	O
which	O
side	O
of	O
the	O
hyperplane	B
a	O
point	O
lies	O
by	O
simply	O
calculating	O
the	O
sign	O
of	O
the	O
left	O
hand	O
side	O
of	O
(	O
9.2	O
)	O
.	O
a	O
hyperplane	B
in	O
two-dimensional	O
space	O
is	O
shown	O
in	O
figure	O
9.1	O
.	O
1the	O
word	O
aﬃne	O
indicates	O
that	O
the	O
subspace	O
need	O
not	O
pass	O
through	O
the	O
origin	O
.	O
9.1	O
maximal	B
margin	I
classiﬁer	O
339	O
5	O
.	O
1	O
0	O
1	O
.	O
5	O
.	O
0	O
2	O
x	O
0	O
0	O
.	O
5	O
.	O
0	O
−	O
0	O
.	O
1	O
−	O
5	O
.	O
1	O
−	O
−1.5	O
−1.0	O
−0.5	O
0.5	O
1.0	O
1.5	O
0.0	O
x1	O
figure	O
9.1.	O
the	O
hyperplane	B
1	O
+	O
2x1	O
+	O
3x2	O
=	O
0	O
is	O
shown	O
.	O
the	O
blue	O
region	O
is	O
the	O
set	B
of	O
points	O
for	O
which	O
1	O
+	O
2x1	O
+	O
3x2	O
>	O
0	O
,	O
and	O
the	O
purple	O
region	O
is	O
the	O
set	B
of	O
points	O
for	O
which	O
1	O
+	O
2x1	O
+	O
3x2	O
<	O
0	O
.	O
9.1.2	O
classiﬁcation	B
using	O
a	O
separating	B
hyperplane	I
now	O
suppose	O
that	O
we	O
have	O
a	O
n×	O
p	O
data	B
matrix	O
x	O
that	O
consists	O
of	O
n	O
training	B
observations	O
in	O
p-dimensional	O
space	O
,	O
⎛	O
⎜⎝	O
x11	O
...	O
x1p	O
⎞	O
⎟⎠	O
,	O
.	O
.	O
.	O
,	O
xn	O
=	O
⎛	O
⎜⎝	O
⎞	O
⎟⎠	O
,	O
xn1	O
...	O
xnp	O
x1	O
=	O
(	O
9.5	O
)	O
and	O
that	O
these	O
observations	B
fall	O
into	O
two	O
classes—that	O
is	O
,	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
∈	O
{	O
−1	O
,	O
1	O
}	O
where	O
−1	O
represents	O
one	O
class	O
and	O
1	O
the	O
other	O
class	O
.	O
we	O
also	O
have	O
a	O
∗	O
test	B
observation	O
,	O
a	O
p-vector	O
of	O
observed	O
features	O
x	O
.	O
our	O
goal	O
is	O
to	O
develop	O
a	O
classiﬁer	B
based	O
on	O
the	O
training	B
data	O
that	O
will	O
correctly	O
classify	O
the	O
test	B
observation	O
using	O
its	O
feature	B
measurements	O
.	O
we	O
have	O
seen	O
a	O
number	O
of	O
approaches	O
for	O
this	O
task	O
,	O
such	O
as	O
linear	B
discriminant	I
analysis	I
and	O
logistic	B
regression	I
in	O
chapter	O
4	O
,	O
and	O
classiﬁcation	B
trees	O
,	O
bagging	B
,	O
and	O
boosting	B
in	O
chapter	O
8.	O
we	O
will	O
now	O
see	O
a	O
new	O
approach	B
that	O
is	O
based	O
upon	O
the	O
concept	O
of	O
a	O
separating	B
hyperplane	I
.	O
∗	O
.	O
.	O
.	O
x	O
p	O
(	O
cid:9	O
)	O
t	O
(	O
cid:8	O
)	O
∗	O
x	O
1	O
=	O
suppose	O
that	O
it	O
is	O
possible	O
to	O
construct	O
a	O
hyperplane	B
that	O
separates	O
the	O
training	B
observations	O
perfectly	O
according	O
to	O
their	O
class	O
labels	O
.	O
examples	O
of	O
three	O
such	O
separating	O
hyperplanes	O
are	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
9.2.	O
we	O
can	O
label	O
the	O
observations	B
from	O
the	O
blue	O
class	O
as	O
yi	O
=	O
1	O
and	O
separating	B
hyperplane	I
340	O
9.	O
support	B
vector	I
machines	O
3	O
2	O
2	O
x	O
1	O
0	O
1	O
−	O
3	O
2	O
2	O
x	O
1	O
0	O
1	O
−	O
−1	O
0	O
1	O
x1	O
2	O
3	O
−1	O
0	O
2	O
3	O
1	O
x1	O
figure	O
9.2.	O
left	O
:	O
there	O
are	O
two	O
classes	O
of	O
observations	B
,	O
shown	O
in	O
blue	O
and	O
in	O
purple	O
,	O
each	O
of	O
which	O
has	O
measurements	O
on	O
two	O
variables	O
.	O
three	O
separating	O
hyperplanes	O
,	O
out	O
of	O
many	O
possible	O
,	O
are	O
shown	O
in	O
black	O
.	O
right	O
:	O
a	O
separating	O
hy-	O
perplane	O
is	O
shown	O
in	O
black	O
.	O
the	O
blue	O
and	O
purple	O
grid	O
indicates	O
the	O
decision	O
rule	O
made	O
by	O
a	O
classiﬁer	B
based	O
on	O
this	O
separating	B
hyperplane	I
:	O
a	O
test	B
observation	O
that	O
falls	O
in	O
the	O
blue	O
portion	O
of	O
the	O
grid	O
will	O
be	O
assigned	O
to	O
the	O
blue	O
class	O
,	O
and	O
a	O
test	B
observation	O
that	O
falls	O
into	O
the	O
purple	O
portion	O
of	O
the	O
grid	O
will	O
be	O
assigned	O
to	O
the	O
purple	O
class	O
.	O
those	O
from	O
the	O
purple	O
class	O
as	O
yi	O
=	O
−1	O
.	O
then	O
a	O
separating	B
hyperplane	I
has	O
the	O
property	O
that	O
β0	O
+	O
β1xi1	O
+	O
β2xi2	O
+	O
.	O
.	O
.	O
+	O
βpxip	O
>	O
0	O
if	O
yi	O
=	O
1	O
,	O
(	O
9.6	O
)	O
and	O
β0	O
+	O
β1xi1	O
+	O
β2xi2	O
+	O
.	O
.	O
.	O
+	O
βpxip	O
<	O
0	O
if	O
yi	O
=	O
−1	O
.	O
(	O
9.7	O
)	O
equivalently	O
,	O
a	O
separating	B
hyperplane	I
has	O
the	O
property	O
that	O
yi	O
(	O
β0	O
+	O
β1xi1	O
+	O
β2xi2	O
+	O
.	O
.	O
.	O
+	O
βpxip	O
)	O
>	O
0	O
(	O
9.8	O
)	O
for	O
all	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
if	O
a	O
separating	B
hyperplane	I
exists	O
,	O
we	O
can	O
use	O
it	O
to	O
construct	O
a	O
very	O
natural	B
classiﬁer	O
:	O
a	O
test	B
observation	O
is	O
assigned	O
a	O
class	O
depending	O
on	O
which	O
side	O
of	O
the	O
hyperplane	B
it	O
is	O
located	O
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
9.2	O
shows	O
∗	O
an	O
example	O
of	O
such	O
a	O
classiﬁer	B
.	O
that	O
is	O
,	O
we	O
classify	O
the	O
test	B
observation	O
x	O
∗	O
based	O
on	O
the	O
sign	O
of	O
f	O
(	O
x	O
)	O
is	O
positive	O
,	O
∗	O
then	O
we	O
assign	O
the	O
test	B
observation	O
to	O
class	O
1	O
,	O
and	O
if	O
f	O
(	O
x	O
)	O
is	O
negative	O
,	O
then	O
we	O
assign	O
it	O
to	O
class	O
−1	O
.	O
we	O
can	O
also	O
make	O
use	O
of	O
the	O
magnitude	O
of	O
f	O
(	O
x	O
∗	O
)	O
.	O
if	O
∗	O
f	O
(	O
x	O
lies	O
far	O
from	O
the	O
hyperplane	B
,	O
∗	O
and	O
so	O
we	O
can	O
be	O
conﬁdent	O
about	O
our	O
class	O
assignment	O
for	O
x	O
.	O
on	O
the	O
other	O
∗	O
∗	O
∗	O
∗	O
p.	O
if	O
f	O
(	O
x	O
2+	O
.	O
.	O
.+βpx	O
1+β2x	O
)	O
=	O
β0+β1x	O
∗	O
)	O
is	O
far	O
from	O
zero	O
,	O
then	O
this	O
means	O
that	O
x	O
9.1	O
maximal	B
margin	I
classiﬁer	O
341	O
∗	O
)	O
is	O
close	O
to	O
zero	O
,	O
then	O
x	O
∗	O
hand	O
,	O
if	O
f	O
(	O
x	O
is	O
located	O
near	O
the	O
hyperplane	B
,	O
and	O
so	O
∗	O
we	O
are	O
less	O
certain	O
about	O
the	O
class	O
assignment	O
for	O
x	O
.	O
not	O
surprisingly	O
,	O
and	O
as	O
we	O
see	O
in	O
figure	O
9.2	O
,	O
a	O
classiﬁer	B
that	O
is	O
based	O
on	O
a	O
separating	B
hyperplane	I
leads	O
to	O
a	O
linear	B
decision	O
boundary	O
.	O
9.1.3	O
the	O
maximal	B
margin	I
classiﬁer	O
in	O
general	O
,	O
if	O
our	O
data	B
can	O
be	O
perfectly	O
separated	O
using	O
a	O
hyperplane	B
,	O
then	O
there	O
will	O
in	O
fact	O
exist	O
an	O
inﬁnite	O
number	O
of	O
such	O
hyperplanes	O
.	O
this	O
is	O
because	O
a	O
given	O
separating	B
hyperplane	I
can	O
usually	O
be	O
shifted	O
a	O
tiny	O
bit	O
up	O
or	O
down	O
,	O
or	O
rotated	O
,	O
without	O
coming	O
into	O
contact	O
with	O
any	O
of	O
the	O
observations	B
.	O
three	O
possible	O
separating	O
hyperplanes	O
are	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
9.2.	O
in	O
order	O
to	O
construct	O
a	O
classiﬁer	B
based	O
upon	O
a	O
separating	B
hyperplane	I
,	O
we	O
must	O
have	O
a	O
reasonable	O
way	O
to	O
decide	O
which	O
of	O
the	O
inﬁnite	O
possible	O
separating	O
hyperplanes	O
to	O
use	O
.	O
a	O
natural	B
choice	O
is	O
the	O
maximal	B
margin	I
hyperplane	O
(	O
also	O
known	O
as	O
the	O
optimal	B
separating	I
hyperplane	I
)	O
,	O
which	O
is	O
the	O
separating	B
hyperplane	I
that	O
is	O
farthest	O
from	O
the	O
training	B
observations	O
.	O
that	O
is	O
,	O
we	O
can	O
compute	O
the	O
(	O
perpendicular	B
)	O
distance	B
from	O
each	O
training	B
observation	O
to	O
a	O
given	O
separat-	O
ing	O
hyperplane	B
;	O
the	O
smallest	O
such	O
distance	B
is	O
the	O
minimal	O
distance	B
from	O
the	O
observations	B
to	O
the	O
hyperplane	B
,	O
and	O
is	O
known	O
as	O
the	O
margin	B
.	O
the	O
maximal	B
margin	I
hyperplane	O
is	O
the	O
separating	B
hyperplane	I
for	O
which	O
the	O
margin	B
is	O
largest—that	O
is	O
,	O
it	O
is	O
the	O
hyperplane	B
that	O
has	O
the	O
farthest	O
minimum	O
dis-	O
tance	O
to	O
the	O
training	B
observations	O
.	O
we	O
can	O
then	O
classify	O
a	O
test	B
observation	O
based	O
on	O
which	O
side	O
of	O
the	O
maximal	B
margin	I
hyperplane	O
it	O
lies	O
.	O
this	O
is	O
known	O
as	O
the	O
maximal	B
margin	I
classiﬁer	O
.	O
we	O
hope	O
that	O
a	O
classiﬁer	B
that	O
has	O
a	O
large	O
margin	B
on	O
the	O
training	B
data	O
will	O
also	O
have	O
a	O
large	O
margin	B
on	O
the	O
test	B
data	O
,	O
and	O
hence	O
will	O
classify	O
the	O
test	B
observations	O
correctly	O
.	O
although	O
the	O
maxi-	O
mal	O
margin	B
classiﬁer	O
is	O
often	O
successful	O
,	O
it	O
can	O
also	O
lead	O
to	O
overﬁtting	B
when	O
p	O
is	O
large	O
.	O
maximal	B
margin	I
hyperplane	O
optimal	B
separating	I
hyperplane	I
margin	O
maximal	B
margin	I
classiﬁer	O
if	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
are	O
the	O
coeﬃcients	O
of	O
the	O
maximal	B
margin	I
hyperplane	O
,	O
based	O
∗	O
then	O
the	O
maximal	B
margin	I
classiﬁer	O
classiﬁes	O
the	O
test	B
observation	O
x	O
∗	O
on	O
the	O
sign	O
of	O
f	O
(	O
x	O
∗	O
∗	O
∗	O
2	O
+	O
.	O
.	O
.	O
+	O
βpx	O
1	O
+	O
β2x	O
)	O
=	O
β0	O
+	O
β1x	O
p.	O
figure	O
9.3	O
shows	O
the	O
maximal	B
margin	I
hyperplane	O
on	O
the	O
data	B
set	O
of	O
figure	O
9.2.	O
comparing	O
the	O
right-hand	O
panel	O
of	O
figure	O
9.2	O
to	O
figure	O
9.3	O
,	O
we	O
see	O
that	O
the	O
maximal	B
margin	I
hyperplane	O
shown	O
in	O
figure	O
9.3	O
does	O
in-	O
deed	O
result	O
in	O
a	O
greater	O
minimal	O
distance	B
between	O
the	O
observations	B
and	O
the	O
separating	O
hyperplane—that	O
is	O
,	O
a	O
larger	O
margin	B
.	O
in	O
a	O
sense	O
,	O
the	O
maximal	B
margin	I
hyperplane	O
represents	O
the	O
mid-line	O
of	O
the	O
widest	O
“	O
slab	O
”	O
that	O
we	O
can	O
insert	O
between	O
the	O
two	O
classes	O
.	O
examining	O
figure	O
9.3	O
,	O
we	O
see	O
that	O
three	O
training	B
observations	O
are	O
equidis-	O
tant	O
from	O
the	O
maximal	B
margin	I
hyperplane	O
and	O
lie	O
along	O
the	O
dashed	O
lines	O
indicating	O
the	O
width	O
of	O
the	O
margin	B
.	O
these	O
three	O
observations	B
are	O
known	O
as	O
342	O
9.	O
support	B
vector	I
machines	O
3	O
2	O
2	O
x	O
1	O
0	O
1	O
−	O
−1	O
0	O
1	O
x1	O
2	O
3	O
figure	O
9.3.	O
there	O
are	O
two	O
classes	O
of	O
observations	B
,	O
shown	O
in	O
blue	O
and	O
in	O
pur-	O
ple	O
.	O
the	O
maximal	B
margin	I
hyperplane	O
is	O
shown	O
as	O
a	O
solid	O
line	B
.	O
the	O
margin	B
is	O
the	O
distance	B
from	O
the	O
solid	O
line	B
to	O
either	O
of	O
the	O
dashed	O
lines	O
.	O
the	O
two	O
blue	O
points	O
and	O
the	O
purple	O
point	O
that	O
lie	O
on	O
the	O
dashed	O
lines	O
are	O
the	O
support	O
vectors	O
,	O
and	O
the	O
dis-	O
tance	O
from	O
those	O
points	O
to	O
the	O
hyperplane	B
is	O
indicated	O
by	O
arrows	O
.	O
the	O
purple	O
and	O
blue	O
grid	O
indicates	O
the	O
decision	O
rule	O
made	O
by	O
a	O
classiﬁer	B
based	O
on	O
this	O
separating	B
hyperplane	I
.	O
support	O
vectors	O
,	O
since	O
they	O
are	O
vectors	O
in	O
p-dimensional	O
space	O
(	O
in	O
figure	O
9.3	O
,	O
p	O
=	O
2	O
)	O
and	O
they	O
“	O
support	O
”	O
the	O
maximal	B
margin	I
hyperplane	O
in	O
the	O
sense	O
that	O
if	O
these	O
points	O
were	O
moved	O
slightly	O
then	O
the	O
maximal	B
margin	I
hyper-	O
plane	O
would	O
move	O
as	O
well	O
.	O
interestingly	O
,	O
the	O
maximal	B
margin	I
hyperplane	O
depends	O
directly	O
on	O
the	O
support	O
vectors	O
,	O
but	O
not	O
on	O
the	O
other	O
observations	B
:	O
a	O
movement	O
to	O
any	O
of	O
the	O
other	O
observations	B
would	O
not	O
aﬀect	O
the	O
separating	B
hyperplane	I
,	O
provided	O
that	O
the	O
observation	O
’	O
s	O
movement	O
does	O
not	O
cause	O
it	O
to	O
cross	O
the	O
boundary	O
set	B
by	O
the	O
margin	B
.	O
the	O
fact	O
that	O
the	O
maximal	B
margin	I
hyperplane	O
depends	O
directly	O
on	O
only	O
a	O
small	O
subset	O
of	O
the	O
observations	B
is	O
an	O
important	O
property	O
that	O
will	O
arise	O
later	O
in	O
this	O
chapter	O
when	O
we	O
discuss	O
the	O
support	B
vector	I
classiﬁer	O
and	O
support	B
vector	I
machines	O
.	O
support	B
vector	I
9.1.4	O
construction	O
of	O
the	O
maximal	B
margin	I
classiﬁer	O
we	O
now	O
consider	O
the	O
task	O
of	O
constructing	O
the	O
maximal	B
margin	I
hyperplane	O
based	O
on	O
a	O
set	B
of	O
n	O
training	B
observations	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
∈	O
rp	O
and	O
associated	O
class	O
labels	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
∈	O
{	O
−1	O
,	O
1	O
}	O
.	O
brieﬂy	O
,	O
the	O
maximal	B
margin	I
hyperplane	O
is	O
the	O
solution	O
to	O
the	O
optimization	O
problem	O
9.1	O
maximal	B
margin	I
classiﬁer	O
343	O
β2	O
j	O
=	O
1	O
,	O
(	O
9.9	O
)	O
(	O
9.10	O
)	O
maximize	O
β0	O
,	O
β1	O
,	O
...	O
,	O
βp	O
,	O
m	O
m	O
p	O
(	O
cid:17	O
)	O
subject	O
to	O
j=1	O
yi	O
(	O
β0	O
+	O
β1xi1	O
+	O
β2xi2	O
+	O
.	O
.	O
.	O
+	O
βpxip	O
)	O
≥	O
m	O
∀	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
(	O
9.11	O
)	O
this	O
optimization	O
problem	O
(	O
9.9	O
)	O
–	O
(	O
9.11	O
)	O
is	O
actually	O
simpler	O
than	O
it	O
looks	O
.	O
first	O
of	O
all	O
,	O
the	O
constraint	O
in	O
(	O
9.11	O
)	O
that	O
yi	O
(	O
β0	O
+	O
β1xi1	O
+	O
β2xi2	O
+	O
.	O
.	O
.	O
+	O
βpxip	O
)	O
≥	O
m	O
∀	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
guarantees	O
that	O
each	O
observation	O
will	O
be	O
on	O
the	O
correct	O
side	O
of	O
the	O
hyper-	O
plane	O
,	O
provided	O
that	O
m	O
is	O
positive	O
.	O
(	O
actually	O
,	O
for	O
each	O
observation	O
to	O
be	O
on	O
the	O
correct	O
side	O
of	O
the	O
hyperplane	B
we	O
would	O
simply	O
need	O
yi	O
(	O
β0	O
+	O
β1xi1	O
+	O
β2xi2	O
+	O
.	O
.	O
.+βpxip	O
)	O
>	O
0	O
,	O
so	O
the	O
constraint	O
in	O
(	O
9.11	O
)	O
in	O
fact	O
requires	O
that	O
each	O
observation	O
be	O
on	O
the	O
correct	O
side	O
of	O
the	O
hyperplane	B
,	O
with	O
some	O
cushion	O
,	O
provided	O
that	O
m	O
is	O
positive	O
.	O
)	O
second	O
,	O
note	O
that	O
(	O
9.10	O
)	O
is	O
not	O
really	O
a	O
constraint	O
on	O
the	O
hyperplane	B
,	O
since	O
if	O
β0	O
+	O
β1xi1	O
+	O
β2xi2	O
+	O
.	O
.	O
.	O
+	O
βpxip	O
=	O
0	O
deﬁnes	O
a	O
hyperplane	B
,	O
then	O
so	O
does	O
k	O
(	O
β0	O
+	O
β1xi1	O
+	O
β2xi2	O
+	O
.	O
.	O
.	O
+	O
βpxip	O
)	O
=	O
0	O
for	O
any	O
k	O
(	O
cid:4	O
)	O
=	O
0.	O
however	O
,	O
(	O
9.10	O
)	O
adds	O
meaning	O
to	O
(	O
9.11	O
)	O
;	O
one	O
can	O
show	O
that	O
with	O
this	O
constraint	O
the	O
perpendicular	B
distance	O
from	O
the	O
ith	O
observation	O
to	O
the	O
hyperplane	B
is	O
given	O
by	O
yi	O
(	O
β0	O
+	O
β1xi1	O
+	O
β2xi2	O
+	O
.	O
.	O
.	O
+	O
βpxip	O
)	O
.	O
therefore	O
,	O
the	O
constraints	O
(	O
9.10	O
)	O
and	O
(	O
9.11	O
)	O
ensure	O
that	O
each	O
observation	O
is	O
on	O
the	O
correct	O
side	O
of	O
the	O
hyperplane	B
and	O
at	O
least	O
a	O
distance	B
m	O
from	O
the	O
hyperplane	B
.	O
hence	O
,	O
m	O
represents	O
the	O
margin	B
of	O
our	O
hyperplane	B
,	O
and	O
the	O
optimization	O
problem	O
chooses	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
to	O
maximize	O
m	O
.	O
this	O
is	O
exactly	O
the	O
deﬁnition	O
of	O
the	O
maximal	B
margin	I
hyperplane	O
!	O
the	O
problem	O
(	O
9.9	O
)	O
–	O
(	O
9.11	O
)	O
can	O
be	O
solved	O
eﬃciently	O
,	O
but	O
details	O
of	O
this	O
optimization	O
are	O
outside	O
of	O
the	O
scope	O
of	O
this	O
book	O
.	O
9.1.5	O
the	O
non-separable	O
case	O
the	O
maximal	B
margin	I
classiﬁer	O
is	O
a	O
very	O
natural	B
way	O
to	O
perform	O
classiﬁ-	O
cation	O
,	O
if	O
a	O
separating	B
hyperplane	I
exists	O
.	O
however	O
,	O
as	O
we	O
have	O
hinted	O
,	O
in	O
many	O
cases	O
no	O
separating	B
hyperplane	I
exists	O
,	O
and	O
so	O
there	O
is	O
no	O
maximal	B
margin	I
classiﬁer	O
.	O
in	O
this	O
case	O
,	O
the	O
optimization	O
problem	O
(	O
9.9	O
)	O
–	O
(	O
9.11	O
)	O
has	O
no	O
solution	O
with	O
m	O
>	O
0.	O
an	O
example	O
is	O
shown	O
in	O
figure	O
9.4.	O
in	O
this	O
case	O
,	O
we	O
can	O
not	O
exactly	O
separate	O
the	O
two	O
classes	O
.	O
however	O
,	O
as	O
we	O
will	O
see	O
in	O
the	O
next	O
section	O
,	O
we	O
can	O
extend	O
the	O
concept	O
of	O
a	O
separating	B
hyperplane	I
in	O
order	O
to	O
develop	O
a	O
hyperplane	B
that	O
almost	O
separates	O
the	O
classes	O
,	O
using	O
a	O
so-called	O
soft	O
margin	O
.	O
the	O
generalization	O
of	O
the	O
maximal	B
margin	I
classiﬁer	O
to	O
the	O
non-separable	O
case	O
is	O
known	O
as	O
the	O
support	B
vector	I
classiﬁer	O
.	O
344	O
9.	O
support	B
vector	I
machines	O
2	O
x	O
0	O
.	O
2	O
5	O
.	O
1	O
0	O
.	O
1	O
5	O
.	O
0	O
0	O
.	O
0	O
5	O
.	O
0	O
−	O
0	O
.	O
1	O
−	O
0	O
1	O
x1	O
2	O
3	O
figure	O
9.4.	O
there	O
are	O
two	O
classes	O
of	O
observations	B
,	O
shown	O
in	O
blue	O
and	O
in	O
pur-	O
ple	O
.	O
in	O
this	O
case	O
,	O
the	O
two	O
classes	O
are	O
not	O
separable	O
by	O
a	O
hyperplane	B
,	O
and	O
so	O
the	O
maximal	B
margin	I
classiﬁer	O
can	O
not	O
be	O
used	O
.	O
9.2	O
support	B
vector	I
classiﬁers	O
9.2.1	O
overview	O
of	O
the	O
support	B
vector	I
classiﬁer	O
in	O
figure	O
9.4	O
,	O
we	O
see	O
that	O
observations	B
that	O
belong	O
to	O
two	O
classes	O
are	O
not	O
necessarily	O
separable	O
by	O
a	O
hyperplane	B
.	O
in	O
fact	O
,	O
even	O
if	O
a	O
separating	O
hyper-	O
plane	O
does	O
exist	O
,	O
then	O
there	O
are	O
instances	O
in	O
which	O
a	O
classiﬁer	B
based	O
on	O
a	O
separating	B
hyperplane	I
might	O
not	O
be	O
desirable	O
.	O
a	O
classiﬁer	B
based	O
on	O
a	O
separating	B
hyperplane	I
will	O
necessarily	O
perfectly	O
classify	O
all	O
of	O
the	O
training	B
observations	O
;	O
this	O
can	O
lead	O
to	O
sensitivity	B
to	O
individual	O
observations	B
.	O
an	O
ex-	O
ample	O
is	O
shown	O
in	O
figure	O
9.5.	O
the	O
addition	O
of	O
a	O
single	B
observation	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
9.5	O
leads	O
to	O
a	O
dramatic	O
change	O
in	O
the	O
maxi-	O
mal	O
margin	B
hyperplane	O
.	O
the	O
resulting	O
maximal	B
margin	I
hyperplane	O
is	O
not	O
satisfactory—for	O
one	O
thing	O
,	O
it	O
has	O
only	O
a	O
tiny	O
margin	B
.	O
this	O
is	O
problematic	O
because	O
as	O
discussed	O
previously	O
,	O
the	O
distance	B
of	O
an	O
observation	O
from	O
the	O
hyperplane	B
can	O
be	O
seen	O
as	O
a	O
measure	O
of	O
our	O
conﬁdence	O
that	O
the	O
obser-	O
vation	O
was	O
correctly	O
classiﬁed	O
.	O
moreover	O
,	O
the	O
fact	O
that	O
the	O
maximal	O
mar-	O
gin	O
hyperplane	B
is	O
extremely	O
sensitive	O
to	O
a	O
change	O
in	O
a	O
single	B
observation	O
suggests	O
that	O
it	O
may	O
have	O
overﬁt	O
the	O
training	B
data	O
.	O
in	O
this	O
case	O
,	O
we	O
might	O
be	O
willing	O
to	O
consider	O
a	O
classiﬁer	B
based	O
on	O
a	O
hy-	O
perplane	O
that	O
does	O
not	O
perfectly	O
separate	O
the	O
two	O
classes	O
,	O
in	O
the	O
interest	O
of	O
9.2	O
support	B
vector	I
classiﬁers	O
345	O
2	O
x	O
3	O
2	O
1	O
0	O
1	O
−	O
2	O
x	O
3	O
2	O
1	O
0	O
1	O
−	O
−1	O
0	O
1	O
x1	O
2	O
3	O
−1	O
0	O
2	O
3	O
1	O
x1	O
figure	O
9.5.	O
left	O
:	O
two	O
classes	O
of	O
observations	B
are	O
shown	O
in	O
blue	O
and	O
in	O
purple	O
,	O
along	O
with	O
the	O
maximal	B
margin	I
hyperplane	O
.	O
right	O
:	O
an	O
additional	O
blue	O
observation	O
has	O
been	O
added	O
,	O
leading	O
to	O
a	O
dramatic	O
shift	O
in	O
the	O
maximal	B
margin	I
hyperplane	O
shown	O
as	O
a	O
solid	O
line	B
.	O
the	O
dashed	O
line	B
indicates	O
the	O
maximal	B
margin	I
hyperplane	O
that	O
was	O
obtained	O
in	O
the	O
absence	O
of	O
this	O
additional	O
point	O
.	O
•	O
greater	O
robustness	O
to	O
individual	O
observations	B
,	O
and	O
•	O
better	O
classiﬁcation	B
of	O
most	O
of	O
the	O
training	B
observations	O
.	O
that	O
is	O
,	O
it	O
could	O
be	O
worthwhile	O
to	O
misclassify	O
a	O
few	O
training	B
observations	O
in	O
order	O
to	O
do	O
a	O
better	O
job	O
in	O
classifying	O
the	O
remaining	O
observations	B
.	O
the	O
support	B
vector	I
classiﬁer	O
,	O
sometimes	O
called	O
a	O
soft	B
margin	I
classiﬁer	I
,	O
does	O
exactly	O
this	O
.	O
rather	O
than	O
seeking	O
the	O
largest	O
possible	O
margin	B
so	O
that	O
every	O
observation	O
is	O
not	O
only	O
on	O
the	O
correct	O
side	O
of	O
the	O
hyperplane	B
but	O
also	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
,	O
we	O
instead	O
allow	O
some	O
observations	B
to	O
be	O
on	O
the	O
incorrect	O
side	O
of	O
the	O
margin	B
,	O
or	O
even	O
the	O
incorrect	O
side	O
of	O
the	O
hyperplane	B
.	O
(	O
the	O
margin	B
is	O
soft	O
because	O
it	O
can	O
be	O
violated	O
by	O
some	O
of	O
the	O
training	B
observations	O
.	O
)	O
an	O
example	O
is	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
9.6.	O
most	O
of	O
the	O
observations	B
are	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
.	O
however	O
,	O
a	O
small	O
subset	O
of	O
the	O
observations	B
are	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
.	O
an	O
observation	O
can	O
be	O
not	O
only	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
,	O
but	O
also	O
on	O
the	O
wrong	O
side	O
of	O
the	O
hyperplane	B
.	O
in	O
fact	O
,	O
when	O
there	O
is	O
no	O
separating	B
hyperplane	I
,	O
such	O
a	O
situation	O
is	O
inevitable	O
.	O
observations	B
on	O
the	O
wrong	O
side	O
of	O
the	O
hyperplane	B
correspond	O
to	O
training	B
observations	O
that	O
are	O
misclassiﬁed	O
by	O
the	O
support	B
vector	I
classiﬁer	O
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
9.6	O
illustrates	O
such	O
a	O
scenario	O
.	O
9.2.2	O
details	O
of	O
the	O
support	B
vector	I
classiﬁer	O
the	O
support	B
vector	I
classiﬁer	O
classiﬁes	O
a	O
test	B
observation	O
depending	O
on	O
which	O
side	O
of	O
a	O
hyperplane	B
it	O
lies	O
.	O
the	O
hyperplane	B
is	O
chosen	O
to	O
correctly	O
support	B
vector	I
classiﬁer	O
soft	B
margin	I
classiﬁer	I
346	O
9.	O
support	B
vector	I
machines	O
2	O
x	O
4	O
3	O
2	O
1	O
0	O
1	O
−	O
10	O
9	O
8	O
2	O
x	O
7	O
1	O
3	O
5	O
2	O
4	O
6	O
4	O
3	O
2	O
1	O
0	O
1	O
−	O
3	O
12	O
5	O
4	O
6	O
10	O
9	O
8	O
7	O
11	O
1	O
2	O
−0.5	O
0.0	O
0.5	O
1.0	O
x1	O
1.5	O
2.0	O
2.5	O
−0.5	O
0.0	O
0.5	O
1.5	O
2.0	O
2.5	O
1.0	O
x1	O
figure	O
9.6.	O
left	O
:	O
a	O
support	B
vector	I
classiﬁer	O
was	O
ﬁt	B
to	O
a	O
small	O
data	B
set	O
.	O
the	O
hyperplane	B
is	O
shown	O
as	O
a	O
solid	O
line	B
and	O
the	O
margins	O
are	O
shown	O
as	O
dashed	O
lines	O
.	O
purple	O
observations	B
:	O
observations	B
3	O
,	O
4	O
,	O
5	O
,	O
and	O
6	O
are	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
,	O
observation	O
2	O
is	O
on	O
the	O
margin	B
,	O
and	O
observation	O
1	O
is	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
.	O
blue	O
observations	B
:	O
observations	B
7	O
and	O
10	O
are	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
,	O
observation	O
9	O
is	O
on	O
the	O
margin	B
,	O
and	O
observation	O
8	O
is	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
.	O
no	O
observations	B
are	O
on	O
the	O
wrong	O
side	O
of	O
the	O
hyperplane	B
.	O
right	O
:	O
same	O
as	O
left	O
panel	O
with	O
two	O
additional	O
points	O
,	O
11	O
and	O
12.	O
these	O
two	O
observations	B
are	O
on	O
the	O
wrong	O
side	O
of	O
the	O
hyperplane	B
and	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
.	O
separate	O
most	O
of	O
the	O
training	B
observations	O
into	O
the	O
two	O
classes	O
,	O
but	O
may	O
misclassify	O
a	O
few	O
observations	B
.	O
it	O
is	O
the	O
solution	O
to	O
the	O
optimization	O
problem	O
maximize	O
m	O
β0	O
,	O
β1	O
,	O
...	O
,	O
βp	O
,	O
1	O
,	O
...	O
,	O
n	O
,	O
m	O
p	O
(	O
cid:17	O
)	O
subject	O
to	O
β2	O
j	O
=	O
1	O
,	O
j=1	O
yi	O
(	O
β0	O
+	O
β1xi1	O
+	O
β2xi2	O
+	O
.	O
.	O
.	O
+	O
βpxip	O
)	O
≥	O
m	O
(	O
1	O
−	O
i	O
)	O
,	O
i	O
≥	O
0	O
,	O
i	O
≤	O
c	O
,	O
n	O
(	O
cid:17	O
)	O
(	O
9.12	O
)	O
(	O
9.13	O
)	O
(	O
9.14	O
)	O
(	O
9.15	O
)	O
i=1	O
where	O
c	O
is	O
a	O
nonnegative	O
tuning	B
parameter	I
.	O
as	O
in	O
(	O
9.11	O
)	O
,	O
m	O
is	O
the	O
width	O
of	O
the	O
margin	B
;	O
we	O
seek	O
to	O
make	O
this	O
quantity	O
as	O
large	O
as	O
possible	O
.	O
in	O
(	O
9.14	O
)	O
,	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
are	O
slack	O
variables	O
that	O
allow	O
individual	O
observations	B
to	O
be	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
or	O
the	O
hyperplane	B
;	O
we	O
will	O
explain	O
them	O
in	O
greater	O
detail	O
momentarily	O
.	O
once	O
we	O
have	O
solved	O
(	O
9.12	O
)	O
–	O
(	O
9.15	O
)	O
,	O
we	O
classify	O
∗	O
a	O
test	B
observation	O
x	O
as	O
before	O
,	O
by	O
simply	O
determining	O
on	O
which	O
side	O
of	O
the	O
hyperplane	B
it	O
lies	O
.	O
that	O
is	O
,	O
we	O
classify	O
the	O
test	B
observation	O
based	O
on	O
the	O
∗	O
sign	O
of	O
f	O
(	O
x	O
∗	O
∗	O
1	O
+	O
.	O
.	O
.	O
+	O
βpx	O
)	O
=	O
β0	O
+	O
β1x	O
p.	O
the	O
problem	O
(	O
9.12	O
)	O
–	O
(	O
9.15	O
)	O
seems	O
complex	O
,	O
but	O
insight	O
into	O
its	O
behavior	O
can	O
be	O
made	O
through	O
a	O
series	O
of	O
simple	B
observations	O
presented	O
below	O
.	O
first	O
of	O
all	O
,	O
the	O
slack	B
variable	I
i	O
tells	O
us	O
where	O
the	O
ith	O
observation	O
is	O
located	O
,	O
relative	O
to	O
the	O
hyperplane	B
and	O
relative	O
to	O
the	O
margin	B
.	O
if	O
i	O
=	O
0	O
then	O
the	O
ith	O
slack	B
variable	I
9.2	O
support	B
vector	I
classiﬁers	O
347	O
observation	O
is	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
,	O
as	O
we	O
saw	O
in	O
section	O
9.1.4.	O
if	O
i	O
>	O
0	O
then	O
the	O
ith	O
observation	O
is	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
,	O
and	O
we	O
say	O
that	O
the	O
ith	O
observation	O
has	O
violated	O
the	O
margin	B
.	O
if	O
i	O
>	O
1	O
then	O
it	O
is	O
on	O
the	O
wrong	O
side	O
of	O
the	O
hyperplane	B
.	O
we	O
now	O
consider	O
the	O
role	O
of	O
the	O
tuning	B
parameter	I
c.	O
in	O
(	O
9.15	O
)	O
,	O
c	O
bounds	O
the	O
sum	O
of	O
the	O
i	O
’	O
s	O
,	O
and	O
so	O
it	O
determines	O
the	O
number	O
and	O
severity	O
of	O
the	O
vio-	O
lations	O
to	O
the	O
margin	B
(	O
and	O
to	O
the	O
hyperplane	B
)	O
that	O
we	O
will	O
tolerate	O
.	O
we	O
can	O
think	O
of	O
c	O
as	O
a	O
budget	O
for	O
the	O
amount	O
that	O
the	O
margin	B
can	O
be	O
violated	O
by	O
the	O
n	O
observations	B
.	O
if	O
c	O
=	O
0	O
then	O
there	O
is	O
no	O
budget	O
for	O
violations	O
to	O
the	O
margin	B
,	O
and	O
it	O
must	O
be	O
the	O
case	O
that	O
1	O
=	O
.	O
.	O
.	O
=	O
n	O
=	O
0	O
,	O
in	O
which	O
case	O
(	O
9.12	O
)	O
–	O
(	O
9.15	O
)	O
simply	O
amounts	O
to	O
the	O
maximal	B
margin	I
hyperplane	O
optimiza-	O
tion	O
problem	O
(	O
9.9	O
)	O
–	O
(	O
9.11	O
)	O
.	O
(	O
of	O
course	O
,	O
a	O
maximal	B
margin	I
hyperplane	O
exists	O
only	O
if	O
the	O
two	O
classes	O
are	O
separable	O
.	O
)	O
for	O
c	O
>	O
0	O
no	O
more	O
than	O
c	O
observa-	O
tions	O
can	O
be	O
on	O
the	O
wrong	O
side	O
of	O
the	O
hyperplane	B
,	O
because	O
if	O
an	O
observation	O
is	O
on	O
the	O
wrong	O
side	O
of	O
the	O
hyperplane	B
then	O
i	O
>	O
1	O
,	O
and	O
(	O
9.15	O
)	O
requires	O
that	O
violations	O
to	O
the	O
margin	B
,	O
and	O
so	O
the	O
margin	B
will	O
widen	O
.	O
conversely	O
,	O
as	O
c	O
decreases	O
,	O
we	O
become	O
less	O
tolerant	O
of	O
violations	O
to	O
the	O
margin	B
and	O
so	O
the	O
margin	B
narrows	O
.	O
an	O
example	O
in	O
shown	O
in	O
figure	O
9.7.	O
i=1	O
i	O
≤	O
c.	O
as	O
the	O
budget	O
c	O
increases	O
,	O
we	O
become	O
more	O
tolerant	O
of	O
(	O
cid:10	O
)	O
n	O
in	O
practice	O
,	O
c	O
is	O
treated	O
as	O
a	O
tuning	B
parameter	I
that	O
is	O
generally	O
chosen	O
via	O
cross-validation	B
.	O
as	O
with	O
the	O
tuning	O
parameters	O
that	O
we	O
have	O
seen	O
through-	O
out	O
this	O
book	O
,	O
c	O
controls	O
the	O
bias-variance	B
trade-oﬀ	O
of	O
the	O
statistical	O
learn-	O
ing	O
technique	O
.	O
when	O
c	O
is	O
small	O
,	O
we	O
seek	O
narrow	O
margins	O
that	O
are	O
rarely	O
violated	O
;	O
this	O
amounts	O
to	O
a	O
classiﬁer	B
that	O
is	O
highly	O
ﬁt	B
to	O
the	O
data	B
,	O
which	O
may	O
have	O
low	O
bias	B
but	O
high	O
variance	B
.	O
on	O
the	O
other	O
hand	O
,	O
when	O
c	O
is	O
larger	O
,	O
the	O
margin	B
is	O
wider	O
and	O
we	O
allow	O
more	O
violations	O
to	O
it	O
;	O
this	O
amounts	O
to	O
ﬁtting	O
the	O
data	B
less	O
hard	O
and	O
obtaining	O
a	O
classiﬁer	B
that	O
is	O
potentially	O
more	O
biased	O
but	O
may	O
have	O
lower	O
variance	B
.	O
the	O
optimization	O
problem	O
(	O
9.12	O
)	O
–	O
(	O
9.15	O
)	O
has	O
a	O
very	O
interesting	O
property	O
:	O
it	O
turns	O
out	O
that	O
only	O
observations	B
that	O
either	O
lie	O
on	O
the	O
margin	B
or	O
that	O
violate	O
the	O
margin	B
will	O
aﬀect	O
the	O
hyperplane	B
,	O
and	O
hence	O
the	O
classiﬁer	B
ob-	O
tained	O
.	O
in	O
other	O
words	O
,	O
an	O
observation	O
that	O
lies	O
strictly	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
does	O
not	O
aﬀect	O
the	O
support	B
vector	I
classiﬁer	O
!	O
changing	O
the	O
position	O
of	O
that	O
observation	O
would	O
not	O
change	O
the	O
classiﬁer	B
at	O
all	O
,	O
provided	O
that	O
its	O
position	O
remains	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
.	O
observations	B
that	O
lie	O
directly	O
on	O
the	O
margin	B
,	O
or	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
for	O
their	O
class	O
,	O
are	O
known	O
as	O
support	O
vectors	O
.	O
these	O
observations	B
do	O
aﬀect	O
the	O
support	B
vector	I
classiﬁer	O
.	O
the	O
fact	O
that	O
only	O
support	O
vectors	O
aﬀect	O
the	O
classiﬁer	B
is	O
in	O
line	B
with	O
our	O
previous	O
assertion	O
that	O
c	O
controls	O
the	O
bias-variance	B
trade-oﬀ	O
of	O
the	O
support	B
vector	I
classiﬁer	O
.	O
when	O
the	O
tuning	B
parameter	I
c	O
is	O
large	O
,	O
then	O
the	O
margin	B
is	O
wide	O
,	O
many	O
observations	B
violate	O
the	O
margin	B
,	O
and	O
so	O
there	O
are	O
many	O
support	O
vectors	O
.	O
in	O
this	O
case	O
,	O
many	O
observations	B
are	O
involved	O
in	O
determining	O
the	O
hyperplane	B
.	O
the	O
top	O
left	O
panel	O
in	O
figure	O
9.7	O
illustrates	O
this	O
setting	O
:	O
this	O
classiﬁer	B
has	O
low	O
variance	B
(	O
since	O
many	O
observations	B
are	O
support	O
vectors	O
)	O
348	O
9.	O
support	B
vector	I
machines	O
3	O
2	O
1	O
2	O
x	O
0	O
1	O
−	O
2	O
−	O
3	O
−	O
3	O
2	O
1	O
2	O
x	O
0	O
1	O
−	O
2	O
−	O
3	O
−	O
−1	O
0	O
1	O
2	O
x1	O
3	O
2	O
1	O
2	O
x	O
0	O
1	O
−	O
2	O
−	O
3	O
−	O
3	O
2	O
1	O
2	O
x	O
0	O
1	O
−	O
2	O
−	O
3	O
−	O
−1	O
0	O
1	O
2	O
x1	O
−1	O
0	O
1	O
2	O
−1	O
0	O
1	O
2	O
x1	O
x1	O
figure	O
9.7.	O
a	O
support	B
vector	I
classiﬁer	O
was	O
ﬁt	B
using	O
four	O
diﬀerent	O
values	O
of	O
the	O
tuning	B
parameter	I
c	O
in	O
(	O
9.12	O
)	O
–	O
(	O
9.15	O
)	O
.	O
the	O
largest	O
value	O
of	O
c	O
was	O
used	O
in	O
the	O
top	O
left	O
panel	O
,	O
and	O
smaller	O
values	O
were	O
used	O
in	O
the	O
top	O
right	O
,	O
bottom	O
left	O
,	O
and	O
bottom	O
right	O
panels	O
.	O
when	O
c	O
is	O
large	O
,	O
then	O
there	O
is	O
a	O
high	O
tolerance	O
for	O
observations	O
being	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
,	O
and	O
so	O
the	O
margin	B
will	O
be	O
large	O
.	O
as	O
c	O
decreases	O
,	O
the	O
tolerance	O
for	O
observations	O
being	O
on	O
the	O
wrong	O
side	O
of	O
the	O
margin	B
decreases	O
,	O
and	O
the	O
margin	B
narrows	O
.	O
but	O
potentially	O
high	O
bias	B
.	O
in	O
contrast	B
,	O
if	O
c	O
is	O
small	O
,	O
then	O
there	O
will	O
be	O
fewer	O
support	O
vectors	O
and	O
hence	O
the	O
resulting	O
classiﬁer	B
will	O
have	O
low	O
bias	B
but	O
high	O
variance	B
.	O
the	O
bottom	O
right	O
panel	O
in	O
figure	O
9.7	O
illustrates	O
this	O
setting	O
,	O
with	O
only	O
eight	O
support	O
vectors	O
.	O
the	O
fact	O
that	O
the	O
support	B
vector	I
classiﬁer	O
’	O
s	O
decision	O
rule	O
is	O
based	O
only	O
on	O
a	O
potentially	O
small	O
subset	O
of	O
the	O
training	B
observations	O
(	O
the	O
support	O
vec-	O
tors	O
)	O
means	O
that	O
it	O
is	O
quite	O
robust	B
to	O
the	O
behavior	O
of	O
observations	B
that	O
are	O
far	O
away	O
from	O
the	O
hyperplane	B
.	O
this	O
property	O
is	O
distinct	O
from	O
some	O
of	O
the	O
other	O
classiﬁcation	B
methods	O
that	O
we	O
have	O
seen	O
in	O
preceding	O
chapters	O
,	O
such	O
as	O
linear	B
discriminant	I
analysis	I
.	O
recall	B
that	O
the	O
lda	O
classiﬁcation	B
rule	O
9.3	O
support	B
vector	I
machines	O
349	O
2	O
x	O
4	O
2	O
0	O
2	O
−	O
4	O
−	O
2	O
x	O
4	O
2	O
0	O
2	O
−	O
4	O
−	O
−4	O
−2	O
0	O
x1	O
2	O
4	O
−4	O
−2	O
2	O
4	O
0	O
x1	O
figure	O
9.8.	O
left	O
:	O
the	O
observations	B
fall	O
into	O
two	O
classes	O
,	O
with	O
a	O
non-linear	B
boundary	O
between	O
them	O
.	O
right	O
:	O
the	O
support	B
vector	I
classiﬁer	O
seeks	O
a	O
linear	B
bound-	O
ary	O
,	O
and	O
consequently	O
performs	O
very	O
poorly	O
.	O
depends	O
on	O
the	O
mean	O
of	O
all	O
of	O
the	O
observations	B
within	O
each	O
class	O
,	O
as	O
well	O
as	O
the	O
within-class	O
covariance	O
matrix	O
computed	O
using	O
all	O
of	O
the	O
observations	B
.	O
in	O
contrast	B
,	O
logistic	B
regression	I
,	O
unlike	O
lda	O
,	O
has	O
very	O
low	O
sensitivity	B
to	O
ob-	O
servations	O
far	O
from	O
the	O
decision	B
boundary	I
.	O
in	O
fact	O
we	O
will	O
see	O
in	O
section	O
9.5	O
that	O
the	O
support	B
vector	I
classiﬁer	O
and	O
logistic	B
regression	I
are	O
closely	O
related	O
.	O
9.3	O
support	B
vector	I
machines	O
we	O
ﬁrst	O
discuss	O
a	O
general	O
mechanism	O
for	O
converting	O
a	O
linear	B
classiﬁer	O
into	O
one	O
that	O
produces	O
non-linear	B
decision	O
boundaries	O
.	O
we	O
then	O
introduce	O
the	O
support	B
vector	I
machine	O
,	O
which	O
does	O
this	O
in	O
an	O
automatic	O
way	O
.	O
9.3.1	O
classiﬁcation	B
with	O
non-linear	B
decision	O
boundaries	O
the	O
support	B
vector	I
classiﬁer	O
is	O
a	O
natural	B
approach	O
for	O
classiﬁcation	O
in	O
the	O
two-class	O
setting	O
,	O
if	O
the	O
boundary	O
between	O
the	O
two	O
classes	O
is	O
linear	B
.	O
how-	O
ever	O
,	O
in	O
practice	O
we	O
are	O
sometimes	O
faced	O
with	O
non-linear	B
class	O
boundaries	O
.	O
for	O
instance	O
,	O
consider	O
the	O
data	B
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
9.8.	O
it	O
is	O
clear	O
that	O
a	O
support	B
vector	I
classiﬁer	O
or	O
any	O
linear	B
classiﬁer	O
will	O
perform	O
poorly	O
here	O
.	O
indeed	O
,	O
the	O
support	B
vector	I
classiﬁer	O
shown	O
in	O
the	O
right-hand	O
panel	O
of	O
figure	O
9.8	O
is	O
useless	O
here	O
.	O
in	O
chapter	O
7	O
,	O
we	O
are	O
faced	O
with	O
an	O
analogous	O
situation	O
.	O
we	O
see	O
there	O
that	O
the	O
performance	O
of	O
linear	B
regression	I
can	O
suﬀer	O
when	O
there	O
is	O
a	O
non-	O
linear	B
relationship	O
between	O
the	O
predictors	O
and	O
the	O
outcome	O
.	O
in	O
that	O
case	O
,	O
we	O
consider	O
enlarging	O
the	O
feature	B
space	O
using	O
functions	O
of	O
the	O
predictors	O
,	O
350	O
9.	O
support	B
vector	I
machines	O
such	O
as	O
quadratic	B
and	O
cubic	B
terms	O
,	O
in	O
order	O
to	O
address	O
this	O
non-linearity	O
.	O
in	O
the	O
case	O
of	O
the	O
support	B
vector	I
classiﬁer	O
,	O
we	O
could	O
address	O
the	O
prob-	O
lem	O
of	O
possibly	O
non-linear	B
boundaries	O
between	O
classes	O
in	O
a	O
similar	O
way	O
,	O
by	O
enlarging	O
the	O
feature	B
space	O
using	O
quadratic	B
,	O
cubic	B
,	O
and	O
even	O
higher-order	O
polynomial	B
functions	O
of	O
the	O
predictors	O
.	O
for	O
instance	O
,	O
rather	O
than	O
ﬁtting	O
a	O
support	B
vector	I
classiﬁer	O
using	O
p	O
features	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
,	O
we	O
could	O
instead	O
ﬁt	B
a	O
support	B
vector	I
classiﬁer	O
using	O
2p	O
features	O
x1	O
,	O
x	O
2	O
1	O
,	O
x2	B
,	O
x	O
2	O
2	O
,	O
.	O
.	O
.	O
,	O
xp	O
,	O
x	O
2	O
p	O
.	O
then	O
(	O
9.12	O
)	O
–	O
(	O
9.15	O
)	O
would	O
become	O
β0	O
,	O
β11	O
,	O
β12	O
...	O
.	O
,	O
βp1	O
,	O
βp2	O
,	O
1	O
,	O
...	O
,	O
n	O
m	O
,	O
m	O
maximize	O
⎛	O
⎝β0	O
+	O
p	O
(	O
cid:17	O
)	O
(	O
9.16	O
)	O
⎞	O
⎠	O
≥	O
m	O
(	O
1	O
−	O
i	O
)	O
,	O
p	O
(	O
cid:17	O
)	O
j=1	O
βj2x2	O
ij	O
subject	O
to	O
yi	O
n	O
(	O
cid:17	O
)	O
i	O
≤	O
c	O
,	O
i	O
≥	O
0	O
,	O
βj1xij	O
+	O
2	O
(	O
cid:17	O
)	O
j=1	O
p	O
(	O
cid:17	O
)	O
β2	O
jk	O
=	O
1.	O
i=1	O
j=1	O
k=1	O
xjxj	O
(	O
cid:2	O
)	O
for	O
j	O
(	O
cid:4	O
)	O
=	O
j	O
why	O
does	O
this	O
lead	O
to	O
a	O
non-linear	B
decision	O
boundary	O
?	O
in	O
the	O
enlarged	O
feature	B
space	O
,	O
the	O
decision	B
boundary	I
that	O
results	O
from	O
(	O
9.16	O
)	O
is	O
in	O
fact	O
lin-	O
ear	O
.	O
but	O
in	O
the	O
original	O
feature	B
space	O
,	O
the	O
decision	B
boundary	I
is	O
of	O
the	O
form	O
q	O
(	O
x	O
)	O
=	O
0	O
,	O
where	O
q	O
is	O
a	O
quadratic	B
polynomial	O
,	O
and	O
its	O
solutions	O
are	O
gener-	O
ally	O
non-linear	B
.	O
one	O
might	O
additionally	O
want	O
to	O
enlarge	O
the	O
feature	B
space	O
with	O
higher-order	O
polynomial	B
terms	O
,	O
or	O
with	O
interaction	B
terms	O
of	O
the	O
form	O
.	O
alternatively	O
,	O
other	O
functions	O
of	O
the	O
predictors	O
could	O
be	O
considered	O
rather	O
than	O
polynomials	O
.	O
it	O
is	O
not	O
hard	O
to	O
see	O
that	O
there	O
are	O
many	O
possible	O
ways	O
to	O
enlarge	O
the	O
feature	B
space	O
,	O
and	O
that	O
unless	O
we	O
are	O
careful	O
,	O
we	O
could	O
end	O
up	O
with	O
a	O
huge	O
number	O
of	O
features	O
.	O
then	O
compu-	O
tations	O
would	O
become	O
unmanageable	O
.	O
the	O
support	B
vector	I
machine	O
,	O
which	O
we	O
present	O
next	O
,	O
allows	O
us	O
to	O
enlarge	O
the	O
feature	B
space	O
used	O
by	O
the	O
support	B
vector	I
classiﬁer	O
in	O
a	O
way	O
that	O
leads	O
to	O
eﬃcient	O
computations	O
.	O
(	O
cid:5	O
)	O
9.3.2	O
the	O
support	B
vector	I
machine	O
the	O
support	B
vector	I
machine	O
(	O
svm	O
)	O
is	O
an	O
extension	O
of	O
the	O
support	B
vector	I
classiﬁer	O
that	O
results	O
from	O
enlarging	O
the	O
feature	B
space	O
in	O
a	O
speciﬁc	O
way	O
,	O
using	O
kernels	O
.	O
we	O
will	O
now	O
discuss	O
this	O
extension	O
,	O
the	O
details	O
of	O
which	O
are	O
somewhat	O
complex	O
and	O
beyond	O
the	O
scope	O
of	O
this	O
book	O
.	O
however	O
,	O
the	O
main	O
idea	O
is	O
described	O
in	O
section	O
9.3.1	O
:	O
we	O
may	O
want	O
to	O
enlarge	O
our	O
feature	B
space	O
support	B
vector	I
machine	O
kernel	B
9.3	O
support	B
vector	I
machines	O
351	O
in	O
order	O
to	O
accommodate	O
a	O
non-linear	B
boundary	O
between	O
the	O
classes	O
.	O
the	O
kernel	B
approach	O
that	O
we	O
describe	O
here	O
is	O
simply	O
an	O
eﬃcient	O
computational	O
approach	B
for	O
enacting	O
this	O
idea	O
.	O
we	O
have	O
not	O
discussed	O
exactly	O
how	O
the	O
support	B
vector	I
classiﬁer	O
is	O
com-	O
puted	O
because	O
the	O
details	O
become	O
somewhat	O
technical	O
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
the	O
solution	O
to	O
the	O
support	B
vector	I
classiﬁer	O
problem	O
(	O
9.12	O
)	O
–	O
(	O
9.15	O
)	O
involves	O
only	O
the	O
inner	O
products	O
of	O
the	O
observations	B
(	O
as	O
opposed	O
to	O
the	O
observations	B
themselves	O
)	O
.	O
the	O
inner	B
product	I
of	O
two	O
r-vectors	O
a	O
and	O
b	O
is	O
deﬁned	O
as	O
(	O
cid:19	O
)	O
a	O
,	O
b	O
(	O
cid:20	O
)	O
=	O
r	O
i=1	O
aibi	O
.	O
thus	O
the	O
inner	B
product	I
of	O
two	O
observations	B
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
is	O
given	O
by	O
(	O
cid:10	O
)	O
(	O
cid:19	O
)	O
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
(	O
cid:20	O
)	O
=	O
p	O
(	O
cid:17	O
)	O
xij	O
xi	O
(	O
cid:2	O
)	O
j.	O
j=1	O
(	O
9.17	O
)	O
(	O
9.18	O
)	O
it	O
can	O
be	O
shown	O
that	O
•	O
the	O
linear	B
support	O
vector	B
classiﬁer	O
can	O
be	O
represented	O
as	O
n	O
(	O
cid:17	O
)	O
αi	O
(	O
cid:19	O
)	O
x	O
,	O
xi	O
(	O
cid:20	O
)	O
,	O
f	O
(	O
x	O
)	O
=	O
β0	O
+	O
i=1	O
where	O
there	O
are	O
n	O
parameters	O
αi	O
,	O
observation	O
.	O
•	O
to	O
estimate	O
the	O
parameters	O
α1	O
,	O
.	O
.	O
.	O
,	O
αn	O
and	O
β0	O
,	O
all	O
we	O
need	O
are	O
the	O
(	O
cid:8	O
)	O
inner	O
products	O
(	O
cid:19	O
)	O
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
(	O
cid:20	O
)	O
between	O
all	O
pairs	O
of	O
training	B
observations	O
.	O
means	O
n	O
(	O
n	O
−	O
1	O
)	O
/2	O
,	O
and	O
gives	O
the	O
number	O
of	O
pairs	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
one	O
per	O
training	B
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
n	O
2	O
(	O
cid:9	O
)	O
n	O
2	O
(	O
the	O
notation	O
among	O
a	O
set	B
of	O
n	O
items	O
.	O
)	O
notice	O
that	O
in	O
(	O
9.18	O
)	O
,	O
in	O
order	O
to	O
evaluate	O
the	O
function	B
f	O
(	O
x	O
)	O
,	O
we	O
need	O
to	O
compute	O
the	O
inner	B
product	I
between	O
the	O
new	O
point	O
x	O
and	O
each	O
of	O
the	O
training	B
points	O
xi	O
.	O
however	O
,	O
it	O
turns	O
out	O
that	O
αi	O
is	O
nonzero	O
only	O
for	O
the	O
support	O
vectors	O
in	O
the	O
solution—that	O
is	O
,	O
if	O
a	O
training	B
observation	O
is	O
not	O
a	O
support	B
vector	I
,	O
then	O
its	O
αi	O
equals	O
zero	O
.	O
so	O
if	O
s	O
is	O
the	O
collection	O
of	O
indices	O
of	O
these	O
support	O
points	O
,	O
we	O
can	O
rewrite	O
any	O
solution	O
function	B
of	O
the	O
form	O
(	O
9.18	O
)	O
as	O
(	O
cid:17	O
)	O
f	O
(	O
x	O
)	O
=	O
β0	O
+	O
i∈s	O
αi	O
(	O
cid:19	O
)	O
x	O
,	O
xi	O
(	O
cid:20	O
)	O
,	O
(	O
9.19	O
)	O
which	O
typically	O
involves	O
far	O
fewer	O
terms	O
than	O
in	O
(	O
9.18	O
)	O
.2	O
to	O
summarize	O
,	O
in	O
representing	O
the	O
linear	B
classiﬁer	O
f	O
(	O
x	O
)	O
,	O
and	O
in	O
computing	O
its	O
coeﬃcients	O
,	O
all	O
we	O
need	O
are	O
inner	O
products	O
.	O
now	O
suppose	O
that	O
every	O
time	O
the	O
inner	B
product	I
(	O
9.17	O
)	O
appears	O
in	O
the	O
representation	O
(	O
9.18	O
)	O
,	O
or	O
in	O
a	O
calculation	O
of	O
the	O
solution	O
for	O
the	O
support	O
2by	O
expanding	O
each	O
of	O
the	O
inner	O
products	O
in	O
(	O
9.19	O
)	O
,	O
it	O
is	O
easy	O
to	O
see	O
that	O
f	O
(	O
x	O
)	O
is	O
a	O
linear	B
function	O
of	O
the	O
coordinates	O
of	O
x.	O
doing	O
so	O
also	O
establishes	O
the	O
correspondence	O
between	O
the	O
αi	O
and	O
the	O
original	O
parameters	O
βj	O
.	O
352	O
9.	O
support	B
vector	I
machines	O
vector	B
classiﬁer	O
,	O
we	O
replace	O
it	O
with	O
a	O
generalization	O
of	O
the	O
inner	B
product	I
of	O
the	O
form	O
k	O
(	O
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
)	O
,	O
(	O
9.20	O
)	O
where	O
k	O
is	O
some	O
function	B
that	O
we	O
will	O
refer	O
to	O
as	O
a	O
kernel	B
.	O
a	O
kernel	B
is	O
a	O
function	B
that	O
quantiﬁes	O
the	O
similarity	O
of	O
two	O
observations	B
.	O
for	O
instance	O
,	O
we	O
could	O
simply	O
take	O
p	O
(	O
cid:17	O
)	O
kernel	B
k	O
(	O
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
)	O
=	O
xij	O
xi	O
(	O
cid:2	O
)	O
j	O
,	O
j=1	O
(	O
9.21	O
)	O
which	O
would	O
just	O
give	O
us	O
back	O
the	O
support	B
vector	I
classiﬁer	O
.	O
equation	O
9.21	O
is	O
known	O
as	O
a	O
linear	B
kernel	I
because	O
the	O
support	B
vector	I
classiﬁer	O
is	O
linear	B
in	O
the	O
features	O
;	O
the	O
linear	B
kernel	I
essentially	O
quantiﬁes	O
the	O
similarity	O
of	O
a	O
pair	O
of	O
observations	B
using	O
pearson	O
(	O
standard	O
)	O
correlation	B
.	O
but	O
one	O
could	O
instead	O
choose	O
another	O
form	O
for	O
(	O
9.20	O
)	O
.	O
for	O
instance	O
,	O
one	O
could	O
replace	O
every	O
instance	O
of	O
p	O
j=1	O
xij	O
xi	O
(	O
cid:2	O
)	O
j	O
with	O
the	O
quantity	O
(	O
cid:10	O
)	O
k	O
(	O
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
)	O
=	O
(	O
1	O
+	O
p	O
(	O
cid:17	O
)	O
j=1	O
xij	O
xi	O
(	O
cid:2	O
)	O
j	O
)	O
d.	O
(	O
9.22	O
)	O
this	O
is	O
known	O
as	O
a	O
polynomial	B
kernel	O
of	O
degree	O
d	O
,	O
where	O
d	O
is	O
a	O
positive	O
integer	O
.	O
using	O
such	O
a	O
kernel	B
with	O
d	O
>	O
1	O
,	O
instead	O
of	O
the	O
standard	O
linear	O
kernel	B
(	O
9.21	O
)	O
,	O
in	O
the	O
support	B
vector	I
classiﬁer	O
algorithm	O
leads	O
to	O
a	O
much	O
more	O
ﬂexible	B
decision	O
boundary	O
.	O
it	O
essentially	O
amounts	O
to	O
ﬁtting	O
a	O
support	B
vector	I
classiﬁer	O
in	O
a	O
higher-dimensional	O
space	O
involving	O
polynomials	O
of	O
degree	O
d	O
,	O
rather	O
than	O
in	O
the	O
original	O
feature	B
space	O
.	O
when	O
the	O
support	B
vector	I
classiﬁer	O
is	O
combined	O
with	O
a	O
non-linear	B
kernel	O
such	O
as	O
(	O
9.22	O
)	O
,	O
the	O
resulting	O
classiﬁer	B
is	O
known	O
as	O
a	O
support	B
vector	I
machine	O
.	O
note	O
that	O
in	O
this	O
case	O
the	O
(	O
non-linear	B
)	O
function	B
has	O
the	O
form	O
(	O
cid:17	O
)	O
f	O
(	O
x	O
)	O
=	O
β0	O
+	O
αik	O
(	O
x	O
,	O
xi	O
)	O
.	O
i∈s	O
(	O
9.23	O
)	O
the	O
left-hand	O
panel	O
of	O
figure	O
9.9	O
shows	O
an	O
example	O
of	O
an	O
svm	O
with	O
a	O
polynomial	B
kernel	O
applied	O
to	O
the	O
non-linear	B
data	O
from	O
figure	O
9.8.	O
the	O
ﬁt	B
is	O
a	O
substantial	O
improvement	O
over	O
the	O
linear	B
support	O
vector	B
classiﬁer	O
.	O
when	O
d	O
=	O
1	O
,	O
then	O
the	O
svm	O
reduces	O
to	O
the	O
support	B
vector	I
classiﬁer	O
seen	O
earlier	O
in	O
this	O
chapter	O
.	O
the	O
polynomial	B
kernel	O
shown	O
in	O
(	O
9.22	O
)	O
is	O
one	O
example	O
of	O
a	O
possible	O
non-linear	B
kernel	O
,	O
but	O
alternatives	O
abound	O
.	O
another	O
popular	O
choice	O
is	O
the	O
radial	B
kernel	I
,	O
which	O
takes	O
the	O
form	O
k	O
(	O
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
)	O
=	O
exp	O
(	O
−γ	O
p	O
(	O
cid:17	O
)	O
j=1	O
(	O
xij	O
−	O
xi	O
(	O
cid:2	O
)	O
j	O
)	O
2	O
)	O
.	O
(	O
9.24	O
)	O
polynomial	B
kernel	O
radial	B
kernel	I
9.3	O
support	B
vector	I
machines	O
353	O
2	O
x	O
4	O
2	O
0	O
2	O
−	O
4	O
−	O
2	O
x	O
4	O
2	O
0	O
2	O
−	O
4	O
−	O
−4	O
−2	O
0	O
x1	O
2	O
4	O
−4	O
−2	O
2	O
4	O
0	O
x1	O
figure	O
9.9.	O
left	O
:	O
an	O
svm	O
with	O
a	O
polynomial	B
kernel	O
of	O
degree	O
3	O
is	O
applied	O
to	O
the	O
non-linear	B
data	O
from	O
figure	O
9.8	O
,	O
resulting	O
in	O
a	O
far	O
more	O
appropriate	O
decision	O
rule	O
.	O
right	O
:	O
an	O
svm	O
with	O
a	O
radial	B
kernel	I
is	O
applied	O
.	O
in	O
this	O
example	O
,	O
either	O
kernel	B
is	O
capable	O
of	O
capturing	O
the	O
decision	B
boundary	I
.	O
in	O
(	O
9.24	O
)	O
,	O
γ	O
is	O
a	O
positive	O
constant	O
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
9.9	O
shows	O
an	O
example	O
of	O
an	O
svm	O
with	O
a	O
radial	B
kernel	I
on	O
this	O
non-linear	B
data	O
;	O
it	O
also	O
does	O
a	O
good	O
job	O
in	O
separating	O
the	O
two	O
classes	O
.	O
(	O
cid:10	O
)	O
∗	O
p	O
j=1	O
(	O
x	O
j	O
how	O
does	O
the	O
radial	B
kernel	I
(	O
9.24	O
)	O
actually	O
work	O
?	O
if	O
a	O
given	O
test	B
obser-	O
∗	O
∗	O
∗	O
p	O
)	O
t	O
is	O
far	O
from	O
a	O
training	B
observation	O
xi	O
in	O
terms	O
of	O
1	O
.	O
.	O
.	O
x	O
=	O
(	O
x	O
vation	O
x	O
−	O
xij	O
)	O
2	O
will	O
be	O
large	O
,	O
and	O
so	O
k	O
(	O
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
)	O
=	O
(	O
cid:10	O
)	O
euclidean	O
distance	B
,	O
then	O
exp	O
(	O
−γ	O
−	O
xij	O
)	O
2	O
)	O
will	O
be	O
very	O
tiny	O
.	O
this	O
means	O
that	O
in	O
(	O
9.23	O
)	O
,	O
xi	O
∗	O
p	O
j=1	O
(	O
x	O
∗	O
j	O
will	O
play	O
virtually	O
no	O
role	O
in	O
f	O
(	O
x	O
)	O
.	O
recall	B
that	O
the	O
predicted	O
class	O
label	O
∗	O
for	O
the	O
test	B
observation	O
x	O
)	O
.	O
in	O
other	O
words	O
,	O
∗	O
training	B
observations	O
that	O
are	O
far	O
from	O
x	O
will	O
play	O
essentially	O
no	O
role	O
in	O
∗	O
the	O
predicted	O
class	O
label	O
for	O
x	O
.	O
this	O
means	O
that	O
the	O
radial	B
kernel	I
has	O
very	O
local	B
behavior	O
,	O
in	O
the	O
sense	O
that	O
only	O
nearby	O
training	B
observations	O
have	O
an	O
eﬀect	O
on	O
the	O
class	O
label	O
of	O
a	O
test	B
observation	O
.	O
∗	O
is	O
based	O
on	O
the	O
sign	O
of	O
f	O
(	O
x	O
what	O
is	O
the	O
advantage	O
of	O
using	O
a	O
kernel	B
rather	O
than	O
simply	O
enlarging	O
the	O
feature	B
space	O
using	O
functions	O
of	O
the	O
original	O
features	O
,	O
as	O
in	O
(	O
9.16	O
)	O
?	O
one	O
advantage	O
is	O
computational	O
,	O
and	O
it	O
amounts	O
to	O
the	O
fact	O
that	O
using	O
kernels	O
,	O
one	O
need	O
only	O
compute	O
k	O
(	O
xi	O
,	O
xi	O
(	O
cid:2	O
)	O
)	O
for	O
all	O
.	O
this	O
can	O
be	O
done	O
without	O
explicitly	O
working	O
in	O
the	O
enlarged	O
feature	B
space	O
.	O
this	O
is	O
im-	O
portant	O
because	O
in	O
many	O
applications	O
of	O
svms	O
,	O
the	O
enlarged	O
feature	B
space	O
is	O
so	O
large	O
that	O
computations	O
are	O
intractable	O
.	O
for	O
some	O
kernels	O
,	O
such	O
as	O
the	O
radial	B
kernel	I
(	O
9.24	O
)	O
,	O
the	O
feature	B
space	O
is	O
implicit	O
and	O
inﬁnite-dimensional	O
,	O
so	O
we	O
could	O
never	O
do	O
the	O
computations	O
there	O
anyway	O
!	O
distinct	O
pairs	O
i	O
,	O
i	O
(	O
cid:8	O
)	O
(	O
cid:5	O
)	O
(	O
cid:9	O
)	O
n	O
2	O
354	O
9.	O
support	B
vector	I
machines	O
e	O
t	O
a	O
r	O
e	O
v	O
i	O
t	O
i	O
s	O
o	O
p	O
e	O
u	O
r	O
t	O
0	O
1	O
.	O
8	O
0	O
.	O
6	O
0	O
.	O
4	O
0	O
.	O
2	O
.	O
0	O
0	O
.	O
0	O
e	O
t	O
a	O
r	O
e	O
v	O
i	O
t	O
i	O
s	O
o	O
p	O
e	O
u	O
r	O
t	O
0	O
1	O
.	O
.	O
8	O
0	O
6	O
0	O
.	O
4	O
0	O
.	O
2	O
.	O
0	O
0	O
.	O
0	O
support	B
vector	I
classifier	O
lda	O
support	B
vector	I
classifier	O
svm	O
:	O
γ=10	O
svm	O
:	O
γ=10	O
svm	O
:	O
γ=10	O
−3	O
−2	O
−1	O
0.0	O
0.2	O
0.4	O
0.6	O
0.8	O
1.0	O
0.0	O
0.2	O
0.4	O
0.6	O
0.8	O
1.0	O
false	B
positive	I
rate	I
false	O
positive	O
rate	O
figure	O
9.10.	O
roc	O
curves	O
for	O
the	O
heart	O
data	B
training	O
set	B
.	O
left	O
:	O
the	O
support	B
vector	I
classiﬁer	O
and	O
lda	O
are	O
compared	O
.	O
right	O
:	O
the	O
support	B
vector	I
classiﬁer	O
is	O
−1	O
.	O
compared	O
to	O
an	O
svm	O
using	O
a	O
radial	B
basis	O
kernel	B
with	O
γ	O
=	O
10	O
−2	O
,	O
and	O
10	O
−3	O
,	O
10	O
9.3.3	O
an	O
application	O
to	O
the	O
heart	O
disease	O
data	B
in	O
chapter	O
8	O
we	O
apply	O
decision	O
trees	O
and	O
related	O
methods	O
to	O
the	O
heart	O
data	B
.	O
the	O
aim	O
is	O
to	O
use	O
13	O
predictors	O
such	O
as	O
age	O
,	O
sex	O
,	O
and	O
chol	O
in	O
order	O
to	O
predict	O
whether	O
an	O
individual	O
has	O
heart	O
disease	O
.	O
we	O
now	O
investigate	O
how	O
an	O
svm	O
compares	O
to	O
lda	O
on	O
this	O
data	B
.	O
after	O
removing	O
6	O
missing	O
observations	O
,	O
the	O
data	B
consist	O
of	O
297	O
subjects	O
,	O
which	O
we	O
randomly	O
split	O
into	O
207	O
training	B
and	O
90	O
test	B
observations	O
.	O
we	O
ﬁrst	O
ﬁt	B
lda	O
and	O
the	O
support	B
vector	I
classiﬁer	O
to	O
the	O
training	B
data	O
.	O
note	O
that	O
the	O
support	B
vector	I
classiﬁer	O
is	O
equivalent	O
to	O
a	O
svm	O
using	O
a	O
poly-	O
nomial	O
kernel	B
of	O
degree	O
d	O
=	O
1.	O
the	O
left-hand	O
panel	O
of	O
figure	O
9.10	O
displays	O
roc	O
curves	O
(	O
described	O
in	O
section	O
4.4.3	O
)	O
for	O
the	O
training	B
set	O
predictions	O
for	O
both	O
lda	O
and	O
the	O
support	B
vector	I
classiﬁer	O
.	O
both	O
classiﬁers	O
compute	O
scores	O
of	O
the	O
form	O
ˆf	O
(	O
x	O
)	O
=	O
ˆβ0	O
+	O
ˆβ1x1	O
+	O
ˆβ2x2	O
+	O
.	O
.	O
.	O
+	O
ˆβpxp	O
for	O
each	O
observation	O
.	O
for	O
any	O
given	O
cutoﬀ	O
t	O
,	O
we	O
classify	O
observations	B
into	O
the	O
heart	O
disease	O
or	O
no	O
heart	O
disease	O
categories	O
depending	O
on	O
whether	O
ˆf	O
(	O
x	O
)	O
<	O
t	O
or	O
ˆf	O
(	O
x	O
)	O
≥	O
t.	O
the	O
roc	O
curve	O
is	O
obtained	O
by	O
forming	O
these	O
predictions	O
and	O
computing	O
the	O
false	B
positive	I
and	O
true	B
positive	I
rates	O
for	O
a	O
range	O
of	O
values	O
of	O
t.	O
an	O
opti-	O
mal	O
classiﬁer	B
will	O
hug	O
the	O
top	O
left	O
corner	O
of	O
the	O
roc	O
plot	B
.	O
in	O
this	O
instance	O
lda	O
and	O
the	O
support	B
vector	I
classiﬁer	O
both	O
perform	O
well	O
,	O
though	O
there	O
is	O
a	O
suggestion	O
that	O
the	O
support	B
vector	I
classiﬁer	O
may	O
be	O
slightly	O
superior	O
.	O
the	O
right-hand	O
panel	O
of	O
figure	O
9.10	O
displays	O
roc	O
curves	O
for	O
svms	O
using	O
a	O
radial	B
kernel	I
,	O
with	O
various	O
values	O
of	O
γ.	O
as	O
γ	O
increases	O
and	O
the	O
ﬁt	B
becomes	O
−1	O
appears	O
to	O
give	O
more	O
non-linear	B
,	O
the	O
roc	O
curves	O
improve	O
.	O
using	O
γ	O
=	O
10	O
an	O
almost	O
perfect	O
roc	O
curve	O
.	O
however	O
,	O
these	O
curves	O
represent	O
training	B
error	O
rates	O
,	O
which	O
can	O
be	O
misleading	O
in	O
terms	O
of	O
performance	O
on	O
new	O
test	B
data	O
.	O
figure	O
9.11	O
displays	O
roc	O
curves	O
computed	O
on	O
the	O
90	O
test	B
observa-	O
9.4	O
svms	O
with	O
more	O
than	O
two	O
classes	O
355	O
e	O
t	O
a	O
r	O
e	O
v	O
i	O
t	O
i	O
s	O
o	O
p	O
e	O
u	O
r	O
t	O
.	O
0	O
1	O
8	O
0	O
.	O
6	O
0	O
.	O
4	O
0	O
.	O
2	O
.	O
0	O
0	O
.	O
0	O
e	O
t	O
a	O
r	O
e	O
v	O
i	O
t	O
i	O
s	O
o	O
p	O
e	O
u	O
r	O
t	O
0	O
1	O
.	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
0	O
.	O
2	O
.	O
0	O
0	O
.	O
0	O
support	B
vector	I
classifier	O
lda	O
support	B
vector	I
classifier	O
svm	O
:	O
γ=10	O
svm	O
:	O
γ=10	O
svm	O
:	O
γ=10	O
−3	O
−2	O
−1	O
0.0	O
0.2	O
0.4	O
0.6	O
0.8	O
1.0	O
0.0	O
0.2	O
0.4	O
0.6	O
0.8	O
1.0	O
false	B
positive	I
rate	I
false	O
positive	O
rate	O
figure	O
9.11.	O
roc	O
curves	O
for	O
the	O
test	B
set	O
of	O
the	O
heart	O
data	B
.	O
left	O
:	O
the	O
support	B
vector	I
classiﬁer	O
and	O
lda	O
are	O
compared	O
.	O
right	O
:	O
the	O
support	B
vector	I
classiﬁer	O
is	O
−1	O
.	O
compared	O
to	O
an	O
svm	O
using	O
a	O
radial	B
basis	O
kernel	B
with	O
γ	O
=	O
10	O
−2	O
,	O
and	O
10	O
−3	O
,	O
10	O
tions	O
.	O
we	O
observe	O
some	O
diﬀerences	O
from	O
the	O
training	B
roc	O
curves	O
.	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
9.11	O
,	O
the	O
support	B
vector	I
classiﬁer	O
appears	O
to	O
have	O
a	O
small	O
advantage	O
over	O
lda	O
(	O
although	O
these	O
diﬀerences	O
are	O
not	O
statisti-	O
−1	O
,	O
which	O
cally	O
signiﬁcant	O
)	O
.	O
in	O
the	O
right-hand	O
panel	O
,	O
the	O
svm	O
using	O
γ	O
=	O
10	O
showed	O
the	O
best	O
results	O
on	O
the	O
training	B
data	O
,	O
produces	O
the	O
worst	O
estimates	O
on	O
the	O
test	B
data	O
.	O
this	O
is	O
once	O
again	O
evidence	O
that	O
while	O
a	O
more	O
ﬂexible	B
method	O
will	O
often	O
produce	O
lower	O
training	B
error	O
rates	O
,	O
this	O
does	O
not	O
neces-	O
−2	O
sarily	O
lead	O
to	O
improved	O
performance	O
on	O
test	B
data	O
.	O
the	O
svms	O
with	O
γ	O
=	O
10	O
−3	O
perform	O
comparably	O
to	O
the	O
support	B
vector	I
classiﬁer	O
,	O
and	O
all	O
and	O
γ	O
=	O
10	O
three	O
outperform	O
the	O
svm	O
with	O
γ	O
=	O
10	O
−1	O
.	O
9.4	O
svms	O
with	O
more	O
than	O
two	O
classes	O
so	O
far	O
,	O
our	O
discussion	O
has	O
been	O
limited	O
to	O
the	O
case	O
of	O
binary	B
classiﬁcation	O
:	O
that	O
is	O
,	O
classiﬁcation	B
in	O
the	O
two-class	O
setting	O
.	O
how	O
can	O
we	O
extend	O
svms	O
to	O
the	O
more	O
general	O
case	O
where	O
we	O
have	O
some	O
arbitrary	O
number	O
of	O
classes	O
?	O
it	O
turns	O
out	O
that	O
the	O
concept	O
of	O
separating	O
hyperplanes	O
upon	O
which	O
svms	O
are	O
based	O
does	O
not	O
lend	O
itself	O
naturally	O
to	O
more	O
than	O
two	O
classes	O
.	O
though	O
a	O
number	O
of	O
proposals	O
for	O
extending	O
svms	O
to	O
the	O
k-class	O
case	O
have	O
been	O
made	O
,	O
the	O
two	O
most	O
popular	O
are	O
the	O
one-versus-one	B
and	O
one-versus-all	B
approaches	O
.	O
we	O
brieﬂy	O
discuss	O
those	O
two	O
approaches	O
here	O
.	O
9.4.1	O
one-versus-one	B
classiﬁcation	O
(	O
cid:9	O
)	O
suppose	O
that	O
we	O
would	O
like	O
to	O
perform	O
classiﬁcation	B
using	O
svms	O
,	O
and	O
there	O
are	O
k	O
>	O
2	O
classes	O
.	O
a	O
one-versus-one	B
or	O
all-pairs	O
approach	B
constructs	O
(	O
cid:8	O
)	O
k	O
2	O
one-versus-	O
one	O
356	O
9.	O
support	B
vector	I
machines	O
svms	O
,	O
each	O
of	O
which	O
compares	O
a	O
pair	O
of	O
classes	O
.	O
for	O
example	O
,	O
one	O
such	O
(	O
cid:5	O
)	O
(	O
cid:9	O
)	O
svm	O
might	O
compare	O
the	O
kth	O
class	O
,	O
coded	O
as	O
+1	O
,	O
to	O
the	O
k	O
th	O
class	O
,	O
coded	O
as	O
−1	O
.	O
we	O
classify	O
a	O
test	B
observation	O
using	O
each	O
of	O
the	O
k	O
classiﬁers	O
,	O
and	O
2	O
we	O
tally	O
the	O
number	O
of	O
times	O
that	O
the	O
test	B
observation	O
is	O
assigned	O
to	O
each	O
of	O
the	O
k	O
classes	O
.	O
the	O
ﬁnal	O
classiﬁcation	B
is	O
performed	O
by	O
assigning	O
the	O
test	B
(	O
cid:8	O
)	O
observation	O
to	O
the	O
class	O
to	O
which	O
it	O
was	O
most	O
frequently	O
assigned	O
in	O
these	O
k	O
2	O
pairwise	O
classiﬁcations	O
.	O
(	O
cid:9	O
)	O
(	O
cid:8	O
)	O
9.4.2	O
one-versus-all	B
classiﬁcation	O
the	O
one-versus-all	B
approach	O
is	O
an	O
alternative	O
procedure	O
for	O
applying	O
svms	O
one-versus-	O
in	O
the	O
case	O
of	O
k	O
>	O
2	O
classes	O
.	O
we	O
ﬁt	B
k	O
svms	O
,	O
each	O
time	O
comparing	O
one	O
of	O
the	O
k	O
classes	O
to	O
the	O
remaining	O
k	O
−	O
1	O
classes	O
.	O
let	O
β0k	O
,	O
β1k	O
,	O
.	O
.	O
.	O
,	O
βpk	O
denote	O
the	O
parameters	O
that	O
result	O
from	O
ﬁtting	O
an	O
svm	O
comparing	O
the	O
kth	O
class	O
(	O
coded	O
as	O
+1	O
)	O
to	O
the	O
others	O
(	O
coded	O
as	O
−1	O
)	O
.	O
let	O
x	O
∗	O
denote	O
a	O
test	B
observation	O
.	O
∗	O
∗	O
we	O
assign	O
the	O
observation	O
to	O
the	O
class	O
for	O
which	O
β0k	O
+	O
β1kx	O
2	O
+	O
.	O
.	O
.	O
+	O
1	O
+	O
β2kx	O
∗	O
βpkx	O
p	O
is	O
largest	O
,	O
as	O
this	O
amounts	O
to	O
a	O
high	O
level	B
of	O
conﬁdence	O
that	O
the	O
test	B
observation	O
belongs	O
to	O
the	O
kth	O
class	O
rather	O
than	O
to	O
any	O
of	O
the	O
other	O
classes	O
.	O
all	O
9.5	O
relationship	O
to	O
logistic	B
regression	I
when	O
svms	O
were	O
ﬁrst	O
introduced	O
in	O
the	O
mid-1990s	O
,	O
they	O
made	O
quite	O
a	O
splash	O
in	O
the	O
statistical	O
and	O
machine	B
learning	O
communities	O
.	O
this	O
was	O
due	O
in	O
part	O
to	O
their	O
good	O
performance	O
,	O
good	O
marketing	O
,	O
and	O
also	O
to	O
the	O
fact	O
that	O
the	O
underlying	O
approach	B
seemed	O
both	O
novel	O
and	O
mysterious	O
.	O
the	O
idea	O
of	O
ﬁnding	O
a	O
hyperplane	B
that	O
separates	O
the	O
data	B
as	O
well	O
as	O
possible	O
,	O
while	O
al-	O
lowing	O
some	O
violations	O
to	O
this	O
separation	O
,	O
seemed	O
distinctly	O
diﬀerent	O
from	O
classical	O
approaches	O
for	O
classiﬁcation	O
,	O
such	O
as	O
logistic	B
regression	I
and	O
lin-	O
ear	O
discriminant	O
analysis	O
.	O
moreover	O
,	O
the	O
idea	O
of	O
using	O
a	O
kernel	B
to	O
expand	O
the	O
feature	B
space	O
in	O
order	O
to	O
accommodate	O
non-linear	B
class	O
boundaries	O
ap-	O
peared	O
to	O
be	O
a	O
unique	O
and	O
valuable	O
characteristic	O
.	O
however	O
,	O
since	O
that	O
time	O
,	O
deep	O
connections	O
between	O
svms	O
and	O
other	O
more	O
classical	O
statistical	O
methods	O
have	O
emerged	O
.	O
it	O
turns	O
out	O
that	O
one	O
can	O
rewrite	O
the	O
criterion	O
(	O
9.12	O
)	O
–	O
(	O
9.15	O
)	O
for	O
ﬁtting	O
the	O
support	B
vector	I
classiﬁer	O
f	O
(	O
x	O
)	O
=	O
β0	O
+	O
β1x1	O
+	O
.	O
.	O
.	O
+	O
βpxp	O
as	O
⎧⎨	O
n	O
(	O
cid:17	O
)	O
⎩	O
i=1	O
max	O
[	O
0	O
,	O
1	O
−	O
yif	O
(	O
xi	O
)	O
]	O
+	O
λ	O
p	O
(	O
cid:17	O
)	O
j=1	O
β2	O
j	O
minimize	O
β0	O
,	O
β1	O
,	O
...	O
,	O
βp	O
⎫⎬	O
⎭	O
,	O
(	O
9.25	O
)	O
9.5	O
relationship	O
to	O
logistic	B
regression	I
357	O
where	O
λ	O
is	O
a	O
nonnegative	O
tuning	B
parameter	I
.	O
when	O
λ	O
is	O
large	O
then	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
are	O
small	O
,	O
more	O
violations	O
to	O
the	O
margin	B
are	O
tolerated	O
,	O
and	O
a	O
low-variance	O
but	O
high-bias	O
classiﬁer	B
will	O
result	O
.	O
when	O
λ	O
is	O
small	O
then	O
few	O
violations	O
to	O
the	O
margin	B
will	O
occur	O
;	O
this	O
amounts	O
to	O
a	O
high-variance	O
but	O
low-bias	O
classiﬁer	B
.	O
thus	O
,	O
a	O
small	O
value	O
of	O
λ	O
in	O
(	O
9.25	O
)	O
amounts	O
to	O
a	O
small	O
value	O
of	O
c	O
in	O
(	O
9.15	O
)	O
.	O
note	O
that	O
the	O
λ	O
j	O
term	B
in	O
(	O
9.25	O
)	O
is	O
the	O
ridge	O
penalty	O
term	B
from	O
section	O
6.2.1	O
,	O
and	O
plays	O
a	O
similar	O
role	O
in	O
controlling	O
the	O
bias-variance	B
trade-oﬀ	O
for	O
the	O
support	B
vector	I
classiﬁer	O
.	O
p	O
j=1	O
β2	O
(	O
cid:10	O
)	O
now	O
(	O
9.25	O
)	O
takes	O
the	O
“	O
loss	O
+	O
penalty	B
”	O
form	O
that	O
we	O
have	O
seen	O
repeatedly	O
throughout	O
this	O
book	O
:	O
minimize	O
β0	O
,	O
β1	O
,	O
...	O
,	O
βp	O
{	O
l	O
(	O
x	O
,	O
y	O
,	O
β	O
)	O
+	O
λp	O
(	O
β	O
)	O
}	O
.	O
(	O
9.26	O
)	O
in	O
(	O
9.26	O
)	O
,	O
l	O
(	O
x	O
,	O
y	O
,	O
β	O
)	O
is	O
some	O
loss	B
function	I
quantifying	O
the	O
extent	O
to	O
which	O
the	O
model	B
,	O
parametrized	O
by	O
β	O
,	O
ﬁts	O
the	O
data	B
(	O
x	O
,	O
y	O
)	O
,	O
and	O
p	O
(	O
β	O
)	O
is	O
a	O
penalty	B
function	O
on	O
the	O
parameter	B
vector	O
β	O
whose	O
eﬀect	O
is	O
controlled	O
by	O
a	O
nonneg-	O
ative	O
tuning	B
parameter	I
λ.	O
for	O
instance	O
,	O
ridge	B
regression	I
and	O
the	O
lasso	B
both	O
take	O
this	O
form	O
with	O
n	O
(	O
cid:17	O
)	O
⎛	O
⎝yi	O
−	O
β0	O
−	O
p	O
(	O
cid:17	O
)	O
i=1	O
j=1	O
⎞	O
⎠	O
2	O
xijβj	O
(	O
cid:10	O
)	O
|βj|	O
for	O
p	O
j=1	O
l	O
(	O
x	O
,	O
y	O
,	O
β	O
)	O
=	O
(	O
cid:10	O
)	O
p	O
j=1	O
β2	O
n	O
(	O
cid:17	O
)	O
and	O
with	O
p	O
(	O
β	O
)	O
=	O
the	O
lasso	B
.	O
in	O
the	O
case	O
of	O
(	O
9.25	O
)	O
the	O
loss	B
function	I
instead	O
takes	O
the	O
form	O
j	O
for	O
ridge	O
regression	B
and	O
p	O
(	O
β	O
)	O
=	O
l	O
(	O
x	O
,	O
y	O
,	O
β	O
)	O
=	O
i=1	O
max	O
[	O
0	O
,	O
1	O
−	O
yi	O
(	O
β0	O
+	O
β1xi1	O
+	O
.	O
.	O
.	O
+	O
βpxip	O
)	O
]	O
.	O
hinge	B
loss	I
this	O
is	O
known	O
as	O
hinge	B
loss	I
,	O
and	O
is	O
depicted	O
in	O
figure	O
9.12.	O
however	O
,	O
it	O
turns	O
out	O
that	O
the	O
hinge	B
loss	I
function	O
is	O
closely	O
related	O
to	O
the	O
loss	B
function	I
used	O
in	O
logistic	B
regression	I
,	O
also	O
shown	O
in	O
figure	O
9.12.	O
an	O
interesting	O
characteristic	O
of	O
the	O
support	B
vector	I
classiﬁer	O
is	O
that	O
only	O
support	O
vectors	O
play	O
a	O
role	O
in	O
the	O
classiﬁer	B
obtained	O
;	O
observations	B
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
do	O
not	O
aﬀect	O
it	O
.	O
this	O
is	O
due	O
to	O
the	O
fact	O
that	O
the	O
loss	B
function	I
shown	O
in	O
figure	O
9.12	O
is	O
exactly	O
zero	O
for	O
observations	O
for	O
which	O
yi	O
(	O
β0	O
+	O
β1xi1	O
+	O
.	O
.	O
.	O
+	O
βpxip	O
)	O
≥	O
1	O
;	O
these	O
correspond	O
to	O
observations	B
that	O
are	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin.3	O
in	O
contrast	B
,	O
the	O
loss	B
function	I
for	O
logistic	B
regression	I
shown	O
in	O
figure	O
9.12	O
is	O
not	O
exactly	O
zero	O
anywhere	O
.	O
but	O
it	O
is	O
very	O
small	O
for	O
observations	O
that	O
are	O
far	O
from	O
the	O
decision	B
boundary	I
.	O
due	O
to	O
the	O
similarities	O
between	O
their	O
loss	O
functions	O
,	O
logistic	B
regression	I
and	O
the	O
support	B
vector	I
classiﬁer	O
often	O
give	O
very	O
similar	O
results	O
.	O
when	O
the	O
classes	O
are	O
well	O
separated	O
,	O
svms	O
tend	O
to	O
behave	O
better	O
than	O
logistic	B
regression	I
;	O
in	O
more	O
overlapping	O
regimes	O
,	O
logistic	B
regression	I
is	O
often	O
preferred	O
.	O
3with	O
this	O
hinge-loss	O
+	O
penalty	B
representation	O
,	O
the	O
margin	B
corresponds	O
to	O
the	O
value	O
one	O
,	O
and	O
the	O
width	O
of	O
the	O
margin	B
is	O
determined	O
by	O
(	O
cid:2	O
)	O
β2	O
j	O
.	O
358	O
9.	O
support	B
vector	I
machines	O
8	O
6	O
s	O
s	O
o	O
l	O
4	O
2	O
0	O
−6	O
svm	O
loss	O
logistic	O
regression	B
loss	O
−2	O
−4	O
2	O
yi	O
(	O
β0	O
+	O
β1xi1	O
+	O
.	O
.	O
.	O
+	O
βpxip	O
)	O
0	O
figure	O
9.12.	O
the	O
svm	O
and	O
logistic	B
regression	I
loss	O
functions	O
are	O
compared	O
,	O
as	O
a	O
function	B
of	O
yi	O
(	O
β0	O
+	O
β1xi1	O
+	O
.	O
.	O
.	O
+	O
βpxip	O
)	O
.	O
when	O
yi	O
(	O
β0	O
+	O
β1xi1	O
+	O
.	O
.	O
.	O
+	O
βpxip	O
)	O
is	O
greater	O
than	O
1	O
,	O
then	O
the	O
svm	O
loss	O
is	O
zero	O
,	O
since	O
this	O
corresponds	O
to	O
an	O
observation	O
that	O
is	O
on	O
the	O
correct	O
side	O
of	O
the	O
margin	B
.	O
overall	O
,	O
the	O
two	O
loss	O
functions	O
have	O
quite	O
similar	O
behavior	O
.	O
when	O
the	O
support	B
vector	I
classiﬁer	O
and	O
svm	O
were	O
ﬁrst	O
introduced	O
,	O
it	O
was	O
thought	O
that	O
the	O
tuning	B
parameter	I
c	O
in	O
(	O
9.15	O
)	O
was	O
an	O
unimportant	O
“	O
nui-	O
sance	O
”	O
parameter	B
that	O
could	O
be	O
set	B
to	O
some	O
default	O
value	O
,	O
like	O
1.	O
however	O
,	O
the	O
“	O
loss	O
+	O
penalty	B
”	O
formulation	O
(	O
9.25	O
)	O
for	O
the	O
support	B
vector	I
classiﬁer	O
indicates	O
that	O
this	O
is	O
not	O
the	O
case	O
.	O
the	O
choice	O
of	O
tuning	B
parameter	I
is	O
very	O
important	O
and	O
determines	O
the	O
extent	O
to	O
which	O
the	O
model	B
underﬁts	O
or	O
over-	O
ﬁts	O
the	O
data	B
,	O
as	O
illustrated	O
,	O
for	O
example	O
,	O
in	O
figure	O
9.7.	O
we	O
have	O
established	O
that	O
the	O
support	B
vector	I
classiﬁer	O
is	O
closely	O
related	O
to	O
logistic	B
regression	I
and	O
other	O
preexisting	O
statistical	O
methods	O
.	O
is	O
the	O
svm	O
unique	O
in	O
its	O
use	O
of	O
kernels	O
to	O
enlarge	O
the	O
feature	B
space	O
to	O
accommodate	O
non-linear	B
class	O
boundaries	O
?	O
the	O
answer	O
to	O
this	O
question	O
is	O
“	O
no	O
”	O
.	O
we	O
could	O
just	O
as	O
well	O
perform	O
logistic	B
regression	I
or	O
many	O
of	O
the	O
other	O
classiﬁcation	B
methods	O
seen	O
in	O
this	O
book	O
using	O
non-linear	B
kernels	O
;	O
this	O
is	O
closely	O
related	O
to	O
some	O
of	O
the	O
non-linear	B
approaches	O
seen	O
in	O
chapter	O
7.	O
however	O
,	O
for	O
his-	O
torical	O
reasons	O
,	O
the	O
use	O
of	O
non-linear	B
kernels	O
is	O
much	O
more	O
widespread	O
in	O
the	O
context	O
of	O
svms	O
than	O
in	O
the	O
context	O
of	O
logistic	B
regression	I
or	O
other	O
methods	O
.	O
though	O
we	O
have	O
not	O
addressed	O
it	O
here	O
,	O
there	O
is	O
in	O
fact	O
an	O
extension	O
of	O
the	O
svm	O
for	O
regression	O
(	O
i.e	O
.	O
for	O
a	O
quantitative	B
rather	O
than	O
a	O
qualita-	O
tive	O
response	B
)	O
,	O
called	O
support	B
vector	I
regression	O
.	O
in	O
chapter	O
3	O
,	O
we	O
saw	O
that	O
least	B
squares	I
regression	O
seeks	O
coeﬃcients	O
β0	O
,	O
β1	O
,	O
.	O
.	O
.	O
,	O
βp	O
such	O
that	O
the	O
sum	O
of	O
squared	O
residuals	B
is	O
as	O
small	O
as	O
possible	O
.	O
(	O
recall	B
from	O
chapter	O
3	O
that	O
residuals	B
are	O
deﬁned	O
as	O
yi	O
−	O
β0	O
−	O
β1xi1	O
−	O
···	O
−	O
βpxip	O
.	O
)	O
support	B
vector	I
regression	O
instead	O
seeks	O
coeﬃcients	O
that	O
minimize	O
a	O
diﬀerent	O
type	O
of	O
loss	O
,	O
where	O
only	O
residuals	B
larger	O
in	O
absolute	O
value	O
than	O
some	O
positive	O
constant	O
support	B
vector	I
regression	O
9.6	O
lab	O
:	O
support	B
vector	I
machines	O
359	O
contribute	O
to	O
the	O
loss	B
function	I
.	O
this	O
is	O
an	O
extension	O
of	O
the	O
margin	B
used	O
in	O
support	B
vector	I
classiﬁers	O
to	O
the	O
regression	B
setting	O
.	O
9.6	O
lab	O
:	O
support	B
vector	I
machines	O
we	O
use	O
the	O
e1071	O
library	O
in	O
r	O
to	O
demonstrate	O
the	O
support	B
vector	I
classiﬁer	O
and	O
the	O
svm	O
.	O
another	O
option	O
is	O
the	O
liblinear	O
library	O
,	O
which	O
is	O
useful	O
for	O
very	O
large	O
linear	B
problems	O
.	O
svm	O
(	O
)	O
9.6.1	O
support	B
vector	I
classiﬁer	O
the	O
e1071	O
library	O
contains	O
implementations	O
for	O
a	O
number	O
of	O
statistical	O
learning	O
methods	O
.	O
in	O
particular	O
,	O
the	O
svm	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
ﬁt	B
a	O
support	B
vector	I
classiﬁer	O
when	O
the	O
argument	B
kernel=	O
''	O
linear	B
''	O
is	O
used	O
.	O
this	O
function	B
uses	O
a	O
slightly	O
diﬀerent	O
formulation	O
from	O
(	O
9.14	O
)	O
and	O
(	O
9.25	O
)	O
for	O
the	O
support	B
vector	I
classiﬁer	O
.	O
a	O
cost	O
argument	O
allows	O
us	O
to	O
specify	O
the	O
cost	O
of	O
a	O
violation	O
to	O
the	O
margin	B
.	O
when	O
the	O
cost	O
argument	O
is	O
small	O
,	O
then	O
the	O
mar-	O
gins	O
will	O
be	O
wide	O
and	O
many	O
support	O
vectors	O
will	O
be	O
on	O
the	O
margin	B
or	O
will	O
violate	O
the	O
margin	B
.	O
when	O
the	O
cost	O
argument	O
is	O
large	O
,	O
then	O
the	O
margins	O
will	O
be	O
narrow	O
and	O
there	O
will	O
be	O
few	O
support	O
vectors	O
on	O
the	O
margin	B
or	O
violating	O
the	O
margin	B
.	O
we	O
now	O
use	O
the	O
svm	O
(	O
)	O
function	B
to	O
ﬁt	B
the	O
support	B
vector	I
classiﬁer	O
for	O
a	O
given	O
value	O
of	O
the	O
cost	O
parameter	O
.	O
here	O
we	O
demonstrate	O
the	O
use	O
of	O
this	O
function	B
on	O
a	O
two-dimensional	O
example	O
so	O
that	O
we	O
can	O
plot	B
the	O
resulting	O
decision	B
boundary	I
.	O
we	O
begin	O
by	O
generating	O
the	O
observations	B
,	O
which	O
belong	O
to	O
two	O
classes	O
,	O
and	O
checking	O
whether	O
the	O
classes	O
are	O
linearly	O
separable	O
.	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
x	O
=	O
matrix	O
(	O
rnorm	O
(	O
20*2	O
)	O
,	O
ncol	O
=2	O
)	O
>	O
y	O
=	O
c	O
(	O
rep	O
(	O
-1	O
,10	O
)	O
,	O
rep	O
(	O
1	O
,10	O
)	O
)	O
>	O
x	O
[	O
y	O
==1	O
,	O
]	O
=	O
x	O
[	O
y	O
==1	O
,	O
]	O
+	O
1	O
>	O
plot	B
(	O
x	O
,	O
col	O
=	O
(	O
3	O
-	O
y	O
)	O
)	O
they	O
are	O
not	O
.	O
next	O
,	O
we	O
ﬁt	B
the	O
support	B
vector	I
classiﬁer	O
.	O
note	O
that	O
in	O
order	O
for	O
the	O
svm	O
(	O
)	O
function	B
to	O
perform	O
classiﬁcation	B
(	O
as	O
opposed	O
to	O
svm-based	O
regression	B
)	O
,	O
we	O
must	O
encode	O
the	O
response	B
as	O
a	O
factor	B
variable	O
.	O
we	O
now	O
create	O
a	O
data	B
frame	I
with	O
the	O
response	B
coded	O
as	O
a	O
factor	B
.	O
>	O
dat	O
=	O
data	B
.	O
frame	O
(	O
x	O
=x	O
,	O
y	O
=	O
as	O
.	O
factor	B
(	O
y	O
)	O
)	O
>	O
library	O
(	O
e1071	O
)	O
>	O
svmfit	O
=	O
svm	O
(	O
y∼	O
.	O
,	O
data	B
=	O
dat	O
,	O
kernel	B
=	O
''	O
linear	B
``	O
,	O
cost	O
=10	O
,	O
scale	O
=	O
false	O
)	O
360	O
9.	O
support	B
vector	I
machines	O
the	O
argument	B
scale=false	O
tells	O
the	O
svm	O
(	O
)	O
function	B
not	O
to	O
scale	O
each	O
feature	B
to	O
have	O
mean	O
zero	O
or	O
standard	O
deviation	O
one	O
;	O
depending	O
on	O
the	O
application	O
,	O
one	O
might	O
prefer	O
to	O
use	O
scale=true	O
.	O
we	O
can	O
now	O
plot	B
the	O
support	B
vector	I
classiﬁer	O
obtained	O
:	O
>	O
plot	B
(	O
svmfit	O
,	O
dat	O
)	O
note	O
that	O
the	O
two	O
arguments	O
to	O
the	O
plot.svm	O
(	O
)	O
function	B
are	O
the	O
output	B
of	O
the	O
call	O
to	O
svm	O
(	O
)	O
,	O
as	O
well	O
as	O
the	O
data	B
used	O
in	O
the	O
call	O
to	O
svm	O
(	O
)	O
.	O
the	O
region	O
of	O
feature	B
space	O
that	O
will	O
be	O
assigned	O
to	O
the	O
−1	O
class	O
is	O
shown	O
in	O
light	O
blue	O
,	O
and	O
the	O
region	O
that	O
will	O
be	O
assigned	O
to	O
the	O
+1	O
class	O
is	O
shown	O
in	O
purple	O
.	O
the	O
decision	B
boundary	I
between	O
the	O
two	O
classes	O
is	O
linear	B
(	O
because	O
we	O
used	O
the	O
argument	B
kernel=	O
''	O
linear	B
''	O
)	O
,	O
though	O
due	O
to	O
the	O
way	O
in	O
which	O
the	O
plotting	O
function	B
is	O
implemented	O
in	O
this	O
library	O
the	O
decision	B
boundary	I
looks	O
somewhat	O
jagged	O
in	O
the	O
plot	B
.	O
(	O
note	O
that	O
here	O
the	O
second	O
feature	B
is	O
plotted	O
on	O
the	O
x-axis	O
and	O
the	O
ﬁrst	O
feature	B
is	O
plotted	O
on	O
the	O
y-axis	O
,	O
in	O
contrast	B
to	O
the	O
behavior	O
of	O
the	O
usual	O
plot	B
(	O
)	O
function	B
in	O
r.	O
)	O
the	O
support	O
vectors	O
are	O
plotted	O
as	O
crosses	O
and	O
the	O
remaining	O
observations	B
are	O
plotted	O
as	O
circles	O
;	O
we	O
see	O
here	O
that	O
there	O
are	O
seven	O
support	O
vectors	O
.	O
we	O
can	O
determine	O
their	O
identities	O
as	O
follows	O
:	O
>	O
s	O
v	O
m	O
f	O
i	O
t	O
$	O
i	O
n	O
d	O
e	O
x	O
[	O
1	O
]	O
1	O
2	O
5	O
7	O
14	O
16	O
17	O
we	O
can	O
obtain	O
some	O
basic	O
information	O
about	O
the	O
support	B
vector	I
classiﬁer	O
ﬁt	B
using	O
the	O
summary	O
(	O
)	O
command	O
:	O
>	O
summary	O
(	O
svmfit	O
)	O
call	O
:	O
svm	O
(	O
formula	O
=	O
y	O
∼	O
.	O
,	O
data	B
=	O
dat	O
,	O
kernel	B
=	O
``	O
linear	B
``	O
,	O
cost	O
=	O
10	O
,	O
scale	O
=	O
false	O
)	O
parameter	B
s	O
:	O
svm	O
-	O
type	O
:	O
svm	O
-	O
kernel	B
:	O
cost	O
:	O
gamma	O
:	O
c	O
-	O
c	O
l	O
a	O
s	O
s	O
i	O
f	O
i	O
c	O
a	O
t	O
i	O
o	O
n	O
linear	B
10	O
0.5	O
number	O
of	O
support	O
vectors	O
:	O
7	O
(	O
4	O
3	O
)	O
number	O
of	O
classes	O
:	O
levels	O
:	O
2	O
-1	O
1	O
this	O
tells	O
us	O
,	O
for	O
instance	O
,	O
that	O
a	O
linear	B
kernel	I
was	O
used	O
with	O
cost=10	O
,	O
and	O
that	O
there	O
were	O
seven	O
support	O
vectors	O
,	O
four	O
in	O
one	O
class	O
and	O
three	O
in	O
the	O
other	O
.	O
what	O
if	O
we	O
instead	O
used	O
a	O
smaller	O
value	O
of	O
the	O
cost	O
parameter	O
?	O
>	O
svmfit	O
=	O
svm	O
(	O
y∼	O
.	O
,	O
data	B
=	O
dat	O
,	O
kernel	B
=	O
''	O
linear	B
``	O
,	O
cost	O
=0.1	O
,	O
scale	O
=	O
false	O
)	O
>	O
plot	B
(	O
svmfit	O
,	O
dat	O
)	O
>	O
s	O
v	O
m	O
f	O
i	O
t	O
$	O
i	O
n	O
d	O
e	O
x	O
[	O
1	O
]	O
1	O
2	O
3	O
4	O
5	O
7	O
9	O
10	O
12	O
13	O
14	O
15	O
16	O
17	O
18	O
20	O
9.6	O
lab	O
:	O
support	B
vector	I
machines	O
361	O
now	O
that	O
a	O
smaller	O
value	O
of	O
the	O
cost	O
parameter	O
is	O
being	O
used	O
,	O
we	O
obtain	O
a	O
larger	O
number	O
of	O
support	O
vectors	O
,	O
because	O
the	O
margin	B
is	O
now	O
wider	O
.	O
unfor-	O
tunately	O
,	O
the	O
svm	O
(	O
)	O
function	B
does	O
not	O
explicitly	O
output	B
the	O
coeﬃcients	O
of	O
the	O
linear	B
decision	O
boundary	O
obtained	O
when	O
the	O
support	B
vector	I
classiﬁer	O
is	O
ﬁt	B
,	O
nor	O
does	O
it	O
output	B
the	O
width	O
of	O
the	O
margin	B
.	O
the	O
e1071	O
library	O
includes	O
a	O
built-in	O
function	B
,	O
tune	O
(	O
)	O
,	O
to	O
perform	O
cross-	O
validation	O
.	O
by	O
default	O
,	O
tune	O
(	O
)	O
performs	O
ten-fold	O
cross-validation	B
on	O
a	O
set	B
of	O
models	O
of	O
interest	O
.	O
in	O
order	O
to	O
use	O
this	O
function	B
,	O
we	O
pass	O
in	O
relevant	O
information	O
about	O
the	O
set	B
of	O
models	O
that	O
are	O
under	O
consideration	O
.	O
the	O
following	O
command	O
indicates	O
that	O
we	O
want	O
to	O
compare	O
svms	O
with	O
a	O
linear	B
kernel	I
,	O
using	O
a	O
range	O
of	O
values	O
of	O
the	O
cost	O
parameter	O
.	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
tune	O
.	O
out	O
=	O
tune	O
(	O
svm	O
,	O
y∼	O
.	O
,	O
data	B
=	O
dat	O
,	O
kernel	B
=	O
''	O
linear	B
``	O
,	O
ranges	O
=	O
list	O
(	O
cost	O
=	O
c	O
(	O
0.001	O
,	O
0.01	O
,	O
0.1	O
,	O
1	O
,5	O
,10	O
,100	O
)	O
)	O
)	O
we	O
can	O
easily	O
access	O
the	O
cross-validation	B
errors	O
for	O
each	O
of	O
these	O
models	O
using	O
the	O
summary	O
(	O
)	O
command	O
:	O
tune	O
(	O
)	O
>	O
summary	O
(	O
tune	O
.	O
out	O
)	O
parameter	B
tuning	O
of	O
’	O
svm	O
’	O
:	O
-	O
sampling	O
method	O
:	O
10	O
-	O
fold	O
cross	O
validation	O
-	O
best	O
parameters	O
:	O
cost	O
0.1	O
-	O
best	O
performan	O
c	O
e	O
:	O
0.1	O
-	O
detailed	O
performa	O
nc	O
e	O
results	O
:	O
cost	O
error	O
dispersio	O
n	O
0.422	O
0.422	O
0.211	O
0.242	O
0.242	O
0.242	O
0.242	O
0.70	O
0.70	O
0.10	O
0.15	O
0.15	O
0.15	O
0.15	O
1	O
1	O
e	O
-03	O
2	O
1	O
e	O
-02	O
3	O
1	O
e	O
-01	O
4	O
1	O
e	O
+00	O
5	O
5	O
e	O
+00	O
6	O
1	O
e	O
+01	O
7	O
1	O
e	O
+02	O
we	O
see	O
that	O
cost=0.1	O
results	O
in	O
the	O
lowest	O
cross-validation	B
error	O
rate	B
.	O
the	O
tune	O
(	O
)	O
function	B
stores	O
the	O
best	O
model	O
obtained	O
,	O
which	O
can	O
be	O
accessed	O
as	O
follows	O
:	O
>	O
bestmod	O
=	O
tune	O
.	O
out	O
$	O
best	O
.	O
model	B
>	O
summary	O
(	O
bestmod	O
)	O
the	O
predict	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
predict	O
the	O
class	O
label	O
on	O
a	O
set	B
of	O
test	B
observations	O
,	O
at	O
any	O
given	O
value	O
of	O
the	O
cost	O
parameter	O
.	O
we	O
begin	O
by	O
generating	O
a	O
test	B
data	O
set	B
.	O
>	O
xtest	O
=	O
matrix	O
(	O
rnorm	O
(	O
20*2	O
)	O
,	O
ncol	O
=2	O
)	O
>	O
ytest	O
=	O
sample	O
(	O
c	O
(	O
-1	O
,1	O
)	O
,	O
20	O
,	O
rep	O
=	O
true	O
)	O
>	O
xtest	O
[	O
ytest	O
==1	O
,	O
]	O
=	O
xtest	O
[	O
ytest	O
==1	O
,	O
]	O
+	O
1	O
>	O
testdat	O
=	O
data	B
.	O
frame	O
(	O
x	O
=	O
xtest	O
,	O
y	O
=	O
as	O
.	O
factor	B
(	O
ytest	O
)	O
)	O
now	O
we	O
predict	O
the	O
class	O
labels	O
of	O
these	O
test	B
observations	O
.	O
here	O
we	O
use	O
the	O
best	O
model	O
obtained	O
through	O
cross-validation	B
in	O
order	O
to	O
make	O
predictions	O
.	O
362	O
9.	O
support	B
vector	I
machines	O
>	O
ypred	O
=	O
predict	O
(	O
bestmod	O
,	O
testdat	O
)	O
>	O
table	O
(	O
predict	O
=	O
ypred	O
,	O
truth	O
=	O
testdat	O
$	O
y	O
)	O
truth	O
predict	O
-1	O
-1	O
11	O
0	O
1	O
1	O
1	O
8	O
thus	O
,	O
with	O
this	O
value	O
of	O
cost	O
,	O
19	O
of	O
the	O
test	B
observations	O
are	O
correctly	O
classiﬁed	O
.	O
what	O
if	O
we	O
had	O
instead	O
used	O
cost=0.01	O
?	O
>	O
svmfit	O
=	O
svm	O
(	O
y∼	O
.	O
,	O
data	B
=	O
dat	O
,	O
kernel	B
=	O
''	O
linear	B
``	O
,	O
cost	O
=.01	O
,	O
scale	O
=	O
false	O
)	O
>	O
ypred	O
=	O
predict	O
(	O
svmfit	O
,	O
testdat	O
)	O
>	O
table	O
(	O
predict	O
=	O
ypred	O
,	O
truth	O
=	O
testdat	O
$	O
y	O
)	O
truth	O
predict	O
-1	O
-1	O
11	O
1	O
0	O
1	O
2	O
7	O
in	O
this	O
case	O
one	O
additional	O
observation	O
is	O
misclassiﬁed	O
.	O
now	O
consider	O
a	O
situation	O
in	O
which	O
the	O
two	O
classes	O
are	O
linearly	O
separable	O
.	O
then	O
we	O
can	O
ﬁnd	O
a	O
separating	B
hyperplane	I
using	O
the	O
svm	O
(	O
)	O
function	B
.	O
we	O
ﬁrst	O
further	O
separate	O
the	O
two	O
classes	O
in	O
our	O
simulated	O
data	B
so	O
that	O
they	O
are	O
linearly	O
separable	O
:	O
>	O
x	O
[	O
y	O
==1	O
,	O
]	O
=	O
x	O
[	O
y	O
==1	O
,	O
]	O
+0.5	O
>	O
plot	B
(	O
x	O
,	O
col	O
=	O
(	O
y	O
+5	O
)	O
/2	O
,	O
pch	O
=19	O
)	O
now	O
the	O
observations	B
are	O
just	O
barely	O
linearly	O
separable	O
.	O
we	O
ﬁt	B
the	O
support	B
vector	I
classiﬁer	O
and	O
plot	B
the	O
resulting	O
hyperplane	B
,	O
using	O
a	O
very	O
large	O
value	O
of	O
cost	O
so	O
that	O
no	O
observations	B
are	O
misclassiﬁed	O
.	O
>	O
dat	O
=	O
data	B
.	O
frame	O
(	O
x	O
=x	O
,	O
y	O
=	O
as	O
.	O
factor	B
(	O
y	O
)	O
)	O
>	O
svmfit	O
=	O
svm	O
(	O
y∼	O
.	O
,	O
data	B
=	O
dat	O
,	O
kernel	B
=	O
''	O
linear	B
``	O
,	O
cost	O
=1	O
e5	O
)	O
>	O
summary	O
(	O
svmfit	O
)	O
call	O
:	O
svm	O
(	O
formula	O
=	O
y	O
∼	O
.	O
,	O
data	B
=	O
dat	O
,	O
kernel	B
=	O
``	O
linear	B
``	O
,	O
cost	O
=	O
1	O
e	O
+05	O
)	O
parameter	B
s	O
:	O
svm	O
-	O
type	O
:	O
svm	O
-	O
kernel	B
:	O
cost	O
:	O
gamma	O
:	O
c	O
-	O
c	O
l	O
a	O
s	O
s	O
i	O
f	O
i	O
c	O
a	O
t	O
i	O
o	O
n	O
linear	B
1	O
e	O
+05	O
0.5	O
number	O
of	O
support	O
vectors	O
:	O
3	O
(	O
1	O
2	O
)	O
number	O
of	O
classes	O
:	O
levels	O
:	O
2	O
-1	O
1	O
>	O
plot	B
(	O
svmfit	O
,	O
dat	O
)	O
no	O
training	B
errors	O
were	O
made	O
and	O
only	O
three	O
support	O
vectors	O
were	O
used	O
.	O
however	O
,	O
we	O
can	O
see	O
from	O
the	O
ﬁgure	O
that	O
the	O
margin	B
is	O
very	O
narrow	O
(	O
because	O
the	O
observations	B
that	O
are	O
not	O
support	O
vectors	O
,	O
indicated	O
as	O
circles	O
,	O
are	O
very	O
9.6	O
lab	O
:	O
support	B
vector	I
machines	O
363	O
close	O
to	O
the	O
decision	B
boundary	I
)	O
.	O
it	O
seems	O
likely	O
that	O
this	O
model	B
will	O
perform	O
poorly	O
on	O
test	B
data	O
.	O
we	O
now	O
try	O
a	O
smaller	O
value	O
of	O
cost	O
:	O
>	O
svmfit	O
=	O
svm	O
(	O
y∼	O
.	O
,	O
data	B
=	O
dat	O
,	O
kernel	B
=	O
''	O
linear	B
``	O
,	O
cost	O
=1	O
)	O
>	O
summary	O
(	O
svmfit	O
)	O
>	O
plot	B
(	O
svmfit	O
,	O
dat	O
)	O
using	O
cost=1	O
,	O
we	O
misclassify	O
a	O
training	B
observation	O
,	O
but	O
we	O
also	O
obtain	O
a	O
much	O
wider	O
margin	B
and	O
make	O
use	O
of	O
seven	O
support	O
vectors	O
.	O
it	O
seems	O
likely	O
that	O
this	O
model	B
will	O
perform	O
better	O
on	O
test	B
data	O
than	O
the	O
model	B
with	O
cost=1e5	O
.	O
9.6.2	O
support	B
vector	I
machine	O
in	O
order	O
to	O
ﬁt	B
an	O
svm	O
using	O
a	O
non-linear	B
kernel	O
,	O
we	O
once	O
again	O
use	O
the	O
svm	O
(	O
)	O
function	B
.	O
however	O
,	O
now	O
we	O
use	O
a	O
diﬀerent	O
value	O
of	O
the	O
parameter	B
kernel	O
.	O
to	O
ﬁt	B
an	O
svm	O
with	O
a	O
polynomial	B
kernel	O
we	O
use	O
kernel=	O
''	O
polynomial	B
''	O
,	O
and	O
to	O
ﬁt	B
an	O
svm	O
with	O
a	O
radial	B
kernel	I
we	O
use	O
kernel=	O
''	O
radial	B
''	O
.	O
in	O
the	O
former	O
case	O
we	O
also	O
use	O
the	O
degree	O
argument	B
to	O
specify	O
a	O
degree	O
for	O
the	O
polynomial	B
kernel	O
(	O
this	O
is	O
d	O
in	O
(	O
9.22	O
)	O
)	O
,	O
and	O
in	O
the	O
latter	O
case	O
we	O
use	O
gamma	O
to	O
specify	O
a	O
value	O
of	O
γ	O
for	O
the	O
radial	B
basis	O
kernel	B
(	O
9.24	O
)	O
.	O
we	O
ﬁrst	O
generate	O
some	O
data	B
with	O
a	O
non-linear	B
class	O
boundary	O
,	O
as	O
follows	O
:	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
x	O
=	O
matrix	O
(	O
rnorm	O
(	O
200*2	O
)	O
,	O
ncol	O
=2	O
)	O
>	O
x	O
[	O
1:100	O
,	O
]	O
=	O
x	O
[	O
1:100	O
,	O
]	O
+2	O
>	O
x	O
[	O
101:150	O
,	O
]	O
=	O
x	O
[	O
101:150	O
,	O
]	O
-2	O
>	O
y	O
=	O
c	O
(	O
rep	O
(	O
1	O
,150	O
)	O
,	O
rep	O
(	O
2	O
,50	O
)	O
)	O
>	O
dat	O
=	O
data	B
.	O
frame	O
(	O
x	O
=x	O
,	O
y	O
=	O
as	O
.	O
factor	B
(	O
y	O
)	O
)	O
plotting	O
the	O
data	B
makes	O
it	O
clear	O
that	O
the	O
class	O
boundary	O
is	O
indeed	O
non-	O
linear	B
:	O
>	O
plot	B
(	O
x	O
,	O
col	O
=	O
y	O
)	O
the	O
data	B
is	O
randomly	O
split	O
into	O
training	B
and	O
testing	O
groups	O
.	O
we	O
then	O
ﬁt	B
the	O
training	B
data	O
using	O
the	O
svm	O
(	O
)	O
function	B
with	O
a	O
radial	B
kernel	I
and	O
γ	O
=	O
1	O
:	O
>	O
train	B
=	O
sample	O
(	O
200	O
,100	O
)	O
>	O
svmfit	O
=	O
svm	O
(	O
y∼	O
.	O
,	O
data	B
=	O
dat	O
[	O
train	B
,	O
]	O
,	O
kernel	B
=	O
''	O
radial	B
``	O
,	O
gamma	O
=1	O
,	O
cost	O
=1	O
)	O
>	O
plot	B
(	O
svmfit	O
,	O
dat	O
[	O
train	B
,	O
]	O
)	O
the	O
plot	B
shows	O
that	O
the	O
resulting	O
svm	O
has	O
a	O
decidedly	O
non-linear	B
boundary	O
.	O
the	O
summary	O
(	O
)	O
function	B
can	O
be	O
used	O
to	O
obtain	O
some	O
information	O
about	O
the	O
svm	O
ﬁt	B
:	O
>	O
summary	O
(	O
svmfit	O
)	O
call	O
:	O
svm	O
(	O
formula	O
=	O
y	O
∼	O
.	O
,	O
data	B
=	O
dat	O
,	O
kernel	B
=	O
``	O
radial	B
``	O
,	O
gamma	O
=	O
1	O
,	O
cost	O
=	O
1	O
)	O
parameter	B
s	O
:	O
svm	O
-	O
type	O
:	O
c	O
-	O
c	O
l	O
a	O
s	O
s	O
i	O
f	O
i	O
c	O
a	O
t	O
i	O
o	O
n	O
364	O
9.	O
support	B
vector	I
machines	O
svm	O
-	O
kernel	B
:	O
cost	O
:	O
gamma	O
:	O
radial	B
1	O
1	O
number	O
of	O
support	O
vectors	O
:	O
37	O
(	O
17	O
20	O
)	O
number	O
of	O
classes	O
:	O
levels	O
:	O
2	O
1	O
2	O
we	O
can	O
see	O
from	O
the	O
ﬁgure	O
that	O
there	O
are	O
a	O
fair	O
number	O
of	O
training	B
errors	O
in	O
this	O
svm	O
ﬁt	B
.	O
if	O
we	O
increase	O
the	O
value	O
of	O
cost	O
,	O
we	O
can	O
reduce	O
the	O
number	O
of	O
training	B
errors	O
.	O
however	O
,	O
this	O
comes	O
at	O
the	O
price	O
of	O
a	O
more	O
irregular	O
decision	B
boundary	I
that	O
seems	O
to	O
be	O
at	O
risk	O
of	O
overﬁtting	B
the	O
data	B
.	O
>	O
svmfit	O
=	O
svm	O
(	O
y∼	O
.	O
,	O
data	B
=	O
dat	O
[	O
train	B
,	O
]	O
,	O
kernel	B
=	O
''	O
radial	B
``	O
,	O
gamma	O
=1	O
,	O
cost	O
=1	O
e5	O
)	O
>	O
plot	B
(	O
svmfit	O
,	O
dat	O
[	O
train	B
,	O
]	O
)	O
we	O
can	O
perform	O
cross-validation	B
using	O
tune	O
(	O
)	O
to	O
select	O
the	O
best	O
choice	O
of	O
γ	O
and	O
cost	O
for	O
an	O
svm	O
with	O
a	O
radial	B
kernel	I
:	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
tune	O
.	O
out	O
=	O
tune	O
(	O
svm	O
,	O
y∼	O
.	O
,	O
data	B
=	O
dat	O
[	O
train	B
,	O
]	O
,	O
kernel	B
=	O
''	O
radial	B
``	O
,	O
ranges	O
=	O
list	O
(	O
cost	O
=	O
c	O
(	O
0.1	O
,1	O
,10	O
,100	O
,1000	O
)	O
,	O
gamma	O
=	O
c	O
(	O
0.5	O
,1	O
,2	O
,3	O
,4	O
)	O
)	O
)	O
>	O
summary	O
(	O
tune	O
.	O
out	O
)	O
parameter	B
tuning	O
of	O
’	O
svm	O
’	O
:	O
-	O
sampling	O
method	O
:	O
10	O
-	O
fold	O
cross	O
validation	O
-	O
best	O
parameters	O
:	O
cost	O
gamma	O
2	O
1	O
-	O
best	O
performan	O
c	O
e	O
:	O
0.12	O
-	O
detailed	O
performa	O
nc	O
e	O
results	O
:	O
cost	O
gamma	O
error	B
dispersion	O
0.1160	O
0.0823	O
0.0707	O
0.0823	O
0.0994	O
0.1354	O
0.0823	O
1e	O
-01	O
1	O
e	O
+00	O
1	O
e	O
+01	O
1	O
e	O
+02	O
1	O
e	O
+03	O
1e	O
-01	O
1	O
e	O
+00	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
.	O
.	O
.	O
0.5	O
0.5	O
0.5	O
0.5	O
0.5	O
1.0	O
1.0	O
0.27	O
0.13	O
0.15	O
0.17	O
0.21	O
0.25	O
0.13	O
therefore	O
,	O
the	O
best	O
choice	O
of	O
parameters	O
involves	O
cost=1	O
and	O
gamma=2	O
.	O
we	O
can	O
view	O
the	O
test	B
set	O
predictions	O
for	O
this	O
model	B
by	O
applying	O
the	O
predict	O
(	O
)	O
function	B
to	O
the	O
data	B
.	O
notice	O
that	O
to	O
do	O
this	O
we	O
subset	O
the	O
dataframe	O
dat	O
using	O
-train	O
as	O
an	O
index	O
set	B
.	O
>	O
table	O
(	O
true	O
=	O
dat	O
[	O
-	O
train	B
,	O
''	O
y	O
``	O
]	O
,	O
pred	O
=	O
predict	O
(	O
tune	O
.	O
out	O
$	O
best	O
.	O
model	B
,	O
newdata	O
=	O
dat	O
[	O
-	O
train	B
,	O
]	O
)	O
)	O
10	O
%	O
of	O
test	B
observations	O
are	O
misclassiﬁed	O
by	O
this	O
svm	O
.	O
9.6	O
lab	O
:	O
support	B
vector	I
machines	O
365	O
9.6.3	O
roc	O
curves	O
the	O
rocr	O
package	O
can	O
be	O
used	O
to	O
produce	O
roc	O
curves	O
such	O
as	O
those	O
in	O
figures	O
9.10	O
and	O
9.11.	O
we	O
ﬁrst	O
write	O
a	O
short	O
function	B
to	O
plot	B
an	O
roc	O
curve	O
given	O
a	O
vector	B
containing	O
a	O
numerical	O
score	O
for	O
each	O
observation	O
,	O
pred	O
,	O
and	O
a	O
vector	B
containing	O
the	O
class	O
label	O
for	O
each	O
observation	O
,	O
truth	O
.	O
>	O
library	O
(	O
rocr	O
)	O
>	O
rocplot	O
=	O
function	B
(	O
pred	O
,	O
truth	O
,	O
...	O
)	O
{	O
+	O
+	O
+	O
predob	O
=	O
prediction	B
(	O
pred	O
,	O
truth	O
)	O
perf	O
=	O
performa	O
nc	O
e	O
(	O
predob	O
,	O
``	O
tpr	O
``	O
,	O
``	O
fpr	O
``	O
)	O
plot	B
(	O
perf	O
,	O
...	O
)	O
}	O
svms	O
and	O
support	B
vector	I
classiﬁers	O
output	B
class	O
labels	O
for	O
each	O
observa-	O
tion	O
.	O
however	O
,	O
it	O
is	O
also	O
possible	O
to	O
obtain	O
ﬁtted	O
values	O
for	O
each	O
observation	O
,	O
which	O
are	O
the	O
numerical	O
scores	O
used	O
to	O
obtain	O
the	O
class	O
labels	O
.	O
for	O
instance	O
,	O
in	O
the	O
case	O
of	O
a	O
support	B
vector	I
classiﬁer	O
,	O
the	O
ﬁtted	B
value	I
for	O
an	O
observation	O
x	O
=	O
(	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
)	O
t	O
takes	O
the	O
form	O
ˆβ0	O
+	O
ˆβ1x1	O
+	O
ˆβ2x2	O
+	O
.	O
.	O
.	O
+	O
ˆβpxp	O
.	O
for	O
an	O
svm	O
with	O
a	O
non-linear	B
kernel	O
,	O
the	O
equation	O
that	O
yields	O
the	O
ﬁtted	B
value	I
is	O
given	O
in	O
(	O
9.23	O
)	O
.	O
in	O
essence	O
,	O
the	O
sign	O
of	O
the	O
ﬁtted	B
value	I
determines	O
on	O
which	O
side	O
of	O
the	O
decision	B
boundary	I
the	O
observation	O
lies	O
.	O
therefore	O
,	O
the	O
relationship	O
between	O
the	O
ﬁtted	B
value	I
and	O
the	O
class	O
prediction	B
for	O
a	O
given	O
observation	O
is	O
simple	B
:	O
if	O
the	O
ﬁtted	B
value	I
exceeds	O
zero	O
then	O
the	O
observation	O
is	O
assigned	O
to	O
one	O
class	O
,	O
and	O
if	O
it	O
is	O
less	O
than	O
zero	O
then	O
it	O
is	O
assigned	O
to	O
the	O
other	O
.	O
in	O
order	O
to	O
obtain	O
the	O
ﬁtted	O
values	O
for	O
a	O
given	O
svm	O
model	B
ﬁt	O
,	O
we	O
use	O
decision.values=true	O
when	O
ﬁtting	O
svm	O
(	O
)	O
.	O
then	O
the	O
predict	O
(	O
)	O
function	B
will	O
output	B
the	O
ﬁtted	O
values	O
.	O
>	O
svmfit	O
.	O
opt	O
=	O
svm	O
(	O
y∼	O
.	O
,	O
data	B
=	O
dat	O
[	O
train	B
,	O
]	O
,	O
kernel	B
=	O
''	O
radial	B
``	O
,	O
gamma	O
=2	O
,	O
cost	O
=1	O
,	O
decision	O
.	O
values	O
=	O
t	O
)	O
>	O
fitted	O
=	O
attributes	O
(	O
predict	O
(	O
svmfit	O
.	O
opt	O
,	O
dat	O
[	O
train	B
,	O
]	O
,	O
decision	O
.	O
values	O
=	O
true	O
)	O
)	O
$	O
decision	O
.	O
values	O
now	O
we	O
can	O
produce	O
the	O
roc	O
plot	B
.	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
1	O
,2	O
)	O
)	O
>	O
rocplot	O
(	O
fitted	O
,	O
dat	O
[	O
train	B
,	O
''	O
y	O
``	O
]	O
,	O
main	O
=	O
''	O
training	B
data	O
``	O
)	O
svm	O
appears	O
to	O
be	O
producing	O
accurate	O
predictions	O
.	O
by	O
increasing	O
γ	O
we	O
can	O
produce	O
a	O
more	O
ﬂexible	B
ﬁt	O
and	O
generate	O
further	O
improvements	O
in	O
accuracy	O
.	O
>	O
svmfit	O
.	O
flex	O
=	O
svm	O
(	O
y∼	O
.	O
,	O
data	B
=	O
dat	O
[	O
train	B
,	O
]	O
,	O
kernel	B
=	O
''	O
radial	B
``	O
,	O
gamma	O
=50	O
,	O
cost	O
=1	O
,	O
decision	O
.	O
values	O
=	O
t	O
)	O
>	O
fitted	O
=	O
attributes	O
(	O
predict	O
(	O
svmfit	O
.	O
flex	O
,	O
dat	O
[	O
train	B
,	O
]	O
,	O
decision	O
.	O
values	O
=	O
t	O
)	O
)	O
$	O
decision	O
.	O
values	O
>	O
rocplot	O
(	O
fitted	O
,	O
dat	O
[	O
train	B
,	O
''	O
y	O
``	O
]	O
,	O
add	O
=t	O
,	O
col	O
=	O
''	O
red	O
``	O
)	O
however	O
,	O
these	O
roc	O
curves	O
are	O
all	O
on	O
the	O
training	B
data	O
.	O
we	O
are	O
really	O
more	O
interested	O
in	O
the	O
level	B
of	O
prediction	B
accuracy	O
on	O
the	O
test	B
data	O
.	O
when	O
we	O
compute	O
the	O
roc	O
curves	O
on	O
the	O
test	B
data	O
,	O
the	O
model	B
with	O
γ	O
=	O
2	O
appears	O
to	O
provide	O
the	O
most	O
accurate	O
results	O
.	O
366	O
9.	O
support	B
vector	I
machines	O
>	O
fitted	O
=	O
attributes	O
(	O
predict	O
(	O
svmfit	O
.	O
opt	O
,	O
dat	O
[	O
-	O
train	B
,	O
]	O
,	O
decision	O
.	O
values	O
=	O
t	O
)	O
)	O
$	O
decision	O
.	O
values	O
>	O
rocplot	O
(	O
fitted	O
,	O
dat	O
[	O
-	O
train	B
,	O
''	O
y	O
``	O
]	O
,	O
main	O
=	O
''	O
test	B
data	O
``	O
)	O
>	O
fitted	O
=	O
attributes	O
(	O
predict	O
(	O
svmfit	O
.	O
flex	O
,	O
dat	O
[	O
-	O
train	B
,	O
]	O
,	O
decision	O
.	O
values	O
=	O
t	O
)	O
)	O
$	O
decision	O
.	O
values	O
>	O
rocplot	O
(	O
fitted	O
,	O
dat	O
[	O
-	O
train	B
,	O
''	O
y	O
``	O
]	O
,	O
add	O
=t	O
,	O
col	O
=	O
''	O
red	O
``	O
)	O
9.6.4	O
svm	O
with	O
multiple	B
classes	O
if	O
the	O
response	B
is	O
a	O
factor	B
containing	O
more	O
than	O
two	O
levels	O
,	O
then	O
the	O
svm	O
(	O
)	O
function	B
will	O
perform	O
multi-class	O
classiﬁcation	B
using	O
the	O
one-versus-one	B
ap-	O
proach	O
.	O
we	O
explore	O
that	O
setting	O
here	O
by	O
generating	O
a	O
third	O
class	O
of	O
obser-	O
vations	O
.	O
>	O
set	B
.	O
seed	B
(	O
1	O
)	O
>	O
x	O
=	O
rbind	O
(	O
x	O
,	O
matrix	O
(	O
rnorm	O
(	O
50*2	O
)	O
,	O
ncol	O
=2	O
)	O
)	O
>	O
y	O
=	O
c	O
(	O
y	O
,	O
rep	O
(	O
0	O
,50	O
)	O
)	O
>	O
x	O
[	O
y	O
==0	O
,2	O
]	O
=	O
x	O
[	O
y	O
==0	O
,2	O
]	O
+2	O
>	O
dat	O
=	O
data	B
.	O
frame	O
(	O
x	O
=x	O
,	O
y	O
=	O
as	O
.	O
factor	B
(	O
y	O
)	O
)	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
1	O
,1	O
)	O
)	O
>	O
plot	B
(	O
x	O
,	O
col	O
=	O
(	O
y	O
+1	O
)	O
)	O
we	O
now	O
ﬁt	B
an	O
svm	O
to	O
the	O
data	B
:	O
>	O
svmfit	O
=	O
svm	O
(	O
y∼	O
.	O
,	O
data	B
=	O
dat	O
,	O
kernel	B
=	O
''	O
radial	B
``	O
,	O
cost	O
=10	O
,	O
gamma	O
=1	O
)	O
>	O
plot	B
(	O
svmfit	O
,	O
dat	O
)	O
the	O
e1071	O
library	O
can	O
also	O
be	O
used	O
to	O
perform	O
support	B
vector	I
regression	O
,	O
if	O
the	O
response	B
vector	O
that	O
is	O
passed	O
in	O
to	O
svm	O
(	O
)	O
is	O
numerical	O
rather	O
than	O
a	O
factor	B
.	O
9.6.5	O
application	O
to	O
gene	O
expression	O
data	B
we	O
now	O
examine	O
the	O
khan	O
data	B
set	O
,	O
which	O
consists	O
of	O
a	O
number	O
of	O
tissue	O
samples	O
corresponding	O
to	O
four	O
distinct	O
types	O
of	O
small	O
round	O
blue	O
cell	O
tu-	O
mors	O
.	O
for	O
each	O
tissue	O
sample	O
,	O
gene	O
expression	O
measurements	O
are	O
available	O
.	O
the	O
data	B
set	O
consists	O
of	O
training	B
data	O
,	O
xtrain	O
and	O
ytrain	O
,	O
and	O
testing	O
data	B
,	O
xtest	O
and	O
ytest	O
.	O
we	O
examine	O
the	O
dimension	O
of	O
the	O
data	B
:	O
''	O
xtest	O
``	O
''	O
ytrain	O
``	O
''	O
ytest	O
``	O
63	O
2308	O
''	O
xtrain	O
``	O
>	O
library	O
(	O
islr	O
)	O
>	O
names	O
(	O
khan	O
)	O
[	O
1	O
]	O
>	O
dim	O
(	O
khan	O
$	O
xtra	O
in	O
)	O
[	O
1	O
]	O
>	O
dim	O
(	O
khan	O
$	O
xtest	O
)	O
[	O
1	O
]	O
>	O
length	O
(	O
khan	O
$	O
ytra	O
in	O
)	O
[	O
1	O
]	O
63	O
>	O
length	O
(	O
khan	O
$	O
ytest	O
)	O
[	O
1	O
]	O
20	O
20	O
2308	O
9.6	O
lab	O
:	O
support	B
vector	I
machines	O
367	O
this	O
data	B
set	O
consists	O
of	O
expression	O
measurements	O
for	O
2,308	O
genes	O
.	O
the	O
training	B
and	O
test	B
sets	O
consist	O
of	O
63	O
and	O
20	O
observations	B
respectively	O
.	O
>	O
table	O
(	O
khan	O
$	O
ytra	O
i	O
n	O
)	O
2	O
1	O
4	O
8	O
23	O
12	O
20	O
3	O
>	O
table	O
(	O
khan	O
$	O
ytes	O
t	O
)	O
1	O
2	O
3	O
4	O
3	O
6	O
6	O
5	O
we	O
will	O
use	O
a	O
support	B
vector	I
approach	O
to	O
predict	O
cancer	O
subtype	O
using	O
gene	O
expression	O
measurements	O
.	O
in	O
this	O
data	B
set	O
,	O
there	O
are	O
a	O
very	O
large	O
number	O
of	O
features	O
relative	O
to	O
the	O
number	O
of	O
observations	B
.	O
this	O
suggests	O
that	O
we	O
should	O
use	O
a	O
linear	B
kernel	I
,	O
because	O
the	O
additional	O
ﬂexibility	O
that	O
will	O
result	O
from	O
using	O
a	O
polynomial	B
or	O
radial	B
kernel	I
is	O
unnecessary	O
.	O
>	O
dat	O
=	O
data	B
.	O
frame	O
(	O
x	O
=	O
khan	O
$	O
xtrain	O
,	O
y	O
=	O
as	O
.	O
factor	B
(	O
khan	O
$	O
ytra	O
in	O
)	O
)	O
>	O
out	O
=	O
svm	O
(	O
y∼	O
.	O
,	O
data	B
=	O
dat	O
,	O
kernel	B
=	O
''	O
linear	B
``	O
,	O
cost	O
=10	O
)	O
>	O
summary	O
(	O
out	O
)	O
call	O
:	O
svm	O
(	O
formula	O
=	O
y	O
∼	O
.	O
,	O
data	B
=	O
dat	O
,	O
kernel	B
=	O
``	O
linear	B
``	O
,	O
cost	O
=	O
10	O
)	O
parameter	B
s	O
:	O
svm	O
-	O
type	O
:	O
svm	O
-	O
kernel	B
:	O
cost	O
:	O
gamma	O
:	O
c	O
-	O
c	O
l	O
a	O
s	O
s	O
i	O
f	O
i	O
c	O
a	O
t	O
i	O
o	O
n	O
linear	B
10	O
0.000433	O
number	O
of	O
support	O
vectors	O
:	O
58	O
(	O
20	O
20	O
11	O
7	O
)	O
number	O
of	O
classes	O
:	O
levels	O
:	O
4	O
1	O
2	O
3	O
4	O
>	O
table	O
(	O
out	O
$	O
fitted	O
,	O
dat	O
$	O
y	O
)	O
2	O
1	O
0	O
8	O
0	O
23	O
0	O
0	O
3	O
0	O
0	O
0	O
12	O
0	O
4	O
0	O
0	O
0	O
0	O
20	O
1	O
2	O
3	O
4	O
we	O
see	O
that	O
there	O
are	O
no	O
training	B
errors	O
.	O
in	O
fact	O
,	O
this	O
is	O
not	O
surprising	O
,	O
because	O
the	O
large	O
number	O
of	O
variables	O
relative	O
to	O
the	O
number	O
of	O
observations	B
implies	O
that	O
it	O
is	O
easy	O
to	O
ﬁnd	O
hyperplanes	O
that	O
fully	O
separate	O
the	O
classes	O
.	O
we	O
are	O
most	O
interested	O
not	O
in	O
the	O
support	B
vector	I
classiﬁer	O
’	O
s	O
performance	O
on	O
the	O
training	B
observations	O
,	O
but	O
rather	O
its	O
performance	O
on	O
the	O
test	B
observations	O
.	O
>	O
dat	O
.	O
te	O
=	O
data	B
.	O
frame	O
(	O
x	O
=	O
khan	O
$	O
xtest	O
,	O
y	O
=	O
as	O
.	O
factor	B
(	O
khan	O
$	O
ytes	O
t	O
)	O
)	O
>	O
pred	O
.	O
te	O
=	O
predict	O
(	O
out	O
,	O
newdata	O
=	O
dat	O
.	O
te	O
)	O
>	O
table	O
(	O
pred	O
.	O
te	O
,	O
dat	O
.	O
te	O
$	O
y	O
)	O
pred	O
.	O
te	O
1	O
2	O
3	O
4	O
1	O
3	O
0	O
0	O
0	O
2	O
0	O
6	O
2	O
0	O
3	O
0	O
0	O
4	O
0	O
4	O
0	O
0	O
0	O
5	O
368	O
9.	O
support	B
vector	I
machines	O
we	O
see	O
that	O
using	O
cost=10	O
yields	O
two	O
test	B
set	O
errors	O
on	O
this	O
data	B
.	O
9.7	O
exercises	O
conceptual	O
1.	O
this	O
problem	O
involves	O
hyperplanes	O
in	O
two	O
dimensions	O
.	O
(	O
a	O
)	O
sketch	O
the	O
hyperplane	B
1	O
+	O
3x1	O
−	O
x2	B
=	O
0.	O
indicate	O
the	O
set	B
of	O
points	O
for	O
which	O
1	O
+	O
3x1	O
−	O
x2	B
>	O
0	O
,	O
as	O
well	O
as	O
the	O
set	B
of	O
points	O
for	O
which	O
1	O
+	O
3x1	O
−	O
x2	B
<	O
0	O
.	O
(	O
b	O
)	O
on	O
the	O
same	O
plot	B
,	O
sketch	O
the	O
hyperplane	B
−2	O
+	O
x1	O
+	O
2x2	O
=	O
0.	O
indicate	O
the	O
set	B
of	O
points	O
for	O
which	O
−2	O
+	O
x1	O
+	O
2x2	O
>	O
0	O
,	O
as	O
well	O
as	O
the	O
set	B
of	O
points	O
for	O
which	O
−2	O
+	O
x1	O
+	O
2x2	O
<	O
0	O
.	O
2.	O
we	O
have	O
seen	O
that	O
in	O
p	O
=	O
2	O
dimensions	O
,	O
a	O
linear	B
decision	O
boundary	O
takes	O
the	O
form	O
β0	O
+β1x1	O
+β2x2	O
=	O
0.	O
we	O
now	O
investigate	O
a	O
non-linear	B
decision	O
boundary	O
.	O
(	O
a	O
)	O
sketch	O
the	O
curve	O
(	O
1	O
+	O
x1	O
)	O
2	O
+	O
(	O
2	O
−	O
x2	B
)	O
2	O
=	O
4	O
.	O
(	O
b	O
)	O
on	O
your	O
sketch	O
,	O
indicate	O
the	O
set	B
of	O
points	O
for	O
which	O
(	O
1	O
+	O
x1	O
)	O
2	O
+	O
(	O
2	O
−	O
x2	B
)	O
2	O
>	O
4	O
,	O
as	O
well	O
as	O
the	O
set	B
of	O
points	O
for	O
which	O
(	O
1	O
+	O
x1	O
)	O
2	O
+	O
(	O
2	O
−	O
x2	B
)	O
2	O
≤	O
4	O
.	O
(	O
c	O
)	O
suppose	O
that	O
a	O
classiﬁer	B
assigns	O
an	O
observation	O
to	O
the	O
blue	O
class	O
if	O
(	O
1	O
+	O
x1	O
)	O
2	O
+	O
(	O
2	O
−	O
x2	B
)	O
2	O
>	O
4	O
,	O
and	O
to	O
the	O
red	O
class	O
otherwise	O
.	O
to	O
what	O
class	O
is	O
the	O
observation	O
(	O
0	O
,	O
0	O
)	O
classiﬁed	O
?	O
(	O
−1	O
,	O
1	O
)	O
?	O
(	O
2	O
,	O
2	O
)	O
?	O
(	O
3	O
,	O
8	O
)	O
?	O
(	O
d	O
)	O
argue	O
that	O
while	O
the	O
decision	B
boundary	I
in	O
(	O
c	O
)	O
is	O
not	O
linear	B
in	O
1	O
,	O
x2	B
,	O
and	O
terms	O
of	O
x1	O
and	O
x2	B
,	O
it	O
is	O
linear	B
in	O
terms	O
of	O
x1	O
,	O
x	O
2	O
x	O
2	O
2	O
.	O
3.	O
here	O
we	O
explore	O
the	O
maximal	B
margin	I
classiﬁer	O
on	O
a	O
toy	O
data	B
set	O
.	O
(	O
a	O
)	O
we	O
are	O
given	O
n	O
=	O
7	O
observations	B
in	O
p	O
=	O
2	O
dimensions	O
.	O
for	O
each	O
observation	O
,	O
there	O
is	O
an	O
associated	O
class	O
label	O
.	O
obs	O
.	O
x1	O
x2	B
4	O
2	O
4	O
4	O
1	O
3	O
1	O
1	O
2	O
3	O
4	O
5	O
6	O
7	O
3	O
2	O
4	O
1	O
2	O
4	O
4	O
9.7	O
exercises	O
369	O
y	O
red	O
red	O
red	O
red	O
blue	O
blue	O
blue	O
sketch	O
the	O
observations	B
.	O
(	O
b	O
)	O
sketch	O
the	O
optimal	B
separating	I
hyperplane	I
,	O
and	O
provide	O
the	O
equa-	O
tion	O
for	O
this	O
hyperplane	B
(	O
of	O
the	O
form	O
(	O
9.1	O
)	O
)	O
.	O
(	O
c	O
)	O
describe	O
the	O
classiﬁcation	B
rule	O
for	O
the	O
maximal	B
margin	I
classiﬁer	O
.	O
it	O
should	O
be	O
something	O
along	O
the	O
lines	O
of	O
“	O
classify	O
to	O
red	O
if	O
β0	O
+	O
β1x1	O
+	O
β2x2	O
>	O
0	O
,	O
and	O
classify	O
to	O
blue	O
otherwise.	O
”	O
provide	O
the	O
values	O
for	O
β0	O
,	O
β1	O
,	O
and	O
β2	O
.	O
(	O
d	O
)	O
on	O
your	O
sketch	O
,	O
indicate	O
the	O
margin	B
for	O
the	O
maximal	B
margin	I
hyperplane	O
.	O
(	O
e	O
)	O
indicate	O
the	O
support	O
vectors	O
for	O
the	O
maximal	B
margin	I
classiﬁer	O
.	O
(	O
f	O
)	O
argue	O
that	O
a	O
slight	O
movement	O
of	O
the	O
seventh	O
observation	O
would	O
not	O
aﬀect	O
the	O
maximal	B
margin	I
hyperplane	O
.	O
(	O
g	O
)	O
sketch	O
a	O
hyperplane	B
that	O
is	O
not	O
the	O
optimal	O
separating	O
hyper-	O
plane	O
,	O
and	O
provide	O
the	O
equation	O
for	O
this	O
hyperplane	B
.	O
(	O
h	O
)	O
draw	O
an	O
additional	O
observation	O
on	O
the	O
plot	B
so	O
that	O
the	O
two	O
classes	O
are	O
no	O
longer	O
separable	O
by	O
a	O
hyperplane	B
.	O
applied	O
4.	O
generate	O
a	O
simulated	O
two-class	O
data	B
set	O
with	O
100	O
observations	B
and	O
two	O
features	O
in	O
which	O
there	O
is	O
a	O
visible	O
but	O
non-linear	B
separation	O
be-	O
tween	O
the	O
two	O
classes	O
.	O
show	O
that	O
in	O
this	O
setting	O
,	O
a	O
support	B
vector	I
machine	O
with	O
a	O
polynomial	B
kernel	O
(	O
with	O
degree	O
greater	O
than	O
1	O
)	O
or	O
a	O
radial	B
kernel	I
will	O
outperform	O
a	O
support	B
vector	I
classiﬁer	O
on	O
the	O
train-	O
ing	O
data	B
.	O
which	O
technique	O
performs	O
best	O
on	O
the	O
test	B
data	O
?	O
make	O
plots	O
and	O
report	O
training	B
and	O
test	B
error	O
rates	O
in	O
order	O
to	O
back	O
up	O
your	O
assertions	O
.	O
5.	O
we	O
have	O
seen	O
that	O
we	O
can	O
ﬁt	B
an	O
svm	O
with	O
a	O
non-linear	B
kernel	O
in	O
order	O
to	O
perform	O
classiﬁcation	B
using	O
a	O
non-linear	B
decision	O
boundary	O
.	O
we	O
will	O
now	O
see	O
that	O
we	O
can	O
also	O
obtain	O
a	O
non-linear	B
decision	O
boundary	O
by	O
performing	O
logistic	B
regression	I
using	O
non-linear	B
transformations	O
of	O
the	O
features	O
.	O
370	O
9.	O
support	B
vector	I
machines	O
(	O
a	O
)	O
generate	O
a	O
data	B
set	O
with	O
n	O
=	O
500	O
and	O
p	O
=	O
2	O
,	O
such	O
that	O
the	O
obser-	O
vations	O
belong	O
to	O
two	O
classes	O
with	O
a	O
quadratic	B
decision	O
boundary	O
between	O
them	O
.	O
for	O
instance	O
,	O
you	O
can	O
do	O
this	O
as	O
follows	O
:	O
>	O
x1	O
=	O
runif	O
(	O
500	O
)	O
-0.5	O
>	O
x2	B
=	O
runif	O
(	O
500	O
)	O
-0.5	O
>	O
y	O
=1*	O
(	O
x1	O
^2	O
-	O
x2	B
^2	O
>	O
0	O
)	O
(	O
b	O
)	O
plot	B
the	O
observations	B
,	O
colored	O
according	O
to	O
their	O
class	O
labels	O
.	O
your	O
plot	B
should	O
display	O
x1	O
on	O
the	O
x-axis	O
,	O
and	O
x2	B
on	O
the	O
y-	O
axis	O
.	O
(	O
c	O
)	O
fit	O
a	O
logistic	B
regression	I
model	O
to	O
the	O
data	B
,	O
using	O
x1	O
and	O
x2	B
as	O
predictors	O
.	O
(	O
d	O
)	O
apply	O
this	O
model	B
to	O
the	O
training	B
data	O
in	O
order	O
to	O
obtain	O
a	O
pre-	O
dicted	O
class	O
label	O
for	O
each	O
training	B
observation	O
.	O
plot	B
the	O
ob-	O
servations	O
,	O
colored	O
according	O
to	O
the	O
predicted	O
class	O
labels	O
.	O
the	O
decision	B
boundary	I
should	O
be	O
linear	B
.	O
(	O
e	O
)	O
now	O
ﬁt	B
a	O
logistic	B
regression	I
model	O
to	O
the	O
data	B
using	O
non-linear	B
1	O
,	O
x1×x2	O
,	O
log	O
(	O
x2	B
)	O
,	O
functions	O
of	O
x1	O
and	O
x2	B
as	O
predictors	O
(	O
e.g	O
.	O
x	O
2	O
and	O
so	O
forth	O
)	O
.	O
(	O
f	O
)	O
apply	O
this	O
model	B
to	O
the	O
training	B
data	O
in	O
order	O
to	O
obtain	O
a	O
pre-	O
dicted	O
class	O
label	O
for	O
each	O
training	B
observation	O
.	O
plot	B
the	O
ob-	O
servations	O
,	O
colored	O
according	O
to	O
the	O
predicted	O
class	O
labels	O
.	O
the	O
decision	B
boundary	I
should	O
be	O
obviously	O
non-linear	B
.	O
if	O
it	O
is	O
not	O
,	O
then	O
repeat	O
(	O
a	O
)	O
-	O
(	O
e	O
)	O
until	O
you	O
come	O
up	O
with	O
an	O
example	O
in	O
which	O
the	O
predicted	O
class	O
labels	O
are	O
obviously	O
non-linear	B
.	O
(	O
g	O
)	O
fit	O
a	O
support	B
vector	I
classiﬁer	O
to	O
the	O
data	B
with	O
x1	O
and	O
x2	B
as	O
predictors	O
.	O
obtain	O
a	O
class	O
prediction	B
for	O
each	O
training	B
observa-	O
tion	O
.	O
plot	B
the	O
observations	B
,	O
colored	O
according	O
to	O
the	O
predicted	O
class	O
labels	O
.	O
(	O
h	O
)	O
fit	O
a	O
svm	O
using	O
a	O
non-linear	B
kernel	O
to	O
the	O
data	B
.	O
obtain	O
a	O
class	O
prediction	B
for	O
each	O
training	B
observation	O
.	O
plot	B
the	O
observations	B
,	O
colored	O
according	O
to	O
the	O
predicted	O
class	O
labels	O
.	O
(	O
i	O
)	O
comment	O
on	O
your	O
results	O
.	O
6.	O
at	O
the	O
end	O
of	O
section	O
9.6.1	O
,	O
it	O
is	O
claimed	O
that	O
in	O
the	O
case	O
of	O
data	B
that	O
is	O
just	O
barely	O
linearly	O
separable	O
,	O
a	O
support	B
vector	I
classiﬁer	O
with	O
a	O
small	O
value	O
of	O
cost	O
that	O
misclassiﬁes	O
a	O
couple	O
of	O
training	B
observations	O
may	O
perform	O
better	O
on	O
test	B
data	O
than	O
one	O
with	O
a	O
huge	O
value	O
of	O
cost	O
that	O
does	O
not	O
misclassify	O
any	O
training	B
observations	O
.	O
you	O
will	O
now	O
investigate	O
this	O
claim	O
.	O
(	O
a	O
)	O
generate	O
two-class	O
data	B
with	O
p	O
=	O
2	O
in	O
such	O
a	O
way	O
that	O
the	O
classes	O
are	O
just	O
barely	O
linearly	O
separable	O
.	O
9.7	O
exercises	O
371	O
(	O
b	O
)	O
compute	O
the	O
cross-validation	B
error	O
rates	O
for	O
support	O
vector	B
classiﬁers	O
with	O
a	O
range	O
of	O
cost	O
values	O
.	O
how	O
many	O
training	B
er-	O
rors	O
are	O
misclassiﬁed	O
for	O
each	O
value	O
of	O
cost	O
considered	O
,	O
and	O
how	O
does	O
this	O
relate	O
to	O
the	O
cross-validation	B
errors	O
obtained	O
?	O
(	O
c	O
)	O
generate	O
an	O
appropriate	O
test	B
data	O
set	B
,	O
and	O
compute	O
the	O
test	B
errors	O
corresponding	O
to	O
each	O
of	O
the	O
values	O
of	O
cost	O
considered	O
.	O
which	O
value	O
of	O
cost	O
leads	O
to	O
the	O
fewest	O
test	B
errors	O
,	O
and	O
how	O
does	O
this	O
compare	O
to	O
the	O
values	O
of	O
cost	O
that	O
yield	O
the	O
fewest	O
training	B
errors	O
and	O
the	O
fewest	O
cross-validation	B
errors	O
?	O
(	O
d	O
)	O
discuss	O
your	O
results	O
.	O
7.	O
in	O
this	O
problem	O
,	O
you	O
will	O
use	O
support	B
vector	I
approaches	O
in	O
order	O
to	O
predict	O
whether	O
a	O
given	O
car	O
gets	O
high	O
or	O
low	O
gas	O
mileage	O
based	O
on	O
the	O
auto	O
data	B
set	O
.	O
(	O
a	O
)	O
create	O
a	O
binary	B
variable	O
that	O
takes	O
on	O
a	O
1	O
for	O
cars	O
with	O
gas	O
mileage	O
above	O
the	O
median	O
,	O
and	O
a	O
0	O
for	O
cars	O
with	O
gas	O
mileage	O
below	O
the	O
median	O
.	O
(	O
b	O
)	O
fit	O
a	O
support	B
vector	I
classiﬁer	O
to	O
the	O
data	B
with	O
various	O
values	O
of	O
cost	O
,	O
in	O
order	O
to	O
predict	O
whether	O
a	O
car	O
gets	O
high	O
or	O
low	O
gas	O
mileage	O
.	O
report	O
the	O
cross-validation	B
errors	O
associated	O
with	O
dif-	O
ferent	O
values	O
of	O
this	O
parameter	B
.	O
comment	O
on	O
your	O
results	O
.	O
(	O
c	O
)	O
now	O
repeat	O
(	O
b	O
)	O
,	O
this	O
time	O
using	O
svms	O
with	O
radial	B
and	O
polyno-	O
mial	O
basis	B
kernels	O
,	O
with	O
diﬀerent	O
values	O
of	O
gamma	O
and	O
degree	O
and	O
cost	O
.	O
comment	O
on	O
your	O
results	O
.	O
(	O
d	O
)	O
make	O
some	O
plots	O
to	O
back	O
up	O
your	O
assertions	O
in	O
(	O
b	O
)	O
and	O
(	O
c	O
)	O
.	O
hint	O
:	O
in	O
the	O
lab	O
,	O
we	O
used	O
the	O
plot	B
(	O
)	O
function	B
for	O
svm	O
objects	O
only	O
in	O
cases	O
with	O
p	O
=	O
2.	O
when	O
p	O
>	O
2	O
,	O
you	O
can	O
use	O
the	O
plot	B
(	O
)	O
function	B
to	O
create	O
plots	O
displaying	O
pairs	O
of	O
variables	O
at	O
a	O
time	O
.	O
essentially	O
,	O
instead	O
of	O
typing	O
>	O
plot	B
(	O
svmfit	O
,	O
dat	O
)	O
where	O
svmfit	O
contains	O
your	O
ﬁtted	O
model	O
and	O
dat	O
is	O
a	O
data	B
frame	I
containing	O
your	O
data	B
,	O
you	O
can	O
type	O
>	O
plot	B
(	O
svmfit	O
,	O
dat	O
,	O
x1∼x4	O
)	O
in	O
order	O
to	O
plot	B
just	O
the	O
ﬁrst	O
and	O
fourth	O
variables	O
.	O
however	O
,	O
you	O
must	O
replace	O
x1	O
and	O
x4	O
with	O
the	O
correct	O
variable	B
names	O
.	O
to	O
ﬁnd	O
out	O
more	O
,	O
type	O
?	O
plot.svm	O
.	O
8.	O
this	O
problem	O
involves	O
the	O
oj	O
data	B
set	O
which	O
is	O
part	O
of	O
the	O
islr	O
package	O
.	O
372	O
9.	O
support	B
vector	I
machines	O
(	O
a	O
)	O
create	O
a	O
training	B
set	O
containing	O
a	O
random	O
sample	O
of	O
800	O
remaining	O
containing	O
the	O
observations	B
,	O
and	O
a	O
test	B
observations	O
.	O
set	B
(	O
b	O
)	O
fit	O
a	O
support	B
vector	I
classiﬁer	O
to	O
the	O
training	B
data	O
using	O
cost=0.01	O
,	O
with	O
purchase	O
as	O
the	O
response	B
and	O
the	O
other	O
variables	O
as	O
predictors	O
.	O
use	O
the	O
summary	O
(	O
)	O
function	B
to	O
produce	O
summary	O
statistics	O
,	O
and	O
describe	O
the	O
results	O
obtained	O
.	O
(	O
c	O
)	O
what	O
are	O
the	O
training	B
and	O
test	B
error	O
rates	O
?	O
(	O
d	O
)	O
use	O
the	O
tune	O
(	O
)	O
function	B
to	O
select	O
an	O
optimal	O
cost	O
.	O
consider	O
val-	O
ues	O
in	O
the	O
range	O
0.01	O
to	O
10	O
.	O
(	O
e	O
)	O
compute	O
the	O
training	B
and	O
test	B
error	O
rates	O
using	O
this	O
new	O
value	O
for	O
cost	O
.	O
(	O
f	O
)	O
repeat	O
parts	O
(	O
b	O
)	O
through	O
(	O
e	O
)	O
using	O
a	O
support	B
vector	I
machine	O
with	O
a	O
radial	B
kernel	I
.	O
use	O
the	O
default	O
value	O
for	O
gamma	O
.	O
(	O
g	O
)	O
repeat	O
parts	O
(	O
b	O
)	O
through	O
(	O
e	O
)	O
using	O
a	O
support	B
vector	I
machine	O
with	O
a	O
polynomial	B
kernel	O
.	O
set	B
degree=2	O
.	O
(	O
h	O
)	O
overall	O
,	O
which	O
approach	B
seems	O
to	O
give	O
the	O
best	O
results	O
on	O
this	O
data	B
?	O
10	O
unsupervised	B
learning	I
this	O
book	O
concerns	O
such	O
as	O
most	O
of	O
regression	B
and	O
classiﬁcation	B
.	O
in	O
the	O
supervised	B
learning	I
setting	O
,	O
we	O
typically	O
have	O
access	O
to	O
a	O
set	B
of	O
p	O
features	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
,	O
measured	O
on	O
n	O
obser-	O
vations	O
,	O
and	O
a	O
response	B
y	O
also	O
measured	O
on	O
those	O
same	O
n	O
observations	B
.	O
the	O
goal	O
is	O
then	O
to	O
predict	O
y	O
using	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
.	O
supervised	B
learning	I
methods	O
this	O
chapter	O
will	O
instead	O
focus	O
on	O
unsupervised	B
learning	I
,	O
a	O
set	B
of	O
sta-	O
tistical	O
tools	O
intended	O
for	O
the	O
setting	O
in	O
which	O
we	O
have	O
only	O
a	O
set	B
of	O
fea-	O
tures	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
measured	O
on	O
n	O
observations	B
.	O
we	O
are	O
not	O
interested	O
in	O
prediction	B
,	O
because	O
we	O
do	O
not	O
have	O
an	O
associated	O
response	B
variable	O
y	O
.	O
rather	O
,	O
the	O
goal	O
is	O
to	O
discover	O
interesting	O
things	O
about	O
the	O
measurements	O
on	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
.	O
is	O
there	O
an	O
informative	O
way	O
to	O
visualize	O
the	O
data	B
?	O
can	O
we	O
discover	O
subgroups	O
among	O
the	O
variables	O
or	O
among	O
the	O
observations	B
?	O
unsupervised	B
learning	I
refers	O
to	O
a	O
diverse	O
set	B
of	O
techniques	O
for	O
answering	O
questions	O
such	O
as	O
these	O
.	O
in	O
this	O
chapter	O
,	O
we	O
will	O
focus	O
on	O
two	O
particu-	O
lar	O
types	O
of	O
unsupervised	B
learning	I
:	O
principal	B
components	I
analysis	O
,	O
a	O
tool	O
used	O
for	O
data	O
visualization	O
or	O
data	B
pre-processing	O
before	O
supervised	O
tech-	O
niques	O
are	O
applied	O
,	O
and	O
clustering	B
,	O
a	O
broad	O
class	O
of	O
methods	O
for	O
discovering	O
unknown	O
subgroups	O
in	O
data	B
.	O
10.1	O
the	O
challenge	O
of	O
unsupervised	B
learning	I
supervised	O
learning	O
is	O
a	O
well-understood	O
area	O
.	O
in	O
fact	O
,	O
if	O
you	O
have	O
read	O
the	O
preceding	O
chapters	O
in	O
this	O
book	O
,	O
then	O
you	O
should	O
by	O
now	O
have	O
a	O
good	O
g.	O
james	O
et	O
al.	O
,	O
an	O
introduction	O
to	O
statistical	O
learning	O
:	O
with	O
applications	O
in	O
r	O
,	O
springer	O
texts	O
in	O
statistics	O
,	O
doi	O
10.1007/978-1-4614-7138-7	O
10	O
,	O
©	O
springer	O
science+business	O
media	O
new	O
york	O
2013	O
373	O
exploratory	B
data	I
analysis	I
374	O
10.	O
unsupervised	B
learning	I
grasp	O
of	O
supervised	B
learning	I
.	O
for	O
instance	O
,	O
if	O
you	O
are	O
asked	O
to	O
predict	O
a	O
binary	B
outcome	O
from	O
a	O
data	B
set	O
,	O
you	O
have	O
a	O
very	O
well	O
developed	O
set	B
of	O
tools	O
at	O
your	O
disposal	O
(	O
such	O
as	O
logistic	B
regression	I
,	O
linear	B
discriminant	I
analysis	I
,	O
classiﬁcation	B
trees	O
,	O
support	B
vector	I
machines	O
,	O
and	O
more	O
)	O
as	O
well	O
as	O
a	O
clear	O
understanding	O
of	O
how	O
to	O
assess	O
the	O
quality	O
of	O
the	O
results	O
obtained	O
(	O
using	O
cross-validation	B
,	O
validation	O
on	O
an	O
independent	B
test	O
set	B
,	O
and	O
so	O
forth	O
)	O
.	O
in	O
contrast	B
,	O
unsupervised	B
learning	I
is	O
often	O
much	O
more	O
challenging	O
.	O
the	O
exercise	O
tends	O
to	O
be	O
more	O
subjective	O
,	O
and	O
there	O
is	O
no	O
simple	B
goal	O
for	O
the	O
analysis	B
,	O
such	O
as	O
prediction	B
of	O
a	O
response	B
.	O
unsupervised	B
learning	I
is	O
often	O
performed	O
as	O
part	O
of	O
an	O
exploratory	B
data	I
analysis	I
.	O
furthermore	O
,	O
it	O
can	O
be	O
hard	O
to	O
assess	O
the	O
results	O
obtained	O
from	O
unsupervised	B
learning	I
methods	O
,	O
since	O
there	O
is	O
no	O
universally	O
accepted	O
mechanism	O
for	O
performing	O
cross-	O
validation	O
or	O
validating	O
results	O
on	O
an	O
independent	B
data	O
set	B
.	O
the	O
reason	O
for	O
this	O
diﬀerence	O
is	O
simple	B
.	O
if	O
we	O
ﬁt	B
a	O
predictive	O
model	B
using	O
a	O
supervised	B
learning	I
technique	O
,	O
then	O
it	O
is	O
possible	O
to	O
check	O
our	O
work	O
by	O
seeing	O
how	O
well	O
our	O
model	B
predicts	O
the	O
response	B
y	O
on	O
observations	B
not	O
used	O
in	O
ﬁtting	O
the	O
model	B
.	O
however	O
,	O
in	O
unsupervised	B
learning	I
,	O
there	O
is	O
no	O
way	O
to	O
check	O
our	O
work	O
because	O
we	O
don	O
’	O
t	O
know	O
the	O
true	O
answer—the	O
problem	O
is	O
unsupervised	O
.	O
techniques	O
for	O
unsupervised	O
learning	O
are	O
of	O
growing	O
importance	B
in	O
a	O
number	O
of	O
ﬁelds	O
.	O
a	O
cancer	O
researcher	O
might	O
assay	O
gene	O
expression	O
levels	O
in	O
100	O
patients	O
with	O
breast	O
cancer	O
.	O
he	O
or	O
she	O
might	O
then	O
look	O
for	O
subgroups	O
among	O
the	O
breast	O
cancer	O
samples	O
,	O
or	O
among	O
the	O
genes	O
,	O
in	O
order	O
to	O
obtain	O
a	O
better	O
understanding	O
of	O
the	O
disease	O
.	O
an	O
online	O
shopping	O
site	O
might	O
try	O
to	O
identify	O
groups	O
of	O
shoppers	O
with	O
similar	O
browsing	O
and	O
purchase	O
histo-	O
ries	O
,	O
as	O
well	O
as	O
items	O
that	O
are	O
of	O
particular	O
interest	O
to	O
the	O
shoppers	O
within	O
each	O
group	O
.	O
then	O
an	O
individual	O
shopper	O
can	O
be	O
preferentially	O
shown	O
the	O
items	O
in	O
which	O
he	O
or	O
she	O
is	O
particularly	O
likely	O
to	O
be	O
interested	O
,	O
based	O
on	O
the	O
purchase	O
histories	O
of	O
similar	O
shoppers	O
.	O
a	O
search	O
engine	O
might	O
choose	O
what	O
search	O
results	O
to	O
display	O
to	O
a	O
particular	O
individual	O
based	O
on	O
the	O
click	O
histories	O
of	O
other	O
individuals	O
with	O
similar	O
search	O
patterns	O
.	O
these	O
statistical	O
learning	O
tasks	O
,	O
and	O
many	O
more	O
,	O
can	O
be	O
performed	O
via	O
unsupervised	B
learning	I
techniques	O
.	O
10.2	O
principal	B
components	I
analysis	O
principal	B
components	I
are	O
discussed	O
in	O
section	O
6.3.1	O
in	O
the	O
context	O
of	O
principal	B
components	I
regression	O
.	O
when	O
faced	O
with	O
a	O
large	O
set	B
of	O
corre-	O
lated	O
variables	O
,	O
principal	B
components	I
allow	O
us	O
to	O
summarize	O
this	O
set	B
with	O
a	O
smaller	O
number	O
of	O
representative	O
variables	O
that	O
collectively	O
explain	O
most	O
of	O
the	O
variability	O
in	O
the	O
original	O
set	B
.	O
the	O
principal	O
component	O
directions	O
are	O
presented	O
in	O
section	O
6.3.1	O
as	O
directions	O
in	O
feature	B
space	O
along	O
which	O
the	O
original	O
data	B
are	O
highly	O
variable	B
.	O
these	O
directions	O
also	O
deﬁne	O
lines	O
and	O
subspaces	O
that	O
are	O
as	O
close	O
as	O
possible	O
to	O
the	O
data	B
cloud	O
.	O
to	O
perform	O
10.2	O
principal	B
components	I
analysis	O
375	O
principal	B
components	I
regression	O
,	O
we	O
simply	O
use	O
principal	B
components	I
as	O
predictors	O
in	O
a	O
regression	B
model	O
in	O
place	O
of	O
the	O
original	O
larger	O
set	B
of	O
vari-	O
ables	O
.	O
principal	O
component	O
analysis	B
(	O
pca	O
)	O
refers	O
to	O
the	O
process	O
by	O
which	O
prin-	O
cipal	O
components	O
are	O
computed	O
,	O
and	O
the	O
subsequent	O
use	O
of	O
these	O
compo-	O
nents	O
in	O
understanding	O
the	O
data	B
.	O
pca	O
is	O
an	O
unsupervised	O
approach	O
,	O
since	O
it	O
involves	O
only	O
a	O
set	B
of	O
features	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
,	O
and	O
no	O
associated	O
response	B
y	O
.	O
apart	O
from	O
producing	O
derived	O
variables	O
for	O
use	O
in	O
supervised	B
learning	I
problems	O
,	O
pca	O
also	O
serves	O
as	O
a	O
tool	O
for	O
data	O
visualization	O
(	O
visualization	O
of	O
the	O
observations	B
or	O
visualization	O
of	O
the	O
variables	O
)	O
.	O
we	O
now	O
discuss	O
pca	O
in	O
greater	O
detail	O
,	O
focusing	O
on	O
the	O
use	O
of	O
pca	O
as	O
a	O
tool	O
for	O
unsupervised	O
data	B
exploration	O
,	O
in	O
keeping	O
with	O
the	O
topic	O
of	O
this	O
chapter	O
.	O
principal	O
component	O
analysis	B
10.2.1	O
what	O
are	O
principal	B
components	I
?	O
(	O
cid:9	O
)	O
p	O
2	O
(	O
cid:8	O
)	O
suppose	O
that	O
we	O
wish	O
to	O
visualize	O
n	O
observations	B
with	O
measurements	O
on	O
a	O
set	B
of	O
p	O
features	O
,	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
,	O
as	O
part	O
of	O
an	O
exploratory	B
data	I
analysis	I
.	O
we	O
could	O
do	O
this	O
by	O
examining	O
two-dimensional	O
scatterplots	O
of	O
the	O
data	B
,	O
each	O
of	O
which	O
contains	O
the	O
n	O
observations	B
’	O
measurements	O
on	O
two	O
of	O
the	O
=	O
p	O
(	O
p−1	O
)	O
/2	O
such	O
scatterplots	O
;	O
for	O
example	O
,	O
features	O
.	O
however	O
,	O
there	O
are	O
with	O
p	O
=	O
10	O
there	O
are	O
45	O
plots	O
!	O
if	O
p	O
is	O
large	O
,	O
then	O
it	O
will	O
certainly	O
not	O
be	O
possible	O
to	O
look	O
at	O
all	O
of	O
them	O
;	O
moreover	O
,	O
most	O
likely	O
none	O
of	O
them	O
will	O
be	O
informative	O
since	O
they	O
each	O
contain	O
just	O
a	O
small	O
fraction	O
of	O
the	O
total	O
information	O
present	O
in	O
the	O
data	B
set	O
.	O
clearly	O
,	O
a	O
better	O
method	O
is	O
required	O
to	O
visualize	O
the	O
n	O
observations	B
when	O
p	O
is	O
large	O
.	O
in	O
particular	O
,	O
we	O
would	O
like	O
to	O
ﬁnd	O
a	O
low-dimensional	B
representation	O
of	O
the	O
data	B
that	O
captures	O
as	O
much	O
of	O
the	O
information	O
as	O
possible	O
.	O
for	O
instance	O
,	O
if	O
we	O
can	O
obtain	O
a	O
two-dimensional	O
representation	O
of	O
the	O
data	B
that	O
captures	O
most	O
of	O
the	O
information	O
,	O
then	O
we	O
can	O
plot	B
the	O
observations	B
in	O
this	O
low-dimensional	B
space	O
.	O
pca	O
provides	O
a	O
tool	O
to	O
do	O
just	O
this	O
.	O
it	O
ﬁnds	O
a	O
low-dimensional	B
represen-	O
tation	O
of	O
a	O
data	B
set	O
that	O
contains	O
as	O
much	O
as	O
possible	O
of	O
the	O
variation	O
.	O
the	O
idea	O
is	O
that	O
each	O
of	O
the	O
n	O
observations	B
lives	O
in	O
p-dimensional	O
space	O
,	O
but	O
not	O
all	O
of	O
these	O
dimensions	O
are	O
equally	O
interesting	O
.	O
pca	O
seeks	O
a	O
small	O
number	O
of	O
dimensions	O
that	O
are	O
as	O
interesting	O
as	O
possible	O
,	O
where	O
the	O
concept	O
of	O
in-	O
teresting	O
is	O
measured	O
by	O
the	O
amount	O
that	O
the	O
observations	B
vary	O
along	O
each	O
dimension	O
.	O
each	O
of	O
the	O
dimensions	O
found	O
by	O
pca	O
is	O
a	O
linear	B
combination	I
of	O
the	O
p	O
features	O
.	O
we	O
now	O
explain	O
the	O
manner	O
in	O
which	O
these	O
dimensions	O
,	O
or	O
principal	B
components	I
,	O
are	O
found	O
.	O
the	O
ﬁrst	O
principal	O
component	O
of	O
a	O
set	B
of	O
features	O
x1	O
,	O
x2	B
,	O
.	O
.	O
.	O
,	O
xp	O
is	O
the	O
normalized	O
linear	B
combination	I
of	O
the	O
features	O
z1	O
=	O
φ11x1	O
+	O
φ21x2	O
+	O
.	O
.	O
.	O
+	O
φp1xp	O
(	O
10.1	O
)	O
(	O
cid:10	O
)	O
that	O
has	O
the	O
largest	O
variance	B
.	O
by	O
normalized	O
,	O
we	O
mean	O
that	O
j1	O
=	O
1.	O
we	O
refer	O
to	O
the	O
elements	O
φ11	O
,	O
.	O
.	O
.	O
,	O
φp1	O
as	O
the	O
loadings	O
of	O
the	O
ﬁrst	O
principal	O
p	O
j=1	O
φ2	O
loading	O
376	O
10.	O
unsupervised	B
learning	I
component	O
;	O
together	O
,	O
the	O
loadings	O
make	O
up	O
the	O
principal	O
component	O
load-	O
ing	O
vector	B
,	O
φ1	O
=	O
(	O
φ11	O
φ21	O
.	O
.	O
.	O
φp1	O
)	O
t	O
.	O
we	O
constrain	O
the	O
loadings	O
so	O
that	O
their	O
sum	B
of	I
squares	I
is	O
equal	O
to	O
one	O
,	O
since	O
otherwise	O
setting	O
these	O
elements	O
to	O
be	O
arbitrarily	O
large	O
in	O
absolute	O
value	O
could	O
result	O
in	O
an	O
arbitrarily	O
large	O
variance	B
.	O
given	O
a	O
n	O
×	O
p	O
data	B
set	O
x	O
,	O
how	O
do	O
we	O
compute	O
the	O
ﬁrst	O
principal	O
com-	O
ponent	O
?	O
since	O
we	O
are	O
only	O
interested	O
in	O
variance	B
,	O
we	O
assume	O
that	O
each	O
of	O
the	O
variables	O
in	O
x	O
has	O
been	O
centered	O
to	O
have	O
mean	O
zero	O
(	O
that	O
is	O
,	O
the	O
col-	O
umn	O
means	O
of	O
x	O
are	O
zero	O
)	O
.	O
we	O
then	O
look	O
for	O
the	O
linear	B
combination	I
of	O
the	O
sample	O
feature	B
values	O
of	O
the	O
form	O
zi1	O
=	O
φ11xi1	O
+	O
φ21xi2	O
+	O
.	O
.	O
.	O
+	O
φp1xip	O
(	O
10.2	O
)	O
(	O
cid:10	O
)	O
j1=1	O
.	O
that	O
has	O
largest	O
sample	O
variance	B
,	O
subject	O
to	O
the	O
constraint	O
that	O
in	O
other	O
words	O
,	O
the	O
ﬁrst	O
principal	O
component	O
loading	B
vector	I
solves	O
the	O
op-	O
timization	O
problem	O
p	O
j=1	O
φ2	O
⎧⎪⎨	O
⎪⎩	O
1	O
n	O
n	O
(	O
cid:17	O
)	O
⎛	O
⎝	O
p	O
(	O
cid:17	O
)	O
i=1	O
j=1	O
⎞	O
⎠2	O
⎫⎪⎬	O
⎪⎭	O
subject	O
to	O
p	O
(	O
cid:17	O
)	O
j=1	O
maximize	O
φ11	O
,	O
...	O
,	O
φp1	O
φj1xij	O
φ2	O
j1	O
=	O
1	O
.	O
(	O
10.3	O
)	O
(	O
cid:10	O
)	O
(	O
cid:10	O
)	O
from	O
(	O
10.2	O
)	O
we	O
can	O
write	O
the	O
objective	O
in	O
(	O
10.3	O
)	O
as	O
1	O
i1	O
.	O
since	O
n	O
n	O
1	O
i=1	O
xij	O
=	O
0	O
,	O
the	O
average	B
of	O
the	O
z11	O
,	O
.	O
.	O
.	O
,	O
zn1	O
will	O
be	O
zero	O
as	O
well	O
.	O
hence	O
n	O
the	O
objective	O
that	O
we	O
are	O
maximizing	O
in	O
(	O
10.3	O
)	O
is	O
just	O
the	O
sample	O
variance	B
of	O
the	O
n	O
values	O
of	O
zi1	O
.	O
we	O
refer	O
to	O
z11	O
,	O
.	O
.	O
.	O
,	O
zn1	O
as	O
the	O
scores	O
of	O
the	O
ﬁrst	O
princi-	O
score	O
pal	O
component	O
.	O
problem	O
(	O
10.3	O
)	O
can	O
be	O
solved	O
via	O
an	O
eigen	O
decomposition	B
,	O
a	O
standard	O
technique	O
in	O
linear	B
algebra	O
,	O
but	O
details	O
are	O
outside	O
of	O
the	O
scope	O
of	O
this	O
book	O
.	O
n	O
i=1	O
z2	O
there	O
is	O
a	O
nice	O
geometric	O
interpretation	O
for	O
the	O
ﬁrst	O
principal	O
component	O
.	O
the	O
loading	B
vector	I
φ1	O
with	O
elements	O
φ11	O
,	O
φ21	O
,	O
.	O
.	O
.	O
,	O
φp1	O
deﬁnes	O
a	O
direction	O
in	O
feature	B
space	O
along	O
which	O
the	O
data	B
vary	O
the	O
most	O
.	O
if	O
we	O
project	O
the	O
n	O
data	B
points	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
onto	O
this	O
direction	O
,	O
the	O
projected	O
values	O
are	O
the	O
princi-	O
pal	O
component	O
scores	O
z11	O
,	O
.	O
.	O
.	O
,	O
zn1	O
themselves	O
.	O
for	O
instance	O
,	O
figure	O
6.14	O
on	O
page	O
230	O
displays	O
the	O
ﬁrst	O
principal	O
component	O
loading	B
vector	I
(	O
green	O
solid	O
line	B
)	O
on	O
an	O
advertising	O
data	B
set	O
.	O
in	O
these	O
data	B
,	O
there	O
are	O
only	O
two	O
features	O
,	O
and	O
so	O
the	O
observations	B
as	O
well	O
as	O
the	O
ﬁrst	O
principal	O
component	O
loading	B
vector	I
can	O
be	O
easily	O
displayed	O
.	O
as	O
can	O
be	O
seen	O
from	O
(	O
6.19	O
)	O
,	O
in	O
that	O
data	B
set	O
φ11	O
=	O
0.839	O
and	O
φ21	O
=	O
0.544.	O
after	O
the	O
ﬁrst	O
principal	O
component	O
z1	O
of	O
the	O
features	O
has	O
been	O
deter-	O
mined	O
,	O
we	O
can	O
ﬁnd	O
the	O
second	O
principal	O
component	O
z2	O
.	O
the	O
second	O
prin-	O
cipal	O
component	O
is	O
the	O
linear	B
combination	I
of	O
x1	O
,	O
.	O
.	O
.	O
,	O
xp	O
that	O
has	O
maximal	O
variance	O
out	O
of	O
all	O
linear	B
combinations	O
that	O
are	O
uncorrelated	O
with	O
z1	O
.	O
the	O
second	O
principal	O
component	O
scores	O
z12	O
,	O
z22	O
,	O
.	O
.	O
.	O
,	O
zn2	O
take	O
the	O
form	O
zi2	O
=	O
φ12xi1	O
+	O
φ22xi2	O
+	O
.	O
.	O
.	O
+	O
φp2xip	O
,	O
(	O
10.4	O
)	O
10.2	O
principal	B
components	I
analysis	O
377	O
murder	O
assault	O
urbanpop	O
rape	O
pc1	O
pc2	O
0.5358995	O
−0.4181809	O
0.5831836	O
−0.1879856	O
0.8728062	O
0.2781909	O
0.1673186	O
0.5434321	O
table	O
10.1.	O
the	O
principal	O
component	O
loading	O
vectors	O
,	O
φ1	O
and	O
φ2	O
,	O
for	O
the	O
usarrests	O
data	B
.	O
these	O
are	O
also	O
displayed	O
in	O
figure	O
10.1.	O
where	O
φ2	O
is	O
the	O
second	O
principal	O
component	O
loading	B
vector	I
,	O
with	O
elements	O
φ12	O
,	O
φ22	O
,	O
.	O
.	O
.	O
,	O
φp2	O
.	O
it	O
turns	O
out	O
that	O
constraining	O
z2	O
to	O
be	O
uncorrelated	O
with	O
z1	O
is	O
equivalent	O
to	O
constraining	O
the	O
direction	O
φ2	O
to	O
be	O
orthogonal	B
(	O
perpen-	O
dicular	O
)	O
to	O
the	O
direction	O
φ1	O
.	O
in	O
the	O
example	O
in	O
figure	O
6.14	O
,	O
the	O
observations	B
lie	O
in	O
two-dimensional	O
space	O
(	O
since	O
p	O
=	O
2	O
)	O
,	O
and	O
so	O
once	O
we	O
have	O
found	O
φ1	O
,	O
there	O
is	O
only	O
one	O
possibility	O
for	O
φ2	O
,	O
which	O
is	O
shown	O
as	O
a	O
blue	O
dashed	O
line	B
.	O
(	O
from	O
section	O
6.3.1	O
,	O
we	O
know	O
that	O
φ12	O
=	O
0.544	O
and	O
φ22	O
=	O
−0.839	O
.	O
)	O
but	O
in	O
a	O
larger	O
data	B
set	O
with	O
p	O
>	O
2	O
variables	O
,	O
there	O
are	O
multiple	B
distinct	O
principal	B
components	I
,	O
and	O
they	O
are	O
deﬁned	O
in	O
a	O
similar	O
manner	O
.	O
to	O
ﬁnd	O
φ2	O
,	O
we	O
solve	O
a	O
problem	O
similar	O
to	O
(	O
10.3	O
)	O
with	O
φ2	O
replacing	O
φ1	O
,	O
and	O
with	O
the	O
additional	O
constraint	O
that	O
φ2	O
is	O
orthogonal	B
to	O
φ1.1	O
once	O
we	O
have	O
computed	O
the	O
principal	B
components	I
,	O
we	O
can	O
plot	B
them	O
against	O
each	O
other	O
in	O
order	O
to	O
produce	O
low-dimensional	B
views	O
of	O
the	O
data	B
.	O
for	O
instance	O
,	O
we	O
can	O
plot	B
the	O
score	B
vector	I
z1	O
against	O
z2	O
,	O
z1	O
against	O
z3	O
,	O
z2	O
against	O
z3	O
,	O
and	O
so	O
forth	O
.	O
geometrically	O
,	O
this	O
amounts	O
to	O
projecting	O
the	O
original	O
data	B
down	O
onto	O
the	O
subspace	O
spanned	O
by	O
φ1	O
,	O
φ2	O
,	O
and	O
φ3	O
,	O
and	O
plotting	O
the	O
projected	O
points	O
.	O
we	O
illustrate	O
the	O
use	O
of	O
pca	O
on	O
the	O
usarrests	O
data	B
set	O
.	O
for	O
each	O
of	O
the	O
50	O
states	O
in	O
the	O
united	O
states	O
,	O
the	O
data	B
set	O
contains	O
the	O
number	O
of	O
arrests	O
per	O
100	O
,	O
000	O
residents	O
for	O
each	O
of	O
three	O
crimes	O
:	O
assault	O
,	O
murder	O
,	O
and	O
rape	O
.	O
we	O
also	O
record	O
urbanpop	O
(	O
the	O
percent	O
of	O
the	O
population	O
in	O
each	O
state	O
living	O
in	O
urban	O
areas	O
)	O
.	O
the	O
principal	O
component	O
score	O
vectors	O
have	O
length	O
n	O
=	O
50	O
,	O
and	O
the	O
principal	O
component	O
loading	O
vectors	O
have	O
length	O
p	O
=	O
4.	O
pca	O
was	O
performed	O
after	O
standardizing	O
each	O
variable	B
to	O
have	O
mean	O
zero	O
and	O
standard	O
deviation	O
one	O
.	O
figure	O
10.1	O
plots	O
the	O
ﬁrst	O
two	O
principal	B
components	I
of	O
these	O
data	B
.	O
the	O
ﬁgure	O
represents	O
both	O
the	O
principal	O
component	O
scores	O
and	O
the	O
loading	O
vectors	O
in	O
a	O
single	B
biplot	O
display	O
.	O
the	O
loadings	O
are	O
also	O
given	O
in	O
table	O
10.1.	O
in	O
figure	O
10.1	O
,	O
we	O
see	O
that	O
the	O
ﬁrst	O
loading	B
vector	I
places	O
approximately	O
equal	O
weight	O
on	O
assault	O
,	O
murder	O
,	O
and	O
rape	O
,	O
with	O
much	O
less	O
weight	O
on	O
biplot	B
1on	O
a	O
technical	O
note	O
,	O
the	O
principal	O
component	O
directions	O
φ1	O
,	O
φ2	O
,	O
φ3	O
,	O
.	O
.	O
.	O
are	O
the	O
ordered	O
sequence	O
of	O
eigenvectors	O
of	O
the	O
matrix	O
xt	O
x	O
,	O
and	O
the	O
variances	O
of	O
the	O
compo-	O
nents	O
are	O
the	O
eigenvalues	O
.	O
there	O
are	O
at	O
most	O
min	O
(	O
n	O
−	O
1	O
,	O
p	O
)	O
principal	B
components	I
.	O
378	O
10.	O
unsupervised	B
learning	I
−0.5	O
0.0	O
0.5	O
urbanpop	O
3	O
2	O
1	O
t	O
n	O
e	O
n	O
o	O
p	O
m	O
o	O
c	O
l	O
i	O
a	O
p	O
c	O
n	O
i	O
r	O
p	O
d	O
n	O
o	O
c	O
e	O
s	O
1	O
−	O
2	O
−	O
3	O
−	O
hawaii	O
rhode	O
island	O
utah	O
massachusetts	O
new	O
jersey	O
connecticut	O
washington	O
wisconsin	O
minnesota	O
pennsylvania	O
ohio	O
oregon	O
delaware	O
nebraska	O
kansas	O
oklahoma	O
indiana	O
missouri	O
0	O
new	O
hampshire	O
iowa	O
maine	O
rth	O
dakota	O
idaho	O
virginia	O
wyoming	O
montana	O
south	O
dakota	O
kentucky	O
vermont	O
west	O
virginia	O
arkansas	O
alabama	O
alaska	O
georgia	O
murder	O
tennessee	O
louisiana	O
colorado	O
illinois	O
new	O
york	O
arizona	O
rape	O
texas	O
michigan	O
new	O
mexico	O
maryland	O
assault	O
california	O
5	O
.	O
0	O
nevada	O
florida	O
0	O
0	O
.	O
.	O
5	O
0	O
−	O
south	O
carolina	O
north	O
carolina	O
mississippi	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
first	O
principal	O
component	O
figure	O
10.1.	O
the	O
ﬁrst	O
two	O
principal	B
components	I
for	O
the	O
usarrests	O
data	B
.	O
the	O
blue	O
state	O
names	O
represent	O
the	O
scores	O
for	O
the	O
ﬁrst	O
two	O
principal	B
components	I
.	O
the	O
orange	O
arrows	O
indicate	O
the	O
ﬁrst	O
two	O
principal	O
component	O
loading	O
vectors	O
(	O
with	O
axes	O
on	O
the	O
top	O
and	O
right	O
)	O
.	O
for	O
example	O
,	O
the	O
loading	O
for	O
rape	O
on	O
the	O
ﬁrst	O
com-	O
ponent	O
is	O
0.54	O
,	O
and	O
its	O
loading	O
on	O
the	O
second	O
principal	O
component	O
0.17	O
(	O
the	O
word	O
rape	O
is	O
centered	O
at	O
the	O
point	O
(	O
0.54	O
,	O
0.17	O
)	O
)	O
.	O
this	O
ﬁgure	O
is	O
known	O
as	O
a	O
biplot	B
,	O
be-	O
cause	O
it	O
displays	O
both	O
the	O
principal	O
component	O
scores	O
and	O
the	O
principal	O
component	O
loadings	O
.	O
urbanpop	O
.	O
hence	O
this	O
component	O
roughly	O
corresponds	O
to	O
a	O
measure	O
of	O
overall	O
rates	O
of	O
serious	O
crimes	O
.	O
the	O
second	O
loading	B
vector	I
places	O
most	O
of	O
its	O
weight	O
on	O
urbanpop	O
and	O
much	O
less	O
weight	O
on	O
the	O
other	O
three	O
features	O
.	O
hence	O
,	O
this	O
component	O
roughly	O
corresponds	O
to	O
the	O
level	B
of	O
urbanization	O
of	O
the	O
state	O
.	O
overall	O
,	O
we	O
see	O
that	O
the	O
crime-related	O
variables	O
(	O
murder	O
,	O
assault	O
,	O
and	O
rape	O
)	O
are	O
located	O
close	O
to	O
each	O
other	O
,	O
and	O
that	O
the	O
urbanpop	O
variable	B
is	O
far	O
from	O
the	O
other	O
three	O
.	O
this	O
indicates	O
that	O
the	O
crime-related	O
variables	O
are	O
corre-	O
lated	O
with	O
each	O
other—states	O
with	O
high	O
murder	O
rates	O
tend	O
to	O
have	O
high	O
assault	O
and	O
rape	O
rates—and	O
that	O
the	O
urbanpop	O
variable	B
is	O
less	O
correlated	O
with	O
the	O
other	O
three	O
.	O
10.2	O
principal	B
components	I
analysis	O
379	O
we	O
can	O
examine	O
diﬀerences	O
between	O
the	O
states	O
via	O
the	O
two	O
principal	O
com-	O
ponent	O
score	O
vectors	O
shown	O
in	O
figure	O
10.1.	O
our	O
discussion	O
of	O
the	O
loading	O
vectors	O
suggests	O
that	O
states	O
with	O
large	O
positive	O
scores	O
on	O
the	O
ﬁrst	O
compo-	O
nent	O
,	O
such	O
as	O
california	O
,	O
nevada	O
and	O
florida	O
,	O
have	O
high	O
crime	O
rates	O
,	O
while	O
states	O
like	O
north	O
dakota	O
,	O
with	O
negative	O
scores	O
on	O
the	O
ﬁrst	O
component	O
,	O
have	O
low	O
crime	O
rates	O
.	O
california	O
also	O
has	O
a	O
high	O
score	O
on	O
the	O
second	O
component	O
,	O
indicating	O
a	O
high	O
level	B
of	O
urbanization	O
,	O
while	O
the	O
opposite	O
is	O
true	O
for	O
states	O
like	O
mississippi	O
.	O
states	O
close	O
to	O
zero	O
on	O
both	O
components	O
,	O
such	O
as	O
indiana	O
,	O
have	O
approximately	O
average	B
levels	O
of	O
both	O
crime	O
and	O
urbanization	O
.	O
10.2.2	O
another	O
interpretation	O
of	O
principal	B
components	I
the	O
ﬁrst	O
two	O
principal	O
component	O
loading	O
vectors	O
in	O
a	O
simulated	O
three-	O
dimensional	O
data	B
set	O
are	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
10.2	O
;	O
these	O
two	O
loading	O
vectors	O
span	O
a	O
plane	O
along	O
which	O
the	O
observations	B
have	O
the	O
highest	O
variance	B
.	O
in	O
the	O
previous	O
section	O
,	O
we	O
describe	O
the	O
principal	O
component	O
loading	O
vec-	O
tors	O
as	O
the	O
directions	O
in	O
feature	B
space	O
along	O
which	O
the	O
data	B
vary	O
the	O
most	O
,	O
and	O
the	O
principal	O
component	O
scores	O
as	O
projections	O
along	O
these	O
directions	O
.	O
however	O
,	O
an	O
alternative	O
interpretation	O
for	O
principal	O
components	O
can	O
also	O
be	O
useful	O
:	O
principal	B
components	I
provide	O
low-dimensional	B
linear	O
surfaces	O
that	O
are	O
closest	O
to	O
the	O
observations	B
.	O
we	O
expand	O
upon	O
that	O
interpretation	O
here	O
.	O
the	O
ﬁrst	O
principal	O
component	O
loading	B
vector	I
has	O
a	O
very	O
special	O
property	O
:	O
it	O
is	O
the	O
line	B
in	O
p-dimensional	O
space	O
that	O
is	O
closest	O
to	O
the	O
n	O
observations	B
(	O
using	O
average	B
squared	O
euclidean	O
distance	B
as	O
a	O
measure	O
of	O
closeness	O
)	O
.	O
this	O
interpretation	O
can	O
be	O
seen	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
6.15	O
;	O
the	O
dashed	O
lines	O
indicate	O
the	O
distance	B
between	O
each	O
observation	O
and	O
the	O
ﬁrst	O
principal	O
component	O
loading	B
vector	I
.	O
the	O
appeal	O
of	O
this	O
interpretation	O
is	O
clear	O
:	O
we	O
seek	O
a	O
single	B
dimension	O
of	O
the	O
data	B
that	O
lies	O
as	O
close	O
as	O
possible	O
to	O
all	O
of	O
the	O
data	B
points	O
,	O
since	O
such	O
a	O
line	B
will	O
likely	O
provide	O
a	O
good	O
summary	O
of	O
the	O
data	B
.	O
the	O
notion	O
of	O
principal	B
components	I
as	O
the	O
dimensions	O
that	O
are	O
clos-	O
est	O
to	O
the	O
n	O
observations	B
extends	O
beyond	O
just	O
the	O
ﬁrst	O
principal	O
com-	O
ponent	O
.	O
for	O
instance	O
,	O
the	O
ﬁrst	O
two	O
principal	B
components	I
of	O
a	O
data	B
set	O
span	O
the	O
plane	O
that	O
is	O
closest	O
to	O
the	O
n	O
observations	B
,	O
in	O
terms	O
of	O
average	B
squared	O
euclidean	O
distance	B
.	O
an	O
example	O
is	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
10.2.	O
the	O
ﬁrst	O
three	O
principal	B
components	I
of	O
a	O
data	B
set	O
span	O
the	O
three-dimensional	O
hyperplane	B
that	O
is	O
closest	O
to	O
the	O
n	O
observations	B
,	O
and	O
so	O
forth	O
.	O
using	O
this	O
interpretation	O
,	O
together	O
the	O
ﬁrst	O
m	O
principal	O
component	O
score	O
vectors	O
and	O
the	O
ﬁrst	O
m	O
principal	O
component	O
loading	O
vectors	O
provide	O
the	O
best	O
m	O
-dimensional	O
approximation	O
(	O
in	O
terms	O
of	O
euclidean	O
distance	B
)	O
to	O
the	O
ith	O
observation	O
xij	O
.	O
this	O
representation	O
can	O
be	O
written	O
380	O
10.	O
unsupervised	B
learning	I
t	O
n	O
e	O
n	O
o	O
p	O
m	O
o	O
c	O
l	O
i	O
a	O
p	O
c	O
n	O
i	O
r	O
p	O
d	O
n	O
o	O
c	O
e	O
s	O
0	O
1	O
.	O
5	O
.	O
0	O
0	O
0	O
.	O
5	O
.	O
0	O
−	O
.	O
0	O
1	O
−	O
−1.0	O
−0.5	O
0.0	O
0.5	O
first	O
principal	O
component	O
1.0	O
figure	O
10.2.	O
ninety	O
observations	B
simulated	O
in	O
three	O
dimensions	O
.	O
left	O
:	O
the	O
ﬁrst	O
two	O
principal	O
component	O
directions	O
span	O
the	O
plane	O
that	O
best	O
ﬁts	O
the	O
data	B
.	O
it	O
minimizes	O
the	O
sum	O
of	O
squared	O
distances	O
from	O
each	O
point	O
to	O
the	O
plane	O
.	O
right	O
:	O
the	O
ﬁrst	O
two	O
principal	O
component	O
score	O
vectors	O
give	O
the	O
coordinates	O
of	O
the	O
projection	B
of	O
the	O
90	O
observations	B
onto	O
the	O
plane	O
.	O
the	O
variance	B
in	O
the	O
plane	O
is	O
maximized	O
.	O
xij	O
≈	O
m	O
(	O
cid:17	O
)	O
m=1	O
zimφjm	O
(	O
10.5	O
)	O
(	O
assuming	O
the	O
original	O
data	B
matrix	O
x	O
is	O
column-centered	O
)	O
.	O
in	O
other	O
words	O
,	O
together	O
the	O
m	O
principal	O
component	O
score	O
vectors	O
and	O
m	O
principal	O
com-	O
ponent	O
loading	O
vectors	O
can	O
give	O
a	O
good	O
approximation	O
to	O
the	O
data	B
when	O
m	O
is	O
suﬃciently	O
large	O
.	O
when	O
m	O
=	O
min	O
(	O
n	O
−	O
1	O
,	O
p	O
)	O
,	O
then	O
the	O
representation	O
is	O
exact	O
:	O
xij	O
=	O
m	O
m=1	O
zimφjm	O
.	O
(	O
cid:10	O
)	O
10.2.3	O
more	O
on	O
pca	O
scaling	O
the	O
variables	O
we	O
have	O
already	O
mentioned	O
that	O
before	O
pca	O
is	O
performed	O
,	O
the	O
variables	O
should	O
be	O
centered	O
to	O
have	O
mean	O
zero	O
.	O
furthermore	O
,	O
the	O
results	O
obtained	O
when	O
we	O
perform	O
pca	O
will	O
also	O
depend	O
on	O
whether	O
the	O
variables	O
have	O
been	O
individually	O
scaled	O
(	O
each	O
multiplied	O
by	O
a	O
diﬀerent	O
constant	O
)	O
.	O
this	O
is	O
in	O
contrast	B
to	O
some	O
other	O
supervised	O
and	O
unsupervised	B
learning	I
techniques	O
,	O
such	O
as	O
linear	B
regression	I
,	O
in	O
which	O
scaling	O
the	O
variables	O
has	O
no	O
eﬀect	O
.	O
(	O
in	O
linear	B
regression	I
,	O
multiplying	O
a	O
variable	B
by	O
a	O
factor	B
of	O
c	O
will	O
simply	O
lead	O
to	O
multiplication	O
of	O
the	O
corresponding	O
coeﬃcient	B
estimate	O
by	O
a	O
factor	B
of	O
1/c	O
,	O
and	O
thus	O
will	O
have	O
no	O
substantive	O
eﬀect	O
on	O
the	O
model	B
obtained	O
.	O
)	O
for	O
instance	O
,	O
figure	O
10.1	O
was	O
obtained	O
after	O
scaling	O
each	O
of	O
the	O
variables	O
to	O
have	O
standard	O
deviation	O
one	O
.	O
this	O
is	O
reproduced	O
in	O
the	O
left-hand	O
plot	B
in	O
figure	O
10.3.	O
why	O
does	O
it	O
matter	O
that	O
we	O
scaled	O
the	O
variables	O
?	O
in	O
these	O
data	B
,	O
10.2	O
principal	B
components	I
analysis	O
381	O
scaled	O
−0.5	O
0.0	O
0.5	O
−0.5	O
urbanpop	O
t	O
n	O
e	O
n	O
o	O
p	O
m	O
o	O
c	O
i	O
l	O
a	O
p	O
c	O
n	O
i	O
r	O
p	O
d	O
n	O
o	O
c	O
e	O
s	O
3	O
2	O
1	O
0	O
1	O
−	O
2	O
−	O
3	O
−	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
5	O
.	O
0	O
0	O
.	O
0	O
5	O
.	O
0	O
−	O
t	O
n	O
e	O
n	O
o	O
p	O
m	O
o	O
c	O
i	O
l	O
a	O
p	O
c	O
n	O
i	O
r	O
p	O
d	O
n	O
o	O
c	O
e	O
s	O
0	O
5	O
1	O
0	O
0	O
1	O
0	O
5	O
0	O
0	O
5	O
−	O
0	O
0	O
1	O
−	O
*	O
*	O
*	O
*	O
*	O
*	O
rape	O
*	O
*	O
*	O
assault	O
*	O
*	O
murder	O
*	O
*	O
*	O
*	O
*	O
*	O
unscaled	O
0.0	O
urbanpop	O
0.5	O
1.0	O
0	O
.	O
1	O
5	O
.	O
0	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
rape	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
murder	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
*	O
assau	O
0	O
.	O
0	O
*	O
5	O
.	O
0	O
−	O
−3	O
−2	O
−1	O
0	O
1	O
2	O
3	O
−100	O
−50	O
0	O
50	O
100	O
150	O
first	O
principal	O
component	O
first	O
principal	O
component	O
figure	O
10.3.	O
two	O
principal	O
component	O
biplots	O
for	O
the	O
usarrests	O
data	B
.	O
left	O
:	O
the	O
same	O
as	O
figure	O
10.1	O
,	O
with	O
the	O
variables	O
scaled	O
to	O
have	O
unit	O
standard	O
deviations	O
.	O
right	O
:	O
principal	B
components	I
using	O
unscaled	O
data	B
.	O
assault	O
has	O
by	O
far	O
the	O
largest	O
loading	O
on	O
the	O
ﬁrst	O
principal	O
component	O
because	O
it	O
has	O
the	O
highest	O
variance	B
among	O
the	O
four	O
variables	O
.	O
in	O
general	O
,	O
scaling	O
the	O
variables	O
to	O
have	O
standard	O
deviation	O
one	O
is	O
recommended	O
.	O
the	O
variables	O
are	O
measured	O
in	O
diﬀerent	O
units	O
;	O
murder	O
,	O
rape	O
,	O
and	O
assault	O
are	O
reported	O
as	O
the	O
number	O
of	O
occurrences	O
per	O
100	O
,	O
000	O
people	O
,	O
and	O
urbanpop	O
is	O
the	O
percentage	O
of	O
the	O
state	O
’	O
s	O
population	O
that	O
lives	O
in	O
an	O
urban	O
area	O
.	O
these	O
four	O
variables	O
have	O
variance	B
18.97	O
,	O
87.73	O
,	O
6945.16	O
,	O
and	O
209.5	O
,	O
respec-	O
tively	O
.	O
consequently	O
,	O
if	O
we	O
perform	O
pca	O
on	O
the	O
unscaled	O
variables	O
,	O
then	O
the	O
ﬁrst	O
principal	O
component	O
loading	B
vector	I
will	O
have	O
a	O
very	O
large	O
loading	O
for	O
assault	O
,	O
since	O
that	O
variable	B
has	O
by	O
far	O
the	O
highest	O
variance	B
.	O
the	O
right-	O
hand	O
plot	B
in	O
figure	O
10.3	O
displays	O
the	O
ﬁrst	O
two	O
principal	B
components	I
for	O
the	O
usarrests	O
data	B
set	O
,	O
without	O
scaling	O
the	O
variables	O
to	O
have	O
standard	O
devia-	O
tion	O
one	O
.	O
as	O
predicted	O
,	O
the	O
ﬁrst	O
principal	O
component	O
loading	B
vector	I
places	O
almost	O
all	O
of	O
its	O
weight	O
on	O
assault	O
,	O
while	O
the	O
second	O
principal	O
component	O
loading	B
vector	I
places	O
almost	O
all	O
of	O
its	O
weight	O
on	O
urpanpop	O
.	O
comparing	O
this	O
to	O
the	O
left-hand	O
plot	B
,	O
we	O
see	O
that	O
scaling	O
does	O
indeed	O
have	O
a	O
substantial	O
eﬀect	O
on	O
the	O
results	O
obtained	O
.	O
however	O
,	O
this	O
result	O
is	O
simply	O
a	O
consequence	O
of	O
the	O
scales	O
on	O
which	O
the	O
variables	O
were	O
measured	O
.	O
for	O
instance	O
,	O
if	O
assault	O
were	O
measured	O
in	O
units	O
of	O
the	O
number	O
of	O
occurrences	O
per	O
100	O
people	O
(	O
rather	O
than	O
number	O
of	O
oc-	O
currences	O
per	O
100	O
,	O
000	O
people	O
)	O
,	O
then	O
this	O
would	O
amount	O
to	O
dividing	O
all	O
of	O
the	O
elements	O
of	O
that	O
variable	B
by	O
1	O
,	O
000.	O
then	O
the	O
variance	B
of	O
the	O
variable	B
would	O
be	O
tiny	O
,	O
and	O
so	O
the	O
ﬁrst	O
principal	O
component	O
loading	B
vector	I
would	O
have	O
a	O
very	O
small	O
value	O
for	O
that	O
variable	B
.	O
because	O
it	O
is	O
undesirable	O
for	O
the	O
principal	B
components	I
obtained	O
to	O
depend	O
on	O
an	O
arbitrary	O
choice	O
of	O
scaling	O
,	O
we	O
typically	O
scale	O
each	O
variable	B
to	O
have	O
standard	O
deviation	O
one	O
before	O
we	O
perform	O
pca	O
.	O
382	O
10.	O
unsupervised	B
learning	I
in	O
certain	O
settings	O
,	O
however	O
,	O
the	O
variables	O
may	O
be	O
measured	O
in	O
the	O
same	O
units	O
.	O
in	O
this	O
case	O
,	O
we	O
might	O
not	O
wish	O
to	O
scale	O
the	O
variables	O
to	O
have	O
stan-	O
dard	O
deviation	O
one	O
before	O
performing	O
pca	O
.	O
for	O
instance	O
,	O
suppose	O
that	O
the	O
variables	O
in	O
a	O
given	O
data	B
set	O
correspond	O
to	O
expression	O
levels	O
for	O
p	O
genes	O
.	O
then	O
since	O
expression	O
is	O
measured	O
in	O
the	O
same	O
“	O
units	O
”	O
for	O
each	O
gene	O
,	O
we	O
might	O
choose	O
not	O
to	O
scale	O
the	O
genes	O
to	O
each	O
have	O
standard	O
deviation	O
one	O
.	O
uniqueness	O
of	O
the	O
principal	B
components	I
each	O
principal	O
component	O
loading	B
vector	I
is	O
unique	O
,	O
up	O
to	O
a	O
sign	O
ﬂip	O
.	O
this	O
means	O
that	O
two	O
diﬀerent	O
software	O
packages	O
will	O
yield	O
the	O
same	O
principal	O
component	O
loading	O
vectors	O
,	O
although	O
the	O
signs	O
of	O
those	O
loading	O
vectors	O
may	O
diﬀer	O
.	O
the	O
signs	O
may	O
diﬀer	O
because	O
each	O
principal	O
component	O
loading	B
vector	I
speciﬁes	O
a	O
direction	O
in	O
p-dimensional	O
space	O
:	O
ﬂipping	O
the	O
sign	O
has	O
no	O
eﬀect	O
as	O
the	O
direction	O
does	O
not	O
change	O
.	O
(	O
consider	O
figure	O
6.14—the	O
principal	O
component	O
loading	B
vector	I
is	O
a	O
line	B
that	O
extends	O
in	O
either	O
direction	O
,	O
and	O
ﬂipping	O
its	O
sign	O
would	O
have	O
no	O
eﬀect	O
.	O
)	O
similarly	O
,	O
the	O
score	O
vectors	O
are	O
unique	O
up	O
to	O
a	O
sign	O
ﬂip	O
,	O
since	O
the	O
variance	B
of	O
z	O
is	O
the	O
same	O
as	O
the	O
variance	B
of	O
−z	O
.	O
it	O
is	O
worth	O
noting	O
that	O
when	O
we	O
use	O
(	O
10.5	O
)	O
to	O
approximate	O
xij	O
we	O
multiply	O
zim	O
by	O
φjm	O
.	O
hence	O
,	O
if	O
the	O
sign	O
is	O
ﬂipped	O
on	O
both	O
the	O
loading	O
and	O
score	O
vectors	O
,	O
the	O
ﬁnal	O
product	O
of	O
the	O
two	O
quantities	O
is	O
unchanged	O
.	O
the	O
proportion	B
of	I
variance	I
explained	O
in	O
figure	O
10.2	O
,	O
we	O
performed	O
pca	O
on	O
a	O
three-dimensional	O
data	B
set	O
(	O
left-	O
hand	O
panel	O
)	O
and	O
projected	O
the	O
data	B
onto	O
the	O
ﬁrst	O
two	O
principal	O
component	O
loading	O
vectors	O
in	O
order	O
to	O
obtain	O
a	O
two-dimensional	O
view	O
of	O
the	O
data	B
(	O
i.e	O
.	O
the	O
principal	O
component	O
score	O
vectors	O
;	O
right-hand	O
panel	O
)	O
.	O
we	O
see	O
that	O
this	O
two-dimensional	O
representation	O
of	O
the	O
three-dimensional	O
data	B
does	O
success-	O
fully	O
capture	O
the	O
major	O
pattern	O
in	O
the	O
data	B
:	O
the	O
orange	O
,	O
green	O
,	O
and	O
cyan	O
observations	B
that	O
are	O
near	O
each	O
other	O
in	O
three-dimensional	O
space	O
remain	O
nearby	O
in	O
the	O
two-dimensional	O
representation	O
.	O
similarly	O
,	O
we	O
have	O
seen	O
on	O
the	O
usarrests	O
data	B
set	O
that	O
we	O
can	O
summarize	O
the	O
50	O
observations	B
and	O
4	O
variables	O
using	O
just	O
the	O
ﬁrst	O
two	O
principal	O
component	O
score	O
vectors	O
and	O
the	O
ﬁrst	O
two	O
principal	O
component	O
loading	O
vectors	O
.	O
we	O
can	O
now	O
ask	O
a	O
natural	B
question	O
:	O
how	O
much	O
of	O
the	O
information	O
in	O
a	O
given	O
data	B
set	O
is	O
lost	O
by	O
projecting	O
the	O
observations	B
onto	O
the	O
ﬁrst	O
few	O
principal	B
components	I
?	O
that	O
is	O
,	O
how	O
much	O
of	O
the	O
variance	B
in	O
the	O
data	B
is	O
not	O
contained	O
in	O
the	O
ﬁrst	O
few	O
principal	B
components	I
?	O
more	O
generally	O
,	O
we	O
are	O
interested	O
in	O
knowing	O
the	O
proportion	B
of	I
variance	I
explained	O
(	O
pve	O
)	O
by	O
each	O
principal	O
component	O
.	O
the	O
total	O
variance	O
present	O
in	O
a	O
data	B
set	O
(	O
assuming	O
that	O
the	O
variables	O
have	O
been	O
centered	O
to	O
have	O
mean	O
zero	O
)	O
is	O
deﬁned	O
as	O
p	O
(	O
cid:17	O
)	O
j=1	O
var	O
(	O
xj	O
)	O
=	O
p	O
(	O
cid:17	O
)	O
j=1	O
1	O
n	O
n	O
(	O
cid:17	O
)	O
i=1	O
x2	B
ij	O
,	O
(	O
10.6	O
)	O
proportion	B
of	I
variance	I
explained	O
10.2	O
principal	B
components	I
analysis	O
383	O
i	O
l	O
d	O
e	O
n	O
a	O
p	O
x	O
e	O
e	O
c	O
n	O
a	O
i	O
r	O
a	O
v	O
.	O
p	O
o	O
r	O
p	O
0	O
.	O
1	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
i	O
l	O
d	O
e	O
n	O
a	O
p	O
x	O
e	O
e	O
c	O
n	O
a	O
i	O
r	O
a	O
v	O
.	O
p	O
o	O
r	O
p	O
e	O
v	O
i	O
t	O
a	O
u	O
m	O
u	O
c	O
l	O
0	O
.	O
1	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
1.0	O
1.5	O
2.0	O
2.5	O
3.0	O
3.5	O
4.0	O
1.0	O
1.5	O
2.0	O
2.5	O
3.0	O
3.5	O
4.0	O
principal	O
component	O
principal	O
component	O
figure	O
10.4.	O
left	O
:	O
a	O
scree	B
plot	I
depicting	O
the	O
proportion	B
of	I
variance	I
explained	O
by	O
each	O
of	O
the	O
four	O
principal	B
components	I
in	O
the	O
usarrests	O
data	B
.	O
right	O
:	O
the	O
cu-	O
mulative	O
proportion	B
of	I
variance	I
explained	O
by	O
the	O
four	O
principal	B
components	I
in	O
the	O
usarrests	O
data	B
.	O
and	O
the	O
variance	B
explained	O
by	O
the	O
mth	O
principal	O
component	O
is	O
n	O
(	O
cid:17	O
)	O
i=1	O
1	O
n	O
z2	O
im	O
=	O
1	O
n	O
n	O
(	O
cid:17	O
)	O
⎛	O
⎝	O
p	O
(	O
cid:17	O
)	O
i=1	O
j=1	O
⎞	O
⎠2	O
φjmxij	O
.	O
(	O
10.7	O
)	O
therefore	O
,	O
the	O
pve	O
of	O
the	O
mth	O
principal	O
component	O
is	O
given	O
by	O
(	O
cid:10	O
)	O
(	O
cid:18	O
)	O
(	O
cid:10	O
)	O
(	O
cid:10	O
)	O
n	O
i=1	O
p	O
j=1	O
p	O
(	O
cid:10	O
)	O
j=1	O
φjmxij	O
n	O
i=1	O
x2	B
ij	O
(	O
cid:19	O
)	O
2	O
.	O
(	O
10.8	O
)	O
the	O
pve	O
of	O
each	O
principal	O
component	O
is	O
a	O
positive	O
quantity	O
.	O
in	O
order	O
to	O
compute	O
the	O
cumulative	O
pve	O
of	O
the	O
ﬁrst	O
m	O
principal	B
components	I
,	O
we	O
can	O
simply	O
sum	O
(	O
10.8	O
)	O
over	O
each	O
of	O
the	O
ﬁrst	O
m	O
pves	O
.	O
in	O
total	O
,	O
there	O
are	O
min	O
(	O
n	O
−	O
1	O
,	O
p	O
)	O
principal	B
components	I
,	O
and	O
their	O
pves	O
sum	O
to	O
one	O
.	O
in	O
the	O
usarrests	O
data	B
,	O
the	O
ﬁrst	O
principal	O
component	O
explains	O
62.0	O
%	O
of	O
the	O
variance	B
in	O
the	O
data	B
,	O
and	O
the	O
next	O
principal	O
component	O
explains	O
24.7	O
%	O
of	O
the	O
variance	B
.	O
together	O
,	O
the	O
ﬁrst	O
two	O
principal	B
components	I
explain	O
almost	O
87	O
%	O
of	O
the	O
variance	B
in	O
the	O
data	B
,	O
and	O
the	O
last	O
two	O
principal	B
components	I
explain	O
only	O
13	O
%	O
of	O
the	O
variance	B
.	O
this	O
means	O
that	O
figure	O
10.1	O
provides	O
a	O
pretty	O
accurate	O
summary	O
of	O
the	O
data	B
using	O
just	O
two	O
dimensions	O
.	O
the	O
pve	O
of	O
each	O
principal	O
component	O
,	O
as	O
well	O
as	O
the	O
cumulative	O
pve	O
,	O
is	O
shown	O
in	O
figure	O
10.4.	O
the	O
left-hand	O
panel	O
is	O
known	O
as	O
a	O
scree	B
plot	I
,	O
and	O
will	O
be	O
discussed	O
next	O
.	O
deciding	O
how	O
many	O
principal	B
components	I
to	O
use	O
in	O
general	O
,	O
a	O
n	O
×	O
p	O
data	B
matrix	O
x	O
has	O
min	O
(	O
n	O
−	O
1	O
,	O
p	O
)	O
distinct	O
principal	B
components	I
.	O
however	O
,	O
we	O
usually	O
are	O
not	O
interested	O
in	O
all	O
of	O
them	O
;	O
rather	O
,	O
scree	B
plot	I
384	O
10.	O
unsupervised	B
learning	I
we	O
would	O
like	O
to	O
use	O
just	O
the	O
ﬁrst	O
few	O
principal	B
components	I
in	O
order	O
to	O
visualize	O
or	O
interpret	O
the	O
data	B
.	O
in	O
fact	O
,	O
we	O
would	O
like	O
to	O
use	O
the	O
smallest	O
number	O
of	O
principal	B
components	I
required	O
to	O
get	O
a	O
good	O
understanding	O
of	O
the	O
data	B
.	O
how	O
many	O
principal	B
components	I
are	O
needed	O
?	O
unfortunately	O
,	O
there	O
is	O
no	O
single	B
(	O
or	O
simple	B
!	O
)	O
answer	O
to	O
this	O
question	O
.	O
we	O
typically	O
decide	O
on	O
the	O
number	O
of	O
principal	B
components	I
required	O
to	O
visualize	O
the	O
data	B
by	O
examining	O
a	O
scree	B
plot	I
,	O
such	O
as	O
the	O
one	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
10.4.	O
we	O
choose	O
the	O
smallest	O
number	O
of	O
principal	B
components	I
that	O
are	O
required	O
in	O
order	O
to	O
explain	O
a	O
sizable	O
amount	O
of	O
the	O
variation	O
in	O
the	O
data	B
.	O
this	O
is	O
done	O
by	O
eyeballing	O
the	O
scree	B
plot	I
,	O
and	O
looking	O
for	O
a	O
point	O
at	O
which	O
the	O
proportion	B
of	I
variance	I
explained	O
by	O
each	O
subsequent	O
principal	O
component	O
drops	O
oﬀ	O
.	O
this	O
is	O
often	O
referred	O
to	O
as	O
an	O
elbow	B
in	O
the	O
scree	B
plot	I
.	O
for	O
instance	O
,	O
by	O
inspection	O
of	O
figure	O
10.4	O
,	O
one	O
might	O
conclude	O
that	O
a	O
fair	O
amount	O
of	O
variance	B
is	O
explained	B
by	O
the	O
ﬁrst	O
two	O
principal	B
components	I
,	O
and	O
that	O
there	O
is	O
an	O
elbow	B
after	O
the	O
second	O
component	O
.	O
after	O
all	O
,	O
the	O
third	O
principal	O
component	O
explains	O
less	O
than	O
ten	O
percent	O
of	O
the	O
variance	B
in	O
the	O
data	B
,	O
and	O
the	O
fourth	O
principal	O
component	O
explains	O
less	O
than	O
half	O
that	O
and	O
so	O
is	O
essentially	O
worthless	O
.	O
however	O
,	O
this	O
type	O
of	O
visual	O
analysis	B
is	O
inherently	O
ad	O
hoc	O
.	O
unfortunately	O
,	O
there	O
is	O
no	O
well-accepted	O
objective	O
way	O
to	O
decide	O
how	O
many	O
principal	O
com-	O
ponents	O
are	O
enough	O
.	O
in	O
fact	O
,	O
the	O
question	O
of	O
how	O
many	O
principal	O
compo-	O
nents	O
are	O
enough	O
is	O
inherently	O
ill-deﬁned	O
,	O
and	O
will	O
depend	O
on	O
the	O
speciﬁc	O
area	O
of	O
application	O
and	O
the	O
speciﬁc	O
data	B
set	O
.	O
in	O
practice	O
,	O
we	O
tend	O
to	O
look	O
at	O
the	O
ﬁrst	O
few	O
principal	B
components	I
in	O
order	O
to	O
ﬁnd	O
interesting	O
patterns	O
in	O
the	O
data	B
.	O
if	O
no	O
interesting	O
patterns	O
are	O
found	O
in	O
the	O
ﬁrst	O
few	O
principal	B
components	I
,	O
then	O
further	O
principal	B
components	I
are	O
unlikely	O
to	O
be	O
of	O
inter-	O
est	O
.	O
conversely	O
,	O
if	O
the	O
ﬁrst	O
few	O
principal	B
components	I
are	O
interesting	O
,	O
then	O
we	O
typically	O
continue	O
to	O
look	O
at	O
subsequent	O
principal	B
components	I
until	O
no	O
further	O
interesting	O
patterns	O
are	O
found	O
.	O
this	O
is	O
admittedly	O
a	O
subjective	O
ap-	O
proach	O
,	O
and	O
is	O
reﬂective	O
of	O
the	O
fact	O
that	O
pca	O
is	O
generally	O
used	O
as	O
a	O
tool	O
for	O
exploratory	O
data	B
analysis	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
we	O
compute	O
principal	B
components	I
for	O
use	O
in	O
a	O
supervised	O
analysis	O
,	O
such	O
as	O
the	O
principal	B
components	I
regression	O
presented	O
in	O
section	O
6.3.1	O
,	O
then	O
there	O
is	O
a	O
simple	B
and	O
objective	O
way	O
to	O
determine	O
how	O
many	O
principal	B
components	I
to	O
use	O
:	O
we	O
can	O
treat	O
the	O
number	O
of	O
principal	O
component	O
score	O
vectors	O
to	O
be	O
used	O
in	O
the	O
regression	B
as	O
a	O
tuning	B
parameter	I
to	O
be	O
selected	O
via	O
cross-validation	B
or	O
a	O
related	O
approach	B
.	O
the	O
comparative	O
simplicity	O
of	O
selecting	O
the	O
number	O
of	O
principal	B
components	I
for	O
a	O
supervised	O
analysis	O
is	O
one	O
manifestation	O
of	O
the	O
fact	O
that	O
supervised	O
analyses	O
tend	O
to	O
be	O
more	O
clearly	O
deﬁned	O
and	O
more	O
objectively	O
evaluated	O
than	O
unsupervised	O
analyses	O
.	O
clustering	B
10.3	O
clustering	B
methods	O
385	O
10.2.4	O
other	O
uses	O
for	O
principal	O
components	O
we	O
saw	O
in	O
section	O
6.3.1	O
that	O
we	O
can	O
perform	O
regression	B
using	O
the	O
principal	O
component	O
score	O
vectors	O
as	O
features	O
.	O
in	O
fact	O
,	O
many	O
statistical	O
techniques	O
,	O
such	O
as	O
regression	B
,	O
classiﬁcation	B
,	O
and	O
clustering	B
,	O
can	O
be	O
easily	O
adapted	O
to	O
use	O
the	O
n	O
×	O
m	O
matrix	O
whose	O
columns	O
are	O
the	O
ﬁrst	O
m	O
(	O
cid:15	O
)	O
p	O
principal	O
com-	O
ponent	O
score	O
vectors	O
,	O
rather	O
than	O
using	O
the	O
full	O
n	O
×	O
p	O
data	B
matrix	O
.	O
this	O
can	O
lead	O
to	O
less	O
noisy	O
results	O
,	O
since	O
it	O
is	O
often	O
the	O
case	O
that	O
the	O
signal	B
(	O
as	O
opposed	O
to	O
the	O
noise	B
)	O
in	O
a	O
data	B
set	O
is	O
concentrated	O
in	O
its	O
ﬁrst	O
few	O
principal	B
components	I
.	O
10.3	O
clustering	B
methods	O
clustering	B
refers	O
to	O
a	O
very	O
broad	O
set	B
of	O
techniques	O
for	O
ﬁnding	O
subgroups	O
,	O
or	O
clusters	O
,	O
in	O
a	O
data	B
set	O
.	O
when	O
we	O
cluster	O
the	O
observations	B
of	O
a	O
data	B
set	O
,	O
we	O
seek	O
to	O
partition	O
them	O
into	O
distinct	O
groups	O
so	O
that	O
the	O
observations	B
within	O
each	O
group	O
are	O
quite	O
similar	O
to	O
each	O
other	O
,	O
while	O
observations	B
in	O
diﬀerent	O
groups	O
are	O
quite	O
diﬀerent	O
from	O
each	O
other	O
.	O
of	O
course	O
,	O
to	O
make	O
this	O
concrete	O
,	O
we	O
must	O
deﬁne	O
what	O
it	O
means	O
for	O
two	O
or	O
more	O
observations	B
to	O
be	O
similar	O
or	O
diﬀerent	O
.	O
indeed	O
,	O
this	O
is	O
often	O
a	O
domain-speciﬁc	O
consideration	O
that	O
must	O
be	O
made	O
based	O
on	O
knowledge	O
of	O
the	O
data	B
being	O
studied	O
.	O
for	O
instance	O
,	O
suppose	O
that	O
we	O
have	O
a	O
set	B
of	O
n	O
observations	B
,	O
each	O
with	O
p	O
features	O
.	O
the	O
n	O
observations	B
could	O
correspond	O
to	O
tissue	O
samples	O
for	O
patients	O
with	O
breast	O
cancer	O
,	O
and	O
the	O
p	O
features	O
could	O
correspond	O
to	O
measurements	O
collected	O
for	O
each	O
tissue	O
sample	O
;	O
these	O
could	O
be	O
clinical	O
measurements	O
,	O
such	O
as	O
tumor	O
stage	O
or	O
grade	O
,	O
or	O
they	O
could	O
be	O
gene	O
expression	O
measurements	O
.	O
we	O
may	O
have	O
a	O
reason	O
to	O
believe	O
that	O
there	O
is	O
some	O
heterogeneity	O
among	O
the	O
n	O
tissue	O
samples	O
;	O
for	O
instance	O
,	O
perhaps	O
there	O
are	O
a	O
few	O
diﬀerent	O
un-	O
known	O
subtypes	O
of	O
breast	O
cancer	O
.	O
clustering	B
could	O
be	O
used	O
to	O
ﬁnd	O
these	O
subgroups	O
.	O
this	O
is	O
an	O
unsupervised	O
problem	O
because	O
we	O
are	O
trying	O
to	O
dis-	O
cover	O
structure—in	O
this	O
case	O
,	O
distinct	O
clusters—on	O
the	O
basis	B
of	O
a	O
data	B
set	O
.	O
the	O
goal	O
in	O
supervised	O
problems	O
,	O
on	O
the	O
other	O
hand	O
,	O
is	O
to	O
try	O
to	O
predict	O
some	O
outcome	O
vector	B
such	O
as	O
survival	O
time	O
or	O
response	B
to	O
drug	O
treatment	O
.	O
both	O
clustering	B
and	O
pca	O
seek	O
to	O
simplify	O
the	O
data	B
via	O
a	O
small	O
number	O
of	O
summaries	O
,	O
but	O
their	O
mechanisms	O
are	O
diﬀerent	O
:	O
•	O
pca	O
looks	O
to	O
ﬁnd	O
a	O
low-dimensional	B
representation	O
of	O
the	O
observa-	O
tions	O
that	O
explain	O
a	O
good	O
fraction	O
of	O
the	O
variance	B
;	O
•	O
clustering	B
looks	O
to	O
ﬁnd	O
homogeneous	O
subgroups	O
among	O
the	O
observa-	O
tions	O
.	O
another	O
application	O
of	O
clustering	B
arises	O
in	O
marketing	O
.	O
we	O
may	O
have	O
ac-	O
cess	O
to	O
a	O
large	O
number	O
of	O
measurements	O
(	O
e.g	O
.	O
median	O
household	O
income	O
,	O
occupation	O
,	O
distance	B
from	O
nearest	O
urban	O
area	O
,	O
and	O
so	O
forth	O
)	O
for	O
a	O
large	O
k-means	O
clustering	B
hierarchical	O
clustering	B
dendrogram	O
386	O
10.	O
unsupervised	B
learning	I
number	O
of	O
people	O
.	O
our	O
goal	O
is	O
to	O
perform	O
market	O
segmentation	O
by	O
identify-	O
ing	O
subgroups	O
of	O
people	O
who	O
might	O
be	O
more	O
receptive	O
to	O
a	O
particular	O
form	O
of	O
advertising	O
,	O
or	O
more	O
likely	O
to	O
purchase	O
a	O
particular	O
product	O
.	O
the	O
task	O
of	O
performing	O
market	O
segmentation	O
amounts	O
to	O
clustering	B
the	O
people	O
in	O
the	O
data	B
set	O
.	O
since	O
clustering	B
is	O
popular	O
in	O
many	O
ﬁelds	O
,	O
there	O
exist	O
a	O
great	O
number	O
of	O
clustering	B
methods	O
.	O
in	O
this	O
section	O
we	O
focus	O
on	O
perhaps	O
the	O
two	O
best-known	O
clustering	B
approaches	O
:	O
k-means	O
clustering	B
and	O
hierarchical	B
clustering	I
.	O
in	O
k-means	O
clustering	B
,	O
we	O
seek	O
to	O
partition	O
the	O
observations	B
into	O
a	O
pre-speciﬁed	O
number	O
of	O
clusters	O
.	O
on	O
the	O
other	O
hand	O
,	O
in	O
hierarchical	B
clustering	I
,	O
we	O
do	O
not	O
know	O
in	O
advance	O
how	O
many	O
clusters	O
we	O
want	O
;	O
in	O
fact	O
,	O
we	O
end	O
up	O
with	O
a	O
tree-like	O
visual	O
representation	O
of	O
the	O
observations	B
,	O
called	O
a	O
dendrogram	B
,	O
that	O
allows	O
us	O
to	O
view	O
at	O
once	O
the	O
clusterings	O
obtained	O
for	O
each	O
possible	O
number	O
of	O
clusters	O
,	O
from	O
1	O
to	O
n.	O
there	O
are	O
advantages	O
and	O
disadvantages	O
to	O
each	O
of	O
these	O
clustering	B
approaches	O
,	O
which	O
we	O
highlight	O
in	O
this	O
chapter	O
.	O
in	O
general	O
,	O
we	O
can	O
cluster	O
observations	O
on	O
the	O
basis	B
of	O
the	O
features	O
in	O
order	O
to	O
identify	O
subgroups	O
among	O
the	O
observations	B
,	O
or	O
we	O
can	O
cluster	O
fea-	O
tures	O
on	O
the	O
basis	B
of	O
the	O
observations	B
in	O
order	O
to	O
discover	O
subgroups	O
among	O
the	O
features	O
.	O
in	O
what	O
follows	O
,	O
for	O
simplicity	O
we	O
will	O
discuss	O
clustering	B
obser-	O
vations	O
on	O
the	O
basis	B
of	O
the	O
features	O
,	O
though	O
the	O
converse	O
can	O
be	O
performed	O
by	O
simply	O
transposing	O
the	O
data	B
matrix	O
.	O
10.3.1	O
k-means	O
clustering	B
k-means	O
clustering	B
is	O
a	O
simple	B
and	O
elegant	O
approach	B
for	O
partitioning	O
a	O
data	B
set	O
into	O
k	O
distinct	O
,	O
non-overlapping	O
clusters	O
.	O
to	O
perform	O
k-means	O
clustering	B
,	O
we	O
must	O
ﬁrst	O
specify	O
the	O
desired	O
number	O
of	O
clusters	O
k	O
;	O
then	O
the	O
k-means	O
algorithm	O
will	O
assign	O
each	O
observation	O
to	O
exactly	O
one	O
of	O
the	O
k	O
clusters	O
.	O
figure	O
10.5	O
shows	O
the	O
results	O
obtained	O
from	O
performing	O
k-means	O
clustering	B
on	O
a	O
simulated	O
example	O
consisting	O
of	O
150	O
observations	B
in	O
two	O
dimensions	O
,	O
using	O
three	O
diﬀerent	O
values	O
of	O
k.	O
the	O
k-means	O
clustering	B
procedure	O
results	O
from	O
a	O
simple	B
and	O
intuitive	O
mathematical	O
problem	O
.	O
we	O
begin	O
by	O
deﬁning	O
some	O
notation	O
.	O
let	O
c1	O
,	O
.	O
.	O
.	O
,	O
ck	O
denote	O
sets	O
containing	O
the	O
indices	O
of	O
the	O
observations	B
in	O
each	O
cluster	O
.	O
these	O
sets	O
satisfy	O
two	O
properties	O
:	O
1.	O
c1	O
∪	O
c2	O
∪	O
.	O
.	O
.	O
∪	O
ck	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
.	O
in	O
other	O
words	O
,	O
each	O
observation	O
belongs	O
to	O
at	O
least	O
one	O
of	O
the	O
k	O
clusters	O
.	O
2.	O
ck	O
∩	O
ck	O
(	O
cid:2	O
)	O
=	O
∅	O
for	O
all	O
k	O
(	O
cid:4	O
)	O
=	O
k	O
(	O
cid:5	O
)	O
.	O
in	O
other	O
words	O
,	O
the	O
clusters	O
are	O
non-	O
overlapping	O
:	O
no	O
observation	O
belongs	O
to	O
more	O
than	O
one	O
cluster	O
.	O
for	O
instance	O
,	O
if	O
the	O
ith	O
observation	O
is	O
in	O
the	O
kth	O
cluster	O
,	O
then	O
i	O
∈	O
ck	O
.	O
the	O
idea	O
behind	O
k-means	O
clustering	B
is	O
that	O
a	O
good	O
clustering	B
is	O
one	O
for	O
which	O
the	O
within-cluster	O
variation	O
is	O
as	O
small	O
as	O
possible	O
.	O
the	O
within-cluster	O
variation	O
k=2	O
k=3	O
k=4	O
10.3	O
clustering	B
methods	O
387	O
figure	O
10.5.	O
a	O
simulated	O
data	B
set	O
with	O
150	O
observations	B
in	O
two-dimensional	O
space	O
.	O
panels	O
show	O
the	O
results	O
of	O
applying	O
k-means	O
clustering	B
with	O
diﬀerent	O
val-	O
ues	O
of	O
k	O
,	O
the	O
number	O
of	O
clusters	O
.	O
the	O
color	O
of	O
each	O
observation	O
indicates	O
the	O
clus-	O
ter	O
to	O
which	O
it	O
was	O
assigned	O
using	O
the	O
k-means	O
clustering	B
algorithm	O
.	O
note	O
that	O
there	O
is	O
no	O
ordering	O
of	O
the	O
clusters	O
,	O
so	O
the	O
cluster	O
coloring	O
is	O
arbitrary	O
.	O
these	O
cluster	O
labels	O
were	O
not	O
used	O
in	O
clustering	B
;	O
instead	O
,	O
they	O
are	O
the	O
outputs	O
of	O
the	O
clustering	B
procedure	O
.	O
for	O
cluster	O
ck	O
is	O
a	O
measure	O
w	O
(	O
ck	O
)	O
of	O
the	O
amount	O
by	O
which	O
the	O
observations	B
within	O
a	O
cluster	O
diﬀer	O
from	O
each	O
other	O
.	O
hence	O
we	O
want	O
to	O
solve	O
the	O
problem	O
(	O
cid:30	O
)	O
k	O
(	O
cid:17	O
)	O
/	O
minimize	O
c1	O
,	O
...	O
,	O
ck	O
k=1	O
w	O
(	O
ck	O
)	O
.	O
(	O
10.9	O
)	O
in	O
words	O
,	O
this	O
formula	O
says	O
that	O
we	O
want	O
to	O
partition	O
the	O
observations	B
into	O
k	O
clusters	O
such	O
that	O
the	O
total	O
within-cluster	O
variation	O
,	O
summed	O
over	O
all	O
k	O
clusters	O
,	O
is	O
as	O
small	O
as	O
possible	O
.	O
solving	O
(	O
10.9	O
)	O
seems	O
like	O
a	O
reasonable	O
idea	O
,	O
but	O
in	O
order	O
to	O
make	O
it	O
actionable	O
we	O
need	O
to	O
deﬁne	O
the	O
within-cluster	O
variation	O
.	O
there	O
are	O
many	O
possible	O
ways	O
to	O
deﬁne	O
this	O
concept	O
,	O
but	O
by	O
far	O
the	O
most	O
common	O
choice	O
involves	O
squared	O
euclidean	O
distance	B
.	O
that	O
is	O
,	O
we	O
deﬁne	O
w	O
(	O
ck	O
)	O
=	O
(	O
cid:17	O
)	O
p	O
(	O
cid:17	O
)	O
i	O
,	O
i	O
(	O
cid:2	O
)	O
∈ck	O
j=1	O
1|ck|	O
(	O
xij	O
−	O
xi	O
(	O
cid:2	O
)	O
j	O
)	O
2	O
,	O
(	O
10.10	O
)	O
where	O
|ck|	O
denotes	O
the	O
number	O
of	O
observations	B
in	O
the	O
kth	O
cluster	O
.	O
in	O
other	O
words	O
,	O
the	O
within-cluster	O
variation	O
for	O
the	O
kth	O
cluster	O
is	O
the	O
sum	O
of	O
all	O
of	O
the	O
pairwise	O
squared	O
euclidean	O
distances	O
between	O
the	O
observations	B
in	O
the	O
kth	O
cluster	O
,	O
divided	O
by	O
the	O
total	O
number	O
of	O
observations	B
in	O
the	O
kth	O
cluster	O
.	O
combining	O
(	O
10.9	O
)	O
and	O
(	O
10.10	O
)	O
gives	O
the	O
optimization	O
problem	O
that	O
deﬁnes	O
k-means	O
clustering	B
,	O
⎧⎨	O
⎩	O
minimize	O
c1	O
,	O
...	O
,	O
ck	O
k	O
(	O
cid:17	O
)	O
k=1	O
(	O
cid:17	O
)	O
p	O
(	O
cid:17	O
)	O
i	O
,	O
i	O
(	O
cid:2	O
)	O
∈ck	O
j=1	O
1|ck|	O
(	O
xij	O
−	O
xi	O
(	O
cid:2	O
)	O
j	O
)	O
2	O
⎫⎬	O
⎭	O
.	O
(	O
10.11	O
)	O
388	O
10.	O
unsupervised	B
learning	I
now	O
,	O
we	O
would	O
like	O
to	O
ﬁnd	O
an	O
algorithm	O
to	O
solve	O
(	O
10.11	O
)	O
—that	O
is	O
,	O
a	O
method	O
to	O
partition	O
the	O
observations	B
into	O
k	O
clusters	O
such	O
that	O
the	O
objective	O
of	O
(	O
10.11	O
)	O
is	O
minimized	O
.	O
this	O
is	O
in	O
fact	O
a	O
very	O
diﬃcult	O
problem	O
to	O
solve	O
precisely	O
,	O
since	O
there	O
are	O
almost	O
k	O
n	O
ways	O
to	O
partition	O
n	O
observations	B
into	O
k	O
clusters	O
.	O
this	O
is	O
a	O
huge	O
number	O
unless	O
k	O
and	O
n	O
are	O
tiny	O
!	O
fortunately	O
,	O
a	O
very	O
simple	B
algorithm	O
can	O
be	O
shown	O
to	O
provide	O
a	O
local	B
optimum—a	O
pretty	O
good	O
solution—to	O
the	O
k-means	O
optimization	O
problem	O
(	O
10.11	O
)	O
.	O
this	O
approach	B
is	O
laid	O
out	O
in	O
algorithm	O
10.1.	O
algorithm	O
10.1	O
k-means	O
clustering	B
1.	O
randomly	O
assign	O
a	O
number	O
,	O
from	O
1	O
to	O
k	O
,	O
to	O
each	O
of	O
the	O
observations	B
.	O
these	O
serve	O
as	O
initial	O
cluster	O
assignments	O
for	O
the	O
observations	B
.	O
2.	O
iterate	O
until	O
the	O
cluster	O
assignments	O
stop	O
changing	O
:	O
(	O
a	O
)	O
for	O
each	O
of	O
the	O
k	O
clusters	O
,	O
compute	O
the	O
cluster	O
centroid	O
.	O
the	O
kth	O
cluster	O
centroid	O
is	O
the	O
vector	B
of	O
the	O
p	O
feature	B
means	O
for	O
the	O
observations	B
in	O
the	O
kth	O
cluster	O
.	O
(	O
b	O
)	O
assign	O
each	O
observation	O
to	O
the	O
cluster	O
whose	O
centroid	B
is	O
closest	O
(	O
where	O
closest	O
is	O
deﬁned	O
using	O
euclidean	O
distance	B
)	O
.	O
algorithm	O
10.1	O
is	O
guaranteed	O
to	O
decrease	O
the	O
value	O
of	O
the	O
objective	O
(	O
10.11	O
)	O
at	O
each	O
step	O
.	O
to	O
understand	O
why	O
,	O
the	O
following	O
identity	O
is	O
illu-	O
minating	O
:	O
(	O
cid:17	O
)	O
p	O
(	O
cid:17	O
)	O
1|ck|	O
i	O
,	O
i	O
(	O
cid:2	O
)	O
∈ck	O
(	O
xij	O
−	O
xi	O
(	O
cid:2	O
)	O
j	O
)	O
2	O
=	O
2	O
(	O
cid:17	O
)	O
p	O
(	O
cid:17	O
)	O
i∈ck	O
j=1	O
(	O
xij	O
−	O
¯xkj	O
)	O
2	O
,	O
(	O
10.12	O
)	O
j=1	O
(	O
cid:10	O
)	O
i∈ck	O
xij	O
where	O
¯xkj	O
=	O
1|ck|	O
is	O
the	O
mean	O
for	O
feature	B
j	O
in	O
cluster	O
ck	O
.	O
in	O
step	O
2	O
(	O
a	O
)	O
the	O
cluster	O
means	O
for	O
each	O
feature	B
are	O
the	O
constants	O
that	O
minimize	O
the	O
sum-of-squared	O
deviations	O
,	O
and	O
in	O
step	O
2	O
(	O
b	O
)	O
,	O
reallocating	O
the	O
observations	B
can	O
only	O
improve	O
(	O
10.12	O
)	O
.	O
this	O
means	O
that	O
as	O
the	O
algorithm	O
is	O
run	O
,	O
the	O
clustering	B
obtained	O
will	O
continually	O
improve	O
until	O
the	O
result	O
no	O
longer	O
changes	O
;	O
the	O
objective	O
of	O
(	O
10.11	O
)	O
will	O
never	O
increase	O
.	O
when	O
the	O
result	O
no	O
longer	O
changes	O
,	O
a	O
local	B
optimum	O
has	O
been	O
reached	O
.	O
figure	O
10.6	O
shows	O
the	O
progression	O
of	O
the	O
algorithm	O
on	O
the	O
toy	O
example	O
from	O
figure	O
10.5.	O
k-means	O
clustering	B
derives	O
its	O
name	O
from	O
the	O
fact	O
that	O
in	O
step	O
2	O
(	O
a	O
)	O
,	O
the	O
cluster	O
centroids	O
are	O
computed	O
as	O
the	O
mean	O
of	O
the	O
observations	B
assigned	O
to	O
each	O
cluster	O
.	O
because	O
the	O
k-means	O
algorithm	O
ﬁnds	O
a	O
local	B
rather	O
than	O
a	O
global	O
opti-	O
mum	O
,	O
the	O
results	O
obtained	O
will	O
depend	O
on	O
the	O
initial	O
(	O
random	O
)	O
cluster	O
as-	O
signment	O
of	O
each	O
observation	O
in	O
step	O
1	O
of	O
algorithm	O
10.1.	O
for	O
this	O
reason	O
,	O
it	O
is	O
important	O
to	O
run	O
the	O
algorithm	O
multiple	B
times	O
from	O
diﬀerent	O
random	O
data	O
step	O
1	O
iteration	O
1	O
,	O
step	O
2a	O
10.3	O
clustering	B
methods	O
389	O
iteration	O
1	O
,	O
step	O
2b	O
iteration	O
2	O
,	O
step	O
2a	O
final	O
results	O
figure	O
10.6.	O
the	O
progress	O
of	O
the	O
k-means	O
algorithm	O
on	O
the	O
example	O
of	O
fig-	O
ure	O
10.5	O
with	O
k=3	O
.	O
top	O
left	O
:	O
the	O
observations	B
are	O
shown	O
.	O
top	O
center	O
:	O
in	O
step	O
1	O
of	O
the	O
algorithm	O
,	O
each	O
observation	O
is	O
randomly	O
assigned	O
to	O
a	O
cluster	O
.	O
top	O
right	O
:	O
in	O
step	O
2	O
(	O
a	O
)	O
,	O
the	O
cluster	O
centroids	O
are	O
computed	O
.	O
these	O
are	O
shown	O
as	O
large	O
col-	O
ored	O
disks	O
.	O
initially	O
the	O
centroids	O
are	O
almost	O
completely	O
overlapping	O
because	O
the	O
initial	O
cluster	O
assignments	O
were	O
chosen	O
at	O
random	O
.	O
bottom	O
left	O
:	O
in	O
step	O
2	O
(	O
b	O
)	O
,	O
each	O
observation	O
is	O
assigned	O
to	O
the	O
nearest	O
centroid	B
.	O
bottom	O
center	O
:	O
step	O
2	O
(	O
a	O
)	O
is	O
once	O
again	O
performed	O
,	O
leading	O
to	O
new	O
cluster	O
centroids	O
.	O
bottom	O
right	O
:	O
the	O
results	O
obtained	O
after	O
ten	O
iterations	O
.	O
initial	O
conﬁgurations	O
.	O
then	O
one	O
selects	O
the	O
best	O
solution	O
,	O
i.e	O
.	O
that	O
for	O
which	O
the	O
objective	O
(	O
10.11	O
)	O
is	O
smallest	O
.	O
figure	O
10.7	O
shows	O
the	O
local	B
optima	O
ob-	O
tained	O
by	O
running	O
k-means	O
clustering	B
six	O
times	O
using	O
six	O
diﬀerent	O
initial	O
cluster	O
assignments	O
,	O
using	O
the	O
toy	O
data	B
from	O
figure	O
10.5.	O
in	O
this	O
case	O
,	O
the	O
best	O
clustering	O
is	O
the	O
one	O
with	O
an	O
objective	O
value	O
of	O
235.8.	O
as	O
we	O
have	O
seen	O
,	O
to	O
perform	O
k-means	O
clustering	B
,	O
we	O
must	O
decide	O
how	O
many	O
clusters	O
we	O
expect	O
in	O
the	O
data	B
.	O
the	O
problem	O
of	O
selecting	O
k	O
is	O
far	O
from	O
simple	B
.	O
this	O
issue	O
,	O
along	O
with	O
other	O
practical	O
considerations	O
that	O
arise	O
in	O
performing	O
k-means	O
clustering	B
,	O
is	O
addressed	O
in	O
section	O
10.3.3	O
.	O
390	O
10.	O
unsupervised	B
learning	I
320.9	O
235.8	O
235.8	O
235.8	O
235.8	O
310.9	O
figure	O
10.7.	O
k-means	O
clustering	B
performed	O
six	O
times	O
on	O
the	O
data	B
from	O
fig-	O
ure	O
10.5	O
with	O
k	O
=	O
3	O
,	O
each	O
time	O
with	O
a	O
diﬀerent	O
random	O
assignment	O
of	O
the	O
ob-	O
servations	O
in	O
step	O
1	O
of	O
the	O
k-means	O
algorithm	O
.	O
above	O
each	O
plot	B
is	O
the	O
value	O
of	O
the	O
objective	O
(	O
10.11	O
)	O
.	O
three	O
diﬀerent	O
local	B
optima	O
were	O
obtained	O
,	O
one	O
of	O
which	O
resulted	O
in	O
a	O
smaller	O
value	O
of	O
the	O
objective	O
and	O
provides	O
better	O
separation	O
between	O
the	O
clusters	O
.	O
those	O
labeled	O
in	O
red	O
all	O
achieved	O
the	O
same	O
best	O
solution	O
,	O
with	O
an	O
objective	O
value	O
of	O
235.8	O
.	O
10.3.2	O
hierarchical	B
clustering	I
one	O
potential	O
disadvantage	O
of	O
k-means	O
clustering	B
is	O
that	O
it	O
requires	O
us	O
to	O
pre-specify	O
the	O
number	O
of	O
clusters	O
k.	O
hierarchical	B
clustering	I
is	O
an	O
alter-	O
native	O
approach	B
which	O
does	O
not	O
require	O
that	O
we	O
commit	O
to	O
a	O
particular	O
choice	O
of	O
k.	O
hierarchical	B
clustering	I
has	O
an	O
added	O
advantage	O
over	O
k-means	O
clustering	B
in	O
that	O
it	O
results	O
in	O
an	O
attractive	O
tree-based	O
representation	O
of	O
the	O
observations	B
,	O
called	O
a	O
dendrogram	B
.	O
in	O
this	O
section	O
,	O
we	O
describe	O
bottom-up	B
or	O
agglomerative	B
clustering	I
.	O
this	O
is	O
the	O
most	O
common	O
type	O
of	O
hierarchical	B
clustering	I
,	O
and	O
refers	O
to	O
the	O
fact	O
that	O
a	O
dendrogram	B
(	O
generally	O
depicted	O
as	O
an	O
upside-down	O
tree	B
;	O
see	O
bottom-up	B
agglomerative	O
10.3	O
clustering	B
methods	O
391	O
4	O
2	O
x	O
2	O
0	O
2	O
−	O
−6	O
−4	O
−2	O
x1	O
0	O
2	O
figure	O
10.8.	O
forty-ﬁve	O
observations	B
generated	O
in	O
two-dimensional	O
space	O
.	O
in	O
reality	O
there	O
are	O
three	O
distinct	O
classes	O
,	O
shown	O
in	O
separate	O
colors	O
.	O
however	O
,	O
we	O
will	O
treat	O
these	O
class	O
labels	O
as	O
unknown	O
and	O
will	O
seek	O
to	O
cluster	O
the	O
observations	B
in	O
order	O
to	O
discover	O
the	O
classes	O
from	O
the	O
data	B
.	O
figure	O
10.9	O
)	O
is	O
built	O
starting	O
from	O
the	O
leaves	O
and	O
combining	O
clusters	O
up	O
to	O
the	O
trunk	O
.	O
we	O
will	O
begin	O
with	O
a	O
discussion	O
of	O
how	O
to	O
interpret	O
a	O
dendrogram	B
and	O
then	O
discuss	O
how	O
hierarchical	B
clustering	I
is	O
actually	O
performed—that	O
is	O
,	O
how	O
the	O
dendrogram	B
is	O
built	O
.	O
interpreting	O
a	O
dendrogram	B
we	O
begin	O
with	O
the	O
simulated	O
data	B
set	O
shown	O
in	O
figure	O
10.8	O
,	O
consisting	O
of	O
45	O
observations	B
in	O
two-dimensional	O
space	O
.	O
the	O
data	B
were	O
generated	O
from	O
a	O
three-class	O
model	B
;	O
the	O
true	O
class	O
labels	O
for	O
each	O
observation	O
are	O
shown	O
in	O
distinct	O
colors	O
.	O
however	O
,	O
suppose	O
that	O
the	O
data	B
were	O
observed	O
without	O
the	O
class	O
labels	O
,	O
and	O
that	O
we	O
wanted	O
to	O
perform	O
hierarchical	B
clustering	I
of	O
the	O
data	B
.	O
hierarchical	B
clustering	I
(	O
with	O
complete	B
linkage	O
,	O
to	O
be	O
discussed	O
later	O
)	O
yields	O
the	O
result	O
shown	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
10.9.	O
how	O
can	O
we	O
interpret	O
this	O
dendrogram	B
?	O
in	O
the	O
left-hand	O
panel	O
of	O
figure	O
10.9	O
,	O
each	O
leaf	B
of	O
the	O
dendrogram	B
rep-	O
resents	O
one	O
of	O
the	O
45	O
observations	B
in	O
figure	O
10.8.	O
however	O
,	O
as	O
we	O
move	O
up	O
the	O
tree	B
,	O
some	O
leaves	O
begin	O
to	O
fuse	O
into	O
branches	O
.	O
these	O
correspond	O
to	O
observations	B
that	O
are	O
similar	O
to	O
each	O
other	O
.	O
as	O
we	O
move	O
higher	O
up	O
the	O
tree	B
,	O
branches	O
themselves	O
fuse	O
,	O
either	O
with	O
leaves	O
or	O
other	O
branches	O
.	O
the	O
earlier	O
(	O
lower	O
in	O
the	O
tree	B
)	O
fusions	O
occur	O
,	O
the	O
more	O
similar	O
the	O
groups	O
of	O
observa-	O
tions	O
are	O
to	O
each	O
other	O
.	O
on	O
the	O
other	O
hand	O
,	O
observations	B
that	O
fuse	O
later	O
(	O
near	O
the	O
top	O
of	O
the	O
tree	B
)	O
can	O
be	O
quite	O
diﬀerent	O
.	O
in	O
fact	O
,	O
this	O
statement	O
can	O
be	O
made	O
precise	O
:	O
for	O
any	O
two	O
observations	B
,	O
we	O
can	O
look	O
for	O
the	O
point	O
in	O
the	O
tree	B
where	O
branches	O
containing	O
those	O
two	O
observations	B
are	O
ﬁrst	O
fused	O
.	O
the	O
height	O
of	O
this	O
fusion	O
,	O
as	O
measured	O
on	O
the	O
vertical	O
axis	O
,	O
indicates	O
how	O
392	O
10.	O
unsupervised	B
learning	I
0	O
1	O
8	O
6	O
4	O
2	O
0	O
0	O
1	O
8	O
6	O
4	O
2	O
0	O
0	O
1	O
8	O
6	O
4	O
2	O
0	O
figure	O
10.9.	O
left	O
:	O
dendrogram	B
obtained	O
from	O
hierarchically	O
clustering	B
the	O
data	B
from	O
figure	O
10.8	O
with	O
complete	B
linkage	O
and	O
euclidean	O
distance	B
.	O
center	O
:	O
the	O
den-	O
drogram	O
from	O
the	O
left-hand	O
panel	O
,	O
cut	O
at	O
a	O
height	O
of	O
nine	O
(	O
indicated	O
by	O
the	O
dashed	O
line	B
)	O
.	O
this	O
cut	O
results	O
in	O
two	O
distinct	O
clusters	O
,	O
shown	O
in	O
diﬀerent	O
colors	O
.	O
right	O
:	O
the	O
dendrogram	B
from	O
the	O
left-hand	O
panel	O
,	O
now	O
cut	O
at	O
a	O
height	O
of	O
ﬁve	O
.	O
this	O
cut	O
results	O
in	O
three	O
distinct	O
clusters	O
,	O
shown	O
in	O
diﬀerent	O
colors	O
.	O
note	O
that	O
the	O
colors	O
were	O
not	O
used	O
in	O
clustering	B
,	O
but	O
are	O
simply	O
used	O
for	O
display	O
purposes	O
in	O
this	O
ﬁgure	O
.	O
diﬀerent	O
the	O
two	O
observations	B
are	O
.	O
thus	O
,	O
observations	B
that	O
fuse	O
at	O
the	O
very	O
bottom	O
of	O
the	O
tree	B
are	O
quite	O
similar	O
to	O
each	O
other	O
,	O
whereas	O
observations	B
that	O
fuse	O
close	O
to	O
the	O
top	O
of	O
the	O
tree	B
will	O
tend	O
to	O
be	O
quite	O
diﬀerent	O
.	O
this	O
highlights	O
a	O
very	O
important	O
point	O
in	O
interpreting	O
dendrograms	O
that	O
is	O
often	O
misunderstood	O
.	O
consider	O
the	O
left-hand	O
panel	O
of	O
figure	O
10.10	O
,	O
which	O
shows	O
a	O
simple	B
dendrogram	O
obtained	O
from	O
hierarchically	O
clustering	B
nine	O
observations	B
.	O
one	O
can	O
see	O
that	O
observations	B
5	O
and	O
7	O
are	O
quite	O
similar	O
to	O
each	O
other	O
,	O
since	O
they	O
fuse	O
at	O
the	O
lowest	O
point	O
on	O
the	O
dendrogram	B
.	O
obser-	O
vations	O
1	O
and	O
6	O
are	O
also	O
quite	O
similar	O
to	O
each	O
other	O
.	O
however	O
,	O
it	O
is	O
tempting	O
but	O
incorrect	O
to	O
conclude	O
from	O
the	O
ﬁgure	O
that	O
observations	B
9	O
and	O
2	O
are	O
quite	O
similar	O
to	O
each	O
other	O
on	O
the	O
basis	B
that	O
they	O
are	O
located	O
near	O
each	O
other	O
on	O
the	O
dendrogram	B
.	O
in	O
fact	O
,	O
based	O
on	O
the	O
information	O
contained	O
in	O
the	O
dendrogram	B
,	O
observation	O
9	O
is	O
no	O
more	O
similar	O
to	O
observation	O
2	O
than	O
it	O
is	O
to	O
observations	B
8	O
,	O
5	O
,	O
and	O
7	O
.	O
(	O
this	O
can	O
be	O
seen	O
from	O
the	O
right-hand	O
panel	O
of	O
figure	O
10.10	O
,	O
in	O
which	O
the	O
raw	O
data	B
are	O
displayed	O
.	O
)	O
to	O
put	O
it	O
mathe-	O
matically	O
,	O
there	O
are	O
2n−1	O
possible	O
reorderings	O
of	O
the	O
dendrogram	B
,	O
where	O
n	O
is	O
the	O
number	O
of	O
leaves	O
.	O
this	O
is	O
because	O
at	O
each	O
of	O
the	O
n	O
−	O
1	O
points	O
where	O
fusions	O
occur	O
,	O
the	O
positions	O
of	O
the	O
two	O
fused	O
branches	O
could	O
be	O
swapped	O
without	O
aﬀecting	O
the	O
meaning	O
of	O
the	O
dendrogram	B
.	O
therefore	O
,	O
we	O
can	O
not	O
draw	O
conclusions	O
about	O
the	O
similarity	O
of	O
two	O
observations	B
based	O
on	O
their	O
proximity	O
along	O
the	O
horizontal	O
axis	O
.	O
rather	O
,	O
we	O
draw	O
conclusions	O
about	O
the	O
similarity	O
of	O
two	O
observations	B
based	O
on	O
the	O
location	O
on	O
the	O
vertical	O
axis	O
where	O
branches	O
containing	O
those	O
two	O
observations	B
ﬁrst	O
are	O
fused	O
.	O
0	O
.	O
3	O
5	O
2	O
.	O
0	O
2	O
.	O
5	O
.	O
1	O
.	O
0	O
1	O
5	O
.	O
0	O
0	O
.	O
0	O
3	O
4	O
9	O
2	O
8	O
1	O
6	O
5	O
7	O
10.3	O
clustering	B
methods	O
393	O
2	O
x	O
5	O
.	O
0	O
.	O
0	O
0	O
5	O
.	O
0	O
−	O
.	O
0	O
1	O
−	O
5	O
.	O
1	O
−	O
3	O
4	O
1	O
6	O
−1.5	O
−1.0	O
−0.5	O
9	O
2	O
0.0	O
x1	O
7	O
5	O
8	O
0.5	O
1.0	O
figure	O
10.10.	O
an	O
illustration	O
of	O
how	O
to	O
properly	O
interpret	O
a	O
dendrogram	B
with	O
nine	O
observations	B
in	O
two-dimensional	O
space	O
.	O
left	O
:	O
a	O
dendrogram	B
generated	O
using	O
euclidean	O
distance	B
and	O
complete	B
linkage	O
.	O
observations	B
5	O
and	O
7	O
are	O
quite	O
similar	O
to	O
each	O
other	O
,	O
as	O
are	O
observations	B
1	O
and	O
6.	O
however	O
,	O
observation	O
9	O
is	O
no	O
more	O
similar	O
to	O
observation	O
2	O
than	O
it	O
is	O
to	O
observations	B
8	O
,	O
5	O
,	O
and	O
7	O
,	O
even	O
though	O
obser-	O
vations	O
9	O
and	O
2	O
are	O
close	O
together	O
in	O
terms	O
of	O
horizontal	O
distance	B
.	O
this	O
is	O
because	O
observations	B
2	O
,	O
8	O
,	O
5	O
,	O
and	O
7	O
all	O
fuse	O
with	O
observation	O
9	O
at	O
the	O
same	O
height	O
,	O
approx-	O
imately	O
1.8.	O
right	O
:	O
the	O
raw	O
data	B
used	O
to	O
generate	O
the	O
dendrogram	B
can	O
be	O
used	O
to	O
conﬁrm	O
that	O
indeed	O
,	O
observation	O
9	O
is	O
no	O
more	O
similar	O
to	O
observation	O
2	O
than	O
it	O
is	O
to	O
observations	B
8	O
,	O
5	O
,	O
and	O
7.	O
now	O
that	O
we	O
understand	O
how	O
to	O
interpret	O
the	O
left-hand	O
panel	O
of	O
fig-	O
ure	O
10.9	O
,	O
we	O
can	O
move	O
on	O
to	O
the	O
issue	O
of	O
identifying	O
clusters	O
on	O
the	O
basis	B
of	O
a	O
dendrogram	B
.	O
in	O
order	O
to	O
do	O
this	O
,	O
we	O
make	O
a	O
horizontal	O
cut	O
across	O
the	O
dendrogram	B
,	O
as	O
shown	O
in	O
the	O
center	O
and	O
right-hand	O
panels	O
of	O
figure	O
10.9.	O
the	O
distinct	O
sets	O
of	O
observations	B
beneath	O
the	O
cut	O
can	O
be	O
interpreted	O
as	O
clus-	O
ters	O
.	O
in	O
the	O
center	O
panel	O
of	O
figure	O
10.9	O
,	O
cutting	O
the	O
dendrogram	B
at	O
a	O
height	O
of	O
nine	O
results	O
in	O
two	O
clusters	O
,	O
shown	O
in	O
distinct	O
colors	O
.	O
in	O
the	O
right-hand	O
panel	O
,	O
cutting	O
the	O
dendrogram	B
at	O
a	O
height	O
of	O
ﬁve	O
results	O
in	O
three	O
clusters	O
.	O
further	O
cuts	O
can	O
be	O
made	O
as	O
one	O
descends	O
the	O
dendrogram	B
in	O
order	O
to	O
ob-	O
tain	O
any	O
number	O
of	O
clusters	O
,	O
between	O
1	O
(	O
corresponding	O
to	O
no	O
cut	O
)	O
and	O
n	O
(	O
corresponding	O
to	O
a	O
cut	O
at	O
height	O
0	O
,	O
so	O
that	O
each	O
observation	O
is	O
in	O
its	O
own	O
cluster	O
)	O
.	O
in	O
other	O
words	O
,	O
the	O
height	O
of	O
the	O
cut	O
to	O
the	O
dendrogram	B
serves	O
the	O
same	O
role	O
as	O
the	O
k	O
in	O
k-means	O
clustering	B
:	O
it	O
controls	O
the	O
number	O
of	O
clusters	O
obtained	O
.	O
figure	O
10.9	O
therefore	O
highlights	O
a	O
very	O
attractive	O
aspect	O
of	O
hierarchical	B
clustering	I
:	O
one	O
single	B
dendrogram	O
can	O
be	O
used	O
to	O
obtain	O
any	O
number	O
of	O
clusters	O
.	O
in	O
practice	O
,	O
people	O
often	O
look	O
at	O
the	O
dendrogram	B
and	O
select	O
by	O
eye	O
a	O
sensible	O
number	O
of	O
clusters	O
,	O
based	O
on	O
the	O
heights	O
of	O
the	O
fusion	O
and	O
the	O
number	O
of	O
clusters	O
desired	O
.	O
in	O
the	O
case	O
of	O
figure	O
10.9	O
,	O
one	O
might	O
choose	O
to	O
select	O
either	O
two	O
or	O
three	O
clusters	O
.	O
however	O
,	O
often	O
the	O
choice	O
of	O
where	O
to	O
cut	O
the	O
dendrogram	B
is	O
not	O
so	O
clear	O
.	O
394	O
10.	O
unsupervised	B
learning	I
the	O
term	B
hierarchical	O
refers	O
to	O
the	O
fact	O
that	O
clusters	O
obtained	O
by	O
cutting	O
the	O
dendrogram	B
at	O
a	O
given	O
height	O
are	O
necessarily	O
nested	O
within	O
the	O
clusters	O
obtained	O
by	O
cutting	O
the	O
dendrogram	B
at	O
any	O
greater	O
height	O
.	O
however	O
,	O
on	O
an	O
arbitrary	O
data	B
set	O
,	O
this	O
assumption	O
of	O
hierarchical	B
structure	O
might	O
be	O
unrealistic	O
.	O
for	O
instance	O
,	O
suppose	O
that	O
our	O
observations	B
correspond	O
to	O
a	O
group	O
of	O
people	O
with	O
a	O
50–50	O
split	O
of	O
males	O
and	O
females	O
,	O
evenly	O
split	O
among	O
americans	O
,	O
japanese	O
,	O
and	O
french	O
.	O
we	O
can	O
imagine	O
a	O
scenario	O
in	O
which	O
the	O
best	O
division	O
into	O
two	O
groups	O
might	O
split	O
these	O
people	O
by	O
gender	O
,	O
and	O
the	O
best	O
division	O
into	O
three	O
groups	O
might	O
split	O
them	O
by	O
nationality	O
.	O
in	O
this	O
case	O
,	O
the	O
true	O
clusters	O
are	O
not	O
nested	O
,	O
in	O
the	O
sense	O
that	O
the	O
best	O
division	O
into	O
three	O
groups	O
does	O
not	O
result	O
from	O
taking	O
the	O
best	O
division	O
into	O
two	O
groups	O
and	O
splitting	O
up	O
one	O
of	O
those	O
groups	O
.	O
consequently	O
,	O
this	O
situation	O
could	O
not	O
be	O
well-represented	O
by	O
hierarchical	B
clustering	I
.	O
due	O
to	O
situations	O
such	O
as	O
this	O
one	O
,	O
hierarchical	B
clustering	I
can	O
sometimes	O
yield	O
worse	O
(	O
i.e	O
.	O
less	O
accurate	O
)	O
results	O
than	O
k-means	O
clustering	B
for	O
a	O
given	O
number	O
of	O
clusters	O
.	O
the	O
hierarchical	B
clustering	I
algorithm	O
the	O
hierarchical	B
clustering	I
dendrogram	O
is	O
obtained	O
via	O
an	O
extremely	O
simple	B
algorithm	O
.	O
we	O
begin	O
by	O
deﬁning	O
some	O
sort	O
of	O
dissimilarity	B
measure	O
between	O
each	O
pair	O
of	O
observations	B
.	O
most	O
often	O
,	O
euclidean	O
distance	B
is	O
used	O
;	O
we	O
will	O
discuss	O
the	O
choice	O
of	O
dissimilarity	B
measure	O
later	O
in	O
this	O
chapter	O
.	O
the	O
algo-	O
rithm	O
proceeds	O
iteratively	O
.	O
starting	O
out	O
at	O
the	O
bottom	O
of	O
the	O
dendrogram	B
,	O
each	O
of	O
the	O
n	O
observations	B
is	O
treated	O
as	O
its	O
own	O
cluster	O
.	O
the	O
two	O
clusters	O
that	O
are	O
most	O
similar	O
to	O
each	O
other	O
are	O
then	O
fused	O
so	O
that	O
there	O
now	O
are	O
n−	O
1	O
clusters	O
.	O
next	O
the	O
two	O
clusters	O
that	O
are	O
most	O
similar	O
to	O
each	O
other	O
are	O
fused	O
again	O
,	O
so	O
that	O
there	O
now	O
are	O
n	O
−	O
2	O
clusters	O
.	O
the	O
algorithm	O
proceeds	O
in	O
this	O
fashion	O
until	O
all	O
of	O
the	O
observations	B
belong	O
to	O
one	O
single	B
cluster	O
,	O
and	O
the	O
dendrogram	B
is	O
complete	B
.	O
figure	O
10.11	O
depicts	O
the	O
ﬁrst	O
few	O
steps	O
of	O
the	O
algorithm	O
,	O
for	O
the	O
data	B
from	O
figure	O
10.9.	O
to	O
summarize	O
,	O
the	O
hierarchical	B
clustering	I
algorithm	O
is	O
given	O
in	O
algorithm	O
10.2.	O
this	O
algorithm	O
seems	O
simple	B
enough	O
,	O
but	O
one	O
issue	O
has	O
not	O
been	O
ad-	O
dressed	O
.	O
consider	O
the	O
bottom	O
right	O
panel	O
in	O
figure	O
10.11.	O
how	O
did	O
we	O
determine	O
that	O
the	O
cluster	O
{	O
5	O
,	O
7	O
}	O
should	O
be	O
fused	O
with	O
the	O
cluster	O
{	O
8	O
}	O
?	O
we	O
have	O
a	O
concept	O
of	O
the	O
dissimilarity	B
between	O
pairs	O
of	O
observations	B
,	O
but	O
how	O
do	O
we	O
deﬁne	O
the	O
dissimilarity	B
between	O
two	O
clusters	O
if	O
one	O
or	O
both	O
of	O
the	O
clusters	O
contains	O
multiple	B
observations	O
?	O
the	O
concept	O
of	O
dissimilarity	B
between	O
a	O
pair	O
of	O
observations	B
needs	O
to	O
be	O
extended	O
to	O
a	O
pair	O
of	O
groups	O
of	O
observations	B
.	O
this	O
extension	O
is	O
achieved	O
by	O
developing	O
the	O
notion	O
of	O
linkage	B
,	O
which	O
deﬁnes	O
the	O
dissimilarity	B
between	O
two	O
groups	O
of	O
observa-	O
tions	O
.	O
the	O
four	O
most	O
common	O
types	O
of	O
linkage—complete	O
,	O
average	B
,	O
single	B
,	O
and	O
centroid	B
—are	O
brieﬂy	O
described	O
in	O
table	O
10.2.	O
average	B
,	O
complete	B
,	O
and	O
single	B
linkage	O
are	O
most	O
popular	O
among	O
statisticians	O
.	O
average	B
and	O
complete	B
linkage	O
algorithm	O
10.2	O
hierarchical	B
clustering	I
10.3	O
clustering	B
methods	O
395	O
1.	O
begin	O
with	O
n	O
observations	B
and	O
a	O
measure	O
(	O
such	O
as	O
euclidean	O
dis-	O
=	O
n	O
(	O
n−	O
1	O
)	O
/2	O
pairwise	O
dissimilarities	O
.	O
treat	O
each	O
(	O
cid:8	O
)	O
(	O
cid:9	O
)	O
tance	O
)	O
of	O
all	O
the	O
observation	O
as	O
its	O
own	O
cluster	O
.	O
n	O
2	O
2.	O
for	O
i	O
=	O
n	O
,	O
n	O
−	O
1	O
,	O
.	O
.	O
.	O
,	O
2	O
:	O
(	O
a	O
)	O
examine	O
all	O
pairwise	O
inter-cluster	O
dissimilarities	O
among	O
the	O
i	O
clusters	O
and	O
identify	O
the	O
pair	O
of	O
clusters	O
that	O
are	O
least	O
dissimilar	O
(	O
that	O
is	O
,	O
most	O
similar	O
)	O
.	O
fuse	O
these	O
two	O
clusters	O
.	O
the	O
dissimilarity	B
between	O
these	O
two	O
clusters	O
indicates	O
the	O
height	O
in	O
the	O
dendro-	O
gram	O
at	O
which	O
the	O
fusion	O
should	O
be	O
placed	O
.	O
the	O
i	O
−	O
1	O
remaining	O
clusters	O
.	O
(	O
b	O
)	O
compute	O
the	O
new	O
pairwise	O
inter-cluster	O
dissimilarities	O
among	O
linkage	B
complete	O
single	B
average	O
centroid	B
description	O
maximal	O
intercluster	O
dissimilarity	B
.	O
compute	O
all	O
pairwise	O
dis-	O
similarities	O
between	O
the	O
observations	B
in	O
cluster	O
a	O
and	O
the	O
observations	B
in	O
cluster	O
b	O
,	O
and	O
record	O
the	O
largest	O
of	O
these	O
dissimilarities	O
.	O
minimal	O
intercluster	O
dissimilarity	B
.	O
compute	O
all	O
pairwise	O
dis-	O
similarities	O
between	O
the	O
observations	B
in	O
cluster	O
a	O
and	O
the	O
observations	B
in	O
cluster	O
b	O
,	O
and	O
record	O
the	O
smallest	O
of	O
these	O
dissimilarities	O
.	O
single	B
linkage	O
can	O
result	O
in	O
extended	O
,	O
trailing	O
clusters	O
in	O
which	O
single	B
observations	O
are	O
fused	O
one-at-a-time	O
.	O
mean	O
intercluster	O
dissimilarity	B
.	O
compute	O
all	O
pairwise	O
dis-	O
similarities	O
between	O
the	O
observations	B
in	O
cluster	O
a	O
and	O
the	O
observations	B
in	O
cluster	O
b	O
,	O
and	O
record	O
the	O
average	B
of	O
these	O
dissimilarities	O
.	O
dissimilarity	B
between	O
the	O
centroid	B
for	O
cluster	O
a	O
(	O
a	O
mean	O
vector	O
of	O
length	O
p	O
)	O
and	O
the	O
centroid	B
for	O
cluster	O
b.	O
centroid	B
linkage	O
can	O
result	O
in	O
undesirable	O
inversions	O
.	O
table	O
10.2.	O
a	O
summary	O
of	O
the	O
four	O
most	O
commonly-used	O
types	O
of	O
linkage	B
in	O
hierarchical	B
clustering	I
.	O
linkage	B
are	O
generally	O
preferred	O
over	O
single	B
linkage	O
,	O
as	O
they	O
tend	O
to	O
yield	O
more	O
balanced	O
dendrograms	O
.	O
centroid	B
linkage	O
is	O
often	O
used	O
in	O
genomics	O
,	O
but	O
suﬀers	O
from	O
a	O
major	O
drawback	O
in	O
that	O
an	O
inversion	B
can	O
occur	O
,	O
whereby	O
two	O
clusters	O
are	O
fused	O
at	O
a	O
height	O
below	O
either	O
of	O
the	O
individual	O
clusters	O
in	O
the	O
dendrogram	B
.	O
this	O
can	O
lead	O
to	O
diﬃculties	O
in	O
visualization	O
as	O
well	O
as	O
in	O
in-	O
terpretation	O
of	O
the	O
dendrogram	B
.	O
the	O
dissimilarities	O
computed	O
in	O
step	O
2	O
(	O
b	O
)	O
of	O
the	O
hierarchical	B
clustering	I
algorithm	O
will	O
depend	O
on	O
the	O
type	O
of	O
linkage	B
used	O
,	O
as	O
well	O
as	O
on	O
the	O
choice	O
of	O
dissimilarity	B
measure	O
.	O
hence	O
,	O
the	O
resulting	O
inversion	B
396	O
10.	O
unsupervised	B
learning	I
2	O
x	O
2	O
x	O
5	O
0	O
.	O
0	O
.	O
0	O
5	O
.	O
0	O
−	O
0	O
.	O
1	O
−	O
5	O
.	O
1	O
−	O
5	O
0	O
.	O
0	O
.	O
0	O
5	O
.	O
0	O
−	O
0	O
.	O
1	O
−	O
5	O
.	O
1	O
−	O
3	O
4	O
1	O
6	O
−1.5	O
−1.0	O
−0.5	O
3	O
4	O
1	O
6	O
−1.5	O
−1.0	O
−0.5	O
9	O
2	O
0.0	O
x1	O
9	O
2	O
0.0	O
x1	O
7	O
8	O
5	O
2	O
x	O
5	O
0	O
.	O
0	O
.	O
0	O
5	O
.	O
0	O
−	O
0	O
.	O
1	O
−	O
5	O
.	O
1	O
−	O
3	O
4	O
1	O
6	O
0.5	O
1.0	O
−1.5	O
−1.0	O
−0.5	O
7	O
8	O
5	O
2	O
x	O
5	O
.	O
0	O
0	O
.	O
0	O
5	O
.	O
0	O
−	O
0	O
.	O
1	O
−	O
5	O
.	O
1	O
−	O
3	O
4	O
1	O
6	O
0.5	O
1.0	O
−1.5	O
−1.0	O
−0.5	O
9	O
2	O
0.0	O
x1	O
9	O
2	O
0.0	O
x1	O
7	O
5	O
8	O
0.5	O
1.0	O
7	O
5	O
8	O
0.5	O
1.0	O
figure	O
10.11.	O
an	O
illustration	O
of	O
the	O
ﬁrst	O
few	O
steps	O
of	O
the	O
hierarchical	B
clustering	I
algorithm	O
,	O
using	O
the	O
data	B
from	O
figure	O
10.10	O
,	O
with	O
complete	B
linkage	O
and	O
euclidean	O
distance	B
.	O
top	O
left	O
:	O
initially	O
,	O
there	O
are	O
nine	O
distinct	O
clusters	O
,	O
{	O
1	O
}	O
,	O
{	O
2	O
}	O
,	O
.	O
.	O
.	O
,	O
{	O
9	O
}	O
.	O
top	O
right	O
:	O
the	O
two	O
clusters	O
that	O
are	O
closest	O
together	O
,	O
{	O
5	O
}	O
and	O
{	O
7	O
}	O
,	O
are	O
fused	O
into	O
a	O
single	B
cluster	O
.	O
bottom	O
left	O
:	O
the	O
two	O
clusters	O
that	O
are	O
closest	O
together	O
,	O
{	O
6	O
}	O
and	O
{	O
1	O
}	O
,	O
are	O
fused	O
into	O
a	O
single	B
cluster	O
.	O
bottom	O
right	O
:	O
the	O
two	O
clus-	O
ters	O
that	O
are	O
closest	O
together	O
using	O
complete	B
linkage	O
,	O
{	O
8	O
}	O
and	O
the	O
cluster	O
{	O
5	O
,	O
7	O
}	O
,	O
are	O
fused	O
into	O
a	O
single	B
cluster	O
.	O
dendrogram	B
typically	O
depends	O
quite	O
strongly	O
on	O
the	O
type	O
of	O
linkage	B
used	O
,	O
as	O
is	O
shown	O
in	O
figure	O
10.12.	O
choice	O
of	O
dissimilarity	B
measure	O
thus	O
far	O
,	O
the	O
examples	O
in	O
this	O
chapter	O
have	O
used	O
euclidean	O
distance	B
as	O
the	O
dissimilarity	B
measure	O
.	O
but	O
sometimes	O
other	O
dissimilarity	B
measures	O
might	O
be	O
preferred	O
.	O
for	O
example	O
,	O
correlation-based	B
distance	O
considers	O
two	O
obser-	O
vations	O
to	O
be	O
similar	O
if	O
their	O
features	O
are	O
highly	O
correlated	O
,	O
even	O
though	O
the	O
observed	O
values	O
may	O
be	O
far	O
apart	O
in	O
terms	O
of	O
euclidean	O
distance	B
.	O
this	O
is	O
average	B
linkage	O
complete	B
linkage	O
single	B
linkage	O
10.3	O
clustering	B
methods	O
397	O
figure	O
10.12.	O
average	B
,	O
complete	B
,	O
and	O
single	B
linkage	O
applied	O
to	O
an	O
example	O
data	B
set	O
.	O
average	B
and	O
complete	B
linkage	O
tend	O
to	O
yield	O
more	O
balanced	O
clusters	O
.	O
an	O
unusual	O
use	O
of	O
correlation	B
,	O
which	O
is	O
normally	O
computed	O
between	O
vari-	O
ables	O
;	O
here	O
it	O
is	O
computed	O
between	O
the	O
observation	O
proﬁles	O
for	O
each	O
pair	O
of	O
observations	B
.	O
figure	O
10.13	O
illustrates	O
the	O
diﬀerence	O
between	O
euclidean	O
and	O
correlation-based	B
distance	O
.	O
correlation-based	B
distance	O
focuses	O
on	O
the	O
shapes	O
of	O
observation	O
proﬁles	O
rather	O
than	O
their	O
magnitudes	O
.	O
the	O
choice	O
of	O
dissimilarity	B
measure	O
is	O
very	O
important	O
,	O
as	O
it	O
has	O
a	O
strong	O
eﬀect	O
on	O
the	O
resulting	O
dendrogram	B
.	O
in	O
general	O
,	O
careful	O
attention	O
should	O
be	O
paid	O
to	O
the	O
type	O
of	O
data	B
being	O
clustered	O
and	O
the	O
scientiﬁc	O
question	O
at	O
hand	O
.	O
these	O
considerations	O
should	O
determine	O
what	O
type	O
of	O
dissimilarity	B
measure	O
is	O
used	O
for	O
hierarchical	O
clustering	B
.	O
for	O
instance	O
,	O
consider	O
an	O
online	O
retailer	O
interested	O
in	O
clustering	B
shoppers	O
based	O
on	O
their	O
past	O
shopping	O
histories	O
.	O
the	O
goal	O
is	O
to	O
identify	O
subgroups	O
of	O
similar	O
shoppers	O
,	O
so	O
that	O
shoppers	O
within	O
each	O
subgroup	O
can	O
be	O
shown	O
items	O
and	O
advertisements	O
that	O
are	O
particularly	O
likely	O
to	O
interest	O
them	O
.	O
sup-	O
pose	O
the	O
data	B
takes	O
the	O
form	O
of	O
a	O
matrix	O
where	O
the	O
rows	O
are	O
the	O
shoppers	O
and	O
the	O
columns	O
are	O
the	O
items	O
available	O
for	O
purchase	O
;	O
the	O
elements	O
of	O
the	O
data	B
matrix	O
indicate	O
the	O
number	O
of	O
times	O
a	O
given	O
shopper	O
has	O
purchased	O
a	O
given	O
item	O
(	O
i.e	O
.	O
a	O
0	O
if	O
the	O
shopper	O
has	O
never	O
purchased	O
this	O
item	O
,	O
a	O
1	O
if	O
the	O
shopper	O
has	O
purchased	O
it	O
once	O
,	O
etc	O
.	O
)	O
what	O
type	O
of	O
dissimilarity	B
measure	O
should	O
be	O
used	O
to	O
cluster	O
the	O
shoppers	O
?	O
if	O
euclidean	O
distance	B
is	O
used	O
,	O
then	O
shoppers	O
who	O
have	O
bought	O
very	O
few	O
items	O
overall	O
(	O
i.e	O
.	O
infrequent	O
users	O
of	O
the	O
online	O
shopping	O
site	O
)	O
will	O
be	O
clustered	O
together	O
.	O
this	O
may	O
not	O
be	O
desir-	O
able	O
.	O
on	O
the	O
other	O
hand	O
,	O
if	O
correlation-based	B
distance	O
is	O
used	O
,	O
then	O
shoppers	O
with	O
similar	O
preferences	O
(	O
e.g	O
.	O
shoppers	O
who	O
have	O
bought	O
items	O
a	O
and	O
b	O
but	O
398	O
10.	O
unsupervised	B
learning	I
0	O
2	O
5	O
1	O
0	O
1	O
5	O
0	O
observation	O
1	O
observation	O
2	O
observation	O
3	O
2	O
3	O
1	O
5	O
10	O
15	O
20	O
variable	B
index	O
figure	O
10.13.	O
three	O
observations	B
with	O
measurements	O
on	O
20	O
variables	O
are	O
shown	O
.	O
observations	B
1	O
and	O
3	O
have	O
similar	O
values	O
for	O
each	O
variable	B
and	O
so	O
there	O
is	O
a	O
small	O
euclidean	O
distance	B
between	O
them	O
.	O
but	O
they	O
are	O
very	O
weakly	O
correlated	O
,	O
so	O
they	O
have	O
a	O
large	O
correlation-based	B
distance	O
.	O
on	O
the	O
other	O
hand	O
,	O
observations	B
1	O
and	O
2	O
have	O
quite	O
diﬀerent	O
values	O
for	O
each	O
variable	B
,	O
and	O
so	O
there	O
is	O
a	O
large	O
euclidean	O
distance	B
between	O
them	O
.	O
but	O
they	O
are	O
highly	O
correlated	O
,	O
so	O
there	O
is	O
a	O
small	O
correlation-based	B
distance	O
between	O
them	O
.	O
never	O
items	O
c	O
or	O
d	O
)	O
will	O
be	O
clustered	O
together	O
,	O
even	O
if	O
some	O
shoppers	O
with	O
these	O
preferences	O
are	O
higher-volume	O
shoppers	O
than	O
others	O
.	O
therefore	O
,	O
for	O
this	O
application	O
,	O
correlation-based	B
distance	O
may	O
be	O
a	O
better	O
choice	O
.	O
in	O
addition	O
to	O
carefully	O
selecting	O
the	O
dissimilarity	B
measure	O
used	O
,	O
one	O
must	O
also	O
consider	O
whether	O
or	O
not	O
the	O
variables	O
should	O
be	O
scaled	O
to	O
have	O
stan-	O
dard	O
deviation	O
one	O
before	O
the	O
dissimilarity	B
between	O
the	O
observations	B
is	O
computed	O
.	O
to	O
illustrate	O
this	O
point	O
,	O
we	O
continue	O
with	O
the	O
online	O
shopping	O
example	O
just	O
described	O
.	O
some	O
items	O
may	O
be	O
purchased	O
more	O
frequently	O
than	O
others	O
;	O
for	O
instance	O
,	O
a	O
shopper	O
might	O
buy	O
ten	O
pairs	O
of	O
socks	O
a	O
year	O
,	O
but	O
a	O
computer	O
very	O
rarely	O
.	O
high-frequency	O
purchases	O
like	O
socks	O
therefore	O
tend	O
to	O
have	O
a	O
much	O
larger	O
eﬀect	O
on	O
the	O
inter-shopper	O
dissimilarities	O
,	O
and	O
hence	O
on	O
the	O
clustering	B
ultimately	O
obtained	O
,	O
than	O
rare	O
purchases	O
like	O
computers	O
.	O
this	O
may	O
not	O
be	O
desirable	O
.	O
if	O
the	O
variables	O
are	O
scaled	O
to	O
have	O
standard	O
de-	O
viation	O
one	O
before	O
the	O
inter-observation	O
dissimilarities	O
are	O
computed	O
,	O
then	O
each	O
variable	B
will	O
in	O
eﬀect	O
be	O
given	O
equal	O
importance	B
in	O
the	O
hierarchical	B
clustering	I
performed	O
.	O
we	O
might	O
also	O
want	O
to	O
scale	O
the	O
variables	O
to	O
have	O
standard	O
deviation	O
one	O
if	O
they	O
are	O
measured	O
on	O
diﬀerent	O
scales	O
;	O
otherwise	O
,	O
the	O
choice	O
of	O
units	O
(	O
e.g	O
.	O
centimeters	O
versus	O
kilometers	O
)	O
for	O
a	O
particular	O
vari-	O
able	O
will	O
greatly	O
aﬀect	O
the	O
dissimilarity	B
measure	O
obtained	O
.	O
it	O
should	O
come	O
as	O
no	O
surprise	O
that	O
whether	O
or	O
not	O
it	O
is	O
a	O
good	O
decision	O
to	O
scale	O
the	O
variables	O
before	O
computing	O
the	O
dissimilarity	B
measure	O
depends	O
on	O
the	O
application	O
at	O
hand	O
.	O
an	O
example	O
is	O
shown	O
in	O
figure	O
10.14.	O
we	O
note	O
that	O
the	O
issue	O
of	O
whether	O
or	O
not	O
to	O
scale	O
the	O
variables	O
before	O
performing	O
clustering	B
applies	O
to	O
k-means	O
clustering	B
as	O
well	O
.	O
10.3	O
clustering	B
methods	O
399	O
0	O
1	O
8	O
6	O
4	O
2	O
0	O
2	O
1	O
.	O
0	O
.	O
1	O
8	O
.	O
0	O
6	O
.	O
0	O
4	O
.	O
0	O
2	O
.	O
0	O
0	O
.	O
0	O
0	O
0	O
5	O
1	O
0	O
0	O
0	O
1	O
0	O
0	O
5	O
0	O
socks	O
computers	O
socks	O
computers	O
socks	O
computers	O
figure	O
10.14.	O
an	O
eclectic	O
online	O
retailer	O
sells	O
two	O
items	O
:	O
socks	O
and	O
computers	O
.	O
left	O
:	O
the	O
number	O
of	O
pairs	O
of	O
socks	O
,	O
and	O
computers	O
,	O
purchased	O
by	O
eight	O
online	O
shop-	O
pers	O
is	O
displayed	O
.	O
each	O
shopper	O
is	O
shown	O
in	O
a	O
diﬀerent	O
color	O
.	O
if	O
inter-observation	O
dissimilarities	O
are	O
computed	O
using	O
euclidean	O
distance	B
on	O
the	O
raw	O
variables	O
,	O
then	O
the	O
number	O
of	O
socks	O
purchased	O
by	O
an	O
individual	O
will	O
drive	O
the	O
dissimilarities	O
ob-	O
tained	O
,	O
and	O
the	O
number	O
of	O
computers	O
purchased	O
will	O
have	O
little	O
eﬀect	O
.	O
this	O
might	O
be	O
undesirable	O
,	O
since	O
(	O
1	O
)	O
computers	O
are	O
more	O
expensive	O
than	O
socks	O
and	O
so	O
the	O
online	O
retailer	O
may	O
be	O
more	O
interested	O
in	O
encouraging	O
shoppers	O
to	O
buy	O
computers	O
than	O
socks	O
,	O
and	O
(	O
2	O
)	O
a	O
large	O
diﬀerence	O
in	O
the	O
number	O
of	O
socks	O
purchased	O
by	O
two	O
shoppers	O
may	O
be	O
less	O
informative	O
about	O
the	O
shoppers	O
’	O
overall	O
shopping	O
preferences	O
than	O
a	O
small	O
diﬀerence	O
in	O
the	O
number	O
of	O
computers	O
purchased	O
.	O
center	O
:	O
the	O
same	O
data	B
is	O
shown	O
,	O
after	O
scaling	O
each	O
variable	B
by	O
its	O
standard	O
deviation	O
.	O
now	O
the	O
number	O
of	O
computers	O
purchased	O
will	O
have	O
a	O
much	O
greater	O
eﬀect	O
on	O
the	O
inter-observation	O
dissimilarities	O
obtained	O
.	O
right	O
:	O
the	O
same	O
data	B
are	O
displayed	O
,	O
but	O
now	O
the	O
y-axis	O
represents	O
the	O
number	O
of	O
dollars	O
spent	O
by	O
each	O
online	O
shopper	O
on	O
socks	O
and	O
on	O
computers	O
.	O
since	O
computers	O
are	O
much	O
more	O
expensive	O
than	O
socks	O
,	O
now	O
computer	O
purchase	O
history	O
will	O
drive	O
the	O
inter-observation	O
dissimilarities	O
obtained	O
.	O
10.3.3	O
practical	O
issues	O
in	O
clustering	B
clustering	O
can	O
be	O
a	O
very	O
useful	O
tool	O
for	O
data	O
analysis	B
in	O
the	O
unsupervised	O
setting	O
.	O
however	O
,	O
there	O
are	O
a	O
number	O
of	O
issues	O
that	O
arise	O
in	O
performing	O
clustering	B
.	O
we	O
describe	O
some	O
of	O
these	O
issues	O
here	O
.	O
small	O
decisions	O
with	O
big	O
consequences	O
in	O
order	O
to	O
perform	O
clustering	B
,	O
some	O
decisions	O
must	O
be	O
made	O
.	O
•	O
should	O
the	O
observations	B
or	O
features	O
ﬁrst	O
be	O
standardized	O
in	O
some	O
way	O
?	O
for	O
instance	O
,	O
maybe	O
the	O
variables	O
should	O
be	O
centered	O
to	O
have	O
mean	O
zero	O
and	O
scaled	O
to	O
have	O
standard	O
deviation	O
one	O
.	O
400	O
10.	O
unsupervised	B
learning	I
•	O
in	O
the	O
case	O
of	O
hierarchical	B
clustering	I
,	O
–	O
what	O
dissimilarity	B
measure	O
should	O
be	O
used	O
?	O
–	O
what	O
type	O
of	O
linkage	B
should	O
be	O
used	O
?	O
–	O
where	O
should	O
we	O
cut	O
the	O
dendrogram	B
in	O
order	O
to	O
obtain	O
clusters	O
?	O
•	O
in	O
the	O
case	O
of	O
k-means	O
clustering	B
,	O
how	O
many	O
clusters	O
should	O
we	O
look	O
for	O
in	O
the	O
data	B
?	O
each	O
of	O
these	O
decisions	O
can	O
have	O
a	O
strong	O
impact	O
on	O
the	O
results	O
obtained	O
.	O
in	O
practice	O
,	O
we	O
try	O
several	O
diﬀerent	O
choices	O
,	O
and	O
look	O
for	O
the	O
one	O
with	O
the	O
most	O
useful	O
or	O
interpretable	O
solution	O
.	O
with	O
these	O
methods	O
,	O
there	O
is	O
no	O
single	B
right	O
answer—any	O
solution	O
that	O
exposes	O
some	O
interesting	O
aspects	O
of	O
the	O
data	B
should	O
be	O
considered	O
.	O
validating	O
the	O
clusters	O
obtained	O
any	O
time	O
clustering	O
is	O
performed	O
on	O
a	O
data	B
set	O
we	O
will	O
ﬁnd	O
clusters	O
.	O
but	O
we	O
really	O
want	O
to	O
know	O
whether	O
the	O
clusters	O
that	O
have	O
been	O
found	O
represent	O
true	O
subgroups	O
in	O
the	O
data	B
,	O
or	O
whether	O
they	O
are	O
simply	O
a	O
result	O
of	O
clustering	B
the	O
noise	B
.	O
for	O
instance	O
,	O
if	O
we	O
were	O
to	O
obtain	O
an	O
independent	B
set	O
of	O
observa-	O
tions	O
,	O
then	O
would	O
those	O
observations	B
also	O
display	O
the	O
same	O
set	B
of	O
clusters	O
?	O
this	O
is	O
a	O
hard	O
question	O
to	O
answer	O
.	O
there	O
exist	O
a	O
number	O
of	O
techniques	O
for	O
assigning	O
a	O
p-value	B
to	O
a	O
cluster	O
in	O
order	O
to	O
assess	O
whether	O
there	O
is	O
more	O
evidence	O
for	O
the	O
cluster	O
than	O
one	O
would	O
expect	O
due	O
to	O
chance	O
.	O
however	O
,	O
there	O
has	O
been	O
no	O
consensus	O
on	O
a	O
single	B
best	O
approach	B
.	O
more	O
details	O
can	O
be	O
found	O
in	O
hastie	O
et	O
al	O
.	O
(	O
2009	O
)	O
.	O
other	O
considerations	O
in	O
clustering	B
both	O
k-means	O
and	O
hierarchical	B
clustering	I
will	O
assign	O
each	O
observation	O
to	O
a	O
cluster	O
.	O
however	O
,	O
sometimes	O
this	O
might	O
not	O
be	O
appropriate	O
.	O
for	O
instance	O
,	O
suppose	O
that	O
most	O
of	O
the	O
observations	B
truly	O
belong	O
to	O
a	O
small	O
number	O
of	O
(	O
unknown	O
)	O
subgroups	O
,	O
and	O
a	O
small	O
subset	O
of	O
the	O
observations	B
are	O
quite	O
diﬀerent	O
from	O
each	O
other	O
and	O
from	O
all	O
other	O
observations	B
.	O
then	O
since	O
k-	O
means	O
and	O
hierarchical	B
clustering	I
force	O
every	O
observation	O
into	O
a	O
cluster	O
,	O
the	O
clusters	O
found	O
may	O
be	O
heavily	O
distorted	O
due	O
to	O
the	O
presence	O
of	O
outliers	O
that	O
do	O
not	O
belong	O
to	O
any	O
cluster	O
.	O
mixture	O
models	O
are	O
an	O
attractive	O
approach	B
for	O
accommodating	O
the	O
presence	O
of	O
such	O
outliers	O
.	O
these	O
amount	O
to	O
a	O
soft	O
version	O
of	O
k-means	O
clustering	B
,	O
and	O
are	O
described	O
in	O
hastie	O
et	O
al	O
.	O
(	O
2009	O
)	O
.	O
in	O
addition	O
,	O
clustering	B
methods	O
generally	O
are	O
not	O
very	O
robust	B
to	O
pertur-	O
bations	O
to	O
the	O
data	B
.	O
for	O
instance	O
,	O
suppose	O
that	O
we	O
cluster	O
n	O
observations	B
,	O
and	O
then	O
cluster	O
the	O
observations	B
again	O
after	O
removing	O
a	O
subset	O
of	O
the	O
n	O
observations	B
at	O
random	O
.	O
one	O
would	O
hope	O
that	O
the	O
two	O
sets	O
of	O
clusters	O
ob-	O
tained	O
would	O
be	O
quite	O
similar	O
,	O
but	O
often	O
this	O
is	O
not	O
the	O
case	O
!	O
10.4	O
lab	O
1	O
:	O
principal	B
components	I
analysis	O
401	O
a	O
tempered	O
approach	B
to	O
interpreting	O
the	O
results	O
of	O
clustering	B
we	O
have	O
described	O
some	O
of	O
the	O
issues	O
associated	O
with	O
clustering	B
.	O
however	O
,	O
clustering	B
can	O
be	O
a	O
very	O
useful	O
and	O
valid	O
statistical	O
tool	O
if	O
used	O
properly	O
.	O
we	O
mentioned	O
that	O
small	O
decisions	O
in	O
how	O
clustering	B
is	O
performed	O
,	O
such	O
as	O
how	O
the	O
data	B
are	O
standardized	O
and	O
what	O
type	O
of	O
linkage	B
is	O
used	O
,	O
can	O
have	O
a	O
large	O
eﬀect	O
on	O
the	O
results	O
.	O
therefore	O
,	O
we	O
recommend	O
performing	O
clustering	B
with	O
diﬀerent	O
choices	O
of	O
these	O
parameters	O
,	O
and	O
looking	O
at	O
the	O
full	O
set	B
of	O
results	O
in	O
order	O
to	O
see	O
what	O
patterns	O
consistently	O
emerge	O
.	O
since	O
clustering	B
can	O
be	O
non-robust	O
,	O
we	O
recommend	O
clustering	B
subsets	O
of	O
the	O
data	B
in	O
order	O
to	O
get	O
a	O
sense	O
of	O
the	O
robustness	O
of	O
the	O
clusters	O
obtained	O
.	O
most	O
importantly	O
,	O
we	O
must	O
be	O
careful	O
about	O
how	O
the	O
results	O
of	O
a	O
clustering	B
analysis	O
are	O
reported	O
.	O
these	O
results	O
should	O
not	O
be	O
taken	O
as	O
the	O
absolute	O
truth	O
about	O
a	O
data	B
set	O
.	O
rather	O
,	O
they	O
should	O
constitute	O
a	O
starting	O
point	O
for	O
the	O
development	O
of	O
a	O
scientiﬁc	O
hypothesis	B
and	O
further	O
study	O
,	O
preferably	O
on	O
an	O
independent	B
data	O
set	B
.	O
10.4	O
lab	O
1	O
:	O
principal	B
components	I
analysis	O
in	O
this	O
lab	O
,	O
we	O
perform	O
pca	O
on	O
the	O
usarrests	O
data	B
set	O
,	O
which	O
is	O
part	O
of	O
the	O
base	O
r	O
package	O
.	O
the	O
rows	O
of	O
the	O
data	B
set	O
contain	O
the	O
50	O
states	O
,	O
in	O
alphabetical	O
order	O
.	O
>	O
states	O
=	O
row	O
.	O
names	O
(	O
usarrests	O
)	O
>	O
states	O
the	O
columns	O
of	O
the	O
data	B
set	O
contain	O
the	O
four	O
variables	O
.	O
>	O
names	O
(	O
usarrests	O
)	O
[	O
1	O
]	O
``	O
murder	O
``	O
''	O
assault	O
``	O
''	O
urbanpop	O
``	O
``	O
rape	O
``	O
we	O
ﬁrst	O
brieﬂy	O
examine	O
the	O
data	B
.	O
we	O
notice	O
that	O
the	O
variables	O
have	O
vastly	O
diﬀerent	O
means	O
.	O
>	O
apply	O
(	O
usarrests	O
,	O
2	O
,	O
mean	O
)	O
murder	O
7.79	O
assault	O
urbanpop	O
65.54	O
170.76	O
rape	O
21.23	O
note	O
that	O
the	O
apply	O
(	O
)	O
function	B
allows	O
us	O
to	O
apply	O
a	O
function—in	O
this	O
case	O
,	O
the	O
mean	O
(	O
)	O
function—to	O
each	O
row	O
or	O
column	O
of	O
the	O
data	B
set	O
.	O
the	O
second	O
input	B
here	O
denotes	O
whether	O
we	O
wish	O
to	O
compute	O
the	O
mean	O
of	O
the	O
rows	O
,	O
1	O
,	O
or	O
the	O
columns	O
,	O
2.	O
we	O
see	O
that	O
there	O
are	O
on	O
average	B
three	O
times	O
as	O
many	O
rapes	O
as	O
murders	O
,	O
and	O
more	O
than	O
eight	O
times	O
as	O
many	O
assaults	O
as	O
rapes	O
.	O
we	O
can	O
also	O
examine	O
the	O
variances	O
of	O
the	O
four	O
variables	O
using	O
the	O
apply	O
(	O
)	O
function	B
.	O
>	O
apply	O
(	O
usarrests	O
,	O
2	O
,	O
var	O
)	O
assault	O
urbanpop	O
209.5	O
murder	O
19.0	O
6945.2	O
rape	O
87.7	O
prcomp	O
(	O
)	O
402	O
10.	O
unsupervised	B
learning	I
not	O
surprisingly	O
,	O
the	O
variables	O
also	O
have	O
vastly	O
diﬀerent	O
variances	O
:	O
the	O
urbanpop	O
variable	B
measures	O
the	O
percentage	O
of	O
the	O
population	O
in	O
each	O
state	O
living	O
in	O
an	O
urban	O
area	O
,	O
which	O
is	O
not	O
a	O
comparable	O
number	O
to	O
the	O
num-	O
ber	O
of	O
rapes	O
in	O
each	O
state	O
per	O
100,000	O
individuals	O
.	O
if	O
we	O
failed	O
to	O
scale	O
the	O
variables	O
before	O
performing	O
pca	O
,	O
then	O
most	O
of	O
the	O
principal	B
components	I
that	O
we	O
observed	O
would	O
be	O
driven	O
by	O
the	O
assault	O
variable	B
,	O
since	O
it	O
has	O
by	O
far	O
the	O
largest	O
mean	O
and	O
variance	B
.	O
thus	O
,	O
it	O
is	O
important	O
to	O
standardize	B
the	O
variables	O
to	O
have	O
mean	O
zero	O
and	O
standard	O
deviation	O
one	O
before	O
performing	O
pca	O
.	O
we	O
now	O
perform	O
principal	B
components	I
analysis	O
using	O
the	O
prcomp	O
(	O
)	O
func-	O
tion	O
,	O
which	O
is	O
one	O
of	O
several	O
functions	O
in	O
r	O
that	O
perform	O
pca	O
.	O
>	O
pr	O
.	O
out	O
=	O
prcomp	O
(	O
usarrests	O
,	O
scale	O
=	O
true	O
)	O
by	O
default	O
,	O
the	O
prcomp	O
(	O
)	O
function	B
centers	O
the	O
variables	O
to	O
have	O
mean	O
zero	O
.	O
by	O
using	O
the	O
option	O
scale=true	O
,	O
we	O
scale	O
the	O
variables	O
to	O
have	O
standard	O
deviation	O
one	O
.	O
the	O
output	B
from	O
prcomp	O
(	O
)	O
contains	O
a	O
number	O
of	O
useful	O
quan-	O
tities	O
.	O
>	O
names	O
(	O
pr	O
.	O
out	O
)	O
[	O
1	O
]	O
``	O
sdev	O
``	O
''	O
rotation	O
``	O
``	O
center	O
``	O
''	O
scale	O
``	O
''	O
x	O
``	O
the	O
center	O
and	O
scale	O
components	O
correspond	O
to	O
the	O
means	O
and	O
standard	O
deviations	O
of	O
the	O
variables	O
that	O
were	O
used	O
for	O
scaling	O
prior	B
to	O
implementing	O
pca	O
.	O
>	O
pr	O
.	O
out	O
$	O
cente	O
r	O
murder	O
7.79	O
assault	O
urbanpop	O
65.54	O
170.76	O
rape	O
21.23	O
>	O
pr	O
.	O
out	O
$	O
scale	O
murder	O
4.36	O
assault	O
urbanpop	O
14.47	O
83.34	O
rape	O
9.37	O
the	O
rotation	O
matrix	O
provides	O
the	O
principal	O
component	O
loadings	O
;	O
each	O
col-	O
umn	O
of	O
pr.out	O
$	O
rotation	O
contains	O
the	O
corresponding	O
principal	O
component	O
loading	O
vector.2	O
>	O
pr	O
.	O
o	O
u	O
t	O
$	O
r	O
o	O
t	O
a	O
t	O
i	O
o	O
n	O
pc2	O
pc1	O
-0.536	O
-0.583	O
murder	O
assault	O
urbanpop	O
-0.278	O
-0.873	O
-0.378	O
rape	O
0.818	O
pc4	O
pc3	O
0.418	O
-0.341	O
0.649	O
0.188	O
-0.268	O
-0.743	O
0.134	O
0.089	O
-0.543	O
-0.167	O
we	O
see	O
that	O
there	O
are	O
four	O
distinct	O
principal	B
components	I
.	O
this	O
is	O
to	O
be	O
expected	O
because	O
there	O
are	O
in	O
general	O
min	O
(	O
n	O
−	O
1	O
,	O
p	O
)	O
informative	O
principal	B
components	I
in	O
a	O
data	B
set	O
with	O
n	O
observations	B
and	O
p	O
variables	O
.	O
2this	O
function	B
names	O
it	O
the	O
rotation	O
matrix	O
,	O
because	O
when	O
we	O
matrix-multiply	O
the	O
x	O
matrix	O
by	O
pr.out	O
$	O
rotation	O
,	O
it	O
gives	O
us	O
the	O
coordinates	O
of	O
the	O
data	B
in	O
the	O
rotated	O
coordinate	O
system	O
.	O
these	O
coordinates	O
are	O
the	O
principal	O
component	O
scores	O
.	O
10.4	O
lab	O
1	O
:	O
principal	B
components	I
analysis	O
403	O
using	O
the	O
prcomp	O
(	O
)	O
function	B
,	O
we	O
do	O
not	O
need	O
to	O
explicitly	O
multiply	O
the	O
data	B
by	O
the	O
principal	O
component	O
loading	O
vectors	O
in	O
order	O
to	O
obtain	O
the	O
principal	O
component	O
score	O
vectors	O
.	O
rather	O
the	O
50	O
×	O
4	O
matrix	O
x	O
has	O
as	O
its	O
columns	O
the	O
principal	O
component	O
score	O
vectors	O
.	O
that	O
is	O
,	O
the	O
kth	O
column	O
is	O
the	O
kth	O
principal	O
component	O
score	B
vector	I
.	O
>	O
dim	O
(	O
pr	O
.	O
out	O
$	O
x	O
)	O
[	O
1	O
]	O
50	O
4	O
we	O
can	O
plot	B
the	O
ﬁrst	O
two	O
principal	B
components	I
as	O
follows	O
:	O
>	O
biplot	B
(	O
pr	O
.	O
out	O
,	O
scale	O
=0	O
)	O
the	O
scale=0	O
argument	B
to	O
biplot	B
(	O
)	O
ensures	O
that	O
the	O
arrows	O
are	O
scaled	O
to	O
represent	O
the	O
loadings	O
;	O
other	O
values	O
for	O
scale	O
give	O
slightly	O
diﬀerent	O
biplots	O
with	O
diﬀerent	O
interpretations	O
.	O
notice	O
that	O
this	O
ﬁgure	O
is	O
a	O
mirror	O
image	O
of	O
figure	O
10.1.	O
recall	B
that	O
the	O
principal	B
components	I
are	O
only	O
unique	O
up	O
to	O
a	O
sign	O
change	O
,	O
so	O
we	O
can	O
reproduce	O
figure	O
10.1	O
by	O
making	O
a	O
few	O
small	O
changes	O
:	O
biplot	B
(	O
)	O
>	O
pr	O
.	O
o	O
u	O
t	O
$	O
r	O
o	O
t	O
a	O
t	O
i	O
o	O
n	O
=	O
-	O
pr	O
.	O
o	O
u	O
t	O
$	O
r	O
o	O
t	O
a	O
t	O
i	O
o	O
n	O
>	O
pr	O
.	O
out	O
$	O
x	O
=	O
-	O
pr	O
.	O
out	O
$	O
x	O
>	O
biplot	B
(	O
pr	O
.	O
out	O
,	O
scale	O
=0	O
)	O
the	O
prcomp	O
(	O
)	O
function	B
also	O
outputs	O
the	O
standard	O
deviation	O
of	O
each	O
prin-	O
cipal	O
component	O
.	O
for	O
instance	O
,	O
on	O
the	O
usarrests	O
data	B
set	O
,	O
we	O
can	O
access	O
these	O
standard	O
deviations	O
as	O
follows	O
:	O
>	O
pr	O
.	O
out	O
$	O
sdev	O
[	O
1	O
]	O
1.575	O
0.995	O
0.597	O
0.416	O
the	O
variance	B
explained	O
by	O
each	O
principal	O
component	O
is	O
obtained	O
by	O
squar-	O
ing	O
these	O
:	O
>	O
pr	O
.	O
var	O
=	O
pr	O
.	O
out	O
$	O
sdev	O
^2	O
>	O
pr	O
.	O
var	O
[	O
1	O
]	O
2.480	O
0.990	O
0.357	O
0.173	O
to	O
compute	O
the	O
proportion	B
of	I
variance	I
explained	O
by	O
each	O
principal	O
compo-	O
nent	O
,	O
we	O
simply	O
divide	O
the	O
variance	B
explained	O
by	O
each	O
principal	O
component	O
by	O
the	O
total	O
variance	O
explained	B
by	O
all	O
four	O
principal	B
components	I
:	O
>	O
pve	O
=	O
pr	O
.	O
var	O
/	O
sum	O
(	O
pr	O
.	O
var	O
)	O
>	O
pve	O
[	O
1	O
]	O
0.6201	O
0.2474	O
0.0891	O
0.0434	O
we	O
see	O
that	O
the	O
ﬁrst	O
principal	O
component	O
explains	O
62.0	O
%	O
of	O
the	O
variance	B
in	O
the	O
data	B
,	O
the	O
next	O
principal	O
component	O
explains	O
24.7	O
%	O
of	O
the	O
variance	B
,	O
and	O
so	O
forth	O
.	O
we	O
can	O
plot	B
the	O
pve	O
explained	B
by	O
each	O
component	O
,	O
as	O
well	O
as	O
the	O
cumulative	O
pve	O
,	O
as	O
follows	O
:	O
>	O
plot	B
(	O
pve	O
,	O
xlab	O
=	O
''	O
principal	O
component	O
``	O
,	O
ylab	O
=	O
''	O
proportio	O
n	O
of	O
variance	B
explained	O
``	O
,	O
ylim	O
=	O
c	O
(	O
0	O
,1	O
)	O
,	O
type	O
=	O
’	O
b	O
’	O
)	O
>	O
plot	B
(	O
cumsum	O
(	O
pve	O
)	O
,	O
xlab	O
=	O
''	O
principal	O
component	O
``	O
,	O
ylab	O
=	O
''	O
cumulative	O
proportio	O
n	O
of	O
variance	B
explained	O
``	O
,	O
ylim	O
=	O
c	O
(	O
0	O
,1	O
)	O
,	O
type	O
=	O
’	O
b	O
’	O
)	O
404	O
10.	O
unsupervised	B
learning	I
the	O
result	O
is	O
shown	O
in	O
figure	O
10.4.	O
note	O
that	O
the	O
function	B
cumsum	O
(	O
)	O
com-	O
putes	O
the	O
cumulative	O
sum	O
of	O
the	O
elements	O
of	O
a	O
numeric	O
vector	B
.	O
for	O
instance	O
:	O
cumsum	O
(	O
)	O
>	O
a	O
=	O
c	O
(	O
1	O
,2	O
,8	O
,	O
-3	O
)	O
>	O
cumsum	O
(	O
a	O
)	O
[	O
1	O
]	O
3	O
11	O
1	O
8	O
10.5	O
lab	O
2	O
:	O
clustering	B
10.5.1	O
k-means	O
clustering	B
the	O
function	B
kmeans	O
(	O
)	O
performs	O
k-means	O
clustering	B
in	O
r.	O
we	O
begin	O
with	O
a	O
simple	B
simulated	O
example	O
in	O
which	O
there	O
truly	O
are	O
two	O
clusters	O
in	O
the	O
data	B
:	O
the	O
ﬁrst	O
25	O
observations	B
have	O
a	O
mean	O
shift	O
relative	O
to	O
the	O
next	O
25	O
observations	B
.	O
kmeans	O
(	O
)	O
>	O
set	B
.	O
seed	B
(	O
2	O
)	O
>	O
x	O
=	O
matrix	O
(	O
rnorm	O
(	O
50*2	O
)	O
,	O
ncol	O
=2	O
)	O
>	O
x	O
[	O
1:25	O
,1	O
]	O
=	O
x	O
[	O
1:25	O
,1	O
]	O
+3	O
>	O
x	O
[	O
1:25	O
,2	O
]	O
=	O
x	O
[	O
1:25	O
,2	O
]	O
-4	O
we	O
now	O
perform	O
k-means	O
clustering	B
with	O
k	O
=	O
2	O
.	O
>	O
km	O
.	O
out	O
=	O
kmeans	O
(	O
x	O
,2	O
,	O
nstart	O
=20	O
)	O
the	O
cluster	O
assignments	O
of	O
km.out	O
$	O
cluster	O
.	O
>	O
km	O
.	O
out	O
$	O
clust	O
e	O
r	O
the	O
50	O
observations	B
are	O
contained	O
in	O
[	O
1	O
]	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
1	O
1	O
1	O
1	O
[	O
30	O
]	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
the	O
k-means	O
clustering	B
perfectly	O
separated	O
the	O
observations	B
into	O
two	O
clus-	O
ters	O
even	O
though	O
we	O
did	O
not	O
supply	O
any	O
group	O
information	O
to	O
kmeans	O
(	O
)	O
.	O
we	O
can	O
plot	B
the	O
data	B
,	O
with	O
each	O
observation	O
colored	O
according	O
to	O
its	O
cluster	O
assignment	O
.	O
>	O
plot	B
(	O
x	O
,	O
col	O
=	O
(	O
km	O
.	O
out	O
$	O
clust	O
er	O
+1	O
)	O
,	O
main	O
=	O
''	O
k	O
-	O
means	O
clustering	B
results	O
with	O
k	O
=2	O
''	O
,	O
xlab	O
=	O
''	O
''	O
,	O
ylab	O
=	O
''	O
''	O
,	O
pch	O
=20	O
,	O
cex	O
=2	O
)	O
here	O
the	O
observations	B
can	O
be	O
easily	O
plotted	O
because	O
they	O
are	O
two-dimensional	O
.	O
if	O
there	O
were	O
more	O
than	O
two	O
variables	O
then	O
we	O
could	O
instead	O
perform	O
pca	O
and	O
plot	B
the	O
ﬁrst	O
two	O
principal	B
components	I
score	O
vectors	O
.	O
in	O
this	O
example	O
,	O
we	O
knew	O
that	O
there	O
really	O
were	O
two	O
clusters	O
because	O
we	O
generated	O
the	O
data	B
.	O
however	O
,	O
for	O
real	O
data	B
,	O
in	O
general	O
we	O
do	O
not	O
know	O
the	O
true	O
number	O
of	O
clusters	O
.	O
we	O
could	O
instead	O
have	O
performed	O
k-means	O
clustering	B
on	O
this	O
example	O
with	O
k	O
=	O
3	O
.	O
>	O
set	B
.	O
seed	B
(	O
4	O
)	O
>	O
km	O
.	O
out	O
=	O
kmeans	O
(	O
x	O
,3	O
,	O
nstart	O
=20	O
)	O
>	O
km	O
.	O
out	O
k	O
-	O
means	O
clusterin	O
g	O
with	O
3	O
clusters	O
of	O
sizes	O
10	O
,	O
23	O
,	O
17	O
10.5	O
lab	O
2	O
:	O
clustering	B
405	O
cluster	O
means	O
:	O
[	O
,1	O
]	O
[	O
,2	O
]	O
1	O
2.3001545	O
-2.69622023	O
2	O
-0.3820397	O
-0.08740753	O
3	O
3.7789567	O
-4.56200798	O
clustering	B
vector	O
:	O
[	O
1	O
]	O
3	O
1	O
3	O
1	O
3	O
3	O
3	O
1	O
3	O
1	O
3	O
1	O
3	O
1	O
3	O
1	O
3	O
3	O
3	O
3	O
3	O
1	O
3	O
3	O
3	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
1	O
2	O
1	O
2	O
2	O
2	O
2	O
within	O
cluster	O
sum	B
of	I
squares	I
by	O
cluster	O
:	O
[	O
1	O
]	O
19.56137	O
52.67700	O
25.74089	O
(	O
between_s	O
s	O
/	O
total_ss	O
=	O
79.3	O
%	O
)	O
available	O
components	O
:	O
[	O
1	O
]	O
``	O
cluster	O
``	O
''	O
centers	O
``	O
''	O
tot	O
.	O
withinss	O
``	O
``	O
betweenss	O
``	O
''	O
totss	O
``	O
''	O
size	O
``	O
''	O
withinss	O
``	O
>	O
plot	B
(	O
x	O
,	O
col	O
=	O
(	O
km	O
.	O
out	O
$	O
clust	O
er	O
+1	O
)	O
,	O
main	O
=	O
''	O
k	O
-	O
means	O
clustering	B
results	O
with	O
k	O
=3	O
''	O
,	O
xlab	O
=	O
''	O
''	O
,	O
ylab	O
=	O
''	O
''	O
,	O
pch	O
=20	O
,	O
cex	O
=2	O
)	O
when	O
k	O
=	O
3	O
,	O
k-means	O
clustering	B
splits	O
up	O
the	O
two	O
clusters	O
.	O
to	O
run	O
the	O
kmeans	O
(	O
)	O
function	B
in	O
r	O
with	O
multiple	B
initial	O
cluster	O
assign-	O
ments	O
,	O
we	O
use	O
the	O
nstart	O
argument	B
.	O
if	O
a	O
value	O
of	O
nstart	O
greater	O
than	O
one	O
is	O
used	O
,	O
then	O
k-means	O
clustering	B
will	O
be	O
performed	O
using	O
multiple	B
random	O
assignments	O
in	O
step	O
1	O
of	O
algorithm	O
10.1	O
,	O
and	O
the	O
kmeans	O
(	O
)	O
function	B
will	O
report	O
only	O
the	O
best	O
results	O
.	O
here	O
we	O
compare	O
using	O
nstart=1	O
to	O
nstart=20	O
.	O
>	O
set	B
.	O
seed	B
(	O
3	O
)	O
>	O
km	O
.	O
out	O
=	O
kmeans	O
(	O
x	O
,3	O
,	O
nstart	O
=1	O
)	O
>	O
km	O
.	O
out	O
$	O
tot	O
.	O
withinss	O
[	O
1	O
]	O
104.3319	O
>	O
km	O
.	O
out	O
=	O
kmeans	O
(	O
x	O
,3	O
,	O
nstart	O
=20	O
)	O
>	O
km	O
.	O
out	O
$	O
tot	O
.	O
withinss	O
[	O
1	O
]	O
97.9793	O
note	O
that	O
km.out	O
$	O
tot.withinss	O
is	O
the	O
total	O
within-cluster	O
sum	B
of	I
squares	I
,	O
which	O
we	O
seek	O
to	O
minimize	O
by	O
performing	O
k-means	O
clustering	B
(	O
equation	O
10.11	O
)	O
.	O
the	O
individual	O
within-cluster	O
sum-of-squares	O
are	O
contained	O
in	O
the	O
vector	B
km.out	O
$	O
withinss	O
.	O
we	O
strongly	O
recommend	O
always	O
running	O
k-means	O
clustering	B
with	O
a	O
large	O
value	O
of	O
nstart	O
,	O
such	O
as	O
20	O
or	O
50	O
,	O
since	O
otherwise	O
an	O
undesirable	O
local	B
optimum	O
may	O
be	O
obtained	O
.	O
when	O
performing	O
k-means	O
clustering	B
,	O
in	O
addition	O
to	O
using	O
multiple	B
ini-	O
tial	O
cluster	O
assignments	O
,	O
it	O
is	O
also	O
important	O
to	O
set	B
a	O
random	O
seed	O
using	O
the	O
set.seed	O
(	O
)	O
function	B
.	O
this	O
way	O
,	O
the	O
initial	O
cluster	O
assignments	O
in	O
step	O
1	O
can	O
be	O
replicated	O
,	O
and	O
the	O
k-means	O
output	B
will	O
be	O
fully	O
reproducible	O
.	O
406	O
10.	O
unsupervised	B
learning	I
10.5.2	O
hierarchical	B
clustering	I
the	O
hclust	O
(	O
)	O
function	B
implements	O
hierarchical	B
clustering	I
in	O
r.	O
in	O
the	O
fol-	O
lowing	O
example	O
we	O
use	O
the	O
data	B
from	O
section	O
10.5.1	O
to	O
plot	B
the	O
hierarchical	B
clustering	I
dendrogram	O
using	O
complete	B
,	O
single	B
,	O
and	O
average	B
linkage	O
cluster-	O
ing	O
,	O
with	O
euclidean	O
distance	B
as	O
the	O
dissimilarity	B
measure	O
.	O
we	O
begin	O
by	O
clustering	B
observations	O
using	O
complete	B
linkage	O
.	O
the	O
dist	O
(	O
)	O
function	B
is	O
used	O
to	O
compute	O
the	O
50	O
×	O
50	O
inter-observation	O
euclidean	O
distance	B
matrix	O
.	O
hclust	O
(	O
)	O
dist	O
(	O
)	O
>	O
hc	O
.	O
complete	B
=	O
hclust	O
(	O
dist	O
(	O
x	O
)	O
,	O
method	O
=	O
''	O
complete	B
``	O
)	O
we	O
could	O
just	O
as	O
easily	O
perform	O
hierarchical	B
clustering	I
with	O
average	B
or	O
single	B
linkage	O
instead	O
:	O
>	O
hc	O
.	O
average	B
=	O
hclust	O
(	O
dist	O
(	O
x	O
)	O
,	O
method	O
=	O
''	O
average	B
``	O
)	O
>	O
hc	O
.	O
single	B
=	O
hclust	O
(	O
dist	O
(	O
x	O
)	O
,	O
method	O
=	O
''	O
single	B
``	O
)	O
we	O
can	O
now	O
plot	B
the	O
dendrograms	O
obtained	O
using	O
the	O
usual	O
plot	B
(	O
)	O
function	B
.	O
the	O
numbers	O
at	O
the	O
bottom	O
of	O
the	O
plot	B
identify	O
each	O
observation	O
.	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
1	O
,3	O
)	O
)	O
>	O
plot	B
(	O
hc	O
.	O
complete	B
,	O
main	O
=	O
''	O
complete	B
linkage	O
``	O
,	O
xlab	O
=	O
''	O
''	O
,	O
sub	O
=	O
''	O
''	O
,	O
cex	O
=.9	O
)	O
>	O
plot	B
(	O
hc	O
.	O
average	B
,	O
main	O
=	O
''	O
average	B
linkage	O
``	O
,	O
xlab	O
=	O
''	O
''	O
,	O
sub	O
=	O
''	O
''	O
,	O
cex	O
=.9	O
)	O
>	O
plot	B
(	O
hc	O
.	O
single	B
,	O
main	O
=	O
''	O
single	B
linkage	O
``	O
,	O
xlab	O
=	O
''	O
''	O
,	O
sub	O
=	O
''	O
''	O
,	O
cex	O
=.9	O
)	O
to	O
determine	O
the	O
cluster	O
labels	O
for	O
each	O
observation	O
associated	O
with	O
a	O
given	O
cut	O
of	O
the	O
dendrogram	B
,	O
we	O
can	O
use	O
the	O
cutree	O
(	O
)	O
function	B
:	O
cutree	O
(	O
)	O
>	O
cutree	O
(	O
hc	O
.	O
complete	B
,	O
2	O
)	O
[	O
1	O
]	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
2	O
2	O
2	O
2	O
[	O
30	O
]	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
>	O
cutree	O
(	O
hc	O
.	O
average	B
,	O
2	O
)	O
[	O
1	O
]	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
2	O
2	O
2	O
2	O
[	O
30	O
]	O
2	O
2	O
2	O
1	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
2	O
1	O
2	O
1	O
2	O
2	O
2	O
2	O
>	O
cutree	O
(	O
hc	O
.	O
single	B
,	O
2	O
)	O
[	O
1	O
]	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
2	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
[	O
30	O
]	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
for	O
this	O
data	B
,	O
complete	B
and	O
average	B
linkage	O
generally	O
separate	O
the	O
observa-	O
tions	O
into	O
their	O
correct	O
groups	O
.	O
however	O
,	O
single	B
linkage	O
identiﬁes	O
one	O
point	O
as	O
belonging	O
to	O
its	O
own	O
cluster	O
.	O
a	O
more	O
sensible	O
answer	O
is	O
obtained	O
when	O
four	O
clusters	O
are	O
selected	O
,	O
although	O
there	O
are	O
still	O
two	O
singletons	O
.	O
>	O
cutree	O
(	O
hc	O
.	O
single	B
,	O
4	O
)	O
[	O
1	O
]	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
2	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
1	O
3	O
3	O
3	O
3	O
[	O
30	O
]	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
4	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
3	O
to	O
scale	O
the	O
variables	O
before	O
performing	O
hierarchical	B
clustering	I
of	O
the	O
observations	B
,	O
we	O
use	O
the	O
scale	O
(	O
)	O
function	B
:	O
scale	O
(	O
)	O
>	O
xsc	O
=	O
scale	O
(	O
x	O
)	O
>	O
plot	B
(	O
hclust	O
(	O
dist	O
(	O
xsc	O
)	O
,	O
method	O
=	O
''	O
complete	B
``	O
)	O
,	O
main	O
=	O
''	O
h	O
i	O
e	O
r	O
a	O
r	O
c	O
h	O
i	O
c	O
a	O
l	O
clustering	B
with	O
scaled	O
features	O
``	O
)	O
10.6	O
lab	O
3	O
:	O
nci60	O
data	B
example	O
407	O
correlation-based	B
distance	O
can	O
be	O
computed	O
using	O
the	O
as.dist	O
(	O
)	O
func-	O
tion	O
,	O
which	O
converts	O
an	O
arbitrary	O
square	O
symmetric	O
matrix	O
into	O
a	O
form	O
that	O
the	O
hclust	O
(	O
)	O
function	B
recognizes	O
as	O
a	O
distance	B
matrix	O
.	O
however	O
,	O
this	O
only	O
makes	O
sense	O
for	O
data	O
with	O
at	O
least	O
three	O
features	O
since	O
the	O
absolute	O
corre-	O
lation	O
between	O
any	O
two	O
observations	B
with	O
measurements	O
on	O
two	O
features	O
is	O
always	O
1.	O
hence	O
,	O
we	O
will	O
cluster	O
a	O
three-dimensional	O
data	B
set	O
.	O
as.dist	O
(	O
)	O
>	O
x	O
=	O
matrix	O
(	O
rnorm	O
(	O
30*3	O
)	O
,	O
ncol	O
=3	O
)	O
>	O
dd	O
=	O
as	O
.	O
dist	O
(	O
1	O
-	O
cor	O
(	O
t	O
(	O
x	O
)	O
)	O
)	O
>	O
plot	B
(	O
hclust	O
(	O
dd	O
,	O
method	O
=	O
''	O
complete	B
``	O
)	O
,	O
main	O
=	O
''	O
complete	B
linkage	O
with	O
correlation	B
-	O
based	O
distance	B
``	O
,	O
xlab	O
=	O
''	O
''	O
,	O
sub	O
=	O
''	O
''	O
)	O
10.6	O
lab	O
3	O
:	O
nci60	O
data	B
example	O
unsupervised	O
techniques	O
are	O
often	O
used	O
in	O
the	O
analysis	O
of	O
genomic	O
data	B
.	O
in	O
particular	O
,	O
pca	O
and	O
hierarchical	B
clustering	I
are	O
popular	O
tools	O
.	O
we	O
illus-	O
trate	O
these	O
techniques	O
on	O
the	O
nci60	O
cancer	O
cell	O
line	B
microarray	O
data	B
,	O
which	O
consists	O
of	O
6,830	O
gene	O
expression	O
measurements	O
on	O
64	O
cancer	O
cell	O
lines	O
.	O
>	O
library	O
(	O
islr	O
)	O
>	O
nci	O
.	O
labs	O
=	O
nci60	O
$	O
lab	O
s	O
>	O
nci	O
.	O
data	B
=	O
nci60	O
$	O
dat	O
a	O
each	O
cell	O
line	B
is	O
labeled	O
with	O
a	O
cancer	O
type	O
.	O
we	O
do	O
not	O
make	O
use	O
of	O
the	O
cancer	O
types	O
in	O
performing	O
pca	O
and	O
clustering	B
,	O
as	O
these	O
are	O
unsupervised	O
techniques	O
.	O
but	O
after	O
performing	O
pca	O
and	O
clustering	B
,	O
we	O
will	O
check	O
to	O
see	O
the	O
extent	O
to	O
which	O
these	O
cancer	O
types	O
agree	O
with	O
the	O
results	O
of	O
these	O
unsupervised	O
techniques	O
.	O
the	O
data	B
has	O
64	O
rows	O
and	O
6,830	O
columns	O
.	O
>	O
dim	O
(	O
nci	O
.	O
data	B
)	O
[	O
1	O
]	O
64	O
6830	O
we	O
begin	O
by	O
examining	O
the	O
cancer	O
types	O
for	O
the	O
cell	O
lines	O
.	O
>	O
nci	O
.	O
labs	O
[	O
1:4	O
]	O
[	O
1	O
]	O
``	O
cns	O
``	O
''	O
cns	O
``	O
>	O
table	O
(	O
nci	O
.	O
labs	O
)	O
nci	O
.	O
labs	O
''	O
cns	O
``	O
''	O
renal	O
``	O
breast	O
7	O
6	O
ovarian	O
6	O
7	O
leukemia	O
mcf7a	O
-	O
repro	O
mcf7d	O
-	O
repro	O
1	O
renal	O
9	O
1	O
prostate	O
2	O
1	O
melanoma	O
8	O
unknown	O
1	O
cns	O
5	O
colon	O
k562a	O
-	O
repro	O
k562b	O
-	O
repro	O
1	O
nsclc	O
9	O
408	O
10.	O
unsupervised	B
learning	I
10.6.1	O
pca	O
on	O
the	O
nci60	O
data	B
we	O
ﬁrst	O
perform	O
pca	O
on	O
the	O
data	B
after	O
scaling	O
the	O
variables	O
(	O
genes	O
)	O
to	O
have	O
standard	O
deviation	O
one	O
,	O
although	O
one	O
could	O
reasonably	O
argue	O
that	O
it	O
is	O
better	O
not	O
to	O
scale	O
the	O
genes	O
.	O
>	O
pr	O
.	O
out	O
=	O
prcomp	O
(	O
nci	O
.	O
data	B
,	O
scale	O
=	O
true	O
)	O
we	O
now	O
plot	B
the	O
ﬁrst	O
few	O
principal	O
component	O
score	O
vectors	O
,	O
in	O
order	O
to	O
visualize	O
the	O
data	B
.	O
the	O
observations	B
(	O
cell	O
lines	O
)	O
corresponding	O
to	O
a	O
given	O
cancer	O
type	O
will	O
be	O
plotted	O
in	O
the	O
same	O
color	O
,	O
so	O
that	O
we	O
can	O
see	O
to	O
what	O
extent	O
the	O
observations	B
within	O
a	O
cancer	O
type	O
are	O
similar	O
to	O
each	O
other	O
.	O
we	O
ﬁrst	O
create	O
a	O
simple	B
function	O
that	O
assigns	O
a	O
distinct	O
color	O
to	O
each	O
element	O
of	O
a	O
numeric	O
vector	B
.	O
the	O
function	B
will	O
be	O
used	O
to	O
assign	O
a	O
color	O
to	O
each	O
of	O
the	O
64	O
cell	O
lines	O
,	O
based	O
on	O
the	O
cancer	O
type	O
to	O
which	O
it	O
corresponds	O
.	O
cols	O
=	O
function	B
(	O
vec	O
)	O
{	O
+	O
+	O
+	O
}	O
cols	O
=	O
rainbow	O
(	O
length	O
(	O
unique	O
(	O
vec	O
)	O
)	O
)	O
return	O
(	O
cols	O
[	O
as	O
.	O
numeric	O
(	O
as	O
.	O
factor	B
(	O
vec	O
)	O
)	O
]	O
)	O
note	O
that	O
the	O
rainbow	O
(	O
)	O
function	B
takes	O
as	O
its	O
argument	B
a	O
positive	O
integer	O
,	O
and	O
returns	O
a	O
vector	B
containing	O
that	O
number	O
of	O
distinct	O
colors	O
.	O
we	O
now	O
can	O
plot	B
the	O
principal	O
component	O
score	O
vectors	O
.	O
rainbow	O
(	O
)	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
1	O
,2	O
)	O
)	O
>	O
plot	B
(	O
pr	O
.	O
out	O
$	O
x	O
[	O
,1:2	O
]	O
,	O
col	O
=	O
cols	O
(	O
nci	O
.	O
labs	O
)	O
,	O
pch	O
=19	O
,	O
xlab	O
=	O
''	O
z1	O
``	O
,	O
ylab	O
=	O
''	O
z2	O
``	O
)	O
>	O
plot	B
(	O
pr	O
.	O
out	O
$	O
x	O
[	O
,	O
c	O
(	O
1	O
,3	O
)	O
]	O
,	O
col	O
=	O
cols	O
(	O
nci	O
.	O
labs	O
)	O
,	O
pch	O
=19	O
,	O
xlab	O
=	O
''	O
z1	O
``	O
,	O
ylab	O
=	O
''	O
z3	O
``	O
)	O
the	O
resulting	O
plots	O
are	O
shown	O
in	O
figure	O
10.15.	O
on	O
the	O
whole	O
,	O
cell	O
lines	O
corresponding	O
to	O
a	O
single	B
cancer	O
type	O
do	O
tend	O
to	O
have	O
similar	O
values	O
on	O
the	O
ﬁrst	O
few	O
principal	O
component	O
score	O
vectors	O
.	O
this	O
indicates	O
that	O
cell	O
lines	O
from	O
the	O
same	O
cancer	O
type	O
tend	O
to	O
have	O
pretty	O
similar	O
gene	O
expression	O
levels	O
.	O
we	O
can	O
obtain	O
a	O
summary	O
of	O
the	O
proportion	B
of	I
variance	I
explained	O
(	O
pve	O
)	O
of	O
the	O
ﬁrst	O
few	O
principal	B
components	I
using	O
the	O
summary	O
(	O
)	O
method	O
for	O
a	O
prcomp	O
object	O
(	O
we	O
have	O
truncated	O
the	O
printout	O
)	O
:	O
>	O
summary	O
(	O
pr	O
.	O
out	O
)	O
importanc	O
e	O
of	O
component	O
s	O
:	O
standard	O
deviation	O
proportio	O
n	O
of	O
variance	B
cumulativ	O
e	O
proportio	O
n	O
pc1	O
pc2	O
pc3	O
pc4	O
pc5	O
27.853	O
21.4814	O
19.8205	O
17.0326	O
15.9718	O
0.0374	O
0.3185	O
0.0575	O
0.2387	O
0.0425	O
0.2812	O
0.0676	O
0.1812	O
0.114	O
0.114	O
using	O
the	O
plot	B
(	O
)	O
function	B
,	O
we	O
can	O
also	O
plot	B
the	O
variance	B
explained	O
by	O
the	O
ﬁrst	O
few	O
principal	B
components	I
.	O
>	O
plot	B
(	O
pr	O
.	O
out	O
)	O
note	O
that	O
the	O
height	O
of	O
each	O
bar	O
in	O
the	O
bar	O
plot	B
is	O
given	O
by	O
squaring	O
the	O
corresponding	O
element	O
of	O
pr.out	O
$	O
sdev	O
.	O
however	O
,	O
it	O
is	O
more	O
informative	O
to	O
10.6	O
lab	O
3	O
:	O
nci60	O
data	B
example	O
409	O
2	O
z	O
0	O
2	O
0	O
0	O
2	O
−	O
0	O
4	O
−	O
0	O
6	O
−	O
3	O
z	O
0	O
4	O
0	O
2	O
0	O
0	O
2	O
−	O
0	O
4	O
−	O
−40	O
−20	O
0	O
20	O
40	O
60	O
−40	O
−20	O
0	O
20	O
40	O
60	O
z1	O
z1	O
figure	O
10.15.	O
projections	O
of	O
the	O
nci60	O
cancer	O
cell	O
lines	O
onto	O
the	O
ﬁrst	O
three	O
principal	B
components	I
(	O
in	O
other	O
words	O
,	O
the	O
scores	O
for	O
the	O
ﬁrst	O
three	O
principal	O
com-	O
ponents	O
)	O
.	O
on	O
the	O
whole	O
,	O
observations	B
belonging	O
to	O
a	O
single	B
cancer	O
type	O
tend	O
to	O
lie	O
near	O
each	O
other	O
in	O
this	O
low-dimensional	B
space	O
.	O
it	O
would	O
not	O
have	O
been	O
possible	O
to	O
visualize	O
the	O
data	B
without	O
using	O
a	O
dimension	B
reduction	I
method	O
such	O
as	O
pca	O
,	O
possible	O
scatterplots	O
,	O
none	O
of	O
since	O
based	O
on	O
the	O
full	O
data	B
set	O
there	O
are	O
2	O
which	O
would	O
have	O
been	O
particularly	O
informative	O
.	O
6,830	O
(	O
cid:6	O
)	O
(	O
cid:7	O
)	O
plot	B
the	O
pve	O
of	O
each	O
principal	O
component	O
(	O
i.e	O
.	O
a	O
scree	B
plot	I
)	O
and	O
the	O
cu-	O
mulative	O
pve	O
of	O
each	O
principal	O
component	O
.	O
this	O
can	O
be	O
done	O
with	O
just	O
a	O
little	O
work	O
.	O
>	O
pve	O
=100*	O
pr	O
.	O
out	O
$	O
sdev	O
^2/	O
sum	O
(	O
pr	O
.	O
out	O
$	O
sdev	O
^2	O
)	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
1	O
,2	O
)	O
)	O
>	O
plot	B
(	O
pve	O
,	O
type	O
=	O
''	O
o	O
``	O
,	O
ylab	O
=	O
''	O
pve	O
``	O
,	O
xlab	O
=	O
''	O
principal	O
component	O
``	O
,	O
col	O
=	O
''	O
blue	O
``	O
)	O
>	O
plot	B
(	O
cumsum	O
(	O
pve	O
)	O
,	O
type	O
=	O
''	O
o	O
``	O
,	O
ylab	O
=	O
''	O
cumulative	O
pve	O
``	O
,	O
xlab	O
=	O
''	O
principal	O
component	O
``	O
,	O
col	O
=	O
''	O
brown3	O
``	O
)	O
(	O
note	O
that	O
the	O
elements	O
of	O
pve	O
can	O
also	O
be	O
computed	O
directly	O
from	O
the	O
sum-	O
mary	O
,	O
summary	O
(	O
pr.out	O
)	O
$	O
importance	B
[	O
2	O
,	O
]	O
,	O
and	O
the	O
elements	O
of	O
cumsum	O
(	O
pve	O
)	O
are	O
given	O
by	O
summary	O
(	O
pr.out	O
)	O
$	O
importance	B
[	O
3	O
,	O
]	O
.	O
)	O
the	O
resulting	O
plots	O
are	O
shown	O
in	O
figure	O
10.16.	O
we	O
see	O
that	O
together	O
,	O
the	O
ﬁrst	O
seven	O
principal	B
components	I
explain	O
around	O
40	O
%	O
of	O
the	O
variance	B
in	O
the	O
data	B
.	O
this	O
is	O
not	O
a	O
huge	O
amount	O
of	O
the	O
variance	B
.	O
however	O
,	O
looking	O
at	O
the	O
scree	B
plot	I
,	O
we	O
see	O
that	O
while	O
each	O
of	O
the	O
ﬁrst	O
seven	O
principal	B
components	I
explain	O
a	O
substantial	O
amount	O
of	O
variance	B
,	O
there	O
is	O
a	O
marked	O
decrease	O
in	O
the	O
variance	B
explained	O
by	O
further	O
principal	B
components	I
.	O
that	O
is	O
,	O
there	O
is	O
an	O
elbow	B
in	O
the	O
plot	B
after	O
approx-	O
imately	O
the	O
seventh	O
principal	O
component	O
.	O
this	O
suggests	O
that	O
there	O
may	O
be	O
little	O
beneﬁt	O
to	O
examining	O
more	O
than	O
seven	O
or	O
so	O
principal	B
components	I
(	O
though	O
even	O
examining	O
seven	O
principal	B
components	I
may	O
be	O
diﬃcult	O
)	O
.	O
410	O
10.	O
unsupervised	B
learning	I
e	O
v	O
p	O
0	O
1	O
8	O
6	O
4	O
2	O
0	O
0	O
0	O
1	O
0	O
8	O
0	O
6	O
0	O
4	O
0	O
2	O
e	O
v	O
p	O
e	O
v	O
i	O
t	O
l	O
a	O
u	O
m	O
u	O
c	O
0	O
10	O
20	O
30	O
40	O
50	O
60	O
0	O
10	O
20	O
30	O
40	O
50	O
60	O
principal	O
component	O
principal	O
component	O
figure	O
10.16.	O
the	O
pve	O
of	O
the	O
principal	B
components	I
of	O
the	O
nci60	O
cancer	O
cell	O
line	B
microarray	O
data	B
set	O
.	O
left	O
:	O
the	O
pve	O
of	O
each	O
principal	O
component	O
is	O
shown	O
.	O
right	O
:	O
the	O
cumulative	O
pve	O
of	O
the	O
principal	B
components	I
is	O
shown	O
.	O
together	O
,	O
all	O
principal	B
components	I
explain	O
100	O
%	O
of	O
the	O
variance	B
.	O
10.6.2	O
clustering	B
the	O
observations	B
of	O
the	O
nci60	O
data	B
we	O
now	O
proceed	O
to	O
hierarchically	O
cluster	O
the	O
cell	O
lines	O
in	O
the	O
nci60	O
data	B
,	O
with	O
the	O
goal	O
of	O
ﬁnding	O
out	O
whether	O
or	O
not	O
the	O
observations	B
cluster	O
into	O
distinct	O
types	O
of	O
cancer	O
.	O
to	O
begin	O
,	O
we	O
standardize	B
the	O
variables	O
to	O
have	O
mean	O
zero	O
and	O
standard	O
deviation	O
one	O
.	O
as	O
mentioned	O
earlier	O
,	O
this	O
step	O
is	O
optional	O
and	O
should	O
be	O
performed	O
only	O
if	O
we	O
want	O
each	O
gene	O
to	O
be	O
on	O
the	O
same	O
scale	O
.	O
>	O
sd	O
.	O
data	B
=	O
scale	O
(	O
nci	O
.	O
data	B
)	O
we	O
now	O
perform	O
hierarchical	B
clustering	I
of	O
the	O
observations	B
using	O
complete	B
,	O
single	B
,	O
and	O
average	B
linkage	O
.	O
euclidean	O
distance	B
is	O
used	O
as	O
the	O
dissimilarity	B
measure	O
.	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
1	O
,3	O
)	O
)	O
>	O
data	B
.	O
dist	O
=	O
dist	O
(	O
sd	O
.	O
data	B
)	O
>	O
plot	B
(	O
hclust	O
(	O
data	B
.	O
dist	O
)	O
,	O
labels	O
=	O
nci	O
.	O
labs	O
,	O
main	O
=	O
''	O
complete	B
linkage	O
``	O
,	O
xlab	O
=	O
''	O
''	O
,	O
sub	O
=	O
''	O
''	O
,	O
ylab	O
=	O
''	O
''	O
)	O
>	O
plot	B
(	O
hclust	O
(	O
data	B
.	O
dist	O
,	O
method	O
=	O
''	O
average	B
``	O
)	O
,	O
labels	O
=	O
nci	O
.	O
labs	O
,	O
main	O
=	O
''	O
average	B
linkage	O
``	O
,	O
xlab	O
=	O
''	O
''	O
,	O
sub	O
=	O
''	O
''	O
,	O
ylab	O
=	O
''	O
''	O
)	O
>	O
plot	B
(	O
hclust	O
(	O
data	B
.	O
dist	O
,	O
method	O
=	O
''	O
single	B
``	O
)	O
,	O
labels	O
=	O
nci	O
.	O
labs	O
,	O
main	O
=	O
''	O
single	B
linkage	O
``	O
,	O
xlab	O
=	O
''	O
''	O
,	O
sub	O
=	O
''	O
''	O
,	O
ylab	O
=	O
''	O
''	O
)	O
the	O
results	O
are	O
shown	O
in	O
figure	O
10.17.	O
we	O
see	O
that	O
the	O
choice	O
of	O
linkage	B
certainly	O
does	O
aﬀect	O
the	O
results	O
obtained	O
.	O
typically	O
,	O
single	B
linkage	O
will	O
tend	O
to	O
yield	O
trailing	O
clusters	O
:	O
very	O
large	O
clusters	O
onto	O
which	O
individual	O
observa-	O
tions	O
attach	O
one-by-one	O
.	O
on	O
the	O
other	O
hand	O
,	O
complete	B
and	O
average	B
linkage	O
tend	O
to	O
yield	O
more	O
balanced	O
,	O
attractive	O
clusters	O
.	O
for	O
this	O
reason	O
,	O
complete	B
and	O
average	B
linkage	O
are	O
generally	O
preferred	O
to	O
single	B
linkage	O
.	O
clearly	O
cell	O
lines	O
within	O
a	O
single	B
cancer	O
type	O
do	O
tend	O
to	O
cluster	O
together	O
,	O
although	O
the	O
10.6	O
lab	O
3	O
:	O
nci60	O
data	B
example	O
411	O
complete	B
linkage	O
l	O
a	O
n	O
e	O
r	O
s	O
n	O
c	O
s	O
n	O
c	O
c	O
l	O
c	O
s	O
n	O
t	O
s	O
a	O
e	O
r	O
b	O
t	O
s	O
a	O
e	O
r	O
b	O
t	O
s	O
a	O
e	O
r	O
b	O
i	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
a	O
r	O
a	O
v	O
o	O
c	O
l	O
c	O
s	O
n	O
l	O
a	O
n	O
e	O
r	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
e	O
t	O
a	O
t	O
s	O
o	O
r	O
p	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
l	O
a	O
n	O
e	O
r	O
c	O
l	O
c	O
s	O
n	O
e	O
t	O
a	O
t	O
s	O
o	O
r	O
p	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
s	O
n	O
c	O
c	O
l	O
c	O
s	O
n	O
s	O
n	O
c	O
s	O
n	O
c	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
w	O
o	O
n	O
k	O
n	O
u	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
t	O
s	O
a	O
e	O
r	O
b	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
t	O
s	O
a	O
e	O
r	O
b	O
t	O
s	O
a	O
e	O
r	O
b	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
ol	O
r	O
p	O
e	O
r	O
−	O
a	O
2	O
6	O
5	O
k	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
o	O
r	O
p	O
e	O
r	O
−	O
b	O
2	O
6	O
5	O
k	O
t	O
s	O
a	O
e	O
r	O
b	O
o	O
r	O
p	O
e	O
r	O
−	O
a	O
7	O
f	O
c	O
m	O
o	O
r	O
p	O
e	O
r	O
−	O
d	O
7	O
f	O
c	O
m	O
average	B
linkage	O
l	O
a	O
n	O
e	O
r	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
t	O
s	O
a	O
e	O
r	O
b	O
t	O
s	O
a	O
e	O
r	O
b	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
o	O
r	O
p	O
e	O
r	O
−	O
b	O
2	O
6	O
5	O
k	O
o	O
r	O
p	O
e	O
r	O
−	O
a	O
2	O
6	O
5	O
k	O
t	O
s	O
a	O
e	O
r	O
b	O
o	O
r	O
p	O
e	O
r	O
−	O
a	O
7	O
f	O
c	O
m	O
o	O
r	O
p	O
e	O
r	O
−	O
d	O
7	O
f	O
c	O
m	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
l	O
a	O
n	O
e	O
r	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
c	O
l	O
c	O
s	O
n	O
i	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
a	O
r	O
a	O
v	O
o	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
w	O
o	O
n	O
k	O
n	O
u	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
t	O
s	O
a	O
e	O
r	O
b	O
t	O
s	O
a	O
e	O
r	O
b	O
s	O
n	O
c	O
c	O
l	O
c	O
s	O
n	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
s	O
n	O
c	O
s	O
n	O
c	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
e	O
t	O
a	O
t	O
s	O
o	O
r	O
p	O
c	O
l	O
c	O
s	O
n	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
e	O
t	O
a	O
t	O
s	O
o	O
r	O
p	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
s	O
n	O
c	O
s	O
n	O
c	O
t	O
s	O
a	O
e	O
r	O
b	O
t	O
s	O
a	O
e	O
r	O
b	O
single	B
linkage	O
s	O
n	O
c	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
i	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
c	O
l	O
c	O
s	O
n	O
t	O
s	O
a	O
e	O
r	O
b	O
n	O
o	O
l	O
o	O
c	O
t	O
s	O
a	O
e	O
r	O
b	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
l	O
a	O
n	O
e	O
r	O
t	O
s	O
a	O
e	O
r	O
b	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
s	O
n	O
c	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
i	O
a	O
m	O
e	O
k	O
u	O
e	O
l	O
o	O
r	O
p	O
e	O
r	O
−	O
b	O
2	O
6	O
5	O
k	O
o	O
r	O
p	O
e	O
r	O
−	O
a	O
2	O
6	O
5	O
k	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
n	O
o	O
l	O
o	O
c	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
e	O
t	O
a	O
t	O
s	O
o	O
r	O
p	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
c	O
l	O
c	O
s	O
n	O
l	O
a	O
n	O
e	O
r	O
n	O
o	O
l	O
o	O
c	O
e	O
t	O
a	O
t	O
s	O
o	O
r	O
p	O
n	O
o	O
l	O
o	O
c	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
o	O
l	O
o	O
c	O
n	O
o	O
l	O
o	O
c	O
c	O
l	O
c	O
s	O
n	O
c	O
l	O
c	O
s	O
n	O
l	O
a	O
n	O
e	O
r	O
c	O
l	O
c	O
s	O
n	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
l	O
a	O
n	O
e	O
r	O
s	O
n	O
c	O
s	O
n	O
c	O
s	O
n	O
c	O
l	O
a	O
n	O
e	O
r	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
n	O
o	O
l	O
o	O
c	O
t	O
s	O
a	O
e	O
r	O
b	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
a	O
m	O
o	O
n	O
a	O
l	O
e	O
m	O
t	O
s	O
a	O
e	O
r	O
b	O
t	O
s	O
a	O
e	O
r	O
b	O
i	O
n	O
a	O
r	O
a	O
v	O
o	O
n	O
w	O
o	O
n	O
k	O
n	O
u	O
t	O
s	O
a	O
e	O
r	O
b	O
o	O
r	O
p	O
e	O
r	O
−	O
a	O
7	O
f	O
c	O
m	O
o	O
r	O
p	O
e	O
r	O
−	O
d	O
7	O
f	O
c	O
m	O
0	O
6	O
1	O
0	O
2	O
1	O
0	O
8	O
0	O
4	O
0	O
2	O
1	O
0	O
0	O
1	O
0	O
8	O
0	O
6	O
0	O
4	O
0	O
0	O
1	O
0	O
8	O
0	O
6	O
0	O
4	O
figure	O
10.17.	O
the	O
nci60	O
cancer	O
cell	O
line	B
microarray	O
data	B
,	O
clustered	O
with	O
av-	O
erage	O
,	O
complete	B
,	O
and	O
single	B
linkage	O
,	O
and	O
using	O
euclidean	O
distance	B
as	O
the	O
dissim-	O
ilarity	O
measure	O
.	O
complete	B
and	O
average	B
linkage	O
tend	O
to	O
yield	O
evenly	O
sized	O
clusters	O
whereas	O
single	B
linkage	O
tends	O
to	O
yield	O
extended	O
clusters	O
to	O
which	O
single	B
leaves	O
are	O
fused	O
one	O
by	O
one	O
.	O
412	O
10.	O
unsupervised	B
learning	I
clustering	O
is	O
not	O
perfect	O
.	O
we	O
will	O
use	O
complete	B
linkage	O
hierarchical	B
cluster-	O
ing	O
for	O
the	O
analysis	B
that	O
follows	O
.	O
we	O
can	O
cut	O
the	O
dendrogram	B
at	O
the	O
height	O
that	O
will	O
yield	O
a	O
particular	O
number	O
of	O
clusters	O
,	O
say	O
four	O
:	O
>	O
hc	O
.	O
out	O
=	O
hclust	O
(	O
dist	O
(	O
sd	O
.	O
data	B
)	O
)	O
>	O
hc	O
.	O
clusters	O
=	O
cutree	O
(	O
hc	O
.	O
out	O
,4	O
)	O
>	O
table	O
(	O
hc	O
.	O
clusters	O
,	O
nci	O
.	O
labs	O
)	O
there	O
are	O
some	O
clear	O
patterns	O
.	O
all	O
the	O
leukemia	O
cell	O
lines	O
fall	O
in	O
cluster	O
3	O
,	O
while	O
the	O
breast	O
cancer	O
cell	O
lines	O
are	O
spread	O
out	O
over	O
three	O
diﬀerent	O
clusters	O
.	O
we	O
can	O
plot	B
the	O
cut	O
on	O
the	O
dendrogram	B
that	O
produces	O
these	O
four	O
clusters	O
:	O
>	O
par	O
(	O
mfrow	O
=	O
c	O
(	O
1	O
,1	O
)	O
)	O
>	O
plot	B
(	O
hc	O
.	O
out	O
,	O
labels	O
=	O
nci	O
.	O
labs	O
)	O
>	O
abline	O
(	O
h	O
=139	O
,	O
col	O
=	O
''	O
red	O
``	O
)	O
the	O
abline	O
(	O
)	O
function	B
draws	O
a	O
straight	O
line	B
on	O
top	O
of	O
any	O
existing	O
plot	B
in	O
r.	O
the	O
argument	B
h=139	O
plots	O
a	O
horizontal	O
line	B
at	O
height	O
139	O
on	O
the	O
den-	O
drogram	O
;	O
this	O
is	O
the	O
height	O
that	O
results	O
in	O
four	O
distinct	O
clusters	O
.	O
it	O
is	O
easy	O
to	O
verify	O
that	O
the	O
resulting	O
clusters	O
are	O
the	O
same	O
as	O
the	O
ones	O
we	O
obtained	O
using	O
cutree	O
(	O
hc.out,4	O
)	O
.	O
printing	O
the	O
output	B
of	O
hclust	O
gives	O
a	O
useful	O
brief	O
summary	O
of	O
the	O
object	O
:	O
>	O
hc	O
.	O
out	O
call	O
:	O
hclust	O
(	O
d	O
=	O
dist	O
(	O
dat	O
)	O
)	O
cluster	O
method	O
distance	B
number	O
of	O
objects	O
:	O
64	O
:	O
complete	B
:	O
euclidean	O
we	O
claimed	O
earlier	O
in	O
section	O
10.3.2	O
that	O
k-means	O
clustering	B
and	O
hier-	O
archical	O
clustering	B
with	O
the	O
dendrogram	B
cut	O
to	O
obtain	O
the	O
same	O
number	O
of	O
clusters	O
can	O
yield	O
very	O
diﬀerent	O
results	O
.	O
how	O
do	O
these	O
nci60	O
hierarchical	B
clustering	I
results	O
compare	O
to	O
what	O
we	O
get	O
if	O
we	O
perform	O
k-means	O
clustering	B
with	O
k	O
=	O
4	O
?	O
>	O
set	B
.	O
seed	B
(	O
2	O
)	O
>	O
km	O
.	O
out	O
=	O
kmeans	O
(	O
sd	O
.	O
data	B
,	O
4	O
,	O
nstart	O
=20	O
)	O
>	O
km	O
.	O
clusters	O
=	O
km	O
.	O
out	O
$	O
cluste	O
r	O
>	O
table	O
(	O
km	O
.	O
clusters	O
,	O
hc	O
.	O
clusters	O
)	O
hc	O
.	O
clusters	O
km	O
.	O
clusters	O
1	O
1	O
11	O
0	O
2	O
3	O
9	O
4	O
20	O
2	O
0	O
0	O
0	O
7	O
3	O
0	O
8	O
0	O
0	O
4	O
9	O
0	O
0	O
0	O
we	O
see	O
that	O
the	O
four	O
clusters	O
obtained	O
using	O
hierarchical	B
clustering	I
and	O
k-	O
means	O
clustering	B
are	O
somewhat	O
diﬀerent	O
.	O
cluster	O
2	O
in	O
k-means	O
clustering	B
is	O
identical	O
to	O
cluster	O
3	O
in	O
hierarchical	B
clustering	I
.	O
however	O
,	O
the	O
other	O
clusters	O
10.7	O
exercises	O
413	O
diﬀer	O
:	O
for	O
instance	O
,	O
cluster	O
4	O
in	O
k-means	O
clustering	B
contains	O
a	O
portion	O
of	O
the	O
observations	B
assigned	O
to	O
cluster	O
1	O
by	O
hierarchical	B
clustering	I
,	O
as	O
well	O
as	O
all	O
of	O
the	O
observations	B
assigned	O
to	O
cluster	O
2	O
by	O
hierarchical	B
clustering	I
.	O
rather	O
than	O
performing	O
hierarchical	B
clustering	I
on	O
the	O
entire	O
data	B
matrix	O
,	O
we	O
can	O
simply	O
perform	O
hierarchical	B
clustering	I
on	O
the	O
ﬁrst	O
few	O
principal	O
component	O
score	O
vectors	O
,	O
as	O
follows	O
:	O
>	O
hc	O
.	O
out	O
=	O
hclust	O
(	O
dist	O
(	O
pr	O
.	O
out	O
$	O
x	O
[	O
,1:5	O
]	O
)	O
)	O
>	O
plot	B
(	O
hc	O
.	O
out	O
,	O
labels	O
=	O
nci	O
.	O
labs	O
,	O
main	O
=	O
''	O
hier	O
.	O
clust	O
.	O
on	O
first	O
five	O
score	O
vectors	O
``	O
)	O
>	O
table	O
(	O
cutree	O
(	O
hc	O
.	O
out	O
,4	O
)	O
,	O
nci	O
.	O
labs	O
)	O
not	O
surprisingly	O
,	O
these	O
results	O
are	O
diﬀerent	O
from	O
the	O
ones	O
that	O
we	O
obtained	O
when	O
we	O
performed	O
hierarchical	B
clustering	I
on	O
the	O
full	O
data	B
set	O
.	O
sometimes	O
performing	O
clustering	B
on	O
the	O
ﬁrst	O
few	O
principal	O
component	O
score	O
vectors	O
can	O
give	O
better	O
results	O
than	O
performing	O
clustering	B
on	O
the	O
full	O
data	B
.	O
in	O
this	O
situation	O
,	O
we	O
might	O
view	O
the	O
principal	O
component	O
step	O
as	O
one	O
of	O
denois-	O
ing	O
the	O
data	B
.	O
we	O
could	O
also	O
perform	O
k-means	O
clustering	B
on	O
the	O
ﬁrst	O
few	O
principal	O
component	O
score	O
vectors	O
rather	O
than	O
the	O
full	O
data	B
set	O
.	O
10.7	O
exercises	O
conceptual	O
1.	O
this	O
problem	O
involves	O
the	O
k-means	O
clustering	B
algorithm	O
.	O
(	O
a	O
)	O
prove	O
(	O
10.12	O
)	O
.	O
(	O
b	O
)	O
on	O
the	O
basis	B
of	O
this	O
identity	O
,	O
argue	O
that	O
the	O
k-means	O
clustering	B
algorithm	O
(	O
algorithm	O
10.1	O
)	O
decreases	O
the	O
objective	O
(	O
10.11	O
)	O
at	O
each	O
iteration	O
.	O
2.	O
suppose	O
that	O
we	O
have	O
four	O
observations	B
,	O
for	O
which	O
we	O
compute	O
a	O
dissimilarity	B
matrix	O
,	O
given	O
by	O
⎡	O
⎢⎢⎣	O
0.3	O
0.3	O
0.4	O
0.5	O
0.7	O
0.8	O
0.4	O
0.5	O
0.45	O
⎤	O
⎥⎥⎦	O
.	O
0.7	O
0.8	O
0.45	O
for	O
instance	O
,	O
the	O
dissimilarity	B
between	O
the	O
ﬁrst	O
and	O
second	O
obser-	O
vations	O
is	O
0.3	O
,	O
and	O
the	O
dissimilarity	B
between	O
the	O
second	O
and	O
fourth	O
observations	B
is	O
0.8	O
.	O
(	O
a	O
)	O
on	O
the	O
basis	B
of	O
this	O
dissimilarity	B
matrix	O
,	O
sketch	O
the	O
dendrogram	B
that	O
results	O
from	O
hierarchically	O
clustering	B
these	O
four	O
observa-	O
tions	O
using	O
complete	B
linkage	O
.	O
be	O
sure	O
to	O
indicate	O
on	O
the	O
plot	B
the	O
height	O
at	O
which	O
each	O
fusion	O
occurs	O
,	O
as	O
well	O
as	O
the	O
observations	B
corresponding	O
to	O
each	O
leaf	B
in	O
the	O
dendrogram	B
.	O
414	O
10.	O
unsupervised	B
learning	I
(	O
b	O
)	O
repeat	O
(	O
a	O
)	O
,	O
this	O
time	O
using	O
single	B
linkage	O
clustering	B
.	O
(	O
c	O
)	O
suppose	O
that	O
we	O
cut	O
the	O
dendogram	O
obtained	O
in	O
(	O
a	O
)	O
such	O
that	O
two	O
clusters	O
result	O
.	O
which	O
observations	B
are	O
in	O
each	O
cluster	O
?	O
(	O
d	O
)	O
suppose	O
that	O
we	O
cut	O
the	O
dendogram	O
obtained	O
in	O
(	O
b	O
)	O
such	O
that	O
two	O
clusters	O
result	O
.	O
which	O
observations	B
are	O
in	O
each	O
cluster	O
?	O
(	O
e	O
)	O
it	O
is	O
mentioned	O
in	O
the	O
chapter	O
that	O
at	O
each	O
fusion	O
in	O
the	O
den-	O
drogram	O
,	O
the	O
position	O
of	O
the	O
two	O
clusters	O
being	O
fused	O
can	O
be	O
swapped	O
without	O
changing	O
the	O
meaning	O
of	O
the	O
dendrogram	B
.	O
draw	O
a	O
dendrogram	B
that	O
is	O
equivalent	O
to	O
the	O
dendrogram	B
in	O
(	O
a	O
)	O
,	O
for	O
which	O
two	O
or	O
more	O
of	O
the	O
leaves	O
are	O
repositioned	O
,	O
but	O
for	O
which	O
the	O
meaning	O
of	O
the	O
dendrogram	B
is	O
the	O
same	O
.	O
3.	O
in	O
this	O
problem	O
,	O
you	O
will	O
perform	O
k-means	O
clustering	B
manually	O
,	O
with	O
k	O
=	O
2	O
,	O
on	O
a	O
small	O
example	O
with	O
n	O
=	O
6	O
observations	B
and	O
p	O
=	O
2	O
features	O
.	O
the	O
observations	B
are	O
as	O
follows	O
.	O
obs	O
.	O
x1	O
x2	B
4	O
3	O
4	O
1	O
2	O
0	O
1	O
1	O
0	O
5	O
6	O
4	O
1	O
2	O
3	O
4	O
5	O
6	O
(	O
a	O
)	O
plot	B
the	O
observations	B
.	O
(	O
b	O
)	O
randomly	O
assign	O
a	O
cluster	O
label	O
to	O
each	O
observation	O
.	O
you	O
can	O
use	O
the	O
sample	O
(	O
)	O
command	O
in	O
r	O
to	O
do	O
this	O
.	O
report	O
the	O
cluster	O
labels	O
for	O
each	O
observation	O
.	O
(	O
c	O
)	O
compute	O
the	O
centroid	B
for	O
each	O
cluster	O
.	O
(	O
d	O
)	O
assign	O
each	O
observation	O
to	O
the	O
centroid	B
to	O
which	O
it	O
is	O
closest	O
,	O
in	O
terms	O
of	O
euclidean	O
distance	B
.	O
report	O
the	O
cluster	O
labels	O
for	O
each	O
observation	O
.	O
(	O
e	O
)	O
repeat	O
(	O
c	O
)	O
and	O
(	O
d	O
)	O
until	O
the	O
answers	O
obtained	O
stop	O
changing	O
.	O
(	O
f	O
)	O
in	O
your	O
plot	B
from	O
(	O
a	O
)	O
,	O
color	O
the	O
observations	B
according	O
to	O
the	O
cluster	O
labels	O
obtained	O
.	O
4.	O
suppose	O
that	O
for	O
a	O
particular	O
data	B
set	O
,	O
we	O
perform	O
hierarchical	B
clus-	O
tering	O
using	O
single	B
linkage	O
and	O
using	O
complete	B
linkage	O
.	O
we	O
obtain	O
two	O
dendrograms	O
.	O
(	O
a	O
)	O
at	O
a	O
certain	O
point	O
on	O
the	O
single	B
linkage	O
dendrogram	B
,	O
the	O
clus-	O
ters	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
and	O
{	O
4	O
,	O
5	O
}	O
fuse	O
.	O
on	O
the	O
complete	B
linkage	O
dendro-	O
gram	O
,	O
the	O
clusters	O
{	O
1	O
,	O
2	O
,	O
3	O
}	O
and	O
{	O
4	O
,	O
5	O
}	O
also	O
fuse	O
at	O
a	O
certain	O
point	O
.	O
which	O
fusion	O
will	O
occur	O
higher	O
on	O
the	O
tree	B
,	O
or	O
will	O
they	O
fuse	O
at	O
the	O
same	O
height	O
,	O
or	O
is	O
there	O
not	O
enough	O
information	O
to	O
tell	O
?	O
10.7	O
exercises	O
415	O
(	O
b	O
)	O
at	O
a	O
certain	O
point	O
on	O
the	O
single	B
linkage	O
dendrogram	B
,	O
the	O
clusters	O
{	O
5	O
}	O
and	O
{	O
6	O
}	O
fuse	O
.	O
on	O
the	O
complete	B
linkage	O
dendrogram	B
,	O
the	O
clus-	O
ters	O
{	O
5	O
}	O
and	O
{	O
6	O
}	O
also	O
fuse	O
at	O
a	O
certain	O
point	O
.	O
which	O
fusion	O
will	O
occur	O
higher	O
on	O
the	O
tree	B
,	O
or	O
will	O
they	O
fuse	O
at	O
the	O
same	O
height	O
,	O
or	O
is	O
there	O
not	O
enough	O
information	O
to	O
tell	O
?	O
5.	O
in	O
words	O
,	O
describe	O
the	O
results	O
that	O
you	O
would	O
expect	O
if	O
you	O
performed	O
k-means	O
clustering	B
of	O
the	O
eight	O
shoppers	O
in	O
figure	O
10.14	O
,	O
on	O
the	O
basis	B
of	O
their	O
sock	O
and	O
computer	O
purchases	O
,	O
with	O
k	O
=	O
2.	O
give	O
three	O
answers	O
,	O
one	O
for	O
each	O
of	O
the	O
variable	B
scalings	O
displayed	O
.	O
explain	O
.	O
6.	O
a	O
researcher	O
collects	O
expression	O
measurements	O
for	O
1,000	O
genes	O
in	O
100	O
tissue	O
samples	O
.	O
the	O
data	B
can	O
be	O
written	O
as	O
a	O
1	O
,	O
000	O
×	O
100	O
matrix	O
,	O
which	O
we	O
call	O
x	O
,	O
in	O
which	O
each	O
row	O
represents	O
a	O
gene	O
and	O
each	O
col-	O
umn	O
a	O
tissue	O
sample	O
.	O
each	O
tissue	O
sample	O
was	O
processed	O
on	O
a	O
diﬀerent	O
day	O
,	O
and	O
the	O
columns	O
of	O
x	O
are	O
ordered	O
so	O
that	O
the	O
samples	O
that	O
were	O
processed	O
earliest	O
are	O
on	O
the	O
left	O
,	O
and	O
the	O
samples	O
that	O
were	O
processed	O
later	O
are	O
on	O
the	O
right	O
.	O
the	O
tissue	O
samples	O
belong	O
to	O
two	O
groups	O
:	O
con-	O
trol	O
(	O
c	O
)	O
and	O
treatment	O
(	O
t	O
)	O
.	O
the	O
c	O
and	O
t	O
samples	O
were	O
processed	O
in	O
a	O
random	O
order	O
across	O
the	O
days	O
.	O
the	O
researcher	O
wishes	O
to	O
deter-	O
mine	O
whether	O
each	O
gene	O
’	O
s	O
expression	O
measurements	O
diﬀer	O
between	O
the	O
treatment	O
and	O
control	O
groups	O
.	O
as	O
a	O
pre-analysis	O
(	O
before	O
comparing	O
t	O
versus	O
c	O
)	O
,	O
the	O
researcher	O
per-	O
forms	O
a	O
principal	O
component	O
analysis	O
of	O
the	O
data	B
,	O
and	O
ﬁnds	O
that	O
the	O
ﬁrst	O
principal	O
component	O
(	O
a	O
vector	B
of	O
length	O
100	O
)	O
has	O
a	O
strong	O
linear	B
trend	O
from	O
left	O
to	O
right	O
,	O
and	O
explains	O
10	O
%	O
of	O
the	O
variation	O
.	O
the	O
re-	O
searcher	O
now	O
remembers	O
that	O
each	O
patient	O
sample	O
was	O
run	O
on	O
one	O
of	O
two	O
machines	O
,	O
a	O
and	O
b	O
,	O
and	O
machine	B
a	O
was	O
used	O
more	O
often	O
in	O
the	O
earlier	O
times	O
while	O
b	O
was	O
used	O
more	O
often	O
later	O
.	O
the	O
researcher	O
has	O
a	O
record	O
of	O
which	O
sample	O
was	O
run	O
on	O
which	O
machine	B
.	O
(	O
a	O
)	O
explain	O
what	O
it	O
means	O
that	O
the	O
ﬁrst	O
principal	O
component	O
“	O
ex-	O
plains	O
10	O
%	O
of	O
the	O
variation	O
”	O
.	O
(	O
b	O
)	O
the	O
researcher	O
decides	O
to	O
replace	O
the	O
(	O
j	O
,	O
i	O
)	O
th	O
element	O
of	O
x	O
with	O
xji	O
−	O
φj1zi1	O
where	O
zi1	O
is	O
the	O
ith	O
score	O
,	O
and	O
φj1	O
is	O
the	O
jth	O
loading	O
,	O
for	O
the	O
ﬁrst	O
principal	O
component	O
.	O
he	O
will	O
then	O
perform	O
a	O
two-sample	O
t-test	O
on	O
each	O
gene	O
in	O
this	O
new	O
data	B
set	O
in	O
order	O
to	O
determine	O
whether	O
its	O
expression	O
diﬀers	O
between	O
the	O
two	O
conditions	O
.	O
critique	O
this	O
idea	O
,	O
and	O
suggest	O
a	O
better	O
approach	B
.	O
(	O
the	O
principal	O
component	O
analysis	B
is	O
performed	O
on	O
xt	O
)	O
.	O
(	O
c	O
)	O
design	O
and	O
run	O
a	O
small	O
simulation	O
experiment	O
to	O
demonstrate	O
the	O
superiority	O
of	O
your	O
idea	O
.	O
416	O
10.	O
unsupervised	B
learning	I
applied	O
7.	O
in	O
the	O
chapter	O
,	O
we	O
mentioned	O
the	O
use	O
of	O
correlation-based	B
distance	O
and	O
euclidean	O
distance	B
as	O
dissimilarity	B
measures	O
for	O
hierarchical	O
clus-	O
tering	O
.	O
it	O
turns	O
out	O
that	O
these	O
two	O
measures	O
are	O
almost	O
equivalent	O
:	O
if	O
each	O
observation	O
has	O
been	O
centered	O
to	O
have	O
mean	O
zero	O
and	O
standard	O
deviation	O
one	O
,	O
and	O
if	O
we	O
let	O
rij	O
denote	O
the	O
correlation	B
between	O
the	O
ith	O
and	O
jth	O
observations	B
,	O
then	O
the	O
quantity	O
1−	O
rij	O
is	O
proportional	O
to	O
the	O
squared	O
euclidean	O
distance	B
between	O
the	O
ith	O
and	O
jth	O
observations	B
.	O
on	O
the	O
usarrests	O
data	B
,	O
show	O
that	O
this	O
proportionality	O
holds	O
.	O
hint	O
:	O
the	O
euclidean	O
distance	B
can	O
be	O
calculated	O
using	O
the	O
dist	O
(	O
)	O
func-	O
tion	O
,	O
and	O
correlations	O
can	O
be	O
calculated	O
using	O
the	O
cor	O
(	O
)	O
function	B
.	O
8.	O
in	O
section	O
10.2.3	O
,	O
a	O
formula	O
for	O
calculating	O
pve	O
was	O
given	O
in	O
equa-	O
tion	O
10.8.	O
we	O
also	O
saw	O
that	O
the	O
pve	O
can	O
be	O
obtained	O
using	O
the	O
sdev	O
output	B
of	O
the	O
prcomp	O
(	O
)	O
function	B
.	O
on	O
the	O
usarrests	O
data	B
,	O
calculate	O
pve	O
in	O
two	O
ways	O
:	O
(	O
a	O
)	O
using	O
the	O
sdev	O
output	B
of	O
the	O
prcomp	O
(	O
)	O
function	B
,	O
as	O
was	O
done	O
in	O
section	O
10.2.3	O
.	O
(	O
b	O
)	O
by	O
applying	O
equation	O
10.8	O
directly	O
.	O
that	O
is	O
,	O
use	O
the	O
prcomp	O
(	O
)	O
function	B
to	O
compute	O
the	O
principal	O
component	O
loadings	O
.	O
then	O
,	O
use	O
those	O
loadings	O
in	O
equation	O
10.8	O
to	O
obtain	O
the	O
pve	O
.	O
these	O
two	O
approaches	O
should	O
give	O
the	O
same	O
results	O
.	O
hint	O
:	O
you	O
will	O
only	O
obtain	O
the	O
same	O
results	O
in	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
if	O
the	O
same	O
data	B
is	O
used	O
in	O
both	O
cases	O
.	O
for	O
instance	O
,	O
if	O
in	O
(	O
a	O
)	O
you	O
performed	O
prcomp	O
(	O
)	O
using	O
centered	O
and	O
scaled	O
variables	O
,	O
then	O
you	O
must	O
center	O
and	O
scale	O
the	O
variables	O
before	O
applying	O
equation	O
10.3	O
in	O
(	O
b	O
)	O
.	O
9.	O
consider	O
the	O
usarrests	O
data	B
.	O
we	O
will	O
now	O
perform	O
hierarchical	B
clus-	O
tering	O
on	O
the	O
states	O
.	O
(	O
a	O
)	O
using	O
hierarchical	B
clustering	I
with	O
complete	B
linkage	O
and	O
euclidean	O
distance	B
,	O
cluster	O
the	O
states	O
.	O
(	O
b	O
)	O
cut	O
the	O
dendrogram	B
at	O
a	O
height	O
that	O
results	O
in	O
three	O
distinct	O
clusters	O
.	O
which	O
states	O
belong	O
to	O
which	O
clusters	O
?	O
(	O
c	O
)	O
hierarchically	O
cluster	O
the	O
states	O
using	O
complete	B
linkage	O
and	O
eu-	O
clidean	O
distance	B
,	O
after	O
scaling	O
the	O
variables	O
to	O
have	O
standard	O
de-	O
viation	O
one	O
.	O
(	O
d	O
)	O
what	O
eﬀect	O
does	O
scaling	O
the	O
variables	O
have	O
on	O
the	O
hierarchical	B
clustering	I
obtained	O
?	O
in	O
your	O
opinion	O
,	O
should	O
the	O
variables	O
be	O
scaled	O
before	O
the	O
inter-observation	O
dissimilarities	O
are	O
computed	O
?	O
provide	O
a	O
justiﬁcation	O
for	O
your	O
answer	O
.	O
10.	O
in	O
this	O
problem	O
,	O
you	O
will	O
generate	O
simulated	O
data	B
,	O
and	O
then	O
perform	O
pca	O
and	O
k-means	O
clustering	B
on	O
the	O
data	B
.	O
10.7	O
exercises	O
417	O
(	O
a	O
)	O
generate	O
a	O
simulated	O
data	B
set	O
with	O
20	O
observations	B
in	O
each	O
of	O
three	O
classes	O
(	O
i.e	O
.	O
60	O
observations	B
total	O
)	O
,	O
and	O
50	O
variables	O
.	O
hint	O
:	O
there	O
are	O
a	O
number	O
of	O
functions	O
in	O
r	O
that	O
you	O
can	O
use	O
to	O
generate	O
data	B
.	O
one	O
example	O
is	O
the	O
rnorm	O
(	O
)	O
function	B
;	O
runif	O
(	O
)	O
is	O
another	O
option	O
.	O
be	O
sure	O
to	O
add	O
a	O
mean	O
shift	O
to	O
the	O
observations	B
in	O
each	O
class	O
so	O
that	O
there	O
are	O
three	O
distinct	O
classes	O
.	O
(	O
b	O
)	O
perform	O
pca	O
on	O
the	O
60	O
observations	B
and	O
plot	B
the	O
ﬁrst	O
two	O
prin-	O
cipal	O
component	O
score	O
vectors	O
.	O
use	O
a	O
diﬀerent	O
color	O
to	O
indicate	O
the	O
observations	B
in	O
each	O
of	O
the	O
three	O
classes	O
.	O
if	O
the	O
three	O
classes	O
appear	O
separated	O
in	O
this	O
plot	B
,	O
then	O
continue	O
on	O
to	O
part	O
(	O
c	O
)	O
.	O
if	O
not	O
,	O
then	O
return	O
to	O
part	O
(	O
a	O
)	O
and	O
modify	O
the	O
simulation	O
so	O
that	O
there	O
is	O
greater	O
separation	O
between	O
the	O
three	O
classes	O
.	O
do	O
not	O
continue	O
to	O
part	O
(	O
c	O
)	O
until	O
the	O
three	O
classes	O
show	O
at	O
least	O
some	O
separation	O
in	O
the	O
ﬁrst	O
two	O
principal	O
component	O
score	O
vectors	O
.	O
(	O
c	O
)	O
perform	O
k-means	O
clustering	B
of	O
the	O
observations	B
with	O
k	O
=	O
3.	O
how	O
well	O
do	O
the	O
clusters	O
that	O
you	O
obtained	O
in	O
k-means	O
cluster-	O
ing	O
compare	O
to	O
the	O
true	O
class	O
labels	O
?	O
hint	O
:	O
you	O
can	O
use	O
the	O
table	O
(	O
)	O
function	B
in	O
r	O
to	O
compare	O
the	O
true	O
class	O
labels	O
to	O
the	O
class	O
labels	O
obtained	O
by	O
clustering	B
.	O
be	O
careful	O
how	O
you	O
interpret	O
the	O
results	O
:	O
k-means	O
clustering	B
will	O
arbitrarily	O
number	O
the	O
clusters	O
,	O
so	O
you	O
can	O
not	O
simply	O
check	O
whether	O
the	O
true	O
class	O
labels	O
and	O
clustering	B
labels	O
are	O
the	O
same	O
.	O
(	O
d	O
)	O
perform	O
k-means	O
clustering	B
with	O
k	O
=	O
2.	O
describe	O
your	O
results	O
.	O
(	O
e	O
)	O
now	O
perform	O
k-means	O
clustering	B
with	O
k	O
=	O
4	O
,	O
and	O
describe	O
your	O
results	O
.	O
(	O
f	O
)	O
now	O
perform	O
k-means	O
clustering	B
with	O
k	O
=	O
3	O
on	O
the	O
ﬁrst	O
two	O
principal	O
component	O
score	O
vectors	O
,	O
rather	O
than	O
on	O
the	O
raw	O
data	B
.	O
that	O
is	O
,	O
perform	O
k-means	O
clustering	B
on	O
the	O
60	O
×	O
2	O
matrix	O
of	O
which	O
the	O
ﬁrst	O
column	O
is	O
the	O
ﬁrst	O
principal	O
component	O
score	B
vector	I
,	O
and	O
the	O
second	O
column	O
is	O
the	O
second	O
principal	O
component	O
score	B
vector	I
.	O
comment	O
on	O
the	O
results	O
.	O
(	O
g	O
)	O
using	O
the	O
scale	O
(	O
)	O
function	B
,	O
perform	O
k-means	O
clustering	B
with	O
k	O
=	O
3	O
on	O
the	O
data	B
after	O
scaling	O
each	O
variable	B
to	O
have	O
standard	O
deviation	O
one	O
.	O
how	O
do	O
these	O
results	O
compare	O
to	O
those	O
obtained	O
in	O
(	O
b	O
)	O
?	O
explain	O
.	O
11.	O
on	O
the	O
book	O
website	O
,	O
www.statlearning.com	O
,	O
there	O
is	O
a	O
gene	O
expres-	O
sion	O
data	B
set	O
(	O
ch10ex11.csv	O
)	O
that	O
consists	O
of	O
40	O
tissue	O
samples	O
with	O
measurements	O
on	O
1,000	O
genes	O
.	O
the	O
ﬁrst	O
20	O
samples	O
are	O
from	O
healthy	O
patients	O
,	O
while	O
the	O
second	O
20	O
are	O
from	O
a	O
diseased	O
group	O
.	O
418	O
10.	O
unsupervised	B
learning	I
(	O
a	O
)	O
load	O
in	O
the	O
data	B
using	O
read.csv	O
(	O
)	O
.	O
you	O
will	O
need	O
to	O
select	O
header=f	O
.	O
(	O
b	O
)	O
apply	O
hierarchical	B
clustering	I
to	O
the	O
samples	O
using	O
correlation-	O
based	O
distance	B
,	O
and	O
plot	B
the	O
dendrogram	B
.	O
do	O
the	O
genes	O
separate	O
the	O
samples	O
into	O
the	O
two	O
groups	O
?	O
do	O
your	O
results	O
depend	O
on	O
the	O
type	O
of	O
linkage	B
used	O
?	O
(	O
c	O
)	O
your	O
collaborator	O
wants	O
to	O
know	O
which	O
genes	O
diﬀer	O
the	O
most	O
across	O
the	O
two	O
groups	O
.	O
suggest	O
a	O
way	O
to	O
answer	O
this	O
question	O
,	O
and	O
apply	O
it	O
here	O
.	O
index	O
cp	O
,	O
78	O
,	O
205	O
,	O
206	O
,	O
210–213	O
r2	O
,	O
68–71	O
,	O
79–80	O
,	O
103	O
,	O
212	O
(	O
cid:2	O
)	O
2	O
norm	O
,	O
216	O
(	O
cid:2	O
)	O
1	O
norm	O
,	O
219	O
additive	B
,	O
12	O
,	O
86–90	O
,	O
104	O
additivity	B
,	O
282	O
,	O
283	O
adjusted	O
r2	O
,	O
78	O
,	O
205	O
,	O
206	O
,	O
210–213	O
advertising	O
data	B
set	O
,	O
15	O
,	O
16	O
,	O
20	O
,	O
59	O
,	O
61–63	O
,	O
68	O
,	O
69	O
,	O
71–76	O
,	O
79	O
,	O
81	O
,	O
82	O
,	O
87	O
,	O
88	O
,	O
102–104	O
agglomerative	B
clustering	I
,	O
390	O
akaike	O
information	O
criterion	O
,	O
78	O
,	O
205	O
,	O
206	O
,	O
210–213	O
alternative	B
hypothesis	I
,	O
67	O
analysis	B
of	I
variance	I
,	O
290	O
area	B
under	I
the	I
curve	I
,	O
147	O
argument	B
,	O
42	O
auc	O
,	O
147	O
auto	O
data	B
set	O
,	O
14	O
,	O
48	O
,	O
49	O
,	O
56	O
,	O
90–93	O
,	O
121	O
,	O
122	O
,	O
171	O
,	O
176–178	O
,	O
180	O
,	O
182	O
,	O
191	O
,	O
193–195	O
,	O
299	O
,	O
371	O
backﬁtting	B
,	O
284	O
,	O
300	O
backward	B
stepwise	I
selection	I
,	O
79	O
,	O
208–209	O
,	O
247	O
bagging	B
,	O
12	O
,	O
26	O
,	O
303	O
,	O
316–319	O
,	O
328–330	O
baseline	B
,	O
86	O
basis	B
function	I
,	O
270	O
,	O
273	O
bayes	O
classiﬁer	B
,	O
37–40	O
,	O
139	O
decision	B
boundary	I
,	O
140	O
error	B
,	O
37–40	O
bayes	O
’	O
theorem	O
,	O
138	O
,	O
139	O
,	O
226	O
bayesian	O
,	O
226–227	O
bayesian	O
information	O
criterion	O
,	O
78	O
,	O
205	O
,	O
206	O
,	O
210–213	O
best	B
subset	I
selection	I
,	O
205	O
,	O
221	O
,	O
244–247	O
bias	B
,	O
33–36	O
,	O
65	O
,	O
82	O
bias-variance	B
decomposition	O
,	O
34	O
trade-oﬀ	B
,	O
33–37	O
,	O
42	O
,	O
105	O
,	O
149	O
,	O
217	O
,	O
230	O
,	O
239	O
,	O
243	O
,	O
278	O
,	O
307	O
,	O
347	O
,	O
357	O
binary	B
,	O
28	O
,	O
130	O
biplot	B
,	O
377	O
,	O
378	O
g.	O
james	O
et	O
al.	O
,	O
an	O
introduction	O
to	O
statistical	O
learning	O
:	O
with	O
applications	O
in	O
r	O
,	O
springer	O
texts	O
in	O
statistics	O
,	O
doi	O
10.1007/978-1-4614-7138-7	O
,	O
©	O
springer	O
science+business	O
media	O
new	O
york	O
2013	O