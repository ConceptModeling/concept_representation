c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
gaussian	O
processes	O
for	O
machine	O
learning	B
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
adaptive	O
computation	O
and	O
machine	O
learning	B
thomas	O
dietterich	O
,	O
editor	O
christopher	O
bishop	O
,	O
david	O
heckerman	O
,	O
michael	O
jordan	O
,	O
and	O
michael	O
kearns	O
,	O
associate	O
editors	O
bioinformatics	O
:	O
the	O
machine	O
learning	B
approach	O
,	O
pierre	O
baldi	O
and	O
søren	O
brunak	O
reinforcement	O
learning	B
:	O
an	O
introduction	O
,	O
richard	O
s.	O
sutton	O
and	O
andrew	O
g.	O
barto	O
graphical	O
models	O
for	O
machine	O
learning	B
and	O
digital	O
communication	O
,	O
brendan	O
j.	O
frey	O
learning	B
in	O
graphical	O
models	O
,	O
michael	O
i.	O
jordan	O
causation	O
,	O
prediction	B
,	O
and	O
search	O
,	O
second	O
edition	O
,	O
peter	O
spirtes	O
,	O
clark	O
glymour	O
,	O
and	O
richard	O
scheines	O
principles	O
of	O
data	O
mining	O
,	O
david	O
hand	O
,	O
heikki	O
mannila	O
,	O
and	O
padhraic	O
smyth	O
bioinformatics	O
:	O
the	O
machine	O
learning	B
approach	O
,	O
second	O
edition	O
,	O
pierre	O
baldi	O
and	O
søren	O
brunak	O
learning	B
kernel	O
classiﬁers	O
:	O
theory	O
and	O
algorithms	O
,	O
ralf	O
herbrich	O
learning	B
with	O
kernels	O
:	O
support	B
vector	I
machines	O
,	O
regularization	B
,	O
optimization	O
,	O
and	O
beyond	O
,	O
bernhard	O
sch¨olkopf	O
and	O
alexander	O
j.	O
smola	O
introduction	O
to	O
machine	O
learning	B
,	O
ethem	O
alpaydin	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
carl	O
edward	O
rasmussen	O
and	O
christopher	O
k.	O
i.	O
williams	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
gaussian	O
processes	O
for	O
machine	O
learning	B
carl	O
edward	O
rasmussen	O
christopher	O
k.	O
i.	O
williams	O
the	O
mit	O
press	O
cambridge	O
,	O
massachusetts	O
london	O
,	O
england	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
all	O
rights	O
reserved	O
.	O
no	O
part	O
of	O
this	O
book	O
may	O
be	O
reproduced	O
in	O
any	O
form	O
by	O
any	O
electronic	O
or	O
mechanical	O
means	O
(	O
including	O
photocopying	O
,	O
recording	O
,	O
or	O
information	O
storage	O
and	O
retrieval	O
)	O
without	O
permission	O
in	O
writing	O
from	O
the	O
publisher	O
.	O
mit	O
press	O
books	O
may	O
be	O
purchased	O
at	O
special	O
quantity	O
discounts	O
for	O
business	O
or	O
sales	O
promotional	O
use	O
.	O
for	O
information	O
,	O
please	O
email	O
special	O
sales	O
@	O
mitpress.mit.edu	O
or	O
write	O
to	O
special	O
sales	O
department	O
,	O
the	O
mit	O
press	O
,	O
55	O
hayward	O
street	O
,	O
cambridge	O
,	O
ma	O
02142.	O
typeset	O
by	O
the	O
authors	O
using	O
latex	O
2ε	O
.	O
this	O
book	O
was	O
printed	O
and	O
bound	O
in	O
the	O
united	O
states	O
of	O
america	O
.	O
library	O
of	O
congress	O
cataloging-in-publication	O
data	O
rasmussen	O
,	O
carl	O
edward	O
.	O
gaussian	O
processes	O
for	O
machine	O
learning	B
/	O
carl	O
edward	O
rasmussen	O
,	O
christopher	O
k.	O
i.	O
williams	O
.	O
p.	O
cm	O
.	O
—	O
(	O
adaptive	O
computation	O
and	O
machine	O
learning	B
)	O
includes	O
bibliographical	O
references	O
and	O
indexes	O
.	O
isbn	O
0-262-18253-x	O
1.	O
gaussian	O
processes—data	O
processing	O
.	O
2.	O
machine	O
learning—mathematical	O
models	O
.	O
i.	O
williams	O
,	O
christopher	O
k.	O
i.	O
ii	O
.	O
title	O
.	O
iii	O
.	O
series	O
.	O
qa274.4.r37	O
2006	O
519.2'3—dc22	O
10	O
9	O
8	O
7	O
6	O
5	O
4	O
3	O
2	O
2005053433	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
the	O
actual	O
science	O
of	O
logic	O
is	O
conversant	O
at	O
present	O
only	O
with	O
things	O
either	O
certain	O
,	O
impossible	O
,	O
or	O
entirely	O
doubtful	O
,	O
none	O
of	O
which	O
(	O
fortunately	O
)	O
we	O
have	O
to	O
reason	O
on	O
.	O
therefore	O
the	O
true	O
logic	O
for	O
this	O
world	O
is	O
the	O
calculus	O
of	O
probabilities	O
,	O
which	O
takes	O
account	O
of	O
the	O
magnitude	O
of	O
the	O
probability	B
which	O
is	O
,	O
or	O
ought	O
to	O
be	O
,	O
in	O
a	O
reasonable	O
man	O
’	O
s	O
mind	O
.	O
—	O
james	O
clerk	O
maxwell	O
[	O
1850	O
]	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
contents	O
series	O
foreword	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
xi	O
preface	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
xiii	O
symbols	O
and	O
notation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
xvii	O
1	O
introduction	O
1.1	O
a	O
pictorial	O
introduction	O
to	O
bayesian	O
modelling	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
1.2	O
roadmap	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2	O
regression	B
2.1	O
weight-space	O
view	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.1.1	O
the	O
standard	O
linear	B
model	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.1.2	O
projections	O
of	O
inputs	O
into	O
feature	B
space	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.2	O
function-space	O
view	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.3	O
varying	O
the	O
hyperparameters	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.4	O
decision	O
theory	O
for	O
regression	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.5	O
an	O
example	O
application	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.6	O
smoothing	O
,	O
weight	O
functions	O
and	O
equivalent	B
kernels	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
incorporating	O
explicit	O
basis	O
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.7.1	O
marginal	B
likelihood	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.8	O
history	O
and	O
related	O
work	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
2.9	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
∗	O
2.7	O
1	O
3	O
5	O
7	O
7	O
8	O
11	O
13	O
19	O
21	O
22	O
24	O
27	O
29	O
29	O
30	O
3	O
classiﬁcation	B
3.1	O
classiﬁcation	B
problems	O
33	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
34	O
3.1.1	O
decision	O
theory	O
for	O
classiﬁcation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
35	O
3.2	O
linear	B
models	O
for	O
classiﬁcation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
37	O
3.3	O
gaussian	O
process	B
classiﬁcation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
39	O
3.4	O
the	O
laplace	O
approximation	O
for	O
the	O
binary	B
gp	O
classiﬁer	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
41	O
3.4.1	O
posterior	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
42	O
3.4.2	O
predictions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
44	O
3.4.3	O
implementation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
45	O
3.4.4	O
marginal	B
likelihood	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
47	O
∗	O
3.5	O
multi-class	B
laplace	O
approximation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
48	O
implementation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
51	O
3.6	O
expectation	B
propagation	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
52	O
3.6.1	O
predictions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
56	O
3.6.2	O
marginal	B
likelihood	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
57	O
3.6.3	O
implementation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
57	O
3.7	O
experiments	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
60	O
3.7.1	O
a	O
toy	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
60	O
3.7.2	O
one-dimensional	O
example	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
62	O
3.7.3	O
binary	B
handwritten	O
digit	O
classiﬁcation	B
example	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
63	O
3.7.4	O
10-class	O
handwritten	O
digit	O
classiﬁcation	B
example	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
70	O
3.8	O
discussion	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
72	O
∗sections	O
marked	O
by	O
an	O
asterisk	O
contain	O
advanced	O
material	O
that	O
may	O
be	O
omitted	O
on	O
a	O
ﬁrst	O
reading	O
.	O
3.5.1	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
viii	O
contents	O
∗	O
3.9	O
appendix	O
:	O
moment	O
derivations	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
3.10	O
exercises	O
74	O
75	O
4	O
covariance	B
functions	O
∗	O
4.1	O
preliminaries	O
4.2	O
examples	O
of	O
covariance	B
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4.1.1	O
mean	B
square	I
continuity	I
and	O
diﬀerentiability	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4.2.1	O
stationary	O
covariance	B
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4.2.2	O
dot	B
product	I
covariance	O
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4.2.3	O
other	O
non-stationary	O
covariance	B
functions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4.2.4	O
making	O
new	O
kernels	O
from	O
old	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4.3	O
eigenfunction	B
analysis	O
of	O
kernels	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4.3.1	O
an	O
analytic	O
example	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
4.3.2	O
numerical	O
approximation	O
of	O
eigenfunctions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
79	O
79	O
81	O
81	O
82	O
89	O
90	O
94	O
96	O
97	O
98	O
99	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
100	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
101	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
102	O
string	B
kernels	O
4.4.1	O
4.4.2	O
fisher	O
kernels	O
4.4	O
kernels	O
for	O
non-vectorial	O
inputs	O
4.5	O
exercises	O
∗	O
5	O
model	B
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
105	O
5.1	O
the	O
model	B
selection	O
problem	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
106	O
5.2	O
bayesian	O
model	B
selection	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
108	O
5.3	O
cross-validation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
111	O
5.4	O
model	B
selection	O
for	O
gp	O
regression	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
112	O
5.4.1	O
marginal	B
likelihood	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
112	O
5.4.2	O
cross-validation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
116	O
5.4.3	O
examples	O
and	O
discussion	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
118	O
5.5	O
model	B
selection	O
for	O
gp	O
classiﬁcation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
124	O
5.5.1	O
derivatives	O
of	O
the	O
marginal	B
likelihood	I
for	O
laplace	O
’	O
s	O
approximation	O
125	O
5.5.2	O
derivatives	O
of	O
the	O
marginal	B
likelihood	I
for	O
ep	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
127	O
5.5.3	O
cross-validation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
127	O
5.5.4	O
example	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
128	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
128	O
5.6	O
exercises	O
∗	O
∗	O
6	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
129	O
6.1	O
reproducing	O
kernel	O
hilbert	O
spaces	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
129	O
6.2	O
regularization	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
132	O
6.2.1	O
regularization	B
deﬁned	O
by	O
diﬀerential	O
operators	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
133	O
6.2.2	O
obtaining	O
the	O
regularized	O
solution	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
135	O
6.2.3	O
the	O
relationship	O
of	O
the	O
regularization	B
view	O
to	O
gaussian	O
process	B
∗	O
∗	O
∗	O
6.4	O
support	B
vector	I
machines	O
prediction	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
135	O
6.3	O
spline	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
136	O
6.3.1	O
a	O
1-d	O
gaussian	O
process	B
spline	O
construction	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
138	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
141	O
support	B
vector	I
classiﬁcation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
141	O
support	B
vector	I
regression	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
145	O
∗	O
6.5	O
least-squares	B
classiﬁcation	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
146	O
6.5.1	O
probabilistic	B
least-squares	I
classiﬁcation	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
147	O
6.4.1	O
6.4.2	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
contents	O
ix	O
∗	O
6.6	O
relevance	O
vector	O
machines	O
6.7	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
149	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
150	O
7.1.1	O
some	O
speciﬁc	O
examples	O
of	O
equivalent	B
kernels	O
7	O
theoretical	O
perspectives	O
7.1	O
the	O
equivalent	B
kernel	I
151	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
151	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
153	O
∗	O
7.2	O
asymptotic	O
analysis	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
155	O
7.2.1	O
consistency	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
155	O
7.2.2	O
equivalence	O
and	O
orthogonality	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
157	O
∗	O
7.3	O
average-case	O
learning	B
curves	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
159	O
∗	O
7.4	O
pac-bayesian	O
analysis	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
161	O
7.4.1	O
the	O
pac	O
framework	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
162	O
7.4.2	O
pac-bayesian	O
analysis	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
163	O
7.4.3	O
pac-bayesian	O
analysis	O
of	O
gp	O
classiﬁcation	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
164	O
7.5	O
comparison	O
with	O
other	O
supervised	B
learning	I
methods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
165	O
∗	O
7.6	O
appendix	O
:	O
learning	B
curve	I
for	O
the	O
ornstein-uhlenbeck	O
process	B
.	O
.	O
.	O
.	O
.	O
.	O
168	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
169	O
7.7	O
exercises	O
8	O
approximation	O
methods	O
for	O
large	O
datasets	O
171	O
8.1	O
reduced-rank	O
approximations	O
of	O
the	O
gram	O
matrix	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
171	O
8.2	O
greedy	O
approximation	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
174	O
8.3	O
approximations	O
for	O
gpr	O
with	O
fixed	O
hyperparameters	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
175	O
8.3.1	O
subset	B
of	I
regressors	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
175	O
8.3.2	O
the	O
nystr¨om	O
method	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
177	O
8.3.3	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
177	O
8.3.4	O
projected	B
process	I
approximation	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
178	O
8.3.5	O
bayesian	O
committee	O
machine	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
180	O
8.3.6	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
181	O
8.3.7	O
comparison	O
of	O
approximate	O
gpr	O
methods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
182	O
8.4	O
approximations	O
for	O
gpc	O
with	O
fixed	O
hyperparameters	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
185	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
185	O
∗	O
8.5	O
approximating	O
the	O
marginal	B
likelihood	I
and	O
its	O
derivatives	O
∗	O
8.6	O
appendix	O
:	O
equivalence	O
of	O
sr	O
and	O
gpr	O
using	O
the	O
nystr¨om	O
approximate	O
iterative	O
solution	O
of	O
linear	B
systems	O
subset	B
of	I
datapoints	I
kernel	O
8.7	O
exercises	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
187	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
187	O
9	O
further	O
issues	O
and	O
conclusions	O
189	O
9.1	O
multiple	B
outputs	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
190	O
9.2	O
noise	O
models	O
with	O
dependencies	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
190	O
9.3	O
non-gaussian	O
likelihoods	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
191	O
9.4	O
derivative	B
observations	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
191	O
9.5	O
prediction	B
with	O
uncertain	B
inputs	I
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
192	O
9.6	O
mixtures	O
of	O
gaussian	O
processes	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
192	O
9.7	O
global	O
optimization	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
193	O
9.8	O
evaluation	O
of	O
integrals	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
193	O
9.9	O
student	O
’	O
s	O
t	O
process	B
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
194	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
194	O
9.10	O
invariances	B
9.11	O
latent	O
variable	O
models	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
196	O
9.12	O
conclusions	O
and	O
future	O
directions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
196	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
x	O
contents	O
appendix	O
a	O
mathematical	O
background	O
199	O
a.1	O
joint	B
,	O
marginal	B
and	O
conditional	B
probability	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
199	O
a.2	O
gaussian	O
identities	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
200	O
a.3	O
matrix	B
identities	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
201	O
a.3.1	O
matrix	B
derivatives	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
202	O
a.3.2	O
matrix	B
norms	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
202	O
a.4	O
cholesky	O
decomposition	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
202	O
a.5	O
entropy	B
and	O
kullback-leibler	O
divergence	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
203	O
a.6	O
limits	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
204	O
a.7	O
measure	B
and	O
integration	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
204	O
a.7.1	O
lp	O
spaces	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
205	O
a.8	O
fourier	O
transforms	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
205	O
a.9	O
convexity	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
206	O
appendix	O
b	O
gaussian	O
markov	O
processes	O
b.1	O
fourier	O
analysis	O
b.2	O
continuous-time	O
gaussian	O
markov	O
processes	O
207	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
208	O
b.1.1	O
sampling	O
and	O
periodization	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
209	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
211	O
b.2.1	O
continuous-time	O
gmps	O
on	O
r	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
211	O
b.2.2	O
the	O
solution	O
of	O
the	O
corresponding	O
sde	O
on	O
the	O
circle	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
213	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
214	O
b.3.1	O
discrete-time	O
gmps	O
on	O
z	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
214	O
b.3.2	O
the	O
solution	O
of	O
the	O
corresponding	O
diﬀerence	O
equation	O
on	O
pn	O
.	O
.	O
215	O
b.3	O
discrete-time	O
gaussian	O
markov	O
processes	O
b.4	O
the	O
relationship	O
between	O
discrete-time	O
and	O
sampled	O
continuous-time	O
gmps	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
217	O
b.5	O
markov	O
processes	O
in	O
higher	O
dimensions	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
.	O
218	O
appendix	O
c	O
datasets	O
and	O
code	O
bibliography	O
author	O
index	O
subject	O
index	O
221	O
223	O
239	O
245	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
series	O
foreword	O
the	O
goal	O
of	O
building	O
systems	O
that	O
can	O
adapt	O
to	O
their	O
environments	O
and	O
learn	O
from	O
their	O
experience	O
has	O
attracted	O
researchers	O
from	O
many	O
ﬁelds	O
,	O
including	O
com-	O
puter	O
science	O
,	O
engineering	O
,	O
mathematics	O
,	O
physics	O
,	O
neuroscience	O
,	O
and	O
cognitive	O
science	O
.	O
out	O
of	O
this	O
research	O
has	O
come	O
a	O
wide	O
variety	O
of	O
learning	B
techniques	O
that	O
have	O
the	O
potential	O
to	O
transform	O
many	O
scientiﬁc	O
and	O
industrial	O
ﬁelds	O
.	O
recently	O
,	O
several	O
research	O
communities	O
have	O
converged	O
on	O
a	O
common	O
set	B
of	O
issues	O
sur-	O
rounding	O
supervised	O
,	O
unsupervised	O
,	O
and	O
reinforcement	O
learning	B
problems	O
.	O
the	O
mit	O
press	O
series	O
on	O
adaptive	O
computation	O
and	O
machine	O
learning	B
seeks	O
to	O
unify	O
the	O
many	O
diverse	O
strands	O
of	O
machine	O
learning	B
research	O
and	O
to	O
foster	O
high	O
quality	O
research	O
and	O
innovative	O
applications	O
.	O
one	O
of	O
the	O
most	O
active	O
directions	O
in	O
machine	O
learning	B
has	O
been	O
the	O
de-	O
velopment	O
of	O
practical	O
bayesian	O
methods	O
for	O
challenging	O
learning	B
problems	O
.	O
gaussian	O
processes	O
for	O
machine	O
learning	B
presents	O
one	O
of	O
the	O
most	O
important	O
bayesian	O
machine	O
learning	B
approaches	O
based	O
on	O
a	O
particularly	O
eﬀective	O
method	O
for	O
placing	O
a	O
prior	O
distribution	O
over	O
the	O
space	O
of	O
functions	O
.	O
carl	O
edward	O
ras-	O
mussen	O
and	O
chris	O
williams	O
are	O
two	O
of	O
the	O
pioneers	O
in	O
this	O
area	O
,	O
and	O
their	O
book	O
describes	O
the	O
mathematical	O
foundations	O
and	O
practical	O
application	O
of	O
gaussian	O
processes	O
in	O
regression	B
and	O
classiﬁcation	B
tasks	O
.	O
they	O
also	O
show	O
how	O
gaussian	O
processes	O
can	O
be	O
interpreted	O
as	O
a	O
bayesian	O
version	O
of	O
the	O
well-known	O
support	B
vector	I
machine	I
methods	O
.	O
students	O
and	O
researchers	O
who	O
study	O
this	O
book	O
will	O
be	O
able	O
to	O
apply	O
gaussian	O
process	B
methods	O
in	O
creative	O
ways	O
to	O
solve	O
a	O
wide	O
range	O
of	O
problems	O
in	O
science	O
and	O
engineering	O
.	O
thomas	O
dietterich	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
preface	O
over	O
the	O
last	O
decade	O
there	O
has	O
been	O
an	O
explosion	O
of	O
work	O
in	O
the	O
“	O
kernel	B
ma-	O
chines	O
”	O
area	O
of	O
machine	O
learning	B
.	O
probably	O
the	O
best	O
known	O
example	O
of	O
this	O
is	O
work	O
on	O
support	B
vector	I
machines	O
,	O
but	O
during	O
this	O
period	O
there	O
has	O
also	O
been	O
much	O
activity	O
concerning	O
the	O
application	O
of	O
gaussian	O
process	B
models	O
to	O
ma-	O
chine	O
learning	B
tasks	O
.	O
the	O
goal	O
of	O
this	O
book	O
is	O
to	O
provide	O
a	O
systematic	O
and	O
uni-	O
ﬁed	O
treatment	O
of	O
this	O
area	O
.	O
gaussian	O
processes	O
provide	O
a	O
principled	O
,	O
practical	O
,	O
probabilistic	B
approach	O
to	O
learning	B
in	O
kernel	B
machines	O
.	O
this	O
gives	O
advantages	O
with	O
respect	O
to	O
the	O
interpretation	O
of	O
model	B
predictions	O
and	O
provides	O
a	O
well-	O
founded	O
framework	O
for	O
learning	B
and	O
model	B
selection	O
.	O
theoretical	O
and	O
practical	O
developments	O
of	O
over	O
the	O
last	O
decade	O
have	O
made	O
gaussian	O
processes	O
a	O
serious	O
competitor	O
for	O
real	O
supervised	B
learning	I
applications	O
.	O
roughly	O
speaking	O
a	O
stochastic	O
process	O
is	O
a	O
generalization	B
of	O
a	O
probability	B
distribution	O
(	O
which	O
describes	O
a	O
ﬁnite-dimensional	O
random	O
variable	O
)	O
to	O
func-	O
tions	O
.	O
by	O
focussing	O
on	O
processes	O
which	O
are	O
gaussian	O
,	O
it	O
turns	O
out	O
that	O
the	O
computations	O
required	O
for	O
inference	O
and	O
learning	B
become	O
relatively	O
easy	O
.	O
thus	O
,	O
the	O
supervised	B
learning	I
problems	O
in	O
machine	O
learning	B
which	O
can	O
be	O
thought	O
of	O
as	O
learning	B
a	O
function	B
from	O
examples	O
can	O
be	O
cast	O
directly	O
into	O
the	O
gaussian	O
process	B
framework	O
.	O
our	O
interest	O
in	O
gaussian	O
process	B
(	O
gp	O
)	O
models	O
in	O
the	O
context	O
of	O
machine	O
learning	B
was	O
aroused	O
in	O
1994	O
,	O
while	O
we	O
were	O
both	O
graduate	O
students	O
in	O
geoﬀ	O
hinton	O
’	O
s	O
neural	O
networks	O
lab	O
at	O
the	O
university	O
of	O
toronto	O
.	O
this	O
was	O
a	O
time	O
when	O
the	O
ﬁeld	O
of	O
neural	O
networks	O
was	O
becoming	O
mature	O
and	O
the	O
many	O
con-	O
nections	O
to	O
statistical	O
physics	O
,	O
probabilistic	B
models	O
and	O
statistics	O
became	O
well	O
known	O
,	O
and	O
the	O
ﬁrst	O
kernel-based	O
learning	B
algorithms	O
were	O
becoming	O
popular	O
.	O
in	O
retrospect	O
it	O
is	O
clear	O
that	O
the	O
time	O
was	O
ripe	O
for	O
the	O
application	O
of	O
gaussian	O
processes	O
to	O
machine	O
learning	B
problems	O
.	O
many	O
researchers	O
were	O
realizing	O
that	O
neural	O
networks	O
were	O
not	O
so	O
easy	O
to	O
apply	O
in	O
practice	O
,	O
due	O
to	O
the	O
many	O
decisions	O
which	O
needed	O
to	O
be	O
made	O
:	O
what	O
architecture	O
,	O
what	O
activation	O
functions	O
,	O
what	O
learning	B
rate	O
,	O
etc.	O
,	O
and	O
the	O
lack	O
of	O
a	O
principled	O
framework	O
to	O
answer	O
these	O
questions	O
.	O
the	O
probabilistic	B
framework	O
was	O
pursued	O
using	O
approximations	O
by	O
mackay	O
[	O
1992b	O
]	O
and	O
using	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
methods	O
by	O
neal	O
[	O
1996	O
]	O
.	O
neal	O
was	O
also	O
a	O
graduate	O
stu-	O
dent	O
in	O
the	O
same	O
lab	O
,	O
and	O
in	O
his	O
thesis	O
he	O
sought	O
to	O
demonstrate	O
that	O
using	O
the	O
bayesian	O
formalism	O
,	O
one	O
does	O
not	O
necessarily	O
have	O
problems	O
with	O
“	O
overﬁtting	O
”	O
when	O
the	O
models	O
get	O
large	O
,	O
and	O
one	O
should	O
pursue	O
the	O
limit	O
of	O
large	O
models	O
.	O
while	O
his	O
own	O
work	O
was	O
focused	O
on	O
sophisticated	O
markov	O
chain	O
methods	O
for	O
inference	O
in	O
large	O
ﬁnite	O
networks	O
,	O
he	O
did	O
point	O
out	O
that	O
some	O
of	O
his	O
networks	O
became	O
gaussian	O
processes	O
in	O
the	O
limit	O
of	O
inﬁnite	O
size	O
,	O
and	O
“	O
there	O
may	O
be	O
sim-	O
pler	O
ways	O
to	O
do	O
inference	O
in	O
this	O
case.	O
”	O
it	O
is	O
perhaps	O
interesting	O
to	O
mention	O
a	O
slightly	O
wider	O
historical	O
perspective	O
.	O
the	O
main	O
reason	O
why	O
neural	O
networks	O
became	O
popular	O
was	O
that	O
they	O
allowed	O
the	O
use	O
of	O
adaptive	O
basis	O
functions	O
,	O
as	O
opposed	O
to	O
the	O
well	O
known	O
linear	B
models	O
.	O
the	O
adaptive	O
basis	O
functions	O
,	O
or	O
hidden	O
units	O
,	O
could	O
“	O
learn	O
”	O
hidden	O
features	O
kernel	B
machines	O
gaussian	O
process	B
gaussian	O
processes	O
in	O
machine	O
learning	B
neural	O
networks	O
large	O
neural	O
networks	O
≡	O
gaussian	O
processes	O
adaptive	O
basis	O
functions	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
xiv	O
many	O
ﬁxed	O
basis	O
functions	O
useful	O
representations	O
supervised	B
learning	I
in	O
statistics	O
statistics	O
and	O
machine	O
learning	B
data	O
and	O
models	O
algorithms	O
and	O
predictions	O
bridging	O
the	O
gap	O
preface	O
useful	O
for	O
the	O
modelling	O
problem	O
at	O
hand	O
.	O
however	O
,	O
this	O
adaptivity	O
came	O
at	O
the	O
cost	O
of	O
a	O
lot	O
of	O
practical	O
problems	O
.	O
later	O
,	O
with	O
the	O
advancement	O
of	O
the	O
“	O
kernel	B
era	O
”	O
,	O
it	O
was	O
realized	O
that	O
the	O
limitation	O
of	O
ﬁxed	O
basis	O
functions	O
is	O
not	O
a	O
big	O
restriction	O
if	O
only	O
one	O
has	O
enough	O
of	O
them	O
,	O
i.e	O
.	O
typically	O
inﬁnitely	O
many	O
,	O
and	O
one	O
is	O
careful	O
to	O
control	O
problems	O
of	O
overﬁtting	O
by	O
using	O
priors	O
or	O
regularization	B
.	O
the	O
resulting	O
models	O
are	O
much	O
easier	O
to	O
handle	O
than	O
the	O
adaptive	O
basis	O
function	B
models	O
,	O
but	O
have	O
similar	O
expressive	O
power	O
.	O
thus	O
,	O
one	O
could	O
claim	O
that	O
(	O
as	O
far	O
a	O
machine	O
learning	B
is	O
concerned	O
)	O
the	O
adaptive	O
basis	O
functions	O
were	O
merely	O
a	O
decade-long	O
digression	O
,	O
and	O
we	O
are	O
now	O
back	O
to	O
where	O
we	O
came	O
from	O
.	O
this	O
view	O
is	O
perhaps	O
reasonable	O
if	O
we	O
think	O
of	O
models	O
for	O
solving	O
practical	O
learning	B
problems	O
,	O
although	O
mackay	O
[	O
2003	O
,	O
ch	O
.	O
45	O
]	O
,	O
for	O
example	O
,	O
raises	O
concerns	O
by	O
asking	O
“	O
did	O
we	O
throw	O
out	O
the	O
baby	O
with	O
the	O
bath	O
water	O
?	O
”	O
,	O
as	O
the	O
kernel	B
view	O
does	O
not	O
give	O
us	O
any	O
hidden	O
representations	O
,	O
telling	O
us	O
what	O
the	O
useful	O
features	O
are	O
for	O
solving	O
a	O
particular	O
problem	O
.	O
as	O
we	O
will	O
argue	O
in	O
the	O
book	O
,	O
one	O
answer	O
may	O
be	O
to	O
learn	O
more	O
sophisticated	O
covariance	B
functions	O
,	O
and	O
the	O
“	O
hidden	O
”	O
properties	O
of	O
the	O
problem	O
are	O
to	O
be	O
found	O
here	O
.	O
an	O
important	O
area	O
of	O
future	O
developments	O
for	O
gp	O
models	O
is	O
the	O
use	O
of	O
more	O
expressive	O
covariance	B
functions	O
.	O
supervised	B
learning	I
problems	O
have	O
been	O
studied	O
for	O
more	O
than	O
a	O
century	O
in	O
statistics	O
,	O
and	O
a	O
large	O
body	O
of	O
well-established	O
theory	O
has	O
been	O
developed	O
.	O
more	O
recently	O
,	O
with	O
the	O
advance	O
of	O
aﬀordable	O
,	O
fast	O
computation	O
,	O
the	O
machine	O
learning	B
community	O
has	O
addressed	O
increasingly	O
large	O
and	O
complex	O
problems	O
.	O
much	O
of	O
the	O
basic	O
theory	O
and	O
many	O
algorithms	O
are	O
shared	O
between	O
the	O
statistics	O
and	O
machine	O
learning	B
community	O
.	O
the	O
primary	O
diﬀerences	O
are	O
perhaps	O
the	O
types	O
of	O
the	O
problems	O
attacked	O
,	O
and	O
the	O
goal	O
of	O
learning	B
.	O
at	O
the	O
risk	B
of	O
oversimpliﬁcation	O
,	O
one	O
could	O
say	O
that	O
in	O
statistics	O
a	O
prime	O
focus	O
is	O
often	O
in	O
understanding	O
the	O
data	O
and	O
relationships	O
in	O
terms	O
of	O
models	O
giving	O
approximate	O
summaries	O
such	O
as	O
linear	B
relations	O
or	O
independencies	O
.	O
in	O
contrast	O
,	O
the	O
goals	O
in	O
machine	O
learning	B
are	O
primarily	O
to	O
make	O
predictions	O
as	O
accurately	O
as	O
possible	O
and	O
to	O
understand	O
the	O
behaviour	O
of	O
learning	B
algorithms	O
.	O
these	O
diﬀering	O
objectives	O
have	O
led	O
to	O
diﬀerent	O
developments	O
in	O
the	O
two	O
ﬁelds	O
:	O
for	O
example	O
,	O
neural	B
network	I
algorithms	O
have	O
been	O
used	O
extensively	O
as	O
black-box	O
function	B
approximators	O
in	O
machine	O
learning	B
,	O
but	O
to	O
many	O
statisticians	O
they	O
are	O
less	O
than	O
satisfactory	O
,	O
because	O
of	O
the	O
diﬃculties	O
in	O
interpreting	O
such	O
models	O
.	O
gaussian	O
process	B
models	O
in	O
some	O
sense	O
bring	O
together	O
work	O
in	O
the	O
two	O
com-	O
munities	O
.	O
as	O
we	O
will	O
see	B
,	O
gaussian	O
processes	O
are	O
mathematically	O
equivalent	B
to	O
many	O
well	O
known	O
models	O
,	O
including	O
bayesian	O
linear	B
models	O
,	O
spline	O
models	O
,	O
large	O
neural	O
networks	O
(	O
under	O
suitable	O
conditions	O
)	O
,	O
and	O
are	O
closely	O
related	O
to	O
others	O
,	O
such	O
as	O
support	B
vector	I
machines	O
.	O
under	O
the	O
gaussian	O
process	B
viewpoint	O
,	O
the	O
models	O
may	O
be	O
easier	O
to	O
handle	O
and	O
interpret	O
than	O
their	O
conventional	O
coun-	O
terparts	O
,	O
such	O
as	O
e.g	O
.	O
neural	O
networks	O
.	O
in	O
the	O
statistics	O
community	O
gaussian	O
processes	O
have	O
also	O
been	O
discussed	O
many	O
times	O
,	O
although	O
it	O
would	O
probably	O
be	O
excessive	O
to	O
claim	O
that	O
their	O
use	O
is	O
widespread	O
except	O
for	O
certain	O
speciﬁc	O
appli-	O
cations	O
such	O
as	O
spatial	O
models	O
in	O
meteorology	O
and	O
geology	O
,	O
and	O
the	O
analysis	O
of	O
computer	O
experiments	O
.	O
a	O
rich	O
theory	O
also	O
exists	O
for	O
gaussian	O
process	B
models	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
preface	O
in	O
the	O
time	O
series	O
analysis	O
literature	O
;	O
some	O
pointers	O
to	O
this	O
literature	O
are	O
given	O
in	O
appendix	O
b.	O
the	O
book	O
is	O
primarily	O
intended	O
for	O
graduate	O
students	O
and	O
researchers	O
in	O
machine	O
learning	B
at	O
departments	O
of	O
computer	O
science	O
,	O
statistics	O
and	O
applied	O
mathematics	O
.	O
as	O
prerequisites	O
we	O
require	O
a	O
good	O
basic	O
grounding	O
in	O
calculus	O
,	O
linear	B
algebra	O
and	O
probability	B
theory	O
as	O
would	O
be	O
obtained	O
by	O
graduates	O
in	O
nu-	O
merate	O
disciplines	O
such	O
as	O
electrical	O
engineering	O
,	O
physics	O
and	O
computer	O
science	O
.	O
for	O
preparation	O
in	O
calculus	O
and	O
linear	B
algebra	O
any	O
good	O
university-level	O
text-	O
book	O
on	O
mathematics	O
for	O
physics	O
or	O
engineering	O
such	O
as	O
arfken	O
[	O
1985	O
]	O
would	O
be	O
ﬁne	O
.	O
for	O
probability	B
theory	O
some	O
familiarity	O
with	O
multivariate	O
distributions	O
(	O
especially	O
the	O
gaussian	O
)	O
and	O
conditional	B
probability	O
is	O
required	O
.	O
some	O
back-	O
ground	O
mathematical	O
material	O
is	O
also	O
provided	O
in	O
appendix	O
a.	O
the	O
main	O
focus	O
of	O
the	O
book	O
is	O
to	O
present	O
clearly	O
and	O
concisely	O
an	O
overview	O
of	O
the	O
main	O
ideas	O
of	O
gaussian	O
processes	O
in	O
a	O
machine	O
learning	B
context	O
.	O
we	O
have	O
also	O
covered	O
a	O
wide	O
range	O
of	O
connections	O
to	O
existing	O
models	O
in	O
the	O
literature	O
,	O
and	O
cover	O
approximate	O
inference	O
for	O
faster	O
practical	O
algorithms	O
.	O
we	O
have	O
pre-	O
sented	O
detailed	O
algorithms	O
for	O
many	O
methods	O
to	O
aid	O
the	O
practitioner	O
.	O
software	O
implementations	O
are	O
available	O
from	O
the	O
website	O
for	O
the	O
book	O
,	O
see	B
appendix	O
c.	O
we	O
have	O
also	O
included	O
a	O
small	O
set	B
of	O
exercises	O
in	O
each	O
chapter	O
;	O
we	O
hope	O
these	O
will	O
help	O
in	O
gaining	O
a	O
deeper	O
understanding	O
of	O
the	O
material	O
.	O
in	O
order	O
limit	O
the	O
size	O
of	O
the	O
volume	O
,	O
we	O
have	O
had	O
to	O
omit	O
some	O
topics	O
,	O
such	O
as	O
,	O
for	O
example	O
,	O
markov	O
chain	O
monte	O
carlo	O
methods	O
for	O
inference	O
.	O
one	O
of	O
the	O
most	O
diﬃcult	O
things	O
to	O
decide	O
when	O
writing	O
a	O
book	O
is	O
what	O
sections	O
not	O
to	O
write	O
.	O
within	O
sections	O
,	O
we	O
have	O
often	O
chosen	O
to	O
describe	O
one	O
algorithm	O
in	O
particular	O
in	O
depth	O
,	O
and	O
mention	O
related	O
work	O
only	O
in	O
passing	O
.	O
although	O
this	O
causes	O
the	O
omission	O
of	O
some	O
material	O
,	O
we	O
feel	O
it	O
is	O
the	O
best	O
approach	O
for	O
a	O
monograph	O
,	O
and	O
hope	O
that	O
the	O
reader	O
will	O
gain	O
a	O
general	O
understanding	O
so	O
as	O
to	O
be	O
able	O
to	O
push	O
further	O
into	O
the	O
growing	O
literature	O
of	O
gp	O
models	O
.	O
the	O
book	O
has	O
a	O
natural	O
split	O
into	O
two	O
parts	O
,	O
with	O
the	O
chapters	O
up	O
to	O
and	O
including	O
chapter	O
5	O
covering	O
core	O
material	O
,	O
and	O
the	O
remaining	O
sections	O
covering	O
the	O
connections	O
to	O
other	O
methods	O
,	O
fast	O
approximations	O
,	O
and	O
more	O
specialized	O
properties	O
.	O
some	O
sections	O
are	O
marked	O
by	O
an	O
asterisk	O
.	O
these	O
sections	O
may	O
be	O
omitted	O
on	O
a	O
ﬁrst	O
reading	O
,	O
and	O
are	O
not	O
pre-requisites	O
for	O
later	O
(	O
un-starred	O
)	O
material	O
.	O
we	O
wish	O
to	O
express	O
our	O
considerable	O
gratitude	O
to	O
the	O
many	O
people	O
with	O
whom	O
we	O
have	O
interacted	O
during	O
the	O
writing	O
of	O
this	O
book	O
.	O
in	O
particular	O
moray	O
allan	O
,	O
david	O
barber	O
,	O
peter	O
bartlett	O
,	O
miguel	O
carreira-perpi˜n´an	O
,	O
marcus	O
gal-	O
lagher	O
,	O
manfred	O
opper	O
,	O
anton	O
schwaighofer	O
,	O
matthias	O
seeger	O
,	O
hanna	O
wallach	O
,	O
joe	O
whittaker	O
,	O
and	O
andrew	O
zisserman	O
all	O
read	O
parts	O
of	O
the	O
book	O
and	O
provided	O
valuable	O
feedback	O
.	O
dilan	O
g¨or¨ur	O
,	O
malte	O
kuss	O
,	O
iain	O
murray	O
,	O
joaquin	O
qui˜nonero-	O
candela	O
,	O
leif	O
rasmussen	O
and	O
sam	O
roweis	O
were	O
especially	O
heroic	O
and	O
provided	O
comments	O
on	O
the	O
whole	O
manuscript	O
.	O
we	O
thank	O
chris	O
bishop	O
,	O
miguel	O
carreira-	O
perpi˜n´an	O
,	O
nando	O
de	O
freitas	O
,	O
zoubin	O
ghahramani	O
,	O
peter	O
gr¨unwald	O
,	O
mike	O
jor-	O
dan	O
,	O
john	O
kent	O
,	O
radford	O
neal	O
,	O
joaquin	O
qui˜nonero-candela	O
,	O
ryan	O
rifkin	O
,	O
ste-	O
fan	O
schaal	O
,	O
anton	O
schwaighofer	O
,	O
matthias	O
seeger	O
,	O
peter	O
sollich	O
,	O
ingo	O
steinwart	O
,	O
xv	O
intended	O
audience	O
focus	O
scope	O
book	O
organization	O
∗	O
acknowledgements	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
xvi	O
preface	O
errata	O
looking	O
ahead	O
amos	O
storkey	O
,	O
volker	O
tresp	O
,	O
sethu	O
vijayakumar	O
,	O
grace	O
wahba	O
,	O
joe	O
whittaker	O
and	O
tong	O
zhang	O
for	O
valuable	O
discussions	O
on	O
speciﬁc	O
issues	O
.	O
we	O
also	O
thank	O
bob	O
prior	O
and	O
the	O
staﬀ	O
at	O
mit	O
press	O
for	O
their	O
support	O
during	O
the	O
writing	O
of	O
the	O
book	O
.	O
we	O
thank	O
the	O
gatsby	O
computational	O
neuroscience	O
unit	O
(	O
ucl	O
)	O
and	O
neil	O
lawrence	O
at	O
the	O
department	O
of	O
computer	O
science	O
,	O
university	O
of	O
sheﬃeld	O
for	O
hosting	O
our	O
visits	O
and	O
kindly	O
providing	O
space	O
for	O
us	O
to	O
work	O
,	O
and	O
the	O
depart-	O
ment	O
of	O
computer	O
science	O
at	O
the	O
university	O
of	O
toronto	O
for	O
computer	O
support	O
.	O
thanks	O
to	O
john	O
and	O
fiona	O
for	O
their	O
hospitality	O
on	O
numerous	O
occasions	O
.	O
some	O
of	O
the	O
diagrams	O
in	O
this	O
book	O
have	O
been	O
inspired	O
by	O
similar	O
diagrams	O
appearing	O
in	O
published	O
work	O
,	O
as	O
follows	O
:	O
figure	O
3.5	O
,	O
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
]	O
;	O
fig-	O
ure	O
5.2	O
,	O
mackay	O
[	O
1992b	O
]	O
.	O
cer	O
gratefully	O
acknowledges	O
ﬁnancial	O
support	O
from	O
the	O
german	O
research	O
foundation	O
(	O
dfg	O
)	O
.	O
ckiw	O
thanks	O
the	O
school	O
of	O
infor-	O
matics	O
,	O
university	O
of	O
edinburgh	O
for	O
granting	O
him	O
sabbatical	O
leave	O
for	O
the	O
period	O
october	O
2003-march	O
2004.	O
finally	O
,	O
we	O
reserve	O
our	O
deepest	O
appreciation	O
for	O
our	O
wives	O
agnes	O
and	O
bar-	O
bara	O
,	O
and	O
children	O
ezra	O
,	O
kate	O
,	O
miro	O
and	O
ruth	O
for	O
their	O
patience	O
and	O
under-	O
standing	O
while	O
the	O
book	O
was	O
being	O
written	O
.	O
despite	O
our	O
best	O
eﬀorts	O
it	O
is	O
inevitable	O
that	O
some	O
errors	O
will	O
make	O
it	O
through	O
to	O
the	O
printed	O
version	O
of	O
the	O
book	O
.	O
errata	O
will	O
be	O
made	O
available	O
via	O
the	O
book	O
’	O
s	O
website	O
at	O
http	O
:	O
//www.gaussianprocess.org/gpml	O
we	O
have	O
found	O
the	O
joint	B
writing	O
of	O
this	O
book	O
an	O
excellent	O
experience	O
.	O
although	O
hard	O
at	O
times	O
,	O
we	O
are	O
conﬁdent	O
that	O
the	O
end	O
result	O
is	O
much	O
better	O
than	O
either	O
one	O
of	O
us	O
could	O
have	O
written	O
alone	O
.	O
now	O
,	O
ten	O
years	O
after	O
their	O
ﬁrst	O
introduction	O
into	O
the	O
machine	O
learning	B
com-	O
munity	O
,	O
gaussian	O
processes	O
are	O
receiving	O
growing	O
attention	O
.	O
although	O
gps	O
have	O
been	O
known	O
for	O
a	O
long	O
time	O
in	O
the	O
statistics	O
and	O
geostatistics	B
ﬁelds	O
,	O
and	O
their	O
use	O
can	O
perhaps	O
be	O
traced	O
back	O
as	O
far	O
as	O
the	O
end	O
of	O
the	O
19th	O
century	O
,	O
their	O
application	O
to	O
real	O
problems	O
is	O
still	O
in	O
its	O
early	O
phases	O
.	O
this	O
contrasts	O
somewhat	O
the	O
application	O
of	O
the	O
non-probabilistic	O
analogue	O
of	O
the	O
gp	O
,	O
the	O
support	O
vec-	O
tor	O
machine	O
,	O
which	O
was	O
taken	O
up	O
more	O
quickly	O
by	O
practitioners	O
.	O
perhaps	O
this	O
has	O
to	O
do	O
with	O
the	O
probabilistic	B
mind-set	O
needed	O
to	O
understand	O
gps	O
,	O
which	O
is	O
not	O
so	O
generally	O
appreciated	O
.	O
perhaps	O
it	O
is	O
due	O
to	O
the	O
need	O
for	O
computational	O
short-cuts	O
to	O
implement	O
inference	O
for	O
large	O
datasets	O
.	O
or	O
it	O
could	O
be	O
due	O
to	O
the	O
lack	O
of	O
a	O
self-contained	O
introduction	O
to	O
this	O
exciting	O
ﬁeld—with	O
this	O
volume	O
,	O
we	O
hope	O
to	O
contribute	O
to	O
the	O
momentum	O
gained	O
by	O
gaussian	O
processes	O
in	O
machine	O
learning	B
.	O
carl	O
edward	O
rasmussen	O
and	O
chris	O
williams	O
t¨ubingen	O
and	O
edinburgh	O
,	O
summer	O
2005	O
second	O
printing	O
:	O
we	O
thank	O
baback	O
moghaddam	O
,	O
mikhail	O
parakhin	O
,	O
leif	O
ras-	O
mussen	O
,	O
benjamin	O
sobotta	O
,	O
kevin	O
s.	O
van	O
horn	O
and	O
aki	O
vehtari	O
for	O
reporting	O
errors	O
in	O
the	O
ﬁrst	O
printing	O
which	O
have	O
now	O
been	O
corrected	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
symbols	O
and	O
notation	O
matrices	O
are	O
capitalized	O
and	O
vectors	O
are	O
in	O
bold	O
type	O
.	O
we	O
do	O
not	O
generally	O
distinguish	O
between	O
proba-	O
bilities	O
and	O
probability	B
densities	O
.	O
a	O
subscript	O
asterisk	O
,	O
such	O
as	O
in	O
x∗	O
,	O
indicates	O
reference	O
to	O
a	O
test	O
set	B
quantity	O
.	O
a	O
superscript	O
asterisk	O
denotes	O
complex	O
conjugate	O
.	O
symbol	O
\	O
,	O
c=	O
|k|	O
|y|	O
hf	O
,	O
gih	O
kfkh	O
y	O
>	O
∝	O
∼	O
∇	O
or	O
∇f	O
∇∇	O
0	O
or	O
0n	O
1	O
or	O
1n	O
c	O
cholesky	O
(	O
a	O
)	O
cov	O
(	O
f∗	O
)	O
d	O
d	O
diag	O
(	O
w	O
)	O
diag	O
(	O
w	O
)	O
δpq	O
e	O
or	O
eq	O
(	O
x	O
)	O
[	O
z	O
(	O
x	O
)	O
]	O
f	O
(	O
x	O
)	O
or	O
f	O
f∗	O
¯f∗	O
gp	O
h	O
(	O
x	O
)	O
or	O
h	O
(	O
x	O
)	O
h	O
or	O
h	O
(	O
x	O
)	O
i	O
or	O
in	O
jν	O
(	O
z	O
)	O
k	O
(	O
x	O
,	O
x0	O
)	O
k	O
or	O
k	O
(	O
x	O
,	O
x	O
)	O
k∗	O
k	O
(	O
x∗	O
)	O
or	O
k∗	O
kf	O
or	O
k	O
(	O
cid:1	O
)	O
1/2	O
i	O
y2	O
i	O
meaning	O
left	O
matrix	B
divide	O
:	O
a\b	O
is	O
the	O
vector	O
x	O
which	O
solves	O
ax	O
=	O
b	O
an	O
equality	O
which	O
acts	O
as	O
a	O
deﬁnition	O
equality	O
up	O
to	O
an	O
additive	O
constant	O
determinant	O
of	O
k	O
matrix	B
euclidean	O
length	O
of	O
vector	O
y	O
,	O
i.e	O
.	O
(	O
cid:0	O
)	O
p	O
rkhs	O
inner	O
product	O
rkhs	O
norm	B
the	O
transpose	O
of	O
vector	O
y	O
proportional	O
to	O
;	O
e.g	O
.	O
p	O
(	O
x|y	O
)	O
∝	O
f	O
(	O
x	O
,	O
y	O
)	O
means	O
that	O
p	O
(	O
x|y	O
)	O
is	O
equal	O
to	O
f	O
(	O
x	O
,	O
y	O
)	O
times	O
a	O
factor	O
which	O
is	O
independent	O
of	O
x	O
distributed	O
according	O
to	O
;	O
example	O
:	O
x	O
∼	O
n	O
(	O
µ	O
,	O
σ2	O
)	O
partial	O
derivatives	O
(	O
w.r.t	O
.	O
f	O
)	O
the	O
(	O
hessian	O
)	O
matrix	B
of	O
second	O
derivatives	O
vector	O
of	O
all	O
0	O
’	O
s	O
(	O
of	O
length	O
n	O
)	O
vector	O
of	O
all	O
1	O
’	O
s	O
(	O
of	O
length	O
n	O
)	O
number	O
of	O
classes	O
in	O
a	O
classiﬁcation	B
problem	O
cholesky	O
decomposition	O
:	O
l	O
is	O
a	O
lower	O
triangular	O
matrix	B
such	O
that	O
ll	O
>	O
=	O
a	O
gaussian	O
process	B
posterior	O
covariance	B
dimension	O
of	O
input	O
space	O
x	O
data	O
set	B
:	O
d	O
=	O
{	O
(	O
xi	O
,	O
yi	O
)	O
|i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
(	O
vector	O
argument	O
)	O
a	O
diagonal	O
matrix	B
containing	O
the	O
elements	O
of	O
vector	O
w	O
(	O
matrix	B
argument	O
)	O
a	O
vector	O
containing	O
the	O
diagonal	O
elements	O
of	O
matrix	B
w	O
kronecker	O
delta	O
,	O
δpq	O
=	O
1	O
iﬀ	O
p	O
=	O
q	O
and	O
0	O
otherwise	O
expectation	O
;	O
expectation	O
of	O
z	O
(	O
x	O
)	O
when	O
x	O
∼	O
q	O
(	O
x	O
)	O
gaussian	O
process	B
(	O
or	O
vector	O
of	O
)	O
latent	O
function	O
values	O
,	O
f	O
=	O
(	O
f	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
f	O
(	O
xn	O
)	O
)	O
>	O
gaussian	O
process	B
(	O
posterior	O
)	O
prediction	B
(	O
random	O
variable	O
)	O
gaussian	O
process	B
posterior	O
mean	O
gaussian	O
process	B
with	O
mean	B
function	I
m	O
(	O
x	O
)	O
and	O
covariance	B
function	I
k	O
(	O
x	O
,	O
x0	O
)	O
either	O
ﬁxed	O
basis	O
function	B
(	O
or	O
set	B
of	O
basis	O
functions	O
)	O
or	O
weight	B
function	I
set	O
of	O
basis	O
functions	O
evaluated	O
at	O
all	O
training	O
points	O
the	O
identity	O
matrix	B
(	O
of	O
size	O
n	O
)	O
bessel	O
function	B
of	O
the	O
ﬁrst	O
kind	O
covariance	B
(	O
or	O
kernel	B
)	O
function	B
evaluated	O
at	O
x	O
and	O
x0	O
n	O
×	O
n	O
covariance	B
(	O
or	O
gram	O
)	O
matrix	B
n	O
×	O
n∗	O
matrix	B
k	O
(	O
x	O
,	O
x∗	O
)	O
,	O
the	O
covariance	B
between	O
training	O
and	O
test	O
cases	O
vector	O
,	O
short	O
for	O
k	O
(	O
x	O
,	O
x∗	O
)	O
,	O
when	O
there	O
is	O
only	O
a	O
single	O
test	O
case	O
covariance	B
matrix	I
for	O
the	O
(	O
noise	O
free	O
)	O
f	O
values	O
gaussian	O
process	B
:	O
f	O
∼	O
gp	O
(	O
cid:0	O
)	O
m	O
(	O
x	O
)	O
,	O
k	O
(	O
x	O
,	O
x0	O
)	O
(	O
cid:1	O
)	O
,	O
the	O
function	B
f	O
is	O
distributed	O
as	O
a	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
xviii	O
symbol	O
ky	O
kν	O
(	O
z	O
)	O
l	O
(	O
a	O
,	O
b	O
)	O
log	O
(	O
z	O
)	O
log2	O
(	O
z	O
)	O
‘	O
or	O
‘	O
d	O
λ	O
(	O
z	O
)	O
m	O
(	O
x	O
)	O
µ	O
n	O
(	O
µ	O
,	O
σ	O
)	O
or	O
n	O
(	O
x|µ	O
,	O
σ	O
)	O
n	O
(	O
x	O
)	O
n	O
and	O
n∗	O
n	O
nh	O
n	O
o	O
(	O
·	O
)	O
o	O
y|x	O
and	O
p	O
(	O
y|x	O
)	O
pn	O
φ	O
(	O
xi	O
)	O
or	O
φ	O
(	O
x	O
)	O
φ	O
(	O
z	O
)	O
π	O
(	O
x	O
)	O
ˆπ	O
(	O
x∗	O
)	O
¯π	O
(	O
x∗	O
)	O
r	O
rl	O
(	O
f	O
)	O
or	O
rl	O
(	O
c	O
)	O
˜rl	O
(	O
l|x∗	O
)	O
rc	O
s	O
(	O
s	O
)	O
σ	O
(	O
z	O
)	O
σ2	O
f	O
σ2	O
n	O
θ	O
tr	O
(	O
a	O
)	O
tl	O
v	O
or	O
vq	O
(	O
x	O
)	O
[	O
z	O
(	O
x	O
)	O
]	O
x	O
x	O
x∗	O
xi	O
xdi	O
z	O
symbols	O
and	O
notation	O
meaning	O
covariance	B
matrix	I
for	O
the	O
(	O
noisy	O
)	O
y	O
values	O
;	O
for	O
independent	O
homoscedastic	O
noise	O
,	O
ky	O
=	O
kf	O
+	O
σ2	O
ni	O
modiﬁed	O
bessel	O
function	B
loss	O
function	B
,	O
the	O
loss	B
of	O
predicting	O
b	O
,	O
when	O
a	O
is	O
true	O
;	O
note	O
argument	O
order	O
natural	O
logarithm	O
(	O
base	O
e	O
)	O
logarithm	O
to	O
the	O
base	O
2	O
characteristic	O
length-scale	B
(	O
for	O
input	O
dimension	O
d	O
)	O
logistic	B
function	I
,	O
λ	O
(	O
z	O
)	O
=	O
1/	O
(	O
cid:0	O
)	O
1	O
+	O
exp	O
(	O
−z	O
)	O
(	O
cid:1	O
)	O
the	O
mean	B
function	I
of	O
a	O
gaussian	O
process	B
a	O
measure	B
(	O
see	B
section	O
a.7	O
)	O
(	O
the	O
variable	O
x	O
has	O
a	O
)	O
gaussian	O
(	O
normal	O
)	O
distribution	O
with	O
mean	O
vector	O
µ	O
and	O
covariance	B
matrix	I
σ	O
short	O
for	O
unit	O
gaussian	O
x	O
∼	O
n	O
(	O
0	O
,	O
i	O
)	O
number	O
of	O
training	O
(	O
and	O
test	O
)	O
cases	O
dimension	O
of	O
feature	B
space	I
number	O
of	O
hidden	O
units	O
in	O
a	O
neural	B
network	I
the	O
natural	O
numbers	O
,	O
the	O
positive	O
integers	O
big	O
oh	O
;	O
for	O
functions	O
f	O
and	O
g	O
on	O
n	O
,	O
we	O
write	O
f	O
(	O
n	O
)	O
=	O
o	O
(	O
g	O
(	O
n	O
)	O
)	O
if	O
the	O
ratio	O
f	O
(	O
n	O
)	O
/g	O
(	O
n	O
)	O
remains	O
bounded	O
as	O
n	O
→	O
∞	O
either	O
matrix	B
of	O
all	O
zeros	O
or	O
diﬀerential	O
operator	B
conditional	O
random	O
variable	O
y	O
given	O
x	O
and	O
its	O
probability	B
(	O
density	O
)	O
the	O
regular	O
n-polygon	O
feature	O
map	O
of	O
input	O
xi	O
(	O
or	O
input	O
set	B
x	O
)	O
cumulative	O
unit	O
gaussian	O
:	O
φ	O
(	O
z	O
)	O
=	O
(	O
2π	O
)	O
−1/2r	O
z	O
−∞	O
exp	O
(	O
−t2/2	O
)	O
dt	O
the	O
sigmoid	O
of	O
the	O
latent	O
value	O
:	O
π	O
(	O
x	O
)	O
=	O
σ	O
(	O
f	O
(	O
x	O
)	O
)	O
(	O
stochastic	O
if	O
f	O
(	O
x	O
)	O
is	O
stochastic	O
)	O
map	O
prediction	B
:	O
π	O
evaluated	O
at	O
¯f	O
(	O
x∗	O
)	O
.	O
mean	O
prediction	O
:	O
expected	O
value	O
of	O
π	O
(	O
x∗	O
)	O
.	O
note	O
,	O
in	O
general	O
that	O
ˆπ	O
(	O
x∗	O
)	O
6=	O
¯π	O
(	O
x∗	O
)	O
the	O
real	O
numbers	O
the	O
risk	B
or	O
expected	O
loss	B
for	O
f	O
,	O
or	O
classiﬁer	B
c	O
(	O
averaged	B
w.r.t	O
.	O
inputs	O
and	O
outputs	B
)	O
expected	O
loss	B
for	O
predicting	O
l	O
,	O
averaged	B
w.r.t	O
.	O
the	O
model	B
’	O
s	O
pred	O
.	O
distr	O
.	O
at	O
x∗	O
decision	B
region	I
for	O
class	O
c	O
power	O
spectrum	O
any	O
sigmoid	O
function	B
,	O
e.g	O
.	O
logistic	B
λ	O
(	O
z	O
)	O
,	O
cumulative	O
gaussian	O
φ	O
(	O
z	O
)	O
,	O
etc	O
.	O
variance	O
of	O
the	O
(	O
noise	O
free	O
)	O
signal	O
noise	O
variance	O
vector	O
of	O
hyperparameters	B
(	O
parameters	O
of	O
the	O
covariance	B
function	I
)	O
trace	O
of	O
(	O
square	O
)	O
matrix	B
a	O
the	O
circle	O
with	O
circumference	O
l	O
variance	O
;	O
variance	O
of	O
z	O
(	O
x	O
)	O
when	O
x	O
∼	O
q	O
(	O
x	O
)	O
input	O
space	O
and	O
also	O
the	O
index	O
set	B
for	O
the	O
stochastic	O
process	O
d	O
×	O
n	O
matrix	B
of	O
the	O
training	O
inputs	O
{	O
xi	O
}	O
n	O
matrix	B
of	O
test	O
inputs	O
the	O
ith	O
training	O
input	O
the	O
dth	O
coordinate	O
of	O
the	O
ith	O
training	O
input	O
xi	O
the	O
integers	O
.	O
.	O
.	O
,	O
−2	O
,	O
−1	O
,	O
0	O
,	O
1	O
,	O
2	O
,	O
.	O
.	O
.	O
i=1	O
:	O
the	O
design	O
matrix	B
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
chapter	O
1	O
introduction	O
in	O
this	O
book	O
we	O
will	O
be	O
concerned	O
with	O
supervised	B
learning	I
,	O
which	O
is	O
the	O
problem	O
of	O
learning	B
input-output	O
mappings	O
from	O
empirical	O
data	O
(	O
the	O
training	O
dataset	B
)	O
.	O
depending	O
on	O
the	O
characteristics	O
of	O
the	O
output	O
,	O
this	O
problem	O
is	O
known	O
as	O
either	O
regression	B
,	O
for	O
continuous	O
outputs	B
,	O
or	O
classiﬁcation	B
,	O
when	O
outputs	B
are	O
discrete	O
.	O
a	O
well	O
known	O
example	O
is	O
the	O
classiﬁcation	B
of	O
images	O
of	O
handwritten	O
digits	O
.	O
the	O
training	O
set	B
consists	O
of	O
small	O
digitized	O
images	O
,	O
together	O
with	O
a	O
classiﬁcation	B
from	O
0	O
,	O
.	O
.	O
.	O
,	O
9	O
,	O
normally	O
provided	O
by	O
a	O
human	O
.	O
the	O
goal	O
is	O
to	O
learn	O
a	O
mapping	O
from	O
image	O
to	O
classiﬁcation	B
label	O
,	O
which	O
can	O
then	O
be	O
used	O
on	O
new	O
,	O
unseen	O
images	O
.	O
supervised	B
learning	I
is	O
an	O
attractive	O
way	O
to	O
attempt	O
to	O
tackle	O
this	O
problem	O
,	O
since	O
it	O
is	O
not	O
easy	O
to	O
specify	O
accurately	O
the	O
characteristics	O
of	O
,	O
say	O
,	O
the	O
handwritten	O
digit	O
4.	O
an	O
example	O
of	O
a	O
regression	B
problem	O
can	O
be	O
found	O
in	O
robotics	O
,	O
where	O
we	O
wish	O
to	O
learn	O
the	O
inverse	O
dynamics	O
of	O
a	O
robot	B
arm	O
.	O
here	O
the	O
task	O
is	O
to	O
map	O
from	O
the	O
state	O
of	O
the	O
arm	O
(	O
given	O
by	O
the	O
positions	O
,	O
velocities	O
and	O
accelerations	O
of	O
the	O
joints	O
)	O
to	O
the	O
corresponding	O
torques	O
on	O
the	O
joints	O
.	O
such	O
a	O
model	B
can	O
then	O
be	O
used	O
to	O
compute	O
the	O
torques	O
needed	O
to	O
move	O
the	O
arm	O
along	O
a	O
given	O
trajectory	O
.	O
another	O
example	O
would	O
be	O
in	O
a	O
chemical	O
plant	O
,	O
where	O
we	O
might	O
wish	O
to	O
predict	O
the	O
yield	O
as	O
a	O
function	B
of	O
process	B
parameters	O
such	O
as	O
temperature	O
,	O
pressure	O
,	O
amount	O
of	O
catalyst	O
etc	O
.	O
in	O
general	O
we	O
denote	O
the	O
input	O
as	O
x	O
,	O
and	O
the	O
output	O
(	O
or	O
target	O
)	O
as	O
y.	O
the	O
input	O
is	O
usually	O
represented	O
as	O
a	O
vector	O
x	O
as	O
there	O
are	O
in	O
general	O
many	O
input	O
variables—in	O
the	O
handwritten	O
digit	O
recognition	O
example	O
one	O
may	O
have	O
a	O
256-	O
dimensional	O
input	O
obtained	O
from	O
a	O
raster	O
scan	O
of	O
a	O
16	O
×	O
16	O
image	O
,	O
and	O
in	O
the	O
robot	B
arm	O
example	O
there	O
are	O
three	O
input	O
measurements	O
for	O
each	O
joint	B
in	O
the	O
arm	O
.	O
the	O
target	O
y	O
may	O
either	O
be	O
continuous	O
(	O
as	O
in	O
the	O
regression	B
case	O
)	O
or	O
discrete	O
(	O
as	O
in	O
the	O
classiﬁcation	B
case	O
)	O
.	O
we	O
have	O
a	O
dataset	B
d	O
of	O
n	O
observations	O
,	O
d	O
=	O
{	O
(	O
xi	O
,	O
yi	O
)	O
|i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
.	O
given	O
this	O
training	O
data	O
we	O
wish	O
to	O
make	O
predictions	O
for	O
new	O
inputs	O
x∗	O
that	O
we	O
have	O
not	O
seen	O
in	O
the	O
training	O
set	B
.	O
thus	O
it	O
is	O
clear	O
that	O
the	O
problem	O
at	O
hand	O
is	O
inductive	B
;	O
we	O
need	O
to	O
move	O
from	O
the	O
ﬁnite	O
training	O
data	O
d	O
to	O
a	O
digit	O
classiﬁcation	B
robotic	O
control	O
the	O
dataset	B
training	O
is	O
inductive	B
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
2	O
introduction	O
two	O
approaches	O
gaussian	O
process	B
consistency	O
tractability	O
function	B
f	O
that	O
makes	O
predictions	O
for	O
all	O
possible	O
input	O
values	O
.	O
to	O
do	O
this	O
we	O
must	O
make	O
assumptions	O
about	O
the	O
characteristics	O
of	O
the	O
underlying	O
function	B
,	O
as	O
otherwise	O
any	O
function	B
which	O
is	O
consistent	O
with	O
the	O
training	O
data	O
would	O
be	O
equally	O
valid	O
.	O
a	O
wide	O
variety	O
of	O
methods	O
have	O
been	O
proposed	O
to	O
deal	O
with	O
the	O
supervised	B
learning	I
problem	O
;	O
here	O
we	O
describe	O
two	O
common	O
approaches	O
.	O
the	O
ﬁrst	O
is	O
to	O
restrict	O
the	O
class	O
of	O
functions	O
that	O
we	O
consider	O
,	O
for	O
example	O
by	O
only	O
considering	O
linear	B
functions	O
of	O
the	O
input	O
.	O
the	O
second	O
approach	O
is	O
(	O
speaking	O
rather	O
loosely	O
)	O
to	O
give	O
a	O
prior	O
probability	B
to	O
every	O
possible	O
function	B
,	O
where	O
higher	O
probabilities	O
are	O
given	O
to	O
functions	O
that	O
we	O
consider	O
to	O
be	O
more	O
likely	O
,	O
for	O
example	O
because	O
they	O
are	O
smoother	O
than	O
other	O
functions.1	O
the	O
ﬁrst	O
approach	O
has	O
an	O
obvious	O
problem	O
in	O
that	O
we	O
have	O
to	O
decide	O
upon	O
the	O
richness	O
of	O
the	O
class	O
of	O
functions	O
considered	O
;	O
if	O
we	O
are	O
using	O
a	O
model	B
based	O
on	O
a	O
certain	O
class	O
of	O
functions	O
(	O
e.g	O
.	O
linear	B
functions	O
)	O
and	O
the	O
target	O
function	B
is	O
not	O
well	O
modelled	O
by	O
this	O
class	O
,	O
then	O
the	O
predictions	O
will	O
be	O
poor	O
.	O
one	O
may	O
be	O
tempted	O
to	O
increase	O
the	O
ﬂexibility	O
of	O
the	O
class	O
of	O
functions	O
,	O
but	O
this	O
runs	O
into	O
the	O
danger	O
of	O
overﬁtting	O
,	O
where	O
we	O
can	O
obtain	O
a	O
good	O
ﬁt	O
to	O
the	O
training	O
data	O
,	O
but	O
perform	O
badly	O
when	O
making	O
test	O
predictions	O
.	O
the	O
second	O
approach	O
appears	O
to	O
have	O
a	O
serious	O
problem	O
,	O
in	O
that	O
surely	O
there	O
are	O
an	O
uncountably	O
inﬁnite	O
set	B
of	O
possible	O
functions	O
,	O
and	O
how	O
are	O
we	O
going	O
to	O
compute	O
with	O
this	O
set	B
in	O
ﬁnite	O
time	O
?	O
this	O
is	O
where	O
the	O
gaussian	O
process	B
comes	O
to	O
our	O
rescue	O
.	O
a	O
gaussian	O
process	B
is	O
a	O
generalization	B
of	O
the	O
gaussian	O
probability	B
distribution	O
.	O
whereas	O
a	O
probability	B
distribution	O
describes	O
random	O
variables	O
which	O
are	O
scalars	O
or	O
vectors	O
(	O
for	O
multivariate	O
distributions	O
)	O
,	O
a	O
stochastic	O
process	O
governs	O
the	O
properties	O
of	O
functions	O
.	O
leaving	O
mathematical	O
sophistication	O
aside	O
,	O
one	O
can	O
loosely	O
think	O
of	O
a	O
function	B
as	O
a	O
very	O
long	O
vector	O
,	O
each	O
entry	O
in	O
the	O
vector	O
specifying	O
the	O
function	B
value	O
f	O
(	O
x	O
)	O
at	O
a	O
particular	O
input	O
x.	O
it	O
turns	O
out	O
,	O
that	O
although	O
this	O
idea	O
is	O
a	O
little	O
na¨ıve	O
,	O
it	O
is	O
surprisingly	O
close	O
what	O
we	O
need	O
.	O
indeed	O
,	O
the	O
question	O
of	O
how	O
we	O
deal	O
computationally	O
with	O
these	O
inﬁnite	O
dimensional	O
objects	O
has	O
the	O
most	O
pleasant	O
resolution	O
imaginable	O
:	O
if	O
you	O
ask	O
only	O
for	O
the	O
properties	O
of	O
the	O
function	B
at	O
a	O
ﬁnite	O
number	O
of	O
points	O
,	O
then	O
inference	O
in	O
the	O
gaussian	O
process	B
will	O
give	O
you	O
the	O
same	O
answer	O
if	O
you	O
ignore	O
the	O
inﬁnitely	O
many	O
other	O
points	O
,	O
as	O
if	O
you	O
would	O
have	O
taken	O
them	O
all	O
into	O
account	O
!	O
and	O
these	O
answers	O
are	O
consistent	O
with	O
answers	O
to	O
any	O
other	O
ﬁnite	O
queries	O
you	O
may	O
have	O
.	O
one	O
of	O
the	O
main	O
attractions	O
of	O
the	O
gaussian	O
process	B
framework	O
is	O
precisely	O
that	O
it	O
unites	O
a	O
sophisticated	O
and	O
consistent	O
view	O
with	O
computational	O
tractability	O
.	O
it	O
should	O
come	O
as	O
no	O
surprise	O
that	O
these	O
ideas	O
have	O
been	O
around	O
for	O
some	O
time	O
,	O
although	O
they	O
are	O
perhaps	O
not	O
as	O
well	O
known	O
as	O
they	O
might	O
be	O
.	O
indeed	O
,	O
many	O
models	O
that	O
are	O
commonly	O
employed	O
in	O
both	O
machine	O
learning	B
and	O
statis-	O
tics	O
are	O
in	O
fact	O
special	O
cases	O
of	O
,	O
or	O
restricted	O
kinds	O
of	O
gaussian	O
processes	O
.	O
in	O
this	O
volume	O
,	O
we	O
aim	O
to	O
give	O
a	O
systematic	O
and	O
uniﬁed	O
treatment	O
of	O
the	O
area	O
,	O
showing	O
connections	O
to	O
related	O
models	O
.	O
1these	O
two	O
approaches	O
may	O
be	O
regarded	O
as	O
imposing	O
a	O
restriction	O
bias	B
and	O
a	O
preference	O
bias	B
respectively	O
;	O
see	B
e.g	O
.	O
mitchell	O
[	O
1997	O
]	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
1.1	O
a	O
pictorial	O
introduction	O
to	O
bayesian	O
modelling	O
3	O
(	O
a	O
)	O
,	O
prior	O
(	O
b	O
)	O
,	O
posterior	O
figure	O
1.1	O
:	O
panel	O
(	O
a	O
)	O
shows	O
four	O
samples	O
drawn	O
from	O
the	O
prior	O
distribution	O
.	O
panel	O
(	O
b	O
)	O
shows	O
the	O
situation	O
after	O
two	O
datapoints	O
have	O
been	O
observed	O
.	O
the	O
mean	O
prediction	O
is	O
shown	O
as	O
the	O
solid	O
line	O
and	O
four	O
samples	O
from	O
the	O
posterior	O
are	O
shown	O
as	O
dashed	O
lines	O
.	O
in	O
both	O
plots	O
the	O
shaded	O
region	O
denotes	O
twice	O
the	O
standard	O
deviation	O
at	O
each	O
input	O
value	O
x	O
.	O
1.1	O
a	O
pictorial	O
introduction	O
to	O
bayesian	O
mod-	O
elling	O
in	O
this	O
section	O
we	O
give	O
graphical	O
illustrations	O
of	O
how	O
the	O
second	O
(	O
bayesian	O
)	O
method	O
works	O
on	O
some	O
simple	O
regression	B
and	O
classiﬁcation	B
examples	O
.	O
we	O
ﬁrst	O
consider	O
a	O
simple	O
1-d	O
regression	B
problem	O
,	O
mapping	O
from	O
an	O
input	O
x	O
to	O
an	O
output	O
f	O
(	O
x	O
)	O
.	O
in	O
figure	O
1.1	O
(	O
a	O
)	O
we	O
show	O
a	O
number	O
of	O
sample	O
functions	O
drawn	O
at	O
random	O
from	O
the	O
prior	O
distribution	O
over	O
functions	O
speciﬁed	O
by	O
a	O
par-	O
ticular	O
gaussian	O
process	B
which	O
favours	O
smooth	O
functions	O
.	O
this	O
prior	O
is	O
taken	O
to	O
represent	O
our	O
prior	O
beliefs	O
over	O
the	O
kinds	O
of	O
functions	O
we	O
expect	O
to	O
observe	O
,	O
before	O
seeing	O
any	O
data	O
.	O
in	O
the	O
absence	O
of	O
knowledge	O
to	O
the	O
contrary	O
we	O
have	O
assumed	O
that	O
the	O
average	O
value	O
over	O
the	O
sample	O
functions	O
at	O
each	O
x	O
is	O
zero	O
.	O
although	O
the	O
speciﬁc	O
random	O
functions	O
drawn	O
in	O
figure	O
1.1	O
(	O
a	O
)	O
do	O
not	O
have	O
a	O
mean	O
of	O
zero	O
,	O
the	O
mean	O
of	O
f	O
(	O
x	O
)	O
values	O
for	O
any	O
ﬁxed	O
x	O
would	O
become	O
zero	O
,	O
in-	O
dependent	O
of	O
x	O
as	O
we	O
kept	O
on	O
drawing	O
more	O
functions	O
.	O
at	O
any	O
value	O
of	O
x	O
we	O
can	O
also	O
characterize	O
the	O
variability	O
of	O
the	O
sample	O
functions	O
by	O
computing	O
the	O
variance	O
at	O
that	O
point	O
.	O
the	O
shaded	O
region	O
denotes	O
twice	O
the	O
pointwise	O
standard	O
deviation	O
;	O
in	O
this	O
case	O
we	O
used	O
a	O
gaussian	O
process	B
which	O
speciﬁes	O
that	O
the	O
prior	O
variance	O
does	O
not	O
depend	O
on	O
x.	O
suppose	O
that	O
we	O
are	O
then	O
given	O
a	O
dataset	B
d	O
=	O
{	O
(	O
x1	O
,	O
y1	O
)	O
,	O
(	O
x2	O
,	O
y2	O
)	O
}	O
consist-	O
ing	O
of	O
two	O
observations	O
,	O
and	O
we	O
wish	O
now	O
to	O
only	O
consider	O
functions	O
that	O
pass	O
though	O
these	O
two	O
data	O
points	O
exactly	O
.	O
(	O
it	O
is	O
also	O
possible	O
to	O
give	O
higher	O
pref-	O
erence	O
to	O
functions	O
that	O
merely	O
pass	O
“	O
close	O
”	O
to	O
the	O
datapoints	O
.	O
)	O
this	O
situation	O
is	O
illustrated	O
in	O
figure	O
1.1	O
(	O
b	O
)	O
.	O
the	O
dashed	O
lines	O
show	O
sample	O
functions	O
which	O
are	O
consistent	O
with	O
d	O
,	O
and	O
the	O
solid	O
line	O
depicts	O
the	O
mean	O
value	O
of	O
such	O
func-	O
tions	O
.	O
notice	O
how	O
the	O
uncertainty	O
is	O
reduced	O
close	O
to	O
the	O
observations	O
.	O
the	O
combination	O
of	O
the	O
prior	O
and	O
the	O
data	O
leads	O
to	O
the	O
posterior	O
distribution	O
over	O
functions	O
.	O
regression	B
random	O
functions	O
mean	B
function	I
pointwise	O
variance	O
functions	O
that	O
agree	O
with	O
observations	O
posterior	O
over	O
functions	O
00.51−2−1012input	O
,	O
xf	O
(	O
x	O
)	O
00.51−2−1012input	O
,	O
xf	O
(	O
x	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
4	O
non-parametric	B
inference	O
prior	O
speciﬁcation	O
covariance	B
function	I
modelling	O
and	O
interpreting	O
classiﬁcation	B
squashing	O
function	B
introduction	O
if	O
more	O
datapoints	O
were	O
added	O
one	O
would	O
see	B
the	O
mean	B
function	I
adjust	O
itself	O
to	O
pass	O
through	O
these	O
points	O
,	O
and	O
that	O
the	O
posterior	O
uncertainty	O
would	O
reduce	O
close	O
to	O
the	O
observations	O
.	O
notice	O
,	O
that	O
since	O
the	O
gaussian	O
process	B
is	O
not	O
a	O
parametric	B
model	O
,	O
we	O
do	O
not	O
have	O
to	O
worry	O
about	O
whether	O
it	O
is	O
possible	O
for	O
the	O
model	B
to	O
ﬁt	O
the	O
data	O
(	O
as	O
would	O
be	O
the	O
case	O
if	O
e.g	O
.	O
you	O
tried	O
a	O
linear	B
model	O
on	O
strongly	O
non-linear	O
data	O
)	O
.	O
even	O
when	O
a	O
lot	O
of	O
observations	O
have	O
been	O
added	O
,	O
there	O
may	O
still	O
be	O
some	O
ﬂexibility	O
left	O
in	O
the	O
functions	O
.	O
one	O
way	O
to	O
imagine	O
the	O
reduction	O
of	O
ﬂexibility	O
in	O
the	O
distribution	O
of	O
functions	O
as	O
the	O
data	O
arrives	O
is	O
to	O
draw	O
many	O
random	O
functions	O
from	O
the	O
prior	O
,	O
and	O
reject	O
the	O
ones	O
which	O
do	O
not	O
agree	O
with	O
the	O
observations	O
.	O
while	O
this	O
is	O
a	O
perfectly	O
valid	O
way	O
to	O
do	O
inference	O
,	O
it	O
is	O
impractical	O
for	O
most	O
purposes—the	O
exact	O
analytical	O
computations	O
required	O
to	O
quantify	O
these	O
properties	O
will	O
be	O
detailed	O
in	O
the	O
next	O
chapter	O
.	O
the	O
speciﬁcation	O
of	O
the	O
prior	O
is	O
important	O
,	O
because	O
it	O
ﬁxes	O
the	O
properties	O
of	O
the	O
functions	O
considered	O
for	O
inference	O
.	O
above	O
we	O
brieﬂy	O
touched	O
on	O
the	O
mean	O
and	O
pointwise	O
variance	O
of	O
the	O
functions	O
.	O
however	O
,	O
other	O
characteristics	O
can	O
also	O
be	O
speciﬁed	O
and	O
manipulated	O
.	O
note	O
that	O
the	O
functions	O
in	O
figure	O
1.1	O
(	O
a	O
)	O
are	O
smooth	O
and	O
stationary	O
(	O
informally	O
,	O
stationarity	B
means	O
that	O
the	O
functions	O
look	O
similar	O
at	O
all	O
x	O
locations	O
)	O
.	O
these	O
are	O
properties	O
which	O
are	O
induced	O
by	O
the	O
co-	O
variance	O
function	B
of	O
the	O
gaussian	O
process	B
;	O
many	O
other	O
covariance	B
functions	O
are	O
possible	O
.	O
suppose	O
,	O
that	O
for	O
a	O
particular	O
application	O
,	O
we	O
think	O
that	O
the	O
functions	O
in	O
figure	O
1.1	O
(	O
a	O
)	O
vary	O
too	O
rapidly	O
(	O
i.e	O
.	O
that	O
their	O
characteristic	O
length-scale	B
is	O
too	O
short	O
)	O
.	O
slower	O
variation	O
is	O
achieved	O
by	O
simply	O
adjusting	O
parameters	O
of	O
the	O
covariance	B
function	I
.	O
the	O
problem	O
of	O
learning	B
in	O
gaussian	O
processes	O
is	O
exactly	O
the	O
problem	O
of	O
ﬁnding	O
suitable	O
properties	O
for	O
the	O
covariance	B
function	I
.	O
note	O
,	O
that	O
this	O
gives	O
us	O
a	O
model	B
of	O
the	O
data	O
,	O
and	O
characteristics	O
(	O
such	O
a	O
smoothness	O
,	O
characteristic	O
length-scale	B
,	O
etc	O
.	O
)	O
which	O
we	O
can	O
interpret	O
.	O
we	O
now	O
turn	O
to	O
the	O
classiﬁcation	B
case	O
,	O
and	O
consider	O
the	O
binary	B
(	O
or	O
two-	O
class	O
)	O
classiﬁcation	B
problem	O
.	O
an	O
example	O
of	O
this	O
is	O
classifying	O
objects	O
detected	O
in	O
astronomical	O
sky	O
surveys	O
into	O
stars	O
or	O
galaxies	O
.	O
our	O
data	O
has	O
the	O
label	O
+1	O
for	O
stars	O
and	O
−1	O
for	O
galaxies	O
,	O
and	O
our	O
task	O
will	O
be	O
to	O
predict	O
π	O
(	O
x	O
)	O
,	O
the	O
probability	B
that	O
an	O
example	O
with	O
input	O
vector	O
x	O
is	O
a	O
star	O
,	O
using	O
as	O
inputs	O
some	O
features	O
that	O
describe	O
each	O
object	O
.	O
obviously	O
π	O
(	O
x	O
)	O
should	O
lie	O
in	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
.	O
a	O
gaussian	O
process	B
prior	O
over	O
functions	O
does	O
not	O
restrict	O
the	O
output	O
to	O
lie	O
in	O
this	O
interval	O
,	O
as	O
can	O
be	O
seen	O
from	O
figure	O
1.1	O
(	O
a	O
)	O
.	O
the	O
approach	O
that	O
we	O
shall	O
adopt	O
is	O
to	O
squash	O
the	O
prior	O
function	B
f	O
pointwise	O
through	O
a	O
response	B
function	I
which	O
restricts	O
the	O
output	O
to	O
lie	O
in	O
[	O
0	O
,	O
1	O
]	O
.	O
a	O
common	O
choice	O
for	O
this	O
function	B
is	O
the	O
logistic	B
function	I
λ	O
(	O
z	O
)	O
=	O
(	O
1	O
+	O
exp	O
(	O
−z	O
)	O
)	O
−1	O
,	O
illustrated	O
in	O
figure	O
1.2	O
(	O
b	O
)	O
.	O
thus	O
the	O
prior	O
over	O
f	O
induces	O
a	O
prior	O
over	O
probabilistic	B
classiﬁcations	O
π.	O
this	O
set	B
up	O
is	O
illustrated	O
in	O
figure	O
1.2	O
for	O
a	O
2-d	O
input	O
space	O
.	O
in	O
panel	O
(	O
a	O
)	O
we	O
see	B
a	O
sample	O
drawn	O
from	O
the	O
prior	O
over	O
functions	O
f	O
which	O
is	O
squashed	O
through	O
the	O
logistic	B
function	I
(	O
panel	O
(	O
b	O
)	O
)	O
.	O
a	O
dataset	B
is	O
shown	O
in	O
panel	O
(	O
c	O
)	O
,	O
where	O
the	O
white	O
and	O
black	O
circles	O
denote	O
classes	O
+1	O
and	O
−1	O
respectively	O
.	O
as	O
in	O
the	O
regression	B
case	O
the	O
eﬀect	O
of	O
the	O
data	O
is	O
to	O
downweight	O
in	O
the	O
posterior	O
those	O
functions	O
that	O
are	O
incompatible	O
with	O
the	O
data	O
.	O
a	O
contour	O
plot	O
of	O
the	O
posterior	O
mean	O
for	O
π	O
(	O
x	O
)	O
is	O
shown	O
in	O
panel	O
(	O
d	O
)	O
.	O
in	O
this	O
example	O
we	O
have	O
chosen	O
a	O
short	O
characteristic	O
length-scale	B
for	O
the	O
process	B
so	O
that	O
it	O
can	O
vary	O
fairly	O
rapidly	O
;	O
in	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
1.2	O
roadmap	O
5	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
1.2	O
:	O
panel	O
(	O
a	O
)	O
shows	O
a	O
sample	O
from	O
prior	O
distribution	O
on	O
f	O
in	O
a	O
2-d	O
input	O
space	O
.	O
panel	O
(	O
b	O
)	O
is	O
a	O
plot	O
of	O
the	O
logistic	B
function	I
λ	O
(	O
z	O
)	O
.	O
panel	O
(	O
c	O
)	O
shows	O
the	O
location	O
of	O
the	O
data	O
points	O
,	O
where	O
the	O
open	O
circles	O
denote	O
the	O
class	O
label	O
+1	O
,	O
and	O
closed	O
circles	O
denote	O
the	O
class	O
label	O
−1	O
.	O
panel	O
(	O
d	O
)	O
shows	O
a	O
contour	O
plot	O
of	O
the	O
mean	O
predictive	O
probability	B
as	O
a	O
function	B
of	O
x	O
;	O
the	O
decision	O
boundaries	O
between	O
the	O
two	O
classes	O
are	O
shown	O
by	O
the	O
thicker	O
lines	O
.	O
this	O
case	O
notice	O
that	O
all	O
of	O
the	O
training	O
points	O
are	O
correctly	O
classiﬁed	O
,	O
including	O
the	O
two	O
“	O
outliers	O
”	O
in	O
the	O
ne	O
and	O
sw	O
corners	O
.	O
by	O
choosing	O
a	O
diﬀerent	O
length-	O
scale	O
we	O
can	O
change	O
this	O
behaviour	O
,	O
as	O
illustrated	O
in	O
section	O
3.7.1	O
.	O
1.2	O
roadmap	O
the	O
book	O
has	O
a	O
natural	O
split	O
into	O
two	O
parts	O
,	O
with	O
the	O
chapters	O
up	O
to	O
and	O
includ-	O
ing	O
chapter	O
5	O
covering	O
core	O
material	O
,	O
and	O
the	O
remaining	O
chapters	O
covering	O
the	O
connections	O
to	O
other	O
methods	O
,	O
fast	O
approximations	O
,	O
and	O
more	O
specialized	O
prop-	O
erties	O
.	O
some	O
sections	O
are	O
marked	O
by	O
an	O
asterisk	O
.	O
these	O
sections	O
may	O
be	O
omitted	O
on	O
a	O
ﬁrst	O
reading	O
,	O
and	O
are	O
not	O
pre-requisites	O
for	O
later	O
(	O
un-starred	O
)	O
material	O
.	O
−50501logistic	O
function	B
(	O
cid:176	O
)	O
(	O
cid:176	O
)	O
(	O
cid:176	O
)	O
•	O
(	O
cid:176	O
)	O
(	O
cid:176	O
)	O
(	O
cid:176	O
)	O
(	O
cid:176	O
)	O
(	O
cid:176	O
)	O
•	O
(	O
cid:176	O
)	O
••••••	O
(	O
cid:176	O
)	O
••0.250.50.50.50.750.25	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
6	O
regression	B
classiﬁcation	O
covariance	B
functions	O
learning	B
connections	O
theory	O
fast	O
approximations	O
introduction	O
chapter	O
2	O
contains	O
the	O
deﬁnition	O
of	O
gaussian	O
processes	O
,	O
in	O
particular	O
for	O
the	O
use	O
in	O
regression	B
.	O
it	O
also	O
discusses	O
the	O
computations	O
needed	O
to	O
make	O
predic-	O
tions	O
for	O
regression	B
.	O
under	O
the	O
assumption	O
of	O
gaussian	O
observation	O
noise	O
the	O
computations	O
needed	O
to	O
make	O
predictions	O
are	O
tractable	O
and	O
are	O
dominated	O
by	O
the	O
inversion	O
of	O
a	O
n	O
×	O
n	O
matrix	B
.	O
in	O
a	O
short	O
experimental	O
section	O
,	O
the	O
gaussian	O
process	B
model	O
is	O
applied	O
to	O
a	O
robotics	O
task	O
.	O
chapter	O
3	O
considers	O
the	O
classiﬁcation	B
problem	O
for	O
both	O
binary	B
and	O
multi-	O
class	O
cases	O
.	O
the	O
use	O
of	O
a	O
non-linear	O
response	B
function	I
means	O
that	O
exact	O
compu-	O
tation	O
of	O
the	O
predictions	O
is	O
no	O
longer	O
possible	O
analytically	O
.	O
we	O
discuss	O
a	O
number	O
of	O
approximation	O
schemes	O
,	O
include	O
detailed	O
algorithms	O
for	O
their	O
implementation	O
and	O
discuss	O
some	O
experimental	O
comparisons	O
.	O
as	O
discussed	O
above	O
,	O
the	O
key	O
factor	O
that	O
controls	O
the	O
properties	O
of	O
a	O
gaussian	O
process	B
is	O
the	O
covariance	B
function	I
.	O
much	O
of	O
the	O
work	O
on	O
machine	O
learning	B
so	O
far	O
,	O
has	O
used	O
a	O
very	O
limited	O
set	B
of	O
covariance	B
functions	O
,	O
possibly	O
limiting	O
the	O
power	O
of	O
the	O
resulting	O
models	O
.	O
in	O
chapter	O
4	O
we	O
discuss	O
a	O
number	O
of	O
valid	O
covariance	B
functions	O
and	O
their	O
properties	O
and	O
provide	O
some	O
guidelines	O
on	O
how	O
to	O
combine	O
covariance	B
functions	O
into	O
new	O
ones	O
,	O
tailored	O
to	O
speciﬁc	O
needs	O
.	O
many	O
covariance	B
functions	O
have	O
adjustable	O
parameters	O
,	O
such	O
as	O
the	O
char-	O
acteristic	O
length-scale	B
and	O
variance	O
illustrated	O
in	O
figure	O
1.1.	O
chapter	O
5	O
de-	O
scribes	O
how	O
such	O
parameters	O
can	O
be	O
inferred	O
or	O
learned	O
from	O
the	O
data	O
,	O
based	O
on	O
either	O
bayesian	O
methods	O
(	O
using	O
the	O
marginal	B
likelihood	I
)	O
or	O
methods	O
of	O
cross-	O
validation	O
.	O
explicit	O
algorithms	O
are	O
provided	O
for	O
some	O
schemes	O
,	O
and	O
some	O
simple	O
practical	O
examples	O
are	O
demonstrated	O
.	O
gaussian	O
process	B
predictors	O
are	O
an	O
example	O
of	O
a	O
class	O
of	O
methods	O
known	O
as	O
kernel	B
machines	O
;	O
they	O
are	O
distinguished	O
by	O
the	O
probabilistic	B
viewpoint	O
taken	O
.	O
in	O
chapter	O
6	O
we	O
discuss	O
other	O
kernel	B
machines	O
such	O
as	O
support	B
vector	I
machines	O
(	O
svms	O
)	O
,	O
splines	B
,	O
least-squares	B
classiﬁers	O
and	O
relevance	O
vector	O
machines	O
(	O
rvms	O
)	O
,	O
and	O
their	O
relationships	O
to	O
gaussian	O
process	B
prediction	O
.	O
in	O
chapter	O
7	O
we	O
discuss	O
a	O
number	O
of	O
more	O
theoretical	O
issues	O
relating	O
to	O
gaussian	O
process	B
methods	O
including	O
asymptotic	O
analysis	O
,	O
average-case	O
learning	B
curves	O
and	O
the	O
pac-bayesian	O
framework	O
.	O
one	O
issue	O
with	O
gaussian	O
process	B
prediction	O
methods	O
is	O
that	O
their	O
basic	O
com-	O
plexity	O
is	O
o	O
(	O
n3	O
)	O
,	O
due	O
to	O
the	O
inversion	O
of	O
a	O
n×n	O
matrix	B
.	O
for	O
large	O
datasets	O
this	O
is	O
prohibitive	O
(	O
in	O
both	O
time	O
and	O
space	O
)	O
and	O
so	O
a	O
number	O
of	O
approximation	O
methods	O
have	O
been	O
developed	O
,	O
as	O
described	O
in	O
chapter	O
8.	O
the	O
main	O
focus	O
of	O
the	O
book	O
is	O
on	O
the	O
core	O
supervised	B
learning	I
problems	O
of	O
regression	B
and	O
classiﬁcation	B
.	O
in	O
chapter	O
9	O
we	O
discuss	O
some	O
rather	O
less	O
standard	O
settings	O
that	O
gps	O
have	O
been	O
used	O
in	O
,	O
and	O
complete	O
the	O
main	O
part	O
of	O
the	O
book	O
with	O
some	O
conclusions	O
.	O
appendix	O
a	O
gives	O
some	O
mathematical	O
background	O
,	O
while	O
appendix	O
b	O
deals	O
speciﬁcally	O
with	O
gaussian	O
markov	O
processes	O
.	O
appendix	O
c	O
gives	O
details	O
of	O
how	O
to	O
access	O
the	O
data	O
and	O
programs	O
that	O
were	O
used	O
to	O
make	O
the	O
some	O
of	O
the	O
ﬁgures	O
and	O
run	O
the	O
experiments	O
described	O
in	O
the	O
book	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
chapter	O
2	O
regression	B
supervised	O
learning	B
can	O
be	O
divided	O
into	O
regression	B
and	O
classiﬁcation	B
problems	O
.	O
whereas	O
the	O
outputs	B
for	O
classiﬁcation	B
are	O
discrete	O
class	O
labels	O
,	O
regression	B
is	O
concerned	O
with	O
the	O
prediction	B
of	O
continuous	O
quantities	O
.	O
for	O
example	O
,	O
in	O
a	O
ﬁ-	O
nancial	O
application	O
,	O
one	O
may	O
attempt	O
to	O
predict	O
the	O
price	O
of	O
a	O
commodity	O
as	O
a	O
function	B
of	O
interest	O
rates	O
,	O
currency	O
exchange	O
rates	O
,	O
availability	O
and	O
demand	O
.	O
in	O
this	O
chapter	O
we	O
describe	O
gaussian	O
process	B
methods	O
for	O
regression	B
problems	O
;	O
classiﬁcation	B
problems	O
are	O
discussed	O
in	O
chapter	O
3.	O
there	O
are	O
several	O
ways	O
to	O
interpret	O
gaussian	O
process	B
(	O
gp	O
)	O
regression	B
models	O
.	O
one	O
can	O
think	O
of	O
a	O
gaussian	O
process	B
as	O
deﬁning	O
a	O
distribution	O
over	O
functions	O
,	O
and	O
inference	O
taking	O
place	O
directly	O
in	O
the	O
space	O
of	O
functions	O
,	O
the	O
function-space	O
view	O
.	O
although	O
this	O
view	O
is	O
appealing	O
it	O
may	O
initially	O
be	O
diﬃcult	O
to	O
grasp	O
,	O
so	O
we	O
start	O
our	O
exposition	O
in	O
section	O
2.1	O
with	O
the	O
equivalent	B
weight-space	O
view	O
which	O
may	O
be	O
more	O
familiar	O
and	O
accessible	O
to	O
many	O
,	O
and	O
continue	O
in	O
section	O
2.2	O
with	O
the	O
function-space	O
view	O
.	O
gaussian	O
processes	O
often	O
have	O
characteristics	O
that	O
can	O
be	O
changed	O
by	O
setting	O
certain	O
parameters	O
and	O
in	O
section	O
2.3	O
we	O
discuss	O
how	O
the	O
properties	O
change	O
as	O
these	O
parameters	O
are	O
varied	O
.	O
the	O
predictions	O
from	O
a	O
gp	O
model	B
take	O
the	O
form	O
of	O
a	O
full	O
predictive	B
distribution	O
;	O
in	O
section	O
2.4	O
we	O
discuss	O
how	O
to	O
combine	O
a	O
loss	B
function	I
with	O
the	O
predictive	B
distributions	O
using	O
decision	O
theory	O
to	O
make	O
point	O
predictions	O
in	O
an	O
optimal	B
way	O
.	O
a	O
practical	O
comparative	O
example	O
involving	O
the	O
learning	B
of	O
the	O
inverse	O
dynamics	O
of	O
a	O
robot	B
arm	O
is	O
presented	O
in	O
section	O
2.5.	O
we	O
give	O
some	O
theoretical	O
analysis	O
of	O
gaussian	O
process	B
regression	O
in	O
section	O
2.6	O
,	O
and	O
discuss	O
how	O
to	O
incorporate	O
explicit	O
basis	O
functions	O
into	O
the	O
models	O
in	O
section	O
2.7.	O
as	O
much	O
of	O
the	O
material	O
in	O
this	O
chapter	O
can	O
be	O
considered	O
fairly	O
standard	O
,	O
we	O
postpone	O
most	O
references	O
to	O
the	O
historical	O
overview	O
in	O
section	O
2.8	O
.	O
2.1	O
weight-space	O
view	O
the	O
simple	O
linear	B
regression	I
model	O
where	O
the	O
output	O
is	O
a	O
linear	B
combination	O
of	O
the	O
inputs	O
has	O
been	O
studied	O
and	O
used	O
extensively	O
.	O
its	O
main	O
virtues	O
are	O
simplic-	O
two	O
equivalent	B
views	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
8	O
regression	B
ity	O
of	O
implementation	O
and	O
interpretability	O
.	O
its	O
main	O
drawback	O
is	O
that	O
it	O
only	O
allows	O
a	O
limited	O
ﬂexibility	O
;	O
if	O
the	O
relationship	O
between	O
input	O
and	O
output	O
can-	O
not	O
reasonably	O
be	O
approximated	O
by	O
a	O
linear	B
function	O
,	O
the	O
model	B
will	O
give	O
poor	O
predictions	O
.	O
in	O
this	O
section	O
we	O
ﬁrst	O
discuss	O
the	O
bayesian	O
treatment	O
of	O
the	O
linear	B
model	O
.	O
we	O
then	O
make	O
a	O
simple	O
enhancement	O
to	O
this	O
class	O
of	O
models	O
by	O
projecting	O
the	O
inputs	O
into	O
a	O
high-dimensional	O
feature	B
space	I
and	O
applying	O
the	O
linear	B
model	O
there	O
.	O
we	O
show	O
that	O
in	O
some	O
feature	O
spaces	O
one	O
can	O
apply	O
the	O
“	O
kernel	B
trick	I
”	O
to	O
carry	O
out	O
computations	O
implicitly	O
in	O
the	O
high	O
dimensional	O
space	O
;	O
this	O
last	O
step	O
leads	O
to	O
computational	O
savings	O
when	O
the	O
dimensionality	O
of	O
the	O
feature	B
space	I
is	O
large	O
compared	O
to	O
the	O
number	O
of	O
data	O
points	O
.	O
we	O
have	O
a	O
training	O
set	B
d	O
of	O
n	O
observations	O
,	O
d	O
=	O
{	O
(	O
xi	O
,	O
yi	O
)	O
|	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
where	O
x	O
denotes	O
an	O
input	O
vector	O
(	O
covariates	O
)	O
of	O
dimension	O
d	O
and	O
y	O
denotes	O
a	O
scalar	O
output	O
or	O
target	O
(	O
dependent	O
variable	O
)	O
;	O
the	O
column	O
vector	O
inputs	O
for	O
all	O
n	O
cases	O
are	O
aggregated	O
in	O
the	O
d	O
×	O
n	O
design	O
matrix	B
1	O
x	O
,	O
and	O
the	O
targets	O
are	O
collected	O
in	O
the	O
vector	O
y	O
,	O
so	O
we	O
can	O
write	O
d	O
=	O
(	O
x	O
,	O
y	O
)	O
.	O
in	O
the	O
regression	B
setting	O
the	O
targets	O
are	O
real	O
values	O
.	O
we	O
are	O
interested	O
in	O
making	O
inferences	O
about	O
the	O
relationship	O
between	O
inputs	O
and	O
targets	O
,	O
i.e	O
.	O
the	O
conditional	B
distribution	O
of	O
the	O
targets	O
given	O
the	O
inputs	O
(	O
but	O
we	O
are	O
not	O
interested	O
in	O
modelling	O
the	O
input	O
distribution	O
itself	O
)	O
.	O
2.1.1	O
the	O
standard	O
linear	B
model	O
we	O
will	O
review	O
the	O
bayesian	O
analysis	O
of	O
the	O
standard	O
linear	B
regression	I
model	O
with	O
gaussian	O
noise	O
f	O
(	O
x	O
)	O
=	O
x	O
>	O
w	O
,	O
y	O
=	O
f	O
(	O
x	O
)	O
+	O
ε	O
,	O
(	O
2.1	O
)	O
where	O
x	O
is	O
the	O
input	O
vector	O
,	O
w	O
is	O
a	O
vector	O
of	O
weights	O
(	O
parameters	O
)	O
of	O
the	O
linear	B
model	O
,	O
f	O
is	O
the	O
function	B
value	O
and	O
y	O
is	O
the	O
observed	O
target	O
value	O
.	O
often	O
a	O
bias	B
weight	O
or	O
oﬀset	O
is	O
included	O
,	O
but	O
as	O
this	O
can	O
be	O
implemented	O
by	O
augmenting	O
the	O
input	O
vector	O
x	O
with	O
an	O
additional	O
element	O
whose	O
value	O
is	O
always	O
one	O
,	O
we	O
do	O
not	O
explicitly	O
include	O
it	O
in	O
our	O
notation	O
.	O
we	O
have	O
assumed	O
that	O
the	O
observed	O
values	O
y	O
diﬀer	O
from	O
the	O
function	B
values	O
f	O
(	O
x	O
)	O
by	O
additive	O
noise	O
,	O
and	O
we	O
will	O
further	O
assume	O
that	O
this	O
noise	O
follows	O
an	O
independent	O
,	O
identically	O
distributed	O
gaussian	O
distribution	O
with	O
zero	O
mean	O
and	O
variance	O
σ2	O
n	O
ε	O
∼	O
n	O
(	O
0	O
,	O
σ2	O
n	O
)	O
.	O
(	O
2.2	O
)	O
training	O
set	B
design	O
matrix	B
bias	O
,	O
oﬀset	O
likelihood	B
this	O
noise	O
assumption	O
together	O
with	O
the	O
model	B
directly	O
gives	O
rise	O
to	O
the	O
likeli-	O
hood	O
,	O
the	O
probability	B
density	O
of	O
the	O
observations	O
given	O
the	O
parameters	O
,	O
which	O
is	O
1in	O
statistics	O
texts	O
the	O
design	O
matrix	B
is	O
usually	O
taken	O
to	O
be	O
the	O
transpose	O
of	O
our	O
deﬁnition	O
,	O
but	O
our	O
choice	O
is	O
deliberate	O
and	O
has	O
the	O
advantage	O
that	O
a	O
data	O
point	O
is	O
a	O
standard	O
(	O
column	O
)	O
vector	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
2.1	O
weight-space	O
view	O
factored	O
over	O
cases	O
in	O
the	O
training	O
set	B
(	O
because	O
of	O
the	O
independence	O
assumption	O
)	O
to	O
give	O
ny	O
i=1	O
p	O
(	O
y|x	O
,	O
w	O
)	O
=	O
=	O
p	O
(	O
yi|xi	O
,	O
w	O
)	O
=	O
ny	O
exp	O
(	O
cid:0	O
)	O
−	O
1	O
i=1	O
1√	O
2πσn	O
exp	O
(	O
cid:0	O
)	O
−	O
(	O
yi	O
−	O
x	O
>	O
(	O
cid:1	O
)	O
|y	O
−	O
x	O
>	O
w|2	O
(	O
cid:1	O
)	O
=	O
n	O
(	O
x	O
>	O
w	O
,	O
σ2	O
i	O
w	O
)	O
2	O
2σ2	O
n	O
1	O
(	O
2πσ2	O
n	O
)	O
n/2	O
2σ2	O
n	O
(	O
2.3	O
)	O
ni	O
)	O
,	O
where	O
|z|	O
denotes	O
the	O
euclidean	O
length	O
of	O
vector	O
z.	O
in	O
the	O
bayesian	O
formalism	O
we	O
need	O
to	O
specify	O
a	O
prior	O
over	O
the	O
parameters	O
,	O
expressing	O
our	O
beliefs	O
about	O
the	O
parameters	O
before	O
we	O
look	O
at	O
the	O
observations	O
.	O
we	O
put	O
a	O
zero	O
mean	O
gaussian	O
prior	O
with	O
covariance	B
matrix	I
σp	O
on	O
the	O
weights	O
w	O
∼	O
n	O
(	O
0	O
,	O
σp	O
)	O
.	O
(	O
2.4	O
)	O
the	O
rˆole	O
and	O
properties	O
of	O
this	O
prior	O
will	O
be	O
discussed	O
in	O
section	O
2.2	O
;	O
for	O
now	O
we	O
will	O
continue	O
the	O
derivation	O
with	O
the	O
prior	O
as	O
speciﬁed	O
.	O
inference	O
in	O
the	O
bayesian	O
linear	B
model	O
is	O
based	O
on	O
the	O
posterior	O
distribution	O
over	O
the	O
weights	O
,	O
computed	O
by	O
bayes	O
’	O
rule	O
,	O
(	O
see	B
eq	O
.	O
(	O
a.3	O
)	O
)	O
2	O
posterior	O
=	O
likelihood	B
×	O
prior	O
marginal	B
likelihood	I
,	O
p	O
(	O
w|y	O
,	O
x	O
)	O
=	O
p	O
(	O
y|x	O
,	O
w	O
)	O
p	O
(	O
w	O
)	O
p	O
(	O
y|x	O
)	O
,	O
(	O
2.5	O
)	O
where	O
the	O
normalizing	O
constant	O
,	O
also	O
known	O
as	O
the	O
marginal	B
likelihood	I
(	O
see	B
page	O
19	O
)	O
,	O
is	O
independent	O
of	O
the	O
weights	O
and	O
given	O
by	O
p	O
(	O
y|x	O
)	O
=	O
p	O
(	O
y|x	O
,	O
w	O
)	O
p	O
(	O
w	O
)	O
dw	O
.	O
(	O
2.6	O
)	O
z	O
9	O
prior	O
posterior	O
marginal	O
likelihood	B
the	O
posterior	O
in	O
eq	O
.	O
(	O
2.5	O
)	O
combines	O
the	O
likelihood	B
and	O
the	O
prior	O
,	O
and	O
captures	O
everything	O
we	O
know	O
about	O
the	O
parameters	O
.	O
writing	O
only	O
the	O
terms	O
from	O
the	O
likelihood	B
and	O
prior	O
which	O
depend	O
on	O
the	O
weights	O
,	O
and	O
“	O
completing	O
the	O
square	O
”	O
we	O
obtain	O
p	O
(	O
w|x	O
,	O
y	O
)	O
∝	O
exp	O
(	O
cid:0	O
)	O
−	O
1	O
∝	O
exp	O
(	O
cid:0	O
)	O
−	O
1	O
(	O
y	O
−	O
x	O
>	O
w	O
)	O
>	O
(	O
y	O
−	O
x	O
>	O
w	O
)	O
(	O
cid:1	O
)	O
exp	O
(	O
cid:0	O
)	O
−	O
1	O
(	O
cid:1	O
)	O
(	O
w	O
−	O
¯w	O
)	O
(	O
cid:1	O
)	O
,	O
(	O
w	O
−	O
¯w	O
)	O
>	O
(	O
cid:0	O
)	O
1	O
xx	O
>	O
+	O
σ−1	O
p	O
w	O
(	O
cid:1	O
)	O
w	O
>	O
σ−1	O
(	O
2.7	O
)	O
2σ2	O
n	O
2	O
p	O
2	O
σ2	O
n	O
where	O
¯w	O
=	O
σ−2	O
posterior	O
distribution	O
as	O
gaussian	O
with	O
mean	O
¯w	O
and	O
covariance	B
matrix	I
a−1	O
p	O
)	O
−1xy	O
,	O
and	O
we	O
recognize	O
the	O
form	O
of	O
the	O
n	O
xx	O
>	O
+	O
σ−1	O
n	O
(	O
σ−2	O
p	O
(	O
w|x	O
,	O
y	O
)	O
∼	O
n	O
(	O
¯w	O
=	O
a−1xy	O
,	O
a−1	O
)	O
,	O
1	O
σ2	O
n	O
(	O
2.8	O
)	O
n	O
xx	O
>	O
+	O
σ−1	O
where	O
a	O
=	O
σ−2	O
p	O
.	O
notice	O
that	O
for	O
this	O
model	B
(	O
and	O
indeed	O
for	O
any	O
gaussian	O
posterior	O
)	O
the	O
mean	O
of	O
the	O
posterior	O
distribution	O
p	O
(	O
w|y	O
,	O
x	O
)	O
is	O
also	O
its	O
mode	O
,	O
which	O
is	O
also	O
called	O
the	O
maximum	B
a	I
posteriori	I
(	O
map	O
)	O
estimate	O
of	O
2often	O
bayes	O
’	O
rule	O
is	O
stated	O
as	O
p	O
(	O
a|b	O
)	O
=	O
p	O
(	O
b|a	O
)	O
p	O
(	O
a	O
)	O
/p	O
(	O
b	O
)	O
;	O
here	O
we	O
use	O
it	O
in	O
a	O
form	O
where	O
we	O
additionally	O
condition	O
everywhere	O
on	O
the	O
inputs	O
x	O
(	O
but	O
neglect	O
this	O
extra	O
conditioning	O
for	O
the	O
prior	O
which	O
is	O
independent	O
of	O
the	O
inputs	O
)	O
.	O
map	O
estimate	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
10	O
regression	B
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
2.1	O
:	O
example	O
of	O
bayesian	O
linear	B
model	O
f	O
(	O
x	O
)	O
=	O
w1	O
+	O
w2x	O
with	O
intercept	O
w1	O
and	O
slope	O
parameter	O
w2	O
.	O
panel	O
(	O
a	O
)	O
shows	O
the	O
contours	O
of	O
the	O
prior	O
distribution	O
p	O
(	O
w	O
)	O
∼	O
n	O
(	O
0	O
,	O
i	O
)	O
,	O
eq	O
.	O
(	O
2.4	O
)	O
.	O
panel	O
(	O
b	O
)	O
shows	O
three	O
training	O
points	O
marked	O
by	O
crosses	O
.	O
panel	O
(	O
c	O
)	O
shows	O
contours	O
of	O
the	O
likelihood	B
p	O
(	O
y|x	O
,	O
w	O
)	O
eq	O
.	O
(	O
2.3	O
)	O
,	O
assuming	O
a	O
noise	O
level	O
of	O
σn	O
=	O
1	O
;	O
note	O
that	O
the	O
slope	O
is	O
much	O
more	O
“	O
well	O
determined	O
”	O
than	O
the	O
intercept	O
.	O
panel	O
(	O
d	O
)	O
shows	O
the	O
posterior	O
,	O
p	O
(	O
w|x	O
,	O
y	O
)	O
eq	O
.	O
(	O
2.7	O
)	O
;	O
comparing	O
the	O
maximum	O
of	O
the	O
posterior	O
to	O
the	O
likelihood	B
,	O
we	O
see	B
that	O
the	O
intercept	O
has	O
been	O
shrunk	O
towards	O
zero	O
whereas	O
the	O
more	O
’	O
well	O
determined	O
’	O
slope	O
is	O
almost	O
unchanged	O
.	O
all	O
contour	O
plots	O
give	O
the	O
1	O
and	O
2	O
standard	O
deviation	O
equi-probability	O
contours	O
.	O
superimposed	O
on	O
the	O
data	O
in	O
panel	O
(	O
b	O
)	O
are	O
the	O
predictive	B
mean	O
plus/minus	O
two	O
standard	O
deviations	O
of	O
the	O
(	O
noise-free	O
)	O
predictive	B
distribution	O
p	O
(	O
f∗|x∗	O
,	O
x	O
,	O
y	O
)	O
,	O
eq	O
.	O
(	O
2.9	O
)	O
.	O
w.	O
in	O
a	O
non-bayesian	O
setting	O
the	O
negative	O
log	O
prior	O
is	O
sometimes	O
thought	O
of	O
as	O
a	O
penalty	O
term	O
,	O
and	O
the	O
map	O
point	O
is	O
known	O
as	O
the	O
penalized	B
maximum	I
likelihood	I
estimate	I
of	O
the	O
weights	O
,	O
and	O
this	O
may	O
cause	O
some	O
confusion	O
between	O
the	O
two	O
approaches	O
.	O
note	O
,	O
however	O
,	O
that	O
in	O
the	O
bayesian	O
setting	O
the	O
map	O
estimate	O
plays	O
no	O
special	O
rˆole.3	O
the	O
penalized	O
maximum	O
likelihood	O
procedure	O
3in	O
this	O
case	O
,	O
due	O
to	O
symmetries	O
in	O
the	O
model	B
and	O
posterior	O
,	O
it	O
happens	O
that	O
the	O
mean	O
of	O
the	O
predictive	B
distribution	O
is	O
the	O
same	O
as	O
the	O
prediction	B
at	O
the	O
mean	O
of	O
the	O
posterior	O
.	O
however	O
,	O
this	O
is	O
not	O
the	O
case	O
in	O
general	O
.	O
intercept	O
,	O
w1slope	O
,	O
w2−2−1012−2−1012−505−505input	O
,	O
xoutput	O
,	O
yintercept	O
,	O
w1slope	O
,	O
w2−2−1012−2−1012intercept	O
,	O
w1slope	O
,	O
w2−2−1012−2−1012	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
11	O
ridge	B
regression	I
predictive	O
distribution	O
feature	B
space	I
polynomial	O
regression	B
linear	O
in	O
the	O
parameters	O
2.1	O
weight-space	O
view	O
is	O
known	O
in	O
this	O
case	O
as	O
ridge	B
regression	I
[	O
hoerl	O
and	O
kennard	O
,	O
1970	O
]	O
because	O
of	O
the	O
eﬀect	O
of	O
the	O
quadratic	O
penalty	O
term	O
1	O
p	O
w	O
from	O
the	O
log	O
prior	O
.	O
2	O
w	O
>	O
σ−1	O
to	O
make	O
predictions	O
for	O
a	O
test	O
case	O
we	O
average	O
over	O
all	O
possible	O
parameter	O
values	O
,	O
weighted	O
by	O
their	O
posterior	O
probability	O
.	O
this	O
is	O
in	O
contrast	O
to	O
non-	O
bayesian	O
schemes	O
,	O
where	O
a	O
single	O
parameter	O
is	O
typically	O
chosen	O
by	O
some	O
crite-	O
rion	O
.	O
thus	O
the	O
predictive	B
distribution	O
for	O
f∗	O
,	O
f	O
(	O
x∗	O
)	O
at	O
x∗	O
is	O
given	O
by	O
averaging	O
the	O
output	O
of	O
all	O
possible	O
linear	B
models	O
w.r.t	O
.	O
the	O
gaussian	O
posterior	O
p	O
(	O
f∗|x∗	O
,	O
x	O
,	O
y	O
)	O
=	O
p	O
(	O
f∗|x∗	O
,	O
w	O
)	O
p	O
(	O
w|x	O
,	O
y	O
)	O
dw	O
(	O
2.9	O
)	O
z	O
=	O
n	O
(	O
cid:0	O
)	O
1	O
σ2	O
n	O
∗	O
a−1x∗	O
(	O
cid:1	O
)	O
.	O
∗	O
a−1xy	O
,	O
x	O
>	O
x	O
>	O
the	O
predictive	B
distribution	O
is	O
again	O
gaussian	O
,	O
with	O
a	O
mean	O
given	O
by	O
the	O
poste-	O
rior	O
mean	O
of	O
the	O
weights	O
from	O
eq	O
.	O
(	O
2.8	O
)	O
multiplied	O
by	O
the	O
test	O
input	O
,	O
as	O
one	O
would	O
expect	O
from	O
symmetry	O
considerations	O
.	O
the	O
predictive	B
variance	O
is	O
a	O
quadratic	B
form	I
of	O
the	O
test	O
input	O
with	O
the	O
posterior	O
covariance	O
matrix	B
,	O
showing	O
that	O
the	O
predictive	B
uncertainties	O
grow	O
with	O
the	O
magnitude	O
of	O
the	O
test	O
input	O
,	O
as	O
one	O
would	O
expect	O
for	O
a	O
linear	B
model	O
.	O
an	O
example	O
of	O
bayesian	O
linear	B
regression	I
is	O
given	O
in	O
figure	O
2.1.	O
here	O
we	O
have	O
chosen	O
a	O
1-d	O
input	O
space	O
so	O
that	O
the	O
weight-space	O
is	O
two-dimensional	O
and	O
can	O
be	O
easily	O
visualized	O
.	O
contours	O
of	O
the	O
gaussian	O
prior	O
are	O
shown	O
in	O
panel	O
(	O
a	O
)	O
.	O
the	O
data	O
are	O
depicted	O
as	O
crosses	O
in	O
panel	O
(	O
b	O
)	O
.	O
this	O
gives	O
rise	O
to	O
the	O
likelihood	B
shown	O
in	O
panel	O
(	O
c	O
)	O
and	O
the	O
posterior	O
distribution	O
in	O
panel	O
(	O
d	O
)	O
.	O
the	O
predictive	B
distribution	O
and	O
its	O
error	B
bars	O
are	O
also	O
marked	O
in	O
panel	O
(	O
b	O
)	O
.	O
2.1.2	O
projections	O
of	O
inputs	O
into	O
feature	B
space	I
in	O
the	O
previous	O
section	O
we	O
reviewed	O
the	O
bayesian	O
linear	B
model	O
which	O
suﬀers	O
from	O
limited	O
expressiveness	O
.	O
a	O
very	O
simple	O
idea	O
to	O
overcome	O
this	O
problem	O
is	O
to	O
ﬁrst	O
project	O
the	O
inputs	O
into	O
some	O
high	O
dimensional	O
space	O
using	O
a	O
set	B
of	O
basis	O
functions	O
and	O
then	O
apply	O
the	O
linear	B
model	O
in	O
this	O
space	O
instead	O
of	O
directly	O
on	O
the	O
inputs	O
themselves	O
.	O
for	O
example	O
,	O
a	O
scalar	O
input	O
x	O
could	O
be	O
projected	O
into	O
the	O
space	O
of	O
powers	O
of	O
x	O
:	O
φ	O
(	O
x	O
)	O
=	O
(	O
1	O
,	O
x	O
,	O
x2	O
,	O
x3	O
,	O
.	O
.	O
.	O
)	O
>	O
to	O
implement	O
polynomial	B
regression	O
.	O
as	O
long	O
as	O
the	O
projections	O
are	O
ﬁxed	O
functions	O
(	O
i.e	O
.	O
independent	O
of	O
the	O
parameters	O
w	O
)	O
the	O
model	B
is	O
still	O
linear	B
in	O
the	O
parameters	O
,	O
and	O
therefore	O
analytically	O
tractable.4	O
this	O
idea	O
is	O
also	O
used	O
in	O
classiﬁcation	B
,	O
where	O
a	O
dataset	B
which	O
is	O
not	O
linearly	O
separable	O
in	O
the	O
original	O
data	O
space	O
may	O
become	O
linearly	O
separable	O
in	O
a	O
high	O
dimensional	O
feature	B
space	I
,	O
see	B
section	O
3.3.	O
application	O
of	O
this	O
idea	O
begs	O
the	O
question	O
of	O
how	O
to	O
choose	O
the	O
basis	O
functions	O
?	O
as	O
we	O
shall	O
demonstrate	O
(	O
in	O
chapter	O
5	O
)	O
,	O
the	O
gaussian	O
process	B
formalism	O
allows	O
us	O
to	O
answer	O
this	O
question	O
.	O
for	O
now	O
,	O
we	O
assume	O
that	O
the	O
basis	O
functions	O
are	O
given	O
.	O
speciﬁcally	O
,	O
we	O
introduce	O
the	O
function	B
φ	O
(	O
x	O
)	O
which	O
maps	O
a	O
d-dimensional	O
input	O
vector	O
x	O
into	O
an	O
n	O
dimensional	O
feature	B
space	I
.	O
further	O
let	O
the	O
matrix	B
4models	O
with	O
adaptive	O
basis	O
functions	O
,	O
such	O
as	O
e.g	O
.	O
multilayer	O
perceptrons	O
,	O
may	O
at	O
ﬁrst	O
seem	O
like	O
a	O
useful	O
extension	O
,	O
but	O
they	O
are	O
much	O
harder	O
to	O
treat	O
,	O
except	O
in	O
the	O
limit	O
of	O
an	O
inﬁnite	O
number	O
of	O
hidden	O
units	O
,	O
see	B
section	O
4.2.3.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
12	O
regression	B
explicit	O
feature	B
space	I
formulation	O
alternative	O
formulation	O
computational	O
load	O
kernel	B
kernel	O
trick	O
φ	O
(	O
x	O
)	O
be	O
the	O
aggregation	O
of	O
columns	O
φ	O
(	O
x	O
)	O
for	O
all	O
cases	O
in	O
the	O
training	O
set	B
.	O
now	O
the	O
model	B
is	O
(	O
2.10	O
)	O
where	O
the	O
vector	O
of	O
parameters	O
now	O
has	O
length	O
n.	O
the	O
analysis	O
for	O
this	O
model	B
is	O
analogous	O
to	O
the	O
standard	O
linear	B
model	O
,	O
except	O
that	O
everywhere	O
φ	O
(	O
x	O
)	O
is	O
substituted	O
for	O
x.	O
thus	O
the	O
predictive	B
distribution	O
becomes	O
f	O
(	O
x	O
)	O
=	O
φ	O
(	O
x	O
)	O
>	O
w	O
,	O
f∗|x∗	O
,	O
x	O
,	O
y	O
∼	O
n	O
(	O
cid:0	O
)	O
1	O
φ	O
(	O
x∗	O
)	O
>	O
a−1φy	O
,	O
φ	O
(	O
x∗	O
)	O
>	O
a−1φ	O
(	O
x∗	O
)	O
(	O
cid:1	O
)	O
(	O
2.11	O
)	O
σ2	O
n	O
with	O
φ	O
=	O
φ	O
(	O
x	O
)	O
and	O
a	O
=	O
σ−2	O
p	O
.	O
to	O
make	O
predictions	O
using	O
this	O
equation	O
we	O
need	O
to	O
invert	O
the	O
a	O
matrix	B
of	O
size	O
n	O
×	O
n	O
which	O
may	O
not	O
be	O
convenient	O
if	O
n	O
,	O
the	O
dimension	O
of	O
the	O
feature	B
space	I
,	O
is	O
large	O
.	O
however	O
,	O
we	O
can	O
rewrite	O
the	O
equation	O
in	O
the	O
following	O
way	O
n	O
φφ	O
>	O
+	O
σ−1	O
f∗|x∗	O
,	O
x	O
,	O
y	O
∼	O
n	O
(	O
cid:0	O
)	O
φ	O
>	O
ni	O
)	O
−1y	O
,	O
∗	O
σpφ	O
(	O
k	O
+	O
σ2	O
∗	O
σpφ∗	O
−	O
φ	O
>	O
>	O
∗	O
σpφ	O
(	O
k	O
+	O
σ2	O
φ	O
ni	O
)	O
−1φ	O
>	O
σpφ∗	O
(	O
cid:1	O
)	O
,	O
(	O
2.12	O
)	O
ni	O
)	O
=	O
σ−2	O
n	O
φ	O
(	O
φ	O
>	O
σpφ	O
+	O
σ2	O
where	O
we	O
have	O
used	O
the	O
shorthand	O
φ	O
(	O
x∗	O
)	O
=	O
φ∗	O
and	O
deﬁned	O
k	O
=	O
φ	O
>	O
σpφ	O
.	O
to	O
show	O
this	O
for	O
the	O
mean	O
,	O
ﬁrst	O
note	O
that	O
using	O
the	O
deﬁnitions	O
of	O
a	O
and	O
k	O
we	O
have	O
σ−2	O
n	O
φ	O
(	O
k	O
+	O
σ2	O
ni	O
)	O
=	O
aσpφ	O
.	O
now	O
multiplying	O
through	O
by	O
a−1	O
from	O
left	O
and	O
(	O
k	O
+	O
σ2	O
n	O
a−1φ	O
=	O
ni	O
)	O
−1	O
,	O
showing	O
the	O
equivalence	O
of	O
the	O
mean	O
expressions	O
in	O
eq	O
.	O
(	O
2.11	O
)	O
σpφ	O
(	O
k	O
+	O
σ2	O
and	O
eq	O
.	O
(	O
2.12	O
)	O
.	O
for	O
the	O
variance	O
we	O
use	O
the	O
matrix	B
inversion	O
lemma	O
,	O
eq	O
.	O
(	O
a.9	O
)	O
,	O
setting	O
z−1	O
=	O
σp	O
,	O
w	O
−1	O
=	O
σ2	O
in	O
eq	O
.	O
(	O
2.12	O
)	O
we	O
need	O
to	O
invert	O
matrices	O
of	O
size	O
n	O
×	O
n	O
which	O
is	O
more	O
convenient	O
when	O
n	O
<	O
n.	O
geometrically	O
,	O
note	O
that	O
n	O
datapoints	O
can	O
span	O
at	O
most	O
n	O
dimensions	O
in	O
the	O
feature	B
space	I
.	O
ni	O
)	O
−1	O
from	O
the	O
right	O
gives	O
σ−2	O
ni	O
and	O
v	O
=	O
u	O
=	O
φ	O
therein	O
.	O
>	O
∗	O
σpφ	O
,	O
or	O
φ	O
notice	O
that	O
in	O
eq	O
.	O
(	O
2.12	O
)	O
the	O
feature	B
space	I
always	O
enters	O
in	O
the	O
form	O
of	O
>	O
φ	O
>	O
σpφ	O
,	O
φ	O
∗	O
σpφ∗	O
;	O
thus	O
the	O
entries	O
of	O
these	O
matrices	O
are	O
invariably	O
of	O
the	O
form	O
φ	O
(	O
x	O
)	O
>	O
σpφ	O
(	O
x0	O
)	O
where	O
x	O
and	O
x0	O
are	O
in	O
either	O
the	O
training	O
or	O
the	O
test	O
sets	O
.	O
let	O
us	O
deﬁne	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
φ	O
(	O
x	O
)	O
>	O
σpφ	O
(	O
x0	O
)	O
.	O
for	O
reasons	O
that	O
will	O
become	O
clear	O
later	O
we	O
call	O
k	O
(	O
·	O
,	O
·	O
)	O
a	O
covariance	B
function	I
or	O
kernel	B
.	O
notice	O
that	O
φ	O
(	O
x	O
)	O
>	O
σpφ	O
(	O
x0	O
)	O
is	O
an	O
inner	O
product	O
(	O
with	O
respect	O
to	O
σp	O
)	O
.	O
as	O
σp	O
is	O
positive	B
deﬁnite	I
we	O
can	O
deﬁne	O
σ1/2	O
so	O
that	O
(	O
σ1/2	O
)	O
2	O
=	O
σp	O
;	O
for	O
example	O
if	O
the	O
svd	O
(	O
singular	O
value	O
decomposition	O
)	O
of	O
σp	O
=	O
u	O
du	O
>	O
,	O
where	O
d	O
is	O
diagonal	O
,	O
then	O
one	O
form	O
for	O
σ1/2	O
is	O
u	O
d1/2u	O
>	O
.	O
then	O
deﬁning	O
ψ	O
(	O
x	O
)	O
=	O
σ1/2	O
p	O
φ	O
(	O
x	O
)	O
we	O
obtain	O
a	O
simple	O
dot	B
product	I
representation	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
ψ	O
(	O
x	O
)	O
·	O
ψ	O
(	O
x0	O
)	O
.	O
p	O
p	O
p	O
if	O
an	O
algorithm	O
is	O
deﬁned	O
solely	O
in	O
terms	O
of	O
inner	O
products	O
in	O
input	O
space	O
then	O
it	O
can	O
be	O
lifted	O
into	O
feature	B
space	I
by	O
replacing	O
occurrences	O
of	O
those	O
inner	O
products	O
by	O
k	O
(	O
x	O
,	O
x0	O
)	O
;	O
this	O
is	O
sometimes	O
called	O
the	O
kernel	B
trick	I
.	O
this	O
technique	O
is	O
particularly	O
valuable	O
in	O
situations	O
where	O
it	O
is	O
more	O
convenient	O
to	O
compute	O
the	O
kernel	B
than	O
the	O
feature	O
vectors	O
themselves	O
.	O
as	O
we	O
will	O
see	B
in	O
the	O
coming	O
sections	O
,	O
this	O
often	O
leads	O
to	O
considering	O
the	O
kernel	B
as	O
the	O
object	O
of	O
primary	O
interest	O
,	O
and	O
its	O
corresponding	O
feature	B
space	I
as	O
having	O
secondary	O
practical	O
importance	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
2.2	O
function-space	O
view	O
13	O
2.2	O
function-space	O
view	O
an	O
alternative	O
and	O
equivalent	B
way	O
of	O
reaching	O
identical	O
results	O
to	O
the	O
previous	O
section	O
is	O
possible	O
by	O
considering	O
inference	O
directly	O
in	O
function	B
space	O
.	O
we	O
use	O
a	O
gaussian	O
process	B
(	O
gp	O
)	O
to	O
describe	O
a	O
distribution	O
over	O
functions	O
.	O
formally	O
:	O
deﬁnition	O
2.1	O
a	O
gaussian	O
process	B
is	O
a	O
collection	O
of	O
random	O
variables	O
,	O
any	O
(	O
cid:3	O
)	O
ﬁnite	O
number	O
of	O
which	O
have	O
a	O
joint	B
gaussian	O
distribution	O
.	O
gaussian	O
process	B
a	O
gaussian	O
process	B
is	O
completely	O
speciﬁed	O
by	O
its	O
mean	B
function	I
and	O
co-	O
variance	O
function	B
.	O
we	O
deﬁne	O
mean	B
function	I
m	O
(	O
x	O
)	O
and	O
the	O
covariance	B
function	I
k	O
(	O
x	O
,	O
x0	O
)	O
of	O
a	O
real	O
process	B
f	O
(	O
x	O
)	O
as	O
covariance	B
and	O
mean	B
function	I
m	O
(	O
x	O
)	O
=	O
e	O
[	O
f	O
(	O
x	O
)	O
]	O
,	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
e	O
[	O
(	O
f	O
(	O
x	O
)	O
−	O
m	O
(	O
x	O
)	O
)	O
(	O
f	O
(	O
x0	O
)	O
−	O
m	O
(	O
x0	O
)	O
)	O
]	O
,	O
and	O
will	O
write	O
the	O
gaussian	O
process	B
as	O
f	O
(	O
x	O
)	O
∼	O
gp	O
(	O
cid:0	O
)	O
m	O
(	O
x	O
)	O
,	O
k	O
(	O
x	O
,	O
x0	O
)	O
(	O
cid:1	O
)	O
.	O
(	O
2.13	O
)	O
(	O
2.14	O
)	O
usually	O
,	O
for	O
notational	O
simplicity	O
we	O
will	O
take	O
the	O
mean	B
function	I
to	O
be	O
zero	O
,	O
although	O
this	O
need	O
not	O
be	O
done	O
,	O
see	B
section	O
2.7.	O
in	O
our	O
case	O
the	O
random	O
variables	O
represent	O
the	O
value	O
of	O
the	O
function	B
f	O
(	O
x	O
)	O
at	O
location	O
x.	O
often	O
,	O
gaussian	O
processes	O
are	O
deﬁned	O
over	O
time	O
,	O
i.e	O
.	O
where	O
the	O
index	O
set	B
of	O
the	O
random	O
variables	O
is	O
time	O
.	O
this	O
is	O
not	O
(	O
normally	O
)	O
the	O
case	O
in	O
our	O
use	O
of	O
gps	O
;	O
here	O
the	O
index	O
set	B
x	O
is	O
the	O
set	B
of	O
possible	O
inputs	O
,	O
which	O
could	O
be	O
more	O
general	O
,	O
e.g	O
.	O
rd	O
.	O
for	O
notational	O
convenience	O
we	O
use	O
the	O
(	O
arbitrary	O
)	O
enumeration	O
of	O
the	O
cases	O
in	O
the	O
training	O
set	B
to	O
identify	O
the	O
random	O
variables	O
such	O
that	O
fi	O
,	O
f	O
(	O
xi	O
)	O
is	O
the	O
random	O
variable	O
corresponding	O
to	O
the	O
case	O
(	O
xi	O
,	O
yi	O
)	O
as	O
would	O
be	O
expected	O
.	O
a	O
gaussian	O
process	B
is	O
deﬁned	O
as	O
a	O
collection	O
of	O
random	O
variables	O
.	O
thus	O
,	O
the	O
deﬁnition	O
automatically	O
implies	O
a	O
consistency	B
requirement	O
,	O
which	O
is	O
also	O
some-	O
times	O
known	O
as	O
the	O
marginalization	B
property	I
.	O
this	O
property	O
simply	O
means	O
that	O
if	O
the	O
gp	O
e.g	O
.	O
speciﬁes	O
(	O
y1	O
,	O
y2	O
)	O
∼	O
n	O
(	O
µ	O
,	O
σ	O
)	O
,	O
then	O
it	O
must	O
also	O
specify	O
y1	O
∼	O
n	O
(	O
µ1	O
,	O
σ11	O
)	O
where	O
σ11	O
is	O
the	O
relevant	O
submatrix	O
of	O
σ	O
,	O
see	B
eq	O
.	O
(	O
a.6	O
)	O
.	O
in	O
other	O
words	O
,	O
examination	O
of	O
a	O
larger	O
set	B
of	O
variables	O
does	O
not	O
change	O
the	O
distribution	O
of	O
the	O
smaller	O
set	B
.	O
notice	O
that	O
the	O
consistency	B
requirement	O
is	O
au-	O
tomatically	O
fulﬁlled	O
if	O
the	O
covariance	B
function	I
speciﬁes	O
entries	O
of	O
the	O
covariance	B
matrix.5	O
the	O
deﬁnition	O
does	O
not	O
exclude	O
gaussian	O
processes	O
with	O
ﬁnite	O
index	O
sets	O
(	O
which	O
would	O
be	O
simply	O
gaussian	O
distributions	O
)	O
,	O
but	O
these	O
are	O
not	O
partic-	O
ularly	O
interesting	O
for	O
our	O
purposes	O
.	O
5note	O
,	O
however	O
,	O
that	O
if	O
you	O
instead	O
speciﬁed	O
e.g	O
.	O
a	O
function	B
for	O
the	O
entries	O
of	O
the	O
inverse	O
covariance	B
matrix	I
,	O
then	O
the	O
marginalization	B
property	I
would	O
no	O
longer	O
be	O
fulﬁlled	O
,	O
and	O
one	O
could	O
not	O
think	O
of	O
this	O
as	O
a	O
consistent	O
collection	O
of	O
random	O
variables—this	O
would	O
not	O
qualify	O
as	O
a	O
gaussian	O
process	B
.	O
index	O
set	B
≡	O
input	O
domain	O
marginalization	B
property	I
ﬁnite	O
index	O
set	B
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
14	O
bayesian	O
linear	B
model	O
is	O
a	O
gaussian	O
process	B
basis	O
functions	O
smoothness	O
characteristic	O
length-scale	B
regression	O
a	O
simple	O
example	O
of	O
a	O
gaussian	O
process	B
can	O
be	O
obtained	O
from	O
our	O
bayesian	O
linear	B
regression	I
model	O
f	O
(	O
x	O
)	O
=	O
φ	O
(	O
x	O
)	O
>	O
w	O
with	O
prior	O
w	O
∼	O
n	O
(	O
0	O
,	O
σp	O
)	O
.	O
we	O
have	O
for	O
the	O
mean	O
and	O
covariance	B
e	O
[	O
f	O
(	O
x	O
)	O
]	O
=	O
φ	O
(	O
x	O
)	O
>	O
e	O
[	O
w	O
]	O
=	O
0	O
,	O
e	O
[	O
f	O
(	O
x	O
)	O
f	O
(	O
x0	O
)	O
]	O
=	O
φ	O
(	O
x	O
)	O
>	O
e	O
[	O
ww	O
>	O
]	O
φ	O
(	O
x0	O
)	O
=	O
φ	O
(	O
x	O
)	O
>	O
σpφ	O
(	O
x0	O
)	O
.	O
(	O
2.15	O
)	O
thus	O
f	O
(	O
x	O
)	O
and	O
f	O
(	O
x0	O
)	O
are	O
jointly	O
gaussian	O
with	O
zero	O
mean	O
and	O
covariance	B
given	O
by	O
φ	O
(	O
x	O
)	O
>	O
σpφ	O
(	O
x0	O
)	O
.	O
indeed	O
,	O
the	O
function	B
values	O
f	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
f	O
(	O
xn	O
)	O
corresponding	O
to	O
any	O
number	O
of	O
input	O
points	O
n	O
are	O
jointly	O
gaussian	O
,	O
although	O
if	O
n	O
<	O
n	O
then	O
this	O
gaussian	O
is	O
singular	O
(	O
as	O
the	O
joint	B
covariance	O
matrix	B
will	O
be	O
of	O
rank	O
n	O
)	O
.	O
in	O
this	O
chapter	O
our	O
running	O
example	O
of	O
a	O
covariance	B
function	I
will	O
be	O
the	O
squared	B
exponential	I
6	O
(	O
se	O
)	O
covariance	B
function	I
;	O
other	O
covariance	B
functions	O
are	O
discussed	O
in	O
chapter	O
4.	O
the	O
covariance	B
function	I
speciﬁes	O
the	O
covariance	B
between	O
pairs	O
of	O
random	O
variables	O
cov	O
(	O
cid:0	O
)	O
f	O
(	O
xp	O
)	O
,	O
f	O
(	O
xq	O
)	O
(	O
cid:1	O
)	O
=	O
k	O
(	O
xp	O
,	O
xq	O
)	O
=	O
exp	O
(	O
cid:0	O
)	O
−	O
1	O
2|xp	O
−	O
xq|2	O
(	O
cid:1	O
)	O
.	O
(	O
2.16	O
)	O
note	O
,	O
that	O
the	O
covariance	B
between	O
the	O
outputs	B
is	O
written	O
as	O
a	O
function	B
of	O
the	O
inputs	O
.	O
for	O
this	O
particular	O
covariance	B
function	I
,	O
we	O
see	B
that	O
the	O
covariance	B
is	O
almost	O
unity	O
between	O
variables	O
whose	O
corresponding	O
inputs	O
are	O
very	O
close	O
,	O
and	O
decreases	O
as	O
their	O
distance	O
in	O
the	O
input	O
space	O
increases	O
.	O
it	O
can	O
be	O
shown	O
(	O
see	B
section	O
4.3.1	O
)	O
that	O
the	O
squared	B
exponential	I
covariance	O
function	B
corresponds	O
to	O
a	O
bayesian	O
linear	B
regression	I
model	O
with	O
an	O
inﬁnite	O
number	O
of	O
basis	O
functions	O
.	O
indeed	O
for	O
every	O
positive	B
deﬁnite	I
covariance	O
function	B
k	O
(	O
·	O
,	O
·	O
)	O
,	O
there	O
exists	O
a	O
(	O
possibly	O
inﬁnite	O
)	O
expansion	O
in	O
terms	O
of	O
basis	O
functions	O
(	O
see	B
mercer	O
’	O
s	O
theorem	O
in	O
section	O
4.3	O
)	O
.	O
we	O
can	O
also	O
obtain	O
the	O
se	O
covariance	B
function	I
from	O
the	O
linear	B
combination	O
of	O
an	O
inﬁnite	O
number	O
of	O
gaussian-shaped	O
basis	O
functions	O
,	O
see	B
eq	O
.	O
(	O
4.13	O
)	O
and	O
eq	O
.	O
(	O
4.30	O
)	O
.	O
the	O
speciﬁcation	O
of	O
the	O
covariance	B
function	I
implies	O
a	O
distribution	O
over	O
func-	O
tions	O
.	O
to	O
see	B
this	O
,	O
we	O
can	O
draw	O
samples	O
from	O
the	O
distribution	O
of	O
functions	O
evalu-	O
ated	O
at	O
any	O
number	O
of	O
points	O
;	O
in	O
detail	O
,	O
we	O
choose	O
a	O
number	O
of	O
input	O
points,7	O
x∗	O
and	O
write	O
out	O
the	O
corresponding	O
covariance	B
matrix	I
using	O
eq	O
.	O
(	O
2.16	O
)	O
elementwise	O
.	O
then	O
we	O
generate	O
a	O
random	O
gaussian	O
vector	O
with	O
this	O
covariance	B
matrix	I
f∗	O
∼	O
n	O
(	O
cid:0	O
)	O
0	O
,	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
(	O
cid:1	O
)	O
,	O
(	O
2.17	O
)	O
and	O
plot	O
the	O
generated	O
values	O
as	O
a	O
function	B
of	O
the	O
inputs	O
.	O
figure	O
2.2	O
(	O
a	O
)	O
shows	O
three	O
such	O
samples	O
.	O
the	O
generation	O
of	O
multivariate	O
gaussian	O
samples	O
is	O
de-	O
scribed	O
in	O
section	O
a.2	O
.	O
in	O
the	O
example	O
in	O
figure	O
2.2	O
the	O
input	O
values	O
were	O
equidistant	O
,	O
but	O
this	O
need	O
not	O
be	O
the	O
case	O
.	O
notice	O
that	O
“	O
informally	O
”	O
the	O
functions	O
look	O
smooth	O
.	O
in	O
fact	O
the	O
squared	B
exponential	I
covariance	O
function	B
is	O
inﬁnitely	O
diﬀerentiable	O
,	O
leading	O
to	O
the	O
process	B
being	O
inﬁnitely	O
mean-square	O
diﬀerentiable	O
(	O
see	B
section	O
4.1	O
)	O
.	O
we	O
also	O
see	B
that	O
the	O
functions	O
seem	O
to	O
have	O
a	O
characteristic	O
length-scale	B
,	O
6sometimes	O
this	O
covariance	B
function	I
is	O
called	O
the	O
radial	B
basis	I
function	I
(	O
rbf	O
)	O
or	O
gaussian	O
;	O
here	O
we	O
prefer	O
squared	B
exponential	I
.	O
7technically	O
,	O
these	O
input	O
points	O
play	O
the	O
rˆole	O
of	O
test	O
inputs	O
and	O
therefore	O
carry	O
a	O
subscript	O
asterisk	O
;	O
this	O
will	O
become	O
clearer	O
later	O
when	O
both	O
training	O
and	O
test	O
points	O
are	O
involved	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
2.2	O
function-space	O
view	O
15	O
(	O
a	O
)	O
,	O
prior	O
(	O
b	O
)	O
,	O
posterior	O
figure	O
2.2	O
:	O
panel	O
(	O
a	O
)	O
shows	O
three	O
functions	O
drawn	O
at	O
random	O
from	O
a	O
gp	O
prior	O
;	O
the	O
dots	O
indicate	O
values	O
of	O
y	O
actually	O
generated	O
;	O
the	O
two	O
other	O
functions	O
have	O
(	O
less	O
correctly	O
)	O
been	O
drawn	O
as	O
lines	O
by	O
joining	O
a	O
large	O
number	O
of	O
evaluated	O
points	O
.	O
panel	O
(	O
b	O
)	O
shows	O
three	O
random	O
functions	O
drawn	O
from	O
the	O
posterior	O
,	O
i.e	O
.	O
the	O
prior	O
conditioned	O
on	O
the	O
ﬁve	O
noise	O
free	O
observations	O
indicated	O
.	O
in	O
both	O
plots	O
the	O
shaded	O
area	O
represents	O
the	O
pointwise	O
mean	O
plus	O
and	O
minus	O
two	O
times	O
the	O
standard	O
deviation	O
for	O
each	O
input	O
value	O
(	O
corresponding	O
to	O
the	O
95	O
%	O
conﬁdence	O
region	O
)	O
,	O
for	O
the	O
prior	O
and	O
posterior	O
respectively	O
.	O
which	O
informally	O
can	O
be	O
thought	O
of	O
as	O
roughly	O
the	O
distance	O
you	O
have	O
to	O
move	O
in	O
input	O
space	O
before	O
the	O
function	B
value	O
can	O
change	O
signiﬁcantly	O
,	O
see	B
section	O
4.2.1.	O
for	O
eq	O
.	O
(	O
2.16	O
)	O
the	O
characteristic	O
length-scale	B
is	O
around	O
one	O
unit	O
.	O
by	O
replacing	O
|xp−xq|	O
by	O
|xp−xq|/	O
‘	O
in	O
eq	O
.	O
(	O
2.16	O
)	O
for	O
some	O
positive	O
constant	O
‘	O
we	O
could	O
change	O
the	O
characteristic	O
length-scale	B
of	O
the	O
process	B
.	O
also	O
,	O
the	O
overall	O
variance	O
of	O
the	O
random	O
function	B
can	O
be	O
controlled	O
by	O
a	O
positive	O
pre-factor	O
before	O
the	O
exp	O
in	O
eq	O
.	O
(	O
2.16	O
)	O
.	O
we	O
will	O
discuss	O
more	O
about	O
how	O
such	O
factors	O
aﬀect	O
the	O
predictions	O
in	O
section	O
2.3	O
,	O
and	O
say	O
more	O
about	O
how	O
to	O
set	B
such	O
scale	O
parameters	O
in	O
chapter	O
5.	O
prediction	B
with	O
noise-free	O
observations	O
we	O
are	O
usually	O
not	O
primarily	O
interested	O
in	O
drawing	O
random	O
functions	O
from	O
the	O
prior	O
,	O
but	O
want	O
to	O
incorporate	O
the	O
knowledge	O
that	O
the	O
training	O
data	O
provides	O
about	O
the	O
function	B
.	O
initially	O
,	O
we	O
will	O
consider	O
the	O
simple	O
special	O
case	O
where	O
the	O
observations	O
are	O
noise	O
free	O
,	O
that	O
is	O
we	O
know	O
{	O
(	O
xi	O
,	O
fi	O
)	O
|i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
.	O
the	O
joint	B
distribution	O
of	O
the	O
training	O
outputs	B
,	O
f	O
,	O
and	O
the	O
test	O
outputs	B
f∗	O
according	O
to	O
the	O
prior	O
is	O
(	O
cid:20	O
)	O
k	O
(	O
x	O
,	O
x	O
)	O
k	O
(	O
x	O
,	O
x∗	O
)	O
(	O
cid:20	O
)	O
f	O
(	O
cid:21	O
)	O
(	O
cid:19	O
)	O
.	O
(	O
2.18	O
)	O
k	O
(	O
x∗	O
,	O
x	O
)	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
(	O
cid:21	O
)	O
f∗	O
(	O
cid:18	O
)	O
∼	O
n	O
0	O
,	O
magnitude	O
joint	B
prior	O
if	O
there	O
are	O
n	O
training	O
points	O
and	O
n∗	O
test	O
points	O
then	O
k	O
(	O
x	O
,	O
x∗	O
)	O
denotes	O
the	O
n	O
×	O
n∗	O
matrix	B
of	O
the	O
covariances	O
evaluated	O
at	O
all	O
pairs	O
of	O
training	O
and	O
test	O
points	O
,	O
and	O
similarly	O
for	O
the	O
other	O
entries	O
k	O
(	O
x	O
,	O
x	O
)	O
,	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
and	O
k	O
(	O
x∗	O
,	O
x	O
)	O
.	O
to	O
get	O
the	O
posterior	O
distribution	O
over	O
functions	O
we	O
need	O
to	O
restrict	O
this	O
joint	B
prior	O
distribution	O
to	O
contain	O
only	O
those	O
functions	O
which	O
agree	O
with	O
the	O
observed	O
data	O
points	O
.	O
graphically	O
in	O
figure	O
2.2	O
you	O
may	O
think	O
of	O
generating	O
functions	O
from	O
the	O
prior	O
,	O
and	O
rejecting	O
the	O
ones	O
that	O
disagree	O
with	O
the	O
observations	O
,	O
al-	O
graphical	O
rejection	O
−505−2−1012input	O
,	O
xoutput	O
,	O
f	O
(	O
x	O
)	O
−505−2−1012input	O
,	O
xoutput	O
,	O
f	O
(	O
x	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
16	O
regression	B
noise-free	O
predictive	B
distribution	O
predictive	B
distribution	O
though	O
this	O
strategy	O
would	O
not	O
be	O
computationally	O
very	O
eﬃcient	O
.	O
fortunately	O
,	O
in	O
probabilistic	B
terms	O
this	O
operation	O
is	O
extremely	O
simple	O
,	O
corresponding	O
to	O
con-	O
ditioning	O
the	O
joint	B
gaussian	O
prior	O
distribution	O
on	O
the	O
observations	O
(	O
see	B
section	O
a.2	O
for	O
further	O
details	O
)	O
to	O
give	O
f∗|x∗	O
,	O
x	O
,	O
f	O
∼	O
n	O
(	O
cid:0	O
)	O
k	O
(	O
x∗	O
,	O
x	O
)	O
k	O
(	O
x	O
,	O
x	O
)	O
−1f	O
,	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
k	O
(	O
x∗	O
,	O
x	O
)	O
k	O
(	O
x	O
,	O
x	O
)	O
−1k	O
(	O
x	O
,	O
x∗	O
)	O
(	O
cid:1	O
)	O
.	O
(	O
2.19	O
)	O
function	B
values	O
f∗	O
(	O
corresponding	O
to	O
test	O
inputs	O
x∗	O
)	O
can	O
be	O
sampled	O
from	O
the	O
joint	B
posterior	O
distribution	O
by	O
evaluating	O
the	O
mean	O
and	O
covariance	B
matrix	I
from	O
eq	O
.	O
(	O
2.19	O
)	O
and	O
generating	O
samples	O
according	O
to	O
the	O
method	O
described	O
in	O
section	O
a.2	O
.	O
figure	O
2.2	O
(	O
b	O
)	O
shows	O
the	O
results	O
of	O
these	O
computations	O
given	O
the	O
ﬁve	O
data-	O
points	O
marked	O
with	O
+	O
symbols	O
.	O
notice	O
that	O
it	O
is	O
trivial	O
to	O
extend	O
these	O
compu-	O
tations	O
to	O
multidimensional	O
inputs	O
–	O
one	O
simply	O
needs	O
to	O
change	O
the	O
evaluation	O
of	O
the	O
covariance	B
function	I
in	O
accordance	O
with	O
eq	O
.	O
(	O
2.16	O
)	O
,	O
although	O
the	O
resulting	O
functions	O
may	O
be	O
harder	O
to	O
display	O
graphically	O
.	O
prediction	B
using	O
noisy	O
observations	O
it	O
is	O
typical	O
for	O
more	O
realistic	O
modelling	O
situations	O
that	O
we	O
do	O
not	O
have	O
access	O
to	O
function	B
values	O
themselves	O
,	O
but	O
only	O
noisy	O
versions	O
thereof	O
y	O
=	O
f	O
(	O
x	O
)	O
+	O
ε.8	O
assuming	O
additive	O
independent	O
identically	O
distributed	O
gaussian	O
noise	O
ε	O
with	O
variance	O
σ2	O
n	O
,	O
the	O
prior	O
on	O
the	O
noisy	O
observations	O
becomes	O
cov	O
(	O
yp	O
,	O
yq	O
)	O
=	O
k	O
(	O
xp	O
,	O
xq	O
)	O
+	O
σ2	O
nδpq	O
or	O
cov	O
(	O
y	O
)	O
=	O
k	O
(	O
x	O
,	O
x	O
)	O
+	O
σ2	O
ni	O
,	O
(	O
2.20	O
)	O
where	O
δpq	O
is	O
a	O
kronecker	O
delta	O
which	O
is	O
one	O
iﬀ	O
p	O
=	O
q	O
and	O
zero	O
otherwise	O
.	O
it	O
follows	O
from	O
the	O
independence9	O
assumption	O
about	O
the	O
noise	O
,	O
that	O
a	O
diagonal	O
matrix10	O
is	O
added	O
,	O
in	O
comparison	O
to	O
the	O
noise	O
free	O
case	O
,	O
eq	O
.	O
(	O
2.16	O
)	O
.	O
introducing	O
the	O
noise	O
term	O
in	O
eq	O
.	O
(	O
2.18	O
)	O
we	O
can	O
write	O
the	O
joint	B
distribution	O
of	O
the	O
observed	O
target	O
values	O
and	O
the	O
function	B
values	O
at	O
the	O
test	O
locations	O
under	O
the	O
prior	O
as	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
y	O
f∗	O
(	O
cid:18	O
)	O
(	O
cid:20	O
)	O
k	O
(	O
x	O
,	O
x	O
)	O
+	O
σ2	O
k	O
(	O
x∗	O
,	O
x	O
)	O
∼	O
n	O
0	O
,	O
(	O
cid:21	O
)	O
(	O
cid:19	O
)	O
ni	O
k	O
(	O
x	O
,	O
x∗	O
)	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
.	O
(	O
2.21	O
)	O
deriving	O
the	O
conditional	B
distribution	O
corresponding	O
to	O
eq	O
.	O
(	O
2.19	O
)	O
we	O
arrive	O
at	O
the	O
key	O
predictive	B
equations	O
for	O
gaussian	O
process	B
regression	O
f∗|x	O
,	O
y	O
,	O
x∗	O
∼	O
n	O
(	O
cid:0	O
)	O
¯f∗	O
,	O
cov	O
(	O
f∗	O
)	O
(	O
cid:1	O
)	O
,	O
where	O
¯f∗	O
,	O
e	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
k	O
(	O
x∗	O
,	O
x	O
)	O
[	O
k	O
(	O
x	O
,	O
x	O
)	O
+	O
σ2	O
cov	O
(	O
f∗	O
)	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
k	O
(	O
x∗	O
,	O
x	O
)	O
[	O
k	O
(	O
x	O
,	O
x	O
)	O
+	O
σ2	O
ni	O
(	O
cid:3	O
)	O
−1	O
ni	O
]	O
−1y	O
,	O
k	O
(	O
x	O
,	O
x∗	O
)	O
.	O
(	O
2.22	O
)	O
(	O
2.23	O
)	O
(	O
2.24	O
)	O
8there	O
are	O
some	O
situations	O
where	O
it	O
is	O
reasonable	O
to	O
assume	O
that	O
the	O
observations	O
are	O
noise-free	O
,	O
for	O
example	O
for	O
computer	O
simulations	O
,	O
see	B
e.g	O
.	O
sacks	O
et	O
al	O
.	O
[	O
1989	O
]	O
.	O
9more	O
complicated	O
noise	O
models	O
with	O
non-trivial	O
covariance	B
structure	O
can	O
also	O
be	O
handled	O
,	O
see	B
section	O
9.2	O
.	O
10notice	O
that	O
the	O
kronecker	O
delta	O
is	O
on	O
the	O
index	O
of	O
the	O
cases	O
,	O
not	O
the	O
value	O
of	O
the	O
input	O
;	O
for	O
the	O
signal	O
part	O
of	O
the	O
covariance	B
function	I
the	O
input	O
value	O
is	O
the	O
index	O
set	B
to	O
the	O
random	O
variables	O
describing	O
the	O
function	B
,	O
for	O
the	O
noise	O
part	O
it	O
is	O
the	O
identity	O
of	O
the	O
point	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
2.2	O
function-space	O
view	O
	O
	O
f1	O
	O
f∗	O
6	O
6	O
6	O
6	O
y1	O
6	O
6	O
y∗	O
6	O
observations	O
gaussian	O
ﬁeld	O
yc	O
6	O
	O
fc	O
	O
6	O
6	O
6	O
17	O
correspondence	O
with	O
weight-space	O
view	O
compact	O
notation	O
predictive	B
distribution	O
linear	B
predictor	O
inputs	O
x1	O
x2	O
x∗	O
xc	O
figure	O
2.3	O
:	O
graphical	O
model	B
(	O
chain	O
graph	O
)	O
for	O
a	O
gp	O
for	O
regression	B
.	O
squares	O
rep-	O
resent	O
observed	O
variables	O
and	O
circles	O
represent	O
unknowns	O
.	O
the	O
thick	O
horizontal	O
bar	O
represents	O
a	O
set	B
of	O
fully	O
connected	O
nodes	O
.	O
note	O
that	O
an	O
observation	O
yi	O
is	O
conditionally	O
independent	O
of	O
all	O
other	O
nodes	O
given	O
the	O
corresponding	O
latent	O
variable	O
,	O
fi	O
.	O
because	O
of	O
the	O
marginalization	B
property	I
of	O
gps	O
addition	O
of	O
further	O
inputs	O
,	O
x	O
,	O
latent	O
variables	O
,	O
f	O
,	O
and	O
unobserved	O
targets	O
,	O
y∗	O
,	O
does	O
not	O
change	O
the	O
distribution	O
of	O
any	O
other	O
variables	O
.	O
notice	O
that	O
we	O
now	O
have	O
exact	O
correspondence	O
with	O
the	O
weight	O
space	O
view	O
in	O
eq	O
.	O
(	O
2.12	O
)	O
when	O
identifying	O
k	O
(	O
c	O
,	O
d	O
)	O
=	O
φ	O
(	O
c	O
)	O
>	O
σpφ	O
(	O
d	O
)	O
,	O
where	O
c	O
,	O
d	O
stand	O
for	O
ei-	O
ther	O
x	O
or	O
x∗	O
.	O
for	O
any	O
set	B
of	O
basis	O
functions	O
,	O
we	O
can	O
compute	O
the	O
corresponding	O
covariance	B
function	I
as	O
k	O
(	O
xp	O
,	O
xq	O
)	O
=	O
φ	O
(	O
xp	O
)	O
>	O
σpφ	O
(	O
xq	O
)	O
;	O
conversely	O
,	O
for	O
every	O
(	O
posi-	O
tive	O
deﬁnite	O
)	O
covariance	B
function	I
k	O
,	O
there	O
exists	O
a	O
(	O
possibly	O
inﬁnite	O
)	O
expansion	O
in	O
terms	O
of	O
basis	O
functions	O
,	O
see	B
section	O
4.3.	O
the	O
expressions	O
involving	O
k	O
(	O
x	O
,	O
x	O
)	O
,	O
k	O
(	O
x	O
,	O
x∗	O
)	O
and	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
etc	O
.	O
can	O
look	O
rather	O
unwieldy	O
,	O
so	O
we	O
now	O
introduce	O
a	O
compact	O
form	O
of	O
the	O
notation	O
setting	O
k	O
=	O
k	O
(	O
x	O
,	O
x	O
)	O
and	O
k∗	O
=	O
k	O
(	O
x	O
,	O
x∗	O
)	O
.	O
in	O
the	O
case	O
that	O
there	O
is	O
only	O
one	O
test	O
point	O
x∗	O
we	O
write	O
k	O
(	O
x∗	O
)	O
=	O
k∗	O
to	O
denote	O
the	O
vector	O
of	O
covariances	O
between	O
the	O
test	O
point	O
and	O
the	O
n	O
training	O
points	O
.	O
using	O
this	O
compact	O
notation	O
and	O
for	O
a	O
single	O
test	O
point	O
x∗	O
,	O
equations	O
2.23	O
and	O
2.24	O
reduce	O
to	O
¯f∗	O
=	O
k	O
>	O
∗	O
(	O
k	O
+	O
σ2	O
ni	O
)	O
−1y	O
,	O
v	O
[	O
f∗	O
]	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
k	O
>	O
∗	O
(	O
k	O
+	O
σ2	O
ni	O
)	O
−1k∗	O
.	O
(	O
2.25	O
)	O
(	O
2.26	O
)	O
let	O
us	O
examine	O
the	O
predictive	B
distribution	O
as	O
given	O
by	O
equations	O
2.25	O
and	O
2.26.	O
note	O
ﬁrst	O
that	O
the	O
mean	O
prediction	O
eq	O
.	O
(	O
2.25	O
)	O
is	O
a	O
linear	B
combination	O
of	O
obser-	O
vations	O
y	O
;	O
this	O
is	O
sometimes	O
referred	O
to	O
as	O
a	O
linear	B
predictor	O
.	O
another	O
way	O
to	O
look	O
at	O
this	O
equation	O
is	O
to	O
see	B
it	O
as	O
a	O
linear	B
combination	O
of	O
n	O
kernel	B
functions	O
,	O
each	O
one	O
centered	O
on	O
a	O
training	O
point	O
,	O
by	O
writing	O
nx	O
¯f	O
(	O
x∗	O
)	O
=	O
αik	O
(	O
xi	O
,	O
x∗	O
)	O
(	O
2.27	O
)	O
i=1	O
ni	O
)	O
−1y	O
.	O
the	O
fact	O
that	O
the	O
mean	O
prediction	O
for	O
f	O
(	O
x∗	O
)	O
can	O
be	O
where	O
α	O
=	O
(	O
k	O
+	O
σ2	O
written	O
as	O
eq	O
.	O
(	O
2.27	O
)	O
despite	O
the	O
fact	O
that	O
the	O
gp	O
can	O
be	O
represented	O
in	O
terms	O
of	O
a	O
(	O
possibly	O
inﬁnite	O
)	O
number	O
of	O
basis	O
functions	O
is	O
one	O
manifestation	O
of	O
the	O
representer	B
theorem	I
;	O
see	B
section	O
6.2	O
for	O
more	O
on	O
this	O
point	O
.	O
we	O
can	O
understand	O
this	O
result	O
intuitively	O
because	O
although	O
the	O
gp	O
deﬁnes	O
a	O
joint	B
gaussian	O
dis-	O
tribution	O
over	O
all	O
of	O
the	O
y	O
variables	O
,	O
one	O
for	O
each	O
point	O
in	O
the	O
index	O
set	B
x	O
,	O
for	O
representer	B
theorem	I
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
18	O
regression	B
(	O
a	O
)	O
,	O
posterior	O
(	O
b	O
)	O
,	O
posterior	O
covariance	O
figure	O
2.4	O
:	O
panel	O
(	O
a	O
)	O
is	O
identical	O
to	O
figure	O
2.2	O
(	O
b	O
)	O
showing	O
three	O
random	O
functions	O
drawn	O
from	O
the	O
posterior	O
.	O
panel	O
(	O
b	O
)	O
shows	O
the	O
posterior	O
co-variance	O
between	O
f	O
(	O
x	O
)	O
and	O
f	O
(	O
x0	O
)	O
for	O
the	O
same	O
data	O
for	O
three	O
diﬀerent	O
values	O
of	O
x0	O
.	O
note	O
,	O
that	O
the	O
covariance	B
at	O
close	O
points	O
is	O
high	O
,	O
falling	O
to	O
zero	O
at	O
the	O
training	O
points	O
(	O
where	O
there	O
is	O
no	O
variance	O
,	O
since	O
it	O
is	O
a	O
noise-free	O
process	B
)	O
,	O
then	O
becomes	O
negative	O
,	O
etc	O
.	O
this	O
happens	O
because	O
if	O
the	O
smooth	O
function	B
happens	O
to	O
be	O
less	O
than	O
the	O
mean	O
on	O
one	O
side	O
of	O
the	O
data	O
point	O
,	O
it	O
tends	O
to	O
exceed	O
the	O
mean	O
on	O
the	O
other	O
side	O
,	O
causing	O
a	O
reversal	O
of	O
the	O
sign	O
of	O
the	O
covariance	B
at	O
the	O
data	O
points	O
.	O
note	O
for	O
contrast	O
that	O
the	O
prior	O
covariance	B
is	O
simply	O
of	O
gaussian	O
shape	O
and	O
never	O
negative	O
.	O
making	O
predictions	O
at	O
x∗	O
we	O
only	O
care	O
about	O
the	O
(	O
n+1	O
)	O
-dimensional	O
distribution	O
deﬁned	O
by	O
the	O
n	O
training	O
points	O
and	O
the	O
test	O
point	O
.	O
as	O
a	O
gaussian	O
distribu-	O
tion	O
is	O
marginalized	O
by	O
just	O
taking	O
the	O
relevant	O
block	O
of	O
the	O
joint	B
covariance	O
matrix	B
(	O
see	B
section	O
a.2	O
)	O
it	O
is	O
clear	O
that	O
conditioning	O
this	O
(	O
n+1	O
)	O
-dimensional	O
distribution	O
on	O
the	O
observations	O
gives	O
us	O
the	O
desired	O
result	O
.	O
a	O
graphical	O
model	B
representation	O
of	O
a	O
gp	O
is	O
given	O
in	O
figure	O
2.3.	O
note	O
also	O
that	O
the	O
variance	O
in	O
eq	O
.	O
(	O
2.24	O
)	O
does	O
not	O
depend	O
on	O
the	O
observed	O
targets	O
,	O
but	O
only	O
on	O
the	O
inputs	O
;	O
this	O
is	O
a	O
property	O
of	O
the	O
gaussian	O
distribution	O
.	O
the	O
variance	O
is	O
the	O
diﬀerence	O
between	O
two	O
terms	O
:	O
the	O
ﬁrst	O
term	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
is	O
simply	O
the	O
prior	O
covariance	B
;	O
from	O
that	O
is	O
subtracted	O
a	O
(	O
positive	O
)	O
term	O
,	O
repre-	O
senting	O
the	O
information	O
the	O
observations	O
gives	O
us	O
about	O
the	O
function	B
.	O
we	O
can	O
very	O
simply	O
compute	O
the	O
predictive	B
distribution	O
of	O
test	O
targets	O
y∗	O
by	O
adding	O
ni	O
to	O
the	O
variance	O
in	O
the	O
expression	O
for	O
cov	O
(	O
f∗	O
)	O
.	O
σ2	O
the	O
predictive	B
distribution	O
for	O
the	O
gp	O
model	B
gives	O
more	O
than	O
just	O
pointwise	O
errorbars	O
of	O
the	O
simpliﬁed	O
eq	O
.	O
(	O
2.26	O
)	O
.	O
although	O
not	O
stated	O
explicitly	O
,	O
eq	O
.	O
(	O
2.24	O
)	O
holds	O
unchanged	O
when	O
x∗	O
denotes	O
multiple	O
test	O
inputs	O
;	O
in	O
this	O
case	O
the	O
co-	O
variance	O
of	O
the	O
test	O
targets	O
are	O
computed	O
(	O
whose	O
diagonal	O
elements	O
are	O
the	O
pointwise	O
variances	O
)	O
.	O
in	O
fact	O
,	O
eq	O
.	O
(	O
2.23	O
)	O
is	O
the	O
mean	B
function	I
and	O
eq	O
.	O
(	O
2.24	O
)	O
the	O
covariance	B
function	I
of	O
the	O
(	O
gaussian	O
)	O
posterior	B
process	I
;	O
recall	O
the	O
deﬁnition	O
of	O
gaussian	O
process	B
from	O
page	O
13.	O
the	O
posterior	O
covariance	O
in	O
illustrated	O
in	O
figure	O
2.4	O
(	O
b	O
)	O
.	O
it	O
will	O
be	O
useful	O
(	O
particularly	O
for	O
chapter	O
5	O
)	O
to	O
introduce	O
the	O
marginal	B
likeli-	O
hood	O
(	O
or	O
evidence	B
)	O
p	O
(	O
y|x	O
)	O
at	O
this	O
point	O
.	O
the	O
marginal	B
likelihood	I
is	O
the	O
integral	O
noisy	O
predictions	O
joint	B
predictions	O
posterior	B
process	I
marginal	O
likelihood	B
−505−2−1012input	O
,	O
xoutput	O
,	O
f	O
(	O
x	O
)	O
−505−0.200.20.40.6input	O
,	O
xpost	O
.	O
covariance	B
,	O
cov	O
(	O
f	O
(	O
x	O
)	O
,	O
f	O
(	O
x	O
’	O
)	O
)	O
x	O
’	O
=−2x	O
’	O
=1x	O
’	O
=3	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
2.3	O
varying	O
the	O
hyperparameters	B
19	O
o	O
input	O
:	O
x	O
(	O
inputs	O
)	O
,	O
y	O
(	O
targets	O
)	O
,	O
k	O
(	O
covariance	B
function	I
)	O
,	O
σ2	O
n	O
(	O
noise	O
level	O
)	O
,	O
x∗	O
(	O
test	O
input	O
)	O
ni	O
)	O
o	O
predictive	B
mean	O
eq	O
.	O
(	O
2.25	O
)	O
2	O
y	O
>	O
α	O
−p	O
2	O
:	O
l	O
:	O
=	O
cholesky	O
(	O
k	O
+	O
σ2	O
α	O
:	O
=	O
l	O
>	O
\	O
(	O
l\y	O
)	O
4	O
:	O
¯f∗	O
:	O
=	O
k	O
>	O
∗	O
α	O
v	O
:	O
=	O
l\k∗	O
predictive	B
variance	O
eq	O
.	O
(	O
2.26	O
)	O
6	O
:	O
v	O
[	O
f∗	O
]	O
:	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
v	O
>	O
v	O
log	O
p	O
(	O
y|x	O
)	O
:	O
=	O
−	O
1	O
eq	O
.	O
(	O
2.30	O
)	O
8	O
:	O
return	O
:	O
¯f∗	O
(	O
mean	O
)	O
,	O
v	O
[	O
f∗	O
]	O
(	O
variance	O
)	O
,	O
log	O
p	O
(	O
y|x	O
)	O
(	O
log	O
marginal	O
likelihood	B
)	O
algorithm	O
2.1	O
:	O
predictions	O
and	O
log	O
marginal	O
likelihood	B
for	O
gaussian	O
process	B
regres-	O
sion	O
.	O
the	O
implementation	O
addresses	O
the	O
matrix	B
inversion	O
required	O
by	O
eq	O
.	O
(	O
2.25	O
)	O
and	O
(	O
2.26	O
)	O
using	O
cholesky	O
factorization	O
,	O
see	B
section	O
a.4	O
.	O
for	O
multiple	O
test	O
cases	O
lines	O
4-6	O
are	O
repeated	O
.	O
the	O
log	O
determinant	O
required	O
in	O
eq	O
.	O
(	O
2.30	O
)	O
is	O
computed	O
from	O
the	O
cholesky	O
factor	O
(	O
for	O
large	O
n	O
it	O
may	O
not	O
be	O
possible	O
to	O
represent	O
the	O
determinant	O
itself	O
)	O
.	O
the	O
computational	O
complexity	O
is	O
n3/6	O
for	O
the	O
cholesky	O
decomposition	O
in	O
line	O
2	O
,	O
and	O
n2/2	O
for	O
solving	O
triangular	O
systems	O
in	O
line	O
3	O
and	O
(	O
for	O
each	O
test	O
case	O
)	O
in	O
line	O
5.	O
i	O
log	O
lii	O
−	O
n	O
2	O
log	O
2π	O
of	O
the	O
likelihood	B
times	O
the	O
prior	O
p	O
(	O
y|x	O
)	O
=	O
z	O
p	O
(	O
y|f	O
,	O
x	O
)	O
p	O
(	O
f|x	O
)	O
df	O
.	O
(	O
2.28	O
)	O
the	O
term	O
marginal	B
likelihood	I
refers	O
to	O
the	O
marginalization	O
over	O
the	O
function	B
values	O
f.	O
under	O
the	O
gaussian	O
process	B
model	O
the	O
prior	O
is	O
gaussian	O
,	O
f|x	O
∼	O
n	O
(	O
0	O
,	O
k	O
)	O
,	O
or	O
log	O
p	O
(	O
f|x	O
)	O
=	O
−	O
1	O
2	O
log	O
|k|	O
−	O
n	O
2	O
log	O
2π	O
,	O
(	O
2.29	O
)	O
and	O
the	O
likelihood	B
is	O
a	O
factorized	O
gaussian	O
y|f	O
∼	O
n	O
(	O
f	O
,	O
σ2	O
ni	O
)	O
so	O
we	O
can	O
make	O
use	O
of	O
equations	O
a.7	O
and	O
a.8	O
to	O
perform	O
the	O
integration	O
yielding	O
the	O
log	O
marginal	O
likelihood	B
2	O
f	O
>	O
k−1f	O
−	O
1	O
log	O
p	O
(	O
y|x	O
)	O
=	O
−	O
1	O
2	O
y	O
>	O
(	O
k	O
+	O
σ2	O
ni	O
)	O
−1y	O
−	O
1	O
2	O
log	O
|k	O
+	O
σ2	O
ni|	O
−	O
n	O
2	O
log	O
2π	O
.	O
this	O
result	O
can	O
also	O
be	O
obtained	O
directly	O
by	O
observing	O
that	O
y	O
∼	O
n	O
(	O
0	O
,	O
k	O
+	O
σ2	O
(	O
2.30	O
)	O
ni	O
)	O
.	O
a	O
practical	O
implementation	O
of	O
gaussian	O
process	B
regression	O
(	O
gpr	O
)	O
is	O
shown	O
in	O
algorithm	O
2.1.	O
the	O
algorithm	O
uses	O
cholesky	O
decomposition	O
,	O
instead	O
of	O
di-	O
rectly	O
inverting	O
the	O
matrix	B
,	O
since	O
it	O
is	O
faster	O
and	O
numerically	O
more	O
stable	O
,	O
see	B
section	O
a.4	O
.	O
the	O
algorithm	O
returns	O
the	O
predictive	B
mean	O
and	O
variance	O
for	O
noise	O
free	O
test	O
data—to	O
compute	O
the	O
predictive	B
distribution	O
for	O
noisy	O
test	O
data	O
y∗	O
,	O
simply	O
add	O
the	O
noise	O
variance	O
σ2	O
n	O
to	O
the	O
predictive	B
variance	O
of	O
f∗	O
.	O
2.3	O
varying	O
the	O
hyperparameters	B
typically	O
the	O
covariance	B
functions	O
that	O
we	O
use	O
will	O
have	O
some	O
free	O
parameters	O
.	O
for	O
example	O
,	O
the	O
squared-exponential	O
covariance	B
function	I
in	O
one	O
dimension	O
has	O
the	O
following	O
form	O
f	O
exp	O
(	O
cid:0	O
)	O
−	O
1	O
2	O
‘	O
2	O
(	O
xp	O
−	O
xq	O
)	O
2	O
(	O
cid:1	O
)	O
+	O
σ2	O
nδpq	O
.	O
ky	O
(	O
xp	O
,	O
xq	O
)	O
=	O
σ2	O
(	O
2.31	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
20	O
regression	B
(	O
a	O
)	O
,	O
‘	O
=	O
1	O
hyperparameters	B
(	O
b	O
)	O
,	O
‘	O
=	O
0.3	O
(	O
c	O
)	O
,	O
‘	O
=	O
3	O
figure	O
2.5	O
:	O
(	O
a	O
)	O
data	O
is	O
generated	O
from	O
a	O
gp	O
with	O
hyperparameters	B
(	O
‘	O
,	O
σf	O
,	O
σn	O
)	O
=	O
(	O
1	O
,	O
1	O
,	O
0.1	O
)	O
,	O
as	O
shown	O
by	O
the	O
+	O
symbols	O
.	O
using	O
gaussian	O
process	B
prediction	O
with	O
these	O
hyperparameters	B
we	O
obtain	O
a	O
95	O
%	O
conﬁdence	O
region	O
for	O
the	O
underlying	O
function	B
f	O
(	O
shown	O
in	O
grey	O
)	O
.	O
panels	O
(	O
b	O
)	O
and	O
(	O
c	O
)	O
again	O
show	O
the	O
95	O
%	O
conﬁdence	O
region	O
,	O
but	O
this	O
time	O
for	O
hyperparameter	O
values	O
(	O
0.3	O
,	O
1.08	O
,	O
0.00005	O
)	O
and	O
(	O
3.0	O
,	O
1.16	O
,	O
0.89	O
)	O
respectively	O
.	O
the	O
covariance	B
is	O
denoted	O
ky	O
as	O
it	O
is	O
for	O
the	O
noisy	O
targets	O
y	O
rather	O
than	O
for	O
the	O
underlying	O
function	B
f.	O
observe	O
that	O
the	O
length-scale	B
‘	O
,	O
the	O
signal	O
variance	O
σ2	O
f	O
and	O
the	O
noise	O
variance	O
σ2	O
n	O
can	O
be	O
varied	O
.	O
in	O
general	O
we	O
call	O
the	O
free	O
parameters	O
hyperparameters.11	O
in	O
chapter	O
5	O
we	O
will	O
consider	O
various	O
methods	O
for	O
determining	O
the	O
hyperpa-	O
rameters	O
from	O
training	O
data	O
.	O
however	O
,	O
in	O
this	O
section	O
our	O
aim	O
is	O
more	O
simply	O
to	O
explore	O
the	O
eﬀects	O
of	O
varying	O
the	O
hyperparameters	B
on	O
gp	O
prediction	B
.	O
consider	O
the	O
data	O
shown	O
by	O
+	O
signs	O
in	O
figure	O
2.5	O
(	O
a	O
)	O
.	O
this	O
was	O
generated	O
from	O
a	O
gp	O
with	O
the	O
se	O
kernel	B
with	O
(	O
‘	O
,	O
σf	O
,	O
σn	O
)	O
=	O
(	O
1	O
,	O
1	O
,	O
0.1	O
)	O
.	O
the	O
ﬁgure	O
also	O
shows	O
the	O
2	O
standard-deviation	O
error	B
bars	O
for	O
the	O
predictions	O
obtained	O
using	O
these	O
values	O
of	O
the	O
hyperparameters	B
,	O
as	O
per	O
eq	O
.	O
(	O
2.24	O
)	O
.	O
notice	O
how	O
the	O
error	B
bars	O
get	O
larger	O
for	O
input	O
values	O
that	O
are	O
distant	O
from	O
any	O
training	O
points	O
.	O
indeed	O
if	O
the	O
x-axis	O
11we	O
refer	O
to	O
the	O
parameters	O
of	O
the	O
covariance	B
function	I
as	O
hyperparameters	B
to	O
emphasize	O
that	O
they	O
are	O
parameters	O
of	O
a	O
non-parametric	B
model	O
;	O
in	O
accordance	O
with	O
the	O
weight-space	O
view	O
,	O
section	O
2.1	O
,	O
the	O
parameters	O
(	O
weights	O
)	O
of	O
the	O
underlying	O
parametric	B
model	O
have	O
been	O
integrated	B
out	O
.	O
−505−3−2−10123input	O
,	O
xoutput	O
,	O
y−505−3−2−10123input	O
,	O
xoutput	O
,	O
y−505−3−2−10123input	O
,	O
xoutput	O
,	O
y	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
2.4	O
decision	O
theory	O
for	O
regression	B
21	O
were	O
extended	O
one	O
would	O
see	B
the	O
error	B
bars	O
reﬂect	O
the	O
prior	O
standard	O
deviation	O
of	O
the	O
process	B
σf	O
away	O
from	O
the	O
data	O
.	O
if	O
we	O
set	B
the	O
length-scale	B
shorter	O
so	O
that	O
‘	O
=	O
0.3	O
and	O
kept	O
the	O
other	O
pa-	O
rameters	O
the	O
same	O
,	O
then	O
generating	O
from	O
this	O
process	B
we	O
would	O
expect	O
to	O
see	B
plots	O
like	O
those	O
in	O
figure	O
2.5	O
(	O
a	O
)	O
except	O
that	O
the	O
x-axis	O
should	O
be	O
rescaled	O
by	O
a	O
factor	O
of	O
0.3	O
;	O
equivalently	O
if	O
the	O
same	O
x-axis	O
was	O
kept	O
as	O
in	O
figure	O
2.5	O
(	O
a	O
)	O
then	O
a	O
sample	O
function	B
would	O
look	O
much	O
more	O
wiggly	O
.	O
if	O
we	O
make	O
predictions	O
with	O
a	O
process	B
with	O
‘	O
=	O
0.3	O
on	O
the	O
data	O
generated	O
from	O
the	O
‘	O
=	O
1	O
process	B
then	O
we	O
obtain	O
the	O
result	O
in	O
figure	O
2.5	O
(	O
b	O
)	O
.	O
the	O
remaining	O
two	O
parameters	O
were	O
set	B
by	O
optimizing	O
the	O
marginal	B
likelihood	I
,	O
as	O
explained	O
in	O
chapter	O
5.	O
in	O
this	O
case	O
the	O
noise	O
parameter	O
is	O
reduced	O
to	O
σn	O
=	O
0.00005	O
as	O
the	O
greater	O
ﬂexibility	O
of	O
the	O
“	O
signal	O
”	O
means	O
that	O
the	O
noise	O
level	O
can	O
be	O
reduced	O
.	O
this	O
can	O
be	O
observed	O
at	O
the	O
two	O
datapoints	O
near	O
x	O
=	O
2.5	O
in	O
the	O
plots	O
.	O
in	O
figure	O
2.5	O
(	O
a	O
)	O
(	O
‘	O
=	O
1	O
)	O
these	O
are	O
essentially	O
explained	O
as	O
a	O
similar	O
function	B
value	O
with	O
diﬀering	O
noise	O
.	O
however	O
,	O
in	O
figure	O
2.5	O
(	O
b	O
)	O
(	O
‘	O
=	O
0.3	O
)	O
the	O
noise	O
level	O
is	O
very	O
low	O
,	O
so	O
these	O
two	O
points	O
have	O
to	O
be	O
explained	O
by	O
a	O
sharp	O
variation	O
in	O
the	O
value	O
of	O
the	O
underlying	O
function	B
f.	O
notice	O
also	O
that	O
the	O
short	O
length-scale	B
means	O
that	O
the	O
error	B
bars	O
in	O
figure	O
2.5	O
(	O
b	O
)	O
grow	O
rapidly	O
away	O
from	O
the	O
datapoints	O
.	O
in	O
contrast	O
,	O
we	O
can	O
set	B
the	O
length-scale	B
longer	O
,	O
for	O
example	O
to	O
‘	O
=	O
3	O
,	O
as	O
shown	O
in	O
figure	O
2.5	O
(	O
c	O
)	O
.	O
again	O
the	O
remaining	O
two	O
parameters	O
were	O
set	B
by	O
optimizing	O
the	O
marginal	B
likelihood	I
.	O
in	O
this	O
case	O
the	O
noise	O
level	O
has	O
been	O
increased	O
to	O
σn	O
=	O
0.89	O
and	O
we	O
see	B
that	O
the	O
data	O
is	O
now	O
explained	O
by	O
a	O
slowly	O
varying	O
function	B
with	O
a	O
lot	O
of	O
noise	O
.	O
of	O
course	O
we	O
can	O
take	O
the	O
position	O
of	O
a	O
quickly-varying	O
signal	O
with	O
low	O
noise	O
,	O
or	O
a	O
slowly-varying	O
signal	O
with	O
high	O
noise	O
to	O
extremes	O
;	O
the	O
former	O
would	O
give	O
rise	O
to	O
a	O
white-noise	O
process	B
model	O
for	O
the	O
signal	O
,	O
while	O
the	O
latter	O
would	O
give	O
rise	O
to	O
a	O
constant	O
signal	O
with	O
added	O
white	O
noise	O
.	O
under	O
both	O
these	O
models	O
the	O
datapoints	O
produced	O
should	O
look	O
like	O
white	O
noise	O
.	O
however	O
,	O
studying	O
figure	O
2.5	O
(	O
a	O
)	O
we	O
see	B
that	O
white	O
noise	O
is	O
not	O
a	O
convincing	O
model	B
of	O
the	O
data	O
,	O
as	O
the	O
sequence	O
of	O
y	O
’	O
s	O
does	O
not	O
alternate	O
suﬃciently	O
quickly	O
but	O
has	O
correlations	O
due	O
to	O
the	O
variability	O
of	O
the	O
underlying	O
function	B
.	O
of	O
course	O
this	O
is	O
relatively	O
easy	O
to	O
see	B
in	O
one	O
dimension	O
,	O
but	O
methods	O
such	O
as	O
the	O
marginal	B
likelihood	I
discussed	O
in	O
chapter	O
5	O
generalize	O
to	O
higher	O
dimensions	O
and	O
allow	O
us	O
to	O
score	O
the	O
various	O
models	O
.	O
in	O
this	O
case	O
the	O
marginal	B
likelihood	I
gives	O
a	O
clear	O
preference	O
for	O
(	O
‘	O
,	O
σf	O
,	O
σn	O
)	O
=	O
(	O
1	O
,	O
1	O
,	O
0.1	O
)	O
over	O
the	O
other	O
two	O
alternatives	O
.	O
2.4	O
decision	O
theory	O
for	O
regression	B
in	O
the	O
previous	O
sections	O
we	O
have	O
shown	O
how	O
to	O
compute	O
predictive	B
distributions	O
for	O
the	O
outputs	B
y∗	O
corresponding	O
to	O
the	O
novel	O
test	O
input	O
x∗	O
.	O
the	O
predictive	B
dis-	O
tribution	O
is	O
gaussian	O
with	O
mean	O
and	O
variance	O
given	O
by	O
eq	O
.	O
(	O
2.25	O
)	O
and	O
eq	O
.	O
(	O
2.26	O
)	O
.	O
in	O
practical	O
applications	O
,	O
however	O
,	O
we	O
are	O
often	O
forced	O
to	O
make	O
a	O
decision	O
about	O
how	O
to	O
act	O
,	O
i.e	O
.	O
we	O
need	O
a	O
point-like	O
prediction	B
which	O
is	O
optimal	B
in	O
some	O
sense	O
.	O
to	O
this	O
end	O
we	O
need	O
a	O
loss	B
function	I
,	O
l	O
(	O
ytrue	O
,	O
yguess	O
)	O
,	O
which	O
speciﬁes	O
the	O
loss	B
(	O
or	O
too	O
short	O
length-scale	B
too	O
long	O
length-scale	B
model	O
comparison	O
optimal	B
predictions	O
loss	B
function	I
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
22	O
regression	B
non-bayesian	O
paradigm	O
bayesian	O
paradigm	O
penalty	O
)	O
incurred	O
by	O
guessing	O
the	O
value	O
yguess	O
when	O
the	O
true	O
value	O
is	O
ytrue	O
.	O
for	O
example	O
,	O
the	O
loss	B
function	I
could	O
equal	O
the	O
absolute	O
deviation	O
between	O
the	O
guess	O
and	O
the	O
truth	O
.	O
notice	O
that	O
we	O
computed	O
the	O
predictive	B
distribution	O
without	O
reference	O
to	O
the	O
loss	B
function	I
.	O
in	O
non-bayesian	O
paradigms	O
,	O
the	O
model	B
is	O
typically	O
trained	O
by	O
minimizing	O
the	O
empirical	O
risk	B
(	O
or	O
loss	B
)	O
.	O
in	O
contrast	O
,	O
in	O
the	O
bayesian	O
setting	O
there	O
is	O
a	O
clear	O
separation	O
between	O
the	O
likelihood	B
function	O
(	O
used	O
for	O
training	O
,	O
in	O
addition	O
to	O
the	O
prior	O
)	O
and	O
the	O
loss	B
function	I
.	O
the	O
likelihood	B
function	O
describes	O
how	O
the	O
noisy	O
measurements	O
are	O
assumed	O
to	O
deviate	O
from	O
the	O
underlying	O
noise-	O
free	O
function	B
.	O
the	O
loss	B
function	I
,	O
on	O
the	O
other	O
hand	O
,	O
captures	O
the	O
consequences	O
of	O
making	O
a	O
speciﬁc	O
choice	O
,	O
given	O
an	O
actual	O
true	O
state	O
.	O
the	O
likelihood	B
and	O
loss	B
function	I
need	O
not	O
have	O
anything	O
in	O
common.12	O
expected	O
loss	B
,	O
risk	B
absolute	O
error	B
loss	O
squared	B
error	O
loss	B
robot	O
arm	O
our	O
goal	O
is	O
to	O
make	O
the	O
point	O
prediction	B
yguess	O
which	O
incurs	O
the	O
smallest	O
loss	B
,	O
but	O
how	O
can	O
we	O
achieve	O
that	O
when	O
we	O
don	O
’	O
t	O
know	O
ytrue	O
?	O
instead	O
,	O
we	O
minimize	O
the	O
expected	O
loss	B
or	O
risk	B
,	O
by	O
averaging	O
w.r.t	O
.	O
our	O
model	B
’	O
s	O
opinion	O
as	O
to	O
what	O
the	O
truth	O
might	O
be	O
z	O
˜rl	O
(	O
yguess|x∗	O
)	O
=	O
l	O
(	O
y∗	O
,	O
yguess	O
)	O
p	O
(	O
y∗|x∗	O
,	O
d	O
)	O
dy∗	O
.	O
(	O
2.32	O
)	O
thus	O
our	O
best	O
guess	O
,	O
in	O
the	O
sense	O
that	O
it	O
minimizes	O
the	O
expected	O
loss	B
,	O
is	O
yoptimal|x∗	O
=	O
argmin	O
yguess	O
˜rl	O
(	O
yguess|x∗	O
)	O
.	O
(	O
2.33	O
)	O
in	O
general	O
the	O
value	O
of	O
yguess	O
that	O
minimizes	O
the	O
risk	B
for	O
the	O
loss	B
function	I
|yguess−	O
y∗|	O
is	O
the	O
median	O
of	O
p	O
(	O
y∗|x∗	O
,	O
d	O
)	O
,	O
while	O
for	O
the	O
squared	B
loss	O
(	O
yguess	O
−	O
y∗	O
)	O
2	O
it	O
is	O
the	O
mean	O
of	O
this	O
distribution	O
.	O
when	O
the	O
predictive	B
distribution	O
is	O
gaussian	O
the	O
mean	O
and	O
the	O
median	O
coincide	O
,	O
and	O
indeed	O
for	O
any	O
symmetric	O
loss	B
function	I
and	O
symmetric	O
predictive	B
distribution	O
we	O
always	O
get	O
yguess	O
as	O
the	O
mean	O
of	O
the	O
predictive	B
distribution	O
.	O
however	O
,	O
in	O
many	O
practical	O
problems	O
the	O
loss	B
functions	O
can	O
be	O
asymmetric	O
,	O
e.g	O
.	O
in	O
safety	O
critical	O
applications	O
,	O
and	O
point	O
predictions	O
may	O
be	O
computed	O
directly	O
from	O
eq	O
.	O
(	O
2.32	O
)	O
and	O
eq	O
.	O
(	O
2.33	O
)	O
.	O
a	O
comprehensive	O
treatment	O
of	O
decision	O
theory	O
can	O
be	O
found	O
in	O
berger	O
[	O
1985	O
]	O
.	O
2.5	O
an	O
example	O
application	O
in	O
this	O
section	O
we	O
use	O
gaussian	O
process	B
regression	O
to	O
learn	O
the	O
inverse	O
dynamics	O
of	O
a	O
seven	O
degrees-of-freedom	O
sarcos	O
anthropomorphic	O
robot	B
arm	O
.	O
the	O
task	O
is	O
to	O
map	O
from	O
a	O
21-dimensional	O
input	O
space	O
(	O
7	O
joint	B
positions	O
,	O
7	O
joint	B
velocities	O
,	O
7	O
joint	B
accelerations	O
)	O
to	O
the	O
corresponding	O
7	O
joint	B
torques	O
.	O
this	O
task	O
has	O
pre-	O
viously	O
been	O
used	O
to	O
study	O
regression	B
algorithms	O
by	O
vijayakumar	O
and	O
schaal	O
[	O
2000	O
]	O
,	O
vijayakumar	O
et	O
al	O
.	O
[	O
2002	O
]	O
and	O
vijayakumar	O
et	O
al	O
.	O
[	O
2005	O
]	O
.13	O
following	O
12beware	O
of	O
fallacious	O
arguments	O
like	O
:	O
a	O
gaussian	O
likelihood	B
implies	O
a	O
squared	B
error	O
loss	B
function	I
.	O
13we	O
thank	O
sethu	O
vijayakumar	O
for	O
providing	O
us	O
with	O
the	O
data	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
2.5	O
an	O
example	O
application	O
this	O
previous	O
work	O
we	O
present	O
results	O
below	O
on	O
just	O
one	O
of	O
the	O
seven	O
mappings	O
,	O
from	O
the	O
21	O
input	O
variables	O
to	O
the	O
ﬁrst	O
of	O
the	O
seven	O
torques	O
.	O
one	O
might	O
ask	O
why	O
it	O
is	O
necessary	O
to	O
learn	O
this	O
mapping	O
;	O
indeed	O
there	O
exist	O
physics-based	O
rigid-body-dynamics	O
models	O
which	O
allow	O
us	O
to	O
obtain	O
the	O
torques	O
from	O
the	O
position	O
,	O
velocity	O
and	O
acceleration	O
variables	O
.	O
however	O
,	O
the	O
real	O
robot	B
arm	O
is	O
actuated	O
hydraulically	O
and	O
is	O
rather	O
lightweight	O
and	O
compliant	O
,	O
so	O
the	O
assumptions	O
of	O
the	O
rigid-body-dynamics	O
model	B
are	O
violated	O
(	O
as	O
we	O
see	B
below	O
)	O
.	O
it	O
is	O
worth	O
noting	O
that	O
the	O
rigid-body-dynamics	O
model	B
is	O
nonlinear	O
,	O
involving	O
trigonometric	O
functions	O
and	O
squares	O
of	O
the	O
input	O
variables	O
.	O
an	O
inverse	O
dynamics	O
model	B
can	O
be	O
used	O
in	O
the	O
following	O
manner	O
:	O
a	O
planning	O
module	O
decides	O
on	O
a	O
trajectory	O
that	O
takes	O
the	O
robot	B
from	O
its	O
start	O
to	O
goal	O
states	O
,	O
and	O
this	O
speciﬁes	O
the	O
desired	O
positions	O
,	O
velocities	O
and	O
accelerations	O
at	O
each	O
time	O
.	O
the	O
inverse	O
dynamics	O
model	B
is	O
used	O
to	O
compute	O
the	O
torques	O
needed	O
to	O
achieve	O
this	O
trajectory	O
and	O
errors	O
are	O
corrected	O
using	O
a	O
feedback	O
controller	O
.	O
the	O
dataset	B
consists	O
of	O
48,933	O
input-output	O
pairs	O
,	O
of	O
which	O
44,484	O
were	O
used	O
as	O
a	O
training	O
set	B
and	O
the	O
remaining	O
4,449	O
were	O
used	O
as	O
a	O
test	O
set	B
.	O
the	O
inputs	O
were	O
linearly	O
rescaled	O
to	O
have	O
zero	O
mean	O
and	O
unit	O
variance	O
on	O
the	O
training	O
set	B
.	O
the	O
outputs	B
were	O
centered	O
so	O
as	O
to	O
have	O
zero	O
mean	O
on	O
the	O
training	O
set	B
.	O
given	O
a	O
prediction	B
method	O
,	O
we	O
can	O
evaluate	O
the	O
quality	O
of	O
predictions	O
in	O
several	O
ways	O
.	O
perhaps	O
the	O
simplest	O
is	O
the	O
squared	B
error	O
loss	B
,	O
where	O
we	O
compute	O
the	O
squared	B
residual	O
(	O
y∗	O
−	O
¯f	O
(	O
x∗	O
)	O
)	O
2	O
between	O
the	O
mean	O
prediction	O
and	O
the	O
target	O
at	O
each	O
test	O
point	O
.	O
this	O
can	O
be	O
summarized	O
by	O
the	O
mean	O
squared	O
error	B
(	O
mse	O
)	O
,	O
by	O
averaging	O
over	O
the	O
test	O
set	B
.	O
however	O
,	O
this	O
quantity	O
is	O
sensitive	O
to	O
the	O
overall	O
scale	O
of	O
the	O
target	O
values	O
,	O
so	O
it	O
makes	O
sense	O
to	O
normalize	O
by	O
the	O
variance	O
of	O
the	O
targets	O
of	O
the	O
test	O
cases	O
to	O
obtain	O
the	O
standardized	B
mean	I
squared	I
error	I
(	O
smse	O
)	O
.	O
this	O
causes	O
the	O
trivial	O
method	O
of	O
guessing	O
the	O
mean	O
of	O
the	O
training	O
targets	O
to	O
have	O
a	O
smse	O
of	O
approximately	O
1.	O
additionally	O
if	O
we	O
produce	O
a	O
predictive	B
distribution	O
at	O
each	O
test	O
input	O
we	O
can	O
evaluate	O
the	O
negative	B
log	I
probability	I
of	O
the	O
target	O
under	O
the	O
model.14	O
as	O
gpr	O
produces	O
a	O
gaussian	O
predictive	B
density	O
,	O
one	O
obtains	O
−	O
log	O
p	O
(	O
y∗|d	O
,	O
x∗	O
)	O
=	O
1	O
2	O
log	O
(	O
2πσ2∗	O
)	O
+	O
(	O
y∗	O
−	O
¯f	O
(	O
x∗	O
)	O
)	O
2	O
2σ2∗	O
,	O
(	O
2.34	O
)	O
where	O
the	O
predictive	B
variance	O
σ2∗	O
for	O
gpr	O
is	O
computed	O
as	O
σ2∗	O
=	O
v	O
(	O
f∗	O
)	O
+	O
σ2	O
n	O
,	O
where	O
v	O
(	O
f∗	O
)	O
is	O
given	O
by	O
eq	O
.	O
(	O
2.26	O
)	O
;	O
we	O
must	O
include	O
the	O
noise	O
variance	O
σ2	O
n	O
as	O
we	O
are	O
predicting	O
the	O
noisy	O
target	O
y∗	O
.	O
this	O
loss	B
can	O
be	O
standardized	O
by	O
subtracting	O
the	O
loss	B
that	O
would	O
be	O
obtained	O
under	O
the	O
trivial	O
model	B
which	O
predicts	O
using	O
a	O
gaussian	O
with	O
the	O
mean	O
and	O
variance	O
of	O
the	O
training	O
data	O
.	O
we	O
denote	O
this	O
the	O
standardized	O
log	O
loss	B
(	O
sll	O
)	O
.	O
the	O
mean	O
sll	O
is	O
denoted	O
msll	O
.	O
thus	O
the	O
msll	O
will	O
be	O
approximately	O
zero	O
for	O
simple	O
methods	O
and	O
negative	O
for	O
better	O
methods	O
.	O
a	O
number	O
of	O
models	O
were	O
tested	O
on	O
the	O
data	O
.	O
a	O
linear	B
regression	I
(	O
lr	O
)	O
model	B
provides	O
a	O
simple	O
baseline	O
for	O
the	O
smse	O
.	O
by	O
estimating	O
the	O
noise	O
level	O
from	O
the	O
14	O
it	O
makes	O
sense	O
to	O
use	O
the	O
negative	B
log	I
probability	I
so	O
as	O
to	O
obtain	O
a	O
loss	B
,	O
not	O
a	O
utility	O
.	O
23	O
why	O
learning	B
?	O
mse	O
smse	O
msll	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
24	O
regression	B
method	O
lr	O
rbd	O
lwpr	O
gpr	O
smse	O
msll	O
0.075	O
-1.29	O
0.104	O
0.040	O
0.011	O
-2.25	O
–	O
–	O
table	O
2.1	O
:	O
test	O
results	O
on	O
the	O
inverse	O
dynamics	O
problem	O
for	O
a	O
number	O
of	O
diﬀerent	O
methods	O
.	O
the	O
“	O
–	O
”	O
denotes	O
a	O
missing	O
entry	O
,	O
caused	O
by	O
two	O
methods	O
not	O
producing	O
full	O
predictive	B
distributions	O
,	O
so	O
msll	O
could	O
not	O
be	O
evaluated	O
.	O
residuals	O
on	O
the	O
training	O
set	B
one	O
can	O
also	O
obtain	O
a	O
predictive	B
variance	O
and	O
thus	O
get	O
a	O
msll	O
value	O
for	O
lr	O
.	O
the	O
rigid-body-dynamics	O
(	O
rbd	O
)	O
model	B
has	O
a	O
number	O
of	O
free	O
parameters	O
;	O
these	O
were	O
estimated	O
by	O
vijayakumar	O
et	O
al	O
.	O
[	O
2005	O
]	O
using	O
a	O
least-squares	B
ﬁtting	O
procedure	O
.	O
we	O
also	O
give	O
results	O
for	O
the	O
locally	O
weighted	O
projection	O
regression	B
(	O
lwpr	O
)	O
method	O
of	O
vijayakumar	O
et	O
al	O
.	O
[	O
2005	O
]	O
which	O
is	O
an	O
on-line	O
method	O
that	O
cycles	O
through	O
the	O
dataset	B
multiple	O
times	O
.	O
for	O
the	O
gp	O
models	O
it	O
is	O
computationally	O
expensive	O
to	O
make	O
use	O
of	O
all	O
44,484	O
training	O
cases	O
due	O
to	O
the	O
o	O
(	O
n3	O
)	O
scaling	O
of	O
the	O
basic	O
algorithm	O
.	O
in	O
chapter	O
8	O
we	O
present	O
several	O
diﬀerent	O
approximate	O
gp	O
methods	O
for	O
large	O
datasets	O
.	O
the	O
result	O
given	O
in	O
table	O
2.1	O
was	O
obtained	O
with	O
the	O
subset	B
of	I
regressors	I
(	O
sr	O
)	O
approximation	O
with	O
a	O
subset	O
size	O
of	O
4096.	O
this	O
result	O
is	O
taken	O
from	O
table	O
8.1	O
,	O
which	O
gives	O
full	O
results	O
of	O
the	O
various	O
approximation	O
methods	O
applied	O
to	O
the	O
inverse	O
dynamics	O
problem	O
.	O
the	O
squared	B
exponential	I
covariance	O
function	B
was	O
used	O
with	O
a	O
separate	O
length-scale	B
parameter	O
for	O
each	O
of	O
the	O
21	O
input	O
dimensions	O
,	O
plus	O
the	O
signal	O
and	O
noise	O
variance	O
n.	O
these	O
parameters	O
were	O
set	B
by	O
optimizing	O
the	O
marginal	B
parameters	O
σ2	O
likelihood	B
eq	O
.	O
(	O
2.30	O
)	O
on	O
a	O
subset	O
of	O
the	O
data	O
(	O
see	B
also	O
chapter	O
5	O
)	O
.	O
f	O
and	O
σ2	O
the	O
results	O
for	O
the	O
various	O
methods	O
are	O
presented	O
in	O
table	O
2.1.	O
notice	O
that	O
the	O
problem	O
is	O
quite	O
non-linear	O
,	O
so	O
the	O
linear	B
regression	I
model	O
does	O
poorly	O
in	O
comparison	O
to	O
non-linear	O
methods.15	O
the	O
non-linear	O
method	O
lwpr	O
improves	O
over	O
linear	B
regression	I
,	O
but	O
is	O
outperformed	O
by	O
gpr	O
.	O
2.6	O
smoothing	O
,	O
weight	O
functions	O
and	O
equiva-	O
lent	O
kernels	O
gaussian	O
process	B
regression	O
aims	O
to	O
reconstruct	O
the	O
underlying	O
signal	O
f	O
by	O
removing	O
the	O
contaminating	O
noise	O
ε.	O
to	O
do	O
this	O
it	O
computes	O
a	O
weighted	O
average	O
of	O
the	O
noisy	O
observations	O
y	O
as	O
¯f	O
(	O
x∗	O
)	O
=	O
k	O
(	O
x∗	O
)	O
>	O
(	O
k	O
+σ2	O
ni	O
)	O
−1y	O
;	O
as	O
¯f	O
(	O
x∗	O
)	O
is	O
a	O
linear	B
combination	O
of	O
the	O
y	O
values	O
,	O
gaussian	O
process	B
regression	O
is	O
a	O
linear	B
smoother	O
(	O
see	B
hastie	O
and	O
tibshirani	O
[	O
1990	O
,	O
sec	O
.	O
2.8	O
]	O
for	O
further	O
details	O
)	O
.	O
in	O
this	O
section	O
we	O
study	O
smoothing	O
ﬁrst	O
in	O
terms	O
of	O
a	O
matrix	B
analysis	O
of	O
the	O
predictions	O
at	O
the	O
training	O
points	O
,	O
and	O
then	O
in	O
terms	O
of	O
the	O
equivalent	B
kernel	I
.	O
15it	O
is	O
perhaps	O
surprising	O
that	O
rbd	O
does	O
worse	O
than	O
linear	B
regression	I
.	O
however	O
,	O
stefan	O
schaal	O
(	O
pers	O
.	O
comm.	O
,	O
2004	O
)	O
states	O
that	O
the	O
rbd	O
parameters	O
were	O
optimized	O
on	O
a	O
very	O
large	O
dataset	B
,	O
of	O
which	O
the	O
training	O
data	O
used	O
here	O
is	O
subset	O
,	O
and	O
if	O
the	O
rbd	O
model	B
were	O
optimized	O
w.r.t	O
.	O
this	O
training	O
set	B
one	O
might	O
well	O
expect	O
it	O
to	O
outperform	O
linear	B
regression	I
.	O
linear	B
smoother	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
2.6	O
smoothing	O
,	O
weight	O
functions	O
and	O
equivalent	B
kernels	O
25	O
the	O
predicted	O
mean	O
values	O
¯f	O
at	O
the	O
training	O
points	O
are	O
given	O
by	O
ni	O
)	O
−1y	O
.	O
¯f	O
=	O
k	O
(	O
k	O
+	O
σ2	O
let	O
k	O
have	O
the	O
eigendecomposition	O
k	O
=	O
pn	O
eigenvectors	O
are	O
mutually	O
orthogonal	O
.	O
let	O
y	O
=pn	O
i	O
,	O
where	O
λi	O
is	O
the	O
ith	O
eigenvalue	B
and	O
ui	O
is	O
the	O
corresponding	O
eigenvector	O
.	O
as	O
k	O
is	O
real	O
and	O
sym-	O
metric	O
positive	B
semideﬁnite	I
,	O
its	O
eigenvalues	O
are	O
real	O
and	O
non-negative	O
,	O
and	O
its	O
i=1γiui	O
for	O
some	O
coeﬃcients	O
γi	O
=	O
u	O
>	O
i=1λiuiu	O
>	O
(	O
2.35	O
)	O
i	O
y.	O
then	O
¯f	O
=	O
γiλi	O
λi	O
+	O
σ2	O
n	O
ui	O
.	O
(	O
2.36	O
)	O
nx	O
i=1	O
n	O
)	O
(	O
cid:28	O
)	O
1	O
then	O
the	O
component	O
in	O
y	O
along	O
ui	O
is	O
eﬀectively	O
notice	O
that	O
if	O
λi/	O
(	O
λi	O
+	O
σ2	O
eliminated	O
.	O
for	O
most	O
covariance	B
functions	O
that	O
are	O
used	O
in	O
practice	O
the	O
eigen-	O
values	O
are	O
larger	O
for	O
more	O
slowly	O
varying	O
eigenvectors	O
(	O
e.g	O
.	O
fewer	O
zero-crossings	O
)	O
so	O
that	O
this	O
means	O
that	O
high-frequency	O
components	O
in	O
y	O
are	O
smoothed	O
out	O
.	O
the	O
eﬀective	O
number	O
of	O
parameters	O
or	O
degrees	B
of	I
freedom	I
of	O
the	O
smoother	O
is	O
n	O
)	O
,	O
see	B
hastie	O
and	O
tibshirani	O
deﬁned	O
as	O
tr	O
(	O
k	O
(	O
k	O
+	O
σ2	O
[	O
1990	O
,	O
sec	O
.	O
3.5	O
]	O
.	O
notice	O
that	O
this	O
counts	O
the	O
number	O
of	O
eigenvectors	O
which	O
are	O
not	O
eliminated	O
.	O
ni	O
)	O
−1	O
)	O
=pn	O
i=1	O
λi/	O
(	O
λi	O
+	O
σ2	O
we	O
can	O
deﬁne	O
a	O
vector	O
of	O
functions	O
h	O
(	O
x∗	O
)	O
=	O
(	O
k	O
+	O
σ2	O
ni	O
)	O
−1k	O
(	O
x∗	O
)	O
.	O
thus	O
we	O
have	O
¯f	O
(	O
x∗	O
)	O
=	O
h	O
(	O
x∗	O
)	O
>	O
y	O
,	O
making	O
it	O
clear	O
that	O
the	O
mean	O
prediction	O
at	O
a	O
point	O
x∗	O
is	O
a	O
linear	B
combination	O
of	O
the	O
target	O
values	O
y.	O
for	O
a	O
ﬁxed	O
test	O
point	O
x∗	O
,	O
h	O
(	O
x∗	O
)	O
gives	O
the	O
vector	O
of	O
weights	O
applied	O
to	O
targets	O
y.	O
h	O
(	O
x∗	O
)	O
is	O
called	O
the	O
weight	B
function	I
[	O
silverman	O
,	O
1984	O
]	O
.	O
as	O
gaussian	O
process	B
regression	O
is	O
a	O
linear	B
smoother	O
,	O
the	O
weight	B
function	I
does	O
not	O
depend	O
on	O
y.	O
note	O
the	O
diﬀerence	O
between	O
a	O
linear	B
model	O
,	O
where	O
the	O
prediction	B
is	O
a	O
linear	B
combination	O
of	O
the	O
inputs	O
,	O
and	O
a	O
linear	B
smoother	O
,	O
where	O
the	O
prediction	B
is	O
a	O
linear	B
combination	O
of	O
the	O
training	O
set	B
targets	O
.	O
understanding	O
the	O
form	O
of	O
the	O
weight	B
function	I
is	O
made	O
complicated	O
by	O
the	O
matrix	B
inversion	O
of	O
k+σ2	O
ni	O
and	O
the	O
fact	O
that	O
k	O
depends	O
on	O
the	O
speciﬁc	O
locations	O
of	O
the	O
n	O
datapoints	O
.	O
idealizing	O
the	O
situation	O
one	O
can	O
consider	O
the	O
observations	O
to	O
be	O
“	O
smeared	O
out	O
”	O
in	O
x-space	O
at	O
some	O
density	O
of	O
observations	O
.	O
in	O
this	O
case	O
analytic	O
tools	O
can	O
be	O
brought	O
to	O
bear	O
on	O
the	O
problem	O
,	O
as	O
shown	O
in	O
section	O
7.1.	O
by	O
analogy	O
to	O
kernel	B
smoothing	O
,	O
silverman	O
[	O
1984	O
]	O
called	O
the	O
idealized	O
weight	B
function	I
the	O
equivalent	B
kernel	I
;	O
see	B
also	O
girosi	O
et	O
al	O
.	O
[	O
1995	O
,	O
sec	O
.	O
2.1	O
]	O
.	O
a	O
kernel	B
smoother	I
centres	O
a	O
kernel	B
function16	O
κ	O
on	O
x∗	O
and	O
then	O
computes	O
κi	O
=	O
κ	O
(	O
|xi	O
−	O
x∗|/	O
‘	O
)	O
for	O
each	O
data	O
point	O
(	O
xi	O
,	O
yi	O
)	O
,	O
where	O
‘	O
is	O
a	O
length-scale	B
.	O
the	O
gaussian	O
is	O
a	O
commonly	O
used	O
kernel	B
function	O
.	O
the	O
prediction	B
for	O
f	O
(	O
x∗	O
)	O
is	O
j=1	O
κj	O
.	O
this	O
is	O
also	O
known	O
i=1wiyi	O
where	O
wi	O
=	O
κi/pn	O
computed	O
as	O
ˆf	O
(	O
x∗	O
)	O
=pn	O
as	O
the	O
nadaraya-watson	O
estimator	O
,	O
see	B
e.g	O
.	O
scott	O
[	O
1992	O
,	O
sec	O
.	O
8.1	O
]	O
.	O
the	O
weight	B
function	I
and	O
equivalent	B
kernel	I
for	O
a	O
gaussian	O
process	B
are	O
illus-	O
trated	O
in	O
figure	O
2.6	O
for	O
a	O
one-dimensional	O
input	O
variable	O
x.	O
we	O
have	O
used	O
the	O
squared	B
exponential	I
covariance	O
function	B
and	O
have	O
set	B
the	O
length-scale	B
‘	O
=	O
0.0632	O
(	O
so	O
that	O
‘	O
2	O
=	O
0.004	O
)	O
.	O
there	O
are	O
n	O
=	O
50	O
training	O
points	O
spaced	O
randomly	O
along	O
16note	O
that	O
this	O
kernel	B
function	O
does	O
not	O
need	O
to	O
be	O
a	O
valid	O
covariance	B
function	I
.	O
eigendecomposition	O
degrees	B
of	I
freedom	I
weight	O
function	B
equivalent	O
kernel	B
kernel	O
smoother	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
26	O
regression	B
(	O
a	O
)	O
(	O
c	O
)	O
(	O
b	O
)	O
(	O
d	O
)	O
figure	O
2.6	O
:	O
panels	O
(	O
a	O
)	O
-	O
(	O
c	O
)	O
show	O
the	O
weight	B
function	I
h	O
(	O
x∗	O
)	O
(	O
dots	O
)	O
corresponding	O
to	O
the	O
n	O
=	O
50	O
training	O
points	O
,	O
the	O
equivalent	B
kernel	I
(	O
solid	O
)	O
and	O
the	O
original	O
squared	B
exponential	I
kernel	O
(	O
dashed	O
)	O
.	O
panel	O
(	O
d	O
)	O
shows	O
the	O
equivalent	B
kernels	O
for	O
two	O
diﬀerent	O
data	O
densities	O
.	O
see	B
text	O
for	O
further	O
details	O
.	O
the	O
small	O
cross	O
at	O
the	O
test	O
point	O
is	O
to	O
scale	O
in	O
all	O
four	O
plots	O
.	O
the	O
x-axis	O
.	O
figures	O
2.6	O
(	O
a	O
)	O
and	O
2.6	O
(	O
b	O
)	O
show	O
the	O
weight	B
function	I
and	O
equivalent	B
kernel	I
for	O
x∗	O
=	O
0.5	O
and	O
x∗	O
=	O
0.05	O
respectively	O
,	O
for	O
σ2	O
n	O
=	O
0.1.	O
figure	O
2.6	O
(	O
c	O
)	O
is	O
also	O
for	O
x∗	O
=	O
0.5	O
but	O
uses	O
σ2	O
n	O
=	O
10.	O
in	O
each	O
case	O
the	O
dots	O
correspond	O
to	O
the	O
weight	B
function	I
h	O
(	O
x∗	O
)	O
and	O
the	O
solid	O
line	O
is	O
the	O
equivalent	B
kernel	I
,	O
whose	O
construction	O
is	O
explained	O
below	O
.	O
the	O
dashed	O
line	O
shows	O
a	O
squared	B
exponential	I
kernel	O
centered	O
on	O
the	O
test	O
point	O
,	O
scaled	O
to	O
have	O
the	O
same	O
height	O
as	O
the	O
maximum	O
value	O
in	O
the	O
equivalent	B
kernel	I
.	O
figure	O
2.6	O
(	O
d	O
)	O
shows	O
the	O
variation	O
in	O
the	O
equivalent	B
kernel	I
as	O
a	O
function	B
of	O
n	O
,	O
the	O
number	O
of	O
datapoints	O
in	O
the	O
unit	O
interval	O
.	O
many	O
interesting	O
observations	O
can	O
be	O
made	O
from	O
these	O
plots	O
.	O
observe	O
that	O
the	O
equivalent	B
kernel	I
has	O
(	O
in	O
general	O
)	O
a	O
shape	O
quite	O
diﬀerent	O
to	O
the	O
original	O
se	O
kernel	B
.	O
in	O
figure	O
2.6	O
(	O
a	O
)	O
the	O
equivalent	B
kernel	I
is	O
clearly	O
oscillatory	O
(	O
with	O
negative	O
sidelobes	O
)	O
and	O
has	O
a	O
higher	O
spatial	O
frequency	O
than	O
the	O
original	O
kernel	B
.	O
figure	O
2.6	O
(	O
b	O
)	O
shows	O
similar	O
behaviour	O
although	O
due	O
to	O
edge	O
eﬀects	O
the	O
equivalent	B
kernel	I
is	O
truncated	O
relative	O
to	O
that	O
in	O
figure	O
2.6	O
(	O
a	O
)	O
.	O
in	O
figure	O
2.6	O
(	O
c	O
)	O
we	O
see	B
that	O
at	O
higher	O
noise	O
levels	O
the	O
negative	O
sidelobes	O
are	O
reduced	O
and	O
the	O
width	O
of	O
the	O
equivalent	B
kernel	I
is	O
similar	O
to	O
the	O
original	O
kernel	B
.	O
also	O
note	O
that	O
the	O
overall	O
height	O
of	O
the	O
equivalent	B
kernel	I
in	O
(	O
c	O
)	O
is	O
reduced	O
compared	O
to	O
that	O
in	O
(	O
a	O
)	O
and	O
00.20.40.60.8100.200.20.40.60.8100.200.20.40.60.8100.0500.20.40.60.8100.110250	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
2.7	O
incorporating	O
explicit	O
basis	O
functions	O
27	O
(	O
b	O
)	O
—it	O
averages	O
over	O
a	O
wider	O
area	O
.	O
the	O
more	O
oscillatory	O
equivalent	B
kernel	I
for	O
lower	O
noise	O
levels	O
can	O
be	O
understood	O
in	O
terms	O
of	O
the	O
eigenanalysis	O
above	O
;	O
at	O
higher	O
noise	O
levels	O
only	O
the	O
large	O
λ	O
(	O
slowly	O
varying	O
)	O
components	O
of	O
y	O
remain	O
,	O
while	O
for	O
smaller	O
noise	O
levels	O
the	O
more	O
oscillatory	O
components	O
are	O
also	O
retained	O
.	O
in	O
figure	O
2.6	O
(	O
d	O
)	O
we	O
have	O
plotted	O
the	O
equivalent	B
kernel	I
for	O
n	O
=	O
10	O
and	O
n	O
=	O
250	O
datapoints	O
in	O
[	O
0	O
,	O
1	O
]	O
;	O
notice	O
how	O
the	O
width	O
of	O
the	O
equivalent	B
kernel	I
decreases	O
as	O
n	O
increases	O
.	O
we	O
discuss	O
this	O
behaviour	O
further	O
in	O
section	O
7.1.	O
the	O
plots	O
of	O
equivalent	B
kernels	O
in	O
figure	O
2.6	O
were	O
made	O
by	O
using	O
a	O
dense	O
grid	O
of	O
ngrid	O
points	O
on	O
[	O
0	O
,	O
1	O
]	O
and	O
then	O
computing	O
the	O
smoother	O
matrix	B
k	O
(	O
k	O
+	O
gridi	O
)	O
−1	O
.	O
each	O
row	O
of	O
this	O
matrix	B
is	O
the	O
equivalent	B
kernel	I
at	O
the	O
appropriate	O
σ2	O
location	O
.	O
however	O
,	O
in	O
order	O
to	O
get	O
the	O
scaling	O
right	O
one	O
has	O
to	O
set	B
σ2	O
grid	O
=	O
nngrid/n	O
;	O
for	O
ngrid	O
>	O
n	O
this	O
means	O
that	O
the	O
eﬀective	O
variance	O
at	O
each	O
of	O
the	O
σ2	O
ngrid	O
points	O
is	O
larger	O
,	O
but	O
as	O
there	O
are	O
correspondingly	O
more	O
points	O
this	O
eﬀect	O
cancels	O
out	O
.	O
this	O
can	O
be	O
understood	O
by	O
imagining	O
the	O
situation	O
if	O
there	O
were	O
ngrid/n	O
independent	O
gaussian	O
observations	O
with	O
variance	O
σ2	O
grid	O
at	O
a	O
single	O
x-	O
position	O
;	O
this	O
would	O
be	O
equivalent	B
to	O
one	O
gaussian	O
observation	O
with	O
variance	O
n.	O
in	O
eﬀect	O
the	O
n	O
observations	O
have	O
been	O
smoothed	O
out	O
uniformly	O
along	O
the	O
σ2	O
interval	O
.	O
the	O
form	O
of	O
the	O
equivalent	B
kernel	I
can	O
be	O
obtained	O
analytically	O
if	O
we	O
go	O
to	O
the	O
continuum	O
limit	O
and	O
look	O
to	O
smooth	O
a	O
noisy	O
function	B
.	O
the	O
relevant	O
theory	O
and	O
some	O
example	O
equivalent	B
kernels	O
are	O
given	O
in	O
section	O
7.1	O
.	O
2.7	O
incorporating	O
explicit	O
basis	O
functions	O
∗	O
it	O
is	O
common	O
but	O
by	O
no	O
means	O
necessary	O
to	O
consider	O
gps	O
with	O
a	O
zero	O
mean	O
func-	O
tion	O
.	O
note	O
that	O
this	O
is	O
not	O
necessarily	O
a	O
drastic	O
limitation	O
,	O
since	O
the	O
mean	O
of	O
the	O
posterior	B
process	I
is	O
not	O
conﬁned	O
to	O
be	O
zero	O
.	O
yet	O
there	O
are	O
several	O
reasons	O
why	O
one	O
might	O
wish	O
to	O
explicitly	O
model	B
a	O
mean	B
function	I
,	O
including	O
interpretability	O
of	O
the	O
model	B
,	O
convenience	O
of	O
expressing	O
prior	O
information	O
and	O
a	O
number	O
of	O
an-	O
alytical	O
limits	O
which	O
we	O
will	O
need	O
in	O
subsequent	O
chapters	O
.	O
the	O
use	O
of	O
explicit	O
basis	O
functions	O
is	O
a	O
way	O
to	O
specify	O
a	O
non-zero	O
mean	O
over	O
functions	O
,	O
but	O
as	O
we	O
will	O
see	B
in	O
this	O
section	O
,	O
one	O
can	O
also	O
use	O
them	O
to	O
achieve	O
other	O
interesting	O
eﬀects	O
.	O
using	O
a	O
ﬁxed	O
(	O
deterministic	O
)	O
mean	B
function	I
m	O
(	O
x	O
)	O
is	O
trivial	O
:	O
simply	O
apply	O
the	O
usual	O
zero	O
mean	O
gp	O
to	O
the	O
diﬀerence	O
between	O
the	O
observations	O
and	O
the	O
ﬁxed	O
mean	B
function	I
.	O
with	O
f	O
(	O
x	O
)	O
∼	O
gp	O
(	O
cid:0	O
)	O
m	O
(	O
x	O
)	O
,	O
k	O
(	O
x	O
,	O
x0	O
)	O
(	O
cid:1	O
)	O
,	O
the	O
predictive	B
mean	O
becomes	O
¯f∗	O
=	O
m	O
(	O
x∗	O
)	O
+	O
k	O
(	O
x∗	O
,	O
x	O
)	O
k−1	O
y	O
(	O
y	O
−	O
m	O
(	O
x	O
)	O
)	O
,	O
where	O
ky	O
=	O
k	O
+	O
σ2	O
eq	O
.	O
(	O
2.24	O
)	O
.	O
ni	O
,	O
and	O
the	O
predictive	B
variance	O
remains	O
unchanged	O
from	O
however	O
,	O
in	O
practice	O
it	O
can	O
often	O
be	O
diﬃcult	O
to	O
specify	O
a	O
ﬁxed	O
mean	B
function	I
.	O
in	O
many	O
cases	O
it	O
may	O
be	O
more	O
convenient	O
to	O
specify	O
a	O
few	O
ﬁxed	O
basis	O
functions	O
,	O
ﬁxed	O
mean	B
function	I
(	O
2.37	O
)	O
(	O
2.38	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
28	O
stochastic	O
mean	O
function	B
polynomial	O
regression	B
whose	O
coeﬃcients	O
,	O
β	O
,	O
are	O
to	O
be	O
inferred	O
from	O
the	O
data	O
.	O
consider	O
g	O
(	O
x	O
)	O
=	O
f	O
(	O
x	O
)	O
+	O
h	O
(	O
x	O
)	O
>	O
β	O
,	O
where	O
f	O
(	O
x	O
)	O
∼	O
gp	O
(	O
cid:0	O
)	O
0	O
,	O
k	O
(	O
x	O
,	O
x0	O
)	O
(	O
cid:1	O
)	O
,	O
(	O
2.39	O
)	O
regression	B
here	O
f	O
(	O
x	O
)	O
is	O
a	O
zero	O
mean	O
gp	O
,	O
h	O
(	O
x	O
)	O
are	O
a	O
set	B
of	O
ﬁxed	O
basis	O
functions	O
,	O
and	O
β	O
are	O
additional	O
parameters	O
.	O
this	O
formulation	O
expresses	O
that	O
the	O
data	O
is	O
close	O
to	O
a	O
global	O
linear	B
model	O
with	O
the	O
residuals	O
being	O
modelled	O
by	O
a	O
gp	O
.	O
this	O
idea	O
was	O
explored	O
explicitly	O
as	O
early	O
as	O
1975	O
by	O
blight	O
and	O
ott	O
[	O
1975	O
]	O
,	O
who	O
used	O
the	O
gp	O
to	O
model	B
the	O
residuals	O
from	O
a	O
polynomial	B
regression	O
,	O
i.e	O
.	O
h	O
(	O
x	O
)	O
=	O
(	O
1	O
,	O
x	O
,	O
x2	O
,	O
.	O
.	O
.	O
)	O
.	O
when	O
ﬁtting	O
the	O
model	B
,	O
one	O
could	O
optimize	O
over	O
the	O
parameters	O
β	O
jointly	O
with	O
the	O
hyperparameters	B
of	O
the	O
covariance	B
function	I
.	O
alternatively	O
,	O
if	O
we	O
take	O
the	O
prior	O
on	O
β	O
to	O
be	O
gaussian	O
,	O
β	O
∼	O
n	O
(	O
b	O
,	O
b	O
)	O
,	O
we	O
can	O
also	O
integrate	O
out	O
these	O
parameters	O
.	O
following	O
o	O
’	O
hagan	O
[	O
1978	O
]	O
we	O
obtain	O
another	O
gp	O
g	O
(	O
x	O
)	O
∼	O
gp	O
(	O
cid:0	O
)	O
h	O
(	O
x	O
)	O
>	O
b	O
,	O
k	O
(	O
x	O
,	O
x0	O
)	O
+	O
h	O
(	O
x	O
)	O
>	O
bh	O
(	O
x0	O
)	O
(	O
cid:1	O
)	O
,	O
(	O
2.40	O
)	O
now	O
with	O
an	O
added	O
contribution	O
in	O
the	O
covariance	B
function	I
caused	O
by	O
the	O
un-	O
certainty	O
in	O
the	O
parameters	O
of	O
the	O
mean	O
.	O
predictions	O
are	O
made	O
by	O
plugging	O
the	O
mean	O
and	O
covariance	B
functions	O
of	O
g	O
(	O
x	O
)	O
into	O
eq	O
.	O
(	O
2.39	O
)	O
and	O
eq	O
.	O
(	O
2.24	O
)	O
.	O
after	O
rearranging	O
,	O
we	O
obtain	O
¯g	O
(	O
x∗	O
)	O
=	O
h	O
>	O
cov	O
(	O
g∗	O
)	O
=	O
cov	O
(	O
f∗	O
)	O
+	O
r	O
>	O
(	O
b−1	O
+	O
hk−1	O
y	O
(	O
y	O
−	O
h	O
>	O
¯β	O
)	O
=	O
¯f	O
(	O
x∗	O
)	O
+	O
r	O
>	O
¯β	O
,	O
∗	O
¯β	O
+	O
k	O
>	O
∗	O
k−1	O
y	O
h	O
>	O
)	O
−1r	O
,	O
(	O
2.41	O
)	O
y	O
h	O
>	O
)	O
−1	O
(	O
hk−1	O
where	O
the	O
h	O
matrix	B
collects	O
the	O
h	O
(	O
x	O
)	O
vectors	O
for	O
all	O
training	O
(	O
and	O
h∗	O
all	O
test	O
)	O
cases	O
,	O
¯β	O
=	O
(	O
b−1	O
+	O
hk−1	O
y	O
k∗	O
.	O
notice	O
the	O
nice	O
interpretation	O
of	O
the	O
mean	O
expression	O
,	O
eq	O
.	O
(	O
2.41	O
)	O
top	O
line	O
:	O
¯β	O
is	O
the	O
mean	O
of	O
the	O
global	O
linear	B
model	O
parameters	O
,	O
being	O
a	O
compromise	O
between	O
the	O
data	O
term	O
and	O
prior	O
,	O
and	O
the	O
predictive	B
mean	O
is	O
simply	O
the	O
mean	O
linear	O
output	O
plus	O
what	O
the	O
gp	O
model	B
predicts	O
from	O
the	O
residuals	O
.	O
the	O
covariance	B
is	O
the	O
sum	O
of	O
the	O
usual	O
covariance	B
term	O
and	O
a	O
new	O
non-negative	O
contribution	O
.	O
y	O
y	O
+	O
b−1b	O
)	O
,	O
and	O
r	O
=	O
h∗	O
−	O
hk−1	O
exploring	O
the	O
limit	O
of	O
the	O
above	O
expressions	O
as	O
the	O
prior	O
on	O
the	O
β	O
param-	O
eter	O
becomes	O
vague	O
,	O
b−1	O
→	O
o	O
(	O
where	O
o	O
is	O
the	O
matrix	B
of	O
zeros	O
)	O
,	O
we	O
obtain	O
a	O
predictive	B
distribution	O
which	O
is	O
independent	O
of	O
b	O
¯g	O
(	O
x∗	O
)	O
=	O
¯f	O
(	O
x∗	O
)	O
+	O
r	O
>	O
¯β	O
,	O
cov	O
(	O
g∗	O
)	O
=	O
cov	O
(	O
f∗	O
)	O
+	O
r	O
>	O
(	O
hk−1	O
y	O
h	O
>	O
)	O
−1r	O
,	O
(	O
2.42	O
)	O
y	O
h	O
>	O
)	O
−1hk−1	O
where	O
the	O
limiting	O
¯β	O
=	O
(	O
hk−1	O
y	O
y.	O
notice	O
that	O
predictions	O
under	O
the	O
limit	O
b−1	O
→	O
o	O
should	O
not	O
be	O
implemented	O
na¨ıvely	O
by	O
plugging	O
the	O
modiﬁed	O
covariance	B
function	I
from	O
eq	O
.	O
(	O
2.40	O
)	O
into	O
the	O
standard	O
prediction	B
equations	O
,	O
since	O
the	O
entries	O
of	O
the	O
covariance	B
function	I
tend	O
to	O
inﬁnity	O
,	O
thus	O
making	O
it	O
unsuitable	O
for	O
numerical	O
implementation	O
.	O
instead	O
eq	O
.	O
(	O
2.42	O
)	O
must	O
be	O
used	O
.	O
even	O
if	O
the	O
non-limiting	O
case	O
is	O
of	O
interest	O
,	O
eq	O
.	O
(	O
2.41	O
)	O
is	O
numerically	O
preferable	O
to	O
a	O
direct	O
implementation	O
based	O
on	O
eq	O
.	O
(	O
2.40	O
)	O
,	O
since	O
the	O
global	O
linear	B
part	O
will	O
often	O
add	O
some	O
very	O
large	O
eigenvalues	O
to	O
the	O
covariance	B
matrix	I
,	O
aﬀecting	O
its	O
condition	O
number	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
2.8	O
history	O
and	O
related	O
work	O
2.7.1	O
marginal	B
likelihood	I
in	O
this	O
short	O
section	O
we	O
brieﬂy	O
discuss	O
the	O
marginal	B
likelihood	I
for	O
the	O
model	B
with	O
a	O
gaussian	O
prior	O
β	O
∼	O
n	O
(	O
b	O
,	O
b	O
)	O
on	O
the	O
explicit	O
parameters	O
from	O
eq	O
.	O
(	O
2.40	O
)	O
,	O
as	O
this	O
will	O
be	O
useful	O
later	O
,	O
particularly	O
in	O
section	O
6.3.1.	O
we	O
can	O
express	O
the	O
marginal	B
likelihood	I
from	O
eq	O
.	O
(	O
2.30	O
)	O
as	O
29	O
log	O
p	O
(	O
y|x	O
,	O
b	O
,	O
b	O
)	O
=	O
−	O
1	O
−	O
1	O
2	O
(	O
h	O
>	O
b	O
−	O
y	O
)	O
>	O
(	O
ky	O
+	O
h	O
>	O
bh	O
)	O
−1	O
(	O
h	O
>	O
b	O
−	O
y	O
)	O
2	O
log	O
|ky	O
+	O
h	O
>	O
bh|	O
−	O
n	O
2	O
log	O
2π	O
,	O
(	O
2.43	O
)	O
where	O
we	O
have	O
included	O
the	O
explicit	O
mean	O
.	O
we	O
are	O
interested	O
in	O
exploring	O
the	O
limit	O
where	O
b−1	O
→	O
o	O
,	O
i.e	O
.	O
when	O
the	O
prior	O
is	O
vague	O
.	O
in	O
this	O
limit	O
the	O
mean	O
of	O
the	O
prior	O
is	O
irrelevant	O
(	O
as	O
was	O
the	O
case	O
in	O
eq	O
.	O
(	O
2.42	O
)	O
)	O
,	O
so	O
without	O
loss	B
of	O
generality	O
(	O
for	O
the	O
limiting	O
case	O
)	O
we	O
assume	O
for	O
now	O
that	O
the	O
mean	O
is	O
zero	O
,	O
b	O
=	O
0	O
,	O
giving	O
log	O
p	O
(	O
y|x	O
,	O
b=0	O
,	O
b	O
)	O
=	O
−	O
1	O
−	O
1	O
y	O
h	O
>	O
and	O
c	O
=	O
k−1	O
2	O
y	O
>	O
k−1	O
y	O
y	O
+	O
1	O
2	O
log	O
|ky|	O
−	O
1	O
where	O
a	O
=	O
b−1	O
+	O
hk−1	O
the	O
matrix	B
inversion	O
lemma	O
,	O
eq	O
.	O
(	O
a.9	O
)	O
and	O
eq	O
.	O
(	O
a.10	O
)	O
.	O
2	O
y	O
>	O
cy	O
2	O
log	O
|b|	O
−	O
1	O
y	O
h	O
>	O
a−1hk−1	O
y	O
and	O
we	O
have	O
used	O
2	O
log	O
|a|	O
−	O
n	O
2	O
log	O
2π	O
,	O
(	O
2.44	O
)	O
we	O
now	O
explore	O
the	O
behaviour	O
of	O
the	O
log	O
marginal	O
likelihood	B
in	O
the	O
limit	O
of	O
vague	O
priors	O
on	O
β.	O
in	O
this	O
limit	O
the	O
variances	O
of	O
the	O
gaussian	O
in	O
the	O
directions	O
spanned	O
by	O
columns	O
of	O
h	O
>	O
will	O
become	O
inﬁnite	O
,	O
and	O
it	O
is	O
clear	O
that	O
this	O
will	O
require	O
special	O
treatment	O
.	O
the	O
log	O
marginal	O
likelihood	B
consists	O
of	O
three	O
terms	O
:	O
a	O
quadratic	B
form	I
in	O
y	O
,	O
a	O
log	O
determinant	O
term	O
,	O
and	O
a	O
term	O
involving	O
log	O
2π	O
.	O
performing	O
an	O
eigendecomposition	O
of	O
the	O
covariance	B
matrix	I
we	O
see	B
that	O
the	O
contributions	O
to	O
quadratic	B
form	I
term	O
from	O
the	O
inﬁnite-variance	O
directions	O
will	O
be	O
zero	O
.	O
however	O
,	O
the	O
log	O
determinant	O
term	O
will	O
tend	O
to	O
minus	O
inﬁnity	O
.	O
the	O
standard	O
solution	O
[	O
wahba	O
,	O
1985	O
,	O
ansley	O
and	O
kohn	O
,	O
1985	O
]	O
in	O
this	O
case	O
is	O
to	O
project	O
y	O
onto	O
the	O
directions	O
orthogonal	O
to	O
the	O
span	O
of	O
h	O
>	O
and	O
compute	O
the	O
marginal	B
likelihood	I
in	O
this	O
subspace	O
.	O
let	O
the	O
rank	O
of	O
h	O
>	O
be	O
m.	O
then	O
as	O
shown	O
in	O
ansley	O
and	O
kohn	O
[	O
1985	O
]	O
this	O
means	O
that	O
we	O
must	O
discard	O
the	O
terms	O
−	O
1	O
2	O
log	O
|b|	O
−	O
m	O
log	O
p	O
(	O
y|x	O
)	O
=	O
−	O
1	O
where	O
a	O
=	O
hk−1	O
2	O
log	O
2π	O
from	O
eq	O
.	O
(	O
2.44	O
)	O
to	O
give	O
2	O
y	O
>	O
cy	O
−	O
1	O
y	O
h	O
>	O
a−1hk−1	O
y	O
.	O
y	O
h	O
>	O
and	O
c	O
=	O
k−1	O
2	O
log	O
|a|	O
−	O
n−m	O
2	O
log	O
|ky|	O
−	O
1	O
2	O
y	O
>	O
k−1	O
y	O
y	O
+	O
1	O
log	O
2π	O
,	O
2	O
(	O
2.45	O
)	O
2.8	O
history	O
and	O
related	O
work	O
prediction	B
with	O
gaussian	O
processes	O
is	O
certainly	O
not	O
a	O
very	O
recent	O
topic	O
,	O
espe-	O
cially	O
for	O
time	O
series	O
analysis	O
;	O
the	O
basic	O
theory	O
goes	O
back	O
at	O
least	O
as	O
far	O
as	O
the	O
work	O
of	O
wiener	O
[	O
1949	O
]	O
and	O
kolmogorov	O
[	O
1941	O
]	O
in	O
the	O
1940	O
’	O
s	O
.	O
indeed	O
lauritzen	O
[	O
1981	O
]	O
discusses	O
relevant	O
work	O
by	O
the	O
danish	O
astronomer	O
t.	O
n.	O
thiele	O
dating	O
from	O
1880.	O
time	O
series	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
30	O
geostatistics	B
kriging	O
computer	O
experiments	O
machine	O
learning	B
regression	O
gaussian	O
process	B
prediction	O
is	O
also	O
well	O
known	O
in	O
the	O
geostatistics	B
ﬁeld	O
(	O
see	B
,	O
e.g	O
.	O
matheron	O
,	O
1973	O
;	O
journel	O
and	O
huijbregts	O
,	O
1978	O
)	O
where	O
it	O
is	O
known	O
as	O
krig-	O
ing,17	O
and	O
in	O
meteorology	O
[	O
thompson	O
,	O
1956	O
,	O
daley	O
,	O
1991	O
]	O
although	O
this	O
litera-	O
ture	O
naturally	O
has	O
focussed	O
mostly	O
on	O
two-	O
and	O
three-dimensional	O
input	O
spaces	O
.	O
whittle	O
[	O
1963	O
,	O
sec	O
.	O
5.4	O
]	O
also	O
suggests	O
the	O
use	O
of	O
such	O
methods	O
for	O
spatial	O
pre-	O
diction	O
.	O
ripley	O
[	O
1981	O
]	O
and	O
cressie	O
[	O
1993	O
]	O
provide	O
useful	O
overviews	O
of	O
gaussian	O
process	B
prediction	O
in	O
spatial	O
statistics	O
.	O
gradually	O
it	O
was	O
realized	O
that	O
gaussian	O
process	B
prediction	O
could	O
be	O
used	O
in	O
a	O
general	O
regression	B
context	O
.	O
for	O
example	O
o	O
’	O
hagan	O
[	O
1978	O
]	O
presents	O
the	O
general	O
theory	O
as	O
given	O
in	O
our	O
equations	O
2.23	O
and	O
2.24	O
,	O
and	O
applies	O
it	O
to	O
a	O
number	O
of	O
one-dimensional	O
regression	B
problems	O
.	O
sacks	O
et	O
al	O
.	O
[	O
1989	O
]	O
describe	O
gpr	O
in	O
the	O
context	O
of	O
computer	O
experiments	O
(	O
where	O
the	O
observations	O
y	O
are	O
noise	O
free	O
)	O
and	O
discuss	O
a	O
number	O
of	O
interesting	O
directions	O
such	O
as	O
the	O
optimization	O
of	O
parameters	O
in	O
the	O
covariance	B
function	I
(	O
see	B
our	O
chapter	O
5	O
)	O
and	O
experimental	B
design	I
(	O
i.e	O
.	O
the	O
choice	O
of	O
x-points	O
that	O
provide	O
most	O
information	O
on	O
f	O
)	O
.	O
the	O
authors	O
describe	O
a	O
number	O
of	O
computer	O
simulations	O
that	O
were	O
modelled	O
,	O
including	O
an	O
example	O
where	O
the	O
response	O
variable	O
was	O
the	O
clock	O
asynchronization	O
in	O
a	O
circuit	O
and	O
the	O
inputs	O
were	O
six	O
transistor	O
widths	O
.	O
santner	O
et	O
al	O
.	O
[	O
2003	O
]	O
is	O
a	O
recent	O
book	O
on	O
the	O
use	O
of	O
gps	O
for	O
the	O
design	O
and	O
analysis	O
of	O
computer	O
experiments	O
.	O
williams	O
and	O
rasmussen	O
[	O
1996	O
]	O
described	O
gaussian	O
process	B
regression	O
in	O
a	O
machine	O
learning	B
context	O
,	O
and	O
described	O
optimization	O
of	O
the	O
parameters	O
in	O
the	O
covariance	B
function	I
,	O
see	B
also	O
rasmussen	O
[	O
1996	O
]	O
.	O
they	O
were	O
inspired	O
to	O
use	O
gaussian	O
process	B
by	O
the	O
connection	O
to	O
inﬁnite	O
neural	O
networks	O
as	O
described	O
in	O
section	O
4.2.3	O
and	O
in	O
neal	O
[	O
1996	O
]	O
.	O
the	O
“	O
kernelization	O
”	O
of	O
linear	B
ridge	O
regression	B
described	O
above	O
is	O
also	O
known	O
as	O
kernel	B
ridge	I
regression	I
see	O
e.g	O
.	O
saunders	O
et	O
al	O
.	O
[	O
1998	O
]	O
.	O
relationships	O
between	O
gaussian	O
process	B
prediction	O
and	O
regularization	B
the-	O
ory	O
,	O
splines	B
,	O
support	B
vector	I
machines	O
(	O
svms	O
)	O
and	O
relevance	O
vector	O
machines	O
(	O
rvms	O
)	O
are	O
discussed	O
in	O
chapter	O
6	O
.	O
2.9	O
exercises	O
1.	O
replicate	O
the	O
generation	O
of	O
random	O
functions	O
from	O
figure	O
2.2.	O
use	O
a	O
regular	O
(	O
or	O
random	O
)	O
grid	O
of	O
scalar	O
inputs	O
and	O
the	O
covariance	B
function	I
from	O
eq	O
.	O
(	O
2.16	O
)	O
.	O
hints	O
on	O
how	O
to	O
generate	O
random	O
samples	O
from	O
multi-variate	O
gaussian	O
distributions	O
are	O
given	O
in	O
section	O
a.2	O
.	O
invent	O
some	O
training	O
data	O
points	O
,	O
and	O
make	O
random	O
draws	O
from	O
the	O
resulting	O
gp	O
posterior	O
using	O
eq	O
.	O
(	O
2.19	O
)	O
.	O
2.	O
in	O
eq	O
.	O
(	O
2.11	O
)	O
we	O
saw	O
that	O
the	O
predictive	B
variance	O
at	O
x∗	O
under	O
the	O
feature	B
space	I
regression	O
model	B
was	O
var	O
(	O
f	O
(	O
x∗	O
)	O
)	O
=	O
φ	O
(	O
x∗	O
)	O
>	O
a−1φ	O
(	O
x∗	O
)	O
.	O
show	O
that	O
cov	O
(	O
f	O
(	O
x∗	O
)	O
,	O
f	O
(	O
x0	O
∗	O
)	O
.	O
check	O
that	O
this	O
is	O
compatible	O
with	O
the	O
expression	O
given	O
in	O
eq	O
.	O
(	O
2.24	O
)	O
.	O
∗	O
)	O
)	O
=	O
φ	O
(	O
x∗	O
)	O
>	O
a−1φ	O
(	O
x0	O
17matheron	O
named	O
the	O
method	O
after	O
the	O
south	O
african	O
mining	O
engineer	O
d.	O
g.	O
krige	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
2.9	O
exercises	O
31	O
3.	O
the	O
wiener	O
process	B
is	O
deﬁned	O
for	O
x	O
≥	O
0	O
and	O
has	O
f	O
(	O
0	O
)	O
=	O
0	O
.	O
(	O
see	B
sec-	O
tion	O
b.2.1	O
for	O
further	O
details	O
.	O
)	O
it	O
has	O
mean	O
zero	O
and	O
a	O
non-stationary	O
covariance	B
function	I
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
min	O
(	O
x	O
,	O
x0	O
)	O
.	O
if	O
we	O
condition	O
on	O
the	O
wiener	O
process	B
passing	O
through	O
f	O
(	O
1	O
)	O
=	O
0	O
we	O
obtain	O
a	O
process	B
known	O
as	O
the	O
brow-	O
nian	O
bridge	O
(	O
or	O
tied-down	B
wiener	O
process	B
)	O
.	O
show	O
that	O
this	O
process	B
has	O
covariance	B
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
min	O
(	O
x	O
,	O
x0	O
)	O
−	O
xx0	O
for	O
0	O
≤	O
x	O
,	O
x0	O
≤	O
1	O
and	O
mean	O
0.	O
write	O
a	O
computer	O
program	O
to	O
draw	O
samples	O
from	O
this	O
process	B
at	O
a	O
ﬁnite	O
grid	O
of	O
x	O
points	O
in	O
[	O
0	O
,	O
1	O
]	O
.	O
4.	O
let	O
varn	O
(	O
f	O
(	O
x∗	O
)	O
)	O
be	O
the	O
predictive	B
variance	O
of	O
a	O
gaussian	O
process	B
regres-	O
sion	O
model	B
at	O
x∗	O
given	O
a	O
dataset	B
of	O
size	O
n.	O
the	O
corresponding	O
predictive	B
variance	O
using	O
a	O
dataset	B
of	O
only	O
the	O
ﬁrst	O
n	O
−	O
1	O
training	O
points	O
is	O
de-	O
noted	O
varn−1	O
(	O
f	O
(	O
x∗	O
)	O
)	O
.	O
show	O
that	O
varn	O
(	O
f	O
(	O
x∗	O
)	O
)	O
≤	O
varn−1	O
(	O
f	O
(	O
x∗	O
)	O
)	O
,	O
i.e	O
.	O
that	O
the	O
predictive	B
variance	O
at	O
x∗	O
can	O
not	O
increase	O
as	O
more	O
training	O
data	O
is	O
ob-	O
tained	O
.	O
one	O
way	O
to	O
approach	O
this	O
problem	O
is	O
to	O
use	O
the	O
partitioned	O
matrix	O
equations	O
given	O
in	O
section	O
a.3	O
to	O
decompose	O
varn	O
(	O
f	O
(	O
x∗	O
)	O
)	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
k	O
>	O
ni	O
)	O
−1k∗	O
.	O
an	O
alternative	O
information	O
theoretic	O
argument	O
is	O
given	O
∗	O
(	O
k	O
+σ2	O
in	O
williams	O
and	O
vivarelli	O
[	O
2000	O
]	O
.	O
note	O
that	O
while	O
this	O
conclusion	O
is	O
true	O
for	O
gaussian	O
process	B
priors	O
and	O
gaussian	O
noise	O
models	O
it	O
does	O
not	O
hold	O
generally	O
,	O
see	B
barber	O
and	O
saad	O
[	O
1996	O
]	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
chapter	O
3	O
classiﬁcation	B
in	O
chapter	O
2	O
we	O
have	O
considered	O
regression	B
problems	O
,	O
where	O
the	O
targets	O
are	O
real	O
valued	O
.	O
another	O
important	O
class	O
of	O
problems	O
is	O
classiﬁcation	B
1	O
problems	O
,	O
where	O
we	O
wish	O
to	O
assign	O
an	O
input	O
pattern	O
x	O
to	O
one	O
of	O
c	O
classes	O
,	O
c1	O
,	O
.	O
.	O
.	O
,	O
cc	O
.	O
practical	O
examples	O
of	O
classiﬁcation	B
problems	O
are	O
handwritten	O
digit	O
recognition	O
(	O
where	O
we	O
wish	O
to	O
classify	O
a	O
digitized	O
image	O
of	O
a	O
handwritten	O
digit	O
into	O
one	O
of	O
ten	O
classes	O
0-9	O
)	O
,	O
and	O
the	O
classiﬁcation	B
of	O
objects	O
detected	O
in	O
astronomical	O
sky	O
surveys	O
into	O
stars	O
or	O
galaxies	O
.	O
(	O
information	O
on	O
the	O
distribution	O
of	O
galaxies	O
in	O
the	O
universe	O
is	O
important	O
for	O
theories	O
of	O
the	O
early	O
universe	O
.	O
)	O
these	O
examples	O
nicely	O
illustrate	O
that	O
classiﬁcation	B
problems	O
can	O
either	O
be	O
binary	B
(	O
or	O
two-class	O
,	O
c	O
=	O
2	O
)	O
or	O
multi-class	B
(	O
c	O
>	O
2	O
)	O
.	O
we	O
will	O
focus	O
attention	O
on	O
probabilistic	B
classiﬁcation	I
,	O
where	O
test	O
predictions	O
take	O
the	O
form	O
of	O
class	O
probabilities	O
;	O
this	O
contrasts	O
with	O
methods	O
which	O
provide	O
only	O
a	O
guess	O
at	O
the	O
class	O
label	O
,	O
and	O
this	O
distinction	O
is	O
analogous	O
to	O
the	O
diﬀerence	O
between	O
predictive	B
distributions	O
and	O
point	O
predictions	O
in	O
the	O
regression	B
setting	O
.	O
since	O
generalization	B
to	O
test	O
cases	O
inherently	O
involves	O
some	O
level	O
of	O
uncertainty	O
,	O
it	O
seems	O
natural	O
to	O
attempt	O
to	O
make	O
predictions	O
in	O
a	O
way	O
that	O
reﬂects	O
these	O
uncertainties	O
.	O
in	O
a	O
practical	O
application	O
one	O
may	O
well	O
seek	O
a	O
class	O
guess	O
,	O
which	O
can	O
be	O
obtained	O
as	O
the	O
solution	O
to	O
a	O
decision	O
problem	O
,	O
involving	O
the	O
predictive	B
probabilities	O
as	O
well	O
as	O
a	O
speciﬁcation	O
of	O
the	O
consequences	O
of	O
making	O
speciﬁc	O
predictions	O
(	O
the	O
loss	B
function	I
)	O
.	O
both	O
classiﬁcation	B
and	O
regression	B
can	O
be	O
viewed	O
as	O
function	B
approximation	O
problems	O
.	O
unfortunately	O
,	O
the	O
solution	O
of	O
classiﬁcation	B
problems	O
using	O
gaussian	O
processes	O
is	O
rather	O
more	O
demanding	O
than	O
for	O
the	O
regression	B
problems	O
considered	O
in	O
chapter	O
2.	O
this	O
is	O
because	O
we	O
assumed	O
in	O
the	O
previous	O
chapter	O
that	O
the	O
likelihood	B
function	O
was	O
gaussian	O
;	O
a	O
gaussian	O
process	B
prior	O
combined	O
with	O
a	O
gaussian	O
likelihood	B
gives	O
rise	O
to	O
a	O
posterior	O
gaussian	O
process	B
over	O
functions	O
,	O
and	O
everything	O
remains	O
analytically	O
tractable	O
.	O
for	O
classiﬁcation	B
models	O
,	O
where	O
the	O
targets	O
are	O
discrete	O
class	O
labels	O
,	O
the	O
gaussian	O
likelihood	B
is	O
inappropriate	O
;	O
2	O
1in	O
the	O
statistics	O
literature	O
classiﬁcation	B
is	O
often	O
called	O
discrimination	O
.	O
2one	O
may	O
choose	O
to	O
ignore	O
the	O
discreteness	O
of	O
the	O
target	O
values	O
,	O
and	O
use	O
a	O
regression	B
treatment	O
,	O
where	O
all	O
targets	O
happen	O
to	O
be	O
say	O
±1	O
for	O
binary	B
classiﬁcation	I
.	O
this	O
is	O
known	O
as	O
binary	B
,	O
multi-class	B
probabilistic	O
classiﬁcation	B
non-gaussian	O
likelihood	B
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
34	O
classiﬁcation	B
in	O
this	O
chapter	O
we	O
treat	O
methods	O
of	O
approximate	O
inference	O
for	O
classiﬁcation	B
,	O
where	O
exact	O
inference	O
is	O
not	O
feasible.3	O
section	O
3.1	O
provides	O
a	O
general	O
discussion	O
of	O
classiﬁcation	B
problems	O
,	O
and	O
de-	O
scribes	O
the	O
generative	O
and	O
discriminative	O
approaches	O
to	O
these	O
problems	O
.	O
in	O
section	O
2.1	O
we	O
saw	O
how	O
gaussian	O
process	B
regression	O
(	O
gpr	O
)	O
can	O
be	O
obtained	O
in	O
section	O
3.2	O
we	O
describe	O
an	O
analogue	O
of	O
by	O
generalizing	O
linear	B
regression	I
.	O
linear	B
regression	I
in	O
the	O
classiﬁcation	B
case	O
,	O
logistic	B
regression	I
.	O
in	O
section	O
3.3	O
logistic	B
regression	I
is	O
generalized	B
to	O
yield	O
gaussian	O
process	B
classiﬁcation	O
(	O
gpc	O
)	O
using	O
again	O
the	O
ideas	O
behind	O
the	O
generalization	B
of	O
linear	B
regression	I
to	O
gpr	O
.	O
for	O
gpr	O
the	O
combination	O
of	O
a	O
gp	O
prior	O
with	O
a	O
gaussian	O
likelihood	B
gives	O
rise	O
to	O
a	O
posterior	O
which	O
is	O
again	O
a	O
gaussian	O
process	B
.	O
in	O
the	O
classiﬁcation	B
case	O
the	O
likelihood	B
is	O
non-gaussian	O
but	O
the	O
posterior	B
process	I
can	O
be	O
approximated	O
by	O
a	O
gp	O
.	O
the	O
laplace	O
approximation	O
for	O
gpc	O
is	O
described	O
in	O
section	O
3.4	O
(	O
for	O
binary	B
classiﬁcation	I
)	O
and	O
in	O
section	O
3.5	O
(	O
for	O
multi-class	B
classiﬁcation	I
)	O
,	O
and	O
the	O
expecta-	O
tion	O
propagation	O
algorithm	O
(	O
for	O
binary	B
classiﬁcation	I
)	O
is	O
described	O
in	O
section	O
3.6.	O
both	O
of	O
these	O
methods	O
make	O
use	O
of	O
a	O
gaussian	O
approximation	O
to	O
the	O
posterior	O
.	O
experimental	O
results	O
for	O
gpc	O
are	O
given	O
in	O
section	O
3.7	O
,	O
and	O
a	O
discussion	O
of	O
these	O
results	O
is	O
provided	O
in	O
section	O
3.8	O
.	O
3.1	O
classiﬁcation	B
problems	O
generative	B
approach	I
discriminative	O
approach	O
generative	O
model	O
example	O
the	O
natural	O
starting	O
point	O
for	O
discussing	O
approaches	O
to	O
classiﬁcation	B
is	O
the	O
joint	B
probability	O
p	O
(	O
y	O
,	O
x	O
)	O
,	O
where	O
y	O
denotes	O
the	O
class	O
label	O
.	O
using	O
bayes	O
’	O
theorem	O
this	O
joint	B
probability	O
can	O
be	O
decomposed	O
either	O
as	O
p	O
(	O
y	O
)	O
p	O
(	O
x|y	O
)	O
or	O
as	O
p	O
(	O
x	O
)	O
p	O
(	O
y|x	O
)	O
.	O
this	O
gives	O
rise	O
to	O
two	O
diﬀerent	O
approaches	O
to	O
classiﬁcation	B
problems	O
.	O
the	O
ﬁrst	O
,	O
which	O
we	O
call	O
the	O
generative	B
approach	I
,	O
models	O
the	O
class-conditional	O
distribu-	O
tions	O
p	O
(	O
x|y	O
)	O
for	O
y	O
=	O
c1	O
,	O
.	O
.	O
.	O
,	O
cc	O
and	O
also	O
the	O
prior	O
probabilities	O
of	O
each	O
class	O
,	O
and	O
then	O
computes	O
the	O
posterior	O
probability	O
for	O
each	O
class	O
using	O
.	O
(	O
3.1	O
)	O
p	O
(	O
y|x	O
)	O
=	O
pc	O
p	O
(	O
y	O
)	O
p	O
(	O
x|y	O
)	O
c=1	O
p	O
(	O
cc	O
)	O
p	O
(	O
x|cc	O
)	O
the	O
alternative	O
approach	O
,	O
which	O
we	O
call	O
the	O
discriminative	B
approach	I
,	O
focusses	O
on	O
modelling	O
p	O
(	O
y|x	O
)	O
directly	O
.	O
dawid	O
[	O
1976	O
]	O
calls	O
the	O
generative	O
and	O
discrimina-	O
tive	O
approaches	O
the	O
sampling	O
and	O
diagnostic	O
paradigms	O
,	O
respectively	O
.	O
to	O
turn	O
both	O
the	O
generative	O
and	O
discriminative	O
approaches	O
into	O
practical	O
methods	O
we	O
will	O
need	O
to	O
create	O
models	O
for	O
either	O
p	O
(	O
x|y	O
)	O
,	O
or	O
p	O
(	O
y|x	O
)	O
respectively.4	O
these	O
could	O
either	O
be	O
of	O
parametric	B
form	O
,	O
or	O
non-parametric	B
models	O
such	O
as	O
those	O
based	O
on	O
nearest	O
neighbours	O
.	O
for	O
the	O
generative	O
case	O
a	O
simple	O
,	O
com-	O
least-squares	B
classiﬁcation	I
,	O
see	B
section	O
6.5	O
.	O
3note	O
,	O
that	O
the	O
important	O
distinction	O
is	O
between	O
gaussian	O
and	O
non-gaussian	O
likelihoods	O
;	O
regression	B
with	O
a	O
non-gaussian	O
likelihood	B
requires	O
a	O
similar	O
treatment	O
,	O
but	O
since	O
classiﬁcation	B
deﬁnes	O
an	O
important	O
conceptual	O
and	O
application	O
area	O
,	O
we	O
have	O
chosen	O
to	O
treat	O
it	O
in	O
a	O
separate	O
chapter	O
;	O
for	O
non-gaussian	O
likelihoods	O
in	O
general	O
,	O
see	B
section	O
9.3	O
.	O
4for	O
the	O
generative	B
approach	I
inference	O
for	O
p	O
(	O
y	O
)	O
is	O
generally	O
straightforward	O
,	O
being	O
esti-	O
mation	O
of	O
a	O
binomial	O
probability	B
in	O
the	O
binary	B
case	O
,	O
or	O
a	O
multinomial	O
probability	B
in	O
the	O
multi-class	B
case	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.1	O
classiﬁcation	B
problems	O
35	O
discriminative	O
model	O
example	O
response	B
function	I
probit	O
regression	B
generative	O
or	O
discriminative	O
?	O
missing	O
values	O
mon	O
choice	O
would	O
be	O
to	O
model	B
the	O
class-conditional	O
densities	O
with	O
gaussians	O
:	O
p	O
(	O
x|cc	O
)	O
=	O
n	O
(	O
µc	O
,	O
σc	O
)	O
.	O
a	O
bayesian	O
treatment	O
can	O
be	O
obtained	O
by	O
placing	O
appro-	O
priate	O
priors	O
on	O
the	O
mean	O
and	O
covariance	B
of	O
each	O
of	O
the	O
gaussians	O
.	O
however	O
,	O
note	O
that	O
this	O
gaussian	O
model	B
makes	O
a	O
strong	O
assumption	O
on	O
the	O
form	O
of	O
class-	O
conditional	B
density	O
and	O
if	O
this	O
is	O
inappropriate	O
the	O
model	B
may	O
perform	O
poorly	O
.	O
for	O
the	O
binary	B
discriminative	O
case	O
one	O
simple	O
idea	O
is	O
to	O
turn	O
the	O
output	O
of	O
a	O
regression	B
model	O
into	O
a	O
class	O
probability	B
using	O
a	O
response	B
function	I
(	O
the	O
inverse	O
of	O
a	O
link	B
function	I
)	O
,	O
which	O
“	O
squashes	O
”	O
its	O
argument	O
,	O
which	O
can	O
lie	O
in	O
the	O
domain	O
(	O
−∞	O
,	O
∞	O
)	O
,	O
into	O
the	O
range	O
[	O
0	O
,	O
1	O
]	O
,	O
guaranteeing	O
a	O
valid	O
probabilistic	B
interpretation	O
.	O
one	O
example	O
is	O
the	O
linear	B
logistic	O
regression	B
model	O
p	O
(	O
c1|x	O
)	O
=	O
λ	O
(	O
x	O
>	O
w	O
)	O
,	O
where	O
λ	O
(	O
z	O
)	O
=	O
1	O
1	O
+	O
exp	O
(	O
−z	O
)	O
,	O
(	O
3.2	O
)	O
standard	O
normal	O
distribution	O
φ	O
(	O
z	O
)	O
=r	O
z	O
which	O
combines	O
the	O
linear	B
model	O
with	O
the	O
logistic	B
response	O
function	B
.	O
another	O
common	O
choice	O
of	O
response	B
function	I
is	O
the	O
cumulative	O
density	O
function	B
of	O
a	O
−∞	O
n	O
(	O
x|0	O
,	O
1	O
)	O
dx	O
.	O
this	O
approach	O
is	O
known	O
as	O
probit	B
regression	I
.	O
just	O
as	O
we	O
gave	O
a	O
bayesian	O
approach	O
to	O
linear	B
regression	I
in	O
chapter	O
2	O
we	O
can	O
take	O
a	O
parallel	O
approach	O
to	O
logistic	B
regression	I
,	O
as	O
discussed	O
in	O
section	O
3.2.	O
as	O
in	O
the	O
regression	B
case	O
,	O
this	O
model	B
is	O
an	O
important	O
step	O
towards	O
the	O
gaussian	O
process	B
classiﬁer	O
.	O
given	O
that	O
there	O
are	O
the	O
generative	O
and	O
discriminative	O
approaches	O
,	O
which	O
one	O
should	O
we	O
prefer	O
?	O
this	O
is	O
perhaps	O
the	O
biggest	O
question	O
in	O
classiﬁcation	B
,	O
and	O
we	O
do	O
not	O
believe	O
that	O
there	O
is	O
a	O
right	O
answer	O
,	O
as	O
both	O
ways	O
of	O
writing	O
the	O
joint	B
p	O
(	O
y	O
,	O
x	O
)	O
are	O
correct	O
.	O
however	O
,	O
it	O
is	O
possible	O
to	O
identify	O
some	O
strengths	O
and	O
weaknesses	O
of	O
the	O
two	O
approaches	O
.	O
the	O
discriminative	B
approach	I
is	O
appealing	O
in	O
that	O
it	O
is	O
directly	O
modelling	O
what	O
we	O
want	O
,	O
p	O
(	O
y|x	O
)	O
.	O
also	O
,	O
density	O
estimation	O
for	O
the	O
class-conditional	O
distributions	O
is	O
a	O
hard	O
problem	O
,	O
particularly	O
when	O
x	O
is	O
high	O
dimensional	O
,	O
so	O
if	O
we	O
are	O
just	O
interested	O
in	O
classiﬁcation	B
then	O
the	O
generative	B
approach	I
may	O
mean	O
that	O
we	O
are	O
trying	O
to	O
solve	O
a	O
harder	O
problem	O
than	O
we	O
need	O
to	O
.	O
however	O
,	O
to	O
deal	O
with	O
missing	O
input	O
values	O
,	O
outliers	O
and	O
unlabelled	O
data	O
points	O
in	O
a	O
principled	O
fashion	O
it	O
is	O
very	O
helpful	O
to	O
have	O
access	O
to	O
p	O
(	O
x	O
)	O
,	O
and	O
this	O
can	O
be	O
obtained	O
from	O
marginalizing	O
out	O
the	O
class	O
label	O
y	O
from	O
the	O
joint	B
y	O
p	O
(	O
y	O
)	O
p	O
(	O
x|y	O
)	O
in	O
the	O
generative	B
approach	I
.	O
a	O
further	O
factor	O
in	O
the	O
choice	O
of	O
a	O
generative	O
or	O
discriminative	B
approach	I
could	O
also	O
be	O
which	O
one	O
is	O
most	O
conducive	O
to	O
the	O
incorporation	O
of	O
any	O
prior	O
information	O
which	O
is	O
available	O
.	O
see	B
ripley	O
[	O
1996	O
,	O
sec	O
.	O
2.1	O
]	O
for	O
further	O
discussion	O
of	O
these	O
issues	O
.	O
the	O
gaussian	O
process	B
classiﬁers	O
developed	O
in	O
this	O
chapter	O
are	O
discriminative	O
.	O
as	O
p	O
(	O
x	O
)	O
=	O
p	O
3.1.1	O
decision	O
theory	O
for	O
classiﬁcation	B
the	O
classiﬁers	O
described	O
above	O
provide	O
predictive	B
probabilities	O
p	O
(	O
y∗|x∗	O
)	O
for	O
a	O
test	O
input	O
x∗	O
.	O
however	O
,	O
sometimes	O
one	O
actually	O
needs	O
to	O
make	O
a	O
decision	O
and	O
to	O
do	O
this	O
we	O
need	O
to	O
consider	O
decision	O
theory	O
.	O
decision	O
theory	O
for	O
the	O
regres-	O
sion	O
problem	O
was	O
considered	O
in	O
section	O
2.4	O
;	O
here	O
we	O
discuss	O
decision	O
theory	O
for	O
classiﬁcation	B
problems	O
.	O
a	O
comprehensive	O
treatment	O
of	O
decision	O
theory	O
can	O
be	O
found	O
in	O
berger	O
[	O
1985	O
]	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
36	O
loss	B
,	O
risk	B
zero-one	O
loss	B
asymmetric	O
loss	B
bayes	O
classiﬁer	B
decision	O
regions	O
reject	B
option	I
risk	O
minimization	O
classiﬁcation	B
given	O
x	O
is	O
rl	O
(	O
c0|x	O
)	O
=p	O
let	O
l	O
(	O
c	O
,	O
c0	O
)	O
be	O
the	O
loss	B
incurred	O
by	O
making	O
decision	O
c0	O
if	O
the	O
true	O
class	O
is	O
cc	O
.	O
usually	O
l	O
(	O
c	O
,	O
c	O
)	O
=	O
0	O
for	O
all	O
c.	O
the	O
expected	O
loss5	O
(	O
or	O
risk	B
)	O
of	O
taking	O
decision	O
c0	O
c	O
l	O
(	O
c	O
,	O
c0	O
)	O
p	O
(	O
cc|x	O
)	O
and	O
the	O
optimal	B
decision	O
c∗	O
is	O
the	O
one	O
that	O
minimizes	O
rl	O
(	O
c0|x	O
)	O
.	O
one	O
common	O
choice	O
of	O
loss	B
function	I
is	O
the	O
zero-one	B
loss	O
,	O
where	O
a	O
penalty	O
of	O
one	O
unit	O
is	O
paid	O
for	O
an	O
incorrect	O
classiﬁcation	B
,	O
and	O
0	O
for	O
a	O
correct	O
one	O
.	O
in	O
this	O
case	O
the	O
optimal	B
decision	O
rule	O
is	O
to	O
choose	O
the	O
class	O
cc	O
that	O
maximizes6	O
p	O
(	O
cc|x	O
)	O
,	O
as	O
this	O
minimizes	O
the	O
expected	O
error	B
at	O
x.	O
however	O
,	O
the	O
zero-one	B
loss	O
is	O
not	O
always	O
appropriate	O
.	O
a	O
classic	O
example	O
of	O
this	O
is	O
the	O
diﬀerence	O
in	O
loss	B
of	O
failing	O
to	O
spot	O
a	O
disease	O
when	O
carrying	O
out	O
a	O
medical	O
test	O
compared	O
to	O
the	O
cost	O
of	O
a	O
false	O
positive	O
on	O
the	O
test	O
,	O
so	O
that	O
l	O
(	O
c	O
,	O
c0	O
)	O
6=	O
l	O
(	O
c0	O
,	O
c	O
)	O
.	O
the	O
optimal	B
classiﬁer	O
(	O
using	O
zero-one	B
loss	O
)	O
is	O
known	O
as	O
the	O
bayes	O
classi-	O
ﬁer	O
.	O
by	O
this	O
construction	O
the	O
feature	B
space	I
is	O
divided	O
into	O
decision	O
regions	O
r1	O
,	O
.	O
.	O
.	O
,	O
rc	O
such	O
that	O
a	O
pattern	O
falling	O
in	O
decision	B
region	I
rc	O
is	O
assigned	O
to	O
class	O
cc	O
.	O
(	O
there	O
can	O
be	O
more	O
than	O
one	O
decision	B
region	I
corresponding	O
to	O
a	O
single	O
class	O
.	O
)	O
the	O
boundaries	O
between	O
the	O
decision	O
regions	O
are	O
known	O
as	O
decision	O
surfaces	O
or	O
decision	O
boundaries	O
.	O
one	O
would	O
expect	O
misclassiﬁcation	O
errors	O
to	O
occur	O
in	O
regions	O
where	O
the	O
max-	O
imum	O
class	O
probability	B
maxj	O
p	O
(	O
cj|x	O
)	O
is	O
relatively	O
low	O
.	O
this	O
could	O
be	O
due	O
to	O
either	O
a	O
region	O
of	O
strong	O
overlap	O
between	O
classes	O
,	O
or	O
lack	O
of	O
training	O
examples	O
within	O
this	O
region	O
.	O
thus	O
one	O
sensible	O
strategy	O
is	O
to	O
add	O
a	O
reject	B
option	I
so	O
that	O
if	O
maxj	O
p	O
(	O
cj|x	O
)	O
≥	O
θ	O
for	O
a	O
threshold	O
θ	O
in	O
(	O
0	O
,	O
1	O
)	O
then	O
we	O
go	O
ahead	O
and	O
classify	O
the	O
pattern	O
,	O
otherwise	O
we	O
reject	O
it	O
and	O
leave	O
the	O
classiﬁcation	B
task	O
to	O
a	O
more	O
sophisticated	O
system	O
.	O
for	O
multi-class	B
classiﬁcation	I
we	O
could	O
alternatively	O
re-	O
quire	O
the	O
gap	O
between	O
the	O
most	O
probable	O
and	O
the	O
second	O
most	O
probable	O
class	O
to	O
exceed	O
θ	O
,	O
and	O
otherwise	O
reject	O
.	O
as	O
θ	O
is	O
varied	O
from	O
0	O
to	O
1	O
one	O
obtains	O
an	O
error-	O
reject	O
curve	O
,	O
plotting	O
the	O
percentage	O
of	O
patterns	O
classiﬁed	O
incorrectly	O
against	O
the	O
percentage	O
rejected	O
.	O
typically	O
the	O
error	B
rate	O
will	O
fall	O
as	O
the	O
rejection	O
rate	O
increases	O
.	O
hansen	O
et	O
al	O
.	O
[	O
1997	O
]	O
provide	O
an	O
analysis	O
of	O
the	O
error-reject	O
trade-oﬀ	O
.	O
we	O
have	O
focused	O
above	O
on	O
the	O
probabilistic	B
approach	O
to	O
classiﬁcation	B
,	O
which	O
involves	O
a	O
two-stage	O
approach	O
of	O
ﬁrst	O
computing	O
a	O
posterior	O
distribution	O
over	O
functions	O
and	O
then	O
combining	O
this	O
with	O
the	O
loss	B
function	I
to	O
produce	O
a	O
decision	O
.	O
however	O
,	O
it	O
is	O
worth	O
noting	O
that	O
some	O
authors	O
argue	O
that	O
if	O
our	O
goal	O
is	O
to	O
eventually	O
make	O
a	O
decision	O
then	O
we	O
should	O
aim	O
to	O
approximate	O
the	O
classiﬁcation	B
function	O
that	O
minimizes	O
the	O
risk	B
(	O
expected	O
loss	B
)	O
,	O
which	O
is	O
deﬁned	O
as	O
z	O
l	O
(	O
cid:0	O
)	O
y	O
,	O
c	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
p	O
(	O
y	O
,	O
x	O
)	O
dydx	O
,	O
rl	O
(	O
c	O
)	O
=	O
(	O
3.3	O
)	O
where	O
p	O
(	O
y	O
,	O
x	O
)	O
is	O
the	O
joint	B
distribution	O
of	O
inputs	O
and	O
targets	O
and	O
c	O
(	O
x	O
)	O
is	O
a	O
clas-	O
siﬁcation	O
function	B
that	O
assigns	O
an	O
input	O
pattern	O
x	O
to	O
one	O
of	O
c	O
classes	O
(	O
see	B
pn	O
e.g	O
.	O
vapnik	O
[	O
1995	O
,	O
ch	O
.	O
1	O
]	O
)	O
.	O
as	O
p	O
(	O
y	O
,	O
x	O
)	O
is	O
unknown	O
,	O
in	O
this	O
approach	O
one	O
often	O
then	O
seeks	O
to	O
minimize	O
an	O
objective	O
function	B
which	O
includes	O
the	O
empirical	O
risk	B
i=1	O
l	O
(	O
yi	O
,	O
c	O
(	O
xi	O
)	O
)	O
as	O
well	O
as	O
a	O
regularization	B
term	O
.	O
while	O
this	O
is	O
a	O
reasonable	O
5in	O
economics	O
one	O
usually	O
talks	O
of	O
maximizing	O
expected	O
utility	O
rather	O
than	O
minimizing	O
expected	O
loss	B
;	O
loss	B
is	O
negative	O
utility	O
.	O
this	O
suggests	O
that	O
statisticians	O
are	O
pessimists	O
while	O
economists	O
are	O
optimists	O
.	O
6if	O
more	O
than	O
one	O
class	O
has	O
equal	O
posterior	O
probability	O
then	O
ties	O
can	O
be	O
broken	O
arbitrarily	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.2	O
linear	B
models	O
for	O
classiﬁcation	B
37	O
method	O
,	O
we	O
note	O
that	O
the	O
probabilistic	B
approach	O
allows	O
the	O
same	O
inference	O
stage	O
to	O
be	O
re-used	O
with	O
diﬀerent	O
loss	B
functions	O
,	O
it	O
can	O
help	O
us	O
to	O
incorporate	O
prior	O
knowledge	O
on	O
the	O
function	B
and/or	O
noise	B
model	I
,	O
and	O
has	O
the	O
advantage	O
of	O
giving	O
probabilistic	B
predictions	O
which	O
can	O
be	O
helpful	O
e.g	O
.	O
for	O
the	O
reject	B
option	I
.	O
3.2	O
linear	B
models	O
for	O
classiﬁcation	B
in	O
this	O
section	O
we	O
brieﬂy	O
review	O
linear	B
models	O
for	O
binary	B
classiﬁcation	I
,	O
which	O
form	O
the	O
foundation	O
of	O
gaussian	O
process	B
classiﬁcation	O
models	O
in	O
the	O
next	O
sec-	O
tion	O
.	O
we	O
follow	O
the	O
svm	O
literature	O
and	O
use	O
the	O
labels	O
y	O
=	O
+1	O
and	O
y	O
=	O
−1	O
to	O
distinguish	O
the	O
two	O
classes	O
,	O
although	O
for	O
the	O
multi-class	B
case	O
in	O
section	O
3.5	O
we	O
use	O
0/1	O
labels	O
.	O
the	O
likelihood	B
is	O
p	O
(	O
y	O
=+1|x	O
,	O
w	O
)	O
=	O
σ	O
(	O
x	O
>	O
w	O
)	O
,	O
(	O
3.4	O
)	O
given	O
the	O
weight	B
vector	I
w	O
and	O
σ	O
(	O
z	O
)	O
can	O
be	O
any	O
sigmoid7	O
function	B
.	O
when	O
using	O
the	O
logistic	B
,	O
σ	O
(	O
z	O
)	O
=	O
λ	O
(	O
z	O
)	O
from	O
eq	O
.	O
(	O
3.2	O
)	O
,	O
the	O
model	B
is	O
usually	O
called	O
simply	O
logistic	B
regression	I
,	O
but	O
to	O
emphasize	O
the	O
parallels	O
to	O
linear	B
regression	I
we	O
prefer	O
the	O
term	O
linear	B
logistic	O
regression	B
.	O
when	O
using	O
the	O
cumulative	O
gaussian	O
σ	O
(	O
z	O
)	O
=	O
φ	O
(	O
z	O
)	O
,	O
we	O
call	O
the	O
model	B
linear	O
probit	B
regression	I
.	O
as	O
the	O
probability	B
of	O
the	O
two	O
classes	O
must	O
sum	O
to	O
1	O
,	O
we	O
have	O
p	O
(	O
y	O
=−1|x	O
,	O
w	O
)	O
=	O
1	O
−	O
p	O
(	O
y	O
=	O
+1|x	O
,	O
w	O
)	O
.	O
thus	O
for	O
a	O
data	O
point	O
(	O
xi	O
,	O
yi	O
)	O
the	O
likelihood	B
is	O
given	O
by	O
i	O
w	O
)	O
if	O
yi	O
=	O
−1	O
.	O
for	O
symmetric	O
likelihood	B
i	O
w	O
)	O
if	O
yi	O
=	O
+1	O
,	O
and	O
1	O
−	O
σ	O
(	O
x	O
>	O
σ	O
(	O
x	O
>	O
functions	O
,	O
such	O
as	O
the	O
logistic	B
or	O
probit	B
where	O
σ	O
(	O
−z	O
)	O
=	O
1	O
−	O
σ	O
(	O
z	O
)	O
,	O
this	O
can	O
be	O
written	O
more	O
concisely	O
as	O
p	O
(	O
yi|xi	O
,	O
w	O
)	O
=	O
σ	O
(	O
yifi	O
)	O
,	O
(	O
3.5	O
)	O
log	O
(	O
cid:0	O
)	O
p	O
(	O
y	O
=	O
+1|x	O
)	O
/p	O
(	O
y	O
=−1|x	O
)	O
(	O
cid:1	O
)	O
we	O
see	B
that	O
the	O
logistic	B
regression	I
model	O
can	O
be	O
where	O
fi	O
,	O
f	O
(	O
xi	O
)	O
=	O
x	O
>	O
i	O
w.	O
deﬁning	O
the	O
logit	O
transformation	O
as	O
logit	O
(	O
x	O
)	O
=	O
written	O
as	O
logit	O
(	O
x	O
)	O
=	O
x	O
>	O
w	O
.	O
the	O
logit	O
(	O
x	O
)	O
function	B
is	O
also	O
called	O
the	O
log	B
odds	I
ratio	I
.	O
generalized	B
linear	O
modelling	O
[	O
mccullagh	O
and	O
nelder	O
,	O
1983	O
]	O
deals	O
with	O
the	O
issue	O
of	O
extending	O
linear	B
models	O
to	O
non-gaussian	O
data	O
scenarios	O
;	O
the	O
logit	O
transformation	O
is	O
the	O
canonical	B
link	O
function	B
for	O
binary	B
data	O
and	O
this	O
choice	O
simpliﬁes	O
the	O
algebra	O
and	O
algorithms	O
.	O
given	O
a	O
dataset	B
d	O
=	O
{	O
(	O
xi	O
,	O
yi	O
)	O
|i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
we	O
assume	O
that	O
the	O
labels	O
are	O
generated	O
independently	O
,	O
conditional	B
on	O
f	O
(	O
x	O
)	O
.	O
using	O
the	O
same	O
gaussian	O
prior	O
w	O
∼	O
n	O
(	O
0	O
,	O
σp	O
)	O
as	O
for	O
regression	B
in	O
eq	O
.	O
(	O
2.4	O
)	O
we	O
then	O
obtain	O
the	O
un-normalized	O
log	O
posterior	O
nx	O
i=1	O
log	O
p	O
(	O
w|x	O
,	O
y	O
)	O
c=	O
−1	O
2	O
w	O
>	O
σ−1	O
p	O
w	O
+	O
log	O
σ	O
(	O
yifi	O
)	O
.	O
(	O
3.6	O
)	O
in	O
the	O
linear	B
regression	I
case	O
with	O
gaussian	O
noise	O
the	O
posterior	O
was	O
gaussian	O
with	O
mean	O
and	O
covariance	B
as	O
given	O
in	O
eq	O
.	O
(	O
2.8	O
)	O
.	O
for	O
classiﬁcation	B
the	O
posterior	O
7a	O
sigmoid	O
function	B
is	O
a	O
monotonically	O
increasing	O
function	B
mapping	O
from	O
r	O
to	O
[	O
0	O
,	O
1	O
]	O
.	O
it	O
derives	O
its	O
name	O
from	O
being	O
shaped	O
like	O
a	O
letter	O
s.	O
linear	B
logistic	O
regression	B
linear	O
probit	B
regression	I
concise	O
notation	O
logit	O
log	B
odds	I
ratio	I
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
38	O
concavity	O
unique	O
maximum	O
irls	O
algorithm	O
properties	O
of	O
maximum	B
likelihood	I
classiﬁcation	O
does	O
not	O
have	O
a	O
simple	O
analytic	O
form	O
.	O
however	O
,	O
it	O
is	O
easy	O
to	O
show	O
that	O
for	O
some	O
sigmoid	O
functions	O
,	O
such	O
as	O
the	O
logistic	B
and	O
cumulative	O
gaussian	O
,	O
the	O
log	O
likelihood	O
is	O
a	O
concave	O
function	B
of	O
w	O
for	O
ﬁxed	O
d.	O
as	O
the	O
quadratic	O
penalty	O
on	O
w	O
is	O
also	O
concave	O
then	O
the	O
log	O
posterior	O
is	O
a	O
concave	O
function	B
,	O
which	O
means	O
that	O
it	O
is	O
relatively	O
easy	O
to	O
ﬁnd	O
its	O
unique	O
maximum	O
.	O
the	O
concavity	O
can	O
also	O
be	O
derived	O
from	O
the	O
fact	O
that	O
the	O
hessian	O
of	O
log	O
p	O
(	O
w|x	O
,	O
y	O
)	O
is	O
negative	O
deﬁnite	O
(	O
see	B
section	O
a.9	O
for	O
further	O
details	O
)	O
.	O
the	O
standard	O
algorithm	O
for	O
ﬁnding	O
the	O
maxi-	O
mum	O
is	O
newton	O
’	O
s	O
method	O
,	O
which	O
in	O
this	O
context	O
is	O
usually	O
called	O
the	O
iteratively	B
reweighted	I
least	I
squares	I
(	O
irls	O
)	O
algorithm	O
,	O
as	O
described	O
e.g	O
.	O
in	O
mccullagh	O
and	O
nelder	O
[	O
1983	O
]	O
.	O
however	O
,	O
note	O
that	O
minka	O
[	O
2003	O
]	O
provides	O
evidence	B
that	O
other	O
optimization	O
methods	O
(	O
e.g	O
.	O
conjugate	O
gradient	O
ascent	O
)	O
may	O
be	O
faster	O
than	O
irls	O
.	O
notice	O
that	O
a	O
maximum	B
likelihood	I
treatment	O
(	O
corresponding	O
to	O
an	O
unpe-	O
nalized	O
version	O
of	O
eq	O
.	O
(	O
3.6	O
)	O
)	O
may	O
result	O
in	O
some	O
undesirable	O
outcomes	O
.	O
if	O
the	O
dataset	B
is	O
linearly	O
separable	O
(	O
i.e	O
.	O
if	O
there	O
exists	O
a	O
hyperplane	B
which	O
separates	O
the	O
positive	O
and	O
negative	O
examples	O
)	O
then	O
maximizing	O
the	O
(	O
unpenalized	O
)	O
likelihood	B
will	O
cause	O
|w|	O
to	O
tend	O
to	O
inﬁnity	O
,	O
however	O
,	O
this	O
will	O
still	O
give	O
predictions	O
in	O
[	O
0	O
,	O
1	O
]	O
for	O
p	O
(	O
y	O
=	O
+1|x	O
,	O
w	O
)	O
,	O
although	O
these	O
predictions	O
will	O
be	O
“	O
hard	O
”	O
(	O
i.e	O
.	O
zero	O
or	O
one	O
)	O
.	O
if	O
the	O
problem	O
is	O
ill-conditioned	O
,	O
e.g	O
.	O
due	O
to	O
duplicate	O
(	O
or	O
linearly	O
dependent	O
)	O
input	O
dimensions	O
,	O
there	O
will	O
be	O
no	O
unique	O
solution	O
.	O
predictions	O
softmax	B
multiple	O
logistic	B
as	O
an	O
example	O
,	O
consider	O
linear	B
logistic	O
regression	B
in	O
the	O
case	O
where	O
x-space	O
is	O
two	O
dimensional	O
and	O
there	O
is	O
no	O
bias	B
weight	O
so	O
that	O
w	O
is	O
also	O
two-dimensional	O
.	O
the	O
prior	O
in	O
weight	O
space	O
is	O
gaussian	O
and	O
for	O
simplicity	O
we	O
have	O
set	B
σp	O
=	O
i.	O
contours	O
of	O
the	O
prior	O
p	O
(	O
w	O
)	O
are	O
illustrated	O
in	O
figure	O
3.1	O
(	O
a	O
)	O
.	O
if	O
we	O
have	O
a	O
data	O
set	B
d	O
as	O
shown	O
in	O
figure	O
3.1	O
(	O
b	O
)	O
then	O
this	O
induces	O
a	O
posterior	O
distribution	O
in	O
weight	O
space	O
as	O
shown	O
in	O
figure	O
3.1	O
(	O
c	O
)	O
.	O
notice	O
that	O
the	O
posterior	O
is	O
non-gaussian	O
and	O
unimodal	O
,	O
as	O
expected	O
.	O
the	O
dataset	B
is	O
not	O
linearly	O
separable	O
but	O
a	O
weight	B
vector	I
in	O
the	O
direction	O
(	O
1	O
,	O
1	O
)	O
>	O
is	O
clearly	O
a	O
reasonable	O
choice	O
,	O
as	O
the	O
posterior	O
distribution	O
shows	O
.	O
to	O
make	O
predictions	O
based	O
the	O
training	O
set	B
d	O
for	O
a	O
test	O
point	O
x∗	O
we	O
have	O
p	O
(	O
y∗	O
=+1|x∗	O
,	O
d	O
)	O
=	O
p	O
(	O
y∗	O
=+1|w	O
,	O
x∗	O
)	O
p	O
(	O
w|d	O
)	O
dw	O
,	O
(	O
3.7	O
)	O
z	O
integrating	O
the	O
prediction	B
p	O
(	O
y∗	O
=+1|w	O
,	O
x∗	O
)	O
=	O
σ	O
(	O
x	O
>	O
∗	O
w	O
)	O
over	O
the	O
posterior	O
distri-	O
bution	O
of	O
weights	O
.	O
this	O
leads	O
to	O
contours	O
of	O
the	O
predictive	B
distribution	O
as	O
shown	O
in	O
figure	O
3.1	O
(	O
d	O
)	O
.	O
notice	O
how	O
the	O
contours	O
are	O
bent	O
,	O
reﬂecting	O
the	O
integration	O
of	O
many	O
diﬀerent	O
but	O
plausible	O
w	O
’	O
s	O
.	O
in	O
the	O
multi-class	B
case	O
we	O
use	O
the	O
multiple	O
logistic	O
(	O
or	O
softmax	B
)	O
function	B
where	O
wc	O
is	O
the	O
weight	B
vector	I
for	O
class	O
c	O
,	O
and	O
all	O
weight	O
vectors	O
are	O
col-	O
lected	O
into	O
the	O
matrix	B
w	O
.	O
the	O
corresponding	O
log	O
likelihood	O
is	O
of	O
the	O
form	O
i	O
wc0	O
)	O
)	O
]	O
.	O
as	O
in	O
the	O
binary	B
case	O
the	O
log	O
i	O
wc	O
−	O
log	O
(	O
p	O
pc	O
c=1	O
δc	O
,	O
yi	O
[	O
x	O
>	O
c0	O
exp	O
(	O
x	O
>	O
pn	O
i=1	O
likelihood	B
is	O
a	O
concave	O
function	B
of	O
w	O
.	O
it	O
is	O
interesting	O
to	O
note	O
that	O
in	O
a	O
generative	B
approach	I
where	O
the	O
class-	O
conditional	B
distributions	O
p	O
(	O
x|y	O
)	O
are	O
gaussian	O
with	O
the	O
same	O
covariance	B
matrix	I
,	O
p	O
(	O
y	O
=	O
cc|x	O
,	O
w	O
)	O
=	O
p	O
exp	O
(	O
x	O
>	O
wc	O
)	O
c0	O
exp	O
(	O
x	O
>	O
wc0	O
)	O
,	O
(	O
3.8	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.3	O
gaussian	O
process	B
classiﬁcation	O
39	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
3.1	O
:	O
linear	B
logistic	O
regression	B
:	O
panel	O
(	O
a	O
)	O
shows	O
contours	O
of	O
the	O
prior	O
distri-	O
bution	O
p	O
(	O
w	O
)	O
=	O
n	O
(	O
0	O
,	O
i	O
)	O
.	O
panel	O
(	O
b	O
)	O
shows	O
the	O
dataset	B
,	O
with	O
circles	O
indicating	O
class	O
+1	O
and	O
crosses	O
denoting	O
class	O
−1	O
.	O
panel	O
(	O
c	O
)	O
shows	O
contours	O
of	O
the	O
posterior	O
distribution	O
p	O
(	O
w|d	O
)	O
.	O
panel	O
(	O
d	O
)	O
shows	O
contours	O
of	O
the	O
predictive	B
distribution	O
p	O
(	O
y∗	O
=	O
+1|x∗	O
)	O
.	O
p	O
(	O
y|x	O
)	O
has	O
the	O
form	O
given	O
by	O
eq	O
.	O
(	O
3.4	O
)	O
and	O
eq	O
.	O
(	O
3.8	O
)	O
for	O
the	O
two-	O
and	O
multi-class	B
cases	O
respectively	O
(	O
when	O
the	O
constant	O
function	B
1	O
is	O
included	O
in	O
x	O
)	O
.	O
3.3	O
gaussian	O
process	B
classiﬁcation	O
for	O
binary	B
classiﬁcation	I
the	O
basic	O
idea	O
behind	O
gaussian	O
process	B
prediction	O
is	O
very	O
simple—we	O
place	O
a	O
gp	O
prior	O
over	O
the	O
latent	O
function	O
f	O
(	O
x	O
)	O
and	O
then	O
“	O
squash	O
”	O
this	O
through	O
the	O
logistic	B
function	I
to	O
obtain	O
a	O
prior	O
on	O
π	O
(	O
x	O
)	O
,	O
p	O
(	O
y	O
=	O
+1|x	O
)	O
=	O
σ	O
(	O
f	O
(	O
x	O
)	O
)	O
.	O
note	O
that	O
π	O
is	O
a	O
deterministic	O
function	B
of	O
f	O
,	O
and	O
since	O
f	O
is	O
stochastic	O
,	O
so	O
is	O
π.	O
this	O
construction	O
is	O
illustrated	O
in	O
figure	O
3.2	O
for	O
a	O
one-	O
it	O
is	O
a	O
natural	O
generalization	B
of	O
the	O
linear	B
logistic	O
dimensional	O
input	O
space	O
.	O
latent	O
function	O
−2−1012−2−1012w1w2−505−505x1x2−2−1012−2−1012w1w2−505−505x1x20.10.50.9	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
40	O
classiﬁcation	B
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
3.2	O
:	O
panel	O
(	O
a	O
)	O
shows	O
a	O
sample	O
latent	O
function	O
f	O
(	O
x	O
)	O
drawn	O
from	O
a	O
gaussian	O
process	B
as	O
a	O
function	B
of	O
x.	O
panel	O
(	O
b	O
)	O
shows	O
the	O
result	O
of	O
squashing	O
this	O
sample	O
func-	O
tion	O
through	O
the	O
logistic	B
logit	O
function	B
,	O
λ	O
(	O
z	O
)	O
=	O
(	O
1	O
+	O
exp	O
(	O
−z	O
)	O
)	O
−1	O
to	O
obtain	O
the	O
class	O
probability	B
π	O
(	O
x	O
)	O
=	O
λ	O
(	O
f	O
(	O
x	O
)	O
)	O
.	O
regression	B
model	O
and	O
parallels	O
the	O
development	O
from	O
linear	B
regression	I
to	O
gp	O
regression	B
that	O
we	O
explored	O
in	O
section	O
2.1.	O
speciﬁcally	O
,	O
we	O
replace	O
the	O
linear	B
f	O
(	O
x	O
)	O
function	B
from	O
the	O
linear	B
logistic	O
model	B
in	O
eq	O
.	O
(	O
3.6	O
)	O
by	O
a	O
gaussian	O
process	B
,	O
and	O
correspondingly	O
the	O
gaussian	O
prior	O
on	O
the	O
weights	O
by	O
a	O
gp	O
prior	O
.	O
the	O
latent	O
function	O
f	O
plays	O
the	O
rˆole	O
of	O
a	O
nuisance	O
function	B
:	O
we	O
do	O
not	O
observe	O
values	O
of	O
f	O
itself	O
(	O
we	O
observe	O
only	O
the	O
inputs	O
x	O
and	O
the	O
class	O
labels	O
y	O
)	O
and	O
we	O
are	O
not	O
particularly	O
interested	O
in	O
the	O
values	O
of	O
f	O
,	O
but	O
rather	O
in	O
π	O
,	O
in	O
particular	O
for	O
test	O
cases	O
π	O
(	O
x∗	O
)	O
.	O
the	O
purpose	O
of	O
f	O
is	O
solely	O
to	O
allow	O
a	O
convenient	O
formulation	O
of	O
the	O
model	B
,	O
and	O
the	O
computational	O
goal	O
pursued	O
in	O
the	O
coming	O
sections	O
will	O
be	O
to	O
remove	O
(	O
integrate	O
out	O
)	O
f.	O
we	O
have	O
tacitly	O
assumed	O
that	O
the	O
latent	O
gaussian	O
process	B
is	O
noise-free	O
,	O
and	O
combined	O
it	O
with	O
smooth	O
likelihood	B
functions	O
,	O
such	O
as	O
the	O
logistic	B
or	O
probit	B
.	O
however	O
,	O
one	O
can	O
equivalently	O
think	O
of	O
adding	O
independent	O
noise	O
to	O
the	O
latent	O
process	O
in	O
combination	O
with	O
a	O
step-function	O
likelihood	B
.	O
in	O
particular	O
,	O
assuming	O
gaussian	O
noise	O
and	O
a	O
step-function	O
likelihood	B
is	O
exactly	O
equivalent	B
to	O
a	O
noise-	O
free8	O
latent	O
process	O
and	O
probit	B
likelihood	O
,	O
see	B
exercise	O
3.10.1.	O
nuisance	O
function	B
noise-free	O
latent	O
process	O
inference	O
is	O
naturally	O
divided	O
into	O
two	O
steps	O
:	O
ﬁrst	O
computing	O
the	O
distribution	O
of	O
the	O
latent	O
variable	O
corresponding	O
to	O
a	O
test	O
case	O
p	O
(	O
f∗|x	O
,	O
y	O
,	O
x∗	O
)	O
=	O
p	O
(	O
f∗|x	O
,	O
x∗	O
,	O
f	O
)	O
p	O
(	O
f|x	O
,	O
y	O
)	O
df	O
,	O
(	O
3.9	O
)	O
z	O
z	O
where	O
p	O
(	O
f|x	O
,	O
y	O
)	O
=	O
p	O
(	O
y|f	O
)	O
p	O
(	O
f|x	O
)	O
/p	O
(	O
y|x	O
)	O
is	O
the	O
posterior	O
over	O
the	O
latent	O
vari-	O
ables	O
,	O
and	O
subsequently	O
using	O
this	O
distribution	O
over	O
the	O
latent	O
f∗	O
to	O
produce	O
a	O
probabilistic	B
prediction	O
¯π∗	O
,	O
p	O
(	O
y∗	O
=+1|x	O
,	O
y	O
,	O
x∗	O
)	O
=	O
σ	O
(	O
f∗	O
)	O
p	O
(	O
f∗|x	O
,	O
y	O
,	O
x∗	O
)	O
df∗	O
.	O
(	O
3.10	O
)	O
8this	O
equivalence	O
explains	O
why	O
no	O
numerical	O
problems	O
arise	O
from	O
considering	O
a	O
noise-free	O
process	B
if	O
care	O
is	O
taken	O
with	O
the	O
implementation	O
,	O
see	B
also	O
comment	O
at	O
the	O
end	O
of	O
section	O
3.4.3	O
.	O
−4−2024input	O
,	O
xlatent	O
function	B
,	O
f	O
(	O
x	O
)	O
01input	O
,	O
xclass	O
probability	B
,	O
π	O
(	O
x	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.4	O
the	O
laplace	O
approximation	O
for	O
the	O
binary	B
gp	O
classiﬁer	B
41	O
in	O
the	O
regression	B
case	O
(	O
with	O
gaussian	O
likelihood	B
)	O
computation	O
of	O
predictions	O
was	O
straightforward	O
as	O
the	O
relevant	O
integrals	B
were	O
gaussian	O
and	O
could	O
be	O
computed	O
analytically	O
.	O
in	O
classiﬁcation	B
the	O
non-gaussian	O
likelihood	B
in	O
eq	O
.	O
(	O
3.9	O
)	O
makes	O
the	O
integral	O
analytically	O
intractable	O
.	O
similarly	O
,	O
eq	O
.	O
(	O
3.10	O
)	O
can	O
be	O
intractable	O
analytically	O
for	O
certain	O
sigmoid	O
functions	O
,	O
although	O
in	O
the	O
binary	B
case	O
it	O
is	O
only	O
a	O
one-dimensional	O
integral	O
so	O
simple	O
numerical	O
techniques	O
are	O
generally	O
adequate	O
.	O
thus	O
we	O
need	O
to	O
use	O
either	O
analytic	O
approximations	O
of	O
integrals	B
,	O
or	O
solutions	O
based	O
on	O
monte	O
carlo	O
sampling	O
.	O
in	O
the	O
coming	O
sections	O
,	O
we	O
describe	O
two	O
ana-	O
lytic	O
approximations	O
which	O
both	O
approximate	O
the	O
non-gaussian	O
joint	B
posterior	O
with	O
a	O
gaussian	O
one	O
:	O
the	O
ﬁrst	O
is	O
the	O
straightforward	O
laplace	O
approximation	O
method	O
[	O
williams	O
and	O
barber	O
,	O
1998	O
]	O
,	O
and	O
the	O
second	O
is	O
the	O
more	O
sophisticated	O
expectation	B
propagation	I
(	O
ep	O
)	O
method	O
due	O
to	O
minka	O
[	O
2001	O
]	O
.	O
(	O
the	O
cavity	O
tap	O
ap-	O
proximation	O
of	O
opper	O
and	O
winther	O
[	O
2000	O
]	O
is	O
closely	O
related	O
to	O
the	O
ep	O
method	O
.	O
)	O
a	O
number	O
of	O
other	O
approximations	O
have	O
also	O
been	O
suggested	O
,	O
see	B
e.g	O
.	O
gibbs	O
and	O
mackay	O
[	O
2000	O
]	O
,	O
jaakkola	O
and	O
haussler	O
[	O
1999	O
]	O
,	O
and	O
seeger	O
[	O
2000	O
]	O
.	O
neal	O
[	O
1999	O
]	O
describes	O
the	O
use	O
of	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
approximations	O
.	O
all	O
of	O
these	O
methods	O
will	O
typically	O
scale	O
as	O
o	O
(	O
n3	O
)	O
;	O
for	O
large	O
datasets	O
there	O
has	O
been	O
much	O
work	O
on	O
further	O
approximations	O
to	O
reduce	O
computation	O
time	O
,	O
as	O
discussed	O
in	O
chapter	O
8.	O
the	O
laplace	O
approximation	O
for	O
the	O
binary	B
case	O
is	O
described	O
in	O
section	O
3.4	O
,	O
and	O
for	O
the	O
multi-class	B
case	O
in	O
section	O
3.5.	O
the	O
ep	O
method	O
for	O
binary	B
clas-	O
siﬁcation	O
is	O
described	O
in	O
section	O
3.6.	O
relationships	O
between	O
gaussian	O
process	B
classiﬁers	O
and	O
other	O
techniques	O
such	O
as	O
spline	O
classiﬁers	O
,	O
support	B
vector	I
ma-	O
chines	O
and	O
least-squares	B
classiﬁcation	I
are	O
discussed	O
in	O
sections	O
6.3	O
,	O
6.4	O
and	O
6.5	O
respectively	O
.	O
3.4	O
the	O
laplace	O
approximation	O
for	O
the	O
binary	B
gp	O
classiﬁer	B
laplace	O
’	O
s	O
method	O
utilizes	O
a	O
gaussian	O
approximation	O
q	O
(	O
f|x	O
,	O
y	O
)	O
to	O
the	O
poste-	O
rior	O
p	O
(	O
f|x	O
,	O
y	O
)	O
in	O
the	O
integral	O
(	O
3.9	O
)	O
.	O
doing	O
a	O
second	O
order	O
taylor	O
expansion	O
of	O
log	O
p	O
(	O
f|x	O
,	O
y	O
)	O
around	O
the	O
maximum	O
of	O
the	O
posterior	O
,	O
we	O
obtain	O
a	O
gaussian	O
approximation	O
q	O
(	O
f|x	O
,	O
y	O
)	O
=	O
n	O
(	O
f|ˆf	O
,	O
a−1	O
)	O
∝	O
exp	O
(	O
cid:0	O
)	O
−	O
1	O
2	O
(	O
f	O
−	O
ˆf	O
)	O
>	O
a	O
(	O
f	O
−	O
ˆf	O
)	O
(	O
cid:1	O
)	O
,	O
(	O
3.11	O
)	O
where	O
ˆf	O
=	O
argmaxf	O
p	O
(	O
f|x	O
,	O
y	O
)	O
and	O
a	O
=	O
−∇∇	O
log	O
p	O
(	O
f|x	O
,	O
y	O
)	O
|f	O
=ˆf	O
is	O
the	O
hessian	O
of	O
the	O
negative	O
log	O
posterior	O
at	O
that	O
point	O
.	O
the	O
structure	O
of	O
the	O
rest	O
of	O
this	O
section	O
is	O
as	O
follows	O
:	O
in	O
section	O
3.4.1	O
we	O
describe	O
how	O
to	O
ﬁnd	O
ˆf	O
and	O
a.	O
section	O
3.4.2	O
explains	O
how	O
to	O
make	O
predictions	O
having	O
obtained	O
q	O
(	O
f|y	O
)	O
,	O
and	O
section	O
3.4.3	O
gives	O
more	O
implementation	O
details	O
for	O
the	O
laplace	O
gp	O
classiﬁer	B
.	O
the	O
laplace	O
approximation	O
for	O
the	O
marginal	B
likelihood	I
is	O
described	O
in	O
section	O
3.4.4.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
42	O
classiﬁcation	B
(	O
a	O
)	O
,	O
logistic	B
(	O
b	O
)	O
,	O
probit	B
figure	O
3.3	O
:	O
the	O
log	O
likelihood	O
and	O
its	O
derivatives	O
for	O
a	O
single	O
case	O
as	O
a	O
function	B
of	O
zi	O
=	O
yifi	O
,	O
for	O
(	O
a	O
)	O
the	O
logistic	B
,	O
and	O
(	O
b	O
)	O
the	O
cumulative	O
gaussian	O
likelihood	B
.	O
the	O
two	O
likelihood	B
functions	O
are	O
fairly	O
similar	O
,	O
the	O
main	O
qualitative	O
diﬀerence	O
being	O
that	O
for	O
large	O
negative	O
arguments	O
the	O
log	O
logistic	O
behaves	O
linearly	O
whereas	O
the	O
log	O
cumulative	O
gaussian	O
has	O
a	O
quadratic	O
penalty	O
.	O
both	O
likelihoods	O
are	O
log	O
concave	O
.	O
3.4.1	O
posterior	O
by	O
bayes	O
’	O
rule	O
the	O
posterior	O
over	O
the	O
latent	O
variables	O
is	O
given	O
by	O
p	O
(	O
f|x	O
,	O
y	O
)	O
=	O
p	O
(	O
y|f	O
)	O
p	O
(	O
f|x	O
)	O
/p	O
(	O
y|x	O
)	O
,	O
but	O
as	O
p	O
(	O
y|x	O
)	O
is	O
independent	O
of	O
f	O
,	O
we	O
need	O
only	O
consider	O
the	O
un-normalized	O
posterior	O
when	O
maximizing	O
w.r.t	O
.	O
f.	O
taking	O
the	O
logarithm	O
and	O
introducing	O
expression	O
eq	O
.	O
(	O
2.29	O
)	O
for	O
the	O
gp	O
prior	O
gives	O
ψ	O
(	O
f	O
)	O
,	O
log	O
p	O
(	O
y|f	O
)	O
+	O
log	O
p	O
(	O
f|x	O
)	O
=	O
log	O
p	O
(	O
y|f	O
)	O
−	O
1	O
2	O
f	O
>	O
k−1f	O
−	O
1	O
2	O
log	O
|k|	O
−	O
n	O
2	O
log	O
2π	O
.	O
(	O
3.12	O
)	O
diﬀerentiating	O
eq	O
.	O
(	O
3.12	O
)	O
w.r.t	O
.	O
f	O
we	O
obtain	O
∇ψ	O
(	O
f	O
)	O
=	O
∇	O
log	O
p	O
(	O
y|f	O
)	O
−	O
k−1f	O
,	O
∇∇ψ	O
(	O
f	O
)	O
=	O
∇∇	O
log	O
p	O
(	O
y|f	O
)	O
−	O
k−1	O
=	O
−w	O
−	O
k−1	O
,	O
(	O
3.13	O
)	O
(	O
3.14	O
)	O
where	O
w	O
,	O
−∇∇	O
log	O
p	O
(	O
y|f	O
)	O
is	O
diagonal	O
,	O
since	O
the	O
likelihood	B
factorizes	O
over	O
cases	O
(	O
the	O
distribution	O
for	O
yi	O
depends	O
only	O
on	O
fi	O
,	O
not	O
on	O
fj6=i	O
)	O
.	O
note	O
,	O
that	O
if	O
the	O
likelihood	B
p	O
(	O
y|f	O
)	O
is	O
log	O
concave	O
,	O
the	O
diagonal	O
elements	O
of	O
w	O
are	O
non-negative	O
,	O
and	O
the	O
hessian	O
in	O
eq	O
.	O
(	O
3.14	O
)	O
is	O
negative	O
deﬁnite	O
,	O
so	O
that	O
ψ	O
(	O
f	O
)	O
is	O
concave	O
and	O
has	O
a	O
unique	O
maximum	O
(	O
see	B
section	O
a.9	O
for	O
further	O
details	O
)	O
.	O
there	O
are	O
many	O
possible	O
functional	B
forms	O
of	O
the	O
likelihood	B
,	O
which	O
gives	O
the	O
target	O
class	O
probability	B
as	O
a	O
function	B
of	O
the	O
latent	O
variable	O
f.	O
two	O
commonly	O
used	O
likelihood	B
functions	O
are	O
the	O
logistic	B
,	O
and	O
the	O
cumulative	O
gaussian	O
,	O
see	B
figure	O
3.3.	O
the	O
expressions	O
for	O
the	O
log	O
likelihood	O
for	O
these	O
likelihood	B
functions	O
and	O
their	O
ﬁrst	O
and	O
second	O
derivatives	O
w.r.t	O
.	O
the	O
latent	O
variable	O
are	O
given	O
in	O
the	O
un-normalized	O
posterior	O
log	O
likelihoods	O
and	O
their	O
derivatives	O
−202−3−2−101latent	O
times	O
target	O
,	O
zi=yifi	O
log	O
likelihood	O
,	O
log	O
p	O
(	O
yi|fi	O
)	O
log	O
likelihood1st	O
derivative2nd	O
derivative−202−6−4−202latent	O
times	O
target	O
,	O
zi=yifi	O
log	O
likelihood	O
,	O
log	O
p	O
(	O
yi|fi	O
)	O
log	O
likelihood1st	O
derivative2nd	O
derivative	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.4	O
the	O
laplace	O
approximation	O
for	O
the	O
binary	B
gp	O
classiﬁer	B
43	O
following	O
table	O
:	O
log	O
p	O
(	O
yi|fi	O
)	O
−	O
log	O
(	O
cid:0	O
)	O
1	O
+	O
exp	O
(	O
−yifi	O
)	O
(	O
cid:1	O
)	O
∂	O
∂fi	O
log	O
p	O
(	O
yi|fi	O
)	O
ti	O
−	O
πi	O
yin	O
(	O
fi	O
)	O
φ	O
(	O
yifi	O
)	O
∂2	O
log	O
p	O
(	O
yi|fi	O
)	O
∂f	O
2	O
i	O
−πi	O
(	O
1	O
−	O
πi	O
)	O
(	O
3.15	O
)	O
(	O
3.16	O
)	O
log	O
φ	O
(	O
yifi	O
)	O
where	O
we	O
have	O
deﬁned	O
πi	O
=	O
p	O
(	O
yi	O
=	O
1|fi	O
)	O
and	O
t	O
=	O
(	O
y	O
+	O
1	O
)	O
/2	O
.	O
at	O
the	O
maximum	O
of	O
ψ	O
(	O
f	O
)	O
we	O
have	O
−	O
n	O
(	O
fi	O
)	O
2	O
φ	O
(	O
yifi	O
)	O
2	O
−	O
yifin	O
(	O
fi	O
)	O
∇ψ	O
=	O
0	O
=⇒	O
ˆf	O
=	O
k	O
(	O
cid:0	O
)	O
∇	O
log	O
p	O
(	O
y|ˆf	O
)	O
(	O
cid:1	O
)	O
,	O
(	O
3.17	O
)	O
as	O
a	O
self-consistent	O
equation	O
for	O
ˆf	O
(	O
but	O
since	O
∇	O
log	O
p	O
(	O
y|ˆf	O
)	O
is	O
a	O
non-linear	O
function	B
of	O
ˆf	O
,	O
eq	O
.	O
(	O
3.17	O
)	O
can	O
not	O
be	O
solved	O
directly	O
)	O
.	O
to	O
ﬁnd	O
the	O
maximum	O
of	O
ψ	O
we	O
use	O
newton	O
’	O
s	O
method	O
,	O
with	O
the	O
iteration	O
φ	O
(	O
yifi	O
)	O
f	O
new	O
=	O
f	O
−	O
(	O
∇∇ψ	O
)	O
−1∇ψ	O
=	O
f	O
+	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
(	O
∇	O
log	O
p	O
(	O
y|f	O
)	O
−	O
k−1f	O
)	O
=	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
(	O
cid:0	O
)	O
w	O
f	O
+	O
∇	O
log	O
p	O
(	O
y|f	O
)	O
(	O
cid:1	O
)	O
.	O
(	O
3.18	O
)	O
to	O
gain	O
more	O
intuition	O
about	O
this	O
update	O
,	O
let	O
us	O
consider	O
what	O
happens	O
to	O
datapoints	O
that	O
are	O
well-explained	O
under	O
f	O
so	O
that	O
∂	O
log	O
p	O
(	O
yi|fi	O
)	O
/∂fi	O
and	O
wii	O
are	O
close	O
to	O
zero	O
for	O
these	O
points	O
.	O
as	O
an	O
approximation	O
,	O
break	O
f	O
into	O
two	O
subvectors	O
,	O
f1	O
that	O
corresponds	O
to	O
points	O
that	O
are	O
not	O
well-explained	O
,	O
and	O
f2	O
to	O
those	O
that	O
are	O
.	O
then	O
it	O
is	O
easy	O
to	O
show	O
(	O
see	B
exercise	O
3.10.4	O
)	O
that	O
=	O
k11	O
(	O
i11	O
+	O
w11k11	O
)	O
−1	O
(	O
cid:0	O
)	O
w11f1	O
+	O
∇	O
log	O
p	O
(	O
y1|f1	O
)	O
(	O
cid:1	O
)	O
,	O
(	O
3.19	O
)	O
f	O
new	O
1	O
f	O
new	O
2	O
=	O
k21k−1	O
11	O
f	O
new	O
1	O
,	O
where	O
k21	O
denotes	O
the	O
n2	O
×	O
n1	O
block	O
of	O
k	O
containing	O
the	O
covariance	B
between	O
is	O
computed	O
by	O
ignoring	O
the	O
two	O
groups	O
of	O
points	O
,	O
etc	O
.	O
this	O
means	O
that	O
f	O
new	O
using	O
the	O
entirely	O
the	O
well-explained	O
points	O
,	O
and	O
f	O
new	O
usual	O
gp	O
prediction	B
methods	O
(	O
i.e	O
.	O
treating	O
these	O
points	O
like	O
test	O
points	O
)	O
.	O
of	O
course	O
,	O
if	O
the	O
predictions	O
of	O
f	O
new	O
fail	O
to	O
match	O
the	O
targets	O
correctly	O
they	O
would	O
cease	O
to	O
be	O
well-explained	O
and	O
so	O
be	O
updated	O
on	O
the	O
next	O
iteration	O
.	O
is	O
predicted	O
from	O
f	O
new	O
2	O
1	O
2	O
1	O
having	O
found	O
the	O
maximum	O
posterior	O
ˆf	O
,	O
we	O
can	O
now	O
specify	O
the	O
laplace	O
approximation	O
to	O
the	O
posterior	O
as	O
a	O
gaussian	O
with	O
mean	O
ˆf	O
and	O
covariance	B
matrix	I
given	O
by	O
the	O
negative	O
inverse	O
hessian	O
of	O
ψ	O
from	O
eq	O
.	O
(	O
3.14	O
)	O
q	O
(	O
f|x	O
,	O
y	O
)	O
=	O
n	O
(	O
cid:0	O
)	O
ˆf	O
,	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
(	O
cid:1	O
)	O
.	O
(	O
3.20	O
)	O
one	O
problem	O
with	O
the	O
laplace	O
approximation	O
is	O
that	O
it	O
is	O
essentially	O
un-	O
controlled	O
,	O
in	O
that	O
the	O
hessian	O
(	O
evaluated	O
at	O
ˆf	O
)	O
may	O
give	O
a	O
poor	O
approximation	O
to	O
the	O
true	O
shape	O
of	O
the	O
posterior	O
.	O
the	O
peak	O
could	O
be	O
much	O
broader	O
or	O
nar-	O
rower	O
than	O
the	O
hessian	O
indicates	O
,	O
or	O
it	O
could	O
be	O
a	O
skew	O
peak	O
,	O
while	O
the	O
laplace	O
approximation	O
assumes	O
it	O
has	O
elliptical	O
contours	O
.	O
newton	O
’	O
s	O
method	O
intuition	O
on	O
inﬂuence	O
of	O
well-explained	O
points	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
44	O
classiﬁcation	B
3.4.2	O
predictions	O
latent	O
mean	O
the	O
posterior	O
mean	O
for	O
f∗	O
under	O
the	O
laplace	O
approximation	O
can	O
be	O
expressed	O
by	O
combining	O
the	O
gp	O
predictive	B
mean	O
eq	O
.	O
(	O
2.25	O
)	O
with	O
eq	O
.	O
(	O
3.17	O
)	O
into	O
eq	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
k	O
(	O
x∗	O
)	O
>	O
k−1ˆf	O
=	O
k	O
(	O
x∗	O
)	O
>	O
∇	O
log	O
p	O
(	O
y|ˆf	O
)	O
.	O
(	O
3.21	O
)	O
compare	O
this	O
with	O
the	O
exact	O
mean	O
,	O
given	O
by	O
opper	O
and	O
winther	O
[	O
2000	O
]	O
as	O
ep	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
=	O
e	O
[	O
f∗|f	O
,	O
x	O
,	O
x∗	O
]	O
p	O
(	O
f|x	O
,	O
y	O
)	O
df	O
k	O
(	O
x∗	O
)	O
>	O
k−1f	O
p	O
(	O
f|x	O
,	O
y	O
)	O
df	O
=	O
k	O
(	O
x∗	O
)	O
>	O
k−1e	O
[	O
f|x	O
,	O
y	O
]	O
,	O
(	O
3.22	O
)	O
z	O
z	O
where	O
we	O
have	O
used	O
the	O
fact	O
that	O
for	O
a	O
gp	O
e	O
[	O
f∗|f	O
,	O
x	O
,	O
x∗	O
]	O
=	O
k	O
(	O
x∗	O
)	O
>	O
k−1f	O
and	O
have	O
let	O
e	O
[	O
f|x	O
,	O
y	O
]	O
denote	O
the	O
posterior	O
mean	O
of	O
f	O
given	O
x	O
and	O
y.	O
notice	O
the	O
similarity	O
between	O
the	O
middle	O
expression	O
of	O
eq	O
.	O
(	O
3.21	O
)	O
and	O
eq	O
.	O
(	O
3.22	O
)	O
,	O
where	O
the	O
exact	O
(	O
intractable	O
)	O
average	O
e	O
[	O
f|x	O
,	O
y	O
]	O
has	O
been	O
replaced	O
with	O
the	O
modal	O
value	O
ˆf	O
=	O
eq	O
[	O
f|x	O
,	O
y	O
]	O
.	O
a	O
simple	O
observation	O
from	O
eq	O
.	O
(	O
3.21	O
)	O
is	O
that	O
positive	O
training	O
examples	O
will	O
give	O
rise	O
to	O
a	O
positive	O
coeﬃcient	O
for	O
their	O
kernel	B
function	O
(	O
as	O
∇i	O
log	O
p	O
(	O
yi|fi	O
)	O
>	O
0	O
in	O
this	O
case	O
)	O
,	O
while	O
negative	O
examples	O
will	O
give	O
rise	O
to	O
a	O
negative	O
coeﬃcient	O
;	O
this	O
is	O
analogous	O
to	O
the	O
solution	O
to	O
the	O
support	B
vector	I
machine	I
,	O
see	B
eq	O
.	O
(	O
6.34	O
)	O
.	O
also	O
note	O
that	O
training	O
points	O
which	O
have	O
∇i	O
log	O
p	O
(	O
yi|fi	O
)	O
’	O
0	O
(	O
i.e	O
.	O
that	O
are	O
well-explained	O
under	O
ˆf	O
)	O
do	O
not	O
contribute	O
strongly	O
to	O
predictions	O
at	O
novel	O
test	O
points	O
;	O
this	O
is	O
similar	O
to	O
the	O
behaviour	O
of	O
non-support	O
vectors	O
in	O
the	O
support	B
vector	I
machine	I
(	O
see	B
section	O
6.4	O
)	O
.	O
we	O
can	O
also	O
compute	O
vq	O
[	O
f∗|x	O
,	O
y	O
]	O
,	O
the	O
variance	O
of	O
f∗|x	O
,	O
y	O
under	O
the	O
gaussian	O
approximation	O
.	O
this	O
comprises	O
of	O
two	O
terms	O
,	O
i.e	O
.	O
vq	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
ep	O
(	O
f∗|x	O
,	O
x∗	O
,	O
f	O
)	O
[	O
(	O
f∗	O
−	O
e	O
[	O
f∗|x	O
,	O
x∗	O
,	O
f	O
]	O
)	O
2	O
]	O
+	O
eq	O
(	O
f|x	O
,	O
y	O
)	O
[	O
(	O
e	O
[	O
f∗|x	O
,	O
x∗	O
,	O
f	O
]	O
−	O
e	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
)	O
2	O
]	O
.	O
(	O
3.23	O
)	O
the	O
ﬁrst	O
term	O
is	O
due	O
to	O
the	O
variance	O
of	O
f∗	O
if	O
we	O
condition	O
on	O
a	O
particular	O
value	O
of	O
f	O
,	O
and	O
is	O
given	O
by	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
k	O
(	O
x∗	O
)	O
>	O
k−1k	O
(	O
x∗	O
)	O
,	O
cf	O
.	O
eq	O
.	O
(	O
2.19	O
)	O
.	O
the	O
second	O
term	O
in	O
eq	O
.	O
(	O
3.23	O
)	O
is	O
due	O
to	O
the	O
fact	O
that	O
e	O
[	O
f∗|x	O
,	O
x∗	O
,	O
f	O
]	O
=	O
k	O
(	O
x∗	O
)	O
>	O
k−1f	O
depends	O
on	O
f	O
and	O
thus	O
there	O
is	O
an	O
additional	O
term	O
of	O
k	O
(	O
x∗	O
)	O
>	O
k−1	O
cov	O
(	O
f|x	O
,	O
y	O
)	O
k−1k	O
(	O
x∗	O
)	O
.	O
under	O
the	O
gaussian	O
approximation	O
cov	O
(	O
f|x	O
,	O
y	O
)	O
=	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
,	O
and	O
thus	O
∗	O
k−1	O
(	O
k−1	O
+	O
w	O
)	O
−1k−1k∗	O
vq	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−k	O
>	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−k	O
>	O
∗	O
k−1k∗	O
+	O
k	O
>	O
∗	O
(	O
k	O
+	O
w	O
−1	O
)	O
−1k∗	O
,	O
(	O
3.24	O
)	O
where	O
the	O
last	O
line	O
is	O
obtained	O
using	O
the	O
matrix	B
inversion	O
lemma	O
eq	O
.	O
(	O
a.9	O
)	O
.	O
given	O
the	O
mean	O
and	O
variance	O
of	O
f∗	O
,	O
we	O
make	O
predictions	O
by	O
computing	O
¯π∗	O
’	O
eq	O
[	O
π∗|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
σ	O
(	O
f∗	O
)	O
q	O
(	O
f∗|x	O
,	O
y	O
,	O
x∗	O
)	O
df∗	O
,	O
(	O
3.25	O
)	O
z	O
sign	O
of	O
kernel	B
coeﬃcients	O
latent	O
variance	O
averaged	B
predictive	O
probability	B
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.4	O
the	O
laplace	O
approximation	O
for	O
the	O
binary	B
gp	O
classiﬁer	B
45	O
map	O
prediction	B
identical	O
binary	B
decisions	O
where	O
q	O
(	O
f∗|x	O
,	O
y	O
,	O
x∗	O
)	O
is	O
gaussian	O
with	O
mean	O
and	O
variance	O
given	O
by	O
equations	O
3.21	O
and	O
3.24	O
respectively	O
.	O
notice	O
that	O
because	O
of	O
the	O
non-linear	O
form	O
of	O
the	O
sigmoid	O
the	O
predictive	B
probability	O
from	O
eq	O
.	O
(	O
3.25	O
)	O
is	O
diﬀerent	O
from	O
the	O
sigmoid	O
of	O
the	O
expectation	O
of	O
f	O
:	O
ˆπ∗	O
=	O
σ	O
(	O
eq	O
[	O
f∗|y	O
]	O
)	O
.	O
we	O
will	O
call	O
the	O
latter	O
the	O
map	O
prediction	B
to	O
distinguish	O
it	O
from	O
the	O
averaged	B
predictions	O
from	O
eq	O
.	O
(	O
3.25	O
)	O
.	O
in	O
fact	O
,	O
as	O
shown	O
in	O
bishop	O
[	O
1995	O
,	O
sec	O
.	O
10.3	O
]	O
,	O
the	O
predicted	O
test	O
labels	O
given	O
by	O
choosing	O
the	O
class	O
of	O
highest	O
probability	B
obtained	O
by	O
averaged	B
and	O
map	O
predictions	O
are	O
identical	O
for	O
binary	B
9	O
classiﬁcation	B
.	O
to	O
see	B
this	O
,	O
note	O
that	O
the	O
decision	O
boundary	O
using	O
the	O
the	O
map	O
value	O
eq	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
corre-	O
sponds	O
to	O
σ	O
(	O
eq	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
)	O
=	O
1/2	O
or	O
eq	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
0.	O
the	O
decision	O
bound-	O
ary	O
of	O
the	O
averaged	B
prediction	O
,	O
eq	O
[	O
π∗|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
1/2	O
,	O
also	O
corresponds	O
to	O
eq	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
0.	O
this	O
follows	O
from	O
the	O
fact	O
that	O
σ	O
(	O
f∗	O
)	O
−	O
1/2	O
is	O
antisym-	O
metric	O
while	O
q	O
(	O
f∗|x	O
,	O
y	O
,	O
x∗	O
)	O
is	O
symmetric	O
.	O
thus	O
if	O
we	O
are	O
concerned	O
only	O
about	O
the	O
most	O
probable	O
classiﬁcation	B
,	O
it	O
is	O
not	O
necessary	O
to	O
compute	O
predictions	O
using	O
eq	O
.	O
(	O
3.25	O
)	O
.	O
however	O
,	O
as	O
soon	O
as	O
we	O
also	O
need	O
a	O
conﬁdence	O
in	O
the	O
prediction	B
(	O
e.g	O
.	O
if	O
we	O
are	O
concerned	O
about	O
a	O
reject	B
option	I
)	O
we	O
need	O
eq	O
[	O
π∗|x	O
,	O
y	O
,	O
x∗	O
]	O
.	O
if	O
σ	O
(	O
z	O
)	O
is	O
the	O
cumulative	O
gaussian	O
function	B
then	O
eq	O
.	O
(	O
3.25	O
)	O
can	O
be	O
computed	O
analytically	O
,	O
as	O
shown	O
in	O
section	O
3.9.	O
on	O
the	O
other	O
hand	O
if	O
σ	O
is	O
the	O
logistic	B
function	I
then	O
we	O
need	O
to	O
resort	O
to	O
sampling	O
methods	O
or	O
analytical	O
approximations	O
to	O
compute	O
this	O
one-dimensional	O
integral	O
.	O
one	O
attractive	O
method	O
is	O
to	O
note	O
that	O
the	O
logistic	B
function	I
λ	O
(	O
z	O
)	O
is	O
the	O
c.d.f	O
.	O
(	O
cumulative	O
density	O
function	B
)	O
corresponding	O
to	O
the	O
p.d.f	O
.	O
(	O
probability	B
density	O
function	B
)	O
p	O
(	O
z	O
)	O
=	O
sech2	O
(	O
z/2	O
)	O
/4	O
;	O
this	O
is	O
known	O
as	O
the	O
logistic	B
or	O
sech-squared	O
distribution	O
,	O
see	B
johnson	O
et	O
al	O
.	O
[	O
1995	O
,	O
ch	O
.	O
23	O
]	O
.	O
then	O
by	O
approximating	O
p	O
(	O
z	O
)	O
as	O
a	O
mixture	O
of	O
gaussians	O
,	O
one	O
can	O
approximate	O
λ	O
(	O
z	O
)	O
by	O
a	O
linear	B
combination	O
of	O
error	B
functions	O
.	O
this	O
approximation	O
was	O
used	O
by	O
williams	O
and	O
barber	O
[	O
1998	O
,	O
app	O
.	O
a	O
]	O
and	O
wood	O
and	O
kohn	O
[	O
1998	O
]	O
.	O
another	O
approximation	O
suggested	O
in	O
mackay	O
[	O
1992d	O
]	O
is	O
¯π∗	O
’	O
λ	O
(	O
κ	O
(	O
f∗|y	O
)	O
¯f∗	O
)	O
,	O
where	O
κ2	O
(	O
f∗|y	O
)	O
=	O
(	O
1	O
+	O
πvq	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
/8	O
)	O
−1	O
.	O
the	O
eﬀect	O
of	O
the	O
latent	O
predictive	O
variance	O
is	O
,	O
as	O
the	O
approximation	O
suggests	O
,	O
to	O
“	O
soften	O
”	O
the	O
prediction	B
that	O
would	O
be	O
obtained	O
using	O
the	O
map	O
prediction	B
ˆπ∗	O
=	O
λ	O
(	O
¯f∗	O
)	O
,	O
i.e	O
.	O
to	O
move	O
it	O
towards	O
1/2	O
.	O
3.4.3	O
implementation	O
we	O
give	O
implementations	O
for	O
ﬁnding	O
the	O
laplace	O
approximation	O
in	O
algorithm	O
3.1	O
and	O
for	O
making	O
predictions	O
in	O
algorithm	O
3.2.	O
care	O
is	O
taken	O
to	O
avoid	O
numer-	O
ically	O
unstable	O
computations	O
while	O
minimizing	O
the	O
computational	O
eﬀort	O
;	O
both	O
can	O
be	O
achieved	O
simultaneously	O
.	O
it	O
turns	O
out	O
that	O
several	O
of	O
the	O
desired	O
terms	O
can	O
be	O
expressed	O
in	O
terms	O
of	O
the	O
symmetric	O
positive	B
deﬁnite	I
matrix	I
b	O
=	O
i	O
+	O
w	O
(	O
3.26	O
)	O
computation	O
of	O
which	O
costs	O
only	O
o	O
(	O
n2	O
)	O
,	O
since	O
w	O
is	O
diagonal	O
.	O
the	O
b	O
matrix	B
has	O
eigenvalues	O
bounded	O
below	O
by	O
1	O
and	O
bounded	O
above	O
by	O
1	O
+	O
n	O
maxij	O
(	O
kij	O
)	O
/4	O
,	O
so	O
for	O
many	O
covariance	B
functions	O
b	O
is	O
guaranteed	O
to	O
be	O
well-conditioned	O
,	O
and	O
it	O
is	O
1	O
2	O
kw	O
1	O
2	O
,	O
9for	O
multi-class	B
predictions	O
discussed	O
in	O
section	O
3.5	O
the	O
situation	O
is	O
more	O
complicated	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
46	O
classiﬁcation	B
1	O
2	O
kw	O
1	O
2	O
)	O
b	O
=	O
i	O
+	O
w	O
1	O
2	O
kw	O
1	O
2	O
)	O
input	O
:	O
k	O
(	O
covariance	B
matrix	I
)	O
,	O
y	O
(	O
±1	O
targets	O
)	O
,	O
p	O
(	O
y|f	O
)	O
(	O
likelihood	B
function	O
)	O
initialization	O
newton	O
iteration	O
eval	O
.	O
w	O
e.g	O
.	O
using	O
eq	O
.	O
(	O
3.15	O
)	O
or	O
(	O
3.16	O
)	O
2	O
:	O
f	O
:	O
=	O
0	O
repeat	O
4	O
:	O
w	O
:	O
=	O
−∇∇	O
log	O
p	O
(	O
y|f	O
)	O
l	O
:	O
=	O
cholesky	O
(	O
i	O
+	O
w	O
b	O
:	O
=	O
w	O
f	O
+	O
∇	O
log	O
p	O
(	O
y|f	O
)	O
a	O
:	O
=	O
b	O
−	O
w	O
2	O
l	O
>	O
\	O
(	O
l\	O
(	O
w	O
f	O
:	O
=	O
ka	O
6	O
:	O
1	O
1	O
8	O
:	O
2	O
kb	O
)	O
)	O
i	O
log	O
lii	O
objective	O
:	O
−	O
1	O
2	O
a	O
>	O
f	O
+	O
log	O
p	O
(	O
y|f	O
)	O
−p	O
eq	O
.	O
(	O
3.18	O
)	O
using	O
eq	O
.	O
(	O
3.27	O
)	O
2	O
a	O
>	O
f	O
+	O
log	O
p	O
(	O
y|f	O
)	O
until	O
convergence	O
10	O
:	O
log	O
q	O
(	O
y|x	O
,	O
θ	O
)	O
:	O
=	O
−	O
1	O
eq	O
.	O
(	O
3.32	O
)	O
return	O
:	O
ˆf	O
:	O
=	O
f	O
(	O
post	O
.	O
mode	O
)	O
,	O
log	O
q	O
(	O
y|x	O
,	O
θ	O
)	O
(	O
approx	O
.	O
log	O
marg	O
.	O
likelihood	B
)	O
algorithm	O
3.1	O
:	O
mode-ﬁnding	O
for	O
binary	B
laplace	O
gpc	O
.	O
commonly	O
used	O
convergence	O
criteria	O
depend	O
on	O
the	O
diﬀerence	O
in	O
successive	O
values	O
of	O
the	O
objective	O
function	B
ψ	O
(	O
f	O
)	O
from	O
eq	O
.	O
(	O
3.12	O
)	O
,	O
the	O
magnitude	O
of	O
the	O
gradient	O
vector	O
∇ψ	O
(	O
f	O
)	O
from	O
eq	O
.	O
(	O
3.13	O
)	O
and/or	O
the	O
magnitude	O
of	O
the	O
diﬀerence	O
in	O
successive	O
values	O
of	O
f	O
.	O
in	O
a	O
practical	O
implementation	O
one	O
needs	O
to	O
secure	O
against	O
divergence	O
by	O
checking	O
that	O
each	O
iteration	O
leads	O
to	O
an	O
increase	O
in	O
the	O
objective	O
(	O
and	O
trying	O
a	O
smaller	O
step	O
size	O
if	O
not	O
)	O
.	O
the	O
computational	O
complexity	O
is	O
dominated	O
by	O
the	O
cholesky	O
decomposition	O
in	O
line	O
5	O
which	O
takes	O
n3/6	O
operations	O
(	O
times	O
the	O
number	O
of	O
newton	O
iterations	O
)	O
,	O
all	O
other	O
operations	O
are	O
at	O
most	O
quadratic	O
in	O
n.	O
thus	O
numerically	O
safe	O
to	O
compute	O
its	O
cholesky	O
decomposition	O
ll	O
>	O
=	O
b	O
,	O
which	O
is	O
useful	O
in	O
computing	O
terms	O
involving	O
b−1	O
and	O
|b|	O
.	O
the	O
mode-ﬁnding	O
procedure	O
uses	O
the	O
newton	O
iteration	O
given	O
in	O
eq	O
.	O
(	O
3.18	O
)	O
,	O
involving	O
the	O
matrix	B
(	O
k−1	O
+w	O
)	O
−1	O
.	O
using	O
the	O
matrix	B
inversion	O
lemma	O
eq	O
.	O
(	O
a.9	O
)	O
we	O
get	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
=	O
k	O
−	O
kw	O
(	O
3.27	O
)	O
where	O
b	O
is	O
given	O
in	O
eq	O
.	O
(	O
3.26	O
)	O
.	O
the	O
advantage	O
is	O
that	O
whereas	O
k	O
may	O
have	O
eigenvalues	O
arbitrarily	O
close	O
to	O
zero	O
(	O
and	O
thus	O
be	O
numerically	O
unstable	O
to	O
invert	O
)	O
,	O
we	O
can	O
safely	O
work	O
with	O
b.	O
in	O
addition	O
,	O
algorithm	O
3.1	O
keeps	O
the	O
vector	O
a	O
=	O
k−1f	O
in	O
addition	O
to	O
f	O
,	O
as	O
this	O
allows	O
evaluation	O
of	O
the	O
part	O
of	O
the	O
objective	O
ψ	O
(	O
f	O
)	O
in	O
eq	O
.	O
(	O
3.12	O
)	O
which	O
depends	O
on	O
f	O
without	O
explicit	O
reference	O
to	O
k−1	O
(	O
again	O
to	O
avoid	O
possible	O
numerical	O
problems	O
)	O
.	O
2	O
b−1w	O
1	O
2	O
k	O
,	O
1	O
similarly	O
,	O
for	O
the	O
computation	O
of	O
the	O
predictive	B
variance	O
vq	O
[	O
f∗|y	O
]	O
from	O
eq	O
.	O
(	O
3.24	O
)	O
we	O
need	O
to	O
evaluate	O
a	O
quadratic	B
form	I
involving	O
the	O
matrix	B
(	O
k	O
+	O
w	O
−1	O
)	O
−1	O
.	O
re-	O
writing	O
this	O
as	O
(	O
k	O
+	O
w	O
−1	O
)	O
−1	O
=	O
w	O
1	O
2	O
w	O
−	O
1	O
2	O
(	O
k	O
+	O
w	O
−1	O
)	O
−1w	O
−	O
1	O
2	O
w	O
1	O
2	O
=	O
w	O
1	O
2	O
b−1w	O
1	O
2	O
(	O
3.28	O
)	O
achieves	O
numerical	O
stability	O
(	O
as	O
opposed	O
to	O
inverting	O
w	O
itself	O
,	O
which	O
may	O
have	O
arbitrarily	O
small	O
eigenvalues	O
)	O
.	O
thus	O
the	O
predictive	B
variance	O
from	O
eq	O
.	O
(	O
3.24	O
)	O
can	O
be	O
computed	O
as	O
vq	O
[	O
f∗|y	O
]	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
k	O
(	O
x∗	O
)	O
>	O
w	O
1	O
2	O
(	O
ll	O
>	O
)	O
−1w	O
1	O
2	O
k	O
(	O
x∗	O
)	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
v	O
>	O
v	O
,	O
where	O
v	O
=	O
l\	O
(	O
w	O
1	O
2	O
k	O
(	O
x∗	O
)	O
)	O
,	O
(	O
3.29	O
)	O
which	O
was	O
also	O
used	O
by	O
seeger	O
[	O
2003	O
,	O
p.	O
27	O
]	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.4	O
the	O
laplace	O
approximation	O
for	O
the	O
binary	B
gp	O
classiﬁer	B
47	O
input	O
:	O
ˆf	O
(	O
mode	O
)	O
,	O
x	O
(	O
inputs	O
)	O
,	O
y	O
(	O
±1	O
targets	O
)	O
,	O
k	O
(	O
covariance	B
function	I
)	O
,	O
p	O
(	O
y|f	O
)	O
(	O
likelihood	B
function	O
)	O
,	O
x∗	O
test	O
input	O
2	O
:	O
w	O
:	O
=	O
−∇∇	O
log	O
p	O
(	O
y|ˆf	O
)	O
1	O
l	O
:	O
=	O
cholesky	O
(	O
i	O
+	O
w	O
2	O
kw	O
4	O
:	O
¯f∗	O
:	O
=	O
k	O
(	O
x∗	O
)	O
>	O
∇	O
log	O
p	O
(	O
y|ˆf	O
)	O
6	O
:	O
v	O
[	O
f∗	O
]	O
:	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
v	O
>	O
v	O
v	O
:	O
=	O
l\	O
(	O
cid:0	O
)	O
w	O
¯π∗	O
:	O
=r	O
σ	O
(	O
z	O
)	O
n	O
(	O
z|	O
¯f∗	O
,	O
v	O
[	O
f∗	O
]	O
)	O
dz	O
2	O
k	O
(	O
x∗	O
)	O
(	O
cid:1	O
)	O
1	O
1	O
2	O
)	O
o	O
b	O
=	O
i	O
+	O
w	O
1	O
2	O
kw	O
1	O
2	O
eq	O
.	O
(	O
3.21	O
)	O
eq	O
.	O
(	O
3.24	O
)	O
using	O
eq	O
.	O
(	O
3.29	O
)	O
eq	O
.	O
(	O
3.25	O
)	O
8	O
:	O
return	O
:	O
¯π∗	O
(	O
predictive	B
class	O
probability	B
(	O
for	O
class	O
1	O
)	O
)	O
algorithm	O
3.2	O
:	O
predictions	O
for	O
binary	B
laplace	O
gpc	O
.	O
the	O
posterior	O
mode	O
ˆf	O
(	O
which	O
can	O
be	O
computed	O
using	O
algorithm	O
3.1	O
)	O
is	O
input	O
.	O
for	O
multiple	O
test	O
inputs	O
lines	O
4−	O
7	O
are	O
applied	O
to	O
each	O
test	O
input	O
.	O
computational	O
complexity	O
is	O
n3/6	O
operations	O
once	O
(	O
line	O
3	O
)	O
plus	O
n2	O
operations	O
per	O
test	O
case	O
(	O
line	O
5	O
)	O
.	O
the	O
one-dimensional	O
integral	O
in	O
line	O
7	O
can	O
be	O
done	O
analytically	O
for	O
cumulative	O
gaussian	O
likelihood	B
,	O
otherwise	O
it	O
is	O
computed	O
using	O
an	O
approximation	O
or	O
numerical	O
quadrature	O
.	O
in	O
practice	O
we	O
compute	O
the	O
cholesky	O
decomposition	O
ll	O
>	O
=	O
b	O
during	O
the	O
newton	O
steps	O
in	O
algorithm	O
3.1	O
,	O
which	O
can	O
be	O
re-used	O
to	O
compute	O
the	O
predictive	B
variance	O
by	O
doing	O
backsubstitution	O
with	O
l	O
as	O
discussed	O
above	O
.	O
in	O
addition	O
,	O
l	O
may	O
again	O
be	O
re-used	O
to	O
compute	O
|in	O
+	O
w	O
2|	O
=	O
|b|	O
(	O
needed	O
for	O
the	O
computation	O
of	O
the	O
marginal	B
likelihood	I
eq	O
.	O
(	O
3.32	O
)	O
)	O
as	O
log	O
|b|	O
=	O
2p	O
log	O
lii	O
.	O
to	O
save	O
computation	O
,	O
one	O
could	O
use	O
an	O
incomplete	O
cholesky	O
factorization	O
in	O
the	O
newton	O
steps	O
,	O
as	O
suggested	O
by	O
fine	O
and	O
scheinberg	O
[	O
2002	O
]	O
.	O
1	O
2	O
kw	O
1	O
sometimes	O
it	O
is	O
suggested	O
that	O
it	O
can	O
be	O
useful	O
to	O
replace	O
k	O
by	O
k	O
+i	O
where	O
	O
is	O
a	O
small	O
constant	O
,	O
to	O
improve	O
the	O
numerical	O
conditioning10	O
of	O
k.	O
however	O
,	O
by	O
taking	O
care	O
with	O
the	O
implementation	O
details	O
as	O
above	O
this	O
should	O
not	O
be	O
necessary	O
.	O
3.4.4	O
marginal	B
likelihood	I
incomplete	O
cholesky	O
factorization	O
z	O
it	O
will	O
also	O
be	O
useful	O
(	O
particularly	O
for	O
chapter	O
5	O
)	O
to	O
compute	O
the	O
laplace	O
ap-	O
proximation	O
of	O
the	O
marginal	B
likelihood	I
p	O
(	O
y|x	O
)	O
.	O
(	O
for	O
the	O
regression	B
case	O
with	O
gaussian	O
noise	O
the	O
marginal	B
likelihood	I
can	O
again	O
be	O
calculated	O
analytically	O
,	O
see	B
eq	O
.	O
(	O
2.30	O
)	O
.	O
)	O
we	O
have	O
p	O
(	O
y|x	O
)	O
=	O
p	O
(	O
y|f	O
)	O
p	O
(	O
f|x	O
)	O
df	O
=	O
(	O
3.30	O
)	O
using	O
a	O
taylor	O
expansion	O
of	O
ψ	O
(	O
f	O
)	O
locally	O
around	O
ˆf	O
we	O
obtain	O
ψ	O
(	O
f	O
)	O
’	O
ψ	O
(	O
ˆf	O
)	O
−	O
2	O
(	O
f	O
−ˆf	O
)	O
>	O
a	O
(	O
f	O
−ˆf	O
)	O
and	O
thus	O
an	O
approximation	O
q	O
(	O
y|x	O
)	O
to	O
the	O
marginal	B
likelihood	I
1	O
as	O
p	O
(	O
y|x	O
)	O
’	O
q	O
(	O
y|x	O
)	O
=	O
exp	O
(	O
cid:0	O
)	O
ψ	O
(	O
ˆf	O
)	O
(	O
cid:1	O
)	O
z	O
exp	O
(	O
cid:0	O
)	O
−	O
1	O
2	O
(	O
f	O
−	O
ˆf	O
)	O
>	O
a	O
(	O
f	O
−	O
ˆf	O
)	O
(	O
cid:1	O
)	O
df	O
.	O
(	O
3.31	O
)	O
z	O
exp	O
(	O
cid:0	O
)	O
ψ	O
(	O
f	O
)	O
(	O
cid:1	O
)	O
df	O
.	O
10neal	O
[	O
1999	O
]	O
refers	O
to	O
this	O
as	O
adding	O
“	O
jitter	B
”	O
in	O
the	O
context	O
of	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
based	O
inference	O
;	O
in	O
his	O
work	O
the	O
latent	O
variables	O
f	O
are	O
explicitly	O
represented	O
in	O
the	O
markov	O
chain	O
which	O
makes	O
addition	O
of	O
jitter	B
diﬃcult	O
to	O
avoid	O
.	O
within	O
the	O
analytical	O
approximations	O
of	O
the	O
distribution	O
of	O
f	O
considered	O
here	O
,	O
jitter	B
is	O
unnecessary	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
48	O
classiﬁcation	B
this	O
gaussian	O
integral	O
can	O
be	O
evaluated	O
analytically	O
to	O
obtain	O
an	O
approximation	O
to	O
the	O
log	O
marginal	O
likelihood	B
log	O
q	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
−	O
1	O
ˆf	O
>	O
k−1ˆf	O
+	O
log	O
p	O
(	O
y|ˆf	O
)	O
−	O
1	O
(	O
3.32	O
)	O
where	O
|b|	O
=	O
|k|	O
·	O
|k−1	O
+	O
w|	O
=	O
|in	O
+	O
w	O
2|	O
,	O
and	O
θ	O
is	O
a	O
vector	O
of	O
hyper-	O
parameters	O
of	O
the	O
covariance	B
function	I
(	O
which	O
have	O
previously	O
been	O
suppressed	O
from	O
the	O
notation	O
for	O
brevity	O
)	O
.	O
1	O
2	O
kw	O
2	O
log	O
|b|	O
,	O
2	O
1	O
∗	O
3.5	O
multi-class	B
laplace	O
approximation	O
f	O
=	O
(	O
cid:0	O
)	O
f	O
1	O
(	O
cid:1	O
)	O
>	O
our	O
presentation	O
follows	O
williams	O
and	O
barber	O
[	O
1998	O
]	O
.	O
we	O
ﬁrst	O
introduce	O
the	O
vector	O
of	O
latent	O
function	O
values	O
at	O
all	O
n	O
training	O
points	O
and	O
for	O
all	O
c	O
classes	O
.	O
(	O
3.33	O
)	O
n	O
,	O
f	O
2	O
1	O
,	O
.	O
.	O
.	O
,	O
f	O
2	O
n	O
,	O
.	O
.	O
.	O
,	O
f	O
c	O
1	O
,	O
.	O
.	O
.	O
,	O
f	O
c	O
n	O
1	O
,	O
.	O
.	O
.	O
,	O
f	O
1	O
thus	O
f	O
has	O
length	O
cn	O
.	O
in	O
the	O
following	O
we	O
will	O
generally	O
refer	O
to	O
quantities	O
pertaining	O
to	O
a	O
particular	O
class	O
with	O
superscript	O
c	O
,	O
and	O
a	O
particular	O
case	O
by	O
subscript	O
i	O
(	O
as	O
usual	O
)	O
;	O
thus	O
e.g	O
.	O
the	O
vector	O
of	O
c	O
latents	O
for	O
a	O
particular	O
case	O
is	O
fi	O
.	O
however	O
,	O
as	O
an	O
exception	O
,	O
vectors	O
or	O
matrices	O
formed	O
from	O
the	O
covariance	B
function	I
for	O
class	O
c	O
will	O
have	O
a	O
subscript	O
c.	O
the	O
prior	O
over	O
f	O
has	O
the	O
form	O
f	O
∼	O
n	O
(	O
0	O
,	O
k	O
)	O
.	O
as	O
we	O
have	O
assumed	O
that	O
the	O
c	O
latent	O
processes	O
are	O
uncorrelated	O
,	O
the	O
covariance	B
matrix	I
k	O
is	O
block	O
diagonal	O
in	O
the	O
matrices	O
k1	O
,	O
.	O
.	O
.	O
,	O
kc	O
.	O
each	O
individual	O
matrix	B
kc	O
expresses	O
the	O
correlations	O
of	O
the	O
latent	O
function	O
values	O
within	O
the	O
class	O
c.	O
note	O
that	O
the	O
covariance	B
functions	O
pertaining	O
to	O
the	O
diﬀerent	O
classes	O
can	O
be	O
diﬀerent	O
.	O
let	O
y	O
be	O
a	O
vector	O
of	O
the	O
same	O
length	O
as	O
f	O
which	O
for	O
each	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
has	O
an	O
entry	O
of	O
1	O
for	O
the	O
class	O
which	O
is	O
the	O
label	O
for	O
example	O
i	O
and	O
0	O
for	O
the	O
other	O
c	O
−	O
1	O
entries	O
.	O
softmax	B
un-normalized	O
posterior	O
let	O
πc	O
i	O
denote	O
output	O
of	O
the	O
softmax	B
at	O
training	O
point	O
i	O
,	O
i.e	O
.	O
p	O
(	O
yc	O
i|fi	O
)	O
=	O
πc	O
i	O
=	O
p	O
i	O
)	O
exp	O
(	O
f	O
c	O
i	O
)	O
.	O
c0	O
exp	O
(	O
f	O
c0	O
(	O
3.34	O
)	O
then	O
π	O
is	O
a	O
vector	O
of	O
the	O
same	O
length	O
as	O
f	O
with	O
entries	O
πc	O
analogue	O
of	O
eq	O
.	O
(	O
3.12	O
)	O
is	O
the	O
log	O
of	O
the	O
un-normalized	O
posterior	O
i	O
.	O
the	O
multi-class	B
2	O
f	O
>	O
k−1f	O
+y	O
>	O
f−	O
nx	O
log	O
(	O
cid:0	O
)	O
cx	O
ψ	O
(	O
f	O
)	O
,	O
−	O
1	O
(	O
cid:1	O
)	O
−	O
1	O
exp	O
f	O
c	O
i	O
2	O
log	O
|k|−	O
cn	O
2	O
log	O
2π	O
.	O
(	O
3.35	O
)	O
as	O
in	O
the	O
binary	B
case	O
we	O
seek	O
the	O
map	O
value	O
ˆf	O
of	O
p	O
(	O
f|x	O
,	O
y	O
)	O
.	O
by	O
diﬀerentiating	O
eq	O
.	O
(	O
3.35	O
)	O
w.r.t	O
.	O
f	O
we	O
obtain	O
i=1	O
c=1	O
(	O
3.36	O
)	O
thus	O
at	O
the	O
maximum	O
we	O
have	O
ˆf	O
=	O
k	O
(	O
y	O
−	O
ˆπ	O
)	O
.	O
diﬀerentiating	O
again	O
,	O
and	O
using	O
∇ψ	O
=	O
−k−1f	O
+	O
y	O
−	O
π.	O
exp	O
(	O
f	O
j	O
i	O
)	O
=	O
πc	O
i	O
δcc0	O
+	O
πc	O
i	O
πc0	O
i	O
,	O
(	O
3.37	O
)	O
logx	O
j	O
−	O
∂2	O
∂f	O
c	O
i	O
∂f	O
c0	O
i	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.5	O
multi-class	B
laplace	O
approximation	O
49	O
we	O
obtain11	O
∇∇ψ	O
=	O
−k−1	O
−	O
w	O
,	O
where	O
w	O
,	O
diag	O
(	O
π	O
)	O
−	O
ππ	O
>	O
,	O
(	O
3.38	O
)	O
where	O
π	O
is	O
a	O
cn×n	O
matrix	B
obtained	O
by	O
stacking	O
vertically	O
the	O
diagonal	O
matrices	O
diag	O
(	O
πc	O
)	O
,	O
and	O
πc	O
is	O
the	O
subvector	O
of	O
π	O
pertaining	O
to	O
class	O
c.	O
as	O
in	O
the	O
binary	B
case	O
notice	O
that	O
−∇∇ψ	O
is	O
positive	B
deﬁnite	I
,	O
thus	O
ψ	O
(	O
f	O
)	O
is	O
concave	O
and	O
the	O
maximum	O
is	O
unique	O
(	O
see	B
also	O
exercise	O
3.10.2	O
)	O
.	O
predictive	B
distribution	O
for	O
f∗	O
z	O
as	O
in	O
the	O
binary	B
case	O
we	O
use	O
newton	O
’	O
s	O
method	O
to	O
search	O
for	O
the	O
mode	O
of	O
ψ	O
,	O
giving	O
f	O
new	O
=	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
(	O
w	O
f	O
+	O
y	O
−	O
π	O
)	O
.	O
(	O
3.39	O
)	O
this	O
update	O
if	O
coded	O
na¨ıvely	O
would	O
take	O
o	O
(	O
c	O
3n3	O
)	O
as	O
matrices	O
of	O
size	O
cn	O
have	O
to	O
be	O
inverted	O
.	O
however	O
,	O
as	O
described	O
in	O
section	O
3.5.1	O
,	O
we	O
can	O
utilize	O
the	O
structure	O
of	O
w	O
to	O
bring	O
down	O
the	O
computational	O
load	O
to	O
o	O
(	O
cn3	O
)	O
.	O
the	O
laplace	O
approximation	O
gives	O
us	O
a	O
gaussian	O
approximation	O
q	O
(	O
f|x	O
,	O
y	O
)	O
to	O
the	O
posterior	O
p	O
(	O
f|x	O
,	O
y	O
)	O
.	O
to	O
make	O
predictions	O
at	O
a	O
test	O
point	O
x∗	O
we	O
need	O
to	O
com-	O
pute	O
the	O
posterior	O
distribution	O
q	O
(	O
f∗|x	O
,	O
y	O
,	O
x∗	O
)	O
where	O
f	O
(	O
x∗	O
)	O
,	O
f∗	O
=	O
(	O
f	O
1∗	O
,	O
.	O
.	O
.	O
,	O
f	O
c∗	O
)	O
>	O
.	O
in	O
general	O
we	O
have	O
q	O
(	O
f∗|x	O
,	O
y	O
,	O
x∗	O
)	O
=	O
p	O
(	O
f∗|x	O
,	O
x∗	O
,	O
f	O
)	O
q	O
(	O
f|x	O
,	O
y	O
)	O
df	O
.	O
(	O
3.40	O
)	O
as	O
p	O
(	O
f∗|x	O
,	O
x∗	O
,	O
f	O
)	O
and	O
q	O
(	O
f|x	O
,	O
y	O
)	O
are	O
both	O
gaussian	O
,	O
q	O
(	O
f∗|x	O
,	O
y	O
,	O
x∗	O
)	O
will	O
also	O
be	O
gaussian	O
and	O
we	O
need	O
only	O
compute	O
its	O
mean	O
and	O
covariance	B
.	O
the	O
predictive	B
mean	O
for	O
class	O
c	O
is	O
given	O
by	O
eq	O
[	O
f	O
c	O
(	O
x∗	O
)	O
|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
kc	O
(	O
x∗	O
)	O
>	O
k−1	O
c	O
ˆf	O
c	O
=	O
kc	O
(	O
x∗	O
)	O
>	O
(	O
yc	O
−	O
ˆπc	O
)	O
,	O
(	O
3.41	O
)	O
where	O
kc	O
(	O
x∗	O
)	O
is	O
the	O
vector	O
of	O
covariances	O
between	O
the	O
test	O
point	O
and	O
each	O
of	O
the	O
training	O
points	O
for	O
the	O
cth	O
covariance	B
function	I
,	O
and	O
ˆf	O
c	O
is	O
the	O
subvector	O
of	O
ˆf	O
pertaining	O
to	O
class	O
c.	O
the	O
last	O
equality	O
comes	O
from	O
using	O
eq	O
.	O
(	O
3.36	O
)	O
at	O
the	O
maximum	O
ˆf	O
.	O
note	O
the	O
close	O
correspondence	O
to	O
eq	O
.	O
(	O
3.21	O
)	O
.	O
this	O
can	O
be	O
put	O
into	O
a	O
vector	O
form	O
eq	O
[	O
f∗|y	O
]	O
=	O
q	O
>	O
∗	O
(	O
y	O
−	O
ˆπ	O
)	O
by	O
deﬁning	O
the	O
cn	O
×	O
c	O
matrix	B
k1	O
(	O
x∗	O
)	O
0	O
	O
0	O
...	O
0	O
q∗	O
=	O
	O
.	O
k2	O
(	O
x∗	O
)	O
...	O
0	O
0	O
0	O
...	O
.	O
.	O
.	O
.	O
.	O
.	O
...	O
.	O
.	O
.	O
kc	O
(	O
x∗	O
)	O
(	O
3.42	O
)	O
(	O
3.43	O
)	O
using	O
a	O
similar	O
argument	O
to	O
eq	O
.	O
(	O
3.23	O
)	O
we	O
obtain	O
covq	O
(	O
f∗|x	O
,	O
y	O
,	O
x∗	O
)	O
=	O
σ	O
+	O
q	O
>	O
∗	O
k−1	O
(	O
k−1	O
+	O
w	O
)	O
−1k−1q∗	O
=	O
diag	O
(	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
)	O
−	O
q	O
>	O
∗	O
(	O
k	O
+	O
w	O
−1	O
)	O
−1q∗	O
,	O
where	O
σ	O
is	O
a	O
diagonal	O
c	O
×	O
c	O
matrix	B
with	O
σcc	O
=	O
kc	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
k	O
>	O
c	O
(	O
x∗	O
)	O
k−1	O
and	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
is	O
a	O
vector	O
of	O
covariances	O
,	O
whose	O
c	O
’	O
th	O
element	O
is	O
kc	O
(	O
x∗	O
,	O
x∗	O
)	O
.	O
c	O
kc	O
(	O
x∗	O
)	O
,	O
11there	O
is	O
a	O
sign	O
error	B
in	O
equation	O
23	O
of	O
williams	O
and	O
barber	O
[	O
1998	O
]	O
but	O
not	O
in	O
their	O
implementation	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
50	O
classiﬁcation	B
input	O
:	O
k	O
(	O
covariance	B
matrix	I
)	O
,	O
y	O
(	O
0/1	O
targets	O
)	O
2	O
:	O
f	O
:	O
=	O
0	O
repeat	O
initialization	O
newton	O
iteration	O
compute	O
π	O
and	O
π	O
from	O
f	O
with	O
eq	O
.	O
(	O
3.34	O
)	O
and	O
defn	O
.	O
of	O
π	O
under	O
eq	O
.	O
(	O
3.38	O
)	O
for	O
c	O
:	O
=	O
1	O
.	O
.	O
.	O
c	O
do	O
4	O
:	O
6	O
:	O
c	O
)	O
e	O
is	O
block	O
diag	O
.	O
d	O
1	O
2	O
(	O
icn	O
+	O
d	O
1	O
1	O
2	O
kd	O
2	O
)	O
−1d	O
2	O
log	O
determinant	O
1	O
2	O
compute	O
1	O
1	O
2	O
c	O
kcd	O
1	O
2	O
zc	O
:	O
=p	O
l	O
:	O
=	O
cholesky	O
(	O
in	O
+	O
d	O
c	O
l	O
>	O
\	O
(	O
l\d	O
ec	O
:	O
=	O
d	O
c	O
)	O
i	O
log	O
lii	O
1	O
2	O
1	O
2	O
8	O
:	O
10	O
:	O
m	O
:	O
=	O
cholesky	O
(	O
p	O
c	O
ec	O
)	O
end	O
for	O
b	O
:	O
=	O
(	O
d	O
−	O
ππ	O
>	O
)	O
f	O
+	O
y	O
−	O
π	O
c	O
:	O
=	O
ekb	O
a	O
:	O
=	O
b−	O
c	O
+	O
erm	O
>	O
\	O
(	O
m\	O
(	O
r	O
>	O
c	O
)	O
)	O
f	O
:	O
=	O
ka	O
12	O
:	O
14	O
:	O
b	O
=	O
w	O
f	O
+	O
y	O
−	O
π	O
from	O
eq	O
.	O
(	O
3.39	O
)	O
)	O
2	O
a	O
>	O
f	O
+	O
y	O
>	O
f	O
+p	O
i	O
log	O
(	O
cid:0	O
)	O
p	O
c	O
)	O
(	O
cid:1	O
)	O
−p	O
c	O
exp	O
(	O
f	O
i	O
i	O
log	O
(	O
cid:0	O
)	O
p	O
eq	O
.	O
(	O
3.39	O
)	O
using	O
eq	O
.	O
(	O
3.45	O
)	O
and	O
(	O
3.47	O
)	O
c	O
)	O
(	O
cid:1	O
)	O
2	O
a	O
>	O
f	O
+	O
y	O
>	O
f	O
+p	O
objective	O
:	O
−	O
1	O
c	O
exp	O
(	O
f	O
i	O
until	O
convergence	O
16	O
:	O
log	O
q	O
(	O
y|x	O
,	O
θ	O
)	O
:	O
=	O
−	O
1	O
eq	O
.	O
(	O
3.44	O
)	O
return	O
:	O
ˆf	O
:	O
=	O
f	O
(	O
post	O
.	O
mode	O
)	O
,	O
log	O
q	O
(	O
y|x	O
,	O
θ	O
)	O
(	O
approx	O
.	O
log	O
marg	O
.	O
likelihood	B
)	O
algorithm	O
3.3	O
:	O
mode-ﬁnding	O
for	O
multi-class	B
laplace	O
gpc	O
,	O
where	O
d	O
=	O
diag	O
(	O
π	O
)	O
,	O
r	O
is	O
a	O
matrix	B
of	O
stacked	O
identity	O
matrices	O
and	O
a	O
subscript	O
c	O
on	O
a	O
block	O
diagonal	O
matrix	B
indicates	O
the	O
n	O
×	O
n	O
submatrix	O
pertaining	O
to	O
class	O
c.	O
the	O
computational	O
complexity	O
is	O
dominated	O
by	O
the	O
cholesky	O
decomposition	O
in	O
lines	O
6	O
and	O
10	O
and	O
the	O
forward	O
and	O
backward	O
substitutions	O
in	O
line	O
7	O
with	O
total	O
complexity	O
o	O
(	O
(	O
c	O
+	O
1	O
)	O
n3	O
)	O
(	O
times	O
the	O
num-	O
ber	O
of	O
newton	O
iterations	O
)	O
,	O
all	O
other	O
operations	O
are	O
at	O
most	O
o	O
(	O
cn2	O
)	O
when	O
exploiting	O
diagonal	O
and	O
block	O
diagonal	O
structures	O
.	O
the	O
memory	O
requirement	O
is	O
o	O
(	O
cn2	O
)	O
.	O
for	O
comments	O
on	O
convergence	O
criteria	O
for	O
line	O
15	O
and	O
avoiding	O
divergence	O
,	O
refer	O
to	O
the	O
caption	O
of	O
algorithm	O
3.1	O
on	O
page	O
46.	O
c	O
zc	O
we	O
now	O
need	O
to	O
consider	O
the	O
predictive	B
distribution	O
q	O
(	O
π∗|y	O
)	O
which	O
is	O
ob-	O
tained	O
by	O
softmaxing	O
the	O
gaussian	O
q	O
(	O
f∗|y	O
)	O
.	O
in	O
the	O
binary	B
case	O
we	O
saw	O
that	O
the	O
predicted	O
classiﬁcation	B
could	O
be	O
obtained	O
by	O
thresholding	O
the	O
mean	O
value	O
of	O
the	O
gaussian	O
.	O
in	O
the	O
multi-class	B
case	O
one	O
does	O
need	O
to	O
take	O
the	O
variability	O
around	O
the	O
mean	O
into	O
account	O
as	O
it	O
can	O
aﬀect	O
the	O
overall	O
classiﬁcation	B
(	O
see	B
exercise	O
3.10.3	O
)	O
.	O
one	O
simple	O
way	O
(	O
which	O
will	O
be	O
used	O
in	O
algorithm	O
3.4	O
)	O
to	O
estimate	O
the	O
mean	O
prediction	O
eq	O
[	O
π∗|y	O
]	O
is	O
to	O
draw	O
samples	O
from	O
the	O
gaussian	O
q	O
(	O
f∗|y	O
)	O
,	O
softmax	B
them	O
and	O
then	O
average	O
.	O
marginal	B
likelihood	I
the	O
laplace	O
approximation	O
to	O
the	O
marginal	B
likelihood	I
can	O
be	O
obtained	O
in	O
the	O
same	O
way	O
as	O
for	O
the	O
binary	B
case	O
,	O
yielding	O
log	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
’	O
log	O
q	O
(	O
y|x	O
,	O
θ	O
)	O
ˆf	O
>	O
k−1ˆf	O
+	O
y	O
>	O
ˆf	O
−	O
nx	O
=	O
−	O
1	O
2	O
(	O
cid:16	O
)	O
cx	O
log	O
exp	O
ˆf	O
c	O
i	O
(	O
cid:17	O
)	O
−	O
1	O
2	O
log	O
|icn	O
+	O
w	O
(	O
3.44	O
)	O
2|	O
.	O
1	O
1	O
2	O
kw	O
i=1	O
c=1	O
as	O
for	O
the	O
inversion	O
of	O
k−1	O
+	O
w	O
,	O
the	O
determinant	O
term	O
can	O
be	O
computed	O
eﬃ-	O
ciently	O
by	O
exploiting	O
the	O
structure	O
of	O
w	O
,	O
see	B
section	O
3.5.1.	O
in	O
this	O
section	O
we	O
have	O
described	O
the	O
laplace	O
approximation	O
for	O
multi-class	B
classiﬁcation	I
.	O
however	O
,	O
there	O
has	O
also	O
been	O
some	O
work	O
on	O
ep-type	O
methods	O
for	O
the	O
multi-class	B
case	O
,	O
see	B
seeger	O
and	O
jordan	O
[	O
2004	O
]	O
.	O
1	O
2	O
c	O
kcd	O
1	O
2	O
c	O
)	O
for	O
c	O
:	O
=	O
1	O
.	O
.	O
.	O
c	O
do	O
4	O
:	O
1	O
2	O
l	O
:	O
=	O
cholesky	O
(	O
in	O
+	O
d	O
c	O
l	O
>	O
\	O
(	O
l\d	O
ec	O
:	O
=	O
d	O
c	O
)	O
m	O
:	O
=	O
cholesky	O
(	O
p	O
1	O
2	O
6	O
:	O
end	O
for	O
8	O
:	O
for	O
c	O
:	O
=	O
1	O
.	O
.	O
.	O
c	O
do	O
c	O
ec	O
)	O
µc∗	O
:	O
=	O
(	O
yc	O
−	O
πc	O
)	O
>	O
kc∗	O
b	O
:	O
=	O
eckc∗	O
c	O
:	O
=	O
ec	O
(	O
r	O
(	O
m	O
>	O
\	O
(	O
m\	O
(	O
r	O
>	O
b	O
)	O
)	O
)	O
)	O
for	O
c0	O
:	O
=	O
1	O
.	O
.	O
.	O
c	O
do	O
σcc0	O
:	O
=	O
c	O
>	O
kc0	O
∗	O
end	O
for	O
σcc	O
:	O
=	O
σcc	O
+	O
kc	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
b	O
>	O
kc∗	O
10	O
:	O
12	O
:	O
14	O
:	O
e	O
is	O
block	O
diag	O
.	O
d	O
1	O
2	O
(	O
icn	O
+	O
d	O
1	O
2	O
kd	O
1	O
2	O
)	O
−1d	O
1	O
2	O
latent	O
test	O
mean	O
from	O
eq	O
.	O
(	O
3.41	O
)	O
)	O
latent	O
test	O
covariance	B
from	O
eq	O
.	O
(	O
3.43	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.5	O
multi-class	B
laplace	O
approximation	O
51	O
input	O
:	O
k	O
(	O
covariance	B
matrix	I
)	O
,	O
ˆf	O
(	O
posterior	O
mode	O
)	O
,	O
x∗	O
(	O
test	O
input	O
)	O
2	O
:	O
compute	O
π	O
and	O
π	O
from	O
ˆf	O
using	O
eq	O
.	O
(	O
3.34	O
)	O
and	O
defn	O
.	O
of	O
π	O
under	O
eq	O
.	O
(	O
3.38	O
)	O
16	O
:	O
end	O
for	O
π∗	O
:	O
=	O
0	O
18	O
:	O
for	O
i	O
:	O
=	O
1	O
:	O
s	O
do	O
f∗	O
∼	O
n	O
(	O
µ∗	O
,	O
σ	O
)	O
π∗	O
:	O
=	O
π∗	O
+	O
exp	O
(	O
f	O
c∗	O
)	O
/p	O
20	O
:	O
initialize	O
monte	O
carlo	O
loop	O
to	O
estimate	O
predictive	B
class	O
probabilities	O
using	O
s	O
samples	O
sample	O
latent	O
values	O
from	O
joint	B
gaussian	O
posterior	O
c0	O
exp	O
(	O
f	O
c0	O
accumulate	O
probability	B
eq	O
.	O
(	O
3.34	O
)	O
∗	O
)	O
22	O
:	O
¯π∗	O
:	O
=	O
π∗/s	O
end	O
for	O
normalize	O
mc	O
estimate	O
of	O
prediction	B
vector	O
return	O
:	O
eq	O
(	O
f	O
)	O
[	O
π	O
(	O
f	O
(	O
x∗	O
)	O
)	O
|x∗	O
,	O
x	O
,	O
y	O
]	O
:	O
=	O
¯π∗	O
(	O
predicted	O
class	O
probability	B
vector	O
)	O
algorithm	O
3.4	O
:	O
predictions	O
for	O
multi-class	B
laplace	O
gpc	O
,	O
where	O
d	O
=	O
diag	O
(	O
π	O
)	O
,	O
r	O
is	O
a	O
matrix	B
of	O
stacked	O
identity	O
matrices	O
and	O
a	O
subscript	O
c	O
on	O
a	O
block	O
diagonal	O
matrix	B
indicates	O
the	O
n	O
×	O
n	O
submatrix	O
pertaining	O
to	O
class	O
c.	O
the	O
computational	O
complexity	O
is	O
dominated	O
by	O
the	O
cholesky	O
decomposition	O
in	O
lines	O
4	O
and	O
7	O
with	O
a	O
total	O
complexity	O
o	O
(	O
(	O
c	O
+	O
1	O
)	O
n3	O
)	O
,	O
the	O
memory	O
requirement	O
is	O
o	O
(	O
cn2	O
)	O
.	O
for	O
multiple	O
test	O
cases	O
repeat	O
from	O
line	O
8	O
for	O
each	O
test	O
case	O
(	O
in	O
practice	O
,	O
for	O
multiple	O
test	O
cases	O
one	O
may	O
reorder	O
the	O
computations	O
in	O
lines	O
8-16	O
to	O
avoid	O
referring	O
to	O
all	O
ec	O
matrices	O
repeatedly	O
)	O
.	O
3.5.1	O
implementation	O
the	O
implementation	O
follows	O
closely	O
the	O
implementation	O
for	O
the	O
binary	B
case	O
de-	O
tailed	O
in	O
section	O
3.4.3	O
,	O
with	O
the	O
slight	O
complications	O
that	O
k	O
is	O
now	O
a	O
block	O
diagonal	O
matrix	B
of	O
size	O
cn	O
×	O
cn	O
and	O
the	O
w	O
matrix	B
is	O
no	O
longer	O
diagonal	O
,	O
see	B
eq	O
.	O
(	O
3.38	O
)	O
.	O
care	O
has	O
to	O
be	O
taken	O
to	O
exploit	O
the	O
structure	O
of	O
these	O
matrices	O
to	O
reduce	O
the	O
computational	O
burden	O
.	O
the	O
newton	O
iteration	O
from	O
eq	O
.	O
(	O
3.39	O
)	O
requires	O
the	O
inversion	O
of	O
k−1	O
+	O
w	O
,	O
which	O
we	O
ﬁrst	O
re-write	O
as	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
=	O
k	O
−	O
k	O
(	O
k	O
+	O
w	O
−1	O
)	O
−1k	O
,	O
(	O
3.45	O
)	O
using	O
the	O
matrix	B
inversion	O
lemma	O
,	O
eq	O
.	O
(	O
a.9	O
)	O
.	O
in	O
the	O
following	O
the	O
inversion	O
of	O
the	O
above	O
matrix	B
k	O
+	O
w	O
−1	O
is	O
our	O
main	O
concern	O
.	O
first	O
,	O
however	O
,	O
we	O
apply	O
the	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
52	O
classiﬁcation	B
matrix	O
inversion	B
lemma	I
,	O
eq	O
.	O
(	O
a.9	O
)	O
to	O
the	O
w	O
matrix:12	O
w	O
−1	O
=	O
(	O
d	O
−	O
ππ	O
>	O
)	O
−1	O
=	O
d−1	O
−	O
r	O
(	O
i	O
−	O
r	O
>	O
dr	O
)	O
−1r	O
>	O
(	O
3.46	O
)	O
=	O
d−1	O
−	O
ro−1r	O
>	O
,	O
where	O
d	O
=	O
diag	O
(	O
π	O
)	O
,	O
r	O
=	O
d−1π	O
is	O
a	O
cn×	O
n	O
matrix	B
of	O
stacked	O
in	O
unit	O
matrices	O
,	O
c	O
dc	O
=	O
in	O
and	O
o	O
is	O
the	O
zero	O
matrix	B
.	O
introducing	O
the	O
above	O
in	O
k	O
+	O
w	O
−1	O
and	O
applying	O
the	O
matrix	B
inversion	O
lemma	O
,	O
eq	O
.	O
(	O
a.9	O
)	O
again	O
we	O
have	O
(	O
k	O
+	O
w	O
−1	O
)	O
−1	O
=	O
(	O
k	O
+	O
d−1	O
−	O
ro−1r	O
>	O
)	O
−1	O
we	O
use	O
the	O
fact	O
that	O
π	O
normalizes	O
over	O
classes	O
:	O
r	O
>	O
dr	O
=p	O
=	O
e	O
−	O
er	O
(	O
o	O
+	O
r	O
>	O
er	O
)	O
−1r	O
>	O
e	O
=	O
e	O
−	O
er	O
(	O
p	O
and	O
r	O
>	O
er	O
=p	O
(	O
3.47	O
)	O
c	O
ec	O
)	O
−1r	O
>	O
e	O
.	O
2	O
is	O
a	O
block	O
diagonal	O
matrix	B
c	O
ec	O
.	O
the	O
newton	O
iterations	O
can	O
now	O
be	O
computed	O
by	O
inserting	O
eq	O
.	O
(	O
3.47	O
)	O
and	O
(	O
3.45	O
)	O
in	O
eq	O
.	O
(	O
3.39	O
)	O
,	O
as	O
detailed	O
in	O
algorithm	O
3.3.	O
the	O
predictions	O
use	O
an	O
equivalent	B
route	O
to	O
compute	O
the	O
gaussian	O
posterior	O
,	O
and	O
the	O
ﬁnal	O
step	O
of	O
deriving	O
predictive	B
class	O
probabilities	O
is	O
done	O
by	O
monte	O
carlo	O
,	O
as	O
shown	O
in	O
algorithm	O
3.4.	O
where	O
e	O
=	O
(	O
k	O
+	O
d−1	O
)	O
−1	O
=	O
d	O
1	O
2	O
)	O
−1d	O
1	O
1	O
2	O
(	O
i	O
+	O
d	O
1	O
2	O
kd	O
3.6	O
expectation	B
propagation	I
the	O
expectation	B
propagation	I
(	O
ep	O
)	O
algorithm	O
[	O
minka	O
,	O
2001	O
]	O
is	O
a	O
general	O
approxi-	O
mation	O
tool	O
with	O
a	O
wide	O
range	O
of	O
applications	O
.	O
in	O
this	O
section	O
we	O
present	O
only	O
its	O
application	O
to	O
the	O
speciﬁc	O
case	O
of	O
a	O
gp	O
model	B
for	O
binary	B
classiﬁcation	I
.	O
we	O
note	O
that	O
opper	O
and	O
winther	O
[	O
2000	O
]	O
presented	O
a	O
similar	O
method	O
for	O
binary	B
gpc	O
based	O
on	O
the	O
ﬁxed-point	O
equations	O
of	O
the	O
thouless-anderson-palmer	O
(	O
tap	O
)	O
type	O
of	O
mean-ﬁeld	B
approximation	I
from	O
statistical	O
physics	O
.	O
the	O
ﬁxed	O
points	O
for	O
the	O
two	O
methods	O
are	O
the	O
same	O
,	O
although	O
the	O
precise	O
details	O
of	O
the	O
two	O
algorithms	O
are	O
diﬀerent	O
.	O
the	O
ep	O
algorithm	O
naturally	O
lends	O
itself	O
to	O
sparse	O
approximations	O
,	O
which	O
will	O
not	O
be	O
discussed	O
in	O
detail	O
here	O
,	O
but	O
touched	O
upon	O
in	O
section	O
8.4.	O
the	O
object	O
of	O
central	O
importance	O
is	O
the	O
posterior	O
distribution	O
over	O
the	O
latent	O
variables	O
,	O
p	O
(	O
f|x	O
,	O
y	O
)	O
.	O
in	O
the	O
following	O
notation	O
we	O
suppress	O
the	O
explicit	O
depen-	O
dence	O
on	O
hyperparameters	B
,	O
see	B
section	O
3.6.2	O
for	O
their	O
treatment	O
.	O
the	O
posterior	O
is	O
given	O
by	O
bayes	O
’	O
rule	O
,	O
as	O
the	O
product	O
of	O
a	O
normalization	O
term	O
,	O
the	O
prior	O
and	O
the	O
likelihood	B
p	O
(	O
f|x	O
,	O
y	O
)	O
=	O
p	O
(	O
f|x	O
)	O
(	O
3.48	O
)	O
where	O
the	O
prior	O
p	O
(	O
f|x	O
)	O
is	O
gaussian	O
and	O
we	O
have	O
utilized	O
the	O
fact	O
that	O
the	O
likeli-	O
hood	O
factorizes	O
over	O
the	O
training	O
cases	O
.	O
the	O
normalization	O
term	O
is	O
the	O
marginal	B
likelihood	I
i=1	O
p	O
(	O
yi|fi	O
)	O
,	O
z	O
=	O
p	O
(	O
y|x	O
)	O
=	O
p	O
(	O
f|x	O
)	O
p	O
(	O
yi|fi	O
)	O
df	O
.	O
(	O
3.49	O
)	O
ny	O
ny	O
1	O
z	O
z	O
12readers	O
who	O
are	O
disturbed	O
by	O
our	O
sloppy	O
treatment	O
of	O
the	O
inverse	O
of	O
singular	O
matrices	O
are	O
invited	O
to	O
insert	O
the	O
matrix	B
(	O
1	O
−	O
ε	O
)	O
in	O
between	O
π	O
and	O
π	O
>	O
in	O
eq	O
.	O
(	O
3.46	O
)	O
and	O
verify	O
that	O
eq	O
.	O
(	O
3.47	O
)	O
coincides	O
with	O
the	O
limit	O
ε	O
→	O
0.	O
i=1	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.6	O
expectation	B
propagation	I
53	O
so	O
far	O
,	O
everything	O
is	O
exactly	O
as	O
in	O
the	O
regression	B
case	O
discussed	O
in	O
chapter	O
2.	O
however	O
,	O
in	O
the	O
case	O
of	O
classiﬁcation	B
the	O
likelihood	B
p	O
(	O
yi|fi	O
)	O
is	O
not	O
gaussian	O
,	O
a	O
property	O
that	O
was	O
used	O
heavily	O
in	O
arriving	O
at	O
analytical	O
solutions	O
for	O
the	O
regression	B
framework	O
.	O
in	O
this	O
section	O
we	O
use	O
the	O
probit	B
likelihood	O
(	O
see	B
page	O
35	O
)	O
for	O
binary	B
classiﬁcation	I
(	O
3.50	O
)	O
and	O
this	O
makes	O
the	O
posterior	O
in	O
eq	O
.	O
(	O
3.48	O
)	O
analytically	O
intractable	O
.	O
to	O
overcome	O
this	O
hurdle	O
in	O
the	O
ep	O
framework	O
we	O
approximate	O
the	O
likelihood	B
by	O
a	O
local	O
like-	O
lihood	B
approximation	O
13	O
in	O
the	O
form	O
of	O
an	O
un-normalized	O
gaussian	O
function	B
in	O
the	O
latent	O
variable	O
fi	O
p	O
(	O
yi|fi	O
)	O
=	O
φ	O
(	O
fiyi	O
)	O
,	O
p	O
(	O
yi|fi	O
)	O
’	O
ti	O
(	O
fi|	O
˜zi	O
,	O
˜µi	O
,	O
˜σ2	O
i	O
)	O
,	O
˜zin	O
(	O
fi|˜µi	O
,	O
˜σ2	O
i	O
)	O
,	O
(	O
3.51	O
)	O
site	O
parameters	O
which	O
deﬁnes	O
the	O
site	O
parameters	O
˜zi	O
,	O
˜µi	O
and	O
˜σ2	O
i	O
.	O
remember	O
that	O
the	O
notation	O
n	O
is	O
used	O
for	O
a	O
normalized	O
gaussian	O
distribution	O
.	O
notice	O
that	O
we	O
are	O
approxi-	O
mating	O
the	O
likelihood	B
,	O
i.e	O
.	O
a	O
probability	B
distribution	O
which	O
normalizes	O
over	O
the	O
targets	O
yi	O
,	O
by	O
an	O
un-normalized	O
gaussian	O
distribution	O
over	O
the	O
latent	O
variables	O
fi	O
.	O
this	O
is	O
reasonable	O
,	O
because	O
we	O
are	O
interested	O
in	O
how	O
the	O
likelihood	B
behaves	O
as	O
a	O
function	B
of	O
the	O
latent	O
fi	O
.	O
in	O
the	O
regression	B
setting	O
we	O
utilized	O
the	O
gaussian	O
shape	O
of	O
the	O
likelihood	B
,	O
but	O
more	O
to	O
the	O
point	O
,	O
the	O
gaussian	O
distribution	O
for	O
the	O
outputs	B
yi	O
also	O
implied	O
a	O
gaussian	O
shape	O
as	O
a	O
function	B
of	O
the	O
latent	O
vari-	O
able	O
fi	O
.	O
in	O
order	O
to	O
compute	O
the	O
posterior	O
we	O
are	O
of	O
course	O
primarily	O
interested	O
in	O
how	O
the	O
likelihood	B
behaves	O
as	O
a	O
function	B
of	O
fi.14	O
the	O
property	O
that	O
the	O
likelihood	B
should	O
normalize	O
over	O
yi	O
(	O
for	O
any	O
value	O
of	O
fi	O
)	O
is	O
not	O
simultaneously	O
achievable	O
with	O
the	O
desideratum	O
of	O
gaussian	O
dependence	O
on	O
fi	O
;	O
in	O
the	O
ep	O
ap-	O
proximation	O
we	O
abandon	O
exact	O
normalization	O
for	O
tractability	O
.	O
the	O
product	O
of	O
the	O
(	O
independent	O
)	O
local	O
likelihoods	O
ti	O
is	O
ny	O
i	O
)	O
=	O
n	O
(	O
˜µ	O
,	O
˜σ	O
)	O
y	O
ti	O
(	O
fi|	O
˜zi	O
,	O
˜µi	O
,	O
˜σ2	O
˜zi	O
,	O
(	O
3.52	O
)	O
i=1	O
i	O
where	O
˜µ	O
is	O
the	O
vector	O
of	O
˜µi	O
and	O
˜σ	O
is	O
diagonal	O
with	O
˜σii	O
=	O
˜σ2	O
the	O
posterior	O
p	O
(	O
f|x	O
,	O
y	O
)	O
by	O
q	O
(	O
f|x	O
,	O
y	O
)	O
i	O
.	O
we	O
approximate	O
ny	O
i=1	O
q	O
(	O
f|x	O
,	O
y	O
)	O
,	O
1	O
zep	O
with	O
µ	O
=	O
σ˜ς−1	O
˜µ	O
,	O
and	O
σ	O
=	O
(	O
k−1	O
+	O
˜σ−1	O
)	O
−1	O
,	O
ti	O
(	O
fi|	O
˜zi	O
,	O
˜µi	O
,	O
˜σ2	O
p	O
(	O
f|x	O
)	O
i	O
)	O
=	O
n	O
(	O
µ	O
,	O
σ	O
)	O
,	O
(	O
3.53	O
)	O
where	O
we	O
have	O
used	O
eq	O
.	O
(	O
a.7	O
)	O
to	O
compute	O
the	O
product	O
(	O
and	O
by	O
deﬁnition	O
,	O
we	O
know	O
that	O
the	O
distribution	O
must	O
normalize	O
correctly	O
over	O
f	O
)	O
.	O
notice	O
,	O
that	O
we	O
use	O
the	O
tilde-parameters	O
˜µ	O
and	O
˜σ	O
(	O
and	O
˜z	O
)	O
for	O
the	O
local	O
likelihood	B
approximations	O
,	O
13note	O
,	O
that	O
although	O
each	O
likelihood	B
approximation	O
is	O
local	O
,	O
the	O
posterior	O
approximation	O
produced	O
by	O
the	O
ep	O
algorithm	O
is	O
global	O
because	O
the	O
latent	O
variables	O
are	O
coupled	O
through	O
the	O
prior	O
.	O
14however	O
,	O
for	O
computing	O
the	O
marginal	B
likelihood	I
normalization	O
becomes	O
crucial	O
,	O
see	B
section	O
3.6.2.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
54	O
classiﬁcation	B
and	O
plain	O
µ	O
and	O
σ	O
for	O
the	O
parameters	O
of	O
the	O
approximate	O
posterior	O
.	O
the	O
nor-	O
malizing	O
term	O
of	O
eq	O
.	O
(	O
3.53	O
)	O
,	O
zep	O
=	O
q	O
(	O
y|x	O
)	O
,	O
is	O
the	O
ep	O
algorithm	O
’	O
s	O
approximation	O
to	O
the	O
normalizing	O
term	O
z	O
from	O
eq	O
.	O
(	O
3.48	O
)	O
and	O
eq	O
.	O
(	O
3.49	O
)	O
.	O
kl	O
divergence	O
how	O
do	O
we	O
choose	O
the	O
parameters	O
of	O
the	O
local	O
approximating	O
distributions	O
ti	O
?	O
one	O
of	O
the	O
most	O
obvious	O
ideas	O
would	O
be	O
to	O
minimize	O
the	O
kullback-leibler	O
(	O
kl	O
)	O
divergence	O
(	O
see	B
section	O
a.5	O
)	O
between	O
the	O
posterior	O
and	O
its	O
approximation	O
:	O
kl	O
(	O
cid:0	O
)	O
p	O
(	O
f|x	O
,	O
y	O
)	O
||q	O
(	O
f|x	O
,	O
y	O
)	O
(	O
cid:1	O
)	O
.	O
direct	O
minimization	O
of	O
this	O
kl	O
divergence	O
for	O
the	O
choose	O
to	O
minimize	O
the	O
reversed	O
kl	O
divergence	O
kl	O
(	O
cid:0	O
)	O
q	O
(	O
f|x	O
,	O
y	O
)	O
||p	O
(	O
f|x	O
,	O
y	O
)	O
(	O
cid:1	O
)	O
with	O
joint	B
distribution	O
on	O
f	O
turns	O
out	O
to	O
be	O
intractable	O
.	O
(	O
one	O
can	O
alternatively	O
respect	O
to	O
the	O
distribution	O
q	O
(	O
f|x	O
,	O
y	O
)	O
;	O
this	O
has	O
been	O
used	O
to	O
carry	O
out	O
variational	O
inference	O
for	O
gpc	O
,	O
see	B
,	O
e.g	O
.	O
seeger	O
[	O
2000	O
]	O
.	O
)	O
instead	O
,	O
the	O
key	O
idea	O
in	O
the	O
ep	O
algorithm	O
is	O
to	O
update	O
the	O
individual	O
ti	O
ap-	O
proximations	O
sequentially	O
.	O
conceptually	O
this	O
is	O
done	O
by	O
iterating	O
the	O
following	O
four	O
steps	O
:	O
we	O
start	O
from	O
some	O
current	O
approximate	O
posterior	O
,	O
from	O
which	O
we	O
leave	O
out	O
the	O
current	O
ti	O
,	O
giving	O
rise	O
to	O
a	O
marginal	B
cavity	O
distribution	O
.	O
secondly	O
,	O
we	O
combine	O
the	O
cavity	O
distribution	O
with	O
the	O
exact	O
likelihood	B
p	O
(	O
yi|fi	O
)	O
to	O
get	O
the	O
desired	O
(	O
non-gaussian	O
)	O
marginal	B
.	O
thirdly	O
,	O
we	O
choose	O
a	O
gaussian	O
approximation	O
to	O
the	O
non-gaussian	O
marginal	B
,	O
and	O
in	O
the	O
ﬁnal	O
step	O
we	O
compute	O
the	O
ti	O
which	O
makes	O
the	O
posterior	O
have	O
the	O
desired	O
marginal	B
from	O
step	O
three	O
.	O
these	O
four	O
steps	O
are	O
iterated	O
until	O
convergence	O
.	O
in	O
more	O
detail	O
,	O
we	O
optimize	O
the	O
ti	O
approximations	O
sequentially	O
,	O
using	O
the	O
approximation	O
so	O
far	O
for	O
all	O
the	O
other	O
variables	O
.	O
in	O
particular	O
the	O
approximate	O
posterior	O
for	O
fi	O
contains	O
three	O
kinds	O
of	O
terms	O
:	O
1.	O
the	O
prior	O
p	O
(	O
f|x	O
)	O
2.	O
the	O
local	O
approximate	O
likelihoods	O
tj	O
for	O
all	O
cases	O
j	O
6=	O
i	O
3.	O
the	O
exact	O
likelihood	B
for	O
case	O
i	O
,	O
p	O
(	O
yi|fi	O
)	O
=	O
φ	O
(	O
yifi	O
)	O
our	O
goal	O
is	O
to	O
combine	O
these	O
sources	O
of	O
information	O
and	O
choose	O
parameters	O
of	O
ti	O
such	O
that	O
the	O
marginal	B
posterior	O
is	O
as	O
accurate	O
as	O
possible	O
.	O
we	O
will	O
ﬁrst	O
combine	O
the	O
prior	O
and	O
the	O
local	O
likelihood	B
approximations	O
into	O
the	O
cavity	O
distribution	O
z	O
p	O
(	O
f|x	O
)	O
y	O
j6=i	O
q−i	O
(	O
fi	O
)	O
∝	O
tj	O
(	O
fj|	O
˜zj	O
,	O
˜µj	O
,	O
˜σ2	O
j	O
)	O
dfj	O
,	O
(	O
3.54	O
)	O
and	O
subsequently	O
combine	O
this	O
with	O
the	O
exact	O
likelihood	B
for	O
case	O
i.	O
concep-	O
tually	O
,	O
one	O
can	O
think	O
of	O
the	O
combination	O
of	O
prior	O
and	O
the	O
n	O
−	O
1	O
approximate	O
likelihoods	O
in	O
eq	O
.	O
(	O
3.54	O
)	O
in	O
two	O
ways	O
,	O
either	O
by	O
explicitly	O
multiplying	O
out	O
the	O
terms	O
,	O
or	O
(	O
equivalently	O
)	O
by	O
removing	O
approximate	O
likelihood	B
i	O
from	O
the	O
approx-	O
imate	O
posterior	O
in	O
eq	O
.	O
(	O
3.53	O
)	O
.	O
here	O
we	O
will	O
follow	O
the	O
latter	O
approach	O
.	O
the	O
marginal	B
for	O
fi	O
from	O
q	O
(	O
f|x	O
,	O
y	O
)	O
is	O
obtained	O
by	O
using	O
eq	O
.	O
(	O
a.6	O
)	O
in	O
eq	O
.	O
(	O
3.53	O
)	O
to	O
give	O
(	O
3.55	O
)	O
i	O
=	O
σii	O
.	O
this	O
marginal	B
eq	O
.	O
(	O
3.55	O
)	O
contains	O
one	O
approximate	O
term	O
q	O
(	O
fi|x	O
,	O
y	O
)	O
=	O
n	O
(	O
fi|µi	O
,	O
σ2	O
i	O
)	O
,	O
where	O
σ2	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.6	O
expectation	B
propagation	I
55	O
(	O
namely	O
ti	O
)	O
“	O
too	O
many	O
”	O
,	O
so	O
we	O
need	O
to	O
divide	O
it	O
by	O
ti	O
to	O
get	O
the	O
cavity	O
dis-	O
tribution	O
cavity	O
distribution	O
q−i	O
(	O
fi	O
)	O
,	O
n	O
(	O
fi|µ−i	O
,	O
σ2−i	O
)	O
,	O
i	O
µi	O
−	O
˜σ−2	O
where	O
µ−i	O
=	O
σ2−i	O
(	O
σ−2	O
i	O
˜µi	O
)	O
,	O
and	O
σ2−i	O
=	O
(	O
σ−2	O
i	O
−	O
˜σ−2	O
i	O
(	O
3.56	O
)	O
)	O
−1	O
.	O
note	O
that	O
the	O
cavity	O
distribution	O
and	O
its	O
parameters	O
carry	O
the	O
subscript	O
−i	O
,	O
indicating	O
that	O
they	O
include	O
all	O
cases	O
except	O
number	O
i.	O
the	O
easiest	O
way	O
to	O
verify	O
eq	O
.	O
(	O
3.56	O
)	O
is	O
to	O
multiply	O
the	O
cavity	O
distribution	O
by	O
the	O
local	O
likelihood	B
approximation	O
ti	O
from	O
eq	O
.	O
(	O
3.51	O
)	O
using	O
eq	O
.	O
(	O
a.7	O
)	O
to	O
recover	O
the	O
marginal	B
in	O
eq	O
.	O
(	O
3.55	O
)	O
.	O
notice	O
that	O
despite	O
the	O
appearance	O
of	O
eq	O
.	O
(	O
3.56	O
)	O
,	O
the	O
cavity	O
mean	O
and	O
variance	O
are	O
(	O
of	O
course	O
)	O
not	O
dependent	O
on	O
˜µi	O
and	O
˜σ2	O
i	O
,	O
see	B
exercise	O
3.10.5.	O
to	O
proceed	O
,	O
we	O
need	O
to	O
ﬁnd	O
the	O
new	O
(	O
un-normalized	O
)	O
gaussian	O
marginal	B
which	O
best	O
approximates	O
the	O
product	O
of	O
the	O
cavity	O
distribution	O
and	O
the	O
exact	O
likelihood	B
(	O
3.57	O
)	O
it	O
is	O
well	O
known	O
that	O
when	O
q	O
(	O
x	O
)	O
is	O
gaussian	O
,	O
the	O
distribution	O
q	O
(	O
x	O
)	O
which	O
min-	O
imizes	O
kl	O
(	O
cid:0	O
)	O
p	O
(	O
x	O
)	O
||q	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
is	O
the	O
one	O
whose	O
ﬁrst	O
and	O
second	O
moments	O
match	O
that	O
i	O
)	O
’	O
q−i	O
(	O
fi	O
)	O
p	O
(	O
yi|fi	O
)	O
.	O
ˆq	O
(	O
fi	O
)	O
,	O
ˆzin	O
(	O
ˆµi	O
,	O
ˆσ2	O
of	O
p	O
(	O
x	O
)	O
,	O
see	B
eq	O
.	O
(	O
a.24	O
)	O
.	O
as	O
ˆq	O
(	O
fi	O
)	O
is	O
un-normalized	O
we	O
choose	O
additionally	O
to	O
impose	O
the	O
condition	O
that	O
the	O
zero-th	O
moments	O
(	O
normalizing	O
constants	O
)	O
should	O
match	O
when	O
choosing	O
the	O
parameters	O
of	O
ˆq	O
(	O
fi	O
)	O
to	O
match	O
the	O
right	O
hand	O
side	O
of	O
eq	O
.	O
(	O
3.57	O
)	O
.	O
this	O
process	B
is	O
illustrated	O
in	O
figure	O
3.4.	O
the	O
derivation	O
of	O
the	O
moments	O
is	O
somewhat	O
lengthy	O
,	O
so	O
we	O
have	O
moved	O
the	O
details	O
to	O
section	O
3.9.	O
the	O
desired	O
posterior	O
marginal	O
moments	O
are	O
ˆzi	O
=	O
φ	O
(	O
zi	O
)	O
,	O
i	O
=	O
σ2−i	O
−	O
σ4−in	O
(	O
zi	O
)	O
(	O
1	O
+	O
σ2−i	O
)	O
φ	O
(	O
zi	O
)	O
ˆµi	O
=	O
µ−i	O
+	O
n	O
(	O
zi	O
)	O
φ	O
(	O
zi	O
)	O
zi	O
+	O
(	O
cid:16	O
)	O
ˆσ2	O
yiσ2−in	O
(	O
zi	O
)	O
√	O
(	O
cid:17	O
)	O
φ	O
(	O
zi	O
)	O
1	O
+	O
σ2−i	O
where	O
,	O
,	O
(	O
3.58	O
)	O
√	O
zi	O
=	O
yiµ−i	O
1	O
+	O
σ2−i	O
.	O
the	O
ﬁnal	O
step	O
is	O
to	O
compute	O
the	O
parameters	O
of	O
the	O
approximation	O
ti	O
which	O
achieves	O
a	O
match	O
with	O
the	O
desired	O
moments	O
.	O
in	O
particular	O
,	O
the	O
product	O
of	O
the	O
cavity	O
distribution	O
and	O
the	O
local	O
approximation	O
must	O
have	O
the	O
desired	O
moments	O
,	O
leading	O
to	O
˜µi	O
=	O
˜σ2	O
˜zi	O
=	O
ˆzi	O
p	O
i	O
ˆµi	O
−	O
σ−2−i	O
µ−i	O
)	O
,	O
i	O
(	O
ˆσ−2	O
√	O
2π	O
i	O
exp	O
(	O
cid:0	O
)	O
1	O
σ2−i	O
+	O
˜σ2	O
i	O
=	O
(	O
ˆσ−2	O
˜σ2	O
i	O
−	O
σ−2−i	O
)	O
−1	O
,	O
2	O
(	O
µ−i	O
−	O
˜µi	O
)	O
2/	O
(	O
σ2−i	O
+	O
˜σ2	O
i	O
)	O
(	O
cid:1	O
)	O
,	O
(	O
3.59	O
)	O
which	O
is	O
easily	O
veriﬁed	O
by	O
multiplying	O
the	O
cavity	O
distribution	O
by	O
the	O
local	O
ap-	O
proximation	O
using	O
eq	O
.	O
(	O
a.7	O
)	O
to	O
obtain	O
eq	O
.	O
(	O
3.58	O
)	O
.	O
note	O
that	O
the	O
desired	O
marginal	B
posterior	O
variance	O
ˆσ2	O
i	O
given	O
by	O
eq	O
.	O
(	O
3.58	O
)	O
is	O
guaranteed	O
to	O
be	O
smaller	O
than	O
the	O
cavity	O
variance	O
,	O
such	O
that	O
˜σ2	O
i	O
>	O
0	O
is	O
always	O
satisﬁed.15	O
this	O
completes	O
the	O
update	O
for	O
a	O
local	O
likelihood	B
approximation	O
ti	O
.	O
we	O
then	O
have	O
to	O
update	O
the	O
approximate	O
posterior	O
using	O
eq	O
.	O
(	O
3.53	O
)	O
,	O
but	O
since	O
only	O
a	O
15in	O
cases	O
where	O
the	O
likelihood	B
is	O
log	O
concave	O
,	O
one	O
can	O
show	O
that	O
˜σ2	O
i	O
>	O
0	O
,	O
but	O
for	O
a	O
general	O
likelihood	B
there	O
may	O
be	O
no	O
such	O
guarantee	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
56	O
classiﬁcation	B
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
3.4	O
:	O
approximating	O
a	O
single	O
likelihood	B
term	O
by	O
a	O
gaussian	O
.	O
panel	O
(	O
a	O
)	O
dash-	O
dotted	O
:	O
the	O
exact	O
likelihood	B
,	O
φ	O
(	O
fi	O
)	O
(	O
the	O
corresponding	O
target	O
being	O
yi	O
=	O
1	O
)	O
as	O
a	O
function	B
of	O
the	O
latent	O
fi	O
,	O
dotted	O
:	O
gaussian	O
cavity	O
distribution	O
n	O
(	O
fi|µ−i	O
=	O
1	O
,	O
σ2−i	O
=	O
9	O
)	O
,	O
solid	O
:	O
posterior	O
,	O
dashed	O
:	O
posterior	O
approximation	O
.	O
panel	O
(	O
b	O
)	O
shows	O
an	O
enlargement	O
of	O
panel	O
(	O
a	O
)	O
.	O
single	O
site	O
has	O
changed	O
one	O
can	O
do	O
this	O
with	O
a	O
computationally	O
eﬃcient	O
rank-	O
one	O
update	O
,	O
see	B
section	O
3.6.3.	O
the	O
ep	O
algorithm	O
is	O
used	O
iteratively	O
,	O
updating	O
each	O
local	O
approximation	O
in	O
turn	O
.	O
it	O
is	O
clear	O
that	O
several	O
passes	O
over	O
the	O
data	O
are	O
required	O
,	O
since	O
an	O
update	O
of	O
one	O
local	O
approximation	O
potentially	O
inﬂuences	O
all	O
of	O
the	O
approximate	O
marginal	B
posteriors	O
.	O
3.6.1	O
predictions	O
the	O
procedure	O
for	O
making	O
predictions	O
in	O
the	O
ep	O
framework	O
closely	O
resembles	O
the	O
algorithm	O
for	O
the	O
laplace	O
approximation	O
in	O
section	O
3.4.2.	O
ep	O
gives	O
a	O
gaus-	O
sian	O
approximation	O
to	O
the	O
posterior	O
distribution	O
,	O
eq	O
.	O
(	O
3.53	O
)	O
.	O
the	O
approximate	O
predictive	B
mean	O
for	O
the	O
latent	O
variable	O
f∗	O
becomes	O
eq	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
k	O
>	O
∗	O
k−1µ	O
=	O
k	O
>	O
=	O
k	O
>	O
∗	O
k−1	O
(	O
k−1	O
+	O
˜σ−1	O
)	O
−1	O
˜σ−1	O
˜µ	O
∗	O
(	O
k	O
+	O
˜σ	O
)	O
−1	O
˜µ	O
.	O
(	O
3.60	O
)	O
the	O
approximate	O
latent	O
predictive	O
variance	O
is	O
analogous	O
to	O
the	O
derivation	O
from	O
eq	O
.	O
(	O
3.23	O
)	O
and	O
eq	O
.	O
(	O
3.24	O
)	O
,	O
with	O
˜σ	O
playing	O
the	O
rˆole	O
of	O
w	O
vq	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
k	O
>	O
∗	O
(	O
k	O
+	O
˜σ	O
)	O
−1k∗	O
.	O
(	O
3.61	O
)	O
q	O
(	O
y∗	O
=	O
1|x	O
,	O
y	O
,	O
x∗	O
)	O
=	O
eq	O
[	O
π∗|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
the	O
approximate	O
predictive	B
distribution	O
for	O
the	O
binary	B
target	O
becomes	O
φ	O
(	O
f∗	O
)	O
q	O
(	O
f∗|x	O
,	O
y	O
,	O
x∗	O
)	O
df∗	O
,	O
(	O
3.62	O
)	O
where	O
q	O
(	O
f∗|x	O
,	O
y	O
,	O
x∗	O
)	O
is	O
the	O
approximate	O
latent	O
predictive	O
gaussian	O
with	O
mean	O
and	O
variance	O
given	O
by	O
eq	O
.	O
(	O
3.60	O
)	O
and	O
eq	O
.	O
(	O
3.61	O
)	O
.	O
this	O
integral	O
is	O
readily	O
evaluated	O
using	O
eq	O
.	O
(	O
3.80	O
)	O
,	O
giving	O
the	O
predictive	B
probability	O
z	O
(	O
cid:17	O
)	O
q	O
(	O
y∗	O
=	O
1|x	O
,	O
y	O
,	O
x∗	O
)	O
=	O
φ	O
k	O
>	O
∗	O
(	O
k	O
+	O
˜σ	O
)	O
−1	O
˜µ	O
1	O
+	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
k	O
>	O
∗	O
(	O
k	O
+	O
˜σ	O
)	O
−1k∗	O
.	O
(	O
3.63	O
)	O
(	O
cid:16	O
)	O
p	O
−5051000.20.40.60.81likelihoodcavityposteriorapproximation−5051000.020.040.060.080.10.120.14	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.6	O
expectation	B
propagation	I
3.6.2	O
marginal	B
likelihood	I
57	O
the	O
ep	O
approximation	O
to	O
the	O
marginal	B
likelihood	I
can	O
be	O
found	O
from	O
the	O
nor-	O
malization	O
of	O
eq	O
.	O
(	O
3.53	O
)	O
z	O
ny	O
zep	O
=	O
q	O
(	O
y|x	O
)	O
=	O
p	O
(	O
f|x	O
)	O
ti	O
(	O
fi|	O
˜zi	O
,	O
˜µi	O
,	O
˜σ2	O
i	O
)	O
df	O
.	O
(	O
3.64	O
)	O
using	O
eq	O
.	O
(	O
a.7	O
)	O
and	O
eq	O
.	O
(	O
a.8	O
)	O
in	O
an	O
analogous	O
way	O
to	O
the	O
treatment	O
of	O
the	O
regression	B
setting	O
in	O
equations	O
(	O
2.28	O
)	O
and	O
(	O
2.30	O
)	O
we	O
arrive	O
at	O
i=1	O
log	O
(	O
zep|θ	O
)	O
=	O
−1	O
2	O
√	O
log	O
φ	O
(	O
cid:0	O
)	O
yiµ−i	O
log	O
|k	O
+	O
˜σ|	O
−	O
1	O
nx	O
2	O
(	O
cid:1	O
)	O
+	O
nx	O
+	O
1	O
2	O
i=1	O
i=1	O
1	O
+	O
σ2−i	O
˜µ	O
>	O
(	O
k	O
+	O
˜σ	O
)	O
−1	O
˜µ	O
log	O
(	O
σ2−i	O
+	O
˜σ2	O
i	O
)	O
+	O
nx	O
i=1	O
(	O
3.65	O
)	O
(	O
µ−i	O
−	O
˜µi	O
)	O
2	O
i	O
)	O
,	O
2	O
(	O
σ2−i	O
+	O
˜σ2	O
where	O
θ	O
denotes	O
the	O
hyperparameters	B
of	O
the	O
covariance	B
function	I
.	O
this	O
expres-	O
sion	O
has	O
a	O
nice	O
intuitive	O
interpretation	O
:	O
the	O
ﬁrst	O
two	O
terms	O
are	O
the	O
marginal	B
likelihood	I
for	O
a	O
regression	B
model	O
for	O
˜µ	O
,	O
each	O
component	O
of	O
which	O
has	O
inde-	O
pendent	O
gaussian	O
noise	O
of	O
variance	O
˜σii	O
(	O
as	O
˜σ	O
is	O
diagonal	O
)	O
,	O
cf	O
.	O
eq	O
.	O
(	O
2.30	O
)	O
.	O
the	O
remaining	O
three	O
terms	O
come	O
from	O
the	O
normalization	O
constants	O
˜zi	O
.	O
the	O
ﬁrst	O
of	O
these	O
penalizes	O
the	O
cavity	O
(	O
or	O
leave-one-out	B
)	O
distributions	O
for	O
not	O
agreeing	O
with	O
the	O
classiﬁcation	B
labels	O
,	O
see	B
eq	O
.	O
(	O
3.82	O
)	O
.	O
in	O
other	O
words	O
,	O
we	O
can	O
see	B
that	O
the	O
marginal	B
likelihood	I
combines	O
two	O
desiderata	O
,	O
(	O
1	O
)	O
the	O
means	O
of	O
the	O
local	O
likelihood	B
approximations	O
should	O
be	O
well	O
predicted	O
by	O
a	O
gp	O
,	O
and	O
(	O
2	O
)	O
the	O
corre-	O
sponding	O
latent	O
function	O
,	O
when	O
ignoring	O
a	O
particular	O
training	O
example	O
,	O
should	O
be	O
able	O
to	O
predict	O
the	O
corresponding	O
classiﬁcation	B
label	O
well	O
.	O
3.6.3	O
implementation	O
the	O
implementation	O
for	O
the	O
ep	O
algorithm	O
follows	O
the	O
derivation	O
in	O
the	O
previous	O
section	O
closely	O
,	O
except	O
that	O
care	O
has	O
to	O
be	O
taken	O
to	O
achieve	O
numerical	O
stability	O
,	O
in	O
similar	O
ways	O
to	O
the	O
considerations	O
for	O
laplace	O
’	O
s	O
method	O
in	O
section	O
3.4.3.	O
in	O
addition	O
,	O
we	O
wish	O
to	O
be	O
able	O
to	O
speciﬁcally	O
handle	O
the	O
case	O
were	O
some	O
site	O
variances	O
˜σ2	O
i	O
may	O
tend	O
to	O
inﬁnity	O
;	O
this	O
corresponds	O
to	O
ignoring	O
the	O
corresponding	O
likelihood	B
terms	O
,	O
and	O
can	O
form	O
the	O
basis	O
of	O
sparse	O
approximations	O
,	O
touched	O
upon	O
in	O
section	O
8.4.	O
in	O
this	O
limit	O
,	O
everything	O
remains	O
well-deﬁned	O
,	O
although	O
this	O
is	O
not	O
obvious	O
e.g	O
.	O
from	O
looking	O
at	O
eq	O
.	O
(	O
3.65	O
)	O
.	O
it	O
turns	O
out	O
to	O
be	O
slightly	O
more	O
convenient	O
to	O
use	O
natural	O
parameters	O
˜τi	O
,	O
˜νi	O
and	O
τ−i	O
,	O
ν−i	O
for	O
the	O
site	O
and	O
cavity	O
parameters	O
˜τi	O
=	O
˜σ−2	O
i	O
,	O
rather	O
than	O
˜σ2	O
importance	O
is	O
ν−i	O
=	O
τ−iµ−i	O
(	O
3.66	O
)	O
i	O
,	O
˜µi	O
and	O
σ2−i	O
,	O
µ−i	O
themselves	O
.	O
the	O
symmetric	O
matrix	B
of	O
central	O
τ−i	O
=	O
σ−2−i	O
,	O
˜s	O
=	O
diag	O
(	O
˜τ	O
)	O
,	O
˜ν	O
=	O
˜s	O
˜µ	O
,	O
(	O
3.67	O
)	O
which	O
plays	O
a	O
rˆole	O
equivalent	B
to	O
eq	O
.	O
(	O
3.26	O
)	O
.	O
expressions	O
involving	O
the	O
inverse	O
of	O
b	O
are	O
computed	O
via	O
cholesky	O
factorization	O
,	O
which	O
is	O
numerically	O
stable	O
since	O
b	O
=	O
i	O
+	O
˜s	O
1	O
2	O
,	O
1	O
2	O
k	O
˜s	O
natural	O
parameters	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
58	O
classiﬁcation	B
input	O
:	O
k	O
(	O
covariance	B
matrix	I
)	O
,	O
y	O
(	O
±1	O
targets	O
)	O
2	O
:	O
˜ν	O
:	O
=	O
0	O
,	O
˜τ	O
:	O
=	O
0	O
,	O
σ	O
:	O
=	O
k	O
,	O
µ	O
:	O
=	O
0	O
initialization	O
and	O
eq	O
.	O
(	O
3.53	O
)	O
repeat	O
o	O
4	O
:	O
6	O
:	O
8	O
:	O
10	O
:	O
12	O
:	O
14	O
:	O
for	O
i	O
:	O
=	O
1	O
to	O
n	O
do	O
i	O
−	O
˜τi	O
i	O
µi	O
−	O
˜νi	O
i	O
−	O
τ−i	O
−	O
˜τi	O
and	O
˜τi	O
:	O
=	O
˜τi	O
+	O
∆˜τ	O
o	O
i	O
ˆµi	O
−	O
ν−i	O
τ−i	O
:	O
=	O
σ−2	O
ν−i	O
:	O
=	O
σ−2	O
compute	O
the	O
marginal	B
moments	O
ˆµi	O
and	O
ˆσ2	O
∆˜τ	O
:	O
=	O
ˆσ−2	O
i	O
˜νi	O
:	O
=	O
ˆσ−2	O
σ	O
:	O
=	O
σ	O
−	O
(	O
cid:0	O
)	O
(	O
∆˜τ	O
)	O
−1	O
+	O
σii	O
(	O
cid:1	O
)	O
−1si	O
s	O
>	O
i	O
o	O
compute	O
approximate	O
cavity	O
para-	O
meters	O
ν−i	O
and	O
τ−i	O
using	O
eq	O
.	O
(	O
3.56	O
)	O
using	O
eq	O
.	O
(	O
3.58	O
)	O
update	O
site	O
parameters	O
˜τi	O
and	O
˜νi	O
using	O
eq	O
.	O
(	O
3.59	O
)	O
update	O
σ	O
and	O
µ	O
by	O
eq	O
.	O
(	O
3.70	O
)	O
and	O
eq	O
.	O
(	O
3.53	O
)	O
.	O
si	O
is	O
column	O
i	O
of	O
σ	O
µ	O
:	O
=	O
σ˜ν	O
end	O
for	O
l	O
:	O
=	O
cholesky	O
(	O
in	O
+	O
˜s	O
2	O
)	O
v	O
:	O
=	O
l	O
>	O
\	O
˜s	O
σ	O
:	O
=	O
k	O
−	O
v	O
>	O
v	O
and	O
µ	O
:	O
=	O
σ˜ν	O
2	O
k	O
˜s	O
1	O
2	O
k	O
1	O
1	O
)	O
re-compute	O
the	O
approximate	O
posterior	O
parameters	O
σ	O
and	O
µ	O
using	O
eq	O
.	O
(	O
3.53	O
)	O
and	O
eq	O
.	O
(	O
3.68	O
)	O
16	O
:	O
until	O
convergence	O
compute	O
log	O
zep	O
using	O
eq	O
.	O
(	O
3.65	O
)	O
,	O
(	O
3.73	O
)	O
and	O
(	O
3.74	O
)	O
and	O
the	O
existing	O
l	O
18	O
:	O
return	O
:	O
˜ν	O
,	O
˜τ	O
(	O
natural	O
site	O
param	O
.	O
)	O
,	O
log	O
zep	O
(	O
approx	O
.	O
log	O
marg	O
.	O
likelihood	B
)	O
algorithm	O
3.5	O
:	O
expectation	B
propagation	I
for	O
binary	B
classiﬁcation	I
.	O
the	O
targets	O
y	O
are	O
used	O
only	O
in	O
line	O
7.	O
in	O
lines	O
13-15	O
the	O
parameters	O
of	O
the	O
approximate	O
posterior	O
are	O
re-computed	O
(	O
although	O
they	O
already	O
exist	O
)	O
;	O
this	O
is	O
done	O
because	O
of	O
the	O
large	O
number	O
of	O
rank-one	O
updates	O
in	O
line	O
10	O
which	O
would	O
eventually	O
cause	O
loss	B
of	O
numerical	O
precision	O
in	O
σ.	O
the	O
computational	O
complexity	O
is	O
dominated	O
by	O
the	O
rank-one	O
updates	O
in	O
line	O
10	O
,	O
which	O
takes	O
o	O
(	O
n2	O
)	O
per	O
variable	O
,	O
i.e	O
.	O
o	O
(	O
n3	O
)	O
for	O
an	O
entire	O
sweep	O
over	O
all	O
variables	O
.	O
similarly	O
re-computing	O
σ	O
in	O
lines	O
13-15	O
is	O
o	O
(	O
n3	O
)	O
.	O
the	O
eigenvalues	O
of	O
b	O
are	O
bounded	O
below	O
by	O
one	O
.	O
the	O
parameters	O
of	O
the	O
gaussian	O
approximate	O
posterior	O
from	O
eq	O
.	O
(	O
3.53	O
)	O
are	O
computed	O
as	O
σ	O
=	O
(	O
k−1	O
+	O
˜s	O
)	O
−1	O
=	O
k	O
−	O
k	O
(	O
k	O
+	O
˜s−1	O
)	O
−1k	O
=	O
k	O
−	O
k	O
˜s	O
2	O
k.	O
(	O
3.68	O
)	O
2	O
b−1	O
˜s	O
1	O
1	O
after	O
updating	O
the	O
parameters	O
of	O
a	O
site	O
,	O
we	O
need	O
to	O
update	O
the	O
approximate	O
posterior	O
eq	O
.	O
(	O
3.53	O
)	O
taking	O
the	O
new	O
site	O
parameters	O
into	O
account	O
.	O
for	O
the	O
inverse	O
covariance	B
matrix	I
of	O
the	O
approximate	O
posterior	O
we	O
have	O
from	O
eq	O
.	O
(	O
3.53	O
)	O
σ−1	O
=	O
k−1	O
+	O
˜s	O
,	O
and	O
thus	O
σ−1	O
)	O
eie	O
>	O
i	O
,	O
(	O
3.69	O
)	O
where	O
ei	O
is	O
a	O
unit	O
vector	O
in	O
direction	O
i	O
,	O
and	O
we	O
have	O
used	O
that	O
˜s	O
=	O
diag	O
(	O
˜τ	O
)	O
.	O
using	O
the	O
matrix	B
inversion	O
lemma	O
eq	O
.	O
(	O
a.9	O
)	O
,	O
on	O
eq	O
.	O
(	O
3.69	O
)	O
we	O
obtain	O
the	O
new	O
σ	O
new	O
=	O
k−1	O
+	O
˜sold	O
+	O
(	O
˜τ	O
new	O
i	O
−	O
˜τ	O
old	O
i	O
σnew	O
=	O
σold	O
−	O
i	O
−	O
˜τ	O
old	O
˜τ	O
new	O
i	O
−	O
˜τ	O
old	O
1	O
+	O
(	O
˜τ	O
new	O
i	O
i	O
)	O
σold	O
ii	O
sis	O
>	O
i	O
,	O
(	O
3.70	O
)	O
in	O
time	O
o	O
(	O
n2	O
)	O
,	O
where	O
si	O
is	O
the	O
i	O
’	O
th	O
column	O
of	O
σold	O
.	O
the	O
posterior	O
mean	O
is	O
then	O
calculated	O
from	O
eq	O
.	O
(	O
3.53	O
)	O
.	O
in	O
the	O
ep	O
algorithm	O
each	O
site	O
is	O
updated	O
in	O
turn	O
,	O
and	O
several	O
passes	O
over	O
all	O
sites	O
are	O
required	O
.	O
pseudocode	O
for	O
the	O
ep-gpc	O
algorithm	O
is	O
given	O
in	O
algorithm	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.6	O
expectation	B
propagation	I
59	O
input	O
:	O
˜ν	O
,	O
˜τ	O
(	O
natural	O
site	O
param	O
.	O
)	O
,	O
x	O
(	O
inputs	O
)	O
,	O
y	O
(	O
±1	O
targets	O
)	O
,	O
1	O
2	O
)	O
1	O
1	O
1	O
z	O
:	O
=	O
˜s	O
2	O
k	O
˜s	O
2	O
k	O
˜ν	O
)	O
)	O
2	O
l	O
>	O
\	O
(	O
l\	O
(	O
˜s	O
v	O
:	O
=	O
l\	O
(	O
cid:0	O
)	O
˜s	O
2	O
:	O
l	O
:	O
=	O
cholesky	O
(	O
in	O
+	O
˜s	O
4	O
:	O
¯f∗	O
:	O
=	O
k	O
(	O
x∗	O
)	O
>	O
(	O
˜ν	O
−	O
z	O
)	O
6	O
:	O
v	O
[	O
f∗	O
]	O
:	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
v	O
>	O
v	O
1	O
+	O
v	O
[	O
f∗	O
]	O
)	O
2	O
k	O
(	O
x∗	O
)	O
(	O
cid:1	O
)	O
¯π∗	O
:	O
=	O
φ	O
(	O
¯f∗/	O
√	O
1	O
k	O
(	O
covariance	B
function	I
)	O
,	O
x∗	O
test	O
input	O
2	O
k	O
˜s	O
eq	O
.	O
(	O
3.60	O
)	O
using	O
eq	O
.	O
(	O
3.71	O
)	O
b	O
=	O
in	O
+	O
˜s	O
1	O
2	O
1	O
o	O
o	O
eq	O
.	O
(	O
3.61	O
)	O
using	O
eq	O
.	O
(	O
3.72	O
)	O
eq	O
.	O
(	O
3.63	O
)	O
8	O
:	O
return	O
:	O
¯π∗	O
(	O
predictive	B
class	O
probability	B
(	O
for	O
class	O
1	O
)	O
)	O
algorithm	O
3.6	O
:	O
predictions	O
for	O
expectation	B
propagation	I
.	O
the	O
natural	O
site	O
parameters	O
˜ν	O
and	O
˜τ	O
of	O
the	O
posterior	O
(	O
which	O
can	O
be	O
computed	O
using	O
algorithm	O
3.5	O
)	O
are	O
input	O
.	O
for	O
multiple	O
test	O
inputs	O
lines	O
4-7	O
are	O
applied	O
to	O
each	O
test	O
input	O
.	O
computational	O
complexity	O
is	O
n3/6	O
+	O
n2	O
operations	O
once	O
(	O
line	O
2	O
and	O
3	O
)	O
plus	O
n2	O
operations	O
per	O
test	O
case	O
(	O
line	O
5	O
)	O
,	O
although	O
the	O
cholesky	O
decomposition	O
in	O
line	O
2	O
could	O
be	O
avoided	O
by	O
storing	O
it	O
in	O
algorithm	O
3.5.	O
note	O
the	O
close	O
similarity	O
to	O
algorithm	O
3.2	O
on	O
page	O
47	O
.	O
3.5.	O
there	O
is	O
no	O
formal	O
guarantee	O
of	O
convergence	O
,	O
but	O
several	O
authors	O
have	O
reported	O
that	O
ep	O
for	O
gaussian	O
process	B
models	O
works	O
relatively	O
well.16	O
for	O
the	O
predictive	B
distribution	O
,	O
we	O
get	O
the	O
mean	O
from	O
eq	O
.	O
(	O
3.60	O
)	O
which	O
is	O
evaluated	O
using	O
eq	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
k	O
>	O
∗	O
(	O
k	O
+	O
˜s−1	O
)	O
−1	O
˜s−1	O
˜ν	O
=	O
k	O
>	O
=	O
k	O
>	O
∗	O
(	O
i	O
−	O
˜s	O
and	O
the	O
predictive	B
variance	O
from	O
eq	O
.	O
(	O
3.61	O
)	O
similarly	O
by	O
∗	O
(	O
cid:0	O
)	O
i	O
−	O
(	O
k	O
+	O
˜s−1	O
)	O
−1k	O
(	O
cid:1	O
)	O
˜ν	O
1	O
2	O
b−1	O
˜s	O
1	O
2	O
k	O
)	O
˜ν	O
,	O
vq	O
[	O
f∗|x	O
,	O
y	O
,	O
x∗	O
]	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
k	O
>	O
∗	O
(	O
k	O
+	O
˜s−1	O
)	O
−1k∗	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
k	O
>	O
∗	O
˜s	O
2	O
k∗	O
.	O
2	O
b−1	O
˜s	O
1	O
1	O
(	O
3.71	O
)	O
(	O
3.72	O
)	O
pseudocode	O
for	O
making	O
predictions	O
using	O
ep	O
is	O
given	O
in	O
algorithm	O
3.6.	O
finally	O
,	O
we	O
need	O
to	O
evaluate	O
the	O
approximate	O
log	O
marginal	O
likelihood	B
from	O
eq	O
.	O
(	O
3.65	O
)	O
.	O
there	O
are	O
several	O
terms	O
which	O
need	O
careful	O
consideration	O
,	O
principally	O
due	O
to	O
the	O
fact	O
the	O
˜τi	O
values	O
may	O
be	O
arbitrarily	O
small	O
(	O
and	O
can	O
not	O
safely	O
be	O
inverted	O
)	O
.	O
we	O
start	O
with	O
the	O
fourth	O
and	O
ﬁrst	O
terms	O
of	O
eq	O
.	O
(	O
3.65	O
)	O
x	O
2	O
log	O
|	O
˜s−1	O
(	O
i	O
+	O
˜st	O
−1	O
)	O
|	O
−	O
1	O
2	O
log	O
|t	O
−1+	O
˜s−1|	O
−	O
1	O
log	O
(	O
1+˜τiτ−1−i	O
)	O
−x	O
2	O
log	O
|k	O
+	O
˜σ|	O
=	O
1	O
=	O
1	O
2	O
2	O
log	O
|	O
˜s−1b|	O
log	O
lii	O
,	O
(	O
3.73	O
)	O
1	O
i	O
i	O
where	O
t	O
is	O
a	O
diagonal	O
matrix	B
of	O
cavity	O
precisions	O
tii	O
=	O
τ−i	O
=	O
σ−2−i	O
and	O
l	O
is	O
the	O
cholesky	O
factorization	O
of	O
b.	O
in	O
eq	O
.	O
(	O
3.73	O
)	O
we	O
have	O
factored	O
out	O
the	O
matrix	B
˜s−1	O
from	O
both	O
determinants	O
,	O
and	O
the	O
terms	O
cancel	O
.	O
continuing	O
with	O
the	O
part	O
of	O
the	O
16it	O
has	O
been	O
conjectured	O
(	O
but	O
not	O
proven	O
)	O
by	O
l.	O
csat´o	O
(	O
personal	O
communication	O
)	O
that	O
ep	O
is	O
guaranteed	O
to	O
converge	O
if	O
the	O
likelihood	B
is	O
log	O
concave	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
60	O
classiﬁcation	B
ﬁfth	O
term	O
from	O
eq	O
.	O
(	O
3.65	O
)	O
which	O
is	O
quadratic	O
in	O
˜µ	O
together	O
with	O
the	O
second	O
term	O
1	O
2	O
˜µ	O
>	O
(	O
t	O
−1	O
+	O
˜s−1	O
)	O
−1	O
˜µ	O
−	O
1	O
2	O
˜µ	O
>	O
(	O
k	O
+	O
˜σ	O
)	O
−1	O
˜µ	O
2	O
˜ν	O
>	O
˜s−1	O
(	O
cid:0	O
)	O
(	O
t	O
−1	O
+	O
˜s−1	O
)	O
−1	O
−	O
(	O
k	O
+	O
˜s−1	O
)	O
−1	O
(	O
cid:1	O
)	O
˜s−1	O
˜ν	O
2	O
˜ν	O
>	O
(	O
cid:0	O
)	O
(	O
k−1	O
+	O
˜s	O
)	O
−1	O
−	O
(	O
t	O
+	O
˜s	O
)	O
−1	O
(	O
cid:1	O
)	O
˜ν	O
2	O
˜ν	O
>	O
(	O
cid:0	O
)	O
k	O
−	O
k	O
˜s	O
2	O
k	O
−	O
(	O
t	O
+	O
˜s	O
)	O
−1	O
(	O
cid:1	O
)	O
˜ν	O
,	O
2	O
b−1	O
˜s	O
1	O
1	O
=	O
1	O
=	O
1	O
=	O
1	O
(	O
3.74	O
)	O
where	O
in	O
eq	O
.	O
(	O
3.74	O
)	O
we	O
apply	O
the	O
matrix	B
inversion	O
lemma	O
eq	O
.	O
(	O
a.9	O
)	O
to	O
both	O
parenthesis	O
to	O
be	O
inverted	O
.	O
the	O
remainder	O
of	O
the	O
ﬁfth	O
term	O
in	O
eq	O
.	O
(	O
3.65	O
)	O
is	O
evaluated	O
using	O
the	O
identity	O
1	O
2	O
µ	O
>	O
−i	O
(	O
t	O
−1	O
+	O
˜s−1	O
)	O
−1	O
(	O
µ−i	O
−	O
2˜µ	O
)	O
=	O
1	O
2	O
µ	O
>	O
−it	O
(	O
˜s	O
+	O
t	O
)	O
−1	O
(	O
˜sµ−i	O
−	O
2˜ν	O
)	O
,	O
(	O
3.75	O
)	O
where	O
µ−i	O
is	O
the	O
vector	O
of	O
cavity	O
means	O
µ−i	O
.	O
the	O
third	O
term	O
in	O
eq	O
.	O
(	O
3.65	O
)	O
requires	O
in	O
no	O
special	O
treatment	O
and	O
can	O
be	O
evaluated	O
as	O
written	O
.	O
3.7	O
experiments	O
in	O
this	O
section	O
we	O
present	O
the	O
results	O
of	O
applying	O
the	O
algorithms	O
for	O
gp	O
clas-	O
siﬁcation	O
discussed	O
in	O
the	O
previous	O
sections	O
to	O
several	O
data	O
sets	O
.	O
the	O
purpose	O
is	O
ﬁrstly	O
to	O
illustrate	O
the	O
behaviour	O
of	O
the	O
methods	O
and	O
secondly	O
to	O
gain	O
some	O
insights	O
into	O
how	O
good	O
the	O
performance	O
is	O
compared	O
to	O
some	O
other	O
commonly-	O
used	O
machine	O
learning	B
methods	O
for	O
classiﬁcation	B
.	O
section	O
3.7.1	O
illustrates	O
the	O
action	O
of	O
a	O
gp	O
classiﬁer	B
on	O
a	O
toy	O
binary	B
pre-	O
diction	O
problem	O
with	O
a	O
2-d	O
input	O
space	O
,	O
and	O
shows	O
the	O
eﬀect	O
of	O
varying	O
the	O
length-scale	B
‘	O
in	O
the	O
se	O
covariance	B
function	I
.	O
in	O
section	O
3.7.2	O
we	O
illustrate	O
and	O
compare	O
the	O
behaviour	O
of	O
the	O
two	O
approximate	O
gp	O
methods	O
on	O
a	O
simple	O
one-	O
dimensional	O
binary	B
task	O
.	O
in	O
section	O
3.7.3	O
we	O
present	O
results	O
for	O
a	O
binary	B
gp	O
classiﬁer	B
on	O
a	O
handwritten	O
digit	O
classiﬁcation	B
task	O
,	O
and	O
study	O
the	O
eﬀect	O
of	O
vary-	O
ing	O
the	O
kernel	B
parameters	O
.	O
in	O
section	O
3.7.4	O
we	O
carry	O
out	O
a	O
similar	O
study	O
using	O
a	O
multi-class	B
gp	O
classiﬁer	B
to	O
classify	O
digits	O
from	O
all	O
ten	O
classes	O
0-9.	O
in	O
section	O
3.8	O
we	O
discuss	O
the	O
methods	O
from	O
both	O
experimental	O
and	O
theoretical	O
viewpoints	O
.	O
3.7.1	O
a	O
toy	O
problem	O
figure	O
3.5	O
illustrates	O
the	O
operation	O
of	O
a	O
gaussian	O
process	B
classiﬁer	O
on	O
a	O
binary	B
problem	O
using	O
the	O
squared	B
exponential	I
kernel	O
with	O
a	O
variable	O
length-scale	B
and	O
the	O
logistic	B
response	O
function	B
.	O
the	O
laplace	O
approximation	O
was	O
used	O
to	O
make	O
the	O
plots	O
.	O
the	O
data	O
points	O
lie	O
within	O
the	O
square	O
[	O
0	O
,	O
1	O
]	O
2	O
,	O
as	O
shown	O
in	O
panel	O
(	O
a	O
)	O
.	O
notice	O
in	O
particular	O
the	O
lone	O
white	O
point	O
amongst	O
the	O
black	O
points	O
in	O
the	O
ne	O
corner	O
,	O
and	O
the	O
lone	O
black	O
point	O
amongst	O
the	O
white	O
points	O
in	O
the	O
sw	O
corner	O
.	O
in	O
panel	O
(	O
b	O
)	O
the	O
length-scale	B
is	O
‘	O
=	O
0.1	O
,	O
a	O
relatively	O
short	O
value	O
.	O
in	O
this	O
case	O
the	O
latent	O
function	O
is	O
free	O
to	O
vary	O
relatively	O
quickly	O
and	O
so	O
the	O
classiﬁcations	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.7	O
experiments	O
61	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
3.5	O
:	O
panel	O
(	O
a	O
)	O
shows	O
the	O
location	O
of	O
the	O
data	O
points	O
in	O
the	O
two-dimensional	O
space	O
[	O
0	O
,	O
1	O
]	O
2.	O
the	O
two	O
classes	O
are	O
labelled	O
as	O
open	O
circles	O
(	O
+1	O
)	O
and	O
closed	O
circles	O
(	O
-	O
1	O
)	O
.	O
panels	O
(	O
b	O
)	O
-	O
(	O
d	O
)	O
show	O
contour	O
plots	O
of	O
the	O
predictive	B
probability	O
eq	O
[	O
π	O
(	O
x∗	O
)	O
|y	O
]	O
for	O
signal	O
variance	O
σ2	O
f	O
=	O
9	O
and	O
length-scales	O
‘	O
of	O
0.1	O
,	O
0.2	O
and	O
0.3	O
respectively	O
.	O
the	O
de-	O
cision	O
boundaries	O
between	O
the	O
two	O
classes	O
are	O
shown	O
by	O
the	O
thicker	O
black	O
lines	O
.	O
the	O
maximum	O
value	O
attained	O
is	O
0.84	O
,	O
and	O
the	O
minimum	O
is	O
0.19.	O
provided	O
by	O
thresholding	O
the	O
predictive	B
probability	O
eq	O
[	O
π	O
(	O
x∗	O
)	O
|y	O
]	O
at	O
0.5	O
agrees	O
with	O
the	O
training	O
labels	O
at	O
all	O
data	O
points	O
.	O
in	O
contrast	O
,	O
in	O
panel	O
(	O
d	O
)	O
the	O
length-	O
scale	O
is	O
set	B
to	O
‘	O
=	O
0.3.	O
now	O
the	O
latent	O
function	O
must	O
vary	O
more	O
smoothly	O
,	O
and	O
so	O
the	O
two	O
lone	O
points	O
are	O
misclassiﬁed	O
.	O
panel	O
(	O
c	O
)	O
was	O
obtained	O
with	O
‘	O
=	O
0.2.	O
as	O
would	O
be	O
expected	O
,	O
the	O
decision	O
boundaries	O
are	O
more	O
complex	O
for	O
shorter	O
length-scales	O
.	O
methods	O
for	O
setting	O
the	O
hyperparameters	B
based	O
on	O
the	O
data	O
are	O
discussed	O
in	O
chapter	O
5	O
.	O
(	O
cid:176	O
)	O
(	O
cid:176	O
)	O
(	O
cid:176	O
)	O
•	O
(	O
cid:176	O
)	O
(	O
cid:176	O
)	O
(	O
cid:176	O
)	O
(	O
cid:176	O
)	O
(	O
cid:176	O
)	O
•	O
(	O
cid:176	O
)	O
••••••	O
(	O
cid:176	O
)	O
••0.250.50.50.50.750.250.30.50.70.30.50.7	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
62	O
classiﬁcation	B
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
3.6	O
:	O
one-dimensional	O
toy	O
classiﬁcation	B
dataset	O
:	O
panel	O
(	O
a	O
)	O
shows	O
the	O
dataset	B
,	O
where	O
points	O
from	O
class	O
+1	O
have	O
been	O
plotted	O
at	O
π	O
=	O
1	O
and	O
class	O
−1	O
at	O
π	O
=	O
0	O
,	O
together	O
with	O
the	O
predictive	B
probability	O
for	O
laplace	O
’	O
s	O
method	O
and	O
the	O
ep	O
approximation	O
.	O
also	O
shown	O
is	O
the	O
probability	B
p	O
(	O
y	O
=	O
+1|x	O
)	O
of	O
the	O
data	O
generating	O
process	B
.	O
panel	O
(	O
b	O
)	O
shows	O
the	O
corresponding	O
distribution	O
of	O
the	O
latent	O
function	O
f	O
(	O
x	O
)	O
,	O
showing	O
curves	O
for	O
the	O
mean	O
,	O
and	O
±2	O
standard	O
deviations	O
,	O
corresponding	O
to	O
95	O
%	O
conﬁdence	O
regions	O
.	O
3.7.2	O
one-dimensional	O
example	O
although	O
laplace	O
’	O
s	O
method	O
and	O
the	O
ep	O
approximation	O
often	O
give	O
similar	O
re-	O
sults	O
,	O
we	O
here	O
present	O
a	O
simple	O
one-dimensional	O
problem	O
which	O
highlights	O
some	O
of	O
the	O
diﬀerences	O
between	O
the	O
methods	O
.	O
the	O
data	O
,	O
shown	O
in	O
figure	O
3.6	O
(	O
a	O
)	O
,	O
consists	O
of	O
60	O
data	O
points	O
in	O
three	O
groups	O
,	O
generated	O
from	O
a	O
mixture	O
of	O
three	O
gaussians	O
,	O
centered	O
on	O
−6	O
(	O
20	O
points	O
)	O
,	O
0	O
(	O
30	O
points	O
)	O
and	O
2	O
(	O
10	O
points	O
)	O
,	O
where	O
the	O
middle	O
component	O
has	O
label	O
−1	O
and	O
the	O
two	O
other	O
components	O
label	O
+1	O
;	O
all	O
components	O
have	O
standard	O
deviation	O
0.8	O
;	O
thus	O
the	O
two	O
left-most	O
components	O
are	O
well	O
separated	O
,	O
whereas	O
the	O
two	O
right-most	O
components	O
overlap	O
.	O
both	O
approximation	O
methods	O
are	O
shown	O
with	O
the	O
same	O
value	O
of	O
the	O
hyperpa-	O
rameters	O
,	O
‘	O
=	O
2.6	O
and	O
σf	O
=	O
7.0	O
,	O
chosen	O
to	O
maximize	O
the	O
approximate	O
marginal	B
likelihood	I
for	O
laplace	O
’	O
s	O
method	O
.	O
notice	O
in	O
figure	O
3.6	O
that	O
there	O
is	O
a	O
consid-	O
erable	O
diﬀerence	O
in	O
the	O
value	O
of	O
the	O
predictive	B
probability	O
for	O
negative	O
inputs	O
.	O
the	O
laplace	O
approximation	O
seems	O
overly	O
cautious	O
,	O
given	O
the	O
very	O
clear	O
separa-	O
tion	O
of	O
the	O
data	O
.	O
this	O
eﬀect	O
can	O
be	O
explained	O
as	O
a	O
consequence	O
of	O
the	O
intuition	O
that	O
the	O
inﬂuence	O
of	O
“	O
well-explained	O
data	O
points	O
”	O
is	O
eﬀectively	O
reduced	O
,	O
see	B
the	O
discussion	O
around	O
eq	O
.	O
(	O
3.19	O
)	O
.	O
because	O
the	O
points	O
in	O
the	O
left	O
hand	O
cluster	O
are	O
relatively	O
well-explained	O
by	O
the	O
model	B
,	O
they	O
don	O
’	O
t	O
contribute	O
as	O
strongly	O
to	O
the	O
posterior	O
,	O
and	O
thus	O
the	O
predictive	B
probability	O
never	O
gets	O
very	O
close	O
to	O
1.	O
notice	O
in	O
figure	O
3.6	O
(	O
b	O
)	O
the	O
95	O
%	O
conﬁdence	O
region	O
for	O
the	O
latent	O
function	O
for	O
laplace	O
’	O
s	O
method	O
actually	O
includes	O
functions	O
that	O
are	O
negative	O
at	O
x	O
=	O
−6	O
,	O
which	O
does	O
not	O
seem	O
appropriate	O
.	O
for	O
the	O
positive	O
examples	O
centered	O
around	O
x	O
=	O
2	O
on	O
the	O
right-hand	O
side	O
of	O
figure	O
3.6	O
(	O
b	O
)	O
,	O
this	O
eﬀect	O
is	O
not	O
visible	O
,	O
because	O
the	O
points	O
around	O
the	O
transition	O
between	O
the	O
classes	O
at	O
x	O
=	O
1	O
are	O
not	O
so	O
“	O
well-explained	O
”	O
;	O
this	O
is	O
because	O
the	O
points	O
near	O
the	O
boundary	O
are	O
competing	O
against	O
the	O
points	O
from	O
the	O
other	O
class	O
,	O
attempting	O
to	O
pull	O
the	O
latent	O
function	O
in	O
opposite	O
di-	O
rections	O
.	O
consequently	O
,	O
the	O
datapoints	O
in	O
this	O
region	O
all	O
contribute	O
strongly	O
.	O
−8−6−4−202400.20.40.60.81input	O
,	O
xpredictive	O
probability	B
,	O
π*class	O
+1class	O
−1laplaceepp	O
(	O
y|x	O
)	O
−8−6−4−2024−10−5051015input	O
,	O
xlatent	O
function	B
,	O
f	O
(	O
x	O
)	O
laplaceep	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.7	O
experiments	O
63	O
another	O
sign	O
of	O
this	O
eﬀect	O
is	O
that	O
the	O
uncertainty	O
in	O
the	O
latent	O
function	O
,	O
which	O
is	O
closely	O
related	O
to	O
the	O
“	O
eﬀective	O
”	O
local	O
density	O
of	O
the	O
data	O
,	O
is	O
very	O
small	O
in	O
the	O
region	O
around	O
x	O
=	O
1	O
;	O
the	O
small	O
uncertainty	O
reveals	O
a	O
high	O
eﬀective	O
density	O
,	O
which	O
is	O
caused	O
by	O
all	O
data	O
points	O
in	O
the	O
region	O
contributing	O
with	O
full	O
weight	O
.	O
it	O
should	O
be	O
emphasized	O
that	O
the	O
example	O
was	O
artiﬁcially	O
constructed	O
speciﬁcally	O
to	O
highlight	O
this	O
eﬀect	O
.	O
finally	O
,	O
figure	O
3.6	O
also	O
shows	O
clearly	O
the	O
eﬀects	O
of	O
uncertainty	O
in	O
the	O
latent	O
function	O
on	O
eq	O
[	O
π∗|y	O
]	O
.	O
in	O
the	O
region	O
between	O
x	O
=	O
2	O
to	O
x	O
=	O
4	O
,	O
the	O
latent	O
mean	O
in	O
panel	O
(	O
b	O
)	O
increases	O
slightly	O
,	O
but	O
the	O
predictive	B
probability	O
decreases	O
in	O
this	O
region	O
in	O
panel	O
(	O
a	O
)	O
.	O
this	O
is	O
caused	O
by	O
the	O
increase	O
in	O
uncertainty	O
for	O
the	O
latent	O
function	O
;	O
when	O
the	O
widely	O
varying	O
functions	O
are	O
squashed	O
through	O
the	O
non-	O
linearity	O
it	O
is	O
possible	O
for	O
both	O
classes	O
to	O
get	O
high	O
probability	B
,	O
and	O
the	O
average	O
prediction	B
becomes	O
less	O
extreme	O
.	O
3.7.3	O
binary	B
handwritten	O
digit	O
classiﬁcation	B
example	O
handwritten	O
digit	O
and	O
character	O
recognition	O
are	O
popular	O
real-world	O
tasks	O
for	O
testing	O
and	O
benchmarking	O
classiﬁers	O
,	O
with	O
obvious	O
application	O
e.g	O
.	O
in	O
postal	O
services	O
.	O
in	O
this	O
section	O
we	O
consider	O
the	O
discrimination	O
of	O
images	O
of	O
the	O
digit	O
3	O
from	O
images	O
of	O
the	O
digit	O
5	O
as	O
an	O
example	O
of	O
binary	B
classiﬁcation	I
;	O
the	O
speciﬁc	O
choice	O
was	O
guided	O
by	O
the	O
experience	O
that	O
this	O
is	O
probably	O
one	O
of	O
the	O
most	O
diﬃcult	O
binary	B
subtasks	O
.	O
10-class	O
classiﬁcation	B
of	O
the	O
digits	O
0-9	O
is	O
described	O
in	O
the	O
following	O
section	O
.	O
we	O
use	O
the	O
us	O
postal	O
service	O
(	O
usps	O
)	O
database	O
of	O
handwritten	O
digits	O
which	O
consists	O
of	O
9298	O
segmented	O
16	O
×	O
16	O
greyscale	O
images	O
normalized	O
so	O
that	O
the	O
intensity	O
of	O
the	O
pixels	O
lies	O
in	O
[	O
−1	O
,	O
1	O
]	O
.	O
the	O
data	O
was	O
originally	O
split	O
into	O
a	O
training	O
set	B
of	O
7291	O
cases	O
and	O
a	O
testset	O
of	O
the	O
remaining	O
2007	O
cases	O
,	O
and	O
has	O
often	O
been	O
used	O
in	O
this	O
conﬁguration	O
.	O
unfortunately	O
,	O
the	O
data	O
in	O
the	O
two	O
partitions	O
was	O
collected	O
in	O
slightly	O
diﬀerent	O
ways	O
,	O
such	O
that	O
the	O
data	O
in	O
the	O
two	O
sets	O
did	O
not	O
stem	O
from	O
the	O
same	O
distribution.17	O
since	O
the	O
basic	O
underlying	O
assumption	O
for	O
most	O
machine	O
learning	B
algorithms	O
is	O
that	O
the	O
distribution	O
of	O
the	O
training	O
and	O
test	O
data	O
should	O
be	O
identical	O
,	O
the	O
original	O
data	O
partitions	O
are	O
not	O
really	O
suitable	O
as	O
a	O
test	O
bed	O
for	O
learning	B
algorithms	O
,	O
the	O
interpretation	O
of	O
the	O
results	O
being	O
hampered	O
by	O
the	O
change	O
in	O
distribution	O
.	O
secondly	O
,	O
the	O
original	O
test	O
set	B
was	O
rather	O
small	O
,	O
sometimes	O
making	O
it	O
diﬃcult	O
to	O
diﬀerentiate	O
the	O
performance	O
of	O
diﬀerent	O
algorithms	O
.	O
to	O
overcome	O
these	O
two	O
problems	O
,	O
we	O
decided	O
to	O
pool	O
the	O
two	O
partitions	O
and	O
randomly	O
split	O
the	O
data	O
into	O
two	O
identically	O
sized	O
partitions	O
of	O
4649	O
cases	O
each	O
.	O
a	O
side-eﬀect	O
is	O
that	O
it	O
is	O
not	O
trivial	O
to	O
compare	O
to	O
results	O
obtained	O
using	O
the	O
original	O
partitions	O
.	O
all	O
experiments	O
reported	O
here	O
use	O
the	O
repartitioned	O
data	O
.	O
the	O
binary	B
3s	O
vs.	O
5s	O
data	O
has	O
767	O
training	O
cases	O
,	O
divided	O
406/361	O
on	O
3s	O
vs.	O
5s	O
,	O
while	O
the	O
test	O
set	B
has	O
773	O
cases	O
split	O
418/355	O
.	O
usps	O
dataset	B
usps	O
repartitioned	O
we	O
present	O
results	O
of	O
both	O
laplace	O
’	O
s	O
method	O
and	O
ep	O
using	O
identical	O
ex-	O
perimental	O
setups	O
.	O
the	O
squared	B
exponential	I
covariance	O
function	B
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
squared	B
exponential	I
covariance	O
function	B
17it	O
is	O
well	O
known	O
e.g	O
.	O
that	O
the	O
original	O
test	O
partition	O
had	O
more	O
diﬃcult	O
cases	O
than	O
the	O
training	O
set	B
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
64	O
classiﬁcation	B
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
3.7	O
:	O
binary	B
laplace	O
approximation	O
:	O
3s	O
vs.	O
5s	O
discrimination	O
using	O
the	O
usps	O
data	O
.	O
panel	O
(	O
a	O
)	O
shows	O
a	O
contour	O
plot	O
of	O
the	O
log	O
marginal	O
likelihood	B
as	O
a	O
function	B
of	O
log	O
(	O
‘	O
)	O
and	O
log	O
(	O
σf	O
)	O
.	O
the	O
marginal	B
likelihood	I
has	O
an	O
optimum	O
at	O
log	O
(	O
‘	O
)	O
=	O
2.85	O
and	O
log	O
(	O
σf	O
)	O
=	O
2.35	O
,	O
with	O
an	O
optimum	O
value	O
of	O
log	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
−99	O
.	O
panel	O
(	O
b	O
)	O
shows	O
a	O
contour	O
plot	O
of	O
the	O
amount	O
of	O
information	O
(	O
in	O
excess	O
of	O
a	O
simple	O
base-line	O
model	B
,	O
see	B
text	O
)	O
about	O
the	O
test	O
cases	O
in	O
bits	B
as	O
a	O
function	B
of	O
the	O
same	O
variables	O
.	O
the	O
statistical	O
uncertainty	O
(	O
because	O
of	O
the	O
ﬁnite	O
number	O
of	O
test	O
cases	O
)	O
is	O
about	O
±0.03	O
bits	B
(	O
95	O
%	O
conﬁdence	O
interval	O
)	O
.	O
panel	O
(	O
c	O
)	O
shows	O
a	O
histogram	O
of	O
the	O
latent	O
means	O
for	O
the	O
training	O
and	O
test	O
sets	O
respectively	O
at	O
the	O
values	O
of	O
the	O
hyperparameters	B
with	O
optimal	B
marginal	O
likelihood	B
(	O
from	O
panel	O
(	O
a	O
)	O
)	O
.	O
panel	O
(	O
d	O
)	O
shows	O
the	O
number	O
of	O
test	O
errors	O
(	O
out	O
of	O
773	O
)	O
when	O
predicting	O
using	O
the	O
sign	O
of	O
the	O
latent	O
mean	O
.	O
f	O
exp	O
(	O
−|x	O
−	O
x0|2/2	O
‘	O
2	O
)	O
was	O
used	O
,	O
so	O
there	O
are	O
two	O
free	O
parameters	O
,	O
namely	O
σf	O
σ2	O
(	O
the	O
process	B
standard	O
deviation	O
,	O
which	O
controls	O
its	O
vertical	O
scaling	O
)	O
,	O
and	O
the	O
length-scale	B
‘	O
(	O
which	O
controls	O
the	O
input	O
length-scale	B
)	O
.	O
let	O
θ	O
=	O
(	O
log	O
(	O
‘	O
)	O
,	O
log	O
(	O
σf	O
)	O
)	O
denote	O
the	O
vector	O
of	O
hyperparameters	B
.	O
we	O
ﬁrst	O
present	O
the	O
results	O
of	O
laplace	O
’	O
s	O
method	O
in	O
figure	O
3.7	O
and	O
discuss	O
these	O
at	O
some	O
length	O
.	O
we	O
then	O
brieﬂy	O
compare	O
these	O
with	O
the	O
results	O
of	O
the	O
ep	O
method	O
in	O
figure	O
3.8.	O
hyperparameters	B
2345012345log	O
lengthscale	O
,	O
log	O
(	O
l	O
)	O
log	O
magnitude	O
,	O
log	O
(	O
σf	O
)	O
log	O
marginal	O
likelihood−100−105−115−115−130−150−200−2002345012345log	O
lengthscale	O
,	O
log	O
(	O
l	O
)	O
log	O
magnitude	O
,	O
log	O
(	O
σf	O
)	O
information	O
about	O
test	O
targets	O
in	O
bits0.250.250.50.50.70.70.80.80.84−50501020latent	O
means	O
,	O
ffrequencytraining	O
set	B
latent	O
means−50501020latent	O
means	O
,	O
ffrequencytest	O
set	B
latent	O
means2345012345log	O
lengthscale	O
,	O
log	O
(	O
l	O
)	O
log	O
magnitude	O
,	O
log	O
(	O
σf	O
)	O
21191919191919191918181819191919191919191919192220181819191817171717171818181818181818181818232321181818171616161515161616161616161616161529262220181717161616161717171717171717171717173329262423191817151616161616161615151515151515343430282625232320191817171717171717171818181835343430302927242322212019181818181818181919193635343432303027262322222221212021202020202020393636343532323130292725242423242322222222222341393736353632323130302625252526262626252424244240393836353632323131292627252526272828282828454240393836363632323131292726252528282829292951454240393836363632323131292726262830293030306051454240393836363632333131292728272830293030896051454240393836373632333131282828293030293088605145424039383637363233313128292826303029886051454240393836373632333131282928293030number	O
of	O
test	O
misclassifications	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.7	O
experiments	O
65	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
(	O
d	O
)	O
figure	O
3.8	O
:	O
the	O
ep	O
algorithm	O
on	O
3s	O
vs.	O
5s	O
digit	O
discrimination	O
task	O
from	O
the	O
usps	O
data	O
.	O
panel	O
(	O
a	O
)	O
shows	O
a	O
contour	O
plot	O
of	O
the	O
log	O
marginal	O
likelihood	B
as	O
a	O
function	B
of	O
the	O
hyperparameters	B
log	O
(	O
‘	O
)	O
and	O
log	O
(	O
σf	O
)	O
.	O
the	O
marginal	B
likelihood	I
has	O
an	O
optimum	O
at	O
log	O
(	O
‘	O
)	O
=	O
2.6	O
at	O
the	O
maximum	O
value	O
of	O
log	O
(	O
σf	O
)	O
,	O
but	O
the	O
log	O
marginal	O
likelihood	B
is	O
essentially	O
ﬂat	O
as	O
a	O
function	B
of	O
log	O
(	O
σf	O
)	O
in	O
this	O
region	O
,	O
so	O
a	O
good	O
point	O
is	O
at	O
log	O
(	O
σf	O
)	O
=	O
4.1	O
,	O
where	O
the	O
log	O
marginal	O
likelihood	B
has	O
a	O
value	O
of	O
−90	O
.	O
panel	O
(	O
b	O
)	O
shows	O
a	O
contour	O
plot	O
of	O
the	O
amount	O
of	O
information	O
(	O
in	O
excess	O
of	O
the	O
baseline	O
model	B
)	O
about	O
the	O
test	O
cases	O
in	O
bits	B
as	O
a	O
function	B
of	O
the	O
same	O
variables	O
.	O
zero	O
bits	B
corresponds	O
to	O
no	O
information	O
and	O
one	O
bit	O
to	O
perfect	O
binary	B
generalization	O
.	O
the	O
773	O
test	O
cases	O
allows	O
the	O
information	O
to	O
be	O
determined	O
within	O
±0.035	O
bits	B
.	O
panel	O
(	O
c	O
)	O
shows	O
a	O
histogram	O
of	O
the	O
latent	O
means	O
for	O
the	O
training	O
and	O
test	O
sets	O
respectively	O
at	O
the	O
values	O
of	O
the	O
hyperparameters	B
with	O
optimal	B
marginal	O
likelihood	B
(	O
from	O
panel	O
a	O
)	O
.	O
panel	O
(	O
d	O
)	O
shows	O
the	O
number	O
of	O
test	O
errors	O
(	O
out	O
of	O
773	O
)	O
when	O
predicting	O
using	O
the	O
sign	O
of	O
the	O
latent	O
mean	O
.	O
in	O
figure	O
3.7	O
(	O
a	O
)	O
we	O
show	O
a	O
contour	O
plot	O
of	O
the	O
approximate	O
log	O
marginal	O
likelihood	B
(	O
lml	O
)	O
log	O
q	O
(	O
y|x	O
,	O
θ	O
)	O
as	O
a	O
function	B
of	O
log	O
(	O
‘	O
)	O
and	O
log	O
(	O
σf	O
)	O
,	O
obtained	O
from	O
runs	O
on	O
a	O
grid	O
of	O
17	O
evenly-spaced	O
values	O
of	O
log	O
(	O
‘	O
)	O
and	O
23	O
evenly-spaced	O
values	O
of	O
log	O
(	O
σf	O
)	O
.	O
notice	O
that	O
there	O
is	O
a	O
maximum	O
of	O
the	O
marginal	B
likelihood	I
laplace	O
results	O
2345012345log	O
lengthscale	O
,	O
log	O
(	O
l	O
)	O
log	O
magnitude	O
,	O
log	O
(	O
σf	O
)	O
log	O
marginal	O
likelihood−92−95−100−105−105−115−130−160−160−200−2002345012345log	O
lengthscale	O
,	O
log	O
(	O
l	O
)	O
log	O
magnitude	O
,	O
log	O
(	O
σf	O
)	O
information	O
about	O
test	O
targets	O
in	O
bits0.250.50.70.70.80.80.840.840.860.880.89−100−5005001020latent	O
means	O
,	O
ffrequencytraining	O
set	B
latent	O
means−100−5005001020latent	O
means	O
,	O
ffrequencytest	O
set	B
latent	O
means2345012345log	O
lengthscale	O
,	O
log	O
(	O
l	O
)	O
log	O
magnitude	O
,	O
log	O
(	O
σf	O
)	O
21201919191919191919191919191919191919191919192220191819191919191919191919191919191919191919242321191817171717171717171717171717171717171729262322201818181818181818181818181818181818183330252523211917171617181818181818181818181818343530292625242120191919181818181818181818181835343431292927262423232120202020212121212121213635343432313131302523232324242424242525252525393636343432323230302724232625252626262626262641393736353632323230292924242626272727272727274240393736353632323131292926232627272727272727454240393836363632333131292826242526282828282751454240393836363632333131292826252828282827286051454240393836363632333131292827272829282727896051454240393836363632333131292827272929282788605145424039383636363233313129282727292828876051454240393836363632333131292828272928number	O
of	O
test	O
misclassifications	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
66	O
classiﬁcation	B
test	O
log	O
predictive	O
probability	B
base-line	O
method	O
interpretation	O
of	O
information	O
score	O
error	B
rate	O
near	O
log	O
(	O
‘	O
)	O
=	O
2.85	O
and	O
log	O
(	O
σf	O
)	O
=	O
2.35.	O
as	O
will	O
be	O
explained	O
in	O
chapter	O
5	O
,	O
we	O
would	O
expect	O
that	O
hyperparameters	B
that	O
yield	O
a	O
high	O
marginal	B
likelihood	I
would	O
give	O
rise	O
to	O
good	O
predictions	O
.	O
notice	O
that	O
an	O
increase	O
of	O
1	O
unit	O
on	O
the	O
log	O
scale	O
means	O
that	O
the	O
probability	B
is	O
2.7	O
times	O
larger	O
,	O
so	O
the	O
marginal	B
likelihood	I
in	O
figure	O
3.7	O
(	O
a	O
)	O
is	O
fairly	O
well	O
peaked	O
.	O
there	O
are	O
at	O
least	O
two	O
ways	O
we	O
can	O
measure	B
the	O
quality	O
of	O
predictions	O
at	O
the	O
test	O
points	O
.	O
the	O
ﬁrst	O
is	O
the	O
test	O
log	O
predictive	O
probability	B
log2	O
p	O
(	O
y∗|x∗	O
,	O
d	O
,	O
θ	O
)	O
.	O
in	O
figure	O
3.7	O
(	O
b	O
)	O
we	O
plot	O
the	O
average	O
over	O
the	O
test	O
set	B
of	O
the	O
test	O
log	O
predictive	O
probability	B
for	O
the	O
same	O
range	O
of	O
hyperparameters	B
.	O
we	O
express	O
this	O
as	O
the	O
amount	O
of	O
information	O
in	O
bits	B
about	O
the	O
targets	O
,	O
by	O
using	O
log	O
to	O
the	O
base	O
2.	O
further	O
,	O
we	O
oﬀ-set	O
the	O
value	O
by	O
subtracting	O
the	O
amount	O
of	O
information	O
that	O
a	O
simple	O
base-line	O
method	O
would	O
achieve	O
.	O
as	O
a	O
base-line	O
model	B
we	O
use	O
the	O
best	O
possible	O
model	B
which	O
does	O
not	O
use	O
the	O
inputs	O
;	O
in	O
this	O
case	O
,	O
this	O
model	B
would	O
just	O
produce	O
a	O
predictive	B
distribution	O
reﬂecting	O
the	O
frequency	O
of	O
the	O
two	O
classes	O
in	O
the	O
training	O
set	B
,	O
i.e	O
.	O
−	O
418/773	O
log2	O
(	O
406/767	O
)	O
−	O
355/773	O
log2	O
(	O
361/767	O
)	O
=	O
0.9956	O
bits	B
,	O
(	O
3.76	O
)	O
essentially	O
1	O
bit	O
.	O
(	O
if	O
the	O
classes	O
had	O
been	O
perfectly	O
balanced	O
,	O
and	O
the	O
training	O
and	O
test	O
partitions	O
also	O
exactly	O
balanced	O
,	O
we	O
would	O
arrive	O
at	O
exactly	O
1	O
bit	O
.	O
)	O
thus	O
,	O
our	O
scaled	O
information	O
score	O
used	O
in	O
figure	O
3.7	O
(	O
b	O
)	O
would	O
be	O
zero	O
for	O
a	O
method	O
that	O
did	O
random	O
guessing	O
and	O
1	O
bit	O
for	O
a	O
method	O
which	O
did	O
perfect	O
classiﬁcation	B
(	O
with	O
complete	O
conﬁdence	O
)	O
.	O
the	O
information	O
score	O
measures	O
how	O
much	O
information	O
the	O
model	B
was	O
able	O
to	O
extract	O
from	O
the	O
inputs	O
about	O
the	O
identity	O
of	O
the	O
output	O
.	O
note	O
that	O
this	O
is	O
not	O
the	O
mutual	O
information	O
between	O
the	O
model	B
output	O
and	O
the	O
test	O
targets	O
,	O
but	O
rather	O
the	O
kullback-leibler	O
(	O
kl	O
)	O
divergence	O
between	O
them	O
.	O
figure	O
3.7	O
shows	O
that	O
there	O
is	O
a	O
good	O
qualitative	O
agreement	O
between	O
the	O
marginal	B
likelihood	I
and	O
the	O
test	O
information	O
,	O
compare	O
panels	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
.	O
the	O
second	O
(	O
and	O
perhaps	O
most	O
commonly	O
used	O
)	O
method	O
for	O
measuring	O
the	O
quality	O
of	O
the	O
predictions	O
is	O
to	O
compute	O
the	O
number	O
of	O
test	O
errors	O
made	O
when	O
using	O
the	O
predictions	O
.	O
this	O
is	O
done	O
by	O
computing	O
eq	O
[	O
π∗|y	O
]	O
(	O
see	B
eq	O
.	O
(	O
3.25	O
)	O
)	O
for	O
each	O
test	O
point	O
,	O
thresholding	O
at	O
1/2	O
to	O
get	O
“	O
hard	O
”	O
predictions	O
and	O
counting	O
the	O
number	O
of	O
errors	O
.	O
figure	O
3.7	O
(	O
d	O
)	O
shows	O
the	O
number	O
of	O
errors	O
produced	O
for	O
each	O
entry	O
in	O
the	O
17	O
×	O
23	O
grid	O
of	O
values	O
for	O
the	O
hyperparameters	B
.	O
the	O
general	O
trend	O
in	O
this	O
table	O
is	O
that	O
the	O
number	O
of	O
errors	O
is	O
lowest	O
in	O
the	O
top	O
left-hand	O
corner	O
and	O
increases	O
as	O
one	O
moves	O
right	O
and	O
downwards	O
.	O
the	O
number	O
of	O
errors	O
rises	O
dramatically	O
in	O
the	O
far	O
bottom	O
righthand	O
corner	O
.	O
however	O
,	O
note	O
in	O
general	O
that	O
the	O
number	O
of	O
errors	O
is	O
quite	O
small	O
(	O
there	O
are	O
773	O
cases	O
in	O
the	O
test	O
set	B
)	O
.	O
the	O
qualitative	O
diﬀerences	O
between	O
the	O
two	O
evaluation	O
criteria	O
depicted	O
in	O
figure	O
3.7	O
panels	O
(	O
b	O
)	O
and	O
(	O
d	O
)	O
may	O
at	O
ﬁrst	O
sight	O
seem	O
alarming	O
.	O
and	O
although	O
panels	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
show	O
similar	O
trends	O
,	O
one	O
may	O
worry	O
about	O
using	O
(	O
a	O
)	O
to	O
select	O
the	O
hyperparameters	B
,	O
if	O
one	O
is	O
interested	O
in	O
minimizing	O
the	O
test	O
misclassiﬁcation	O
rate	O
.	O
indeed	O
a	O
full	O
understanding	O
of	O
all	O
aspects	O
of	O
these	O
plots	O
is	O
quite	O
involved	O
,	O
but	O
as	O
the	O
following	O
discussion	O
suggests	O
,	O
we	O
can	O
explain	O
the	O
major	O
trends	O
.	O
first	O
,	O
bear	O
in	O
mind	O
that	O
the	O
eﬀect	O
of	O
increasing	O
‘	O
is	O
to	O
make	O
the	O
kernel	B
function	O
broader	O
,	O
so	O
we	O
might	O
expect	O
to	O
observe	O
eﬀects	O
like	O
those	O
in	O
figure	O
3.5	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.7	O
experiments	O
67	O
where	O
large	O
widths	O
give	O
rise	O
to	O
a	O
lack	O
of	O
ﬂexibility	O
.	O
keeping	O
‘	O
constant	O
,	O
the	O
eﬀect	O
of	O
increasing	O
σf	O
is	O
to	O
increase	O
the	O
magnitude	O
of	O
the	O
values	O
obtained	O
for	O
ˆf	O
.	O
by	O
itself	O
this	O
would	O
lead	O
to	O
“	O
harder	O
”	O
predictions	O
(	O
i.e	O
.	O
predictive	B
probabilities	O
closer	O
to	O
0	O
or	O
1	O
)	O
,	O
but	O
we	O
have	O
to	O
bear	O
in	O
mind	O
that	O
the	O
variances	O
associated	O
will	O
also	O
increase	O
and	O
this	O
increased	O
uncertainty	O
for	O
the	O
latent	O
variables	O
tends	O
to	O
“	O
soften	O
”	O
the	O
predictive	B
probabilities	O
,	O
i.e	O
.	O
move	O
them	O
closer	O
to	O
1/2	O
.	O
the	O
most	O
marked	O
diﬀerence	O
between	O
figure	O
3.7	O
(	O
b	O
)	O
and	O
(	O
d	O
)	O
is	O
the	O
behaviour	O
in	O
the	O
the	O
top	O
left	O
corner	O
,	O
where	O
classiﬁcation	B
error	O
rate	O
remains	O
small	O
,	O
but	O
the	O
test	O
information	O
and	O
marginal	B
likelihood	I
are	O
both	O
poor	O
.	O
in	O
the	O
left	O
hand	O
side	O
of	O
the	O
plots	O
,	O
the	O
length	O
scale	O
‘	O
is	O
very	O
short	O
.	O
this	O
causes	O
most	O
points	O
to	O
be	O
deemed	O
“	O
far	O
away	O
”	O
from	O
most	O
other	O
points	O
.	O
in	O
this	O
regime	O
the	O
prediction	B
is	O
dominated	O
by	O
the	O
class-label	O
of	O
the	O
nearest	O
neighbours	O
,	O
and	O
for	O
the	O
task	O
at	O
hand	O
,	O
this	O
happens	O
to	O
give	O
a	O
low	O
misclassiﬁcation	O
rate	O
.	O
in	O
this	O
parameter	O
region	O
the	O
test	O
latent	O
variables	O
f∗	O
are	O
very	O
close	O
to	O
zero	O
,	O
corresponding	O
to	O
probabilities	O
very	O
close	O
to	O
1/2	O
.	O
consequently	O
,	O
the	O
predictive	B
probabilities	O
carry	O
almost	O
no	O
information	O
about	O
the	O
targets	O
.	O
in	O
the	O
top	O
left	O
corner	O
,	O
the	O
predictive	B
probabilities	O
for	O
all	O
773	O
test	O
cases	O
lie	O
in	O
the	O
interval	O
[	O
0.48	O
,	O
0.53	O
]	O
.	O
notice	O
that	O
a	O
large	O
amount	O
of	O
information	O
implies	O
a	O
high	O
degree	O
of	O
correct	O
classiﬁcation	B
,	O
but	O
not	O
vice	O
versa	O
.	O
at	O
the	O
optimal	B
marginal	O
likelihood	B
values	O
of	O
the	O
hyperparameters	B
,	O
there	O
are	O
21	O
misclassiﬁcations	O
,	O
which	O
is	O
slightly	O
higher	O
that	O
the	O
minimum	O
number	O
attained	O
which	O
is	O
15	O
errors	O
.	O
in	O
exercise	O
3.10.6	O
readers	O
are	O
encouraged	O
to	O
investigate	O
further	O
the	O
behaviour	O
of	O
ˆf	O
and	O
the	O
predictive	B
probabilities	O
etc	O
.	O
as	O
functions	O
of	O
log	O
(	O
‘	O
)	O
and	O
log	O
(	O
σf	O
)	O
for	O
themselves	O
.	O
in	O
figure	O
3.8	O
we	O
show	O
the	O
results	O
on	O
the	O
same	O
experiment	O
,	O
using	O
the	O
ep	O
method	O
.	O
the	O
ﬁndings	O
are	O
qualitatively	O
similar	O
,	O
but	O
there	O
are	O
signiﬁcant	O
dif-	O
ferences	O
.	O
in	O
panel	O
(	O
a	O
)	O
the	O
approximate	O
log	O
marginal	O
likelihood	B
has	O
a	O
diﬀerent	O
shape	O
than	O
for	O
laplace	O
’	O
s	O
method	O
,	O
and	O
the	O
maximum	O
of	O
the	O
log	O
marginal	O
likeli-	O
hood	O
is	O
about	O
9	O
units	O
on	O
a	O
natural	O
log	O
scale	O
larger	O
(	O
i.e	O
.	O
the	O
marginal	B
probability	O
is	O
exp	O
(	O
9	O
)	O
’	O
8000	O
times	O
higher	O
)	O
.	O
also	O
note	O
that	O
the	O
marginal	B
likelihood	I
has	O
a	O
ridge	B
(	O
for	O
log	O
‘	O
=	O
2.6	O
)	O
that	O
extends	O
into	O
large	O
values	O
of	O
log	O
σf	O
.	O
for	O
these	O
very	O
large	O
latent	O
amplitudes	O
(	O
see	B
also	O
panel	O
(	O
c	O
)	O
)	O
the	O
probit	B
likelihood	O
function	B
is	O
well	O
approximated	O
by	O
a	O
step	O
function	B
(	O
since	O
it	O
transitions	O
from	O
low	O
to	O
high	O
values	O
in	O
the	O
domain	O
[	O
−3	O
,	O
3	O
]	O
)	O
.	O
once	O
we	O
are	O
in	O
this	O
regime	O
,	O
it	O
is	O
of	O
course	O
irrelevant	O
exactly	O
how	O
large	O
the	O
magnitude	O
is	O
,	O
thus	O
the	O
ridge	B
.	O
notice	O
,	O
however	O
,	O
that	O
this	O
does	O
not	O
imply	O
that	O
the	O
prediction	B
will	O
always	O
be	O
“	O
hard	O
”	O
,	O
since	O
the	O
variance	O
of	O
the	O
latent	O
function	O
also	O
grows	O
.	O
figure	O
3.8	O
shows	O
a	O
good	O
qualitative	O
agreement	O
between	O
the	O
approximate	O
log	O
marginal	O
likelihood	B
and	O
the	O
test	O
information	O
,	O
compare	O
panels	O
(	O
a	O
)	O
and	O
(	O
b	O
)	O
.	O
the	O
best	O
value	O
of	O
the	O
test	O
information	O
is	O
signiﬁcantly	O
higher	O
for	O
ep	O
than	O
for	O
laplace	O
’	O
s	O
method	O
.	O
the	O
classiﬁcation	B
error	O
rates	O
in	O
panel	O
(	O
d	O
)	O
show	O
a	O
fairly	O
similar	O
behaviour	O
to	O
that	O
of	O
laplace	O
’	O
s	O
method	O
.	O
in	O
figure	O
3.8	O
(	O
c	O
)	O
we	O
show	O
the	O
latent	O
means	O
for	O
training	O
and	O
test	O
cases	O
.	O
these	O
show	O
a	O
clear	O
separation	O
on	O
the	O
training	O
set	B
,	O
and	O
much	O
larger	O
magnitudes	O
than	O
for	O
laplace	O
’	O
s	O
method	O
.	O
the	O
absolute	O
values	O
of	O
the	O
entries	O
in	O
f∗	O
are	O
quite	O
large	O
,	O
often	O
well	O
in	O
excess	O
of	O
50	O
,	O
which	O
may	O
suggest	O
very	O
“	O
hard	O
”	O
predictions	O
(	O
probabilities	O
close	O
to	O
zero	O
or	O
one	O
)	O
,	O
ep	O
results	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
68	O
classiﬁcation	B
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
3.9	O
:	O
map	O
vs.	O
averaged	B
predictions	O
for	O
the	O
ep	O
algorithm	O
for	O
the	O
3	O
’	O
s	O
vs.	O
5	O
’	O
s	O
digit	O
discrimination	O
using	O
the	O
usps	O
data	O
.	O
the	O
optimal	B
values	O
of	O
the	O
hyperparameters	B
from	O
figure	O
3.8	O
(	O
a	O
)	O
log	O
(	O
‘	O
)	O
=	O
2.6	O
and	O
log	O
(	O
σf	O
)	O
=	O
4.1	O
are	O
used	O
.	O
the	O
map	O
predictions	O
σ	O
(	O
eq	O
[	O
f∗|y	O
]	O
)	O
are	O
“	O
hard	O
”	O
,	O
mostly	O
being	O
very	O
close	O
to	O
zero	O
or	O
one	O
.	O
on	O
the	O
other	O
hand	O
,	O
the	O
averaged	B
predictions	O
eq	O
[	O
π∗|y	O
]	O
from	O
eq	O
.	O
(	O
3.25	O
)	O
are	O
a	O
lot	O
less	O
extreme	O
.	O
in	O
panel	O
(	O
a	O
)	O
the	O
21	O
cases	O
that	O
were	O
misclassiﬁed	O
are	O
indicated	O
by	O
crosses	O
(	O
correctly	O
classiﬁed	O
cases	O
are	O
shown	O
by	O
points	O
)	O
.	O
note	O
that	O
only	O
4	O
of	O
the	O
21	O
misclassiﬁed	O
points	O
have	O
conﬁdent	O
predictions	O
(	O
i.e	O
.	O
outside	O
[	O
0.1	O
,	O
0.9	O
]	O
)	O
.	O
notice	O
that	O
all	O
points	O
fall	O
in	O
the	O
triangles	O
below	O
and	O
above	O
the	O
horizontal	O
line	O
,	O
conﬁrming	O
that	O
averaging	O
does	O
not	O
change	O
the	O
“	O
most	O
probable	O
”	O
class	O
,	O
and	O
that	O
it	O
always	O
makes	O
the	O
probabilities	O
less	O
extreme	O
(	O
i.e	O
.	O
closer	O
to	O
1/2	O
)	O
.	O
panel	O
(	O
b	O
)	O
shows	O
histograms	O
of	O
averaged	B
and	O
map	O
predictions	O
,	O
where	O
we	O
have	O
truncated	O
values	O
over	O
30.	O
since	O
the	O
sigmoid	O
saturates	O
for	O
smaller	O
arguments	O
.	O
however	O
,	O
when	O
taking	O
the	O
uncertainties	O
in	O
the	O
latent	O
variables	O
into	O
account	O
,	O
and	O
computing	O
the	O
predictions	O
using	O
averaging	O
as	O
in	O
eq	O
.	O
(	O
3.25	O
)	O
the	O
predictive	B
probabilities	O
are	O
“	O
softened	O
”	O
.	O
in	O
figure	O
3.9	O
we	O
can	O
verify	O
that	O
the	O
averaged	B
predictive	O
probabilities	O
are	O
much	O
less	O
extreme	O
than	O
the	O
map	O
predictions	O
.	O
in	O
order	O
to	O
evaluate	O
the	O
performance	O
of	O
the	O
two	O
approximate	O
methods	O
for	O
gp	O
classiﬁcation	B
,	O
we	O
compared	O
to	O
a	O
linear	B
probit	O
model	B
,	O
a	O
support	B
vector	I
ma-	O
chine	O
,	O
a	O
least-squares	B
classiﬁer	O
and	O
a	O
nearest	O
neighbour	O
approach	O
,	O
all	O
of	O
which	O
are	O
commonly	O
used	O
in	O
the	O
machine	O
learning	B
community	O
.	O
in	O
figure	O
3.10	O
we	O
show	O
error-reject	O
curves	O
for	O
both	O
misclassiﬁcation	O
rate	O
and	O
the	O
test	O
information	O
mea-	O
sure	O
.	O
the	O
error-reject	B
curve	I
shows	O
how	O
the	O
performance	O
develops	O
as	O
a	O
function	B
of	O
the	O
fraction	O
of	O
test	O
cases	O
that	O
is	O
being	O
rejected	O
.	O
to	O
compute	O
these	O
,	O
we	O
ﬁrst	O
modify	O
the	O
methods	O
that	O
do	O
not	O
naturally	O
produce	O
probabilistic	B
predictions	O
to	O
do	O
so	O
,	O
as	O
described	O
below	O
.	O
based	O
on	O
the	O
predictive	B
probabilities	O
,	O
we	O
reject	O
test	O
cases	O
for	O
which	O
the	O
maximum	O
predictive	O
probability	B
is	O
smaller	O
than	O
a	O
threshold	O
.	O
varying	O
the	O
threshold	O
produces	O
the	O
error-reject	B
curve	I
.	O
the	O
gp	O
classiﬁers	O
applied	O
in	O
figure	O
3.10	O
used	O
the	O
hyperparameters	B
which	O
optimized	O
the	O
approximate	O
marginal	B
likelihood	I
for	O
each	O
of	O
the	O
two	O
methods	O
.	O
for	O
the	O
gp	O
classiﬁers	O
there	O
were	O
two	O
free	O
parameters	O
σf	O
and	O
‘	O
.	O
the	O
linear	B
pro-	O
bit	O
model	B
(	O
linear	B
logistic	O
models	O
are	O
probably	O
more	O
common	O
,	O
but	O
we	O
chose	O
the	O
probit	B
here	O
,	O
since	O
the	O
other	O
likelihood	B
based	O
methods	O
all	O
used	O
probit	B
)	O
can	O
be	O
error-reject	B
curve	I
linear	O
probit	B
model	O
00.20.40.60.8100.20.40.60.81π*	O
mapπ*	O
averaged00.20.40.60.810510152025π*	O
mapfrequency00.20.40.60.810510152025π*	O
averagedfrequency	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.7	O
experiments	O
69	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
3.10	O
:	O
panel	O
(	O
a	O
)	O
shows	O
the	O
error-reject	B
curve	I
and	O
panel	O
(	O
b	O
)	O
the	O
amount	O
of	O
information	O
about	O
the	O
test	O
cases	O
as	O
a	O
function	B
of	O
the	O
rejection	O
rate	O
.	O
the	O
probabilistic	B
one	I
nearest	I
neighbour	I
(	O
p1nn	O
)	O
method	O
has	O
much	O
worse	O
performance	O
than	O
the	O
other	O
methods	O
.	O
gaussian	O
processes	O
with	O
ep	O
behaves	O
similarly	O
to	O
svm	O
’	O
s	O
although	O
the	O
clas-	O
siﬁcation	O
rate	O
for	O
svm	O
for	O
low	O
rejection	O
rates	O
seems	O
to	O
be	O
a	O
little	O
better	O
.	O
laplace	O
’	O
s	O
method	O
is	O
worse	O
than	O
ep	O
and	O
svm	O
.	O
the	O
gp	O
least	O
squares	O
classiﬁer	B
(	O
lsc	O
)	O
described	O
in	O
section	O
6.5	O
performs	O
the	O
best	O
.	O
implemented	O
as	O
gp	O
model	B
using	O
laplace	O
’	O
s	O
method	O
,	O
which	O
is	O
equivalent	B
to	O
(	O
al-	O
though	O
not	O
computationally	O
as	O
eﬃcient	O
as	O
)	O
iteratively	B
reweighted	I
least	I
squares	I
(	O
irls	O
)	O
.	O
the	O
covariance	B
function	I
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
θ2x	O
>	O
x0	O
has	O
a	O
single	O
hyperparam-	O
eter	O
,	O
θ	O
,	O
which	O
was	O
set	B
by	O
maximizing	O
the	O
log	O
marginal	O
likelihood	B
.	O
this	O
gives	O
log	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
−105	O
,	O
at	O
θ	O
=	O
2.0	O
,	O
thus	O
the	O
marginal	B
likelihood	I
for	O
the	O
linear	B
covariance	O
function	B
is	O
about	O
6	O
units	O
on	O
a	O
natural	O
log	O
scale	O
lower	O
than	O
the	O
max-	O
imum	O
log	O
marginal	O
likelihood	B
for	O
the	O
laplace	O
approximation	O
using	O
the	O
squared	B
exponential	I
covariance	O
function	B
.	O
the	O
support	B
vector	I
machine	I
(	O
svm	O
)	O
classiﬁer	B
(	O
see	B
section	O
6.4	O
for	O
further	O
de-	O
tails	O
on	O
the	O
svm	O
)	O
used	O
the	O
same	O
se	O
kernel	B
as	O
the	O
gp	O
classiﬁers	O
.	O
for	O
the	O
svm	O
the	O
rˆole	O
of	O
‘	O
is	O
identical	O
,	O
and	O
the	O
trade-oﬀ	O
parameter	O
c	O
in	O
the	O
svm	O
formulation	O
f	O
.	O
we	O
carried	O
out	O
5-fold	O
cross	O
validation	O
(	O
see	B
eq	O
.	O
(	O
6.37	O
)	O
)	O
plays	O
a	O
similar	O
rˆole	O
to	O
σ2	O
on	O
a	O
grid	O
in	O
parameter	O
space	O
to	O
identify	O
the	O
best	O
combination	O
of	O
parameters	O
w.r.t	O
.	O
the	O
error	B
rate	O
;	O
this	O
turned	O
out	O
to	O
be	O
at	O
c	O
=	O
1	O
,	O
‘	O
=	O
10.	O
our	O
experiments	O
were	O
conducted	O
using	O
the	O
svmtorch	O
software	O
[	O
collobert	O
and	O
bengio	O
,	O
2001	O
]	O
.	O
in	O
order	O
to	O
compute	O
probabilistic	B
predictions	O
,	O
we	O
squashed	O
the	O
test-activities	O
through	O
a	O
cumulative	O
gaussian	O
,	O
using	O
the	O
methods	O
proposed	O
by	O
platt	O
[	O
2000	O
]	O
:	O
we	O
made	O
a	O
parameterized	O
linear	B
transformation	O
of	O
the	O
test-activities	O
and	O
fed	O
this	O
through	O
the	O
cumulative	O
gaussian.18	O
the	O
parameters	O
of	O
the	O
linear	B
trans-	O
formation	O
were	O
chosen	O
to	O
maximize	O
the	O
log	O
predictive	O
probability	B
,	O
evaluated	O
on	O
the	O
hold-out	O
sets	O
of	O
the	O
5-fold	O
cross	O
validation	O
.	O
the	O
probabilistic	B
one	I
nearest	I
neighbour	I
(	O
p1nn	O
)	O
method	O
is	O
a	O
simple	O
nat-	O
ural	O
extension	O
to	O
the	O
classical	O
one	O
nearest	O
neighbour	O
method	O
which	O
provides	O
probabilistic	B
predictions	O
.	O
it	O
computes	O
the	O
leave-one-out	B
(	O
loo	O
)	O
one	O
nearest	O
neighbour	O
prediction	B
on	O
the	O
training	O
set	B
,	O
and	O
records	O
the	O
fraction	O
of	O
cases	O
π	O
where	O
the	O
loo	O
predictions	O
were	O
correct	O
.	O
on	O
test	O
cases	O
,	O
the	O
method	O
then	O
pre-	O
18platt	O
[	O
2000	O
]	O
used	O
a	O
logistic	B
whereas	O
we	O
use	O
a	O
cumulative	O
gaussian	O
.	O
support	B
vector	I
machine	I
probabilistic	O
one	O
nearest	O
neighbour	O
00.10.20.300.010.020.03rejection	O
ratemisclassification	O
rateeplaplacesvmp1nnlsclin	O
probit00.20.40.60.810.850.90.951rejection	O
ratetest	O
information	O
,	O
bitseplaplacesvmp1nnlsclin	O
probit	B
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
70	O
classiﬁcation	B
dicts	O
the	O
one	O
nearest	O
neighbour	O
class	O
with	O
probability	B
π	O
,	O
and	O
the	O
other	O
class	O
with	O
probability	B
1−	O
π.	O
rejections	O
are	O
based	O
on	O
thresholding	O
on	O
the	O
distance	O
to	O
the	O
nearest	O
neighbour	O
.	O
the	O
least-squares	B
classiﬁer	O
(	O
lsc	O
)	O
is	O
described	O
in	O
section	O
6.5.	O
in	O
order	O
to	O
produce	O
probabilistic	B
predictions	O
,	O
the	O
method	O
of	O
platt	O
[	O
2000	O
]	O
was	O
used	O
(	O
as	O
de-	O
scribed	O
above	O
for	O
the	O
svm	O
)	O
using	O
the	O
predictive	B
means	O
only	O
(	O
the	O
predictive	B
variances	O
were	O
ignored19	O
)	O
,	O
except	O
that	O
instead	O
of	O
the	O
5-fold	O
cross	O
validation	O
,	O
leave-one-out	B
cross-validation	I
(	O
loo-cv	O
)	O
was	O
used	O
,	O
and	O
the	O
kernel	B
parameters	O
were	O
also	O
set	B
using	O
loo-cv	O
.	O
figure	O
3.10	O
shows	O
that	O
the	O
three	O
best	O
methods	O
are	O
the	O
ep	O
approximation	O
for	O
gpc	O
,	O
the	O
svm	O
and	O
the	O
least-squares	B
classiﬁer	O
(	O
lsc	O
)	O
.	O
presenting	O
both	O
the	O
error	B
rates	O
and	O
the	O
test	O
information	O
helps	O
to	O
highlight	O
diﬀerences	O
which	O
may	O
not	O
be	O
apparent	O
from	O
a	O
single	O
plot	O
alone	O
.	O
for	O
example	O
,	O
laplace	O
’	O
s	O
method	O
and	O
ep	O
seem	O
very	O
similar	O
on	O
error	B
rates	O
,	O
but	O
quite	O
diﬀerent	O
in	O
test	O
information	O
.	O
notice	O
also	O
,	O
that	O
the	O
error-reject	B
curve	I
itself	O
reveals	O
interesting	O
diﬀerences	O
,	O
e.g	O
.	O
notice	O
that	O
although	O
the	O
p1nn	O
method	O
has	O
an	O
error	B
rate	O
comparable	O
to	O
other	O
methods	O
at	O
zero	O
rejections	O
,	O
things	O
don	O
’	O
t	O
improve	O
very	O
much	O
when	O
rejections	O
are	O
allowed	O
.	O
refer	O
to	O
section	O
3.8	O
for	O
more	O
discussion	O
of	O
the	O
results	O
.	O
3.7.4	O
10-class	O
handwritten	O
digit	O
classiﬁcation	B
example	O
we	O
apply	O
the	O
multi-class	B
laplace	O
approximation	O
developed	O
in	O
section	O
3.5	O
to	O
the	O
10-class	O
handwritten	O
digit	O
classiﬁcation	B
problem	O
from	O
the	O
(	O
repartitioned	O
)	O
usps	O
dataset	B
,	O
having	O
n	O
=	O
4649	O
training	O
cases	O
and	O
n∗	O
=	O
4649	O
cases	O
for	O
testing	O
,	O
see	B
page	O
63.	O
we	O
used	O
a	O
squared	B
exponential	I
covariance	O
function	B
with	O
two	O
hyper-	O
parameters	O
:	O
a	O
single	O
signal	O
amplitude	O
σf	O
,	O
common	O
to	O
all	O
10	O
latent	O
functions	O
,	O
and	O
a	O
single	O
length-scale	B
parameter	O
‘	O
,	O
common	O
to	O
all	O
10	O
latent	O
functions	O
and	O
common	O
to	O
all	O
256	O
input	O
dimensions	O
.	O
the	O
behaviour	O
of	O
the	O
method	O
was	O
investigated	O
on	O
a	O
grid	O
of	O
values	O
for	O
the	O
hyperparameters	B
,	O
see	B
figure	O
3.11.	O
note	O
that	O
the	O
correspondence	O
between	O
the	O
log	O
marginal	O
likelihood	B
and	O
the	O
test	O
information	O
is	O
not	O
as	O
close	O
as	O
for	O
laplace	O
’	O
s	O
method	O
for	O
binary	B
classiﬁcation	I
in	O
figure	O
3.7	O
on	O
page	O
64.	O
the	O
maximum	O
value	O
of	O
the	O
log	O
marginal	O
likelihood	B
attained	O
is	O
-1018	O
,	O
and	O
for	O
the	O
hyperparameters	B
corresponding	O
to	O
this	O
point	O
the	O
error	B
rate	O
is	O
3.1	O
%	O
and	O
the	O
test	O
information	O
2.67	O
bits	B
.	O
as	O
with	O
the	O
binary	B
classiﬁcation	I
problem	O
,	O
the	O
test	O
information	O
is	O
standardized	O
by	O
subtracting	O
oﬀ	O
the	O
negative	O
entropy	O
(	O
information	O
)	O
of	O
the	O
targets	O
which	O
is	O
−3.27	O
bits	B
.	O
the	O
classiﬁcation	B
error	O
rate	O
in	O
figure	O
3.11	O
(	O
c	O
)	O
shows	O
a	O
clear	O
minimum	O
,	O
and	O
this	O
is	O
also	O
attained	O
at	O
a	O
shorter	O
length-scale	B
than	O
where	O
the	O
marginal	B
likelihood	I
and	O
test	O
information	O
have	O
their	O
maxima	O
.	O
this	O
eﬀect	O
was	O
also	O
seen	O
in	O
the	O
experiments	O
on	O
binary	B
classiﬁcation	I
.	O
to	O
gain	O
some	O
insight	O
into	O
the	O
level	O
of	O
performance	O
we	O
compared	O
these	O
re-	O
sults	O
with	O
those	O
obtained	O
with	O
the	O
probabilistic	B
one	I
nearest	I
neighbour	I
method	O
p1nn	O
,	O
a	O
multiple	O
logistic	O
regression	B
model	O
and	O
a	O
svm	O
.	O
the	O
p1nn	O
ﬁrst	O
uses	O
an	O
19of	O
course	O
,	O
one	O
could	O
also	O
have	O
tried	O
a	O
variant	O
where	O
the	O
full	O
latent	O
predictive	O
distribution	O
was	O
averaged	B
over	O
,	O
but	O
we	O
did	O
not	O
do	O
that	O
here	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.7	O
experiments	O
71	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
3.11	O
:	O
10-way	O
digit	O
classiﬁcation	B
using	O
the	O
laplace	O
approximation	O
.	O
panel	O
(	O
a	O
)	O
shows	O
the	O
approximate	O
log	O
marginal	O
likelihood	B
,	O
reaching	O
a	O
maximum	O
value	O
of	O
log	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
−1018	O
at	O
log	O
‘	O
=	O
2.35	O
and	O
log	O
σf	O
=	O
2.6.	O
in	O
panel	O
(	O
b	O
)	O
information	O
about	O
the	O
test	O
cases	O
is	O
shown	O
.	O
the	O
maximum	O
possible	O
amount	O
of	O
information	O
about	O
the	O
test	O
targets	O
,	O
corresponding	O
to	O
perfect	O
classiﬁcation	B
,	O
would	O
be	O
3.27	O
bits	B
(	O
the	O
entropy	B
of	O
the	O
targets	O
)	O
.	O
at	O
the	O
point	O
of	O
maximum	O
marginal	O
likelihood	B
,	O
the	O
test	O
information	O
is	O
2.67	O
bits	B
.	O
in	O
panel	O
(	O
c	O
)	O
the	O
test	O
set	B
misclassiﬁcation	O
rate	O
is	O
shown	O
in	O
percent	O
.	O
at	O
the	O
point	O
of	O
maximum	O
marginal	O
likelihood	B
the	O
test	O
error	B
rate	O
is	O
3.1	O
%	O
.	O
internal	O
leave-one-out	B
assessment	O
on	O
the	O
training	O
set	B
to	O
estimate	O
its	O
probabil-	O
ity	B
of	O
being	O
correct	O
,	O
π.	O
for	O
the	O
test	O
set	B
it	O
then	O
predicts	O
the	O
nearest	O
neighbour	O
with	O
probability	B
π	O
and	O
all	O
other	O
classes	O
with	O
equal	O
probability	B
(	O
1	O
−	O
π	O
)	O
/9	O
.	O
we	O
obtained	O
π	O
=	O
0.967	O
,	O
a	O
test	O
information	O
of	O
2.98	O
bits	B
and	O
a	O
test	O
set	B
classiﬁcation	O
error	B
rate	O
of	O
3.0	O
%	O
.	O
we	O
also	O
compare	O
to	O
multiple	O
linear	O
logistic	B
regression	I
.	O
one	O
way	O
to	O
imple-	O
ment	O
this	O
method	O
is	O
to	O
view	O
it	O
as	O
a	O
gaussian	O
process	B
with	O
a	O
linear	B
covariance	O
2345012345log	O
lengthscale	O
,	O
log	O
(	O
l	O
)	O
log	O
magnitude	O
,	O
log	O
(	O
σf	O
)	O
log	O
marginal	O
likelihood−1050−1100−1200−1300−1500−2000−3000−3000−30002345012345log	O
lengthscale	O
,	O
log	O
(	O
l	O
)	O
log	O
magnitude	O
,	O
log	O
(	O
σf	O
)	O
information	O
about	O
the	O
test	O
targets	O
in	O
bits11222.52.52.82.82.952.992345012345log	O
lengthscale	O
,	O
log	O
(	O
l	O
)	O
log	O
magnitude	O
,	O
log	O
(	O
σf	O
)	O
test	O
set	B
misclassification	O
percentage2.62.72.833.3451010	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
72	O
classiﬁcation	B
function	O
,	O
although	O
it	O
is	O
equivalent	B
and	O
computationally	O
more	O
eﬃcient	O
to	O
do	O
the	O
laplace	O
approximation	O
over	O
the	O
“	O
weights	O
”	O
of	O
the	O
linear	B
model	O
.	O
in	O
our	O
case	O
there	O
are	O
10×257	O
weights	O
(	O
256	O
inputs	O
and	O
one	O
bias	B
)	O
,	O
whereas	O
there	O
are	O
10×4696	O
latent	O
function	O
values	O
in	O
the	O
gp	O
.	O
the	O
linear	B
covariance	O
function	B
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
θ2x	O
>	O
x0	O
has	O
a	O
single	O
hyperparameter	O
θ	O
(	O
used	O
for	O
all	O
10	O
latent	O
functions	O
)	O
.	O
optimizing	O
the	O
log	O
marginal	O
likelihood	B
w.r.t	O
.	O
θ	O
gives	O
log	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
−1339	O
at	O
θ	O
=	O
1.45.	O
using	O
this	O
value	O
for	O
the	O
hyperparameter	O
,	O
the	O
test	O
information	O
is	O
2.95	O
bits	B
and	O
the	O
test	O
set	B
error	O
rate	O
is	O
5.7	O
%	O
.	O
finally	O
,	O
a	O
support	B
vector	I
machine	I
(	O
svm	O
)	O
classiﬁer	B
was	O
trained	O
using	O
the	O
same	O
se	O
kernel	B
as	O
the	O
gaussian	O
process	B
classiﬁers	O
.	O
(	O
see	B
section	O
6.4	O
for	O
further	O
details	O
on	O
the	O
svm	O
.	O
)	O
as	O
in	O
the	O
binary	B
svm	O
case	O
there	O
were	O
two	O
free	O
parameters	O
‘	O
(	O
the	O
length-scale	B
of	O
the	O
kernel	B
)	O
,	O
and	O
the	O
trade-oﬀ	O
parameter	O
c	O
(	O
see	B
eq	O
.	O
(	O
6.37	O
)	O
)	O
,	O
f	O
.	O
we	O
carried	O
out	O
5-fold	O
cross-validation	B
on	O
a	O
grid	O
which	O
plays	O
a	O
similar	O
rˆole	O
to	O
σ2	O
in	O
parameter	O
space	O
to	O
identify	O
the	O
best	O
combination	O
of	O
parameters	O
w.r.t	O
.	O
the	O
error	B
rate	O
;	O
this	O
turned	O
out	O
to	O
be	O
at	O
c	O
=	O
1	O
,	O
‘	O
=	O
5.	O
our	O
experiments	O
were	O
conducted	O
using	O
the	O
svmtorch	O
software	O
[	O
collobert	O
and	O
bengio	O
,	O
2001	O
]	O
,	O
which	O
implements	O
multi-class	B
svm	O
classiﬁcation	B
using	O
the	O
one-versus-rest	B
method	O
de-	O
scribed	O
in	O
section	O
6.5.	O
the	O
test	O
set	B
error	O
rate	O
for	O
the	O
svm	O
is	O
2.2	O
%	O
;	O
we	O
did	O
not	O
attempt	O
to	O
evaluate	O
the	O
test	O
information	O
for	O
the	O
multi-class	B
svm	O
.	O
3.8	O
discussion	O
in	O
the	O
previous	O
section	O
we	O
presented	O
several	O
sets	O
of	O
experiments	O
comparing	O
the	O
two	O
approximate	O
methods	O
for	O
inference	O
in	O
gpc	O
models	O
,	O
and	O
comparing	O
them	O
to	O
other	O
commonly-used	O
supervised	B
learning	I
methods	O
.	O
in	O
this	O
section	O
we	O
discuss	O
the	O
results	O
and	O
attempt	O
to	O
relate	O
them	O
to	O
the	O
properties	O
of	O
the	O
models	O
.	O
for	O
the	O
binary	B
examples	O
from	O
figures	O
3.7	O
and	O
3.8	O
,	O
we	O
saw	O
that	O
the	O
two	O
ap-	O
proximations	O
showed	O
quite	O
diﬀerent	O
qualitative	O
behaviour	O
of	O
the	O
approximated	O
log	O
marginal	O
likelihood	B
,	O
although	O
the	O
exact	O
marginal	B
likelihood	I
is	O
of	O
course	O
iden-	O
tical	O
.	O
the	O
ep	O
approximation	O
gave	O
a	O
higher	O
maximum	O
value	O
of	O
the	O
log	O
marginal	O
likelihood	B
(	O
by	O
about	O
9	O
units	O
on	O
the	O
log	O
scale	O
)	O
and	O
the	O
test	O
information	O
was	O
somewhat	O
better	O
than	O
for	O
laplace	O
’	O
s	O
method	O
,	O
although	O
the	O
test	O
set	B
error	O
rates	O
were	O
comparable	O
.	O
however	O
,	O
although	O
this	O
experiment	O
seems	O
to	O
favour	O
the	O
ep	O
approximation	O
,	O
it	O
is	O
interesting	O
to	O
know	O
how	O
close	O
these	O
approximations	O
are	O
to	O
the	O
exact	O
(	O
analytically	O
intractable	O
)	O
solutions	O
.	O
in	O
figure	O
3.12	O
we	O
show	O
the	O
results	O
of	O
running	O
a	O
sophisticated	O
markov	O
chain	O
monte	O
carlo	O
method	O
called	O
annealed	O
importance	O
sampling	O
[	O
neal	O
,	O
2001	O
]	O
carried	O
out	O
by	O
kuss	O
and	O
rasmussen	O
[	O
2005	O
]	O
.	O
the	O
usps	O
dataset	B
for	O
these	O
experiments	O
was	O
identical	O
to	O
the	O
one	O
used	O
in	O
fig-	O
ures	O
3.7	O
and	O
3.8	O
,	O
so	O
the	O
results	O
are	O
directly	O
comparable	O
.	O
it	O
is	O
seen	O
that	O
the	O
mcmc	O
results	O
indicate	O
that	O
the	O
ep	O
method	O
achieves	O
a	O
very	O
high	O
level	O
of	O
accu-	O
racy	O
,	O
i.e	O
.	O
that	O
the	O
diﬀerence	O
between	O
ep	O
and	O
laplace	O
’	O
s	O
method	O
is	O
caused	O
almost	O
exclusively	O
by	O
approximation	O
errors	O
in	O
laplace	O
’	O
s	O
method	O
.	O
the	O
main	O
reason	O
for	O
the	O
inaccuracy	O
of	O
laplace	O
’	O
s	O
method	O
is	O
that	O
the	O
high	O
dimensional	O
posterior	O
is	O
skew	O
,	O
and	O
that	O
the	O
symmetric	O
approximation	O
centered	O
on	O
the	O
mode	O
is	O
not	O
characterizing	O
the	O
posterior	O
volume	O
very	O
well	O
.	O
the	O
posterior	O
monte	O
carlo	O
results	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.8	O
discussion	O
73	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
3.12	O
:	O
the	O
log	O
marginal	O
likelihood	B
,	O
panel	O
(	O
a	O
)	O
,	O
and	O
test	O
information	O
,	O
panel	O
(	O
b	O
)	O
,	O
for	O
the	O
usps	O
3	O
’	O
s	O
vs.	O
5	O
’	O
s	O
binary	B
classiﬁcation	I
task	O
computed	O
using	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
.	O
comparing	O
this	O
to	O
the	O
laplace	O
approximation	O
figure	O
3.7	O
and	O
figure	O
3.8	O
shows	O
that	O
the	O
ep	O
approximation	O
is	O
surprisingly	O
accurate	O
.	O
the	O
slight	O
wiggliness	O
of	O
the	O
contour	O
lines	O
are	O
caused	O
by	O
ﬁnite	O
sample	O
eﬀects	O
in	O
the	O
mcmc	O
runs	O
.	O
is	O
a	O
combination	O
of	O
the	O
(	O
correlated	B
)	O
gaussian	O
prior	O
centered	O
on	O
the	O
origin	O
and	O
the	O
likelihood	B
terms	O
which	O
(	O
softly	O
)	O
cut	O
oﬀ	O
half-spaces	O
which	O
do	O
not	O
agree	O
with	O
the	O
training	O
set	B
labels	O
.	O
therefore	O
the	O
posterior	O
looks	O
like	O
a	O
correlated	B
gaussian	O
restricted	O
to	O
the	O
orthant	O
which	O
agrees	O
with	O
the	O
labels	O
.	O
its	O
mode	O
will	O
be	O
located	O
close	O
to	O
the	O
origin	O
in	O
that	O
orthant	O
,	O
and	O
it	O
will	O
decrease	O
rapidly	O
in	O
the	O
direction	O
towards	O
the	O
origin	O
due	O
to	O
conﬂicts	O
from	O
the	O
likelihood	B
terms	O
,	O
and	O
decrease	O
only	O
slowly	O
in	O
the	O
opposite	O
direction	O
(	O
because	O
of	O
the	O
prior	O
)	O
.	O
seen	O
in	O
this	O
light	O
it	O
is	O
not	O
surprising	O
that	O
the	O
laplace	O
approximation	O
is	O
somewhat	O
inaccurate	O
.	O
this	O
explanation	O
is	O
corroborated	O
further	O
by	O
kuss	O
and	O
rasmussen	O
[	O
2005	O
]	O
.	O
it	O
should	O
be	O
noted	O
that	O
all	O
the	O
methods	O
compared	O
on	O
the	O
binary	B
digits	O
clas-	O
siﬁcation	O
task	O
except	O
for	O
the	O
linear	B
probit	O
model	B
are	O
using	O
the	O
squared	B
distance	O
between	O
the	O
digitized	O
digit	O
images	O
measured	O
directly	O
in	O
the	O
image	O
space	O
as	O
the	O
sole	O
input	O
to	O
the	O
algorithm	O
.	O
this	O
distance	O
measure	B
is	O
not	O
very	O
well	O
suited	O
for	O
the	O
digit	O
discrimination	O
task—for	O
example	O
,	O
two	O
similar	O
images	O
that	O
are	O
slight	O
translations	O
of	O
each	O
other	O
may	O
have	O
a	O
huge	O
squared	B
distance	O
,	O
although	O
of	O
course	O
identical	O
labels	O
.	O
one	O
of	O
the	O
strengths	O
of	O
the	O
gp	O
formalism	O
is	O
that	O
one	O
can	O
use	O
prior	O
distributions	O
over	O
(	O
latent	O
,	O
in	O
this	O
case	O
)	O
functions	O
,	O
and	O
do	O
inference	O
based	O
on	O
these	O
.	O
if	O
however	O
,	O
the	O
prior	O
over	O
functions	O
depends	O
only	O
on	O
one	O
particular	O
as-	O
pect	O
of	O
the	O
data	O
(	O
the	O
squared	B
distance	O
in	O
image	O
space	O
)	O
which	O
is	O
not	O
so	O
well	O
suited	O
for	O
discrimination	O
,	O
then	O
the	O
prior	O
used	O
is	O
also	O
not	O
very	O
appropriate	O
.	O
it	O
would	O
be	O
more	O
interesting	O
to	O
design	O
covariance	B
functions	O
(	O
parameterized	O
by	O
hyperparame-	O
ters	O
)	O
which	O
are	O
more	O
appropriate	O
for	O
the	O
digit	O
discrimination	O
task	O
,	O
e.g	O
.	O
reﬂecting	O
on	O
the	O
known	O
invariances	B
in	O
the	O
images	O
,	O
such	O
as	O
the	O
“	O
tangent-distance	O
”	O
ideas	O
from	O
simard	O
et	O
al	O
.	O
[	O
1992	O
]	O
;	O
see	B
also	O
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
,	O
ch	O
.	O
11	O
]	O
and	O
section	O
9.10.	O
the	O
results	O
shown	O
here	O
follow	O
the	O
common	O
approach	O
of	O
using	O
a	O
generic	O
suitablility	O
of	O
the	O
covariance	B
function	I
2345012345log	O
lengthscale	O
,	O
log	O
(	O
l	O
)	O
log	O
magnitude	O
,	O
log	O
(	O
σf	O
)	O
log	O
marginal	O
likelihood−92−95−100−105−105−115−130−160−160−200−2002345012345log	O
lengthscale	O
,	O
log	O
(	O
l	O
)	O
log	O
magnitude	O
,	O
log	O
(	O
σf	O
)	O
information	O
about	O
test	O
targets	O
in	O
bits0.250.50.70.70.80.80.840.840.860.880.89	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
74	O
classiﬁcation	B
covariance	O
function	B
with	O
a	O
minimum	O
of	O
hyperparameters	B
,	O
but	O
this	O
doesn	O
’	O
t	O
allow	O
us	O
to	O
incorporate	O
much	O
prior	O
information	O
about	O
the	O
problem	O
.	O
for	O
an	O
example	O
in	O
the	O
gp	O
framework	O
for	O
doing	O
inference	O
about	O
multiple	O
hyperparameters	O
with	O
more	O
complex	O
covariance	B
functions	O
which	O
provide	O
clearly	O
interpretable	O
infor-	O
mation	O
about	O
the	O
data	O
,	O
see	B
the	O
carbon	O
dioxide	O
modelling	O
problem	O
discussed	O
on	O
page	O
118	O
.	O
∗	O
3.9	O
appendix	O
:	O
moment	O
derivations	O
consider	O
the	O
integral	O
of	O
a	O
cumulative	O
gaussian	O
,	O
φ	O
,	O
with	O
respect	O
to	O
a	O
gaussian	O
z	O
∞	O
−∞	O
z	O
=	O
zv	O
>	O
0	O
=	O
or	O
in	O
matrix	B
notation	O
zv	O
>	O
0	O
=	O
=	O
initially	O
for	O
the	O
special	O
case	O
v	O
>	O
0	O
.	O
writing	O
out	O
in	O
full	O
,	O
substituting	O
z	O
=	O
y	O
−	O
x	O
+	O
µ	O
−	O
m	O
and	O
w	O
=	O
x	O
−	O
µ	O
and	O
interchanging	O
the	O
order	O
of	O
the	O
integrals	B
z	O
x	O
−∞	O
n	O
(	O
y	O
)	O
dy	O
,	O
(	O
3.77	O
)	O
(	O
cid:1	O
)	O
dy	O
dx	O
−	O
(	O
x	O
−	O
µ	O
)	O
2	O
(	O
cid:1	O
)	O
dw	O
dz	O
,	O
2σ2	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
w	O
2σ2	O
1	O
v2	O
1	O
v2	O
z	O
(	O
3.78	O
)	O
(	O
cid:21	O
)	O
(	O
cid:1	O
)	O
dw	O
dz	O
dw	O
dz	O
,	O
(	O
3.79	O
)	O
=	O
v	O
1	O
1	O
−∞	O
−∞	O
2v2	O
2πσv	O
φ	O
(	O
cid:0	O
)	O
x	O
−	O
m	O
(	O
cid:1	O
)	O
n	O
(	O
x|µ	O
,	O
σ2	O
)	O
dx	O
,	O
where	O
φ	O
(	O
x	O
)	O
=	O
z	O
∞	O
z	O
x	O
exp	O
(	O
cid:0	O
)	O
−	O
(	O
y	O
−	O
m	O
)	O
2	O
z	O
∞	O
z	O
µ−m	O
exp	O
(	O
cid:0	O
)	O
−	O
(	O
z	O
+	O
w	O
)	O
2	O
2v2	O
−	O
w2	O
(	O
cid:21	O
)	O
>	O
(	O
cid:20	O
)	O
1	O
(	O
cid:20	O
)	O
w	O
z	O
µ−m	O
z	O
∞	O
exp	O
(	O
cid:0	O
)	O
−	O
1	O
z	O
µ−m	O
z	O
∞	O
i	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
0	O
,	O
n	O
(	O
cid:16	O
)	O
h	O
w	O
h	O
σ2	O
z	O
µ−m	O
−σ2	O
v2	O
+	O
σ2	O
v2	O
+	O
1	O
σ2	O
i	O
(	O
cid:17	O
)	O
−σ2	O
2πσv	O
2πσv	O
−∞	O
−∞	O
−∞	O
2	O
z	O
−∞	O
−∞	O
−∞	O
1	O
v2	O
1	O
z	O
(	O
cid:16	O
)	O
−	O
(	O
cid:17	O
)	O
dz	O
=	O
φ	O
(	O
cid:0	O
)	O
µ	O
−	O
m√	O
(	O
cid:1	O
)	O
,	O
v2	O
+	O
σ2	O
i.e	O
.	O
an	O
(	O
incomplete	O
)	O
integral	O
over	O
a	O
joint	B
gaussian	O
.	O
the	O
inner	O
integral	O
corre-	O
sponds	O
to	O
marginalizing	O
over	O
w	O
(	O
see	B
eq	O
.	O
(	O
a.6	O
)	O
)	O
,	O
yielding	O
(	O
3.80	O
)	O
which	O
assumed	O
v	O
>	O
0.	O
if	O
v	O
is	O
negative	O
,	O
we	O
can	O
substitute	O
the	O
symmetry	O
φ	O
(	O
−z	O
)	O
=	O
1	O
−	O
φ	O
(	O
z	O
)	O
into	O
eq	O
.	O
(	O
3.77	O
)	O
to	O
get	O
z2	O
−∞	O
exp	O
zv	O
>	O
0	O
=	O
2	O
(	O
v2	O
+	O
σ2	O
)	O
1p2π	O
(	O
v2	O
+	O
σ2	O
)	O
(	O
cid:1	O
)	O
=	O
φ	O
(	O
cid:0	O
)	O
−	O
µ	O
−	O
m√	O
zv	O
<	O
0	O
=	O
1	O
−	O
φ	O
(	O
cid:0	O
)	O
µ	O
−	O
m√	O
(	O
cid:1	O
)	O
n	O
(	O
x|µ	O
,	O
σ2	O
)	O
dx	O
=	O
φ	O
(	O
z	O
)	O
,	O
where	O
z	O
=	O
(	O
cid:1	O
)	O
n	O
(	O
x|µ	O
,	O
σ2	O
)	O
,	O
q	O
(	O
x	O
)	O
=	O
z−1φ	O
(	O
cid:0	O
)	O
x	O
−	O
m	O
v2	O
+	O
σ2	O
collecting	O
the	O
two	O
cases	O
,	O
eq	O
.	O
(	O
3.80	O
)	O
and	O
eq	O
.	O
(	O
3.81	O
)	O
we	O
arrive	O
at	O
√	O
z	O
=	O
for	O
general	O
v	O
6=	O
0.	O
we	O
wish	O
to	O
compute	O
the	O
moments	O
of	O
φ	O
(	O
cid:0	O
)	O
x	O
−	O
m	O
v2	O
+	O
σ2	O
z	O
v	O
v	O
(	O
cid:1	O
)	O
.	O
(	O
3.81	O
)	O
,	O
(	O
3.82	O
)	O
(	O
3.83	O
)	O
µ	O
−	O
m	O
1	O
+	O
σ2/v2	O
v	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.10	O
exercises	O
75	O
where	O
z	O
is	O
given	O
in	O
eq	O
.	O
(	O
3.82	O
)	O
.	O
perhaps	O
the	O
easiest	O
way	O
to	O
do	O
this	O
is	O
to	O
diﬀer-	O
entiate	O
w.r.t	O
.	O
µ	O
on	O
both	O
sides	O
of	O
eq	O
.	O
(	O
3.82	O
)	O
z	O
x	O
−	O
µ	O
σ2	O
φ	O
(	O
cid:0	O
)	O
x	O
−	O
m	O
xφ	O
(	O
cid:0	O
)	O
x	O
−	O
m	O
v	O
(	O
cid:1	O
)	O
n	O
(	O
x|µ	O
,	O
σ2	O
)	O
dx	O
=	O
∂	O
v	O
(	O
cid:1	O
)	O
n	O
(	O
x|µ	O
,	O
σ2	O
)	O
dx	O
−	O
µz	O
σ2	O
=	O
∂z	O
∂µ	O
=	O
z	O
1	O
σ2	O
φ	O
(	O
z	O
)	O
⇐⇒	O
n	O
(	O
z	O
)	O
1	O
+	O
σ2/v2	O
∂µ	O
√	O
v	O
(	O
3.84	O
)	O
,	O
where	O
we	O
have	O
used	O
∂φ	O
(	O
z	O
)	O
/∂µ	O
=	O
n	O
(	O
z	O
)	O
∂z/∂µ	O
.	O
we	O
recognize	O
the	O
ﬁrst	O
term	O
in	O
the	O
integral	O
in	O
the	O
top	O
line	O
of	O
eq	O
.	O
(	O
3.84	O
)	O
as	O
z/σ2	O
times	O
the	O
ﬁrst	O
moment	O
of	O
q	O
which	O
we	O
are	O
seeking	O
.	O
multiplying	O
through	O
by	O
σ2/z	O
and	O
rearranging	O
we	O
obtain	O
ﬁrst	O
moment	O
eq	O
[	O
x	O
]	O
=	O
µ	O
+	O
σ2n	O
(	O
z	O
)	O
√	O
φ	O
(	O
z	O
)	O
v	O
1	O
+	O
σ2/v2	O
.	O
(	O
3.85	O
)	O
similarly	O
,	O
the	O
second	O
moment	O
can	O
be	O
obtained	O
by	O
diﬀerentiating	O
eq	O
.	O
(	O
3.82	O
)	O
twice	O
z	O
h	O
x2	O
σ4	O
−	O
2µx	O
⇐⇒	O
eq	O
[	O
x2	O
]	O
=	O
2µeq	O
[	O
x	O
]	O
−	O
µ2	O
+	O
σ2	O
−	O
σ4zn	O
(	O
z	O
)	O
φ	O
(	O
cid:0	O
)	O
x	O
−	O
m	O
i	O
σ4	O
+	O
µ2	O
σ4	O
−	O
1	O
σ2	O
v	O
(	O
cid:1	O
)	O
n	O
(	O
x|µ	O
,	O
σ2	O
)	O
dx	O
=	O
−	O
zn	O
(	O
z	O
)	O
v2	O
+	O
σ2	O
∂2z	O
∂µ2	O
=	O
(	O
3.86	O
)	O
φ	O
(	O
z	O
)	O
(	O
v2	O
+	O
σ2	O
)	O
,	O
second	O
moment	O
where	O
the	O
ﬁrst	O
and	O
second	O
terms	O
of	O
the	O
integral	O
in	O
the	O
top	O
line	O
of	O
eq	O
.	O
(	O
3.86	O
)	O
are	O
multiples	O
of	O
the	O
ﬁrst	O
and	O
second	O
moments	O
.	O
the	O
second	O
central	O
moment	O
after	O
reintroducing	O
eq	O
.	O
(	O
3.85	O
)	O
into	O
eq	O
.	O
(	O
3.86	O
)	O
and	O
simplifying	O
is	O
given	O
by	O
(	O
cid:2	O
)	O
(	O
x−eq	O
[	O
x	O
]	O
)	O
2	O
(	O
cid:3	O
)	O
=	O
eq	O
[	O
x2	O
]	O
−eq	O
[	O
x	O
]	O
2	O
=	O
σ2−	O
eq	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
σ4n	O
(	O
z	O
)	O
(	O
v2	O
+	O
σ2	O
)	O
φ	O
(	O
z	O
)	O
z	O
+	O
n	O
(	O
z	O
)	O
φ	O
(	O
z	O
)	O
.	O
(	O
3.87	O
)	O
3.10	O
exercises	O
1.	O
for	O
binary	B
gpc	O
,	O
show	O
the	O
equivalence	O
of	O
using	O
a	O
noise-free	O
latent	O
process	O
combined	O
with	O
a	O
probit	B
likelihood	O
and	O
a	O
latent	O
process	O
with	O
gaussian	O
noise	O
combined	O
with	O
a	O
step-function	O
likelihood	B
.	O
hint	O
:	O
introduce	O
explicitly	O
additional	O
noisy	O
latent	O
variables	O
˜fi	O
,	O
which	O
diﬀer	O
from	O
fi	O
by	O
gaussian	O
noise	O
.	O
write	O
down	O
the	O
step	O
function	B
likelihood	O
for	O
a	O
single	O
case	O
as	O
a	O
function	B
of	O
˜fi	O
,	O
integrate	O
out	O
the	O
noisy	O
variable	O
,	O
to	O
arrive	O
at	O
the	O
probit	B
likelihood	O
as	O
a	O
function	B
of	O
the	O
noise-free	O
process	B
.	O
2.	O
consider	O
a	O
multinomial	O
random	O
variable	O
y	O
having	O
c	O
states	O
,	O
with	O
yc	O
=	O
1	O
if	O
the	O
variable	O
is	O
in	O
state	O
c	O
,	O
and	O
0	O
otherwise	O
.	O
state	O
c	O
occurs	O
with	O
probability	B
πc	O
.	O
show	O
that	O
cov	O
(	O
y	O
)	O
=	O
e	O
[	O
(	O
y	O
−	O
π	O
)	O
(	O
y	O
−	O
π	O
)	O
>	O
]	O
=	O
diag	O
(	O
π	O
)	O
−	O
ππ	O
>	O
.	O
ob-	O
serve	O
that	O
cov	O
(	O
y	O
)	O
,	O
being	O
a	O
covariance	B
matrix	I
,	O
must	O
necessarily	O
be	O
positive	B
semideﬁnite	I
.	O
using	O
this	O
fact	O
show	O
that	O
the	O
matrix	B
w	O
=	O
diag	O
(	O
π	O
)	O
−	O
ππ	O
>	O
from	O
eq	O
.	O
(	O
3.38	O
)	O
is	O
positive	B
semideﬁnite	I
.	O
by	O
showing	O
that	O
the	O
vector	O
of	O
all	O
ones	O
is	O
an	O
eigenvector	O
of	O
cov	O
(	O
y	O
)	O
with	O
eigenvalue	B
zero	O
,	O
verify	O
that	O
the	O
ma-	O
trix	O
is	O
indeed	O
positive	B
semideﬁnite	I
,	O
and	O
not	O
positive	B
deﬁnite	I
.	O
(	O
see	B
section	O
4.1	O
for	O
deﬁnitions	O
of	O
positive	B
semideﬁnite	I
and	O
positive	B
deﬁnite	I
matrices	O
.	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
76	O
classiﬁcation	B
figure	O
3.13	O
:	O
the	O
decision	O
regions	O
for	O
the	O
three-class	O
softmax	B
function	O
in	O
z2-z3	O
space	O
.	O
3.	O
consider	O
the	O
3-class	O
softmax	B
function	O
p	O
(	O
cc	O
)	O
=	O
exp	O
(	O
fc	O
)	O
exp	O
(	O
f1	O
)	O
+	O
exp	O
(	O
f2	O
)	O
+	O
exp	O
(	O
f3	O
)	O
,	O
where	O
c	O
=	O
1	O
,	O
2	O
,	O
3	O
and	O
f1	O
,	O
f2	O
,	O
f3	O
are	O
the	O
corresponding	O
activations	O
.	O
to	O
more	O
easily	O
visualize	O
the	O
decision	O
boundaries	O
,	O
let	O
z2	O
=	O
f2	O
−	O
f1	O
and	O
z3	O
=	O
f3	O
−	O
f1	O
.	O
thus	O
p	O
(	O
c1	O
)	O
=	O
1	O
1	O
+	O
exp	O
(	O
z2	O
)	O
+	O
exp	O
(	O
z3	O
)	O
,	O
(	O
3.88	O
)	O
and	O
similarly	O
for	O
the	O
other	O
classes	O
.	O
the	O
decision	O
boundary	O
relating	O
to	O
p	O
(	O
c1	O
)	O
>	O
1/3	O
is	O
the	O
curve	O
exp	O
(	O
z2	O
)	O
+	O
exp	O
(	O
z3	O
)	O
=	O
2.	O
the	O
decision	O
regions	O
for	O
the	O
three	O
classes	O
are	O
illustrated	O
in	O
figure	O
3.13.	O
let	O
f	O
=	O
(	O
f1	O
,	O
f2	O
,	O
f3	O
)	O
>	O
have	O
a	O
gaussian	O
distribution	O
centered	O
on	O
the	O
origin	O
,	O
and	O
let	O
π	O
(	O
f	O
)	O
=	O
softmax	B
(	O
f	O
)	O
.	O
we	O
now	O
consider	O
the	O
eﬀect	O
of	O
this	O
distribution	O
on	O
¯π	O
=r	O
π	O
(	O
f	O
)	O
p	O
(	O
f	O
)	O
df	O
.	O
for	O
a	O
gaussian	O
with	O
given	O
covariance	B
structure	O
this	O
integral	O
is	O
easily	O
approxi-	O
mated	O
by	O
drawing	O
samples	O
from	O
p	O
(	O
f	O
)	O
.	O
show	O
that	O
the	O
classiﬁcation	B
can	O
be	O
made	O
to	O
fall	O
into	O
any	O
of	O
the	O
three	O
categories	O
depending	O
on	O
the	O
covariance	B
matrix	I
.	O
thus	O
,	O
by	O
considering	O
displacements	O
of	O
the	O
mean	O
of	O
the	O
gaussian	O
by	O
	O
from	O
the	O
origin	O
into	O
each	O
of	O
the	O
three	O
regions	O
we	O
have	O
shown	O
that	O
overall	O
classiﬁcation	B
depends	O
not	O
only	O
on	O
the	O
mean	O
of	O
the	O
gaussian	O
but	O
also	O
on	O
its	O
covariance	B
.	O
show	O
that	O
this	O
conclusion	O
is	O
still	O
valid	O
when	O
it	O
is	O
recalled	O
that	O
z	O
is	O
derived	O
from	O
f	O
as	O
z	O
=	O
t	O
f	O
where	O
(	O
cid:18	O
)	O
1	O
0	O
(	O
cid:19	O
)	O
,	O
0	O
−1	O
1	O
−1	O
t	O
=	O
so	O
that	O
cov	O
(	O
z	O
)	O
=	O
t	O
cov	O
(	O
f	O
)	O
t	O
>	O
.	O
4.	O
consider	O
the	O
update	O
equation	O
for	O
f	O
new	O
given	O
by	O
eq	O
.	O
(	O
3.18	O
)	O
when	O
some	O
of	O
the	O
training	O
points	O
are	O
well-explained	O
under	O
f	O
so	O
that	O
ti	O
’	O
πi	O
and	O
wii	O
’	O
0	O
r1r2r3−303−303z2z3	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
3.10	O
exercises	O
77	O
for	O
these	O
points	O
.	O
break	O
f	O
into	O
two	O
subvectors	O
,	O
f1	O
that	O
corresponds	O
to	O
points	O
that	O
are	O
not	O
well-explained	O
,	O
and	O
f2	O
to	O
those	O
that	O
are	O
.	O
re-write	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
from	O
eq	O
.	O
(	O
3.18	O
)	O
as	O
k	O
(	O
i	O
+	O
w	O
k	O
)	O
−1	O
and	O
let	O
k	O
be	O
partitioned	O
as	O
k11	O
,	O
k12	O
,	O
k21	O
,	O
k22	O
and	O
similarly	O
for	O
the	O
other	O
matrices	O
.	O
using	O
the	O
partitioned	O
matrix	O
inverse	O
equations	O
(	O
see	B
section	O
a.3	O
)	O
show	O
that	O
=	O
k11	O
(	O
i11	O
+	O
w11k11	O
)	O
−1	O
(	O
cid:0	O
)	O
w11f1	O
+	O
∇	O
log	O
p	O
(	O
y1|f1	O
)	O
(	O
cid:1	O
)	O
,	O
(	O
3.89	O
)	O
f	O
new	O
1	O
f	O
new	O
2	O
11	O
f	O
new	O
1	O
=	O
k21k−1	O
.	O
see	B
section	O
3.4.1	O
for	O
the	O
consequences	O
of	O
this	O
result	O
.	O
5.	O
show	O
that	O
the	O
expressions	O
in	O
eq	O
.	O
(	O
3.56	O
)	O
for	O
the	O
cavity	O
mean	O
µ−i	O
and	O
vari-	O
ance	O
σ2−i	O
do	O
not	O
depend	O
on	O
the	O
approximate	O
likelihood	B
terms	O
˜µi	O
and	O
˜σ2	O
i	O
for	O
the	O
corresponding	O
case	O
,	O
despite	O
the	O
appearance	O
of	O
eq	O
.	O
(	O
3.56	O
)	O
.	O
6.	O
consider	O
the	O
usps	O
3s	O
vs.	O
5s	O
prediction	B
problem	O
discussed	O
in	O
section	O
3.7.3.	O
use	O
the	O
implementation	O
of	O
the	O
laplace	O
binary	B
gpc	O
provided	O
to	O
investi-	O
gate	O
how	O
ˆf	O
and	O
the	O
predictive	B
probabilities	O
etc	O
.	O
vary	O
as	O
functions	O
of	O
log	O
(	O
‘	O
)	O
and	O
log	O
(	O
σf	O
)	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
chapter	O
4	O
covariance	B
functions	O
we	O
have	O
seen	O
that	O
a	O
covariance	B
function	I
is	O
the	O
crucial	O
ingredient	O
in	O
a	O
gaussian	O
process	B
predictor	O
,	O
as	O
it	O
encodes	O
our	O
assumptions	O
about	O
the	O
function	B
which	O
we	O
wish	O
to	O
learn	O
.	O
from	O
a	O
slightly	O
diﬀerent	O
viewpoint	O
it	O
is	O
clear	O
that	O
in	O
supervised	B
learning	I
the	O
notion	O
of	O
similarity	O
between	O
data	O
points	O
is	O
crucial	O
;	O
it	O
is	O
a	O
basic	O
assumption	O
that	O
points	O
with	O
inputs	O
x	O
which	O
are	O
close	O
are	O
likely	O
to	O
have	O
similar	O
target	O
values	O
y	O
,	O
and	O
thus	O
training	O
points	O
that	O
are	O
near	O
to	O
a	O
test	O
point	O
should	O
be	O
informative	O
about	O
the	O
prediction	B
at	O
that	O
point	O
.	O
under	O
the	O
gaussian	O
process	B
view	O
it	O
is	O
the	O
covariance	B
function	I
that	O
deﬁnes	O
nearness	O
or	O
similarity	O
.	O
an	O
arbitrary	O
function	B
of	O
input	O
pairs	O
x	O
and	O
x0	O
will	O
not	O
,	O
in	O
general	O
,	O
be	O
a	O
valid	O
covariance	B
function.1	O
the	O
purpose	O
of	O
this	O
chapter	O
is	O
to	O
give	O
examples	O
of	O
some	O
commonly-used	O
covariance	B
functions	O
and	O
to	O
examine	O
their	O
properties	O
.	O
section	O
4.1	O
deﬁnes	O
a	O
number	O
of	O
basic	O
terms	O
relating	O
to	O
covariance	B
functions	O
.	O
section	O
4.2	O
gives	O
examples	O
of	O
stationary	O
,	O
dot-product	O
,	O
and	O
other	O
non-stationary	O
covariance	B
functions	O
,	O
and	O
also	O
gives	O
some	O
ways	O
to	O
make	O
new	O
ones	O
from	O
old	O
.	O
section	O
4.3	O
introduces	O
the	O
important	O
topic	O
of	O
eigenfunction	B
analysis	O
of	O
covariance	B
functions	O
,	O
and	O
states	O
mercer	O
’	O
s	O
theorem	O
which	O
allows	O
us	O
to	O
express	O
the	O
covariance	B
function	I
(	O
under	O
certain	O
conditions	O
)	O
in	O
terms	O
of	O
its	O
eigenfunctions	O
and	O
eigenvalues	O
.	O
the	O
covariance	B
functions	O
given	O
in	O
section	O
4.2	O
are	O
valid	O
when	O
the	O
input	O
domain	O
x	O
is	O
a	O
subset	O
of	O
rd	O
.	O
in	O
section	O
4.4	O
we	O
describe	O
ways	O
to	O
deﬁne	O
covariance	B
functions	O
when	O
the	O
input	O
domain	O
is	O
over	O
structured	O
objects	O
such	O
as	O
strings	O
and	O
trees	O
.	O
4.1	O
preliminaries	O
a	O
stationary	O
covariance	B
function	I
is	O
a	O
function	B
of	O
x	O
−	O
x0	O
.	O
thus	O
it	O
is	O
invariant	O
to	O
translations	O
in	O
the	O
input	O
space.2	O
for	O
example	O
the	O
squared	B
exponential	I
co-	O
1to	O
be	O
a	O
valid	O
covariance	B
function	I
it	O
must	O
be	O
positive	B
semideﬁnite	I
,	O
see	B
eq	O
.	O
(	O
4.2	O
)	O
.	O
2in	O
stochastic	O
process	O
theory	O
a	O
process	B
which	O
has	O
constant	O
mean	O
and	O
whose	O
covariance	B
function	I
is	O
invariant	O
to	O
translations	O
is	O
called	O
weakly	O
stationary	O
.	O
a	O
process	B
is	O
strictly	O
sta-	O
tionary	O
if	O
all	O
of	O
its	O
ﬁnite	O
dimensional	O
distributions	O
are	O
invariant	O
to	O
translations	O
[	O
papoulis	O
,	O
1991	O
,	O
sec	O
.	O
10.1	O
]	O
.	O
similarity	O
valid	O
covariance	B
functions	O
stationarity	B
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
80	O
isotropy	B
dot	O
product	O
covariance	B
kernel	O
gram	O
matrix	B
covariance	O
matrix	B
positive	O
semideﬁnite	O
covariance	B
functions	O
variance	O
function	B
given	O
in	O
equation	O
2.16	O
is	O
stationary	O
.	O
if	O
further	O
the	O
covariance	B
function	I
is	O
a	O
function	B
only	O
of	O
|x	O
−	O
x0|	O
then	O
it	O
is	O
called	O
isotropic	O
;	O
it	O
is	O
thus	O
in-	O
variant	O
to	O
all	O
rigid	O
motions	O
.	O
for	O
example	O
the	O
squared	B
exponential	I
covariance	O
function	B
given	O
in	O
equation	O
2.16	O
is	O
isotropic	O
.	O
as	O
k	O
is	O
now	O
only	O
a	O
function	B
of	O
r	O
=	O
|x	O
−	O
x0|	O
these	O
are	O
also	O
known	O
as	O
radial	O
basis	O
functions	O
(	O
rbfs	O
)	O
.	O
if	O
a	O
covariance	B
function	I
depends	O
only	O
on	O
x	O
and	O
x0	O
through	O
x	O
·	O
x0	O
we	O
call	O
it	O
a	O
dot	B
product	I
covariance	O
function	B
.	O
a	O
simple	O
example	O
is	O
the	O
covariance	B
function	I
0	O
+	O
x	O
·	O
x0	O
which	O
can	O
be	O
obtained	O
from	O
linear	B
regression	I
by	O
putting	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
σ2	O
n	O
(	O
0	O
,	O
1	O
)	O
priors	O
on	O
the	O
coeﬃcients	O
of	O
xd	O
(	O
d	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
d	O
)	O
and	O
a	O
prior	O
of	O
n	O
(	O
0	O
,	O
σ2	O
0	O
)	O
on	O
the	O
bias	B
(	O
or	O
constant	O
function	B
)	O
1	O
,	O
see	B
eq	O
.	O
(	O
2.15	O
)	O
.	O
another	O
important	O
example	O
0	O
+	O
x	O
·	O
x0	O
)	O
p	O
where	O
p	O
is	O
a	O
is	O
the	O
inhomogeneous	B
polynomial	I
kernel	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
(	O
σ2	O
positive	O
integer	O
.	O
dot	B
product	I
covariance	O
functions	O
are	O
invariant	O
to	O
a	O
rotation	O
of	O
the	O
coordinates	O
about	O
the	O
origin	O
,	O
but	O
not	O
translations	O
.	O
a	O
general	O
name	O
for	O
a	O
function	B
k	O
of	O
two	O
arguments	O
mapping	O
a	O
pair	O
of	O
inputs	O
x	O
∈	O
x	O
,	O
x0	O
∈	O
x	O
into	O
r	O
is	O
a	O
kernel	B
.	O
this	O
term	O
arises	O
in	O
the	O
theory	O
of	O
integral	O
operators	O
,	O
where	O
the	O
operator	B
tk	O
is	O
deﬁned	O
as	O
(	O
tkf	O
)	O
(	O
x	O
)	O
=	O
k	O
(	O
x	O
,	O
x0	O
)	O
f	O
(	O
x0	O
)	O
dµ	O
(	O
x0	O
)	O
,	O
(	O
4.1	O
)	O
z	O
x	O
where	O
µ	O
denotes	O
a	O
measure	B
;	O
see	B
section	O
a.7	O
for	O
further	O
explanation	O
of	O
this	O
point.3	O
a	O
real	O
kernel	B
is	O
said	O
to	O
be	O
symmetric	O
if	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
k	O
(	O
x0	O
,	O
x	O
)	O
;	O
clearly	O
covariance	B
functions	O
must	O
be	O
symmetric	O
from	O
the	O
deﬁnition	O
.	O
given	O
a	O
set	B
of	O
input	O
points	O
{	O
xi|i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
we	O
can	O
compute	O
the	O
gram	O
matrix	B
k	O
whose	O
entries	O
are	O
kij	O
=	O
k	O
(	O
xi	O
,	O
xj	O
)	O
.	O
if	O
k	O
is	O
a	O
covariance	B
function	I
we	O
call	O
the	O
matrix	B
k	O
the	O
covariance	B
matrix	I
.	O
a	O
real	O
n	O
×	O
n	O
matrix	B
k	O
which	O
satisﬁes	O
q	O
(	O
v	O
)	O
=	O
v	O
>	O
kv	O
≥	O
0	O
for	O
all	O
vectors	O
v	O
∈	O
rn	O
is	O
called	O
positive	B
semideﬁnite	I
(	O
psd	O
)	O
.	O
if	O
q	O
(	O
v	O
)	O
=	O
0	O
only	O
when	O
v	O
=	O
0	O
the	O
matrix	B
is	O
positive	B
deﬁnite	I
.	O
q	O
(	O
v	O
)	O
is	O
called	O
a	O
quadratic	B
form	I
.	O
a	O
symmetric	O
matrix	B
is	O
psd	O
if	O
and	O
only	O
if	O
all	O
of	O
its	O
eigenvalues	O
are	O
non-negative	O
.	O
a	O
gram	O
matrix	B
corresponding	O
to	O
a	O
general	O
kernel	B
function	O
need	O
not	O
be	O
psd	O
,	O
but	O
the	O
gram	O
matrix	B
corresponding	O
to	O
a	O
covariance	B
function	I
is	O
psd	O
.	O
a	O
kernel	B
is	O
said	O
to	O
be	O
positive	B
semideﬁnite	I
if	O
k	O
(	O
x	O
,	O
x0	O
)	O
f	O
(	O
x	O
)	O
f	O
(	O
x0	O
)	O
dµ	O
(	O
x	O
)	O
dµ	O
(	O
x0	O
)	O
≥	O
0	O
,	O
(	O
4.2	O
)	O
z	O
for	O
all	O
f	O
∈	O
l2	O
(	O
x	O
,	O
µ	O
)	O
.	O
equivalently	O
a	O
kernel	B
function	O
which	O
gives	O
rise	O
to	O
psd	O
gram	O
matrices	O
for	O
any	O
choice	O
of	O
n	O
∈	O
n	O
and	O
d	O
is	O
positive	B
semideﬁnite	I
.	O
to	O
see	B
this	O
let	O
f	O
be	O
the	O
weighted	O
sum	O
of	O
delta	O
functions	O
at	O
each	O
xi	O
.	O
since	O
such	O
functions	O
are	O
limits	O
of	O
functions	O
in	O
l2	O
(	O
x	O
,	O
µ	O
)	O
eq	O
.	O
(	O
4.2	O
)	O
implies	O
that	O
the	O
gram	O
matrix	B
corresponding	O
to	O
any	O
d	O
is	O
psd	O
.	O
for	O
a	O
one-dimensional	O
gaussian	O
process	B
one	O
way	O
to	O
understand	O
the	O
charac-	O
teristic	O
length-scale	B
of	O
the	O
process	B
(	O
if	O
this	O
exists	O
)	O
is	O
in	O
terms	O
of	O
the	O
number	O
of	O
upcrossings	O
of	O
a	O
level	O
u.	O
adler	O
[	O
1981	O
,	O
theorem	O
4.1.1	O
]	O
states	O
that	O
the	O
expected	O
3informally	O
speaking	O
,	O
readers	O
will	O
usually	O
be	O
able	O
to	O
substitute	O
dx	O
or	O
p	O
(	O
x	O
)	O
dx	O
for	O
dµ	O
(	O
x	O
)	O
.	O
upcrossing	B
rate	I
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
4.2	O
examples	O
of	O
covariance	B
functions	O
81	O
number	O
of	O
upcrossings	O
e	O
[	O
nu	O
]	O
of	O
the	O
level	O
u	O
on	O
the	O
unit	O
interval	O
by	O
a	O
zero-mean	O
,	O
stationary	O
,	O
almost	O
surely	O
continuous	O
gaussian	O
process	B
is	O
given	O
by	O
s−k00	O
(	O
0	O
)	O
k	O
(	O
0	O
)	O
(	O
cid:16	O
)	O
−	O
u2	O
2k	O
(	O
0	O
)	O
(	O
cid:17	O
)	O
e	O
[	O
nu	O
]	O
=	O
1	O
2π	O
exp	O
.	O
(	O
4.3	O
)	O
if	O
k00	O
(	O
0	O
)	O
does	O
not	O
exist	O
(	O
so	O
that	O
the	O
process	B
is	O
not	O
mean	O
square	O
diﬀerentiable	O
)	O
then	O
if	O
such	O
a	O
process	B
has	O
a	O
zero	O
at	O
x0	O
then	O
it	O
will	O
almost	O
surely	O
have	O
an	O
inﬁnite	O
number	O
of	O
zeros	O
in	O
the	O
arbitrarily	O
small	O
interval	O
(	O
x0	O
,	O
x0	O
+	O
δ	O
)	O
[	O
blake	O
and	O
lindsey	O
,	O
1973	O
,	O
p.	O
303	O
]	O
.	O
4.1.1	O
mean	B
square	I
continuity	I
and	O
diﬀerentiability	O
∗	O
we	O
now	O
describe	O
mean	B
square	I
continuity	I
and	O
diﬀerentiability	O
of	O
stochastic	O
pro-	O
cesses	O
,	O
following	O
adler	O
[	O
1981	O
,	O
sec	O
.	O
2.2	O
]	O
.	O
let	O
x1	O
,	O
x2	O
,	O
.	O
.	O
.	O
be	O
a	O
sequence	O
of	O
points	O
and	O
x∗	O
be	O
a	O
ﬁxed	O
point	O
in	O
rd	O
such	O
that	O
|xk	O
−	O
x∗|	O
→	O
0	O
as	O
k	O
→	O
∞	O
.	O
then	O
a	O
process	B
f	O
(	O
x	O
)	O
is	O
continuous	O
in	O
mean	O
square	O
at	O
x∗	O
if	O
e	O
[	O
|f	O
(	O
xk	O
)	O
−	O
f	O
(	O
x∗	O
)	O
|2	O
]	O
→	O
0	O
as	O
mean	B
square	I
continuity	I
k	O
→	O
∞	O
.	O
if	O
this	O
holds	O
for	O
all	O
x∗	O
∈	O
a	O
where	O
a	O
is	O
a	O
subset	O
of	O
rd	O
then	O
f	O
(	O
x	O
)	O
is	O
said	O
to	O
be	O
continuous	O
in	O
mean	O
square	O
(	O
ms	O
)	O
over	O
a.	O
a	O
random	O
ﬁeld	O
is	O
continuous	O
in	O
mean	O
square	O
at	O
x∗	O
if	O
and	O
only	O
if	O
its	O
covariance	B
function	I
k	O
(	O
x	O
,	O
x0	O
)	O
is	O
continuous	O
at	O
the	O
point	O
x	O
=	O
x0	O
=	O
x∗	O
.	O
for	O
stationary	O
covariance	B
functions	O
this	O
reduces	O
to	O
checking	O
continuity	O
at	O
k	O
(	O
0	O
)	O
.	O
note	O
that	O
ms	O
continuity	O
does	O
not	O
necessarily	O
imply	O
sample	O
function	B
continuity	O
;	O
for	O
a	O
discussion	O
of	O
sample	O
function	B
continuity	O
and	O
diﬀerentiability	O
see	B
adler	O
[	O
1981	O
,	O
ch	O
.	O
3	O
]	O
.	O
the	O
mean	O
square	O
derivative	O
of	O
f	O
(	O
x	O
)	O
in	O
the	O
ith	O
direction	O
is	O
deﬁned	O
as	O
mean	B
square	I
diﬀerentiability	I
∂f	O
(	O
x	O
)	O
∂xi	O
=	O
l.	O
i.	O
m	O
h→0	O
f	O
(	O
x	O
+	O
hei	O
)	O
−	O
f	O
(	O
x	O
)	O
h	O
,	O
(	O
4.4	O
)	O
when	O
the	O
limit	O
exists	O
,	O
where	O
l.i.m	O
denotes	O
the	O
limit	O
in	O
mean	O
square	O
and	O
ei	O
is	O
the	O
unit	O
vector	O
in	O
the	O
ith	O
direction	O
.	O
the	O
covariance	B
function	I
of	O
∂f	O
(	O
x	O
)	O
/∂xi	O
is	O
given	O
by	O
∂2k	O
(	O
x	O
,	O
x0	O
)	O
/∂xi∂x0	O
i.	O
these	O
deﬁnitions	O
can	O
be	O
extended	O
to	O
higher	O
order	O
derivatives	O
.	O
for	O
stationary	O
processes	O
,	O
if	O
the	O
2kth-order	O
partial	O
derivative	O
∂2kk	O
(	O
x	O
)	O
/∂2xi1	O
.	O
.	O
.	O
∂2xik	O
exists	O
and	O
is	O
ﬁnite	O
at	O
x	O
=	O
0	O
then	O
the	O
kth	O
order	O
partial	O
derivative	O
∂kf	O
(	O
x	O
)	O
/∂xi1	O
.	O
.	O
.	O
xik	O
exists	O
for	O
all	O
x	O
∈	O
rd	O
as	O
a	O
mean	O
square	O
limit	O
.	O
notice	O
that	O
it	O
is	O
the	O
properties	O
of	O
the	O
kernel	B
k	O
around	O
0	O
that	O
determine	O
the	O
smoothness	O
properties	O
(	O
ms	O
diﬀerentiability	O
)	O
of	O
a	O
stationary	O
process	B
.	O
4.2	O
examples	O
of	O
covariance	B
functions	O
in	O
this	O
section	O
we	O
consider	O
covariance	B
functions	O
where	O
the	O
input	O
domain	O
x	O
is	O
a	O
subset	O
of	O
the	O
vector	O
space	O
rd	O
.	O
more	O
general	O
input	O
spaces	O
are	O
considered	O
in	O
section	O
4.4.	O
we	O
start	O
in	O
section	O
4.2.1	O
with	O
stationary	O
covariance	B
functions	O
,	O
then	O
consider	O
dot-product	O
covariance	B
functions	O
in	O
section	O
4.2.2	O
and	O
other	O
varieties	O
of	O
non-stationary	O
covariance	B
functions	O
in	O
section	O
4.2.3.	O
we	O
give	O
an	O
overview	O
of	O
some	O
commonly	O
used	O
covariance	B
functions	O
in	O
table	O
4.1	O
and	O
in	O
section	O
4.2.4	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
82	O
covariance	B
functions	O
we	O
describe	O
general	O
methods	O
for	O
constructing	O
new	O
kernels	O
from	O
old	O
.	O
there	O
exist	O
several	O
other	O
good	O
overviews	O
of	O
covariance	B
functions	O
,	O
see	B
e.g	O
.	O
abrahamsen	O
[	O
1997	O
]	O
.	O
4.2.1	O
stationary	O
covariance	B
functions	O
in	O
this	O
section	O
(	O
and	O
section	O
4.3	O
)	O
it	O
will	O
be	O
convenient	O
to	O
allow	O
kernels	O
to	O
be	O
a	O
map	O
from	O
x	O
∈	O
x	O
,	O
x0	O
∈	O
x	O
into	O
c	O
(	O
rather	O
than	O
r	O
)	O
.	O
if	O
a	O
zero-mean	O
process	B
f	O
is	O
complex-	O
valued	O
,	O
then	O
the	O
covariance	B
function	I
is	O
deﬁned	O
as	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
e	O
[	O
f	O
(	O
x	O
)	O
f∗	O
(	O
x0	O
)	O
]	O
,	O
where	O
∗	O
denotes	O
complex	O
conjugation	O
.	O
a	O
stationary	O
covariance	B
function	I
is	O
a	O
function	B
of	O
τ	O
=	O
x	O
−	O
x0	O
.	O
sometimes	O
in	O
this	O
case	O
we	O
will	O
write	O
k	O
as	O
a	O
function	B
of	O
a	O
single	O
argument	O
,	O
i.e	O
.	O
k	O
(	O
τ	O
)	O
.	O
the	O
covariance	B
function	I
of	O
a	O
stationary	O
process	B
can	O
be	O
represented	O
as	O
the	O
fourier	O
transform	O
of	O
a	O
positive	O
ﬁnite	O
measure	B
.	O
bochner	O
’	O
s	O
theorem	O
theorem	O
4.1	O
(	O
bochner	O
’	O
s	O
theorem	O
)	O
a	O
complex-valued	O
function	B
k	O
on	O
rd	O
is	O
the	O
covariance	B
function	I
of	O
a	O
weakly	O
stationary	O
mean	O
square	O
continuous	O
complex-	O
valued	O
random	O
process	B
on	O
rd	O
if	O
and	O
only	O
if	O
it	O
can	O
be	O
represented	O
as	O
where	O
µ	O
is	O
a	O
positive	O
ﬁnite	O
measure	B
.	O
k	O
(	O
τ	O
)	O
=	O
rd	O
e2πis·τ	O
dµ	O
(	O
s	O
)	O
(	O
4.5	O
)	O
(	O
cid:3	O
)	O
z	O
spectral	O
density	O
power	O
spectrum	O
the	O
statement	O
of	O
bochner	O
’	O
s	O
theorem	O
is	O
quoted	O
from	O
stein	O
[	O
1999	O
,	O
p.	O
24	O
]	O
;	O
a	O
proof	O
can	O
be	O
found	O
in	O
gihman	O
and	O
skorohod	O
[	O
1974	O
,	O
p.	O
208	O
]	O
.	O
if	O
µ	O
has	O
a	O
density	O
s	O
(	O
s	O
)	O
then	O
s	O
is	O
known	O
as	O
the	O
spectral	O
density	O
or	O
power	O
spectrum	O
corresponding	O
to	O
k.	O
the	O
construction	O
given	O
by	O
eq	O
.	O
(	O
4.5	O
)	O
puts	O
non-negative	O
power	O
into	O
each	O
fre-	O
quency	O
s	O
;	O
this	O
is	O
analogous	O
to	O
the	O
requirement	O
that	O
the	O
prior	O
covariance	B
matrix	I
σp	O
on	O
the	O
weights	O
in	O
equation	O
2.4	O
be	O
non-negative	O
deﬁnite	O
.	O
z	O
z	O
in	O
the	O
case	O
that	O
the	O
spectral	O
density	O
s	O
(	O
s	O
)	O
exists	O
,	O
the	O
covariance	B
function	I
and	O
the	O
spectral	O
density	O
are	O
fourier	O
duals	O
of	O
each	O
other	O
as	O
shown	O
in	O
eq	O
.	O
(	O
4.6	O
)	O
;	O
4	O
this	O
is	O
known	O
as	O
the	O
wiener-khintchine	O
theorem	O
,	O
see	B
,	O
e.g	O
.	O
chatﬁeld	O
[	O
1989	O
]	O
k	O
(	O
τ	O
)	O
=	O
notice	O
that	O
the	O
variance	O
of	O
the	O
process	B
is	O
k	O
(	O
0	O
)	O
=r	O
s	O
(	O
s	O
)	O
ds	O
so	O
the	O
power	O
spectrum	O
s	O
(	O
s	O
)	O
=	O
k	O
(	O
τ	O
)	O
e−2πis·τ	O
dτ	O
.	O
s	O
(	O
s	O
)	O
e2πis·τ	O
ds	O
,	O
(	O
4.6	O
)	O
must	O
be	O
integrable	O
to	O
deﬁne	O
a	O
valid	O
gaussian	O
process	B
.	O
to	O
gain	O
some	O
intuition	O
for	O
the	O
deﬁnition	O
of	O
the	O
power	O
spectrum	O
given	O
in	O
eq	O
.	O
(	O
4.6	O
)	O
it	O
is	O
important	O
to	O
realize	O
that	O
the	O
complex	O
exponentials	O
e2πis·x	O
are	O
eigenfunctions	O
of	O
a	O
stationary	O
kernel	B
with	O
respect	O
to	O
lebesgue	O
measure	B
(	O
see	B
section	O
4.3	O
for	O
further	O
details	O
)	O
.	O
thus	O
s	O
(	O
s	O
)	O
is	O
,	O
loosely	O
speaking	O
,	O
the	O
amount	O
of	O
power	O
allocated	O
on	O
average	O
to	O
the	O
eigenfunction	B
e2πis·x	O
with	O
frequency	O
s.	O
s	O
(	O
s	O
)	O
must	O
eventually	O
decay	O
suﬃciently	O
fast	O
as	O
|s|	O
→	O
∞	O
so	O
that	O
it	O
is	O
integrable	O
;	O
the	O
4see	O
appendix	O
a.8	O
for	O
details	O
of	O
fourier	O
transforms	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
4.2	O
examples	O
of	O
covariance	B
functions	O
83	O
rate	O
of	O
this	O
decay	O
of	O
the	O
power	O
spectrum	O
gives	O
important	O
information	O
about	O
the	O
smoothness	O
of	O
the	O
associated	O
stochastic	O
process	O
.	O
for	O
example	O
it	O
can	O
deter-	O
mine	O
the	O
mean-square	O
diﬀerentiability	O
of	O
the	O
process	B
(	O
see	B
section	O
4.3	O
for	O
further	O
details	O
)	O
.	O
if	O
the	O
covariance	B
function	I
is	O
isotropic	O
(	O
so	O
that	O
it	O
is	O
a	O
function	B
of	O
r	O
,	O
where	O
r	O
=	O
|τ|	O
)	O
then	O
it	O
can	O
be	O
shown	O
that	O
s	O
(	O
s	O
)	O
is	O
a	O
function	B
of	O
s	O
,	O
|s|	O
only	O
[	O
adler	O
,	O
1981	O
,	O
theorem	O
2.5.2	O
]	O
.	O
in	O
this	O
case	O
the	O
integrals	B
in	O
eq	O
.	O
(	O
4.6	O
)	O
can	O
be	O
simpliﬁed	O
by	O
changing	O
to	O
spherical	O
polar	O
coordinates	O
and	O
integrating	O
out	O
the	O
angular	O
variables	O
(	O
see	B
e.g	O
.	O
bracewell	O
,	O
1986	O
,	O
ch	O
.	O
12	O
)	O
to	O
obtain	O
z	O
∞	O
z	O
∞	O
0	O
0	O
k	O
(	O
r	O
)	O
=	O
s	O
(	O
s	O
)	O
=	O
2π	O
rd/2−1	O
2π	O
sd/2−1	O
s	O
(	O
s	O
)	O
jd/2−1	O
(	O
2πrs	O
)	O
sd/2	O
ds	O
,	O
k	O
(	O
r	O
)	O
jd/2−1	O
(	O
2πrs	O
)	O
rd/2	O
dr	O
,	O
(	O
4.7	O
)	O
(	O
4.8	O
)	O
where	O
jd/2−1	O
is	O
a	O
bessel	O
function	B
of	O
order	O
d/2−1	O
.	O
note	O
that	O
the	O
dependence	O
on	O
the	O
dimensionality	O
d	O
in	O
equation	O
4.7	O
means	O
that	O
the	O
same	O
isotropic	O
functional	B
form	O
of	O
the	O
spectral	O
density	O
can	O
give	O
rise	O
to	O
diﬀerent	O
isotropic	O
covariance	B
func-	O
tions	O
in	O
diﬀerent	O
dimensions	O
.	O
similarly	O
,	O
if	O
we	O
start	O
with	O
a	O
particular	O
isotropic	O
covariance	B
function	I
k	O
(	O
r	O
)	O
the	O
form	O
of	O
spectral	O
density	O
will	O
in	O
general	O
depend	O
on	O
d	O
(	O
see	B
,	O
e.g	O
.	O
the	O
mat´ern	O
class	O
spectral	O
density	O
given	O
in	O
eq	O
.	O
(	O
4.15	O
)	O
)	O
and	O
in	O
fact	O
k	O
(	O
r	O
)	O
may	O
not	O
be	O
valid	O
for	O
all	O
d.	O
a	O
necessary	O
condition	O
for	O
the	O
spectral	O
density	O
to	O
exist	O
is	O
thatr	O
rd−1|k	O
(	O
r	O
)	O
|	O
dr	O
<	O
∞	O
;	O
see	B
stein	O
[	O
1999	O
,	O
sec	O
.	O
2.10	O
]	O
for	O
more	O
details	O
.	O
we	O
now	O
give	O
some	O
examples	O
of	O
commonly-used	O
isotropic	O
covariance	B
func-	O
tions	O
.	O
the	O
covariance	B
functions	O
are	O
given	O
in	O
a	O
normalized	O
form	O
where	O
k	O
(	O
0	O
)	O
=	O
1	O
;	O
we	O
can	O
multiply	O
k	O
by	O
a	O
(	O
positive	O
)	O
constant	O
σ2	O
f	O
to	O
get	O
any	O
desired	O
process	B
vari-	O
ance	O
.	O
squared	B
exponential	I
covariance	O
function	B
the	O
squared	B
exponential	I
(	O
se	O
)	O
covariance	B
function	I
has	O
already	O
been	O
introduced	O
in	O
chapter	O
2	O
,	O
eq	O
.	O
(	O
2.16	O
)	O
and	O
has	O
the	O
form	O
kse	O
(	O
r	O
)	O
=	O
exp	O
,	O
(	O
4.9	O
)	O
with	O
parameter	O
‘	O
deﬁning	O
the	O
characteristic	O
length-scale	B
.	O
using	O
eq	O
.	O
(	O
4.3	O
)	O
we	O
see	B
that	O
the	O
mean	O
number	O
of	O
level-zero	O
upcrossings	O
for	O
a	O
se	O
process	B
in	O
1-	O
d	O
is	O
(	O
2π	O
‘	O
)	O
−1	O
,	O
which	O
conﬁrms	O
the	O
rˆole	O
of	O
‘	O
as	O
a	O
length-scale	B
.	O
this	O
covari-	O
ance	O
function	B
is	O
inﬁnitely	O
diﬀerentiable	O
,	O
which	O
means	O
that	O
the	O
gp	O
with	O
this	O
covariance	B
function	I
has	O
mean	O
square	O
derivatives	O
of	O
all	O
orders	O
,	O
and	O
is	O
thus	O
very	O
smooth	O
.	O
the	O
spectral	O
density	O
of	O
the	O
se	O
covariance	B
function	I
is	O
s	O
(	O
s	O
)	O
=	O
(	O
2π	O
‘	O
2	O
)	O
d/2	O
exp	O
(	O
−2π2	O
‘	O
2s2	O
)	O
.	O
stein	O
[	O
1999	O
]	O
argues	O
that	O
such	O
strong	O
smoothness	O
assumptions	O
are	O
unrealistic	O
for	O
modelling	O
many	O
physical	O
processes	O
,	O
and	O
rec-	O
ommends	O
the	O
mat´ern	O
class	O
(	O
see	B
below	O
)	O
.	O
however	O
,	O
the	O
squared	B
exponential	I
is	O
probably	O
the	O
most	O
widely-used	O
kernel	B
within	O
the	O
kernel	B
machines	O
ﬁeld	O
.	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
−	O
r2	O
2	O
‘	O
2	O
squared	B
exponential	I
characteristic	O
length-scale	B
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
84	O
covariance	B
functions	O
inﬁnitely	O
divisible	O
the	O
se	O
kernel	B
is	O
inﬁnitely	O
divisible	O
in	O
that	O
(	O
k	O
(	O
r	O
)	O
)	O
t	O
is	O
a	O
valid	O
kernel	B
for	O
all	O
t	O
>	O
0	O
;	O
the	O
eﬀect	O
of	O
raising	O
k	O
to	O
the	O
power	O
of	O
t	O
is	O
simply	O
to	O
rescale	O
‘	O
.	O
inﬁnite	O
network	O
construction	O
for	O
se	O
covariance	B
function	I
mat´ern	O
class	O
we	O
now	O
digress	O
brieﬂy	O
,	O
to	O
show	O
that	O
the	O
squared	B
exponential	I
covariance	O
function	B
can	O
also	O
be	O
obtained	O
by	O
expanding	O
the	O
input	O
x	O
into	O
a	O
feature	B
space	I
deﬁned	O
by	O
gaussian-shaped	O
basis	O
functions	O
centered	O
densely	O
in	O
x-space	O
.	O
for	O
simplicity	O
of	O
exposition	O
we	O
consider	O
scalar	O
inputs	O
with	O
basis	O
functions	O
φc	O
(	O
x	O
)	O
=	O
exp	O
(	O
cid:0	O
)	O
−	O
(	O
x	O
−	O
c	O
)	O
2	O
(	O
cid:1	O
)	O
,	O
2	O
‘	O
2	O
(	O
4.10	O
)	O
where	O
c	O
denotes	O
the	O
centre	O
of	O
the	O
basis	O
function	B
.	O
from	O
sections	O
2.1	O
and	O
2.2	O
we	O
recall	O
that	O
with	O
a	O
gaussian	O
prior	O
on	O
the	O
weights	O
w	O
∼	O
n	O
(	O
0	O
,	O
σ2	O
pi	O
)	O
,	O
this	O
gives	O
rise	O
to	O
a	O
gp	O
with	O
covariance	B
function	I
k	O
(	O
xp	O
,	O
xq	O
)	O
=	O
σ2	O
p	O
φc	O
(	O
xp	O
)	O
φc	O
(	O
xq	O
)	O
.	O
(	O
4.11	O
)	O
now	O
,	O
allowing	O
an	O
inﬁnite	O
number	O
of	O
basis	O
functions	O
centered	O
everywhere	O
on	O
an	O
interval	O
(	O
and	O
scaling	O
down	O
the	O
variance	O
of	O
the	O
prior	O
on	O
the	O
weights	O
with	O
the	O
number	O
of	O
basis	O
functions	O
)	O
we	O
obtain	O
the	O
limit	O
nx	O
c=1	O
z	O
cmax	O
cmin	O
nx	O
c=1	O
lim	O
n→∞	O
σ2	O
p	O
n	O
φc	O
(	O
xp	O
)	O
φc	O
(	O
xq	O
)	O
=	O
σ2	O
p	O
φc	O
(	O
xp	O
)	O
φc	O
(	O
xq	O
)	O
dc	O
.	O
(	O
4.12	O
)	O
plugging	O
in	O
the	O
gaussian-shaped	O
basis	O
functions	O
eq	O
.	O
(	O
4.10	O
)	O
and	O
letting	O
the	O
in-	O
tegration	O
limits	O
go	O
to	O
inﬁnity	O
we	O
obtain	O
z	O
∞	O
exp	O
(	O
cid:0	O
)	O
−	O
(	O
xp	O
−	O
c	O
)	O
2	O
p	O
exp	O
(	O
cid:0	O
)	O
−	O
(	O
xp	O
−	O
xq	O
)	O
2	O
2	O
‘	O
2	O
√	O
2	O
(	O
2	O
‘	O
)	O
2	O
−∞	O
π	O
‘	O
σ2	O
(	O
cid:1	O
)	O
dc	O
(	O
cid:1	O
)	O
exp	O
(	O
cid:0	O
)	O
−	O
(	O
xq	O
−	O
c	O
)	O
2	O
(	O
cid:1	O
)	O
,	O
2	O
‘	O
2	O
k	O
(	O
xp	O
,	O
xq	O
)	O
=	O
σ2	O
p	O
√	O
=	O
√	O
which	O
we	O
recognize	O
as	O
a	O
squared	B
exponential	I
covariance	O
function	B
with	O
a	O
2	O
times	O
longer	O
length-scale	B
.	O
the	O
derivation	O
is	O
adapted	O
from	O
mackay	O
[	O
1998	O
]	O
.	O
it	O
is	O
straightforward	O
to	O
generalize	O
this	O
construction	O
to	O
multivariate	O
x.	O
see	B
also	O
eq	O
.	O
(	O
4.30	O
)	O
for	O
a	O
similar	O
construction	O
where	O
the	O
centres	O
of	O
the	O
basis	O
functions	O
are	O
sampled	O
from	O
a	O
gaussian	O
distribution	O
;	O
the	O
constructions	O
are	O
equivalent	B
when	O
the	O
variance	O
of	O
this	O
gaussian	O
tends	O
to	O
inﬁnity	O
.	O
(	O
4.13	O
)	O
the	O
mat´ern	O
class	O
of	O
covariance	B
functions	O
the	O
mat´ern	O
class	O
of	O
covariance	B
functions	O
is	O
given	O
by	O
kmatern	O
(	O
r	O
)	O
=	O
21−ν	O
γ	O
(	O
ν	O
)	O
2νr	O
‘	O
kν	O
(	O
cid:16	O
)	O
√	O
(	O
cid:17	O
)	O
ν	O
(	O
cid:16	O
)	O
√	O
(	O
cid:17	O
)	O
,	O
2νr	O
‘	O
(	O
4.14	O
)	O
with	O
positive	O
parameters	O
ν	O
and	O
‘	O
,	O
where	O
kν	O
is	O
a	O
modiﬁed	O
bessel	O
function	B
[	O
abramowitz	O
and	O
stegun	O
,	O
1965	O
,	O
sec	O
.	O
9.6	O
]	O
.	O
this	O
covariance	B
function	I
has	O
a	O
spectral	O
density	O
(	O
cid:16	O
)	O
2ν	O
‘	O
2	O
+	O
4π2s2	O
(	O
cid:17	O
)	O
−	O
(	O
ν+d/2	O
)	O
(	O
4.15	O
)	O
s	O
(	O
s	O
)	O
=	O
2dπd/2γ	O
(	O
ν	O
+	O
d/2	O
)	O
(	O
2ν	O
)	O
ν	O
γ	O
(	O
ν	O
)	O
‘	O
2ν	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
4.2	O
examples	O
of	O
covariance	B
functions	O
85	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
4.1	O
:	O
panel	O
(	O
a	O
)	O
:	O
covariance	B
functions	O
,	O
and	O
(	O
b	O
)	O
:	O
random	O
functions	O
drawn	O
from	O
gaussian	O
processes	O
with	O
mat´ern	O
covariance	B
functions	O
,	O
eq	O
.	O
(	O
4.14	O
)	O
,	O
for	O
diﬀerent	O
values	O
of	O
ν	O
,	O
with	O
‘	O
=	O
1.	O
the	O
sample	O
functions	O
on	O
the	O
right	O
were	O
obtained	O
using	O
a	O
discretization	O
of	O
the	O
x-axis	O
of	O
2000	O
equally-spaced	O
points	O
.	O
in	O
d	O
dimensions	O
.	O
note	O
that	O
the	O
scaling	O
is	O
chosen	O
so	O
that	O
for	O
ν	O
→	O
∞	O
we	O
obtain	O
the	O
se	O
covariance	B
function	I
e−r2/2	O
‘	O
2	O
,	O
see	B
eq	O
.	O
(	O
a.25	O
)	O
.	O
stein	O
[	O
1999	O
]	O
named	O
this	O
the	O
mat´ern	O
class	O
after	O
the	O
work	O
of	O
mat´ern	O
[	O
1960	O
]	O
.	O
for	O
the	O
mat´ern	O
class	O
the	O
process	B
f	O
(	O
x	O
)	O
is	O
k-times	O
ms	O
diﬀerentiable	O
if	O
and	O
only	O
if	O
ν	O
>	O
k.	O
the	O
mat´ern	O
covariance	B
functions	O
become	O
especially	O
simple	O
when	O
ν	O
is	O
half-integer	O
:	O
ν	O
=	O
p	O
+	O
1/2	O
,	O
where	O
p	O
is	O
a	O
non-negative	O
integer	O
.	O
in	O
this	O
case	O
the	O
covariance	B
function	I
is	O
a	O
product	O
of	O
an	O
exponential	B
and	O
a	O
polynomial	B
of	O
order	O
p	O
,	O
the	O
general	O
expression	O
can	O
be	O
derived	O
from	O
[	O
abramowitz	O
and	O
stegun	O
,	O
1965	O
,	O
eq	O
.	O
10.2.15	O
]	O
,	O
giving	O
(	O
cid:16	O
)	O
−	O
√	O
(	O
cid:17	O
)	O
γ	O
(	O
p	O
+	O
1	O
)	O
px	O
γ	O
(	O
2p	O
+	O
1	O
)	O
i=0	O
2νr	O
‘	O
(	O
cid:16	O
)	O
√	O
(	O
cid:17	O
)	O
p−i	O
(	O
p	O
+	O
i	O
)	O
!	O
i	O
!	O
(	O
p	O
−	O
i	O
)	O
!	O
8νr	O
‘	O
.	O
(	O
4.16	O
)	O
kν=p+1/2	O
(	O
r	O
)	O
=	O
exp	O
it	O
is	O
possible	O
that	O
the	O
most	O
interesting	O
cases	O
for	O
machine	O
learning	B
are	O
ν	O
=	O
3/2	O
and	O
ν	O
=	O
5/2	O
,	O
for	O
which	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
√	O
3r	O
√	O
‘	O
5r	O
‘	O
exp	O
+	O
5r2	O
3	O
‘	O
2	O
(	O
cid:16	O
)	O
−	O
(	O
cid:17	O
)	O
√	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
−	O
3r	O
‘	O
exp	O
kν=3/2	O
(	O
r	O
)	O
=	O
kν=5/2	O
(	O
r	O
)	O
=	O
1	O
+	O
1	O
+	O
(	O
cid:17	O
)	O
,	O
(	O
4.17	O
)	O
,	O
√	O
5r	O
‘	O
since	O
for	O
ν	O
=	O
1/2	O
the	O
process	B
becomes	O
very	O
rough	O
(	O
see	B
below	O
)	O
,	O
and	O
for	O
ν	O
≥	O
7/2	O
,	O
in	O
the	O
absence	O
of	O
explicit	O
prior	O
knowledge	O
about	O
the	O
existence	O
of	O
higher	O
order	O
derivatives	O
,	O
it	O
is	O
probably	O
very	O
hard	O
from	O
ﬁnite	O
noisy	O
training	O
examples	O
to	O
distinguish	O
between	O
values	O
of	O
ν	O
≥	O
7/2	O
(	O
or	O
even	O
to	O
distinguish	O
between	O
ﬁnite	O
values	O
of	O
ν	O
and	O
ν	O
→	O
∞	O
,	O
the	O
smooth	O
squared	B
exponential	I
,	O
in	O
this	O
case	O
)	O
.	O
for	O
example	O
a	O
value	O
of	O
ν	O
=	O
5/2	O
was	O
used	O
in	O
[	O
cornford	O
et	O
al.	O
,	O
2002	O
]	O
.	O
ornstein-uhlenbeck	O
process	B
and	O
exponential	B
covariance	O
function	B
the	O
special	O
case	O
obtained	O
by	O
setting	O
ν	O
=	O
1/2	O
in	O
the	O
mat´ern	O
class	O
gives	O
the	O
exponential	B
covariance	O
function	B
k	O
(	O
r	O
)	O
=	O
exp	O
(	O
−r/	O
‘	O
)	O
.	O
the	O
corresponding	O
process	B
exponential	O
012300.20.40.60.81input	O
distance	O
,	O
rcovariance	O
,	O
k	O
(	O
r	O
)	O
ν=1/2ν=2ν→∞−505−202input	O
,	O
xoutput	O
,	O
f	O
(	O
x	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
86	O
covariance	B
functions	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
4.2	O
:	O
panel	O
(	O
a	O
)	O
covariance	B
functions	O
,	O
and	O
(	O
b	O
)	O
random	O
functions	O
drawn	O
from	O
gaussian	O
processes	O
with	O
the	O
γ-exponential	B
covariance	O
function	B
eq	O
.	O
(	O
4.18	O
)	O
,	O
for	O
diﬀerent	O
values	O
of	O
γ	O
,	O
with	O
‘	O
=	O
1.	O
the	O
sample	O
functions	O
are	O
only	O
diﬀerentiable	O
when	O
γ	O
=	O
2	O
(	O
the	O
se	O
case	O
)	O
.	O
the	O
sample	O
functions	O
on	O
the	O
right	O
were	O
obtained	O
using	O
a	O
discretization	O
of	O
the	O
x-axis	O
of	O
2000	O
equally-spaced	O
points	O
.	O
is	O
ms	O
continuous	O
but	O
not	O
ms	O
diﬀerentiable	O
.	O
in	O
d	O
=	O
1	O
this	O
is	O
the	O
covariance	B
function	I
of	O
the	O
ornstein-uhlenbeck	O
(	O
ou	O
)	O
process	B
.	O
the	O
ou	O
process	B
[	O
uhlenbeck	O
and	O
ornstein	O
,	O
1930	O
]	O
was	O
introduced	O
as	O
a	O
mathematical	O
model	B
of	O
the	O
velocity	O
of	O
a	O
particle	O
undergoing	O
brownian	O
motion	O
.	O
more	O
generally	O
in	O
d	O
=	O
1	O
setting	O
ν	O
+	O
1/2	O
=	O
p	O
for	O
integer	O
p	O
gives	O
rise	O
to	O
a	O
particular	O
form	O
of	O
a	O
continuous-time	O
ar	O
(	O
p	O
)	O
gaussian	O
process	B
;	O
for	O
further	O
details	O
see	B
section	O
b.2.1	O
.	O
the	O
form	O
of	O
the	O
mat´ern	O
covariance	B
function	I
and	O
samples	O
drawn	O
from	O
it	O
for	O
ν	O
=	O
1/2	O
,	O
ν	O
=	O
2	O
and	O
ν	O
→	O
∞	O
are	O
illustrated	O
in	O
figure	O
4.1.	O
the	O
γ-exponential	B
covariance	O
function	B
the	O
γ-exponential	B
family	O
of	O
covariance	B
functions	O
,	O
which	O
includes	O
both	O
the	O
ex-	O
ponential	B
and	O
squared	B
exponential	I
,	O
is	O
given	O
by	O
k	O
(	O
r	O
)	O
=	O
exp	O
(	O
cid:0	O
)	O
−	O
(	O
r/	O
‘	O
)	O
γ	O
(	O
cid:1	O
)	O
for	O
0	O
<	O
γ	O
≤	O
2	O
.	O
(	O
4.18	O
)	O
although	O
this	O
function	B
has	O
a	O
similar	O
number	O
of	O
parameters	O
to	O
the	O
mat´ern	O
class	O
,	O
it	O
is	O
(	O
as	O
stein	O
[	O
1999	O
]	O
notes	O
)	O
in	O
a	O
sense	O
less	O
ﬂexible	O
.	O
this	O
is	O
because	O
the	O
corre-	O
sponding	O
process	B
is	O
not	O
ms	O
diﬀerentiable	O
except	O
when	O
γ	O
=	O
2	O
(	O
when	O
it	O
is	O
in-	O
ﬁnitely	O
ms	O
diﬀerentiable	O
)	O
.	O
the	O
covariance	B
function	I
and	O
random	O
samples	O
from	O
the	O
process	B
are	O
shown	O
in	O
figure	O
4.2.	O
a	O
proof	O
of	O
the	O
positive	O
deﬁniteness	O
of	O
this	O
covariance	B
function	I
can	O
be	O
found	O
in	O
schoenberg	O
[	O
1938	O
]	O
.	O
rational	B
quadratic	I
covariance	O
function	B
the	O
rational	B
quadratic	I
(	O
rq	O
)	O
covariance	B
function	I
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
−α	O
krq	O
(	O
r	O
)	O
=	O
1	O
+	O
r2	O
2α	O
‘	O
2	O
(	O
4.19	O
)	O
ornstein-uhlenbeck	O
process	B
γ-exponential	O
rational	B
quadratic	I
012300.20.40.60.81input	O
distancecovarianceγ=1γ=1.5γ=2−505−3−2−10123input	O
,	O
xoutput	O
,	O
f	O
(	O
x	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
4.2	O
examples	O
of	O
covariance	B
functions	O
87	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
4.3	O
:	O
panel	O
(	O
a	O
)	O
covariance	B
functions	O
,	O
and	O
(	O
b	O
)	O
random	O
functions	O
drawn	O
from	O
gaussian	O
processes	O
with	O
rational	B
quadratic	I
covariance	O
functions	O
,	O
eq	O
.	O
(	O
4.20	O
)	O
,	O
for	O
diﬀer-	O
ent	O
values	O
of	O
α	O
with	O
‘	O
=	O
1.	O
the	O
sample	O
functions	O
on	O
the	O
right	O
were	O
obtained	O
using	O
a	O
discretization	O
of	O
the	O
x-axis	O
of	O
2000	O
equally-spaced	O
points	O
.	O
with	O
α	O
,	O
‘	O
>	O
0	O
can	O
be	O
seen	O
as	O
a	O
scale	B
mixture	I
(	O
an	O
inﬁnite	O
sum	O
)	O
of	O
squared	B
exponential	I
(	O
se	O
)	O
covariance	B
functions	O
with	O
diﬀerent	O
characteristic	O
length-scales	O
(	O
sums	O
of	O
covariance	B
functions	O
are	O
also	O
a	O
valid	O
covariance	B
,	O
see	B
section	O
4.2.4	O
)	O
.	O
parameterizing	O
now	O
in	O
terms	O
of	O
inverse	O
squared	B
length	O
scales	O
,	O
τ	O
=	O
‘	O
−2	O
,	O
and	O
putting	O
a	O
gamma	B
distribution	I
on	O
p	O
(	O
τ|α	O
,	O
β	O
)	O
∝	O
τ	O
α−1	O
exp	O
(	O
−ατ	O
/β	O
)	O
,5	O
we	O
can	O
add	O
up	O
the	O
contributions	O
through	O
the	O
following	O
integral	O
p	O
(	O
τ|α	O
,	O
β	O
)	O
kse	O
(	O
r|τ	O
)	O
dτ	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
−	O
ατ	O
β	O
(	O
cid:16	O
)	O
−	O
τ	O
r2	O
2	O
(	O
cid:17	O
)	O
dτ	O
∝	O
(	O
cid:16	O
)	O
1	O
+	O
r2	O
2α	O
‘	O
2	O
(	O
cid:17	O
)	O
−α	O
,	O
(	O
4.20	O
)	O
exp	O
z	O
z	O
krq	O
(	O
r	O
)	O
=	O
∝	O
τ	O
α−1	O
exp	O
scale	B
mixture	I
where	O
we	O
have	O
set	B
β−1	O
=	O
‘	O
2	O
.	O
the	O
rational	B
quadratic	I
is	O
also	O
discussed	O
by	O
mat´ern	O
[	O
1960	O
,	O
p.	O
17	O
]	O
using	O
a	O
slightly	O
diﬀerent	O
parameterization	O
;	O
in	O
our	O
notation	O
the	O
limit	O
of	O
the	O
rq	O
covariance	B
for	O
α	O
→	O
∞	O
(	O
see	B
eq	O
.	O
(	O
a.25	O
)	O
)	O
is	O
the	O
se	O
covariance	B
function	I
with	O
characteristic	O
length-scale	B
‘	O
,	O
eq	O
.	O
(	O
4.9	O
)	O
.	O
figure	O
4.3	O
illustrates	O
the	O
behaviour	O
for	O
diﬀerent	O
values	O
of	O
α	O
;	O
note	O
that	O
the	O
process	B
is	O
inﬁnitely	O
ms	O
diﬀerentiable	O
for	O
every	O
α	O
in	O
contrast	O
to	O
the	O
mat´ern	O
covariance	B
function	I
in	O
figure	O
4.1.	O
the	O
previous	O
example	O
is	O
a	O
special	O
case	O
of	O
kernels	O
which	O
can	O
be	O
written	O
as	O
superpositions	O
of	O
se	O
kernels	O
with	O
a	O
distribution	O
p	O
(	O
‘	O
)	O
of	O
length-scales	O
‘	O
,	O
k	O
(	O
r	O
)	O
=	O
r	O
exp	O
(	O
−r2/2	O
‘	O
2	O
)	O
p	O
(	O
‘	O
)	O
d	O
‘	O
.	O
this	O
is	O
in	O
fact	O
the	O
most	O
general	O
representation	O
for	O
an	O
isotropic	O
kernel	B
which	O
deﬁnes	O
a	O
valid	O
covariance	B
function	I
in	O
any	O
dimension	O
d	O
,	O
see	B
[	O
stein	O
,	O
1999	O
,	O
sec	O
.	O
2.10	O
]	O
.	O
piecewise	B
polynomial	O
covariance	B
functions	O
with	O
compact	B
support	I
a	O
family	O
of	O
piecewise	B
polynomial	O
functions	O
with	O
compact	B
support	I
provide	O
an-	O
other	O
interesting	O
class	O
of	O
covariance	B
functions	O
.	O
compact	B
support	I
means	O
that	O
5	O
note	O
that	O
there	O
are	O
several	O
common	O
ways	O
to	O
parameterize	O
the	O
gamma	O
distribution—our	O
choice	O
is	O
convenient	O
here	O
:	O
α	O
is	O
the	O
“	O
shape	O
”	O
and	O
β	O
is	O
the	O
mean	O
.	O
piecewise	B
polynomial	O
covariance	B
functions	O
with	O
compact	B
support	I
012300.20.40.60.81input	O
distancecovarianceα=1/2α=2α→∞−505−3−2−10123input	O
,	O
xoutput	O
,	O
f	O
(	O
x	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
88	O
covariance	B
functions	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
4.4	O
:	O
panel	O
(	O
a	O
)	O
:	O
covariance	B
functions	O
,	O
and	O
(	O
b	O
)	O
:	O
random	O
functions	O
drawn	O
from	O
gaussian	O
processes	O
with	O
piecewise	B
polynomial	O
covariance	B
functions	O
with	O
compact	O
sup-	O
port	O
from	O
eq	O
.	O
(	O
4.21	O
)	O
,	O
with	O
speciﬁed	O
parameters	O
.	O
positive	O
deﬁniteness	O
restricted	O
dimension	O
the	O
covariance	B
between	O
points	O
become	O
exactly	O
zero	O
when	O
their	O
distance	O
exceeds	O
a	O
certain	O
threshold	O
.	O
this	O
means	O
that	O
the	O
covariance	B
matrix	I
will	O
become	O
sparse	O
by	O
construction	O
,	O
leading	O
to	O
the	O
possibility	O
of	O
computational	O
advantages.6	O
the	O
challenge	O
in	O
designing	O
these	O
functions	O
is	O
how	O
to	O
guarantee	O
positive	O
deﬁnite-	O
ness	O
.	O
multiple	O
algorithms	O
for	O
deriving	O
such	O
covariance	B
functions	O
are	O
discussed	O
by	O
wendland	O
[	O
2005	O
,	O
ch	O
.	O
9	O
]	O
.	O
these	O
functions	O
are	O
usually	O
not	O
positive	B
deﬁnite	I
for	O
all	O
input	O
dimensions	O
,	O
but	O
their	O
validity	O
is	O
restricted	O
up	O
to	O
some	O
maximum	O
dimension	O
d.	O
below	O
we	O
give	O
examples	O
of	O
covariance	B
functions	O
kppd	O
,	O
q	O
(	O
r	O
)	O
which	O
are	O
positive	B
deﬁnite	I
in	O
rd	O
kppd,0	O
(	O
r	O
)	O
=	O
(	O
1	O
−	O
r	O
)	O
j	O
+	O
,	O
kppd,1	O
(	O
r	O
)	O
=	O
(	O
1	O
−	O
r	O
)	O
j+1	O
kppd,2	O
(	O
r	O
)	O
=	O
(	O
1	O
−	O
r	O
)	O
j+2	O
kppd,3	O
(	O
r	O
)	O
=	O
(	O
1	O
−	O
r	O
)	O
j+3	O
+	O
+	O
+	O
2	O
c	O
+	O
q	O
+	O
1	O
,	O
where	O
j	O
=	O
b	O
d	O
(	O
cid:0	O
)	O
(	O
j	O
+	O
1	O
)	O
r	O
+	O
1	O
(	O
cid:1	O
)	O
,	O
(	O
cid:0	O
)	O
(	O
j2	O
+	O
4j	O
+	O
3	O
)	O
r2	O
+	O
(	O
3j	O
+	O
6	O
)	O
r	O
+	O
3	O
(	O
cid:1	O
)	O
/3	O
,	O
(	O
cid:0	O
)	O
(	O
j3	O
+	O
9j2	O
+	O
23j	O
+	O
15	O
)	O
r3+	O
(	O
6j2	O
+	O
36j	O
+	O
45	O
)	O
r2	O
+	O
(	O
15j	O
+	O
45	O
)	O
r	O
+	O
15	O
(	O
cid:1	O
)	O
/15	O
.	O
(	O
4.21	O
)	O
the	O
properties	O
of	O
three	O
of	O
these	O
covariance	B
functions	O
are	O
illustrated	O
in	O
fig-	O
ure	O
4.4.	O
these	O
covariance	B
functions	O
are	O
2q-times	O
continuously	O
diﬀerentiable	O
,	O
and	O
thus	O
the	O
corresponding	O
processes	O
are	O
q-times	O
mean-square	O
diﬀerentiable	O
,	O
see	B
section	O
4.1.1.	O
it	O
is	O
interesting	O
to	O
ask	O
to	O
what	O
extent	O
one	O
could	O
use	O
the	O
compactly-supported	O
covariance	B
functions	O
described	O
above	O
in	O
place	O
of	O
the	O
other	O
covariance	B
functions	O
mentioned	O
in	O
this	O
section	O
,	O
while	O
obtaining	O
inferences	O
that	O
are	O
similar	O
.	O
one	O
advantage	O
of	O
the	O
compact	B
support	I
is	O
that	O
it	O
gives	O
rise	O
to	O
spar-	O
sity	O
of	O
the	O
gram	O
matrix	B
which	O
could	O
be	O
exploited	O
,	O
for	O
example	O
,	O
when	O
using	O
iterative	O
solutions	O
to	O
gpr	O
problem	O
,	O
see	B
section	O
8.3.6	O
.	O
6if	O
the	O
product	O
of	O
the	O
inverse	O
covariance	B
matrix	I
with	O
a	O
vector	O
(	O
needed	O
e.g	O
.	O
for	O
prediction	B
)	O
is	O
computed	O
using	O
a	O
conjugate	O
gradient	O
algorithm	O
,	O
then	O
products	O
of	O
the	O
covariance	B
matrix	I
with	O
vectors	O
are	O
the	O
basic	O
computational	O
unit	O
,	O
and	O
these	O
can	O
obviously	O
be	O
carried	O
out	O
much	O
faster	O
if	O
the	O
matrix	B
is	O
sparse	O
.	O
00.20.40.60.8100.20.40.60.81input	O
distance	O
,	O
rcovariance	O
,	O
k	O
(	O
r	O
)	O
d=1	O
,	O
q=1d=3	O
,	O
q=1d=1	O
,	O
q=2−2−1012−202input	O
,	O
xoutput	O
,	O
f	O
(	O
x	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
4.2	O
examples	O
of	O
covariance	B
functions	O
89	O
further	O
properties	O
of	O
stationary	O
covariance	B
functions	O
the	O
covariance	B
functions	O
given	O
above	O
decay	O
monotonically	O
with	O
r	O
and	O
are	O
always	O
positive	O
.	O
however	O
,	O
this	O
is	O
not	O
a	O
necessary	O
condition	O
for	O
a	O
covariance	B
function	I
.	O
for	O
example	O
yaglom	O
[	O
1987	O
]	O
shows	O
that	O
k	O
(	O
r	O
)	O
=	O
c	O
(	O
αr	O
)	O
−νjν	O
(	O
αr	O
)	O
is	O
a	O
valid	O
covari-	O
ance	O
function	B
for	O
ν	O
≥	O
(	O
d	O
−	O
2	O
)	O
/2	O
and	O
α	O
>	O
0	O
;	O
this	O
function	B
has	O
the	O
form	O
of	O
a	O
damped	O
oscillation	O
.	O
anisotropic	O
versions	O
of	O
these	O
isotropic	O
covariance	B
functions	O
can	O
be	O
created	O
by	O
setting	O
r2	O
(	O
x	O
,	O
x0	O
)	O
=	O
(	O
x	O
−	O
x0	O
)	O
>	O
m	O
(	O
x	O
−	O
x0	O
)	O
for	O
some	O
positive	B
semideﬁnite	I
m.	O
if	O
m	O
is	O
diagonal	O
this	O
implements	O
the	O
use	O
of	O
diﬀerent	O
length-scales	O
on	O
diﬀerent	O
dimensions—for	O
further	O
discussion	O
of	O
automatic	B
relevance	I
determination	O
see	B
section	O
5.1.	O
general	O
m	O
’	O
s	O
have	O
been	O
considered	O
by	O
mat´ern	O
[	O
1960	O
,	O
p.	O
19	O
]	O
,	O
poggio	O
and	O
girosi	O
[	O
1990	O
]	O
and	O
also	O
in	O
vivarelli	O
and	O
williams	O
[	O
1999	O
]	O
;	O
in	O
the	O
latter	O
work	O
a	O
low-rank	O
m	O
was	O
used	O
to	O
implement	O
a	O
linear	B
dimensionality	O
reduction	O
step	O
from	O
the	O
input	O
space	O
to	O
lower-dimensional	O
feature	B
space	I
.	O
more	O
generally	O
,	O
one	O
could	O
assume	O
the	O
form	O
(	O
4.22	O
)	O
where	O
λ	O
is	O
a	O
d	O
×	O
k	O
matrix	B
whose	O
columns	O
deﬁne	O
k	O
directions	O
of	O
high	O
relevance	O
,	O
and	O
ψ	O
is	O
a	O
diagonal	O
matrix	B
(	O
with	O
positive	O
entries	O
)	O
,	O
capturing	O
the	O
(	O
usual	O
)	O
axis-	O
aligned	O
relevances	O
,	O
see	B
also	O
figure	O
5.1	O
on	O
page	O
107.	O
thus	O
m	O
has	O
a	O
factor	B
analysis	I
form	O
.	O
for	O
appropriate	O
choices	O
of	O
k	O
this	O
may	O
represent	O
a	O
good	O
trade-oﬀ	O
between	O
ﬂexibility	O
and	O
required	O
number	O
of	O
parameters	O
.	O
m	O
=	O
λλ	O
>	O
+	O
ψ	O
k	O
(	O
x	O
)	O
,	O
the	O
kernel	B
kt	O
(	O
x	O
)	O
=p	O
stationary	O
kernels	O
can	O
also	O
be	O
deﬁned	O
on	O
a	O
periodic	B
domain	O
,	O
and	O
can	O
be	O
readily	O
constructed	O
from	O
stationary	O
kernels	O
on	O
r.	O
given	O
a	O
stationary	O
kernel	B
m∈z	O
k	O
(	O
x	O
+	O
ml	O
)	O
is	O
periodic	B
with	O
period	O
l	O
,	O
as	O
shown	O
in	O
section	O
b.2.2	O
and	O
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
,	O
eq	O
.	O
4.42	O
]	O
.	O
anisotropy	B
factor	O
analysis	O
distance	O
periodization	O
if	O
σ2	O
4.2.2	O
dot	B
product	I
covariance	O
functions	O
0	O
+	O
x	O
·	O
x0	O
can	O
as	O
we	O
have	O
already	O
mentioned	O
above	O
the	O
kernel	B
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
σ2	O
0	O
=	O
0	O
we	O
call	O
this	O
the	O
homogeneous	O
be	O
obtained	O
from	O
linear	B
regression	I
.	O
linear	B
kernel	O
,	O
otherwise	O
it	O
is	O
inhomogeneous	O
.	O
of	O
course	O
this	O
can	O
be	O
generalized	B
0	O
+	O
x	O
>	O
σpx0	O
by	O
using	O
a	O
general	O
covariance	B
matrix	I
σp	O
on	O
the	O
to	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
σ2	O
components	O
of	O
x	O
,	O
as	O
described	O
in	O
eq	O
.	O
(	O
2.4	O
)	O
.7	O
it	O
is	O
also	O
the	O
case	O
that	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
0	O
+	O
x	O
>	O
σpx0	O
)	O
p	O
is	O
a	O
valid	O
covariance	B
function	I
for	O
positive	O
integer	O
p	O
,	O
because	O
of	O
(	O
σ2	O
the	O
general	O
result	O
that	O
a	O
positive-integer	O
power	O
of	O
a	O
given	O
covariance	B
function	I
is	O
also	O
a	O
valid	O
covariance	B
function	I
,	O
as	O
described	O
in	O
section	O
4.2.4.	O
however	O
,	O
it	O
is	O
also	O
interesting	O
to	O
show	O
an	O
explicit	O
feature	B
space	I
construction	O
for	O
the	O
polynomial	B
covariance	O
function	B
.	O
we	O
consider	O
the	O
homogeneous	O
polynomial	B
case	O
as	O
the	O
inhomogeneous	O
case	O
can	O
simply	O
be	O
obtained	O
by	O
considering	O
x	O
to	O
be	O
extended	O
7indeed	O
the	O
bias	B
term	O
could	O
also	O
be	O
included	O
in	O
the	O
general	O
expression	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
90	O
covariance	B
functions	O
by	O
concatenating	O
a	O
constant	O
.	O
we	O
write	O
(	O
cid:16	O
)	O
dx	O
d=1	O
(	O
cid:17	O
)	O
p	O
(	O
cid:16	O
)	O
dx	O
d1=1	O
=	O
(	O
cid:17	O
)	O
···	O
(	O
cid:16	O
)	O
dx	O
dp=1	O
(	O
cid:17	O
)	O
xdpx0	O
dp	O
xd1x0	O
d1	O
xdx0	O
d	O
(	O
xd1	O
···	O
xdp	O
)	O
(	O
x0	O
d1	O
···	O
x0	O
dp	O
)	O
,	O
φ	O
(	O
x	O
)	O
·	O
φ	O
(	O
x0	O
)	O
.	O
(	O
4.23	O
)	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
(	O
x	O
·	O
x0	O
)	O
p	O
=	O
dx	O
···	O
dx	O
d1=1	O
dp=1	O
=	O
d	O
appears	O
in	O
the	O
monomial	O
,	O
under	O
the	O
constraint	O
that	O
pd	O
notice	O
that	O
this	O
sum	O
apparently	O
contains	O
dp	O
terms	O
but	O
in	O
fact	O
it	O
is	O
less	O
than	O
this	O
as	O
the	O
order	O
of	O
the	O
indices	O
in	O
the	O
monomial	O
xd1	O
···	O
xdp	O
is	O
unimportant	O
,	O
e.g	O
.	O
for	O
p	O
=	O
2	O
,	O
x1x2	O
and	O
x2x1	O
are	O
the	O
same	O
monomial	O
.	O
we	O
can	O
remove	O
the	O
redundancy	O
by	O
deﬁning	O
a	O
vector	O
m	O
whose	O
entry	O
md	O
speciﬁes	O
the	O
number	O
of	O
times	O
index	O
i=1	O
mi	O
=	O
p.	O
thus	O
φm	O
(	O
x	O
)	O
,	O
the	O
feature	O
corresponding	O
to	O
vector	O
m	O
is	O
proportional	O
to	O
the	O
monomial	O
m1	O
!	O
...	O
md	O
!	O
(	O
where	O
as	O
usual	O
we	O
deﬁne	O
xm1	O
1	O
0	O
!	O
=	O
1	O
)	O
,	O
giving	O
the	O
feature	O
map	O
d	O
.	O
the	O
degeneracy	O
of	O
φm	O
(	O
x	O
)	O
is	O
.	O
.	O
.	O
xmd	O
p	O
!	O
r	O
φm	O
(	O
x	O
)	O
=	O
p	O
!	O
m1	O
!	O
···	O
md	O
!	O
xm1	O
1	O
···	O
xmd	O
d	O
.	O
(	O
4.24	O
)	O
√	O
2x1x2	O
)	O
>	O
.	O
dot-	O
for	O
example	O
,	O
for	O
p	O
=	O
2	O
in	O
d	O
=	O
2	O
,	O
we	O
have	O
φ	O
(	O
x	O
)	O
=	O
(	O
x2	O
product	O
kernels	O
are	O
sometimes	O
used	O
in	O
a	O
normalized	O
form	O
given	O
by	O
eq	O
.	O
(	O
4.35	O
)	O
.	O
for	O
regression	B
problems	O
the	O
polynomial	B
kernel	O
is	O
a	O
rather	O
strange	O
choice	O
as	O
the	O
prior	O
variance	O
grows	O
rapidly	O
with	O
|x|	O
for	O
|x|	O
>	O
1.	O
however	O
,	O
such	O
kernels	O
have	O
proved	O
eﬀective	O
in	O
high-dimensional	O
classiﬁcation	B
problems	O
(	O
e.g	O
.	O
take	O
x	O
to	O
be	O
a	O
vectorized	O
binary	B
image	O
)	O
where	O
the	O
input	O
data	O
are	O
binary	B
or	O
greyscale	O
normalized	O
to	O
[	O
−1	O
,	O
1	O
]	O
on	O
each	O
dimension	O
[	O
sch¨olkopf	O
and	O
smola	O
,	O
2002	O
,	O
sec	O
.	O
7.8	O
]	O
.	O
1	O
,	O
x2	O
2	O
,	O
4.2.3	O
other	O
non-stationary	O
covariance	B
functions	O
above	O
we	O
have	O
seen	O
examples	O
of	O
non-stationary	O
dot	B
product	I
kernels	O
.	O
however	O
,	O
there	O
are	O
also	O
other	O
interesting	O
kernels	O
which	O
are	O
not	O
of	O
this	O
form	O
.	O
in	O
this	O
section	O
we	O
ﬁrst	O
describe	O
the	O
covariance	B
function	I
belonging	O
to	O
a	O
particular	O
type	O
of	O
neural	B
network	I
;	O
this	O
construction	O
is	O
due	O
to	O
neal	O
[	O
1996	O
]	O
.	O
consider	O
a	O
network	O
which	O
takes	O
an	O
input	O
x	O
,	O
has	O
one	O
hidden	O
layer	O
with	O
nh	O
units	O
and	O
then	O
linearly	O
combines	O
the	O
outputs	B
of	O
the	O
hidden	O
units	O
with	O
a	O
bias	B
b	O
to	O
obtain	O
f	O
(	O
x	O
)	O
.	O
the	O
mapping	O
can	O
be	O
written	O
f	O
(	O
x	O
)	O
=	O
b	O
+	O
vjh	O
(	O
x	O
;	O
uj	O
)	O
,	O
(	O
4.25	O
)	O
j=1	O
where	O
the	O
vjs	O
are	O
the	O
hidden-to-output	O
weights	O
and	O
h	O
(	O
x	O
;	O
u	O
)	O
is	O
the	O
hidden	O
unit	O
transfer	O
function	B
(	O
which	O
we	O
shall	O
assume	O
is	O
bounded	O
)	O
which	O
depends	O
on	O
the	O
input-to-hidden	O
weights	O
u.	O
for	O
example	O
,	O
we	O
could	O
choose	O
h	O
(	O
x	O
;	O
u	O
)	O
=	O
tanh	O
(	O
x·	O
u	O
)	O
.	O
this	O
architecture	O
is	O
important	O
because	O
it	O
has	O
been	O
shown	O
by	O
hornik	O
[	O
1993	O
]	O
that	O
networks	O
with	O
one	O
hidden	O
layer	O
are	O
universal	O
approximators	O
as	O
the	O
number	O
of	O
nhx	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
4.2	O
examples	O
of	O
covariance	B
functions	O
91	O
hidden	O
units	O
tends	O
to	O
inﬁnity	O
,	O
for	O
a	O
wide	O
class	O
of	O
transfer	O
functions	O
(	O
but	O
exclud-	O
ing	O
polynomials	O
)	O
.	O
let	O
b	O
and	O
the	O
v	O
’	O
s	O
have	O
independent	O
zero-mean	O
distributions	O
v	O
,	O
respectively	O
,	O
and	O
let	O
the	O
weights	O
uj	O
for	O
each	O
hidden	O
unit	O
of	O
variance	O
σ2	O
be	O
independently	O
and	O
identically	O
distributed	O
.	O
denoting	O
all	O
weights	O
by	O
w	O
,	O
we	O
obtain	O
(	O
following	O
neal	O
[	O
1996	O
]	O
)	O
b	O
and	O
σ2	O
ew	O
[	O
f	O
(	O
x	O
)	O
]	O
=	O
0	O
ew	O
[	O
f	O
(	O
x	O
)	O
f	O
(	O
x0	O
)	O
]	O
=	O
σ2	O
b	O
+x	O
eu	O
[	O
h	O
(	O
x	O
;	O
uj	O
)	O
h	O
(	O
x0	O
;	O
uj	O
)	O
]	O
σ2	O
v	O
j	O
=	O
σ2	O
b	O
+	O
nh	O
σ2	O
v	O
eu	O
[	O
h	O
(	O
x	O
;	O
u	O
)	O
h	O
(	O
x0	O
;	O
u	O
)	O
]	O
,	O
(	O
4.26	O
)	O
(	O
4.27	O
)	O
(	O
4.28	O
)	O
where	O
eq	O
.	O
(	O
4.28	O
)	O
follows	O
because	O
all	O
of	O
the	O
hidden	O
units	O
are	O
identically	O
dis-	O
tributed	O
.	O
the	O
ﬁnal	O
term	O
in	O
equation	O
4.28	O
becomes	O
ω2eu	O
[	O
h	O
(	O
x	O
;	O
u	O
)	O
h	O
(	O
x0	O
;	O
u	O
)	O
]	O
by	O
letting	O
σ2	O
v	O
scale	O
as	O
ω2/nh	O
.	O
the	O
sum	O
in	O
eq	O
.	O
(	O
4.27	O
)	O
is	O
over	O
nh	O
identically	O
and	O
independently	O
distributed	O
random	O
variables	O
.	O
as	O
the	O
transfer	O
function	B
is	O
bounded	O
,	O
all	O
moments	O
of	O
the	O
distribution	O
will	O
be	O
bounded	O
and	O
hence	O
the	O
central	O
limit	O
theorem	O
can	O
be	O
applied	O
,	O
showing	O
that	O
the	O
stochastic	O
process	O
will	O
converge	O
to	O
a	O
gaussian	O
process	B
in	O
the	O
limit	O
as	O
nh	O
→	O
∞	O
.	O
πr	O
z	O
by	O
evaluating	O
eu	O
[	O
h	O
(	O
x	O
;	O
u	O
)	O
h	O
(	O
x0	O
;	O
u	O
)	O
]	O
we	O
can	O
obtain	O
the	O
covariance	B
function	I
of	O
the	O
neural	B
network	I
.	O
for	O
example	O
if	O
we	O
choose	O
the	O
error	B
function	I
h	O
(	O
z	O
)	O
=	O
erf	O
(	O
z	O
)	O
=	O
√	O
j=1ujxj	O
)	O
and	O
2/	O
choose	O
u	O
∼	O
n	O
(	O
0	O
,	O
σ	O
)	O
then	O
we	O
obtain	O
[	O
williams	O
,	O
1998	O
]	O
2˜x	O
>	O
σ˜x0	O
dt	O
as	O
the	O
transfer	O
function	B
,	O
let	O
h	O
(	O
x	O
;	O
u	O
)	O
=	O
erf	O
(	O
u0	O
+pd	O
(	O
cid:17	O
)	O
0	O
e−t2	O
p	O
(	O
1	O
+	O
2˜x	O
>	O
σ˜x	O
)	O
(	O
1	O
+	O
2˜x0	O
>	O
σ˜x0	O
)	O
,	O
sin−1	O
(	O
cid:16	O
)	O
2	O
π	O
knn	O
(	O
x	O
,	O
x0	O
)	O
=	O
(	O
4.29	O
)	O
where	O
˜x	O
=	O
(	O
1	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xd	O
)	O
>	O
is	O
an	O
augmented	O
input	O
vector	O
.	O
this	O
is	O
a	O
true	O
“	O
neural	B
network	I
”	O
covariance	B
function	I
.	O
the	O
“	O
sigmoid	O
”	O
kernel	B
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
tanh	O
(	O
a	O
+	O
bx·	O
x0	O
)	O
has	O
sometimes	O
been	O
proposed	O
,	O
but	O
in	O
fact	O
this	O
kernel	B
is	O
never	O
positive	O
deﬁ-	O
nite	O
and	O
is	O
thus	O
not	O
a	O
valid	O
covariance	B
function	I
,	O
see	B
,	O
e.g	O
.	O
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
,	O
p.	O
113	O
]	O
.	O
figure	O
4.5	O
shows	O
a	O
plot	O
of	O
the	O
neural	B
network	I
covariance	O
function	B
and	O
samples	O
from	O
the	O
prior	O
.	O
we	O
have	O
set	B
σ	O
=	O
diag	O
(	O
σ2	O
0	O
,	O
σ2	O
)	O
.	O
samples	O
from	O
a	O
gp	O
with	O
this	O
covariance	B
function	I
can	O
be	O
viewed	O
as	O
superpositions	O
of	O
the	O
functions	O
erf	O
(	O
u0	O
+ux	O
)	O
,	O
where	O
σ2	O
0	O
controls	O
the	O
variance	O
of	O
u0	O
(	O
and	O
thus	O
the	O
amount	O
of	O
oﬀset	O
of	O
these	O
functions	O
from	O
the	O
origin	O
)	O
,	O
and	O
σ2	O
controls	O
u	O
and	O
thus	O
the	O
scaling	O
on	O
the	O
x-axis	O
.	O
in	O
figure	O
4.5	O
(	O
b	O
)	O
we	O
observe	O
that	O
the	O
sample	O
functions	O
with	O
larger	O
σ	O
vary	O
more	O
quickly	O
.	O
notice	O
that	O
the	O
samples	O
display	O
the	O
non-stationarity	O
of	O
the	O
covariance	B
function	I
in	O
that	O
for	O
large	O
values	O
of	O
+x	O
or	O
−x	O
they	O
should	O
tend	O
to	O
a	O
constant	O
value	O
,	O
consistent	O
with	O
the	O
construction	O
as	O
a	O
superposition	O
of	O
sigmoid	O
functions	O
.	O
another	O
interesting	O
construction	O
is	O
to	O
set	B
h	O
(	O
x	O
;	O
u	O
)	O
=	O
exp	O
(	O
−|x	O
−	O
u|2/2σ2	O
g	O
)	O
,	O
where	O
σg	O
sets	O
the	O
scale	O
of	O
this	O
gaussian	O
basis	O
function	B
.	O
with	O
u	O
∼	O
n	O
(	O
0	O
,	O
σ2	O
ui	O
)	O
neural	B
network	I
covariance	O
function	B
modulated	O
squared	B
exponential	I
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
92	O
covariance	B
functions	O
(	O
a	O
)	O
,	O
covariance	B
(	O
b	O
)	O
,	O
sample	O
functions	O
figure	O
4.5	O
:	O
panel	O
(	O
a	O
)	O
:	O
a	O
plot	O
of	O
the	O
covariance	B
function	I
knn	O
(	O
x	O
,	O
x0	O
)	O
for	O
σ0	O
=	O
10	O
,	O
σ	O
=	O
10.	O
panel	O
(	O
b	O
)	O
:	O
samples	O
drawn	O
from	O
the	O
neural	B
network	I
covariance	O
function	B
with	O
σ0	O
=	O
2	O
and	O
σ	O
as	O
shown	O
in	O
the	O
legend	O
.	O
the	O
samples	O
were	O
obtained	O
using	O
a	O
discretization	O
of	O
the	O
x-axis	O
of	O
500	O
equally-spaced	O
points	O
.	O
(	O
4.30	O
)	O
,	O
we	O
obtain	O
kg	O
(	O
x	O
,	O
x0	O
)	O
=	O
z	O
(	O
cid:16	O
)	O
−	O
x	O
>	O
x	O
(	O
cid:16	O
)	O
−	O
|x	O
−	O
u|2	O
(	O
cid:17	O
)	O
2σ2	O
g	O
exp	O
exp	O
−	O
|x0	O
−	O
u|2	O
(	O
cid:16	O
)	O
−	O
|x	O
−	O
x0|2	O
(	O
cid:17	O
)	O
2σ2	O
g	O
(	O
cid:17	O
)	O
−	O
u	O
>	O
u	O
(	O
cid:16	O
)	O
−	O
x0	O
>	O
x0	O
2σ2	O
u	O
du	O
(	O
cid:17	O
)	O
exp	O
2σ2	O
m	O
s	O
=	O
2σ2	O
2σ2	O
s	O
u	O
and	O
σ2	O
2σ2	O
m	O
u	O
+	O
σ2	O
1	O
(	O
2πσ2	O
u	O
)	O
d/2	O
(	O
cid:16	O
)	O
σe	O
(	O
cid:17	O
)	O
d	O
exp	O
=	O
σu	O
e	O
=	O
2/σ2	O
g/σ2	O
u	O
,	O
σ2	O
g	O
+	O
σ4	O
g	O
+	O
1/σ2	O
m	O
=	O
2σ2	O
where	O
1/σ2	O
g.	O
this	O
is	O
u	O
→	O
∞	O
(	O
while	O
scaling	O
in	O
general	O
a	O
non-stationary	O
covariance	B
function	I
,	O
but	O
if	O
σ2	O
ω2	O
appropriately	O
)	O
we	O
recover	O
the	O
squared	B
exponential	I
kg	O
(	O
x	O
,	O
x0	O
)	O
∝	O
exp	O
(	O
−|x	O
−	O
x0|2/4σ2	O
u	O
,	O
kg	O
(	O
x	O
,	O
x0	O
)	O
comprises	O
a	O
squared	B
exponen-	O
tial	B
covariance	O
function	B
modulated	O
by	O
the	O
gaussian	O
decay	O
envelope	O
function	B
exp	O
(	O
−x	O
>	O
x/2σ2	O
m	O
)	O
,	O
cf	O
.	O
the	O
vertical	O
rescaling	O
construction	O
de-	O
scribed	O
in	O
section	O
4.2.4.	O
g	O
)	O
.	O
for	O
a	O
ﬁnite	O
value	O
of	O
σ2	O
m	O
)	O
exp	O
(	O
−x0	O
>	O
x0/2σ2	O
one	O
way	O
to	O
introduce	O
non-stationarity	O
is	O
to	O
introduce	O
an	O
arbitrary	O
non-linear	O
mapping	O
(	O
or	O
warping	O
)	O
u	O
(	O
x	O
)	O
of	O
the	O
input	O
x	O
and	O
then	O
use	O
a	O
stationary	O
covariance	B
function	I
in	O
u-space	O
.	O
note	O
that	O
x	O
and	O
u	O
need	O
not	O
have	O
the	O
same	O
dimensionality	O
as	O
each	O
other	O
.	O
this	O
approach	O
was	O
used	O
by	O
sampson	O
and	O
guttorp	O
[	O
1992	O
]	O
to	O
model	B
patterns	O
of	O
solar	O
radiation	O
in	O
southwestern	O
british	O
columbia	O
using	O
gaussian	O
processes	O
.	O
another	O
interesting	O
example	O
of	O
this	O
warping	O
construction	O
is	O
given	O
in	O
mackay	O
[	O
1998	O
]	O
where	O
the	O
one-dimensional	O
input	O
variable	O
x	O
is	O
mapped	O
to	O
the	O
two-dimensional	O
u	O
(	O
x	O
)	O
=	O
(	O
cos	O
(	O
x	O
)	O
,	O
sin	O
(	O
x	O
)	O
)	O
to	O
give	O
rise	O
to	O
a	O
periodic	B
random	O
function	B
of	O
x.	O
if	O
we	O
use	O
the	O
squared	B
exponential	I
kernel	O
in	O
u-space	O
,	O
then	O
(	O
cid:16	O
)	O
−	O
2	O
sin2	O
(	O
cid:0	O
)	O
x−x0	O
2	O
(	O
cid:1	O
)	O
(	O
cid:17	O
)	O
‘	O
2	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
exp	O
as	O
(	O
cos	O
(	O
x	O
)	O
−	O
cos	O
(	O
x0	O
)	O
)	O
2	O
+	O
(	O
sin	O
(	O
x	O
)	O
−	O
sin	O
(	O
x0	O
)	O
)	O
2	O
=	O
4	O
sin2	O
(	O
x−x0	O
2	O
(	O
4.31	O
)	O
,	O
)	O
.	O
warping	O
periodic	B
random	O
function	B
−404−404input	O
,	O
xinput	O
,	O
x	O
’	O
−0.5−0.5000.50.50.950.95−404−101input	O
,	O
xoutput	O
,	O
f	O
(	O
x	O
)	O
σ	O
=	O
10σ	O
=	O
3σ	O
=	O
1	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
4.2	O
examples	O
of	O
covariance	B
functions	O
93	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
4.6	O
:	O
panel	O
(	O
a	O
)	O
shows	O
the	O
chosen	O
length-scale	B
function	O
‘	O
(	O
x	O
)	O
.	O
panel	O
(	O
b	O
)	O
shows	O
three	O
samples	O
from	O
the	O
gp	O
prior	O
using	O
gibbs	O
’	O
covariance	B
function	I
eq	O
.	O
(	O
4.32	O
)	O
.	O
this	O
ﬁgure	O
is	O
based	O
on	O
fig	O
.	O
3.9	O
in	O
gibbs	O
[	O
1997	O
]	O
.	O
we	O
have	O
described	O
above	O
how	O
to	O
make	O
an	O
anisotropic	O
covariance	B
function	I
by	O
scaling	O
diﬀerent	O
dimensions	O
diﬀerently	O
.	O
however	O
,	O
we	O
are	O
not	O
free	O
to	O
make	O
these	O
length-scales	O
‘	O
d	O
be	O
functions	O
of	O
x	O
,	O
as	O
this	O
will	O
not	O
in	O
general	O
produce	O
a	O
valid	O
covariance	B
function	I
.	O
gibbs	O
[	O
1997	O
]	O
derived	O
the	O
covariance	B
function	I
varying	O
length-scale	B
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
dy	O
(	O
cid:16	O
)	O
2	O
‘	O
d	O
(	O
x	O
)	O
‘	O
d	O
(	O
x0	O
)	O
(	O
cid:17	O
)	O
1/2	O
d	O
(	O
x	O
)	O
+	O
‘	O
2	O
‘	O
2	O
d	O
(	O
x0	O
)	O
d=1	O
(	O
cid:16	O
)	O
−	O
dx	O
d=1	O
exp	O
(	O
cid:17	O
)	O
(	O
xd	O
−	O
x0	O
d	O
(	O
x	O
)	O
+	O
‘	O
2	O
‘	O
2	O
d	O
)	O
2	O
d	O
(	O
x0	O
)	O
,	O
(	O
4.32	O
)	O
where	O
each	O
‘	O
i	O
(	O
x	O
)	O
is	O
an	O
arbitrary	O
positive	O
function	O
of	O
x.	O
note	O
that	O
k	O
(	O
x	O
,	O
x	O
)	O
=	O
1	O
for	O
all	O
x.	O
this	O
covariance	B
function	I
is	O
obtained	O
by	O
considering	O
a	O
grid	O
of	O
n	O
gaussian	O
basis	O
functions	O
with	O
centres	O
cj	O
and	O
a	O
corresponding	O
length-scale	B
on	O
input	O
dimension	O
d	O
which	O
varies	O
as	O
a	O
positive	O
function	O
‘	O
d	O
(	O
cj	O
)	O
.	O
taking	O
the	O
limit	O
as	O
n	O
→	O
∞	O
the	O
sum	O
turns	O
into	O
an	O
integral	O
and	O
after	O
some	O
algebra	O
eq	O
.	O
(	O
4.32	O
)	O
is	O
obtained	O
.	O
an	O
example	O
of	O
a	O
variable	O
length-scale	B
function	O
and	O
samples	O
from	O
the	O
prior	O
corresponding	O
to	O
eq	O
.	O
(	O
4.32	O
)	O
are	O
shown	O
in	O
figure	O
4.6.	O
notice	O
that	O
as	O
the	O
length-	O
scale	O
gets	O
shorter	O
the	O
sample	O
functions	O
vary	O
more	O
rapidly	O
as	O
one	O
would	O
expect	O
.	O
the	O
large	O
length-scale	B
regions	O
on	O
either	O
side	O
of	O
the	O
short	O
length-scale	B
region	O
can	O
be	O
quite	O
strongly	O
correlated	B
.	O
if	O
one	O
tries	O
the	O
converse	O
experiment	O
by	O
creating	O
a	O
length-scale	B
function	O
‘	O
(	O
x	O
)	O
which	O
has	O
a	O
longer	O
length-scale	B
region	O
between	O
two	O
shorter	O
ones	O
then	O
the	O
behaviour	O
may	O
not	O
be	O
quite	O
what	O
is	O
expected	O
;	O
on	O
initially	O
transitioning	O
into	O
the	O
long	O
length-scale	B
region	O
the	O
covariance	B
drops	O
oﬀ	O
quite	O
sharply	O
due	O
to	O
the	O
prefactor	O
in	O
eq	O
.	O
(	O
4.32	O
)	O
,	O
before	O
stabilizing	O
to	O
a	O
slower	O
variation	O
.	O
see	B
gibbs	O
[	O
1997	O
,	O
sec	O
.	O
3.10.3	O
]	O
for	O
further	O
details	O
.	O
exercises	O
4.5.4	O
and	O
4.5.5	O
invite	O
you	O
to	O
investigate	O
this	O
further	O
.	O
paciorek	O
and	O
schervish	O
[	O
2004	O
]	O
have	O
generalized	B
gibbs	O
’	O
construction	O
to	O
obtain	O
non-stationary	O
versions	O
of	O
arbitrary	O
isotropic	O
covariance	B
functions	O
.	O
let	O
ks	O
be	O
a	O
0123400.511.5lengthscale	O
l	O
(	O
x	O
)	O
input	O
,	O
x01234−101input	O
,	O
xoutput	O
,	O
f	O
(	O
x	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
94	O
covariance	B
functions	O
covariance	B
function	I
constant	O
linear	B
polynomial	O
squared	B
exponential	I
mat´ern	O
exponential	B
γ-exponential	O
rational	B
quadratic	I
neural	O
network	O
d	O
expression	O
σ2	O
0	O
pd	O
d=1	O
σ2	O
1	O
dxdx0	O
(	O
x	O
·	O
x0	O
+	O
σ2	O
0	O
)	O
p	O
exp	O
(	O
−	O
r2	O
2	O
‘	O
2	O
)	O
(	O
cid:16	O
)	O
√	O
‘	O
)	O
γ	O
(	O
cid:1	O
)	O
exp	O
(	O
cid:0	O
)	O
−	O
(	O
r	O
sin−1	O
(	O
cid:0	O
)	O
2α	O
‘	O
2	O
)	O
−α	O
(	O
1	O
+	O
r2	O
√	O
2ν−1γ	O
(	O
ν	O
)	O
exp	O
(	O
−	O
r	O
‘	O
)	O
(	O
cid:17	O
)	O
ν	O
(	O
cid:17	O
)	O
(	O
cid:16	O
)	O
√	O
2ν	O
‘	O
r	O
kν	O
2ν	O
‘	O
r	O
2˜x	O
>	O
σ˜x0	O
(	O
1+2˜x	O
>	O
σ˜x	O
)	O
(	O
1+2˜x0	O
>	O
σ˜x0	O
)	O
s	O
nd	O
√	O
√	O
√	O
√	O
√	O
√	O
√	O
√	O
√	O
√	O
√	O
√	O
(	O
cid:1	O
)	O
table	O
4.1	O
:	O
summary	O
of	O
several	O
commonly-used	O
covariance	B
functions	O
.	O
the	O
covariances	O
are	O
written	O
either	O
as	O
a	O
function	B
of	O
x	O
and	O
x0	O
,	O
or	O
as	O
a	O
function	B
of	O
r	O
=	O
|x	O
−	O
x0|	O
.	O
two	O
columns	O
marked	O
‘	O
s	O
’	O
and	O
‘	O
nd	O
’	O
indicate	O
whether	O
the	O
covariance	B
functions	O
are	O
stationary	O
and	O
nondegenerate	B
respectively	O
.	O
degenerate	B
covariance	O
functions	O
have	O
ﬁnite	O
rank	O
,	O
see	B
section	O
4.3	O
for	O
more	O
discussion	O
of	O
this	O
issue	O
.	O
stationary	O
,	O
isotropic	O
covariance	B
function	I
that	O
is	O
valid	O
in	O
every	O
euclidean	O
space	O
rd	O
for	O
d	O
=	O
1	O
,	O
2	O
,	O
.	O
.	O
..	O
let	O
σ	O
(	O
x	O
)	O
be	O
a	O
d	O
×	O
d	O
matrix-valued	O
function	B
which	O
is	O
positive	B
deﬁnite	I
for	O
all	O
x	O
,	O
and	O
let	O
σi	O
,	O
σ	O
(	O
xi	O
)	O
.	O
(	O
the	O
set	B
of	O
gibbs	O
’	O
‘	O
i	O
(	O
x	O
)	O
functions	O
deﬁne	O
a	O
diagonal	O
σ	O
(	O
x	O
)	O
.	O
)	O
then	O
deﬁne	O
the	O
quadratic	B
form	I
qij	O
=	O
(	O
xi	O
−	O
xj	O
)	O
>	O
(	O
(	O
σi	O
+	O
σj	O
)	O
/2	O
)	O
−1	O
(	O
xi	O
−	O
xj	O
)	O
.	O
paciorek	O
and	O
schervish	O
[	O
2004	O
]	O
show	O
that	O
kns	O
(	O
xi	O
,	O
xj	O
)	O
=	O
2d/2|σi|1/4|σj|1/4|σi	O
+	O
σj|−1/2ks	O
(	O
pqij	O
)	O
,	O
(	O
4.33	O
)	O
(	O
4.34	O
)	O
wiener	O
process	B
is	O
a	O
valid	O
non-stationary	O
covariance	B
function	I
.	O
in	O
chapter	O
2	O
we	O
described	O
the	O
linear	B
regression	I
model	O
in	O
feature	B
space	I
f	O
(	O
x	O
)	O
=	O
φ	O
(	O
x	O
)	O
>	O
w	O
.	O
o	O
’	O
hagan	O
[	O
1978	O
]	O
suggested	O
making	O
w	O
a	O
function	B
of	O
x	O
to	O
allow	O
for	O
diﬀerent	O
values	O
of	O
w	O
to	O
be	O
appropriate	O
in	O
diﬀerent	O
regions	O
.	O
thus	O
he	O
put	O
a	O
gaussian	O
process	B
prior	O
on	O
w	O
of	O
the	O
form	O
cov	O
(	O
w	O
(	O
x	O
)	O
,	O
w	O
(	O
x0	O
)	O
)	O
=	O
w0kw	O
(	O
x	O
,	O
x0	O
)	O
for	O
some	O
positive	B
deﬁnite	I
matrix	I
w0	O
,	O
giving	O
rise	O
to	O
a	O
prior	O
on	O
f	O
(	O
x	O
)	O
with	O
covariance	B
kf	O
(	O
x	O
,	O
x0	O
)	O
=	O
φ	O
(	O
x	O
)	O
>	O
w0φ	O
(	O
x0	O
)	O
kw	O
(	O
x	O
,	O
x0	O
)	O
.	O
finally	O
we	O
note	O
that	O
the	O
wiener	O
process	B
with	O
covariance	B
function	I
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
min	O
(	O
x	O
,	O
x0	O
)	O
is	O
a	O
fundamental	O
non-stationary	O
process	B
.	O
see	B
section	O
b.2.1	O
and	O
texts	O
such	O
as	O
grimmett	O
and	O
stirzaker	O
[	O
1992	O
,	O
ch	O
.	O
13	O
]	O
for	O
further	O
details	O
.	O
4.2.4	O
making	O
new	O
kernels	O
from	O
old	O
in	O
the	O
previous	O
sections	O
we	O
have	O
developed	O
many	O
covariance	B
functions	O
some	O
of	O
which	O
are	O
summarized	O
in	O
table	O
4.1.	O
in	O
this	O
section	O
we	O
show	O
how	O
to	O
combine	O
or	O
modify	O
existing	O
covariance	B
functions	O
to	O
make	O
new	O
ones	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
95	O
sum	O
product	O
vertical	O
rescaling	O
convolution	O
direct	O
sum	O
tensor	O
product	O
additive	O
model	B
functional	O
anova	O
4.2	O
examples	O
of	O
covariance	B
functions	O
the	O
sum	O
of	O
two	O
kernels	O
is	O
a	O
kernel	B
.	O
proof	O
:	O
consider	O
the	O
random	O
process	B
f	O
(	O
x	O
)	O
=	O
f1	O
(	O
x	O
)	O
+	O
f2	O
(	O
x	O
)	O
,	O
where	O
f1	O
(	O
x	O
)	O
and	O
f2	O
(	O
x	O
)	O
are	O
independent	O
.	O
then	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
k1	O
(	O
x	O
,	O
x0	O
)	O
+	O
k2	O
(	O
x	O
,	O
x0	O
)	O
.	O
this	O
construction	O
can	O
be	O
used	O
e.g	O
.	O
to	O
add	O
together	O
kernels	O
with	O
diﬀerent	O
characteristic	O
length-scales	O
.	O
the	O
product	O
of	O
two	O
kernels	O
is	O
a	O
kernel	B
.	O
proof	O
:	O
consider	O
the	O
random	O
process	B
f	O
(	O
x	O
)	O
=	O
f1	O
(	O
x	O
)	O
f2	O
(	O
x	O
)	O
,	O
where	O
f1	O
(	O
x	O
)	O
and	O
f2	O
(	O
x	O
)	O
are	O
independent	O
.	O
then	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
k1	O
(	O
x	O
,	O
x0	O
)	O
k2	O
(	O
x	O
,	O
x0	O
)	O
.8	O
a	O
simple	O
extension	O
of	O
this	O
argument	O
means	O
that	O
kp	O
(	O
x	O
,	O
x0	O
)	O
is	O
a	O
valid	O
covariance	B
function	I
for	O
p	O
∈	O
n.	O
let	O
a	O
(	O
x	O
)	O
be	O
a	O
given	O
deterministic	O
function	B
and	O
consider	O
g	O
(	O
x	O
)	O
=	O
a	O
(	O
x	O
)	O
f	O
(	O
x	O
)	O
where	O
f	O
(	O
x	O
)	O
is	O
a	O
random	O
process	B
.	O
then	O
cov	O
(	O
g	O
(	O
x	O
)	O
,	O
g	O
(	O
x0	O
)	O
)	O
=	O
a	O
(	O
x	O
)	O
k	O
(	O
x	O
,	O
x0	O
)	O
a	O
(	O
x0	O
)	O
.	O
such	O
a	O
construction	O
can	O
be	O
used	O
to	O
normalize	O
kernels	O
by	O
choosing	O
a	O
(	O
x	O
)	O
=	O
k−1/2	O
(	O
x	O
,	O
x	O
)	O
(	O
assuming	O
k	O
(	O
x	O
,	O
x	O
)	O
>	O
0	O
∀x	O
)	O
,	O
so	O
that	O
˜k	O
(	O
x	O
,	O
x0	O
)	O
=	O
pk	O
(	O
x	O
,	O
x	O
)	O
pk	O
(	O
x0	O
,	O
x0	O
)	O
k	O
(	O
x	O
,	O
x0	O
)	O
.	O
(	O
4.35	O
)	O
this	O
ensures	O
that	O
˜k	O
(	O
x	O
,	O
x	O
)	O
=	O
1	O
for	O
all	O
x.	O
we	O
can	O
also	O
obtain	O
a	O
new	O
process	B
by	O
convolution	O
(	O
or	O
blurring	O
)	O
.	O
consider	O
an	O
arbitrary	O
ﬁxed	O
kernel	B
h	O
(	O
x	O
,	O
z	O
)	O
and	O
the	O
map	O
g	O
(	O
x	O
)	O
=	O
r	O
h	O
(	O
x	O
,	O
z	O
)	O
f	O
(	O
z	O
)	O
dz	O
.	O
then	O
clearly	O
cov	O
(	O
g	O
(	O
x	O
)	O
,	O
g	O
(	O
x0	O
)	O
)	O
=r	O
h	O
(	O
x	O
,	O
z	O
)	O
k	O
(	O
z	O
,	O
z0	O
)	O
h	O
(	O
x0	O
,	O
z0	O
)	O
dz	O
dz0	O
.	O
i=1fi	O
(	O
xi	O
)	O
,	O
if	O
k	O
(	O
x1	O
,	O
x0	O
1	O
)	O
+	O
k2	O
(	O
x2	O
,	O
x0	O
i=1fi	O
(	O
xi	O
)	O
+p	O
shirani	O
,	O
1990	O
]	O
has	O
the	O
form	O
f	O
(	O
x	O
)	O
=	O
c	O
+pd	O
f	O
(	O
x	O
)	O
=	O
c	O
+pd	O
form	O
k	O
(	O
x	O
,	O
x0	O
)	O
=pd	O
2	O
)	O
are	O
covariance	B
functions	O
over	O
diﬀerent	O
spaces	O
x1	O
1	O
)	O
and	O
k	O
(	O
x2	O
,	O
x0	O
and	O
x2	O
,	O
then	O
the	O
direct	O
sum	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
k1	O
(	O
x1	O
,	O
x0	O
2	O
)	O
and	O
the	O
tensor	O
1	O
)	O
k2	O
(	O
x2	O
,	O
x0	O
product	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
k1	O
(	O
x1	O
,	O
x0	O
2	O
)	O
are	O
also	O
covariance	B
functions	O
(	O
deﬁned	O
on	O
the	O
product	O
space	O
x1×x2	O
)	O
,	O
by	O
virtue	O
of	O
the	O
sum	O
and	O
product	O
constructions	O
.	O
the	O
direct	O
sum	O
construction	O
can	O
be	O
further	O
generalized	B
.	O
consider	O
a	O
func-	O
tion	O
f	O
(	O
x	O
)	O
,	O
where	O
x	O
is	O
d-dimensional	O
.	O
an	O
additive	O
model	B
[	O
hastie	O
and	O
tib-	O
i.e	O
.	O
a	O
linear	B
combina-	O
tion	O
of	O
functions	O
of	O
one	O
variable	O
.	O
if	O
the	O
individual	O
fi	O
’	O
s	O
are	O
taken	O
to	O
be	O
in-	O
dependent	O
stochastic	O
processes	O
,	O
then	O
the	O
covariance	B
function	I
of	O
f	O
will	O
have	O
the	O
form	O
of	O
a	O
direct	O
sum	O
.	O
if	O
we	O
now	O
admit	O
interactions	O
of	O
two	O
variables	O
,	O
so	O
that	O
ij	O
,	O
j	O
<	O
i	O
fij	O
(	O
xi	O
,	O
xj	O
)	O
and	O
the	O
various	O
fi	O
’	O
s	O
and	O
fij	O
’	O
s	O
are	O
independent	O
stochastic	O
processes	O
,	O
then	O
the	O
covariance	B
function	I
will	O
have	O
the	O
j	O
)	O
.	O
indeed	O
this	O
pro-	O
cess	O
can	O
be	O
extended	O
further	O
to	O
provide	O
a	O
functional	B
anova9	O
decomposition	O
,	O
ranging	O
from	O
a	O
simple	O
additive	O
model	B
up	O
to	O
full	O
interaction	O
of	O
all	O
d	O
input	O
vari-	O
ables	O
.	O
(	O
the	O
sum	O
can	O
also	O
be	O
truncated	O
at	O
some	O
stage	O
.	O
)	O
wahba	O
[	O
1990	O
,	O
ch	O
.	O
10	O
]	O
and	O
stitson	O
et	O
al	O
.	O
[	O
1999	O
]	O
suggest	O
using	O
tensor	O
products	O
for	O
kernels	O
with	O
inter-	O
actions	O
so	O
that	O
in	O
the	O
example	O
above	O
kij	O
(	O
xi	O
,	O
xj	O
;	O
x0	O
j	O
)	O
would	O
have	O
the	O
form	O
ki	O
(	O
xi	O
;	O
x0	O
j	O
)	O
.	O
note	O
that	O
if	O
d	O
is	O
large	O
then	O
the	O
large	O
number	O
of	O
pairwise	O
(	O
or	O
higher-order	O
)	O
terms	O
may	O
be	O
problematic	O
;	O
plate	O
[	O
1999	O
]	O
has	O
investigated	O
using	O
a	O
combination	O
of	O
additive	O
gp	O
models	O
plus	O
a	O
general	O
covariance	B
function	I
that	O
permits	O
full	O
interactions	O
.	O
pi−1	O
j=1	O
kij	O
(	O
xi	O
,	O
xj	O
;	O
x0	O
i	O
)	O
+pd	O
i=1ki	O
(	O
xi	O
,	O
x0	O
i=2	O
i	O
)	O
kj	O
(	O
xj	O
;	O
x0	O
i	O
,	O
x0	O
i	O
,	O
x0	O
8if	O
f1	O
and	O
f2	O
are	O
gaussian	O
processes	O
then	O
the	O
product	O
f	O
will	O
not	O
in	O
general	O
be	O
a	O
gaussian	O
process	B
,	O
but	O
there	O
exists	O
a	O
gp	O
with	O
this	O
covariance	B
function	I
.	O
9anova	O
stands	O
for	O
analysis	O
of	O
variance	O
,	O
a	O
statistical	O
technique	O
that	O
analyzes	O
the	O
interac-	O
tions	O
between	O
various	O
attributes	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
96	O
covariance	B
functions	O
4.3	O
eigenfunction	B
analysis	O
of	O
kernels	O
we	O
ﬁrst	O
deﬁne	O
eigenvalues	O
and	O
eigenfunctions	O
and	O
discuss	O
mercer	O
’	O
s	O
theorem	O
which	O
allows	O
us	O
to	O
express	O
the	O
kernel	B
(	O
under	O
certain	O
conditions	O
)	O
in	O
terms	O
of	O
these	O
quantities	O
.	O
section	O
4.3.1	O
gives	O
the	O
analytical	O
solution	O
of	O
the	O
eigenproblem	O
for	O
the	O
se	O
kernel	B
under	O
a	O
gaussian	O
measure	B
.	O
section	O
4.3.2	O
discusses	O
how	O
to	O
compute	O
approximate	O
eigenfunctions	O
numerically	O
for	O
cases	O
where	O
the	O
exact	O
solution	O
is	O
not	O
known	O
.	O
it	O
turns	O
out	O
that	O
gaussian	O
process	B
regression	O
can	O
be	O
viewed	O
as	O
bayesian	O
linear	B
regression	I
with	O
a	O
possibly	O
inﬁnite	O
number	O
of	O
basis	O
functions	O
,	O
as	O
discussed	O
in	O
chapter	O
2.	O
one	O
possible	O
basis	O
set	B
is	O
the	O
eigenfunctions	O
of	O
the	O
covariance	B
function	I
.	O
a	O
function	B
φ	O
(	O
·	O
)	O
that	O
obeys	O
the	O
integral	O
equation	O
k	O
(	O
x	O
,	O
x0	O
)	O
φ	O
(	O
x	O
)	O
dµ	O
(	O
x	O
)	O
=	O
λφ	O
(	O
x0	O
)	O
,	O
(	O
4.36	O
)	O
z	O
eigenvalue	B
,	O
eigenfunction	B
mercer	O
’	O
s	O
theorem	O
is	O
called	O
an	O
eigenfunction	B
of	O
kernel	B
k	O
with	O
eigenvalue	B
λ	O
with	O
respect	O
to	O
measure10	O
µ.	O
the	O
two	O
measures	O
of	O
particular	O
interest	O
to	O
us	O
will	O
be	O
(	O
i	O
)	O
lebesgue	O
measure	B
over	O
a	O
compact	O
subset	O
c	O
of	O
rd	O
,	O
or	O
(	O
ii	O
)	O
when	O
there	O
is	O
a	O
density	O
p	O
(	O
x	O
)	O
so	O
that	O
dµ	O
(	O
x	O
)	O
can	O
be	O
written	O
p	O
(	O
x	O
)	O
dx	O
.	O
in	O
general	O
there	O
are	O
an	O
inﬁnite	O
number	O
of	O
eigenfunctions	O
,	O
which	O
we	O
label	O
φ1	O
(	O
x	O
)	O
,	O
φ2	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
we	O
assume	O
the	O
ordering	O
is	O
chosen	O
such	O
that	O
λ1	O
≥	O
λ2	O
≥	O
.	O
.	O
..	O
the	O
eigenfunctions	O
are	O
orthogonal	O
with	O
respect	O
to	O
µ	O
and	O
can	O
be	O
chosen	O
to	O
be	O
normalized	O
so	O
thatr	O
φi	O
(	O
x	O
)	O
φj	O
(	O
x	O
)	O
dµ	O
(	O
x	O
)	O
=	O
δij	O
where	O
δij	O
is	O
the	O
kronecker	O
delta	O
.	O
mercer	O
’	O
s	O
theorem	O
(	O
see	B
,	O
e.g	O
.	O
k¨onig	O
,	O
1986	O
)	O
allows	O
us	O
to	O
express	O
the	O
kernel	B
k	O
in	O
terms	O
of	O
the	O
eigenvalues	O
and	O
eigenfunctions	O
.	O
theorem	O
4.2	O
(	O
mercer	O
’	O
s	O
theorem	O
)	O
.	O
let	O
(	O
x	O
,	O
µ	O
)	O
be	O
a	O
ﬁnite	O
measure	B
space	O
and	O
k	O
∈	O
l∞	O
(	O
x	O
2	O
,	O
µ2	O
)	O
be	O
a	O
kernel	B
such	O
that	O
tk	O
:	O
l2	O
(	O
x	O
,	O
µ	O
)	O
→	O
l2	O
(	O
x	O
,	O
µ	O
)	O
is	O
positive	B
deﬁnite	I
(	O
see	B
eq	O
.	O
(	O
4.2	O
)	O
)	O
.	O
let	O
φi	O
∈	O
l2	O
(	O
x	O
,	O
µ	O
)	O
be	O
the	O
normalized	O
eigenfunctions	O
of	O
tk	O
associated	O
with	O
the	O
eigenvalues	O
λi	O
>	O
0.	O
then	O
:	O
1.	O
the	O
eigenvalues	O
{	O
λi	O
}	O
∞	O
2.	O
i=1	O
are	O
absolutely	O
summable	O
∞x	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
λiφi	O
(	O
x	O
)	O
φ∗	O
i	O
(	O
x0	O
)	O
,	O
(	O
4.37	O
)	O
holds	O
µ2	O
almost	O
everywhere	O
,	O
where	O
the	O
series	O
converges	O
absolutely	O
and	O
(	O
cid:3	O
)	O
uniformly	O
µ2	O
almost	O
everywhere	O
.	O
i=1	O
this	O
decomposition	O
is	O
just	O
the	O
inﬁnite-dimensional	O
analogue	O
of	O
the	O
diagonaliza-	O
tion	O
of	O
a	O
hermitian	O
matrix	B
.	O
note	O
that	O
the	O
sum	O
may	O
terminate	O
at	O
some	O
value	O
n	O
∈	O
n	O
(	O
i.e	O
.	O
the	O
eigenvalues	O
beyond	O
n	O
are	O
zero	O
)	O
,	O
or	O
the	O
sum	O
may	O
be	O
inﬁnite	O
.	O
we	O
have	O
the	O
following	O
deﬁnition	O
[	O
press	O
et	O
al.	O
,	O
1992	O
,	O
p.	O
794	O
]	O
10for	O
further	O
explanation	O
of	O
measure	B
see	O
appendix	O
a.7	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
4.3	O
eigenfunction	B
analysis	O
of	O
kernels	O
97	O
deﬁnition	O
4.1	O
a	O
degenerate	B
kernel	O
has	O
only	O
a	O
ﬁnite	O
number	O
of	O
non-zero	O
eigen-	O
(	O
cid:3	O
)	O
values	O
.	O
a	O
degenerate	B
kernel	O
is	O
also	O
said	O
to	O
have	O
ﬁnite	O
rank	O
.	O
if	O
a	O
kernel	B
is	O
not	O
degenerate	B
it	O
is	O
said	O
to	O
be	O
nondegenerate	B
.	O
as	O
an	O
example	O
a	O
n-dimensional	O
linear	B
regression	I
model	O
in	O
feature	B
space	I
(	O
see	B
eq	O
.	O
(	O
2.10	O
)	O
)	O
gives	O
rise	O
to	O
a	O
degenerate	B
kernel	O
with	O
at	O
most	O
n	O
non-zero	O
eigenvalues	O
.	O
(	O
of	O
course	O
if	O
the	O
measure	B
only	O
puts	O
weight	O
on	O
a	O
ﬁnite	O
number	O
of	O
points	O
n	O
in	O
x-space	O
then	O
the	O
eigendecomposition	O
is	O
simply	O
that	O
of	O
a	O
n	O
×	O
n	O
matrix	B
,	O
even	O
if	O
the	O
kernel	B
is	O
nondegenerate	B
.	O
)	O
degenerate	B
,	O
nondegenerate	B
kernel	O
the	O
statement	O
of	O
mercer	O
’	O
s	O
theorem	O
above	O
referred	O
to	O
a	O
ﬁnite	O
measure	B
µ.	O
if	O
we	O
replace	O
this	O
with	O
lebesgue	O
measure	B
and	O
consider	O
a	O
stationary	O
covariance	B
function	I
,	O
then	O
directly	O
from	O
bochner	O
’	O
s	O
theorem	O
eq	O
.	O
(	O
4.5	O
)	O
we	O
obtain	O
k	O
(	O
x	O
−	O
x0	O
)	O
=	O
e2πis·	O
(	O
x−x0	O
)	O
dµ	O
(	O
s	O
)	O
=	O
dµ	O
(	O
s	O
)	O
.	O
(	O
4.38	O
)	O
z	O
rd	O
e2πis·x	O
(	O
cid:16	O
)	O
e2πis·x0	O
(	O
cid:17	O
)	O
∗	O
z	O
rd	O
the	O
complex	O
exponentials	O
e2πis·x	O
are	O
the	O
eigenfunctions	O
of	O
a	O
stationary	O
kernel	B
w.r.t	O
.	O
lebesgue	O
measure	B
.	O
note	O
the	O
similarity	O
to	O
eq	O
.	O
(	O
4.37	O
)	O
except	O
that	O
the	O
summation	O
has	O
been	O
replaced	O
by	O
an	O
integral	O
.	O
the	O
rate	O
of	O
decay	O
of	O
the	O
eigenvalues	O
gives	O
important	O
information	O
about	O
the	O
smoothness	O
of	O
the	O
kernel	B
.	O
for	O
example	O
ritter	O
et	O
al	O
.	O
[	O
1995	O
]	O
showed	O
that	O
in	O
1-d	O
with	O
µ	O
uniform	O
on	O
[	O
0	O
,	O
1	O
]	O
,	O
processes	O
which	O
are	O
r-times	O
mean-square	O
diﬀerentiable	O
have	O
λi	O
∝	O
i−	O
(	O
2r+2	O
)	O
asymptotically	O
.	O
this	O
makes	O
sense	O
as	O
“	O
rougher	O
”	O
processes	O
have	O
more	O
power	O
at	O
high	O
frequencies	O
,	O
and	O
so	O
their	O
eigenvalue	B
spectrum	O
decays	O
more	O
slowly	O
.	O
the	O
same	O
phenomenon	O
can	O
be	O
read	O
oﬀ	O
from	O
the	O
power	O
spectrum	O
of	O
the	O
mat´ern	O
class	O
as	O
given	O
in	O
eq	O
.	O
(	O
4.15	O
)	O
.	O
hawkins	O
[	O
1989	O
]	O
gives	O
the	O
exact	O
eigenvalue	B
spectrum	O
for	O
the	O
ou	O
process	B
on	O
[	O
0	O
,	O
1	O
]	O
.	O
widom	O
[	O
1963	O
;	O
1964	O
]	O
gives	O
an	O
asymptotic	O
analysis	O
of	O
the	O
eigenvalues	O
of	O
stationary	O
kernels	O
taking	O
into	O
account	O
the	O
eﬀect	O
of	O
the	O
density	O
dµ	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
)	O
dx	O
;	O
bach	O
and	O
jordan	O
[	O
2002	O
,	O
table	O
3	O
]	O
use	O
these	O
results	O
to	O
show	O
the	O
eﬀect	O
of	O
varying	O
p	O
(	O
x	O
)	O
for	O
the	O
se	O
kernel	B
.	O
an	O
exact	O
eigenanalysis	O
of	O
the	O
se	O
kernel	B
under	O
the	O
gaussian	O
density	O
is	O
given	O
in	O
the	O
next	O
section	O
.	O
4.3.1	O
an	O
analytic	O
example	O
∗	O
for	O
the	O
case	O
that	O
p	O
(	O
x	O
)	O
is	O
a	O
gaussian	O
and	O
for	O
the	O
squared-exponential	O
kernel	B
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
exp	O
(	O
−	O
(	O
x−x0	O
)	O
2/2	O
‘	O
2	O
)	O
,	O
there	O
are	O
analytic	O
results	O
for	O
the	O
eigenvalues	O
and	O
eigenfunctions	O
,	O
as	O
given	O
by	O
zhu	O
et	O
al	O
.	O
[	O
1998	O
,	O
sec	O
.	O
4	O
]	O
.	O
putting	O
p	O
(	O
x	O
)	O
=	O
n	O
(	O
x|0	O
,	O
σ2	O
)	O
we	O
ﬁnd	O
that	O
the	O
eigenvalues	O
λk	O
and	O
eigenfunctions	O
φk	O
(	O
for	O
convenience	O
let	O
k	O
=	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
)	O
are	O
given	O
by	O
r2a	O
a	O
λk	O
=	O
φk	O
(	O
x	O
)	O
=	O
exp	O
(	O
cid:0	O
)	O
−	O
(	O
c	O
−	O
a	O
)	O
x2	O
(	O
cid:1	O
)	O
hk	O
bk	O
,	O
(	O
cid:0	O
)	O
√	O
2cx	O
(	O
cid:1	O
)	O
,	O
(	O
4.39	O
)	O
(	O
4.40	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
98	O
covariance	B
functions	O
figure	O
4.7	O
:	O
the	O
ﬁrst	O
3	O
eigenfunctions	O
of	O
the	O
squared	B
exponential	I
kernel	O
w.r.t	O
.	O
a	O
gaussian	O
density	O
.	O
the	O
value	O
of	O
k	O
=	O
0	O
,	O
1	O
,	O
2	O
is	O
equal	O
to	O
the	O
number	O
of	O
zero-crossings	O
of	O
the	O
function	B
.	O
the	O
dashed	O
line	O
is	O
proportional	O
to	O
the	O
density	O
p	O
(	O
x	O
)	O
.	O
where	O
hk	O
(	O
x	O
)	O
=	O
(	O
−1	O
)	O
k	O
exp	O
(	O
x2	O
)	O
dk	O
(	O
see	B
gradshteyn	O
and	O
ryzhik	O
[	O
1980	O
,	O
sec	O
.	O
8.95	O
]	O
)	O
,	O
a−1	O
=	O
4σ2	O
,	O
b−1	O
=	O
2	O
‘	O
2	O
and	O
dxk	O
exp	O
(	O
−x2	O
)	O
is	O
the	O
kth	O
order	O
hermite	O
polynomial	B
a2	O
+	O
2ab	O
,	O
a	O
=	O
a	O
+	O
b	O
+	O
c	O
,	O
b	O
=	O
b/a	O
.	O
(	O
4.41	O
)	O
hints	O
on	O
the	O
proof	O
of	O
this	O
result	O
are	O
given	O
in	O
exercise	O
4.5.9.	O
a	O
plot	O
of	O
the	O
ﬁrst	O
three	O
eigenfunctions	O
for	O
a	O
=	O
1	O
and	O
b	O
=	O
3	O
is	O
shown	O
in	O
figure	O
4.7.	O
the	O
result	O
for	O
the	O
eigenvalues	O
and	O
eigenfunctions	O
is	O
readily	O
generalized	B
to	O
the	O
multivariate	O
case	O
when	O
the	O
kernel	B
and	O
gaussian	O
density	O
are	O
products	O
of	O
the	O
univariate	O
expressions	O
,	O
as	O
the	O
eigenfunctions	O
and	O
eigenvalues	O
will	O
simply	O
be	O
products	O
too	O
.	O
for	O
the	O
case	O
that	O
a	O
and	O
b	O
are	O
equal	O
on	O
all	O
d	O
dimensions	O
,	O
the	O
degeneracy	O
of	O
the	O
eigenvalue	B
(	O
2a	O
(	O
cid:1	O
)	O
which	O
is	O
o	O
(	O
kd−1	O
)	O
.	O
as	O
(	O
cid:1	O
)	O
’	O
th	O
eigenvalue	B
has	O
a	O
value	O
given	O
by	O
(	O
cid:1	O
)	O
we	O
see	B
that	O
the	O
(	O
cid:0	O
)	O
k+d	O
(	O
cid:1	O
)	O
=	O
(	O
cid:0	O
)	O
k+d	O
a	O
)	O
d/2bk	O
is	O
(	O
cid:0	O
)	O
k+d−1	O
d−1	O
pk	O
(	O
cid:0	O
)	O
j+d−1	O
d−1	O
a	O
)	O
d/2bk	O
,	O
and	O
this	O
can	O
be	O
used	O
to	O
determine	O
the	O
rate	O
of	O
decay	O
of	O
the	O
spectrum	O
.	O
(	O
2a	O
j=0	O
d	O
d	O
4.3.2	O
numerical	O
approximation	O
of	O
eigenfunctions	O
the	O
standard	O
numerical	O
method	O
for	O
approximating	O
the	O
eigenfunctions	O
and	O
eigen-	O
values	O
of	O
eq	O
.	O
(	O
4.36	O
)	O
is	O
to	O
use	O
a	O
numerical	O
routine	O
to	O
approximate	O
the	O
integral	O
(	O
see	B
,	O
e.g	O
.	O
baker	O
[	O
1977	O
,	O
ch	O
.	O
3	O
]	O
)	O
.	O
for	O
example	O
letting	O
dµ	O
(	O
x	O
)	O
=	O
p	O
(	O
x	O
)	O
dx	O
in	O
eq	O
.	O
(	O
4.36	O
)	O
one	O
could	O
use	O
the	O
approximation	O
c	O
=p	O
z	O
λiφi	O
(	O
x0	O
)	O
=	O
k	O
(	O
x	O
,	O
x0	O
)	O
p	O
(	O
x	O
)	O
φi	O
(	O
x	O
)	O
dx	O
’	O
1	O
n	O
k	O
(	O
xl	O
,	O
x0	O
)	O
φi	O
(	O
xl	O
)	O
,	O
(	O
4.42	O
)	O
nx	O
l=1	O
−202−0.200.20.4	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
4.4	O
kernels	O
for	O
non-vectorial	O
inputs	O
99	O
where	O
the	O
xl	O
’	O
s	O
are	O
sampled	O
from	O
p	O
(	O
x	O
)	O
.	O
plugging	O
in	O
x0	O
=	O
xl	O
for	O
l	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
into	O
eq	O
.	O
(	O
4.42	O
)	O
we	O
obtain	O
the	O
matrix	B
eigenproblem	O
kui	O
=	O
λmat	O
i	O
ui	O
,	O
(	O
4.43	O
)	O
i	O
√	O
n	O
(	O
ui	O
)	O
j	O
where	O
the	O
where	O
k	O
is	O
the	O
n×	O
n	O
gram	O
matrix	B
with	O
entries	O
kij	O
=	O
k	O
(	O
xi	O
,	O
xj	O
)	O
,	O
λmat	O
is	O
the	O
ith	O
i	O
ui	O
=	O
1	O
)	O
.	O
we	O
have	O
φi	O
(	O
xj	O
)	O
∼	O
√	O
matrix	B
eigenvalue	O
and	O
ui	O
is	O
the	O
corresponding	O
eigenvector	O
(	O
normalized	O
so	O
that	O
u	O
>	O
n	O
factor	O
arises	O
from	O
the	O
diﬀering	O
normalizations	O
of	O
the	O
eigenvector	O
and	O
eigenfunction	B
.	O
thus	O
1	O
is	O
an	O
obvious	O
estimator	O
for	O
λi	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
for	O
ﬁxed	O
n	O
one	O
would	O
expect	O
that	O
the	O
larger	O
eigenvalues	O
would	O
be	O
better	O
estimated	O
than	O
the	O
smaller	O
ones	O
.	O
the	O
theory	O
of	O
the	O
numerical	O
solution	O
of	O
eigenvalue	B
problems	O
shows	O
that	O
for	O
a	O
ﬁxed	O
i	O
,	O
will	O
converge	O
to	O
λi	O
in	O
the	O
limit	O
that	O
n	O
→	O
∞	O
[	O
baker	O
,	O
1977	O
,	O
theorem	O
3.4	O
]	O
.	O
1	O
n	O
λmat	O
it	O
is	O
also	O
possible	O
to	O
study	O
the	O
convergence	O
further	O
;	O
for	O
example	O
it	O
is	O
quite	O
easy	O
using	O
the	O
properties	O
of	O
principal	O
components	O
analysis	O
(	O
pca	O
)	O
in	O
feature	B
space	I
to	O
show	O
that	O
for	O
any	O
l	O
,	O
1	O
≤	O
l	O
≤	O
n	O
,	O
en	O
[	O
1	O
i=1λi	O
and	O
en	O
[	O
1	O
i=l+1λi	O
,	O
where	O
en	O
denotes	O
expectation	O
with	O
respect	O
to	O
samples	O
of	O
size	O
n	O
drawn	O
from	O
p	O
(	O
x	O
)	O
.	O
for	O
further	O
details	O
see	B
shawe-taylor	O
and	O
williams	O
[	O
2003	O
]	O
.	O
]	O
≥	O
pl	O
]	O
≤pn	O
pn	O
i=l+1λmat	O
i	O
pl	O
n	O
λmat	O
i	O
i=1λmat	O
i	O
n	O
i	O
n	O
nystr¨om	O
method	O
kernel	B
pca	O
the	O
nystr¨om	O
method	O
for	O
approximating	O
the	O
ith	O
eigenfunction	B
(	O
see	B
baker	O
[	O
1977	O
]	O
and	O
press	O
et	O
al	O
.	O
[	O
1992	O
,	O
section	O
18.1	O
]	O
)	O
is	O
given	O
by	O
φi	O
(	O
x0	O
)	O
’	O
k	O
(	O
x0	O
)	O
>	O
ui	O
,	O
(	O
4.44	O
)	O
√	O
n	O
λmat	O
i	O
where	O
k	O
(	O
x0	O
)	O
>	O
=	O
(	O
k	O
(	O
x1	O
,	O
x0	O
)	O
,	O
.	O
.	O
.	O
,	O
k	O
(	O
xn	O
,	O
x0	O
)	O
)	O
,	O
which	O
is	O
obtained	O
from	O
eq	O
.	O
(	O
4.42	O
)	O
by	O
dividing	O
both	O
sides	O
by	O
λi	O
.	O
equation	O
4.44	O
extends	O
the	O
approximation	O
φi	O
(	O
xj	O
)	O
’	O
√	O
n	O
(	O
ui	O
)	O
j	O
from	O
the	O
sample	O
points	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
to	O
all	O
x.	O
there	O
is	O
an	O
interesting	O
relationship	O
between	O
the	O
kernel	B
pca	O
method	O
of	O
sch¨olkopf	O
et	O
al	O
.	O
[	O
1998	O
]	O
and	O
the	O
eigenfunction	B
expansion	O
discussed	O
above	O
.	O
the	O
eigenfunction	B
expansion	O
has	O
(	O
at	O
least	O
potentially	O
)	O
an	O
inﬁnite	O
number	O
of	O
non-	O
zero	O
eigenvalues	O
.	O
in	O
contrast	O
,	O
the	O
kernel	B
pca	O
algorithm	O
operates	O
on	O
the	O
n	O
×	O
n	O
matrix	B
k	O
and	O
yields	O
n	O
eigenvalues	O
and	O
eigenvectors	O
.	O
eq	O
.	O
(	O
4.42	O
)	O
clariﬁes	O
the	O
relationship	O
between	O
the	O
two	O
.	O
however	O
,	O
note	O
that	O
eq	O
.	O
(	O
4.44	O
)	O
is	O
identical	O
(	O
up	O
to	O
scaling	O
factors	O
)	O
to	O
sch¨olkopf	O
et	O
al	O
.	O
[	O
1998	O
,	O
eq	O
.	O
4.1	O
]	O
which	O
describes	O
the	O
projection	O
of	O
a	O
new	O
point	O
x0	O
onto	O
the	O
ith	O
eigenvector	O
in	O
the	O
kernel	B
pca	O
feature	B
space	I
.	O
4.4	O
kernels	O
for	O
non-vectorial	O
inputs	O
so	O
far	O
in	O
this	O
chapter	O
we	O
have	O
assumed	O
that	O
the	O
input	O
x	O
is	O
a	O
vector	O
,	O
measuring	O
the	O
values	O
of	O
a	O
number	O
of	O
attributes	O
(	O
or	O
features	O
)	O
.	O
however	O
,	O
for	O
some	O
learning	B
problems	O
the	O
inputs	O
are	O
not	O
vectors	O
,	O
but	O
structured	O
objects	O
such	O
as	O
strings	O
,	O
trees	O
or	O
general	O
graphs	O
.	O
for	O
example	O
,	O
we	O
may	O
have	O
a	O
biological	O
problem	O
where	O
we	O
want	O
to	O
classify	O
proteins	O
(	O
represented	O
as	O
strings	O
of	O
amino	O
acid	O
symbols	O
)	O
.11	O
11proteins	O
are	O
initially	O
made	O
up	O
of	O
20	O
diﬀerent	O
amino	O
acids	O
,	O
of	O
which	O
a	O
few	O
may	O
later	O
be	O
modiﬁed	O
bringing	O
the	O
total	O
number	O
up	O
to	O
26	O
or	O
30.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
100	O
covariance	B
functions	O
or	O
our	O
input	O
may	O
be	O
parse-trees	O
derived	O
from	O
a	O
linguistic	O
analysis	O
.	O
or	O
we	O
may	O
wish	O
to	O
represent	O
chemical	O
compounds	O
as	O
labelled	O
graphs	O
,	O
with	O
vertices	O
denoting	O
atoms	O
and	O
edges	O
denoting	O
bonds	O
.	O
to	O
follow	O
the	O
discriminative	B
approach	I
we	O
need	O
to	O
extract	O
some	O
features	O
from	O
the	O
input	O
objects	O
and	O
build	O
a	O
predictor	O
using	O
these	O
features	O
.	O
(	O
for	O
a	O
classiﬁcation	B
problem	O
,	O
the	O
alternative	O
generative	B
approach	I
would	O
construct	O
class-conditional	O
models	O
over	O
the	O
objects	O
themselves	O
.	O
)	O
below	O
we	O
describe	O
two	O
approaches	O
to	O
this	O
feature	O
extraction	O
problem	O
and	O
the	O
eﬃcient	O
computation	O
of	O
kernels	O
from	O
them	O
:	O
in	O
section	O
4.4.1	O
we	O
cover	O
string	B
kernels	O
,	O
and	O
in	O
section	O
4.4.2	O
we	O
describe	O
fisher	O
kernels	O
.	O
there	O
exist	O
other	O
proposals	O
for	O
constructing	O
kernels	O
for	O
strings	O
,	O
for	O
example	O
watkins	O
[	O
2000	O
]	O
describes	O
the	O
use	O
of	O
pair	O
hidden	O
markov	O
models	O
(	O
hmms	O
that	O
generate	O
output	O
symbols	O
for	O
two	O
strings	O
conditional	B
on	O
the	O
hidden	O
state	O
)	O
for	O
this	O
purpose	O
.	O
4.4.1	O
string	B
kernels	O
we	O
start	O
by	O
deﬁning	O
some	O
notation	O
for	O
strings	O
.	O
let	O
a	O
be	O
a	O
ﬁnite	O
alphabet	O
of	O
characters	O
.	O
the	O
concatenation	O
of	O
strings	O
x	O
and	O
y	O
is	O
written	O
xy	O
and	O
|x|	O
denotes	O
the	O
length	O
of	O
string	B
x.	O
the	O
string	B
s	O
is	O
a	O
substring	O
of	O
x	O
if	O
we	O
can	O
write	O
x	O
=	O
usv	O
for	O
some	O
(	O
possibly	O
empty	O
)	O
u	O
,	O
s	O
and	O
v.	O
let	O
φs	O
(	O
x	O
)	O
denote	O
the	O
number	O
of	O
times	O
that	O
substring	O
s	O
appears	O
in	O
string	B
x.	O
then	O
we	O
deﬁne	O
the	O
kernel	B
between	O
two	O
strings	O
x	O
and	O
x0	O
as	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
x	O
s∈a∗	O
wsφs	O
(	O
x	O
)	O
φs	O
(	O
x0	O
)	O
,	O
(	O
4.45	O
)	O
where	O
ws	O
is	O
a	O
non-negative	O
weight	O
for	O
substring	O
s.	O
for	O
example	O
,	O
we	O
could	O
set	B
ws	O
=	O
λ|s|	O
,	O
where	O
0	O
<	O
λ	O
<	O
1	O
,	O
so	O
that	O
shorter	O
substrings	O
get	O
more	O
weight	O
than	O
longer	O
ones	O
.	O
a	O
number	O
of	O
interesting	O
special	O
cases	O
are	O
contained	O
in	O
the	O
deﬁnition	O
4.45	O
:	O
•	O
setting	O
ws	O
=	O
0	O
for	O
|s|	O
>	O
1	O
gives	O
the	O
bag-of-characters	B
kernel	O
.	O
this	O
takes	O
the	O
feature	O
vector	O
for	O
a	O
string	B
x	O
to	O
be	O
the	O
number	O
of	O
times	O
that	O
each	O
character	O
in	O
a	O
appears	O
in	O
x	O
.	O
•	O
in	O
text	O
analysis	O
we	O
may	O
wish	O
to	O
consider	O
the	O
frequencies	O
of	O
word	O
occur-	O
rence	O
.	O
if	O
we	O
require	O
s	O
to	O
be	O
bordered	O
by	O
whitespace	O
then	O
a	O
“	O
bag-of-words	O
”	O
representation	O
is	O
obtained	O
.	O
although	O
this	O
is	O
a	O
very	O
simple	O
model	B
of	O
text	O
(	O
which	O
ignores	O
word	O
order	O
)	O
it	O
can	O
be	O
surprisingly	O
eﬀective	O
for	O
document	O
classiﬁcation	B
and	O
retrieval	O
tasks	O
,	O
see	B
e.g	O
.	O
hand	O
et	O
al	O
.	O
[	O
2001	O
,	O
sec	O
.	O
14.3	O
]	O
.	O
the	O
weights	O
can	O
be	O
set	B
diﬀerently	O
for	O
diﬀerent	O
words	O
,	O
e.g	O
.	O
using	O
the	O
“	O
term	O
frequency	O
inverse	O
document	O
frequency	O
”	O
(	O
tf-idf	O
)	O
weighting	O
scheme	O
de-	O
veloped	O
in	O
the	O
information	O
retrieval	O
area	O
[	O
salton	O
and	O
buckley	O
,	O
1988	O
]	O
.	O
•	O
if	O
we	O
only	O
consider	O
substrings	O
of	O
length	O
k	O
,	O
then	O
we	O
obtain	O
the	O
k-spectrum	B
kernel	O
[	O
leslie	O
et	O
al.	O
,	O
2003	O
]	O
.	O
bag-of-characters	B
bag-of-words	O
k-spectrum	B
kernel	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
4.4	O
kernels	O
for	O
non-vectorial	O
inputs	O
101	O
importantly	O
,	O
there	O
are	O
eﬃcient	O
methods	O
using	O
suﬃx	O
trees	O
that	O
can	O
compute	O
a	O
string	B
kernel	O
k	O
(	O
x	O
,	O
x0	O
)	O
in	O
time	O
linear	B
in	O
|x|	O
+|x0|	O
(	O
with	O
some	O
restrictions	O
on	O
the	O
weights	O
{	O
ws	O
}	O
)	O
[	O
leslie	O
et	O
al.	O
,	O
2003	O
,	O
vishwanathan	O
and	O
smola	O
,	O
2003	O
]	O
.	O
work	O
on	O
string	B
kernels	O
was	O
started	O
by	O
watkins	O
[	O
1999	O
]	O
and	O
haussler	O
[	O
1999	O
]	O
.	O
there	O
are	O
many	O
further	O
developments	O
of	O
the	O
methods	O
we	O
have	O
described	O
above	O
;	O
for	O
example	O
lodhi	O
et	O
al	O
.	O
[	O
2001	O
]	O
go	O
beyond	O
substrings	O
to	O
consider	O
subsequences	O
of	O
x	O
which	O
are	O
not	O
necessarily	O
contiguous	O
,	O
and	O
leslie	O
et	O
al	O
.	O
[	O
2003	O
]	O
describe	O
mismatch	O
string	B
kernels	O
which	O
allow	O
substrings	O
s	O
and	O
s0	O
of	O
x	O
and	O
x0	O
respectively	O
to	O
match	O
if	O
there	O
are	O
at	O
most	O
m	O
mismatches	O
between	O
them	O
.	O
we	O
expect	O
further	O
developments	O
in	O
this	O
area	O
,	O
tailoring	O
(	O
or	O
engineering	O
)	O
the	O
string	B
kernels	O
to	O
have	O
properties	O
that	O
make	O
sense	O
in	O
a	O
particular	O
domain	O
.	O
the	O
idea	O
of	O
string	B
kernels	O
,	O
where	O
we	O
consider	O
matches	O
of	O
substrings	O
,	O
can	O
easily	O
be	O
extended	O
to	O
trees	O
,	O
e.g	O
.	O
by	O
looking	O
at	O
matches	O
of	O
subtrees	O
[	O
collins	O
and	O
duﬀy	O
,	O
2002	O
]	O
.	O
leslie	O
et	O
al	O
.	O
[	O
2003	O
]	O
have	O
applied	O
string	B
kernels	O
to	O
the	O
classiﬁcation	B
of	O
protein	O
domains	O
into	O
scop12	O
superfamilies	O
.	O
the	O
results	O
obtained	O
were	O
signiﬁcantly	O
better	O
than	O
methods	O
based	O
on	O
either	O
psi-blast13	O
searches	O
or	O
a	O
generative	O
hidden	O
markov	O
model	B
classiﬁer	O
.	O
similar	O
results	O
were	O
obtained	O
by	O
jaakkola	O
et	O
al	O
.	O
[	O
2000	O
]	O
using	O
a	O
fisher	O
kernel	B
(	O
described	O
in	O
the	O
next	O
section	O
)	O
.	O
saunders	O
et	O
al	O
.	O
[	O
2003	O
]	O
have	O
also	O
described	O
the	O
use	O
of	O
string	B
kernels	O
on	O
the	O
problem	O
of	O
classifying	O
natural	O
language	O
newswire	O
stories	O
from	O
the	O
reuters-2157814	O
database	O
into	O
ten	O
classes	O
.	O
4.4.2	O
fisher	O
kernels	O
score	O
vector	O
as	O
explained	O
above	O
,	O
our	O
problem	O
is	O
that	O
the	O
input	O
x	O
is	O
a	O
structured	O
object	O
of	O
arbitrary	O
size	O
e.g	O
.	O
a	O
string	B
,	O
and	O
we	O
wish	O
to	O
extract	O
features	O
from	O
it	O
.	O
the	O
fisher	O
kernel	B
(	O
introduced	O
by	O
jaakkola	O
et	O
al.	O
,	O
2000	O
)	O
does	O
this	O
by	O
taking	O
a	O
generative	O
model	O
p	O
(	O
x|θ	O
)	O
,	O
where	O
θ	O
is	O
a	O
vector	O
of	O
parameters	O
,	O
and	O
computing	O
the	O
feature	O
vector	O
φθ	O
(	O
x	O
)	O
=	O
∇θ	O
log	O
p	O
(	O
x|θ	O
)	O
.	O
φθ	O
(	O
x	O
)	O
is	O
sometimes	O
called	O
the	O
score	O
vector	O
.	O
in	O
string	B
x.	O
then	O
a	O
markov	O
model	B
gives	O
p	O
(	O
x|θ	O
)	O
=	O
p	O
(	O
x1|π	O
)	O
q|x|−1	O
take	O
,	O
for	O
example	O
,	O
a	O
markov	O
model	B
for	O
strings	O
.	O
let	O
xk	O
be	O
the	O
kth	O
symbol	O
i=1	O
p	O
(	O
xi+1|xi	O
,	O
a	O
)	O
,	O
where	O
θ	O
=	O
(	O
π	O
,	O
a	O
)	O
.	O
here	O
(	O
π	O
)	O
j	O
gives	O
the	O
probability	B
that	O
x1	O
will	O
be	O
the	O
jth	O
symbol	O
in	O
the	O
alphabet	O
a	O
,	O
and	O
a	O
is	O
a	O
|a|	O
×	O
|a|	O
stochastic	O
matrix	O
,	O
with	O
ajk	O
giving	O
the	O
probability	B
that	O
p	O
(	O
xi+1	O
=	O
k|xi	O
=	O
j	O
)	O
.	O
given	O
such	O
a	O
model	B
it	O
is	O
straightforward	O
to	O
compute	O
the	O
score	O
vector	O
for	O
a	O
given	O
x.	O
it	O
is	O
also	O
possible	O
to	O
consider	O
other	O
generative	O
models	O
p	O
(	O
x|θ	O
)	O
.	O
for	O
example	O
we	O
might	O
try	O
a	O
kth-order	O
markov	O
model	B
where	O
xi	O
is	O
predicted	O
by	O
the	O
preceding	O
k	O
symbols	O
.	O
see	B
leslie	O
et	O
al	O
.	O
[	O
2003	O
]	O
and	O
saunders	O
et	O
al	O
.	O
[	O
2003	O
]	O
for	O
an	O
interesting	O
discussion	O
of	O
the	O
similarities	O
of	O
the	O
features	O
used	O
in	O
the	O
k-spectrum	B
kernel	O
and	O
the	O
score	O
vector	O
derived	O
from	O
an	O
order	O
k	O
−	O
1	O
markov	O
model	B
;	O
see	B
also	O
exercise	O
12structural	O
classiﬁcation	B
of	O
proteins	O
database	O
,	O
http	O
:	O
//scop.mrc-lmb.cam.ac.uk/scop/	O
.	O
13position-speciﬁc	O
iterative	O
basic	O
local	O
alignment	B
search	O
tool	O
,	O
see	B
http	O
:	O
//www.ncbi.nlm.nih.gov/education/blastinfo/psi1.html	O
.	O
14http	O
:	O
//www.daviddlewis.com/resources/testcollections/reuters21578/	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
102	O
covariance	B
functions	O
fisher	O
information	O
matrix	B
fisher	O
kernel	B
top	O
kernel	B
4.5.12.	O
another	O
interesting	O
choice	O
is	O
to	O
use	O
a	O
hidden	O
markov	O
model	B
(	O
hmm	O
)	O
as	O
the	O
generative	O
model	O
,	O
as	O
discussed	O
by	O
jaakkola	O
et	O
al	O
.	O
[	O
2000	O
]	O
.	O
see	B
also	O
exercise	O
4.5.11	O
for	O
a	O
linear	B
kernel	O
derived	O
from	O
an	O
isotropic	O
gaussian	O
model	B
for	O
x	O
∈	O
rd	O
.	O
we	O
deﬁne	O
a	O
kernel	B
k	O
(	O
x	O
,	O
x0	O
)	O
based	O
on	O
the	O
score	O
vectors	O
for	O
x	O
and	O
x0	O
.	O
one	O
simple	O
choice	O
is	O
to	O
set	B
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
φ	O
>	O
θ	O
(	O
x	O
)	O
m−1φθ	O
(	O
x0	O
)	O
,	O
(	O
4.46	O
)	O
where	O
m	O
is	O
a	O
strictly	O
positive	B
deﬁnite	I
matrix	I
.	O
alternatively	O
we	O
might	O
use	O
the	O
squared	B
exponential	I
kernel	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
exp	O
(	O
−α|φθ	O
(	O
x	O
)	O
−φθ	O
(	O
x0	O
)	O
|2	O
)	O
for	O
some	O
α	O
>	O
0.	O
the	O
structure	O
of	O
p	O
(	O
x|θ	O
)	O
as	O
θ	O
varies	O
has	O
been	O
studied	O
extensively	O
in	O
informa-	O
tion	O
geometry	O
(	O
see	B
,	O
e.g	O
.	O
amari	O
,	O
1985	O
)	O
.	O
it	O
can	O
be	O
shown	O
that	O
the	O
manifold	O
of	O
log	O
p	O
(	O
x|θ	O
)	O
is	O
riemannian	O
with	O
a	O
metric	O
tensor	O
which	O
is	O
the	O
inverse	O
of	O
the	O
fisher	O
information	O
matrix	B
f	O
,	O
where	O
f	O
=	O
ex	O
[	O
φθ	O
(	O
x	O
)	O
φ	O
>	O
θ	O
(	O
x	O
)	O
]	O
.	O
(	O
4.47	O
)	O
setting	O
m	O
=	O
f	O
in	O
eq	O
.	O
(	O
4.46	O
)	O
gives	O
the	O
fisher	O
kernel	B
.	O
if	O
f	O
is	O
diﬃcult	O
to	O
compute	O
then	O
one	O
might	O
resort	O
to	O
setting	O
m	O
=	O
i.	O
the	O
advantage	O
of	O
using	O
the	O
fisher	O
information	O
matrix	B
is	O
that	O
it	O
makes	O
arc	O
length	O
on	O
the	O
manifold	O
invariant	O
to	O
reparameterizations	O
of	O
θ.	O
the	O
fisher	O
kernel	B
uses	O
a	O
class-independent	O
model	B
p	O
(	O
x|θ	O
)	O
.	O
tsuda	O
et	O
al	O
.	O
[	O
2002	O
]	O
have	O
developed	O
the	O
tangent	B
of	I
posterior	I
odds	I
(	O
top	O
)	O
kernel	B
based	O
on	O
∇θ	O
(	O
log	O
p	O
(	O
y	O
=	O
+1|x	O
,	O
θ	O
)	O
−log	O
p	O
(	O
y	O
=	O
−1|x	O
,	O
θ	O
)	O
)	O
,	O
which	O
makes	O
use	O
of	O
class-conditional	O
distributions	O
for	O
the	O
c+	O
and	O
c−	O
classes	O
.	O
4.5	O
exercises	O
1.	O
the	O
ou	O
process	B
with	O
covariance	B
function	I
k	O
(	O
x	O
−	O
x0	O
)	O
=	O
exp	O
(	O
−|x	O
−	O
x0|/	O
‘	O
)	O
is	O
the	O
unique	O
stationary	O
ﬁrst-order	O
markovian	O
gaussian	O
process	B
(	O
see	B
ap-	O
pendix	O
b	O
for	O
further	O
details	O
)	O
.	O
consider	O
training	O
inputs	O
x1	O
<	O
x2	O
.	O
.	O
.	O
<	O
xn−1	O
<	O
xn	O
on	O
r	O
with	O
corresponding	O
function	B
values	O
f	O
=	O
(	O
f	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
f	O
(	O
xn	O
)	O
)	O
>	O
.	O
let	O
xl	O
denote	O
the	O
nearest	O
training	O
input	O
to	O
the	O
left	O
of	O
a	O
test	O
point	O
x∗	O
,	O
and	O
similarly	O
let	O
xu	O
denote	O
the	O
nearest	O
training	O
input	O
to	O
the	O
right	O
of	O
x∗	O
.	O
then	O
the	O
markovian	O
property	O
means	O
that	O
p	O
(	O
f	O
(	O
x∗	O
)	O
|f	O
)	O
=	O
p	O
(	O
f	O
(	O
x∗	O
)	O
|f	O
(	O
xl	O
)	O
,	O
f	O
(	O
xu	O
)	O
)	O
.	O
demonstrate	O
this	O
by	O
choosing	O
some	O
x-points	O
on	O
the	O
line	O
and	O
computing	O
the	O
predictive	B
distribution	O
p	O
(	O
f	O
(	O
x∗	O
)	O
|f	O
)	O
using	O
eq	O
.	O
(	O
2.19	O
)	O
,	O
and	O
observing	O
that	O
non-zero	O
contributions	O
only	O
arise	O
from	O
xl	O
and	O
xu	O
.	O
note	O
that	O
this	O
only	O
occurs	O
in	O
the	O
noise-free	O
case	O
;	O
if	O
one	O
allows	O
the	O
training	O
points	O
to	O
be	O
cor-	O
rupted	O
by	O
noise	O
(	O
equations	O
2.23	O
and	O
2.24	O
)	O
then	O
all	O
points	O
will	O
contribute	O
in	O
general	O
.	O
2.	O
computer	O
exercise	O
:	O
write	O
code	O
to	O
draw	O
samples	O
from	O
the	O
neural	B
network	I
covariance	O
function	B
,	O
eq	O
.	O
(	O
4.29	O
)	O
in	O
1-d	O
and	O
2-d.	O
consider	O
the	O
cases	O
when	O
var	O
(	O
u0	O
)	O
is	O
either	O
0	O
or	O
non-zero	O
.	O
explain	O
the	O
form	O
of	O
the	O
plots	O
obtained	O
when	O
var	O
(	O
u0	O
)	O
=	O
0.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
103	O
4.5	O
exercises	O
3.	O
consider	O
the	O
random	O
process	B
f	O
(	O
x	O
)	O
=	O
erf	O
(	O
u0	O
+pd	O
i=1ujxj	O
)	O
,	O
where	O
u	O
∼	O
n	O
(	O
0	O
,	O
σ	O
)	O
.	O
show	O
that	O
this	O
non-linear	O
transform	O
of	O
a	O
process	B
with	O
an	O
inho-	O
mogeneous	O
linear	B
covariance	O
function	B
has	O
the	O
same	O
covariance	B
function	I
as	O
the	O
erf	O
neural	B
network	I
.	O
however	O
,	O
note	O
that	O
this	O
process	B
is	O
not	O
a	O
gaussian	O
process	B
.	O
draw	O
samples	O
from	O
the	O
given	O
process	B
and	O
compare	O
them	O
to	O
your	O
results	O
from	O
exercise	O
4.5.2	O
.	O
4.	O
derive	O
gibbs	O
’	O
non-stationary	O
covariance	B
function	I
,	O
eq	O
.	O
(	O
4.32	O
)	O
.	O
5.	O
computer	O
exercise	O
:	O
write	O
code	O
to	O
draw	O
samples	O
from	O
gibbs	O
’	O
non-stationary	O
covariance	B
function	I
eq	O
.	O
(	O
4.32	O
)	O
in	O
1-d	O
and	O
2-d.	O
investigate	O
various	O
forms	O
of	O
length-scale	B
function	O
‘	O
(	O
x	O
)	O
.	O
6.	O
show	O
that	O
the	O
se	O
process	B
is	O
inﬁnitely	O
ms	O
diﬀerentiable	O
and	O
that	O
the	O
ou	O
process	B
is	O
not	O
ms	O
diﬀerentiable	O
.	O
7.	O
prove	O
that	O
the	O
eigenfunctions	O
of	O
a	O
symmetric	O
kernel	B
are	O
orthogonal	O
w.r.t	O
.	O
the	O
measure	B
µ	O
.	O
8.	O
let	O
˜k	O
(	O
x	O
,	O
x0	O
)	O
=	O
p1/2	O
(	O
x	O
)	O
k	O
(	O
x	O
,	O
x0	O
)	O
p1/2	O
(	O
x0	O
)	O
,	O
and	O
assume	O
p	O
(	O
x	O
)	O
>	O
0	O
for	O
all	O
x	O
.	O
˜φi	O
(	O
x0	O
)	O
has	O
the	O
same	O
show	O
that	O
the	O
eigenproblem	O
r	O
˜k	O
(	O
x	O
,	O
x0	O
)	O
˜φi	O
(	O
x	O
)	O
dx	O
=	O
˜λi	O
eigenvalues	O
asr	O
k	O
(	O
x	O
,	O
x0	O
)	O
p	O
(	O
x	O
)	O
φi	O
(	O
x	O
)	O
dx	O
=	O
λiφi	O
(	O
x0	O
)	O
,	O
and	O
that	O
the	O
eigenfunc-	O
tions	O
are	O
related	O
by	O
˜φi	O
(	O
x	O
)	O
=	O
p1/2	O
(	O
x	O
)	O
φi	O
(	O
x	O
)	O
.	O
also	O
give	O
the	O
matrix	B
version	O
of	O
this	O
problem	O
(	O
hint	O
:	O
introduce	O
a	O
diagonal	O
matrix	B
p	O
to	O
take	O
the	O
rˆole	O
of	O
p	O
(	O
x	O
)	O
)	O
.	O
the	O
signiﬁcance	O
of	O
this	O
connection	O
is	O
that	O
it	O
can	O
be	O
easier	O
to	O
ﬁnd	O
eigenvalues	O
of	O
symmetric	O
matrices	O
than	O
general	O
matrices	O
.	O
9.	O
apply	O
the	O
construction	O
in	O
the	O
previous	O
exercise	O
to	O
the	O
eigenproblem	O
for	O
the	O
se	O
kernel	B
and	O
gaussian	O
density	O
given	O
in	O
section	O
4.3.1	O
,	O
with	O
p	O
(	O
x	O
)	O
=	O
exp	O
(	O
−ax2	O
)	O
exp	O
(	O
−b	O
(	O
x−x0	O
)	O
2	O
)	O
exp	O
(	O
−a	O
(	O
x0	O
)	O
2	O
)	O
.	O
using	O
equation	O
7.374.8	O
in	O
grad-	O
shteyn	O
and	O
ryzhik	O
[	O
1980	O
]	O
:	O
p2a/π	O
exp	O
(	O
−2ax2	O
)	O
.	O
thus	O
consider	O
the	O
modiﬁed	O
kernel	B
given	O
by	O
˜k	O
(	O
x	O
,	O
x0	O
)	O
=	O
z	O
∞	O
(	O
cid:17	O
)	O
exp	O
(	O
cid:0	O
)	O
−	O
(	O
x	O
−	O
y	O
)	O
2	O
(	O
cid:1	O
)	O
hn	O
(	O
αx	O
)	O
dx	O
=	O
π	O
(	O
1	O
−	O
α2	O
)	O
n/2hn	O
(	O
cid:16	O
)	O
√	O
αy	O
(	O
1	O
−	O
α2	O
)	O
1/2	O
,	O
−∞	O
√	O
verify	O
that	O
˜φk	O
(	O
x	O
)	O
=	O
exp	O
(	O
−cx2	O
)	O
hk	O
(	O
and	O
4.40	O
.	O
2cx	O
)	O
,	O
and	O
thus	O
conﬁrm	O
equations	O
4.39	O
10.	O
computer	O
exercise	O
:	O
the	O
analytic	O
form	O
of	O
the	O
eigenvalues	O
and	O
eigenfunc-	O
tions	O
for	O
the	O
se	O
kernel	B
and	O
gaussian	O
density	O
are	O
given	O
in	O
section	O
4.3.1.	O
compare	O
these	O
exact	O
results	O
to	O
those	O
obtained	O
by	O
the	O
nystr¨om	O
approxi-	O
mation	O
for	O
various	O
values	O
of	O
n	O
and	O
choice	O
of	O
samples	O
.	O
11.	O
let	O
x	O
∼	O
n	O
(	O
µ	O
,	O
σ2i	O
)	O
.	O
consider	O
the	O
fisher	O
kernel	B
derived	O
from	O
this	O
model	B
with	O
respect	O
to	O
variation	O
of	O
µ	O
(	O
i.e	O
.	O
regard	O
σ2	O
as	O
a	O
constant	O
)	O
.	O
show	O
that	O
:	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
µ=0	O
∂	O
log	O
p	O
(	O
x|µ	O
)	O
∂µ	O
=	O
x	O
σ2	O
and	O
that	O
f	O
=	O
σ−2i	O
.	O
thus	O
the	O
fisher	O
kernel	B
for	O
this	O
model	B
with	O
µ	O
=	O
0	O
is	O
the	O
linear	B
kernel	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
1	O
σ2	O
x	O
·	O
x0	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
104	O
covariance	B
functions	O
12.	O
consider	O
a	O
k	O
−	O
1	O
order	O
markov	O
model	B
for	O
strings	O
on	O
a	O
ﬁnite	O
alphabet	O
.	O
let	O
this	O
model	B
have	O
parameters	O
θt|s1	O
,	O
...	O
,	O
sk−1	O
denoting	O
the	O
probability	B
p	O
(	O
xi	O
=	O
t|xi−1	O
=	O
s1	O
,	O
.	O
.	O
.	O
,	O
xk−1	O
=	O
sk−1	O
)	O
.	O
of	O
course	O
as	O
these	O
are	O
probabilities	O
they	O
t0	O
θt0|s1	O
,	O
...	O
,	O
sk−1	O
=	O
1.	O
enforcing	O
this	O
constraint	O
obey	O
the	O
constraint	O
that	O
p	O
can	O
be	O
achieved	O
automatically	O
by	O
setting	O
θt|s1	O
,	O
...	O
,	O
sk−1	O
=	O
p	O
θt	O
,	O
s1	O
,	O
...	O
,	O
sk−1	O
t0	O
θt0	O
,	O
s1	O
,	O
...	O
,	O
sk−1	O
,	O
where	O
the	O
θt	O
,	O
s1	O
,	O
...	O
,	O
sk−1	O
parameters	O
are	O
now	O
independent	O
,	O
as	O
suggested	O
in	O
[	O
jaakkola	O
et	O
al.	O
,	O
2000	O
]	O
.	O
the	O
current	O
parameter	O
values	O
are	O
denoted	O
θ0	O
.	O
let	O
the	O
current	O
values	O
of	O
θ0	O
t0	O
,	O
s1	O
,	O
...	O
,	O
sk−1	O
=	O
1	O
,	O
i.e	O
.	O
that	O
θ0	O
show	O
that	O
log	O
p	O
(	O
x|θ	O
)	O
=p	O
nt	O
,	O
s1	O
,	O
...	O
,	O
sk−1	O
log	O
θt|s1	O
,	O
...	O
,	O
sk−1	O
where	O
nt	O
,	O
s1	O
,	O
...	O
,	O
sk−1	O
is	O
t	O
,	O
s1	O
,	O
...	O
,	O
sk−1	O
be	O
set	B
so	O
that	O
p	O
the	O
number	O
of	O
instances	O
of	O
the	O
substring	O
sk−1	O
.	O
.	O
.	O
s1t	O
in	O
x.	O
thus	O
,	O
following	O
leslie	O
et	O
al	O
.	O
[	O
2003	O
]	O
,	O
show	O
that	O
t0θ0	O
t	O
,	O
s1	O
,	O
...	O
,	O
sk−1	O
=	O
θ0	O
t|s1	O
,	O
...	O
,	O
sk−1	O
.	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
θ=θ0	O
∂	O
log	O
p	O
(	O
x|θ	O
)	O
∂θt	O
,	O
s1	O
,	O
...	O
,	O
sk−1	O
=	O
nt	O
,	O
s1	O
,	O
...	O
,	O
sk−1	O
θ0	O
t|s1	O
,	O
...	O
,	O
sk−1	O
−	O
ns1	O
,	O
...	O
,	O
sk−1	O
,	O
t|s1	O
,	O
...	O
,	O
sk−1	O
where	O
ns1	O
,	O
...	O
,	O
sk−1	O
is	O
the	O
number	O
of	O
instances	O
of	O
the	O
substring	O
sk−1	O
.	O
.	O
.	O
s1	O
in	O
x.	O
as	O
ns1	O
,	O
...	O
,	O
sk−1θ0	O
is	O
the	O
expected	O
number	O
of	O
occurrences	O
of	O
the	O
string	B
sk−1	O
.	O
.	O
.	O
s1t	O
given	O
the	O
count	O
ns1	O
,	O
...	O
,	O
sk−1	O
,	O
the	O
fisher	O
score	O
captures	O
the	O
degree	O
to	O
which	O
this	O
string	B
is	O
over-	O
or	O
under-represented	O
relative	O
to	O
the	O
model	B
.	O
for	O
the	O
k-spectrum	B
kernel	O
the	O
relevant	O
feature	O
is	O
φsk−1	O
...	O
,	O
s1	O
,	O
t	O
(	O
x	O
)	O
=	O
nt	O
,	O
s1	O
,	O
...	O
,	O
sk−1	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
chapter	O
5	O
model	B
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
in	O
chapters	O
2	O
and	O
3	O
we	O
have	O
seen	O
how	O
to	O
do	O
regression	B
and	O
classiﬁcation	B
using	O
a	O
gaussian	O
process	B
with	O
a	O
given	O
ﬁxed	O
covariance	B
function	I
.	O
however	O
,	O
in	O
many	O
practical	O
applications	O
,	O
it	O
may	O
not	O
be	O
easy	O
to	O
specify	O
all	O
aspects	O
of	O
the	O
covari-	O
ance	O
function	B
with	O
conﬁdence	O
.	O
while	O
some	O
properties	O
such	O
as	O
stationarity	B
of	O
the	O
covariance	B
function	I
may	O
be	O
easy	O
to	O
determine	O
from	O
the	O
context	O
,	O
we	O
typically	O
have	O
only	O
rather	O
vague	O
information	O
about	O
other	O
properties	O
,	O
such	O
as	O
the	O
value	O
of	O
free	O
(	O
hyper-	O
)	O
parameters	O
,	O
e.g	O
.	O
length-scales	O
.	O
in	O
chapter	O
4	O
several	O
examples	O
of	O
covariance	B
functions	O
were	O
presented	O
,	O
many	O
of	O
which	O
have	O
large	O
numbers	O
of	O
parameters	O
.	O
in	O
addition	O
,	O
the	O
exact	O
form	O
and	O
possible	O
free	O
parameters	O
of	O
the	O
likelihood	B
function	O
may	O
also	O
not	O
be	O
known	O
in	O
advance	O
.	O
thus	O
in	O
order	O
to	O
turn	O
gaussian	O
processes	O
into	O
powerful	O
practical	O
tools	O
it	O
is	O
essential	O
to	O
develop	O
meth-	O
ods	O
that	O
address	O
the	O
model	B
selection	O
problem	O
.	O
we	O
interpret	O
the	O
model	B
selection	O
problem	O
rather	O
broadly	O
,	O
to	O
include	O
all	O
aspects	O
of	O
the	O
model	B
including	O
the	O
dis-	O
crete	O
choice	O
of	O
the	O
functional	B
form	O
for	O
the	O
covariance	B
function	I
as	O
well	O
as	O
values	O
for	O
any	O
hyperparameters	B
.	O
in	O
section	O
5.1	O
we	O
outline	O
the	O
model	B
selection	O
problem	O
.	O
in	O
the	O
following	O
sec-	O
tions	O
diﬀerent	O
methodologies	O
are	O
presented	O
:	O
in	O
section	O
5.2	O
bayesian	O
principles	O
are	O
covered	O
,	O
and	O
in	O
section	O
5.3	O
cross-validation	B
is	O
discussed	O
,	O
in	O
particular	O
the	O
leave-one-out	B
estimator	O
.	O
in	O
the	O
remaining	O
two	O
sections	O
the	O
diﬀerent	O
methodolo-	O
gies	O
are	O
applied	O
speciﬁcally	O
to	O
learning	B
in	O
gp	O
models	O
,	O
for	O
regression	B
in	O
section	O
5.4	O
and	O
classiﬁcation	B
in	O
section	O
5.5.	O
model	B
selection	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
106	O
model	B
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
5.1	O
the	O
model	B
selection	O
problem	O
in	O
order	O
for	O
a	O
model	B
to	O
be	O
a	O
practical	O
tool	O
in	O
an	O
application	O
,	O
one	O
needs	O
to	O
make	O
decisions	O
about	O
the	O
details	O
of	O
its	O
speciﬁcation	O
.	O
some	O
properties	O
may	O
be	O
easy	O
to	O
specify	O
,	O
while	O
we	O
typically	O
have	O
only	O
vague	O
information	O
available	O
about	O
other	O
aspects	O
.	O
we	O
use	O
the	O
term	O
model	B
selection	O
to	O
cover	O
both	O
discrete	O
choices	O
and	O
the	O
setting	O
of	O
continuous	O
(	O
hyper-	O
)	O
parameters	O
of	O
the	O
covariance	B
functions	O
.	O
in	O
fact	O
,	O
model	B
selection	O
can	O
help	O
both	O
to	O
reﬁne	O
the	O
predictions	O
of	O
the	O
model	B
,	O
and	O
give	O
a	O
valuable	O
interpretation	O
to	O
the	O
user	O
about	O
the	O
properties	O
of	O
the	O
data	O
,	O
e.g	O
.	O
that	O
a	O
non-stationary	O
covariance	B
function	I
may	O
be	O
preferred	O
over	O
a	O
stationary	O
one	O
.	O
a	O
multitude	O
of	O
possible	O
families	O
of	O
covariance	B
functions	O
exists	O
,	O
including	O
squared	B
exponential	I
,	O
polynomial	B
,	O
neural	B
network	I
,	O
etc.	O
,	O
see	B
section	O
4.2	O
for	O
an	O
overview	O
.	O
each	O
of	O
these	O
families	O
typically	O
have	O
a	O
number	O
of	O
free	O
hyperparameters	B
whose	O
values	O
also	O
need	O
to	O
be	O
determined	O
.	O
choosing	O
a	O
covariance	B
function	I
for	O
a	O
particular	O
application	O
thus	O
comprises	O
both	O
setting	O
of	O
hyperparameters	B
within	O
a	O
family	O
,	O
and	O
comparing	O
across	O
diﬀerent	O
families	O
.	O
both	O
of	O
these	O
problems	O
will	O
be	O
treated	O
by	O
the	O
same	O
methods	O
,	O
so	O
there	O
is	O
no	O
need	O
to	O
distinguish	O
between	O
them	O
,	O
and	O
we	O
will	O
use	O
the	O
term	O
“	O
model	B
selection	O
”	O
to	O
cover	O
both	O
meanings	O
.	O
we	O
will	O
refer	O
to	O
the	O
selection	O
of	O
a	O
covariance	B
function	I
and	O
its	O
parameters	O
as	O
training	O
of	O
a	O
gaussian	O
process.1	O
in	O
the	O
following	O
paragraphs	O
we	O
give	O
example	O
choices	O
of	O
parameterizations	O
of	O
distance	O
measures	O
for	O
stationary	O
covariance	B
functions	O
.	O
enable	O
interpretation	O
hyperparameters	B
training	O
covariance	B
functions	O
such	O
as	O
the	O
squared	B
exponential	I
can	O
be	O
parameterized	O
in	O
terms	O
of	O
hyperparameters	B
.	O
for	O
example	O
k	O
(	O
xp	O
,	O
xq	O
)	O
=	O
σ2	O
(	O
5.1	O
)	O
where	O
θ	O
=	O
(	O
{	O
m	O
}	O
,	O
σ2	O
n	O
)	O
>	O
is	O
a	O
vector	O
containing	O
all	O
the	O
hyperparameters,2	O
and	O
{	O
m	O
}	O
denotes	O
the	O
parameters	O
in	O
the	O
symmetric	O
matrix	B
m.	O
possible	O
choices	O
for	O
the	O
matrix	B
m	O
include	O
f	O
,	O
σ2	O
nδpq	O
,	O
f	O
exp	O
(	O
cid:0	O
)	O
−	O
1	O
2	O
(	O
xp	O
−	O
xq	O
)	O
>	O
m	O
(	O
xp	O
−	O
xq	O
)	O
(	O
cid:1	O
)	O
+	O
σ2	O
characteristic	O
length-scale	B
automatic	O
relevance	O
determination	O
m1	O
=	O
‘	O
−2i	O
,	O
m2	O
=	O
diag	O
(	O
‘	O
)	O
−2	O
,	O
m3	O
=	O
λλ	O
>	O
+	O
diag	O
(	O
‘	O
)	O
−2	O
,	O
(	O
5.2	O
)	O
where	O
‘	O
is	O
a	O
vector	O
of	O
positive	O
values	O
,	O
and	O
λ	O
is	O
a	O
d	O
×	O
k	O
matrix	B
,	O
k	O
<	O
d.	O
the	O
properties	O
of	O
functions	O
with	O
these	O
covariance	B
functions	O
depend	O
on	O
the	O
values	O
of	O
the	O
hyperparameters	B
.	O
for	O
many	O
covariance	B
functions	O
it	O
is	O
easy	O
to	O
interpret	O
the	O
meaning	O
of	O
the	O
hyperparameters	B
,	O
which	O
is	O
of	O
great	O
importance	O
when	O
trying	O
to	O
understand	O
your	O
data	O
.	O
for	O
the	O
squared	B
exponential	I
covariance	O
function	B
eq	O
.	O
(	O
5.1	O
)	O
with	O
distance	O
measure	B
m2	O
from	O
eq	O
.	O
(	O
5.2	O
)	O
,	O
the	O
‘	O
1	O
,	O
.	O
.	O
.	O
,	O
‘	O
d	O
hyperparameters	B
play	O
the	O
rˆole	O
of	O
characteristic	O
length-scales	O
;	O
loosely	O
speaking	O
,	O
how	O
far	O
do	O
you	O
need	O
to	O
move	O
(	O
along	O
a	O
particular	O
axis	O
)	O
in	O
input	O
space	O
for	O
the	O
function	B
values	O
to	O
be-	O
come	O
uncorrelated	O
.	O
such	O
a	O
covariance	B
function	I
implements	O
automatic	B
relevance	I
determination	O
(	O
ard	O
)	O
[	O
neal	O
,	O
1996	O
]	O
,	O
since	O
the	O
inverse	O
of	O
the	O
length-scale	B
deter-	O
mines	O
how	O
relevant	O
an	O
input	O
is	O
:	O
if	O
the	O
length-scale	B
has	O
a	O
very	O
large	O
value	O
,	O
the	O
1this	O
contrasts	O
the	O
use	O
of	O
the	O
word	O
in	O
the	O
svm	O
literature	O
,	O
where	O
“	O
training	O
”	O
usually	O
refers	O
to	O
ﬁnding	O
the	O
support	O
vectors	O
for	O
a	O
ﬁxed	O
kernel	B
.	O
2sometimes	O
the	O
noise	O
level	O
parameter	O
,	O
σ2	O
n	O
is	O
not	O
considered	O
a	O
hyperparameter	O
;	O
however	O
it	O
plays	O
an	O
analogous	O
role	O
and	O
is	O
treated	O
in	O
the	O
same	O
way	O
,	O
so	O
we	O
simply	O
consider	O
it	O
a	O
hyperpa-	O
rameter	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
5.1	O
the	O
model	B
selection	O
problem	O
107	O
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
figure	O
5.1	O
:	O
functions	O
with	O
two	O
dimensional	O
input	O
drawn	O
at	O
random	O
from	O
noise	O
free	O
squared	B
exponential	I
covariance	O
function	B
gaussian	O
processes	O
,	O
corresponding	O
to	O
the	O
three	O
diﬀerent	O
distance	O
measures	O
in	O
eq	O
.	O
(	O
5.2	O
)	O
respectively	O
.	O
the	O
parameters	O
were	O
:	O
(	O
a	O
)	O
‘	O
=	O
1	O
,	O
(	O
b	O
)	O
‘	O
=	O
(	O
1	O
,	O
3	O
)	O
>	O
,	O
and	O
(	O
c	O
)	O
λ	O
=	O
(	O
1	O
,	O
−1	O
)	O
>	O
,	O
‘	O
=	O
(	O
6	O
,	O
6	O
)	O
>	O
.	O
in	O
panel	O
(	O
a	O
)	O
the	O
two	O
inputs	O
are	O
equally	O
important	O
,	O
while	O
in	O
(	O
b	O
)	O
the	O
function	B
varies	O
less	O
rapidly	O
as	O
a	O
function	B
of	O
x2	O
than	O
x1	O
.	O
in	O
(	O
c	O
)	O
the	O
λ	O
column	O
gives	O
the	O
direction	O
of	O
most	O
rapid	O
variation	O
.	O
covariance	B
will	O
become	O
almost	O
independent	O
of	O
that	O
input	O
,	O
eﬀectively	O
removing	O
it	O
from	O
the	O
inference	O
.	O
ard	O
has	O
been	O
used	O
successfully	O
for	O
removing	O
irrelevant	O
input	O
by	O
several	O
authors	O
,	O
e.g	O
.	O
williams	O
and	O
rasmussen	O
[	O
1996	O
]	O
.	O
we	O
call	O
the	O
pa-	O
rameterization	O
of	O
m3	O
in	O
eq	O
.	O
(	O
5.2	O
)	O
the	O
factor	B
analysis	I
distance	O
due	O
to	O
the	O
analogy	O
with	O
the	O
(	O
unsupervised	O
)	O
factor	B
analysis	I
model	O
which	O
seeks	O
to	O
explain	O
the	O
data	O
through	O
a	O
low	O
rank	O
plus	O
diagonal	O
decomposition	O
.	O
for	O
high	O
dimensional	O
datasets	O
the	O
k	O
columns	O
of	O
the	O
λ	O
matrix	B
could	O
identify	O
a	O
few	O
directions	O
in	O
the	O
input	O
space	O
with	O
specially	O
high	O
“	O
relevance	O
”	O
,	O
and	O
their	O
lengths	O
give	O
the	O
inverse	O
characteristic	O
length-scale	B
for	O
those	O
directions	O
.	O
in	O
figure	O
5.1	O
we	O
show	O
functions	O
drawn	O
at	O
random	O
from	O
squared	B
exponential	I
covariance	O
function	B
gaussian	O
processes	O
,	O
for	O
diﬀerent	O
choices	O
of	O
m.	O
in	O
panel	O
(	O
a	O
)	O
we	O
get	O
an	O
isotropic	O
behaviour	O
.	O
in	O
panel	O
(	O
b	O
)	O
the	O
characteristic	O
length-scale	B
is	O
diﬀerent	O
along	O
the	O
two	O
input	O
axes	O
;	O
the	O
function	B
varies	O
rapidly	O
as	O
a	O
function	B
of	O
x1	O
,	O
but	O
less	O
rapidly	O
as	O
a	O
function	B
of	O
x2	O
.	O
in	O
panel	O
(	O
c	O
)	O
the	O
direction	O
of	O
most	O
rapid	O
variation	O
is	O
perpendicular	O
to	O
the	O
direction	O
(	O
1	O
,	O
1	O
)	O
.	O
as	O
this	O
ﬁgure	O
illustrates	O
,	O
factor	B
analysis	I
distance	O
−202−202−2−1012input	O
x1input	O
x2output	O
y−202−202−2−1012input	O
x1input	O
x2output	O
y−202−202−2−1012input	O
x1input	O
x2output	O
y	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
108	O
model	B
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
there	O
is	O
plenty	O
of	O
scope	O
for	O
variation	O
even	O
inside	O
a	O
single	O
family	O
of	O
covariance	B
functions	O
.	O
our	O
task	O
is	O
,	O
based	O
on	O
a	O
set	B
of	O
training	O
data	O
,	O
to	O
make	O
inferences	O
about	O
the	O
form	O
and	O
parameters	O
of	O
the	O
covariance	B
function	I
,	O
or	O
equivalently	O
,	O
about	O
the	O
relationships	O
in	O
the	O
data	O
.	O
it	O
should	O
be	O
clear	O
from	O
the	O
above	O
example	O
that	O
model	B
selection	O
is	O
essentially	O
open	O
ended	O
.	O
even	O
for	O
the	O
squared	B
exponential	I
covariance	O
function	B
,	O
there	O
is	O
a	O
huge	O
variety	O
of	O
possible	O
distance	O
measures	O
.	O
however	O
,	O
this	O
should	O
not	O
be	O
a	O
cause	O
for	O
despair	O
,	O
rather	O
seen	O
as	O
a	O
possibility	O
to	O
learn	O
.	O
it	O
requires	O
,	O
however	O
,	O
a	O
sys-	O
tematic	O
and	O
practical	O
approach	O
to	O
model	B
selection	O
.	O
in	O
a	O
nutshell	O
we	O
need	O
to	O
be	O
able	O
to	O
compare	O
two	O
(	O
or	O
more	O
)	O
methods	O
diﬀering	O
in	O
values	O
of	O
particular	O
param-	O
eters	O
,	O
or	O
the	O
shape	O
of	O
the	O
covariance	B
function	I
,	O
or	O
compare	O
a	O
gaussian	O
process	B
model	O
to	O
any	O
other	O
kind	O
of	O
model	B
.	O
although	O
there	O
are	O
endless	O
variations	O
in	O
the	O
suggestions	O
for	O
model	B
selection	O
in	O
the	O
literature	O
three	O
general	O
principles	O
cover	O
most	O
:	O
(	O
1	O
)	O
compute	O
the	O
probability	B
of	O
the	O
model	B
given	O
the	O
data	O
,	O
(	O
2	O
)	O
estimate	O
the	O
generalization	B
error	I
and	O
(	O
3	O
)	O
bound	O
the	O
generalization	B
error	I
.	O
we	O
use	O
the	O
term	O
generalization	B
error	I
to	O
mean	O
the	O
average	O
error	B
on	O
unseen	O
test	O
examples	O
(	O
from	O
the	O
same	O
distribution	O
as	O
the	O
training	O
cases	O
)	O
.	O
note	O
that	O
the	O
training	O
error	B
is	O
usually	O
a	O
poor	O
proxy	O
for	O
the	O
generalization	B
error	I
,	O
since	O
the	O
model	B
may	O
ﬁt	O
the	O
noise	O
in	O
the	O
training	O
set	B
(	O
over-ﬁt	O
)	O
,	O
leading	O
to	O
low	O
training	O
error	B
but	O
poor	O
generalization	B
performance	O
.	O
in	O
the	O
next	O
section	O
we	O
describe	O
the	O
bayesian	O
view	O
on	O
model	B
selection	O
,	O
which	O
involves	O
the	O
computation	O
of	O
the	O
probability	B
of	O
the	O
model	B
given	O
the	O
data	O
,	O
based	O
on	O
the	O
marginal	B
likelihood	I
.	O
in	O
section	O
5.3	O
we	O
cover	O
cross-validation	B
,	O
which	O
estimates	O
the	O
generalization	B
performance	O
.	O
these	O
two	O
paradigms	O
are	O
applied	O
to	O
gaussian	O
process	B
models	O
in	O
the	O
remainder	O
of	O
this	O
chapter	O
.	O
the	O
probably	O
approximately	O
correct	O
(	O
pac	O
)	O
framework	O
is	O
an	O
example	O
of	O
a	O
bound	O
on	O
the	O
gen-	O
eralization	O
error	B
,	O
and	O
is	O
covered	O
in	O
section	O
7.4.2	O
.	O
5.2	O
bayesian	O
model	B
selection	O
in	O
this	O
section	O
we	O
give	O
a	O
short	O
outline	O
description	O
of	O
the	O
main	O
ideas	O
in	O
bayesian	O
model	B
selection	O
.	O
the	O
discussion	O
will	O
be	O
general	O
,	O
but	O
focusses	O
on	O
issues	O
which	O
will	O
be	O
relevant	O
for	O
the	O
speciﬁc	O
treatment	O
of	O
gaussian	O
process	B
models	O
for	O
regression	B
in	O
section	O
5.4	O
and	O
classiﬁcation	B
in	O
section	O
5.5.	O
it	O
is	O
common	O
to	O
use	O
a	O
hierarchical	O
speciﬁcation	O
of	O
models	O
.	O
at	O
the	O
lowest	O
level	O
are	O
the	O
parameters	O
,	O
w.	O
for	O
example	O
,	O
the	O
parameters	O
could	O
be	O
the	O
parameters	O
in	O
a	O
linear	B
model	O
,	O
or	O
the	O
weights	O
in	O
a	O
neural	B
network	I
model	O
.	O
at	O
the	O
second	O
level	O
are	O
hyperparameters	B
θ	O
which	O
control	O
the	O
distribution	O
of	O
the	O
parameters	O
at	O
the	O
bottom	O
level	O
.	O
for	O
example	O
the	O
“	O
weight	O
decay	O
”	O
term	O
in	O
a	O
neural	B
network	I
,	O
or	O
the	O
“	O
ridge	B
”	O
term	O
in	O
ridge	B
regression	I
are	O
hyperparameters	B
.	O
at	O
the	O
top	O
level	O
we	O
may	O
have	O
a	O
(	O
discrete	O
)	O
set	B
of	O
possible	O
model	B
structures	O
,	O
hi	O
,	O
under	O
consideration	O
.	O
we	O
will	O
ﬁrst	O
give	O
a	O
“	O
mechanistic	O
”	O
description	O
of	O
the	O
computations	O
needed	O
for	O
bayesian	O
inference	O
,	O
and	O
continue	O
with	O
a	O
discussion	O
providing	O
the	O
intuition	O
about	O
what	O
is	O
going	O
on	O
.	O
inference	O
takes	O
place	O
one	O
level	O
at	O
a	O
time	O
,	O
by	O
applying	O
hierarchical	O
models	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
5.2	O
bayesian	O
model	B
selection	O
the	O
rules	O
of	O
probability	B
theory	O
,	O
see	B
e.g	O
.	O
mackay	O
[	O
1992b	O
]	O
for	O
this	O
framework	O
and	O
mackay	O
[	O
1992a	O
]	O
for	O
the	O
context	O
of	O
neural	O
networks	O
.	O
at	O
the	O
bottom	O
level	O
,	O
the	O
posterior	O
over	O
the	O
parameters	O
is	O
given	O
by	O
bayes	O
’	O
rule	O
p	O
(	O
w|y	O
,	O
x	O
,	O
θ	O
,	O
hi	O
)	O
=	O
p	O
(	O
y|x	O
,	O
w	O
,	O
hi	O
)	O
p	O
(	O
w|θ	O
,	O
hi	O
)	O
p	O
(	O
y|x	O
,	O
θ	O
,	O
hi	O
)	O
,	O
(	O
5.3	O
)	O
109	O
level	O
1	O
inference	O
where	O
p	O
(	O
y|x	O
,	O
w	O
,	O
hi	O
)	O
is	O
the	O
likelihood	B
and	O
p	O
(	O
w|θ	O
,	O
hi	O
)	O
is	O
the	O
parameter	O
prior	O
.	O
the	O
prior	O
encodes	O
as	O
a	O
probability	B
distribution	O
our	O
knowledge	O
about	O
the	O
pa-	O
rameters	O
prior	O
to	O
seeing	O
the	O
data	O
.	O
if	O
we	O
have	O
only	O
vague	O
prior	O
information	O
about	O
the	O
parameters	O
,	O
then	O
the	O
prior	O
distribution	O
is	O
chosen	O
to	O
be	O
broad	O
to	O
reﬂect	O
this	O
.	O
the	O
posterior	O
combines	O
the	O
information	O
from	O
the	O
prior	O
and	O
the	O
data	O
(	O
through	O
the	O
likelihood	B
)	O
.	O
the	O
normalizing	O
constant	O
in	O
the	O
denominator	O
of	O
eq	O
.	O
(	O
5.3	O
)	O
p	O
(	O
y|x	O
,	O
θ	O
,	O
hi	O
)	O
is	O
independent	O
of	O
the	O
parameters	O
,	O
and	O
called	O
the	O
marginal	B
likelihood	I
(	O
or	O
evidence	B
)	O
,	O
and	O
is	O
given	O
by	O
p	O
(	O
y|x	O
,	O
θ	O
,	O
hi	O
)	O
=	O
p	O
(	O
y|x	O
,	O
w	O
,	O
hi	O
)	O
p	O
(	O
w|θ	O
,	O
hi	O
)	O
dw	O
.	O
(	O
5.4	O
)	O
z	O
z	O
at	O
the	O
next	O
level	O
,	O
we	O
analogously	O
express	O
the	O
posterior	O
over	O
the	O
hyperparam-	O
eters	O
,	O
where	O
the	O
marginal	B
likelihood	I
from	O
the	O
ﬁrst	O
level	O
plays	O
the	O
rˆole	O
of	O
the	O
likelihood	B
p	O
(	O
θ|y	O
,	O
x	O
,	O
hi	O
)	O
=	O
p	O
(	O
y|x	O
,	O
θ	O
,	O
hi	O
)	O
p	O
(	O
θ|hi	O
)	O
(	O
5.5	O
)	O
where	O
p	O
(	O
θ|hi	O
)	O
is	O
the	O
hyper-prior	O
(	O
the	O
prior	O
for	O
the	O
hyperparameters	B
)	O
.	O
the	O
normalizing	O
constant	O
is	O
given	O
by	O
p	O
(	O
y|x	O
,	O
hi	O
)	O
,	O
p	O
(	O
y|x	O
,	O
hi	O
)	O
=	O
p	O
(	O
y|x	O
,	O
θ	O
,	O
hi	O
)	O
p	O
(	O
θ|hi	O
)	O
dθ	O
.	O
(	O
5.6	O
)	O
level	O
2	O
inference	O
at	O
the	O
top	O
level	O
,	O
we	O
compute	O
the	O
posterior	O
for	O
the	O
model	B
level	O
3	O
inference	O
,	O
(	O
5.7	O
)	O
p	O
(	O
y|x	O
)	O
where	O
p	O
(	O
y|x	O
)	O
=	O
p	O
p	O
(	O
hi|y	O
,	O
x	O
)	O
=	O
p	O
(	O
y|x	O
,	O
hi	O
)	O
p	O
(	O
hi	O
)	O
i	O
p	O
(	O
y|x	O
,	O
hi	O
)	O
p	O
(	O
hi	O
)	O
.	O
we	O
note	O
that	O
the	O
implementation	O
of	O
bayesian	O
inference	O
calls	O
for	O
the	O
evaluation	O
of	O
several	O
integrals	B
.	O
depending	O
on	O
the	O
details	O
of	O
the	O
models	O
,	O
these	O
integrals	B
may	O
or	O
may	O
not	O
be	O
analytically	O
tractable	O
and	O
in	O
general	O
one	O
may	O
have	O
to	O
resort	O
to	O
analytical	O
approximations	O
or	O
markov	O
chain	O
monte	O
carlo	O
(	O
mcmc	O
)	O
methods	O
.	O
in	O
practice	O
,	O
especially	O
the	O
evaluation	O
of	O
the	O
integral	O
in	O
eq	O
.	O
(	O
5.6	O
)	O
may	O
be	O
diﬃcult	O
,	O
and	O
as	O
an	O
approximation	O
one	O
may	O
shy	O
away	O
from	O
using	O
the	O
hyperparameter	O
posterior	O
in	O
eq	O
.	O
(	O
5.5	O
)	O
,	O
and	O
instead	O
maximize	O
the	O
marginal	B
likelihood	I
in	O
eq	O
.	O
(	O
5.4	O
)	O
w.r.t	O
.	O
the	O
hyperparameters	B
,	O
θ.	O
this	O
approximation	O
is	O
known	O
as	O
type	O
ii	O
maximum	B
likelihood	I
(	O
ml-ii	O
)	O
.	O
of	O
course	O
,	O
one	O
should	O
be	O
careful	O
with	O
such	O
an	O
optimization	O
step	O
,	O
since	O
it	O
opens	O
up	O
the	O
possibility	O
of	O
overﬁtting	O
,	O
especially	O
if	O
there	O
are	O
many	O
hyperparameters	B
.	O
the	O
integral	O
in	O
eq	O
.	O
(	O
5.6	O
)	O
can	O
then	O
be	O
approximated	O
using	O
a	O
local	O
expansion	O
around	O
the	O
maximum	O
(	O
the	O
laplace	O
approximation	O
)	O
.	O
this	O
approximation	O
will	O
be	O
good	O
if	O
the	O
posterior	O
for	O
θ	O
is	O
fairly	O
well	O
peaked	O
,	O
which	O
is	O
more	O
often	O
the	O
case	O
for	O
the	O
ml-ii	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
110	O
model	B
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
figure	O
5.2	O
:	O
the	O
marginal	B
likelihood	I
p	O
(	O
y|x	O
,	O
hi	O
)	O
is	O
the	O
probability	B
of	O
the	O
data	O
,	O
given	O
the	O
model	B
.	O
the	O
number	O
of	O
data	O
points	O
n	O
and	O
the	O
inputs	O
x	O
are	O
ﬁxed	O
,	O
and	O
not	O
shown	O
.	O
the	O
horizontal	O
axis	O
is	O
an	O
idealized	O
representation	O
of	O
all	O
possible	O
vectors	O
of	O
targets	O
y.	O
the	O
marginal	B
likelihood	I
for	O
models	O
of	O
three	O
diﬀerent	O
complexities	O
are	O
shown	O
.	O
note	O
,	O
that	O
since	O
the	O
marginal	B
likelihood	I
is	O
a	O
probability	B
distribution	O
,	O
it	O
must	O
normalize	O
to	O
unity	O
.	O
for	O
a	O
particular	O
dataset	B
indicated	O
by	O
y	O
and	O
a	O
dotted	O
line	O
,	O
the	O
marginal	B
likelihood	I
prefers	O
a	O
model	B
of	O
intermediate	O
complexity	O
over	O
too	O
simple	O
or	O
too	O
complex	O
alternatives	O
.	O
hyperparameters	B
than	O
for	O
the	O
parameters	O
themselves	O
,	O
see	B
mackay	O
[	O
1999	O
]	O
for	O
an	O
illuminating	O
discussion	O
.	O
the	O
prior	O
over	O
models	O
hi	O
in	O
eq	O
.	O
(	O
5.7	O
)	O
is	O
often	O
taken	O
to	O
be	O
ﬂat	O
,	O
so	O
that	O
a	O
priori	O
we	O
do	O
not	O
favour	O
one	O
model	B
over	O
another	O
.	O
in	O
this	O
case	O
,	O
the	O
probability	B
for	O
the	O
model	B
is	O
proportional	O
to	O
the	O
expression	O
from	O
eq	O
.	O
(	O
5.6	O
)	O
.	O
it	O
is	O
primarily	O
the	O
marginal	B
likelihood	I
from	O
eq	O
.	O
(	O
5.4	O
)	O
involving	O
the	O
integral	O
over	O
the	O
parameter	O
space	O
which	O
distinguishes	O
the	O
bayesian	O
scheme	O
of	O
inference	O
from	O
other	O
schemes	O
based	O
on	O
optimization	O
.	O
it	O
is	O
a	O
property	O
of	O
the	O
marginal	B
likelihood	I
that	O
it	O
automatically	O
incorporates	O
a	O
trade-oﬀ	O
between	O
model	B
ﬁt	O
and	O
model	B
complexity	O
.	O
this	O
is	O
the	O
reason	O
why	O
the	O
marginal	B
likelihood	I
is	O
valuable	O
in	O
solving	O
the	O
model	B
selection	O
problem	O
.	O
in	O
figure	O
5.2	O
we	O
show	O
a	O
schematic	O
of	O
the	O
behaviour	O
of	O
the	O
marginal	B
likelihood	I
for	O
three	O
diﬀerent	O
model	B
complexities	O
.	O
let	O
the	O
number	O
of	O
data	O
points	O
n	O
and	O
the	O
inputs	O
x	O
be	O
ﬁxed	O
;	O
the	O
horizontal	O
axis	O
is	O
an	O
idealized	O
representation	O
of	O
all	O
possible	O
vectors	O
of	O
targets	O
y	O
,	O
and	O
the	O
vertical	O
axis	O
plots	O
the	O
marginal	B
likelihood	I
p	O
(	O
y|x	O
,	O
hi	O
)	O
.	O
a	O
simple	O
model	B
can	O
only	O
account	O
for	O
a	O
limited	O
range	O
of	O
possible	O
sets	O
of	O
target	O
values	O
,	O
but	O
since	O
the	O
marginal	B
likelihood	I
is	O
a	O
probability	B
distribution	O
over	O
y	O
it	O
must	O
normalize	O
to	O
unity	O
,	O
and	O
therefore	O
the	O
data	O
sets	O
which	O
the	O
model	B
does	O
account	O
for	O
have	O
a	O
large	O
value	O
of	O
the	O
marginal	B
likelihood	I
.	O
conversely	O
for	O
a	O
complex	O
model	B
:	O
it	O
is	O
capable	O
of	O
accounting	O
for	O
a	O
wider	O
range	O
of	O
data	O
sets	O
,	O
and	O
consequently	O
the	O
marginal	B
likelihood	I
doesn	O
’	O
t	O
attain	O
such	O
large	O
values	O
as	O
for	O
the	O
simple	O
model	B
.	O
for	O
example	O
,	O
the	O
simple	O
model	B
could	O
be	O
a	O
linear	B
model	O
,	O
and	O
the	O
complex	O
model	B
a	O
large	O
neural	B
network	I
.	O
the	O
ﬁgure	O
illustrates	O
why	O
the	O
marginal	B
likelihood	I
doesn	O
’	O
t	O
simply	O
favour	O
the	O
models	O
that	O
ﬁt	O
the	O
training	O
data	O
the	O
best	O
.	O
this	O
eﬀect	O
is	O
called	O
occam	O
’	O
s	O
razor	O
after	O
william	O
of	O
occam	O
1285-1349	O
,	O
whose	O
principle	O
:	O
“	O
plurality	O
should	O
not	O
be	O
assumed	O
without	O
necessity	O
”	O
he	O
used	O
to	O
encourage	O
simplicity	O
in	O
explanations	O
.	O
see	B
also	O
rasmussen	O
and	O
ghahramani	O
[	O
2001	O
]	O
for	O
an	O
investigation	O
into	O
occam	O
’	O
s	O
razor	O
in	O
statistical	O
models	O
.	O
occam	O
’	O
s	O
razor	O
ymarginal	O
likelihood	B
,	O
p	O
(	O
y|x	O
,	O
hi	O
)	O
all	O
possible	O
data	O
setssimpleintermediatecomplex	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
5.3	O
cross-validation	B
notice	O
that	O
the	O
trade-oﬀ	O
between	O
data-ﬁt	O
and	O
model	B
complexity	O
is	O
automatic	O
;	O
there	O
is	O
no	O
need	O
to	O
set	B
a	O
parameter	O
externally	O
to	O
ﬁx	O
the	O
trade-oﬀ	O
.	O
do	O
not	O
confuse	O
the	O
automatic	O
occam	O
’	O
s	O
razor	O
principle	O
with	O
the	O
use	O
of	O
priors	O
in	O
the	O
bayesian	O
method	O
.	O
even	O
if	O
the	O
priors	O
are	O
“	O
ﬂat	O
”	O
over	O
complexity	O
,	O
the	O
marginal	B
likelihood	I
will	O
still	O
tend	O
to	O
favour	O
the	O
least	O
complex	O
model	B
able	O
to	O
explain	O
the	O
data	O
.	O
thus	O
,	O
a	O
model	B
complexity	O
which	O
is	O
well	O
suited	O
to	O
the	O
data	O
can	O
be	O
selected	O
using	O
the	O
marginal	B
likelihood	I
.	O
in	O
the	O
preceding	O
paragraphs	O
we	O
have	O
thought	O
of	O
the	O
speciﬁcation	O
of	O
a	O
model	B
as	O
the	O
model	B
structure	O
as	O
well	O
as	O
the	O
parameters	O
of	O
the	O
priors	O
,	O
etc	O
.	O
if	O
it	O
is	O
unclear	O
how	O
to	O
set	B
some	O
of	O
the	O
parameters	O
of	O
the	O
prior	O
,	O
one	O
can	O
treat	O
these	O
as	O
hyperparameters	B
,	O
and	O
do	O
model	B
selection	O
to	O
determine	O
how	O
to	O
set	B
them	O
.	O
at	O
the	O
same	O
time	O
it	O
should	O
be	O
emphasized	O
that	O
the	O
priors	O
correspond	O
to	O
(	O
proba-	O
bilistic	O
)	O
assumptions	O
about	O
the	O
data	O
.	O
if	O
the	O
priors	O
are	O
grossly	O
at	O
odds	O
with	O
the	O
distribution	O
of	O
the	O
data	O
,	O
inference	O
will	O
still	O
take	O
place	O
under	O
the	O
assumptions	O
encoded	O
by	O
the	O
prior	O
,	O
see	B
the	O
step-function	O
example	O
in	O
section	O
5.4.3.	O
to	O
avoid	O
this	O
situation	O
,	O
one	O
should	O
be	O
careful	O
not	O
to	O
employ	O
priors	O
which	O
are	O
too	O
narrow	O
,	O
ruling	O
out	O
reasonable	O
explanations	O
of	O
the	O
data.3	O
5.3	O
cross-validation	B
in	O
this	O
section	O
we	O
consider	O
how	O
to	O
use	O
methods	O
of	O
cross-validation	B
(	O
cv	O
)	O
for	O
model	B
selection	O
.	O
the	O
basic	O
idea	O
is	O
to	O
split	O
the	O
training	O
set	B
into	O
two	O
disjoint	O
sets	O
,	O
one	O
which	O
is	O
actually	O
used	O
for	O
training	O
,	O
and	O
the	O
other	O
,	O
the	O
validation	O
set	B
,	O
which	O
is	O
used	O
to	O
monitor	O
performance	O
.	O
the	O
performance	O
on	O
the	O
validation	O
set	B
is	O
used	O
as	O
a	O
proxy	O
for	O
the	O
generalization	B
error	I
and	O
model	B
selection	O
is	O
carried	O
out	O
using	O
this	O
measure	B
.	O
in	O
practice	O
a	O
drawback	O
of	O
hold-out	O
method	O
is	O
that	O
only	O
a	O
fraction	O
of	O
the	O
full	O
data	O
set	B
can	O
be	O
used	O
for	O
training	O
,	O
and	O
that	O
if	O
the	O
validation	O
set	B
it	O
small	O
,	O
the	O
performance	O
estimate	O
obtained	O
may	O
have	O
large	O
variance	O
.	O
to	O
minimize	O
these	O
problems	O
,	O
cv	O
is	O
almost	O
always	O
used	O
in	O
the	O
k-fold	O
cross-validation	B
setting	O
:	O
the	O
data	O
is	O
split	O
into	O
k	O
disjoint	O
,	O
equally	O
sized	O
subsets	O
;	O
validation	O
is	O
done	O
on	O
a	O
single	O
subset	O
and	O
training	O
is	O
done	O
using	O
the	O
union	O
of	O
the	O
remaining	O
k	O
−	O
1	O
subsets	O
,	O
the	O
entire	O
procedure	O
being	O
repeated	O
k	O
times	O
,	O
each	O
time	O
with	O
a	O
diﬀerent	O
subset	O
for	O
validation	O
.	O
thus	O
,	O
a	O
large	O
fraction	O
of	O
the	O
data	O
can	O
be	O
used	O
for	O
training	O
,	O
and	O
all	O
cases	O
appear	O
as	O
validation	O
cases	O
.	O
the	O
price	O
is	O
that	O
k	O
models	O
must	O
be	O
trained	O
instead	O
of	O
one	O
.	O
typical	O
values	O
for	O
k	O
are	O
in	O
the	O
range	O
3	O
to	O
10.	O
an	O
extreme	O
case	O
of	O
k-fold	O
cross-validation	B
is	O
obtained	O
for	O
k	O
=	O
n	O
,	O
the	O
number	O
of	O
training	O
cases	O
,	O
also	O
known	O
as	O
leave-one-out	B
cross-validation	I
(	O
loo-cv	O
)	O
.	O
of-	O
ten	O
the	O
computational	O
cost	O
of	O
loo-cv	O
(	O
“	O
training	O
”	O
n	O
models	O
)	O
is	O
prohibitive	O
,	O
but	O
in	O
certain	O
cases	O
,	O
such	O
as	O
gaussian	O
process	B
regression	O
,	O
there	O
are	O
computational	O
shortcuts	O
.	O
3this	O
is	O
known	O
as	O
cromwell	O
’	O
s	O
dictum	O
[	O
lindley	O
,	O
1985	O
]	O
after	O
oliver	O
cromwell	O
who	O
on	O
august	O
5th	O
,	O
1650	O
wrote	O
to	O
the	O
synod	O
of	O
the	O
church	O
of	O
scotland	O
:	O
“	O
i	O
beseech	O
you	O
,	O
in	O
the	O
bowels	O
of	O
christ	O
,	O
consider	O
it	O
possible	O
that	O
you	O
are	O
mistaken.	O
”	O
111	O
automatic	O
trade-oﬀ	O
cross-validation	B
k-fold	O
cross-validation	B
leave-one-out	O
cross-validation	B
(	O
loo-cv	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
112	O
model	B
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
other	O
loss	B
functions	O
model	B
parameters	O
cross-validation	B
can	O
be	O
used	O
with	O
any	O
loss	B
function	I
.	O
although	O
the	O
squared	B
error	O
loss	B
is	O
by	O
far	O
the	O
most	O
common	O
for	O
regression	B
,	O
there	O
is	O
no	O
reason	O
not	O
to	O
allow	O
other	O
loss	B
functions	O
.	O
for	O
probabilistic	B
models	O
such	O
as	O
gaussian	O
processes	O
it	O
is	O
natural	O
to	O
consider	O
also	O
cross-validation	B
using	O
the	O
negative	O
log	O
probabil-	O
ity	B
loss	O
.	O
craven	O
and	O
wahba	O
[	O
1979	O
]	O
describe	O
a	O
variant	O
of	O
cross-validation	B
using	O
squared	B
error	O
known	O
as	O
generalized	B
cross-validation	O
which	O
gives	O
diﬀerent	O
weight-	O
ings	O
to	O
diﬀerent	O
datapoints	O
so	O
as	O
to	O
achieve	O
certain	O
invariance	O
properites	O
.	O
see	B
wahba	O
[	O
1990	O
,	O
sec	O
.	O
4.3	O
]	O
for	O
further	O
details	O
.	O
5.4	O
model	B
selection	O
for	O
gp	O
regression	B
we	O
apply	O
bayesian	O
inference	O
in	O
section	O
5.4.1	O
and	O
cross-validation	B
in	O
section	O
5.4.2	O
to	O
gaussian	O
process	B
regression	O
with	O
gaussian	O
noise	O
.	O
we	O
conclude	O
in	O
section	O
5.4.3	O
with	O
some	O
more	O
detailed	O
examples	O
of	O
how	O
one	O
can	O
use	O
the	O
model	B
selection	O
principles	O
to	O
tailor	O
covariance	B
functions	O
.	O
5.4.1	O
marginal	B
likelihood	I
bayesian	O
principles	O
provide	O
a	O
persuasive	O
and	O
consistent	O
framework	O
for	O
inference	O
.	O
unfortunately	O
,	O
for	O
most	O
interesting	O
models	O
for	O
machine	O
learning	B
,	O
the	O
required	O
computations	O
(	O
integrals	B
over	O
parameter	O
space	O
)	O
are	O
analytically	O
intractable	O
,	O
and	O
good	O
approximations	O
are	O
not	O
easily	O
derived	O
.	O
gaussian	O
process	B
regression	O
mod-	O
els	O
with	O
gaussian	O
noise	O
are	O
a	O
rare	O
exception	O
:	O
integrals	B
over	O
the	O
parameters	O
are	O
analytically	O
tractable	O
and	O
at	O
the	O
same	O
time	O
the	O
models	O
are	O
very	O
ﬂexible	O
.	O
in	O
this	O
section	O
we	O
ﬁrst	O
apply	O
the	O
general	O
bayesian	O
inference	O
principles	O
from	O
section	O
5.2	O
to	O
the	O
speciﬁc	O
gaussian	O
process	B
model	O
,	O
in	O
the	O
simpliﬁed	O
form	O
where	O
hy-	O
perparameters	O
are	O
optimized	O
over	O
.	O
we	O
derive	O
the	O
expressions	O
for	O
the	O
marginal	B
likelihood	I
and	O
interpret	O
these	O
.	O
since	O
a	O
gaussian	O
process	B
model	O
is	O
a	O
non-parametric	B
model	O
,	O
it	O
may	O
not	O
be	O
immediately	O
obvious	O
what	O
the	O
parameters	O
of	O
the	O
model	B
are	O
.	O
generally	O
,	O
one	O
may	O
regard	O
the	O
noise-free	O
latent	O
function	O
values	O
at	O
the	O
training	O
inputs	O
f	O
as	O
the	O
parameters	O
.	O
the	O
more	O
training	O
cases	O
there	O
are	O
,	O
the	O
more	O
parameters	O
.	O
using	O
the	O
weight-space	O
view	O
,	O
developed	O
in	O
section	O
2.1	O
,	O
one	O
may	O
equivalently	O
think	O
of	O
the	O
parameters	O
as	O
being	O
the	O
weights	O
of	O
the	O
linear	B
model	O
which	O
uses	O
the	O
basis-functions	O
φ	O
,	O
which	O
can	O
be	O
chosen	O
as	O
the	O
eigenfunctions	O
of	O
the	O
covariance	B
function	I
.	O
of	O
course	O
,	O
we	O
have	O
seen	O
that	O
this	O
view	O
is	O
inconvenient	O
for	O
nondegen-	O
erate	O
covariance	B
functions	O
,	O
since	O
these	O
would	O
then	O
have	O
an	O
inﬁnite	O
number	O
of	O
weights	O
.	O
we	O
proceed	O
by	O
applying	O
eq	O
.	O
(	O
5.3	O
)	O
and	O
eq	O
.	O
(	O
5.4	O
)	O
for	O
the	O
1st	O
level	O
of	O
inference—	O
which	O
we	O
ﬁnd	O
that	O
we	O
have	O
already	O
done	O
back	O
in	O
chapter	O
2	O
!	O
the	O
predictive	B
dis-	O
tribution	O
from	O
eq	O
.	O
(	O
5.3	O
)	O
is	O
given	O
for	O
the	O
weight-space	O
view	O
in	O
eq	O
.	O
(	O
2.11	O
)	O
and	O
eq	O
.	O
(	O
2.12	O
)	O
and	O
equivalently	O
for	O
the	O
function-space	O
view	O
in	O
eq	O
.	O
(	O
2.22	O
)	O
.	O
the	O
marginal	B
likelihood	I
(	O
or	O
evidence	B
)	O
from	O
eq	O
.	O
(	O
5.4	O
)	O
was	O
computed	O
in	O
eq	O
.	O
(	O
2.30	O
)	O
,	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
5.4	O
model	B
selection	O
for	O
gp	O
regression	B
113	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
5.3	O
:	O
panel	O
(	O
a	O
)	O
shows	O
a	O
decomposition	O
of	O
the	O
log	O
marginal	O
likelihood	B
into	O
its	O
constituents	O
:	O
data-ﬁt	O
and	O
complexity	O
penalty	O
,	O
as	O
a	O
function	B
of	O
the	O
characteristic	O
length-scale	B
.	O
the	O
training	O
data	O
is	O
drawn	O
from	O
a	O
gaussian	O
process	B
with	O
se	O
covariance	B
function	I
and	O
parameters	O
(	O
‘	O
,	O
σf	O
,	O
σn	O
)	O
=	O
(	O
1	O
,	O
1	O
,	O
0.1	O
)	O
,	O
the	O
same	O
as	O
in	O
figure	O
2.5	O
,	O
and	O
we	O
are	O
ﬁtting	O
only	O
the	O
length-scale	B
parameter	O
‘	O
(	O
the	O
two	O
other	O
parameters	O
have	O
been	O
set	B
in	O
accordance	O
with	O
the	O
generating	O
process	B
)	O
.	O
panel	O
(	O
b	O
)	O
shows	O
the	O
log	O
marginal	O
likelihood	B
as	O
a	O
function	B
of	O
the	O
characteristic	O
length-scale	B
for	O
diﬀerent	O
sizes	O
of	O
training	O
sets	O
.	O
also	O
shown	O
,	O
are	O
the	O
95	O
%	O
conﬁdence	O
intervals	O
for	O
the	O
posterior	O
length-scales	O
.	O
and	O
we	O
re-state	O
the	O
result	O
here	O
log	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
−1	O
2	O
y	O
>	O
k−1	O
y	O
y	O
−	O
1	O
2	O
log	O
|ky|	O
−	O
n	O
2	O
log	O
2π	O
,	O
(	O
5.8	O
)	O
where	O
ky	O
=	O
kf	O
+	O
σ2	O
ni	O
is	O
the	O
covariance	B
matrix	I
for	O
the	O
noisy	O
targets	O
y	O
(	O
and	O
kf	O
is	O
the	O
covariance	B
matrix	I
for	O
the	O
noise-free	O
latent	O
f	O
)	O
,	O
and	O
we	O
now	O
explicitly	O
write	O
the	O
marginal	B
likelihood	I
conditioned	O
on	O
the	O
hyperparameters	B
(	O
the	O
parameters	O
of	O
the	O
covariance	B
function	I
)	O
θ.	O
from	O
this	O
perspective	O
it	O
becomes	O
clear	O
why	O
we	O
call	O
eq	O
.	O
(	O
5.8	O
)	O
the	O
log	O
marginal	O
likelihood	B
,	O
since	O
it	O
is	O
obtained	O
through	O
marginaliza-	O
tion	O
over	O
the	O
latent	O
function	O
.	O
otherwise	O
,	O
if	O
one	O
thinks	O
entirely	O
in	O
terms	O
of	O
the	O
function-space	O
view	O
,	O
the	O
term	O
“	O
marginal	B
”	O
may	O
appear	O
a	O
bit	O
mysterious	O
,	O
and	O
similarly	O
the	O
“	O
hyper	O
”	O
from	O
the	O
θ	O
parameters	O
of	O
the	O
covariance	B
function.4	O
the	O
three	O
terms	O
of	O
the	O
marginal	B
likelihood	I
in	O
eq	O
.	O
(	O
5.8	O
)	O
have	O
readily	O
inter-	O
pretable	O
rˆoles	O
:	O
the	O
only	O
term	O
involving	O
the	O
observed	O
targets	O
is	O
the	O
data-ﬁt	O
−y	O
>	O
k−1	O
y	O
y/2	O
;	O
log	O
|ky|/2	O
is	O
the	O
complexity	O
penalty	O
depending	O
only	O
on	O
the	O
co-	O
variance	O
function	B
and	O
the	O
inputs	O
and	O
n	O
log	O
(	O
2π	O
)	O
/2	O
is	O
a	O
normalization	O
constant	O
.	O
in	O
figure	O
5.3	O
(	O
a	O
)	O
we	O
illustrate	O
this	O
breakdown	O
of	O
the	O
log	O
marginal	O
likelihood	B
.	O
the	O
data-ﬁt	O
decreases	O
monotonically	O
with	O
the	O
length-scale	B
,	O
since	O
the	O
model	B
be-	O
comes	O
less	O
and	O
less	O
ﬂexible	O
.	O
the	O
negative	O
complexity	O
penalty	O
increases	O
with	O
the	O
length-scale	B
,	O
because	O
the	O
model	B
gets	O
less	O
complex	O
with	O
growing	O
length-scale	B
.	O
the	O
marginal	B
likelihood	I
itself	O
peaks	O
at	O
a	O
value	O
close	O
to	O
1.	O
for	O
length-scales	O
somewhat	O
longer	O
than	O
1	O
,	O
the	O
marginal	B
likelihood	I
decreases	O
rapidly	O
(	O
note	O
the	O
4another	O
reason	O
that	O
we	O
like	O
to	O
stick	O
to	O
the	O
term	O
“	O
marginal	B
likelihood	I
”	O
is	O
that	O
it	O
is	O
the	O
likelihood	B
of	O
a	O
non-parametric	B
model	O
,	O
i.e	O
.	O
a	O
model	B
which	O
requires	O
access	O
to	O
all	O
the	O
training	O
data	O
when	O
making	O
predictions	O
;	O
this	O
contrasts	O
the	O
situation	O
for	O
a	O
parametric	B
model	O
,	O
which	O
“	O
absorbs	O
”	O
the	O
information	O
from	O
the	O
training	O
data	O
into	O
its	O
(	O
posterior	O
)	O
parameter	O
(	O
distribution	O
)	O
.	O
this	O
diﬀerence	O
makes	O
the	O
two	O
“	O
likelihoods	O
”	O
behave	O
quite	O
diﬀerently	O
as	O
a	O
function	B
of	O
θ.	O
marginal	B
likelihood	I
interpretation	O
100−100−80−60−40−2002040log	O
probabilitycharacteristic	O
lengthscaleminus	O
complexity	O
penaltydata	O
fitmarginal	O
likelihood100−100−80−60−40−20020characteristic	O
lengthscalelog	O
marginal	B
likelihood95	O
%	O
conf	O
int	O
82155	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
114	O
model	B
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
figure	O
5.4	O
:	O
contour	O
plot	O
showing	O
the	O
log	O
marginal	O
likelihood	B
as	O
a	O
function	B
of	O
the	O
characteristic	O
length-scale	B
and	O
the	O
noise	O
level	O
,	O
for	O
the	O
same	O
data	O
as	O
in	O
figure	O
2.5	O
and	O
figure	O
5.3.	O
the	O
signal	O
variance	O
hyperparameter	O
was	O
set	B
to	O
σ2	O
f	O
=	O
1.	O
the	O
optimum	O
is	O
close	O
to	O
the	O
parameters	O
used	O
when	O
generating	O
the	O
data	O
.	O
note	O
,	O
the	O
two	O
ridges	O
,	O
one	O
for	O
small	O
noise	O
and	O
length-scale	B
‘	O
=	O
0.4	O
and	O
another	O
for	O
long	O
length-scale	B
and	O
noise	O
σ2	O
n	O
=	O
1.	O
the	O
contour	O
lines	O
spaced	O
2	O
units	O
apart	O
in	O
log	O
probability	O
density	O
.	O
log	O
scale	O
!	O
)	O
,	O
due	O
to	O
the	O
poor	O
ability	O
of	O
the	O
model	B
to	O
explain	O
the	O
data	O
,	O
compare	O
to	O
figure	O
2.5	O
(	O
c	O
)	O
.	O
for	O
smaller	O
length-scales	O
the	O
marginal	B
likelihood	I
decreases	O
some-	O
what	O
more	O
slowly	O
,	O
corresponding	O
to	O
models	O
that	O
do	O
accommodate	O
the	O
data	O
,	O
but	O
waste	O
predictive	B
mass	O
at	O
regions	O
far	O
away	O
from	O
the	O
underlying	O
function	B
,	O
compare	O
to	O
figure	O
2.5	O
(	O
b	O
)	O
.	O
in	O
figure	O
5.3	O
(	O
b	O
)	O
the	O
dependence	O
of	O
the	O
log	O
marginal	O
likelihood	B
on	O
the	O
charac-	O
teristic	O
length-scale	B
is	O
shown	O
for	O
diﬀerent	O
numbers	O
of	O
training	O
cases	O
.	O
generally	O
,	O
the	O
more	O
data	O
,	O
the	O
more	O
peaked	O
the	O
marginal	B
likelihood	I
.	O
for	O
very	O
small	O
numbers	O
of	O
training	O
data	O
points	O
the	O
slope	O
of	O
the	O
log	O
marginal	O
likelihood	B
is	O
very	O
shallow	O
as	O
when	O
only	O
a	O
little	O
data	O
has	O
been	O
observed	O
,	O
both	O
very	O
short	O
and	O
intermediate	O
values	O
of	O
the	O
length-scale	B
are	O
consistent	O
with	O
the	O
data	O
.	O
with	O
more	O
data	O
,	O
the	O
complexity	O
term	O
gets	O
more	O
severe	O
,	O
and	O
discourages	O
too	O
short	O
length-scales	O
.	O
marginal	B
likelihood	I
gradient	O
to	O
set	B
the	O
hyperparameters	B
by	O
maximizing	O
the	O
marginal	B
likelihood	I
,	O
we	O
seek	O
the	O
partial	O
derivatives	O
of	O
the	O
marginal	B
likelihood	I
w.r.t	O
.	O
the	O
hyperparameters	B
.	O
using	O
eq	O
.	O
(	O
5.8	O
)	O
and	O
eq	O
.	O
(	O
a.14-a.15	O
)	O
we	O
obtain	O
k−1y	O
−	O
1	O
2	O
(	O
αα	O
>	O
−	O
k−1	O
)	O
∂k	O
∂θj	O
tr	O
(	O
cid:0	O
)	O
k−1	O
∂k	O
(	O
cid:17	O
)	O
where	O
α	O
=	O
k−1y	O
.	O
log	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
y	O
>	O
k−1	O
∂k	O
∂θj	O
(	O
cid:16	O
)	O
(	O
cid:1	O
)	O
∂θj	O
(	O
5.9	O
)	O
∂	O
∂θj	O
1	O
2	O
1	O
2	O
=	O
tr	O
the	O
complexity	O
of	O
computing	O
the	O
marginal	B
likelihood	I
in	O
eq	O
.	O
(	O
5.8	O
)	O
is	O
dominated	O
by	O
the	O
need	O
to	O
invert	O
the	O
k	O
matrix	B
(	O
the	O
log	O
determinant	O
of	O
k	O
is	O
easily	O
com-	O
puted	O
as	O
a	O
by-product	O
of	O
the	O
inverse	O
)	O
.	O
standard	O
methods	O
for	O
matrix	B
inversion	O
of	O
positive	B
deﬁnite	I
symmetric	O
matrices	O
require	O
time	O
o	O
(	O
n3	O
)	O
for	O
inversion	O
of	O
an	O
n	O
by	O
n	O
matrix	B
.	O
once	O
k−1	O
is	O
known	O
,	O
the	O
computation	O
of	O
the	O
derivatives	O
in	O
eq	O
.	O
(	O
5.9	O
)	O
requires	O
only	O
time	O
o	O
(	O
n2	O
)	O
per	O
hyperparameter.5	O
thus	O
,	O
the	O
computational	O
over-	O
5note	O
that	O
matrix-by-matrix	O
products	O
in	O
eq	O
.	O
(	O
5.9	O
)	O
should	O
not	O
be	O
computed	O
directly	O
:	O
in	O
the	O
ﬁrst	O
term	O
,	O
do	O
the	O
vector-by-matrix	O
multiplications	O
ﬁrst	O
;	O
in	O
the	O
trace	O
term	O
,	O
compute	O
only	O
the	O
diagonal	O
terms	O
of	O
the	O
product	O
.	O
10010110−1100characteristic	O
lengthscalenoise	O
standard	O
deviation	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
5.4	O
model	B
selection	O
for	O
gp	O
regression	B
115	O
head	O
of	O
computing	O
derivatives	O
is	O
small	O
,	O
so	O
using	O
a	O
gradient	O
based	O
optimizer	O
is	O
advantageous	O
.	O
estimation	O
of	O
θ	O
by	O
optimzation	O
of	O
the	O
marginal	B
likelihood	I
has	O
a	O
long	O
history	O
in	O
spatial	O
statistics	O
,	O
see	B
e.g	O
.	O
mardia	O
and	O
marshall	O
[	O
1984	O
]	O
.	O
as	O
n	O
increases	O
,	O
one	O
would	O
hope	O
that	O
the	O
data	O
becomes	O
increasingly	O
informative	O
about	O
θ.	O
however	O
,	O
it	O
is	O
necessary	O
to	O
contrast	O
what	O
stein	O
[	O
1999	O
,	O
sec	O
.	O
3.3	O
]	O
calls	O
ﬁxed-domain	O
asymp-	O
totics	O
(	O
where	O
one	O
gets	O
increasingly	O
dense	O
observations	O
within	O
some	O
region	O
)	O
with	O
increasing-domain	O
asymptotics	O
(	O
where	O
the	O
size	O
of	O
the	O
observation	O
region	O
grows	O
with	O
n	O
)	O
.	O
increasing-domain	O
asymptotics	O
are	O
a	O
natural	O
choice	O
in	O
a	O
time-series	O
context	O
but	O
ﬁxed-domain	O
asymptotics	O
seem	O
more	O
natural	O
in	O
spatial	O
(	O
and	O
ma-	O
chine	O
learning	B
)	O
settings	O
.	O
for	O
further	O
discussion	O
see	B
stein	O
[	O
1999	O
,	O
sec	O
.	O
6.4	O
]	O
.	O
figure	O
5.4	O
shows	O
an	O
example	O
of	O
the	O
log	O
marginal	O
likelihood	B
as	O
a	O
function	B
of	O
the	O
characteristic	O
length-scale	B
and	O
the	O
noise	O
standard	O
deviation	O
hyperpa-	O
rameters	O
for	O
the	O
squared	B
exponential	I
covariance	O
function	B
,	O
see	B
eq	O
.	O
(	O
5.1	O
)	O
.	O
the	O
signal	O
variance	O
σ2	O
f	O
was	O
set	B
to	O
1.0.	O
the	O
marginal	B
likelihood	I
has	O
a	O
clear	O
maximum	O
around	O
the	O
hyperparameter	O
values	O
which	O
were	O
used	O
in	O
the	O
gaussian	O
process	B
from	O
which	O
the	O
data	O
was	O
generated	O
.	O
note	O
that	O
for	O
long	O
length-scales	O
and	O
a	O
noise	O
level	O
of	O
σ2	O
n	O
=	O
1	O
,	O
the	O
marginal	B
likelihood	I
becomes	O
almost	O
independent	O
of	O
the	O
length-scale	B
;	O
this	O
is	O
caused	O
by	O
the	O
model	B
explaining	O
everything	O
as	O
noise	O
,	O
and	O
no	O
longer	O
needing	O
the	O
signal	O
covariance	B
.	O
similarly	O
,	O
for	O
small	O
noise	O
and	O
a	O
length-scale	B
of	O
‘	O
=	O
0.4	O
,	O
the	O
marginal	B
likelihood	I
becomes	O
almost	O
independent	O
of	O
the	O
noise	O
level	O
;	O
this	O
is	O
caused	O
by	O
the	O
ability	O
of	O
the	O
model	B
to	O
exactly	O
interpolate	O
the	O
data	O
at	O
this	O
short	O
length-scale	B
.	O
we	O
note	O
that	O
although	O
the	O
model	B
in	O
this	O
hyperparameter	O
region	O
explains	O
all	O
the	O
data-points	O
exactly	O
,	O
this	O
model	B
is	O
still	O
disfavoured	O
by	O
the	O
marginal	B
likelihood	I
,	O
see	B
figure	O
5.2.	O
there	O
is	O
no	O
guarantee	O
that	O
the	O
marginal	B
likelihood	I
does	O
not	O
suﬀer	O
from	O
mul-	O
tiple	O
local	O
optima	O
.	O
practical	O
experience	O
with	O
simple	O
covariance	B
functions	O
seem	O
to	O
indicate	O
that	O
local	O
maxima	O
are	O
not	O
a	O
devastating	O
problem	O
,	O
but	O
certainly	O
they	O
do	O
exist	O
.	O
in	O
fact	O
,	O
every	O
local	O
maximum	O
corresponds	O
to	O
a	O
particular	O
interpre-	O
tation	O
of	O
the	O
data	O
.	O
in	O
figure	O
5.5	O
an	O
example	O
with	O
two	O
local	O
optima	O
is	O
shown	O
,	O
together	O
with	O
the	O
corresponding	O
(	O
noise	O
free	O
)	O
predictions	O
of	O
the	O
model	B
at	O
each	O
of	O
the	O
two	O
local	O
optima	O
.	O
one	O
optimum	O
corresponds	O
to	O
a	O
relatively	O
complicated	O
model	B
with	O
low	O
noise	O
,	O
whereas	O
the	O
other	O
corresponds	O
to	O
a	O
much	O
simpler	O
model	B
with	O
more	O
noise	O
.	O
with	O
only	O
7	O
data	O
points	O
,	O
it	O
is	O
not	O
possible	O
for	O
the	O
model	B
to	O
conﬁdently	O
reject	O
either	O
of	O
the	O
two	O
possibilities	O
.	O
the	O
numerical	O
value	O
of	O
the	O
marginal	B
likelihood	I
for	O
the	O
more	O
complex	O
model	B
is	O
about	O
60	O
%	O
higher	O
than	O
for	O
the	O
simple	O
model	B
.	O
according	O
to	O
the	O
bayesian	O
formalism	O
,	O
one	O
ought	O
to	O
weight	O
predictions	O
from	O
alternative	O
explanations	O
according	O
to	O
their	O
posterior	O
probabil-	O
ities	O
.	O
in	O
practice	O
,	O
with	O
data	O
sets	O
of	O
much	O
larger	O
sizes	O
,	O
one	O
often	O
ﬁnds	O
that	O
one	O
local	O
optimum	O
is	O
orders	O
of	O
magnitude	O
more	O
probable	O
than	O
other	O
local	O
optima	O
,	O
so	O
averaging	O
together	O
alternative	O
explanations	O
may	O
not	O
be	O
necessary	O
.	O
however	O
,	O
care	O
should	O
be	O
taken	O
that	O
one	O
doesn	O
’	O
t	O
end	O
up	O
in	O
a	O
bad	O
local	O
optimum	O
.	O
above	O
we	O
have	O
described	O
how	O
to	O
adapt	O
the	O
parameters	O
of	O
the	O
covariance	B
function	I
given	O
one	O
dataset	B
.	O
however	O
,	O
it	O
may	O
happen	O
that	O
we	O
are	O
given	O
several	O
datasets	O
all	O
of	O
which	O
are	O
assumed	O
to	O
share	O
the	O
same	O
hyperparameters	B
;	O
this	O
is	O
known	O
as	O
multi-task	B
learning	I
,	O
see	B
e.g	O
.	O
caruana	O
[	O
1997	O
]	O
.	O
in	O
this	O
case	O
one	O
can	O
multiple	O
local	O
maxima	O
multi-task	B
learning	I
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
116	O
model	B
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
(	O
a	O
)	O
(	O
b	O
)	O
(	O
c	O
)	O
n	O
(	O
noise	O
standard	O
deviation	O
)	O
,	O
where	O
σ2	O
figure	O
5.5	O
:	O
panel	O
(	O
a	O
)	O
shows	O
the	O
marginal	B
likelihood	I
as	O
a	O
function	B
of	O
the	O
hyperparame-	O
ters	O
‘	O
(	O
length-scale	B
)	O
and	O
σ2	O
f	O
=	O
1	O
(	O
signal	O
standard	O
deviation	O
)	O
for	O
a	O
data	O
set	B
of	O
7	O
observations	O
(	O
seen	O
in	O
panels	O
(	O
b	O
)	O
and	O
(	O
c	O
)	O
)	O
.	O
there	O
are	O
two	O
local	O
optima	O
,	O
indicated	O
with	O
’	O
+	O
’	O
:	O
the	O
global	O
optimum	O
has	O
low	O
noise	O
and	O
a	O
short	O
length-scale	B
;	O
the	O
local	O
optimum	O
has	O
a	O
high	O
noise	O
and	O
a	O
long	O
length	O
scale	O
.	O
in	O
(	O
b	O
)	O
and	O
(	O
c	O
)	O
the	O
inferred	O
underlying	O
functions	O
(	O
and	O
95	O
%	O
conﬁdence	O
intervals	O
)	O
are	O
shown	O
for	O
each	O
of	O
the	O
two	O
solutions	O
.	O
in	O
fact	O
,	O
the	O
data	O
points	O
were	O
generated	O
by	O
a	O
gaussian	O
process	B
with	O
(	O
‘	O
,	O
σ2	O
f	O
,	O
σ2	O
n	O
)	O
=	O
(	O
1	O
,	O
1	O
,	O
0.1	O
)	O
in	O
eq	O
.	O
(	O
5.1	O
)	O
.	O
simply	O
sum	O
the	O
log	O
marginal	O
likelihoods	O
of	O
the	O
individual	O
problems	O
and	O
optimize	O
this	O
sum	O
w.r.t	O
.	O
the	O
hyperparameters	B
[	O
minka	O
and	O
picard	O
,	O
1999	O
]	O
.	O
5.4.2	O
cross-validation	B
the	O
predictive	B
log	O
probability	B
when	O
leaving	O
out	O
training	O
case	O
i	O
is	O
log	O
p	O
(	O
yi|x	O
,	O
y−i	O
,	O
θ	O
)	O
=	O
−1	O
2	O
log	O
σ2	O
i	O
−	O
(	O
yi	O
−	O
µi	O
)	O
2	O
2σ2	O
i	O
−	O
1	O
2	O
log	O
2π	O
,	O
(	O
5.10	O
)	O
negative	O
log	O
validation	O
density	O
loss	B
where	O
the	O
notation	O
y−i	O
means	O
all	O
targets	O
except	O
number	O
i	O
,	O
and	O
µi	O
and	O
σ2	O
i	O
are	O
computed	O
according	O
to	O
eq	O
.	O
(	O
2.23	O
)	O
and	O
(	O
2.24	O
)	O
respectively	O
,	O
in	O
which	O
the	O
training	O
sets	O
are	O
taken	O
to	O
be	O
(	O
x−i	O
,	O
y−i	O
)	O
.	O
accordingly	O
,	O
the	O
loo	O
log	O
predictive	O
probability	B
is	O
lloo	O
(	O
x	O
,	O
y	O
,	O
θ	O
)	O
=	O
log	O
p	O
(	O
yi|x	O
,	O
y−i	O
,	O
θ	O
)	O
,	O
(	O
5.11	O
)	O
nx	O
i=1	O
10010110−1100characteristic	O
lengthscalenoise	O
standard	O
deviation−505−2−1012input	O
,	O
xoutput	O
,	O
y−505−2−1012input	O
,	O
xoutput	O
,	O
y	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
5.4	O
model	B
selection	O
for	O
gp	O
regression	B
117	O
pseudo-likelihood	B
see	O
[	O
geisser	O
and	O
eddy	O
,	O
1979	O
]	O
for	O
a	O
discussion	O
of	O
this	O
and	O
related	O
approaches	O
.	O
lloo	O
in	O
eq	O
.	O
(	O
5.11	O
)	O
is	O
sometimes	O
called	O
the	O
log	O
pseudo-likelihood	O
.	O
notice	O
,	O
that	O
in	O
each	O
of	O
the	O
n	O
loo-cv	O
rotations	O
,	O
inference	O
in	O
the	O
gaussian	O
process	B
model	O
(	O
with	O
ﬁxed	O
hyperparameters	B
)	O
essentially	O
consists	O
of	O
computing	O
the	O
inverse	O
co-	O
variance	O
matrix	B
,	O
to	O
allow	O
predictive	B
mean	O
and	O
variance	O
in	O
eq	O
.	O
(	O
2.23	O
)	O
and	O
(	O
2.24	O
)	O
to	O
be	O
evaluated	O
(	O
i.e	O
.	O
there	O
is	O
no	O
parameter-ﬁtting	O
,	O
such	O
as	O
there	O
would	O
be	O
in	O
a	O
parametric	B
model	O
)	O
.	O
the	O
key	O
insight	O
is	O
that	O
when	O
repeatedly	O
applying	O
the	O
pre-	O
diction	O
eq	O
.	O
(	O
2.23	O
)	O
and	O
(	O
2.24	O
)	O
,	O
the	O
expressions	O
are	O
almost	O
identical	O
:	O
we	O
need	O
the	O
inverses	O
of	O
covariance	B
matrices	O
with	O
a	O
single	O
column	O
and	O
row	O
removed	O
in	O
turn	O
.	O
this	O
can	O
be	O
computed	O
eﬃciently	O
from	O
the	O
inverse	O
of	O
the	O
complete	O
covariance	B
matrix	I
using	O
inversion	O
by	O
partitioning	O
,	O
see	B
eq	O
.	O
(	O
a.11-a.12	O
)	O
.	O
a	O
similar	O
insight	O
has	O
also	O
been	O
used	O
for	O
spline	O
models	O
,	O
see	B
e.g	O
.	O
wahba	O
[	O
1990	O
,	O
sec	O
.	O
4.2	O
]	O
.	O
the	O
ap-	O
proach	O
was	O
used	O
for	O
hyperparameter	O
selection	O
in	O
gaussian	O
process	B
models	O
in	O
sundararajan	O
and	O
keerthi	O
[	O
2001	O
]	O
.	O
the	O
expressions	O
for	O
the	O
loo-cv	O
predictive	B
mean	O
and	O
variance	O
are	O
µi	O
=	O
yi	O
−	O
[	O
k−1y	O
]	O
i/	O
[	O
k−1	O
]	O
ii	O
,	O
and	O
i	O
=	O
1/	O
[	O
k−1	O
]	O
ii	O
,	O
σ2	O
(	O
5.12	O
)	O
where	O
careful	O
inspection	O
reveals	O
that	O
the	O
mean	O
µi	O
is	O
in	O
fact	O
independent	O
of	O
yi	O
as	O
indeed	O
it	O
should	O
be	O
.	O
the	O
computational	O
expense	O
of	O
computing	O
these	O
quantities	O
is	O
o	O
(	O
n3	O
)	O
once	O
for	O
computing	O
the	O
inverse	O
of	O
k	O
plus	O
o	O
(	O
n2	O
)	O
for	O
the	O
entire	O
loo-	O
cv	O
procedure	O
(	O
when	O
k−1	O
is	O
known	O
)	O
.	O
thus	O
,	O
the	O
computational	O
overhead	O
for	O
the	O
loo-cv	O
quantities	O
is	O
negligible	O
.	O
plugging	O
these	O
expressions	O
into	O
eq	O
.	O
(	O
5.10	O
)	O
and	O
(	O
5.11	O
)	O
produces	O
a	O
performance	O
estimator	O
which	O
we	O
can	O
optimize	O
w.r.t	O
.	O
hy-	O
perparameters	O
to	O
do	O
model	B
selection	O
.	O
in	O
particular	O
,	O
we	O
can	O
compute	O
the	O
partial	O
derivatives	O
of	O
lloo	O
w.r.t	O
.	O
the	O
hyperparameters	B
(	O
using	O
eq	O
.	O
(	O
a.14	O
)	O
)	O
and	O
use	O
con-	O
jugate	O
gradient	O
optimization	O
.	O
to	O
this	O
end	O
,	O
we	O
need	O
the	O
partial	O
derivatives	O
of	O
the	O
loo-cv	O
predictive	B
mean	O
and	O
variances	O
from	O
eq	O
.	O
(	O
5.12	O
)	O
w.r.t	O
.	O
the	O
hyperpa-	O
rameters	O
−	O
αi	O
[	O
zjk−1	O
]	O
ii	O
,	O
∂µi	O
∂θj	O
=	O
[	O
zjα	O
]	O
i	O
[	O
k−1	O
]	O
ii	O
[	O
k−1	O
]	O
2	O
where	O
α	O
=	O
k−1y	O
and	O
zj	O
=	O
k−1	O
∂k	O
obtained	O
by	O
using	O
the	O
chain-rule	O
and	O
eq	O
.	O
(	O
5.13	O
)	O
to	O
give	O
∂θj	O
ii	O
∂σ2	O
i	O
∂θj	O
=	O
[	O
zjk−1	O
]	O
ii	O
[	O
k−1	O
]	O
2	O
ii	O
,	O
(	O
5.13	O
)	O
.	O
the	O
partial	O
derivatives	O
of	O
eq	O
.	O
(	O
5.11	O
)	O
are	O
∂lloo	O
∂θj	O
=	O
=	O
nx	O
nx	O
i=1	O
i=1	O
(	O
cid:16	O
)	O
∂	O
log	O
p	O
(	O
yi|x	O
,	O
y−i	O
,	O
θ	O
)	O
(	O
cid:16	O
)	O
∂µi	O
∂θj	O
1	O
+	O
α2	O
i	O
+	O
∂	O
log	O
p	O
(	O
yi|x	O
,	O
y−i	O
,	O
θ	O
)	O
(	O
cid:17	O
)	O
∂σ2	O
i	O
∂θj	O
/	O
[	O
k−1	O
]	O
ii	O
.	O
[	O
zjk−1	O
]	O
ii	O
(	O
cid:17	O
)	O
∂σ2	O
i	O
[	O
k−1	O
]	O
ii	O
∂µi	O
αi	O
[	O
zjα	O
]	O
i	O
−	O
1	O
2	O
(	O
5.14	O
)	O
the	O
computational	O
complexity	O
is	O
o	O
(	O
n3	O
)	O
for	O
computing	O
the	O
inverse	O
of	O
k	O
,	O
and	O
o	O
(	O
n3	O
)	O
per	O
hyperparameter	O
6	O
for	O
the	O
derivative	O
eq	O
.	O
(	O
5.14	O
)	O
.	O
thus	O
,	O
the	O
computa-	O
tional	O
burden	O
of	O
the	O
derivatives	O
is	O
greater	O
for	O
the	O
loo-cv	O
method	O
than	O
for	O
the	O
method	O
based	O
on	O
marginal	B
likelihood	I
,	O
eq	O
.	O
(	O
5.9	O
)	O
.	O
6computation	O
of	O
the	O
matrix-by-matrix	O
product	O
k−1	O
∂k	O
∂θj	O
for	O
each	O
hyperparameter	O
is	O
un-	O
avoidable	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
118	O
model	B
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
loo-cv	O
with	O
squared	B
error	O
loss	B
in	O
eq	O
.	O
(	O
5.10	O
)	O
we	O
have	O
used	O
the	O
log	O
of	O
the	O
validation	O
density	O
as	O
a	O
cross-	O
validation	O
measure	B
of	O
ﬁt	O
(	O
or	O
equivalently	O
,	O
the	O
negative	O
log	O
validation	O
density	O
as	O
a	O
loss	B
function	I
)	O
.	O
one	O
could	O
also	O
envisage	O
using	O
other	O
loss	B
functions	O
,	O
such	O
as	O
the	O
commonly	O
used	O
squared	B
error	O
.	O
however	O
,	O
this	O
loss	B
function	I
is	O
only	O
a	O
function	B
of	O
the	O
predicted	O
mean	O
and	O
ignores	O
the	O
validation	O
set	B
variances	O
.	O
further	O
,	O
since	O
the	O
mean	O
prediction	O
eq	O
.	O
(	O
2.23	O
)	O
is	O
independent	O
of	O
the	O
scale	O
of	O
the	O
covariances	O
(	O
i.e	O
.	O
you	O
can	O
multiply	O
the	O
covariance	B
of	O
the	O
signal	O
and	O
noise	O
by	O
an	O
arbitrary	O
positive	O
constant	O
without	O
changing	O
the	O
mean	O
predictions	O
)	O
,	O
one	O
degree	O
of	O
freedom	O
is	O
left	O
undetermined7	O
by	O
a	O
loo-cv	O
procedure	O
based	O
on	O
squared	B
error	O
loss	B
(	O
or	O
any	O
other	O
loss	B
function	I
which	O
depends	O
only	O
on	O
the	O
mean	O
predictions	O
)	O
.	O
but	O
,	O
of	O
course	O
,	O
the	O
full	O
predictive	B
distribution	O
does	O
depend	O
on	O
the	O
scale	O
of	O
the	O
covariance	B
function	I
.	O
also	O
,	O
computation	O
of	O
the	O
derivatives	O
based	O
on	O
the	O
squared	B
error	O
loss	B
has	O
similar	O
computational	O
complexity	O
as	O
the	O
negative	O
log	O
validation	O
density	O
loss	B
.	O
in	O
conclusion	O
,	O
it	O
seems	O
unattractive	O
to	O
use	O
loo-cv	O
based	O
on	O
squared	B
error	O
loss	B
for	O
hyperparameter	O
selection	O
.	O
comparing	O
the	O
pseudo-likelihood	B
for	O
the	O
loo-cv	O
methodology	O
with	O
the	O
marginal	B
likelihood	I
from	O
the	O
previous	O
section	O
,	O
it	O
is	O
interesting	O
to	O
ask	O
under	O
which	O
circumstances	O
each	O
method	O
might	O
be	O
preferable	O
.	O
their	O
computational	O
demands	O
are	O
roughly	O
identical	O
.	O
this	O
issue	O
has	O
not	O
been	O
studied	O
much	O
empir-	O
ically	O
.	O
however	O
,	O
it	O
is	O
interesting	O
to	O
note	O
that	O
the	O
marginal	B
likelihood	I
tells	O
us	O
the	O
probability	B
of	O
the	O
observations	O
given	O
the	O
assumptions	O
of	O
the	O
model	B
.	O
this	O
contrasts	O
with	O
the	O
frequentist	O
loo-cv	O
value	O
,	O
which	O
gives	O
an	O
estimate	O
for	O
the	O
(	O
log	O
)	O
predictive	B
probability	O
,	O
whether	O
or	O
not	O
the	O
assumptions	O
of	O
the	O
model	B
may	O
be	O
fulﬁlled	O
.	O
thus	O
wahba	O
[	O
1990	O
,	O
sec	O
.	O
4.8	O
]	O
has	O
argued	O
that	O
cv	O
procedures	O
should	O
be	O
more	O
robust	O
against	O
model	B
mis-speciﬁcation	O
.	O
5.4.3	O
examples	O
and	O
discussion	O
in	O
the	O
following	O
we	O
give	O
three	O
examples	O
of	O
model	B
selection	O
for	O
regression	B
models	O
.	O
we	O
ﬁrst	O
describe	O
a	O
1-d	O
modelling	O
task	O
which	O
illustrates	O
how	O
special	O
covariance	B
functions	O
can	O
be	O
designed	O
to	O
achieve	O
various	O
useful	O
eﬀects	O
,	O
and	O
can	O
be	O
evaluated	O
using	O
the	O
marginal	B
likelihood	I
.	O
secondly	O
,	O
we	O
make	O
a	O
short	O
reference	O
to	O
the	O
model	B
selection	O
carried	O
out	O
for	O
the	O
robot	B
arm	O
problem	O
discussed	O
in	O
chapter	O
2	O
and	O
again	O
in	O
chapter	O
8.	O
finally	O
,	O
we	O
discuss	O
an	O
example	O
where	O
we	O
deliberately	O
choose	O
a	O
covariance	B
function	I
that	O
is	O
not	O
well-suited	O
for	O
the	O
problem	O
;	O
this	O
is	O
the	O
so-called	O
mis-speciﬁed	O
model	B
scenario	O
.	O
mauna	O
loa	O
atmospheric	O
carbon	O
dioxide	O
we	O
will	O
use	O
a	O
modelling	O
problem	O
concerning	O
the	O
concentration	O
of	O
co2	O
in	O
the	O
atmosphere	O
to	O
illustrate	O
how	O
the	O
marginal	B
likelihood	I
can	O
be	O
used	O
to	O
set	B
multiple	O
hyperparameters	B
in	O
hierarchical	O
gaussian	O
process	B
models	O
.	O
a	O
complex	O
covari-	O
ance	O
function	B
is	O
derived	O
by	O
combining	O
several	O
diﬀerent	O
kinds	O
of	O
simple	O
covariance	B
functions	O
,	O
and	O
the	O
resulting	O
model	B
provides	O
an	O
excellent	O
ﬁt	O
to	O
the	O
data	O
as	O
well	O
7in	O
the	O
special	O
case	O
where	O
we	O
know	O
either	O
the	O
signal	O
or	O
the	O
noise	O
variance	O
there	O
is	O
no	O
indeterminancy	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
5.4	O
model	B
selection	O
for	O
gp	O
regression	B
119	O
figure	O
5.6	O
:	O
the	O
545	O
observations	O
of	O
monthly	O
averages	O
of	O
the	O
atmospheric	O
concentra-	O
tion	O
of	O
co2	O
made	O
between	O
1958	O
and	O
the	O
end	O
of	O
2003	O
,	O
together	O
with	O
95	O
%	O
predictive	B
conﬁdence	O
region	O
for	O
a	O
gaussian	O
process	B
regression	O
model	B
,	O
20	O
years	O
into	O
the	O
future	O
.	O
rising	O
trend	O
and	O
seasonal	O
variations	O
are	O
clearly	O
visible	O
.	O
note	O
also	O
that	O
the	O
conﬁdence	O
interval	O
gets	O
wider	O
the	O
further	O
the	O
predictions	O
are	O
extrapolated	O
.	O
as	O
insights	O
into	O
its	O
properties	O
by	O
interpretation	O
of	O
the	O
adapted	O
hyperparame-	O
ters	O
.	O
although	O
the	O
data	O
is	O
one-dimensional	O
,	O
and	O
therefore	O
easy	O
to	O
visualize	O
,	O
a	O
total	O
of	O
11	O
hyperparameters	B
are	O
used	O
,	O
which	O
in	O
practice	O
rules	O
out	O
the	O
use	O
of	O
cross-validation	B
for	O
setting	O
parameters	O
,	O
except	O
for	O
the	O
gradient-based	O
loo-cv	O
procedure	O
from	O
the	O
previous	O
section	O
.	O
the	O
data	O
[	O
keeling	O
and	O
whorf	O
,	O
2004	O
]	O
consists	O
of	O
monthly	O
average	O
atmospheric	O
co2	O
concentrations	O
(	O
in	O
parts	O
per	O
million	O
by	O
volume	O
(	O
ppmv	O
)	O
)	O
derived	O
from	O
in	O
situ	O
air	O
samples	O
collected	O
at	O
the	O
mauna	O
loa	O
observatory	O
,	O
hawaii	O
,	O
between	O
1958	O
and	O
2003	O
(	O
with	O
some	O
missing	O
values	O
)	O
.8	O
the	O
data	O
is	O
shown	O
in	O
figure	O
5.6.	O
our	O
goal	O
is	O
the	O
model	B
the	O
co2	O
concentration	O
as	O
a	O
function	B
of	O
time	O
x.	O
several	O
features	O
are	O
immediately	O
apparent	O
:	O
a	O
long	O
term	O
rising	O
trend	O
,	O
a	O
pronounced	O
seasonal	O
variation	O
and	O
some	O
smaller	O
irregularities	O
.	O
in	O
the	O
following	O
we	O
suggest	O
contributions	O
to	O
a	O
combined	O
covariance	B
function	I
which	O
takes	O
care	O
of	O
these	O
individual	O
properties	O
.	O
this	O
is	O
meant	O
primarily	O
to	O
illustrate	O
the	O
power	O
and	O
ﬂexibility	O
of	O
the	O
gaussian	O
process	B
framework—it	O
is	O
possible	O
that	O
other	O
choices	O
would	O
be	O
more	O
appropriate	O
for	O
this	O
data	O
set	B
.	O
to	O
model	B
the	O
long	O
term	O
smooth	O
rising	O
trend	O
we	O
use	O
a	O
squared	B
exponential	I
(	O
se	O
)	O
covariance	B
term	O
,	O
with	O
two	O
hyperparameters	B
controlling	O
the	O
amplitude	O
θ1	O
and	O
characteristic	O
length-scale	B
θ2	O
smooth	O
trend	O
(	O
cid:16	O
)	O
−	O
(	O
x	O
−	O
x0	O
)	O
2	O
(	O
cid:17	O
)	O
k1	O
(	O
x	O
,	O
x0	O
)	O
=	O
θ2	O
1	O
exp	O
.	O
(	O
5.15	O
)	O
2θ2	O
2	O
note	O
that	O
we	O
just	O
use	O
a	O
smooth	O
trend	O
;	O
actually	O
enforcing	O
the	O
trend	O
a	O
priori	O
to	O
be	O
increasing	O
is	O
probably	O
not	O
so	O
simple	O
and	O
(	O
hopefully	O
)	O
not	O
desirable	O
.	O
we	O
can	O
use	O
the	O
periodic	B
covariance	O
function	B
from	O
eq	O
.	O
(	O
4.31	O
)	O
with	O
a	O
period	O
of	O
one	O
year	O
to	O
model	B
the	O
seasonal	O
variation	O
.	O
however	O
,	O
it	O
is	O
not	O
clear	O
that	O
the	O
seasonal	O
trend	O
is	O
8the	O
data	O
is	O
available	O
from	O
http	O
:	O
//cdiac.esd.ornl.gov/ftp/trends/co2/maunaloa.co2	O
.	O
seasonal	O
component	O
1960197019801990200020102020320340360380400420yearco2	O
concentration	O
,	O
ppm	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
120	O
model	B
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
(	O
b	O
)	O
(	O
a	O
)	O
figure	O
5.7	O
:	O
panel	O
(	O
a	O
)	O
:	O
long	O
term	O
trend	O
,	O
dashed	O
,	O
left	O
hand	O
scale	O
,	O
predicted	O
by	O
the	O
squared	B
exponential	I
contribution	O
;	O
superimposed	O
is	O
the	O
medium	O
term	O
trend	O
,	O
full	O
line	O
,	O
right	O
hand	O
scale	O
,	O
predicted	O
by	O
the	O
rational	B
quadratic	I
contribution	O
;	O
the	O
vertical	O
dash-	O
dotted	O
line	O
indicates	O
the	O
upper	O
limit	O
of	O
the	O
training	O
data	O
.	O
panel	O
(	O
b	O
)	O
shows	O
the	O
seasonal	O
variation	O
over	O
the	O
year	O
for	O
three	O
diﬀerent	O
years	O
.	O
the	O
concentration	O
peaks	O
in	O
mid	O
may	O
and	O
has	O
a	O
low	O
in	O
the	O
beginning	O
of	O
october	O
.	O
the	O
seasonal	O
variation	O
is	O
smooth	O
,	O
but	O
not	O
of	O
exactly	O
sinusoidal	O
shape	O
.	O
the	O
peak-to-peak	O
amplitude	O
increases	O
from	O
about	O
5.5	O
ppm	O
in	O
1958	O
to	O
about	O
7	O
ppm	O
in	O
2003	O
,	O
but	O
the	O
shape	O
does	O
not	O
change	O
very	O
much	O
.	O
the	O
characteristic	O
decay	O
length	O
of	O
the	O
periodic	B
component	O
is	O
inferred	O
to	O
be	O
90	O
years	O
,	O
so	O
the	O
seasonal	O
trend	O
changes	O
rather	O
slowly	O
,	O
as	O
also	O
suggested	O
by	O
the	O
gradual	O
progression	O
between	O
the	O
three	O
years	O
shown	O
.	O
exactly	O
periodic	B
,	O
so	O
we	O
modify	O
eq	O
.	O
(	O
4.31	O
)	O
by	O
taking	O
the	O
product	O
with	O
a	O
squared	B
exponential	I
component	O
(	O
using	O
the	O
product	O
construction	O
from	O
section	O
4.2.4	O
)	O
,	O
to	O
allow	O
a	O
decay	O
away	O
from	O
exact	O
periodicity	O
(	O
cid:16	O
)	O
−	O
(	O
x	O
−	O
x0	O
)	O
2	O
2θ2	O
4	O
(	O
cid:17	O
)	O
−	O
2	O
sin2	O
(	O
π	O
(	O
x	O
−	O
x0	O
)	O
)	O
θ2	O
5	O
,	O
(	O
5.16	O
)	O
k2	O
(	O
x	O
,	O
x0	O
)	O
=	O
θ2	O
3	O
exp	O
where	O
θ3	O
gives	O
the	O
magnitude	O
,	O
θ4	O
the	O
decay-time	O
for	O
the	O
periodic	B
component	O
,	O
and	O
θ5	O
the	O
smoothness	O
of	O
the	O
periodic	B
component	O
;	O
the	O
period	O
has	O
been	O
ﬁxed	O
to	O
one	O
(	O
year	O
)	O
.	O
the	O
seasonal	O
component	O
in	O
the	O
data	O
is	O
caused	O
primarily	O
by	O
diﬀerent	O
rates	O
of	O
co2	O
uptake	O
for	O
plants	O
depending	O
on	O
the	O
season	O
,	O
and	O
it	O
is	O
probably	O
reasonable	O
to	O
assume	O
that	O
this	O
pattern	O
may	O
itself	O
change	O
slowly	O
over	O
time	O
,	O
partially	O
due	O
to	O
the	O
elevation	O
of	O
the	O
co2	O
level	O
itself	O
;	O
if	O
this	O
eﬀect	O
turns	O
out	O
not	O
to	O
be	O
relevant	O
,	O
then	O
it	O
can	O
be	O
eﬀectively	O
removed	O
at	O
the	O
ﬁtting	O
stage	O
by	O
allowing	O
θ4	O
to	O
become	O
very	O
large	O
.	O
to	O
model	B
the	O
(	O
small	O
)	O
medium	O
term	O
irregularities	O
a	O
rational	B
quadratic	I
term	O
(	O
cid:16	O
)	O
(	O
cid:17	O
)	O
−θ8	O
k3	O
(	O
x	O
,	O
x0	O
)	O
=	O
θ2	O
6	O
1	O
+	O
(	O
x	O
−	O
x0	O
)	O
2	O
2θ8θ2	O
7	O
,	O
(	O
5.17	O
)	O
medium	O
term	O
irregularities	O
is	O
used	O
,	O
eq	O
.	O
(	O
4.19	O
)	O
where	O
θ6	O
is	O
the	O
magnitude	O
,	O
θ7	O
is	O
the	O
typical	O
length-scale	B
and	O
θ8	O
is	O
the	O
shape	O
pa-	O
rameter	O
determining	O
diﬀuseness	O
of	O
the	O
length-scales	O
,	O
see	B
the	O
discussion	O
on	O
page	O
87.	O
one	O
could	O
also	O
have	O
used	O
a	O
squared	B
exponential	I
form	O
for	O
this	O
component	O
,	O
but	O
it	O
turns	O
out	O
that	O
the	O
rational	B
quadratic	I
works	O
better	O
(	O
gives	O
higher	O
marginal	B
likelihood	I
)	O
,	O
probably	O
because	O
it	O
can	O
accommodate	O
several	O
length-scales	O
.	O
1960197019801990200020102020320340360380400co2	O
concentration	O
,	O
ppmyear−1−0.500.51co2	O
concentration	O
,	O
ppmjfmamjjasond−3−2−10123co2	O
concentration	O
,	O
ppmmonth195819702003	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
5.4	O
model	B
selection	O
for	O
gp	O
regression	B
121	O
figure	O
5.8	O
:	O
the	O
time	O
course	O
of	O
the	O
seasonal	O
eﬀect	O
,	O
plotted	O
in	O
a	O
months	O
vs.	O
year	O
plot	O
(	O
with	O
wrap-around	O
continuity	O
between	O
the	O
edges	O
)	O
.	O
the	O
labels	O
on	O
the	O
contours	O
are	O
in	O
ppmv	O
of	O
co2	O
.	O
the	O
training	O
period	O
extends	O
up	O
to	O
the	O
dashed	O
line	O
.	O
note	O
the	O
slow	O
development	O
:	O
the	O
height	O
of	O
the	O
may	O
peak	O
may	O
have	O
started	O
to	O
recede	O
,	O
but	O
the	O
low	O
in	O
october	O
may	O
currently	O
(	O
2005	O
)	O
be	O
deepening	O
further	O
.	O
the	O
seasonal	O
eﬀects	O
from	O
three	O
particular	O
years	O
were	O
also	O
plotted	O
in	O
figure	O
5.7	O
(	O
b	O
)	O
.	O
finally	O
we	O
specify	O
a	O
noise	B
model	I
as	O
the	O
sum	O
of	O
a	O
squared	B
exponential	I
con-	O
tribution	O
and	O
an	O
independent	O
component	O
(	O
cid:16	O
)	O
−	O
(	O
xp	O
−	O
xq	O
)	O
2	O
(	O
cid:17	O
)	O
2θ2	O
10	O
k4	O
(	O
xp	O
,	O
xq	O
)	O
=	O
θ2	O
9	O
exp	O
+	O
θ2	O
11δpq	O
,	O
(	O
5.18	O
)	O
where	O
θ9	O
is	O
the	O
magnitude	O
of	O
the	O
correlated	B
noise	O
component	O
,	O
θ10	O
is	O
its	O
length-	O
scale	O
and	O
θ11	O
is	O
the	O
magnitude	O
of	O
the	O
independent	O
noise	O
component	O
.	O
noise	O
in	O
the	O
series	O
could	O
be	O
caused	O
by	O
measurement	O
inaccuracies	O
,	O
and	O
by	O
local	O
short-term	O
weather	O
phenomena	O
,	O
so	O
it	O
is	O
probably	O
reasonable	O
to	O
assume	O
at	O
least	O
a	O
modest	O
amount	O
of	O
correlation	O
in	O
time	O
.	O
notice	O
that	O
the	O
correlated	B
noise	O
component	O
,	O
the	O
ﬁrst	O
term	O
of	O
eq	O
.	O
(	O
5.18	O
)	O
,	O
has	O
an	O
identical	O
expression	O
to	O
the	O
long	O
term	O
component	O
in	O
eq	O
.	O
(	O
5.15	O
)	O
.	O
when	O
optimizing	O
the	O
hyperparameters	B
,	O
we	O
will	O
see	B
that	O
one	O
of	O
these	O
components	O
becomes	O
large	O
with	O
a	O
long	O
length-scale	B
(	O
the	O
long	O
term	O
trend	O
)	O
,	O
while	O
the	O
other	O
remains	O
small	O
with	O
a	O
short	O
length-scale	B
(	O
noise	O
)	O
.	O
the	O
fact	O
that	O
we	O
have	O
chosen	O
to	O
call	O
one	O
of	O
these	O
components	O
‘	O
signal	O
’	O
and	O
the	O
other	O
one	O
‘	O
noise	O
’	O
is	O
only	O
a	O
question	O
of	O
interpretation	O
.	O
presumably	O
we	O
are	O
less	O
interested	O
in	O
very	O
short-term	O
eﬀect	O
,	O
and	O
thus	O
call	O
it	O
noise	O
;	O
if	O
on	O
the	O
other	O
hand	O
we	O
were	O
interested	O
in	O
this	O
eﬀect	O
,	O
we	O
would	O
call	O
it	O
signal	O
.	O
the	O
ﬁnal	O
covariance	B
function	I
is	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
k1	O
(	O
x	O
,	O
x0	O
)	O
+	O
k2	O
(	O
x	O
,	O
x0	O
)	O
+	O
k3	O
(	O
x	O
,	O
x0	O
)	O
+	O
k4	O
(	O
x	O
,	O
x0	O
)	O
,	O
(	O
5.19	O
)	O
with	O
hyperparameters	B
θ	O
=	O
(	O
θ1	O
,	O
.	O
.	O
.	O
,	O
θ11	O
)	O
>	O
.	O
we	O
ﬁrst	O
subtract	O
the	O
empirical	O
mean	O
of	O
the	O
data	O
(	O
341	O
ppm	O
)	O
,	O
and	O
then	O
ﬁt	O
the	O
hyperparameters	B
by	O
optimizing	O
the	O
marginal	B
likelihood	I
using	O
a	O
conjugate	O
gradient	O
optimizer	O
.	O
to	O
avoid	O
bad	O
local	O
minima	O
(	O
e.g	O
.	O
caused	O
by	O
swapping	O
rˆoles	O
of	O
the	O
rational	B
quadratic	I
and	O
squared	B
exponential	I
terms	O
)	O
a	O
few	O
random	O
restarts	O
are	O
tried	O
,	O
picking	O
the	O
run	O
with	O
the	O
best	O
marginal	B
likelihood	I
,	O
which	O
was	O
log	O
p	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
−108.5	O
.	O
noise	O
terms	O
parameter	O
estimation	O
jfmamjjasond1960197019801990200020102020monthyear−3.6−3.3−2.8−2.8−2−2−1−10011222.833.1	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
122	O
model	B
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
we	O
now	O
examine	O
and	O
interpret	O
the	O
hyperparameters	B
which	O
optimize	O
the	O
marginal	B
likelihood	I
.	O
the	O
long	O
term	O
trend	O
has	O
a	O
magnitude	O
of	O
θ1	O
=	O
66	O
ppm	O
and	O
a	O
length	O
scale	O
of	O
θ2	O
=	O
67	O
years	O
.	O
the	O
mean	O
predictions	O
inside	O
the	O
range	O
of	O
the	O
training	O
data	O
and	O
extending	O
for	O
20	O
years	O
into	O
the	O
future	O
are	O
depicted	O
in	O
figure	O
5.7	O
(	O
a	O
)	O
.	O
in	O
the	O
same	O
plot	O
(	O
with	O
right	O
hand	O
axis	O
)	O
we	O
also	O
show	O
the	O
medium	O
term	O
eﬀects	O
modelled	O
by	O
the	O
rational	B
quadratic	I
component	O
with	O
magnitude	O
θ6	O
=	O
0.66	O
ppm	O
,	O
typical	O
length	O
θ7	O
=	O
1.2	O
years	O
and	O
shape	O
θ8	O
=	O
0.78.	O
the	O
very	O
small	O
shape	O
value	O
allows	O
for	O
covariance	B
at	O
many	O
diﬀerent	O
length-scales	O
,	O
which	O
is	O
also	O
evident	O
in	O
figure	O
5.7	O
(	O
a	O
)	O
.	O
notice	O
that	O
beyond	O
the	O
edge	O
of	O
the	O
training	O
data	O
the	O
mean	O
of	O
this	O
contribution	O
smoothly	O
decays	O
to	O
zero	O
,	O
but	O
of	O
course	O
it	O
still	O
has	O
a	O
contribution	O
to	O
the	O
uncertainty	O
,	O
see	B
figure	O
5.6.	O
the	O
hyperparameter	O
values	O
for	O
the	O
decaying	O
periodic	B
contribution	O
are	O
:	O
mag-	O
nitude	O
θ3	O
=	O
2.4	O
ppm	O
,	O
decay-time	O
θ4	O
=	O
90	O
years	O
,	O
and	O
the	O
smoothness	O
of	O
the	O
periodic	B
component	O
is	O
θ5	O
=	O
1.3.	O
the	O
quite	O
long	O
decay-time	O
shows	O
that	O
the	O
data	O
have	O
a	O
very	O
close	O
to	O
periodic	B
component	O
in	O
the	O
short	O
term	O
.	O
in	O
figure	O
5.7	O
(	O
b	O
)	O
we	O
show	O
the	O
mean	O
periodic	O
contribution	O
for	O
three	O
years	O
corresponding	O
to	O
the	O
beginning	O
,	O
middle	O
and	O
end	O
of	O
the	O
training	O
data	O
.	O
this	O
component	O
is	O
not	O
exactly	O
sinusoidal	O
,	O
and	O
it	O
changes	O
its	O
shape	O
slowly	O
over	O
time	O
,	O
most	O
notably	O
the	O
amplitude	O
is	O
increasing	O
,	O
see	B
figure	O
5.8	O
.	O
√	O
for	O
the	O
noise	O
components	O
,	O
we	O
get	O
the	O
amplitude	O
for	O
the	O
correlated	B
compo-	O
nent	O
θ9	O
=	O
0.18	O
ppm	O
,	O
a	O
length-scale	B
of	O
θ10	O
=	O
1.6	O
months	O
and	O
an	O
independent	O
noise	O
magnitude	O
of	O
θ11	O
=	O
0.19	O
ppm	O
.	O
thus	O
,	O
the	O
correlation	O
length	O
for	O
the	O
noise	O
component	O
is	O
indeed	O
inferred	O
to	O
be	O
short	O
,	O
and	O
the	O
total	O
magnitude	O
of	O
the	O
noise	O
11	O
=	O
0.26	O
ppm	O
,	O
indicating	O
that	O
the	O
data	O
can	O
be	O
explained	O
very	O
is	O
just	O
well	O
by	O
the	O
model	B
.	O
note	O
also	O
in	O
figure	O
5.6	O
that	O
the	O
model	B
makes	O
relatively	O
conﬁdent	O
predictions	O
,	O
the	O
95	O
%	O
conﬁdence	O
region	O
being	O
16	O
ppm	O
wide	O
at	O
a	O
20	O
year	O
prediction	B
horizon	O
.	O
9	O
+	O
θ2	O
θ2	O
in	O
conclusion	O
,	O
we	O
have	O
seen	O
an	O
example	O
of	O
how	O
non-trivial	O
structure	O
can	O
be	O
inferred	O
by	O
using	O
composite	O
covariance	B
functions	O
,	O
and	O
that	O
the	O
ability	O
to	O
leave	O
hyperparameters	B
to	O
be	O
determined	O
by	O
the	O
data	O
is	O
useful	O
in	O
practice	O
.	O
of	O
course	O
a	O
serious	O
treatment	O
of	O
such	O
data	O
would	O
probably	O
require	O
modelling	O
of	O
other	O
eﬀects	O
,	O
such	O
as	O
demographic	O
and	O
economic	O
indicators	O
too	O
.	O
finally	O
,	O
one	O
may	O
want	O
to	O
use	O
a	O
real	O
time-series	O
approach	O
(	O
not	O
just	O
a	O
regression	B
from	O
time	O
to	O
co2	O
level	O
as	O
we	O
have	O
done	O
here	O
)	O
,	O
to	O
accommodate	O
causality	O
,	O
etc	O
.	O
nevertheless	O
,	O
the	O
ability	O
of	O
the	O
gaussian	O
process	B
to	O
avoid	O
simple	O
parametric	B
assumptions	O
and	O
still	O
build	O
in	O
a	O
lot	O
of	O
structure	O
makes	O
it	O
,	O
as	O
we	O
have	O
seen	O
,	O
a	O
very	O
attractive	O
model	B
in	O
many	O
application	O
domains	O
.	O
robot	B
arm	O
inverse	O
dynamics	O
we	O
have	O
discussed	O
the	O
use	O
of	O
gpr	O
for	O
the	O
sarcos	O
robot	B
arm	O
inverse	O
dynamics	O
problem	O
in	O
section	O
2.5.	O
this	O
example	O
is	O
also	O
further	O
studied	O
in	O
section	O
8.3.7	O
where	O
a	O
variety	O
of	O
approximation	O
methods	O
are	O
compared	O
,	O
because	O
the	O
size	O
of	O
the	O
training	O
set	B
(	O
44	O
,	O
484	O
examples	O
)	O
precludes	O
the	O
use	O
of	O
simple	O
gpr	O
due	O
to	O
its	O
o	O
(	O
n2	O
)	O
storage	O
and	O
o	O
(	O
n3	O
)	O
time	O
complexity	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
5.4	O
model	B
selection	O
for	O
gp	O
regression	B
123	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
5.9	O
:	O
mis-speciﬁcation	O
example	O
.	O
fit	O
to	O
64	O
datapoints	O
drawn	O
from	O
a	O
step	O
func-	O
tion	O
with	O
gaussian	O
noise	O
with	O
standard	O
deviation	O
σn	O
=	O
0.1.	O
the	O
gaussian	O
process	B
models	O
are	O
using	O
a	O
squared	B
exponential	I
covariance	O
function	B
.	O
panel	O
(	O
a	O
)	O
shows	O
the	O
mean	O
and	O
95	O
%	O
conﬁdence	O
interval	O
for	O
the	O
noisy	O
signal	O
in	O
grey	O
,	O
when	O
the	O
hyperparameters	B
are	O
chosen	O
to	O
maximize	O
the	O
marginal	B
likelihood	I
.	O
panel	O
(	O
b	O
)	O
shows	O
the	O
resulting	O
model	B
when	O
the	O
hyperparameters	B
are	O
chosen	O
using	O
leave-one-out	B
cross-validation	I
(	O
loo-cv	O
)	O
.	O
note	O
that	O
the	O
marginal	B
likelihood	I
chooses	O
a	O
high	O
noise	O
level	O
and	O
long	O
length-scale	B
,	O
whereas	O
loo-cv	O
chooses	O
a	O
smaller	O
noise	O
level	O
and	O
shorter	O
length-scale	B
.	O
it	O
is	O
not	O
immediately	O
obvious	O
which	O
ﬁt	O
it	O
worse	O
.	O
one	O
of	O
the	O
techniques	O
considered	O
in	O
section	O
8.3.7	O
is	O
the	O
subset	B
of	I
datapoints	I
(	O
sd	O
)	O
method	O
,	O
where	O
we	O
simply	O
discard	O
some	O
of	O
the	O
data	O
and	O
only	O
make	O
use	O
of	O
m	O
<	O
n	O
training	O
examples	O
.	O
given	O
a	O
subset	O
of	O
the	O
training	O
data	O
of	O
size	O
m	O
selected	O
at	O
random	O
,	O
we	O
adjusted	O
the	O
hyperparameters	B
by	O
optimizing	O
either	O
the	O
marginal	B
likelihood	I
or	O
lloo	O
.	O
as	O
ard	O
was	O
used	O
,	O
this	O
involved	O
adjusting	O
d	O
+	O
2	O
=	O
23	O
hyperparameters	B
.	O
this	O
process	B
was	O
repeated	O
10	O
times	O
with	O
diﬀerent	O
random	O
subsets	O
of	O
the	O
data	O
selected	O
for	O
both	O
m	O
=	O
1024	O
and	O
m	O
=	O
2048.	O
the	O
results	O
show	O
that	O
the	O
predictive	B
accuracy	O
obtained	O
from	O
the	O
two	O
optimization	O
methods	O
is	O
very	O
similar	O
on	O
both	O
standardized	B
mean	I
squared	I
error	I
(	O
smse	O
)	O
and	O
mean	B
standardized	I
log	I
loss	I
(	O
msll	O
)	O
criteria	O
,	O
but	O
that	O
the	O
marginal	B
likelihood	I
optimization	O
is	O
much	O
quicker	O
.	O
step	O
function	B
example	O
illustrating	O
mis-speciﬁcation	O
in	O
this	O
section	O
we	O
discuss	O
the	O
mis-speciﬁed	O
model	B
scenario	O
,	O
where	O
we	O
attempt	O
to	O
learn	O
the	O
hyperparameters	B
for	O
a	O
covariance	B
function	I
which	O
is	O
not	O
very	O
well	O
suited	O
to	O
the	O
data	O
.	O
the	O
mis-speciﬁcation	O
arises	O
because	O
the	O
data	O
comes	O
from	O
a	O
function	B
which	O
has	O
either	O
zero	O
or	O
very	O
low	O
probability	B
under	O
the	O
gp	O
prior	O
.	O
one	O
could	O
ask	O
why	O
it	O
is	O
interesting	O
to	O
discuss	O
this	O
scenario	O
,	O
since	O
one	O
should	O
surely	O
simply	O
avoid	O
choosing	O
such	O
a	O
model	B
in	O
practice	O
.	O
while	O
this	O
is	O
true	O
in	O
theory	O
,	O
for	O
practical	O
reasons	O
such	O
as	O
the	O
convenience	O
of	O
using	O
standard	O
forms	O
for	O
the	O
covariance	B
function	I
or	O
because	O
vague	O
prior	O
information	O
,	O
one	O
inevitably	O
ends	O
up	O
in	O
a	O
situation	O
which	O
resembles	O
some	O
level	O
of	O
mis-speciﬁcation	O
.	O
as	O
an	O
example	O
,	O
we	O
use	O
data	O
from	O
a	O
noisy	O
step	O
function	B
and	O
ﬁt	O
a	O
gp	O
model	B
with	O
a	O
squared	B
exponential	I
covariance	O
function	B
,	O
figure	O
5.9.	O
there	O
is	O
mis-	O
speciﬁcation	O
because	O
it	O
would	O
be	O
very	O
unlikely	O
that	O
samples	O
drawn	O
from	O
a	O
gp	O
−1−0.500.51−2−1012output	O
,	O
yinput	O
,	O
x−1−0.500.51−2−1012output	O
,	O
yinput	O
,	O
x	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
124	O
model	B
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
5.10	O
:	O
same	O
data	O
as	O
in	O
figure	O
5.9.	O
panel	O
(	O
a	O
)	O
shows	O
the	O
result	O
of	O
using	O
a	O
covariance	B
function	I
which	O
is	O
the	O
sum	O
of	O
two	O
squared-exponential	O
terms	O
.	O
although	O
this	O
is	O
still	O
a	O
stationary	O
covariance	B
function	I
,	O
it	O
gives	O
rise	O
to	O
a	O
higher	O
marginal	B
likelihood	I
than	O
for	O
the	O
squared-exponential	O
covariance	B
function	I
in	O
figure	O
5.9	O
(	O
a	O
)	O
,	O
and	O
probably	O
also	O
a	O
better	O
ﬁt	O
.	O
in	O
panel	O
(	O
b	O
)	O
the	O
neural	B
network	I
covariance	O
function	B
eq	O
.	O
(	O
4.29	O
)	O
was	O
used	O
,	O
providing	O
a	O
much	O
larger	O
marginal	B
likelihood	I
and	O
a	O
very	O
good	O
ﬁt	O
.	O
with	O
the	O
stationary	O
se	O
covariance	B
function	I
would	O
look	O
like	O
a	O
step	O
function	B
.	O
for	O
short	O
length-scales	O
samples	O
can	O
vary	O
quite	O
quickly	O
,	O
but	O
they	O
would	O
tend	O
to	O
vary	O
rapidly	O
all	O
over	O
,	O
not	O
just	O
near	O
the	O
step	O
.	O
conversely	O
a	O
stationary	O
se	O
covariance	B
function	I
with	O
a	O
long	O
length-scale	B
could	O
model	B
the	O
ﬂat	O
parts	O
of	O
the	O
step	O
function	B
but	O
not	O
the	O
rapid	O
transition	O
.	O
note	O
that	O
gibbs	O
’	O
covariance	B
function	I
eq	O
.	O
(	O
4.32	O
)	O
would	O
be	O
one	O
way	O
to	O
achieve	O
the	O
desired	O
eﬀect	O
.	O
it	O
is	O
interesting	O
to	O
note	O
the	O
dif-	O
ferences	O
between	O
the	O
model	B
optimized	O
with	O
marginal	B
likelihood	I
in	O
figure	O
5.9	O
(	O
a	O
)	O
,	O
and	O
one	O
optimized	O
with	O
loo-cv	O
in	O
panel	O
(	O
b	O
)	O
of	O
the	O
same	O
ﬁgure	O
.	O
see	B
exercise	O
5.6.2	O
for	O
more	O
on	O
how	O
these	O
two	O
criteria	O
weight	O
the	O
inﬂuence	O
of	O
the	O
prior	O
.	O
for	O
comparison	O
,	O
we	O
show	O
the	O
predictive	B
distribution	O
for	O
two	O
other	O
covari-	O
ance	O
functions	O
in	O
figure	O
5.10.	O
in	O
panel	O
(	O
a	O
)	O
a	O
sum	O
of	O
two	O
squared	B
exponential	I
terms	O
were	O
used	O
in	O
the	O
covariance	B
.	O
notice	O
that	O
this	O
covariance	B
function	I
is	O
still	O
stationary	O
,	O
but	O
it	O
is	O
more	O
ﬂexible	O
than	O
a	O
single	O
squared	B
exponential	I
,	O
since	O
it	O
has	O
two	O
magnitude	O
and	O
two	O
length-scale	B
parameters	O
.	O
the	O
predictive	B
distribution	O
looks	O
a	O
little	O
bit	O
better	O
,	O
and	O
the	O
value	O
of	O
the	O
log	O
marginal	O
likelihood	B
improves	O
from	O
−37.7	O
in	O
figure	O
5.9	O
(	O
a	O
)	O
to	O
−26.1	O
in	O
figure	O
5.10	O
(	O
a	O
)	O
.	O
we	O
also	O
tried	O
the	O
neural	B
network	I
covariance	O
function	B
from	O
eq	O
.	O
(	O
4.29	O
)	O
,	O
which	O
is	O
ideally	O
suited	O
to	O
this	O
case	O
,	O
since	O
it	O
allows	O
saturation	O
at	O
diﬀerent	O
values	O
in	O
the	O
positive	O
and	O
negative	O
direc-	O
tions	O
of	O
x.	O
as	O
shown	O
in	O
figure	O
5.10	O
(	O
b	O
)	O
the	O
predictions	O
are	O
also	O
near	O
perfect	O
,	O
and	O
the	O
log	O
marginal	O
likelihood	B
is	O
much	O
larger	O
at	O
50.2	O
.	O
5.5	O
model	B
selection	O
for	O
gp	O
classiﬁcation	B
in	O
this	O
section	O
we	O
compute	O
the	O
derivatives	O
of	O
the	O
approximate	O
marginal	B
likeli-	O
hood	O
for	O
the	O
laplace	O
and	O
ep	O
methods	O
for	O
binary	B
classiﬁcation	I
which	O
are	O
needed	O
for	O
training	O
.	O
we	O
also	O
give	O
the	O
detailed	O
algorithms	O
for	O
these	O
,	O
and	O
brieﬂy	O
discuss	O
the	O
possible	O
use	O
of	O
cross-validation	B
and	O
other	O
methods	O
for	O
training	O
binary	B
gp	O
−1−0.500.51−2−1012output	O
,	O
yinput	O
,	O
x−1−0.500.51−2−1012output	O
,	O
yinput	O
,	O
x	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
5.5	O
model	B
selection	O
for	O
gp	O
classiﬁcation	B
125	O
classiﬁers	O
.	O
5.5.1	O
derivatives	O
of	O
the	O
marginal	B
likelihood	I
for	O
laplace	O
’	O
s	O
∗	O
approximation	O
recall	O
from	O
section	O
3.4.4	O
that	O
the	O
approximate	O
log	O
marginal	O
likelihood	B
was	O
given	O
in	O
eq	O
.	O
(	O
3.32	O
)	O
as	O
log	O
q	O
(	O
y|x	O
,	O
θ	O
)	O
=	O
−1	O
2	O
ˆf	O
>	O
k−1ˆf	O
+	O
log	O
p	O
(	O
y|ˆf	O
)	O
−	O
1	O
2	O
log	O
|b|	O
,	O
(	O
5.20	O
)	O
1	O
1	O
2	O
kw	O
2	O
and	O
ˆf	O
is	O
the	O
maximum	O
of	O
the	O
posterior	O
eq	O
.	O
(	O
3.12	O
)	O
where	O
b	O
=	O
i	O
+	O
w	O
found	O
by	O
newton	O
’	O
s	O
method	O
in	O
algorithm	O
3.1	O
,	O
and	O
w	O
is	O
the	O
diagonal	O
matrix	B
w	O
=	O
−∇∇	O
log	O
p	O
(	O
y|ˆf	O
)	O
.	O
we	O
can	O
now	O
optimize	O
the	O
approximate	O
marginal	B
likeli-	O
hood	O
q	O
(	O
y|x	O
,	O
θ	O
)	O
w.r.t	O
.	O
the	O
hyperparameters	B
,	O
θ.	O
to	O
this	O
end	O
we	O
seek	O
the	O
partial	O
derivatives	O
of	O
∂q	O
(	O
y|x	O
,	O
θ	O
)	O
/∂θj	O
.	O
the	O
covariance	B
matrix	I
k	O
is	O
a	O
function	B
of	O
the	O
hy-	O
perparameters	O
,	O
but	O
ˆf	O
and	O
therefore	O
w	O
are	O
also	O
implicitly	O
functions	O
of	O
θ	O
,	O
since	O
when	O
θ	O
changes	O
,	O
the	O
optimum	O
of	O
the	O
posterior	O
ˆf	O
also	O
changes	O
.	O
thus	O
∂	O
ˆfi	O
∂θj	O
=	O
∂	O
log	O
q	O
(	O
y|x	O
,	O
θ	O
)	O
∂	O
log	O
q	O
(	O
y|x	O
,	O
θ	O
)	O
∂	O
log	O
q	O
(	O
y|x	O
,	O
θ	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
explicit	O
nx	O
(	O
5.21	O
)	O
∂	O
ˆfi	O
∂θj	O
∂θj	O
+	O
i=1	O
by	O
the	O
chain	O
rule	O
.	O
using	O
eq	O
.	O
(	O
a.14	O
)	O
and	O
eq	O
.	O
(	O
a.15	O
)	O
the	O
explicit	O
term	O
is	O
given	O
by	O
=	O
1	O
2	O
ˆf	O
>	O
k−1	O
∂k	O
∂θj	O
k−1ˆf−	O
1	O
2	O
tr	O
(	O
w	O
−1+k	O
)	O
−1	O
∂k	O
∂θj	O
.	O
(	O
5.22	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
explicit	O
∂	O
log	O
q	O
(	O
y|x	O
,	O
θ	O
)	O
∂θj	O
when	O
evaluating	O
the	O
remaining	O
term	O
from	O
eq	O
.	O
(	O
5.21	O
)	O
,	O
we	O
utilize	O
the	O
fact	O
that	O
ˆf	O
is	O
the	O
maximum	O
of	O
the	O
posterior	O
,	O
so	O
that	O
∂ψ	O
(	O
f	O
)	O
/∂f	O
=	O
0	O
at	O
f	O
=	O
ˆf	O
,	O
where	O
the	O
(	O
un-normalized	O
)	O
log	O
posterior	O
ψ	O
(	O
f	O
)	O
is	O
deﬁned	O
in	O
eq	O
.	O
(	O
3.12	O
)	O
;	O
thus	O
the	O
implicit	O
derivatives	O
of	O
the	O
two	O
ﬁrst	O
terms	O
of	O
eq	O
.	O
(	O
5.20	O
)	O
vanish	O
,	O
leaving	O
only	O
∂	O
log	O
q	O
(	O
y|x	O
,	O
θ	O
)	O
∂	O
ˆfi	O
=	O
−1	O
2	O
=	O
−1	O
2	O
∂	O
log	O
|b|	O
=	O
−1	O
2	O
∂	O
ˆfi	O
(	O
cid:2	O
)	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
(	O
cid:3	O
)	O
tr	O
(	O
k−1	O
+	O
w	O
)	O
−1	O
∂w	O
∂	O
ˆfi	O
log	O
p	O
(	O
y|ˆf	O
)	O
.	O
∂3	O
∂f	O
3	O
i	O
ii	O
(	O
5.23	O
)	O
=	O
∂k	O
∂θj	O
in	O
order	O
to	O
evaluate	O
the	O
derivative	O
∂ˆf	O
/∂θj	O
,	O
we	O
diﬀerentiate	O
the	O
self-consistent	O
eq	O
.	O
(	O
3.17	O
)	O
ˆf	O
=	O
k∇	O
log	O
p	O
(	O
y|ˆf	O
)	O
to	O
obtain	O
∂ˆf	O
∇	O
log	O
p	O
(	O
y|ˆf	O
)	O
,	O
∂θj	O
(	O
5.24	O
)	O
where	O
we	O
have	O
used	O
the	O
chain	O
rule	O
∂/∂θj	O
=	O
∂ˆf	O
/∂θj	O
·	O
∂/∂ˆf	O
and	O
the	O
identity	O
∂∇	O
log	O
p	O
(	O
y|ˆf	O
)	O
/∂ˆf	O
=	O
−w	O
.	O
the	O
desired	O
derivatives	O
are	O
obtained	O
by	O
plugging	O
eq	O
.	O
(	O
5.22-5.24	O
)	O
into	O
eq	O
.	O
(	O
5.21	O
)	O
.	O
=	O
(	O
i	O
+kw	O
)	O
−1	O
∂k	O
∂θj	O
∇	O
log	O
p	O
(	O
y|ˆf	O
)	O
+k	O
∂∇	O
log	O
p	O
(	O
y|ˆf	O
)	O
∂ˆf	O
∂θj	O
∂ˆf	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
,	O
(	O
cid:17	O
)	O
(	O
cid:17	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
126	O
model	B
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
(	O
f	O
,	O
a	O
)	O
:	O
=	O
mode	O
(	O
cid:0	O
)	O
k	O
,	O
y	O
,	O
p	O
(	O
y|f	O
)	O
(	O
cid:1	O
)	O
input	O
:	O
x	O
(	O
inputs	O
)	O
,	O
y	O
(	O
±1	O
targets	O
)	O
,	O
θ	O
(	O
hypers	O
)	O
,	O
p	O
(	O
y|f	O
)	O
(	O
likelihood	B
function	O
)	O
compute	O
covariance	B
matrix	I
from	O
x	O
and	O
θ	O
locate	O
posterior	O
mode	O
using	O
algorithm	O
3.1	O
2	O
:	O
compute	O
k	O
4	O
:	O
w	O
:	O
=	O
−∇∇	O
log	O
p	O
(	O
y|f	O
)	O
l	O
:	O
=	O
cholesky	O
(	O
i	O
+	O
w	O
6	O
:	O
log	O
z	O
:	O
=	O
−	O
1	O
2	O
l	O
>	O
\	O
(	O
l\w	O
1	O
1	O
8	O
:	O
c	O
:	O
=	O
l\	O
(	O
w	O
r	O
:	O
=	O
w	O
s2	O
:	O
=	O
−	O
1	O
1	O
2	O
)	O
1	O
2	O
kw	O
2	O
a	O
>	O
f	O
+	O
log	O
p	O
(	O
y|f	O
)	O
−p	O
log	O
(	O
diag	O
(	O
l	O
)	O
)	O
2	O
diag	O
(	O
cid:0	O
)	O
diag	O
(	O
k	O
)	O
−	O
diag	O
(	O
c	O
>	O
c	O
)	O
(	O
cid:1	O
)	O
∇3	O
log	O
p	O
(	O
y|f	O
)	O
r	O
=	O
w	O
2	O
k	O
)	O
2	O
)	O
1	O
solve	O
ll	O
>	O
=	O
b	O
=	O
i	O
+	O
w	O
1	O
2	O
kw	O
1	O
2	O
1	O
2	O
(	O
i	O
+	O
w	O
1	O
2	O
kw	O
1	O
eq	O
.	O
(	O
5.20	O
)	O
2	O
)	O
−1w	O
eq	O
.	O
(	O
5.23	O
)	O
1	O
2	O
o	O
o	O
eq	O
.	O
(	O
5.24	O
)	O
eq	O
.	O
(	O
5.21	O
)	O
10	O
:	O
for	O
j	O
:	O
=	O
1	O
.	O
.	O
.	O
dim	O
(	O
θ	O
)	O
do	O
12	O
:	O
14	O
:	O
c	O
:	O
=	O
∂k/∂θj	O
2	O
a	O
>	O
ca	O
−	O
1	O
s1	O
:	O
=	O
1	O
b	O
:	O
=	O
c∇	O
log	O
p	O
(	O
y|f	O
)	O
s3	O
:	O
=	O
b	O
−	O
krb	O
∇j	O
log	O
z	O
:	O
=	O
s1	O
+	O
s	O
>	O
2	O
s3	O
2	O
tr	O
(	O
rc	O
)	O
compute	O
derivative	O
matrix	O
from	O
x	O
and	O
θ	O
eq	O
.	O
(	O
5.22	O
)	O
16	O
:	O
end	O
for	O
return	O
:	O
log	O
z	O
(	O
log	O
marginal	O
likelihood	B
)	O
,	O
∇	O
log	O
z	O
(	O
partial	O
derivatives	O
)	O
algorithm	O
5.1	O
:	O
compute	O
the	O
approximate	O
log	O
marginal	O
likelihood	B
and	O
its	O
derivatives	O
w.r.t	O
.	O
the	O
hyperparameters	B
for	O
binary	B
laplace	O
gpc	O
for	O
use	O
by	O
an	O
optimization	O
routine	O
,	O
such	O
as	O
conjugate	O
gradient	O
optimization	O
.	O
in	O
line	O
3	O
algorithm	O
3.1	O
on	O
page	O
46	O
is	O
called	O
to	O
locate	O
the	O
posterior	O
mode	O
.	O
in	O
line	O
12	O
only	O
the	O
diagonal	O
elements	O
of	O
the	O
matrix	B
product	O
should	O
be	O
computed	O
.	O
in	O
line	O
15	O
the	O
notation	O
∇j	O
means	O
the	O
partial	O
derivative	O
w.r.t	O
.	O
the	O
j	O
’	O
th	O
hyperparameter	O
.	O
an	O
actual	O
implementation	O
may	O
also	O
return	O
the	O
value	O
of	O
f	O
to	O
be	O
used	O
as	O
an	O
initial	O
guess	O
for	O
the	O
subsequent	O
call	O
(	O
as	O
an	O
alternative	O
the	O
zero	O
initialization	O
in	O
line	O
2	O
of	O
algorithm	O
3.1	O
)	O
.	O
details	O
of	O
the	O
implementation	O
the	O
implementation	O
of	O
the	O
log	O
marginal	O
likelihood	B
and	O
its	O
partial	O
derivatives	O
w.r.t	O
.	O
the	O
hyperparameters	B
is	O
shown	O
in	O
algorithm	O
5.1.	O
it	O
is	O
advantageous	O
to	O
re-	O
write	O
the	O
equations	O
from	O
the	O
previous	O
section	O
in	O
terms	O
of	O
well-conditioned	O
sym-	O
metric	O
positive	B
deﬁnite	I
matrices	O
,	O
whose	O
solutions	O
can	O
be	O
obtained	O
by	O
cholesky	O
factorization	O
,	O
combining	O
numerical	O
stability	O
with	O
computational	O
speed	O
.	O
in	O
detail	O
,	O
the	O
matrix	B
of	O
central	O
importance	O
turns	O
out	O
to	O
be	O
2	O
)	O
−1w	O
r	O
=	O
(	O
w	O
−1	O
+	O
k	O
)	O
−1	O
=	O
w	O
2	O
(	O
i	O
+	O
w	O
1	O
2	O
kw	O
1	O
1	O
1	O
2	O
,	O
(	O
5.25	O
)	O
where	O
the	O
right	O
hand	O
side	O
is	O
suitable	O
for	O
numerical	O
evaluation	O
as	O
in	O
line	O
7	O
of	O
algorithm	O
5.1	O
,	O
reusing	O
the	O
cholesky	O
factor	O
l	O
from	O
the	O
newton	O
scheme	O
above	O
.	O
remember	O
that	O
w	O
is	O
diagonal	O
so	O
eq	O
.	O
(	O
5.25	O
)	O
does	O
not	O
require	O
any	O
real	O
matrix-by-	O
matrix	B
products	O
.	O
rewriting	O
eq	O
.	O
(	O
5.22-5.23	O
)	O
is	O
straightforward	O
,	O
and	O
for	O
eq	O
.	O
(	O
5.24	O
)	O
we	O
apply	O
the	O
matrix	B
inversion	O
lemma	O
(	O
eq	O
.	O
(	O
a.9	O
)	O
)	O
to	O
(	O
i	O
+	O
kw	O
)	O
−1	O
to	O
obtain	O
i	O
−	O
kr	O
,	O
which	O
is	O
used	O
in	O
the	O
implementation	O
.	O
the	O
computational	O
complexity	O
is	O
dominated	O
by	O
the	O
cholesky	O
factorization	O
in	O
line	O
5	O
which	O
takes	O
n3/6	O
operations	O
per	O
iteration	O
of	O
the	O
newton	O
scheme	O
.	O
in	O
addition	O
the	O
computation	O
of	O
r	O
in	O
line	O
7	O
is	O
also	O
o	O
(	O
n3	O
)	O
,	O
all	O
other	O
computations	O
being	O
at	O
most	O
o	O
(	O
n2	O
)	O
per	O
hyperparameter	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
5.5	O
model	B
selection	O
for	O
gp	O
classiﬁcation	B
127	O
input	O
:	O
x	O
(	O
inputs	O
)	O
,	O
y	O
(	O
±1	O
targets	O
)	O
,	O
θ	O
(	O
hyperparameters	B
)	O
(	O
˜ν	O
,	O
˜τ	O
,	O
log	O
zep	O
)	O
:	O
=	O
ep	O
(	O
cid:0	O
)	O
k	O
,	O
y	O
(	O
cid:1	O
)	O
2	O
:	O
compute	O
k	O
2	O
k	O
˜s	O
4	O
:	O
l	O
:	O
=	O
cholesky	O
(	O
i	O
+	O
˜s	O
2	O
l\	O
(	O
l	O
>	O
\	O
˜s	O
b	O
:	O
=	O
˜ν	O
−	O
˜s	O
6	O
:	O
r	O
:	O
=	O
bb	O
>	O
−	O
˜s	O
2	O
l	O
>	O
\	O
(	O
l\	O
˜s	O
for	O
j	O
:	O
=	O
1	O
.	O
.	O
.	O
dim	O
(	O
θ	O
)	O
do	O
2	O
)	O
2	O
k	O
˜ν	O
)	O
2	O
)	O
1	O
1	O
1	O
1	O
1	O
1	O
compute	O
covariance	B
matrix	I
from	O
x	O
and	O
θ	O
run	O
the	O
ep	O
algorithm	O
3.5	O
2	O
k	O
˜s	O
b	O
from	O
under	O
eq	O
.	O
(	O
5.27	O
)	O
r	O
=	O
bb	O
>	O
−	O
˜s	O
2	O
b−1	O
˜s	O
solve	O
ll	O
>	O
=	O
b	O
=	O
i	O
+	O
˜s	O
1	O
2	O
1	O
2	O
1	O
1	O
8	O
:	O
2	O
tr	O
(	O
rc	O
)	O
10	O
:	O
end	O
for	O
c	O
:	O
=	O
∂k/∂θj	O
∇j	O
log	O
zep	O
:	O
=	O
1	O
compute	O
derivative	O
matrix	O
from	O
x	O
and	O
θ	O
eq	O
.	O
(	O
5.27	O
)	O
return	O
:	O
log	O
zep	O
(	O
log	O
marginal	O
likelihood	B
)	O
,	O
∇	O
log	O
zep	O
(	O
partial	O
derivatives	O
)	O
algorithm	O
5.2	O
:	O
compute	O
the	O
log	O
marginal	O
likelihood	B
and	O
its	O
derivatives	O
w.r.t	O
.	O
the	O
hyperparameters	B
for	O
ep	O
binary	B
gp	O
classiﬁcation	B
for	O
use	O
by	O
an	O
optimization	O
routine	O
,	O
such	O
as	O
conjugate	O
gradient	O
optimization	O
.	O
˜s	O
is	O
a	O
diagonal	O
precision	O
matrix	B
with	O
entries	O
˜sii	O
=	O
˜τi	O
.	O
in	O
line	O
3	O
algorithm	O
3.5	O
on	O
page	O
58	O
is	O
called	O
to	O
compute	O
parameters	O
of	O
the	O
ep	O
approximation	O
.	O
in	O
line	O
9	O
only	O
the	O
diagonal	O
of	O
the	O
matrix	B
product	O
should	O
be	O
computed	O
and	O
the	O
notation	O
∇j	O
means	O
the	O
partial	O
derivative	O
w.r.t	O
.	O
the	O
j	O
’	O
th	O
hyperparameter	O
.	O
the	O
computational	O
complexity	O
is	O
dominated	O
by	O
the	O
cholesky	O
factorization	O
in	O
line	O
4	O
and	O
the	O
solution	O
in	O
line	O
6	O
,	O
both	O
of	O
which	O
are	O
o	O
(	O
n3	O
)	O
.	O
5.5.2	O
derivatives	O
of	O
the	O
marginal	B
likelihood	I
for	O
ep	O
∗	O
optimization	O
of	O
the	O
ep	O
approximation	O
to	O
the	O
marginal	B
likelihood	I
w.r.t	O
.	O
the	O
hyperparameters	B
of	O
the	O
covariance	B
function	I
requires	O
evaluation	O
of	O
the	O
partial	O
derivatives	O
from	O
eq	O
.	O
(	O
3.65	O
)	O
.	O
luckily	O
,	O
it	O
turns	O
out	O
that	O
implicit	O
terms	O
in	O
the	O
derivatives	O
caused	O
by	O
the	O
solution	O
of	O
ep	O
being	O
a	O
function	B
of	O
the	O
hyperparam-	O
eters	O
is	O
exactly	O
zero	O
.	O
we	O
will	O
not	O
present	O
the	O
proof	O
here	O
,	O
see	B
seeger	O
[	O
2005	O
]	O
.	O
consequently	O
,	O
we	O
only	O
have	O
to	O
take	O
account	O
of	O
the	O
explicit	O
dependencies	O
(	O
cid:0	O
)	O
−	O
1	O
log	O
|k	O
+	O
˜σ|	O
(	O
cid:1	O
)	O
∂	O
log	O
zep	O
∂θj	O
˜µ	O
>	O
(	O
k	O
+	O
˜σ	O
)	O
−1	O
˜µ	O
−	O
1	O
2	O
=	O
∂	O
∂θj	O
1	O
˜µ	O
>	O
(	O
k	O
+	O
˜s−1	O
)	O
−1	O
∂k	O
2	O
∂θj	O
=	O
2	O
(	O
k	O
+	O
˜s−1	O
)	O
−1	O
˜µ	O
−	O
1	O
2	O
(	O
5.26	O
)	O
(	O
cid:1	O
)	O
.	O
tr	O
(	O
cid:0	O
)	O
(	O
k	O
+	O
˜s−1	O
)	O
−1	O
∂k	O
(	O
cid:17	O
)	O
2	O
(	O
cid:1	O
)	O
∂k	O
∂θj	O
1	O
,	O
(	O
5.27	O
)	O
∂θj	O
(	O
cid:16	O
)	O
(	O
cid:0	O
)	O
bb	O
>	O
−	O
˜s	O
1	O
2	O
b−1s	O
in	O
algorithm	O
5.2	O
the	O
derivatives	O
from	O
eq	O
.	O
(	O
5.26	O
)	O
are	O
implemented	O
using	O
∂	O
log	O
zep	O
=	O
1	O
2	O
tr	O
where	O
b	O
=	O
(	O
i	O
−	O
˜s	O
∂θj	O
2	O
b−1	O
˜s	O
1	O
1	O
2	O
k	O
)	O
˜ν	O
.	O
5.5.3	O
cross-validation	B
whereas	O
the	O
loo-cv	O
estimates	O
were	O
easily	O
computed	O
for	O
regression	B
through	O
the	O
use	O
of	O
rank-one	O
updates	O
,	O
it	O
is	O
not	O
so	O
obvious	O
how	O
to	O
generalize	O
this	O
to	O
classiﬁcation	B
.	O
opper	O
and	O
winther	O
[	O
2000	O
,	O
sec	O
.	O
5	O
]	O
use	O
the	O
cavity	O
distributions	O
of	O
their	O
mean-ﬁeld	O
approach	O
as	O
loo-cv	O
estimates	O
,	O
and	O
one	O
could	O
similarly	O
use	O
the	O
cavity	O
distributions	O
from	O
the	O
closely-related	O
ep	O
algorithm	O
discussed	O
in	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
128	O
model	B
selection	O
and	O
adaptation	O
of	O
hyperparameters	B
section	O
3.6.	O
although	O
technically	O
the	O
cavity	O
distribution	O
for	O
site	O
i	O
could	O
depend	O
on	O
the	O
label	O
yi	O
(	O
because	O
the	O
algorithm	O
uses	O
all	O
cases	O
when	O
converging	O
to	O
its	O
ﬁxed	O
point	O
)	O
,	O
this	O
eﬀect	O
is	O
probably	O
very	O
small	O
and	O
indeed	O
opper	O
and	O
winther	O
[	O
2000	O
,	O
sec	O
.	O
8	O
]	O
report	O
very	O
high	O
precision	O
for	O
these	O
loo-cv	O
estimates	O
.	O
as	O
an	O
alternative	O
k-fold	O
cv	O
could	O
be	O
used	O
explicitly	O
for	O
some	O
moderate	O
value	O
of	O
k.	O
other	O
methods	O
for	O
setting	O
hyperparameters	B
alignment	O
above	O
we	O
have	O
considered	O
setting	O
hyperparameters	B
by	O
optimizing	O
the	O
marginal	B
likelihood	I
or	O
cross-validation	B
criteria	O
.	O
however	O
,	O
some	O
other	O
criteria	O
have	O
been	O
proposed	O
in	O
the	O
literature	O
.	O
for	O
example	O
cristianini	O
et	O
al	O
.	O
[	O
2002	O
]	O
deﬁne	O
the	O
alignment	B
between	O
a	O
gram	O
matrix	B
k	O
and	O
the	O
corresponding	O
+1/−	O
1	O
vector	O
of	O
targets	O
y	O
as	O
y	O
>	O
ky	O
nkkkf	O
a	O
(	O
k	O
,	O
y	O
)	O
=	O
trices	O
ki	O
so	O
that	O
k	O
=p	O
(	O
5.28	O
)	O
where	O
kkkf	O
denotes	O
the	O
frobenius	O
norm	B
of	O
the	O
matrix	B
k	O
,	O
as	O
deﬁned	O
in	O
eq	O
.	O
(	O
a.16	O
)	O
.	O
lanckriet	O
et	O
al	O
.	O
[	O
2004	O
]	O
show	O
that	O
if	O
k	O
is	O
a	O
convex	B
combination	O
of	O
gram	O
ma-	O
i	O
νiki	O
with	O
νi	O
≥	O
0	O
for	O
all	O
i	O
then	O
the	O
optimization	O
of	O
the	O
alignment	B
score	O
w.r.t	O
.	O
the	O
νi	O
’	O
s	O
can	O
be	O
achieved	O
by	O
solving	O
a	O
semideﬁnite	O
programming	O
problem	O
.	O
,	O
5.5.4	O
example	O
for	O
an	O
example	O
of	O
model	B
selection	O
,	O
refer	O
to	O
section	O
3.7.	O
although	O
the	O
experi-	O
ments	O
there	O
were	O
done	O
by	O
exhaustively	O
evaluating	O
the	O
marginal	B
likelihood	I
for	O
a	O
whole	O
grid	O
of	O
hyperparameter	O
values	O
,	O
the	O
techniques	O
described	O
in	O
this	O
chapter	O
could	O
be	O
used	O
to	O
locate	O
the	O
same	O
solutions	O
more	O
eﬃciently	O
.	O
5.6	O
exercises	O
1.	O
the	O
optimization	O
of	O
the	O
marginal	B
likelihood	I
w.r.t	O
.	O
the	O
hyperparameters	B
is	O
generally	O
not	O
possible	O
in	O
closed	O
form	O
.	O
consider	O
,	O
however	O
,	O
the	O
situation	O
where	O
one	O
hyperparameter	O
,	O
θ0	O
gives	O
the	O
overall	O
scale	O
of	O
the	O
covariance	B
ky	O
(	O
x	O
,	O
x0	O
)	O
=	O
θ0	O
˜ky	O
(	O
x	O
,	O
x0	O
)	O
,	O
(	O
5.29	O
)	O
where	O
ky	O
is	O
the	O
covariance	B
function	I
for	O
the	O
noisy	O
targets	O
(	O
i.e	O
.	O
including	O
noise	O
contributions	O
)	O
and	O
˜ky	O
(	O
x	O
,	O
x0	O
)	O
may	O
depend	O
on	O
further	O
hyperparam-	O
eters	O
,	O
θ1	O
,	O
θ2	O
,	O
.	O
.	O
..	O
show	O
that	O
the	O
marginal	B
likelihood	I
can	O
be	O
optimized	O
w.r.t	O
.	O
θ0	O
in	O
closed	O
form	O
.	O
p	O
given	O
by	O
p	O
2.	O
consider	O
the	O
diﬀerence	O
between	O
the	O
log	O
marginal	O
likelihood	B
given	O
by	O
:	O
i	O
log	O
p	O
(	O
yi|	O
{	O
yj	O
,	O
j	O
<	O
i	O
}	O
)	O
,	O
and	O
the	O
loo-cv	O
using	O
log	O
probability	O
which	O
is	O
i	O
log	O
p	O
(	O
yi|	O
{	O
yj	O
,	O
j	O
6=	O
i	O
}	O
)	O
.	O
from	O
the	O
viewpoint	O
of	O
the	O
marginal	B
likelihood	I
the	O
loo-cv	O
conditions	O
too	O
much	O
on	O
the	O
data	O
.	O
show	O
that	O
the	O
expected	O
loo-cv	O
loss	B
is	O
greater	O
than	O
the	O
expected	O
marginal	B
likelihood	I
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
chapter	O
6	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
in	O
this	O
chapter	O
we	O
discuss	O
a	O
number	O
of	O
concepts	O
and	O
models	O
that	O
are	O
related	O
to	O
gaussian	O
process	B
prediction	O
.	O
in	O
section	O
6.1	O
we	O
cover	O
reproducing	O
kernel	O
hilbert	O
spaces	O
(	O
rkhss	O
)	O
,	O
which	O
deﬁne	O
a	O
hilbert	O
space	O
of	O
suﬃciently-smooth	O
functions	O
corresponding	O
to	O
a	O
given	O
positive	B
semideﬁnite	I
kernel	O
k.	O
as	O
we	O
discussed	O
in	O
chapter	O
1	O
,	O
there	O
are	O
many	O
functions	O
that	O
are	O
consistent	O
with	O
a	O
given	O
dataset	B
d.	O
we	O
have	O
seen	O
how	O
the	O
gp	O
approach	O
puts	O
a	O
prior	O
over	O
functions	O
in	O
order	O
to	O
deal	O
with	O
this	O
issue	O
.	O
a	O
related	O
viewpoint	O
is	O
provided	O
by	O
regularization	B
theory	O
(	O
described	O
in	O
section	O
6.2	O
)	O
where	O
one	O
seeks	O
a	O
trade-oﬀ	O
between	O
data-ﬁt	O
and	O
the	O
rkhs	O
norm	B
of	O
function	B
.	O
this	O
is	O
closely	O
related	O
to	O
the	O
map	O
estimator	O
in	O
gp	O
prediction	B
,	O
and	O
thus	O
omits	O
uncertainty	O
in	O
predictions	O
and	O
also	O
the	O
marginal	B
likelihood	I
.	O
in	O
section	O
6.3	O
we	O
discuss	O
splines	B
,	O
a	O
special	O
case	O
of	O
regularization	B
which	O
is	O
obtained	O
when	O
the	O
rkhs	O
is	O
deﬁned	O
in	O
terms	O
of	O
diﬀerential	O
operators	O
of	O
a	O
given	O
order	O
.	O
there	O
are	O
a	O
number	O
of	O
other	O
families	O
of	O
kernel	B
machines	O
that	O
are	O
related	O
to	O
gaussian	O
process	B
prediction	O
.	O
in	O
section	O
6.4	O
we	O
describe	O
support	B
vector	I
ma-	O
chines	O
,	O
in	O
section	O
6.5	O
we	O
discuss	O
least-squares	B
classiﬁcation	I
(	O
lsc	O
)	O
,	O
and	O
in	O
section	O
6.6	O
we	O
cover	O
relevance	O
vector	O
machines	O
(	O
rvms	O
)	O
.	O
6.1	O
reproducing	O
kernel	O
hilbert	O
spaces	O
here	O
we	O
present	O
a	O
brief	O
introduction	O
to	O
reproducing	O
kernel	O
hilbert	O
spaces	O
.	O
the	O
theory	O
was	O
developed	O
by	O
aronszajn	O
[	O
1950	O
]	O
;	O
a	O
more	O
recent	O
treatise	O
is	O
saitoh	O
[	O
1988	O
]	O
.	O
information	O
can	O
also	O
be	O
found	O
in	O
wahba	O
[	O
1990	O
]	O
,	O
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
]	O
and	O
wegman	O
[	O
1982	O
]	O
.	O
the	O
collection	O
of	O
papers	O
edited	O
by	O
weinert	O
[	O
1982	O
]	O
provides	O
an	O
overview	O
of	O
the	O
uses	O
of	O
rkhss	O
in	O
statistical	O
signal	O
processing	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
130	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
we	O
start	O
with	O
a	O
formal	O
deﬁnition	O
of	O
a	O
rkhs	O
,	O
and	O
then	O
describe	O
two	O
speciﬁc	O
bases	O
for	O
a	O
rkhs	O
,	O
ﬁrstly	O
through	O
mercer	O
’	O
s	O
theorem	O
and	O
the	O
eigenfunctions	O
of	O
k	O
,	O
and	O
secondly	O
through	O
the	O
reproducing	O
kernel	O
map	O
.	O
deﬁnition	O
6.1	O
(	O
reproducing	O
kernel	O
hilbert	O
space	O
)	O
.	O
let	O
h	O
be	O
a	O
hilbert	O
space	O
of	O
real	O
functions	O
f	O
deﬁned	O
on	O
an	O
index	O
set	B
x	O
.	O
then	O
h	O
is	O
called	O
a	O
reproducing	O
kernel	O
hilbert	O
space	O
endowed	O
with	O
an	O
inner	O
product	O
h·	O
,	O
·ih	O
(	O
and	O
norm	B
kfkh	O
=	O
phf	O
,	O
fih	O
)	O
if	O
there	O
exists	O
a	O
function	B
k	O
:	O
x	O
×x	O
→	O
r	O
with	O
the	O
following	O
properties	O
:	O
1.	O
for	O
every	O
x	O
,	O
k	O
(	O
x	O
,	O
x0	O
)	O
as	O
a	O
function	B
of	O
x0	O
belongs	O
to	O
h	O
,	O
and	O
2.	O
k	O
has	O
the	O
reproducing	O
property	O
hf	O
(	O
·	O
)	O
,	O
k	O
(	O
·	O
,	O
x	O
)	O
ih	O
=	O
f	O
(	O
x	O
)	O
.	O
(	O
cid:3	O
)	O
see	B
e.g	O
.	O
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
]	O
and	O
wegman	O
[	O
1982	O
]	O
.	O
note	O
also	O
that	O
as	O
k	O
(	O
x	O
,	O
·	O
)	O
and	O
k	O
(	O
x0	O
,	O
·	O
)	O
are	O
in	O
h	O
we	O
have	O
that	O
hk	O
(	O
x	O
,	O
·	O
)	O
,	O
k	O
(	O
x0	O
,	O
·	O
)	O
ih	O
=	O
k	O
(	O
x	O
,	O
x0	O
)	O
.	O
reproducing	O
property	O
the	O
rkhs	O
uniquely	O
determines	O
k	O
,	O
and	O
vice	O
versa	O
,	O
as	O
stated	O
in	O
the	O
following	O
theorem	O
:	O
theorem	O
6.1	O
(	O
moore-aronszajn	O
theorem	O
,	O
aronszajn	O
[	O
1950	O
]	O
)	O
.	O
let	O
x	O
be	O
an	O
in-	O
dex	O
set	B
.	O
then	O
for	O
every	O
positive	B
deﬁnite	I
function	O
k	O
(	O
·	O
,	O
·	O
)	O
on	O
x	O
×	O
x	O
there	O
exists	O
(	O
cid:3	O
)	O
a	O
unique	O
rkhs	O
,	O
and	O
vice	O
versa	O
.	O
the	O
hilbert	O
space	O
l2	O
(	O
which	O
has	O
the	O
dot	B
product	I
hf	O
,	O
gil2	O
=r	O
f	O
(	O
x	O
)	O
g	O
(	O
x	O
)	O
dx	O
)	O
function	B
is	O
the	O
representer	O
of	O
evaluation	O
,	O
i.e	O
.	O
f	O
(	O
x	O
)	O
=r	O
f	O
(	O
x0	O
)	O
δ	O
(	O
x−x0	O
)	O
dx0	O
.	O
kernels	O
contains	O
many	O
non-smooth	O
functions	O
.	O
in	O
l2	O
(	O
which	O
is	O
not	O
a	O
rkhs	O
)	O
the	O
delta	O
are	O
the	O
analogues	O
of	O
delta	O
functions	O
within	O
the	O
smoother	O
rkhs	O
.	O
note	O
that	O
the	O
delta	O
function	B
is	O
not	O
itself	O
in	O
l2	O
;	O
in	O
contrast	O
for	O
a	O
rkhs	O
the	O
kernel	B
k	O
is	O
the	O
representer	O
of	O
evaluation	O
and	O
is	O
itself	O
in	O
the	O
rkhs	O
.	O
inner	O
product	O
hf	O
,	O
gih	O
the	O
above	O
description	O
is	O
perhaps	O
rather	O
abstract	O
.	O
for	O
our	O
purposes	O
the	O
key	O
intuition	O
behind	O
the	O
rkhs	O
formalism	O
is	O
that	O
the	O
squared	B
norm	O
kfk2h	O
can	O
be	O
thought	O
of	O
as	O
a	O
generalization	B
to	O
functions	O
of	O
the	O
n-dimensional	O
quadratic	B
form	I
f	O
>	O
k−1f	O
we	O
have	O
seen	O
in	O
earlier	O
chapters	O
.	O
consider	O
a	O
real	O
positive	B
semideﬁnite	I
kernel	O
k	O
(	O
x	O
,	O
x0	O
)	O
with	O
an	O
eigenfunction	B
i=1λiφi	O
(	O
x	O
)	O
φi	O
(	O
x0	O
)	O
relative	O
to	O
a	O
measure	B
µ.	O
recall	O
from	O
mercer	O
’	O
s	O
theorem	O
that	O
the	O
eigenfunctions	O
are	O
orthonormal	O
w.r.t	O
.	O
µ	O
,	O
i.e	O
.	O
we	O
have	O
expansion	O
k	O
(	O
x	O
,	O
x0	O
)	O
=pn	O
r	O
φi	O
(	O
x	O
)	O
φj	O
(	O
x	O
)	O
dµ	O
(	O
x	O
)	O
=	O
δij	O
.	O
we	O
now	O
consider	O
a	O
hilbert	O
space	O
comprised	O
of	O
linear	B
combinations	O
of	O
the	O
eigenfunctions	O
,	O
i.e	O
.	O
f	O
(	O
x	O
)	O
=pn	O
functions	O
f	O
(	O
x	O
)	O
and	O
g	O
(	O
x	O
)	O
=pn	O
i	O
/λi	O
<	O
∞	O
.	O
we	O
assert	O
that	O
the	O
inner	O
product	O
hf	O
,	O
gih	O
in	O
the	O
hilbert	O
space	O
between	O
i=1fiφi	O
(	O
x	O
)	O
withpn	O
i=1giφi	O
(	O
x	O
)	O
is	O
deﬁned	O
as	O
i=1f	O
2	O
pn	O
thus	O
this	O
hilbert	O
space	O
is	O
equipped	O
with	O
a	O
norm	B
kfkh	O
where	O
kfk2h	O
=	O
hf	O
,	O
fih	O
=	O
i	O
/λi	O
.	O
note	O
that	O
for	O
kfkh	O
to	O
be	O
ﬁnite	O
the	O
sequence	O
of	O
coeﬃcients	O
{	O
fi	O
}	O
must	O
decay	O
quickly	O
;	O
eﬀectively	O
this	O
imposes	O
a	O
smoothness	O
condition	O
on	O
the	O
space	O
.	O
i=1f	O
2	O
i=1	O
nx	O
figi	O
λi	O
hf	O
,	O
gih	O
=	O
.	O
(	O
6.1	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
6.1	O
reproducing	O
kernel	O
hilbert	O
spaces	O
131	O
we	O
now	O
need	O
to	O
show	O
that	O
this	O
hilbert	O
space	O
is	O
the	O
rkhs	O
corresponding	O
to	O
the	O
kernel	B
k	O
,	O
i.e	O
.	O
that	O
it	O
has	O
the	O
reproducing	O
property	O
.	O
this	O
is	O
easily	O
achieved	O
as	O
hf	O
(	O
·	O
)	O
,	O
k	O
(	O
·	O
,	O
x	O
)	O
ih	O
=	O
=	O
f	O
(	O
x	O
)	O
.	O
(	O
6.2	O
)	O
similarly	O
nx	O
fiλiφi	O
(	O
x	O
)	O
λi	O
i=1	O
nx	O
λiφi	O
(	O
x	O
)	O
λiφi	O
(	O
x0	O
)	O
hk	O
(	O
x	O
,	O
·	O
)	O
,	O
k	O
(	O
x0	O
,	O
·	O
)	O
ih	O
=	O
=	O
k	O
(	O
x	O
,	O
x0	O
)	O
.	O
(	O
6.3	O
)	O
i=1	O
λi	O
notice	O
also	O
that	O
k	O
(	O
x	O
,	O
·	O
)	O
is	O
in	O
the	O
rkhs	O
as	O
it	O
has	O
normpn	O
linear	B
combinations	O
of	O
the	O
eigenfunctions	O
with	O
the	O
restrictionpn	O
i=1	O
(	O
λiφi	O
(	O
x	O
)	O
)	O
2/λi	O
=	O
k	O
(	O
x	O
,	O
x	O
)	O
<	O
∞	O
.	O
we	O
have	O
now	O
demonstrated	O
that	O
the	O
hilbert	O
space	O
comprised	O
of	O
i	O
/λi	O
<	O
∞	O
fulﬁls	O
the	O
two	O
conditions	O
given	O
in	O
deﬁnition	O
6.1.	O
as	O
there	O
is	O
a	O
unique	O
rkhs	O
associated	O
with	O
k	O
(	O
·	O
,	O
·	O
)	O
,	O
this	O
hilbert	O
space	O
must	O
be	O
that	O
rkhs	O
.	O
i=1f	O
2	O
the	O
advantage	O
of	O
the	O
abstract	O
formulation	O
of	O
the	O
rkhs	O
is	O
that	O
the	O
eigenbasis	O
will	O
change	O
as	O
we	O
use	O
diﬀerent	O
measures	O
µ	O
in	O
mercer	O
’	O
s	O
theorem	O
.	O
however	O
,	O
the	O
rkhs	O
norm	B
is	O
in	O
fact	O
solely	O
a	O
property	O
of	O
the	O
kernel	B
and	O
is	O
invariant	O
under	O
this	O
change	O
of	O
measure	B
.	O
this	O
can	O
be	O
seen	O
from	O
the	O
fact	O
that	O
the	O
proof	O
of	O
the	O
rkhs	O
properties	O
above	O
is	O
not	O
dependent	O
on	O
the	O
measure	B
;	O
see	B
also	O
kailath	O
[	O
1971	O
,	O
sec	O
.	O
ii.b	O
]	O
.	O
a	O
ﬁnite-dimensional	O
example	O
of	O
this	O
measure	B
invariance	O
is	O
explored	O
in	O
exercise	O
6.7.1.	O
notice	O
the	O
analogy	O
between	O
the	O
rkhs	O
norm	B
kfk2h	O
=	O
hf	O
,	O
fih	O
=pn	O
if	O
we	O
sample	O
the	O
coeﬃcients	O
fi	O
in	O
the	O
eigenexpansion	O
f	O
(	O
x	O
)	O
=pn	O
i	O
/λi	O
and	O
the	O
quadratic	B
form	I
f	O
>	O
k−1f	O
;	O
if	O
we	O
express	O
k	O
and	O
f	O
in	O
terms	O
of	O
the	O
eigen-	O
vectors	O
of	O
k	O
we	O
obtain	O
exactly	O
the	O
same	O
form	O
(	O
but	O
the	O
sum	O
has	O
only	O
n	O
terms	O
if	O
f	O
has	O
length	O
n	O
)	O
.	O
i=1f	O
2	O
i=1fiφi	O
(	O
x	O
)	O
from	O
n	O
(	O
0	O
,	O
λi	O
)	O
then	O
e	O
[	O
kfk2h	O
]	O
=	O
nx	O
i=1	O
e	O
[	O
f	O
2	O
i	O
]	O
λi	O
=	O
nx	O
i=1	O
1	O
.	O
(	O
6.4	O
)	O
thus	O
if	O
n	O
is	O
inﬁnite	O
the	O
sample	O
functions	O
are	O
not	O
in	O
h	O
(	O
with	O
probability	B
1	O
)	O
as	O
the	O
expected	O
value	O
of	O
the	O
rkhs	O
norm	B
is	O
inﬁnite	O
;	O
see	B
wahba	O
[	O
1990	O
,	O
p.	O
5	O
]	O
and	O
kailath	O
[	O
1971	O
,	O
sec	O
.	O
ii.b	O
]	O
for	O
further	O
details	O
.	O
however	O
,	O
note	O
that	O
although	O
sample	O
functions	O
of	O
this	O
gaussian	O
process	B
are	O
not	O
in	O
h	O
,	O
the	O
posterior	O
mean	O
after	O
observing	O
some	O
data	O
will	O
lie	O
in	O
the	O
rkhs	O
,	O
due	O
to	O
the	O
smoothing	O
properties	O
of	O
averaging	O
.	O
another	O
view	O
of	O
the	O
rkhs	O
can	O
be	O
obtained	O
from	O
the	O
reproducing	O
kernel	O
map	O
construction	O
.	O
we	O
consider	O
the	O
space	O
of	O
functions	O
f	O
deﬁned	O
as	O
n	O
f	O
(	O
x	O
)	O
=	O
nx	O
i=1	O
αik	O
(	O
x	O
,	O
xi	O
)	O
:	O
n	O
∈	O
n	O
,	O
xi	O
∈	O
x	O
,	O
αi	O
∈	O
ro	O
.	O
(	O
6.5	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
132	O
regularizer	O
(	O
kernel	B
)	O
ridge	B
regression	I
representer	O
theorem	O
now	O
let	O
g	O
(	O
x	O
)	O
=pn0	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
j=1α0	O
jk	O
(	O
x	O
,	O
x0	O
j	O
)	O
.	O
then	O
we	O
deﬁne	O
the	O
inner	O
product	O
hf	O
,	O
gih	O
=	O
αiα0	O
jk	O
(	O
xi	O
,	O
x0	O
j	O
)	O
.	O
(	O
6.6	O
)	O
nx	O
n0x	O
i=1	O
j=1	O
nx	O
clearly	O
condition	O
1	O
of	O
deﬁnition	O
6.1	O
is	O
fulﬁlled	O
under	O
the	O
reproducing	O
kernel	O
map	O
construction	O
.	O
we	O
can	O
also	O
demonstrate	O
the	O
reproducing	O
property	O
,	O
as	O
hk	O
(	O
·	O
,	O
x	O
)	O
,	O
f	O
(	O
·	O
)	O
ih	O
=	O
αik	O
(	O
x	O
,	O
xi	O
)	O
=	O
f	O
(	O
x	O
)	O
.	O
(	O
6.7	O
)	O
i=1	O
6.2	O
regularization	B
the	O
problem	O
of	O
inferring	O
an	O
underlying	O
function	B
f	O
(	O
x	O
)	O
from	O
a	O
ﬁnite	O
(	O
and	O
possibly	O
noisy	O
)	O
dataset	B
without	O
any	O
additional	O
assumptions	O
is	O
clearly	O
“	O
ill	O
posed	O
”	O
.	O
for	O
example	O
,	O
in	O
the	O
noise-free	O
case	O
,	O
any	O
function	B
that	O
passes	O
through	O
the	O
given	O
data	O
points	O
is	O
acceptable	O
.	O
under	O
a	O
bayesian	O
approach	O
our	O
assumptions	O
are	O
charac-	O
terized	O
by	O
a	O
prior	O
over	O
functions	O
,	O
and	O
given	O
some	O
data	O
,	O
we	O
obtain	O
a	O
posterior	O
over	O
functions	O
.	O
the	O
problem	O
of	O
bringing	O
prior	O
assumptions	O
to	O
bear	O
has	O
also	O
been	O
addressed	O
under	O
the	O
regularization	B
viewpoint	O
,	O
where	O
these	O
assumptions	O
are	O
encoded	O
in	O
terms	O
of	O
the	O
smoothness	O
of	O
f.	O
we	O
consider	O
the	O
functional	B
j	O
[	O
f	O
]	O
=	O
λ	O
2	O
kfk2h	O
+	O
q	O
(	O
y	O
,	O
f	O
)	O
,	O
(	O
6.8	O
)	O
where	O
y	O
is	O
the	O
vector	O
of	O
targets	O
we	O
are	O
predicting	O
and	O
f	O
=	O
(	O
f	O
(	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
f	O
(	O
xn	O
)	O
)	O
>	O
is	O
the	O
corresponding	O
vector	O
of	O
function	B
values	O
,	O
and	O
λ	O
is	O
a	O
scaling	O
parameter	O
that	O
trades	O
oﬀ	O
the	O
two	O
terms	O
.	O
the	O
ﬁrst	O
term	O
is	O
called	O
the	O
regularizer	O
and	O
represents	O
smoothness	O
assumptions	O
on	O
f	O
as	O
encoded	O
by	O
a	O
suitable	O
rkhs	O
,	O
and	O
the	O
second	O
term	O
is	O
a	O
data-ﬁt	O
term	O
assessing	O
the	O
quality	O
of	O
the	O
prediction	B
f	O
(	O
xi	O
)	O
for	O
the	O
observed	O
datum	O
yi	O
,	O
e.g	O
.	O
the	O
negative	O
log	O
likelihood	O
.	O
indeed	O
,	O
recalling	O
that	O
kfk2h	O
=	O
pn	O
ridge	B
regression	I
(	O
described	O
in	O
section	O
2.1	O
)	O
can	O
be	O
seen	O
as	O
a	O
particular	O
case	O
i	O
/λi	O
where	O
fi	O
is	O
the	O
of	O
regularization	B
.	O
coeﬃcient	O
of	O
eigenfunction	B
φi	O
(	O
x	O
)	O
,	O
we	O
see	B
that	O
we	O
are	O
penalizing	O
the	O
weighted	O
squared	B
coeﬃcients	O
.	O
this	O
is	O
taking	O
place	O
in	O
feature	B
space	I
,	O
rather	O
than	O
simply	O
in	O
input	O
space	O
,	O
as	O
per	O
the	O
standard	O
formulation	O
of	O
ridge	B
regression	I
(	O
see	B
eq	O
.	O
(	O
2.4	O
)	O
)	O
,	O
so	O
it	O
corresponds	O
to	O
kernel	B
ridge	I
regression	I
.	O
the	O
representer	B
theorem	I
shows	O
that	O
each	O
minimizer	O
f	O
∈	O
h	O
of	O
j	O
[	O
f	O
]	O
has	O
the	O
i=1	O
αik	O
(	O
x	O
,	O
xi	O
)	O
.1	O
the	O
representer	B
theorem	I
was	O
ﬁrst	O
stated	O
by	O
kimeldorf	O
and	O
wahba	O
[	O
1971	O
]	O
for	O
the	O
case	O
of	O
squared	B
error.2	O
o	O
’	O
sullivan	O
et	O
al	O
.	O
[	O
1986	O
]	O
showed	O
that	O
the	O
representer	B
theorem	I
could	O
be	O
extended	O
to	O
likelihood	B
form	O
f	O
(	O
x	O
)	O
=	O
pn	O
i=1f	O
2	O
1if	O
the	O
rkhs	O
contains	O
a	O
null	B
space	I
of	O
unpenalized	O
functions	O
then	O
the	O
given	O
form	O
is	O
correct	O
modulo	O
a	O
term	O
that	O
lies	O
in	O
this	O
null	B
space	I
.	O
this	O
is	O
explained	O
further	O
in	O
section	O
6.3	O
.	O
2schoenberg	O
[	O
1964	O
]	O
proved	O
the	O
representer	B
theorem	I
for	O
the	O
special	O
case	O
of	O
cubic	O
splines	B
and	O
squared	B
error	O
.	O
this	O
was	O
result	O
extended	O
to	O
general	O
rkhss	O
in	O
kimeldorf	O
and	O
wahba	O
[	O
1971	O
]	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
6.2	O
regularization	B
133	O
functions	O
arising	O
from	O
generalized	B
linear	O
models	O
.	O
the	O
representer	B
theorem	I
can	O
be	O
generalized	B
still	O
further	O
,	O
see	B
e.g	O
.	O
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
,	O
sec	O
.	O
4.2	O
]	O
.	O
if	O
the	O
data-ﬁt	O
term	O
is	O
convex	B
(	O
see	B
section	O
a.9	O
)	O
then	O
there	O
will	O
be	O
a	O
unique	O
minimizer	O
ˆf	O
of	O
j	O
[	O
f	O
]	O
.	O
for	O
gaussian	O
process	B
prediction	O
with	O
likelihoods	O
that	O
involve	O
the	O
values	O
of	O
f	O
at	O
the	O
n	O
training	O
points	O
only	O
(	O
so	O
that	O
q	O
(	O
y	O
,	O
f	O
)	O
is	O
the	O
negative	O
log	O
likelihood	O
up	O
to	O
some	O
terms	O
not	O
involving	O
f	O
)	O
,	O
the	O
analogue	O
of	O
the	O
representer	B
theorem	I
is	O
obvious	O
.	O
this	O
is	O
because	O
the	O
predictive	B
distribution	O
of	O
f	O
(	O
x∗	O
)	O
,	O
f∗	O
at	O
test	O
point	O
x∗	O
given	O
the	O
data	O
y	O
is	O
p	O
(	O
f∗|y	O
)	O
=r	O
p	O
(	O
f∗|f	O
)	O
p	O
(	O
f|y	O
)	O
df	O
.	O
as	O
derived	O
in	O
eq	O
.	O
(	O
3.22	O
)	O
we	O
thus	O
e	O
[	O
f∗|y	O
]	O
=pn	O
(	O
6.9	O
)	O
due	O
to	O
the	O
formulae	O
for	O
the	O
conditional	B
distribution	O
of	O
a	O
multivariate	O
gaussian	O
.	O
i=1αik	O
(	O
x∗	O
,	O
xi	O
)	O
,	O
where	O
α	O
=	O
k−1e	O
[	O
f|y	O
]	O
.	O
e	O
[	O
f∗|y	O
]	O
=	O
k	O
(	O
x∗	O
)	O
>	O
k−1e	O
[	O
f|y	O
]	O
have	O
the	O
regularization	B
approach	O
has	O
a	O
long	O
tradition	O
in	O
inverse	O
problems	O
,	O
dat-	O
ing	O
back	O
at	O
least	O
as	O
far	O
as	O
tikhonov	O
[	O
1963	O
]	O
;	O
see	B
also	O
tikhonov	O
and	O
arsenin	O
[	O
1977	O
]	O
.	O
for	O
the	O
application	O
of	O
this	O
approach	O
in	O
the	O
machine	O
learning	B
literature	O
see	B
e.g	O
.	O
poggio	O
and	O
girosi	O
[	O
1990	O
]	O
.	O
in	O
section	O
6.2.1	O
we	O
consider	O
rkhss	O
deﬁned	O
in	O
terms	O
of	O
diﬀerential	O
operators	O
.	O
in	O
section	O
6.2.2	O
we	O
demonstrate	O
how	O
to	O
solve	O
the	O
regularization	B
problem	O
in	O
the	O
speciﬁc	O
case	O
of	O
squared	B
error	O
,	O
and	O
in	O
section	O
6.2.3	O
we	O
compare	O
and	O
contrast	O
the	O
regularization	B
approach	O
with	O
the	O
gaussian	O
process	B
viewpoint	O
.	O
∗	O
null	B
space	I
6.2.1	O
regularization	B
deﬁned	O
by	O
diﬀerential	O
operators	O
for	O
x	O
∈	O
rd	O
deﬁne	O
z	O
x	O
(	O
cid:16	O
)	O
∂2f	O
(	O
cid:17	O
)	O
2	O
j1+	O
...	O
+jd=m	O
(	O
cid:16	O
)	O
∂mf	O
(	O
x	O
)	O
(	O
cid:16	O
)	O
∂2f	O
(	O
cid:17	O
)	O
2	O
∂xj1	O
(	O
cid:17	O
)	O
2	O
(	O
cid:17	O
)	O
2i	O
1	O
.	O
.	O
.	O
xjd	O
d	O
komfk2	O
=	O
z	O
h	O
(	O
cid:16	O
)	O
∂2f	O
for	O
example	O
for	O
m	O
=	O
2	O
and	O
d	O
=	O
2	O
dx	O
.	O
(	O
6.10	O
)	O
ko2fk2	O
=	O
now	O
set	B
kp	O
fk2	O
=pm	O
+	O
2	O
∂x2	O
1	O
(	O
6.11	O
)	O
m=0	O
amkomfk2	O
with	O
non-negative	O
coeﬃcients	O
am	O
.	O
notice	O
dx1	O
dx2	O
.	O
∂x1∂x2	O
∂x2	O
2	O
+	O
that	O
kp	O
fk2	O
is	O
translation	O
and	O
rotation	O
invariant	O
.	O
in	O
this	O
section	O
we	O
assume	O
that	O
a0	O
>	O
0	O
;	O
if	O
this	O
is	O
not	O
the	O
case	O
and	O
ak	O
is	O
the	O
ﬁrst	O
non-zero	O
coeﬃcient	O
,	O
then	O
there	O
is	O
a	O
null	B
space	I
of	O
functions	O
that	O
are	O
unpenalized	O
.	O
for	O
example	O
if	O
k	O
=	O
2	O
then	O
constant	O
and	O
linear	B
functions	O
are	O
in	O
the	O
null	B
space	I
.	O
this	O
case	O
is	O
dealt	O
with	O
in	O
section	O
6.3.	O
kp	O
fk2	O
penalizes	O
f	O
in	O
terms	O
of	O
the	O
variability	O
of	O
its	O
function	B
values	O
and	O
derivatives	O
up	O
to	O
order	O
m.	O
how	O
does	O
this	O
correspond	O
to	O
the	O
rkhs	O
formulation	O
of	O
section	O
6.1	O
?	O
the	O
key	O
is	O
to	O
recognize	O
that	O
the	O
complex	O
exponentials	O
exp	O
(	O
2πis·	O
x	O
)	O
are	O
eigenfunctions	O
of	O
the	O
diﬀerential	O
operator	B
if	O
x	O
=	O
rd	O
.	O
in	O
this	O
case	O
kp	O
fk2	O
=	O
am	O
(	O
4π2s	O
·	O
s	O
)	O
m|	O
˜f	O
(	O
s	O
)	O
|2ds	O
,	O
(	O
6.12	O
)	O
z	O
mx	O
m=0	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
134	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
where	O
˜f	O
(	O
s	O
)	O
is	O
the	O
fourier	O
transform	O
of	O
f	O
(	O
x	O
)	O
.	O
comparing	O
eq	O
.	O
(	O
6.12	O
)	O
with	O
eq	O
.	O
(	O
6.1	O
)	O
we	O
see	B
that	O
the	O
kernel	B
has	O
the	O
power	O
spectrum	O
s	O
(	O
s	O
)	O
=	O
,	O
(	O
6.13	O
)	O
1	O
pm	O
m=0	O
am	O
(	O
4π2s	O
·	O
s	O
)	O
m	O
pm	O
m=0	O
am	O
(	O
4π2s	O
·	O
s	O
)	O
m	O
e2πis·x	O
z	O
and	O
thus	O
by	O
fourier	O
inversion	O
we	O
obtain	O
the	O
stationary	O
kernel	B
k	O
(	O
x	O
)	O
=	O
ds	O
.	O
(	O
6.14	O
)	O
a	O
slightly	O
diﬀerent	O
approach	O
to	O
obtaining	O
the	O
kernel	B
is	O
to	O
use	O
calculus	O
of	O
variations	O
to	O
minimize	O
j	O
[	O
f	O
]	O
with	O
respect	O
to	O
f.	O
the	O
euler-lagrange	O
equation	O
leads	O
to	O
nx	O
f	O
(	O
x	O
)	O
=	O
αig	O
(	O
x	O
−	O
xi	O
)	O
,	O
with	O
mx	O
i=1	O
(	O
−1	O
)	O
mam∇2mg	O
=	O
δ	O
(	O
x	O
−	O
x0	O
)	O
,	O
(	O
6.15	O
)	O
(	O
6.16	O
)	O
green	O
’	O
s	O
function	B
≡	O
kernel	B
m=0	O
diﬀerential	O
operatorpm	O
where	O
g	O
(	O
x	O
,	O
x0	O
)	O
is	O
known	O
as	O
a	O
green	O
’	O
s	O
function	B
.	O
notice	O
that	O
the	O
green	O
’	O
s	O
func-	O
tion	O
also	O
depends	O
on	O
the	O
boundary	O
conditions	O
.	O
for	O
the	O
case	O
of	O
x	O
=	O
rd	O
by	O
fourier	O
transforming	O
eq	O
.	O
(	O
6.16	O
)	O
we	O
recognize	O
that	O
g	O
is	O
in	O
fact	O
the	O
kernel	B
k.	O
the	O
m=0	O
(	O
−1	O
)	O
mam∇2m	O
and	O
the	O
integral	O
operator	B
k	O
(	O
·	O
,	O
·	O
)	O
are	O
in	O
fact	O
inverses	O
,	O
as	O
shown	O
by	O
eq	O
.	O
(	O
6.16	O
)	O
.	O
see	B
poggio	O
and	O
girosi	O
[	O
1990	O
]	O
for	O
further	O
details	O
.	O
arfken	O
[	O
1985	O
]	O
provides	O
an	O
introduction	O
to	O
calculus	O
of	O
variations	O
and	O
green	O
’	O
s	O
functions	O
.	O
rkhss	O
for	O
regularizers	O
deﬁned	O
by	O
diﬀerential	O
operators	O
are	O
sobolev	O
spaces	O
;	O
see	B
e.g	O
.	O
adams	O
[	O
1975	O
]	O
for	O
further	O
details	O
on	O
sobolev	O
spaces	O
.	O
we	O
now	O
give	O
two	O
speciﬁc	O
examples	O
of	O
kernels	O
derived	O
from	O
diﬀerential	O
oper-	O
ators	O
.	O
example	O
1.	O
set	B
a0	O
=	O
α2	O
,	O
a1	O
=	O
1	O
and	O
am	O
=	O
0	O
for	O
m	O
≥	O
2	O
in	O
d	O
=	O
1.	O
using	O
the	O
fourier	O
pair	O
e−α|x|	O
↔	O
2α/	O
(	O
α2	O
+	O
4π2s2	O
)	O
we	O
obtain	O
k	O
(	O
x	O
−	O
x0	O
)	O
=	O
1	O
2α	O
e−α|x−x0|	O
.	O
note	O
that	O
this	O
is	O
the	O
covariance	B
function	I
of	O
the	O
ornstein-uhlenbeck	O
process	B
,	O
see	B
section	O
4.2.1.	O
example	O
2.	O
by	O
setting	O
am	O
=	O
σ2m	O
we	O
obtain	O
m	O
!	O
2m	O
and	O
using	O
the	O
power	O
series	O
ey	O
=p∞	O
k=0	O
yk/k	O
!	O
k	O
(	O
x	O
−	O
x0	O
)	O
=	O
exp	O
(	O
2πis	O
·	O
(	O
x	O
−	O
x0	O
)	O
)	O
exp	O
(	O
−	O
σ2	O
2	O
(	O
4π2s	O
·	O
s	O
)	O
)	O
ds	O
=	O
1	O
(	O
2πσ2	O
)	O
d/2	O
exp	O
(	O
−	O
1	O
2σ2	O
(	O
x	O
−	O
x0	O
)	O
>	O
(	O
x	O
−	O
x0	O
)	O
)	O
,	O
(	O
6.17	O
)	O
(	O
6.18	O
)	O
z	O
as	O
shown	O
by	O
yuille	O
and	O
grzywacz	O
[	O
1989	O
]	O
.	O
this	O
is	O
the	O
squared	B
exponential	I
co-	O
variance	O
function	B
that	O
we	O
have	O
seen	O
earlier	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
6.2	O
regularization	B
135	O
6.2.2	O
obtaining	O
the	O
regularized	O
solution	O
the	O
representer	B
theorem	I
tells	O
us	O
the	O
general	O
form	O
of	O
the	O
solution	O
to	O
eq	O
.	O
(	O
6.8	O
)	O
.	O
we	O
now	O
consider	O
a	O
speciﬁc	O
functional	B
j	O
[	O
f	O
]	O
=	O
kfk2h	O
+	O
1	O
2	O
1	O
2σ2	O
n	O
(	O
yi	O
−	O
f	O
(	O
xi	O
)	O
)	O
2	O
,	O
(	O
6.19	O
)	O
nx	O
i=1	O
which	O
uses	O
a	O
squared	B
error	O
data-ﬁt	O
term	O
(	O
corresponding	O
to	O
the	O
negative	O
log	O
likelihood	O
of	O
a	O
gaussian	O
noise	B
model	I
with	O
variance	O
σ2	O
n	O
)	O
.	O
substituting	O
f	O
(	O
x	O
)	O
=	O
pn	O
i=1αik	O
(	O
x	O
,	O
xi	O
)	O
and	O
using	O
hk	O
(	O
·	O
,	O
xi	O
)	O
,	O
k	O
(	O
·	O
,	O
xj	O
)	O
ih	O
=	O
k	O
(	O
xi	O
,	O
xj	O
)	O
we	O
obtain	O
j	O
[	O
α	O
]	O
=	O
=	O
1	O
2	O
α	O
>	O
kα	O
+	O
1	O
2	O
α	O
>	O
(	O
k	O
+	O
1	O
|y	O
−	O
kα|2	O
2σ2	O
n	O
k	O
2	O
)	O
α	O
−	O
1	O
1	O
σ2	O
σ2	O
n	O
n	O
y	O
>	O
kα	O
+	O
(	O
6.20	O
)	O
y	O
>	O
y	O
.	O
1	O
2σ2	O
n	O
minimizing	O
j	O
by	O
diﬀerentiating	O
w.r.t	O
.	O
the	O
vector	O
of	O
coeﬃcients	O
α	O
we	O
obtain	O
ni	O
)	O
−1y	O
,	O
so	O
that	O
the	O
prediction	B
for	O
a	O
test	O
point	O
x∗	O
is	O
ˆf	O
(	O
x∗	O
)	O
=	O
ˆα	O
=	O
(	O
k	O
+	O
σ2	O
k	O
(	O
x∗	O
)	O
>	O
(	O
k	O
+	O
σ2	O
ni	O
)	O
−1y	O
.	O
this	O
should	O
look	O
very	O
familiar—it	O
is	O
exactly	O
the	O
form	O
of	O
the	O
predictive	B
mean	O
obtained	O
in	O
eq	O
.	O
(	O
2.23	O
)	O
.	O
in	O
the	O
next	O
section	O
we	O
compare	O
and	O
contrast	O
the	O
regularization	B
and	O
gp	O
views	O
of	O
the	O
problem	O
.	O
the	O
solution	O
f	O
(	O
x	O
)	O
=pn	O
regularization	B
network	I
in	O
poggio	O
and	O
girosi	O
[	O
1990	O
]	O
.	O
i=1αik	O
(	O
x	O
,	O
xi	O
)	O
that	O
minimizes	O
eq	O
.	O
(	O
6.19	O
)	O
was	O
called	O
a	O
regularization	B
network	I
6.2.3	O
the	O
relationship	O
of	O
the	O
regularization	B
view	O
to	O
gaus-	O
sian	O
process	B
prediction	O
the	O
regularization	B
method	O
returns	O
ˆf	O
=	O
argminf	O
j	O
[	O
f	O
]	O
.	O
for	O
a	O
gaussian	O
process	B
predictor	O
we	O
obtain	O
a	O
posterior	O
distribution	O
over	O
functions	O
.	O
can	O
we	O
make	O
a	O
connection	O
between	O
these	O
two	O
views	O
?	O
in	O
fact	O
we	O
shall	O
see	B
in	O
this	O
section	O
that	O
ˆf	O
can	O
be	O
viewed	O
as	O
the	O
maximum	B
a	I
posteriori	I
(	O
map	O
)	O
function	B
under	O
the	O
posterior	O
.	O
following	O
szeliski	O
[	O
1987	O
]	O
and	O
poggio	O
and	O
girosi	O
[	O
1990	O
]	O
we	O
consider	O
exp	O
(	O
−j	O
[	O
f	O
]	O
)	O
=	O
exp	O
(	O
cid:0	O
)	O
−	O
λ	O
kp	O
fk2	O
(	O
cid:1	O
)	O
×	O
exp	O
(	O
−q	O
(	O
y	O
,	O
f	O
)	O
)	O
.	O
2	O
(	O
6.21	O
)	O
the	O
ﬁrst	O
term	O
on	O
the	O
rhs	O
is	O
a	O
gaussian	O
process	B
prior	O
on	O
f	O
,	O
and	O
the	O
second	O
is	O
proportional	O
to	O
the	O
likelihood	B
.	O
as	O
ˆf	O
is	O
the	O
minimizer	O
of	O
j	O
[	O
f	O
]	O
,	O
it	O
is	O
the	O
map	O
function	B
.	O
f.	O
thus	O
we	O
obtain	O
kp	O
fk2	O
’	O
pm	O
to	O
get	O
some	O
intuition	O
for	O
the	O
gaussian	O
process	B
prior	O
,	O
imagine	O
f	O
(	O
x	O
)	O
being	O
represented	O
on	O
a	O
grid	O
in	O
x-space	O
,	O
so	O
that	O
f	O
is	O
now	O
an	O
(	O
inﬁnite	O
dimensional	O
)	O
vector	O
mdm	O
)	O
f	O
where	O
dm	O
is	O
an	O
appropriate	O
ﬁnite-diﬀerence	O
approximation	O
of	O
the	O
diﬀerential	O
operator	B
om	O
.	O
observe	O
that	O
this	O
prior	O
term	O
is	O
a	O
quadratic	B
form	I
in	O
f.	O
m=0	O
am	O
(	O
dmf	O
)	O
>	O
(	O
dmf	O
)	O
=	O
f	O
>	O
(	O
p	O
m	O
amd	O
>	O
to	O
go	O
into	O
more	O
detail	O
concerning	O
the	O
map	O
relationship	O
we	O
consider	O
three	O
cases	O
:	O
(	O
i	O
)	O
when	O
q	O
(	O
y	O
,	O
f	O
)	O
is	O
quadratic	O
(	O
corresponding	O
to	O
a	O
gaussian	O
likelihood	B
)	O
;	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
136	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
(	O
ii	O
)	O
when	O
q	O
(	O
y	O
,	O
f	O
)	O
is	O
not	O
quadratic	O
but	O
convex	B
and	O
(	O
iii	O
)	O
when	O
q	O
(	O
y	O
,	O
f	O
)	O
is	O
not	O
convex	B
.	O
in	O
case	O
(	O
i	O
)	O
we	O
have	O
seen	O
in	O
chapter	O
2	O
that	O
the	O
posterior	O
mean	O
function	B
can	O
be	O
obtained	O
exactly	O
,	O
and	O
the	O
posterior	O
is	O
gaussian	O
.	O
as	O
the	O
mean	O
of	O
a	O
gaussian	O
is	O
also	O
its	O
mode	O
this	O
is	O
the	O
map	O
solution	O
.	O
the	O
correspondence	O
between	O
the	O
gp	O
posterior	O
mean	O
and	O
the	O
solution	O
of	O
the	O
regularization	B
problem	O
ˆf	O
was	O
made	O
in	O
kimeldorf	O
and	O
wahba	O
[	O
1970	O
]	O
.	O
in	O
case	O
(	O
ii	O
)	O
we	O
have	O
seen	O
in	O
chapter	O
3	O
for	O
classiﬁcation	B
problems	O
using	O
the	O
logistic	B
,	O
probit	B
or	O
softmax	B
response	O
functions	O
that	O
q	O
(	O
y	O
,	O
f	O
)	O
is	O
convex	B
.	O
here	O
the	O
map	O
solution	O
can	O
be	O
found	O
by	O
ﬁnding	O
ˆf	O
(	O
the	O
map	O
solution	O
to	O
the	O
n-dimensional	O
problem	O
deﬁned	O
at	O
the	O
training	O
points	O
)	O
and	O
then	O
extending	O
it	O
to	O
other	O
x-values	O
through	O
the	O
posterior	O
mean	O
conditioned	O
on	O
ˆf	O
.	O
in	O
case	O
(	O
iii	O
)	O
there	O
will	O
be	O
more	O
than	O
one	O
local	O
minimum	O
of	O
j	O
[	O
f	O
]	O
under	O
the	O
regularization	B
approach	O
.	O
one	O
could	O
check	O
these	O
minima	O
to	O
ﬁnd	O
the	O
deepest	O
one	O
.	O
however	O
,	O
in	O
this	O
case	O
the	O
argument	O
for	O
map	O
is	O
rather	O
weak	O
(	O
especially	O
if	O
there	O
are	O
multiple	O
optima	O
of	O
similar	O
depth	O
)	O
and	O
suggests	O
the	O
need	O
for	O
a	O
fully	O
bayesian	O
treatment	O
.	O
while	O
the	O
regularization	B
solution	O
gives	O
a	O
part	O
of	O
the	O
gaussian	O
process	B
solu-	O
tion	O
,	O
there	O
are	O
the	O
following	O
limitations	O
:	O
1.	O
it	O
does	O
not	O
characterize	O
the	O
uncertainty	O
in	O
the	O
predictions	O
,	O
nor	O
does	O
it	O
handle	O
well	O
multimodality	O
in	O
the	O
posterior	O
.	O
2.	O
the	O
analysis	O
is	O
focussed	O
at	O
approximating	O
the	O
ﬁrst	O
level	O
of	O
bayesian	O
infer-	O
ence	O
,	O
concerning	O
predictions	O
for	O
f.	O
it	O
is	O
not	O
usually	O
extended	O
to	O
the	O
next	O
level	O
,	O
e.g	O
.	O
to	O
the	O
computation	O
of	O
the	O
marginal	B
likelihood	I
.	O
the	O
marginal	B
likelihood	I
is	O
very	O
useful	O
for	O
setting	O
any	O
parameters	O
of	O
the	O
covariance	B
func-	O
tion	O
,	O
and	O
for	O
model	B
comparison	O
(	O
see	B
chapter	O
5	O
)	O
.	O
in	O
addition	O
,	O
we	O
ﬁnd	O
the	O
speciﬁcation	O
of	O
smoothness	O
via	O
the	O
penalties	O
on	O
deriva-	O
tives	O
to	O
be	O
not	O
very	O
intuitive	O
.	O
the	O
regularization	B
viewpoint	O
can	O
be	O
thought	O
of	O
as	O
directly	O
specifying	O
the	O
inverse	O
covariance	B
rather	O
than	O
the	O
covariance	B
.	O
as	O
marginalization	O
is	O
achieved	O
for	O
a	O
gaussian	O
distribution	O
directly	O
from	O
the	O
covari-	O
ance	O
(	O
and	O
not	O
the	O
inverse	O
covariance	B
)	O
it	O
seems	O
more	O
natural	O
to	O
us	O
to	O
specify	O
the	O
covariance	B
function	I
.	O
also	O
,	O
while	O
non-stationary	O
covariance	B
functions	O
can	O
be	O
obtained	O
from	O
the	O
regularization	B
viewpoint	O
,	O
e.g	O
.	O
by	O
replacing	O
the	O
lebesgue	O
measure	B
in	O
eq	O
.	O
(	O
6.10	O
)	O
with	O
a	O
non-uniform	O
measure	B
µ	O
(	O
x	O
)	O
,	O
calculation	O
of	O
the	O
cor-	O
responding	O
covariance	B
function	I
can	O
then	O
be	O
very	O
diﬃcult	O
.	O
6.3	O
spline	O
models	O
in	O
section	O
6.2	O
we	O
discussed	O
regularizers	O
which	O
had	O
a0	O
>	O
0	O
in	O
eq	O
.	O
(	O
6.12	O
)	O
.	O
we	O
now	O
consider	O
the	O
case	O
when	O
a0	O
=	O
0	O
;	O
in	O
particular	O
we	O
consider	O
the	O
regularizer	O
to	O
be	O
of	O
the	O
form	O
komfk2	O
,	O
as	O
deﬁned	O
in	O
eq	O
.	O
(	O
6.10	O
)	O
.	O
in	O
this	O
case	O
polynomials	O
of	O
degree	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
6.3	O
spline	O
models	O
137	O
up	O
to	O
m	O
−	O
1	O
are	O
in	O
the	O
null	B
space	I
of	O
the	O
regularization	B
operator	O
,	O
in	O
that	O
they	O
are	O
not	O
penalized	B
at	O
all	O
.	O
in	O
the	O
case	O
that	O
x	O
=	O
rd	O
we	O
can	O
again	O
use	O
fourier	O
techniques	O
to	O
ob-	O
tain	O
the	O
green	O
’	O
s	O
function	B
g	O
corresponding	O
to	O
the	O
euler-lagrange	O
equation	O
(	O
−1	O
)	O
m∇2mg	O
(	O
x	O
)	O
=	O
δ	O
(	O
x	O
)	O
.	O
the	O
result	O
,	O
as	O
shown	O
by	O
duchon	O
[	O
1977	O
]	O
and	O
meinguet	O
[	O
1979	O
]	O
is	O
(	O
cid:26	O
)	O
cm	O
,	O
d|x	O
−	O
x0|2m−d	O
log	O
|x	O
−	O
x0|	O
cm	O
,	O
d|x	O
−	O
x0|2m−d	O
g	O
(	O
x−x0	O
)	O
=	O
if	O
2m	O
>	O
d	O
and	O
d	O
even	O
otherwise	O
,	O
(	O
6.22	O
)	O
where	O
cm	O
,	O
d	O
is	O
a	O
constant	O
(	O
wahba	O
[	O
1990	O
,	O
p.	O
31	O
]	O
gives	O
the	O
explicit	O
form	O
)	O
.	O
note	O
that	O
the	O
constraint	O
2m	O
>	O
d	O
has	O
to	O
be	O
imposed	O
to	O
avoid	O
having	O
a	O
green	O
’	O
s	O
function	B
that	O
is	O
singular	O
at	O
the	O
origin	O
.	O
explicit	O
calculation	O
of	O
the	O
green	O
’	O
s	O
function	B
for	O
other	O
domains	O
x	O
is	O
sometimes	O
possible	O
;	O
for	O
example	O
see	B
wahba	O
[	O
1990	O
,	O
sec	O
.	O
2.2	O
]	O
for	O
splines	B
on	O
the	O
sphere	O
.	O
because	O
of	O
the	O
null	B
space	I
,	O
a	O
minimizer	O
of	O
the	O
regularization	B
functional	O
has	O
the	O
form	O
nx	O
f	O
(	O
x	O
)	O
=	O
kx	O
αig	O
(	O
x	O
,	O
xi	O
)	O
+	O
βjhj	O
(	O
x	O
)	O
,	O
(	O
6.23	O
)	O
i=1	O
j=1	O
where	O
h1	O
(	O
x	O
)	O
,	O
.	O
.	O
.	O
,	O
hk	O
(	O
x	O
)	O
are	O
polynomials	O
that	O
span	O
the	O
null	B
space	I
.	O
the	O
exact	O
values	O
of	O
the	O
coeﬃcients	O
α	O
and	O
β	O
for	O
a	O
speciﬁc	O
problem	O
can	O
be	O
obtained	O
in	O
an	O
analogous	O
manner	O
to	O
the	O
derivation	O
in	O
section	O
6.2.2	O
;	O
in	O
fact	O
the	O
solution	O
is	O
equivalent	B
to	O
that	O
given	O
in	O
eq	O
.	O
(	O
2.42	O
)	O
.	O
to	O
gain	O
some	O
more	O
insight	O
into	O
the	O
form	O
of	O
the	O
green	O
’	O
s	O
function	B
we	O
consider	O
the	O
equation	O
(	O
−1	O
)	O
m∇2mg	O
(	O
x	O
)	O
=	O
δ	O
(	O
x	O
)	O
in	O
fourier	O
space	O
,	O
leading	O
to	O
˜g	O
(	O
s	O
)	O
=	O
(	O
4π2s·	O
s	O
)	O
−m	O
.	O
˜g	O
(	O
s	O
)	O
plays	O
a	O
rˆole	O
like	O
that	O
of	O
the	O
power	O
spectrum	O
in	O
eq	O
.	O
(	O
6.13	O
)	O
,	O
but	O
notice	O
thatr	O
˜g	O
(	O
s	O
)	O
ds	O
is	O
inﬁnite	O
,	O
which	O
would	O
imply	O
that	O
the	O
corresponding	O
process	B
has	O
inﬁnite	O
variance	O
.	O
the	O
problem	O
is	O
of	O
course	O
that	O
the	O
null	B
space	I
is	O
unpenalized	O
;	O
for	O
example	O
any	O
arbitrary	O
constant	O
function	B
can	O
be	O
added	O
to	O
f	O
without	O
changing	O
the	O
regularizer	O
.	O
tions	O
of	O
f	O
(	O
x	O
)	O
of	O
the	O
form	O
g	O
(	O
x	O
)	O
=pk	O
because	O
of	O
the	O
null	B
space	I
we	O
have	O
seen	O
that	O
one	O
can	O
not	O
obtain	O
a	O
simple	O
connection	O
between	O
the	O
spline	O
solution	O
and	O
a	O
corresponding	O
gaussian	O
process	B
problem	O
.	O
however	O
,	O
by	O
introducing	O
the	O
notion	O
of	O
an	O
intrinsic	B
random	I
function	I
(	O
irf	O
)	O
one	O
can	O
deﬁne	O
a	O
generalized	B
covariance	O
;	O
see	B
cressie	O
[	O
1993	O
,	O
sec	O
.	O
5.4	O
]	O
and	O
stein	O
[	O
1999	O
,	O
section	O
2.9	O
]	O
for	O
details	O
.	O
the	O
basic	O
idea	O
is	O
to	O
consider	O
linear	B
combina-	O
i=1	O
aif	O
(	O
x+δi	O
)	O
for	O
which	O
g	O
(	O
x	O
)	O
is	O
second-order	O
stationary	O
and	O
where	O
(	O
hj	O
(	O
δ1	O
)	O
,	O
.	O
.	O
.	O
,	O
hj	O
(	O
δk	O
)	O
)	O
a	O
=	O
0	O
for	O
j	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
k.	O
a	O
careful	O
de-	O
scription	O
of	O
the	O
equivalence	O
of	O
spline	O
and	O
irf	O
prediction	B
is	O
given	O
in	O
kent	O
and	O
mardia	O
[	O
1994	O
]	O
.	O
the	O
power-law	O
form	O
of	O
˜g	O
(	O
s	O
)	O
=	O
(	O
4π2s·s	O
)	O
−m	O
means	O
that	O
there	O
is	O
no	O
character-	O
istic	O
length-scale	B
for	O
random	O
functions	O
drawn	O
from	O
this	O
(	O
improper	O
)	O
prior	O
.	O
thus	O
we	O
obtain	O
the	O
self-similar	O
property	O
characteristic	O
of	O
fractals	O
;	O
for	O
further	O
details	O
see	B
szeliski	O
[	O
1987	O
]	O
and	O
mandelbrot	O
[	O
1982	O
]	O
.	O
some	O
authors	O
argue	O
that	O
the	O
lack	O
of	O
a	O
characteristic	O
length-scale	B
is	O
appealing	O
.	O
this	O
may	O
sometimes	O
be	O
the	O
case	O
,	O
but	O
if	O
we	O
believe	O
there	O
is	O
an	O
appropriate	O
length-scale	B
(	O
or	O
set	B
of	O
length-scales	O
)	O
irf	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
138	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
for	O
a	O
given	O
problem	O
but	O
this	O
is	O
unknown	O
in	O
advance	O
,	O
we	O
would	O
argue	O
that	O
a	O
hierarchical	O
bayesian	O
formulation	O
of	O
the	O
problem	O
(	O
as	O
described	O
in	O
chapter	O
5	O
)	O
would	O
be	O
more	O
appropriate	O
.	O
splines	B
were	O
originally	O
introduced	O
for	O
one-dimensional	O
interpolation	O
and	O
smoothing	O
problems	O
,	O
and	O
then	O
generalized	B
to	O
the	O
multivariate	O
setting	O
.	O
schoen-	O
berg	O
[	O
1964	O
]	O
considered	O
the	O
problem	O
of	O
ﬁnding	O
the	O
function	B
that	O
minimizes	O
spline	O
interpolation	O
natural	O
polynomial	B
spline	O
smoothing	O
spline	O
z	O
b	O
a	O
(	O
f	O
(	O
m	O
)	O
(	O
x	O
)	O
)	O
2	O
dx	O
,	O
(	O
6.24	O
)	O
where	O
f	O
(	O
m	O
)	O
denotes	O
the	O
m	O
’	O
th	O
derivative	O
of	O
f	O
,	O
subject	O
to	O
the	O
interpolation	O
con-	O
straints	O
f	O
(	O
xi	O
)	O
=	O
fi	O
,	O
xi	O
∈	O
(	O
a	O
,	O
b	O
)	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
and	O
for	O
f	O
in	O
an	O
appropriate	O
sobolev	O
space	O
.	O
he	O
showed	O
that	O
the	O
solution	O
is	O
the	O
natural	O
polynomial	B
spline	O
,	O
which	O
is	O
a	O
piecewise	B
polynomial	O
of	O
order	O
2m	O
−	O
1	O
in	O
each	O
interval	O
[	O
xi	O
,	O
xi+1	O
]	O
,	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
−	O
1	O
,	O
and	O
of	O
order	O
m	O
−	O
1	O
in	O
the	O
two	O
outermost	O
intervals	O
.	O
the	O
pieces	O
are	O
joined	O
so	O
that	O
the	O
solution	O
has	O
2m	O
−	O
2	O
continuous	O
derivatives	O
.	O
schoen-	O
berg	O
also	O
proved	O
that	O
the	O
solution	O
to	O
the	O
univariate	O
smoothing	O
problem	O
(	O
see	B
eq	O
.	O
(	O
6.19	O
)	O
)	O
is	O
a	O
natural	O
polynomial	B
spline	O
.	O
a	O
common	O
choice	O
is	O
m	O
=	O
2	O
,	O
leading	O
to	O
the	O
cubic	O
spline	O
.	O
one	O
possible	O
way	O
of	O
writing	O
this	O
solution	O
is	O
1x	O
nx	O
f	O
(	O
x	O
)	O
=	O
βjxj	O
+	O
j=0	O
i=1	O
αi	O
(	O
x	O
−	O
xi	O
)	O
3	O
+	O
,	O
where	O
(	O
x	O
)	O
+	O
=	O
(	O
cid:26	O
)	O
x	O
if	O
x	O
>	O
0	O
0	O
otherwise	O
.	O
(	O
6.25	O
)	O
it	O
turns	O
out	O
that	O
the	O
coeﬃcients	O
α	O
and	O
β	O
can	O
be	O
computed	O
in	O
time	O
o	O
(	O
n	O
)	O
using	O
an	O
algorithm	O
due	O
to	O
reinsch	O
;	O
see	B
green	O
and	O
silverman	O
[	O
1994	O
,	O
sec	O
.	O
2.3.3	O
]	O
for	O
details	O
.	O
splines	B
were	O
ﬁrst	O
used	O
in	O
regression	B
problems	O
.	O
however	O
,	O
by	O
using	O
general-	O
ized	O
linear	B
modelling	O
[	O
mccullagh	O
and	O
nelder	O
,	O
1983	O
]	O
they	O
can	O
be	O
extended	O
to	O
classiﬁcation	B
problems	O
and	O
other	O
non-gaussian	O
likelihoods	O
,	O
as	O
we	O
did	O
for	O
gp	O
classiﬁcation	B
in	O
section	O
3.3.	O
early	O
references	O
in	O
this	O
direction	O
include	O
silverman	O
[	O
1978	O
]	O
and	O
o	O
’	O
sullivan	O
et	O
al	O
.	O
[	O
1986	O
]	O
.	O
there	O
is	O
a	O
vast	O
literature	O
in	O
relation	O
to	O
splines	B
in	O
both	O
the	O
statistics	O
and	O
numerical	O
analysis	O
literatures	O
;	O
for	O
entry	O
points	O
see	B
citations	O
in	O
wahba	O
[	O
1990	O
]	O
and	O
green	O
and	O
silverman	O
[	O
1994	O
]	O
.	O
∗	O
6.3.1	O
a	O
1-d	O
gaussian	O
process	B
spline	O
construction	O
in	O
this	O
section	O
we	O
will	O
further	O
clarify	O
the	O
relationship	O
between	O
splines	B
and	O
gaus-	O
sian	O
processes	O
by	O
giving	O
a	O
gp	O
construction	O
for	O
the	O
solution	O
of	O
the	O
univariate	O
cubic	O
spline	O
smoothing	O
problem	O
whose	O
cost	O
functional	B
is	O
nx	O
(	O
cid:0	O
)	O
f	O
(	O
xi	O
)	O
−	O
yi	O
(	O
cid:1	O
)	O
2	O
+	O
λ	O
z	O
1	O
(	O
cid:0	O
)	O
f00	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
2	O
dx	O
,	O
(	O
6.26	O
)	O
i=1	O
0	O
where	O
the	O
observed	O
data	O
are	O
{	O
(	O
xi	O
,	O
yi	O
)	O
|i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
0	O
<	O
x1	O
<	O
···	O
<	O
xn	O
<	O
1	O
}	O
and	O
λ	O
is	O
a	O
smoothing	O
parameter	O
controlling	O
the	O
trade-oﬀ	O
between	O
the	O
ﬁrst	O
term	O
,	O
the	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
6.3	O
spline	O
models	O
139	O
data-ﬁt	O
,	O
and	O
the	O
second	O
term	O
,	O
the	O
regularizer	O
,	O
or	O
complexity	O
penalty	O
.	O
recall	O
that	O
the	O
solution	O
is	O
a	O
piecewise	B
polynomial	O
as	O
in	O
eq	O
.	O
(	O
6.25	O
)	O
.	O
following	O
wahba	O
[	O
1978	O
]	O
,	O
we	O
consider	O
the	O
random	O
function	B
1x	O
j=0	O
g	O
(	O
x	O
)	O
=	O
βjxj	O
+	O
f	O
(	O
x	O
)	O
(	O
6.27	O
)	O
where	O
β	O
∼	O
n	O
(	O
0	O
,	O
σ2	O
where	O
ksp	O
(	O
x	O
,	O
x0	O
)	O
,	O
and	O
v	O
=	O
min	O
(	O
x	O
,	O
x0	O
)	O
.	O
0	O
z	O
1	O
β	O
i	O
)	O
and	O
f	O
(	O
x	O
)	O
is	O
a	O
gaussian	O
process	B
with	O
covariance	B
σ2	O
f	O
ksp	O
(	O
x	O
,	O
x0	O
)	O
,	O
(	O
x	O
−	O
u	O
)	O
+	O
(	O
x0	O
−	O
u	O
)	O
+	O
du	O
=	O
|x	O
−	O
x0|v2	O
2	O
+	O
v3	O
3	O
,	O
(	O
6.28	O
)	O
to	O
complete	O
the	O
analogue	O
of	O
the	O
regularizer	O
in	O
eq	O
.	O
(	O
6.26	O
)	O
,	O
we	O
need	O
to	O
remove	O
any	O
penalty	O
on	O
polynomial	B
terms	O
in	O
the	O
null	B
space	I
by	O
making	O
the	O
prior	O
vague	O
,	O
β	O
→	O
∞	O
.	O
notice	O
that	O
the	O
covariance	B
has	O
the	O
form	O
of	O
i.e	O
.	O
by	O
taking	O
the	O
limit	O
σ2	O
contributions	O
from	O
explicit	O
basis	O
functions	O
,	O
h	O
(	O
x	O
)	O
=	O
(	O
1	O
,	O
x	O
)	O
>	O
and	O
a	O
regular	O
covari-	O
ance	O
function	B
ksp	O
(	O
x	O
,	O
x0	O
)	O
,	O
a	O
problem	O
which	O
we	O
have	O
already	O
studied	O
in	O
section	O
2.7.	O
β	O
→	O
∞	O
,	O
indeed	O
we	O
have	O
computed	O
the	O
limit	O
where	O
the	O
prior	O
becomes	O
vague	O
σ2	O
the	O
result	O
is	O
given	O
in	O
eq	O
.	O
(	O
2.42	O
)	O
.	O
plugging	O
into	O
the	O
mean	O
equation	O
from	O
eq	O
.	O
(	O
2.42	O
)	O
,	O
we	O
get	O
the	O
predictive	B
mean	O
¯f	O
(	O
x∗	O
)	O
=	O
k	O
(	O
x∗	O
)	O
>	O
k−1	O
y	O
(	O
y	O
−	O
h	O
>	O
¯β	O
)	O
+	O
h	O
(	O
x∗	O
)	O
>	O
¯β	O
,	O
(	O
6.29	O
)	O
nδij	O
eval-	O
where	O
ky	O
is	O
the	O
covariance	B
matrix	I
corresponding	O
to	O
σ2	O
uated	O
at	O
the	O
training	O
points	O
,	O
h	O
is	O
the	O
matrix	B
that	O
collects	O
the	O
h	O
(	O
xi	O
)	O
vectors	O
at	O
all	O
training	O
points	O
,	O
and	O
¯β	O
=	O
(	O
hk−1	O
y	O
y	O
is	O
given	O
below	O
eq	O
.	O
(	O
2.42	O
)	O
.	O
it	O
is	O
not	O
diﬃcult	O
to	O
show	O
that	O
this	O
predictive	B
mean	O
function	B
is	O
a	O
piecewise	B
cu-	O
bic	O
polynomial	B
,	O
since	O
the	O
elements	O
of	O
k	O
(	O
x∗	O
)	O
are	O
piecewise3	O
cubic	O
polynomials	O
.	O
showing	O
that	O
the	O
mean	B
function	I
is	O
a	O
ﬁrst	O
order	O
polynomial	B
in	O
the	O
outer	O
intervals	O
[	O
0	O
,	O
x1	O
]	O
and	O
[	O
xn	O
,	O
1	O
]	O
is	O
left	O
as	O
exercise	O
6.7.3.	O
y	O
h	O
>	O
)	O
−1hk−1	O
f	O
ksp	O
(	O
xi	O
,	O
xj	O
)	O
+	O
σ2	O
so	O
far	O
ksp	O
has	O
been	O
produced	O
rather	O
mysteriously	O
“	O
from	O
the	O
hat	O
”	O
;	O
we	O
now	O
provide	O
some	O
explanation	O
.	O
shepp	O
[	O
1966	O
]	O
deﬁned	O
the	O
l-fold	O
integrated	B
wiener	O
process	B
as	O
l	O
!	O
+	O
z	O
(	O
u	O
)	O
du	O
,	O
wl	O
(	O
x	O
)	O
=	O
l	O
=	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
(	O
6.30	O
)	O
where	O
z	O
(	O
u	O
)	O
denotes	O
the	O
gaussian	O
white	O
noise	O
process	O
with	O
covariance	B
δ	O
(	O
u−	O
u0	O
)	O
.	O
note	O
that	O
w0	O
is	O
the	O
standard	O
wiener	O
process	B
.	O
it	O
is	O
easy	O
to	O
show	O
that	O
ksp	O
(	O
x	O
,	O
x0	O
)	O
is	O
the	O
covariance	B
of	O
the	O
once-integrated	O
wiener	O
process	B
by	O
writing	O
w1	O
(	O
x	O
)	O
and	O
w1	O
(	O
x0	O
)	O
using	O
eq	O
.	O
(	O
6.30	O
)	O
and	O
taking	O
the	O
expectation	O
using	O
the	O
covariance	B
of	O
the	O
white	O
noise	O
process	O
.	O
note	O
that	O
wl	O
is	O
the	O
solution	O
to	O
the	O
stochastic	B
diﬀerential	I
equation	I
(	O
sde	O
)	O
x	O
(	O
l+1	O
)	O
=	O
z	O
;	O
see	B
appendix	O
b	O
for	O
further	O
details	O
on	O
sdes	O
.	O
thus	O
3the	O
pieces	O
are	O
joined	O
at	O
the	O
datapoints	O
,	O
the	O
points	O
where	O
the	O
min	O
(	O
x	O
,	O
x0	O
)	O
from	O
the	O
covari-	O
z	O
1	O
0	O
(	O
x	O
−	O
u	O
)	O
l	O
ance	O
function	B
is	O
non-diﬀerentiable	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
140	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
(	O
a	O
)	O
,	O
spline	O
covariance	B
(	O
b	O
)	O
,	O
squared	B
exponential	I
cov	O
.	O
figure	O
6.1	O
:	O
panel	O
(	O
a	O
)	O
shows	O
the	O
application	O
of	O
the	O
spline	O
covariance	B
to	O
a	O
simple	O
dataset	B
.	O
the	O
full	O
line	O
shows	O
the	O
predictive	B
mean	O
,	O
which	O
is	O
a	O
piecewise	B
cubic	O
polyno-	O
mial	O
,	O
and	O
the	O
grey	O
area	O
indicates	O
the	O
95	O
%	O
conﬁdence	O
area	O
.	O
the	O
two	O
thin	O
dashed	O
and	O
dash-dotted	O
lines	O
are	O
samples	O
from	O
the	O
posterior	O
.	O
note	O
that	O
the	O
posterior	O
samples	O
are	O
not	O
as	O
smooth	O
as	O
the	O
mean	O
.	O
for	O
comparison	O
a	O
gp	O
using	O
the	O
squared	B
exponential	I
covariance	O
function	B
is	O
shown	O
in	O
panel	O
(	O
b	O
)	O
.	O
the	O
hyperparameters	B
in	O
both	O
cases	O
were	O
optimized	O
using	O
the	O
marginal	B
likelihood	I
.	O
the	O
regularizerr	O
(	O
f00	O
(	O
x	O
)	O
)	O
2dx	O
.	O
for	O
the	O
cubic	O
spline	O
we	O
set	B
l	O
=	O
1	O
to	O
obtain	O
the	O
sde	O
x00	O
=	O
z	O
,	O
corresponding	O
to	O
we	O
can	O
also	O
give	O
an	O
explicit	O
basis-function	O
construction	O
for	O
the	O
covariance	B
function	I
ksp	O
.	O
consider	O
the	O
family	O
of	O
random	O
functions	O
given	O
by	O
fn	O
(	O
x	O
)	O
=	O
1√	O
n	O
γi	O
(	O
x	O
−	O
i	O
n	O
)	O
+	O
,	O
(	O
6.31	O
)	O
n−1x	O
i=0	O
n−1x	O
i=0	O
where	O
γ	O
is	O
a	O
vector	O
of	O
parameters	O
with	O
γ	O
∼	O
n	O
(	O
0	O
,	O
i	O
)	O
.	O
note	O
that	O
the	O
sum	O
has	O
the	O
form	O
of	O
evenly	O
spaced	O
“	O
ramps	O
”	O
whose	O
magnitudes	O
are	O
given	O
by	O
the	O
entries	O
in	O
the	O
γ	O
vector	O
.	O
thus	O
e	O
[	O
fn	O
(	O
x	O
)	O
fn	O
(	O
x0	O
)	O
]	O
=	O
1	O
n	O
(	O
x	O
−	O
i	O
n	O
)	O
+	O
(	O
x0	O
−	O
i	O
n	O
)	O
+	O
.	O
(	O
6.32	O
)	O
taking	O
the	O
limit	O
n	O
→	O
∞	O
,	O
we	O
obtain	O
eq	O
.	O
(	O
6.28	O
)	O
,	O
a	O
derivation	O
which	O
is	O
also	O
found	O
in	O
[	O
vapnik	O
,	O
1998	O
,	O
sec	O
.	O
11.6	O
]	O
.	O
notice	O
that	O
the	O
covariance	B
function	I
ksp	O
given	O
in	O
eq	O
.	O
(	O
6.28	O
)	O
corresponds	O
to	O
a	O
gaussian	O
process	B
which	O
is	O
ms	O
continuous	O
but	O
only	O
once	O
ms	O
diﬀerentiable	O
.	O
thus	O
samples	O
from	O
the	O
prior	O
will	O
be	O
quite	O
“	O
rough	O
”	O
,	O
although	O
(	O
as	O
noted	O
in	O
section	O
6.1	O
)	O
the	O
posterior	O
mean	O
,	O
eq	O
.	O
(	O
6.25	O
)	O
,	O
is	O
smoother	O
.	O
the	O
constructions	O
above	O
can	O
be	O
generalized	B
to	O
the	O
regularizerr	O
(	O
f	O
(	O
m	O
)	O
(	O
x	O
)	O
)	O
2	O
dx	O
by	O
replacing	O
(	O
x	O
−	O
u	O
)	O
+	O
with	O
(	O
x	O
−	O
u	O
)	O
m−1	O
eq	O
.	O
(	O
6.32	O
)	O
,	O
and	O
setting	O
h	O
(	O
x	O
)	O
=	O
(	O
1	O
,	O
x	O
,	O
.	O
.	O
.	O
,	O
xm−1	O
)	O
>	O
.	O
+	O
/	O
(	O
m	O
−	O
1	O
)	O
!	O
in	O
eq	O
.	O
(	O
6.28	O
)	O
and	O
similarly	O
in	O
thus	O
,	O
we	O
can	O
use	O
a	O
gaussian	O
process	B
formulation	O
as	O
an	O
alternative	O
to	O
the	O
usual	O
spline	O
ﬁtting	O
procedure	O
.	O
note	O
that	O
the	O
trade-oﬀ	O
parameter	O
λ	O
from	O
eq	O
.	O
(	O
6.26	O
)	O
−505−4−3−2−1012input	O
,	O
xoutput	O
,	O
y−505−4−3−2−1012input	O
,	O
xoutput	O
,	O
y	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
6.4	O
support	B
vector	I
machines	O
141	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
6.2	O
:	O
panel	O
(	O
a	O
)	O
shows	O
a	O
linearly	O
separable	O
binary	B
classiﬁcation	I
problem	O
,	O
and	O
a	O
separating	O
hyperplane	B
.	O
panel	O
(	O
b	O
)	O
shows	O
the	O
maximum	O
margin	O
hyperplane	B
.	O
n/σ2	O
f	O
and	O
σ2	O
f	O
.	O
the	O
hyperparameters	B
σ2	O
n	O
can	O
be	O
set	B
is	O
now	O
given	O
as	O
the	O
ratio	O
σ2	O
using	O
the	O
techniques	O
from	O
section	O
5.4.1	O
by	O
optimizing	O
the	O
marginal	B
likelihood	I
given	O
in	O
eq	O
.	O
(	O
2.45	O
)	O
.	O
kohn	O
and	O
ansley	O
[	O
1987	O
]	O
give	O
details	O
of	O
an	O
o	O
(	O
n	O
)	O
algorithm	O
(	O
based	O
on	O
kalman	O
ﬁltering	O
)	O
for	O
the	O
computation	O
of	O
the	O
spline	O
and	O
the	O
marginal	B
likelihood	I
.	O
in	O
addition	O
to	O
the	O
predictive	B
mean	O
the	O
gp	O
treatment	O
also	O
yields	O
an	O
explicit	O
estimate	O
of	O
the	O
noise	O
level	O
and	O
predictive	B
error	O
bars	O
.	O
figure	O
6.1	O
shows	O
a	O
simple	O
example	O
.	O
notice	O
that	O
whereas	O
the	O
mean	B
function	I
is	O
a	O
piecewise	B
cubic	O
polynomial	B
,	O
samples	O
from	O
the	O
posterior	O
are	O
not	O
smooth	O
.	O
in	O
contrast	O
,	O
for	O
the	O
squared	B
exponential	I
covariance	O
functions	O
shown	O
in	O
panel	O
(	O
b	O
)	O
,	O
both	O
the	O
mean	O
and	O
functions	O
drawn	O
from	O
the	O
posterior	O
are	O
inﬁnitely	O
diﬀerentiable	O
.	O
6.4	O
support	B
vector	I
machines	O
∗	O
since	O
the	O
mid	O
1990	O
’	O
s	O
there	O
has	O
been	O
an	O
explosion	O
of	O
interest	O
in	O
kernel	B
machines	O
,	O
and	O
in	O
particular	O
the	O
support	B
vector	I
machine	I
(	O
svm	O
)	O
.	O
the	O
aim	O
of	O
this	O
section	O
is	O
to	O
provide	O
a	O
brief	O
introduction	O
to	O
svms	O
and	O
in	O
particular	O
to	O
compare	O
them	O
to	O
gaussian	O
process	B
predictors	O
.	O
we	O
consider	O
svms	O
for	O
classiﬁcation	B
and	O
re-	O
gression	O
problems	O
in	O
sections	O
6.4.1	O
and	O
6.4.2	O
respectively	O
.	O
more	O
comprehensive	O
treatments	O
can	O
be	O
found	O
in	O
vapnik	O
[	O
1995	O
]	O
,	O
cristianini	O
and	O
shawe-taylor	O
[	O
2000	O
]	O
and	O
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
]	O
.	O
6.4.1	O
support	B
vector	I
classiﬁcation	O
for	O
support	B
vector	I
classiﬁers	O
,	O
the	O
key	O
notion	O
that	O
we	O
need	O
to	O
introduce	O
is	O
that	O
of	O
the	O
maximum	O
margin	O
hyperplane	B
for	O
a	O
linear	B
classiﬁer	I
.	O
then	O
by	O
using	O
the	O
“	O
kernel	B
trick	I
”	O
this	O
can	O
be	O
lifted	O
into	O
feature	B
space	I
.	O
we	O
consider	O
ﬁrst	O
the	O
sep-	O
arable	O
case	O
and	O
then	O
the	O
non-separable	O
case	O
.	O
we	O
conclude	O
this	O
section	O
with	O
a	O
comparison	O
between	O
gp	O
classiﬁers	O
and	O
svms	O
.	O
xixj     w·x+w0	O
<	O
0w·x+w0	O
>	O
0.     wmarginxixj	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
142	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
the	O
separable	O
case	O
figure	O
6.2	O
(	O
a	O
)	O
illustrates	O
the	O
case	O
where	O
the	O
data	O
is	O
linearly	O
separable	O
.	O
for	O
a	O
linear	B
classiﬁer	I
with	O
weight	B
vector	I
w	O
and	O
oﬀset	O
w0	O
,	O
let	O
the	O
decision	O
boundary	O
be	O
deﬁned	O
by	O
w	O
·	O
x	O
+	O
w0	O
=	O
0	O
,	O
and	O
let	O
˜w	O
=	O
(	O
w	O
,	O
w0	O
)	O
.	O
clearly	O
,	O
there	O
is	O
a	O
whole	O
version	O
space	O
of	O
weight	O
vectors	O
that	O
give	O
rise	O
to	O
the	O
same	O
classiﬁcation	B
of	O
the	O
training	O
points	O
.	O
the	O
svm	O
algorithm	O
chooses	O
a	O
particular	O
weight	B
vector	I
,	O
that	O
gives	O
rise	O
to	O
the	O
“	O
maximum	O
margin	O
”	O
of	O
separation	O
.	O
functional	B
margin	O
geometrical	B
margin	O
let	O
the	O
training	O
set	B
be	O
pairs	O
of	O
the	O
form	O
(	O
xi	O
,	O
yi	O
)	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
,	O
where	O
yi	O
=	O
±1	O
.	O
for	O
a	O
given	O
weight	B
vector	I
we	O
can	O
compute	O
the	O
quantity	O
˜γi	O
=	O
yi	O
(	O
w·	O
x	O
+	O
w0	O
)	O
,	O
which	O
is	O
known	O
as	O
the	O
functional	B
margin	O
.	O
notice	O
that	O
˜γi	O
>	O
0	O
if	O
a	O
training	O
point	O
is	O
correctly	O
classiﬁed	O
.	O
if	O
the	O
equation	O
f	O
(	O
x	O
)	O
=	O
w	O
·	O
x	O
+	O
w0	O
deﬁnes	O
a	O
discriminant	O
function	B
(	O
so	O
that	O
the	O
output	O
is	O
sgn	O
(	O
f	O
(	O
x	O
)	O
)	O
)	O
,	O
then	O
the	O
hyperplane	B
cw	O
·	O
x	O
+	O
cw0	O
deﬁnes	O
the	O
same	O
discriminant	O
function	B
for	O
any	O
c	O
>	O
0.	O
thus	O
we	O
have	O
the	O
freedom	O
to	O
choose	O
the	O
scaling	O
of	O
˜w	O
so	O
that	O
mini	O
˜γi	O
=	O
1	O
,	O
and	O
in	O
this	O
case	O
˜w	O
is	O
known	O
as	O
the	O
canonical	B
form	O
of	O
the	O
hyperplane	B
.	O
the	O
geometrical	B
margin	O
is	O
deﬁned	O
as	O
γi	O
=	O
˜γi/|w|	O
.	O
for	O
a	O
training	O
point	O
xi	O
that	O
is	O
correctly	O
classiﬁed	O
this	O
is	O
simply	O
the	O
distance	O
from	O
xi	O
to	O
the	O
hyperplane	B
.	O
to	O
see	B
this	O
,	O
let	O
c	O
=	O
1/|w|	O
so	O
that	O
ˆw	O
=	O
w/|w|	O
is	O
a	O
unit	O
vector	O
in	O
the	O
direction	O
of	O
w	O
,	O
and	O
ˆw0	O
is	O
the	O
corresponding	O
oﬀset	O
.	O
then	O
ˆw	O
·	O
x	O
computes	O
the	O
length	O
of	O
the	O
projection	O
of	O
x	O
onto	O
the	O
direction	O
orthogonal	O
to	O
the	O
hyperplane	B
and	O
ˆw·x+	O
ˆw0	O
computes	O
the	O
distance	O
to	O
the	O
hyperplane	B
.	O
for	O
training	O
points	O
that	O
are	O
misclassiﬁed	O
the	O
geometrical	B
margin	O
is	O
the	O
negative	O
distance	O
to	O
the	O
hyperplane	B
.	O
the	O
geometrical	B
margin	O
for	O
a	O
dataset	B
d	O
is	O
deﬁned	O
as	O
γd	O
=	O
mini	O
γi	O
.	O
thus	O
for	O
a	O
canonical	B
separating	O
hyperplane	B
the	O
margin	B
is	O
1/|w|	O
.	O
we	O
wish	O
to	O
ﬁnd	O
the	O
maximum	O
margin	O
hyperplane	B
,	O
i.e	O
.	O
the	O
one	O
that	O
maximizes	O
γd	O
.	O
optimization	O
problem	O
by	O
considering	O
canonical	B
hyperplanes	O
,	O
we	O
are	O
thus	O
led	O
to	O
the	O
following	O
op-	O
timization	O
problem	O
to	O
determine	O
the	O
maximum	O
margin	O
hyperplane	B
:	O
|w|2	O
1	O
2	O
minimize	O
subject	O
to	O
yi	O
(	O
w	O
·	O
xi	O
+	O
w0	O
)	O
≥	O
1	O
over	O
w	O
,	O
w0	O
for	O
all	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n.	O
(	O
6.33	O
)	O
it	O
is	O
clear	O
by	O
considering	O
the	O
geometry	O
that	O
for	O
the	O
maximum	O
margin	O
solution	O
there	O
will	O
be	O
at	O
least	O
one	O
data	O
point	O
in	O
each	O
class	O
for	O
which	O
yi	O
(	O
w·xi+w0	O
)	O
=	O
1	O
,	O
see	B
figure	O
6.2	O
(	O
b	O
)	O
.	O
let	O
the	O
hyperplanes	O
that	O
pass	O
through	O
these	O
points	O
be	O
denoted	O
h+	O
and	O
h−	O
respectively	O
.	O
this	O
constrained	O
optimization	O
problem	O
can	O
be	O
set	B
up	O
using	O
lagrange	O
multi-	O
pliers	O
,	O
and	O
solved	O
using	O
numerical	O
methods	O
for	O
quadratic	O
programming4	O
(	O
qp	O
)	O
problems	O
.	O
the	O
form	O
of	O
the	O
solution	O
is	O
w	O
=	O
x	O
4a	O
quadratic	B
programming	I
problem	O
is	O
an	O
optimization	O
problem	O
where	O
the	O
objective	O
func-	O
tion	O
is	O
quadratic	O
and	O
the	O
constraints	O
are	O
linear	B
in	O
the	O
unknowns	O
.	O
i	O
λiyixi	O
,	O
(	O
6.34	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
6.4	O
support	B
vector	I
machines	O
143	O
support	O
vectors	O
kernel	B
trick	I
soft	O
margin	B
where	O
the	O
λi	O
’	O
s	O
are	O
non-negative	O
lagrange	O
multipliers	O
.	O
notice	O
that	O
the	O
solution	O
is	O
a	O
linear	B
combination	O
of	O
the	O
xi	O
’	O
s	O
.	O
the	O
key	O
feature	O
of	O
equation	O
6.34	O
is	O
that	O
λi	O
is	O
zero	O
for	O
every	O
xi	O
except	O
those	O
which	O
lie	O
on	O
the	O
hyperplanes	O
h+	O
or	O
h−	O
;	O
these	O
points	O
are	O
called	O
the	O
support	O
vectors	O
.	O
the	O
fact	O
that	O
not	O
all	O
of	O
the	O
training	O
points	O
contribute	O
to	O
the	O
ﬁnal	O
solution	O
is	O
referred	O
to	O
as	O
the	O
sparsity	O
of	O
the	O
solution	O
.	O
the	O
support	O
vectors	O
lie	O
closest	O
to	O
the	O
decision	O
boundary	O
.	O
note	O
that	O
if	O
all	O
of	O
the	O
other	O
training	O
points	O
were	O
removed	O
(	O
or	O
moved	O
around	O
,	O
but	O
not	O
crossing	O
h+	O
or	O
h−	O
)	O
the	O
same	O
maximum	O
margin	O
hyperplane	B
would	O
be	O
found	O
.	O
the	O
quadratic	B
programming	I
problem	O
for	O
ﬁnding	O
the	O
λi	O
’	O
s	O
is	O
convex	B
,	O
i.e	O
.	O
there	O
are	O
no	O
local	O
minima	O
.	O
notice	O
the	O
similarity	O
of	O
this	O
to	O
the	O
convexity	O
of	O
the	O
optimization	O
problem	O
for	O
gaussian	O
process	B
classiﬁers	O
,	O
as	O
described	O
in	O
section	O
3.4.	O
to	O
make	O
predictions	O
for	O
a	O
new	O
input	O
x∗	O
we	O
compute	O
sgn	O
(	O
w	O
·	O
x∗	O
+	O
w0	O
)	O
=	O
sgn	O
λiyi	O
(	O
xi	O
·	O
x∗	O
)	O
+	O
w0	O
(	O
cid:16	O
)	O
nx	O
(	O
cid:17	O
)	O
.	O
(	O
6.35	O
)	O
i=1	O
in	O
the	O
qp	O
problem	O
and	O
in	O
eq	O
.	O
(	O
6.35	O
)	O
the	O
training	O
points	O
{	O
xi	O
}	O
and	O
the	O
test	O
point	O
x∗	O
enter	O
the	O
computations	O
only	O
in	O
terms	O
of	O
inner	O
products	O
.	O
thus	O
by	O
using	O
the	O
kernel	B
trick	I
we	O
can	O
replace	O
occurrences	O
of	O
the	O
inner	O
product	O
by	O
the	O
kernel	B
to	O
obtain	O
the	O
equivalent	B
result	O
in	O
feature	B
space	I
.	O
the	O
non-separable	O
case	O
for	O
linear	B
classiﬁers	O
in	O
the	O
original	O
x	O
space	O
there	O
will	O
be	O
some	O
datasets	O
that	O
are	O
not	O
linearly	O
separable	O
.	O
one	O
way	O
to	O
generalize	O
the	O
svm	O
problem	O
in	O
this	O
case	O
is	O
to	O
allow	O
violations	O
of	O
the	O
constraint	O
yi	O
(	O
w	O
·	O
xi	O
+	O
w0	O
)	O
≥	O
1	O
but	O
to	O
impose	O
a	O
penalty	O
when	O
this	O
occurs	O
.	O
this	O
leads	O
to	O
the	O
soft	B
margin	I
support	O
vector	O
machine	O
problem	O
,	O
the	O
minimization	O
of	O
nx	O
i=1	O
|w|2	O
+	O
c	O
1	O
2	O
(	O
1	O
−	O
yifi	O
)	O
+	O
(	O
6.36	O
)	O
with	O
respect	O
to	O
w	O
and	O
w0	O
,	O
where	O
fi	O
=	O
f	O
(	O
xi	O
)	O
=	O
w	O
·	O
xi	O
+	O
w0	O
and	O
(	O
z	O
)	O
+	O
=	O
z	O
if	O
z	O
>	O
0	O
and	O
0	O
otherwise	O
.	O
here	O
c	O
>	O
0	O
is	O
a	O
parameter	O
that	O
speciﬁes	O
the	O
relative	O
importance	O
of	O
the	O
two	O
terms	O
.	O
this	O
convex	B
optimization	O
problem	O
can	O
again	O
be	O
solved	O
using	O
qp	O
methods	O
and	O
yields	O
a	O
solution	O
of	O
the	O
form	O
given	O
in	O
eq	O
.	O
(	O
6.34	O
)	O
.	O
in	O
this	O
case	O
the	O
support	O
vectors	O
(	O
those	O
with	O
λi	O
6=	O
0	O
)	O
are	O
not	O
only	O
those	O
data	O
points	O
which	O
lie	O
on	O
the	O
separating	O
hyperplanes	O
,	O
but	O
also	O
those	O
that	O
incur	O
penalties	O
.	O
this	O
can	O
occur	O
in	O
two	O
ways	O
(	O
i	O
)	O
the	O
data	O
point	O
falls	O
in	O
between	O
h+	O
and	O
h−	O
but	O
on	O
the	O
correct	O
side	O
of	O
the	O
decision	B
surface	I
,	O
or	O
(	O
ii	O
)	O
the	O
data	O
point	O
falls	O
on	O
the	O
wrong	O
side	O
of	O
the	O
decision	B
surface	I
.	O
in	O
a	O
feature	B
space	I
of	O
dimension	O
n	O
,	O
if	O
n	O
>	O
n	O
then	O
there	O
will	O
always	O
be	O
separating	O
hyperplane	B
.	O
however	O
,	O
this	O
hyperplane	B
may	O
not	O
give	O
rise	O
to	O
good	O
generalization	B
performance	O
,	O
especially	O
if	O
some	O
of	O
the	O
labels	O
are	O
incorrect	O
,	O
and	O
thus	O
the	O
soft	B
margin	I
svm	O
formulation	O
is	O
often	O
used	O
in	O
practice	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
144	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
6.3	O
:	O
(	O
a	O
)	O
a	O
comparison	O
of	O
the	O
hinge	O
error	O
,	O
gλ	O
and	O
gφ	O
.	O
(	O
b	O
)	O
the	O
-insensitive	B
error	I
function	I
used	O
in	O
svr	O
.	O
for	O
both	O
the	O
hard	O
and	O
soft	B
margin	I
svm	O
qp	O
problems	O
a	O
wide	O
variety	O
of	O
algorithms	O
have	O
been	O
developed	O
for	O
their	O
solution	O
;	O
see	B
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
,	O
ch	O
.	O
10	O
]	O
for	O
details	O
.	O
basic	O
interior	O
point	O
methods	O
involve	O
inversions	O
of	O
n×n	O
matrices	O
and	O
thus	O
scale	O
as	O
o	O
(	O
n3	O
)	O
,	O
as	O
with	O
gaussian	O
process	B
prediction	O
.	O
however	O
,	O
there	O
are	O
other	O
algorithms	O
,	O
such	O
as	O
the	O
sequential	O
minimal	O
optimization	O
(	O
smo	O
)	O
algorithm	O
due	O
to	O
platt	O
[	O
1999	O
]	O
,	O
which	O
often	O
have	O
better	O
scaling	O
in	O
practice	O
.	O
above	O
we	O
have	O
described	O
svms	O
for	O
the	O
two-class	O
(	O
binary	B
)	O
classiﬁcation	B
prob-	O
lem	O
.	O
there	O
are	O
many	O
ways	O
of	O
generalizing	O
svms	O
to	O
the	O
multi-class	B
problem	O
,	O
see	B
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
,	O
sec	O
.	O
7.6	O
]	O
for	O
further	O
details	O
.	O
comparing	O
support	B
vector	I
and	O
gaussian	O
process	B
classiﬁers	O
for	O
the	O
soft	B
margin	I
classiﬁer	O
we	O
obtain	O
a	O
solution	O
of	O
the	O
form	O
w	O
=	O
p	O
(	O
with	O
αi	O
=	O
λiyi	O
)	O
and	O
thus	O
|w|2	O
=p	O
i	O
αixi	O
i	O
,	O
j	O
αiαj	O
(	O
xi	O
·	O
xj	O
)	O
.	O
kernelizing	O
this	O
we	O
obtain	O
|w|2	O
=	O
α	O
>	O
kα	O
=	O
f	O
>	O
k−1f	O
,	O
as5	O
kα	O
=	O
f.	O
thus	O
the	O
soft	B
margin	I
objective	O
nx	O
function	B
can	O
be	O
written	O
as	O
f	O
>	O
k−1f	O
+	O
c	O
(	O
1	O
−	O
yifi	O
)	O
+	O
.	O
(	O
6.37	O
)	O
1	O
2	O
1	O
2	O
i=1	O
f	O
>	O
k−1f	O
−	O
nx	O
i=1	O
for	O
the	O
binary	B
gp	O
classiﬁer	B
,	O
to	O
obtain	O
the	O
map	O
value	O
ˆf	O
of	O
p	O
(	O
f|y	O
)	O
we	O
minimize	O
the	O
quantity	O
log	O
p	O
(	O
yi|fi	O
)	O
,	O
(	O
6.38	O
)	O
cf	O
.	O
eq	O
.	O
(	O
3.12	O
)	O
.	O
(	O
the	O
ﬁnal	O
two	O
terms	O
in	O
eq	O
.	O
(	O
3.12	O
)	O
are	O
constant	O
if	O
the	O
kernel	B
is	O
ﬁxed	O
.	O
)	O
for	O
log-concave	O
likelihoods	O
(	O
such	O
as	O
those	O
derived	O
from	O
the	O
logistic	B
or	O
pro-	O
bit	O
response	O
functions	O
)	O
there	O
is	O
a	O
strong	O
similarity	O
between	O
the	O
two	O
optimiza-	O
tion	O
problems	O
in	O
that	O
they	O
are	O
both	O
convex	B
.	O
let	O
gλ	O
(	O
z	O
)	O
,	O
log	O
(	O
1	O
+	O
e−z	O
)	O
,	O
gφ	O
=	O
5here	O
the	O
oﬀset	O
w0	O
has	O
been	O
absorbed	O
into	O
the	O
kernel	B
so	O
it	O
is	O
not	O
an	O
explicit	O
extra	O
param-	O
eter	O
.	O
−2014012log	O
(	O
1	O
+	O
exp	O
(	O
−z	O
)	O
)	O
−log	O
f	O
(	O
z	O
)	O
max	O
(	O
1−z	O
,	O
0	O
)	O
zg	O
(	O
z	O
)	O
−0−	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
6.4	O
support	B
vector	I
machines	O
145	O
hinge	B
error	I
function	I
−	O
log	O
φ	O
(	O
z	O
)	O
,	O
and	O
ghinge	O
(	O
z	O
)	O
,	O
(	O
1	O
−	O
z	O
)	O
+	O
where	O
z	O
=	O
yifi	O
.	O
we	O
refer	O
to	O
ghinge	O
as	O
the	O
hinge	B
error	I
function	I
,	O
due	O
to	O
its	O
shape	O
.	O
as	O
shown	O
in	O
figure	O
6.3	O
(	O
a	O
)	O
all	O
three	O
data	O
ﬁt	O
terms	O
are	O
monotonically	O
decreasing	O
functions	O
of	O
z.	O
all	O
three	O
functions	O
tend	O
to	O
inﬁnity	O
as	O
z	O
→	O
−∞	O
and	O
decay	O
to	O
zero	O
as	O
z	O
→	O
∞	O
.	O
the	O
key	O
diﬀerence	O
is	O
that	O
the	O
hinge	B
function	O
takes	O
on	O
the	O
value	O
0	O
for	O
z	O
≥	O
1	O
,	O
while	O
the	O
other	O
two	O
just	O
decay	O
slowly	O
.	O
it	O
is	O
this	O
ﬂat	O
part	O
of	O
the	O
hinge	B
function	O
that	O
gives	O
rise	O
to	O
the	O
sparsity	O
of	O
the	O
svm	O
solution	O
.	O
thus	O
there	O
is	O
a	O
close	O
correspondence	O
between	O
the	O
map	O
solution	O
of	O
the	O
gp	O
classiﬁer	B
and	O
the	O
svm	O
solution	O
.	O
can	O
this	O
correspondence	O
be	O
made	O
closer	O
by	O
considering	O
the	O
hinge	B
function	O
as	O
a	O
negative	O
log	O
likelihood	O
?	O
the	O
answer	O
to	O
this	O
is	O
no	O
[	O
seeger	O
,	O
2000	O
,	O
sollich	O
,	O
2002	O
]	O
.	O
if	O
cghinge	O
(	O
z	O
)	O
deﬁned	O
a	O
negative	O
log	O
likelihood	O
,	O
then	O
exp	O
(	O
−cghinge	O
(	O
f	O
)	O
)	O
+	O
exp	O
(	O
−cghinge	O
(	O
−f	O
)	O
)	O
should	O
be	O
a	O
constant	O
independent	O
of	O
f	O
,	O
but	O
this	O
is	O
not	O
the	O
case	O
.	O
to	O
see	B
this	O
,	O
consider	O
the	O
quantity	O
ν	O
(	O
f	O
;	O
c	O
)	O
=	O
κ	O
(	O
c	O
)	O
[	O
exp	O
(	O
−c	O
(	O
1	O
−	O
f	O
)	O
+	O
)	O
+	O
exp	O
(	O
−c	O
(	O
1	O
+	O
f	O
)	O
+	O
)	O
]	O
.	O
(	O
6.39	O
)	O
κ	O
(	O
c	O
)	O
can	O
not	O
be	O
chosen	O
so	O
as	O
to	O
make	O
ν	O
(	O
f	O
;	O
c	O
)	O
=	O
1	O
independent	O
of	O
the	O
value	O
of	O
f	O
for	O
c	O
>	O
0.	O
by	O
comparison	O
,	O
for	O
the	O
logistic	B
and	O
probit	B
likelihoods	O
the	O
analogous	O
expression	O
is	O
equal	O
to	O
1.	O
sollich	O
[	O
2002	O
]	O
suggests	O
choosing	O
κ	O
(	O
c	O
)	O
=	O
1/	O
(	O
1	O
+	O
exp	O
(	O
−2c	O
)	O
)	O
which	O
ensures	O
that	O
ν	O
(	O
f	O
,	O
c	O
)	O
≤	O
1	O
(	O
with	O
equality	O
only	O
when	O
f	O
=	O
±1	O
)	O
.	O
he	O
also	O
gives	O
an	O
ingenious	O
interpretation	O
(	O
involving	O
a	O
“	O
don	O
’	O
t	O
know	O
”	O
class	O
to	O
soak	O
up	O
the	O
unassigned	O
probability	B
mass	O
)	O
that	O
does	O
yield	O
the	O
svm	O
solution	O
as	O
the	O
map	O
solution	O
to	O
a	O
certain	O
bayesian	O
problem	O
,	O
although	O
we	O
ﬁnd	O
this	O
construction	O
rather	O
contrived	O
.	O
exercise	O
6.7.2	O
invites	O
you	O
to	O
plot	O
ν	O
(	O
f	O
;	O
c	O
)	O
as	O
a	O
function	B
of	O
f	O
for	O
various	O
values	O
of	O
c.	O
one	O
attraction	O
of	O
the	O
gp	O
classiﬁer	B
is	O
that	O
it	O
produces	O
an	O
output	O
with	O
a	O
clear	O
probabilistic	B
interpretation	O
,	O
a	O
prediction	B
for	O
p	O
(	O
y	O
=	O
+1|x	O
)	O
.	O
one	O
can	O
try	O
to	O
interpret	O
the	O
function	B
value	O
f	O
(	O
x	O
)	O
output	O
by	O
the	O
svm	O
probabilistically	O
,	O
and	O
platt	O
[	O
2000	O
]	O
suggested	O
that	O
probabilistic	B
predictions	O
can	O
be	O
generated	O
from	O
the	O
svm	O
by	O
computing	O
σ	O
(	O
af	O
(	O
x	O
)	O
+	O
b	O
)	O
for	O
some	O
constants	O
a	O
,	O
b	O
that	O
are	O
ﬁtted	O
using	O
some	O
“	O
unbiased	O
version	O
”	O
of	O
the	O
training	O
set	B
(	O
e.g	O
.	O
using	O
cross-validation	B
)	O
.	O
one	O
disadvantage	O
of	O
this	O
rather	O
ad	O
hoc	O
procedure	O
is	O
that	O
unlike	O
the	O
gp	O
classiﬁers	O
it	O
does	O
not	O
take	O
into	O
account	O
the	O
predictive	B
variance	O
of	O
f	O
(	O
x	O
)	O
(	O
cf	O
.	O
eq	O
.	O
(	O
3.25	O
)	O
)	O
.	O
seeger	O
[	O
2003	O
,	O
sec	O
.	O
4.7.2	O
]	O
shows	O
that	O
better	O
error-reject	O
curves	O
can	O
be	O
obtained	O
on	O
an	O
experiment	O
using	O
the	O
mnist	O
digit	O
classiﬁcation	B
problem	O
when	O
the	O
eﬀect	O
of	O
this	O
uncertainty	O
is	O
taken	O
into	O
account	O
.	O
6.4.2	O
support	B
vector	I
regression	I
the	O
svm	O
was	O
originally	O
introduced	O
for	O
the	O
classiﬁcation	B
problem	O
,	O
then	O
extended	O
to	O
deal	O
with	O
the	O
regression	B
case	O
.	O
the	O
key	O
concept	O
is	O
that	O
of	O
the	O
-insensitive	B
error	I
function	I
.	O
this	O
is	O
deﬁned	O
as	O
(	O
6.40	O
)	O
(	O
cid:26	O
)	O
|z|	O
−	O
	O
0	O
g	O
(	O
z	O
)	O
=	O
if	O
|z|	O
≥	O
	O
,	O
otherwise	O
.	O
this	O
function	B
is	O
plotted	O
in	O
figure	O
6.3	O
(	O
b	O
)	O
.	O
as	O
in	O
eq	O
.	O
(	O
6.21	O
)	O
we	O
can	O
interpret	O
exp	O
(	O
−g	O
(	O
z	O
)	O
)	O
as	O
a	O
likelihood	B
model	O
for	O
the	O
regression	B
residuals	O
(	O
c.f	O
.	O
the	O
squared	B
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
146	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
error	B
function	I
corresponding	O
to	O
a	O
gaussian	O
model	B
)	O
.	O
however	O
,	O
we	O
note	O
that	O
this	O
is	O
quite	O
an	O
unusual	O
choice	O
of	O
model	B
for	O
the	O
distribution	O
of	O
residuals	O
and	O
is	O
basically	O
motivated	O
by	O
the	O
desire	O
to	O
obtain	O
a	O
sparse	O
solution	O
(	O
see	B
below	O
)	O
as	O
in	O
support	B
vector	I
classiﬁer	O
.	O
if	O
	O
=	O
0	O
then	O
the	O
error	B
model	O
is	O
a	O
laplacian	O
distribution	O
,	O
which	O
corresponds	O
to	O
least	O
absolute	O
values	O
regression	B
(	O
edgeworth	O
[	O
1887	O
]	O
,	O
cited	O
in	O
rousseeuw	O
[	O
1984	O
]	O
)	O
;	O
this	O
is	O
a	O
heavier-tailed	O
distribution	O
than	O
the	O
gaussian	O
and	O
provides	O
some	O
protection	O
against	O
outliers	O
.	O
girosi	O
[	O
1991	O
]	O
showed	O
that	O
the	O
laplacian	O
distribution	O
can	O
be	O
viewed	O
as	O
a	O
continuous	O
mixture	O
of	O
zero-	O
mean	O
gaussians	O
with	O
a	O
certain	O
distribution	O
over	O
their	O
variances	O
.	O
pontil	O
et	O
al	O
.	O
[	O
1998	O
]	O
extended	O
this	O
result	O
by	O
allowing	O
the	O
means	O
to	O
uniformly	O
shift	O
in	O
[	O
−	O
,	O
	O
]	O
in	O
order	O
to	O
obtain	O
a	O
probabilistic	B
model	O
corresponding	O
to	O
the	O
-insensitive	B
error	I
function	I
.	O
see	B
also	O
section	O
9.3	O
for	O
work	O
on	O
robustiﬁcation	O
of	O
the	O
gp	O
regression	B
problem	O
.	O
for	O
the	O
linear	B
regression	I
case	O
with	O
an	O
-insensitive	B
error	I
function	I
and	O
a	O
gaussian	O
prior	O
on	O
w	O
,	O
the	O
map	O
value	O
of	O
w	O
is	O
obtained	O
by	O
minimizing	O
g	O
(	O
yi	O
−	O
fi	O
)	O
(	O
6.41	O
)	O
nx	O
w.r.t	O
.	O
w.	O
the	O
solution6	O
is	O
f	O
(	O
x∗	O
)	O
=pn	O
solution	O
f	O
(	O
x∗	O
)	O
=pn	O
i=1	O
αik	O
(	O
xi	O
,	O
x∗	O
)	O
.	O
i=1	O
|w|2	O
+	O
c	O
1	O
2	O
i=1	O
αixi	O
·	O
x∗	O
where	O
the	O
coeﬃcients	O
α	O
are	O
obtained	O
from	O
a	O
qp	O
problem	O
.	O
the	O
problem	O
can	O
also	O
be	O
kernelized	O
to	O
give	O
the	O
as	O
for	O
support	B
vector	I
classiﬁcation	O
,	O
many	O
of	O
the	O
coeﬃcients	O
αi	O
are	O
zero	O
.	O
the	O
data	O
points	O
which	O
lie	O
inside	O
the	O
-	O
“	O
tube	O
”	O
have	O
αi	O
=	O
0	O
,	O
while	O
those	O
on	O
the	O
edge	O
or	O
outside	O
have	O
non-zero	O
αi	O
.	O
∗	O
6.5	O
least-squares	B
classiﬁcation	I
in	O
chapter	O
3	O
we	O
have	O
argued	O
that	O
the	O
use	O
of	O
logistic	B
or	O
probit	B
likelihoods	O
pro-	O
vides	O
the	O
natural	O
route	O
to	O
develop	O
a	O
gp	O
classiﬁer	B
,	O
and	O
that	O
it	O
is	O
attractive	O
in	O
that	O
the	O
outputs	B
can	O
be	O
interpreted	O
probabilistically	O
.	O
however	O
,	O
there	O
is	O
an	O
even	O
simpler	O
approach	O
which	O
treats	O
classiﬁcation	B
as	O
a	O
regression	B
problem	O
.	O
our	O
starting	O
point	O
is	O
binary	B
classiﬁcation	I
using	O
the	O
linear	B
predictor	O
f	O
(	O
x	O
)	O
=	O
w	O
>	O
x	O
.	O
this	O
is	O
trained	O
using	O
linear	B
regression	I
with	O
a	O
target	O
y+	O
for	O
patterns	O
that	O
have	O
label	O
+1	O
,	O
and	O
target	O
y−	O
for	O
patterns	O
that	O
have	O
label	O
−1	O
.	O
(	O
targets	O
y+	O
,	O
y−	O
give	O
slightly	O
more	O
ﬂexibility	O
than	O
just	O
using	O
targets	O
of	O
±1	O
.	O
)	O
as	O
shown	O
in	O
duda	O
and	O
hart	O
[	O
1973	O
,	O
section	O
5.8	O
]	O
,	O
choosing	O
y+	O
,	O
y−	O
appropriately	O
allows	O
us	O
to	O
obtain	O
the	O
same	O
solution	O
as	O
fisher	O
’	O
s	O
linear	B
discriminant	O
using	O
the	O
decision	O
criterion	O
f	O
(	O
x	O
)	O
≷	O
0.	O
also	O
,	O
they	O
show	O
that	O
using	O
targets	O
y+	O
=	O
+1	O
,	O
y−	O
=	O
−1	O
with	O
the	O
least-squares	B
error	O
function	B
gives	O
a	O
minimum	O
squared-error	O
approximation	O
to	O
the	O
bayes	O
discriminant	O
function	B
p	O
(	O
c+|x	O
)	O
−	O
p	O
(	O
c−|x	O
)	O
as	O
n	O
→	O
∞	O
.	O
following	O
rifkin	O
and	O
klautau	O
[	O
2004	O
]	O
we	O
call	O
such	O
methods	O
least-squares	B
classiﬁcation	I
(	O
lsc	O
)	O
.	O
note	O
that	O
under	O
a	O
probabilistic	B
interpretation	O
the	O
squared-error	O
criterion	O
is	O
rather	O
an	O
6here	O
we	O
have	O
assumed	O
that	O
the	O
constant	O
1	O
is	O
included	O
in	O
the	O
input	O
vector	O
x.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
6.5	O
least-squares	B
classiﬁcation	I
147	O
odd	O
choice	O
as	O
it	O
implies	O
a	O
gaussian	O
noise	B
model	I
,	O
yet	O
only	O
two	O
values	O
of	O
the	O
target	O
(	O
y+	O
and	O
y−	O
)	O
are	O
observed	O
.	O
it	O
is	O
natural	O
to	O
extend	O
the	O
least-squares	B
classiﬁer	O
using	O
the	O
kernel	B
trick	I
.	O
this	O
has	O
been	O
suggested	O
by	O
a	O
number	O
of	O
authors	O
including	O
poggio	O
and	O
girosi	O
[	O
1990	O
]	O
and	O
suykens	O
and	O
vanderwalle	O
[	O
1999	O
]	O
.	O
experimental	O
results	O
reported	O
in	O
rifkin	O
and	O
klautau	O
[	O
2004	O
]	O
indicate	O
that	O
performance	O
comparable	O
to	O
svms	O
can	O
be	O
obtained	O
using	O
kernel	B
lsc	O
(	O
or	O
as	O
they	O
call	O
it	O
the	O
regularized	O
least-squares	B
classiﬁer	O
,	O
rlsc	O
)	O
.	O
consider	O
a	O
single	O
random	O
variable	O
y	O
which	O
takes	O
on	O
the	O
value	O
+1	O
with	O
proba-	O
bility	O
p	O
and	O
value	O
−1	O
with	O
probability	B
1−p	O
.	O
then	O
the	O
value	O
of	O
f	O
which	O
minimizes	O
the	O
squared	B
error	O
function	B
e	O
=	O
p	O
(	O
f	O
−	O
1	O
)	O
2	O
+	O
(	O
1	O
−	O
p	O
)	O
(	O
f	O
+	O
1	O
)	O
2	O
is	O
ˆf	O
=	O
2p	O
−	O
1	O
,	O
which	O
is	O
a	O
linear	B
rescaling	O
of	O
p	O
to	O
the	O
interval	O
[	O
−1	O
,	O
1	O
]	O
.	O
(	O
equivalently	O
if	O
the	O
targets	O
are	O
1	O
and	O
0	O
,	O
we	O
obtain	O
ˆf	O
=	O
p.	O
)	O
hence	O
we	O
observe	O
that	O
lsc	O
will	O
estimate	O
p	O
correctly	O
in	O
the	O
large	O
data	O
limit	O
.	O
if	O
we	O
now	O
consider	O
not	O
just	O
a	O
single	O
random	O
variable	O
,	O
but	O
wish	O
to	O
estimate	O
p	O
(	O
c+|x	O
)	O
(	O
or	O
a	O
linear	B
rescaling	O
of	O
it	O
)	O
,	O
then	O
as	O
long	O
as	O
the	O
approximating	O
function	B
f	O
(	O
x	O
)	O
is	O
suﬃciently	O
ﬂexible	O
,	O
we	O
would	O
expect	O
that	O
in	O
the	O
limit	O
n	O
→	O
∞	O
it	O
would	O
converge	O
to	O
p	O
(	O
c+|x	O
)	O
.	O
(	O
for	O
more	O
technical	O
detail	O
on	O
this	O
issue	O
,	O
see	B
section	O
7.2.1	O
on	O
consistency	B
.	O
)	O
hence	O
lsc	O
is	O
quite	O
a	O
sensible	O
procedure	O
for	O
classiﬁcation	B
,	O
although	O
note	O
that	O
there	O
is	O
no	O
guarantee	O
that	O
f	O
(	O
x	O
)	O
will	O
be	O
constrained	O
to	O
lie	O
in	O
the	O
interval	O
[	O
y−	O
,	O
y+	O
]	O
.	O
if	O
we	O
wish	O
to	O
guarantee	O
a	O
proba-	O
bilistic	O
interpretation	O
,	O
we	O
could	O
“	O
squash	O
”	O
the	O
predictions	O
through	O
a	O
sigmoid	O
,	O
as	O
suggested	O
for	O
svms	O
by	O
platt	O
[	O
2000	O
]	O
and	O
described	O
on	O
page	O
145.	O
when	O
generalizing	O
from	O
the	O
binary	B
to	O
multi-class	B
situation	O
there	O
is	O
some	O
freedom	O
as	O
to	O
how	O
to	O
set	B
the	O
problem	O
up	O
.	O
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
,	O
sec	O
.	O
7.6	O
]	O
identify	O
four	O
methods	O
,	O
namely	O
one-versus-rest	B
(	O
where	O
c	O
binary	B
classiﬁers	O
are	O
trained	O
to	O
classify	O
each	O
class	O
against	O
all	O
the	O
rest	O
)	O
,	O
all	O
pairs	O
(	O
where	O
c	O
(	O
c	O
−	O
1	O
)	O
/2	O
binary	B
classiﬁers	O
are	O
trained	O
)	O
,	O
error-correcting	O
output	O
coding	O
(	O
where	O
each	O
class	O
is	O
assigned	O
a	O
binary	B
codeword	O
,	O
and	O
binary	B
classiﬁers	O
are	O
trained	O
on	O
each	O
bit	O
separately	O
)	O
,	O
and	O
multi-class	B
objective	O
functions	O
(	O
where	O
the	O
aim	O
is	O
to	O
train	O
c	O
classiﬁers	O
simultaneously	O
rather	O
than	O
creating	O
a	O
number	O
of	O
binary	B
classiﬁcation	I
problems	O
)	O
.	O
one	O
also	O
needs	O
to	O
specify	O
how	O
the	O
outputs	B
of	O
the	O
various	O
classiﬁers	O
that	O
are	O
trained	O
are	O
combined	O
so	O
as	O
to	O
produce	O
an	O
overall	O
answer	O
.	O
for	O
the	O
one-versus-rest7	O
method	O
one	O
simple	O
criterion	O
is	O
to	O
choose	O
the	O
classiﬁer	B
which	O
produces	O
the	O
most	O
positive	O
output	O
.	O
rifkin	O
and	O
klautau	O
[	O
2004	O
]	O
performed	O
ex-	O
tensive	O
experiments	O
and	O
came	O
to	O
the	O
conclusion	O
that	O
the	O
one-versus-rest	B
scheme	O
using	O
either	O
svms	O
or	O
rlsc	O
is	O
as	O
accurate	O
as	O
any	O
other	O
method	O
overall	O
,	O
and	O
has	O
the	O
merit	O
of	O
being	O
conceptually	O
simple	O
and	O
straightforward	O
to	O
implement	O
.	O
6.5.1	O
probabilistic	B
least-squares	I
classiﬁcation	I
the	O
lsc	O
algorithm	O
discussed	O
above	O
is	O
attractive	O
from	O
a	O
computational	O
point	O
of	O
view	O
,	O
but	O
to	O
guarantee	O
a	O
valid	O
probabilistic	B
interpretation	O
one	O
may	O
need	O
to	O
use	O
a	O
separate	O
post-processing	O
stage	O
to	O
“	O
squash	O
”	O
the	O
predictions	O
through	O
a	O
sigmoid	O
.	O
however	O
,	O
it	O
is	O
not	O
so	O
easy	O
to	O
enforce	O
a	O
probabilistic	B
interpretation	O
7this	O
method	O
is	O
also	O
sometimes	O
called	O
one-versus-all	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
148	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
during	O
the	O
training	O
stage	O
.	O
one	O
possible	O
solution	O
is	O
to	O
combine	O
the	O
ideas	O
of	O
training	O
using	O
leave-one-out	B
cross-validation	I
,	O
covered	O
in	O
section	O
5.4.2	O
,	O
with	O
the	O
use	O
of	O
a	O
(	O
parameterized	O
)	O
sigmoid	O
function	B
,	O
as	O
in	O
platt	O
[	O
2000	O
]	O
.	O
we	O
will	O
call	O
this	O
method	O
the	O
probabilistic	O
least-squares	O
classiﬁer	O
(	O
plsc	O
)	O
.	O
in	O
section	O
5.4.2	O
we	O
saw	O
how	O
to	O
compute	O
the	O
gaussian	O
leave-one-out	B
(	O
loo	O
)	O
predictive	B
probabilities	O
,	O
and	O
that	O
training	O
of	O
hyperparameters	B
can	O
be	O
based	O
on	O
the	O
sum	O
of	O
the	O
log	O
loo	O
probabilities	O
.	O
using	O
this	O
idea	O
,	O
we	O
express	O
the	O
loo	O
probability	B
by	O
squashing	O
a	O
linear	B
function	O
of	O
the	O
gaussian	O
predictive	B
probability	O
through	O
a	O
cumulative	O
gaussian	O
p	O
(	O
yi|x	O
,	O
y−i	O
,	O
θ	O
)	O
=	O
φ	O
(	O
cid:0	O
)	O
yi	O
(	O
αfi	O
+	O
β	O
)	O
(	O
cid:1	O
)	O
n	O
(	O
fi|µi	O
,	O
σ2	O
(	O
cid:16	O
)	O
yi	O
(	O
αµi	O
+	O
β	O
)	O
(	O
6.42	O
)	O
i	O
)	O
dfi	O
(	O
cid:17	O
)	O
z	O
=	O
φ	O
√	O
,	O
1	O
+	O
α2σ2	O
i	O
where	O
the	O
integral	O
is	O
given	O
in	O
eq	O
.	O
(	O
3.82	O
)	O
and	O
the	O
leave-one-out	B
predictive	O
mean	O
µi	O
and	O
variance	O
σ2	O
i	O
are	O
given	O
in	O
eq	O
.	O
(	O
5.12	O
)	O
.	O
the	O
objective	O
function	B
is	O
the	O
sum	O
of	O
the	O
log	O
loo	O
probabilities	O
,	O
eq	O
.	O
(	O
5.11	O
)	O
which	O
can	O
be	O
used	O
to	O
set	B
the	O
hyperparameters	B
as	O
well	O
as	O
the	O
two	O
additional	O
parameters	O
of	O
the	O
linear	B
transformation	O
,	O
α	O
and	O
β	O
in	O
eq	O
.	O
(	O
6.42	O
)	O
.	O
introducing	O
the	O
likelihood	B
in	O
eq	O
.	O
(	O
6.42	O
)	O
into	O
the	O
objective	O
eq	O
.	O
(	O
5.11	O
)	O
and	O
taking	O
derivatives	O
we	O
obtain	O
∂	O
log	O
p	O
(	O
yi|x	O
,	O
y−i	O
,	O
θ	O
)	O
∂µi	O
yiα√	O
1	O
+	O
α2σ2	O
i	O
n	O
(	O
ri	O
)	O
φ	O
(	O
yiri	O
)	O
√	O
∂µi	O
∂θj	O
(	O
cid:16	O
)	O
∂µi	O
∂θj	O
+	O
∂	O
log	O
p	O
(	O
yi|x	O
,	O
y−i	O
,	O
θ	O
)	O
(	O
cid:17	O
)	O
∂σ2	O
i	O
−	O
α	O
(	O
αµi	O
+	O
β	O
)	O
2	O
(	O
1	O
+	O
α2σ2	O
i	O
)	O
∂σ2	O
i	O
∂θj	O
,	O
∂σ2	O
i	O
∂θj	O
(	O
6.43	O
)	O
1	O
+	O
α2σ2	O
where	O
ri	O
=	O
(	O
αµi	O
+	O
β	O
)	O
/	O
loo	O
parameters	O
∂µi/∂θj	O
and	O
∂σ2	O
linear	B
transformation	O
parameters	O
we	O
have	O
i	O
and	O
the	O
partial	O
derivatives	O
of	O
the	O
gaussian	O
i	O
/∂θj	O
are	O
given	O
in	O
eq	O
.	O
(	O
5.13	O
)	O
.	O
finally	O
,	O
for	O
the	O
∂lloo	O
∂θj	O
=	O
=	O
nx	O
nx	O
i=1	O
i=1	O
nx	O
nx	O
i=1	O
i=1	O
∂lloo	O
∂α	O
∂lloo	O
∂β	O
=	O
=	O
n	O
(	O
ri	O
)	O
φ	O
(	O
yiri	O
)	O
n	O
(	O
ri	O
)	O
φ	O
(	O
yiri	O
)	O
yi√	O
1	O
+	O
α2σ2	O
i	O
yip1	O
+	O
α2σ2	O
i	O
µi	O
−	O
βασ2	O
i	O
1	O
+	O
α2σ2	O
i	O
,	O
.	O
(	O
6.44	O
)	O
these	O
partial	O
derivatives	O
can	O
be	O
used	O
to	O
train	O
the	O
parameters	O
of	O
the	O
gp	O
.	O
there	O
are	O
several	O
options	O
on	O
how	O
to	O
do	O
predictions	O
,	O
but	O
the	O
most	O
natural	O
would	O
seem	O
to	O
be	O
to	O
compute	O
predictive	B
mean	O
and	O
variance	O
and	O
squash	O
it	O
through	O
the	O
sigmoid	O
,	O
parallelling	O
eq	O
.	O
(	O
6.42	O
)	O
.	O
applying	O
this	O
model	B
to	O
the	O
usps	O
3s	O
vs.	O
5s	O
binary	B
classiﬁcation	I
task	O
discussed	O
in	O
section	O
3.7.3	O
,	O
we	O
get	O
a	O
test	O
set	B
error	O
rate	O
of	O
12/773	O
=	O
0.0155	O
%	O
,	O
which	O
compares	O
favourably	O
with	O
the	O
results	O
reported	O
for	O
other	O
methods	O
in	O
figure	O
3.10.	O
however	O
,	O
the	O
test	O
set	B
information	O
is	O
only	O
0.77	O
bits,8	O
which	O
is	O
very	O
poor	O
.	O
8the	O
test	O
information	O
is	O
dominated	O
by	O
a	O
single	O
test	O
case	O
,	O
which	O
is	O
predicted	O
conﬁdently	O
to	O
belong	O
to	O
the	O
wrong	O
class	O
.	O
visual	O
inspection	O
of	O
the	O
digit	O
reveals	O
that	O
indeed	O
it	O
looks	O
as	O
though	O
the	O
testset	O
label	O
is	O
wrong	O
for	O
this	O
case	O
.	O
this	O
observation	O
highlights	O
the	O
danger	O
of	O
not	O
explicitly	O
allowing	O
for	O
data	O
mislabelling	O
in	O
the	O
model	B
for	O
this	O
kind	O
of	O
data	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
6.6	O
relevance	O
vector	O
machines	O
149	O
6.6	O
relevance	O
vector	O
machines	O
∗	O
although	O
usually	O
not	O
presented	O
as	O
such	O
,	O
the	O
relevance	B
vector	I
machine	I
(	O
rvm	O
)	O
introduced	O
by	O
tipping	O
[	O
2001	O
]	O
is	O
actually	O
a	O
special	O
case	O
of	O
a	O
gaussian	O
process	B
.	O
the	O
covariance	B
function	I
has	O
the	O
form	O
nx	O
j=1	O
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
φj	O
(	O
x	O
)	O
φj	O
(	O
x0	O
)	O
,	O
1	O
αj	O
(	O
6.45	O
)	O
where	O
αj	O
are	O
hyperparameters	B
and	O
the	O
n	O
basis	O
functions	O
φj	O
(	O
x	O
)	O
are	O
usually	O
,	O
but	O
not	O
necessarily	O
taken	O
to	O
be	O
gaussian-shaped	O
basis	O
functions	O
centered	O
on	O
each	O
of	O
the	O
n	O
training	O
data	O
points	O
(	O
cid:16	O
)	O
−	O
|x	O
−	O
xj|2	O
(	O
cid:17	O
)	O
2	O
‘	O
2	O
φj	O
(	O
x	O
)	O
=	O
exp	O
,	O
(	O
6.46	O
)	O
where	O
‘	O
is	O
a	O
length-scale	B
hyperparameter	O
controlling	O
the	O
width	O
of	O
the	O
basis	O
function	B
.	O
notice	O
that	O
this	O
is	O
simply	O
the	O
construction	O
for	O
the	O
covariance	B
function	I
corresponding	O
to	O
an	O
n-dimensional	O
set	B
of	O
basis	O
functions	O
given	O
in	O
section	O
2.1.2	O
,	O
with	O
σp	O
=	O
diag	O
(	O
α−1	O
1	O
,	O
.	O
.	O
.	O
,	O
α−1	O
n	O
)	O
.	O
the	O
covariance	B
function	I
in	O
eq	O
.	O
(	O
6.45	O
)	O
has	O
two	O
interesting	O
properties	O
:	O
ﬁrstly	O
,	O
it	O
is	O
clear	O
that	O
the	O
feature	B
space	I
corresponding	O
to	O
the	O
covariance	B
function	I
is	O
ﬁnite	O
dimensional	O
,	O
i.e	O
.	O
the	O
covariance	B
function	I
is	O
degenerate	B
,	O
and	O
secondly	O
the	O
covariance	B
function	I
has	O
the	O
odd	O
property	O
that	O
it	O
depends	O
on	O
the	O
training	O
data	O
.	O
this	O
dependency	O
means	O
that	O
the	O
prior	O
over	O
functions	O
depends	O
on	O
the	O
data	O
,	O
a	O
property	O
which	O
is	O
at	O
odds	O
with	O
a	O
strict	O
bayesian	O
interpretation	O
.	O
although	O
the	O
usual	O
treatment	O
of	O
the	O
model	B
is	O
still	O
possible	O
,	O
this	O
dependency	O
of	O
the	O
prior	O
on	O
the	O
data	O
may	O
lead	O
to	O
some	O
surprising	O
eﬀects	O
,	O
as	O
discussed	O
below	O
.	O
training	O
the	O
rvm	O
is	O
analogous	O
to	O
other	O
gp	O
models	O
:	O
optimize	O
the	O
marginal	B
likelihood	I
w.r.t	O
.	O
the	O
hyperparameters	B
.	O
this	O
optimization	O
often	O
leads	O
to	O
a	O
sig-	O
niﬁcant	O
number	O
of	O
the	O
αj	O
hyperparameters	B
tending	O
towards	O
inﬁnity	O
,	O
eﬀectively	O
removing	O
,	O
or	O
pruning	O
,	O
the	O
corresponding	O
basis	O
function	B
from	O
the	O
covariance	B
function	I
in	O
eq	O
.	O
(	O
6.45	O
)	O
.	O
the	O
basic	O
idea	O
is	O
that	O
basis	O
functions	O
that	O
are	O
not	O
sig-	O
niﬁcantly	O
contributing	O
to	O
explaining	O
the	O
data	O
should	O
be	O
removed	O
,	O
resulting	O
in	O
a	O
sparse	O
model	B
.	O
the	O
basis	O
functions	O
that	O
survive	O
are	O
called	O
relevance	O
vectors	O
.	O
empirically	O
it	O
is	O
often	O
observed	O
that	O
the	O
number	O
of	O
relevance	O
vectors	O
is	O
smaller	O
than	O
the	O
number	O
of	O
support	O
vectors	O
on	O
the	O
same	O
problem	O
[	O
tipping	O
,	O
2001	O
]	O
.	O
the	O
original	O
rvm	O
algorithm	O
[	O
tipping	O
,	O
2001	O
]	O
was	O
not	O
able	O
to	O
exploit	O
the	O
sparsity	O
very	O
eﬀectively	O
during	O
model	B
ﬁtting	O
as	O
it	O
was	O
initialized	O
with	O
all	O
of	O
the	O
αis	O
set	B
to	O
ﬁnite	O
values	O
,	O
meaning	O
that	O
all	O
of	O
the	O
basis	O
functions	O
contributed	O
to	O
the	O
model	B
.	O
however	O
,	O
careful	O
analysis	O
of	O
the	O
rvm	O
marginal	B
likelihood	I
by	O
faul	O
and	O
tipping	O
[	O
2002	O
]	O
showed	O
that	O
one	O
can	O
carry	O
out	O
optimization	O
w.r.t	O
.	O
a	O
single	O
αi	O
analytically	O
.	O
this	O
has	O
led	O
to	O
the	O
accelerated	O
training	O
algorithm	O
described	O
in	O
tipping	O
and	O
faul	O
[	O
2003	O
]	O
which	O
starts	O
with	O
an	O
empty	O
model	B
(	O
i.e	O
.	O
all	O
αis	O
set	B
to	O
inﬁnity	O
)	O
and	O
adds	O
basis	O
functions	O
sequentially	O
.	O
as	O
the	O
number	O
of	O
relevance	O
vectors	O
is	O
(	O
usually	O
much	O
)	O
less	O
than	O
the	O
number	O
of	O
training	O
cases	O
it	O
will	O
often	O
be	O
much	O
faster	O
to	O
train	O
and	O
make	O
predictions	O
using	O
a	O
rvm	O
than	O
a	O
non-sparse	O
relevance	O
vectors	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
150	O
relationships	O
between	O
gps	O
and	O
other	O
models	O
gp	O
.	O
also	O
note	O
that	O
the	O
basis	O
functions	O
can	O
include	O
additional	O
hyperparameters	B
,	O
e.g	O
.	O
one	O
could	O
use	O
an	O
automatic	B
relevance	I
determination	O
(	O
ard	O
)	O
form	O
of	O
basis	O
function	B
by	O
using	O
diﬀerent	O
length-scales	O
on	O
diﬀerent	O
dimensions	O
in	O
eq	O
.	O
(	O
6.46	O
)	O
.	O
these	O
additional	O
hyperparameters	B
could	O
also	O
be	O
set	B
by	O
optimizing	O
the	O
marginal	B
likelihood	I
.	O
the	O
use	O
of	O
a	O
degenerate	B
covariance	O
function	B
which	O
depends	O
on	O
the	O
data	O
imagine	O
a	O
test	O
point	O
,	O
x∗	O
,	O
which	O
lies	O
far	O
away	O
has	O
some	O
undesirable	O
eﬀects	O
.	O
from	O
the	O
relevance	O
vectors	O
.	O
at	O
x∗	O
all	O
basis	O
functions	O
will	O
have	O
values	O
close	O
to	O
zero	O
,	O
and	O
since	O
no	O
basis	O
function	B
can	O
give	O
any	O
appreciable	O
signal	O
,	O
the	O
predictive	B
distribution	O
will	O
be	O
a	O
gaussian	O
with	O
a	O
mean	O
close	O
to	O
zero	O
and	O
variance	O
close	O
to	O
zero	O
(	O
or	O
to	O
the	O
inferred	O
noise	O
level	O
)	O
.	O
this	O
behaviour	O
is	O
undesirable	O
,	O
and	O
could	O
lead	O
to	O
dangerously	O
false	O
conclusions	O
.	O
if	O
the	O
x∗	O
is	O
far	O
from	O
the	O
relevance	O
vectors	O
,	O
then	O
the	O
model	B
shouldn	O
’	O
t	O
be	O
able	O
to	O
draw	O
strong	O
conclusions	O
about	O
the	O
output	O
(	O
we	O
are	O
extrapolating	O
)	O
,	O
but	O
the	O
predictive	B
uncertainty	O
becomes	O
very	O
small—this	O
is	O
the	O
opposite	O
behaviour	O
of	O
what	O
we	O
would	O
expect	O
from	O
a	O
reasonable	O
model	B
.	O
here	O
,	O
we	O
have	O
argued	O
that	O
for	O
localized	O
basis	O
functions	O
,	O
the	O
rvm	O
has	O
undesirable	O
properties	O
,	O
but	O
as	O
argued	O
in	O
rasmussen	O
and	O
qui˜nonero-candela	O
[	O
2005	O
]	O
it	O
is	O
actually	O
the	O
degeneracy	O
of	O
the	O
covariance	B
function	I
which	O
is	O
the	O
core	O
of	O
the	O
problem	O
.	O
although	O
the	O
work	O
of	O
rasmussen	O
and	O
qui˜nonero-candela	O
[	O
2005	O
]	O
goes	O
some	O
way	O
towards	O
ﬁxing	O
the	O
problem	O
,	O
there	O
is	O
an	O
inherent	O
conﬂict	O
:	O
degeneracy	O
of	O
the	O
covariance	B
function	I
is	O
good	O
for	O
computational	O
reasons	O
,	O
but	O
bad	O
for	O
modelling	O
reasons	O
.	O
6.7	O
exercises	O
sis	O
vectors	O
f	O
=	O
pn	O
pn	O
1.	O
we	O
motivate	O
the	O
fact	O
that	O
the	O
rkhs	O
norm	B
does	O
not	O
depend	O
on	O
the	O
den-	O
sity	O
p	O
(	O
x	O
)	O
using	O
a	O
ﬁnite-dimensional	O
analogue	O
.	O
consider	O
the	O
n-dimensional	O
vector	O
f	O
,	O
and	O
let	O
the	O
n×	O
n	O
matrix	B
φ	O
be	O
comprised	O
of	O
non-colinear	O
columns	O
φ1	O
,	O
.	O
.	O
.	O
,	O
φn	O
.	O
then	O
f	O
can	O
be	O
expressed	O
as	O
a	O
linear	B
combination	O
of	O
these	O
ba-	O
i=1	O
ciφi	O
=	O
φc	O
for	O
some	O
coeﬃcients	O
{	O
ci	O
}	O
.	O
let	O
the	O
φs	O
be	O
eigenvectors	O
of	O
the	O
covariance	B
matrix	I
k	O
w.r.t	O
.	O
a	O
diagonal	O
matrix	B
p	O
with	O
non-negative	O
entries	O
,	O
so	O
that	O
kp	O
φ	O
=	O
φλ	O
,	O
where	O
λ	O
is	O
a	O
diagonal	O
matrix	B
containing	O
the	O
eigenvalues	O
.	O
note	O
that	O
φ	O
>	O
p	O
φ	O
=	O
in	O
.	O
show	O
that	O
i	O
/λi	O
=	O
c	O
>	O
λ−1c	O
=	O
f	O
>	O
k−1f	O
,	O
and	O
thus	O
observe	O
that	O
f	O
>	O
k−1f	O
can	O
be	O
expressed	O
as	O
c	O
>	O
λ−1c	O
for	O
any	O
valid	O
p	O
and	O
corresponding	O
φ.	O
hint	O
:	O
you	O
may	O
ﬁnd	O
it	O
useful	O
to	O
set	B
˜φ	O
=	O
p	O
1/2φ	O
,	O
˜k	O
=	O
p	O
1/2kp	O
1/2	O
etc	O
.	O
i=1	O
c2	O
2.	O
plot	O
eq	O
.	O
(	O
6.39	O
)	O
as	O
a	O
function	B
of	O
f	O
for	O
diﬀerent	O
values	O
of	O
c.	O
show	O
that	O
there	O
is	O
no	O
value	O
of	O
c	O
and	O
κ	O
(	O
c	O
)	O
which	O
makes	O
ν	O
(	O
f	O
;	O
c	O
)	O
equal	O
to	O
1	O
for	O
all	O
values	O
of	O
f.	O
try	O
setting	O
κ	O
(	O
c	O
)	O
=	O
1/	O
(	O
1	O
+	O
exp	O
(	O
−2c	O
)	O
)	O
as	O
suggested	O
in	O
sollich	O
[	O
2002	O
]	O
and	O
observe	O
what	O
eﬀect	O
this	O
has	O
.	O
3.	O
show	O
that	O
the	O
predictive	B
mean	O
for	O
the	O
spline	O
covariance	B
gp	O
in	O
eq	O
.	O
(	O
6.29	O
)	O
is	O
a	O
linear	B
function	O
of	O
x∗	O
when	O
x∗	O
is	O
located	O
either	O
to	O
the	O
left	O
or	O
to	O
the	O
right	O
of	O
all	O
training	O
points	O
.	O
hint	O
:	O
consider	O
the	O
eigenvectors	O
corresponding	O
to	O
the	O
two	O
largest	O
eigenvalues	O
of	O
the	O
training	O
set	B
covariance	O
matrix	B
from	O
eq	O
.	O
(	O
2.40	O
)	O
in	O
the	O
vague	O
limit	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
chapter	O
7	O
theoretical	O
perspectives	O
this	O
chapter	O
covers	O
a	O
number	O
of	O
more	O
theoretical	O
issues	O
relating	O
to	O
gaussian	O
processes	O
.	O
in	O
section	O
2.6	O
we	O
saw	O
how	O
gpr	O
carries	O
out	O
a	O
linear	B
smoothing	O
of	O
the	O
datapoints	O
using	O
the	O
weight	B
function	I
.	O
the	O
form	O
of	O
the	O
weight	B
function	I
can	O
be	O
understood	O
in	O
terms	O
of	O
the	O
equivalent	B
kernel	I
,	O
which	O
is	O
discussed	O
in	O
section	O
7.1.	O
as	O
one	O
gets	O
more	O
and	O
more	O
data	O
,	O
one	O
would	O
hope	O
that	O
the	O
gp	O
predictions	O
would	O
converge	O
to	O
the	O
true	O
underlying	O
predictive	B
distribution	O
.	O
this	O
question	O
of	O
consistency	B
is	O
reviewed	O
in	O
section	O
7.2	O
,	O
where	O
we	O
also	O
discuss	O
the	O
concepts	O
of	O
equivalence	O
and	O
orthogonality	O
of	O
gps	O
.	O
when	O
the	O
generating	O
process	B
for	O
the	O
data	O
is	O
assumed	O
to	O
be	O
a	O
gp	O
it	O
is	O
particu-	O
larly	O
easy	O
to	O
obtain	O
results	O
for	O
learning	B
curves	O
which	O
describe	O
how	O
the	O
accuracy	O
of	O
the	O
predictor	O
increases	O
as	O
a	O
function	B
of	O
n	O
,	O
as	O
described	O
in	O
section	O
7.3.	O
an	O
alternative	O
approach	O
to	O
the	O
analysis	O
of	O
generalization	B
error	I
is	O
provided	O
by	O
the	O
pac-bayesian	O
analysis	O
discussed	O
in	O
section	O
7.4.	O
here	O
we	O
seek	O
to	O
relate	O
(	O
with	O
high	O
probability	B
)	O
the	O
error	B
observed	O
on	O
the	O
training	O
set	B
to	O
the	O
generalization	B
error	I
of	O
the	O
gp	O
predictor	O
.	O
gaussian	O
processes	O
are	O
just	O
one	O
of	O
the	O
many	O
methods	O
that	O
have	O
been	O
devel-	O
oped	O
for	O
supervised	B
learning	I
problems	O
.	O
in	O
section	O
7.5	O
we	O
compare	O
and	O
contrast	O
gp	O
predictors	O
with	O
other	O
supervised	B
learning	I
methods	O
.	O
7.1	O
the	O
equivalent	B
kernel	I
in	O
this	O
section	O
we	O
consider	O
regression	B
problems	O
.	O
we	O
have	O
seen	O
in	O
section	O
6.2	O
that	O
the	O
posterior	O
mean	O
for	O
gp	O
regression	B
can	O
be	O
obtained	O
as	O
the	O
function	B
which	O
minimizes	O
the	O
functional	B
(	O
cid:0	O
)	O
yi	O
−	O
f	O
(	O
xi	O
)	O
(	O
cid:1	O
)	O
2	O
,	O
(	O
7.1	O
)	O
j	O
[	O
f	O
]	O
=	O
kfk2h	O
+	O
1	O
2	O
1	O
2σ2	O
n	O
nx	O
i=1	O
where	O
kfkh	O
is	O
the	O
rkhs	O
norm	B
corresponding	O
to	O
kernel	B
k.	O
our	O
goal	O
is	O
now	O
to	O
understand	O
the	O
behaviour	O
of	O
this	O
solution	O
as	O
n	O
→	O
∞	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
152	O
theoretical	O
perspectives	O
let	O
µ	O
(	O
x	O
,	O
y	O
)	O
be	O
the	O
probability	B
measure	O
from	O
which	O
the	O
data	O
pairs	O
(	O
xi	O
,	O
yi	O
)	O
are	O
generated	O
.	O
observe	O
that	O
eh	O
nx	O
(	O
cid:0	O
)	O
yi	O
−	O
f	O
(	O
xi	O
)	O
(	O
cid:1	O
)	O
2i	O
z	O
(	O
cid:0	O
)	O
y	O
−	O
f	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
2	O
=	O
n	O
dµ	O
(	O
x	O
,	O
y	O
)	O
.	O
(	O
7.2	O
)	O
i=1	O
let	O
η	O
(	O
x	O
)	O
=	O
e	O
[	O
y|x	O
]	O
be	O
the	O
regression	B
function	O
corresponding	O
to	O
the	O
probability	B
then	O
writing	O
y	O
−	O
f	O
=	O
(	O
y	O
−	O
η	O
)	O
+	O
(	O
η	O
−	O
f	O
)	O
we	O
obtain	O
measure	B
µ.	O
the	O
variance	O
around	O
η	O
(	O
x	O
)	O
is	O
denoted	O
σ2	O
(	O
x	O
)	O
=r	O
(	O
y	O
−	O
η	O
(	O
x	O
)	O
)	O
2dµ	O
(	O
y|x	O
)	O
.	O
z	O
(	O
cid:0	O
)	O
y	O
−	O
f	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
2	O
z	O
(	O
cid:0	O
)	O
η	O
(	O
x	O
)	O
−	O
f	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
2	O
σ2	O
(	O
x	O
)	O
dµ	O
(	O
x	O
)	O
,	O
dµ	O
(	O
x	O
,	O
y	O
)	O
=	O
dµ	O
(	O
x	O
)	O
+	O
(	O
7.3	O
)	O
z	O
as	O
the	O
cross	O
term	O
vanishes	O
due	O
to	O
the	O
deﬁnition	O
of	O
η	O
(	O
x	O
)	O
.	O
as	O
the	O
second	O
term	O
on	O
the	O
right	O
hand	O
side	O
of	O
eq	O
.	O
(	O
7.3	O
)	O
is	O
independent	O
of	O
f	O
,	O
an	O
idealization	O
of	O
the	O
regression	B
problem	O
consists	O
of	O
minimizing	O
the	O
functional	B
z	O
(	O
cid:0	O
)	O
η	O
(	O
x	O
)	O
−	O
f	O
(	O
x	O
)	O
(	O
cid:1	O
)	O
2	O
jµ	O
[	O
f	O
]	O
=	O
n	O
2σ2	O
n	O
dµ	O
(	O
x	O
)	O
+	O
kfk2h	O
.	O
1	O
2	O
(	O
7.4	O
)	O
the	O
form	O
of	O
the	O
minimizing	O
solution	O
is	O
most	O
easily	O
understood	O
in	O
terms	O
of	O
the	O
eigenfunctions	O
{	O
φi	O
(	O
x	O
)	O
}	O
of	O
the	O
kernel	B
k	O
w.r.t	O
.	O
to	O
µ	O
(	O
x	O
)	O
,	O
wherer	O
φi	O
(	O
x	O
)	O
φj	O
(	O
x	O
)	O
dµ	O
(	O
x	O
)	O
=	O
form	O
a	O
complete	O
orthonormal	O
basis	O
,	O
we	O
write	O
f	O
(	O
x	O
)	O
=p∞	O
i=1	O
ηiφi	O
(	O
x	O
)	O
,	O
where	O
ηi	O
=r	O
η	O
(	O
x	O
)	O
φi	O
(	O
x	O
)	O
dµ	O
(	O
x	O
)	O
.	O
thus	O
η	O
(	O
x	O
)	O
=p∞	O
∞x	O
δij	O
,	O
see	B
section	O
4.3.	O
assuming	O
that	O
the	O
kernel	B
is	O
nondegenerate	B
so	O
that	O
the	O
φs	O
i=1	O
fiφi	O
(	O
x	O
)	O
.	O
similarly	O
,	O
∞x	O
jµ	O
[	O
f	O
]	O
=	O
n	O
2σ2	O
n	O
i=1	O
(	O
ηi	O
−	O
fi	O
)	O
2	O
+	O
1	O
2	O
i=1	O
f	O
2	O
i	O
λi	O
.	O
(	O
7.5	O
)	O
this	O
is	O
readily	O
minimized	O
by	O
diﬀerentiation	O
w.r.t	O
.	O
each	O
fi	O
to	O
obtain	O
fi	O
=	O
λi	O
λi	O
+	O
σ2	O
n/n	O
ηi	O
.	O
(	O
7.6	O
)	O
the	O
generalized	B
fourier	O
seriesp∞	O
n/n	O
→	O
0	O
as	O
n	O
→	O
∞	O
so	O
that	O
in	O
this	O
limit	O
we	O
would	O
notice	O
that	O
the	O
term	O
σ2	O
expect	O
that	O
f	O
(	O
x	O
)	O
will	O
converge	O
to	O
η	O
(	O
x	O
)	O
.	O
there	O
are	O
two	O
caveats	O
:	O
(	O
1	O
)	O
we	O
have	O
assumed	O
that	O
η	O
(	O
x	O
)	O
is	O
suﬃciently	O
well-behaved	O
so	O
that	O
it	O
can	O
be	O
represented	O
by	O
i=1	O
ηiφi	O
(	O
x	O
)	O
,	O
and	O
(	O
2	O
)	O
we	O
assumed	O
that	O
the	O
kernel	B
is	O
nondegenerate	B
.	O
if	O
the	O
kernel	B
is	O
degenerate	B
(	O
e.g	O
.	O
a	O
polynomial	B
kernel	O
)	O
then	O
f	O
should	O
converge	O
to	O
the	O
best	O
µ-weighted	O
l2	O
approximation	O
to	O
η	O
within	O
the	O
span	O
of	O
the	O
φ	O
’	O
s	O
.	O
in	O
section	O
7.2.1	O
we	O
will	O
say	O
more	O
about	O
rates	O
of	O
convergence	O
of	O
f	O
to	O
η	O
;	O
clearly	O
in	O
general	O
this	O
will	O
depend	O
on	O
the	O
smoothness	O
of	O
η	O
,	O
the	O
kernel	B
k	O
and	O
the	O
measure	B
µ	O
(	O
x	O
,	O
y	O
)	O
.	O
from	O
a	O
bayesian	O
perspective	O
what	O
is	O
happening	O
is	O
that	O
the	O
prior	O
on	O
f	O
is	O
being	O
overwhelmed	O
by	O
the	O
data	O
as	O
n	O
→	O
∞	O
.	O
looking	O
at	O
eq	O
.	O
(	O
7.6	O
)	O
we	O
also	O
see	B
n	O
(	O
cid:29	O
)	O
nλi	O
then	O
fi	O
is	O
eﬀectively	O
zero	O
.	O
this	O
means	O
that	O
we	O
can	O
not	O
ﬁnd	O
that	O
if	O
σ2	O
out	O
about	O
the	O
coeﬃcients	O
of	O
eigenfunctions	O
with	O
small	O
eigenvalues	O
until	O
we	O
get	O
suﬃcient	O
amounts	O
of	O
data	O
.	O
ferrari	O
trecate	O
et	O
al	O
.	O
[	O
1999	O
]	O
demonstrated	O
this	O
by	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
7.1	O
the	O
equivalent	B
kernel	I
153	O
showing	O
that	O
regression	B
performance	O
of	O
a	O
certain	O
nondegenerate	B
gp	O
could	O
be	O
approximated	O
by	O
taking	O
the	O
ﬁrst	O
m	O
eigenfunctions	O
,	O
where	O
m	O
was	O
chosen	O
so	O
that	O
λm	O
’	O
σ2	O
using	O
the	O
fact	O
that	O
ηi	O
=	O
r	O
η	O
(	O
x0	O
)	O
φi	O
(	O
x0	O
)	O
dµ	O
(	O
x0	O
)	O
and	O
deﬁning	O
σ2	O
n/n	O
.	O
of	O
course	O
as	O
more	O
data	O
is	O
obtained	O
then	O
m	O
has	O
to	O
be	O
increased	O
.	O
,	O
σ2	O
n/n	O
we	O
eﬀ	O
obtain	O
∞x	O
i=1	O
z	O
h	O
∞x	O
i=1	O
i	O
λiφi	O
(	O
x	O
)	O
φi	O
(	O
x0	O
)	O
λi	O
+	O
σ2	O
eﬀ	O
z	O
f	O
(	O
x	O
)	O
=	O
λiηi	O
λi	O
+	O
σ2	O
eﬀ	O
φi	O
(	O
x	O
)	O
=	O
η	O
(	O
x0	O
)	O
dµ	O
(	O
x0	O
)	O
.	O
(	O
7.7	O
)	O
with	O
¯f	O
(	O
x0	O
)	O
=	O
r	O
hn	O
(	O
x	O
,	O
x0	O
)	O
y	O
(	O
x	O
)	O
dµ	O
(	O
x	O
)	O
.	O
notice	O
that	O
in	O
the	O
limit	O
n	O
→	O
∞	O
(	O
so	O
that	O
the	O
term	O
in	O
square	O
brackets	O
in	O
eq	O
.	O
(	O
7.7	O
)	O
is	O
the	O
equivalent	B
kernel	I
for	O
the	O
smooth-	O
ing	O
problem	O
;	O
we	O
denote	O
it	O
by	O
hn	O
(	O
x	O
,	O
x0	O
)	O
.	O
notice	O
the	O
similarity	O
to	O
the	O
vector-valued	O
weight	B
function	I
h	O
(	O
x	O
)	O
deﬁned	O
in	O
section	O
2.6.	O
the	O
diﬀerence	O
is	O
that	O
there	O
the	O
pre-	O
diction	O
was	O
obtained	O
as	O
a	O
linear	B
combination	O
of	O
a	O
ﬁnite	O
number	O
of	O
observations	O
yi	O
with	O
weights	O
given	O
by	O
hi	O
(	O
x	O
)	O
while	O
here	O
we	O
have	O
a	O
noisy	O
function	B
y	O
(	O
x	O
)	O
instead	O
,	O
eﬀ	O
→	O
0	O
)	O
the	O
equivalent	B
kernel	I
tends	O
towards	O
the	O
delta	O
function	B
.	O
σ2	O
the	O
form	O
of	O
the	O
equivalent	B
kernel	I
given	O
in	O
eq	O
.	O
(	O
7.7	O
)	O
is	O
not	O
very	O
useful	O
in	O
practice	O
as	O
it	O
requires	O
knowledge	O
of	O
the	O
eigenvalues/functions	O
for	O
the	O
combina-	O
tion	O
of	O
k	O
and	O
µ.	O
however	O
,	O
in	O
the	O
case	O
of	O
stationary	O
kernels	O
we	O
can	O
use	O
fourier	O
methods	O
to	O
compute	O
the	O
equivalent	B
kernel	I
.	O
consider	O
the	O
functional	B
jρ	O
[	O
f	O
]	O
=	O
ρ	O
2σ2	O
n	O
(	O
y	O
(	O
x	O
)	O
−	O
f	O
(	O
x	O
)	O
)	O
2	O
dx	O
+	O
kfk2h	O
,	O
1	O
2	O
(	O
7.8	O
)	O
where	O
ρ	O
has	O
dimensions	O
of	O
the	O
number	O
of	O
observations	O
per	O
unit	O
of	O
x-space	O
(	O
length/area/volume	O
etc	O
.	O
as	O
appropriate	O
)	O
.	O
using	O
a	O
derivation	O
similar	O
to	O
eq	O
.	O
(	O
7.6	O
)	O
we	O
obtain	O
˜h	O
(	O
s	O
)	O
=	O
sf	O
(	O
s	O
)	O
sf	O
(	O
s	O
)	O
+	O
σ2	O
n/ρ	O
=	O
1	O
1	O
+	O
s−1	O
f	O
(	O
s	O
)	O
σ2	O
n/ρ	O
,	O
(	O
7.9	O
)	O
where	O
sf	O
(	O
s	O
)	O
is	O
the	O
power	O
spectrum	O
of	O
the	O
kernel	B
k.	O
the	O
term	O
σ2	O
n/ρ	O
corresponds	O
to	O
the	O
power	O
spectrum	O
of	O
a	O
white	O
noise	O
process	O
,	O
as	O
the	O
delta	O
function	B
covari-	O
ance	O
function	B
of	O
white	O
noise	O
corresponds	O
to	O
a	O
constant	O
in	O
the	O
fourier	O
domain	O
.	O
this	O
analysis	O
is	O
known	O
as	O
wiener	O
ﬁltering	O
;	O
see	B
,	O
e.g	O
.	O
papoulis	O
[	O
1991	O
,	O
sec	O
.	O
14-1	O
]	O
.	O
equation	O
(	O
7.9	O
)	O
is	O
the	O
same	O
as	O
eq	O
.	O
(	O
7.6	O
)	O
except	O
that	O
the	O
discrete	O
eigenspectrum	O
has	O
been	O
replaced	O
by	O
a	O
continuous	O
one	O
.	O
as	O
can	O
be	O
observed	O
in	O
figure	O
2.6	O
,	O
the	O
equivalent	B
kernel	I
essentially	O
gives	O
a	O
weighting	O
to	O
the	O
observations	O
locally	O
around	O
x.	O
thus	O
identifying	O
ρ	O
with	O
np	O
(	O
x	O
)	O
we	O
can	O
obtain	O
an	O
approximation	O
to	O
the	O
equivalent	B
kernel	I
for	O
stationary	O
kernels	O
when	O
the	O
width	O
of	O
the	O
kernel	B
is	O
smaller	O
than	O
the	O
length-scale	B
of	O
variations	O
in	O
p	O
(	O
x	O
)	O
.	O
this	O
form	O
of	O
analysis	O
was	O
used	O
by	O
silverman	O
[	O
1984	O
]	O
for	O
splines	B
in	O
one	O
dimension	O
.	O
7.1.1	O
some	O
speciﬁc	O
examples	O
of	O
equivalent	B
kernels	O
we	O
ﬁrst	O
consider	O
the	O
ou	O
process	B
in	O
1-d.	O
this	O
has	O
k	O
(	O
r	O
)	O
=	O
exp	O
(	O
−α|r|	O
)	O
(	O
setting	O
α	O
=	O
1/	O
‘	O
relative	O
to	O
our	O
previous	O
notation	O
and	O
r	O
=	O
x	O
−	O
x0	O
)	O
,	O
and	O
power	O
spectrum	O
equivalent	B
kernel	I
wiener	O
ﬁltering	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
154	O
theoretical	O
perspectives	O
s	O
(	O
s	O
)	O
=	O
2α/	O
(	O
4π2s2	O
+	O
α2	O
)	O
.	O
let	O
vn	O
,	O
σ2	O
n/ρ	O
.	O
using	O
eq	O
.	O
(	O
7.9	O
)	O
we	O
obtain	O
˜h	O
(	O
s	O
)	O
=	O
2α	O
vn	O
(	O
4π2s2	O
+	O
β2	O
)	O
,	O
(	O
7.10	O
)	O
where	O
β2	O
=	O
α2	O
+	O
2α/vn	O
.	O
this	O
again	O
has	O
the	O
form	O
of	O
fourier	O
transform	O
of	O
an	O
vnβ	O
e−β|r|	O
.	O
in	O
ou	O
covariance	B
function1	O
and	O
can	O
be	O
inverted	O
to	O
obtain	O
h	O
(	O
r	O
)	O
=	O
α	O
particular	O
notice	O
that	O
as	O
n	O
increases	O
(	O
and	O
thus	O
vn	O
decreases	O
)	O
the	O
inverse	O
length-	O
scale	O
β	O
of	O
h	O
(	O
r	O
)	O
increases	O
;	O
asymptotically	O
β	O
∼	O
n1/2	O
for	O
large	O
n.	O
this	O
shows	O
that	O
the	O
width	O
of	O
equivalent	B
kernel	I
for	O
the	O
ou	O
covariance	B
function	I
will	O
scale	O
as	O
n−1/2	O
asymptotically	O
.	O
similarly	O
the	O
width	O
will	O
scale	O
as	O
p	O
(	O
x	O
)	O
−1/2	O
asymptotically	O
.	O
a	O
similar	O
analysis	O
can	O
be	O
carried	O
out	O
for	O
the	O
ar	O
(	O
2	O
)	O
gaussian	O
process	B
in	O
1-d	O
(	O
see	B
section	O
b.2	O
)	O
which	O
has	O
a	O
power	O
spectrum	O
∝	O
(	O
4π2s2	O
+	O
α2	O
)	O
−2	O
(	O
i.e	O
.	O
it	O
is	O
in	O
the	O
mat´ern	O
class	O
with	O
ν	O
=	O
3/2	O
)	O
.	O
in	O
this	O
case	O
we	O
can	O
show	O
(	O
using	O
the	O
fourier	O
relationships	O
given	O
by	O
papoulis	O
[	O
1991	O
,	O
p.	O
326	O
]	O
)	O
that	O
the	O
width	O
of	O
the	O
equivalent	B
kernel	I
scales	O
as	O
n−1/4	O
asymptotically	O
.	O
analyzes	O
end-eﬀects	O
if	O
the	O
domain	O
of	O
interest	O
is	O
a	O
bounded	O
open	O
interval	O
.	O
for	O
analysis	O
of	O
the	O
equivalent	B
kernel	I
has	O
also	O
been	O
carried	O
out	O
for	O
spline	O
models	O
.	O
silverman	O
[	O
1984	O
]	O
gives	O
the	O
explicit	O
form	O
of	O
the	O
equivalent	B
kernel	I
in	O
the	O
case	O
of	O
a	O
one-dimensional	O
cubic	O
spline	O
(	O
corresponding	O
to	O
the	O
regularizer	O
kp	O
fk2	O
=	O
00	O
)	O
2dx	O
)	O
.	O
thomas-agnan	O
[	O
1996	O
]	O
gives	O
a	O
general	O
expression	O
for	O
the	O
equivalent	B
r	O
(	O
f	O
kernel	B
for	O
the	O
spline	O
regularizer	O
kp	O
fk2	O
=r	O
(	O
f	O
(	O
m	O
)	O
)	O
2dx	O
in	O
one	O
dimension	O
and	O
also	O
the	O
regularizer	O
kp	O
fk2	O
=r	O
(	O
∇2f	O
)	O
2dx	O
in	O
two	O
dimensions	O
,	O
the	O
equivalent	B
kernel	I
is	O
sponding	O
to	O
a	O
roughness	O
penalty	O
of	O
r	O
(	O
f	O
(	O
m	O
)	O
)	O
2	O
dx	O
)	O
the	O
width	O
of	O
the	O
equivalent	B
silverman	O
[	O
1984	O
]	O
has	O
also	O
shown	O
that	O
for	O
splines	B
of	O
order	O
m	O
in	O
1-d	O
(	O
corre-	O
kernel	B
will	O
scale	O
as	O
n−1/2m	O
asymptotically	O
.	O
in	O
fact	O
it	O
can	O
be	O
shown	O
that	O
this	O
is	O
true	O
for	O
splines	B
in	O
d	O
>	O
1	O
dimensions	O
too	O
,	O
see	B
exercise	O
7.7.1.	O
given	O
in	O
terms	O
of	O
the	O
kelvin	O
function	B
kei	O
(	O
poggio	O
et	O
al	O
.	O
1985	O
,	O
stein	O
1991	O
)	O
.	O
another	O
interesting	O
case	O
to	O
consider	O
is	O
the	O
squared	B
exponential	I
kernel	O
,	O
where	O
s	O
(	O
s	O
)	O
=	O
(	O
2π	O
‘	O
2	O
)	O
d/2	O
exp	O
(	O
−2π2	O
‘	O
2|s|2	O
)	O
.	O
thus	O
˜hse	O
(	O
s	O
)	O
=	O
1	O
1	O
+	O
b	O
exp	O
(	O
2π2	O
‘	O
2|s|2	O
)	O
,	O
(	O
7.11	O
)	O
where	O
b	O
=	O
σ2	O
n/ρ	O
(	O
2π	O
‘	O
2	O
)	O
d/2	O
.	O
we	O
are	O
unaware	O
of	O
an	O
exact	O
result	O
in	O
this	O
case	O
,	O
but	O
the	O
following	O
approximation	O
due	O
to	O
sollich	O
and	O
williams	O
[	O
2005	O
]	O
is	O
simple	O
but	O
eﬀective	O
.	O
for	O
large	O
ρ	O
(	O
i.e	O
.	O
large	O
n	O
)	O
b	O
will	O
be	O
small	O
.	O
thus	O
for	O
small	O
s	O
=	O
|s|	O
we	O
have	O
that	O
˜hse	O
’	O
1	O
,	O
but	O
for	O
large	O
s	O
it	O
is	O
approximately	O
0.	O
the	O
change	O
takes	O
place	O
around	O
the	O
point	O
sc	O
where	O
b	O
exp	O
(	O
2π2	O
‘	O
2s2	O
c	O
=	O
log	O
(	O
1/b	O
)	O
/2π2	O
‘	O
2	O
.	O
as	O
exp	O
(	O
2π2	O
‘	O
2s2	O
)	O
grows	O
quickly	O
with	O
s	O
,	O
the	O
transition	O
of	O
˜hse	O
between	O
1	O
and	O
0	O
can	O
be	O
expected	O
to	O
be	O
rapid	O
,	O
and	O
thus	O
be	O
well-approximated	O
by	O
a	O
step	O
function	B
.	O
by	O
using	O
the	O
standard	O
result	O
for	O
the	O
fourier	O
transform	O
of	O
the	O
step	O
function	B
we	O
obtain	O
c	O
)	O
=	O
1	O
,	O
i.e	O
.	O
s2	O
hse	O
(	O
x	O
)	O
=	O
2scsinc	O
(	O
2πscx	O
)	O
(	O
7.12	O
)	O
1the	O
fact	O
that	O
˜h	O
(	O
s	O
)	O
has	O
the	O
same	O
form	O
as	O
sf	O
(	O
s	O
)	O
is	O
particular	O
to	O
the	O
ou	O
covariance	B
function	I
and	O
is	O
not	O
generally	O
the	O
case	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
7.2	O
asymptotic	O
analysis	O
155	O
for	O
d	O
=	O
1	O
,	O
where	O
sinc	O
(	O
z	O
)	O
=	O
sin	O
(	O
z	O
)	O
/z	O
.	O
a	O
similar	O
calculation	O
in	O
d	O
>	O
1	O
using	O
eq	O
.	O
(	O
4.7	O
)	O
gives	O
(	O
cid:16	O
)	O
sc	O
(	O
cid:17	O
)	O
d/2	O
hse	O
(	O
r	O
)	O
=	O
r	O
jd/2	O
(	O
2πscr	O
)	O
.	O
(	O
7.13	O
)	O
notice	O
that	O
sc	O
scales	O
as	O
(	O
log	O
(	O
n	O
)	O
)	O
1/2	O
so	O
that	O
the	O
width	O
of	O
the	O
equivalent	B
kernel	I
will	O
decay	O
very	O
slowly	O
as	O
n	O
increases	O
.	O
notice	O
that	O
the	O
plots	O
in	O
figure	O
2.6	O
show	O
the	O
sinc-type	O
shape	O
,	O
although	O
the	O
sidelobes	O
are	O
not	O
quite	O
as	O
large	O
as	O
would	O
be	O
predicted	O
by	O
the	O
sinc	O
curve	O
(	O
because	O
the	O
transition	O
is	O
smoother	O
than	O
a	O
step	O
function	B
in	O
fourier	O
space	O
so	O
there	O
is	O
less	O
“	O
ringing	O
”	O
)	O
.	O
7.2	O
asymptotic	O
analysis	O
∗	O
in	O
this	O
section	O
we	O
consider	O
two	O
asymptotic	O
properties	O
of	O
gaussian	O
processes	O
,	O
consistency	B
and	O
equivalence/orthogonality	O
.	O
7.2.1	O
consistency	B
in	O
section	O
7.1	O
we	O
have	O
analyzed	O
the	O
asymptotics	O
of	O
gp	O
regression	B
and	O
have	O
seen	O
how	O
the	O
minimizer	O
of	O
the	O
functional	B
eq	O
.	O
(	O
7.4	O
)	O
converges	O
to	O
the	O
regression	B
function	O
as	O
n	O
→	O
∞	O
.	O
we	O
now	O
broaden	O
the	O
focus	O
by	O
considering	O
loss	B
functions	O
other	O
than	O
squared	B
loss	O
,	O
and	O
the	O
case	O
where	O
we	O
work	O
directly	O
with	O
eq	O
.	O
(	O
7.1	O
)	O
rather	O
than	O
the	O
smoothed	O
version	O
eq	O
.	O
(	O
7.4	O
)	O
.	O
the	O
set	B
up	O
is	O
as	O
follows	O
:	O
let	O
l	O
(	O
·	O
,	O
·	O
)	O
be	O
a	O
pointwise	O
loss	B
function	I
.	O
consider	O
a	O
procedure	O
that	O
takes	O
training	O
data	O
d	O
and	O
this	O
loss	B
function	I
,	O
and	O
returns	O
a	O
function	B
fd	O
(	O
x	O
)	O
.	O
for	O
a	O
measurable	O
function	B
f	O
,	O
the	O
risk	B
(	O
expected	O
loss	B
)	O
is	O
deﬁned	O
as	O
z	O
rl	O
(	O
f	O
)	O
=	O
l	O
(	O
y	O
,	O
f	O
(	O
x	O
)	O
)	O
dµ	O
(	O
x	O
,	O
y	O
)	O
.	O
(	O
7.14	O
)	O
l	O
denote	O
the	O
function	B
that	O
minimizes	O
this	O
risk	B
.	O
for	O
squared	B
loss	O
f∗	O
let	O
f∗	O
l	O
(	O
x	O
)	O
=	O
e	O
[	O
y|x	O
]	O
.	O
for	O
0/1	O
loss	B
with	O
classiﬁcation	B
problems	O
,	O
we	O
choose	O
f∗	O
l	O
(	O
x	O
)	O
to	O
be	O
the	O
class	O
c	O
at	O
x	O
such	O
that	O
p	O
(	O
cc|x	O
)	O
>	O
p	O
(	O
cj|x	O
)	O
for	O
all	O
j	O
6=	O
c	O
(	O
breaking	O
ties	O
arbitrarily	O
)	O
.	O
deﬁnition	O
7.1	O
we	O
will	O
say	O
that	O
a	O
procedure	O
that	O
returns	O
fd	O
is	O
consistent	O
for	O
a	O
given	O
measure	B
µ	O
(	O
x	O
,	O
y	O
)	O
and	O
loss	B
function	I
l	O
if	O
consistency	B
rl	O
(	O
fd	O
)	O
→	O
rl	O
(	O
f∗	O
l	O
)	O
as	O
n	O
→	O
∞	O
,	O
(	O
7.15	O
)	O
where	O
convergence	O
is	O
assessed	O
in	O
a	O
suitable	O
manner	O
,	O
e.g	O
.	O
in	O
probability	B
.	O
if	O
fd	O
(	O
x	O
)	O
is	O
consistent	O
for	O
all	O
borel	O
probability	B
measures	O
µ	O
(	O
x	O
,	O
y	O
)	O
then	O
it	O
is	O
said	O
to	O
be	O
uni-	O
(	O
cid:3	O
)	O
versally	O
consistent	O
.	O
ing	O
ˆf	O
(	O
x∗	O
)	O
=pn	O
a	O
simple	O
example	O
of	O
a	O
consistent	O
procedure	O
is	O
the	O
kernel	B
regression	O
method	O
.	O
as	O
described	O
in	O
section	O
2.6	O
one	O
obtains	O
a	O
prediction	B
at	O
test	O
point	O
x∗	O
by	O
comput-	O
j=1κj	O
(	O
the	O
nadaraya-watson	O
estima-	O
tor	O
)	O
.	O
let	O
h	O
be	O
the	O
width	O
of	O
the	O
kernel	B
κ	O
and	O
d	O
be	O
the	O
dimension	O
of	O
the	O
input	O
i=1	O
wiyi	O
where	O
wi	O
=	O
κi/pn	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
156	O
theoretical	O
perspectives	O
space	O
.	O
it	O
can	O
be	O
shown	O
that	O
under	O
suitable	O
regularity	O
conditions	O
if	O
h	O
→	O
0	O
and	O
nhd	O
→	O
∞	O
as	O
n	O
→	O
∞	O
then	O
the	O
procedure	O
is	O
consistent	O
;	O
see	B
e.g	O
.	O
[	O
gy¨orﬁ	O
et	O
al.	O
,	O
2002	O
,	O
theorem	O
5.1	O
]	O
for	O
the	O
regression	B
case	O
with	O
squared	B
loss	O
and	O
devroye	O
et	O
al	O
.	O
[	O
1996	O
,	O
theorem	O
10.1	O
]	O
for	O
the	O
classiﬁcation	B
case	O
using	O
0/1	O
loss	B
.	O
an	O
intuitive	O
understanding	O
of	O
this	O
result	O
can	O
be	O
obtained	O
by	O
noting	O
that	O
h	O
→	O
0	O
means	O
that	O
only	O
datapoints	O
very	O
close	O
to	O
x∗	O
will	O
contribute	O
to	O
the	O
prediction	B
(	O
eliminating	O
bias	B
)	O
,	O
while	O
the	O
condition	O
nhd	O
→	O
∞	O
means	O
that	O
a	O
large	O
number	O
of	O
datapoints	O
will	O
contribute	O
to	O
the	O
prediction	B
(	O
eliminating	O
noise/variance	O
)	O
.	O
combination	O
of	O
eigenfunctionsp∞	O
it	O
will	O
ﬁrst	O
be	O
useful	O
to	O
consider	O
why	O
we	O
might	O
hope	O
that	O
gpr	O
and	O
gpc	O
should	O
be	O
universally	O
consistent	O
.	O
as	O
discussed	O
in	O
section	O
7.1	O
,	O
the	O
key	O
property	O
is	O
that	O
a	O
non-degenerate	O
kernel	B
will	O
have	O
an	O
inﬁnite	O
number	O
of	O
eigenfunctions	O
forming	O
an	O
orthonormal	O
set	B
.	O
thus	O
from	O
generalized	B
fourier	O
analysis	O
a	O
linear	B
i=1	O
ciφi	O
(	O
x	O
)	O
should	O
be	O
able	O
to	O
represent	O
a	O
suf-	O
ﬁciently	O
well-behaved	O
target	O
function	B
f∗	O
l.	O
however	O
,	O
we	O
have	O
to	O
estimate	O
the	O
inﬁnite	O
number	O
of	O
coeﬃcients	O
{	O
ci	O
}	O
from	O
the	O
noisy	O
observations	O
.	O
this	O
makes	O
it	O
clear	O
that	O
we	O
are	O
playing	O
a	O
game	O
involving	O
inﬁnities	O
which	O
needs	O
to	O
be	O
played	O
with	O
care	O
,	O
and	O
there	O
are	O
some	O
results	O
[	O
diaconis	O
and	O
freedman	O
,	O
1986	O
,	O
freedman	O
,	O
1999	O
,	O
gr¨unwald	O
and	O
langford	O
,	O
2004	O
]	O
which	O
show	O
that	O
in	O
certain	O
circumstances	O
bayesian	O
inference	O
in	O
inﬁnite-dimensional	O
objects	O
can	O
be	O
inconsistent	O
.	O
however	O
,	O
there	O
are	O
some	O
positive	O
recent	O
results	O
on	O
the	O
consistency	B
of	O
gpr	O
and	O
gpc	O
.	O
choudhuri	O
et	O
al	O
.	O
[	O
2005	O
]	O
show	O
that	O
for	O
the	O
binary	B
classiﬁcation	I
case	O
under	O
certain	O
assumptions	O
gpc	O
is	O
consistent	O
.	O
the	O
assumptions	O
include	O
smooth-	O
ness	O
on	O
the	O
mean	O
and	O
covariance	B
function	I
of	O
the	O
gp	O
,	O
smoothness	O
on	O
e	O
[	O
y|x	O
]	O
and	O
an	O
assumption	O
that	O
the	O
domain	O
is	O
a	O
bounded	O
subset	O
of	O
rd	O
.	O
their	O
result	O
holds	O
for	O
the	O
class	O
of	O
response	O
functions	O
which	O
are	O
c.d.f.s	O
of	O
a	O
unimodal	O
symmetric	O
density	O
;	O
this	O
includes	O
the	O
probit	B
and	O
logistic	B
functions	O
.	O
for	O
gpr	O
,	O
choi	O
and	O
schervish	O
[	O
2004	O
]	O
show	O
that	O
for	O
a	O
one-dimensional	O
input	O
space	O
of	O
ﬁnite	O
length	O
under	O
certain	O
assumptions	O
consistency	B
holds	O
.	O
here	O
the	O
assumptions	O
again	O
include	O
smoothness	O
of	O
the	O
mean	O
and	O
covariance	B
function	I
of	O
the	O
gp	O
and	O
smoothness	O
of	O
e	O
[	O
y|x	O
]	O
.	O
an	O
additional	O
assumption	O
is	O
that	O
the	O
noise	O
has	O
a	O
normal	O
or	O
laplacian	O
distribution	O
(	O
with	O
an	O
unknown	O
variance	O
which	O
is	O
inferred	O
)	O
.	O
there	O
are	O
also	O
some	O
consistency	B
results	O
relating	O
to	O
the	O
functional	B
jλn	O
[	O
f	O
]	O
=	O
λn	O
2	O
kfk2h	O
+	O
1	O
n	O
(	O
7.16	O
)	O
l	O
(	O
cid:0	O
)	O
yi	O
,	O
f	O
(	O
xi	O
)	O
(	O
cid:1	O
)	O
,	O
nx	O
i=1	O
m	O
=pm	O
where	O
λn	O
→	O
0	O
as	O
n	O
→	O
∞	O
.	O
note	O
that	O
to	O
agree	O
with	O
our	O
previous	O
formulations	O
we	O
would	O
set	B
λn	O
=	O
1/n	O
,	O
but	O
other	O
decay	O
rates	O
on	O
λn	O
are	O
often	O
considered	O
.	O
in	O
the	O
splines	B
literature	O
,	O
cox	O
[	O
1984	O
]	O
showed	O
that	O
for	O
regression	B
problems	O
us-	O
ing	O
the	O
regularizer	O
kfk2	O
k=0	O
kokfk2	O
(	O
using	O
the	O
deﬁnitions	O
in	O
eq	O
.	O
(	O
6.10	O
)	O
)	O
consistency	B
can	O
be	O
obtained	O
under	O
certain	O
technical	O
conditions	O
.	O
cox	O
and	O
o	O
’	O
sulli-	O
van	O
[	O
1990	O
]	O
considered	O
a	O
wide	O
range	O
of	O
problems	O
(	O
including	O
regression	B
problems	O
with	O
squared	B
loss	O
and	O
classiﬁcation	B
using	O
logistic	B
loss	O
)	O
where	O
the	O
solution	O
is	O
obtained	O
by	O
minimizing	O
the	O
regularized	O
risk	B
using	O
a	O
spline	O
smoothness	O
term	O
.	O
l	O
∈	O
h	O
(	O
where	O
h	O
is	O
the	O
rkhs	O
corresponding	O
to	O
the	O
spline	O
they	O
showed	O
that	O
if	O
f∗	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
7.2	O
asymptotic	O
analysis	O
157	O
regularizer	O
)	O
then	O
as	O
n	O
→	O
∞	O
and	O
λn	O
→	O
0	O
at	O
an	O
appropriate	O
rate	O
,	O
one	O
gets	O
convergence	O
of	O
fd	O
to	O
f∗	O
l.	O
more	O
recently	O
,	O
zhang	O
[	O
2004	O
,	O
theorem	O
4.4	O
]	O
has	O
shown	O
that	O
for	O
the	O
classiﬁca-	O
tion	O
problem	O
with	O
a	O
number	O
of	O
diﬀerent	O
loss	B
functions	O
(	O
including	O
logistic	B
loss	O
,	O
hinge	B
loss	O
and	O
quadratic	O
loss	O
)	O
and	O
for	O
general	O
rkhss	O
with	O
a	O
nondegenerate	B
kernel	O
,	O
that	O
if	O
λn	O
→	O
0	O
,	O
λnn	O
→	O
∞	O
and	O
µ	O
(	O
x	O
,	O
y	O
)	O
is	O
suﬃciently	O
regular	O
then	O
the	O
classiﬁcation	B
error	O
of	O
fd	O
will	O
converge	O
to	O
the	O
bayes	O
optimal	B
error	O
in	O
probability	B
as	O
n	O
→	O
∞	O
.	O
similar	O
results	O
have	O
also	O
been	O
obtained	O
by	O
steinwart	O
[	O
2005	O
]	O
with	O
various	O
rates	O
on	O
the	O
decay	O
of	O
λn	O
depending	O
on	O
the	O
smoothness	O
of	O
the	O
kernel	B
.	O
bartlett	O
et	O
al	O
.	O
[	O
2003	O
]	O
have	O
characterized	O
the	O
loss	B
functions	O
that	O
lead	O
to	O
universal	O
consistency	B
.	O
above	O
we	O
have	O
focussed	O
on	O
regression	B
and	O
classiﬁcation	B
problems	O
.	O
however	O
,	O
similar	O
analyses	O
can	O
also	O
be	O
given	O
for	O
other	O
problems	O
such	O
as	O
density	O
estimation	O
and	O
deconvolution	O
;	O
see	B
wahba	O
[	O
1990	O
,	O
chs	O
.	O
8	O
,	O
9	O
]	O
for	O
references	O
.	O
also	O
we	O
have	O
discussed	O
consistency	B
using	O
a	O
ﬁxed	O
decay	O
rate	O
for	O
λn	O
.	O
however	O
,	O
it	O
is	O
also	O
possible	O
to	O
analyze	O
the	O
asymptotics	O
of	O
methods	O
where	O
λn	O
is	O
set	B
in	O
a	O
data-dependent	O
way	O
,	O
e.g	O
.	O
by	O
cross-validation	B
;	O
2	O
see	B
wahba	O
[	O
1990	O
,	O
sec	O
.	O
4.5	O
]	O
and	O
references	O
therein	O
for	O
further	O
details	O
.	O
consistency	B
is	O
evidently	O
a	O
desirable	O
property	O
of	O
supervised	B
learning	I
proce-	O
dures	O
.	O
however	O
,	O
it	O
is	O
an	O
asymptotic	O
property	O
that	O
does	O
not	O
say	O
very	O
much	O
about	O
how	O
a	O
given	O
prediction	B
procedure	O
will	O
perform	O
on	O
a	O
particular	O
problem	O
with	O
a	O
given	O
dataset	B
.	O
for	O
instance	O
,	O
note	O
that	O
we	O
only	O
required	O
rather	O
general	O
prop-	O
erties	O
of	O
the	O
kernel	B
function	O
(	O
e.g	O
.	O
non-degeneracy	O
)	O
for	O
some	O
of	O
the	O
consistency	B
results	O
.	O
however	O
,	O
the	O
choice	O
of	O
the	O
kernel	B
can	O
make	O
a	O
huge	O
diﬀerence	O
to	O
how	O
a	O
procedure	O
performs	O
in	O
practice	O
.	O
some	O
analyses	O
related	O
to	O
this	O
issue	O
are	O
given	O
in	O
section	O
7.3	O
.	O
7.2.2	O
equivalence	O
and	O
orthogonality	O
the	O
presentation	O
in	O
this	O
section	O
is	O
based	O
mainly	O
on	O
stein	O
[	O
1999	O
,	O
ch	O
.	O
4	O
]	O
.	O
for	O
two	O
probability	B
measures	O
µ0	O
and	O
µ1	O
deﬁned	O
on	O
a	O
measurable	O
space	O
(	O
ω	O
,	O
f	O
)	O
,3	O
µ0	O
is	O
said	O
to	O
be	O
absolutely	O
continuous	O
w.r.t	O
.	O
µ1	O
if	O
for	O
all	O
a	O
∈	O
f	O
,	O
µ1	O
(	O
a	O
)	O
=	O
0	O
implies	O
µ0	O
(	O
a	O
)	O
=	O
0.	O
if	O
µ0	O
is	O
absolutely	O
continuous	O
w.r.t	O
.	O
µ1	O
and	O
µ1	O
is	O
absolutely	O
continuous	O
w.r.t	O
.	O
µ0	O
the	O
two	O
measures	O
are	O
said	O
to	O
be	O
equivalent	B
,	O
written	O
µ0	O
≡	O
µ1	O
.	O
µ0	O
and	O
µ1	O
are	O
said	O
to	O
be	O
orthogonal	O
,	O
written	O
µ0	O
⊥	O
µ1	O
,	O
if	O
there	O
exists	O
an	O
a	O
∈	O
f	O
such	O
that	O
µ0	O
(	O
a	O
)	O
=	O
1	O
and	O
µ1	O
(	O
a	O
)	O
=	O
0	O
.	O
(	O
note	O
that	O
in	O
this	O
case	O
we	O
have	O
µ0	O
(	O
ac	O
)	O
=	O
0	O
and	O
µ1	O
(	O
ac	O
)	O
=	O
1	O
,	O
where	O
ac	O
is	O
the	O
complement	O
of	O
a	O
.	O
)	O
the	O
dichotomy	O
theorem	O
for	O
gaussian	O
processes	O
(	O
due	O
to	O
hajek	O
[	O
1958	O
]	O
and	O
,	O
independently	O
,	O
feldman	O
[	O
1958	O
]	O
)	O
states	O
that	O
two	O
gaussian	O
processes	O
are	O
either	O
equivalent	B
or	O
orthogonal	O
.	O
equivalence	O
and	O
orthogonality	O
for	O
gaussian	O
measures	O
µ0	O
,	O
µ1	O
with	O
corre-	O
sponding	O
probability	B
densities	O
p0	O
,	O
p1	O
,	O
can	O
be	O
characterized	O
in	O
terms	O
of	O
the	O
2cross	O
validation	O
is	O
discussed	O
in	O
section	O
5.3	O
.	O
3see	O
section	O
a.7	O
for	O
background	O
on	O
measurable	O
spaces	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
158	O
theoretical	O
perspectives	O
z	O
symmetrized	O
kullback-leibler	O
divergence	O
klsym	O
between	O
them	O
,	O
given	O
by	O
klsym	O
(	O
p0	O
,	O
p1	O
)	O
=	O
(	O
7.17	O
)	O
the	O
measures	O
are	O
equivalent	B
if	O
klsym	O
<	O
∞	O
and	O
orthogonal	O
otherwise	O
.	O
for	O
two	O
ﬁnite-dimensional	O
gaussian	O
distributions	O
n	O
(	O
µ0	O
,	O
k0	O
)	O
and	O
n	O
(	O
µ1	O
,	O
k1	O
)	O
we	O
have	O
[	O
kullback	O
,	O
1959	O
,	O
sec	O
.	O
9.1	O
]	O
p1	O
(	O
f	O
)	O
df	O
.	O
(	O
p0	O
(	O
f	O
)	O
−	O
p1	O
(	O
f	O
)	O
)	O
log	O
p0	O
(	O
f	O
)	O
klsym	O
=	O
1	O
+	O
1	O
2	O
tr	O
(	O
k0	O
−	O
k1	O
)	O
(	O
k−1	O
2	O
tr	O
(	O
k−1	O
1	O
+	O
k−1	O
1	O
−	O
k−1	O
0	O
)	O
0	O
)	O
(	O
µ0	O
−	O
µ1	O
)	O
(	O
µ0	O
−	O
µ1	O
)	O
>	O
.	O
(	O
7.18	O
)	O
this	O
expression	O
can	O
be	O
simpliﬁed	O
considerably	O
by	O
simultaneously	O
diagonalizing	O
k0	O
and	O
k1	O
.	O
two	O
ﬁnite-dimensional	O
gaussian	O
distributions	O
are	O
equivalent	B
if	O
the	O
null	O
spaces	O
of	O
their	O
covariance	B
matrices	O
coincide	O
,	O
and	O
are	O
orthogonal	O
otherwise	O
.	O
things	O
can	O
get	O
more	O
interesting	O
if	O
we	O
consider	O
inﬁnite-dimensional	O
distribu-	O
tions	O
,	O
i.e	O
.	O
gaussian	O
processes	O
.	O
consider	O
some	O
closed	O
subset	O
r	O
∈	O
rd	O
.	O
choose	O
some	O
ﬁnite	O
number	O
n	O
of	O
x-points	O
in	O
r	O
and	O
let	O
f	O
=	O
(	O
f1	O
,	O
.	O
.	O
.	O
,	O
fn	O
)	O
>	O
denote	O
the	O
values	O
corresponding	O
to	O
these	O
inputs	O
.	O
we	O
consider	O
the	O
klsym-divergence	O
as	O
above	O
,	O
but	O
in	O
the	O
limit	O
n	O
→	O
∞	O
.	O
klsym	O
can	O
now	O
diverge	O
if	O
the	O
rates	O
of	O
decay	O
of	O
the	O
eigenvalues	O
of	O
the	O
two	O
processes	O
are	O
not	O
the	O
same	O
.	O
for	O
example	O
,	O
consider	O
zero-mean	O
periodic	B
processes	O
with	O
period	O
1	O
where	O
the	O
eigenvalue	B
λi	O
j	O
indicates	O
the	O
amount	O
of	O
power	O
in	O
the	O
sin/cos	O
terms	O
of	O
frequency	O
2πj	O
for	O
process	B
i	O
=	O
0	O
,	O
1.	O
then	O
using	O
eq	O
.	O
(	O
7.18	O
)	O
we	O
have	O
klsym	O
=	O
(	O
λ0	O
0	O
−	O
λ1	O
0	O
)	O
2	O
0λ1	O
λ0	O
0	O
+	O
2	O
(	O
λ0	O
j	O
−	O
λ1	O
j	O
)	O
2	O
λ0	O
j	O
λ1	O
j	O
(	O
7.19	O
)	O
∞x	O
j=1	O
(	O
see	B
also	O
[	O
stein	O
,	O
1999	O
,	O
p.	O
119	O
]	O
)	O
.	O
some	O
corresponding	O
results	O
for	O
the	O
equiva-	O
lence	O
or	O
orthogonality	O
of	O
non-periodic	O
gaussian	O
processes	O
are	O
given	O
in	O
stein	O
[	O
1999	O
,	O
pp	O
.	O
119-122	O
]	O
.	O
stein	O
(	O
p.	O
109	O
)	O
gives	O
an	O
example	O
of	O
two	O
equivalent	B
gaussian	O
processes	O
on	O
r	O
,	O
those	O
with	O
covariance	B
functions	O
exp	O
(	O
−r	O
)	O
and	O
1/2	O
exp	O
(	O
−2r	O
)	O
.	O
(	O
it	O
is	O
easy	O
to	O
check	O
that	O
for	O
large	O
s	O
these	O
have	O
the	O
same	O
power	O
spectrum	O
.	O
)	O
we	O
now	O
turn	O
to	O
the	O
consequences	O
of	O
equivalence	O
for	O
the	O
model	B
selection	O
problem	O
.	O
suppose	O
that	O
we	O
know	O
that	O
either	O
gp	O
0	O
or	O
gp	O
1	O
is	O
the	O
correct	O
model	B
.	O
then	O
if	O
gp	O
0	O
≡	O
gp	O
1	O
then	O
it	O
is	O
not	O
possible	O
to	O
determine	O
which	O
model	B
is	O
correct	O
with	O
probability	B
1.	O
however	O
,	O
under	O
a	O
bayesian	O
setting	O
all	O
this	O
means	O
is	O
if	O
we	O
have	O
prior	O
probabilities	O
π0	O
and	O
π1	O
=	O
1−	O
π0	O
on	O
these	O
two	O
hypotheses	O
,	O
then	O
after	O
observing	O
some	O
data	O
d	O
the	O
posterior	O
probabilities	O
p	O
(	O
gp	O
i|d	O
)	O
(	O
for	O
i	O
=	O
0	O
,	O
1	O
)	O
will	O
not	O
be	O
0	O
or	O
1	O
,	O
but	O
could	O
be	O
heavily	O
skewed	O
to	O
one	O
model	B
or	O
the	O
other	O
.	O
the	O
other	O
important	O
observation	O
is	O
to	O
consider	O
the	O
predictions	O
made	O
by	O
gp	O
0	O
or	O
gp	O
1.	O
consider	O
the	O
case	O
where	O
gp	O
0	O
is	O
the	O
correct	O
model	B
and	O
gp	O
1	O
≡	O
gp	O
0.	O
then	O
stein	O
[	O
1999	O
,	O
sec	O
.	O
4.3	O
]	O
shows	O
that	O
the	O
predictions	O
of	O
gp	O
1	O
are	O
asymptotically	O
optimal	B
,	O
in	O
the	O
sense	O
that	O
the	O
expected	O
relative	O
prediction	O
error	B
between	O
gp	O
1	O
and	O
gp	O
0	O
tends	O
to	O
0	O
as	O
n	O
→	O
∞	O
under	O
some	O
technical	O
conditions	O
.	O
stein	O
’	O
s	O
corol-	O
lary	O
9	O
(	O
p.	O
132	O
)	O
shows	O
that	O
this	O
conclusion	O
remains	O
true	O
under	O
additive	O
noise	O
if	O
the	O
un-noisy	O
gps	O
are	O
equivalent	B
.	O
one	O
caveat	O
about	O
equivalence	O
is	O
although	O
the	O
predictions	O
of	O
gp	O
1	O
are	O
asymptotically	O
optimal	B
when	O
gp	O
0	O
is	O
the	O
correct	O
model	B
and	O
gp	O
0	O
≡	O
gp	O
1	O
,	O
one	O
would	O
see	B
diﬀering	O
predictions	O
for	O
ﬁnite	O
n.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
7.3	O
average-case	O
learning	B
curves	O
159	O
7.3	O
average-case	O
learning	B
curves	O
∗	O
in	O
section	O
7.2	O
we	O
have	O
discussed	O
the	O
asymptotic	O
properties	O
of	O
gaussian	O
process	B
predictors	O
and	O
related	O
methods	O
.	O
in	O
this	O
section	O
we	O
will	O
say	O
more	O
about	O
the	O
speed	O
of	O
convergence	O
under	O
certain	O
speciﬁc	O
assumptions	O
.	O
our	O
goal	O
will	O
be	O
to	O
obtain	O
a	O
learning	B
curve	I
describing	O
the	O
generalization	B
error	I
as	O
a	O
function	B
of	O
the	O
training	O
set	B
size	O
n.	O
this	O
is	O
an	O
average-case	O
analysis	O
,	O
averaging	O
over	O
the	O
choice	O
of	O
target	O
functions	O
(	O
drawn	O
from	O
a	O
gp	O
)	O
and	O
over	O
the	O
x	O
locations	O
of	O
the	O
training	O
points	O
.	O
in	O
more	O
detail	O
,	O
we	O
ﬁrst	O
consider	O
a	O
target	O
function	B
f	O
drawn	O
from	O
a	O
gaussian	O
process	B
.	O
n	O
locations	O
are	O
chosen	O
to	O
make	O
observations	O
at	O
,	O
giving	O
rise	O
to	O
the	O
train-	O
ing	O
set	B
d	O
=	O
(	O
x	O
,	O
y	O
)	O
.	O
the	O
yis	O
are	O
(	O
possibly	O
)	O
noisy	O
observations	O
of	O
the	O
underlying	O
function	B
f.	O
given	O
a	O
loss	B
function	I
l	O
(	O
·	O
,	O
·	O
)	O
which	O
measures	O
the	O
diﬀerence	O
between	O
the	O
prediction	B
for	O
f	O
and	O
f	O
itself	O
,	O
we	O
obtain	O
an	O
estimator	O
fd	O
for	O
f.	O
below	O
we	O
will	O
use	O
the	O
squared	B
loss	O
,	O
so	O
that	O
the	O
posterior	O
mean	O
¯fd	O
(	O
x	O
)	O
is	O
the	O
estimator	O
.	O
then	O
the	O
generalization	B
error	I
(	O
given	O
f	O
and	O
d	O
)	O
is	O
given	O
by	O
egd	O
(	O
f	O
)	O
=	O
l	O
(	O
f	O
(	O
x∗	O
)	O
,	O
¯fd	O
(	O
x∗	O
)	O
)	O
p	O
(	O
x∗	O
)	O
dx∗	O
.	O
(	O
7.20	O
)	O
generalization	B
error	I
z	O
z	O
as	O
this	O
is	O
an	O
expected	O
loss	B
it	O
is	O
technically	O
a	O
risk	B
,	O
but	O
the	O
term	O
generalization	B
error	I
is	O
commonly	O
used	O
.	O
egd	O
(	O
f	O
)	O
depends	O
on	O
both	O
the	O
choice	O
of	O
f	O
and	O
on	O
x	O
.	O
(	O
note	O
that	O
y	O
depends	O
on	O
the	O
choice	O
of	O
f	O
,	O
and	O
also	O
on	O
the	O
noise	O
,	O
if	O
present	O
.	O
)	O
the	O
ﬁrst	O
level	O
of	O
averaging	O
we	O
consider	O
is	O
over	O
functions	O
f	O
drawn	O
from	O
a	O
gp	O
prior	O
,	O
to	O
obtain	O
eg	O
(	O
x	O
)	O
=	O
egd	O
(	O
f	O
)	O
p	O
(	O
f	O
)	O
df	O
.	O
(	O
7.21	O
)	O
it	O
will	O
turn	O
out	O
that	O
for	O
regression	B
problems	O
with	O
gaussian	O
process	B
priors	O
and	O
predictors	O
this	O
average	O
can	O
be	O
readily	O
calculated	O
.	O
the	O
second	O
level	O
of	O
averaging	O
assumes	O
that	O
the	O
x-locations	O
of	O
the	O
training	O
set	B
are	O
drawn	O
i.i.d	O
.	O
from	O
p	O
(	O
x	O
)	O
to	O
give	O
z	O
eg	O
(	O
n	O
)	O
=	O
eg	O
(	O
x	O
)	O
p	O
(	O
x1	O
)	O
.	O
.	O
.	O
p	O
(	O
xn	O
)	O
dx1	O
.	O
.	O
.	O
dxn	O
.	O
(	O
7.22	O
)	O
a	O
plot	O
of	O
eg	O
(	O
n	O
)	O
against	O
n	O
is	O
known	O
as	O
a	O
learning	B
curve	I
.	O
learning	B
curve	I
rather	O
than	O
averaging	O
over	O
x	O
,	O
an	O
alternative	O
is	O
to	O
minimize	O
eg	O
(	O
x	O
)	O
w.r.t	O
.	O
x.	O
this	O
gives	O
rise	O
to	O
the	O
optimal	B
experimental	I
design	I
problem	O
.	O
we	O
will	O
not	O
say	O
more	O
about	O
this	O
problem	O
here	O
,	O
but	O
it	O
has	O
been	O
subject	O
to	O
a	O
large	O
amount	O
of	O
investigation	O
.	O
an	O
early	O
paper	O
on	O
this	O
subject	O
is	O
by	O
ylvisaker	O
[	O
1975	O
]	O
.	O
these	O
questions	O
have	O
been	O
addressed	O
both	O
in	O
the	O
statistical	O
literature	O
and	O
in	O
theoretical	O
numerical	O
analysis	O
;	O
for	O
the	O
latter	O
area	O
the	O
book	O
by	O
ritter	O
[	O
2000	O
]	O
provides	O
a	O
useful	O
overview	O
.	O
we	O
now	O
proceed	O
to	O
develop	O
the	O
average-case	O
analysis	O
further	O
for	O
the	O
speciﬁc	O
case	O
of	O
gp	O
predictors	O
and	O
gp	O
priors	O
for	O
the	O
regression	B
case	O
using	O
squared	B
loss	O
.	O
let	O
f	O
be	O
drawn	O
from	O
a	O
zero-mean	O
gp	O
with	O
covariance	B
function	I
k0	O
and	O
noise	O
level	O
σ2	O
0.	O
similarly	O
the	O
predictor	O
assumes	O
a	O
zero-mean	O
process	B
,	O
but	O
covariance	B
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
160	O
theoretical	O
perspectives	O
1.	O
at	O
a	O
particular	O
test	O
location	O
x∗	O
,	O
averaging	O
over	O
function	B
k1	O
and	O
noise	O
level	O
σ2	O
f	O
we	O
have	O
e	O
[	O
(	O
f	O
(	O
x∗	O
)	O
−	O
k1	O
(	O
x∗	O
)	O
>	O
k−1	O
1	O
,	O
yy	O
)	O
2	O
]	O
=	O
e	O
[	O
f	O
2	O
(	O
x∗	O
)	O
]	O
−	O
2k1	O
(	O
x∗	O
)	O
>	O
k−1	O
=	O
k0	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
2k1	O
(	O
x∗	O
)	O
>	O
k−1	O
e	O
[	O
f	O
(	O
x∗	O
)	O
y	O
]	O
+	O
k1	O
(	O
x∗	O
)	O
>	O
k−1	O
1	O
,	O
yk0	O
(	O
x∗	O
)	O
+	O
k1	O
(	O
x∗	O
)	O
>	O
k−1	O
1	O
,	O
y	O
1	O
,	O
y	O
e	O
[	O
yy	O
>	O
]	O
k−1	O
1	O
,	O
yk1	O
(	O
x∗	O
)	O
1	O
,	O
y	O
k0	O
,	O
yk−1	O
(	O
7.23	O
)	O
1	O
,	O
yk1	O
(	O
x∗	O
)	O
where	O
ki	O
,	O
y	O
=	O
ki	O
,	O
f	O
+	O
σ2	O
assumed	O
noise	O
.	O
the	O
above	O
expression	O
reduces	O
to	O
k0	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
k0	O
(	O
x∗	O
)	O
>	O
k−1	O
variance	O
of	O
the	O
gp	O
.	O
i	O
for	O
i	O
=	O
0	O
,	O
1	O
,	O
i.e	O
.	O
the	O
covariance	B
matrix	I
including	O
the	O
if	O
k1	O
=	O
k0	O
so	O
that	O
the	O
predictor	O
is	O
correctly	O
speciﬁed	O
then	O
0	O
,	O
yk0	O
(	O
x∗	O
)	O
,	O
the	O
predictive	B
z	O
z	O
averaging	O
the	O
error	B
over	O
p	O
(	O
x∗	O
)	O
we	O
obtain	O
eg	O
(	O
x	O
)	O
=	O
=	O
e	O
[	O
(	O
f	O
(	O
x∗	O
)	O
−	O
k1	O
(	O
x∗	O
)	O
>	O
k−1	O
k0	O
(	O
x∗	O
,	O
x∗	O
)	O
p	O
(	O
x∗	O
)	O
dx∗	O
−	O
2	O
tr	O
k−1	O
1	O
,	O
y	O
k0	O
,	O
yk−1	O
z	O
1	O
,	O
y	O
(	O
cid:16	O
)	O
(	O
cid:16	O
)	O
+	O
tr	O
1	O
,	O
yy	O
)	O
2	O
]	O
p	O
(	O
x∗	O
)	O
dx∗	O
z	O
k1	O
(	O
x∗	O
)	O
k1	O
(	O
x	O
)	O
>	O
p	O
(	O
x∗	O
)	O
dx∗	O
.	O
k−1	O
1	O
,	O
y	O
k0	O
(	O
x∗	O
)	O
k1	O
(	O
x∗	O
)	O
>	O
p	O
(	O
x∗	O
)	O
dx∗	O
(	O
cid:17	O
)	O
(	O
7.24	O
)	O
(	O
cid:17	O
)	O
for	O
some	O
choices	O
of	O
p	O
(	O
x∗	O
)	O
and	O
covariance	B
functions	O
these	O
integrals	B
will	O
be	O
an-	O
alytically	O
tractable	O
,	O
reducing	O
the	O
computation	O
of	O
eg	O
(	O
x	O
)	O
to	O
a	O
n	O
×	O
n	O
matrix	B
computation	O
.	O
to	O
obtain	O
eg	O
(	O
n	O
)	O
we	O
need	O
to	O
perform	O
a	O
ﬁnal	O
level	O
of	O
averaging	O
over	O
x.	O
in	O
general	O
this	O
is	O
diﬃcult	O
even	O
if	O
eg	O
(	O
x	O
)	O
can	O
be	O
computed	O
exactly	O
,	O
but	O
it	O
is	O
some-	O
times	O
possible	O
,	O
e.g	O
.	O
for	O
the	O
noise-free	O
ou	O
process	B
on	O
the	O
real	O
line	O
,	O
see	B
section	O
7.6.	O
k1	O
we	O
use	O
the	O
deﬁnition	O
k	O
(	O
x	O
,	O
x0	O
)	O
=p	O
the	O
form	O
of	O
eg	O
(	O
x	O
)	O
can	O
be	O
simpliﬁed	O
considerably	O
if	O
we	O
express	O
the	O
covari-	O
ance	O
functions	O
in	O
terms	O
of	O
their	O
eigenfunction	B
expansions	O
.	O
in	O
the	O
case	O
that	O
k0	O
=	O
λiφi	O
(	O
x0	O
)	O
.	O
let	O
λ	O
be	O
a	O
diagonal	O
matrix	B
of	O
the	O
eigenvalues	O
and	O
φ	O
be	O
the	O
n	O
×	O
n	O
design	O
matrix	B
,	O
as	O
deﬁned	O
in	O
section	O
2.1.2.	O
then	O
from	O
eq	O
.	O
(	O
7.24	O
)	O
we	O
obtain	O
i	O
λiφi	O
(	O
x	O
)	O
φi	O
(	O
x0	O
)	O
andr	O
k	O
(	O
x	O
,	O
x0	O
)	O
φi	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
dx	O
=	O
eg	O
(	O
x	O
)	O
=	O
tr	O
(	O
λ	O
)	O
−	O
tr	O
(	O
(	O
σ2	O
=	O
tr	O
(	O
λ−1	O
+	O
σ−2	O
ni	O
+	O
φ	O
>	O
λφ	O
)	O
−1φ	O
>	O
λ2φ	O
)	O
n	O
φφ	O
>	O
)	O
−1	O
,	O
(	O
7.25	O
)	O
where	O
the	O
second	O
line	O
follows	O
through	O
the	O
use	O
of	O
the	O
matrix	B
inversion	O
lemma	O
eq	O
.	O
(	O
a.9	O
)	O
(	O
or	O
directly	O
if	O
we	O
use	O
eq	O
.	O
(	O
2.11	O
)	O
)	O
,	O
as	O
shown	O
in	O
sollich	O
[	O
1999	O
]	O
or	O
opper	O
and	O
vivarelli	O
[	O
1999	O
]	O
.	O
using	O
the	O
fact	O
that	O
ex	O
[	O
φφ	O
>	O
]	O
=	O
ni	O
,	O
a	O
na¨ıve	O
approximation	O
would	O
replace	O
φφ	O
>	O
inside	O
the	O
trace	O
with	O
its	O
expectation	O
;	O
in	O
fact	O
opper	O
and	O
vivarelli	O
[	O
1999	O
]	O
showed	O
that	O
this	O
gives	O
a	O
lower	O
bound	O
,	O
so	O
that	O
eg	O
(	O
n	O
)	O
≥	O
tr	O
(	O
λ−1	O
+	O
nσ−2	O
n	O
i	O
)	O
−1	O
=	O
σ2	O
.	O
(	O
7.26	O
)	O
nx	O
λi	O
n	O
+	O
nλi	O
σ2	O
i=1	O
examining	O
the	O
asymptotics	O
of	O
eq	O
.	O
(	O
7.26	O
)	O
,	O
we	O
see	B
that	O
for	O
each	O
eigenvalue	B
where	O
λi	O
(	O
cid:29	O
)	O
σ2	O
n/n	O
onto	O
the	O
bound	O
on	O
the	O
generalization	B
error	I
.	O
as	O
we	O
saw	O
n/n	O
we	O
add	O
σ2	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
7.4	O
pac-bayesian	O
analysis	O
161	O
p∞	O
in	O
section	O
7.1	O
,	O
more	O
eigenfunctions	O
“	O
come	O
into	O
play	O
”	O
as	O
n	O
increases	O
,	O
so	O
the	O
rate	O
of	O
decay	O
of	O
eg	O
(	O
n	O
)	O
is	O
slower	O
than	O
1/n	O
.	O
sollich	O
[	O
1999	O
]	O
derives	O
a	O
number	O
of	O
more	O
accurate	O
approximations	O
to	O
the	O
learning	B
curve	I
than	O
eq	O
.	O
(	O
7.26	O
)	O
.	O
for	O
the	O
noiseless	O
case	O
with	O
k1	O
=	O
k0	O
,	O
there	O
is	O
a	O
simple	O
lower	O
bound	O
eg	O
(	O
n	O
)	O
≥	O
i=n+1	O
λi	O
due	O
to	O
micchelli	O
and	O
wahba	O
[	O
1981	O
]	O
.	O
this	O
bound	O
is	O
obtained	O
by	O
demonstrating	O
that	O
the	O
optimal	B
n	O
pieces	O
of	O
information	O
are	O
the	O
projections	O
of	O
the	O
random	O
function	B
f	O
onto	O
the	O
ﬁrst	O
n	O
eigenfunctions	O
.	O
as	O
observations	O
which	O
simply	O
consist	O
of	O
function	B
evaluations	O
will	O
not	O
in	O
general	O
provide	O
such	O
information	O
this	O
is	O
a	O
lower	O
bound	O
.	O
plaskota	O
[	O
1996	O
]	O
generalized	B
this	O
result	O
to	O
give	O
a	O
bound	O
on	O
the	O
learning	B
curve	I
if	O
the	O
observations	O
are	O
noisy	O
.	O
some	O
asymptotic	O
results	O
for	O
the	O
learning	B
curves	O
are	O
known	O
.	O
for	O
example	O
,	O
in	O
ritter	O
[	O
2000	O
,	O
sec	O
.	O
v.2	O
]	O
covariance	B
functions	O
obeying	O
sacks-ylvisaker	O
conditions4	O
of	O
order	O
r	O
in	O
1-d	O
are	O
considered	O
.	O
he	O
shows	O
that	O
for	O
an	O
optimal	B
sampling	O
of	O
the	O
input	O
space	O
the	O
generalization	B
error	I
goes	O
as	O
o	O
(	O
n−	O
(	O
2r+1	O
)	O
/	O
(	O
2r+2	O
)	O
)	O
for	O
the	O
noisy	O
problem	O
.	O
similar	O
rates	O
can	O
also	O
be	O
found	O
in	O
sollich	O
[	O
2002	O
]	O
for	O
random	O
designs	O
.	O
for	O
the	O
noise-free	O
case	O
ritter	O
[	O
2000	O
,	O
p.	O
103	O
]	O
gives	O
the	O
rate	O
as	O
o	O
(	O
n−	O
(	O
2r+1	O
)	O
)	O
.	O
one	O
can	O
examine	O
the	O
learning	B
curve	I
not	O
only	O
asymptotically	O
but	O
also	O
for	O
small	O
n	O
,	O
where	O
typically	O
the	O
curve	O
has	O
a	O
roughly	O
linear	B
decrease	O
with	O
n.	O
williams	O
and	O
vivarelli	O
[	O
2000	O
]	O
explained	O
this	O
behaviour	O
by	O
observing	O
that	O
the	O
introduction	O
of	O
a	O
datapoint	O
x1	O
reduces	O
the	O
variance	O
locally	O
around	O
x1	O
(	O
assuming	O
a	O
stationary	O
covariance	B
function	I
)	O
.	O
the	O
addition	O
of	O
another	O
datapoint	O
at	O
x2	O
will	O
also	O
create	O
a	O
“	O
hole	O
”	O
there	O
,	O
and	O
so	O
on	O
.	O
with	O
only	O
a	O
small	O
number	O
of	O
datapoints	O
it	O
is	O
likely	O
that	O
these	O
holes	O
will	O
be	O
far	O
apart	O
so	O
their	O
contributions	O
will	O
add	O
,	O
thus	O
explaining	O
the	O
initial	O
linear	B
trend	O
.	O
sollich	O
[	O
2002	O
]	O
has	O
also	O
investigated	O
the	O
mismatched	O
case	O
where	O
k0	O
6=	O
k1	O
.	O
this	O
can	O
give	O
rise	O
to	O
a	O
rich	O
variety	O
of	O
behaviours	O
in	O
the	O
learning	B
curves	O
,	O
includ-	O
ing	O
plateaux	O
.	O
stein	O
[	O
1999	O
,	O
chs	O
.	O
3	O
,	O
4	O
]	O
has	O
also	O
carried	O
out	O
some	O
analysis	O
of	O
the	O
mismatched	O
case	O
.	O
although	O
we	O
have	O
focused	O
on	O
gp	O
regression	B
with	O
squared	B
loss	O
,	O
we	O
note	O
that	O
malzahn	O
and	O
opper	O
[	O
2002	O
]	O
have	O
developed	O
more	O
general	O
techniques	O
that	O
can	O
be	O
used	O
to	O
analyze	O
learning	B
curves	O
for	O
other	O
situations	O
such	O
as	O
gp	O
classiﬁcation	B
.	O
7.4	O
pac-bayesian	O
analysis	O
∗	O
in	O
section	O
7.3	O
we	O
gave	O
an	O
average-case	O
analysis	O
of	O
generalization	B
,	O
taking	O
the	O
average	O
with	O
respect	O
to	O
a	O
gp	O
prior	O
over	O
functions	O
.	O
in	O
this	O
section	O
we	O
present	O
a	O
diﬀerent	O
kind	O
of	O
analysis	O
within	O
the	O
probably	O
approximately	O
correct	O
(	O
pac	O
)	O
framework	O
due	O
to	O
valiant	O
[	O
1984	O
]	O
.	O
seeger	O
[	O
2002	O
;	O
2003	O
]	O
has	O
presented	O
a	O
pac-	O
bayesian	O
analysis	O
of	O
generalization	B
in	O
gaussian	O
process	B
classiﬁers	O
and	O
we	O
get	O
to	O
this	O
in	O
a	O
number	O
of	O
stages	O
;	O
we	O
ﬁrst	O
present	O
an	O
introduction	O
to	O
the	O
pac	O
framework	O
(	O
section	O
7.4.1	O
)	O
,	O
then	O
describe	O
the	O
pac-bayesian	O
approach	O
(	O
section	O
4roughly	O
speaking	O
,	O
a	O
stochastic	O
process	O
which	O
possesses	O
r	O
ms	O
derivatives	O
but	O
not	O
r	O
+	O
1	O
is	O
said	O
to	O
satisfy	O
sacks-ylvisaker	O
conditions	O
of	O
order	O
r	O
;	O
in	O
1-d	O
this	O
gives	O
rise	O
to	O
a	O
spectrum	O
λi	O
∝	O
i−	O
(	O
2r+2	O
)	O
asymptotically	O
.	O
the	O
ou	O
process	B
obeys	O
sacks-ylvisaker	O
conditions	O
of	O
order	O
0.	O
pac	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
162	O
theoretical	O
perspectives	O
7.4.2	O
)	O
and	O
then	O
ﬁnally	O
the	O
application	O
to	O
gp	O
classiﬁcation	B
(	O
section	O
7.4.3	O
)	O
.	O
our	O
presentation	O
is	O
based	O
mainly	O
on	O
seeger	O
[	O
2003	O
]	O
.	O
7.4.1	O
the	O
pac	O
framework	O
consider	O
a	O
ﬁxed	O
measure	B
µ	O
(	O
x	O
,	O
y	O
)	O
.	O
given	O
a	O
loss	B
function	I
l	O
there	O
exists	O
a	O
function	B
η	O
(	O
x	O
)	O
which	O
minimizes	O
the	O
expected	O
risk	B
.	O
by	O
running	O
a	O
learning	B
algorithm	O
on	O
a	O
data	O
set	B
d	O
of	O
size	O
n	O
drawn	O
i.i.d	O
.	O
from	O
µ	O
(	O
x	O
,	O
y	O
)	O
we	O
obtain	O
an	O
estimate	O
fd	O
of	O
η	O
p	O
which	O
attains	O
an	O
expected	O
risk	B
rl	O
(	O
fd	O
)	O
.	O
we	O
are	O
not	O
able	O
to	O
evaluate	O
rl	O
(	O
fd	O
)	O
as	O
p	O
we	O
do	O
not	O
know	O
µ.	O
however	O
,	O
we	O
do	O
have	O
access	O
to	O
the	O
empirical	O
distribution	O
of	O
i	O
δ	O
(	O
x−xi	O
)	O
δ	O
(	O
y−yi	O
)	O
and	O
can	O
compute	O
the	O
empirical	O
the	O
training	O
set	B
ˆµ	O
(	O
x	O
,	O
y	O
)	O
=	O
1	O
n	O
i	O
l	O
(	O
yi	O
,	O
fd	O
(	O
xi	O
)	O
)	O
.	O
because	O
the	O
training	O
set	B
had	O
been	O
used	O
to	O
risk	B
ˆrl	O
(	O
fd	O
)	O
=	O
1	O
compute	O
fd	O
we	O
would	O
expect	O
ˆrl	O
(	O
fd	O
)	O
to	O
underestimate	O
rl	O
(	O
fd	O
)	O
,5	O
and	O
the	O
aim	O
of	O
the	O
pac	O
analysis	O
is	O
to	O
provide	O
a	O
bound	O
on	O
rl	O
(	O
fd	O
)	O
based	O
on	O
ˆrl	O
(	O
fd	O
)	O
.	O
n	O
a	O
pac	O
bound	O
has	O
the	O
following	O
format	O
pd	O
{	O
rl	O
(	O
fd	O
)	O
≤	O
ˆrl	O
(	O
fd	O
)	O
+	O
gap	O
(	O
fd	O
,	O
d	O
,	O
δ	O
)	O
}	O
≥	O
1	O
−	O
δ	O
,	O
(	O
7.27	O
)	O
where	O
pd	O
denotes	O
the	O
probability	B
distribution	O
of	O
datasets	O
drawn	O
i.i.d	O
.	O
from	O
µ	O
(	O
x	O
,	O
y	O
)	O
,	O
and	O
δ	O
∈	O
(	O
0	O
,	O
1	O
)	O
is	O
called	O
the	O
conﬁdence	O
parameter	O
.	O
the	O
bound	O
states	O
that	O
,	O
averaged	B
over	O
draws	O
of	O
the	O
dataset	B
d	O
from	O
µ	O
(	O
x	O
,	O
y	O
)	O
,	O
rl	O
(	O
fd	O
)	O
does	O
not	O
exceed	O
the	O
sum	O
of	O
ˆrl	O
(	O
fd	O
)	O
and	O
the	O
gap	O
term	O
with	O
probability	B
of	O
at	O
least	O
1	O
−	O
δ.	O
the	O
δ	O
accounts	O
for	O
the	O
“	O
probably	O
”	O
in	O
pac	O
,	O
and	O
the	O
“	O
approximately	O
”	O
derives	O
from	O
the	O
fact	O
that	O
the	O
gap	O
term	O
is	O
positive	O
for	O
all	O
n.	O
it	O
is	O
important	O
to	O
note	O
that	O
pac	O
analyses	O
are	O
distribution-free	O
,	O
i.e	O
.	O
eq	O
.	O
(	O
7.27	O
)	O
must	O
hold	O
for	O
any	O
measure	B
µ.	O
there	O
are	O
two	O
kinds	O
of	O
pac	O
bounds	O
,	O
depending	O
on	O
whether	O
gap	O
(	O
fd	O
,	O
d	O
,	O
δ	O
)	O
actually	O
depends	O
on	O
the	O
particular	O
sample	O
d	O
(	O
rather	O
than	O
on	O
simple	O
statistics	O
like	O
n	O
)	O
.	O
bounds	O
that	O
do	O
depend	O
on	O
d	O
are	O
called	O
data	O
dependent	O
,	O
and	O
those	O
that	O
do	O
not	O
are	O
called	O
data	O
independent	O
.	O
the	O
pac-bayesian	O
bounds	O
given	O
below	O
are	O
data	O
dependent	O
.	O
has	O
mean	O
m.	O
an	O
estimate	O
of	O
m	O
is	O
given	O
by	O
the	O
sample	O
mean	O
¯x	O
=	O
p	O
it	O
is	O
important	O
to	O
understand	O
the	O
interpretation	O
of	O
a	O
pac	O
bound	O
and	O
to	O
clarify	O
this	O
we	O
ﬁrst	O
consider	O
a	O
simpler	O
case	O
of	O
statistical	O
inference	O
.	O
we	O
are	O
given	O
a	O
dataset	B
d	O
=	O
{	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
}	O
drawn	O
i.i.d	O
.	O
from	O
a	O
distribution	O
µ	O
(	O
x	O
)	O
that	O
i	O
xi/n	O
.	O
under	O
certain	O
assumptions	O
we	O
can	O
obtain	O
(	O
or	O
put	O
bounds	O
on	O
)	O
the	O
sampling	O
distribution	O
p	O
(	O
¯x|m	O
)	O
which	O
relates	O
to	O
the	O
choice	O
of	O
dataset	B
d.	O
however	O
,	O
if	O
we	O
wish	O
to	O
perform	O
probabilistic	B
inference	O
for	O
m	O
we	O
need	O
to	O
combine	O
p	O
(	O
¯x|m	O
)	O
with	O
a	O
prior	O
distribution	O
p	O
(	O
m	O
)	O
and	O
use	O
bayes	O
’	O
theorem	O
to	O
obtain	O
the	O
posterior.6	O
the	O
situation	O
is	O
similar	O
(	O
although	O
somewhat	O
more	O
complex	O
)	O
for	O
pac	O
bounds	O
as	O
these	O
concern	O
the	O
sampling	O
distribution	O
of	O
the	O
expected	O
and	O
empirical	O
risks	O
of	O
fd	O
w.r.t	O
.	O
d.	O
5it	O
is	O
also	O
possible	O
to	O
consider	O
pac	O
analyses	O
of	O
other	O
empirical	O
quantities	O
such	O
as	O
the	O
cross-validation	B
error	O
(	O
see	B
section	O
5.3	O
)	O
which	O
do	O
not	O
have	O
this	O
bias	B
.	O
6in	O
introductory	O
treatments	O
of	O
frequentist	O
statistics	O
the	O
logical	O
hiatus	O
of	O
going	O
from	O
the	O
sampling	O
distribution	O
to	O
inference	O
on	O
the	O
parameter	O
of	O
interest	O
is	O
often	O
not	O
well	O
explained	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
7.4	O
pac-bayesian	O
analysis	O
163	O
we	O
might	O
wish	O
to	O
make	O
a	O
conditional	B
statement	O
like	O
pd	O
{	O
rl	O
(	O
fd	O
)	O
≤	O
r	O
+	O
gap	O
(	O
fd	O
,	O
d	O
,	O
δ	O
)	O
|	O
ˆrl	O
(	O
fd	O
)	O
=	O
r	O
}	O
≥	O
1	O
−	O
δ	O
,	O
(	O
7.28	O
)	O
where	O
r	O
is	O
a	O
small	O
value	O
,	O
but	O
such	O
a	O
statement	O
can	O
not	O
be	O
inferred	O
directly	O
from	O
the	O
pac	O
bound	O
.	O
this	O
is	O
because	O
the	O
gap	O
might	O
be	O
heavily	O
anti-correlated	O
with	O
ˆrl	O
(	O
fd	O
)	O
so	O
that	O
the	O
gap	O
is	O
large	O
when	O
the	O
empirical	O
risk	B
is	O
small	O
.	O
pac	O
bounds	O
are	O
sometimes	O
used	O
to	O
carry	O
out	O
model	B
selection—given	O
a	O
learn-	O
ing	O
machine	O
which	O
depends	O
on	O
a	O
(	O
discrete	O
or	O
continuous	O
)	O
parameter	O
vector	O
θ	O
,	O
one	O
can	O
seek	O
to	O
minimize	O
the	O
generalization	B
bound	O
as	O
a	O
function	B
of	O
θ.	O
however	O
,	O
this	O
procedure	O
may	O
not	O
be	O
well-justiﬁed	O
if	O
the	O
generalization	B
bounds	O
are	O
loose	O
.	O
let	O
the	O
slack	O
denote	O
the	O
diﬀerence	O
between	O
the	O
value	O
of	O
the	O
bound	O
and	O
the	O
generalization	B
error	I
.	O
the	O
danger	O
of	O
choosing	O
θ	O
to	O
minimize	O
the	O
bound	O
is	O
that	O
if	O
the	O
slack	O
depends	O
on	O
θ	O
then	O
the	O
value	O
of	O
θ	O
that	O
minimizes	O
the	O
bound	O
may	O
be	O
very	O
diﬀerent	O
from	O
the	O
value	O
of	O
θ	O
that	O
minimizes	O
the	O
generalization	B
error	I
.	O
see	B
seeger	O
[	O
2003	O
,	O
sec	O
.	O
2.2.4	O
]	O
for	O
further	O
discussion	O
.	O
7.4.2	O
pac-bayesian	O
analysis	O
we	O
now	O
consider	O
a	O
bayesian	O
set	B
up	O
,	O
with	O
a	O
prior	O
distribution	O
p	O
(	O
w	O
)	O
over	O
the	O
pa-	O
rameters	O
w	O
,	O
and	O
a	O
“	O
posterior	O
”	O
distribution	O
q	O
(	O
w	O
)	O
.	O
(	O
strictly	O
speaking	O
the	O
analysis	O
does	O
not	O
require	O
q	O
(	O
w	O
)	O
to	O
be	O
the	O
posterior	O
distribution	O
,	O
just	O
some	O
other	O
distribu-	O
tion	O
,	O
but	O
in	O
practice	O
we	O
will	O
consider	O
q	O
to	O
be	O
an	O
(	O
approximate	O
)	O
posterior	O
distri-	O
bution	O
.	O
)	O
we	O
also	O
limit	O
our	O
discussion	O
to	O
binary	B
classiﬁcation	I
with	O
labels	O
{	O
−1	O
,	O
1	O
}	O
,	O
although	O
more	O
general	O
cases	O
can	O
be	O
considered	O
,	O
see	B
seeger	O
[	O
2003	O
,	O
sec	O
.	O
3.2.2	O
]	O
.	O
r	O
q	O
(	O
f∗|w	O
,	O
x∗	O
)	O
q	O
(	O
w	O
)	O
dw	O
,	O
and	O
the	O
predictive	B
classiﬁer	O
outputs	B
sgn	O
(	O
q	O
(	O
f∗|x∗	O
)	O
−	O
1/2	O
)	O
.	O
the	O
predictive	B
distribution	O
for	O
f∗	O
at	O
a	O
test	O
point	O
x∗	O
given	O
q	O
(	O
w	O
)	O
is	O
q	O
(	O
f∗|x∗	O
)	O
=	O
the	O
gibbs	O
classiﬁer	B
has	O
also	O
been	O
studied	O
in	O
learning	B
theory	O
;	O
given	O
a	O
test	O
point	O
x∗	O
one	O
draws	O
a	O
sample	O
˜w	O
from	O
q	O
(	O
w	O
)	O
and	O
predicts	O
the	O
label	O
using	O
sgn	O
(	O
f	O
(	O
x∗	O
;	O
˜w	O
)	O
)	O
.	O
the	O
main	O
reason	O
for	O
introducing	O
the	O
gibbs	O
classiﬁer	B
here	O
is	O
that	O
the	O
pac-	O
bayesian	O
theorems	O
given	O
below	O
apply	O
to	O
gibbs	O
classiﬁers	O
.	O
for	O
a	O
given	O
parameter	O
vector	O
w	O
giving	O
rise	O
to	O
a	O
classiﬁer	B
c	O
(	O
x	O
;	O
w	O
)	O
,	O
the	O
ex-	O
pected	O
risk	B
and	O
empirical	O
risk	B
are	O
given	O
by	O
rl	O
(	O
w	O
)	O
=	O
l	O
(	O
y	O
,	O
c	O
(	O
x	O
;	O
w	O
)	O
)	O
dµ	O
(	O
x	O
,	O
y	O
)	O
,	O
ˆrl	O
(	O
w	O
)	O
=	O
l	O
(	O
yi	O
,	O
c	O
(	O
xi	O
;	O
w	O
)	O
)	O
.	O
(	O
7.29	O
)	O
as	O
the	O
gibbs	O
classiﬁer	B
draws	O
samples	O
from	O
q	O
(	O
w	O
)	O
we	O
consider	O
the	O
averaged	B
risks	O
rl	O
(	O
q	O
)	O
=	O
rl	O
(	O
w	O
)	O
q	O
(	O
w	O
)	O
dw	O
,	O
ˆrl	O
(	O
q	O
)	O
=	O
ˆrl	O
(	O
w	O
)	O
q	O
(	O
w	O
)	O
dw	O
.	O
(	O
7.30	O
)	O
nx	O
i=1	O
1	O
n	O
z	O
z	O
z	O
theorem	O
7.1	O
(	O
mcallester	O
’	O
s	O
pac-bayesian	O
theorem	O
)	O
for	O
any	O
probability	B
mea-	O
sures	O
p	O
and	O
q	O
over	O
w	O
and	O
for	O
any	O
bounded	O
loss	B
function	I
l	O
for	O
which	O
l	O
(	O
y	O
,	O
c	O
(	O
x	O
)	O
)	O
∈	O
[	O
0	O
,	O
1	O
]	O
for	O
any	O
classiﬁer	B
c	O
and	O
input	O
x	O
we	O
have	O
n	O
s	O
pd	O
rl	O
(	O
q	O
)	O
≤	O
ˆrl	O
(	O
q	O
)	O
+	O
kl	O
(	O
q||p	O
)	O
+	O
log	O
1	O
2n	O
−	O
1	O
δ	O
+	O
log	O
n	O
+	O
2	O
∀	O
q	O
o	O
≥	O
1	O
−	O
δ	O
.	O
(	O
7.31	O
)	O
(	O
cid:3	O
)	O
predictive	B
classiﬁer	O
gibbs	O
classiﬁer	B
mcallester	O
’	O
s	O
pac-bayesian	O
theorem	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
164	O
theoretical	O
perspectives	O
the	O
proof	O
can	O
be	O
found	O
in	O
mcallester	O
[	O
2003	O
]	O
.	O
the	O
kullback-leibler	O
(	O
kl	O
)	O
diver-	O
gence	O
kl	O
(	O
q||p	O
)	O
is	O
deﬁned	O
in	O
section	O
a.5	O
.	O
an	O
example	O
of	O
a	O
loss	B
function	I
which	O
obeys	O
the	O
conditions	O
of	O
the	O
theorem	O
is	O
the	O
0/1	O
loss	B
.	O
for	O
the	O
special	O
case	O
of	O
0/1	O
loss	B
,	O
seeger	O
[	O
2002	O
]	O
gives	O
the	O
following	O
tighter	O
seeger	O
’	O
s	O
pac-	O
bayesian	O
theorem	O
n	O
bound	O
.	O
theorem	O
7.2	O
(	O
seeger	O
’	O
s	O
pac-bayesian	O
theorem	O
)	O
for	O
any	O
distribution	O
over	O
x	O
×	O
{	O
−1	O
,	O
+1	O
}	O
and	O
for	O
any	O
probability	B
measures	O
p	O
and	O
q	O
over	O
w	O
the	O
following	O
bound	O
holds	O
for	O
i.i.d	O
.	O
samples	O
drawn	O
from	O
the	O
data	O
distribution	O
(	O
kl	O
(	O
q||p	O
)	O
+	O
log	O
n	O
+	O
1	O
(	O
7.32	O
)	O
(	O
cid:3	O
)	O
here	O
klber	O
(	O
·||·	O
)	O
is	O
the	O
kl	O
divergence	O
between	O
two	O
bernoulli	O
distributions	O
(	O
de-	O
ﬁned	O
in	O
eq	O
.	O
(	O
a.22	O
)	O
)	O
.	O
thus	O
the	O
theorem	O
bounds	O
(	O
with	O
high	O
probability	B
)	O
the	O
kl	O
divergence	O
between	O
ˆrl	O
(	O
q	O
)	O
and	O
rl	O
(	O
q	O
)	O
.	O
o	O
≥	O
1	O
−	O
δ.	O
pd	O
klber	O
(	O
ˆrl	O
(	O
q	O
)	O
||rl	O
(	O
q	O
)	O
)	O
≤	O
1	O
n	O
)	O
∀	O
q	O
δ	O
the	O
pac-bayesian	O
theorems	O
above	O
refer	O
to	O
a	O
gibbs	O
classiﬁer	B
.	O
if	O
we	O
are	O
interested	O
in	O
the	O
predictive	B
classiﬁer	O
sgn	O
(	O
q	O
(	O
f∗|x∗	O
)	O
−	O
1/2	O
)	O
then	O
seeger	O
[	O
2002	O
]	O
shows	O
that	O
if	O
q	O
(	O
f∗|x∗	O
)	O
is	O
symmetric	O
about	O
its	O
mean	O
then	O
the	O
expected	O
risk	B
of	O
the	O
predictive	B
classiﬁer	O
is	O
less	O
than	O
twice	O
the	O
expected	O
risk	B
of	O
the	O
gibbs	O
classiﬁer	B
.	O
however	O
,	O
this	O
result	O
is	O
based	O
on	O
a	O
simple	O
bounding	O
argument	O
and	O
in	O
practice	O
one	O
would	O
expect	O
that	O
the	O
predictive	B
classiﬁer	O
will	O
usually	O
give	O
better	O
performance	O
than	O
the	O
gibbs	O
classiﬁer	B
.	O
recent	O
work	O
by	O
meir	O
and	O
zhang	O
[	O
2003	O
]	O
provides	O
some	O
pac	O
bounds	O
directly	O
for	O
bayesian	O
algorithms	O
(	O
like	O
the	O
predictive	B
classiﬁer	O
)	O
whose	O
predictions	O
are	O
made	O
on	O
the	O
basis	O
of	O
a	O
data-dependent	O
posterior	O
distribution	O
.	O
7.4.3	O
pac-bayesian	O
analysis	O
of	O
gp	O
classiﬁcation	B
to	O
apply	O
this	O
bound	O
to	O
the	O
gaussian	O
process	B
case	O
we	O
need	O
to	O
compute	O
the	O
kl	O
divergence	O
kl	O
(	O
q||p	O
)	O
between	O
the	O
posterior	O
distribution	O
q	O
(	O
w	O
)	O
and	O
the	O
prior	O
distribution	O
p	O
(	O
w	O
)	O
.	O
although	O
this	O
could	O
be	O
considered	O
w.r.t	O
.	O
the	O
weight	B
vector	I
w	O
in	O
the	O
eigenfunction	B
expansion	O
,	O
in	O
fact	O
it	O
turns	O
out	O
to	O
be	O
more	O
convenient	O
to	O
consider	O
the	O
latent	O
function	O
value	O
f	O
(	O
x	O
)	O
at	O
every	O
possible	O
point	O
in	O
the	O
input	O
space	O
x	O
as	O
the	O
parameter	O
.	O
we	O
divide	O
this	O
(	O
possibly	O
inﬁnite	O
)	O
vector	O
into	O
two	O
parts	O
,	O
(	O
1	O
)	O
the	O
values	O
corresponding	O
to	O
the	O
training	O
points	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn	O
,	O
denoted	O
f	O
,	O
and	O
(	O
2	O
)	O
those	O
at	O
the	O
remaining	O
points	O
in	O
x-space	O
(	O
the	O
test	O
points	O
)	O
f∗	O
.	O
the	O
key	O
observation	O
is	O
that	O
all	O
methods	O
we	O
have	O
described	O
for	O
dealing	O
with	O
gp	O
classiﬁcation	B
problems	O
produce	O
a	O
posterior	O
approximation	O
q	O
(	O
f|y	O
)	O
which	O
is	O
deﬁned	O
at	O
the	O
training	O
points	O
.	O
(	O
this	O
is	O
an	O
approximation	O
for	O
laplace	O
’	O
s	O
method	O
and	O
for	O
ep	O
;	O
mcmc	O
methods	O
sample	O
from	O
the	O
exact	O
posterior	O
.	O
)	O
this	O
posterior	O
over	O
f	O
is	O
then	O
extended	O
to	O
the	O
test	O
points	O
by	O
setting	O
q	O
(	O
f	O
,	O
f∗|y	O
)	O
=	O
q	O
(	O
f|y	O
)	O
p	O
(	O
f∗|f	O
)	O
.	O
of	O
course	O
for	O
the	O
prior	O
distribution	O
we	O
have	O
a	O
similar	O
decomposition	O
p	O
(	O
f	O
,	O
f∗	O
)	O
=	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
7.5	O
comparison	O
with	O
other	O
supervised	B
learning	I
methods	O
165	O
p	O
(	O
f	O
)	O
p	O
(	O
f∗|f	O
)	O
.	O
thus	O
the	O
kl	O
divergence	O
is	O
given	O
by	O
z	O
z	O
kl	O
(	O
q||p	O
)	O
=	O
=	O
q	O
(	O
f|y	O
)	O
p	O
(	O
f∗|f	O
)	O
log	O
q	O
(	O
f|y	O
)	O
p	O
(	O
f∗|f	O
)	O
q	O
(	O
f|y	O
)	O
log	O
q	O
(	O
f|y	O
)	O
p	O
(	O
f	O
)	O
p	O
(	O
f∗|f	O
)	O
df	O
df∗	O
p	O
(	O
f	O
)	O
df	O
,	O
(	O
7.33	O
)	O
as	O
shown	O
e.g	O
.	O
in	O
seeger	O
[	O
2002	O
]	O
.	O
notice	O
that	O
this	O
has	O
reduced	O
a	O
rather	O
scary	O
inﬁnite-dimensional	O
integration	O
to	O
a	O
more	O
manageable	O
n-dimensional	O
integra-	O
tion	O
;	O
in	O
the	O
case	O
that	O
q	O
(	O
f|y	O
)	O
is	O
gaussian	O
(	O
as	O
for	O
the	O
laplace	O
and	O
ep	O
approxima-	O
tions	O
)	O
,	O
this	O
kl	O
divergence	O
can	O
be	O
computed	O
using	O
eq	O
.	O
(	O
a.23	O
)	O
.	O
for	O
the	O
laplace	O
approximation	O
with	O
p	O
(	O
f	O
)	O
=	O
n	O
(	O
0	O
,	O
k	O
)	O
and	O
q	O
(	O
f|y	O
)	O
=	O
n	O
(	O
ˆf	O
,	O
a−1	O
)	O
this	O
gives	O
kl	O
(	O
q||p	O
)	O
=	O
1	O
2	O
tr	O
(	O
cid:0	O
)	O
a−1	O
(	O
k−1	O
−	O
a	O
)	O
(	O
cid:1	O
)	O
+	O
1	O
ˆf	O
>	O
k−1ˆf	O
.	O
(	O
7.34	O
)	O
2	O
2	O
log	O
|k|	O
+	O
1	O
2	O
log	O
|a|	O
+	O
1	O
seeger	O
[	O
2002	O
]	O
has	O
evaluated	O
the	O
quality	O
of	O
the	O
bound	O
produced	O
by	O
the	O
pac-	O
bayesian	O
method	O
for	O
a	O
laplace	O
gpc	O
on	O
the	O
task	O
of	O
discriminating	O
handwritten	O
2s	O
and	O
3s	O
from	O
the	O
mnist	O
handwritten	O
digits	O
database.7	O
he	O
reserved	O
a	O
test	O
set	B
of	O
1000	O
examples	O
and	O
used	O
training	O
sets	O
of	O
size	O
500	O
,	O
1000	O
,	O
2000	O
,	O
5000	O
and	O
9000.	O
the	O
classiﬁcations	O
were	O
replicated	O
ten	O
times	O
using	O
draws	O
of	O
the	O
training	O
sets	O
from	O
a	O
pool	O
of	O
12089	O
examples	O
.	O
we	O
quote	O
example	O
results	O
for	O
n	O
=	O
5000	O
where	O
the	O
training	O
error	B
was	O
0.0187	O
±	O
0.0016	O
,	O
the	O
test	O
error	B
was	O
0.0195	O
±	O
0.0011	O
and	O
the	O
pac-bayesian	O
bound	O
on	O
the	O
generalization	B
error	I
(	O
evaluated	O
for	O
δ	O
=	O
0.01	O
)	O
was	O
0.076	O
±	O
0.002	O
.	O
(	O
the	O
±	O
ﬁgures	O
denote	O
a	O
95	O
%	O
conﬁdence	O
interval	O
.	O
)	O
the	O
classiﬁcation	B
results	O
are	O
for	O
the	O
gibbs	O
classiﬁer	B
;	O
for	O
the	O
predictive	B
classiﬁer	O
the	O
test	O
error	B
rate	O
was	O
0.0171±	O
0.0016.	O
thus	O
the	O
generalization	B
error	I
is	O
around	O
2	O
%	O
,	O
while	O
the	O
pac	O
bound	O
is	O
7.6	O
%	O
.	O
many	O
pac	O
bounds	O
struggle	O
to	O
predict	O
error	B
rates	O
below	O
100	O
%	O
(	O
!	O
)	O
,	O
so	O
this	O
is	O
an	O
impressive	O
and	O
highly	O
non-trivial	O
result	O
.	O
further	O
details	O
and	O
experiments	O
can	O
be	O
found	O
in	O
seeger	O
[	O
2002	O
]	O
.	O
7.5	O
comparison	O
with	O
other	O
supervised	O
learn-	O
ing	O
methods	O
the	O
focus	O
of	O
this	O
book	O
is	O
on	O
gaussian	O
process	B
methods	O
for	O
supervised	B
learning	I
.	O
however	O
,	O
there	O
are	O
many	O
other	O
techniques	O
available	O
for	O
supervised	B
learning	I
such	O
as	O
linear	B
regression	I
,	O
logistic	B
regression	I
,	O
decision	O
trees	O
,	O
neural	O
networks	O
,	O
support	B
vector	I
machines	O
,	O
kernel	B
smoothers	O
,	O
k-nearest	O
neighbour	O
classiﬁers	O
,	O
etc.	O
,	O
and	O
we	O
need	O
to	O
consider	O
the	O
relative	O
strengths	O
and	O
weaknesses	O
of	O
these	O
approaches	O
.	O
supervised	B
learning	I
is	O
an	O
inductive	B
process—given	O
a	O
ﬁnite	O
training	O
set	B
we	O
wish	O
to	O
infer	O
a	O
function	B
f	O
that	O
makes	O
predictions	O
for	O
all	O
possible	O
input	O
values	O
.	O
the	O
additional	O
assumptions	O
made	O
by	O
the	O
learning	B
algorithm	O
are	O
known	O
as	O
its	O
inductive	B
bias	O
(	O
see	B
e.g	O
.	O
mitchell	O
[	O
1997	O
,	O
p.	O
43	O
]	O
)	O
.	O
sometimes	O
these	O
assumptions	O
are	O
explicit	O
,	O
but	O
for	O
other	O
algorithms	O
(	O
e.g	O
.	O
for	O
decision	O
tree	O
induction	O
)	O
they	O
can	O
be	O
rather	O
more	O
implicit	O
.	O
7see	O
http	O
:	O
//yann.lecun.com/exdb/mnist	O
.	O
inductive	B
bias	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
166	O
theoretical	O
perspectives	O
however	O
,	O
for	O
all	O
their	O
variety	O
,	O
supervised	B
learning	I
algorithms	O
are	O
based	O
on	O
the	O
idea	O
that	O
similar	O
input	O
patterns	O
will	O
usually	O
give	O
rise	O
to	O
similar	O
outputs	B
(	O
or	O
output	O
distributions	O
)	O
,	O
and	O
it	O
is	O
the	O
precise	O
notion	O
of	O
similarity	O
that	O
diﬀerentiates	O
the	O
algorithms	O
.	O
for	O
example	O
some	O
algorithms	O
may	O
do	O
feature	O
selection	O
and	O
decide	O
that	O
there	O
are	O
input	O
dimensions	O
that	O
are	O
irrelevant	O
to	O
the	O
predictive	B
task	O
.	O
some	O
algorithms	O
may	O
construct	O
new	O
features	O
out	O
of	O
those	O
provided	O
and	O
measure	B
similarity	O
in	O
this	O
derived	O
space	O
.	O
as	O
we	O
have	O
seen	O
,	O
many	O
regression	B
techniques	O
can	O
be	O
seen	O
as	O
linear	B
smoothers	O
(	O
see	B
section	O
2.6	O
)	O
and	O
these	O
techniques	O
vary	O
in	O
the	O
deﬁnition	O
of	O
the	O
weight	B
function	I
that	O
is	O
used	O
.	O
one	O
important	O
distinction	O
between	O
diﬀerent	O
learning	B
algorithms	O
is	O
how	O
they	O
relate	O
to	O
the	O
question	O
of	O
universal	O
consistency	B
(	O
see	B
section	O
7.2.1	O
)	O
.	O
for	O
example	O
a	O
linear	B
regression	I
model	O
will	O
be	O
inconsistent	O
if	O
the	O
function	B
that	O
minimizes	O
the	O
risk	B
can	O
not	O
be	O
represented	O
by	O
a	O
linear	B
function	O
of	O
the	O
inputs	O
.	O
in	O
general	O
a	O
model	B
with	O
a	O
ﬁnite-dimensional	O
parameter	O
vector	O
will	O
not	O
be	O
universally	O
consistent	O
.	O
examples	O
of	O
such	O
models	O
are	O
linear	B
regression	I
and	O
logistic	B
regression	I
with	O
a	O
ﬁnite-dimensional	O
feature	O
vector	O
,	O
and	O
neural	O
networks	O
with	O
a	O
ﬁxed	O
number	O
of	O
hidden	O
units	O
.	O
in	O
contrast	O
to	O
these	O
parametric	B
models	O
we	O
have	O
non-parametric	B
models	O
(	O
such	O
as	O
k-nearest	O
neighbour	O
classiﬁers	O
,	O
kernel	B
smoothers	O
and	O
gaussian	O
processes	O
and	O
svms	O
with	O
nondegenerate	B
kernels	O
)	O
which	O
do	O
not	O
compress	O
the	O
training	O
data	O
into	O
a	O
ﬁnite-dimensional	O
parameter	O
vector	O
.	O
an	O
intermediate	O
po-	O
sition	O
is	O
taken	O
by	O
semi-parametric	B
models	O
such	O
as	O
neural	O
networks	O
where	O
the	O
number	O
of	O
hidden	O
units	O
k	O
is	O
allowed	O
to	O
increase	O
as	O
n	O
increases	O
.	O
in	O
this	O
case	O
uni-	O
versal	O
consistency	B
results	O
can	O
be	O
obtained	O
[	O
devroye	O
et	O
al.	O
,	O
1996	O
,	O
ch	O
.	O
30	O
]	O
under	O
certain	O
technical	O
conditions	O
and	O
growth	O
rates	O
on	O
k.	O
although	O
universal	O
consistency	B
is	O
a	O
“	O
good	O
thing	O
”	O
,	O
it	O
does	O
not	O
necessarily	O
mean	O
that	O
we	O
should	O
only	O
consider	O
procedures	O
that	O
have	O
this	O
property	O
;	O
for	O
example	O
if	O
on	O
a	O
speciﬁc	O
problem	O
we	O
knew	O
that	O
a	O
linear	B
regression	I
model	O
was	O
consistent	O
for	O
that	O
problem	O
then	O
it	O
would	O
be	O
very	O
natural	O
to	O
use	O
it	O
.	O
in	O
the	O
1980	O
’	O
s	O
there	O
was	O
a	O
large	O
surge	O
in	O
interest	O
in	O
artiﬁcial	O
neural	O
networks	O
(	O
anns	O
)	O
,	O
which	O
are	O
feedforward	O
networks	O
consisting	O
of	O
an	O
input	O
layer	O
,	O
followed	O
by	O
one	O
or	O
more	O
layers	O
of	O
non-linear	O
transformations	O
of	O
weighted	O
combinations	O
of	O
the	O
activity	O
from	O
previous	O
layers	O
,	O
and	O
an	O
output	O
layer	O
.	O
one	O
reason	O
for	O
this	O
surge	O
of	O
interest	O
was	O
the	O
use	O
of	O
the	O
backpropagation	O
algorithm	O
for	O
training	O
anns	O
.	O
initial	O
excitement	O
centered	O
around	O
that	O
fact	O
that	O
training	O
non-linear	O
networks	O
was	O
possible	O
,	O
but	O
later	O
the	O
focus	O
came	O
onto	O
the	O
generalization	B
performance	O
of	O
anns	O
,	O
and	O
how	O
to	O
deal	O
with	O
questions	O
such	O
as	O
how	O
many	O
layers	O
of	O
hidden	O
units	O
to	O
use	O
,	O
how	O
many	O
units	O
there	O
should	O
be	O
in	O
each	O
layer	O
,	O
and	O
what	O
type	O
of	O
non-linearities	O
should	O
be	O
used	O
,	O
etc	O
.	O
for	O
a	O
particular	O
ann	O
the	O
search	O
for	O
a	O
good	O
set	B
of	O
weights	O
for	O
a	O
given	O
training	O
set	B
is	O
complicated	O
by	O
the	O
fact	O
that	O
there	O
can	O
be	O
local	O
optima	O
in	O
the	O
optimization	O
problem	O
;	O
this	O
can	O
cause	O
signiﬁcant	O
diﬃculties	O
in	O
practice	O
.	O
in	O
contrast	O
for	O
gaus-	O
sian	O
process	B
regression	O
and	O
classiﬁcation	B
the	O
posterior	O
for	O
the	O
latent	O
variables	O
is	O
convex	B
.	O
neural	O
networks	O
bayesian	O
neural	O
networks	O
one	O
approach	O
to	O
the	O
problems	O
raised	O
above	O
was	O
to	O
put	O
anns	O
in	O
a	O
bayesian	O
framework	O
,	O
as	O
developed	O
by	O
mackay	O
[	O
1992a	O
]	O
and	O
neal	O
[	O
1996	O
]	O
.	O
this	O
gives	O
rise	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
7.5	O
comparison	O
with	O
other	O
supervised	B
learning	I
methods	O
167	O
to	O
posterior	O
distributions	O
over	O
weights	O
for	O
a	O
given	O
architecture	O
,	O
and	O
the	O
use	O
of	O
the	O
marginal	B
likelihood	I
(	O
see	B
section	O
5.2	O
)	O
for	O
model	B
comparison	O
and	O
selection	O
.	O
in	O
contrast	O
to	O
gaussian	O
process	B
regression	O
the	O
marginal	B
likelihood	I
for	O
a	O
given	O
ann	O
model	B
is	O
not	O
analytically	O
tractable	O
,	O
and	O
thus	O
approximation	O
techniques	O
such	O
as	O
the	O
laplace	O
approximation	O
[	O
mackay	O
,	O
1992a	O
]	O
and	O
markov	O
chain	O
monte	O
carlo	O
methods	O
[	O
neal	O
,	O
1996	O
]	O
have	O
to	O
be	O
used	O
.	O
neal	O
’	O
s	O
observation	O
[	O
1996	O
]	O
that	O
certain	O
anns	O
with	O
one	O
hidden	O
layer	O
converge	O
to	O
a	O
gaussian	O
process	B
prior	O
over	O
functions	O
(	O
see	B
section	O
4.2.3	O
)	O
led	O
us	O
to	O
consider	O
gps	O
as	O
alternatives	O
to	O
anns	O
.	O
mackay	O
[	O
2003	O
,	O
sec	O
.	O
45.7	O
]	O
raises	O
an	O
interesting	O
question	O
whether	O
in	O
moving	O
from	O
neural	O
networks	O
to	O
gaussian	O
processes	O
we	O
have	O
“	O
thrown	O
the	O
baby	O
out	O
with	O
the	O
bathwater	O
?	O
”	O
.	O
this	O
question	O
arises	O
from	O
his	O
statements	O
that	O
“	O
neural	O
networks	O
were	O
meant	O
to	O
be	O
intelligent	O
models	O
that	O
discovered	O
features	O
and	O
patterns	O
in	O
data	O
”	O
,	O
while	O
“	O
gaussian	O
processes	O
are	O
simply	O
smoothing	O
devices	O
”	O
.	O
our	O
answer	O
to	O
this	O
question	O
is	O
that	O
gps	O
give	O
us	O
a	O
computationally	O
attractive	O
method	O
for	O
dealing	O
with	O
the	O
smoothing	O
problem	O
for	O
a	O
given	O
kernel	B
,	O
and	O
that	O
issues	O
of	O
feature	O
discovery	O
etc	O
.	O
can	O
be	O
addressed	O
through	O
methods	O
to	O
select	O
the	O
kernel	B
function	O
(	O
see	B
chapter	O
5	O
for	O
more	O
details	O
on	O
how	O
to	O
do	O
this	O
)	O
.	O
note	O
that	O
using	O
a	O
distance	O
function	B
r2	O
(	O
x	O
,	O
x0	O
)	O
=	O
(	O
x	O
−	O
x0	O
)	O
>	O
m	O
(	O
x	O
−	O
x0	O
)	O
with	O
m	O
having	O
a	O
low-rank	O
form	O
m	O
=	O
λλ	O
>	O
+	O
ψ	O
as	O
in	O
eq	O
.	O
(	O
4.22	O
)	O
,	O
features	O
are	O
described	O
by	O
the	O
columns	O
of	O
λ.	O
however	O
,	O
some	O
of	O
the	O
non-convexity	O
of	O
the	O
neural	B
network	I
optimization	O
problem	O
now	O
returns	O
,	O
as	O
optimizing	O
the	O
marginal	B
likelihood	I
in	O
terms	O
of	O
the	O
parameters	O
of	O
m	O
may	O
well	O
have	O
local	O
optima	O
.	O
as	O
we	O
have	O
seen	O
from	O
chapters	O
2	O
and	O
3	O
linear	B
regression	I
and	O
logistic	B
regres-	O
sion	O
with	O
gaussian	O
priors	O
on	O
the	O
parameters	O
are	O
a	O
natural	O
starting	O
point	O
for	O
the	O
development	O
of	O
gaussian	O
process	B
regression	O
and	O
gaussian	O
process	B
classiﬁ-	O
cation	O
.	O
however	O
,	O
we	O
need	O
to	O
enhance	O
the	O
ﬂexibility	O
of	O
these	O
models	O
,	O
and	O
the	O
use	O
of	O
non-degenerate	O
kernels	O
opens	O
up	O
the	O
possibility	O
of	O
universal	O
consistency	B
.	O
kernel	B
smoothers	O
and	O
classiﬁers	O
have	O
been	O
described	O
in	O
sections	O
2.6	O
and	O
7.2.1.	O
at	O
a	O
high	O
level	O
there	O
are	O
similarities	O
between	O
gp	O
prediction	B
and	O
these	O
methods	O
as	O
a	O
kernel	B
is	O
placed	O
on	O
every	O
training	O
example	O
and	O
the	O
prediction	B
is	O
obtained	O
through	O
a	O
weighted	O
sum	O
of	O
the	O
kernel	B
functions	O
,	O
but	O
the	O
details	O
of	O
the	O
prediction	B
and	O
the	O
underlying	O
logic	O
diﬀer	O
.	O
note	O
that	O
the	O
gp	O
prediction	B
view	O
gives	O
us	O
much	O
more	O
,	O
e.g	O
.	O
error	B
bars	O
on	O
the	O
predictions	O
and	O
the	O
use	O
of	O
the	O
marginal	B
likelihood	I
to	O
set	B
parameters	O
in	O
the	O
kernel	B
(	O
see	B
section	O
5.2	O
)	O
.	O
on	O
the	O
other	O
hand	O
the	O
computational	O
problem	O
that	O
needs	O
to	O
be	O
solved	O
to	O
carry	O
out	O
gp	O
prediction	B
is	O
more	O
demanding	O
than	O
that	O
for	O
simple	O
kernel-based	O
methods	O
.	O
kernel	B
smoothers	O
and	O
classiﬁers	O
are	O
non-parametric	B
methods	O
,	O
and	O
consis-	O
tency	O
can	O
often	O
be	O
obtained	O
under	O
conditions	O
where	O
the	O
width	O
h	O
of	O
the	O
kernel	B
tends	O
to	O
zero	O
while	O
nhd	O
→	O
∞	O
.	O
the	O
equivalent	B
kernel	I
analysis	O
of	O
gp	O
regression	B
(	O
section	O
7.1	O
)	O
shows	O
that	O
there	O
are	O
quite	O
close	O
connections	O
between	O
the	O
kernel	B
regression	O
method	O
and	O
gpr	O
,	O
but	O
note	O
that	O
the	O
equivalent	B
kernel	I
automatically	O
reduces	O
its	O
width	O
as	O
n	O
grows	O
;	O
in	O
contrast	O
the	O
decay	O
of	O
h	O
has	O
to	O
be	O
imposed	O
for	O
kernel	B
regression	O
.	O
also	O
,	O
for	O
some	O
kernel	B
smoothing	O
and	O
classiﬁcation	B
algorithms	O
the	O
width	O
of	O
the	O
kernel	B
is	O
increased	O
in	O
areas	O
of	O
low	O
observation	O
density	O
;	O
for	O
ex-	O
ample	O
this	O
would	O
occur	O
in	O
algorithms	O
that	O
consider	O
the	O
k	O
nearest	O
neighbours	O
of	O
a	O
test	O
point	O
.	O
again	O
notice	O
from	O
the	O
equivalent	B
kernel	I
analysis	O
that	O
the	O
width	O
linear	B
and	O
logistic	B
regression	I
kernel	O
smoothers	O
and	O
classiﬁers	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
168	O
theoretical	O
perspectives	O
regularization	B
networks	O
,	O
splines	B
,	O
svms	O
and	O
rvms	O
∗	O
of	O
the	O
equivalent	B
kernel	I
is	O
larger	O
in	O
regions	O
of	O
low	O
density	O
,	O
although	O
the	O
exact	O
dependence	O
on	O
the	O
density	O
will	O
depend	O
on	O
the	O
kernel	B
used	O
.	O
the	O
similarities	O
and	O
diﬀerences	O
between	O
gp	O
prediction	B
and	O
regularization	B
networks	O
,	O
splines	B
,	O
svms	O
and	O
rvms	O
have	O
been	O
discussed	O
in	O
chapter	O
6	O
.	O
7.6	O
appendix	O
:	O
learning	B
curve	I
for	O
the	O
ornstein-	O
uhlenbeck	O
process	B
we	O
now	O
consider	O
the	O
calculation	O
of	O
the	O
learning	B
curve	I
for	O
the	O
ou	O
covariance	B
function	I
k	O
(	O
r	O
)	O
=	O
exp	O
(	O
−α|r|	O
)	O
on	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
,	O
assuming	O
that	O
the	O
training	O
x	O
’	O
s	O
are	O
drawn	O
from	O
the	O
uniform	O
distribution	O
u	O
(	O
0	O
,	O
1	O
)	O
.	O
our	O
treatment	O
is	O
based	O
on	O
williams	O
and	O
vivarelli	O
[	O
2000	O
]	O
.8	O
we	O
ﬁrst	O
calculate	O
eg	O
(	O
x	O
)	O
for	O
a	O
ﬁxed	O
design	O
,	O
and	O
then	O
integrate	O
over	O
possible	O
designs	O
to	O
obtain	O
eg	O
(	O
n	O
)	O
.	O
in	O
the	O
absence	O
of	O
noise	O
the	O
ou	O
process	B
is	O
markovian	O
(	O
as	O
discussed	O
in	O
ap-	O
pendix	O
b	O
and	O
exercise	O
4.5.1	O
)	O
.	O
we	O
consider	O
the	O
interval	O
[	O
0	O
,	O
1	O
]	O
with	O
points	O
x1	O
<	O
x2	O
.	O
.	O
.	O
<	O
xn−1	O
<	O
xn	O
placed	O
on	O
this	O
interval	O
.	O
also	O
let	O
x0	O
=	O
0	O
and	O
xn+1	O
=	O
1.	O
due	O
to	O
the	O
markovian	O
nature	O
of	O
the	O
process	B
the	O
prediction	B
at	O
a	O
test	O
point	O
x	O
depends	O
only	O
on	O
the	O
function	B
values	O
of	O
the	O
training	O
points	O
immediately	O
to	O
the	O
left	O
and	O
right	O
of	O
x.	O
thus	O
in	O
the	O
i-th	O
interval	O
(	O
counting	O
from	O
0	O
)	O
the	O
bounding	O
points	O
are	O
xi	O
and	O
xi+1	O
.	O
let	O
this	O
interval	O
have	O
length	O
δi	O
.	O
using	O
eq	O
.	O
(	O
7.24	O
)	O
we	O
have	O
z	O
1	O
0	O
z	O
xi+1	O
nx	O
i=0	O
xi	O
eg	O
(	O
x	O
)	O
=	O
f	O
(	O
x	O
)	O
dx	O
=	O
σ2	O
f	O
(	O
x	O
)	O
dx	O
,	O
σ2	O
(	O
7.35	O
)	O
where	O
σ2	O
we	O
have	O
in	O
interval	O
i	O
(	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
−	O
1	O
)	O
that	O
σ2	O
where	O
k	O
is	O
the	O
2	O
×	O
2	O
gram	O
matrix	B
f	O
(	O
x	O
)	O
is	O
the	O
predictive	B
variance	O
at	O
input	O
x.	O
using	O
the	O
markovian	O
property	O
f	O
(	O
x	O
)	O
=	O
k	O
(	O
0	O
)	O
−	O
k	O
(	O
x	O
)	O
>	O
k−1k	O
(	O
x	O
)	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
k	O
(	O
0	O
)	O
(	O
cid:18	O
)	O
k	O
(	O
0	O
)	O
−k	O
(	O
δi	O
)	O
and	O
k	O
(	O
x	O
)	O
is	O
the	O
corresponding	O
vector	O
of	O
length	O
2.	O
thus	O
k	O
(	O
δi	O
)	O
k	O
(	O
0	O
)	O
(	O
7.36	O
)	O
k	O
(	O
δi	O
)	O
k	O
=	O
(	O
cid:19	O
)	O
,	O
(	O
7.37	O
)	O
1	O
∆i	O
−k	O
(	O
δi	O
)	O
k	O
(	O
0	O
)	O
k−1	O
=	O
where	O
∆i	O
=	O
k2	O
(	O
0	O
)	O
−	O
k2	O
(	O
δi	O
)	O
and	O
f	O
(	O
x	O
)	O
=	O
k	O
(	O
0	O
)	O
−	O
1	O
σ2	O
∆i	O
z	O
xi+1	O
thus	O
[	O
k	O
(	O
0	O
)	O
(	O
k2	O
(	O
xi+1−x	O
)	O
+k2	O
(	O
x−xi	O
)	O
)	O
−2k	O
(	O
δi	O
)	O
k	O
(	O
x−xi	O
)	O
k	O
(	O
xi+1−x	O
)	O
]	O
.	O
(	O
7.38	O
)	O
f	O
(	O
x	O
)	O
dx	O
=	O
δik	O
(	O
0	O
)	O
−	O
2	O
σ2	O
∆i	O
xi	O
(	O
i1	O
(	O
δi	O
)	O
−	O
i2	O
(	O
δi	O
)	O
)	O
(	O
7.39	O
)	O
8cw	O
thanks	O
manfred	O
opper	O
for	O
pointing	O
out	O
that	O
the	O
upper	O
bound	O
developed	O
in	O
williams	O
and	O
vivarelli	O
[	O
2000	O
]	O
is	O
exact	O
for	O
the	O
noise-free	O
ou	O
process	B
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
7.7	O
exercises	O
where	O
i1	O
(	O
δ	O
)	O
=	O
k	O
(	O
0	O
)	O
z	O
δ	O
0	O
k2	O
(	O
z	O
)	O
dz	O
,	O
i2	O
(	O
δ	O
)	O
=	O
k	O
(	O
δ	O
)	O
z	O
δ	O
0	O
k	O
(	O
z	O
)	O
k	O
(	O
δ	O
−	O
z	O
)	O
dz	O
.	O
(	O
7.40	O
)	O
169	O
for	O
k	O
(	O
r	O
)	O
=	O
exp	O
(	O
−α|r|	O
)	O
these	O
equations	O
reduce	O
to	O
i1	O
(	O
δ	O
)	O
=	O
(	O
1	O
−	O
e−2αδ	O
)	O
/	O
(	O
2α	O
)	O
,	O
i2	O
(	O
δ	O
)	O
=	O
δe−2αδ	O
and	O
∆	O
=	O
1	O
−	O
e−2αδ	O
.	O
thus	O
f	O
(	O
x	O
)	O
dx	O
=	O
δi	O
−	O
1	O
σ2	O
α	O
2δie−2αδi	O
1	O
−	O
e−2αδi	O
.	O
+	O
(	O
7.41	O
)	O
this	O
calculation	O
is	O
not	O
correct	O
in	O
the	O
ﬁrst	O
and	O
last	O
intervals	O
where	O
only	O
x1	O
f	O
(	O
x	O
)	O
=	O
and	O
xn	O
are	O
relevant	O
(	O
respectively	O
)	O
.	O
for	O
the	O
0th	O
interval	O
we	O
have	O
that	O
σ2	O
k	O
(	O
0	O
)	O
−	O
k2	O
(	O
x1	O
−	O
x	O
)	O
/k	O
(	O
0	O
)	O
and	O
thus	O
z	O
x1	O
f	O
(	O
x	O
)	O
=	O
δ0k	O
(	O
0	O
)	O
−	O
1	O
σ2	O
k	O
(	O
0	O
)	O
(	O
1	O
−	O
e−2αδ0	O
)	O
,	O
=	O
δ0	O
−	O
1	O
2α	O
0	O
k2	O
(	O
x1	O
−	O
x	O
)	O
dx	O
z	O
xi+1	O
xi	O
z	O
x1	O
0	O
(	O
7.42	O
)	O
(	O
7.43	O
)	O
(	O
7.44	O
)	O
and	O
a	O
similar	O
result	O
holds	O
forr	O
1	O
f	O
(	O
x	O
)	O
.	O
σ2	O
putting	O
this	O
all	O
together	O
we	O
obtain	O
xn	O
eg	O
(	O
x	O
)	O
=	O
1	O
−	O
n	O
α	O
+	O
1	O
2α	O
(	O
e−2αδ0	O
+	O
e−2αδn	O
)	O
+	O
n−1x	O
i=1	O
2δie−2αδi	O
1	O
−	O
e−2αδi	O
.	O
choosing	O
a	O
regular	O
grid	O
so	O
that	O
δ0	O
=	O
δn	O
=	O
1/2n	O
and	O
δi	O
=	O
1/n	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
−	O
1	O
it	O
is	O
straightforward	O
to	O
show	O
(	O
see	B
exercise	O
7.7.4	O
)	O
that	O
eg	O
scales	O
as	O
o	O
(	O
n−1	O
)	O
,	O
in	O
agreement	O
with	O
the	O
general	O
sacks-ylvisaker	O
result	O
[	O
ritter	O
,	O
2000	O
,	O
p.	O
103	O
]	O
when	O
it	O
is	O
recalled	O
that	O
the	O
ou	O
process	B
obeys	O
sacks-ylvisaker	O
conditions	O
of	O
order	O
0.	O
a	O
similar	O
calculation	O
is	O
given	O
in	O
plaskota	O
[	O
1996	O
,	O
sec	O
.	O
3.8.2	O
]	O
for	O
the	O
wiener	O
process	B
on	O
[	O
0	O
,	O
1	O
]	O
(	O
note	O
that	O
this	O
is	O
also	O
markovian	O
,	O
but	O
non-stationary	O
)	O
.	O
we	O
have	O
now	O
worked	O
out	O
the	O
generalization	B
error	I
for	O
a	O
ﬁxed	O
design	O
x.	O
however	O
to	O
compute	O
eg	O
(	O
n	O
)	O
we	O
need	O
to	O
average	O
eg	O
(	O
x	O
)	O
over	O
draws	O
of	O
x	O
from	O
the	O
uniform	O
distribution	O
.	O
the	O
theory	O
of	O
order	O
statistics	O
david	O
[	O
1970	O
,	O
eq	O
.	O
2.3.4	O
]	O
tells	O
us	O
that	O
p	O
(	O
δ	O
)	O
=	O
n	O
(	O
1−	O
δ	O
)	O
n−1	O
for	O
all	O
the	O
δi	O
,	O
i	O
=	O
0	O
,	O
.	O
.	O
.	O
,	O
n.	O
taking	O
the	O
expectation	O
of	O
eg	O
(	O
x	O
)	O
then	O
turns	O
into	O
the	O
problem	O
of	O
evaluating	O
the	O
one-dimensional	O
integrals	B
r	O
e−2αδp	O
(	O
δ	O
)	O
dδ	O
and	O
r	O
δe−2αδ	O
(	O
1	O
−	O
e−2αδ	O
)	O
−1p	O
(	O
δ	O
)	O
dδ	O
.	O
exercise	O
7.7.5	O
asks	O
you	O
to	O
compute	O
these	O
integrals	B
numerically	O
.	O
7.7	O
exercises	O
1.	O
consider	O
a	O
spline	O
regularizer	O
with	O
sf	O
(	O
s	O
)	O
=	O
c−1|s|−2m	O
.	O
(	O
as	O
we	O
noted	O
in	O
section	O
6.3	O
this	O
is	O
not	O
strictly	O
a	O
power	O
spectrum	O
as	O
the	O
spline	O
is	O
an	O
im-	O
proper	O
prior	O
,	O
but	O
it	O
can	O
be	O
used	O
as	O
a	O
power	O
spectrum	O
in	O
eq	O
.	O
(	O
7.9	O
)	O
for	O
the	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
170	O
theoretical	O
perspectives	O
purposes	O
of	O
this	O
analysis	O
.	O
)	O
the	O
equivalent	B
kernel	I
corresponding	O
to	O
this	O
spline	O
is	O
given	O
by	O
z	O
exp	O
(	O
2πis	O
·	O
x	O
)	O
h	O
(	O
x	O
)	O
=	O
(	O
7.45	O
)	O
n/ρ	O
.	O
by	O
changing	O
variables	O
in	O
the	O
integration	O
to	O
|t|	O
=	O
1	O
+	O
γ|s|2m	O
ds	O
,	O
where	O
γ	O
=	O
cσ2	O
γ1/2m|s|	O
show	O
that	O
the	O
width	O
of	O
h	O
(	O
x	O
)	O
scales	O
as	O
n−1/2m	O
.	O
2.	O
equation	O
7.45	O
gives	O
the	O
form	O
of	O
the	O
equivalent	B
kernel	I
for	O
a	O
spline	O
regular-	O
izer	O
.	O
show	O
that	O
h	O
(	O
0	O
)	O
is	O
only	O
ﬁnite	O
if	O
2m	O
>	O
d.	O
(	O
hint	O
:	O
transform	O
the	O
inte-	O
gration	O
to	O
polar	O
coordinates	O
.	O
)	O
this	O
observation	O
was	O
made	O
by	O
p.	O
whittle	O
in	O
the	O
discussion	O
of	O
silverman	O
[	O
1985	O
]	O
,	O
and	O
shows	O
the	O
need	O
for	O
the	O
condition	O
2m	O
>	O
d	O
for	O
spline	O
smoothing	O
.	O
3.	O
computer	O
exercise	O
:	O
space	O
n	O
+	O
1	O
points	O
out	O
evenly	O
along	O
the	O
interval	O
(	O
−1/2	O
,	O
1/2	O
)	O
.	O
(	O
take	O
n	O
to	O
be	O
even	O
so	O
that	O
one	O
of	O
the	O
sample	O
points	O
falls	O
at	O
0	O
.	O
)	O
calculate	O
the	O
weight	B
function	I
(	O
see	B
section	O
2.6	O
)	O
corresponding	O
to	O
gaussian	O
process	B
regression	O
with	O
a	O
particular	O
covariance	B
function	I
and	O
noise	O
level	O
,	O
and	O
plot	O
this	O
for	O
the	O
point	O
x	O
=	O
0.	O
now	O
compute	O
the	O
equivalent	B
kernel	I
cor-	O
responding	O
to	O
the	O
covariance	B
function	I
(	O
see	B
,	O
e.g	O
.	O
the	O
examples	O
in	O
section	O
7.1.1	O
)	O
,	O
plot	O
this	O
on	O
the	O
same	O
axes	O
and	O
compare	O
results	O
.	O
hint	O
1	O
:	O
recall	O
that	O
the	O
equivalent	B
kernel	I
is	O
deﬁned	O
in	O
terms	O
of	O
integration	O
(	O
see	B
eq	O
.	O
(	O
7.7	O
)	O
)	O
so	O
that	O
there	O
will	O
be	O
a	O
scaling	O
factor	O
of	O
1/	O
(	O
n	O
+	O
1	O
)	O
.	O
hint	O
2	O
:	O
if	O
you	O
wish	O
to	O
use	O
large	O
n	O
(	O
say	O
>	O
1000	O
)	O
,	O
use	O
the	O
ngrid	O
method	O
described	O
in	O
section	O
2.6	O
.	O
4.	O
consider	O
eg	O
(	O
x	O
)	O
as	O
given	O
in	O
eq	O
.	O
(	O
7.44	O
)	O
and	O
choose	O
a	O
regular	O
grid	O
design	O
x	O
so	O
that	O
δ0	O
=	O
δn	O
=	O
1/2n	O
and	O
δi	O
=	O
1/n	O
for	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
n−1	O
.	O
show	O
that	O
eg	O
(	O
x	O
)	O
scales	O
as	O
o	O
(	O
n−1	O
)	O
asymptotically	O
.	O
hint	O
:	O
when	O
expanding	O
1	O
−	O
exp	O
(	O
−2αδi	O
)	O
,	O
be	O
sure	O
to	O
extend	O
the	O
expansion	O
to	O
suﬃcient	O
order	O
.	O
5.	O
compute	O
numerically	O
the	O
expectation	O
of	O
eg	O
(	O
x	O
)	O
eq	O
.	O
(	O
7.44	O
)	O
over	O
random	O
designs	O
for	O
the	O
ou	O
process	B
example	O
discussed	O
in	O
section	O
7.6.	O
make	O
use	O
of	O
the	O
fact	O
[	O
david	O
,	O
1970	O
,	O
eq	O
.	O
2.3.4	O
]	O
that	O
p	O
(	O
δ	O
)	O
=	O
n	O
(	O
1	O
−	O
δ	O
)	O
n−1	O
for	O
all	O
the	O
δi	O
,	O
i	O
=	O
0	O
,	O
.	O
.	O
.	O
,	O
n.	O
investigate	O
the	O
scaling	O
behaviour	O
of	O
eg	O
(	O
n	O
)	O
w.r.t	O
.	O
n.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
chapter	O
8	O
approximation	O
methods	O
for	O
large	O
datasets	O
as	O
we	O
have	O
seen	O
in	O
the	O
preceding	O
chapters	O
a	O
signiﬁcant	O
problem	O
with	O
gaus-	O
sian	O
process	B
prediction	O
is	O
that	O
it	O
typically	O
scales	O
as	O
o	O
(	O
n3	O
)	O
.	O
for	O
large	O
problems	O
(	O
e.g	O
.	O
n	O
>	O
10	O
,	O
000	O
)	O
both	O
storing	O
the	O
gram	O
matrix	B
and	O
solving	O
the	O
associated	O
linear	B
systems	O
are	O
prohibitive	O
on	O
modern	O
workstations	O
(	O
although	O
this	O
boundary	O
can	O
be	O
pushed	O
further	O
by	O
using	O
high-performance	O
computers	O
)	O
.	O
an	O
extensive	O
range	O
of	O
proposals	O
have	O
been	O
suggested	O
to	O
deal	O
with	O
this	O
prob-	O
lem	O
.	O
below	O
we	O
divide	O
these	O
into	O
ﬁve	O
parts	O
:	O
in	O
section	O
8.1	O
we	O
consider	O
reduced-	O
rank	O
approximations	O
to	O
the	O
gram	O
matrix	B
;	O
in	O
section	O
8.2	O
a	O
general	O
strategy	O
for	O
greedy	O
approximations	O
is	O
described	O
;	O
in	O
section	O
8.3	O
we	O
discuss	O
various	O
methods	O
for	O
approximating	O
the	O
gp	O
regression	B
problem	O
for	O
ﬁxed	O
hyperparameters	B
;	O
in	O
sec-	O
tion	O
8.4	O
we	O
describe	O
various	O
methods	O
for	O
approximating	O
the	O
gp	O
classiﬁcation	B
problem	O
for	O
ﬁxed	O
hyperparameters	B
;	O
and	O
in	O
section	O
8.5	O
we	O
describe	O
methods	O
to	O
approximate	O
the	O
marginal	B
likelihood	I
and	O
its	O
derivatives	O
.	O
many	O
(	O
although	O
not	O
all	O
)	O
of	O
these	O
methods	O
use	O
a	O
subset	O
of	O
size	O
m	O
<	O
n	O
of	O
the	O
training	O
examples	O
.	O
8.1	O
reduced-rank	O
approximations	O
of	O
the	O
gram	O
matrix	B
in	O
the	O
gp	O
regression	B
problem	O
we	O
need	O
to	O
invert	O
the	O
matrix	B
k	O
+	O
σ2	O
ni	O
(	O
or	O
at	O
least	O
to	O
solve	O
a	O
linear	B
system	O
(	O
k	O
+	O
σ2	O
ni	O
)	O
v	O
=	O
y	O
for	O
v	O
)	O
.	O
if	O
the	O
matrix	B
k	O
has	O
rank	O
q	O
(	O
so	O
that	O
it	O
can	O
be	O
represented	O
in	O
the	O
form	O
k	O
=	O
qq	O
>	O
where	O
q	O
is	O
an	O
n	O
×	O
q	O
matrix	B
)	O
then	O
this	O
matrix	B
inversion	O
can	O
be	O
speeded	O
up	O
using	O
the	O
matrix	B
inversion	O
lemma	O
eq	O
.	O
(	O
a.9	O
)	O
as	O
(	O
qq	O
>	O
+	O
σ2	O
niq	O
+	O
q	O
>	O
q	O
)	O
−1q	O
>	O
.	O
notice	O
that	O
the	O
inversion	O
of	O
an	O
n	O
×	O
n	O
matrix	B
has	O
now	O
been	O
transformed	O
to	O
the	O
inversion	O
of	O
a	O
q	O
×	O
q	O
matrix.1	O
nin	O
)	O
−1	O
=	O
σ−2	O
n	O
in	O
−	O
σ−2	O
n	O
q	O
(	O
σ2	O
1for	O
numerical	O
reasons	O
this	O
is	O
not	O
the	O
best	O
way	O
to	O
solve	O
such	O
a	O
linear	B
system	O
but	O
it	O
does	O
illustrate	O
the	O
savings	O
that	O
can	O
be	O
obtained	O
with	O
reduced-rank	O
representations	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
172	O
approximation	O
methods	O
for	O
large	O
datasets	O
in	O
the	O
case	O
that	O
the	O
kernel	B
is	O
derived	O
from	O
an	O
explicit	O
feature	O
expansion	O
with	O
n	O
features	O
,	O
then	O
the	O
gram	O
matrix	B
will	O
have	O
rank	O
min	O
(	O
n	O
,	O
n	O
)	O
so	O
that	O
exploitation	O
of	O
this	O
structure	O
will	O
be	O
beneﬁcial	O
if	O
n	O
>	O
n.	O
even	O
if	O
the	O
kernel	B
is	O
non-degenerate	O
it	O
may	O
happen	O
that	O
it	O
has	O
a	O
fast-decaying	O
eigenspectrum	O
(	O
see	B
e.g	O
.	O
section	O
4.3.1	O
)	O
so	O
that	O
a	O
reduced-rank	O
approximation	O
will	O
be	O
accurate	O
.	O
if	O
k	O
is	O
not	O
of	O
rank	O
<	O
n	O
,	O
we	O
can	O
still	O
consider	O
reduced-rank	O
approximations	O
to	O
k.	O
the	O
optimal	B
reduced-rank	O
approximation	O
of	O
k	O
w.r.t	O
.	O
the	O
frobenius	O
norm	B
(	O
see	B
eq	O
.	O
(	O
a.16	O
)	O
)	O
is	O
uqλqu	O
>	O
q	O
,	O
where	O
λq	O
is	O
the	O
diagonal	O
matrix	B
of	O
the	O
leading	O
q	O
eigenvalues	O
of	O
k	O
and	O
uq	O
is	O
the	O
matrix	B
of	O
the	O
corresponding	O
orthonormal	O
eigenvectors	O
[	O
golub	O
and	O
van	O
loan	O
,	O
1989	O
,	O
theorem	O
8.1.9	O
]	O
.	O
unfortunately	O
,	O
this	O
is	O
of	O
limited	O
interest	O
in	O
practice	O
as	O
computing	O
the	O
eigendecomposition	O
is	O
an	O
o	O
(	O
n3	O
)	O
operation	O
.	O
however	O
,	O
it	O
does	O
suggest	O
that	O
if	O
we	O
can	O
more	O
cheaply	O
obtain	O
an	O
approximate	O
eigendecomposition	O
then	O
this	O
may	O
give	O
rise	O
to	O
a	O
useful	O
reduced-	O
rank	O
approximation	O
to	O
k.	O
(	O
cid:18	O
)	O
kmm	O
we	O
now	O
consider	O
selecting	O
a	O
subset	O
i	O
of	O
the	O
n	O
datapoints	O
;	O
set	B
i	O
has	O
size	O
m	O
<	O
n.	O
the	O
remaining	O
n	O
−	O
m	O
datapoints	O
form	O
the	O
set	B
r.	O
(	O
as	O
a	O
mnemonic	O
,	O
i	O
is	O
for	O
the	O
included	O
datapoints	O
and	O
r	O
is	O
for	O
the	O
remaining	O
points	O
.	O
)	O
we	O
sometimes	O
call	O
the	O
included	O
set	B
the	O
active	O
set	B
.	O
without	O
loss	B
of	O
generality	O
we	O
assume	O
that	O
the	O
datapoints	O
are	O
ordered	O
so	O
that	O
set	B
i	O
comes	O
ﬁrst	O
.	O
thus	O
k	O
can	O
be	O
partitioned	O
as	O
.	O
k	O
=	O
k	O
(	O
n−m	O
)	O
m	O
k	O
(	O
n−m	O
)	O
(	O
n−m	O
)	O
(	O
8.1	O
)	O
the	O
top	O
m	O
×	O
n	O
block	O
will	O
also	O
be	O
referred	O
to	O
as	O
kmn	O
and	O
its	O
transpose	O
as	O
knm	O
.	O
in	O
section	O
4.3.2	O
we	O
saw	O
how	O
to	O
approximate	O
the	O
eigenfunctions	O
of	O
a	O
kernel	B
using	O
the	O
nystr¨om	O
method	O
.	O
we	O
can	O
now	O
apply	O
the	O
same	O
idea	O
to	O
approximating	O
the	O
eigenvalues/vectors	O
of	O
k.	O
we	O
compute	O
the	O
eigenvectors	O
and	O
eigenvalues	O
of	O
}	O
m	O
kmm	O
and	O
denote	O
them	O
{	O
λ	O
(	O
m	O
)	O
i=1	O
.	O
these	O
are	O
extended	O
to	O
all	O
n	O
points	O
using	O
eq	O
.	O
(	O
4.44	O
)	O
to	O
give	O
i=1	O
and	O
{	O
u	O
(	O
m	O
)	O
}	O
m	O
i	O
i	O
(	O
cid:19	O
)	O
km	O
(	O
n−m	O
)	O
λ	O
(	O
m	O
)	O
i	O
,	O
,	O
n	O
m	O
r	O
m	O
1	O
˜λ	O
(	O
n	O
)	O
i	O
,	O
˜u	O
(	O
n	O
)	O
i	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
i	O
=	O
1	O
,	O
.	O
.	O
.	O
,	O
m	O
(	O
8.2	O
)	O
(	O
8.3	O
)	O
knmu	O
(	O
m	O
)	O
,	O
i	O
n	O
λ	O
(	O
m	O
)	O
i	O
has	O
been	O
chosen	O
so	O
that	O
|˜u	O
(	O
n	O
)	O
|	O
’	O
1.	O
in	O
general	O
we	O
have	O
where	O
the	O
scaling	O
of	O
˜u	O
(	O
n	O
)	O
a	O
choice	O
of	O
how	O
many	O
of	O
the	O
approximate	O
eigenvalues/vectors	O
to	O
include	O
in	O
our	O
)	O
>	O
.	O
approximation	O
of	O
k	O
;	O
choosing	O
the	O
ﬁrst	O
p	O
we	O
get	O
˜k	O
=	O
pp	O
(	O
˜u	O
(	O
n	O
)	O
i	O
i	O
˜λ	O
(	O
n	O
)	O
i	O
˜u	O
(	O
n	O
)	O
i	O
i	O
i=1	O
below	O
we	O
will	O
set	B
p	O
=	O
m	O
to	O
obtain	O
nystr¨om	O
approximation	O
˜k	O
=	O
knmk−1	O
mmkmn	O
(	O
8.4	O
)	O
using	O
equations	O
8.2	O
and	O
8.3	O
,	O
which	O
we	O
call	O
the	O
nystr¨om	O
approximation	O
to	O
k.	O
computation	O
of	O
˜k	O
takes	O
time	O
o	O
(	O
m2n	O
)	O
as	O
the	O
eigendecomposition	O
of	O
kmm	O
is	O
o	O
(	O
m3	O
)	O
and	O
the	O
computation	O
of	O
each	O
˜u	O
(	O
n	O
)	O
is	O
o	O
(	O
mn	O
)	O
.	O
fowlkes	O
et	O
al	O
.	O
[	O
2001	O
]	O
have	O
applied	O
the	O
nystr¨om	O
method	O
to	O
approximate	O
the	O
top	O
few	O
eigenvectors	O
in	O
a	O
computer	O
vision	O
problem	O
where	O
the	O
matrices	O
in	O
question	O
are	O
larger	O
than	O
106×106	O
in	O
size	O
.	O
i	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
8.1	O
reduced-rank	O
approximations	O
of	O
the	O
gram	O
matrix	B
173	O
√	O
the	O
nystr¨om	O
approximation	O
has	O
been	O
applied	O
above	O
to	O
approximate	O
the	O
elements	O
of	O
k.	O
however	O
,	O
using	O
the	O
approximation	O
for	O
the	O
ith	O
eigenfunction	B
,	O
where	O
km	O
(	O
x	O
)	O
=	O
(	O
k	O
(	O
x	O
,	O
x1	O
)	O
,	O
.	O
.	O
.	O
,	O
k	O
(	O
x	O
,	O
xm	O
)	O
)	O
>	O
(	O
a	O
˜φi	O
(	O
x	O
)	O
=	O
(	O
restatement	O
of	O
eq	O
.	O
(	O
4.44	O
)	O
using	O
the	O
current	O
notation	O
)	O
and	O
λi	O
’	O
λ	O
(	O
m	O
)	O
/m	O
it	O
is	O
easy	O
to	O
see	B
that	O
in	O
general	O
we	O
obtain	O
an	O
approximation	O
for	O
the	O
kernel	B
k	O
(	O
x	O
,	O
x0	O
)	O
=	O
)	O
km	O
(	O
x	O
)	O
>	O
u	O
(	O
m	O
)	O
m/λ	O
(	O
m	O
)	O
i	O
i	O
i	O
pn	O
i=1	O
λiφi	O
(	O
x	O
)	O
φi	O
(	O
x0	O
)	O
as	O
˜k	O
(	O
x	O
,	O
x0	O
)	O
=	O
mx	O
mx	O
i=1	O
λ	O
(	O
m	O
)	O
i	O
m	O
˜φi	O
(	O
x	O
)	O
˜φi	O
(	O
x0	O
)	O
=	O
λ	O
(	O
m	O
)	O
i	O
m	O
m	O
(	O
λ	O
(	O
m	O
)	O
mmkm	O
(	O
x0	O
)	O
.	O
=	O
km	O
(	O
x	O
)	O
>	O
k−1	O
)	O
2	O
i=1	O
i	O
km	O
(	O
x	O
)	O
>	O
u	O
(	O
m	O
)	O
i	O
(	O
u	O
(	O
m	O
)	O
i	O
)	O
>	O
km	O
(	O
x0	O
)	O
(	O
8.5	O
)	O
(	O
8.6	O
)	O
(	O
8.7	O
)	O
clearly	O
eq	O
.	O
(	O
8.4	O
)	O
is	O
obtained	O
by	O
evaluating	O
eq	O
.	O
(	O
8.7	O
)	O
for	O
all	O
pairs	O
of	O
datapoints	O
in	O
the	O
training	O
set	B
.	O
by	O
multiplying	O
out	O
eq	O
.	O
(	O
8.4	O
)	O
using	O
kmn	O
=	O
[	O
kmmkm	O
(	O
n−m	O
)	O
]	O
it	O
is	O
easy	O
to	O
show	O
that	O
kmm	O
=	O
˜kmm	O
,	O
km	O
(	O
n−m	O
)	O
=	O
˜km	O
(	O
n−m	O
)	O
,	O
k	O
(	O
n−m	O
)	O
m	O
=	O
˜k	O
(	O
n−m	O
)	O
m	O
,	O
but	O
that	O
˜k	O
(	O
n−m	O
)	O
(	O
n−m	O
)	O
=	O
k	O
(	O
n−m	O
)	O
mk−1	O
k	O
(	O
n−m	O
)	O
(	O
n−m	O
)	O
−	O
˜k	O
(	O
n−m	O
)	O
(	O
n−m	O
)	O
is	O
in	O
fact	O
the	O
schur	O
complement	O
of	O
kmm	O
[	O
golub	O
and	O
van	O
loan	O
,	O
1989	O
,	O
p.	O
103	O
]	O
.	O
it	O
is	O
easy	O
to	O
see	B
that	O
k	O
(	O
n−m	O
)	O
(	O
n−m	O
)	O
−	O
˜k	O
(	O
n−m	O
)	O
(	O
n−m	O
)	O
is	O
positive	O
semi-deﬁnite	O
;	O
if	O
a	O
vector	O
f	O
is	O
partitioned	O
as	O
f	O
>	O
=	O
(	O
f	O
>	O
n−m	O
)	O
and	O
f	O
has	O
a	O
gaussian	O
distribution	O
with	O
zero	O
mean	O
and	O
covariance	B
k	O
then	O
fn−m|fm	O
has	O
the	O
schur	O
complement	O
as	O
its	O
covariance	B
matrix	I
,	O
see	B
eq	O
.	O
(	O
a.6	O
)	O
.	O
mmkm	O
(	O
n−m	O
)	O
.	O
the	O
diﬀerence	O
m	O
,	O
f	O
>	O
the	O
nystr¨om	O
approximation	O
was	O
derived	O
in	O
the	O
above	O
fashion	O
by	O
williams	O
and	O
seeger	O
[	O
2001	O
]	O
for	O
application	O
to	O
kernel	B
machines	O
.	O
an	O
alternative	O
view	O
which	O
gives	O
rise	O
to	O
the	O
same	O
approximation	O
is	O
due	O
to	O
smola	O
and	O
sch¨olkopf	O
[	O
2000	O
]	O
(	O
and	O
also	O
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
,	O
sec	O
.	O
10.2	O
]	O
)	O
.	O
here	O
the	O
starting	O
point	O
is	O
that	O
we	O
wish	O
to	O
approximate	O
the	O
kernel	B
centered	O
on	O
point	O
xi	O
as	O
a	O
linear	B
combination	O
of	O
kernels	O
from	O
the	O
active	O
set	B
,	O
so	O
that	O
cijk	O
(	O
xj	O
,	O
x	O
)	O
,	O
ˆk	O
(	O
xi	O
,	O
x	O
)	O
(	O
8.8	O
)	O
k	O
(	O
xi	O
,	O
x	O
)	O
’	O
x	O
j∈i	O
for	O
some	O
coeﬃcients	O
{	O
cij	O
}	O
that	O
are	O
to	O
be	O
determined	O
so	O
as	O
to	O
optimize	O
the	O
approximation	O
.	O
a	O
reasonable	O
criterion	O
to	O
minimize	O
is	O
kk	O
(	O
xi	O
,	O
x	O
)	O
−	O
ˆk	O
(	O
xi	O
,	O
x	O
)	O
k2h	O
nx	O
e	O
(	O
c	O
)	O
=	O
(	O
8.9	O
)	O
i=1	O
=	O
tr	O
k	O
−	O
2	O
tr	O
(	O
ckmn	O
)	O
+	O
tr	O
(	O
ckmmc	O
>	O
)	O
,	O
(	O
8.10	O
)	O
where	O
the	O
coeﬃcients	O
are	O
arranged	O
into	O
a	O
n	O
×	O
m	O
matrix	B
c.	O
minimizing	O
e	O
(	O
c	O
)	O
w.r.t	O
.	O
c	O
gives	O
copt	O
=	O
knmk−1	O
mm	O
;	O
thus	O
we	O
obtain	O
the	O
approximation	O
ˆk	O
=	O
knmk−1	O
mmkmn	O
in	O
agreement	O
with	O
eq	O
.	O
(	O
8.4	O
)	O
.	O
also	O
,	O
it	O
can	O
be	O
shown	O
that	O
e	O
(	O
copt	O
)	O
=	O
tr	O
(	O
k	O
−	O
ˆk	O
)	O
.	O
smola	O
and	O
sch¨olkopf	O
[	O
2000	O
]	O
suggest	O
a	O
greedy	O
algorithm	O
to	O
choose	O
points	O
to	O
include	O
into	O
the	O
active	O
set	B
so	O
as	O
to	O
minimize	O
the	O
error	B
criterion	O
.	O
as	O
it	O
takes	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
174	O
approximation	O
methods	O
for	O
large	O
datasets	O
o	O
(	O
mn	O
)	O
operations	O
to	O
evaluate	O
the	O
change	O
in	O
e	O
due	O
to	O
including	O
one	O
new	O
dat-	O
apoint	O
(	O
see	B
exercise	O
8.7.2	O
)	O
it	O
is	O
infeasible	O
to	O
consider	O
all	O
members	O
of	O
set	B
r	O
for	O
inclusion	O
on	O
each	O
iteration	O
;	O
instead	O
smola	O
and	O
sch¨olkopf	O
[	O
2000	O
]	O
suggest	O
ﬁnd-	O
ing	O
the	O
best	O
point	O
to	O
include	O
from	O
a	O
randomly	O
chosen	O
subset	O
of	O
set	O
r	O
on	O
each	O
iteration	O
.	O
recent	O
work	O
by	O
drineas	O
and	O
mahoney	O
[	O
2005	O
]	O
analyzes	O
a	O
similar	O
algorithm	O
to	O
the	O
nystr¨om	O
approximation	O
,	O
except	O
that	O
they	O
use	O
biased	O
sampling	O
with	O
re-	O
placement	O
(	O
choosing	O
column	O
i	O
of	O
k	O
with	O
probability	B
∝	O
k2	O
ii	O
)	O
and	O
a	O
pseudoinverse	O
of	O
the	O
inner	O
m	O
×	O
m	O
matrix	B
.	O
for	O
this	O
algorithm	O
they	O
are	O
able	O
to	O
provide	O
prob-	O
abilistic	O
bounds	O
on	O
the	O
quality	O
of	O
the	O
approximation	O
.	O
earlier	O
work	O
by	O
frieze	O
et	O
al	O
.	O
[	O
1998	O
]	O
had	O
developed	O
an	O
approximation	O
to	O
the	O
singular	O
value	O
decomposi-	O
tion	O
(	O
svd	O
)	O
of	O
a	O
rectangular	O
matrix	B
using	O
a	O
weighted	O
random	O
subsampling	O
of	O
its	O
rows	O
and	O
columns	O
,	O
and	O
probabilistic	B
error	O
bounds	O
.	O
however	O
,	O
this	O
is	O
rather	O
diﬀer-	O
ent	O
from	O
the	O
nystr¨om	O
approximation	O
;	O
see	B
drineas	O
and	O
mahoney	O
[	O
2005	O
,	O
sec	O
.	O
5.2	O
]	O
for	O
details	O
.	O
fine	O
and	O
scheinberg	O
[	O
2002	O
]	O
suggest	O
an	O
alternative	O
low-rank	O
approximation	O
to	O
k	O
using	O
the	O
incomplete	O
cholesky	O
factorization	O
(	O
see	B
golub	O
and	O
van	O
loan	O
[	O
1989	O
,	O
sec	O
.	O
10.3.2	O
]	O
)	O
.	O
the	O
idea	O
here	O
is	O
that	O
when	O
computing	O
the	O
cholesky	O
de-	O
composition	O
of	O
k	O
pivots	O
below	O
a	O
certain	O
threshold	O
are	O
skipped.2	O
if	O
the	O
number	O
of	O
pivots	O
greater	O
than	O
the	O
threshold	O
is	O
k	O
the	O
incomplete	O
cholesky	O
factorization	O
takes	O
time	O
o	O
(	O
nk2	O
)	O
.	O
8.2	O
greedy	O
approximation	O
many	O
of	O
the	O
methods	O
described	O
below	O
use	O
an	O
active	O
set	B
of	O
training	O
points	O
of	O
size	O
m	O
selected	O
from	O
the	O
training	O
set	B
of	O
size	O
n	O
>	O
m.	O
we	O
assume	O
that	O
it	O
is	O
impossible	O
to	O
search	O
for	O
the	O
optimal	B
subset	O
of	O
size	O
m	O
due	O
to	O
combinatorics	O
.	O
the	O
points	O
in	O
the	O
active	O
set	B
could	O
be	O
selected	O
randomly	O
,	O
but	O
in	O
general	O
we	O
might	O
expect	O
better	O
performance	O
if	O
the	O
points	O
are	O
selected	O
greedily	O
w.r.t	O
.	O
some	O
criterion	O
.	O
in	O
the	O
statistics	O
literature	O
greedy	O
approaches	O
are	O
also	O
known	O
as	O
forward	O
selection	O
strategies	O
.	O
a	O
general	O
recipe	O
for	O
greedy	O
approximation	O
is	O
given	O
in	O
algorithm	O
8.1.	O
the	O
algorithm	O
starts	O
with	O
the	O
active	O
set	B
i	O
being	O
empty	O
,	O
and	O
the	O
set	B
r	O
containing	O
the	O
indices	O
of	O
all	O
training	O
examples	O
.	O
on	O
each	O
iteration	O
one	O
index	O
is	O
selected	O
from	O
r	O
and	O
added	O
to	O
i.	O
this	O
is	O
achieved	O
by	O
evaluating	O
some	O
criterion	O
∆	O
and	O
selecting	O
the	O
data	O
point	O
that	O
optimizes	O
this	O
criterion	O
.	O
for	O
some	O
algorithms	O
it	O
can	O
be	O
too	O
expensive	O
to	O
evaluate	O
∆	O
on	O
all	O
points	O
in	O
r	O
,	O
so	O
some	O
working	O
set	B
j	O
⊂	O
r	O
can	O
be	O
chosen	O
instead	O
,	O
usually	O
at	O
random	O
from	O
r.	O
greedy	O
selection	O
methods	O
have	O
been	O
used	O
with	O
the	O
subset	B
of	I
regressors	I
(	O
sr	O
)	O
,	O
subset	B
of	I
datapoints	I
(	O
sd	O
)	O
and	O
the	O
projected	O
process	O
(	O
pp	O
)	O
methods	O
described	O
below	O
.	O
2as	O
a	O
technical	O
detail	O
,	O
symmetric	O
permutations	O
of	O
the	O
rows	O
and	O
columns	O
are	O
required	O
to	O
stabilize	O
the	O
computations	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
8.3	O
approximations	O
for	O
gpr	O
with	O
fixed	O
hyperparameters	B
175	O
input	O
:	O
m	O
,	O
desired	O
size	O
of	O
active	O
set	B
2	O
:	O
initialization	O
i	O
=	O
∅	O
,	O
r	O
=	O
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
for	O
j	O
:	O
=	O
1	O
.	O
.	O
.	O
m	O
do	O
create	O
working	O
set	B
j	O
⊆	O
r	O
compute	O
∆j	O
for	O
all	O
j	O
∈	O
j	O
i	O
=	O
argmaxj∈j∆j	O
update	O
model	B
to	O
include	O
data	O
from	O
example	O
i	O
i	O
←	O
i	O
∪	O
{	O
i	O
}	O
,	O
r	O
←	O
r\	O
{	O
i	O
}	O
4	O
:	O
6	O
:	O
8	O
:	O
end	O
for	O
10	O
:	O
return	O
:	O
i	O
algorithm	O
8.1	O
:	O
general	O
framework	O
for	O
greedy	O
subset	O
selection	O
.	O
∆j	O
is	O
the	O
criterion	O
function	B
evaluated	O
on	O
data	O
point	O
j	O
.	O
8.3	O
approximations	O
for	O
gpr	O
with	O
fixed	O
hy-	O
perparameters	O
we	O
present	O
six	O
approximation	O
schemes	O
for	O
gpr	O
below	O
,	O
namely	O
the	O
subset	B
of	I
regressors	I
(	O
sr	O
)	O
,	O
the	O
nystr¨om	O
method	O
,	O
the	O
subset	B
of	I
datapoints	I
(	O
sd	O
)	O
,	O
the	O
pro-	O
jected	O
process	B
(	O
pp	O
)	O
approximation	O
,	O
the	O
bayesian	O
committee	O
machine	O
(	O
bcm	O
)	O
and	O
the	O
iterative	O
solution	O
of	O
linear	B
systems	O
.	O
section	O
8.3.7	O
provides	O
a	O
summary	O
of	O
these	O
methods	O
and	O
a	O
comparison	O
of	O
their	O
performance	O
on	O
the	O
sarcos	O
data	O
which	O
was	O
introduced	O
in	O
section	O
2.5	O
.	O
8.3.1	O
subset	B
of	I
regressors	I
pn	O
silverman	O
[	O
1985	O
,	O
sec	O
.	O
6.1	O
]	O
showed	O
that	O
the	O
mean	O
gp	O
predictor	O
can	O
be	O
ob-	O
tained	O
from	O
a	O
ﬁnite-dimensional	O
generalized	B
linear	O
regression	B
model	O
f	O
(	O
x∗	O
)	O
=	O
i=1	O
αik	O
(	O
x∗	O
,	O
xi	O
)	O
with	O
a	O
prior	O
α	O
∼	O
n	O
(	O
0	O
,	O
k−1	O
)	O
.	O
to	O
see	B
this	O
we	O
use	O
the	O
mean	O
prediction	O
for	O
linear	B
regression	I
model	O
in	O
feature	B
space	I
given	O
by	O
eq	O
.	O
(	O
2.11	O
)	O
,	O
i.e	O
.	O
¯f	O
(	O
x∗	O
)	O
=	O
σ−2	O
n	O
φφ	O
>	O
.	O
setting	O
φ	O
(	O
x∗	O
)	O
=	O
k	O
(	O
x∗	O
)	O
,	O
φ	O
=	O
φ	O
>	O
=	O
k	O
and	O
σ−1	O
¯f	O
(	O
x∗	O
)	O
=	O
σ−2	O
n	O
φ	O
(	O
x∗	O
)	O
>	O
a−1φy	O
with	O
a	O
=	O
σ−1	O
p	O
+	O
σ−2	O
p	O
=	O
k	O
we	O
obtain	O
n	O
k	O
>	O
(	O
x∗	O
)	O
[	O
σ−2	O
=	O
k	O
>	O
(	O
x∗	O
)	O
(	O
k	O
+	O
σ2	O
n	O
k	O
(	O
k	O
+	O
σ2	O
ni	O
)	O
−1y	O
,	O
ni	O
)	O
]	O
−1ky	O
(	O
8.11	O
)	O
(	O
8.12	O
)	O
in	O
agreement	O
with	O
eq	O
.	O
(	O
2.25	O
)	O
.	O
note	O
,	O
however	O
,	O
that	O
the	O
predictive	B
(	O
co	O
)	O
variance	O
of	O
this	O
model	B
is	O
diﬀerent	O
from	O
full	O
gpr	O
.	O
a	O
simple	O
approximation	O
to	O
this	O
model	B
is	O
to	O
consider	O
only	O
a	O
subset	O
of	O
regres-	O
sors	O
,	O
so	O
that	O
mx	O
i=1	O
fsr	O
(	O
x∗	O
)	O
=	O
αik	O
(	O
x∗	O
,	O
xi	O
)	O
,	O
with	O
αm	O
∼	O
n	O
(	O
0	O
,	O
k−1	O
mm	O
)	O
.	O
(	O
8.13	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
176	O
approximation	O
methods	O
for	O
large	O
datasets	O
again	O
using	O
eq	O
.	O
(	O
2.11	O
)	O
we	O
obtain	O
¯fsr	O
(	O
x∗	O
)	O
=	O
km	O
(	O
x∗	O
)	O
>	O
(	O
kmnknm	O
+	O
σ2	O
nkmm	O
)	O
−1kmny	O
,	O
v	O
[	O
fsr	O
(	O
x∗	O
)	O
]	O
=	O
σ2	O
nkm	O
(	O
x∗	O
)	O
>	O
(	O
kmnknm	O
+	O
σ2	O
nkmm	O
)	O
−1km	O
(	O
x∗	O
)	O
.	O
(	O
8.14	O
)	O
(	O
8.15	O
)	O
sr	O
marginal	B
likelihood	I
nkmm	O
)	O
−1kmny	O
.	O
thus	O
the	O
posterior	O
mean	O
for	O
αm	O
is	O
given	O
by	O
¯αm	O
=	O
(	O
kmnknm	O
+	O
σ2	O
(	O
8.16	O
)	O
this	O
method	O
has	O
been	O
proposed	O
,	O
for	O
example	O
,	O
in	O
wahba	O
[	O
1990	O
,	O
chapter	O
7	O
]	O
,	O
and	O
in	O
poggio	O
and	O
girosi	O
[	O
1990	O
,	O
eq	O
.	O
25	O
]	O
via	O
the	O
regularization	B
framework	O
.	O
the	O
name	O
“	O
subset	B
of	I
regressors	I
”	O
(	O
sr	O
)	O
was	O
suggested	O
to	O
us	O
by	O
g.	O
wahba	O
.	O
the	O
computa-	O
tions	O
for	O
equations	O
8.14	O
and	O
8.15	O
take	O
time	O
o	O
(	O
m2n	O
)	O
to	O
carry	O
out	O
the	O
necessary	O
matrix	B
computations	O
.	O
after	O
this	O
the	O
prediction	B
of	O
the	O
mean	O
for	O
a	O
new	O
test	O
point	O
takes	O
time	O
o	O
(	O
m	O
)	O
,	O
and	O
the	O
predictive	B
variance	O
takes	O
o	O
(	O
m2	O
)	O
.	O
under	O
the	O
subset	B
of	I
regressors	I
model	O
we	O
have	O
f	O
∼	O
n	O
(	O
0	O
,	O
˜k	O
)	O
where	O
˜k	O
is	O
y	O
>	O
(	O
˜k	O
+	O
σ2	O
log	O
|	O
˜k	O
+	O
σ2	O
nin|	O
−	O
1	O
2	O
deﬁned	O
as	O
in	O
eq	O
.	O
(	O
8.4	O
)	O
.	O
thus	O
the	O
log	O
marginal	O
likelihood	B
under	O
this	O
model	B
is	O
log	O
psr	O
(	O
y|x	O
)	O
=	O
−1	O
2	O
notice	O
that	O
the	O
covariance	B
function	I
deﬁned	O
by	O
the	O
sr	O
model	B
has	O
the	O
form	O
˜k	O
(	O
x	O
,	O
x0	O
)	O
=	O
k	O
(	O
x	O
)	O
>	O
k−1	O
mmk	O
(	O
x0	O
)	O
,	O
which	O
is	O
exactly	O
the	O
same	O
as	O
that	O
from	O
the	O
nystr¨om	O
approximation	O
for	O
the	O
covariance	B
function	I
eq	O
.	O
(	O
8.7	O
)	O
.	O
in	O
fact	O
if	O
the	O
covariance	B
function	I
k	O
(	O
x	O
,	O
x0	O
)	O
in	O
the	O
predictive	B
mean	O
and	O
variance	O
equations	O
2.25	O
and	O
2.26	O
is	O
replaced	O
systematically	O
with	O
˜k	O
(	O
x	O
,	O
x0	O
)	O
we	O
obtain	O
equations	O
8.14	O
and	O
8.15	O
,	O
as	O
shown	O
in	O
appendix	O
8.6.	O
nin	O
)	O
−1y	O
−	O
n	O
2	O
log	O
(	O
2π	O
)	O
.	O
(	O
8.17	O
)	O
if	O
the	O
kernel	B
function	O
decays	O
to	O
zero	O
for	O
|x|	O
→	O
∞	O
for	O
ﬁxed	O
x0	O
,	O
then	O
˜k	O
(	O
x	O
,	O
x	O
)	O
will	O
be	O
near	O
zero	O
when	O
x	O
is	O
distant	O
from	O
points	O
in	O
the	O
set	B
i.	O
this	O
will	O
be	O
the	O
case	O
even	O
when	O
the	O
kernel	B
is	O
stationary	O
so	O
that	O
k	O
(	O
x	O
,	O
x	O
)	O
is	O
independent	O
of	O
x.	O
thus	O
we	O
might	O
expect	O
that	O
using	O
the	O
approximate	O
kernel	B
will	O
give	O
poor	O
predictions	O
,	O
especially	O
underestimates	O
of	O
the	O
predictive	B
variance	O
,	O
when	O
x	O
is	O
far	O
from	O
points	O
in	O
the	O
set	B
i.	O
ysr∗	O
(	O
x∗	O
)	O
=	O
pm	O
an	O
interesting	O
idea	O
suggested	O
by	O
rasmussen	O
and	O
qui˜nonero-candela	O
[	O
2005	O
]	O
to	O
mitigate	O
this	O
problem	O
is	O
to	O
deﬁne	O
the	O
sr	O
model	B
with	O
m	O
+	O
1	O
basis	O
func-	O
tions	O
,	O
where	O
the	O
extra	O
basis	O
function	B
is	O
centered	O
on	O
the	O
test	O
point	O
x∗	O
,	O
so	O
that	O
i=1	O
αik	O
(	O
x∗	O
,	O
xi	O
)	O
+	O
α∗k	O
(	O
x∗	O
,	O
x∗	O
)	O
.	O
this	O
model	B
can	O
then	O
be	O
used	O
to	O
make	O
predictions	O
,	O
and	O
it	O
can	O
be	O
implemented	O
eﬃciently	O
using	O
the	O
partitioned	O
matrix	O
inverse	O
equations	O
a.11	O
and	O
a.12	O
.	O
the	O
eﬀect	O
of	O
the	O
extra	O
basis	O
function	B
centered	O
on	O
x∗	O
is	O
to	O
maintain	O
predictive	B
variance	O
at	O
the	O
test	O
point	O
.	O
so	O
far	O
we	O
have	O
not	O
said	O
how	O
the	O
subset	O
i	O
should	O
be	O
chosen	O
.	O
one	O
sim-	O
ple	O
method	O
is	O
to	O
choose	O
it	O
randomly	O
from	O
x	O
,	O
another	O
is	O
to	O
run	O
clustering	O
on	O
{	O
xi	O
}	O
n	O
i=1	O
to	O
obtain	O
centres	O
.	O
alternatively	O
,	O
a	O
number	O
of	O
greedy	O
forward	O
selection	O
algorithms	O
for	O
i	O
have	O
been	O
proposed	O
.	O
luo	O
and	O
wahba	O
[	O
1997	O
]	O
choose	O
the	O
next	O
kernel	B
so	O
as	O
to	O
minimize	O
the	O
residual	O
sum	O
of	O
squares	O
(	O
rss	O
)	O
|y−	O
knmαm|2	O
after	O
optimizing	O
αm	O
.	O
smola	O
and	O
bartlett	O
[	O
2001	O
]	O
take	O
a	O
similar	O
approach	O
,	O
but	O
choose	O
as	O
their	O
criterion	O
the	O
quadratic	B
form	I
|y	O
−	O
knm	O
¯αm|2	O
+	O
¯α	O
>	O
mkmm	O
¯αm	O
=	O
y	O
>	O
(	O
˜k	O
+	O
σ2	O
nin	O
)	O
−1y	O
,	O
(	O
8.18	O
)	O
1	O
σ2	O
n	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
8.3	O
approximations	O
for	O
gpr	O
with	O
fixed	O
hyperparameters	B
177	O
comparison	O
with	O
rvm	O
where	O
the	O
right	O
hand	O
side	O
follows	O
using	O
eq	O
.	O
(	O
8.16	O
)	O
and	O
the	O
matrix	B
inversion	O
lemma	O
.	O
alternatively	O
,	O
qui˜nonero-candela	O
[	O
2004	O
]	O
suggests	O
using	O
the	O
approxi-	O
mate	O
log	O
marginal	O
likelihood	B
log	O
psr	O
(	O
y|x	O
)	O
(	O
see	B
eq	O
.	O
(	O
8.17	O
)	O
)	O
as	O
the	O
selection	O
cri-	O
terion	O
.	O
in	O
fact	O
the	O
quadratic	O
term	O
from	O
eq	O
.	O
(	O
8.18	O
)	O
is	O
one	O
of	O
the	O
terms	O
comprising	O
log	O
psr	O
(	O
y|x	O
)	O
.	O
for	O
all	O
these	O
suggestions	O
the	O
complexity	O
of	O
evaluating	O
the	O
criterion	O
on	O
a	O
new	O
example	O
is	O
o	O
(	O
mn	O
)	O
,	O
by	O
making	O
use	O
of	O
partitioned	O
matrix	O
equations	O
.	O
thus	O
it	O
is	O
likely	O
to	O
be	O
too	O
expensive	O
to	O
consider	O
all	O
points	O
in	O
r	O
on	O
each	O
iteration	O
,	O
and	O
we	O
are	O
likely	O
to	O
want	O
to	O
consider	O
a	O
smaller	O
working	O
set	B
,	O
as	O
described	O
in	O
algorithm	O
8.1.	O
note	O
that	O
the	O
sr	O
model	B
is	O
obtained	O
by	O
selecting	O
some	O
subset	O
of	O
the	O
data-	O
points	O
of	O
size	O
m	O
in	O
a	O
random	O
or	O
greedy	O
manner	O
.	O
the	O
relevance	B
vector	I
machine	I
(	O
rvm	O
)	O
described	O
in	O
section	O
6.6	O
has	O
a	O
similar	O
ﬂavour	O
in	O
that	O
it	O
automatically	O
selects	O
(	O
in	O
a	O
greedy	O
fashion	O
)	O
which	O
datapoints	O
to	O
use	O
in	O
its	O
expansion	O
.	O
however	O
,	O
note	O
one	O
important	O
diﬀerence	O
which	O
is	O
that	O
the	O
rvm	O
uses	O
a	O
diagonal	O
prior	O
on	O
the	O
α	O
’	O
s	O
,	O
while	O
for	O
the	O
sr	O
method	O
we	O
have	O
αm	O
∼	O
n	O
(	O
0	O
,	O
k−1	O
mm	O
)	O
.	O
8.3.2	O
the	O
nystr¨om	O
method	O
williams	O
and	O
seeger	O
[	O
2001	O
]	O
suggested	O
approximating	O
the	O
gpr	O
equations	O
by	O
replacing	O
the	O
matrix	B
k	O
by	O
˜k	O
in	O
the	O
mean	O
and	O
variance	O
prediction	B
equations	O
2.25	O
and	O
2.26	O
,	O
and	O
called	O
this	O
the	O
nystr¨om	O
method	O
for	O
approximate	O
gpr	O
.	O
notice	O
that	O
in	O
this	O
proposal	O
the	O
covariance	B
function	I
k	O
is	O
not	O
systematically	O
replaced	O
by	O
˜k	O
,	O
it	O
is	O
only	O
occurrences	O
of	O
the	O
matrix	B
k	O
that	O
are	O
replaced	O
.	O
as	O
for	O
the	O
sr	O
model	B
the	O
time	O
complexity	O
is	O
o	O
(	O
m2n	O
)	O
to	O
carry	O
out	O
the	O
necessary	O
matrix	B
computations	O
,	O
and	O
then	O
o	O
(	O
n	O
)	O
for	O
the	O
predictive	B
mean	O
of	O
a	O
test	O
point	O
and	O
o	O
(	O
mn	O
)	O
for	O
the	O
predictive	B
variance	O
.	O
experimental	O
evidence	O
in	O
williams	O
et	O
al	O
.	O
[	O
2002	O
]	O
suggests	O
that	O
for	O
large	O
m	O
the	O
sr	O
and	O
nystr¨om	O
methods	O
have	O
similar	O
performance	O
,	O
but	O
for	O
small	O
m	O
the	O
nystr¨om	O
method	O
can	O
be	O
quite	O
poor	O
.	O
also	O
the	O
fact	O
that	O
k	O
is	O
not	O
systematically	O
replaced	O
by	O
˜k	O
means	O
that	O
embarrassments	O
can	O
occur	O
like	O
the	O
approximated	O
predictive	B
variance	O
being	O
negative	O
.	O
for	O
these	O
reasons	O
we	O
do	O
not	O
recommend	O
the	O
nystr¨om	O
method	O
over	O
the	O
sr	O
method	O
.	O
however	O
,	O
the	O
nystr¨om	O
method	O
can	O
be	O
eﬀective	O
when	O
λm+1	O
,	O
the	O
(	O
m	O
+	O
1	O
)	O
th	O
eigenvalue	B
of	O
k	O
,	O
is	O
much	O
smaller	O
than	O
σ2	O
n.	O
8.3.3	O
subset	B
of	I
datapoints	I
the	O
subset	B
of	I
regressors	I
method	O
described	O
above	O
approximated	O
the	O
form	O
of	O
the	O
predictive	B
distribution	O
,	O
and	O
particularly	O
the	O
predictive	B
mean	O
.	O
another	O
simple	O
approximation	O
to	O
the	O
full-sample	O
gp	O
predictor	O
is	O
to	O
keep	O
the	O
gp	O
predictor	O
,	O
but	O
only	O
on	O
a	O
smaller	O
subset	O
of	O
size	O
m	O
of	O
the	O
data	O
.	O
although	O
this	O
is	O
clearly	O
wasteful	O
of	O
data	O
,	O
it	O
can	O
make	O
sense	O
if	O
the	O
predictions	O
obtained	O
with	O
m	O
points	O
are	O
suﬃciently	O
accurate	O
for	O
our	O
needs	O
.	O
clearly	O
it	O
can	O
make	O
sense	O
to	O
select	O
which	O
points	O
are	O
taken	O
into	O
the	O
active	O
set	B
i	O
,	O
and	O
typically	O
this	O
is	O
achieved	O
by	O
greedy	O
algorithms	O
.	O
however	O
,	O
one	O
has	O
to	O
be	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
178	O
approximation	O
methods	O
for	O
large	O
datasets	O
wary	O
of	O
the	O
amount	O
of	O
computation	O
that	O
is	O
needed	O
,	O
especially	O
if	O
one	O
considers	O
each	O
member	O
of	O
r	O
at	O
each	O
iteration	O
.	O
lawrence	O
et	O
al	O
.	O
[	O
2003	O
]	O
suggest	O
choosing	O
as	O
the	O
next	O
point	O
(	O
or	O
site	O
)	O
for	O
in-	O
clusion	O
into	O
the	O
active	O
set	B
the	O
one	O
that	O
maximizes	O
the	O
diﬀerential	O
entropy	B
score	O
∆j	O
,	O
h	O
[	O
p	O
(	O
fj	O
)	O
]	O
−	O
h	O
[	O
pnew	O
(	O
fj	O
)	O
]	O
,	O
where	O
h	O
[	O
p	O
(	O
fj	O
)	O
]	O
is	O
the	O
entropy	B
of	O
the	O
gaus-	O
sian	O
at	O
site	O
j	O
∈	O
r	O
(	O
which	O
is	O
a	O
function	B
of	O
the	O
variance	O
at	O
site	O
j	O
as	O
the	O
poste-	O
rior	O
is	O
gaussian	O
,	O
see	B
eq	O
.	O
(	O
a.20	O
)	O
)	O
,	O
and	O
h	O
[	O
pnew	O
(	O
fj	O
)	O
]	O
is	O
the	O
entropy	B
at	O
this	O
site	O
once	O
the	O
observation	O
at	O
site	O
j	O
has	O
been	O
included	O
.	O
let	O
the	O
posterior	O
variance	O
of	O
fj	O
before	O
inclusion	O
be	O
vj	O
.	O
as	O
p	O
(	O
fj|yi	O
,	O
yj	O
)	O
∝	O
p	O
(	O
fj|yi	O
)	O
n	O
(	O
yj|fj	O
,	O
σ2	O
)	O
we	O
have	O
j	O
+	O
σ−2	O
.	O
using	O
the	O
fact	O
that	O
the	O
entropy	B
of	O
a	O
gaussian	O
with	O
(	O
vnew	O
variance	O
v	O
is	O
log	O
(	O
2πev	O
)	O
/2	O
we	O
obtain	O
)	O
−1	O
=	O
v−1	O
j	O
ivm	O
∆j	O
=	O
1	O
2	O
log	O
(	O
1	O
+	O
vj/σ2	O
)	O
.	O
(	O
8.19	O
)	O
∆j	O
is	O
a	O
monotonic	O
function	B
of	O
vj	O
so	O
that	O
it	O
is	O
maximized	O
by	O
choosing	O
the	O
site	O
with	O
the	O
largest	O
variance	O
.	O
lawrence	O
et	O
al	O
.	O
[	O
2003	O
]	O
call	O
their	O
method	O
the	O
informative	B
vector	I
machine	I
(	O
ivm	O
)	O
if	O
coded	O
na¨ıvely	O
the	O
complexity	O
of	O
computing	O
the	O
variance	O
at	O
all	O
sites	O
in	O
r	O
on	O
a	O
single	O
iteration	O
is	O
o	O
(	O
m3	O
+	O
(	O
n−	O
m	O
)	O
m2	O
)	O
as	O
we	O
need	O
to	O
evaluate	O
eq	O
.	O
(	O
2.26	O
)	O
at	O
ni	O
can	O
be	O
done	O
once	O
in	O
o	O
(	O
m3	O
)	O
each	O
site	O
(	O
and	O
the	O
matrix	B
inversion	O
of	O
kmm	O
+	O
σ2	O
then	O
stored	O
)	O
.	O
however	O
,	O
as	O
we	O
are	O
incrementally	O
growing	O
the	O
matrices	O
kmm	O
and	O
km	O
(	O
n−m	O
)	O
in	O
fact	O
the	O
cost	O
is	O
o	O
(	O
mn	O
)	O
per	O
inclusion	O
,	O
leading	O
to	O
an	O
overall	O
complexity	O
of	O
o	O
(	O
m2n	O
)	O
when	O
using	O
a	O
subset	O
of	O
size	O
m.	O
for	O
example	O
,	O
once	O
a	O
site	O
has	O
been	O
chosen	O
for	O
inclusion	O
the	O
matrix	B
kmm	O
+	O
σ2	O
ni	O
is	O
grown	O
by	O
including	O
an	O
extra	O
row	O
and	O
column	O
.	O
the	O
inverse	O
of	O
this	O
expanded	O
matrix	B
can	O
be	O
found	O
using	O
eq	O
.	O
(	O
a.12	O
)	O
although	O
it	O
would	O
be	O
better	O
practice	O
numerically	O
to	O
use	O
a	O
cholesky	O
decomposition	O
approach	O
as	O
described	O
in	O
lawrence	O
et	O
al	O
.	O
[	O
2003	O
]	O
.	O
the	O
scheme	O
evaluates	O
∆j	O
over	O
all	O
j	O
∈	O
r	O
at	O
each	O
step	O
to	O
choose	O
the	O
inclusion	O
site	O
.	O
this	O
makes	O
sense	O
when	O
m	O
is	O
small	O
,	O
but	O
as	O
it	O
gets	O
larger	O
it	O
can	O
make	O
sense	O
to	O
select	O
candidate	O
inclusion	O
sites	O
from	O
a	O
subset	O
of	O
r.	O
lawrence	O
et	O
al	O
.	O
[	O
2003	O
]	O
call	O
this	O
the	O
randomized	O
greedy	O
selection	O
method	O
and	O
give	O
further	O
ideas	O
on	O
how	O
to	O
choose	O
the	O
subset	O
.	O
the	O
diﬀerential	O
entropy	B
score	O
∆j	O
is	O
not	O
the	O
only	O
criterion	O
that	O
can	O
be	O
used	O
for	O
site	O
selection	O
.	O
for	O
example	O
the	O
information	O
gain	O
criterion	O
kl	O
(	O
pnew	O
(	O
fj	O
)	O
||p	O
(	O
fj	O
)	O
)	O
can	O
also	O
be	O
used	O
(	O
see	B
seeger	O
et	O
al.	O
,	O
2003	O
)	O
.	O
the	O
use	O
of	O
greedy	O
selection	O
heuristics	O
here	O
is	O
similar	O
to	O
the	O
problem	O
of	O
active	O
learning	B
,	O
see	B
e.g	O
.	O
mackay	O
[	O
1992c	O
]	O
.	O
8.3.4	O
projected	B
process	I
approximation	I
the	O
sr	O
method	O
has	O
the	O
unattractive	O
feature	O
that	O
it	O
is	O
based	O
on	O
a	O
degenerate	B
gp	O
,	O
the	O
ﬁnite-dimensional	O
model	B
given	O
in	O
eq	O
.	O
(	O
8.13	O
)	O
.	O
the	O
sd	O
method	O
is	O
a	O
non-	O
degenerate	B
process	O
model	B
but	O
it	O
only	O
makes	O
use	O
of	O
m	O
datapoints	O
.	O
the	O
projected	O
process	O
(	O
pp	O
)	O
approximation	O
is	O
also	O
a	O
non-degenerate	O
process	B
model	O
but	O
it	O
can	O
make	O
use	O
of	O
all	O
n	O
datapoints	O
.	O
we	O
call	O
it	O
a	O
projected	B
process	I
approximation	I
as	O
it	O
represents	O
only	O
m	O
<	O
n	O
latent	O
function	O
values	O
,	O
but	O
computes	O
a	O
likelihood	B
involving	O
all	O
n	O
datapoints	O
by	O
projecting	O
up	O
the	O
m	O
latent	O
points	O
to	O
n	O
dimensions	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
8.3	O
approximations	O
for	O
gpr	O
with	O
fixed	O
hyperparameters	B
179	O
one	O
problem	O
with	O
the	O
basic	O
gpr	O
algorithm	O
is	O
the	O
fact	O
that	O
the	O
likelihood	B
term	O
requires	O
us	O
to	O
have	O
f-values	O
for	O
the	O
n	O
training	O
points	O
.	O
however	O
,	O
say	O
we	O
only	O
represent	O
m	O
of	O
these	O
values	O
explicitly	O
,	O
and	O
denote	O
these	O
as	O
fm	O
.	O
then	O
the	O
remain-	O
ing	O
f-values	O
in	O
r	O
denoted	O
fn−m	O
have	O
a	O
conditional	B
distribution	O
p	O
(	O
fn−m|fm	O
)	O
,	O
the	O
mean	O
of	O
which	O
is	O
given	O
by	O
e	O
[	O
fn−m|fm	O
]	O
=	O
k	O
(	O
n−m	O
)	O
mk−1	O
mmfm.3	O
say	O
we	O
replace	O
the	O
true	O
likelihood	B
term	O
for	O
the	O
points	O
in	O
r	O
by	O
n	O
(	O
yn−m|e	O
[	O
fn−m|fm	O
]	O
,	O
σ2	O
ni	O
)	O
.	O
including	O
also	O
the	O
likelihood	B
contribution	O
of	O
the	O
points	O
in	O
set	B
i	O
we	O
have	O
q	O
(	O
y|fm	O
)	O
=	O
n	O
(	O
y|knmk−1	O
ni	O
)	O
,	O
(	O
8.20	O
)	O
mmfm	O
,	O
σ2	O
which	O
can	O
also	O
be	O
written	O
as	O
q	O
(	O
y|fm	O
)	O
=	O
n	O
(	O
y|e	O
[	O
f|fm	O
]	O
,	O
σ2	O
ni	O
)	O
.	O
the	O
key	O
feature	O
here	O
is	O
that	O
we	O
have	O
absorbed	O
the	O
information	O
in	O
all	O
n	O
points	O
of	O
d	O
into	O
the	O
m	O
points	O
in	O
i.	O
the	O
form	O
of	O
q	O
(	O
y|fm	O
)	O
in	O
eq	O
.	O
(	O
8.20	O
)	O
might	O
seem	O
rather	O
arbitrary	O
,	O
but	O
in	O
fact	O
it	O
can	O
be	O
shown	O
that	O
if	O
we	O
consider	O
minimizing	O
kl	O
(	O
q	O
(	O
f|y	O
)	O
||p	O
(	O
f|y	O
)	O
)	O
,	O
the	O
kl-	O
divergence	O
between	O
the	O
approximating	O
distribution	O
q	O
(	O
f|y	O
)	O
and	O
the	O
true	O
posterior	O
p	O
(	O
f|y	O
)	O
over	O
all	O
q	O
distributions	O
of	O
the	O
form	O
q	O
(	O
f|y	O
)	O
∝	O
p	O
(	O
f	O
)	O
r	O
(	O
fm	O
)	O
where	O
r	O
is	O
positive	O
and	O
depends	O
on	O
fm	O
only	O
,	O
this	O
is	O
the	O
form	O
we	O
obtain	O
.	O
see	B
seeger	O
[	O
2003	O
,	O
lemma	O
4.1	O
and	O
sec	O
.	O
c.2.1	O
]	O
for	O
detailed	O
derivations	O
,	O
and	O
also	O
csat´o	O
[	O
2002	O
,	O
sec	O
.	O
3.3	O
]	O
.	O
to	O
make	O
predictions	O
we	O
ﬁrst	O
have	O
to	O
compute	O
the	O
posterior	O
distribution	O
mmkmn	O
so	O
that	O
e	O
[	O
f|fm	O
]	O
=	O
p	O
>	O
fm	O
.	O
then	O
q	O
(	O
fm|y	O
)	O
.	O
deﬁne	O
the	O
shorthand	O
p	O
=	O
k−1	O
we	O
have	O
q	O
(	O
y|fm	O
)	O
∝	O
exp	O
(	O
cid:0	O
)	O
−	O
1	O
(	O
y	O
−	O
p	O
>	O
fm	O
)	O
>	O
(	O
y	O
−	O
p	O
>	O
fm	O
)	O
(	O
cid:1	O
)	O
.	O
2σ2	O
n	O
q	O
(	O
fm|y	O
)	O
∝	O
exp	O
(	O
cid:0	O
)	O
−	O
1	O
combining	O
this	O
with	O
the	O
prior	O
p	O
(	O
fm	O
)	O
∝	O
exp	O
(	O
−f	O
>	O
mk−1	O
p	O
p	O
>	O
)	O
fm	O
+	O
which	O
can	O
be	O
recognized	O
as	O
a	O
gaussian	O
n	O
(	O
µ	O
,	O
a	O
)	O
with	O
a−1	O
=	O
σ−2	O
µ	O
=	O
σ−2	O
n	O
(	O
σ2	O
n	O
ap	O
y	O
=	O
kmm	O
(	O
σ2	O
mm	O
+	O
p	O
p	O
>	O
)	O
=	O
σ−2	O
f	O
>	O
m	O
(	O
k−1	O
nk−1	O
nkmm	O
+	O
kmnknm	O
)	O
−1kmny	O
.	O
mm	O
+	O
1	O
σ2	O
n	O
2	O
mmfm/2	O
)	O
we	O
obtain	O
y	O
>	O
p	O
>	O
fm	O
1	O
σ2	O
n	O
(	O
cid:1	O
)	O
,	O
n	O
k−1	O
mm	O
(	O
σ2	O
nkmm	O
+	O
kmnknm	O
)	O
k−1	O
mm	O
,	O
(	O
8.21	O
)	O
(	O
8.22	O
)	O
(	O
8.23	O
)	O
(	O
8.24	O
)	O
thus	O
the	O
predictive	B
mean	O
is	O
given	O
by	O
eq	O
[	O
f	O
(	O
x∗	O
)	O
]	O
=	O
km	O
(	O
x∗	O
)	O
>	O
k−1	O
=	O
km	O
(	O
x∗	O
)	O
>	O
(	O
σ2	O
(	O
8.25	O
)	O
(	O
8.26	O
)	O
which	O
turns	O
out	O
to	O
be	O
just	O
the	O
same	O
as	O
the	O
predictive	B
mean	O
under	O
the	O
sr	O
model	B
,	O
as	O
given	O
in	O
eq	O
.	O
(	O
8.14	O
)	O
.	O
however	O
,	O
the	O
predictive	B
variance	O
is	O
diﬀerent	O
.	O
the	O
argument	O
is	O
the	O
same	O
as	O
in	O
eq	O
.	O
(	O
3.23	O
)	O
and	O
yields	O
mmµ	O
nkmm	O
+	O
kmnknm	O
)	O
−1kmny	O
,	O
vq	O
[	O
f	O
(	O
x∗	O
)	O
]	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
km	O
(	O
x∗	O
)	O
>	O
k−1	O
+	O
km	O
(	O
x∗	O
)	O
>	O
k−1	O
mmkm	O
(	O
x∗	O
)	O
mmcov	O
(	O
fm|y	O
)	O
k−1	O
mmkm	O
(	O
x∗	O
)	O
=	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
km	O
(	O
x∗	O
)	O
>	O
k−1	O
+	O
σ2	O
nkm	O
(	O
x∗	O
)	O
>	O
(	O
σ2	O
mmkm	O
(	O
x∗	O
)	O
nkmm	O
+	O
kmnknm	O
)	O
−1km	O
(	O
x∗	O
)	O
.	O
(	O
8.27	O
)	O
3there	O
is	O
no	O
a	O
priori	O
reason	O
why	O
the	O
m	O
points	O
chosen	O
have	O
to	O
be	O
a	O
subset	O
of	O
the	O
n	O
points	O
in	O
d—they	O
could	O
be	O
disjoint	O
from	O
the	O
training	O
set	B
.	O
however	O
,	O
for	O
our	O
derivations	O
below	O
we	O
will	O
consider	O
them	O
to	O
be	O
a	O
subset	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
180	O
approximation	O
methods	O
for	O
large	O
datasets	O
notice	O
that	O
predictive	B
variance	O
is	O
the	O
sum	O
of	O
the	O
predictive	B
variance	O
under	O
the	O
sr	O
model	B
(	O
last	O
term	O
in	O
eq	O
.	O
(	O
8.27	O
)	O
)	O
plus	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
km	O
(	O
x∗	O
)	O
>	O
k−1	O
mmkm	O
(	O
x∗	O
)	O
which	O
is	O
the	O
predictive	B
variance	O
at	O
x∗	O
given	O
fm	O
.	O
thus	O
eq	O
.	O
(	O
8.27	O
)	O
is	O
never	O
smaller	O
than	O
the	O
sr	O
predictive	B
variance	O
and	O
will	O
become	O
close	O
to	O
k	O
(	O
x∗	O
,	O
x∗	O
)	O
when	O
x∗	O
is	O
far	O
away	O
from	O
the	O
points	O
in	O
set	B
i.	O
as	O
for	O
the	O
sr	O
model	B
it	O
takes	O
time	O
o	O
(	O
m2n	O
)	O
to	O
carry	O
out	O
the	O
necessary	O
matrix	B
computations	O
.	O
after	O
this	O
the	O
prediction	B
of	O
the	O
mean	O
for	O
a	O
new	O
test	O
point	O
takes	O
time	O
o	O
(	O
m	O
)	O
,	O
and	O
the	O
predictive	B
variance	O
takes	O
o	O
(	O
m2	O
)	O
.	O
we	O
have	O
q	O
(	O
y|fm	O
)	O
=	O
n	O
(	O
y|p	O
>	O
fm	O
,	O
σ2	O
ni	O
)	O
and	O
p	O
(	O
fm	O
)	O
=	O
n	O
(	O
0	O
,	O
kmm	O
)	O
.	O
by	O
integrat-	O
ing	O
out	O
fm	O
we	O
ﬁnd	O
that	O
y	O
∼	O
n	O
(	O
0	O
,	O
˜k	O
+	O
σ2	O
nin	O
)	O
.	O
thus	O
the	O
marginal	B
likelihood	I
for	O
the	O
projected	B
process	I
approximation	I
is	O
the	O
same	O
as	O
that	O
for	O
the	O
sr	O
model	B
eq	O
.	O
(	O
8.17	O
)	O
.	O
again	O
the	O
question	O
of	O
how	O
to	O
choose	O
which	O
points	O
go	O
into	O
the	O
set	B
i	O
arises	O
.	O
csat´o	O
and	O
opper	O
[	O
2002	O
]	O
present	O
a	O
method	O
in	O
which	O
the	O
training	O
examples	O
are	O
presented	O
sequentially	O
(	O
in	O
an	O
“	O
on-line	O
”	O
fashion	O
)	O
.	O
given	O
the	O
current	O
active	O
set	B
i	O
one	O
can	O
compute	O
the	O
novelty	O
of	O
a	O
new	O
input	O
point	O
;	O
if	O
this	O
is	O
large	O
,	O
then	O
this	O
point	O
is	O
added	O
to	O
i	O
,	O
otherwise	O
the	O
point	O
is	O
added	O
to	O
r.	O
to	O
be	O
precise	O
,	O
the	O
novelty	O
of	O
an	O
input	O
x	O
is	O
computed	O
as	O
k	O
(	O
x	O
,	O
x	O
)	O
−	O
km	O
(	O
x	O
)	O
>	O
k−1	O
mmk	O
(	O
x	O
)	O
,	O
which	O
can	O
be	O
recognized	O
as	O
the	O
predictive	B
variance	O
at	O
x	O
given	O
non-noisy	O
observations	O
at	O
the	O
points	O
in	O
i.	O
if	O
the	O
active	O
set	B
gets	O
larger	O
than	O
some	O
preset	O
maximum	O
size	O
,	O
then	O
points	O
can	O
be	O
deleted	O
from	O
i	O
,	O
as	O
speciﬁed	O
in	O
section	O
3.3	O
of	O
csat´o	O
and	O
opper	O
[	O
2002	O
]	O
.	O
later	O
work	O
by	O
csat´o	O
et	O
al	O
.	O
[	O
2002	O
]	O
replaced	O
the	O
dependence	O
of	O
the	O
algorithm	O
described	O
above	O
on	O
the	O
input	O
sequence	O
by	O
an	O
expectation-propagation	O
type	O
algorithm	O
(	O
see	B
section	O
3.6	O
)	O
.	O
as	O
an	O
alternative	O
method	O
for	O
selecting	O
the	O
active	O
set	B
,	O
seeger	O
et	O
al	O
.	O
[	O
2003	O
]	O
suggest	O
using	O
a	O
greedy	O
subset	O
selection	O
method	O
as	O
per	O
algorithm	O
8.1.	O
com-	O
putation	O
of	O
the	O
information	O
gain	O
criterion	O
after	O
incorporating	O
a	O
new	O
site	O
takes	O
o	O
(	O
mn	O
)	O
and	O
is	O
thus	O
too	O
expensive	O
to	O
use	O
as	O
a	O
selection	O
criterion	O
.	O
however	O
,	O
an	O
ap-	O
proximation	O
to	O
the	O
information	O
gain	O
can	O
be	O
computed	O
cheaply	O
(	O
see	B
seeger	O
et	O
al	O
.	O
[	O
2003	O
,	O
eq	O
.	O
3	O
]	O
and	O
seeger	O
[	O
2003	O
,	O
sec	O
.	O
c.4.2	O
]	O
for	O
further	O
details	O
)	O
and	O
this	O
allows	O
the	O
greedy	O
subset	O
algorithm	O
to	O
be	O
run	O
on	O
all	O
points	O
in	O
r	O
on	O
each	O
iteration	O
.	O
8.3.5	O
bayesian	O
committee	O
machine	O
tresp	O
[	O
2000	O
]	O
introduced	O
the	O
bayesian	O
committee	O
machine	O
(	O
bcm	O
)	O
as	O
a	O
way	O
of	O
speeding	O
up	O
gaussian	O
process	B
regression	O
.	O
let	O
f∗	O
be	O
the	O
vector	O
of	O
function	B
val-	O
ues	O
at	O
the	O
test	O
locations	O
.	O
under	O
gpr	O
we	O
obtain	O
a	O
predictive	B
gaussian	O
distri-	O
bution	O
for	O
p	O
(	O
f∗|d	O
)	O
.	O
for	O
the	O
bcm	O
we	O
split	O
the	O
dataset	B
into	O
p	O
parts	O
d1	O
,	O
.	O
.	O
.	O
,	O
dp	O
qp	O
where	O
di	O
=	O
(	O
xi	O
,	O
yi	O
)	O
and	O
make	O
the	O
approximation	O
that	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yp|f∗	O
,	O
x	O
)	O
’	O
qp	O
i=1	O
p	O
(	O
yi|f∗	O
,	O
xi	O
)	O
.	O
under	O
this	O
approximation	O
we	O
have	O
i=1	O
p	O
(	O
f∗|di	O
)	O
p	O
(	O
yi|f∗	O
,	O
xi	O
)	O
=	O
c	O
pp−1	O
(	O
f∗	O
)	O
q	O
(	O
f∗|d1	O
,	O
.	O
.	O
.	O
,	O
dp	O
)	O
∝	O
p	O
(	O
f∗	O
)	O
py	O
i=1	O
,	O
(	O
8.28	O
)	O
where	O
c	O
is	O
a	O
normalization	O
constant	O
.	O
using	O
the	O
fact	O
that	O
the	O
terms	O
in	O
the	O
numerator	O
and	O
denomination	O
are	O
all	O
gaussian	O
distributions	O
over	O
f∗	O
it	O
is	O
easy	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
8.3	O
approximations	O
for	O
gpr	O
with	O
fixed	O
hyperparameters	B
181	O
to	O
show	O
(	O
see	B
exercise	O
8.7.1	O
)	O
that	O
the	O
predictive	B
mean	O
and	O
covariance	B
for	O
f∗	O
are	O
given	O
by	O
eq	O
[	O
f∗|d	O
]	O
=	O
[	O
covq	O
(	O
f∗|d	O
)	O
]	O
[	O
cov	O
(	O
f∗|di	O
)	O
]	O
−1e	O
[	O
f∗|di	O
]	O
,	O
px	O
px	O
(	O
8.29	O
)	O
(	O
8.30	O
)	O
[	O
covq	O
(	O
f∗|d	O
)	O
]	O
−1	O
=	O
−	O
(	O
p	O
−	O
1	O
)	O
k−1∗∗	O
+	O
i=1	O
[	O
cov	O
(	O
f∗|di	O
)	O
]	O
−1	O
,	O
i=1	O
where	O
k∗∗	O
is	O
the	O
covariance	B
matrix	I
evaluated	O
at	O
the	O
test	O
points	O
.	O
here	O
e	O
[	O
f∗|di	O
]	O
and	O
cov	O
(	O
f∗|di	O
)	O
are	O
the	O
mean	O
and	O
covariance	B
of	O
the	O
predictions	O
for	O
f∗	O
given	O
di	O
,	O
as	O
given	O
in	O
eqs	O
.	O
(	O
2.23	O
)	O
and	O
(	O
2.24	O
)	O
.	O
note	O
that	O
eq	O
.	O
(	O
8.29	O
)	O
has	O
an	O
interesting	O
form	O
in	O
that	O
the	O
predictions	O
from	O
each	O
part	O
of	O
the	O
dataset	B
are	O
weighted	O
by	O
the	O
inverse	O
predictive	B
covariance	O
.	O
we	O
are	O
free	O
to	O
choose	O
how	O
to	O
partition	O
the	O
dataset	B
d.	O
this	O
has	O
two	O
aspects	O
,	O
the	O
number	O
of	O
partitions	O
and	O
the	O
assignment	O
of	O
data	O
points	O
to	O
the	O
partitions	O
.	O
if	O
we	O
wish	O
each	O
partition	O
to	O
have	O
size	O
m	O
,	O
then	O
p	O
=	O
n/m	O
.	O
tresp	O
[	O
2000	O
]	O
used	O
a	O
random	O
assignment	O
of	O
data	O
points	O
to	O
partitions	O
but	O
schwaighofer	O
and	O
tresp	O
[	O
2003	O
]	O
recommend	O
that	O
clustering	O
the	O
data	O
(	O
e.g	O
.	O
with	O
p-means	O
clustering	O
)	O
can	O
lead	O
to	O
improved	O
performance	O
.	O
however	O
,	O
note	O
that	O
compared	O
to	O
the	O
greedy	O
schemes	O
used	O
above	O
clustering	O
does	O
not	O
make	O
use	O
of	O
the	O
target	O
y	O
values	O
,	O
only	O
the	O
inputs	O
x.	O
although	O
it	O
is	O
possible	O
to	O
make	O
predictions	O
for	O
any	O
number	O
of	O
test	O
points	O
n∗	O
,	O
this	O
slows	O
the	O
method	O
down	O
as	O
it	O
involves	O
the	O
inversion	O
of	O
n∗	O
×	O
n∗	O
matrices	O
.	O
schwaighofer	O
and	O
tresp	O
[	O
2003	O
]	O
recommend	O
making	O
test	O
predictions	O
on	O
blocks	O
of	O
size	O
m	O
so	O
that	O
all	O
matrices	O
are	O
of	O
the	O
same	O
size	O
.	O
in	O
this	O
case	O
the	O
computational	O
complexity	O
of	O
bcm	O
is	O
o	O
(	O
pm3	O
)	O
=	O
o	O
(	O
m2n	O
)	O
for	O
predicting	O
m	O
test	O
points	O
,	O
or	O
o	O
(	O
mn	O
)	O
per	O
test	O
point	O
.	O
the	O
bcm	O
approach	O
is	O
transductive	O
[	O
vapnik	O
,	O
1995	O
]	O
rather	O
than	O
inductive	B
,	O
in	O
the	O
sense	O
that	O
the	O
method	O
computes	O
a	O
test-set	O
dependent	O
model	B
making	O
use	O
of	O
the	O
test	O
set	B
input	O
locations	O
.	O
note	O
also	O
that	O
if	O
we	O
wish	O
to	O
make	O
a	O
prediction	B
at	O
just	O
one	O
test	O
point	O
,	O
it	O
would	O
be	O
necessary	O
to	O
“	O
hallucinate	O
”	O
some	O
extra	O
test	O
points	O
as	O
eq	O
.	O
(	O
8.28	O
)	O
generally	O
becomes	O
a	O
better	O
approximation	O
as	O
the	O
number	O
of	O
test	O
points	O
increases	O
.	O
8.3.6	O
iterative	O
solution	O
of	O
linear	B
systems	O
one	O
straightforward	O
method	O
to	O
speed	O
up	O
gp	O
regression	B
is	O
to	O
note	O
that	O
the	O
lin-	O
ear	O
system	O
(	O
k	O
+	O
σ2	O
ni	O
)	O
v	O
=	O
y	O
can	O
be	O
solved	O
by	O
an	O
iterative	O
method	O
,	O
for	O
example	O
conjugate	O
gradients	O
(	O
cg	O
)	O
.	O
(	O
see	B
golub	O
and	O
van	O
loan	O
[	O
1989	O
,	O
sec	O
.	O
10.2	O
]	O
for	O
fur-	O
ther	O
details	O
on	O
the	O
cg	O
method	O
.	O
)	O
conjugate	O
gradients	O
gives	O
the	O
exact	O
solution	O
(	O
ignoring	O
round-oﬀ	O
errors	O
)	O
if	O
run	O
for	O
n	O
iterations	O
,	O
but	O
it	O
will	O
give	O
an	O
approxi-	O
mate	O
solution	O
if	O
terminated	O
earlier	O
,	O
say	O
after	O
k	O
iterations	O
,	O
with	O
time	O
complexity	O
o	O
(	O
kn2	O
)	O
.	O
this	O
method	O
has	O
been	O
suggested	O
by	O
wahba	O
et	O
al	O
.	O
[	O
1995	O
]	O
(	O
in	O
the	O
context	O
of	O
numerical	O
weather	O
prediction	B
)	O
and	O
by	O
gibbs	O
and	O
mackay	O
[	O
1997	O
]	O
(	O
in	O
the	O
con-	O
text	O
of	O
general	O
gp	O
regression	B
)	O
.	O
cg	O
methods	O
have	O
also	O
been	O
used	O
in	O
the	O
context	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
182	O
approximation	O
methods	O
for	O
large	O
datasets	O
method	O
m	O
sd	O
sr	O
pp	O
bcm	O
smse	O
0.0813	O
±	O
0.0198	O
0.0532	O
±	O
0.0046	O
0.0398	O
±	O
0.0036	O
0.0290	O
±	O
0.0013	O
0.0200	O
±	O
0.0008	O
0.0351	O
±	O
0.0036	O
0.0259	O
±	O
0.0014	O
0.0193	O
±	O
0.0008	O
0.0150	O
±	O
0.0005	O
0.0110	O
±	O
0.0004	O
0.0351	O
±	O
0.0036	O
0.0259	O
±	O
0.0014	O
0.0193	O
±	O
0.0008	O
0.0150	O
±	O
0.0005	O
0.0110	O
±	O
0.0004	O
0.0314	O
±	O
0.0046	O
0.0281	O
±	O
0.0055	O
0.0180	O
±	O
0.0010	O
0.0136	O
±	O
0.0007	O
msll	O
-1.4291	O
±	O
0.0558	O
-1.5834	O
±	O
0.0319	O
-1.7149	O
±	O
0.0293	O
-1.8611	O
±	O
0.0204	O
-2.0241	O
±	O
0.0151	O
-1.6088	O
±	O
0.0984	O
-1.8185	O
±	O
0.0357	O
-1.9728	O
±	O
0.0207	O
-2.1126	O
±	O
0.0185	O
-2.2474	O
±	O
0.0204	O
-1.6940	O
±	O
0.0528	O
-1.8423	O
±	O
0.0286	O
-1.9823	O
±	O
0.0233	O
-2.1125	O
±	O
0.0202	O
-2.2399	O
±	O
0.0160	O
-1.7066	O
±	O
0.0550	O
-1.7807	O
±	O
0.0820	O
-2.0081	O
±	O
0.0321	O
-2.1364	O
±	O
0.0266	O
mean	O
runtime	O
(	O
s	O
)	O
0.8	O
2.1	O
6.5	O
25.0	O
100.7	O
11.0	O
27.0	O
79.5	O
284.8	O
927.6	O
17.3	O
41.4	O
95.1	O
354.2	O
964.5	O
506.4	O
660.5	O
1043.2	O
1920.7	O
256	O
512	O
1024	O
2048	O
4096	O
256	O
512	O
1024	O
2048	O
4096	O
256	O
512	O
1024	O
2048	O
4096	O
256	O
512	O
1024	O
2048	O
table	O
8.1	O
:	O
test	O
results	O
on	O
the	O
inverse	O
dynamics	O
problem	O
for	O
a	O
number	O
of	O
diﬀerent	O
methods	O
.	O
ten	O
repetitions	O
were	O
used	O
,	O
the	O
mean	O
loss	O
is	O
shown	O
±	O
one	O
standard	O
deviation	O
.	O
of	O
laplace	O
gpc	O
,	O
where	O
linear	B
systems	O
are	O
solved	O
repeatedly	O
to	O
obtain	O
the	O
map	O
solution	O
˜f	O
(	O
see	B
sections	O
3.4	O
and	O
3.5	O
for	O
details	O
)	O
.	O
one	O
way	O
that	O
the	O
cg	O
method	O
can	O
be	O
speeded	O
up	O
is	O
by	O
using	O
an	O
approximate	O
rather	O
than	O
exact	O
matrix-vector	O
multiplication	O
.	O
for	O
example	O
,	O
recent	O
work	O
by	O
yang	O
et	O
al	O
.	O
[	O
2005	O
]	O
uses	O
the	O
improved	O
fast	O
gauss	O
transform	O
for	O
this	O
purpose	O
.	O
8.3.7	O
comparison	O
of	O
approximate	O
gpr	O
methods	O
above	O
we	O
have	O
presented	O
six	O
approximation	O
methods	O
for	O
gpr	O
.	O
of	O
these	O
,	O
we	O
retain	O
only	O
those	O
methods	O
which	O
scale	O
linearly	O
with	O
n	O
,	O
so	O
the	O
iterative	O
solu-	O
tion	O
of	O
linear	B
systems	O
must	O
be	O
discounted	O
.	O
also	O
we	O
discount	O
the	O
nystr¨om	O
ap-	O
proximation	O
in	O
preference	O
to	O
the	O
sr	O
method	O
,	O
leaving	O
four	O
alternatives	O
:	O
subset	B
of	I
regressors	I
(	O
sr	O
)	O
,	O
subset	O
of	O
data	O
(	O
sd	O
)	O
,	O
projected	O
process	O
(	O
pp	O
)	O
and	O
bayesian	O
committee	O
machine	O
(	O
bcm	O
)	O
.	O
table	O
8.1	O
shows	O
results	O
of	O
the	O
four	O
methods	O
on	O
the	O
robot	B
arm	O
inverse	O
dy-	O
namics	O
problem	O
described	O
in	O
section	O
2.5	O
which	O
has	O
d	O
=	O
21	O
input	O
variables	O
,	O
44,484	O
training	O
examples	O
and	O
4,449	O
test	O
examples	O
.	O
as	O
in	O
section	O
2.5	O
we	O
used	O
the	O
squared	B
exponential	I
covariance	O
function	B
with	O
a	O
separate	O
length-scale	B
pa-	O
rameter	O
for	O
each	O
of	O
the	O
21	O
input	O
dimensions	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
8.3	O
approximations	O
for	O
gpr	O
with	O
fixed	O
hyperparameters	B
183	O
method	O
sd	O
sr	O
pp	O
bcm	O
storage	O
o	O
(	O
m2	O
)	O
o	O
(	O
m3	O
)	O
o	O
(	O
mn	O
)	O
o	O
(	O
m2n	O
)	O
o	O
(	O
mn	O
)	O
o	O
(	O
m2n	O
)	O
o	O
(	O
mn	O
)	O
variance	O
initialization	O
mean	O
o	O
(	O
m2	O
)	O
o	O
(	O
m	O
)	O
o	O
(	O
m2	O
)	O
o	O
(	O
m	O
)	O
o	O
(	O
m2	O
)	O
o	O
(	O
m	O
)	O
o	O
(	O
mn	O
)	O
o	O
(	O
mn	O
)	O
table	O
8.2	O
:	O
a	O
comparison	O
of	O
the	O
space	O
and	O
time	O
complexity	O
of	O
the	O
four	O
methods	O
using	O
random	O
selection	O
of	O
subsets	O
.	O
initialization	O
gives	O
the	O
time	O
needed	O
to	O
carry	O
out	O
preliminary	O
matrix	B
computations	O
before	O
the	O
test	O
point	O
x∗	O
is	O
known	O
.	O
mean	O
(	O
resp	O
.	O
variance	O
)	O
refers	O
to	O
the	O
time	O
needed	O
to	O
compute	O
the	O
predictive	B
mean	O
(	O
variance	O
)	O
at	O
x∗	O
.	O
for	O
the	O
sd	O
method	O
a	O
subset	O
of	O
the	O
training	O
data	O
of	O
size	O
m	O
was	O
selected	O
at	O
random	O
,	O
and	O
the	O
hyperparameters	B
were	O
set	B
by	O
optimizing	O
the	O
marginal	B
likeli-	O
hood	O
on	O
this	O
subset	O
.	O
as	O
ard	O
was	O
used	O
,	O
this	O
involved	O
the	O
optimization	O
of	O
d	O
+	O
2	O
hyperparameters	B
.	O
this	O
process	B
was	O
repeated	O
10	O
times	O
,	O
giving	O
rise	O
to	O
the	O
mean	O
and	O
standard	O
deviation	O
recorded	O
in	O
table	O
8.1.	O
for	O
the	O
sr	O
,	O
pp	O
and	O
bcm	O
meth-	O
ods	O
,	O
the	O
same	O
subsets	O
of	O
the	O
data	O
and	O
hyperparameter	O
vectors	O
were	O
used	O
as	O
had	O
been	O
obtained	O
from	O
the	O
sd	O
experiments.4	O
note	O
that	O
the	O
m	O
=	O
4096	O
result	O
is	O
not	O
available	O
for	O
bcm	O
as	O
this	O
gave	O
an	O
out-of-memory	O
error	B
.	O
these	O
experiments	O
were	O
conducted	O
on	O
a	O
2.0	O
ghz	O
twin	O
processor	O
machine	O
with	O
3.74	O
gb	O
of	O
ram	O
.	O
the	O
code	O
for	O
all	O
four	O
methods	O
was	O
written	O
in	O
matlab.5	O
a	O
summary	O
of	O
the	O
time	O
complexities	O
for	O
the	O
four	O
methods	O
are	O
given	O
in	O
table	O
8.2.	O
thus	O
for	O
a	O
test	O
set	B
of	O
size	O
n∗	O
and	O
using	O
full	O
(	O
mean	O
and	O
variance	O
)	O
predictions	O
we	O
ﬁnd	O
that	O
the	O
sd	O
method	O
has	O
time	O
complexity	O
o	O
(	O
m3	O
)	O
+	O
o	O
(	O
m2n∗	O
)	O
,	O
for	O
the	O
sr	O
and	O
pp	O
methods	O
it	O
is	O
o	O
(	O
m2n	O
)	O
+	O
o	O
(	O
m2n∗	O
)	O
,	O
and	O
for	O
the	O
bcm	O
method	O
it	O
is	O
o	O
(	O
mnn∗	O
)	O
.	O
assuming	O
that	O
n∗	O
≥	O
m	O
these	O
reduce	O
to	O
o	O
(	O
m2n∗	O
)	O
,	O
o	O
(	O
m2n	O
)	O
and	O
o	O
(	O
mnn∗	O
)	O
respectively	O
.	O
these	O
complexities	O
are	O
in	O
broad	O
agreement	O
with	O
the	O
timings	O
in	O
table	O
8.1.	O
the	O
results	O
from	O
table	O
8.1	O
are	O
plotted	O
in	O
figure	O
8.1.	O
as	O
we	O
would	O
expect	O
,	O
the	O
general	O
trend	O
is	O
that	O
as	O
m	O
increases	O
the	O
smse	O
and	O
msll	O
scores	O
decrease	O
.	O
notice	O
that	O
it	O
is	O
well	O
worth	O
doing	O
runs	O
with	O
small	O
m	O
so	O
as	O
to	O
obtain	O
a	O
learning	B
curve	I
with	O
respect	O
to	O
m	O
;	O
this	O
helps	O
in	O
getting	O
a	O
feeling	O
of	O
how	O
useful	O
runs	O
at	O
large	O
m	O
will	O
be	O
.	O
both	O
in	O
terms	O
of	O
smse	O
and	O
msll	O
we	O
see	B
(	O
not	O
surprisingly	O
)	O
that	O
sd	O
is	O
inferior	O
to	O
the	O
other	O
methods	O
,	O
all	O
of	O
which	O
have	O
similar	O
performance	O
.	O
these	O
results	O
were	O
obtained	O
using	O
a	O
random	O
selection	O
of	O
the	O
active	O
set	B
.	O
some	O
experiments	O
were	O
also	O
carried	O
out	O
using	O
active	O
selection	O
for	O
the	O
sd	O
method	O
(	O
ivm	O
)	O
and	O
for	O
the	O
sr	O
method	O
but	O
these	O
did	O
not	O
lead	O
to	O
signiﬁcant	O
improve-	O
ments	O
in	O
performance	O
.	O
for	O
bcm	O
we	O
also	O
experimented	O
with	O
the	O
use	O
of	O
p-means	O
clustering	O
instead	O
of	O
random	O
assignment	O
to	O
partitions	O
;	O
again	O
this	O
did	O
not	O
lead	O
to	O
signiﬁcant	O
improvements	O
in	O
performance	O
.	O
overall	O
on	O
this	O
dataset	B
our	O
con-	O
4in	O
the	O
bcm	O
case	O
it	O
was	O
only	O
the	O
hyperparameters	B
that	O
were	O
re-used	O
;	O
the	O
data	O
was	O
parti-	O
tioned	O
randomly	O
into	O
blocks	O
of	O
size	O
m.	O
5we	O
thank	O
anton	O
schwaighofer	O
for	O
making	O
his	O
bcm	O
code	O
available	O
to	O
us	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
184	O
approximation	O
methods	O
for	O
large	O
datasets	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
8.1	O
:	O
panel	O
(	O
a	O
)	O
:	O
plot	O
of	O
smse	O
against	O
m.	O
panel	O
(	O
b	O
)	O
shows	O
the	O
msll	O
for	O
the	O
four	O
methods	O
.	O
the	O
error	B
bars	O
denote	O
one	O
standard	O
deviation	O
.	O
for	O
clarity	O
in	O
both	O
panels	O
the	O
bcm	O
results	O
are	O
slightly	O
displaced	O
horizontally	O
w.r.t	O
.	O
the	O
sr	O
results	O
.	O
clusion	O
is	O
that	O
for	O
ﬁxed	O
m	O
sr	O
or	O
pp	O
are	O
the	O
methods	O
of	O
choice	O
,	O
as	O
bcm	O
has	O
longer	O
running	O
times	O
for	O
similar	O
performance	O
.	O
however	O
,	O
notice	O
that	O
if	O
we	O
com-	O
pare	O
on	O
runtime	O
,	O
then	O
sd	O
for	O
m	O
=	O
4096	O
is	O
competitive	O
with	O
the	O
sr	O
,	O
pp	O
and	O
bcm	O
results	O
for	O
m	O
=	O
1024	O
on	O
both	O
time	O
and	O
performance	O
.	O
in	O
the	O
above	O
experiments	O
the	O
hyperparameters	B
for	O
all	O
methods	O
were	O
set	B
by	O
optimizing	O
the	O
marginal	B
likelihood	I
of	O
the	O
sd	O
model	B
of	O
size	O
m.	O
this	O
means	O
that	O
we	O
get	O
a	O
direct	O
comparison	O
of	O
the	O
diﬀerent	O
methods	O
using	O
the	O
same	O
hyperparam-	O
eters	O
and	O
subsets	O
.	O
however	O
,	O
one	O
could	O
alternatively	O
optimize	O
the	O
(	O
approximate	O
)	O
marginal	B
likelihood	I
for	O
each	O
method	O
(	O
see	B
section	O
8.5	O
)	O
and	O
then	O
compare	O
results	O
.	O
notice	O
that	O
the	O
hyperparameters	B
which	O
optimize	O
the	O
approximate	O
marginal	B
like-	O
lihood	B
may	O
depend	O
on	O
the	O
method	O
.	O
for	O
example	O
figure	O
5.3	O
(	O
b	O
)	O
shows	O
that	O
the	O
maximum	O
in	O
the	O
marginal	B
likelihood	I
occurs	O
at	O
shorter	O
length-scales	O
as	O
the	O
amount	O
of	O
data	O
increases	O
.	O
this	O
eﬀect	O
has	O
also	O
been	O
observed	O
by	O
v.	O
tresp	O
and	O
a.	O
schwaighofer	O
(	O
pers	O
.	O
comm.	O
,	O
2004	O
)	O
when	O
comparing	O
the	O
sd	O
marginal	B
likeli-	O
hood	O
eq	O
.	O
(	O
8.31	O
)	O
with	O
the	O
full	O
marginal	B
likelihood	I
computed	O
on	O
all	O
n	O
datapoints	O
eq	O
.	O
(	O
5.8	O
)	O
.	O
schwaighofer	O
and	O
tresp	O
[	O
2003	O
]	O
report	O
some	O
experimental	O
comparisons	O
be-	O
tween	O
the	O
bcm	O
method	O
and	O
some	O
other	O
approximation	O
methods	O
for	O
a	O
number	O
of	O
synthetic	O
regression	B
problems	O
.	O
in	O
these	O
experiments	O
they	O
optimized	O
the	O
ker-	O
nel	O
hyperparameters	B
for	O
each	O
method	O
separately	O
.	O
their	O
results	O
are	O
that	O
for	O
ﬁxed	O
m	O
bcm	O
performs	O
as	O
well	O
as	O
or	O
better	O
than	O
the	O
other	O
methods	O
.	O
however	O
,	O
these	O
results	O
depend	O
on	O
factors	O
such	O
as	O
the	O
noise	O
level	O
in	O
the	O
data	O
generating	O
pro-	O
cess	O
;	O
they	O
report	O
(	O
pers	O
.	O
comm.	O
,	O
2005	O
)	O
that	O
for	O
relatively	O
large	O
noise	O
levels	O
bcm	O
no	O
longer	O
displays	O
an	O
advantage	O
.	O
based	O
on	O
the	O
evidence	B
currently	O
available	O
we	O
are	O
unable	O
to	O
provide	O
ﬁrm	O
recommendations	O
for	O
one	O
approximation	O
method	O
over	O
another	O
;	O
further	O
research	O
is	O
required	O
to	O
understand	O
the	O
factors	O
that	O
aﬀect	O
performance	O
.	O
25651210242048409600.050.1smsemsdsr	O
and	O
ppbcm256512102420484096−2.2−1.8−1.4msllmsdppsrbcm	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
8.4	O
approximations	O
for	O
gpc	O
with	O
fixed	O
hyperparameters	B
185	O
8.4	O
approximations	O
for	O
gpc	O
with	O
fixed	O
hy-	O
perparameters	O
the	O
approximation	O
methods	O
for	O
gpc	O
are	O
similar	O
to	O
those	O
for	O
gpr	O
,	O
but	O
need	O
to	O
deal	O
with	O
the	O
non-gaussian	O
likelihood	B
as	O
well	O
,	O
either	O
by	O
using	O
the	O
laplace	O
approximation	O
,	O
see	B
section	O
3.4	O
,	O
or	O
expectation	B
propagation	I
(	O
ep	O
)	O
,	O
see	B
section	O
3.6.	O
in	O
this	O
section	O
we	O
focus	O
mainly	O
on	O
binary	B
classiﬁcation	I
tasks	O
,	O
although	O
some	O
of	O
the	O
methods	O
can	O
also	O
be	O
extended	O
to	O
the	O
multi-class	B
case	O
.	O
pm	O
for	O
the	O
subset	B
of	I
regressors	I
(	O
sr	O
)	O
method	O
we	O
again	O
use	O
the	O
model	B
fsr	O
(	O
x∗	O
)	O
=	O
i=1	O
αik	O
(	O
x∗	O
,	O
xi	O
)	O
with	O
αm	O
∼	O
n	O
(	O
0	O
,	O
k−1	O
mm	O
)	O
.	O
the	O
likelihood	B
is	O
non-gaussian	O
but	O
the	O
optimization	O
problem	O
to	O
ﬁnd	O
the	O
map	O
value	O
of	O
αm	O
is	O
convex	B
and	O
can	O
be	O
obtained	O
using	O
a	O
newton	O
iteration	O
.	O
using	O
the	O
map	O
value	O
ˆαm	O
and	O
the	O
hessian	O
at	O
this	O
point	O
we	O
obtain	O
a	O
predictive	B
mean	O
and	O
variance	O
for	O
f	O
(	O
x∗	O
)	O
which	O
can	O
be	O
fed	O
through	O
the	O
sigmoid	O
function	B
to	O
yield	O
probabilistic	B
predictions	O
.	O
as	O
usual	O
the	O
question	O
of	O
how	O
to	O
choose	O
a	O
subset	O
of	O
points	O
arises	O
;	O
lin	O
et	O
al	O
.	O
[	O
2000	O
]	O
select	O
these	O
using	O
a	O
clustering	O
method	O
,	O
while	O
zhu	O
and	O
hastie	O
[	O
2002	O
]	O
propose	O
a	O
forward	O
selection	O
strategy	O
.	O
the	O
subset	B
of	I
datapoints	I
(	O
sd	O
)	O
method	O
for	O
gpc	O
was	O
proposed	O
in	O
lawrence	O
et	O
al	O
.	O
[	O
2003	O
]	O
,	O
using	O
an	O
ep-style	O
approximation	O
of	O
the	O
posterior	O
,	O
and	O
the	O
diﬀer-	O
ential	O
entropy	B
score	O
(	O
see	B
section	O
8.3.3	O
)	O
to	O
select	O
new	O
sites	O
for	O
inclusion	O
.	O
note	O
that	O
the	O
ep	O
approximation	O
lends	O
itself	O
very	O
naturally	O
to	O
sparsiﬁcation	O
:	O
a	O
sparse	O
model	B
results	O
when	O
some	O
site	O
precisions	O
(	O
see	B
eq	O
.	O
(	O
3.51	O
)	O
)	O
are	O
zero	O
,	O
making	O
the	O
cor-	O
responding	O
likelihood	B
term	O
vanish	O
.	O
a	O
computational	O
gain	O
can	O
thus	O
be	O
achieved	O
by	O
ignoring	O
likelihood	B
terms	O
whose	O
site	O
precisions	O
are	O
very	O
small	O
.	O
the	O
projected	O
process	O
(	O
pp	O
)	O
approximation	O
can	O
also	O
be	O
used	O
with	O
non-	O
gaussian	O
likelihoods	O
.	O
csat´o	O
and	O
opper	O
[	O
2002	O
]	O
present	O
an	O
“	O
online	O
”	O
method	O
where	O
the	O
examples	O
are	O
processed	O
sequentially	O
,	O
while	O
csat´o	O
et	O
al	O
.	O
[	O
2002	O
]	O
give	O
an	O
expectation-propagation	O
type	O
algorithm	O
where	O
multiple	O
sweeps	O
through	O
the	O
training	O
data	O
are	O
permitted	O
.	O
the	O
bayesian	O
committee	O
machine	O
(	O
bcm	O
)	O
has	O
also	O
been	O
generalized	B
to	O
deal	O
with	O
non-gaussian	O
likelihoods	O
in	O
tresp	O
[	O
2000	O
]	O
.	O
as	O
in	O
the	O
gpr	O
case	O
the	O
dataset	B
is	O
broken	O
up	O
into	O
blocks	O
,	O
but	O
now	O
approximate	O
inference	O
is	O
carried	O
out	O
using	O
the	O
laplace	O
approximation	O
in	O
each	O
block	O
to	O
yield	O
an	O
approximate	O
predictive	B
mean	O
eq	O
[	O
f∗|di	O
]	O
and	O
approximate	O
predictive	B
covariance	O
covq	O
(	O
f∗|di	O
)	O
.	O
these	O
predictions	O
are	O
then	O
combined	O
as	O
before	O
using	O
equations	O
8.29	O
and	O
8.30	O
.	O
8.5	O
approximating	O
the	O
marginal	B
likelihood	I
and	O
∗	O
its	O
derivatives	O
we	O
consider	O
approximations	O
ﬁrst	O
for	O
gp	O
regression	B
,	O
and	O
then	O
for	O
gp	O
classiﬁca-	O
tion	O
.	O
for	O
gpr	O
,	O
both	O
the	O
sr	O
and	O
pp	O
methods	O
give	O
rise	O
to	O
the	O
same	O
approximate	O
marginal	B
likelihood	I
as	O
given	O
in	O
eq	O
.	O
(	O
8.17	O
)	O
.	O
for	O
the	O
sd	O
method	O
,	O
a	O
very	O
simple	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
186	O
approximation	O
methods	O
for	O
large	O
datasets	O
2	O
y	O
>	O
2	O
log	O
|kmm	O
+	O
σ2i|−	O
1	O
m	O
(	O
kmm	O
+	O
σ2i	O
)	O
−1ym−	O
m	O
approximation	O
(	O
ignoring	O
the	O
datapoints	O
not	O
in	O
the	O
active	O
set	B
)	O
is	O
given	O
by	O
log	O
psd	O
(	O
ym|xm	O
)	O
=	O
−	O
1	O
2	O
log	O
(	O
2π	O
)	O
,	O
(	O
8.31	O
)	O
where	O
ym	O
is	O
the	O
subvector	O
of	O
y	O
corresponding	O
to	O
the	O
active	O
set	B
;	O
eq	O
.	O
(	O
8.31	O
)	O
is	O
simply	O
the	O
log	O
marginal	O
likelihood	B
under	O
the	O
model	B
ym	O
∼	O
n	O
(	O
0	O
,	O
kmm	O
+	O
σ2i	O
)	O
.	O
for	O
the	O
bcm	O
,	O
a	O
simple	O
approach	O
would	O
be	O
to	O
sum	O
eq	O
.	O
(	O
8.31	O
)	O
evaluated	O
on	O
each	O
partition	O
of	O
the	O
dataset	B
.	O
this	O
ignores	O
interactions	O
between	O
the	O
partitions	O
.	O
tresp	O
and	O
schwaighofer	O
(	O
pers	O
.	O
comm.	O
,	O
2004	O
)	O
have	O
suggested	O
a	O
more	O
sophisti-	O
cated	O
bcm-based	O
method	O
which	O
approximately	O
takes	O
these	O
interactions	O
into	O
account	O
.	O
for	O
gpc	O
under	O
the	O
sr	O
approximation	O
,	O
one	O
can	O
simply	O
use	O
the	O
laplace	O
or	O
ep	O
approximations	O
on	O
the	O
ﬁnite-dimensional	O
model	B
.	O
for	O
sd	O
one	O
can	O
again	O
ignore	O
all	O
datapoints	O
not	O
in	O
the	O
active	O
set	B
and	O
compute	O
an	O
approximation	O
to	O
log	O
p	O
(	O
ym|xm	O
)	O
using	O
either	O
laplace	O
or	O
ep	O
.	O
for	O
the	O
projected	O
process	O
(	O
pp	O
)	O
method	O
,	O
seeger	O
[	O
2003	O
,	O
p.	O
162	O
]	O
suggests	O
the	O
following	O
lower	O
bound	O
z	O
q	O
(	O
f	O
)	O
p	O
(	O
y|f	O
)	O
p	O
(	O
f	O
)	O
q	O
(	O
f	O
)	O
df	O
z	O
z	O
z	O
z	O
nx	O
≥	O
=	O
=	O
log	O
p	O
(	O
y|x	O
)	O
=	O
log	O
p	O
(	O
y|f	O
)	O
p	O
(	O
f	O
)	O
df	O
=	O
log	O
(	O
cid:16	O
)	O
p	O
(	O
y|f	O
)	O
p	O
(	O
f	O
)	O
(	O
cid:17	O
)	O
q	O
(	O
f	O
)	O
q	O
(	O
f	O
)	O
log	O
df	O
(	O
8.32	O
)	O
q	O
(	O
f	O
)	O
log	O
q	O
(	O
y|f	O
)	O
df	O
−	O
kl	O
(	O
q	O
(	O
f	O
)	O
||p	O
(	O
f	O
)	O
)	O
q	O
(	O
fi	O
)	O
log	O
p	O
(	O
yi|fi	O
)	O
dfi	O
−	O
kl	O
(	O
q	O
(	O
fm	O
)	O
||p	O
(	O
fm	O
)	O
)	O
,	O
i=1	O
where	O
q	O
(	O
f	O
)	O
is	O
a	O
shorthand	O
for	O
q	O
(	O
f|y	O
)	O
and	O
eq	O
.	O
(	O
8.32	O
)	O
follows	O
from	O
the	O
equation	O
on	O
the	O
previous	O
line	O
using	O
jensen	O
’	O
s	O
inequality	O
.	O
the	O
kl	O
divergence	O
term	O
can	O
be	O
readily	O
evaluated	O
using	O
eq	O
.	O
(	O
a.23	O
)	O
,	O
and	O
the	O
one-dimensional	O
integrals	B
can	O
be	O
tackled	O
using	O
numerical	O
quadrature	O
.	O
we	O
are	O
not	O
aware	O
of	O
work	O
on	O
extending	O
the	O
bcm	O
approximations	O
to	O
the	O
marginal	B
likelihood	I
to	O
gpc	O
.	O
given	O
the	O
various	O
approximations	O
to	O
the	O
marginal	B
likelihood	I
mentioned	O
above	O
,	O
we	O
may	O
also	O
want	O
to	O
compute	O
derivatives	O
in	O
order	O
to	O
optimize	O
it	O
.	O
clearly	O
it	O
will	O
make	O
sense	O
to	O
keep	O
the	O
active	O
set	B
ﬁxed	O
during	O
the	O
optimization	O
,	O
although	O
note	O
that	O
this	O
clashes	O
with	O
the	O
fact	O
that	O
methods	O
that	O
select	O
the	O
active	O
set	B
might	O
choose	O
a	O
diﬀerent	O
set	B
as	O
the	O
covariance	B
function	I
parameters	O
θ	O
change	O
.	O
for	O
the	O
classiﬁcation	B
case	O
the	O
derivatives	O
can	O
be	O
quite	O
complex	O
due	O
to	O
the	O
fact	O
that	O
site	O
parameters	O
(	O
such	O
as	O
the	O
map	O
values	O
ˆf	O
,	O
see	B
section	O
3.4.1	O
)	O
change	O
as	O
θ	O
changes	O
.	O
(	O
we	O
have	O
already	O
seen	O
an	O
example	O
of	O
this	O
in	O
section	O
5.5	O
for	O
the	O
non-sparse	O
laplace	O
approximation	O
.	O
)	O
seeger	O
[	O
2003	O
,	O
sec	O
.	O
4.8	O
]	O
describes	O
some	O
ex-	O
periments	O
comparing	O
sd	O
and	O
pp	O
methods	O
for	O
the	O
optimization	O
of	O
the	O
marginal	B
likelihood	I
on	O
both	O
regression	B
and	O
classiﬁcation	B
problems	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
8.6	O
appendix	O
:	O
equivalence	O
of	O
sr	O
and	O
gpr	O
using	O
the	O
nystr¨om	O
approximate	O
kernel	B
187	O
8.6	O
appendix	O
:	O
equivalence	O
of	O
sr	O
and	O
gpr	O
us-	O
∗	O
ing	O
the	O
nystr¨om	O
approximate	O
kernel	B
in	O
section	O
8.3	O
we	O
derived	O
the	O
subset	B
of	I
regressors	I
predictors	O
for	O
the	O
mean	O
and	O
variance	O
,	O
as	O
given	O
in	O
equations	O
8.14	O
and	O
8.15.	O
the	O
aim	O
of	O
this	O
appendix	O
is	O
to	O
show	O
that	O
these	O
are	O
equivalent	B
to	O
the	O
predictors	O
that	O
are	O
obtained	O
by	O
replacing	O
k	O
(	O
x	O
,	O
x0	O
)	O
systematically	O
with	O
˜k	O
(	O
x	O
,	O
x0	O
)	O
in	O
the	O
gpr	O
prediction	B
equations	O
2.25	O
and	O
2.26.	O
first	O
for	O
the	O
mean	O
.	O
the	O
gpr	O
predictor	O
is	O
e	O
[	O
f	O
(	O
x∗	O
)	O
]	O
=	O
k	O
(	O
x∗	O
)	O
>	O
(	O
k	O
+	O
σ2	O
replacing	O
all	O
occurrences	O
of	O
k	O
(	O
x	O
,	O
x0	O
)	O
with	O
˜k	O
(	O
x	O
,	O
x0	O
)	O
we	O
obtain	O
ni	O
)	O
−1y	O
.	O
ni	O
)	O
−1y	O
e	O
[	O
˜f	O
(	O
x∗	O
)	O
]	O
=	O
˜k	O
(	O
x∗	O
)	O
>	O
(	O
˜k	O
+	O
σ2	O
=	O
km	O
(	O
x∗	O
)	O
>	O
k−1	O
=	O
σ−2	O
=	O
σ−2	O
=	O
σ−2	O
=	O
km	O
(	O
x∗	O
)	O
>	O
q−1kmny	O
,	O
n	O
km	O
(	O
x∗	O
)	O
>	O
k−1	O
n	O
km	O
(	O
x∗	O
)	O
>	O
k−1	O
n	O
km	O
(	O
x∗	O
)	O
>	O
k−1	O
mm	O
mm	O
mmkmn	O
mmkmn	O
(	O
knmk−1	O
ni	O
)	O
−1y	O
mmkmn	O
+	O
σ2	O
(	O
cid:2	O
)	O
in	O
−	O
knmq−1kmn	O
(	O
cid:3	O
)	O
y	O
(	O
cid:2	O
)	O
im	O
−	O
kmnknmq−1	O
(	O
cid:3	O
)	O
kmny	O
nkmmq−1	O
(	O
cid:3	O
)	O
kmny	O
(	O
cid:2	O
)	O
σ2	O
(	O
8.33	O
)	O
(	O
8.34	O
)	O
(	O
8.35	O
)	O
(	O
8.36	O
)	O
(	O
8.37	O
)	O
(	O
8.38	O
)	O
nkmm	O
+	O
kmnknm	O
,	O
which	O
agrees	O
with	O
eq	O
.	O
(	O
8.14	O
)	O
.	O
equation	O
(	O
8.35	O
)	O
where	O
q	O
=	O
σ2	O
follows	O
from	O
eq	O
.	O
(	O
8.34	O
)	O
by	O
use	O
of	O
the	O
matrix	B
inversion	O
lemma	O
eq	O
.	O
(	O
a.9	O
)	O
and	O
nkmm	O
+	O
kmnknm	O
)	O
q−1	O
.	O
for	O
eq	O
.	O
(	O
8.37	O
)	O
follows	O
from	O
eq	O
.	O
(	O
8.36	O
)	O
using	O
im	O
=	O
(	O
σ2	O
the	O
predictive	B
variance	O
we	O
have	O
v	O
[	O
˜f∗	O
]	O
=	O
˜k	O
(	O
x∗	O
,	O
x∗	O
)	O
−	O
˜k	O
(	O
x∗	O
)	O
>	O
(	O
˜k	O
+	O
σ2	O
ni	O
)	O
−1˜k	O
(	O
x∗	O
)	O
=	O
km	O
(	O
x∗	O
)	O
>	O
k−1	O
km	O
(	O
x∗	O
)	O
>	O
k−1	O
=	O
km	O
(	O
x∗	O
)	O
>	O
k−1	O
mmkm	O
(	O
x∗	O
)	O
−	O
mmkmn	O
(	O
knmk−1	O
mmkm	O
(	O
x∗	O
)	O
−	O
km	O
(	O
x∗	O
)	O
>	O
q−1kmnknmk−1	O
mmkmn	O
+	O
σ2	O
ni	O
)	O
−1knmk−1	O
=	O
km	O
(	O
x∗	O
)	O
>	O
(	O
cid:2	O
)	O
im	O
−	O
q−1kmnknm	O
(	O
cid:3	O
)	O
k−1	O
mmkm	O
(	O
x∗	O
)	O
=	O
km	O
(	O
x∗	O
)	O
>	O
q−1σ2	O
=	O
σ2	O
nkmmk−1	O
nkm	O
(	O
x∗	O
)	O
>	O
q−1km	O
(	O
x∗	O
)	O
,	O
mmkm	O
(	O
x∗	O
)	O
(	O
8.39	O
)	O
(	O
8.40	O
)	O
mmkm	O
(	O
x∗	O
)	O
mmkm	O
(	O
x∗	O
)	O
(	O
8.41	O
)	O
(	O
8.42	O
)	O
(	O
8.43	O
)	O
(	O
8.44	O
)	O
in	O
agreement	O
with	O
eq	O
.	O
(	O
8.15	O
)	O
.	O
the	O
step	O
between	O
eqs	O
.	O
(	O
8.40	O
)	O
and	O
(	O
8.41	O
)	O
is	O
obtained	O
from	O
eqs	O
.	O
(	O
8.34	O
)	O
and	O
(	O
8.38	O
)	O
above	O
,	O
and	O
eq	O
.	O
(	O
8.43	O
)	O
follows	O
from	O
eq	O
.	O
(	O
8.42	O
)	O
using	O
im	O
=	O
(	O
σ2	O
nkmm	O
+	O
kmnknm	O
)	O
q−1	O
.	O
8.7	O
exercises	O
1.	O
verify	O
that	O
the	O
mean	O
and	O
covariance	B
of	O
the	O
bcm	O
predictions	O
(	O
equations	O
8.29	O
and	O
8.30	O
)	O
are	O
correct	O
.	O
if	O
you	O
are	O
stuck	O
,	O
see	B
tresp	O
[	O
2000	O
]	O
for	O
details	O
.	O
mm	O
show	O
that	O
e	O
(	O
copt	O
)	O
=	O
tr	O
(	O
k	O
−	O
˜k	O
)	O
,	O
where	O
˜k	O
=	O
knmk−1	O
mmkmn	O
.	O
now	O
consider	O
adding	O
one	O
data-	O
point	O
into	O
set	B
i	O
,	O
so	O
that	O
kmm	O
grows	O
to	O
k	O
(	O
m+1	O
)	O
(	O
m+1	O
)	O
.	O
using	O
eq	O
.	O
(	O
a.12	O
)	O
2.	O
using	O
eq	O
.	O
(	O
8.10	O
)	O
and	O
the	O
fact	O
that	O
copt	O
=	O
knmk−1	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
188	O
approximation	O
methods	O
for	O
large	O
datasets	O
show	O
that	O
the	O
change	O
in	O
e	O
due	O
to	O
adding	O
the	O
extra	O
datapoint	O
can	O
be	O
computed	O
in	O
time	O
o	O
(	O
mn	O
)	O
.	O
if	O
you	O
need	O
help	O
,	O
see	B
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
,	O
sec	O
.	O
10.2.2	O
]	O
for	O
further	O
details	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
chapter	O
9	O
further	O
issues	O
and	O
conclusions	O
in	O
the	O
previous	O
chapters	O
of	O
the	O
book	O
we	O
have	O
concentrated	O
on	O
giving	O
a	O
solid	O
grounding	O
in	O
the	O
use	O
of	O
gps	O
for	O
regression	B
and	O
classiﬁcation	B
problems	O
,	O
includ-	O
ing	O
model	B
selection	O
issues	O
,	O
approximation	O
methods	O
for	O
large	O
datasets	O
,	O
and	O
con-	O
nections	O
to	O
related	O
models	O
.	O
in	O
this	O
chapter	O
we	O
provide	O
some	O
short	O
descriptions	O
of	O
other	O
issues	O
relating	O
to	O
gaussian	O
process	B
prediction	O
,	O
with	O
pointers	O
to	O
the	O
literature	O
for	O
further	O
reading	O
.	O
so	O
far	O
we	O
have	O
mainly	O
discussed	O
the	O
case	O
when	O
the	O
output	O
target	O
y	O
is	O
a	O
single	O
label	O
,	O
but	O
in	O
section	O
9.1	O
we	O
describe	O
how	O
to	O
deal	O
with	O
the	O
case	O
that	O
there	O
are	O
multiple	O
output	O
targets	O
.	O
similarly	O
,	O
for	O
the	O
regression	B
problem	O
we	O
have	O
focussed	O
on	O
i.i.d	O
.	O
gaussian	O
noise	O
;	O
in	O
section	O
9.2	O
we	O
relax	O
this	O
condition	O
to	O
allow	O
the	O
noise	O
process	O
to	O
have	O
correlations	O
.	O
the	O
classiﬁcation	B
problem	O
is	O
characterized	O
by	O
a	O
non-gaussian	O
likelihood	B
function	O
;	O
however	O
,	O
there	O
are	O
other	O
non-gaussian	O
likelihoods	O
of	O
interest	O
,	O
as	O
described	O
in	O
section	O
9.3.	O
we	O
may	O
not	O
only	O
have	O
observations	O
of	O
function	B
values	O
,	O
by	O
also	O
on	O
derivatives	O
of	O
the	O
target	O
function	B
.	O
in	O
section	O
9.4	O
we	O
discuss	O
how	O
to	O
make	O
use	O
of	O
this	O
infor-	O
mation	O
in	O
the	O
gpr	O
framework	O
.	O
also	O
it	O
may	O
happen	O
that	O
there	O
is	O
noise	O
on	O
the	O
observation	O
of	O
the	O
input	O
variable	O
x	O
;	O
in	O
section	O
9.5	O
we	O
explain	O
how	O
this	O
can	O
be	O
handled	O
.	O
in	O
section	O
9.6	O
we	O
mention	O
how	O
more	O
ﬂexible	O
models	O
can	O
be	O
obtained	O
using	O
mixtures	O
of	O
gaussian	O
process	B
models	O
.	O
as	O
well	O
as	O
carrying	O
out	O
prediction	B
for	O
test	O
inputs	O
,	O
one	O
might	O
also	O
wish	O
to	O
try	O
to	O
ﬁnd	O
the	O
global	O
optimum	O
of	O
a	O
function	B
within	O
some	O
compact	O
set	O
.	O
approaches	O
based	O
on	O
gaussian	O
processes	O
for	O
this	O
problem	O
are	O
described	O
in	O
section	O
9.7.	O
the	O
use	O
of	O
gaussian	O
processes	O
to	O
evaluate	O
integrals	B
is	O
covered	O
in	O
section	O
9.8.	O
by	O
using	O
a	O
scale	B
mixture	I
of	O
gaussians	O
construction	O
one	O
can	O
obtain	O
a	O
mul-	O
tivariate	O
student	O
’	O
s	O
t	O
distribution	O
.	O
this	O
construction	O
can	O
be	O
extended	O
to	O
give	O
a	O
student	O
’	O
s	O
t	O
process	B
,	O
as	O
explained	O
in	O
section	O
9.9.	O
one	O
key	O
aspect	O
of	O
the	O
bayesian	O
framework	O
relates	O
to	O
the	O
incorporation	O
of	O
prior	O
knowledge	O
into	O
the	O
problem	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
190	O
further	O
issues	O
and	O
conclusions	O
formulation	O
.	O
in	O
some	O
applications	O
we	O
not	O
only	O
have	O
the	O
dataset	B
d	O
but	O
also	O
ad-	O
ditional	O
information	O
.	O
for	O
example	O
,	O
for	O
an	O
optical	O
character	O
recognition	O
problem	O
we	O
know	O
that	O
translating	O
the	O
input	O
pattern	O
by	O
one	O
pixel	O
will	O
not	O
change	O
the	O
label	O
of	O
the	O
pattern	O
.	O
approaches	O
for	O
incorporating	O
this	O
knowledge	O
are	O
discussed	O
in	O
section	O
9.10.	O
in	O
this	O
book	O
we	O
have	O
concentrated	O
on	O
supervised	B
learning	I
problems	O
.	O
how-	O
ever	O
,	O
gps	O
can	O
be	O
used	O
as	O
components	O
in	O
unsupervised	O
learning	B
models	O
,	O
as	O
de-	O
scribed	O
in	O
section	O
9.11.	O
finally	O
,	O
we	O
close	O
with	O
some	O
conclusions	O
and	O
an	O
outlook	O
to	O
the	O
future	O
in	O
section	O
9.12	O
.	O
9.1	O
multiple	B
outputs	I
throughout	O
this	O
book	O
we	O
have	O
concentrated	O
on	O
the	O
problem	O
of	O
predicting	O
a	O
single	O
output	O
variable	O
y	O
from	O
an	O
input	O
x.	O
however	O
,	O
it	O
can	O
happen	O
that	O
one	O
may	O
wish	O
to	O
predict	O
multiple	O
output	O
variables	O
(	O
or	O
channels	O
)	O
simultaneously	O
.	O
for	O
example	O
in	O
the	O
robot	B
inverse	O
dynamics	O
problem	O
described	O
in	O
section	O
2.5	O
there	O
are	O
really	O
seven	O
torques	O
to	O
be	O
predicted	O
.	O
a	O
simple	O
approach	O
is	O
to	O
model	B
each	O
output	O
variable	O
as	O
independent	O
from	O
the	O
others	O
and	O
treat	O
them	O
separately	O
.	O
however	O
,	O
this	O
may	O
lose	O
information	O
and	O
be	O
suboptimal	O
.	O
one	O
way	O
in	O
which	O
correlation	O
can	O
occur	O
is	O
through	O
a	O
correlated	B
noise	O
process	B
.	O
even	O
if	O
the	O
output	O
channels	O
are	O
a	O
priori	O
independent	O
,	O
if	O
the	O
noise	O
process	O
is	O
correlated	B
then	O
this	O
will	O
induce	O
correlations	O
in	O
the	O
posterior	O
processes	O
.	O
such	O
a	O
situation	O
is	O
easily	O
handled	O
in	O
the	O
gp	O
framework	O
by	O
considering	O
the	O
joint	B
,	O
block-diagonal	O
,	O
prior	O
over	O
the	O
function	B
values	O
of	O
each	O
channel	O
.	O
another	O
way	O
that	O
correlation	O
of	O
multiple	O
channels	O
can	O
occur	O
is	O
if	O
the	O
prior	O
already	O
has	O
this	O
structure	O
.	O
for	O
example	O
in	O
geostatistical	O
situations	O
there	O
may	O
be	O
correlations	O
between	O
the	O
abundances	O
of	O
diﬀerent	O
ores	O
,	O
e.g	O
.	O
silver	O
and	O
lead	O
.	O
this	O
situation	O
requires	O
that	O
the	O
covariance	B
function	I
models	O
not	O
only	O
the	O
correlation	O
structure	O
of	O
each	O
channel	O
,	O
but	O
also	O
the	O
cross-correlations	O
between	O
channels	O
.	O
some	O
work	O
on	O
this	O
topic	O
can	O
be	O
found	O
in	O
the	O
geostatistics	B
literature	O
under	O
the	O
name	O
of	O
cokriging	B
,	O
see	B
e.g	O
.	O
cressie	O
[	O
1993	O
,	O
sec	O
.	O
3.2.3	O
]	O
.	O
one	O
way	O
to	O
induce	O
correlations	O
between	O
a	O
number	O
of	O
output	O
channels	O
is	O
to	O
obtain	O
them	O
as	O
linear	B
combinations	O
of	O
a	O
number	O
of	O
latent	O
channels	O
,	O
as	O
described	O
in	O
teh	O
et	O
al	O
.	O
[	O
2005	O
]	O
;	O
see	B
also	O
micchelli	O
and	O
pontil	O
[	O
2005	O
]	O
.	O
a	O
related	O
approach	O
is	O
taken	O
by	O
boyle	O
and	O
frean	O
[	O
2005	O
]	O
who	O
introduce	O
correlations	O
between	O
two	O
processes	O
by	O
deriving	O
them	O
as	O
diﬀerent	O
convolutions	O
of	O
the	O
same	O
underlying	O
white	O
noise	O
process	O
.	O
9.2	O
noise	O
models	O
with	O
dependencies	O
the	O
noise	O
models	O
used	O
so	O
far	O
have	O
almost	O
exclusively	O
assumed	O
gaussianity	O
and	O
independence	O
.	O
non-gaussian	O
likelihoods	O
are	O
mentioned	O
in	O
section	O
9.3	O
below	O
.	O
inside	O
the	O
family	O
of	O
gaussian	O
noise	O
models	O
,	O
it	O
is	O
not	O
diﬃcult	O
to	O
model	B
depen-	O
dencies	O
.	O
this	O
may	O
be	O
particularly	O
useful	O
in	O
models	O
involving	O
time	O
.	O
we	O
simply	O
add	O
terms	O
to	O
the	O
noise	O
covariance	O
function	B
with	O
the	O
desired	O
structure	O
,	O
including	O
cokriging	B
coloured	O
noise	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
191	O
arma	O
9.3	O
non-gaussian	O
likelihoods	O
hyperparameters	B
.	O
in	O
fact	O
,	O
we	O
already	O
used	O
this	O
approach	O
for	O
the	O
atmospheric	O
carbon	O
dioxide	O
modelling	O
task	O
in	O
section	O
5.4.3.	O
also	O
,	O
murray-smith	O
and	O
girard	O
[	O
2001	O
]	O
have	O
used	O
an	O
autoregressive	O
moving-average	O
(	O
arma	O
)	O
noise	B
model	I
(	O
see	B
also	O
eq	O
.	O
(	O
b.51	O
)	O
)	O
in	O
a	O
gp	O
regression	B
task	O
.	O
9.3	O
non-gaussian	O
likelihoods	O
our	O
main	O
focus	O
has	O
been	O
on	O
regression	B
with	O
gaussian	O
noise	O
,	O
and	O
classiﬁcation	B
using	O
the	O
logistic	B
or	O
probit	B
response	O
functions	O
.	O
however	O
,	O
gaussian	O
processes	O
can	O
be	O
used	O
as	O
priors	O
with	O
other	O
likelihood	B
functions	O
.	O
for	O
example	O
,	O
diggle	O
et	O
al	O
.	O
[	O
1998	O
]	O
were	O
concerned	O
with	O
modelling	O
count	O
data	O
measured	O
geographically	O
using	O
a	O
poisson	O
likelihood	B
with	O
a	O
spatially	O
varying	O
rate	O
.	O
they	O
achieved	O
this	O
by	O
placing	O
a	O
gp	O
prior	O
over	O
the	O
log	O
poisson	O
rate	O
.	O
goldberg	O
et	O
al	O
.	O
[	O
1998	O
]	O
stayed	O
with	O
a	O
gaussian	O
noise	B
model	I
,	O
but	O
introduced	O
heteroscedasticity	O
,	O
i.e	O
.	O
allowing	O
the	O
noise	O
variance	O
to	O
be	O
a	O
function	B
of	O
x.	O
this	O
was	O
achieved	O
by	O
placing	O
a	O
gp	O
prior	O
on	O
the	O
log	O
variance	O
function	B
.	O
neal	O
[	O
1997	O
]	O
robustiﬁed	O
gp	O
regression	B
by	O
using	O
a	O
student	O
’	O
s	O
t-distributed	O
noise	B
model	I
rather	O
than	O
gaussian	O
noise	O
.	O
chu	O
and	O
ghahramani	O
[	O
2005	O
]	O
have	O
described	O
how	O
to	O
use	O
gps	O
for	O
the	O
ordinal	O
regression	B
problem	O
,	O
where	O
one	O
is	O
given	O
ranked	O
preference	O
information	O
as	O
the	O
target	O
data	O
.	O
9.4	O
derivative	B
observations	I
since	O
diﬀerentiation	O
is	O
a	O
linear	B
operator	O
,	O
the	O
derivative	O
of	O
a	O
gaussian	O
process	B
is	O
another	O
gaussian	O
process	B
.	O
thus	O
we	O
can	O
use	O
gps	O
to	O
make	O
predictions	O
about	O
derivatives	O
,	O
and	O
also	O
to	O
make	O
inference	O
based	O
on	O
derivative	O
information	O
.	O
in	O
general	O
,	O
we	O
can	O
make	O
inference	O
based	O
on	O
the	O
joint	B
gaussian	O
distribution	O
of	O
function	B
values	O
and	O
partial	O
derivatives	O
.	O
a	O
covariance	B
function	I
k	O
(	O
·	O
,	O
·	O
)	O
on	O
function	B
values	O
implies	O
the	O
following	O
(	O
mixed	O
)	O
covariance	B
between	O
function	B
values	O
and	O
partial	O
derivatives	O
,	O
and	O
between	O
partial	O
derivatives	O
cov	O
(	O
cid:0	O
)	O
fi	O
,	O
(	O
cid:1	O
)	O
=	O
∂k	O
(	O
xi	O
,	O
xj	O
)	O
,	O
∂xdj	O
∂fj	O
∂xdj	O
cov	O
(	O
cid:0	O
)	O
∂fi	O
∂xdi	O
,	O
∂fj	O
∂xej	O
(	O
cid:1	O
)	O
=	O
∂2k	O
(	O
xi	O
,	O
xj	O
)	O
∂xdi∂xej	O
,	O
(	O
9.1	O
)	O
see	B
e.g	O
.	O
papoulis	O
[	O
1991	O
,	O
ch	O
.	O
10	O
]	O
or	O
adler	O
[	O
1981	O
,	O
sec	O
.	O
2.2	O
]	O
.	O
with	O
n	O
datapoints	O
in	O
d	O
dimensions	O
,	O
the	O
complete	O
joint	B
distribution	O
of	O
f	O
and	O
its	O
d	O
partial	O
derivatives	O
involves	O
n	O
(	O
d+1	O
)	O
quantities	O
,	O
but	O
in	O
a	O
typical	O
application	O
we	O
may	O
only	O
have	O
access	O
to	O
or	O
interest	O
in	O
a	O
subset	O
of	O
these	O
;	O
we	O
simply	O
remove	O
the	O
rows	O
and	O
columns	O
from	O
the	O
joint	B
matrix	O
which	O
are	O
not	O
needed	O
.	O
observed	O
function	B
values	O
and	O
derivatives	O
may	O
often	O
have	O
diﬀerent	O
noise	O
levels	O
,	O
which	O
are	O
incorporated	O
by	O
adding	O
a	O
diagonal	O
contribution	O
with	O
diﬀering	O
hyperparameters	B
.	O
inference	O
and	O
predictions	O
are	O
done	O
as	O
usual	O
.	O
this	O
approach	O
was	O
used	O
in	O
the	O
context	O
of	O
learning	B
in	O
dynamical	O
systems	O
by	O
solak	O
et	O
al	O
.	O
[	O
2003	O
]	O
.	O
in	O
figure	O
9.1	O
the	O
posterior	B
process	I
with	O
and	O
without	O
derivative	B
observations	I
are	O
compared	O
.	O
noise-free	O
derivatives	O
may	O
be	O
a	O
useful	O
way	O
to	O
enforce	O
known	O
constraints	O
in	O
a	O
modelling	O
problem	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
192	O
further	O
issues	O
and	O
conclusions	O
(	O
a	O
)	O
(	O
b	O
)	O
figure	O
9.1	O
:	O
in	O
panel	O
(	O
a	O
)	O
we	O
show	O
four	O
data	O
points	O
in	O
a	O
one	O
dimensional	O
noise-free	O
regression	B
problem	O
,	O
together	O
with	O
three	O
functions	O
sampled	O
from	O
the	O
posterior	O
and	O
the	O
95	O
%	O
conﬁdence	O
region	O
in	O
light	O
grey	O
.	O
in	O
panel	O
(	O
b	O
)	O
the	O
same	O
observations	O
have	O
been	O
augmented	O
by	O
noise-free	O
derivative	O
information	O
,	O
indicated	O
by	O
small	O
tangent	O
segments	O
at	O
the	O
data	O
points	O
.	O
the	O
covariance	B
function	I
is	O
the	O
squared	B
exponential	I
with	O
unit	O
process	B
variance	O
and	O
unit	O
length-scale	B
.	O
9.5	O
prediction	B
with	O
uncertain	B
inputs	I
it	O
can	O
happen	O
that	O
the	O
input	O
values	O
to	O
a	O
prediction	B
problem	O
can	O
be	O
uncer-	O
tain	O
.	O
for	O
example	O
,	O
for	O
a	O
discrete	O
time	O
series	O
one	O
can	O
perform	O
multi-step-ahead	O
predictions	O
by	O
iterating	O
one-step-ahead	O
predictions	O
.	O
however	O
,	O
if	O
the	O
one-step-	O
ahead	O
predictions	O
include	O
uncertainty	O
,	O
then	O
it	O
is	O
necessary	O
to	O
propagate	O
this	O
uncertainty	O
forward	O
to	O
get	O
the	O
proper	O
multi-step-ahead	O
predictions	O
.	O
one	O
sim-	O
ple	O
approach	O
is	O
to	O
use	O
sampling	O
methods	O
.	O
alternatively	O
,	O
it	O
may	O
be	O
possible	O
to	O
use	O
analytical	O
approaches	O
.	O
girard	O
et	O
al	O
.	O
[	O
2003	O
]	O
showed	O
that	O
it	O
is	O
possible	O
to	O
compute	O
the	O
mean	O
and	O
variance	O
of	O
the	O
output	O
analytically	O
when	O
using	O
the	O
se	O
covariance	B
function	I
and	O
gaussian	O
input	O
noise	O
.	O
more	O
generally	O
,	O
the	O
problem	O
of	O
regression	B
with	O
uncertain	B
inputs	I
has	O
been	O
studied	O
in	O
the	O
statistics	O
literature	O
under	O
the	O
name	O
of	O
errors-in-variables	B
regres-	O
sion	O
.	O
see	B
dellaportas	O
and	O
stephens	O
[	O
1995	O
]	O
for	O
a	O
bayesian	O
treatment	O
of	O
the	O
problem	O
and	O
pointers	O
to	O
the	O
literature	O
.	O
9.6	O
mixtures	O
of	O
gaussian	O
processes	O
in	O
chapter	O
4	O
we	O
have	O
seen	O
many	O
ideas	O
for	O
making	O
the	O
covariance	B
functions	O
more	O
ﬂexible	O
.	O
another	O
route	O
is	O
to	O
use	O
a	O
mixture	O
of	O
diﬀerent	O
gaussian	O
process	B
models	O
,	O
each	O
one	O
used	O
in	O
some	O
local	O
region	O
of	O
input	O
space	O
.	O
this	O
kind	O
of	O
model	B
is	O
generally	O
known	O
as	O
a	O
mixture	B
of	I
experts	I
model	O
and	O
is	O
due	O
to	O
jacobs	O
et	O
al	O
.	O
[	O
1991	O
]	O
.	O
in	O
addition	O
to	O
the	O
local	O
expert	O
models	O
,	O
the	O
model	B
has	O
a	O
manager	O
that	O
(	O
probabilistically	O
)	O
assigns	O
points	O
to	O
the	O
experts	O
.	O
rasmussen	O
and	O
ghahramani	O
[	O
2002	O
]	O
used	O
gaussian	O
process	B
models	O
as	O
local	O
experts	O
,	O
and	O
based	O
their	O
manager	O
on	O
another	O
type	O
of	O
stochastic	O
process	O
:	O
the	O
dirichlet	O
process	B
.	O
inference	O
in	O
this	O
model	B
required	O
mcmc	O
methods	O
.	O
−4−2024−2−1012input	O
,	O
xoutput	O
,	O
y	O
(	O
x	O
)	O
−4−2024−2−1012input	O
,	O
xoutput	O
,	O
y	O
(	O
x	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
9.7	O
global	O
optimization	O
193	O
9.7	O
global	O
optimization	O
often	O
one	O
is	O
faced	O
with	O
the	O
problem	O
of	O
being	O
able	O
to	O
evaluate	O
a	O
continuous	O
function	B
g	O
(	O
x	O
)	O
,	O
and	O
wishing	O
to	O
ﬁnd	O
the	O
global	O
optimum	O
(	O
maximum	O
or	O
minimum	O
)	O
of	O
this	O
function	B
within	O
some	O
compact	O
set	O
a	O
⊂	O
rd	O
.	O
there	O
is	O
a	O
very	O
large	O
literature	O
on	O
the	O
problem	O
of	O
global	O
optimization	O
;	O
see	B
neumaier	O
[	O
2005	O
]	O
for	O
a	O
useful	O
overview	O
.	O
given	O
a	O
dataset	B
d	O
=	O
{	O
(	O
xi	O
,	O
g	O
(	O
xi	O
)	O
)	O
|i=1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
one	O
appealing	O
approach	O
is	O
to	O
ﬁt	O
a	O
gp	O
regression	B
model	O
to	O
this	O
data	O
.	O
this	O
will	O
give	O
a	O
mean	O
prediction	O
and	O
predictive	B
variance	O
for	O
every	O
x	O
∈	O
a.	O
jones	O
[	O
2001	O
]	O
examines	O
a	O
number	O
of	O
criteria	O
that	O
have	O
been	O
suggested	O
for	O
where	O
to	O
make	O
the	O
next	O
function	B
evaluation	O
based	O
on	O
the	O
predictive	B
mean	O
and	O
variance	O
.	O
one	O
issue	O
with	O
this	O
approach	O
is	O
that	O
one	O
may	O
need	O
to	O
search	O
to	O
ﬁnd	O
the	O
optimum	O
of	O
the	O
criterion	O
,	O
which	O
may	O
itself	O
be	O
multimodal	O
optimization	O
problem	O
.	O
however	O
,	O
if	O
evaluations	O
of	O
g	O
are	O
expensive	O
or	O
time-consuming	O
,	O
it	O
can	O
make	O
sense	O
to	O
work	O
hard	O
on	O
this	O
new	O
optimization	O
problem	O
.	O
for	O
historical	O
references	O
and	O
further	O
work	O
in	O
this	O
area	O
see	B
jones	O
[	O
2001	O
]	O
and	O
ritter	O
[	O
2000	O
,	O
sec	O
.	O
viii.2	O
]	O
.	O
9.8	O
evaluation	O
of	O
integrals	B
another	O
interesting	O
and	O
unusual	O
application	O
of	O
gaussian	O
processes	O
is	O
for	O
the	O
evaluation	O
of	O
the	O
integrals	B
of	O
a	O
deterministic	O
function	B
f.	O
one	O
evaluates	O
the	O
function	B
at	O
a	O
number	O
of	O
locations	O
,	O
and	O
then	O
one	O
can	O
use	O
a	O
gaussian	O
process	B
as	O
a	O
posterior	O
over	O
functions	O
.	O
this	O
posterior	O
over	O
functions	O
induces	O
a	O
posterior	O
over	O
the	O
value	O
of	O
the	O
integral	O
(	O
since	O
each	O
possible	O
function	B
from	O
the	O
posterior	O
would	O
give	O
rise	O
to	O
a	O
particular	O
value	O
of	O
the	O
integral	O
)	O
.	O
for	O
some	O
covariance	B
functions	O
(	O
e.g	O
.	O
the	O
squared	B
exponential	I
)	O
,	O
one	O
can	O
compute	O
the	O
expectation	O
and	O
variance	O
of	O
the	O
value	O
of	O
the	O
integral	O
analytically	O
.	O
it	O
is	O
perhaps	O
unusual	O
to	O
think	O
of	O
the	O
value	O
of	O
the	O
integral	O
as	O
being	O
random	O
(	O
as	O
it	O
does	O
have	O
one	O
particular	O
deterministic	O
value	O
)	O
,	O
but	O
it	O
is	O
perfectly	O
in	O
line	O
of	O
bayesian	O
thinking	O
that	O
you	O
treat	O
all	O
kinds	O
of	O
uncertainty	O
using	O
probabilities	O
.	O
this	O
idea	O
was	O
proposed	O
under	O
the	O
name	O
of	O
bayes-hermite	O
quadrature	O
by	O
o	O
’	O
hagan	O
[	O
1991	O
]	O
,	O
and	O
later	O
under	O
the	O
name	O
of	O
bayesian	O
monte	O
carlo	O
in	O
rasmussen	O
and	O
ghahramani	O
[	O
2003	O
]	O
.	O
another	O
approach	O
is	O
related	O
to	O
the	O
ideas	O
of	O
global	O
optimization	O
in	O
the	O
section	O
9.7	O
above	O
.	O
one	O
can	O
use	O
a	O
gp	O
model	B
of	O
a	O
function	B
to	O
aid	O
an	O
mcmc	O
sampling	O
procedure	O
,	O
which	O
may	O
be	O
advantageous	O
if	O
the	O
function	B
of	O
interest	O
is	O
computa-	O
tionally	O
expensive	O
to	O
evaluate	O
.	O
rasmussen	O
[	O
2003	O
]	O
combines	O
hybrid	O
monte	O
carlo	O
with	O
a	O
gp	O
model	B
of	O
the	O
log	O
of	O
the	O
integrand	O
,	O
and	O
also	O
uses	O
derivatives	O
of	O
the	O
function	B
(	O
discussed	O
in	O
section	O
9.4	O
)	O
to	O
get	O
an	O
accurate	O
model	B
of	O
the	O
integrand	O
with	O
very	O
few	O
evaluations	O
.	O
combining	O
gps	O
with	O
mcmc	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
194	O
scale	B
mixture	I
noise	O
entanglement	O
further	O
issues	O
and	O
conclusions	O
9.9	O
student	O
’	O
s	O
t	O
process	B
a	O
student	O
’	O
s	O
t	O
process	B
can	O
be	O
obtained	O
by	O
applying	O
the	O
scale	B
mixture	I
of	O
gaus-	O
sians	O
construction	O
of	O
a	O
student	O
’	O
s	O
t	O
distribution	O
to	O
a	O
gaussian	O
process	B
[	O
o	O
’	O
hagan	O
,	O
1991	O
,	O
o	O
’	O
hagan	O
et	O
al.	O
,	O
1999	O
]	O
.	O
we	O
divide	O
the	O
covariances	O
by	O
the	O
scalar	O
τ	O
and	O
put	O
a	O
gamma	B
distribution	I
on	O
τ	O
with	O
shape	O
α	O
and	O
mean	O
β	O
so	O
that	O
βαγ	O
(	O
α	O
)	O
τ	O
α−1	O
exp	O
˜k	O
(	O
x	O
,	O
x0	O
)	O
=	O
τ−1k	O
(	O
x	O
,	O
x0	O
)	O
,	O
(	O
cid:16	O
)	O
−	O
τ	O
α	O
p	O
(	O
τ|α	O
,	O
β	O
)	O
=	O
(	O
9.2	O
)	O
(	O
cid:17	O
)	O
αα	O
β	O
,	O
z	O
where	O
k	O
is	O
any	O
valid	O
covariance	B
function	I
.	O
now	O
the	O
joint	B
prior	O
distribution	O
of	O
any	O
ﬁnite	O
number	O
n	O
of	O
function	B
values	O
y	O
becomes	O
n	O
(	O
y|0	O
,	O
τ−1ky	O
)	O
p	O
(	O
τ|α	O
,	O
β	O
)	O
dτ	O
γ	O
(	O
α	O
+	O
n/2	O
)	O
(	O
2πα	O
)	O
−n/2	O
(	O
cid:17	O
)	O
−	O
(	O
α+n/2	O
)	O
p	O
(	O
y|α	O
,	O
β	O
,	O
θ	O
)	O
=	O
βy	O
>	O
k−1	O
y	O
y	O
(	O
9.3	O
)	O
(	O
cid:16	O
)	O
,	O
=	O
γ	O
(	O
α	O
)	O
|β−1ky|−1/2	O
1	O
+	O
2α	O
which	O
is	O
recognized	O
as	O
the	O
zero	O
mean	O
,	O
multivariate	O
student	O
’	O
s	O
t	O
distribution	O
with	O
2α	O
degrees	B
of	I
freedom	I
:	O
p	O
(	O
y|α	O
,	O
β	O
,	O
θ	O
)	O
=	O
t	O
(	O
0	O
,	O
β−1ky	O
,	O
2α	O
)	O
.	O
we	O
could	O
state	O
a	O
deﬁnition	O
analogous	O
to	O
deﬁnition	O
2.1	O
on	O
page	O
13	O
for	O
the	O
gaussian	O
process	B
,	O
and	O
write	O
(	O
9.4	O
)	O
cf	O
.	O
eq	O
.	O
(	O
2.14	O
)	O
.	O
the	O
marginal	B
likelihood	I
can	O
be	O
directly	O
evaluated	O
using	O
eq	O
.	O
(	O
9.3	O
)	O
,	O
and	O
training	O
can	O
be	O
achieved	O
using	O
the	O
methods	O
discussed	O
in	O
chapter	O
5	O
regarding	O
α	O
and	O
β	O
as	O
hyperparameters	B
.	O
the	O
predictive	B
distribution	O
for	O
test	O
cases	O
are	O
also	O
t	O
distributions	O
,	O
the	O
derivation	O
of	O
which	O
is	O
left	O
as	O
an	O
exercise	O
below	O
.	O
f	O
∼	O
t	O
p	O
(	O
0	O
,	O
β−1k	O
,	O
2α	O
)	O
,	O
notice	O
that	O
the	O
above	O
construction	O
is	O
clear	O
for	O
noise-free	O
processes	O
,	O
but	O
that	O
the	O
interpretation	O
becomes	O
more	O
complicated	O
if	O
the	O
covariance	B
function	I
k	O
(	O
x	O
,	O
x0	O
)	O
contains	O
a	O
noise	O
contribution	O
.	O
the	O
noise	O
and	O
signal	O
get	O
entangled	O
by	O
the	O
com-	O
mon	O
factor	O
τ	O
,	O
and	O
the	O
observations	O
can	O
no	O
longer	O
be	O
written	O
as	O
the	O
sum	O
of	O
independent	O
signal	O
and	O
noise	O
contributions	O
.	O
allowing	O
for	O
independent	O
noise	O
contributions	O
removes	O
analytic	O
tractability	O
,	O
which	O
may	O
reduce	O
the	O
usefulness	O
of	O
the	O
t	O
process	B
.	O
exercise	O
using	O
the	O
scale	B
mixture	I
representation	O
from	O
eq	O
.	O
(	O
9.3	O
)	O
derive	O
the	O
poste-	O
rior	O
predictive	B
distribution	O
for	O
a	O
student	O
’	O
s	O
t	O
process	B
.	O
exercise	O
consider	O
the	O
generating	O
process	B
implied	O
by	O
eq	O
.	O
(	O
9.2	O
)	O
,	O
and	O
write	O
a	O
pro-	O
gram	O
to	O
draw	O
functions	O
at	O
random	O
.	O
characterize	O
the	O
diﬀerence	O
between	O
the	O
student	O
’	O
s	O
t	O
process	B
and	O
the	O
corresponding	O
gaussian	O
process	B
(	O
obtained	O
in	O
the	O
limit	O
α	O
→	O
∞	O
)	O
,	O
and	O
explain	O
why	O
the	O
t	O
process	B
is	O
perhaps	O
not	O
as	O
exciting	O
as	O
one	O
might	O
have	O
hoped	O
.	O
9.10	O
invariances	B
it	O
can	O
happen	O
that	O
the	O
input	O
is	O
apparently	O
in	O
vector	O
form	O
but	O
in	O
fact	O
it	O
has	O
additional	O
structure	O
.	O
a	O
good	O
example	O
is	O
a	O
pixelated	O
image	O
,	O
where	O
the	O
2-d	O
array	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
9.10	O
invariances	B
195	O
of	O
pixels	O
can	O
be	O
arranged	O
into	O
a	O
vector	O
(	O
e.g	O
.	O
in	O
raster-scan	O
order	O
)	O
.	O
imagine	O
that	O
the	O
image	O
is	O
of	O
a	O
handwritten	O
digit	O
;	O
then	O
we	O
know	O
that	O
if	O
the	O
image	O
is	O
translated	O
by	O
one	O
pixel	O
it	O
will	O
remain	O
the	O
same	O
digit	O
.	O
thus	O
we	O
have	O
knowledge	O
of	O
certain	O
invariances	B
of	O
the	O
input	O
pattern	O
.	O
in	O
this	O
section	O
we	O
describe	O
a	O
number	O
of	O
ways	O
in	O
which	O
such	O
invariances	B
can	O
be	O
exploited	O
.	O
our	O
discussion	O
is	O
based	O
on	O
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
,	O
ch	O
.	O
11	O
]	O
.	O
prior	O
knowledge	O
about	O
the	O
problem	O
tells	O
us	O
that	O
certain	O
transformations	O
of	O
the	O
input	O
would	O
leave	O
the	O
class	O
label	O
invariant—these	O
include	O
simple	O
geometric	O
transformations	O
such	O
as	O
translations	O
,	O
rotations,1	O
rescalings	O
,	O
and	O
rather	O
less	O
ob-	O
vious	O
ones	O
such	O
as	O
line	O
thickness	O
transformations.2	O
given	O
enough	O
data	O
it	O
should	O
be	O
possible	O
to	O
learn	O
the	O
correct	O
input-output	O
mapping	O
,	O
but	O
it	O
would	O
make	O
sense	O
to	O
try	O
to	O
make	O
use	O
of	O
these	O
known	O
invariances	B
to	O
reduce	O
the	O
amount	O
of	O
training	O
data	O
needed	O
.	O
there	O
are	O
at	O
least	O
three	O
ways	O
in	O
which	O
this	O
prior	O
knowledge	O
has	O
been	O
used	O
,	O
as	O
described	O
below	O
.	O
the	O
ﬁrst	O
approach	O
is	O
to	O
generate	O
synthetic	O
training	O
examples	O
by	O
applying	O
valid	O
transformations	O
to	O
the	O
examples	O
we	O
already	O
have	O
.	O
this	O
is	O
simple	O
but	O
it	O
does	O
have	O
the	O
disadvantage	O
of	O
creating	O
a	O
larger	O
training	O
set	B
.	O
as	O
kernel-machine	O
training	O
algorithms	O
typically	O
scale	O
super-linearly	O
with	O
n	O
this	O
can	O
be	O
problematic	O
.	O
a	O
second	O
approach	O
is	O
to	O
make	O
the	O
predictor	O
invariant	O
to	O
small	O
transforma-	O
tions	O
of	O
each	O
training	O
case	O
;	O
this	O
method	O
was	O
ﬁrst	O
developed	O
by	O
simard	O
et	O
al	O
.	O
[	O
1992	O
]	O
for	O
neural	O
networks	O
under	O
the	O
name	O
of	O
“	O
tangent	O
prop	O
”	O
.	O
for	O
a	O
single	O
training	O
image	O
we	O
consider	O
the	O
manifold	O
of	O
images	O
that	O
are	O
generated	O
as	O
various	O
transformations	O
are	O
applied	O
to	O
it	O
.	O
this	O
manifold	O
will	O
have	O
a	O
complex	O
structure	O
,	O
but	O
locally	O
we	O
can	O
approximate	O
it	O
by	O
a	O
tangent	O
space	O
.	O
the	O
idea	O
in	O
“	O
tangent	O
prop	O
”	O
is	O
that	O
the	O
output	O
should	O
be	O
invariant	O
to	O
perturbations	O
of	O
the	O
training	O
example	O
in	O
this	O
tangent	O
space	O
.	O
for	O
neural	O
networks	O
it	O
is	O
quite	O
straightforward	O
to	O
modify	O
the	O
training	O
objective	O
function	B
to	O
penalize	O
deviations	O
from	O
this	O
in-	O
variance	O
,	O
see	B
simard	O
et	O
al	O
.	O
[	O
1992	O
]	O
for	O
details	O
.	O
section	O
11.4	O
in	O
sch¨olkopf	O
and	O
smola	O
[	O
2002	O
]	O
describes	O
some	O
ways	O
in	O
which	O
these	O
ideas	O
can	O
be	O
extended	O
to	O
kernel	B
machines	O
.	O
the	O
third	O
approach	O
to	O
dealing	O
with	O
invariances	B
is	O
to	O
develop	O
a	O
representation	O
of	O
the	O
input	O
which	O
is	O
invariant	O
to	O
some	O
or	O
all	O
of	O
the	O
transformations	O
.	O
for	O
example	O
,	O
binary	B
images	O
of	O
handwritten	O
digits	O
are	O
sometimes	O
“	O
skeletonized	O
”	O
to	O
remove	O
the	O
eﬀect	O
of	O
line	O
thickness	O
.	O
if	O
an	O
invariant	O
representation	O
can	O
be	O
achieved	O
for	O
all	O
transformations	O
it	O
is	O
the	O
most	O
desirable	O
,	O
but	O
it	O
can	O
be	O
diﬃcult	O
or	O
perhaps	O
impossible	O
to	O
achieve	O
.	O
for	O
example	O
,	O
if	O
a	O
given	O
training	O
pattern	O
can	O
belong	O
to	O
more	O
than	O
one	O
class	O
(	O
e.g	O
.	O
an	O
ambiguous	O
handwritten	O
digit	O
)	O
then	O
it	O
is	O
clearly	O
not	O
possible	O
to	O
ﬁnd	O
a	O
new	O
representation	O
which	O
is	O
invariant	O
to	O
transformations	O
yet	O
leaves	O
the	O
classes	O
distinguishable	O
.	O
1the	O
digit	O
recognition	O
problem	O
is	O
only	O
invariant	O
to	O
small	O
rotations	O
;	O
we	O
must	O
avoid	O
turning	O
a	O
6	O
into	O
a	O
9	O
.	O
2i.e	O
.	O
changing	O
the	O
thickness	O
of	O
the	O
pen	O
we	O
write	O
with	O
within	O
reasonable	O
bounds	O
does	O
not	O
change	O
the	O
digit	O
we	O
write	O
.	O
synthetic	O
training	O
examples	O
tangent	O
prop	O
invariant	O
representation	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
196	O
further	O
issues	O
and	O
conclusions	O
9.11	O
latent	O
variable	O
models	O
gtm	O
gplvm	O
our	O
main	O
focus	O
in	O
this	O
book	O
has	O
been	O
on	O
supervised	B
learning	I
.	O
however	O
,	O
gps	O
have	O
also	O
been	O
used	O
as	O
components	O
for	O
models	O
carrying	O
out	O
non-linear	O
dimen-	O
sionality	O
reduction	O
,	O
a	O
form	O
of	O
unsupervised	O
learning	B
.	O
the	O
key	O
idea	O
is	O
that	O
data	O
which	O
is	O
apparently	O
high-dimensional	O
(	O
e.g	O
.	O
a	O
pixelated	O
image	O
)	O
may	O
really	O
lie	O
on	O
a	O
low-dimensional	O
non-linear	O
manifold	O
which	O
we	O
wish	O
to	O
model	B
.	O
let	O
z	O
∈	O
rl	O
be	O
a	O
latent	O
(	O
or	O
hidden	O
)	O
variable	O
,	O
and	O
let	O
x	O
∈	O
rd	O
be	O
a	O
visible	O
variable	O
.	O
we	O
suppose	O
that	O
our	O
visible	O
data	O
is	O
generated	O
by	O
picking	O
a	O
point	O
in	O
z-space	O
and	O
mapping	O
this	O
point	O
into	O
the	O
data	O
space	O
through	O
a	O
(	O
possibly	O
non-	O
linear	B
)	O
mapping	O
,	O
and	O
then	O
optionally	O
adding	O
noise	O
.	O
thus	O
p	O
(	O
x	O
)	O
=r	O
p	O
(	O
x|z	O
)	O
p	O
(	O
z	O
)	O
dz	O
.	O
if	O
the	O
mapping	O
from	O
z	O
to	O
x	O
is	O
linear	B
and	O
z	O
has	O
a	O
gaussian	O
distribution	O
then	O
this	O
is	O
the	O
factor	B
analysis	I
model	O
,	O
and	O
the	O
mean	O
and	O
covariance	B
of	O
the	O
gaussian	O
in	O
x-space	O
can	O
easily	O
be	O
determined	O
.	O
however	O
,	O
if	O
the	O
mapping	O
is	O
non-linear	O
then	O
the	O
integral	O
can	O
not	O
be	O
computed	O
exactly	O
.	O
in	O
the	O
generative	B
topographic	I
mapping	I
(	O
gtm	O
)	O
model	B
[	O
bishop	O
et	O
al.	O
,	O
1998b	O
]	O
the	O
integral	O
was	O
approximated	O
using	O
a	O
grid	O
of	O
points	O
in	O
z-space	O
.	O
in	O
the	O
original	O
gtm	O
paper	O
the	O
non-linear	O
mapping	O
was	O
taken	O
to	O
be	O
a	O
linear	B
combination	O
of	O
non-linear	O
basis	O
functions	O
,	O
but	O
in	O
bishop	O
et	O
al	O
.	O
[	O
1998a	O
]	O
this	O
was	O
replaced	O
by	O
a	O
gaussian	O
process	B
mapping	O
between	O
the	O
latent	O
and	O
visible	O
spaces	O
.	O
more	O
recently	O
lawrence	O
[	O
2004	O
]	O
has	O
introduced	O
a	O
rather	O
diﬀerent	O
model	B
known	O
as	O
the	O
gaussian	O
process	B
latent	O
variable	O
model	B
(	O
gplvm	O
)	O
.	O
instead	O
of	O
having	O
a	O
prior	O
(	O
and	O
thus	O
a	O
posterior	O
)	O
distribution	O
over	O
the	O
latent	O
space	O
,	O
we	O
consider	O
that	O
each	O
data	O
point	O
xi	O
is	O
derived	O
from	O
a	O
corresponding	O
latent	O
point	O
zi	O
through	O
a	O
non-linear	O
mapping	O
(	O
with	O
added	O
noise	O
)	O
.	O
if	O
a	O
gaussian	O
process	B
is	O
used	O
for	O
this	O
non-linear	O
mapping	O
,	O
then	O
one	O
can	O
easily	O
write	O
down	O
the	O
joint	B
distribution	O
p	O
(	O
x|z	O
)	O
of	O
the	O
visible	O
variables	O
conditional	B
on	O
the	O
latent	O
variables	O
.	O
optimization	O
routines	O
can	O
then	O
be	O
used	O
to	O
ﬁnd	O
the	O
locations	O
of	O
the	O
latent	O
points	O
that	O
opti-	O
mize	O
p	O
(	O
x|z	O
)	O
.	O
this	O
has	O
some	O
similarities	O
to	O
the	O
work	O
on	O
regularized	O
principal	O
manifolds	O
[	O
sch¨olkopf	O
and	O
smola	O
,	O
2002	O
,	O
ch	O
.	O
17	O
]	O
except	O
that	O
in	O
the	O
gplvm	O
one	O
integrates	O
out	O
the	O
latent-to-visible	O
mapping	O
rather	O
than	O
optimizing	O
it	O
.	O
9.12	O
conclusions	O
and	O
future	O
directions	O
in	O
this	O
section	O
we	O
brieﬂy	O
wrap	O
up	O
some	O
of	O
the	O
threads	O
we	O
have	O
developed	O
throughout	O
the	O
book	O
,	O
and	O
discuss	O
possible	O
future	O
directions	O
of	O
work	O
on	O
gaussian	O
processes	O
.	O
in	O
chapter	O
2	O
we	O
saw	O
how	O
gaussian	O
process	B
regression	O
is	O
a	O
natural	O
extension	O
of	O
bayesian	O
linear	B
regression	I
to	O
a	O
more	O
ﬂexible	O
class	O
of	O
models	O
.	O
for	O
gaussian	O
noise	O
the	O
model	B
can	O
be	O
treated	O
analytically	O
,	O
and	O
is	O
simple	O
enough	O
that	O
the	O
gp	O
model	B
could	O
be	O
often	O
considered	O
as	O
a	O
replacement	O
for	O
the	O
traditional	O
linear	B
analogue	O
.	O
we	O
have	O
also	O
seen	O
that	O
historically	O
there	O
have	O
been	O
numerous	O
ideas	O
along	O
the	O
lines	O
of	O
gaussian	O
process	B
models	O
,	O
although	O
they	O
have	O
only	O
gained	O
a	O
sporadic	O
following	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
9.12	O
conclusions	O
and	O
future	O
directions	O
197	O
one	O
may	O
indeed	O
speculate	O
,	O
why	O
are	O
gps	O
not	O
currently	O
used	O
more	O
widely	O
in	O
applications	O
?	O
we	O
see	B
three	O
major	O
reasons	O
:	O
(	O
1	O
)	O
firstly	O
,	O
that	O
the	O
application	O
of	O
gaussian	O
processes	O
requires	O
the	O
handling	O
(	O
inversion	O
)	O
of	O
large	O
matrices	O
.	O
while	O
these	O
kinds	O
of	O
computations	O
were	O
tedious	O
20	O
years	O
ago	O
,	O
and	O
impossible	O
further	O
in	O
the	O
past	O
,	O
even	O
na¨ıve	O
implementations	O
suﬃce	O
for	O
moderate	O
sized	O
problems	O
on	O
an	O
anno	O
2005	O
pc	O
.	O
(	O
2	O
)	O
another	O
possibility	O
is	O
that	O
most	O
of	O
the	O
historical	O
work	O
on	O
gps	O
was	O
done	O
using	O
ﬁxed	O
covariance	B
functions	O
,	O
with	O
very	O
little	O
guide	O
as	O
to	O
how	O
to	O
choose	O
such	O
functions	O
.	O
the	O
choice	O
was	O
to	O
some	O
degree	O
arbitrary	O
,	O
and	O
the	O
idea	O
that	O
one	O
should	O
be	O
able	O
to	O
infer	O
the	O
structure	O
or	O
parameters	O
of	O
the	O
covariance	B
function	I
as	O
we	O
discuss	O
in	O
chapter	O
5	O
is	O
not	O
so	O
well	O
known	O
.	O
this	O
is	O
probably	O
a	O
very	O
important	O
step	O
in	O
turning	O
gps	O
into	O
an	O
interesting	O
method	O
for	O
practitioners	O
.	O
(	O
3	O
)	O
the	O
viewpoint	O
of	O
placing	O
gaussian	O
process	B
priors	O
over	O
functions	O
is	O
a	O
bayesian	O
one	O
.	O
although	O
the	O
adoption	O
of	O
bayesian	O
methods	O
in	O
the	O
machine	O
learning	B
community	O
is	O
quite	O
widespread	O
,	O
these	O
ideas	O
have	O
not	O
always	O
been	O
appreciated	O
more	O
widely	O
in	O
the	O
statistics	O
community	O
.	O
although	O
modern	O
computers	O
allow	O
simple	O
implementations	O
for	O
up	O
to	O
a	O
few	O
thousand	O
training	O
cases	O
,	O
the	O
computational	O
constraints	O
are	O
still	O
a	O
signiﬁcant	O
limitation	O
for	O
applications	O
where	O
the	O
datasets	O
are	O
signiﬁcantly	O
larger	O
than	O
this	O
.	O
in	O
chapter	O
8	O
we	O
have	O
given	O
an	O
overview	O
of	O
some	O
of	O
the	O
recent	O
work	O
on	O
approx-	O
imations	O
for	O
large	O
datasets	O
.	O
although	O
there	O
are	O
many	O
methods	O
and	O
a	O
lot	O
of	O
work	O
is	O
currently	O
being	O
undertaken	O
,	O
both	O
the	O
theoretical	O
and	O
practical	O
aspects	O
of	O
these	O
approximations	O
need	O
to	O
be	O
understood	O
better	O
in	O
order	O
to	O
be	O
a	O
useful	O
tool	O
to	O
the	O
practitioner	O
.	O
the	O
computations	O
required	O
for	O
the	O
gaussian	O
process	B
classiﬁcation	O
models	O
developed	O
in	O
chapter	O
3	O
are	O
a	O
lot	O
more	O
involved	O
than	O
for	O
regression	B
.	O
although	O
the	O
theoretical	O
foundations	O
of	O
gaussian	O
process	B
classiﬁcation	O
are	O
well	O
developed	O
,	O
it	O
is	O
not	O
yet	O
clear	O
under	O
which	O
circumstances	O
one	O
would	O
expect	O
the	O
extra	O
work	O
and	O
approximations	O
associated	O
with	O
treating	O
a	O
full	O
probabilistic	B
latent	O
variable	O
model	B
to	O
pay	O
oﬀ	O
.	O
the	O
answer	O
may	O
depend	O
heavily	O
on	O
the	O
ability	O
to	O
learn	O
meaningful	O
covariance	B
functions	O
.	O
the	O
incorporation	O
of	O
prior	O
knowledge	O
through	O
the	O
choice	O
and	O
parameter-	O
ization	O
of	O
the	O
covariance	B
function	I
is	O
another	O
prime	O
target	O
for	O
future	O
work	O
on	O
gps	O
.	O
in	O
chapter	O
4	O
we	O
have	O
presented	O
many	O
families	O
of	O
covariance	B
functions	O
with	O
widely	O
diﬀering	O
properties	O
,	O
and	O
in	O
chapter	O
5	O
we	O
presented	O
principled	O
methods	O
for	O
choosing	O
between	O
and	O
adapting	O
covariance	B
functions	O
.	O
particularly	O
in	O
the	O
machine	O
learning	B
community	O
,	O
there	O
has	O
been	O
a	O
tendency	O
to	O
view	O
gaussian	O
pro-	O
cesses	O
as	O
a	O
“	O
black	O
box	O
”	O
—what	O
exactly	O
goes	O
on	O
in	O
the	O
box	O
is	O
less	O
important	O
,	O
as	O
long	O
as	O
it	O
gives	O
good	O
predictions	O
.	O
to	O
our	O
mind	O
,	O
we	O
could	O
perhaps	O
learn	O
some-	O
thing	O
from	O
the	O
statisticians	O
here	O
,	O
and	O
ask	O
how	O
and	O
why	O
the	O
models	O
work	O
.	O
in	O
fact	O
the	O
hierarchical	O
formulation	O
of	O
the	O
covariance	B
functions	O
with	O
hyperparameters	B
,	O
the	O
testing	O
of	O
diﬀerent	O
hypotheses	O
and	O
the	O
adaptation	O
of	O
hyperparameters	B
gives	O
an	O
excellent	O
opportunity	O
to	O
understand	O
more	O
about	O
the	O
data	O
.	O
we	O
have	O
attempted	O
to	O
illustrate	O
this	O
line	O
of	O
thinking	O
with	O
the	O
carbon	O
dioxide	O
prediction	B
example	O
developed	O
at	O
some	O
length	O
in	O
section	O
5.4.3.	O
although	O
this	O
problem	O
is	O
comparatively	O
simple	O
and	O
very	O
easy	O
to	O
get	O
an	O
intuitive	O
understanding	O
of	O
,	O
the	O
principles	O
of	O
trying	O
out	O
diﬀerent	O
components	O
in	O
the	O
covariance	B
structure	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
198	O
further	O
issues	O
and	O
conclusions	O
and	O
adapting	O
their	O
parameters	O
could	O
be	O
used	O
universally	O
.	O
indeed	O
,	O
the	O
use	O
of	O
the	O
isotropic	O
squared	B
exponential	I
covariance	O
function	B
in	O
the	O
digit	O
classiﬁcation	B
examples	O
in	O
chapter	O
3	O
is	O
not	O
really	O
a	O
choice	O
which	O
one	O
would	O
expect	O
to	O
provide	O
very	O
much	O
insight	O
to	O
the	O
classiﬁcation	B
problem	O
.	O
although	O
some	O
of	O
the	O
results	O
presented	O
are	O
as	O
good	O
as	O
other	O
current	O
methods	O
in	O
the	O
literature	O
,	O
one	O
could	O
indeed	O
argue	O
that	O
the	O
use	O
of	O
the	O
squared	B
exponential	I
covariance	O
function	B
for	O
this	O
task	O
makes	O
little	O
sense	O
,	O
and	O
the	O
low	O
error	B
rate	O
is	O
possibly	O
due	O
to	O
the	O
inherently	O
low	O
diﬃculty	O
of	O
the	O
task	O
.	O
there	O
is	O
a	O
need	O
to	O
develop	O
more	O
sensible	O
covariance	B
functions	O
which	O
allow	O
for	O
the	O
incorporation	O
of	O
prior	O
knowledge	O
and	O
help	O
us	O
to	O
gain	O
real	O
insight	O
into	O
the	O
data	O
.	O
going	O
beyond	O
a	O
simple	O
vectorial	O
representation	O
of	O
the	O
input	O
data	O
to	O
take	O
into	O
account	O
structure	O
in	O
the	O
input	O
domain	O
is	O
also	O
a	O
theme	O
which	O
we	O
see	B
as	O
very	O
important	O
.	O
examples	O
of	O
this	O
include	O
the	O
invariances	B
described	O
in	O
section	O
9.10	O
arising	O
from	O
the	O
structure	O
of	O
images	O
,	O
and	O
the	O
kernels	O
described	O
in	O
section	O
4.4	O
which	O
encode	O
structured	O
objects	O
such	O
as	O
strings	O
and	O
trees	O
.	O
as	O
this	O
brief	O
discussion	O
shows	O
,	O
we	O
see	B
the	O
current	O
level	O
of	O
development	O
of	O
gaussian	O
process	B
models	O
more	O
as	O
a	O
rich	O
,	O
principled	O
framework	O
for	O
supervised	B
learning	I
than	O
a	O
fully-developed	O
set	B
of	O
tools	O
for	O
applications	O
.	O
we	O
ﬁnd	O
the	O
gaus-	O
sian	O
process	B
framework	O
very	O
appealing	O
and	O
are	O
conﬁdent	O
that	O
the	O
near	O
future	O
will	O
show	O
many	O
important	O
developments	O
,	O
both	O
in	O
theory	O
,	O
methodology	O
and	O
prac-	O
tice	O
.	O
we	O
look	O
forward	O
very	O
much	O
to	O
following	O
these	O
developments	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
appendix	O
a	O
mathematical	O
background	O
a.1	O
joint	B
,	O
marginal	B
and	O
conditional	B
probability	O
let	O
the	O
n	O
(	O
discrete	O
or	O
continuous	O
)	O
random	O
variables	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
have	O
a	O
joint	B
probability	O
p	O
(	O
y1	O
,	O
.	O
.	O
.	O
,	O
yn	O
)	O
,	O
or	O
p	O
(	O
y	O
)	O
for	O
short.1	O
technically	O
,	O
one	O
ought	O
to	O
distin-	O
guish	O
between	O
probabilities	O
(	O
for	O
discrete	O
variables	O
)	O
and	O
probability	B
densities	O
for	O
continuous	O
variables	O
.	O
throughout	O
the	O
book	O
we	O
commonly	O
use	O
the	O
term	O
“	O
prob-	O
ability	O
”	O
to	O
refer	O
to	O
both	O
.	O
let	O
us	O
partition	O
the	O
variables	O
in	O
y	O
into	O
two	O
groups	O
,	O
ya	O
and	O
yb	O
,	O
where	O
a	O
and	O
b	O
are	O
two	O
disjoint	O
sets	O
whose	O
union	O
is	O
the	O
set	B
{	O
1	O
,	O
.	O
.	O
.	O
,	O
n	O
}	O
,	O
so	O
that	O
p	O
(	O
y	O
)	O
=	O
p	O
(	O
ya	O
,	O
yb	O
)	O
.	O
each	O
group	O
may	O
contain	O
one	O
or	O
more	O
variables	O
.	O
the	O
marginal	B
probability	O
of	O
ya	O
is	O
given	O
by	O
z	O
p	O
(	O
ya	O
)	O
=	O
p	O
(	O
ya	O
,	O
yb	O
)	O
dyb	O
.	O
(	O
a.1	O
)	O
the	O
integral	O
is	O
replaced	O
by	O
a	O
sum	O
if	O
the	O
variables	O
are	O
discrete	O
valued	O
.	O
notice	O
that	O
if	O
the	O
set	B
a	O
contains	O
more	O
than	O
one	O
variable	O
,	O
then	O
the	O
marginal	B
probability	O
is	O
itself	O
a	O
joint	B
probability—whether	O
it	O
is	O
referred	O
to	O
as	O
one	O
or	O
the	O
other	O
depends	O
on	O
the	O
context	O
.	O
if	O
the	O
joint	B
distribution	O
is	O
equal	O
to	O
the	O
product	O
of	O
the	O
marginals	O
,	O
then	O
the	O
variables	O
are	O
said	O
to	O
be	O
independent	O
,	O
otherwise	O
they	O
are	O
dependent	O
.	O
the	O
conditional	B
probability	O
function	B
is	O
deﬁned	O
as	O
p	O
(	O
ya|yb	O
)	O
=	O
p	O
(	O
ya	O
,	O
yb	O
)	O
p	O
(	O
yb	O
)	O
,	O
(	O
a.2	O
)	O
deﬁned	O
for	O
p	O
(	O
yb	O
)	O
>	O
0	O
,	O
as	O
it	O
is	O
not	O
meaningful	O
to	O
condition	O
on	O
an	O
impossible	O
event	O
.	O
if	O
ya	O
and	O
yb	O
are	O
independent	O
,	O
then	O
the	O
marginal	B
p	O
(	O
ya	O
)	O
and	O
the	O
condi-	O
tional	O
p	O
(	O
ya|yb	O
)	O
are	O
equal	O
.	O
1one	O
can	O
deal	O
with	O
more	O
general	O
cases	O
where	O
the	O
density	O
function	B
does	O
not	O
exist	O
by	O
using	O
the	O
distribution	O
function	B
.	O
joint	B
probability	O
marginal	B
probability	O
independence	O
conditional	B
probability	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
200	O
bayes	O
’	O
rule	O
gaussian	O
deﬁnition	O
conditioning	O
and	O
marginalizing	O
products	O
mathematical	O
background	O
theorem	O
using	O
the	O
deﬁnitions	O
of	O
both	O
p	O
(	O
ya|yb	O
)	O
and	O
p	O
(	O
yb|ya	O
)	O
we	O
obtain	O
bayes	O
’	O
p	O
(	O
ya|yb	O
)	O
=	O
p	O
(	O
ya	O
)	O
p	O
(	O
yb|ya	O
)	O
p	O
(	O
yb	O
)	O
.	O
(	O
a.3	O
)	O
since	O
conditional	B
distributions	O
are	O
themselves	O
probabilities	O
,	O
one	O
can	O
use	O
all	O
of	O
the	O
above	O
also	O
when	O
further	O
conditioning	O
on	O
other	O
variables	O
.	O
for	O
example	O
,	O
in	O
supervised	B
learning	I
,	O
one	O
often	O
conditions	O
on	O
the	O
inputs	O
throughout	O
,	O
which	O
would	O
lead	O
e.g	O
.	O
to	O
a	O
version	O
of	O
bayes	O
’	O
rule	O
with	O
additional	O
conditioning	O
on	O
x	O
in	O
all	O
four	O
probabilities	O
in	O
eq	O
.	O
(	O
a.3	O
)	O
;	O
see	B
eq	O
.	O
(	O
2.5	O
)	O
for	O
an	O
example	O
of	O
this	O
.	O
a.2	O
gaussian	O
identities	O
the	O
multivariate	O
gaussian	O
(	O
or	O
normal	O
)	O
distribution	O
has	O
a	O
joint	B
probability	O
den-	O
sity	O
given	O
by	O
p	O
(	O
x|m	O
,	O
σ	O
)	O
=	O
(	O
2π	O
)	O
−d/2|σ|−1/2	O
exp	O
(	O
cid:0	O
)	O
−	O
1	O
2	O
(	O
x	O
−	O
m	O
)	O
>	O
σ−1	O
(	O
x	O
−	O
m	O
)	O
(	O
cid:1	O
)	O
,	O
(	O
a.4	O
)	O
where	O
m	O
is	O
the	O
mean	O
vector	O
(	O
of	O
length	O
d	O
)	O
and	O
σ	O
is	O
the	O
(	O
symmetric	O
,	O
positive	B
deﬁnite	I
)	O
covariance	B
matrix	I
(	O
of	O
size	O
d	O
×	O
d	O
)	O
.	O
as	O
a	O
shorthand	O
we	O
write	O
x	O
∼	O
n	O
(	O
m	O
,	O
σ	O
)	O
.	O
let	O
x	O
and	O
y	O
be	O
jointly	O
gaussian	O
random	O
vectors	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
x	O
y	O
(	O
cid:20	O
)	O
a	O
(	O
cid:18	O
)	O
(	O
cid:20	O
)	O
µx	O
(	O
cid:21	O
)	O
µy	O
∼	O
n	O
,	O
c	O
c	O
>	O
b	O
=	O
n	O
(	O
cid:21	O
)	O
(	O
cid:19	O
)	O
(	O
cid:20	O
)	O
µx	O
(	O
cid:21	O
)	O
µy	O
(	O
cid:20	O
)	O
˜a	O
,	O
˜c	O
>	O
(	O
cid:21	O
)	O
−1	O
!	O
˜c	O
˜b	O
,	O
(	O
a.5	O
)	O
then	O
the	O
marginal	B
distribution	O
of	O
x	O
and	O
the	O
conditional	B
distribution	O
of	O
x	O
given	O
y	O
are	O
x	O
∼	O
n	O
(	O
µx	O
,	O
a	O
)	O
,	O
and	O
x|y	O
∼	O
n	O
(	O
cid:0	O
)	O
µx	O
+	O
cb−1	O
(	O
y	O
−	O
µy	O
)	O
,	O
a	O
−	O
cb−1c	O
>	O
(	O
cid:1	O
)	O
or	O
x|y	O
∼	O
n	O
(	O
cid:0	O
)	O
µx	O
−	O
˜a−1	O
˜c	O
(	O
y	O
−	O
µy	O
)	O
,	O
˜a−1	O
(	O
cid:1	O
)	O
.	O
(	O
a.6	O
)	O
see	B
,	O
e.g	O
.	O
von	O
mises	O
[	O
1964	O
,	O
sec	O
.	O
9.3	O
]	O
,	O
and	O
eqs	O
.	O
(	O
a.11	O
-	O
a.13	O
)	O
.	O
the	O
product	O
of	O
two	O
gaussians	O
gives	O
another	O
(	O
un-normalized	O
)	O
gaussian	O
n	O
(	O
x|a	O
,	O
a	O
)	O
n	O
(	O
x|b	O
,	O
b	O
)	O
=	O
z−1n	O
(	O
x|c	O
,	O
c	O
)	O
(	O
a.7	O
)	O
where	O
c	O
=	O
c	O
(	O
a−1a	O
+	O
b−1b	O
)	O
and	O
c	O
=	O
(	O
a−1	O
+	O
b−1	O
)	O
−1	O
.	O
notice	O
that	O
the	O
resulting	O
gaussian	O
has	O
a	O
precision	O
(	O
inverse	O
variance	O
)	O
equal	O
to	O
the	O
sum	O
of	O
the	O
precisions	O
and	O
a	O
mean	O
equal	O
to	O
the	O
convex	B
sum	O
of	O
the	O
means	O
,	O
weighted	O
by	O
the	O
precisions	O
.	O
the	O
normalizing	O
constant	O
looks	O
itself	O
like	O
a	O
gaussian	O
(	O
in	O
a	O
or	O
b	O
)	O
z−1	O
=	O
(	O
2π	O
)	O
−d/2|a	O
+	O
b|−1/2	O
exp	O
(	O
cid:0	O
)	O
−	O
1	O
2	O
(	O
a	O
−	O
b	O
)	O
>	O
(	O
a	O
+	O
b	O
)	O
−1	O
(	O
a	O
−	O
b	O
)	O
(	O
cid:1	O
)	O
.	O
(	O
a.8	O
)	O
to	O
prove	O
eq	O
.	O
(	O
a.7	O
)	O
simply	O
write	O
out	O
the	O
(	O
lengthy	O
)	O
expressions	O
by	O
introducing	O
eq	O
.	O
(	O
a.4	O
)	O
and	O
eq	O
.	O
(	O
a.8	O
)	O
into	O
eq	O
.	O
(	O
a.7	O
)	O
,	O
and	O
expand	O
the	O
terms	O
inside	O
the	O
exp	O
to	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
a.3	O
matrix	B
identities	O
201	O
generating	O
multivariate	O
gaussian	O
samples	O
verify	O
equality	O
.	O
hint	O
:	O
it	O
may	O
be	O
helpful	O
to	O
expand	O
c	O
using	O
the	O
matrix	B
inversion	O
lemma	O
,	O
eq	O
.	O
(	O
a.9	O
)	O
,	O
c	O
=	O
(	O
a−1+b−1	O
)	O
−1	O
=	O
a−a	O
(	O
a+b	O
)	O
−1a	O
=	O
b−b	O
(	O
a+b	O
)	O
−1b	O
.	O
to	O
generate	O
samples	O
x	O
∼	O
n	O
(	O
m	O
,	O
k	O
)	O
with	O
arbitrary	O
mean	O
m	O
and	O
covariance	B
matrix	I
k	O
using	O
a	O
scalar	O
gaussian	O
generator	O
(	O
which	O
is	O
readily	O
available	O
in	O
many	O
programming	O
environments	O
)	O
we	O
proceed	O
as	O
follows	O
:	O
ﬁrst	O
,	O
compute	O
the	O
cholesky	O
decomposition	O
(	O
also	O
known	O
as	O
the	O
“	O
matrix	B
square	O
root	O
”	O
)	O
l	O
of	O
the	O
positive	O
def-	O
inite	O
symmetric	O
covariance	B
matrix	I
k	O
=	O
ll	O
>	O
,	O
where	O
l	O
is	O
a	O
lower	O
triangular	O
matrix	B
,	O
see	B
section	O
a.4	O
.	O
then	O
generate	O
u	O
∼	O
n	O
(	O
0	O
,	O
i	O
)	O
by	O
multiple	O
separate	O
calls	O
to	O
the	O
scalar	O
gaussian	O
generator	O
.	O
compute	O
x	O
=	O
m	O
+	O
lu	O
,	O
which	O
has	O
the	O
desired	O
distribution	O
with	O
mean	O
m	O
and	O
covariance	B
le	O
[	O
uu	O
>	O
]	O
l	O
>	O
=	O
ll	O
>	O
=	O
k	O
(	O
by	O
the	O
independence	O
of	O
the	O
elements	O
of	O
u	O
)	O
.	O
in	O
practice	O
it	O
may	O
be	O
necessary	O
to	O
add	O
a	O
small	O
multiple	O
of	O
the	O
identity	O
matrix	B
i	O
to	O
the	O
covariance	B
matrix	I
for	O
numerical	O
reasons	O
.	O
this	O
is	O
because	O
the	O
eigenvalues	O
of	O
the	O
matrix	B
k	O
can	O
decay	O
very	O
rapidly	O
(	O
see	B
section	O
4.3.1	O
for	O
a	O
closely	O
related	O
analytical	O
result	O
)	O
and	O
without	O
this	O
stabilization	O
the	O
cholesky	O
decomposition	O
fails	O
.	O
the	O
eﬀect	O
on	O
the	O
generated	O
samples	O
is	O
to	O
add	O
additional	O
independent	O
noise	O
of	O
variance	O
	O
.	O
from	O
the	O
context	O
	O
can	O
usually	O
be	O
chosen	O
to	O
have	O
inconsequential	O
eﬀects	O
on	O
the	O
samples	O
,	O
while	O
ensuring	O
numerical	O
stability	O
.	O
a.3	O
matrix	B
identities	O
the	O
matrix	B
inversion	O
lemma	O
,	O
also	O
known	O
as	O
the	O
woodbury	O
,	O
sherman	O
&	O
morri-	O
matrix	B
inversion	O
lemma	O
son	O
formula	O
(	O
see	B
e.g	O
.	O
press	O
et	O
al	O
.	O
[	O
1992	O
,	O
p.	O
75	O
]	O
)	O
states	O
that	O
(	O
z	O
+	O
u	O
w	O
v	O
>	O
)	O
−1	O
=	O
z−1	O
−	O
z−1u	O
(	O
w	O
−1	O
+	O
v	O
>	O
z−1u	O
)	O
−1v	O
>	O
z−1	O
,	O
(	O
a.9	O
)	O
assuming	O
the	O
relevant	O
inverses	O
all	O
exist	O
.	O
here	O
z	O
is	O
n×n	O
,	O
w	O
is	O
m×m	O
and	O
u	O
and	O
v	O
are	O
both	O
of	O
size	O
n×m	O
;	O
consequently	O
if	O
z−1	O
is	O
known	O
,	O
and	O
a	O
low	O
rank	O
(	O
i.e	O
.	O
m	O
<	O
n	O
)	O
perturbation	O
is	O
made	O
to	O
z	O
as	O
in	O
left	O
hand	O
side	O
of	O
eq	O
.	O
(	O
a.9	O
)	O
,	O
considerable	O
speedup	O
can	O
be	O
achieved	O
.	O
a	O
similar	O
equation	O
exists	O
for	O
determinants	O
|z	O
+	O
u	O
w	O
v	O
>	O
|	O
=	O
|z|	O
|w|	O
|w	O
−1	O
+	O
v	O
>	O
z−1u|	O
.	O
(	O
a.10	O
)	O
let	O
the	O
invertible	O
n	O
×	O
n	O
matrix	B
a	O
and	O
its	O
inverse	O
a−1	O
be	O
partitioned	O
into	O
(	O
cid:18	O
)	O
p	O
q	O
(	O
cid:19	O
)	O
r	O
s	O
(	O
cid:19	O
)	O
(	O
cid:18	O
)	O
˜p	O
˜q	O
˜r	O
˜s	O
a	O
=	O
,	O
a−1	O
=	O
,	O
(	O
a.11	O
)	O
where	O
p	O
and	O
˜p	O
are	O
n1	O
×	O
n1	O
matrices	O
and	O
s	O
and	O
˜s	O
are	O
n2	O
×	O
n2	O
matrices	O
with	O
n	O
=	O
n1	O
+	O
n2	O
.	O
the	O
submatrices	O
of	O
a−1	O
are	O
given	O
in	O
press	O
et	O
al	O
.	O
[	O
1992	O
,	O
p.	O
77	O
]	O
as	O
˜p	O
=	O
p	O
−1	O
+	O
p	O
−1qm	O
rp	O
−1	O
˜q	O
=	O
−p	O
−1qm	O
˜r	O
=	O
−m	O
rp	O
−1	O
˜s	O
=	O
m	O
	O
where	O
m	O
=	O
(	O
s	O
−	O
rp	O
−1q	O
)	O
−1	O
,	O
(	O
a.12	O
)	O
determinants	O
inversion	O
of	O
a	O
partitioned	O
matrix	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
202	O
mathematical	O
background	O
or	O
equivalently	O
˜p	O
=	O
n	O
˜q	O
=	O
−n	O
qs−1	O
˜r	O
=	O
−s−1rn	O
˜s	O
=	O
s−1	O
+	O
s−1rn	O
qs−1	O
	O
where	O
n	O
=	O
(	O
p	O
−	O
qs−1r	O
)	O
−1	O
.	O
a.3.1	O
matrix	B
derivatives	O
derivatives	O
of	O
the	O
elements	O
of	O
an	O
inverse	O
matrix	B
:	O
k−1	O
=	O
−k−1	O
∂k	O
∂θ	O
∂	O
∂θ	O
k−1	O
,	O
(	O
a.13	O
)	O
(	O
a.14	O
)	O
where	O
∂k	O
positive	B
deﬁnite	I
symmetric	O
matrix	B
we	O
have	O
∂θ	O
is	O
a	O
matrix	B
of	O
elementwise	O
derivatives	O
.	O
for	O
the	O
log	O
determinant	O
of	O
a	O
log	O
|k|	O
=	O
tr	O
(	O
cid:0	O
)	O
k−1	O
∂k	O
(	O
cid:1	O
)	O
.	O
∂θ	O
∂	O
∂θ	O
(	O
a.15	O
)	O
derivative	O
of	O
inverse	O
derivative	O
of	O
log	O
determinant	O
a.3.2	O
matrix	B
norms	O
the	O
frobenius	O
norm	B
kakf	O
of	O
a	O
n1	O
×	O
n2	O
matrix	B
a	O
is	O
deﬁned	O
as	O
n1x	O
n2x	O
kak2	O
f	O
=	O
|aij|2	O
=	O
tr	O
(	O
aa	O
>	O
)	O
,	O
(	O
a.16	O
)	O
[	O
golub	O
and	O
van	O
loan	O
,	O
1989	O
,	O
p.	O
56	O
]	O
.	O
i=1	O
j=1	O
a.4	O
cholesky	O
decomposition	O
the	O
cholesky	O
decomposition	O
of	O
a	O
symmetric	O
,	O
positive	B
deﬁnite	I
matrix	I
a	O
decom-	O
poses	O
a	O
into	O
a	O
product	O
of	O
a	O
lower	O
triangular	O
matrix	B
l	O
and	O
its	O
transpose	O
ll	O
>	O
=	O
a	O
,	O
(	O
a.17	O
)	O
where	O
l	O
is	O
called	O
the	O
cholesky	O
factor	O
.	O
the	O
cholesky	O
decomposition	O
is	O
useful	O
for	O
solving	O
linear	B
systems	O
with	O
symmetric	O
,	O
positive	B
deﬁnite	I
coeﬃcient	O
matrix	B
a.	O
to	O
solve	O
ax	O
=	O
b	O
for	O
x	O
,	O
ﬁrst	O
solve	O
the	O
triangular	O
system	O
ly	O
=	O
b	O
by	O
forward	O
substitution	O
and	O
then	O
the	O
triangular	O
system	O
l	O
>	O
x	O
=	O
y	O
by	O
back	O
substitution	O
.	O
using	O
the	O
backslash	O
operator	B
,	O
we	O
write	O
the	O
solution	O
as	O
x	O
=	O
l	O
>	O
\	O
(	O
l\b	O
)	O
,	O
where	O
the	O
notation	O
a\b	O
is	O
the	O
vector	O
x	O
which	O
solves	O
ax	O
=	O
b.	O
both	O
the	O
forward	O
and	O
backward	O
substitution	O
steps	O
require	O
n2/2	O
operations	O
,	O
when	O
a	O
is	O
of	O
size	O
n	O
×	O
n.	O
the	O
computation	O
of	O
the	O
cholesky	O
factor	O
l	O
is	O
considered	O
numerically	O
extremely	O
stable	O
and	O
takes	O
time	O
n3/6	O
,	O
so	O
it	O
is	O
the	O
method	O
of	O
choice	O
when	O
it	O
can	O
be	O
applied	O
.	O
solving	O
linear	B
systems	O
computational	O
cost	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
a.5	O
entropy	B
and	O
kullback-leibler	O
divergence	O
ny	O
nx	O
note	O
also	O
that	O
the	O
determinant	O
of	O
a	O
positive	B
deﬁnite	I
symmetric	O
matrix	B
can	O
be	O
calculated	O
eﬃciently	O
by	O
|a|	O
=	O
ii	O
,	O
or	O
l2	O
log	O
|a|	O
=	O
2	O
log	O
lii	O
,	O
(	O
a.18	O
)	O
where	O
l	O
is	O
the	O
cholesky	O
factor	O
from	O
a.	O
i=1	O
i=1	O
a.5	O
entropy	B
and	O
kullback-leibler	O
divergence	O
the	O
entropy	B
h	O
[	O
p	O
(	O
x	O
)	O
]	O
of	O
a	O
distribution	O
p	O
(	O
x	O
)	O
is	O
a	O
non-negative	O
measure	B
of	O
the	O
amount	O
of	O
“	O
uncertainty	O
”	O
in	O
the	O
distribution	O
,	O
and	O
is	O
deﬁned	O
as	O
h	O
[	O
p	O
(	O
x	O
)	O
]	O
=	O
−	O
p	O
(	O
x	O
)	O
log	O
p	O
(	O
x	O
)	O
dx	O
.	O
(	O
a.19	O
)	O
z	O
the	O
integral	O
is	O
substituted	O
by	O
a	O
sum	O
for	O
discrete	O
variables	O
.	O
entropy	B
is	O
measured	O
in	O
bits	B
if	O
the	O
log	O
is	O
to	O
the	O
base	O
2	O
and	O
in	O
nats	B
in	O
the	O
case	O
of	O
the	O
natural	O
log	O
.	O
the	O
entropy	B
of	O
a	O
gaussian	O
in	O
d	O
dimensions	O
,	O
measured	O
in	O
nats	B
is	O
h	O
[	O
n	O
(	O
µ	O
,	O
σ	O
)	O
]	O
=	O
1	O
(	O
a.20	O
)	O
the	O
kullback-leibler	O
(	O
kl	O
)	O
divergence	O
(	O
or	O
relative	B
entropy	I
)	O
kl	O
(	O
p||q	O
)	O
be-	O
2	O
(	O
log	O
2πe	O
)	O
.	O
tween	O
two	O
distributions	O
p	O
(	O
x	O
)	O
and	O
q	O
(	O
x	O
)	O
is	O
deﬁned	O
as	O
p	O
(	O
x	O
)	O
log	O
p	O
(	O
x	O
)	O
kl	O
(	O
p||q	O
)	O
=	O
q	O
(	O
x	O
)	O
dx	O
.	O
(	O
a.21	O
)	O
2	O
log	O
|σ|	O
+	O
d	O
z	O
it	O
is	O
easy	O
to	O
show	O
that	O
kl	O
(	O
p||q	O
)	O
≥	O
0	O
,	O
with	O
equality	O
if	O
p	O
=	O
q	O
(	O
almost	O
everywhere	O
)	O
.	O
for	O
the	O
case	O
of	O
two	O
bernoulli	O
random	O
variables	O
p	O
and	O
q	O
this	O
reduces	O
to	O
klber	O
(	O
p||q	O
)	O
=	O
p	O
log	O
p	O
q	O
+	O
(	O
1	O
−	O
p	O
)	O
log	O
(	O
1	O
−	O
p	O
)	O
(	O
1	O
−	O
q	O
)	O
,	O
(	O
a.22	O
)	O
where	O
we	O
use	O
p	O
and	O
q	O
both	O
as	O
the	O
name	O
and	O
the	O
parameter	O
of	O
the	O
bernoulli	O
distributions	O
.	O
for	O
two	O
gaussian	O
distributions	O
n	O
(	O
µ0	O
,	O
σ0	O
)	O
and	O
n	O
(	O
µ1	O
,	O
σ1	O
)	O
we	O
have	O
[	O
kullback	O
,	O
1959	O
,	O
sec	O
.	O
9.1	O
]	O
kl	O
(	O
n0||n1	O
)	O
=	O
1	O
0	O
|	O
+	O
(	O
cid:0	O
)	O
(	O
µ0	O
−	O
µ1	O
)	O
(	O
µ0	O
−	O
µ1	O
)	O
>	O
+	O
σ0	O
−	O
σ1	O
(	O
cid:1	O
)	O
.	O
consider	O
a	O
general	O
distribution	O
p	O
(	O
x	O
)	O
on	O
rd	O
and	O
a	O
gaussian	O
distribution	O
q	O
(	O
x	O
)	O
=	O
n	O
(	O
µ	O
,	O
σ	O
)	O
.	O
then	O
kl	O
(	O
p||q	O
)	O
=	O
1	O
2	O
(	O
x	O
−	O
µ	O
)	O
>	O
σ−1	O
(	O
x	O
−	O
µ	O
)	O
p	O
(	O
x	O
)	O
dx	O
+	O
z	O
2	O
log	O
2π	O
+	O
p	O
(	O
x	O
)	O
log	O
p	O
(	O
x	O
)	O
dx	O
.	O
1	O
1	O
2	O
log	O
|σ1σ−1	O
2	O
tr	O
σ−1	O
z	O
2	O
log	O
|σ|	O
+	O
d	O
1	O
(	O
a.23	O
)	O
(	O
a.24	O
)	O
203	O
determinant	O
entropy	B
divergence	O
of	O
bernoulli	O
random	O
variables	O
divergence	O
of	O
gaussians	O
minimizing	O
kl	O
(	O
p||q	O
)	O
divergence	O
leads	O
to	O
moment	O
matching	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
204	O
mathematical	O
background	O
equation	O
(	O
a.24	O
)	O
can	O
be	O
minimized	O
w.r.t	O
.	O
µ	O
and	O
σ	O
by	O
diﬀerentiating	O
w.r.t	O
.	O
these	O
parameters	O
and	O
setting	O
the	O
resulting	O
expressions	O
to	O
zero	O
.	O
the	O
optimal	B
q	O
is	O
the	O
one	O
that	O
matches	O
the	O
ﬁrst	O
and	O
second	O
moments	O
of	O
p.	O
the	O
kl	O
divergence	O
can	O
be	O
viewed	O
as	O
the	O
extra	O
number	O
of	O
nats	B
needed	O
on	O
average	O
to	O
code	O
data	O
generated	O
from	O
a	O
source	O
p	O
(	O
x	O
)	O
under	O
the	O
distribution	O
q	O
(	O
x	O
)	O
as	O
opposed	O
to	O
p	O
(	O
x	O
)	O
.	O
a.6	O
limits	O
the	O
limit	O
of	O
a	O
rational	B
quadratic	I
is	O
a	O
squared	B
exponential	I
(	O
cid:0	O
)	O
1	O
+	O
x2	O
2α	O
(	O
cid:1	O
)	O
−α	O
=	O
exp	O
(	O
cid:0	O
)	O
−	O
x2	O
(	O
cid:1	O
)	O
.	O
2	O
lim	O
α→∞	O
(	O
a.25	O
)	O
a.7	O
measure	B
and	O
integration	O
here	O
we	O
sketch	O
some	O
deﬁnitions	O
concerning	O
measure	B
and	O
integration	O
;	O
fuller	O
treatments	O
can	O
be	O
found	O
e.g	O
.	O
in	O
doob	O
[	O
1994	O
]	O
and	O
bartle	O
[	O
1995	O
]	O
.	O
let	O
ω	O
be	O
the	O
set	B
of	O
all	O
possible	O
outcomes	O
of	O
an	O
experiment	O
.	O
for	O
example	O
,	O
for	O
a	O
d-dimensional	O
real-valued	O
variable	O
,	O
ω	O
=	O
rd	O
.	O
let	O
f	O
be	O
a	O
σ-ﬁeld	O
of	O
subsets	O
of	O
ω	O
which	O
contains	O
all	O
the	O
events	O
in	O
whose	O
occurrences	O
we	O
may	O
be	O
interested.2	O
then	O
µ	O
is	O
a	O
countably	O
additive	O
measure	B
if	O
it	O
is	O
real	O
and	O
non-negative	O
and	O
for	O
all	O
mutually	O
disjoint	O
sets	O
a1	O
,	O
a2	O
,	O
.	O
.	O
.	O
∈	O
f	O
we	O
have	O
µ	O
(	O
cid:0	O
)	O
∞	O
[	O
ai	O
(	O
cid:1	O
)	O
=	O
∞x	O
µ	O
(	O
ai	O
)	O
.	O
(	O
a.26	O
)	O
ﬁnite	O
measure	B
probability	O
measure	B
lebesgue	O
measure	B
i=1	O
i=1	O
if	O
µ	O
(	O
ω	O
)	O
<	O
∞	O
then	O
µ	O
is	O
called	O
a	O
ﬁnite	O
measure	B
and	O
if	O
µ	O
(	O
ω	O
)	O
=	O
1	O
it	O
is	O
called	O
a	O
probability	B
measure	O
.	O
the	O
lebesgue	O
measure	B
deﬁnes	O
a	O
uniform	O
measure	B
over	O
subsets	O
of	O
euclidean	O
space	O
.	O
here	O
an	O
appropriate	O
σ-algebra	O
is	O
the	O
borel	O
σ-algebra	O
bd	O
,	O
where	O
b	O
is	O
the	O
σ-algebra	O
generated	O
by	O
the	O
open	O
subsets	O
of	O
r.	O
for	O
example	O
on	O
the	O
line	O
r	O
the	O
lebesgue	O
measure	B
of	O
the	O
interval	O
(	O
a	O
,	O
b	O
)	O
is	O
b	O
−	O
a.	O
we	O
now	O
restrict	O
ω	O
to	O
be	O
rd	O
and	O
wish	O
to	O
give	O
meaning	O
to	O
integration	O
of	O
a	O
function	B
f	O
:	O
rd	O
→	O
r	O
with	O
respect	O
to	O
a	O
measure	B
µ	O
z	O
f	O
(	O
x	O
)	O
dµ	O
(	O
x	O
)	O
.	O
(	O
a.27	O
)	O
we	O
assume	O
that	O
f	O
is	O
measurable	O
,	O
i.e	O
.	O
that	O
for	O
any	O
borel-measurable	O
set	B
a	O
∈	O
r	O
,	O
f−1	O
(	O
a	O
)	O
∈	O
bd	O
.	O
there	O
are	O
two	O
cases	O
that	O
will	O
interest	O
us	O
(	O
i	O
)	O
when	O
µ	O
is	O
the	O
lebesgue	O
measure	B
and	O
(	O
ii	O
)	O
when	O
µ	O
is	O
a	O
probability	B
measure	O
.	O
for	O
the	O
ﬁrst	O
case	O
expression	O
(	O
a.27	O
)	O
reduces	O
to	O
the	O
usual	O
integral	O
notationr	O
f	O
(	O
x	O
)	O
dx	O
.	O
2the	O
restriction	O
to	O
a	O
σ-ﬁeld	O
of	O
subsets	O
is	O
important	O
technically	O
to	O
avoid	O
paradoxes	O
such	O
as	O
the	O
banach-tarski	O
paradox	O
.	O
informally	O
,	O
we	O
can	O
think	O
of	O
the	O
σ-ﬁeld	O
as	O
restricting	O
consideration	O
to	O
“	O
reasonable	O
”	O
subsets	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
a.8	O
fourier	O
transforms	O
205	O
for	O
a	O
probability	B
measure	O
µ	O
on	O
x	O
,	O
the	O
non-negative	O
function	B
p	O
(	O
x	O
)	O
is	O
called	O
the	O
density	O
of	O
the	O
measure	B
if	O
for	O
all	O
a	O
∈	O
bd	O
we	O
have	O
µ	O
(	O
a	O
)	O
=	O
p	O
(	O
x	O
)	O
dx	O
.	O
(	O
a.28	O
)	O
if	O
such	O
a	O
density	O
exists	O
it	O
is	O
uniquely	O
determined	O
almost	O
everywhere	O
,	O
i.e	O
.	O
except	O
for	O
sets	O
with	O
measure	B
zero	O
.	O
not	O
all	O
probability	B
measures	O
have	O
densities—only	O
distributions	O
that	O
assign	O
zero	O
probability	B
to	O
individual	O
points	O
in	O
x-space	O
can	O
have	O
densities.3	O
if	O
p	O
(	O
x	O
)	O
exists	O
then	O
we	O
have	O
z	O
f	O
(	O
x	O
)	O
dµ	O
(	O
x	O
)	O
=	O
f	O
(	O
x	O
)	O
p	O
(	O
x	O
)	O
dx	O
.	O
(	O
a.29	O
)	O
z	O
a	O
z	O
if	O
µ	O
does	O
not	O
have	O
a	O
density	O
expression	O
(	O
a.27	O
)	O
still	O
has	O
meaning	O
by	O
the	O
standard	O
construction	O
of	O
the	O
lebesgue	O
integral	O
.	O
for	O
ω	O
=	O
rd	O
the	O
probability	B
measure	O
µ	O
can	O
be	O
related	O
to	O
the	O
distribution	O
function	B
f	O
:	O
rd	O
→	O
[	O
0	O
,	O
1	O
]	O
which	O
is	O
deﬁned	O
as	O
f	O
(	O
z	O
)	O
=	O
µ	O
(	O
x1	O
≤	O
z1	O
,	O
.	O
.	O
.	O
xd	O
≤	O
zd	O
)	O
.	O
the	O
distribution	O
function	B
is	O
more	O
general	O
than	O
the	O
density	O
as	O
it	O
is	O
always	O
deﬁned	O
for	O
a	O
given	O
probability	B
measure	O
.	O
a	O
simple	O
example	O
of	O
a	O
random	O
variable	O
which	O
has	O
a	O
distribution	O
function	B
but	O
no	O
density	O
is	O
obtained	O
by	O
the	O
following	O
construction	O
:	O
a	O
coin	O
is	O
tossed	O
and	O
with	O
probability	B
p	O
it	O
comes	O
up	O
heads	O
;	O
if	O
it	O
comes	O
up	O
heads	O
x	O
is	O
chosen	O
from	O
u	O
(	O
0	O
,	O
1	O
)	O
(	O
the	O
uniform	O
distribution	O
on	O
[	O
0	O
,	O
1	O
]	O
)	O
,	O
otherwise	O
(	O
with	O
probability	B
1−	O
p	O
)	O
x	O
is	O
set	B
to	O
1/2	O
.	O
this	O
distribution	O
has	O
a	O
“	O
point	O
mass	O
”	O
(	O
or	O
atom	O
)	O
at	O
x	O
=	O
1/2	O
.	O
“	O
point	O
mass	O
”	O
example	O
a.7.1	O
lp	O
spaces	O
let	O
µ	O
be	O
a	O
measure	B
on	O
an	O
input	O
set	B
x	O
.	O
for	O
some	O
function	B
f	O
:	O
x	O
→	O
r	O
and	O
1	O
≤	O
p	O
<	O
∞	O
,	O
we	O
deﬁne	O
kfklp	O
(	O
x	O
,	O
µ	O
)	O
,	O
(	O
cid:16	O
)	O
z	O
(	O
cid:17	O
)	O
1/p	O
|f	O
(	O
x	O
)	O
|p	O
dµ	O
(	O
x	O
)	O
if	O
the	O
integral	O
exists	O
.	O
for	O
p	O
=	O
∞	O
we	O
deﬁne	O
kfkl∞	O
(	O
x	O
,	O
µ	O
)	O
=	O
ess	O
sup	O
x∈x	O
|f	O
(	O
x	O
)	O
|	O
,	O
,	O
(	O
a.30	O
)	O
(	O
a.31	O
)	O
where	O
ess	O
sup	O
denotes	O
the	O
essential	O
supremum	O
,	O
i.e	O
.	O
the	O
smallest	O
number	O
that	O
upper	O
bounds	O
|f	O
(	O
x	O
)	O
|	O
almost	O
everywhere	O
.	O
the	O
function	B
space	O
lp	O
(	O
x	O
,	O
µ	O
)	O
is	O
deﬁned	O
for	O
any	O
p	O
in	O
1	O
≤	O
p	O
≤	O
∞	O
as	O
the	O
space	O
of	O
functions	O
for	O
which	O
kfklp	O
(	O
x	O
,	O
µ	O
)	O
<	O
∞	O
.	O
a.8	O
fourier	O
transforms	O
for	O
suﬃciently	O
well-behaved	O
functions	O
on	O
rd	O
we	O
have	O
f	O
(	O
x	O
)	O
=	O
˜f	O
(	O
s	O
)	O
e2πis·x	O
ds	O
,	O
˜f	O
(	O
s	O
)	O
=	O
f	O
(	O
x	O
)	O
e−2πis·x	O
dx	O
,	O
(	O
a.32	O
)	O
z	O
∞	O
−∞	O
z	O
∞	O
−∞	O
3a	O
measure	B
µ	O
has	O
a	O
density	O
if	O
and	O
only	O
if	O
it	O
is	O
absolutely	O
continuous	O
with	O
respect	O
to	O
lebesgue	O
measure	B
on	O
rd	O
,	O
i.e	O
.	O
every	O
set	B
that	O
has	O
lebesgue	O
measure	B
zero	O
also	O
has	O
µ-measure	O
zero	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
206	O
mathematical	O
background	O
where	O
˜f	O
(	O
s	O
)	O
is	O
called	O
the	O
fourier	O
transform	O
of	O
f	O
(	O
x	O
)	O
,	O
see	B
e.g	O
.	O
bracewell	O
[	O
1986	O
]	O
.	O
we	O
refer	O
to	O
the	O
equation	O
on	O
the	O
left	O
as	O
the	O
synthesis	O
equation	O
,	O
and	O
the	O
equation	O
on	O
the	O
right	O
as	O
the	O
analysis	O
equation	O
.	O
there	O
are	O
other	O
conventions	O
for	O
fourier	O
transforms	O
,	O
particularly	O
those	O
involving	O
ω	O
=	O
2πs	O
.	O
however	O
,	O
this	O
tends	O
to	O
de-	O
stroy	O
symmetry	O
between	O
the	O
analysis	O
and	O
synthesis	O
equations	O
so	O
we	O
use	O
the	O
deﬁnitions	O
given	O
above	O
.	O
here	O
we	O
have	O
deﬁned	O
fourier	O
transforms	O
for	O
f	O
(	O
x	O
)	O
being	O
a	O
function	B
on	O
rd	O
.	O
for	O
related	O
transforms	O
for	O
periodic	B
functions	O
,	O
functions	O
deﬁned	O
on	O
the	O
integer	O
lattice	O
and	O
on	O
the	O
regular	O
n-polygon	O
see	B
section	O
b.1	O
.	O
a.9	O
convexity	O
convex	B
sets	O
convex	B
function	O
below	O
we	O
state	O
some	O
deﬁnitions	O
and	O
properties	O
of	O
convex	B
sets	O
and	O
functions	O
taken	O
from	O
boyd	O
and	O
vandenberghe	O
[	O
2004	O
]	O
.	O
a	O
set	B
c	O
is	O
convex	B
if	O
the	O
line	O
segment	O
between	O
any	O
two	O
points	O
in	O
c	O
lies	O
in	O
c	O
,	O
i.e	O
.	O
if	O
for	O
any	O
x1	O
,	O
x2	O
∈	O
c	O
and	O
for	O
any	O
θ	O
with	O
0	O
≤	O
θ	O
≤	O
1	O
,	O
we	O
have	O
(	O
a.33	O
)	O
a	O
function	B
f	O
:	O
x	O
→	O
r	O
is	O
convex	B
if	O
its	O
domain	O
x	O
is	O
a	O
convex	B
set	O
and	O
if	O
for	O
all	O
x1	O
,	O
x2	O
∈	O
x	O
and	O
θ	O
with	O
0	O
≤	O
θ	O
≤	O
1	O
,	O
we	O
have	O
:	O
θx1	O
+	O
(	O
1	O
−	O
θ	O
)	O
x2	O
∈	O
c.	O
f	O
(	O
θx1	O
+	O
(	O
1	O
−	O
θ	O
)	O
x2	O
)	O
≤	O
θf	O
(	O
x1	O
)	O
+	O
(	O
1	O
−	O
θ	O
)	O
f	O
(	O
x2	O
)	O
,	O
(	O
a.34	O
)	O
where	O
x	O
is	O
a	O
(	O
possibly	O
improper	O
)	O
subset	O
of	O
rd	O
.	O
f	O
is	O
concave	O
if	O
−f	O
is	O
convex	B
.	O
a	O
function	B
f	O
is	O
convex	B
if	O
and	O
only	O
if	O
its	O
domain	O
x	O
is	O
a	O
convex	B
set	O
and	O
its	O
hessian	O
is	O
positive	B
semideﬁnite	I
for	O
all	O
x	O
∈	O
x	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
appendix	O
b	O
gaussian	O
markov	O
processes	O
a	O
discrete-time	O
autoregressive	O
(	O
ar	O
)	O
process	B
of	O
order	O
p	O
can	O
be	O
written	O
as	O
ar	O
process	B
px	O
particularly	O
when	O
the	O
index	O
set	B
for	O
a	O
stochastic	O
process	O
is	O
one-dimensional	O
such	O
as	O
the	O
real	O
line	O
or	O
its	O
discretization	O
onto	O
the	O
integer	O
lattice	O
,	O
it	O
is	O
very	O
interesting	O
to	O
investigate	O
the	O
properties	O
of	O
gaussian	O
markov	O
processes	O
(	O
gmps	O
)	O
.	O
in	O
this	O
appendix	O
we	O
use	O
x	O
(	O
t	O
)	O
to	O
deﬁne	O
a	O
stochastic	O
process	O
with	O
continuous	O
time	O
pa-	O
rameter	O
t.	O
in	O
the	O
discrete	O
time	O
case	O
the	O
process	B
is	O
denoted	O
.	O
.	O
.	O
,	O
x−1	O
,	O
x0	O
,	O
x1	O
,	O
.	O
.	O
.	O
etc	O
.	O
we	O
assume	O
that	O
the	O
process	B
has	O
zero	O
mean	O
and	O
is	O
,	O
unless	O
otherwise	O
stated	O
,	O
stationary	O
.	O
xt	O
=	O
akxt−k	O
+	O
b0zt	O
,	O
(	O
b.1	O
)	O
k=1	O
where	O
zt	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
and	O
all	O
zt	O
’	O
s	O
are	O
i.i.d	O
.	O
.	O
notice	O
the	O
order-p	O
markov	O
property	O
that	O
given	O
the	O
history	O
xt−1	O
,	O
xt−2	O
,	O
.	O
.	O
.	O
,	O
xt	O
depends	O
only	O
on	O
the	O
previous	O
p	O
x	O
’	O
s	O
.	O
this	O
relationship	O
can	O
be	O
conveniently	O
expressed	O
as	O
a	O
graphical	O
model	B
;	O
part	O
of	O
an	O
ar	O
(	O
2	O
)	O
process	B
is	O
illustrated	O
in	O
figure	O
b.1	O
.	O
the	O
name	O
autoregressive	O
stems	O
from	O
the	O
fact	O
that	O
xt	O
is	O
predicted	O
from	O
the	O
p	O
previous	O
x	O
’	O
s	O
through	O
a	O
regression	B
equation	O
.	O
if	O
one	O
stores	O
the	O
current	O
x	O
and	O
the	O
p	O
−	O
1	O
previous	O
values	O
as	O
a	O
state	O
vector	O
,	O
then	O
the	O
ar	O
(	O
p	O
)	O
scalar	O
process	B
can	O
be	O
written	O
equivalently	O
as	O
a	O
vector	O
ar	O
(	O
1	O
)	O
process	B
.	O
figure	O
b.1	O
:	O
graphical	O
model	B
illustrating	O
an	O
ar	O
(	O
2	O
)	O
process	B
.	O
moving	O
from	O
the	O
discrete	O
time	O
to	O
the	O
continuous	O
time	O
setting	O
,	O
the	O
question	O
arises	O
as	O
to	O
how	O
generalize	O
the	O
markov	O
notion	O
used	O
in	O
the	O
discrete-time	O
ar	O
process	B
to	O
deﬁne	O
a	O
continuoous-time	O
ar	O
process	B
.	O
it	O
turns	O
out	O
that	O
the	O
correct	O
generalization	B
uses	O
the	O
idea	O
of	O
having	O
not	O
only	O
the	O
function	B
value	O
but	O
also	O
p	O
of	O
its	O
derivatives	O
at	O
time	O
t	O
giving	O
rise	O
to	O
the	O
stochastic	B
diﬀerential	I
equation	I
(	O
sde	O
)	O
1	O
sde	O
:	O
stochastic	B
diﬀerential	I
equation	I
.	O
.	O
..	O
.	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
208	O
gaussian	O
markov	O
processes	O
apx	O
(	O
p	O
)	O
(	O
t	O
)	O
+	O
ap−1x	O
(	O
p−1	O
)	O
(	O
t	O
)	O
+	O
.	O
.	O
.	O
+	O
a0x	O
(	O
t	O
)	O
=	O
b0z	O
(	O
t	O
)	O
,	O
(	O
b.2	O
)	O
where	O
x	O
(	O
i	O
)	O
(	O
t	O
)	O
denotes	O
the	O
ith	O
derivative	O
of	O
x	O
(	O
t	O
)	O
and	O
z	O
(	O
t	O
)	O
is	O
a	O
white	O
gaus-	O
sian	O
noise	O
process	O
with	O
covariance	B
δ	O
(	O
t	O
−	O
t0	O
)	O
.	O
this	O
white	O
noise	O
process	O
can	O
be	O
considered	O
the	O
derivative	O
of	O
the	O
wiener	O
process	B
.	O
to	O
avoid	O
redundancy	O
in	O
the	O
coeﬃcients	O
we	O
assume	O
that	O
ap	O
=	O
1.	O
a	O
considerable	O
amount	O
of	O
mathemati-	O
cal	O
machinery	O
is	O
required	O
to	O
make	O
rigorous	O
the	O
meaning	O
of	O
such	O
equations	O
,	O
see	B
e.g	O
.	O
øksendal	O
[	O
1985	O
]	O
.	O
as	O
for	O
the	O
discrete-time	O
case	O
,	O
one	O
can	O
write	O
eq	O
.	O
(	O
b.2	O
)	O
as	O
a	O
ﬁrst-order	O
vector	O
sde	O
by	O
deﬁning	O
the	O
state	O
to	O
be	O
x	O
(	O
t	O
)	O
and	O
its	O
ﬁrst	O
p	O
−	O
1	O
derivatives	O
.	O
we	O
begin	O
this	O
chapter	O
with	O
a	O
summary	O
of	O
some	O
fourier	O
analysis	O
results	O
in	O
section	O
b.1	O
.	O
fourier	O
analysis	O
is	O
important	O
to	O
linear	B
time	O
invariant	O
systems	O
such	O
as	O
equations	O
(	O
b.1	O
)	O
and	O
(	O
b.2	O
)	O
because	O
e2πist	O
is	O
an	O
eigenfunction	B
of	O
the	O
corre-	O
sponding	O
diﬀerence	O
(	O
resp	O
diﬀerential	O
)	O
operator	B
.	O
we	O
then	O
move	O
on	O
in	O
section	O
b.2	O
to	O
discuss	O
continuous-time	O
gaussian	O
markov	O
processes	O
on	O
the	O
real	O
line	O
and	O
their	O
relationship	O
to	O
the	O
same	O
sde	O
on	O
the	O
circle	O
.	O
in	O
section	O
b.3	O
we	O
describe	O
discrete-time	O
gaussian	O
markov	O
processes	O
on	O
the	O
integer	O
lattice	O
and	O
their	O
re-	O
lationship	O
to	O
the	O
same	O
diﬀerence	O
equation	O
on	O
the	O
circle	O
.	O
in	O
section	O
b.4	O
we	O
explain	O
the	O
relationship	O
between	O
discrete-time	O
gmps	O
and	O
the	O
discrete	O
sampling	O
of	O
continuous-time	O
gmps	O
.	O
finally	O
in	O
section	O
b.5	O
we	O
discuss	O
generalizations	O
of	O
the	O
markov	O
concept	O
in	O
higher	O
dimensions	O
.	O
much	O
of	O
this	O
material	O
is	O
quite	O
standard	O
,	O
although	O
the	O
relevant	O
results	O
are	O
often	O
scattered	O
through	O
diﬀerent	O
sources	O
,	O
and	O
our	O
aim	O
is	O
to	O
provide	O
a	O
uniﬁed	O
treatment	O
.	O
the	O
relationship	O
be-	O
tween	O
the	O
second-order	O
properties	O
of	O
the	O
sdes	O
on	O
the	O
real	O
line	O
and	O
the	O
circle	O
,	O
and	O
diﬀerence	O
equations	O
on	O
the	O
integer	O
lattice	O
and	O
the	O
regular	O
polygon	O
is	O
,	O
to	O
our	O
knowledge	O
,	O
novel	O
.	O
b.1	O
fourier	O
analysis	O
we	O
follow	O
the	O
treatment	O
given	O
by	O
kammler	O
[	O
2000	O
]	O
.	O
we	O
consider	O
fourier	O
analysis	O
of	O
functions	O
on	O
the	O
real	O
line	O
r	O
,	O
of	O
periodic	B
functions	O
of	O
period	O
l	O
on	O
the	O
circle	O
tl	O
,	O
of	O
functions	O
deﬁned	O
on	O
the	O
integer	O
lattice	O
z	O
,	O
and	O
of	O
functions	O
on	O
pn	O
,	O
the	O
regular	O
n-polygon	O
,	O
which	O
is	O
a	O
discretization	O
of	O
tl	O
.	O
for	O
suﬃciently	O
well-behaved	O
functions	O
on	O
r	O
we	O
have	O
f	O
(	O
x	O
)	O
=	O
˜f	O
(	O
s	O
)	O
e2πisx	O
ds	O
,	O
˜f	O
(	O
s	O
)	O
=	O
f	O
(	O
x	O
)	O
e−2πisx	O
dx	O
.	O
(	O
b.3	O
)	O
we	O
refer	O
to	O
the	O
equation	O
on	O
the	O
left	O
as	O
the	O
synthesis	O
equation	O
,	O
and	O
the	O
equation	O
on	O
the	O
right	O
as	O
the	O
analysis	O
equation	O
.	O
for	O
functions	O
on	O
tl	O
we	O
obtain	O
the	O
fourier	O
series	O
representations	O
f	O
(	O
x	O
)	O
e−2πikx/l	O
dx	O
,	O
˜f	O
[	O
k	O
]	O
e2πikx/l	O
,	O
˜f	O
[	O
k	O
]	O
=	O
f	O
(	O
x	O
)	O
=	O
∞x	O
(	O
b.4	O
)	O
1the	O
ak	O
coeﬃcients	O
in	O
equations	O
(	O
b.1	O
)	O
and	O
(	O
b.2	O
)	O
are	O
not	O
intended	O
to	O
have	O
a	O
close	O
relation-	O
ship	O
.	O
an	O
approximate	O
relationship	O
might	O
be	O
established	O
through	O
the	O
use	O
of	O
ﬁnite-diﬀerence	O
approximations	O
to	O
derivatives	O
.	O
z	O
∞	O
−∞	O
k=−∞	O
z	O
∞	O
−∞	O
z	O
l	O
0	O
1	O
l	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
b.1	O
fourier	O
analysis	O
209	O
z	O
l	O
0	O
n−1x	O
k=0	O
where	O
˜f	O
[	O
k	O
]	O
denotes	O
the	O
coeﬃcient	O
of	O
e2πikx/l	O
in	O
the	O
expansion	O
.	O
we	O
use	O
square	O
]	O
to	O
denote	O
that	O
the	O
argument	O
is	O
discrete	O
,	O
so	O
that	O
xt	O
and	O
x	O
[	O
t	O
]	O
are	O
brackets	O
[	O
equivalent	B
notations	O
.	O
similarly	O
for	O
z	O
we	O
obtain	O
f	O
[	O
n	O
]	O
=	O
˜f	O
(	O
s	O
)	O
e2πisn/l	O
ds	O
,	O
˜f	O
(	O
s	O
)	O
=	O
∞x	O
n=−∞	O
1	O
l	O
f	O
[	O
n	O
]	O
e−2πisn/l	O
.	O
(	O
b.5	O
)	O
note	O
that	O
˜f	O
(	O
s	O
)	O
is	O
periodic	B
with	O
period	O
l	O
and	O
so	O
is	O
deﬁned	O
only	O
for	O
0	O
≤	O
s	O
<	O
l	O
to	O
avoid	O
aliasing	O
.	O
often	O
this	O
transform	O
is	O
deﬁned	O
for	O
the	O
special	O
case	O
l	O
=	O
1	O
but	O
the	O
general	O
case	O
emphasizes	O
the	O
duality	O
between	O
equations	O
(	O
b.4	O
)	O
and	O
(	O
b.5	O
)	O
.	O
finally	O
,	O
for	O
functions	O
on	O
pn	O
we	O
have	O
the	O
discrete	O
fourier	O
transform	O
f	O
[	O
n	O
]	O
=	O
˜f	O
[	O
k	O
]	O
e2πikn/n	O
,	O
˜f	O
[	O
k	O
]	O
=	O
1	O
n	O
f	O
[	O
n	O
]	O
e−2πikn/n	O
.	O
(	O
b.6	O
)	O
n−1x	O
n=0	O
note	O
that	O
there	O
are	O
other	O
conventions	O
for	O
fourier	O
transforms	O
,	O
particularly	O
those	O
involving	O
ω	O
=	O
2πs	O
.	O
however	O
,	O
this	O
tends	O
to	O
destroy	O
symmetry	O
between	O
the	O
analysis	O
and	O
synthesis	O
equations	O
so	O
we	O
use	O
the	O
deﬁnitions	O
given	O
above	O
.	O
in	O
the	O
case	O
of	O
stochastic	O
processes	O
,	O
the	O
most	O
important	O
fourier	O
relationship	O
is	O
between	O
the	O
covariance	B
function	I
and	O
the	O
power	O
spectrum	O
;	O
this	O
is	O
known	O
as	O
the	O
wiener-khintchine	O
theorem	O
,	O
see	B
e.g	O
.	O
chatﬁeld	O
[	O
1989	O
]	O
.	O
b.1.1	O
sampling	O
and	O
periodization	O
we	O
can	O
obtain	O
relationships	O
between	O
functions	O
and	O
their	O
transforms	O
on	O
r	O
,	O
tl	O
,	O
z	O
,	O
pn	O
through	O
the	O
notions	O
of	O
sampling	O
and	O
periodization	O
.	O
deﬁnition	O
b.1	O
h-sampling	O
:	O
given	O
a	O
function	B
f	O
on	O
r	O
and	O
a	O
spacing	O
parameter	O
h	O
>	O
0	O
,	O
we	O
construct	O
a	O
corresponding	O
discrete	O
function	B
φ	O
on	O
z	O
using	O
(	O
b.7	O
)	O
(	O
cid:3	O
)	O
similarly	O
we	O
can	O
discretize	O
a	O
function	B
deﬁned	O
on	O
tl	O
onto	O
pn	O
,	O
but	O
in	O
this	O
case	O
we	O
must	O
take	O
h	O
=	O
l/n	O
so	O
that	O
n	O
steps	O
of	O
size	O
h	O
will	O
equal	O
the	O
period	O
l.	O
φ	O
[	O
n	O
]	O
=	O
f	O
(	O
nh	O
)	O
,	O
n	O
∈	O
z.	O
deﬁnition	O
b.2	O
periodization	O
by	O
summation	O
:	O
let	O
f	O
(	O
x	O
)	O
be	O
a	O
function	B
on	O
r	O
that	O
rapidly	O
approaches	O
0	O
as	O
x	O
→	O
±∞	O
.	O
we	O
can	O
sum	O
translates	O
of	O
the	O
function	B
to	O
produce	O
the	O
l-periodic	O
function	B
g	O
(	O
x	O
)	O
=	O
f	O
(	O
x	O
−	O
ml	O
)	O
,	O
(	O
b.8	O
)	O
for	O
l	O
>	O
0.	O
analogously	O
,	O
when	O
φ	O
is	O
deﬁned	O
on	O
z	O
and	O
φ	O
[	O
n	O
]	O
rapidly	O
approaches	O
0	O
as	O
n	O
→	O
±∞	O
we	O
can	O
construct	O
a	O
function	B
γ	O
on	O
pn	O
by	O
n-summation	O
by	O
setting	O
γ	O
[	O
n	O
]	O
=	O
φ	O
[	O
n	O
−	O
mn	O
]	O
.	O
(	O
b.9	O
)	O
(	O
cid:3	O
)	O
∞x	O
m=−∞	O
∞x	O
m=−∞	O
h-sampling	O
periodization	O
by	O
summation	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
210	O
gaussian	O
markov	O
processes	O
let	O
φ	O
[	O
n	O
]	O
be	O
obtained	O
by	O
h-sampling	O
from	O
f	O
(	O
x	O
)	O
,	O
with	O
corresponding	O
fourier	O
transforms	O
˜φ	O
(	O
s	O
)	O
and	O
˜f	O
(	O
s	O
)	O
.	O
then	O
we	O
have	O
z	O
∞	O
−∞	O
φ	O
[	O
n	O
]	O
=	O
f	O
(	O
nh	O
)	O
=	O
˜f	O
(	O
s	O
)	O
e2πisnh	O
ds	O
,	O
φ	O
[	O
n	O
]	O
=	O
˜φ	O
(	O
s	O
)	O
e2πisn/l	O
ds	O
.	O
by	O
breaking	O
up	O
the	O
domain	O
of	O
integration	O
in	O
eq	O
.	O
(	O
b.10	O
)	O
we	O
obtain	O
(	O
b.10	O
)	O
(	O
b.11	O
)	O
(	O
b.12	O
)	O
(	O
b.13	O
)	O
z	O
l	O
0	O
∞x	O
∞x	O
z	O
l	O
z	O
(	O
m+1	O
)	O
l	O
z	O
l	O
ml	O
(	O
cid:16	O
)	O
∞x	O
∞x	O
m=−∞	O
m=−∞	O
∞x	O
m=−∞	O
φ	O
[	O
n	O
]	O
=	O
=	O
m=−∞	O
m=−∞	O
0	O
˜f	O
(	O
s	O
)	O
e2πisnh	O
ds	O
˜f	O
(	O
s0	O
+	O
ml	O
)	O
e2πinh	O
(	O
s0+ml	O
)	O
ds0	O
,	O
using	O
the	O
change	O
of	O
variable	O
s0	O
=	O
s	O
−	O
ml	O
.	O
now	O
set	B
hl	O
=	O
1	O
and	O
use	O
e2πinm	O
=	O
1	O
for	O
n	O
,	O
m	O
integers	O
to	O
obtain	O
φ	O
[	O
n	O
]	O
=	O
˜f	O
(	O
s	O
+	O
ml	O
)	O
e2πisn/l	O
ds	O
,	O
(	O
b.14	O
)	O
(	O
cid:17	O
)	O
which	O
implies	O
that	O
0	O
˜φ	O
(	O
s	O
)	O
=	O
˜f	O
(	O
s	O
+	O
ml	O
)	O
,	O
(	O
b.15	O
)	O
p∞	O
with	O
l	O
=	O
1/h	O
.	O
alternatively	O
setting	O
l	O
=	O
1	O
one	O
obtains	O
˜φ	O
(	O
s	O
)	O
=	O
1	O
similarly	O
if	O
f	O
is	O
deﬁned	O
on	O
tl	O
and	O
φ	O
[	O
n	O
]	O
=	O
f	O
(	O
nl	O
m=−∞	O
˜f	O
(	O
s+m	O
h	O
)	O
.	O
n	O
)	O
is	O
obtained	O
by	O
sampling	O
then	O
h	O
˜φ	O
[	O
n	O
]	O
=	O
˜f	O
[	O
n	O
+	O
mn	O
]	O
.	O
(	O
b.16	O
)	O
thus	O
we	O
see	B
that	O
sampling	O
in	O
x-space	O
causes	O
periodization	O
in	O
fourier	O
space	O
.	O
periodic	B
function	O
g	O
(	O
x	O
)	O
,	O
p∞	O
now	O
consider	O
the	O
periodization	O
of	O
a	O
function	B
f	O
(	O
x	O
)	O
with	O
x	O
∈	O
r	O
to	O
give	O
the	O
l-	O
m=−∞	O
f	O
(	O
x−ml	O
)	O
.	O
let	O
˜g	O
[	O
k	O
]	O
be	O
the	O
fourier	O
coeﬃcients	O
of	O
g	O
(	O
x	O
)	O
.	O
we	O
obtain	O
z	O
l	O
z	O
∞	O
0	O
−∞	O
˜g	O
[	O
k	O
]	O
=	O
=	O
1	O
l	O
1	O
l	O
g	O
(	O
x	O
)	O
e−2πikx/l	O
dx	O
=	O
1	O
l	O
f	O
(	O
x	O
)	O
e−2πikx/l	O
dx	O
=	O
z	O
l	O
∞x	O
(	O
cid:16	O
)	O
k	O
(	O
cid:17	O
)	O
0	O
m=−∞	O
˜f	O
1	O
l	O
,	O
l	O
f	O
(	O
x	O
−	O
ml	O
)	O
e−2πikx/l	O
dx	O
(	O
b.17	O
)	O
(	O
b.18	O
)	O
assuming	O
that	O
f	O
(	O
x	O
)	O
is	O
suﬃciently	O
well-behaved	O
that	O
the	O
summation	O
and	O
inte-	O
gration	O
operations	O
can	O
be	O
exchanged	O
.	O
a	O
similar	O
relationship	O
can	O
be	O
obtained	O
for	O
the	O
periodization	O
of	O
a	O
function	B
deﬁned	O
on	O
z.	O
thus	O
we	O
see	B
that	O
periodization	O
in	O
x-space	O
gives	O
rise	O
to	O
sampling	O
in	O
fourier	O
space	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
b.2	O
continuous-time	O
gaussian	O
markov	O
processes	O
211	O
b.2	O
continuous-time	O
gaussian	O
markov	O
processes	O
we	O
ﬁrst	O
consider	O
continuous-time	O
gaussian	O
markov	O
processes	O
on	O
the	O
real	O
line	O
,	O
and	O
then	O
relate	O
the	O
covariance	B
function	I
obtained	O
to	O
that	O
for	O
the	O
stationary	O
solution	O
of	O
the	O
sde	O
on	O
the	O
circle	O
.	O
our	O
treatment	O
of	O
continuous-time	O
gmps	O
on	O
r	O
follows	O
papoulis	O
[	O
1991	O
,	O
ch	O
.	O
10	O
]	O
.	O
b.2.1	O
continuous-time	O
gmps	O
on	O
r	O
we	O
wish	O
to	O
ﬁnd	O
the	O
power	O
spectrum	O
and	O
covariance	B
function	I
for	O
the	O
stationary	O
process	B
corresponding	O
to	O
the	O
sde	O
given	O
by	O
eq	O
.	O
(	O
b.2	O
)	O
.	O
recall	O
that	O
the	O
covariance	B
function	I
of	O
a	O
stationary	O
process	B
k	O
(	O
t	O
)	O
and	O
the	O
power	O
spectrum	O
s	O
(	O
s	O
)	O
form	O
a	O
fourier	O
transform	O
pair	O
.	O
the	O
fourier	O
transform	O
of	O
the	O
stochastic	O
process	O
x	O
(	O
t	O
)	O
is	O
a	O
stochastic	O
process	O
z	O
∞	O
−∞	O
˜x	O
(	O
s	O
)	O
given	O
by	O
z	O
∞	O
−∞	O
˜x	O
(	O
s	O
)	O
=	O
x	O
(	O
t	O
)	O
e−2πist	O
dt	O
,	O
x	O
(	O
t	O
)	O
=	O
˜x	O
(	O
s	O
)	O
e2πist	O
ds	O
,	O
(	O
b.19	O
)	O
where	O
the	O
integrals	B
are	O
interpreted	O
as	O
a	O
mean-square	O
limit	O
.	O
let	O
∗	O
denote	O
complex	O
conjugation	O
and	O
h.	O
.	O
.i	O
denote	O
expectation	O
with	O
respect	O
to	O
the	O
stochastic	O
process	O
.	O
then	O
for	O
a	O
stationary	O
gaussian	O
process	B
we	O
have	O
h	O
˜x	O
(	O
s1	O
)	O
˜x∗	O
(	O
s2	O
)	O
i	O
=	O
hx	O
(	O
t	O
)	O
x∗	O
(	O
t0	O
)	O
ie−2πis1te2πis2t0	O
dt	O
dt0	O
z	O
∞	O
z	O
∞	O
−∞	O
z	O
∞	O
dt0e−2πi	O
(	O
s1−s2	O
)	O
t0z	O
∞	O
−∞	O
=	O
=	O
s	O
(	O
s1	O
)	O
δ	O
(	O
s1	O
−	O
s2	O
)	O
,	O
−∞	O
dτ	O
k	O
(	O
τ	O
)	O
e−2πis1τ	O
−∞	O
(	O
b.20	O
)	O
(	O
b.21	O
)	O
the	O
delta	O
function	B
r	O
e−2πistdt	O
=	O
δ	O
(	O
s	O
)	O
.	O
this	O
shows	O
that	O
˜x	O
(	O
s1	O
)	O
and	O
˜x	O
(	O
s2	O
)	O
are	O
(	O
b.22	O
)	O
using	O
the	O
change	O
of	O
variables	O
τ	O
=	O
t	O
−	O
t0	O
and	O
the	O
integral	O
representation	O
of	O
uncorrelated	O
for	O
s1	O
6=	O
s2	O
,	O
i.e	O
.	O
that	O
the	O
fourier	O
basis	O
are	O
eigenfunctions	O
of	O
the	O
diﬀerential	O
operator	B
.	O
also	O
from	O
eq	O
.	O
(	O
b.19	O
)	O
we	O
obtain	O
x	O
(	O
k	O
)	O
(	O
t	O
)	O
=	O
(	O
2πis	O
)	O
k	O
˜x	O
(	O
s	O
)	O
e2πist	O
ds	O
.	O
(	O
b.23	O
)	O
now	O
if	O
we	O
fourier	O
transform	O
eq	O
.	O
(	O
b.2	O
)	O
we	O
obtain	O
ak	O
(	O
2πis	O
)	O
k	O
˜x	O
(	O
s	O
)	O
=	O
b0	O
˜z	O
(	O
s	O
)	O
,	O
(	O
b.24	O
)	O
z	O
∞	O
−∞	O
px	O
k=0	O
where	O
˜z	O
(	O
s	O
)	O
denotes	O
the	O
fourier	O
transform	O
of	O
the	O
white	O
noise	O
.	O
taking	O
the	O
product	O
of	O
equation	O
b.24	O
with	O
its	O
complex	O
conjugate	O
and	O
taking	O
expectations	O
we	O
obtain	O
(	O
cid:20	O
)	O
px	O
(	O
cid:21	O
)	O
(	O
cid:20	O
)	O
px	O
(	O
cid:21	O
)	O
ak	O
(	O
2πis1	O
)	O
k	O
ak	O
(	O
−2πis2	O
)	O
k	O
h	O
˜x	O
(	O
s1	O
)	O
˜x∗	O
(	O
s2	O
)	O
i	O
=	O
b2	O
0h	O
˜z	O
(	O
s1	O
)	O
˜z∗	O
(	O
s2	O
)	O
i.	O
k=0	O
k=0	O
(	O
b.25	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
212	O
ar	O
(	O
1	O
)	O
process	B
ar	O
(	O
p	O
)	O
process	B
let	O
a	O
(	O
z	O
)	O
=	O
pp	O
spectrum	O
of	O
white	O
noise	O
is	O
1	O
,	O
we	O
obtain	O
k=0	O
akzk	O
.	O
then	O
using	O
eq	O
.	O
(	O
b.22	O
)	O
and	O
the	O
fact	O
that	O
the	O
power	O
gaussian	O
markov	O
processes	O
sr	O
(	O
s	O
)	O
=	O
b2	O
0	O
|a	O
(	O
2πis	O
)	O
|2	O
.	O
(	O
b.26	O
)	O
note	O
that	O
the	O
denominator	O
is	O
a	O
polynomial	B
of	O
order	O
p	O
in	O
s2	O
.	O
the	O
relationship	O
of	O
stationary	O
solutions	O
of	O
pth-order	O
sdes	O
to	O
rational	O
spectral	O
densities	O
can	O
be	O
traced	O
back	O
at	O
least	O
as	O
far	O
as	O
doob	O
[	O
1944	O
]	O
.	O
above	O
we	O
have	O
assumed	O
that	O
the	O
process	B
is	O
stationary	O
.	O
however	O
,	O
this	O
de-	O
pends	O
on	O
the	O
coeﬃcients	O
a0	O
,	O
.	O
.	O
.	O
,	O
ap	O
.	O
to	O
analyze	O
this	O
issue	O
we	O
assume	O
a	O
solution	O
of	O
the	O
form	O
xt	O
∝	O
eλt	O
when	O
the	O
driving	O
term	O
b0	O
=	O
0.	O
this	O
leads	O
to	O
the	O
condition	O
k=0	O
akλk	O
must	O
lie	O
in	O
the	O
left	O
for	O
stationarity	B
that	O
the	O
roots	O
of	O
the	O
polynomialpp	O
half	O
plane	O
[	O
arat´o	O
,	O
1982	O
,	O
p.	O
127	O
]	O
.	O
example	O
:	O
ar	O
(	O
1	O
)	O
process	B
.	O
in	O
this	O
case	O
we	O
have	O
the	O
sde	O
x0	O
(	O
t	O
)	O
+	O
a0x	O
(	O
t	O
)	O
=	O
b0z	O
(	O
t	O
)	O
,	O
(	O
b.27	O
)	O
where	O
a0	O
>	O
0	O
for	O
stationarity	B
.	O
this	O
gives	O
rise	O
to	O
the	O
power	O
spectrum	O
s	O
(	O
s	O
)	O
=	O
b2	O
0	O
(	O
2πis	O
+	O
a0	O
)	O
(	O
−2πis	O
+	O
a0	O
)	O
=	O
b2	O
0	O
(	O
2πs	O
)	O
2	O
+	O
a2	O
0	O
.	O
(	O
b.28	O
)	O
taking	O
the	O
fourier	O
transform	O
we	O
obtain	O
k	O
(	O
t	O
)	O
=	O
b2	O
0	O
2a0	O
e−a0|t|	O
.	O
(	O
b.29	O
)	O
to	O
the	O
power	O
spectrum	O
s	O
(	O
s	O
)	O
=	O
(	O
[	O
pp	O
this	O
process	B
is	O
known	O
as	O
the	O
ornstein-uhlenbeck	O
(	O
ou	O
)	O
process	B
[	O
uhlenbeck	O
and	O
ornstein	O
,	O
1930	O
]	O
and	O
was	O
introduced	O
as	O
a	O
mathematical	O
model	B
of	O
the	O
velocity	O
of	O
a	O
particle	O
undergoing	O
brownian	O
motion	O
.	O
it	O
can	O
be	O
shown	O
that	O
the	O
ou	O
process	B
is	O
the	O
unique	O
stationary	O
ﬁrst-order	O
gaussian	O
markov	O
process	B
.	O
example	O
:	O
ar	O
(	O
p	O
)	O
process	B
.	O
in	O
general	O
the	O
covariance	B
transform	O
corresponding	O
k=0	O
ak	O
(	O
−2πis	O
)	O
k	O
]	O
)	O
−1	O
can	O
be	O
quite	O
complicated	O
.	O
for	O
example	O
,	O
papoulis	O
[	O
1991	O
,	O
p.	O
326	O
]	O
gives	O
three	O
forms	O
of	O
1	O
−	O
4a0	O
is	O
the	O
covariance	B
function	I
for	O
the	O
ar	O
(	O
2	O
)	O
process	B
depending	O
on	O
whether	O
a2	O
greater	O
than	O
,	O
equal	O
to	O
or	O
less	O
than	O
0.	O
however	O
,	O
if	O
the	O
coeﬃcients	O
a0	O
,	O
a1	O
,	O
.	O
.	O
.	O
,	O
ap	O
are	O
chosen	O
in	O
a	O
particular	O
way	O
then	O
one	O
can	O
obtain	O
k=0	O
ak	O
(	O
2πis	O
)	O
k	O
]	O
[	O
pp	O
s	O
(	O
s	O
)	O
=	O
1	O
(	O
4π2s2	O
+	O
α2	O
)	O
p	O
(	O
b.30	O
)	O
ance	O
function	B
is	O
of	O
the	O
formpp−1	O
for	O
some	O
α.	O
it	O
can	O
be	O
shown	O
[	O
stein	O
,	O
1999	O
,	O
p.	O
31	O
]	O
that	O
the	O
corresponding	O
covari-	O
k=0	O
βk|t|ke−α|t|	O
for	O
some	O
coeﬃcients	O
β0	O
,	O
.	O
.	O
.	O
,	O
βp−1	O
.	O
2α	O
e−α|t|	O
for	O
the	O
ou	O
process	B
.	O
for	O
4α3	O
e−α|t|	O
(	O
1+α|t|	O
)	O
.	O
these	O
are	O
special	O
cases	O
of	O
the	O
mat´ern	O
for	O
p	O
=	O
1	O
we	O
have	O
already	O
seen	O
that	O
k	O
(	O
t	O
)	O
=	O
1	O
p	O
=	O
2	O
we	O
obtain	O
k	O
(	O
t	O
)	O
=	O
1	O
class	O
of	O
covariance	B
functions	O
described	O
in	O
section	O
4.2.1.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
213	O
wiener	O
process	B
b.2	O
continuous-time	O
gaussian	O
markov	O
processes	O
example	O
:	O
wiener	O
process	B
.	O
although	O
our	O
derivations	O
have	O
focussed	O
on	O
stationary	O
gaussian	O
markov	O
processes	O
,	O
there	O
are	O
also	O
several	O
important	O
non-stationary	O
processes	O
.	O
one	O
of	O
the	O
most	O
important	O
is	O
the	O
wiener	O
process	B
that	O
satisﬁes	O
the	O
sde	O
x0	O
(	O
t	O
)	O
=	O
z	O
(	O
t	O
)	O
for	O
t	O
≥	O
0	O
with	O
the	O
initial	O
condition	O
x	O
(	O
0	O
)	O
=	O
0.	O
this	O
process	B
has	O
covariance	B
function	I
k	O
(	O
t	O
,	O
s	O
)	O
=	O
min	O
(	O
t	O
,	O
s	O
)	O
.	O
an	O
interesting	O
variant	O
of	O
the	O
wiener	O
process	B
known	O
as	O
the	O
brownian	O
bridge	O
(	O
or	O
tied-down	B
wiener	O
process	B
)	O
is	O
obtained	O
by	O
conditioning	O
on	O
the	O
wiener	O
process	B
passing	O
through	O
x	O
(	O
1	O
)	O
=	O
0.	O
this	O
has	O
covariance	B
k	O
(	O
t	O
,	O
s	O
)	O
=	O
min	O
(	O
t	O
,	O
s	O
)	O
−	O
st	O
for	O
0	O
≤	O
s	O
,	O
t	O
≤	O
1.	O
see	B
e.g	O
.	O
grimmett	O
and	O
stirzaker	O
[	O
1992	O
]	O
for	O
further	O
information	O
on	O
these	O
processes	O
.	O
markov	O
processes	O
derived	O
from	O
sdes	O
of	O
order	O
p	O
are	O
p−	O
1	O
times	O
ms	O
diﬀeren-	O
tiable	O
.	O
this	O
is	O
easy	O
to	O
see	B
heuristically	O
from	O
eq	O
.	O
(	O
b.2	O
)	O
;	O
given	O
that	O
a	O
process	B
gets	O
rougher	O
the	O
more	O
times	O
it	O
is	O
diﬀerentiated	O
,	O
eq	O
.	O
(	O
b.2	O
)	O
tells	O
us	O
that	O
x	O
(	O
p	O
)	O
(	O
t	O
)	O
is	O
like	O
the	O
white	O
noise	O
process	O
,	O
i.e	O
.	O
not	O
ms	O
continuous	O
.	O
so	O
,	O
for	O
example	O
,	O
the	O
ou	O
process	B
(	O
and	O
also	O
the	O
wiener	O
process	B
)	O
are	O
ms	O
continuous	O
but	O
not	O
ms	O
diﬀerentiable	O
.	O
b.2.2	O
the	O
solution	O
of	O
the	O
corresponding	O
sde	O
on	O
the	O
cir-	O
cle	O
the	O
analogous	O
analysis	O
to	O
that	O
on	O
the	O
real	O
line	O
is	O
carried	O
out	O
on	O
tl	O
using	O
x	O
(	O
t	O
)	O
=	O
˜x	O
[	O
n	O
]	O
e2πint/l	O
,	O
˜x	O
[	O
n	O
]	O
=	O
1	O
l	O
x	O
(	O
t	O
)	O
e−2πint/ldt	O
.	O
(	O
b.31	O
)	O
∞x	O
n=−∞	O
z	O
l	O
0	O
as	O
x	O
(	O
t	O
)	O
is	O
assumed	O
stationary	O
we	O
obtain	O
an	O
analogous	O
result	O
to	O
eq	O
.	O
(	O
b.22	O
)	O
,	O
i.e	O
.	O
that	O
the	O
fourier	O
coeﬃcients	O
are	O
independent	O
(	O
cid:26	O
)	O
s	O
[	O
n	O
]	O
0	O
px	O
h	O
˜x	O
[	O
m	O
]	O
˜x∗	O
[	O
n	O
]	O
i	O
=	O
if	O
m	O
=	O
n	O
otherwise	O
.	O
(	O
b.32	O
)	O
p∞	O
similarly	O
,	O
the	O
covariance	B
function	I
on	O
the	O
cirle	O
is	O
given	O
by	O
k	O
(	O
t−s	O
)	O
=	O
hx	O
(	O
t	O
)	O
x∗	O
(	O
s	O
)	O
i	O
=	O
x	O
(	O
k	O
)	O
(	O
t	O
)	O
=p∞	O
n=−∞	O
s	O
[	O
n	O
]	O
e2πin	O
(	O
t−s	O
)	O
/l	O
.	O
let	O
ωl	O
=	O
2π/l	O
.	O
then	O
plugging	O
in	O
the	O
expression	O
n=−∞	O
(	O
inωl	O
)	O
k	O
˜x	O
[	O
n	O
]	O
einωlt	O
into	O
the	O
sde	O
eq	O
.	O
(	O
b.2	O
)	O
and	O
equating	O
terms	O
in	O
[	O
n	O
]	O
we	O
obtain	O
ak	O
(	O
inωl	O
)	O
k	O
˜x	O
[	O
n	O
]	O
=	O
b0	O
˜z	O
[	O
n	O
]	O
.	O
(	O
b.33	O
)	O
k=0	O
as	O
in	O
the	O
real-line	O
case	O
we	O
form	O
the	O
product	O
of	O
equation	O
b.33	O
with	O
its	O
complex	O
conjugate	O
and	O
take	O
expectations	O
to	O
give	O
b2	O
0	O
note	O
that	O
st	O
[	O
n	O
]	O
is	O
equal	O
to	O
sr	O
(	O
cid:0	O
)	O
n	O
st	O
[	O
n	O
]	O
=	O
|a	O
(	O
inωl	O
)	O
|2	O
.	O
(	O
cid:1	O
)	O
,	O
i.e	O
.	O
that	O
it	O
is	O
a	O
sampling	O
of	O
sr	O
at	O
intervals	O
(	O
b.34	O
)	O
1/l	O
,	O
where	O
sr	O
(	O
s	O
)	O
is	O
the	O
power	O
spectrum	O
of	O
the	O
continuous	O
process	B
on	O
the	O
real	O
line	O
given	O
in	O
equation	O
b.26	O
.	O
let	O
kt	O
(	O
h	O
)	O
denote	O
the	O
covariance	B
function	I
on	O
the	O
l	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
214	O
gaussian	O
markov	O
processes	O
circle	O
and	O
kr	O
(	O
h	O
)	O
denote	O
the	O
covariance	B
function	I
on	O
the	O
real	O
line	O
for	O
the	O
sde	O
.	O
then	O
using	O
eq	O
.	O
(	O
b.15	O
)	O
we	O
ﬁnd	O
that	O
∞x	O
m=−∞	O
kt	O
(	O
t	O
)	O
=	O
kr	O
(	O
t	O
−	O
ml	O
)	O
.	O
1st	O
order	O
sde	O
example	O
:	O
1st-order	O
sde	O
.	O
on	O
r	O
for	O
the	O
ou	O
process	B
we	O
have	O
kr	O
(	O
t	O
)	O
=	O
b2	O
by	O
summing	O
the	O
series	O
(	O
two	O
geometric	O
progressions	O
)	O
we	O
obtain	O
0	O
2a0	O
kt	O
(	O
t	O
)	O
=	O
b2	O
0	O
2a0	O
(	O
1	O
−	O
e−a0l	O
)	O
(	O
cid:16	O
)	O
e−a0|t|	O
+	O
e−a0	O
(	O
l−|t|	O
)	O
(	O
cid:17	O
)	O
=	O
b2	O
0	O
2a0	O
cosh	O
[	O
a0	O
(	O
l	O
2	O
−	O
|t|	O
)	O
]	O
sinh	O
(	O
a0l	O
2	O
)	O
(	O
b.36	O
)	O
(	O
b.35	O
)	O
e−a0|t|	O
.	O
for	O
−l	O
≤	O
t	O
≤	O
l.	O
eq	O
.	O
(	O
b.36	O
)	O
is	O
also	O
given	O
(	O
up	O
to	O
scaling	O
factors	O
)	O
in	O
grenander	O
et	O
al	O
.	O
[	O
1991	O
,	O
eq	O
.	O
2.15	O
]	O
,	O
where	O
it	O
is	O
obtained	O
by	O
a	O
limiting	O
argument	O
from	O
the	O
discrete-time	O
gmp	O
on	O
pn	O
,	O
see	B
section	O
b.3.2	O
.	O
b.3	O
discrete-time	O
gaussian	O
markov	O
processes	O
we	O
ﬁrst	O
consider	O
discrete-time	O
gaussian	O
markov	O
processes	O
on	O
z	O
,	O
and	O
then	O
re-	O
late	O
the	O
covariance	B
function	I
obtained	O
to	O
that	O
of	O
the	O
stationary	O
solution	O
of	O
the	O
diﬀerence	O
equation	O
on	O
pn	O
.	O
chatﬁeld	O
[	O
1989	O
]	O
and	O
diggle	O
[	O
1990	O
]	O
provide	O
good	O
coverage	O
of	O
discrete-time	O
arma	O
models	O
on	O
z.	O
b.3.1	O
discrete-time	O
gmps	O
on	O
z	O
assuming	O
that	O
the	O
process	B
is	O
stationary	O
the	O
covariance	B
function	I
k	O
[	O
i	O
]	O
denotes	O
hxtxt+ii	O
∀t	O
∈	O
z	O
.	O
(	O
note	O
that	O
because	O
of	O
stationarity	B
k	O
[	O
i	O
]	O
=	O
k	O
[	O
−i	O
]	O
.	O
)	O
we	O
ﬁrst	O
use	O
a	O
fourier	O
approach	O
to	O
derive	O
the	O
power	O
spectrum	O
and	O
hence	O
the	O
covariance	B
function	I
of	O
the	O
ar	O
(	O
p	O
)	O
process	B
.	O
deﬁning	O
a0	O
=	O
−1	O
,	O
we	O
can	O
rewrite	O
k=0	O
akxt−k	O
+	O
b0zt	O
=	O
0.	O
the	O
fourier	O
pair	O
for	O
x	O
[	O
t	O
]	O
is	O
∞x	O
t=−∞	O
˜x	O
(	O
s	O
)	O
e2πist/l	O
ds	O
,	O
˜x	O
(	O
s	O
)	O
=	O
1	O
l	O
x	O
[	O
t	O
]	O
e−2πist/l	O
.	O
(	O
b.37	O
)	O
k=0	O
akxt−k	O
+	O
b0zt	O
=	O
0	O
we	O
obtain	O
(	O
cid:16	O
)	O
px	O
ake−iωlsk	O
(	O
cid:17	O
)	O
˜x	O
(	O
s	O
)	O
+	O
b0	O
˜z	O
(	O
s	O
)	O
=	O
0	O
,	O
(	O
b.38	O
)	O
eq	O
.	O
(	O
b.1	O
)	O
aspp	O
z	O
l	O
x	O
[	O
t	O
]	O
=	O
plugging	O
this	O
intopp	O
0	O
k=0	O
where	O
ωl	O
=	O
2π/l	O
.	O
as	O
above	O
,	O
taking	O
the	O
product	O
of	O
eq	O
.	O
(	O
b.38	O
)	O
with	O
its	O
complex	O
conjugate	O
and	O
taking	O
expectations	O
we	O
obtain	O
sz	O
(	O
s	O
)	O
=	O
b2	O
0	O
|a	O
(	O
eiωls	O
)	O
|2	O
.	O
(	O
b.39	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
b.3	O
discrete-time	O
gaussian	O
markov	O
processes	O
215	O
above	O
we	O
have	O
assumed	O
that	O
the	O
process	B
is	O
stationary	O
.	O
however	O
,	O
this	O
de-	O
pends	O
on	O
the	O
coeﬃcients	O
a0	O
,	O
.	O
.	O
.	O
,	O
ap	O
.	O
to	O
analyze	O
this	O
issue	O
we	O
assume	O
a	O
solution	O
of	O
the	O
form	O
xt	O
∝	O
zt	O
when	O
the	O
driving	O
term	O
b0	O
=	O
0.	O
this	O
leads	O
to	O
the	O
condition	O
k=0	O
akzp−k	O
must	O
lie	O
inside	O
for	O
stationarity	B
that	O
the	O
roots	O
of	O
the	O
polynomial	B
pp	O
the	O
unit	O
circle	O
.	O
see	B
hannan	O
[	O
1970	O
,	O
theorem	O
5	O
,	O
p.	O
19	O
]	O
for	O
further	O
details	O
.	O
as	O
well	O
as	O
deriving	O
the	O
covariance	B
function	I
from	O
the	O
fourier	O
transform	O
of	O
the	O
power	O
spectrum	O
it	O
can	O
also	O
be	O
obtained	O
by	O
solving	O
a	O
set	B
of	O
linear	B
equations	O
.	O
our	O
ﬁrst	O
observation	O
is	O
that	O
xs	O
is	O
independent	O
of	O
zt	O
for	O
s	O
<	O
t.	O
multiplying	O
equation	O
b.1	O
through	O
by	O
zt	O
and	O
taking	O
expectations	O
,	O
we	O
obtain	O
hxtzti	O
=	O
b0	O
and	O
hxt−izti	O
=	O
0	O
for	O
i	O
>	O
0.	O
by	O
multiplying	O
equation	O
b.1	O
through	O
by	O
xt−j	O
for	O
j	O
=	O
0	O
,	O
1	O
,	O
.	O
.	O
.	O
and	O
taking	O
expectations	O
we	O
obtain	O
the	O
yule-walker	O
equations	O
yule-walker	O
equations	O
ar	O
(	O
1	O
)	O
process	B
k	O
[	O
0	O
]	O
=	O
k	O
[	O
j	O
]	O
=	O
aik	O
[	O
i	O
]	O
+	O
b2	O
0	O
aik	O
[	O
j	O
−	O
i	O
]	O
∀j	O
>	O
0	O
.	O
(	O
b.40	O
)	O
(	O
b.41	O
)	O
the	O
ﬁrst	O
p	O
+	O
1	O
of	O
these	O
equations	O
form	O
a	O
linear	B
system	O
that	O
can	O
be	O
used	O
to	O
solve	O
for	O
k	O
[	O
0	O
]	O
,	O
.	O
.	O
.	O
,	O
k	O
[	O
p	O
]	O
in	O
terms	O
of	O
b0	O
and	O
a1	O
,	O
.	O
.	O
.	O
,	O
ap	O
,	O
and	O
eq	O
.	O
(	O
b.41	O
)	O
can	O
be	O
used	O
to	O
obtain	O
k	O
[	O
j	O
]	O
for	O
j	O
>	O
p	O
recursively	O
.	O
example	O
:	O
ar	O
(	O
1	O
)	O
process	B
.	O
the	O
simplest	O
example	O
of	O
an	O
ar	O
process	B
is	O
the	O
ar	O
(	O
1	O
)	O
process	B
deﬁned	O
as	O
xt	O
=	O
a1xt−1	O
+	O
b0zt	O
.	O
this	O
gives	O
rise	O
to	O
the	O
yule-walker	O
equations	O
k	O
[	O
0	O
]	O
−	O
a1k	O
[	O
1	O
]	O
=	O
b2	O
0	O
,	O
and	O
k	O
[	O
1	O
]	O
−	O
a1k	O
[	O
0	O
]	O
=	O
0	O
.	O
(	O
b.42	O
)	O
0/	O
(	O
1	O
−	O
a2	O
x	O
,	O
where	O
the	O
linear	B
system	O
for	O
k	O
[	O
0	O
]	O
,	O
k	O
[	O
1	O
]	O
can	O
easily	O
be	O
solved	O
to	O
give	O
k	O
[	O
j	O
]	O
=	O
a	O
1	O
)	O
is	O
the	O
variance	O
of	O
the	O
process	B
.	O
notice	O
that	O
for	O
the	O
process	B
to	O
x	O
=	O
b2	O
σ2	O
be	O
stationary	O
we	O
require	O
|a1|	O
<	O
1.	O
the	O
corresponding	O
power	O
spectrum	O
obtained	O
from	O
equation	O
b.39	O
is	O
|j|	O
1	O
σ2	O
s	O
(	O
s	O
)	O
=	O
b2	O
0	O
1	O
−	O
2a1	O
cos	O
(	O
ωls	O
)	O
+	O
a2	O
1	O
.	O
(	O
b.43	O
)	O
similarly	O
to	O
the	O
continuous	O
case	O
,	O
the	O
covariance	B
function	I
for	O
the	O
discrete-time	O
ar	O
(	O
2	O
)	O
process	B
has	O
three	O
diﬀerent	O
forms	O
depending	O
on	O
a2	O
1	O
+	O
4a2	O
.	O
these	O
are	O
described	O
in	O
diggle	O
[	O
1990	O
,	O
example	O
3.6	O
]	O
.	O
b.3.2	O
the	O
solution	O
of	O
the	O
corresponding	O
diﬀerence	O
equa-	O
tion	O
on	O
pn	O
we	O
now	O
consider	O
variables	O
x	O
=	O
x0	O
,	O
x1	O
,	O
.	O
.	O
.	O
,	O
xn−1	O
arranged	O
around	O
the	O
circle	O
with	O
n	O
≥	O
p.	O
by	O
appropriately	O
modifying	O
eq	O
.	O
(	O
b.1	O
)	O
we	O
obtain	O
xt	O
=	O
akxmod	O
(	O
t−k	O
,	O
n	O
)	O
+	O
b0zt	O
.	O
(	O
b.44	O
)	O
px	O
px	O
i=1	O
i=1	O
px	O
k=1	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
216	O
gaussian	O
markov	O
processes	O
pn−1	O
the	O
zt	O
’	O
s	O
are	O
i.i.d	O
.	O
and	O
∼	O
n	O
(	O
0	O
,	O
1	O
)	O
.	O
thus	O
z	O
=	O
z0	O
,	O
z1	O
,	O
.	O
.	O
.	O
,	O
zn−1	O
has	O
density	O
p	O
(	O
z	O
)	O
∝	O
exp−	O
1	O
t	O
.	O
equation	O
(	O
b.44	O
)	O
shows	O
that	O
x	O
and	O
z	O
are	O
related	O
by	O
a	O
linear	B
transformation	O
and	O
thus	O
t=0	O
z	O
2	O
2	O
p	O
(	O
x	O
)	O
∝	O
exp	O
(	O
cid:16	O
)	O
−	O
1	O
2b2	O
0	O
n−1x	O
(	O
cid:16	O
)	O
xt	O
−	O
px	O
t=0	O
k=1	O
(	O
cid:17	O
)	O
2	O
(	O
cid:17	O
)	O
akxmod	O
(	O
t−k	O
,	O
n	O
)	O
.	O
(	O
b.45	O
)	O
this	O
is	O
an	O
n-dimensional	O
multivariate	O
gaussian	O
.	O
for	O
an	O
ar	O
(	O
p	O
)	O
process	B
the	O
inverse	O
covariance	B
matrix	I
has	O
a	O
circulant	O
structure	O
[	O
davis	O
,	O
1979	O
]	O
consisting	O
of	O
a	O
diagonal	O
band	O
(	O
2p	O
+	O
1	O
)	O
entries	O
wide	O
and	O
appropriate	O
circulant	O
entries	O
in	O
the	O
corners	O
.	O
thus	O
p	O
(	O
xt|x\	O
xt	O
)	O
=	O
p	O
(	O
xt|xmod	O
(	O
t−1	O
,	O
n	O
)	O
,	O
.	O
.	O
.	O
,	O
xmod	O
(	O
t−p	O
,	O
n	O
)	O
,	O
xmod	O
(	O
t+1	O
,	O
n	O
)	O
,	O
.	O
.	O
.	O
,	O
xmod	O
(	O
t+p	O
,	O
n	O
)	O
)	O
,	O
which	O
geman	O
and	O
geman	O
[	O
1984	O
]	O
call	O
the	O
“	O
two-sided	O
”	O
markov	O
property	O
.	O
notice	O
that	O
it	O
is	O
the	O
zeros	O
in	O
the	O
inverse	O
covariance	B
matrix	I
that	O
indicate	O
the	O
conditional	B
independence	O
structure	O
;	O
see	B
also	O
section	O
b.5	O
.	O
the	O
properties	O
of	O
eq	O
.	O
(	O
b.44	O
)	O
have	O
been	O
studied	O
by	O
a	O
number	O
of	O
authors	O
,	O
e.g	O
.	O
whittle	O
[	O
1963	O
]	O
(	O
under	O
the	O
name	O
of	O
circulant	O
processes	O
)	O
,	O
kashyap	O
and	O
chel-	O
lappa	O
[	O
1981	O
]	O
(	O
under	O
the	O
name	O
of	O
circular	O
autoregressive	O
models	O
)	O
and	O
grenander	O
et	O
al	O
.	O
[	O
1991	O
]	O
(	O
as	O
cyclic	O
markov	O
process	B
)	O
.	O
as	O
above	O
,	O
we	O
deﬁne	O
the	O
fourier	O
transform	O
pair	O
x	O
[	O
n	O
]	O
=	O
˜x	O
[	O
m	O
]	O
e2πinm/n	O
,	O
˜x	O
[	O
m	O
]	O
=	O
1	O
n	O
x	O
[	O
n	O
]	O
e−2πinm/n	O
.	O
(	O
b.46	O
)	O
n−1x	O
n=0	O
n−1x	O
m=0	O
px	O
by	O
similar	O
arguments	O
to	O
those	O
above	O
we	O
obtain	O
ak	O
˜x	O
[	O
m	O
]	O
(	O
e2πim/n	O
)	O
k	O
+	O
b0	O
˜z	O
[	O
m	O
]	O
=	O
0	O
,	O
(	O
b.47	O
)	O
where	O
a0	O
=	O
−1	O
,	O
and	O
thus	O
k=0	O
sp	O
[	O
m	O
]	O
=	O
b2	O
0	O
|a	O
(	O
e2πim/n	O
)	O
|2	O
.	O
(	O
b.48	O
)	O
as	O
in	O
the	O
continuous-time	O
case	O
,	O
we	O
see	B
that	O
sp	O
[	O
m	O
]	O
is	O
obtained	O
by	O
sampling	O
the	O
power	O
spectrum	O
of	O
the	O
corresponding	O
process	B
on	O
the	O
line	O
,	O
so	O
that	O
sp	O
[	O
m	O
]	O
=	O
sz	O
(	O
cid:0	O
)	O
ml	O
n	O
(	O
cid:1	O
)	O
.	O
thus	O
using	O
eq	O
.	O
(	O
b.16	O
)	O
we	O
have	O
∞x	O
kp	O
[	O
n	O
]	O
=	O
m=−∞	O
kz	O
[	O
n	O
+	O
mn	O
]	O
.	O
(	O
b.49	O
)	O
ar	O
(	O
1	O
)	O
process	B
example	O
:	O
ar	O
(	O
1	O
)	O
process	B
.	O
for	O
this	O
process	B
xt	O
=	O
a1xmod	O
(	O
t−1	O
,	O
n	O
)	O
+	O
b0zt	O
,	O
the	O
diagonal	O
entries	O
in	O
the	O
inverse	O
covariance	B
are	O
(	O
1	O
+	O
a2	O
0	O
and	O
the	O
non-zero	O
oﬀ-	O
diagonal	O
entries	O
are	O
−a1/b2	O
0	O
.	O
1	O
)	O
/b2	O
by	O
summing	O
the	O
covariance	B
function	I
kz	O
[	O
n	O
]	O
=	O
σ2	O
|n|	O
1	O
we	O
obtain	O
x	O
a	O
kp	O
[	O
n	O
]	O
=	O
σ2	O
x	O
(	O
1	O
−	O
an	O
1	O
)	O
|n|	O
1	O
+	O
a	O
|n−n|	O
1	O
(	O
a	O
)	O
n	O
=	O
0	O
,	O
.	O
.	O
.	O
,	O
n	O
−	O
1	O
.	O
(	O
b.50	O
)	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
b.4	O
the	O
relationship	O
between	O
discrete-time	O
and	O
sampled	O
continuous-time	O
gmps	O
217	O
we	O
now	O
illustrate	O
this	O
result	O
for	O
n	O
=	O
3.	O
in	O
this	O
case	O
the	O
covariance	B
matrix	I
has	O
diagonal	O
entries	O
of	O
1	O
)	O
.	O
1	O
)	O
(	O
a1	O
+	O
a2	O
the	O
inverse	O
covariance	B
matrix	I
has	O
the	O
structure	O
described	O
above	O
.	O
multiplying	O
these	O
two	O
matrices	O
together	O
we	O
do	O
indeed	O
obtain	O
the	O
identity	O
matrix	B
.	O
1	O
)	O
and	O
oﬀ-diagonal	O
entries	O
of	O
σ2	O
x	O
(	O
1−a3	O
σ2	O
x	O
(	O
1−a3	O
1	O
)	O
(	O
1	O
+	O
a3	O
b.4	O
the	O
relationship	O
between	O
discrete-time	O
and	O
sampled	O
continuous-time	O
gmps	O
we	O
now	O
consider	O
the	O
relationship	O
between	O
continuous-time	O
and	O
discrete-time	O
gmps	O
.	O
in	O
particular	O
we	O
ask	O
the	O
question	O
,	O
is	O
a	O
regular	O
sampling	O
of	O
a	O
continuous-	O
time	O
ar	O
(	O
p	O
)	O
process	B
a	O
discrete-time	O
ar	O
(	O
p	O
)	O
process	B
?	O
it	O
turns	O
out	O
that	O
the	O
answer	O
will	O
,	O
in	O
general	O
,	O
be	O
negative	O
.	O
first	O
we	O
deﬁne	O
a	O
generalization	B
of	O
ar	O
processes	O
known	O
as	O
autoregressive	O
moving-average	O
(	O
arma	O
)	O
processes	O
.	O
arma	O
processes	O
the	O
ar	O
(	O
p	O
)	O
process	B
deﬁned	O
above	O
is	O
a	O
special	O
case	O
of	O
the	O
more	O
general	O
arma	O
(	O
p	O
,	O
q	O
)	O
process	B
which	O
is	O
deﬁned	O
as	O
px	O
qx	O
xt	O
=	O
aixt−i	O
+	O
bjzt−j	O
.	O
(	O
b.51	O
)	O
i=1	O
j=0	O
observe	O
that	O
the	O
ar	O
(	O
p	O
)	O
process	B
is	O
in	O
fact	O
also	O
an	O
arma	O
(	O
p	O
,	O
0	O
)	O
process	B
.	O
a	O
spectral	O
analysis	O
of	O
equation	O
b.51	O
similar	O
to	O
that	O
performed	O
in	O
section	O
b.3.1	O
gives	O
(	O
b.52	O
)	O
(	O
b.53	O
)	O
where	O
b	O
(	O
z	O
)	O
=pq	O
density	O
of	O
the	O
form	O
s	O
(	O
s	O
)	O
=	O
|b	O
(	O
eiωls	O
)	O
|2	O
|a	O
(	O
eiωls	O
)	O
|2	O
,	O
s	O
(	O
s	O
)	O
=	O
|b	O
(	O
2πis	O
)	O
|2	O
|a	O
(	O
2πis	O
)	O
|2	O
j=0	O
bjzj	O
.	O
in	O
continuous	O
time	O
a	O
process	B
with	O
a	O
rational	O
spectral	O
we	O
require	O
q	O
<	O
p	O
as	O
k	O
(	O
0	O
)	O
=r	O
s	O
(	O
s	O
)	O
ds	O
<	O
∞	O
.	O
is	O
known	O
as	O
a	O
arma	O
(	O
p	O
,	O
q	O
)	O
process	B
.	O
for	O
this	O
to	O
deﬁne	O
a	O
valid	O
covariance	B
function	I
discrete-time	O
observation	O
of	O
a	O
continuous-time	O
process	B
let	O
x	O
(	O
t	O
)	O
be	O
a	O
continuous-time	O
process	B
having	O
covariance	B
function	I
k	O
(	O
t	O
)	O
and	O
power	O
spectrum	O
s	O
(	O
s	O
)	O
.	O
let	O
xh	O
be	O
the	O
discrete-time	O
process	B
obtained	O
by	O
sampling	O
x	O
(	O
t	O
)	O
at	O
interval	O
h	O
,	O
so	O
that	O
xh	O
[	O
n	O
]	O
=	O
x	O
(	O
nh	O
)	O
for	O
n	O
∈	O
z.	O
clearly	O
the	O
covariance	B
function	I
of	O
this	O
process	B
is	O
given	O
by	O
kh	O
[	O
n	O
]	O
=	O
k	O
(	O
nh	O
)	O
.	O
by	O
eq	O
.	O
(	O
b.15	O
)	O
this	O
means	O
that	O
∞x	O
m=−∞	O
sh	O
(	O
s	O
)	O
=	O
s	O
(	O
s	O
+	O
m	O
h	O
)	O
(	O
b.54	O
)	O
where	O
sh	O
(	O
s	O
)	O
is	O
deﬁned	O
using	O
l	O
=	O
1/h	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
218	O
gaussian	O
markov	O
processes	O
theorem	O
b.1	O
let	O
x	O
be	O
a	O
continuous-time	O
stationary	O
gaussian	O
process	B
and	O
xh	O
be	O
the	O
discretization	O
of	O
this	O
process	B
.	O
if	O
x	O
is	O
an	O
arma	O
process	B
then	O
xh	O
is	O
also	O
an	O
arma	O
process	B
.	O
however	O
,	O
if	O
x	O
is	O
an	O
ar	O
process	B
then	O
xh	O
is	O
not	O
(	O
cid:3	O
)	O
necessarily	O
an	O
ar	O
process	B
.	O
the	O
proof	O
is	O
given	O
in	O
ihara	O
[	O
1993	O
,	O
theorem	O
2.7.1	O
]	O
.	O
it	O
is	O
easy	O
to	O
see	B
using	O
the	O
covariance	B
functions	O
given	O
in	O
sections	O
b.2.1	O
and	O
b.3.1	O
that	O
the	O
discretization	O
of	O
a	O
continuous-time	O
ar	O
(	O
1	O
)	O
process	B
is	O
indeed	O
a	O
discrete-time	O
ar	O
(	O
1	O
)	O
process	B
.	O
however	O
,	O
ihara	O
shows	O
that	O
,	O
in	O
general	O
,	O
the	O
discretization	O
of	O
a	O
continuous-time	O
ar	O
(	O
2	O
)	O
process	B
is	O
not	O
a	O
discrete-time	O
ar	O
(	O
2	O
)	O
process	B
.	O
b.5	O
markov	O
processes	O
in	O
higher	O
dimensions	O
we	O
have	O
concentrated	O
above	O
on	O
the	O
case	O
where	O
t	O
is	O
one-dimensional	O
.	O
in	O
higher	O
dimensions	O
it	O
is	O
interesting	O
to	O
ask	O
how	O
the	O
markov	O
property	O
might	O
be	O
general-	O
ized	O
.	O
let	O
∂s	O
be	O
an	O
inﬁnitely	O
diﬀerentiable	O
closed	O
surface	O
separating	O
rd	O
into	O
a	O
bounded	O
part	O
s−	O
and	O
an	O
unbounded	O
part	O
s+	O
.	O
loosely	O
speaking2	O
a	O
random	O
ﬁeld	O
x	O
(	O
t	O
)	O
is	O
said	O
to	O
be	O
quasi-markovian	O
if	O
x	O
(	O
t	O
)	O
for	O
t	O
∈	O
s−	O
and	O
x	O
(	O
u	O
)	O
for	O
u	O
∈	O
s+	O
are	O
independent	O
given	O
x	O
(	O
s	O
)	O
for	O
s	O
∈	O
∂s	O
.	O
wong	O
[	O
1971	O
]	O
showed	O
that	O
the	O
only	O
isotropic	O
quasi-markov	O
gaussian	O
ﬁeld	O
with	O
a	O
continuous	O
covariance	B
function	I
is	O
the	O
degen-	O
erate	O
case	O
x	O
(	O
t	O
)	O
=	O
x	O
(	O
0	O
)	O
,	O
where	O
x	O
(	O
0	O
)	O
is	O
a	O
gaussian	O
variate	O
.	O
however	O
,	O
if	O
instead	O
of	O
conditioning	O
on	O
the	O
values	O
that	O
the	O
ﬁeld	O
takes	O
on	O
in	O
∂s	O
,	O
one	O
conditions	O
on	O
a	O
somewhat	O
larger	O
set	B
,	O
then	O
gaussian	O
random	O
ﬁelds	O
with	O
non-trivial	O
markov-	O
type	O
structure	O
can	O
be	O
obtained	O
.	O
for	O
example	O
,	O
random	O
ﬁelds	O
with	O
an	O
inverse	O
j=1	O
kj	O
≤	O
2p	O
pseudo-markovian	O
of	O
order	O
p.	O
for	O
example	O
,	O
the	O
d-dimensional	O
tensor-product	O
i=1	O
e−αi|ti|	O
is	O
pseudo-markovian	O
of	O
order	O
d.	O
for	O
further	O
discussion	O
of	O
markov	O
properties	O
of	O
random	O
ﬁelds	O
see	B
the	O
appendix	O
in	O
adler	O
[	O
1981	O
]	O
.	O
power	O
spectrum	O
of	O
the	O
formp	O
and	O
c	O
(	O
s	O
·	O
s	O
)	O
p	O
≤	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
p	O
of	O
the	O
ou	O
process	B
k	O
(	O
t	O
)	O
=	O
qd	O
d	O
with	O
k	O
>	O
1	O
=pd	O
(	O
cid:12	O
)	O
(	O
cid:12	O
)	O
for	O
some	O
c	O
>	O
0	O
are	O
said	O
to	O
be	O
k	O
ak1	O
,	O
...	O
,	O
kd	O
sk1	O
1	O
···	O
skd	O
k	O
>	O
1=2p	O
ak1	O
,	O
...	O
,	O
kd	O
sk1	O
1	O
···	O
skd	O
d	O
if	O
instead	O
of	O
rd	O
we	O
wish	O
to	O
deﬁne	O
a	O
markov	O
random	O
ﬁeld	O
(	O
mrf	O
)	O
on	O
a	O
graph-	O
ical	O
structure	O
(	O
for	O
example	O
the	O
lattice	O
zd	O
)	O
things	O
become	O
more	O
straightforward	O
.	O
we	O
follow	O
the	O
presentation	O
in	O
jordan	O
[	O
2005	O
]	O
.	O
let	O
g	O
=	O
(	O
x	O
,	O
e	O
)	O
be	O
a	O
graph	O
where	O
x	O
is	O
a	O
set	B
of	O
nodes	O
that	O
are	O
in	O
one-to-one	O
correspondence	O
with	O
a	O
set	B
of	O
ran-	O
dom	O
variables	O
,	O
and	O
e	O
be	O
the	O
set	B
of	O
undirected	O
edges	O
of	O
the	O
graph	O
.	O
let	O
c	O
be	O
the	O
set	B
of	O
all	O
maximal	O
cliques	O
of	O
g.	O
a	O
potential	O
function	B
ψc	O
(	O
xc	O
)	O
is	O
a	O
function	B
on	O
the	O
possible	O
realizations	O
xc	O
of	O
the	O
maximal	O
clique	O
xc	O
.	O
potential	O
functions	O
are	O
assumed	O
to	O
be	O
(	O
strictly	O
)	O
positive	O
,	O
real-valued	O
functions	O
.	O
the	O
probability	B
distribution	O
p	O
(	O
x	O
)	O
corresponding	O
to	O
the	O
markov	O
random	O
ﬁeld	O
is	O
given	O
by	O
y	O
function	B
)	O
obtained	O
by	O
summing/integratingq	O
p	O
(	O
x	O
)	O
=	O
c∈c	O
1	O
z	O
ψc	O
(	O
xc	O
)	O
,	O
(	O
b.55	O
)	O
where	O
z	O
is	O
a	O
normalization	O
factor	O
(	O
known	O
in	O
statistical	O
physics	O
as	O
the	O
partition	O
c∈c	O
ψc	O
(	O
xc	O
)	O
over	O
all	O
possible	O
as-	O
2for	O
a	O
precise	O
formulation	O
of	O
this	O
deﬁnition	O
involving	O
σ-ﬁelds	O
see	B
adler	O
[	O
1981	O
,	O
p.	O
256	O
]	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
b.5	O
markov	O
processes	O
in	O
higher	O
dimensions	O
219	O
signments	O
of	O
values	O
to	O
the	O
nodes	O
x.	O
under	O
this	O
deﬁnition	O
it	O
is	O
easy	O
to	O
show	O
that	O
a	O
local	O
markov	O
property	O
holds	O
,	O
i.e	O
.	O
that	O
for	O
any	O
variable	O
x	O
the	O
conditional	B
distribution	O
of	O
x	O
given	O
all	O
other	O
variables	O
in	O
x	O
depends	O
only	O
on	O
those	O
variables	O
that	O
are	O
neighbours	O
of	O
x.	O
a	O
useful	O
reference	O
on	O
markov	O
random	O
ﬁelds	O
is	O
winkler	O
[	O
1995	O
]	O
.	O
a	O
simple	O
example	O
of	O
a	O
gaussian	O
markov	O
random	O
ﬁeld	O
has	O
the	O
form	O
,	O
(	O
b.56	O
)	O
(	O
cid:16	O
)	O
−	O
α1	O
x	O
x	O
(	O
xi	O
−	O
xj	O
)	O
2	O
(	O
cid:17	O
)	O
p	O
(	O
x	O
)	O
∝	O
exp	O
i	O
−	O
α2	O
x2	O
i	O
i	O
,	O
j	O
:	O
j∈n	O
(	O
i	O
)	O
where	O
n	O
(	O
i	O
)	O
denotes	O
the	O
set	B
of	O
neighbours	O
of	O
node	O
xi	O
and	O
α1	O
,	O
α2	O
>	O
0.	O
on	O
z2	O
one	O
might	O
choose	O
a	O
four-connected	O
neighbourhood	O
,	O
i.e	O
.	O
those	O
nodes	O
to	O
the	O
north	O
,	O
south	O
,	O
east	O
and	O
west	O
of	O
a	O
given	O
node	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
appendix	O
c	O
datasets	O
and	O
code	O
the	O
datasets	O
used	O
for	O
experiments	O
in	O
this	O
book	O
and	O
implementations	O
of	O
the	O
algorithms	O
presented	O
are	O
available	O
for	O
download	O
at	O
the	O
website	O
of	O
the	O
book	O
:	O
http	O
:	O
//www.gaussianprocess.org/gpml	O
the	O
programs	O
are	O
short	O
stand-alone	O
implementations	O
and	O
not	O
part	O
of	O
a	O
larger	O
package	O
.	O
they	O
are	O
meant	O
to	O
be	O
simple	O
to	O
understand	O
and	O
modify	O
for	O
a	O
desired	O
purpose	O
.	O
some	O
of	O
the	O
programs	O
allow	O
speciﬁcation	O
of	O
covariance	B
functions	O
from	O
a	O
selection	O
provided	O
,	O
or	O
to	O
link	O
in	O
user	O
deﬁned	O
covariance	B
code	O
.	O
for	O
some	O
of	O
the	O
plots	O
,	O
code	O
is	O
provided	O
which	O
produces	O
a	O
similar	O
plot	O
,	O
as	O
this	O
may	O
be	O
a	O
convenient	O
way	O
of	O
conveying	O
the	O
details	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
bibliography	O
abrahamsen	O
,	O
p.	O
(	O
1997	O
)	O
.	O
a	O
review	O
of	O
gaussian	O
random	O
fields	O
and	O
correlation	O
functions	O
.	O
tech-	O
http	O
:	O
//publications.nr.no/	O
p.	O
82	O
nical	O
report	O
917	O
,	O
norwegian	O
computing	O
center	O
,	O
oslo	O
,	O
norway	O
.	O
917	O
rapport.pdf	O
.	O
abramowitz	O
,	O
m.	O
and	O
stegun	O
,	O
i.	O
a	O
.	O
(	O
1965	O
)	O
.	O
handbook	O
of	O
mathematical	O
functions	O
.	O
dover	O
,	O
new	O
york	O
.	O
pp	O
.	O
84	O
,	O
85	O
adams	O
,	O
r.	O
(	O
1975	O
)	O
.	O
sobolev	O
spaces	O
.	O
academic	O
press	O
,	O
new	O
york	O
.	O
p.	O
134	O
adler	O
,	O
r.	O
j	O
.	O
(	O
1981	O
)	O
.	O
the	O
geometry	O
of	O
random	O
fields	O
.	O
wiley	O
,	O
chichester	O
.	O
pp	O
.	O
80	O
,	O
81	O
,	O
83	O
,	O
191	O
,	O
218	O
amari	O
,	O
s.	O
(	O
1985	O
)	O
.	O
diﬀerential-geometrical	O
methods	O
in	O
statistics	O
.	O
springer-verlag	O
,	O
berlin	O
.	O
p.	O
102	O
ansley	O
,	O
c.	O
f.	O
and	O
kohn	O
,	O
r.	O
(	O
1985	O
)	O
.	O
estimation	O
,	O
filtering	O
,	O
and	O
smoothing	O
in	O
state	O
space	O
models	O
with	O
p.	O
29	O
incompletely	O
speciﬁed	O
initial	O
conditions	O
.	O
annals	O
of	O
statistics	O
,	O
13	O
(	O
4	O
)	O
:1286–1316	O
.	O
arat´o	O
,	O
m.	O
(	O
1982	O
)	O
.	O
linear	B
stochastic	O
systems	O
with	O
constant	O
coeﬃcients	O
.	O
springer-verlag	O
,	O
berlin	O
.	O
lecture	O
p.	O
212	O
notes	O
in	O
control	O
and	O
information	O
sciences	O
45.	O
arfken	O
,	O
g.	O
(	O
1985	O
)	O
.	O
mathematical	O
methods	O
for	O
physicists	O
.	O
academic	O
press	O
,	O
san	O
diego	O
.	O
pp	O
.	O
xv	O
,	O
134	O
aronszajn	O
,	O
n.	O
(	O
1950	O
)	O
.	O
theory	O
of	O
reproducing	O
kernels	O
.	O
trans	O
.	O
amer	O
.	O
math	O
.	O
soc.	O
,	O
68:337–404	O
.	O
pp	O
.	O
129	O
,	O
130	O
bach	O
,	O
f.	O
r.	O
and	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
2002	O
)	O
.	O
kernel	B
independent	O
component	O
analysis	O
.	O
journal	O
of	O
machine	O
p.	O
97	O
learning	B
research	O
,	O
3	O
(	O
1	O
)	O
:1–48	O
.	O
baker	O
,	O
c.	O
t.	O
h.	O
(	O
1977	O
)	O
.	O
the	O
numerical	O
treatment	O
of	O
integral	O
equations	O
.	O
clarendon	O
press	O
,	O
oxford	O
.	O
pp	O
.	O
98	O
,	O
99	O
barber	O
,	O
d.	O
and	O
saad	O
,	O
d.	O
(	O
1996	O
)	O
.	O
does	O
extra	O
knowledge	O
necessarily	O
improve	O
generalisation	O
?	O
neural	O
p.	O
31	O
computation	O
,	O
8:202–214	O
.	O
bartle	O
,	O
r.	O
g.	O
(	O
1995	O
)	O
.	O
the	O
elements	O
of	O
integration	O
and	O
lebesgue	O
measure	B
.	O
wiley	O
,	O
new	O
york	O
.	O
p.	O
204	O
bartlett	O
,	O
p.	O
l.	O
,	O
jordan	O
,	O
m.	O
i.	O
,	O
and	O
mcauliﬀe	O
,	O
j.	O
d.	O
(	O
2003	O
)	O
.	O
convexity	O
,	O
classiﬁcation	B
and	O
risk	B
bounds	O
.	O
technical	O
report	O
638	O
,	O
department	O
of	O
statistics	O
,	O
university	O
of	O
california	O
,	O
berkeley	O
.	O
available	O
from	O
http	O
:	O
//www.stat.berkeley.edu/tech-reports/638.pdf	O
.	O
accepted	O
for	O
publication	O
in	O
journal	O
of	O
the	O
american	O
statistical	O
association	O
.	O
p.	O
157	O
berger	O
,	O
j.	O
o	O
.	O
(	O
1985	O
)	O
.	O
statistical	O
decision	O
theory	O
and	O
bayesian	O
analysis	O
.	O
springer	O
,	O
new	O
york	O
.	O
second	O
pp	O
.	O
22	O
,	O
35	O
edition	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
224	O
bibliography	O
bishop	O
,	O
c.	O
m.	O
(	O
1995	O
)	O
.	O
neural	O
networks	O
for	O
pattern	O
recognition	O
.	O
clarendon	O
press	O
,	O
oxford	O
.	O
p.	O
45	O
bishop	O
,	O
c.	O
m.	O
,	O
svensen	O
,	O
m.	O
,	O
and	O
williams	O
,	O
c.	O
k.	O
i	O
.	O
(	O
1998a	O
)	O
.	O
developments	O
of	O
the	O
generative	O
topographic	O
p.	O
196	O
mapping	O
.	O
neurocomputing	O
,	O
21:203–224	O
.	O
bishop	O
,	O
c.	O
m.	O
,	O
svensen	O
,	O
m.	O
,	O
and	O
williams	O
,	O
c.	O
k.	O
i	O
.	O
(	O
1998b	O
)	O
.	O
gtm	O
:	O
the	O
generative	O
topographic	O
p.	O
196	O
mapping	O
.	O
neural	O
computation	O
,	O
10	O
(	O
1	O
)	O
:215–234	O
.	O
blake	O
,	O
i.	O
f.	O
and	O
lindsey	O
,	O
w.	O
c.	O
(	O
1973	O
)	O
.	O
level-crossing	O
problems	O
for	O
random	O
processes	O
.	O
ieee	O
trans	O
p.	O
81	O
information	O
theory	O
,	O
19	O
(	O
3	O
)	O
:295–315	O
.	O
blight	O
,	O
b.	O
j.	O
n.	O
and	O
ott	O
,	O
l.	O
(	O
1975	O
)	O
.	O
a	O
bayesian	O
approach	O
to	O
model	B
inadequacy	O
for	O
polynomial	B
regression	O
.	O
p.	O
28	O
biometrika	O
,	O
62	O
(	O
1	O
)	O
:79–88	O
.	O
boyd	O
,	O
s.	O
and	O
vandenberghe	O
,	O
l.	O
(	O
2004	O
)	O
.	O
convex	B
optimization	O
.	O
cambridge	O
university	O
press	O
,	O
cambridge	O
,	O
p.	O
206	O
uk	O
.	O
boyle	O
,	O
p.	O
and	O
frean	O
,	O
m.	O
(	O
2005	O
)	O
.	O
dependent	O
gaussian	O
processes	O
.	O
in	O
saul	O
,	O
l.	O
k.	O
,	O
weiss	O
,	O
y.	O
,	O
and	O
bottou	O
,	O
l.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
17	O
,	O
pages	O
217–224	O
.	O
mit	O
press	O
.	O
p.	O
190	O
bracewell	O
,	O
r.	O
n.	O
(	O
1986	O
)	O
.	O
the	O
fourier	O
transform	O
and	O
its	O
applications	O
.	O
mcgraw-hill	O
,	O
singapore	O
,	O
inter-	O
pp	O
.	O
83	O
,	O
206	O
national	O
edition	O
.	O
caruana	O
,	O
r.	O
(	O
1997	O
)	O
.	O
multitask	O
learning	B
.	O
machine	O
learning	B
,	O
28	O
(	O
1	O
)	O
:41–75	O
.	O
p.	O
115	O
chatﬁeld	O
,	O
c.	O
(	O
1989	O
)	O
.	O
the	O
analysis	O
of	O
time	O
series	O
:	O
an	O
introduction	O
.	O
chapman	O
and	O
hall	O
,	O
london	O
,	O
4th	O
pp	O
.	O
82	O
,	O
209	O
,	O
214	O
edition	O
.	O
choi	O
,	O
t.	O
and	O
schervish	O
,	O
m.	O
j	O
.	O
(	O
2004	O
)	O
.	O
posterior	O
consistency	O
in	O
nonparametric	O
regression	B
prob-	O
lems	O
under	O
gaussian	O
process	B
priors	O
.	O
technical	O
report	O
809	O
,	O
department	O
of	O
statistics	O
,	O
cmu	O
.	O
http	O
:	O
//www.stat.cmu.edu/tr/tr809/tr809.html	O
.	O
p.	O
156	O
choudhuri	O
,	O
n.	O
,	O
ghosal	O
,	O
s.	O
,	O
and	O
roy	O
,	O
a	O
.	O
(	O
2005	O
)	O
.	O
nonparametric	O
binary	B
regression	O
using	O
a	O
gaussian	O
p.	O
156	O
process	B
prior	O
.	O
unpublished	O
.	O
http	O
:	O
//www4.stat.ncsu.edu/∼sghosal/papers.html	O
.	O
chu	O
,	O
w.	O
and	O
ghahramani	O
,	O
z	O
.	O
(	O
2005	O
)	O
.	O
gaussian	O
processes	O
for	O
ordinal	O
regression	B
.	O
journal	O
of	O
machine	O
p.	O
191	O
learning	B
research	O
,	O
6:1019–1041	O
.	O
collins	O
,	O
m.	O
and	O
duﬀy	O
,	O
n.	O
(	O
2002	O
)	O
.	O
convolution	O
kernels	O
for	O
natural	O
language	O
.	O
in	O
diettrich	O
,	O
t.	O
g.	O
,	O
becker	O
,	O
s.	O
,	O
and	O
ghahramani	O
,	O
z.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
14.	O
mit	O
press	O
.	O
p.	O
101	O
collobert	O
,	O
r.	O
and	O
bengio	O
,	O
s.	O
(	O
2001	O
)	O
.	O
gression	O
problems	O
.	O
∼bengio/projects/svmtorch.html	O
.	O
journal	O
of	O
machine	O
learning	B
research	O
,	O
1:143–160	O
.	O
svmtorch	O
:	O
support	B
vector	I
machines	O
for	O
large-scale	O
re-	O
http	O
:	O
//www.idiap.ch/	O
pp	O
.	O
69	O
,	O
72	O
cornford	O
,	O
d.	O
,	O
nabney	O
,	O
i.	O
t.	O
,	O
and	O
williams	O
,	O
c.	O
k.	O
i	O
.	O
(	O
2002	O
)	O
.	O
modelling	O
frontal	O
discontinuities	O
in	O
wind	O
p.	O
85	O
fields	O
.	O
journal	O
of	O
nonparameteric	O
statsitics	O
,	O
14	O
(	O
1-2	O
)	O
:43–58	O
.	O
cox	O
,	O
d.	O
d.	O
(	O
1984	O
)	O
.	O
multivariate	O
smoothing	O
spline	O
functions	O
.	O
siam	O
journal	O
on	O
numerical	O
analysis	O
,	O
p.	O
156	O
21	O
(	O
4	O
)	O
:789–813	O
.	O
cox	O
,	O
d.	O
d.	O
and	O
o	O
’	O
sullivan	O
,	O
f.	O
(	O
1990	O
)	O
.	O
asymptotic	O
analysis	O
of	O
penalized	B
likelihood	O
and	O
related	O
esti-	O
p.	O
156	O
mators	O
.	O
annals	O
of	O
statistics	O
,	O
18	O
(	O
4	O
)	O
:1676–1695	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
bibliography	O
225	O
craven	O
,	O
p.	O
and	O
wahba	O
,	O
g.	O
(	O
1979	O
)	O
.	O
smoothing	O
noisy	O
data	O
with	O
spline	O
functions	O
.	O
numer	O
.	O
math.	O
,	O
31:377–	O
p.	O
112	O
403.	O
cressie	O
,	O
n.	O
a.	O
c.	O
(	O
1993	O
)	O
.	O
statistics	O
for	O
spatial	O
data	O
.	O
wiley	O
,	O
new	O
york	O
.	O
pp	O
.	O
30	O
,	O
137	O
,	O
190	O
cristianini	O
,	O
n.	O
and	O
shawe-taylor	O
,	O
j	O
.	O
(	O
2000	O
)	O
.	O
an	O
introduction	O
to	O
support	B
vector	I
machines	O
.	O
cambridge	O
p.	O
141	O
university	O
press	O
.	O
cristianini	O
,	O
n.	O
,	O
shawe-taylor	O
,	O
j.	O
,	O
elisseeﬀ	O
,	O
a.	O
,	O
and	O
kandola	O
,	O
j	O
.	O
(	O
2002	O
)	O
.	O
on	O
kernel-target	O
alignment	B
.	O
in	O
diettrich	O
,	O
t.	O
g.	O
,	O
becker	O
,	O
s.	O
,	O
and	O
ghahramani	O
,	O
z.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
14.	O
mit	O
press	O
.	O
p.	O
128	O
csat´o	O
,	O
l.	O
(	O
2002	O
)	O
.	O
gaussian	O
processes—iterative	O
sparse	O
approximations	O
.	O
phd	O
thesis	O
,	O
aston	O
university	O
,	O
p.	O
179	O
uk	O
.	O
csat´o	O
,	O
l.	O
and	O
opper	O
,	O
m.	O
(	O
2002	O
)	O
.	O
sparse	O
on-line	O
gaussian	O
processes	O
.	O
neural	O
computation	O
,	O
14	O
(	O
3	O
)	O
:641–	O
pp	O
.	O
180	O
,	O
185	O
668.	O
csat´o	O
,	O
l.	O
,	O
opper	O
,	O
m.	O
,	O
and	O
winther	O
,	O
o	O
.	O
(	O
2002	O
)	O
.	O
tap	O
gibbs	O
free	O
energy	O
,	O
belief	O
propagation	O
and	O
spar-	O
sity	O
.	O
in	O
diettrich	O
,	O
t.	O
g.	O
,	O
becker	O
,	O
s.	O
,	O
and	O
ghahramani	O
,	O
z.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
14	O
,	O
pages	O
657–663	O
.	O
mit	O
press	O
.	O
pp	O
.	O
180	O
,	O
185	O
daley	O
,	O
r.	O
(	O
1991	O
)	O
.	O
atmospheric	O
data	O
analysis	O
.	O
cambridge	O
university	O
press	O
,	O
cambridge	O
,	O
uk	O
.	O
p.	O
30	O
david	O
,	O
h.	O
a	O
.	O
(	O
1970	O
)	O
.	O
order	O
statistics	O
.	O
wiley	O
,	O
new	O
york	O
.	O
davis	O
,	O
p.	O
j	O
.	O
(	O
1979	O
)	O
.	O
circulant	O
matrices	O
.	O
wiley	O
,	O
new	O
york	O
.	O
dawid	O
,	O
a.	O
p.	O
(	O
1976	O
)	O
.	O
properties	O
of	O
diagnostic	O
data	O
distributions	O
.	O
biometrics	O
,	O
32:647–658	O
.	O
pp	O
.	O
169	O
,	O
170	O
p.	O
216	O
p.	O
34	O
dellaportas	O
,	O
p.	O
and	O
stephens	O
,	O
d.	O
a	O
.	O
(	O
1995	O
)	O
.	O
bayesian	O
analysis	O
of	O
errors-in-variables	B
regression	I
models	O
.	O
p.	O
192	O
biometrics	O
,	O
51:1085–1095	O
.	O
devroye	O
,	O
l.	O
,	O
gy¨orﬁ	O
,	O
l.	O
,	O
and	O
lugosi	O
,	O
g.	O
(	O
1996	O
)	O
.	O
a	O
probabilistic	B
theory	O
of	O
pattern	O
recognition	O
.	O
springer	O
,	O
pp	O
.	O
156	O
,	O
166	O
new	O
york	O
.	O
diaconis	O
,	O
p.	O
and	O
freedman	O
,	O
d.	O
(	O
1986	O
)	O
.	O
on	O
the	O
consistency	B
of	O
bayes	O
estimates	O
.	O
annals	O
of	O
statistics	O
,	O
p.	O
156	O
14	O
(	O
1	O
)	O
:1–26	O
.	O
diggle	O
,	O
p.	O
j	O
.	O
(	O
1990	O
)	O
.	O
time	O
series	O
:	O
a	O
biostatistical	O
introduction	O
.	O
clarendon	O
press	O
,	O
oxford	O
.	O
pp	O
.	O
214	O
,	O
215	O
diggle	O
,	O
p.	O
j.	O
,	O
tawn	O
,	O
j.	O
a.	O
,	O
and	O
moyeed	O
,	O
r.	O
a	O
.	O
(	O
1998	O
)	O
.	O
model-based	O
geostatistics	B
(	O
with	O
discussion	O
)	O
.	O
p.	O
191	O
applied	O
statistics	O
,	O
47:299–350	O
.	O
doob	O
,	O
j.	O
l.	O
(	O
1944	O
)	O
.	O
the	O
elementary	O
gaussian	O
processes	O
.	O
annals	O
of	O
mathematical	O
statistics	O
,	O
15	O
(	O
3	O
)	O
:229–	O
p.	O
212	O
282.	O
doob	O
,	O
j.	O
l.	O
(	O
1994	O
)	O
.	O
measure	B
theory	O
.	O
springer-verlag	O
,	O
new	O
york	O
.	O
p.	O
204	O
drineas	O
,	O
p.	O
and	O
mahoney	O
,	O
m.	O
w.	O
(	O
2005	O
)	O
.	O
on	O
the	O
nystr¨om	O
method	O
for	O
approximating	O
a	O
gram	O
ma-	O
trix	O
for	O
improved	O
kernel-based	O
learning	B
.	O
technical	O
report	O
yaleu/dcs/tr-1319	O
,	O
yale	O
university	O
.	O
p.	O
174	O
http	O
:	O
//cs-www.cs.yale.edu/homes/mmahoney	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
226	O
bibliography	O
duchon	O
,	O
j	O
.	O
(	O
1977	O
)	O
.	O
splines	B
minimizing	O
rotation-invariant	O
semi-norms	O
in	O
sobolev	O
spaces	O
.	O
in	O
schempp	O
,	O
w.	O
and	O
zeller	O
,	O
k.	O
,	O
editors	O
,	O
constructive	O
theory	O
of	O
functions	O
of	O
several	O
variables	O
,	O
pages	O
85–100	O
.	O
springer-verlag	O
.	O
p.	O
137	O
duda	O
,	O
r.	O
o.	O
and	O
hart	O
,	O
p.	O
e.	O
(	O
1973	O
)	O
.	O
pattern	O
classiﬁcation	B
and	O
scene	O
analysis	O
.	O
john	O
wiley	O
,	O
new	O
york	O
.	O
p.	O
146	O
edgeworth	O
,	O
f.	O
y	O
.	O
(	O
1887	O
)	O
.	O
on	O
observations	O
relating	O
to	O
several	O
quantities	O
.	O
hermathena	O
,	O
6:279–285	O
.	O
p.	O
146	O
faul	O
,	O
a.	O
c.	O
and	O
tipping	O
,	O
m.	O
e.	O
(	O
2002	O
)	O
.	O
analysis	O
of	O
sparse	O
bayesian	O
learning	B
.	O
in	O
dietterich	O
,	O
t.	O
g.	O
,	O
becker	O
,	O
s.	O
,	O
and	O
ghahramani	O
,	O
z.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
14	O
,	O
pages	O
383–389	O
,	O
cambridge	O
,	O
massachussetts	O
.	O
mit	O
press	O
.	O
p.	O
149	O
feldman	O
,	O
j	O
.	O
(	O
1958	O
)	O
.	O
equivalence	O
and	O
perpendicularity	O
of	O
gaussian	O
processes	O
.	O
paciﬁc	O
j	O
.	O
math.	O
,	O
8:699–	O
p.	O
157	O
708.	O
erratum	O
in	O
paciﬁc	O
j.	O
math	O
.	O
9	O
,	O
1295-1296	O
(	O
1959	O
)	O
.	O
ferrari	O
trecate	O
,	O
g.	O
,	O
williams	O
,	O
c.	O
k.	O
i.	O
,	O
and	O
opper	O
,	O
m.	O
(	O
1999	O
)	O
.	O
finite-dimensional	O
approximation	O
of	O
gaussian	O
processes	O
.	O
in	O
kearns	O
,	O
m.	O
s.	O
,	O
solla	O
,	O
s.	O
a.	O
,	O
and	O
cohn	O
,	O
d.	O
a.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
11	O
,	O
pages	O
218–224	O
.	O
mit	O
press	O
.	O
p.	O
152	O
fine	O
,	O
s.	O
and	O
scheinberg	O
,	O
k.	O
(	O
2002	O
)	O
.	O
eﬃcient	O
svm	O
training	O
using	O
low-rank	O
kernel	B
representations	O
.	O
pp	O
.	O
47	O
,	O
174	O
journal	O
of	O
machine	O
learning	B
research	O
,	O
2	O
(	O
2	O
)	O
:243–264	O
.	O
fowlkes	O
,	O
c.	O
,	O
belongie	O
,	O
s.	O
,	O
and	O
malik	O
,	O
j	O
.	O
(	O
2001	O
)	O
.	O
eﬃcient	O
spatiotemporal	O
grouping	O
using	O
the	O
nystr¨om	O
method	O
.	O
in	O
proceedings	O
of	O
the	O
ieee	O
conference	O
on	O
computer	O
vision	O
and	O
pattern	O
recognition	O
,	O
cvpr	O
2001.	O
p.	O
172	O
freedman	O
,	O
d.	O
(	O
1999	O
)	O
.	O
on	O
the	O
bernstein-von	O
mises	O
theorem	O
with	O
inﬁnite-dimensional	O
parameters	O
.	O
p.	O
156	O
annals	O
of	O
statistics	O
,	O
27	O
(	O
4	O
)	O
:1119–1140	O
.	O
frieze	O
,	O
a.	O
,	O
kannan	O
,	O
r.	O
,	O
and	O
vempala	O
,	O
s.	O
(	O
1998	O
)	O
.	O
fast	O
monte-carlo	O
algorithms	O
for	O
finding	O
low-rank	O
approximations	O
.	O
in	O
39th	O
conference	O
on	O
the	O
foundations	O
of	O
computer	O
science	O
,	O
pages	O
370–378	O
.	O
p.	O
174	O
geisser	O
,	O
s.	O
and	O
eddy	O
,	O
w.	O
f.	O
(	O
1979	O
)	O
.	O
a	O
predictive	B
approach	O
to	O
model	B
selection	O
.	O
journal	O
of	O
the	O
americal	O
p.	O
117	O
statistical	O
association	O
,	O
74	O
(	O
365	O
)	O
:153–160	O
.	O
geman	O
,	O
s.	O
and	O
geman	O
,	O
d.	O
(	O
1984	O
)	O
.	O
stochastic	O
relaxation	O
,	O
gibbs	O
distributions	O
,	O
and	O
the	O
bayesian	O
restora-	O
p.	O
216	O
tion	O
of	O
images	O
.	O
ieee	O
trans	O
.	O
pattern	O
analysis	O
and	O
machine	O
intellligence	O
,	O
6	O
(	O
6	O
)	O
:721–741	O
.	O
gibbs	O
,	O
m.	O
n.	O
(	O
1997	O
)	O
.	O
bayesian	O
gaussian	O
processes	O
for	O
regression	B
and	O
classiﬁcation	B
.	O
phd	O
thesis	O
,	O
p.	O
93	O
department	O
of	O
physics	O
,	O
university	O
of	O
cambridge	O
.	O
gibbs	O
,	O
m.	O
n.	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1997	O
)	O
.	O
eﬃcient	O
implementation	O
of	O
gaussian	O
processes	O
.	O
unpub-	O
lished	O
manuscript	O
.	O
cavendish	O
laboratory	O
,	O
cambridge	O
,	O
uk	O
.	O
http	O
:	O
//www.inference.phy.cam.ac.uk/	O
mackay/bayesgp.html	O
.	O
p.	O
181	O
gibbs	O
,	O
m.	O
n.	O
and	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
2000	O
)	O
.	O
variational	O
gaussian	O
process	B
classiﬁers	O
.	O
ieee	O
transactions	O
p.	O
41	O
on	O
neural	O
networks	O
,	O
11	O
(	O
6	O
)	O
:1458–1464	O
.	O
gihman	O
,	O
i.	O
i.	O
and	O
skorohod	O
,	O
a.	O
v.	O
(	O
1974	O
)	O
.	O
the	O
theory	O
of	O
stochastic	O
processes	O
,	O
volume	O
1.	O
springer	O
p.	O
82	O
verlag	O
,	O
berlin	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
bibliography	O
227	O
girard	O
,	O
a.	O
,	O
rasmussen	O
,	O
c.	O
e.	O
,	O
qui˜nonero-candela	O
,	O
j.	O
,	O
and	O
murray-smith	O
,	O
r.	O
(	O
2003	O
)	O
.	O
gaussian	O
process	B
priors	O
with	O
uncertain	B
inputs	I
:	O
application	O
to	O
multiple-step	O
ahead	O
time	O
series	O
forecasting	O
.	O
in	O
becker	O
,	O
s.	O
,	O
thrun	O
,	O
s.	O
,	O
and	O
obermayer	O
,	O
k.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
15.	O
mit	O
press	O
.	O
p.	O
192	O
girosi	O
,	O
f.	O
(	O
1991	O
)	O
.	O
models	O
of	O
noise	O
and	O
robust	O
estimates	O
.	O
technical	O
report	O
ai	O
memo	O
1287	O
,	O
mit	O
ai	O
p.	O
146	O
laboratory	O
.	O
girosi	O
,	O
f.	O
,	O
jones	O
,	O
m.	O
,	O
and	O
poggio	O
,	O
t.	O
(	O
1995	O
)	O
.	O
regularization	B
theory	O
and	O
neural	O
networks	O
architectures	O
.	O
p.	O
25	O
neural	O
computation	O
,	O
7	O
(	O
2	O
)	O
:219–269	O
.	O
goldberg	O
,	O
p.	O
w.	O
,	O
williams	O
,	O
c.	O
k.	O
i.	O
,	O
and	O
bishop	O
,	O
c.	O
m.	O
(	O
1998	O
)	O
.	O
regression	B
with	O
input-dependent	O
noise	O
:	O
a	O
gaussian	O
process	B
treatment	O
.	O
in	O
jordan	O
,	O
m.	O
i.	O
,	O
kearns	O
,	O
m.	O
j.	O
,	O
and	O
solla	O
,	O
s.	O
a.	O
,	O
editors	O
,	O
advances	O
p.	O
191	O
in	O
neural	O
information	O
processing	O
systems	O
10.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
golub	O
,	O
g.	O
h.	O
and	O
van	O
loan	O
,	O
c.	O
f.	O
(	O
1989	O
)	O
.	O
matrix	B
computations	O
.	O
johns	O
hopkins	O
university	O
press	O
,	O
pp	O
.	O
172	O
,	O
173	O
,	O
174	O
,	O
181	O
,	O
202	O
baltimore	O
.	O
second	O
edition	O
.	O
gradshteyn	O
,	O
i.	O
s.	O
and	O
ryzhik	O
,	O
i.	O
m.	O
(	O
1980	O
)	O
.	O
tables	O
of	O
integrals	B
,	O
series	O
and	O
products	O
.	O
academic	O
press	O
.	O
pp	O
.	O
98	O
,	O
103	O
corrected	O
and	O
enlarged	O
edition	O
prepared	O
by	O
a.	O
jeﬀrey	O
.	O
green	O
,	O
p.	O
j.	O
and	O
silverman	O
,	O
b.	O
w.	O
(	O
1994	O
)	O
.	O
nonparametric	O
regression	B
and	O
generalized	B
linear	O
models	O
.	O
p.	O
138	O
chapman	O
and	O
hall	O
,	O
london	O
.	O
grenander	O
,	O
u.	O
,	O
chow	O
,	O
y.	O
,	O
and	O
keenan	O
,	O
d.	O
m.	O
(	O
1991	O
)	O
.	O
hands	O
:	O
a	O
pattern	O
theoretic	O
study	O
of	O
biological	O
pp	O
.	O
214	O
,	O
216	O
shapes	O
.	O
springer-verlag	O
,	O
new	O
york	O
.	O
grimmett	O
,	O
g.	O
r.	O
and	O
stirzaker	O
,	O
d.	O
r.	O
(	O
1992	O
)	O
.	O
probability	B
and	O
random	O
processes	O
.	O
oxford	O
university	O
pp	O
.	O
94	O
,	O
213	O
press	O
,	O
oxford	O
,	O
england	O
,	O
second	O
edition	O
.	O
gr¨unwald	O
,	O
p.	O
d.	O
and	O
langford	O
,	O
j	O
.	O
(	O
2004	O
)	O
.	O
suboptimal	O
behaviour	O
of	O
bayes	O
and	O
mdl	O
in	O
classiﬁcation	B
under	O
misspeciﬁcation	O
.	O
in	O
proc	O
.	O
seventeenth	O
annual	O
conference	O
on	O
computational	O
learning	B
theory	O
p.	O
156	O
(	O
colt	O
2004	O
)	O
.	O
gy¨orﬁ	O
,	O
l.	O
,	O
kohler	O
,	O
m.	O
,	O
krzy˙zak	O
,	O
a.	O
,	O
and	O
walk	O
,	O
h.	O
(	O
2002	O
)	O
.	O
a	O
distribution-free	O
theory	O
of	O
nonparametric	O
p.	O
156	O
regression	B
.	O
springer	O
,	O
new	O
york	O
.	O
hajek	O
,	O
j	O
.	O
(	O
1958	O
)	O
.	O
on	O
a	O
property	O
of	O
normal	O
distributions	O
of	O
any	O
stochastic	O
process	O
(	O
in	O
russian	O
)	O
.	O
czechoslovak	O
math	O
.	O
j.	O
,	O
8:610–618	O
.	O
translated	O
in	O
selected	O
trans	O
.	O
math	O
.	O
statist	O
.	O
probab	O
.	O
1	O
245-252	O
(	O
1961	O
)	O
.	O
also	O
available	O
in	O
collected	O
works	O
of	O
jaroslav	O
hajek	O
,	O
eds	O
.	O
m.	O
huˇskov´a	O
,	O
r.	O
beran	O
,	O
v.	O
dupaˇc	O
,	O
wiley	O
,	O
(	O
1998	O
)	O
.	O
p.	O
157	O
hand	O
,	O
d.	O
j.	O
,	O
mannila	O
,	O
h.	O
,	O
and	O
smyth	O
,	O
p.	O
(	O
2001	O
)	O
.	O
principles	O
of	O
data	O
mining	O
.	O
mit	O
press	O
.	O
hannan	O
,	O
e.	O
j	O
.	O
(	O
1970	O
)	O
.	O
multiple	O
time	O
series	O
.	O
wiley	O
,	O
new	O
york	O
.	O
p.	O
100	O
p.	O
215	O
hansen	O
,	O
l.	O
k.	O
,	O
liisberg	O
,	O
c.	O
,	O
and	O
salamon	O
,	O
p.	O
(	O
1997	O
)	O
.	O
the	O
error-reject	O
tradeoﬀ	O
.	O
open	O
sys	O
.	O
&	O
information	O
p.	O
36	O
dyn.	O
,	O
4:159–184	O
.	O
hastie	O
,	O
t.	O
j.	O
and	O
tibshirani	O
,	O
r.	O
j	O
.	O
(	O
1990	O
)	O
.	O
generalized	B
additive	O
models	O
.	O
chapman	O
and	O
hall	O
.	O
pp	O
.	O
24	O
,	O
25	O
,	O
95	O
haussler	O
,	O
d.	O
(	O
1999	O
)	O
.	O
convolution	O
kernels	O
on	O
discrete	O
structures	O
.	O
technical	O
report	O
ucsc-crl-99-10	O
,	O
p.	O
101	O
dept	O
of	O
computer	O
science	O
,	O
university	O
of	O
california	O
at	O
santa	O
cruz	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
228	O
bibliography	O
hawkins	O
,	O
d.	O
l.	O
(	O
1989	O
)	O
.	O
some	O
practical	O
problems	O
in	O
implementing	O
a	O
certain	O
sieve	O
estimator	O
of	O
the	O
gaussian	O
mean	B
function	I
.	O
communications	O
in	O
statistics—simulation	O
and	O
computation	O
,	O
18	O
(	O
2	O
)	O
:481–	O
500.	O
p.	O
97	O
hoerl	O
,	O
a.	O
e.	O
and	O
kennard	O
,	O
r.	O
w.	O
(	O
1970	O
)	O
.	O
ridge	B
regression	I
:	O
biased	O
estimation	O
for	O
nonorthogonal	O
p.	O
11	O
problems	O
.	O
technometrics	O
,	O
12	O
(	O
1	O
)	O
:55–67	O
.	O
hornik	O
,	O
k.	O
(	O
1993	O
)	O
.	O
some	O
new	O
results	O
on	O
neural	B
network	I
approximation	O
.	O
neural	O
networks	O
,	O
6	O
(	O
8	O
)	O
:1069–	O
p.	O
90	O
1072.	O
ihara	O
,	O
s.	O
(	O
1993	O
)	O
.	O
information	O
theory	O
for	O
continuous	O
systems	O
.	O
world	O
scientiﬁc	O
,	O
singapore	O
.	O
p.	O
218	O
jaakkola	O
,	O
t.	O
s.	O
,	O
diekhans	O
,	O
m.	O
,	O
and	O
haussler	O
,	O
d.	O
(	O
2000	O
)	O
.	O
a	O
discriminative	O
framework	O
for	O
detecting	O
pp	O
.	O
101	O
,	O
102	O
,	O
104	O
remote	O
protein	O
homologies	O
.	O
journal	O
of	O
computational	O
biology	O
,	O
7:95–114	O
.	O
jaakkola	O
,	O
t.	O
s.	O
and	O
haussler	O
,	O
d.	O
(	O
1999	O
)	O
.	O
probabilistic	B
kernel	O
regression	B
models	O
.	O
in	O
heckerman	O
,	O
d.	O
and	O
whittaker	O
,	O
j.	O
,	O
editors	O
,	O
workshop	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
7.	O
morgan	O
kaufmann	O
.	O
p.	O
41	O
jacobs	O
,	O
r.	O
a.	O
,	O
jordan	O
,	O
m.	O
i.	O
,	O
nowlan	O
,	O
s.	O
j.	O
,	O
and	O
hinton	O
,	O
g.	O
e.	O
(	O
1991	O
)	O
.	O
adaptive	O
mixtures	O
of	O
local	O
p.	O
192	O
experts	O
.	O
neural	O
computation	O
,	O
3:79–87	O
.	O
johnson	O
,	O
n.	O
l.	O
,	O
kotz	O
,	O
s.	O
,	O
and	O
balakrishnan	O
,	O
n.	O
(	O
1995	O
)	O
.	O
continuous	O
univariate	O
distributions	O
volume	O
2.	O
p.	O
45	O
john	O
wiley	O
and	O
sons	O
,	O
new	O
york	O
,	O
second	O
edition	O
.	O
jones	O
,	O
d.	O
r.	O
(	O
2001	O
)	O
.	O
a	O
taxonomy	O
of	O
global	O
optimization	O
methods	O
based	O
on	O
response	O
surfaces	O
.	O
j.	O
p.	O
193	O
global	O
optimization	O
,	O
21:345–383	O
.	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
2005	O
)	O
.	O
an	O
introduction	O
to	O
probabilistic	B
graphical	O
models	O
.	O
draft	O
book	O
.	O
journel	O
,	O
a.	O
g.	O
and	O
huijbregts	O
,	O
c.	O
j	O
.	O
(	O
1978	O
)	O
.	O
mining	O
geostatistics	B
.	O
academic	O
press	O
.	O
p.	O
218	O
p.	O
30	O
kailath	O
,	O
t.	O
(	O
1971	O
)	O
.	O
rkhs	O
approach	O
to	O
detection	O
and	O
estimation	O
problems—part	O
i	O
:	O
deterministic	O
p.	O
131	O
signals	O
in	O
gaussian	O
noise	O
.	O
ieee	O
trans	O
.	O
information	O
theory	O
,	O
17	O
(	O
5	O
)	O
:530–549	O
.	O
kammler	O
,	O
d.	O
w.	O
(	O
2000	O
)	O
.	O
a	O
first	O
course	O
in	O
fourier	O
analysis	O
.	O
prentice-hall	O
,	O
upper	O
saddle	O
river	O
,	O
nj	O
.	O
p.	O
208	O
kashyap	O
,	O
r.	O
l.	O
and	O
chellappa	O
,	O
r.	O
(	O
1981	O
)	O
.	O
stochastic	O
models	O
for	O
closed	O
boundary	O
analysis	O
:	O
represen-	O
p.	O
216	O
tation	O
and	O
reconstruction	O
.	O
ieee	O
trans	O
.	O
on	O
information	O
theory	O
,	O
27	O
(	O
5	O
)	O
:627–637	O
.	O
keeling	O
,	O
c.	O
d.	O
and	O
whorf	O
,	O
t.	O
p.	O
(	O
2004	O
)	O
.	O
atmospheric	O
co2	O
records	O
from	O
sites	O
in	O
the	O
sio	O
air	O
sampling	O
network	O
.	O
in	O
trends	O
:	O
a	O
compendium	O
of	O
data	O
on	O
global	O
change	O
.	O
carbon	O
dioxide	O
information	O
analysis	O
p.	O
119	O
center	O
,	O
oak	O
ridge	B
national	O
laboratory	O
,	O
oak	O
ridge	B
,	O
tenn.	O
,	O
u.s.a.	O
kent	O
,	O
j.	O
t.	O
and	O
mardia	O
,	O
k.	O
v.	O
(	O
1994	O
)	O
.	O
the	O
link	O
between	O
kriging	B
and	O
thin-plate	O
splines	B
.	O
in	O
kelly	O
,	O
p.	O
137	O
f.	O
p.	O
,	O
editor	O
,	O
probability	B
,	O
statsitics	O
and	O
optimization	O
,	O
pages	O
325–339	O
.	O
wiley	O
.	O
kimeldorf	O
,	O
g.	O
and	O
wahba	O
,	O
g.	O
(	O
1970	O
)	O
.	O
a	O
correspondence	O
between	O
bayesian	O
estimation	O
of	O
stochastic	O
p.	O
136	O
processes	O
and	O
smoothing	O
by	O
splines	B
.	O
annals	O
of	O
mathematical	O
statistics	O
,	O
41:495–502	O
.	O
kimeldorf	O
,	O
g.	O
and	O
wahba	O
,	O
g.	O
(	O
1971	O
)	O
.	O
some	O
results	O
on	O
tchebycheﬃan	O
spline	O
functions	O
.	O
j.	O
mathematical	O
p.	O
132	O
analysis	O
and	O
applications	O
,	O
33	O
(	O
1	O
)	O
:82–95	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
bibliography	O
229	O
kohn	O
,	O
r.	O
and	O
ansley	O
,	O
c.	O
f.	O
(	O
1987	O
)	O
.	O
a	O
new	O
algorithm	O
for	O
spline	O
smoothing	O
based	O
on	O
smoothing	O
a	O
p.	O
141	O
stochastic	O
process	O
.	O
siam	O
j.	O
sci	O
.	O
stat	O
.	O
comput.	O
,	O
8	O
(	O
1	O
)	O
:33–48	O
.	O
kolmogorov	O
,	O
a.	O
n.	O
(	O
1941	O
)	O
.	O
interpolation	O
und	O
extrapolation	O
von	O
station¨aren	O
zuf¨aligen	O
folgen	O
.	O
izv	O
.	O
akad	O
.	O
p.	O
29	O
nauk	O
sssr	O
,	O
5:3–14	O
.	O
k¨onig	O
,	O
h.	O
(	O
1986	O
)	O
.	O
eigenvalue	B
distribution	O
of	O
compact	O
operators	O
.	O
birkh¨auser	O
.	O
kullback	O
,	O
s.	O
(	O
1959	O
)	O
.	O
information	O
theory	O
and	O
statistics	O
.	O
dover	O
,	O
new	O
york	O
.	O
p.	O
96	O
pp	O
.	O
158	O
,	O
203	O
kuss	O
,	O
m.	O
and	O
rasmussen	O
,	O
c.	O
e.	O
(	O
2005	O
)	O
.	O
assessing	O
approximations	O
for	O
gaussian	O
process	B
classiﬁcation	O
.	O
in	O
weiss	O
,	O
y.	O
,	O
sch¨olkopf	O
,	O
b.	O
,	O
and	O
platt	O
,	O
j.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
18.	O
mit	O
press	O
.	O
pp	O
.	O
72	O
,	O
73	O
lanckriet	O
,	O
g.	O
r.	O
g.	O
,	O
cristianini	O
,	O
n.	O
,	O
bartlett	O
,	O
p.	O
l.	O
,	O
el	O
ghaoui	O
,	O
l.	O
,	O
and	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
2004	O
)	O
.	O
learning	B
the	O
kernel	B
matrix	O
with	O
semideﬁnite	O
programming	O
.	O
journal	O
of	O
machine	O
learning	B
research	O
,	O
5	O
(	O
1	O
)	O
:27–72	O
.	O
p.	O
128	O
lauritzen	O
,	O
s.	O
l.	O
(	O
1981	O
)	O
.	O
time	O
series	O
analysis	O
in	O
1880	O
:	O
a	O
discussion	O
of	O
contributions	O
made	O
by	O
p.	O
29	O
t.	O
n.	O
thiele	O
.	O
international	O
statistical	O
review	O
,	O
49:319–333	O
.	O
lawrence	O
,	O
n.	O
(	O
2004	O
)	O
.	O
gaussian	O
process	B
latent	O
variable	O
models	O
for	O
visualization	O
of	O
high	O
dimensional	O
data	O
.	O
in	O
thrun	O
,	O
s.	O
,	O
saul	O
,	O
l.	O
,	O
and	O
sch¨olkopf	O
,	O
b.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
p.	O
196	O
systems	O
16	O
,	O
pages	O
329–336	O
.	O
mit	O
press	O
.	O
lawrence	O
,	O
n.	O
,	O
seeger	O
,	O
m.	O
,	O
and	O
herbrich	O
,	O
r.	O
(	O
2003	O
)	O
.	O
fast	O
sparse	O
gaussian	O
process	B
methods	O
:	O
the	O
infor-	O
mative	O
vector	O
machine	O
.	O
in	O
becker	O
,	O
s.	O
,	O
thrun	O
,	O
s.	O
,	O
and	O
obermayer	O
,	O
k.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
pp	O
.	O
178	O
,	O
185	O
information	O
processing	O
systems	O
15	O
,	O
pages	O
625–632	O
.	O
mit	O
press	O
.	O
leslie	O
,	O
c.	O
,	O
eskin	O
,	O
e.	O
,	O
weston	O
,	O
j.	O
,	O
and	O
staﬀord	O
noble	O
,	O
w.	O
(	O
2003	O
)	O
.	O
mismatch	O
string	B
kernels	O
for	O
svm	O
in	O
becker	O
,	O
s.	O
,	O
thrun	O
,	O
s.	O
,	O
and	O
obermayer	O
,	O
k.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
pp	O
.	O
100	O
,	O
101	O
,	O
104	O
protein	O
classiﬁcation	B
.	O
information	O
processing	O
systems	O
15.	O
mit	O
press	O
.	O
lin	O
,	O
x.	O
,	O
wahba	O
,	O
g.	O
,	O
xiang	O
,	O
d.	O
,	O
gao	O
,	O
f.	O
,	O
klein	O
,	O
r.	O
,	O
and	O
klein	O
,	O
b	O
.	O
(	O
2000	O
)	O
.	O
smoothing	O
spline	O
anova	O
models	O
for	O
large	O
data	O
sets	O
with	O
bernoulli	O
observations	O
and	O
the	O
randomized	O
gacv	O
.	O
annals	O
of	O
statistics	O
,	O
28:1570–1600	O
.	O
p.	O
185	O
lindley	O
,	O
d.	O
v.	O
(	O
1985	O
)	O
.	O
making	O
decisions	O
.	O
john	O
wiley	O
and	O
sons	O
,	O
london	O
,	O
uk	O
,	O
second	O
edition	O
.	O
p.	O
111	O
lodhi	O
,	O
h.	O
,	O
shawe-taylor	O
,	O
j.	O
,	O
cristianini	O
,	O
n.	O
,	O
and	O
watkins	O
,	O
c.	O
j.	O
c.	O
h.	O
(	O
2001	O
)	O
.	O
text	O
classiﬁcation	B
using	O
string	B
kernels	O
.	O
in	O
leen	O
,	O
t.	O
k.	O
,	O
diettrich	O
,	O
t.	O
g.	O
,	O
and	O
tresp	O
,	O
v.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
p.	O
101	O
processing	O
systems	O
13.	O
mit	O
press	O
.	O
luo	O
,	O
z.	O
and	O
wahba	O
,	O
g.	O
(	O
1997	O
)	O
.	O
hybrid	O
adaptive	O
splines	B
.	O
j.	O
amer	O
.	O
statist	O
.	O
assoc.	O
,	O
92:107–116	O
.	O
p.	O
176	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1992a	O
)	O
.	O
a	O
practical	O
bayesian	O
framework	O
for	O
backpropagation	O
networks	O
.	O
neural	O
pp	O
.	O
109	O
,	O
166	O
,	O
167	O
computation	O
,	O
4	O
(	O
3	O
)	O
:448–472	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1992b	O
)	O
.	O
bayesian	O
interpolation	O
.	O
neural	O
computation	O
,	O
4	O
(	O
3	O
)	O
:415–447	O
.	O
pp	O
.	O
xiii	O
,	O
xvi	O
,	O
109	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1992c	O
)	O
.	O
information-based	O
objective	O
functions	O
for	O
active	O
data	O
selection	O
.	O
neural	O
p.	O
178	O
computation	O
,	O
4	O
(	O
4	O
)	O
:590–604	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
230	O
bibliography	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1992d	O
)	O
.	O
the	O
evidence	B
framework	O
applied	O
to	O
classiﬁcation	B
networks	O
.	O
neural	O
com-	O
p.	O
45	O
putation	O
,	O
4	O
(	O
5	O
)	O
:720–736	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1998	O
)	O
.	O
introduction	O
to	O
gaussian	O
processes	O
.	O
in	O
bishop	O
,	O
c.	O
m.	O
,	O
editor	O
,	O
neural	O
networks	O
pp	O
.	O
84	O
,	O
92	O
and	O
machine	O
learning	B
.	O
springer-verlag	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
1999	O
)	O
.	O
comparison	O
of	O
approximate	O
methods	O
for	O
handling	O
hyperparameters	B
.	O
neural	O
p.	O
110	O
computation	O
,	O
11	O
(	O
5	O
)	O
:1035–1068	O
.	O
mackay	O
,	O
d.	O
j.	O
c.	O
(	O
2003	O
)	O
.	O
information	O
theory	O
,	O
inference	O
,	O
and	O
learning	B
algorithms	O
.	O
cambridge	O
univer-	O
pp	O
.	O
xiv	O
,	O
167	O
sity	O
press	O
,	O
cambridge	O
,	O
uk	O
.	O
malzahn	O
,	O
d.	O
and	O
opper	O
,	O
m.	O
(	O
2002	O
)	O
.	O
a	O
variational	O
approach	O
to	O
learning	B
curves	O
.	O
in	O
diettrich	O
,	O
t.	O
g.	O
,	O
becker	O
,	O
s.	O
,	O
and	O
ghahramani	O
,	O
z.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
14.	O
mit	O
press	O
.	O
p.	O
161	O
mandelbrot	O
,	O
b.	O
b	O
.	O
(	O
1982	O
)	O
.	O
the	O
fractal	B
geometry	O
of	O
nature	O
.	O
w.	O
h.	O
freeman	O
,	O
san	O
francisco	O
.	O
p.	O
137	O
mardia	O
,	O
k.	O
v.	O
and	O
marshall	O
,	O
r.	O
j	O
.	O
(	O
1984	O
)	O
.	O
maximum	B
likelihood	I
estimation	O
for	O
models	O
of	O
residual	O
p.	O
115	O
covariance	B
in	O
spatial	O
regression	B
.	O
biometrika	O
,	O
71	O
(	O
1	O
)	O
:135–146	O
.	O
mat´ern	O
,	O
b	O
.	O
(	O
1960	O
)	O
.	O
spatial	O
variation	O
.	O
meddelanden	O
fr˚an	O
statens	O
skogsforskningsinstitut	O
,	O
49	O
,	O
no.5	O
.	O
pp	O
.	O
85	O
,	O
87	O
,	O
89	O
alm¨anna	O
f¨orlaget	O
,	O
stockholm	O
.	O
second	O
edition	O
(	O
1986	O
)	O
,	O
springer-verlag	O
,	O
berlin	O
.	O
matheron	O
,	O
g.	O
(	O
1973	O
)	O
.	O
the	O
intrinsic	O
random	O
functions	O
and	O
their	O
applications	O
.	O
advances	O
in	O
applied	O
p.	O
30	O
probability	B
,	O
5:439–468	O
.	O
maxwell	O
,	O
j.	O
c.	O
(	O
1850	O
)	O
.	O
letter	O
to	O
lewis	O
campbell	O
;	O
reproduced	O
in	O
l.	O
campbell	O
and	O
w.	O
garrett	O
,	O
the	O
life	O
p.	O
v	O
of	O
james	O
clerk	O
maxwell	O
,	O
macmillan	O
,	O
1881.	O
mcallester	O
,	O
d.	O
(	O
2003	O
)	O
.	O
pac-bayesian	O
stochastic	O
model	O
selection	O
.	O
machine	O
learning	B
,	O
51	O
(	O
1	O
)	O
:5–21	O
.	O
p.	O
164	O
mccullagh	O
,	O
p.	O
and	O
nelder	O
,	O
j	O
.	O
(	O
1983	O
)	O
.	O
generalized	B
linear	O
models	O
.	O
chapman	O
and	O
hall	O
.	O
pp	O
.	O
37	O
,	O
38	O
,	O
138	O
meinguet	O
,	O
j	O
.	O
(	O
1979	O
)	O
.	O
multivariate	O
interpolation	O
at	O
arbitrary	O
points	O
made	O
simple	O
.	O
journal	O
of	O
applied	O
p.	O
137	O
mathematics	O
and	O
physics	O
(	O
zamp	O
)	O
,	O
30:292–304	O
.	O
meir	O
,	O
r.	O
and	O
zhang	O
,	O
t.	O
(	O
2003	O
)	O
.	O
generalization	B
error	I
bounds	O
for	O
bayesian	O
mixture	O
algorithms	O
.	O
journal	O
p.	O
164	O
of	O
machine	O
learning	B
research	O
,	O
4	O
(	O
5	O
)	O
:839–860	O
.	O
micchelli	O
,	O
c.	O
a.	O
and	O
pontil	O
,	O
m.	O
(	O
2005	O
)	O
.	O
kernels	O
for	O
multi-task	B
learning	I
.	O
in	O
saul	O
,	O
l.	O
k.	O
,	O
weiss	O
,	O
y.	O
,	O
and	O
bottou	O
,	O
l.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
17	O
,	O
pages	O
921–928	O
.	O
mit	O
p.	O
190	O
press	O
.	O
micchelli	O
,	O
c.	O
a.	O
and	O
wahba	O
,	O
g.	O
(	O
1981	O
)	O
.	O
design	O
problems	O
for	O
optimal	B
surface	O
interpolation	O
.	O
in	O
ziegler	O
,	O
p.	O
161	O
z.	O
,	O
editor	O
,	O
approximation	O
theory	O
and	O
applications	O
,	O
pages	O
329–348	O
.	O
academic	O
press	O
.	O
minka	O
,	O
t.	O
p.	O
(	O
2001	O
)	O
.	O
a	O
family	O
of	O
algorithms	O
for	O
approximate	O
bayesian	O
inference	O
.	O
phd	O
thesis	O
,	O
mas-	O
pp	O
.	O
41	O
,	O
52	O
sachusetts	O
institute	O
of	O
technology	O
.	O
minka	O
,	O
t.	O
p.	O
(	O
2003	O
)	O
.	O
a	O
comparison	O
of	O
numerical	O
optimizers	O
for	O
logistic	B
regression	I
.	O
http	O
:	O
//research.microsoft.com/∼minka/papers/logreg	O
.	O
p.	O
38	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
bibliography	O
minka	O
,	O
t.	O
p.	O
and	O
picard	O
,	O
r.	O
w.	O
(	O
1999	O
)	O
.	O
learning	B
how	O
to	O
learn	O
is	O
learning	B
with	O
point	O
sets	O
.	O
http	O
:	O
//research.microsoft.com/∼minka/papers/point-sets.html	O
.	O
mitchell	O
,	O
t.	O
m.	O
(	O
1997	O
)	O
.	O
machine	O
learning	B
.	O
mcgraw-hill	O
,	O
new	O
york	O
.	O
231	O
p.	O
116	O
pp	O
.	O
2	O
,	O
165	O
murray-smith	O
,	O
r.	O
and	O
girard	O
,	O
a	O
.	O
(	O
2001	O
)	O
.	O
gaussian	O
process	B
priors	O
with	O
arma	O
noise	O
models	O
.	O
in	O
irish	O
signals	O
and	O
systems	O
conference	O
,	O
pages	O
147–153	O
,	O
maynooth	O
.	O
http	O
:	O
//www.dcs.gla.ac.uk/∼rod/	O
publications/murgir01.pdf	O
.	O
p.	O
191	O
neal	O
,	O
r.	O
m.	O
(	O
1996	O
)	O
.	O
bayesian	O
learning	B
for	O
neural	O
networks	O
.	O
springer	O
,	O
new	O
york	O
.	O
lecture	O
notes	O
in	O
pp	O
.	O
xiii	O
,	O
30	O
,	O
90	O
,	O
91	O
,	O
106	O
,	O
166	O
,	O
167	O
statistics	O
118.	O
neal	O
,	O
r.	O
m.	O
(	O
1997	O
)	O
.	O
monte	O
carlo	O
implementation	O
of	O
gaussian	O
process	B
models	O
for	O
bayesian	O
regres-	O
sion	O
and	O
classiﬁcation	B
.	O
technical	O
report	O
9702	O
,	O
department	O
of	O
statistics	O
,	O
university	O
of	O
toronto	O
.	O
http	O
:	O
//www.cs.toronto.edu/∼radford	O
.	O
p.	O
191	O
neal	O
,	O
r.	O
m.	O
(	O
1999	O
)	O
.	O
regression	B
and	O
classiﬁcation	B
using	O
gaussian	O
process	B
priors	O
.	O
in	O
bernardo	O
,	O
j.	O
m.	O
,	O
berger	O
,	O
j.	O
o.	O
,	O
dawid	O
,	O
a.	O
p.	O
,	O
and	O
smith	O
,	O
a.	O
f.	O
m.	O
,	O
editors	O
,	O
bayesian	O
statistics	O
6	O
,	O
pages	O
475–501	O
.	O
oxford	O
pp	O
.	O
41	O
,	O
47	O
university	O
press	O
.	O
(	O
with	O
discussion	O
)	O
.	O
neal	O
,	O
r.	O
m.	O
(	O
2001	O
)	O
.	O
annealed	O
importance	O
sampling	O
.	O
statistics	O
and	O
computing	O
,	O
11:125–139	O
.	O
p.	O
72	O
neumaier	O
,	O
a	O
.	O
(	O
2005	O
)	O
.	O
introduction	O
to	O
global	O
optimization	O
.	O
http	O
:	O
//www.mat.univie.ac.at/∼neum/	O
p.	O
193	O
glopt/intro.html	O
.	O
o	O
’	O
hagan	O
,	O
a	O
.	O
(	O
1978	O
)	O
.	O
curve	O
fitting	O
and	O
optimal	B
design	O
for	O
prediction	B
.	O
journal	O
of	O
the	O
royal	O
statistical	O
pp	O
.	O
28	O
,	O
30	O
,	O
94	O
society	O
b	O
,	O
40:1–42	O
.	O
(	O
with	O
discussion	O
)	O
.	O
o	O
’	O
hagan	O
,	O
a	O
.	O
(	O
1991	O
)	O
.	O
bayes-hermite	O
quadrature	O
.	O
journal	O
of	O
statistical	O
planning	O
and	O
inference	O
,	O
29:245–	O
pp	O
.	O
193	O
,	O
194	O
260.	O
o	O
’	O
hagan	O
,	O
a.	O
,	O
kennedy	O
,	O
m.	O
c.	O
,	O
and	O
oakley	O
,	O
j.	O
e.	O
(	O
1999	O
)	O
.	O
uncertainty	O
analysis	O
and	O
other	O
inference	O
tools	O
for	O
complex	O
computer	O
codes	O
.	O
in	O
bernardo	O
,	O
j.	O
m.	O
,	O
berger	O
,	O
j.	O
o.	O
,	O
dawid	O
,	O
a.	O
p.	O
,	O
and	O
smith	O
,	O
a.	O
f.	O
m.	O
,	O
editors	O
,	O
bayesian	O
statistics	O
6	O
,	O
pages	O
503–524	O
.	O
oxford	O
university	O
press	O
.	O
(	O
with	O
discussion	O
)	O
.	O
p.	O
194	O
øksendal	O
,	O
b	O
.	O
(	O
1985	O
)	O
.	O
stochastic	O
diﬀerential	O
equations	O
.	O
springer-verlag	O
,	O
berlin	O
.	O
p.	O
208	O
opper	O
,	O
m.	O
and	O
vivarelli	O
,	O
f.	O
(	O
1999	O
)	O
.	O
general	O
bounds	O
on	O
bayes	O
errors	O
for	O
regression	B
with	O
gaussian	O
processes	O
.	O
in	O
kearns	O
,	O
m.	O
s.	O
,	O
solla	O
,	O
s.	O
a.	O
,	O
and	O
cohn	O
,	O
d.	O
a.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
11	O
,	O
pages	O
302–308	O
.	O
mit	O
press	O
.	O
p.	O
160	O
opper	O
,	O
m.	O
and	O
winther	O
,	O
o	O
.	O
(	O
2000	O
)	O
.	O
gaussian	O
processes	O
for	O
classiﬁcation	B
:	O
mean-field	O
algorithms	O
.	O
pp	O
.	O
41	O
,	O
44	O
,	O
52	O
,	O
127	O
,	O
128	O
neural	O
computation	O
,	O
12	O
(	O
11	O
)	O
:2655–2684	O
.	O
o	O
’	O
sullivan	O
,	O
f.	O
,	O
yandell	O
,	O
b.	O
s.	O
,	O
and	O
raynor	O
,	O
w.	O
j	O
.	O
(	O
1986	O
)	O
.	O
automatic	O
smoothing	O
of	O
regression	B
functions	O
in	O
generalized	B
linear	O
models	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	O
,	O
81:96–103	O
.	O
pp	O
.	O
132	O
,	O
138	O
paciorek	O
,	O
c.	O
and	O
schervish	O
,	O
m.	O
j	O
.	O
(	O
2004	O
)	O
.	O
nonstationary	O
covariance	B
functions	O
for	O
gaussian	O
process	B
re-	O
gression	O
.	O
in	O
thrun	O
,	O
s.	O
,	O
saul	O
,	O
l.	O
,	O
and	O
sch¨olkopf	O
,	O
b.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
pp	O
.	O
93	O
,	O
94	O
systems	O
16.	O
mit	O
press	O
.	O
papoulis	O
,	O
a	O
.	O
(	O
1991	O
)	O
.	O
probability	B
,	O
random	O
variables	O
,	O
and	O
stochastic	O
processes	O
.	O
mcgraw-hill	O
,	O
new	O
york	O
.	O
pp	O
.	O
79	O
,	O
153	O
,	O
154	O
,	O
191	O
,	O
211	O
,	O
212	O
third	O
edition	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
232	O
bibliography	O
plaskota	O
,	O
l.	O
(	O
1996	O
)	O
.	O
noisy	O
information	O
and	O
computational	O
complexity	O
.	O
cambridge	O
university	O
press	O
,	O
pp	O
.	O
161	O
,	O
169	O
cambridge	O
.	O
plate	O
,	O
t.	O
a	O
.	O
(	O
1999	O
)	O
.	O
accuarcy	O
versus	O
interpretability	O
in	O
flexible	O
modeling	O
:	O
implementing	O
a	O
tradeoﬀ	O
p.	O
95	O
using	O
gaussian	O
process	B
models	O
.	O
behaviourmetrika	O
,	O
26	O
(	O
1	O
)	O
:29–50	O
.	O
platt	O
,	O
j.	O
c.	O
(	O
1999	O
)	O
.	O
fast	O
training	O
of	O
support	B
vector	I
machines	O
using	O
sequential	O
minimal	O
optimization	O
.	O
in	O
sch¨olkopf	O
,	O
b.	O
,	O
burges	O
,	O
c.	O
j.	O
c.	O
,	O
and	O
smola	O
,	O
a.	O
j.	O
,	O
editors	O
,	O
advances	O
in	O
kernel	B
methods	O
,	O
pages	O
p.	O
144	O
185–208	O
.	O
mit	O
press	O
.	O
platt	O
,	O
j.	O
c.	O
(	O
2000	O
)	O
.	O
probabilities	O
for	O
sv	O
machines	O
.	O
in	O
smola	O
,	O
a.	O
,	O
bartlett	O
,	O
p.	O
,	O
sch¨olkopf	O
,	O
b.	O
,	O
and	O
schuurmans	O
,	O
d.	O
,	O
editors	O
,	O
advances	O
in	O
large	O
margin	B
classiﬁers	O
,	O
pages	O
61–74	O
.	O
mit	O
press	O
.	O
pp	O
.	O
69	O
,	O
70	O
,	O
145	O
,	O
147	O
,	O
148	O
poggio	O
,	O
t.	O
and	O
girosi	O
,	O
f.	O
(	O
1990	O
)	O
.	O
networks	O
for	O
approximation	O
and	O
learning	B
.	O
proceedings	O
of	O
ieee	O
,	O
pp	O
.	O
89	O
,	O
133	O
,	O
134	O
,	O
135	O
,	O
147	O
,	O
176	O
78:1481–1497	O
.	O
poggio	O
,	O
t.	O
,	O
voorhees	O
,	O
h.	O
,	O
and	O
yuille	O
,	O
a	O
.	O
(	O
1985	O
)	O
.	O
a	O
regularized	O
solution	O
to	O
edge	O
detection	O
.	O
technical	O
p.	O
154	O
report	O
ai	O
memo	O
833	O
,	O
mit	O
ai	O
laboratory	O
.	O
pontil	O
,	O
m.	O
,	O
mukherjee	O
,	O
s.	O
,	O
and	O
girosi	O
,	O
f.	O
(	O
1998	O
)	O
.	O
on	O
the	O
noise	B
model	I
of	O
support	B
vector	I
machine	I
p.	O
146	O
regression	B
.	O
technical	O
report	O
ai	O
memo	O
1651	O
,	O
mit	O
ai	O
laboratory	O
.	O
press	O
,	O
w.	O
h.	O
,	O
teukolsky	O
,	O
s.	O
a.	O
,	O
vetterling	O
,	O
w.	O
t.	O
,	O
and	O
flannery	O
,	O
b.	O
p.	O
(	O
1992	O
)	O
.	O
numerical	O
recipes	O
in	O
c.	O
pp	O
.	O
96	O
,	O
99	O
,	O
201	O
cambridge	O
university	O
press	O
,	O
second	O
edition	O
.	O
qui˜nonero-candela	O
,	O
j	O
.	O
(	O
2004	O
)	O
.	O
learning	B
with	O
uncertainty—gaussian	O
processes	O
and	O
relevance	O
vector	O
machines	O
.	O
phd	O
thesis	O
,	O
informatics	O
and	O
mathematical	O
modelling	O
,	O
technical	O
univeristy	O
of	O
denmark	O
.	O
p.	O
177	O
rasmussen	O
,	O
c.	O
e.	O
(	O
1996	O
)	O
.	O
evaluation	O
of	O
gaussian	O
processes	O
and	O
other	O
methods	O
for	O
non-linear	O
re-	O
gression	O
.	O
phd	O
thesis	O
,	O
dept	O
.	O
of	O
computer	O
science	O
,	O
university	O
of	O
toronto	O
.	O
http	O
:	O
//www.kyb.mpg.de/	O
p.	O
30	O
publications/pss/ps2304.ps	O
.	O
rasmussen	O
,	O
c.	O
e.	O
(	O
2003	O
)	O
.	O
gaussian	O
processes	O
to	O
speed	O
up	O
hybrid	O
monte	O
carlo	O
for	O
expensive	O
bayesian	O
integrals	B
.	O
in	O
bernardo	O
,	O
j.	O
m.	O
,	O
bayarri	O
,	O
m.	O
j.	O
,	O
berger	O
,	O
j.	O
o.	O
,	O
dawid	O
,	O
a.	O
p.	O
,	O
heckerman	O
,	O
d.	O
,	O
smith	O
,	O
a.	O
f.	O
m.	O
,	O
and	O
west	O
,	O
m.	O
,	O
editors	O
,	O
bayesian	O
statistics	O
7	O
,	O
pages	O
651–659	O
.	O
oxford	O
university	O
press	O
.	O
p.	O
193	O
rasmussen	O
,	O
c.	O
e.	O
and	O
ghahramani	O
,	O
z	O
.	O
(	O
2001	O
)	O
.	O
occam	O
’	O
s	O
razor	O
.	O
in	O
leen	O
,	O
t.	O
,	O
dietterich	O
,	O
t.	O
g.	O
,	O
and	O
tresp	O
,	O
v.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
13	O
,	O
pages	O
294–300	O
.	O
mit	O
press	O
.	O
p.	O
110	O
rasmussen	O
,	O
c.	O
e.	O
and	O
ghahramani	O
,	O
z	O
.	O
(	O
2002	O
)	O
.	O
in	O
diettrich	O
,	O
t.	O
g.	O
,	O
becker	O
,	O
s.	O
,	O
and	O
ghahramani	O
,	O
z.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
14.	O
mit	O
press	O
.	O
p.	O
192	O
inﬁnite	O
mixtures	O
of	O
gaussian	O
process	B
experts	O
.	O
rasmussen	O
,	O
c.	O
e.	O
and	O
ghahramani	O
,	O
z	O
.	O
(	O
2003	O
)	O
.	O
bayesian	O
monte	O
carlo	O
.	O
in	O
suzanna	O
becker	O
,	O
s.	O
t.	O
and	O
obermayer	O
,	O
k.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
15	O
,	O
pages	O
489–496	O
.	O
mit	O
press	O
.	O
p.	O
193	O
rasmussen	O
,	O
c.	O
e.	O
and	O
qui˜nonero-candela	O
,	O
j	O
.	O
(	O
2005	O
)	O
.	O
healing	O
the	O
relevance	B
vector	I
machine	I
through	O
pp	O
.	O
150	O
,	O
176	O
augmentation	O
.	O
in	O
proc	O
.	O
22nd	O
international	O
conference	O
on	O
machine	O
learning	B
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
bibliography	O
233	O
rifkin	O
,	O
r.	O
and	O
klautau	O
,	O
a	O
.	O
(	O
2004	O
)	O
.	O
in	O
defense	O
of	O
one-vs-all	O
classiﬁcation	B
.	O
journal	O
of	O
machine	O
learning	B
pp	O
.	O
146	O
,	O
147	O
research	O
,	O
5:101–141	O
.	O
ripley	O
,	O
b	O
.	O
(	O
1981	O
)	O
.	O
spatial	O
statistics	O
.	O
wiley	O
,	O
new	O
york	O
.	O
p.	O
30	O
ripley	O
,	O
b	O
.	O
(	O
1996	O
)	O
.	O
pattern	O
recognition	O
and	O
neural	O
networks	O
.	O
cambridge	O
university	O
press	O
,	O
cambridge	O
,	O
p.	O
35	O
uk	O
.	O
ritter	O
,	O
k.	O
(	O
2000	O
)	O
.	O
average-case	O
analysis	O
of	O
numerical	O
problems	O
.	O
springer	O
verlag	O
.	O
pp	O
.	O
159	O
,	O
161	O
,	O
169	O
,	O
193	O
ritter	O
,	O
k.	O
,	O
wasilkowski	O
,	O
g.	O
w.	O
,	O
and	O
wo´zniakowski	O
,	O
h.	O
(	O
1995	O
)	O
.	O
multivariate	O
integration	O
and	O
approxima-	O
tion	O
of	O
random	O
fields	O
satisfying	O
sacks-ylvisaker	O
conditions	O
.	O
annals	O
of	O
applied	O
probability	B
,	O
5:518–540	O
.	O
p.	O
97	O
rousseeuw	O
,	O
p.	O
j	O
.	O
(	O
1984	O
)	O
.	O
least	O
median	O
of	O
squares	O
regression	B
.	O
journal	O
of	O
the	O
american	O
statistical	O
p.	O
146	O
association	O
,	O
79:871–880	O
.	O
sacks	O
,	O
j.	O
,	O
welch	O
,	O
w.	O
j.	O
,	O
mitchell	O
,	O
t.	O
j.	O
,	O
and	O
wynn	O
,	O
h.	O
p.	O
(	O
1989	O
)	O
.	O
design	O
and	O
analysis	O
of	O
computer	O
pp	O
.	O
16	O
,	O
30	O
experiments	O
.	O
statistical	O
science	O
,	O
4	O
(	O
4	O
)	O
:409–435	O
.	O
saitoh	O
,	O
s.	O
(	O
1988	O
)	O
.	O
theory	O
of	O
reproducing	O
kernels	O
and	O
its	O
applications	O
.	O
longman	O
,	O
harlow	O
,	O
england	O
.	O
p.	O
129	O
salton	O
,	O
g.	O
and	O
buckley	O
,	O
c.	O
(	O
1988	O
)	O
.	O
term-weighting	O
approaches	O
in	O
automatic	O
text	O
retrieval	O
.	O
information	O
p.	O
100	O
processing	O
and	O
management	O
,	O
24:513–523	O
.	O
sampson	O
,	O
p.	O
d.	O
and	O
guttorp	O
,	O
p.	O
(	O
1992	O
)	O
.	O
nonparametric	O
estimation	O
of	O
nonstationary	O
covariance	B
struc-	O
p.	O
92	O
ture	O
.	O
journal	O
of	O
the	O
american	O
statistical	O
association	O
,	O
87:108–119	O
.	O
santner	O
,	O
t.	O
j.	O
,	O
williams	O
,	O
b.	O
j.	O
,	O
and	O
notz	O
,	O
w.	O
(	O
2003	O
)	O
.	O
the	O
design	O
and	O
analysis	O
of	O
computer	O
experiments	O
.	O
p.	O
30	O
springer	O
,	O
new	O
york	O
.	O
saunders	O
,	O
c.	O
,	O
gammerman	O
,	O
a.	O
,	O
and	O
vovk	O
,	O
v.	O
(	O
1998	O
)	O
.	O
ridge	B
regression	I
learning	O
algorithm	O
in	O
dual	O
variables	O
.	O
in	O
shavlik	O
,	O
j.	O
,	O
editor	O
,	O
proceedings	O
of	O
the	O
fifteenth	O
international	O
conference	O
on	O
machine	O
learning	B
(	O
icml	O
1998	O
)	O
.	O
morgan	O
kaufmann	O
.	O
p.	O
30	O
saunders	O
,	O
c.	O
,	O
shawe-taylor	O
,	O
j.	O
,	O
and	O
vinokourov	O
,	O
a	O
.	O
(	O
2003	O
)	O
.	O
string	B
kernels	O
,	O
fisher	O
kernels	O
and	O
finite	O
state	O
automata	O
.	O
in	O
becker	O
,	O
s.	O
,	O
thrun	O
,	O
s.	O
,	O
and	O
obermayer	O
,	O
k.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
15.	O
mit	O
press	O
.	O
p.	O
101	O
schoenberg	O
,	O
i.	O
j	O
.	O
(	O
1938	O
)	O
.	O
metric	O
spaces	O
and	O
positive	B
deﬁnite	I
functions	O
.	O
trans	O
.	O
american	O
mathematical	O
p.	O
86	O
society	O
,	O
44	O
(	O
3	O
)	O
:522–536	O
.	O
schoenberg	O
,	O
i.	O
j	O
.	O
(	O
1964	O
)	O
.	O
spline	O
functions	O
and	O
the	O
problem	O
of	O
graduation	O
.	O
proc	O
.	O
nat	O
.	O
acad	O
.	O
sci	O
.	O
usa	O
,	O
pp	O
.	O
132	O
,	O
138	O
52:947–950	O
.	O
sch¨olkopf	O
,	O
b.	O
and	O
smola	O
,	O
a.	O
j	O
.	O
(	O
2002	O
)	O
.	O
learning	B
with	O
kernels	O
.	O
mit	O
press	O
.	O
pp	O
.	O
xvi	O
,	O
73	O
,	O
89	O
,	O
90	O
,	O
91	O
,	O
129	O
,	O
130	O
,	O
133	O
,	O
141	O
,	O
144	O
,	O
147	O
,	O
173	O
,	O
188	O
,	O
195	O
,	O
196	O
sch¨olkopf	O
,	O
b.	O
,	O
smola	O
,	O
a.	O
j.	O
,	O
and	O
m¨uller	O
,	O
k.-r.	O
(	O
1998	O
)	O
.	O
nonlinear	O
component	O
analysis	O
as	O
a	O
kernel	B
p.	O
99	O
eigenvalue	B
problem	O
.	O
neural	O
computation	O
,	O
10:1299–1319	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
234	O
bibliography	O
schwaighofer	O
,	O
a.	O
and	O
tresp	O
,	O
v.	O
(	O
2003	O
)	O
.	O
transductive	O
and	O
inductive	B
methods	O
for	O
approximate	O
gaus-	O
sian	O
process	B
regression	O
.	O
in	O
becker	O
,	O
s.	O
,	O
thrun	O
,	O
s.	O
,	O
and	O
obermayer	O
,	O
k.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
15.	O
mit	O
press	O
.	O
pp	O
.	O
181	O
,	O
184	O
scott	O
,	O
d.	O
w.	O
(	O
1992	O
)	O
.	O
multivariate	O
density	O
estimation	O
.	O
wiley	O
,	O
new	O
york	O
.	O
p.	O
25	O
seeger	O
,	O
m.	O
(	O
2000	O
)	O
.	O
bayesian	O
model	B
selection	O
for	O
support	B
vector	I
machines	O
,	O
gaussian	O
processes	O
and	O
other	O
kernel	B
classiﬁers	O
.	O
in	O
solla	O
,	O
s.	O
a.	O
,	O
leen	O
,	O
t.	O
k.	O
,	O
and	O
m¨uller	O
,	O
k.-r.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
pp	O
.	O
41	O
,	O
54	O
,	O
145	O
information	O
processing	O
systems	O
12.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
seeger	O
,	O
m.	O
(	O
2002	O
)	O
.	O
pac-bayesian	O
generalisation	O
error	B
bounds	O
for	O
gaussian	O
process	B
classiﬁcation	O
.	O
pp	O
.	O
161	O
,	O
164	O
,	O
165	O
journal	O
of	O
machine	O
learning	B
research	O
,	O
3:322–269	O
.	O
seeger	O
,	O
m.	O
(	O
2003	O
)	O
.	O
bayesian	O
gaussian	O
process	B
models	O
:	O
pac-bayesian	O
generalisation	O
error	B
bounds	O
and	O
sparse	O
approximations	O
.	O
phd	O
thesis	O
,	O
school	O
of	O
informatics	O
,	O
university	O
of	O
edinburgh	O
.	O
http	O
:	O
//www.cs.berkeley.edu/∼mseeger	O
.	O
pp	O
.	O
46	O
,	O
145	O
,	O
161	O
,	O
162	O
,	O
163	O
,	O
179	O
,	O
180	O
,	O
186	O
seeger	O
,	O
m.	O
(	O
2005	O
)	O
.	O
expectation	B
propagation	I
for	O
exponential	B
families	O
.	O
http	O
:	O
//www.cs.berkeley.edu/	O
p.	O
127	O
∼mseeger/papers/epexpfam.ps.gz	O
.	O
seeger	O
,	O
m.	O
and	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
2004	O
)	O
.	O
sparse	O
gaussian	O
process	B
classiﬁcation	O
with	O
multiple	O
classes	O
.	O
p.	O
50	O
technical	O
report	O
tr	O
661	O
,	O
department	O
of	O
statistics	O
,	O
university	O
of	O
california	O
at	O
berkeley	O
.	O
seeger	O
,	O
m.	O
,	O
williams	O
,	O
c.	O
k.	O
i.	O
,	O
and	O
lawrence	O
,	O
n.	O
(	O
2003	O
)	O
.	O
fast	O
forward	O
selection	O
to	O
speed	O
up	O
sparse	O
gaussian	O
process	B
regression	O
.	O
in	O
bishop	O
,	O
c.	O
and	O
frey	O
,	O
b.	O
j.	O
,	O
editors	O
,	O
proceedings	O
of	O
the	O
ninth	O
in-	O
ternational	O
workshop	O
on	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
.	O
society	O
for	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
.	O
pp	O
.	O
178	O
,	O
180	O
shawe-taylor	O
,	O
j.	O
and	O
williams	O
,	O
c.	O
k.	O
i	O
.	O
(	O
2003	O
)	O
.	O
the	O
stability	O
of	O
kernel	B
principal	O
components	O
analysis	O
and	O
its	O
relation	O
to	O
the	O
process	B
eigenspectrum	O
.	O
in	O
becker	O
,	O
s.	O
,	O
thrun	O
,	O
s.	O
,	O
and	O
obermayer	O
,	O
k.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
15.	O
mit	O
press	O
.	O
p.	O
99	O
shepp	O
,	O
l.	O
a	O
.	O
(	O
1966	O
)	O
.	O
radon-nikodym	O
derivatives	O
of	O
gaussian	O
measures	O
.	O
annals	O
of	O
mathematical	O
statis-	O
p.	O
139	O
tics	O
,	O
37	O
(	O
2	O
)	O
:321–354	O
.	O
silverman	O
,	O
b.	O
w.	O
(	O
1978	O
)	O
.	O
density	O
ratios	O
,	O
empirical	O
likelihood	B
and	O
cot	O
death	O
.	O
applied	O
statistics	O
,	O
p.	O
138	O
27	O
(	O
1	O
)	O
:26–33	O
.	O
silverman	O
,	O
b.	O
w.	O
(	O
1984	O
)	O
.	O
spline	O
smoothing	O
:	O
the	O
equivalent	B
variable	O
kernel	B
method	O
.	O
annals	O
of	O
pp	O
.	O
25	O
,	O
153	O
,	O
154	O
statistics	O
,	O
12	O
(	O
3	O
)	O
:898–916	O
.	O
silverman	O
,	O
b.	O
w.	O
(	O
1985	O
)	O
.	O
some	O
aspects	O
of	O
the	O
spline	O
smoothing	O
approach	O
to	O
non-parametric	B
regression	O
pp	O
.	O
170	O
,	O
175	O
curve	O
fitting	O
(	O
with	O
discussion	O
)	O
.	O
j.	O
roy	O
.	O
stat	O
.	O
soc	O
.	O
b	O
,	O
47	O
(	O
1	O
)	O
:1–52	O
.	O
simard	O
,	O
p.	O
,	O
victorri	O
,	O
b.	O
,	O
le	O
cun	O
,	O
y.	O
,	O
and	O
denker	O
,	O
j	O
.	O
(	O
1992	O
)	O
.	O
tangent	O
prop—a	O
formalism	O
for	O
specifying	O
selected	O
invariances	B
in	O
an	O
adaptive	O
network	O
.	O
in	O
moody	O
,	O
j.	O
e.	O
,	O
hanson	O
,	O
s.	O
j.	O
,	O
and	O
lippmann	O
,	O
r.	O
p.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
4	O
,	O
pages	O
895–903	O
.	O
morgan	O
kaufmann	O
.	O
pp	O
.	O
73	O
,	O
195	O
smola	O
,	O
a.	O
j.	O
and	O
bartlett	O
,	O
p.	O
l.	O
(	O
2001	O
)	O
.	O
sparse	O
greedy	O
gaussian	O
process	B
regression	O
.	O
in	O
leen	O
,	O
t.	O
k.	O
,	O
diettrich	O
,	O
t.	O
g.	O
,	O
and	O
tresp	O
,	O
v.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
13	O
,	O
pages	O
p.	O
176	O
619–625	O
.	O
mit	O
press	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
bibliography	O
235	O
smola	O
,	O
a.	O
j.	O
and	O
sch¨olkopf	O
,	O
b	O
.	O
(	O
2000	O
)	O
.	O
sparse	O
greedy	O
matrix	B
approximation	O
for	O
machine	O
learning	B
.	O
in	O
proceedings	O
of	O
the	O
seventeenth	O
international	O
conference	O
on	O
machine	O
learning	B
.	O
morgan	O
kaufmann	O
.	O
pp	O
.	O
173	O
,	O
174	O
solak	O
,	O
e.	O
,	O
murray-smith	O
,	O
r.	O
,	O
leithead	O
,	O
w.	O
e.	O
,	O
leith	O
,	O
d.	O
,	O
and	O
rasmussen	O
,	O
c.	O
e.	O
(	O
2003	O
)	O
.	O
derivative	B
observations	I
in	O
gaussian	O
process	B
models	O
of	O
dynamic	O
systems	O
.	O
in	O
becker	O
,	O
s.	O
,	O
s.	O
t.	O
and	O
obermayer	O
,	O
k.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
15	O
,	O
pages	O
1033–1040	O
.	O
mit	O
press	O
.	O
p.	O
191	O
sollich	O
,	O
p.	O
(	O
1999	O
)	O
.	O
learning	B
curves	O
for	O
gaussian	O
processes	O
.	O
in	O
kearns	O
,	O
m.	O
s.	O
,	O
solla	O
,	O
s.	O
a.	O
,	O
and	O
cohn	O
,	O
pp	O
.	O
160	O
,	O
161	O
d.	O
a.	O
,	O
editors	O
,	O
neural	O
information	O
processing	O
systems	O
,	O
vol	O
.	O
11.	O
mit	O
press	O
.	O
sollich	O
,	O
p.	O
(	O
2002	O
)	O
.	O
bayesian	O
methods	O
for	O
support	B
vector	I
machines	O
:	O
evidence	B
and	O
predictive	B
class	O
pp	O
.	O
145	O
,	O
150	O
,	O
161	O
probabilities	O
.	O
machine	O
learning	B
,	O
46:21–52	O
.	O
sollich	O
,	O
p.	O
and	O
williams	O
,	O
c.	O
k.	O
i	O
.	O
(	O
2005	O
)	O
.	O
using	O
the	O
equivalent	B
kernel	I
to	O
understand	O
gaussian	O
process	B
in	O
saul	O
,	O
l.	O
k.	O
,	O
weiss	O
,	O
y.	O
,	O
and	O
bottou	O
,	O
l.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
p.	O
154	O
regression	B
.	O
processing	O
systems	O
17.	O
mit	O
press	O
.	O
stein	O
,	O
m.	O
l.	O
(	O
1991	O
)	O
.	O
a	O
kernel	B
approximation	O
to	O
the	O
kriging	B
predictor	O
of	O
a	O
spatial	O
process	B
.	O
ann	O
.	O
inst	O
.	O
p.	O
154	O
statist	O
.	O
math	O
,	O
43	O
(	O
1	O
)	O
:61–75	O
.	O
stein	O
,	O
m.	O
l.	O
(	O
1999	O
)	O
.	O
interpolation	O
of	O
spatial	O
data	O
.	O
springer-verlag	O
,	O
new	O
york	O
.	O
pp	O
.	O
82	O
,	O
83	O
,	O
85	O
,	O
86	O
,	O
87	O
,	O
115	O
,	O
137	O
,	O
157	O
,	O
158	O
,	O
161	O
,	O
212	O
steinwart	O
,	O
i	O
.	O
(	O
2005	O
)	O
.	O
consistency	B
of	O
support	B
vector	I
machines	O
and	O
other	O
regularized	O
kernel	B
classiﬁers	O
.	O
p.	O
157	O
ieee	O
trans	O
.	O
on	O
information	O
theory	O
,	O
51	O
(	O
1	O
)	O
:128–142	O
.	O
stitson	O
,	O
m.	O
o.	O
,	O
gammerman	O
,	O
a.	O
,	O
vapnik	O
,	O
v.	O
n.	O
,	O
vovk	O
,	O
v.	O
,	O
watkins	O
,	O
c.	O
j.	O
c.	O
h.	O
,	O
and	O
weston	O
,	O
j	O
.	O
(	O
1999	O
)	O
.	O
support	B
vector	I
regression	I
with	O
anova	O
decomposition	O
kernels	O
.	O
in	O
sch¨olkopf	O
,	O
b.	O
,	O
burges	O
,	O
c.	O
j.	O
c.	O
,	O
and	O
smola	O
,	O
a.	O
j.	O
,	O
editors	O
,	O
advances	O
in	O
kernel	B
methods	O
.	O
mit	O
press	O
.	O
p.	O
95	O
sundararajan	O
,	O
s.	O
and	O
keerthi	O
,	O
s.	O
s.	O
(	O
2001	O
)	O
.	O
predictive	B
approaches	O
for	O
choosing	O
hyperparameters	B
in	O
p.	O
117	O
gaussian	O
processes	O
.	O
neural	O
computation	O
,	O
13:1103–1118	O
.	O
suykens	O
,	O
j.	O
a.	O
k.	O
and	O
vanderwalle	O
,	O
j	O
.	O
(	O
1999	O
)	O
.	O
least	O
squares	O
support	B
vector	I
machines	O
.	O
neural	O
processing	O
p.	O
147	O
letters	O
,	O
9:293–300	O
.	O
szeliski	O
,	O
r.	O
(	O
1987	O
)	O
.	O
regularization	B
uses	O
fractal	B
priors	O
.	O
in	O
proceedings	O
of	O
the	O
6th	O
national	O
conference	O
pp	O
.	O
135	O
,	O
137	O
on	O
artiﬁcial	O
intelligence	O
(	O
aaai-87	O
)	O
.	O
teh	O
,	O
y.	O
w.	O
,	O
seeger	O
,	O
m.	O
,	O
and	O
jordan	O
,	O
m.	O
i	O
.	O
(	O
2005	O
)	O
.	O
semiparametric	O
latent	O
factor	O
models	O
.	O
in	O
cowell	O
,	O
r.	O
g.	O
and	O
ghahramani	O
,	O
z.	O
,	O
editors	O
,	O
proceedings	O
of	O
tenth	O
international	O
workshop	O
on	O
artiﬁcial	O
inte	O
lligence	O
and	O
statistics	O
,	O
pages	O
333–340	O
.	O
society	O
for	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
.	O
p.	O
190	O
thomas-agnan	O
,	O
c.	O
(	O
1996	O
)	O
.	O
computing	O
a	O
family	O
of	O
reproducing	O
kernels	O
for	O
statistical	O
applications	O
.	O
p.	O
154	O
numerical	O
algorithms	O
,	O
13:21–32	O
.	O
thompson	O
,	O
p.	O
d.	O
(	O
1956	O
)	O
.	O
optimum	O
smoothing	O
of	O
two-dimensional	O
fields	O
.	O
tellus	O
,	O
8:384–393	O
.	O
p.	O
30	O
tikhonov	O
,	O
a.	O
n.	O
(	O
1963	O
)	O
.	O
solution	O
of	O
incorrectly	O
formulated	O
problems	O
and	O
the	O
regularization	B
method	O
.	O
p.	O
133	O
soviet	O
.	O
math	O
.	O
dokl.	O
,	O
5:1035–1038	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
236	O
bibliography	O
tikhonov	O
,	O
a.	O
n.	O
and	O
arsenin	O
,	O
v.	O
y	O
.	O
(	O
1977	O
)	O
.	O
solutions	O
of	O
ill-posed	O
problems	O
.	O
w.	O
h.	O
winston	O
,	O
washington	O
,	O
p.	O
133	O
d.c.	O
tipping	O
,	O
m.	O
e.	O
(	O
2001	O
)	O
.	O
sparse	O
bayesian	O
learning	B
and	O
the	O
relevance	B
vector	I
machine	I
.	O
journal	O
of	O
machine	O
p.	O
149	O
learning	B
research	O
,	O
1:211–244	O
.	O
tipping	O
,	O
m.	O
e.	O
and	O
faul	O
,	O
a.	O
c.	O
(	O
2003	O
)	O
.	O
fast	O
marginal	B
likelihood	I
maximisation	O
for	O
sparse	O
bayesian	O
models	O
.	O
in	O
bishop	O
,	O
c.	O
m.	O
and	O
frey	O
,	O
b.	O
j.	O
,	O
editors	O
,	O
proceedings	O
of	O
ninth	O
international	O
workshop	O
on	O
p.	O
149	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
.	O
society	O
for	O
artiﬁcial	O
intelligence	O
and	O
statistics	O
.	O
tresp	O
,	O
v.	O
(	O
2000	O
)	O
.	O
a	O
bayesian	O
committee	O
machine	O
.	O
neural	O
computation	O
,	O
12	O
(	O
11	O
)	O
:2719–2741	O
.	O
pp	O
.	O
180	O
,	O
181	O
,	O
185	O
,	O
187	O
tsuda	O
,	O
k.	O
,	O
kawanabe	O
,	O
m.	O
,	O
r¨atsch	O
,	O
g.	O
,	O
sonnenburg	O
,	O
s.	O
,	O
and	O
m¨uller	O
,	O
k.-r.	O
(	O
2002	O
)	O
.	O
a	O
new	O
discriminative	O
p.	O
102	O
kernel	B
from	O
probabilistic	B
models	O
.	O
neural	O
computation	O
,	O
14	O
(	O
10	O
)	O
:2397–2414	O
.	O
uhlenbeck	O
,	O
g.	O
e.	O
and	O
ornstein	O
,	O
l.	O
s.	O
(	O
1930	O
)	O
.	O
on	O
the	O
theory	O
of	O
brownian	O
motion	O
.	O
phys	O
.	O
rev.	O
,	O
36:823–841	O
.	O
pp	O
.	O
86	O
,	O
212	O
valiant	O
,	O
l.	O
g.	O
(	O
1984	O
)	O
.	O
a	O
theory	O
of	O
the	O
learnable	O
.	O
communications	O
of	O
the	O
acm	O
,	O
27	O
(	O
11	O
)	O
:1134–1142	O
.	O
p.	O
161	O
vapnik	O
,	O
v.	O
n.	O
(	O
1995	O
)	O
.	O
the	O
nature	O
of	O
statistical	O
learning	B
theory	O
.	O
springer	O
verlag	O
,	O
new	O
york	O
.	O
pp	O
.	O
36	O
,	O
141	O
,	O
181	O
vapnik	O
,	O
v.	O
n.	O
(	O
1998	O
)	O
.	O
statistical	O
learning	B
theory	O
.	O
john	O
wiley	O
and	O
sons	O
.	O
p.	O
140	O
vijayakumar	O
,	O
s.	O
,	O
d	O
’	O
souza	O
,	O
a.	O
,	O
and	O
schaal	O
,	O
s.	O
(	O
2005	O
)	O
.	O
incremental	O
online	O
learning	B
in	O
high	O
dimensions	O
.	O
pp	O
.	O
22	O
,	O
24	O
accepted	O
for	O
publication	O
in	O
neural	O
computation	O
.	O
vijayakumar	O
,	O
s.	O
,	O
d	O
’	O
souza	O
,	O
a.	O
,	O
shibata	O
,	O
t.	O
,	O
conradt	O
,	O
j.	O
,	O
and	O
schaal	O
,	O
s.	O
(	O
2002	O
)	O
.	O
statistical	O
learning	B
for	O
p.	O
22	O
humanoid	O
robots	O
.	O
autonomous	O
robot	B
,	O
12	O
(	O
1	O
)	O
:55–69	O
.	O
vijayakumar	O
,	O
s.	O
and	O
schaal	O
,	O
s.	O
(	O
2000	O
)	O
.	O
lwpr	O
:	O
an	O
o	O
(	O
n	O
)	O
algorithm	O
for	O
incremental	O
real	O
time	O
learning	B
in	O
high	O
dimensional	O
space	O
.	O
in	O
proc	O
.	O
of	O
the	O
seventeenth	O
international	O
conference	O
on	O
machine	O
learning	B
(	O
icml	O
2000	O
)	O
,	O
pages	O
1079–1086	O
.	O
p.	O
22	O
vishwanathan	O
,	O
s.	O
v.	O
n.	O
and	O
smola	O
,	O
a.	O
j	O
.	O
(	O
2003	O
)	O
.	O
fast	O
kernels	O
for	O
string	B
and	O
tree	O
matching	O
.	O
in	O
becker	O
,	O
s.	O
,	O
thrun	O
,	O
s.	O
,	O
and	O
obermayer	O
,	O
k.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
15.	O
mit	O
press	O
.	O
p.	O
101	O
vivarelli	O
,	O
f.	O
and	O
williams	O
,	O
c.	O
k.	O
i	O
.	O
(	O
1999	O
)	O
.	O
discovering	O
hidden	O
features	O
with	O
gaussian	O
processes	O
regression	B
.	O
in	O
kearns	O
,	O
m.	O
s.	O
,	O
solla	O
,	O
s.	O
a.	O
,	O
and	O
cohn	O
,	O
d.	O
a.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
11.	O
mit	O
press	O
.	O
p.	O
89	O
von	O
mises	O
,	O
r.	O
(	O
1964	O
)	O
.	O
mathematical	O
theory	O
of	O
probability	B
and	O
statistics	O
.	O
academic	O
press	O
.	O
p.	O
200	O
wahba	O
,	O
g.	O
(	O
1978	O
)	O
.	O
improper	O
priors	O
,	O
spline	O
smoothing	O
and	O
the	O
problem	O
of	O
guarding	O
against	O
model	B
p.	O
139	O
errors	O
in	O
regression	B
.	O
journal	O
of	O
the	O
royal	O
statistical	O
society	O
b	O
,	O
40	O
(	O
3	O
)	O
:364–372	O
.	O
wahba	O
,	O
g.	O
(	O
1985	O
)	O
.	O
a	O
comparison	O
of	O
gcv	O
and	O
gml	O
for	O
choosing	O
the	O
smoothing	O
parameter	O
in	O
the	O
p.	O
29	O
generalized	B
spline	O
smoothing	O
problem	O
.	O
annals	O
of	O
statistics	O
,	O
13:1378–1402	O
.	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
bibliography	O
237	O
wahba	O
,	O
g.	O
(	O
1990	O
)	O
.	O
society	O
for	O
industrial	O
and	O
applied	O
mathematics	O
,	O
philadelphia	O
,	O
pa.	O
cbms-nsf	O
regional	O
conference	O
series	O
in	O
applied	O
mathematics	O
.	O
pp	O
.	O
95	O
,	O
112	O
,	O
117	O
,	O
118	O
,	O
129	O
,	O
131	O
,	O
137	O
,	O
138	O
,	O
157	O
,	O
176	O
spline	O
models	O
for	O
observational	O
data	O
.	O
wahba	O
,	O
g.	O
,	O
johnson	O
,	O
d.	O
r.	O
,	O
gao	O
,	O
f.	O
,	O
and	O
gong	O
,	O
j	O
.	O
(	O
1995	O
)	O
.	O
adaptive	O
tuning	O
of	O
numerical	O
weather	O
prediction	B
models	O
:	O
randomized	O
gcv	O
in	O
three-and	O
four-dimensional	O
data	O
assimilation	O
.	O
monthly	O
weather	O
review	O
,	O
123:3358–3369	O
.	O
p.	O
181	O
watkins	O
,	O
c.	O
j.	O
c.	O
h.	O
(	O
1999	O
)	O
.	O
dynamic	O
alignment	B
kernels	O
.	O
technical	O
report	O
csd-tr-98-11	O
,	O
dept	O
of	O
p.	O
101	O
computer	O
science	O
,	O
royal	O
holloway	O
,	O
university	O
of	O
london	O
.	O
watkins	O
,	O
c.	O
j.	O
c.	O
h.	O
(	O
2000	O
)	O
.	O
dynamic	O
alignment	B
kernels	O
.	O
in	O
smola	O
,	O
a.	O
j.	O
,	O
bartlett	O
,	O
p.	O
l.	O
,	O
and	O
sch¨olkopf	O
,	O
b.	O
,	O
editors	O
,	O
advances	O
in	O
large	O
margin	B
classiﬁers	O
,	O
pages	O
39–50	O
.	O
mit	O
press	O
,	O
cambridge	O
,	O
ma	O
.	O
p.	O
100	O
wegman	O
,	O
e.	O
j	O
.	O
(	O
1982	O
)	O
.	O
reproducing	O
kernel	O
hilbert	O
spaces	O
.	O
in	O
kotz	O
,	O
s.	O
and	O
johnson	O
,	O
n.	O
l.	O
,	O
editors	O
,	O
pp	O
.	O
129	O
,	O
130	O
encyclopedia	O
of	O
statistical	O
sciences	O
,	O
volume	O
8	O
,	O
pages	O
81–84	O
.	O
wiley	O
,	O
new	O
york	O
.	O
weinert	O
,	O
h.	O
l.	O
,	O
editor	O
(	O
1982	O
)	O
.	O
reproducing	O
kernel	O
hilbert	O
spaces	O
.	O
hutchinson	O
ross	O
,	O
stroudsburg	O
,	O
p.	O
129	O
pennsylvania	O
.	O
wendland	O
,	O
h.	O
(	O
2005	O
)	O
.	O
scattered	O
data	O
approximation	O
.	O
cambridge	O
monographs	O
on	O
applied	O
and	O
compu-	O
p.	O
88	O
tational	O
mathematics	O
.	O
cambridge	O
university	O
press	O
.	O
whittle	O
,	O
p.	O
(	O
1963	O
)	O
.	O
prediction	B
and	O
regulation	O
by	O
linear	B
least-square	O
methods	O
.	O
english	O
universities	O
pp	O
.	O
30	O
,	O
216	O
press	O
.	O
widom	O
,	O
h.	O
(	O
1963	O
)	O
.	O
asymptotic	O
behavior	O
of	O
the	O
eigenvalues	O
of	O
certain	O
integral	O
equations	O
.	O
trans	O
.	O
of	O
the	O
p.	O
97	O
american	O
mathematical	O
society	O
,	O
109	O
(	O
2	O
)	O
:278–295	O
.	O
widom	O
,	O
h.	O
(	O
1964	O
)	O
.	O
asymptotic	O
behavior	O
of	O
the	O
eigenvalues	O
of	O
certain	O
integral	O
equations	O
ii	O
.	O
archive	O
p.	O
97	O
for	O
rational	O
mechanics	O
and	O
analysis	O
,	O
17:215–229	O
.	O
wiener	O
,	O
n.	O
(	O
1949	O
)	O
.	O
extrapolation	O
,	O
interpolation	O
and	O
smoothing	O
of	O
stationary	O
time	O
series	O
.	O
mit	O
press	O
,	O
p.	O
29	O
cambridge	O
,	O
mass	O
.	O
williams	O
,	O
c.	O
k.	O
i	O
.	O
(	O
1998	O
)	O
.	O
computation	O
with	O
inﬁnite	O
neural	O
networks	O
.	O
neural	O
computation	O
,	O
10	O
(	O
5	O
)	O
:1203–	O
p.	O
91	O
1216.	O
williams	O
,	O
c.	O
k.	O
i.	O
and	O
barber	O
,	O
d.	O
(	O
1998	O
)	O
.	O
bayesian	O
classiﬁcation	B
with	O
gaussian	O
processes	O
.	O
transactions	O
on	O
pattern	O
analysis	O
and	O
machine	O
intelligence	O
,	O
20	O
(	O
12	O
)	O
:1342–1351	O
.	O
ieee	O
pp	O
.	O
41	O
,	O
45	O
,	O
48	O
,	O
49	O
williams	O
,	O
c.	O
k.	O
i.	O
and	O
rasmussen	O
,	O
c.	O
e.	O
(	O
1996	O
)	O
.	O
gaussian	O
processes	O
for	O
regression	B
.	O
in	O
touretzky	O
,	O
d.	O
s.	O
,	O
mozer	O
,	O
m.	O
c.	O
,	O
and	O
hasselmo	O
,	O
m.	O
e.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
8	O
,	O
pages	O
514–520	O
.	O
mit	O
press	O
.	O
pp	O
.	O
30	O
,	O
107	O
williams	O
,	O
c.	O
k.	O
i.	O
,	O
rasmussen	O
,	O
c.	O
e.	O
,	O
schwaighofer	O
,	O
a.	O
,	O
and	O
tresp	O
,	O
v.	O
(	O
2002	O
)	O
.	O
observations	O
on	O
the	O
nystr¨om	O
method	O
for	O
gaussian	O
process	B
prediction	O
.	O
technical	O
report	O
,	O
university	O
of	O
edinburgh	O
.	O
http	O
:	O
//www.dai.ed.ac.uk/homes/ckiw/online	O
pubs.html	O
.	O
p.	O
177	O
williams	O
,	O
c.	O
k.	O
i.	O
and	O
seeger	O
,	O
m.	O
(	O
2001	O
)	O
.	O
using	O
the	O
nystr¨om	O
method	O
to	O
speed	O
up	O
kernel	B
machines	O
.	O
in	O
leen	O
,	O
t.	O
k.	O
,	O
diettrich	O
,	O
t.	O
g.	O
,	O
and	O
tresp	O
,	O
v.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
13	O
,	O
pages	O
682–688	O
.	O
mit	O
press	O
.	O
pp	O
.	O
173	O
,	O
177	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O
238	O
bibliography	O
williams	O
,	O
c.	O
k.	O
i.	O
and	O
vivarelli	O
,	O
f.	O
(	O
2000	O
)	O
.	O
upper	O
and	O
lower	O
bounds	O
on	O
the	O
learning	B
curve	I
for	O
gaussian	O
pp	O
.	O
31	O
,	O
161	O
,	O
168	O
proccesses	O
.	O
machine	O
learning	B
,	O
40:77–102	O
.	O
winkler	O
,	O
g.	O
(	O
1995	O
)	O
.	O
image	O
analysis	O
,	O
random	O
fields	O
and	O
dynamic	O
monte	O
carlo	O
methods	O
.	O
springer	O
,	O
p.	O
219	O
berlin	O
.	O
wong	O
,	O
e.	O
(	O
1971	O
)	O
.	O
stochastic	O
processes	O
in	O
information	O
and	O
dynamical	O
systems	O
.	O
mcgraw-hill	O
,	O
new	O
york	O
.	O
p.	O
218	O
wood	O
,	O
s.	O
and	O
kohn	O
,	O
r.	O
(	O
1998	O
)	O
.	O
a	O
bayesian	O
approach	O
to	O
robust	O
binary	B
nonparametric	O
regression	B
.	O
j.	O
p.	O
45	O
american	O
statistical	O
association	O
,	O
93	O
(	O
441	O
)	O
:203–213	O
.	O
yaglom	O
,	O
a.	O
m.	O
(	O
1987	O
)	O
.	O
correlation	O
theory	O
of	O
stationary	O
and	O
related	O
random	O
functions	O
volume	O
i	O
:	O
basic	O
p.	O
89	O
results	O
.	O
springer	O
verlag	O
.	O
yang	O
,	O
c.	O
,	O
duraiswami	O
,	O
r.	O
,	O
and	O
david	O
,	O
l.	O
(	O
2005	O
)	O
.	O
eﬃcient	O
kernel	B
machines	O
using	O
the	O
improved	O
fast	O
gauss	O
transform	O
.	O
in	O
saul	O
,	O
l.	O
k.	O
,	O
weiss	O
,	O
y.	O
,	O
and	O
bottou	O
,	O
l.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
17.	O
mit	O
press	O
.	O
p.	O
182	O
ylvisaker	O
,	O
d.	O
(	O
1975	O
)	O
.	O
designs	O
on	O
random	O
fields	O
.	O
in	O
srivastava	O
,	O
j.	O
n.	O
,	O
editor	O
,	O
a	O
survey	O
of	O
statistical	O
p.	O
159	O
design	O
and	O
linear	B
models	O
,	O
pages	O
593–608	O
.	O
north-holland	O
.	O
yuille	O
,	O
a.	O
and	O
grzywacz	O
,	O
n.	O
m.	O
(	O
1989	O
)	O
.	O
a	O
mathematical	O
analysis	O
of	O
motion	O
coherence	O
theory	O
.	O
inter-	O
p.	O
134	O
national	O
journal	O
of	O
computer	O
vision	O
,	O
3:155–175	O
.	O
zhang	O
,	O
t.	O
(	O
2004	O
)	O
.	O
statistical	O
behaviour	O
and	O
consistency	B
of	O
classiﬁcation	B
methods	O
based	O
on	O
convex	B
p.	O
157	O
risk	B
minimization	O
(	O
with	O
discussion	O
)	O
.	O
annals	O
of	O
statistics	O
,	O
32	O
(	O
1	O
)	O
:56–85	O
.	O
zhu	O
,	O
h.	O
,	O
williams	O
,	O
c.	O
k.	O
i.	O
,	O
rohwer	O
,	O
r.	O
j.	O
,	O
and	O
morciniec	O
,	O
m.	O
(	O
1998	O
)	O
.	O
gaussian	O
regression	B
and	O
optimal	B
finite	O
dimensional	O
linear	B
models	O
.	O
in	O
bishop	O
,	O
c.	O
m.	O
,	O
editor	O
,	O
neural	O
networks	O
and	O
machine	O
learning	B
.	O
springer-verlag	O
,	O
berlin	O
.	O
p.	O
97	O
zhu	O
,	O
j.	O
and	O
hastie	O
,	O
t.	O
j	O
.	O
(	O
2002	O
)	O
.	O
kernel	B
logistic	O
regression	B
and	O
the	O
import	O
vector	O
machine	O
.	O
in	O
diettrich	O
,	O
t.	O
g.	O
,	O
becker	O
,	O
s.	O
,	O
and	O
ghahramani	O
,	O
z.	O
,	O
editors	O
,	O
advances	O
in	O
neural	O
information	O
processing	O
systems	O
14	O
,	O
pages	O
1081–1088	O
.	O
mit	O
press	O
.	O
p.	O
185	O
c.	O
e.	O
rasmussen	O
&	O
c.	O
k.	O
i.	O
williams	O
,	O
gaussian	O
processes	O
for	O
machine	O
learning	B
,	O
the	O
mit	O
press	O
,	O
2006	O
,	O
c	O
(	O
cid:13	O
)	O
2006	O
massachusetts	O
institute	O
of	O
technology	O
.	O
www.gaussianprocess.org/gpml	O
isbn	O
026218253x	O
.	O